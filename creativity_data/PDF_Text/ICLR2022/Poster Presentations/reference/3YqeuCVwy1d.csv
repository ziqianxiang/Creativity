title,year,conference
 Iterative procedures for nonlinear integral equations,1965,1965
 Nonlinear acceleration of mo-mentum and primal-dual algorithms,2018, arXiv preprint arXiv:1810
 On the weak convergence of an ergodic iteration for the solution of variationalinequalities for monotone operators in hilbert space,1977, Journal of Mathematical Analysis andApplications
 On the regularizing properties of the gmresmethod,2002, Numerische Mathematik
 Chebyshev Polynomials are not always oPtimal,1991, Journal ofApproximation Theory
 A varia-tional inequality PersPective on generative adversarial networks,2019, In 7th International Conferenceon Learning Representations
 Generative adversarial nets,2014, In NIPS
 Iterative methods for solving linear systems,1997, SIAM
 Ganstrained by a two time-scale update rule converge to a local nash equilibrium,2017, In Advances in NeuralInformation Processing Systems
 On the convergence of single-callstochastic extra-gradient methods,2019, In NeurIPS
 Learning multiple layers of features from tiny images,2009,2009
 Adversarial machine learning at scale,2017, ArXiv
 Last iterate convergence inno-regret learning: constrained min-max optimization for convex-concave landscapes,2021, In AISTATS
 Robust multi-agentreinforcement learning via minimax deep deterministic policy gradient,2019, In AAAI
 On gradient descent ascent for nonconvex-concaveminimax problems,2020, In ICML
 Deep learning face attributes in the wild,2015, InProceedings of International Conference on Computer Vision (ICCV)
 Stochastic recursive gradient descent ascentfor stochastic nonconvex-strongly-concave minimax problems,2020, In H
 Optimistic mirror descent in saddle-point problems: Going the extragradient mile,2019, In 7th International Conference on Learning Representations
 The numerics of gans,2017, In Proceedings ofthe 31st International Conference on Neural Information Processing Systems
 Spectral normalization forgenerative adversarial networks,2018, ArXiv
 A unified analysis of extra-gradient andoptimistic gradient methods for saddle point problems: Proximal point approach,2020, In InternationalConference on Artificial Intelligence and Statistics
 A unified analysis of extra-gradient andoptimistic gradient methods for saddle point problems: Proximal point approach,2020, In Proceedingsof the Twenty Third International Conference on Artificial Intelligence and Statistics
 Solving aClass of Non-Convex Min-Max Games Using Iterative First Order Methods,2019, Curran AssociatesInc
 Ridge rider: Finding diversesolutions by following eigenvectors of the hessian,2020, In Advances in Neural Information ProcessingSystems
 A modification of the arrow-hurwicz method for search of saddle points,1980, Mathematicalnotes of the Academy of Sciences of the USSR
 Gmres: A generalized minimal residual algorithm for solvingnonsymmetric linear systems,1986, SIAM Journal on Scientific and Statistical Computing
 Iterative methods for sparse linear systems,2003, SIAM
 Improved techniques for training gans,2016, In Advances in Neural Information ProcessingSystems
 Competitive gradient descent,2019, In H
 Efficient algorithmsfor smooth minimax optimization,2019, In H
 Theory of Games and Economic Behavior,1944, PrincetonUniversity Press
 Anderson acceleration for fixed-point iterations,2011, SIAM Journal onNumericalAnalysis
 Anderson acceleration for fixed-point iterations,2011, 2011b
 On solving minimax optimization locally: Afollow-the-ridge approach,2020, In 8th International Conference on Learning Representations
 Linear last-iterate convergencein constrained saddle-point optimization,2021, In International Conference on Learning Representations
 Stochastic anderson mixing for nonconvex stochasticoptimization,2021, arXiv preprint arXiv:2110
 Improving gan training with probabilityratio clipping and sample reweighting,2020, ArXiv
 A stochastic extra-step quasi-newton method fornonsmooth nonconvex optimization,2019, arXiv: Optimization and Control
 The unusual effectiveness of averaging in GAN training,2019, In 7th InternationalConference on Learning Representations
 On the suboptimality of negative momentum for minimaxoptimization,2021, In AISTATS
 Theoretically principled trade-off between robustness and accuracy,2019, CoRR
 A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems,2020, In H
