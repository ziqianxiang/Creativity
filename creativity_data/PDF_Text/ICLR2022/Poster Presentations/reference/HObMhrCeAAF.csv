title,year,conference
 Tensorflow: A system for large-scalemachine learning,2016, In 12th {USENIX} symposium on operating systems design and implementation({OSDI} 16)
 Zero-costproxies for lightweight nas,2021, arXiv preprint arXiv:2101
 Zero-CostProxies for Lightweight NAS,2021, In International Conference on Learning Representations (ICLR)
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Random search for hyper-parameter optimization,2012, Journal ofmachine learning research
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, In International conference on machine learning
 Proxylessnas: Direct neural architecture search on target taskand hardware,2018, arXiv preprint arXiv:1812
 Once-for-all: Train one networkand specialize it for efficient deployment,2019, arXiv preprint arXiv:1908
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Fitting the search space ofweight-sharing nas with graph convolutional networks,2020, arXiv preprint arXiv:2004
 Chamnet: Towards efficient network design throughplatform-aware model adaptation,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Applied nonparametric statistics,1990, 1990
 Nas-bench-201: Extending the scope of reproducible neural architecturesearch,2020, arXiv preprint arXiv:2001
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In International Conference onMachine Learning
 Bohb: Robust and efficient hyperparameter opti-mization at scale,2018, In International Conference on Machine Learning
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Automl: A survey of the state-of-the-art,2021, Knowledge-Based Systems
 Searchingfor mobilenetv3,2019, In Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV)
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convolu-tional neural networks,1557, Communications oftheACM
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Hyperband:A novel bandit-based approach to hyperparameter optimization,2017, The Journal of Machine LearningResearch
 Zen-nas: A zero-shot nas for high-performance image recognition,2021, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 Progressive neural architecture search,2018, In Proceedingsof the European conference on computer vision (ECCV)
 Darts: Differentiable architecture search,2018, arXivpreprint arXiv:1806
 Semi-supervised neuralarchitecture search,2020, arXiv preprint arXiv:2002
 New insights and perspectives on the natural gradient method,2014, arXiv preprintarXiv:1412
 A generic graph-basedneural architecture encoding scheme for predictor-based nas,2020, In Computer Vision-ECCV 2020:16th European Conference
 Towards nngp-guidedneural architecture search,2020, arXiv preprint arXiv:2011
 Automatic differentiation inpytorch,2017, 2017
 Efficient neural architecture searchvia parameters sharing,2018, In International Conference on Machine Learning
 On network designspaces for visual recognition,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the aaai conference on artificial intelligence
 Learning representations byback-propagating errors,1986, nature
 Imagenet large scale visual recognitionchallenge,2015, International journal of computer vision
 Bounding and counting linearregions of deep neural networks,4558, In International Conference on Machine Learning
 Nas-bench-301 and the case for surrogate benchmarks for neural architecture search,2020, arXiv preprintarXiv:2008
 Learning relus via gradient descent,2017, arXiv preprint arXiv:1705
 EfficientNet: Rethinking model scaling for convolutional neuralnetworks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Blockswap: Fisher-guided block substitution for network compression on a budget,2019, arXiv preprint arXiv:1906
 Neural architecture search as program trans-formation exploration,2021, In Proceedings of the 26th ACM International Conference on ArchitecturalSupport for Programming Languages and Operating Systems
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Alphax: exploringneural architectures with deep neural networks and monte carlo tree search,2019, arXiv preprintarXiv:1903
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Machine learning
 Disentangling trainability and generaliza-tion in deep neural networks,1046, In International Conference on Machine Learning
 Nas-bench-101: Towards reproducible neural architecture search,2019, In International Conference onMachine Learning
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Econas: Finding proxies for economical neural architecture search,2020, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition
