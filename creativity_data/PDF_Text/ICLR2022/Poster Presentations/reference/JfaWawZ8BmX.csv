title,year,conference
 Understanding double descent requires a fine-grained bias-variance decomposition,2020, arXiv preprint arXiv:2011
 A random matrix perspective on mixtures ofnonlinearities for deep learning,2019, arXiv preprint arXiv:1912
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 High-dimensional asymptotics of prediction: Ridge regressionand classification,2018, The Annals of Statistics
 Spectra of large blockmatrices,2006, arXiv preprint cs/0610045
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Generalized cross-validation as a method forchoosing a good ridge parameter,1979, Technometrics
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Noncommutative convexity arisesfrom linear matrix inequalities,2006, Journal of Functional Analysis
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 The spectrum of kernel random matrices,2010, Annals of Statistics
 Deep neural networks as gaussian processes,2017, arXiv preprint arXiv:1711
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, Advances in neural information processing systems
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Bayesian deep convolutional networks with manychannels are gaussian processes,2018, arXiv preprint arXiv:1810
 A note on the pennington-worah distribution,2019, Electronic Communications in Probability
 The spectrum of the fisher information matrix of a single-hidden-layer neural network,2018, In NeurIPS
 Nonlinear random matrix theory for deep learning,2019, Journal ofStatistical Mechanics: Theory and Experiment
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Random features for large-scale kernel machines,2007, In NIPS
 Covariate shift in high-dimensional randomfeature regression,2021, arXiv preprint arXiv:2111
 Overparameterization improves robustnessto covariate shift in high dimensions,2021, Advances in Neural Information Processing Systems
 On the optimal weighted `2 regularization in overparameterized linearregression,2020, arXiv preprint arXiv:2006
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
