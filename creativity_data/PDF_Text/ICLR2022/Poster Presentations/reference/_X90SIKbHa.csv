title,year,conference
 Anderson acceleration and application to the three-temperature energy equations,2017, Journal OfComputational Physics
 Iterative procedures for nonlinear integral equations,1965, Journal of the ACM(JACM)
 Learning to learn by gradientdescent by gradient descent,2016, In Proceedings of the 30th International Conference on NeuralInformation Processing Systems
 Accelerating self-consistentfield theory of block polymers in a variable unit cell,2017, The Journal of Chemical Physics
 Deep equilibrium models,2019, Advances in NeuralInformation Processing Systems
 Multiscale deep equilibrium models,2020, In Advancesin Neural Information Processing Systems (NeurIPS)
 Limited-memory BFGS with displacementaggregation,2021, Mathematical Programming
 Optimization methods for large-scale machinelearning,2018, SIAM Review
 Shanks sequence transformations andAnderson acceleration,2018, SIAM Review
 A class of methods for solving nonlinear simultaneous equations,1965, Mathematicsof Computation
 A stochastic quasi-Newtonmethod for large-scale optimization,2016, SIAM Journal on Optimization
 A polynomial extrapolation method for finding limits and antilimitsof vector sequences,1976, SIAM Journal on Numerical Analysis
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 First-order methods for nonconvex quadratic minimization,2020, SIAMReview
 ImageNet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Deep learning on mobile devices: A review,2019, In Mobile Multimedia/Image Processing
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of Machine Learning Research
 Extrapolating to the limit of a vector sequence,1979, In Peter C
 Anderson accelerated Douglas-Rachford splitting,2020, SIAMJournal on Scientific Computing
 Comparison of self-consistent field convergence accel-eration techniques,2012, The Journal of Chemical Physics
 Stochastic first-and zeroth-order methods for nonconvex stochas-tic programming,2013, SIAM Journal on Optimization
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 A survey of nonlinear conjugate gradient methods,2006, PacificJournal of Optimization
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Numerical methods for nonlinear equations,2018, Acta Numerica
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 BFGS with update skipping and varyingmemory,1998, SIAM Journal on OPtimization
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Numerical methods for Kohn-Sham density functionaltheory,2019, Acta Numerica
 On the variance of the adaptive learning rate and beyond,2019, In International Conference onLearning RePresentations
 Adaptive gradient methods with dynamicbound of learning rate,2018, In International Conference on Learning RePresentations
 Anderson acceleration of proximal gradient methods,2020, In Interna-tional Conference on Machine Learning
 Building a large annotatedcorpus of English: The Penn Treebank,1993,1993
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning RePresentations
 IQN: An incremental quasi-Newton methodwith local superlinear convergence rate,2018, SIAM Journal on OPtimization
 Linear convergence of first order methods fornon-strongly convex optimization,2019, Mathematical Programming
 Problem complexity and methodefficiency in optimization,1983,1983
 Numerical oPtimization,2006, Springer Science & Business Media
 BLEU: A method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting of the Associationfor ComPutational Linguistics
 A characterization of the behavior of the Anderson accelerationon linear problems,2013, Linear Algebra and Its APPlications
 Overfitting in adversarially robust deep learning,2020, InInternational Conference on Machine Learning
 A stochastic approximation method,1951, The Annals of Mathemat-ical Statistics
 GMRES: A generalized minimal residual algorithm for solvingnonsymmetric linear systems,1986, SIAM Journal on Scientific and Statistical Computing
 Iterative methods for sparse linear systems,2003, SIAM
 Non-linear transformations of divergent and slowly convergent sequences,1955, Journalof Mathematics and Physics
 Lecture 6,2012,5-RMSprop: Divide the gradient by a runningaverage of its recent magnitude
 Convergence analysis for Anderson acceleration,2015, SIAM Journal onNumerical Analysis
 The superlinear convergence behaviour of GMRES,1993, Journal ofComputational and Applied Mathematics
 Attention is all you need,2017, In Advances in Neural Infor-mation Processing Systems
 Anderson acceleration for fixed-point iterations,2011, SIAM Journal onNumerical Analysis
 Stochastic quasi-Newton methods fornonconvex stochastic optimization,2017, SIAM Journal on Optimization
 Stochastic Anderson mixing for nonconvex stochasticoptimization,2021, Advances in Neural Information Processing Systems
 Acceleration techniques for iterated vector and matrix problems,1962, Mathematics ofComputation
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Anderson acceleration for seismic inversion,2021, Geophysics
 Wide residual netWorks,2016, In British Machine VisionConference 2016
 AdaBelief optimizer: Adapting stepsizes by the belief in observedgradients,2020, Advances in Neural Information Processing Systems
