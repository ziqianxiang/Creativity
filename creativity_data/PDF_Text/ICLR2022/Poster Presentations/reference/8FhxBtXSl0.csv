title,year,conference
 Unitary evolution recurrent neural networks,2016, InInternational Conference on Machine Learning
 Layer normalization,2016, arXiv preprintarXiv:1607
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling,2018, arXivpreprint arXiv:1810
 Mad max: Affine spline insights into deep learning,2018, arXivpreprint arXiv:1805
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Recurrent neuralnetworks for multivariate time series with missing values,2018, Scientific reports
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Very deep convolutional networksfor text classification,2016, arXiv preprint arXiv:1606
 Very deep convolutional neuralnetworks for raw waveforms,2017, In 2017 IEEE International Conference on Acoustics
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Language modeling with gatedconvolutional networks,2017, In International conference on machine learning
 Gru-ode-bayes: Continuousmodeling of sporadically-observed time series,2019, In Advances in Neural Information ProcessingSystems
 Deepsphere: agraph-based spherical cnn,2020, arXiv preprint arXiv:2012
 Generalizing convolutionalneural networks for equivariance to lie groups on arbitrary continuous data,2020, arXiv preprintarXiv:2002
 Speech recognition with deep recurrentneural networks,2013, In 2013 IEEE international conference on acoustics
 Hippo: Recurrent memory withoptimal polynomial projections,2020, arXiv preprint arXiv:2008
 Improving thegating mechanism of recurrent neural networks,2020, In International Conference on Machine Learning
 Complexity of linear regions in deep networks,2019, arXiv preprintarXiv:1901
 Untersuchungen zu dynamischen neuronalen netzen,1991, Diploma
 Long short-term memory,1997, Neural computation
 Lietransformer: Equivariant self-attention for lie groups,2020, arXiv preprint arXiv:2012
 Neural controlled differential equationsfor irregular time series,2020, arXiv preprint arXiv:2005
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009,2009
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Building a large annotated corpus of english: The penn treebank,1994, UsingLarge Corpora
 Mogrifier lstm,2019, arXiv preprint arXiv:1909
 Nerf: Representing scenes as neural radiance fields for view synthesis,2020, arXiv preprintarXiv:2003
 Rectified linear units improve restricted boltzmann machines,2010, InIcml
 Wavenet: A generative model for rawaudio,2016, arXiv preprint arXiv:1609
 Deepsdf:Learning continuous signed distance functions for shape representation,2019, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 How to construct deeprecurrent neural networks,2013, arXiv preprint arXiv:1312
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Searching for activation functions,2017, arXiv preprintarXiv:1710
 Stand-alone self-attention in vision models,2019, arXiv preprint arXiv:1906
 Group equivariant stand-alone self-attention forvision,2020, arXiv preprint arXiv:2010
 Wavelet networks:Scale equivariant learning from raw waveforms,2020, arXiv preprint arXiv:2006
 Learning internal representations byerror propagation,1985, Technical report
 Weight normalization: A simple reparameterization toaccelerate training of deep neural networks,2016, arXiv preprint arXiv:1602
 Graf: Generative radiance fieldsfor 3d-aware image synthesis,2020, arXiv preprint arXiv:2007
 Bounding and counting linearregions of deep neural networks,4558, In International Conference on Machine Learning
 From points to parts:3d object detection from point cloud with part-aware and part-aggregation network,2019, arXiv preprintarXiv:1907
 Dynamic edge-conditioned filters in convolutional neuralnetworks on graphs,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Im-plicit neural representations with periodic activation functions,2020, Advances in Neural InformationProcessing Systems
 Fourier features let networks learnhigh frequency functions in low dimensional domains,2020, arXiv preprint arXiv:2006
 Learning longer-term dependen-cies in rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Deep parametriccontinuous convolutional neural networks,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Speech commands: A dataset for limited-vocabulary speech recognition,2018, arXiv preprintarXiv:1804
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE
 Empirical evaluation of rectified activations inconvolutional network,2015, arXiv preprint arXiv:1505
 5 are obtained with shallow CKCNNs composed of 2 residualblocks only,2022, An interesting question is whether going deeper can be used to improve the performance20Published as a conference paper at ICLR 2022RandomFigure 7: Function approximation via ReLU
 8 and vary only in the number of channels,2020, We use layernormalization (Ba et al
 CKCNNs are very susceptible to the value of ω0 ,2022, In order to obtain a reasonable ω0
