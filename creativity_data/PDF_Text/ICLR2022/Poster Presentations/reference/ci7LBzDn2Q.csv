title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning (ICML)
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning (ICML)
 Spectrally-normalized margin bounds for neuralnetworks,2017, Preprint arXiv:1706
 On the complexity of neural network classifiers: A com-parison between shallow and deep architectures,2014, IEEE Transactions on Neural Networks andLearning Systems
 On the expressive power of deep learning: A tensoranalysis,2016, In Conference on Learning Theory (COLT)
 Depth separation for neural networks,2017, In Conference on Learning Theory (COLT)
 The power of depth for feedforward neural networks,2016, In Conferenceon Learning Theory (COLT)
 Products of many large random matrices and gradients in deep neuralnetworks,2019, Communications in Mathematical Physics
 Finite depth and width corrections to the neural tangent kernel,2020, InInternational Conference on Learning Representations (ICLR)
 Complexity of linear regions in deep networks,2019, In InternationalConference on Machine Learning (ICML)
 Deep ReLU networks have surprisingly few activation patterns,2020, InConference on Neural Information Processing Systems (NeurIPS)
 Delving deep into rectifiers: Surpass-ing human-level performance on ImageNet classification,2015, In IEEE International Conference onComputer Vision (ICCV)
 Fantasticgeneralization measures and where to find them,2019, Preprint arXiv:1912
 On the number of lin-ear regions of deep neural networks,2014, In Conference on Neural Information Processing Systems(NeurIPS)
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Conference on NeuralInformation Processing Systems (NeurIPS)
 Trajectory growth lower bounds for random sparse deep ReLU net-works,2019, Preprint arXiv:1911
 On theexpressive power of deep neural networks,2017, In International Conference on Machine Learning(ICML)
 Representation benefits of deep feedforward networks,2015, PreprintarXiv:1509
 Benefits of depth in neural networks,2016, In Conference on Learning Theory (COLT)
