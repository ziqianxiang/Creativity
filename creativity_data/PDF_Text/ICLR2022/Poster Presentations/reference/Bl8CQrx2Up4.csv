title,year,conference
 Adaptive input representations for neural language modeling,2018, InInternational Conference on Learning Representations
 wav2vec 2,2020,0: A frame-work for self-supervised learning of speech representations
 Jax: composable transformations of python+ numpyprograms,2018, Version 0
 Language models are few-shot learners,2020, arXiv
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 What does bert lookat? an analysis of bert¡¯s attention,2019, In Proceedings of the 2019 ACL Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Smyrf - efficient atten-tion using asymmetric clustering,2020, In H
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 An im-age is worth 16x16 words: Transformers for image recognition at scale,2020, In International Confer-ence on Learning Representations
 On the properties of the softmax function with application in gametheory and reinforcement learning,2017, arXiv preprint arXiv:1704
 Long short-term memory,1997, Neural computation
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In International Conference on Ma-chine Learning
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Revealing the dark secretsof bert,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Learning multiple layers of features from tiny images,2009, Master¡¯sthesis
 Settransformer: A framework for attention-based permutation-invariant neural networks,2019, In ICML
 Learninglong-range spatial dependencies with horizontal gated recurrent units,2018, In Proceedings of the 32ndInternational Conference on Neural Information Processing Systems
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Pointer sentinel mixturemodels,2017, 5th International Conference on Learning Representations
 Listops: A diagnostic dataset for latent tree learning,2018, InProceedings of the 2018 Conference of the North American Chapter of the Association for Com-putational Linguistics: Student Research Workshop
 Justifying recommendations using distantly-labeledreviews and fine-grained aspects,2019, In Proceedings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP)
 The aclanthology network corpus,2013, Language Resources and Evaluation
 Random features for large-scale kernel machines,2008, In J
 Efficient content-basedsparse attention with routing transformers,2020, In TACL
 wav2vec: Unsupervisedpre-training for speech recognition,2019, In INTERSPEECH
 Roformer: Enhanced transformerwith rotary position embedding,2021, In arXiv
 Sparse sinkhorn attention,2020, InInternational Conference on Machine Learning
 Long range arena: A benchmark for efficienttransformers,2020, In International Conference on Learning Representations
 Synthesizer: Re-thinking self-attention for transformer models,2021, In International Conference on Machine Learning
 Transformer dissection: An unified understanding for transformer¡¯s attention viathe lens of kernel,2019, In EMNLP
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Fast transformers with clustered attention,2020, In NeurIPS
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Nystromformer: A nystrom-based algorithm for approximating self-attention,2021, InAAAI
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
