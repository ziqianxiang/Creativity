title,year,conference
 Task-free continual learning,2019, In IEEE/CVFConference on ComPUter ViSion and Pattern Recognition (CVPR)
 Spectral analysis of the neuraltangent kernel for deep residual networks,2021, arXiv PrePrint arXiv:2104
 Generalisation guarantees for continuallearning with orthogonal gradient descent,2020, arXiv PrePrint arXiv:2006
 Spectrum dependent learning curvesin kernel regression and wide neural networks,2020, In International Conference on Machine Learning(ICML)
 Spectral bias and task-model align-ment explain generalization in kernel regression and infinitely wide neural networks,2021, NatUeecommunications
 Deep neural tangent kernel and Laplace kernel have the same RKHS,2020, InInternational Conference on Learning RePreSentationS (ICLR)
 Statistical mechanics of support vectornetworks,1999, PhySical review letters
 Locality defeats the curse of dimen-Sionality in convolutional teacher-student scenarios,2021, arXiv PrePrint arXiv:2106
 Onthe similarity between the Laplace and neural tangent kernels,2020, In AdVanceS in neural informationProceSSing SyStemS (NeUrIPS)
 Embracing change: Continuallearning in deep neural networks,2020, TreedS in cognitive sciences
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In AdVanceS in neural information ProceSSing SyStemS (NeUrIPS)
 Overcomingcatastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Wide neural networks of any depth evolve as linear models under gradient descent,2019, InAdvanCeS in neural information processing SyStemS (NeUrIPS)
 Continual learning in the teacher-student setup:Impact of task similarity,2021, In International COnferenCe on MaChine Learning (ICML)
 Gradient episodic memory for continual learning,2017, InAdvanCeS in Neural InfOrmatiOn PrOCeSSing SyStemS (NIPS)
 Self-distillation amplifies regularization inHilbert space,2020, In AdvanCeS in NeUral InfOrmatiOn Processing SyStemS (NeUrIPS)
 Deepdouble descent: Where bigger models and more data hurt,2020, In InternatiOnal COnferenCe on LearningRePreSentatiOnS (ICLR)
 A PAC-Bayesian bound for lifelong learning,2014, InInternatiOnal COnferenCe on MaChine Learning (ICML)
 Anatomy of catastrophic forgetting: Hid-den representations and task semantics,2020, In InternatiOnal COnferenCe on Learning RePreSentatiOnS(ICLR)
 Asymptotic learning curves of kernel methods:empirical data versus teacher-student paradigm,2020, JOUrnaI of StatiStiCaI Mechanics: TheOry andExperiment
 On the theory of transfer learning: The importance oftask diversity,2020, In AdvanCeS in NeUral InfOrmatiOn PrOCeSSing SyStemS (NeUrIPS)
 Tensor programs IIb: Architectural universality of neural tangent kerneltraining dynamics,1176, In InternatiOnal COnferenCe on MaChine Learning (ICML)
 Understand-ing deep learning requires rethinking generalization,2017, In International COnferenCe on LearningRePreSentatiOnS (ICLR)
