title,year,conference
 PonderNet: Learning to Ponder,2021, PreprintarXiv:2107
 Reinforcementlearning of imPlicit and exPlicit control flow instructions,2021, In Proc
 Language models are few-shot learners,2020, In Proc
 Can transformers jump around right innatural language? Assessing performance transfer from SCAN,2021, Preprint arXiv:2107
 Compositional generaliza-tion via neural-symbolic stack machines,2020, In Proc
 Modeling hierarchical structures with continuousrecursive neural networks,2021, In Proc
 The devil is in the detail: Simple tricksimprove systematic generalization of Transformers,2021, In Proc
 In Proc,2019, Associationfor Computational Linguistics (ACL)
 Language modeling with gatedconvolutional networks,2017, In Proc
 CNNs found to jump around more skillfully than RNNs: Composi-tional generalization in seq2seq convolutional networks,2019, In Proc
 Location attention for extrapolationto longer sequences,2020, In Proc
 Connectionism and the problem of systematicity: WhySmolensky¡¯s solution doesn¡¯t work,1990, Cognition
 Connectionism and cognitive architecture: A criticalanalysis,1988, Cognition
 Compositional generalization insemantic parsing: Pre-training vs,2020, specialized architectures
 Adaptive computation time for recurrent neural networks,2016, In Int
 On the binding problem in artificialneural networks,2020, Preprint arXiv:2012
 Cooperative learning of disjoint syntaxand semantics,2019, In Proc
 Deep residual learning for imagerecognition,2016, In Proc
 Learning com-positionally through attentive guidance,2019, In Proc
 Advancing Neural Language Modeling in Automatic Speech Recognition,2020, PhD thesis
 Language modeling with deep Trans-formers,2019, In Proc
 Going beyond linear Trans-formers with recurrent fast weight programmers,2021, Preprint arXiv:2106
 Measuring compositional generalization: A comprehensivemethod on realistic data,2020, In Int
 Character-aware neural languagemodels,2016, In Proc
 Transcoding compositionally: Usingattention to find more generalizable solutions,2019, In Proc
 Compositional generalization through meta sequence-to-sequence learning,2019, InProc
 Generalization without systematicity: On the compositionalskills of sequence-to-sequence recurrent networks,2018, In Proc
 Compositional generalization forprimitive substitutions,2019, In Proc
 Memorize or generalize? Searching for acompositional RNN in a haystack,2018, In AEGAP Workshop ICML
 Decoupled weight decay regularization,2019, In Int
 Making Transformers solvecompositional tasks,2021, Preprint arXiv:2108
 Compositional generalization in adeep seq2seq model by separating syntax and semantics,2019, Preprint arXiv:1904
 Self-delimiting neural networks,2012, Technical Report IDSIA-08-12
 Bidirectional recurrent neural networks,1997, IEEE Transactions onSignal Processing
 GLU variants improve transformer,2020, Preprint arXiv:2002
 Ordered memory,2019, In Proc
 Long Range Arena : A benchmark for efficienttransformers,2021, In Int
 Attention is all yoU need,2017, In Proc
 Thinking like Transformers,2021, In Proc
 Nystromformer: A Nystrom-based algorithm for approximating self-attention,2021, InProc
