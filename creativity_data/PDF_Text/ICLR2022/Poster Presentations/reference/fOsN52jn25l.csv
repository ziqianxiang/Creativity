title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In ICML
 Deep rewiring: Trainingvery sparse deep networks,2017, arXiv preprint arXiv:1711
 A survey of model compression and accelerationfor deep neural networks,2017, arXiv preprint arXiv:1710
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Model compression and hardwareacceleration for neural networks: A comprehensive survey,2020, Proceedings of the IEEE
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Rigging the lottery:Making all tickets winners,2020, In ICML
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Learning both weights and connections forefficient neural networks,2015, arXiv preprint arXiv:1506
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, Morgan Kaufmann
 Deep residual learning for image recog-nition,2016, In CVPR
 Channel pruning for accelerating very deep neural net-works,2017, In ICCV
 Optimal brain damage,1990, In NeurIPS
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 A signal propagationperspective for pruning neural networks at initialization,2019, arXiv preprint arXiv:1906
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Learn-ing efficient convolutional networks through network slimming,2017, In ICCV
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 Parameter efficient training of deep convolutional neural networksby dynamic sparse reparameterization,2019, In ICML
 Woodfisher: Efficient second-order approximations for modelcompression,2020, arXiv preprint arXiv:2004
 Generalized dropout,2016, arXiv preprint arXiv:1611
 Eigendamage: Structured pruningin the kronecker-factored eigenbasis,2019, In ICML
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Structured pruning for efficientconvnets via incremental regularization,2019, In IJCNN
 Neural pruning via growing regularization,2020, arXivpreprint arXiv:2012
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, arXiv preprint arXiv:1802
 Drawing early-bird tickets: Toward more efficienttraining of deep networks,2020, In ICLR
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Fast convergence of natural gradient descentfor overparameterized neural networks,2019, arXiv preprint arXiv:1905
