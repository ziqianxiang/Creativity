title,year,conference
 Generating long sequences with sparsetransformers,2019, CoRR
 Masked language modeling forproteins via linearly scalable long-context transformers,2020, CoRR
 Rethinking attention withperformers,2021, In 9th International Conference on Learning Representations
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Ernie-doc: A retrospective long-document modeling transformer,2021, In Chengqing Zong
 Coordinationamong neural modules through a shared global workspace,2021, CoRR
 Beyond self-attention: Externalattention using two linear layers for visual tasks,2021, CoRR
 Long document classification from localword glimpses via recurrent attention learning,2019, IEEE Access
 Pretraining with contrastive sentenceobjectives improves discourse performance of language models,2020, In Dan Jurafsky
 Transformersare rnns: Fast autoregressive transformers with linear attention,2020, In Proceedings of the 37thInternational Conference on Machine Learning
 Reformer: The efficient transformer,2020, In8th International Conference on Learning Representations
 ALBERT: A lite BERT for self-supervised learning of language representations,2020, In8th International Conference on Learning Representations
 SLM: learning adiscourse language representation with sentence unshuffling,1551, In Bonnie Webber
 FNet: mixing tokens withfourier transforms,2021, CoRR
 RoBERTa: A robustly optimized BERTpretraining approach,2019, CoRR
 Luna: Linear unified nested attention,2021, In Advances in Neural Information ProcessingSystems
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 Random feature attention,2021, In 9th International Conference on Learning Representations
 Long Range Arena : A benchmark for efficienttransformers,2021, In 9th International Conference on Learning Representations
 MLP-Mixer: an all-mlp architecture for vision,2021, CoRR
 Attention is all you need,2017, In Isabelle Guyon
 Linformer: Self-attentionwith linear complexity,2020, CoRR
 Fastformer: Additive attentioncan be all you need,2021, CoRR
 Nystromformer: A nystrÎ¸m-based algorithm for approximating self-attention,2021, InThirty-Fifth AAAI Conference on Artificial Intelligence
 Big Bird:transformers for longer sequences,2020, In Hugo Larochelle
 An attention free transformer,2021, CoRR
 Poolingformer: Long document modeling with pooling attention,2021, In Marina Meila andTong Zhang (eds
 Character-level convolutional networks fortext classification,2015, In Corinna Cortes
 Adaptive multi-resolution attention withlinear complexity,2021, CoRR
 H-Transformer-1D: fast one-dimensional hierarchical attention forsequences,2021, In Chengqing Zong
