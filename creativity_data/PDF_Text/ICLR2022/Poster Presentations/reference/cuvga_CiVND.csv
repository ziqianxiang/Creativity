title,year,conference
 The second PASCALrecognising textual entailment challenge,2006, In Proceedings of the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment
 The generalization-stabilitytradeoff in neural network pruning,2019, arXiv preprint arXiv:1906
 The fifthpascal recognizing textual entailment challenge,2009, In In Proc Text Analysis Conference (TAC¡¯09)
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 The pascal recognising textual entailmentchallenge,2006, In Proceedings of the First International Conference on Machine Learning Chal-lenges: Evaluating Predictive Uncertainty Visual Object Classification
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Global sparsemomentum sgd for pruning very deep neural networks,2019, arXiv preprint arXiv:1909
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of machine learning research
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Sharpness-aware minimizationfor efficiently improving generalization,2020, arXiv preprint arXiv:2010
 The minimum description length principle,2007,2007
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Neural networks for machine learning lecture6a overview of mini-batch gradient descent,2012, Cited on
 Dynabert: Dynamic bertwith adaptive width and depth,2020, arXiv preprint arXiv:2004
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principled regu-larized optimization,2019, arXiv preprint arXiv:1911
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009,2009
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 Super tickets in pre-trained language models: From model compression toimproving generalization,2021, arXiv preprint arXiv:2105
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 The microsoft toolkit of multi-task deep neuralnetworks for natural language understanding,2020, In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics: System Demonstrations
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Fixing weight decay regularization in adam,2018,2018
 Pruning convolutionalneural networks for resource efficient inference,2016, arXiv preprint arXiv:1611
 Importance estimationfor neural network pruning,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 One ticket to win them all: general-izing lottery ticket initializations across datasets and optimizers,2019, arXiv preprint arXiv:1906
 Occam¡¯s razor,2001, Advances in neural informationprocessing systems
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Movement pruning: Adaptive sparsity byfine-tuning,2020, arXiv preprint arXiv:2005
 Edinburgh neural machine translation systemsfor wmt 16,2016, arXiv preprint arXiv:1606
 Faster gaze prediction withdense networks and fisher pruning,2018, arXiv preprint arXiv:1801
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Structured pruning of large language models,2019, arXivpreprint arXiv:1910
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-gies
 Autoprune: Automatic network pruning byregularizing auxiliary parameters,2019, Advances in neural information processing systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
