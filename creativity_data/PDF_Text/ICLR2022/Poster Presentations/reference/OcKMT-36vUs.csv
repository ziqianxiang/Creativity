title,year,conference
 Convex optimization,2004, Cambridge university press
 Characterizing signal propagation to close the performance gap inunnormalized resnets,2021, arXiv preprint arXiv:2101
 One billion word benchmark formeasuring progress in statistical language modeling,2013, INTERSPEECH
 On empirical comparisons ofoptimizers for deep learning,2019, arXiv preprint arXiv:1910
 Gradient descent on neural networkstypically occurs at the edge of stability,2021, arXiv preprint arXiv:2103
 Metainit: Initializing learning by learning to initialize,2019, Advancesin Neural Information Processing Systems
 An investigation into neural net optimization via hessianeigenvalue density,2019, In International Conference on Machine Learning
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residual networks,2016, In Europeanconference on computer vision
 Matrix analysis,2012, Cambridge university press
 Densely connected convolutionalnetworks,2017, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International conference on machine learning
 Three factorsinfluencing minima in sgd,2017, arXiv preprint arXiv:1711
 The break-even pointon optimization trajectories of deep neural networks,2020, arXiv preprint arXiv:2002
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Learning multiple layers of features from tiny images,2009, Technical report
 The large learning rate phaseof deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 The full spectrum of deepnet hessians at scale: Dynamics with sgd training and samplesize,2018, arXiv preprint arXiv:1811
 On the difficulty of training recurrent neural networks,2013, InInternational conference on machine learning
 Fast exact multiplication by the hessian,1994, Neural computation
 Imagenet large scale visual recognition challenge,2015, International journal ofcomputer vision
 Measuring theeffects of data parallelism on neural network training,2018, arXiv preprint arXiv:1811
 Optimizer benchmarking needs toaccount for hyperparameter tuning,2020, In H
 A mean field theory ofbatch normalization,2019, arXiv preprint arXiv:1902
 Wide residual networks,2016, arXiv preprint arXiv:1605
 Whichalgorithmic choices matter at which batch sizes? insights from a noisy quadratic model,2019, Advancesin neural information processing systems
 Gradinit: Learning to initialize neuralnetworks for stable and efficient training,2021, arXiv preprint arXiv:2102
