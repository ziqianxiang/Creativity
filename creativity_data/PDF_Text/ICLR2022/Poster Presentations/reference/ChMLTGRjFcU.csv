title,year,conference
 Living on the edge: Phasetransitions in convex programs with random data,2014, Information and Inference: A Journal of theIMA
 Statistical mechanics of deep learning,2020, Annual Review of Condensed MatterPhysics
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in Neural Information Processing Systems
 The goldilocks zone: Towards better understanding of neuralnetwork loss landscapes,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 On milman¡¯s inequality and random subspaces which escape through a mesh inRn,1988, In Geometric aspects of functional analysis
 Gradient descent happens in a tiny subspace,2018, arXivpreprint arXiv:1812
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Mnist handwritten digit database,2010,2010
 SNIP: Single-shot network pruningbased on connection sensitivity,2018, October 2018
 Readingdigits in natural images with unsupervised feature learning,2011,2011
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Picking winning tickets before training bypreserving gradient flow,2020, February 2020
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
