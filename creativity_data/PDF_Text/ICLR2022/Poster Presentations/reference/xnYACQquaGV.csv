title,year,conference
 Improved algorithms for linear stochasticbandits,2011, In Advances in Neural Information Processing Systems
 Tamingthe monster: A fast and simple algorithm for contextual bandits,2014, In International Conference onMachine Learning
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Exploration-exploitation tradeoff usingvariance estimates in multi-armed bandits,2009, Theoretical Computer Science
 Finite-time analysis of the multiarmed banditproblem,2002, Machine learning
 Statistical guarantees for the emalgorithm: From population to sample-based analysis,2017, The Annals of Statistics
 Neural temporal-difference learning con-verges to global optima,2019, In Advances in Neural Information Processing Systems
 A generalization theory of gradient descent for learning over-parameterized deep relu networks,2019, arXiv preprint arXiv:1902
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In Advances in Neural Information Processing Systems
 An empirical evaluation of thompson sampling,2011, In Advances inneural information processing systems
 On kernelized multi-armed bandits,2017, In InternationalConference on Machine Learning
 Contextual bandits with linear payoff func-tions,2011, In Proceedings of the Fourteenth International Conference on Artificial Intelligence andStatistics
 Deep contextual multi-armed bandits,2018, arXiv preprintarXiv:1807
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 Stochastic linear optimization under banditfeedback,2008, In Conference on Learning Theory
 Self-supervised contextual bandits in computer vision,2020, arXiv preprintarXiv:2003
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Parametric bandits: Thegeneralized linear case,2010, In Advances in Neural Information Processing Systems
 Deep learning,2016, MIT press
 RedUcing the dimensionality of data with neUralnetworks,2006, science
 Neural tangent kernel: Convergence and gen-eralization in neUral networks,2018, In Advances in neural information processing systems
 Randomized exploration in generalized linear bandits,2020, In International Confer-ence on Artificial Intelligence and Statistics
 The epoch-greedy algorithm for multi-armed bandits with sideinformation,2008, In Advances in neural information processing systems
 Bandit algorithms,2020, Cambridge University Press
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Deep learning,2015, nature
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in neural information processing systems
 A contextual-bandit approach topersonalized news article recommendation,2010, In Proceedings of the 19th international conferenceon World wide web
 Provably optimal algorithms for generalized linear contex-tual bandits,2017, In International Conference on Machine Learning
 Towards moderate overparameterization: global con-vergence guarantees for training shallow neural networks,2020, IEEE Journal on Selected Areas inInformation Theory
 Deep bayesian bandits showdown: An em-pirical comparison of bayesian deep networks for thompson sampling,2018, In International Confer-ence on Learning Representations
 Linearly parameterized bandits,2010, Mathematics ofOperations Research
 A tutorial onthompson sampling,2018, Foundations and TrendsR in Machine Learning
 One-armed bandit problems with covariates,1991, The Annals of Statistics
 Scalable bayesian optimization using deepneural networks,2015, In International conference on machine learning
 On the likelihood that one unknown probability exceeds another in view ofthe evidence of two samples,1933, Biometrika
 Finite-time anal-ysis of kernelised contextual bandits,2013, In Proceedings of the Twenty-Ninth Conference on Uncer-tainty in Artificial Intelligence
 Neural policy gradient methods: Globaloptimality and rates of convergence,2020, In International Conference on Learning Representations
 Optimal computational and statistical rates of conver-gence for sparse nonconvex learning problems,2014, Annals of statistics
 A finite-time analysis of q-learning with neural network function ap-proximation,2020, In International Conference on Machine Learning
 Deep neural linear bandits: Overcoming catastrophic forgettingthrough likelihood matching,2019, arXiv preprint arXiv:1901
 Understandingdeep learning requires rethinking generalization,2017, In International Conference on Learning Rep-resentations
 Neural thompson sampling,2020, arXivpreprint arXiv:2010
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In Advances in Neural Information Processing Systems
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
