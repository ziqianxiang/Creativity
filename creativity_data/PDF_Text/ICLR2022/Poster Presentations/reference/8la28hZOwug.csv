title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Near-linear time approximation algorithmsfor optimal transport via sinkhorn iteration,2017, arXiv preprint arXiv:1705
 Self-labelling via simultaneousclustering and representation learning,2019, arXiv preprint arXiv:1911
 Learning representations by maximizingmutual information across views,2019, arXiv preprint arXiv:1906
 Vicreg: Variance-invariance-covariance regularizationfor self-supervised learning,2021, arXiv preprint arXiv:2105
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Bigself-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Exploring simple siamese representation learning,2021, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, Advances in neuralinformation processing systems
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Seed:Self-supervised distillation for visual representation,2021, arXiv preprint arXiv:2101
 Bootstrap your own latent: A new approach to self-supervised learning,2020, arXiv preprintarXiv:2006
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the thirteenth international conference onartificial intelligence and statistics
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Paraphrasing complex network: Network compressionvia factor transfer,2018, arXiv preprint arXiv:1802
 Learning multiple layers of features from tiny images,2009,2009
 Prototypical contrastive learning ofunsupervised representations,2020, arXiv preprint arXiv:2005
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Boosting self-supervisedlearning via knowledge transfer,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Relational knowledge distillation,2019, In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Correlation congruence for knowledge distillation,2019, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 On variationalbounds of mutual information,5171, In International Conference on Machine Learning
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 S2-bnn:Bridging the gap between self-supervised real and 1-bit neural networks via guided distribution cali-bration,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Multi-label contrastive predictive coding,2020, arXiv preprintarXiv:2007
 Deepcluster: A general clustering framework based ondeep learning,2017, In Joint European Conference on Machine Learning and Knowledge Discovery inDatabases
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Contrastive multiview coding,2020, In ComputerVision-ECCV 2020: 16th European Conference
 On mutualinformation maximization for representation learning,2019, arXiv preprint arXiv:1907
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Wide residual networks,2016, arXiv preprint arXiv:1605
 BarloW twins: Self-supervisedlearning via redundancy reduction,2021, arXiv preprint arXiv:2103
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Local aggregation for unsupervised learningof visual embeddings,2019, In Proceedings of the IEEE/CVF International Conference on ComputerVision
