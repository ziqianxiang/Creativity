title,year,conference
 Generating natural language adversarial examples,2018, In Proceedings of the 2018Conference on EmpiricaI Methods in NatUraI LangUage Processing
 Synthetic and natural noise both break neural machine trans-lation,2018, In International Conference on Learning RePreSentations
 Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models,2021, arXiv e-prints
 Knowledgeable or educated guess? revisiting language models as knowledge bases,2021, InProceedingS of the 59th AnnUal Meeting of the ASSociation for ComPUtational LingUiSticS and the11th Intemational Joint Conference on NatUral LangUage ProceSSing (Volume 1: Long Papers)
 Commonsense knowledge mining frompretrained models,2019, In PrOceedingS of the 2019 COnference on EmPirical MethOdS in NatUralLanguage Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv PrePrint arXiv:1810
 Training verified learners with learned ver-ifiers,2018, arXiv Preprint arXiv:1805
 Measuring and improving consistency in pretrained language mod-els,2021, arXiv preprint arXiv:2102
 T-REx: A large scale alignment of natural language with knowledgebase triples,2018, In ProceedingS of the EleVenth International Conference on LangUage Resources andEVaIUation (LREC 2018)
 BERTese: Learning to speak to BERT,2021, InProCeedingS of the 16th Conference of the EUropean Chapter of the ASSoCiation for CompUtationalLingUiStics: Main Volume
 Parameter-efficient transfer learningfor NLP,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Adversarial example genera-tion with syntactically controlled paraphrase networks,2018, In ProCeedingS of the 2018 ConferenCeof the North AmeriCan Chapter of the ASSoCiation for CompUtational LingUiStics: HUmanLangUage Technologies
 Robust encodings: a framework forcombating adversarial typos,2020, TranSaCtionS of the ASSoCiation for Computational LingUiStics
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Swords: A benchmarkfor lexical substitution with improved data coverage and quality,2021, In Proceedings of the 2021ConferenCe of the North AmeriCan Chapter of the ASSoCiation for CompUtational LingUiStics:HUman LangUage Technologies
 The power of scale for parameter-efficient prompttuning,2021, ArXiv
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of the 59th AnnUal Meeting of the Association for CompUtationaI LingUistics and the11th IntemationaI Joint Conference on NatUral LangUage Processing (VoIUme 1: Long Papers)
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv PrePrint arXiv:1907
 Decoupled weight decay regularization,2018, In InternationalConference on Learning RePresentations
 Nlp augmentation,2019, https://github
 Rethinking search: making domain expertsout of dilettantes,2021, In ACM SIGIR Forum
 E-BERT: Efficient-yet-effective entity em-beddings for BERT,2020, In Findings of the Association for Computational Linguistics: EMNLP2020
 Certifying some distributionalrobustness with principled adversarial training,2017, arXiv PrePrint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv PrePrint arXiv:1312
 A simple method for commonsense reasoning,2018, arXiv PrePrintarXiv:1806
 Transformers: State-of-the-artnatural language processing,2020, In ProCeedings of the 2020 Conference on EmpiriCal Methods inNatUraI Language Processing: System Demonstrations
