title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Deep rewiring: Trainingvery sparse deep networks,2018, In International Conference on Learning Representations
 Dota 2 with large scaledeep reinforcement learning,2019, arXiv preprint arXiv:1912
 Language models are few-shot learners,2020, In H
 The lottery ticket hypothesis for pre-trained bert networks,2020, Advances in neuralinformation processing systems
 Chasing sparsityin vision transformers: An end-to-end exploration,2021, Advances in Neural Information ProcessingSystems
 The lottery tickets hypothesis for supervised and self-supervised pre-training incomputer vision models,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Big self-supervised models are strong semi-supervised learners,2020, In H
 Earlybert:Efficient bert training via early-bird lottery tickets,2021, In Proceedings of the 59th Annual Meetingof the Association for Computational Linguistics and the 11th International Joint Conference onNatural Language Processing (Volume 1: Long Papers)
 Compressing neural networks using the variationalinformation bottleneck,2018, In International Conference on Machine Learning
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Sparse networks from scratch: Faster training withoUt losingperformance,2019, arXiv preprint arXiv:1907
 On random graphs i,1959, Publicationes Mathematicae (Debrecen)
 Rigging the lottery:Making all tickets winners,2943, In International Conference on Machine Learning
 Gradient flow in sparse neUral networksand how lottery tickets win,2020, arXiv preprint arXiv:2010
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, Advances in Neural Information Processing Systems
 Linear modeconnectivity and the lottery ticket hypothesis,2020, In International Conference on Machine Learning
 The state of sparsity in deep neUral networks,2019, arXivpreprint arXiv:1902
 Explaining and harnessing adversarialexamples,2014, ICLR
 Modelcompression with adversarial robustness: A unified optimization framework,2019, Advances in NeuralInformation Processing Systems
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Sparse dnns with improved adver-sarial robustness,2018, In S
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, Morgan Kaufmann
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem,2019, InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Benchmarking neural network robustness to commoncorruptions and perturbations,2019, International Conference on Learning Representations
 The many faces of robustness: A criticalanalysis of out-of-distribution generalization,2021, arXiv preprint arXiv:2006
 Pruning versus clipping in neural networks,1989, Physical Review A
 Top-kast: Top-kalways sparse training,2020, Advances in Neural Information Processing Systems
 Highly accurateprotein structure prediction with alphafold,2021, Nature
 Learning mUltiple layers of featUres from tiny images,2009, 2009
 Soft threshold weight reparameterization for learnable sparsity,2020, In InternationalConference on Machine Learning
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2016, arXiv preprint arXiv:1612
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Layer-adaptive sparsity forthe magnitude-based pruning,2021, In International Conference on Learning Representations
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International Conference on LearningRepresentations
 Sparse evolutionary deep learning with over one million artificial neurons oncommodity hardware,2020, Neural Computing and Applications
 Topological insights into sparse neural networks,2020, In Joint EuropeanConference on Machine Learning and Knowledge Discovery in Databases
 Sparse training via boostingpruning plasticity with neuroregeneration,2021, Advances in Neural Information Processing Systems(NeurIPs)
 Do we actually needdense over-parameterization? in-time over-parameterization in sparse training,2021, In Proceedings ofthe 39th International Conference on Machine Learning
 Learning sparse neural networks throughl_0 regularization,2018, International Conference on Learning Representations
 Diversity networks: Neural network compression using determinantalpoint processes,2016, In International Conference on Learning Representations
 Towards neural networks that provably know when theydonâ€™t know,2020, In International Conference on Learning Representations
 A topological insight into restricted boltzmann machines,1573, Machine Learning
 Scalable training of artificial neural networks with adaptive sparse connectivityinspired by network science,2018, Nature communications
 Pruning convolutionalneural networks for resource efficient inference,2016, International Conference on Learning Represen-tations
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning 2011
 The role ofover-parametrization in generalization of neural networks,2019, In International Conference on LearningRepresentations
 The limitations of deep learning in adversarial settings,2016, In 2016 IEEE European symposiumon security and privacy (EuroS&P)
 Deep expander networks: Efficient deepnetworks from graph theory,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Exploring the limits of transfer learning with a unifiedtext-to-text transformer,2020, Journal of Machine Learning Research
 Comparing rewinding and fine-tuning inneural network pruning,2020, In International Conference on Learning Representations
 Movement pruning: Adaptive sparsity byfine-tuning,2020, In H
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Advances in NeuralInformation Processing Systems,2020, arXiv:2009
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, Advances in Neural Information Process-ing Systems
 Keep the gradients flowing: Using gradientflow to study sparse network optimization,2021, arXiv preprint arXiv:2102
 Ro-bustness may be at odds with accuracy,2019, In International Conference on Learning Representations
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Intriguing properties of adversarial training at scale,2020, In InternationalConference on Learning Representations
 Adversarial robustness vs,2019, model compression
 Drawing early-bird tickets: Towards more efficient trainingof deep networks,2020, In International Conference on Learning Representations 2020 (ICLR 2020)
 Wide residual networks,2016, In Edwin R
 Generating natural adversarial examples,2018, InInternational Conference on Learning Representations
