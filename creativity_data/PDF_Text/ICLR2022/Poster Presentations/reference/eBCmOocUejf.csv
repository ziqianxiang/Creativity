title,year,conference
 Quantifying Attention Flow in Transformers,2020, In Dan Ju-rafsky
 Generating Natural Language Adversarial Examples,2018, In Proceedings of the 2018 Con-ference on Empirical Methods in Natural Language Processing
 T-Miner: A Generative Approach to De-fend Against Trojan Attacks on DNN-based Text Classification,2021, In Michael Bailey and RachelGreenstadt (eds
 On the Opportunities and Risks of Foundation Models,2021, CoRR
 A Large An-notated Corpus for Learning Natural Language Inference,2015, In Proceedings of the 2015 Confer-ence on Empirical Methods in Natural Language Processing
 Language Models are Few-ShotLearners,2020, In H
 Towards Robust Neural Networks via Close-loopControl,2021, In 9th International Conference on Learning Representations
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2019, In Proceedings of the 2019 Confer-ence of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies
 Towards Robustness Against NaturalLanguage Word Substitutions,2021, In 9th International Conference on Learning Representations
 HotFlip: White-Box AdversarialExamples for Text Classification,2018, In Proceedings of the 56th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 2: Short Papers)
 Text Processing Like Humans Do:Visually Attacking and Shielding NLP Systems,1634, In Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Making Pre-trained Language Models Better Few-shotLearners,2021, In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing (Volume1: Long Papers)
 BAE: BERT-based Adversarial Examples for TextClassification,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 WARP: Word-level AdversarialReProgramming,2021, In Proceedings of the 59th Annual Meeting of the Association for Computa-tional Linguistics and the 11th International Joint Conference on Natural Language Processing(Volume 1: Long Papers)
 Ar-ray programming with NumPy,2649, Nature
 Parameter-Efficient Transfer Learning forNLP,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Achieving Verified Robustness to Symbol Sub-stitutions via Interval Bound Propagation,2019, In Proceedings of the 2019 Conference on Empir-ical Methods in Natural Language Processing and the 9th International Joint Conference onNatural Language Processing (EMNLP-IJCNLP)
 Adversarial Example Gener-ation with Syntactically Controlled Paraphrase Networks,2018, In Proceedings of the 2018 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
 Attention is not Explanation,3543, In Jill Burstein
 Certified Robustness to AdversarialWord Substitutions,2019, In Proceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing and the 9th International Joint Conference on Natural Language Process-ing (EMNLP-IJCNLP)
 SMART:Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through PrincipledRegularized Optimization,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
 Robust Encodings: A Framework forCombating Adversarial Typos,2020, In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics
 On the Geometry of Adversarial Examples,2018, CoRR
 Adam: A Method for Stochastic Optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Statistical Significance Tests for Machine Translation Evaluation,2004, In Proceedings ofthe 2004 Conference on Empirical Methods in Natural Language Processing 
 Pontryagin maximum principle,1962, In Mathematics in Science and Engineering
 A Sweet Rabbit Hole by DARCY: Using Honeypotsto Detect Universal Trigger¡¯s Adversarial Attacks,3831, In Chengqing Zong
 The Power of Scale for Parameter-Efficient PromptTuning,2021, In Marie-Francine Moens
 Knowledge Enhanced Attention for Robust Natural Lan-guage Inference,2019, CoRR
 TextBugger: Generating Adversar-ial Text Against Real-world Applications,2019, In 26th Annual Network and Distributed SystemSecurity Symposium
 Prefix-Tuning: Optimizing Continuous Prompts for Generation,2021, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 Searching for an Effective Defender: Benchmarking Defense against Ad-versarial Word Substitution,2021, In Marie-Francine Moens
 Adversarial Training for Large Neural Language Models,2020, CoRR
 RoBERTa: A Robustly Optimized BERT Pre-training Approach,2019, CoRR
 Decoupled Weight Decay Regularization,2019, In 7th InternationalConference on Learning Representations
 Scientific Credibility of Machine Transla-tion Research: A Meta-Evaluation of 769 Papers,7297, In Chengqing Zong
 The Natural LanguageDecathlon: Multitask Learning as Question Answering,2018, CoRR
 Adversarial Training Methods for Semi-Supervised Text Classification,2017, In 5th International Conference on Learning Representations
 Virtual Adversarial Training:A Regularization Method for Supervised and Semi-Supervised Learning,2019, IEEE Trans
 Deep Contextualized Word Representations,2018, In Proceedings of the2018 Conference of the North American Chapter of the Association for Computational Linguis-tics: Human Language Technologies
 Combating Adversarial Misspellings withRobust Word Recognition,2019, In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics
 Learning How to Ask: Querying LMs with Mixtures of SoftPrompts,2021, In Proceedings of the 2021 Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Language Technologies
 LanguageModels are Unsupervised Multitask Learners,2019,2019
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2020, Journal ofMachine Learning Research
 Generating Natural Language Adver-sarial Examples through Probability Weighted Word Saliency,2019, In Proceedings of the 57th An-nual Meeting of the Association for Computational Linguistics
 Robustness Verifi-cation for Transformers,2020, In International Conference on Learning Representations
 Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Ro-bust Finetuning,2021, In Chengqing Zong
 Recursive Deep Models for Semantic Compositionality Over a SentimentTreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Energy and Policy Considerations forDeep Learning in NLP,2019, In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics
 The Bottom-up Evolution of Representations in theTransformer: A Study with Machine Translation and Language Modeling Objectives,2019, In Pro-ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Universal Adversar-ial Triggers for Attacking and Analyzing NLP,2019, In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP)
 InfoBERT:Improving Robustness of Language Models from An Information Theoretic Perspective,2021, In9th International Conference on Learning Representations
 Attention is not not Explanation,2019, In Kentaro Inui
 Transformers: State-of-the-Art Nat-ural Language Processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Automatic Perturbation Analysis for Scalable Certi-fied Robustness and Beyond,2020, In H
 Grey-box Adversarial Attack AndDefence For Sentiment Classification,2021, In Proceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019, InH
 Hi-erarchical Attention Networks for Document Classification,2016, In Kevin Knight
 SAFER: A Structure-free Approach for CertifiedRobustness to Adversarial Word Substitutions,2020, In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics
 OpenAttack: An Open-source Textual Adversarial AttackToolkit,2021, In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing: SystemDemonstrations
 Character-level Convolutional Networks forText Classification,2015, In C
 Defense againstSynonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble,2021, In Pro-ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 Learning to Discriminate Perturba-tions for Blocking Adversarial Attacks in Text Classification,2019, In Proceedings of the 2019 Con-ference on Empirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing (EMNLP-IJCNLP)
 FreeLB: EnhancedAdversarial Training for Natural Language Understanding,2020, In 8th International Conferenceon Learning Representations
