title,year,conference
 Layer normalization,2016,"	CoRR"
 Language models are few-shot learners,2020, In Neural Informa-tion Processing Systems
 End-to-end object detection with transformers,2020, In Computer Vision Euro-Pean Conference
 Crossvit: Cross-attention multi-scale visiontransformer for image classification,2021, CoRR
 Regionvit: Regional-to-local attention for visiontransformers,2021, CoRR
 MMDetection: Open mmlab detection toolbox andbenchmark,2019, arXiv preprint arXiv:1906
 Generating long sequences with sparsetransformers,2019, CoRR
 Twins: Revisiting spatial attention design in vision transformers,2021, CoRR
 Randaugment: Practical automated dataaugmentation with a reduced search space,2020, In Neural Information Processing Systems
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Conference of the North AmericanChapter of the Association for Computational Linguistics
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, In International Conference on Learning Representations
 Transformer intransformer,2021, CoRR
 Mask R-CNN,2017, In InternationalConference on Computer Vision
 Deep networks withstochastic depth,2016, In Bastian Leibe
 Shuffle trans-former: Rethinking spatial shuffle for vision transformer,2021, CoRR
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 PanoPtic feature pyramidnetworks,2019, In Conference on Computer Vision and Pattern Recognition
 CAT: cross attention in vision transformer,2021, CoRR
 Microsoft COCO: common objects in context,2014, In EuropeanConference on Computer Vision
 Focal loss for denseobject detection,2020, Transactions on Pattern Analysis and Machine Intelligence
 Transformer inconvolutional neural networks,2021, CoRR
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, CoRR
 Rectified linear units improve restricted boltzmann machines,2010, InJohannes Furnkranz and Thorsten Joachims (eds
 Scalable visual transformers withhierarchical pooling,2021, CoRR
 Imagenet large scale visual recognition challenge,2015, International Journal of Computer Vision
 Self-attention with relative position represen-tations,2018, In Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
 Mixconv: Mixed depthwise convolutional kernels,2019, In British Ma-chine Vision Conference
 Training data-efficient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Attention is all you need,2017, In Neural Information ProcessingSystems
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, CoRR
 Cvt:Introducing convolutions to vision transformers,2021, CoRR
 Unified perceptual parsing forscene understanding,1120, In European Conference on Computer Vision
 Co-scale conv-attentional image trans-formers,2021, CoRR
 Vitae: Vision transformer advanced byexploring intrinsic inductive bias,2021, CoRR
 Big bird:Transformers for longer sequences,2020, In Hugo Larochelle
 mixup: Beyond empiri-cal risk minimization,2018, In International Conference on Learning Representations
 Rest: An efficient transformer for visual recognition,2021, CoRR
 Interleaved group convolutions for deepneural networks,2017, CoRR
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In IEEE Conference on Computer Vision andPattern Recognition
 Aggregating nested trans-formers,2021, CoRR
 Random erasing data aug-mentation,2020, In Association for the Advancement of Artificial Intelligence
 Sceneparsing through ADE20K dataset,2017, In Conference on Computer Vision and Pattern Recognition
