title,year,conference
 Mining source code repositories at massive scale usinglanguage modeling,2013, In 2013 10th Working Conference on Mining Software Repositories (MSR)
 A survey of machinelearning for big code and naturalness,2018, ACM Computing Surveys (CSUR)
 Structural language models of code,2020, InInternational Conference on Machine Learning
 Neural machine translation by jointlylearning to align and translate,2015, In Proceedings of ICLR
 A neural probabilisticlanguage model,2003, Journal ofmaChine learning research
 Effective domain mixing for neural machine translation,2017, InProceedings of the Second Conference on Machine Translation
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Evaluating large language models trained oncode,2021, arXiv preprint arXiv:2107
 An empirical comparison of domain adaptationmethods for neural machine translation,2017, In Proceedings of the 55th Annual Meeting of theAssociation for Computational Linguistics (Volume 2: Short Papers)
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Unbounded cache model for online languagemodeling with open vocabulary,2017, arXiv preprint arXiv:1711
 Generating sentences byediting prototypes,2018, Transactions of the Association for Computational Linguistics
 A retrieve-and-edit frameworkfor predicting structured outputs,2018, In Proceedings of NeurIPS
 Learning sparse prototypes for textgeneration,2020, arXiv preprint arXiv:2006
 On the naturalnessof software,2016, Communications of the ACM
 Bigcode!= big vocabulary: Open-vocabulary models for source code,2020, In 2020 IEEE/ACM 42ndInternational Conference on Software Engineering (ICSE)
