title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Efficient and robust parallel dnn trainingthrough model parallelism on multi-gpu platform,2018, arXiv preprint arXiv:1809
 Training deep nets with sublinearmemory cost,2016, arXiv preprint arXiv:1604
 Project adam:Building an efficient and scalable deep learning training system,2014, In 11th {USENIX} Symposiumon Operating Systems Design and Implementation ({ OSDI} 14)
 Christian Sarofeen,2021, Nvidia/apex
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Edward Z,2021, Yang
 Francisco Massa,2021, pytorch/vision
 Pipedream: Fast and efficient pipeline parallel dnn training,2018, arXiv preprintarXiv:1806
 Checkmate: Breaking the memory wall with optimal tensorrematerialization,2019, arXiv preprint arXiv:1910
 Beyond data and model parallelism for deep neuralnetworks,2018, arXiv preprint arXiv:1807
 Learning multiple layers of features from tiny images,2009, 2009
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 M6: A chinese multimodal pretrainer,2021, arXiv preprintarXiv:2103
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Memory-efficientpipeline-parallel dnn training,7937, In International Conference on Machine Learning
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Megatron-lm: Training multi-billion parameter language models using gpu modelparallelism,2019, arXiv preprint arXiv:1909
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Transformers: State-of-the-art natural languageprocessing,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-cessing: System Demonstrations
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
