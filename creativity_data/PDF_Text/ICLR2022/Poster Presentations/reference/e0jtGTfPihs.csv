title,year,conference
 Ternary neural networksfor resource-efficient ai applications,2016, 9 2016
 Estimating or propagating gradients throughstochastic neurons for conditional computation,2013, 8 2013
 Textual evidence for the perfunctorinessof independent medical reviews,1613, CEUR Workshop Proceedings
 PruningRandomly Initialized Neural Networks with Iterative Randomization,2021, jun 2021
 BinaryConnect: Training Deep NeuralNetworks with binary weights during propagations,2015, nov 2015
 Binarizedneural networks: Training deep neural networks with weights and activations constrained to +1 or-1,2016, 2 2016
 An embarrassingly simple approach to training ternary weightnetworks,2020, 11 2020
 Multi-Prize Lottery Ticket Hypothesis: Finding AccurateBinary Neural Networks by Pruning A Randomly Weighted Network,2021, mar 2021
 Rigging the Lottery:Making All Tickets Winners,2019, nov 2019
 Stabilizing thelottery ticket hypothesis,2019, 3 2019
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, Proceedings of the Thirteenth International Conference on Artificial Intelligenceand Statistics
 Learning both weights and connections forefficient neural networks,2015, 6 2015
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, 2 2015
 Deep residual learning for imagerecognition,1063, Proceedings of the IEEE Computer Society Conference on Computer Vision andPattern Recognition
 At-scale sparse deep neural network inference with efficient gpuimplementation,2020, 2020 IEEE High Performance Extreme Computing Conference
 Neural networks for machine learning,2012, coursera
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, 32nd International Conference on Machine Learning
 Pruning versus clipping in neural networks,0556, Physical Review A
 Variational dropout and the local reparameteri-zation trick,2015, 6 2015
 Optimal brain damage,1990, Advances in NeuralInformation Processing Systems 2
 Ternary weight networks,2016, 5 2016a
 Pruning filters forefficient convnets,2016, 8 2016b
 Sparse Training via BoostingPruning Plasticity with Neuroregeneration,2021, jun 2021
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2041, Nature Communications
 Importance estimationfor neural network pruning,2019, arXiv
 DeepResidual Networks with Exponential Linear Unit,2016, apr 2016
 Learning discrete weights using the local reparameteriza-tion trick,2017, 10 2017
 Balanced Binary Neural Networks withGated Residual,2019, sep 2019
 Very Deep Convolutional Networks for Large-Scale ImageRecognition,2014, sep 2014
 Energy and policy considerations for deeplearning in nlp,2020, ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics
 Trained ternary quantization,2016, 12 2016
