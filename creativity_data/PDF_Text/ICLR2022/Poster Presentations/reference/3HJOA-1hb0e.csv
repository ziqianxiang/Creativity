title,year,conference
 Shifted and squeezed 8-bit floating point format for low-precision training of deep neuralnetWorks,2020, arXiv preprint arXiv:2001
 Accurate and efficient 2-bit quantized neural netWorks,2019, In MLSys
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Deepshift: To-Wards multiplication-less neural netWorks,2021, In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition
 A block minifloat repre-sentation for training deep neural netWorks,2020, In International Conference on Learning Represen-tations
 Float-fix: An efficient andhardWare-friendly data type for deep neural netWork,2019, International Journal of Parallel Program-ming
 A study of bfloat16 for deep learning training,2019, arXiv preprint arXiv:1905
 Flexpoint: An adaptive numericalformat for efficient training of deep neural netWorks,2017, arXiv preprint arXiv:1711
 Lognet:Energy-efficient neural netWorks using logarithmic computation,2017, In 2017 IEEE InternationalConference on Acoustics
 Ssd: Single shot multibox detector,2016, In European conference on computervision
 Building a large annotatedcorpus of english: The penn treebank,1993,1993
 Training binary neuralnetworks with real-to-binary convolutions,2020, arXiv preprint arXiv:2003
 Up ordown? adaptive rounding for post-training quantization,2020, In International Conference on MachineLearning
 A neural network training processor with 8-bitshared exponent bias floating point and multiple-way fused multiply-add trees,2021, IEEE Journal ofSolid-State Circuits
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Algorithm-hardware co-design of adaptive floating-point en-codings for resilient deep learning inference,2020, In 2020 57th ACM/IEEE Design Automation Con-ference (DAC)
 Train-ing deep neural networks with 8-bit floating point numbers,2018, In Proceedings of the 32nd Interna-tional Conference on Neural Information Processing Systems
 Swalp: Stochastic weight averaging in low precision training,2019, In International Conferenceon Machine Learning
 Quantization networks,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
