title,year,conference
 Optimal Gradient Compression forDistributed and Federated Learning,2020, arXiv preprint arXiv:2010
 QSGD: Communication-EfficientSGD via Gradient Quantization and Encoding,2017, In Neural Information Processing Systems (NeurIPS)
 Towards Generalized and Distributed Privacy-preserving Representation Learning,2021, arXiv preprintarXiv:2010
 SignSGD: Com-pressed Optimisation for Non-convex Problems,2018, In International Conference on Machine Learning (ICML)
 Continual Learning in Low-rankOrthogonal Subspaces,2020, In Neural Information Processing Systems (NeurIPS)
 DKM: Differentiable K-MeansClustering Layer for Neural Network Compression,2021, arXiv preprint arXiv:2108
 Support-vector Networks,1995, Machine Learning
 Hybrid Deterministic-stochastic Methods for Data Fitting,2012, SIAMJournal on Scientific Computing
 An Investigation into Neural Net Optimization viaHessian Eigenvalue Density,2019, In International Conference on Machine Learning (ICML)
 Gradient Descent Happens in a Tiny Subspace,2018, arXiv preprintarXiv:1812
 Federated Learningwith Compression: Unified Analysis and Sharp Guarantees,2021, In International Conference on ArtificialIntelligence and Statistics (AISTATS)
 Learning both Weights and Connections for EfficientNeural Networks,2015, In Neural Information Processing Systems (NeurIPS)
 Deep Residual Learning for Image Recognition,2016, InIEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Distilling the Knowledge in a Neural Network,2015, arXiv preprintarXiv:1503
 Multi-stage Hybrid Federated Learning over Large-scale Wireless FogNetworks,2020, arXiv preprint arXiv:2007
 Densely Connected ConvolutionalNetworks,2017, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Error Feedback fixes SignSGDand other Gradient Compression Schemes,2019, In International Conference on Machine Learning (ICML)
 Learning Multiple Layers of Features from Tiny Images,2009, 2009
 Optimal Brain Damage,1990, In Neural Information ProcessingSystems (NeurIPS)
 LotteryFL: Personalizedand Communication-efficient Federated Learning with Lottery Ticket Hypothesis on Non-iid Datasets,2020, arXivpreprint arXiv:2008
 Microsoft COCO: Common Objects in Context,2014, In European Conference on ComputerVision (ECCV)
 RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019, arXivpreprint arXiv:1907
 Rethinking the Value of NetworkPruning,2019, In International Conference on Learning Representations (ICLR)
 Deep Learning Face Attributes in the Wild,2015, In IEEEInternational Conference on Computer Vision (ICCV)
 SGDR: Stochastic Gradient Descent with Warm Restarts,2016, In InternationalConference on Learning Representations (ICLR)
 U-Net: Convolutional Networks for Biomedical ImageSegmentation,2015, In International Conference on Medical Image Computing and Computer-assisted Intervention
 A Generic Framework for Privacy Preserving Deep Learning,2018, arXiv preprintarXiv:1811
 Eigenvalues of the Hessian in Deep Learning: Singularity andBeyond,2016, arXiv preprint arXiv:1611
 Empirical Analysis of the Hessianof Over-parametrized Neural Networks,2017, arXiv preprint arXiv:1706
 Gradient Projection Memory for Continual Learning,2020, In Interna-tional Conference on Learning Representations (ICLR)
 Robust and Communication-efficient Federated Learning from Non-iid Data,2019, IEEE Transactions on Neural Networks and LearningSystems
 1-bit Stochastic Gradient Descent and itsApplication to Data-parallel Distributed Training of Speech DNNs,2014, In Conference of the International SpeechCommunication Association (INTERSPEECH)
 Privacy-preserving Deep Learning,2015, In ACM SIGSAC Conference onComputer and Communications Security (CCS)
 Clustering Convolutional Kernels to Compress DeepNeural Networks,2018, In European Conference on Computer Vision (ECCV)
 Local SGD Converges Fast and Communicates Little,2019, In International Conference onLearning Representations (ICLR)
 PowerSGD: Practical Low-rank Gradient Compressionfor Distributed Optimization,2019, In Neural Information Processing Systems (NeurIPS)
 Cooperative SGD: A Unified Framework for the Design and Analysis ofCommunication-efficient SGD Algorithms,2018, arXiv preprint arXiv:1808
 Tackling the Objective InconsistencyProblem in Heterogeneous Federated Optimization,2020, In Neural Information Processing Systems (NeurIPS)
 SlowMo: Improving Communication-efficient Distributed SGD with Slow Momentum,2020, In International Conference on Learning Representations(ICLR)
 Gradient Sparsification for Communication-efficientDistributed Optimization,2018, In Neural Information Processing Systems (NeurIPS)
