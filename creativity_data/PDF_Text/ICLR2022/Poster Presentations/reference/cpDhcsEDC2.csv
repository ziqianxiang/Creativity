title,year,conference
 Language models arefew-shot learners,2020, In Advances in neural information processing systems
 Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts,2021, In IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Training deep nets with sublinearmemory cost,2016, preprint arXiv:1604
 Uniter: Universal image-text representation learning,2020, In European conference oncomputer vision
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Annual Conference of the North Amer-ican Chapter of the Association for Computational Linguistics
 M5product: A multi-modal pretraining benchmark for e-commercial product downstreamtasks,2021, Preprint arXiv:2109
 An im-age is worth 16x16 words: Transformers for image recognition at scale,2020, In International Confer-ence on Learning Representations
 Large-scale adversarialtraining for vision-and-language representation learning,2020, arXiv preprint arXiv:2006
 Algorithm 799: revolve: an implementation of check-pointing for the reverse or adjoint mode of computational differentiation,2000, ACM Transactions onMathematical Software (TOMS)
 Aug-ment your batch: Improving generalization through instance repetition,2020, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Seeingout of the box: End-to-end pre-training for vision-language representation learning,2021, In IEEE/CVFConference on Computer Vision and Pattern Recognition
 Scaling up visual and vision-language representation learningwith noisy text supervision,2021, In International Conference on Machine Learning
 Colbert: Efficient and effective passage search via contextualizedlate interaction over bert,2020, In International ACM SIGIR Conference on Research and Developmentin Information Retrieval
 Vilt: Vision-and-language transformer without convo-lution or region supervision,2021, In International Conference on Machine Learning
 Stacked cross attention forimage-text matching,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Unicoder-vl: A universal encoderfor vision and language by cross-modal pre-training,2020, In Proceedings of the AAAI Conference onArtificial Intelligence
 Align before fuse: Vision and language representation learning with momentumdistillation,2021, Technical Report arXiv:2107
 Visualbert: A simpleand performant baseline for vision and language,2019, Preprint arXiv:1908
 Oscar: Object-semantics aligned pre-training for vision-languagetasks,2020, In European Conference on Computer Vision
 M6: A chinese multimodal pretrainer,2021, Preprint arXiv:2103
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Sgdr: Stochastic gradient descent With Warm restarts,2016, ICLR 2017 (5thInternational Conference on Learning Representations)
 Vilbert: pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, In International Conference on NeuralInformation Processing Systems
 Mixed precisiontraining,2018, In International Conference on Learning Representations
 Scikit-learn:Machine learning in python,2011, the Journal of machine Learning research
 Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models,2015, In IEEE international conference on computer vision
 Imagebert: Cross-modalpre-training With large-scale Weak-supervised image-text data,2020, arXiv preprint arXiv:2001
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Learning transferable visualmodels from natural language supervision,2021, Preprint arXiv:2103
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal ofMachine Learning Research
 Zero-shot text-to-image generation,2021, Preprint arXiv:2102
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, Advances in neural information processing systems
 Apac: Augmented pattern classification withneural networks,2015, Technical Report arXiv:1505
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In IEEE international conference on computer vision
 Improving neural machine translation modelswith monolingual data,2016, In Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers)
 Neural machine translation of rare words withsubword units,2016, In Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers)
 Yfcc100m: The new data in multimedia research,2016, Communicationsof the ACM
 Contrastive multiview coding,2020, In ComputerVision-ECCV 2020: 16th European Conference
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Simvlm: Simplevisual language model pretraining with weak supervision,2021, Preprint arXiv:2108
 Unsupervised data augmentationfor consistency training,2020, In Advances in Neural Information Processing Systems
 Self-training with noisy studentimproves imagenet classification,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Ernie-vil: Knowl-edge enhanced vision-language representations through scene graphs,2021, In Proceedings of the AAAIConference on Artificial Intelligence
 Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining,2021, In International Conference on Computer Vision
 Vinvl: Revisiting visual representations in vision-language models,2021, In IEEE/CVFConference on Computer Vision and Pattern Recognition
 Following the setting in Section 4,2022,4
 The inference time of retrieval is shown in Table 17,5000, Benefitting from the efficiency opti-mizations (i
