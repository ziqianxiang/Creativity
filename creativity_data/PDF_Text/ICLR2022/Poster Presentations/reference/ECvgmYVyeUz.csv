title,year,conference
 Investigating the role ofnegatives in contrastive representation learning,2021, arXiv preprint arXiv:2106
 Further reverse results for jensen¡¯s discreteinequality and applications in information theory,2000, RGMIA research report collection
 A simple framework forcontrastive learning of visual representations,2020, ICML
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Exploring simple siamese representation learning,2020, arXiv preprintarXiv:2011
 An empirical study of training self-supervised visiontransformers,2021, arXiv preprint arXiv:2104
 RIFD-CNN: Rotation-invariant and fisher discrimi-native convolutional neural networks for object detection,2016, In CVPR
 Bootstrap your own latent: Anew approach to self-supervised learning,2020, NeurIPS
 Provable guarantees for self-superviseddeep learning with spectral contrastive loss,2021, NeurIPS
 Momentum contrast forunsupervised visual representation learning,2020, CVPR
 Learning deep representations by mutual information estimationand maximization,2019, ICLR
 Towards the generalization of contrastive self-supervised learning,2021, arXiv preprint arXiv:2111
 Predicting what you already know helps:Provable self-supervised learning,2020, arXiv preprint arXiv:2008
 About contrastive unsupervisedrepresentation learning for classification and its convergence,2020, arXiv preprint arXiv:2012
 Understanding negative samples in instance discriminative self-supervised representation learning,2021, NeurIPS
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 A Strong Law for the Longest Edge of the Minimal Spanning Tree,1999, The AnnalsOfProbability
 Scaling universalities of kth-nearest neighbor distances onclosed manifolds,196, Advances in Applied Mathematics
 On variationalbounds of mutual information,2019, In ICML
 Selfaugment:Automatic augmentation policies for self-supervised learning,2021, In CVPR
 Understanding the limitations of variational mutual informationestimators,2020, In ICLR
 Whatmakes for good views for contrastive learning,2020, NeurIPS
 Contrastive estimation reveals topicposterior information to linear models,2020, arXiv preprint arXiv:2003
 Self-supervised learning from a multi-view perspective,2021, In ICLR
 Residual relaxation for multi-view representation learning,2021, In NeurIPS
 Scale-invariant con-volutional neural networks,2014, arXiv preprint arXiv:1411
 Similar to the proof of Theorem A,2022,3
