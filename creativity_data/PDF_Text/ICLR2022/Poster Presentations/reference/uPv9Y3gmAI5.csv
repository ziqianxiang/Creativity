title,year,conference
 Online embedding com-pression for text classification using low rank matrix factorization,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Groupreduce: block-wiselow-rank approximation for neural language model shrinking,2018, In Proceedings of the 32nd Inter-national Conference on Neural Information Processing Systems
 Quora question pairs,2018, 2018b
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Advances in neural informa-tion processing systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Microsoft research paraphrase corpus,2005, RetrievedMarch
 Singular value decomposition and least squares solutions,1971, InLinear algebra
 Hyperparameter-freecontinuous learning for domain classification in natural language understanding,2021, In Proceedingsof the 2021 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 Over-coming catastrophic forgetting in neural networks,2017, volume 114
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Group fisher pruning for practicalnetwork compression,2021, In International Conference on Machine Learning
 Fine-tune bert for extractive summarization,2019, arXiv preprint arXiv:1903
 Importance estimationfor neural network pruning,2019, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Compressing pre-trained language models by matrix de-composition,2020, In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associationfor Computational Linguistics and the 10th International Joint Conference on Natural LanguageProcessing
 Revisiting natural gradient for deep networks,2014, In In Interna-tional Conference on Learning Representations (ICLR)
 Improving language under-standing with unsupervised learning,2018, 2018
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 conference on empirical methods in natural language pro-cessing
 Weighted low-rank approximations,2003, In Proceedings of the 20thInternational Conference on Machine Learning (ICML-03)
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Mobile-bert: a compact task-agnostic bert for resource-limited devices,2020, arXiv preprint arXiv:2004
 Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers,2020, arXiv preprintarXiv:2012
 Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,2020, arXiv preprintarXiv:2002
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In 2018 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Swag: A large-scale adversarialdataset for grounded commonsense inference,2018, arXiv preprint arXiv:1808
 Accelerating very deep convolutionalnetworks for classification and detection,2015, IEEE transactions on pattern analysis and machineintelligence
