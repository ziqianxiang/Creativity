title,year,conference
 nocaps: novel object captioning at scale,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
 A large anno-tated corpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Bigself-supervised models are strong semi-supervised learners,2020, Advances in Neural InformationProcessing Systems
 Microsoft coco captions: Data collection and evaluation server,2015, arXivpreprint arXiv:1504
 Uniter: Universal image-text representation learning,2020, In ECCV
 Unifying vision-and-language tasks via textgeneration,2021, arXiv preprint arXiv:2102
 Meshed-memory trans-former for image captioning,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Coatnet: Marrying convolution andattention for all data sizes,2021, arXiv preprint arXiv:2106
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 An image is Worth 16x16 Words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Multi30k: Multilingual english-german image descriptions,2016, arXiv preprint arXiv:1605
 Large-scale adversarialtraining for vision-and-language representation learning,2020, arXiv preprint arXiv:2006
 Making the v in vqamatter: Elevating the role of image understanding in visual question answering,2017, In Proceedingsofthe IEEE Conference on Computer Vision and Pattern Recognition
 Vivo:Surpassing human performance in novel object captioning with visual vocabulary pre-training,2021, InAAAI
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Seeing outof the box: End-to-end pre-training for vision-language representation learning,2021, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Effect of vision-and-language extensions on natural language under-standing in vision-and-language models,2021, arXiv preprint arXiv:2104
 Scaling up visual and vision-language representation learningwith noisy text supervision,2021, arXiv preprint arXiv:2102
 Visualgenome: Connecting language and vision using crowdsourced dense image annotations,2016, 2016
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, arXiv preprint arXiv:1808
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Oscar: Object-semantics aligned pre-training for vision-language tasks,2020, ECCV 2020
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks,2019, In H
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In C
 Self-criticalsequence training for image captioning,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 Superglue:Learning feature matching with graph neural networks,2020, In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition
 A shared task on multimodalmachine translation and crosslingual image description,2016, In Proceedings of the First Conferenceon Machine Translation: Volume 2
 Wit:Wikipedia-based image text dataset for multimodal multilingual machine learning,2021, arXiv preprintarXiv:2103
 Vl-bert: Pre-training of generic visual-linguistic representations,2020, In International Conference on LearningRepresentations
 A corpus forreasoning about natural language grounded in photographs,2018, arXiv preprint arXiv:1811
 LXMERT: Learning cross-modality encoder representations fromtransformers,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP)
 Multi-modal few-shot learning with frozen language models,2021, Advances in Neural Information Process-ing Systems
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Visual entailment: A novel task for fine-grained image understanding,2019, arXiv preprint arXiv:1901
 Ernie-vil: Knowl-edge enhanced vision-language representations through scene graphs,2021, In Proceedings of the AAAIConference on Artificial Intelligence
 Vinvl: Revisiting visual representations in vision-language models,2021, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
