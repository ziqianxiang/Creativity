title,year,conference
 Multigrain: aunified image embedding for classes and instances,2019, arXiv preprint arXiv:1902
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Advances in Neural Information ProcessingSystems
 Improved regularization of convolutional neural networkswith cutout,2017, arXiv preprint arXiv:1708
 Adapting auxiliary losses using gradient similarity,2018, arXiv preprintarXiv:1812
 Drawing multipleaugmentation samples per image during training efficiently decreases test error,2021, arXiv preprintarXiv:2105
 Deep learning,2016, MIT pressCambridge
 Faster autoaugment:Learning augmentation strategies using backpropagation,2020, In European Conference on ComputerVision
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Population based augmentation:Efficient learning of augmentation policy schedules,2019, In International Conference on MachineLearning
 Augmentyour batch: Improving generalization through instance repetition,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Data augmentation by pairing samples for images classification,2018, arXiv preprintarXiv:1801
 Categorical reparameterization with gumbel-softmax,2017, InInternational Conference on Learning Representations
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Differentiable automatic data augmentation,2020, In European Conference on Computer Vision
 Uniformaugment: A search-free probabilistic data augmentationapproach,2020, arXiv preprint arXiv:2003
 Darts: Differentiable architecture search,2018, InInternational Conference on Learning Representations
 In-loop meta-learning with gradient-alignmentreward,2021, arXiv preprint arXiv:2102
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Optimizing data usage via differentiable rewards,2020, In International Conference on MachineLearning
 Resnet strikes back: An improved trainingprocedure in timm,2021, volume 34
 Improve unsupervised domainadaptation with mixup training,2020, In arXiv preprint arXiv: 2001
 Wide residual networks,2016, In British Machine VisionConference 2016
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
 mixup: Beyond empiricalrisk minimization,2018, International Conference on Learning Representations
 Adversarial autoaugment,2019, In InternationalConference on Learning Representations
 The best found parameters are summarized in Table 8 inAppendix,2019, We did not tune the hyperparameters of AdvAA (Zhang et al
