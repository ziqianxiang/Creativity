title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In International Conference on Learning Representations
 Metaquant: Learning to quantize by learning topenetrate non-differentiable quantization,2019, In Advances in Neural Information Processing Systems
 Towards the limit of network quantization,2017, InInternational Conference on Learning Representations
 Training with quantization noise for extreme model compression,2021, In InternationalConference on Learning Representations
 Deep residUal learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Mobilenets: Efficient convolUtional neUral networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Network qUantization with element-wise gradient scaling,2021, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Categorical reparameterization with gUmbel-softmax,2017, InInternational Conference on Learning Representations
 ALBERT: A lite BERT for self-sUpervised learning of langUage representations,2019, In Interna-tional Conference on Learning Representations
 Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks,2019, In International Conference on Learning Representa-tions
 Towards accurate binary convolutional neural network,2017, InAdvances in Neural Information Processing Systems
 Darts: Differentiable architecture search,2019, InInternational Conference on Learning Representations
 Up ordown? Adaptive rounding for post-training quantization,2020, In International Conference on MachineLearning
 Weighted-entropy-based quantization for deepneural networks,2017, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-nition
 Value-aware quantization for training and inferenceof neural networks,2018, In European Conference on Computer Vision
 Lookahead: A far-sighted alternative ofmagnitude-based pruning,2019, In International Conference on Learning Representations
 Model compression via distillation andqUantization,2018, In International Conference on Learning Representations
 Xnor-net: Imagenetclassification Using binary convolUtional neUral networks,2016, In European Conference on ComputerVision
 Mo-bilenetv2: Inverted residUals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Learning discrete weights Using the local reparameteri-zation trick,2018, In International Conference on Learning Representations
 And the bit goesdown: Revisiting the qUantization of neUral networks,2020, In International Conference on LearningRepresentations
 Mobilebert:a compact task-agnostic bert for resource-limited devices,2020, In Proceedings of the 58th AnnualMeeting of the Associationfor Computational Linguistics
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Efficientnetv2: Smaller models and faster training,2021, arXiv preprintarXiv:2104
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Well-read students learn better:The impact of student initialization on knowledge distillation,2019, arXiv preprint arXiv:1908
 Soft weight-sharing for neural network compres-sion,2017, In International Conference on Learning Representations
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2019, In Interna-tional Conference on Learning Representations
 Haq: Hardware-aware automated quan-tization with mixed precision,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Deepk-means: Re-training and parameter sharing with harder cluster assignments for compressingdeep convolutions,2018, In International Conference on Machine Learning
 Under-standing straight-through estimator in training activation quantized neural nets,2019, In InternationalConference on Learning Representations
 Gobo: Quantizingattention-based nlp models for low latency and energy efficient inference,2020, In 2020 53rd AnnualIEEE/ACM International Symposium on Microarchitecture
 Lq-nets: Learned quantizationfor highly accurate and compact deep neural networks,2018, In European Conference on ComputerVision
 Linear symmetric quantizationof neural networks for low-precision integer hardware,2019, In International Conference on LearningRepresentations
 Neural epit-ome search for architecture-agnostic network compression,2019, In International Conference on Learn-ing Representations
