title,year,conference
 Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes,2017, arXiv preprint arXiv:1711
 Understanding and improving fast adversarialtraining,2020, In Advances in Neural Information Processing Systems
 Multigrain:a unified image embedding for classes and instances,2019, arXiv preprint arXiv:1902
 Robustand accurate object detection via adversarial learning,2021, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR)
 Autoaugment:Learning augmentation strategies from data,2019, In IEEE Conference on Computer Vision and PatternRecognition
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In IEEE Conference on Computer Vision and Pattern Recognition
 Adabatch: Adaptive batch sizes fortraining deep neural networks,2017, arXiv preprint arXiv:1712
 BERT: pre-training ofdeep bidirectional transformers for language understanding,2019, In Jill Burstein
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 DeeP residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Firecaffe: near-linear acceleration of deeP neural network training on comPute clusters,2016, In IEEE Conference onComputer Vision and Pattern Recognition
 Highly scalable deeP learning training system withmixed-Precision: Training imagenet in four minutes,2018, arXiv preprint arXiv:1807
 Characterizing the decision boundary of deeP neuralnetworks,2019, arXiv preprint arXiv:1912
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InInternational Conference on Learning Representations
 ExPloring the limits of concurrency in ml training on google tPus,2021, Machine Learning andSystems
 Minimax statistical learning with wasserstein distances,2017, arXivpreprint arXiv:1705
 Scaling distributed machine learning with system and algorithm co-design,2017, PhD thesis
 Towards efficient and scalablesharPness-aware minimization,2022, arXiv preprint arXiv:2203
 SParse-mlP: A fully-mlP architecturewith conditional comPutation,2021, arXiv preprint arXiv:2109
 OPtimizing neural networks with kronecker-factored aPProximatecurvature,2015, In International conference on machine learning
 Fast advProP,2022, In International Conference on Learning Representations
 Transferability in machine learning: fromphenomena to black-box attacks using adversarial samples,2016, arXiv preprint arXiv:1605
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 Certifiable distributional robustness withprincipled adversarial training,2017, arXiv preprint arXiv:1710
 On theconvergence and robustness of adversarial training,2019, In ICML
 Adversar-ial examples improve image recognition,2020, In IEEE Conference on Computer Vision and PatternRecognition
 Yet another accelerated sgd:Resnet-50 training on imagenet in 74,2019,7 seconds
 Large batch size training of neural networks with adversarial training andsecond-order information,2018, arXiv preprint arXiv:1810
 Hessian-based analysisof large batch training and robustness to adversaries,2018, arXiv preprint arXiv:1802
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Imagenet training inminutes,2018, In International Conference on Parallel Processing
 Large-batch training for lstm and beyond,2019, In Proceedings of the International Conference for HighPerformance Computing
 You only propagateonce: Accelerating adversarial training via maximal principle,2019, arXiv preprint arXiv:1905
