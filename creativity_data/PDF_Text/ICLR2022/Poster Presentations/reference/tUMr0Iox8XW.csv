title,year,conference
 Wide Neural Networks with Bot-tlenecks are Deep Gaussian Processes,2020, arXiv:2001
 Why bigger is not always better: on finite and infinite neural networks,2020, In Pro-Ceedings of the 37th International Conference on Machine Learning
 Deep kernel processes,2021, In Proceedings ofthe 38th International Conference on Machine Learning
 A mean-field limit for certain deep neuralnetworks,2019, arXiv:1906
 On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,2018, arXiv:1805
 Kernel methods for deep learning,2009, In Advances in neuralinformation processing systems
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Modeling from Features: a Mean-fieldFramework for Over-parameterized Deep Neural Networks,2020, arXiv:2007
 Model-Agnostic Meta-Learning for Fast Adaptationof Deep Networks,2017, arXiv:1703
 Efficient Trainingof BERT by Progressively Stacking,2019, In Proceedings of the 36th International Conference onMachine Learning
 On the Trans-former Growth for Progressive BERT Training,2021, In Proceedings of the 2021 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Neural Tangent Kernel: Convergence andGeneralization in Neural Networks,2018, arXiv:1806
 Learning multiple layers of features from tiny images,2009, 2009
 Human-level concept learningthrough probabilistic program induction,0036, Science
 Deep Neural Networks as Gaussian Processes,2018, In International Conference on LearningRepresentations
 Finite Versus Infinite Neural Networks: an EmpiricalStudy,2020, arXiv:2007
 Measuring the Intrinsic Dimensionof Objective Landscapes,2018, arXiv:1804
 A mean field view of the landscape oftwo-layer neural networks,1091, Proceedings of the National Academy of Sciences
 A Rigorous Framework for the Mean Field Limit ofMultilayer Neural Networks,2020, arXiv:2001
 Neural Networks as Interacting Particle Systems:Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Er-ror,2018, arXiv:1805
 Doubly Stochastic Variational Inference for Deep GaussianProcesses,2017, arXiv:1705
 Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architectureare Gaussian Processes,2019, arXiv:1910
 Tensor Programs II: Neural Tangent Kernel for Any Architecture,2020, arXiv:2006
 Tensor Programs III: Neural Matrix Laws,2020, arXiv:2009
 Feature Learning in Infinite-Width Neural Networks,2020, arXiv:2011
 Tensor Programs IIb: Architectural Universality of Neural TangentKernel Training Dynamics,2021, arXiv:2105
 Tensor Programs V: Tuning Large NeuralNetworks via Zero-Shot Hyperparameter Transfer,2022, arXiv:2203
