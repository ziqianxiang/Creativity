title,year,conference
 Analysis of SGD with biased gradient estimators,2020, arXivpreprint arXiv:2008
 QSGD:communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems (NIPS)
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems (NeurIPS)
 signSGD withmajority vote is communication efficient and fault tolerant,2019, In Proceedings ofthe 7th InternationalConference on Learning Representations (ICLR)
 On biased compres-sion for distributed learning,2020, arXiv preprint arXiv:2002
 Distributed opti-mization and statistical learning via the alternating direction method of multipliers,2011, Found
 Distributed deep learningnetworks among institutions for medical imaging,2018, J
 Quantized adam with error feedback,2021, ACMTrans
 On the convergence ofA class of adam-typealgorithms for non-convex optimization,2019, In Proceedings of the 7th International Conference onLearning Representations (ICLR)
 On the convergence of decentralizedadaptive gradient methods,2021, arXiv preprint arXiv:2109
 Project adam:Building an efficient and scalable deep learning training system,2014, In Proceedings of the 11thUSENIX Symposium on Operating Systems Design and Implementation (OSDI)
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprintarXiv:1910
 8-bit approximations for parallelism in deep learning,2016, In Proceedings of the 4thInternational Conference on Learning Representations (ICLR)
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies (NAACL-HLT)
 Adaptive subgradient methods for online learningand stochastic optimization,2010, In Proceedings of the 23rd Conference on Learning Theory (COLT)
 Dual averaging for distributed optimiza-tion: Convergence analysis and network scaling,2012, IEEE Trans
 Stochastic first- and zeroth-order methods for nonconvexstochastic programming,2013, SIAM J
 Generative adversarial nets,2014, In Advances in NeuralInformation Processing Systems (NIPS)
 Speech recognition with deeprecurrent neural networks,2013, In Proceedings of IEEE International Conference on Acoustics
 Fedsketch: Communication-efficientand private federated learning via sketching,2020, arXiv preprint arXiv:2008
 Deep residual learning for image recog-nition,2016, In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Prox-pda: The proximal primal-dual algo-rithm for fast distributed nonconvex optimization and learning over networks,2017, In Proceedings ofthe 34th International Conference on Machine Learning (ICML)
 Sketchml: Accelerating distributed machinelearning with data sketches,2018, In Proceedings of the 2018 ACM International Conference on Man-agement of Data (SIGMOD)
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Fed-LAMB: Layerwise and dimensionwise locally adaptiveoptimization algorithm,2021, arXiv preprint arXiv:2110
 Error feedbackfixes signsgd and other gradient compression schemes,2019, In Proceedings of the 36th InternationalConference on Machine Learning (ICML)
 Mime: Mimicking centralized stochastic algorithmsin federated learning,2020, arXiv preprint arXiv:2008
 Adam: A method for stochastic optimization,2015, In Proceedingsof the 3rd International Conference on Learning Representations (ICLR)
 Decentralized stochastic optimizationand gossip algorithms with compressed communication,2019, In Proceedings of the 36th InternationalConference on Machine Learning (ICML)
 Learning multiple layers of features from tiny images,2009, Master’sthesis
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 End-to-end training of deep visuo-motor policies,2016, J
 Deep gradient compression: Reducingthe communication bandwidth for distributed training,2018, In Proceedings of the 6th InternationalConference on Learning Representations (ICLR)
 GNSD: a gradient-tracking basednonconvex stochastic algorithm for decentralized optimization,2019, In Proceedings of the 2019 IEEEData Science Workshop (DSW)
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingofthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)
 Playing atari with deep reinforcement learning,2013, arXiv preprintarXiv:1312
 Dadam: A consensus-baseddistributed adaptive gradient method for online optimization,2019, arXiv preprint arXiv:1901
 Distributed subgradient methods for multi-agent optimiza-tion,2009, IEEE Trans
 Robust stochasticapproximation approach to stochastic programming,2009, SIAM J
 On the convergence of adam and beyond,2018, InProceedings ofthe 6th International Conference on Learning Representations (ICLR)
 Adaptive federated optimization,2021, In Proceedingsof the 9th International Conference on Learning Representations (ICLR)
 Understanding and op-timizing asynchronous low-precision stochastic gradient descent,2017, In Proceedings of the 44th An-nual International Symposium on Computer Architecture (ISCA)
 1-bit stochastic gradient descent andits application to data-parallel distributed training of speech dnns,2014, In Proceedings of the 15thAnnual Conference of the International Speech Communication Association (ISCA)
 Towards more efficientstochastic decentralized learning: Faster convergence and sparse communication,2018, In Proceed-ings of the 35th International Conference on Machine Learning (ICML)
 A convergence anal-ysis of distributed SGD with communication-efficient gradient sparsification,2019, In Proceedings ofthe 28th International Joint Conference on Artificial Intelligence (IJCAI)
 Master-ing the game of go without human knowledge,2017, Nat
 The error-feedback framework: Better rates forsgd with delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 1-bit adam: Communication efficient large-scale trainingwith adam’s convergence speed,2021, In Proceedings of the 38th International Conference on MachineLearning (ICML)
 An optimistic acceleration of amsgradfor nonconvex optimization,2021, In Proceedings of Asian Conference on Machine Learning (ACML)
 Gradient sparsification for communication-efficient distributed optimization,2018, In Advances in Neural Information Processing Systems(NeurIPS)
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems (NIPS)
 Error compensated quantized SGDand its applications to large-scale distributed optimization,2018, In Proceedings of the 35th Interna-tional Conference on Machine Learning (ICML)
 Agileand accurate CTR prediction model training for massive-scale online advertising systems,2021, InProceedings ofthe International Conference on Management ofData (SIGMOD)
 SWALP : Stochastic weight averaging in low precision training,2019, In Proceedings ofthe 36th International Conference on Machine Learning (ICML)
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2020, In Proceedings of the 8th International Conferenceon Learning Representations (ICLR)
 Recent trends in deep learn-ing based natural language processing,2018, IEEE Comput
 On the linear speedup analysis of communication efficient mo-mentum SGD for distributed non-convex optimization,2019, In Proceedings of the 36th InternationalConference on Machine Learning (ICML)
 Exploring fast and communication-efficient algorithmsin large-scale distributed networks,2019, In Proceedings of the 22nd International Conference on Arti-ficial Intelligence and Statistics (AISTATS)
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Deep learning for sentiment analysis: A survey,2018, WileyInterdiscip
 Revisiting few-sample BERT fine-tuning,2021, In Proceedings of the 9th International Conference on Learning Rep-resentations (ICLR)
 AIBox: CTR pre-diction model training on a single node,2019, In Proceedings of the 28th ACM International Conferenceon Information and Knowledge Management (CIKM)
 Communication-efficient terabyte-scale model training framework for online advertising,2022, arXiv preprintarXiv:2201
 Communication-efficient distributed blockwisemomentum SGD with error-feedback,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Onthe convergence of adaptive gradient methods for nonconvex optimization,2018, arXiv preprintarXiv:1808
 Towards better generalizationof adaptive gradient methods,2020, In Advances in Neural Information Processing Systems (NeurIPS)
