title,year,conference
 Revisiting resnets: Improved training and scaling strategies,2021, arXivpreprint arXiv:2103
 All you need is a few shifts: Designing efficientconvolutional neural networks for image classification,2019, In CVPR
 Progressive differentiable architecture search: Bridgingthe depth gap between search and evaluation,2019, In ICCV
 Fair darts: Eliminating unfair advantagesin differentiable architecture search,2020, In ECCV
 Randaugment: Practical data aug-mentation with no separate search,2019, arXiv preprint arXiv:1909
 Deformableconvolutional networks,2017, In ICCV
 An imageis worth 16x16 words: Transformers for image recognition at scale,2021, In ICLR
OP-timizing grouped convolutions on edge devices,2020, In ASAP
 Highway and residual networks learnunrolled iterative estimation,2017, In ICLR
 Rethinking channel dimensionsfor efficient model design,2021, In CVPR
 Ghostnet: Morefeatures from cheap operations,2020, In CVPR
 Identity mappings in deep residualnetworks,2016, In ECCV
 Deep residual learning for image recog-nition,2016, In CVPR
 Addressnet: Shift-based primitives forefficient convolutional neural networks,2019, In WACV
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Natural adversarialexamples,2021, In CVPR
 Adamp: Slowing down the slowdown for momentum optimiz-ers on scale-invariant weights,2021, In ICLR
 In defense of the triplet loss for person re-identification,2017, arXiv preprint arXiv:1703
 Searching for mobilenetv3,2019, In ICCV
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Squeeze-and-excitation networks,2017, In arXiv:1709
 Deep networks with stochas-tic depth,2016, In ECCV
 Densely connected convolutional networks,2017, InCVPR
 Data-driven sparse structure selection for deep neural networks,2018, InECCV
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Residual connections encourage iterative inference,2018, In ICLR
 NSML: Meet the MLaaS platformWith a real-World case study,2018, arXiv preprint arXiv:1810
 Learning multiple layers of features from tiny images,2009, In Tech Report
 Darts+: Improved differentiable architecture search With early stopping,2019, arXivpreprint arXiv:1909
 Microsoft coco: Common objects in context,2014, In ECCV
 Darts: Differentiable architecture search,2019, InICLR
 Sgdr: Stochastic gradient descent with warm restarts,2017, In ICLR
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Optimizing depthwise separable convolution oper-ations on gpus,2021, IEEE Transactions on Parallel and Distributed Systems
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In ICCV
 Understanding the effective receptivefield in deep convolutional neural networks,2016, In NeurIPS
 Slimconv: Reducingchannel redundancy in convolutional neural networks by features recombining,2021, IEEE Transac-tions on Image Processing
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In NeurIPS
 Imagenetlarge scale visual recognition challenge,2015, International Journal of Computer Vision
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In ICCV
 Towards flatterloss surface via nonmonotonic learning rate scheduling,2018, In UAI
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Going deeper with convolutions,2015, InCVPR
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Efficientnetv2: Smaller models and faster training,2021, In ICML
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In NeurIPS
 Training data-effiCient image transformers & distillation through attention,2021, InICML
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Residual networks behave like ensembles ofrelatively shallow networks,2016, In NeurIPS
 Learning versatile filters forefficient convolutional neural networks,2018, NeurIPS
 Aggregated residual trans-formations for deep neural networks,2017, In CVPR
 Wide residual networks,2016, In BMVC
 Shufflenet: An extremely efficient con-volutional neural network for mobile devices,2018, In CVPR
 The landscape of deep learning algorithms,2017, arXiv preprintarXiv:1705
