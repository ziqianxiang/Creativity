title,year,conference
 An estimator for the diagonal of a matrix,2007, AppliedNumericalMathematics
 A multi-batch l-bfgs method for machinelearning,2016, In Advances in Neural Information Processing Systems
 An investigation of Newton-Sketch andsubsampled Newton methods,2020, Optimization Methods and Software
 Exact and inexact subsampled newtonmethods for optimization,2019, IMA Journal of Numerical Analysis
 Optimization methods for large-scale machinelearning,2018, Siam Review
 On the use of stochastic hessianinformation in optimization methods for machine learning,2011, SIAM Journal on Optimization
 Gradient descent: Theultimate optimizer,2019, arXiv preprint arXiv:1909
 Entropy-sgd: Biasing gradient descentinto wide valleys,2019, Journal of Statistical Mechanics: Theory and Experiment
 A self-correcting variable-metric algorithm for stochastic optimization,2016, In Interna-tional Conference on Machine Learning
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of machine learning research
 Practical Methods of Optimization,1987, John Wiley & Sons
 Sgd: General analysis and improved rates,2019, In International Conference on MachineLearning
 Deep residual learning for imagerecognition,2015, CoRR
 Efficient distributed hessian free algorithm for large-scale empirical risk minimiza-tion via accumulating sample strategy,2020, In International Conference on Artificial Intelligence andStatistics
 Scalingup quasi-newton algorithms: Communication efficient distributed sr1,2020, In International Conferenceon Machine Learning
 Sonia:A symmetric blockwise truncated optimization algorithm,2021, In International Conference on ArtificialIntelligence and Statistics
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence,2020, arXiv preprint arXiv:2002
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Adaptive gradient descent without descent,2020, In Interna-tional Conference on Machine Learning
 Deep learning via hessian-free optimization,2010, In ICML
 Adaptive bound optimization for online convex opti-mization,2010, Proceedings of the 23rd Annual Conference on Learning Theory (COLT)
 Global convergence of online limited memory bfgs,2015, TheJournal of Machine Learning Research
 SARAH: a novel method for machine learningproblems using stochastic recursive gradient,2017, In Advances in neural information processingsystems
 Sgdand hogwild! convergence without the bounded gradients assumption,2018, In InternationalConference on Machine Learning
 New convergence aspects of stochastic gradient algorithms,2019, J
 Numerical Optimization,2006, Springer Series in OperationsResearch
 A newton-based method for nonconvexoptimization with fast evasion of saddle points,2019, SIAM Journal on Optimization
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Newton-MR: Newton¡¯s method without smoothness orconvexity,2018, Technical Report Preprint: arXiv:1810
 Sub-sampled newton methods,2018, MathematicalProgramming
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Lecture 6,2012,5-rmsprop: Divide the gradient by a runningaverage of its recent magnitude
 Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Newton-type methods for non-convex optimizationunder inexact Hessian information,2017, Technical Report Preprint: arXiv:1708
 Second-order optimization for non-convex machinelearning: An empirical study,2020, In Proceedings of the 2020 SIAM International Conference on DataMining
 PyHessian: Neural networks through the lensof the Hessian,2019, Technical Report Preprint: arXiv:1912
 Adahessian: Anadaptive second order optimizer for machine learning,2020, arXiv preprint arXiv:2006
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
