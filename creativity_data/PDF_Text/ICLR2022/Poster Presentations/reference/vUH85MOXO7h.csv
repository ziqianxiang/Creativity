title,year,conference
 The Recurrent NeuralTangent Kernel,2021, In International Conference on Learning Representations
 TabNet: Attentive Interpretable Tabular Learning,2019, CoRR
 OnExact Computation with an Infinitely Wide Neural Net,2019, In Advances in Neural InformationProcessing Systems
 Har-nessing the Power of Infinitely Wide Deep Nets on Small-data Tasks,2020, In International Conferenceon Learning Representations
 Rademacher and Gaussian Complexities: Risk Bounds andStructural Results,2003, Journal of Machine Learning Research
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings ofthe National Academy ofSciences
 Classification and RegressionTrees,1984, Chapman and Hall/CRC
 Neural Architecture Search on ImageNet inFour GPU Hours: A Theoretically Inspired Perspective,2021, In International Conference on LearningRepresentations
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in Neural Information Processing Systems
 Gradient Descent Finds GlobalMinima of Deep Neural Networks,2019, In Proceedings of the 36th International Conference onMachine Learning
 Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels,2019, InAdvances in Neural Information Processing Systems
 The TreeEnsemble Layer: Differentiability meets Conditional Computation,2020, In Proceedings of the 37thInternational Conference on Machine Learning
 Deep Residual Learning for ImageRecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Why Do Deep Residual NetWorksGeneralize Better than Deep FeedforWard NetWorks? ¡ª A Neural Tangent Kernel Perspective,2020, InAdvances in Neural Information Processing Systems
 Hierarchical mixtures of experts and the EM algorithm,1993, In Proceedingsof 1993 International Conference on Neural Networks
 Learning AccurateDecision Trees With Bandit Feedback via Quantized Gradient Descent,2021, CoRR
 DeepGBM: A Deep LearningFrameWork Distilled by GBDT for Online Prediction Tasks,2019, In Proceedings of the 25th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining
 Deep NeuralDecision Forests,2015, In 2015 IEEE International Conference on Computer Vision
 Wide Neural NetWorks of Any Depth Evolve as Linear ModelsUnder Gradient Descent,2019, In Advances in Neural Information Processing Systems
 GShard: Scaling Giant Models With ConditionalComputation and Automatic Sharding,2021, In International Conference on Learning Representations
 Enhanced Convolutional Neural Tangent Kernels,2019, CoRR
 SDTR: Soft Decision Tree Regressor for TabularData,2021, IEEE Access
 From Softmax to Sparsemax: A Sparse Model of Attentionand Multi-Label Classification,2016, In Proceedings of The 33rd International Conference on MachineLearning
 Sparse Sequence-to-Sequence Models,2019, InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics
 Neural Oblivious Decision Ensembles forDeep Learning on Tabular Data,2020, In International Conference on Learning Representations
 CatBoost: unbiased boosting with categorical features,2018, In Advances in Neural InformationProcessing Systems
 Induction of Decision Trees,1986, Machine Learning
 Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-ExpertsLayer,2017, In International Conference on Learning Representations
 Adap-tive Neural Trees,2019, In Proceedings of the 36th International Conference on Machine Learning
 NBDT: Neural-Backed Decision Tree,2021, In International Conference onLearning Representations
 Computing with Infinite Networks,1996, In Advances in Neural InformationProcessing Systems
