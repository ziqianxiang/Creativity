title,year,conference
 Choose a transformer: Fourier or galerkin,2021, arXiv preprint arXiv:2105
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 The cityscapes dataset for semantic urbanscene understanding,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Multiwavelet-based operator learning for differen-tial equations,2021, arXiv preprint arXiv:2109
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In International Conference on Ma-chine Learning
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 3d object representations for fine-grainedcategorization,2013, In 4th International IEEE Workshop on 3D Representation and Recognition(3dRR-13)
 Fnet: Mixing tokens withfourier transforms,2021, arXiv preprint arXiv:2105
 Fourier neural operator for parametric partial differentialequations,2020, arXiv preprint arXiv:2010
 Neural operator: Graph kernel network for partial differ-ential equations,2020, arXiv preprint arXiv:2003
 As-mlp: An axial shifted mlp architecturefor vision,2021, arXiv preprint arXiv:2107
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Deeponet: Learning nonlinear operators for iden-tifying differential equations based on the universal approximation theorem of operators,2019, arXivpreprint arXiv:1910
 Image transformer,2018, In International Conference on Machine Learning
 Global filter networks forimage classification,2021, arXiv preprint arXiv:2107
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguis-tics
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Continuous and discrete signals and systems,1990, EnglewoodCliffs
 Sparse sinkhorn attention,2020, InInternational Conference on Machine Learning
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Regression shrinkage and selection via the lasso,1996, Journal of the Royal StatisticalSociety: Series B (Methodological)
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Resmlp: Feedforwardnetworks for image classification with data-efficient training,2021, arXiv preprint arXiv:2105
 Transformer dissection: A unified understanding of transformerâ€™s attention viathe lens of kernel,2019, arXiv preprint arXiv:1908
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 U-fno-an enhanced fourier neural operator based-deep learning model for multiphase flow,2021, arXivpreprint arXiv:2109
 Seg-former: Simple and efficient design for semantic segmentation with transformers,2021, arXiv preprintarXiv:2105
 Lsun: Construction ofa large-scale image dataset using deep learning with humans in the loop,2015, arXiv preprintarXiv:1506
