title,year,conference
 Structured prediction energy networks,2016, In International Conferenceon Machine Learning
 Language models are few-shot learners,2020, arXiv preprintarXiv:2005
 Pre-training transformers as energy-based clozemodels,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-cessing (EMNLP)
 Electra: Pre-training text encoders asdiscriminators rather than generators,2020, arXiv preprint arXiv:2003
 Residual energy-based models for textgeneration,2020, arXiv preprint arXiv:2004
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2018, arXiv preprint arXiv:1810
 Sparse graphical models forexploring gene expression data,2004, Journal of Multivariate Analysis
 Implicit generation and modeling with energy based models,2019, 2019
 Gibbs sampling,2000, Journal of the American statistical Association
 Sampling-based approaches to calculating marginal densities,1990, Journalof the American statistical association
 Mask-predict: Parallel decoding of condi-tional masked language models,2019, In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing
 An empirical investigation of global and local normal-ization for recurrent neural sequence models using a continuous relaxation to beam search,2019, InProceedings of 2019 Annual Conference of the North American Chapter of the Association forComputational Linguistics
 Oops I took a gradient:Scalable sampling for discrete distributions,2021, arXiv preprint arXiv:2102
 Monte carlo sampling methods using markov chains and their applications,1970, 1970
 Roberta: A robustly optimized bert pretraining approach,2019, arXiv preprint arXiv:1907
 Closed-form learning of markov networks from dependency networks,2012, arXiv preprintarXiv:1210
 A generalized framework of sequence generationwith application to undirected sequence models,2019, arXiv preprint arXiv:1905
 Deepcontextualized word representations,2018, arXiv preprint arXiv:1802
 Language models areunsupervised multitask learners,2019, OpenAI blog
 A primer in Bertology: What we know about how BERTworks,2021, Transactions ofthe Association for Computational Linguistics
 Sequence to sequence learning with neural networks,2014, arXivpreprint arXiv:1409
 Engine: Energy-based inference networks fornon-autoregressive machine translation,2020, arXiv preprint arXiv:2005
 Language modeling with neural trans-dimensional random fields,2017, In 2017 IEEEAutomatic Speech Recognition and Understanding Workshop (ASRU)
 Learning neural trans-dimensional random field language models with noise-contrastive estimation,2018, In 2018 IEEE International Conference on Acoustics
 Sequence-to-sequence learning as beam-search optimization,2016, arXivpreprint arXiv:1606
 Xlnet: Generalizedautoregressive pretraining for language understanding,2019, arXiv preprint arXiv:1906
 Bertscore: Evaluating text generationwith BERT,2019, arXiv preprint arXiv:1904
 Adversarial feature matchingfor text generation,2017, In International Conference on Machine Learning
 Energy-based generative adversarial network,2016, arXiv preprintarXiv:1609
