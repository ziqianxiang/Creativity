title,year,conference
 Backward feature correction: How deep learning performsdeep learning,2020, arXiv preprint arXiv:2001
 A convergence theory for deep learning viaover-parameterization,2019, In International Conference on Machine Learning
 A mean-field limit for certain deepneural networks,2019, arXiv preprint arXiv:1906
 Fine-grained analysisof optimization and generalization for overparameterized two-layer neural networks,2019, arXivpreprint arXiv:1901
 Breaking the curse of dimensionality with convex neural networks,2017, The Journalof Machine Learning Research
 Beyond linearization: On quadratic and higher-order approximationof wide neural networks,2020, In International Conference on Learning Representations
 Gradient descent with identity initializa-tion efficiently learns positive definite linear transformations by deep residual networks,2018, InInternational conference on machine learning
 Finite-sample analysis of interpolating linear classifiersin the overparameterized regime,2020, arXiv preprint arXiv:2004
 Towards understanding hierarchical learning: Benefits of neural representations,2020, arXivpreprint arXiv:2006
 A dynamical centrallimit theorem for two-layer neural networks,2020, Advances in Neural Information ProcessingSystems
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in Neural Information ProcessingSystems
 Implicit bias of gradient descent for wide two-layer neuralnetworks trained with the logistic loss,2020, arXiv preprint arXiv:2002
 On lazy training in differentiable program-ming,2019, In Advances in Neural Information Processing Systems
 Gradient descent findsglobal minima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov
 Gradient descent findsglobal minima of deep neUral networks,2018, arXiv preprint arXiv:1811
 Asymptotics of wide networks from feynman diagrams,2020, InInternational Conference on Learning Representations
 Modeling from featUres: a mean-fieldframework for over-parameterized deep neUral networks,2020, arXiv preprint arXiv:2007
 Topology and geometry of half-rectified network optimiza-tion,2016, arXiv preprint arXiv:1611
 Landscape and training regimes in deeplearning,2021, Physics Reports
 Disentangling featUre andlazy learning in deep neUral networks: an empirical stUdy,2019, arXiv preprint arXiv:1906
 Limitations oflazy training of two-layers neUral network,2019, In Advances in Neural Information ProcessingSystems
 Finite depth and width corrections to the neUral tangent kernel,2020, InInternational Conference on Learning Representations
 Mean-field langevin dynamicsand energy landscape of neUral networks,2019, arXiv preprint arXiv:1905
 Mean-field neural odes Via relaxedoptimal control,2019, arXiv preprint arXiv:1912
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
 Analysis of a two-layer neuralnetwork via displacement convexity,2020, The Annals of Statistics
 Deep learning without poor local minima,2016, arXiv preprint arXiv:1605
 Deep neural networks as gaussian processes,2017, arXiv preprintarXiv:1711
 Finite versus infinite neural networks: an empirical study,2020, InHugo Larochelle
 Learning overparameterized neural networks via stochasticgradient descent on structured data,2018, arXiv preprint arXiv:1808
 Learning over-parametrized two-layerneural networks beyond ntk,2020, In Jacob Abernethy and Shivani Agarwal
 Loss landscapes and optimization in over-parameterized non-linear systems and neural networks,2020, arXiv preprint arXiv:2003
 A topological property of real analytic subsets,1963, Coll
 A mean-field analysis of deepresnet and beyond: Towards provable optimization via overparameterization from depth,2020, arXivpreprint arXiv:2003
 Phase diagram for two-layer reluneural networks at infinite-width limit,2021, Journal of Machine Learning Research
 The barron space and the flow-induced function spaces for neuralnetwork models,2021, Constructive Approximation
 Mean-field theory of two-layersneural networks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 A mean field view of the landscape oftwo-layer neural networks,2018, Proceedings of the National Academy of Sciences
 A rigorous framework for the mean field limit ofmultilayer neural networks,2020, arXiv preprint arXiv:2001
 Particle dual averaging: Optimization of meanfield neural networks with global convergence rate analysis,2020, arXiv preprint arXiv:2012
 Toward moderate overparameterization: Globalconvergence guarantees for training shallow neural networks,2020, IEEE Journal on Selected Areasin Information Theory
 Global convergence of three-layer neural networks inthe mean field regime,2021, ICLR
 Gradient methods for the minimisation of functionals,1963, Ussr ComputationalMathematics and Mathematical Physics
 Parameters as interacting particles: long timeconvergence and asymptotic error scaling of neural networks,2018, In Advances in Neural InformationProcessing Systems
 Spurious local minima are common in two-layer ReLU neuralnetworks,2018, In International Conference on Machine Learning
 Optimization and gen-eralization of shallow neural networks with quadratic activation functions,2020, In H
 Mean field analysis of neural networks: A lawof large numbers,2020, SIAM Journal on Applied Mathematics
 Theoretical insights into the op-timization landscape of over-parameterized shallow neural networks,2018, IEEE Transactions onInformation Theory
 No bad local minima: Data independent training errorguarantees for multilayer neural networks,2016, arXiv preprint arXiv:1605
 An analytical formula of population gradient for two-layered ReLU networkand its applications in convergence and critical point analysis,2017, In Doina Precup and Yee WhyeTeh
 Spurious valleys in one-hidden-layer neuralnetwork optimization landscapes,2019, Journal of Machine Learning Research
 High-Dimensional Probability: An Introduction with Applications in DataScience,2018, Cambridge Series in Statistical and Probabilistic Mathematics
 Regularization matters: Generalizationand optimization of neural nets vs their induced kernel,2019, In Advances in Neural InformationProcessing Systems
 On the convergence of gradient descent training for two-layer relu-networks in the mean field regime,2020, arXiv preprint arXiv:2005
 Can shallow neural networks beat the curse of dimen-sionality? a mean field training perspective,2020, IEEE Transactions on Artificial Intelligence
 Tensor programs iv: Feature learning in infinite-width neuralnetworks,2021, In Marina Meila and Tong Zhang
 A local convergence theory for mildly over-parameterizedtwo-layer neural network,2021, arXiv preprint arXiv:2102
 The global optimizationgeometry of shallow linear neural networks,2020, Journal of Mathematical Imaging and Vision
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
