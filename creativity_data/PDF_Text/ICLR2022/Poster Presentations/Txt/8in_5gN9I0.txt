Published as a conference paper at ICLR 2022
Triangle and Four Cycle Counting with Pre-
dictions in Graph Streams
Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, and Sandeep SilWal *
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{justc, indyk, shyamsn, ronitt, silwal}@mit.edu
Honghao Lin, David P. Woodruff, Michael Zhang
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{honghaol, dwoodruf, jinyaoz}@andrew.cmu.edu
Tal Wagner
Microsoft Research
Redmond, WA 98052, USA
tal.wagner@gmail.com
Talya Eden
Massachusetts Institute of Technology and Boston University
Cambridge, MA 02139, USA
teden@mit.edu
Ab stract
We propose data-driven one-pass streaming algorithms for estimating the number
of triangles and four cycles, two fundamental problems in graph analytics that are
widely studied in the graph data stream literature. Recently, Hsu et al. (2019a)
and Jiang et al. (2020) applied machine learning techniques in other data stream
problems, using a trained oracle that can predict certain properties of the stream
elements to improve on prior “classical” algorithms that did not use oracles. In
this paper, we explore the power of a “heavy edge” oracle in multiple graph edge
streaming models. In the adjacency list model, we present a one-pass triangle
counting algorithm improving upon the previous space upper bounds without such
an oracle. In the arbitrary order model, we present algorithms for both triangle and
four cycle estimation with fewer passes and the same space complexity as in previ-
ous algorithms, and we show several of these bounds are optimal. We analyze our
algorithms under several noise models, showing that the algorithms perform well
even when the oracle errs. Our methodology expands upon prior work on “clas-
sical” streaming algorithms, as previous multi-pass and random order streaming
algorithms can be seen as special cases of our algorithms, where the first pass or
random order was used to implement the heavy edge oracle. Lastly, our experi-
ments demonstrate advantages of the proposed method compared to state-of-the-
art streaming algorithms.
1	Introduction
Counting the number of cycles in a graph is a fundamental problem in the graph stream model
(e.g., Atserias et al. (2008); Bera & Chakrabarti (2017); Seshadhri et al. (2013); Kolountzakis et al.
(2010); Bar-Yossef et al. (2002); Kallaugher et al. (2019)). The special case of counting triangles
is widely studied, as it has a vast range of applications. In particular, it provides important insights
into the structural properties of networks (Prat-Perez et al., 2012; Farkas et al., 2011), and is used
to discover motifs in protein interaction networks (Milo et al., 2002), understand social networks
(Foucault Welles et al., 2010), and evaluate large graph models (Leskovec et al., 2008). See Al Hasan
& Dave (2018) for a survey of these and other applications.
* All authors contributed equally.
1
Published as a conference paper at ICLR 2022
Because of its importance, a large body of research has been devoted to space-efficient streaming
algorithms for (1 + )-approximate triangle counting. Such algorithms perform computation in one
or few passes over the data using only a sub-linear amount of space. A common difficulty which
arises in all previous works is the existence of heavy edges, i.e., edges that are incident to many
triangles (four cycles). As sublinear space algorithms often rely on sampling of edges, and since a
single heavy edge can greatly affect the number of triangles (four cycles) in a graph, sampling and
storing these edges are often the key to an accurate estimation. Therefore, multiple techniques have
been developed to determine whether a given edge is heavy or not.
Recently, based on the observation that many underlying patterns in real-world data sets do not
change quickly over time, machine learning techniques have been incorporated into the data stream
model via the training of heavy-hitter oracles. Given access to such a learning-based oracle, a wide
range of significant problems in data stream processing — including frequency estimation, esti-
mating the number of distinct elements, Fp-Moments or (k, p)-Cascaded Norms — can all achieve
space bounds that are better than those provided by “classical” algorithms, see, e.g., Hsu et al.
(2019a); Cohen et al. (2020); Jiang et al. (2020); Eden et al. (2021); Du et al. (2021). More gen-
erally, learning-based approaches have had wide success in other algorithmic tasks, such as data
structures (Kraska et al., 2018; Ferragina et al., 2020; Mitzenmacher, 2018; Rae et al., 2019; Vaidya
et al., 2021), online algorithms (Lykouris & Vassilvtiskii, 2018; Purohit et al., 2018; Gollapudi &
Panigrahi, 2019; Rohatgi, 2020; Wei, 2020; Mitzenmacher, 2020; Lattanzi et al., 2020; Bamas et al.,
2020), similarity search (Wang et al., 2016; Dong et al., 2020) and combinatorial optimization (Dai
et al., 2017; Balcan et al., 2017; 2018a;b; 2019). See the survey and references therein for additional
works (Mitzenmacher & Vassilvitskii, 2020).
Inspired by these recent advancements, we ask: is it possible to utilize a learned heavy edge oracle
to improve the space complexity of subgraph counting in the graph stream model? Our results
demonstrate that the answer is yes.
1.1	Our Results and Comparison to Previous Theoretical Works
We present theoretical and empirical results in several graph streaming models, and with several
notions of prediction oracles. Conceptually, it is useful to begin by studying perfect oracles that
provide exact predictions. While instructive theoretically, such oracles are typically not available in
practice. We then extend our theoretical results to noisy oracles that can provide inaccurate or wrong
predictions. We validate the practicality of such oracles in two ways: by directly showing they can
be constructed for multiple real datasets, and by showing that on those datasets, our algorithms attain
significant empirical improvements over baselines, when given access to these oracles.
We proceed to a precise account of our results. Let G = (V, E) denote the input graph, and let n,
m, and T denote the number of vertices, edges, and triangles (or four-cycles) in G, respectively.
There are two major graph edge streaming models: the adjacency list model and the arbitrary order
model. We show that training heavy edge oracles is possible in practice in both models, and that such
oracles make it possible to design new algorithms that significantly improve the space complexity of
triangle and four-cycle counting, both in theory and in practice. Furthermore, our formalization of
a heavy edge prediction framework makes it possible to show provable lower bounds as well. Our
results are summarized in Table 1.
In our algorithms, we assume that we know a large-constant approximation of T for the purposes of
setting various parameters. This is standard practice in the subgraph counting streaming literature
(e.g., see (Braverman et al., 2013, Section 1), (McGregor et al., 2016, Section 1.2) for an extensive
discussions on this assumption). Moreover, when this assumption cannot be directly carried over in
practice, in Subsection F.3 we discuss how to adapt our algorithms to overcome this issue.
1.1.1	Perfect Oracle
Our first results apply for the case that the algorithms are given access to a perfect heavy edge oracle.
That is, for some threshold ρ, the oracle perfectly predicts whether or not a given edge is incident to
at least ρ triangles (four cycles). We describe how to relax this assumption in Section 1.1.2.
Adjacency List Model All edges incident to the same node arrive together. We show:
2
Published as a conference paper at ICLR 2022
Table 1: Our results compared to existing theoretical algorithms. ∆E (∆V ) denotes the maximum
number of triangles incident to any edge (vertex), and κ denotes the arboricity of the graph.
Problem	Previous Results (no oracle)	Our Results
Triangle, Adjacency	O(e-2m∕√T), 1-pass (McGregor et al., 2016)	O(min(e-2m2/3/T1/3, e-1m1/2)), 1-pass
Triangle, Arbitrary	O(e-2m3/2/T), 3-pass (McGregor et al., 2016)	O(e-1(m/√T +m1/2)), 1-pass
	O(e-2m∕√T), 2-pass (McGregor et al., 2016)	
	O(e-2(m∕√T + m∆E/T)), 1-pass (Pagh & Tsourakakis, 2012)	
	O(e-2(m∕T2/3 + m∆E/T + m√∆v/T)), 2-pass (Kallaugher & Price, 2017)	
	O(Poly(e-1)(m∆E/T + m√∆v/T)), 1-pass (Jayaram & Kallaugher, 2021)	
	O(Poly(e-1)mK/T), multi-pass (Bera & Sheshadhri, 2020)	
4-cycle, Arbitrary	O(e-2m/T1/4), 3-pass (McGregor & Vorotnikova, 2020)	O(T1/3 + e-2m/T1/3), 1-pass
	O(e-2m/T1/3), 3-pass (Vorotnikova, 2020)	
Theorem 1.1.	There exists a one-pass algorithm, Algorithm 1, with space complexity1
Oe(min(-2m2/3/T 1/3, -1m1/2)) in the adjacency list model that, using a learning-based ora-
cle, returns a (1 ± )-approximation to the number T of triangles with probability at least2 7/10.
An overview of Algorithm 1 is given in Section 2, and the full analysis is provided in Appendix B.
Arbitrary Order Model In this model, the edges arrive in the stream in an arbitrary order. We
present a one-pass algorithm for triangle counting and another one-pass algorithm for four cycle
counting in this model, both reducing the number of passes compared to the currently best known
space complexity algorithms. Our next result is as follows:
Theorem 1.2.	There exists a one-pass algorithm, Algorithm 4, with space complexity
O(e-1(m∕√T + √m)) in the arbitrary order model that, using a learning-based oracle, returns a
(1 ± )-approximation to the number T of triangles with probability at least 7/10.
An overview of Algorithm 4 is given in Section 3, and full details are provided in Appendix C.
We also show non-trivial space lower bounds that hold even if appropriate predictors are available.
In Theorem C.2 in Appendix C.3, We provide a lower bound for this setting by giving a construction
that requires Ω(min(m/√T, m3/2/T)) space even with the help of an oracle, proving that our result
is nearly tight in some regimes. Therefore, the triangle counting problem remains non-trivial even
when extra information is available.
Four Cycle Counting. For four cycle counting in the arbitrary order model, we give Theorem 1.3
which is proven in Appendix D.
Theorem 1.3.	There exists a one-pass algorithm, Algorithm 5, with space complexity O (T1/3 +
e-2m/T1/3) in the arbitrary order model that, using a learning-based oracle, returns a (1 ± e)-
approximation to the number T of four cycles with probability at least 7/10.
To summarize our theoretical contributions, for the first set of results of counting triangles in the
adjacency list model, our bounds always improve on the previous state of the art due to McGregor
et al. (2016) for all values of m and T. For a concrete example, consider the case that T = Θ(√m).
1We use O(f) to denote O(f ∙ polylog(f)).
2The success probability can be 1 一 δ by running log(1∕δ) copies of the algorithm and taking the median.
3
Published as a conference paper at ICLR 2022
In this setting previous bounds result in an Oe(m3/4)-space algorithm, while our algorithm only
requires O(√m) space (for constant e).
For the other two problems of counting triangles and 4-cycles in the arbitrary arrival model, our
space bounds have an additional additive term compared to McGregor et al. (2016) (for triangles)
and Vorotnikova (2020) (for 4-cycles) but importantly run in a single pass rather than multiple
passes. In the case where the input graph has high triangles density, T = Ω(m∕e2), our space
bound is worse due to the additive factor. When T = O(m/e2), our results achieve the same
dependence on m and T as that of the previous algorithms with an improved dependency in e.
Moreover, the case T ≤ m/e2 is natural for many real world datasets: for e = 0.05, this condition
holds for all of the datasets in our experimental results (see Table 2). Regardless of the triangle
density, a key benefit of our results is that they are achieved in a single pass rather than multiple
passes. Finally, our results are for general graphs, and make no assumptions on the input graph
(unlike Pagh & Tsourakakis (2012); Kallaugher & Price (2017); Bera & Sheshadhri (2020)). Most
of our algorithms are relatively simple and easy to implement and deploy. At the same time, some of
our results require the use of novel techniques in this context, such as the use of exponential random
variables (see Section 2).
1.1.2 Noisy Oracles
The aforementioned triangle counting results are stated under the assumption that the algorithms are
given access to perfect heavy edge oracles. In practice, this assumption is sometimes unrealistic.
Hence, we consider several types of noisy oracles. The first such oracle, which we refer to as a
K-noisy oracle, is defined below (see Figure 3 in the Supplementary Section C.2).
Definition 1.1. For an edge e = xy in the stream, define Ne as the number of triangles that contain
both x and y. For a fixed constant K ≥ 1 and for a threshold ρ we say that an oracle Oρ is a
K-noisy oracle if for every edge e, 1 一 K ∙ N ≤ Pr[Oρ(e) = HEAVY] ≤ K ∙ N.
This oracle ensures that if an edge is extremely heavy or extremely light, it is classified correctly
with high probability, but if the edge is close to the threshold, the oracle may be inaccurate. We
further discuss the properties of this oracle in Section G.
For this oracle, we prove the following two theorems. First, in the adjacency list model, we prove:
Theorem 1.4. Suppose that the oracle given to Algorithm 1 is a K-noisy oracle as defined in
Definition 1.1. Then with probability 2/3, Algorithm 1 returns a value in (1 ± √K ∙ e)T, and
uses space at most O(min(e-2m2/3/T1/3, K ∙ e-1m1/2)).
Hence, even if our oracle is inaccurate for edges near the threshold, our algorithm still obtains an
effective approximation with low space in the adjacency list model. Likewise, for the arbitrary order
model, we prove in Theorem C.1 that the O(e-1(m∕√T + √m)) 1-pass algorithm of Theorem 1.2
also works when Algorithm 4 is only given access to a K-noisy oracle.
The proof of Theorem 1.4 is provided in Appendix B.1, and the proof of Theorem C.1 is provided
in Appendix C.2. We remark that Theorems 1.4 and C.1 automatically imply Theorems 1.1 and 1.2,
since the perfect oracle is automatically a K-noisy oracle.
(Noisy) Value Oracles In the adjacency list model, when we see an edge xy, we also have access
to all the neighbors of either x or y, which makes it possible for the oracle to give a more accurate
prediction. For an edge xy, let Rxy denote the number of triangles {x, z, y} so that x precedes z
and z precedes y in the stream arrival order. Formally, Rxy = |z : {x, y, z} ∈ ∆ and x <s z <s y|
where x <s y denotes that the adjacency list of x arrives before that of y in the stream.
Motivated by our empirical results in Section F.7, it is reasonable in some settings to assume we
have access to oracles that can predict a good approximation to Rxy . We refer to such oracles as
value oracles.
In the first version of this oracle, we assume that the probability of approximation error decays
linearly with the error from above but exponentially with the error from below.
Definition 1.2. Given an edge e, an (α, β) value-prediction oracle outputs a random value p(e)
where E[p(e)] ≤ αRe + β, and Pr[p(e) < R — β] ≤ Kef for some constant K and any λ ≥ 1.
4
Published as a conference paper at ICLR 2022
For this variant, we prove the following theorem.
Theorem 1.5. Given an oracle with parameters (α, β), there exists a one-pass algorithm, Algo-
rithm 2, with space complexity O(e-2 log2(K∕e)(α + mβ∕T)) in the adjacency list model that
returns a (1 ± )-approximation to the number of triangles T with probability at least 7/10.
In the second version of this noisy oracle, we assume that the probability of approximation error
decays linearly with the error from both above and below. For this variant, we prove that we can
achieve the same guarantees as Theorem 1.5 up to logarithmic factors (see Theorem B.1). The
algorithms and proofs for both Theorem 1.5 and Theorem B.1 appear in Appendix B.2.
Experiments We conduct experiments to verify our results for triangle counting on a variety of
real world networks (see Table 2) in both the arbitrary and adjacency list models. Our algorithms
use additional information through predictors to improve empirical performance. The predictors
are data dependent and include: memorizing heavy edges in a small portion of the first graph in a
sequence of graphs, linear regression, and graph neural networks (GNNs). Our experimental results
show that we can achieve up to 5x decrease in estimation error while keeping the same amount of
edges as other state of the art empirical algorithms. For more details, see Section 4. In Section F.7,
we show that our noisy oracle models are realistic for real datasets.
Related Empirical Works On the empirical side, most of the focus has been on triangle counting
in the arbitrary order model for which there are several algorithms that work well in practice. We
primarily focus on two state-of-the-art baselines, ThinkD (Shin et al., 2018) and WRS (Shin, 2017).
In these works, the authors compare to previous empirical benchmarks such as the ones given in
Stefani et al. (2017); Han & Sethu (2017); Lim & Kang (2015) and demonstrate that their algorithms
achieve superior estimates over these benchmarks. There are also other empirical works such as
Ahmed et al. (2017) and Ahmed & Duffield (2020) studying this model but they do not compare
to either ThinkD or WRS. While these empirical papers demonstrate that their algorithm returns
unbiased estimates, their theoretical guarantees on space is incomparable to the previously stated
space bounds for theoretical algorithms in Table 1. Nevertheless, we use ThinkD and WRS as part
of our benchmarks due to their strong practical performance and code accessibility.
Implicit Predictors in Prior Works The idea of using a predictor is implicit in many prior works.
The optimal two pass triangle counting algorithm of McGregor et al. (2016) can be viewed as an
implementation of a heavy edge oracle after the first pass. This oracle is even stronger than the K-
noisy oracle as it is equivalent to an oracle that is always correct on an edge e if Ne either exceeds
or is under the threshold ρ by a constant multiplicative factor. This further supports our choice of
oracles in our theoretical results, as a stronger version of our oracle can be implemented using one
additional pass through the data stream (see Section G). Similarly, the optimal triangle counting
streaming algorithm (assuming a random order) given in McGregor & Vorotnikova (2020) also
implicitly defines a heavy edge oracle using a small initial portion of the random stream (see Lemma
2.2 in McGregor & Vorotnikova (2020)). The random order assumption allows for the creation of
such an oracle since heavy edges are likely to have many of their incident triangle edges appearing
in an initial portion of the stream. We view these two prior works as theoretical justification for our
oracle definitions. Lastly, the WRS algorithm also shares the feature of defining an implicit oracle:
some space is reserved for keeping the most recent edges while the rest is used to keep a random
sample of edges. This can be viewed as a specific variant of our model, where the oracle predicts
recent edges as heavy.
Preliminaries. G = (V, E) denotes the input graph, and n, m and T denote the number of ver-
tices, edges and triangles (or four-cycles) in G, respectively. We use N(v) to denote the set of neigh-
bors of a node v, and ∆ to denote the set of triangles. In triangle counting, for each xy ∈ E(G),
We recall that Nxy = |z : {χ,y, z} ∈ ∆∣ is the number of triangles incident to edge xy, and
Rxy = |z : {x, y, z} ∈ ∆, x <s z <s y| is the number of triangles adjacent to xy with the third
vertex z of the triangle betWeen x and y in the adjacency list order. Table A summarizes the notation.
2 Triangle Counting in the Adjacency List Model
We describe an algorithm With a heavy edge oracle, and one With a value oracle.
Heavy Edge Oracle. We present an overvieW of our one-pass algorithm, Algorithm 1, With a space
complexity of Oe(min(e-2m2/3/T1/3, e-1m1/2)), given in Theorem 1.1. We defer the pseudocode
5
Published as a conference paper at ICLR 2022
and proof of the theorem to Appendix B. The adaptations and proofs for Theorems 1.4, 1.5 and B.1
for the various noisy oracles appear in Appendix B.1 and Appendix B.2.
Our algorithm works differently depending on the value of T . We first consider the case that
T ≥ (m/)1/2. Assume that for each sampled edge xy in the stream, we can exactly know the
number of triangles Rxy this edge contributes to T. Then the rate at which we would need to sample
each edge would be proportional to Pnaive ≈ 6-2Δe/T, where Δe is the maximum number of tri-
angles incident to any edge. Hence, our first idea is to separately consider light and non-light edges
using the heavy edge oracle. This allows us to sample edges that are deemed light by the oracle
at a lower rate, p1 , and compute their contribution by keeping track of Rxy for each such sampled
edge. Intuitively, light edges offer us more flexibility and thus we can sample them with a lower
probability while ensuring the estimator’s error does not drastically increase. In order to estimate
the contribution due to non-light edges, we again partition them into two types: medium and heavy,
according to some threshold ρ. We then use an observation from McGregor et al. (2016), that since
for heavy edges Rxy > ρ, it is sufficient to sample from the entire stream at rate p3 ≈ e-2/ρ, in
order to both detect if some edge xy is heavy and if so to estimate Rxy.
Therefore, it remains to estimate the contribution to T due to medium edges (these are the edges
that are deemed non-light by the oracle, and also non-heavy according to the sub-sampling above).
Since the number of triangles incident to medium edges is higher than that of light ones, we have
to sample them at some higher rate p2 > p1. However, since their number is bounded, this is still
space efficient. We get the desired bounds by correctly setting the thresholds between light, medium
and heavy edges.
When T < (m/e)1/2 our algorithm becomes much simpler. We only consider two types of edges,
light and heavy, according to some threshold T/ρ. To estimate the contribution due to heavy edges
we simply store them and keep track of their Rxy values. To estimate the contribution due to light
edges We sub-sample them with rate e 2 ∙ Δe/T = e 2/ρ. The total space We use is O(e 1 √m),
which is optimal in this case. See Algorithm 1 for more details of the implementation.
Value-Based Oracle. We also consider the setting where the predictor returns an estimate p(e) of
Re, and we assume Re ≤ p(e) ≤ α ∙ Re, where ɑ ≥ 1 is an approximation factor. We relax this
assumption to also handle additive error as well as noise, but for intuition we focus on this case.
The value-based oracle setting requires the use of novel techniques, such as the use of exponential
random variables (ERVs). Given this oracle, for an edge e, we compute p(e)/ue, were ue is a
standard ERV. We then store the O(α log(1/e)) edges e for which p(e)/ue is largest. Since we
are in the adjacency list model, once we start tracking edge e = xy, we can also compute the
true value Re of triangles that the edge e participates in (since for each future vertex z we see,
we can check if x and y are both neighbors of z). Note that we track this quantity only for the
O(α log(1/e)) edges that we store. Using the max-stability property of ERVs, maxe Re/ue is equal
in distribution to T/u, where u is another ERV. Importantly, using the density function of an ERV,
one can show that the edge e for which Re/ue is largest is, with probability 1 - O(e3), in our list
of the O(α log(1/e)) largest p(e)/ue values that we store. Repeating this scheme r = O(1/e2)
times, we obtain independent estimates T/u1, . . . , T/ur, where u1, . . . , ur are independent ERVs.
Taking the median of these then gives a (1 ± e)-approximation to the total number T of triangles.
We note that ERVs are often used in data stream applications (see, e.g., Andoni (2017)), though to
the best of our knowledge they have not previously been used in the context of triangle estimation.
We also give an alternative algorithm, based on subsampling at O(log n) scales, which has worse
logarithmic factors in theory but performs well empirically.
3	Triangle Counting in the Arbitrary Order Model
In this section we discuss Algorithm 4 for estimating the number of triangles in an arbitrary order
stream of edges. The pseudo-code of the algorithm as well as omitted proofs for the different
oracles are given in Supplementary Section C. Here we give the intuition behind the algorithm. Our
approach relies on sampling the edges of the stream as they arrive and checking if every new edge
forms a triangle with the previously sampled edges. However, as previously discussed, this approach
alone fails if some edges have a large number of triangles incident to them as “overlooking” such
edges might lead to an underestimation of the number of triangles. Therefore, we utilize a heavy
edge oracle, and refer to edges that are not heavy as light. Whenever a new edge arrives, we first
6
Published as a conference paper at ICLR 2022
query the oracle to determine if the edge is heavy. If the edge is heavy we keep it, and otherwise
we sample it with some probability. As in the adjacency list arrival model case, this strategy allows
us to reduce the variance of our estimator. By balancing the sampling rate and our threshold for
heaviness, we ensure that the space requirement is not too high, while simultaneously guaranteeing
that our estimate is accurate.
In more detail, our algorithm works as follows. First, we set a heaviness threshold ρ, so that if we
predict an edge e to be part of ρ or more triangles, we label it as heavy. We also set a sampling
parameter p. We let H be the set of edges predicted to be heavy and let SL be a random sample of
edges predicted to be light. Then, we count three types of triangles. The first counter, `1, counts the
triangles ∆ = (e1, e2, e), where the first two edges seen in this triangle by the algorithm, represented
by e1 and e2, are both in SL. Note that we only count the triangle if e1 and e2 ,were both in SL at
the time e arrives in the stream. Similarly, `2 counts triangles whose first two edges are in SL and
H (in either order), and `3 counts triangles whose first two edges are in H . Finally, we return the
estimate ' = 'ι∕p2 + '2 /p + '3. Note that if the first two edges in any triangle are light, they will
both be in SL with probability p2, and if exactly one of the first two edges is light, it will be in SL
with probability p. Therefore, we divide '1 by p2 and '2 by p so that ' is an unbiased estimator.
4 Experiments
We now evaluate our algorithm on real and synthetic data whose properties are summarized in
Table 2 (see Appendix F for more details).
Table 2: Datasets used in our experiments. Snapshot graphs are a sequence of graphs over time (the
length of the sequence is given in parentheses) and temporal graphs are formed by edges appearing
over time. The listed values for n (number of vertices), m (number of edges), and T (number of
triangles) for Oregon and CAIDA are approximated across all graphs. The Oregon and CAIDA
datasets come from Leskovec & Krevl (2014); Leskovec et al. (2005), the Wikibooks dataset comes
from Rossi & Ahmed (2015), the Reddit dataset comes from Leskovec & Krevl (2014); Kumar et al.
(2018), the Twitch dataset comes from Rozemberczki et al. (2019), the Wikipedia dataset comes
from Rossi & Ahmed (2015), and the Powerlaw graphs are sampled from the Chung-Lu-Vu random
graph model with expected degree of the i-th vertex proportional to 1/i2 (Chung et al., 2003).
Name	Type	Predictor	n	m	T
Oregon	Snapshot (9)	1st graph	~~^I04~~	~ 2.2 ∙ 104	~ 1.8 ∙ 104
CAIDA 2006	Snapshot (52)	1st graph	~ 2.2 ∙ 104	~ 4.5 ∙ 104	~ 3.4 ∙ 104
CAIDA 2007	Snapshot (46)	1st graph	~ 2.5 ∙ 104	~ 5.1 ∙ 104	~ 3.9 ∙ 104
Wikibooks	Temporal	Prefix	~ 1.3 ∙ 105	~ 3.9 ∙ 105	~ 1.8 ∙ 105
Reddit	Temporal	Regression	~ 3.6 ∙ 104	~ 1.2 ∙ 105	~ 4.1 ∙ 105
Twitch	-	GNN	~ 6.5 ∙ 103	~ 5.7 ∙ 104	~ 5.4 ∙ 104
Wikipedia	-	GNN	~ 4.8 ∙ 103	~ 4.6 ∙ 104	~ 9.5 ∙ 104
Powerlaw	Synthetic	EV	~ 1.7 ∙ 105	~ 106	~ 3.9 ∙ 107
We now describe the edge heaviness predictors that we use (see also Table 2). Our predictors adapt
to the type of dataset and information available for each dataset. Some datasets we use contain
only the graph structure (nodes and edges) without semantic features, thus not enabling us to train a
classical machine learning predictor for edge heaviness. In those cases we use the true counts on a
small prefix of the data (either 10% of the first graph in a sequence of graphs, or a prefix of edges
in a temporal graph) as predicted counts for subsequent data. However, we are able to create more
sophisticated predictors on three of our datasets, using feature vectors in linear regression or a Graph
Neural Network. Precise details of the predictors follow.
•	Snapshot: For Oregon / CAIDA graph datasets, which contain a sequence of graphs, we use
exact counting on a small fraction of the first graph as the predictor for all the subsequent graphs.
Specifically, we count the number of triangles per edge, Ne, on the first graph for each snapshot
dataset. We then only store 10% of the top heaviest edges and use these values as estimates for edge
heaviness in all later graphs. If a queried edge is not stored, its predicted Ne value is 0.
7
Published as a conference paper at ICLR 2022
•	Prefix: In the WikiBooks temporal graph, we use the exact Ne counts on the first half of the graph
edges (when sorted by their timestamps) as the predicted values for the second half.
•	Linear Regression: In the Reddit Hyperlinks temporal graph, we use a separate dataset
(Kumar et al., 2019) that contains 300-dimensional feature embeddings of subreddits (graph
nodes). Two embeddings are close in the feature space if their associated subreddits have
similar sub-communities. To produce an edge f (e) embedding for an edge e = uv from
the node embedding of its endpoints, f(u) and f (v), we use the 602-dimensional embedding
(f (u), f(v), k(f (u) - f(v)k1 , k(f (u) - f (v)k2). We then train a linear regressor to predict Ne
given the edge embedding f (e). Training is done on a prefix of the first half of the edges.
•	Link Prediction (GNN): For each of the two networks, we start with a graph that has twice as
many edges as listed in Table 2,(〜1.1 ∙ 105 edges for Twitch and 9.2 ∙ 104 edges for Wikipedia).
We then randomly remove 50% of the total edges to be the training data set, and use the remaining
edges as the graph we test on. We use the method proposed in Zhang & Chen (2018) to train a link
prediction oracle using a Graph Neural Network (GNN) that will be used to predict the heaviness
of the testing edges. For each edge that arrives in the stream of the test edges, we use the predicted
likelihood of forming an edge given by the the neural network to the other vertices as our estimate
for Nuv, the number of triangles on edge uv. See Section F.2 for details of training methodology.
•	Expected Value (EV): In the Powerlaw graph, the predicted number of triangles incident to each
Ne is its expected value, which can be computed analytically in the CLV random graph model.
Baselines. We compare our algorithms with the following baselines.
•	ThinkD and WRS (Arbitrary Order): These are the state of the art empirical one-pass algorithms
from Shin et al. (2018) and Shin (2017) respectively. The ThinkD paper presents two versions of
the algorithm, called ‘fast’ and ‘accurate’. We use the ‘accurate’ version since it provides better
estimates. We use the authors’ code for our experiments (Shin et al., 2020; Shin, 2020).
•	MVV (Arbitrary Order and Adjacency List): We use the one pass streaming algorithms given in
McGregor et al. (2016) for the arbitrary order model and the adjacency list model.
Error measurement. We measure accuracy using the relative error |1 - Te/T |, where T is the true
triangle count and T is the estimate returned by an algorithm. Our plots show the space used by an
algorithm (in terms of the number of edges) versus the relative error. We report median errors over
50 independent executions of each experiment, ± one standard deviation.
4.1 Results for Arbitrary Order Triangle Counting Experiments
In this section, we give experimental results for Algorithm 4 which approximates the triangle count
in arbitrary order streams. Note that we need to set two parameters for Algorithm 4: p, which is the
edge sampling probability, and ρ, which is the heaviness threshold. In our theoretical analysis, we
assume knowledge of a lower bound on T in order to set p and ρ, as is standard in the theoretical
streaming literature. However, in practice, such an estimate may not be available; in most cases,
the only parameter we are given is a space bound for the number of edges that can be stored. To
remedy this discrepancy, we modify our algorithm slightly by setting a fixed fraction of space to use
for heavy edges (10% of space for all of our experiments) and setting p correspondingly to use up
the rest of the space bound given as input. See details in Supplementary Section F.3.
Oregon and CAIDA In Figures 1(a) and 1(b), we display the relative error as a function of in-
creasing space for graph #4 in the dataset for Oregon and graph #30 for CAIDA 2006. These
figures show that our algorithm outperforms the other baselines by as much as a factor of 5. We do
not display the error bars for MVV and WRS for the sake of visual clarity, but they are comparable
to or larger than both ThinkD and our algorithm. A similar remark applies to all figures in Figure
1. As shown in Figure 2, these specific examples are reflective of the performance of our algorithm
across the whole sequence of graphs for Oregon and CAIDA. We also show qualitatively similar
results for CAIDA 2007 in Figure 4(a) in Supplementary Section F.4.
We also present accuracy results over the various graphs of the Oregon and CAIDA datasets. We
fix the space to be 10% of the number of edges (which varies across graphs). Our results for the
Oregon dataset are plotted in Figure 2(a) and the results for CAIDA 2006 and 2007 are plotted in
8
Published as a conference paper at ICLR 2022
Oregon, GraPh#4
Jαu",aΛAe-SH
Our Alg
ThInkD
MW
WRS
Calda-2006, GraPh#30
OurAlg
ThInkD
Reddlt Hyperilnks GraPh
(a) Oregon
WlkIbooIcs Graph
(d) WikibookS
Wlldpedla Graph
(e) Wikipedia
PowerIawGraph
(f) Powerlaw
0.20
10-15
I
jo-10-
0.05
0.00
Oregon, Space = 0.1m
23456789
Graph #
(a) Oregon
Figure 1: Error aS a function of Space in the arbitrary order model.
Ca Ida-2006, Space = 0.1m
20	30
Graph #
(b) CAIDA 2006
Ca Ida-2007, Space = 0.1m
(c) CAIDA 2007
Figure 2: Error acroSS SnapShot graphS with Space 0.1m in the arbitrary order model.
FigureS 2(b) and 2(c), reSpectively. TheSe figureS illuStrate that the quality of the predictor remainS
conSiStent over time even after a year haS elapSed between when the firSt and the laSt graphS were
created aS our algorithm outperformS the baSelineS on average by up to a factor of 2.
Reddit Our reSultS are diSplayed in Figure 1(c). All four algorithmS are comparable aS we vary
Space. While we do not improve over baSelineS in thiS caSe, thiS dataSet ServeS to highlight the fact
that predictorS can be trained uSing node or edge Semantic information (i.e., featureS).
Wikibooks and Powerlaw For WikibookS, we See in Figure 1(d) that our algorithm iS outperform-
ing ThinkD and WRS by at leaSt a factor of 2, and theSe algorithmS heavily outperform the MVV
algorithm. NonetheleSS, it iS important to note that our algorithm uSeS the exact countS on the firSt
half of the edgeS (encoded in the predictor), which encode a lot information not available to the
baSelineS. ThuS the takeaway iS that in temporal graphS, where edgeS arrive continuouSly over time
(e.g., citation networkS, road networkS, etc.), uSing a prefix of the edgeS to form noiSy predictorS can
lead to a Significant advantage in handling future data on the Same graph. Our reSultS for Powerlaw
are preSented in Figure 1(f). Here too we See that aS the Space allocation increaSeS, our algorithm
outperformS ThinkD, MVV, and WRS.
Twitch and Wikipedia In Figure 1(e), we See that three algorithmS, ourS, MVV, and WRS, are
all comparable aS we vary Space. The qualitatively Similar reSult for Twitch iS given in Figure 4(b).
TheSe dataSetS Serve to highlight that predictorS can be trained uSing modern ML techniqueS Such aS
Graph Neural NetworkS. NevertheleSS, theSe predictorS help our algorithmS improve over baSelineS
for experimentS in the adjacency liSt model (See Section below).
Results for Adjacency List Experiments Our experimental reSultS for adjacency liSt experimentS,
which are qualitatively Similar to the arbitrary order experimentS, are given in full detail in SectionS
F.5 and F.6.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Justin is supported by the NSF Graduate Research Fellowship under Grant No. 1745302 and Math-
Works Engineering Fellowship. Sandeep and Shyam are supported by the NSF Graduate Research
Fellowship under Grant No. 1745302. Ronitt was supported by NSF awards CCF-2006664, DMS
2022448, and CCF-1740751. Piotr was supported by the NSF TRIPODS program (awards CCF-
1740751 and DMS-2022448), NSF award CCF-2006798 and Simons Investigator Award. Talya is
supported in part by the NSF TRIPODS program, award CCF-1740751 and Ben Gurion University
Postdoctoral Scholarship. Honghao Lin and David Woodruff would like to thank for partial support
from the National Science Foundation (NSF) under Grant No. CCF-1815840.
References
Nesreen Ahmed and Nick Duffield. Adaptive shrinkage estimation for streaming graphs.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 10595-10606. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
780261c4b9a55cd803080619d0cc3e11- Paper.pdf.
Nesreen K. Ahmed, Nick Duffield, Theodore L. Willke, and Ryan A. Rossi. On sampling from
massive graph streams. Proc. VLDB Endow., 10(11):1430-1441, August 2017. ISSN 2150-
8097. doi: 10.14778/3137628.3137651. URL https://doi.org/10.14778/3137628.
3137651.
Mohammad Al Hasan and Vachik S Dave. Triangle counting in large networks: a review. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(2):e1226, 2018.
Alexandr Andoni. High frequency moments via max-stability. In 2017 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March
5-9, 2017, pp. 6364-6368. IEEE, 2017.
Alexandr Andoni, Collin Burns, Ying Li, Sepideh Mahabadi, and David P. Woodruff. Streaming
complexity of svms. In APPROX-RANDOM, 2020.
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cam-
bridge University Press, 1999. doi: 10.1017/CBO9780511624216.
Albert Atserias, Martin Grohe, and Daniel Marx. Size bounds and query plans for relational joins.
In 49th Annual IEEE Symposium on Foundations of Computer Science, 2008.
Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic
foundations of algorithm configuration for combinatorial partitioning problems. In Conference
on Learning Theory, pp. 213-274. PMLR, 2017.
Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In
International Conference on Machine Learning, 2018a.
Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,
online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations
of Computer Science (FOCS), pp. 603-614. IEEE, 2018b.
Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. arXiv preprint
arXiv:1907.00533, 2019.
Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning aug-
mented algorithms. In Advances in Neural Information Processing Systems, 2020.
Ziv Bar-Yossef, Ravi Kumar, and D. Sivakumar. Reductions in streaming algorithms, with an ap-
plication to counting triangles in graphs. In 13th Annual ACM-SIAM Symposium on Discrete
Algorithms, 2002.
Suman K. Bera and Amit Chakrabarti. Towards tighter space bounds for counting triangles and
other substructures in graph streams. In Symposium on Theoretical Aspects of Computer Science
(STACS 2017), 2017.
10
Published as a conference paper at ICLR 2022
Suman K. Bera and C. Sheshadhri. How the degeneracy helps for traingle counting in graph streams.
In ACM Symposium on Principles of Database Systems, June 2020.
Vladimir Braverman, Rafail Ostrovsky, and Dan Vilenchik. How hard is counting triangles in the
streaming model? In International Colloquium on Automata, Languages, and Programming, pp.
244-254. Springer, 2013.
Fan Chung, Linyuan Lu, and Van Vu. The spectra of random graphs with given expected degrees. In-
ternet Math., 1(3):257-275, 2003. URL https://Projecteuclid.org: 4 4 3/euclid.
im/1109190962.
Edith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of frequencies:
Beyond the worst case. In International Conference on Machine Learning. PMLR, 2020.
Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-
timization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351-6361, 2017.
Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster
matchings via learned duals. In Advances in Neural Information Processing Systems, 2021. URL
https://arxiv.org/abs/2107.09770.
Yihe Dong, Piotr Indyk, Ilya P Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. ICLR, 2020.
Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-
augmented algorithms for frequency estimation. In International Conference on Machine Learn-
ing, 2021.
Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.
Learning-based support estimation in sublinear time. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=tilovEHA3YS.
Illes J Farkas, Imre Derenyi, A-L Barabasi, and Tamas Vicsek. Spectra of” real-world“ graphs:
Beyond the semicircle law. In The Structure and Dynamics of Networks, pp. 372-383. Princeton
University Press, 2011.
Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. Why are learned indexes so effective? In
International Conference on Machine Learning, pp. 3123-3132. PMLR, 2020.
Brooke Foucault Welles, Anne Van Devender, and Noshir Contractor. Is a ”friend” a friend? investi-
gating the structure of friendship networks in virtual worlds. In CHI 2010 - The 28th Annual CHI
Conference on Human Factors in Computing Systems, Conference Proceedings and Extended Ab-
stracts, pp. 4027-4032, June 2010. ISBN 9781605589312. doi: 10.1145/1753846.1754097. 28th
Annual CHI Conference on Human Factors in Computing Systems, CHI 2010 ; Conference date:
10-04-2010 Through 15-04-2010.
Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.
In International Conference on Machine Learning, pp. 2319-2327. PMLR, 2019.
Heitor Murilo Gomes, Jesse Read, Albert Bifet, Jean Paul Barddal, and Joao Gama. Machine
learning for streaming data: state of the art, challenges, and opportunities. SIGKDD Explor.,
21:6-22, 2019.
Guyue Han and Harish Sethu. Edge sample and discard: A new algorithm for counting triangles
in large dynamic graphs. In Proceedings of the 2017 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining 2017, ASONAM ’17, pp. 44-49, New York,
NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349932. doi: 10.1145/
3110025.3110061. URL https://doi.org/10.1145/3110025.3110061.
Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In International Conference on Learning Representations, 2019a. URL https:
//openreview.net/forum?id=r1lohoCqY7.
11
Published as a conference paper at ICLR 2022
Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In International Conference on Learning Representations, 2019b.
Zachary Izzo, Sandeep Silwal, and Samson Zhou. Dimensionality reduction for wasserstein
barycenter. In Advances in Neural Information Processing Systems, 2021.
Rajesh Jayaram and John Kallaugher. An optimal algorithm for triangle counting in the stream.
In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques
(APPROX/RANDOM 2021). Schloss DagstUhl-Leibniz-ZentrUm fur Informatik, 2021.
Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented
data stream algorithms. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HyxJ1xBYDH.
John KallaUgher and Eric Price. A hybrid sampling scheme for triangle coUnting. In Philip N. Klein
(ed.), Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,
SODA 2017, Barcelona, Spain, HotelPorta Fira, January 16-19, pp. 1778-1797. SIAM, 2017.
John KallaUgher, Andrew McGregor, Eric Price, and Sofya Vorotnikova. The complexity of coUnt-
ing cycles in the adjacency list streaming model. In Proceedings of the 38th ACM SIGMOD-
SIGACT-SIGAI Symposium on Principles of Database Systems, PODS ’19, pp. 119-133, New
York, NY, USA, 2019. Association for CompUting Machinery. ISBN 9781450362276. doi:
10.1145/3294052.3319706. URL https://doi.org/10.1145/3294052.3319706.
Mihail N. KoloUntzakis, Gary L. Miller, Richard Peng, and Charalampos E. TsoUrakakis. Efficient
triangle coUnting in large graphs via degree-based vertex partitioning. Lecture Notes in Computer
Science ,pp.15-24,2010. ISSN 1611-3349. doi: 10.1007/978-3-642-18009-5_3. URL http:
//dx.doi.org/10.1007/978-3-642-18009-5_3.
Tim Kraska, Alex BeUtel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index strUctUres. In Proceedings of the 2018 International Conference on Management of Data
(SIGMOD), pp. 489-504. ACM, 2018.
Srijan KUmar, William L Hamilton, JUre Leskovec, and Dan JUrafsky. CommUnity interaction and
conflict on the web. In Proceedings of the 2018 World Wide Web Conference on World Wide Web,
pp. 933-943. International World Wide Web Conferences Steering Committee, 2018.
Srijan KUmar, XikUn Zhang, and JUre Leskovec. Predicting dynamic embedding trajectory in tem-
poral interaction networks. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1269-1278. ACM, 2019.
Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedUl-
ing via learned weights. In Proceedings of the 31st Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 1859-1877. SIAM, 2020.
JUre Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data, JUne 2014.
JUre Leskovec, Jon Kleinberg, and Christos FaloUtsos. Graphs over time: Densification laws,
shrinking diameters and possible explanations. In Proceedings of the Eleventh ACM SIGKDD
International Conference on Knowledge Discovery in Data Mining, KDD ’05, pp. 177-187,
New York, NY, USA, 2005. Association for CompUting Machinery. ISBN 159593135X. doi:
10.1145/1081870.1081893. URL https://doi.org/10.1145/1081870.1081893.
JUre Leskovec, Lars Backstrom, Ravi KUmar, and Andrew Tomkins. Microscopic evolUtion of social
networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’08, pp. 462-470, New York, NY, USA, 2008. Association for
CompUting Machinery. ISBN 9781605581934. doi: 10.1145/1401890.1401948. URL https:
//doi.org/10.1145/1401890.1401948.
YongsUb Lim and U Kang. Mascot: Memory-efficient and accUrate sampling for coUnting local
triangles in graph streams. In Proceedings of the 21th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’15, pp. 685-694, New York, NY, USA, 2015.
Association for CompUting Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783285.
URL https://doi.org/10.1145/2783258.2783285.
12
Published as a conference paper at ICLR 2022
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture
models at scale via coresets. Journal of Machine Learning Research, 18(160):1-25, 2018. URL
http://jmlr.org/papers/v18/15-506.html.
Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In
International Conference on Machine Learning, pp. 3296-3305. PMLR, 2018.
M. Mahoney. Large text compression benchmark., 2011.
Andrew McGregor and Sofya Vorotnikova. Triangle and four cycle counting in the data stream
model. PODS’20, pp. 445-456, New York, NY, USA, 2020. Association for Computing Machin-
ery. ISBN 9781450371087. doi: 10.1145/3375395.3387652. URL https://doi.org/10.
1145/3375395.3387652.
Andrew McGregor, Sofya Vorotnikova, and Hoa T. Vu. Better algorithms for counting triangles in
data streams. In ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,
June 2016.
R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: Simple
building blocks of complex networks. Science, 298(5594):824-827, 2002. ISSN 0036-8075. doi:
10.1126/science.298.5594.824. URL https://science.sciencemag.org/content/
298/5594/824.
Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In
Advances in Neural Information Processing Systems, 2018.
Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS, 2020.
Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. arXiv preprint
arXiv:2006.09123, 2020.
Rasmus Pagh and Charalampos E. Tsourakakis. Colorful triangle counting and a mapreduce im-
plementation. Inf. Process. Lett., 112(7):277-281, 2012. doi: 10.1016/j.ipl.2011.12.007. URL
https://doi.org/10.1016/j.ipl.2011.12.007.
Arnau Prat-Perez, David Dominguez-Sal, Josep M Brunat, and JoSeP-LlUiS Larriba-Pey. Shaping
communities out of triangles. In Proceedings of the 21st ACM international conference on Infor-
mation and knowledge management, pp. 1677-1681, 2012.
Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.
In Advances in Neural Information Processing Systems, pp. 9661-9670, 2018.
Jack Rae, Sergey Bartunov, and Timothy Lillicrap. Meta-learning neural bloom filters. In Interna-
tional Conference on Machine Learning, pp. 5271-5280, 2019.
Piyush Rai, Hal Daume, and Suresh Venkatasubramanian. Streamed learning: One-pass svms.
ArXiv, abs/0908.0572, 2009.
Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceed-
ings of the 31st Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1834-1845. SIAM,
2020.
Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
and visualization. In AAAI, 2015. URL http://networkrepository.com.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding, 2019.
C. Seshadhri, Ali Pinar, and Tamara G. Kolda. Fast triangle counting through wedge sampling. In
the International Conference on Data Mining (ICDM), 2013.
Kijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.
2017 IEEE International Conference on Data Mining (ICDM), pp. 1087-1092, 2017.
Kijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.
https://github.com/kijungs/waiting_room, 2020.
13
Published as a conference paper at ICLR 2022
Kijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accurate
triangle counting in graph streams with deletions. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 141-157. Springer, 2018.
Kijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accu-
rate triangle counting in graph streams with deletions. https://github.com/kijungs/
thinkd, 2020.
Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, and Eli Upfal. TriEst: Counting local and
global triangles in fully dynamic streams with fixed memory size. ACM Trans. Knowl. Discov.
Data, 11(4), June 2017. ISSN 1556-4681. doi: 10.1145/3059194. URL https://doi.org/
10.1145/3059194.
Kapil Vaidya, Eric Knorr, Tim Kraska, and Michael Mitzenmacher. Partitioned learned bloom filter.
In International Conference on Learning Representations, 2021.
Sofya Vorotnikova. Improved 3-pass algorithm for counting 4-cycles in arbitrary order streaming.
CoRR, abs/2007.13466, 2020. URL https://arxiv.org/abs/2007.13466.
Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - a
survey. Proceedings of the IEEE, 104(1):34-57, 2016.
Alexander Wei. Better and simpler learning-augmented online caching. In Approximation, Ran-
domization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM
2020). Schloss DagStUhI-Leibniz-ZentrUm fur Informatik, 2020.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor. Comput.
Sci., 10:1-157, 2014.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada,
pp. 5171-5181, 2018.
14
Published as a conference paper at ICLR 2022
A Notations Table
Notation	Definition
G	= (V,E)	a graph G with vertex set V and edge set E
n	number of triangles
m	number of edges
T	number of triangles (or 4-cycles)
€	approximation parameter
<s	total ordering on the vertices according to their arrival in the adjacency list arrival model
^∆	set of triangles in G
□	set of 4-cycles in G
N (V)	set of neighbors of node V
Nxy	number of triangles (4-cycles) incident to edge xy, i.e., KzI(X,y,z) ∈ δH (Kw,z|(X,y,w∙z) ∈ □;l)	
Rxy	number of triangles (x, z, y ) where x precedes z and z precedes y in the adjacency list arrival model, i.e., ∣{z∣(x,z,y) ∈ ∆,x < z <s y}∣	
Δe	maximum number of triangles incident to any edge
Δv		maximum number of triangles incident to any vertex
OP	heavy edge oracle with threshold P
B	Omitted Proofs from Section 2
In this section, we prove correctness and bound the space of Algorithm 1, thus proving Theorem 1.1.
In the adjacency list model, recall that we have a total ordering on nodes <s based on the stream
ordering, where x <s y if and only if the adjacency list of x arrives before the adjacency list of y in
the stream. For each xy ∈ E(G), we define Rxy = |z : {x, y, z} ∈ ∆ and x <s z <s y|. Note that
ify <s x, then Rxy = 0. It holds that Pxy∈E Rxy = T.
We first give a detailed overview of the algorithm. We first consider the case when T ≥ (m/)1/2.
Let P = (mT)1/3. We define the edge Xy to be heavy if Rxy ≥ ρ, light if Rxy ≤ T, and medium if
T < Rxy < ρ. We train the oracle to predict, upon seeing xy, whether Rxy is light or not. We define
H, M, and L as the set of heavy, medium, and light edges, respectively. Define TL = Pxy∈L Rxy.
We define TH and TM similarly. Since Pxy∈E Rxy = T = TL + TM + TH, it suffices to estimate
each of these three terms.
For the light edges, as shown in Lemma B.2, if we sample edges with rate p1 ≈ -2/P, with
high probability we obtain an estimate of TL within error ±T. For the heavy edges, we recall an
observation in McGregor et al. (2016): for an edge xy where Rxy is large, even if we do not sample
xy directly, if we sample from the stream (at rate p3 ≈ -2/P), we are likely to sample some edges
in the set |xz : {x, y, z} ∈ ∆ and x <s z <s y|. We refer to the sampled set of these edges as Saux.
Further, we will show that not only can we use Saux to recognize whether Rxy is large, but also to
estimate Rxy (Lemma B.3). What remains to handle is the medium edges. Since medium edges have
higher values of Rxy than light edges, we sample with a larger rate, p2, to reduce variance. However,
we show there cannot be too many medium edges, so we do not pay too much extra storage space.
The overall space we use is Oe(-2m2/3/T 1/3).
When T < (m/)1/2our algorithm becomes much simpler. Let P = √m∕e. We define the edge Xy
to be heavy if Rxy ≥ T/P, and light otherwise (so in this case all the medium edges become heavy
15
Published as a conference paper at ICLR 2022
edges). We directly store all the heavy edges and sub-sample with rate e-2∕ρ to estimate Tl. The
total space We use is O(e 1 √m), which is optimal in this case. See Algorithm 1 for more details of
the implementation.
Algorithm 1 Counting Triangles in the Adjacency List Model
1:	Input: Adjacency list edge stream and an oracle O that outputs heavy if Rxy ≥ T/ρ and
light otherwise.
2:	Output: An estimate of the triangle count T.
3:	Initialize Ai, Am, Ah = 0, Sl, SM, Saux = 0
4:	if T ≥ (m/e)1/2 then
5:	Set P = (mT)1/3, pi = αe-2∕ρ, p2 = min(βe-2ρ∕T, 1), p3 = γe-2 log n∕ρ.
6:	else
7:	Set ρ = m1∕2∕e, pi = 0, p2 = 1, p3 = γe-2∕ρ. {α, β, and Y are large enough constants.}
8:	while seeing edges adjacent to v do
9:	for all ab ∈ SL ∪ SM do
10:	if a, b ∈ N(v) then Cab = Cab + 1. {Update the counter for all the sampled light and
medium edges}
11:	for all av ∈ Saux do
12:	Bav = 1 {seen av twice}.
13:	for each incident edge vu do
14:	W.p. p3 : Saux = {vu} ∪ Saux, Bvu = 0
15:	if O(uv) outputs LIGHT then
16:	if uv ∈ SL then Al = Al + Cuv . {Finished counting Ruv }
17:	else w.p. p1, SL = {vu} ∪ SL
18:	else {uv is either MEDIUM or HEAVY}
19:	# := |{z : uz ∈ Saux, Buz = 1, z ∈ N(v)}| .
20:	if # ≥ p3P then {uv is HEAVY}
21:	Ah = Ah + # .
22:	else if uv ∈ SM then
23:	Am = Am + Cuv. {Finished counting Ruv}
24:	else
24:	With probability p2, SM = {vu} ∪ SM .
25:	return: Al/p1 + Am/p2 + Ah/p3.
Remark B.1. In the adjacency list model, we assume the oracle can predict whether Rxy ≥ T
or not, where Rxy is dependent on the node order. This may sometimes be impractical. However,
observe that Nxy ≥ Rxy and Nxy = 3T. Hence, our proof still holds if we assume what the
oracle can predict is whether Nxy ≥ T, which could be a more practical assumption.
Recall the following notation from Section 2. We define the edge xy to be heavy if Rxy ≥ P, light if
Rxy ≤ T, and medium if T < Rxy < ρ. We define H, M, and L as the set of heavy, medium, and
light edges, respectively. Define TL = Pxy∈L Rxy. We define TH and TM similarly. Also, recall
that since Pxy∈E Rxy = T = TL + TM + TH, it suffices to estimate of these three terms.
Lemma B.2. Let A be an edge set such that for each edge Xy ∈ A, Rxy ≤ T ,for some parameter
c, and let S be a subset of A such that every element in A is included in S with probability P = T1
for T = e2 ∙ c∕α, so that 1∕τ = ɑe-2∣ independently, for a sufficient large constant a. Then, with
probability at least 9/10, we have
τ X Rxy = X Rxy ± eT
xy∈S	xy∈A
16
Published as a conference paper at ICLR 2022
Proof. We let Y = τ i∈S Ri = τ i∈A fiRi, where fi = 1 when i ∈ S and fi = 0 otherwise. It
follows that E[Y] = TA. Then,
Var[Y] = E[Y2] -E[Y]2
=τ2 (EX fiR2 + X ffjRiRj∖) - TL
i∈A	i6=j
≤ τ2 卜X fiR2] + X τ12RiRj- TL
i∈A	i,j
= τ X Ri2 .
i∈A
To bound this, we notice that Ri ≤ T and Pi Ri = TA ≤ T, So
T X R2 ≤ T ∙ max Ri ∙ X Ri ≤ T ∙ T ∙ T = TT2 = ɪe2T2 .
By Chebyshev’s inequality, we have that
Pr[∣Y - Ta| >eT] ≤ ^ffT~ ≤ -.
e2T2	α
□
The following lemma shows that for the heavy edges, we can use the edges we have sampled to
recognize and estimate them.
Lemma B.3. Let xy be an edge in E. Assume we sample the edges with rate p3 = βe-2 log n/p in
the stream for a sufficiently large constant β, and consider the set
# = |xz has been sampled : {x, y, z} ∈ ∆ and x <s z <s y| .
Then we have the following: if Rxy ≥ 22, with probability at least 1 一 nl5, we have p-1 ∙ # =
(1 ± e)Rxy. If Rxy < 2, with probability at least 1 —今,we have p-1 ∙ # < p.
Proof. We first consider the case when Rxy ≥ 22.
For each edge i ∈ |xz : {x, y, z} ∈ ∆ and x <s z <s y|, let Xi = 1 if i has been sampled, or
Xi = 0 otherwise. We can see the Xi are i.i.d. Bernoulli random variables and E[Xi] = p3. By a
Chernoff bound we have
Pr | X Xi- P3Rχy | ≥ eP3 ∙ Rxy ≤ 2 exp (-e2p3 - Rxy∕3) ≤ n .
i
Therefore, we have
Pr [|P3 1 ∙ # - Rxy | ≥ eRxy ] =Pr ∣P3 1 ∙ X Xi- Rxy | ≥ eRxy ≤ n .
i
Alternatively, When Rxy < 22, we have
Pr |p3 1 ∙ # - Rxy | ≥ 2 P ≤ n5 ,
which means p-1 ∙ # < p with probability at least 1 一 白.	□
We are now ready to prove Theorem 1.1, restated here for the sake of convenience.
17
Published as a conference paper at ICLR 2022
Theorem 1.1. There exists a one-pass algorithm, Algorithm 1, with space complexity3
Oe(min(e-2m2/3/T1/3, e-1m1/2 )) in the adjacency list model that, using a learning-based ora-
cle, returns a (1 ± e)-approximation to the number T of triangles with probability at least4 7/10.
Proof. We first consider the case when T ≥ (m/e)i/2 .
For each edge Xy ∈ H, from Lemma B.3 we have that with probability at least 1 - n⅛, #/p3 =
(1 ± e)Rχy in Algorithm 1. If Xy ∈ H, then with probability 1 — *, Xy will not contribute to Ah.
Taking a union bound, we have that with probability at least 1 - 1/n3,
Ah/p3 = (1 ± e) X Rxy = (1 ± e)TH .
xy∈H
We now turn to deal with the light and medium edges. For a light edge Xy, it satisfies Rxy ≤ T/P
and therefore we can invoke Lemma B.2 with the parameter c set to P. For a medium edge Xy, it
satisfies Rxy ≤ P = T /(T /P), and therefore we can invoke Lemma B.2 with the parameter c set to
T/P. Hence, we have that with probability 4/5,
Am/p2 = TM ± eT, Al/pi = TL ± eT .
Putting everything together, we have that with probability 7/10,
|Ah/p3 + Am/p2 + Al/pi - T| ≤ 3eT.
Now we analyze the space complexity. We note that there are at most P medium edges. Hence, the
expected total space we use is O(mp3 + Pp2 + mpi) = O(e-2m2/3/T i/3 log n)5.
When T < (m/e)i/2, as described in Section 2, we only have two classes of edges: heavy and light.
We will save all the heavy edges and use sub-sampling to estimate TL . The analysis is almost the
same.
□
B.1 Noisy Oracles for the Adjacency List Model
Proofof Theorem 1.4. We first consider the case that T < (m/e)1/2. Recall that at this time, our
algorithms becomes that we save all the heavy edges Xy such that p(e) ≥ eT/√m and sampling the
light edges with rate pl =e-1 /√m. We define L as the set of deemed light edges, H as the set of
deemed heavy edges and L = |L|, H = |H|. Let P = eT∕√m, then We have the condition that for
every edge e,
1 - K ∙ -ɪ ≤ Pr[Oρ(e) = HEAVY] ≤ K ∙ R.
We will show that, (i) E[H] ≤ (K + 1)√m, (ii) Var[Al/pi] ≤ (4K + 2)e2T2. Under the above
conditions we can get that with probability at least 9/10 we can get a (1 ± O(√K) ∙e)-approximation
of T by Chebyshev’s inequality.
For (i), we divide H = Hh ∪ Hl, where the edges in Hh are indeed heavyedges, and the edges
in Hl are indeed lightedges. Then, it is easy to see that |Hh| ≤ T/P. For every light edges, the
probability that it is included in Hl is at most K ∙ R, hence we have
E[∣Hh∣] ≤ K ∙ X(R) ≤ K ∙ Tp = K√m
which implies E[H] ≤ (K + 1) ɪm.
For (ii), we also divide L = Ll ∪ Lh similarly. Then we have Var[AL] ≤ 2(Var[X] + Var[Y]),
where X = Pe∈Ll,e has been sampled Re, Y = Pe∈Lh,ehasbeensampledRe. Similar to the proof in
Lemma B.2, we have
VarX] ≤ pi X R ≤ pi Tρ2 = pιTρ.
e∈L e	P
3We use O(f) to denote O(f ∙ polylog(f)).
4The success probability can be 1 一 δ by running log(1∕δ) copies of the algorithm and taking the median.
5Using Markov’s inequality, we have that with probability at least 9/10, the total space we use is less than
10 times the expected total space.
18
Published as a conference paper at ICLR 2022
Now We bound Var[Y]. For every heavy edge We have that Pr[e ∈ Lh] ≤ K ∙ RP-. Then, condition
on the oracle Oρ , we have
E[Var[Y∣Oρ]] ≤ pιK X RPR ≤ pιK X PRe ≤ PIKPT .
e∈H e	e∈H
Also, We have
Var[E[Y∣Oρ]] < pιK X RrRe ≤ pιKρT .
e∈H Re
From Var[Y] = EVar[Y∣Oρ]] + Var[E[Y∣Oρ]] we know Var[Y] ≤ 2pιKρT, which means
Var[Aι∕pι] ≤ )(4K + 2)ρT = (4K + 2)e2T2.
When T ≥ (m/)1/2, recall that the oracle Oρ only needs to predict the edges that are medium
edges or light edges. So, we can use the same way to bound the expected space for the deemed
medium edges and the variance of the deemed light edges.	□
B.	2 A Value Prediction Oracle for the Adjacency List Model
In this section, we consider two types of value-prediction oracles, and variants of Algorithm 1 that
take advantage of these oracles. Recall that given an edge xy, Rxy = |z : {x, y, z} ∈ ∆ and x <s
z <s y| .
Definition B.4. A value-prediction oracle with parameter (α, β) is an oracle that, for any edge e,
outputs a value p(e) for which E[p(e)] ≤ αRe + β, and Pr[p(e) < Re — β] ≤ Kef for some
constant K and any λ ≥ 1.
Our algorithm is based on the following stability property of exponential random variables (see
e.g. Andoni (2017))
Lemma B.5. Let x1, x2, ..., xn > 0 be real numbers, and let u1, u2, ...un be i.i.d. standard
exponential random variables with parameter λ = 1. Then we have that the random variable
max(x1 ,…，Xn)〜X, where X = E - Xi and U 〜ExP(I).
u1	un	u	i
Lemma B.6. Letr = O(ln(1∕δ)∕e2) ande ≤ 1∕3. Ifwe take r independent samples X1, X2, ..., Xr
from ExP(1) and let X = median(X1, X2, ..., Xr), then with probability 1 — δ, X ∈ [ln 2(1 —
e), ln 2(1 + 5e)].
Proof. We first prove the following statement: with probability 1 — δ, F (X) ∈ [1∕2 — e, 1∕2 + e],
where F is the cumulative density function of a standard exponential random variable.
Consider the case when F (X) ≤ 1∕2 — e. We use the indicator variable Zi where Zi = 1 if
F(Xi) ≤ 1∕2 — e or Zi = 0 otherwise. Notice that when F(X) ≤ 1∕2 — e, at least half of the Zi
are 1, so Z = Pi Zi ≥ r∕2. On the other hand, we have E[Z] = r∕2 — re, by a Chernoff bound.
We thus have	Pr[|Z—E[Z]| ≥ er] ≤ 2e-2r/3 ≤ δ∕2 .
Therefore, we have with probability at most δ∕2, F (X) ≤ 1∕2 — e. Similarly, we have that with
probability at most δ∕2, F(X) ≥ 1∕2 + e. Taking a union bound, we have that with probability at
least 1 — δ, F(X) ∈ [1∕2 — e, 1∕2 + e].
Now, condition on F(X) ∈ [1∕2 — e, 1∕2 + e]. Note that F-1(X) = — ln(1 — X), so if we consider
the two endpoints of F (X), we have
F-1(1∕2 — e) = — ln(1∕2 + e) ≥ (1 —e)ln2,
and
F-1(1∕2 + e) = — ln(1∕2 — e) ≤ (1+5e)ln2.
which indicates that X ∈ [ln 2(1 — e), ln 2(1 + 5e)].	□
The algorithm utilizing this oracle is shown in Algorithm 2. As described in Section 2, here we runs
O(口)copies of the subroutine and takes a medium of them. For each subroutine, we aim to output
the maximum value maxe Re∕ue. We will show that with high probability for each subroutine,
19
Published as a conference paper at ICLR 2022
this maximum will be at least T/ log(K/). Hence, we only need to save the edge e, such that
(Re + β)∕ue is larger than T/ log(K∕e). However, one issue here is We need the information of T.
We can remove this assumption when β = 0 using the following way: we will show that with high
probability, the total number of edges we save is less than H = O(* * log2 (K∕e)(α + mβ∕T)), so
every time when a new edge comes, we can search the minimum value M such that the total number
of edges e that in at least one subroutine (p(e) + β)∕ue > M is less than H(we count an edge
multiple times if it occurs in multiple subroutines), then use M as the threshold. We can see M will
increase as the new edge arriving and it will be always less than H.
Algorithm 2 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
Input: Adjacency list edge stream and an value prediction oracle with parameter(α, β).
Output: An estimate of the number T of triangles.
Initialize X Ju and H = O(* log2(K∕e)(α + mβ∕T)), and let C be some large constant.
for i = 0 to ce-2 do {Initialize sets for ce-2 copies of samples, where Si stores edges, Qi stores
all the exponential random variables for each sampled edge, and Ai stores the current maximum
of each sample}
Si J u, Qi J u, and Ai J 0 .
while seeing edges adjacent to y in the stream do
for i = 0 to ce-2 do {Update the counters for each sample}
For all ab ∈ Si: ifa,b ∈ N(y) thenCaib J Caib+ 1
for each incident edge yx do
for i = 0 to ce-2 do
if xy ∈ Si then {Finished counting Cxy, update the current maximum}
Ai J max(Ai,Cxiy∕uixy) .
else {Put yx into the sample sets temporarily}
Let Ryx be the predicted value of Ryχ.
Ryx J Ryx + β.
Generate Uyx 〜ExP⑴.Set Qi — Qi ∪ {uizx}, Si — Si ∪ {yχ}.
n , Ti ʃ , i .ι ∙	i ∙	ι.ι	X~ʌ	/ τ τ	i	I r 一 τ~tlA∕d∖ Tt ʃl I
Set M to be the minimal integer such that i si ≤ H, where si = |{e ∈ E | Re∕uie ≥ M}|.
18:	for i = 0 to ce-2 do {Adjust the samples to be within the space limit}
19:	Let Qi J Qi \ {Uab},Si — Si \ {ab} if R^ab∕ulab < M for all ab ∈ E.
20:	for i = 0 to ce-2 do
21:	X JX∪{Ai}
22:	return: ln2 ∙ median(X).
We are now ready to prove Theorem 1.5. We first recall the theorem.
Theorem 1.5. Given an oracle with parameters (α, β), there exists a one-pass algorithm, Algo-
rithm 2, with space complexity O(e-2 log2(K∕e)(α + mβ∕T)) in the adjacency list model that
returns a (1 ± e)-approximation to the number of triangles T with probability at least 7∕10.
Proof. It is easy to see that for the i-th subroutine, the value f(i) = maxe Re∕uie is the sample from
the distribution T∕u, where U 〜ExP⑴.And if the edge ei, for which the true value Rei ∕ueei is the
maximum value among E, is included in Si , then the output of the i-th subroutine will be a sample
from T∕u. Intuitively, the prediction value p(e) will help us find this edge.
We will first show that with probability at least 9∕10 the following events will happen:
• (i) f(i) ≥ c1 T∕ log(1∕e) for all i, where c1 is a constant.
• (ii) Let si be the number of the edge e in the i-th subroutine such that (p(e) + β)∕uie ≥
c2T∕ log2(K∕e), then Pi si ≤ C2(* log2(K∕e)(α + mβ∕T)) for some constant C2.
• (iii) p(ei) + β ≥ c1 f(i)∕ log(K∕e) for all i.
20
Published as a conference paper at ICLR 2022
For (i), recall that f (i)〜T/u, where U 〜Exp(1). Hence, We have
Pr[f (i) < c1T/ log(1/)] = e- log(1/)/c1
1	1
≤ 1O0Cɪ2
Taking an union bound we get that with probability at least 99/1OO, (i) will happen. For each edge
e, we have
Pr[p(e) + β < Re∕λ] < Ke-λ,
similarly we can get that with probability at least 99/1OO, (iii) will happen.
For (ii), we first bound the expectation of si. For each edge e, we have
Pr [(p(e) + β)∕U ≥ c2T∕log2(K∕e)] = Pr* ≤ c2(p(e) + β) log2(K∕e)∕T]
≤ c12(p(e) + β) log2(K/)/T
≤ c12(αRe + 2β) log2(K∕e)∕T .
Hence, we have
E[si] = X c2 (αRe + 2βTlog2(K∕0 = O(log2(K∕e)(α + mβ∕T)).
e
Using Markov’s inequality, we get that with probability at least 99∕1OO, (ii) will happen. Finally,
taking a union bound, we can get with probability at least 9∕1O, all the three events will happen.
Now, conditioned on the above three events, we will show that all the subroutine i will output f (i).
For the subroutine i, from (ii) we know that M will be always smaller than O(T∕ log2(K∕e)), and
from (i) and (iii) we get that (p(e∙i)+ β) ∕ u% ≥ Ω(T∕ log2(K∕e)). Hence, the edge e% will be saved
in the subroutine i. From Lemma B.6 we get that with probability at least 7∕1O, we can finally get a
(1 ± e)-approximation of T.	□
We note that when β = 0, the space complexity will become O(± log2(K∕e)α), and in this case
we will not need a lower bound of T .
We will also consider the following noise model.
Definition B.7. A value-prediction oracle with parameter (α, β) is an oracle that, for any edge e,
outputs a value p(e) for which Pr[p(e) > λαRe + β] ≤ K , and Pr[p(e) < RRe 一 β] ≤ K for some
constant K and any λ ≥ 1.
The algorithm for this oracle is shown in Algorithm 3.
We now state our theorem.
Theorem B.1. Given an oracle with parameter (α, β), there exists a one-pass algorithm, Algo-
rithm 3, with space CompIexity O(KK- (α(log n)3 log log n + mβ∕T)) in the adjacency list model
that returns a (1 ± √K ∙ e)-approximation to T with probability at least 7∕10.
Proof. Define q(e) = p(e) + β, it is easy to see that from the condition we can get that
Pr[q(e) > 2λαRe] ≤ Kγ when Re > 2β, Pr q(e) < 牛 ≤ Ky
λ	λλ
We define the set Ei such that e ∈ Ei if and only if q(e) ∈ Ii. We can see E[Ai∕pi] = e∈E Re.
Now, we bound Var[Ai∕pi] when i ≥ 1.
LetX = Pe∈Ei,Re≤2i+1β Re andY = Pe∈Ei,Re>2i+1β Re, then we have Var[Ai] ≤ 2(Var[X]+
Var[Y]).
Similar to the proof in Lemma B.2, we can get that
T
Var[X] ≤ Pi	E	R2≤ Pi(2i+1β)2 -- = pi2l+1βT.
2i+1β
e∈Ei,Re≤2i+1β
21
Published as a conference paper at ICLR 2022
Algorithm 3 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle
1:	Input: Adjacency list edge stream and an value prediction oracle with parameter(α, β).
2:	Output: An estimate of the number T of triangles.
3:	Initialize Sj JQ and Aj — 0 where i = O (log n) and j = O (log log n). po = ce-22i β∕T
and pi = ce-22iβ(log n)2/T for some constant c. Define I。= [0, 2β) and Ii = [2iβ, 2i+1β),
HJ0.
4:	while seeing edges adjacent to y in the stream do
5:	For all ab ∈ Sij : if a, b ∈ N(y) then Cab J Cab + 1
6:	for each incident edge yx do
7:	if xy ∈ Sij then
8:	Aij J Aij + Cyx .
9:	else
10:	Let Ryx be the predicted value of Ryχ.
11:	Search i such that (Ryx + β) ∈ Ii
12:	For each j let Sij J Sij ∪ {yx} with probability pi .
13:	for i = 0 to O(log n) do
14:	H J H + median(Aij)/pi
15:	return: H .
For Var[Y], recall that for an edge e such that Re ≥ 2i+1β, We have Pr[e ∈ Ei] ≤ KqRe) ≤
K 2+β. Then, condition on the oracle Oθ , we have
E[Var[Y∣Oθ]] ≤ Pi	X	K2iR1βR2 = PiK2i+1 β	X	Re= PiK2i+1βT.
Re≥2i+1β	e	Re≥2i+1β
And
2i+1β
Var[E[Y∣Oθ]] < Pi	E K-^R = PiK2i+1βT.
R
Re ≥2i+1 β	e
From Var [Y] = E[Var [Y∣Oθ]] + Var [E[Y∣Oθ]] we get that Var [Y] ≤ 2γiK2i+1βT, from which
we can get that Var[Ai∕Pi] = 3Var[Ai] = O(K(e/logn)2T2). Using Chebyshev,s inequality
pi
and taking a median over O(log log n) independent trials, we can get Xi = median(Aij) satisfies
|Xi - Pe∈E. Re| ≤ √K(e/ log n)T with probability 1 - O(1∕ logn). A similar argument also
holds for I。, which means that after taking a union bound, with probability 9/10, the output of the
algorithm 3isa (1 ± √K ∙ e)- approximation of T.
Now we analyze the space complexity of Algorithm 3.
For i ≥ 1, we have
E [|Ei|]	= E X [e ∈	Ei]	+ X	[e	∈	Ei]	≤ E X	[e	∈	Ei]	+	2Tae
≤
R
e
Σ
≤2i-1β∕α
Tα
+ 2i-1β ≤	∑ K
R	Re≤2i-1 β∕α
Tα
2i-1β + 2i-1β
Tα
≤ (	+ 1)2i-1β,
and |Eo| ≤ m, hence we have that the total expectation of the space is Ei PiΕ[∣Ei∣]=
O( K (ɑ(log n)3 log log n + mB/T)).	□
C Omitted Proofs from Section 3
We first present Algorithm 4, and then continue to prove Theorem 1.2. In the following algorithm,
for two sets A, B of edges, we let wA,B represent the set of pairs of edges (u, v), where u ∈ A, v ∈
B, and u, v share a common vertex.
22
Published as a conference paper at ICLR 2022
Algorithm 4 Counting Triangles in the Arbitrary Order Model
1:	Input: Arbitrary order edge stream and an oracle Oρ that outputs HEAVY if Nxy > ρ and LIGHT
otherwise.
2:	Output: An estimate of the triangle count T .
3:	Initialize P = max {源,1}, P = C max {-√^,禺^} for a constant C.
4:	Initialize '1,'2,'3 = 0, and Sl, H = 0.
5:	for every arriving edge e in the stream do
6:	if the oracle Oρ outputs HEAVY then
7:	Add e to H
8:	else
9:	Add e to SL with probability p.
10:	for each w ∈ wSL ,SL do
11:	if (e, w) is a triangle then
12:	Increment `1 = `1 + 1.
13:	for each w ∈ wSL ,H do
14:	if (e, w) is a triangle then
15:	Increment `2 = `2 + 1.
16:	for each w ∈ wH,H do
17:	if (e, w) is a triangle then
18:	Increment `3 = `3 + 1.
19:	return: ' = '1∕p2 + 'z/p + '3.
C.1 Proof of Theorem 1.2
Recall that we first assume that Algorithm 4 is given access to a perfect oracle Oρ . That is, for every
edge e ∈ E,
O ( )	LIGHT	ifTe ≤ P
ρ e	HEAVY ifTe > P,
where Te as the number of triangles incident to the edge e.
Theorem 1.2. First, We assume that T ≥ e-2. If not, our algorithm can just store all of the edges,
since m ≤ e-1 ∙ m∕√T.
For each integer i ≥ 1, let E(i) denote the set of edges that are part of exactly i triangles, and let
Ei = |E(i) |. Since each triangle has 3 edges, we have that Pi≥ι i ∙ E(i) = 3T.
Let T1 denote the set of triangles Whose first tWo edges in the stream are light (according to Oρ ).
For every triangle ti in T1 , let χi denote the indicator of the event that the first two edges of ti
are sampled to SL, and let `1 = Pi χi. Since each χi is 1 with probability p2, Ex[χi] = p2, and
Var[χi] = p2 -p4 ≤ p2. For any two variables χi, χj, they must be uncorrelated unless the triangles
ti , tj share a light edge that is among the first two edges of ti and among the first two edges of tj .
Moreover, in this case, Cov[χi, χj] ≤ Ex[χiχj] = P(χi = χj = 1). This event only happens if
the first two edges of both ti and tj are sampled in SL, but due to the shared edge, this comprises 3
edges in total, so P(χi = χj = 1) = p3. Hence, if we define ti12 as the first two edges of ti in the
stream for each triangle ti,
Eχ['ι]= p2∣T1∣,
and
Var['ι] ≤ X P2 + X P3 ≤ P2ITlI + P3 X T
ti ∈T1	ti ,tj ∈T1	Te ≤ρ
t12∩t12 = 0
ρ
= P2ITlI + p3 ∙ £t2 ∙ E(t) ≤ (p2 + 3p3 ∙ ρ) ∙ T .
t=1
The first line follows by adding the covariance terms; the second line follows since each light edge e
has at most Te2 pairs (ti, tj) intersecting at e; the third line follows by summing over t = Te, ranging
from 1 to ρ, instead over e; and the last line follows since Pt≥ι t ∙ E(t) = 3T.
23
Published as a conference paper at ICLR 2022
Now, let T2 denote the set of triangles whose first two edges in the stream are light and heavy (in
either order). For every triangle t = (e1, e2, e3) in T2, e1, e2 are sampled to SL ∪H with probability
p. Also all other triangles have no chance to contribute to '2. Hence, Ex['2] = P ∙ T2. Also,
two triangles ti , tj ∈ T2 that intersect on an edge e have Cov(χi , χj ) 6= 0 only if e is light and
e ∈ ti12, tj12. In this case.
Cov(χi, χj) ≤ Ex[χiχj] ≤ p,
since if e is added to SL (which happens with probability p) then χi = χj = 1 as the other edges in
ti12, tj12 are heavy and are added to H with probability 1. Therefore,
Var[42] = Ep +	E	P ≤ (P + 3p ∙ P) ∙ T,
ti∈T2	ti,tj ∈T2
ti12 ∩tj12 =e, Oρ (e)=LIGHT
by the same calculations as was done to compute Var['ι].
Finally, let T3 denote the set of triangles whose first two edges in the stream are heavy. Then
`3 = |T3|.
Now, using the well known fact that Var[X +Y] ≤ 2(Var[X]+Var[Y]) for any (possibly correlated)
random variables X, Y, we have that
Var[p-2'1 + p-1'2 + '3] ≤ 2 (p-4(p2 + 3p3ρ)T + p-2(p + 3PP)T)
≤ 4T (P-2 +3P-1P),
since '3 has no variance. Moreover, Ex[P-2'1 + P-1'2 + '3] = |T1 | + |T2| + |T3| = T. So, by
Chebyshev’s inequality, since ' = '1/P2 + '2/P + '3,
Pr[|' - T| > T] <
Var [']	4(p-2 + 3p-1ρ)
e2 ∙ T2 ‹
e2 ∙ T
Therefore, setting P
C ∙ max{l∕(e∙√T),ρ∕(e2 ∙ T)}
for some fixed constant C implies that,
with probability at least 2∕3, ' is a (1 ± e)-multiplicative estimate of T.
Furthermore, the expected space complexity is O(mP + H) = O(mP + T ∕P). Setting P =
max{eT∕√m, 1} implies that the space complexity is O(e-1m∕√T + e-2m∕T + e-1√m ∙ T/T)=
O(e-1m∕√T + e-1√m ∙ T∕T), since We are assuming that T ≥ e-2. Hence, assuming that T is a
constant factor approximation of T, the space complexity is O(e-1m∕√T + e-1√m).	口
C.2 Proof of Theorem 1.2 for the K-noisy oracle
In this section, we prove Theorem 1.2 for the case that the given oracle is a K-noisy oracle, as
defined in Definition 1.1. That is, we prove the following:
Theorem C.1. Suppose that the oracle in Algorithm 4 is a K -noisy oracle as defined in Defini-
tion 1.1 for a fixed constant K. Then with probability 2∕3, Algorithm 4 returns ' ∈ (1 ± e)T, and
uses space at most O (e-1 (m∕√T + √m)).
Recall that in Definition 1.1, we defined a K-noisy oracle if the following holds. For every edge e,
1 - K ∙ NP- ≤ Pr[Oρ(e) = heavy] ≤ K ∙ N. We first visualize this model. To visualize this error
model, in Figure 3 we have plotted Ne versus the range [l - K ∙ ɪ, K ∙ Ne] for K = 2. We set
Ne to vary on a logarithmic scale for clarity. For example, if Ne exceeds the threshold P by a factor
of 2, there is no restriction on the oracle output, whereas if Ne exceeds P by a factor of 4, then the
oracle must classify the edge as heavy with probability at least 0.5. In contrast, the blue piece-wise
line shows the probability Pr[Oρ(e) = HEAVY] if the oracle Oρ is a perfect oracle.
Proof of Theorem C.1. We follow the proof of Theorem 4.1 from the main paper. Let T1 be the set
of triangles such that their first two edges in the stream are light according to the oracle. Let T2
be the set of triangles such that their first two edges in the stream are determined light and heavy
according to the oracle. Finally, let T3 be the set of triangles for which their first two edges are
24
Published as a conference paper at ICLR 2022
Foq SF pəsssFQ皆 Oq ə JO □≡qpqojd
Number of triangles incident to edge e
Figure 3: PlotofNe, the number of triangles containing edge e, versus the allowed oracle probability
range Pr[Oρ (e) = HEAVY], shaded in orange. The blue piece-wise line shows the probability
Pr[Oρ(e) = HEAVY] if the oracle Oρ is a perfect oracle.
determined heavy by the oracle. Furthermore, we define χi for each triangle ti, t(e) for each edge
e, and E (i), Ei for each i ≥ 1 as done in Theorem 4.1. Finally, we define L(i) as the set of deemed
light edges that are part of exactly i triangles, and L(i) = |L(i)|.
First, note that the expectation (over the randomness of Oρ) of L(i) is at most E(i) ∙ K ∙ P, since
our assumption on Pr[Oρ(e) = heavy] tells us that Pr[Oρ(e) = light] ≤ K ∙ i if t(e) = i.
Therefore, by the same computation as in Theorem 4.1, we have that, conditioning on the oracle,
Ex['ι∣Oρ] = P|T1 ] and
Var['ι ∣Oρ] = X P2 + X	P3
ti∈T1	ti,tj ∈T1
t12 ∩j2=0
≤ P2 |T11 + P3 X t(e)2
e light
= P2ITlI + P3 ∙ Xt2 ∙ L(t)
t≥1
But since ExOP [L(t)] ≤ KP ∙ E(t) for all t, We have that
ExOP [Var['ι ∣Oρ]] ≤ ExOP p2 ∙ |T11 + p3 ∙ X t2L(t)
t≥1
=P2 [Tl i + KP ∙ P3 Xt ∙ E ⑴
t≥1
≤ (P + 3Kp3 ∙ P) ∙ T.
A similar analysis to the above shows that Ex['2] = P ∙ |T21 and that
ExOP [Var['2∣Ορ]] ≤ ExOp P ∙ ∣T21 + P ∙ X t2L(t)
t≥1
=P|T2 | + KP ∙ P X t ∙E⑴
t≥1
≤ (P + 3Kp∙ p) ∙ t.
Hence, as in Theorem 4.1, we have Ex['∣Ορ] = T and ExOP [Var['∣Ορ]] ≤ 4T ∙ (p-2 + 3Kp-1 ρ).
Thus, Ex['] = Ex[Ex['∣Ορ]] = T and Var['] = ExOP [Var['∣Ορ]]+ VarOP [Ex['∣Ορ]] ≤ 4T∙ (p-2 +
3KP-1P) by the laws of total expectation/variance. Therefore, since K is a constant, the variance is
25
Published as a conference paper at ICLR 2022
the same as in Theorem 4.1, UP to a constant. Therefore, We still have that' ∈ (1 土 O(e)) ∙ T with
probability at least 2/3.
Finally, the exPected sPace comPlexity is boUnded by
mp + X P r[Oρ(e) = heavy] ≤ mp + ^X E(t) ∙ K ∙—
e∈m	t≥1	ρ
=mp + — ∙	E(t) ∙ t
ρ
ρ t≥1
= O(mp + T /ρ),
since Pt≥ι E(t) ∙ t = 3T and K = O(1). Hence, setting P and P as in the proof of Theorem 4.1
imPlies that the retUrned valUe is a (1 ± e)-aPProximation of T, and the the sPace comPlexity is
O(e-1(m∕√T + √m)) as before.
C.3 Lower Bound
In this section, we prove a lower boUnd for algorithms in the arbitrary order model that have access
to a perfect heavy edge oracle. Here heavy means Nuv ≥ T/c for a pre-determined threshold c, and
we assUme c = o(m) and T/c > 1, as otherwise the threshold will be too small or too close to the
average to give an accurate prediction. The following theorem shows an Ω(min(m/√T, m3/2/T))
lower boUnd even with sUch an oracle.
Theorem C.2. Suppose that the threshold of the oracle c = O(mq), where 0 ≤ q < 1. Then for
any T and m, T ≤ m, there exists m0 = Θ(m) and T0 = Θ(T) such that any one-pass arbi-
trary order Streaming algorithm that distinguishes between m0 edge graphs with 0 and T0 triangles
with probability at least 2/3 requires Ω(m∕√T) space. For any T and m, T = Ω(m1+δ) where
max(θ, q 一 2) ≤ δ < 2, there exists m0 = Θ(m) and T0 = Θ(T) such that any one-pass arbitrary
order streaming algorithm that distinguishes between m0 edge graphs with 0 and T0 triangles with
probability at least 2/3 requires Ω(m3/2/T) space.
When T ≤ m, we consider the hubs graph mentioned in Kallaugher & Price (2017).
Definition C.3 (Hubs Graph, Kallaugher & Price (2017)). The hubs graph Hr,d consists of a single
vertex v with 2rd incident edges, and d edges connecting disjoint pairs of v ’s neighbors to form
triangles.
It is easy to see that in Hr,d, each edge has at most one triangle. Hence, there will not be any heavy
edges of this kind in the graph. In Kallaugher & Price (2017), the authors show an Ω(r√d) lower
bound for the hubs graph.
Lemma C.4 (Kallaugher & Price (2017)). Given r and d, there exist two distributions G1 , G2 on
subgraphs G1 and G2 of Hr,d, such that any algorithm which distinguishes them with probability at
least 2/3 requires Ω(r√d) space, where Gi has Θ(rd) edges and T(Gι) = d, T(G2) = 0.
Now, given T and m, where T ≤ m, we let r = Θ(m,T) and d = T. We can see Hr,√ has Θ(m)
edges and T triangles, and we need Ω(r√d) = Ω(m/√T) space.
When T > m, we consider the following C(m3/2/T) lower bound in Bera & Chakrabarti (2017).
Let H be a complete bipartite graph with b vertices on each side (we denote the two sides of vertices
by A and B). We add an additional N vertex blocks V1, V2, ...VN with each |Vi| = d. Alice has
an N -dimensional binary vector x. Bob has an N -dimensional binary vector y. Both x and y have
exactly N/3 coordinates that are equal to 1. Then, we define the edge sets
EAlice =	{{u, v}, u ∈ A, v ∈ Vi}
i:xi =1
and
EBob =	{{u, v}, u ∈ B, v ∈ Vi} .
i:yi=1
26
Published as a conference paper at ICLR 2022
Let the final resulting graph be denoted by G = (V, E) where V = VH ∪ V1 ∪ ... ∪ VN and
E = EH ∪ EAlice ∪ EBob.
In Bera & Chakrabarti (2017), the authors show by a reduction to the DISJNN/3 problem in commu-
nication complexity, that We need Ω(N) space to distinguish the case when G has 0 triangles from
the case when G has at least b2 d triangles.
Given m and T, T = Θ(m1+δ) where 1 ≤ δ < 1/2, as shown in Bera & Chakrabarti (2017),
we can set b = Ns and d = Ns-1 where s = 1/(1 - 2δ). This will make m = Θ(N2s) and
T = Θ(N3s-1), which will make the C(m3/2/T) lower bound hold. Note that at this time each
edge will have an O(m)-fraction of triangles or an O( JP )-fraction of triangles. Assume the
threshold of the oracle C = O(mq). When δ ≥ max(0, q 一 ɪ), there will be no heavy edges in the
graph.
Theorem C.2 follows from the above discussion.
D	Four-Cycle Counting in the Arbitrary Order Model
In the 4-cycle counting problem, for each xy ∈ E(G), define Nxy = |z, w : {x, y, z, w} ∈	| as
the number of 4-cycles attached to edge xy.
In this section, we present the one-pass Oe(T 1/3 + -2 m/T1/3) space algorithm behind Theorem 1.3
and the proof of the theorem.6
We begin with the following basic idea.
• Initialize: C J 0, S — 0. On the arrival of edge uv:
-With probability p, S J {uv} ∪ S.
- C J C + |{{w, z} : uw, Wz and Zv ∈ S}|.
• Return C/p3 .
Following the analysis in Vorotnikova (2020), we have
Var[C] ≤ O(Tp3+T∆Ep5+T∆Wp4) ,
where ∆E and ∆W denote the maximum number of 4-cycles sharing a single edge or a single
wedge.
There are two important insights from the analysis: first, this simple sampling scheme can output
a good estimate to the number of 4-cycles on graphs that do not have too many heavy edges and
heavy wedges (the definitions of heavy will be shown later). Second, assuming that we can store all
the heavy edges, then at the end of the stream we can also estimate the number of 4-cycles that have
exactly one heavy edge but do not have heavy wedges.
For the 4-cycles that have heavy wedges, we use an idea proposed in McGregor & Vorotnikova
(2020); Vorotnikova (2020): for any node pair u and v , if we know their number of common neigh-
bors (denoted by k), we can compute the exact number of 4-cycles with u, v as a diagonal pair (i.e.,
the four cycle contains two wedges with the endpoints u, v), and this counts to k2 . Furthermore,
if k is large (in this case we say all the wedges with endpoints u, v are heavy), we can detect it and
obtain a good estimation to k by a similar vertex sampling method mentioned in Section 3. We can
then estimate the total number of 4-cycles that have heavy wedges with our samples.
At this point, we have not yet estimated the number of 4-cycles having more than one heavy edge
but without heavy wedges. However, McGregor & Vorotnikova (2020) shows this class of 4-cycles
only takes up a small fraction ofT, and hence we can get a (1 ± )-approximation to T even without
counting this class of 4-cycles.
The reader is referred to Algorithm 5 for a detailed version of our one-pass, 4-cycle counting algo-
rithm. Before the stream, we randomly select a node set S, where each vertex is in S with probability
6We assume -11/6 ≤ O(e), which is the same assumption as in previous work
27
Published as a conference paper at ICLR 2022
Algorithm 5 Counting 4-cycles in the Arbitrary Order Model
1:	Input: Arbitrary Order Stream and an oracle that outputs TRUE if Nxy ≥ T/ρ and false
otherwise.
2:	Output: An estimate of the number T of 4-cycles.
3:	Initialize Al — 0, Ah — 0, Aw — 0 EL — 0 EH — 0, ES — 0, and W — 0. Set P — T1/3,
P — ae-2 log n/p for a sufficiently large a, and Let S be a random subset of nodes such that
each node is in S with probability p.
4:	while seeing edge uv in the stream do
5:	if u ∈ S or v ∈ S then
6:	ES — {uv } ∪ ES .
7:	if the oracle outputs FALSE then
8:	With probability p, EL — {uv} ∪ EL .
9:	Find pair (w, z) such that uw, wz, and zv	∈ EL,	let D — D ∪ {(u, w,	z, v)}	.
10:	else
11:	EH — {uv} ∪ EH .
12:	for each node pair (u, v) do
13:	let q(u, v) be the number of wedges with center in S and endpoints u and v.
14:	if q(u, v) ≥ pT 1/3 then
15:	Aw — Aw + (吗，v)).
16:	W — W ∪ {(u, v)} .
17:	for each 4-cycle d in D do
18:	if the end points of wedges in d are not in W then
19:	Al —Al+1
20:	for each edge uv in EH do
21:	for each 4-cycle d formed with uv and e ∈ EL do
22:	if the end points of wedges in d are not in W then
23:	Ah — Ah + 1
24:	return: Al/p3 + Ah/p3 + Aw .
p independently, and we later store all edges that are incident to S during the stream. In the mean-
time, we define the edge uv to be heavy if Nuv ≥ T2/3 and train the oracle to predict whether uv
is heavy or not when we see the edge uv during the stream. Let p = Oe(e-2/T 1/3). We store all
the heavy edges and sample each light edge with probability p during the stream. Upon seeing see
a light edge uv , we look for the 4-cycles that are formed by uv and the light edges that have been
sampled before, and then record them in set D. At the end of the stream, we first find all the node
pairs that share many common neighbors in S and identify them as heavy wedges. Then, for each
4-cycle d ∈ D, we check if d has heavy wedges, and if so, remove it from D. Finally, for each
heavy edge uv indicated by the oracle, we compute the number of 4-cycles that are formed by uv
and the sampled light edges, and without heavy wedges. This completes all parts of our estimation
procedure.
We now prove Theorem 1.3, restated here for the sake of convenience.
Theorem 1.3. There exists a one-pass algorithm, Algorithm 5, with space complexity Oe(T1/3 +
e-2m/T1/3) in the arbitrary order model that, using a learning-based oracle, returns a (1 ± e)-
approximation to the number T of four cycles with probability at least 7/10.
Proof. From the Lemma 3 in Vorotnikova (2020), we know that with probability at least 9/10, we
can get an estimate Aw such that
Aw = Tw ± eT.
Where Tw is the number of the 4-cycles that have at least one heavy wedge. We note that if a 4-
cycle has two heavy wedges, it will be counted twice. However, Vorotnikova (2020) shows that this
double counting is at most O(T2/3) = O(e)T.
For the edge sampling algorithm mentioned in D, from the analysis in Vorotnikova (2020), we have
Var[Al∕p3] ≤ O(T∕p3 + TΔe/p + T∆w/p2).
28
Published as a conference paper at ICLR 2022
Notice that in our algorithm, the threshold of the heavy edges and heavy wedges are Nuv ≥ T2/3
and Nw ≥ T 1/3, respectively, which means Var［啬］= O(e2T2). Hence, We can get an estimate Al
such that with probability at least 9/10,
Al = Tl ± eT ,
where Tl is the number of 4-cycles that have no heavy edges. Similarly, we have with probability at
least 9/10,
Ah = Th ± eT ,
where Th is the number of 4-cycles that have at most one heavy edge.
One issue is that the algorithm has not yet estimated the number of 4-cycles having more than one
heavy edge, but without heavy wedges. However, from Lemma 5.1 in McGregor & Vorotnikova
(2020) we get that the number of this kind of 4-cycles is at most O(T 5/6) = O(e)T. Hence putting
everything together, we have
|Al/p3+Ah/p3+Aw-T| ≤ O(e)T,
which is what we need.
Now we analyze the space complexity. The expected number of light edges we sample is
O(mp) = Oe(e-2m/T 1/3) and the expected number of nodes in S and edges in ES is O(2mp) =
Oe(e-2m/T1/3). The expected number of 4-cycles we store in D is O(T p3) = Oe(e-6). We store
all the heavy edges, and the number of heavy edges is at most O(T 1/3). Therefore, the expected
total space is O(T 1/3 + e-2m∕T 1/3).	□
E Runtime Analysis
In this section, we verify the runtimes of our Algorithms 1, 2, 3, 4, and 5.
Proposition E.1. Algorithm 1 runs in expected time at most O(min(e-2m5∕3∕T 1/3, e-1m3∕2)) in
the SettingofTheorem 1.1, or O(min(e-2m5∕3∕T 1/3, K ∙ e-1m3∕2)) in the SettingofTheorem 1.4.
Proof. For each edge ab ∈ SL ∪ SM, we check whether we see both va and vb (lines 9-10) when
looking at edges adjacent to v. So, this takes time |SL| + |SM| per edge in the stream. We similarly
check for each edge in Saux (lines 11-12), so this takes time |Saux|. Finally, for each edge vu (line
13), we note that lines 14-17 can trivially be implemented in O(1) time, along with 1 call to the
oracle. The remainder of the oracle simply involves looking through the set Saux and SM to search
or count for elements. Thus, the overall runtime is O(|SL| + |SM| + |Saux|) per stream element, so
the runtime is O (m ∙ (|Sl| + |Sm| + ∣Sauχ∣)).
This is at most O(m ∙ S), where S is the space used by the algorithm. So, the run-
time is O(min(e-2m5/3/T 1/3, e-1m3/2) in the setting of Theorem 1.1, and is at most
O(min(e-2m5/3/T 1/3, K ∙ e-1m3∕2)) in the setting of Theorem 1.4.	□
Proposition E.2. Algorithm 2 runs in time O(e-2 (α + mβ ∕T)m).
Proof. For each edge ab ∈ Si for each 0 ≤ i ≤ ce-2, we check whether we see both ya and yb
in the stream when looking at edges adjacent to y. So, lines 7-8 take time O(P |Si|) for each edge
we see in the stream. By storing each Si (and each Qi) in a balanced binary search tree or a similar
data structure, we can search for xy ∈ Si in time O(log n) in line 11, and it is clear that lines
12-16 take time O(log n) (since insertion into the data structure for Qi, Si can take time O(log n)).
Since we do this for each i from 0 to ce-2 and for each incident edge yx we see, in total we spend
O(e-2 log n + P |Si|) time UP to line 16. The remainder of the lines take time O(e-2 ∙ m log n),
since the slowest operation is potentially deleting an edge from each Si and a value from each Qi
up to m times (for each edge).
Overall, the runtime is O(e-2 ∙ m + m ∙ P |Si|)=O(m ∙ S), where S is the total space used by the
algorithm. Hence, the runtime is O(e2 (α + mβ∕T )m).	□
29
Published as a conference paper at ICLR 2022
Proposition E.3. Algorithm 3 runs in time O(Ke 2(α + mβ/T)m).
Proof. For each edge ab ∈ Sij for each 1 ≤ i ≤ O(log n), 1 ≤ j ≤ O(log log n), we check whether
we see both ya and yb in the stream when looking at edges adjacent to y. So, line 5 takes time
O(P |Sij |) for each edge we see in the stream. By storing each Sij in a balanced binary search tree
or a similar data structure, we can search for xy ∈ Si in time O(log n) in line 7, and it is clear that
lines 8-12 take time O(logn) (since insertion into the data structure for Qij can take time O(log n)).
Since we do this for each i from 0 to ce-2 and for each incident edge yx we see, in total we spend
Poly log n ∙O(P |Si|) time up to line 12. Finally, lines 13-15 take time poly log n.
Overall, the runtime is O(m £ |Si |)= O(m ∙ S), where S is the total space used by the algorithm.
Hence, the runtime is O(Ke 2(α + mβ/T)m).	口
Proposition E.4. Algorithm 4 runs in time O ^e-1 ∙ m2∕√T + e-1 ∙ m3/2).
Proof. For each edge e in the stream, first we check the oracle, and we either add e to H orto SL with
probability p (lines 6-9), which take O(1) time. The remaining lines (lines 10-18) involve operations
that take O(1) time for each w which represents a pair (e1, e2) of edges where e1, e2 ∈ SL ∪ H
and (e, e1, e2) forms a triangle. So, the runtime is bounded by the amount of time it takes to find all
e1, e2 ∈ SL ∪ H such that (e, e1, e2) forms a triangle. If the edge e = (u, v), we just find all edges
in SL ∪ H adjacent to u, and all edges in SL ∪ H adjacent to v. Then, we sort these edges based on
their other endpoint and match the edges if they form triangles. So, the runtime is O(|SL| + |H|)
per edge e in the stream. Finally, line 19 takes O(1) time.
Overall, the runtime is O(m ∙ (|Sl| + |H |))=O(m ∙ S) where S is the total space of the algorithm.
So, the runtime is Oe e-1 ∙ m2∕√T + e-1 ∙ m3/2) .	口
Proposition E.5. Algorithm 5 runs in time O(e-2∕T1/3 ∙ (n3 + m2)).
Proof. We note that for each edge uv in the stream (line 4), the code in the loop (lines 5-11) can
be implemented using 1 oracle call and O(|EL|) time. The only nontrivial step here is to find pairs
(w, z) such that uw, wz, zv are all in EL . However, by storing EL in a balanced binary search tree
or similar data structure, one can enumerate through each edge wz ∈ EL and determine if uw and
zv are in EL in O(log |EL|) time. So, lines 4-11 take time O(|EL|) per stream element.
Next, lines 12-16 can easily be implemented in time O(n2 ∙ |S|), lines 17-19 can be easily imple-
mented in time Oe(|D|) if the vertices in W are stored properly, and lines 20-23 can be done in
O(∣Eh| ∙ |El|) time. The last part is true since We check each Uv ∈ EH and e = (u0, v0) ∈ El,
and then check ifu, u0, v0, v form a 4-cycle by determining ifu, u0 and v, v0 are in EL ∪ EH (which
takes time O(log |EL| + log |EH |) time assuming EL, EH are stored in search tree or similar data
structure).
Overall, we can bound the runtime as O(m∙∖EL∣+n2 ∙∣S∣ + ∣D∣ + ∣Eh ∣∙∣El∣)=O(m∙∣EL∣+n2∙∣S∣).
As each edge is in EL with probability at most p and each edge is in S with probability at most p,
the total runtime is O((m2 + n3) ∙ P)=O(e-2∕T1/3 ∙ (n3 + m2)).	口
F Additional Experimental Results
All of our graph experiments were done on a CPU with i5 2.7 GHz dual core and 8 GB RAM or
a CPU with i7 1.9 GHz 8 core and 16GB RAM. The link prediction training was done on a single
GPU.
F.1 Dataset descriptions
•	Oregon: 9 graphs sampled over 3 months representing a communication network of inter-
net routers Leskovec & Krevl (2014); Leskovec et al. (2005).
30
Published as a conference paper at ICLR 2022
•	CAIDA: 98 graphs sampled approximately weekly over 2 years, representing a communi-
cation network of internet routers Leskovec & Krevl (2014); Leskovec et al. (2005).
•	Reddit Hyperlinks: Network where nodes represent sub communities (subreddits) and
edges represent posts that link two different sub communities Leskovec & Krevl (2014);
Kumar et al. (2018).
•	WikiBooks: Network representing Wikipedia users and pages, with editing relationships
between them Rossi & Ahmed (2015).
•	Twitch: A user-user social network of gamers who stream in a certain language. Vertices
are the users themselves and the links are mutual friendships between them. Rozemberczki
et al. (2019)
•	Wikipedia: This is a co-occurrence network of words appearing in the first million bytes
of the Wikipedia dump. Mahoney (2011)
•	Synthetic Power law: Power law graph sampled from the Chung-Lu-Vu (CLV) model with
the expected degree of the ith vertex proportional to 1/i2 Chung et al. (2003). To create this
graph, the vertices are ‘revealed’ in order. When the jth vertex arrives, the probability of
forming an edge between the jth vertex and ith vertex for i < j is proportional to 1/(ij)2.
F.2 Details on link prediction oracle
Our oracle for the Twitch and Wikipedia graphs is based on Link Prediction, where the task is to
estimate the likelihood of the existence of edges or to find missing links in the network. Here we
use the method and code proposed in Zhang & Chen (2018). For each target link, it will extract
a local enclosing subgraph around a node pair, and use a Graph Neural Network to learn general
graph structure features for link prediction. For the Twitch network, we use all the training links
as the training data for the graph neural network, while for the Wikipedia network, we use 30% of
the training links as the training data due to memory limitations. For the Twitch network, our set of
links that we will try to predict are between two nodes that have a common neighbor in the training
network, but do not form a link in the training network. This is about 3.8 million pairs, and we
call this the candidate set. We do this for memory considerations. For the remaining node pairs,
we set the probability they form a link to be 0. For the Wikipedia network, we randomly select a
link candidate set of size 20 times the number of edges (about 1 million pairs) from the entire set of
testing links. These link candidate sets will be used by the oracle to determine heaviness. Then, we
use this network to do our prediction for all links in our candidate sets for the two networks.
Now we are ready to build our heavy edge oracle. For the adjacency list model, when we see the
edge uv in the stream, we know all the neighbors of u, and hence, we only need to predict the
neighbors of v. Let deg(v) be the degree of v in the training graph. The training graph and testing
graph are randomly split. Hence, we can use the training set to provide a good estimation to the
degree of v. Next, we choose the largest deg(v) edges incident to v from the candidate set, in terms
of their predicted likelihood given by the the neural network, as our prediction of N (v), which leads
to an estimate of Ruv. For the arbitrary order model, we use the same technique as above to predict
N(u) and N (v), and estimate Nuv.
F.3 Parameter Selection for Algorithm 2
We need to set two parameters for Algorithm 2: p, which is the edge sampling probability, and
θ, which is the heaviness threshold. In our theoretical analysis, we assume knowledge of a lower
bound on T in order to set p and θ, as is standard in the theoretical streaming literature. However, in
practice, such an estimate may not be available; in most cases, the only parameter we are given is a
space bound for the number of edges that can be stored. To remedy this discrepancy, we modify our
algorithm in experiments as follows.
First, we assume we only have access to the stream, a space parameter Z indicating the maximum
number of edges that we are allowed to store, and an estimate of m, the number of edges in the
stream. Given Z, we need to designate a portion of it for storing heavy edges, and the rest for
storing light edges. The trade-off is that the more heavy edges we store, the smaller our sampling
probability of light edges would be. We manage this trade off in our implementation by reserving
0.3 ∙ Z of the edge 'slots' for heavy edges. The constant 0.3 is fixed throughout all our experiments.
31
Published as a conference paper at ICLR 2022
We then set P = 0.7 ∙ Z/m. To improve the performance of our algorithm, We optimize for space
usage by always insuring that we are storing exactly Z space (after observing the first Z edges of the
stream). To do so and still maintain our theoretical guarantees, We perform the folloWing procedure.
Call the first Z edges of the stream the early phase and the rest of the edges the late phase.
We always keep edges in the early phase to use our space allocation Z and also keep track of the
0.3-fraction of the heaviest edges. After the early phase is over, i.e., more than Z edges have passed
in the stream, if a new incoming edge is heavier than the lightest of the stored heavy edges, we
replace the least heavy stored edge with the new arriving edge and re-sample the replaced edge with
probability p. Otherwise, if the new edge is not heavier than the lightest stored edge, we sample the
new incoming edge with probability p. If we exceed Z, the space threshold, we replace one of the
light edges sampled in the early phase. Then similarly as before, we re-sample this replaced edge
with probability P and continue this procedure until one of the early light edges has been evicted. In
the case that there are no longer any of the light edges sampled in the early phase stored, we replace
a late light edge and then any arbitrary edge (again performing the re-sampling procedure for the
evicted edge).
Note that in our modification, we only require our predictor be able to compare the heaviness be-
tween two edges, i.e., we do not need the predictor to output an estimate of the number of triangles
on an edge. This potentially allows for more flexibility in the choice of predictors.
If the given space bound meets the space requirements of Theorem C.1, then the theoretical guaran-
tees of this modification simply carry over from Theorem 1.2: we always keep the heaviest edges
and always sample light edges with probability at least p. In case the space requirements are not
met, the algorithm stores the most heavy edges as to reduce the overall variance.
F.4 Additional Figures from Arbitrary Order Triangle Counting Experiments
Additional figures for our arbitrary order triangle counting experiments are given in Figures 4 and 5.
0.25
0.20
0.15
0.10
0.05
0.00
Figure 4: Error as a function of space for various graph datasets.
F.5 Experimental Design for Adjacency List Experiments
We now present our adjacencly list experiments. At a high level overview, similarly to the arbitrary
order experiments, for our learning-based algorithm, we reserve the top 10% of the total space for
storing the heavy edges. To do this in practice, we can maintain the heaviest edges currently seen so
far and evict the smallest edge in the set when a new heavy edge is predicted by the oracle and we no
longer have sufficient space. We also consider a multi-layer sub-sampling version of the algorithm
in section B.2. Here we use more information from the oracle by adapting the sub-sampling rates
of edges based on their predicted value. For more details, see section F.5. our results are presented
in Figure 6 (with additional plots given in Figure 7). our algorithms soundly outperform the MVV
baseline for most graph datasets. We only show the error bars for the multi-layer algorithm and
MVV for clarity. Additional details follow.
We use the same predictor for Nxy in section 4 as a prediction for Rxy . The experiment is done
under a random vertex arrival order. For the learning-based algorithm, suppose Z is the maximum
number of edges that we are allowed to store. we set the k = Z/10 edges with the highest predicted
Ruv values to be the heavy edges, and store them during the stream (i.e., we use 10% of the total
32
Published as a conference paper at ICLR 2022
0.30
0.25
e o.2o
^0.15
H 0.10
0.05
0.00
Calda-2006, GraPh#20
Our A⅛
ThInkD
MW
0.30
0.25
e o.2o
^0.15
H 0.10
0.05
Ca Ida-2 006, Graph⅛40
0 I lO4 2 IO4 3 IO4 4-IO4
Space
(b) CAIDA 2006, Graph #40
Space
(a) CAIDA 2006, Graph #20
Oregon, GraPh#3
0.25
0.20
g
m 0.15
S
E
E o.io
0.05
o∙M
Our A⅛
ThInkD
MW
WRS
5 ∙ IO3
2 IO4
Space
000O
(c) Oregon, Graph #3
Figure 5: Error as a function of space for various graph datasets.
0.25
0.20-
£ 0.15
W
E
法 0.10
0.05
Oregon, Graph#7
5 IO3 l∙104	1.5∙IO4 2∙
Space
(d) Oregon, Graph #7
space for storing the heavy edges). For the remaining edges, we use a similar sampling-based
method. Note that it is impossible to know the k-heaviest edges before we see all the edges in the
stream. However, in the implementation we can maintain the k heaviest edges we currently have
seen so far, and evict the smallest edge in the set when a new heavy edge is predicted by the oracle.
We also consider the multi-layer sub-sampling algorithm mentioned in Section B.2, which uses more
information from the oracle. We notice that in many graph datasets, most of the edges having very
few number of triangles attached to. Taking an example of the Oregon and CAIDA graph, only
about 3%-5% of edges will satisfy Re ≥ 5 under a random vertex arrival order. Hence, for this
edges, intuitively we can estimate them using a slightly smaller space.
For the implementation of this algorithm(we call it multi-layer version), we use 10% of the total
space for storing the top k = Z/10 edges, and 70% of the space for sub-sampling the edges that the
oracle predict value is very tiny(the threshold may be slightly different for different datasets, like
for the Oregon and CAIDA graph, we set the threshold to be 5). Then, we use 20% of the space for
sub-sampling the remaining edges, for which we call them the medium edges.
F.6 Figures from Adjacency List Triangle Counting Experiments
Calda-2006 #30
Jαu",α>Ae-SH
l∙10t	2∙10t	3∙104	4∙10*
Space
(a) CAIDA 2006
0.5∙10t l∙10t 1.5∙10t 2∙10t 2.5 TO4 3∙10t
Space
(b) WikiPedia
PoWerlaW
l∙10t 2∙10t 3∙10t 4∙10t 5∙10t 6∙10t
Space
(c) Powerlaw
Figure 6: Error as a function of space in the adjacency list model.
Additional figures from the adjacency list triangle counting experiments are shown in Figure 7. They
are qualitatively similar to the results presented in Figure 6 as the multi-layer sampling algorithm is
superior over the MVV baseline for all of our datasets.
33
Published as a conference paper at ICLR 2022
4-10∙5	8 10j 1,2∙ Itr 1.6, Itr
Space
(a) Oregon
(b) CAIDA 2007
(c) Wikibooks
O.1O-
TWltCh
(d) Twitch
Figure 7: Error as a function of space for various graph datasets.
J0t,,j9n*j-9h
F.7 Accuracy of the Oracle
In this section, we evaluate the accuracy of the predictions the oracle gives in our experiments.
Value Prediction Oracle : We use the prediction of Nxy in Section 4 as a value prediction for
Rxy (x ≤s y), under a fixed random vertex arrival order. The results are shown in Figure 8. For a
fixed approximation factor k, we compute the failure probability δ of the value prediction oracle as
follows: δ = #/m, where m is the number of total edges and # equals to the number of edges e
that p(e) ≥ kɑRe + β or p(e) ≤ 1 Re - β, respectively. Here We set α = 1,β = 10 for all graph
datasets.
We can see for the smaller side, there are very few edges e such that p(e) ≤ 1 Re — β. This meets
the assumption of the exponential decay tail bound of the error. For the larger side, it also meets the
assumption of the linear decay tail bound on most of the graph datasets.
F.8 Details on Oracle Training Overhead
The overhead of the oracles used in our experiments vary from task to task. For the important
use case illustrated by the Oregon and CAIDA datasets in which we are interested in repeatedly
counting triangles over many related streams, we can pay a small upfront cost to create the oracle
which can be reused over and over again. Thus, the time complexity of building the oracle can be
amortized over many problem instances, and the space complexity of the oracle is relatively small
as we only need to store the top 10% of heavy edges from the first graph (a similar strategy is used
in prior work on learning-augmented algorithms in Hsu et al. (2019b)). To give more details, for
this snapshot oracle, we simply calculate the Ne values for all edges only in the first graph. We then
keep the heaviest edges to form our oracle. Note that this takes polynomial time to train. The time
complexity of using this oracle in the stream is as follows: when an edge in the stream comes, we
simply check if it’s among the predicted heavy edges in our oracle. This is a simple lookup which
can even be done in constant time using hashing.
For learning-based oracles like the linear regression predictor, we similarly need to pay an upfront
cost to train the model, but the space required to store the trained model depends on the dimension
of the edge features. For the Reddit dataset with 〜300 features, this means that the storage cost
for the oracle is a small fraction of the space of the streaming algorithm. Training of this oracle
can be done in polynomial time and can even be computed in a stream via sketching and sampling
34
Published as a conference paper at ICLR 2022
OregOn
4	6	8
Approximation Factor k
0.035
0.035
——larger side
I— Smallerslde
CAlDA-2006
0 000-------2---------4---------6---------8--------10	0 000
Approximation Factor k
C AIDA-2 007
Approximation Factor k
(a) Oregon	(b) CAIDA 2006	(c) CAIDA 2007
I
⅛ 0.004
ra
0.002
Wlkltwols
2	4	6
Approximation Factor k
(d) Wikibooks
0.175
_ ____ ____ ____ ♦
ff=qeqojd 2n=£
0.025-
o.ooo∙
Fvttch
Approximation Factor k
(e) Twitch
ff=qeqojd 2n"d
Wlklpedla
2	4	6	8	10
Approximation Factor k
⑴ WikiPedia
0.35
Random
0.30
⅛0.25
Ξ
2 0.20
e
g 0.15
2 o.io
0.05
o.oo∙
Approximation Factor k
(g) Powerlaw
larger side
Smallerslde
Figure 8: Failure Probability as a function of aPProximation factor k for various graPh datasets.
techniques which reduce the number of constraints from m (number of edges) to roughly linear in
the number of features.
Our exPected value oracle exPloits the fact that our inPut graPh is samPled from the CLV random
graPh model. Given this, we can exPlicitly calculate the exPected value of Ne for every edge which
requires no training time and nearly constant sPace. For training details for our link Prediction
model, see Section F.2. Note that in general, there is a wide and rich family of Predictors which can
be trained using sublinear sPace or in a stream such as regression Woodruff (2014), classification for
examPle using SVMs Andoni et al. (2020); Rai et al. (2009) and even deeP learning modelsGomes
et al. (2019).
G Implicit Predictor in Prior Works
We Prove that the first Pass of the two Pass triangle counting Algorithm given in Section 3.1 of
McGregor et al. (2016), satisfies the conditions of the K-noisy oracle in Definition 1.1. Therefore,
our work can be seen as a generalization of their aPProach when handling multiPle related data sets,
where instead of Performing two Passes on each data set and using the first of which to train the
heavy edge oracle, we Perform the training once according to the first related dataset, and we get a
one Pass algorithm for all remaining sets.
We first recall the first Pass of (McGregor et al., 2016, Section 3.1):
1.	Sample each node Z of the graph with probability P = Ce-2 log m/p for some large con-
stant C > 0. Let Z be the set of samPled nodes.
35
Published as a conference paper at ICLR 2022
2.	Collect all edges incident on the set Z.
CTrl	1	C ~1	1 , S"7 ∖ I r _ rz	_ AT/ ∖ ~1 I FFC ,1	1
3.	For any edge e = {u, v}, let t(e) = |{z ∈ Z : u, v ∈ N(z)}| and define the oracle as
oracle(e)
LIGHT
HEAVY
if t(e) < p ∙ P
if t(e) ≥ P ∙ p.
Lemma G.1. For any edge e = (u, V), oracle(e) = LIGHT implies Ne ≤ 2p and oracle(e)
HEAVY implies Ne > p/√2 Withfailure probability at most 1/n10.
T-I /»	1—1	1	∙ ,	1' 11	1 1	7，	∖ ɪʌ ∙ /AT ∖ r-ɪ-ɪl	l' ∙ 1' A 7^	rʌ
Proof. For	any edge e,	it	follows	that t(e)〜Bm(Ne,p). Therefore if Ne	> 2p,
Pr[t(e) < p ∙ p] ≤ exp(-Ω(p ∙ p)) ≤ 1/n10
by picking C large enough in the definition of p. The other case follows similarly.	□
Lemma G.2. The expected space used by the above oracle is O(pm).
Proof. Each vertex is sampled in Z with probability p and we keep all of its incident edges. There-
fore, the expected number of edges saved is O(P Pv dv)=O(Pm).	□
Note that the oracle satisfies the conditions of the K-noisy oracle in Definition 1.1. For example, if
Ne ≥ C0p for C0 1, Definition 1.1 only assumes that we incorrectly classify e with probability
1/C0, whereas the oracle presented above incorrectly classifies e with probability exp(-C0)/n10
which is much smaller than the required 1/C0.
H	Learnability Results
In this section, we give formal learning bounds for efficient predictor learning for edge heaviness.
In particular, we wish to say that a good oracle or predictor for edge heaviness and related graph
parameters can be learned efficiently using few samples if we observe graph instances drawn from
a distribution. We can view the result of this section, which will be derived via the PAC learning
framework, as one formalization of data driven algorithm design. Our results are quite general but
we state simple examples throughout the exposition for clarity. Our setting is formally the following.
Suppose there is an underlying distribution D which generates graph instances H1,H2,… all on
n vertices. Note that this mirrors some of our experimental setting, in particular our graph datasets
which are similar snapshots of a dynamic graph across time.
Our goal is to efficiently learn a good predictor f among some family of functions F . The input
of each f is a graph instance H and the output is a feature vector in k dimensions. The feature
vector represents the prediction of the oracle and can encapsulate a variety of different meanings.
One example is when k = |E | and f outputs an estimate of edge heaviness for all edges. Another
is when k << |E | and f outputs the id’s of the k heaviest edges. We also think of each input
instance H as encoded in a vector in Rp forP ≥ n2 (for example, each instance is represented as an
adjacency matrix). Note that We allow for P > Q) if for example, each edge or vertex for H 〜D
also has an endowed feature vector.
To select the ‘best’ f, we need to precisely define the meaning of best. Note that in many settings,
this involves a loss function which captures the quality ofa solution. Indeed, suppose we have a loss
function L : f × H → R which represents how well a predictor f performs on some input H. An
example ofL could be squared error from the output off to the true edge heaviness values of edges
in H. Note that such a loss function clearly optimizes for predicting the heavy edges well.
Our goal is to learn the best function f ∈ F which minimizes the following objective:
EH 〜D [L(f, H)].	(1)
Let f * be such the optimal f ∈ F, and assume that for each instance H and each f ∈ F, f (H) can
be computed in time T(P, k). For example, suppose graphs drawn from D possess edge features in
Rd, and that our family Fis parameterized by a single vector θ ∈ Rd and represents linear functions
36
Published as a conference paper at ICLR 2022
which outputs the dot product of each edge feature with θ. Then it is clear that T (p, k) is a (small)
polynomial in the relevant parameters.
Our main result is the following.
Theorem H.1. There is an algorithm which after poly(T (p, k), 1/) samples, returns a function f
that satisfies
一 .^ . , 一 . 、一
EH〜D[L(f,H)] ≤ EH〜D[L(f*,H)] + e
with probability at least 9/10.
We remark that one can boost the probability of success to 1 - δ by taking additional log(1∕δ)
multiplicative samples.
The above theorem is a PAC-style bound which shows that only a small number of samples are
needed in order to ensure a good probability of learning an approximately-optimal function f. The
algorithm to compute f is the following: we simply minimize the empirical loss after an appropriate
number of samples are drawn, i.e., we perform empirical risk minimization. This result is proven
by Theorem H.3. Before introducing it, we need to define the concept of pseudo-dimension for a
function class which is the more familiar VC dimension, generalized to real functions.
Definition H.2 (Pseudo-Dimension, Definition 9 Lucic et al. (2018)). Let X be a ground set and F
be a set of functions from X to the interval [0,1]. Fix a set S = {xι, .一，xn} ⊂ X, a set of real
numbers R = {rι, ∙∙∙ , rn} with r ∈ [0,1] and afunCtion f ∈ F. The set Sf = {xi ∈ S | f (xi) ≥
ri } is called the induced subset of S formed by f and R. The set S with associated values R is
shattered by F if |{Sf | f ∈ F}| = 2n. The pseudo-dimension ofF is the cardinality of the largest
shattered subset of X (or ∞).
The following theorem relates the performance of empirical risk minimization and the number of
samples needed, to the notion of pseudo-dimension. We specialize the theorem statement to our
situation at hand. For notational simplicity, we define A be the class of functions in f composed
with L:
A = {L ◦ f : f ∈F}.
Furthermore, by normalizing, we can assume that the range of L is equal to [0, 1].
Theorem H.3 (Anthony & Bartlett (1999)). Let D be a distribution over graph instances and A
be a class of functions a : H → [0, 1] with pseudo-dimension dA. Consider t i.i.d. samples
H1,H2,..., Ht from D. There is a universal constant co, such thatfor any e > 0, if t ≥ co ∙ d//e2,
then we have
1	t
-ɪ2 α (Hi)- EH〜D a(H) ≤ e
t i=1
for all a ∈ A with probability at least 9/10.
The following corollary follows from the triangle inequality.
Corollary H.4. Consider a set of t independent samples Hi,..., Ht from D and let ^ be afunction
in A which minimizes t Pt=i α(Hi). Ifthe number ofsamples t is chosen as in Theorem H.3, then
EH〜D [^(H)] ≤ EH〜D [a* (H)]+ 2e
holds with probability at least 9/10.
The main challenge is to bound the pseudo-dimension of our given function class A. To do so, we
first relate the pseudo-dimension to the VC dimension of a related class of threshold functions. This
relationship has been fruitful in obtaining learning bounds in a variety of works such as Lucic et al.
(2018); Izzo et al. (2021).
Lemma H.5 (Pseudo-dimension to VC dimension, Lemma 10 in Lucic et al. (2018)). For any
a ∈ A, let Ba be the indicator function of the region on or below the graph ofa, i.e., Ba (x, y) =
sgn(a(x) - y). The pseudo-dimension ofA is equivalent to the VC-dimension of the subgraph class
BA = {Ba | a ∈ A}.
Finally, the following theorem relates the VC dimension of a given function class to its computa-
tional complexity, i.e., the complexity of computing a function in the class in terms of the number
of operations needed.
37
Published as a conference paper at ICLR 2022
Lemma H.6 (Theorem 8.14 in Anthony & Bartlett (1999)). Letw : Rα ×Rβ → {0, 1}, determining
the class
W = {x → w(θ, x) : θ ∈ Rα}.
Suppose that any w can be computed by an algorithm that takes as input the pair (θ, x) ∈ Rα × Rβ
and returns w(θ, x) after no more than r of the following operations:
•	arithmetic operations +, -, ×, and / on real numbers,
•	jumps conditioned on >, ≥, <, ≤, =, and = comparisons of real numbers, and
•	output 0, 1,
then the VC dimension of W is O(α2r2 + r2α log α).
Combining the previous results allows us prove Theorem H.1. At a high level, we are instantiating
Lemma H.6 with the complexity of computing any function in the function class A.
Proof of Theorem H.1. First by Theorem H.3 and Corollary H.4, it suffices to bound the pseudo-
dimension of the class A = L ◦ F. Then from Lemmas H.5, the pseudo-dimension of A is the VC
dimension of threshold functions defined by A. Finally from Lemma H.6, the VC dimension of the
appropriate class of threshold functions is polynomial in the complexity of computing a member of
the function class. In other words, Lemma H.6 tells us that the VC dimension of BA as defined in
Lemma H.5 is polynomial in the number of arithmetic operations needed to compute the threshold
function associated to some a ∈ A. By our definition, this quantity is polynomial in T (p, k). Hence,
the pseudo-dimension of G is also polynomial in T(p, k) and the result follows.	□
Note that we can consider initializing Theorem H.1 with specific predictions. If the family of oracles
we are interested in is efficient to compute (which is the case of the predictors we employ in our
experiments), then Theorem H.1 assures us that only polynomially many samples are required (in
terms of the computational complexity of our function class), to be able to learn a nearly optimal
oracle. Furthermore, computing the empirical risk minimizer needed in Theorem H.1 is also efficient
for a wide verity of function classes. For example in practice, we can simply use gradient descent
or stochastic gradient descent for a range of predictor models, such as regression or even general
neural networks.
We remark that our learnability result is in similar in spirit to one given in the recent learning-
augmented paper Dinitz et al. (2021). There, they derive sample complexity learning bounds for
the different algorithmic problem of computing matchings in a graph (not in a stream). Since they
specialize their analysis to a specific function class and loss function, their bounds are tighter com-
pared to the possibly loose polynomial bounds we have stated. However, our analysis above is
more general as it allows for a variety of predictors and loss functions to measure the quality of the
predictions.
38