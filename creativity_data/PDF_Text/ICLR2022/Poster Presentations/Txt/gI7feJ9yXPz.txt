Published as a conference paper at ICLR 2022
High Probability Generalization Bounds with
Fast Rates for Minimax Problems
Shaojie Li1,2, Yong Liu1,2,*
1 Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
2020000277@ruc.edu.cn, liuyonggsai@ruc.edu.cn
Ab stract
Minimax problems are receiving an increasing amount of attention in a wide range
of applications in machine learning (ML), for instance, reinforcement learning, ro-
bust optimization, adversarial learning, and distributed computing, to mention but
a few. Current studies focus on the fundamental understanding of general minimax
problems with an emphasis on convergence behavior. As a comparison, there is
far less work to study the generalization performance. Additionally, existing gen-
eralization bounds are almost all derived in expectation, and the high probability
bounds are all presented in the slow order O(1/√n), where n is the sample size.
In this paper, we provide improved generalization analyses and obtain sharper
high probability generalization bounds for most existing generalization measures
of minimax problems. We then use the improved learning bounds to establish
high probability generalization bounds with fast rates for classical empirical sad-
dle point (ESP) solution and several popular gradient-based optimization algo-
rithms, including gradient descent ascent (GDA), stochastic gradient descent as-
cent (SGDA), proximal point method (PPM), extra-gradient (EG), and optimistic
gradient descent ascent (OGDA). In summary, we provide a systematical analysis
of sharper generalization bounds of minimax problems.
1 Introduction
Minimax learning problems have achieved great success over a broad range of learning tasks in ma-
chine learning, with examples including reinforcement learning (Du et al., 2017; Dai et al., 2018),
robust optimization (Chen et al., 2017; Namkoong & Duchi, 2017), adversarial learning (Goodfel-
low et al., 2014), distributed computing (Razaviyayn et al., 2020; Shamma, 2008; Mateos et al.,
2010), and AUC maximization (Lei & Ying, 2021b), to just name a few. This framework is formu-
lated as a zero-sum game characterized as two groups of decision variables, one for minimization
and one for maximization. The coupling of the two groups of variables makes analysis of minimax
problems more complex than the standard statistical learning theory setting, with only one mini-
mization operator (Liu et al., 2021b; Yin et al., 2020; Li & Liu, 2021; Li et al., 2018; Liu et al.,
2020; Li & Liu, 2021). Researchers have designed various optimization algorithms, for instance,
gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method
(PPM), extra-gradient (EG), and optimistic gradient descent ascent (OGDA), to solve the minimax
optimization problem (Farnia & Ozdaglar, 2021). Current theoretical research in ML literature is
mainly devoted to the convergence rate and optimality of these minimax optimization algorithms in
different setting, such as convex-concave settings (Nemirovski et al., 2008), nonconvex-concave set-
ting (Rafique et al., 2018), strongly convex-strongly-concave setting (Balamurugan & Bach, 2016),
and nonconvex-nonconcave setting (Liu et al., 2021a; Yang et al., 2020). In contrast, there is far
less work on the generalization performance analysis, which is an important measure to indicate the
performance of the learned model based on training samples when generalized to the test data.
To the best of our knowledge, there is only three work on the generalization bounds of minimax op-
timization algorithms (Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). Among them,
* Corresponding Author.
1
Published as a conference paper at ICLR 2022
(Zhang et al., 2021a) studies the generalization bounds for ESP solution to minimax problems, (Far-
nia & Ozdaglar, 2021) analyzes the generalization properties of several gradient-based optimization
algorithms: GDA, SGDA, GDmax and PPM, and (Lei et al., 2021) provides a systematical gen-
eralization analysis of SGDA. However, in the above-mentioned papers, almost all generalization
bounds are derived in expectation. Only two high probability bounds exist, proposed in (Lei et al.,
2021). Unfortunately, they are of the slow order O (1/√n).
It is known that the high probability bound is beneficial to understand the robustness of optimization
algorithms (Bousquet et al., 2020; Klochkov & Zhivotovskiy, 2021) and is much more challenging
to be derived (Bousquet et al., 2020; Lei et al., 2021; Lv et al., 2021). In this paper, our goal is
to provide the sharper high probability generalization bounds for minimax learning problems. We
leverage the lens of algorithmic stability, which is also served as an important tool in (Zhang et al.,
2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). Our contributions are summarized below.
1.	In view of the coupling construction between the minimization variable and the maximization
variable, minimax learning problems have many generalization measures (Lei et al., 2021; Farnia
& Ozdaglar, 2021; Zhang et al., 2021a). In this paper, we provide improved stability analyses for
almost all existing generalization measures, based on which we establish sharper high probability
generalization bounds. These developed learning bounds can be employed to derive generalization
bounds with fast rates for stable minimax learning algorithms.
2.	The generalization performance of the ESP solution and gradient-based optimization algorithms
stands a central place in the learning theory of minimax problems (Lei et al., 2021). In this paper, we
develop high probability generalization bounds with fast rates for ESP solution and several popular
gradient-based optimization algorithms: GDA, SGDA, PPM, EG, and OGDA. Overall, we provide
a systematical analysis of sharper generalization bounds for minimax learning problems.
2	Related Work
Algorithmic stability. Algorithmic stability is a fundamental concept in learning theory (Bousquet
& Elisseeff, 2002), which has a deep connection with learnability (Rakhlin et al., 2005; Shalev-
Shwartz & Ben-David, 2014; Shalev-Shwartz et al., 2010). A training algorithm is stable if small
changes in the training set lead to small differences in the output predictions of the trained model.
Different algorithmic stability measures have been developed, including uniform stability (Bousquet
& Elisseeff, 2002; Feldman & Vondrak, 2018; 2019; Klochkov & Zhivotovskiy, 2021; Hardt et al.,
2016; Lei et al., 2020), uniform argument stability (Liu et al., 2017; Bassily et al., 2020), hypothesis
stability (Bousquet & Elisseeff, 2002; Charles & Papailiopoulos, 2018), hypothesis set stability (Fos-
ter et al., 2019), on average stability (Shalev-Shwartz et al., 2010; Lei & Ying, 2020; Kuzborskij &
Lampert, 2018; Zhang et al., 2021b; Lei & Ying, 2021a), locally elastic stability (Deng et al., 2021),
collective stability (London et al., 2016), and PAC-Bayesian stability (Li et al., 2020). These stabil-
ity measures have been extensively studied in the generalization analysis of the standard statistical
learning theory setting (Chen et al., 2018; Zhang et al., 2021b). Several stability measures have also
been extended to minimax learning problems, for instance, weak stability, argument stability, and
uniform stability (Farnia & Ozdaglar, 2021; Zhang et al., 2021a; Lei et al., 2021). In related work
(Farnia & Ozdaglar, 2021; Zhang et al., 2021a; Lei et al., 2021), they mostly focus on the expecta-
tion form of these stability measures since they are to derive bounds in expectation. In this paper,
we will focus on the last two measures, which are often used when establishing high probability
generalization bounds (Feldman & Vondrak, 2018; 2019; Klochkov & Zhivotovskiy, 2021).
Convergence analysis. Convergence analysis has been widely studied in different settings, includ-
ing convex-concave learning (Nemirovski, 2005; Nedic & Ozdaglar, 2009; Mokhtari et al., 2020;
Cherukuri et al., 2017; Mokhtari et al., 2019; Balamurugan & Bach, 2016; Hsieh et al., 2019; Yan
et al., 2020; Lin et al., 2020b; Wang & Li, 2020; Yoon & Ryu, 2021), nonconvex-concave learning
(Rafique et al., 2018; Kong & Monteiro, 2019; Luo et al., 2020; Grnarova et al., 2017; Thekumpara-
mpil et al., 2019; Lu et al., 2020; Namkoong & Duchi, 2016; Sanjabi et al., 2018; Nouiehed et al.,
2019; Lin et al., 2020a; Sinha et al., 2017; Chen et al., 2021), and nonconvex-nonconcave learning
(Heusel et al., 2017; Balduzzi et al., 2018; Daskalakis & Panageas, 2018; Mertikopoulos et al., 2019;
Loizou et al., 2020; Yang et al., 2020; Liu et al., 2021a; Lin et al., 2018; Diakonikolas et al., 2021;
Wang et al., 2020; Loizou et al., 2021; Fiez & Ratliff, 2021). There are so many studies on conver-
gence. Thus, considering the length limit, the references listed here are not complete. Please refer
2
Published as a conference paper at ICLR 2022
to the related references concerning the above work. We investigate the generalization performance
of minimax problems instead of the convergence behavior. Note that the convergence analysis also
plays an essential role in this paper, formalized as strong PD empirical risk (please refer to Defi-
nition 1), which is defined on the function value difference and referred to as optimization error or
primal-dual gap in some convergence literature (Lei et al., 2021; Nemirovski, 2005; Mokhtari et al.,
2019; 2020).
3	Preliminaries
Let X and Y be two parameter spaces in Rd . Let P be a probability measure defined on a sample
space Z . We define f : X × Y × Z 7→ R and consider the following minimax optimization problem
minmaχF(x,寸)=Ez〜p[f (x, y； z)]∙	(1)
x∈X y∈Y
The above minimax objective represents an expectation of a cost function f(x, y； z) for minimiza-
tion variable x, maximization variable y and data variable z . Unfortunately, we typically are not
available to the underlying distribution P. In practice, F is approximated by the corresponding em-
pirical risk. Let S = {z1, ..., zn} be a dataset whose samples are independent drawn according to P,
the empirical risk is defined as
1n
FS (χ, y) = n∑^f(χ, y; zi).	⑵
Let the output ofa (randomized) algorithm A on a dataset S be A(S) := (Ax(S), Ay(S)) ∈ X × Y.
Since A(S) is just an empirical approximated solution of the true minimax optimization problem,
we are interested in studying how well A(S) generalizes to the unseen data. As claimed in (Farnia
& Ozdaglar, 2021; Lei et al., 2021), the coupling between the minimization variable and the maxi-
mization variable in (1) makes minimax problems have many different generalization performance
measures. These measures are collected in (Lei et al., 2021). For better readability, we use their
symbols. Let E be the expectation with respect to (w.r.t.) the randomness of algorithm A and the
dataset S. These generalization measures are listed below.
Definition 1. (Lei et al., 2021) There are four groups of generalization measures.
1 (Primal Measures.) The primal population risk of a model x is defined as R(x) =
supy∈Y F(x, y), and the corresponding primal empirical risk is defined as RS (x)	=
supy∈Y FS (x, y). Then, when using empirical risk RS (x) to bound R(x), we call this error of
the model x the primal generalization error. While using optimal inf x∈X R(x) to bound R(x), we
call this error of the model x the excess primal population risk.
2 (Plain Measure.) When using FS (x, y) to bound F(x, y), we call the this error of a model (x, y)
the plain generalization error.
3 (Strong Measures.) The strong primal-dual (PD) population risk of a model (x, y) is defined as
42 3 4 s (x, y) = sup F (x, y0) - inf F(x0, y),
y0∈Y	x0∈X
and the corresponding strong PD empirical risk is defined as
4sS (x, y) = sup FS (x, y0) - inf FS(x0, y).
y0∈Y	x0∈X
Then, the strong PD generalization error 4s(x, y) - 4sS (x, y) of the model (x, y) is defined as
sup F(x, y0) - sup FS(x,y0) + inf FS(x0,y) - inf F (x0, y).
y0∈Y	y0∈Y	x0∈X	x0∈X
4 (Weak Measures.) The weak PD population risk of a (randomized) model (x, y) is defined as
4w (x, y) = sup E[F (x, y0)] - inf E[F (x0, y)],
y0∈Y	x0∈X
and the corresponding weak PD empirical risk is defined as
4Sw(x,y) = sup E[FS (x, y0)] - inf E[FS(x0,y)].
y0∈Y	x0∈X
3
Published as a conference paper at ICLR 2022
Then, the weak PD generalization error 4w(x, y) - 4Sw (x, y) of the model (x, y) is defined as
sup E[F (x, y0)] - sup E[FS(x,y0)] + inf E[FS (x0, y)] - inf E[F (x0, y)].
y0 ∈Y	y0 ∈Y	x0 ∈X	x0 ∈X
Remark 1. We provide some discussions for the four groups of measures. (1. Primal Measures:)
In the context of GANs, the primal population risk R(x) represents a divergence measure between
the learned and true distributions, and in the context of adversarial training it represents the learner’s
risk under adversarial perturbations (Farnia & Ozdaglar, 2021). One would be interested in the re-
lationship between R(x) and its corresponding empirical risk RS (x), and the relationship between
R(x) and its infimum infx0∈X R(x0). (2. Plain Measure:) This generalization measure is a direct
extension of the standard generalization error in the minimization optimization. (3. Strong Mea-
sures:) 4sS(x, y) is referred to as the primal-dual gap in the optimization literature. 4s(x, y) is
the primal-dual gap of the population risk. 4s (x, y) - 4sS (x, y) studies the difference between
the population primal-dual gap and its empirical counterpart. (4. Weak Measures:) The difference
between the strong and weak measures is that weak measures take the expectation over the ran-
domness of the dataset and the algorithm, for instance, supy0 ∈Y F (x, y0) - infx0∈X F (x0 , y) in the
strong measures and supy0∈Y E[F (x, y0)] - infx0∈X E[F(x0, y)] in the weak measures. Therefore,
the upper bounds of weak measures hold in expectation, while the upper bounds of strong measures
hold uniformly for any dataset.
Denote the Lp norm of arandomvariable Z as ∣∣Z∣∣p = (EZ |Z|p)1/p. Let ∣∣∙ ∣∣ be the Euclidean norm
and h∙, ∙i be the inner product. A differentiable function g : W → R is called μ-strongly-convex in
w if the following inequality holds for every w1 , w2 :
g(wi) — g(w2) ≥ hVg(w2), Wi — W2)+ 2 ∣∣wι — wιk2,
where V is the gradient operator. We say g is μ-strongly-concave if —g is μ-strongly-convex.
Definition 2. Let g : X × Y 7→ R. Assume that X and Y are convex feasible sets. Then
1.	g is μ-strongly-convex-strongly-concave (μ-SC-SC) if g(∙, y) is μ-Strongly-convexfor any y ∈ Y
and g(x, ∙) is μ-Strongly-ConCaVefor any X ∈ X.
2.	g is convex-concave (C-C) if g is 0-SC-SC.
We then introduce the definition of algorithmic stability this paper used. Algorithmic stability plays
an important role in studying the generalization behavior of a learning algorithm. Intuitively, an
algorithm A : Zn 7→ (X, Y) is said to be stable if the output model (Ax(S), Ay(S)) is insensitive
to perturbations. Let S0 be a neighboring dataset that differs at most one single example to S.
Definition 3 (Algorithmic Stability). Let A be a learning algorithm and > 0.
1.	We say A is -uniformly-stable iffor any training datasets S, S0 ∈ Zn we have
sup[f(Ax(S),Ay(S);z)—f(Ax(S0),Ay(S0);z)] ≤.
z
2.	We say A is -argument-stable iffor any training datasets S, S0 ∈ Zn we have
∣Aχ(S) — Aχ(S0)k + ∣Ay(S) — Ay(S0)k ≤ e.
From Definition 3, one can see that the uniform stability measures the sensitivity of the function
values, while the argument stability measures the sensitivity of the arguments.
We finally introduce two standard assumptions in minimax problems. Assumption 1 implies f is
Lipschitz continuous w.r.t. both X and y, while Assumption 2 implies f is smooth w.r.t. (X, y).
Assumption 1 (Lipschitz continuity). Let L > 0. Assume that for any X ∈ X, y ∈ Y and z ∈ Z,
f (X, y; z) satisfies
l∣Vχf(x,y; z)∣ ≤ L and	∣∣Vyf(x,y; z)∣ ≤ L.
Assumption 2 (Smoothness). Let β > 0. Assume that for any X1, X2 ∈ X, y1, y2 ∈ Y and z ∈ Z,
f(x, y; z) satisfies
Vxf (x1, y1; z) — Vxf(x2, y2; z)	≤ β x1 — x2
Vyf (x1, y1; z) — Vyf(x2, y2; z)	y1 — y2	.
Under Assumption 1, the argument stability implies the uniform stability. Therefore, the argument
stability is the main stability measure that we will focus on.
4
Published as a conference paper at ICLR 2022
4 Main Results
In this section, we provide sharper high probability bounds for the generalization measures of Defi-
nition 1, shown as follows.
Theorem 1. Let A be a learning algorithm and > 0. Suppose |f (x, y; z)| ≤ M for some M > 0
and x ∈ X , y ∈ Y , z ∈ Z. Fixed any η > 0. There exists an absolute positive constant C.
(a.) If the algorithm A is -uniformly stable, then for any δ > 0, with probability at least 1 - δ,
F (Ax(S), Ay(S)) ≤ (1 + η)Fs (Ax(S),Ay(S)) + C T (M log(1∕δ) + e 1鸣 n log；).
(b.) Assume that for all X, the function y → F (x, y) is μ-strongly-concave. Ifthe algorithm A is
-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
R(Ax(S)) ≤ (I + η)Rs(Ax(S)) + C-η~^-(~nlog ； + (μ + I)LElog2nlog；).
(c.) Assume that for all X and y, thefunCtion F (x, y) is μ-SC-SC Ifthe algorithm A is E-argument
stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
4s(Ax(S),Ay(S)) ≤4sS(Ax(S),Ay(S))+ηES4sS(Ax(S),Ay(S))
+ C(1 +	η)( L (1 + η)	+ —	+ (1	+ β )eL log2 n)	log (1).
∖ nμη n ∖ μ/	； δ∂∕
(d.) Assume that for all X and y, thefunction F (x, y) is μ-SC-SC. Ifthe algorithm A is E-argument
stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
4s (Ax(S),Ay(S))-4sS(Ax(S),Ay(S)) ≤ ηES 4sS (Ax(S), Ay(S))
+Ca+η) ( l n1z+η)+MM+(1+β 卜L log2 n) log (；).
nμ∕/	n	μ
(e.) Assume that for all X, the function y → F (x, y) is μ-strongly-concave. Ifthe algorithm A is
E-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
R(Ax(S))≤(1+η) inf R(X)
x∈X
+ C +"(—log ʌ + (— + I)LE log2 n log ʌ + 4S(Ax(S), Ay(S))).
η'nδ∖μ∕	δ	/
According to Definition 1 and Jensen’s inequality, we know that 4w (X, y) ≤ E[4s (X, y)] and
4Sw (X, y) ≤ E[4sS (X, y)]. By this connection, we have 4w (X, y) - 4Sw (X, y) ≤ E[4s (X, y)] +
|E[4sS (X, y)]|. We therefore obtain the following results for 4w (X, y) and 4w(X, y) - 4Sw(X, y).
Corollary 1. Suppose the same conditions as Theorem 1 hold.
(f.) If the assumptions of Part (c) in Theorem 1 hold, then with probability at least 1 - δ,
4w(Ax(S),Ay(S)) ≤ (1 + η)E 4sS (Ax(S), Ay(S))
+c (1+η)	(L	η)	+ MM +	(1	+	β 卜L log2n)	log	(δ).
nμ∕/	n	μ
(g.) If the assumptions of Part (d) in Theorem 1 hold, then with probability at least 1 - δ,
4w(Ax(S),Ay(S))-4Sw(Ax(S),Ay(S)) ≤ |E4sS(Ax(S),Ay(S))|
+ (1+ η)E4S (Ax(S),Ay(S)) + C(1 + η)(L^") + — + (1 + μ卜Llog2n) log (；).
5
Published as a conference paper at ICLR 2022
Remark 2. In Theorem 1, we have established a quantitative connection between the generalization
measures and the stability measures. The complete proof of Theorem 1 is provided in Appendix A.
Part (a) provides the relationship between the uniform stability and the plain generalization er-
ror of (Ax(S), Ay(S)). If the uniform stability of algorithm A is of fast order O(1/n), then
F(Ax(S), Ay(S)) is bounded by (1 + η)θ(Fs(Ax(S), Ay(S)) + logn器⑴").Usually for a
well-trained model (Ax(S), Ay (S)) over the training set, the empirical risk FS(Ax(S), Ay(S)) is
small or even zero (Lever et al., 2013; Yang et al., 2019; Cortes et al., 2021). If the empirical risk is
of order O(1/n), then we can choose a proper constant for η and the plain generalization error will
be of fast order O(lognlθg(1∕δ)). It is O(1∕n) when We hide the logarithmic term. In the related
work, (Lei et al., 2021) also establish the plain generalization error bound under the same assump-
tions, but their bound is of slow order O(E log n log(1∕δ) + Mn- 1 vzlog(1∕δ)). Even if they get
a sharper bound for stability measure e, the influence of O (n- 1 ,log(1∕δ)) can not disappear. By
comparison, we have completely removed the O(1∕√n) term. Thus, our plain generalization error
bound enables the fast O(1/n) rate when the empirical risk is small.
Part (b) provides the connection between the argument stability and the primal generalization error.
Similar to the analysis of Part (a), if both the argument stability of algorithm A and RS (Ax(S))
are of order O(1/n), then the primal generalization error implies a fast O 1/n rate. Consider-
ing that we assume the function f is well-behaved, i.e., Lipschitz continuity, smoothness, and the
strong-concavity of its population risk F, and that RS(Ax(S)) is data-dependent, thus it is reason-
able to assume RS(Ax(S)) is small for a well-trained model Ax(S) (Lever et al., 2013; Yang et al.,
2019; Cortes et al., 2021). In (Lei et al., 2021), they also establish a bound for primal general-
ization error under the same assumptions. However, their bound is O(Lβμ一^1elognlog(1∕δ) +
Mn- 1 ,log(1∕δ)), limited to the O(1∕√n) order. In contrast, we successfully removed the
O(1∕√n) term, which makes the fast rate possible. (Farnia & Ozdaglar, 2021) studies the expected
primal generalization error, i.e., bounding ES,A [R(Ax(S))] by ES,A [RS(Ax(S))]. They establish
the connection between the stability measure and the expected error under the same assumptions
as Part (b), which is then used to derive generalization bounds for (S)GDA, (S)GDmax, and PPM
algorithms. By comparison, our bound is derived in high probability.
Part (c) provides the relationship between the argument stability and the strong PD population risk.
If both the argument stability of algorithm A and the strong PD empirical risk are all of the order
O(1∕n), the strong PD population risk will be of the fast order O(1∕n). Note that in our proof for
the gradient-based optimization algorithms, the strong PD empirical risk mainly has a dependence
on the iterative number T (see Lemma 8 of GDA, Lemma 11 of SGDA, etc.). To obtain sharper
generalization bounds, we require T to be associated with n, such as T = O(n2) for GDA, the
strong PD empirical risk finally has a dependence on the sample size n. To our best knowledge,
this is the first high probability strong PD population risk bound. The expected version of this
risk is studied for the ESP solution in (Zhang et al., 2021a). However, the discussion there does
not establish the connection between stability and generalization. Their analysis is restricted to the
specific ESP problem. Under the same assumptions, they provide the upper bound of order O (1∕n).
Compared with their result, our result is presented in high probability. Additionally, our strong PD
population risk bound is applicable for any stable minimax optimization algorithms.
Part (d) provides the connection between the argument stability and the strong PD generalization
error. Similarly, if both the argument stability of algorithm A and the strong PD empirical risk are
all of O(1∕n) order, the strong PD generalization error will be of the fast order O(1∕n). Although
Part (c) and Part (d) have a similar upper bound, they are different generalization measures (Lei
et al., 2021). To our best knowledge, this is also the first high probability strong PD generalization
error bound. The expected version of this generalization error is studied in (Lei et al., 2021), that
is ES,A [4s(Ax(S), Ay(S)) - 4sS(Ax(S), Ay(S))]. Under the same assumptions, their expected
strong PD generalization error is bounded by (1 + β∕μ)L√2e, which can also be used to obtain
O (1∕n) order rate when E is of order O(1∕n). However, this bound is provided for the expected
error, while our bound is high probabilistic and holds uniformly for any dataset.
Part (e) provides the relationship between the argument stability and the excess primal population
risk. Similar to the analysis of Part (a) and Part (b), if the argument stability of algorithm A, the
strong PD empirical risk, and infx∈X R(x) are all of the order O(1∕n), the excess primal population
6
Published as a conference paper at ICLR 2022
risk will also be of the fast order O(1/n). Meanwhile, in the minimization learning problems,
assuming the optimal population risk F * is small or even zero, i.e., F * ≤ O(1∕n), can be found
in (Lei & Ying, 2021a; Zhang et al., 2017; Zhang & Zhou, 2019; Srebro et al., 2010; Lei & Ying,
2020). Note that the optimal population risk F* = O(1/n) just to show that the improved bound can
be got under low noise conditions. F* should be independent ofn. Similar to the assumption on F*
and considering that we assume the function f is well-behaved, it will also be reasonable to assume
inf x∈X R(x) is small. High probability excess primal population risk bound is also studied for
SGDA in (Lei et al., 2021). Their bound, however, is of slow order O((β∕μ)n- 1 lognlog2(1∕δ))
and is restricted to SGDA. By comparison, our result in Part (e) enables O(1∕n) bounds for stable
minimax learning algorithms since We have completely removed the O(1∕√n) term.
We discuss a noteworthy difference between (Farnia & Ozdaglar, 2021; Lei et al., 2021) and ours.
In Part (a), Part (b), and Part (e), we study the upper bounds of F(Ax(S), Ay(S)), R(Ax(S)) (w.r.t.
RS(Ax(S))), and R(Ax(S)) (w.r.t. inf x∈X R(x)), respectively, while (Lei et al., 2021; Farnia &
Ozdaglar, 2021) study the upper bounds of F(Ax (S), Ay(S)) - FS(Ax(S), Ay(S)), R(Ax(S)) -
RS(Ax(S)), and R(Ax(S)) - infx∈X R(x) (or their expected forms). One of our motivations to
study such forms is that, in practice, we are often directly interested in the true risk, i.e., how the
learned models behave on the testing data, such as F(Ax(S), Ay(S)), instead of the error between
the true risk and empirical risk. Note that in the above comparison between Theorem 1 and the
results in (Lei et al., 2021; Farnia & Ozdaglar, 2021), we all take the right side of the generalization
bound inequalities to compare, which is fair since our bounds can be written as F(Ax(S), Ay(S)) -
FS(Ax(S), Ay(S)) ≤ ηFs(Ax(S), Ay (S)) + C 1+η(M log(1∕δ) + elog2 nlog 1), etc.
Remark 3. From Remark 2, one can see that compared with (Zhang et al., 2021a; Farnia &
Ozdaglar, 2021; Lei et al., 2021), we have established sharper high probability generalization
bounds. In the applications of Section 5, we will establish O(1∕n) order bounds for two terms
in Theorem 1: stability measures and strong PD empirical risk. Hence, the strong PD population
risk and the strong PD generalization error will be of the fast order O(1∕n) when applying Theorem
1 to these applications. These bounds are clearly of order O(1∕n) and sharper than the results in
(Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). For the plain generalization er-
ror, the primal generalization error, and the excess primal population risk, to obtain O(1∕n) order
bounds for these applications, we need to assume the extra corresponding terms F(Ax(S), Ay(S)),
R(Ax(S)), and infx∈X R(x) are of order O(1∕n), respectively. The clear motivation is that in
practice, learning algorithms achieve a small or even zero empirical risk, as discussed in Remark 2.
Remark 4. This remark discusses η in Part (a), Part (b), and Part (e). (1:) When establishing sharper
generalization error bound (i.e., Pf - Pnf), the existence ofη is common in the standard statistical
learning theory. Specifically, in the uniform localized convergence theory, the generalization error
bound in (Bartlett et al., 2005) is of the form Pf ≤ n-1 Pnf + O(ηr* + η 吗1/')) with η >
1 (see Theorem 3.3 and Theorem 4.1). In the PAC-Bayesian theory, the generalization bounds
in (Catoni, 2007) (see Theorem 1.2.6), (Lever et al., 2013) (see Theorem 6), (Yang et al., 2019)
(see Proposition 3.1 and Theorem 4.3), etc., also have η. For instance, the Catoni’s bound is of
the form PQ ≤ 1-e—n(ηPnQ + O(KL(QkPrior)+log(1/")) with η > 0 (Catoni, 2007). In the
algorithmic stability theory, Theorem 1.2 in (Klochkov & Zhivotovskiy, 2021) is of the form Pf ≤
(1 + η)Pnf + 1+nO((elogn + 1 )log(1)) with η > 0. In the recent Cortes’s deviation margin
bounds (Cortes et al., 2021), they also imply a multiplier η. The above bounds can be transformed
into the form of empirical risk multiplied by 1 + η, similar to our results. It is discussed in (Lever
et al., 2013; Yang et al., 2019; Cortes et al., 2021; Bartlett et al., 2005; Klochkov & Zhivotovskiy,
2021) that this type of generalization error bound can obtain a fast rate when the empirical risk
is small. Note that Part (e) also involves generalization error bounds due to the decomposition,
see (31). The above generalization error analysis thus holds for Part (e). (2:) Furthermore, in
(10), we show that F(Ax(S),Ay(S)) - FS(Ax(S),Ay(S)) ≤ O((MF(Ax(S)个(S))Iog(I㈤)2 +
E log( 1)), where M means that |f (x, y; z)| ≤ M, ∀x, y, z. Using the elementary inequality √ab ≤
ηα + 1 b for any a, b, η > 0 and by some rearrangements, the form of Part (a) appears. This is
the reason why η exists. The corresponding bound in (Lei et al., 2021) is F(Ax(S), Ay(S)) -
FS(Ax(S), Ay(S)) ≤ O(Elognlog(∣) + Mn- 1 log1/2(1)). Focusing on the dominated term,
it is clear that F(Ax(S), Ay(S)) M since F(Ax(S), Ay(S)) is data-dependent, which implies
that our plain generalization error bound is sharper. Similar analysis holds for Part (b) and Part (e).
7
Published as a conference paper at ICLR 2022
Reference	Algorithm	Assumption	Generalization Measure	Learning Bound
Zhang	ESP	SC-SC, Lip	Weak PD Risk	O(1∕n)
		-SC-SC, Lip, S-	(E.) Strong PD Risk	θ(l∕n)
	-R-ESP-	C-C, Lip	WeakPDRiSk 一	O(1∕√n)一
Farnia	SGDA	-SC-SC, Lip, S-	(E.) Primal generalization	O(1∕n)
	SGDmax	-SC-SC, Lip, S-	(E.) Primal generalization	O(1∕n)
	-GDA-	-SC-SC, Lip, S-	(E.) Primal generalization	θ(1∕n)
	-GDmax-	-SC-SC, Lip, S-	(E.) Primal generalization	θ(1∕n)
	-PPM-	-SC-SC, Lip, S-	(E.) Primal generalization	θ(1∕n)
	PPM	C-C,Lip, S	(E.) Primal generalization	O(1∕√n)一
	SGDA	Lip,S	(E.) Primal generalization	O(T β‰ ∕n)
	SGDmax	NC-SC, Lip, S	(E.) Primal generalization	；	(k + 1)βc~~- O(T(k+10Lβ+1 ∕n) I
Lei	SGDA	C-C, Lip	Weak PD Risk	o(1∕√n)一
		C-C, Lip, S	Weak PD Risk	O(1∕√n)一
		SC-SC, Lip	Weak PD Risk	Ο(√log n∕n)
		-SC-SC, Lip, S-	Weak PD Risk	O(log n∕n)
		-C-SC, Lip, S-	-(E.) Excess Primal Risk-	O(1∕√n)一
		-C-SC, Lip, S-	(H.P.) Excess Primal Risk	O(log n∕√n)	一
		C-C,Lip	(H.P.) Plain Generalization	O(log n∕√n)
		WC-WC, Lip	Weak PD Generalization	2cμ	2cμ+1 O(T 2cμ + 3 ∕n 2cμ+3 )
		V-WC-WC,Lip, S	Weak PD Generalization	O(1∕√n)一
	AGDA	NC-SC, PL, Lip, S	(E.) Excess Primal Risk	ΣT^7~ cβ+ι、 O(n- 2cβ+1)
Ours	ESP	SC-SC, Lip, LN	Plain Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Primal Generalization	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Risk	"	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Excess Primal Risk	O(log n∕n)
	GDA	SC-SC, Lip, LN	Plain Generalization	-O((log n)3∕2∕n)
		SC-SC, Lip, S, LN	Primal Generalization	-O((log n)3∕2∕n)
		SC-SC, Lip, S	StrOng PD RiSk	一	-O((log n)3∕2∕n)
		-SC-SC, Lip, S	Strong PD Generalization	-O((log n)3∕2∕n)
		SC-SC, Lip, S, LN	Excess Primal Risk	-O((log n)3∕2∕n)
	SGDA	SC-SC, Lip, LN	Plain Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Primal Generalization	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Risk	"	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Excess Primal Risk	O(log n∕n)
	PPM	SC-SC, Lip, S, LN	Plain Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Primal Generalization	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Risk	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Excess Primal Risk	O(log n∕n)
	EG	SC-SC, Lip, S, LN	Plain Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Primal Generalization	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Risk	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Excess Primal Risk	O(log n∕n)
	OGDA	SC-SC, Lip, S, LN	Plain Generalization	O(log n∕n)
		SC-SC, Lip, S, LN	Primal Generalization	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Risk	"	O(log n∕n)
		-SC-SC, Lip, S-	Strong PD Generalization	O(log n∕n)
		SC-SC,Lip, S,LN	Excess Primal Risk	O(log n∕n)
Table 1: Summary of Results. Here, “Zhang” means reference (Zhang et al., 2021a), “Farnia” means
reference (Farnia & Ozdaglar, 2021), and “Lei” means reference (Lei et al., 2021). The bounds are
established by choosing an optimal iterate number T . “LN” means the low noise condition, see
Section 5.1. Other auxiliary descriptions of Table 1 are shown in Appendix H.
8
Published as a conference paper at ICLR 2022
In summary, the above analyses from two different perspectives support our claim that Part (a), Part
(b), and Part (e) provide sharper high probability generalization bounds.
Remark 5. Different measures quantify different degrees of the generalization error. Thus, deriv-
ing bounds of different generalization measures requires different assumptions (Lei et al., 2021;
Zhang et al., 2021a; Farnia & Ozdaglar, 2021). Strong measures require stronger assumptions
compared with the weak (Lei et al., 2021). For instance, for the term supy∈Y F(Ax(S), y) in
4s (Ax(S), Ay (S)), one has to consider the fact that for different Ax(S), y is different, which
makes the proof more challenging. While in 4w (Ax (S), Ay (S)) and 4w (Ax(S), Ay (S)) -
4Sw (Ax(S), Ay (S)), both the supremum over Ax(S) and Ay(S) are outside the expectation op-
erator, thus one does not need to consider the coupling between Ax (S) and y. The upper bounds
shown in Corollary 1 directly derived from Theorem 1 are sub-optimal since 4w(Ax(S),Ay(S))
and 4w(Ax(S), Ay(S)) - 4Sw(Ax(S), Ay(S)) are pretty weak generalization measures (Lei et al.,
2021). We list Corollary 1 here to suggest that, when Theorem 1 is established, the fast order O(1/n)
is easy to be achieved for 4w (Ax(S), Ay (S)) and 4w (Ax(S), Ay (S)) - 4Sw (Ax(S), Ay (S)). On
the other hand, the two weak generalization measures in (Zhang et al., 2021a) is studied for the
specific ESP solution, while Corollary 1 is applicable for any stable minimax learning algorithms, it
thus may be useful in some applications.
5	Applications
We now apply Theorem 1 to the ESP solution and several gradient-based optimization algorithms:
GDA, SGDA, PPM, EG, and OGDA. Considering the length limit, we postpone the introductions
and theorems of these applications to the Appendix. Here, we list the generalization bounds of these
optimization algorithms in Table 1.
5.1	Descriptions of Table 1
Table 1 gives almost all existing generalization bounds in minimax learning. In Table 1, “LN” means
the low noise conditions, i.e., the corresponding F(Ax(S), Ay(S)), R(Ax(S)), or infx∈X R(x) of
these applications is of the order O(1∕n). For instance, for the ESP solution (XS, yS), we assume
F(XS, yS), R(XS), and infχ∈χ R(X) are of the order O(1∕n) for the plain generalization error, the
primal generalization error, and the excess primal population risk, respectively. For other learning
algorithms, please refer to the Remarks in the Appendix. In Table 1, we compare our results with
(Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021) in the way described in the last
paragraph of Remark 2. (E.) denotes that the bound is derived in expectation, while (H.P.) denotes
high probability. Since our results are all established with high probability, we thus omit (H.P.) for
brevity. The descriptions of other notations are shown in Appendix H.
In Table 1, (Zhang et al., 2021a) and (Farnia & Ozdaglar, 2021) focus on the expected generalization
measures. We improve the learning bounds of the ESP solution in (Zhang et al., 2021a) to high prob-
ability guarantees. Compared with (Farnia & Ozdaglar, 2021), we have provided high probability
primal generalization error bounds for GDA, SGDA, and PPM. Additionally, we also study other
generalization measures. (Lei et al., 2021) focus on SGDA and mainly provide guarantees for weak
generalization measures, i.e., weak PD risk and weak PD generalization error. In contrast, we have
developed bounds for strong PD risk and strong PD generalization error. Note that the two type of
bounds don’t require the “LN” condition. Moreover, although (Lei et al., 2021) provides two high
probability bounds, however, in slow order. Note that in addition to the classical GDA, SGDA, and
PPM, we also provide sharper high probability bounds for EG and OGDA in that their widespread
use in training GANs (Mokhtari et al., 2019; Daskalakis et al., 2017; Liang & Stokes, 2019).
6	Conclusion
In this paper, we provide a systematical analysis of sharper generalization bounds for minimax
problems. We first establish sharper high probability bounds for almost all existing generalization
measures via algorithmic stability and then apply these bounds to several important applications. We
believe that our research can provide in-depth insights into minimax learning problems. For future
work, it would be important to relax the assumptions in this paper. Also, it would be interesting to
investigate how well other theoretical tools perform on the generalization of minimax problems.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We appreciate all the anonymous reviewers for their invaluable and constructive comments.
This work is supported in part by the National Natural Science Foundation of China (No.
62076234, No.61703396, No. 62106257), Beijing Outstanding Young Scientist Program
NO.BJJWZYJH012019100020098, Intelligent Social Governance Platform, Major Innovation &
Planning Interdisciplinary Platform for the ”Double-First Class” initiative, Renmin University of
China, China Unicom Innovation Ecological Cooperation Plan, Public Computing Cloud of Renmin
University of China, Beijing Natural Science Foundation (No. 4222029).
References
Palaniappan Balamurugan and Francis Bach. Stochastic variance reduction methods for saddle-point
problems. In Advances in Neural Information Processing Systems, 2016.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob N. Foerster, Karl Tuyls, and Thore
Graepel. The mechanics of n-player differentiable games. In International Conference on Ma-
chine Learning, 2018.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals
OfStatistics, 33(4):1497-1537, 2005.
RaefBassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems,
2020.
StePhane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonaSymp-
totic theory of independence. Oxford university press, 2013.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2(3):499-526, 2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. SharPer bounds for uniformly stable
algorithms. In Conference On Learning Theory, 2020.
Olivier Catoni. PAC-BAYESIAN SUPERVISED CLASSIFICATION: The Thermodynamics of Statis-
tical Learning. 2007.
Zachary Charles and Dimitris PaPailioPoulos. Stability and generalization of learning algorithms
that converge to global oPtima. In International Conference on Machine Learning, 2018.
Robert S. Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust oPtimization for
non-convex objectives. In Advances in Neural Information Processing Systems, 2017.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative oPtimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.
Ziyi Chen, Yi Zhou, Tengyu Xu, and Yingbin Liang. Proximal gradient descent-ascent: Variable
convergence under klgeometry. In International Conference on Learning Representations, 2021.
Ashish Cherukuri, Bahman Gharesifard, and Jorge Cortes. Saddle-Point dynamics: conditions for
asymPtotic stability of saddle Points. Siam Journal on Control and Optimization, 55(1):486-511,
2017.
Corinna Cortes, Mehryar Mohri, and Ananda Theertha Suresh. Relative deviation margin bounds.
In International Conference on Machine Learning, PP. 2122-2131, 2021.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function aPProximation. In International Con-
ference on Machine Learning, 2018.
Constantinos Daskalakis and Ioannis Panageas. The limit Points of (oPtimistic) gradient descent in
min-max oPtimization. In Advances in Neural Information Processing Systems, 2018.
10
Published as a conference paper at ICLR 2022
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In International Conference on Learning Representations, 2017.
Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally elastic
stability. In International Conference on Machine Learning, 2021.
Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. Efficient methods for struc-
tured nonconvex-nonconcave min-max optimization. In International Conference on Artificial
Intelligence and Statistics, 2021.
Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduc-
tion methods for policy evaluation. In International Conference on Machine Learning, 2017.
Farzan Farnia and Asuman Ozdaglar. Train simultaneously, generalize better: Stability of gradient-
based minimax learners. In International Conference on Machine Learning, 2021.
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad-
vances in Neural Information Processing Systems, 2018.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algo-
rithms with nearly optimal rate. In Conference on Learning Theory, 2019.
Tanner Fiez and Lillian J Ratliff. Local convergence analysis of gradient descent ascent with finite
timescale separation. In International Conference on Learning Representations, 2021.
Dylan J. Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Srid-
haran. Hypothesis set stability and generalization. In Advances in Neural Information Processing
Systems, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, 2014.
Paulina Grnarova, Kfir Y. Levy, AUrelien Lucchi, Thomas Hofmann, and Andreas Krause. An online
learning approach to generative adversarial networks. In International Conference on Learning
Representations, 2017.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: stability of
stochastic gradient descent. In International Conference on Machine Learning, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, 2017.
Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence
of single-call stochastic extra-gradient methods. In Advances in Neural Information Processing
Systems, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the POlyak-IOjaSieWiCz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811, 2016.
Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds With conver-
gence rate O(1/n). Advances in Neural Information Processing Systems, 2021.
WeiWei Kong and Renato D. C. Monteiro. An accelerated inexact proximal point method for solving
nonconvex-concave min-max problems. arXiv preprint arXiv:1905.13433, 2019.
G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Mate-
con, 12:747-756, 1976.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, 2018.
11
Published as a conference paper at ICLR 2022
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, 2020.
Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2021a.
Yunwen Lei and Yiming Ying. Stochastic proximal auc maximization. Journal of Machine Learning
Research, 22(61):1-45, 2021b.
Yunwen Lei, Antoine Ledent, and Marius Kloft. Sharper generalization bounds for pairwise learn-
ing. In Advances in Neural Information Processing Systems, 2020.
Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying. Stability and generalization of
stochastic gradient methods for minimax problems. In International Conference on Machine
Learning, 2021.
GUy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter pac-bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, and Weiping Wang. Multi-class learning:
From theory to algorithm. In Advances in Neural Information Processing Systems, pp. 1586-
1595, 2018.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020.
Shaojie Li and Yong Liu. Sharper generalization bounds for clustering. In International Conference
on Machine Learning, pp. 6392-6402, 2021.
Shaojie Li and Yong Liu. Towards sharper generalization bounds for structured prediction. In
Advances in Neural Information Processing Systems, 2021.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In International Conference on Artificial Intelligence and
Statistics, 2019.
Qihang Lin, Mingrui Liu, Hassan Rafique, and Tianbao Yang. Solving weakly-convex-weakly-
concave saddle-point problems as weakly-monotone variational inequality. 2018.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-
max problems. In International Conference on Machine Learning, 2020a.
Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization. In
Conference on Learning Theory, 2020b.
Mingrui Liu, Hassan Rafique, Qihang Lin, and Tianbao Yang. First-order convergence theory for
weakly-convex-weakly-concave min-max problems. Journal of Machine Learning Research, 22
(169):1-34, 2021a.
Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, 2017.
Yong Liu, Shizhong Liao, Shali Jiang, Lizhong Ding, Hailun Lin, and Weiping Wang. Fast cross-
validation for kernel-based algorithms. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 42(5):1083-1096, 2020.
Yong Liu, Jiankun Liu, and Shuqiang Wang. Effective distributed learning with random features:
Improved bounds and algorithms. In International Conference on Learning Representations,
2021b.
Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien,
and Ioannis Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In Interna-
tional Conference on Machine Learning, 2020.
12
Published as a conference paper at ICLR 2022
Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien.
Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence
analysis under expected co-coercivity. arXiv preprint arXiv:2107.00052, 2021.
Ben London, Bert Huang, and Lise Getoor. Stability and generalization in structured prediction.
Journal of Machine Learning Research ,17(221):7808-7859, 2016.
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive ap-
proximation for one-sided non-convex min-max problems: Algorithms and applications. IEEE
Transactions on Signal Processing, 68:3676-3691, 2020.
Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent
ascent for stochastic nonconvex-strongly-concave minimax problems. In Advances in Neural
Information Processing Systems, 2020.
Shaogao Lv, Junhui Wang, Jiankun Liu, and Yong Liu. Improved learning rates of a functional lasso-
type svm with sparse multi-kernel representation. In Advances in Neural Information Processing
Systems, 2021.
Gonzalo Mateos, JUan Andres Bazerque, and Georgios B Giannakis. Distributed sparse linear re-
gression. IEEE Transactions on Signal Processing, 58(10):5262-5276, 2010.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In International Conference on Learning Representations, 2019.
Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. Proximal point approximations achiev-
ing a convergence rate of O(1/k) for smooth convex-concave saddle point problems: Optimistic
gradient and extra-gradient methods. 2019.
Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artificial Intelligence and Statistics, 2020.
Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Advances in Neural Information Processing Systems, 2016.
Hongseok Namkoong and John C. Duchi. Variance-based regularization with convex objectives. In
Advances in Neural Information Processing Systems, 2017.
Angelia Nedic and Asuman E. Ozdaglar. Subgradient methods for saddle-point problems. Journal
of Optimization Theory and Applications, 142(1):205-228, 2009.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. Siam Journal on Optimization, 19(4):1574-1609, 2008.
Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities
with lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
Siam Journal on Optimization, 15(1):229-251, 2005.
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative first order methods. In Advances in Neural
Information Processing Systems, 2019.
L. D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical
Notes, 28(5):845-848, 1980.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv: Optimization and Control,
2018.
Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.
Analysis and Applications, 3(04):397-417, 2005.
13
Published as a conference paper at ICLR 2022
Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi
Hong. Non-convex min-max optimization: Applications, challenges, and recent theoretical ad-
vances. arXiv preprint arXiv:2006.08141, 2020.
R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. Siam Journal on
Control and Optimization,14(5):877-898,1976.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D. Lee. On the convergence and ro-
bustness of training gans with regularized optimal transport. In Advances in Neural Information
Processing Systems, 2018.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal of Machine Learning Research, 11(90):2635-2670, 2010.
Jeff Shamma. Cooperative Control of Distributed Multi-Agent Systems. 2008.
Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness
with principled adversarial training. In International Conference on Learning Representations,
2017.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth
loss. arXiv preprint arXiv:1009.3896, 2010.
Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths:
Optimality and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716-
5735, 2014.
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algo-
rithms for smooth minimax optimization. In Advances in Neural Information Processing Systems,
2019.
Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. In
Advances in Neural Information Processing Systems, 2020.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020.
Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient de-
scent ascent methods for min-max optimization. In Advances in Neural Information Processing
Systems, 2020.
Jun Yang, Shengyang Sun, and Daniel M. Roy. Fast-rate pac-bayes generalization bounds via shifted
rademacher processes. In Advances in Neural Information Processing Systems, volume 32, pp.
10802-10812, 2019.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class
of nonconvex-nonconcave minimax problems. In Advances in Neural Information Processing
Systems, 2020.
Rong Yin, Yong Liu, Weiping Wang, and Dan Meng. Sketch kernel ridge regression using circulant
matrix: Algorithm and theory. IEEE Transactions on Neural Networks, 31(9):3512-3524, 2020.
TaeHo Yoon and Ernest K. Ryu. Accelerated algorithms for smooth convex-concave minimax prob-
lems with O(1/k2) rate on squared gradient norm. arXiv preprint arXiv:2102.07922, 2021.
Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang. Generalization bounds for
stochastic saddle point problems. In International Conference on Artificial Intelligence and Statis-
tics, 2021a.
Lijun Zhang and Zhi-Hua Zhou. Stochastic approximation of smooth and strongly convex functions:
Beyond the O(1/t) convergence rate. In Conference on Learning Theory, pp. 3160-3179, 2019.
14
Published as a conference paper at ICLR 2022
Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex op-
timization: O(1/n)- and O(1/n2)-type of risk bounds. In Conference on Learning Theory, pp.
1954-1979, 2017.
Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami. Sta-
bility of sgd: Tightness analysis and improved bounds. arXiv preprint arXiv:2102.05274, 2021b.
A	Proof of Theorem 1
We now provide proofs of Theorem 1. For better readability, we restate Theorem 1 below.
Theorem 2. Let A be a learning algorithm and > 0. Suppose |f (x, y; z)| ≤ M for some M > 0
and x ∈ X , y ∈ Y , z ∈ Z. Fixed any η > 0. There exists an absolute positive constant C.
(a.) If the algorithm A is -uniformly stable, then for any δ > 0, with probability at least 1 - δ,
F (Ax(S), Ay(S)) ≤ (1 + η)Fs (Ax(S),Ay(S)) + C T (M log(1∕δ) + e 1鸣 n log；).
(b.) Assume that for all X, the function y → F (x, y) is μ-strongly-concave. Ifthe algorithm A is
-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
R(Ax(S)) ≤ (I + η)Rs(Ax(S)) + C-η~^-(~nlog ； + (μ + I)LElog2nlog；).
(c.) Assume that for all X and y, thefunCtion F (x, y) is μ-SC-SC Ifthe algorithm A is E-argument
stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
4s(Ax(S),Ay(S)) ≤4sS(Ax(S),Ay(S))+ηES4sS(Ax(S),Ay(S))
+C(1+η) (LnXη) + MM + (1 + β )eL log2n)log (δ).
nμ∕/	n	μ
(d.) Assume that for all X and y, thefunction F (x, y) is μ-SC-SC. Ifthe algorithm A is E-argument
stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
4s (Ax(S),Ay(S))-4sS(Ax(S),Ay(S)) ≤ ηES 4sS (Ax(S), Ay(S))
+ C(1 + η)( L (1 + n) + — + (1 + β )eL log2 n) log (1).
∖ nμη n ∖ μ/	δ ∖δ∕
(e.) Assume that for all X, the funCtion y → F (x, y) is μ-strongly-concave. Ifthe algorithm A is
E-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ,
R(Ax(S))≤(1+η) inf R(X)
x∈X
+ C +"(—log ʌ + (— + I)LE log2 n log ʌ + 4S(Ax(S), Ay(S))).
η'nδ∖μ∕	δ	/
Remark 6. To prove sharper high probability bounds than (Lei et al., 2021), the concentration
inequality for a summation of weakly-dependent random variables proposed in (Bousquet et al.,
2020) (Lemma 2 in Appendix A) plays a key role in our analysis. However, the direct use of this
inequality will inevitably lead to a slow order bound since it contains a sampling error of slow order
O(1∕√n). We exploit the proof techniques of the recent breakthrough work (Klochkov & Zhiv-
otovskiy, 2021) to make new constructions of gi (S) so that the parameter — in Lemma 2 is 0.
However, the proof techniques of (Klochkov & Zhivotovskiy, 2021) can not be directly extended
to minimax problems. The coupling construction between the minimization variables and the max-
imization variables makes the proofs of minimax problems more difficult than the minimization
problem studied by (Klochkov & Zhivotovskiy, 2021). We must proceed with novel decomposi-
tions for the generalization measures. Note that different decompositions are required for different
generalization measures. A pretty technical decomposition is exploited in the proof of Part (c).
15
Published as a conference paper at ICLR 2022
Moreover, the proof of minimax problems needs refined analyses due to the minimax structure. For
instance, in proving the primal generalization error, we need to quantify the fact that for different
Ax(S), the optimal y is different in R(Ax(S)). And the analysis of excess primal population risk in
Part (e) is different from the excess risk analysis in (Klochkov & Zhivotovskiy, 2021). The reason
is that the supremum operator in infx∈X R(x) makes the Bernstein condition used in (Klochkov
& Zhivotovskiy, 2021) not applicable for excess primal population risk. Additionally, (Klochkov
& Zhivotovskiy, 2021) only study ERM and GD, while we study more optimization algorithms:
SGDA, PPM, EG, and OGDA.
Remark 7. According to Definition 1 and Jensen’s inequality, we know that 4Sw (x, y) ≤
E[4sS (x, y)]. Meanwhile, since we will provide the strong PD empirical risk bounds for several
important optimization algorithms in Section 5, it thus implies that we also establish bounds with
fast rates for 4Sw (x, y).
To begin the proof of Theorem 1, we first introduce some key lemmas on concentration inequalities.
The first lemma translates a moment bound into a high probability bound.
Lemma 1. (Bousquet et al., 2020) Let Z be a random variable with
IlZIlp ≤ √pa + Pb
for some a, b > 0 and for any p ≥ 2. Then for any δ ∈ (0, 1) we have, with probability at least
1 - δ,
|Z| ≤ e
where e is the base of the natural logarithm.
The second lemma establishes a concentration inequality for a summation of weakly-dependent
random variables.
Lemma 2. (Bousquet et al., 2020) Let S = {z1, ..., zn} be a set of independent random variables
each taking values in Z and M > 0. Denote [n] as the set {1, ..., n}. Define S\{zi} be set
{z1, ..., zi-1, zi+1, ..., zn}. Let g1 , ..., gn be some functions gi : Zn 7→ R such that the following
inequalities hold for any i ∈ [n],
•	ES\{zi} [gi(S)] ≤ M almost surely (a.s.),
•	Ezi [gi(S)] = 0 a.s.,
•	for any j ∈ [n] with j 6= i, and zj00 ∈ Z
gi(S) - gi(z1, ..., zj-1,zj00, zj+1, ...,zn) ≤ β.
Then, for any p ≥ 2
≤ 12√2pnβdlog2 n∖ + 4M√pn.
The following definition and lemma give the concentration inequality for non-negative weakly self-
bounded functions.
Definition 4. (Weakly Self-Bounded Function) Assume that a, b > 0. A function f : Zn 7→ [0, +∞)
is said to be (a, b)-weakly self-bounded if there exist functions fi : Zn-1 7→ [0, +∞) that satisfies
for all Zn ∈ Zn,
n
X(f(Zn) - fi(Zn))2 ≤ af(Zn) +b.
i=1
Lemma 3. (Klochkov & Zhivotovskiy, 2021) Suppose that z1, ..., zn are independent random vari-
ables and the function f : Zn 7→ [0, +∞) is (a, b)-weakly self-bounded and the corresponding
function fi satisfy fi(Zn) ≥ f(Zn) for i = 1, ..., n and any Zn ∈ Zn. Then, for any t > 0,
t2
Pr(Ef(z1,...,zn) ≥ f(ZI,…，Zn)+ t) ≤ exp--2aEf(zι,...,Zn) + 2b)
16
Published as a conference paper at ICLR 2022
The following lemma is the classical Bernstein concentration inequality.
Lemma 4. (Boucheron et al., 2013)Letz1, ..., zn be i.i.d. random variables and assume thatE[zi]
μ. Suppose ∣z∕ ≤ C for any i. Thenfor any δ ∈ (0,1), with probability at least 1 一 δ,
∣1 X zi - μ∖≤∕
i=1
2σ2 log(1∕δ) +
2clog(1∕δ)
3n
n
where σ2 is the variance of zi.
A. 1 Proof of Part (a)
We first prove the plain generalization error bound.
Proof. Let S = {z1, ..., zn} be a set of independent random variables each taking values
in Z and S0 = {z10 , ..., zn0 } be its independent copy. For any i ∈ [n], define S(i) =
{z1, ..., zi-1, zi0, zi+1, ..., zn} be a dataset by replacing the i-th sample in S with another i.i.d. sam-
ple zi0 . We first have the following decomposition
nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))
n
=XEZ[f(Ax(S),Ay(S);Z)-Ezi0[f(Ax(S(i)),Ay(S(i));Z)]]
i=1
n
+ XEzi0[EZ[f(Ax(S(i)),Ay(S(i));Z)] - f(Ax(S(i)), Ay(S(i)); zi)]
i=1
nn
+ XEzi0[f(Ax(S(i)),Ay(S(i));zi)] -Xf(Ax(S),Ay(S);zi).
i=1	i=1
According to the definition of uniform stability (Part 1 of Definition 3), we have
n
nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S)) ≤2n + Xgi(S),
i=1
where we have introduced	gi(S)	=	Ez0 [EZ[f(Ax(S(i)), Ay(S(i)); Z)]	-
f(Ax(S(i)), Ay(S(i)); zi)]. Thus, by a rearrangement, we have
n
∣∣nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))-Xgi(S)∣∣ ≤ 2n.	(3)
i=1
Then, for any i = 1, ..., n, we define hi(S) = gi(S) - ES\{zi} [gi(S)]. It is easy to verify that
ES\{zi}[hi(S)] = 0 and Ezi [hi (S)] = Ezi[gi(S)] - EziES\{zi}[gi(S)] = 0 - 0 = 0. Also, for any
j ∈ [n] with j 6= i, and zj00 ∈ Z, we have the following inequality
∣hi(S) - hi(z1, ..., zj-1, zj00, zj+1, ..., zn)∣ ≤ ∣gi(S) - gi(z1, ..., zj-1, zj00, zj+1, ..., zn)∣
+ IES∖{zi}[gi(S)] — ES∖{zi}[gi(Z1, ∙∙∙,zj-1,zj0,zj+1, ∙∙∙,zn)]| .
For the first term |gi(S) - gi(z1, ..., zj-1, zj00 , zj+1, ..., zn)|, it can be bounded by 2
according to the definition of uniform stability. Similar result holds for the sec-
ond term ∣Es∖5} [gi(S)] ― Es∖{ζi}[gi(zι, ...,zj-1,zj,zj+1, ...,zn)]∣ according to the defini-
tion of uniform stability. By a combination of the above analysis, we get |hi(S) -
hi(z1, ..., zj-1, zj00 , zj+1, ..., zn)| ≤ 4.
We thus have verified that the three conditions in Lemma 2 are satisfied for hi (S). There will hold
the following result for any p ≥ 2
n
Il X hi(S)( ≤ 48√2epndlog2 n∖.	(4)
17
Published as a conference paper at ICLR 2022
Furthermore, we can derive that
nn
nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))-Xgi(S)+Xhi(S)
i=1	i=1
n
=nF(Ax(S),Ay(S)) -nFS(Ax(S),Ay(S)) -XES\{zi}[gi(S)]
i=1
=nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))-nES0F(Ax(S0),Ay(S0))
+nES0FS(Ax(S0),Ay(S0))
Due to the i.i.d. property between S and S0, we know that ES0F (Ax(S0), Ay(S0))
ESF(Ax(S),Ay(S)).
Thus, combined (3) with (4), we have
knF(Ax(S),Ay(S)) -nFS(Ax(S),Ay(S)) -nESF(Ax(S),Ay(S)) + nES0FS(Ax(S0), Ay(S0))kp
n
≤ nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))-Xgi(S)
i=1	p
n
+ Xgi(S) - nESF (Ax(S), Ay(S)) + nES0FS(Ax(S0), Ay(S0))
p
i=1
nn
= nF(Ax(S),Ay(S))-nFS(Ax(S),Ay(S))-Xgi(S)	+Xhi(S)
≤ 2ne + 48√2epndlog2 n∖
≤ 50√2cpndlog2 n∖.
According to Lemma 1, for any δ ∈ (0,1), with probability at least 1 一 δ∕3, We have
F(Ax(S),Ay(S)) - FS(Ax(S), Ay(S))
≤∣EsoFS(Ax(S)),Ay(S')) - ESF(Ax(S), Ay(S))∣ + 50√2e"log2 n∖ log(3e∕δ).	(5)
We now begin to bound the term ES0FS(Ax(S 0), Ay(S 0)) - ESF(Ax(S), Ay(S)). There holds
that ESES0FS(Ax(S 0), Ay(S 0)) = ESF(Ax(S), Ay(S)). We first consider the variance of
ES0f(Ax(S 0), Ay(S 0); zi). By the Jensen’s inequality, we have
Ezi[(Esof(Ax(S0),Ay(S0); zi))2] ≤ EziESO[(f(Ax(S0),Ay(S0); zi))2]
=EZES0[(f(Ax(S 0),Ay(S0);Z))2]
= EZES[(f(Ax(S),Ay(S);Z))2].
Then, by the Bernstein inequality in Lemma 4, we obtain the following inequality with probability
at least 1 - δ∕3,
|ES0FS(Ax(S0),Ay(S0)) -ESF(Ax(S),Ay(S))|
(6)
≤
2Ez ES [(f (Ax(S), Ay (S); Z ))2] log(3∕δ)
n
2M log(3∕δ)
3n
+
Combined (5) with (6), we finally obtain that with probability at least 1 - 2δ∕3,
F (Ax(S), Ay(S)) - Fs (Ax(S), Ay(S))
∕2EZES[(f(Ax(S),Ay(S); Z))2]log(3∕δ) +2m0/S) + 50√2Mog2 ^ ^⑶⑷.
n	3n
(7)
In the following, we define q = q(z1, ..., zn) = EZ [(f(Ax(S), Ay(S); Z))2] and qi =
qi(z1, ..., zn) = supzi∈Z q(z1, ..., zn). So there holds qi ≥ q for any i = 1, .., n and any
18
Published as a conference paper at ICLR 2022
{z1, ..., zn} ∈ Zn. Also, there holds that
n
X(q - qi)2
i=1
n
i=1
2
ZsuZ EZ f (Ax(S),Ay (S )； Z))2])
n2
≤X2 EZf(Ax(S), Ay(S); Z) + supf(Ax(S),Ay(S);Z)
i=1	zi∈Z
≤ne2 (2EZf(Ax(S), Ay(S); Z))] + 拼
≤8n2q + 2n4,	(8)
where the first inequality follows from the Jensen’s inequality and the definition of uniform stability,
and where the second inequality also follows from the definition of uniform stability.
From (8), we know that q is (8n2, 2n4) weakly self-bounded. Thus, by Lemma 3, we obtain that
with probability at least 1 - δ∕3,
ESEZf(Ax(S), Ay(S); Z))2] - EZf(Ax(S), Ay(S); Z))2]
≤,(16ne2EsEZf(Ax(S),Ay(S); Z))2] + 4ne4)log(3∕δ)
=J(ESEZf(Ax(S), Ay(S); Z))2] + 4e2)l6ne2 log(3∕δ)
≤ 1 (EsEZf (Ax(S), Ay(S); Z))2] + 1 e2) + 8ne2 log(3∕δ),
where the last inequality follows from that √ab ≤ a+b for all a, b > 0.
Since EZ[(f(Ax(S), Ay(S); Z))2] ≤MF(Ax(S),Ay(S)),wehave
ESEZf(Ax(S), Ay(S); Z))2] - 2MF(Ax(S),Ay(S)) ≤ 4e2 + 16ne2 log(3∕δ).	⑼
Substituting (9) into (7), we finally obtain that with probability at least 1 - δ,
F(Ax(S), Ay(S)) - FS(Ax(S), Ay(S))
≤ h4MF(Ax(S), Ay(S)) + 2e2 + 32ne2 log(3∕δ)) log(3∕δ)
n
+ 2M log，30 + 50√Mlog2 ne log(3e∕δ).	(10)
3n
According to inequalities √ob ≤ ηα + 1 b and √α + b ≤ √a + √b for any a, b, η > 0, We have the
following inequality with probability at least 1 - δ
F(Ax(S), Ay(S)) - FS(Ax(S), Ay(S))
≤ ;1，2 + 32ne2loj(3∕δ))l。三+ GF (Ax(S),Ay (S)) +
+ 2M log(3∕δ) + 50√2eedlog2 n] log(3e∕δ),
3n
which implies that
F(Ax(S),Ay(S))-(1+η)FS(Ax(S),Ay(S))
1 + η 4M log(3∕δ)
n
η
/ (2e2 +32ne2 log(3∕δ))log(3∕δ)	1 + η 4M log(3∕δ)
≤(1+叭 V---------------n-------------+ F---------n—
+ 2Μlog(3∕δ)+50√2
edlog2 ne log(3e∕δ)
3n
≤Cɪ+^ (M log(1∕δ) + E log2 n log(1∕δ)),
19
Published as a conference paper at ICLR 2022
where C is an absolute constant. The proof is complete.	□
A.2 Proof of Part (b)
We then prove the primal generalization error bound. Before presenting the proof, we first introduce
a lemma that quantifies the sensitivity of the optimal y and x w.r.t the perturbation of x and y
respectively.
Lemma 5. (Zhang et al., 2021a) Let f : X ×Y → R. Assume that f is μ-Stnmgly-convex-strongly-
concave. Suppose that for any x, x0 ∈ X and y, y0 ∈ Y we have
l∣Vyf (x, y) - Vyf(x0, y)k ≤ β∣∣x - x0k	and	∣∣Vχf (x, y) - Vxf (x, y0)k ≤ βky - y0∣∣.
Define x* (y) = arg minx∈χ h(x, y) and y* (x) = arg maxy∈γ h(x, y) for any y and X respec-
tively. Then, for any x, x0 ∈ X and y, y0 ∈ Y there holds that
ky*(x) -y*(x0)k ≤ μkx -x0k	and	kx*(y) - x*(y0)k ≤ μky -y0k.
The proof of Part (b) shares similar proof techniques with Part (a), but requires a novel decompo-
sition and several important changes. For instance, Lemma 5 should be needed to quantify the fact
that for different Ax(S), the optimal y is different in R(Ax(S)).
Proof. Let S = {z1, ..., zn} be a set of independent random variables each taking values
in Z and S0 = {z10 , ..., zn0 } be its independent copy. For any i ∈ [n], define S(i) =
{z1, ..., zi-1, zi0, zi+1, ..., zn} be a dataset by replacing the i-th sample in S with another i.i.d. sam-
Ple z0. Denote YS = argmaxy∈γ F(Ax(S), y) and yS = argmaxy∈γ FS(Ax(S), y). We have
the following decomposition
nR(Ax(S)) - nRS(Ax(S))
=nF(Ax(S), yS) - nF(Ax(S), yS)
n
=XEZ[f(Ax(S),y*S;Z) -Ezi0[f(Ax(S(i)),yS*(i);Z)]]
i=1
n
+XEzi0[EZ[f(Ax(S(i)),y*S(i);Z)] -f(Ax(S(i)),yS*(i);zi)]
i=1
nn
+ XEzi [f (Ax(S(i)), yS(i); Zi)] - X f (Ax(S), yS; Zi).	(11)
i=1	i=1
Firstly, we have
f(Ax(S),yS*;Z)-f(Ax(S(i)),yS*(i);Z)
=f(Ax(S),yS*;Z)-f(Ax(S),yS*(i);Z)+f(Ax(S),y*S(i);Z)-f(Ax(S(i)),yS*(i);Z)
≤LlyS* -yS*(i)l +LlAx(S)-Ax(S(i))l
≤(l + μ”∣∣Ax(S)- Ax(S(i))k
≤(1+ μ ”的	(I2)
where the second inequality follows from Lemma 5 with the fact that F is smooth and μ-SC-SC.
20
Published as a conference paper at ICLR 2022
Secondly, we have
n
X Ez0 [f (Ax(S(i)),寸S⑸；Zi)]
i=1
n
=XEz0 [f (Ax(S⑺),寸S⑸;Zi)- f (Ax(S), yS; Zi) + f (Ax(S), yS; &)]
i=1
nn
≤ XEz0(1 + β"kAx(S) - Ax(S⑴)k + X f (Ax(S), yS; Zi)
i=1	μ	i=1
n
≤ (1 + μ)Ln'+^Xf (Ax(S), yS; Zi),
where the first and the last inequalities follow from (12). Substituting the above two results into
(11), we obtain that
nF(Ax(S), yS)-通(Ax(S), yS)
≤2(1+μ
≤2(1 + β
∖ μ
n	nn
Lne + X f (Ax(S),寸S; %) + X gi(S) - X f (Ax(S),寸S; Zi)
i=1	i=1	i=1
n
Lne + X gi (S),
i=1
where the last inequality follows from the facts that Pn= 1 f (Ax(S), yS; Zi)-
Pn=I f (Ax(S), yS； Zi) ≤ 0 and that we have introduced gi(S) = Ezi [Ez [f (Ax(S(i)), 丫飞⑺;Z)]-
f(Ax(S⑴),yk; Zi)].	'
Now we get
n
nF(Ax(S), yS) - nFS(Ax(S), y)- Xgi(S) ≤ 2(1 + β)Lne∙	(13)
μ
For any i = 1, ..., n, we define hi(S) = gi(S) - ES\{zi}[gi(S)].
WealsogetthatES\{zi}[hi(S)] = 0andEzi[hi(S)] = Ezi[gi(S)]-EziES\{zi}[gi(S)] = 0-0 = 0.
Moreover, for any j ∈ [n] with j 6= i, and Zj00 ∈ Z, we have the following inequality
|hi(S) - hi(Z1, ..., Zj-1, Zj00, Zj+1, ..., Zn)| ≤ |gi(S) - gi(Z1, ..., Zj-1, Zj00, Zj+1, ..., Zn)|
+ |ES\{zi}[gi(S)] - ES\{zi}[gi(Z1, ...,Zj-1, Zj00, Zj+1, ..., Zn)]|.
Denote Sj(i) as the set collected by replacing the j-th element of S(i) with Zj00 . For the first term
|gi (S) - gi (Z1 , ..., Zj -1 , Zj , Zj+1 , ..., Zn )|, we have
gi(S) - gi(Z1, ..., Zj-1, Zj00, Zj+1, ..., Zn)
=Ez0∣Ez[f (Ax(S⑴),yS⑸；Z)] - f(Ax(S⑴),& ；Μ)]
-Ez0 [Ez[f (Ax(Sji)), yS(i)； Z)] - f (Ax(Sji)), yS?； &)] ∣
≤∣Ez0[Ez[f (Ax(S⑴),yS⑸;Z)] - f (Ax(Sji)),寸S甲;Z)] |
+ ∣Ez0 [f (Ax(S⑺),yS(i； Zi)- f (Ax(Sji)),寸SP; Zi)] |.	(14)
21
Published as a conference paper at ICLR 2022
Furthermore, for any z, we have the following result which can help to bound the above inequality.
f (Ax(S⑴),yS®; z) - f (Aχ(sji)), y(i); Z) J
≤f(Aχ(S⑴),yS(i); z) -f(Ax(S(i)),yS甲;z)∣ + If(Ax(S⑴),y葩； z) - f(Aχ(sji)),y；苧;Z)I
≤L∣∣yS(i) - yS(i)∣∣ + LllAx(S⑴)-Ax(Sji))Il
≤( μ+ι”∣∣Ax(S⑴)一Ax(Sji)) II
≤( μ+1)Le,	(15)
where the third inequality follows from Lemma 5. Thus, We can bound the first term by 2 (j+ +1)Le.
Bya similar analysis, the second term |ES\{zi}[gi(S)] - ES\{zi}[gi(z1, ...,zj-1, zj00, zj+1, ..., zn)]|
can also be bounded by 2(μ + 1)Le.
We now have verified that the three conditions in Lemma 2 are satisfied for hi(S). We obtain that
for any p ≥ 2, there holds
∣∣ X hi(S)∣∣ ≤ 48√2pn(β + 1)Ledlog2 n]∙
lli=l	"p	vμ 7
(16)
Thus, combined (13) with (16), we derive that
IInF(Ax(S), yS) - nF；(Ax(S), y；) - nEs，F(Ax(S0), yS，) + nEs，FS(Ax(S0), yS,)∣∣p
nnn
= ∣∣nF(Ax(S), yS) - Xgi(S) + Xgi(S) - XES∖{Zi}[gi(S)]∣∣
i=1	i=1	i=1	p
nn
≤∣∣nF(Ax(S), yS) -nFS(Ax(S), yS) - Xgi(S)∣∣ + ∣∣ X hi(S)∣∣
i=1	p	i=1	p
≤2(1 + 2) Lne + 48√2pn G + 1)Ledlog? n]
≤50√2(l + — ) eLpn dlog? n].	(17)
Then, according to Lemma 1, for any δ ∈ (0,1), with probability at least 1 一 δ∕3, we have
f (Ax(S), yS) - FS (Ax(S), yS)
≤∣Esof(Ax(S0),yS0) - EsoFs(Ax(S0),yS，)| +50√2eeL0 + /)「log? n] log(3e∕δ).	(18)
With a similar analysis to the proof of Part (a), we now begin to bound the variance of
Eso f (Ax(S0), yS，; Zi).
Ez」(Es，f (Ax(S0), ySo; Zi))2] ≤ ESs，[(f (Ax(S0), y；0; Zi))2]
=EZEso[(f (Ax(S0), yS，； Z))2]
=EZEs[(f (Ax(S), yS; Z))2].
There holds that ESEs，FS(Ax(S0), YS，) = Es，F(Ax(S0), y；，). Then, by the Bernstein inequality
in Lemma 4, we obtain that with probability at least 1 - δ∕3,
|Es，F(Ax(S0),y；，)- E；，FS(Ax(S0), y；，)|
≤
2EzES[(f(Ax(S), y；; Z))2]log(3∕δ) +
2M log(3∕δ)
3n
(19)
n
22
Published as a conference paper at ICLR 2022
Combined (18) with (19), We finally obtain that with probability at least 1 - 2δ∕3,
F(Ax(S), yS) - FS(Ax(S), yS)
>fA三三三I + 2M l°g(3) + 50√2eeLβ+± diog2 ne 侬隹).
V	n	3n	μ	δ δ /
(20)
In the following, we define q = q(zι,…，Zn) = Ez [(f (Ax(S), yS; Z ))2] and qi = q£zi,…，Zn)=
supzi∈Z q(z1, ..., zn). So there holds qi ≥ q for any i = 1, .., n and any {z1, ..., zn} ∈ Zn. Also,
there holds that
n
X(q - qi)2
i=1
=XX (EZ[(f (Ax(S), yS; Z))2] - sup EZf(Ax(S),寸S; Z))2])
i=1	zi∈Z
n2
≤ X sup (f (Ax(s), yS; Z ))2 - (f (Ax(s), yS; Z ))2]
i=1	zi∈Z
≤n(- + 1) L2e2 (2EZ[f(Ax(S), yS; Z)] + (— + 1) Le)
'μ	J ∖
≤8n(β + l)2L2e2q + 2n(β + l)4L4e4,
μ
where the first inequality follows from the Jensen’s inequality and the second inequality follows from
a similar analysis to (12) or (15). Now, we know that q is ^8nQ + 1)L2e2,2n(μ + 1)L4e4)-
weakly self-bounded. Thus, by Lemma 3, we obtain the following inequality with probability at
least 1 - δ∕3,
ESEz[(f (Ax(S), yS; Z))2] - EZ[(f (Ax(S),寸S； Z))2]
≤y(16n(μ + 1)2L2e2ESEz[(f (Ax(S), yS; Z))2] +4n(+ 1)4L4e4) log(3∕δ)
=j(ESEz[(f (Ax(S), yS; Z))2] + ； (: + 1)2L2e2) 16n(/ + 1)2L2e2 log(3∕δ)
≤ 2(ES EZf(Ax(S), yS;Z))2]+4( μ+1)L2e2)+8n( μ+1)L2e2 logEδ),
where the last inequality follows from √ab ≤ a++b for all a, b > 0.
Since Ez[(f (Ax(S), yS; Z))2] ≤ MF(Ax(S), yS), we have
EsEz[(f (Ax(S), yS; Z))2] - 2MF(Ax(S), yS)
≤4 (μ + 1)2L2e2 + 16n(/ + 1)2L2e2 log(3∕δ).	(21)
Plugging (21) into (20), we finally obtain that with probability at least 1 - δ,
F(Ax(S), yS) - Fs(Ax(S), yS)
≤
(4MF(Ax(S), yS) + 1 (μ + 1)2L2e2 + 32n(μ + 1)2L2e2 log(3∕δ)) log(3∕δ)
n
ι 2M log(3∕δ)
+	3n
+ 50V2eeL' + "dlog2 n] log(3e∕δ).
(22)
μ
23
Published as a conference paper at ICLR 2022
By the elementary inequalities √ab ≤ ηa + 1 b and √α + b ≤ √a + ʌ/b for any a,b,η > 0, We
have the following inequality with probability at least 1 - δ
F(Ax(S), yS) - (1+ η)Fs(Ax(S), yS) ≤ C1+η (M log 1 + (β + i)l≡log2 nlog 1),
η ∖ n δ ∖μ )	δ)
where C is an absolute constant. The proof is complete.	口
A.3 Proof of Part (C)
We then prove the strong PD population risk bound. To begin, we introduce the following concen-
tration inequality, which is a moment version of the Bernstein inequality.
Lemma 6. (Boucheron et al., 2013) If z1, ..., zn are i.i.d., zero mean and |zi| ≤ M almost surely.
Then, for any p ≥ 2,
nu
Xzip ≤6tu
n
X Ezi2 p + 4pM.
i=1
Proof. Let S	=	{z1,	..., zn}	be a set of independent random variables	each taking values
in Z and S0	=	{z10 ,	..., zn0 }	be its independent copy. For any i ∈	[n], define S(i) =
{z1, ..., zi-1, zi0, zi+1, ..., zn} be a dataset by replacing the i-th sample in S with another i.i.d.
sample Zi Denote yS = argmaxy∈γ F(Ax(S), y), yS = argmaxy∈γ FS(Ax(S), y), XS =
argminx∈x F(x,Ay(S)) and XS = argminx∈χ FS(x,Ay(S)).
The proof of Part (c) requires a pretty technical error decomposition, i.e.,
4s (Ax(S),Ay(S))
=F(Ax(S), yS) - inf F(x0, Ay(S))
x0 ∈X
=F (Ax(S), yS) - FS (Ax(S), yS) + Eso FS (Ax(S0), yS，)- Es F (Ax(S), y)
+ Es F (xS , Ay(S)) - Eso FS (xS，,Ay (S 0)) + FS (XS ,Ay (S)) - if F H Ay(S))
x， ∈X
-EsoFs(Ax(S0), yS，)+ ESF(Ax(S), yS) + Es，FS(xS，,Ay(S0)) - ESF(XS, Ay(S))
+ Fs(Ax(S), yS) - Fs(XS,Ay(S)).
Let,s first consider the term F(Ax(S),yS) - FS(Ax(S),yS) + Es，FS(Ax(S0),yS，)-
ESF(Ax(S), yS). It can be then decomposed into
n
F(Ax(S), yS) - Fs(Ax(S), yS) + n X Ez，忸Z[f (Ax(S⑴),y⑸；Z)] - f (Ax(S(i)), y(i ； T
i=1
n
--XEzJEZ[f(Ax(S⑺),yS⑸；Z)] - f (Ax(S⑴),yS®; T + Es，FS(Ax(S0), yS，)
n i=1
-Es F (Ax(S), yS),
which can be bounded by the proof techniques in the proof of Part (b). According to (17), we know
that
kF(Ax(S), yS) - Fs(Ax(S), yS) + Es，FS(Ax(S0),寸S，)- ESF(Ax(S),寸S)kp
≤50√2(l + β)eLpdlog2 n].	(23)
For the second term ESF(XS, Ay(S)) - Es，FS(xS，, Ay(S0)) + FS(XS,Ay(S))-
inf x， ∈X F(X0, Ay(S)), we have the following decomposition
nFs(XS, Ay(S)) - n inf F(X0,Ay(S))
x， ∈X
n
=nFs(XS, Ay(S)) - XEZ [f (xS, Ay(S), Z) - E"[/区⑸,Ay(S(i)); Z)]]
i=1
nn
+ XEzi [f(xS(i),Ay(S⑴);Zi)- Ez[f(xS⑸,Ay(S⑴);Z)]] - XEzJf(XS® ,Ay(S⑺);zi)].
i=1	i=1
24
Published as a conference paper at ICLR 2022
It is clear that
n
XEz0 f(xS(i) ,Ay(S⑴);T
i=1
n
=X Ez0 f (xS(i) , Ay (S ⑴);Zi)- f (xS, Ay(S); Zi) + f (xS, Ay(S); &)]
i=1
n
≥nFs(XS,Ay(S)) + XEzi f(xS(i)八⑶力;Zi)-收,Ay(S);&)] .	(24)
i=1
Denote gi(S) = Ez0 [f (xS(i),Ay(S⑴);Zi)- Ez [f (xS(i),Ay(S⑴);Z)]]. By (24), we now get
n
nFs(XS,Ay (S))- nXnX F(x0,Ay (S)) — X gi(S)
x ∈	i=1
n
≤-X EZ [f (xS ,Ay (S); Z)- Ez0 [f (xS(i) ,Ay(S(i)); Z)]]
i=1
n
-XEz0 [f (xS(i) ,Ay(S(i)); Zi)- f (xS, Ay(S); Zi)].
i=1
Furthermore, according to Lemma 5, we have
|f (xS,Ay(S); Z) - f (xS(i),Ay(S(i)); Z)| ≤(1 + μ"kAy(S) - Ay(S(i))k.
Similarly,
|f (xS(i) ,Ay (S ⑺);Zi)- f (xS , Ay(S); Zi) । ≤ (l + μ "kAy(S)- Ay (S ⑴)k.
Thus, we obtain
n
|nFs(XS, Ay(S)) - n*X F(x0, Ay(S)) - Xgi(S)|
x ∈	i=1
≤ X 2(1 + β)L∣∣Ay(S⑴)-Ay(S)k ≤ 2n(l+ β)Le,	(25)
i=1	` μ	' μ
where the last inequality follows from the definition of argument stability (Part 2 of Definition 3).
Furthermore, we define hi(S) = gi(S) - ES\{zi}[gi(S)]. For hi(S), We have ES\{zi}[hi(S)] = 0
and Ezi [hi(S)] = Ezi [gi (S)] - Ezi ES\{zi} [gi (S)] = 0 - 0 = 0. Moreover, for any j ∈ [n] with
j 6= i, and Zj00 ∈ Z, we get
Ihi(S) - hi(Z1, …，Zj-1, Zj0, Zj+1, …，Zn)| ≤ 2(1 + μ)L3
where this inequality follows from a similar analysis to (14) and (15) of the proof of Part (b). We
thus can obtain that for any p ≥ 2, there holds
Il X hi(S)∣∣ ≤ 48√2pn(β + 1)LEdlog2 n]∙
lli=1	"p	vμ	7
Combined (25) with (26), we finally get the bound of the second term:
∣∣EsF(xS,Ay (S)) - Es0FS(x"Ay(S0)) + FS(XS,Ay (S)) - XnX F(x0,Ay (S))(
∣	1 n ∣	∣1 n ∣
≤∣∣F⅛(XS, Ay(S))-JnXF(x0,Ay(S)) - -Xgi(s)∣∣p+∣∣-Xhzllp
≤50√2(l + 2卜Lpdlog2 n].
(26)
(27)
25
Published as a conference paper at ICLR 2022
We then consider the third term -Es，FS(Ax(S0), ySo )+EsF(Ax(S), YS)+Es，FS(xS，, Ay(S0))-
ESF(XS,Ay(S)). It is clear that ES[Es，FS(xS，,Ay(S0))] = ESF(XS,Ay(S)) and
Es [Es,Fs(Ax(S0), yS，)]= ESF(Ax(S), yS).
Moreover, we having the following important property due to the strong convexity and strong con-
cavity of F ,
E [(Es，f (xS，,Ay(S0); Zi)- Es，f (Ax(S0), yS，； a))2]
≤EEso [L2(kχSo - Ax(S0)k + kAy(S0) - yS，k)2]
≤2L2EEs,[kxS，- Ax(S0)k2 + kAy(S0) - yS，k2]
≤4L2μ-1EEs,[F(Ax(S0), yS，) - F(xS，,Ax(S0))]
=4L2μ-1Es,[F(Ax(S0), yS，) - F(xS，, Ax(S0))],	(28)
where the first inequality follows from Jensen’s inequality and the Lipschitz continuity of f (As-
sumption 1), the second inequality follows from that (a + b)2 ≤ 2a2 + 2b2 and the third inequality
follows from the property of strong convexity and strong concavity of F and the optimality condi-
tion, derived as follows,
F(Ax(S0), yS，) - F(xS，,Ax(S0))
=F(Ax(S0), yS，) - F(xS，, yS，) + F(xS，, yS，) - F(xS，,Ax(S0))
≥2hkAx(S0) -xS，k2 + kys，- Ax(S0)k].
It is clear that E[(Z - EZ)2] ≤ E[Z2]. Therefore, by the variance bound in (28) and apply-
ing the moment Bernstein inequality in Lemma 6 to the sum of independent random variables
-Es，f (Ax(S0), yS，； Zi)+ Es，f (xS，,Ay(S0); Zi) + ES F (Ax(S), yS) - ES F(XS, Ay(S)),wehave
the following inequality for all p ≥ 2,
1 n
Il n X -Es，f (Ax(S0), yS，； Zi) + Es，f (xS，,Ay(S0); Zi) + ESF(Ax(S), yS) - ESF(XS, Ay(S)) ∣∣
i=1	p
≤ 6∖SEs，[F(Ax(S0), yS,) - F(xS，, Ax(S0))]4⅛ + 16pM.
V	nμ n
From Definition 1, we know that the last term FS(Ax(S), yS) - FS(XS, Ay(S)) is actually the
strong PD empirical risk 4sS(Ax(S), Ay (S)).
Based on the above analysis, we have derived that for each p ≥ 2,
∣∣F(Ax(S), yS) - xi,nχ F(x0, Ay(S)) -4S(Ax(S),Ay(S))(
≤12^Es，[F(Ax(S0), yS，) - F(xS，, Ax(S0))]pL2 + -nM + 100√2(1 + /KLpdlOg2 n]
≤ɪEs，[F(Ax(S0), yS,) - F(xS，,Ax(S0))] + 山 12pL2 + 16pM
1 + η	η nμ n
+ 100√2 (1 + μ) ELpdlog2 n],	(29)
where the last inequality holds since for any a, b, η > 0, √ob ≤ ηa + M.
Taking p = 2 and using the Cauchy-Schwarz inequality, we obtain that
EShF(Ax(S),yS) — inf F(x0, Ay(S)) -4S(Ax(S), Ay(S))]
x,∈X
≤kF(Ax(S),yS) — inf F(x0,Ay(S)) - 4S(Ax(S),Ay(S))k2
x， ∈X
≤ ɪ Es，[F (Ax(S0), yS，) — F 3,,Ax(S0))] + 山 24L2 + 32M
1 + η	η nμ n
+ 200√2 (1 + -) eLdlog2 n].
26
Published as a conference paper at ICLR 2022
Since Es，[F(Ax(S0), yS，) - F(xS,,Aχ(S0))] = ES[F(Ax(S), yS) - F(XS, Aχ(S))], we finally
get
ES[F(Ax(S), yS) - inf F(x0, Ay(S))]
x0 ∈X
≤(1+ η) (Es 4S (Ax(S), Ay (S)) + 24L2(1 + 4) +32M + 200√2(l + β)四脸 n]).
nμη	n	μ/
Plugging this inequality into (29), we thus have that for each p ≥ 2,
IIF(Ax(S), yS) - xinχ F(x0, Ay(S)) — 4(Ax(S), Ay(S))IL
≤η(Es 4S (Ax(S),Ay(S)) + 24L2(1 + 4) +32M + 200√2(i + β)eLdlog2 n])
∖	nμη	n	∖ μ/	/
+ 12pL2(1 + η) +竺PM + 100√2(1 + β) eLpdlog2 ne.
nμη	n	∖	μ/
According to Lemma 1, for any δ > 0, with probability at least 1 - δ, there holds that
F(Ax(S), yS) — i，nf F(x0, Ay(S)) ≤ 4S(Ax(S), Ay(S)) + ηEs 4S (Ax(S), Ay(S))
x0 ∈X
+ C(1+ η)(L2(1 + 4) + M +(1 + β)乩log2 n) log (ɪ),	(30)
∖ nμη n ∖ μ/	δ δ∂ /
where C > 0 is an absolute constant. The proof is complete.	□
A.4 Proof of Part (d)
We now prove the strong PD generalization error bound.
Proof. From (30) in the proof of Part (c), we know that for anyδ	> 0, with probability at least 1 -δ ,
there holds that
4s(Ax(S),Ay(S)) -4sS(Ax(S),Ay(S))
=F(Ax(S),yS) — inf F(x0,Ay(S)) —4S(Ax(S), Ay(S))
x0 ∈X
≤ηES 4S (Ax(S), Ay(S)) + C(1 + η)(L2(1+ η) + M +(1 + β)eLlog? n) log (1).
∖ nμη n ∖ μ/	δ δ∂∕
Therefore, the proof is complete.	□
A.5 Proof of Part (e)
We finally prove the excess primal population risk bound.
Proof. Denote x* = arg minx∈χ R(x) and yS = argmaxy∈γ F(Ax(S), y). Firstly, we have the
following decomposition
R(Ax(S))— inf R(x0) = R(Ax(S)) —RS(Ax(S))+RS(Ax(S))—FS(x*,Ay(S))
x0∈X
+FS(x*,Ay(S))—F(x*,Ay(S))+F(x*,Ay(S))—R(x*). (31)
Consider the first term R(Ax(S)) — RS(Ax(S)). From (22) of the Part (b), we know that with
probability at least 1 一 δ,
R(Ax(S)) — RS(Ax(S)) ≤ 2M log,30 + 50√2eeLβ+-^「log? n] log(3e∕δ)
3n	μ
(4MF(Ax(S), yS) + 1 (μ + 1)2L2e2 + 32n(μ + 1)2L2e2 log(3∕δ)) log(3∕δ)
.
n
+
27
Published as a conference paper at ICLR 2022
For the second term RS(Ax(S)) - FS(x*, Ay(S)), We have RS(Ax(S)) - FS(x*, Ay(S)) ≤
RS(Ax(S)) - infx0∈Y FS(x0, Ay(S)) =4sS(Ax(S),Ay(S)).
Note that under Assumption 1, the argument stability implies the uniform stability. Therefore, for
the third term FS (x*, Ay(S)) - F (x*, Ay (S)), from (10) of Part (a), we know that with probability
at least 1 - δ
Fs(x*, Ay(S)) - F(x*,Ay(S)) ≤ 2M 咪30 +50√Mlog? n] log(3e∕δ)
/(4MF(x*, Ay(S)) + 2e2 + 32ne2 log(3∕δ)) log(3∕δ)
n
It is clear that F(x*, Ay(S)) - R(x*) ≤ 0.
Since F(x*,Ay (S)) ≤ supyo∈γ F(x*, y0) = R(x*) = infxo∈χ R(x), based on the above results,
we have the following inequality with probability at least 1 - 2δ
R(Ax(S)) - xi0n∈fX R(x)
≤
(4MF(Ax(S), yS) + 1 (μ + l)2L2e2 + 32n(" + l)2L2e2 log(3∕δ)) log(3∕δ)
n
4M log(3∕δ)	/(4M infxo∈χ R(x) + 1 e2 + 32ne2 log(3∕δ)) log(3∕δ)
+ 3n +v--------------------------------r---------------------------
+ 50V2EeL' 十 " dlog2 n] log(3e∕δ) + 4S(Ax(S), Ay(S)) + 50√2e∈dlog2 n] log(3e∕δ)
μ
(1 (μ + l)2L2E2 + 32n(μ + l)2L2e2 log(3∕δ)) log(3∕δ)	〃
≤t	----------—--------------'——+ 卡F(Ax(S )，yS)
1 + η 4M log(3∕δ)	4M log(3∕δ)	I
+ F----------L +	3n	+V
(* 1 e2 + 32ne2 log(3∕δ))log(3∕δ)
n
+ ɪ inf R(x) +山 4M log(3∕δ)
1 + η x0∈X	η	n
+ 50V2EeL' 十 " dlog2 n] log(3e∕δ) + 4S(Ax(S), Ay (S)) + 50√2e∈dlog2 n] log(3e∕δ),
μ
where the last inequality follows from the elementary inequalities √ab ≤ ηα + ^b and √α + b ≤
√a + ʌ/b for any a, b > 0. Therefore, by a rearrangement, we have the following inequality with
probability at least 1 - δ
R(Ax(S)) - (1 + 2η) inf R(x)
x0 ∈X
≤C η(( n- log δ+( μ+I)LE log2 n log δ+4s (Ax(S), Ay(S))),
that is
R(Ax(S)) - (1 + η) inf R(x)
x0 ∈X
≤C十(M log 1 + (μ + 1) Le log2 nlog δ + 4(Ax(S), Ay(S))),
where C is an absolute constant. The proof is complete.	□
Till here, the proof of Theorem 1 is complete.
28
Published as a conference paper at ICLR 2022
B Empirical Saddle Point
Empirical saddle point (ESP) problem refers to problem (2), which is also known as sample average
approximation (SAA) (Zhang et al., 2021a). We denote (XS, yS) as the ESP solution to (2), which
is analogy to the ERM in stochastic optimization (Shalev-Shwartz et al., 2010). We first provide the
main theorem of the ESP solution, as shown below.
Theorem 3. Assumefor all Z,thefUnction (x, y) → f (x, y; z) is μ-SC-SC Suppose |f (x, y; z)| ≤
M for some M > 0 and X ∈ X, y ∈ Y, z ∈ Z. Denote Ax(S) = XS and Ay (S) = yS for
(XS, yS). Fixed any η > 0. There exists an absolute positive constant C.
(a)	If Assumption 1 holds, then for any δ > 0, with probability at least 1 - δ, we have
F(XS, yS) ≤ (1 + η)FS(XS, yS) + C1+η (M iog(i∕δ) + 4L i0g2 niog(i∕δ)).
η ∖n	nμ	J
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(XS) ≤ (1 + η)RS(XS) + C号
M 1 β 4L2	1
匕log δ+ (μ + 1) nμr l0g2n log δ)∙
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s(XS,yS) ≤ c(i + η)(L2(1 + η) + M +
∖ nμη n
(1 + β) 4L- log2 n) log (1).
∖	μ/ nμ δ ∖∂∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s (XS, yS)-4S(XS, yS)
≤C(1 + η) (L (1+ η) + M +(1 + β) 4L- log? n) log (1).
∖ nμη n	∖ μ J nμ δ	∖δ)
(e)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(XS) ≤ (1 + η) xnχ R(X) + C+
Proof. To prove Theorem 3, we should derive the strong PD empirical risk bound and the stability
bound of (XS, yS). It is easy to verify that 4S(XS, yS) = 0 (Zhang et al., 2021a). We then
investigate the stability bound of (XS, yS).
Let S = {z1, ..., zn} be a set of independent random variables each taking values in Z. For any
i ∈ [n], define S(i) = {z1, ..., zi-1, zi0, zi+1, ..., zn} be a dataset by replacing the i-th sample in
S with another i.i.d. sample zi0. We define FS(i) be the empirical risk on dataset S(i) and define
(XS(i), yS(i)) be the ESP solution on dataset S(i).
29
Published as a conference paper at ICLR 2022
Then we have
Fs (XS(i), yS) - Fs (XS,勺S⑺)
1n
=n E (f (XS ⑸,YS ； Zj) -f (X s, y (i) ； Zj 力
n j=1
n
=n( X (f(Xs(i), ys； Zj)- f(Xs, ys(i)； Zj))+〃设⑸,ys； Zi) - f(xs, ys(i) ； Zi))
j=1,j6=i
+ n (f (Xs(i), ys ； Zi)- f (xs , ys ⑸；Zi)) - n (f (x s® , ys ； Z) - f (x s , ys ⑶；Zi))
=FS⑸(XS(i), yS) - FS⑸(XS, yS(i))
+ n (f (xs(i), ys ； Zi)-	f (xs,	ys ； &) +	f(xs, ys ；	Zi)- f(xs, ys ⑸；Zi))
-n (f (xs(i), ys ； z0 )	-	f (xs,	ys ； Zi) +	f(xs, ys ；	Zi)- f(xs, ys ⑸；Zi))
2L
≤FS(i) (XS (i), ys) - FS (i) (xS, ys(i) ) + ~n (kxs(i) - xs k + kyS - ys(i) k)
=Fs(i)(xs(i), ys) - Fs(i)(xs(i), ys ⑸)+ Fs(i)*⑶,ys(i)) - Fs(i)(xs, ys(i))
2L
+ K(kxS(i)- xSk + kyS - yS(i) k)
≤ - μkxS(i) - XSk2 - 2kyS - ys(i)k2 + 2L(kxS(，)- XSk + kyS - yS⑸k),
where the first inequality follows from the Lipschitz continuous assumption, and where the second
inequality follows from the facts that the μ-SC-SC property of Fs(i)and (XS(i), yS⑸)is the ESP
solution of Fs(i) .
Similarly, according to the μ-SC-SC property of FS, We have
Fs (x s(i), ys) - Fs (XS, ys(i))
=Fs (x s(i), ys) - Fs (XS, ys) + FS (XS, ys) - FS (XS, ys ⑸)
≥ 2 kxS(i)- xs k2+2 kys- ys (i) k2
Based on the above results, we have
μkxS(i) - XSk2 + μkyS - yS(i)k2
2L
≤K(kxS(i) - XSk + kys -ys(i)k)
≤2L√2qkxS(i) - XSk2 + kys - yS(i) k2,
where the last inequality uses the Caucy-Schwarz inequality. Therefore, we have
kxS(i) - XSk + kys - yS(i) k
≤,2(kxS(i)- XS k2 + kys -ys(i)k2)
<4L
-nμ
(32)
(33)
Now, plugging this stability bound into Theorem 1, we obtain generalization bounds of the ESP
solution. The proof of Theorem 3 is complete.
□
Remark 8. When conditions in Theorem 3 hold, we obtain that (a) If Assumption 1 holds and
FS (XS, yS) = O (1), then for any δ > 0, with probability at least 1 - δ, the plain generalization
error of (xS, yS) is of the order O (log2 n log(1∕δ)). (b) If Assumptions 1 and 2 hold and RS (XS)=
O(1), then for any δ > 0, with probability at least 1 - δ, the primal generalization error of
30
Published as a conference paper at ICLR 2022
(XS, yS) is of the order O (log2 n log(1∕δ)). (c) If Assumptions 1 and 2 hold, then for any δ > 0,
with probability at least 1 - δ, the strong PD population risk and the strong PD generalization
error of (XS, yS) are all of the order O (log2 n log(1∕δ)). (d) If Assumptions 1 and 2 hold and
infχ∈χ R(X) = O(1), then for any δ > 0, with probability at least 1 - δ, the excess primal
population risk of (XS, yS) is of the order O (log2 n log(1∕δ)).
Remark 9. (Zhang et al., 2021a) also studies the generalization bound of the ESP solution. They
provide O(1∕n) order bounds for weak PD population risk and expected strong PD population
risk. Their proofs also show that the expected strong PD population risk is more difficult to an-
alyze than the former. They have to consider the fact that different XS corresponds to different
y, as discussed in Remark 5. Moreover, the expectation operator in expected strong PD popula-
tion risk also relaxes the difficulty of proof. Specifically, define S(i) be a dataset by replacing the
i-th sample in S with another i.i.d. sample Zi and y*(X)= argmaxy∈γF(x,y), there holds
the following important property E 卜upy∈γ F(XS, y)] = 1 Pn=I E [F(文鼻⑸,y*(XS®))]=
1 Pn=I E [/(XS®, y*(XS(i))； Zn)] because (XS, y*(XS)) and (文鼻⑸,y*(XS(i))) are identically dis-
tributed and the independence between zi and S(i) . On the contrary, when there is no expectation
operator, we do not have this property and the proof is much more challenging.
C Gradient Descent Ascent
We need some notations to state results on GDA. Specifically, assume the initial point satisfies
X1 = 0 and y1 = 0. Let {ηt } be a sequence of positive step sizes. At the t-th iteration, GDA
updates
Xt+ι = Xt - ηNχFS(Xt, yt),
yt+1 = yt + ηtVyFSg, yt).	(34)
We denote the average of iterates by
XT = PTrIXt and yτ = PTrI yt.	(35)
Here, we first provide an important lemma to connect the argument stability with the strong PD
empirical risk, which will also be used in the remaining applications.
Lemma 7. For any i ∈ [n], define S(n) = {Z1, ..., Zn-1, Zn0, Zn+1, ..., Zn }. Let (Xt, yt) be the output
produced by FS on dataset S in running a minimax learning algorithm. Let (Xnt, ytn) be the corre-
sponding output produced by FS(i) on dataset S(n), where FS(i) is empirical risk on dataset S(n).
Suppose Assumption 1 holds. Assume for all Z, the function (x, y) → f (x, y; z) is μ-SC-SC. For
any S(n) and S, we have
kxt—Xtk+kyt—ytk ≤ ~~+4∖ 匚 q 4s (Xt, y”.
nnμ	V μ V
Proof. Define (XS(i), yS(i)) be the ESP solution on dataset S⑺ and (XS, yS) be the ESP solution
on dataset S. To prove the stability bound, we consider
kXnt -Xtk + kytn -ytk
=kχt - xS(i) + xS(i) — XS + XS - Xtk + kyi -寸S ⑸ + yS(i)— yS + yS - ytk
≤kχt - xS(i)k + kxS(i)- XSk + kXS -xtk + kyt - yS⑸k + kyS⑸一ySk + kyS -ytk
4L
≤ — + kxt - xS(i)k + kXS - xt k + kyi - yS® k + kyS - ytk
nμ
≤ nμ + √2jkxt- X S(i)k2 + kyi- ys (i)k2 + √2jkys -ytk2 + kXS -xtk2
≤~~+、匚 qFs(i)(Xt, ys (i))- Fs(i)(X s(i), yi)+、匚 qF(Xt, ys)- FS (XS, yt),
nnμ V μ V	V μ V
31
Published as a conference paper at ICLR 2022
where the second inequality uses the result in (33), the third inequality uses the Caucy-Schwarz
inequality, and the last inequality uses the strong convexity and strong concavity of FS(i) and
FS and the optimality condition (please refer to (32)). As will see in the rest paper, we bound
Fs(i)(xt, yS(i)) - FSG(XSi), yS(i)) and FS(xt, yS) - FS(XS, yt) With the same upper bound
since they are all strong PD empirical risk. Thus, for brevity, we derive the following inequality
kxit-xtk+kyti-ytk
FS(Xt, yS) - FS(χS, yt)
≤ 4L + 4
nμ
≤ 4L +4∖∕1
nμ V μ
4sS(xt, yt).
The proof is complete.
□
Remark 10. Lemma 7 provides the connection betWeen the stability bound and the strong PD em-
pirical risk. The subscript t here represents not only the output of an iterative optimization algorithm,
but any output of the empirical risk of any minimax learning algorithm.
Remark 11. In studying the stability bound of gradient-based optimization algorithms, a popular
approach is to use the property of smoothness to establish the nonexpansiveness of gradient mapping,
proposed in the seminal Work (Hardt et al., 2016). (Farnia & Ozdaglar, 2021; Lei et al., 2021) extend
this approach to the minimax problems and use it to analyze the stability bound of SGDA, GDA,
PPM, etc. HoWever, their stability bounds are often derived in expectation. In (Lei et al., 2021),
the authors also use the Chernoff bounds of Bernoulli variables to establish high probability stability
bounds when they are to derive high probability generalization bounds. Unfortunately, these stability
bounds are often of slow order O(1/√n). To derive sharper stability bounds, we established Lemma
7.
The following lemma shows the strong PD empirical risk of GDA.
Lemma 8. Suppose Assumption 1 holds and FS(∙, ∙) be μ-SC-SC with μ > 0. Let {xt, yt} be the
Sequence produced by (34) with η = 川α］加).Assume to ≥ 0. Suppose supχ∈χ ∣∣xk ≤ RX and
suPy∈Y l∣yk ≤ RY. Thenfor (xT, yτ) in (35) we have
μ μt μ ∙f R X - 0 R μto(Rχ + RY) , L2 log(eT)
SUp FS(xt, y) — inf FS(x, yτ) ≤--------X———+-------------不—.
y∈γ	χ∈X	T	μT
If t0 = 0, then
SUp Fs(Xτ, y) — inf FS(x, ST) ≤ L log(eT).
y∈Y	x∈X	μτ
Proof. Firstly, we have
l∣x⅛+ι — x∣2 = l∣x⅛ — ηt^χFs (xt, yt) — x∣2
=l∣xt - x∣2 + η2∣VχFs(xt,yt)k2 + 2ηthx — xt, VxFS(xt,yt)i
≤ l∣xt - x∣2 + η2L2 + 2ηthx — xt, VxFS(xt, yt)i,
where the first inequality holds because of Assumption 1. By the strong convexity of FS(∙, yt), we
have
2ηt(Fs(xt,yt) - Fs(x,yt)) ≤ (1 -%〃)|氏-x∣∣2 - lxt+ι - xl2 + η2L2.
Since η = *伯；), we further get
2	1	12
μt+0)(Fs(xt,yt) - FsX yt)) ≤ (1-)lxt - xl -lxt+1- xl + (μt+oj) L
Multiplying both sides by t + t0, we have
2	L2
μ(FS(Xt,yt) - Fs(X, yt)) ≤	(t+to	- I)IlXt -	χ∣ι	-	(t+to)∣∣xt+ι - χ∣ι	+	μ2(t+tɔ).
32
Published as a conference paper at ICLR 2022
Since x1 = 0 and PtT=1 t-1 ≤ log(eT), by taking a summation of the above inequality from t = 1
to T , we obtain
XXX(FS(xt, yt) - FS(x, yt)) ≤ 2toRX + L2 lt2g(eT).
t=1	μ
From the concavity of FS (x, ∙) We get
XXX(Fs(xt, yt) - FS(x, yτ)) ≤ 2toRX + L2 l0g(eT).
t=1	μ
Since this inequality holds for any x, We get
XXX(Fs(xt, yt) - inf FS(x, yτ)) ≤ %0RX + L2 lOg(eT).
z—x	χ∈X	2	2μ
t=1
This implies that
1 vTΛ, π /	、 , rt0 R _ μ μtoRX L2 log(eT)
万 E(FS(xt, yt) - inf FS(x, yτ)) ≤ ——I 厂亍—.
T	χ∈X	2T	2μT
In a similar Way, We have the folloWing inequality
π	、	1 vTΛ, π / 八	μtoRY	L2 log(eT)
SUP FS(Xτ, y)-斤 E(FS(Xt, yt)) ≤「厅 Y + -5■斤一.
y∈Y	T M	2T	2μT
Combined the above tWo inequalities together We get
IrL 、	∙f R X - O R μto(Rχ + Rγ) , L2 log(eT)
SUP FS(XT, y) - inf FS(x, yτ) ≤-------X———+-----------不—.
y∈γ	χ∈X	T	μT
□
For optimization algorithm GDA, substituting the strong PD empirical risk bound of (XT, NT) into
Lemma 7, We get the folloWing stability bound,
IIxT - xτIl + kyT - yτIl ≤ —+ 4∖ —q4S(XT, yτ)
nμ y μ V Q
4L 尸 Sμto(Rχ + RY)	L2 log(eT)
≤ nμ+4v μy T + μτ
Furthermore, for any x ∈ X , y ∈ Y and z ∈ Z,
f(x,y;z) - f(0,0;z) ≤ LIx - 0I +LIy - 0I ≤ L(RY + RY),
Which implies that
f(x, y; z) ≤ SUPf(0,0;z) + L(RY +RY).
z∈Z
(36)
(37)
Till here, plugging (37), the stability bound in (36) and the strong PD empirical risk bound in Lemma
8 into Theorem 1, We obtain generalization bounds of GDA.
We noW Write the main theorem of GDA.
Theorem 4. Assumeforall Z,thefunction (x, y) → f (x, y; z) is μ-SC-SC Suppose supχ∈χ ∣∣x∣ ≤
RX and supy∈γ ∣∣y∣ ≤ RY. Let {xt, yt} be produced by (34) with η =川α］加).Assume to ≥ 0.
Denote AX(S) = XT and Ay(S) = Nt for (XT, NT) in (35). Let M = supz∈z f (0,0; z) + L(RX +
RY ). Fixed any η > 0. There exists an absolute positive constant C.
33
Published as a conference paper at ICLR 2022
(a)	If Assumption 1 holds, then for any δ > 0, with probability at least 1 - δ, we have
F(XT, yτ) ≤ (1 + η)Fs(Xτ, yτ)
+ C + (? Iog(10+
(nμ +4 C ^RTJRY +，m n log").
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
1+η
R(Xτ) ≤ (I + η)Rs(XT) + C--
η
× ( M⅛δ + β+μ L( 4L + 4r∕I S μt0(Rx + RY)+ L2i°gP) log2 n log 1).
∖ n	μ	∖μμ	y μ y T	μT )	δ)
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s (XT, yτ) ≤ (1 + η) (μt0(RXr + R) + L2log(eT)) + C(1 + η)
∖ T	μT J
X(+M+山(4L+4√I S μt0(Rχ + RY) + L⅝T))L log2 n) log (1).
∖ nμη n μ ∖nμ	y μ y T	μT /	δ	∖∂∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
人 s/_ ʌ 人 s/_ _ 、	∩t0Q∏X + RY) L2 log(eT八 厂… 、
4 (XT yT) - 4SIXT yT) ≤ N~j~f-----+ μT ) + C(I + η)
X (L2(1 + η) + M+β + μ
n nμη n μ
(M+4∏	+ r)L log2 n) log (δ).
(e)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(XT) ≤ (1 + η) inf R(X) + C2(μ00(RX+ RY) + L^P)
x∈X	η	T	μT
+C 2 ( M log 1+( β + 1)L( 4L +4∖∕I S μ00(RX + RY)+ L⅛T)) log2 n log 1).
η n δ μ nμ μ	T	μT	δ
Remark 12. When conditions in Theorem 4 hold, we obtain that (a) If Assumption 1 holds and
FS(Xt, NT) = O (n), then for any δ > 0, with probability at least 1 - δ, the plain generalization
error of (XT, NT) of GDA is of the order θ(( 1 + JlogT) log? nlog(1∕δ)). (b) If Assumptions
1 and 2 hold and RS(XT) = O (nɔ, then for any δ > 0, with probability at least 1 一 δ, the primal
generalization error of (XT, NT) of GDA is of the order θ((ɪ + JlogT) log2 nlog(1∕δ)). (c)
If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, the strong PD
population risk and the strong PD generalization error of (XT, NT) of GDA are all of the order
θ(( 1 + JlogT) log2 n log(1∕δ)). (d) If Assumptions 1 and 2 hold and inf χ∈χ R(x) = O (ɪ),
then for any δ > 0, with probability at least 1 一 δ, the excess primal population risk of (XT, NT)
of GDA is of the order O((n + JlogT}og2 nlog(1∕δ)). For the above bounds, We can take
T = O(n2) gradient evaluations to get bound of the order O (log： n log(1∕δ)).
D Stochastic Gradient Descent Ascent
We need some notations to state results on SGDA. Specifically, assume the initial point satisfies
X1 = 0 and N1 = 0. Let {ηt } be a sequence of positive step sizes. At the 0-th iteration, SGDA
34
Published as a conference paper at ICLR 2022
first randomly select an index it form the uniform distribution over [n] := {1, ..., n} and then do the
update
Xt+1 = Xt - ηtVχf(xt, yt,Zit),
yt+ι = yt + ηtVy f(χt, yt,zit).	(38)
We denote the average of iterates by
t=1 Xt	t=1 yt
XT = —tT— and yτ = —ψ—.	(39)
Let’s first introduce two concentration inequalities for martingales, which are required in deriving
the strong PD empirical risk bound of SGDA.
Lemma 9. (Boucheron et al., 2013) Let z1, ..., zn be a sequence of random variables such that
zk may depend the previous variables z1, ..., zk-1 for all k = 1, ..., n. Consider a sequence of
functionals ξk(zι,..., Zk), k = 1,..., n. Assume ∣ξk 一 Ezk [ξk]| ≤ bk for each k. Let δ ∈ (0,1). With
probability at least 1 - δ
n	n	n	II
Xξk-XEzk[ξk] ≤(2Xbklog：)2.
k=1 k=1 k=1
Lemma 10. (Tarres & Yao, 2014) Let {ξk}k∈N be a martingale difference sequence in Rd. Suppose
that almost surely ∣∣ξk ∣∣ ≤ D and Pk=ι E[∣∣ξk k2∣ξι,…，ξk-ι] ≤ σ2. Then, for any 0 < δ < 1, the
following inequality holds with probability at least 1 - δ
j
maXtIIX ξk∣∣ ≤ 2(「σt) log δ∙
k=1
The following lemma shows the strong PD empirical risk bound of SGDA.
Lemma 11. Suppose Assumption 1 holds and FS(∙, ∙) be μ-SC-SC with μ > 0. Let {xt, yt} be the
Sequence produced by (38) with η =川α]加).Assume to ≥ 0. Suppose supχ∈χ IlXk ≤ RX and
suPy∈Y kyk ≤ RY. Let δ > 0. Thenfor (XT, yτ) in (39), with probability at least 1 一 δ we have
LL 、	2μtο(RX + RY) ɪ L2 lοg(eT)
SUp FS(xt, y) — inf FS(x, y) ≤ ——T--------- +--------虎~~L
y∈Y	x∈X	T	μT
I 2(RX + RY)(2L + 2L√T)log 6 + 2L(Rχ + RY)(2Tlοg(6∕δ))1 .
Ift0 = 0, then with probability at least 1 一 δ we have
L L L ∙fzr∕ -、jL2 lοg(eT) I 2(RX + ry ) (2L √ √ 行、1	6
SUp FS(xt, y) 一 inf FS(x, yτ) ≤--------- +------------ — + 2L√T log -
y∈γ	χ∈X	μT	T 3 3	δ δ
2L(Rχ + RY )(2T log(6∕δ)) 1
+	T	.
Proof. This proof follows from (Lei et al., 2021). Firstly, we have
∣xt+1 一 x∣2 = ∣xt 一 ηtVxf(xt, yt; zit ) 一 x∣2
=l∣xt - x∣2 + η2∣Vχf(xt,yt； Zit)k2 + 2ηthx - xt, Vxf(Xt, y Zj
≤ ∣xt 一 x∣2 + ηt2L2 + 2ηthx 一 xt, Vxf(xt, yt; zit) 一 VxFS(xt,yt)i + 2ηthx 一 xt,VxFS(xt,yt)i,
where the first inequality holds because of Assumption 1. By the strong convexity of FS(∙, yt), we
have
2ηt(FS(xt, yt) - FS(x,yt)) ≤ (1 一 ηtμ)∣xt - x∣2 一 ∣xt+ι - x∣2 + η2L2
+2ηt hx 一 xt, Vxf(xt, yt； Zit ) 一 VxFS(xt, yt)i.
35
Published as a conference paper at ICLR 2022
Since ηt = “(t；t0), We further get
μ(t⅛(FS (Xt，yt)- FS(X,yt)) ≤ (1- (t⅛) kχt- χk2 -kχt+1- χk2
+ (μ(⅛0) )2L2 + μ(t⅛)hx-Xt，Vxf(Xt，yt; Zit)-VxFS (Xt，yt)i.
Multiplying both sides by t +t0, We have
2	L2
一(FS(Xt, yt) - FS(x, Yt)) ≤ (t + t0 - I)IlXt - Xk - (t + t0)∣xt+ι - Xk +—277~Γ~Γ∖
μ	μ2(t + to)
2
+-〈X — Xt, Vx f (Xt, yt; Zit) — VxFS (Xt, yt)i.
μ
Since X1 = 0 and PtT=1 t-1 ≤ log(eT), by taking a summation of the above inequality from t = 1
to T , We obtain
T
X(FS(Xt, yt) — FS(x, yt)) ≤ μtoRX +
t=1
L2 log(eT)
2μ
TT
+XhX, Vxf(Xt, yt; Zit) — VxFS(Xt,yt)i +XhXt, VxFS(Xt, yt) — Vxf(Xt, yt; Zit)i.
t=1	t=1
From the concavity of FS(x, ∙) we get
X(FS(xt, yt) — FS(x, yτ)) ≤ ^t0RX + LJogeT)
t=1	2	2μ
TT
+XhX, Vxf(Xt, yt; Zit) — VxFS(Xt,yt)i +XhXt, VxFS(Xt, yt) — Vxf(Xt, yt; Zit)i.
t=1
t=1
Since this inequality holds for any X, we get
X(FS(xt, yt) — inf FS(x, yτ)) ≤ /RX + 妇孚吧
x∈X	x∈X	2	2μ
t=1
TT
+	suphX,Vxf(Xt,yt;Zit) — VxFS(Xt,yt)i +	hXt, VxFS(Xt,yt) — Vxf(Xt,yt; Zit)i.
t=1 x∈	t=1
By Schwarz’s inequality, we have
T
X(FS(xt, yt) — 黑χ Fs(x, yτ)) ≤ μtoRX +
t=1	x∈
L2 log(eT)
2μ
TT
+RX	X Vxf(Xt,	yt;	Zit)	—	VxFS(Xt,yt)	+XhXt,	VxFS(Xt,yt)	—	Vxf(Xt,yt; Zit)i.
t=1	t=1
Denote ξt = hXt, VxFS(Xt,yt) — Vxf(Xt,yt;Zit)i. Since Eit[hXt,VxFS(Xt,yt) —
Vxf (xt, yt; Zit )i] = 0, so {ξt |t = 1,..., T} is a martingale difference sequence. By Schwarz's
inequality and Assumption 1, we know that |hXt, VxFS(Xt,yt) — Vxf(Xt, yt; Zit)i| ≤ 2LRX.
Then, according to Lemma 9, we have the following inequality with probability at least 1 — δ∕6
T
Xg, VxFS(xt, yt) — Vxf (xt, yt; Zit)) ≤ 2LRχ(2Tlog(6∕δ))2.
t=1
Define ξt0 = Vxf(xt,yt;Zit) — VxFS(xt,yt). Then we get kξt0k ≤ 2L and
T
XE[kξ0k2∣ξ1,…,ξ0-ι] ≤ 4TL2.
t=1
36
Published as a conference paper at ICLR 2022
Applying Lemma 10 to the martingale difference sequence {ξt0 }, we have the following inequality
with probability at least 1 - δ∕3
T
IIX Et Il ≤ 2( 2L + 2L√T) logδ.
t=1
That is, with probability at least 1 - δ∕3
T
∣l X Vxf(Xt, yt; Zit)- NxFS(χt,yt)∣∣ ≤ 2(ɪ + 2L√T) log 5.
t=1
Combined with the above results, we finally have the following inequality with probability at least
1 - δ∕2
1T
T E(FS(Xt, yt) - XnXFS(X,yτ)) ≤
t=1	x
μtoRχ + L2 log(eT)
2T + —2μτ一
+2RX (2L+2l√t )log 6 +
2LRχ (2T log(6∕δ))1
T
In a similar way, we have the following inequality with probability at least 1 - δ∕2
sup FS (Xt , y)-
y∈Y
1T
T E(FS(Xt, yt)) ≤
t=1
μtoRY	L2 log(eT)
2T + —2μT―
+2RY (2L+2l√t )logδ+
2LRγ (2T log(6∕δ))1
T
Combined the above two inequalities together we get the result with probability at least 1 - δ
FL μ -f R X - 0 R μto(RX + RY) , L2 log(eT)
yuγFS (XT，y)- xnXFS (X，yT) ≤ —X—+
⅛T⅛ (2l +2L√T) log 6 +
2L(Rχ + RY )(2X log(6∕δ))1
X
□
Denote E
μto(RX +RY) , L log(eT) , 2(RX +RY)(2L +2L√T) log 6
τ	1 μτ 1	T
1
,2L(Rχ + Ry)(2T log(6∕δ))2
+	T
Now, plugging Lemma 11 to Lemma 7, we know that the argument stability bound of SGDA is
IIxT - XTIl + IIyT - yTIl ≤ —+ 4a Jq4s(XT, yT) ≤ 4a[-√E +—.	(4O)
nμ y μ V	μ μ nμ
Furthermore, for any X ∈ X , y ∈ Y and z ∈ Z,
f(X, y; z) ≤ f(0, 0; z) + LIX - 0I + LIy - 0I ≤ sup f(0, 0; z) + L(RX + RY),	(41)
z∈Z
Note that since SGDA is a randomized algorithm, thus we need the following variant of Theorem 1.
Theorem 5. Let A be a randomized learning algorithm and > 0. Suppose |f (X, y; z)| ≤ M for
some M > 0 and X ∈ X , y ∈ Y , z ∈ Z. Fixed any η > 0. There exists an absolute positive
constant C.
(a.) If A has -uniform stability with probability at least 1 - δ0 for some δ0 ∈ (0, 1) over the
randomness of A, i.e.,
PrA(sup[f(Ax(S),Ay(S);z)-f(Ax(S0),Ay(S0);z)]) ≤,
And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability
at least 1 - δ0 - δ,
F (Ax(S),Ay(S)) ≤ (1 + η)FS (Ax(S),Ay(S)) + C 号(M lοg(1∕δ) + e l0g2 n log；).
37
Published as a conference paper at ICLR 2022
(b.) Assume that for all X, the function y → F (x, y) is μ-Strongly-Concave. Suppose Assumptions
1 and 2 hold. If the algorithm A is -argument stable with probability at least 1 - δ0 for some
δ0 ∈ (0, 1) over the randomness ofA, i.e.,
PrAkAx(S)-Ax(S0)k+kAy(S)-Ay(S0)k ≤.
And if the randomness ofA is independent of the training set S. Then for any δ > 0, with probability
at least 1 - δ0 - δ,
R(Ax(S)) ≤ (I + η)RS (Ax(S)) + C-η^^ (^n log δ + (μ + I)LE log2 n log δ).
(c.) Assume that for all X and y, the function F (x, y) is μ-SC-SC Suppose Assumptions 1 and 2
hold. If the algorithm A is E-argument stable with probability at least 1 - δ0 for some δ0 ∈ (0, 1)
over the randomness ofA, i.e.,
PrA(kAx(S)-Ax(S0)k+kAy(S)-Ay(S0)k) ≤E.
And if the randomness ofA is independent of the training set S. Then for any δ > 0, with probability
at least 1 - δ0 - δ,
4s(Ax(S),Ay(S)) ≤4sS(Ax(S),Ay(S))+ηES4sS(Ax(S),Ay(S))
+ C(I + η) ( L n+ η) + MM + (1 + β卜L log2 n) log (δ J
n nμη n	μ
(d.) Assume that for all X and y, the function F (x, y) is μ-SC-SC. Suppose Assumptions 1 and 2
hold. If the algorithm A is E-argument stable with probability at least 1 - δ0 for some δ0 ∈ (0, 1)
over the randomness ofA, i.e.,
PrA(kAx(S)-Ax(S0)k+kAy(S)-Ay(S0)k) ≤E.
And if the randomness ofA is independent of the training set S. Then for any δ > 0, with probability
at least 1 - δ0 - δ,
4s(Ax(S),Ay(S))-4sS(Ax(S),Ay(S)) ≤ ηES 4sS (Ax(S), Ay(S))
+C(I+η) (Ln^η)+MM+(1+β 卜L log2 n) log (δ).
n nμη n μ μ
(e.) Assume that for all X, the function y → F (x, y) is μ-StrOngly-concave. Suppose Assumptions
1 and 2 hold. If the algorithm A is E-argument stable with probability at least 1 - δ0 for some
δ0 ∈ (0, 1) over the randomness ofA, i.e.,
PrA(kAx(S)-Ax(S0)k+kAy(S)-Ay(S0)k) ≤E.
And if the randomness ofA is independent of the training set S. Then for any δ > 0, with probability
at least 1 - δ0 - δ,
R(Ax(S)) ≤ (1+η) inf R(X)
x∈X
+C-η^^ (^n log δ + (~ + 1J Le log2 n log δ + 4S (Ax(S), Ay(S ))J.
Therefore, plugging (41), the stability bound in (40) and the strong PD empirical risk bound in
Lemma 11 into Theorem 5, we obtain generalization bounds of SGDA. Now, we write the main
theorem of SGDA as follows.
Theorem 6. Assumeforall Z, thefunction (x, y) → f (x, y; Z) is μ-SC-SC. Suppose supx∈χ ∣∣xk ≤
RX and supy∈γ IlyIl ≤ RY. Let {xt, yt} be produced by (38) with η = 1∕(μ(t + to)). Assume
to ≥ 0. Denote Ax(S) = XT and Ay(S) = yτ for (XT, yτ) in (39). Fixed any η > 0. Let
38
Published as a conference paper at ICLR 2022
M = supz∈Z f (0,0; Z) + L(RX + RY). Let E =的埠+RY) + L 嗒Μ)+ 2网产) (2L +
1
2L√T) log 6 +2L(RX+RY )T2T log(6∕δ))2 and B = nμ +4 /1 √E. There exists an absolute positive
constant C.
(a)	If Assumption 1 holds, then for any δ > 0, with probability at least 1 - 2δ, we have
F(XT, yτ) ≤ (1 + η)Fs(XT, yτ) + C 1+η (M log(1∕δ) + B log2 n log(1∕δ)).
ηn
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - 2δ, we have
R(Xτ) ≤ (1 + η)Rs(XT) + C 1+η (M log δ + (μ + 1)LB log2 n log 1).
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - 2δ, we have
4s(X τ, yτ) ≤	(1 + η)E +	C (1	+ η)( L2(1+ η)	+ M	+ (1	+ β )bl log2 n)	log (1).
∖ nμη n μ μ/	δ ∖o∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - 2δ, we have
4s (Xτ, yτ) - 4S(Xτ, yτ)
≤ ηE + C(1 + η)(L2(1 + η) + M + (1 + β)BLlog2n) log (1).
nμη n	μ	o
(e)	If Assumptions 1 and 2 hold, then for any o > 0, with probability at least 1 - 2o, we have
R(xT) ≤ (1 + η) inf R(x) + C2-+^ (M log 1 + (β + 1)LB log? n log 1 +
x∈X	η n o μ	o
Remark 13. When conditions in Theorem 6 hold, we obtain that (a) If Assumption 1 holds and
FS(Xτ,yτ) = O(1), then for any O > 0, with probability at least 1 - O, the plain gen-
eralization error of (XT,NT) of SGDA is of the order O((1 + Jlo^F)) log2 nlog(1∕O)).
(b) If Assumptions 1 and 2 hold and RS (XT) = O (1), then for any O > 0, with prob-
ability at least 1 - O, the primal generalization error of (XT, yτ) of SGDA is of the order
O ((1 + Jlo^yδ)) log2 nlog(1∕O)). (C)If Assumptions 1 and 2 hold, then for any O > 0,
with probability at least 1 - O, the strong PD population risk and strong PD generalization error of
(Xτ, yτ) of SGDA are all of the order O ((1 + J log(1l∕δ)) log? n log(1∕O)). (d) If Assumptions 1
and 2 hold and infχ∈χ R(x) = O(1),then for any O > 0, with probability at least 1 - O, the excess
primal population risk of (X T, yτ) of SGDA is of the order O ((n + J lo^1/6)) log2 n log(1∕O)).
For the above bounds, we can take T = O(n4 ) stochastic gradient evaluations to get bound of the
order O (* log3(1∕O)).
E Proximal Point Method
One of the classical algorithms studied for solving the minimax problem is the Proximal Point
method (Rockafellar, 1976). We denote the t-th iterate of PPM as (Xt, yt). The averaged iterate is
defined as
1τ	1τ
Xτ = TEXt and yτ = TEyt.
t=1	t=1
(42)
39
Published as a conference paper at ICLR 2022
Given stepsize parameter ν, the PPM generates the iterate {xt+1, yt+1} by
argmin argmax <j FS(x, y) + 1- ∣∣x - Xtk - 1- ∣∣y - ytk \ .	(43)
x∈X	y∈Y	2ν	2ν
{Xt+1, yt+1} is the unique solution since the objective function of problem (43) is strongly convex
in X and strongly concave in y. From the discussion in (Mokhtari et al., 2019), the update of PPM
can be written as
xt+ι = Xt - VVχFs(xt+ι, yt+ι),
yt+ι = Yt + v Vy FS (xt+ι, yt+ι),	(44)
Assume that the initial point satisfies X0 = 0 and y0 = 0.
We now begin to prove the strong PD empirical risk. Firstly, two lemmas are introduced.
Lemma 12. (Nemirovski, 2005) Define vector v = [X, y] ∈ R2d and the operator P : R2d 7→ R2d
as
P(v) = [VxFS(X,y); -VyFS (X, y)].
(45)
Consider (XT, NT) in (42). Suppose the ESP solution exists. Assume the function FS(x, y) is
continuously differentiable in X and y. Assume that FS (X, y) is a convex function of X for any y
and is a concave function of y for any X. Then for any v = [X, y] ∈ R2d, we have
1T
FS(XT, y) - FS(x, yτ) ≤ TEP(Vt)T(Vt- v).
t=1
Lemma 13. (Mokhtari et al., 2019) Consider the sequence of iterates {Vt} ∈ R2d generated by the
following update
Vt+1 = Vt - νP(Vt+1),
where P is a monotone and Lipschitz continuous operator, and ν is a positive constant. Then for
any V ∈ R2d and for each t ≥1 we have
P(Vt+i)T(vt+ι - V) = 2Vkvt - vk2 - 2Vkvt+ι - vk2 - 21vkvt+ι - vtk2.
The following lemma is the strong PD empirical risk bound of PPM.
Lemma 14. Let {Xt, yt} be the iterates generated by PPM in (44). Assume ν is a positive constant.
Suppose the ESP solution exists. Assume that FS (X, y) is a convex function of X for any y and
is a concave function of y for any X. Suppose supx∈X ∣X∣ ≤ RX and supy∈Y ∣y∣ ≤ RY. If
Assumption 2 holds, then for all T ≥1, we have
sup FS(XT, y) - inf FS(x, y) ≤
y∈Y	x∈X
RX + RY
-2VT-
Proof. The update of the PPM in (44) can be written as
vt+1 = vt - νP(vt+1).
According to Lemma 1 in (Mokhtari et al., 2019), if FS(X, y) is convex-concave and Assumption 2
holds, then P(v) defined in (45) is monotone and Lipschitz continuous. According to Lemma 13,
we have
P(vt+I)T(Vt+ι - v) = 2Vkvt - vk2 - 2Vkvt+ι - vk2 - 21vkvt+ι - vtk2.
Taking a summation of the above inequality from t = 0 to T -1, we obtain
T-1	1	1
X P(Vt+ι)T(Vt+ι - v) ≤ 2kvo- vk2 - 2kvT - vk2.
t=0	2ν	2ν
(46)
40
Published as a conference paper at ICLR 2022
According to (46), we know that
T-1	1
P(v P(vt+I)T(Vt+1 - V) ≤ 2Vkv0 - vk2
t=0	2ν
=kχo- χk2 + kyo- yk2
=	2ν
Combined this result with Lemma 12, we can write
FS(XT, y) - FS(x, yτ) ≤ kx0-xk22+F-yk2,
which implies that
SUp FS(XT, y) - inf FS(x, ST) ≤ RX +RY ∙
y∈Y	x∈X	2νT
The proof is complete.
□
Combined Lemma 7 and Lemma 14, we know that the argument stability bound of PPM is
IIxT - XTk + kyT - yTk ≤-+ 4I∖q4S(XT, yT)
nμ y μ v
≤ 4L + 4√1 rRXHE
—nμ +4V μV	2νT	.
Furthermore, for any X ∈ X , y ∈ Y and z ∈ Z,
f(X,y;z) ≤ f(0,0;z) +LkX - 0k +Lky - 0k ≤ sUp f(0, 0; z) +L(RX+ RY),
z∈Z
(47)
(48)
Therefore, plugging (48), the stability bound in (47) and the strong PD empirical risk bound in
Lemma 14 into Theorem 1, we obtain generalization bounds of PPM, shown as below.
Theorem 7. Assumeforall Z,thefunction (x, y) → f (x, y; z) is μ-SC-SC Suppose supχ∈χ ∣∣x∣ ≤
RX and sUpy∈Y kyk ≤ RY. Let {Xt , yt } be produced by (44). Assume ν is a positive con-
stant. Denote Ax(S) = XT and Ay(S) = NT for (xT, St) in (42). Fixed any η > 0. Let
M = supz∈z f (0,0; z) + L(Rχ + RY) .Let E = RX+RY and B = nμ + 4^1 √E. There exists
an absolute positive constant C.
(a)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
1 + η
η
F(Xτ, yτ) ≤ (1 + η)FS(Xτ, yτ) + C
log(1∕δ) + B log2 n log(1∕δ).
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(xτ) ≤ (1 + η)RS(Xτ) + C1+^ (M log δ + (μ + 1)LB log2 n log 1).
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s(x T, yτ) ≤	(1 + η)E	+ C (1 + η)( L2(1+ n)	+ M +	(1 + β )bl log2 n)	log (1).
∖ nμη n	∖ μ/	δ ∖o∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s (Xτ, yτ) — 4S(Xτ, yτ)
≤ ηE + C(1 + η)(L2(1 + η) + M + (1 + β)BLlog2n) log (1).
∖ nμη n ∖ μ/	0	0∂∕
(e)	If Assumptions 1 and 2 hold, then for any o > 0, with probability at least 1 - o, we have
R(xT) — (1 + η) inf R(x) ≤ C 2 + η (M log 1 + (β + 1)lB log? n log 1 + e).
x∈X	η'nδ∖μ∕	0	/
41
Published as a conference paper at ICLR 2022
Remark 14. When conditions in Theorem 7 hold, we obtain that (a) If Assumption 1 and 2 hold and
FS (XT, yτ) = O (n), then for any δ > 0, with probability at least 1 - δ, the plain generalization
error of (XT, NT) of PPM is of the order O((1 + ,1)log2 n log(1∕δ)). (b) If Assumptions
1 and 2 hold and RS (XT) = O(1), then for any δ > 0, with probability at least 1 - δ, the
primal generalization error of (XT, NT) of PPM is of the order O ((1 + Ti) log2 n log(1∕δ) .
(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, the strong
PD population risk and strong PD generalization error of (XT, NT) of PPM are all of the order
O((1 + y∕T) log2 nlog(1∕δ)). (d) If Assumptions 1 and 2 hold and infχ∈χ R(X) = θ( 1),
then for any δ > 0, with probability at least 1 - δ, the excess primal population risk of (XT, NT) of
PPM is of the order O ((1+⑪) log2 nlog(1∕δ) . For the above bounds, we can take T = O(n2)
gradient evaluations to get bound of the order O (Ionn log(1∕δ)).
F	Extragradient Method
EG is a classical algorithm for solving minimax problems introduced by (Korpelevich, 1976). We
now introduce some notations. Followed (Mokhtari et al., 2019), we consider the following update
of EG: given stepsize parameter ν, We first compute a set of mid-point iterates {Xt+1, y+1}
Xt+ 2 = Xt - VyxFS(Xt, yt),
yt+2 = yt + VPyFS (Xt, yt),	(49)
we then compute the next iterates {Xt+1, Nt+1}
Xt+1 = Xt - VVxFS(Xt+1, yt+2),
yt+1 = yt + VVy FS (Xt+2, yt+2).	(5O)
Consider the averaged iterate
1T	1T
XT = T £xt and Nt = T Eyt.	(51)
t=1	t=1
Assume that the initial point satisfies X0 = X-1/2 and N0 = N-1/2 .
We now show the strong PD empirical risk bound for (XT, NT) of EG.
Lemma 15. (Mokhtari et al., 2019) Let {Xt, Nt}, {Xt+1/2, Nt+1/2} be the iterates generated by the
EG updates in (49) and (50). Assume that the initial point satisfies X0 = X-1/2 and N0 = N-1/2.
Suppose the ESP solution (X*, N*) exists. Assume that FS(x, n) is a Convexfunction of X for any
N and is a concave function of N for any X. If Assumption 2 holds and the stepsize V satisfies the
condition V =京 for any C ∈ (0,1), then:
(a)	the iterates {Xt, Nt}, {Xt+1/2, Nt+1/2} stay within the compact convex set
D = {(x, N)lkX - X*k2 + kN - N*k2 ≤ (2+ 1- 4v2供)(kX0 - X*k2 + kN0 - N*k2)}.
(52)
(b)	for all T ≥ 1, we have
sup	FS (XT, N)-MFS (X, NT) ≤ 2"+ 代*「*『+ 加7^
y：(xT ,y)∈D	x:(x,yT )∈d	T
42
Published as a conference paper at ICLR 2022
Combined Lemma 7 and Lemma 15, we know that the argument stability bound of EG is
IlxT - XTk + IIyT — yτk ≤ —+ 4[ ∣~q 4S (xτ, yτ)
nμ	y μ V Q
< 4L+ 4 r1 s2β(16+ 2(i⅜))(kx0-X*k2 + IR-V*k2)
-nμ	V μ V	T
(53)
Furthermore, for any x ∈ X , y ∈ Y and z ∈ Z,
f(x,y; Z) ≤ f (X*, y*; Z) + Lkx - x*k + Lky - y*k
≤ SUp f (x*, y*; z) + √2Lp∣∣χ - x*k2 + ky - y*k2
z∈Z
≤ SUp f (X*, y*; Z) + Ll / (4+ —3^2 )(kx0 - X *k2 + M - y *k2),	(54)
z∈Z	1 - 4ν2β2
where the first inequality follows from Assumption 1, the second inequality follows from Caucy-
Schwarz inequality and the last inequality follows from (52). Now, plugging (54), the stability
bound in (53) and the strong PD empirical risk bound in Lemma 15 into Theorem 1, we obtain
generalization bounds of EG. The main theorem is shown below.
Theorem 8. Assume for all Z, the function (x, y) → f (x, y; z) is μ-SC-SC Let {xt, yt} and
{xt+1/2 , yt+1/2 } be the iterates produced by (49)-(50). Assume the stepsize ν satisfies the con-
dition V = 2β for any C ∈ (0,1). Denote Ax(S) = XT and Ay(S) = yτ for (XT, yτ) in
(51). Denote the ESP solution as (X*, y*). Consider the compact convex set in (52). Fixed any
η>
B=
0. Let M = supz∈z f(X*, y*; z) + Ld(4+ 「含声)(kxo - X*k2 + kyo - y*k2). Let
2β(16+ 2⅛ Xkx0-x*k2 + ky0-y*k3	4L,,Hgb	一	，一 .
-------(---L----T---------------and E = 4Lμ + 4、μ - √B. There exists an absolute PoSI-
tive constant C.
(a)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
F(Xt, Vt) ≤ (1 + η)Fs(Xt, NT) + C1+η (M log(1∕δ) + B log2 n log(1∕δ)).
ηn
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(Xτ) ≤ (1 + η)Rs(xt) + C ： η (- log δ + (μ + I)LB l0g2n log δ).
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4S(Xτ, yτ)	≤ (1 + η)E + C(1	+ η) (—~~η^+ +-+	(1	+	2)BL log2	n)	log	(K).
∖ nμη n ∖ μ/	δ δo∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
4s (Xt, yτ) — 4S(Xt, yτ)
≤ ηE + C(1 + η)(L2(1 + η) + M + (1 + β)BLlog2n) log (1).
∖ nμη n ∖ μ/	δ δ∂∕
(e)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, we have
R(XT) ≤ (1 + η) inf R(x) + C 2 + η (M log 1 + (β + 1)lB log2 n log1 + e).
x∈X	η'nδ∖μ∕	δ /
Remark 15. When conditions in Theorem 8 hold, we obtain that (a) If Assumptions 1 and 2 hold and
FS (Xt , yτ) = O (n), then for any δ > 0, with probability at least 1 一 δ, the plain generalization
43
Published as a conference paper at ICLR 2022
error of (XT, NT) of EG is of the order O( +	log? n log(1∕δ)). (b) If Assumptions 1
and 2 hold and RS(XT) = O (nl), then for any δ > 0, with probability at least 1 - δ, the primal
generalization error of (XT, NT) of EG is of the order O((. + ʌ∕T^) log2 nlog(1∕δ)). (c) If
Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 - δ, the strong PD
population risk and the strong PD generalization error of (XT, y) ofEG are all of the order O ((n1+
^z∕T0 log2 n log(1∕δ)). (d) If Assumptions 1 and 2 hold and infχ∈χ R(X) = O(1), then for
any δ > 0, with probability at least 1 - δ, the excess primal population risk of (XT, NT) of EG
is o((1 + qT) log2 nlog(1∕δ)). For the above bounds, We can take T = O(n2) gradient
evaluations to get bound of the order O (Ionnn log(1∕δ)).
G Optimistic Gradient Descent Ascent
OGDA is introduced by Popov (1980), as a variant of the EG method. We introduce some notations
to state the result of OGDA. Given a stepsize parameter ν > 0, OGDA do the following update for
each t ≥ 0
Xt+1 = Xt - 2νVχFs(Xt, Yt) + VVχFs(x-i, yt-ι),
yt+1 = Yt + 2νVyFS(Xt, yt) - VVyFS(x-i, yt-ι).	(55)
Assume that the initial point satisfies X0 = X-1 and N0 = N-1. Consider the averaged iterate
1T	1T
XT = T EXt and Yt = T Eyt∙	(56)
t=1	t=1
We first provide a lemma on the strong PD empirical risk of OGDA.
Lemma 16. (Mokhtari et al., 2019) Let {Xt, Yt} be the iterates generated by the OGDA updates in
(55). Assume that the initial point satisfies X0 = X-1 and Y0 = Y-1. Suppose the ESP solution
(X*, y *) exists. Assume that FS (x, y) is a convex function of X for any y and is a concave function
of y for any X. IfASSumPtiOn 2 holds and the stepsize V satisfies 0 < ν ≤ 4β, then:
(a)	the iterates {Xt, Yt} stay within the compact convex set
D ：= {(χ,y)lkχ -X*k2 + ky - Y*k2 ≤ 2(kχ0 - X*k2 + kyo -y*k2)}.	(57)
(b)	for all T ≥ 1, we have
FL ( ∙f	― ∖^(16β	+ 2ν )(kχo	- X *k2 + kyo-y *k2)
sup FS (xt , y)	- , inf、	FS (x, YT) ≤---------2ν---------------------------.
y:(又 T ,y)∈D	x:(x,yT )∈D	1
Combined Lemma 7 and Lemma 16, we know that the argument stability bound of OGDA is
kχ T - χ T k+kYT - yt k ≤ nμ+4 Jl ∕4w
≤ 4L + 4
nμ
(16β + 2V )(kx。-X*∣∣2 + ∣∣Yo-Y*∣∣2)
T
(58)
Moreover, similar to (54), we have
f (χ, y; z) ≤ f(x*, Y*; z) + LkX - X* k + Lky - Y* k
≤ SUp f (X*, Y*; z) + 2LPkχo - X*k2 + kyo - y*k2.	(59)
z∈Z
Therefore, plugging (59), the stability bound in (58) and the strong PD empirical risk bound in
Lemma 16 into Theorem 1, we obtain generalization bounds of OGDA.
44
Published as a conference paper at ICLR 2022
Theorem 9. Assume for all Z, the function (x, y) → f (x, y; z) is μ-SC-SC. Let {xt, yt} be pro-
duced by (55). Assume the stepsize parameter V satisfies 0 < ν ≤ 4β. Denote Ax(S) = XT and
Ay (S) = yτ for (XT, yτ) in (56). Denote the ESP solution as (X*, y*). Consider the compact con-
vex set in (57). Fixed any η > 0. Let M = suPz∈z f (X*, y*; z) + 2Lpkxo — X*k2 + kyo — y*k2.
Define B = (16β+ 2ν)(kx0-T k +ky0-y k ) and E = 4L + 41/1 √B. There exists an absolute
T	nμ V μ
positive constant C.
(a)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, we have
F(Xt, yτ) ≤ (1 + η)Fs(Xt, yτ) + C 1+η (M log(1∕δ) + B log2 n log(1∕δ)).
ηn
(b)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, we have
R(xτ) ≤ (1 + η)Rs(XT) + C 1+η (M log δ + (μ + 1)LB log2 n log 1).
(c)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, we have
△s(Xτ,yτ) ≤ (1 + η)E + C(1 + η)(L(1 + η) +竺 + (1 + -)BLlog?n) log (∣).
∖ nμη n ∖	μ/	δ δ∂∕
(d)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, we have
△ (XT, yτ) — 4S (XT, yτ)
≤ ηE + C(1 + η)( L2(1+ η) + M + (1 + β) BL lοg2 n) log (1).
∖ nμη n ∖	μ/	δ δo∕
(e)	If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, we have
R(XT) ≤ (1 + η) inf R(x) + C2-+^ (M log 1 + (β + 1)LB log? n log1 +
x∈X	η ∖ n δ ∖μ )	δ
Remark 16. When conditions in Theorem 9 hold, we obtain that (a) If Assumptions 1 and 2 hold and
FS (Xt , yT) = O (n), then for any δ > 0, with probability at least 1 — δ, the plain generalization
error of (XT, NT) of OGDA is of the order O ((1 +，T1) log2 n log(1∕δ)). (b) If Assumptions 1
and 2 hold and RS (XT) = O (nn), then for any δ > 0, with probability at least 1 — δ, the primal
generalization error of (XT, NT) of OGDA is of the order O((1 + ʌ/lə log? nlog(1∕δ)). (c)
If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 — δ, the strong PD
population risk and the strong PD generalization error of (XT, NT) of OGDA are all of the order
o(( 1 + ʌ∕T) log2 nlog(1∕δ)). (d) If Assumptions 1 and 2 hold and infx∈χ R(x) = o(ɪ),
then for any δ > 0, with probability at least 1 — δ, the excess primal population risk of OGDA is of
the order O
T) log2 n log(1∕δ) . For the above bounds, we can take T = O(n2) gradient
evaluations to get bound of the order O (Ionn log(1∕δ)).
H Auxiliary Descriptions of Table 1
In Table 1, Lip means Lipschitz continuity and S means smoothness. (R)-ESP means the
(regularized)-empirical risk saddle point (Zhang et al., 2021a). C-SC means convex-μ-strongly-
concave, and NC-SC means nonconvex-μ-strongly-concave. A function f(x, y) is called
nonconvex-strongly-concave if f (x, ∙) is strongly-concave for every x. Moreover, a function f (x, y)
is μ-weakly-convex-weakly-concave (WC-WC) if f + 2 (IlXk2 + IlylI2) is convex-concave. V-WC-
WC is a variant of WC-WC, please refer to (Lei et al., 2021). PL means the two-sided PL condition,
which relaxes the convex-concavity requirement of the objective function (Yang et al., 2020) and is
usually used to guarantee the linear convergence rate (Karimi et al., 2016; Yang et al., 2020). AGDA
algorithm is variant of GDA with alternating updates of the primal-dual variables. c is a parameter
in the step size, β is a parameter in Assumption 2 and k := β∕μ.
45
Published as a conference paper at ICLR 2022
I Numerical Experiments
In this section, we report preliminary experimental results to verify our theoretical results by per-
forming numerical experiments on the simulated data. We study how the generalization error would
behave along the number of samples. To this aim, we consider an isotropic Gaussian data vector
Z 〜N(0,Id×d) with zero mean and identity covariance. We Win draw n independent samples
from the underlying Gaussian distribution to form a training dataset S = {z1, ..., zn}. We set the
dimension d of Z as 50. Similar to the strongly-convex-strong-concave case of (Farnia & Ozdaglar,
2021), we consider the following minimax objective function
f (X, y; Z) = XT(Z - y) + 2(IlXlI2 TIyII2).
In the experiments, we set μ = 1 and constrain optimization variables X and y to satisfy ∣∣x∣, ∣y∣ ≤
100 which we enforced by projection. For this minimax objective function, one can verify that
F(X,y)TFS(X,y) =XT(E[Z] TES[Z]);	R(X)TRS(X) =XT(E[Z]TES[Z]),
where E[Z] = 0 since the mean of the underlying Gaussian distribution is 0, and where ES [Z] =
* Pn=I zi. For brevity, we call IXT(E[Z] — ES [Z])| the “generalization error”.
We apply the above experimental settings to validate the theoretical results of GDA, SGDA, EG,
and OGDA. We evaluate the generalization error |XT (E[Z] — ES [Z])| and apply these algorithms
to S. For GDA and SGDA, we consider the stepsize parameter as 1/t. We iterate GDA with n2
times and SGDA with n4 times. The generalization error of GDA and SGDA with different sizes of
training data are reported in Figure 1. And for EG and OGDA, we select the stepsize parameter as
0.003. We run EG and OGDA n2 times. Similarly, the generalization error of EG and OGDA with
different sizes of training data are given in Figure 2. From the two figures, we can see that the line of
best fit for the generalization error is log：：n for GDA, nθgc⅛ for SGDA,喝0；8 n for EG, and nogn
for OGDA. These results match the predictive rates of the plain generalization error and the primal
generalization error in Table 1, i.e., log3/2 n for GDA, Ionn for SGDA, Ionn for EG, and Ionn for
OGDA, which verifies our theoretical findings.
46