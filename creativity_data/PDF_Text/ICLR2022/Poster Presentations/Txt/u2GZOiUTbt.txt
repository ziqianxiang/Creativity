Published as a conference paper at ICLR 2022
Task Affinity with Maximum Bipartite Match-
ing in Few- S hot Learning
Cat P. Le, Juncheng Dong, Mohammadreza Soltani, Vahid Tarokh
Department of Electrical and Computer Engineering, Duke University
Ab stract
We propose an asymmetric affinity score for representing the complexity of uti-
lizing the knowledge of one task for learning another one. Our method is based
on the maximum bipartite matching algorithm and utilizes the Fisher Information
matrix. We provide theoretical analyses demonstrating that the proposed score is
mathematically well-defined, and subsequently use the affinity score to propose a
novel algorithm for the few-shot learning problem. In particular, using this score,
we find relevant training data labels to the test data and leverage the discovered
relevant data for episodically fine-tuning a few-shot model. Results on various
few-shot benchmark datasets demonstrate the efficacy of the proposed approach
by improving the classification accuracy over the state-of-the-art methods even
when using smaller models.
1	Introduction
Leveraging the knowledge of one task in training the other related tasks is an effective approach to
training deep neural networks with limited data. In fact, transfer learning, multi-task learning (Stan-
dley et al., 2020), and meta-learning (Finn et al., 2017) are examples of training a new task using the
knowledge of others. In fact, a strong piece of work (Standley et al., 2020) has shown that training
similar tasks together in multi-task learning often achieves higher accuracy on average. However,
characterizing the similarity between tasks remains a challenging problem. In this paper, we present
a task similarity measure representing the complexity of utilizing the knowledge of one task for
learning another one. Our measure, called Task Affinity Score (TAS), is non-commutative and is
defined as a function of the Fisher Information matrix, which is based on the second-derivative of
the loss function with respect to the parameters of the model under consideration. By definition, the
TAS between two tasks is always greater or equal to 0, where the equality holds if and only if both
tasks are identical. For the classification tasks, the TAS is invariant to the permutation of the data la-
bels. In other words, modifying the numeric order of the data labels does not affect the affinity score
between tasks. Additionally, TAS is mathematically well-defined, as we will prove in the sequel.
Following the introduction of TAS, we propose a few-shot learning method based on the similarity
between tasks. The lack of sufficient data in the few-shot learning problem has motivated us to
use the knowledge of similar tasks for our few-shot learning method. In particular, our approach
is capable of finding the relevant training labels to the ones in the given few-shot target tasks, and
utilizing the corresponding data samples for episodically fine-tuning the few-shot model. Similar
to recent few-shot approaches (Chen et al., 2021; Tian et al., 2020), we first use the entire training
dataset to train a Whole-Classification network. Next, this trained model is used for extraction
of the feature vectors for a set of constructed source task(s) generated from the training dataset.
The purpose of the sources task(s) is to establish the most related task(s) to the target task defined
according to the test data. In our framework, TAS with a graph matching algorithm is applied to find
the affinity scores and the identification of the most related source task(s) to the target task. Lastly,
we follow the standard few-shot meta-learning in which a set of base tasks are first constructed,
and a few-shot model is fine-tuned according to the query set of these base tasks. Our approach
has a unique distinguishing property from the common meta-learning approaches: our base tasks
are constructed only based on the previously discovered related source tasks to episodically fine-
tune the few-shot model. Specifically, the feature vectors of the query data from the base tasks are
extracted by the encoder of the Whole-Classification network, and a k-nearest neighbors (k-NN) is
applied to classify the features into the correct classes by updating the weights in the encoder.
1
Published as a conference paper at ICLR 2022
Using extensive simulations, we demonstrate that our approach of utilizing only the related training
data is an effective method for boosting the performance of the few-shot model with less number of
parameters in both 5-way 1-shot and 5-way 5-shot settings for various benchmark datasets. Exper-
imental results on miniImageNet (Vinyals et al., 2016), tieredImageNet (Ren et al., 2018), CIFAR-
FS (Bertinetto et al., 2018), and FC-100 (Oreshkin et al., 2018) datasets are provided demonstrating
the efficacy of the proposed approach compared to other state-of-the-art few-shot learning methods.
2	Related Work
The similarity between tasks has been mainly studied in the transfer learning literature. Many ap-
proaches in transfer learning (Silver & Bennett, 2008; Finn et al., 2016; Mihalkova et al., 2007;
Niculescu-Mizil & Caruana, 2007; Luo et al., 2017; Razavian et al., 2014; Pan & Yang, 2010; Mallya
& Lazebnik, 2018; Fernando et al., 2017; Rusu et al., 2016; Zamir et al., 2018; Kirkpatrick et al.,
2017; Chen et al., 2018) are based on the assumption that similar tasks often share similar archi-
tectures. However, these works mainly focus on transferring the trained weights from the previous
tasks to an incoming task, and do not seek to define the measurement that can identify the related
tasks. Though the relationship between visual tasks has been recently investigated by various pa-
pers (Zamir et al., 2018; Pal & Balasubramanian, 2019; Dwivedi & Roig, 2019; Achille et al., 2019;
Wang et al., 2019; Standley et al., 2020), these works only focus on the weight-transferring and
do not use task similarity for discovering the closest tasks for improving the overall performance.
Additionally, the measures of task similarity from these papers are often assumed to be symmetric,
which is not typically a realistic assumption. For example, it is easier to utilize the knowledge of a
comprehensive task for learning a simpler task than the other way around.
In the context of the few-shot learning (FSL), the task affinity (similarity) has not been explicitly
considered. Most of the recent few-shot learning approaches are based on the meta-learning frame-
works (Santoro et al., 2016; Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017). In these
approaches, episodic learning is often used in the training phase, in which the FSL models are ex-
posed to data episodes. Each episode, consisting of the support and query sets, is characterized
by the number of classes, the number of samples per class in the support set, and the number of
samples per class in the query set. During the training phase, the loss over these training episodes
is minimized. Generally, these episodic learning approaches can be divided into three main cate-
gories: metric-based method, optimization-based method, and memory-based method. In metric-
based methods (Vinyals et al., 2016; Snell et al., 2017; Koch et al., 2015; Sung et al., 2018), a kernel
function learns to measure the distance between data samples in the support sets, then classifies the
data in the query set according to the closest data samples in the support set. On the other hand,
the goal of optimization-based methods (Finn et al., 2017; Grant et al., 2018; Rusu et al., 2018; Lee
et al., 2019; Nichol et al., 2018) is to find the models with faster adaption and convergence. Lastly,
the memory-based methods (Santoro et al., 2016; Ravi & Larochelle, 2017; Munkhdalai et al., 2018)
use the network architectures with memory as the meta-learner for the few-shot learning. Overall,
episodic learning in FSL has achieved great success on various few-shot meta-datasets.
Recently, several methods with a pre-trained Whole-Classification network have achieved state-of-
the-art performance on multiple FSL benchmarks (Chen et al., 2021; Tian et al., 2020; RodrIgUez
et al., 2020; Rizve et al., 2021). Instead of initializing the FSL model from scratches, these meth-
ods focus on leveraging the entire training labels for pre-training a powerful and robust classifier.
The pre-trained model, by itself, outperforms several meta-learning approaches in numerous FSL
datasets (e.g., miniImageNet, tieredImageNet, CIFAR-FS, etc.). Next, the Whole-Classification net-
work is used as a feature extractor for a simple base learner (e.g., logistic regression, K-nearest
neighbor, etc.) and is often fine-tuned using episodic learning. Various efforts have been investi-
gated to improve the Whole-Classification network for the few-shot learning, including manifold
mixup as self-supervised loss (Mangla et al., 2020), knowledge distillation on the pre-trained clas-
sifier (Verma et al., 2019; Tian et al., 2020), and data-augmentation with combined loss functions
(Rodriguez et al., 2020; Rizve et al., 2021). However, none of these approaches consider the task
affinity in their training procedure. Here, we propose a task affinity measure that can identify the
related tasks to a target task, given only a few data samples. Then we utilize the data samples in the
related tasks for episodically fine-tuning the final few-shot classifier.
2
Published as a conference paper at ICLR 2022
3	Preliminaries
In this section, we present the definition of the task affinity score. First, we need to define
some notations and definitions used throughout this paper. We denote the matrix infinity-norm
by ∣∣B∣∣∞ = maxi,j ∣Bj|. We also denote a task T and its dataset X jointly by a pair (T, X). To
be consistent with the few-shot terminologies, a dataset X is shown by the union of the support set,
Xsupport and the query set, Xquery, i.e., X = Xsupport ∪ Xquery. Let PNθ (T, Xquery) ∈ [0, 1]
be a function that measures the performance of a given model Nθ, parameterized by θ ∈ Rd on the
query set Xquery of the taskT. We define an ε-approximation network, representing the task-dataset
pair (T, X) as follows:
Definition 1 (ε-approximation Network). A model Nθ is called an ε-approximation network for a
pair task-dataset (T, X) ifit is trained using the support data X support such that PNθ (T, X query) ≥
1 - ε, for a given 0 < ε < 1.
In practice, the architectures for the ε-approximation networks for a given task T are selected from
a pool of well-known hand-designed architectures, such as ResNet, VGG, DenseNet, etc. We also
need to recall the definition of the Fisher Information matrix for a neural network.
Definition 2 (Fisher Information Matrix). Fora neural network Nθ with weights θ, data X, and the
negative log-likelihood loss function L(θ) := L(θ, X), the Fisher Information matrix is defined as:
F (θ) = e[Vθ L(Θ)Vθ L(θ)T i = -EhH(L(θ))],	(1)
where H is the Hessian matrix, i.e., H (L(θ)) = VθL(θ), and expectation is taken w.r.t the data.
In practice, we use the empirical Fisher Information matrix computed as follows:
F(θ)= ∣⅛ X VθLi(θ)VθLi(θ)T,
|X| i∈X
(2)
where Li(θ) is the loss value for the ith data point in X. Next, we define the task affinity score,
which measures the similarity from a source task, Ta to a target task, Tb .
Definition 3 (Task Affinity Score (TAS)). Let (Ta, Xa) be the source task-dataset pair with Nθa
denotes its corresponding ε-approximation network. Let Fa,a be the Fisher Information matrix of
Nθa with the query data Xaquery from the task Ta. For the target task-dataset pair (Tb, Xb), let Fa,b
be the Fisher Information matrix of Nθa with the support data Xbsupport from the task Tb. We define
the TASfrom the source task Ta to the target task Tb based on Frechet distance as follows:
s[a, b] :
1
—TraCe
√2
+Fa,b-2(Fa,aFa,b)1/21/2.
(3)
Here, we use the diagonal approximation of the Fisher Information matrix since computing the full
Fisher matrix is prohibitive in the huge space of neural network parameters. We also normalize these
matrices to have unit trace. As a result, the TAS in equation (3) can be simplified by the following
formula:
s[a, b] = √2 IM/2 - FafIlF = √2 [X ((Faia)1/2 - (Faib y)，：	⑷
where Fii denotes the ith diagonal entry of the Fisher Information matrix. The TAS ranges from
0 to 1, with the score s = 0 denotes a perfect similarity and the score s = 1 indicates a perfect
dissimilarity. In the next section, we present our few-shot approach based on the above TAS.
4	Few- S hot Approach
In the few-shot learning problem, a few-shot task of M -way K-shot is defined as the classification
of M classes, where each class has only K data points for learning. One common approach (some-
times called meta-learning) to train a model for M -way K-shot classification is to first construct
some training tasks from the training dataset, such that each task has a support set with M × K
3
Published as a conference paper at ICLR 2022


E,
training
dataset
∕θ
output
predictions
source &
target data
∕θ
→ →
[------1 mean()
centroids
encoder fully-connected
feature vectors
(i)	Training Whole-Classification Network
(ii)	Feature Extraction
0
1
2
3
ntest- 1
ntest- 1
source data w.
source
centroids
target
centroids
(iii)	Matching Labels
the related
training set
M-way K-shot
support set
of base task j T
constructed from
Class 0
Class 1
matched labels
fully-connected^
Isource data
e"
^target data
fully-connected
(iv)	Constructing ε-approx. Network
Class (M-1)
query set
of base taskj
encoder
∕θ
feature vectors
------
K
FSOUrCe
H
Ftarget)
(v)	Fisher Information Matrix
k-nearest neighbors
k-NN
update
predicted labels
for query set
.CrOSS-entropy IoSS J
1
2
3
F
C
√

√

□
.
a
F
C
>
F
C

(vi) Episodic Fine-tuning
Figure 1: The overview of our proposed few-shot learning approach. (i) Train the whole-
classification network with the entire labels from the training dataset. (ii) Use the encoder fθ to
extract the feature vectors for each class in the source tasks and the target task, and compute their
mean (or centroid). (iii) Maximum matching algorithm is applied to map the source task’s centroids
to the target task’s centroids. (iv) Construct the ε-approximation network for the source task(s), and
using the modified dataset of the source task(s) to train the ε-approximation network. (v) Obtain
the Fisher Information matrices for the source task(s) and the target task, and computing the TAS
between them. The source tasks with the smallest TAS are considered as related tasks. (vi) From the
related-training set (consists of the data samples from the related tasks), generate few-shot M -way
K -shot base tasks. Use the support and query sets of the base tasks to episodically fine-tune fθ .
samples and a separate query set of data points with the same M labels in the support set. The goal
is to use the training data (the support set for training and the query set for evaluating the perfor-
mance of the training tasks), and the support set of the test data to train a model to achieve high
performance on the query set of the test data. Recently, there is another few-shot approach called
Whole-Classification (Chen et al., 2021; Tian et al., 2020) is proposed to use the whole training
dataset for training a base classifier with high performance. Here the assumption is that the training
set is a large dataset, which is sufficient for training a high accuracy classifier with all the labels in
the training set. On the other hand, the test set, which is used to generate the few-shot test tasks, is
not sufficiently large to train a classifier for the entire labels. Here, our proposed few-shot learning
approach, whose pseudo-code is shown in Algorithm 1, consists of three phases:
1. Training Whole-Classification Network and Feature Extraction. In phase 1 (steps (i)
and (ii) in Figure 1), we construct a Whole-Classification network to represent the entire
training set. To do so, we use the standard ResNet-12 as a common backbone used in many
few-shot approaches for constructing the Whole-Classification network. Next, we train
this network to classify the whole classes in the training set. Additionally, the knowledge
distillation technique can be applied in order to improve the classification accuracy and
generalization of the extracted features (Sun et al., 2019). Next, we define a set of few-shot
training tasks (i.e.source tasks) and a target task with the number of classes equals to the
number of labels in the test data. Using the encoder of the trained Whole-Classification
network, we extract the embedded features and compute the mean of each label.
4
Published as a conference paper at ICLR 2022
Algorithm 1: Few-Shot Learning with Task Affinity Score
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Data: (Xtrain, ytrain, ntrain), ({Xtseusptport ∪ Xtqeusetry}, ytest, ntest)
Input: The Whole-Classification network Nθ
Output: Few-shot classifier model fθ*
FunctionmTAS(Xa,Ca,Xb,Cb,Nθa):
Obtain the mapping order: mapping = MaximumMatching(Ca, Cb)
Fine-tune Nθa using Xa with mapped labels (building the ε-approximation network)
Compute Fa,a, Fa,b using Nθa with Xa, Xb, respectively
- Fa1,/b2F
return s
Function Main:
Train Nθ with the entire ntrain labels in Xtrain	. Beginning of phase 1
Construct S source tasks, where each has ntest labels from Xtrain
Construct target task using ntest labels in Xtseusptport
Extract set Csource of class centroids for each source task using the encoder fθ of Nθ
Extract set Ctarget of class centroids for the target task using the encoder fθ of Nθ
for i = 1, 2, . . . , S do
Beginning of phase 2
L Si
mTAS(Xs
ourcei ,
Csourcei ,
support
Xtest
, Ctarget , Nθ)
return closest tasks: i* = argmin Si
i
Define set L which consists of labels from closest tasks	. Beginning of phase 3
Define a subset data Xrelated = {Xtrain |ytrain ∈ L}
Construct M -way K-shot base tasks and fine-tune fθ episodically using Xrelated
2.	Task Affinity. Using the computed mean of the feature vectors from the previous phase,
phase 2 (steps (iii), (iv) and (v) in Figure 1) identifies the closest source tasks to the target
task by applying the TAS and a pre-process step called matching labels.
3.	Episodic Fine-tuning. After obtaining the closest source tasks, we construct a training
dataset called related-training set, consisting of samples only from the labels in the closest
source tasks. In phase 3 (step (vi) in Figure 1), we follow the standard meta-learning
approach by constructing the M -way K-shot base tasks from the related-training set to
episodically fine-tune the classifier model. Since the classifier model is fine-tuned only
with the labels of the related-training set, the complexity of training of the few-shot model
is reduced; hence, the overall performance of the few-shot classification increases.
4.1	Phase 1 - Training Whole-Classification Network and Feature Extraction
Recent few-shot learning approaches (e.g., Chen et al. (2021); Tian et al. (2020)) have shown advan-
tages of a pre-trained Whole-Classification network on the entire training dataset. In this paper, we
train a classifier network, Nθ using the standard architecture of ResNet-12 with the entire training
classes. The Whole-Classification network Nθ can be described as concatenation of the encoder
fθ and the last fully-connected layer (illustrated in Figure 1(i)). Since Nθ is trained on the entire
training set Xtrain, it can extract meaningful hidden features for the data samples in the training set.
Next, we construct a target task using ntest classes and its corresponding data points in the support
set of the test data (i.e., Xtseusptport). We also construct a set of S few-shot source tasks using the
training set, Xtrain, where each task has ntest labels sampled from ntrain classes of the training set.
This results in a set of source tasks with their corresponding training data, i.e., (Tsourcei , Xsourcei)
for i = 1, 2 . . . , S. Using the encoder part of Nθ denoted by fθ, we extract the feature centroids
from Xsourcei and Xtseusptport . Finally, we compute the mean (or centroid) of the extracted features
for every class in Xsourcei of the source task Tsourcei, and similarly for Xtseusptport of the target task.
That is, We compute ∣χ^ Pχ∈χ° fθ (x), where Xc denotes the set of data samples belonging to class
c in either source tasks, or the target task. In phase 2, we use the computed centroids to find the best
way to match the classes in source tasks to the target task.
5
Published as a conference paper at ICLR 2022
4.2	Phase 2 - Task Affinity
In this phase, our goal is to find the most related source task(s) in the training set to the few-shot
target task in the test set. To do this end, we apply the proposed task affinity score. However, naively
applying the TAS on the extracted centroids may result in finding a non-related source task(s) to the
target task. This is because neural networks are not invariant to the label’s permutation (Zhang et al.,
2021). For instance, consider task I of classifying cat images as 0 and dog images as 1 and consider
the ε-approximation network NθI with zero-error (i.e., ε = 0). Also, let task J be defined as the
classification of the dog images encoded as 0 and cat images as 1. Assume that the dataset used for
both tasks I and J is the same. If we use NθI to predict the data from task J, the prediction accuracy
will be zero. For human perception, these tasks are the same. However, the neural network NθI can
only be a good approximation network for task I, and not for task J. A naive approach to overcome
this issue is to try all permutations of labels. However, it is not a practical approach, due to the
intense computation requirement. As a result, we propose a method of pre-processing the labels in
the source tasks to make the task affinity score invariant to the permutation of labels.
4.2.1	Maximum B ipartite Matching
We use the Maximum Bipartite Graph Matching algorithm to match each set of ntest centroids of
source tasks to the ones in the target task based on the Euclidean distance from each centroid to
another. The goal is to find a matching order so that the total distance between all pairs of centroids
to be minimized. We apply the Hungarian maximum matching algorithm (Kuhn, 1955), as shown
in Figure 1(iii). The result of this algorithm is the way to match labels in the source tasks to the
classes in the target task. After the matching, we modify the labels of the source tasks according to
the matching result and use them to construct an ε-approximation network Nθ0 for the source task,
as marked (iv) in Figure 1. Please note that the weights are fine-tuned using the modified matched
labels; hence, we use θ0 instead of θ for the parameters of the ε-approximation network.
4.2.2	Task Affinity Score
Now, we can find the closest source tasks to the target task. To do so, for each source task, we
compute two Fisher Information matrices (Figure 1(v)) by passing the source tasks’ and target task’s
datasets through the ε-approximation network Nθ0, and compute the task affinity score defined in
the equation (4) between two tasks. Since we use the matched labels of source tasks w.r.t. the
target task, the resulted TAS is consistent and invariant to the permutation of labels. The helper
function mTAS() in the Algorithm 1 implements all the steps in the second phase. Finally, the
top-R scores and their corresponding classes are selected. These classes and their corresponding
data points are considered as the closest source task(s) to the target task. Next, we show that our
definition of the task affinity score is mathematically well-defined under some assumptions. First,
from the equation (4), it is clear that the TAS of a task from itself always equals zero. Now assume
that the objective function corresponding to the ε-approximation network is strongly convex. In the
Theorem 1, we show that the TAS from task A to task B calculated using the Fisher Information
matrix of the approximation network of task A on different epochs of the SGD algorithm converges
to a constant value given by the TAS between the above tasks when the Fisher Information matrix is
computed on the global optimum of the approximation network1.
Theorem 1. Let XA be the dataset for the task TA with the objective function L, and XB be
the dataset for the task TB. Assume XA and XB have the same distribution. Consider an ε-
approximation network Nθ for the pair task-dataset (TA , XA) with the objective functions L and
with the SGD algorithm to result weights θt at time t. Assume that the function L is strongly con-
vex, and its 3rd-order continuous derivative exists and bounded. Let the noisy gradient function in
training N network using SGD algorithm be given by: g(θt, Et) = VL(θt) + Et, where θt is the
estimation of the weights for network N at time t, and VL(θt) is the true gradient at θt. Assume
that Et satisfies E[Et|E0, ..., Et-1] = 0, and satisfies s = lim	[EtEtT |E0, . . . , Et-1]	< ∞ almost
t-→∞	∞
surely (a.s.). Then the task affinity score between TA and TB computed on the average of estimated
1As mentioned in the Theorem 1, we assume that loss function of the approximation network is strongly
convex; hence, it has a global optimum.
6
Published as a conference paper at ICLR 2022
weights up to the current time t converges to a constant as t → ∞. That is,
st = √2 忸1/2-FB/J√2 恒1/2 - FB / F,	⑸
where FAt = F (θt,Xf"y), FBt = F(θt,XBupport) With θt = t Ptθ. Moreover, FA =
F(θ*,XAuery) and FB = F(θ*,XB"pport) denote the Fisher Information matrices computed using
the optimum approximation network (i.e., a network with the global minimum weights, θ*).
To prove Theorem 1 (please see the appendix for the complete proof), we use the Polyak & Juditsky
(1992) theorem for the convergence of the SGD algorithm for strongly convex functions. Although
the loss function in a deep neural network is not strongly convex, establishing the fact that the TAS is
mathematically well-defined for this case is an important step towards the more general deep neural
networks and a justification for the success of our empirical observations. Furthermore, some recent
works try to generalize Polyak & Juditsky (1992) theorem for the convex or even some non-convex
functions in an (non)-asymptotic way (Gadat & Panloup, 2017). However, we do not pursue these
versions here.
4.3 Phase 3 - Episodic Fine-tuning
After obtaining the closest source tasks, we construct a training dataset called related-training set,
consisting of samples only from the labels in the closest source tasks. Following the standard meta-
learning approach, we consider various M -way K-shot base tasks from the related-training dataset,
we fine-tune the encoder fθ from the original Whole-Classification network in phase 1. To be more
precise, we extract the feature vectors from data samples in the support set and the query set of the
base task(s). The embedded features from the support set serve as the training data for the k-NN
classifier. We utilize the k-NN model to predict the labels for the data samples in the query set of
the base task under consideration using their feature vectors. Finally, the cross-entropy loss is used
to update the encoder fθ by comparing the predicting labels and the labels of the query set, and
backpropagating the error to the model fθ . Then this process can be repeated for several base tasks
until fθ achieves a high accuracy2. Figure 1(vi) shows the above process for the jth base task.
5	Experimental Study
In this section, we present our experimental results in various few-shot learning benchmarks, includ-
ing miniImageNet (Vinyals et al., 2016), tieredImageNet (Ren et al., 2018), CIFAR-FS (Bertinetto
et al., 2018), and FC-100 (Oreshkin et al., 2018) 3. The miniImageNet dataset consists of 100
classes, sampled from ImageNet (Russakovsky et al., 2015), and randomly split into 64, 16, and 20
classes for training, validation, and testing, respectively. Similarly, the tieredImageNet dataset is
also a derivative of ImageNet, containing a total of 608 classes from 34 categories. It is split into
20, 6, and 8 categories (or 351, 97, and 160 classes) for training, validation, and testing, respectively.
Each class, in both miniImageNet and tieredImageNet, includes 600 colorful images of size 84 × 84.
We use the architecture of ResNet-12 as the Whole-Classification network, which consists of4 resid-
ual blocks, each block has 3 convolutional layers and a max-pooling layer. There are different varia-
tions of ResNet-12 in the literature, each with different size of parameters (e.g., [64, 128, 256, 512],
[64, 160, 320, 640]). In our experiments, we report the number of parameters for all the models we
use and for those ones compare with. For training the Whole-Classification network, the SGD opti-
mizer is applied with the momentum of 0.9, and the learning rate is initialized at 0.05 with the decay
factor of 0.1 for all experiments. In miniImageNet experiment, we train the model for 100 epochs
with a batch size of 64, and the learning rate decaying at epoch 90. For tieredImageNet, we train for
120 epochs with a batch size of 64, and the learning rate decaying two times at epochs 40 and 80.
For miniImageNet, we define S = 2000 source tasks by randomly drawing samples from 20 labels
(number of labels of the target task) out of 64 training labels from the whole training set. Next,
we use the Whole-Classification network to extract the feature vectors from S source tasks, and
match them with labels of the target task. Next, we construct the ε-approximation network using the
2The final fine-tuned few-shot model is denoted by fθ* in the Algorithm 1.
3Due to page limits, we discuss the experiments on CIFAR-FS and FC-100 in the appendix.
7
Published as a conference paper at ICLR 2022
(a) Task affinity scores in miniImageNet
(b) Related training classes in miniImageNet
(c) Task affinity scores in tieredImageNet
Figure 2: (a) The distribution of TAS found in miniImageNet. (b) The frequency of 64 classes in the
top-8 closest source tasks in miniImageNet. (c) The distribution of TAS found in tieredImageNet.
(d) The frequency of 351 classes in the top-6 closest source tasks in tieredImageNet.
(d) Related training classes in tieredImageNet
encoder fθ of the Whole-Classification network, and use it to compute the TAS in the task affinity
phase. The distribution of the TAS found from the 2000 generated source tasks in the miniImageNet
is shown in Figure 2a. As we can seen, the probability distribution of the task affinity is not a
non-informative or uniform distribution. Only a few source tasks, with particular sets of labels, can
achieve the small TAS and are considered as the related tasks. Now, we plot the frequency of all 64
labels in the training set which exist in the top-8 closest source tasks in Figure 2b. This shows that
the class with the label 6 has 8 samples in the top-8 closest sources task, and this class is the most
relevant one to the labels in the target task. Moreover, there are only 49 unique classes that construct
the top-8 closest source tasks. This means that there are some classes which do not have any samples
among the top-8 closest sources tasks. These labels are the ones with the least similarity to the labels
in target task. Similarly, we define S = 1000 source tasks in tieredImageNet by randomly drawing
samples from 160 labels out of entire 351 training labels. The distribution of the TAS from these
1000 generated source tasks is shown in Figure 2c. Similar to the miniImageNet case, Figure 2d
illustrates the frequency of the 351 training classes in the top-6 closest source tasks. Among 351
training classes, there are only 293 unique classes that construct the top-6 closest source tasks.
In the episodic fine-tuning phase, we use the SGD optimizer with momentum 0.9, and the learning
rate is set to 0.001. The batch size is set to 4, where each batch of data includes 4 few-shot tasks. The
loss is defined as the average loss among few-shot tasks. After we trained our few-shot model, we
tested it on the query sets of test tasks. Table 1 and Table 2 show the performance of our approach in
5-way 1-shot and 5-way 5-shot on miniImageNet and tieredImageNet datasets, respectively. In both
cases, our approach with the standard ResNet-12 is comparable to or better than IE-distill (Rizve
et al., 2021), while outperforming RFS-distill (Tian et al., 2θ20), EPNet (Rodriguez et al., 2020) and
other methods with a significantly smaller classifier model in terms of the number of parameters.
Note that, EPNet (Rodriguez et al., 2020) and IE-distill (Rizve et al., 2021) also utilize the data
augmentation during pre-training for better feature extraction.
8
Published as a conference paper at ICLR 2022
Table 1: Comparison of the accuracy against state-of-the art methods for 5-way 1-shot and 5-way
5-shot classification with 95% confidence interval on miniImageNet dataset.
Model	Backbone	Params	1-shot	5-shot
Matching-Net (VinyalS et al., 2016)	ConvNet-4	0.11M	43.56±0.84	55.31±0.73
MAML (Finn et al., 2017)	ConvNet-4	0.11M	48.70±1.84	63.11±0.92
Prototypical-Net (Snell et al., 2017)	ConvNet-4	0.11M	49.42±o.78	68.20±0.66
Simple CNAPS (Bateni et al., 2020)	ReSNet-18	11M	53.2±o.90	70.8±0.70
Activation-ParamS (Qiao et al., 2018)	WRN-28-10	37.58M	59.60±o.41	73.74±0.19
LEO (RuSu et al., 2018)	WRN-28-10	37.58M	61.76±o.08	77.59±0.12
BaSeline++ (Chen et al., 2019)	ResNet-18	11.17M	51.87±o.77	75.68±0.63
SNAIL (MiShra et al., 2017)	ResNet-12	7.99M	55.71±o.99	68.88±0.92
AdaReSNet (Munkhdalai et al., 2018)	ResNet-12	7.99M	56.88±o.62	71.94±0.57
TADAM (OreShkin et al., 2018)	ResNet-12	7.99M	58.50±o.30	76.70±0.30
MTL (Sun et al., 2019)	ResNet-12	8.29M	61.20±i.8O	75.50±0.80
MetaOptNet (Lee et al., 2019)	ResNet-12	12.42M	62.64±o.61	78.63±0.46
SLA-AG (Lee et al., 2020)	ResNet-12	7.99M	62.93±o.63	79.63±0.47
ConStellationNet (Xu et al., 2020)	ResNet-12	7.99M	64.89±o.23	79.95±0.17
RFS-diStill (Tian et al., 2020)	ResNet-12	13.55M	64.82±o.60	82.14±0.43
EPNet (Rodriguez et al., 2020)	ResNet-12	7.99M	65.66±o.85	81.28±0.62
Meta-BaSeline (Chen et al., 2021)	ResNet-12	7.99M	63.17±0.23	79.26±0.17
IE-diStill1 (Rizve et al., 2021)	ResNet-12	9.13M	65.32±o.81	83.69±0.52
TAS-simple (ours)	ResNet-12	7.99M	64.71±o.43	82.08±0.45
TAS-distill (ours)	ResNet-12	7.99M	65.13±o.39	82.47±0.52
TAS-distill2 (ours)	ResNet-12	12.47M	65.68±o.45	83.92±0.55
1 performs with standard ResNet-12 with Dropblock as a regularizer, 2 performs with wide-layer ResNet-12
Table 2: Comparison of the accuracy against state-of-the art methods for 5-way 1-shot and 5-way
5-shot classification with 95% confidence interval on tieredImageNet dataset .
Model	Backbone Params I-Shot 5-shot
MAML (Finn et al., 2017)	ConvNet-4	0.11M	51.67±ι.8i	70.30±0.08
Prototypical-Net (Snell et al., 2017)	ConvNet-4	0.11M	53.31±o.89	72.69±0.74
Relation-Net (Sung et al., 2018)	ConvNet-4	0.11M	54.48±o.93	71.32±0.78
Simple CNAPS (Bateni et al., 2020)	ResNet-18	11M	63.00±ι.οο	80.00±0.80
LEO-trainval (Rusu et al., 2018)	ResNet-12	7.99M	66.58±ο.70	85.55±0.48
Shot-Free (Ravichandran et al., 2019)	ResNet-12	7.99M	63.52±n∕a	82.59±n/a
Fine-tuning (Dhillon et al., 2019)	ResNet-12	7.99M	68.07±o.26	83.74±0.18
MetaOptNet (Lee et al., 2019)	ResNet-12	12.42M	65.99±o.72	81.56±0.53
RFS-distill (Tian et al., 2020)	ResNet-12	13.55M	71.52±o.69	86.03±0.49
EPNet (Rodriguez et al., 2020)	ResNet-12	7.99M	72.60±o.91	85.69±0.65
Meta-Baseline (Chen et al., 2021)	ResNet-12	7.99M	68.62±o.27	83.74±0.18
IE-distill1 (Rizve et al., 2021)	ResNet-12	13.55M	72.21±o.90	87.08±0.58
TAS-simple (ours)	ResNet-12	7.99M	71.98±o.39	86.58±0.46
TAS-distill (ours)	ResNet-12	7.99M	72.81±o.48	87.21±0.52
1 performs with wide-layer ResNet-12 with Dropblock as a regularizer
6	Conclusions
A taSk affinity Score, which iS non-commutative and invariant to the permutation of labelS iS intro-
duced in thiS paper. The application of thiS affinity Score in the few-Shot learning iS inveStigated. In
particular, the taSk affinity Score iS applied to identify the related training claSSeS, which are uSed for
epiSodically fine-tuning a few-Shot model. ThiS approach helpS to improve the performance of the
few-Shot claSSifier on variouS benchmarkS. Overall, the taSk affinity Score ShowS the importance of
Selecting the relevant data for training the neural networkS with limited data and can be applied to
other applicationS in machine learning.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was supported in part by the Army Research Office grant No. W911NF-15-1-0479.
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless
Fowlkes, Stefano Soatto, and Pietro Perona. Task2Vec: Task Embedding for Meta-Learning.
arXiv e-prints, art. arXiv:1902.03545, Feb. 2019.
Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved few-
shot visual classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition, pp. 14493-14502, 2020.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.
Shixing Chen, Caojin Zhang, and Ming Dong. Coupled end-to-end transfer learning with general-
ized fisher information. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4329-4338, 2018.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classification. arXiv preprint arXiv:1904.04232, 2019.
Yinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xiaolong Wang. Meta-baseline: explor-
ing simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021.
Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for
few-shot image classification. arXiv preprint arXiv:1909.02729, 2019.
Kshitij Dwivedi and Gemma Roig. Representation similarity analysis for efficient task taxonomy &
transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 12387-12396, 2019.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. CoRR, abs/1701.08734, 2017. URL http://arxiv.org/abs/1701.
08734.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep
spatial autoencoders for visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE
International Conference on, pp. 512-519. IEEE, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR,
2017.
S. Gadat and F. Panloup. Optimal non-asymptotic bound of the ruppert-polyak averaging without
strong convexity. arXiv preprint arXiv:1709.03342, 2017.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Citeseer, 2009.
10
Published as a conference paper at ICLR 2022
Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
quarterly, 2(1-2):83-97,1955.
Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Self-supervised label augmentation via input trans-
formations. In International Conference on Machine Learning, pp. 5714-5724. PMLR, 2020.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10657-10665, 2019.
Zelun Luo, Yuliang Zou, Judy Hoffman, and Li F Fei-Fei. Label efficient learning of transferable
representations acrosss domains and tasks. In Advances in Neural Information Processing Sys-
tems, pp. 164-176, 2017.
Arun Mallya and Svetlana Lazebnik. Piggyback: Adding multiple tasks to a single, fixed network
by learning to mask. CoRR, abs/1801.06519, 2018. URL http://arxiv.org/abs/1801.
06519.
Puneet Mangla, Nupur Kumari, Abhishek Sinha, Mayank Singh, Balaji Krishnamurthy, and Vi-
neeth N Balasubramanian. Charting the right manifold: Manifold mixup for few-shot learning.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.
2218-2227, 2020.
Lilyana Mihalkova, Tuyen Huynh, and Raymond J Mooney. Mapping and revising markov logic
networks for transfer learning. In AAAI, volume 7, pp. 608-614, 2007.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. arXiv preprint arXiv:1707.03141, 2017.
Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with
conditionally shifted neurons. In International Conference on Machine Learning, pp. 3664-3673.
PMLR, 2018.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Alexandru Niculescu-Mizil and Rich Caruana. Inductive transfer for bayesian network structure
learning. In Artificial Intelligence and Statistics, pp. 339-346, 2007.
Boris N Oreshkin, Pau Rodriguez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric
for improved few-shot learning. arXiv preprint arXiv:1805.10123, 2018.
Arghya Pal and Vineeth N Balasubramanian. Zero-shot task transfer, 2019.
S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data
Engineering, 22(10):1345-1359, Oct 2010. ISSN 1041-4347. doi: 10.1109/TKDE.2009.191.
B. Polyak and A. Juditsky. Acceleration of stochastic approximation by averaging. Siam Journal on
Control and Optimization, 30:838-855, 1992.
Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, and Yonghong Tian. Transductive
episodic-wise adaptive metric for few-shot learning. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pp. 3603-3612, 2019.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting
parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7229-7238, 2018.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class
models and shot-free meta training. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 331-339, 2019.
11
Published as a conference paper at ICLR 2022
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn fea-
tures off-the-shelf: An astounding baseline for recognition. In Proceedings of the 2014 IEEE
Conference on Computer Vision and Pattern Recognition Workshops, CVPRW ,14, pp. 512-
519, Washington, DC, USA, 2014. IEEE Computer Society. ISBN 978-1-4799-4308-1. doi:
10.1109/CVPRW.2014.131. URL http://dx.doi.org.stanford.idm.oclc.org/
10.1109/CVPRW.2014.131.
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,
Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classifica-
tion. arXiv preprint arXiv:1803.00676, 2018.
Mamshad Nayeem Rizve, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Exploring com-
plementary strengths of invariant and equivariant representations for few-shot learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10836-
10846, 2021.
PaU RodriguezJssam Laradji, Alexandre Drouin, and Alexandre Lacoste. Embedding propagation:
Smoother manifold for few-shot classification. In European Conference on Computer Vision, pp.
121-138. Springer, 2020.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR,
abs/1606.04671, 2016. URL http://arxiv.org/abs/1606.04671.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960, 2018.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learn-
ing, pp. 1842-1850. PMLR, 2016.
Daniel L Silver and Kristin P Bennett. Guest editor’s introduction: special issue on inductive transfer
learning. Machine Learning, 73(3):215-220, 2008.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv
preprint arXiv:1703.05175, 2017.
Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Which tasks should be learned together in multi-task learning? In Hal Daume In and Aarti
Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 9120-9132. PMLR, 13-18 Jul 2020.
URL http://proceedings.mlr.press/v119/standley20a.html.
Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 403-412, 2019.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 1199-1208, 2018.
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking
few-shot image classification: a good embedding is all you need? In Computer Vision-ECCV
2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV16,
pp. 266-282. Springer, 2020.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In International Conference on Machine Learning, pp. 6438-6447. PMLR, 2019.
12
Published as a conference paper at ICLR 2022
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. Advances in neural information processing Systems, 29:3630-3638, 2016.
Aria Y Wang, Leila Wehbe, and Michael J Tarr. Neural taskonomy: Inferring the similarity of
task-derived representations from brain activity. BioRxiv, pp. 708016, 2019.
Weijian Xu, Huaijin Wang, Zhuowen Tu, et al. Attentional constellation nets for few-shot learning.
In International Conference on Learning Representations, 2020.
Amir R Zamir, Alexander Sax, William B Shen, Leonidas Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). IEEE, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-
115, 2021.
A	Appendix
Here, we fist present the proof of the Theorem 1, and then we provide more experimental results on
CIFAR-FS and FC-100 datasets. Additionally, we present the ablation study to show the efficacy of
our proposed task affinity score. Moreover, we also discuss about the computation complexity of
our approach, and the future works.
A.1 Proof of Theorem 1
In the proof of Theorem 1, we invoke the Polyak & Juditsky (1992) theorem on the convergence of
the average sequence of estimation in different epochs from the SGD algorithm. Since the original
form of this theorem has focused on the strongly convex functions, we have assumed that the ob-
jective function in the training of the approximation network is strongly convex. However, there are
some recent works that try to generalize Polyak & Juditsky (1992) theorem for the convex or even
some non-convex functions in an (non)-asymptotic way Gadat & Panloup (2017). Here, we apply
only on the asymptotic version of the theorem proposed originally by Polyak & Juditsky (1992). We
first recall the definition of the strongly convex function.
Definition 4 (Strongly Convex Function). A differentiable function f : Rn → R is strongly convex
iffor all x,y ∈ Rn and some μ > 0, f satisfies the following inequality:
f(y) ≥ f (x) + v(f)T(y - x) + μ∣∣y - x||2.	(6)
Theorem 1. Let XA be the dataset for the task TA with the objective function L, and XB be
the dataset for the task TB. Assume XA and XB have the same distribution. Consider an ε-
approximation network Nθ for the pair task-dataset (TA, XA) with the objective functions L and
with the SGD algorithm to result weights θt at time t. Assume that the function L is strongly con-
vex, and its 3rd-order continuous derivative exists and bounded. Let the noisy gradient function in
training N network using SGD algorithm be given by: g(θt, Et) = VL(θt) + Et, where θt is the
estimation of the weights for network N at time t, and VL(θt) is the true gradient at θt. Assume
that Et satisfies E[Et|E0, ..., Et-1] = 0, and satisfies s = lim	[EtEtT|E0, . . . , Et-1]	< ∞ almost
t-→∞	∞
surely (a.s.). Then the task affinity score between TA and TB computed on the average of estimated
weights up to the current time t converges to a constant as t → ∞. That is,
st = √2 μ1/2- FB / F -→√2 M1/2 - FB 勺 F，	⑺
where FAt = F(θt,XAlery), FBt = F(&, XBupport) with & = t £用.Moreover, FA =
F(θ*, XAuery) and FB = F(θ*, XBupport) denote the Fisher Information matrices computed using
the optimum approximation network (i.e., a network with the global minimum weights, θ*).
Proof of Theorem 1. Consider task TA as the source task and task TB as the target task. Let
θt be the set of weights at time t from the ε-approximation network N trained using dataset
13
Published as a conference paper at ICLR 2022
XAsupport with the objective functions L. Since the objective function L is strongly convex, the
set of weights θ will obtain the optimum solutions θ* after training a certain number of epochs
with stochastic gradient descend. By the assumption on the conditional mean of the noisy gra-
dient function and the assumption on S, the conditional covariance matrix is finite as well, i.e.,
C = limt-→∞ E[ttT |0, . . . , t-1] < ∞; hence, we can invoke the following result due to Polyak
et al. Polyak & Juditsky (1992):
√t(θt - θ*) →N(0, H(L(θ*))-1CHT(L(θ*))-1}	(8)
as t → ∞. Here, H is Hessian matrix, θ* is the global minimum of the loss function, and θt =
t Pt θt. In other words, for the network N,，t(& - θ*) is asymptotically normal random vector:
√t(θt- θ*) →N(0,∑),
(9)
where Σ = H(L(θ*))-1CHT(L(θ*))-1. For a given dataset X, the Fisher Information FX(θ) is
a continuous and differentiable function of θ, and it is also a positive definite matrix; thus, FX (θ)1/2
is well-defined. Now, by applying the Delta method to Equation (9) with dataset XA of task TA, we
have:
(FA/2 - FA1/2) →N(0,1 ∑a),	(10)
where FAt = FXA (θt), FA = FXA (θ*), and the covariance matrix is given by ∑a =
Jθ^vec(Fxa(θ*)"))ΣJθ ^vec(Fxa(θ*)")) . Here, vec() is the vectorization operator, θ*
is a n X 1 vector of the optimum parameters, FXA (θ*) is a n X n Matrix evaluated at the minimum
using dataset Xa, and Jθ(Vec(FXA (θ*)1/2)) is a n2 X n Jacobian matrix of the Fisher Information
Matrix. Likewise, from Equation (9) with dataset XB of task TB, we have:
(FB/2 - FB1/2) →N(0,1 ∑B),	(11)
where FBt = FXB (&), FB = FXB (θ*), and the covariance matrix is given by ∑b =
Jθ kec(FχB (θB)1/2)) Jθ 卜ec(FχB (θB)1/2)) . From Equation (10) and (11), we obtain:
(FAt2 - FB/2) →N(μ,V),	(12)
where μ = (FA1/2 - F∣1/2) and V = ɪ(∑a + ∑b). Since (FA/2 - Fg/2) -(FA1/2 - FB1/2) is
asymptotically normal with the covariance goes to zero as t approaches infinity, all of the entries go
to zero, we conclude that:
st = √2 Mt2 - FB/2|IF -→√2 IIFA1/2 - FB1/2IIF.
(13)
□
A.2 EXPERIMENTS ON CIFAR-FS AND FC- 100
Here, we conduct the experiments on the CIFAR-FS and the FC-100 datasets. The CIFAR-FS
dataset (Bertinetto et al., 2018) is derived from the original CIFAR-100 dataset (Krizhevsky et al.,
2009) by splitting 100 classes into 64 training classes, 16 validation classes, and 20 testing classes.
The FC-100 dataset (Oreshkin et al., 2018) is also a subset of the CIFAR-100 and consists of 60
training classes, 20 validation classes, and 20 testing classes. The size of data sample in both of
these datasets is 32 X 32.
Similar to previous experiments on miniImageNet and tieredImageNet, we train the ResNet-12 for
100 epochs with batch size of 64, and the learning rate decaying at epoch 60. Next, we compare our
proposed few-shot approach with other state-of-the-art methods on CIFAR-FS and FC-100 datasets.
The results in Table 3, and Table 4 indicates the competitiveness of our method in both datasets. Our
approach with standard ResNet-12 is comparable to IE-distill (Rizve et al., 2021) and outperforms
RFS-distill (Tian et al., 2020) while having a significantly smaller classifier model in term of the
number of parameters.
14
Published as a conference paper at ICLR 2022
Table 3: Comparison of the accuracy against state-of-the art methods for 5-way 1-shot and 5-way
5-shot classification with 95% confidence interval on CIFAR-FS dataset.
CIFAR-FS
Model	Backbone Params 1-shot 5-shot
MAML (Finn et al., 2017)	ConvNet-4	0.11M	58.90±1.90	71.50±1.00
Prototypical-Net (Snell et al., 2017)	ConvNet-4	0.11M	55.50±o.70	72.00±0.60
Relation-Net (Sung et al., 2018)	ConvNet-4	0.11M	55.00±1.00	69.30±0.80
Prototypical-Net (Snell et al., 2017)	ResNet-12	7.99M	72.20±o.70	83.50±0.50
Shot-Free (Ravichandran et al., 2019)	ResNet-12	7.99M	69.20±n∕a	84.70±n/a
TEWAM (Qiao et al., 2019)	ResNet-12	7.99M	70.40±n∕a	81.30±n/a
MetaOptNet (Lee et al., 2019)	ResNet-12	12.42M	72.60±o.70	84.30±0.50
RFS-simple (Tian et al., 2020)	ResNet-12	13.55M	71.50±o.80	86.00±0.50
RFS-distill (Tian et al., 2020)	ResNet-12	13.55M	73.90±o.80	86.90±0.50
IE-distill1 (Rizve et al., 2021)	ResNet-12	9.13M	75.46±o.86	88.67±0.58
TAS-simple (ours)	ResNet-12	7.99M	73.47±0.42	86.82±0.49
TAS-distill (ours)	ResNet-12	7.99M	74.02±0.55	87.65±0.58
TAS-distill2 (ours)	ResNet-12	12.47M	75.56±o.62	88.95±0.65
1 performs with standard ResNet-12 with Dropblock as a regularizer, 2 performs with wide-layer ResNet-12
Table 4: Comparison of the accuracy against state-of-the art methods for 5-way 1-shot and 5-way
5-shot classification with 95% confidence interval on FC-100 dataset.
FC-100
Model	Backbone Params 1-shot 5-shot
Prototypical-Net (Snell et al., 2017)	ConvNet-4	0.11M	35.30±o.60	48.60±0.60
Prototypical-Net (Snell et al., 2017)	ResNet-12	7.99M	37.50±o.60	52.50±0.60
TADAM (Oreshkin et al., 2018)	ResNet-12	7.99M	40.10±o.4O	56.10±0.40
MetaOptNet (Lee et al., 2019)	ResNet-12	12.42M	41.10±o.6O	55.50±0.60
RFS-simple (Tian et al., 2020)	ResNet-12	13.55M	42.60±o.70	59.10±0.60
RFS-distill (Tian et al., 2020)	ResNet-12	13.55M	44.60±o.70	60.90±0.60
IE-distill1 (Rizve et al., 2021)	ResNet-12	9.13M	44.65±o.77	61.24±0.75
TAS-simple (ours)	ResNet-12	7.99M	43.10±O.67	60.65±0.62
TAS-distill (ours)	ResNet-12	7.99M	44.62±o.70	61.46±0.65
1 performs with standard ResNet-12 with Dropblock as a regularizer
A.3 Ablation Study
First, we show the stability of the TAS by applying our distance on various classification tasks
in CIFAR-10, CIFAR-100, ImageNet datasets using ResNet-18, and VGG-16 as the backbone for
the ε-approximation networks. For each dataset, we define 4 classification tasks, all of which are
variations of the full class classification task. For each task, we consider a balanced training dataset.
That is, except for the classification tasks with all the labels, only a subset of the original training
dataset is used such that the number of training samples across all the class labels to be equal.
Additionally, we use 3 different ε values for the ε-approximation networks, ranging from 0.2 (good)
to 0.6 (bad). To make sure that our results are statistically significant, we run our experiments 10
times with each of the ε-approximation networks being initialized with a different random seed each
time and report the mean and the standard deviation of the computed distance.
In CIFAR-10, we define 4 tasks as follow:
•	Task 0 is a binary classification of indicating 3 objects: automobile, cat, ship (i.e., the goal
is to decide if the given input image consists of one of these three objects or not).
•	Task 1 is a binary classification of indicating 3 objects: cat, ship, truck.
•	Task 2 is a 4-class classification with labels bird, frog, horse, and anything else.
•	Task 3 is the standard 10 objects classification.
15
Published as a conference paper at ICLR 2022
*SBl ①OJnOS
1 2
*SBl ①号0ω
Target Task
0j
XSEl ①。」nos
Target Task
0^1
*SEl Oo.Inos
Target Task
8
0	0.02134	0.01914	0.03844
0	0.02755	0.02481	0.0402
0	0.04258	0.03862	0.05108
0.0201	0	0.02549	0.0346
0.02381	0.02708	0	0.03589
U_ ①O.Inos
0.02833	0	0.02775	0.03389
0.02928	0.02855	0	0.04152
H①o」nos
0.04101	0	0.03755	0.04288
0.04063	0.03752	0	0.04687
0.01527	0.01322	0.01689	0
0.02276	0.01863	0.02187	0
0.02853	0.03089	0.03866	0
Target Task
(a)	ε = 0.2
Target Task
(b)	ε = 0.4
Target Task
(c)	ε = 0.6

Figure 3: Distance from source tasks to the target tasks on CIFAR-10 using ResNet-18 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
XSEl ①OJnOS
3
XSEl ①OJnOS
0	0.1691
0.6109
0.149	0
0.6416
0.4623
0.4692
0.7583
0.7325
0.3646
0.4161
0.3693
0.5114
0	1	2
Target Task
0	0.02108 0.03902 0.04435
0.01791	0	0.03234 0.03042
0.03212 0.03737	0	0.04655
0.02383 0.02983 0.02864	0
XSEl ①。」nos
Target Task
0.2
0	0.02881	0.04854	0.04568
H ①O.Inos
0.1	3
0
Target Task
0
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
H①o」nos
0.02245	0	0.04007	0.03152
0.03811	0.03549	0	0.05152
0.03389	0.03174	0.03408	0
*SE1 ①o」nos
0	12	3
Target Task	Target Task	Target Task
(a)ε = 0.2	(b)ε = 0.4	(c)ε = 0.6
0
Figure 4: Distance from source tasks to the target tasks on CIFAR-10 using VGG-16 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
In Figure 3, the mean and standard deviation of the TAS between CIFAR-10 tasks over 10 trial runs,
using 3 different ε values for the ResNet-18 approximation networks. For ε = 0.2, the approxima-
tion network’s performance on the corresponding task’s test data is at least 80% and it is considered
to be a good representation. As shown in Figure 3(a), the standard deviation of TAS is relatively
16
Published as a conference paper at ICLR 2022
3
*SBl ①。」noω
0.05
Target Task
0
0	0.01161	0.0175	0.02783
0.00736	0	0.02825	0.02549
0.00768	0.01387	0	0.02144
0.01934	0.00996	0.01296	0
*SEI anos
5 5 5
3 3 2 2 1 1
■ ■■■■■
Oooooo
0.05
0
0
Target Task
0	0.01948	0.02074	0.03248
*SBl ①O.Inos
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
XSEl Oo.Inos
0.01825	0	0.03381	0.03721
0.01512	0.02145	0	0.03204
0.03075	0.01492	0.01929	0
Target Task
0
U_①o」nos
χSBl ①。」noω
Target Task	Target Task	Target Task
(a)ε = 0.2	(b)ε = 0.4	(c)ε = 0.6
Figure 5: Distance from source tasks to the target tasks on CIFAR-100 using ResNet-18 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
small and the TAS is stable (or consistent) regardless of the network’s initialization. Thus, we can
easily identify the closest tasks to target tasks. The result for ε = 0.4, shown in Figure 3(b), the
order of tasks remains the same for most cases, however, the standard deviations are much larger
than the previous case. Lastly, Figure 3(c) shows the results for ε = 0.6. In this setup, TAS between
tasks fluctuates widely and it is considered to be unreliable. Consequently, if the TAS between a
pair of tasks is computed only once, the order of the closest tasks to the target task would easily be
incorrect. Similarly, we conduct the experiment in CIFAR-10 using VGG-16 as the backbone. In
Figure 4, we observe similar trend as the approximation network with smaller ε value will obtain
the more stable and consistent task distances.
In CIFAR-100, which consisting of 100 objects equally distributed in 20 sub-classes, each sub-class
has 5 object, we define 4 tasks as follow:
•	Task 0 is a binary classification of detecting an object that belongs to vehicles 1 and 2 sub-
classes or not (i.e., the goal is to decide if the given input image consists of one of these 10
vehicles or not).
•	Task 1 is a binary classification of detecting an object that belongs to household furniture
and household devices or not.
•	Task 2 is a multi-classification with 11 labels defined on vehicles 1, vehicles 2, and anything
else.
•	Task 3 is a multi-classification with the 21-labels in vehicles 1, vehicles 2, household fur-
niture, household devices, and anything else.
In Figure 5, the mean and standard deviation of the TAS between CIFAR-100 tasks over 10 trial
runs, using 3 different ε values for the ResNet-18 approximation networks. For ε = 0.2, the ap-
proximation network’s performance on the corresponding task’s test data is at least 80% and it is
considered to be a good representation. As shown in Figure 5(a), TAS is stable regardless of the
network initialization. As ε increases from 0.2 to 0.6, we observe the significant rise in standard
deviation. For ε = 0.6, the TAS results are widely fluctuating and unreliable. In particular, consider
Task 0 as the incoming task, and Task 1, 2, 3 are the base tasks. From the approximation network
ε = 0.2 in Figure 5(a), the closest task to Task 0 is Task 2. However, the approximation network
17
Published as a conference paper at ICLR 2022
3
1 2
XSEl①号OS
0	1	2
Target TaSk
0	0.01357 0.04861 0.02401
H əo.lnos
Target Task
0	0.02056	0.05239	0.03395
U_ ①O.Inos
0	3
0
Target Task
0.6
0.5
0.4
0.3
0.2
0.1
0
0	0.04082	0.05947	0.04295
0.01459	0	0.03852 0.03866
0.0108	0.02364	0	0.01915
*SEl ①o」nos
0.01508	0	0.03086	0.03956
0.01882	0.02152	0	0.02776
U_ ①O.Inos
0.03607	0	0.03866	0.04253
0.0352	0.03087	0	0.03604
0.01309 0.01144 0.00467	0
3
0	12	3
Target Task
(a)	ε = 0.2
0.01294	0.01558	0.01493	0
0.02758	0.02969	0.03061	0
Target Task
(b)	ε = 0.4
Target Task
(c)	ε = 0.6
XSEi SnoS

3
Figure 6:	Distance from source tasks to the target tasks on CIFAR-100 using VGG-16 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
ε = 0.6 in Figure 5(c) identifies Task 1 as the closest task of Task 0. Thus, in order to achieve a
meaningful TAS between tasks, we first need to make sure that we represent the tasks correctly by
the approximation networks. If the approximation network is not good in term of performance (i.e.,
ε is large), then, that network is not a good representation for the task. Similarly, we conduct the
experiment in CIFAR-100 using VGG-16 as the backbone. In Figure 6, we observe similar trend
as the approximation network with smaller ε value will obtain the more stable and consistent task
distances.
In ImageNet, we define four 10-class classification tasks in ImageNet dataset. For each class, we
consider 800 for training and 200 for the test samples. The list of 10 classes for each task is described
below:
•	Task 0 includes tench, English springer, cassette player, chain saw, church, French horn,
garbage truck, gas pump, golf ball, parachute.
•	Task 1 is similar to Task 0; however, instead of 3 labels of tench, golf ball, and parachute,
it has samples from the grey whale, volleyball, umbrella classes.
•	In Task 2, we also replace 5 labels of grey whale, cassette player, chain saw, volleyball,
umbrella in Task 0 with another 5 labels given by platypus, laptop, lawnmower, baseball,
cowboy hat.
•	Task 3 includes analog clock, candle, sweatshirt, birdhouse, ping-pong ball, hotdog, pizza,
school bus, iPod, beaver.
In Figure 7 and Figure 8, we demonstrate the mean and standard deviation of the TAS between 4
ImageNet tasks over 10 runs using ResNet-18 and VGG-16 as the backbone. For each backbone, we
conduct 3 different ε values and report results as the above experiments. Similar to the CIFAR-10
and CIFAR-100, we observe that the approximation network with better performance (or smaller ε
value) will results more stable TAS. Additionally, regardless of the architecture backbone, the trend
of distance between these tasks is consistent across all experiments.
Next, we want to analyze the effectiveness of our approach of choosing the most related data classes
by performing several few-shot learning experiments with different number of related classes. Ad-
18
Published as a conference paper at ICLR 2022
3
XSEl ①OJnOS
0
1	2
Target Task
H ①O.Inos
5 5 5 5
Il
Ooooooo
0.1
0.05
0
Target Task
U_ ①O.Inos
Target Task
0
0.5
0.4
0.3
0.2
0.1
0	0.03248	0.03189	0.03012
0	0.03941	0.03953	0.03077
0	0.04285	0.05219	0.03295
0.01439	0	0.02214	0.02309
0.02538	0.03124	0	0.02043
H ①O.Inos
0.02328	0	0.02579	0.0286
0.03448	0.03736	0	0.03262
H əo.lnos
0.03592	0	0.03618	0.03808
0.05214	0.05811	0	0.05938
0.02998	0.0202	0.02375	0
0.03371	0.02701	0.02933	0
0.04591	0.03807	0.04922	0
0	12	3
Target Task
(a)	ε = 0.2
Target Task
(b)	ε = 0.4
Target Task
(c)	ε = 0.6
0
XSEl ①OJnOS
3
3
0
Figure 7:	Distance from source tasks to the target tasks on ImageNet using ResNet-18 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
U_①号OS
1 2
XSEl ①OJnOS
1	2	3
Target Task
0	0.01922	0.02552	0.03144
0.02413	0	0.02894	0.02322
0.02211	0.02334	0	0.01563
0.02307	0.02125	0.01875	0
0	12	3
Target Task
(a)	ε = 0.2
H əo.lnos
5 5 5 5
3 3 2 2 1 1 0
Target Task
454353252151
■ ■■■■■■
Oooooooo
0	0.02231	0.02911	0.03452
U_ ①O.Inos
0.02215	0	0.02792	0.02985
0.02933	0.03133	0	0.02408
0.03358	0.02947	0.02595	0
1	2
Target Task
(b)	ε = 0.4
U_ ①O.Inos
H əo.lnos
Target Task
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Target Task
(c) ε = 0.6
Figure 8:	Distance from source tasks to the target tasks on ImageNet using VGG-16 backbone.
The top row shows the mean values and the bottom row denotes the standard deviation of distances
between classification tasks over 10 different trials.
ditionally, we conduct the few-shot experiment with non-related classes and randomized classes to
show the efficacy of our proposed method. Table 5 shows the performances of our proposed few-
shot method with different settings. We achieve the best performance with 49 related tasks in both
5-way 1-shot and 5-way 5-shot classification. We observe a drop in performance when the number
19
Published as a conference paper at ICLR 2022
Table 5: The performance of TAS with different settings for 5-way 1-shot and 5-way 5-shot classi-
fication with 95% confidence interval on miniImageNet dataset.
miniImageNet
Model	Params 1-shot 5-shot
TAS-SimPle with top-54 related classes	7.99M	63.54±0.49	80.92±o.55
TAS-simple with top-49 related classes	7.99M	64.71±0.43	82.08±o.45
TAS-simple with top-38 related classes	7.99M	63.92±o.47	81.29±o.50
TAS-SimPle with top-32 related classes	7.99M	63.35±0.55	80.67±o.53
TAS-SimPle with top-29 non-related classes	7.99M	61.82±o.44	78.14±o.48
TAS-simple with 32 random classes	7.99M	62.57±o.45	78.96±o.44
Table 6: The performance TAS with different settings for 5-way 1-shot and 5-way 5-shot classifica-
tion with 95% confidence interval on tieredImageNet dataset.
miniImageNet
Model	Params 1-shot 5-shot
TAS-simple with top-318 related classes	7.99M	71.02±0.42	85.26±o.48
TAS-simple with top-293 related classes	7.99M	71.98±0.39	86.58±o.46
TAS-simple with top-249 related classes	7.99M	70.89±0.45	85.65±0.52
TAS-simple with top-211 related classes	7.99M	70.11±0.45	84.86±0.50
TAS-simple with top-223 non-related classes	7.99M	67.33±0.50	-83.24±o.52-
TAS-simple with 211 random classes	7.99M	68.04±0.46	83.55±0.52
of related classes increases to 54 classes. As we reduce the number of related classes from 49 to 32
classes, the overall performance of our model reduces significantly in both 1-shot and 5-shot. As
the result, selecting the optimal number of relevant classes is crucial to the overall performance of
our few-shot approach. Additionally, we observe a drop in performance when the model utilizing 29
non-related classes as well as 32 random classes. Therefore, we show that using a relevant classes
can boost the performance of the few-shot model. Moreover, we conduct the similar analysis for the
tieredImageNet dataset, in which the results are shown in Table 6. Here, we observe the same trend
as in previous experiments. Briefly, by selecting the relevant classes, we consistently achieve higher
performances when compare with selecting non-relevant or randomized classes.
A.4 Computation Complexity
Below are the complexity of 3 phases in our approach:
•	Phase 1 consists of 2 operations: (i) Train Whole-Classification net. (ii) Feature extraction.
These 2 operations is executed only once, and the complexity is constant.
•	Phase 2 consists of 3 operations: (iii) Matching labels (iv) Constructing approx. net. (v)
Fisher Information matrix. The operation (iii) is based on the Hungarian algorithm and
the complexity is O(n3), where n is the number of tasks. For each base task, (iv) is done
once, and the complexity of finding FIM in (v) is linear with respect to the number of
parameters due to the fact that we only consider the diagonal entries of the matrix. Note
that all the tasks are few-shot and only consists of few data samples. These operations can
be effectively implemented in GPU.
•	Phase 3 is (vi) Episodic fine-tuning, which is often used in few-shot approach [R3, R4].
Here, we use the k-NN as the clustering algorithm. The complexity of k-NN is linear with
respect to the number of samples.
A.5 Future Works
In this paper, we propose a novel task similarity measure that inherently asymmetric and invariant
to label permutation, called Task Affinity Score (TAS). Next, we introduce the TAS measure to the
20
Published as a conference paper at ICLR 2022
few-shot learning approach and achieve significant improvements over the current state-of-the-art
methods. In future work, we would like to extend the applications of TAS to other few-shot learning
frameworks, and also to other meta-learning areas, e.g., multi-task learning, continual learning.
Additionally, we wish to establish theoretical justification for the TAS measure in convex or non-
convex cases.
21