Published as a conference paper at ICLR 2022
Distributionally Robust Fair
Principal Components via Geodesic Descents
Hieu Vu, Toan Tran Man-Chung Yue	Viet Anh Nguyen
VinAI Research, Vietnam Hong Kong Polytechnic University VinAI Research, Vietnam
arXiv:2202.03071v1 [cs.LG] 7 Feb 2022
Ab stract
Principal component analysis is a simple yet useful dimensionality reduction tech-
nique in modern machine learning pipelines. In consequential domains such as
college admission, healthcare and credit approval, it is imperative to take into
account emerging criteria such as the fairness and the robustness of the learned
projection. In this paper, we propose a distributionally robust optimization prob-
lem for principal component analysis which internalizes a fairness criterion in the
objective function. The learned projection thus balances the trade-off between
the total reconstruction error and the reconstruction error gap between subgroups,
taken in the min-max sense over all distributions in a moment-based ambiguity
set. The resulting optimization problem over the Stiefel manifold can be effi-
ciently solved by a Riemannian subgradient descent algorithm with a sub-linear
convergence rate. Our experimental results on real-world datasets show the merits
of our proposed method over state-of-the-art baselines.
1 Introduction
Machine learning models are ubiquitous in our daily lives and supporting the decision-making pro-
cess in diverse domains. With their flourishing applications, there also surface numerous concerns
regarding the fairness of the models’ outputs (Mehrabi et al., 2021). Indeed, these models are prone
to biases due to various reasons (Barocas et al., 2018). First, the collected training data is likely
to include some demographic disparities due to the bias in the data acquisition process (e.g., con-
ducting surveys on a specific region instead of uniformly distributed places), or the imbalance of
observed events at a specific period of time. Second, because machine learning methods only care
about data statistics and are objective driven, groups that are under-represented in the data can be
neglected in exchange for a better objective value. Finally, even human feedback to the predictive
models can also be biased, e.g., click counts are human feedback to recommendation systems but
they are highly correlated with the menu list suggested previously by a potentially biased system.
Real-world examples of machine learning models that amplify biases and hence potentially cause
rates
To tackle the issue, various fairness criteria for supervised learning have been proposed in the lit-
erature, which encourage the (conditional) independence of the model’s predictions on a particular
sensitive attribute (Dwork et al., 2012; Hardt et al., 2016b; Kusner et al., 2017; Chouldechova, 2017;
Verma & Rubin, 2018; Berk et al., 2021). Strategies to mitigate algorithmic bias are also investi-
gated for all stages of the machine learning pipelines (Berk et al., 2021). For the pre-processing
steps, (Kamiran & Calders, 2012) proposed reweighting or resampling techniques to achieve sta-
tistical parity between subgroups; in the training steps, fairness can be encouraged by adding con-
straints (Donini et al., 2018) or regularizing the original objective function (Kamishima et al., 2012;
Zemel et al., 2013); and in the post-processing steps, adjusting classification threshold by examining
black-box models over a holdout dataset can be used (Hardt et al., 2016b; Wei et al., 2019).
Since biases may already exist in the raw data, itis reasonable to demand machine learning pipelines
to combat biases as early as possible. We focus in this paper on the Principal Component Analy-
sis (PCA), which is a fundamental dimensionality reduction technique in the early stage of the
1	https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
2	https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212
1
unfairness are commonplace, ranging from recidivism prediction giving higher false positive
f A l' "	1	∙	1	1 l' ∙	1	∙ < ∙	1	1	∙	1	1 f	/
1^cι* A τt'i八 mancfjt> j∙ 1^c to∕^-∣oi τ*accrvτwt^ιcτ> cλrcr^Tnc hc∖xτ>o ICI*oa aι∙twt* mta 1^cι*，XTCTT⅜aτ>4
for African-American to facial recognition systems having large error rate for women .
Published as a conference paper at ICLR 2022
pipelines (Pearson, 1901; Hotelling, 1933). PCA finds a linear transformation that embeds the origi-
nal data into a lower-dimensional subspace that maximizes the variance of the projected data. Thus,
PCA may amplify biases if the data variability is different between the majority and the minority
subgroups, see an example in Figure 1. A naive approach to promote fairness is to train one inde-
pendent transformation for each subgroup. However, this requires knowing the sensitive attribute of
each sample, which would raise disparity concerns. On the contrary, using a single transformation
for all subgroups is “group-blinded” and faces no discrimination problem (Lipton et al., 2018).
Learning a fair PCA has attracted attention from many fields from machine learning, statistics to
signal processing. Samadi et al. (2018) and Zalcberg & Wiesel (2021) propose to find the principal
components that minimize the maximum subgroup reconstruction error; the min-max formulations
can be relaxed and solved as semidefinite programs. Olfat & Aswani (2019) propose to learn a
transformation that minimizes the possibility of predicting the sensitive attribute from the projected
data. Apart from being a dimensionality reduction technique, PCA can also be thought of as a
representation learning toolkit. Viewed in this way, we can also consider a more general family of
fair representation learning methods that can be applied before any further analysis steps. There
are a number of works develop towards this idea (Kamiran & Calders, 2012; Zemel et al., 2013;
Calmon et al., 2017; Feldman et al., 2015; Beutel et al., 2017; Madras et al., 2018; Zhang et al.,
2018; Tantipongpipat et al., 2019), which apply a multitude of fairness criteria.
In addition, we also focus on the robustness criteria for the linear transformation. Recently, it has
been observed that machine learning models are susceptible to small perturbations of the data (Good-
fellow et al., 2014; Madry et al., 2017; Carlini & Wagner, 2017). These observations have fuelled
many defenses using adversarial training (Akhtar & Mian, 2018; Chakraborty et al., 2018) and dis-
tributionally robust optimization (Rahimian & Mehrotra, 2019; Kuhn et al., 2019; Blanchet et al.,
2021).
Contributions. This paper blends the ideas from the field of fairness in artifical intelligence and
distributionally robust optimization. Our contributions can be described as follows.
•	We propose the fair principal components which balance between the total reconstruction error
and the absolute gap of reconstruction error between subgroups. Moreover, we also add a layer
of robustness to the principal components by considering a min-max formulation that hedges
against all perturbations of the empirical distribution in a moment-based ambiguity set.
•	We provide the reformulation of the distributionally robust fair PCA problem as a finite-
dimensional optimization problem over the Stiefel manifold. We provide a Riemannian gradient
descent algorithm and show that it has a sub-linear convergence rate.
Figure 1 illustrates the qualitative comparison
between (fair) PCA methods and our proposed
method on a 2-dimensional toy example. The
majority group (blue dots) spreads on the hor-
izontal axis, while the minority group (yellow
triangles) spreads on the slanted vertical axis.
The nominal PCA (red) captures the majority
direction to minimize the total error, while the
fair PCA of Samadi et al. (2018) returns the
diagonal direction to minimize the maximum
subgroup error. Our fair PCA can probe the
full spectrum in between these two extremes by
sweeping through our penalization parameters
appropriately. If we do not penalize the error
gap between subgroups, we recover the PCA
method; if we penalize heavily, we recover the
fair PCA of Samadi et al. (2018). Extensive nu-
merical results on real datasets are provided in
Section 5. Proofs are relegated to the appendix.
Figure 1: Nominal PCA (red arrow), fair PCA by
Samadi et al. (2018) (green arrow), and our spec-
trum of fair PCA (shorter arrows). Arrows show
directions and are not normalized to unit length.
2
Published as a conference paper at ICLR 2022
2 Fair Principal Component Analysis
2.1 Principal Component Analysis
We first briefly revisit the classical PCA. Suppose that we are given a collection of N i.i.d. samples
{Xi}N=ι generated by some underlying distribution P. For simplicity, We assume that both the
empirical and population mean are zero vectors. The goal of PCA is to find a k-dimensional linear
subspace of Rd that explains as much variance contained in the data {Xi}N=ι as possible, where k <
d is a given integer. More precisely, We parametrize k-dimensional linear subspaces by orthonormal
matrices, i.e., matrices whose columns are orthogonal and have unit Euclidean norm. Given any
such matrix V , the associated k-dimensional subspace is the one spanned by the columns of V .
The projection matrix onto the subspace is V V >, and hence the variance of the projected data is
given by tr (VV>ΞΞ>), where Ξ = [Xi, ∙∙∙ , Xn] ∈ Rd×N is the data matrix. By a slight abuse of
terminology, sometimes we refer to V as the projection matrix. The problem of PCA then reads
max tr(VV>ΞΞ>) .	(1)
V ∈Rd×k,V>V =Ik
For any vector X ∈ Rd and orthonormal matrix V, denote by '(V, X) the reconstruction error, i.e.,
'(V,X) = ∣∣X - VV>Xk2 = X>(Id - VV>)X.
The problem of PCA can alternatively be formulated as a stochastic optimization problem
min
V∈Rd×k,V>V =Ik
EP ['(V, X)],
(2)
1	TTΛ ∙	.)	∙	∙	1	1∙	.	∙1	∙	.	1	∙ .1	.1	1	f 八、1∖T	1 PT-	TTΛ τ. ∙	11
where P is the empirical distribution associated with the samples {χi}i=ι and X 〜 P. It is well-
known that PCA admits an analytical solution. In particular, the optimal solution to problem (2) (and
also problem (1)) is given by any orthonormal matrix whose columns are the eigenvectors associated
with the k largest eigenvalues of the sample covariance matrix ΞΞ> .
2.2 Fair Principal Component Analysis
In the fair PCA setting, we are also given a discrete sensitive attribute A ∈ A, where A may represent
features such as race, gender or education. We consider binary attribute A and let A = {0, 1}. A
straightforward idea to define fairness is to require the (strict) balance ofa certain objective between
the two groups. For example, this is the strategy in Hardt et al. (2016a) for developing fair supervised
learning algorithms. A natural objective to balance in the PCA context is the reconstruction error.
Definition 2.1 (Fair projection). Let Q be an arbitrary distribution of (X, A). A projection matrix
V ∈ Rd×k is fair relative to Q if the conditional expected reconstruction error is equal between
subgroups, i.e., Eq['(V, X)|A = a] = Eq['(V, X)|A = a0] for any (a, a0) ∈ A× A.
Unfortunately, Definition 2.1 is too stringent: for a general probability distribution Q, it is possible
that there exists no fair projection matrix V .
Proposition 2.2 (Impossibility result). For any distribution Q on X ×A, there exists a fair projection
matrix V ∈ Rd×k relative to Q if and only ifrank(EQ[XX> |A = 0] - EQ[XX> |A = 1]) ≤ k.
One way to circumvent the impossibility result is to relax the requirement of strict balance to ap-
proximate balance. In other words, an inequality constraint of the following form is imposed:
∣Eq['(V,X)|A = a] — Eq['(V,X)|A = a0]∣ ≤ E	∀(a,a0) ∈A×A,
where > 0 is some prescribed fairness threshold. This approach has been adopted in other fair
machine learning settings, see Donini et al. (2018) and Agarwal et al. (2019) for example.
In this paper, instead of imposing the fairness requirement as a constraint, we penalize the unfairness
in the objective function. Specifically, for any projection matrix V, we define the unfairness as the
absolute difference between the conditional loss between two subgroups:
U(V,Q) , ∣Eq['(V,X)|A = 0] - Eq['(V,X)|A = 1]|.
We thus consider the following fairness-aware PCA problem
__________ . _____ʌ ,
min	EP['(V, X)] + λU(V,P),	(3)
V∈Rd×k, V>V =Ik P
where λ ≥ 0 is a penalty parameter to encourage fairness. Note that for fair PCA, the dataset is
{(Xi, ai)}N=ι and hence the empirical distribution P is given by P = 芸 PN=I 方⑶鱼).
3
Published as a conference paper at ICLR 2022
3 Distributionally Robust Fair PCA
The weakness of empirical distribution-based stochastic optimization has been well-documented,
see (Smith & Winkler, 2006; Homem-de Mello & Bayraksan, 2014). In particular, due to overfit-
ting, the out-of-sample performance of the decision, prediction, or estimation obtained from such a
stochastic optimization model is unsatisfactory, especially in the low sample size regime. Ideally,
we could improve the performance by using the underlying distribution P instead of the empirical
distribution P. But the underlying distribution P is unavailable in most practical situations, if not all.
Distributional robustification is an emerging approach to handle this issue and has been shown to
deliver promising out-of-sample performance in many applications (Delage & Ye, 2010; Namkoong
& Duchi, 2017; Kuhn et al., 2019; Rahimian & Mehrotra, 2019). Motivated by the success of distri-
butional robustification, especially in machine learning Nguyen et al. (2019); Taskesen et al. (2021),
we propose a robustified version of model (3), called the distributionally robust fairness-aware PCA:
min sup Eq['(V,X)] + λU(V, Q),	(4)
V∈Rd×k,V>V=Ik Q∈B(P)
where B(P) is a set of probability distributions similar to the empirical distribution P in a certain
sense, called the ambiguity set. The empirical distribution P is also called the nominal distribu-
tion. Many different ambiguity sets have been developed and studied in the optimization literature,
see Rahimian & Mehrotra (2019) for an extensive overview.
3.1	The Wasserstein-type Ambiguity Set
To present our ambiguity set and main results, we need to introduce some definitions and notations.
Definition 3.1 (Wasserstein-type divergence). The divergence W between two probability distribu-
tions Qi 〜(μι, ∑ι) ∈ Rd X Sj and Q2 〜(μ?, ∑2) ∈ Rd X Sj is defined as
W(QI k q2) , kμi - μ2 k2 + tr (ς1 + ς2 - 2(ςS ςiςS ) 2 ).
The divergence W coincides with the squared type-2 Wasserstein distance between two Gaus-
sian distributions N(μι, ∑ι) and N(μ2, ∑2) (Givens & Shortt, 1984). One can readily show that
W is non-negative, and it vanishes if and only if (μι, ∑ι) = (μ2, ∑2), which implies that Qi
and Q2 have the same first- and second-moments. Recently, distributional robustification with
Wasserstein-type ambiguity sets has been applied widely to various problems including domain
adaption (Taskesen et al., 2021), risk measurement (Nguyen et al., 2021b) and statistical estima-
tion (Nguyen et al., 2021a). The Wasserstein-type divergence in Definition 3.1 is also related to the
theory of optimal transport with its applications in robust decision making (Mohajerin Esfahani &
Kuhn, 2018; Blanchet & Murthy, 2019; Yue et al., 2021) and potential applications in fair machine
learning (Taskesen et al., 2020; Si et al., 2021; Wang et al., 2021).
Recall that the nominal distribution is P=焉 PN=I 方⑶鱼).For any a ∈ A, its conditional distri-
bution given A = a is given by
Pa = iʃ I X :	δxi,	Where	Ia	, {i ∈	{1,...,N} :	ai	=	a}.
|Ia | i∈Ia
We also use (μ0, ∑a) to denote the empirical mean vector and covariance matrix of X given A = a:
μa = E^α [X ] = EP [X |A = a] and Σ a + ^a^> = E^a XX >] = EP [XX >|A = a].
For any a ∈ A, the empirical marginal distribution of A is denoted by Pa = ∣Ia∣∕N.
Finally, for any set S, we use P(S) to denote the set of all probability distributions supported on S.
For any integer k, the k-by-k identity matrix is denoted Ik. We then define our ambiguity set as
(	∃Qa ∈ P(X) such that:	)
B(P)，Q Q ∈ P(X χ	A)	:	Q(X	x	{a})	=	PaQa(X)	∀X	⊆	Rd,	a ∈ A > ,	(5)
〔	W(Qa, Pa) ≤ Ea ∀a ∈ A	J
4
Published as a conference paper at ICLR 2022
where Qa is the conditional distribution of X |A = a. Intuitively, each Q ∈ B(P) is a joint distribu-
tion of the random vector (X, A), formed by taking a mixture of conditional distributions Qa with
mixture weight a.Each conditional distribution Qa is constrained in an ε0-neighborhood of the
nominal conditional distribution Pa with respect to the W divergence. Because the loss function '
is a quadratic function of X , the (conditional) expected losses only involve the first two moments of
X , and thus prescribing the ambiguity set using W would suffice for the purpose of robustification.
3.2	Reformulation
We now present the reformulation of problem (4) under the ambiguity set B(P).
Theorem 3.2 (Reformulation). Suppose that for any a ∈ A, either of the following two conditions
holds:
(i)	Marginal probability bounds: 0 ≤ λ ≤ pa,
(ii)	Eigenvalue bounds: the empirical second moment matrix Ma =N1a Pi∈ia xix> satisfies
pd-k σj(Ma) ≥ εa, where σj(MMa) is the j-th smallest eigenvalues of MMa.
Then problem (4) is equivalent to
min	max{J0(V ), J1(V )},	(6a)
V∈Rd×k,V>V=Ik
where for each (a, a0) ∈ {(0, 1), (1, 0)}, the function Ja is defined as
Ja(V) = Ka + θa /〈% - VV>,M“〉+ 九0 ,〈% - VV>,Mo,> + Ud - VV>,。“〉,
and the parameters K ∈ R, θ ∈ R, H ∈ R and C ∈ S； are defined as
Ka	=	(Pa	+ λ)εa +	(E-	λ)ε°o,	θ° = 2|α + λ∣√εa,%0	= 2∣pa0	- λ∣√εa0,
ʌ ʌ
Ca =(Pa + λ)Ma + 优，-λ)Ma .
(6b)
(6c)
We now briefly explain the steps that lead to the results in Theorem 3.2. Letting
Jo(V) = SUp (p^0 + λ)EQ['(V,X)|A = 0] + (Pι- λ)EQ['(V,X)|A = 1],
Q∈B(P)
J1(V )= SUp (Po - λ)EQ['(V,X )|A = 0] + (P1 + X)Eq['(V,X )|A = 1],
Q∈B(P)
then by expanding the term U(V, Q) using its definition, problem (4) becomes
min	max{J0(V), J1(V)}.
V∈Rd×k,V>V=Ik
Leveraging the definition the ambiguity set B(P), for any pair (a, a0) ∈ {(0, 1), (1, 0)}, we can
decompose Ja into two separate supremum problems as follows
Ja(V )=	SUp	(Pa + λ)EQa ['(V, X)]+	SUp	(PaO- REq.，['(V, X)].
Qa：W(Qa,Pa)S£a	Q/ ：W(Qa0 ,Pα0 )≤εα0
The next proposition asserts that each individual supremum in the above expression admits an ana-
lytical expression.
Proposition 3.3 (Reformulation). Fix a ∈ A. For any υ ∈ R, εa ∈ R+, it holds that
SUp	υEQa['(V, X)]
Qa：W(Qa,Fa)≤εα
ifυ ≥ 0,
if υ < 0 and Ud - VV>,M°〉≥ %
if υ < 0 and (Id - VV>, Ma)< s。.
5
Published as a conference paper at ICLR 2022
The proof of Theorem 3.2 now follows by applying Proposition 3.3 to each term in Ja, and balance
the parameters to obtain (6c). A detailed proof is relegated to the appendix. In the next section, we
study an efficient algorithm to solve problem (6a).
Remark 3.4 (Recovery of the nominal PCA). If λ = 0 and εa = 0 ∀a ∈ A, our formulation (4)
becomes the standard PCA problem (2). In this case, our robust fair principal components reduce
to the standard principal components. On the contrary, existing fair PCA methods such as Samadi
et al. (2018) and Olfat & Aswani (2019) cannot recover the standard principal components.
4 Riemannian Gradient Descent Algorithm
The distributionally robust fairness-aware PCA problem (4) is originally an infinite-dimensional
min-max problem. Indeed, the inner maximization problem in (4) optimizes over the space of prob-
ability measures. Thanks to Theorem 3.2, it is reduced to the simpler finite-dimensional minimax
problem (6a), where the inner problem is only a maximization over two points. Problem (6a) is,
however, still challenging as it is a non-convex optimization problem over a non-convex feasible re-
gion defined by the orthogonality constraint V >V = Id. The purpose of this section is to devise an
efficient algorithm for solving problem (6a) to local optimality based on Riemannian optimization.
4.1	Reparametrization
As mentioned above, the non-convexity of problem (6a) comes from both the objective function and
the feasible region. It turns out that we can get rid of the non-convexity of the objective function
via a simple change of variables. To see that, we let U ∈ Rd×(d-k) be an orthonormal matrix
complement to V , that is, U and V satisfy UU> + V V > = Id. Thus, we can express the objective
function J via
J(V) =F(U) ,max{F0(U),F1(U)},
where for (a, a0) ∈ {(0, 1), (1, 0)}, the function Fa is defined as
Fa(U)，Ka + θa∕<UU >,MO)+ %，J(UU >,Mo0)+〈UU >, Ca).
Moreover, letting M , {U ∈ Rd×(d-k) : U>U = Id-k}, we can re-express problem (6a) as
min F(U).	(7)
U∈M
The set M of problem (7) is a Riemannian manifold, called the Stiefel manifold (Absil et al., 2007,
Section 3.3.2). It is then natural to solve (7) using a Riemannian optimization algorithms (Absil
et al., 2007). In fact, problem (6a) itself (before the change of variables) can also be cast as a
Riemannian optimization problem over another Stiefel manifold. The change of variables above
might seem unnecessary. Nonetheless, the upshot of problem (7) is that the objective function F is
convex (in the traditional sense). This faciliates the application of the theoretical and algorithmic
framework developed in Li et al. (2021) for (weakly) convex optimization over the Stiefel manifolds.
4.2	The Riemannian Subgradient
Note that the objective function F is non-smooth since it is defined as the maximum of two func-
tions F0 and F1 . To apply the framework in Li et al. (2021), we need to compute the Riemannian
subgradient of the objective function F. Since the Stiefel manifold M is an embedded manifold in
Euclidean space, the Riemannian subgradient of F at any point U ∈ M is given by the orthogonal
projection of the usual Euclidean subgradient onto the tangent space of the manifold M at the point
U, see Absil et al. (2007, Section 3.6.1) for example.
Lemma 4.1. For any point U ∈ M, let3 aU ∈ arg maxa∈{0,1} Fa (U) and a0U = 1 - aU. Then, a
Riemannian subgradient of the objective function F at the point U is given by
gradF(U) = (Id - UU>) I	θaU	M“°U + /	久U	MaUU + 2。“°U
〈，〈UU >,MOU〉	J〈UU >,MaU〉
3 It is possible that the maximizer is not unique. In that case, choosing aU to be either 0 or 1 would work.
6
Published as a conference paper at ICLR 2022
4.3	Retractions
Another important instrument required by the framework in Li et al. (2021) is a retraction of the
Stiefel manifold M. At each iteration, the point U - γ∆ obtained by moving from the current
iterate U in the opposite direction of the Riemannian gradient ∆ may not lie on the manifold in
general, where γ > 0 is the stepsize. In Riemannian optimization, this is circumvented by the
concept of retraction. Given a point U ∈ M on the manifold, the Riemannian gradient ∆ ∈
TUM (which must lie in the tangent space TUM) and a stepsize γ, the retraction map Rtr defines a
point RtrU (-γ ∆) which is guaranteed to lie on the manifold M. Roughly speaking, the retraction
RtrU (∙) approximates the geodesic curve through U along the input tangential direction. For a
formal definition of retractions, we refer the readers to (Absil et al., 2007, Section 4.1). In this
paper, we focus on the following two commonly used retractions for Stiefel manifolds. The first one
is the QR decomposition-based retraction using the Q-factor qf( ∙) in the QR decomposition:
RtrqUf(∆) = qf(U + ∆), U ∈ M,∆ ∈ TUM.
The second one is the polar decomposition-based retraction
R唠lar(∆) = (U + ∆)(Id-k +△>△) — 2, U ∈M, ∆ ∈ TUM.	(8)
4.4 Algorithm and Convergence Guarantees
Associated with any choice of retraction Rtr is a concrete instantiation of the Riemannian subgradi-
ent descent algorithm for our problem (7), which is presented in Algorithm 1 with specific choice of
the stepsizes γt motivated by the theoretical results of (Li et al., 2021).
Algorithm 1 Riemannian Subgradient Descent for (7)
1:	Input: An initial point U0, a number of iterations τ and a retraction Rtr : (U, △) 7→ RtrU (△).
2:	for t = 0, 1, . . . , τ - 1, do
3:	Find at , arg maxa∈{0,1}{Fa (Ut)}.
4:	Compute the Riemannian subgradient △t = gradF (Ut) using the formula
△t = (I — UtUj) I /	"at	MatUt + /	%	MatUt + 2CaM ].
\，〈UtU>,Mat〉	J<UtU>,Mat〉	)
5:	Set Ut+ι = RtrUt (-γt∆t), where the step-size Yt ≡ √⅛ is constant.
6:	end for
7:	Output: Uτ .
We now study the convergence guarantee of Algorithm 1. The following lemma shows that the
objective function F is Lipschitz continuous (with respect to the Riemannian metric on the Stiefel
manifold M) with an explicit Lipschitz constant L.
Lemma 4.2 (Lipschitz continuity). The function F is L-Lipschitz continuous on M, where L > 0
is given by
L , max
θ0
σmax(M0)
yσmi∏(MM0)
, θ1
σmax(Ml)
Jbmin(Ml)
,“0
max(M0)	σ
min(M0)
max(Ml)
min(M1)
(9)
σ
q
2√d-kσmax(C0), 2Vd-kσmax(Cl)
.
We now proceed to show that Algorithm 1 enjoys a sub-linear convergence rate. To state the result,
we define the Moreau envelope
Fμ(U), UmiM {f (u 0) + 2μ kU0 - UkF
7
Published as a conference paper at ICLR 2022
where ∣∣ ∙ ∣∣f denotes the FrobeniUs norm of a matrix. Also, to measure the progress of the algorithm,
we need to introduce the proximal mapping on the Stiefel manifold (Li et al., 2021):
ProxμF (U) ∈ argmin [F(U O) + ɪ IIU 0 - Uk F ].
u0∈M 〔	2μ	J
From Li et al. (2021, Equation (22)), we have that
kgradF(U)||尸 ≤ ""F(U)- Ub，gap“(U).
μ
Therefore, the number gap*(U) is a good candidate to quantify the progress of optimization algo-
rithms for solving problem (7).
Theorem 4.3 (Convergence guarantee). Let {Ut}t=1,...,τ be the sequence of iterates generated by
Algorithm 1. Suppose that μ = 1/4L, where L is the Lipschitz constant of F in (9). Then, We have
min
t=0,...,τ
gap“(Ut) ≤
2,Fμ(Uo) - minu F"(U) + 2L3(L+IJ
(τ + I)1/4
5 Numerical Experiments
We compare our proposed method, denoted RFPCA, against two state-of-the-art methods for fair
PCA: 1) FairPCA Samadi et al. (2018)4, and 2) CFPCA Olfat & Aswani (2019)5 with both cases:
only mean constraint, and both mean and covariance constraints. We consider a wide variety of
datasets with ranging sample sizes and number of features. Further details about the datatasets can
be found in Appendix C. The code for all experiments is available in supplementary materials. We
include here some details about the hyper-parameters that we search in the cross-validation steps.
•	RFPCA. We notice that the neighborhood size εa should be inversely proportional to the size of
subgroup a. Indeed, a subgroup with large sample size is likely to have more reliable estimate of
the moment information. Then we parameterize the neighborhood size ε° by a common scalar
α, and we have εΟ = α∕√Na, where Na is the number of samples in group a. We search
α ∈ {0.05, 0.1, 0.15} and λ ∈ {0., 0.5, 1., 1.5, 2.0, 2.5}. For better convergence quality, we set
the number of iteration for our subgradient descent algorithm to τ = 1000 and also repeat the
Riemannian descent for 20 randomly generated initial point U0 .
•	FairPCA. According to Samadi et al. (2018), we only need tens of iterations for the multiplica-
tive weight algorithm to provide good-quality solution; however, to ensure a fair comparison, we
set the number of iterations to 1000 for the convergence guarantee. We search the learning rate
η of the algorithm from set of 17 values evenly spaced in [0.25, 4.25] and {0.1}.
•	CFPCA. Following Olfat & Aswani (2019), for the mean-constrained version of CFPCA, we
search δ from {0., 0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9}, and for both the mean and covariance con-
strained version, we fix δ = 0 while searching μ in {0.0001,0.001,0.01,0.05,0.5}.
Trade-offs. First, we examine the trade-off between the total reconstruction error and the gap be-
tween the subgroup error. In this experiment, we only compare our model with FairPCA and
CFPCA mean-constraint version. We plot a pareto curve for each of them over the two criteria with
different hyper-parameters (hyper-parameters test range are mentioned above). The whole datasets
are used for training and evaluation. The results averaged over 5 runs are shown in Figure 2.
In testing methods with different principal components, we first split each dataset into training set
and test set with equal size (50% each), the projection matrix of each method is learned from training
set and tested over both sets. In this case, we only compare our method with traditional PCA and
FairPCA method. We fix one set hyper-parameters for each method. For FairPCA, we set η = 0.1
and for RFPCA we set α = 0.15, λ = 0.5, others hyper-parameters are kept as discussed before.
The results are averaged over 5 different splits. Figure 3 shows the consistence of our method
performing fair projections over different values of k. Our method (cross) exhibits smaller gap of
subgroup errors. More results and discussions on the effect of ε can be found in Appendix D.2.
4 https://github.com/samirasamadi/Fair-PCA 5 https://github.com/molfat66/FairML
8
Published as a conference paper at ICLR 2022
PCA - GroupO
PCA - Groupl
FairPCA - GroupO
FairPCA - Groupl
RFPCA - GroupO
RFPCA - Groupl
• RFPCA
▼ FairPCA
+ CFPCA
10,0	10.1	10.2	10.3	10.4
Reconstruction error (all data)
2	4	6	8	10	12	14	16	18	20
k
Figure 3: Subgroup average error with different k
on Biodeg dataset (Out-of-sample).
Figure 2: Pareto curves on Default Credit
dataset (all data) with 3 principal components

Cross-validations. Next, we report the performance of all methods based on three criteria: absolute
difference between average reconstruction error between groups (ABDiff.), average reconstruction
error of all data (ARE.), and the fairness criterion defined by Olfat & Aswani (2019) with respect
to a linear SVM’s classifier family (4FLin).6 Due to the space constraint, we only include the
first two criteria in the main text, see Appendix 4 for full results. To emphasize the generalization
capacity of each algorithm, we split each dataset into a training set and a test set with ratio of
30% - 70% respectively, and only extract top three principal components from the training set.
We find the best hyper-parameters by 3-fold cross validation, and prioritize the one giving minimum
value of the summation (ABDiff.+ARE.). The results are averaged over 10 different training-testing
splits. We report the performance on both training set (In-sample data) and test set (Out-of-sample
data). The details results for Out-of-sample data is given in Table 1, more details about settings and
performance can be found at Appendix D.
Results. Our proposed RFPCA method outperforms on 11 out of 15 datasets in terms of the subgroup
error gap ABDiff, and 9 out of 15 with the totall error ARE. criterion. There are 5 datasets that
RFPCA gives the best results for both criteria, and for the remaining datasets, RFPCA has small
performance gaps compared with the best method.
Table 1: Out-of-sample errors on real datasets. Bold indicates the lowest error for each dataset.
DataSet	RFPCA		FairPCA		CFPCA-Mean Con.		CFPCA - Both Con.	
	ABDiff.	ARE.	ABDiff.	ARE.	ABDiff.	ARE.	ABDiff.	ARE.
Default Credit	0.9483	10.3995	1.4401	10.4439	0.9367	10.9451	3.3359	22.0310
Biodeg	23.0066	33.8571	27.5159	34.6184	29.1728	37.6052	37.9533	50.7090
E. Coli	1.1500	1.7210	1.5280	2.4799	1.1005	2.9466	5.1275	5.6674
Energy	0.0125	0.2238	0.0138	0.2225	0.1229	2.7318	0.1001	7.9511
German Credit	2.0588	43.9032	1.3670	44.0064	1.7845	43.9648	1.4955	49.5014
Image	0.7522	6.0199	1.6129	10.2616	1.1499	14.3725	4.7013	19.3356
Letter	0.1712	7.4176	1.2489	7.4470	0.4427	8.7445	0.5743	15.1779
Magic	1.8314	3.9094	2.9405	3.3815	5.5790	4.2105	8.7810	9.0064
ParkinSonS	0.3273	5.0597	0.8678	4.9044	3.3804	5.7260	18.3312	19.7001
SkillCraft	0.7669	8.2828	0.7771	8.2494	1.0283	9.9484	1.2849	15.9751
Statlog	0.0838	3.0998	0.3356	7.9734	0.4476	10.8263	13.8437	35.8268
Steel	1.1472	12.5944	1.2208	12.3096	4.8710	16.4015	3.8084	25.8953
Taiwan Credit	0.5523	10.9845	0.5710	10.9415	0.5744	13.0437	0.9535	21.8963
Wine Quality	0.6359	4.2801	0.3046	6.0936	1.5020	6.1118	3.0451	10.1001
LFW	0.4463	7.6229	0.5340	7.6361		fail to converge		
6 The code to estimate this quantity is provided at the author’s repository
9
Published as a conference paper at ICLR 2022
References
Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix
Manifolds. Princeton University Press, 2007.
Alekh Agarwal, Miroslav Dudik, and ZhiWei Steven Wu. Fair regression: Quantitative definitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120-129.
PMLR, 2019.
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision:
A survey. Ieee Access, 6:14410-14430, 2018.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and machine learning. fairmlbook.
org, 2019, 2018.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal
justice risk assessments: The state of the art. Sociological Methods & Research, 50(1):3-44,
2021.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 44(2):565-600, 2019.
Jose Blanchet, Karthyek Murthy, and Viet Anh Nguyen. Statistical analysis of Wasserstein distribu-
tionally robust estimators. INFORMS TutORials in Operations Research, 2021.
Flavio P Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R Varshney. Optimized pre-processing for discrimination prevention. In Proceedings of
the 31st International Conference on Neural Information Processing Systems, pp. 3995-4004,
2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopad-
hyay. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069, 2018.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big Data, 5(2):153-163, 2017.
Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with
application to data-driven problems. Operations Research, 58(3):595-612, 2010.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Advances in Neural Information Process-
ing Systems, pp. 2791-2801, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226, 2012.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 259-268, 2015.
C.R. Givens and R.M. Shortt. A class of Wasserstein metrics for probability distributions. The
Michigan Mathematical Journal, 31(2):231-240, 1984.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning.
In Advances in Neural Information Processing Systems 29, pp. 3315-3323, 2016a.
10
Published as a conference paper at ICLR 2022
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing Systems, 29:3315-3323, 2016b.
Tito Homem-de Mello and Guzin Bayraksan. Monte Carlo sampling-based methods for stochastic
optimization. Surveys in Operations Research and Management Science, 19(1):56-85, 2014.
Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal
of educational psychology, 24(6):417, 1933.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrim-
ination. Knowledge and Information Systems, 33(1):1-33, 2012.
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35-50. Springer, 2012.
Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pp. 130-166. INFORMS,
2019.
Matt J Kusner, Joshua R Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. arXiv
preprint arXiv:1703.06856, 2017.
Xiao Li, Shixiang Chen, Zengde Deng, Qing Qu, Zhihui Zhu, and Anthony Man Cho So. Weakly
convex optimization over Stiefel manifold using Riemannian subgradient-type methods. SIAM
Journal on Optimization, 33(3):1605-1634, 2021.
Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. Does mitigating ML’s impact dis-
parity require treatment disparity? In Advances in Neural Information Processing Systems, pp.
8125-8135, 2018.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pp. 3384-3393.
PMLR, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1-35, 2021.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization us-
ing the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
Advances in Neural Information Processing Systems 30, pp. 2971-2980, 2017.
Viet Anh Nguyen. Adversarial Analytics. PhD thesis, Ecole PolytechniqUe Federale de Lausanne,
2019.
Viet Anh Nguyen, Soroosh Shafieezadeh Abadeh, Man-Chung Yue, Daniel Kuhn, and Wolfram
Wiesemann. Calculating optimistic likelihoods using (geodesically) convex optimization. In
Advances in Neural Information Processing Systems, pp. 13942-13953, 2019.
Viet Anh Nguyen, S. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani. Bridging Bayesian
and minimax mean square error estimation via Wasserstein distributionally robust optimization.
Mathematics of Operations Research, 2021a.
Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh, Damir Filipovic, and Daniel Kuhn. Mean-
covariance robust risk measurement. arXiv preprint arXiv:2112.09959, 2021b.
11
Published as a conference paper at ICLR 2022
Matt Olfat and Anil Aswani. Convex formulations for fair principal component analysis. In Pro-
Ceedings of the AAAI Conference on Artificial Intelligence, volume 33,pp. 663-670, 2019.
Karl Pearson. Liii. On lines and planes of closest fit to systems of points in space. The London,
Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559-572, 1901.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659, 2019.
Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, and Santosh Vem-
pala. The price of fair PCA: One extra dimension. In Advances in Neural Information Processing
Systems, pp. 10976-10987, 2018.
Nian Si, Karthyek Murthy, Jose Blanchet, and Viet Anh Nguyen. Testing group fairness via optimal
transport projections. In Proceedings of the 38th International Conference on Machine Learning,
2021.
James E Smith and Robert L Winkler. The optimizer’s curse: Skepticism and postdecision surprise
in decision analysis. Management Science, 52(3):311-322, 2006.
Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie Morgenstern, and Santosh Vem-
pala. Multi-criteria dimensionality reduction with applications to fairness. arXiv preprint
arXiv:1902.11281, 2019.
Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distributionally robust
approach to fair classification. arXiv preprint arXiv:2007.09530, 2020.
Bahar Taskesen, Man-Chung Yue, Jose Blanchet, Daniel Kuhn, and Viet Anh Nguyen. Sequential
domain adaptation by synthesizing distributionally robust experts. In Proceedings of the 38th
International Conference on Machine Learning, 2021.
Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International
Workshop on Software Fairness (fairware), pp. 1-7. IEEE, 2018.
Yijie Wang, Viet Anh Nguyen, and Grani Hanasusanto. Wasserstein robust support vector machines
with fairness constraints. arXiv preprint arXiv:2103.06828, 2021.
Dennis Wei, Karthikeyan Natesan Ramamurthy, and Flavio du Pin Calmon. Optimized score trans-
formation for fair classification. arXiv preprint arXiv:1906.00066, 2019.
Man-Chung Yue, Daniel Kuhn, and Wolfram Wiesemann. On linear optimization over Wasserstein
balls. Mathematical Programming, 2021.
Gad Zalcberg and Ami Wiesel. Fair principal component analysis and filter design. IEEE Transac-
tions on Signal Processing, 69:4835-4842, 2021.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333. PMLR, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335-340, 2018.
A	Proofs
A.1 Proofs of Section 2
Proof of Proposition 2.2. Let S = EQ[XX>|A = 0] - EQ[XX>|A = 1]. We first prove the
“only if” direction. Suppose that there exists a fair projection matrix V ∈ Mk relative to Q. Let
U ∈ Md-k be a complement matrix of V . Then, Definition 2.1 can be rewritten as
hUU>, Si = 0,
12
Published as a conference paper at ICLR 2022
which implies that the null space of S has a dimension at least d - k. By the rank-nullity duality,
we have rank(S) ≤ k.
Next, we prove the “if” direction. Suppose that rank(S) ≤ k. Then, the matrix S has at least d - k
(repeated) zero eigenvalues. Let U ∈ Md-k be an orthonormal matrix whose columns are any d- k
eigenvectors corresponding to the zero eigenvalues of S and V ∈ Mk be a complement matrix of
U . Then,
hid- VV >,Si = hUU >,S i =0.
Therefore, V is a fair projection matrix relative to Q. This completes the proof.	□
A.2 Proof of Section 3
Proofs of Proposition 3.3. By exploiting the definition of the loss function `, we find
sup	υEQa['(V,X)]
QaW(Qa,Pa)≤εa
sup
μa ,Ea
s.t.
inf
tr (υ(I - VV>)(∑a + μ0μ>))
kμa - μak2 + tr Qa + ςa - 2(ς会工£(2 )2 ) ≤ εa
γ(εa - tr (∑a)) + γ2 tr ((YI - U(I - VV>))-1Σ0) + T
s.t.
YI — U(I — VV >)	γμa
.	Yμ>	Ykμak2 + T
0, γI υ(I - VV>), γ ≥ 0,
where the last equality follows from Nguyen (2019, Lemma 3.22). By the Woodbury matrix inver-
sion, we have
(YI - U(I - VV>))-1 = γTI - 丁一一- (I - VV>).
Y(U-Y)
Moreover, using the Schur complement, the semidefinite constraint is equivalent to
Ykμa I∣2 + T ≥ Y2μ> (YI-U(I- VV>))-1μa,
which implies that at optimality, we have
T = -uy-R> (I - VV>)μa.
Y-U
At the same time, the constraint YI U(I - VV>) is equivalent to Y > U. Combining all previous
equations, we have
SUp	υEQa ['(V,X)]=	inf	Yεa + ^YU-Ud- VV>,M侪.
QaW(Qa,Pa)≤εa	γ>maχ{0,υ}	Y - U
The dual optimal solution Y? is given by
](l + TWM^
Y ?：(1-hM Ma
、0
ifU ≥ 0,
if u< 0 and〈Id - VV >,Mɔ ≥ ε0,
if u< 0 and〈Id - VV >,Mɔ <ε°.
Note that Y? ≥ max{0, U} in all the cases. Therefore, we have
SUP	UEQa ['(V, X )]
_ ʌ
Qa
ifU ≥0,
This completes the proof.
if U < 0 and〈Id - VV>,Ma)≥ ε0,
if U < 0 and〈!& - VV>,M&)< ε0.
□
13
Published as a conference paper at ICLR 2022
We are now ready to prove Theorem 3.2.
Proof of Theorem 3.2. By expanding the absolute value, problem (4) is equivalent to
min	max{J0(V ), J1(V )},
V∈Rd×k,V>V=Ik
where for each (a, a0) ∈ {(0, 1), (1, 0)}, we can re-express Ja as
Ja(V) =	sup	(α + λ)EQa ['(V, X)]+	sup	(pa0 - X^q., ['(V, X)]
QaW(Qa,Pa)≤εa	Qa，：W(Qa，此，)≤ε.0
Using Proposition 3.3 to reformulate the two individual supremum problems, we have
Ja(V) = (Pa + λ)εa + 2向 + Nq它a^d - VV>,M4 + (Pa + λ)(ld - VV>, Ma)
+ (PaO- λ)εa0 + 2|pa0 - λ∖{ εa0(Id - VV >, Ma0) + (PaO- λ){1d - VV >, Ma0).
By defining the necessary parameters κ, θ, H and C as in the statement of the theorem, We arrive at
the postulated result.	□
A.3 Proofs of Section 4
Proof of Lemma 4.1. Let aU ∈ arg maxa∈{0,1} Fa(U) and a0U = 1 - aU. Then, an Euclidean
subgradient of F is given by
θ	HO
VF(U) =	aU ʌ = MaUU +	au	= Mao U + 2CaUU ∈ Rd×(d-k).
√(UU >,MaU〉	√(UU >,MaU〉	U
The tangent space of the Stiefel manifold M at U is given by
TUM = {∆ ∈ Rd×(d-k) : ∆>U+ U>∆ = 0},
Whose orthogonal projection (Absil et al., 2007, Example 3.6.2) can be computed explicitly via
PrOjTUM(D) = (Id — UU>)D + 1U(U>D - DTU),	D ∈ Rd×(d-k).
Therefore, a Riemannian subgradient of F at any point U ∈ M is given by
gradF (U) = PrOjTUM(VF(U))
=(Id — UU>) I /	θaU .	Mau U + /	"aU	MaU U + 2Cau U I .
∖"u >,Mau〉	J〈UU LMaU〉	)
In the last line, We have used the fact that, if D = SU for some symmetric matrix S, then
U>D-D>U= U>SU-U>S>U=0.
This completes the proof.	□
The proof of Lemma 4.2 relies on the folloWing preliminary result.
Lemma A.1. Let M ∈ R(d-k)×(d-k) be a positive definite matrix. Then,
KUU>,M〉— (U0U0>,M)∣ ≤ 2√d- kσmaχ(M)∣∣U — U0∣∣f ∀U,U0 ∈ M,	(10)
and
，(UU >,M〉— J(U 0U 0>,M〉
σmaχ(M )
Pσmin(M )
kU—U0kF
∀U, U0 ∈ M,
(11)
≤
Where σmax(M) and σmin (M) denote the maximum and minimum eigenvalues of the matrix M.
14
Published as a conference paper at ICLR 2022
Proof of Lemma A.1. For inequality (10),
UU>,M - U0U0>,M ≤ UU>,M - UU0>,M + UU0>,M - U0U0>,M
≤	U,M(U-U0)+U0,M(U-U0)
≤ kUkFkM(U - U0)kF + kU0kFkM(U - U0)kF
=2√d - kσmaxkU - U0kF.
For inequality (11), we first note that the function X → √x is 1∕(2√xmin)-Lipschitz on [xmin, +∞)
and that
UU>, M ≥ (d - k)σmin (M) ∀U ∈ M.
Therefore,
，(UU >,M〉— J〈U 0U 0>,M〉
1
2√(d - k)σmin(M
UU>,M - U0U0>,M
σmax(M )
√σmin(M )
kU - U0kF
≤
≤
where the last inequality follows from (10). This completes the proof.
□
We are now ready to prove Lemma 4.2.
Proof of Lemma 4.2. Let U, U0 ∈ M be two arbitrary points. We have
|F(U) - F(U0)|
= |max {F0 (U), F1(U)} - max {F0(U0), F1(U0)}|
≤ max |Fa(U) - Fa(U0)|
a∈{0,1}
≤ max max[θa CTmax(Mα) ,%_“ ：max(M1-a) , 2√d-k σmaχ(Ca) ∖ ∣∣U - U 0∣∣f ,
a∈{0,1}	[ Jbmin(Ma)	Jσmm(Mj)	J
where the last inequality follows from the definition of Fa and Lemma A.1. This completes the
proof.	□
Proof of Theorem 4.3. The proof follows from the fact that F is convex on the Euclidean space
Rd×(d-k), Lemma 4.2 and Li et al. (2021, Theorem 2) (and the remarks following it).	□
B	Extension to Non-b inary Sensitive Attributes
The main paper focuses on the case ofa binary sensitive attribute with A = {0, 1}. In this appendix,
we extend our approach to the case when the sensitive attribute is non-binary. Concretely, we sup-
pose that the sensitive attribute A can take on any of the m possible values from 1 to m. In other
words, the attribute space now becomes A = {1, . . . , m}.
Definition B.1 (Generalized unfairness measure). The generalized unfairness measure is defined as
the maximum pairwise unfairness measure, that is,
Umax(V,Q) ,	max	∣Eq['(V,X)|A = a] — Eq['(V,X)|A = a0]∣.
(a,a0)∈A×A
Notice that if A = {0, 1}, then Umax ≡ U recovers the unfairness measure for binary sensitive
attribute defined in Section 2.2. We now consider the following generalized fairness-aware PCA
problem
min sup Eq['(V,X )]+ λUmax(V,Q).	(12)
V∈Rd×k,V>V=Ik Q∈B(P)
Here We recall that the ambiguity set B(P) is defined in (5). The next theorem provides the refor-
mulation of (12).
15
Published as a conference paper at ICLR 2022
Theorem B.2 (Reformulation of non-binary fairness-aware PCA). Suppose that for any a ∈ A,
either of the following two conditions holds:
(i)	Marginal probability bounds: 0 ≤ λ ≤ pa,
(ii)	Eigenvalue bounds: the empirical second moment matrix Ma =N1a Pi∈Ia χiχ> satisfies
pd-k σj(Ma) ≥ εα, Where σj(MMa) is the j-th smallest eigenvalues of MMa.
Then problem (12) is equivalent to
min max
V∈Rd×k,V>V=Ik a6=a0
2ca,a0,b ∖J εbhId - VV >,Mbi + λhId - VV >,Ma -
-", -,
Ma0i + λ(εa -
εa0)
Where the parameter ca,a0,b admits values
(Pa + λ
ca,a0,b = < ∣Pa0 - λ∣
[Pb
if b = a,
ifb= a0,
otherWise.
ProofofTheorem B.2. For simplicity, we let E(V, Q, b) = Eq['(V, X)|A = b]. Then, the objective
function of problem (12) can be re-Written as
sup
Q∈B(P)
Eq['(V,X)] + λUmaχ(V, Q)
sup
Q∈B(P)
Σ
b∈A
Pb E (V, Q,b)+ λ max {E(V, Q, a) - E(V, Q, a0)}
a6=a0
max
a6=aO
b6=a,a
0
sup	PbE(V, Qb,b) +	sup	(Pa + λ)E(V, Qa,a) +
__ ʌ . , ___________________ ʌ .,
W(Qb,Pb)≤εb	W(Qa,Pa)≤εa
max
a6=a0
{ X Pb Hhld- VV>,Mb +
b6=a,aO
+ (Pa + λ) (q1d
sup	(PaO - λ)E(V, QaO, a0)
, ʌ .
W(Qa0 ,PaO )≤εa0
2
一 ʌ .
-VV>,Mai +
+ (PaO - λ) ( qhId - VV> , Ma0 i + Sgn(Pa0 - λ
2
maxx∣ X pb (hId - VV >,Mbi + εb) + X ”0,b{bhId - VV >,Mb
+ λ (〈Id - VV>, Ma - Ma0i + εa
εa0	,
—
where the first equality follows from the definition of Umax(V, Q) and E(V, Q, b), the second from
the definition (5) of the ambiguity set B(P), the third from Proposition 3.3 and the fourth from the
definition of ca,a0,b. Noting that the first sum in the above maximization is independent of a and a0,
the proof is completed.	口
Theorem 12 indicates that if the sensitive attribute admits finite values, then the distributionally
robust fairness-aware PCA problem using an Umax unfairness measure can be reformulated as an
optimization problem over the Stiefel manifold, where the objective function is a pointwise maxi-
mization of finite number of individual functions. It is also easy to see that each individual function
can be reparametrized using U, and the Riemannian gradient descent algorithm in Section 4 can be
adapted to solve for the optimal solution. The details on the algorithm are omitted.
16
Published as a conference paper at ICLR 2022
C Information on Datasets
We summarize here the number of observations, dimensions, and the sensitive attribute of the data
sets. For further information about the data sets and pre-processing steps, please refer to Samadi
et al. (2018) for Default Credit and Labeled Faces in the Wild (LFW) data sets, and Olfat & Aswani
(2019) for others. For each data set, we further remove columns with too small standard deviation
(≤ 1e-5) as they do not significantly affect the results, and ones with too large standard deviation
(≥ 1000) which we consider as unreliable features.
Table 2: Number of observations N, dimensions d, and sensitive attribute A of datasets used in this
paper. (y - yes, n - no)
	Default Credit	Biodeg	E. Coli	Energy	German Credit
NT	30000	1055	333	768	1000
d	22	40	7	8	48
	Education	Ready Biodegradable	isCytoplasm	Orientation< 4	A13 ≥ 200DM
A	(high/low)	(y/n)	(y/n)	(y/n)	(y/n)
	Image	Letter	Magic	Parkinsons	SkillCraft
NT	660	20000	19020	5875	3337
d	18	16	10	20	17
A	class (path/grass)	Vowel (y/n)	ClaSSISGamma (y/n)	Sex (male/female)	Age> 20 (y/n)
	Statlog	Steel	Taiwan Credit	Wine Quality	LFW
NT	3071	1941	29623	6497	4000
d	36	24	22	11	576
	RedSoil				
A	(VSgrey/damPgrey)	FaultOther (y/n)	Sex (male/female)	isWhite (y/n)	Sex (male/female)
D Additional Results
D.1 Detail Performances
Table 3 shows the performances of four examined methods with two criteria ABDiff. and ARE. It is
clear that our method achieves the best results over all 14 datasets w.r.t. ABDiff., and 7 datasets on
ARE., which is equal to the number of datasets FairPCA out-perform others.
Table 4 complements Table 1 from the main text, from which we can see that two versions of CFPCA
out-perform others over all datasets w.r.t. 4FLin, which is the criteria they optimize for.
Table 3: In-sample performance over two criteria
Dataset	RFPCA		FairPCA		CFPCA-Mean Con.		CFPCA - Both Con.	
	ABDiff.	ARE.	ABDiff.	ARE.	ABDiff.	ARE.	ABDiff.	ARE.
Default Credit	0.9457	9.9072	1.5821	9.9049	0.9949	10.5164	3.2827	21.4523
Biodeg	9.4093	23.1555	14.2587	23.8227	15.5545	26.6540	24.8706	39.8737
E. Coli	0.5678	1.4804	0.9191	2.0840	0.9539	2.8360	4.5225	5.2155
Energy	0.0094	0.2295	0.0153	0.2273	0.2658	2.7893	0.2136	7.8768
German Credit	1.6265	40.1512	2.9824	40.3393	2.6109	40.1860	2.8741	47.1006
Image	0.1320	5.0924	0.7941	9.0437	0.6910	13.4491	3.0118	18.0000
Letter	0.1121	7.4088	1.2560	7.4375	0.4572	8.7764	0.5301	15.2234
Magic	1.7405	3.8766	2.8679	3.3500	5.5405	4.1938	8.7963	8.9695
Parkinsons	0.1238	5.0471	0.6702	4.8760	3.9470	5.9379	17.8122	19.9788
SkillCraft	0.4231	8.1569	0.5576	8.1096	0.7156	9.7755	0.9334	15.8245
Statlog	0.1972	3.0588	0.3315	7.9980	0.3857	10.9358	13.0725	35.9214
Steel	0.6943	11.0396	1.8015	10.7653	2.8933	14.5680	1.9322	23.9906
Taiwan Credit	1.1516	10.5136	1.3362	10.4478	1.3158	12.5867	2.2720	21.4365
Wine Quality	0.1125	4.1491	0.1705	5.8999	1.1359	5.9117	2.5852	9.8959
LFW	0.4147	7.5137	0.5300	7.5127		fail to converge		
Adjustment for the LFW dataset. To demonstrate the efficacy of our method on high-dimensional
data sets, we also do experiments on a subset of 2000 faces for each of male and female group (4000
17
Published as a conference paper at ICLR 2022
Table 4: Out-of-sample performance measured using the 4FLin criterion.
	RFPCA	FairPCA	CFPCA-Mean Con.	CFPCA - Both Con.
Default Credit	0.1596	0.2236	0.0574	0.0413
Biodeg	0.4892	0.4759	0.2014	0.1371
E. Coli	0.8556	0.7444	0.4455	0.2532
Energy	0.0580	0.0554	0.0502	0.0736
German Credit	0.1997	0.1737	0.1408	0.1093
Image	0.9996	0.9498	0.1874	0.2013
Letter	0.0954	0.0942	0.0556	0.0455
Magic	0.2195	0.2531	0.1561	0.0882
Parkinson’s	0.1459	0.1061	0.1805	0.0480
SkillCraft	0.1126	0.1141	0.0721	0.0742
Statlog	0.9804	0.6309	0.1359	0.0669
Steel	0.2288	0.2240	0.1418	0.0875
Taiwan Credit	0.0604	0.0535	0.0391	0.0370
Wine Quality	0.9699	0.4639	0.2192	0.0817
in total) from LFW dataset,7 all images are rescaled to resolution 24 × 24 (dimensions d = 576).
The experiment follows the same procedure in Section 5, with reducing the number of iterations
to 500 for both RFPCA and FairPCA and 2-fold cross validation, the results are averaged over
10 train-test simulations. Due to the high dimension of the input, the implementation of Olfat &
Aswani (2019) fails to return any result.
7 https://github.com/samirasamadi/Fair-PCA
18
Published as a conference paper at ICLR 2022
D.2 Visualization
D.2. 1 Effects of the Ambiguity Set Radius
We examine the change of the model’s performance with respect to the change of the radius of the
ambiguity sets. To generate the toy data (also used for Figure 1), we use two 2-dimensional Gaussian
distributions to represent two groups of a sensitive attribute, A = 0 and A = 1, or groups 0 and 1
for simplicity. The two distributions both have the mean at (0, 0) and covariance matrices for group
0 and 1 are
4.0	0	0.2 0.4
0	0.2 and 0.4 3.0 ,
respectively. For the test set, the number of samples is 8000 for group 0 and 4000 for group 1, while
for the training set, we have 200 for group 0 and 100 for group 1. We average the results over 100
simulations, for each simulation, the test data is fixed, the training data is randomly generated with
the number of samples mentioned above. The projections are learned on training data and measured
on test data by the summation of ARE. and ABDiff. We fixed λ = 0.1, which is not too small for
achieving fair projections, and not too large to clearly observe the effects of ε, and We also fixed
ε0 for better visualization. Note that we still compute ε1 = α/yfN1 in which, a is tested with 100
values evenly spaced in [0, 10].
The experiment results are visualized in Figure 4. The result suggests that increasing the ambiguity
set radius can improve the overall model’s performance. This justifies the benefit of adding distribu-
tional robustness to the fairness-aware PCA model. After a saturation point, a too large radius can
lessen the role of empirical data, and the model prioritizes a more extreme distribution that is far
from the target distribution, which causes the reduction in the model’s performance on target data.
Figure 4: Performance changes w.r.t. the ambiguity set’s radius. The solid line is the average over
100 simulations, and the shade represent the 1-standard deviation range.
19
Published as a conference paper at ICLR 2022
D.2.2 Pareto Curves
Figures 5 and 6 plot the Pareto frontier for two datasets (Biodeg and German Credit) with 3 principal
components. One can observe that RFPCA produces points that dominate other methods based on
the trade-off between ARE. and ABDiff.
Pareto plot with 3 features -
InSampIe (Biodeg 40-features)
2 0 8 6 4 2 0
2 2 11111
Sdn0」6 U①①①q①Uo-ZprLUSUOU①」Jo①UU①」①J±:P Bqn-Osqq
23,5	24.0	24.5	25.0	25.5	26.0	26.5	27.0
Reconstruction error (all data)
Figure 5: Pareto curves on Biodeg	Figure 6: Pareto curves on German Credit
dataset (all data) with 3 principal components	dataset (all data) with 3 principal components
050505050
Q.7.52Q.7.52.0
NLLLL0.0.0.0.
SdnU①①①q①Uo-Zp ≡lsuou①」JO①UU①」①J⅛zp①In-OSq<
Pareto plot with 3 features -
InSamPIe (GenTlan Credit 48-features)
RFPCA
FairPCA
CFPCA
40.6	40.8	41.0	41.2
Reconstruction error (all data)
20
Published as a conference paper at ICLR 2022
D.2.3 Performance with different principal components
We collect here the reconstruction errors for different numbers of principal components.
Average reconstruction error	Average reconstruction error
on Default Credit (22-dim) - In-sample	on Default Credit (22-dim) - Out-of-sample
PCA - GroupO
PCA - Groupl
FairPCA - GroupO
FairPCA - Groupl
RFPCA - GroupO
RFPCA - Groupl
2	4	6	8	10	12	14	16	18	20
k
2	4	6	8	10	12	14	16	18	20
k
PCA - GroupO
PCA - Groupl
FairPCA - GroupO
FairPCA - Groupl
RFPCA - GroupO
RFPCA - Groupl
Figure 7: Subgroup average error
with different k on Default Credit dataset
Figure 8: Subgroup average error
with different k on Default Credit dataset
Average reconstruction error
on E. Coli (7-dim) - Out-of-sample
PCA - GroupO
PCA - Groupl
FairPCA - GroupO
FairPCA - Groupl
RFPCA - GroupO
RFPCA - Groupl
-∙- PCA - GroupO
-∙- PCA - Groupl
T- FairPCA - GroupO
—FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA-Groupl
Figure 9: Subgroup average error
with different k on E. Coli dataset
Average reconstruction error
On E. Coli (7-dim) - In-SamPIe
Figure 10: Subgroup average error
with different k on E. Coli dataset
21
Published as a conference paper at ICLR 2022
Figure 11: Subgroup average error
with different k on Magic dataset
Average reconstruction error
On MagiC (IO-dim) - In-SamPIe
-∙- PCA - GroupO
-∙- PCA - Groupl
-T- FairPCA - GroupO
T- FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA-Groupl
Average reconstruction error
On MagiC (IO-dim) - OUt-Of-SamPIe
-∙- PCA - GroupO
-∙- PCA - Groupl
—FairPCA - GroupO
T- FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA - Groupl
Figure 12: Subgroup average error
with different k on Magic dataset
Average reconstruction error
on Steel (24-dim) - In-sample
Average reconstruction error
on Steel (24-dim) - Out-of-sample
PCA - GroupO
PCA - Groupl
FairPCA - GroupO
FairPCA - Groupl
RFPCA - GroupO
RFPCA - Groupl
-∙- PCA - GroupO
-∙- PCA - Groupl
T- FairPCA - GroupO
T- FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA - Groupl
2	4	6	8	10	12	14	16	18	20	2	4	6	8	10	12	14	16	18	20
k	k
Figure 13: Subgroup average error
with different k on Steel dataset
Figure 14: Subgroup average error
with different k on Steel dataset
22
Published as a conference paper at ICLR 2022
Average reconstruction error
on Wine Quality (ll-dim) - In-sample
Figure 15: Subgroup average error
with different k on Wine Quality dataset
2	4	6	8
k
-∙- PCA - GroupO
-∙- PCA - Groupl
T- FairPCA - GroupO
T- FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA - Groupl
Average reconstruction error
on Wine Quality (ll-dim) - Out-of-sample
-∙- PCA - GroupO
-∙- PCA - Groupl
T- FairPCA - GroupO
T- FairPCA - Groupl
-+- RFPCA - GroupO
-+- RFPCA - Groupl
10	2	4	6	8	10
k
Figure 16: Subgroup average error
with different k on Wine Quality dataset
23