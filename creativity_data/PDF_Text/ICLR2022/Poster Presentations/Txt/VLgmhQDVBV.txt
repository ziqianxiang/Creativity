Published as a conference paper at ICLR 2022
Implicit Bias of MSE Gradient Optimization in
Underparameterized Neural Networks
Benjamin Bowman	Guido Montufar
UCLA Departments of Mathematics	UCLA Departments of Mathematics and Statistics
benbowman314@math.ucla.edu and MPI MIS
montufar@math.ucla.edu
Ab stract
We study the dynamics of a neural network in function space when optimizing
the mean squared error via gradient flow. We show that in the underparameterized
regime the network learns eigenfunctions of an integral operator TK∞ determined
by the Neural Tangent Kernel (NTK) at rates corresponding to their eigenvalues.
For example, for uniformly distributed data on the sphere Sd-1 and rotation invari-
ant weight distributions, the eigenfunctions of TK∞ are the spherical harmonics.
Our results can be understood as describing a spectral bias in the underparameter-
ized regime. The proofs use the concept of “Damped Deviations”, where devia-
tions of the NTK matter less for eigendirections with large eigenvalues due to the
occurence of a damping factor. Aside from the underparameterized regime, the
damped deviations point-of-view can be used to track the dynamics of the empir-
ical risk in the overparameterized setting, allowing us to extend certain results in
the literature. We conclude that damped deviations offers a simple and unifying
perspective of the dynamics when optimizing the squared error.
1	Introduction
A surprising but well established empirical fact is that neural networks optimized by gradient descent
can find solutions to the empirical risk minimization (ERM) problem that generalize. This is surpris-
ing from an optimization point-of-view because the ERM problem induced by neural networks is
nonconvex (Sontag & Sussmann, 1989; 1991) and can even be NP-Complete in certain cases (Blum
& Rivest, 1993). Perhaps even more surprising is that the discovered solution can generalize even
when the network is able to fit arbitrary labels (Zhang et al., 2017), rendering traditional complexity
measures such as Rademacher complexity inadequate. How does deep learning succeed in the face
of pathological behavior by the standards of classical optimization and statistical learning theory?
Towards addressing generalization, a modern line of thought that has emerged is that gradient de-
scent performs implicit regularization, limiting the solutions one encounters in practice to a favorable
subset of the model’s full capacity (see, e.g., Neyshabur et al., 2015; 2017; Gunasekar et al., 2017;
Wu et al., 2017). An empirical observation is that neural networks optimized by gradient descent
tend to fit the low frequencies of the target function first, and only pick up the higher frequencies later
in training (Rahaman et al., 2019; Ronen et al., 2019; Basri et al., 2020; Xu et al., 2019). A closely
related theme is gradient descent’s bias towards smoothness for regression problems (Williams et al.,
2019; Jin & Montufar, 2021). For classification problems, in suitable settings gradient descent Prov-
ably selects max-margin solutions (Soudry et al., 2018; Ji & Telgarsky, 2019). Gradient descent is
not impartial, thus understanding its bias is an important program in modern deep learning.
Generalization concerns aside, the fact that gradient descent can succeed in a nonconvex optimiza-
tion landscape warrants attention on its own. A brilliant insight made by Jacot et al. (2018) is that
in function space the neural network follows a kernel gradient descent with respect to the “Neural
Tangent Kernel” (NTK). This kernel captures how the parameterization biases the trajectory in func-
tion space, an abstraction that allows one to largely ignore parameter space and its complications.
This is a profitable point-of-view, but there is a caveat. The NTK still depends on the evolution of
the network parameters throughout time, and thus is in general time-dependent and complicated to
analyze. However, under appropriate scaling of the parameters in the infinite-width limit it remains
1
Published as a conference paper at ICLR 2022
constant (Jacot et al., 2018). Once the NTK matrix has small enough deviations to remain strictly
positive definite throughout training, the optimization dynamics start to become comparable to that
of a linear model (Lee et al., 2019). For wide networks (quadratic or higher polynomial dependence
on the number of training data samples n and other parameters) this property holds and this has
been used by a variety of works to prove global convergence guarantees for the optimization (Du
et al., 2019b; Oymak & Soltanolkotabi, 2020; Du et al., 2019a; Allen-Zhu et al., 2019a;b; Zou et al.,
2020; Zou & Gu, 2019; Song & Yang, 2020; Dukler et al., 2020)1 and to characterize the solution
throughout time (Arora et al., 2019; Basri et al., 2020). The NTK has been so heavily exploited
in this setting that it has become synonymous with polynomially wide networks where the NTK
is strictly positive definite throughout training. This begs the question, to what extent is the NTK
informative outside this regime?
While the NTK has hitherto been associated with the heavily overparameterized regime, we demon-
strate that refined analysis is possible in the underparameterized setting. Our theorems primarily
concern a one-hidden layer network, however unlike many NTK results appearing in the literature
our network has biases and both layers are trained. In fact, the machinery we build is strong enough
to extend some existing results in the overparameterized regime appearing in the literature to the
case of training both layers.
1.1	Related Work
There has been a deluge of works on the Neural Tangent Kernel since it was introduced by Jacot
et al. (2018), and thus we do our best to provide a partial list. Global convergence guarantees
for the optimization, and to a lesser extent generalization, for networks polynomially wide in the
number of training samples n and other parameters has been addressed in several works (Du et al.,
2019b; Oymak & Soltanolkotabi, 2020; Du et al., 2019a; Allen-Zhu et al., 2019a;b; Zou et al., 2020;
Zou & Gu, 2019; Song & Yang, 2020; Arora et al., 2019). To our knowledge, for the regression
problem with arbitrary labels, quadratic overparameterization m & n2 is state-of-the art (Oymak
& Soltanolkotabi, 2020; Song & Yang, 2020; Nguyen & Mondelli, 2020). E et al. (2020) gave a
fairly comprehensive study of optimization and generalization of shallow networks trained under the
standard parameterization. Under the standard parameterization, changes in the outer layer weights
are more significant, whereas under the NTK parameterization both layers have roughly equal effect.
Since we study the NTK parameterization in this work, we view the analysis as complementary.
Our work is perhaps most closely connected with Arora et al. (2019). In Theorem 4.1 in that work
they showed that for a shallow network in the polynomially overparameterized regime m & n7, the
training error along eigendirections of the NTK matrix decay linearly at rates that correspond to their
eigenvalues. Our main Theorem 3.5 can be viewed as an analogous statement for the actual risk (not
the empirical risk) in the underparameterized regime: eigenfunctions of the NTK integral operator
TK∞ are approximately learned linearly at rates that correspond to their eigenvalues. In contrast with
Arora et al. (2019), we have that the requirements on width m and number of samples n required to
learn eigenfunctions with large eigenvalues are smaller compared to those with small eigenvalues.
Surprisingly the machinery we build is also strong enough to prove in our setting the direct analog
of Theorem 4.1. Note that Arora et al. (2019) train the hidden layer of a ReLU network via gradient
descent, whereas we are training both layers with biases for a network with smooth activations via
gradient flow. Due to the different settings, the results are not directly comparable. This important
detail notwithstanding, our overparameterization requirement ignoring logarithmic factors is smaller
by a factor of 矗 where n is the number of input samples, d is the input dimension, and δ is the
failure probability. Basri et al. (2020) extended Theorem 4.1 in Arora et al. (2019) to deep ReLU
networks without bias where the first and last layer are fixed, with a higher overparameterization
requirement than the original (Arora et al., 2019). Since the first and last layers are fixed this cannot
be specialized to get a guarantee for training both layers of a shallow network even with ReLU
activations.
Although it was not our focus, the tools to prove Theorem 3.5 are enough to prove analogs of
Theorem 4 and Corollary 2 in the work of Su & Yang (2019). Theorem 4 and Corollary 2 of Su &
Yang (2019) are empirical risk guarantees that show that for target functions that participate mostly
1Not all these works explicitly use that the NTK is positive definite. However, they all operate in the regime
where the weights do not vary much and thus are typically associated with the NTK regime.
2
Published as a conference paper at ICLR 2022
in the top eigendirections of the NTK integral operator TK∞ , moderate overparameterization is
possible. Again in this work they train the hidden layer of a ReLU network via gradient descent,
whereas we are training both layers with biases for a network with smooth activations via gradient
flow. Again due to the different settings, we emphasize the results are not directly comparable. In
our results the bounds and requirements are comparable to Su & Yang (2019), with neither appearing
better. Nevertheless we think it is important to demonstrate that these results hold for training both
layers with biases, and we hope our “Damped Deviations” approach will simplify the interpretation
of the aforementioned works.
Cao et al. (2020, Theorem 4.2) provide an analogous statement to our Theorem 3.5 if you replace
our quantities with their empirical counterparts. While our statement concerns the projections of
the test residual onto the eigenfunctions of an operator associated with the Neural Tangent Kernel,
their statement concerns the inner products of the empirical residual with those eigenfunctions.
Their work was a crucial step towards explaining the spectral bias from gradient descent, however
we view the difference between tracking the empirical quantities versus the actual quantities to
be highly nontrivial. Another difference is they consider a ReLU network whereas we consider
smooth activations; also they consider gradient descent versus we consider gradient flow. Due to the
different settings we would like to emphasize that the scalings of the different parameters are not
directly comparable, nevertheless the networks they consider are significantly wider. They require
at least m ≥ O(max{σ-14, e-6}), where σk is a cutoff eigenvalue and E is the error tolerance. By
contrast in our work, to have the projection onto the top k eigenvectors be bounded by epsilon in L2
norm requires m = Ω(σ-4e-2). Another detail is their network has no bias whereas ours does.
1.2	Our Contributions
The key idea for our work is the concept of “Damped Deviatons”, the fact that for the squared error
deviations of the NTK are softened by a damping factor, with large eigendirections being damped
the most. This enables the following results.
•	In Theorem 3.5 we characterize the bias of the neural network to learn the eigenfunctions of the
integral operator TK∞ associated with the Neural Tangent Kernel (NTK) at rates proportional to
the corresponding eigenvalues.
•	In Theorem 3.7 we show that in the overparameterized setting the training error along different
directions can be sharply characterized, showing that Theorem 4.1 in Arora et al. (2019) holds for
smooth activations when training both layers with a smaller overparameterization requirement.
•	In Theorem 3.8 and Corollary 3.9 we show that moderate overparameterization is sufficient for
solving the ERM problem when the target function has a compact representation in terms of
eigenfunctions of TK∞ . This extends the results in Su & Yang (2019) to the setting of training
both layers with smooth activations.
2 Gradient Dynamics and Damped Deviations
2.1 Notations
We will use ∣∣∙k2 and h∙, ∙i2 to denote the L2 norm and inner product respectively (for vectors orfor
functions depending on context). For a symmetric matrix A ∈ Rk×k, λi (A) denotes its ith largest
eigenvalue, i.e. λι(A) ≥ λ2(A) ≥ ∙∙∙ ≥ λk(A). For a matrix A, ∣A∣op := Supkxk2≤1 l∣Ax∣2
is the operator norm induced by the Euclidean norm. We will let h∙, ∙)Rn denote the standard
inner product on Rn normalized by ɪ, namelyhx, y〉Rn = n〈x, y〉2 = n P2ι XiJi∙ We will let
IlxIlRn =，(x,x〉Rn be the associated norm. This normalized inner product has the convenient
property that if v ∈ Rn such that vi = O(1) for each i then ∣v∣Rn = O(1), where by contrast
∣∣v∣2 = O(√n). This is convenient as we will often consider what happens when n → ∞. ∣∙k∞
will denote the supremum norm with associated space L∞. We will use the standard big O and Ω
notation with O and Ω hiding logarithmic terms.
3
Published as a conference paper at ICLR 2022
2.2 Gradient Dynamics And The NTK Integral Operator
We will let f(x; θ) denote our neural network taking input x ∈ Rd and parameterized by θ ∈ Rp.
The specific architecture of the network does not matter for the purposes of this section. Our training
data consists of n input-label pairs {(x1, y1), . . . , (xn, yn)} where xi ∈ Rd and yi ∈ R. We focus
on the setting where the labels are generated from a fixed target function f*, i.e. yi = f* (xi). We
will concatenate the labels into a label vector y ∈ Rn, i.e. yi = f* (χi). We will let r(θ) ∈ Rn be the
vector whose ith entry is equal to f (xi； θ) - f * (xi). Hence r(θ) is the residual vector that measures
the difference between our neural networks predictions and the labels. We will be concerned with
optimizing the squared loss
Φ(θ) = 21n kr(θ)k2 = 1 kr(θ)kRn.
Optimization will be done by gradient flow
∂tθt = -∂θΦ(θ),
which is the continuous time analog of gradient descent. We will denote the residual at time t,
r(θt), as ^ for the sake of brevity and similarly We will let ft(x) = f(x; θt). We will let rt(x):=
ft(x) — f * (x) denote the residual off of the training set for an arbitrary input x.
We quickly recall some facts about the Neural Tangent Kernel and its connection to the gradient
dynamics. For a comprehensive tutorial we suggest Jacot et al. (2018). The analytical NTK is the
kernel given by
∂f(x; θ) ∂f (x0; θ)
K∞(x, x0) := E
∂θ
∂θ 2
where the expectation is taken with respect to the parameter initialization for θ. We associate K∞
with the integral operator TK∞ : Lρ2(X) → Lρ2(X) defined by
TK∞ f(x) :=	K∞(x, s)f (s)dρ(s),
X
where X is our input space with probability measure ρ. Our training data xi ∈ X are distributed
according to this measure Xi 〜ρ. By Mercer,s theorem we can decompose
∞
K∞ (x, x0) = Xσiφi(x)φi(x0),
i=1
where {φi}in=1 is an orthonormal basis ofL2, {σi}i∞=1 is a nonincreasing sequence of positive values,
and each φi is an eigenfunction of TK∞ with eigenvalue σi > 0. When X = Sd-1 is the unit
sphere, ρ is the uniform distribution, and the weights of the network are from a rotation invariant
distribution (e.g. standard Gaussian), {φi }i∞=1 are the spherical harmonics (which in d = 2 is the
Fourier basis) due to K∞ being rotation-invariant (see Bullins et al., 2018, Theorem 2.2). We will
let κ := maxx∈X K∞(x, x) which will be a relevant quantity in our later theorems. In our setting
κ will always be finite as K∞ will be continuous and X will be bounded. The training data inputs
{x1, . . . , xn} induce a discretization of the integral operator TK∞, namely
1n
Tnf(X) := n∑^κ ∞(x,Xi)f (Xi) = J K ∞(x,s)f (s)dpn(s),
where Pn = n Pn=1 δχi is the empirical measure. We recall the definition of the time-dependent
NTK2,
∂f(x; θt) ∂f (x0; θt)
Kt(x, x0) :
∂θ
∂θ 2
We can look at the version of Tn corresponding to Kt, namely
1n
Tn f (x) := n 工 Kt(x,Xi)f (Xi) = J Kt(x,s')f(s')dpn(s').
2The NTK is an overloaded term. To help ameliorate the confusion, we will use NTK to describe K∞ and
NT K (italic font) to describe the time-dependent version Kt .
4
Published as a conference paper at ICLR 2022
We recall that the residual rt(x) := f(χ; θ) - f * (x) follows the update rule
1n
∂trt(x) = -— EKt(X,xi)rt(xi) = -Tn rt.
n i=1
We will let (Ht)i,j := Kt(xi, xj) and Hi∞,j := K∞(xi, xj) denote the Gram matrices induced
by these kernels and we will let Gt := 1 Ht and G∞ := n H∞ be their normalized versions3 *.
Throughout we will let u1, . . . , un denote the eigenvectors of G∞ with corresponding eigenvalues
λι,...,λn. The uι,...,un are chosen tobe orthonormal with respect to the inner product h∙, ∙)Rn.
When restricted to the training set we have the update rule
∂t^ = -1 Htrt = -Gtrt.
n
2.3 Damped Deviations
The concept of damped deviations comes from the very simple lemma that follows (the proof is
provided in Appendix D). The lemma compares the dynamics of the residual r(t) on the training set
to the dynamics of an arbitrary kernel regression exp(-Gt)r(0):
Lemma 2.1. Let G ∈ Rn×n be an arbitrary positive semidefinite matrix and let Gs be the time
dependent NTK matrix at time s. Then
^
=exp(-Gt)ro + Z
0
exp(-G(t - S))(G — Gs)ιrsds.
Let,s specialize the lemma to the case where G = G∞. In this case the first term is exp(-G∞t)ro,
which is exactly the dynamics of the residual in the exact NTK regime when Gt = G∞ for all t.
The second term is a correction term that weights the NTK deviations (G∞ - Gs ) by the damping
factorexp(-G∞(t - s)). We see that damping is largest along the large eigendirections ofG∞. The
equation becomes most interpretable when projected along a specific eigenvector. Fix an eigenvector
ui of G∞ corresponding to eigenvalue λi . Then the equation along this component becomes
hrt,UiiRn
exp(-λit)hro, Ui〉Rn + Z hexp(一λi(t — s))(G∞
0
—Gs)rs, UiiRnds.
The first term above converges to zero at rate λi . The second term is a correction term that weights
the deviatiations of the NTK matrix Gs from G∞ by the damping factor exp(-λi(t - s)). The
second term can be upper bounded by
lʃ hexp(-λi (t - s))(G∞ - Gs)rs, uiiRn ds| ≤ Z exp(-λi(t - S)) kG∞ - Gskop IlrsllRn ds
≤ [1- exp(-λit)] sup kG∞ - Gskop kro∣Rn ,
λi	s∈[0,t]
where We have used the property ∣∣rs kRn ≤ 1f0 ∣Rn from gradient flow. When f * = O(1) We have
that 疗。IlRn = O(1), thus whenever ∣∣G∞ - Gskop is small relative to λ% this term is negligible. It
has been identified that the NTK matrices tend to have a small number of outlier large eigenvalues
and exhibit a low rank structure (Oymak et al., 2020; Arora et al., 2019). In light of this, the de-
pendence of the above bound on the magnitude of λi is particularly interesting. We reach following
important conclusion.
Observation 2.2. The dynamics in function space will be similar to the NTK regime dynamics
along eigendirections whose eigenvalues are large relative to the deviations of the time-dependent
NTK matrix from the analytical NTK matrix.
The equation in Lemma 2.1 concerns the residual restricted to the training set, but we will be inter-
ested in the residual for arbitrary inputs. Recall that rt(x) = f(x; θt) - f* (x) denotes the residual at
time t for an arbitrary input. Then more generally we have the following damped deviations lemma
for the whole residual (proved in Appendix C.3).
3 Gt and G∞ are the natural matrices to work with when working with the mean squared error as opposed to
the unnormalized squared error. Also G∞'s spectra concentrates around the spectrum of the associated integral
operator TK∞ and is thus a more convenient choice in our setting.
5
Published as a conference paper at ICLR 2022
Lemma 2.3. Let K(x, x0) be an arbitrary continuous, symmetric, positive-definite kernel. Let
[Tkh](∙) = JX K(•, s)h(s)dρ(s) be the integral operator associated with K and let [TSh](∙)=
1 ∑S=ι Ks(∙, xi)h(xi) denote the operator associated with the time-dependent NTK KS. Then
rt
exp(-TK t)r0 + Z exp(-TK (t -
0
s))(TK -TnS)rSds,
where the equality is in the L2 sense.
For our main results we will specialize the above lemma to the case where K = K∞. However there
are other natural kernels to compare against, say K0 or the kernel corresponding to some subset of
parameters. We will elaborate further on this point after we introduce the main theorem. When
specializing Lemma 2.3 to the case K = K∞, we have that TK∞ and TnS are the operator analogs of
G∞ and GS respectively. From this statement the same concepts holds as before, the dynamics of rt
will be similar to that of exp(-TK∞t)r0 along eigendirections whose eigenvalues are large relative
to the deviations (TK∞ - TnS ). In the underparameterized regime we can bound the second term
and make it negligible (Theorem 3.5) and thus demonstrate that the eigenfunctions φi of TK∞ with
eigenvalues σi will be learned at rate σi. When the input data are distributed uniformly on the sphere
Sd-1 and the network weights are from a rotation-invariant distribution, the eigenfunctions of TK∞
are the spherical harmonics (which is the Fourier basis when d = 2). In this case the network is
biased towards learning the spherical harmonics that correspond to large eigenvalues of TK∞ . It is
in this vein that we will demonstrate a spectral bias.
3 Main Results
Our theorems will concern the shallow neural network
1m	1T
f (x； θ) = -^= T agσ(hwg, x〉2 + b`) + b0 = -=αrσ(Wx + b) + b。,
vm t1	vm
where W ∈ Rm×d, a, b ∈ Rm and b。∈ R and w` = We,： denotes the 'th row of W and σ : R → R
is applied entry-wise. θ = (αT, vec(W)T, bT, b0)T ∈ Rp where p =md + 2m + 1 is the total
number of parameters. Here we are utilizing the NTK parameterization (Jacot et al., 2018). For a
thorough analysis using the standard parameterization we suggest E et al. (2020). We will consider
two parameter initialization schemes. The first initializes Wi^ (0)〜 W, b`(θ)〜 B, a`(θ)〜 A,
b。〜B0 i.i.d., where W, B, A, B0 represent zero-mean unit variance SUbgaUSSian distributions. In
the second initialization scheme we initialize the parameters according to the first scheme and then
perform the following swaps W(0) → WW ((00)) , b(0) → bb((00)) , a(0) → -aa((00)) , b。 → 0 and
replace the √m factor in the parameterization with √m. This is called the “doubling trick” (Chizat
et al., 2019; Zhang et al., 2020) and ensures that the network is identically zero f (x; θo) ≡ 0 at
initialization. We will explicitly state where we use the second scheme and otherwise will be using
the first scheme.
The following assumptions will persist throughout the rest of the paper:
Assumption 3.1. σ	is a C2 function satisfying kσ 0 k∞ , kσ 00 k∞ < ∞.
Assumption 3.2. The inputs satisfy kxk2 ≤ M.
The following assumptions will be used in most, but not all theorems. We will explicitly state when
they apply.
Assumption 3.3. The input domain X is compact with strictly positive Borel measure ρ.
Assumption 3.4. TK∞ is strictly positive, i.e., hf, TK∞fi2 > 0 for f 6= 0.
Most activation functions other than ReLU satisfy Assumption 3.1, such as Softplus σ (x) = ln(1 +
ex), Sigmoid σ(x) = ι+-X, and Tanh σ(x)= 工：—；_X. Assumption 3.2 is a mild assumption
which is satisfied for instance for RGB images and has been commonly used (Du et al., 2019b;a;
Oymak & Soltanolkotabi, 2020). Assumption 3.3 is so that Mercer’s decomposition holds, which
6
Published as a conference paper at ICLR 2022
is often assumed implicitly. Assumption 3.4 is again a mild assumption that is satisfied for a broad
family of parameter initializations (e.g. Gaussian) anytime σ is not a polynomial function, as we
will show in Appendix G. Assumption 3.4 is not strictly necessary but it simplifies the presentation
by ensuring TK∞ has no zero eigenvalues.
We will track most constants that depend on parameters of our theorems such as M, the activation
function σ, and the target function f *. However, constants appearing in concentration inequalities
such as Hoeffding's or Bernstein,s inequality or constants arising from δ∕2 or δ∕3 arguments will
not be tracked. We will reserve c, C > 0 for untracked constants whose precise meaning can vary
from statement to statement. In the proofs in the appendix it will be explicit which constants are
involved.
3.1 Underparameterized Regime
Our main result compares the dynamics of the residual rt(x) = f (x; θt) - f * (x) to that
of exp(-TK∞t)r0 in the underparameterized setting. Note that hexp(-TK∞t)r0, φii2 =
exp(-σit)hr0, φii2, thus exp(-TK∞t)r0 learns the eigenfunctions φi ofTK∞ at rate σi. Therefore
exp(-TK∞t)r0 exhibits a bias to learn the eigenfunctions of TK∞ corresponding to large eigen-
values more quickly. To our knowledge no one has been able to rigorously relate the dynamics in
function space of the residual rt to exp(-TK∞t)r0, although that seems to be what is suggested by
Ronen et al. (2019); Basri et al. (2020). The existing works we are aware of (Arora et al., 2019;
Basri et al., 2020; Cao et al., 2020) characterize the bias of the empirical residual primarily in the
heavily overparameterized regime (Cao et al. (2020) stands out as requiring wide but not necessar-
ily overparameterized networks). By contrast, we characterize the bias of the whole residual in the
underparameterized regime.
Theorem 3.5. Assume that Assumptions 3.3 and 3.4 hold. Let Pk be the orthogonal projection in
L2 onto span{φι,..., φk} and let D := 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0k∞ , 1}. Ifwe are doing the
doubling trick set S 0 = 0 and otherwise set S0 = O (j O(d) + log(c∕δ)) , S = ∣∣f * ∣∞ + S0.
Also let T > 0. Assume m ≥ D2 ∣y∣Rn T2, and m ≥ O(log(c∕δ) + O(d)) max {T2,1}. Then with
probability at least 1 - δ we have that for all t ≤ T and k ∈ N
kPk(rt - exp(-Tκ∞t)ro)∣2 ≤-exj-%" O (S [1 + tS] √m + S(1 +
and
Ilrt - exp(-Tk∞t)r0k2 ≤ tO (S [1 + tS] √= + S(1 +
Theorem 3.5 will be proved in Appendix C. The proof uses the uniform deviation bounds for the
NTK to bound Tn -Tns and tools from empirical process theory to show convergence ofTn to TK∞
uniformly over a class of functions corresponding to networks with bounded parameter norms.
To interpret the results, we observe that to track the dynamics for eigenfunctions corresponding to
eigenvalue σk and above, the expression under the O needs to be small relative to +.ThUS the bias
towards learning the eigenfunctions corresponding to large eigenvalues appears more pronounced.
When t = log(∣r0∣2 ∕)∕σk, we have that ∣Pk exp(-TK∞t)r0∣2 ≤ . Thus by applying this
stopping time we get that to learn the eigenfunctions corresponding to eigenvalue σk and above
up to E accuracy We need √tm . E and √p . E which translates to m & σ-4e-2 and n &
pσk-4-2. In typical NTK works the width m needs to be polynomially large relative to the number
of samples n, where by contrast here the width depends on the inverse of the eigenvalues for the
relevant components of the target function. From an approximation point-of-view this makes sense;
the more complicated the target function the more expressive the model must be. We believe future
works can adopt more precise requirements on the width m that do not require growth relative to the
number of samples n. To further illustrate the scaling of the parameters required by Theorem 3.5,
we can apply Theorem 3.5 for an appropriate stopping time to get a bound on the test error.
Corollary 3.6. Assume Assumptions 3.3 and 3.4 hold. Suppose that f* = O(1) and assume we
are performing the doubling trick where f0 ≡ 0 so that r0 = -f*. Let k ∈ N and let Pk be
7
Published as a conference paper at ICLR 2022
the orthogonal projection onto span{φ1, . . . , φk}. Set t
log(√2kPk f k2" 1 ) Then we have that
σk
m = Ω(亲)and n = Ω (急)suffices to ensure with probability at least 1 一 δ
1 krtk2 ≤ 2e + 2 k(I- PQf*k2 ∙
If one specialized to the case where f * is a finite sum of eigenfunctions of Tk∞ (when the data
is uniformly distributed on the sphere S d-1 and the network weights are from a rotation invariant
distribution this corresponds to a finite sum of spherical harmonics, which in d = 2 is equivalently
a bandlimited function) one can choose k such that k(I 一 Pk)f* k22 = 0. It is interesting to note that
in this special case gradient flow with early stopping achieves essentially the same rates with respect
to m and n (up to constants and logarithms) as the estimated network in the classical approximation
theory paper by Barron (1994). It is also interesting to note that the approximation results by Barron
(1994) depend on the decay in frequency domain of the target function f * via their constant Cf*,
and similarly for US the constant 1∕σ4 grows with the bandwidth of the target function in the case of
uniform distribution on the sphere S1 which we mentioned parenthetically above.
While in Theorem 3.5 We compared the dynamics of r against that of exp(-TK∞t)r°, the damped
deviations equation given by Lemma 2.3 enables you to compare against exp(-Tκt)r° for an arbi-
trary kernel K. There are other natural choices for K besides K = K∞, the most obvious being
K = K0. In Appendix C.8 we prove a version of Theorem 3.5 where K = K0 and θ0 is an arbitrary
deterministic parameter initialization. This could be interesting in scenarios where the parameters
are initialized from a pretrained network or one has a priori knowledge that informs the selection of
θ0. One could let K be the kernel corresponding to some subset of parameters, such as the random
feature kernel (Rahimi & Recht, 2008b) corresponding to the outer layer. This would compare the
dynamics of training all layers to that of training a subset of the parameters. If one wanted to account
for adaptations of the kernel Kt one could try to set K = Kt0 for some t0 > 0. However since
θt0 depends on the training data it is not obvious how one could produce a bound for Tns 一 Kt0 .
Nevertheless we leave the suggestion open as a possibility for future work.
3.2 Overparameterized Regime
Once one has deviation bounds for the NTK so that the quantity kG∞ 一 Gskop is controlled, the
damped deviations equation (Lemma 2.1) allows one to control the dynamics of the empirical risk.
In this section we will demonstrate three such results that follow from this approach. The following
is our analog of Theorem 4.1 from Arora et al. (2019) in our setting, proved in Appendix E. The
result demonstrates that when the network is heavily overparameterized, the dynamics of the residual
^ follow the NTK regime dynamics exp(-G∞t)r0.
Theorem 3.7. Assume m = Ω(dn5c-2λn(H∞)-4) and m ≥ O(log(c∕δ) + O(d)) and f * =
O(1). Assume we are performing the doubling trick so that ro = —y. Let vι,... ,Vn denote the
eigenvectors of G∞ normalized to have unit L2 norm kvik2 = 1. Then with probability at least
1-δ
^ = exp(-G∞t)(-y) + δ(t),
where supt≥0 kδ(t)k2 ≤ . In particular
n
∖
krtk2
EeXp(-2λit)∣hy, Vii2∣2 ± e.
i=1
In the work of Arora et al. (2019) the requirement is m = Ω(入(口耳蜡台匕2) and K = O(√n) where
w` 〜N(0, κ2I) (not to be confused with our definition of K := maxχ K∞(x, x)). By contrast our
weights have unit variance, which for Gaussian initialization corresponds to w` 〜N(0,I). They
require K to be small to ensure the neural network is small in magnitude at initializeation. To achieve
the same effect we can perform antisymmetric initializeation to ensure the network is equivalently 0
at initializeation. Our overparameterization requirement ignoring logarithmic factors is smaller by a
factor of %. Again due to the different settings we do not claim superiority over this work.
8
Published as a conference paper at ICLR 2022
The following is our analog of Theorem 4 by Su & Yang (2019) proved in Appendix F. This shows
that when the target function has a compact representation in terms of eigenfunctions of TK∞ , a
more moderate overparametrization is sufficient to approximately solve the ERM problem.
Theorem 3.8. Assume Assumptions 3.3 and 3.4 hold. Furthermore assume m =
Ω G-2dT2 kf *k∞ (1 + Tkf *k∞)2) where T > 0 is a time parameter and m ≥ O(log(c∕δ) 十
O(d)) and n ≥ 彳：-)：］；/?∙ Also assume f * ∈ L∞(X) ⊂ L2(X) and let PTK∞ be the or-
thogonal projection onto the eigenspaces of TK∞ corresponding to the eigenvalue α ∈ σ(TK∞ )
and higher. Assume that (I - PTK∞ )f* ∞ ≤ 0 for some 0 ≥ 0. Pick k so that σk = α and
σk+1 < α, i.e. k is the index of the last repeated eigenvalue corresponding to α in the ordered
Sequence {σi}i. Also assume we are performing the doubling trick so that r(0) = —y. Then we
have with probability at least 1 - 3δ over the sampling of x1 , . . . , xn and θ0 that for t ≤ T
VHr / XQ…	,4κ ∣∣f*∣∣2 Pl0log(2∕δ)
IIrtkRn ≤ exp(-λkt) IlyllRn +	(σfc - σfc+1)√n	+ 2e + 匚
SU & Yang (2019) have kf* |卜 ≤ kf*k∞ ≤ 1, K ≤ 2 and they treat d as a constant. Taking these
into account we do not see the overparameterization requirements or bounds of either work being
sUperior to the other. From Theorem 3.8, setting
4κ∣f *∣∣2 √10log(2∕δ)
(σk-σk+ι)√n
and 0 = 0 we immediately
get the analog of Corollary 2 in the work of SU & Yang (2019). This explains how in the special
case that the target fUnction is a finite sUm of eigenfUnctions of TK∞ , the width m and the nUmber
of samples n can grow at the same rate, Up to logarithms, and still solve the ERM problem. This is
an ERM guarantee for m = Ω(n) and thus attains moderate overparameterization.
Corollary 3.9. Assume Assumptions 3.3 and 3.4 hold. Furhtermore assume m =
Ω (n(σk-σk+1)2d2f**k⅛1+λ-1f*k∞)2) m ≥ O(log(c∕δ) + O(d)) n ≥ 124 * * * 8⅛≡. Let
κ2 kf* k22 λ2k	(σk -σk+1)2
f*, PTK∞, and k be the same as in the hypothesis of Theorem 3.8. Furthermore assume that
∣∣(I — PTK∞ )f ［∣	= 0. Also assume we are performing the doubling trick so that r(0) = y.
Set T = log(√n kr(0)kRn )∕λk. Then we have with probability at least 1 — 3δ over the sampling of
x1 , . . . , xn and θ0 that for t ≤ T
krt kRn
Y ( '八 U Il q8κ kf*k2 √10log(2∕δ)
≤ eXP(—λkt) kykRn +	(σk - σk+ι)√n
Note Su & Yang (2019) are training only the hidden layer of a ReLU network by gradient descent,
by contrast we are training both layers with biases ofa network with smooth activations by gradient
flow. For Corollary 2 by Su & Yang (2019) they have the overparameterization requirement m &
n log n (亲 + (log- n］：)(卷4). Thus both bounds scale like λ4.OUr bound has the extra factor
(σk — σk+1 )2 in front which could make it appear smaller at first glance but their Theorem 4 is
strong enough to include this factor in the corollary they just chose not to. Thus we view both
overparameterization requirements as comparable with neither superior to the other.
4 Conclusion and Future Directions
The damped deviations equation allows one to compare the dynamics when optimizing the squared
error to that of an arbitrary kernel regression. We showed how this simple equation can be used
to track the dynamics of the test residual in the underparameterized regime and extend existing re-
sults in the overparameterized setting. In the underparameterized setting the neural network learns
eigenfunctions of the integral operator TK∞ determined by the Neural Tangent Kernel at rates cor-
responding to their eigenvalues. In the overparameterized setting the damped deviations equation
combined with NTK deviation bounds allows one to track the dynamics of the empirical risk. In
this fashion we extended existing work to the setting of a network with smooth activations where all
parameters are trained as in practice. We hope damped deviations offers a simple interpretation of
the MSE dynamics and encourages others to compare against other kernels in future work.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Benjamin Bowman was at the Max Planck Institute for Mathematics in the Sciences while working
on parts of this project. This project has received funding from the European Research Council
(ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agree-
ment no 757983).
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 242-252. PMLR, 09-15 JUn 2019a. URL https://Proceedings.
mlr.press/v97/allen-zhu19a.html.
ZeyUan Allen-ZhU, YUanzhi Li, and Zhao Song. On the convergence rate of training recUrrent
neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volUme 32. CUrran Asso-
ciates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/
0ee8b85a85a49346fdff9665312a5cc4- Paper.pdf.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322-332.
PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/arora19a.
html.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
Frequency bias in neural networks for input of non-uniform density. In Hal Daume In and Aarti
Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 685-694. PMLR, 13-18 Jul 2020. URL
https://proceedings.mlr.press/v119/basri20a.html.
Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and
Statistics. Springer, Boston, MA, 2004.
Avrim L. Blum and Ronald L. Rivest. Training a 3-node neural network is NP-complete, pp. 9-28.
Springer Berlin Heidelberg, Berlin, Heidelberg, 1993. URL https://doi.org/10.1007/
3-540-56483-7_20.
Brian Bullins, Cyril Zhang, and Yi Zhang. Not-so-random features. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
Hk8XMWgRb.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning, 2020.
LenaIc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable program-
ming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche—Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ae614c557843b1df326cb29c57225459- Paper.pdf.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 1675-1685. PMLR, 09-15 Jun 2019a. URL
https://proceedings.mlr.press/v97/du19c.html.
10
Published as a conference paper at ICLR 2022
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Yonatan Dukler, QUanqUan Gu, and GUido Montufar. Optimization theory for ReLU neural networks
trained with normalization layers. In Hal DaUme In and Aarti Singh (eds.), Proceedings of the
37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
LearningResearch,pp. 2751-2760. PMLR, 13-18 Jul 2020. URL https://prOceedings.
mlr.press/v119/dukler20a.html.
Weinan E, Chao Ma, and Lei Wu. A comparative analysis of optimization and generalization prop-
erties of two-layer neural network and random feature models under gradient descent dynamics.
Sci. China Math. 63, 1235-1258, 2020.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
Nati Srebro. Implicit regularization in matrix factorization. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
58191d2a914c6dae66371c9dcdc91b41- Paper.pdf.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierar-
chy.In Hal Daume In and Aarti Singh (eds.), Proceedings ofthe 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4542-4551.
PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/huang20l.
html.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In Alina
Beygelzimer and Daniel Hsu (eds.), Proceedings of the Thirty-Second Conference on Learning
Theory, volume 99 of Proceedings of Machine Learning Research, pp. 1772-1798. PMLR, 25-28
Jun 2019. URL https://proceedings.mlr.press/v99/ji19a.html.
Hui Jin and Guido Montufar. Implicit bias of gradient descent for mean squared error regression
with wide neural networks, 2021. arXiv:2006.07356.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf.
Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward net-
works with a nonpolynomial activation function can approximate any function. Neural Networks,
6(6):861-867, 1993. URL https://www.sciencedirect.com/science/article/
pii/S0893608005801315.
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
15954-15964. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. In Yoshua Bengio and Yann LeCun (eds.), 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-
9, 2015, Workshop Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6614.
11
Published as a conference paper at ICLR 2022
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of opti-
mization and implicit regularization in deep learning, 2017. arXiv:1705.03071.
Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide
layer followed by pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Bal-
can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
11961-11972. Curran Associates, Inc., 2020. URL https://Proceedings.neurips.
cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 1(1):84-105, 2020.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees
for neural networks via harnessing the low-rank structure of the jacobian, 2020. URL https:
//openreview.net/forum?id=ryl5CJSFPS.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5301-5310. PMLR,
09-15 Jun 2019. URL https://proceedings.mlr.press/v97/rahaman19a.html.
Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. In 2008
46th Annual Allerton Conference on Communication, Control, and Computing, pp. 555-561,
2008a.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008b. URL https://proceedings.neurips.cc/
paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate
of neural networks for learned functions of different frequencies. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5ac8bb8a7d745102a978c5f8ccdb61b8- Paper.pdf.
Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal
of Machine Learning Research, 11(30):905-934, 2010. URL http://jmlr.org/papers/
v11/rosasco10a.html.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix Chernoff bound,
2020. arXiv:1906.03593.
Eduardo D. Sontag and Hector J. Sussmann. BackProPagation can give rise to spurious local minima
even for networks without hidden layers. Complex Systems, 3:91-106, 1989.
Eduardo D. Sontag and Hector J. Sussmann. Back propagation separates where perceptrons
do. Neural Networks, 4(2):243-249, 1991. URL https://www.sciencedirect.com/
science/article/pii/089360809190008S.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):
1-57, 2018. URL http://jmlr.org/papers/v19/18-188.html.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approx-
imation perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche—Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
253f7b5d921338af34da817c00f42753-Paper.pdf.
12
Published as a conference paper at ICLR 2022
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed
Sensing, chapter 5. Cambridge University Press, 2012.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,
2018.
Francis Williams, Matthew Trager, Daniele Panozzo, Claudio Silva, Denis Zorin, and
Joan Bruna. Gradient dynamics of shallow univariate relu networks. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1f6419b1cbe79c71410cb320fc094775-Paper.pdf.
Lei Wu, Zhanxing Zhu, and Weinan E. Towards understanding generalization of deep learning:
Perspective of loss landscapes. CoRR, abs/1706.10239, 2017. URL http://arxiv.org/
abs/1706.10239.
Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. In Tom Gedeon, Kok Wai Wong, and Minho Lee (eds.), Neural Information
Processing, pp. 264-274, Cham, 2019. Springer International Publishing.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error in-
duced by initialization in deep neural networks. In Jianfeng Lu and Rachel Ward (eds.), Pro-
ceedings of The First Mathematical and Scientific Machine Learning Conference, volume 107
of Proceedings of Machine Learning Research, pp. 144-164. PMLR, 20-24 Jul 2020. URL
https://proceedings.mlr.press/v107/zhang20a.html.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neu-
ral networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
6a61d423d02a1c56250dc23ae7ff12f3- Paper.pdf.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine learning, 109:467-492, 2020.
Appendix
A Additional Notations
We let [k] := {1,2, 3,..., k}. For a set A We let |A| denote its cardinality. |同恨 denotes the
Frobenius norm for matrices, and for two matrices A, B ∈ Rn×m we will let hA, Bi = Tr(ATB) =
Pin=1 Pjm=1 Ai,j Bi,j denote the Frobenius or entry-wise inner product. We will let BR := {x :
kxk2 ≤ R} to be the Euclidean ball of radius R > 0.
B NTK Deviation and Parameter Norm Bounds
Let Γ > 1. At the end of this section we will prove a high probability bound of the form
.. ._ ,	... ~
sup	∣Kt(x,x0) — K ∞(x,x0)∣ = O
(x,x0)∈BM ×BM
[1 + tr3 kr(0)kRn ]
13
Published as a conference paper at ICLR 2022
Ideally we would like to use the results in Huang & Yau (2020) where they prove for a deep feed-
forward network without biases:
sup ∣Kt(Xi,Xj ) — K∞(Xi,Xj)| = O (—+—^).
1≤i,j≤n	m m
However there are three problems that prevent this. The first is that the have a constant under the O
above that depends on the training data. Specifically their Assumption 2.2 requires that the smallest
singular value of the data matrix [xα1 , . . . , xαr] is greater than cr > 0 where 1 ≤ α1, . . . , αr ≤ n
are arbitrary distinct indices. As you send the number of samples to infinity you will have cr → 0,
thus itis not clear how the bound will scale in the large sample regime. The second is that their bound
only holds on the training data, whereas we need a bound that is uniform over all inputs. The final
one is their network does not have biases. In the following section we will overcome these issues.
The main difference between our argument and theirs is how we prove convergence at initialization.
In their argument for convergence at initialization they make repeated use of a Gaussian conditioning
lemma as they pass through the layers, and this relies on their Assumption 2.2. By contrast we will
use Lipschitzness of the NTK and convergence over an net to prove convergence at initialization.
As we see it, our deviation bounds for the time derivative ∂tKt are proved in a very similar fashion
and the rest of the argument is very much inspired by their approach.
At the time of submission of this manuscript, we were made aware of the work by Liu et al. (2020)
that provides an alternative uniform NTK deviation bound by providing a uniform bound on the
operator norm of the Hessian. Their work is very nice, and it opens the door to extending the results
of this paper to the other architectures they consider. Nevertheless, we proceed with our original
analysis below.
This section is conceptually simple but technical. We will take care to outline the high level structure
of each section to prevent the technicalities from obfuscating the overall simplicity of the approach.
Our argument runs through the following steps:
•	Control parameter norms throughout training.
•	Bound the Lipschitz constant of the NT K with respect to spatial inputs.
•	Use concentration of subexponential random variables (Bernstein’s inequality) to show that
|K∞(z0) - Ko(z0)| = O(1∕√m) (roughly) for all z0 in an E net of the spatial domain.
Combine with the Lipschitz property of the NT K to show convergence over all inputs,
namely suPz∈bm |K∞(z) - Ko(z)∣ = <O(1∕√m) (roughly).
iʌ -l	.1 1	1	ICT7∕∖l	K ∕r∕ I ∖ /	11、
•	Produce the bound suPz∈bm ×bm ∣∂tKt(z)∣ = O(1∕√m) (roughly).
•	Conclude that suPz∈bm×bm Kt(z) - Ko(z)∣ = O(t∕√m) (roughly).
B.1 Important Equations
The following list contains the equations that are relevant for this section. We found it easier to read
the following proofs by keeping these equations on a separate piece of paper or in a separate tab.
T
YVe Wiite a Qv X — ax . .^aiso IhIOUghOUt IhiS SeCtiOn the Iraining data will be COnSidered fixed and
thus the randomness of the inputs is not relevant to this section. The randomness will come entirely
from the parameter initialization θ0 .
f (x; θ) = LaTσ(Wx + b) + bo
x(1) := √= σ(Wx + b)
σ10 (x) := diag(σ0(W x + b))
σ100(x) := diag(σ00 (W x + b))
∂af(x; θ) = -^=σ(Wx + b) = X⑴
m
∂wf (x; θ) = -j=σ1 (x)a V x
14
Published as a conference paper at ICLR 2022
∂bf(χ; θ) = -j=σ1 (x)a
m
∂b0 f (X; θ) = 1
n
- n X rix(1)
i=1
1n 1	0
∂tW =——Eri √mσι (Xi )a 0 Xi
i=1
1n 1	0
dtb =-->I/ √m O] (Xi) O
i=1
∂tb0
1n
--X ri
n
i=1
∂tx(1) = ∂t —J=σ(Wx + b) = J=σj (x)[∂tWx + ∂tb]
1n
一 X ri
n i=1
[hx,xii2 + 1]
∂tσ10 (x) = ∂tσ0(W x + b) = σ100 (x)diag (∂t W x + ∂tb)
1n 1
=一一Eri√mσι (x)σ1(xi)diag(a)[hx,xii2 + 1]
i=1
B.2 A Priori Parameter Norm B ounds
In this section we will provide bounds for the following quantities:
so=max{√m kw (t)kop, √m kb(t)k2, √m ka(t)k2, 1}
~， 、 ，	.. ， 、 ，， ，，，、，， ，，.，、，， .、
ξ(t) = maχ{max kw'(t)k2 , ka(t)k∞ , kb(t)k∞ , 1}.
'∈[m]
Here w` = W',： ∈ Rd is the vector of input weights to the 'th unit. These quantities appear
repeatedly throughout the rest of the proofs of this section and thus need to be controlled. The
parameter norm bounds will also be useful for the purpose of the covering number argument in
Section C.6. This section is broken down as follows:
•	Prove Lemma B.1
•	Bound ξ(t)
•	Bound ξ(t)
The time derivatives throughout will repeatedly be of the form n Pn=1 ri (t)vi. Lemma B.1 provides
a simple bound that we will use over and over again.
Lemma B.1. Let k∙k be any norm overa vector space V. Thenforany vι,...,vn, ∈ V we have
1n
-ɪ^ri(t)vi
n i=1
≤ max kvik kr(t)kRn ≤ max Ilvik kr(0)kRn .
i∈[n]	i∈[n]
Proof. Note that
1n
n X ri(t)vi
i=1
nn
≤ n X |ri(t)| kvik ≤ max kvik n X |ri(t)|
i=1	i=1
≤ max	Ilvik √	Ilr(t)k2 = max	Ilvik	kr(t)kRn	≤ max Ilvik	kr(0)kRn	,
i∈[n]	n	i∈[n]	i∈[n]
where the last inequality follows from kr(t)kRn ≤ kr(0)kRn from gradient flow.
□
15
Published as a conference paper at ICLR 2022
We now proceed to bound ξ(t).
LemmaB2. Let ξ(t) =max{√1m IIW(t)kop , √1m ∣∣b(t)k2 , √1m ∣∣a(t)k2 , 1} and
D = 3maχ{Q(O儿 M ∣σ0k∞ , ∣σ0k∞ , 1}.
Then for any initial conditions W (0), b(0), a(0) we have for all t
ξ(t) ≤ eχp
kr(S)kRn ds
ξ(0) ≤ eχp
∣r(0)∣Rn t) ξ(0).
Proof. Recall that
n
-n X rixi1)
i=1
1n 1
∂tW =----Eri√mσι(Xi)a 0 Xi
i=1
1n 1
帅=-n∑ri √mσι(Xi)a.
i=1
We will show that each of the above derivatives is . ∣∣r(t) kRn ξ(t) then apply GronWall's inequality.
By Lemma B.1 it suffices to show that the terms multiplied by Iri in the above sums are . ξ(t). First
we note that
卜(1) IL= √mσ(Wxi + b) 2 ≤ ∣σ(0)∣ + -ɪ= ∣σ0k∞ ∣Wxi + b∣2
≤ ∣σ(0)∣ + √m kσ0k∞ h∣Wkop ∣Xi∣2 + kbk2i ≤ ∣σ(0)∣ + √m ∣σ0k∞ [∣Wkop M + 悯卜]
≤ Dξ.
Second we have that
Finally we have that
op
IIXik2 ≤ M ∣∣σ0k∞ √m ka∣2 ≤ Dξ.
≤kσ0k∞ √m kak2 ≤ Dξ.
Thus by Lemma B.1 and the above bounds we have
k∂tW(t)kop, k∂ta(t)k2, k∂tb(t)k2 ≤ D kr(t)kRηξ(t).
Let v(t) be a placeholder for one of the functions √ma(t), √mW(t), √mb(t) with corresponding
norm k∙k. Then we have that
kv(t)k ≤ kv(0)k + kv(t) - v(0)k = kv(0)k +
∂s v(s)ds
0
≤ kV(0)k + O. kdsv(s)k ds ≤ ξ(0)+ Z ɪDξ(s)ds.
This inequality holds for any of the three choices of v thus we get that
ξ(t) ≤ ξ(0) +Zt
0
Μ‰ Dξ(s)ds.
Therefore by GrOnWall's inequality we get that
ξ(t) ≤ eχp
忻⑶岛ds
ξ(0) ≤ exp (√m kr(0)kRn t) ξ(0).
□
16
Published as a conference paper at ICLR 2022
…	…	1	I :，、	.	..„	..	...	.
We will now bound ξ(t) using essentially the same argument as in the previous lemma.
LemmaB.3. Let ξ(t) = max{max'∈m] ∣∣w'(t)k2, ∣∣a(t)k∞ , kb(t)k∞ ,1} and
D = 3max{∣σ(0)∣,M ∣∣σ0k∞ , ∣∣σ0k∞ ,1}.
Then for any initial conditions W (0), b(0), a(0) we have for all t
ξ(t) ≤ exp
ξ(0) ≤ exp (√m ∣∣r(0)∣∣Rn t) ξ(0).
Proof. The proof is basically the same as Lemma B.2. We have that
∂tW'
aL σ0(hw',xii2 + b` )xi.
Now note
1n
一 X ri
n
i=1
2
Thus by Lemma B.1 we have that
On the other hand
D
Ildtw'⑴∣∣2 ≤ √m Ilr⑴IlRnξ⑴.
1n
dta = - n X rixi1)
i=1
with
xi(1)
之σ(Wxi+b)
≤√m [∣σ(0)∣ + kσ0k∞ IWxi + bk∞]
∣σ(0)∣ + kσ0k∞ (Mmax 忖乩 + ∣b∣∞)
Thus again by Lemma B.1 we have
D
k∂ta(t)k∞ ≤√m kr(t)∣Rn ξ(t).
Finally we have
∂tb
1n
- - X ri
n
i=1
σ10 (xi)a
with
Again applying Lemma B.1 one last time we get
D
k∂tb(t)k∞ ≤√m kr(t)IIRn ξ(t).
Therefore by the same argument as in Lemma B.2 using GronWall's inequality We get that
ξ(t) ≤ exp
kr(S)kRn ds
~，八
ξ(0) ≤ exp
kr(0)kRn t f(0)∙
□
17
Published as a conference paper at ICLR 2022
B.3 NTK IS LIPSCHITZ WITH RESPECT TO S PATIAL INPUTS
The NTK being Lipschitz with respect to spatial inputs is essential to our proof. The Lipschitz
property means that to show convergence uniformly for all inputs it suffices to show convergence
on an net of the spatial domain. Since the parameters are changing throughout time, the Lipschitz
constant of the NTK will change throughout time. We will see that the Lipschitz constant depends
on the quantities ξ(t) and ξ(t) from the previous Section B.2.
The NTK Kt(x, x0) is a sum of terms of the form g(x)T g(x0) where g is one of the derivatives
∂af(x; θt), ∂bf(x; θt), ∂Wf(x; θt), ∂b0 f (x; θt). Since ∂b0 f (x; θt) ≡ 1 this term can be ignored for
the rest of the section. The upcomming Lemma B.4 shows that if g is Lipschitz and bounded then
(x, x0) 7→ g(x)T g(x0) is Lipschitz. This lemma guides the structure of this section:
•	Prove Lemma B.4
•	Show that ∂af(x; θ), ∂bf(x; θ), ∂Wf(x; θ) are bounded and Lipschitz
•	Conclude the NT K is Lipschitz
Lemma B.4. Let g : Rk → Rl be L-Lipschitz with respect to the 2-norm, i.e.
kg(x) -g(z)k2 ≤ Lkx-zk2
and satisfy kg(x)k2 ≤ M for all x in some set X. Then Kg : X × X → R
Kg(x, x0) := g(x)T g(x0)
is M L-Lipschitz with respect to the norm
k(x,x0)k := kxk2 + kx0k2.
Proof. We have
|Kg(x,x0) - Kg(z, z0)| = |g(x)T g(x0) - g(z)T g(z0)|
= |g(x)T (g(x0) -g(z0))| + |(g(x) - g(z))T g(z0)|
≤ kg(x)k2 kg(x0) - g(z0)k2 + kg(x) - g(z)k2 kg(z0)k2
≤ ML kx0 - z0k2 + ML kx - zk2 ≤ ML k(x, x0) - (z, z0)k .
□
By the previous Lemma B.4, to show that the NT K is Lipschitz it suffices to show that
∂af(x; θ), ∂bf(x; θ), ∂Wf(x; θ), ∂b0 f (x; θ) are bounded and Lipschitz. The following lemma
bounds the norms of the derivatives ∂af(x; θ), ∂Wf(x; θ), ∂bf(x; θ).
Lemma B.5. Let D = 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0k∞ , 1} and
ξ = max{√m kW kop, √m kb∣2, √m ιιak2,1}.
Then
k∂af (x; θ)k2 , k∂W f (x; θ)kF , k∂bf(x; θ)k2 ≤ Dξ.
Proof. We have
kdaf (x； θ)k2 = √1mσ(Wx + b)	≤ lσ(O)I + √m kσ'k∞ kWx + bk2
≤∣σ(0)∣ + √m kσ0k∞ h∣Wkop kx∣2 + |叽］
≤ ∣σ(0)∣ + √m kσ0k∞ h∣Wkop M + kbk2i ≤ Dξ,
k∂W f (x; θ)kF
10
尸σι(x)a 0 x
=√mσi(X)a	kxk2 ≤ √m ι∣σ0k∞ kak2 ≤ Dξ,
18
Published as a conference paper at ICLR 2022
Mbf(X；θ)k2 = √1mσi(X)a	≤ √1m kσ0k∞ kak2 ≤ Dξ.
□
The following lemma demonstrates that ∂af(x; θ), ∂Wf(x; θ), and ∂bf(x; θ) are Lipschitz as func-
tions of the input X.
Lemma B.6"“	ξ = max{√1m kW kop , √1m kbk2 , √1m 1同卜 U
N	Γ	Il Il Il Il Illll -1 T
ξ= max{max kw`k2, kak∞, kbk∞, 1},
'∈[m]
D0 = max{kσ0k∞, M kσ00k∞, kσ00k∞},
L = 2ξξD0.
Then ∂af (x; θ), ∂bf(x; θ), ∂w f (x; θ) are all L-Lipschitz with respect to the Euclidean norm k∙∣∣2∙
In symbols:
k∂af(x; θ) — ∂af (y； θ)k2 ≤ L kX -y∣∣2
k∂wf (x； θ) - ∂wf (y; θ)kF ≤ L kx -yk2
k∂bf (x; θ) - ∂bf (y; θ)k2 ≤ Lkx -yk2 .
Proof. We have
k∂af (x； θ) - ∂af(y; θ)k2 = √m(σ(Wx + b) - σ(Wy + b)) ?
≤ √1m kσ0k∞ kW(x — y)k2 ≤ √1m kσ0k∞ kWkop kx - yk2 ≤ L kx — yk2 ,
k∂wf (x; θ) - ∂wf (y; θ)kF
T=σ1 (x)a 0 X------J=σ1 (y)a 0 y
F
≤
二σ1 (x)a 0 [x - y]
√1m [σ1(X)a-σ1(y)a] 0 y
+
F
F
≤ √1m kσ1 (x)ak2 kx - yk2 + √1m ll[σ1(X) - σ1 (y)]ak2 kyk2
≤ √1m kσ0k∞ kak2 kx - yk2 + √1m kσ0(Wx + b) - σ0(Wy + b)k∞ kak2 M
≤ √1m kσ0k∞ kak2 kx - yk2 + √1m kσ00k∞ kW(x - y)k∞ kak2 M
≤ √1m kσ0k∞ kak2 kx - yk2 + √1m l∣σ00k∞max kw'k2 kx - yk2 kak2M
≤ Lkx-yk2,
k∂bf (x; θ) - ∂bf (y; θ)k2
产 σ1(X)a —~j=σ (y)a
2
≤ √1m kσ0(Wx + b) - σ0(Wy + b)k∞ kak2
≤√1m kσ00k∞ kW (x - y)k∞ kak2
≤ ιη= kσ00k∞ max kw'k2 kx - yk2 kak2 ≤ L kx - yk2.
ʌ/m	'∈[m]
□
19
Published as a conference paper at ICLR 2022
Finally we can prove that the Neural Tangent Kernel is Lipschitz.
Theorem B.7.	Consider the Neural Tangent Kernel
K(x, y) = h∂af (x; θ),∂af (y; θ)i2 + h∂bf (x; θ),∂bf (y; θ)i2 + h∂wf (x; θ), ∂wf (y; θ)i + 1
ξ = max{√m kWkop , √m kbk2 , √m kak2 , 1},
E	r	H H H H mu -i ɔ
ξ= max{max kw`k2, kak∞, kbk∞, 1},
'∈[m]
D = 3max{lσ(0)1, M kσ0k∞, kσ0k∞, 1},
D0 = max{kσ0k∞, M kσ00k∞, kσ00k∞}.
Then the Neural Tangent Kernel is Lipschitz with respect to the norm
k(x, y)k := kxk2 + kyk2
• .1 τ ∙	1 ∙.	. . τ	CrArλ / >=9 1 τ	ι ι
with Lipschitz constant L := 6DD0ξ2ξ. In symbols:
|K (x, y) - K(x0, y0)| ≤ Lk(x,y) - (x0, y0)k .
Proof. By Lemma B.5, we have that the gradients are bounded
k∂af (X； θ)k2 , k∂wf(X； θ)kF , k∂bf(X； θ)k2 ≤ Dξ.
Also by Lemma B.6 the gradients are Lipschitz with Lipschitz constant 2ξξD0. Thus these
two facts combined with Lemma B.4 tell us that each of the three terms h∂af(X; θ), ∂af (y; θ)i,
h∂bf(x; θ), ∂bf(y; θ)i, and <∂wf (x; θ),∂wf (y; θ)i are individually Lipschitz with constant (Dξ) ∙
(2ξξD0). Thus the Lipschitz constant of the NT K itself is bounded by the sum of the 3 Lipschitz
constants, for a total of 6DD0ξ2Q.	□
Using that the NTK at time zero K0(x, y) is Lipschitz we can prove that the analytical NTK
K∞ = E[K0 (x, y)] is Lipschitz. We will use this primarily as a qualitative statement, meaning
that the estimate that we derive for the Lipschitz constant will not be used as it is not very explicit.
Rather, in theorems where we use the fact that K∞ is Lipschitz we will simply take the Lipschitz
constant of K∞ as an external parameter.
Theorem B.8.	Assume that Wij(0)〜W, b`(θ)〜B, a`(θ)〜A are all i.i.d. zero-mean, unit
variance subgaussian random variables. Let
ξ(0) = max{√m kW(0)kop , √m kb(0)k2 , √m ka(0)k2 , 1},
~ , . ,	.. , , .. ...... .. ,... 、
ξ(0) = maχ{max kw'(0)k2 , ka(0)k∞ , kb(0)k∞ , 1},
'∈[m]
D = 3max{∣σ(0)∣,M kσ0k∞ , ∣∣σ0k∞ , 1},
D0 = max{kσ0k∞, M kσ00k∞, kσ00k∞}.
Then the analytical Neural Tangent Kernel K∞(x, y) = E[K0(x, y)] is Lipschitz with respect to the
norm
k(x, y)k := kxk2 + kyk2
with Lipschitz constant ≤ 6DD0E[ξ2f] < ∞. If one instead does the doubling trick then the same
conclusion holds.
Proof. First assume we are not doing the doubling trick. We note that
IK∞(χ,y) - K∞(χ0,y0)1 = |E[Ko(x,y)] -E[κo(χ0,y0)]1
≤ ElKo(x,y) -K0(x0,y0)1 ≤ 6DD0E[ξ2ξ] k(x,y) -(X0,y0)k
where the last line follows from the Lipschitzness of K0 provided by Theorem B.7. Using that
kW (0)kop ≤ kW (0)kF and the fact that the Euclidean norm of a vector with i.i.d. subgaussian
entries is subgaussian (Vershynin, 2018, Theorem 3.1.1), we have that ξ(0) and ξ(0) are maximums
of subgaussian random variables. Since a maximum of subgaussian random variables is subgaus-
sian, We have that ξ(0) and f(0) are SUbgaUSsian. From the inequality ab ≤ 2 (a2 + b2) We get
E[ξ2ξ] ≤ 1 E[ξ4] + 2E[ξ2] < ∞ since moments of subgaussian random variables are all finite.
Since the doubling trick does not change the distribution of K0, the same conclusion holds under
that initialization scheme.	□
20
Published as a conference paper at ICLR 2022
B.4 NTK CONVERGENCE AT INITIALIZATION
In this section We prove that suPz∈Bm×Bm ∣Ko(z) - K∞(z)∣ = O(1∕√m). Our argument traces
the following steps:
•	ShoW that K0 is sum of averages ofm independent subexponential random variables
•	Use subexponential concentration to show that suPz0∈∆ ∣K0(z0) - K∞(z )| = O(1/√m)
for all z0 in an net ∆ of BM × BM
•	Use that K0 is Lipschitz and convergence over the epsilon net ∆ to show that
sup	∣K0(z) - K∞(z)∣ = O(1∕√m) (roughly)
z∈BM ×BM
We recall the following definitions 2.5.6 and 2.7.5 from Vershynin (2018).
Definition B.9. (Vershynin 2018) Let Y be a random variable. Then we define the subgaussian
norm of Y to be
kY kψ2 = inf{t > 0 : E exp(Y 2∕t2) ≤ 2}
If kY kψ < ∞, then we say Y is subgaussian.
Definition B.10. (Vershynin 2018) Let Y be a random variable. Then we define the subexponential
norm of Y to be
kY kψι =inf {t> 0: E exp(∣Y∣∕t) ≤ 2}
If kY kψ < ∞, then we say Y is subexponential.
We also recall the following useful lemma Vershynin (2018, Lemma 2.7.7).
Lemma B.11. (Vershynin 2018) Let X and Y be subgaussian random variables. Then XY is
subexponential. Moreover
kXYkψ1 ≤ kXkψ2 kYkψ2
We recall one last definition Vershynin (2018, Definition 3.4.1)
Definition B.12. (Vershynin 2018) A random vector Y ∈ Rk is called subgaussian if the one dimen-
sional marginals hY, xi are subgaussian random variables for all x ∈ Rk. The subgaussian norm
of Y is defined as
kY kψ2 = sup khY,xikψ2
The typical example of a subgaussian random vector is a random vector with independent subgaus-
sian coordinates. The following lemma demonstrates that the NT K at initializeation is a sum of
terms that are averages of independent subexponential random variables, which will enable us to use
concentration arguments later.
Theorem B.13. Let w`, b`, a` all be independent subgaussian random variables with subgaussian
norms satisfying ∣∣∙kψ2 ≤ K. Furthermore assume ∣∣1kψ2 ≤ K. Also let
D = 3max{∣σ(0)∣,M ∣σ0∣∞ , ∣σ0∣∞ ,1}.
Then for fixed x, y, each of the following
h∂af (x; θ),∂af (y; θ)i, h∂bf(x; θ),∂bf (y; θ)i, h∂wf (x; θ),∂wf (y; θ)
is an average of m independent subexponential random variables with subexponential norms
bounded by D2K2.
Proof. We first observe that
h∂af(x; θ),∂af (y; θ)i2 = ɪhσ(Wx + b),σ(Wy + b%
m
1m
=一 kσ(hw',x>2 + b')σ(hw',y>2 + b`),
m
'=1
21
Published as a conference paper at ICLR 2022
h∂bf (x; θ), ∂bf(y; θ)i2
σ10 (y)ai2
1m
—y^a2σ0(hw',xi2 + b')σ0(hw',yi2 + b`),
m
'=1
h∂wf (x; θ),∂wf(y; θ)i = h —1= σ1 (x)a 0 x, —1= σj(y)a 0 y)2
mm
1 0	0	hx, yi2 m 2 0	0
一hσι(x)a,σι(y)ai2hx,yi2 = -Ta2σ (<w',x>2 + b')σ (〈w',y〉2 + b`).
mm
'=1
Note that
lσ(hw',χi2+ b')| ≤ lσ(O)I + l∣σ0k∞ [lhw',χil + |b'|].
Thus
kσ(hw',xi2 + b')kψ2 ≤ ∣σ(0)∣∣1kψ2 + ∣σ0k∞ [∣∣hw',xi∣kψ2 + k∣b'∣kψ2]
≤∣σ(0)∣k1kψ2 + kσ0k∞ [M kw'kψ2 + 的限]
≤ 3maχ{∣σ(0儿 M ∣∣σ0k∞, kσ0k∞}κ ≤ DK.
Also
∣a'σ0(<w',x>2 + b')| ≤ ∣a'∣∣∣σ0k∞ ≤ D∣a'∣,
therefore
lla'σ0(hw',xi2 + b')kψ2 ≤ D ka'kψ2 ≤ DK.
Finally
Whx,yi2l1%'σ0(hw',χi2 + b')∣∣ψ2 ≤ M ∣∣σ0k∞ ∣∣a'kψ2 ≤ DK.
It follows by Lemma B.11 that each of h∂af(x; θ), ∂af(y; θ)i, h∂W f (x; θ), ∂Wf(y; θ)i, and
h∂b f (x; θ), ∂bf(y; θ)i is an average of m independent subexponential random variables with subex-
ponential norm k∙kψι ≤ D2K2.	□
We now recall the following Theorem from Vershynin (2012, Theorem 5.39) which will be useful.
Lemma B.14 (Vershynin 2012). LetA be an N × n matrix whose rows Ai are independent subgaus-
sian isotropic random vectors in Rn. Then for every t ≥ 0, with probability at least 1- 2 eχp(-ct2 )
one has the following bounds on the singular values
√N — C√n — t ≤ Smin(A) ≤ Smax(A) ≤ √N + C√n + t.
Here C = CK > 0 depends only on the subgaussian norms K = maχi ∣Ai∣ψ of the rows.
Also the following special case of Vershynin (2012, Lemma 5.5) will be useful for us.
Lemma B.15 (Vershynin 2012). Let Y be subgaussian. Then
P(IYI >t) ≤ Ceχp(-ct2/∣Ykψ2).
It will be useful to remind the reader that C, c > 0 denote absolute constants whose meaning will
vary from statement-to-statement, as this abuse of notation becomes especially prevalent during the
concentration of measure arguments of the rest of the section. The following lemma provides a
concentration inequality for the maximum of subgaussian random variables which will be useful for
bounding ξ and ξ later which is necessary for bounding the Lipschitz constant of K0 .
Lemma B.16. LetY1, . . . , Yn be subgaussian random variables with ∣Yi∣ψ ≤ K fori ∈ [n]. Then
there exists absolute constants c, c0 , C > 0 such that
P (max ∣Yi∣ > t + KPc0 logn ) ≤ Cexp(-ct2∕K2).
i∈[n]
22
Published as a conference paper at ICLR 2022
Proof. Since each Yi is subgaussian we have for any t ≥ 0 (Lemma B.15)
P(|Yi| >t) ≤ C exp -ct2/ kYik2ψ2 .
By the union bound,
P I max |Yi| > t + K/c-1 logn
i∈[n]
n
≤ X P (M >t + KPc-1 log n)
≤ nC exp
t + K∖∕c-1 log n
C exp(-ct2 /K 2).
Thus by setting c0 := c-1 we get the desired result.
□
We now introduce a high probability bound for ξ .
Lemma B.17. Assume that Wij 〜 W, b` 〜 B, a` 〜 A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume kw` kψ , ka` kψ , kb` kψ ≤ K for each
` ∈ [m] where K ≥ 1. Let
ξ = max{√m kWkop，√m kbk2 , √m kak2}
Then with probability at least 1 - δ
ξ≤1+C
√d+K2 ,iog(c∕δ)
√m
Proof. Note that by setting t = ,c-1 log(2∕δ) in Lemma B.14 We have that with probability at
least 1 - δ
1m kWkop ≤ 1 +
C√d + VZcTlog(2∕δ)
√m
Also by Theorem 3.1.1 in (Vershynin, 2018)
Ilkak2 -√mllψ2 ≤ ck2
Ilkbk2-√⅛ ≤ CK2
Thus by Lemma B.15 and a union bound we have with probability at least 1 - 2δ
11	CC ,-------
~a= kak2 , ~i= kbk2 ≤ 1 + ~∕=K PlOg(C0
Thus by replacing every δ in the above arguments with δ∕3 and using the union bound we have with
probability at least 1 - δ
ξ≤1+C
√d + K 2plog(c∕δ)
√m
n ∙	∙1	1	∙	.	1	< ∙	<	FF ∙1 ∙ . F	1 Γ∙ 二
Similarly we now introduce a high probability bound for ξ.
Lemma B.18. Assume that Wij 〜 W, b` 〜 B, a` 〜 A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume kw` kψ , ka` kψ , kb` kψ ≤ K for each
` ∈ [m] where K ≥ 1. Let
E	r n n n n
ξ = max{max kw`k2 , kak∞
'∈[m]
kbk∞}
Then with probability at least 1 - δ we have
ξξ ≤ √d + CK2 [plog(c∕δ) + plog m]
□
23
Published as a conference paper at ICLR 2022
Proof. By Theorem 3.1.1 in (Vershynin, 2018) we have
pw`k2-矶2 ≤ CK2
Well then by Lemma B.16 there is a constant c0 > 0 so that
P [ max ∣∣W'k2 — √d > t + CK2 pC log m )
v∈[m]	)
≤ P max I ∣w'∣2 — √d∣ > t + CK2 pC log m) ≤ Cexp(-ct2/K4).
Thus by setting t = CK2 ,log(c∕δ) We have with probability at least 1 — δ
max ∣∣W'∣2 ≤ √d + CK2 plog(c∕δ) + CK2plog m
'∈[m]
where we have absorbed the constant √c7 into C. Similarly by Lemma B.16 and a union bound we
get with probability at least 1 — 2δ that
l∣ak∞, kbk∞ ≤ ckPlOg(C0 + Ckpom
Thus by replacing each δ with δ∕3 in the above arguments and using the union bound we get with
probability at least 1 — δ
W ≤ √d + CK2 [plog(c∕δ) + plog m]
□
We are now finally ready to prove the main theorem of this section.
Theorem B.19. Assume that Wi,j 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume ∣∣w'∣ψ2 , ∣a'∣ψ2 , ∣b'kψ2 ≤ K for each
` ∈ [m] where K ≥ 1. Let
D = 3maχ{1σ(0儿 M ∣∣σ0k∞ , kσ0k∞ , 1},
D0 = max{∣σ0∣∞, M ∣σ00∣∞, ∣σ00∣∞}.
Define
ρ(M, σ, d, K, δ, m) :=
CDD0(1 + C√d + K√mogwδy)2n√d + CK2 [p∣o≡δ) + pɪog^io.
Let L(K∞) denote the Lipschitz constant ofK∞. If
m ≥ C[log(c∕δ) + 2dlog(CMmax{ρ, L(K∞)}√m)],
then with probability at least 1 — δ
sup	∣Ko(z) — K∞(z)∣≤ ɪ 1 + CD2K2λ/log(c∕δ) + 2dlog(CMmax{ρ, L(K∞)}√m)
z∈BM ×BM	m
If one instead does the doubling trick then the same conclusion holds.
Proof. First assume we are not doing the doubling trick. Recall that by Theorem B.7 that K0 is
Lipschitz with constant at most
CDDoξ(0)2ξ(0),
1	ʌ 1 二	Λ r- Λ ∙ .Λ .Λ	I-V T 11 ,IFT	1 ʌ <r 1 ʌ <C Λ	♦ F	Λ
where ξ and ξ are defined as in the theorem. Well then by Lemmas B.17, B.18 and a union bound
we have with probability at least 1 — 2δ
ξ(0)2 ξ(0) ≤ (l + C √d + κ√mlog(c∕δ) ) n√d + CK2 h√log(c∕δ) + Plog mio .
24
Published as a conference paper at ICLR 2022
Let L(K0), L(K∞ ) denote the Lipschitz constant of K0 and K∞ respectively. Then assuming the
above inequality holds we have that
L(K0) ≤ ρ(M, σ, d, K, δ, m).	(1)
For conciseness from now on we will suppress the arguments of ρ. Now set
1
Y :=--------------———.
2 max{ρ, L(K∞)}ʌ/m
LetNγ(BM) be the cardinality of a maximal γ-net of the ball BM = {x : kxk2 ≤M} with respect
to the L2 norm ∣∣∙k2. By a standard volume argument We have that
NY(BM) ≤ (CM[
By taking the product of two γ∕2 nets of BM it follows that we can choose a Y net of BM X BM,
say ∆, With respect to the norm
k(x, y)k = kxk2 + kyk2
such that
∣∆∣ ≤ M∕2(Bm)∣2 ≤ (CMY = MY.
By Theorem B.13 for (x, y) ∈ BM × BM fixed each of the following
h∂af (x; θ), ∂af (y; θ)i2, h∂bf (x; θ), ∂bf (y; θ)i2, h∂W f (x; θ), ∂W f (y; θ)i
is an average of m subexponential random variables with subexponential norm at most D2K2 .
Therefore separately from the randomness discussed before by Bernstein’s inequality Vershynin
(2018, Theorem 2.8.1) and a union bound we have
P(|K0(x, y) - K∞(x, y)| > t) ≤ 3 × 2exp
-c min
mt2	mt
D4K4 ,D2K2
Thus for t ≤ D2 K2 we have
mt2
P(∣K0(x,y) — K (x,y)l >t) ≤ 6exp -c 4 4
D4K4
Then by a union bound and the previous inequality we have that for t ≤ D2K2
P (ma∆ 1K0 (ZO)- K ∞(ZO)I >t) ≤ 6Mγexp (-cD⅛:
Thus by setting t = CD2K2 VZlog(C√+logMY (note that the condition on m in the hypothesis
ensures that t ≤ D2K2) we get that with probability 1 - δ
max |K0(ZO) - K∞ (ZO)| ≤ t.
Now fix z ∈ BM × BM and choose z0 ∈ ∆ such that ∣z - z0 ∣ ≤ Y. Then
∣Ko(z) - K∞(z)∣≤∣Ko(z)- K0(z0)∣
+ |Ko(z0) - K∞(z0)∣ + |K∞(z0) - K∞(z)∣
≤ 2max{L(K0),L(K∞)}Y+t.	(2)
Note that this argument runs through for any Z ∈ BM × BM therefore
sup	∣K0(z) — K∞(z)∣ ≤ 2max{L(Ko), L(K∞)}γ +1.
z∈BM ×BM
25
Published as a conference paper at ICLR 2022
Well by replacing δ with δ∕3 in the previous arguments by taking a union bound We can assume that
equations (1) and (2) hold simultaneously. In which case
sup	∣Ko(z) — K∞(z)∣ ≤ 2max{L(Ko), L(K∞)}γ + t ≤ 2max{ρ,L(K∞)}γ + t
z∈BM ×BM
≤ ɪ + cD2K2 Plog(C0+ log MY = ɪ + cd2K2 Plog(C6)+ 2dIog(CMyYy
一 √m	√m	√m	√m
= √= 1 + CD2K2 Jlog(C/δ) + 2dTog(CMmax{ρ, L(K∞)}√m),
where we have used the definition of Mγ in the second-to-last equality and the definition ofγ in the
last equality. Since the doubling trick does not change the distribution of K0 , the same conclusion
holds under that initialization scheme.	□
We immediately get the following corollary.
Corollary B.20. Assume that Wij 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume kw` kψ , ka` kψ , kb` kψ ≤ K for each
` ∈ [m] where K ≥ 1. Then
- . . ~ ,.-
m ≥ C[log(c∕δ) + O(d)]
suffices to ensure that with probability at least 1 — δ
sup	|Ko (z) — K ∞(z)∣ = O
z∈BM ×BM
If one instead does the doubling trick then the same conclusion holds.
B.5 Control of Network At Initialization
Many of our previous results depend on the quantity ∣∣r(0)∣∣Rn which depends on the network at
initialization. Before we proceed we must control the infinity norm of the network at initialization
and work out a few consequences of this. The following lemma controls kf (•; θ0)k∞.
Lemma B.21. Assume that Wi,j 〜 W, b` 〜 B, a` 〜 A, bo 〜 B0 are all i.i.d zero-mean, subgaus-
Sian random variables with unit variance. Furthermore assume ∣∣1kψ2 , kw'kψ2 , ka'kψ2 , kb'kψ2 ≤
K for each ` ∈ [m] where K ≥ 1. Let
D = 3max{lσ(0)1, Mkσ0k∞, kσ0k∞, 1}
√	/— Ii 0∣∣ J1 "√d + K 2plog(c∕δ) V
L(m, σ, d, K, δ) := √m kσ k∞ 1 1 + C------√m-------).
Assume that
m ≥ C[log(c∕δ) + dlog(CML)].
Then with probability at least 1 — δ
SUp |f(x; θ0)∣ ≤ CDKKPdlog(CML) + log(c∕δ) = O(√d).
x∈BM
Proof. First we note that
aT= σ(Wx + b) — aT= σ(Wy + b) ≤ k⅛ ∣∣σ(Wx + b) — σ(Wy + b)∣2
m	m	m	2
≤ 曜 kσ0k∞ kW(X - y)∣2 ≤ √B kσ0k∞ kWkop kx - y∣2 ≤ √m ∣σ0∣∞ ξ(0)2 ∣x - y∣K ,
mm
where ξ(0) is defined as in Lemma B.17. Thus f (•; θo) is Lipschitz with constant L =
√m ∣∣σ0k∞ ξ(0)2. Well then by Lemma B.17 we have with probability at least 1 — δ
ξ(0)2 ≤ (1 + C√d + k2p°E)2.	⑶
m
26
Published as a conference paper at ICLR 2022
When the above holds We have that f (•; θ0) is LiPschitz with constant
L = √m∣∣σ0∣∣∞ {1 + C
√m
)2.
On the other hand note that
lσ(hw',xi2 + b')1 ≤ lσ(O)| + llσ0k∞ [lhw',xi2l + |b'|].
Thus
∣σ(hw',xi2 + b')∣ψ2 ≤ Iσ(0)∣∣1∣ψ2 + ∣σ0∣∞ [∣∣hw',*|鼠 + 那'|鼠]
≤∣σ(0)∣∣1∣ψ2 + ∣σ0∣∞ [M∣w'∣ψ2 + ∣∣b'∣∣ψ2]
≤ 3max{|b(0)|,M ∣∣σ0l∞ Jσ1l∞}K ≤ DK.
Therefore by Lemma B.11 we have
∣∣a'σ(hw',x>2 + b')∣∣ψι ≤ DK2.
Thus for each x fixed we have by Bernstein’s inequality Vershynin (2018, Theorem 2.8.1)
P
m
£a'b(hw',x〉2 + b`) >
'=1
≤ 2 exp -c min
t2
tʌ/m
[DK2]2, DK2	.
Thus for t ≤ √mDK2 this simplifies to
P (IXa'σ(hw',x>2 + b`)
>
t2
≤ 2expLDK4
∖∣'=ι
Let ∆ bea Y net of the ball BM = {x : ∣∣χ∣∣2 ≤ M} with respect to the Euclidean ∣∣∙∣2 norm. Then
by a standard volume argument we have that
∆l≤ (CM)d=: MY.
Thus by a union bound we have for t ≤ √mDK2
P max
x∈∆
m
£a'b(hw',x〉2 + b`) >
'=1
t2
≤ 2MY eχp LDK4
Thus by setting t = CDK2 ,log(cΜγ/δ) assuming t ≤ √mDK2 we have with probability at
least 1 - δ
max
x∈∆
m
∑a'
√m
'=1 V
(4)
On the other hand by Lemma B.15 our prior definition of t is large enough (up to a redefinition of
the constants c, C) to ensure that with Probability at least 1 - δ
|b0 | ≤ t.
(5)
When (4) and (5) hold simultaneously we have that maxχo∈∆ |f (χ0, θo)∣ ≤ 2t. By a union bound
we have with probability at least 1 - 3δ that (3), (4), (5) hold simultaneously. Well then for any
x ∈ BM we may choose x0 ∈ ∆ so that lx - x0 l2 ≤ Y. Then
|f (x; θo)l ≤ |f (x0; θo)l + If (x; θo) - f (x0; θo)∣ ≤ 2t + Lγ.
Therefore
sup ∣f(x; θo)∣ ≤ 2t + LY
x∈BM
and this argument runs through for any γ > 0. We will set Y = 1/L. Note that for this choice of Y
the hypothesis on m ensures that t ≤ √mDK2. Thus the preceding argument goes through in this
case. Thus by replacing δ with δ∕3 in the previous argument we get the desired conclusion up to a
redefinition of c, C .
□
27
Published as a conference paper at ICLR 2022
We quickly introduce the following lemma.
Lemma B.22.
k^0)kRn ≤kf(t θθ)k∞ + kykRn.
Proof. Let y ∈ Rn be the vector whose ith entry is equal to f(xi; θ0). Well then note that |同|限九 ≤
kf(∙; θo)k∞. Therefore
kr(0)kRn = ky -ykRn ≤ 1刚篙 + kylK ≤ kf(∙;θo)k∞ + kylK .
□
Finally we prove one last lemma that will be useful later.
Lemma B.23. Assume that Wij 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume ∣∣1kψ2 , kw`kψ2 , ka`kψ2 , kb'kψ2 ≤ K
for each ' ∈ [m] where K ≥ LLetr > 1, D = 3max{∣σ(0)∣,M ∣∣σ0k∞ , ∣∣σ0∣∞ , 1},
√	/— Ii o∣∣ J √ ,d+K+ K 2plog(c∕δ) V
L(m, σ, d, K, δ) := mm kσ k∞ 1 1 + C----√m--------- ?,
P := CDK2 VZdlog(CML) + log(c∕δ) = O(√d).
Suppose
m≥
4D2 ky∣Rn T2
[log(Γ)]2
4D2ρ2T 2	ρ 2
and m ≥ maxl[iOg(rF ΛDK2)卜
Then with probability at least 1 - δ
mt≤aTx ξ(t) ≤ Γξ(0)
~ , 、	— ~ ,、
maxξ(t) ≤ rξ(0),
where ξ(t) and ξ(t) are defined as in Lemmas B.2 and B.3. If one instead does the doubling trick
the second hypothesis on m can be removed and the conclusion holds with probability 1.
Proof. First assume We are not doing the doubling trick. Well from the condition m ≥ (D-^ )2 We
have by Lemma B.21 that with probability at least 1 - δ
sup |f (x; θo)∣ ≤ CDK2 PdTog(CML) + log(c∕δ) =: ρ.
x∈BM
Also by Lemma B.22 and the above bound We have
∣r(0)∣Rn ≤ky∣Rn + ρ.
Well in this case
D kr(0)kRnt V D[kykRn + P]t V 2Dmax{kykRn ,ρ}t 1
√m	≤	√m	-	√m	- g(r),
Where We have used the hypothesis on m in the last inequality. Therefore by Lemmas B.2, B.3 We
have in this case that
.,,—.,, ~,.	— ~,.
maχ ξ(t) ≤ Γξ(0)	maχ ξ(t) ≤ Γξ(0)
NoW assume We are performing the doubling trick so that f(∙; θ0) ≡ 0. Then ρ in the previous ar-
gument can simply be replaced With zero and the same argument runs through, except using Lemma
B.21 is no longer necessary (and thus the second hypothesis on m is not needed). Without us-
ing Lemma B.21 the Whole argument is deterministic so that the conclusion holds With probability
1.	□
28
Published as a conference paper at ICLR 2022
B.6 NTK TIME DEVIATIONS BOUNDS
In this section we bound the deviations of the NT K throughout time. This section runs through the
following steps
•	Bound suP(x,y)∈BM×bm ldtKt(χ,y)1
•	Boundsup(x,y)∈BM×BM |Kt(x,y) - K0(x, y)|
In the following lemma we will provide an upper bound on the NTK derivative
SuPx,y∈BM ×B M IdtKt(X,y)|.
Lemma B.24. Let
ξ(t) = max{—m kW(t)kop，—m kb(t)k2 , —m ka(t)k2 , 1},
~，、 ， .. ，、，， .. ，、，， ，，-，、，， .、
ξ⑴=maχ{maχ kw'⑴ k2 , ka(t)k∞ , kb⑴k∞ , 1}
'∈ [m]
D = 3max{lσ(0儿 m kσ0k∞, kσ0k∞, 1}
D0:= max{kσ0k∞, kσ00k∞}2[M2 + 1] + D kσ0k∞ max{1, M}.
Then for any initial conditions W (0), b(0), a(0) we have for all t
CDD0
SUp	∣∂tKt(x, y)| ≤	ξ(t)2ξ(t) kr(t)kRn .
x,y∈BM ×BM	m
Proof. We need to bound the following time derivatives
1n
∂t∂af(x; θ) = ∂tX⑴=-n ɪɪ^ri
[hx,xii2 + 1]
∂t∂w f (x; θ) = ∂t-J=σ1 (x)a 0 x
7=[(∂tσ1 (x))a + σ1 (x)(∂ta)] 0 X
∂t∂bf(x; θ) = ∂t-ɪσ1 (x)a = -ɪ ([∂tσ1 (x)]a + σj(x)∂ta).
Note that
标σ1(x)σ1(Xi) √m [hx,xii2 + 1]
2
≤ ɪ kσ0k∞ ɪ kak2 [M2 + 1].
Thus by Lemma B.1
k∂t∂af(x; θ)k2 ≤ ɪ kσ0k∞
⅛= H2[M2 + 1] kr(t)kRn
≤
kσ0k2∞ [M2+1]]
√m
ξ(t) kr(t)kRn.
On the other hand
[∂tσ10 (x)]a
1n 1
-n∑^i —= σj0(x)σ1 (xi)diag(a)a[hx,xii2 + 1].
i=1
29
Published as a conference paper at ICLR 2022
Well
√1m ι∣σi0(x)σi (Xi )diag(a)a[hχ, xii2 + 1k ≤ √m ι∣σ00k∞ ι∣σ0k∞ ιιak∞ ιιak2 [M2 + 1]
≤kσ00k∞ ισ0ι∞ [M2 + 1]ξ(t)ξ(t).
Thus by Lemma B.1 we have that
ι[∂tσ1 (x)]a∣∣ ≤ Ikb ισ0ι∞ [M2 + 1]ξ(t)ξ(t)疗⑼型”.
Finally we have
1n
σ1 (x)∂ta =——T *。1 (x)x(1).
n
i=1
Well
卜 I(X)X(Il ≤ g'h 卜 i1)( = ι∣σ01∞ √√m ι∣σ(Wxi + b)ι2
≤ ισ0ι∞ [∣σ(0)∣ + √1m ισ0ι∞ (∣∣WQ M + 悯卜)]
≤ ισ0ι∞ [∣σ(0)∣ + M ισ0ι∞ + ξ(t) ≤ |巾匕 Dξ(t).
Thus we finally by Lemma B.1 again we get that
ισ1 (x)∂taι ≤ ισ0ι∞ Dξ(t)忸制μ.
It follows that
ι∂t∂bf (X; θ)ι
≤ √1m [lgk ισ0ι∞ [M2 + 1] + ισ0ι∞ D] ξ(t)ξ(t)疗⑼型“
and similarly
ι∂t∂W f (X; θ)ι
≤ √Mm [Ikb ισ0ι∞ [M2 + 1] + Ihh D] ξ(t)ξ(t) ∣∣r(t)∣∣Rn .
Thus in total we can say
D0
I∣∂t∂af (x; θ)∣∣2 , ∣∣∂t∂bf(x; θ)∣∣2 , ∣∣∂t∂wf (x; θ)∣∣F ≤ √mξ(t)ξ(t) ∣∣r(t)ιRn .
It thus follows by the chain rule and Lemma B.5 that
CDD0
SUp	∣∂tKt(x, y)| ≤	ξ(t)2ξ(t) ∣∣r(t)ιRn .
(x,y)∈BM ×BM	m
□
Using the previous lemma we can now bound the deviations of the NT K.
Theorem B.25. Assume that Wi,j 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume ∣∣W'∣∣ψ2 ,110^^2 , ιb'ιψ2 ≤ K Jor each
` ∈ [m] where K ≥ 1. Let Γ > 1 and T > 0 be positive constants,
D := 3max{lσ(0儿mισ0ι∞, ισ0ι∞, 1},
D0:= [max{ισ0ι∞, ισ00ι∞}2[M2 + 1] + D ισ0ι∞] max{1, M},
and assume
m≥
4D2 IMRn T2
[log(Γ)]2
and m ≥ max^Oy 22。(咏亿⑷ + O(d))}.
30
Published as a conference paper at ICLR 2022
Then with probability at least 1 - δ
sup	|Kt(x,y) - K0(x, y)| ≤
(x,y)∈BM ×BM
2	2-----------2
tr3CDD kr(0)∣∣Rn{ι + CAZd + K√mlog@} n√d + CK2 hpiθg(√δ) + Piogm]}.
If one instead does the doubling trick then the second condition on m can be removed from the
hypothesis and the same conclusion holds.
Proof. First assume we are not doing the doubling trick. By Lemmas B.17, B.18 and a union bound
we have with probability at least 1 - δ
ξ(0)2f(0) ≤ (1 + C√d + K√moE)2 n√d+CK2 hpιog(C∕δ) + Eio.
(6)
■‰ τ . .1	1 f` 1 ∙ τ	τ-> CC . ∙ r∙	9 八/i /∕c∖,K∕τ∖∖ EI	. 1 1	.1 ♦
Note that P as defined in LemmaB.23 satisfies ρ2 = O(log(c∕δ) + O(d)). Thus the hypothesis on m
is strong enough to apply Lemma B.23, therefore by applying this lemma we have with probability
at least 1 - δ
mt≤aTxξ(t) ≤ Γξ(0)
~, 、 — ~,、
ɪmɪa^ξ(t) ≤ rξ(0).
(7)
Thus by replacing δ with δ∕2 and taking a union bound we have that with probability at least 1 - δ
(6) and (7) hold simultaneously. Then using Lemma B.24 and the fact that ∣∣r(t) IlRn ≤ kr(0)kRn
we have for t ≤ T
CDD0
sup	∣∂tKt(x, y)| ≤ Γ3-√^ξ(0)2ξ(0) kr(0)∣Rn .
(x,y)∈BM ×BM	m
Therefore by the fundamental theorem of calculus for t ≤ T
CDD0
SUp	∣Kt(χ,y) - Ko(χ,y)∣ ≤ tΓ3	ξ(0)2ξ(0) ∣∣r(0)∣Rn
(x,y)∈BM ×BM	m
≤ tr3cDD ∣r(0)∣Rn(1 + C√d + K√mlog(C/δ)) n√d + CK2 hpiog(C∕δ) + Piogmio.
Now consider if one instead does the doubling trick where one does the following swaps W (0) →
WW ((00)) , b(0) → bb((00)) , a(0) → -aa((00)) and m → 2m where W (0), b(0), and a(0) are initial-
♦	1	1	1'	EI	>=∕c∖	∙>∙Z∕c∖∙>	.	1	-I-V T	.1	.1	1	.1	.	r∙
ized as before. Then ξ(0) and ξ(0) do not change. We can then run through the same exact proof as
before except when We apply Lemma B.23 the second hypothesis on m is no longer needed. □
Theorem B.26.	Assume that Wi,j 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Let Γ > 1 and T > 0 be positive constants and letD :=
3max{Q(0)|,M kσ0k∞ , kσ0k∞ , 1}. Assume
m≥
4D2 ky∣Rn T2	,	、	∫4D2O(log(c∕δ)+ O(d))T2	/
[log(Γ)]2 and m ≥ max∣---------[iog(f>---------,O(log(c∕δ) + O(d))j .
Then with probability at least 1 - δ we have for t ≤ T
sup	∣Kt(χ,y) - K ∞(χ,y)l = O(牛[1 + tf3 ∣r(0)∣Rn ]).
(x,y)∈BM ×BM	m
If one instead does the doubling trick then one can remove the assumption m ≥
4D O(Iog(C/δ)+OW))T and have h same conclusion hold.
Proof. The condition m ≥ O (iog(C∕δ) + O(d)) is sufficient to satisfy the hypothesis of Theorem
B.19. The condition on m also immediately satisfies the hypothesis of Theorem B.25. The desired
result then follows from a union bound.	□
31
Published as a conference paper at ICLR 2022
Theorem B.27.	Under the same assumptions as Theorem B.26 we have that with probability at
least 1 - δ for all t ≤ T
kH ∞ - Htkop ≤ nO (N [1 + tr3 kr(0)kRn ])
Sup kG∞ - Gtkop ≤ O (√√m [1 + tr3 kr(0)kRn]).
Proof. Recall that for a matrix A ∈ Rm×n 口/以？ ≤ √mnmaxi,j ∣Ai,j |. Thus by Theorem B.26
with probability at least 1 - δ
kH ∞ — Htkop ≤ n max IH∞ -(Htkj I ≤ n SuP	∣Kt(x,y) — K ∞(x,y)l
i,j	(x,y)∈BM ×BM
=n0 (√√m [1 + tr3 kr(O)kRn ]).
The second bound follows from Gs = n Hs and G∞ = 1H∞.	□
B.7 NTK Deviations For ReLU Approximations
The NTK deviation bounds given in the previous subsections assumed kσ00 k∞ < ∞. For ReLU
this assumption is not satisfied. It is natural to ask to what extent we might expect the results
to hold when the activation function is σ(x) = ReLU (x) = max{0, x}. The closest we can
get to ReLU without modifying the proofs is to use the Softmax approximation to ReLU, namely
σ(x) = 1 ln(1 + exp(ax)), and consider what happens as α → ∞. For this choice of σ We have
that kσ00k∞ = O(α). In Subsection B.6 where you will pay the biggest penalty is in Theorem B.25
via the constant D0 = O(∣∣σ00k∞) = O(α2). Since the final bound depends on the ratio W you
will have that m will grow like O(α4). This is no moderate penalty, although we might expect
the results to hold for wide ReLU networks if a finite α provides a reasonable approximation. In
particular Softmax ln(1 + exp(x)) leads to a fixed constant for D0.
C Underparameterized Regime
In this section we build the tools to study the implicit bias in the underparameterized case. Our
ultimate goal is prove Theorem 3.5.
Outline of this section
•	Review operator theory
•	Prove damped deviations equation
•	Bound k(TK∞ -Tns)rtk2
-Bound ∣∣(Tn 一 Tn)rtk2 using NTK deviation results (comparatively easy)
-BOund Il(TK∞ — Tn)rtk2
*	Derive covering number for a class of functions C
*	Use covering number to bound Supg∈C k(TK∞ - Tn)gk
*	Show that rt is in class C
• Prove Theorem 3.5
C.1 RKHS and Mercer’ s Theorem
We recall some facts about Reproducing Kernel Hilbert Spaces (RKHS) and Mercer’s Theorem. For
additional background we suggest Berlinet & Thomas-Agnan (2004). Let X ⊂ Rd be a compact
space equipped with a strictly positive (regular Borel) probability measure ρ. Let K : X×X → Rbe
32
Published as a conference paper at ICLR 2022
a continuous, symmetric, positive definite function. We define the integral operator TK
L2ρ(X)
L2ρ (X) →
TKf(x) :=
X
K(x, s)f (s)dρ(s).
In this setting TK is a compact, positive, self-adjoint operator. By the spectral theorem there is a
countable nonincreasing sequence of nonnegative values {σi}i∞=1 and an orthonormal set {φi}i∞=1
in L2 such that TKφi = σiφi. We will assume that TK is strictly positive, i.e. hf, TKfi2 > 0
for f 6= 0, so that we have further that {φi}i∞=1 is an orthonormal basis of L2 and σi > 0 for all
i. Moreover since K is continuous we may select the φi so that they are continuous functions, i.e.
φi ∈ C(X) for each i. Then by Mercer’s theorem we can decompose
∞
K(x, y) =	σiφi(x)φi(y),
i=1
where the convergence is uniform. Furthermore the RKHS H associated with K is given by the set
of functions
(∞
f ∈ L2 ： X
i=1
where the inner product on H is given by
σi
<∞ ,
∞ ∞	∞^h hf, φii2hg, φii2
f,giH = T —σ—.
i=1	σi
Note that in this setting {√σ^φi}∞=ι is an orthonormal basis of H. Define Kx := K(•, x). Recall
the RKHS has the defining properties
Kx ∈ H ∀x ∈ X
h(x) = hh, KxiH ∀(x, h) ∈ X × H.
We will let κ := supx∈X K(x, x) < ∞. From this we will have the useful inequality: for h ∈ H
Ih(X)I = Kh,KxiHl ≤ IIhkH IIKxkH = khkH PhKx,KxiH = IIhllH PK(X'X)
≤ κ11/ khkH .
Furthermore the elements of H are bounded continuous functions and H is seperable.
C.2 Hilbert-Schmidt and Trace Class Operators
We will recall some definitions from (Rosasco et al., 2010). A bounded operator on a separable
Hilbert space with associated norm k∙k is called Hilbert-Schmidt if
∞
XkAeik2<∞
i=1
for some (any) orthonormal basis {ei}i. For such an operator we define its Hilbert-Schmidt norm
kAkHS to be the square root of the above sum. The Hilbert-Schmidt norm is the analog of the
Frobenius norm for matrices. It is useful to note that every Hilbert-Schmidt operator is compact.
The space of Hilbert-Schmidt operators is a Hilbert space with respect to the inner product
hA, Bi = XhAej, Beji.
j
A stronger notion is that ofa trace class operator. We say a bounded operator on a separable Hilbert
space is trace class if
∞
X(√A*Aei, eii < ∞
i=1
33
Published as a conference paper at ICLR 2022
for some (any) orthonormal bases {ei}i. For such an operator we may define
∞
Tr(A) := XhAei, eii.
i=1
By Lidskii’s theorem the above sum is also equal to the sum of the eigenvalues of A repeated
by multiplicity. The space of trace class operators is a Banach space with the norm kAkT C =
Tr(√A*A). The following inequalities will be useful
kAk ≤ kAk
HS ≤ kAk
TC .
Furthermore if A is Hilbert-Schmidt and B is bounded we have
kBAkHS, kABkHS ≤ kAkHS kBk.
Note that in our setting we have
∞∞
K ≥ J K(x, x)dρ(x) = J iy^σi∣φi(x)∣2dρ(x) = ɪ2 σi J ∣φi(x)∣2dρ(x)
∞
X σi = T r(TK),
i=1
where the interchange of integration and summation is justified by the monotone convergence theo-
rem. Thus TK is a trace class operator and we have the inequality
∞
κ ≥	σi
i=1
which will prove useful later.
C.3 Damped Deviations
Let x 7→ gs(x) ∈ L2 for each s ∈ [0, t] such that s 7→ hφi, gsi2 is measureable for each i and
R0tkgsk22 < ∞. Then we define the integral
Zt
0
gsds
coordinatewise, meaning that R0t gsds is the L2 function h such that
hh, φii2 =	hgs, φii2ds.
0
Using this definition, we can now prove the “Damped Deviations” lemma.
Lemma 2.3. Let K(x, x0) be an arbitrary continuous, symmetric, positive-definite kernel. Let
[Tκh](∙) = JX K(•, s)h(s)dρ(s) be the integral operator associated with K and let [TSh](∙)=
1 ∑n=ι Ks(∙, xi)h(xi) denote the operator associated with the time-dependent NTK KS. Then
rt
exp(-TK t)r0 + Z exp(-TK (t -
0
s))(TK -TnS)rSds,
where the equality is in the L2 sense.
Proof. We have that
1n
∂s∕(x) = -— EKS(X,xi)rs(xi) = -[Tars](x),
i=1
where the equality is pointwise over x. ∂SrS(x) is a continuous function ofx since KS is continuous
and is thus in L2 . Therefore we can consider
h∂SrS, φii2 = h-TnSrS, φii2.
34
Published as a conference paper at ICLR 2022
By the continuity of s 7→ θs we have the parameters are locally bounded in time and thus by Lemma
B.5wehavethat k Ks ∣∣∞ is also locally bounded therefore for any δ > 0, s0: sup∣s-so ∣≤δ IlKsI∣∞ <
∞. Note then that
1n
|dsrs(X)I ≤ n EIKs(X, Xi)IIrs(Xi)I ≤ IIKsk∞ krs l∣Rn ≤ IIKsk∞ IIrOkRn .
n i=1
It follows that I∂srs I∞ is bounded locally uniformly in s. Therefore the following differentiation
under the integral sign is justified
d- hrs, Φii2 = h∂srs,φii2.
ds
Thus combined with our previous equality we get
丁 hrs, φii2 = h-Tsrs, φii2 = h-TKrs,φii2 + h(TK - Ts)rs, φii2
ds	n	n
= hrs, -TK φi i2 + h(TK - Tns)rs, φii2 = -σi hrs, φii2 + h(TK - Tns)rs, φii2,
where we have used that TK is self-adjoint. Therefore
~r- hrs, φii2 + σihrs, φii2 = h(TK - Ts)rs, φii2.
ds	n
Multiplying by the integrating factor exp(σis) we get
-d [exp(σi s)hrs,φi)2 ] = exp(σi s)h(TK — TnJrs,φii 2.
ds	n
Therefore applying the fundamental theorem of calculus after rearrangement we get
hrt,φii2 = exp(—σit)hr0,φii2 + e exp(-σi(t — s))h(TK — Ts)rs, φi)2ds,
0
which is just the coordinatewise version of the desired result.
rt = exp(—TK t)r0 + Z exp(—TK (t — s))(TK — Tns)rsds.
0
□
C.4 Covering Number of Class
We will now estimate the covering number of the class of shallow networks with bounds on their pa-
rameter norms. This lemma is slightly more general than what we will use but we will particularize
it latter as it’s general formulation presents no additional difficulty.
Lemma C.1. Let
aT
C = { √=σ(wx + b) + b0 ： IIa - all2 ≤ P1, IlW — WIlF ≤ P2,
Ib — b0I2 ≤ ρ3, Ib0 — b00I ≤ ρ4
√m Halb ≤ ρ1, √m IIW Ilop ≤ ρ2, √m 11^2 S P3 }
and
γ0:= Iσ(0)I + Iσ0I∞ [ρ02M + ρ03].
Then the (proper) covering number of C in the uniform norm satisfies
C0 p
N(SlU ≤ ㈠
where p =md + 2m + 1 is the total number of parameters and C0 equals
C 0 =C max {ρ1γ0, ρ2M Iσ0I∞ρ01,ρ3 Iσ0I∞ρ01,ρ4} ,
where C	> 0 is an absolute constant.
35
Published as a conference paper at ICLR 2022
Proof. We will bound the pertubation of the function when changing the weights, specifically We
will bound
T	~t
a .ττr t、 a , -r ~.
sup -=σ(Wb) + b)-------σ(Wx + b).
x∈Bm vm	√m
Let x(1) = √mσ(Wx + b) and X(1)= √= σ(Wx + b). Then note that We have
≤√m kσ0k∞
≤√m kσ'k∞ M
〜
X(I)-叫 2 ≤√1m kσ'k∞
〜
W - W
W-Wh
Well then
IaTX(I)- aτX(1)| ≤ |aT(X⑴一 X(I))| + |(a - a)TX(I)I
≤ ι∣ak2 γ+ι∣a - ak2 ||x(1)||2 .
Finally
X(Ii
1	,▼妥	，
7^σ(WX + b)
2*(0)1 + 篇 M J|Wx + b||2
Therefore
小(0)| + √⅛ kσ川||WL 同2 + M
*(0)1+W 防』WIL M+||b||2
≤ ∣σ(0)| + ∣σ'∣∣∞ [ρ2M + &]=:γ0.
|aTx(1) — aτx(1)| ≤ ∣∣a∣2 Y + ∣∣a — a∣b YZ.
Thus if We have
€
≤ 一
― 4γ0
Ila -训2
:€1,
W - W|If ≤ 8M ∣σz∣∞ 4
b b∣∣2 ≤ 8 l∣σ0k∞ P1
then
l∣a∣2 Y ≤
€la∣2 ≤
4ρ1√m ―
€
4
Therefore
IaTX(I)- aTX(I)I ≤ €/2
1	_ I >	T i` ∙>•>]♦	τ	∙> τ~	1 .1 .lτ	τ~ I ， /rʌ	♦	1
and this bound holds for any X ∈ BM. If add biases b0 and b0 such that |b0 - bd ≤ €/2 we simply
get by the triangle inequality
|aTx(1) + b0 — (aτX(1) + bo)| ≤ €.
Thus to get a cover we can simply cover the sets
{a ： ∣a - a0∣2 ≤ ρ1} (W : IW - WllF ≤ ρ2}
{b ： ∣b - b0∣2 ≤ p3} {b0 ： |b0 - b0| ≤ p4}
in the Euclidean norm and multiply the covering numbers. Recall that the € covering number for a
Euclidean ball of radius R in Rs, say M, using the Euclidean norm satisfies
空 Y≤m ≤ ( cr Y
€
€
for two absolute constants c, C > 0. Therefore we get that
N(C,€,||IU ≤
36
Published as a conference paper at ICLR 2022
The desired result follows from
max P1 ,P2 ,P
1 2 3
≤ CC max {P1Y0,P2M ∣∣σ0k∞ pl ,P3 ∣∣σ0k∞ pl ,P4} .
□
We can now prove the following corollary which is the version of the previous lemma that we will
actually use for our neural network.
Corollary C.2. Let
C = { √√mσ(Wx + b) + b0 : √m kak2，√m kWIlop，√m kbk2 ≤ A，|b0| ≤ B}
and
Y0 := ∣σ(0)∣ + kσ0k∞ [AM + A]
and assume m ≥ d. Then the (proper) covering number of C in the uniform norm satisfies
N(C,e,kk∞) ≤(ψ(md))p,
where
Ψ(m, d) = Cmax{√mAγ0, √mdA2 kσ0k∞ M, √mA2 kσ0k∞ , B}
max A2,
m mdO
Proof. The idea is to apply Lemma C.1 with a0 = 0, W0 = 0, b0 = 0, and b00 = 0. Note that
kW∣∣f ≤ √d kWkop ≤ VzmdA. The result then follows by applying lemma with p1 = √mA,
p2 = mmdA, p3 = √mA and p4 = B and pl = p2 = pɜ = A.	□
C.5 Uniform Convergence Over The Class
We now show that k(Tn - TK∞ )gk2 is uniformly small for all g in a suitable class of functions C0.
Ultimately we will show that rt ∈ C0 and thus this result is towards proving that k(Tn - TK∞ )rtk2
is small.
Lemma C.3. Let K(x, x0) by a continuous, symmetric, positive-definite kernel and let
K = maxχ∈χ K(x, x) < ∞, Let TKh(∙) = JX K(•, s)h(s)dp(s) and Tnh(∙) =
I ∑n=1 K(•, xi)h(xi) be the associated operators, Let σl denote the largest eigenvalue of TK,
Let C and Ψ(m, d) be defined as in Corollary C,2, We let C0 = {g 一 f * : g ∈ C}∩{g : kgk∞ ≤ S}
be the set where C is translated by the targetfUnction f * then intersected with the L∞ ball ofradius
S > 0, Then with probability at least 1 一 δ over the sampling of xl , . . . , xn
IS T 2 II V 2S√σκ,2log(C∕δ) + 2plog(∣Kk∞ ψ(m, d)√n)	2
SUPo k(Tn- TK)g∣∣2 ≤ ---------------√n-------------------- + √
2 1 + S √σ1κ ,2log(c∕δ) + O(P)
=	√n	.
Proof Let g ∈ C0. We introduce the random variables Yi := Kxig(xi) 一 Ex〜ρ[Kχg(x)] taking
values in the Hilbert space H for i ∈ [n] where H is the RKHS associated with K. Note that for any
x
∣∣Kxg(χ)∣∣H = ∣g(χ)∣PhKx,Kx)h ≤ SpK(x,x) ≤ Sκ1/2.
Thus ∣∣Yi∣H ≤ 2Sκ1/2 a.s. Thus by Hoeffding's inequality for random variables taking values in a
separable Hilbert space (see Rosasco et al. (2010, Section 2.4)) we have
P
n
XYi
i=l
> t j ≤ 2 exp (—nt2∕2[2Sκ"]2).
H
37
Published as a conference paper at ICLR 2022
Note that by basic properties of the covering number we have thatN(C0, , kk∞) ≤N(C,e∕2Jk∞),
thus by Corollary C.2 the covering number of C0 satisfies (up to a redefinition of C)
N(C0,e,k∣U ≤ (ψ⅞-d2J.
Let ∆ bean E net of C0 in the uniform norm. Note that n P n=1 匕=(Tn - TK )g. Thus by taking a
union bound we have
P (maχ k(Tn - TK)g∣∣H ≥ t) ≤ (a：, d)) 2exp (-nt2∕2[2Sκ"]2).
Note that for any probability measure ν and h ∈ L∞
∣^ K(X, s)h(s)dv(s)| ≤ ]χ IK(x, S)IIh(S)IdV(S) ≤ kKk∞ khk∞ .
It follows that for any h ∈ L∞
k(TK - Tn)hk∞ ≤2kKk∞khk∞.
Note for any g ∈ C0 We can pick ^ in ∆ such that kg - ^k∞ ≤ e. Then
Il(Tn- TK)gk2 ≤ k (Tn- TK)gk2 + k (Tn - TK)(g - 0∣∣2
≤ √σi k(Tn - Tk)gkH + k(Tn - Tk)(g - ^)k∞
≤ √σ1t + 2 kKk∞ kg - 0k∞
≤ √σ1t + 2 kKk∞ e,
where we have used the fact that k∙k2 ≤ √σ1 k∙kH and k∙k2 ≤ k∙k∞ in the second inequality.
Thus by setting
2Sκ1 /2 p2log(c∕δ) + 2p log(Ψ(m, d)∕e)
t =	√n
we have with probability at least 1 - δ
sup k(Tn - TK)gk2 ≤
g∈C0
√στ
2Sκ1 /2 p2log(c∕δ) + 2p log(Ψ(m, d)∕e)
+2kKk∞E.
This argument runs through for any e > 0. Thus by setting E =	√n we get the desired
result.	□
C.6 Neural Network Is In The Class
In this section we demonstrate that the neural network in such a class as C as defined in Lemma C.1.
Once we have this we can use Lemma C.3 to show that k(TK∞ - Tn)rtk2 is uniformly small. The
first step is to bound the parameter norms, hence the following lemma.
Lemma C.4. Assume that Wi,j 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian random
variables with unit variance. Furthermore assume ∣∣1kψ2 , kw'kψ2 , ka'kψ2 , kb'kψ2 ≤ K for each
` ∈ [m] where K ≥ 1. Let Γ > 1, T > 0, D := 3 max{Iσ(0)I, M kσ0k∞ , kσ0k∞ , 1}, and
ξ(t)=max{√m kWkop , √m kbk2 , √m kak2 ,1}.
Furthermore assume
4D2 kykRn T2	,	f 4D2O(log(c∕δ) + O(d))T 2~, …、~, λJ
m ≥	[log(rR]2	and m ≥ max I-----------[log(r)]2-------, O(log(c∕6 + O(d)) j .
Then with probability at least 1 - δ
max ξ (t) ≤ Γ 1 + C
t∈[0,T]
If one instead does the doubling trick then the second condition on m can be removed from the
hypothesis and the same conclusion holds.
38
Published as a conference paper at ICLR 2022
Proof. First assume we are not doing the doubling trick. Note that the hypothesis on m is strong
enough to satisfy the hypothesis of Lemma B.23, therefore we have with probability at least 1 - δ
mt≤aTx ξ(t) ≤ Γξ(0).
Well then separately by Lemma B.17 with probability at least 1 - δ
ξ(0)≤ 1 + C √d + K 2plog.
m
Thus by replacing δ with δ∕2 in the previous statements and taking a union bound We have with
probability at least 1 - δ
max
t∈[0,T]
ξ(t) ≤ Γ
1+C
√m
which is the desired result. Now suppose instead one does the doubling trick. We recall that the
doubling trick does not change ξ(0). Thus we can run through the exact same argument as before
except when we apply Lemma B.23 we can remove the second condition onm from the hypothesis.
□
The following lemma bounds the bias term.
Lemma C.5. For any initial conditions we have
∣bo(t)l ≤ ∣bo(0)l +1 kr(0)kRn.
Proof. Note that
1 n
1dtMt)I = nΣSr(t)i ≤ kr(t)kRn ≤ kr(O)kRn
n i=1
Thus by the fundamental theorem of calculus
∣bo(t)∣≤∣bo(0)∣ +1 kr(0)kRn.
□
The following lemma demonstrates that the residual r = ft - f * is bounded.
Lemma C.6. Assume that Wi,j 〜W, b` 〜B, a` 〜A are all i.i.d zero-mean, Subgaussian random
variables with unit variance. Furthermore assume ∣∣1kψ2 , kw'kψ2 , ka'kψ2 , kb'kψ2 ≤ K for each
' ∈ [m] where K ≥ 1. Let Γ > 1, T > 0, D := 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0k∞ , 1}, and assume
4D2 kykRn T2	,	f 4D2O(log(c∕δ) + O(d))T 2~, …、~, λJ
m ≥	[log(rR]2	and m ≥ max I-------------[log(r)]2--------,O(log(c0 + O(d)) j .
Then with probability at least 1 - δ for t ≤ T
kft - f*k∞ ≤kfo - f*k∞ +1 kr(0)kRn cd2γ2 1 + CVE、√mOg(C/。)
If one instead does the doubling trick then the second condition on m can be removed from the
hypothesis and the same conclusion holds.
Proof. Recall that
nn
∂t(ft(X)- f*(x)) = —- XKt(x,Xi)(ft(xi) - f*(xi)) = —- XKt(x,Xi)r(t)i.
n i=1	n i=1
Thus
1n
∣∂t(ft(χ)-f*(x))| ≤ -E∣Kt(χ,Xi)∣∣r(t)i∣ ≤ kKtk∞ kr(t)kRn ≤ kKtk∞ 疗(0脑”.
n i=1
39
Published as a conference paper at ICLR 2022
Well by Lemma B.5 we have that kKt k∞ ≤ CD2ξ2 (t) where
ξ(t) = max{√m k Wkop , √m kbk2 , √m ka∣∣2 ,1}.
Well by Lemma C.4 we have that with probability at least - δ
max
t∈[0,T]
ξ(t) ≤ Γ
1+C
Thus by the fundamental theorem of calculus for t ≤ T
lft(χ)- f*(x)| ≤ lfo(χ) - f*(x)| +1 kr(0)kRn cd2γ2 1 + CVzd + *第°以呦
Thus by taking the supremum over x we get
kft - f *k∞ ≤ kfo - f *k∞ +1 kr(0)kRn cd2γ2 1 + C V + √mogc )
which is the desired conclusion.	□
We can now finally prove that k(TK∞ - Tn)rtk2 is uniformly small.
Lemma C.7. Let K(x, x0) by a continuous, symmetric, positive-definite kernel and let κ =
maxx K(x,x) < ∞, Let TKh(∙) = JX K(∙,s)h(s)dP(S) and Tnh(∙) = 1 En=I K(•, xi)h(xi)
be the associated operators. Assume that Wij 〜 W, b` 〜 B, a` 〜 A are all
i,i,d zero-mean, subgaussian random variables with unit variance, Furthermore assume
k1kψ2 ,kw'kψ2	,ka'kψ2	,kb'kψ2	, kb0kψ2	≤	K0 for each ' ∈	[m]	Where K0 ≥ 1.	Let γ >	1,
T > 0, D := 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0k∞ , 1}, and assume
4D2 IIykRn T2	,	f 4D2O(log(c∕δ) + O(d))T 2~, …、~, λJ
m ≥	[log(rR]2	and m ≥ max I-----------[log(r)]2-------,O(log(c0 + O(d)) j .
If we are doing the doubling trick set S0 = 0 and otherwise set
S0 = CD(K 0)2 Jd Iog(CMO(√m)) + log(c∕δ) = O(√d).
Then with probability at least 1 - δ
U	Qdkf*k∞ + S0)(1 + TΓ2)√σικp∖
Sup k (Tn - TK )rt∣2 = O (-----------√n--------Y------)
and
krok∞ ≤kf*k∞ + S0.
If we are performing the doubling trick the second condition on m can be removed and the same
conclusion holds.
Proof. By Lemma C.4 we have with probability at least 1 - δ
max
t∈[0,T]
ξ(t) ≤ Γ
1+C
: A.
(8)
Also by Lemma C.5
∣bo(t)l ≤ Ibo(O)I +1 kr(0)kRn.
If we are doing the doubling trick then b0(0) = 0. Otherwise by Lemma B.15 we have with proba-
bility at least 1 - δ
Ibo(O)I ≤ CK0Plog(c∕δ).
40
Published as a conference paper at ICLR 2022
Furthermore by Lemma B.22 we have
kr(0)kRn SkykRn + kf (∙； θθ ) k ∞ ∙
Let L be defined as in Lemma B.21, i.e.
L(m, σ, d, K0, δ) := √m kσ0k∞ (l + C+(K√plogM)) = O)(√m).
If we are not performing the doubling trick set
S 0 = CD(K 0)2 Pd Iog(CML) + log(c∕δ).
Otherwise if we are performing the doubling trick set S0 = 0. In either case by Lemma B.21 we
have with probability at least 1 - δ
kf(•；θo)k∞ ≤ S.	(9)
In particular by Lemma B.22 we have
kr(0)kRn ≤kykRn + kf(•;θo)k∞ 二|训型“ + S0.
Thus we can say
∣bo(t)l ≤ Ibo(O)I +1 kr(0)kRn ≤ CK0Plog(c∕δ) + T(ky∣∣Rn + S0) =: B
and this holds whether or not we are performing the doubling trick. Thus up until time T the
neural network is in class C as defined in Corollary C.2 with parameters A and B as defined above.
Moreover by Lemma C.6 separate from the randomness before we have that with probability at least
1-δ
-	,—	C  ---------I 2
krtk∞ ≤ krok∞ +1 kr(0)kRn CD2Γ2 1 + C^ +(Klog^
Well note that when (9) holds we have
kr(0)kRn ≤ krοk∞ ≤ kf *k∞ + kf (•； θo)k∞ ≤ kf *k∞ + S0.
Thus
krtk∞ ≤ (kf*k∞ + S0)∣1+ TCD2Γ2 1 + C8 +(KK√p°oS~	} =： S
Thus by taking a union bound and redefining δ we have by an application of Lemma C.3 with S as
defined in the hypothesis of the current theorem that with probability at least 1 - δ
2 1 + S√σiκ ,2log(c∕δ) + O(P)
sup Il(Tn - TK)rt∣∣2 ≤ -------------7=-------------Z-
t≤T	n
=O ((kf*k∞ + S0)(1 + TΓ2)√σw)
where We have used that S = O([kf *∣∣∞ + S0][1 + TΓ2]).	□
C.7 Proof Of Theorem 3.5
We are almost ready to prove Theorem 3.5. However first we must introduce a couple lemmas.
The following lemma uses the damped deviations equation to bound the difference between rt and
exp(-TKt)r0.
Lemma C.8. Let K(x, x0) by a continuous, symmetric, positive-definite kernel with associated
operator TKh(∙) = JX K(∙,s)h(s)dP(S). Let Tnh(∙) = 1 En=I KS (•, xi)h(xi) denote the oper-
ator associated with the time-dependent NTK. Then
kPk(rt - exp(-Tκt)r0)k2 ≤ 1 - ‘*?(—外力)sup k(Tκ - Tn)rsk2 .
σk	S≤t
and
M — exp(-Tκt)r0k2 ≤ t ∙ SUp 11 (TK - TS)/k2 ∙
S≤t
41
Published as a conference paper at ICLR 2022
Proof. From Lemma 2.3 we have
rt = exp(-TK t)r0 +	exp(-TK (t- s))(TK - Tns)rsds.
0
Thus for any k ∈ N
Pk(rt - exp(-TK t)r0) = Pk Z exp(-TK (t- s))(TK - Tns)rsds
0
Z Pk exp(-TK (t- s))(TK - Tns)rsds.
0
Therefore
kPk(rt - exp(-TKt)r0)k2
Z Pk exp(-TK (t- s))(TK - Tns)rsds
0
2
≤ Z t kPk exp(-TK (t- s))(TK - Tns)rsk2 ds
0
≤ t kPk exp(-TK (t - s))k k(TK - Tns)rsk2 ds
0
≤	exp(-σk (t- s)) k(TK - Tns)rs k2 ds
0
≤ 1 - exp(-σkt
σk
sup k(TK -Tns)rsk2.
s≤t
Similarly
krt - exp(-TKt)r0k2
Z exp(-TK (t- s))(TK - Tns)rsds
0
2
≤ Z kexp(-TK (t- s))(TK - Tns)rsk2 ds
0
≤ t kexp(-TK (t - s))k k(TK - Tns)rsk2 ds
0
≤ Zt k(Tκ - TS)rsk2 ds ≤ t ∙ sup k(Tκ - TS)rsk2 .
0	s≤t
□
In light of the previous lemma we would like to have a bound for k(TK - Tns)rs k2. This is accom-
plished by the following lemma.
Lemma C.9. Let K(x,x0) by a continuous, symmetric, positive-definite kernel. Let TKh(∙)=
JX K(•, s)h(s)dρ(s) and Tnh(•) = 1 ∑22ι K(•, xi)h(xi) be the associated operators. Let
Tnh(•) = n En=I Ks(∙, xi)h(xi) denote the operator associated with the time-dependent NTK.
Then
SUp k(TK - Tn)rsk2 ≤ SUp k(TK - Tn)TS k2 +suP kK - KSk∞ kr(O)kRn ∙
s≤T	s≤T	s≤T
Proof. We have that
k(TK - TnS)rSk2 ≤ k(TK - Tn)rSk2 + k(Tn - TnS)rSk2.
Now observe that
1 n
|(Tn - Ts)rs(x)∣ = — £[K(x,Xi) - Ks(x,Xi)]rs(xi)
n i=1
1n
≤ —£|K(χ,χi) - KS(χ,χi)∣∣rs(χi)∣
n i=1
≤kK - Ksk∞ kT(s)kRn ≤kK - Ksk∞ kT(0)kRn ∙
42
Published as a conference paper at ICLR 2022
Therefore
k(Tn - TS)rsk2 ≤ k(Tn - TS)rsk∞ ≤ kK - Ksk∞ kr(0)kRn .
Thus
SUp k(TK - Tn)rsk2 ≤ SUp k(TK - Tn)rsk2 +sup kK - Ksk∞ kr⑼ kRn .
s≤T	s≤T	s≤T
□
We are almost ready to finally prove Theorem 3.5. We must prove one final lemma that combines
Lemma C.7 with the NTK deviation bounds in Theorem B.26 to show that k(TK∞ - Tns)rsk2 is
uniformly small.
Lemma C.10. Assume that Wij 〜 W, b` 〜 B, a` 〜 A are all i.i.d zero-mean, Subgaussian
random variables with unit variance. Furthermore assume ∣∣1kψ2 , kw`∣∣ψ2 , ka`∣∣ψ2 , kb'kψ2 ≤ K
for each ' ∈ [m] where K ≥ LLetr > 1, T > 0, D := 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0∣∣∞ , 1}, and
assume
4D2 kykRn T2	,	、	4 4D2O(log(c∕δ) + O(d))T2
m ≥	[lθg(1R]2	and m ≥ max∖ -----------[log(r)]2--------,O(log(c/S) + °(d))}
If we are doing the doubling trick set S0 = 0 and otherwise set
S = CDK2，dlog(CMθ(√m)) + log(c∕δ) = O(√d),S = S0 + kf *k∞ .
Then with probability at least 1 - δ
sup k(Tκ∞ - Tn)rs k2 = O 卜√m [1 + tΓ3S] + S(1 + T√n)√σlκp!.
If we are performing the doubling trick the condition m ≥ 4D O(Iogog(Γ)+OW))T can be removed
and the same conclusion holds.
Proof. Note by Lemma C.9 we have
SUp k(Tκ∞ - Ts)rsk2 ≤ sup k(Tκ∞ - ‰)rsk2 + sup kK∞ - Ksk∞ 忻(0)1总.
s≤T	s≤T	s≤T
Well then by Theorem B.26 we have with probability at least 1 - δ that
SUpkKt - K∞k∞ = O (√! [1 + tr3 kr(0)kRn]).
Separately by Lemma C.7 we have with probability at least 1 - δ
ll 方((kf*k∞ + s0)(1 + TΓ2)√σ1κp∖	方 p(1 + TΓ2)√σικρ∖
SUP k(TΚ∞ - Tn)rsk2 = O (----------------√n-------Y------J= O (----------√nL-------)
and
kr(0)kRn ≤ krok∞ ≤ S.
The result follows then from taking a union bound and replacing δ with δ∕2.	□
We now proceed to prove the main theorem of this paper.
Theorem 3.5. Assume that Assumptions 3.3 and 3.4 hold. Let Pk be the orthogonal projection in
L2 onto span{φι,..., φk} and let D := 3max{∣σ(0)∣,M ∣∣σ0k∞ , ∣∣σ0∣∣∞ , 1}. Ifwe are doing the
doubling trick set S 0 = 0 and otherwise set S0 = O (j O(d) + log(c∕δ)) , S = kf * k∞ + S0.
Also let T > 0. Assume m ≥ D2 ∣y∣∣Rn T2, and m ≥ O(log(c∕δ) + O(d)) max {T2,1}. Then with
probability at least 1 - δ we have that for all t ≤ T and k ∈ N
∣∣Pk(rt - exp(-TK∞t)r0)k2 ≤ ^—exp σk" O (S [1 + tS] √= + S(1 +
and
∣∣rt — exp(-TK∞t)r0k2 ≤ tO (S [1 + tS] √- + S(1 +
43
Published as a conference paper at ICLR 2022
Proof. By Lemma C.8 we have for any k ∈ N
kPk(rt — exp(-Tκ∞t)r0)k2 ≤ 1 - exp(-σk“ SUp k(Tκ∞ - Ts)%|卜
σk	s≤t
and furthermore
krt - exp(-TK ∞ t)r0 k ≤ t sUp k(TK∞ - Tns)rs k2
s≤t
Well the conditions on m in the hypothesis suffice to apply Lemma C.10 with Γ = e2 ensure that
with probability at least 1 - δ
SUp k(Tκ∞ - Tn)rsk2 = o (S √√d [i+ts] + S(1++√n√σ1κ!.
Since κand σ1 only depend on K∞ which is fixed we will treat them as constants for simplicity of
presentation of the main result (note that they were tracked in all previous results for anyone inter-
ested in the specific constants). The desired result follows from plugging in the above expression
into the previous bounds after setting σ1 and K as constants.	□
Theorem 3.5 is strong enough to get a bound on the test error, which is demonstrated by the following
corollary.
Corollary 3.6. Assume Assumptions 3.3 and 3.4 hold. Suppose that f * = O(1) and assume we
are performing the doubling trick where fo ≡ 0 so that r0 = —f *. Let k ∈ N and let Pk be
the orthogonal projection onto span {φι,... ,φk}. Set t = Iog(Vz2kPk f k2/' ' ) Then we have that
σk
m = ΩΩ(*)and n = ΩΩ (芝)suffices to ensure with probability at least 1 — δ
1 krtk2 ≤ 2e + 2 k(I- PQf*k2 ∙
Proof. Set t = klg(√2kPσf* k""). Note that
1	21	2
2 Ilrtk2 ≤ 2 [kexp(-TK∞t)r0k2 + krt - eχp(-TK∞t)r0k2]
≤ 2max{kexp(-TK∞t)r0k2 , krt - exp(-TK∞t)r0k2}2
≤ 2 hIexp(-TK ∞ t)r0 I2 + Irt - exp(-TK ∞ t)r0 I2i .
Note that
∞
∣∣exp(-Tκ∞ t)ro∣2 = ∣∣exp(-TK∞ t)f *k2 = X exp(-2σit)∣hf * ,φi i2∣2
i=1
k∞
≤ exp(-2σkt) X Kf*,φii2∣2 + X Kf*,φii2∣2
i=1	i=k+1
=| + k(I - Pk)f *k2 .
We want to apply Theorem 3.5 with T = t. We need
m ≥ D2 ∣y∣∣Rn t2 and m ≥ O(log(c∕δ) + O(d)) max{t2,1}.
Note that since f * = O(1) Wehavethatkr(0)IlRn = IlyIlRn =。⑴.Then
D2ky∣Rn t2 = O(t2)= O (ɪ).
thus our condition on m is strong enough to satisfy the first condition. Also O(log(c∕δ) +
O(d))max{t2,1} = O(dt2) which is satisfied by our condition on m. Thus by an application
of Theorem 3.5 With T = t We have With probability at least 1 - δ
.. ，一 、，， ~
∣∣rt - exp(-Tκ∞t)r0k2 ≤ tO
If*I∞[1+tIf*I∞]
√√d+kf *k∞ (ι+t) √n).
44
Published as a conference paper at ICLR 2022
Recall that f * = O⑴.Thus the first term above is
Thus setting m
second term is
Ω(张)suffices to ensure the first term is bounded by e1∕2∕(2√2). Similarly the
Thus setting n = Ω
this case we have
suffices to ensure that the second term bounded by e1∕2∕(2√2). Thus in
1/2
krt - exp(-TK∞t)r0k2 ≤ ~√2 ∙
Thus we have
2 krtk2 ≤ 2 hkexp(-Tκ∞t)roii2 + krt- eχp(-Tκ∞t)r0k2i ≤ 2e + 2 k(I- Pk)f*k2 ∙
C.8 Deterministic Initialization
In this section we will prove a version of Theorem 3.5 where instead of θ0 being chosen randomly
we take θ0 to be some deterministic value. θ0 could represent the parameters given by the output
of some pretraining procedure that is independent of the training data, or selected with a priori
knowledge.
Lemma C.11. Let θ0 be a fixed parameter initialization. Let Γ > 1, T > 0, D :=
3max{∣σ(0)∣,M ∣σ0∣∞ , ∣σ0∣∞ , 1},
ξ(t) = max{√m kW(t)kop , √m kb(t)k2 , √1= ka(t)k2 , 1},
~，、 ， .. ，、，， ，，，、，， ，，-，、，， .、
ξ(t) = max{max kw'⑴k2 , ka⑴k∞ , kb(t)k∞ , 1}∙
'∈[m]
Furthermore assume
Then
D2 kr(0)kRnT2
≥ [lθg(Γ)]2	∙
.,, —.,, ~ , .	— ~ ,.
tmaT]ξ(t) ≤ rξ(0) t∈maT]ξ(t) ≤ rξ(0)∙
Proof. By the hypothesis onm we have that fort ≤ T
Dkr(0)kRnt
√m
≤ log Γ∙
□
Therefore by Lemmas B.2 and B.3 the desired result holds.
□
Lemma C.12. Let θ0 be a fixed parameter initialization. Let Γ > 1, T > 0, D
3max{∣σ(0)∣,M ∣∣σ0∣∣∞ , ∣∣σ0∣∣∞ , 1},
ξ(t)=max{√m kw(t)kop, √m kb(t)∣2, √m ∣a(t)∣2,4，
and assume
D2 kr(0)kRn T2
≥ [log(Γ)]2	∙
Then for t ≤ T
kft-f*k∞ ≤ kfo - f*k∞ + t kr(0)kRn CD2Γ2ξ(0)2∙
45
Published as a conference paper at ICLR 2022
Proof. Recall that
nn
∂t(ft(X)- f *(x)) = --X Kt(x,Xi)(ft(xi) - f *(xi)) = --X Kt(x,Xi)r(t)i.
n i=1	n i=1
Thus
1n
∣∂t(ft(χ)- f*(x))| ≤ — E∣Kt(χ,Xi)∣∣r(t)i∣ ≤ kKtk∞ kr(t)kRn ≤ kKtk∞ 疗(。)]总.
n i=1
Well by Lemma B.5 we have that kKtk∞ ≤ CD2ξ2 (t). Also by Lemma C.11 we have that
max ξ(t) ≤ Γξ(0).
t∈[0,T]
Thus by the fundamental theorem of calculus for t ≤ T
Ift(χ) - f*(χ)l ≤ lfo(χ) - f*(χ)l +1 kr(0)kRn CD2r2ξ(0)2.
Thus by taking the supremum over x we get
kft - f*k∞ ≤ kfo - f*k∞ +1 kr(0)kRn CD2r2ξ(0)2
which is the desired conclusion.
□
Lemma C.13. Let θ0 be a fixed parameter initialization. Let K0 denote the time-dependent NTK at
initialization θo. Let Tk°h(∙) = JX Ko(∙,s)h(s)dρ(s) and Tnh(∙) = 1 5∑n=ι Ko(∙, Xi)h(xi) be
the associated operators. Let κ = maxx K0(x, x) and let σ1 denote the largest eigenvalue of TK0.
Let Γ > 1,T> 0, D := 3max{∣σ(0)∣,M ∣∣σ0∣∣∞ , ∣∣σ0∣∣∞ , 1},
ξ(0) = max{√m kW(0)kop , √1m kb(0)k2 , √1m ka(0)k2 , 1},
and assume
“D2 [kf *k∞ + kf0k∞]2 T2
≥	[iog(r)]2	.
Then with probability at least 1 - δ over the sampling of x1, . . . , xn we have that
sup k(Tn - Tκo)rtk2 = O ((kf*k∞ + kf0k∞)√+ Tr2ξ(0)2)√σκp).
Proof. First note that
kr(0)kRn ≤ krok∞ ≤kf*k∞ + kfok∞ .	(10)
Thus our hypothesis on m is strong enough to apply Lemma C.11 so that we have
max ξ(t) ≤ Γξ(0) =: A.	(11)
t∈[0,T]
Also by Lemma C.5
∣bo(t)l ≤ Ibo(O)I + t kr(0)kRn,
therefore
max IMt)I ≤ IMO)I + Tkr(O)kRn ：= B.
Thus up until time T the neural network is in class C as defined in Corollary C.2 with parameters A
and B as defined above. Furthermore by Lemma C.12 we have
kft - f*k∞ ≤ kfo -f*k∞ +1 kr(0)kRnCD2r2ξ(0)2.
Well then by (10) and the above we have
krtk∞ ≤ (kf*k∞ + kfok∞) {1 + TCD2Γ2ξ(0)2} =: S.
46
Published as a conference paper at ICLR 2022
Thus by an application of Lemma C.3 with K = K0 we have with probability at least 1 - δ over the
sampling of x1, . . . , xn that
sup k(Tn - TK0)rtk2 ≤
t≤T
2 1 + S√σiκ ,2log(c∕δ) + O(P)
〜
O
(kf *k∞ + kfok∞)(1+ TΓ2ξ(0)2)
where We have used that S = O([kf *k∞ + kfok∞][1 + TΓ2ξ(0)2]).
Lemma C.14. Let	θ0	be a fixed parameter initialization. Let Γ
1，	T >	O,	D ：=	3max{∣σ(0)∣,M kσ0k∞, kσ0k∞, 1}, D0
max{kσ0k∞, kσ00k∞}2[M 2 + 1] + D kσ0k∞ max{1, M},
ξ(t) = max{√m kW(t)kop , √m kb(t)k2 , √m ka(t)k2 , 1},
□
>
~，、 ， .. ，、，， ，，，、，， ，，-，、，， .、
ξ⑴=max{max kw'⑴k2 , ka(t)k∞ , kb(t)k∞ , 1}.
'∈[m]
Furthermore assume
m≥
D2kr(0)kRn T2
[log(Γ)]2
Then for t ≤ T
CDD0
kKo - Ktk∞ ≤ t-^Γ3ξ(0)2ξ(0) kr(0)kRn .
Proof. Note by Lemma B.24 we have that
CDD0
SUp	∣∂tKt(χ, y)| ≤	ξ(t)2ξ(t) kr(t)kRn .
x,y∈BM ×BM	m
Now applying Lemma C.11 and the fact that ∣∣r(t)kRn ≤ ∣∣r(0)kRn from the above we get that for
t≤T
CDD0
SUp	∣∂tKt(x, y)| ≤	Γ3ξ(0)2ξ(0) kr(0)kRn .
x,y∈BM ×BM	m
Thus by the fundamental theorem of calculus we have that for t ≤ T
CDD0
kKo - Ktk∞ ≤ t-√m-Γ3ξ(0)2ξ(O) kr(0)kRn.
□
Theorem C.15. Let θ0 be a fixed parameter initialization. Assume that Assumption 3.3 holds.
Let {φi}i denote the eigenfunctions of TK0 corresponding to the nonzero eigenvalues, which we
enumerate σι ≥ σ2 ≥ ∙∙∙. Let Pk be the orthogonal projection in L2 onto span {φι,..., φk} and
let D := 3max{∣σ(0)∣, M ∣∣σ0k∞ , ∣∣σ0∣∣∞ , 1}. Also let T > O and set
ξ(0)=maχ{√m ∣w (O)kop, √m kb(0)k2, √m ka(0)k2, 1},
~ , . , .. , , .. ............. .. ,... 、
ξ(O) = maχ{max kw'(O)k2 , ka(O)k∞ , kb(O)k∞ , 1}.
'∈[m]
S := O([kf*k∞ + kf0k∞][1 + Tξ(O)2]).
Assume
m ≥ D2 [kf *k∞ + kfok∞]2 T2.
Then with probability at least 1 - δ over the sampling of x1 , . . . , xn we have that for all t ≤ T and
k∈N
kPk(rt — exp(-Tκot)r0)k2 ≤ 1-exp(-σkt)O (√=ξ(O)2ξ(O) kr(O)kRn + S3^)
0	2	σk	m	R	n
and	____
krt - exp(-TKOt)rok2 ≤ to (√mξ(O)话(O) kr(O)IlRn +	λ√n P).
47
Published as a conference paper at ICLR 2022
Proof. By Lemma C.8 we have for any k ∈ N
kPk(rt - exp(-Tκot)r0)k2 ≤ 1- *外t) SUp k(¾, - Tn)rsk2
σk	s≤t
and furthermore
krt -exp(-TK0t)r0k ≤ t sUp k(TK0 -Tns)rsk2 .
s≤t
Let Tnh(∙) = * pn=1 Ko(∙, xi)h(xi) be the discretization of Tk. Thus by Lemma C.9 We have
Sup k(TKO- Tn)rs k2 ≤ Sup k(TKO- Tn)rsk2 +sup IIKO - Ksk∞ llr(0)kRn .
s≤t	s≤t	s≤t
Note from the inequality
kr(0)kRn ≤krok∞ ≤kf*k∞ + kfok∞
the hypothesis on m is strong enough to apply Lemma C.14 With Γ = e. Well then by an application
of Lemma C.14 With Γ = e We have that
Sup kKs - KOk∞ = O (√√mξ(0)与(0) kr(0)kRn).
Separately by Lemma C.13 We have With probability at least 1 - δ
SUp k(Tκo - Tn)rsk2 = O ((kf*k∞ + kf°k∞"+ T£(0)2)E).
s≤t	2	n
Combining these results We get that
Sup ktko - Ts)rsk2 ≤ o( √√mξ(0)与(0)kr(o)kRn+s√σnκp).
The desired result follows from plugging in the above expression into the previous bounds. □
D Damped Deviations on Training Set
The damped deviations lemma for the training set is incredibly simple to prove and yet is incredibly
powerful as we will see later. Here is the proof.
Lemma 2.1. Let G ∈ Rn×n be an arbitrary positive semidefinite matrix and let Gs be the time
dependent NTK matrix at time s. Then
^ = exp(-Gt)^0 + Z
O
exp(-G(t - S))(G — Gs)ιTsds.
Proof. Note that we have the equation
∂trt = -Gtrt = -Grt + (G — Gt)Ir t.
Thus by multiplying by the integrating factor exp(Gt) and using the fact that exp(Gt) and G com-
mute we have that
∂t exp(Gt)∣t = exp(Gt)(G — Gt)∣t.
Therefore by the fundamental theorem of calculus
exp(Gt)∣t - ro
Z exp(Gs)(G
O
-Gs)rsds,
which after rearrangement gives
Irt = exp(-Gt)^0 + Z
O
exp(-G(t - s))(G — Gs)ιrsds.
□
48
Published as a conference paper at ICLR 2022
Throughout we will let u1 , . . . , un denote the eigenvectors of G∞ with corresponding eigenvalues
λι,..., λn, normalized to have unit norm in ∣∣∙∣∣Rn, i.e. ∣∣ui∣∣Rn = 1. The following corollary
demonstrates that if one is only interested in approximating the top eigenvectors, then the deviations
of the NTK only need to be small relative to the cutoff eigenvalue λi that you care about.
Corollary D.1. Let Pk be the orthogonal projection onto span{u1, . . . , uk}. Then for any k ∈ [n]
kPkE- eχp(-G∞t)r0)kRn ≤ sup kG∞ - Gskop krokRn1 - exp(-λkt)
s≤t	λk
≤ SUP kG∞ - Gskop IIrOkRn t.
s≤t
In particular
krt - exp(-G∞t)r0∣Rn ≤ sup kG∞ - GskRn krokRn一呼…≤ SUp ∣G∞ - Gskop krok
t.
Proof. Note by Lemma 2.1 we have that
rt - exp(-G∞t)ro = Z exp(-G∞(t - s))(G∞ - Gs)rsds
O
Therefore for any k ∈ [n]
Pk(rt - exp(-G∞t)ro) = Pk Z exp(-G∞(t — s))(G∞ — Gs)rsds
0
Z Pk exp(-G∞(t - S))(G∞ - Gs)Irsds
0
Thus
kPk(rt - exp(-G∞t)ro)kRn
t
Pk exp(-G∞(t - s))(G∞ - Gs)rsds
Rn
≤ Z kPk exp(-G∞(t - S))(G∞ - Gs)rskRn ds
O
≤ ∣ot kPk exp(-G∞(t - s))kop kG∞ - Gskop krskRn ds
≤ / exp(-λk (t-s)) kG∞ - Gsk op IIrOkRn ds
≤ SuP kG∞ - Gsk op IIrOkRn Z exp(-λk (t - s))ds
s≤t	O
≤ SUP kG∞ - Gskop IIrOkRn
s≤t
1 - exp(-λk t)
≤ sup kG∞ - Gskop krok
s≤t
λk
Rn t
where we have used the inequality 1 + x ≤ exp(x) in the last inequality. By specializing to the case
k = n since span{u1, . . . , un} = Rn we have
krt - exp(-G∞t)r0kRn ≤ SuP kG∞ - Gskop IIrOkRn
s≤t
≤ SuP kG∞ - Gskop IIrOkRn t.
s≤t
1 - exp(-λn t)
This completes the proof.
□
Theorem 3.5 uses the concept of damped deviations to compare rt with exp(-TK∞t)rO. We can
also prove the analogous statement on the training set that compares rt to exp(-G∞ t)r0. The
following is the analog of Theorem 3.5 on the training set.
49
Published as a conference paper at ICLR 2022
Theorem D.2. Let D := 3max{∣σ(0)∣, M ∣∣σ0k∞ ,∣∣σ0k∞ , 1}. Also let Γ > 1,T > 0. Further-
more assume
m≥
4D2 kykRn T2
[log(Γ)]2
and m ≥ max
^ I
4D2Ο(log(c∕δ) + O(d))T2
[log(Γ)]2
,Ο(log(c∕δ) + Ο(d))}
Let Pk be the orthogonal projection onto span{u1, . . . , uk}. Then with probability at least 1 - δ we
have for any k ∈ [n] and t ≤ T
kPk(rt — exp(-G∞t)r°)∣∣Rn ≤
1 - exp(-λkt)
...	..	,T
krOkRn O
[1 + tr3 kr(0)kRn ]
λk
.... ~
≤ t IIrOkRn O
[1 + tr3 kr(0)kRn ]
in particular
krt - eχp(-G∞t)r0kRn
≤ 1 - eXp(一λnt)
... .. ~
krθkRn O
[1 + tr3 kr(0)kRn ]
.... ≈
≤ t IIrOkRn O
[1 + tr3 kr(0)kRn].
If one instead does the doubling trick the condition
hypothesis and the same conclusion holds.
4D2O(log(c∕δ) + O(d))T
[log(Γ)]2
2
can be removed from the
Proof. By Corollary D.1 we have
kPk(rt - exp(-G∞t)ro)kRn ≤ sup kG∞
s≤t
1 - exp(-λkt)
λk
≤ SUP IIG∞ - GskRn IIrOkRn t
s≤t
Well by Theorem B.27 we have with probability at least 1 - δ
sup kG∞ - Gtkop ≤ O
[1 + tr3 kr(0)kRn ]
—
The desired result follows from plugging this in to the previous bounds.
□
E Proof of Theorem 3.7
We can noW quickly prove our analog of Theorem 4.1 from Arora et al. (2019).
Theorem 3.7. Assume m = ΩΩ(dn5e-2λn(H∞)-4) and m ≥ O(log(c∕δ) + 0(d)) and f * =
0(1). Assume we are performing the doubling trick so that ro = -y. Let vι,..., Vn denote the
eigenvectors of G∞ normalized to have unit L2 norm kvi k2 = 1. Then with probability at least
1-δ
Irt = exp(-G∞t)(-y) + δ(t),
where supt≥0 kδ(t)k2 ≤ . In particular
krtk2 = t
n
Xexp(-2λit)∣hy, Vii2∣2 ± e.
i=1
Proof. Set T = log(kr(0)∣∣Rn √n∕e)∕λn. Note that since f* = 0(1) and We are performing the
doubling triCkWehavethatkrOkRn = kykRn = 0(1). Recall that λn := 1 λn(H∞) therefore
〜
〜
m = Ω(dn5e-2λn(H∞)-4) = Ω(dne-2λ-4) is strong enough to ensure that
m≥
4D2 kykRn T2
[log(2)]2
, , . . ~∙ , ~ .
m ≥ O(log(c∕δ) + O(d)) = O(d)
-T

50
Published as a conference paper at ICLR 2022
Then by an application of Theorem D.2 with Γ = 2 we have with probability at least 1 - δ that for
all t ≤ T
k^ - eXp(-G∞t)r°kRn ≤ l-exp(-λnt) krokRn O
λn
[1 + tr3 kr(0)kRn ]
≤ kr0kRnO
λ	λn
[1 + T Γ3 kr(0)kRn ]
Since f * = O(1) Wehavethatkrok Rn = IlykRn = O(1) therefore the above bound is
Thus m = ΩΩ(dn56-2λn(H∞)-4) = ΩΩ(dne-2λ-4) suffices to make the above term bounded by
e∕√n. Thus in this case
sup ∣∣rt — exp(-G∞t)ro∣Rn ≤ e∕√n.
t≤T
Let δ(t) = ^ — exp(—G∞t)ro. We have just shown that supt≤τ ∣∣δ(t)∣Rn ≤ e∕√n. We will now
bound δ(t) for t ≥ T. Note that fort ≥ T
∣∣exp(-G∞t)r0∣Rn ≤ exp(—λnt) ∣∣ro∣∣Rn ≤ exp(-λnT) |岛||限九 ≤ e∕√n
Also for t ≥ T
krtkRn ≤ krτ∣Rn ≤ kexp(-G∞T+ kδ(T)∣Rn ≤ 2e/，n
where we have used that ||九 ∣∣Rn is nonincreasing for gradient flow. Therefore for t ≥ T
kδ(t)kRn ≤ krtkRn + kexP(-G∞t)^0kRn ≤ &/5
Thus we have shown
sup kδ(t)∣∣Rn ≤ 3"√n.
t≥0
The desired result follows from replacing E with e/3 in the previous argument and using the fact that
l∣∙k2 = √n k∙∣Rn and ro = y.	□
F Proof of Theorem 3.8
Using some lemmas that we leave to the following section, we can prove Theorem 3.8 quite quickly
using the damped deviations equation and the NTK deviation bounds.
F.1 Main Theorem
Theorem 3.8. Assume Assumptions 3.3 and 3.4 hold. Furthermore assume m =
ΩΩ 0-2dT2 kf *k∞ (1 + T∣∣f*k∞)2) where T > 0 is a time parameter and m ≥ O(log(c∕δ) +
O(d)) and n ≥
128κ2 log(2∕δ)
(σk-σk+ι)2
. Also
assume f* ∈ L∞ (X) ⊂ L2 (X) and let PTK∞ be the or-
thogonal projection onto the eigenspaces of TK∞ corresponding to the eigenvalue α ∈ σ(TK∞ )
and higher. Assume that (I — PTK∞ )f* ∞ ≤ E0 for some E0 ≥ 0. Pick k so that σk = α and
σk+1 < α, i.e. k is the index of the last repeated eigenvalue corresponding to α in the ordered
Sequence {σi}i. Also assume we are performing the doubling trick so that r(0) = —y. Then we
have with probability at least 1 — 3δ over the sampling of x1 , . . . , xn and θ0 that for t ≤ T
krtkRn ≤ exp(—λkt) kykRn + 4κ kf*k2,10lO√≡∙ + 2e0 + E.
(σk — σk+1) n
Proof. Recall that we have ∣∣r(0)kRn = k—y∣∣Rn ≤ kf*k∞. Note that m =
ΩΩ g-2dT2 kf*k∞ (1 + Tkf*k∞)2) and m ≥ O(log(c∕δ) + O(d)) are strong enough to ensure
51
Published as a conference paper at ICLR 2022
the hypothesis of Theorem D.2 is satisfied with Γ = 2. From now on Γ = 2 = O(1) and will be
treated as a constant. Then by Theorem D.2 with probability at least 1 - δ we have for t ≤ T
krt - exp(-GWt)r0kRn ≤ TkrOkRn O (√m [1 + T γ3 kr(0)kRn ]
Thus using the fact from the doubling trick that ∣∣r(0)∣∣Rn = ∣∣y∣∣Rn ≤ kf*k∞ setting m =
Ω G-2dT2 kf*k∞ (1 + Tkf*k∞)2) suffices to ensure that ∣∣rt - exp(-G∞t)ro∣∣Rn ≤ E for
t ≤ T. Let Pk be the orthogonal projection onto span{u1, . . . , uk}. Well then for t ≤ T
krtkRn ≤ kexp(-G∞t)r0kRn + E ≤ IlPk eXp( - G∞t) r0 k Rn + || (I - Pk) exp(-G∞t)r0 kRn + E
≤ exp(-λkt) IIrOkRn + Il(I - Pk )rθl∣Rn + E
By Theorem F.6 we have with probability at least 1 - 2δ over the sampling ofx1, . . . , xn that
k(I - Pk )y kRn ≤ 2E0 +
4κ kf*∣∣2 √10log(2∕δ)
(σk - σk+ι)√n
Since we are using the doubling trick we have ^ = -y. Thus we have
k(I - Pk )rokRn
≤ 2EO I 4κ kf*k2 /10log(2∕δ)
一	(σk - σk+ι)√n
Thus by taking a union bound we have with probability at least 1 - 3δ for all t ≤ T
’	…U	4κ kf*k2 √10log(2∕δ)	…
IIrtkRn ≤ exP(-λkt) IIrOkRn T--/-----------------1 +2E + E•
R	R	(σk - σk+1) n
The desired result follows from ro = -y.	□
F.2 Control of Initial Residual
We will use some of the notation and operator theory from Section C.1 and Section C.2 in this
section, thus it is recommended to have read those sections first. Let u1 , . . . , un denote the eigen-
vectors of G∞ normalized to have unit norm in k∙∣∣Rn, i.e. ∣∣uikRn = 1. Let Pk be the orthogonal
projection onto span{u1, . . . , uk}. The goal of this section is to upper bound the extent to which
the labels y participate in the bottom eigendirections of G∞, i.e. to show that k(I - Pk)ykRn
is small. Let PTK∞ be some projection onto the top eigenspaces of TK∞ . The idea is to show
that if Il(I - PTK∞ )f *∣∣2 is small then by picking Pk so that rank(Pk) = rank(PTK∞) then
k(I - Pk )ykRn is also small with high probability. The results in this section essentially all ap-
pear in the proofs in Su & Yang (2019). We repeat the arguments here for completeness and due to
differences in notation and constants.
We use some of the same machinery in (Rosasco et al., 2010). We define operators LH : H → H
and Tn : H → H by
THf :
hf,KsiHKsdρ(s)
X
1n
Tnf ：= n Ehf,KχlHKχi
i=1
Note that TH is equal to TK∞ on H and Tn is simply the operator you get if you replace ρ in
the defintion of TH with the empirical measure ɪ Pn=ι δχi. We define the “restriction” operator
Rn : H → Rn by
Rnf= [f(x1), f(x2),..., f(xn)]T
Note here the domain of Rn is H but in other parts of this paper we will allow Rn to take more
general functions as input. Define Rn : Rn → H by
1n
Rn(vi,...,Vn) = — EviKxi .
n i=1
52
Published as a conference paper at ICLR 2022
It can be seen that
hRnv, fiH = hv, RnfiRn .
and thus Rn is the adjoint of Rn. Using these operators We may write Tn = RnRn and G∞ =
RnRn. It will follow that Tn and G∞ have the same eigenvalues (UP to some zero eigenvalues) and
their eigenvectors are related. We recall the following result from (Rosasco et al., 2010) (Proposition
9)
Theorem F.1. (Rosasco et al., 2010) The following hold
•	The operator Tn is finite rank, self-adjoint and positive, and the matrix G∞ is symmetric
and semi-positive definite. In particular the spectrum σ(Tn) of Tn has finitely many non-
zero elements and they are contained in [0, κ].
•	The spectrum of Tn and G∞ are the same up to zero, specifically σ(G∞ ) \ {0} =
σ(Tn) \ {0}. Moreover if λi is a nonzero eigenvalue and ui and vi are the corresponding
eigenvector and eigenfunctionfor G∞ and Tn respectively (normalized to norm 1 in k∙∣∣Rn
and ∣∣∙∣∣h respectively), then
Ui =	1/2 Rnvi
λ1/2
1	11n
vi = 71/2 Rnui = 71/2 n Σ KXj (Ui) j
λi	λi n j=1
where (ui)j is the jth component of the vector ui.
• The following decompositions hold
k
G∞w =	λj hw, uj iRn uj
j=1
k
Tnf = X λj hf, vj iHvj
j=1
where k = rank(G∞) = rank(Tn) and both sums run over the positive eigenvalues.
{ui}ik=1 is an orthonormal basis for ker(G∞)⊥ and {vi}ik=1 is an orthonormal basis for
ker(Tn)⊥.
We will make use of the following lemma from Rosasco et al. (2010, Proposition 6):
Lemma F.2. (Rosasco et al., 2010) Let α1 > α2 > . . . > αN > αN+1 be the top N + 1 distinct
eigenvalues ofTH. Let PTH be the orthogonal projection onto the eigenfunctions ofTH correspond-
ing to eigenvalues αN and above. Let PTn be the projection onto the top k eigenvectors ofTn so
that k = dim(range(Tn)) = dim(range(TH)). Assume further that
αN - αN+1
kTH - TnkHS ≤ -------4-----.
Then
帜TH- PTiHS ≤ ∙ON-ON+ kTH - TnkHS .
The following lemma will be useful.
Lemma F.3. Let f * ∈ L2 and let PTH and PTn be defined as in Lemma F2. Then
XX KRnPTK∞ f*),uiiRn∣2 ≤ kf*k2 λk+1 IIPTH- PTnIlHS
σk
Proof. We repeat the same proof as in (Su & Yang, 2019) for completeness and to remove confusion
that may arise from differences in notation. The proof was originally given in (Rosasco et al., 2010)
albeit with a minor error involving missing multiplicative factors. Note that
k
PTK∞f* =Xhf*,φji2φj
j=1
53
Published as a conference paper at ICLR 2022
Therefore
k
(RnP TK∞ f *,U⅛n = Xhf *,φj2(Rnφj ,UiiKn .
j=i
Applying Cauchy-Schwarz We get
-k	]「k
∖hRnPTκ∞ f *,u⅛n |2 ≤ X |hf *,φj〉2 I2	X ∖hRnΦj ,u∙⅛n |2
j=i	_| [j=i
k
≤kf*k2 X IhRnΦj, UiiRn |2.
j=i
Well then note that
k	k	k
X ∖hRnφj ,uiiRn |2 = X Kφj,Rn UiiH|2 = X λi∖hφj ,viiH|2.
j=1	j=1	j=1
Therefore
n	n k
X IhRnPTK∞f*),UiiRn|2 ≤kf"2 X XλiKφj,ViiH|2
i=k+1	i=k+1j=1
nk
≤kf*k2kι X X他,。〉丸|2
i=k+1j=1
(12)
On the other hand
IIPTH-PTnIIHS ≥ XIl(PTH-PTn )√σ7 φ∕∣H
j=i
kn
≥ X X Ih(PTH- PTn)√σ-φj,3ih|2.
j=1i=k+1
Note that for 1 ≤ j ≤ k and k + 1 ≤ i ≤ n we have
h(PTH - PTn)√σ-φj,ViiH = hPTH √σ-φj,ViiH -hPTn√σ-φ-mH
=h√σ-φj,viiH - h√σ-φj,pTnPiH = h√σ-φj,viiH.
So
kn	kn
X X Ih(PTH-PTn)√σ-φ-,ViiH|2 = X X K√σ- φj ,ViiH|2
-=1 i=k+1	- = 1 i=k+1
kn
≥ σk X X Ihφ-,ViiHI2
-=1 i=k + 1
To summarize we have shown
ɪ HPTH
Qk
kn
-PTnUHS ≥ X X Ihφ-,ViiHI2.
-=1 i=k+1
Combining this with (12) we get the final result
n
E IhRnPTK∞ f *),UiiRn I2 ≤
i=k + 1
kf *k2 尤+1
Qk
IIPTH-PTn IIHs .
□
54
Published as a conference paper at ICLR 2022
We can use Lemma F.3 to produce the following bound.
Lemma F.4. Let f * ∈ L2 and let P TH and P Tn be defined as in Lemma F2. Then
n
|hRnf*,uiiRn|2
i=k+1
≤ 2 XX |(I - PTK∞ )f*(χi)∣2 + 2kf*k2 λk+1 ∣∣pTH - PTnIlHS.
n i=1	σk
Proof. We have that
hRnf*, uiiRn = hRn(I - PTK∞ )f* , uiiRn + hRnPTK∞ f* , uiiRn .
Thus from the inequality (a + b)2 ≤ 2(a2 + b2) we get
nn	n
X KRnf *, UiiRn I2 ≤ 2 X KRn(I-PTK∞ )f *, UiiRn I2 +2 X KRnPTK∞ f *,〃i〉Rn |2.
i=k+1	i=k+1	i=k+1
To control the first term we have
nn
X KRn(I- P TK∞ )f *, UiiRn I2 ≤∣∣(i - P TK∞ )f * IlRn = - X I(I - P TK∞ )f *(χi)I2.
i=k+1	i=1
Then by applying Lemma F.3 to the second term We get the desired result.	□
We recall the following lemma from Rosasco et al. (2010, Theorem 7)
Lemma F.5. (Rosasco et al., 2010) With probability at least - - δ over the sampling of x1, . . . , xn
kTH- TnkHS ≤ W2 詈 W .
HS	n
NoW finally We can provide a bound on the labels participation in the bottom eigendirections.
Theorem F.6. Assume f* ∈ L2 (X) and let PTK∞ be the orthogonal projection onto the
eigenspaces of TK∞ corresponding to the eigenvalue α ∈ σ(TK∞) and higher. Assume that
∣∣(I-PTK∞)f*∣∣∞≤0
for some 0 ≥ 0. Pick k so that σk = α and σk+1 < α, i.e. k is the index of the last repeated eigen-
value corresponding to α in the ordered sequence {σi }i. Let Pk denote the orthogonal projection
onto span{U1, . . . , Uk}. Finally assume
128κ2 log(2∕δ)
一(σk - σk+1)2
Then we have with probability at least - - 2δ over the sampling of x1, . . . , xn that
11ZI P、R f*∣∣	— [“I P、||	<9 0^ 4κ kf *k2 √10lθg(2∕δ)
k(I -	Pk)Rnf	IlRn	=	Il(I	- Pk)y∣∣Rn	≤ 2 +	(σfc	- σk+1)√η	.
Proof. From Lemma F.4 We have
nn
X KRnf *, UiiRn I2 ≤ - X ∣(I-PTK∞ )f *(Χi )|2 +2
i=k+1
i=1
kf* k22 λk+1
σk
∣∣PTH-PTn∣∣2HS
By assumption We have that the first term is bounded by 2(0)2. NoW We must control the term
2 kf *k2 λk+ι
σk
∣∣PTH-PTn∣∣2HS.
By Lemma F.5 We have With probability at least - - δ
MH- TnkHS ≤ Yg亘
55
Published as a conference paper at ICLR 2022
Then
128κ2 log(2∕δ)
一 (σk - σk+1)2
suffices so that the right hand side above is less than or equal to σk-σk+1. Thus by Lemma F.2 We
have that
IIPTH-PTnIlHS ≤ —ɪ- kTH - TnkHS ≤
σk - σk+1
2	2κ √2 log(2∕δ)
σk - σk+ι	√n
Thus from the above inequality We get that
2 kf *k2 λk+ι ii P TH _ P Tn ιι2	64κ2 If* k2 λk+ι log(2∕δ)
2	σk	11P -P 11hs ≤	σk(σk- σk+ι)2 ∙ n
By Proposition 10 in Rosasco et al. (2010) We have separately With probability at least 1 - δ
Note that
implies that
therefore
Thus
2κ √2log(2∕δ)
λk + 1 ≤ σk+1 +----7=-----.
n
128κ2 log(2∕δ)
n ~ (σk - σk+ι)2
1 ≤	σk - σk+1
√n - 8κP2log(2∕δ),
λk+ι ≤ σk+ι + 2κP2log(2/δ) ≤ σk + 1(σk - σk+ι) ≤ 5σk.
n4	4
2 kf *k2 λk+ι ii PTH _ PTn ιι2 ≤ 64κ2 kf *k2 λk+ι log(2∕δ) ≤ 80κ2 ∣f*∣∣2 log(2∕δ)
σk	"	hhs ~ σk(σk - σk+ι)2n	—	(σk - σk+ι)2n
Thus combined With our previous results We finally get that
n
k(I- Pk)Rnf*k2Rn = X |hRnf*,uiiRn|2
i=k+1
2 0 2	80κ2 kf *k2 log(2∕δ)
一 ( ) +	(σk- σk+ι)2n
ThUS from the inequality √a + b ≤ √2(√α + ʌ/b) which holds for all a,b ≥ 0 we have
k(I- Pk)Rnf*kRn ≤20+
4κ ∣∣f*∣∣2 /10log(2∕δ)
(σk - σk+ι)√n
Since y = Rnf* this provides the desired conclusion.
□
G TK∞ IS STRICTLY POSITIVE
Note that
K∞ (x, x0) = E[σ(hw, xi2+b)σ(hw, x0i2+b)]+E[a2σ0(hw, xi2+b)σ0(hw, x0i2+b)][hx, x0i2+1]+1
where the expectation is taken with respect to the parameter initialization. It suffices to show that
the kernel corresponding to the first term above
Ka(x, x0) := E[σ(hw, xi2 + b)σ(hw, x0i2 + b)]
induces a strictly positive operator TKa f (x) = X Ka(x, s)f (s)dρ(s). From the discussion in
Section C.1 it suffices to show that the RKHS corresponding to Ka is dense in L2 . In Proposition
4.1 in Rahimi & Recht (2008a) they showed that the RKHS associated with Ka has dense subset
F := < x → / a(w, b)σ(hw, x〉2 + b)dμ(w, b) : / |a(w, b)∣2dμ(w, b) < ∞
ΘΘ
where μ is the measure for the parameter initialization, i.e. (w, b) 〜μ. Since C(X) is dense in
L2(X) it suffices to show that F is dense in C(X) which is provided by the following theorem:
56
Published as a conference paper at ICLR 2022
Theorem G.1. Let Q be L-LiPschitz and not a polynomial. Assume that μ is a strictly positive
measure supported on all of Rd+1 . Also assume that
f
Rd+1
[kwk2 + kbk2]dμ(W,b)
< ∞.
Then F is dense in C(X) under the uniform norm.
Proof. We first show that F ⊂ C(X).	Suppose we have f ∈ F and write f(x)
t∫Rd+ι a(w, b)σ(hw, x〉2 + b)dμ(w, b). Well then
If (x) — f (x0)| = I /	a(w, b)[σ(hw,x>2 + b) - σ(hw,x0〉2 + b)]dμ(w, b)
Rd+1
≤ J	∣a(w, b)∣∣σ(hw, x〉2 + b) — σ(hw, x0〉2 + b)∣dμ(w, b)
≤	|a(w, b)|L|hw
Rd+1
x — x0i∣dμ(w, b) ≤
/
Rd+1
∣a(w, b)∣L ∣∣wk2 ∣∣x — x0∣∣2 dμ(w, b)
≤ L kx - x0k2
7
Rd+1
∣a(w, b) ∣2dμ(w, b)
1/2
J	∣∣w∣2 dμ(w, b)
1/2
Thus f is Lipschitz and thus continuous. Now suppose that F is not dense in C(X). Then by the
Riesz representation theorem there exists a nonzero signed measure ν(x) with finite total variation
such that X f (x)dν (x) = 0 for all f ∈ F. Well then writing f(x) = Rd+1 a(w, b)σ(hw, xi2 +
b)dμ(w, b) as before We have
a(w, b)σ(hw, x〉2 + b)dμ(w, b)dν(x) = 0
(13)
Note that
J	∣a(w, b)∣∣σ(hw, x〉2 + b)∣dμ(w, b)
≤ /	∣a(w, b)∣[∣σ(0)∣ + L∣hw,xi2 + b∣]dμ(w, b)
≤ /	|a(w, b)∣[∣σ(0)∣ + L(IlwI∣2 M + kb∣∣2)]dμ(w,b) < ∞
Where We have used Cauchy-SchWarz and the hypothesis on the integrability of∣w∣22 , ∣b∣22 in the
last step. Thus the integrand in (13) is μ X V integrable thus by FUbini's theorem we may interchange
the order of integration. To get that
a(w, b)
σ(hw, x〉2 + b)dν(x)dμ(w, b)
and the above holds for any a ∈ L2(Rd+1,μ). Thus JX σ(hw,x>2 + b)dν(x) = 0 for μ-almost every
w, b. However by essentially the same proof as when we showed F ⊂ C(X) we may show that
JX σ(hw, x〉2 + b)dν(x) = 0 is a continuous function of (w, b). Thus since μ is a strictly positive
measure on Rd+1 this implies that X σ(hw, xi2 + b)dν (x) = 0 for every (w, b) ∈ Rd+1. However
by Theorem 1 in Leshno et al. (1993) we have that span{σ(hw, xi2 + b) : (w, b) ∈ Rd+1} is dense
in C(X). However by our previous conclusion and linearity we have that g(x)dν(x) = 0 for any
g in span{σ(hw, xi2 +b) : (w, b) ∈ Rd+1}, which implies then that ν must equal 0. Thus F is dense
in C(X).	□
Since Gaussians are supported on all of Rd+1 we have the following corollary:
Corollary G.2. If (w, b)〜N(0, Id+ι) then K∞ is strictly positive.
57