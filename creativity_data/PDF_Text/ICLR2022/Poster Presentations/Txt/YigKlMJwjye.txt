Published as a conference paper at ICLR 2022
Generalized Demographic Parity for Group
Fairness
Zhimeng Jiang1, Xiaotian Han1, Chao Fan1, Fan Yang2, Ali Mostafavi1, and Xia Hu2
1 Texas A&M University, 2Rice University
Ab stract
This work aims to generalize demographic parity to continuous sensitive attributes
while preserving tractable computation. Current fairness metrics for continuous
sensitive attributes largely rely on intractable statistical independence between
variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information.
Statistical fairness metrics estimation relying on either tractable bounds or neural
network approximation, however, are not sufficiently trustful to rank algorithms
prediction bias due to lack of estimation accuracy guarantee. To make fairness
metrics trustable, We propose Generalized Demographic Parity (GDP), a group
fairness metric for continuous and discrete attributes. We show the understanding
of GDP from the probability perspective and theoretically reveal the connection
between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt
hard and soft group strategies via the one-hot or the soft group indicator, represent-
ing the membership of each sample in different groups of the sensitive attribute.
We provably and numerically show that the soft group strategy achieves a faster
estimation error convergence rate. Experiments show the better bias mitigation
performance of GDP regularizer, compared with adversarial debiasing, for regres-
sion and classification tasks in tabular and graph benchmarks 1.
1 Introduction
Fairness problem has attracted increasing attention in many high-stakes applications, such as credit
rating, insurance pricing and college admission (Mehrabi et al., 2021; Du et al., 2020; Bellamy
et al., 2018), the adopted machine learning models encode and even amplify societal biases toward
the group with different sensitive attributes. The majority of existing fairness metrics, such as de-
mographic parity (DP) (Feldman et al., 2015), equal odds (EO) (Hardt et al., 2016), presumably
consider discrete sensitive variables such as gender and race. In many real-world applications in-
cluding urban studies and mobility predictions (Tessum et al., 2021), however, individuals’ sensitive
attributes are unavailable due to privacy constraints. Instead, only aggregated attributes presenting
in continuous distributions are available, and thus fairness requires unbiased prediction over neigh-
borhood or region-level objects. Additionally, the sensitive attributes, such as age and weight, are
inherently continuous (Mary et al., 2019; Grari et al., IJCAI’20). The widely existing continuous
sensitive attributes stimulate further fairness metrics definition and bias mitigation methods.
Existing fairness metrics on continuous sensitive attributes rely on the statistical measurement of
independence, such as Hirschfeld-Gebelein-Renyi (HGR) maximal correlation coefficient (Mary
et al., 2019) and mutual information (Jha et al., 2021; Creager et al., 2019), which are computation-
intractable due to the involved functional optimization. Note that the mutual information involves
the ratio of probability density function, it is intractable to directly estimate mutual information
via probability density function estimation due to the sensitivity over probability density function,
especially for the low probability density value. Previous works (Roh et al., 2020; Lowy et al., 2021;
Cho et al., 2020) adopting mutual information or variations as regularizer, however, rely on tractable
bound, or computationally complex singular value decomposition operation (Mary et al., 2019), or
training-needed neural network approximation (Belghazi et al., 2018), such as Donsker-Varadhan
representation (Belghazi et al., 2018), variational bounds (Poole et al., 2019). Nevertheless, it is
1Codes are available at https://github.com/zhimengj0326/GDP
1
Published as a conference paper at ICLR 2022
Figure 1: Illustration of demographic parity definition for binary and quaternary sensitive attributes,
and generalized demographic parity for continuous sensitive attribute. Markers ■ ∙ * ♦ represent av-
erage prediction among specific discrete sensitive attributes, Red dashed line - - and blue solid line
γ∖j represent prediction average among all data and that with specific sensitive attribute. TV(∙, ∙)
represents weighted total variation distance.
unreliable to adopt the mathematical bound of fairness metric to evaluate different algorithms since
lower metrics bound does not necessarily imply lower prediction bias. A question is raised:
Can we extend DP for continuous attributes while preserving tractable computation?
In this work, we provide positive answers via proposing generalized demographic parity (GDP)
from regression perspective. Figure 1 provides an illustrative example for DP and GDP. The local
prediction average (blue solid curve r∖j) and the global prediction average (red dashed line --)
represent the average prediction value given sensitive attributes and the whole data samples, respec-
tively. The local and global prediction average should be consistent at any specific continuous sen-
sitive attributes. Therefore, we define GDP, via the weighted total variation distance, to measure the
distance between the local and global prediction average, where the weight is the probability density
of continuous sensitive attributes. We also theoretically demonstrate the equivalence of GDP and DP
for binary sensitive attributes, provide an understanding of GDP from probability perspective, and
reveal the bias mitigation methods connection between GDP regularizer and adversarial debiasing.
Although GDP is clearly defined on the unknown underlying joint distribution of prediction and
sensitive attributes, only data samples, in practice, are available. To this end, we propose two GDP
estimation methods, named histogram estimation (hard group strategy) and kernel estimation (soft
group strategy) methods, where kernel estimation is provable with faster estimation error conver-
gence rate w.r.t. data sample size. Specifically, histogram estimation manually divides continuous
sensitive attributes into several disjointed and complementary sensitive attributes bins, and then
each sample only belongs to one specific bin containing the sensitive attribute of the sample. In
other words, the group indicator of the sample is one-hot. As for kernel estimation, instead of quan-
tizing continuous sensitive attributes as several groups, the group indicator of the sample is treated
as a kernel function. In other words, to calculate the mean prediction value given specific sensi-
tive attributes, group smoothing strategy is adopted via a soft indicator determined by the sensitive
attribute distance between the sample sensitive attribute with target-specific sensitive attribute.
In short, the contributions of this paper are:
•	We develop a tractable group fairness metric GDP for continuous sensitive attributes. We theo-
retically justify GDP via demonstrating the equivalence with DP for binary sensitive attributes,
providing GDP understanding from probability perspective, and revealing the connection be-
tween GDP regularizer and adversarial debiasing.
•	We propose histogram and kernel GDP estimation and provably demonstrate the superiority of
kernel GDP estimation method with faster estimation error convergence rate w.r.t. sample size.
•	We experimentally evaluate the effectiveness and expansibility of GDP on different domain
benchmarks (e.g., tabular, graph, and temporal graph data), tasks (e.g, classification and re-
gression tasks), and compositional sensitive attributes.
2	Related Work
Machine Learning Fairness Fair machine learning targets bias mitigation for automated decision-
making systems. Various fairness definitions, such as group fairness and individual fairness, have
been proposed (Zemel et al., 2013). Group metrics, such as DP and EO, measure prediction dif-
2
Published as a conference paper at ICLR 2022
ference between the groups with different sensitive attributes such as gender, age (Louizos et al.,
2016; Hardt et al., 2016). While pre- and post-processing methods have been proposed for fairness
boosting, these methods can still lead to higher prediction bias (Barocas et al., 2017) compared with
in-processing methods, such as adding regularizer, adversarial debiasing and data augmentation. For
example, the covariance between the predictions and sensitive attributes regularization are imposed
to boost the independence in (Woodworth et al., 2017). (Zafar et al., 2017) constrains the deci-
sion boundaries of classifier to minimize prediction disparity between different groups. Adversarial
training has been originally proposed for deep generative modeling (Goodfellow et al., 2014)and
has been introduced for prediction debias in representation learning (Zhao et al., 2020; Beutel et al.,
2017; Louppe et al., 2017) and transfer learning (Madras et al., 2018). Data augmentation, such as
fair mixup (Chuang & Mroueh, 2020), can improve the generalization ability for fairness. Repre-
sentation neutralization is proposed to boost fairness without sensitive attribute (Du et al., 2021).
Kernel Density Estimation and Kernel Regression Kernel density estimation (KDE) is a non-
parametric method to estimate the continuous probability density function of a random variable
(Davis et al., 2011; Parzen, 1962). Given finite data samples, KDE smoothly estimate the probabil-
ity function via weighted summation, where the weight is determined via kernel function (Epanech-
nikov, 1969). Kernel regression is a non-parametric technique to estimate the conditional expecta-
tion of a random variable (Nadaraya, 1964). Nadaraya-Watson Kernel regression function estimator
is proposed for regression via locally normalized weighted average in (Bierens, 1988), where the
sample weight is determined by kernel function.
3	Generalized Demographic Parity
Without loss of generality, we consider a binary classification task to predict the output variable
Y given the input variable X, while avoiding prediction bias for sensitive attribute S. Define the
input X ∈ X ⊂ Rd, labels Y ∈ {0, 1}, and machine learning model f : Rd → [0, 1] provides
prediction score Y = f (X). Fairness requires predictor Y to be independent of sensitive attribute
S, regardless of continuous or discrete, i.e., P(Y = y) = P(Y = y|S = S) for any support value
y and s (Beutel et al., 2017). Since the independent constraint is difficult to optimize, the relaxed
demographic parity (DP) (Madras et al., 2018) metrics are proposed to quantitatively measure the
predictor bias for binary sensitive attribute S ∈ {0, 1}. Formally, the demographic parity is defined
as ∆DP = ∣Eγ[Y|S = 0] - EY[Y|S = 1]|, where EH represents variable expectation. For
categorical sensitive attribute S ∈ S, work (Cho et al., 2020) introduces a fairness metric, named
difference w.r.t. demographic parity (DDP) ∆DDP = Ps∈s ∣Eγ[Y|S = s] - Eγ[Y]∣.
Although DP has been widely used to evaluate the prediction bias, it is still inapplicable for con-
tinuous sensitive attributes since the data samples cannot be directly divided into several distinctive
groups based on the sensitive attributes. Without loss of generality, we assume continuous sensitive
attributes S ∈ [0, 1] and propose GDP to extend tractable DP for continuous sensitive attributes.
Assume the joint distribution of tuple (S, Y) is PSY (s, y), the local prediction average and global
prediction average are defined as the prediction expectation given sensitive attribute S = s and
without any sensitive attribute condition, i.e., local prediction average m(s) 4 E[Y|S = s] and
4
global prediction average mavg = ES [m(S)] = E[Y], respectively. Then, we adopt weighted total
variation distance on local prediction average and global prediction average, where the weight is
specified by the probability density function of the sensitive attribute. The formal definition of the
discrepancy demographic parity for continuous sensitive attributes is as follows:
∆GDP
m(s) - mavgPS(S = s)ds
ES [|m(S) - mavg|],
(1)
We also provide the connection of GDP and DP for binary sensitive attributes, which implies that
GDP is equivalent to DP for binary sensitive attributes, as follows:
Theorem 1 (Connection between DP and GDP). For binary sensitive attribute S ∈ {0, 1}, GPD
and DP are equivalent except the coefficient only dependent on datasets. Specifically, the relation of
∆GDP and ∆DP satisfies ∆GDP = 2Ps(S = 1) ∙ PS(S = 0) ∙ ∆DP.
The proof of Theorem 1 is presented in Appendix A. For categorical sensitive attributes, it is easy to
obtain that ∆GDP = PssES PS(S = s)∣Eγ[Y|S = s] - Eγ[Y]∣, i.e., GDP is weighted DDP for
3
Published as a conference paper at ICLR 2022
categorical sensitive attributes. In a nutshell, GDP is a natural fairness metric extension for binary
and categorical sensitive attributes. Since the independence between prediction Y and sensitive at-
tribute S implies that thejoint distribution PSY (s, y) and product marginal distribution PS (S)PY (y)
are the same, the bias can be measured by the distance of the joint distribution and product marginal
distribution. Subsequently, we show the connection of GDP and prediction-weighted total variation
distance between these two distributions as follows:
Theorem 2 (Probability View of GDP). Assume the joint distribution of (Y , S) with support
[0,1]2 is PSY(s, y). Define the prediction-weighted total variation distance as TVpred(P 1,P2) 4
R1 R1 y|P 1(y, S) — P2(y, s)∣dyds. Then the proposed GDPfor continuous Senstive attribute is
upper bounded by prediction- weighted total variation distance between the joint distribution and
product marginal distribution:
∆GDP
L1L 1∣yhPs,γ (s,y)—Ps (s)Pγ(y)i|
dyds ≤ TVpred
(PSY (s,y),Ps (S)PY (y)).
The proof of Theorem 2 is presented in Appendix B. Theorem 2 demonstrates that GDP is actually
a lower bound for prediction-weighted total variation distance between these two distributions and
implies the necessity of GDP for bias measurement.
4	GDP Estimation
GDP is defined based on the underlying joint distribution PSY(s,y) of tuple (S, Y), where
S,Y ∈ [0,1]. The underlying joint distribution, however, is unknown. Thus, We aim to estimate
GDP given samples {(sn, yn), n ∈ [N]}, where [N] 4 {1,…，N}. To bridge this gap, We propose
histogram GDP estimation and kernel GDP estimation methods based on different group strategies.
Specifically, histogram GDP estimation hardly groups data samples via creating consecutive, non-
overlapping intervals bins, and the local prediction average is estimated by the average prediction
among the data samples in the bin. As for kernel GDP estimation, a soft group indicator, determined
by the kernel function and sensitive attribute distance, is adopted to provide group smoothing strat-
egy. Specifically, the local prediction average for target sensitive attribute is calculated via weighted
prediction, where the sample with sensitive attribute close to the target sensitive attribute possesses
large weight.
Histogram GDP Estimation A histogram is originally an approximate representation of the un-
derlying data probability distribution via creating several consecutive, non-overlapping intervals or
bins with, usually but not required, equal bandwidth. In this paper, we assume all bins with equal
bandwidth h and the number of bins Nh = ɪ is an integer. In other words, the data tuples can
be divided into Nh groups, and the bin intervals are given by Bi = [0, h), B2 = [h, 2h), .…，
BNh = [(Nh — 1)h, 1]. Define indicator function I(A) as 1 if event A happens, otherwise is 0.
Thereby, the group indicator wh(n, i) of sample (sn,yn) for i — th bin is given by I(Sn ∈ Bi). Note
that all bin intervals are complementary; each sample belongs one and only one bin, i.e., the indi-
cator vectors wh(n) = [wh(n, 1),…，wh(n, Nh)] for sample n is one-hot, which formally define
the hard group strategy. The local prediction average and probability of sensitive attribute can be
point-wisely estimated based on the empirical expectation and distribution. Specifically, for S ∈ Bi,
the local and global prediction average are given by
NN
mh (S ∈ Bi) = X I(Sn ∈ Bi)yi, for i ∈ [Nh];	mhvg = X yi.	⑵
n=1	n=1
Similarly, the probability of sensitive attribute is given by PS(S ∈ Bi) = P孔=1 INsn∈Bi). Finally,
we combine all estimations to calculate prediction bias as follows:
Nh
∆GDP(h) = X ∣mh (S ∈ Bi) — mhvgIPh(S ∈ Bi).	(3)
i=1
Kernel GDP Estimation Histogram GDP estimation provides a hard group strategy via creating
several bins. However, a tiny sensitive attribute perturbation can lead to different group indicators
4
Published as a conference paper at ICLR 2022
and thus histogram estimation is not robust on sensitive attribute. On the other hand, a data tuple
not necessarily belongs to one group for continuous attributes. For example, the data sample with
s = 0.5 may have the same probability belonging to target sensitive attributes s = 0.4 and s = 0.6.
Based on these observations, we propose kernel GDP estimation via group smoothing. Intuitively,
when calculating local prediction average or probability density function for target sensitive at-
tribute, the tuple with more close attribute is entrusted higher weight. Specifically, we introduce a
symmetric one-dimensional smoothing kernel function K(s) ≥ 0 satisfying normalized condition
R K (s)ds = 1, symmetry condition R sK (s)ds = 0 and finite variance σK2 =4 R s2 K (s)ds > 0.
Define h > 0 as the kernel bandwidth. For target sensitive attribute s, the tuple weight for sample
with sensitive attribute Sn is given by w(sn, S) = 1K(lsn^sl). In short, kernel function provides
the group smoothing strategy based on sensitive attribute distance of tuple pair.
Given the smoothing kernel function K(S), the local and global prediction average can be obtained
via normalized weighted average (Nadaraya-Watson kernel estimator) as follows:
m h(s)
P= ynK( W )
PLl κ (STs)
h
m avg
1 yn
N-
(4)
Similarly, the probability of sensitive attribute is given by PS(s)= 忐 PN=I K(SnhLs). Finally,
we combine all estimations to calculate kernel GDP estimation as follows:
~ ,.
∆ GDP (h)
∕1 Im h(s) —
rm hvg∖pS (s)ds.
(5)
Estimation Error Analysis We provide the theoretical analysis on GDP estimation error and prove
the superiority of kernel GDP estimation. Assume that each data sample is independent and identi-
cally distributed random variables. Therefore, the estimated GDP is still a random variable, and we
adopt the expectation of the mean squared error (MSE) to quantify the accuracy of the estimation
method. Formally, the error of histogram estimation and kernel estimation are given by
Errhist = E[∆GDP - ∆GDP|2];	Errkernel = E[∣∆GDP - ∆GDP|2].	(6)
where the expectation is taken across N tuples. Here, we provide an asymptotic analysis on estima-
tion error and show the superiority of kernel GDP estimation in the following:
Theorem 3 (Estimation Error Convergence Rate). Assume that the mean prediction function m(s),
given sensitive attribute s, is smooth and satisfies L- Lipschitz condition 2 on local average |m(s) -
m(s0)| ≤ L|s - s0 | for any s, s0. the optimal bandwidth choice for histogram and kernel estimation
methods are hhist =O(N-3) and hkernel =O(N-1). Additionally, the estimation error satisfy
c/nr_2、	c/nr_4、
Errhist =O(N 3) and Errkernel =O(N 5), where OG) is big O notation.
The proof of Theorem 3 is presented in Appendix C. The proof sketch is to separately provide upper
bounds for the estimation error of local average and sensitive attribute probability density estimation.
Finally, we combine these two estimations to obtain the estimation error for GDP.
Computation Complexity Analysis Since GDP calculation involves in integral operations, the ap-
proximated numerical integration is usually adopted with M probing sensitive attribute. The com-
plexity to calculate local prediction average and probability density at M probing sensitive attributes
are O(MN) and thus, the complexity for histogram and kernel estimation both are O(MN). In
(Mary et al., 2019), intractable HGR coefficient equals second large singular value of distribution
ratio matrix or can be upper bounded by tractable chi-square distance between the joint distribution
and marginal product distribution. Assume that there are M probing prediction, the complexity
for two-dimensional probability density function estimation and SVD is O(M2N) and O(M 3),
and that of the chi-square distance is O(M 2N). Therefore, the computation complexity for HGR
is O(M2 (M + N)). As for the neural based approximation for HGR or mutual information, the
complexity for training is quite large compared with directly computation.
2Lipschitz condition requires bounded gradient w.r.t. sensitive attribute and guarantees the rationale local
average estimation based on neighbor data samples.
5
Published as a conference paper at ICLR 2022
5 Analysis on GDP Regularizer and Adversarial Debiasing
With GDP bias measurement for continuous sensitive attributes, it is natural to add GDP regularizer
to enforce fairness. Another bias mitigation is adversarial debiasing, a two-player framework for
predictor and adversary with regression task. We establish the connection between GDP regular-
izer and adversarial debiasing, and demonstrate that adversarial debiasing with specific adversary
regression objective is actually minimizing GDP implicitly.
GDP Regularizer: As a reminder, our goal is to learn a predictor Y = f (X) that approximates
the label Y for each input while reducing the prediction bias ∆GDP w.r.t. continuous sensitive
attributes S. It is natural to add GDP regularization to enforce fairness. Given the prediction loss
Lpred, the fairness-enforcing objective function is min Lpred f(X), Y +λ∆GDP, where L could
be regression or classification task loss and λ is the hyperparameter to control the trade-off between
the prediction performance and prediction bias reduction.
Adversarial Debiasing: Adversarial debiasing is another natural method to ensure fair prediction
via a two-player game between predictor and adversary. Specifically, the predictor f yields pre-
diction Y , and given prediction Y , the adversary g tries to predict continuous sensitive attributes
S = g(Y ) in regression task. Similar to adversarial debiasing (Louppe et al., 2017), we adopt
the same classifier and adversary structure, where the input of adversary is the output of the clas-
sifier. For objective function of adversary, we adopted the utility function proposed in (Madras
et al., 2018) 3 to represent sensitive attribute prediction accuracy. In this case, the predictor aims to
generate accurate prediction and fool the adversarial simultaneously, while adversary targets high
utility for accurate sensitive attribute prediction. Let Lpred denote the prediction loss and Ladv
represent the adversarial utility. Then adversarial debiasing is trained following the min-max proce-
dure minmax Lpred (f(X ),Y) + λL0dv (g(f(X )),S ). Define g* = arg min Ladv (g(f(X )),S)
as the optimal adversarial given predictor f, the min-max procedure is simplified to the objective
function min Lpred (f (X ),Y) + λL0dv (g* (f (X )),S).
Theoretical Connection: We provide an inherent connection between GDP regularizer
and adversarial debiasing. Specifically, we demonstrate that the optimal adversarial utility
Ladv (g* (f (X)),S)is actually the upper bound of GDP ∆GDP as long as the utility function
in adversary is Ladv(S, S) = 1 - |S - S| in the following theorem:
Theorem 4 (GDP and Adversarial Debiasing Connection). Considering a predictor Y = f (X) and
adversary S = g(Y), given adversary utility Ladv (S, S) = 1 - |S - S| and optimal adversary g*,
then GDP ∆GDP is bounded by the utility function Ladv (g* (f (X )),S)With optimal adversary,
i.e., Ladv (g*(f (X)),S) ≥ ∆GDP.
The proof of Theorem 4 is presented in Appendix E. Intuitively, a fair classifier aims to minimize
the adversary utility to make prediction fair, and thus induces lower GDP metric. Theorem 4 reveals
the inherent connection between GDP regularizer and adversarial debiasing: adversarial debiasing
behaviors like predictor loss optimization with GDP regularization, as long as the adversary is op-
timal. Additionally, such connection not only holds for underlying data distribution, but also for
empirical data distribution. The proof sketch is similar via replacing underlying data distribution as
empirical data distribution in the expectation operation. In practice, the alternative optimization is
usually adopted in adversarial debiasing via alternatively updating either predictor or adversary at
each training step while keeping the other one fixed.
3Work (Madras et al., 2018) adopts encoder-decoder structure for transferable representation learning. We
only adopt utility function to connect GDP and adversarial debiasing (Louppe et al., 2017).
6
Published as a conference paper at ICLR 2022
6	Experiments
We evaluate the effectiveness and expansibility of GDP. First, we show the lower GDP estimation
error for kernel GDP estimation 4 with group smoothing, compared to that of histogram, via two
synthetic experiments. We empirically evaluate the effectiveness and expansibility of kernel estima-
tion for multiple prediction tasks, including classification and regression tasks, and multiple domain
real-world datasets, including tabular, graph and temporal graph data (See Appendix H.1). More-
over, the kernel estimation is also applicable for compositional continuous sensitive attributes. For
a fair comparison, we compare our method kernel, adding kernel GDP estimation as regularization,
with (a) vanilla: training with empirical risk minimization (ERM) without any regularization; (b)
histogram: histogram estimation with continuous sensitive attribute; (c) adv: adversarial debiasing
(Louppe et al., 2017); (d) adv-bn: adversarial debiasing with binary-quantized sensitive attribute;
(e) hgr: the upper bound of HGR as regularizer (Mary et al., 2019); and (f)hgr-bn: the upper bound
of HGR as regularizer with binary-quantized sensitive attribute. Specifically, we demonstrate the
trade-off between prediction performance and GDP by varying the hyper-parameter λ. In particular,
we adopt accuracy (Acc) for classification task and mean absolute error (MAE) for regression task.
Details about the baselines and experimental setups for each dataset can be found in Appendix H.2.
6.1	Synthetic Experiments
We test GDP estimation error and investigate the robustness for histogram and kernel estimation via
two synthetic experiments. The goal is to investigate the effect of bandwidth choice and number
of samples on GDP estimation error. For data generation, we first generate bivariate Gaussian
distribution N(μ, Σ) for samples (S, Y), where expectation μ = [0.5,1.0] and covariance matrix
.Additionally, we generate the samples based on PSY(s,y) = S + y if 0 ≤ s,y ≤
1,	via acceptance-rejection method (Wells et al., 2004). For estimation, we select two typical kernel
functions (tricube and Aitchison-Aitken kernel (Mussa, 2013)) for local prediction average, and two
kernel functions (linear and cosine kernel functions) for probability density function estimation.
Figure 2 shows the GDP estimation error w.r.t. bandwidth h and number of samples N for bivari-
ate Gaussian distribution and second synthetic bivariate distribution, respectively. We observe that
histogram GDP estimation error is highly sensitive to bandwidth choice, while kernel estimation is
robust to bandwidth and kernel function choice due to the flexibility of group smoothing. As for er-
ror rate with number of samples, our experiments show the error rate curve for histogram and kernel
estimation. It is seen that kernel estimation can achieve the fast error convergence rate if searching
the optimal bandwidth for each method. In addition, the estimation error result is almost the same
for different kernel functions, which supports the theoretical results in Theorem 3.
1	0.5
0.5 2.0
Figure 2: Demographic parity estimation error analysis with respect to bandwidth and number of
samples for Gaussian distribution and second synthetic distributions.
6.2	Experiments on Tabular data
Dataset:We consider two benchmark tabular datasets, UCI Adult and Crimes, to evaluate the effec-
tiveness of kernel estimation for classification and regression tasks. UCI Adult dataset 5 contains
more than 40, 000 individual information from 1994 US Census. The classification task is to predict
4Since there is not ground truth on underlying data distribution, we adopt reliable kernel-based estimated
GDP as fairness metric in real-world datasets.
5https://archive.ics.uci.edu/ml/datasets/adult
7
Published as a conference paper at ICLR 2022
whether a person’s income exceeds $50k/yr (KOHAVI, 1996). We consider normalized age 6 sen-
sitive attribute to measure the fairness of algorithms. The Crime dataset 7 includes 128 attributes for
1, 994 samples from communities in the US. The regression task is to predict the number of violent
crimes per population in US communities. We adopt the black group ratio as continuous sensitive
attribute. Model: We adopt two-layer selu networks model (Klambauer et al., 2017) with hidden
size 50 and report the mean prediction performance and GDP with 5 running times.
GDP
(a) Adult dataset
⅜	vanilla	—f— histogram	—ady-bn	—⅛-	hgr_bn
—φ-	kernel	—φ- adv	—f—	hgr
(b) Crimes Dataset
Figure 3: Mitigation performance on tabular dataset with kernel estimation and other baselines. (a)
The tradeoff between Accuracy (classification) and GDP for Adult dataset; (b) The tradeoff between
MAE (regression) and GDP for Crimes dataset.
Results: We compare the bias mitigation performance, i.e., the tradeoff between prediction perfor-
mance and GDP, of kernel estimation with other baselines for the two tabular datasets in Figure 3.
The hyper-parameter λ in Eq. (5) controls the tradeoff between prediction performance and GDP.
Specifically, we choose accuracy metric for classification task in Adult dataset, and MAE metric
for regression task in Crimes datset. For adversarial debiasing, we vary the regularization weights
to obtain the tradeoff curve. Overall, we make the following observations: (a) kernel estimation
outperforms all other baseline methods in terms of performance-fairness tradeoff curve for Adult
and Crimes datasets. Specifically, kernel estimation can achieve more than 1% accuracy improve-
ment if GDP is lower than 0.02 for adult dataset, and more than 2% MAE reduction for Crimes
dataset; (b) kernel estimation has lower computation complexity compared with adversarial debias-
ing. Kernel estimation and histogram decrease more than 50% running time on an average across
all tabular dataset, which makes kernel and histogram estimation readily usable for large scale real-
world datasets; (c) The histogram estimation and binary-quatized sensitive attribute would lead to
mitigation performance drop for kernel estimation, adversarial debiasing and HGR. This fact implies
the importance of order information in continuous sensitive attributes for bias mitigation. In addi-
tion, we observe adversarial debiasing training and HGR are not stable and large hyper-parameter
unnecessarily leads to bias mitigation.
6.3	Experiments on Graph data
Dataset: We consider two real-world graph datasets, Pokec-z and Pokec-n, sampled from a larger
one Facebook-like social network Pokec in Slovakia. User profiles contain gender, age, interest, ed-
ucation, working field and etc. We treat the normalized age as the continuous sensitive attributes and
the node classification task is to predict the working field of the users. Model: We use three graph
neural network backbones, graph convolutional networks (GCN) (Kipf & Welling, 2017), graph
attention networks (GAT) (VelickoVic et al., 2018) and Simplifying graph convolutional networks
(SGC) (Wu et al., 2019) with 64 feature dimensions. We train GNN with 200 epochs with 5 running
times and report the average accuracy and GDP. In each trial, the dataset is randomly split into a
training, validation, and test set with 50%, 25%, and 25% partition, respectively.
Results: We compare the mitigation performance of kernel estimation with other baselines for two
datasets with three backbones in Figure 4. Similarly, kernel estimation consistently outperforms the
other baselines by a large margin and binary-quantized sensitive attributes inevitably deteriorate the
mitigation performance. Another observation is that GDP can be reduced at least 80% at the cost
6The biggest difference between continuous and discrete sensitive attribute falls in the existence of order
information (Mary et al., 2019) for attribute values.
7https://archive.ics.uci.edu/ml/datasets/communities+and+crime
8
Published as a conference paper at ICLR 2022
of 2% accuracy for kernel estimation in two datasets and three backbones, while results in larger
accuracy drop for other baselines. Additionally, adversarial debiasing and HGR are also unstable
during training and higher hyper-parameter may lead to larger GDP.
GCN
GAT
* vanilla -⅜- histogram	—⅛- adv_bn —hgr-bn <⅜ vanilla -4- histogram -⅛- adv-bn -⅛- hgr_bn
GDP	GDP
SGC
* vanilla	—∣-	histogram	—f—	adv_bn	—f—	hgr_bn
-⅛- kernel	—⅜-	adv	—f-	hgr
GDP
，VanilIa + histogram + adv bn + hgr bn * *≡nllla 干 histogram T- adv.bn T- hg∏bn
+ kernel 十 adv + hgr"	^	+ keme' + adv + h≡r
4 vanilla	—+- histogram	—⅛- adv_bn	—hgr_bn
-⅛- kernel	—f- adv	—f- hgr
Figure 4: Mitigation performance of GCN, GAT and SGC for Pokec-n and Pokec-z dataset.
6.4	Experiments on Compositional Continuous Sensitive Attributes
Dataset: Similarly, the same two benchmark tabular datasets are adopted to evaluate the effective-
ness of kernel estimation for compositional continuous sensitive attributes. Specifically, we treat the
normalized age and education number for UCI dataset, and black group ratio and normalized age
for Crimes dataset as compositional continuous sensitive attributes. to evaluate bias mitigation per-
formance. Model: We also adopt two-layer selu networks model with hidden size 50 with 5 running
times and report the average mean prediction performance and GDP.
Results: Figure 5 shows bias mitigation performance between prediction performance and GDP.
Again, kernel estimation consistently achieves a better tradeoff compared with all other baselines,
and binary-quantized compositional sensitive attribute leads to mitigation performance drop.
(a) Adult dataset
(b) Crimes Dataset
Figure 5: Mitigation performance for compositional sensitive attributes.
7	Conclusion
We generalize demographic parity fairness metric, named GDP, to continuous sensitive attributes
while preserving tractable computation. We theoretically justify the unification of proposed GDP
for continuous and discrete sensitive attributes, and show the necessity of GDP via demonstrating
the connection with joint and product margin distributions distance. We also propose two GDP
estimation methods, named histogram and kernel, with linear computation complexity via hard and
soft group strategies, and provide corresponding estimation error analysis. For the superiority of
kernel estimation, we provably demonstrate the faster estimation error convergence rate compared
with histogram estimation, and experimentally show better bias mitigation performances in multiple
domains, multiple tasks and compositional sensitive attributes.
9
Published as a conference paper at ICLR 2022
8	Acknowledgements
We would like to sincerely thank everyone who has provided their generous feedback for this work.
Thank the anonymous reviewers for their thorough comments and suggestions. This work was
supported in part by X-Grant project, National Science Foundation IIS-1939716, IIS-1900990, IIS-
1750074 grant, and JPMorgan Faculty Research Award.
References
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial,
1:2017, 2017.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning,pp. 531-540. PMLR, 2018.
Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya
Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. Ai fair-
ness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic
bias. arXiv preprint arXiv:1810.01943, 2018.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
HJ Bierens. The nadaraya-watson kernel regression function estimator. Faculty of Economics and
Business Administration, Vrije Universiteit Amsterdam Serie Research Memoranda, (1988-58),
1988.
US Census Bureau. American community survey 5-year data, 2021. URL https:
//www.census.gov/programs-surveys/acs/technical-documentation/
table- and- geography- changes/2018/5- year.html.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785-794,
2016.
Jaewoong Cho, Gyeongjo Hwang, and Changho Suh. A fair classifier using kernel density estima-
tion. Advances in Neural Information Processing Systems, 33:15088-15099, 2020.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In International
Conference on Learning Representations, 2020.
Elliot Creager, David Madras, Jorn-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi,
and Richard Zemel. Flexibly fair representation learning by disentanglement. In International
conference on machine learning, pp. 1436-1445. PMLR, 2019.
Cuebiq. Data for good: Location intelligence for good is our contribution to the scientific commu-
nity, 2021. URL https://www.cuebiq.com/about/data- for- good/.
da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. Inductive representation
learning on temporal graphs. In International Conference on Learning Representations, 2020.
Richard A Davis, Keh-Shin Lii, and Dimitris N Politis. Remarks on some nonparametric estimates
of a density function. In Selected Works of Murray Rosenblatt, pp. 95-100. Springer, 2011.
Mengnan Du, Fan Yang, Na Zou, and Xia Hu. Fairness in deep learning: A computational perspec-
tive. IEEE Intelligent Systems, 2020.
Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Hassan Awadallah,
and Xia Hu. Fairness via representation neutralization. arXiv preprint arXiv:2106.12674, 2021.
Vassiliy A Epanechnikov. Non-parametric estimation of a multivariate probability density. Theory
of Probability & Its Applications, 14(1):153-158, 1969.
10
Published as a conference paper at ICLR 2022
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD
international conference on knowledge discovery and data mining, pp. 259-268, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Vincent Grari, Sylvain Lamprier, and Marcin Detyniecki. Fairness-aware neural renyi minimization
for continuous features. In Proceedings of the 29th International Joint Conference on Artificial
Intelligence, IJCAI’20.
Xiaotian Han, Zhimeng Jiang, Ninghao Liu, Qingquan Song, Jundong Li, and Xia Hu. Geometric
graph representation learning via maximizing rate reduction. In Proceedings of the Web Confer-
ence, 2022.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing systems, 29:3315-3323, 2016.
Akshita Jha, Bhanukiran Vinzamuri, and Chandan K Reddy. Fair representation learning using
interpolation enabled disentanglement. arXiv preprint arXiv:2108.00295, 2021.
Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and Xia Hu. FMP:
Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Proceedings of the 31st international conference on neural information pro-
cessing systems, pp. 972-981, 2017.
R KOHAVI. Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In Second
International Conference on Knowledge Discovery and Data Mining, 1996, pp. 202-207, 1996.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S Zemel. The variational fair
autoencoder. In ICLR, 2016.
Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with adversarial networks. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
982-991, 2017.
Andrew Lowy, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and Ahmad Beirami. Fermi:
Fair empirical risk minimization via exponential r\’enyi mutual information. arXiv preprint
arXiv:2102.12586, 2021.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pp. 3384-3393.
PMLR, 2018.
Jeremie Mary, Clement Calauzenes, and Noureddine El Karoui. Fairness-aware learning for contin-
uous attributes and treatments. In International Conference on Machine Learning, pp. 4382-4391.
PMLR, 2019.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1-35, 2021.
Hamse Y Mussa. The aitchison and aitken kernel function revisited. Journal of Mathematics Re-
search, 5(1):22, 2013.
Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):
141-142, 1964.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065-1076, 1962.
11
Published as a conference paper at ICLR 2022
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171-
5180. PMLR, 2019.
Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh. Fr-train: A mutual information-based
approach to fair and robust training. In International Conference on Machine Learning, pp. 8147-
8157. PMLR, 2020.
Christopher W. Tessum, David A. Paolella, Sarah E. Chambliss, Joshua S. Apte, Jason D. Hill, and
Julian D. Marshall. PmisUbZ2.5j/sub% polluters disproportionately and systemically affect people
of color in the united states. Science Advances, 7(18):eabf4491, 2021. doi: 10.1126/sciadv.
abf4491. URL https://www.science.org/doi/abs/10.1126/sciadv.abf4491.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Martin T Wells, George Casella, and Christian P Robert. Generalized accept-reject sampling
schemes. In A Festschrift for Herman Rubin, pp. 342-347. Institute of Mathematical Statistics,
2004.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. In Conference on Learning Theory, pp. 1920-1953. PMLR, 2017.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph convolutional networks. In International conference on machine learning, pp.
6861-6871. PMLR, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962-970. PMLR, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International conference on machine learning, pp. 325-333. PMLR, 2013.
Han Zhao, Jianfeng Chi, Yuan Tian, and Geoffrey J Gordon. Trade-offs and guarantees of adversarial
representation learning for information obfuscation. Advances in Neural Information Processing
Systems, 33, 2020.
Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li. T-gcn:
A temporal graph convolutional network for traffic prediction. IEEE Transactions on Intelligent
Transportation Systems, 21(9):3848-3858, 2019.
12
Published as a conference paper at ICLR 2022
A Proof of Theorem 1
For binary sensitive attribute S ∈ {0, 1}, the probability of sensitive attribute follows Bernoulli
distribution PS(S = 0), PS(S = 1) . Therefore, the global prediction average is given by mavg =
PS(S = 0)m(0) +PS(S = 1)m(1) and GDP is
∆GDP = PS(S = 0)m(0) - mavg + PS(S = 1)m(1) - mavg
= PS(S = 0)m(0) - PS(S = 0)m(0) - PS(S = 1)m(1)
+PS(S= 1)PS(S=0)m(0)+PS(S= 1)m(1) - m(1)
= 2PS(S = 0)PS(S = 1)|m(0) - m(1)|
= 2PS(S = 0)PS(S = 1)∆DP.
where the coefficient 2PS (S = 0)PS (S = 1) only depends on data. This fact demonstrates the
applicability of GDP for categorical sensitive attribute with discrete measure choice for sensitive
attributes, and it is equivalent to demographic parity for binary sensitive attribute.
B Proof of Theorem 2
Notice that the fairness constraint ideally requires the independence of prediction Y and sensi-
tive attribute S, i.e., the joint distribution and product margin distribution equals: PY S(y, S)=
PS(S)PY (y). Aiming to measure independence deviation, a natural intuition is to quantify the prob-
ability deviation via the distance between the joint distribution and product margin distribution. We
show the connection between the proposed GDP and prediction-weighted total variation distance of
these two distributions. Recall the definition of GDP, it is easy to obtain
∆GDP
≤
m(S) - mavg PS (S = S)dS
/ ∣/ yPY∣s(y|s)dy - / yPY(y)dy∣Ps(S =S)ds
(PY ,s (y, S)- PS (S)PY (y))dy∣dyds
(y,S)- PS (S)PY (O)IdydS
TVy(PY ,S (O,s),Ps (s)Py (y).
which completes the proof.
C	Proof of Theorem 3
Notice that histogram and kernel estimation require local prediction average and probability density
function estimation for sensitive attributes, we start with the analysis on local prediction average and
probability density function estimation, and then provide the proof of GDP error analysis.
C.1 Proof for Histogram Estimation
Recall that the number of bins is Nh with same bandwidth h, and bins interval are given by B1 =
[0, h), B2 = [h, 2h),…，BNh = [(Nh - 1)h, 1]. Firstly, We will separately analyze the error for
local prediction average and sensitive attribute probability. Next, these two estimation error results
can be combined for GDP estimation error.
13
Published as a conference paper at ICLR 2022
Since the continuous sensitive attributes is considered, we defined the probability density function
of sensitive attribute S as pS (s). The estimated probability density function is given by:
1	1N
^(S) = τPS(S ∈ Bi)=行 EI(Sn ∈ Bi) for S ∈ Bi；	(7)
h	Nh
n=1
Subsequently, we define the pointwise MSE error M S Ehpidsft (S) to measure probability density func-
tion estimation error given sensitive attribute S as follows:
MSEhst ⑸=E[∣P^S (s) - PS (S)12]，	(8)
where the expectation is took across N samples. Then we have following Lemma 1 on optimal
pointwise MSE error for probability density function:
Lemma 1. Assume the mean prediction function m(S), given sensitive attribute S, is smooth and
satisfies L-Lipschitz condition |m(S) - m(S0)| ≤ L|S - S0 | for any S, S0. Given N i.i.d. samples
{(yn, Sn), n ∈ [N]}, then the optimaIMSEerror min MSEhdst(S) is O(N-2), where the optimal
h
bandwidth satisfies h = O(N- 1) for any sensitive attribute S.
As for the local prediction average, similarly, we have the estimated local prediction average as
follows:
t^(s)
PnN=I I(Sn ∈ Bi)^
Pn=I I(Sn ∈ Bi)
for S ∈ Bi ；
(9)
Subsequently, we also define the pointwise MSE error M SEhriesgt (S) to measure local prediction
average estimation error given sensitive attribute S as follows:
MSEhiSt(S) = E[∣mh(S) - m(S)l2],
(10)
where the expectation is took across N samples. Then we have following Lemma 2 on optimal
pointwise MSE error for local prediction average:
Lemma 2. Assume the mean prediction function m(S), given sensitive attribute S, is smooth and
satisfies L-Lipschitz condition |m(S)	-	m(S0)|	≤	L|S	-	S0 |	for any S, S0.	Define the bounded
prediction variance σ2(S) ≤ σ2, given sensitive attribute s, as σ2(S) 4 Eγ∣s[(Y — m(S))2∣S =
s]. Given N i.i.d. samples {(yn, Sn), n ∈ [N]}, then the optimal MSE error min MSESst(S) is
h
O(N-3), where the optimal bandwidth satisfies h = O(N- 3) Jbr any sensitive attribute S.
Proof for histogram estimation: Based on the definition of MSE error in Eq. (6), we have
Errhist	=
(a)
≤
-.人	.C -
E[∣∆ GDP — ∆GDP |2]
E
mh(s) — mavgIPS(s) — |m(s) — mavg|ps(s)∣dso ]
(c)
≤
(d)
≤
(b)
≤
E
mh(s) — mavg| ∙ IPS(s) — Ps(s)| — |m(s) — mavg — mh(s) + mavg| ∙Ps(S)IdSo ]
2e[∕ Imh(s) — mavgI2ds] ∙ e[∕ IPS(S)-PS(s)∣2ds]
+2 * 2{e[ Z Im h(s) — m(s)∣2Ps (s)ds + Eh Z Im hvg — m@vg ∣2Ps (s)ds]}
2 * O(N-2) + 4 * O(N-2) + 4 * N-1 = O(N-3).
where inequality (a) holds due to absolute value inequality, inequality (b) holds due to Ia1b1 —
a2b2 I ≤ Ia1IIb1 — b2 I + Ia1 — a2IIb2 I, inequality (c) holds based on (a + b)2 ≤ 2(a2 + b2) and
CaUchy-SchWarz inequality, inequality (d) holds based on Lemmas (1) and (2). Note that the order
of the optimal bandwidth are the same in Lemmas (1) and (2), the optimal bandwidth for minimizing
GDP MSE is O(N-3).
14
Published as a conference paper at ICLR 2022
C.2 Kernel Estimation
Recall that we assume that kernel function satisfies normalized condition K(s)ds = 1, symmetry
R SK(s)ds = 1 and finite variance σK 4 R s2 * *K(s)ds > 0. We also define σK = R K2(y)dy.
Similarly, the probability density function of sensitive attribute is given by pS (s). The estimated
probability density function is given by:
1N
PS = Nh X K 丁 )，	(11)
n=1
Subsequently, we define the pointwise MSE error M S Ekpedfrnel (s) to measure probability density
function estimation error given sensitive attribute s as follows:
MSEhst ⑸=E[∣PS (S)- PS (s)∣2],	(12)
where the expectation is took across N samples. Then we have following Lemma 3 on optimal
pointwise MSE error for probability density function:
Lemma 3. Given N i.i.d. samples {(yn, sn),n ∈ [N]}, then the optimal MSE error
minMSEpf l(S) is O(N-5), where the optimal bandwidth satisfies h* = O(N-5) for any
h	kernel
sensitive attribute S.
As for the local prediction average, similarly, we have local prediction average estimation as follows:
Tm(S)
P= K(胃)yn
∑N=ι k (sn-s)
(13)
Subsequently, we also define pointwise MSE error M SEkreergnel(S) to measure local prediction aver-
age estimation error given sensitive attribute S as follows:
MSEregnel(S) = E[∣mh(s) - m(s)∣2],	(14)
where the expectation is took across N samples. Then we have following Lemma 4 on optimal
pointwise MSE error for local prediction average:
Lemma 4. Define the bounded prediction variance σ2 (S) ≤ σ2, given sensitive attribute S, as
σ2(S) 4 Eγ∣s[(Y — m(S))2∣S = s]. Given N i.i.d. samples {(yn, Sn),n ∈ [N]}, then the optimal
MSE error min MSEregnel (S) is O(N-4), where the optimal bandwidth satisfies h = O(N- 1)
h	kernel
for any sensitive attribute S.
Proof for kernel estimation: Based on the definition of MSE error in Eq. (6), we have
Errkernel	=
(e)
≤
E[∣∆GDP - ∆GDP|2]
mavg IPS(s) - |m(s)-
E
mav
|PS(S)dSo2i
(f)
≤
(g)
≤
h(s) - m avg I ∙ IPS (s) - PS (s)∣ - Im(S) - mavg 一 m h (S) + m avg ∣ ∙ PS (S)IdSo ]
(s) - mavgI2ds] ∙ EhZ IPS(s) -PS(s)∣2ds]
+2 * 2{e[/ |mh(s) - m(s)∣2PS(s)ds + e[∕ |mhvg - mavg∣2Ps(S)ds]}
(h)
≤
2 * O(N-4) +4 * O(N-4) +4 * N-1 = O(N-5).
where inequality (e) holds due to absolute value inequality, inequality (f) holds due to Ia1b1 -
a2b2I ≤ Ia1IIb1 - b2I + Ia1 - a2IIb2I, inequality (g) holds based on (a + b)2 ≤ 2(a2 + b2) and
CaUchy-SchWarz inequality, inequality (h) holds based on Lemmas (3) and (4). Note that the order
of the optimal bandwidth are the same in Lemmas (3) and (4), the optimal bandwidth for minimizing
GDP MSE is O(N-5).
15
Published as a conference paper at ICLR 2022
D Proof of Lemmas
D.1 Proof of Lemma 1
Proof. Note that there exists bias-variance tradeoff for MSEhpidsft (s), i.e.,
MSEhf (S)	= E[∣P^S (s) - PS (s)l2 ]
=∣E[PS(s)] -ps(s)∣2 + EMS(S)- E[p^S(s)]∣2],
{z}	{}
Biasphdifst(s)	Varhpdifst(s)
Next we analyze the bias and variance for probability density function MSE. For the bias part, note
that, for S ∈ Bi, the expectation of estimated probability density function satisfies:
1	R ih 1 h PS (S)dS
E[PS(s)] = hP(Sn ∈ Bi) = (i-1)h^----------= PS(s*),	S* ∈ Bi
where the last equality holds by the mean value theorem. Therefore, the bias satisfies
BiaShdst(S) = ∣E[PS(s)] -ps(s)∣
≤ L|S* - S| ≤ Lh.
As for variance part, note that the variance of Bernoulli distribution with parameter P is P(1 - P),
for S ∈ Bi, we have the variance as follows,
Varpft(S)=和[NN XI(s ∈ 笈评=P(Sn W Bi)N-2P(Sn W
n=1
V	hps(s*) _ Ps(s*).
- Nh = Nh ;
Combining the bias and variance part, we have
MSEhist(S) = [BiaSpft(S)]2 + Varpft(S) ≤ L2h2 + PN^,	(15)
It is easy to obtain the optimal bandwidth h* = [IpS2N] 1 = O(N-3), and the minimized MSE is
lower than [Lp2Ns*)] 3 = O(N-3).	□
D.2 Proof of Lemma 2
Proof. Note that the local prediction average by histogram, for S ∈ Bi, is given by Tm(S) =
PPnN1I(sn∈Bi)Yn. Define the normalized weight as Wn(Sn) = PNI(Sn∈B)、, then local prediction
n=1 I(sn ∈Bi)	n=1 I(sn ∈Bi)
average is given by t^(s) = PN=I Wn(Sn)m(Sn). Similarly, We can obtain the bias and variance
tradeoff for local prediction average error MSEhreisgt (S) as follows,
MSEregt(S)	=	E[|m h(s) — m(s)∣2]
∣E[m h(s)] — Im
X-----------------
(s)∣2 + E[∣m h(s) — E[m h(s)]∣2],
{^^^^^
Biasrheisgt(s)
}
|
V arrheisgt(s)
}
For the bias part, based on
PnN=1 Wn(Sn) = 1, we have
Lipschitz condition on
the mean prediction function, note that
N
BiaShist(S)
reg
∣E[mh(s)] 一 m(s)∣ = ∣ X Wn(Sn)[m(Sn) — m(s)]∣
n=1
N
N

X wn(Sn)∣∣[m(Sn) - m(S)]∣∣ ≤ X wn(Sn)Lh = Lh.
(16)
n=1
n=1
16
Published as a conference paper at ICLR 2022
For the variance part, we have variance for local prediction average as follows,
Varhigt(S)	= D[X Wn(Sn)On]=旧[[PN= M	Bi)(Yn -[屈"]2]
n=1	nN=1 I(sn ∈ Bi)
≤ X Eh	I(Sn ∈ BM
n=1	PnN=1 I(Sn ∈B
i / Nσ2∕Nh _ σ2
ɪl ≤ (N∕Nhy2 = Nh
Combining the bias and variance part, we have
2
MSEhegt(S) = [Biashigt(S)]2 + Varhegt(S) ≤ Lh + Nh
(17)
(18)
21	1
It is easy to obtain the optimal bandwidth h = [2LrN" =O(N-3), and the minimized MSE is
lower than [察]3 = O(N- 3).	□
D.3 Proof of Lemma 3
Proof. Note that there exists bias-variance tradeoff for M SEkpedfrnel (S), i.e.,
MSEkfel (s)	= E[∣PS (s) - PS (s)∣2 ]
2
2
=EPS(s)] -PS(S)∣ + E[∣PS(s)- E[PS(s)]∣ ],
、------------{z--------} 、----------{z---------}
BiaSpkdefrnel (S)	V arkpedfrnel(S)
Next we analyze the bias and variance for probability density function MSE. For the bias part, the
expectation of estimated probability density function satisfies:
1N S S
E[PS(s)] - PS(S)	= E[NhE K(-ɪ)] - PS(s)
n=1
h EK(
Sn - S
h IK (
h
Sn -
h
)]— PS(S)
S )PS (Sn)dSn — PS (s)
/ K(y)ps(s + hy)dy - PS(S).
(19)
where the last equality holds by adopting transformation y = Sn-S. By Taylor expansion, when h
is small, we have
0	h2y2 00
Ps (s + hy) = Ps (s) + hyp S (s) + ~2~ Ps (s) + o(h)
Based ob Eqs. (19) and (20), we have
BiaSpfnel (s) = EPS (s)] - PS (s)l
(20)
= ps(s) / K(y)dy + hPS(s) / yK(y)dy + hpS(S)
h2σj,)	J
=~kp- Ps (s)；
As for the variance part, we have
y2K(y)dy + o(h2) - PS(S)
(21)
N
Varkfnel (S) = D[ Nh Σ K ( -ɪ )] = Nh2 D[K( ɪ ^≤ Nh2 E[K 2( -ɪ )]
n=1
= Nhy J K2( Snh S )PS(Sn)d-n = Nh J K2(y)Ps(s + hy)dy
Nh / K2(y)[Ps(s) + hyps(-) + o(h)]dy
PSNF+o( N)
PNh) Z K 2(y)dy+o(h)
(22)
17
Published as a conference paper at ICLR 2022
Combining the bias and variance part, we have
MSEkpedfrnel(s)	=	[Biaspkdefrnel (s)]2 + V arkpedfrnel(s)
≤	h4lpS4s)l2σ4+pSNσk+0(h4)+o(N)
=O(h4)+O( N )，
(23)
It is easy to obtain that the optimal bandwidth h = O(N-5), and thus, the minimized MSE is
lower than O(N-4).	□
D.4 Proof of Lemma 4
Proof. Recall that the mean prediction conditioned on sensitive attribute s is given by m(s) =
E[Y |S = s], we rewrite prediction as Y = m(S) + e, where e is the regression noise and satisfies
E[e] = 0 and E[e2 |S = s] = σ2(s). Note that the prediction Y = m(s) + [m(S) - m(s)] + e, we
have:
1N
Nh X K)yn
n=1
Nh X K ( snh-s )m(S) + Nh X K ( snh-s )[m(Sn) - m(S)]
n=1
N
+ N X K(sɪ)en
n=1
PS (s)m(s) + m 1 (S) + m 2(s).
n=1
Based on Eq. (13), we have
m(s) = m(s) + * + m≠).
PS (s)	PS (s)
(24)
Since E[e∣S = s] = 0, We have the expectation of E[m2(s)]
E[K(Sn-S)E[e∣S = Sn]] = 0. As for the variance of m2(s), we have
0 since E [K (Sn-S )e]
D[m 2(s)]
总 E[K(包产)e2]=焉 E[K(包)σ2(Sn)]
Nh2	h	Nh2	h
Nh2 Z K ( S- )σ2(Sn)PS (Sn)dSn
Nh / K(y)σ2(S + yh)pS(s + yh)dSn
σΚσ2(S)PS(S) .	( 1 ).
Nh + (Nh);
(25)
Subsequently, we consider the expectation and variance of mi(s). Specifically, for expectation, we
have
E[m i(s)]
1 EhK ( snh-s )(m(Sn) - m(s))i
1 / K( Sn -S )(m(Sn) - m(s))PS(Sn)dSn
K(y)m(S + hy) - m(S)PS(S + hy)dy
Z0	y2h2 00	0
K(y) Gym(S4 +	m"(s)) PSs(s) + -VPs(S)) + o(h2)dy
σΚ h2( 'mψ(s + m0 (s)ps (s)) + o(h2).
(26)
18
Published as a conference paper at ICLR 2022
As for the variance of mι(s), we have
D[m ι(s)]
磊 DhK (SnF XmI(Sn- m(S))i	2
N12 Z nK ( sn h s )(m(Sn) - m(s)) - Em(S)]} PS (Sn)dsn
N1h / hK(y)hym (S) - E[m(S)]i (PS(S) + yhPs(S))dSn
σ2(s)σK2
PS(s)Nh
+ o( N1h ) = O( N1h).
Combining the bias and variance part, we have
MSEregnel (S) = [BiaSkegnel(S)]2 + Varregnel(S) ≤ L h + Nh ≤ W) + O Nh), QD
Based on the inequality of arithmetic and geometric means, itis easy to obtain the optimal bandwidth
h* = O(N-5), and the minimized MSE is lower than O(N-4).
□
E Proof of Theorem 4
Given predictor Y = f (X), adversary S = g(Y), and adversary utility Ladv (S, S) = 1 -∖S 一 S|,
GDP and adversary utility are given by
∆GDP = Es h∣Eχ∣s[f (X)] 一 EX[f (X)] Ii ； Ladv = ES 忸x∣s [1 - ∣S 一 g(f (X)) ∣]].
Intuitively, higher model prediction implies larger sensitive attribute if mean prediction function
m(S) is more close to S compared with 1 - S and vice versa. Therefore, we construct adversary g as
follows:
o#f(X)) = J f (X )，	if Es [IS - m(S)∣] ≤ Es [∣S - (1 - m(S))∣];
g	f(X) =	1 - f(X), Otherwise.
Suppose without loss of generality (WLOG) that ES[|S - m(S)|] ≤ ES[∣S - (1 - m(S)) ∣], i.e.,
higher model prediction implies larger sensitive attribute. Then adversary utility is given by
Ladv (g#(f (X )),S) = Es 回 S [1 -∣S - f (X )∣]i ≤ Es [[1-∣S - Eχ∣s [f(X )]∣]]
= ESh1 - ∣∣S-m(S)∣∣i
where the inequality holds due to Jensen’s inequality and convex function |x - t| for any constant
t. Next we show, under the constructive adversary g, the adversarial utility Ladv ≥ 11 ≥ ∆GDP.
Firstly, notice that, for any function m(S) ∈ [0, 1] and S ∈ [0, 1], we have
|S - m(S)∣ + ∣S - (1 - m(S))∣ ≤ max {|1 - m(S)∣ + ∣1 - (1 - m(S))∣,
|0 - m(S)∣ + ∣0 - (1 - m(S))∣0 = 1,
which implies that
Es [1 - ∣S - m(S)ɑ	≤ J (ES[∣S - m(S)|] + ES[∣S - (1 - m(S))∣])
=J (ES [IS - m(S )| +∣S - (1 - m(S))∣]) ≤ ∣.
As for the analysis on GDP, we consider the worst case of mean prediction function m(S) since
GDP satisfies ∆GDP = Es[∣∣m(S) - Es [m(S)]∣∣]. Note that function IxI is strictly convex and the
solution to maximize a strictly convex function over all finite support given first moment is achieved
by a distribution of two mass extreme points, GDP can achieve maximal value when m(S) is 0 or 1.
Define Ppos = Es P m(S) = 1	, thenEs[m(S)] = Ppos and ∆GDP = Ppos 1 - Ppos + (1 -
Ppos)Ppos ≤ 1. Therefore, for the optimal adversary g*, we have
Ladv (g*(f(X)),S) ≥ Ladv (g#(f (X)),S) ≥ ∆GDP∙	(28)
19
Published as a conference paper at ICLR 2022
F	Data Statistics
For fair comparision with previous work, we perform the classification and regression task on five
datasets, including Crimes, Adult, Pokec-n, Pokec-z and Harris dataset. The first four dataset have
been widely adopted to study the fairness problem in tabular data and graph data, while Harris
dataset is collected by ourself for temporal graph data. Table 1 presents additional information on
the real-world tabular, graph and temporal graph datasets. For task type column, “Reg” and “Clf”
represents regression task and classification task, respectively.
Table 1: Statistical Information on Datasets
Data Type	Dataset	Task Type	# Nodes /Samples	# Edges	# Features	Metric	
Tabular	Crimes	Reg	-1994-	—	121	MAE	GDP
	Adult	Clf	45222	—	13	Acc	
Graph	Pokec-n	Clf	66569	729129	59		
	Pokec-z	Clf	67797	882765	59		
Temporal Graph	Harris	Clf	4204	19946	36		
G More Details on S ynthetic Experiments
G.1 GDP Calculation
We firstly provide ground truth analysis in synthetic experiments so that we can evaluate propsoed
two GDP estimation methods error. Considering bivariate Gaussian distribution with mean μ =
[μ1,μ2] and covariance matrix Σ = ：11 ：12 and note that covariance matrix is positive definite
matrix, it is easy to obtain inverse covariance matrix Σ-1
λ11
λ21
：12 , where λ22 =尚 and
λi2 =-得.Thejoint distribution of (S,Y) follows PSY (s,y)
√⅛exp (-2 λ11(S-μI)2-
2λ22(s - μ2)2 + λ12(s - μι)(y - μ2)). Based on probability theory, we can have the condition
functionPY∣s(y|s) as follows:
Py ∣s ⑶S)
Ps,y(S⑻-1	λ	λ22(y - λ22μ2+⅛22s-λ12"1 )2
PS (S)	= qιπexp(	2
λ22
〜
σ11 μ2 + σι2(s - μι)
σ11
which means the mean prediction function m(s) = σιιμ2+σ12(s-μι). Notice that the probability
density function of sensitive attribute is also Guassian with N(μ1,σ11), therefore, the GDP is
∆GDP = / |m(s) - μ2 |ps(s)ds
-μI) I 1	(	(S - μI)2∖χ	2σι2
expdS =
11	2πσ11	2σ11	2πσ11
Next, we calculate GDP for the second synthetic probability density functionPSY(s,^ = S + y
if 0 ≤ S,y ≤ 1. It is easy to obtain the conditional probability PY∣s(0|s) = -s+∣ if 0 ≤ S,y ≤ 1
Is	S + 2
ʌ	1 s+ 1
and thus the mean prediction function m(S) = E[Y[S = s] = 2 13. Similarly, the probability of
s+ 2
20
Published as a conference paper at ICLR 2022
sensitive attribute is PS(S) = S + 11 if 0 ≤ s, y ≤ 1. Thus, the GDP satisfies
∕∣	I	/1 I 1 s + 1	7 I ι	ι
Im(S) - E[m(S)] ∣ps(s)ds = JQ 12s + ι3 —121(S + 2)ds = 48
G.2 More Synthetic Experiment Results on Estimation Error
Figure 6 shows local prediction average and sensitive attribute probability density function estima-
tion results for different kernel function choice. The top two subfigures show the local prediction
average estimation error w.r.t. bandwidth choice. It is seen that the tricube and aitchison aitken ker-
nel function achieve better and robust local prediction average estimation compared with Gaussian
kernel function. The mid subfigures show the local prediction average result for different sensitive
attribute and bottom two subfigures shows probability density function estimation results with dif-
ferent kernel and histogram choice. It is seen that kernel estimation possesses more smooth and
accurate probability density function estimation.
uo-ssalboɑ
regression error
O 1
O -
1 W
ωSE UO-SSaIB
2× IoT 3 × IOT4 x 10-τ 6× IOT 100
bandwidth
regression error
3 4
O O
1 1
φωE UO-SSaJB
IO-2
10^1
bandwidth
Sensitive attribute
Itue ---------- gaussian ---------- cosine
bandwidth=0.1 bandwidth=0.2 bandwidth=0.5
bandwidth=0.1 bandwidth=0.2 bandwidth=0.5
Figure 6: Local prediction average and sensitive attribute probability density function estimation
error analysis with respect to the kernel bandwidth and number of samples for bivariate Gaussian
distribution and second synthetic distributions.
21
Published as a conference paper at ICLR 2022
G.3 More Synthetic Experiment Results on Robustness
In this subsection, we aim to investigate the robustness of GDP estimation error over different mean
and covariance parameters. Note that only the parameters expectation μ = [μι, μ2] = [0.5,1.0]
and covariance matrix Σ = σ11
σ12
σ12
σ22
1
0.5
0.5
2.0
are adopted on synthetic experiments, we
adopt these parameters as default and only modify the one parameter to inspect GDP estimation
error. Figures 7 and 8 show the GDP estimation error with mean parameters μι and μ2, and covari-
ance parameters σ11, σ12, and σ22 given the same data size 1000. We have two observations: (1)
kernel-based GDP estimation methods consistently achieves lower estimation errors than histogram-
based counterpart for any Gaussian distribution parameter, which further validates the superiority of
kernel-based method. (2) kernel-based GDP estimation error is almost the same for any kernel
function pair choice, which validates our theoretical analysis in Theorem. 3.
---- histogram
—tri-lin
一-----tri-cos
----aa-lin
y-----aa-cos
0.036
0.034
0.032
0.030
0.028
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Mean μ1	Mean μ2
Figure 7: GDP estimation error for different mean parameters
Variance ɑɪɪ	Variance Og	Variance σ⅛2
Figure 8: GDP estimation error for different covariance parameters
H	More Details on Real-World Experimental Results
Many real-world data are organized as tabular, graph and temporal graph data. Various machine
learning models and training strategy are specifically developed for these data types (Chen &
Guestrin, 2016; Han et al., 2022; Jiang et al., 2022; Zhao et al., 2019). Our experiments demon-
strate that the proposed GDP and corresponding regularizer can be adopted for these data types.
H. 1 Mitigation Performance for Temporal Graph Data
The temporal graph data, provided by data intelligence company Cuebiq (Cuebiq, 2021), is col-
lected from anonymous human movement activities, including the coordinates and time of mobile
devices at stop points, during August 2017 in Harris County (Houston) Texas, USA. To generate
the temporal graph, we first divide the Harris County into several grid cells with equal size approxi-
mately 1km × 1km. Each grid cell is treated as a graph node, and temporal link between two nodes
represents at least one user movement in hourly basis duration. The node features are generated
from socio-demographic data of the American Community Survey (ACS) 2014-2018 (5-year) data
by the U.S. Census Bureau (Bureau, 2021). In the experiment, the white race ratio is treated as
continuous sensitive attributes and our task is to predict whether the income of each node is high
22
Published as a conference paper at ICLR 2022
or low. We adopt temporal graph attention (TGAT) (da Xu et al., 2020)8 with map and product
attention mechanism to efficiently aggregate temporal-topological neighborhood features and report
the mean prediction performance and GDP with 5 running times.
We compare the mitigation performance of kernel estimation and other baselines for private Harris
dataset with two backbones in Figure 9. Simialrly, the hyper-parameter λ control the tradeoff be-
tween accuracy and GDP. Again, kernel estimation consistently outperforms the other baselines by
a large margin and binary-quantized sensitive attributes inevitably deteriorate the mitigation perfor-
mance.
—kernel —histogram —adv—•— adv bn * vanilla
(a) Map
Figure 9: Mitigation performance for temporal graph Harris dataset. (a) TGAT with map attention;
(b) TGAT with product attention.
—kernel —histogram —adv—•— advbn * vanilla
(b) Prod
H.2 Prediction Performance and GDP tradeoff Curve during Training
Training curve on tabular data: Aiming to inspect the dynamic prediction performance and GDP
during model training, we provide prediction performance and GDP tradeoff curve for Adult and
Crimes dataset in Figure 10. The left and right y-axis represent the prediction performance and GDP
metric, respectively. It is seen that, for kernel or histogram as regularization, the hyperparameter can
control the prediction performance and GDP tradeoff, while the training is highly unstable with large
variance for adversarial debiasing. Additionally, kernel estimation as regularization possesses better
bias mitigation performance for Adult and Crimes dataset.
--ksπιel-0.0 -histαgmιn4.0 -adv^0.0 -βdv-bn-0∙0 -kemel-0.7 -hbtogeπ⅛4.7 -adv-50.0 -adv-bn-50.0	-ksπel-5.0 -hbtogeιn∙5.0 -adv-500.0 -adv_bn-500.0
∙∙'s'0,k∙∙ms0,s
≡=R->UBB8<
0	20	40 BO 80	100
Training PBgESS
S 5 QSQSQS
7 75 5
-OS-AMlnMV
ClMUDPa
I _ I I __ I I ___ I I ____
O 20	4«	60	80 IOO
Training pπ>gκss
ClMUDP£a»
∙∙'s'0,k∙∙ms0,s
-OS-AMlnMV
O 20	40	60	80 IOO
Training PBgESS
ClMUDP£a»
adv-b∏-0.0
adv_bn-500-0
kernel-0.0
h btogram-O.O
adv4.0
-x-β⅞x
O	20	«	SO	βθ IOO
Training progress
terne 1-0.7
hlstogram∙O .7 -----adv-50.0
≈s∙2,2∙"=2∙
-旦 3V*
6anop-dα9
O	20	«	60	80 IOO
Training progress
■-旦 3V*
6anop-dα9
kernel-5.0 ----hbtQgr«m-5.0 -------adv6OO.0
s,≈s∙2,2∙2∙22
6anop-dα9
_
Figure 10:	Prediction performance and GDP training curve for Adult (top) and Crimes (bottom)
datasets with different hyperparameters.
Training curve on graph data: Figures 11 and 12 demonstrates prediction performance and GDP
tradeoff curve for Pokec-n and Pokec-z datasets using GCN and GAT model. It is seen that, for
8https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs
23
Published as a conference paper at ICLR 2022
kernel or histogram as regularization, the hyperparameter can control the prediction performance
and GDP tradeoff, while the training is highly unstable with large variance for adversarial debiasing.
Additionally, kernel estimation as regularization possesses better bias mitigation performance for
Pokec-n and Pokec-z datasets in GCN and GAT model.
kemel-v.0 -------hιstogrβm-O.O -------adv-v.0 — adv_bn-0.0
»»,0»s0
6-A3ul⅜MV
1.75
20	4« CO 80 IM
Training pnjg/ss
CPaHopldae
temel-3.0 -----histogram-3.0 -----adv-10.0	∙ «dv_bn-10.0
OOO
»»,0»s0
6A3ul⅜MV
«■
6	20	40	60 SO IOO
Training PBgESS
CPaHopldae
keπιe!"5.0 ---htstogem∙5.0 -------adv-50.0	-- mv-dπ>5u∙v
M » U »
6A3ul⅜MV
O 20	4«	60	80 IOO
Training pnjgess
CPaHopldae
0.∞
kerπel-Q.0 ----hktogram-O.O --------βdv-0.0 ------«dv_bn-0.0
kerne 1-3.0 ---hbtQgrβm-3Λ) ------adv~lθ.0 ------9dv-b∏-10.0
O 2Q 40 eo 80 IOO
Training progress
kernel-54) ----WstngramSO ---------adv-50.0 ------advJ>n-50.0
0.25
0.50
1.00 3
1.25 S
1.50
1.75
2.<J<J
Figure 11:	Prediction performance and GDP training curve for Pokec-n (top) and Pokec-z (bottom)
datasets with GAT model.
adv-b∏-0.0
-----kerne 1-0.0 --------h Estugre ∣∏-O.O -------a&q.O
O 2Q 40 SO 80 IOO
Training progress
kerne 1-3.0 ---hbtQgrβm-3Λ) ------adv~lθ.0 ------9dv-b∏-10.0
O 2Q 40 eo 80 IOO
Training progress
-0.00	75
OW	70
kerne 1-5.0 ------hEstaigreni-SX) ---------adv~50.0 — 9dv-b∏-50.0
e e 5 5,54°
-≡-8-fcusas
6anop-dα9
O	20	«	60	80 IOO
Training progress
-0.00
0.25
-0.50
-0.755
S
liλo⅞
1≈g
-1.50
■1.75
-2.00
kerπel-Q.0 ----hktogram-O.O --------βdv-0.0 ------«dv_bn-0.0
O 20	4« SO 80 IOO
Training progress
≈-0SJAaaJn339
CPaHodα9
D.W
----kerne 1-3.0 ----hbtQgrβm-3Λ) ------adv~lθ.0 — adv-bπ
75
O 2Q 40 eo 80 IOO
Training progress
(PaMO≡dα9
1 O5O5O5Q5O
.OJZ 5 7.P257 P
10QaaaLLLLN
kernel-5.0 ------h Estog wn-5.0 --------9dv-50.0 --------∙dvjn6(l∙θ
",∙∙5∙∙555∙∙5
-0SJAaaJn339
O 20	4« eo 80 IOO
Training progress
CPaHosdα9
Figure 12:	Prediction performance and GDP training curve for Pokec-n (top) and Pokec-z (bottom)
datasets with GCN model.
Training curve on temporal graph data: Figure 13 demonstrates prediction performance and GDP
tradeoff curve for Harris datasets using TGAT with map and product attention mechanism. It is seen
that, for kernel or histogram as regularization, the hyperparameter can control the prediction perfor-
mance and GDP tradeoff, while the training is highly unstable with large variance for adversarial
debiasing. Additionally, kernel estimation as regularization possesses better bias mitigation perfor-
mance for TGAT with map and product attention mechanism.
Training curve on compositional sensitive attribute: Figure 14 demonstrates prediction perfor-
mance and GDP tradeoff curve for Adult and Crimes dataset. It is seen that, for kernel or histogram
as regularization, the hyperparameter can control the prediction performance and GDP tradeoff,
while the training is highly unstable with large variance for adversarial debiasing. Additionally, ker-
nel estimation as regularization possesses better bias mitigation performance for Adult and Crimes
dataset.
24
Published as a conference paper at ICLR 2022
temel-v.v ------- hiStogram-O-O -------adv-0.0	- -- adv-bπ-v.0
⅛mel-5.0 -------histogram-5.0 -------adv-50.0 adv b∏-50.Q
keπel-3.v -----htstogem<3.0 -------adv-10.0	«dv_bn-10.0
0.0	75 ∙
75 5
SlfcUBBi
CPaHodαe
-00	70
O 20	4«	60	80 IOO
Training pπ>gκss
∙5 5t
SlfcUBBi
CPaHodαe
CPaHop-doe
75 ∙
70
1«
E60
ba-
se
45
0	20	40	&0 SO IM
Training pπ>gιcss
----kerne M).0 -----h Estugre ∣∏-OΛ) --a&q.O ---------9dγ-b∏∙0.0
O 20	40	60	80 IOO
Training PBgESS
kernel-3.0
adv-b∏-10.0
50
",∙∙5∙∙55
E-OS-AMJnaaV
--OS-AMJnaaV
(PaHOPIdag
O 20	40 Gq
Training progress
1.4	50
1.6
80 IOO	«
h Estog r9in-3.0 -----9dv-10.0
2Q 40 SO 80 IOO
kernel-5.0 ----hbtQgr«m-5.0 -------9dv-50.0 ------9dv-b∏-50ΛI
--OS-AMJnaaV
(PaHOPIdag
Li i
Training progress
Figure 13:	Prediction performance and GDP training curve for TGAT with map (top) and product
(bottom) attention mechanism with Harris data.
kerne M.0 --------h IstDgrani-O-O --------adv-0.0 ---------advjHi-0.0
ker∏eM.7 -------hbtngraι∏-0.7 -------adv-50.0 -------adv-b∏-50.0
kernel-5.0 ------hIstngrani-SX) ------adv-500.。---------advJ>∏-500.0
m,∙∙∙5∙
E-OSIAMJnaaV
m然,∙∙5∙∙555∙∙5
-OSJAMJnaaV
CPaHoPida9
β 7 6 5 φ 3
-OSJAMJnaaV
CPaHoPida9
CPaHoPida9
O	2«	40	60	90 IOO
Training progress
----ker∏el∙0.0 -----hbtogrβm-0.0 -------adv∙4∙0 -------9dγ-b∏∙0.0
34
32
30
I 26
*
≡ 24
22-
»■
U
O 20	40	60	80 IOO
Training progress
O	2«	40	60	90 IOO
Training progress
kerne M).7 ----h Isingr9 m∙0.7 ----adv6O.0 --------9dγ-b∏-50.0
-1.75
-1.50
-1.25 —
-1.00 O
心!
-0.50
-0.25
0.00
6 2Q 4« W 80	100
Training progress
O 2Q 4«	60	80 IOO
Training progress
kernel-54)
adv-500.0 — advjHi-500Λ)
l,l2l∙Be 4 2
3 3 3 2 2 2 2
-x-β≡*x
WstogramSO
Figure 14:	Prediction performance and GDP training curve for Adult (top) and Crimes (bottom)
datasets with compositional attributes.
I	Future Works
There are several interesting future work related to proposed GDP:(1)Extend other fairness metrics,
such as Equal Odds (EO), for continuous and discrete sensitive attributes. The connection between
extended fairness metric and adversarial debiasing is also interesting. (2) Provide a more fine-
grained analysis on estimation error, including the exact optimal bandwidth and estimation error
expression, the lower bound of the estimation error for histogram and kernel GDP estimation over
all data distribution and model prediction, the estimation error analysis over compositional sensitive
attributes. (3)Investigate the better GDP estimation method with faster convergence guarantee.
25