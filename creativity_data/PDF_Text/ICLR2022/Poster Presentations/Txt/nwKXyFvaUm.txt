Published as a conference paper at ICLR 2022
Diverse Client Selection for Federated
Learning via Submodular Maximization
Ravikumar Balakrishnan*	Tian Li*	Tianyi Zhou*
Intel Labs	CMU	University of Washington
ravikumar.balakrishnan@intel.com	tianli@cmu.edu	tianyizh@uw.edu
Nageen Himayat
Intel Labs
nageen.himayat@intel.com
Virginia Smith
CMU
smithv@cmu.edu
Jeffrey Bilmes
University of Washington
bilmes@uw.edu
Ab stract
In every communication round of federated learning, a random subset of clients
communicate their model updates back to the server which then aggregates them
all. The optimal size of this subset is not known and several studies have shown
that typically random selection does not perform very well in terms of convergence,
learning efficiency and fairness. We, in this paper, propose to select a small diverse
subset of clients, namely those carrying representative gradient information, and
we transmit only these updates to the server. Our aim is for updating via only a
subset to approximate updating via aggregating all client information. We achieve
this by choosing a subset that maximizes a submodular facility location function
defined over gradient space. We introduce “federated averaging with diverse
client selection (DivFL)”. We provide a thorough analysis of its convergence
in the heterogeneous setting and apply it both to synthetic and to real datasets.
Empirical results show several benefits of our approach, including improved
learning efficiency, faster convergence, and more uniform (i.e., fair) performance
across clients. We further show a communication-efficient version of DivFL that
can still outperform baselines on the above metrics.
1	Introduction
Federated learning (FL) involves collaboratively training of machine learning model across a large
number of clients while keeping client data local. Recent approaches to this problem repeatedly
alternate between device-local (stochastic) gradient descent steps and server-aggregation of the clients’
model updates (McMahan et al., 2017). In cross-device settings, a server and its model usually serve
several thousands of devices. Therefore, the communication between clients and the server can be
costly and slow, forming a huge impediment to FL’s viability.
One property of the collection of clients that can mitigate these problems, however, is often not
exploited, and that is redundancy. Specifically, many clients might provide similar, and thus redundant,
gradient information for updating the server model. Therefore, transmitting all such updates to the
server is a waste of communication and computational resources. How best to select a representative
and more informative client set while adhering to practical constraints in federated learning is still
an open challenge. Although several selection criteria have been investigated in recent literature, e.g.,
sampling clients with probabilities proportional to their local dataset size (McMahan et al., 2017),
sampling clients of larger update norm with higher probability (Chen et al., 2020), and selecting clients
with higher losses (Balakrishnan et al., 2020; Cho et al., 2020), the redundancy and similarity of the
clients’ updates sent to the server is not represented and exploited in these approaches. In particular,
communicating multiple clients’ updates to the server may cause statistical and system inefficiency
if too many of them are too similar to each other. The commonly studied modular score/probability
for each individual client is incapable of capturing information as a property over a group of clients.
*Equal contributions
1
Published as a conference paper at ICLR 2022
Ideally, a diverse set of clients would be selected, thereby increasing the impact of under-represented
clients that contribute different information, and thereby improving fairness. This, in fact, is a topic
of increasing interest (Mohri et al., 2019; Cho et al., 2020; Dennis et al., 2021; Huang et al., 2021).
In this paper, we introduce diversity to client selection in FL, namely a strategy to measure how a
selected subset of clients can represent the whole when being aggregated on the server. Specifically, in
each communication round, we aim to find a subset whose aggregated model update approximates the
aggregate update over all clients. By doing this, we aim to limit the impact of subset selection which
introduces variance in the model updates across round, that could otherwise slow the learning process.
Inspired by the CRAIG method of coreset selection for efficient machine learning training (Mirza-
soleiman et al., 2020), we derive an upper bound of the approximation error as a supermodular
set function (in particular, the min-form of the facility location function (ComUejols et al., 1977))
evaluated on the selected subset. We can then apply submodular maximization (Fujishige, 2005; Iyer
et al., 2013; Wei et al., 2014) on a complement submodular function to (approximately) minimize
the error upper bound. We employ the greedy selection (Nemhauser et al., 1978) of a subset of
clients according to the marginal gain of the submodular function to achieve a solution with provable
approximation guarantee (Conforti & Cornuejols, 1984). By integrating the diverse client selection
into the most commonly studied FL scheme, i.e., Federated Averaging (FedAvg) (McMahan et al.,
2017), we propose DivFL that applies global model aggregation over a selected subset of clients
after multiple local steps on every client. We present theoretical convergence analysis of DivFL and
show its tolerance to the heterogeneity of data distributions across clients and large numbers of local
steps. However, our method differs from the CRAIG method where selection is performed based
on model updates (involving multiple epochs at the clients). In addition, our approach allows for
partial device participation where the server does not have access to all data at any communication
round, as is standard in FL (McMahan et al., 2017). In experiments, we compare DivFL with other
client selection approaches on both synthetic dataset and FEMNIST, wherein our method excels on
convergence, fairness, and learning efficiency.
2	Background and Related Work
We consider a typical federated learning objective:
N
min f(w) =	pkFk (w),
w
k=1
where for each client k ∈ [N], pk is a pre-defined weight (such that PkN=1 pk = 1) that can be set to
N or the fraction of training samples, and Fk is the client-specific empirical loss. While there are
various possible modeling approaches, we consider this canonical objective of fitting a single global
model to the non-identically distributed data across all clients (McMahan et al., 2017).
Client Selection in Federated Learning. Client1 sampling is a critical problem particularly for
cross-device settings where it is prohibitive to communicate with all devices. Two common (or
default) strategies are (a) sampling the clients based on the number of local data points and uniformly
averaging the model updates, and (b) sampling the clients uniformly at random and aggregating the
model updates with weights proportional to the local samples (Li et al., 2020). There is also recent
work proposing advanced sampling techniques to incorporate dynamic systems constraints, accelerate
the convergence of federated optimization, or to obtain a better model with higher accuracy (Nishio
& Yonetani, 2019; Ribero & Vikalo, 2020; Cho et al., 2020; Lai et al., 2020). We investigate client
selection through the lens of encouraging client diversity at each communication round which largely
remains unexplored in previous work. The closest client selection method to ours is based on
clustering (e.g., selecting representative clients from separate clusters (Dennis et al., 2021)). We note
that performing (private) clustering in federated settings is still an open problem, and our method can
be viewed as a soft version of dynamic clustering at each round (discussed in the next paragraph).
The benefits of gradient (or model) diversity has been demonstrated in other related contexts, such
as scaling up mini-batch stochastic gradient descent (SGD) (Yin et al., 2018). Enforcing sample or
gradient diversity during optimization also implicitly places more emphasis on the underrepresented 1
1Following conventions, we use the term ‘client’ for the problem of client selection. Throughout the paper,
we use ‘devices’ and ‘clients’ interchangeably.
2
Published as a conference paper at ICLR 2022
sub-population of clients, and can promote fairness defined as representative disparity (Hashimoto
et al., 2018). Similar to previous work (e.g., Cho et al., 2020; Balakrishnan et al., 2020), we observe
our approach yields more fair solutions across the network in Section 5.
Diverse Subset Selection via Submodularity. Modular scores have been widely studied for subset
selection in machine learning and federated learning, e.g., a utility score for each sample or client
often measured by the loss. However, the diversity of a subset cannot be fully captured by such
modular scores since there is no score interaction. Diversity is often well modeled by a diminishing
return property, i.e., the (marginal) gain an element brings to a subset diminishes as more elements
added to the subset. There exists a rich and expressive family of functions, all of which are natural
for measuring diversity, and all having the diminishing returns property: given a finite ground set V
of size n, and any subset A ⊆ B ⊆ V and a v ∈/ B, a set function F : 2V → R is submodular if
F(v∪A) -F(A) ≥ F(v∪B) -F(B).	(1)
This implies v is no less valuable to the smaller set A than to the larger set B . The marginal gain
of v conditioned on A is denoted f(v|A) , f(v ∪ A) - f(A) and reflects the importance of v to A.
Submodular functions (Fujishige, 2005) have been widely used for diversity models (Lin & Bilmes,
2011; Batra et al., 2012; Prasad et al., 2014; Gillenwater et al., 2012; Bilmes & Bai, 2017).
Maximizing a submodular function usually encourages the diversity and reduces the redundancy of
a subset. This property has been utilized for data selection in active learning (Guillory & Bilmes,
2011), curriculum learning (Zhou & Bilmes, 2018), mini-batch partitioning (Wang et al., 2019),
gradient approximation (Mirzasoleiman et al., 2020), etc. Although the number of possible subsets
A is nk , enumerating them all to find the maximum is intractable. Thanks to submodularity, fast
approximate algorithms (Nemhauser et al., 1978; Minoux, 1978; Mirzasoleiman et al., 2015) exist
to find an approximately optimal A with provable bounds (Nemhauser et al., 1978; Conforti &
Cornuejols, 1984). Despite its success in data selection, submodularity has not been explored for
client selection in federated learning. Encouraging diversity amongst local gradients (or model
updates) of selected clients can effectively reduce redundant communication and promote fairness.
Moreover, it raises several new challenges in the FL setting, e.g., (1) it is unclear which submodular
function to optimize and in which space to measure the similarity/diversity between clients; (2) What
convergence guarantee can be obtained under practical assumptions such as heterogeneity among
clients, and (3) What are the effects of outdated client selection due to communication constraints?
3	Diverse Client Selection
In this section, we introduce “federated averaging with diverse client selection” (or DivFL), a
method that incorporates diverse client selection into the most widely studied FL scheme, federated
averaging (FedAvg). We will first derive a combinatorial objective for client selection via an
approximation of the full communication from all clients, which naturally morphs into a facility
location function in the gradient space that can be optimized by submodular maximization. We then
present the standard greedy algorithm that optimizes the objective by selecting a diverse subset of
clients at every communication round.
3.1	Approximation of Full Communication
We aim to find a subset S of clients whose aggregated gradient can approximate the full aggregation
over all the N clients V = [N]. To formulate this problem, we start by following the logic
in Mirzasoleiman et al. (2020). Given a subset S, we define a mapping σ : V → S such that the
gradient information VF1k(Vk) from client k is approximated by the gradient information from a
selected client σ(k) ∈ S. For i ∈ S, let Ci，{k ∈ V∣σ(k) = i} be the set of clients approximated
by client-i and γi , |Ci|. The full aggregated gradient can be written as
X VFk(vk) = X hVFk(vk) - VFσ(k)(vσ(k))i +XγkVFk(vk).	(2)
k∈[N]	k∈[N]	k∈S
Subtracting the second term from both sides, taking the norms, and applying triangular inequality, we
can obtain an upper bound for the approximation to the aggregated gradient by S, i.e.,
X VFk(vk) - X γkVFk(vk) ≤ X VFk(vk) - VFσ(k)(vσ(k)).	(3)
k∈[N]	k∈S	k∈[N]
3
Published as a conference paper at ICLR 2022
The above inequality holds for any feasible mapping σ since the left hand side does not depend on σ.
So we can take the minimum of the right-hand side w.r.t. σ(k), ∀k ∈ [N], i.e.,
EVFk (Vk)-EYk VFk(Vk) ≤ EminlIVFk (Vk) -VFi(vi)∣∣，G(S).
i∈S
(4)
k∈[N]
k∈S
k∈[N]
The right hand side provides a relaxed objective G(S) for minimizing the approximation error on the
left hand. Minimizing G(S) (or maximizing G, a constant minus its negation) equals maximizing
a well-known SUbmodUlar function, i.e., the facility location function (CornU向ols et al., 1977). To
restrict the communication cost, we usually limit the number of selected clients to be no greater than
K, i.e., |S| ≤ K. This resorts to a submodular maximization problem under cardinality constraint,
which is NP-hard but an approximation solution with 1 - e-1 bound can be achieved via the greedy
algorithm (Nemhauser et al., 1978).
3.2	Greedy Selection of Clients
The naive greedy algorithm for minimizing the upper bound of gradient approximation starts from
S — 0, and adds one client k ∈ V\S with the greatest marginal gain to S in every step, i.e.,
S — S ∪ k*, k ∈ argmax[G(S) — G({k} ∪ S)]	(5)
k∈V \S
until |S | = K. Although it requires evaluating the marginal gain for all clients k ∈ V\S in every
step, there exists several practical accelerated algorithms (Minoux, 1978; Mirzasoleiman et al., 2015)
to substantially reduce the number of clients participating in the evaluation. For example, stochastic
greedy algorithm (Mirzasoleiman et al., 2015) selects a client k from a small random subset of V\S
with size s in each step, i.e.,
S — S ∪ k*, k* ∈ argmax	[G(S) — G({k}∪ S)]	(6)
k∈rand(V \S, size=s)
To incorporate the client selection into any federated learning algorithm, we apply the stochastic
greedy algorithm in each aggregation round and perform selection only from a random subset of
active clients in the network. The complete procedure is given in Algorithm 1 assuming the base
algorithm is Federated Averaging (FedAvg) (McMahan et al., 2017).
On the left-hand side of Eq. (3)-(4), we aim
at approximating the full communication
by a weighted sum over selected clients in
S with weights {γi}i∈S. However, since
we relax the problem to minimizing its up-
per bound and the (stochastic) greedy solu-
tion is not guaranteed to achieve the global
minimum of the relaxed objective, the
weight associated with the greedy solution
S, i.e., γi = |Ci | with Ci = {k ∈ V|i ∈
arg minj∈S ∣∣VFk (Vk) - VFj (Vj )∣∣}, is
sub-optimal. In fact, given S, the optimal
weight {γi}i∈S can be achieved by directly
minimizing the left hand side of Eq. (3)-(4)
but it is infeasible because the full aggre-
gation Pk∈[N] VFk(Vk) is not available in
our setting. Though there might exist bet-
ter choices, we find that simple uniform
weights work promisingly in all evaluated
scenarios of our experiments. The stochas-
tic greedy selection in line 2 of Algorithm 1
Algorithm 1 DivFL
Input: T, E, η, w0
1
2
3
4
5
6
7
8
9
for t = 0,…，T 一 1 do
Server selects a subset of K active clients St
using the stochastic greedy algorithm in Eq. (6),
and sends wt to them.
for device k ∈ St in parallel do
wk — wt
Solve the local sub-problem of client-k inex-
actly by updating wk for E local mini-batch
SGD steps:
wk = wk - ηVFk(wk)
Send ∆tk := wtk - wt back to Server
end
Server aggregates {∆tk}:
wt+1 — wt +
end
10 return wT
requires access to the gradients from all clients, which might be expensive in communication costs.
In practice, we may take several approaches to minimize the communication costs. One is to receive
periodic gradient updates from all clients every m communication rounds. Another is to only use the
4
Published as a conference paper at ICLR 2022
gradients from the selected clients at the current round to update part of the N × N dissimilarity ma-
trix. In our evaluation, we take the latter method with “no-overheads”. While this maybe suboptimal
since a large part of the dissimilarity matrix will contain stale gradients, we observe no significant
loss in performance in our empirical studies.
4	Convergence Analysis
In this section, we provide theoretical analyses of the convergence behavior of Algorithm 1 for
strongly convex problems under practical assumptions of non-identically distributed data, partial
device participation, and local updating. While the current analysis only holds for the proposed client
selection algorithm applied to FedAvg, it can be naturally extended to other federated optimization
methods as well.
As discussed in Section 3.1, we draw connections between full gradient approximation and submodu-
lar function maximization. By solving a submodular maximization problem in the client selection
procedure, we effectively guarantee that the approximation error is small (see Eq. (4)). We state an
assumption on this below.
Assumption 1 (Gradient approximation error). At each communication round t, we assume the
server selects a set St of devices such that their aggregated gradients (with weights {γk}k∈St) is a
good approximation of the full gradients on all devices with error , i.e.,
N X YkVFk(Vk) - N X NFk(Vk) ≤ e.
k∈St	k∈[N]
The same assumption has been studied in previous works on coreset selection for mini-batch
SGD (Mirzasoleiman et al., 2020, Theorem 1). Note that is used as a measure to characterize how
good the approximation is and our theorem holds for any < ∞. Our algorithm (Algorithm 1) is
effectively minimizing an upper bound of to achieve a potentially small value via running submod-
ular maximization to select diverse clients (Eq. (4)). Next, we state other assumptions used in our
proof, which are standard in the federated optimization literature (e.g., Li et al., 2019).
Assumption 2. Each Fk (k ∈ [N]) is L-smooth.
Assumption 3. Each Fk (k ∈ [N]) is μ-strongly convex.
Assumption 4. For k ∈ [N] and all t, in-device variance of stochastic gradients on random samples
ζ are bounded, i.e., E[kVFk (wtk, ζ) - VFk (wtk)k2] ≤ σ2.
Assumption 5. For k ∈ [N] and all t, the stochastic gradients on random samples ζ are uniformly
bounded, i.e., kVFk (wtk, ζ)k2 ≤ G2.
Assumption 6 (Bounded heterogeneity). Statistical heterogeneity defined as F * 一 Pk∈[Ν ] Pk Fk is
bounded by C, where F * := mi□w f (W) and Fk := min。Fk(V).
Let w* ∈ arg minw f(w) and Vk* ∈ arg minv Fk(V) for k ∈ [N]. In our analysis, we need to
bound another variant of heterogeneity k Pk∈[N] pkVk* - w* k in the parameter space. Note that
under Assumption 3 (μ-strongly convexity), Assumption 6 implies that ∣∣ Pk∈[N] pkv* 一 w*k is also
bounded by a constant.
Setup. Following Li et al. (2019), we flatten local SGD iterations at each communication round,
and index gradient evaluation steps with t (slightly abusing notation). We define virtual sequences
{Vtk}k∈[N] and {wtk}k∈[N] where
Vtk+1 = wtk 一 ηtVFk(wtk), wtk+1
Vtk+1, if not aggregate,
select St+1 and average {Vtk+1}k∈St+1 , otherwise.
While all devices virtually participate in the updates of {Vtk} at each virtual iteration t, the effective
updating rule of {wk} is the same as that in Algorithm 1. Further, let Vt := Pk∈[N] PkVk,Wt ：=
Pk∈[N] Pkwk. TherefOg Wt = Vt P	kk
K 乙k∈St Vt
if not aggregate,
.The goal of defining Vt and Wt
otherwise.
5
Published as a conference paper at ICLR 2022
is to relate the updates of Vt to mini-batch SGD-Style updates, and relate the actual updates of Wt to
those of Vt. We aim at approximating Vt+ι by Wt+ι (when aggregating) and next state a main lemma
bounding ∣∣Wt+ι - Vt+J∣.
Lemma 1. For any virtual iteration t, under Algorithm 1 and Assumptions 1-6, we have
kwt+1 - vt+ιk ≤ LGE(E -I)η2o + EentO,
where L is the smoothness parameter, G is the bounded stochastic gradient parameter, E is the
number of local iterations, and ηt0 is indexing the latest communication round.
We defer the proof to Appendix A. The main step involves using Assumption 1 to bound the
approximate error of gradients and relating accumulative gradients with model updates. With
Lemma 1, we state our convergence results as follows.
Theorem 1 (Convergence of Algorithm 1). Under Assumptions 1-6, we have
E[∣w* — wtk2] ≤ O(1∕t) + O(e).
The non-vanishing term e encodes the gradient approximation error (Assumption 1), and will become
zero when we select all clients at each round (i.e., K = N). In experiments, we observe that
DivFL allows us to achieve faster convergence (empirically) at the cost of additional solution bias (a
non-diminishing term dependent on e).
We provide a sketch of the proof here and defer complete analysis to Appendix A. Examine the
distances between Wt+ι and w*,
I∣wt+1 一 w*k2 = I∣wt+1 一 Vt+ιk2 + I∣vt+1 一 w*k2 +2(Wt+ι — Vt+ι,Vt+ι — w)
If iteration t is not an aggregation step, Wt+ι = Vt+ι and
kwt+1 - w*k2 = kvt+1 - w*k2,
which we can bound with Lemma 1 in Li et al. (2019):
E[kvt+ι - w*k2] ≤ (I-ntμ)E[∣∣wt - w*∣∣2] + n2B	⑺
for some constant B . If t is an aggregation step, we need to bound
E[∣Wt+ι — Vt+1 k2]+ E[∣Vt+ι — w*k2] + 2E[hWt+ι - Vt+ι,Vt+ι — w*〉].
The second term can be bounded by Eq. (7), which contains (1 - ntμ)E[∣wt - w* k2]. Therefore,
combined with Lemma 1, with a decaying step size, we can obtain a recursion on E[∣wt+ι - w*∣2]
which leads to Theorem 1. We provide the complete proof in Appendix A. 5
5	Experiments
Setup. We evaluate the DivFL approach utilizing both synthetic and real federated datasets from the
LEAF federated learning benchmark (Caldas et al., 2019). The synthetic dataset enables us to control
the data heterogeneity across clients for evaluation. We consider two baselines to compare our DivFL
against: a) random sampling without replacement, and b) the power-of-choice approach (Cho et al.,
2020) where clients with the largest training losses are selected. For DivFL, we consider an ideal
setting where 1-step gradients are queried from every device in the network for each global round. In
addition, we also evaluate the “no overhead” setting where (a) client updates from previous rounds
are utilized to update part of the dissimilarity matrix, and (b) we run the stochastic greedy algorithm
to avoid selecting from an entire set of clients. While the former provides the upper bound on the
performance, the “no overhead” approach and other variants are more suited to realistic settings. For
all the methods, we fix the number of clients per round K = 10 and report the performance for other
choices of K in Appendix. Each selected client performs τ = 1 round of local model update before
sharing the updates with the server unless otherwise noted. We report the performance of DivFL
across metrics including convergence, fairness and learning efficiency. We further report the impact
of the subset size K on these metrics. Our code is publicly available at github.com/melodi-lab/divfl.
6
Published as a conference paper at ICLR 2022
5.1	Results on the Synthetic Dataset
We generate synthetic data following the setup described in Li et al. (2020). The parameters
and data are generated from Gaussian distributions and the model is logistic regression. y =
arg max(softmax(WT X + b)). We consider a total of 30 clients where the local dataset sizes for
each client follow the power law. We set the mini batch-size to 10 and learning rate η = 0.01.
We report training loss as well as the mean and variance of test accuracies versus the number of
communication rounds in Figure 1 for the synthetic IID setting. We observe three key benefits of
DivFL compared to random sampling and power-of-choice approaches. On the one hand, DivFL
achieves a significant convergence speedup (〜10× faster) to reach the same loss and accuracy relative
to random sampling and power-of-choice. The convergence speed could potentially be attributed
to the reduction in the variance of updates across epochs as DivFL aims to minimize the gradient
approximation error w.r.t the true gradient. Furthermore, DivFL also achieves the lowest loss and
highest accuracy among the client selection approaches. By reaching a lower variance of accuracy in
comparison to the baselines, DivFL also shows marginal improvement in fairness, even when the
data distribution is IID.
As one would expect, while being impractical, utilizing the gradient computation from all clients to
update the dissimilarity matrix provides an upper bound on the achievable performance. Our “no
overhead” approach to update the dissimilarity matrix partially from only the participating clients
still outperforms the baselines in the test accuracy and training loss, while preserving the faster
convergence of DivFL. In Appendix B.1, we report the above metrics for different choices of K.
S3 Su88-1-
Num. of Seected Clients κ - 10
Figure 1: Training loss, mean and variance of test accuracies of DivFL compared with random
sampling and power-of-choice on the synthetic IID data. For DivFL (no overhead), we utilize only
the gradients from clients participating in the previous round. We see that DivFL achieves faster
convergence and converges to more accurate and slightly more fair solutions than all baselines.
We report the training loss as well as the mean and variance of testing accuracies for the synthetic
non-IID dataset in Figure 2. We notice the noisy updates of the random sampling approach and
slightly less noisy ones of the power-of-choice approach. In addition, both the baselines converge
to a less optimal solution than DivFL. The mean accuracy gains of DivFL are quite significant
(10% points). In terms of convergence, power-of-choice approach converges in 2x fewer rounds
than random sampling to reach an accuracy of 70% but DivFL converges in 5x fewer rounds with
less noisy updates than both baselines. The fairness gains of DivFL for the non-IID setting is more
significant. This is due to the higher degree of heterogeneity in client updates and the ability of
DivFL to find diverse representative clients in the update space. In Appendix B.1, we provide more
ablation studies for different choices of the hyperparameter K .
°5
2 1
S3 0c-c⅞F
Figure 2: Training loss and test accuracy of DivFL compared with random sampling and
power-of-choice on synthetic non-IID data. DivFL converges faster and to more accurate solutions
and much improved fairness than all baselines.
7
Published as a conference paper at ICLR 2022
5.2	Results on Real Datasets
We present extensive results on the robustness of DivFL on LEAF federated learning datasets. This
includes image datasets (FEMNIST, CelebA) and a language dataset (Shakespeare). For each of these
cases, we report the convergence behavior and fairness performance of DivFL.
For FEMNIST, there are a total of 500 clients where each client contains 3 out of 10 lowercase hand-
written characters. Clients use a CNN-based 10-class classifier model with two 5x5-convolutional
and 2x2-maxpooling (with a stride of 2) layers followed by a dense layer with 128 activations. For
our experiments, CelebA contains 515 clients where each client is a celebrity and a CNN-based binary
classifier is utilized with 4 3x3-convolutional and 2x2-maxpooling layers followed by a dense layer.
For Shakespeare, a two-layer LSTM classifier containing 100 hidden units with an 8D embedding
layer is utilized. The task is next-character prediction with 80 classes of characters in total. There are
a total of 109 clients. The model takes as input a sequence of 80 characters, embeds the characters
into a learned 8-dimensional space and outputs one character per training sample after 2 LSTM layers
and a densely-connected layer. For the Shakespeare case, each client performs 5 local updates in
every global round. For the other two datasets, client updates are shared after only 1 local update.
5.2.1	Convergence Behavior
We first present the convergence behavior of DivFL in comparison to the baselines for the above
3 datasets in Figure 3. On FEMNIST, DivFL converges faster than both random sampling and
power-of-choice approaches also with less noisy updates. DivFL with “no overhead” performs as
well as the ideal case. We note that in the case of FEMNIST, there are several clients that have
similar distribution over class labels. DivFL can be crucial in such cases where redundancies can be
minimized and diversity encouraged by maximizing a submodular function.
On CelebA, DivFL has a faster convergence than both baselines in the ideal setting. However,
the “no overhead” setting converges at the same rate as random sampling followed by the power-
of-choice approach. In the case of Shakespeare, interestingly, both DivFL and power-of-choice
approaches converge at the same rate and faster than random sampling (by 6x). Overall, for CelebA
and Shakespeare datasets, we note that DivFL either converges at least at the same rate as the fastest
baseline.
FEMNISΓ: Num. of Selected Clients K = IO
FEMNisr： Num. OfSelected Clients K = M)
0 Sβ lβθ IM 28	28	38 3M Ma
≠ Hounds
Figure 3:	Training loss and test accuracy of DivFL compared with random sampling and
power-of-choice on real datasets. We observe clear improvement for DivFL on FEMNIST. For the
other two datasets, the communication-efficient DivFL “no-overhead” converges at the same rate
as the fastest baseline.
5.2.2 Fairnes s
We also compare the fairness performance of the different approaches for the above 3 real datasets in
Figure 4. We measure this through the variance of test accuracies vs training rounds and observe both
the trajectory as well as the final variance of the test accuracies. For the image datasets FEMNIST and
CelebA, it’s clear that DivFL has better fairness after convergence with superior mean accuracy and
better/comparable variance of accuracies. This shows that diversity helps result in a more uniform
performance across clients. Interestingly for the case of Shakespeare, we observe the variance of
8
Published as a conference paper at ICLR 2022
accuracies for DivFL, especially under the “no overhead” setting is larger than both baselines. This
could possibly be the result of the sparsity in the gradient updates for language models, in general.
While the random sampling approach has lower accuracy variance, the mean accuracies are also
lower showing that it is not necessarily more “fair”.
O IM 200	3∞	400	58	68
≠ HoiIIXfc
Figure 4:	Variance of test accuracies over the 3 real datasets shows that DivFL has fairness benefits
on the image datasets. Poorer fairness performance for Shakespeare could be attributed to the sparsity
in the gradient updates for language models.
5.2.3 Impact of K and Learning Efficiency
We also empirically evaluate how the choice of number of clients per round (K) affects the final model
performance. We take the case of CelebA dataset and observe the (smoothened) mean and variance of
final test accuracies after 400 rounds for different choices of K. The final mean test accuracy generally
increases and then decreases for random sampling showing that more participation has the potential
to improve the final model performance but not always due to potential risk of overfitting. We also
note that for DivFL, the test accuracy improves as K grows to 30, but does not fall off as steeply
as in the case of random sampling. On the one hand, this highlights the robustness of DivFL. On
the other hand, DivFL achieves the highest mean accuracy of about 0.79 for K = 30. This shows
considerably fewer client participation in comparison to K = 60 for random sampling and K = 80
for power-of-choice to achieve their respective highest test accuracies. This reveals that DivFL offers
improved learning efficiency by its ability to learn from fewer clients per round compared to the
baselines. It must also be noted that DivFL outperforms both baselines in final mean and variance of
test accuracies for all choices of K in Figure 5.
Figure 5: Final Testing Accuracies (smoothened) after 400 epochs on CelebA dataset shows the
power of DivFL to have robust learning beating baselines for all choices of K. Furthermore, DivFL
can achieve the highest test accuracy and lowest variance using the smallest number of clients per
round in comparison to baselines.
6	Conclusion
In this paper, we posed the problem of selecting the most diverse subset of clients for federated
learning as the solution to maximizing a submodular facility location function defined over gradient
space. To this end, we developed DivFL and provided a thorough analysis of its convergence in
heterogeneous settings. Through extensive empirical results on both synthetic and real datasets,
we demonstrated strong gains of DivFL over state-of-the-art baseline client selection strategies
across key metrics including convergence speed, fairness, and learning efficiency. Furthermore, our
communication-efficient variant of DivFL holds all these advantages while making them amenable
to practical federated learning systems.
9
Published as a conference paper at ICLR 2022
7	Ethics and Reproducibility Statement
Our algorithm to select the most diverse set of clients not only aims to speed up learning but can
also allow for a fair representation of clients in the training process. This issue has been identified
as a key challenge in federated learning in several studies and algorithms exist that specifically
address this concern. In our paper, our approach is unique in addressing client diversity (without
any additional knowledge of client data) by encouraging diversity in the gradient space with (near)
optimality guarantees through the optimization framework.
In order to make our paper reproducible, we provide a detailed proof of Theorem 1 in Appendix A.
Further, detailed experimental description including 1) data preparation, 2) model architecture, 3)
training algorithm and hyper-parameters are provided in Section 5 and Appendix B.
Acknowledgements
This work was supported in part by the CONIX Research Center, one of six centers in JUMP, a
Semiconductor Research Corporation (SRC) program sponsored by DARPA.
10
Published as a conference paper at ICLR 2022
References
Ravikumar Balakrishnan, Mustafa Akdeniz, Sagar Dhakal, and Nageen Himayat. Resource man-
agement and fairness for federated learning over wireless edge networks. In 2020 IEEE 21st
International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), pp.
1-5,2020. doi:10.1109/SPAWC48557.2020.9154285.
Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse
m-best solutions in markov random fields. In ECCV, pp. 1-16, 2012.
Jeffrey A. Bilmes and Wenruo Bai. Deep submodular functions. CoRR, abs/1701.08939, 2017. URL
http://arxiv.org/abs/1701.08939.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings, 2019.
Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning,
2020.
Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Client selection in federated learning: Convergence
analysis and power-of-choice selection strategies, 2020.
Michele Conforti and Gerard Cornuejols. Submodular set functions, matroids and the greedy
algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem.
Discrete Applied Mathematics, 7(3):251-274, 1984.
G. Cornu6jols, M. Fisher, and G.L. Nemhauser. On the uncapacitated location problem. Annals of
Discrete Mathematics, 1:163-177, 1977.
Don Kurian Dennis, Tian Li, and Virginia Smith. Heterogeneity for the win: One-shot federated
clustering. arXiv preprint arXiv:2103.00697, 2021.
Satoru Fujishige. Submodular functions and optimization. Annals of discrete mathematics. Elsevier,
2005.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal
point processes. In NeurIPS, pp. 2735-2743, 2012.
Andrew Guillory and Jeff Bilmes. Active semi-supervised learning using submodular functions. In
Uncertainty in Artificial Intelligence (UAI), Barcelona, Spain, July 2011. AUAI.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In International Conference on Machine Learning,
2018.
Tiansheng Huang, Weiwei Lin, Li Shen, Keqin Li, and Albert Y. Zomaya. Stochastic client selection
for federated learning with volatile clients, 2021.
Rishabh Iyer, Stefanie Jegelka, and Jeff A. Bilmes. Fast semidifferential-based submodular function
optimization. In ICML, 2013.
Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury. Oort: Efficient federated
learning via guided participant selection. arxiv. org/abs/2010.06081, 2020.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems,
2020.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In ACL, pp.
510-520, 2011.
11
Published as a conference paper at ICLR 2022
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In International
Conference on Artificial Intelligence and Statistics, 2017.
Michel Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Opti-
mization Techniques, volume 7 of Lecture Notes in Control and Information Sciences, chapter 27,
pp. 234-243. Springer Berlin Heidelberg,1978.
Baharan Mirzasoleiman, AshWinkUmar Badanidiyuru, Amin Karbasi, Jan Vondrdk, and Andreas
Krause. Lazier than lazy greedy. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial
Intelligence, pp. 1812-1818, 2015.
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of
machine learning models. In International Conference on Machine Learning, 2020.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, 2019.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing
submodular set functions. Mathematical Programming, 14(1):265-294, 1978.
Takayuki Nishio and Ryo Yonetani. Client selection for federated learning With heterogeneous
resources in mobile edge. In ICC 2019-2019 IEEE International Conference on Communications,
2019.
Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse
subsets in exponentially-large structured item sets. In NeurIPS, pp. 2645-2653, 2014.
Monica Ribero and Haris Vikalo. Communication-efficient federated learning via optimal client
sampling. arXiv preprint arXiv:2007.15197, 2020.
Shengjie Wang, Wenruo Bai, Chandrashekhar Lavania, and Jeff Bilmes. Fixing mini-batch sequences
With hierarchical robust partitioning. In Proceedings of the Twenty-Second International Conference
on Artificial Intelligence and Statistics, volume 89, pp. 3352-3361, 2019.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Fast multi-stage submodular maximization. In ICML, 2014.
Dong Yin, AshWin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter
Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International
Conference on Artificial Intelligence and Statistics, 2018.
Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching With desirable
difficulties and scheduled diversity. In ICLR, 2018.
Appendix
A Complete Convergence Analysis
BeloW shoWs the convergence analysis. Many steps make a distinction betWeen if We are doing an
aggregating steps (from the clients to the server), or not (When the clients do not communicate). We
assume that We aggregate every E time steps. Define virtual sequences {vtk}k∈[N] and {wtk }k∈[N]
Where for all k ∈ [N],
vk+ι = wk - nNFk(Wt)
if not aggregating,
St+1 and average {vtk+1}k∈St+1 otherWise.
(8)
(9)
12
Published as a conference paper at ICLR 2022
Let
vt := X Pkvk,	(10)
k∈[N]
Wt := E Pkwk.	(11)
k∈[N]
where pk ≥ 0 is the given weight of the kth client and w.l.o.g., we assume Pk pk = 1. Therefore,
— ʃvt	if not aggregating, i.e., when t = 'E for some integer ',
wt一(春 Pi∈St Vl otherwise.
Let
gt ：= X PkFk(wk； Zk),
k∈[N]
and
vt+ι = Wt - ηt	EPkFk (wk ,Ztk)	：= Wt - ηtgt.
k∈[N]
We have
∣∣wt+1 - w*k2 = I∣wt+1 - vt+1 + vt+1 - w*k2
=I∣wt+1 - vt+1k2 + I∣vt+1 - w*k2 + 2hwt+ι - Vt+1, Vt+1 - w*i∙
If not aggregating,
(12)
(13)
(14)
(15)
(16)
wt+1 = Vt+1.	(17)
Hence
kwt+1 - w*k2 = kVt+1 - w*k2.	(18)
Using Lemma 1 in Li et al.(2019), We know E[∣Vt+ι 一 w*∣∣2] ≤ (1 一 ηtμ)E[∣wt 一 w*∣∣2] + η2B
holds for some constant B . If we are aggregating, we need to bound
E[∣wt+1 - Vt+ιk2] + E[∣Vt+ι - w*k2] + 2E[hwt+ι - Vt+1, Vt+1 - w*〉].	(19)
Let the last time of aggregation happens at step t0 = t + 1 - E, when we select a subset S (associated
with weights {γk}k∈S) using the greedy algorithm. Let ∆Vτk be the updates onVk at the τ-th iteration,
i.e., ΔvT ：= vT+1 - VT. Then Vt+1 = wto + N Pk∈[N] PT=t0 ΔvT. To bound the first term above,
∣∣wt+1 - Vt+1
1t
+N X γk x ∆vτ
k∈S	T=t0
W + N XX ∆vT
k∈[N] T=t0
—
=IX (NXγk∆vτ - N X ∆vτ
T=t0	k∈S	k∈[N]
≤X N X γk ∆vτ - N X ∆vkI
T=t0 I k∈S	k∈[N]	I
(20)
(21)
(22)
Similar to the CRAIG paper (Mirzasoleiman et al., 2020), we assume that the subset S selected in
step t0 = t + 1 - E provides an approximation of the full gradient such that
N X Yk VFk(Vk0) - N X VFk(vko)≤ °，	(23)
k∈S	k∈[N]	I
13
Published as a conference paper at ICLR 2022
For every local step τ ∈ (t0 , t], we use the same S to approximate the full gradient because we only
communicate the local gradients every E local steps. To bound the gradient approximation at step τ
using the stale S, we have
N XYkVFk(vT) - N X VFk(VT)
k∈S	k∈[N]
N X Yk VFk(vko)- N X VFk(vko)+
k∈S	k∈[N]
(25)
+
(24)
N X VFk (vτ)- N X VFk(vko)	(26)
k∈[N]	k∈[N]
τ
≤ 2LG X ην + ,	(27)
ν=t0
where the first and the third term on the right hand side are bounded using the L-Smoothness of Fk(∙)
and G-bounded norm of its stochastic gradient. Hence, we can continue to bound the first term in
Eq. (22) by
t
kWt+1- Vt+lk ≤ X
τ=t0
N X Yk∆vτ- ⅛ X ∆vT
k∈S	k∈[N]
t 1	1
=X ητ	N X YkVFk(VT) - N X VFk(VT)
τ=t0	k∈S	k∈[N]
tT
≤ 2LGXX
ηTην + EηT
T=t0 ν=t0
≤ LGE(E - 1)ηt20 + Eηt0
(28)
(29)
(30)
(31)
LGE(E - 1) (1 + t + YE-(E - 1)) η2 + Ee (1 + t + Yj-(E - 1)) ηt
(32)
where E is the number of local steps between two communication (aggregation) rounds. Therefore,
Eq. (19) can be bounded as follows:
E[kwt+1 - w*k2]	(33)
≤E[∣∣Wt+ι - Vt+ιk2]+ E[∣∣Vt+ι - w*∣∣2] + 2E[hWt+ι - Vt+1,Vt+1 - w*〉]	(34)
≤ (LGE(E - 1)η2o + EentO)2 + [(1 - ηtμ)E[kwt - w*k2] + η2B] + 2 (LGE(E - 1)η2 + EentO) E[kvt+ι - w*k]
(35)
≤(I- ntμ)E[kwt - w*k2] + EePnto + [LGE(E -I)P + (LGE(E - 1)nto + Ee)2] n20 + Bn2
(36)
≤(I- ntμ)E[kwt - w*k2] + eρEnto + [LGρ + (LGEnt。+e)2] E2n2o + Bn2,	(37)
14
Published as a conference paper at ICLR 2022
where ηt =亳,ηt0 =右-石：1十7 and E[kVt+ι - w* k] ≤ ρ, shown as follows.
vt+1 - E Piv
i∈[N]
≤ E vt+1 - E Pi V
i∈[N]
+ E	Pivi* - w*
i∈[N]
+M
≤ E Ekpi(Vt+1-v*)k + M
i∈[N]
≤ X piEWfi(Vt)Il + M
i∈[N] μ
≤ G + M ≤ ρ.
μ
(38)
(39)
(40)
(41)
(42)
The final convergence rates follows from Lemma 3 in Mirzasoleiman et al. (2020).
B Additional Experiments
B.1 More ablation studies on Synthetic and FEMNIST Datasets
We present more findings from training DivFL on synthetic IID and non-IID dataset for K = 20
in Figure 6, 7 and 8. In addition to the training loss, mean and variance of test accuracies, we also
plot the 10th percentile test accuracy that shows the worst-case performance of the different client
selection strategies. As in the case of K = 10, we observe gains across convergence, fairness and
improved model accuracy for DivFL. In addition, the 10th percentile accuracy further confirms that
DivFL improves the worst-case clients’ accuracies.
Num. OfSelected Clients K = 20
O 10	20	30	«	50	60
# Rounds
--κcuf^oo∈H- U-Sc8bαcsI
Num. of Selected Clients K= 20
1 nds
Figure 6: We measure the training loss, mean and variance of test accuracies as well as 10th
percentile test accuracy for DivFL on synthetic IID dataset for K = 20.
B.2 IMPACT OF NUMBER OF LOCAL EPOCHS τ
In our experiments, we already adopted local updating schemes with a fixed number of local epochs
(τ) being 1 (running multiple local iterations). However, we further show the robustness of DivFL
under different choices of τ via observing the training loss and variance of test accuracies. The results
for Synthetic IID, non-IID and FEMNIST are presented in Figure 9, 10, 11, 12, 13 for different values
of τ. We observe that for different choices of the number of clients K and the number of local epochs
15
Published as a conference paper at ICLR 2022
Num. of selected Clients K = 20
0.9
-.0β
5 0.7
∣0∙β
*5
Num. OfSelected Clients K = 20
0	10	20	30	«	50	60
# Rounds
Num. of Selected Clients K= 20
1 nds
Figure 7: The training loss, mean and variance of test accuracies as well as 10th percentile test
accuracy for DivFL on synthetic non-IID dataset for K = 20.
Num. Cf SeteCte<l CIientS K = 2。
Num. Ofseiected Clients K = 20
Figure 8: Training loss, mean and variance of test accuracies as well as 10th percentile test accuracy
for DivFL on FEMNIST dataset for K = 20.
τ , both variants of DivFL converge faster than both the baselines. The fairness gains over random
sampling are also preserved under large values of τ as shown in Figure 13.
16
Published as a conference paper at ICLR 2022
2.25
^2.00
3 1.75
ct1-50
.≡ 1.25
- 1.00
2 0.75
0.50
0.25
K = 10, Local epochs τ =5
-------Random Sampling
Power-of-Choice
-------DivFL (ideal)
、------ DiVFL (no overhead)
K = 10, Local epochs τ =10
Q 5 Q 5
2 110
sso~∣ 6u-u-eJ
----Random Sampling
Power-of-Choice
----DivFL (ideal)
---- DivFL (no overhead)
2.25
^2.00
S 1.75
ct1-50
.≡ 1.25
- 1.00
R 0.75
0.50
0.25
0	50 100 150 200 250
# Rounds
K = 20, Local epochs τ =5
0
Q 5 Q 5
2 110
sso~∣ 6u 三
0	50 100 150 200 250 300
# Rounds
K = 20, Local epochs τ =10
----Random Sampling
Power-of-Choice
----DivFL (ideal)
---- DivFL (no overhead)
50 100 150 200 250 300	0	50 100 150 200 250 300
# Rounds	# Rounds
Figure 9: Training Loss on Synthetic IID for different choices of τ .
K
Local
τ =5
⊂ 0.030
,ra 0.025
>0.020
§ 0.015
^0.010
	V		Random Sampling Power-of-Choice 	DivFL (ideal) 	 DivFL (no overhead)
		>vj~∖⅛y√⅛⅜-∕r⅛,r .. . =F
H 0.035
- 0.030
> 0.025
&0.020
§ 0.015
y o.oιo
« 0.005
K - 10, Local epochs τ =10
-------Random Sampling
Power-of-Choice
-------DivFL (ideal)
U ----- DivFL (no overhead)
	
0	50 100 150 200 250 300
# Rounds
K = 20, Local epochs τ =5
0	50 100 150 200 250 300
# Rounds
K = 20, Local epochs τ =10
U 0.035 2 0.030			Random Sampling	占 0.035 0.030				Random Sampling
		Power-of-Choice				Power-of-Choice
> 0.025			DivFL (ideal)	> 0.025				DivFL (ideal)
u0.020		V1I	 DiVFL (no overhead)	>0.020		lʌ		 DivFL (no overhead)
⅛ 0.015 ⅛ 0.010			Accura ppp goo Ul O Ul			
						
100 150 200 250 300
# Rounds
100 150 200 250 300
# Rounds
Figure 10: Variance of Test Accuracy on Synthetic IID
K = 10, Local epochs τ =5	K = 10, Local epochs τ =10
3.0
S
ω o t-
C> 2.5
62.0
C
⊂ 1.5
,ro
匚1.0
0.5
I -------- Random Sampling
ι -------- Power-of-Choice
⅛	---- DivFL (ideal)
∖	---- DivFL (no overhead)
0 5 0 5 0 5
3 2 2 1 1 0
sso~∣ 6u 三屈
0	50 100 150 200 250 300
# Rounds
K = 20, Local epochs τ =5
3.0
S
ω o t-
C> 2.5
ct2.0
c
⊂ 1.5
,(5
匚1.0
0.5
I -------- Random Sampling
I -------- Power-of-Choice
∖	---- DivFL (ideal)
∖>	---- DivFL (no overhead)
0	50	100 150 200 250 300
# Rounds
K = 20, Local epochs τ =10
0 5 0 5 0 5
3 2 2 1 1 0
sso~∣ 6u 三屈
0	50	100 150 200 250 300
# Rounds
Figure
0	50 100 150 200 250 300
# Rounds
11: Training Loss on Synthetic non-IID for different choices of τ .
17
Published as a conference paper at ICLR 2022
φ0.16
c 0.14
- 0.12
§0.10
>0.08
世 0.06
30.04
30.02
K = 10, Local epochs τ =5
L ---- Random Sampling
Λ ---- Power-of-Choice
W——DivFL (ideal)
⅛ ---- DivFL (no overhead)
① 0.16
⊂ 0.14
ɪ 0.12
>0.10
>0.08
£0.06
U 0.04
< 0.02
0	50 100 150 200 250 300
K = 10, Local epochs τ =10
N ------ Random Sampling
⅛ ------ Power-of-Choice
W—— DivFL (ideal)
V ----- DivFL (no overhead)

0	50 100 150 200 250 300
# Rounds
① 0.16
c 0.14
■Z 0.12
>0.10
>0.08
2 0.06
y 0.04
< 0.02
K = 20, Local epochs τ =5
0	50 100 150 200 250 300
# Rounds
Figure 12: Variance of Test Accuracy on Synthetic non-IID
# Rounds
K = 20, Local epochs τ =10
① 0.16
⊂ 0.14
m
ɪ 0.12
ra
> 0.10
&0.08
目 0.06
U 0.04
« 0.02
	A 	 Random Sampling ⅛∖ 	 Power-of-Choice
	——DivFL (ideal) M ⅛ ~⅛ DivFL (no overhead)
	
0	50 100 150 200 250 300
# Rounds
Figure 13: Faster convergence benefits of DivFL over Random Sampling and Power-of-Choice
approaches is preserved even when clients run multiple local epochs before sharing model updates.
18