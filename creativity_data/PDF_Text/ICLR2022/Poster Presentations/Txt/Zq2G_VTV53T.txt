Published as a conference paper at ICLR 2022
FastSHAP: Real-Time Shapley Value
Estimation
Neil Jethani *
New York University
Mukund Sudarshan*
New York University
Ian Covert*
University of Washington
Su-In Lee
University of Washington
Rajesh Ranganath
New York University
Ab stract
Although Shapley values are theoretically appealing for explaining black-box
models, they are costly to calculate and thus impractical in settings that involve
large, high-dimensional models. To remedy this issue, we introduce FastSHAP,
a new method for estimating Shapley values in a single forward pass using a
learned explainer model. To enable efficient training without requiring ground
truth Shapley values, we develop an approach to train FastSHAP via stochastic
gradient descent using a weighted least squares objective function. In our experi-
ments with tabular and image datasets, we compare FastSHAP to existing estima-
tion approaches and find that it generates accurate explanations with an orders-of-
magnitude speedup.
1	Introduction
With the proliferation of black-box models, Shapley values (Shapley, 1953) have emerged as a
popular explanation approach due to their strong theoretical properties (Lipovetsky and Conklin,
2001; Strumbelj and Kononenko, 2014; Datta et al., 2016; LUndberg and Lee, 2017). In practice,
however, Shapley value-based explanations are known to have high computational complexity, with
an exact calculation requiring an exponential number of model evaluations (Van den Broeck et al.,
2021). Speed becomes a critical issue as models increase in size and dimensionality, and for the
largest models in fields such as computer vision and natural language processing, there is an unmet
need for significantly faster Shapley value approximations that maintain high accuracy.
Recent work has addressed the computational challenges with Shapley values using two main ap-
proaches. First, many works have proposed stochastic estimators (Castro et al., 2009; Strumbelj
and Kononenko, 2014; Lundberg and Lee, 2017; Covert et al., 2020) that rely on sampling either
feature subsets or permutations; though often consistent, these estimators require many model eval-
uations and impose an undesirable trade-off between run-time and accuracy. Second, some works
have proposed model-specific approximations, e.g., for trees (Lundberg et al., 2020) or neural net-
works (Shrikumar et al., 2017; Chen et al., 2018b; Ancona et al., 2019; Wang et al., 2021); while
generally faster, these approaches can still require many model evaluations, often induce bias, and
typically lack flexibility regarding the handling held-out features—a subject of ongoing debate in
the field (Aas et al., 2019; Janzing et al., 2020; Frye et al., 2020; Covert et al., 2021).
Here, we introduce a new approach for efficient Shapley value estimation: to achieve the fastest
possible run-time, we propose learning a separate explainer model that outputs precise Shapley value
estimates in a single forward pass. Naively, such a learning-based approach would seem to require
a large training set of ground truth Shapley values, which would be computationally intractable.
Instead, our approach trains an explainer model by minimizing an objective function inspired by
the Shapley value’s weighted least squares characterization (Charnes et al., 1988), which enables
efficient gradient-based optimization.
Our contributions. We introduce FastSHAP, an amortized approach for generating real-time Shap-
ley value explanations.* 1 We derive an objective function from the Shapley value’s weighted least
* Equal contribution
1https://git.io/JCqFV (PyTorch), https://git.io/JCqbP (TensorFlow)
1
Published as a conference paper at ICLR 2022
U-」ds
q=6uu
-End Wfflo
FastSHAP
KemeISHAP
KerneISHAP-S
GradCAM
Integrated
Gradients
SmoothGrad DeepSHAP
Figure 1: Explanations generated by each method for Imagenette images.
CXPIain
squares characterization and investigate several ways to reduce gradient variance during training.
Our experiments show that FastSHAP provides accurate Shapley value estimates with an orders-
of-magnitude speedup relative to non-amortized estimation approaches. Finally, we also find that
FastSHAP generates high-quality image explanations (fig. 1) that outperform gradient-based meth-
ods (e.g., IntGrad and GradCAM) on quantitative inclusion and exclusion metrics.
2	Background
In this section, we introduce notation used throughout the paper and provide an overview of Shapley
values and their weighted least squares characterization. Let x ∈ X be a random vector consisting
of d features, or x = (x1, . . . , xd), and let y ∈ Y = {1, . . . , K} be the response variable for a
classification problem. We use s ∈ {0, 1}d to denote subsets of the indices {1, . . . , d} and define
xs := {xi}i:si=1. The symbols x, y, s are random variables and x, y, s denote possible values. We
use 1 and 0 to denote vectors of ones and zeros in Rd, so that 1>s is a subset’s cardinality, and we
use ei to denote the ith standard basis vector. Finally, f (x; η) : X 7→ ∆K-1 is a model that outputs
a probability distribution over y given x, and fy(x; η) is the probability for the yth class.
2.1	Shapley values
Shapley values were originally developed as a credit allocation technique in cooperative game the-
ory (Shapley, 1953), but they have since been adopted to explain predictions from black-box ma-
chine learning models (Strumbelj and Kononenko, 2014; Datta et al., 2016; LUndberg and Lee,
2017). For any value function (or set function) v : 2d 7→ R, the Shapley values φ(v) ∈ Rd, or
φi(v) ∈ R for each feature i = 1, . . . , d, are given by the formula
φi(v)
s + ei)
(1)
The difference v(s + ei) - v(s) represents the ith feature’s contribution to the subset s, and the
summation represents a weighted average across all subsets that do not include i. In the model
explanation context, the value function is chosen to represent how an individual prediction varies
as different subsets of features are removed. For example, given an input-output pair (x, y), the
prediction for the yth class can be represented by a value function vx,y defined as
vx,y(s) = link E	[fy (xs, x1-s; η)] ,	(2)
p(x1-s )
where the held out features x1-s are marginalized out using their joint marginal distribution
p(x1-s), and a link function (e.g., logit) is applied to the model output. Recent work has debated
the properties of different value function formulations, particularly the choice of how to remove
features (Aas et al., 2019; Janzing et al., 2020; Frye et al., 2020; Covert et al., 2021). However,
regardless of the formulation, this approach to model explanation enjoys several useful theoretical
properties due to its use of Shapley values: for example, the attributions are zero for irrelevant fea-
tures, and they are guaranteed to sum to the model’s prediction. We direct readers to prior work for
a detailed discussion of these properties (Lundberg and Lee, 2017; Covert et al., 2021).
Unfortunately, Shapley values also introduce computational challenges: the summation in eq. (1)
involves an exponential number of subsets, which makes it infeasible to calculate for large d. Fast
approximations are therefore required in practice, as we discuss next.
2
Published as a conference paper at ICLR 2022
2.2	KERNELSHAP
KernelSHAP (Lundberg and Lee, 2017) is a popular Shapley value implementation that relies on
an alternative Shapley value interpretation. Given a value function vx,y(s), eq. (1) shows that the
values φ(vx,y) are the features’ weighted average contributions; equivalently, their weighted least
squares characterization says that they are the solution to an optimization problem over φx,y ∈ Rd,
φ(vχ,y)= argmin E	(Vχ,y(S) — Vχ,y(0) — s>Φx,y)2
φx,y	p(s)
s.t.	1 φx,y = vx,y (1) — vx,y (0),
where the distribution p(s) is defined as
p ( S ) H
d-1
(1 ds) ∙ 1 Ts ∙ (d- 1 > S)
(3)
(Efficiency constraint)
(Shapley kernel)
for s such that 0 < 1TS < d (Charnes et al., 1988). Based on this view of the Shapley value,
Lundberg and Lee (2017) introduced KernelSHAP, a stochastic estimator that solves an approxi-
mate version of eq. (3) given some number of subsets sampled from p(s). Although the estimator
is consistent and empirically unbiased (Covert and Lee, 2021), KernelSHAP often requires many
samples to achieve an accurate estimate, and it must solve eq. (3) separately for each input-output
pair (x, y). As a result, it is unacceptably slow for some use cases, particularly in settings with large,
high-dimensional models. Our approach builds on KernelSHAP, leveraging the Shapley value’s
weighted least squares characterization to design a faster, amortized estimation approach.
3	FastSHAP
We now introduce FastSHAP, a method that amortizes the cost of generating Shapley values across
many data samples. FastSHAP has two main advantages over existing approaches: (1) it avoids
solving separate optimization problems for each input to be explained, and (2) it can use similar
data points to efficiently learn the Shapley value function φ(vx,y).
3.1	Amortizing Shapley values
In our approach, we propose generating Shapley value explanations using a learned parametric func-
tion φfast (x, y; θ) : X × Y 7→ Rd. Once trained, the parametric function can generate explanations
in a single forward pass, providing a significant speedup over methods that approximate Shapley val-
ues separately for each sample (x, y). Rather than using a dataset of ground truth Shapley values for
training, we train φfast(x, y; θ) by penalizing its predictions according to the weighted least squares
objective in eq. (3), or by minimizing the following loss,
L(θ ) = p Jx) unE(y) PE) [(v x,y (S)-V x,y (O)-S >φ fast(x xy;θ ))2],	(4)
where Unif(y) represents a uniform distribution over classes. If the model’s predictions are forced
to satisfy the Efficiency constraint, then given a large enough dataset and a sufficiently expressive
model class for φfast, the global optimizer φfast(XX y; θ*) is a function that outputs exact Shapley
values (see proof in appendix A). Formally, the global optimizer satisfies the following:
φfast(x, y; θ^) = Φ(VX,y) almost surely in P(X, y).	(5)
We explore two approaches to address the efficiency requirement. First, we can enforce efficiency
by adjusting the Shapley value predictions using their additive efficient normalization (Ruiz et al.,
1998), which applies the following operation to the model’s outputs:
φfafJ x,y ； θ) = φ fast( x,y ； θ ) + d (vx,y ⑴-vx,y ⑼-1 >φ fast( x,y;θ)).	⑹
|--------------------V-------------------}
Efficiency gap
The normalization step can be applied at inference time and optionally during training; in ap-
pendix B, we show that this step is guaranteed to make the estimates closer to the true Shapley values.
Second, we can relax the efficiency property by augmenting L(θ) with a penalty on the efficiency
gap (see eq. (6)); the penalty requires a parameter γ > 0, and as we set γ → ∞ we can guarantee
that efficiency holds (see appendix A). Algorithm 1 summarizes our training approach.
3
Published as a conference paper at ICLR 2022
Empirical considerations. Optimizing L(θ) using a single set of samples (x, y, s) is problematic
because of high variance in the gradients, which can lead to poor optimization. We therefore consider
several steps to reduce gradient variance. First, as is conventional in deep learning, we minibatch
across multiple samples from p(x). Next, when possible, we calculate the loss jointly across all
classes y ∈ {1,..., K}. Then, We experiment with using multiple samples S 〜P(S) for each input
sample x. Finally, we explore paired sampling, where each sample s is paired with its complement
1 - s, which has been shown to reduce KernelSHAP’s variance (Covert and Lee, 2021). Appendix C
shows proofs that these steps are guaranteed to reduce gradient variance, and ablation experiments
in appendix D demonstrate their improvement on FastSHAP’s accuracy.
3.2	A default value function for FastSHAP
FastSHAP has the flexibility to work with
any value function vx,y (s). Here, we de-
scribe a default value function that is useful
for explaining predictions from a classifica-
tion model.
The value function’s aim is to assess, for
each subset s, the classification probability
when only the features xs are observed. Be-
cause most models f(x; η) do not support
making predictions without all the features,
we require an approximation that simulates
the inclusion of only xs (Covert et al., 2021).
To this end, we use a supervised surrogate
model (Frye et al., 2020; Jethani et al., 2021)
to approximate marginalizing out the remain-
ing features x1-s using their conditional dis-
tribution.
Algorithm 1: FastSHAP training
Input: Value function Vxy, learning rate α
Output: FastSHAP explainer φfast (x, y; θ)
initialize φfast(x, y; θ)
while not converged do
sample x 〜P(x), y 〜Unif(y), S 〜P(S)
ʌ
predict φ — φfast(x,y; θ)
if normalize then
ʌ
set φ J
Φ + d- 1 "y (1) - Vx,y (0) - 1Tφ
end
calculate	2
L J (Vx,y ( S) - vx,y (0) - sTφ
update θ J θ — OQ § L
end
Separate from the original model f(x; η), the surrogate model Psurr(y | m(x, S); β) takes as input
a vector of masked features m(x, S), where the masking function m replaces features xi such that
Si = 0 with a [mask] value that is not in the support of X. Similar to prior work (Frye et al., 2020;
Jethani et al., 2021), the parameters β are learned by minimizing the following loss function:
L (β ) = PE PE)ID KL (f(x; η) Il P surr(y I m (X, S); β))] -	(7)
It has been shown that the global optimizer to eq. (7), or Psurr(y ∣ m(x, s); β*), is equiva-
lent to marginalizing out features from f(X; η) with their conditional distribution (Covert et al.,
2021):
Psurr (y ∣ m (x,s ); β* ) = E[ fy (x； n) I X S = Xs ].	(8)
The choice of distribution over P(S) does not affect the global optimizer of eq. (7), but we use the
Shapley kernel to put more weight on subsets likely to be encountered when training FastSHAP. We
use the surrogate model as a default choice for two reasons. First, it requires a single prediction for
each evaluation of Vx,y(S), which permits faster training than the common approach of averaging
across many background samples (Lundberg and Lee, 2017; Janzing et al., 2020). Second, it yields
explanations that reflect the model’s dependence on the information communicated by each feature,
rather than its algebraic dependence (Frye et al., 2020; Covert et al., 2021).
4	Related Work
Recent work on Shapley value explanations has largely focused on how to remove features (Aas
et al., 2019; Frye et al., 2020; Covert et al., 2021) and how to approximate Shapley values effi-
ciently (Chen et al., 2018b; Ancona et al., 2019; Lundberg et al., 2020; Covert and Lee, 2021).
Model-specific approximations are relatively fast, but they often introduce bias and are entangled
with specific feature removal approaches (Shrikumar et al., 2017; Ancona et al., 2019; Lundberg
et al., 2020). In contrast, model-agnostic stochastic approximations are more flexible, but they must
trade off run-time and accuracy in the explanation. For example, KernelSHAP samples subsets to
approximate the solution to a weighted least squares problem (Lundberg and Lee, 2017), while other
4
Published as a conference paper at ICLR 2022
# Evals	# Evals	# Evals
KerneISHAP KerneISHAP (Paired) Permutation Sampling ■ ■ ■ι Permutation Sampling (Antithetical) FastSHAP
Figure 2: Comparison of Shapley value approximation accuracy across methods. Using three datasets,
we measure the distance of each method’s estimates to the ground truth as a function of the number of model
evaluations. FastSHAP is represented by a horizontal line since it requires only a single forward pass. The
baselines require 200-2000× model evaluations to achieve FastSHAP’s level of accuracy.
approaches sample marginal contributions (Castro et al., 2009; Strumbelj and Kononenko, 2014) or
feature permutations (Ill6s and Kerenyi, 2019; Mitchell et al., 2021). FaStSHAP trains an explainer
model to output an estimate that would otherwise require orders of magnitude more model evalu-
ations, and, unlike other fast approximations, it is agnostic to the model class and feature removal
approach.
Other methods have been proposed to generate explanations using learned explainer models. These
are referred to as amortized explanation methods (Covert et al., 2021; Jethani et al., 2021), and
they include several approaches that are comparable to gradient-based methods in terms of compute
time (Dabkowski and Gal, 2017; Chen et al., 2018a; Yoon et al., 2018; Schwab and Karlen, 2019;
Schulz et al., 2020; Jethani et al., 2021). Notably, one approach generates a training dataset of ground
truth explanations and then learns an explainer model to output explanations directly (Schwab and
Karlen, 2019)—a principle that can be applied with any attribution method, at least in theory. How-
ever, for Shapley values, generating a large training set would be very costly, so FastSHAP sidesteps
the need for a training set using a custom loss function based on the Shapley value’s weighted least
squares characterization (Charnes et al., 1988).
5	Structured Data Experiments
We analyze FastSHAP’s performance by comparing it to several well-understood baselines. First,
we evaluate its accuracy on tabular (structured) datasets by comparing its outputs to the ground
truth Shapley values. Then, to disentangle the benefits of amortization from the in-distribution
value function, we make the same comparisons using different value function formulations vx,y (s).
Unless otherwise stated, we use the surrogate model value function introduced in section 3.2. Later,
in section 6, we test FastSHAP’s ability to generate image explanations.
Baseline methods. To contextualize FastSHAP’s accuracy, we compare it to several non-
amortized stochastic estimators. First, we compare to KernelSHAP (Lundberg and Lee, 2017) and
its acceleration that uses paired sampling (Covert and Lee, 2021). Next, we compare to a permu-
tation sampling approach and its acceleration that uses antithetical sampling (Mitchell et al., 2021).
As a performance metric, we calculate the proximity to Shapley values that were obtained by run-
ning KernelSHAP to convergence; we use these values as our ground truth because KernelSHAP is
known to converge to the true Shapley values given infinite samples (Covert and Lee, 2021). These
baselines were all run using an open-source implementation.2
Implementation details. We use either neural networks or tree-based models for each of f (x; η)
and psurr (y | m(x, s); β). The FastSHAP explainer model φfast (x, y; θ) is implemented with a
network g(x; θ) : X → Rd × Y that outputs a vector of Shapley values for every y ∈ Y; deep neural
networks are ideal for FastSHAP because they have high representation capacity, they can provide
many-to-many mappings, and they can be trained by stochastic gradient descent. Appendix D con-
tains more details about our implementation, including model classes, network architectures and
training hyperparameters.
2https://github.com/iancovert/shapley-regression/ (License: MIT)
5
Published as a conference paper at ICLR 2022
Marginal
KerneISHAP KerneISHAP (Paired) Permutation Sampling ■ ■ ■ι Permutation Sampling (Antithetical) FastSHAP
Figure 3: FastSHAP approximation accuracy for different value functions. Using the marketing
dataset, we find that FastSHAP provides accurate Shapley value estimates regardless of the value function
(surrogate, marginal, baseline), with the baselines requiring 200-1000× model evaluations to achieve Fast-
SHAP’s level of accuracy. Error bars represent 95% confidence intervals.
We also perform a series of experiments to determine several training hyperparameters for FastSHAP,
exploring (1) whether or not to use paired sampling, (2) the number of subset samples to use, and
(3) how to best enforce the efficiency constraint. Based on the results (see appendix D), we use the
following settings for our tabular data experiments: we use paired sampling, between 32 and 64
samples of s per x sample, additive efficient normalization during both training and inference, and
we set γ = 0 (since the normalization step is sufficient to enforce efficiency).
5.1	Accuracy of FastSHAP explanations
Here, we test whether FastSHAP’s estimates are close to the ground truth Shapley values. Our exper-
iments use data from a 1994 United States census, a bank marketing campaign, bankruptcy
statistics, and online news articles (Dua and Graff, 2017). The census data contains 12 input fea-
tures, and the binary label indicates whether a person makes over $50K a year (Kohavi et al., 1996).
The marketing dataset contains 17 input features, and the label indicates whether the customer
subscribed to a term deposit (Moro et al., 2014). The bankruptcy dataset contains 96 features de-
scribing various companies and whether they went bankrupt (Liang et al., 2016). The news dataset
contains 60 numerical features about articles published on Mashable, and our label indicates whether
the share count exceeds the median number (1400) (Fernandes et al., 2015). The datasets were each
split 80/10/10 for training, validation and testing.
In fig. 2, we show the distance of each method’s estimates to the ground truth as a function of the
number of model evaluations for the news, census and bankruptcy datasets. Figure 3 shows
results for the marketing dataset with three different value functions (see section 5.2). For the
baselines, each sample s requires evaluating the model given a subset of features, but since Fast-
SHAP requires only a single forward pass of φfast (x, y; θ), we show it as a horizontal line.
To reach FastSHAP’s level of accuracy on the news, census and bankruptcy datasets, Ker-
nelSHAP requires between 1,200-2,000 model evaluations; like prior work (Covert and Lee, 2021),
we find that paired sampling improves KernelSHAP’s rate of convergence, helping reach Fast-
SHAP’s accuracy in 250-1,000 model evaluations. The permutation sampling baselines tend to be
faster: the original version requires between 300-1,000 evaluations, and antithetical sampling takes
200-500 evaluations to reach an accuracy equivalent to FastSHAP. Across all four datasets, however,
FastSHAP achieves its level of accuracy at least at least 600× faster than the original version of
KernelSHAP, and 200× faster than the best non-amortized baseline.
5.2	Disentangling amortization and the choice of value function
In this experiment, we verify that FastSHAP produces accurate Shapley value estimates regardless
of the choice of value function. We use the marketing dataset for this experiment and test the
following value functions:
1.	(Surrogate/In-distribution) vx,y(s) = psurr(y | m(x, s); β)
2.	(Marginal/Out-of-distribution) vx,y(s) = Ep(x1-s) [fy (xs, x1-s; η)]
3.	(Baseline removal) vx,y (s) = fy(xs, xb1-s; η), where xb ∈ X are fixed baseline values (the
mean for continuous features and mode for discrete ones)
6
Published as a conference paper at ICLR 2022
SmoothGrad
DeepSHAP	CXPIaIn
Figure 4: Explanations generated by each method for CIFAR-10 images.
In fig. 3 we compare FastSHAP to the same non-amortized baseline methods, where each method
generates Shapley value estimates using the value functions listed above. The results show that
FastSHAP maintains the same computational advantage across all three cases: to achieve the same
accuracy as FastSHAP’s single forward pass, the baseline methods require at least 200 model evalu-
ations, but in some cases up to nearly 1,000.
6	Image Experiments
Images represent a challenging setting for Shapley values due to their high dimensionality and the
computational cost of model evaluation. We therefore compare FastSHAP to KernelSHAP on two
image datasets. We also consider several widely used gradient-based explanation methods, because
they are the most commonly used methods for explaining image classifiers.
6.1	Datasets
We consider two popular image datasets for our experiments. CIFAR-10 (Krizhevsky et al., 2009)
contains 60,000 32 × 32 images across 10 classes, and we use 50,000 samples for training and
5,000 samples each for validation and testing. Each image is resized to 224 × 224 using bilinear
interpolation to interface with the ResNet-50 architecture (He et al., 2016). Figure 4 shows example
CIFAR-10 explanations generated by each method. The Imagenette dataset (Howard and Gugger,
2020), a subset of 10 classes from the ImageNet dataset, contains 13,394 total images. Each image
is cropped to keep the 224 × 224 central region, and the data is split 9,469/1,963/1,962. Example
Imagenette explanations are shown in fig. 1.
6.2	Explanation methods
We test three Shapley value estimators, FastSHAP, KernelSHAP, and DeepSHAP (Lundberg and
Lee, 2017), where the last is an existing approximation designed for neural networks. We test Ker-
nelSHAP with the zeros baseline value function, which we refer to simply as KernelSHAP, and
with the in-distribution surrogate value function, which we refer to as KernelSHAP-S. We also com-
pare these methods to the gradient-based explanation methods GradCAM (Selvaraju et al., 2017),
SmoothGrad (Smilkov et al., 2017) and IntGrad (Sundararajan et al., 2017). Gradient-based methods
are relatively fast and have therefore been widely adopted for explaining image classifiers. Finally,
we also compare to CXPlain (Schwab and Karlen, 2019), an amortized explanation method that
generates attributions that are not based on Shapley values.
Implementation details. The models f(x; η) and psurr(y | m(x, s); β) are both ResNet-50 net-
works (He et al., 2016) pretrained on ImageNet and fine-tuned on the corresponding imaging dataset.
FastSHAP, CXPlain, and KernelSHAP are all implemented to output 14 × 14 superpixel attributions
for each class. For FastSHAP, we parameterize φfast (x, y; θ) to output superpixel attributions: we
use an identical pretrained ResNet-50 but replace the final layers with a 1 × 1 convolutional layer
so that the output is 14 × 14 × K (see details appendix D). We use an identical network to produce
attributions for CXPlain. For FastSHAP, we do not use additive efficient normalization, and we
set γ = 0; we find that this relaxation of the Shapley value’s efficiency property does not inhibit
FastSHAP’s ability to produce high-quality image explanations. KernelSHAP and KernelSHAP-S
are implemented using the shap3 package’s default parameters, and GradCAM, SmoothGrad, and
IntGrad are implemented using the tf-explain4 package’s default parameters.
3https://shap.readthedocs.io/en/latest/ (License: MIT)
4https://tf-explain.readthedocs.io/en/latest/ (License: MIT)
7
Published as a conference paper at ICLR 2022
6.3	Qualitative remarks
Explanations generated by each method are shown in fig. 4 for CIFAR-10 and fig. 1 for Imagenette
(see appendix E for more examples). While a qualitative evaluation is insufficient to draw conclu-
sions about each method, we offer several remarks on these examples. FastSHAP, and to some extent
GradCAM, appear to reliably highlight the important objects, while the KernelSHAP explanations
are noisy and fail to localize important regions. To a lesser extent, CXPlain occasionally highlights
important regions. In comparison, the remaining methods (SmoothGrad, IntGrad and DeepSHAP)
are granulated and highlight only small parts of the key objects. Next, we consider quantitative
metrics that test these observations more systematically.
6.4	Quantitative evaluation
Evaluating the quality of Shapley value estimates
requires access to ground truth Shapley values,
which is computationally infeasible for images. In-
stead, we use two metrics that evaluate an expla-
nation’s ability to identify informative image re-
gions. These metrics build on several recent pro-
posals (Petsiuk et al., 2018; Hooker et al., 2018;
Jethani et al., 2021) and evaluate the model’s clas-
sification accuracy after including or excluding pix-
els according to their estimated importance.
Similar to Jethani et al. (2021), we begin by train-
ing a single evaluation model peval to approximate
the f (x; η) model’s output given a subset of fea-
tures; this serves as an alternative to training sepa-
rate models on each set of features (Hooker et al.,
2018) and offers a more realistic option than mask-
ing features with zeros (Schwab and Karlen, 2019).
This procedure is analogous to the psurr training
procedure in section 3.2, except it sets the subset
distribution to p(s) = Uniform({0, 1}d) to ensure
all subsets are equally weighted.
Next, we analyze how the model’s predictions
change as we remove either important or unimpor-
tant features according to each explanation. Using
a set of 1,000 images, each image is first labeled by
the original model f(x; η) using the most likely pre-
dicted class. We then use explanations generated by
each method to produce feature rankings and com-
pute the top-1 accuracy (a measure of agreement
with the original model) as we either include or ex-
clude the most important features, ranging from 0-
0.8
u
(D
M 0.6
U
U
<
rd 0.4
d
0.2
Figure 5: Imagenette inclusion and exclusion
curves. The change in top-1 accuracy as an in-
creasing percentage of the pixels estimated to be
important are excluded (top) or included (bottom).
---- KerneISHAP
---KerneISHAP-S
GradCAM
----Integrated Gradients
----SmoothGrad
DeepSHAP
CXPIain
----FastSHAP
0.0-∣-------1-------1-------1-------1--------
0	20	40	60	80	100
Inclusion %
100%. The area under each curve (AUC) is termed the Inclusion AUC or Exclusion AUC.
These metrics match the idea that an accurate image explanation should (1) maximally degrade the
performance of peval when important features are excluded, and (2) maximally improve the perfor-
mance of peval when important features are included (Petsiuk et al., 2018; Hooker et al., 2018). The
explanations are evaluated by removing superpixels; for gradient-based methods, we coarsen the ex-
planations using the sum total importance within each superpixel. In appendix E, we replicate these
metrics using log-odds rather than top-1 accuracy, finding a similar ordering among methods.
Results. Table 1 shows the Inclusion and Exclusion AUC achieved by each method for both
CIFAR-10 and Imagenette. In fig. 5, we also present the curves used to generate these AUCs for
Imagenette. Lower Exclusion AUCs and higher Inclusion AUCs are better. These results show that
FastSHAP outperforms all baseline methods when evaluated with Exclusion AUC: when the pixels
identified as important by FastSHAP are removed from the images, the sharpest decline in top-1 ac-
curacy is observed. Additionally, FastSHAP performs well when evaluated on the basis of Inclusion
AUC, second only to KernelSHAP-S.
8
Published as a conference paper at ICLR 2022
Table 1: Exclusion and Inclusion AUCs. Evaluation of each method on the basis of Exclusion AUC (lower
is better) and Inclusion AUC (higher is better) calculated using top-1 accuracy. Parentheses indicate 95%
confidence intervals, and the best methods are bolded in each column.
	CIFAR-10		Imagenette	
	Exclusion AUC	Inclusion AUC	Exclusion AUC	Inclusion AUC
FastSHAP	0.42 (0.41, 0.43)	0.78 (0.77, 0.79)	0.51 (0.49, 0.52)	0.79 (0.78, 0.80)
KernelSHAP	0.64 (0.63, 0.65)	0.78 (0.77, 0.79)	0.68 (0.67, 0.70)	0.77 (0.75, 0.78)
KernelSHAP-S	0.54 (0.52, 0.55)	0.86 (0.85, 0.87)	0.61 (0.60, 0.62)	0.82 (0.80, 0.83)
GradCAM	0.52 (0.51, 0.53)	0.76 (0.75, 0.77)	0.52 (0.50, 0.53)	0.74 (0.73, 0.76)
Integrated Gradients	0.55 (0.54, 0.56)	0.74 (0.73, 0.75)	0.65 (0.64, 0.67)	0.73 (0.71, 0.74)
SmoothGrad	0.70 (0.69, 0.71)	0.72 (0.71, 0.73)	0.72 (0.71, 0.73)	0.73 (0.72, 0.75)
DeepSHAP	0.65 (0.64, 0.66)	0.79 (0.78, 0.80)	0.69 (0.68, 0.71)	0.74 (0.73, 0.75)
CXPlain	0.56 (0.55, 0.57)	0.71 (0.70, 0.72)	0.60 (0.58, 0.61)	0.72 (0.71, 0.74)
For Imagenette, GradCAM performs competitively with FastSHAP on Exclusion AUC and
KernelSHAP-S marginally beats FastSHAP on Inclusion AUC. KernelSHAP-S also outperforms
on Inclusion AUC with CIFAR-10, which is perhaps surprising given its high level of noise (fig. 4).
However, KernelSHAP-S does not do as well when evaluated using Exclusion AUC, and GradCAM
does not do as well on Inclusion AUC. The remaining methods are, by and large, not competitive
on either metric (except DeepSHAP on CIFAR-10 Inclusion AUC). An accurate explanation should
perform well on both metrics, so these results show that FastSHAP provides the most versatile ex-
planations, because it is the only approach to excel at both Inclusion and Exclusion AUC.
Finally, we also test FastSHAP’s robustness to limited training data. In appendix E, we find that
FastSHAP outperforms most baseline methods on Inclusion and Exclusion AUC when using just
25% of the Imagenette data, and that it remains competitive when using just 10%.
6.5 Speed evaluation The image experiments were run using 8 cores of an Intel Xeon Gold 6148 processor and a sin- gle NVIDIA Tesla V100. Table 2 records the	Table 2: Training and explanation run-times for 1,000 images (in minutes).			
	CIFAR-10			Imagenette
time required to explain 1,000 images. For Fast-		FastSHAP	0.04	0.04
SHAP, KernelSHAP-S and CXPlain, we also		KernelSHAP	453.69	1089.50
report the time required to train the surrogate		KernelSHAP-S	460.10	586.12
and/or explainer models.		GradCAM IntGrad	0.38 0.91	0.30 0.92
The amortized explanation methods, FastSHAP		SmoothGrad	1.00	1.05
and CXPlain, incur a fixed training cost but very		DeepSHAP	5.39	6.01
low marginal cost for each explanation. The		CXPlain	0.04	0.04
gradient-based methods are slightly slower, but	U U	FastSHAP	693.57	146.49
KernelSHAP requires significantly more time.		KernelSHAP-S	362.03	73.22
These results suggest that FastSHAP is well		CXPlain	538.49	93.00
suited for real-time applications where it is CrU- ----------------------------------------------
cial to keep explanation times as low as possible. Further, when users need to explain a large quantity
of data, FastSHAP’s low explanation cost can quickly compensate for its training time.
7	Discussion
In this work, we introduced FastSHAP, a method for estimating Shapley values in a single for-
ward pass using a learned explainer model. To enable efficient training, we sidestepped the need
for a training set and derived a learning approach from the Shapley value’s weighted least squares
characterization. Our experiments demonstrate that FastSHAP can produce accurate Shapley value
estimates while achieving a significant speedup over non-amortized approaches, as well as more
accurate image explanations than popular gradient-based methods.
While Shapley values provide a strong theoretical grounding for model explanation, they have not
been widely adopted for explaining large-scale models due to their high computational cost. Fast-
SHAP can solve this problem, making fast and high-quality explanations possible in fields such as
computer vision and natural language processing. By casting model explanation as a learning prob-
lem, FastSHAP stands to benefit as the state of deep learning advances, and it opens anew direction
of research for efficient Shapley value estimation.
9
Published as a conference paper at ICLR 2022
8	Reproducibility
Code to implement FastSHAP is available online in two separate repositories: https://github.
com/iancovert/fastshap contains a PyTorch implementation and https://github.
com/neiljethani/fastshap/ a TensorFlow implementation, both with examples of tabu-
lar and image data experiments. The complete code for our experiments is available at https:
//github.com/iclr1814/fastshap, and details are described throughout section 5 and
section 6, with model architectures and hyperparameters reported in appendix D. Proofs for our
theoretical claims are provided in appendix A, appendix B, and appendix C.
9	Acknowledgements
We thank the reviewers for their thoughtful feedback, and we thank the Lee Lab for helpful dis-
cussions. Neil Jethani was partially supported by NIH T32 GM136573. Mukund Sudarshan
was partially supported by a PhRMA Foundation Predoctoral Fellowship. Mukund Sudarshan
and Rajesh Ranganath were partly supported by NIH/NHLBI Award R01HL148248, and by NSF
Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Sci-
ence. Ian Covert and Su-In Lee were supported by the NSF Awards CAREER DBI-1552309 and
DBI-1759487; the NIH Awards R35GM128638 and R01NIAAG061132; and the American Cancer
Society Award 127332-RSG-15-097-01-TBG.
References
Aas, K., Jullum, M., and L0land, A. (2019). Explaining individual predictions When features are
dependent: More accurate approximations to Shapley values. arXiv preprint arXiv:1903.10464.
Ancona, M., Oztireli, C., and Gross, M. (2019). Explaining deep neural netWorks With a polyno-
mial time algorithm for Shapley value approximation. In International Conference on Machine
Learning, pages 272-281. PMLR.
Castro, J., G6mez, D., and Tejada, J. (2009). Polynomial calculation of the Shapley value based on
sampling. Computers & Operations Research, 36(5):1726-1730.
Charnes, A., Golany, B., Keane, M., and Rousseau, J. (1988). Extremal principle solutions of
games in characteristic function form: core, Chebychev and Shapley value generalizations. In
Econometrics of Planning and Efficiency, pages 123-133. Springer.
Chen, J., Song, L., WainWright, M., and Jordan, M. (2018a). Learning to explain: An information-
theoretic perspective on model interpretation. In International Conference on Machine Learning,
pages 883-892. PMLR.
Chen, J., Song, L., WainWright, M. J., and Jordan, M. I. (2018b). L-Shapley and C-Shapley: Efficient
model interpretation for structured data. arXiv preprint arXiv:1808.02610.
Chen, T. and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages
785-794.
Covert, I. and Lee, S.-I. (2021). Improving KernelSHAP: Practical Shapley value estimation using
linear regression. In International Conference on Artificial Intelligence and Statistics, pages 3457-
3465. PMLR.
Covert, I., Lundberg, S., and Lee, S.-I. (2020). Understanding global feature contributions With
additive importance measures. Advances in Neural Information Processing Systems, 33.
Covert, I., Lundberg, S., and Lee, S.-I. (2021). Explaining by removing: A unified frameWork for
model explanation. Journal of Machine Learning Research, 22(209):1-90.
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of
Control, Signals and Systems, 2(4):303-314.
DabkoWski, P. and Gal, Y. (2017). Real time image saliency for black box classifiers. In Advances
in Neural Information Processing Systems, pages 6967-6976.
10
Published as a conference paper at ICLR 2022
Datta, A., Sen, S., and Zick, Y. (2016). Algorithmic Transparency via Quantitative Input Influence:
Theory and Experiments with Learning Systems. In Proceedings - 2016 IEEE Symposium on
Security and Privacy, SP 2016, pages 598-617. Institute of Electrical and Electronics Engineers
Inc.
Dua, D. and Graff, C. (2017). UCI machine learning repository.
Fernandes, K., Vinagre, P., and Cortez, P. (2015). A proactive intelligent decision support system
for predicting the popularity of online news. In Portuguese Conference on Artificial Intelligence,
pages 535-546. Springer.
Frye, C., de Mijolla, D., Begley, T., Cowton, L., Stanley, M., and Feige, I. (2020). Shapley explain-
ability on the data manifold. In International Conference on Learning Representations.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-
778.
Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. (2018). A benchmark for interpretability
methods in deep neural networks. arXiv preprint arXiv:1806.10758.
Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural Net-
works, 4(2):251-257.
Howard, J. and Gugger, S. (2020). FastAI: A layered API for deep learning. Information, 11(2):108.
Ill6s, F. and Kerenyi, P. (2019). Estimation of the Shapley value by ergodic sampling. arXiv preprint
arXiv:1906.05224.
Janzing, D., Minorics, L., and Blobaum, P. (2020). Feature relevance quantification in explainable
AI: A causal problem. In International Conference on Artificial Intelligence and Statistics, pages
2907-2916. PMLR.
Jethani, N., Sudarshan, M., Aphinyanaphongs, Y., and Ranganath, R. (2021). Have we learned to
explain?: How interpretability methods can learn to encode predictions in their interpretations. In
International Conference on Artificial Intelligence and Statistics, pages 1459-1467. PMLR.
Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. (2017). LightGBM:
A highly efficient gradient boosting decision tree. Advances in Neural Information Processing
Systems, 30:3146-3154.
Kohavi, R. et al. (1996). Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.
In Knowledge Discovery and Data Mining, volume 96, pages 202-207.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
Liang, D., Lu, C.-C., Tsai, C.-F., and Shih, G.-A. (2016). Financial ratios and corporate governance
indicators in bankruptcy prediction: A comprehensive study. European Journal of Operational
Research, 252(2):561-572.
Lipovetsky, S. and Conklin, M. (2001). Analysis of regression in game theory approach. Applied
Stochastic Models in Business and Industry, 17(4):319-330.
Lundberg, S. M., Erion, G., Chen, H., DeGrave, A., Prutkin, J. M., Nair, B., Katz, R., Himmel-
farb, J., Bansal, N., and Lee, S.-I. (2020). From local explanations to global understanding with
explainable AI for trees. Nature Machine Intelligence, 2(1):56-67.
Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model predictions. Ad-
vances in Neural Information Processing Systems, 30:4765-4774.
Mitchell, R., Cooper, J., Frank, E., and Holmes, G. (2021). Sampling permutations for Shapley
value estimation. arXiv preprint arXiv:2104.12199.
Moro, S., Cortez, P., and Rita, P. (2014). A data-driven approach to predict the success of bank
telemarketing. Decision Support Systems, 62:22-31.
11
Published as a conference paper at ICLR 2022
Petsiuk, V., Das, A., and Saenko, K. (2018). RISE: Randomized input sampling for explanation of
black-box models. arXiv preprint arXiv:1806.07421.
Ruiz, L. M., Valenciano, F., and Zarzuelo, J. M. (1998). The family of least square values for
transferable utility games. Games and Economic Behavior, 24(1-2):109-130.
Schulz, K., Sixt, L., Tombari, F., and Landgraf, T. (2020). Restricting the flow: Information bottle-
necks for attribution. arXiv preprint arXiv:2001.00396.
Schwab, P. and Karlen, W. (2019). CXPlain: Causal explanations for model interpretation under
uncertainty. In Advances in Neural Information Processing Systems, pages 10220-10230.
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. (2017). Grad-
CAM: Visual explanations from deep networks via gradient-based localization. In Proceedings
of the IEEE International Conference on Computer Vision, pages 618-626.
Shapley, L. S. (1953). A value for n-person games. Contributions to the Theory of Games, 2(28):307-
317.
Shrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning important features through propagat-
ing activation differences. In International Conference on Machine Learning, pages 3145-3153.
PMLR.
Smilkov, D., Thorat, N., Kim, B., Viegas, F., and Wattenberg, M. (2017). Smoothgrad: removing
noise by adding noise. arXiv preprint arXiv:1706.03825.
Strumbelj, E. and Kononenko, I. (2014). Explaining prediction models and individual predictions
with feature contributions. Knowledge and Information Systems, 41(3):647-665.
Sundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic attribution for deep networks. In Inter-
national Conference on Machine Learning, pages 3319-3328. PMLR.
Van den Broeck, G., Lykov, A., Schleich, M., and Suciu, D. (2021). On the tractability of SHAP
explanations. In Proceedings of the 35th Conference on Artificial Intelligence (AAAI).
Wang, R., Wang, X., and Inouye, D. I. (2021). Shapley explanation networks. In International
Conference on Learning Representations.
Yoon, J., Jordon, J., and van der Schaar, M. (2018). INVASE: Instance-wise variable selection using
neural networks. In International Conference on Learning Representations.
12
Published as a conference paper at ICLR 2022
A FastSHAP global optimizer
Here, we prove that FastSHAP is trained using an objective function whose global optimizer outputs
the true Shapley values. Recall that the loss function for the explainer model φfast(x, y; θ) is
L ( θ )= E、E、E、[( V X, y (S)-V X ,y(0)-s >φ fast(x, y； θ ))1.
p(x) Unif(y) p(s)
As mentioned in the main text, it is necessary to force the model to satisfy the Efficiency constraint,
or the property that the predictions from φfast (x, y; θ) satisfy
1>φfast(x, y; θ) = Vx,y(1) - Vx,y(0) ∀ x ∈ X,y ∈ Y.
One option for guaranteeing the efficiency property is to adjust the model outputs using their additive
efficient normalization (see section 3.1). Incorporating this constraint on the predictions, we can then
view the loss function as an expectation across (x, y) and write the expected loss for each sample
(x, y) as a separate optimization problem over the variable φx,y ∈ Rd :
min E	(Vχ,y(S)- Vχ,y(0)- s>φxy)2
φx,y p(s)
(9)
s.t.	φx,y = Vx,y (1) - Vx,y (0).
This is a constrained weighted least squares problem with a unique global minimizer, and it is
precisely the Shapley value’s weighted least squares characterization (see eq. (3)). We can therefore
conclude that the optimal prediction for each pair (x, y) is the true Shapley values, or that φχ,y =
φ(vχ,y). As a result, the global optimizer for our objective is a model φfast (X, y; θ*) that outputs the
true Shapley values almost everywhere in the data distribution p(x, y).
Achieving the global optimum requires the ability to sample from p(X) (or a sufficiently large
dataset), perfect optimization, and a function class for φfast (X, y; θ) that is expressive enough to
contain the global optimizer. The universal approximation theorem (Cybenko, 1989; Hornik, 1991)
implies that a sufficiently large neural network can represent the Shapley value function to arbitrary
accuracy as long as itis a continuous function. Specifically, we require the function hy(x) = φ(Vx,y)
to be continuous in x for all y . This holds in practice when we use a surrogate model parameterized
by a continuous neural network, because Vx,y(s) is continuous in x for all (s, y), and the Shapley
value is a linear combination of Vx,y (s) across different values of s (see eq. (1)).
Another approach to enforce the efficiency property is by using efficiency regularization, or penal-
izing the efficiency gap in the explainer model’s predictions (section 3.1). If we incorporate this
regularization term with parameter γ > 0, then our objective yields the following optimization
problem for each (x, y) pair:
m in E	(Vχ,y (S)- Vxy ⑼ — S > Φxy )2 + Y (Vχ,y (1) — Vχ,y (0) - 1 > Φx,y )2 .
φx,y p(s)
For finite hyperparameter values γ ∈ [0, ∞), this problem relaxes the Shapley value’s efficiency
property and eliminates the requirement that predictions must sum to the grand coalition’s value.
However, as we let γ → ∞, the penalty term becomes closer to the hard constraint in eq. (9). Note
that in practice, we use finite values for γ and observe sufficiently accurate results, whereas using
excessively large γ values would render gradient-based optimization ineffective.
B Additive efficient normalization
Here, we provide a geometric interpretation for the additive efficient normalization step and prove
that it is guaranteed to yield Shapley value estimates closer to their true values. Consider a game
V with Shapley values φ(V) ∈ Rd, and assume that We have Shapley values estimates φ ∈ Rd that
do not satisfy the efficiency property. To force this property to hold, we can project these estimates
13
Published as a conference paper at ICLR 2022
onto the efficient hyperplane, or the subset of Rd where the efficiency property is satisfied. This
corresponds to solving the following optimization problem over φeff ∈ Rd :
min ∣∣φeff — φ∣∣2 S.t.	1 >φeff =。(1) —。(0).
φeff
We can solve the problem via its Lagrangian, denoted by L(φeff, ν), with the Lagrange multiplier
ν ∈ R as follows:
L (Φ eff ,V) = ∖∖φ eff — φ || 2 + ν(v (1) — v(0) — 1 > φ eff)
,,*	^ IV(I)-v(0)- 1>φ
=φ eff= φ — 1---------d---------.
This transformation, where the efficiency gap is split evenly and added to each estimate, is known
as additive efficient normalization (Ruiz et al., 1998). We implement it as an output layer for Fast-
SHAP’s predictions to ensure that they satisfy the efficiency property (section 3). This step can
therefore be understood as a projection of the network’s output onto the efficient hyperplane.
The normalization step is guaranteed to produce corrected estimates φe任 that are closer to the true
Shapley values φ(v) than the original estimates φ. To see this, note that the projection step guar-
ʌ
antees that φ — φe任 and φ*任—φ(V) are orthogonal vectors, so the Pythagorean theorem yields the
following inequality:
iiφ (V) — Φ ii 2 = ∣∣φ (V) — φ e ff || 2 + ∣∣φ e @ — φ∣∣2
≥iiφ (V) — φ e ff ii 2.
C Reducing gradient variance
Recall that our objective function L(θ) is defined as follows:
L (θ )= E、“E、Ej(V X ,y (s) — Vχ,y (0) — s >φTast(X, y; θ ))2].
p(x) Unif(y) p(s)
The objective’s gradient is given by
NθL(θ)= EeE、EjV(x,y,s;θ)i,	(10)
p(X) Unif(y) p(s)
where we define
V(x,y, s; θ):= Vθ (Vχ,y(S) — Vχ,y(0) — s>φfast(x,y; θ))2.
When FastSHAP is trained with a single sample (x, y, s), the gradient covariance is given by
Cov V(x, y, s; θ) , which may be too large for effective optimization. We use several strategies to
reduce gradient variance. First, given a model that outputs estimates for all classes y ∈ {1, . . . , K},
we calculate the loss jointly for all classes. This yields gradients that we denote as V(x, s; θ), defined
as
V(x, s; θ) := E [V(x, y, s; θ)],
Unif(y)
where we have the relationship
14
Published as a conference paper at ICLR 2022
CoV(V(X, s; θ)) W COV(V(X, y, s; θ))
due to the law of total covariance. Next, we consider minibatches of b independent x samples, which
yields gradients Vb(X, s; θ) with covariance given by
Cov(Vb(X, s; θ)) = bCov(V(x, s; θ)).
We then consider sampling m independent coalitions s for each input x, resulting in the gradients
Vbm(X, s; θ) with covariance given by
CoV(Vm(x, s; θ)) = m1bCoV(V(x, s; θ)).
Finally, We consider a paired sampling approach, where each sample S 〜P (s) is paired with its Com-
plement 1 - s. Paired sampling has been shown to reduce KernelSHAP’s variance (Covert and Lee,
2021), and our experiments show that it helps improve FastSHAP’s accuracy (appendix D).
The training algorithm in the main text is simplified by omitting these gradient variance reduc-
tion techniques, so we also provide algorithm 2 below, which includes minibatching, multiple
coalition samples, paired sampling, efficiency regularization and parallelization over all output
classes.
Algorithm 2: Full FaStSHAP training
Input: Value function Vxy, learning rate α, batch size b, samples m, penalty parameter Y
Output: FastSHAP explainer φfast(x, y; θ)
initialize φfast(x, y; θ)
while not converged do
set R — 0, LJ 0
for i = 1, . . . , b do
sample x 〜P(x)
for y = 1, . . . , K do
predict φ — φfast (x, y; θ)
calculate RJR + Qxy (1) - Vxy (0) - 1 >φ^j	// Pre-normalization
if normalize then
I set φ J φ+ d-* 1 kx,y (I) - vχ,y (0) - 1>φ)
end
for j = 1, . . . , m do
if paired sampling and i mod 2 = 0 then
I set S J 1 — s // Invert previous subset
else
I sample S 〜P(s)
end
calculate LJL + (VX,y (S) - Vχ,y(0) - s>φ)
end
end
end
update θ J θ - Ks (bLK + γ bK)
end
15
Published as a conference paper at ICLR 2022
Bankruptcy
4.∣ 0.10
记
圭
≡ 0-05 ⅛.
S
0	20	40	60
# Training samples
News
0	20	40	60
# Training samples
# Training samples	# Training samples
—f— FastSHAP	FastSHAP (Paired Sampling)
Figure 6: FastSHAP accuracy as a function of the number of training samples. The results show that
using more s samples per x improves FastSHAP’s closeness to the ground truth Shapley values, as does the use
of paired sampling.
D	FastSHAP models and hyperparameters
In this section, we describe the models and architectures used for each dataset, as well as the hyper-
parameters used when training FastSHAP.
D.1 Models
Tabular datasets. For the original model f (x; η), we use neural networks for the news and
marketing datasets and gradient boosted trees for the census (LightGBM (Ke et al., 2017)) and
bankruptcy (XGBoost (Chen and Guestrin, 2016)) datasets. The FastSHAP model φfast(x, y; θ)
and the surrogate model psurr (y | m(x, s); β) are implemented using neural networks that consist of
2-3 fully connected layers with 128 units and ReLU activations. The psurr models use a softmax out-
put layer, while φfast has no output activation. The models are trained using Adam with a learning
rate of 10-3, and we use a learning rate scheduler that multiplies the learning rate by a factor of 0.5
after 3 epochs of no validation loss improvement. Early stopping was triggered after the validation
loss ceased to improve for 10 epochs.
Image datasets. The models f (x; η) and psurr are ResNet-50 models pretrained on Imagenet. We
use these without modification to the architecture and fine-tune them on each image dataset. To
create the φfast(x, y; θ) model, we modify the architecture to return a tensor of size 14 × 14 × K.
First, the layers after the 4th convolutional block are removed; the output of this block is 14 × 14 ×
256. We then append a 2D convolutional layer with K filters, each of size 1 × 1, so that the output
is 14 × 14 × K and the yth 14 × 14 slice corresponds to the superpixel-level Shapley values for each
class y ∈ Y. Each model is trained using Adam with a learning rate of 10-3, and we use a learning
rate scheduler that multiplies the learning rate by a factor of 0.8 after 3 epochs of no validation
loss improvement. Early stopping was triggered after the validation loss ceased to improve for 20
epochs.
D.2 FastSHAP hyperparameters
We now explore various settings of FastSHAP’s hyperparameters and observe their impact on Fast-
SHAP’s performance. There are two types of hyperparameters: sampling hyperparameters, which
affect the number of samples of s taken during training, and efficiency hyperparameters, which con-
trol how we enforce the Efficiency constraint. Sampling hyperparameters include: (1) whether to
use paired sampling, and (2) the number of samples of s per x to take during training. Efficiency
hyperparameters include: (1) the choice of γ in eq. (4), and (2) whether to perform the additive
efficient normalization during training, inference or both.
To understand the effect of sampling hyperparameters, we perform experiments using the same
tabular datasets from the main text. We use the in-distribution value function psurr and compute
the ground truth SHAP values the same way as in our previous experiments (i.e., by running Ker-
nelSHAP to convergence).
Figure 6 shows the mean `2 distance between FastSHAP’s estimates and the ground truth. We find
that across all four datasets, increasing the number of training samples of s generally improves the
mean `2 distance to ground truth. We also find that for any fixed number of samples (greater than 1),
using paired sampling improves FastSHAP’s accuracy.
Table 3 shows the results of an ablation study for the efficiency hyperparameters. Normalization
(or Norm.) refers to the additive efficient normalization step (applied during training and inference,
16
Published as a conference paper at ICLR 2022
Table 3: FastSHAP ablation results. The distance to the ground truth Shapley values is displayed for several
FastSHAP variations, showing that normalization helps and that the penalty is unnecessary.
	Census		Bankruptcy	
	' 2	' 1	`2	' 1
Normalization	0.0229	0.0863	0.0295	0.2436
Normalization + Penalty	0.0261	0.0971	0.0320	0.2740
Inference Norm.	0.0406	0.1512	0.0407	0.3450
Inference Norm. + Penalty	0.0452	0.1671	0.0473	0.4471
No Norm.	0.0501	0.1933	0.0408	0.3474
No Norm. + Penalty	0.0513	0.1926	0.0474	0.4490
or only during inference), and penalty refers to the efficiency regularization technique with the
parameter set to γ = 0.1. We find that using normalization during training uniformly achieves
better results than without normalization or with normalization only during inference. The efficiency
regularization approach proves to be less effective, generally leading to less accurate Shapley value
estimates. Based on these results, we opt to use additive efficient normalization in our tabular data
experiments.
E	Additional results for image experiments
In this section, we provide additional results for the FastSHAP image experiments.
E.1 Inclusion and exclusion metrics
Table 4 shows our inclusion and exclusion metrics when replicated using log-odds rather than accu-
racy. Similar to our metrics described in the main text, we choose the class predicted by the original
model for each image, and we measure the average log-odds for that class as we include or exclude
important features according to the explanations generated by each method. The results confirm
roughly the same ordering between methods, with FastSHAP being the only method to achieve
strong results on both metrics for both datasets. Figure 7 shows the raw inclusion and exclusion
curves for both the accuracy and log-odds-derived metrics.
Table 4: Exclusion and Inclusion AUCs calculated using the average log-odds of the predicted class.
CIFAR-10
Imagenette
	Exclusion AUC	Inclusion AUC	Exclusion AUC	Inclusion AUC
FastSHAP	5.92 (5.62, 6.14)	5.36 (5.16,5.63)	7.98 (7.68, 8.33)	5.40 (5.16, 5.60)
KernelSHAP	9.88 (9.55, 10.20)	5.36 (5.14, 5.63)	10.68 (10.36, 11.00)	5.07 (4.81, 5.31)
KernelSHAP-S	8.01 (7.68, 8.34)	6.80 (6.65, 6.96)	9.39 (9.11, 9.66)	6.01 (5.78, 6.26)
GradCAM	7.75 (7.44, 8.09)	4.99 (4.81, 5.26)	7.77 (7.49, 8.05)	4.65 (4.40, 4.89)
Integrated Gradients	8.34 (8.03, 8.61)	4.58 (4.37, 4.85)	10.14 (9.79, 10.46)	4.34 (4.10, 4.58)
SmoothGrad	10.99 (10.67, 11.29)	4.30 (4.08, 4.58)	11.19 (10.84, 11.48)	4.47 (4.24, 4.70)
DeepSHAP	9.96 (9.61, 10.24)	5.47 (5.28, 5.76)	10.93 (10.61, 11.20)	4.63 (4.38, 4.85)
CXPlain	8.34 (8.00, 8.58)	4.02 (3.80, 4.31)	9.13 (8.83, 9.41)	4.33 (4.11, 4.57)
17
Published as a conference paper at ICLR 2022
Exclusion Curve
6 4 2 0 2 4 6
- - -
SPPO 6o-∣
Inclusion Curve
6 4 2 0 2 4 6
- - -
SPPO 6o-∣
0	20	40	60	80	100	0	20	40	60	80	100
Exclusion %	Inclusion %
(a) Imagenette: Log-odds derived inclusion and exclusion curves.
0.9
0.8
>0.7
u
2 0.6
U 0.5
<
i 0.4
Q.
O 0.3
0.2
0.1
Exclusion Curve
0	20	40	60	80
Exclusion %
-8-6-4-2。
Ooooo
>UEDUU< I doɪ
Inclusion Curve
20	40	60	80	100
Inclusion %
(b)	CIFAR-10: Accuracy derived inclusion and exclusion curves.
0.9
0.8
>0.7
u
2 0.6
U 0.5
<
i 0.4
Q.
o 0.3
0.2
0.1
Exclusion Curve
0	20	40	60	80
Exclusion %
-8-6-4-2。
Ooooo
>υra⅛υu< ɪ doɪ
Inclusion Curve
20	40	60	80	100
Inclusion %
(c)	CIFAR-10: Log-odds derived inclusion and exclusion curves.
Figure 7: Additional inclusion and exclusion curves. The change in top-1 accuracy or average log-odds
of the predicted class as an increasing percentage of the pixels estimated to be important are excluded (left) or
included (right) from the set of 1,000 images.
18
Published as a conference paper at ICLR 2022
----FastSHAP ---------KerneISHAP-S --------Integrated Gradients	—— DeepSHAP
——■ KerneISHAP	GradCAM -----------SmoothGrad	— CXPIain
Figure 8: FastSHAP robustness to limited data. The curves are generated by training FastSHAP with
varying portions of the Imagenette dataset and evaluating the Inclusion and Exclusion AUC. Horizontal lines
show the Exclusion and Inclusion AUCs for each of the baseline methods, as reported in table 1.
E.2 FastSHAP robustness to limited data
To test FastSHAP’s robustness to the size of the training data, we compare its performance when
trained with varying amounts of the Imagenette dataset. Figure 8 plots the change in inclusion
and exclusion AUC, calculated using top-1 accuracy, achieved when training FastSHAP with 95%,
85%, 75%, 50%, 25%, 15%, 10%, and 5% of the training dataset. We find that FastSHAP remains
competitive when using just 10% of the data, and that it outperforms most baseline methods by a
large margin when using just 25%.
19
Published as a conference paper at ICLR 2022
E.3 Example FastSHAP image explanations
Finally, we show additional explanations generated by FastSHAP and the baseline methods for both
CIFAR-10 and Imagenette.
-12.2	-16.1	-13.4
-16.1	-16.1	-16.1
-15.8	10.1	-16.1
-16.1	-16.1	-16.1
-16.1	-16.1	8.1
1.7	-1.9	-16.1
-13.1
-16.1
-16.1
-13.7
-13.0
-8.0
-9.5
-16.1
-8.5
-9.4
-10.2
-16.1
-16.1
-13.。
-9.9
-16.1
5.5
-5.7
-16.1
-16.1
-2.4
-8.1
-16.1
-15.3
-16.1
-16.1
15.2
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-16.1
-8.1
-16.1
-16.1
-16.1
-16.1
-12.5
-16.1
-16.1
-16.1
-16.1
-14.8
-9.9
-3.7
Figure 9: Explanations generated by FastSHAP for 18 randomly selected CIFAR-10 images. Each col-
umn corresponds to a CIFAR-10 class, and the model’s prediction (in logits) is provided below each image.
20
Published as a conference paper at ICLR 2022
English
Cassette
French
Garbage
-9.0
-7.3
-7.4
-9.3
■.
> >
7.7
a
-10.2
-8.0
-7.3
-7.2
-6.β
-4.7
46
-9.4
-3.7
-10.2
-8.7
-9.4
-9.5
-9.3
-9.3
7.Z
-9.6
-9.8
-9.8
-9.6
-5.2
-9.2
-9.8
-8.8
-9.5
-9.4
-9.8
-10.1
-9.4
-9.4
-9.3
-9.6
-9.3
7.Z
-9.5
-9.6
-9.2
-9.3
-9.4
-9.3
-9.4
-9.5
-10.2
-3.7
-9.6
-3.7
-7.5
-7.1
3.7
-4.2
-7.2
-6.0
-6.9
-6.8
Figure 10: Explanations generated by FastSHAP for 18 randomly selected Imagenette images. Each
column corresponds to an Imagenette class, and the model’s prediction (in logits) is provided below each
image.
	■ . ≡	I F
	I ⅞χ it	
-9.8	-—9.8	-12.4
	L%
-8.6	-10.2
-7.5
■	
-8.6
-7.2
-9.4	-9.4
	Γ'∙" 1
b刁	L一
-4.3	-7.2
	0 U
-9.0	-9.1
6.7	-8.9	-9.0	-9.5	-8.8
⅛t ..		It	∣3l	
・			¥
-6.2
		
-7.9	了 -6.7	
■	1	
		
♦ *		
-7.5	-5.9	
		
		
-9.4	-9.3	
任.		
-9.6	∙⅛"	
		
.tT -10.2	-ɪo.ɪ ■	
	•二	
	ŋfi	
-7.9	-7.7	
产		.
d承		
-9.4	-9.3	
耳r	--	
4	■■ ■	
7.4	-9.8	
		
		
		
	-7.1	
4	喳	
		
-9.2	-9.0	
		
		
-9.0	-8.7	
r	⅛ iɪ	
■		
7.4	-9.8	
■ 1		
-2⅛	⅞τ.	
-9.4	-9.4	
ð V	■ ,	
		
		
-9.2	7.3	
		
W		‹: F	ɪ
-7.4
21
Published as a conference paper at ICLR 2022
"Up≡60i
-_q OEoInV
FastSHAP
KemeISHAP KerneISHAP-S
GradCAM
DeepSHAP
CXPIaIn
SmaathGrad
Figure 11: Explanations generated for the predicted class for 15 randomly selected CIFAR-10 images.
Each column corresponds to an explanation method, and each row is labeled with the image’s corresponding
class.
22
Published as a conference paper at ICLR 2022
dE>d seo
LP-InLO
LP-Inqu
="≈-0°
LPU3h-
LPU3h
EoH
LPU3上
u」OH
LPU3上
£-d
ωsωsrou
dEnd s"0M"s u-"qu
IpJnio dEnd sɪŋo
-Bm-0°
」36Uμds
US=M
Integrated
Gradients
SmoothGrad DeepSHAP
CXPIaIn
Figure 12: Explanations generated for the predicted class for 15 randomly selected Imagenette images.
Each column corresponds to an explanation method, and each row is labeled with the image’s corresponding
class.
23