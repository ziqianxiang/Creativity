Published as a conference paper at ICLR 2022
Surrogate Gap Minimization
Improves Sharpness-Aware Training
Juntang Zhuang1 *	Boqing Gong2, Liangzhe Yuan2, Yin Cui2, HartWig Adam2
j.zhuang@yale.edu {bgong, lzyuan, yincui, hadam}@google.com
Nicha C. Dvornek1 , Sekhar Tatikonda1 , James S. Duncan1
{nicha.dvornek, sekhar.tatikonda, james.duncan}@yale.edu
Ting Liu2
liuti@google.com	1 Yale University, 2 Google Research
Ab stract
The recently proposed Sharpness-Aware Minimization (SAM) improves gener-
alization by minimizing a perturbed loss defined as the maximum loss within a
neighborhood in the parameter space. However, we show that both sharp and flat
minima can have a low perturbed loss, implying that SAM does not always prefer
flat minima. Instead, we define a surrogate gap, a measure equivalent to the dom-
inant eigenvalue of Hessian at a local minimum when the radius of neighborhood
(to derive the perturbed loss) is small. The surrogate gap is easy to compute and
feasible for direct minimization during training. Based on the above observations,
we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a
novel improvement over SAM with negligible computation overhead. Conceptu-
ally, GSAM consists of two steps: 1) a gradient descent like SAM to minimize
the perturbed loss, and 2) an ascent step in the orthogonal direction (after gradi-
ent decomposition) to minimize the surrogate gap and yet not affect the perturbed
loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by
step 2), giving rise to a model with high generalization capabilities. Theoretically,
we show the convergence of GSAM and provably better generalization than SAM.
Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM
and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is
released at https://sites.google.com/view/gsam-iclr22/home.
1	Introduction
Modern neural networks are typically highly over-parameterized and easy to overfit to training data,
yet the generalization performances on unseen data (test set) often suffer a gap from the training
performance (Zhang et al., 2017a). Many studies try to understand the generalization of machine
learning models, including the Bayesian perspective (McAllester, 1999; Neyshabur et al., 2017),
the information perspective (Liang et al., 2019), the loss surface geometry perspective (Hochreiter
& Schmidhuber, 1995; Jiang et al., 2019) and the kernel perspective (Jacot et al., 2018; Wei et al.,
2019). Besides analyzing the properties of a model after training, some works study the influence
of training and the optimization process, such as the implicit regularization of stochastic gradient
descent (SGD) (Bottou, 2010; Zhou et al., 2020), the learning rate’s regularization effect (Li et al.,
2019), and the influence of the batch size (Keskar et al., 2016).
These studies have led to various modifications to the training process to improve generalization.
Keskar & Socher (2017) proposed to use Adam in early training phases for fast convergence and
then switch to SGD in late phases for better generalization. Izmailov et al. (2018) proposed to
average weights to achieve a wider local minimum, which is expected to generalize better than sharp
minima. A similar idea was later used in Lookahead (Zhang et al., 2019). Entropy-SGD (Chaudhari
* Work was done during an internship at Google
1
Published as a conference paper at ICLR 2022
et al., 2019) derived the gradient of local entropy to avoid solutions in sharp valleys. Entropy-SGD
has a nested Langevin iteration, inducing much higher computation costs than vanilla training.
The recently proposed Sharpness-Aware Minimization (SAM) (Foret et al., 2020) is a generic train-
ing scheme that improves generalization and has been shown especially effective for Vision Trans-
formers (Dosovitskiy et al., 2020) when large-scale pre-training is unavailable (Chen et al., 2021).
Suppose vanilla training minimizes loss f(w) (e.g., the cross-entropy loss for classification), where
W is the parameter. SAM minimizes a perturbed loss defined as fp(w)，max∣∣δ∣∣≤ρ f (W + δ),
which is the maximum loss within radius ρ centered at the model parameter w. Intuitively, vanilla
training seeks a single point with a low loss, while SAM searches for a neighborhood within which
the maximum loss is low. However, we show that a low perturbed loss fp could appear in both flat
and sharp minima, implying that only minimizing fp is not always sharpness-aware.
Although the perturbed loss fp(W) might disagree with sharpness, we find a surrogate gap defined
as h(W) , fp (W) - f(W) agrees with sharpness — Lemma 3.3 shows that the surrogate gap h is
an equivalent measure of the dominant eigenvalue of Hessian at a local minimum. Inspired by this
observation, we propose the Surrogate Gap Guided Sharpness Aware Minimization (GSAM) which
jointly minimizes the perturbed loss fp and the surrogate gap h: a low perturbed loss fp indicates
a low training loss within the neighborhood, and a small surrogate gap h avoids solutions in sharp
valleys and hence narrows the generalization gap between training and test performances (Thm. 5.3).
When both criteria are satisfied, we find a generalizable model with good performances.
GSAM consists of two steps for each update: 1) descend gradient Vfp(w) to minimize the perturbed
loss fp (this step is exactly the same as SAM), and 2) decompose gradient Vf (W) of the original
loss f(W) into components that are parallel and orthogonal to Vfp(W), i.e., Vf (W) = Vkf(W) +
V⊥f(W), and perform an ascent step in V⊥f (W) to minimize the surrogate gap h(W). Note that
this ascent step does not change the perturbed loss fp because Vf⊥(W) ⊥ Vfp(W) by construction.
We summarize our contribution as follows:
•	We define surrogate gap, which measures the sharpness at local minima and is easy to compute.
•	We propose the GSAM method to improve the generalization of neural networks. GSAM is
widely applicable and incurs negligible computation overhead compared to SAM.
•	We demonstrate the convergence of GSAM and its provably better generalization than SAM.
•	We empirically validate GSAM over image classification tasks with various neural architec-
tures, including ResNets (He et al., 2016), Vision Transformers (Dosovitskiy et al., 2020), and
MLP-Mixers (Tolstikhin et al., 2021).
2	Preliminaries
2.1	Notations
•	f (W): A loss function f with parameter W ∈ Rk, where k is the parameter dimension.
•	ρt ∈ R: A scalar value controlling the amplitude of perturbation at step t.
•	∈ R: A small positive constant (to avoid division by 0, = 10-12 by default).
•	Wadv，Wt + Pt *f¾3: The solution to max∣∣wo-wt∣∣≤ρt f (w0) when Pt is small.
•	fp(Wt)，max∣∣δ∣∣≤ρt f(Wt + δ) ≈ f 5:加):The perturbed loss induced by f (w∕. For each
Wt, fp(Wt) returns the worst possible loss f within a ball of radius Pt centered at Wt. When
Pt is small, by Taylor expansion, the solution to the maximization problem is equivalent to a
gradient ascent from Wt to Wtadv .
•	h(W) , fp(W) - f(W): The surrogate gap defined as the difference between fp(W) and f (W).
•	ηt ∈ R: Learning rate at step t.
•	α ∈ R: A constant value that controls the scaled learning rate of the ascent step in GSAM.
•	g(t), gp(t) ∈ Rk: At the t-th step, the noisy observation of the gradients Vf (Wt), Vfp(Wt) of
the original loss and perturbed loss, respectively.
2
Published as a conference paper at ICLR 2022
sharpness(wγ) > sharpness(w2) > sharpness(w3), /i(wɪ) > ⅛(w2) > /i(wɜ),
fp(wι) = fp(w3) < Jp(w2)5 hence h agrees with sharpness while fp might not.
Figure 1: Consider original loss f (solid line), perturbed loss fp，max∣∣δ∣∣≤ρ f (w+δ) (dashed line),
and surrogate gap h(w) , fp(w) - f(w). Intuitively, fp is approximately a max-pooled version of
f with a pooling kernel of width 2ρ, and SAM minimizes fp . From left to right are the local minima
centered at w1 , w2, w3, and the valleys become flatter. Since fp(w1) = fp(w3) < fp(w2), SAM
prefers w1 and w3 to w2. However, a low fp could appear in both sharp (w1) and flat (w3) minima,
so fp might disagree with sharpness. On the contrary, a smaller surrogate gap h indicates a flatter
loss surface (Lemma 3.3). From w1 to w3, the loss surface is flatter, and h is smaller.
•	Vf(Wt) = Nfk(Wt) + Vf⊥(wt): Decompose Vf (Wt) into parallel component Nfk(Wt) and
vertical component Vf⊥(wt) by projection Vf (Wt) onto Vfp(Wt).
2.2	Sharpness-Aware Minimization
Conventional optimization of neural networks typically minimizes the training loss f(W) by gradient
descent w.r.t. Vf (W) and searches fora single point W with a low loss. However, this vanilla training
often falls into a sharp valley of the loss surface, resulting in inferior generalization performance
(Chaudhari et al., 2019). Instead of searching for a single point solution, SAM seeks a region with
low losses so that small perturbation to the model weights does not cause significant performance
degradation. SAM formulates the problem as:
minw fp(W) where %(w)，max∣∣δ∣∣≤ρ f (w + δ)	(1)
where ρ is a predefined constant controlling the radius of a neighborhood. This perturbed loss
fp induced by f(W) is the maximum loss within the neighborhood. When the perturbed loss is
minimized, the neighborhood corresponds to low losses (below the perturbed loss). For a small ρ,
using Taylor expansion around W, the inner maximization in Eq. 1 turns into a linear constrained
optimization with solution
argmax∣∣δ∣∣≤ρ f (w + δ) = argmax∣∣δ∣∣≤ρ f (w) + δ>Vf (w) + O(ρ2)
Vf(W)
P ι∣Vf(W)∣∣
(2)
As a result, the optimization problem of SAM reduces to
minw fp(W)
≈ minw f (Wadv ) where Wadv
W+P
Vf(W)
||Vf(W)|| + e
(3)
where e is a scalar (default: 1e-12) to avoid division by 0, and Wadv is the “perturbed weight” with
the highest loss within the neighborhood. Equivalently, SAM seeks a solution on the surface of the
perturbed loss fp(W) rather than the original loss f(W) (Foret et al., 2020).
3	The surrogate gap measures the sharpnes s at a local minimum
3.1	The perturbed loss is not always sharpnes s -aware
Despite that SAM searches for a region of low losses, we show that a solution by SAM is not
guaranteed to be flat. Throughout this paper we measure the sharpness at a local minimum of loss
f(W) by the dominant eigenvalue σmax (eigenvalue with the largest absolute value) of Hessian. For
simplicity, we do not consider the influence of reparameterization on the geometry of loss surfaces,
which is thoroughly discussed in (Laurent & Massart, 2000; Kwon et al., 2021).
3
Published as a conference paper at ICLR 2022
Figure 2: Vf is decomposed into
parallel and vertical (Vf⊥) Com-
ponents by projection onto Vfp .
VfGSAM = Vfp - αVf⊥
Algorithm 1 GSAM Algorithm
For t = 1 to T
0) ρt schedule: ρt = ρmin +
(Pmax - Pmin ) (Ir - Ir min )
lrmax -lrmin
Ia) δWt = PtNfft)产'
1b) wtadv = wt + ∆wt
2)	Get Vfp(t) by back-propagation at wtadv .
3)	Vf (t) = Vfk(t) + Vf⊥(t) Decompose Vf (t) into compo-
nents that are parallel and orthogonal to Vfp(t).
4) Update weights:
Vanilla wt+1 = wt - ηtVf (t)
SAM	wt+1 = wt - ηtVfp(t)
GSAM wt+1 = wt - ηt(Vfp(t) - αVf⊥(t))
Lemma 3.1. For some fixed	ρ,	consider two local minima	w1	and	w2,	fp(w1)	≤	fp(w2)	=6⇒
σmax (w1) ≤ σmax(w2), where σmax is the dominant eigenvalue of the Hessian.
We leave the proof to Appendix. Fig. 1 illustrates Lemma 3.1 with an example. Consider three
local minima denoted as w1 to w3 , and suppose the corresponding loss surfaces are flatter from w1
to w3 . For some fixed ρ, we plot the perturbed loss fp and surrogate gap h , fp - f around each
solution. Comparing w2 with w3: Suppose their vanilla losses are equal, f (w2) = f (w3), then
fp(w2) > fp(w3) because the loss surface is flatter around w3, implying that SAM will prefer w3
to w2. Comparing w1 and w2: fp(w1) < fp(w2), and SAM will favor w1 over w2 because it only
cares about the perturbed loss fp, even though the loss surface is sharper around w1 than w2 .
3.2 The surrogate gap agrees with sharpness
We introduce the surrogate gap that agrees with sharpness, defined as:
h(w) , max∣∣δ∣∣≤ρ f(w + δ) - f(w) ≈ f (wadv) - f (w)	(4)
Intuitively, the surrogate gap represents the difference between the maximum loss within the neigh-
borhood and the loss at the center point. The surrogate gap has the following properties.
Lemma 3.2. Suppose the perturbation amplitude ρ is sufficiently small, then the approximation to
the surrogate gap in Eq. 4 is always non-negative, h(w) ≈ f (wadv) - f(w) ≥ 0, ∀w.
Lemma 3.3. For a local minimum w*, consider the dominate eigenvalue σmaχ of the Hessian of
loss f as a measure OfsharPness. Considering the neighborhood centered at w* with a small radius
ρ, the surrogate gap h(w*) is an equivalent measure ofthe sharpness: σmaχ ≈ 2h(w*)∕ρ2.
The proof is in Appendix. Lemma 3.2 tells that the surrogate gap is non-negative, and Lemma 3.3
shows that the loss surface is flatter as h gets closer to 0. The two lemmas together indicate that we
can find a region with a flat loss surface by minimizing the surrogate gap h(w).
4 Surrogate Gap Guided Sharpness-Aware Minimization
4.1	General idea: Simultaneously minimize the perturbed loss and surrogate
GAP
Inspired by the analysis in Section 3, we propose Surrogate Gap Guided Sharpness-Aware
Minimzation (GSAM) to simultaneously minimize two objectives, the perturbed loss fp and the
surrogate gap h:
minw (fp(w),h(w))	(5)
Intuitively, by minimizng fp we search for a region with a low perturbed loss similar to SAM, and
by minimizing h we search for a local minimum with a flat surface. A low perturbed loss implies
4
Published as a conference paper at ICLR 2022
low training losses within the neighborhood, and a flat loss surface reduces the generalization gap
between training and test performances (Chaudhari et al., 2019). When both are minimized, the
solution gives rise to high accuracy and good generalization.
Potential caveat in optimization It is tempting and yet sub-optimal to combine the objectives in
Eq. 5 to arrive at minw fp(w)+λh(w), where λ is some positive scalar. One caveat when solving this
weighted combination is the potential conflict between the gradients of the two terms, i.e., Nfp(W)
and Nh(W) We illustrate this conflict by Fig. 2, where Vh(w) = Vfp (W) — Vf (w) (the grey
dashed arrow) has a negative inner product with Nfp(w) and Nf (w). Hence, the gradient descent
for the surrogate gap could potentially increase the loss fp, harming the model’s performance. We
empirically validate this argument in Sec. 6.4.
4.2	Gradient decomposition and ascent for the multi-objective optimization
Our primary goal is to minimize fp because otherwise a flat solution of high loss is meaningless,
and the minimization of h should not increase fp. We propose to decompose Vf(Wt) and Vh into
components that are parallel and orthogonal to Vfp(Wt), respectively (see Fig. 2):
V	f(Wt) = Vfk(Wt) + Vf⊥(Wt)
V	h(Wt) = Vhk(Wt) + Vh⊥(Wt)	(6)
V	h⊥(Wt) = -Vf⊥(Wt)
The key is that updating in the direction of Vh⊥ (Wt) does not change the value of the perturbed loss
fp(Wt) because Vh⊥ ⊥ Vfp by construction. Therefore, we propose to perform a descent step in
the Vh⊥(Wt) direction, which is equivalent to an ascent step in the Vf⊥(Wt) direction (because
Vh⊥ = -Vf⊥ by the definition of h), and achieve two goals simultaneously — it keeps the value
offp(Wt) intact and meanwhile decreases the surrogate gap h(Wt) = fp(Wt) -f(Wt) (by increasing
f(Wt) and not affect fp(Wt)).
The full GSAM Algorithm is shown in Algo. 1 and Fig. 2, whereg(t), gp(t) are noisy observations of
Vf(Wt) and Vfp(Wt), respectively, and gk(t), g⊥(t) are noisy observations of Vfk (Wt) and Vf⊥(Wt),
respectively, by projecting g(t) onto gp(t) . We introduce a constant α to scale the stepsize of the
ascent step. Steps 1) to 2) are the same as SAM: At current point Wt, step 1) takes a gradient ascent
to Wtadv followed by step 2) evaluating the gradient gp(t) at Wtadv. Step 3) projects g(t) onto gp(t),
which requires negligible computation compared to the forward and backward passes. In step 4),
-ηtgp(t) is the same as in SAM and minimizes the perturbed loss fp(Wt) with gradient descent, and
αηtg⊥(t) performs an ascent step in the orthogonal direction of gp(t) to minimize the surrogate gap
h(Wt) ( equivalently increase f(Wt) and keep fp(Wt) intact). In coding, GSAM feeds the “surrogate
gradient” VftGSAM , gp(t) - αg⊥(t) to first-order gradient optimizers such as SGD and Adam.
The ascent step along g⊥(t) does not harm convergence SAM demonstrates that minimizing fp
makes the network generalize better than minimizing f. Even though our ascent step along g⊥(t)
increases f (W), it does not affect fp(W), so GSAM still decreases the perturbed loss fp in a way
similar to SAM. In Thm. 5.1, we formally prove the convergence of GSAM. In Sec. 6 and Appendix
C, we empirically validate that the loss decreases and accuracy increases with training.
Illustration with a toy example We demonstrate different algorithms by a numerical toy example
shown in Fig. 3. The trajectory of GSAM is closer to the ridge and tends to find a flat minimum.
Intuitively, since the loss surface is smoother along the ridge than in sharp local minima, the sur-
rogate gap h(W) is small near the ridge, and the ascent step in GSAM minimizes h to pushes the
trajectory closer to the ridge. More concretely, Vf(Wt) points to a sharp local solution and deviates
from the ridge; in contrast, Wtadv is closer to the ridge and Vf (Wtadv) is closer to the ridge descent
direction than Vf(Wt). Note that VftGSAM and Vf(Wt) always lie at different sides of Vfp(Wt)
by construction (see Fig. 2), hence VftGSAM pushes the trajectory closer to the ridge than Vfp(Wt)
does. The trajectory of GSAM is like descent along the ridge and tends to find flat minima.
5
Published as a conference paper at ICLR 2022
Figure 3: Consider the loss surface with a few sharp local minima. Left: Overview of the procedures
of SGD, SAM and GSAM. SGD takes a descent step at Wt using Vf(Wt) (orange), which points to
a sharp local minima. SAM first performs gradient ascent in the direction of Vf (Wt) to reach Wadv
with a higher loss, followed by descent with gradient Vf (Wtadv) (green) at the perturbed weight.
Based on Vf(Wt) and Vf (Wtadv ), GSAM updates in a new direction (red) that points to a flatter
region. Right: Trajectories by different methods. SGD and SAM fall into different sharp local
minima, while GSAM reaches a flat region. A video is in the supplement for better visualization.
5 Theoretical properties of GSAM
5.1 Convergence during training
Theorem 5.1. Consider a non-convex function f(W) with Lipschitz-smooth constant L and lower
bound fmin. Suppose we can access a noisy, bounded observation g(t) (||g(t) ||2 ≤ G, ∀t) of the
true gradient Vf (Wt) at the t-th step. For some Constant a, with learning rate η = no/ʌ/t, and
perturbation amplitude Pt proportional to the learning rate, e.g., Pt = po/ʌ/t, we have
TT
TXElVfp(Wt)I ≤ "FT,	TXEEWt)I ≤ C3+fogτ
t=1	t=1
where C1, C2 , C3, C4 are some constants.
Thm. 5.1 implies both fp and f converge in GSAM at rate O (log T/ √T) for non-convex stochastic
optimization, matching the convergence rate of first-order gradient optimizers like Adam.
5.2 Generalization of GSAM
In this section, we show the surrogate gap in GSAM is provably lower than SAM’s, so GSAM is
expected to find a smoother minimum with better generalization.
Theorem 5.2 (PAC-Bayesian Theorem (McAllester, 2003)). Suppose the training set has m ele-
ments drawn i.i.d. from the true distribution, and denote the loss on the training set as f(W) =
ml Em=I f (w, xi), where we use Xi to denote the (input, target) pair of the i-th element. Let W be
learned from the training set. Suppose W is drawn from posterior distribution Q. Denote the prior
distribution (independent of training) as P, then
Ew〜QExf(W,x) ≤ Ew〜Qf(W) +4∖ (KL(QIIP) + log
with probability at least 1- a
Corollary 5.2.1. Suppose perturbation δ is drawn from distribution δ 〜 N(0, b2Ik), δ ∈ Rk, k is
the dimension of W, then with probability at least	1- a	1- e
k)2i

Ew〜QExf (w, x) ≤ h + C + 4,(KL(QIIP) + log-) /m
(7)
1m
b，max∣∣δ∣∣2≤ρ F(w + δ) — F(w) = 一 X [max∣∣δ∣∣2≤ρ f (w + δ,Xi) — f(W,xj]	(8)
m i=1
6
Published as a conference paper at ICLR 2022
where C = f(w) is the empirical training loss, and h is the surrogate gap evaluated on the training
set.
Corollary 5.2.1 implies that minimizing h (right hand side of Eq. 7) is expected to achieve a tighter
upper bound of the generalization performance (left hand side of Eq. 7). The third term on the right
of Eq. 7 is typically hard to analyze and often simplified to L2 regularization (Foret et al., 2020).
Note that fp = C + h only holds when ρtrain (the perturbation amplitude specified by users during
training) equals ρtrue (the ground truth value determined by underlying data distribution); when
ρtrain 6= ρtrue, min(fp, h) is more effective than min(fp) in terms of minimizing generalization
loss. A detailed discussion is in Appendix A.7.
Theorem 5.3 (Unlike SAM, GSAM decreases the surrogate gap). Under the assumption in
Thm. 5.1, Thm. 5.2 and Corollary 5.2.1, we assume the Hessian has a lower-bound ∣σ∣min on the
absolute value of eigenvalue, and the variance of noisy observation g(t) is lower-bounded by c2. The
surrogate gap h can be minimized by the ascent step along the orthogonal direction g⊥(t). During
training we minimize the sample estimate of h. We use ∆ht to denote the amount that the ascent
step in GSAM decreases h for the t-th step. Compared to SAM, the proposed method generates a
total decrease in surrogate gap PtT=1 ∆bht, which is bounded by
ɑc2ρ0ηθlσimin
G2
T
≤ lim X ∆bht ≤ 2.7αL2η0ρ20
T→∞ t=1
(9)
T
We provide proof in the appendix. The lower-bound of t=1 ∆ht indicates that GSAM achieves a
provably non-trivial decrease in the surrogate gap. Combined with Corollary 5.2.1, GSAM provably
improves the generalization performance over SAM.
6	Experiments
6.1	GSAM improves test performance on various model architectures
We conduct experiments with ResNets (He et al., 2016), Vision Transformers (ViTs) (Dosovitskiy
et al., 2020) and MLP-Mixers (Tolstikhin et al., 2021). Following the settings by Chen et al. (2021),
we train on the ImageNet-1k (Deng et al., 2009) training set using the Inception-style (Szegedy
et al., 2015) pre-processing without extra training data or strong augmentation. For all models, we
search for the best learning rate and weight decay for vanilla training, and then use the same values
for the experiments with SAM and GSAM. For ResNets, we search for ρ from 0.01 to 0.05 with
a stepsize 0.01. For ViTs and Mixers, we search for ρ from 0.05 to 0.6 with a stepsize 0.05. In
GSAM, we search for α in {0.01, 0.02, 0.03} for ResNets and α in {0.1, 0.2, 0.3} for ViTs and
Mixers. Considering that each step in SAM and GSAM requires twice the computation of vanilla
training, we experiment with the vanilla training for twice the epochs of SAM and GSAM, but we
observe no significant improvements from the longer training (Table 5 in appendix). We summarize
the best hyper-parameters for each model in Appendix B.
We report the performances on ImageNet (Deng et al., 2009), ImageNet-v2 (Recht et al., 2019) and
ImageNet-Real (Beyer et al., 2020) in Table 1. GSAM consistently improves over SAM and vanilla
training (with SGD or AdamW): on ViT-B/32, GSAM achieves +5.4% improvement over AdamW
and +3.2% over SAM in top-1 accuracy; on Mixer-B/32, GSAM achieves +11.1% over AdamW
and +1.2% over SAM. We ignore the standard deviation since it is typically negligible (< 0.1%)
compared to the improvements. We also test the generalization performance on out-of-distribution
data (ImageNet-R and ImageNet-C), and the observation is consistent with that on ImageNet, e.g.,
+5.1% on ImageNet-R and +5.9% on ImageNet-C for Mixer-B/32.
6.2	GSAM finds a minimum whose Hessian has small dominant eigenvalues
Lemma 3.3 indicates that the surrogate gap h is an equivalent measure of the dominant eigenvalue
of the Hessian, and minimizing h equivalently searches for a flat minimum. We empirically validate
this in Fig. 4. As shown in the left subfigure, for some fixed ρ, increasing α decreases the dominant
value and improves generalization (test accuracy). In the middle subfigure, we plot the dominant
7
Published as a conference paper at ICLR 2022
Table 1: Top-1 Accuracy (%) on ImageNet datasets for ResNets, ViTs and MLP-Mixers trained with
Vanilla SGD or AdamW, SAM, and GSAM optimizers.
Model I	Training	∣ ImageNet-VI ImageNet-Real ImageNet-V2 ∣ ImageNet-R ImageNet-C ResNet			
ResNet50	Vanilla (SGD) SAM GSAM	76.0	82.4	63.6 76.9	83.3	64.4 77.2	83.9	64.6	22.2	44.6 23.8	46.5 23.6	47.6
ResNet101	-Vanilla (SGD)- SAM GSAM	778	839	65.3 78.6	84.8	66.7 78.9	85.2	67.3	244	48.5 25.9	51.3 26.3	51.8
ResNet152	-Vanilla (SGD)- SAM GSAM	78.5	842	6613 79.3	84.9	67.3 80.0	85.9	68.6	253	500 25.7	52.2 27.3	54.1
Vision Transformer			
ViT-S/32	Vanilla (AdamW) SAM GSAM	68.4	75.2	54.3 70.5	77.5	56.9 73.8	80.4	60.4	19.0	43.3 21.4	46.2 22.5	48.2
ViT-S/16	Vanilla (AdamW) SAM GSAM	744	804	617 78.1	84.1	65.6 79.5	85.3	67.3	200	465 24.7	53.0 25.3	53.3
ViT-B/32	Vanilla (AdamW) SAM GSAM	71.4	775	575 73.6	80.3	60.0 76.8	82.7	63.0	23.4	440 24.0	50.7 25.1	51.7
ViT-B/16	Vanilla (AdamW) SAM GSAM	746	798	613 79.9	85.2	67.5 81.0	86.5	69.2	20.1	466 26.4	56.5 27.1	55.7
MLP-Mixer			
Mixer-S/32	Vanilla (AdamW) SAM GSAM	63.9	70.3	49.5 66.7	73.8	52.4 68.6	75.8	55.0	16.9	35.2 18.6	39.3 22.6	44.6
Mixer-S/16	Vanilla (AdamW) SAM GSAM	68.8	75∏	548 72.9	79.8	58.9 75.0	81.7	61.9	159	356 20.1	42.0 23.7	48.5
Mixer-S/8	Vanilla (AdamW) SAM GSAM	702	762	56.1 75.9	82.5	62.3 76.8	83.4	64.0	15.4	346 20.5	42.4 24.6	47.8
Mixer-B/32	Vanilla (AdamW) SAM GSAM	625	68.1	476 72.4	79.0	58.0 73.6	80.2	59.9	146	3378 22.8	46.2 27.9	52.1
Mixer-B/16	Vanilla (AdamW) SAM GSAM	664	721	508 77.4	83.5	63.9 77.8	84.0	64.9	145	3378 24.7	48.8 28.3	54.4
Top-I Accuracy
6 5 4 3 2 1
7 7 7 7 7 7
(％) >UEDUU<dol
0.05	0.10	0.15	0.20	0.25	0.30
P
-*- a = 0.05
a = 0.15
→- α = 0.2
Estimation of dominant eigenvalue
0.05	0.10	0.15	0.20	0.25	0.30
P
Measured dominant eigenvalue
0.05	0.10	0.15	0.20	0.25	0.30
P
Figure 4: Influence of ρ (set as constant for ease of comparison, other experiments use decayed
ρt schedule) and α on the training of ViT-B/32. Left: Top-1 accuracy on ImageNet. Middle:
Estimation of the dominant eigenvalues from the surrogate gap, σmaχ ≈ 2h∕ρ2. Right: Dominant
eigenvalues of the Hessian calculated via the power iteration. Middle and right figures match in
the trend of curves, validating that the surrogate gap can be viewed as a proxy of the dominant
eigenvalue of Hessian.
8
Published as a conference paper at ICLR 2022
ImageNet accuracy
0 8 6 4
7 6 6 6
(％) A.e-lnuvdoj.
70
早78-
A
X 76-
U 74 -
0.72 -
ImageNet-ReaI accuracy
lmageNet-v2 accuracy
6 4 2。
5 5 5 5
(％) A.e-lnuvdoj.
Figure 5: Top-1 accuracy of Mixer-S/32 trained with different methods. “+ascent" represents
applying the ascent step in Algo. 1 to an optimizer. Note that our GSAM is described as
sAM+ascent(=GSAM) for consistency.
Table 3: Transfer learning results (top-1 accuracy, %)
Table 2: Results (%) of GSAM and
min(fp + λh) on ViT-B/32
Dataset	min(fp + λh)	GSAM
ImageNet	754	76.8
ImageNet-Real	81.1	82.7
ImageNet-v2	60.9	63.0
ImageNet-R	23.9	25.1
	ViT-B/16		
	Vanilla	SAM	GSAM
Cifar10	98.1	-986-	98.8
Cifar100	87.6	89.1	89.7
Flowers	88.5	91.8	91.2
Pets	91.9	93.1	94.4
mean	91.5	93.2	93.5
ViT-S/16
Vanilla	SAM	GSAM
97.6	98.2	-98.4^
85.7	87.6	88.1
86.4	91.5	90.3
90.4	92.9	93.5
90.0	92.6	-92.6^
eigenvalues estimated by the surrogate gap, σmaχ ≈ 2h∕ρ2 (Lemma 3.3). In the right subfigure, We
directly calculate the dominant eigenvalues using the power-iteration (Mises & Pollaczek-Geiringer,
1929). The estimated dominant eigenvalues (middle) match the real eigenvalues σmaχ (right) in
terms of the trend that σmaχ decreases with a and ρ. Note that the surrogate gap h is derived over
the whole training set, while the measured eigenvalues are over a subset to save computation. These
results show that the ascent step in GSAM minimizes the dominant eigenvalue by minimizing the
surrogate loss, validating Thm 5.3.
6.3	Comparison with methods in THE literature
Section 6.1 compares GSAM to SAM and vanilla training. In this subsection, we further compare
GSAM against Entropy-SGD (Chaudhari et al., 2019) and Adaptive-SAM (ASAM) (Kwon et al.,
2021), which are designed to improve generalization. Note that Entropy-SGD uses SGD in the inner
Langevin iteration and can be combined with other base optimizers such as AdamW as the outer
loop. For Entropy-SGD, we find the hyper-parameter “scope” from 0.0 and 0.9, and search for the
inner-loop iteration number between 1 and 14. For ASAM, we search for P between 1 and 7 (10 ×
larger than in SAM) as recommended by the ASAM authors. Note that the only difference between
ASAM and SAM is the derivation of the perturbation, so both can be combined with the proposed
ascent step. As shown in Fig. 5, the proposed ascent step increases test accuracy when combined
with both SAM and ASAM and outperforms Entropy-SGD and vanilla training.
6.4	Additional studies
GSAM outperforms a weighted combination of the perturbed loss and surrogate gap With an
example in Fig. 2, we demonstrate that directly minimizing fp(w) + λh(w) as discussed in Sec. 4.1
is sub-optimal because Vh(W) could conflict with Nfp(W) and Vf (W). We empirically validate
this argument on ViT-B/32. We search for λ between 0.0 and 0.5 with a step 0.1 and search for P in
the same grid as SAM and GSAM. We report the best accuracy of each method. Top-1 accuracy in
Table 2 show the superior performance of GSAM, validating our analysis.
min(fp, h) vs. min(f, h) GSAM solves min(fp, h) by descent in Vfp, decomposing Vf onto Vfp,
and an ascent step in the orthogonal direction to increase f while keep fp intact. Alternatively, we
can also optimize min(f, h) by descent in Vf, decomposing Vfp onto Vf, and a descent step in
the orthogonal direction to decrease fp while keep f intact. The two GSAM variations perform
similarly (see Fig. 6, right). We choose min(fp, h) mainly to make the minimal change to SAM.
GSAM benefits transfer learning Using weights trained on ImageNet-1k, we finetune models with
SGD on downstream tasks including the CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford-
9
Published as a conference paper at ICLR 2022
a。Influence Ofaugmentations
78 Influence of base optimizers
h) v.s. min优 h)
委78-
■	Vanllla	- Vanllla	: 80-
■	SAM	@ - SAM ■ u
■	| ■ ,∣ i - I IQ ■
LLi⅛LU0J
72 -
Light Medium Strong
■ mln(¾,h)
mln(⅛h)
Adam	AdaBeIief	ImageNet ImageNet-ReaI lmageNet-v2
舒6-
2 74-
最
Figure 6: Top-1 accuracy ofViT-B/32 for the additional studies (Section 6.4). Left: from left to right
are performances under different data augmentations (details in Appendix B.3), where the vanilla
method is trained for 2× the epochs. Middle: performance with different base optimizers. Right:
Comparison between min(fp, h) and min(f, h).
flowers (Nilsback & Zisserman, 2008) and Oxford-IITPets (Parkhi et al., 2012). Results in Table 3
shows that GSAM leads to better transfer performance than vanilla training and SAM.
GSAM remains effective under various data augmentations We plot the top-1 accuracy of a
ViT-B/32 model under various Mixup (Zhang et al., 2017b) augmentations in Fig. 6 (left subfigure).
Under different augmentations, GSAM consistently outperforms SAM and vanilla training.
GSAM is compatible with different base optimizers GSAM is generic and applicable to various
base optimizers. We compare vanilla training, SAM and GSAM using AdamW (Loshchilov &
Hutter, 2017) and AdaBelief (Zhuang et al., 2020) with default hyper-parameters. Fig. 6 (middle
subfigure) shows that GSAM performs the best, and SAM improves over vanilla training.
7	Conclusion
We propose the surrogate gap as an equivalent measure of sharpness which is easy to compute and
feasible to optimize. We propose the GSAM method, which improves the generalization over SAM
at negligible computation cost. We show the convergence and provably better generalization of
GSAM compared to SAM, and validate the superior performance of GSAM on various models.
Acknowledgement
We would like to thank Xiangning Chen (UCLA) and Hossein Mobahi (Google) for discussions, Yi
Tay (Google) for help with datasets, and Yeqing Li, Xianzhi Du, and Shawn Wang (Google) for help
with TensorFlow implementation.
Ethics Statement
This paper focuses on the development of optimization methodologies and can be applied to the
training of different deep neural networks for a wide range of applications. Therefore, the ethical
impact of our work would primarily be determined by the specific models that are trained using our
new optimization strategy.
Reproducibility Statement
We provide the detailed proof of theoretical results in Appendix A and provide the data pre-
processing and hyper-parameter settings in Appendix B. Together with the references to existing
works and public codebases, we believe the paper contains sufficient details to ensure reproducibil-
ity. We plan to release the models trained by using GSAM upon publication.
References
Randall Balestriero, Jerome Pesenti, and Yann LeCun. Learning in high dimension always amounts
to extrapolation. arXiv preprint arXiv:2110.09485, 2021.
10
Published as a conference paper at ICLR 2022
Lucas Beyer, Olivier J. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are
we done with imagenet? arXiv preprint arXiv:2002.05709, 2020.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT,2010, pp. 177-186. Springer, 2010.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):
124018, 2019.
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets
without pretraining or strong data augmentations, 2021.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
Alex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers flat global minimizers.
arXiv preprint arXiv:2106.06530, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(Jul):2121-2159, 2011.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim,
Youngjung Uh, and Jung-Woo Ha. Adamp: Slowing down the slowdown for momentum optimiz-
ers on scale-invariant weights. arXiv preprint arXiv:2006.08217, 2020.
SePP Hochreiter and Jurgen SChmidhuber. Simplifying neural nets by discovering flat minima. In
Advances in neural information processing systems, pp. 529-536, 1995.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
son. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
11
Published as a conference paper at ICLR 2022
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-
aware minimization for scale-invariant learning of deep neural networks. arXiv preprint
arXiv:2102.11600, 2021.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
tion. Annals of Statistics ,pp. 1302-1338, 2000.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, ge-
ometry, and complexity of neural networks. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 888-896. PMLR, 2019.
Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi. Extrapolation for large-batch training in
deep learning. In International Conference on Machine Learning, pp. 6094-6104. PMLR, 2020.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel ma-
chines, pp. 203-215. Springer, 2003.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170, 1999.
RV Mises and Hilda Pollaczek-Geiringer. Praktische Verfahren der gleichungsauflθsung. ZAMM-
Journal of Applied Mathematics and MechanicsZZeitschrift fiir Angewandte Mathematik Und
Mechanik, 9(1):58-77, 1929.
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? arXiv
preprint arXiv:1906.02629, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
pp. 722-729. IEEE, 2008.
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012
IEEE conference on computer vision and pattern recognition, pp. 3498-3505. IEEE, 2012.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? In International Conference on Machine Learning, pp. 5389-5400, 2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
12
Published as a conference paper at ICLR 2022
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations
by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive
Science, 1985.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. 2019.
Zeke Xie, Li Yuan, Zhanxing Zhu, and Masashi Sugiyama. Positive-negative momentum: Manip-
ulating stochastic gradient noise to improve generalization. arXiv preprint arXiv:2103.17182,
2021.
Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Salr: Sharpness-aware learning rates for improved
generalization. arXiv preprint arXiv:2011.05348, 2020.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-
9803, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. 2017a.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017b.
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. In Advances in Neural Information Processing Systems, pp. 9593-9604,
2019.
Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8156-8165, 2021.
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, et al. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. arXiv preprint arXiv:2010.05627,
2020.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James S Duncan. Adabelief optimizer: Adapting stepsizes by the belief in ob-
served gradients. arXiv preprint arXiv:2010.07468, 2020.
13
Published as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Lemma. 3.1
Suppose ρ is small, perform Taylor expansion around the local minima w, we have:
f(w + δ) = f(w) + Vf (w)>δ + 2 δ>Hδ + O(∣∣δ∣∣3)	(10)
where H is the Hessian, and is positive Semidefinite at a local minima. At a local minima, Vf (W)=
0, hence we have
f(w + δ) = f(w) + 2 δ>Hδ + O(∣∣δ∣∣3)	(11)
and
fp(w) =max∣∣δ∣∣≤ρ f(w + δ) = f (w) + 1 ρ2σmaχ(H) + O(∣∣δ∣∣3)	(12)
where σmax is the dominate eigenvalue (eigenvalue with the largest absolute value). Now consider
two local minima w1 and w2 with dominate eigenvalue σ1 and σ2 respectively, we have
fp(wι) ≈ f(wι) + 2P2σι	fp(w2) ≈ f (w2) + 1 P2σ2
We have fp(w1) > fp(w2) =6⇒ σ1 > σ2 and σ1 > σ2 =6⇒ fp(w1) > fp(w2) because the
relation between f(w1) and f(w2) is undetermined.
A.2 Proof of Lemma. 3.2
Since P is small, we can perform Taylor expansion around w,
h(w) = f(w+δ) - f(w)
= δ>Vf(w) + O(P2)
=P||Vf(w)||2+O(P2) >0	(13)
where the last line is because δ is approximated as δ = P ∣∣f W(W)+e, hence has the same direction
as Vf (w).
A.3 Proof of Lemma. 3.3
Since P is small, we can approximate f(w) with a quadratic model around a local minima w:
f(w + δ)= f(w) + 1 δ>Hδ + O(ρ3)
where H is the Hessian at w, assumed to be positive semidefinite at local minima. Normalize δ such
that ∣∣δ∣∣2 = ρ, Hence We have:
h(w) = fp(w) — f (w) = max∣∣δ∣∣2≤ρ f (w + δ) — f (w) =2 σmaxρ + O(P3)	(14)
where σmax is the dominate eigenvalue of the hessian H, and first order term is 0 because the
gradient is 0 at local minima. Therefore, we have σmax ≈ 2h(w)∕ρ2.	□
A.4 Proof of Thm. 5.1
For simplicity we consider the base optimizer is SGD. For other optimizers such as Adam, we can
derive similar results by applying standard proof techniques in the literature to our proof.
14
Published as a conference paper at ICLR 2022
Step 1: Convergence w.r.t function fp(w)
For simplicity of notation, We denote the update at step t as
dt = -ηtgpt + ηtag⊥⊥	(15)
By L-smoothness of f and the definition of fp(wt) = f (Wadv), and definition of dt = wt+ι - Wt
and Wadv = Wt + & we have
fp(wt+ι) = f(wa+v) ≤ f (wadv)+ Ef (Wadv), w；+； - Wadv〉+ ∣∙∣∣wafv - wadv ∣∣2
=f(w产)+ Nf(W产),Wt+1 + δt+ι- Wt - M
+ 211 Wt+1 + δt+1 - Wt - δt ∣ ∣
≤ f(wadv)+ Ef(W 产),dt〉+ Ll ∣ dt ∣ ∣ 2
+ Ef (Wadv ),δt+ι- M + l∣ ∣ δt+ι - δt ∣ ∣ 2
(16)
(17)
(18)
(19)
Step 1.0: Bound eq. 18
We first bound Eq. 18. Take expectation conditioned on observation up to step t (for simplicity of
notation, we use E short for Ex to denote expectation over all possible data points) conditioned on
observations up to step t, also by definition of dt, we have
Efp(Wt+1) - fp(wt) ≤ -ηt"fp(wt), Egpt))+ αηtNfp(wt), Eg。)
+ Lη2E∣ ∣-gPt) + ag(t) ∣ ∣ :	(20)
≤ -ηtE ∣ 忖fp(Wt)I L +0 + (α + i)2G2η2	(21)
(Since Egf) is orthogonal to NfP(Wt) by construction,
||g(t)|| ≤ G by assumption)
Step 1.1: Bound eq. 19
By definition of δt, We have
g(t)
δ=Pt ]⅛	(22)
g(t+I)
δt+1 = Pt+1 ||g(t+D|| + e	(23)
where g(t) is the gradient of f at Wt evaluated with a noisy data sample. When learning rate ηt is
small, the update in weight dt is small, and expected gradient is
Vf (wt+1) = Nf(Wt + dt) = Vf (Wt) + Hdt + O(||dt||2)	(24)
where H is the Hessian at wt. Therefore, we have
g(t)	g(t+1)
EhVf (Wadv ),δt+1 - δt) = "f(Wadv ),ρtE Pg^ - Pt+1E ||g(g+1)|| + e)	(25)
g(t)	g(t+1)
金νf(Wadv )m ∣ ∣ E p⅛ - E p⅛ ∣ ∣	(26)
≤ ||Vf(Wadv )|3血	(27)
where the first inequality is due to (1) Pt is monotonically decreasing with t, and (2) triangle in-
equality that(a, b) ≤ ||a|| ∙ ||b||. φt is the angle between the unit vector in the direction of Vf (Wt)
15
Published as a conference paper at ICLR 2022
and Vf (wt+ι). The second inequality comes from that (1) ∣∣ ∣∣g∖,∣+e 11 < 1 strictly, so We can replace
δt in Eq. 25 with a unit vector in corresponding directions multiplied by ρt and get the upper bound,
(2) the norm of difference in unit vectors can be upper bounded by the arc length on a unit circle.
When learning rate ηt and update stepsize dt is small, φt is also small. Using the limit that
	tanx = x + O(x2), sinx = x + O(x2),	x → 0
We have:	fa ,	∖∖Vf(wt+1) -Vf(wt)∖∖	2 φt =	∖∖VfM∖	+ O(φt)	(28) ∖∖Hdt + O(∖∖dt∖∖2)∖∖* C京、	宙 =∖∖Vf (wt)∖∖	+O(φt)	(29) ≤ ηtL(1 +α)	(30)
Where the last inequality is due to (1) max eigenvalue of H is upper bounded by L because f is
L-smooth, (2) ||dt|| = ∣∣ηt(g∣∣ + αg⊥)∣∣ andEgt = VfIwt).
Plug into Eq. 27, also note that the perturbation amplitude ρt is small so wt is close to wtadv , then
we have	EhVf(wtadv),δt+1 -δti ≤ L(1 + α)Gρtηt	(31)
Similarly, we have	∣∣	∣∣2	∣∣	g(t)	g(t+1)	∣∣2 Ellδt+1 - δtll ≤ Pt Ell∖∖g(t) ∖∖ + e - ∖∖g(t+1)∖∖ + e ∣∣	(32) ≤ ρt2φt2	(33) ≤ ρt2ηt2L2(1 +α)2	(34)
Step 1.2: Total bound
Reuse results from Eq. 21 (replace Lp With 2L) and plug into Eq. 18, and plug Eq. 31 and Eq. 34
into Eq. 19, We have
Efp(wt+ι) - fp(wt) ≤ -ηtE∣∣Vfp(wt)∣∣2 + 2L(α2+1)2G2η2
+ L(I + α)Gρtη +	~~—η2ρ2	(35)
Perform telescope sum, We have
TT
Efp(WT) - fp(wo) ≤ -^XηtE∣∣vfp(wt)||2 + [L(1 + α)2G2η0 + L(I + α)Gp0ηo^ ^X -
t=1	t=1
T1
+ L3(1 + α)2η0ρ0 X -2	(36)
t=1
Hence
T	T	π2E
ητ ^E∖∖Vfp(wt)∖∖2 ≤ EntE∣∣Vfp(wt)∣∣2 ≤ fp(wo) - Efp(wT) + DlogT + -6-	(37)
t=1	t=1
Where
D = L(1 + α)2G2η02 + L(1 + α)Gρ0η0,	- = L3(1 + α)2η02ρ02	(38)
Note that nτ =器,we have
1 XE∖∖Vfp(wt)∖∖2 ≤ fp(w0)- fmin + -2-/6-L + Dl√T	(39)
T t=1	η0	T η0 T
which implies that GSAM enables fp to converge at a rate of O (log T/ √T), and all the constants
here are well-bounded.
16
Published as a conference paper at ICLR 2022
Step 2: Convergence w.r.t. function f (W)
We prove the risk for f (w) convergences for non-convex stochastic optimization case using SOD.
Denote the update at step t as
dt = -ηtgpt + αηtg⊥	(40)
By smoothness of f, we have
f (Wt+1) ≤f (Wt) + "f (Wt), dti+ 2 I I dt I I 2	(41)
=f (Wt) + hvf (wt), -ηtgpt + αηtg⊥) + L11 dt ∣ 1	(42)
For simplicity, we introduce a scalar βt such that
Vfk(Wt)= βt Vfp(Wt)	(43)
where Vfk (Wt) is the projection of Vf (Wt) onto VfP (Wt). When perturbation amplitude P is small,
we expect βt to be very close to 1.
Take expectation conditioned on observations up to step t for both sides of Eq. 42, we have:
Ef(Wt+1) ≤ f (Wt4 + 卜f(Wt),-β (Vf(Wt)- vf⊥(Wt)) + αηtEg⊥): + 2EI ∣dt ∣ ∣ 2
=f (Wt)-	βt I	I Vf(Wt)I	I 2 +	(βι+	(Wt), Vf,(Wt)) + 2 E i i dt I I 2
=f (Wt)	-	βt I	I Vf(Wt)I	I 2 +	(β1- +	α)ηt<Vf (Wt), Vf(Wt)Sin Q + LE∣ 同「
(44)
(45)
(46)
Qt is the angle between VfP(Wt) and Vf (Wt))
=f (Wt) - β I I Vf (Wt) ∣ I 2 + ( ± + α)ηt∣ ∣ Vf(Wt)I ∣ ：(| tan %| + O(θf)) + 2 E ∣ ∣ dt∣ ∣ ;
'	'	(47)
(Sinx = x + O(x2), tanx = x + O(x2) when x T 0.)
Also note when perturbation amplitude Pt is small, we have
VfP(Wt) = Vf(Wt + δt) = Vf(Wt) + ||Vf(WPt) ∣∣2 + e H (Wt)Vf(Wt) + O(Pt)	(48)
where δt = Pt UfW by definition, H(Wt) is the Hessian. Hence we have
| tan θt | ≤
||VfP(Wt)-Vf(Wt)||	Pt L
||Vf(Wt)||	≤ ||Vf(Wt)||
(49)
where L is the Lipschitz constant of f, and L-smoothness of f indicates the maximum absolute
eigenvalue of H is upper bounded by L. Plug Eq. 49 into Eq. 47, we have
Ef(Wt+1) ≤ f (Wt)- β1 1 Vf (Wt)I I 2 + (β + α)ηt∣ ∣ Vf (Wt) ∣ I 21tan θt| + 2 e 1 1 dt ∣ 1 2	(50)
≤f(Wt)- βt I i Vf (Wt) U 2+(βt+α)LPtηt∣∣ Vf(Wt) W+2 e i I dt I I 2	(51)
≤f(Wt)- βt i I Vf (Wt) l∣ 2+(βt+α)LPtηtG+2 EIIdt I i 2	(52)
(ASSUme gradient has bounded norm G)	(53)
≤ f (Wt) - β"t ∣ 忖f (Wt)I L + (β~~；—+ α)LPtηtG + 2E(α + I)2G2η2	(54)
βt is close to 1 assuming P is small,
hence it,s natural to assume 0 < βmin ≤ βt ≤ βmαx )
17
Published as a conference paper at ICLR 2022
Re-arranging above formula, We have
Ft 口 ▽/(Wt)Il ≤ f(wt) — Ef(wt+1) + (~^---------+ α^LGηtpt + —(α + 1)2G2η2	(55)
βmax H	2 2 2	β βmin	J	2
perform telescope sum and taking expectations on each step, We have
T T	C
F Xηt Il W(Wt) I I 2 ≤ f(w°)—Ef(WT)+(K
TT
+ 0) LGX ηtpt + 2(α + 1)2G2 X η2
t=1	t=1
(56)
Take the schedule to be ηt = √ and Pt = M, then we have
T 4 √T XllVf(Wt) ∣∣2 Pmax VT t=i	2	≤ LHS		(57)
	≤ RHS		(58)
	≤ f (W0)— fmin + (	，1	t 1 L p-	+ 0)LGη0p0 X - + 2(a +	T1 1)2G2η2 X t t=1
		+	+ 0) LGη0pθ(1 + log T) 'Pmin	'	(59)
	≤ f (W0)— fmin + (		
	+ ɪ(ɑ +1)2G2η2(I + log T)		(60)
Hence	T t XIIVf(Wt) ∣∣2 t=1	C3	log T ≤√T + C4 F	(61)
where C1, C4 are some constants. This implies the convergence rate w.r.t f (w) is O(log T∕√Γ).
Step 3: Convergence w.r.t. surrogate gap h(w)
Note that we have proved convergence for fp(w) in step 1, and convergence for f (w) in step 3. Also
note that
I IVh(Wt) I I 2=∣ ∣ vfp(wt)—vf(wt)∣ ∣ 2 ≤ 2 ∣ ∣ VfP(Wt) I I 2+2 ∣ ∣ Vf(Wt) ∣ ∣ 2	④幻
Hence
TT	T
T X i i Vh(Wt) i L ≤ T X i i VfP(Wt) i L + T X i i Vf(Wt) i L	(63)
t=1	t=1	t=1
also converges at rate O (log T/ √T) because each item in the RHS converges at rate O(log T√T).
□
A.5 Proof of Corollary. 5.2.1
Using the results from Thm. 5.2, with probability at least 1 — a, we have
i
πτ> πτ> c( ∖/ E ^T7∖, 彳
Ew〜QEXf (w, x) ≤ Ew〜Qf (w) + 4
KL(Q∣∣P )+log
m
2m
a
(64)
Assume δ 〜N(0, b2Ik) where k is the dimension of model parameters, hence δ2 (element-wise
square) follows a a Chi-square distribution. By Lemma.1 in Laurent & Massart (2000), we have
P(∣∣δ∣∣2 — kb2 ≥ 2b2√kt + 2tb2) ≤ exp(—t)
hence with probability at least 1 — 1/√n, we have
∣∣δ∣∣2 ≤ b2(2log √n + k + 2^k log √n) ≤ 2b2k(1 + ^logʌ^n) ≤ p2
(65)
(66)
18
Published as a conference paper at ICLR 2022
Therefore, with probability at least 1 - 1 /√n
1 - exp


Eδf (W + δ) ≤ max∣∣δ∣∣2≤ρ f (W + δ)
(67)
Combine Eq. 65 and Eq. 67, subtract the same constant C on both sides, and under the same as-
sumption as in (Foret et al., 2020) that Ew〜QExf (w, x) ≤ Eδ〜N(0炉/石)旧W〜QExf (w + δ, X)We
finish the proof.
A.6 Proof of Thm. 5.3
Step 1: a sufficient condition that the loss gap is expected to decrease for
EACH STEP
Take Taylor expansion, then the expected change of loss gap caused by descent step is
EhNfp(Wt) - Pf(Wt),-ηt Pfp(Wt)	(68)
Wheree Eg⊥ = Nf⊥(Wt))
= ηt - Pfp(Wt)22 + Pfp(Wt)2Pf(Wt)2cosθt	(69)
where θt is the angle between vector Pfp(Wt) and Pf(Wt).
The expected change of loss gap caused by ascent step is
EhPfp(Wt) - Pf(Wt), αηtPf⊥(Wt)i = -αηtPf⊥(Wt)22 < 0	(70)
Above results demonstrate that ascent step decreases the loss gap, while descent step might increase
the loss gap. A sufficient (but not necessary) condition for EhPh(Wt), dti ≤ 0 requires α to be large
or |Pf (Wt)2 cos θt ≤ Pfp(Wt). In practice, the perturbation amplitude ρ is small and we can
assume θt is close to 0 and Pfp(Wt) is close to Pf (Wt), we can also set the parameter α to
be large in order to decrease the loss gap.
Step 2: upper and lower bound of decrease in loss gap (by the ascent step in
orthogonal gradient direction) compared to SAM.
Next we give an estimate of the decrease in h caused by our ascent step. We refer to Eq. 69 and
Eq. 70 to analyze the change in loss gap caused by the descent and ascent (orthogonally) respectively.
It can be seen that gradient descent step might not decrease loss gap, in fact they often increase loss
gap in practice; while the ascent step is guaranteed to decrease the loss gap.
The decrease in loss gap is:
2
∆bht = -hPfbp(Wt) - Pfb(Wt), αηtPfb⊥(Wt)i = αηtPfb⊥(Wt)2	(71)
=αηt 11 Nf(Wt )∣∣2∣ tan θt |2	(72)
TT
X∆bht≤XαL2ηtρt2	(73)
By Eq. 49	(74)
T1
≤ fαLniPO t3/2	(75)
t=1
≤ 2.7αL2η0ρ20	(76)
Hence we derive an upper bound for PtT=1 ∆bht .
19
Published as a conference paper at ICLR 2022
T
Next we derive a lower bound for	t=1 ∆ht Note that when ρt is small, by Taylor expansion
ρ
Vfp(Wt) = Vf(Wt + δt) = Vf(Wt) +	H (Wt)Vf(Wt) + O(ρ2)	(77)
“▽f(Wt)||
where Hb (wt ) is the Hessian evaluated on training samples. Also when ρt is small, the angle θt
between Vfp(Wt) and Vf(Wt) is small, by the limit that
tanx = x + O(x2), x → 0
sinx = x + O(x2), x → 0
We have
| tan θtl = | Sin θt∣ + O(θ2) = ∣θt∣ + O(θ2)
Omitting high order term, we have
| tanθt| ≈ ∣θt∣
||Vfp(Wt) - vf(Wt)||	||PtHH(Wt) + O(P2)||
K	-	K
IIf(Wt)II	IVf(Wt)II
≥ ρt1σ〔min
(78)
G
where G is the upper-bound on norm of gradient, Iσ Imin is the minimum absolute eigenvalue of the
Hessian. The intuition is that as perturbation amplitude decreases, the angle θt decreases at a similar
rate, though the scale constant might be different. Hence we have
TT
X ∆bht = X αηtVfb(Wt)22I tan θtI2 + O(θt4)	(79)
t=1	t=1
T 2 ρtIσImin 2
≥ 2^aηtc (-G-)	(80)
t=1
_ 0c2ρ0ηθ ^mmin XX 1	f2n
--------G------工即	(81)
t=1
≥ αc2ρ0ηo ⑸min
≥-------G------	(82)
where c2 is the lower bound of IIVfbII2 (e.g. due to noise in data and gradient observation). Re-
sults above indicate that the decrease in loss gap caused by the ascent step is non-trivial, hence our
proposed method efficiently improves generalization compared with SAM.
A.7 Discussion on Corollary 5.2. 1
The comment “‘The corollary gives a bound on the risk in terms of the perturbed training loss
if one removes C from both sides”’ is correct. But there is a misunderstanding in the statement
“‘the perturbed training loss is small then the model has a small risk”’: it’s only true when ρtrain
for training equals its real value ρtrue determined by the data distribution; in practice, we never
know ρtrue . In the following we show that the minimization of both h and fp is better than simply
minimizing fp when ρtrue 6= ρtrain .
1. First, we re-write the conclusion of Corollary 5.2.1 as
EwEχf (w, x) ≤ fp + R = C + h + R = C + ρ2σ∕2 + R + O(ρ3)
with probability (1 一 a)[1 一 e-(√Pb-√k) ]
where R is the regularization term, C is the training loss, σ is the dominant eigenvalue of Hessian.
As in lemma 3.3, we perform Taylor-expansion and can ignore the high-order term O(ρ3). We focus
on
fp = C +bh = C + ρ2σ∕2
2. When ρtrue 6= ρtrain, minimizing h achieves a lower risk than only minimizing fp. (1) Note
that after training, C (training loss) is fixed, but h could vary with ρ (e.g. when training on dataset
A and testing on an unrelated dataset B, the training loss remains unchanged, but the risk would
be huge and a large ρ is required for a valid bound). (2) With an example, we show a low fp is
insufficient for generalization, and a low σ is necessary:
20
Published as a conference paper at ICLR 2022
A Suppose we use ρtrain for training, and consider two solutions with C1 , σ1 (SAM) and
C2 , σ2 (GSAM). Suppose they have the same fp during training for some ρtrain, so
fp1 = CI + σ1/2 × Ptrain = C2 + σ2/2 X Ptrain = fp2
Suppose C1 < C2 so σ1 > σ2 .
B When Ptrue > Ptrain , we have
risk_bound_1 = Ci + σι∕2 × ρ2rue + R > risk_bound_2 = C2 + σ2∕2 × ρ2rue + R
This implies that a small σ helps generalization, but only a low fp1 (caused by a low C1
and high σ1 ) is insufficient for a good generalization.
C Note that Ptrain is fixed during training, so minimizing htrain during training is equiva-
lently minimizing σ by Lemma 3.3
3.	Why we are often unlucky to have Ptrue > Ptrain (1) First, the test sets are almost surely
outside the convex hull of the training set because “‘interpolation almost surely never occurs in
high-dimensional (> 100) cases”’ Balestriero et al. (2021). As a result, the variability of (train
+ test) sets is almost surely larger than the variability of (train) set. Since P increases with data
variability (See point 4 below), We have Ptrue > Ptrainset almost surely. (2) Second, We don't
know the value of Ptrue and can only guess it. In practice, we often guess a small value because
training often diverges With large P (as observed in Foret et al. (2020); Chen et al. (2021)).
4.	Why P increases with data variability. In Corollary 5.2.1, We assume Weight perturbation
δ 〜N(0, b2Ik). The meaning of b is the following. If we can randomly sample a fixed number
of samples from the underlying distribution, then training the model from scratch (With a fixed seed
for random initialization) gives rise to a set of weights. Repeating this process, we get many sets of
weights, and their standard deviation is b. Since the number of training samples is limited and fixed,
the more variability in data, the more variability in weights, and the larger b. Note that Corollary
stated that the bound holds with probability proportional to [1 一 e-(√P2b-Ck ]. In order for the
result to hold with a fixed probability, P must stay proportional to b, hence P also increases with the
variability of data.
21
Published as a conference paper at ICLR 2022
Table 4: Hyper-parameters to reproduce experimental results
Model	Pmax	ρmin	α	lrmax	lrmin	Weight Decay	Base Optimizer	Epochs	Warmup Steps	LR schedule
ResNet50	0.04	0.02	0.01	1.6	1.6e-2	03	SGD	90	5k	Linear
ResNet101	0.04	0.02	0.01	1.6	1.6e-2	0.3	SGD	90	5k	Linear
ResNet512	0.04	0.02	0.005	1.6	1.6e-2	0.3	SGD	90	5k	Linear
ViT-S/32	0.6	0.0	0.4	3e-3	3e-5	03	AdamW	300	10k	Linear
ViT-S/16	0.6	0.0	1.0	3e-3	3e-5	0.3	AdamW	300	10k	Linear
ViT-B/32	0.6	0.1	0.6	3e-3	3e-5	0.3	AdamW	300	10k	Linear
ViT-B/16	0.6	0.2	0.4	3e-3	3e-5	0.3	AdamW	300	10k	Linear
Mixer-S/32	0.5	0.0	0.2	3e-3	3e-5	03	AdamW	300	10k	Linear
Mixer-S/16	0.5	0.0	0.6	3e-3	3e-5	0.3	AdamW	300	10k	Linear
Mixer-S/8	0.5	0.1	0.1	3e-3	3e-5	0.3	AdamW	300	10k	Linear
Mixer-B/32	0.7	0.2	0.05	3e-3	3e-5	0.3	AdamW	300	10k	Linear
Mixer-B/16	0.5	0.2	0.01	3e-3	3e-5	0.3	AdamW	300	10k	Linear
B Experimental Details
B.1	Training details
For ViT and Mixer, we search the learning rate in {1e-3, 3e-3, 1e-2, 3e-3}, and search weight decay
in {0.003, 0.03, 0.3}. For ResNet, we search the learning rate in {1.6, 0.16, 0.016}, and search
the weight decay in {0.001, 0.01,0.1}. For ViT and Mixer, we use the AdamW optimizer with
β1 = 0.9, β2 = 0.999; for ResNet we use SGD with momentum= 0.9. We train ResNets for 90
epochs, and train ViTs and Mixers for 300 epochs following the settings in (Chen et al., 2021) and
(Dosovitskiy et al., 2020). Considering that SAM and GSAM uses twice the computation of vanilla
training for each step, for vanilla training we try 2× longer training, and does not find significant
improvement as in Table. 5.
We first search the optimal learning rate and weight decay for vanilla training, and keep these two
hyper-parameters fixed for SAM and GSAM. For ViT and Mixer, we search ρ in {0.1, 0.2, 0.3, 0.4,
0.5, 0.6} for SAM and GSAM; for ResNet, we search ρ from 0.01 to 0.05 with a stepsize 0.01.
For ASAM, we amplify ρ by 10× compared to SAM, as recommended by Kwon et al. (2021). For
GSAM, we search α in {0.1, 0.2, 0.3} throughout the paper. We report the best configuration of
each individual model in Table. 4.
B.2	Transfer learning experiments
Using weights trained on ImageNet-1k, we finetune models with SGD on downstream tasks in-
cluding the CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford-flowers (Nilsback & Zisserman,
2008) and Oxford-IITPets (Parkhi et al., 2012). For all experiments, we use the SGD optimizer with
no weight decay under a linear learning rate schedule and gradient clipping with global norm 1. We
search the maximum learning rate in {0.001, 0.003, 0.01, 0.03}. On Cifar datasets, we train models
for 10k steps with a warmup step of 500; on Oxford datasets, we train models for 500 steps with a
wamup step of 100.
B.3	Experimental setup with ablation studies on data augmentation
We follow the settings in (Tolstikhin et al., 2021) to perform ablation studies on data augmenta-
tion. In the left subfigure of Fig. 6, “Light” refers to Inception-style data augmentation with random
flip and crop of images, “Medium” refers to the mixup augmentation with probability 0.2 and Ran-
dAug magnitude 10; “Strong” refers to the mixup augmentation with probability 0.2 and RandAug
magnitude 15.
C Ablation studies and discussions
C.1 INFLUENCE OF ρ AND α
We plot the performance of a ViT-B/32 model varying with ρ (Fig. 7a) and α (Fig. 7b). We empir-
ically validate that fine-tuning ρ in SAM can not achieve comparable performance with GSAM, as
22
Published as a conference paper at ICLR 2022
Top-I accuracy of ViT-B/32 under different P
78 η---------------------------------------------
77-
SAM
GSAM
76
累
J 75
A
u
2 74
界
2 72
住
71
70
69

p = 0.1 p = 0.2 p = 0.3 p = 0.4 p = 0.5 p = 0.6
Top-I accuracy of ViT-B/32 varying with a
(b) Performance of GSAM under different ɑ
(a) Performance of SAM and GSAM under different ρ.
Figure 7: Performance of GSAM varying with P and α.
Table 5: Top-1 accuracy of ViT-B/32 on ImageNet with Inception-style data augmentation. For
vanilla training We report results for training 300 epochs and 600 epochs, for GSAM We report the
results for 300 epochs.
MethOd	Epochs	ImageNet	ImageNet-Real	ImageNet-v2	ImageNet-R
Vanilla	-300-	-714~~	775	575	23.4
	600	72.0	78.2	57.9	23.6
GSAM	300	76.8	82.7 —	63.0	—	25.1 一
shown in Fig. 7a. Considering that GSAM has one more parameter a, we plot the accuracy varying
with a in Fig. 7b, and show that GSAM consistently outperforms SAM and vanilla training.
C.2 Constant P v.s. decayed Pt schedule
Note that Thm. 5.1 assumes Pt to decay with t in order to prove the convergence, while SAM uses a
constant P during training. To eliminate the influence of Pt schedule, we conduct ablation study as
in Table. 6. The ascent step in GSAM can be applied to both constant P or a decayed Pt schedule,
and improves accuracy for both cases. Without ascent step, constant P and decayed Pt achieve
similar performance. Results in Table. 6 implies that the ascent step in GSAM is the main reason
for improvement of generalization performance.
Cosine
Figure 8: The value of cos θt varying with train-
ing steps, where θt is the angle between Vf (Wt)
and VfP(Wt) as in Fig. 2.
Figure 9: Surrogate gap curve under different ɑ
values.
23
Published as a conference paper at ICLR 2022
Table 6: Top-1 Accuracy on ViT-B/32 on ImageNet. Ablation studies on constant ρ or a decayed ρt .
Vanilla Constant P (SAM) Constant P + ascent Decayed Pt	Decayed Pt + ascent
ɪθ	75.8	762	75.8	76.8
C.3 Visualize the training process
In the proof of Thm. 5.3, our analysis relies on assumption that θt is small. We empirically validated
this assumption by plotting cos θt in Fig. 8, where θt is the angle between Vf (Wt) and NfP(Wt).
Note that the cosine value is calculated in the parameter space of dimension 8.8 × 107, and in
high-dimensional space two random vectors are highly likely to be perpendicular. In Fig. 8 the
cosine value is always above 0.9, indicating that Vf(Wt) and VfP(Wt) point to very close directions
considering the high dimension of parameters. This empirically validates our assumption that θt is
small during training.
We also plot the surrogate gap during training in Fig. 9. As α increases, the surrogate gap decreases,
validating that the ascent step in GSAM efficiently minimizes the surrogate gap. Furthermore, the
surrogate gap increases with training steps for any fixed α, indicating that the training process grad-
ually falls into local minimum in order to minimize the training loss.
D Related works
Besides SAM and ASAM, other methods were proposed in the literature to improve generalization:
Lin et al. (2020) proposed extrapolation of gradient, Xie et al. (2021) proposed to manipulate the
noise in gradient, and Damian et al. (2021) proved label noise improves generalization, Yue et al.
(2020) proposed to adjust learning rate according to sharpness, and Zheng et al. (2021) proposed
model perturbation with similar idea to SAM. Izmailov et al. (2018) proposed averaging weights
to improve generalization, and Heo et al. (2020) restricted the norm of updated weights to improve
generalization. Many of aforementioned methods can be combined with GSAM to further improve
generalization.
Besides modified training schemes, there are other two types of techniques to improve generaliza-
tion: data augmentation and model regularization. Data augmentation typically generates new data
from training samples; besides standard data augmentation such as flipping or rotation of images,
recent data augmentations include label smoothing (Muller et al., 2019) and mixup (Muller et al.,
2019) which trains on convex combinations of both inputs and labels, automatically learned augmen-
tation (Cubuk et al., 2018), and cutout (DeVries & Taylor, 2017) which randomly masks out parts
of an image. Model regularization typically applies auxiliary losses besides the training loss such as
weight decay (Loshchilov & Hutter, 2017), other methods randomly modify the model architecture
during training, such as dropout (Srivastava et al., 2014) and shake-shake regularization (Gastaldi,
2017). Note that the data augmentation and model regularization literature mentioned here typically
train with the standard back-propagation (Rumelhart et al., 1985) and first-order gradient optimizers,
and both techniques can be combined with GSAM.
Besides SGD, Adam and AdaBelief, GSAM can be combined with other first-order gradient opti-
mizers, such as AdaBound (Luo et al., 2019), RAdam (Liu et al., 2019), Yogi (Zaheer et al., 2018),
AdaGrad (Duchi et al., 2011), AMSGrad (Reddi et al., 2019) and AdaDelta (Zeiler, 2012).
24