Published as a conference paper at ICLR 2022
Improving Mutual Information Estimation
with Annealed and Energy-Based Bounds
Rob Brekelmans*,1	Sicong Huang*,2,3	Marzyeh Ghassemi2,4
Greg Ver Steeg1	Roger Grosse2,3	Alireza Makhzani2,3
1	Information Sciences Institute, University of Southern California
2	Vector Institute 3 University of Toronto 4 MIT EECS / IMES / CSAIL
Ab stract
Mutual information (mi) is a fundamental quantity in information theory and
machine learning. However, direct estimation of mi is intractable, even if the
true joint probability density for the variables of interest is known, as it involves
estimating a potentially high-dimensional log partition function. In this work, we
present a unifying view of existing mi bounds from the perspective of importance
sampling, and propose three novel bounds based on this approach. Since a tight mi
bound without density information requires a sample size exponential in the true mi,
we assume either a single marginal or the full joint density information is known.
In settings where the full joint density is available, we propose Multi-Sample
Annealed Importance Sampling (ais) bounds on mi, which we demonstrate can
tightly estimate large values of mi in our experiments. In settings where only a
single marginal distribution is known, we propose Generalized IWAE (GIWAE) and
mine-ais bounds. Our giwae bound unifies variational and contrastive bounds in
a single framework that generalizes InfoNCE, iwae, and Barber-Agakov bounds.
Our mine-ais method improves upon existing energy-based methods such as
mine-dv and mine-f by directly optimizing a tighter lower bound on mi. mine-
ais uses mcmc sampling to estimate gradients for training and Multi-Sample ais
for evaluating the bound. Our methods are particularly suitable for evaluating mi
in deep generative models, since explicit forms of the marginal or joint densities
are often available. We evaluate our bounds on estimating the mi of vaes and
gans trained on the mnist and cifar datasets, and showcase significant gains
over existing bounds in these challenging settings with high ground truth mi.
1 Introduction
Mutual information (mi) is among the most general measures of dependence between two random
variables. Among other applications in machine learning, mi has been used for both training (Alemi
et al., 2016; 2018; Chen et al., 2016; Zhao et al., 2018) and evaluating (Alemi & Fischer, 2018; Huang
et al., 2020) generative models. Furthermore, successes in neural network function approximation
have encouraged a wave of variational or contrastive methods for mi estimation from samples only
(Belghazi et al., 2018; van den Oord et al., 2018; Poole et al., 2019). However, McAllester & Stratos
(2020) have shown strong theoretical limitations on any estimator based on direct sampling without
an analytic form of at least one marginal distribution. In light of these limitations, we consider mi
estimation in settings where a single marginal or the full joint distribution are known.
In this work, we view mi estimation from the perspective of importance sampling. Using a general
approach for constructing extended state space bounds on mi, we combine insights from importance-
weighted autoencoder (iwae) (Burda et al., 2016; Sobolev & Vetrov, 2019) and annealed importance
sampling (AIS) (Neal, 2001) to propose Multi-Sample AIS bounds in Sec. 3. We empirically show
that this approach can tightly estimate large values of mi when the full joint distribution is known.
* Equal Contribution. brekelma@usc.edu; {huang, makhzani}@cs.toronto.edu. See arXiv for extended paper.
1
Published as a conference paper at ICLR 2022
Lower Bound on KU
Lower and Upper Bound on MI
Figure 1: Schematic of MI bounds discussed
in this paper. Green shading indicates our contri-
butions, while columns and gold labels indicate
single- or multi-sample bounds. Blue arrows in-
dicate special cases using the indicated proposal
distribution. Relationships based on learned critic
functions are indicated by red arrows. We obtain
only lower bounds on MI with unknown p(x|z),
but both upper and lower bounds with known
p(x|z). All bounds require a known marginal p(z)
for evaluation, apart from (Structured) Info-NCE.
Our importance sampling perspective also suggests improved mi lower bounds that assume access
to only joint samples for optimization, but require a single marginal distribution for evaluation. In
Sec. 2.4, we propose Generalized IWAE (GIWAE), which generalizes both IWAE and INFONCE
(Poole et al., 2019) and highlights how variational learning can complement multi-sample contrastive
estimation to improve MI lower bounds. Finally, in Sec. 4 we propose MINE-AIS, which optimizes
a tighter lower bound than MINE (Belghazi et al., 2018), called the Implicit Barber-Agakov Lower
bound (IBAL). We demonstrate that the IBAL corresponds to the infinite-sample limit of the GIWAE
lower bound, although our proposed energy-based training scheme involves only a single ‘negative’
contrastive sample obtained using Markov Chain Monte Carlo (mcmc). mine-ais then uses Multi-
Sample ais to evaluate the lower bound on mi, and shows notable improvement over existing
variational methods in the challenging setting of mi estimation for deep generative models. We
summarize the mi bounds discussed in this paper and the relationships between them in Fig. 1.
1.1	Problem Setting
The mutual information between two random variables x and z with joint distribution p(x, z) is
I(x; Z) = Ep(χ,z) log p(x, z< = H(x) - H(x|z) = Ep(x,z)[logp(x∣z)] - Ep(x)[logp(x)],	(1)
p(x)p(z)
where H(x|z) denotes the conditional entropy -Ep(x,z) log p(x|z). We primarily focus on bounds
that assume either a single marginal distribution or the full joint distribution are available. A natural
setting where the full joint distribution is available is estimating mi in deep generative models between
the latent variables, with a known prior Z 〜p(z), and data X 〜P(X) simulated from the model
(Alemi & Fischer, 2018). 1 Settings where only a single marginal is known appear, for example, in
simulation-based inference (Cranmer et al., 2020), where information about input parameters θ is
known and a simulator can generate X for a given θ, but the likelihoodp(x∣θ) is intractable.
While sampling from the posterior p(z|X) for an arbitrary X is often intractable, we can obtain a
single posterior sample for X 〜P(X) in cases where samples from the joint distribution p(x)p(z∣x)
are available. We will refer to bounds which involve only a single posterior sample as practical, and
those involving multiple posterior samples as impractical.
When the conditional P(X|z) is tractable to sample and evaluate, simple Monte Carlo sampling
provides an unbiased, low variance estimate of the conditional entropy term in Eq. (1). In this case,
the difficulty of MI estimation reduces to estimating the log partition function log P(X), for which
importance sampling (is) based methods are among the most well studied and successful solutions.
2	Unifying Mutual Information Bounds via Importance Sampling
In this section, we present a unified view of mutual information estimation from the perspective of ex-
tended state space importance sampling. This general approach provides a probabilistic interpretation
of many existing mi bounds and will suggest novel extensions in Sec. 3 and Sec. 4.
2.1	A General Approach for Extended State Space Importance Sampling Bounds
To estimate the log partition function, we construct a proposal qPROP (zext |X) and target PTGT(X, zext)
distribution over an extended state space, such that the normalization constant of PTGT (X, zext) is
ZTGT = PTGT(X, zext)dzext = P(X) and the normalization constant of qPROP (X, zext) is ZPROP = 1.
Taking expectations of the log importance weights logPTGT(X, zext)/qPROP(zext|X) under the proposal
1An alternative, “encoding” mi between the real data and the latent code is often of interest (see App. N), but
cannot be directly estimated using our methods due to the unavailability of pd (x) or q(z) = pd(x)q(z|x)dx.
2
Published as a conference paper at ICLR 2022
and target, respectively, we obtain lower and upper bounds on the log partition function
E
qPROP
「PTGT(x, ZeXt) ^
(ZextIx)Jg qpRθp(ZeXtIx) 
≤ log p(x) ≤ EpTGT (zext |x) log
pTGT (x, zext)
qPROP (zext |x)	.
(2)
ELBO (x； qPROP , PTGT )
V---------------{-----------
EUBO (x； qPROP,pTGT)


These bounds correspond to eXtended state space versions of the elbo and eubo. In particular, the
gap in the lower bound is the forward KL divergence DKL[qPROP(zeXt|x)kpTGT(zeXt|x)] and the gap in
the upper bound equal to the reverse KL divergence DKL[pTGT(zeXt|x)kqPROP(zeXt|x)]. See App. A.
2.2	Barber-Agakov Lower and Upper Bounds
As a first eXample, consider the standard ELBO(qθ) and EUBO(qθ) bounds, which are derived from
simple importance sampling using a variational distribution qθ(z|x) and ZeXt = Z in Eq. (2). Plugging
these lower and upper bounds on log p(x) into Eq. (1), we obtain upper and lower bounds on MI as
IBAL (qθ) := Ep(x,z) l * * *og "jj ] ≤ I (x； Z) ≤ Ep(x)qθ (z|x) log ^-γ - - - H (XIZ) =: IBAU (Qe) ∙	(3)
P(Z)	P(x, Z)
The left hand side of Eq. (3) is the well-known Barber-Agakov (ba) bound (Barber & Agakov,
2003), which has a gap of Ep(x)[Dkl [p(z∣x)kqθ(z|x)]]. We refer to the right hand side as the BA
upper bound IBAU (qθ), With a gap of Ep(x) [Dkl [qθ(z∣x)kp(z∣x)]]. In contrast to IBAU (qθ), note that
IBAL (qθ) does not require access to the conditional p(x|Z) to evaluate the bound.
2.3	Importance Weighted Autoencoder
The IWAE lower and upper bounds on log p(x) (Burda et al., 2016; Sobolev & Vetrov, 2019) improve
upon simple importance sampling by eXtending the state space using multiple samples ZeXt = Z(1:K).
Consider a proposal qPIWROAPE(Z(1:K) |x) with K independent samples from a given variational distribution
qθ(z|x). The extended state space target PTWAE(z(1:K) |x) is a mixture distribution involving a single
sample from the posterior p(z∣x) or joint p(x, Z) and K - 1 samples from q® (z|x)
K	KK
qPWAE(z(1:K)Ix) := Y Qθ(z(s)∣x)	PTGAE(x,z(1:K)) := κ X p(x,z(s))	Y qe(z(k)∣x).	(4)
s=1	s=1	k=1,k6=s
pIWAE (x z(1:K) )
The log importance weight log 彳卷标2反)reduces to the familiar ratio in the IWAE objective, while
the normalization constant ofpITWGATE(x, Z(1:K)) is p(x). As in Sec. 2.1, taking expectations under the
proposal or target yields a lower or upper bound, respectively,
EK
Q qθ(z(k)∣x)
1 XK p(x,z(k))
K M qθ (Z(k)|x)
≤ log p(x) ≤ E	K
p(z(1)∣x) Q qθ(z(k)∣x)
k=2
1 X p(x,z(k))
K M qθ (z(k)|x)
(5)
I
{z
ELBOIWAE (x； qe , K)
{z
EUBOIWAE (x； qe , K)
As for the standard ELBO and EUBO, the gap in the lower and upper bounds are DKL [qPIWROAPE kpITWGATE]
and DKL [pITWGATE kqPIWROAPE], respectively. See App. B for detailed derivations. As in Sec. 1.1, with
known p(x|Z), the lower and upper bounds on log p(x), ELBOIWAE (x; qθ, K) and EUBOIWAE (x; qθ, K)
translate to upper and lower bounds on MI, which we denote as IIWAEU (qθ, K) and IIWAEL (qθ, K).
While it is well-known that increasing K leads to tighter IWAE bounds (Burda et al., 2016; Sobolev &
Vetrov, 2019), in App. B.2, we characterize the improvement of ELBOIWAE(qθ, K) over ELBO(qθ) as
a KL divergence. For EUBOIWAE(qθ, K), we show that the KL divergence measuring its improvement
over EUBO(qθ) is limited to log K, which implies IIWAEL (qθ, K) ≤ IBAL (qθ) + log K.
2.4 Generalized IWAE
In this section, we consider a family of Generalized IWAE (GIWAE) lower bounds, which improve
upon IBAL (qθ) using multiple samples and a contrastive critic function Tφ(x, Z), but do not require
access to p(x|Z). While similar bounds appear in (Lawson et al., 2019; Sobolev, 2019), we provide a
thorough discussion of special cases, and empirical analysis for mi estimation in Sec. 5.2. We also
show that our ibal bound in Sec. 4 corresponds to the infinite sample limit of giwae.
To derive a probabilistic interpretation for giwae, we begin by further extending the state space of
the IWAE target distribution in Eq. (4), using a uniform index variable p(s) = K ∀s that specifies
which sample Z(S) ~ p(z∣x) is drawn from the posterior
3
Published as a conference paper at ICLR 2022
1K
PGGWAE(z(1：K),s[x) := Kp(Z(S)|x) Y q(z(k)∣x).	⑹
k=1,k6=s
Note that marginalization over s leads to the IWAE target pITWGATE(z(1:K) |x) in Eq. (4). For the GIWAE
extended state space proposal, consider a categorical index variable qPGRIWOPAE(s|z(1:K), x) drawn using
self-normalized importance sampling (SNIS), with weights calculated by a learned critic function Tφ
K	Tφ(x,z(s))
qGRWAE(z(1：K),s|x) = ( Y q(z(k)∣x) ) qGRWAE(s|z(1：K),x), WhereqGRWAE(s|z(1:K),X)=----------.
k=1	P eTφ(x,z(k))
k=1
We discuss the giwae probabilistic interpretation in App. C.1, and find that only the giwae upper
bound on log p(x) provides practical benefit in App. C.2. The corresponding MI loWer bound is
IGIWAEL (qθ , Tφ, K)
E	[log qθ(ZlX) ] + E	κ
p(x,z)	p(z)	p(x)p(z(1) |x) Q qθ (z(k) |x)
k=2
eTφ(x,z(1))
log ‰---------------------
.K P= eTΦ(x,z(k))
(7)
} |
IBAL (qθ)
~{Z^^^^^^^^^^^"
0 ≤ contrastive term ≤ log K
We observe that the giwae loWer bound decomposes into the sum of tWo terms, Where the first is the
BA variational lower bound for q® (z|x) and the second is a contrastive term which distinguishes a
single positive sample drawn from p(z∣x) from K - 1 negative samples drawn from q® (z|x).
Relationship with BA With a constant Tφ(x, z) = const, the second term in GIWAE vanishes and
we have IGIWAEL (q, Tφ = const, K) = IBAL (q®) for all K. For K = 1, IGIWAEL (q, Tφ, K = 1)
also equals the BA lower bound for all Tφ . Similarly to BA, GIWAE requires access to the analytical
form of p(z) to evaluate the bound on MI. However, both the BA and GIWAE lower bounds can be
used to optimize MI even if no marginal is available. See App. N. for detailed discussion.
Relationship with InfoNCE When the prior p(z) is used in place of q®(z|x), we can recognize
the second term in Eq. (7) as the InfoNCE contrastive lower bound (van den Oord et al., 2018; Poole
et al., 2019), with IINFONCEL (Tφ, K) = IGIWAEL (p(z), Tφ, K). From this perspective, the GIWAE
lower bound highlights how variational learning can complement contrastive bounds to improve mi
estimation beyond the known log K limitations of INFONCE (van den Oord et al., 2018).
Relationship with IWAE The following proposition characterizes the relationship between
IGIWAEL (q®, Tφ, K) in Eq. (7) and IIWAEL (q®, K) from Sec. 2.3. See App. C.5 for proofs.
Proposition 2.1 (Improvement of IWAE over GIWAE). For a given q®(z|x) and any Tφ(x, z),
IIWAEL (qθ,K) = IGIWAEL (qθ, Tφ,K) + Ep(x)pTGGIWTAE(z(1:K) |x)
hDKL[pTGGIWTAE(s|z(1:K), x)kqPGRIWOPAE(s|z(1:K)
Corollary 2.2. For a given q® (z|x) and K > 1, the optimal critic function is the true log importance
weight UP to an arbitrary constant: T*(x, z) = log Px篇 + c(x). With this choice of T*(x, z),
IgiwAEl (qθ ,T *, K) = IIWAEL (qθ, K) .	(8)
Corollary 2.3. Suppose the critic function Tφ (x, z) is parameterized by φ, and
∃ φo s.t. ∀ (x, z), Tφo (x, z) = const. For a given q®(z∣x), let Tφ*(x, z) denote the critic
function that maximizes the GIWAE lower bound. Using Cor. 2.2, we have
IBAL (qθ) ≤ IGIWAEL (qθ, Tφ* , K) ≤ IIWAEL (q, K) ≤ IBAL (qθ) + logK.	(9)
While the giwae lower bound on mi does not assume access to the full joint distribution, Cor. 2.2
suggests that the role of the critic function is to learn the true log importance weights for q®(z|x).
Thus, when p(x, z) is known, IWAE is always preferable to GIWAE. Cor. 2.3 shows that while
IGIWAEL (q, Tφ* , K) can improve upon BA, this improvement is at most logarithmic in K.
Relationship with Structured InfoNCE Since INFONCE and Structured INFONCE (S-
INFONCE, see App. B.5, Poole et al. (2019)) are special cases of GIWAE and IWAE that use p(z) as
the variational distribution, Cor. 2.3 suggests the following relationship (see App. C.6)
0 ≤ IINFONCEL (Tφ, K) ≤ IS-INFONCEL (K) ≤ logK .	(10)
4
Published as a conference paper at ICLR 2022
	IWAE	Single-Sample AIS	Independent Multi-Sample AIS		Independent Reverse Multi- Sample AIS		Coupled Reverse Multi-Sample AIS	
Practical?	√	√	✓		X		√	
Target EUBO			(gH⅛χ...	③娥	(S)√jχ...		(gHgχ... V	
ILB							(jχg>...-	÷<⅛)⅜)
							0^^∙∙∙	
Proposal ELBO	z,	W/	- zι -...	~⅛-ιz	- zτ/	(gHj)→...	招`				
			(JX⅛>...					-R∕
					(gHgκ...			
Figure 2: Extended state-space probabilistic interpretations of multi-sample AIS bounds. Forward
chains are colored in blue, and backward chains are colored in red. Note that ELBOS and EUBOS are
obtained by taking the expectation of the log importance weight logPtgtS%propS under either the
proposal or target distribution, and can then be translated to MI bounds.
3 Multi-Sample AIS Bounds for Estimating Mutual Information
In the previous sections, we derived probabilistic interpretations of extended state space importance
sampling bounds using multiple samples K, as in IWAE. In this section, we first review AIS, which
extends the state space using T intermediate densities. We then show that these approaches are
complementary, and derive two practical Multi-Sample AIS methods that provide tighter bounds by
combining insights from iwae and ais.
3.1 Annealed Importance Sampling Background
AIS (Neal, 2001) constructs a sequence of intermediate distributions {πt (z)}tT=0, which bridge
between a normalized initial distribution ∏o(z∣x) and target distribution ∏t(z|x) = p(z∣x) with
unnormalized density πT (x, z) = p(x, z) and normalizing constant ZT (x) = p(x). A common
choice for intermediate distributions is the geometric mixture path parameterized by {βt }tT=0
∏t(z∣x) := π0(zlx)	πT(x,z) , where Zt(X) := Z ∏o(z∣x)1-βt ∏t(x,z)βtdz .
Zt(x)
(11)
In the probabilistic interpretation of AIS, we consider an extended state space proposal qPARISOP (z0:T |x),
obtained by sampling from the initial ∏o(z∣x) and constructing transitions Tt (zt ∣zt-ι) which leave
∏t-ι(z∣x) invariant. The target distribution PAGSr(z0：T|x) is given by running the reverse transitions
Tt(zt-1 |zt) starting from a target or posterior sample πT (z|x), as shown in Fig. 2,
TT
qAROp(Z0：T|x) := ∏o(zo∣x) Y Tt(zt∣Zt-ι) , PAGSr(x, Z0:T) := ∏T(x, ZT) Y T(zt-ι∣Zt).	(12)
t=1	t=1
raking expectations of the log importance weights under the proposal and target again yields a lower
and upper bound on the log partition function log p(x) (App. E). rhese single-chain lower and
upper bounds translate to upper and lower bounds on MI, IAISU (π0, T) and IAISL (π0, T), which were
suggested for mi estimation in the blog post of Sobolev (2019). ro characterize the bias reduction for
AIS with increasing T , we prove the following proposition.
Proposition 3.1 (Complexity in T). Assuming perfect transitions and a geometric annealing path
with linearly-spaced {βt}tT=1, the sum of the gaps in the AIS sandwich bounds on MI, IAISU (π0, T) -
IAISL (π0, T), reduces linearly with increasing T.
See App. D.1 for a proof. In our experiments in Sec. 5, we will find that this linear bias reduction in
T is crucial for achieving tight MI estimation when both p(z) and p(x|z) are known. However, we
can further tighten these AIS bounds using multiple annealing chains (K > 1), and we present two
practical extended state space approaches in the following sections.
3.2	Independent Multi-Sample AIS Bounds
ro derive Independent Multi-Sample AIS (IM-AIS), we construct an extended state space proposal by
running K independent AIS forward chains ZST 〜qAROp in parallel. Similarly to the IWAE upper
bound (Eq. (5)), the extended state space target involves selecting a index s uniformly at random, and
running a backward AIS chain z0sT 〜PAGr starting from a true posterior sample ZT 〜p(z∣x). The
remaining K - 1 samples are obtained by running forward AIS chains, as visualized in Fig. 2
5
Published as a conference paper at ICLR 2022
K	1K	K
qPMOAIS(Z0：T)IX)= Q qAROp(Z0：TIX),	PTGTUS(X,z0：T)) = K PPAGSr(X,z0sT)	Q	qAROp(Z0：TIX), (13)
k=1	K s=1	k=1,k6=s
where qpARISOp and PTAGIST were defined in Eq. (12). Note that sampling from the extended state space
target distribution is practical, as it only requires one sample from the true posterior distribution.
As in Sec. 2.1, taking the expectation of the log unnormalized density ratio under the proposal and
target yields lower and upper bounds on log P(X), respectively,
Ez01TK)-qAROp
log 1 XX PAGr (X,z0kT)
K k = 1 qAROp (ZOkT |x)-
≤ log P(X) ≤ E Z⑴〜PAIS
z0-T ~pTGT
z02TK) 〜qAROp
ELBOIM-AIS (x; π0, K, T)
lθg 1 XX PAGT (x,z0kT)
K k = 1 qAROP (z0kT |x)-
{z	'
(14)
EUBOIM-AIS (X; π0, K, T)
|

which again have kl divergences as the gap in their bounds (see App. E). Independent Multi-Sample
AIS reduces to IWAE for T = 1, and reduces to single-sample AIS for K = 1. Both upper and
lower bounds are tight as K → ∞ or T → ∞, and translate to lower and upper bounds on MI as in
Sec. 1.1. In App. E.3, we show that the Independent Multi-Sample AIS lower bound on MI is limited
to logarithmic improvement over single-sample AIS, with IIM-AISL (π0, K, T) ≤ IAISL (π0, T) + logK.
3.3	Coupled Reverse Multi-Sample AIS Bounds
We can exchange the role of the forward and backward annealing chains in Independent Multi-
Sample AIS to obtain alternative bounds on the log partition function. We define Independent Reverse
Multi-Sample AIS (IR-AIS) using the following proposal and target distribution, as shown in Fig. 2.
qpIRR-OApIS(X, z(01:TK)) :
1 K	K
K X qAROp(z0sT|x)	Y	PAGST(X, zOkT),
s=1	k=1,k6=s
K
IR-AIS (X z(1:K)) := Y AIS (X z(k))
PTGT (X, zO:T ) := PTGT (X, zO:T) .
k=1
Note that partition function ratio is ZTGT /ZpROp = P(X)K/P(X)K-1 = P(X). Using these distribu-
tions, we derive log P(X) and MI bounds in App. G. However, the these bounds will be impractical in
most settings since they require multiple true posterior samples (see Sec. 1.1).
To address this, we propose Coupled Reverse Multi-Sample AIS (CR-AIS). As shown in Fig. 2, the
extended state space target distribution initializes K backward chains from a single target sample
ZT 〜∏t(z∣x), with the remaining transitions PA(Gr(Z0：T-IIZT) matching standard AIS in Eq.(12).
K
PTCGR-TAIS(z(O1:T:K-)1, zT, X) := πT(zT,X) Y PTAGIST(z(Ok:T) -1|zT, X) .	(15)
k=1
The extended state space proposal is obtained by selecting an index s uniformly at random and
running a single forward AIS chain. We then run K - 1 backward chains, all starting from the last
state of the selected forward chain, as visualized in Fig. 2
1 K	K
qCROAIS(zO1T-)ι,ZT|x) := KEqAOP(ZOsT-1,ZT|x) ∏ PAGT(ZOkT-ι∣zτ,x) .	(16)
s=1	k=1,k6=s
Taking the expected log ratio under the proposal and target yields lower and upper bounds on log P(X),
-E	J log ɪ XX qAROp(∙)
z0:T-1，ZT-qAROp(z0:TIx)	K k= PAGTG)
(2:K)	AIS
z0∙T -1~PAGΓ(Z0:T-1 |zT，x)
'------------------------{z-----------------------
ELBOCR-AIS (x; πO , K, T)
}
≤ logP(X) ≤-E	ZT~∏T(ZTlx)	J log -K XX ⅛⅛ 1
z0:TK)1~PAGΓ (z0:T-1 lzT,x)
EUB OCR-AIS(x; πO, K, T)
We show in App. H.3 that the Coupled Reverse Multi-Sample AIS upper bound on MI is limited to
logarithmic improvement over single-sample AIS, with ICR-AISU (π0, K, T) ≥ IAISU (π0, T) - log K.
3.4 Discussion
Relationship with BDMC While Bidirectional Monte Carlo (BDMC) (Grosse et al., 2015; 2016)
was the first method proposing multi-sample log partition function bounds using ais chains, our
probabilistic interpretations provide novel perspective on bdmc. perhaps surprisingly, we find that
bdmc lower and upper bounds do not correspond to the same extended state space proposal and
target distributions. In particular, the BDMC lower bound on log P(X) corresponds to the lower bound
of Independent Multi-Sample ais (Fig. 2 Col. 4, Row 4), while the upper bound of bdmc matches
the upper bound of Coupled Reverse Multi-Sample ais (Fig. 2 Col. 6, Row 3).
6
Published as a conference paper at ICLR 2022
Effect of K and T We have shown in Prop. 3.1 that Multi-Sample AIS bounds can achieve linear
bias reduction with increasing T, although this computation must be done in serial fashion. While
increasing K involves parallel computation, its bias reduction is often only logarithmic, as we show for
IIWAEL(qθ,K) (Cor. B.3), IIM-AISL(π0, K, T) (App. E.3), and ICR-AISU (π0, K, T) (App. H.3). Based
on these arguments, we recommend increasing K until computation can no longer be parallelized on
a given hardware and allocating all remaining resources to increasing T .
Comparing Multi-Sample AIS Bounds In App. I Fig. 5, we compare performance of our various
Multi-Sample ais bounds in order to recommend which to use in practice. For the upper bound on mi,
we recommend the Independent Multi-Sample ais elbo, or the forward direction of bdmc, since it
uses independent samples and is not limited to log K improvement. The results are less conclusive
for the lower bounds on MI. While the MI lower bound obtained from EUB OIM-AIS (x; π0 , K, T ) (RHS
of Eq. (14)) can improve upon single-sample AIS by at most log K (App. E.3), this improvement is
easily obtained for low T and may be used to quickly estimate MI of a similar magnitude as log K.
The Coupled Reverse AIS lower bound on MI requires moderate values of T to match or marginally
improve on Independent Multi-Sample ais, suggesting that the preferred mi lower bound may differ
based on the scale of the true mi and amount of available computation.
4 MINE-AIS Estimation of Mutual Information
For settings where the conditional p(x|z) is unknown, we propose MINE-AIS, which is inspired by
mine but optimizes a tighter lower bound on mi (App. J). Although this bound involves an intractable
log partition function, we present a stable, energy-based training scheme and use our Multi-Sample
ais methods from Sec. 3 to evaluate the bound. Consider an flexible, energy-based distribution
∏θ,φ(z∣x) as an approximation to the posterior p(z∣x) (Poole et al., 2019; Arbel et al., 2020)
∏θ,φ(z∣x)=	1	qθ⑵x)eTφ(x,z) , where Zθ,φ(x) = Eq@(z|x)heTφ(x,z)i .	(17)
Zθ,φ (x)
Plugging ∏θ,φ(z∣x) into the BA lower bound, we denote the resulting bound as the Implicit Barber-
Agakov Lower bound (IBAL), since it is often difficult to evaluate explicitly due to the intractable log
partition function term. After simplifying in App. J.1, we obtain
I(x; z) ≥ IBAL (πθ,φ) = Ep(x,z)	log qθP(Zx2 ]+ Ep(W) {	' 1		eTφ(x,z) .log Eg。(z∣χ) [e…L	=: IBAL(qθ, Tφ), (18)
^^^^^^^^^^^^^^--{z^^^^^^^^^^^≡
IBAL (qθ )	≤Ep(x) [DKL[P(ZIX)kqθ (z|x)]]
with the gap of the IBAL equal to Ep(X)DKL[p(z∣x))k∏θ,φ(z∣x)]]. Note that the IBAL generalizes
the Unnormalized Barber-Agakov bound from Poole et al. (2019), with IUBA(Tφ) = IBAL(p(z), Tφ).
Proposition 4.1. For a given qθ (z|x), the optimal IBAL critic function equals the log importance
weights up to a constant T*(x, Z) = log 看裾?)+ C(X). For this T*, we have IBAL(q6, T*) = I(x; Z).
Relationship with GIWAE We can immediately notice similarities between giwae and mine-ais,
including that their optimal energy functions match and that both bounds include a contrastive term
which improves upon the ba lower bound. In fact, we prove the following proposition in App. L.2.
Proposition 4.2. For given qθ (Z|x) and Tφ(x, Z), lim IGIWAEL (qθ, Tφ, K) = IBAL(qθ, Tφ).
K→∞
Thus, we may view the IBAL as the limiting behavior of the finite-sample GIWAE bounds as K → ∞.
While Cor. 2.3 shows that IGIWAEL (qθ, T) can improve IBAL (qθ) by at most log K nats, we show in
App. L.1 that the ibal contrastive term is flexible enough to close the entire gap in the ba bound.
However, this flexible contrastive term comes at the cost of tractability, as the ibal in Eq. (18)
involves a log partition function log Eq@(z|x)[eTφ(X,Z)] compared to the finite-sum term in GIWAE.
Energy-Based Training of IBAL Although the log partition function log Zθ,φ (x) in the IBAL is in-
tractable to evaluate, we only require an unbiased estimator of its gradient for training. Differentiating
Eq. (18) with respect to the parameters θ and φ, respectively, we obtain
AM a⅜
IBAL(qθ,Tφ) = Ep(χ,z) ɪ log qθ(z|x) - Ep(x)∏θ φ(z∣x) S log qθ(z|x) ,	(19)
∂ θ	,φ ∂ θ
IBAL(qθ,Tφ) = Ep(χ,z) ∂φTφ(x,z) - Ep(χ)∏θ,φ(z∣χ) Qd(x,z) .	(20)
7
Published as a conference paper at ICLR 2022
Method	Proposal	MNIST-VAE10	MNIST-VAE100	MNIST-GAN10	MNIST-GAN100	Method	Proposal	CIFAR-GAN10	CIFAR-GAN100
AIS	P(Z)	(0.00, 1929.84)	(0.00, 5830.52)	(0.00,786.12)	(0.00, 861.38)	AIS	P(Z)	(0.00,4035635.75)	(0.00, 4853410.50)
(T=1)	q(z|x)	(21.06,63.00)	(34.49, 362.13)	(3.67, 314.72)	(2.61, 513.33)	T=1	q(ZIX)	(17.30,403679.22)	(20.17, 2378257.50)
AIS	P(Z)	(34.05, 39.09)	(79.90,95.17)	(21.57,22.47)	(25.86, 27.55)	AIS	P(Z)	(29.52, 33089.90)	(104.51, 63290.40)
(T=500)	q(ZIX)	(34.16,34.29)	(80.19,82.34)	(21.60,23.06)	(25.58, 29.53)	T=500	q(ZIX)	(48.16,136.15)	(145.19, 2786.53)
AIS	P(Z)	(34.21,34.21)	(80.78,80.84)	(21.97,22.02)	(26.47, 26.52)	AIS	P(Z)	~(71.87,73.98)-	(480.26, 488.07)
(T=30K)	q(ZIX)	(34.21,34.21)	(80.77,80.80)	(22.01,22.01)	(26.53, 26.54)	T=100K	q(ZIX)	~(72.85,73.54)-	(479.27, 484.84)
IWAE	P(Z)	(0.00, 3827.58)	(0.00,11501.92)	(0.00,1630.00)	(0.00, 1740.39)	IWAE	P(Z)	(0.00, 7765695.50)	(0.00, 9916102.00)
(K=1)	q(ZIX)	(25.20,35.34)	(44.54, 95.63)	(4.23, 57.47)	(3.23, 260.87)	K=1	q(ZIX)	(17.45, 77.52)	(20.00, 5346.85)
IWAE	P(Z)	(6.91, 1197.75)	(6.91,4234.19)	(6.91, 446.80)	(6.91, 494.73)	IWAE	P(Z)	(6.91, 2044170.75)	(6.91, 2856714.50)
(K=1K)	q(ZIX)	(31.69,34.24)	(51.44, 85.30)	(11.14, 52.73)	(10.14, 201.18)	K=1K	q(ZIX)	(23.58, 74.00)	(26.98, 5283.13)
IWAE	P(Z)	(13.82, 376.89)	(13.82, 2247.73)	(13.81, 81.51)	(13.82, 114.01)	IWAE	P(Z)	(13.82, 710511.63)	(13.82, 1903854.50)
(K=1M)	q(ZIx)	(34.10,34.22)	(58.35, 83.39)	(17.76, 30.88)	(16.98, 58.04)	K=1M	q(ZIx)	(30.73,73.36)	(33.81, 5271.56)
Table 1: MI Estimation with IWAE (with varying K) and Multi-Sample AIS (with varying T) on
mnist (left) and cifar (right). Tight estimates, with gap of less than 2 nats, are in bold.
Eq. (19) indicates that to maximize the IBAL as a function of θ and φ, we need to increase the value
of Tφ(x, z) or log qθ(z|x) on samples from p(x, z), and lower it on samples from p(x)∏θ,φ(z∣x).
As is common in training energy-based models, it is difficult to draw samples from ∏θ,φ(z∣x). To
reduce the cost and variance of the estimated gradient, we initialize chains from true posterior sample
zo 〜p(z∣x) as in contrastive divergence training (Hinton, 2002) and run M steps of Hamiltonian
Monte Carlo (HMC) transition kernels T1:M (z|z0, x) (Neal, 2011). See App. M.1 for details.
Multi-Sample AIS Evaluation of IBAL After training the critic function using the procedure
above, we still need to evaluate the IBAL lower bound on MI. We can easily upper bound IBAL(qθ, Tφ)
using a Multi-Sample AIS lower bound on log Zθ,φ(x), but this does not ensure a lower bound on
MI. In order to upper bound log Zθ,φ(x) and preserve a lower bound on I(x; z), Multi-Sample AIS
requires true samples from ∏θ,φ(z∣x) to initialize backward AIS chains. Since these samples are
unavailable, We instead initialize backward chains from the true posterior p(z∣x) instead of ∏θ,φ(z∣x).
We derive sufficient conditions under which this scheme preserves an upper bound on log Zθ,φ(x) in
App. M.2 and provide empirical validation in App. M.3.
5	Experiments
In this section, we evaluate our proposed mi bounds on vaes and gans trained on mnist and cifar.
5.1	Multi-Sample AIS Estimation of Mutual Information
We compare Multi-Sample ais mi estimation against iwae, since both methods assume the full joint
distribution is available. For the initial distribution of ais or variational distribution of iwae, we
experiment using both the prior p(z) and a learned Gaussian qe(z|x). Table 1 summarizes our results.
IWAE As described in Sec. 2.3, IWAE bounds encompass a wide range of MI estimators. The K = 1
bounds with learned qe(z|x) correspond to BA bounds, while for K > 1 andp(z) as the proposal,
we obtain Structured INFONCE. While the IWAE upper bound on MI, which uses the log p(x) lower
bound with independent sampling from qe(z|x), is tight for certain models, we can see that the
improvement of the IWAE lower bound on MI is limited by log K . In particular, we need exponentially
large sample size to close the gap from the BA lower bound (K = 1) to the true MI. For example, on
CIFAR GAN 1 00, at least e460 total samples are required to match the lower bound estimated by AIS.
Multi-Sample AIS We evaluate Multi-Sample AIS with K = 48 chains on MNIST and K = 12 on
CIFAR, and a varying number of intermediate T. We show results for the Independent Multi-Sample
ais mi lower bound and Coupled Reverse Multi-Sample ais upper bound in Table 1. Using large
enough values of T, Multi-Sample AIS can tightly sandwich large values of ground truth MI for
all models and datasets considered. This is in stark contrast to the exponential sample complexity
required for the IWAE MI lower bound, and highlights that increasing T in Multi-Sample AIS is a
practical way to reduce bias using additional computation. We provide runtime details in App. O.1.3,
and provide additional results comparing different Multi-Sample ais bounds in App. I Fig. 5.
5.2	Energy-Based Estimation of Mutual Information
In this section, we evaluate the family of giwae and mine-ais bounds, which assume access to a
known marginal p(z) but not the conditional p(x|z). We summarize these bounds in Fig. 3b.
BA, IWAE, and GIWAE Bounds Recalling that IGIWAEL (qe, Tφ, K) in Eq. (7) consists of the BA
lower bound and a contrastive term, we report the contribution of each term in Fig. 3a. For a fixed
qe(z|x), Cor. 2.3 shows IBAL (qe) ≤ IGIWAEL (qe, Tφ*, K) ≤ IIWAEL (qe, K) ≤ IBAL (qe) + log K.
8
Published as a conference paper at ICLR 2022
Input Used	7^∙∙^^^ Model Bound ~^^'^-^-^^	Linear VAE10	MNIST VAE20	MNIST GAN20	/(x*) E[Dkl[p(z 区 	IBAL(qp
p(x, z)	Analytical	23.23	N/A	N/A	
p(x, z) Joint Samples	AIS Bound on True MI	(23.23,23.23)	(65.11, 65.17)	(53.43, 53.50)	
	IWAE LB(K = 1000)	20.53 + 2.66 = 23.19	38.21 + 6.90 = 45.11	20.97 + 6.91 = 27.88	(MINE-AIS) EHKL 弗式Qg 国 χ)∣∣ 曾(* 叱 s∣×)l] (加κ) IX)Ilg^(Z 出 K)I嘲] E[-Dkl[p(z∣x)∣∣%(z∣x)]]
	IWAE LB(K = 100)	21.64 + 1.50 = 23.14	38.86 + 4.61 = 43.47	20.86 + 4.60 = 25.46	
	Structured InfoNCE LB (K = 1000)	6.91	6.91	6.91	
	Structured InfoNCE LB (K = 100)	4.61	4.61	4.61	
p(z) Joint Samples	AIS Bound on IBAL (MINE-AIS)	(23.15, 23.15)	(57.72, 57.74)	(40.79, 40.79)	logjc I1WAE1, (ga,g) 卜	X)K)IIH∙(∙Mιm,χ)I] W 『giwael (qe,Tφ> K) 卜⑷KL端(SIZ(IR切〃⑹] U	，BAL(qe)
	AIS Bound on IBAL (GIWAE K = 100)	(22.87, 22.87)	(44.97, 44.97)	(28.61, 28.62)	
	AIS Bound on IBAL (InfoNCE K = 100)	(11.38,11.39)	(5.18,5.18)	(7.42, 7.42)	
	Generalized IWAE LB (K = 1000)	22.31 + 0.38 = 22.69	37.23 + 6.55 = 43.78	20.50 + 6.72 = 27.22	
	Generalized IWAE LB (K = 100)	22.48 + 0.39 = 22.87	37.56 + 4.34 = 41.90	20.68 + 4.57 = 25.25	
	Barber-Agakov LB (K = 1)	22.69	37.92	21.42	
Joint Samples	InfoNCE LB (K = 1000)	6.91	6.91	6.91	
	InfoNCE LB (K = 100)	4.61	4.61	4.61	,,	
(a)	(b)
Figure 3: (a) Comparison of energy-based bounds (giwae and mine-ais) with other mi bounds.
(b) Visualizing the gaps of various energy-based lower bounds and their relationships.
Although we perform separate optimizations for each entry in Fig. 3a, we find that these relationships
hold in almost all cases. We also observe that giwae can approach the performance of iwae, despite
the fact that GIWAE uses a learned Tφ(x, z) instead of the optimal critic in IWAE (Cor. 2.2).
For the Linear VAE, the true posterior is in the Gaussian variational family and IBAL (qθ) is close
to the analytical MI. In this case, the contrastive term provides much less than logK improvement
for GIWAE and IWAE, since even the optimal critic function cannot distinguish between qθ (z|x) and
p(z|x). As K increases, we learn a worse qθ in almost all cases, as measured by a lower BA term.
This allows the contrastive term to achieve closer to its full potential log K, resulting in a higher
overall bound. For more complex vae and gan posteriors, there is a reduced tradeoff between the
terms since the variational family is far enough from the true posterior (in reverse kl divergence)
that either GIWAE or IWAE critic functions can approach log K improvement without significantly
lowering the BA term. In all cases, (Structured) INFO-NCE bounds saturate to log K.
MINE-AIS Bounds We report MINE-AIS results using a fixed Gaussian p(z) as the base variational
distribution and Multi-Sample AIS evaluation of IBAL(p(z), Tφ). We can see in Fig. 3a that MINE-AIS
improves over ba due to its flexible, energy-based variational family. To evaluate the quality of the
learned Tφ(x, z), we compare the IBAL to the Multi-Sample AIS lower bound, which assumes access
to p(x|z) and corresponds to the optimal critic in Prop. 4.1. We find that MINE-AIS underestimates
the ground truth MI by 11% and 24% on MNIST-VAE and MNIST-GAN, respectively.
We also observe that MINE-AIS outperforms GIWAE (and IWAE) and by 31% and 49% on MNIST-VAE
and mnist-gan, respectively. To investigate whether this is due to its improved critic function or a
more costly evaluation procedure, in Fig. 3a we report Multi-Sample AIS evaluation of IBAL(qθ , Tφ)
for qθ , Tφ learned by optimizing the GIWAE or INFONCE lower bounds with K = 100. GIWAE and
InfoNCE results only marginally improve, indicating that their critic functions are suboptimal and
far from the true log importance weights.2 We conclude that the improvement of mine-ais can
primarily be attributed to learning a better critic function using energy-based training.
6	Conclusion
We have provided a unifying view of mutual information estimation from the perspective of impor-
tance sampling. We derived probabilistic interpretations of each bound, which shed light on the
limitations of existing estimators and motivated our novel giwae, Multi-Sample ais, and mine-ais
bounds. When the conditional is not known, our giwae bounds highlight how variational bounds can
complement contrastive learning to improve lower bounds on MI beyond known log K limitations.
When the full joint distribution is known, we show that our Multi-Sample ais bounds can tightly
estimate large values of mi without exponential sample complexity, and thus should be considered
the gold standard for mi estimation in these settings. Finally, mine-ais extends Multi-Sample ais
evaluation to unknown conditional densities, and can be viewed as the infinite-sample behavior of
giwae and existing contrastive bounds. Our mine-ais and Multi-Sample ais methods highlight
how mcmc techniques can be used to improve mi estimation when a single analytic marginal or
conditional density is available.
2Note that IWAE and Structured INFONCE use the true log importance weights T* (x, Z) = log qp(x,Z))) +c(x)
(Cor. 2.2). For this optimal critic, IBAL(qθ, T*) = I(x; z), ∀q0 (Prop. 4.1) and AIS will sandwich the true MI.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The authors thank Shengyang Sun, Guodong Zhang, Vaden Masrani, and Umang Gupta for helpful
comments on drafts of this work. We also thank the anonymous reviewers whose comments greatly
improved the presentation and encouraged us to derive several additional propositions. MG, RG and
AM acknowledge support from the Canada CIFAR AI Chairs program.
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a
broken elbo. In International Conference on Machine Learning, pp. 159-168, 2018.
Alexander A Alemi and Ian Fischer. Gilbo: one metric to measure them all. In Proceedings of the
32nd International Conference on Neural Information Processing Systems, pp. 7037-7046, 2018.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Shun-Ichi Amari. Alpha-divergence is unique, belonging to both f-divergence and bregman divergence
classes. IEEE Transactions on Information Theory, 55(11):4925-4931, 2009.
Michael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based models. In International
Conference on Learning Representations, 2020.
David Barber and Felix Agakov. The im algorithm: a variational approach to information maxi-
mization. In Proceedings of the 16th International Conference on Neural Information Processing
Systems, pp. 201-208, 2003.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 531-540. PMLR, 2018.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf
log-likelihood using reverse annealing. In Artificial Intelligence and Statistics, pp. 102-110. PMLR,
2015.
Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
International Conference on Learning Representations, 2016.
Sourav Chatterjee, Persi Diaconis, et al. The sample size required in importance sampling. The
Annals of Applied Probability, 28(2):1099-1135, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Proceedings of the 30th International Conference on Neural Information Processing Systems, pp.
2180-2188, 2016.
Andrzej Cichocki and Shun-ichi Amari. Families of alpha-beta-and gamma-divergences: Flexible
and robust measures of similarities. Entropy, 12(6):1532-1568, 2010.
Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference.
Proceedings of the National Academy of Sciences, 117(48):30055-30062, 2020.
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-weighted autoencoders.
arXiv preprint arXiv:1704.02916, 2017.
Justin Domke and Daniel R Sheldon. Importance weighting and variational inference. In Advances
in neural information processing systems, pp. 4470-4479, 2018.
Arnaud Doucet, Will Sussman Grathwohl, Alexander G de G Matthews, and Heiko Strathmann.
Annealed importance sampling meets score matching. In ICLR Workshop on Deep Generative
Models for Highly Structured Data, 2022.
10
Published as a conference paper at ICLR 2022
Roger B Grosse, Chris J Maddison, and Ruslan Salakhutdinov. Annealing between distributions by
averaging moments. In NIPS, pp. 2769-2777. Citeseer, 2013.
Roger B Grosse, Zoubin Ghahramani, and Ryan P Adams. Sandwiching the marginal likelihood
using bidirectional monte carlo. arXiv preprint arXiv:1511.02543, 2015.
Roger B Grosse, Siddharth Ancha, and Daniel M Roy. Measuring the reliability of mcmc inference
with bidirectional monte carlo. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 2459-2467, 2016.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression
rates of deep generative models. In International Conference on Machine Learning. PMLR, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Dieterich Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: learning
with sampler-induced distributions. In Proceedings of the 33rd International Conference on Neural
Information Processing Systems, pp. 8501-8513, 2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Chris J Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy
Mnih, Arnaud Doucet, and Yee Whye Teh. Filtering variational objectives. In Proceedings of the
31st International Conference on Neural Information Processing Systems, pp. 6576-6586, 2017.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In International Conference on Artificial Intelligence and Statistics, pp. 875-884. PMLR, 2020.
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning. Advances in Neural Information Processing Systems, 28:2125-
2133, 2015.
Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125-139, 2001.
Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,
2011.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, pp. 271-279, 2016.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Avraham Ruderman, Mark D Reid, Dario Garcia-Garcia, and James Petterson. Tighter variational
representations of f-divergences via restriction to probability measures. In Proceedings of the 29th
International Coference on International Conference on Machine Learning, pp. 1155-1162, 2012.
11
Published as a conference paper at ICLR 2022
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp.
2234-2242, 2016.
Artem Sobolev. Thoughts on mutual information estimation: More estima-
tors. Blog post,	2019. URL http://artem.sobolev.name/posts/
2019-08-10-thoughts-on-mutual-information-more-estimators.html.
Artem Sobolev and Dmitry P Vetrov. Importance weighted hierarchical variational inference. In
Advances in Neural Information Processing Systems, volume 32, 2019.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. In International Conference on Learning Representations, 2019.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of
decoder-based generative models. arXiv preprint arXiv:1611.04273, 2016.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. The information autoencoding family: A la-
grangian perspective on latent variable generative models. In Proc. 34th Conference on Uncertainty
in Artificial Intelligence, 2018.
12
Published as a conference paper at ICLR 2022
Supplementary Material
Table of Contents
A A General Approach for Deriving Extended State Space Bounds on Log Partition
Functions	14
B	Importance Weighted Autoencoder (IWAE)	15
B.1	Probabilistic Interpretation and Bounds ................................. 15
B.2	Proof of Logarithmic Improvement in K for IWAE EUBO ..................... 17
B.3	Experimental Results showing Logarithmic Improvement for IIWAEL (qθ, K) . . .	18
B.4	Bias Reduction in K for IWAE Lower Bound on log p(x) / Upper Bound on MI 19
B.5	Relationship with Structured InfoNCE .................................... 19
C Generalized IWAE	19
C.1 Probabilistic Interpretation and Bounds ................................... 19
C.2 GIWAE Upper Bound on MI Does Not Provide Benefit Over IWAE ................ 21
C.3 ELBO and EUBO are Special Cases of GIWAE Log Partition Function Bounds .	22
C.4 Proof of Relationship between IWAE and GIWAE Probabilistic Interpretations
(Prop. 2.1) .............................................................. 22
C.5 Proof of GIWAE Optimal Critic Function and Logarithmic Improvement (Cor. 2.2
and Cor. 2.3) ............................................................ 23
C.6 Properties of InfoNCE ..................................................... 25
D	Single-Sample AIS	25
D.1 Proof of Prop. 3.1 (Complexity in T for Single-Sample AIS) ................ 25
E	Independent Multi-Sample ais	26
E.1 Probabilistic Interpretation and Bounds ................................... 26
E.2 Proof of Linear Bias Reduction in T for IM-AIS ............................ 26
E.3 Proof of Logarithmic Improvement of IM-AIS EUBO ........................... 27
F Reverse IWAE	27
F.1 Probabilistic Interpretation and Bounds ................................... 27
F.2 Improvement of RIWAE over ELBO and EUBO ................................... 28
G Independent Reverse Multi-Sample AIS	28
G.1 Probabilistic Interpretation and Bounds ................................... 28
G.2 Proof of Logarithmic Improvement in K for Independent Reverse AIS ......... 29
H Coupled Reverse Multi-Sample AIS	29
H.1 Probabilistic Interpretation and Bounds ................................... 29
H.2 Proof that CR-AIS Bounds Tighten with Increasing K ........................ 30
H.3 Proof of Logarithmic Improvement in K for CR-AIS ELBO ..................... 32
H.4 Proof of Linear Bias Reduction in T for CR-AIS ............................ 33
I Comparison of Multi-Sample AIS Bounds	33
J Generalized Mutual Information Neural Estimation (gmine)	34
J.1	Probabilistic Interpretation of IBAL .................................... 35
J.2	Probabilistic Interpretation of Generalized mine-dv ..................... 35
J.3	Probabilistic Interpretation of Generalized mine-f ...................... 37
K Conjugate Duality Interpretations	39
K.1 Convex Conjugate Background ............................................... 39
K.2	Conjugate Duality Interpretation of IBAL ................................ 39
K.3	Conjugate Duality Interpretation of Generalized MINE-DV ................. 40
K.4	Conjugate Duality Interpretation of Generalized MINE-F .................. 41
13
Published as a conference paper at ICLR 2022
K.5 Conjugate Duality Interpretation of GIWAE, IWAE, and InfoNCE ............ 42
L Properties of the IBAL	45
L.1 Proofs for IBAL Optimal Critic Function (Prop. 4.1 and L.1) ............. 45
L.2 Proof of IBAL as Limiting Behavior of the GIWAE Objective as K → ∞ (Prop. L.2) 46
L.3 Convergence of GIWAE SNIS Distribution to IBAL Energy-Based Posterior . . .	47
M	MINE-AIS	48
M.1 Energy-Based Training of the IBAL ....................................... 48
M.2 Multi-Sample AIS Evaluation of the IBAL ................................. 48
M.3 MINE-AIS Experiments .................................................... 51
N	Applications to Mutual Information Estimation without Known Marginals	52
N.1 BA Lower Bound .......................................................... 52
N.2 GIWAE Lower Bound ....................................................... 53
N.3 MINE-AIS / IBAL Lower Bound ............................................. 53
O	Experimental Details	54
O.1 Experiment Details of Sec. 5.2 .......................................... 54
O.2 Experiment Details for Energy-Based Bounds (Sec. 5.2) ................... 55
O.3 Analytical Solution of the Mutual Information on the Linear mnist-vae ... 56
O.4 Confidence Intervals for Multi-Sample AIS Experiments ................... 56
A A General Approach for Deriving Extended State Space Bounds
on Log Partition Functions
In this section, we give a short proof that the gap in our general extended state space bounds from
Sec. 2.1 corresponds to a forward or reverse kl divergence. We derive various upper and lower
bounds on log p(x) using this approach throughout the paper and appendix, and we provide a visual
summary in Fig. 4.
First, we consider an extended state space target pTGT (x, zext) and proposal qPROP (x, zext) dis-
tributions. For all cases discussed in this work, we will choose our target and proposal dis-
tributions such that log ZGP(X) = logp(x). For example, a common construction is to have
ZTGT(x) = R pTGT(x, zext)dzext = p(x) and ZPROP(x) = R qPROP(x, zext)dzext = 1. Our ‘reverse’
importance sampling bounds App. F-G construct target and proposal such that ZTGT(x) = p(x)K
and ZPROP (x) = p(x)K-1, which still yields ZTGT (x)/ZPROP (x) = p(x).
Each pair of extended state-space proposal and target distributions provides both an upper and lower
bound on the log partition function. Taking the expected log ratio of unnormalized densities under
either the proposal or target distribution, we have
qPROP(zext |x)
log PTGT(X,Zext)一
qPROP(x, zext)
≤ log f⅛ ≤ EpTGT(ZextlX)
ZPROP (X)
log
PTGT(X,Zext)
qPROP (X, zext )
(21)
To confirm that these are indeed lower and upper bounds for any qPROP and PTGT, we can show that the
gap in the lower bound in Eq. (21) is the forward KL divergence DKL [qPROP kPTGT], and the gap in the
upper bound is the reverse KL divergence, DKL [PTGT kqPROP]
EqPROP(ZeXtIxJPTEx ZeXt) I = log Z!z⅛ - DKL[qPROP(Zext|x)kpTGT(Zext|x)] ≤ log Z^^	(22)
qPROP (x, zext)	ZPROP(x)	ZPROP (x)
V--------------------{z----------------------}
ELBO(x; qPROP ,pTGT)
EpTGT (zext |x)
PTGT (x, ZeXt)
qPROP (x, zext)
ZTGT (x)
θg ZPROP (X)
+ DKL[PTGT(Zext|x)kqPROP(Zext|x)] ≥ log
__________ - '
ZTGT (x)
ZPROP (x)
EUB O(x; qPROP , PTGT )
(23)
14
Published as a conference paper at ICLR 2022
Thus, the bounds in Eq. (21) directly generalize the standard ELBO (log p(x) - DKL [qθ (z|x)kp(z|x)])
and EUBO(logP(X) + DKL [p(z∣x)kqθ(z|x)]), which appear as special cases When K = 1, T = 1,
the proposal distribution is qpR°p(z∣x) = qθ(z|x), and the target distribution is PTGT(Z|x) = p(z∣x).
In what follows, our extended state space proposal or target distributions may include qθ(z|x) as
an initial or base variational distribution, with the posterior P(z|X) often appearing within target
distributions PTGT(X, zext).
In Fig. 4, we summarize various extended state space proposal (third column) and target distributions
(fourth column). We emphasize that the base variational distribution qθ(z|x) (blue circles) and
posterior distribution P(z|X) (red circles) may be used multiple times, in either or both of the
extended state space proposal and target distributions. Similarly, forward ais chains (blue circles)
starting from the initial distribution and the backward ais chains (shown in red circles) starting from
the posterior may be used repeatedly in both the proposal or target. In the next sections, we proceed
to derive each of the bounds in Fig. 4 as special cases of this general approach, thus interpreting each
importance sampling bound in terms of probabilistic inference in an extended state space.
B Importance Weighted Autoencoder (IWAE)
B.1 Probabilistic Interpretation and B ounds
Consider a K-sample proposal distribution qPIWROAPE(z(1:K)) using independent draws from an initial
distribution qθ(z|x). The target distribution is defined as a uniform mixture of K components, where
each component replaces the initial sample qθ(z(k) |X) in index k with a sample from the target
distribution P(z|X).
K
qPWAE (z(1:K)IX) = Y qθ(Z(S) |x),
s=1
KK
PTGAE(x,z(1：K)) = K X PgZ(Ss) Y qθ(Z(S)IX).
s=1	k=1
k6=S
(24)
Note that the normalizing constant of PITWGATE(x, Z(1:K)), or PITWGATE(x, Z(1:K))dZ(1:K), equals P(x)
since qθ(ZIx) is normalized and P(x, Z)dZ = P(x). To connect this with the IWAE bound, we show
that the log importance ratio reduces to
PTWAE(X,z(1:K))
qPWAE(z(1:K)IX)
log
1K	K
P P p(x, z(s)) Q qθ(z(k)∣X)
K S=1	k=1
k6=S
K
Q qθ (z(k) IX)
k=1
=I ɪ X P(X,z(k))
=og K k=1 qθ(z(k)|X).
(25)
As in Eq. (21), we can obtain lower and upper bounds on log P(X) by taking expectations under the
proposal and target distributions, respectively.
Alternative Probabilistic Interpretation We now present an alternative probabilistic interpreta-
tion of iwae, which is similar to Domke & Sheldon (2018) and will be used as the foundation for our
giwae bounds in App. C. Consider the following extended state space target distribution,
1K
PTWAE(x, z(1:K),s) = KP(X,z(S)) Y q(z(k)∣X).
k=1
k6=S
(26)
Note that marginalization over s leads to the IWAE mixture target in Eq. (4) or Eq. (24). We consider
the extended state space proposal
qPWAE(z(1：K),s|x)= (Y qθ(z(k)∣x))qPWAE(S|z(1：K), x) ,	(27)
k=1
15
Published as a conference paper at ICLR 2022
	Practicality	Extended State-Space Proposal Lower Bound on log p(x) (ELBO) Upper Bound on I(x; z)	Extended State-Space Target Upper Bound on log p(x) (EUBO) Lower Bound on I(x; z)
Simple IS	/	(D	―	④ 一
IWAE	/	Θ ©		®	
Generalized IWA.E	/	er		Θ ©MD ©	
Reverse IWAE	X	Θ © ©	Θ ©
Single-Sample AIS		ZOT Zl f ... ∙	¾-l	ZT	~~~... (¾)
Independent Multi-Sample AIS			Zf)A型户…X幽χ*) 甲	城K)	...	⅛ 姬
Independent Reverse Multi-Sample AIS	X		→(⅛h⅞)
Coupled Reverse Multi-Sample AIS			(⅞X0^∙∙∙ ^Sl (gκθ<-... →(⅛f
Figure 4: Comparison of the probabilistic extended state-space interpretations of different multi-
sample bounds. Forward chains of ais and variational distributions in is / iwae are colored in blue.
Backward chains in ais or posterior distributions in is / iwae are colored in red. Note that a lower
bound on log p(x), translates to an upper bound on MI, and vice versa.
where We have defined qpWWOE(SIZ(LK), X)= Px,z)sX)) IP Px(Z()kX)).
As desired, the log importance weight match Eq. (25)
log
pITWGATE(x, z(1:K), s)
qpWWAE(Z(LK),s|x)
1K
—p(x, z(s)) Q qθ(z(k)∣x)
K	k=1
k6=s
log----------------;—(Syr-
K	p(χ,z(S))
Q (Jz(k)∣χ)	qθ(z(S)Ix)
k=1qθ (Z JIx) p p(x,z(k))
k=ι qθ(z(k) Ix)
1	1 X
log KE
k=1
p(x,z(k))
qθ (z(k)|x)
(28)
16
Published as a conference paper at ICLR 2022
Lower Bound on log p(x) and Upper Bound on MI Using the general approach in App. A, taking
expectations under qPIWROAPE leads to a lower bound on log p(x)
ELB OIWAE (x; qθ , K) := E K
Q qθ (z(k) |x)
l 1 XX p(x,z(k))-
°g K k=ι qθ(z(k)|x)_
This corresponds to the following upper bound on MI for known p(x|z)
I(x;z) ≤ IIWAEU (qθ, K) = Ep(x,z) [log p(x|z)] - Ep(x) ELB OIWAE (x; qθ, K)
加 n r l γ∣ 加	l^ι1 5K P(x,z(k))
=Ep(χ,z) [logP(x∣z)] - E κ ,,、 log X / - 、
p(x) Q qθ(z(k)1x) _ K 匕 qθ(Zik)Ix)
(29)
(30)
(31)
Upper Bound on log p(x) and Lower Bound on MI Similarly, with expectations under
pITWGATE(z(1:K) |x), we obtain an upper bound on log p(x). Since, for independent draws, the uni-
form mixture pITWGATE is invariant to index permutations, we may choose the target sample to be z(1)
and obtain the upper bound of Sobolev & Vetrov (2019):
1 K p(x, z(k))
EUBOIWAe(x; qθ,κ) := E ,、 K , 、 log V7 V /	、 ≥ logp(x).	(32)
p(z(1) |x) Q qθ(z(k) |x)	K i 1 q(z(k) |x)
k=2	i=1
Translating this to a lower bound on MI with known p(x|z),
I(x; z) ≥ IIWAEL (qθ,K) = Ep(x,z) [log p(xIz)] - Ep(x) EUBOIWAE(x; qθ, K)	(33)
= Ep(x,z) [logP(x∣z)] E	κ	[log K X华卓1∙	(34)
p(x)p(z(1)∣x) Q qθ (z(k)∣x) L K 七 qθ (Z(k)|x)」
k=2	k=1
B.2 PROOF OF LOGARITHMIC IMPROVEMENT IN K FOR IWAE EUBO
We first recall results that IWAE bounds tighten with increasing K.
Proposition B.1. For given qθ (z|x), ELBOIWAE (x; qθ, K +1) ≥ ELBOIWAE (x; qθ, K) (Burda et al.,
2016) and EUBOIWAE (x; qθ, K +1) ≤ EUBOIWAE (x; qθ, K) (Sobolev & Vetrov, 2019).
Proposition B.2 (Improvement of IWAE and ELBO or EUB O). Let pITWGATE (s|x, z(1:K))
p(x,z(S)) / PK	p(x,z(k))
qθ(z(S)|x)/ k=1 = l qθ(z(k)∣x)
denote the normalized importance weights and U(s) indicate the
uniform distribution over K discrete values. Then, we can characterize the improvement of
ELBOIWAE (x; qθ, K) and EUBOIWAE (x; qθ, K) over ELBO(x; qθ) and EUBO(x; qθ) using KL diver-
gences, as follows
ELBOIWAE(x; qθ, K) = ELBO(x; qθ) + EqPIWRAOPE(z(1:K)|x) DKL[ U(s)kpITWGATE(s|z(1:K), x)] ,
(35)
|
}
''^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^
0 ≤ KL of uniform from SNIS weights ≤ DKL[qθ (ZIx)kp(ZIx)]
EUBOIWAE(x; qθ, K) = EUBO(x; qθ) - EpITWGATE(z(1:K)|x) DKL[pITWGATE(s|z(1:K), x) k U (s)]	. (36)
0 ≤ KL of SNIS weights from uniform ≤ log K
Prop. B.2 demonstrates that the improvement of the iwae log partition function bounds over its
single-sample counterparts is larger for more non-uniform snis weights. Notably, the improvement
of EUB OIWAE (x; qθ, K) over the single-sample EUBO(x; qθ) is limited by log K.
Proof. We first note that the single-sample ELBO(x; qθ) and EUBO(x; qθ) are special cases of
ELBOGIWAE (x; qθ, Tφ, K) and EUB OGIWAE (x; qθ, Tφ, K) (Lemma C.1) with Tφ = const. As a re-
sult, the gap between ELBO(x; qθ) and ELBOIWAE(x; qθ, K), for example, follows as a special case
of the gap between ELBOGIWAE (x; qθ, Tφ, K) and ELBOIWAE (x; qθ, K), which we characterize in
Lemma C.2 (App. C.4). The result in Prop. B.2 follows directly.
17
Published as a conference paper at ICLR 2022
We now justify the range of the kl divergences in Eq. (35) and Eq. (36) referenced in the underbraces.
Improvement of EUB OIWAE: Note DKL[pITWGATE(s|z(1:K), x)kU (s)] = log K - H(pITWGATE(s|z(1:K), x)) is
bounded above by log K since the entropy of a discrete random variable is nonnegative.
Thus, the improvement of EUB OIWAE (x; qθ, K) over EUBO(x; qθ) is limited to log K. We prove
similar results in Prop. 2.1 and Cor. 2.3 (App. C.4), Prop. E.2 (App. E.3) and Prop. H.2 (App. H.3).
Improvement of ELB OIWAE: On the other hand, the KL divergence DKL[U(s)kpTGGIWTAE(s|z(1:K), x)]
is not limited by log K. However, we do know that the improvement of ELBOIWAE (x; qθ , K) over
ELBO(x; qθ) will be limited by DKL [qθ(z∣x)∣∣p(z∣x)],the gap of ELBO(x; qθ).
□
Prop. B.2 shows that both IWAE upper and lower bounds on log p(x) improve upon their single-
sample counterparts, with the improvement of EUBOIWAE (x; qθ, K) over EUBO(x; qθ) limited by
log K. Cor. B.3 follows directly by translating these results to bounds on MI.
Corollary B.3. IWAE bounds on MI improve upon the BA bounds with the following relationships:
IBAL (qθ) ≤ IIWAEL (qθ, K) ≤ IBAL (qθ) + log K,	IIWAEU (qθ, K) ≤ IBAU (qθ).	(37)
Proof. Recall from Sec. 1.1 and Sec. 2.2 that IBAL (qθ) = Ep(x,z) [log p(x|z)] - Ep(x) [EUBO(x; qθ)],
and IIWAEL(qθ,K) = Ep(x,z) [logp(x|z)] - Ep(x)[EUBOIWAE(x; qθ,K)]. Using Prop. B.2, the fact
that EUBO(x; qθ) - logK ≤ EUBOIWAE (x; qθ, K) for any x implies that
IIWAEL (qθ , K) - IBAL (qθ) = Ep(x) [EUBO(x; qθ) - EUBOIWAE(x; qθ, K)] ≤ log K, (38)
which results in IIWAEL (qθ, K) ≤ IBAL (qθ) + log K, as desired.
IBAL (qθ) ≤ IIWAEL (qθ, K) and IIWAEU 3, K) ≤ IBAU 3) follow from the fact that IWAE bounds
tighten with increasing K in Prop. B.1.	□
B.3 EXPERIMENTAL RESULTS SHOWING LOGARITHMIC IMPROVEMENT FOR IIWAEL (qθ, K)
In Sec. 2.4, we showed that iwae is a special case of giwae, which decomposes into the sum of
a variational BA lower bound and a K-sample contrastive term. This suggests that the IWAE lower
bound on MI, which arises from an upper bound on log p(x), may be written as
IIWAEL (qθ , K )
Ep(x,z)
log *]+ E	K
p(z)	p(x)p(z(1) |x) Q qθ (z(k) |x)
k=2
log
P(x,z ⑴)
qθ (z(1) ∣χ)
1 PK	p(x,z(k))
K i=1=θ qθ (Z(k) |x)
(39)
|
IBA (q)
0≤ contrastive term ≤log K
This way of writing IIWAEL (qθ, K) provides additional intuition for the result in Prop. B.2 and
Cor. B.3. In particular, the improvement of IIWAEL (qθ, K) over IBAL (qθ) is simply the contrastive
term, which is limited to log K .
Method I	logK	I Proposal	I	VAE2		VAE10		VAE100		I	GAN2		GAN10		GAN100	
IWAE I (K=I)	0	I P(Z)	I	0 + 0 =	0	0+0=	0	0 + 0 =	0	I	0 + 0 =	0	0+0=	0	0+0=	0
		I q(z|x)	I 8.63 + 0 =	8.63	25.20+0=	25.20	44.54+0=	44.54	8.83 + 0 =	8.83	4.23+0=	4.23	3.23+0=	3.23
IWAE I (K=1K),	6.91	I P(Z)	I 0 + 6.81 =	6.81	0+6.91 =	6.91	0 + 6.91 =	6.91	I 0 + 6.88 =	6.88	0+6.91 =	6.91	0+6.91=	6.91
		I q(z|x)	I 7.29 + 1.8 =	9.09	25.20 + 6.49	= 31.69	44.54 + 6.90	=51.44	I 7.82 + 2.92 =	10.74	4.23 + 6.91	11.14	3.23 + 6.91	= 10.14
IWAE I (K=1M),	13.82	I P(Z)	I 0 + 9.09 =	9.09	0+ 13.82 =	13.82	-0 + 13.82 =	13.82	I 0 + 10.76 =	10.76	0+ 13.81 =	13.81	0+13.82=	13.82
		I q(z|x)	I 3.78 + 5.31	= 9.09	25.20 + 8.90	= 34.10	44.54 + 13.81	= 58.35	I 6.02 + 4.79 =	10.81	4.23 + 13.53	= 17.76	3.17 + 13.81	= 16.98
Table 2: Decomposition of IIWAEL (qθ, K) into BA term and contrastive term (< log K) on MI
estimation for vae and gan models trained on mnist.
Table 2 and Table 3 show the iwae objective decomposition to the ba term and the contrastive term,
on vaes and gans trained mnist and cifar-10 dataset. We can see that in all the experiments, the
contribution of the contrastive term is always less than or equal to log K.
For mnist vaes and gans with two dimensional latent spaces, the contrastive term may contribute
notably less than log K. In these cases, even the optimal critic function T* (x, Z) = log Px,X)+ c(x),
as used in iwae, has difficulty distinguishing posterior and variational samples.
18
Published as a conference paper at ICLR 2022
Method	I log K	Proposal	GAN5		GAN10		GAN100	
IWAE K=1	I 0	I P(Z) I	0+0=	0	0+0=	0	0+0=	0
		I q(z|X) I	14.53+0=	14.53	~17.45 + 0 =	17.45	20.00 + 0 =	20.00
IWAE K=1k	I 6.91	I P(Z) I	0 + 6.91 =	6.91	0 + 6.91 =	6.91	0+6.91 =	6.91
		I q(ZIX) I	14.53 + 6.90	=21.43	16.68 + 6.9	23.58	20.07 + 6.91	= 26.98
IWAE K=1M	I 13.82	I P(Z) I	0 + 13.82 =	13.82	0 + 13.82 =	13.82	0+ 13.82 =	13.82
		I q(ZIX) I	14.53 + 13.81	=28.34	16.92 + 13.81	= 30.73	20.00 + 13.81	= 33.81
Table 3: Decomposition of IIWAEL (qθ, K) into BA term and contrastive term (< log K) on MI
estimation for gan models trained on cifar-10.
However, the contribution of the contrastive term is almost exactly log K for higher dimensional VAE
and GAN models, where the posterior p(z|x) is more complex and is more easily distinguishable
from the variational qθ(z∣x). These results highlight the inherent exponential sample complexity of
the iwae lower bound on mi.
B.4	BIAS REDUCTION IN K FOR IWAE LOWER BOUND ON log p(x) / UPPER BOUND ON MI
We have seen in App. B.2 Prop. B.2 that the improvement of EUBOIWAE(x; qθ, K) and IIWAEL (x; qθ, K)
over the single-sample EUBO(x; qθ) and IBAL (x; qθ) is limited by logK. This suggests that expo-
nential sample complexity in the gap of the EUBO, K α exp{DκL [p(z∣x)kqθ (z∣x)]}, is required to
obtain a tight lower bound.
The quantity DKL [U(s)kpITWGATE(s|z(1:K), x)], which measures the improvement of the IWAE lower
bound on log p(x) over the ELBO, is not explicitly limited by log K. However, Chatterjee et al. (2018)
suggest that the same exponential sample complexity, K α exp{DKL [p(z∣x)kqθ(z∣x)]}, is required
for accurate importance sampling estimation with proposal q§(z|x). Maddison et al. (2017); Domke &
Sheldon (2018) find that the bias of the IWAE lower bound in the limit of K → ∞ reduces at the rate
of O(2KVar[qp((Zx))]), although the Var["x,X)] term is at least exponential in DKL [p(z∣x)kqθ(z|x)]
(Song & Ermon, 2019).
This exponential sample complexity for exact estimation of log p(x) or I(x, z) is usually impractical
for complex target distributions and limited variational families, where DKL [p(z∣x)kqθ(z|x)] may
be large. This motivates our improved, multi-sample ais proposals in Sec. 3.2, which achieve more
favorable (linear) bias reduction by introducing MCMC transition kernels to bridge between qθ (z|x)
and p(z|x).
B.5	Relationship with Structured InfoNCE
We can recognize the Structured INFONCE upper and lower bounds for known p(x|z) (Poole et al.
(2019) Sec. 2.5) as simply applying the standard IWAE bounds, using the marginal p(z) in place of
the variational qθ(z|x)
E	κ	" log —P(X|Z())——# ≤ I(x; Z) ≤ E κ	" log-----1-----# - H(x∣z).
p(x)p(z"x) k=2 p(z(k))[	K P p(χ∣z(k)j -	- P(X) k=ιP(z(k))[	K P p(χ∣z"
k=1	k=1
We refer to the lower bound as IS-INFONCEL (K) and the upper bound as IS-INFONCEU (K). From
Cor. B.3, we obtain an alternative proof that the Structured InfoNCE lower bound is upper bounded
by log K. Since IBAL (p(z)) ≤ IIWAEL (p(z), K) ≤ IBAL (p(z)) + log K and the BA bound with a
prior proposal equals 0 from Eq. (3), we have that 0 ≤ IS-INFONCEL (K) ≤ log K.
C Generalized IWAE
C.1 Probabilistic Interpretation and B ounds
To derive a probabilistic interpretation for giwae, our starting point is to further extend the state
space of the IWAE target distribution in Eq. (4), using a uniform index variable p(s) = K ∀s that
specifies which sample z(k) is drawn from the posterior p(z|x). This is shown in Fig. 4, and leads to
19
Published as a conference paper at ICLR 2022
a joint distribution over (x, z(1:K) , s) as
1K
PGGWAE(x,z(1:K),s) = kp(x,Z(S)) Y q(z(k)∣χ),	(40)
k=1
k6=s
with marginalization over s leading to the mixture in Eq. (4). The posterior over the index variable s,
which infers the ‘positive’ sample drawn from p(z|x) given a set of samples z(1:K), corresponds to
the normalized importance weights
P(x,z(S))
PGGWAE(s|x,z(1:K)) = PKqθ(zp;Xxz)(k)) ,	(41)
k=1=θ qθ(z(k) |x)
which can be derived from PGGWAE (S|x,z(LK)) = Plz(xχzz(ιKK)S) = HPKxzsjQ qφ(z;ZSIX).
PTGT lx/' ') κ j=ι=p. p(x,z ')ιik=j qφ(z' 1 |x)
For the giwae extended state space proposal distribution, we consider a categorical index variable
qPGRIWOPAE(S|z(1:K), x) drawn according SNIS, with weights calculated using the critic function Tφ.
qPGRIWOPAE(z(1:K), s|x) = Y qθ (z(k) |x) qPGRIWOPAE(s|z(1:K),x),	(42)
eTφ(x,z(s))
where qGWAE(s|z(	), X)二次---------------.	(43)
P eTφ(x,z(k))
k=1
Similarly to Lawson et al. (2019), the variational SNIS distribution qPGRIWOPAE(S|z(1:K), x) is approxi-
mating the true SNIS distribution PTGGIWTAE(S|x, z(1:K)). As we show in App. C.4, the optimal critic
function is Tφ(x, z) = log p(χ,x) + c(x), which recovers the IWAE probabilistic interpretation (see
App. B.1).
Log Importance Ratio To derive bounds on the log partition function, we first calculate the log
unnormalized density ratio
K
KP(x,z(S)) Q q(z(k)|x)
log PGGWAE(Z(LK) ,s, x) = log________掺____________
溜OPE(Z(LK),s1x)	q(s|z(1：K), x) Q q(z(k)∣x)
k=1
l 1 Pk eT(x,z(k)) p(x,z(S))
og K eτ(x,z(s)) q(z(S)IX)
log *∖ )) — T(x, z(S)) +logɪ X eT(x,z(k))
q(z(S) |x)	K
(44)
(45)
(46)
Taking the expectation of the log unnormalized density ratio under the proposal or target distribution
yields a lower or upper bound, respectively, on log P(x)
PTGGIWTAE(z(1:K), S, x)	PTGGI WTA E (z(1:K), S, x)
成WAE [og qGWAE(z(1：K),s|x)_| ≤ ogP(X) ≤ PGGWAE [og qGWAE(z(1：K),s|x)_| .	( 7)
、----------------{z--------------}	、-----------------V---------------}
ELB OGIWAE (x; qθ , Tφ , K)	EUBOGIWAE (x; qθ , Tφ , K)
As in Sec. 2.1 and App. A, the gap in the lower and upper bounds can be derived as kl divergences
in the extended state space.
20
Published as a conference paper at ICLR 2022
Upper Bound on log p(x) and Lower Bound on MI To derive an explicit form for the GIWAE
upper bound on log p(x), we write
EUBOGIWAE(qθ, Tφ, K) = EpTGGIWTAE(z(1:K),s|x)
1 p(x,z(S))
og q(z(S)IX)
K
-T(x,z(S))+log KK XeT(X,z(k))
k=1
p(x, z)
Ep(ZIx) [log q(z∣χJ - Ep(Za)1x) fc∏ q(z(k)∣x)
eT(x,z(1))
log -----------------
表 P eT(x,z(k))
K k=1
(48)
where, in the first term of the second line, we note that PGGWAE (Z(LK), s∣x) specifies that Z(S) 〜p(z∣x).
Since S 〜p(s) = K is sampled uniformly, We can assume S = 1 in the second term due to
permutation invariance.
Translating this into a lower bound on MI, we consider I(x; z) = -Ep(x) [log p(x)] - H(x∣z) ≥
-Ep(x) [EUB OGIWAE (qθ, Tφ, K )] - H(x∣z). Writing the conditional entropy term over the index S,
I(x; z) ≥ Ep(x,Z) [logp(x∣z)]
-(Ep(X)P(Z1x) _log P(Xx)
(49)
eTφ(x,Z(1))
-E	K	log-----K-------------)
P(X)P(Za)1x)k∏2q(z(k)1x) L	ι P eTφ(x,z(k))J/
K k=1
q(z∣x)	e φ(x,Z )
Ep(x z) log --- + E	K	log  ---T7--------——
,l_	P(Z)J	p(x)p(z⑴1x)Rqe(z(k)1x)	K Pi=ι eTφ(x,z( ))
which matches Eq. (7) from the main text.
C.2 GIWAE Upper B ound on MI Does Not Provide B enefit Over IWAE
To derive an explicit form for the GIWAE lower bound on log P(x),
ELBOGIWAE (x; qθ, Tφ, K) = EqPGRIWOPAE(z(1:K) ,s|x)
叫 qχ,z≡
K
-Tφ(x, z(s))+log⅛ X e43)
K k=1
(51)
Translating this to an upper bound on mi yields
K	eTφ(x,z(s))	(x z(s))
I(X;Z) ≤ Ep(Z(Mlog P(XIZ(S))] -(E K q(z(k)J X	log ⅛⅛)1	(52)
k=1	e
k=1
eT(x,z(s))
- EqGRWAE(z(1:K),s|x) log -K	)
L	1. P eT(x,z(k))-∣×
K k=1
Thus, in Eq. (52), knowledge of the full joint density is required to evaluate both the conditional
entropy and log p(x,z)(∣x) terms. If both p(z) and p(x∣z) are known, then we will show in Cor. 2.2
below that the optimal critic or negative energy function in giwae yields the true importance weights,
and the resulting MI or log P(x) bounds matches the IWAE bounds.
We thus conclude that the GIWAE upper bound on MI and lower bound on log P(x) does not provide
any benefit over IWAE in practice. However, ELBOGIWAE(x; qθ, Tφ, K ) is still useful for analysis,
as our proof of Lemma C.1 below allows us characterize the gap between ELBOIWAE (x; qθ, K ) and
ELBO(x; qθ) in Prop. B.2.
21
Published as a conference paper at ICLR 2022
C.3 ELBO and EUBO are Special Cases of GIWAE Log Partition Function Bounds
Lemma C.1. The single-sample ELBO and EUBO are special cases of GIWAE, with
ELB O(x; qθ) = ELBOGIWAE (x; qθ , Tφ0 = const, K),
EUBO(x; qθ) = EUBOGIWAE(x; qθ, Tφ0 = const, K).
In both cases, the SNIS sampling distribution (Eq. (43)) is uniform qGWAE(z(1:K), s|x)=表=U(S).
Proof. We consider the GIWAE probabilistic interpretation (App. C.1) for Tφ0 = const. We refer to
this extended state space proposal as qPBRAOP, since it leads to IBAL (qθ) and IBAU (qθ) bounds on MI.
qBAOP(Z(LK),s|X) = ( Y qθ(Z(k)|X))qBAOP(S|z(1:K), X),
(53)
where qPBRAOP(s|z(1:K), x)
k=1
eTφ0(x,z(s))
PK=I eTφ0 (Xmk))
U (S) =五.
K
(54)
Note that the SNIS sampling distribution qBAOP(s∣Z(LK), x) will be uniform, which matches P(S) = K
in the giwae extended state space target distribution
1K
PGGWAE(z(1：K), s|x) =	P(Z(S)|x) Y qθ(z(k)|x)
K
(55)
k=1
k6=s
Now, taking the log unnormalized importance weights, we obtain
log
pTGGIWTAE(Z(1:K ),s|X)
log
K
KP(Z(S)IX) Q qθ(Z(A)IX)
k=1
k6=s
K
κK Q qθ(Z(k)|X)
k=1
log 等(S
(56)
Taking expectations with respect to qPBRAOP(z(1:K), S|x) or PTGGIWTAE(z(1:K), S|x) leads to ELBO(x; qθ)
and eubo(x; qθ) respectively, as in Sec. 2-2.2.	口
C.4 Proof of Relationship between IWAE and GIWAE Probabilistic
Interpretations (Prop. 2.1)
We first prove a lemma which relates both the GIWAE lower and upper bounds on log P(x) to the
respective iwae bounds. From this lemma, Prop. B.2 follows directly and relates the elbo and eubo
(which are special cases of elb oGIWAE and euboGIWAE) to elb oIWAE and eub oIWAE.
Lemma C.2. We can characterize the difference between IWAE and GIWAE bounds on log P(x) using
KL divergences between their respective SNIS distributions.
ELBOIWAE (x; qθ, K) = ELBOGIWAE (x; qθ, Tφ, K) + EqGIWAE(z(1:K) |x) DKL qPGRIWOPAE(s|z(1:K), x)pITWGATE(s|z(1:K),x)	(57)
EUBOIWAE (x; qθ, K) = EUBOGIWAE (x; qθ, Tφ, K) - EpIWAE(z(1:K) |x) DKL pITWGATE(s|z(1:K), x) qPGRIWOPAE(s|z(1:K) , x)	(58)
Proof. Recall from Sec. 2.1 or App. A that the gap of the lower bound ELBOGIWAE (x; qθ, Tφ, K)
is DKL[qPGRIWOPAE(Z(1:K), S|x)kPTGGIWTAE(Z(1:K), S, x)], while the gap of the upper bound
EUBOGIWAE(x; qθ, Tφ, K) is DKL[PTGGIWTAE(Z(1:K), S|x)kqPGRIWOPAE(Z(1:K), S|x)]. We will expand
these kl divergences to reveal the relationship between the giwae bounds and iwae bounds.
First, recall from Eq. (41) that the posterior over the index variable S, or target SNIS distribution, is
GIWAE(Z(1：K) S x)	K1 p(χ,z (S))QJ qθ (Z(k) IX)	Pxs^
PGIWAE(s|z(1:K) χ) = PTGT (Z	,s, X) =	k = s	= q(z(s) |x)
T0T	, Σ PGGWAE(Z(1：K),S, X)	K Pj(X,Z(s)) QK=1 qθ (Z(k)∣X)	g⅛sg
22
Published as a conference paper at ICLR 2022
Thejoint distribution then factorizes asPGGWAE(z(1:K), s, x) = PGGWAE(z(1:K), x) ∙PGGWAE(s|z(1:K), x).
ELBO Case: Using this factorization of pTGGIWTAE(z(1:K), s, x), we can rewrite the gap of
ELB OGIWAE (x; qθ, Tφ, K) as follows
DKL[qPGRIWOPAE(z(1:K), s|x)kpTGGIWTAE(z(1:K), s|x)]	(59)
=E	", " qGRWAE(Z(LK) 1x) qGRWAE(s1z(1：K), x) #
qGRWAE(Z(LK) ,s|X) [ g PGGWAE(Z(LK) ∣χ) PGGWAE(s|z(1:K), χ)J
K	1 K	K
=Dkl Y qθ(z(k)lx)∣∣ κ XP(Z(S)Ix) Y qθ(z(k)lx) + EqGRWAE [DKL[qGRWAE(S1z(1:K),x)kpGGWAE(s1z(1:K),x)]]
k=1	K s=1	k=1
k6=s
= DKL qPIWROAPE(z(1:K)|x)PITWGATE(z(1:K)|x) + EqPGRIWOPAE hDKL[qPGRIWOPAE(s|z(1:K), x)kPTGGIWTAE(s|z(1:K), x)]i ,	(60)
where we can recognize the first term as the gap in ELBOIWAE (x; qθ, K). Noting
that ELBOIWAE(x; qθ, K) - ELBOGIWAE(x; qθ, Tφ, K) = DKL[qPGRIWOPAE(z(1:K), s|x)kpTGGIWTAE(z(1:K), s|x)] -
DKL[qPIWROAPE(z(1:K) |x)kpITWGATE(z(1:K)|x)] , we obtain Eq. (57), as desired
ELBOIWAE(x; qθ, K) = ELBOGIWAE(x; qθ, Tφ, K) + EqPGRIWOPAE(z(1:K)|x) DKL[qPGRIWOPAE(s|z(1:K), x)kPTGGIWTAE(s|z(1:K), x)] .
EUBO Case: For ELBOGIWAE (x; qθ, Tφ, K), the derivations follow in a similar fashion to Eq. (59)-
Eq. (60), but using the reverse KL divergence and expectations under PGGWAE(z(1:K), s|x).	口
Translating Lemma C.2 to a statement relating iwae and giwae bounds on mi, we obtain the
following proposition.
Proposition 2.1 (Improvement of IWAE over giwae). For a given qθ(z|x) and any Tφ(x, Z),
IIWAEL (qθ, K) = IGIWAEL (qθ, Tφ, K) + Ep(x)pTGGIWTAE(z(1:K) |x) hDKL[pTGGIWTAE(s|z(1:K), x)kqPGRIWOPAE(s|z(1:K), x)]i
Proof. The result follows directly from Lemma C.2. First, note that PITWGATE(x, z(1:K), s) =
PTGGIWTAE(x, z(1:K), s). Then, using the upper bounds on log P(x) and taking outer expectations with
respect to P(x), we have
IIWAEL (qθ , K) - IGIWAEL (qθ , Tφ, K) = -H (x|z) - Ep(x) [EUBOIWAE(x; qθ , K)]	(61)
+ H (x|z) + Ep(x) [EUB OGIWAE (x; qθ, Tφ, K)]
=Ep(x)pITWGATE(z(1:K) |x) hDKL[pITWGATE(s|z(1:K), x) k qPGRIWOPAE(s|z(1:K), x)]i .
(62)
□
C.5 Proof of GIWAE Optimal Critic Function and Logarithmic Improvement
(COR. 2.2 AND COR. 2.3)
We now prove Cor. 2.2 and Cor. 2.3 from the main text, with a corollary stating the results for the
special case of InfoNCE in App. C.6.
Corollary 2.2. For a given qθ (z|x) and K > 1, the optimal critic function is the true log importance
weight UP to an arbitrary constant: T*(x, Z) = log pχ,∣Z)) + C(X). With this choice of T*(x, Z),
IGIWAEL (qθ,T*, K) = IIWAEL (lθ, K) .	(8)
Proof. Using Prop. 2.1, we can see that the gap in the GIWAE and IWAE bounds, which corresponds
to the posterior KL divergence over the index variable s, will equal zero iff
eT (x,z(s))
qGRWAE(SIZ(LK), X)=PGGWAE(SIZ(LK), x) =⇒ -K-----------------
P eT(x,z(k))
k=1
P(x,z(S))
qe (Z(S) |x)
~K~~一,^^∑Γ
P p(x,z(S))
J qe(z(S)Ix)
s=1
(63)
23
Published as a conference paper at ICLR 2022
This condition also ensures that the overall GIWAE proposal pTGGIWTAE(z(1:K), s|x) (Eq. (40)) and target
qPGRIWOPAE(z(1:K), s|x) (Eq. (42)) distributions match. We will show that any T(x, z) which satisfies
Eq. (63) has the form
T*(x, z) = log P(X：： + c(x).	(64)
qθ (z∣χ)
Let f (x, Z) = log 牖|?)+ g(x, z), which represents an arbitrary choice of critic function. We will
show that g(x, z) must be constant with respect to z
log p(x,z(s)) +g(x,z(s))	p(x,z(S))
e	q (Z(S)M__________ _	qθ (z(s)∣x)
P Jog 意瑞⅛+g …=P^⅛k)ι
e	qθ (z(k) ∣x)
k=1	k=1
log τθ⅛z⅛⅛ ∙ eg(χ,Z(S)) ∙ X P(X，z(k)) = P(x，z(S)) ∙ X Jog qθ⅛z‰ +g(χ,z(k))
k=1 qθ(z(k)|x)	qθ(Z(S)IX) k=l
XX P(X,z(k)) P(X,z(S)) eg(χ,z(s)) = XX P(X,z(k)) P(X,z(S)) eg(χ,z(k))
k=11 qθ(z(k)∣X) qθ(Z(S) |x)	k=1 qθ(z(k)∣X) q©(Z(S) |x)
=⇒ g(X, z) = c(X)
where g(X, z) = c(X) is required in order to ensure that g(X, z(S)) = g(X, z(k)) for arbitrary choices
of z samples.
This form for the optimal critic function T* (x, z) in Eq. (64) implies that learning Tφ(x, z) in GIWAE
becomes unnecessary when the density ratio qp(X,∣?) is available in closed form, as is assumed in the
iwae bound.
For this choice of T* (X, z), the value of the GIWAE objective matches the IWAE lower bound on MI.
IGIWAE (qθ ,T *,K) = Ep(x,z) log qpZ(ZXχ)
+E	K
p(x,z(1)) Q qθ (z(k) |x)
k=2
p(x,Z(1))
elog q(zwτ ∙ ec(χ)
K log p(x,z(k))
K P e	Qθ(z(k |x) ∙ ec(x)
k=1
E I^1	Jq(Z1灯	1	p(x, z) ^∣	1 Ky p(x, z(k))
Ep(X Z) log+ log〉I I — E	κ	log 一 ∖	', ’,、|、
,[g P(Z)	Sq(Zwd	p(χ,z(1)) Q qθ(z(k)ix)	K £i qθ(Zk)IX)
k=2	i=1
(65)
—H (x|Z) +	—E	K
∖	p(x)p(Z(S)M kQ2 qθ(z(k)|x)
log K X ⅛O
= IIWAEL (qθ , K).
The second term in Eq. (65) is exactly the negative of the IWAE upper bound on log P(X) in Eq. (34).
Combined with the entropy -H(X|z), we obtain the IIWAEL (qθ, K) lower bound on MI as desired.
口
Corollary 2.3. Suppose the critic function Tφ(X, z) is parameterized by φ, and
∃ φo s.t. ∀ (x, z), Tφo (x, z) = const. For a given q©(z∣x), let Tφ*(x, z) denote the critic
function that maximizes the GIWAE lower bound. Using Cor. 2.2, we have
IBAL (qθ) ≤ IGIWAEL (qθ, Tφ* , K) ≤ IIWAEL (q, K) ≤ IBAL (qθ) + log K .	(9)
Proof. We begin by showing that IBAL (q©) ≤ IGIWAEL (q©, Tφ* , K). This follows from the assump-
tion that there exists φ0 in the parameter space of the neural network Tφ such that Tφ0 = const. With
this φ0, we would have IBAL (q©) = IGIWAEL (q©, Tφ0 , K) as in Lemma C.1. Thus, the optimal φ* in
the parameter space can only improve upon the ba bound.
Next, for any given φ (including φ*), we have IGIWAEL (q©, Tφ, K) ≤ IIWAEL (q©, K), since IWAE
uses the (unconstrained) optimal critic function T* = log qp(Xz)) + c(x) (Cor. 2.2). The final
inequality follows from Prop. B.2, which shows that IIWAEL (q©, K) improves by at most log K over
IBAL (qθ). These relationships are visualized in Fig. 3b.	□
24
Published as a conference paper at ICLR 2022
C.6 Properties of InfoNCE
Corollary C.3. Using the prior qθ(z∣x) = p(z) as in InfoNCE,
(a)	For the optimal critic function, Cor 2.2 implies T*(x, Z) = logp(x∣z) + C(X), and
IGIWAEL (p(z), T*, K) = IinfoNCEl (T*, K) = IS-INFONCEL (K)∙	(66)
(b)	For an arbitrary critic function Tφ(X, z), Cor. 2.3 translates to
0 ≤ IINFoNCEL (Tφ, K) ≤ IS-INFoNCEL (K) ≤ log K.	(67)
For InfoNCE, note that using the prior as the proposal does allow the critic to admit an efficient
bi-linear implementation Tφ(X, z) = fφx (X)T fφz (z), which requires only N + K forward passes
instead of NK for GIWAE, where N is the batch size and K is the total number of positive and
negative samples.
D Single-Sample AIS
D. 1 PRooF oF PRoP. 3.1 (CoMPLEXITY IN T FoR SINGLE-SAMPLE AIS)
In this section, we prove Prop. 3.1, which relates the sum of the gaps in the single-sample ais upper
and lower bounds to the symmetrized kl divergence between the endpoint distributions. We extend
this result to im-ais in Cor. E.1 and cr-ais in Cor. H.3.
Proposition 3.1 (Complexity in T). Assuming perfect transitions and a geometric annealing path
with linearly-spaced {βt}tT=1, the sum of the gaps in the AIS sandwich bounds on MI, IAISU (π0, T) -
IAISL (π0, T), reduces linearly with increasing T.
In particular, we will show that
EUBOais (x; ∏0,T) - ELBOAIS(X； ∏0,T) ≤ " (Dkl[∏t(z∣x) k∏θ(z)] + DKL [∏o(z)k∏τ (z|x)]	(68)
where the left hand side also corresponds to the sum of the gaps DKL[qPARISoP(z0:T |X)kpTAGIST(z0:T |X)] +
DKL[pTAGIST(z0:T |X)kqPARISOP(z0:T |X)]. This result translates to mutual information bounds as in Sec. 1.1.
In contrast to Grosse et al. (2013) Thm. 1, our linear bias reduction result holds for finite T.
Proof. For linear scheduling and perfect transitions, we simplify the difference in the single-sample
upper and lower bounds as
δLT,K=1 + δUT,K=1 = EUBOAIS (x； π0 , T) - ELBOAIS (x； π0 , T)
Ez0:T ~pAGT
PAGT(X, z0:T) _ E	I PAGT(X, z0:T)
qAROP(Z0:T∖x)i	z0：T~qPROP [ θg qAROP(Z0:T|x)
Ez0:T ~pAGT
Ez0:T ~pAGT
Ez0:T ~pAGT
log
T
/ ɪ ∖ τ r z-7- /	ɪ ∖
πT (ZT ∖X)	Tt(Zt-1∖Zt)
___________t = 1___________
T
π0(Z0∖X) Q Tt(Zt∖Zt-1)
t=1
∏t (x, Zt) )βt-βt-1
∏o(zt∖x) J
T
logY
t=1
T
t=1
∏t (x, Zt)
∏o(zt∖x)
-Ezo：T~qAROP
T
log
T
/ ɪ ∖ τ r z-7- /	ɪ ∖
πT (ZT ∖X)	Tt(Zt-1∖Zt)
___________t = 1___________
T
π0(Z0∖X) Q Tt(Zt∖Zt-1)
t=1
T
logY
t=1
∏t (x, Zt)
∏o(zt∖x)
t=1
βt -βt-1
∏t (x, Zt)
∏o(Zt∖x).
(69)
TT
=) X E∏βt (z) [(βt - βt-1)lOg ⅛⅛] - X Enet-I (z) [(βt - βt-l)lOg 1⅛⅛
(2) 1 而 Γ1	∏t (x, z)]	1 .
=TEnT(Z)卜g	] - TEnO(Z)
log
∏t (x, Z)
∏o(z∖x)
T (DKL[∏ok∏T] + Dkl[∏t ∣∣∏o]),
25
Published as a conference paper at ICLR 2022
where in (2), we use the linear annealing schedule βt - βt-ι = T ∀t and note that intermediate
terms cancel in telescoping fashion. In (1), we have used the assumption of perfect transitions (PT),
which is common in analysis of ais (Neal, 2001; Grosse et al., 2013). In this case, the ais proposal
and target distributions have the following factorial form
T
zo：T 〜qAROP(Z01TK)lχ) (=) πo(ZO) Yπβ-ι (Zt),	(7O)
t=1
T
Z0:T -PAGST(z01TK)∣x) (=T) ∏t(zt) Y ∏βt-ι (Zt-1).	(71)
t=1
In other words, for 1 ≤ t ≤ T , perfect transitions results in independent, exact samples from
Zt 〜∏βt-ι (z) in the forward direction, and Zt 〜∏βt (z) in the reverse direction. Using the factorized
structure of Eq. (70) and Eq. (71), the expectations over the extended state space simplify to a sum of
expectations at each Zt.
The above proves the proposition for the case of single sample ais, but should also hold for our
tighter multi-sample ais bounds. We extend this result to Independent Multi-Sample ais in App. E.2
below, and extend the result to Coupled Reverse Multi-Sample AIS in App. H.4.	□
E Independent Multi-Sample ais
E.1 Probabilistic Interpretation and Bounds
Our Independent Multi-Sample ais (im-ais) are identical to the standard iwae bounds in App. B,
but using AIS forward qpARISOp(Z0:T |x) and backward PTAGIST(Z0:T|x) chains of length T instead of the
endpoint distributions only qθ(z∣x) andp(z∣x).
In particular, we construct an extended state space proposal by running K independent AIS forward
chains z0kT 〜qAROp in parallel. AS in the IWAE upper bound (Eq. (5)), the extended state space
target involves selecting a single index s uniformly at random, and running a backward AIS chain
ZOsT 〜PAGT starting from a true posterior sample ZT 〜p(z∣x). The remaining K — 1 samples are
obtained by running forward ais chains, as visualized in Fig. 2
K
IM-AIS (z(1:K) |x) := Y AIS (z(k) |x)	IM-AIS(x z(1:K)) :
qPROP (z0:T |x) :=	qPROP (z0:T |x),	pTGT (x, z0:T ) :
k=1
1K	K
K XPAGST(X,z0sT) Y QAROp(ZOkTIx).
s=1
k=1
k6=s
where qPARISOP and pTAGIST were defined in Eq. (12). Expanding the log unnormalized density ratio,
IM-AIS (	(1:K))
log PTGT (x,z0:T ) = log
lθg 户-AIS(Z(1:K)IX) lθg
qPROP (Z0:T |x)
KK
K P PAGST(X,z0:T) Q qARS>P(Z0：T|x)
s=1	k=1
k6=s
1K
= log K X
k=1
K
Q qPARISOP(Z0:T |x)
k=1
PAGT (XlzkT)
qAROp(z0kT ∣χ)
(72)
(73)
which is similar to the iwae ratio but involves ais chains. Taking the expectation under the
proposal and target as in App. A recovers the lower and upper bounds in Eq. (14). The gap in
the lower bound is DKL [qPIMRO-APIS (Z(01:T:K) |x)kpITMG-TAIS (Z(01:T:K) |x))] and the gap in the upper bound is
DKL[pITMG-TAIS(Z(01:T:K)|x))kqPIMRO-APIS(Z(01:T:K)|x)].
E.2 pROOF OF LINEAR BIAS REDUCTION IN T FOR IM-AIS
Corollary E.1 (Complexity in T for Independent Multi-Sample AIS Bound). Assuming perfect
transitions and a geometric annealing path with linearly-spaced {βt}tT=O, the sum of the gaps in the
Independent Multi-Sample AIS sandwich bounds on MI, IIM-AISU (πO, T) - IIM-AISL (πO, T), reduces
linearly with increasing T.
26
Published as a conference paper at ICLR 2022
Proof. Using identical proof techniques as for IWAE (Burda et al. (2016); Sobolev & Vetrov (2019),
Prop. B.1), we can show that our Independent Multi-Sample AIS bounds ELBOIM-AIS (x; π0, T, K -
1) ≤ ELB OIM-AIS (x; π0, T, K) and EUB OIM-AIS (x; π0, T, K) ≤ EUBOIM-AIS (x; π0, T, K - 1) im-
prove with increasing K. Thus, the bias of our multi-sample bounds is less than the bias of the
single-sample bounds, so the inequality in Eq. (68) and linear bias reduction in Prop. 3.1 also hold
for Independent Multi-Sample AIS. We characterize this improvement in Prop. E.2 below. 口
E.3 Proof of Logarithmic Improvement of IM-AIS EUBO
Proposition E.2 (Improvement of Independent Multi-Sample AIS over Single-Sample AIS).
Let PTM-AIS (s|x, z(1τK)) = PGT(X(Z0:T) / PK1 PTGT(X(Z):T) denote the normalized importance
TGT	0:T	AIS (s)	k=1 AIS (k)
qPROP(Z0:T |X)	qPROP(Z0:T |X)
weights over AIS chains, and let U(s) indicate the uniform distribution over K discrete val-
ues. Then, we can characterize the improvement of the Independent Multi-Sample AIS bounds
on log p(x), ELBOIM-AIS (x; π0, T, K) and EUB OIM-AIS (x; π0, T, K), over the single-sample AIS
bounds ELBOAIS (x; π0, T) and EUBOAIS (x; π0, T) using KL divergences, as follows
ELBOIM-AIS (x; π0, T, K) = ELBOAIS (x; π0, T) + E IM-AIS (1:K)
qPROP (z0:T |x)
DKL U(s)pITMG-TAIS(s|z(01:T:K), x)	,

~^^^^^^^^^^^^{^^^^^^^^^^^^~^^^^^^^^^^^^}
0 ≤ KL of uniform from SNIS weights ≤ DKL [qPARISOP (Z0:T |X)kPTAGIST(Z0:T |X)]
(74)
EUBOIM-AIS (x; π0, T, K) = EUBOAIS (x; π0, T) - EpIM-AIS(z(1:K)|x)
DKL pITMG-TAIS(s|z(01:T:K), x)U(s)	.

{z	}
0 ≤ KL of SNIS weights from uniform ≤ log K
(75)
Proof. The result follows directly from App. C.4 Prop. 2.1 by viewing Independent Multi-Sample
ais as iwae with an ais proposal as in App. E.1.
□
F Reverse IWAE
In this section, we propose Reverse iwae (riwae), which is an impractical alternative to standard
iwae. However, we use this as the basis for our Independent Reverse (App. G) and Coupled Reverse
Multi-Sample ais bounds (Sec. 3.3 and App. H).
F.1 Probabilistic Interpretation and Bounds
Similarly to simple reverse importance sampling, Px = Ep(z∣χ) [ Pxizx) ] , we consider K indepen-
dent posterior samples in an extended state space target distribution. The proposal distribution for
our importance sampling scheme is a mixture of K - 1 posterior distributions and one variational
distribution,
KK
qRRWAE(x,z(1:K)) = κ X qθ(z(s)∣x) Y p(x, z(k)),
s=1	k=1
k6=s
K
pTRGIWTAE(x,z(1:K))=Yp(x,z(k)).
k=1
(76)
Note that we have normalization constants of	pTRGIWTAE (x, z(1:K))dz(1:K) = p(x)K and
qPRRIWOPAE(x, z(1:K))dz(1:K) = p(x)K-1 since the transition kernels of the reverse chains do not
change the normalization.
27
Published as a conference paper at ICLR 2022
We visualize this sampling scheme in Fig. 4. Similarly to Eq. (25), the log unnormalized density ratio
simplifies to
PRsTAE (χ, z(1:K))
qRRWAE(x, Z(LK))
log
K
Q p(x, Z(k))
k=1
1K	K
P P qθ(Z(S)|x) Q p(χ, z(k))
K s=1	k=1
k6=s
(77)
__ ι 1 G qθ(Z(k)|x)
=- log K∑p(χ, z(k)).
(78)
Taking the expectation under the proposal and target distributions yield lower and upper bounds on
log p(x). These translate to upper and lower bounds on log p(x) which are different that standard
iwae bounds in Eq. (5),
店	I- I 1 Vλ qθ(z(k)∣x)"∣	r ʌ W	I- 1	1 G qθ(z(k)∣x)
E	K	- log	≤≤∖l	≤ logP(X) ≤ E K	— log
qθ (Za)Ix) Q p(z(k)∣χ)	K 台 p(x,z(k))	—	— Q p(z(k)∣χ)	K 台 Ρ(x, z(k))
k=2	k=1	k=1	k=1
Note these bounds are impractical as they would require more than one true posterior sample from
p(Z|x). However, we use them in Sec. 3.3 and App. H to derive practical multi-sample reverse AIS
bounds.
F.2 Improvement of RIWAE over ELBO and EUBO
Proposition F.1 (Improvement of Reverse IWAE over ELBO and EUB O). Let qPRRIWOPAE (s|x, Z(1:K)) =
q/(z((；|x))/ P qθ(Z；；，)denote the normalized reverse importance sampling weights, or the Pos-
p(x,z |x) k=1 p(x,z |x)
terior over the index variable in Eq. (76). Let U (s) indicate the uniform distribution over K
discrete values. Then, we can characterize the improvement of the Reverse IWAE bounds on log p(x),
ELBORIWAE(x; qθ, K) and EUBORIWAE(x; qθ, K), over the single-sample bounds ELBO(x; qθ) and
EUB O(x; qθ) using KL divergences, as follows
ELB ORIWAE (x; qθ, K) = ELBO(x; qθ) + EqRIWAE(z(1:K) |x) DKL qPRRIWOPAE(s|x, z(1:K))U(s)
V-------------------------{z-------------------------}
0 KL of uniform from SNIS weights ≤	log K
EUB ORIWAE (x; qθ , K) = EUBO(x; qθ) - EpRIWAE(z(1:K) |x)
DKLU(s)qPRRIWOPAE(s|x, z(1:K))	.
•	____	，
'	{z	}
0 ≤ KL of SNIS weights from uniform ≤ Dkl [p(z∣x)kqθ (z|x)]
Proof. The proof follows similarly as Prop. B.2 or Lemma C.2.
G Independent Reverse Multi-Sample AIS
G. 1 Probabilistic Interpretation and Bounds
In similar fashion to Reverse IWAE, we now use Kindependent reverse AIS chains to form an extended
state space target distribution pITRG-TAIS(Z(01:T:K), x), and a mixture proposal distribution qPIRR-OAPIS(Z(01:T:K), x)
which includes a single forward ais chain (see Fig. 2)
KK
q；R-CAIS(X,z01τK)) = K X qAROp(ZOsT|x) Y PAGSr(X,z0kT),
s=1
K
PITRG-TAIS(x, Z(01:T:K)) = YPTAGIST(x,Z(0k:T)).
k=1
k=1
k6=s
(79)
□
28
Published as a conference paper at ICLR 2022
Similarly to Eq. (77)-(78), the log unnormalized density ratio becomes
IR-AIS ( (1:K)	)
log PTGT(Z0iτK),x) =log-
qPROPs (Z0∙T , x)	1
K
K
Q pTAGIST(x, z0:T )
k=1
KT
P qPARISOP(z(0:T) |x) Q pTAGIST(x, z(0:T) )
k=1	t=1
=-loɑ-ɪ XX qARSOP(ZOkT, X)
= g K 乙 PAIS (χ z(k)).
k=1 pTGT(x, Z0:T )
(80)
(81)
Taking expectations of the log unnormalized density ratio under the proposal and target, respectively,
yield lower and upper bounds on log p(x)
E z0：T ~qAROp
z02τK) ~pAGT
1K
-log K X
k=1
PAGSr (x,Z0kT)-
≤ logp(x) ≤ EZ(LK)~*S
z0:T	PTGT
1K
-log K X
k=1
PAGSr(X,z0kT )_l
{^^^^^^^^^^^≡
ELBOIR-AIS (x; π0, K, T)
{^^^^^^^^^^^^^^^^^^^^^≡
EUBOIR-AIS (x; π0, K, T)
(82)
|
However, Independent Reverse Multi-Sample ais may be impractical in common settings, since it is
infeasible to have access to more than one true posterior sample.
G.2 PROOF OF LOGARITHMIC IMPROVEMENT IN K FOR INDEPENDENT REVERSE AIS
Proposition G.1 (Improvement of Independent Reverse Multi-Sample AIS over Single-Sample AIS).
Let qpR-AJs(s∣x, z01τK)) = q：?(Zw / P q；OP(z0:Tkx) denote the normalized reverse importance
pTGT (x,z0:T ) k=1 pTGT (x,z0:T )
sampling weights over AIS chains, and let U(s) indicate the uniform distribution over K discrete
values. Then, we can characterize the improvement of the Independent Reverse Multi-Sample AIS
bounds on log P(x), ELB OIR-AIS (x; π0, K, T) and EUBOIR-AIS (x; π0, K, T), over the single-sample
ELB OAIS (x; π0, T) and ELBOAIS (x; π0, T) using KL divergences, as follows
ELBOIR-AIS (x; π0, K, T) = ELBOAIS (x; π0, T) + E IR-AIS (1:K)
qPROP (z0:T	|x)
DKL[qPIRR-OAPIS(s|x, z(01:T:K))kU(s)]	,
0 ≤ KL of uniform from SNIS weights ≤ log K
J
EUBOIR-AIS (x; π0, K, T) = EUBOAIS (x; π0, T) - EpIR-AIS(z(1:K) |x)
DKL[U(s)kqPIRR-OAPIS(s|x, z(01:T:K))]
J
ι^^^^^^^^^^^~~^^^^^^^^^^^z^^^^^^^^^^^^^^^^^^^^^^^}
0 ≤ KL of SNIS weights from uniform ≤ DKL [pTAGIST(z0:T |x)kqPARISOP (z0:T |x)]
Proof. The proof follows similarly as Prop. B.2 or Lemma C.2.
□
H	Coupled Reverse Multi- S ample AIS
H. 1 Probabilistic Interpretation and Bounds
The Coupled Reverse Multi-Sample ais extended state space target distribution in Fig. 2 initializes
K backward chains from a single target sample ZT 〜∏t(z|x), which makes the bound useful
in practical situations. We denote the remaining transitions as PTAGIST (Z0:T -1|ZT , x), since they are
identical to standard ais in Eq. (12). Thus, the cr-ais extended state space target distribution is
K
PTCGR-TAIS(x, Z(01:T:K-)1, ZT) = πT(x,ZT) YPTAGIST(Z(0k:T)-1|ZT,x).	(83)
k=1
The extended state space proposal is obtained by selecting an index s uniformly at random and
running a single forward AIS chain. We then run K - 1 backward chains, all starting from the last
29
Published as a conference paper at ICLR 2022
state of the selected forward chain,
KK
qCRοAIS(Z01 阳,zτ IX) = κ X qAROp(ZOsT-ι,zτIX) Y PAGt (ZOkT _/ZT, X).
s=1	k=1
k6=s
(84)
See Fig. 2 or Fig. 4 for a graphical model description.
We can construct log-partition bounds using the log unnormalized density ratio
1	PCGTAIS(Z01T-)1, zT, x)
log-----------(1K)----------- = log
qPCRRO-APIS(ZO:T-1, ZT IX)
K AIS	(k)
πT(zT,x)	pTAGIST(z0:T -1|zT, x)
k=1
KK
K P qPROP(ZO:T-1, zTIX) Q PTGT(ZO:T-1|ZT)
s=1	k=1
k6=s
(85)
1K
=-log K X
k=1
qPARISOP(z(0k:T) -1, zT |x)
PAGt (z0kT-1∣ZT )∏τ(zτ, x)
(86)
Taking the expected log ratio under the proposal and target yields lower and upper bounds on log p(x),
ELBOCR-AIS(X; π0,K,T) = -E z0：T-i,zτ ~qAROp MTx
z02T-)ι~PAGT (z0:T-1|zT ,x)
lo 1XX qPROp(z0kT-ι,zτ∣χ)-
K k = 1 PAGT (x,z0kT-ι,zτ)]
≤ log p(x)
EUBOCR-A∣s(x; ∏0,K,T):= -E	ZT~∏T(zτ∣x)
Z0：T —ι ~pTGT(Z0:T — 1 | zT ,x)
lo 1 XX qAROp (z0kT-1, ZT |x)-
K k=1 PAGr (x,z0kT-1,ZT )]
≥ log p(x).
H.2 PROOF THAT CR-AIS BOUNDS TIGHTEN WITH INCREASING K
In this section, we prove that Coupled Reverse Multi-Sample ais bounds get tighter with increasing
K. Our proof provides an alternative perspective to Burda et al. (2016); Sobolev & Vetrov (2019) for
showing the monotonic improvement of IWAE or Independent Multi-Sample AIS with K. We will
also characterize the improvement of cr-ais bounds over single-sample ais bounds in Prop. H.2, as
a direct consequence of this lemma.
Lemma H.1 (CR-AIS Bounds Tighten with Increasing K). Coupled Reverse Multi-Sample AIS
bounds get tighter with increasing number of samples K. In other words, for any K > 1,
ELBOCR-AIS (x; π0, T, K -1) ≤ ELBOCR-AIS (x; π0, T, K) ≤ log P(x),	(87)
EUBOCR-AIS (x; π0, T, K -1) ≥ EUBOCR-AIS (x; π0, T, K) ≥ log P(x).	(88)
Proof. Our proof will proceed by introducing an additional set of M index variables s1:M
into the K-sample probabilistic interpretation of CR-AIS in Eq. (83)-(84), with M < K.
We will show that the KL divergence in this joint state space (including s1:M) is equal to
the gap of the M -sample ELBOCR-AIS (X; πO, T, M) or EUBOCR-AIS (X; πO, T, M). We then
show that marginalizing over s1:M yields the gap of the K-sample ELBOCR-AIS (X; πO, T, K)
or EUBOCR-AIS (X; πO, T, K). Since marginalization cannot increase the KL divergence, we
will have shown that for any M < K, DKL[qPCRRO-APIS(Z(O1:T:K-)1, ZT IX)kPTCGR-TAIS(Z(O1:T:K-)1, ZT IX)]] ≤
DKL[qPCRRO-APIS(Z(O1:T:M-)1, ZT IX)kPTCGR-TAIS(Z(O1:T:M-)1, ZT IX)]] and thus ELBOCR-AIS(X; πO, T, M)	≤
ELBOCR-AIS (X; πO, T, K). Identical reasoning holds for the EUBOCR-AIS .
Sub-Sampling Probabilistic Interpretation LetU(s1, ..., sM) indicate the probability of drawing
M < K sample indices uniformly without replacement (i.e. each sm is distinct). In the CR-AIS
target distribution, there is no distinction between the indices {s1:M} and k 6∈ {s1:M} after drawing
ZT 〜∏t(x, z). We can write
K
PCRTAIS (x,si:M, Z0⅛K)1, ZT) = U (si,…,SM) ∙ ∏t (x, ZT) Y PAGT (z0kT-1∣ZT, x),	(89)
k=1
30
Published as a conference paper at ICLR 2022
which, after marginalization over s1:M, clearly matches pTCGR-TAIS (x, z(01:T:K-)1 , zT).
We also draw si：M 〜U (si,..., SM) for the extended state space proposal. Next, We select an index
m uniformly at random from {1, . . . , M}, which is used to to specify which chain z(0s:Tm-) 1 is run
in the forward direction to obtain zT, as in Eq. (84). After marginalizing over m, we obtain the
following mixture proposal distribution
MM
qCRROAIS(SLM,Z0⅛K-)1,ZT|x) := U(sι,...,SM) ∙ (M X qAOp(z0sm)ι,ZT|x) Y PAGST(Z0sT)-ι∣zτ,x))
m=1	j=1
j6=m
K
• Y PAGST(z0kT-1∣ZT, x).	(90)
k=1
k∈{s1:M }
We will consider marginalizing over si:M below, but first write the KL divergence in the extended
state space which includes si:M. For example, the forward KL divergence matches the gap of the
M -sample ELB OCR-AIS (x; π0, T, M),
DKL[qpCRRO-ApIS(si:M,z(0i:T:K-)i,zT|x)kpTCGR-TAIS(si:M,z(0i:T:K-)i,zT|x)]	(91)
E
log
JU(FMT,(七 P qAROP(z0sm)ι,ZTIX) Q PAGT(Z0sτ)i|zT,X)) ∙ Q	PA最&综-TzTrx)
m=1	j = 1	k = 1
j = m	Jk∈{-s17M}
T(TMT∙ πτ(x, zT) Q	PAGT(Z0kT -1|ZT, X)	Q	PAGST(第CTzTTx)
k∈{s1:M}	∣"k=JrTc
CTTTM}
DKLqpCRRO-ApIS(z(0i:T:M-)i, zT |x)kpTCGR-TAIS(z(0i:T:M-)i, zT |x) ,
(92)
which matches the M -sample probabilistic interpretation of CR-AIS from App. H.1. Identical reason-
ing holds for the case of EUBOCR-AIS (x; π0, T, K) ≤ EUBOCR-AIS (x; π0, T, M) using the reverse KL
divergence.
Marginalization over si:M We have already seen from Eq. (89) that the marginal
Ps pTCGR-TAIS(x, si:M, z(0i:T:K-)i, zT) matches the K-sample target distribution pTCGR-TAIS(x, z(0i:T:K-)i, zT).
We would now like to marginalize over si:M in qpCRRO- Ap IS (si:M, z(0i:T:K-)i, zT |x). Combining the two
product terms in Eq. (90),
MK
X	qCRROAIS(S1：M,z01T-)ι,zτIX)= Eu(si：M)M X	qARO>P(Z0：T)ι,zτ|x) Y	PAGST(ZOkT-i|zT,X)
s1:M	m=1	k=1
k6=sm
1M
M £ EU(SLM)
m=1
K
qPARISOP(Z(0s：Tm-)1, ZT |X) Y PTAGIST(Z(0k：T)-1|ZT,X)
k=1
k6=sm
MK
=) M X Eu(Sm) qARO>P(ZOsm)1,zt|x) Y PAGST(ZOkT-i|zt,χ)
m=1	k=1
k6=Sm
MK	K
= M X κ X WROP(ZOjT-i,ztIX)YPAGST(ZOkT)i|zt,χ)
m=1 j=1	k=1
k6=j
KK X [qARSOP(zOjT)i,Zt∣χ) YPAGST(ZOkT-i∣zt,χ)
j=1	k=1
k6=j
(93)
(94)
(95)
(96)
(97)
31
Published as a conference paper at ICLR 2022
where in (1), we can write the marginal U(sm) since the terms inside expectation do not explicitly
depend on the other indices in s1:M. In (2), we use the fact that the marginal over any sm is uniform.
Thus, we have shown that the marginal distributions match the standard K-sample target and proposal
distributions of cr-ais, with
DKL X qPCRRO-APIS(s1:M, z(01:T:K-)1 , zT |x) X pTCGR-TAIS(s1:M , z(01:T:K-)1 , zT |x) = DKL [qPCRRO-APIS (z(01:T:K-)1 , zT |x)kpTCGR-TAIS (z(01:T:K-)1 , zT |x)]
s 1: M	s1:M	'-----------------------<-----Σ-T7--------------}
gap of elboCR-AIS (x; π0 , T, K)
and similar reasoning for the eubo using the reverse kl divergence.
Conclusion of Proof Since marginalization can not increase the KL divergence, we have
cr-ais (1:K)	cr-ais (1:K)	cr-ais	(1:K)	cr-ais	(1:K)
DKL [qPROP (z0:T -1, zT |x)kpTGT (z0:T -1, zT |x)] ≤ DKL [qPROP (s1:M, z0:T -1, zT |x)kpTGT (s1:M, z0:T-1, zT |x)] .
'------------------------{-------------------------}	'------------------------------{-------------------------------}
gap of elboCR-AIS (x; π0 , T, K)	gap of elboCR-AIS (x; π0 , T, M) (Eq. (92))
(98)
This shows that ELBOCR-AIS (x; π0 , T, K) is tighter than ELBOCR-AIS (x; π0 , T, M), with
ELB OCR-AIS (x; π0, T, K) ≥ ELB OCR-AIS (x; π0, T, M). Identical reasoning holds for the case of
EUBOCR-AIS (x; π0, T, K) ≤ EUB OCR-AIS (x; π0, T, M) using the reverse KL divergence. Choosing
M = K - 1 proves that the bounds tighten as K increases, as desired.	□
Characterizing the Improvement of K-Sample over M -Sample CR-AIS Note that we can
explicitly write the gap of the inequality Eq. (98) using the chain rule for joint probability, for
example qPCRRO-APIS(s1:M, z(01:T:K-)1, zT |x) = qPCRRO-APIS(z(01:T:K-)1, zT |x)qPCRRO-APIS(s1:M |z(01:T:K-)1, zT, x).
The gap in Eq. (98) also corresponds to the gap between ELB OCR-AIS (x; π0, T, K) and
ELBOCR-AIS (x; π0, T, M), so we can write
ELB OCR-AIS (x; π0, T, K) - ELB OCR-AIS (x; π0, T, M)
= DKL[qPCRRO-APIS(s1:M |z(01:T:K-)1, zT, x)kpTCGR-TAIS(s1:M |
z0:T: -1 , zT , x)].	(99)
Below, we analyze the posterior over the index variable for M = 1 as a special case. This allows us
to characterize the gap between ELB OCR-AIS (x; π0, T, K) and the single-sample ELB OAIS (x; π0, T) =
ELB OCR-AIS (x; π0, T, K = 1).
H.3 PROOF OF LOGARITHMIC IMPROVEMENT IN K FOR CR-AIS ELBO
Proposition H.2 (Improvement of Coupled Reverse Multi-Sample AIS over Single-Sample AIS). Let
qPARISOP (z(0s:T) -1, zT |x)
CR-AIS	(1:K)
qPROP (s|x, z0:T-1, zT)
PAGST(X,z0sT-ι∣zτ)
PP QaiROp(Z0：T-i, ZTIx)
k=1 PAGST(X,z0kT-1∣ZT)
(100)
denote the normalized importance weights over the AIS chains used in Coupled Reverse Multi-
Sample AIS, and let U(s) indicate the uniform distribution over K discrete values. Then,
we can characterize the improvement of the Coupled Reverse Multi-Sample AIS bounds on
log p(x), ELB OCR-AIS (x; π0, T, K) and EUB OCR-AIS (x; π0, T, K), over the single-sample AIS bounds
ELBOAIS (x; π0, T) and EUB OAIS (x; π0, T) using KL divergences, as follows
ELB OCR-AIS (x; π0 , T, K)
ELBOAIS(X; π0, T) + EqCR-AIS(z(1:K) ,z |x) DKLqpCRRO-ApIS(s|X, Z(01:T:K-)1, ZT)U(s)
V----------------------------------------------------{z-------------------------------
0 ≤ KL of uniform from SNIS weights ≤ log K
EUBOCR-AIS (X; π0, T, K) = EUB OAIS (X; π0, T) - EpCR-AIS(z(1:K) ,z |x) DKL U(s)qpCRRO-ApIS(s|X, Z(01:T:K-)1, ZT)
S-------------------------------------{z--------------------------------------}
0 ≤ KL of SNIS weights from uniform ≤ DKL [pTAGIST (z0:T |x)kqpARISOp (z0:T |x)]
Proof. Using M = 1 in Eq. (99) above, we obtain
ELBOCR-AIS(x; π0, T, K) - ELBOAIS(x; π0, T, M = 1)
= DKLqpCRRO-ApIS(s|z(01:T:K-)1, zT, x)pTCGR-TAIS(s|z(01:T:K-)1, zT, x)	(101)
32
Published as a conference paper at ICLR 2022
From Eq. (90), we can write the posterior over the index variable as
qPCRRO-APIS(s|z(01:T:K-)1, zT, x)
qAROp(ZOsT-ι,zτ Ix)
PAGT(X,z0sT -i|zT )
pp qAROp(ZOkT-ι,zτIx)
k=1 PAGT(X,z0:T-i|ZT)
(102)
For the target distribution in Eq. (89), the posterior pTCGR-TAIS(s|z(01:T:K-)1, zT, x) = U(s) is uniform. Thus,
we can characterize the improvement of the K -sample CR-AIS ELBO over its single-chain AIS
ELBOCR-AIS (x; π0, T,	K)	- ELBOAIS (x;	π0, T) = E	CR-AIS	(1:K)	DKL [qpCRRO-ApIS (s|x, z(01:T:K-)1,	zT)kU(s)] .
qpROp (zO:T -1 ,zT |x)	: -
(103)
The proof follows identically for the eubo.
For the elbo, the kl divergence with the uniform distribution in the second argument is bounded
above by log K. For the EUBO, the improvement of CR-AIS is limited by the gap in the single-chain
EUBOAis(x; ∏0, T), which corresponds to Dkl[pa(Gr(Z0：T|x)kqAROp(zo：T|x)].	□
H.4 pROOF OF LINEAR BIAS REDUCTION IN T FOR CR-AIS
Corollary H.3 (Complexity in T for Coupled Reverse Multi-sample Ais Bound). Assuming perfect
transitions and a geometric annealing path with linearly-spaced {βt}tT=0, the sum of the gaps in the
Coupled Reverse Multi-Sample Ais sandwich bounds on Mi, ICR-AisU (π0, K, T) - ICR-AisL (π0, K, T),
reduces linearly with increasing T.
Proof. since we have shown in prop. H.2 that ELBOCR-Ais (π0, K, T) and EUBOCR-Ais(π0, K, T)
are tighter than ELBOAis (π0, T) and EUBOAis (π0, T), the bias in the CR-Ais sandwich bounds
EUBOCR-Ais(π0, K, T) - ELBOCR-Ais (π0, K, T) will be less than that of single-sample Ais. Thus,
inequality in Eq. (68) and linear bias reduction in prop. 3.1 (see App. D.1) also hold for cr-ais,
which implies linear bias reduction under perfect transitions and linear scheduling.	□
i	Comparison of Multi-Sample Ais Bounds
in Fig. 5, we compare the performance of our various Multi-sample ais bounds for mi estimation of
a Linear vae with 10 latent variables and random weights, and vae and gan models with 20 latent
variables trained on mnist.
To obtain an upper bound on mi, we recommend using the independent Multi-sample ais elbo for
log partition function estimation. This corresponds to the forward direction of bdmc and achieves the
best performance in all cases. This upper bound uses independent samples and is not limited to log K
improvement, in contrast to the Coupled Reverse Multi-sample ais upper bound on mi (prop. H.2).
The results are less conclusive for the Multi-sample ais lower bounds on mi, where either the
independent Multi-sample ais eubo or Coupled Reverse ais eubo may be preferable for log
partition function estimation. Recall that these bounds have different sources of stochasticity that
provide improvement over single-chain ais. The stochasticity in the independent Multi-sample ais
lower bound on Mi comes from K - 1 independent negative forward chains which, by App. E.3
prop. E.2, can only lead to logK improvement over single-sample Ais. However, these gains are
easily attained for low values of T. For example, with two total Ais distributions, which corresponds
to simple importance sampling with π0(z) = p(z), the independent Multi-sample Ais lower bound
on Mi reduces to structured iNFO-NCE and saturates to log K. This may be useful for quickly
estimating values of Mi at a similar order of magnitude as log K.
The stochasticity in the Coupled Reverse ais lower bound on mi is induced by mcmc transitions in
K coupled backward chains. While this does not formally limit the improvement over single-sample
Ais, we see in Fig. 5 that at least moderate values of T may be needed to match or marginally improve
upon independent Multi-sample ais. These observations suggest that the preferred lower bounds on
mi may vary based on the scale of the true mi and the amount of computation available.
33
Published as a conference paper at ICLR 2022
LInearVAE (k=l)
1500
¢¢6
295
58
26
5
2
工 * H M 32 M US 2Sβ 5U IflM ZOOO MM WOOO ZMOO
NumberofAlS DlsvllXitIors
—Independent AIS Upper Bound
--- Independent AS Iawer Bound
---CoupM Reveraed AIS Upper Bouixl
—Coupled Reveraed AIS Lower Bound
—AnaIyticaIlhMMl
Linear VAE (k=100)
UnearVAEfk=IOOOOJ
(c) Linear VAE (K = 10000)
(a) Linear VAE (K = 1)
Mnisfvae (k=ιj
u=-euuQIU- -SmCH
(d) VAE (K = 1)
MNlSreAN (k=li
(g) GAN (K = 1)
u=-euuQIU- -SmCH
(e) VAE (K = 100)
(h) GAN (K = 100)
Mnistaae (k=ιoooj
u=-euuQIU- -SmCH
(i) GAN (K = 1000)
Figure 5: Comparing Multi-Sample AIS sandwich bounds and evaluating the effect of K and T for
mi estimation in deep generative models on mnist.
J Generalized Mutual Information Neural Estimation (gmine)
In this section, we provide probabilistic interpretations for the Mutual Information Neural Estimation
(MINE) lower bounds on MI from Belghazi et al. (2018), which allows us to derive novel Generalized
mine bounds. In a similar spirit to giwae, Generalized mine uses a base variational distribution
qθ(z|x) to tighten the MINE-DV or MINE-F bound and can be evaluated whenp(z) is available.
See Fig. 6 for a summary of Generalized mine bounds and their relationships. We discuss probabilistic
interpretations in this section, and provide complementary interpretations in terms of conjugate dual
representations of the kl divergence in App. K.
We begin by deriving a probabilistic interpretation for the ibal lower bound in Sec. 4, which we
will show is closely related to the probabilistic interpretations of mine. Our mine-ais method is
designed to optimize and evaluate the ibal lower bound on mi, with detailed discussion in Sec. 4
and App. M.
34
Published as a conference paper at ICLR 2022
_____________________________/(x; z) = Ep(χ,z)[
E PKL 卜(ZlX)Il 标产(z∣x)e%xα)]]
_____________________IBAL (qQ = EP(X,z)[
log
g。(ZlX)
P(Z)
]+Ep(x)[ OKL[p(z∣x)∣∣ 的(ZIx)]
log
Qe(z∣x)
⅛tp(χ)lk0,≠(χ)l
DKL 卜(x, 2)||',)物修怔)/4")]
________________Egmine~dv(Qs,T0)=
P(Z)
]+ ‰(x,z) [τ,^(x5 z)] — Ep(x) [ɪɑg [Eqe(z∣x)eT*'Z)]
Qe(ZIX)
[dgkl [焉 qe(z∣x)eTg)IIge(z∣x)eTg)τ]]
,gkl ['(x)%(z∣x)e%3 IlP(X)媪 z∣x)eTg)τ]
e[
R
e[
P(Z)
]+ ‰(x,z) [T0(x, z)] — 1OgEP(XMe(ZIX) [e"(x")]
'[dgkl ,(ZlX)IIqe(Z ∣x)尹* ㈤T ]]
_______________IGMINE~F(g.鸟)=EP(X,z) [ lθg ] + 6/仪/)[T0(x, Z)]
E[%lp(ZIX)I.阈__/ba@) =Ejl(XjIog 鬻 j
-岛㈤啾 ZlX) [eTg)τ]
Figure 6: Generalized Energy Based Bounds. Arrows indicate the gaps in each mi lower bound or
its relationship to other lower bounds. DGKL (∙∣∣∙) represents the generalized KL divergence between
two unnormalized densities (see App. J.3). All bounds are written in terms of a base variational
distribution q&(z|x), which may be chosen to be the marginalP(Z) as in MINE-DV and MINE-F.
J.1 Probabilistic Interpretation of IBAL
For an energy-based posterior approximation
∏θ,φ(z∣x) :=	1	qθ(z∣x)eTφ(Xz , where Zθ,φ(x) = Eq0(z∣χ) [eTφ(X①],
Zθ,φ(x)
we consider the ba lower bound on mi,
(104)
IBAL(∏θ,φ) = I(x, z) - Ep(X) [Dkl[p(z∣x)k∏θ,φ(z∣x)]]
(105)
Ep(X,z)
Ep(X,z)
l p(x,z) 一
.°g P(X)P(Z) _
- Ep(X,z) [log p(z|x)] + Ep(X,z) [T (x, z)] - Ep(X,z) [log Zθ,φ]
log P(Z)	+ Ep(χ,z) [Tφ(x, z)] - Ep(X) [log Zθ,φ(x)]
Ep(X,Z) log P(Z)	+ Ep(X,Z)
IBAL (qθ )
eTφ(X,Z)
log ——e —、,
Eqθ(z∣X) IeTφ(X,Z)]
contrastive term
=: IBAL(qθ, Tφ),
where the gap in the mutual information bound is Ep(X) [Dkl[p(z∣x)∣∣∏θ,φ(z|x)]]. Note that
IBAL(qθ,Tφ) includes IBAL(qθ) as one of its terms, where we refer to qθ(z|x) as the base varia-
tional distribution. We visualize relationships between various energy-based bounds in Fig. 3b and
Fig. 6.
Proposition 4.1. For a given qθ (z|x), the optimal IBAL critic function equals the log importance
Weights up to a constant T*(x, z) = log tp((Z,∣X)) + c(x). For this T*, we have IBAL(qθ, T*) = I(x; z).
See App. L.1 for the proof.
J.2 Probabilistic Interpretation of Generalized mine-dv
We can interpret the mine-dv bound of Belghazi et al. (2018) as arising from an energy-based
variational approximation πθ,φ(x, z) of the full joint distribution P(x, z) as follows
∏θ,φ(x, z) := Z1 P(x)qθ(z∣x)eτφ(x,Z), where Z = Ep(X)qθ(z∣X) [eTM,] ,	(106)
35
Published as a conference paper at ICLR 2022
qθ(z|x) is a base variational distribution, and Tφ is a critic or negative energy function.
Note that for the induced marginal πθ,φ(x) := πθ,φ(x, z)dz 6= p(x) due to the contribution of the
critic function. Instead, we have
∏θ,φ(x) = -1 P(X)Z (x),	(107)
where Z(x) = R q(z∣x)eTφ(X,Z)dz and Z = Rp(x)q(z∣x)eTφ(X'Z)dxdz = Ep(x)[Z(X)].
Subtracting the joint KL divergence DKL[p(X, z)kπθ,φ(X, z)] from I(X; z), we obtain the Generalized
mine-dv lower bound on mi
I(x; Z) ≥ I(x; z) - DKLlp(X,z)∣∣(p(x)qθ(z∣x)eTφ(x,z)i	(108)
=Ep(x,z) log qθ(ZIX) + Ep(x,z) [Tφ(x, z)] — log Ep(x)qθ(z∣x) [eTφ(x,z)i	(109)
p(z)
=: IGMINE-DV (qθ, Tφ).
By construction, Eq. (108) shows that the gap in Generalized mine-dv is
DKL[p(x,z)kz1 p(x)qθ(z∣x)eTφ(x,z)], which has a probabilistic interpretation in terms of the
approximate joint distribution πθ,φ(x, z).
Relationship with MINE-DV For qθ(z∣x) = p(z), we obtain the MINE-DV bound (Belghazi et al.,
2018; Poole et al., 2019) as a special case with IMINE-DV (Tφ) = IGMINE-DV (p(z), Tφ). In particular, the
joint base distribution in Eq. (106) corresponds to the product of marginals p(x)p(z). Our probabilistic
interpretation shows that the gap in MINE-DV corresponds to DKL[p(x, z)k gp(x)p(z)eTφ(x,z)].
We expect that the Generalized MINE, with a learned variational distribution q§(z∣x), to obtain tighter
bounds than MINE-DV. We can guarantee that IGMINE-DV(qθ,Tφ) ≥ IMINE-DV(Tφ) for the optimal qθ
with a given Tφ, so long as p(z) is in the variational family (i.e., ∃ θ0 such that qθ0 (z|x) = p(z)).
Relationship with BA Bound Choosing a constant critic function Tφ0 (x, z) = const, we can see
that ∏θ,φo (x, z) = p(x)qθ(z∣x) and IGMINE-DV(qθ,Tφ0 = const) = IBAL 3) for a given qe(z∣x).
Optimal Critic Function For a given qθ (z|x), the optimal critic function of Generalized MINE-DV
corresponds to the log importance weight between the target p(x, z) and the joint base distribution
p(x)qe(z∣x), plus a constant
T *(x,z) = log
p(x,z)
p(x)qe (z∣x)
+c.
(110)
For the optimal critic function associated with a given qe(z∣x), we have IMINE-DV(qe, T*) = I(x; z).
Relationship with IBAL We can use our probabilistic interpretation to show that the IBAL is
tighter than Generalized MINE-DV, with IBAL(qe, Tφ) ≥ IGMINE-DV (qe, Tφ). Subtracting the gaps in
the bounds in Eq. (105) and Eq. (108),
IBAL(qe, Tφ) = IG
mine-dv
(qe,Tφ) + DKL[p(x,z)k∏θ,φ(x,z)] — Ep(X) [Dkl[p(z∣x)k∏e,φ(z∣x)]]
S-----------{z-----------} 、---------------{---------------}
gap in GMINE-DV(qθ, Tφ)	gap in IBAL(qθ, Tφ)
IGMINE-DV (qe , Tφ ) + Ep(X)
P(X) i
πe,φ(X)J
= IGMINE-DV (qe, Tφ) + DKL [p(x)kπe,φ(x)]
≥ IGMINE-DV(qe,Tφ).
(111)
(112)
(113)
We see that the difference between IBAL(qe, Tφ) and IGMINE-DV(qe, Tφ) corresponds to a marginal KL
divergence in the x space, where πe,φ(x) is defined in Eq. (107).
36
Published as a conference paper at ICLR 2022
As in Poole et al. (2019), we can also interpret the gap between the IBAL(qθ, Tφ) and IGMINE-DV (qθ, Tφ)
as an application of Jensen’s inequality, with
Dkl [p(x)k∏θ,φ(x)] = DKLhp(Xu 'P(x)Z(x)i	(114)
= log Z - Ep(x) [log Z(x)]	(115)
= log Ep(x) [Z (X)] - Ep(x) [logZ(X)],	(116)
since log Ep(x) [Z(X)] ≥ Ep(x) [log Z(X)].
J.3 Probabilistic Interpretation of Generalized mine-f
The BA lower bound and its corresponding EUBO upper bound on log p(X) are derived using a KL
divergence between the true and approximate posterior, which are normalized conditional distributions
over z given X. In this section, we interpret the MINE-F bound (Belghazi et al., 2018) as arising
from a generalized notion of the KL divergence between possibly unnormalized density functions.
In particular, our probabilistic interpretation will involve an unnormalized ∏θ,φ (z|x) which seeks to
approximate the true (normalized) posterior p(z|X).
Generalized KL Divergence (GKL) First, we state the definition of the generalized KL divergence,
which takes unnormalized measures as input arguments and corresponds to the limiting behavior of
the α-divergence (Cichocki & Amari (2010))
DGKL [r(Z)I恒(Z)]
∕r(Z)Iog IMdz -	rI(Z)dZ +	sI(Z)dZ .
(117)
As in the case of the standard kl divergence, this quantity is nonnegative, convex in either argu-
ment, and vanishes for rI(Z) = sI(Z). It is also a member of both the family of Bregman diver-
gences and f -divergences (Amari, 2009). If r(Z) and s(Z) are normalized, then DGKL [r(Z)ks(Z)] =
DKL[r(Z)ks(Z)]. Finally, if r(Z) is normalized and sI(Z) is unnormalized, one can easily confirm that
DGKL [r(Z)ksI(Z)] = DKL [r(Z)ks(Z)] + DGKL [s(Z)ksI(Z)]
≥ DKL[r(Z)ks(Z)].
(118)
Generalized EUBO (GEUBO) Since the Generalized KL divergence is always nonnegative, we
define a Generalized eubo by adding the Generalized kl divergence between the normalized true
posterior and an unnormalized approximate posterior ∏(z∣x).
GEUBO(x; ∏) := logp(x) + DGKL[p(z∣x)k∏(z∣x)]
(119)
Ep(z|x)
p(x, z)
og n(z|x)_
(120)
where we define Zn(x) := ∏ ∏(z∣x)dz. In general, we have EUBO(x; ∏) ≤ GEUBO(x; ∏) where ∏
is the normalized distribution of πI , with equality if πI is normalized.
Generalized BA (GBA) Using the Generalized EUBO in place of log p(x), we obtain the following
lower bound on mi, which we denote as the Generalized ba lower bound
I(x; z) ≥ I(x; z) — Ep(X) [Dgkl[p(z∣x)k∏(z∣x)]]
Ep(x,z)
'1	∏(z∣x)^
Iog m
+ 1 - Ep(X) [Zn (x)]
(121)
: IGBAL (πI).
Generalized MINE-F (GMINE-F) We now consider an unnormalized, energy-based approximation
to the true posterior, involving a base variational distribution q® (z|x) and a learned critic function
Tφ(x,z)
∏θ,φ(z∣x) := q®(z∣x)eTφ(x,Z)T .	(122)
37
Published as a conference paper at ICLR 2022
Using this unnormalized approximate posterior in the gba lower bound in Eq. (121), we obtain the
Generalized MINE-F lower bound
I(x; Z) ≥ I(x; Z)- Ep(X) [Dgkl[p(z∣x)∣∣∏θ,φ(z∣x)]]	(123)
=Ep(x,z) log qθ^/ L + + ÷ Ep(x,z) [Tφ(X,Z)] - Ep(x)qθ (z|x) heTφ(x,z) 1i	(124)
p(Z)
=: IGMINE-F (qθ , Tφ).	(125)
By construction, we can see that the gap in IGMINE-F (qθ, Tφ) is equal to the Generalized KL divergence
Ep(X) Dgkl[p(z∣x)∣∣∏θ,φ(z∣x)]] in Eq. (123). As in the case of MINE-DV, We obtain the standard
MINE-F lower bound IMINE-F (Tφ) = IGMINE-F(p(z), Tφ) when using the marginal p(z) as the proposal.
Optimal Critic Function The optimal critic function of Generalized MINE-F is T*(x, z) = 1 +
log p(p)X,Z∣x) (Poole et al., 2019). In this case, We obtain IGMINE-F(qs,T*) = I(x；Z) in Eq. (124).
Relationship with IBAL We Would noW like to relate the gap in IGMINE-F (qθ, Tφ) to
the gap in IIBAL(qθ, Tφ). First, note that the normalized distribution corresponding to
∏θ,φ(z∣x) = qθ(z∣x)eTφ(χ,Z)T matches the the energy-based posterior in the ibal, ∏θ,φ(z∣x)=
Z(X)qθ(z∣x)eTφ(x,z). Using Eq. (118), we have
Ep(X) [Dgkl[p(z∣x)k∏θ,φ(z∣x)]] = Ep(X) [Dkl[p(z∣x)k∏θ,φ(z∣x)]] ÷ Ep(X) [Dgkl[∏θ,φ(z∣x)∣∣∏θ,φ(z∣x)]].
and substituting in the definition of each term
Ep(x) DGKLhp(Z|x)M(ZIx)eTφ(x,z)-1i = Ep(x) DKLhp(Z|x»ZIX)qθ(z∣x)eTφ(x,z)i	(126)
'-------------------------------------Z '-----------------------------------------Z
gap of gmine-f
gap of ibal
÷ Ep(x) DGKLhZX)qθ(Z∣x)eTφ(x,z)∣∣qθ(Z∣x)eTφ(x,z)-1i
、------------------------V-----------------------
≥0
where the final term DGKL [∏θ,φ(z∣x) ∣∣∏θ,φ(z∣x)] ≥ 0 is nonnegative because it is a generalized KL
divergence. Thus, We have
IBAL(qθ , Tφ) = IGMINE-F (qθ , Tφ) + Ep(X) DGKL [πθ,φ (ZlX) kπθ,φ(ZlX)]] ≥ IGMINE-F (qθ ,Tφ). (127)
Alternatively, Poole et al. (2019) use the inequality log u ≤ u - 1 to show that
Dgkl[∏θ,φ(z∣x)∣∣∏θ,φ(z∣x)] = Ep(X) [Z(x) - 1 — logZ(x)] ≥ 0.
We visualize the relationship between ibal and Generalized mine-f in Fig. 6.
Relationship with Generalized MINE-DV To characterize the gap between Generalized MINE-
dv and Generalized mine-f, we can again use the equality from Eq. (118), but this time using
divergences over joint distributions. DGKL [p(x,z)∣∣∏θ,φ(x,z)]= DGKL hp(x,Z)p(x)qθ(Zlx)eTφ(X,z)-1i S	{z	} gap in gmine-f	=DKL [p(x, z) ∣∣∏θ,φ(x, z)] + DGKL [∏θ,φ(x, z) ∣∣∏θ,φ(x, z)] =DKLhp(x, z)∣l Z1 p(x)qθ(z∣x)eTφ(x,z)i	(128) 、	{z	} gap in gmine-dv + DGKLhZ1 p(x)qθ(z∣x)eTφ(x,Z)∣∣p(x)qθ(z∣x)eTφ(x,z)-1i . '	V	} ≥0
In this case, we have DGKL [Z1 ρ(x)qθ(z∣x)eτφ(x,z)∣p(x)qθ(z∣x)eTφ(x,z)-1] ≥ 0. Thus, Generalized
mine-dv is tighter than Generalized mine-f (see Fig. 6), which generalizes the finding in Poole et al.
(2019) that standard mine-dv is tighter than standard mine-f.
38
Published as a conference paper at ICLR 2022
K Conjugate Duality Interpretations
In this section, we interpret the energy-based mi lower bounds in mine-ais, Generalized mine-dv,
Generalized mine-f, giwae, iwae, and InfoNCE from the perspective of conjugate duality. In
particular, we highlight that the critic or negative energy function in the above bounds arises as a dual
variable in the convex conjugate representation of the kl divergence. In all cases, the kl divergence
of interest corresponds to the gap in the ba lower bound
I(x; Z)= Ep(χ,z) log qθ(zlχ) + Ep(X) DKL[p(z∣x)kqθ(z|x)]] .	(129)
p(z)
For qθ(z|x) = p(z), the BA lower bound term is 0 and our derivations correspond to taking dual
representation of MI directly, e.g. Ep(x) [DKL[p(z|x)kp(z)]], as in Belghazi et al. (2018).
Our conjugate duality interpretations are complementary to our probabilistic interpretations, with
either approach equally valid for deriving lower bounds and characterizing their gaps.
K. 1 Convex Conjugate Background
The KL divergence to a fixed reference distribution π0 is a convex function of the distribution in the
first argument Ω(∙) := Dkl[∙∣∣∏o(z)]. In later subsections, We will also consider KL divergences over
joint distributions, unnormalized density functions, and extended state spaces.
For a given Ω(∙), we can define the conjugate function Ω*(∙) over a dual variable or function T(z),
with the following relationships (Boyd & Vandenberghe, 2004)
Ω*(T(Z)) := SUp h∏(z),T(z)i - Ω(π(z)),	(130)
π(z)
Ω(∏(z)) = SUp h∏(z),T(z)i- Ω*(T(z)),	(131)
T(z)
where inner product notation indicates hπ(z), T (z)i :=	π(z)T (z)dz. Solving for the optimizing
argument in each of Eq. (130) and Eq. (131) yields the following dual correspondences
∏t(z) = VtΩ*(T(z)) ,	Tn(z) = V∏Ω(π(z)).	(132)
We will proceed to derive closed form expressions for various special cases of Ω and Ω* below.
For an arbitrary p(z) and T(z) which are not in dual correspondence according to Eq. (132), we can
use Eq. (131) to derive a lower bound on Ω(p(z)) known as Fenchel,s inequality
Ω(p(z)) = hp(z),T(z)i - Ω*(T(z)) + Dω(∙)[p(z),∏t(z)]	(133)
≥ hp(z),T(z)i- Ω*(T(z)).	(134)
where the gap in the inequality Dω(∙) [p(z), ∏t(z)] is the Bregman divergence generated by Ω and
πT (z) is the dual variable corresponding to T(z) using Eq. (132).3
K. 2 Conjugate Duality Interpretation of IBAL
To obtain an alternative derivation of IBAL(qθ, Tφ), we consider the conditional KL divergence from
a reference qθ(z|x), which is a convex function of its first argument
Ω(∙) = DKLMqθ(z|x)],
(135)
To derive the conjugate function Ω*(T), note that we must restrict the optimization to the simplex
∏(z∣x) ∈ ∆, since the standard KL divergence requires a normalized distribution as input
Ω*(T(x, z)) := SUp ∏ π(z∣x) ∙ T(x, z)dz — Ω(π(z∣x))
π(z∣x) J
= log Eqθ(z∣x) heT(X,I
π (z|x)dz - 1
(136)
(137)
3This follows directly from the definition of the Bregman divergence by using Tq = VΩ(q) and the
identity in Eq. (130), Dn[p,q] = Ω(p) - Ω(q) - (VΩ(q),p - q〉= Ω(p) - Ω(q) +〈Tq,q〉- E,p)=
ω (P) + ω* (Q)-〈Tq ,pi.
39
Published as a conference paper at ICLR 2022
where we have solved for the optimizing argument πT (z|x) to obtain the conjugate function in
Eq. (137). Eq. (132) suggests the following dual correspondence between primal and dual variables
πT(ZIX) = Z(⅛ qθ (z|x)eT (x,z)	…= log π(zx)+ c(x).	(138)
We would like to leverage this duality to estimate the KL divergence Ω(p(z∣x)) =
DKL[p(z∣x)kqθ(z|x)] from qθ(z|x) to the true posterior p(z∣x). In particular, plugging Eq. (138)
into Eq. (131) suggests the following variational representation
DKL [p(z∣x)kqθ ⑵ x)]
sup	p(z|x) T (x, z)dz - log Eqθ (z|x) heT(x,z)i.
T (x,z)
(139)
For a suboptimal Tπ(x, z), which is in dual correspondence with πT (z|x) instead of the desired
posterior p(z∣x), We can use Eq. (134) to obtain a lower bound on Dkl[p(z∣x)kqθ(z|x)]. To
characterize the gap in this inequality, one can confirm that the Bregman divergence generated by the
KL divergence in Eq. (135) is also the KL divergence。Q陋[」@ [p, ∏] = DKL [pk∏]. Thus, We have
DKL [p(z∣x)kqθ (z|x)]
p(z|x) T (x, z)dz - log Eqθ (z|x) heT(x,z)i
+ DKL [p(z∣x)k∏τ (z|x)]
(140)
Finally, the IBAL(qθ, Tφ) uses this variational representation of the gap in the BA lower bound,
Ep(X) [Dkl[p(z∣x)kqθ(z|x)]], to obtain a tighter bound on ML In particular, for any learned critic
function T(x, z), we can use Eq. (140) to derive the IBAL and its gap
I(X; Z) = Ep(χ,z) Ilog qθ((Zχ)] +Ep(χ) [DKL[p(Zlx) kqθ(ZIx)]]
'--------------------{z---------}
IBAL (qθ)
(141)
=Ep(x,z) Ilog Lx)] + Ep(X) hEp(z∣x) [T(X, Z)I - log Eqθ (z|x) heT (x,z)ii +Ep(χ) [DKL [p(ZIx)kπτ (ZIx)]].
I	{Z	}
IBAL(qθ,Tφ)
The optimal critic function T* (x, Z) provides the maximizing argument in Eq. (139) and is in dual
correspondence with the true posterior p(z∣x). In particular, we have ∏t* (z∣x) = p(z∣x), resulting
in I(x;Z) = IBAL(qθ, T*).
K. 3 Conjugate Duality Interpretation of Generalized MINE-DV
To obtain a conjugate duality interpretation of (Generalized) mine-dv, we consider the dual rep-
resentation of the KL divergence over joint distributions. Choosing ∏o(x, Z) = p(x)qθ (z∣x) as the
reference distribution, the kl divergence is a convex function of the first argument
Ω(∙) = DKL Mp(X)qθ(z∣x)]	(142)
This KL divergence is equivalent to the gap in the BA bound Ep(x) [Dkl [p(z∣x)kqθ(z|x)]] after noting
the marginal distribution of both p(x, z) and p(x)qθ (z∣x) is p(x).
However, the duality associated with Ω(∙) = Dkl [∙k∏o(x, z)] holds for general joint distributions,
and we will see that the conjugate duality perspective using this divergence leads to looser bound
on MI than in App. K.2. To evaluate the expression Ω(∏(x, z)) = SuPT(x/) h∏(x, z), T(x, z)i -
Ω*(T(x,z)), we need to derive the conjugate function Ω*(T(x, z)). Similarly to Eq. (136), we
constrain the joint distribution to be normalized and obtain
Ω*(T(x, z)) := SuP ∏ π(x, z) ∙ T(x, z)dxdz — Ω(π(x, z))
π(x,Z)
π(x, z)dzdx - 1	(143)
= log Ep(x)qθ(ZIx) eT (x,Z) ,	(144)
where we have used ∏o(x, z) = p(x)qθ(z∣x). Note that the expectation over p(x) now appears inside
the log in Ω*(T(x, z)), compared with the conjugate for the conditional KL divergence in Eq. (137).
Solving for the optimizing arguments in Eq. (131) and Eq. (143), we have the following relationship
between primal and dual variables,
∏t(x, z) := -^p(p(x)qθ(z∣x)eτ(x,z) ,	T∏(x, z) =log j(X；Z)、+ J (145)
Z (T)	p(x)qθ (z|x)
40
Published as a conference paper at ICLR 2022
Finally, we use Eq. (131) to write the dual representation of joint kl divergence as
Dkl [p(x, z)kp(x)qθ (z|x)] = SUp ∕p(x,z) ∙ T (x,z)dxdz - log Ep(χ)q0(z∣χ) [eT (x,Z)].	(146)
T (x,z)
As in the previous section and Eq. (134), a suboptimal T(x, z), which is in dual correspondence with
∏t(z|x) instead of the desired posterior p(z∣x), yields a lower bound on DKL [p(x, z)kp(x)qθ(z|x)],
with the gap equal to a kl divergence
Dkl[p(x, z)∣∣p(x)qθ(z∣x)] = /p(x, Z)T(x, z)dxdz - logEp(X)q@(z∣x)[eT(X,z)] + DKL[p(x, Z)IlnT(x, z)].
(147)
The Generalized MINE-DV bound IGMINE-DV (qθ, Tφ) uses this variational representation of the gap in
the BA lower bound to obtain a tighter bound on MI. For a learned critic T(x, z), we can use Eq. (147)
to write
I(x; Z) = Ep(χ,z) Ilog qθ((Zx) ] +Ep(χ) [DKL[p(ZIx) kqθ(ZIx)]]
'-------------------{z---------'
IBAL (qθ)
(148)
=Ep(x,z) Ilog qfp((Zχ)] + Ep(X)p(z∣x) [T(x, Z)] - log Eqθ (z|x) heT(x,z)i +DKL [p(X)P(ZIx) kπT (x, Z)]
I	{Z	}
IGMINE-DV (qθ ,Tφ )
As noted in App. J.2, the Generalized mine-dv bound is looser than the ibal.
K.4 Conjugate Duality Interpretation of Generalized MINE-F
Nguyen et al. (2010) consider the conjugate duality associated with the family of f -divergences, of
which the kl divergence is a special case. We will show that this dual representation corresponds
to taking the conjugate function of the Generalized kl divergence, which can take unnormalized
densities as input. See App. J.3 for definition of the Generalized kl divergence, which is convex in
either argument since it differs from the standard kl divergence by only a linear term..
For Ω(∙) = DGKL[∙∣血(z|x)], We write the dual variable using the notation T0(x, Z) and consider
Ω*(T0(x, z)) := SUP ∏ π(z∣x)T0(x, z)dz - DGKL [∏(z∣x)k⅞θ(z|x)]
π(z∣x) J
(149)
Note that we do not explicitly include a Lagrange multiplier to enforce restriction to normalized
distributions. Solving for the optimizing argument or writing the dual correspondence in Eq. (132),
we obtain
∏τ(z∣x) = qθ(z∣x)eτ0(x,z) ,	Tn(x, z) = log π((ZIx)).
qθ (zIx)
Plugging this ∏t(z∣x) back into Eq. (149) yields the conjugate function
Ω*(T0(x, z)) = / q(z∣x)eτo(x,z)dz — / q(z∣x)dz,
(150)
(151)
which leads to a dual representation of the Generalized kl divergence that matches Nguyen et al.
(2010) after plugging into Eq. (131)
Dgkl[p(z∣x)∣M(z∣x)] = SuP ∕p(z∣x)T0(x, z)dz - / q(z∣x)eτ (x,z)dz + [ q(z∖x)dz .	(152)
T 0 (x,z)
We now use the reparameterization T0(x, Z) = T(x, Z) - 1. Assuming a normalized q(z∣x)=
q(z∣x) and p(z∣x) = p(z∣x), and noting that Dgkl[p(z∣x)kqθ(z|x)] = Dkl[p(z∣x)kqθ(z|x)] for
normalized distributions, we obtain
DKL[p(z∣x)kqθ(z|x)]= sup Pp(z∣x) T(x, z)dz - / qθ(z∣x)eT(x,z)-1dz ,
T (x,z)
(153)
which matches dual representation of the kl divergence found in Belghazi et al. (2018); Nowozin
et al. (2016). See Ruderman et al. (2012) for further discussion.
41
Published as a conference paper at ICLR 2022
For a suboptimal Tπ(x, z) which is in dual correspondence with πT (z|x) instead of the desired
posterior p(z∣x), We can use Eq. (134) to obtain a lower bound on DGKL∣p(z∣x)∣∣qθ(z|x)] =
DKL [p(z∣x)kqθ (z∣x)]∙ To characterize the gap of this lower bound, note that the Bregman divergence
generated by the DGKL divergence is also the DGKL divergence DDGKL [∙k(^[Pk∏] = DGKL [Pk∏]∙ Thus,
using Eq. (133), we have
DKL [p(z∣x)kqθ(z∣x)]
p(z|x) T (x, z)dz -
/qθ(z∣x)eτ(x,z)-1dz + DGKL[p(z|x)||qe(z|x)eT(x,z)-1].
(154)
Finally, we obtain the Generalized mine-f bound by evaluating the dual representation of the
Generalized KL divergence for normalizedp(z∣x) and q®(z|x). In particular, IGMINE-F(qθ, Tφ) uses
the critic function Tφ to tighten the gap in IBAL(qθ) via the dual optimization in Eq. (154),
I(XiZ) = Ep(χ,z) Ilog qθ((Zx)] +Ep(x) [DKL[P(ZIx) kqθ(ZIx)]]
I	{Z	}
IBAL (qθ)
(155)
qθ(ZIx)
Ep(X，z) [log ^P(Zr
+ Ep(X) hEp(z∣x) [T(X,Z)] - log Eqθ (z|x) heT (x,z)ii +Ep(x) [DGKL[P(ZIx) kπT (ZIx)]]
---------------V---------------------------------------}
IGMINE-F (qθ ,Tφ)
(156)
The optimal critic function is the dual variable of the true posterior p(z|x), which can be found using
Eq. (150) as T(x, z) = 1 + log 郡X)). With this optimal critic, the Generalized MINE-F bound is
tight.
K. 5 Conjugate Duality Interpretation of GIWAE, IWAE, and InfoNCE
In this section, we use conjugate duality to derive the giwae, iwae, and Info-NCE bounds on
mutual information and characterize their gaps. Our approach extends that of Poole et al. (2019),
where the mine-f dual representation (App. K.4) was used to derive Info-NCE. We provide an
alternative derivation using the dual representation associated with the conditional kl divergence and
ibal in App. K.2. For either dual representation, giwae, iwae, and Info-NCE arise from limiting
the family of the critic functions Tφ in order to eliminate the intractable log partition function term.
We start from the decomposition of I(x; z) into IBAL(qθ) and its gap
I(x；Z) = Ep(x,Z) Iogq ( 、	+Ep(x) [Dkl[p(z∣x)∣∣qθ(z[x)]] .	(157)
p(z)
、----------{z---------}
IBAL (qθ )
We will focus on the dual representation of the normalized, conditional KL divergence Ω(∙)
DKL[∙kqθ(z|x)], as in App. K.2.
Multi-Sample IBAL Consider extending the state space of the posterior or target distribution
p(z∣x), by using an additional K -1 samples from a base variational distribution q® (z|x) to construct
K
pTGT(z(1:K), s|x) := U(s)p(z(s)|x) Y q®(z(k) |x),	(158)
k6=s
k=1
where S is an index variable S ~ U(s) drawn uniformly at random from 1 ≤ S ≤ K, which specifies
the index of the posterior sample p(z|x). We similarly expand the state space of the base variational
distribution to write
K
qPROP(z(1:K), S|x) := U(S) Y q®(z(k) |x).	(159)
k=1
42
Published as a conference paper at ICLR 2022
It can be easily verified that this construction does not change the value of the kl divergence
DKL[pTGT(z(1:K),s|x)kqpR0P(z(1：K),s|x)] = EpTGT(z(i：K),s|x), PTGT((Z(Ux)
EpTGT(z(1:K),s|x)
纸S)P(Z(S)Ix) Qk=S qθ (Z(k)|X)
K
m(sS) Q qθ(z(k)∣x)
k=1
P(Z(S) Ix)
U(S)P(Z(S)Ix) [θg qθ(z(S)Ix)
Ep(z|x)
log
P(ZIx)]
qθ (ZIx)J
DKL[P(ZIx)kqθ(ZIx)] .
Consider the convex function
Ω(∙) := DKL [ ∙ IlqPROP(z(1:K), s|x)],	(160)
where the primal variable is a distribution in the extended-state space of z(1:K), s , and the dual
variable is a critic function T(x, z(1:K), s). We derive a conjugate optimization in similar fashion to
App. K.2, but now over the extended state space. For this Ω(∙), the conjugate function Ω*(T) takes a
log-mean-exp form analogous to Eq. (137)4. We can write the variational representation of Ω(pTGT)
as
K
QmTGlO=Sup/ X PTGT(Z(LK),sIx)T(X,z(1：K), S)dz(1:K)-Iog EqPROP(z(i：k),s|x) [eT(x,z(L ^^ ,S)] .
S=1	(161)
For a particular choice of T(x, z(1:K), s), we can use Eq. (161) to obtain a lower bound on the KL
divergence DKL[p(z∣x)kqθ(z|x)] (as in Eq. (134)). This lower bound translates to the Multi-SamPle
IBAL lower bound on MI, IMS-IBAL (qθ, T) via Eq. (157) .
I(x; z)	≥	Ep(x,z)卜g "P(Z)，] +	Ep(X) [EPTGT(Z(LK) ,sιx) hT (X,Z(1：K), S)ii -	log EqPROP(Z(LK) ,s|x)	heT(x,z	,S)]
Z(x；T )
=: IMS-IBAL (qθ , T),	(162)
where Z(x; T) is the normalization constant of the dual distribution ∏t(z(1:K), s|x) corresponding
to T,
∏T(Z(LK), s|x) = J. T、qPROP(z(1：K), s∣x)eτ(X,z(1：K),S).	(163)
Z (x; T)
As in Eq. (133), we can write the gap of IMS-IBAL(qθ, T) as a Bregman divergence or KL divergence
I(x; z) = Ims-ibal(qθ, T) + Ep(X) [Dkl [ptgt(z(1:k), s∣x)k∏τ(z(1:K), s[x)]] .	(164)
The optimal K-sample energy function in Eq. (161) should result in ∏T* (z(1:K), s|x)=
pTGT(z(1:K), s|x). Using similar reasoning as in App. C.5 or App. L.1, this occurs for
T*(x, z(1:K), S) = log A((Z((SS))X)) + c(x), for which we have I(x; z) = IMS-IBal(qθT).
GIWAE is a Multi-Sample IBAL with a Restricted Function Family Although extending the
state space did not change the value of the kl divergence or alter the optimal critic function, it does
allow us to consider a restricted class of multi-sample energy functions that yield tractable, low
variance estimators. In particular, giwae and Info-NCE arise from choosing a restricted family of
functions TGIWAE(x, z(1:K), s), under which the problematic log Z(x; T) term evaluates to 0. This
function family is defined as
4We consider the conjugate with restriction to normalized distributions as in App. K.2 Eq. (136).
43
Published as a conference paper at ICLR 2022
1 K	eTφ(x,z(s))
TGiwae(x, Z( K),s) ：= log K
ɪ P eTφ(χ,z(k))
K k=1
(165)
where TGIWAE (x, z(1:K), s) is specified by an arbitrary single-sample critic function Tφ(x, z). We can
now see that log Z (x; TGIWAE) = 0,
lθg Z(X； TGIWAE) = Iog %ROP(Z(LK),S∣ JeTGIWAE(X'z(lf[ =log EU(S) Q qθ (z(k)∣χ)[ 1 pφ:(χ,Z(k)) # =0 ,
k = 1	K e φ
k=1
K
using the fact that Z(LK)〜 Q qg(z(k)∣x) is invariant to re-indexing. With this simplification,
k=1
IMS-IBAL(qθ, TGIWAE) recovers the GIWAE lower bound on MI
I(x; z)
≥ IMS-IBAL (qθ, TGIWAE)
(166)
Ep(x,z) log
qθ (z∣χ)
p(z)
+E	K
K1 p(x)p(z(s)∣x) Q qθ(z(k)∣x)
k6=s
k=1
eTφ(x,z(s))
log ------------------
-1 P eTφ(χ,ZK))
k=1
(167)
Ep(x,z)
'1 qθ (z∣χ)
log F
+E	K
p(x)p(z(1)|x) Q qθ (z(k) |x)
k=2
eTφ(x,z(1))
log -------------------
ɪ P eTφ(χ,z(k))
K k=1
(168)
IGIWAE (qθ, Tφ, K).
Finally, we can use Eq. (164) to recover the probabilistic interpretation of giwae and the gap in the
lower bound on MI. As we saw above, the dual distribution πTGIWAE (Z(1:K), s|x) is normalized with
Z(x; TGIWAE) = 1. In particular, we can write
∏TGiwae (z(1:K),s|x) =	1——V qPROP(z(1:K),s∣x)eTGIWAE(x,z(1：K),s)	(169)
Z(x; TGI
WAE)
1	K	eTφ(x,z(s) )
=z(χ.T	)M^ Y qg (z(k)∣χ) —K------------------ (170)
Z(X，/GIWAE)	k=ι	/ P eTφ(x,z(k))
K k=1
K	k	eTφ(x,z(s) )
=∏qg(z(k)|x) -K	(171)
k=1	P eTφ(x,z(k) )
k=1
which recovers qPGRIWOPAE(z(1:K), s|x) from the probabilistic interpretation of GIWAE in Eq. (42) or
App. C.1. We can write the gap ofIGIWAE(qg,Tφ, K) = IMS-IBAL(qg, TGIWAE) as the Bregman diver-
gence or kl divergence as in Eq. (133)
I(x; z) = IMS-IBAL(qθ, TGiwae) + Ep(X)IDKLlpTGT(z(1:K),s|x)卜TGIWAE(z(1:K),s|x)]] ,	(172)
which matches the reverse KL divergence Ep(x) DKL [pTGGIWTAE (z(1:K), s|x)kqPGRIWOPAE(z(1:K), s|x)] de-
rived from the probabilistic approach in App. C.1. Recall that Info-NCE is a special case of giwae
with qg(z|x) = p(z) (Sec. 2.4).
Conjugate Duality Interpretation of iwae We can gain alternative perspective on IWAE from
this conjugate duality interpretation. In particular, iwae is a special case of giwae, where the optimal
single-sample critic function T*(x, z) = log Px,京 + c(x) (see Sec. 2.4) is used in Eq. (165) to
construct the optimal multi-sample function TIWAE , within the GIWAE restricted multi-sample function
family. Thus, we have IMS-IBAL(qg, TIWAE) = IIWAE (qg, K).
44
Published as a conference paper at ICLR 2022
Although iwae uses the optimal critic function, the restriction to the function family in Eq. (165)
is necessary to obtain a tractable bound on the KL divergence DKL[p(z∣x)kqθ(z∣x)] and mutual
information. Without the restricted function family, the intractable log partition term in Eq. (161)
would require mcmc methods such as ais to accurately estimate, as we saw for the single-sample
ibal in Sec. 4.
L Properties of the IBAL
Our MINE-AIS method in Sec. 4 and App. M optimizes the Implicit Barber Agakov lower bound
(ibal) on mi from Eq. (18). We first recall the probabilistic interpretation of the ibal bound
from App. J.1. For a posterior approximation with a learned negative energy function Tφ and base
variational distribution q®(z|x),
∏θ,φ(z∣x) =	1	qθ(z∣x)eTφ(XW	where Zθ,φ(x) = Eq0(z∣χ) [eTφ(XZ],
Zθ,φ (x)
(173)
we consider the ba lower bound on mi,
(174)
(175)
I(x, z) ≥ IBAL (πθ,φ)
=I(x,z) - Ep(X) [Dkl[p(z∣x)k∏θ,φ(z∣x)]]
Ep(x,z)	log ⅛⅛2 ]+ Ep(x,z) ~V	} 		eTφ(x,z) .log Eg。(z∣χ) [e…L	(176) }
contrastive term
(177)
IBAL (qθ )
IBAL(qθ , Tφ).
The gap of this lower bound on mutual information is Ep(x)[Dkl[p(z∣x)∣∣∏θ,φ(z∣x)]] , as in Sec. 2.2.
L.1 Proofs for IBAL Optimal Critic Function (Prop. 4.1 and L.1)
Proposition 4.1. For a given qθ (z|x), the optimal IBAL critic function equals the log importance
weights up to a constant T*(x, z) = log 看以?)+ c(x). For this T*, we have IBAL(q6, T*) = I(x; z).
Proof. Recall that the gap in IBAL(q0,Tφ*) is Ep(X) [Dkl[p(z∣x)k∏θ,φ(z∣x)]] = I(x; z)-
IBAL(q6,Tφ*). This implies that the bound will be tight iff p(z∣x) = ∏θ,φ(z∣x) 8 p(x, z). We
can easily show that the true log importance weights (plus a constant) satisfy this property
πθ,φ(ZIX) = Zθ⅛ qθ(ZIX)elog
p(x,z)
qθ (ZIx)
ec(X)
+c(X) = K P(X,Z)= P(ZIX),	(178)
where ec(X) is absorbed into the normalization constant. Conversely, using g(x, z) which depends on
Z in T(x, z) = log qp((ZjX)) + g(x, z) would change the density over Z to no longer match p(z∣x).
At this value, the IBAL is exactly equal to I(x; z)
IBAL(qθ ,T *) = Ep(x,z) Ilog qθ((Zχ)] + Ep(x,z) [Tφ(x, Z)] — Ep(x) [log Zθ,φ(X)]
qθ(z|x)	p(x, Z)	p(χ, Z)
=Ep(x,z) [l°g M	+ Ep(x,z) [log qθ(z^)+log C(X)- Ep(x) [log Eqθ(z∣x) qθ(z^ + l°gC(X)
=Ep(x,z) flog p(:z1 ] = I(x； Z).
p(Z)p(X)
□
Proposition L.1. Suppose the critic function Tφ(x, z) is parameterized by φ, and that
∃ φo s.t.Tφo (x, z) = const. For a given q®(z∣x), let Tφ* (x, z) denote the critic function that
maximizes IBAL(q6, Tφ). Then,
IBAL (q6) ≤ IBAL(q6, Tφ*) ≤ I(x; z) = IBAL (q6) + Ep(X) [DKL[P(zIx)kq6(zIx)]] .	(179)
In particular, the contrastive term in Eq. (18) is upper bounded by Ep(X) [DKL [P(zIx)kq6 (zIx)]].
45
Published as a conference paper at ICLR 2022
Proof. The BA bound is a special case of the IBAL(qθ, Tφ) with constant Tφ0 = c and the contrastive
term equal to 0. Since φ0 is a possible parameterization, we can only improve upon IBAL (qθ) by
learning Tφ . The parameterized family of Tφ may not be expressive enough to match the true log
importance weights, so lBAL(qθ, Tφ*) ≤ lBAL(qθ, T*) = I(x; Z) using Prop. 4.1.	□
L.2 PROOF OF IBAL AS LIMITING BEHAVIOR OF THE GIWAE OBJECTIVE AS K → ∞
(PROP. L.2)
Proposition L.2 (IBAL as Limiting Behavior of GIWAE). For given q§ (z|x) and Tφ(x, Z), we have
lim IGIWAEL (qθ, Tφ, K) = IBAL(qθ, Tφ) .	(180)
K→∞
Proof. Comparing the form of IGIWAEL (qθ, Tφ, K) to the IBAL(qθ, Tφ) for a fixed qθ and Tφ,
q(z|x)	eTφ(x,z(1))
IGIWAE (qθ , Tφ, K) =EP(X,z) log	+ E ,小、K , ,入、log 1 LK	他
L	P(Z)」	p(x,z⑴)Q qθ(z(k)1x) [ K P=1 eτφ(X,z(k))
IBAL(qθ ,Tφ) = Ep(χ,z) log I(Z)，] + Ep(x,z)
eTφ(X,z)
log Eg。(z|x) [eTΦ(χ,z)]
we can see that both bounds include the same first term IBAL (qθ) and the same numerator of the
contrastive term. To prove the proposition, we can thus focus on characterizing the limiting behavior
of the denominator
∀x :	lim E	K
K→∞ p(z(1) |x) Q qθ (z(k) |x)
k=2
X------------------------
1K
log ⅛ X eTΦ(x,z)
k=1
= log ZGIWAE (x, K )
log E	eTφ (x,z) ,
S------------{------------}
= log Zθ,φ (x)
(181)
}
where we introduce the notation log ZGIWAE(x, K ) for convenience and the right hand side is the log
partition function for the IBAL energy-based posterior πθ,φ (Z|x). Intuitively, we expect Eq. (181)
to hold since the contribution of the single posterior sample p(Z|x) in the GIWAE expectation will
vanish as K → ∞.
More formally, we consider the sequence of values log ZGIWAE(x, K ) as a function of K . We
derive lower and upper bounds on the value of log ZGIWAE(x, K ) for fixed K and show that
each of these sequences of lower and upper bounds converge to log Zθ,φ (x) in the limit as
K → ∞. Using the squeeze theorem for sequences, this is sufficient to demonstrate the claim that
limK→∞ log ZGIWAE(x, K ) = log Zθ,φ(x) in Eq. (181). Since the other terms in IGIWAEL(qθ, Tφ,K )
and IBAL(qθ, Tφ) are identical, this is sufficient to prove the proposition.
Lower Bound on log ZGIWAE(x, K ): We rely on the fact that the exponential function eTφ(x,z) ≥ 0 to
simply ignore the contribution of the p(Z|x) term.
log ZGIWAE (x, K) = E	K p(z(1) |x) Q qθ (z(k) |x) k=2		K log X eTφ(x,z(k)) k=1		- log K	(182)
(1) ≥EK Q qθ (z(k) |x) k=2	K log X eTφ (x,z(k)) k=2		- log K		(183)
=EK Q qθ (z(k) |x) k=2	K logɪ X eTφXz(% K-1 k=2			K-1 + log ɪ	(184)
=: LB(x; K),					(185)
where in (1), we obtain a lower bound by ignoring the contribution of the positive sample from
p(Z|x). The first term is the k-sample IWAE lower bound with the proposal qθ(Z) and target πθ,φ(x, Z):
Eqθ(zi:K)[log 去 Pk πθ,φZ荷X)) ], and thus converges to log Zθ,φ as K → ∞. As K → ∞, we also
46
Published as a conference paper at ICLR 2022
have log K-1 → 0, so that the limit of their sum is
JimLB(x； K) = logEqθ(z∣χ) heTφ(x,z)i =1°g Zθ,φ(x)
(186)
Upper Bound on log ZGIWAE(x, K): To upper bound log ZGIWAE(x, K), we separately consider terms
arising from p(z∣x) samples and q§(z|x) samples. Noting that PGGWAE(z(1:K), s|x) in Eq. (26) is
invariant to the index s, We can assume Z(I) ~ p(z∣x) and write
log ZGIWAE (x, K) =E	K
p(z(1) |x) Q qθ (z(k) |x)
k=2
log (IeTφ(x,z(1)) + K XX
k=2
eTφ(x,z(k))]
EK
p(z(1) |x) Q qθ (z(k) |x)
k=2
lθg K K
1	XX eTφ(x,z(k))
K -1 k=2
+
K - 1
K
≤ log ( KEp(Za)|x) [eTg(I))i + E Q
K	qθ(z(k) |x)
k=2
1	XX eTφ(χ,z(k))
k=2
K - 1
K
log(⅛Ep(Za)∣χFφ(x,z(1))i + K- ∙ E xx Eq(z(k)∣χ) [eTO
k=2
log (KEp(Za)|x) heTφ(x,z(1))i + K-Zθ,φ(x))
: UB(x; K).
(187)
(188)
Since log(u) is a continuous function, we know that limK→∞ log(f (K)) = loglimK→∞ f(K). We
thus reason about the limiting behavior of the terms inside the logarithm in Eq. (187). As K → ∞,
we have -- → 0, and thus the first term inside the log goes to 0. For the second term, we also have
KK1 → 1. Thus, we have
JimO UB(x； K) = logEqθ(z∣x) [eTφ(x,z)] = logZθ,φ(x).
(189)
As reasoned above, the convergence of the sequence of both upper and lower bounds to log Zθ,φ(x)
implies that limK→∞ log ZGIWAE(x, K) = log Zθ,φ(x). By the reasoning surrounding Eq. (181), this
implies lim-→∞ IGIWAEL (qθ, Tφ) = IBAL(qθ, Tφ) as desired.	口
L.3 Convergence of GIWAE SNIS Distribution to IBAL Energy-Based Posterior
In this section, we consider the marginal snis distribution of giwae, which is induced by sampling
K times from qθ(z|x) and returning the sample in index S with probability qGRWAE(s|x, z(1:K)) 8
eTφ(x,z(s)). As K → ∞, we show that this distribution converges to the single-sample energy-based
posterior approximation underlying the ibal and mine-ais. A similar observation is made in Sec.
3.2 of Lawson et al. (2019). This result regarding the probabilistic interpretations of giwae and the
ibal is complementary to the result in Prop. L.2 regarding the limiting behavior of the bounds.
Proposition L.3. Define the marginal SNIS distribution of GIWAE, qPGRIWOPAE(z|x; K), using the follow-
ing sampling procedure
1.	Samplefrom s, z(1:K)〜qGRWAE(s, z(1:K)|x) according to Eq. (42)
2.	Return z = z(s).
Then, as K → ∞, the KL divergence (in either direction) between the marginal SNIS distribution of
GIWAE and the energy-based variational distribution of IBAL, ∏θ,φ(z∣x) H qθ(z∣x)eTφ(x,z), goes to
zero.
Jim Dkl[qGRWAE(z|x； K)kπθ,φ(Z|x)] =0
K→∞
and lim DKL[∏θ,φ(z∣x)∣∣qGRWAE(z|x;K)] =0.
K→∞
47
Published as a conference paper at ICLR 2022
Proof. To prove the proposition, we consider a mixture target distribution similar to the case of IWAE.
However, in this case, We take a single sample from ∏θ,φ(z∣x) instead of p(z∣x)
1K
Ptgt ,π(s, z(1: ), X) = κ∏θ,φ(Z(S) |x) Y qθ(Z(S) |x).
k=1
k6=S
(190)
Note that the probabilistic interpretations qPGRIWOPAE (s, Z(1:K) |x) and pTGGIWTAE,π(s, Z(1:K), x) exactly
match the iwae proposal and target distributions in App. B.1 where, in the target distribution,
the posterior p(Z|x) has been replaced by πθ,φ (Z|x). Thus, we can analyze the KL divergences
DKL[qGRWAE(s,z(LK)∣x)kpGGT,π(s,z(LK)∣x)]and Dkl[pGGWAe,π(s,z(LK)∣x)kqGRWAE(s,Z(LK)|x)]
using techniques from previous work on iwae. Following similar arguments as in Domke & Shel-
don (2018) (Thm 2) and Cremer et al. (2017), these extended state space kl divergences upper
bound the kl divergence between the marginal snis distribution and energy-based target, e.g.
DKL[qGRWAE(z∣x;K)k∏θ,φ(z∣x)] ≤ Dkl[qGRWAE(s,Z(LK)∣x)kpGGWAE,πGZ(LK)|x)]. AS K → ∞,
DKL[qGWAE(s,z(1:K)IX)IlpGGWAE'π(s,z(1:K)|x)] → 0 since it is an instance of the IWAE gap. Thus,
the KL divergence DKL [qPGRIWOPAE(z|x; K)kπθ,φ (z|x)] also vanishes. Similar reasoning applies for the
reverse KL divergence.	口
M MINE-AIS
M.1 Energy-Based Training of the IBAL
Eq. (19) indicates that in order to maximize the IBAL as a function of θ and φ, we need to increase
the value of the energy function Tφ or score function log qθ(z∣x) on the real samples of the truejoint
p(x, z) and decrease the value on fake samples from the approximate p(x)∏θ,φ(z∣x). However, as is
common in training energy-based models, it is difficult to draw samples from πθ,φ.
In order to sample from ∏θ,φ(z∣x), a natural approach is to initialize MCMC chains from a sample of
the base distribution qθ(z∣x), using HMC transition kernels (Neal, 2011), for example. However, we
may require infeasibly long mcmc chains when the base distribution is far from desired energy-based
model ∏θ,φ(z∣x). Instead, we can choose to initialize the chains from the true posterior sample
z 〜p(z∣x) for a simulated data point X 〜p(x). Letting 71：M indicate the composition of T
transition steps, the approximate energy function gradient becomes
∂∂	∂
∂φIBAL(qs, Tφ) ≈ Ep(x,z) ∂φTφ(x, z) ― Ep(x,zo)T1:M(z∣Z0,x) ∂φTφ(x, z) ,	(191)
We can use an identical modification for the gradient with respect to θ.
This initialization greatly reduces the computational cost and variance of the estimated gradient and
enables training energy functions in high dimensional latent spaces, as shown in our experiments.
This approach is in spirit similar to Contrastive Divergence learning of energy-based models (Hinton,
2002) where one starts an mcmc chain from the true data distribution to obtain a lower variance
gradient estimate.
M.2 Multi-Sample AIS Evaluation of the IBAL
After training the variational base distribution q§ (z ∣x) and the critic function Tφ(x, z) using the MINE-
AIS training procedure above, we still need to evaluate the IBAL lower bound on MI, IBAL(qθ, Tφ).
We can easily upper bound IBAL(qθ, Tφ) using a Multi-Sample AIS lower bound on log Zθ,φ (x) with
expectations under the forward sampling procedure qPARISOP(z0:T |x). However, an upper bound on
IBAL(qθ, Tφ) is not guaranteed to preserve a lower bound on MI.
In order to obtain a lower bound on the ibal, we would need to obtain an upper bound on
log Zθ,φ(x) = logEqθ(z∣x) [eTφ(x,z)], the log partition function of ∏θ,φ(z∣x). However, consid-
ering this to be the target distribution πT (z|x) in Multi-Sample AIS, we would require exact samples
from ∏θ,φ(z∣x) to guarantee an upper bound on log Zθ,φ(x). Since these samples are unavailable,
we demonstrate conditions under which we can preserve an upper bound on log Zθ,φ(x) (and lower
48
Published as a conference paper at ICLR 2022
bound on iBAL(qθ, Tφ)) by sampling from p(z∣x) instead of ∏θ,φ(z∣x) to initialize our backward
annealing chains in Prop. M.1 below.
Using the single sample AIS bounds to estimate log Zθ,φ(x), we have the following extended state
space proposal and target distributions,
TT
PAGSTn(Z0：T|x) := ∏θ,φ(zτ|x) Y T(Zt-ι∣Zt),	qARO,π(z0:T|x):= qθ(zo|x) Y Tt(ZtIzt-i).
t=1	t=1
(192)
We emphasize that in pTAGIST,π , qPARISO,Pπ, the transition kernels and intermediate densities are based on the
critic Tφ(x, Z) and energy-based posterior πθ,φ(ZIx) whose log partition function we seek to estimate.
Recall from Sec. 2.1 that taking the expected log importance weights under PAGSTn (zo：T ∣x) yields an
upper bound on log Zθ,φ (x). However, since it is difficult to draw exact samples from πθ,φ(ZIx) to
initialize backward annealing chains and sample fromPTAGIST,n(Z0:TIx), we instead consider sampling
from the posterior p(z∣x). Using the same transition kernels T as above, we first define the conditional
distribution PTAGIST,n (Z0:T-1 Ix, ZT) of the backward chain for a given ZT
T
PTAGIST,n (Z0:T -1 Ix, ZT) := YT(Zt-ι∣Zt).	(193)
t=1
We can then define the approximate extended state space target distribution which samples ZT 〜
P(ZIx)
T
PAGTROX(Zo：T|x) := p(zt|x) YT(Zt-ι∣Zt) = p(zt∣x)PAGSTn(zo：T-i∣x,ZT).	(194)
t=1
Using this notation, we can also writePTAGIST,n(Zo：TIx) = πθ,φ(ZT Ix)PTAGIST,n(Zo：T-1 Ix, ZT) in Eq. (192).
We now characterize the conditions under which sampling from PTAGPPTROX(Zo：T) preserves an upper
bound on log Zθ,φ(x).
Proposition M.1. Define the AIS marginal distribution qPARISO,Pn(ZTIx) over the final state in the
extended state space proposal as qPARISO,Pn(ZTIx) := qPARISO,Pn (Zo：T Ix)dZo：T -1. If we have
DKL[P(ZT Ix)kqPARISO,Pn(ZT Ix)] ≥ DKL[P(ZTIx)kπθ,φ(ZTIx)] ,	(195)
then initializing the backward AIS Chain using ZT 〜p(z∣x) (i.e., sampling underPAGTROX(zo：T|x)),
yields an upper bound on log Zθ,φ (x),
AIS,n
PTG T (x, Zo：T )
EpAGTROX(zo：T|x) [log qAIS,n(Z0 τ∣x) ] ≥ log Zθ,φ(X) .	(196)
Proof. We begin by writing several definitions, which will allow us to factorize the extended state
space proposal qPARISO,Pn(Zo：TIx) in the time-reversed direction. This factorization includes the final AIS
marginal qPARISO,Pn(ZTIx).
Starting from Eq. (192), the forward transitions qPARISO,Pn (Zo：T Ix) = qθ(ZoIx) QtT=1 Tt(ZtIZt-1) induce
the marginal distributions qPARISO,Pn(ZtIx) at each step. The forward transitions and marginals induce a
posterior kernel Tq (zt-ι |zt), which allows US to rewrite qAROfo：T |x) using a reverse factorization
T
qARO,π(zo：T|x)= qAROn(zt|x) YTq(z-∣Zt),	(197)
t=1
where
Tq (Zt-1∣Zt)
qARO,π(zt-ι∣x)7t(zt∣Zt-ι)
qARO,π(zt∣x)
(198)
q
The posterior reverse transitions Ttq(Zt-1 IZt) are intractable in practice, and cannot be simplified
to match the kernels in the target distribution PAGSTn(zo：T-ι∣x, ZT) = QT=I T(Zt-1 |zj using the
49
Published as a conference paper at ICLR 2022
invariance or detailed balance conditions. Doucet et al. (2022) provide a promising approach using
q
score matching to approximate these posterior transitions Ttq(zt-1 |zt).
Finally, we write the posterior reverse process conditioned on a particular zT as
T
qPRO,P (Z0：T-ι∣x,zτ) := YTq(zt-ι∣zt).	(199)
t=1
With the goal of upper bounding log Zθ,φ(x) = J qθ(z∣x)eTφ(X,z)dz, We consider the
log importance weights with expectations under the target distribution pTAGPPTROX(z0:T|x) =
P(ZT |x)PAGSTn(Z0:T-1 |x, zT), as in Eq. (196). Using the above notation, We have
AIS,π	AIS,π
EpTAGPPTROX (z0:T |x) log %,∏(x,z0:T) = log Zθ,φ(x) + EpAGTROX(z0:T |x) log PAGTn (z0Tlx)	(200)
qPROP (z0:T |x)	qPROP (z0:T |x)
Tc OZ &—E	Lo∙ πθ,φ(ZT IX)PAGS,π (Z0:T-1∣X,ZT) P(ZT IX) ] ”01、
=log Zθφ(x) + EpAPTROX(Z0Mx) [lθg 痔(ZT∣x)qAROF(Z0:T-1∣X,ZT) P(ZTlX) J (201)
= log Zθ,φ(x) + DKLP(ZT Ix)kqPARISO,Pπ(ZT Ix) - DKL P(ZT Ix)kπθ,φ (ZT Ix)	(202)
+ Ep(zT |x) DKL [PTAGIST,π (Z0:T -1Ix, ZT)kqPRO,P (Z0:T -1Ix, ZT)]
The intractable KL divergence TKL[pAGSTπ(z0：T-ι∣x,ZT)kqARO，n(zo：T-ι|x,ZT)] compares the re-
q
verse kernels in the target distribution Tt(zt-1|zt) against the posterior Ttq(zt-1 |zt). Ignoring this
nonnegative term, We can loWer bound the expectation on the lhs
AIS,π
EpAPTROX log PTGT,π(Z,：；)	≥ logZθ,φ(x) + DKL [p(zt|x)IIqAROPr(ZT|x)] - DKL [p(zt∣x)∣∣∏θ,φ(zt|x)].
Finally, under the assumption of the proposition that TKL [p(zt|x)|IqAROn(ZT|x)]	≥
TKL [p(zT∣x)k∏θ,φ(zT|x)], we have the desired result.	□
KL Divergence Condition In Prop. M.1, We have shoWn that We can preserve an upper bound on
log Zθ,φ(x) by initializing the reverse chain using a posterior sample, under a condition on the KL
divergence,
Tkl[p(zt∣x)kqARO,n(zT|x)] ≥ TKL [p(zT∣x)k∏θ,φ(ZT|x)].	(203)
While we cannot guarantee this condition, we intuitively expect Eq. (203) to hold in practice since
πθ,φ (z|x) has been directly trained to match P(z|x). By contrast, qPARISO,Pn(zT |x) is the final state of an
AIS procedure, which approximates ∏θ,φ(z∣x) and does not have access to information about p(z∣x).
Burda et al. (2015) use a similar approach for lower bounding the log likelihood in ebms, but give an
example of a Restricted Boltzmann Machines (rbm) model (in their Sec. 5) in which Eq. (203) does
not hold.
As desired, we find in our experiments in Fig. 7 that our approximate reverse annealing procedure
underestimates the ibal in all of the mine-ais, giwae and InfoNCE experiments, for all numbers
of intermediate distributions T .
T = 1 Special Case For T = 1, we have qPARISO,Pn(z1|x) = qθ(z1 |x). In particular, Prop. M.1 will
provide an upper bound on log Zθ,φ(x) if
Tkl[p(z∣x)∣qθ(z|x)] ≥ Tkl[p(z∣x)∣∏θ,φ(z∣x)] .	(204)
This condition is guaranteed under the assumptions of Prop. L.1, where IBAL(qθ, Tφ*) = IBAL (∏θ,φ* )
improves upon IBAL (qθ) for an energy function Tφ* (x, z) which has been trained to maximize the
ibal. In Eq. (204) the kl divergence on the left-hand side corresponds to the gap in the ba lower
50
Published as a conference paper at ICLR 2022
7 6 5 4 3 2 1
2 2 2 2 2 2 2
UoHeuJ」咨二 enw
(a) Linear VAE
MNIST-VAE
UO-E^S'BM
Nurriwr of AJS ŋɪstributlvis
(b) MNIST VAE
110
105
100
95
90
85
80
75
S 7。
S 65
g 60
f 55
⅞ 50
S 45
ɪ 40
35
30
25
20
15
10
5
0
MNIST-GAN
NumberofAIS Distributions
(c) MNIST GAN
Figure 7: Estimating the mutual information of deep generative models with ais and mine-ais.
bound, which is larger than the gap in the ibal on the right-hand side. We can further show that the
lower bound on IBAL(qθ, Tφ) resulting from Prop. M.1 with T = 1 is the BA lower bound,
IBAL(qθ , Tφ ) = Ep(x,z)	log	qθ(z∣χ)^ p(z).	+ Ep(x,z) [Tφ(x, z)]	- Ep(x) [log Zθ,φ(χ)]	
≥ Ep(x,z)	log	qθ(z∣χ)^ p(z).	+Ep(x,z)[Tφ(χ,z)]	E APPROX pTGT	AIS,π l	PTGT(X, z0:T) .°g qAO,π(zo：T|x).
= Ep(x,z)	log	qθ(z∣χ)^ p(z).	+Ep(x,z)[Tφ(χ,z)]	- Ep(x,z)	: qθ (z∣x)eTφ(x,z)^ log	— L	qθ(z∣χ)」
= Ep(x,z)	log	”(z∣χ)- p(z).	+ JEp(xτz)[τφ(x7zJΓ	-JE2f(χz)∙[2φ(XTz)T	
= IBAL (qθ ) .					
We can confirm this in Fig. 7, where for T = 1, the approximate lower bounds on IBAL(qθ , Tφ)
begin from the appropriate ba lower bound. For example, in the case of ibal evaluation for giwae
(K = 100), the light green curve starts from the BA lower bound term reported in the decomposition
of the GIWAE (K = 100) lower bound in Fig. 3a. Using more intermediate distributions (T > 1)
for the ais approximate bound in Prop. M.1, the estimates in Fig. 7 approach the true value of
IBAL(qθ, Tφ) in all cases.
M.3 MINE-AIS EXPERIMENTS
In Fig. 7, we assess the performance of our mine-ais method, which learns an energy function
Tφ and uses Multi-Sample AIS to evaluate lower and upper bounds on the IBAL(qθ, Tφ), in cases
where p(x|z) is unknown. As a comparison, we consider Multi-Sample AIS lower and upper bounds
with known p(x|z), since these estimators tightly bound the ground truth MI and correspond to
the optimal energy function in MINE-AIS (Prop. 4.1). We train MINE-AIS using IBAL(p(z), Tφ) for
p(z) = N(0, I), and plot our Multi-Sample AIS lower and upper bounds on the IBAL as a function of
the number of intermediate AIS distributions T used for evaluation.
Note that our lower bound on ibal, which uses the approximate reverse annealing procedure from
App. M.2, preserves a lower bound on MI for all values of T. The upper bound on IBAL may be used
to validate the convergence of the Multi-Sample ais evaluation procedure. For example, in Fig. 7, we
observe that our lower and upper bounds converge to the same estimate for high T , finding the true
value of IBAL(p(z), Tφ).
We find that MINE-AIS underestimates the ground truth MI by 11%, and 24% on MNIST-VAE and
mnist-gan, respectively. We also compare against the ba lower bound with a Gaussian variational
family for qθ(z∣x). Note that the IBAL corresponds to the BA lower bound for a more flexible,
energy-based posterior approximation ∏θ,φ(z∣x), and both bounds assume access to only a single
known marginal distribution. We find that MINE-AIS outperforms the BA baseline by 2%, 52%, and
90%, on linear VAE, MNIST-VAE, and MNIST-GAN, respectively.
51
Published as a conference paper at ICLR 2022
We also use Multi-Sample AIS to evaluate the IBAL corresponding to (qθ, Tφ), which are learned by
optimizing the GIWAE (with K = 100) and INFONCE (with K = 100 and qθ = p(z)) lower bounds.
As argued in Prop. L.2, the IBAL corresponds to the limiting behavior of GIWAE as K → ∞. We
observe that the ais evaluation of the ibal corresponding to giwae or InfoNCE critic functions
only marginally improves upon evaluation of the original giwae or InfoNCE lower bounds. This
indicates that the improvement of mine-ais over giwae or InfoNCE can be primarily attributed
to learning a better critic function using energy-based training. For these evaluation tasks, the
approximate reverse annealing procedure from App. M.2 also underestimates the ibal for all values
of T.
N Applications to Mutual Information Estimation without Known
Marginals
While the focus in of our work is evaluating the mutual information I(x; z) in settings where at
least a single marginal is available, we are often interested in estimating or optimizing the mutual
information where no marginal distribution is available. A natural setting where no marginal distri-
bution is available is representation learning where the goal is to maximize the mutual information
between the data distribution qdata(x) and the representation induced by a stochastic mapping qψ (z|x)
parameterized by ψ,
I(X; Z) = Eqdata(x)qψ(ZIxUlog qdata(χ)qψ (Z) J = Eqdata(X)qψ(ZIx)K ⅛)]	(205)
where qψ(z) = J qdata(x)qψ(z∖x)dx represents the “aggregated posterior” (Makhzanietal., 2015) or
induced marginal distribution over Z.
N.1 BA Lower Bound
Using the notation of Eq. (205), we can write the ba lower bound as
I(x; z) ≥ I(x; z) - Eqψ(Z) DKL[qψ(x∖z)kpθ(x∖z)]
、---------------------------------V--------------}
gap
pθ(x∖z)
=Eqdata(X)Eqψ(ZIx) log qdag(x)
Eqdata(x) Eqψ(ZIx) logpθ(x∖z) +Hdata(x)
I
}
negative reconstruction loss of x
|-----V-----}
constant
: IBAL(pθ(x∖z))
(206)
(207)
(208)
where pθ(x∖z) is a variational distribution, parameterized by θ, that tries to match to the inverse
encoding distribution qψ (x∖z) H qdata(x)qψ (z∖x). The first term in Eq. (207) can be interpreted as
the reconstruction term, and the second term is the entropy of the data distribution, which is constant.
Evaluating MI up to a Constant Note that the gradient of IBAL (pθ(x∖z)) with respect to the
parameters θ does not depend on the marginal qdata(x), and thus we may still optimize the variational
distribution when the data distribution is unknown. We can then use the resulting pθ(x∖z) to estimate
the BA lower bound on mutual information up to a constant, Hdata(x). This is useful in comparing
the MI induced by two different representations qψ1 (z∖x) and qψ2 (z∖x) of the same data distribution.
Optimizing MI with the BA Lower Bound The BA lower bound is also amenable to backprop-
agation through the parameters of the encoding distribution qψ(z∖x), even when analytic marginal
densities for qdata(x) or qψ(z) are not available.
Maximization of the ba lower bound appears in various settings, including in representation learning
(Alemi et al., 2016; 2018), reinforcement learning (Mohamed & Rezende, 2015), improving inter-
pretability in gans (Chen et al., 2016), and variational information bottleneck methods (Tishby et al.,
2000; Alemi et al., 2016; 2018).
52
Published as a conference paper at ICLR 2022
N.2 GIWAE LOWER BOUND
In this section, we discuss the applicability of our giwae lower bound in mutual information
maximization settings. Rewriting the GIWAE lower bound in Eq. (7) for the case of estimating I(x; z)
in Eq. (205), we have
eTφ(x,z)
I(X⑶ ≥%ata(x)qψ (zlx)[lθg pθ3zH + Hdata(X) + Eqψ(x,z)包 3 (χ(k)∣z) [log T^φ^77PK7^Tφ^W^
|	{ .	} |{,} |	=	,	,
negative reconstruction loss of x	constant	contrastive term≤log K
(209)
=: IGIWAEL (pθ (X|z), Tφ, K)
Note that We can use a single joint sample from qdata(x)qψ (z|x) = qψ (z)qψ (x|z) to obtain a positive
sample from the inverse encoding distribution X 〜qψ (x|z) for a particular Z 〜qψ (z), in a similar
fashion to our ancestral sampling in Sec. 1.1. For a given z, the negative samples correspond to K - 1
samples from the stochastic decoder x(2:K)〜pθ(x|z).
From the importance sampling perspective, We can vieW the giwae loWer bound as corresponding to
an upper bound on the “log partition function” log qψ (Z) for a particular Z 〜qψ (z). The contrastive
term in Eq. (209) arises from SNIS sampling of a single X(s) in the extended state space proposal,
with qGRWAE(s|z, X(LK)) = eTφ(X(S)，z)/PK=I eτΦ(X(k)，z).
Finally, we note that optimization over the energy function Tφ(x, z) improves upon IBAL (pθ(XIZ)) in
Eq. (207) by at most log K nats using the contrastive term.
Evaluating MI up to a Constant Similar to the BA bound, the GIWAE lower bound can be used to
evaluate the mutual information up to the constant data entropy when qdata(X) is unknown. This may
be useful in comparing the MI induced by two different representations qψ1 (Z|X) and qψ2 (Z|X) of
the same data distribution. Note that the reconstruction and contrastive terms in Eq. (209) depend
on samples from qdata(X) and not the density. Thus, our ability to take gradients with respect to the
parameters of the variational distribution pθ (X|Z) or energy function Tφ(X, Z) are not affected by the
fact that the marginal distribution is unknown.
Optimizing MI with the GIWAE Lower Bound The giwae lower bound may also be used for
mutual information maximization with respect to the parameters of qψ(z∣x), since each term in
Eq. (209) is amenable to backpropagation. Since giwae generalizes both the ba and Info-NCE
lower bounds, it can be used as a drop-in replacement for either of these bounds for optimizing mi
(e.g., see van den Oord et al. (2018)).
N.3 MINE-AIS / IBAL Lower Bound
Recall that mine-ais estimation would involve an energy based variational approximation to the
inverse encoding distribution qψ (x∣z),
∏θ,φ(x∣z) = ^~θ Pθ (x∣z)eTφ(χ, Z)
Z(z)
(210)
Note that IBAL lower bound on I(x; z) involves an intractable log partition function term
Eqψ(z) [log Z(z)] = Eqψ(z) log Epθ (X|z) eTφ(X,z) .
I(x; z) ≥ Eqdata(x)
Eqψ(z|x) [logPθ(x|z)] + Hdata(x) +
s-----------------{z----------------} V--------{z-----}
negative reconstruction loss of x constant
eTφ(x,z)
Eqψ(x,z) K Epθ(x∣z) [eτΦ(χ,z)]
S---------------------V--------------------}
COntraStive term≤Eqψ (Z) [dkl [qψ (XIz) ∣∣pθ (XIz)]]
(211)
IBAL(pθ (x∣z),Tφ)
where the term in the denominator is the partition function Z(z) = Epg(χ∣z) [eTφ(x,z)].
53
Published as a conference paper at ICLR 2022
a 丽 a⅜
When taking gradients as in Eq. (19)-(20), we obtain
∂∂
IBAL(Pθ, TΦ) = Eqdata(χ)qψ(z∣χ) ∂θ logpθ(XIz) ― Eqψ(z)∏θ,φ(x∣z) ∂θ logpθ(XIz) ， (212)
∂∂
IBAL(Pθ, τφ) = Eqdata(x)qψ(z|x) ∂φTΦ(x, z) ― Eqψ(z)∏θ,φ(x∣z) ∂φTΦ(x, z) .	(213)
To obtain approximate negative samples from πθ,φ(XIz) for a given z, we can use MCMC transition
kernels since the unnormalized target density ∏θ,φ(x∣z) = pθ(x∣z)eTφ(x,z) is tractable.
Evaluating MI up to a Constant Since only samples from qdata(X) are required for the MINE-AIS
training procedure in Eq. (212)-(213), we can learn the base variational distribution Pθ(XIz) and
energy function τφ(X, z) in cases when the marginal distribution qdata(X) is unknown.
As in the case of giwae and ba lower bounds above, we can also evaluate the ibal lower bound in
Eq. (211) up to a constant. As in the main text, this involves using multi-sample ais techniques to
bound the intractable log partition function term log Z(z) = log Epg(χ∣z) [eTφ(x,z)].
Optimizing MI with the MINE-AIS Lower Bound If we are interested in optimizing the mine-
AIS lower bound with respect to the parameters of a stochastic encoder qψ(zIX), we would need
to backpropagate through the Multi-Sample AIS procedure used to estimate the log Z(z) term, but
computing the gradients are intractable. We thus conclude that, among our proposed methods, giwae
is the most directly applicable in settings of mi for representation learning.
O	Experimental Details
O. 1 Experiment Details of Sec. 5.2
In this section, we provide the experiment details used in Sec. 5.2. For more details, see the public
GitHub repository, https://github.com/huangsicong/ais_mi_estimation.
O.1.1 Datasets and Models
We used mnist (LeCun et al., 1998) and cifar-10 (Krizhevsky & Hinton, 2009) datasets in our
experiments.
Real-Valued MNIST For the VAE experiments on the real-valued MNIST dataset (Table 1), the
encoder’s architecture is 784 - 1024 - 1024 - 1024 - z, where z is the latent code size shown in
the row header of Table 1. The decoder architecture is the reverse of the encoder architecture. The
decoder variance is learned scalar.
For the gan experiments on mnist (Table 1), we used the same decoder architecture as our vaes. In
order to stabilize the training dynamics, we used the gradient penalty (GP) (Salimans et al., 2016).
The network was trained for 300 epochs with the learning rate of 0.0001 using the Adam opti-
mizer (Kingma & Ba, 2014), and the checkpoint with the best validation loss was used for the
evaluation.
CIFAR-10 For the CIFAR-10 experiments (Table 1), we experimented with a smaller version of
dcgan (Radford et al., 2015) (see the public code). The number at the end of each model name in
Table 1 indicates the latent code size.
O.1.2 Experiment Details of Sec. 5.2
For the ais temperature schedule, We used sigmoid schedules as used in Wu et al. (2016). The step
size of HMC was adaptively tuned to achieve an average acceptance probability of 65% as suggested
in Neal (2001). For all mnist experiments in Table 1, we evaluated on a single batch size of 128
simulated data. For all cifar experiments in Table 1 we used a single batch of 32 simulated data. All
experiments are run on on Tesla P100 or Quadro RTX 6000 or Tesla T4 GPUs.
54
Published as a conference paper at ICLR 2022
O.1.3 Runtime Comparison
We benchmarked the runtime on Tesla P100 GPUs. For mnist, it took about 35 minutes to run iwae
with K = 1M , about 8 hours to run AIS with T = 30K . For CIFAR, it took about 45 minutes to run
IWAE with K = 1M, and about 12 hours for the AIS with T = 100K.
In Fig. 8, we evaluate the tradeoff between runtime and bound tightness for evaluating the generative
mi for vae and gan models with 100-dimensional latent codes trained on the mnist dataset. We
compare iwae and multi-sample ais evaluation, with the same experimental settings as in App. O.1.2
and an initial distribution q® (z|x). We plot wall clock time on the x-axis, where increasing runtime
reflects increasing K for IWAE and increasing T for AIS.
IWAE Lower Bound
IWAE Upper Bound
Multi-SamPle AIS Lower Bound
Multi-SamPle AIS Upper Bound
100
200
175
150
IWAE Lower Bound
WAE Upper Bound
MUlti-SamPle AIS Lower Bound
MUlti-Sample AIS Upper Bound
(a) VAE-100
Figure 8: Runtime vs. Bound Tightness for vae (left) and gan (right) models on mnist.
Minutes
(b) GAN-100
O.2 Experiment Details for Energy-Based B ounds (Sec. 5.2)
Models and Data The linear VAE model has a Gaussian prior Z 〜N(0, I) with the dimension of
10. The dimension of the output x is 100. The weights are sampled randomly from a Gaussian
distribution with the standard deviation of 1, and the standard deviation of the Gaussian observation
noise at the output is 1. The MNIST-VAE20, is a VAE model that is trained on the real-valued MNIST
dataset. It has a Gaussian prior Z 〜N(0, I) with the dimension of 20. The decoder has one layer
of ReLU non-linearity of size 1000, followed by a linear layer that predicts the mean of a Gaussian
observation model with the fixed standard deviation of 0.1. MNIST-GAN20 decoder uses one layer of
ReLU non-linearity of size 1000 followed by a sigmoid layer that predicts the mean of a Gaussian
observation model with the fixed standard deviation of 0.1. In all the experiments, we use a fixed
batch of 100 data points.
BA, IWAE and GIWAE For the BA bound, qθ (Z|x) is parameterized using a neural network that
predicts the mean and log-std of a diagonal Gaussian distribution. The neural network has two layers
OfReLU non-linearity of size 2000. The IWAE experiments used the same architecture for qθ(z|x).
The GIWAE experiments, in addition to qθ (z|x) with the same architecture, used a critic function
Tφ(x, Z) that is parameterized by a neural network that concatenates (x, Z) and pass them through
two layers of ReLU non-linearity of size 2000, followed by a linear layer that outputs a scalar value.
The parameters of qθ (z|x) and Tφ(x, Z) are trained jointly.
Multi-Sample AIS For the Multi-Sample AIS bounds, for all models, we used up to K = 1000
chains (see Fig. 5), and up to T = 50K intermediate distributions with linear schedule (see Fig. 5
and Fig. 7). We used HMC as the AIS kernel, with = 0.02 and L = 20 leap frog steps.
MINE-AIS For the training of the MINE-AIS, we used a critic function Tφ(x, Z) that is parameterized
by a neural network that concatenates (x, Z) and pass them through three layers of ReLU non-linearity
of size 2000, followed by a linear layer that outputs a scalar value. We chose the Gaussian prior
N(0,1) as the base distribution qθ(z|x). In order to take a sample from ∏θ,φ(z∣x), We used the
HMC method that is initialized with a true posterior p(Z|x) sample, with M = 10 iterations each
with L = 20 leapfrog steps. For the step size of HMC, we used = 0.05 for the linear VAE model
55
Published as a conference paper at ICLR 2022
and = 0.02 for the MNIST-VAE20 and MNIST-GAN20. For the evaluation of MINE-AIS, we used
Multi-Sample ais with the same parameters as the ais evaluation experiments described above.
O.3 Analytical Solution of the Mutual Information on the Linear mnist-vae
In order to verify our implementations, we have derived the mi analytically for the linear VAEs
and verified that it matches the mi estimated by ais. For simplicity, we assume a fixed identity
covariance matrix I at the output of the conditional likelihood of the linear VAE decoder, i.e., the
decoder of the VAE is simply: x = Wz+b+, where x is the observation, z is the latent code vector
Z 〜N(0, I), W is the decoder weight matrix and b is the bias. The observation noise of the decoder
is E 〜N(0, I). It is easily shown that the conditional likelihood is p(x∣z) = N(x| Wz + b, I) and
thus we can solve for the marginal
P(X) = N(x∣μχ = b, Σχ = I + WW|).	(214)
The differential entropy of x is:
H(X) = 2 + 2 log(2∏) + 1 log(det Σχ),	(215)
where k is the dimension of the observation. The conditional entropy is
H(XIZ) = 2 + 2 log(2∏) + 1 log(det I)	(216)
=2 + 2log(2π).	(217)
Thus, the mutual information is
I(X; Z) = H(X) - H (X|Z)	(218)
=1 log(det Σχ).	(219)
O.4 Confidence Intervals for Multi-Sample AIS Experiments
Table 4 and Table 5 provides the 95% confidence intervals for the AIS results reported in Table 1. The
confidence intervals were computed over confidence interval over 8 batches each with 16 data points
for mnist; and over 8 batches 4 data points for cifar.
56
Published as a conference paper at ICLR 2022
Method I Proposal			VAE10	VAE100	GAN10	GAN100
	P(Z)	UB	(1849.03,2010.66)	(5564.53,6096.51)	(745.68,826.55)	(795.81, 926.94)
AIS T=1		LB	(0.00,0.00)	(0.00, 0.00)	(0.00,0.00)	(0.00, 0.00)
		UB	(59.34,66.66)	(345.45,378.80)	(293.60,335.83)	(482.40, 544.26)
	q(ZIx)	LB	(60.09,66.6l)	(32.46,36.52)	(3.41,3.94)	(2.40, 2.83)
	P(Z)	UB	(38.61,39.57)	(92.32,98.03)	(21.91,23.04)	(26.89, 28.22)
AIS T=500		LB	(33.69,34.4l)	(77.77,82.02)	(21.18,21.96)	(25.36, 26.36)
		UB	(33.94,34.63)	(79.96,84.71)	(22.53,23.58)	(28.82, 30.24)
	q(ZIx)	LB	(33.80,34.53)	(78.01, 82.37)	(21.19,22.00)	(25.09, 26.06)
	P(Z)	UB	(33.82,34.60)	(78.62,83.07)	(21.54,22.50)	(25.97, 27.07)
AIS T=30K		LB	(33.86,34.56)	(78.51, 83.05)	(21.50,22.45)	(25.96, 26.98)
		UB	(33.85,34.58)	(78.56,83.03)	(21.54,22.47)	(26.01, 27.07)
	q(ZIx)	LB	(33.86,34.56)	(78.52,83.02)	(21.55,22.48)	(26.02, 27.03)
	P(Z)	UB	(3628.86,4026.29)	(10705.98,12297.86)	(1556.00,1704.00)	(1680.50, 1800.28)
IWAE K=1		LB	(0.00,0.00)	(0.00, 0.00)	(0.00,0.00)	(0.00, 0.00)
		UB	(34.82,35.86)	(92.82,98.44)	(38.80,76.14)	(243.34, 278.41)
	q(ZIx)	LB	(24.54,25.86)	(41.53,47.54)	(3.85,4.61)	(2.94, 3.52)
	P(Z)	UB	(1132.54,1262.97)	(3987.51,4480.86)	(430.40,463.19)	(462.95, 526.51)
IWAE K=1K		LB	(6.91,6.91)	(6.91,6.91)	(6.91,6.91)	(6.91, 6.91)
		UB	(33.87,34.61)	(83.15,87.46)	(34.11,71.36)	(182.82, 219.55)
	q(ZIx)	LB	(31.08,32.30)	(48.44,54.45)	(10.75,11.52)	(9.84, 10.43)
	P(Z)	UB	(352.91,400.87)	(2078.30,2417.17)	(71.93,91.09)	(100.27, 127.75)
IWAE K=1M		LB	(13.82,13.82)	(13.82, 13.82)	(13.79,13.82)	(13.82, 13.82)
		UB	(33.87,34.57)	(81.42,85.36)	(28.51,33.26)	(52.41, 63.68)
	q(ZIx)	LB	(33.67,34.53)	(55.35,61.36)	(17.31,18.21)	(16.63, 17.32)
Table 4: Confidence intervals of ais and iwae estimates of mi on mnist. UB stands for Upper
Bound, and LB stands for Lower Bound.
Model	Proposal	GAN10	GAN100
	-二~UB P(Z)	LB	(3593559.12,4477712.38)	(4038603.09, 5668217.91)
AIS (T=1)		(0.00, 0.00)	(0.00, 0.00)
	-~UB q(ZIx) LB	(241342.62, 566015.82)	(1874938.58, 2881576.42)
		(15.70,18.89)	(18.41, 21.93)
	-二~UB P(Z)	LB	(24683.10,41496.70)	(25453.02, 101127.79)
AIS (T=500)		(28.41,30.63)	(98.37, 110.65)
	UB q(ZIx) LB	(116.02,156.28)	(1603.13, 3969.92)
		(45.45, 50.88)	(134.35, 156.02)
	-二~UB P(Z)	LB	(72.17, 75.80)	(479.04, 497.10)
AIS (T=100K)		(70.19, 73.55)	(470.05, 490.47)
	UB q(ZIX) LB	(71.87, 75.22)	(475.07, 494.61)
		(71.35, 74.75)	(468.64, 489.89)
	-二~UB P(Z)	LB	(6019901.15,9511489.86)	(7339411.95, 12492792.05)
IWAE		(0.00, 0.00)	(0.00, 0.00)
(K=1)	UB q(ZIX) LB	(75.00,80.03)	(-1810.70, 12504.41)
		(16.28,18.62)	(18.39, 21.60)
	-二~UB P(Z)	LB	(1673333.03, 2415008.47)	(2181660.84, 3531768.16)
IWAE (K=1K)		(6.91,6.91)	(6.91, 6.91)
	UB q(ZIX) LB	(72.31, 75.68)	(-1872.12, 12438.39)
		(22.18, 24.99)	(25.32, 28.64)
	-匚~UB P(Z)	LB	(582880.62,838142.63)	(1479829.44, 2327879.56)
IWAE (K=1M)		(13.82,13.82)	(13.82, 13.82)
	UB q(ZIX) LB	(71.63, 75.08)	(-1883.22, 12426.34)
		(29.86,31.60)	(32.21, 35.42)
Table 5: Confidence intervals of ais and iwae estimates of mi on cifar. UB stands for Upper
Bound, and LB stands for Lower Bound.
57