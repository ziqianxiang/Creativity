Published as a conference paper at ICLR 2022
Shallow and Deep Networks are Near-
Optimal Approximators of Korobov Functions
Moise Blanchard
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA, 02139
moiseb@mit.edu
Amine Bennouna
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA, 02139
amineben@mit.edu
Ab stract
In this paper, we analyze the number of neurons and training parameters that a neu-
ral network needs to approximate multivariate functions of bounded second mixed
derivatives — Korobov functions. We prove upper bounds on these quantities for
shallow and deep neural networks, drastically lessening the curse of dimensional-
ity. Our bounds hold for general activation functions, including ReLU. We further
prove that these bounds nearly match the minimal number of parameters any con-
tinuous function approximator needs to approximate Korobov functions, showing
that neural networks are near-optimal function approximators.
1	Introduction
Neural networks have known tremendous success in many applications such as computer vision and
pattern detection (Krizhevsky et al., 2017; Silver et al., 2016). A natural question is how to ex-
plain their practical success theoretically. Neural networks are shown to be universal (Hornik et al.,
1989): any Borel-measurable function can be approximated arbitrarily well by a neural network with
sufficient number of neurons. Furthermore, universality holds for as low as 1-hidden-layer neural
network with reasonable activation functions. However, these results do not specify the needed
number of neurons and parameters to train. If these numbers are unreasonably high, the universality
of neural networks would not explain their practical success.
We are interested in evaluating the number of neurons and training parameters needed to approx-
imate a given function within with a neural network. An interesting question is how do these
numbers scale with and the dimensionality of the problem, i.e., the number of variables. Mhaskar
(1996) showed that any function of the Sobolev space of order r and dimension d can be approxi-
mated within E with a 1-layer neural network with O(E- d) neurons and an infinitely differentiable
activation function. This bound exhibits the curse of dimensionality: the number of neurons needed
for an E-approximation scales exponentially in the dimension of the problem d. Thus, Mhaskar’s
bound raises the question of whether this curse is inherent to neural networks.
Towards answering this question, DeVore et al. (1989) proved that any continuous function approx-
imator (see Section 5) that approximates all Sobolev functions of order r and dimension d within
E, needs at least Θ(e-d) parameters. This result meets Mhaskar's bound and confirms that neural
networks cannot escape the curse of dimensionality for the Sobolev space. A main question is then
for which set of functions can neural networks break this curse of dimensionality.
One way to circumvent the curse of dimensionality is to restrict considerably the considered space of
functions and focus on specific structures adapted to neural networks. For example, Mhaskar et al.
(2016) showed that compositional functions with regularity r can be approximated within E with
,*,C∕T-2∖
deep neural networks With O(d ∙ E-r) neurons. Other structural constraints have been considered
for compositions of functions (Kohler & Krzyzak, 2016), piecewise smooth functions (Petersen &
Voigtlaender, 2018; Imaizumi & Fukumizu, 2019), or structures on the data space, e.g., lying on
a manifold (Mhaskar, 2010; Nakada & Imaizumi, 2019; Schmidt-Hieber, 2019). Approximation
bounds have also been obtained for the function approximation from data under smoothness con-
straints (Kohler & Krzyzak, 2005; Kohler & Mehnert, 2011) and specifically on mixed smooth Besov
1
Published as a conference paper at ICLR 2022
spaces which are known to circumvent the curse of dimensionality (Suzuki, 2018). Another example
is the class of Sobolev functions of order d∕a and dimension d for which Mhaskar's bound becomes
O(-α). Recently, Montanelli et al. (2019) considered bandlimited functions and showed that they
can be approximated within E by deep networks with depth O((log ɪ)2) and O(e-2(log ɪ)2) neu-
rons. Weinan et al. (2019) showed that the closure of the space of 2-layer neural networks with
specific regularity (namely a restriction on the size of the network’s weights) is the Barron space.
They further show that Barron functions can be approximated within E with 2-layer networks with
O(E-2) neurons. Similar line of work restrict the function space with spectral conditions, to write
functions as limits of shallow networks (Barron, 1994; Klusowski & Barron, 2016; 2018).
In this work, we are interested in more general and generic spaces of functions. Our space of inter-
est is the space of multivariate functions of bounded second mixed derivatives, the Korobov space.
This space is included in the Sobolev space but is reasonably large and general. The Korobov space
presents two motivations. First, it is a natural candidate for a large and general space included in the
Sobolev space where numerical approximation methods can overcome the curse of dimensionality
to some extent (see Section 2.1). Second, Korobov spaces are practically useful for solving partial
differential equations (Korobov, 1959) and have been used for high-dimensional function approxi-
mation (Zenger & Hackbusch, 1991; Zenger, 1991). Recently, Montanelli & Du (2019) showed that
deep neural networks with depth O (log ɪ) and O(E- 1 (log ɪ) 3(d- 1) +1) neurons can approximate
Korobov functions within E, lessening the curse of dimensionality for deep neural networks asymp-
totically in E. While they used deep structures to prove their result, the question of whether shallow
neural networks also break the curse of dimensionality for the Korobov space remains open.
In this paper, we study deep and shallow neural network’s approximation power for the Korobov
space and make the following contributions:
1.	Representation power of shallow neural networks. We prove that any Korobov function
can be approximated within E with a 2-layer neural network with ReLU activation function with
O(E-1(log 1) * 1) +1) neurons and O(E- 1 (log ɪ)3d-1) training parameters (Theorem 3.1). We
further extend this result to a large class of commonly used activation functions (Theorem 3.4).
Asymptotically in E, our bound can be written as O(E-1-δ) for all δ > 0, and in that sense breaks
the curse of dimensionality for shallow neural networks.
2.	Representation power of deep neural networks. We show that any function of the Korobov
space can be approximated within E with a deep neural network of depth dlog2(d)e + 1 independent
of E, with non-linear C2 activation function, O(E-2 (log ɪ) 3(d- 1)) neurons and O(E- 1 (log ɪ) 3(d- 1))
training parameters (Theorem 4.1). This result improves that of Montanelli & Du (2019) who con-
structed an approximating neural network with larger depth O (log ɪ log d) -increasing with e- and
larger number of neurons O(E-2 (log ɪ) / 1) +1). However, they used ReLU activation function.
3.	Near-optimality of neural networks as function approximators. Under the continuous func-
tion approximators model introduced by DeVore et al. (1989), we prove that any continuous func-
tion approximator needs Θ(e- 1 (log ɪ) ~-) parameters to approximate Korobov functions within E
(Theorem 5.2). This lower bound nearly matches our established upper bounds on the number of
training parameters needed by deep and shallow neural networks to approximate functions of the
Korobov space, proving that they are near-optimal function approximators of the Korobov space.
Table 1 summarizes our new bounds and existing bounds on shallow and deep neural network ap-
proximation power for the Korobov space, Sobolev space and bandlimited functions. Our proofs
are constructive and give explicit structures to construct such neural networks with ReLU and gen-
eral activation functions. Our constructions rely on sparse grid approximation introduced by Zenger
(1991), and studied in detail in Bungartz (1992); Bungartz & Griebel (2004). Specifically, we use the
sparse grid approach to approximate smooth functions with sums of products then construct neural
networks which approximate this structure. A key difficulty is to approximate the product function.
In particular in the case of shallow neural networks, we propose, to the best of our knowledge, the
first architecture approximating the product function with polynomial number of neurons. To de-
rive our lower bound on the number of parameters needed to approximate the Korobov space, we
construct a linear subspace of the Korobov space, with large Bernstein width. This subspace is then
used to apply a general lower bound on nonlinear approximation derived by DeVore et al. (1989).
2
Published as a conference paper at ICLR 2022
The rest of the paper is structured as follows. In Section 2, we formalize our objective and introduce
the sparse grids approach. In Section 3 (resp. 4), we prove our bounds on the number of neurons
and training parameters for Korobov functions approximation with shallow (resp. deep) networks.
Finally, we formalize in Section 5 the notion of optimal continuous function approximators and
prove our novel near-optimality result.
Space	Nb. of neurons	Nb. of training parameters	Depth	Activation σ	Reference
W r,p		d e-r		 d E- r	1	C∞, non-poly	MhaSkar(1996)
X 2,∞	w-1(log ；) 3(d-)+]	E- 1(lθg : ) ")	2	ReLU-Iike	This paper
X 2,∞	E-2(log ;)——	E-2(log g)吗出	2	Sigmoid-like	This paper
W r,p	b-∙ log ɪ	E- d log ɪ	O(log ɪ)	ReLU	YarOtSky(2017)	二 Liang & Srikant (2016)
Bandlimited functions	E-2 (log ɪ )2	E-2(lθg ɪ)2	O((log ɪ )2)	ReLU	Montanelli et al. (2019)
X 2,∞	E- 1(lθg : ) — +「	E-2(log :) 3d-)	O(logd ∙ log ;)	ReLU	MOntanelli&Du (2019)
X 2,∞	E-"I。；看）3⅜山一	E- 1(lθg g )修	dlog2 d] + 1	C2, non-linear	This paper
Table 1: Approximation results for Sobolev Wr,p and Korobov X2,∞ functions by shallow and deep
networks. Number of neurons and training parameters are given in O notation.
2	Preliminaries
In this work, we consider feed-forward neural networks, using a linear output neuron and a non-
linear activation function σ : R → R for the other neurons, such as the popular rectified unit (ReLU)
σ(x) = max(x, 0), the sigmoid σ(x) = (1 + e-x)-1 or the Heaviside function σ(x) = 1{x≥0}.
Let d ≥ 1 be the dimension of the input. We define a 1-hidden-layer network with N neurons as
x 7→ PkN=1 ukσ(wk>x + bk), where wk ∈ Rd, bk ∈ R for i = 1, . . . , N, are parameters. A neural
network with several hidden layers is obtained by feeding the outputs of a given layer as inputs to the
next layer. We study the expressive power of neural networks, i.e., the ability to approximate a target
function f : Rd → R with as few neurons as possible, on the unit hyper-cube Ω := [0,1]d. Another
relevant metric is the number of parameters that need to be trained to approximate the function, i.e.,
the number of parameters of the approximating network (uk, wk and bk) depending on the function
to approximate. We will adopt L∞ norm as a measure of approximation error.
We now define some notations necessary to introduce our function spaces of interest. For an integer
r, we denote Cr the space of one dimensional functions differentiable r times and with continu-
ous derivatives. In our analysis, we consider functions f with bounded mixed derivatives. For a
multi-index α ∈ Nd, the derivative of order a is Daf := 9，；f , where ∣ɑ∣ι = Pd=1 ∣α∕.
x1 ... xd
Two common function spaces in a compact Ω ⊂ Rd are the Sobolev spaces Wr,p (Ω) of functions
having weak partial derivatives up to order r in Lp(Ω) and the Korobov spaces X r,p(Ω) of functions
vanishing at the boundary and having weak mixed second derivatives up to order r in Lp(Ω).
Wr,p(Ω) = {f ∈	Lp(Ω)	:	Daf ∈	Lp(Ω),∣α∣ι ≤ r},
Xr,p(Ω) = {f ∈	Lp(Ω)	:	f ∣∂Ω =	0,Daf ∈ Lp(Ω), ∣α∣∞	≤ r}.
where ∂Ω denotes the boundary of Ω, ∣α∣ι = Pid=i ∣α∕ and ∣α∣∞ = supi=ι, d ∣α∕ are respec-
tively the L1 and infinity norm. Note that Korobov spaces Xr,p(Ω) are subsets of Sobolev spaces
Wr,p (Ω). For P = ∞, the usual norms on these spaces are given by
|f |wr,p(Ω) := max kDαf k∞ ,	|f |xr,p(Ω) := l max kDaf k∞ ,
∣a∣ι≤r	∣a∣∞≤r
For simplicity, we will write | ∙ ∣2,∞ for | ∙ ∣χ2,∞. We focus our analysis on approximating func-
tions on the Korobov space X2,∞ (Ω) for which the curse of dimensionality is drastically lessened
and we show that neural networks are near-optimal. Intuitively, a key difference compared to the
Sobolev space is that Korobov functions do not have high frequency oscillations in all directions
at a time. Such functions may require an exponential number of neurons Telgarsky (2016) and are
one of the main difficulties for Sobolev space approximation, which therefore exhibits the curse of
3
Published as a conference paper at ICLR 2022
dimensionality DeVore et al. (1989). On the contrary, the Korobov space prohibits such behaviour
by ensuring that functions can be differentiable on all dimensions together. Further discussions and
concrete examples are given in Appendix A.
2.1	The curse of dimensionality
We adopt the point of view of asymptotic results in (or equivalently, in the number of neurons),
which is a well-established setting in the neural networks representation power literature (Mhaskar,
1996; Bungartz & Griebel, 2004; Yarotsky, 2017; Montanelli & Du, 2019) and numerical analysis
literature (Novak, 2006). In the rest of the paper, we use O notation which hide constants in d. For
each result, full dependencies in d are provided in appendix. Previous efforts to quantify the number
of neurons needed to approximate large general class of functions, showed that neural networks and
most classical functional approximation schemes exhibit the curse of dimensionality. For example,
for Sobolev functions, Mhaskar proved the following approximation bound.
Theorem 2.1 (Mhaskar (1996)). Let p, r ≥ 1, and σ : R → R be an infinitely differentiable
activation function, non-polynomial on any interval of R. Let > 0 sufficiently small. For any
f ∈ W r,p, there exists a shallow neural network with one hidden layer, activation function σ, and
O(E-d) neurons approximating f within E for the infinity norm.
Therefore, the approximation of Sobolev functions by neural networks suffers from the curse of
dimensionality since the number of neurons needed grows exponentially with the input space di-
mension d. This curse is not due to poor performance of neural networks but rather to the choice
of the Sobolev space. DeVore et al. (1989) proved that any learning algorithm with continuous pa-
rameters needs at least Θ(e-d) parameters to approximate the Sobolev space Wr,p. This shows that
the class of Sobolev functions suffers inherently from the curse of dimensionality and no continuous
function approximator can overcome it. We detail this notion later in Section 5.
The natural question is whether there exists a reasonable and sufficiently large class of functions
for which there is no inherent curse of dimensionality. Instead of the Sobolev space, we aim to add
more regularity to overcome the curse of dimensionality while preserving a reasonably large space.
The Korobov space X2,∞(Ω)—functions with bounded mixed derivatives—is a natural candidate:
it is known in the numerical analysis community as a reasonably large space where numerical ap-
proximation methods can lessen the curse of dimensionality (Bungartz & Griebel, 2004). Korobov
functions have been introduced for solving partial differential equations (Korobov, 1959; Smolyak,
1963), and have then been used extensively for high-dimensional function approximation (Zenger
& Hackbusch, 1991; Bungartz & Griebel, 2004). This space of functions is included in the Sobolev
space, but still reasonably large as the regularity condition concerns only second order derivatives.
Two questions are of interest. First, how many neurons and training parameters a neural network
needs to approximate any Korobov function within E in the L∞ norm? Second, how do neural
networks perform compared to the optimal theoretical rates for Korobov spaces?
2.2	Sparse grids and hierarchical basis
In this subsection, we introduce sparse grids which will be key in our neural networks construc-
tions. These were introduced by Zenger (1991) and extensively used for high-dimensional function
approximation. We refer to Bungartz & Griebel (2004) for a thorough review of the topic.
The goal is to define discrete approximation spaces with basis functions. Instead of a classical
uniform grid partition of the hyper-cube [0, 1]d involving nd components, where n is the number
of partitions in each coordinate, the sparse grid approach uses a smarter partitioning of the cube
preserving the approximation accuracy while drastically reducing the number of components of the
grid. The construction involves a 1-dimensional mother function φ which is used to generate all the
functions of the basis. For example, a simple choice for the building block φ is the standard hat
function φ(x) := (1 - |x|)+. The hat function is not the only possible choice. In the latter proofs
we will specify which mother function is used, in our case either the interpolates of Deslauriers &
Dubuc (1989) (which we define rigorously later in our proofs) or the hat function φ which can be
seen as the Deslaurier-Dubuc interpolet of order 1. These more elaborate mother functions enjoy
more smoothness while essentially preserving the approximation power.
4
Published as a conference paper at ICLR 2022
φ3,1 φ2,1 φ3,3 φ1,1 φ3,5 φ2,3 φ3,7
01
Figure 1: Hierarchical basis from the sparse grid construction using the hat function (1 -∣∙∣)+.
Assume the mother function has support in [-k, k]. Forj = 1, . . . , d, it can be used to generate a set
of local functions φj,j : [0,1] —→ R for all j ≥ 1 and 1 ≤ j ≤ 2lj - 1 with support [ j-k, jjk]
as follows, φlj,ij (x) := φ(2lj x - ij), x ∈ [0, 1]. We then define a basis of d-dimensional functions
by taking the tensor product of these 1-dimensional functions. For all l, i ∈ Nd with l ≥ 1 and
1 ≤ i ≤ 2l - 1 where 2l denotes (2l1 , . . . , 2ld), define φl,i(x) := Qjd=1 φlj,ij (xj), x ∈ Rd. For a
fixed l ∈ Nd , we will consider the hierarchical increment space Wl which is the subspace spanned
by the functions {φl,i : 1 ≤ i ≤ 2l - 1}, as illustrated in Figure 1,
Wl := span{φl,i, 1 ≤ i ≤ 2l - 1, ij odd for all 1 ≤ j ≤ d}.
Note that in the hierarchical increment Wl , all basis functions have disjoint support. Also, Korobov
functions X2,p(Ω) can be expressed uniquely in this hierarchical basis. Precisely, there is a unique
representation of u ∈ X2,p(Ω) as u(x) = Pli vι,iφι,i(x), where the sum is taken over all multi-
indices l ≥ 1 and 1 ≤ i ≤ 2l - 1 where all components of i are odd. In particular, all basis
functions are linearly independent. Notice that this sum is infinite, the objective is now to define a
finite-dimensional subspace of X2,p(Ω) that will serve as an approximation space. Sparse grids use
a carefully chosen subset of the hierarchical basis functions to construct the approximation space
Vn() := L∣l∣ι≤n+d-1Wl . When φ is the hat function, Bungartz and Griebel Bungartz & Griebel
(2004) showed that this choice of approximating space leads to a good approximating error.
Theorem 2.2 (Bungartz & Griebel (2004)). Let f ∈ X 2,∞(Ω) and f1，be the projection of f on the
subspace Vn(I). Wehave, kf — f∖1)k∞ = O (2-2nnd-1 . Furthermore, if vl,i denotes the coefficient
of φι,i in the decomposition of fn1) in Vn(I), then we have the upper bound | vl,i | ≤ 2-d2-2lll1 |f ∣2,∞,
for all l, i ∈ Nd with |l|1 ≤ n + d - 1, 1 ≤ i ≤ 2l - 1 where i has odd components.
3	The Representation Power of S hallow Neural Networks
It has recently been shown that deep neural networks, with depth scaling with , lessen the curse of
dimensionality on the numbers of neuron needed to approximate the Korobov space Montanelli &
Du (2019). However, to the best of our knowledge, the question of whether shallow neural networks
with fixed universal depth — independent of and d — escape the curse of dimensionality as well
for the Korobov space remains open. We settle this question by proving that shallow neural networks
also lessen the curse of dimensionality for Korobov space.
Theorem 3.1. Let e > 0. For all f ∈ X2,∞(Ω), there exists a neural network with 2 layers,
1	1 3(d-1) ∣1	1	1 3(d-1)
ReLU activation, O(e-1(log -)-2	+1) neurons, and O(e-2 (log -)-2-) training parameters
that approximates f within e for the infinity norm.
In order to prove Theorem 3.1, we construct the approximating neural network explicitly. The first
step is to construct a neural network architecture with two layers and O(d2 e- 1 log -) neurons that
approximates the product function p : x ∈ [0, 1]d 7-→ Qin=1 xi within e for all e > 0.
Proposition 3.2. For all e > 0, there exists a neural network with depth 2, ReLU activation and
O(d3e-2 log -)neurons, that approximates the product function p : x ∈ [0, 1]d -→ Qid=1 xi
within e for the infinity norm.
Sketch of proof The proof builds upon the observation that p(x) = exp(Pid=1 logxi). We
construct an approximating 2-layer neural network where the first layer approximates log xi for
1 ≤ i ≤ d, and the second layer approximates the exponential. We illustrate the construction in
5
Published as a conference paper at ICLR 2022
Figure 2: Shallow neural network with ReLU activation implementing the product function Qid=1 xi
within e in infinity norm. The network has O(d2e-2 log ɪ) neurons on the first layer and
O(e- 2 log ɪ) neurons on the second layer.
Figure 2. More precisely, fix e > 0. Consider the function h : x ∈ [0, 1] 7→ max(log x, log e).
We approximate he within d with a piece-wise affine function with O(d2e- 1 log ɪ) pieces, then
represent this piece-wise affine function with a single layer neural network he with the same number
of neurons as the number of pieces (Lemma B.1, Appendix B.1). This 1-layer network then has
Ilhe - hek∞ ≤ d. The first layer of our final network is the union of d copies of he: one for each
dimension i, approximating log xi . Similarly, consider the exponential g : x ∈ R- 7→ ex . We
construct a 1-layer neural network ge with O(e-2 log ɪ) neurons with ∣∣g 一 ge∣∞ ≤ e. This will
serve as second layer. Formally, the constructed network Pe is Pe = ge (Pd=I he(xi)). This 2-layer
neural network has O(d3 e-2 log ɪ) neurons and verifies ∣pe - P∣∞ ≤ e.
We use this result to prove Theorem 3.1 and show that we can approximate any Korobov function
f ∈ X2,∞(Ω) within e with a 2-layer neural network of O(e-2 (log ɪ) Md2 1)) neurons. Consider
the sparse grid construction of the approximating space Vn(1) using the standard hat function as
mother function to create the hierarchical basis Wl (introduced in Section 2.2). The key idea is to
construct a shallow neural network approximating the sparse grid approximation and then use the
result of Theorem 2.2 to derive the approximation error. Let fn(1) be the projection of f on the
subspace Vn(1) defined in Section 2.2. fn(1) can be written as fn(1)(x) = P(l,i)∈U (1) vl,iφl,i(x),
where Un(1) contains the indices (l, i) of basis functions present in Vn(1). We can use Theorem 2.2
and choose n carefully such that fn(1) approximates f within e for L∞ norm. The goal is now to
approximate fn(1) with a shallow neural network. Note that the basis functions can be written as
a product of univariate functions φl,i = Qjd=1 φlj,ij . We can therefore use a similar structure to
the product approximation of Proposition 3.2 to approximate the basis functions. Specifically, the
first layer approximate the d(2n - 1) = O(e- 1 (log ɪ)) terms log φj,j necessary to construct
the basis functions of Vn(1) and a second layer to approximate the exponential in order to obtain
approximations of the O(2nnd-1) = O(e-2 (log ɪ) Md2 1)) basis functions of V(1). We provide a
detailed figure illustrating the construction, Figure 5 in Appendix B.3.
The shallow network that we constructed in Theorem 3.1 uses the ReLU activation function. We
extend this result to a larger class of activation functions which include commonly used ones.
Definition 3.3. A sigmoid-like activation function σ : R → R is a non-decreasing function having
finite limits in ±∞. A ReLU-like activation function σ : R → R is a function having a horizontal
asymptote in -∞ i.e. σ is bounded in R-, and an affine (non-horizontal) asymptote in +∞, i.e.
there exists b > 0 such that σ(x) - bx is bounded in R+.
Most common activation functions fall into these classes. Examples of sigmoid-like activations in-
clude Heaviside, logistic, tanh, arctan and softsign activations, while ReLU-like activations include
ReLU, ISRLU, ELU and soft-plus activations. We extend Theorem 3.1 to all these activations.
6
Published as a conference paper at ICLR 2022
Theorem 3.4. For any approximation tolerance e > 0, and for any f ∈ X2,∞ (Ω) there exists
a neural network with depth 2 and O(E-2 (log ɪ) * " ) training parameters that approximates f
within E for the infinity norm, with O (e-1 log( ɪ) 2 +1) (resp. O(E-2 log( ɪ) R " )) neurons
for a ReLU-like (resp. sigmoid-like) activation.
We note that these results can further be extended to more general Korobov spaces Xr,p. Indeed, the
main dependence of our neural network architectures in the parameters r and p arise from sparse grid
approximation. Bungartz & Griebel (2004) show that results similar to Theorem 2.2 can be extended
to various values of r, p and different error norms with a similar sparse grid construction. For
instance, we can use these results combined with our proposed architecture to show that the Korobov
space Xr,∞ can be approximated in infinite norm by neural networks with O(E-1 (log ɪ)r+1(d-1))
training parameters and same number of neurons up to a polynomial factor in E.
4 The Representation Power of Deep Neural Networks
Montanelli & Du (2019) used the sparse grid approach to construct deep neural networks with
ReLU activation, approximating Korobov functions with O(E-2 (log ɪ) / 1) +1) neurons, and
depth O(log ɪ) for the L∞ norm. We improve this bound for deep neural networks with C2 non-
1	1	3(d-1)
linear activation functions. We prove that We only need O(E-2 (log 亳)-2 -) neurons and fixed
depth, independent of E, to approximate the unit ball of the Korobov space within E in the L∞ norm.
Theorem 4.1. Let σ ∈ C2 be a non-linear activation function. Let E > 0. For any function
f ∈ X2,∞(Ω), there exists a neural network of depth dlog? d∖ + 1, with ReLU activation on
1	1	3(d-1)
the first layer and activation function σ for the next layers, O(E-2 (log 方)—2 一) neurons, and
1	1	3(d-1)
O(E-2 (log -) -2 一) training parameters approximating f within E for the infinity norm.
Compared to the bound of shallow networks in Theorem 3.1, the number of neurons for deep net-
works is lower by a factor O(√E), while the number of training parameters is the same. Hence, deep
neural networks are more efficient than shallow neural network in the sense that shallow networks
need more “inactive” neurons to reach the same approximation power, but have the same number of
parameters. This gap in the number of “inactive” neurons can be consequent in practice, as we may
not know exactly which neurons to train and which neurons to fix.
This new bound on the number of parameters and neurons matches the approximation power of
sparse grids. In fact, sparse grids use Θ(e- 11 (log -) R 1)) parameters (weights of basis functions)
to approximate Korobov functions within E. Our construction in Theorem 4.1 shows that deep neural
networks with fixed depth in E can fully encode sparse grids approximators. Neural networks are
therefore more powerful function approximators. In particular, any sparse grid approximation using
O(N (E)) parameters, can be represented exactly by a neural network using O(N (E)) neurons.
The deep approximating network (see Figure 3) has a very similar structure to our construction of
an approximating shallow network of Theorem 3.1. The main difference lies in the approximation
of the product function. Instead of using a 2-layer neural network, we now use a deep network. The
following result shows that deep neural networks can represent exactly the product function.
Proposition 4.2 (Lin et al. (2017), Appendix A). Let σ be C2 non linear activation function. For
any approximation error E > 0, there exists a neural network with dlog2 d∖ hidden layers and
activation σ, using at most 8d neurons arranged in a binary tree network that approximates the
product function Qid=1 xi on [0, 1]d within Efor the infinity norm.
An important remark is that the structure of the constructed neural network is independent of E. In
particular, the depth and number of neurons is independent of the approximation precision E, which
we refer to as exact approximation. It is known that an exponential number of neurons is needed in
order to exactly approximate the product function with a 1-layer neural network Lin et al. (2017),
however, the question of whether one could approximate the product with a shallow network and
a polynomial number of neurons, remained open. In Proposition 3.2, we answer positively to this
question by constructing an E-approximating neural network of depth 2 with ReLU activation and
O(d3e-2 log -) neurons. Using the same ideas as in Theorem 3.4, we can generalize this result to
7
Published as a conference paper at ICLR 2022
O(e-1 (log ɪ) d-1) functions φj,j(xj)
f(x)
UnI)I = O(e-2 (log ɪ) ( 2 )) basis functions φι,i
Figure 3: Deep neural network approximating a Korobov function f ∈ X2,∞ (Ω) within e. The
complete network has O(e- 1 (log ɪ) 3(d2 I)) neurons and depth「log? d] +1.
obtain an e-apprOximating neural network of depth 2 with O(d3 e-2 log ɪ) neurons for a ReLU-like
activation, or O(d2e-1 log ɪ) neurons for a sigmoid-like activation.
5 Neural Networks are Near- Optimal Function Approximators
In the previous sections, we proved upper bounds on the number of neurons and training parameters
needed by deep and shallow neural networks to approximate the Korobov space X2,∞(Ω). We
now investigate how good is the performance of neural networks as function approximators. We
prove a lower bound on the number of parameters needed by any continuous function approximator
to approximate the Korobov space. In particular, neural networks, deep and shallow, will nearly
match this lower bound, making them near-optimal function approximators. Let us first formalize
the notion of continuous function approximators, following the framework of DeVore et al. (1989).
For any Banach space X —e.g., a function space—and a subset K ⊂ X of elements to approximate,
we define a continuous function approximator with N parameters as a continuous parametrization
a : K → RN together with a reconstruction scheme which is a N -dimensional manifold MN :
RN → X. For any element f ∈ K, the approximation given is MN (a(f)): the parametrization a
is derived continuously from the function f and then given as input to the reconstruction manifold
that outputs an approximation function in X . The error of this function approximator is defined
as EN,a,MN (K)X := supf∈K |f - MN (a(f))|X. The best function approximator for space K
minimizes this error. The minimal error for space K is given by
EN(K)X = min EN,a,MN (K)X.
a,MN
In other terms, a continuous function approximator with N parameters cannot hope to approximate
K better than within EN(K)X. A class of function approximators is a set of function approximators
with a given structure. For example, neural networks with continuous parametrizations are a class
of function approximators where the number of parameters is the number of training parameters.
We say that a class of function approximators is optimal for the space of functions K if it matches
this minimal error asymptotically in N, within a constant multiplicative factor. In other words,
the number of parameters needed by the class to approximate functions in K within e matches
asymptotically, within a constant, the least number of parameters N needed to satisfy EN(K)X ≤ e.
The norm considered in the approximation of the functions of K is the norm associated to the
space X. DeVore et al. (1989) showed that this minimum error EN(K)X is lower bounded by the
Bernstein width of the subset K ⊂ X, defined as
bN(K)X := sup sup{ρ : ρU (XN+1) ⊂ K},
XN +1
8
Published as a conference paper at ICLR 2022
where the outer sup is taken over all N + 1 dimensional linear sub-spaces of X, and U(Y ) denotes
the unit ball of Y for any linear subspace Y of X .
Theorem 5.1 (DeVore et al. (1989)). Let X a Banach space and K ⊂ X. EN(K)X ≥ bN(K)X.
We prove a lower bound on the least number of parameters any class of continuous function approx-
imators needs to approximate functions of the Korobov space.
Theorem 5.2. Take X = L∞(Ω) and K = {f ∈ X2,∞(Ω) : |f ∣χ2,∞(Ω) ≤ 1} the Unit ball of
the Korobov space. Then, there exists c > 0 with EN(K)χ ≥ NI(log N)d-1. Equivalently, for
> 0, a continuous function approximator approximating K within in L∞ norm uses at least
Θ(e-1 (log ɪ) ^-) parameters.
Sketch of proof We seek an appropriate subspace XN+1 in order to get lower bound the Bernstein
width bN(K)X, which in turn provides a lower bound on the approximation error (Theorem 5.1). To
do so, we use the Deslaurier-Dubuc interpolet of degree 2, φ(2) (see Figure 6) which is C2. Using the
sparse grids approach, We construct a hierarchical basis in X2,∞(Ω) using φ(2) as mother function
and define XN+1 as the approximation space Vn(1). Here n is chosen such that the dimension of
Vn(1) is roughly N + 1. The goal is to estimate sup{ρ : ρU(XN+1) ⊂ K}, Which Will lead to a
bound on bN(K)X. To do so, We upper bound the Korobov norm by the L∞ norm for elements of
XN+1. For any function U ∈ XN+1 we can write U = Pli vι,i ∙ φι,i. Using a stencil representation
of the coefficients vl,i, we are able to obtain an upper bound ∣u∣χ2,∞ ≤ Γd∣∣uk∞ where Γd =
O(22nnd-1). Then, b'N(K)χ ≥ 1∕Γd which yields the desired bound.
This lower bound matches within a logarithmic factor the upper bound on the number of training
parameters needed by deep and shallow neural networks to approximate the Korobov space within :
1	-1 3(d-i)
O(E-2 (log ʌ)-2-) (Theorem 3.1 and TheOrem 4.1). It exhibits the same exponential dependence
in d with base log ɪ and the same main dependence on E of E- 2. Note that the upper and lower bound
can be rewritten as O(E-1∕2-δ) for all δ > 0. Moreover, our constructions in Theorem 3.1 and
Theorem 4.1 are continuous, which comes directly from the continuity of the sparse grid parameters
(see bound on vl,i in Theorem 2.2). Our bounds prove therefore that deep and shallow neural
networks are near optimal classes of function approximators for the Korobov space. Interestingly,
the subspace XN+1 our proof uses to show the lower bound is essentially the same as the subspace
we use to approximate Koborov functions in our proof of the upper bounds (Theorem 3.1 and 4.1).
The difference is in the choice of the interpolate φ to construct the basis functions, degree 2 for the
former (which provides needed regularity for the proof), and 1 for the later.
6 Conclusion and Discussion
We proved new upper and lower bounds on the number of neurons and training parameters needed by
shallow and deep neural networks to approximate Korobov functions. Our work shows that shallow
and deep networks not only lessen the curse of dimensionality but are also near-optimal.
Our work suggests several extensions. First, it would be very interesting to see if our proposed
theoretical near-optimal architectures have powerful empirical performance. While commonly used
structures (e.g. Convolution Neural Networks, or Recurrent Neural Networks) are motivated by
properties of the data such as symmetries, our structures are motivated by theoretical insights on
how to optimally approximate a large class of functions with a given number of neurons and param-
eters. Second, our upper bounds (Theorem 3.1 and 4.1) nearly match our lower bounds (Theorem
5.2) on the least number of training parameters needed to approximate the Korobov space. We
wonder if it is possible to close the gap between these bounds and hence prove neural network’s
optimality, e.g., one could prove that sparse grids are optimal function approximators by improv-
1	-	3(d-1)
ing our lower bound to match sparse grid number of parameters O(E-2 (log ʌ) —2 —). Finally, we
showed the near-optimality of neural networks among the set of continuous function approximators.
It would be interesting to explore lower bounds (analog to Theorem 5.2) when considering larger sets
of function approximators, e.g., discontinuous function approximators. Could some discontinuous
neural network construction break the curse of dimensionality for the Sobolev space? The question
is then whether neural networks are still near-optimal in these larger sets of function approximators.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors are grateful to Tomaso Poggio and the MIT 6.520 course teaching staff for several
discussions, remarks and comments that were useful to this work.
References
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning ,14(1):115-133,1994.
Hans-Joachim Bungartz. Dunne Gitter Und deren Anwendung bei der adaptiven LosUng der dreidi-
mensiOnalen POissOn-GleichUng. Technische Universitat Munchen, 1992.
Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta nUmerica, 13(1):147-269, 2004.
Gilles Deslauriers and Serge Dubuc. Symmetric iterative interpolation processes. In COnstrUctive
apprOximatiOn, pp. 49-68. Springer, 1989.
Ronald A DeVore, Ralph Howard, and Charles Micchelli. Optimal nonlinear approximation.
ManUscripta mathematica, 63(4):469-478, 1989.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. NeUral netwOrks, 2(5):359-366, 1989.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effec-
tively. In The 22nd internatiOnal cOnference On artificial intelligence and statistics, pp. 869-878.
PMLR, 2019.
Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function com-
binations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared
relu ridge functions with l1 and l0 controls. IEEE TransactiOns On InfOrmatiOn TheOry, 64(12):
7649-7656, 2018.
Michael Kohler and Adam Krzyzak. Adaptive regression estimation with multilayer feedforward
neural networks. NOnparametric Statistics, 17(8):891-913, 2005.
Michael Kohler and Adam Krzyzak. Nonparametric regression based on hierarchical interaction
models. IEEE TransactiOns On InfOrmatiOn TheOry, 63(3):1620-1630, 2016.
Michael Kohler and Jens Mehnert. Analysis of the rate of convergence of least squares neural
network regression estimates in case of measurement errors. NeUral NetwOrks, 24(3):273-279,
2011.
NM Korobov. Approximate solution of integral equations. DOklady Akademii NaUk SSSR, 128(2):
235-238, 1959.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. COmmUnicatiOns Of the ACM, 60(6):84-90, 2017.
Shiyu Liang and Rayadurgam Srikant. Why deep neural networks for function approximation?
arXiv preprint arXiv:1610.04161, 2016.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
JOUrnal Of Statistical Physics, 168(6):1223-1247, 2017.
Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic func-
tions. NeUral cOmpUtatiOn, 8(1):164-177, 1996.
Hrushikesh N Mhaskar. Eignets for function approximation on manifolds. Applied and COmpUta-
tiOnal HarmOnic Analysis, 29(1):63-87, 2010.
10
Published as a conference paper at ICLR 2022
Hrushikesh N Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: when is deep better
than shallow. arXiv preprint arXiv:1603.00988, 2016.
Hadrien Montanelli and Qiang Du. New error bounds for deep ReLU networks using sparse grids.
SIAM Journal on Mathematics ofData Science,1(1):78-92, 2019.
Hadrien Montanelli, Haizhao Yang, and Qiang Du. Deep ReLU networks overcome the curse of
dimensionality for bandlimited functions. arXiv preprint arXiv:1903.00735, 2019.
Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and estimation of deep neural
network with intrinsic dimensionality. 2019.
Erich Novak. Deterministic and stochastic error bounds in numerical analysis. 2006.
Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions
using deep ReLU neural networks. Neural Networks, 108:296-330, 2018.
Johannes Schmidt-Hieber. Deep ReLU network approximation of functions on a manifold. arXiv
preprint arXiv:1908.00695, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Sergei Abramovich Smolyak. Quadrature and interpolation formulas for tensor products of certain
classes of functions. In Doklady Akademii Nauk, volume 148, pp. 1042-1045. Russian Academy
of Sciences, 1963.
Taiji Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov
spaces: optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on learning theory, pp.
1517-1539. PMLR, 2016.
E Weinan, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for neural
network models. arXiv preprint arXiv:1906.08039, 2019.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks,
94:103-114, 2017.
Ch Zenger and W Hackbusch. Parallel algorithms for partial differential equations. In Notes on
Numerical Fluid Mechanics, volume 31, pp. 241-251. Vieweg, 1991.
Christoph Zenger. Sparse grids. Parallel Algorithms for Partial Differential Equations, 31, 1991.
Appendix
A On Korob ov Functions
In this section, We further discuss Korobov functions X2,p(Ω). Korobov functions enjoy more
smoothness than Sobolev functions: smoothness for X2,p(Ω) is measured in terms of mixed deriva-
tives of order two. Korobov functions X2,p(Ω) can be differentiated twice in each coordinates
simultaneously, while Sobolev W2,p(Ω) functions can only be differentiated twice in total. For
example, in two dimensions, for a function f to be Korobov it is required to have
∂f ∂f ∂2 f ∂ 2f	∂2f	∂3f	∂3 f	∂ 4f
∂xι，∂x2，∂2Xι，∂2X2，∂x1∂x2，∂2X1∂x2，∂x1∂2X2，∂2Xι∂2X2
∈ Lp(Ω),
while for f to be Sobolev it requires only
∂f ∂f ∂2f ∂2f	∂2f ∈
∂xι，∂x2，∂2X1，∂2X2，∂x]∂x2
11
Published as a conference paper at ICLR 2022
The former can be seen from ∣α∣∞ ≤ 2 and the latter from ∣α∣ι ≤ 2 in the definition of Xr,p(Ω)
and Wr,p(Ω).
We now provide intuition on why Korobov functions are easier to approximate. One of the key
difficulties in approximating Sobolev functions are possible high frequency oscillations which may
require an exponential number of neurons (Telgarsky, 2016). For instance, consider functions which
have similar structure to W(n...n) (defined in Subsection 2.2): for any smooth basis function φ with
support on the unit cube (see Figure 6 for example), consider the linear function space formed by
linear combinations of dilated function φ with support on each cube d-dimensional grid of step 2-n.
This corresponds exactly to the construction of W(n...n) which uses the product of hat function on
each dimension as basis function φ. This function space can have strong oscillations in all directions
at a time. The Korobov space prohibits such behavior by ensuring that functions can be differenti-
ated twice on each dimension, simultaneously. As a result, functions cannot oscillate in all directions
at a time without having large Korobov norm. We end this paragraph by comparing the Korobov
space to the space of bandlimited functions which was shown to avoid the curse of dimensionality
(Montanelli et al., 2019). These are functions for which the support frequency components is re-
stricted to a fixed compact. Intuitively, approximating these functions can be achieved because the
set of frequencies is truncated to a compact, which then allows to sample frequencies and obtain
approximation guarantees. Instead of imposing a hard constraint of cutting high frequencies, the
Korobov space asks for smoothness conditions which do not prohibit high frequencies but rather
impose a budget for high frequency oscillations. We precise this idea in the next example.
A concrete example of Korobov functions is given by an analogous of the function space Vn(1)
which we used as approximation space in the proof of our results (see Section 2.2). Similarly to the
previous paragraph, one should use a smooth basis function to ensure differentiability. Recall that
Vn(1) is defined as
Vn(1) :=	M	Wl .
|l|1 ≤n+d-1
Intuitively, this approximation space introduces a ”budget” of oscillations for all dimensions through
the constraint Pid=1 li ≤ n + d - 1. As a result, dilations of the basis function can only occur in a
restricted set of directions at a time, which ensures that the Korobov norm stays bounded.
B Proofs of Section 3
B.1	Approximating the product function
In this subsection, We construct a neural network architecture with two layers and O(d3 E-2 log ɪ)
neurons that approximates the product function p : x ∈ [0, 1]d 7-→ Qin=1 xi within for all > 0,
which proves Proposition 3.2. We first prove a simple lemma to represent univariate piece-wise
affine functions by shallow neural networks.
Lemma B.1. Any one dimensional continuous piece-wise affine function with m pieces is repre-
sentable exactly by a shallow neural network with ReLU activation, with m neurons on a single
layer.
Proof. This is a simple consequence from Proposition 1 in Yarotsky (2017). We recall the proof for
completeness. Let xi ≤ ∙∙∙ ≤ Xm-ι be the subdivision of the piece-wise affine function f. We use
a neural network of the form
m-1
g(x) := f(x1) +	wk(x - xk)+ - w0(x1 - x)+,
k=1
where w0 is the slope of f on the piece ≤ x1, w1 is the slope of f on the piece [x1, x2],
f (xk+1) - f(x1) - Pik=-11 wi(xk+1 - xi)
wk =	,
xk+1 - xk
for k = 1, ∙∙∙ , m - 2, and wm-i = w - Pm-L2 wk where w is the slope of f on the piece ≥ Xm-ι.
Notice that f and g coincide on all xk for 1 ≤ k ≤ m - 1. Furthermore, g has same slope as f on
each pieces, therefore, g = f.	□
12
Published as a conference paper at ICLR 2022
We can approximate univariate right continuous functions by piece-wise affine functions, and then
use Lemma B.1 to represent them by shallow neural networks. The following lemma shows that
O(-1) neurons are sufficient to represent an increasing right-continuous function with a shallow
neural network.
Lemma B.2. Let f : I -→ [c, d] be a right-continuous increasing function where I is an interval,
and let e > 0. There exists a shallow neural network with ReLU activation, with dd-c-∣ neurons on
a single layer, that approximates f within for the infinity norm.
Proof. Let m = [d-c[ Define a subdivision of the image interval C ≤ yι ≤ ... ≤ ym ≤ d where
yk = c + ke for k = 1,..., m. Note that this subdivision contains exactly「d-c] pieces. Now define
a subdivision of I, x1 ≤ x2 ≤ . . . ≤ xm by
xk := sup{x ∈ I, f(x) ≤ yk},
for k = 1,..., m. This subdivision stills has [d-c] pieces. We now construct our approximation
function f on I as the continuous piece-wise affine function on the subdivision x1 ≤ . . . ≤ xm such
that f(xk) = yk for all 1 ≤ k ≤ m and f is constant before x1 and after xm (see Figure 4). Let
x ∈ I.
•	If x ≤ x1, because f is increasing and right-continuous, c ≤ f(x) ≤ f(x1) ≤ y1 = c + e.
. . , . ^ ,.. ..,. , ..
Therefore |f(x) - f(x)∣ = |f(x) - (c + e)| ≤ e.
•	If xk < x ≤ xk+1, we have yk < f(x) ≤ f(xk+1) ≤ yk+1. Further note that yk ≤
f(x) ≤ yk+1. Therefore |f (x) - f (x)| ≤ yk+1 - yk = e.
•	Ifxm < x, then ym < f(x) ≤ d. Again, |f (x) - f (x)| = |f (x) - ym| ≤ d - ym ≤ e.
El i` II" 11 ,5 T	ɪ	ɪʌ t .	1 .1	i`	I~I
Therefore ∣∣f - f k∞ ≤ e. We can now use Lemma B.1 to end the proof.	□
Figure 4: Approximation of a right-continuous increasing function (blue) in an interval [c, d] within
e by a piece-wise linear function (red) with [d-cC pieces. The approximation is constructed using a
regular subdivision of the y axis of step e and constructing a linear approximation in the pre-image
of each part of the subdivision.
If the function to approximate has some regularity, the number of neurons needed for approximation
can be significantly reduced. In the following lemma, we show that O(e-2) neurons are sufficient
to approximate a C2 univariate function with a shallow neural network.
13
Published as a conference paper at ICLR 2022
Lemma B.3. Let f : [a, b] -→ [c, d] ∈ C2, and let > 0. There exists a shallow neural network
with ReLU activation, with √2^ min(∕ f∖f001 (1 + μ(f, e)), (b 一 a) ,∣f 00k∞) neurons on a single
layer, where μ(f, C) → 1 as E → 0, that approximates f within E for the infinity norm.
Proof. See Appendix B.2.1.
□
We will now use the ideas of Lemma B.2 and Lemma B.3 to approximate a truncated log function,
which we will use in the construction of our neural network approximating the product.
Corollary B.4. Let E > 0 sufficiently small and δ > 0. Consider the truncated logarithm function
log : [δ, 1] —→ R. There exists a shallow neural network with ReLU activation, with c- 1 log δ
neurons on a single layer, that approximates f within E for the infinity norm.
Proof. See Appendix B.2.2.
□
We are now ready to construct a neural network approximating the product function and prove
Proposition 3.2. The proof builds upon on the observation that Qid=1 xi = exp(Pid=1 log xi).
We construct an approximating 2-layer neural network where the first layer computes log xi for
1 ≤ i ≤ d, and the second layer computes the exponential. We illustrate the construction of the
proof in Figure 2.
Proof of Proposition 3.2 Fix E > 0. Consider the function h : x ∈ [0, 1] 7→ max(log x, log E) ∈
[log c, 0]. Using Corollary B.4, there exists a neural network he : [0,1] —> [log c, 0] with
1 + dd 1 E- 1 log ɪe neurons on a single layer such that ||鼠 一 h∈k∞ ≤ d. Indeed, one can take
the E-approximation of h : x ∈ [E, 1] 7→ log x ∈ [log E, 0], then extend this function to [0, E] with a
constant equal to log E. The resulting piece-wise affine function has one additional segment corre-
sponding to one additional neuron in the approximating function. Similarly, consider the exponential
g : x ∈ R- 7→ ex ∈ [0, 1]. Because g is right-continuous increasing, we can use Lemma B.3 to con-
struct a neural network ge : R— -> [0,1] with 1 + ∣"√√^ log ∣"∣ neurons on a single layer such that
kg — gek∞ ≤ e. Indeed, again one can take the c-approximation of ge : X ∈ [log e, 0] → ex ∈ [0,1],
then extend this function to (-∞, log e] with a constant equal to e. The corresponding neural net-
work has an additional neuron. We construct our final neural network φ>e (see Figure 2) as
Φ^ = ge (X he (Xi)
Note that φ)e can be represented as a 2-layer neural network: the first layer is composed of the union
ofthe 1+ dd 1 e- 1 log ɪ[ neurons composing each of the 1 -layer neural networks h : X ∈ [0,1]d →
he(χi) ∈ R for each dimension i ∈ {1,..., d}. The second layer is composed of the 1+ [√√^ log ⅛"∣
neurons of ge. Hence, the constructed neural network φe has O(d2e-2 log ɪ) neurons. Let us now
analyze the approximation error. Let X ∈ [0,1]d. For the sake of brevity, denote y = Pd=1 he(χi)
andy = Pid=1 log(Xi). We have,
d
∖φe(x) 一p(x)∖ ≤ ∖φe(x) 一 exp(y)∖ + ∖ exp(y) — exp(y)∖ ≤ e + ][xi ∙ ∖ exp(y — y) 一 1∖,
i=1
where we used the fact that ∖φe(x) 一 exp(y)∖ = ∖ge(y) — g(y)∖ ≤ ∣∣ge 一 gk∞ ≤ e.
First suppose that X ≥ e. In this case, for all i ∈ {1, . . . , d} we have ∖he(Xi) 一 log(Xi)∖ = ∖he(Xi) 一
e
he(xi)∖ ≤ d. Then, \y-y\ ≤ e. COnSeqUently, ∖φe(x)ιp(x)∖ ≤ e+max(\ee — 1\, ∖e e-1\) ≤ 3e, for
e > 0 sufficiently small. Without loss of generality now suppose x1 ≤ e. Then y ≤ he(xι) ≤ log e,
so by definition of ^e, We have 0 ≤ φe(x) = ge(y) ≤ exp(log e) = e. Also, 0 ≤ P(X) ≤ e so finally
∖φe (X) 一 p(X)∖ ≤ e.
14
Published as a conference paper at ICLR 2022
Remark B.5. Note that using Lemma B.2 instead of Lemma B.3 to construct approximating shallow
networksfor log and exp would yield approximation functions he with O( ∖d log ɪ]) neurons and ge
with O( |"|"|) neurons. Therefore, the corresponding neural network would approximate the product
P with O(d2e-1 log ɪ) neurons.
B.2 Missing proofs of Section B.1
B.2.1 Proof of Lemma B.3
Proof. Similarly as the proof of Lemma B.2, the goal is to approximate f by a piece-wise affine
function f defined on a subdivision x0 = a ≤ x1 ≤ . . . ≤ xm ≤ xm+1 = b such that f and
f coincide on x0 , . . . , xm+1 . We first analyse the error induced by a linear approximation of the
function on each piece. Let x ∈ [u, v] for u, v ∈ I. Using the mean value theorem, there exists
αx ∈ [u, x] such that f(x) - f(u) = f0(αx)(x - u) and βx ∈ [x, v] such that f(v) - f(x) =
f0(βx)(v - x). Combining these two equalities, we get,
f(x)- f(U) - (X - U)f(V) - f(U) = (V - x)(f(X) - f(U)) - (X - UHf(V) - f(X))
v- u	v- u
/ M 、f (βχ) - f(αχ)
=(x — u)(x — v)--------------
V-U
Rβx f00(t)dt
=(X - u)(v - x) Jax
V-U
Hence,
f(V) - f(U)	R x f 00 (t)dt
f (x) = f (u) + (X - u) f~)2 + (X - u)(v - x) Jax	.	(1)
V-U	V-U
We now apply this result to bound the approximation error on each pieces of the subdivision. Let
k ∈ [m]. Recall f is linear on the subdivision [Xk, Xk+1 ] and f(Xk) = f(Xk) and f (Xk+1 ) =
f (Xk+1). Hence, for all X ∈ [Xk, Xk+1], /(x) = f (Xk) + (x - Xk) f(xk+I)-X(Xk). Using Equation
xk+1 -xk
equation 1 with u = Xk and V = Xk+1 , we get,
kf - fk∞,[Xk,Xk+1] ≤ sup
X∈[Xk,Xk+1]
(X-Xk)(Xk+1 -X)
RaX f"
xk + 1 - Xk
≤
≤
1	Xk+1
2(xk+ι - Xk)J	If00(t)∣dt
2(xk+1 - Xk)2 kf"∣∣∞,[xk ,Xk+ι].
Therefore, using a regular subdivision with
""W Pieces.
SteP √⅛
yields an -aPProximation of f with
…	，	，，	一	♦.….，	RΛ l∖f7∣∖ /r 、
We now show that for any μ > 0, there exists an ^-approximation of f With at most J √^ (1 + μ)
pieces. To do so, we use the fact that the upper Riemann sum for √f converges to the integral
since √f0 is continuous on [a, b]. First define a partition a = Xo ≤ XK = b of [a, b] such that
the upper Riemann sum R(√f7°) on this subdivision satisfies R(√f77) ≤ (1 + μ∕2) Ra √f77. Now
define on each interval Ik of the partition a regular subdivision with step Jkf 2' as before. Finally,
consider the subdivision union of all these subdivisions, and construct the approximation f on this
final subdivision. By construction, kf - f k∞ ≤ because the inequality holds on each piece of the
subdivision. Further, the number of pieces is
K-1
X1+
i=0
(Xi+1 - Xi) SUPXi,Xi+1] f7 = R(√f7) . K <
√.	= ^^√2F + K ≤
(1 + μ),
for > 0 small enough. Using Lemma B.1 we can complete the proof.
□
15
Published as a conference paper at ICLR 2022
d(2n - 1) = O(e-1 (log ɪ)d-1) functions φj,j (xj)
x1	x2	xd
O(d1 e-2 log ɪ)
log Φ1,1(x1
>n,2n-1(x2)	log φn,2n-1(xd)
n

O(e-2 log ɪ)
d
g,i(X) := ∏φlj,ij (Xj)
~
fe(x)
j=1
V
UnI)I = O(e-2 (log ɪ) ( 2 )) basis functions φι,i
Figure 5: Shallow neural network with ReLU activation approximating a Korobov function f ∈
X2,∞(Ω) within e in infinity norm. The network has O(e-1(log ɪ)d+1) neurons on the first layer
3(d — 1) ∣ι
and O(e-1 (log ɪ) 2	) neurons on the second layer.
B.2.2 Proof of Corollary B.4
Proof. In view of Lemma B.3, the goal is to show that We can remove the dependence of μ(f, e) in
δ. This essentially comes from the fact that the upper Riemann sum behaves well for approximating
log. Consider the subdivision xo := δ ≤ xι ≤ ... ≤ Xm ≤ Xm+ι := 1 with m = |_4 log 1_| where
e := log(1 + √2e), such that Xk = elog δ+ke, for k = 0,..., m - 1. Denote f the corresponding
piece-wise affine approximation. Similarly to the proof of Lemma B.3, for k = 0, . . . , m - 1,
lll ʌ,	1	⑵S0”	(eg - 1)2
k log -f k∞,[xk,Xk 十1] ≤ 2(Xk+1 - Xk ) kf k∞,[xk ,Xk 十1] ≤	2	≤ e.
The proof follows.
□
B.3 PROOF OF THEOREM 3.1: APPROXIMATING THE KOROBOV SPACE X2,∞(Ω)
In this subsection, we prove Theorem 3.1 and show that we can approximate any Korobov function
f ∈ X2,∞(Ω) within e with a 2-layer neural network of O(e-2 (log ɪ) 3(d- 1)) neurons. We illustrate
the construction in Figure 5. Our proof combines the constructed network approximating the product
function and a decomposition off as a sum of separable functions, i.e. a decomposition of the form
Kd
f(x) ≈ X Y φ(jk) (xj),	∀x ∈ [0, 1]d.
k=1 j=1
Consider the sparse grid construction of the approximating space Vn(1) using the standard hat func-
tion as mother function to create the hierarchical basis Wl (introduced in Section 2.2). We recall that
the approximation space is defined as Vn(1) := L|l| ≤n+d-1 Wl. We will construct a neural network
approximating the sparse grid approximation and then use the result of Theorem 2.2 to derive the
approximation error. Figure 5 gives an illustration of the construction.
Let fn(1) be the projection of f on the subspace Vn(1). fn(1) can be written as
fn(1) (x) =	vl,iφl,i(x),
(l,i)∈Un(1)
16
Published as a conference paper at ICLR 2022
where Un(1) contains the indices (l, i) of basis functions present in Vn(1) i.e.
Un(1) := {(l, i),	|l|1 ≤ n + d - 1, 1 ≤ i ≤ 2l - 1, ij odd for all 1 ≤ j ≤ d}.	(2)
Throughout the proof, we explicitly construct a neural network that uses this decomposition to ap-
proximate fn(1) . We then use Theorem 2.2 and choose n carefully such that fn(1) approximates
f within for L∞ norm. Note that the basis functions can be written as a product of univariate
functions φl,i = Qjd=1 φlj,ij . We can therefore use the product approximation of Proposition 3.2
to approximate the basis functions. Specifically, we will use one layer to approximate the terms
log φlj,ij and a second layer to approximate the exponential.
We now present in detail the construction of the first layer. First, recall that φlj ,ij is a piece-wise
affine function with subdivision 0 ≤ ⅛1 ≤ 2⅛ ≤ j+1 ≤ L Define the errοr term "= f∞.
ij-I；+15, ij +1-1 . We define it as follows:
≤ … ≤ x2m+2 = ij +i:where m =
We consider a symmetric subdivision of the interval
X0 = ij-ij+e ≤ X1 ≤ …≤ Xm+1 = 2j ≤ Xm+2
[春 log ij and €o := log(1 + p2(≡∕d), such that
ij - 1 + elog e5+ke0
Xk = ---------*---------- 0 ≤ k ≤ m,
ij + 1 - elog e5+(2m+2-k)ke0
Xk = j--------------1------------- m + 2 ≤ k ≤ 2m + 2.
Note that with this definition, the terms log(2lj xk - ij) form a regular sequence with step o. We
now construct the piece-wise affine function ^j,j on the subdivision xo ≤ ∙∙∙ ≤ x2m+2 which co-
incides with log φj,ij on xo,…，x2m+2 and is constant on [0, xo] and [x2m+2,1]. By Lemma B.1,
this function can be represented by a 1-layer neural network with as much neurons as the number of
pieces of ^, i.e. at most 2 J3d log i neurons for E sufficiently small. A similar proof to that of Corol-
lary B.4 shows that g approximates max(log φιj,ij, log(<≡∕3)) within "(3d) for the infinity norm.
We use this construction to compute in parallel, e/(3d)-approximations of max(log φιj,ij (Xj), log e)
for all 1 ≤ j ≤ d, and 1 ≤ lj ≤ n, 1 ≤ ij ≤ 2ιj where ij is odd. These are exactly the 1-dimensional
functions that we will need, in order to compute the d-dimensional function basis of the approxima-
tion space Vn(i). There are d(2n - 1) such univariate functions, therefore our first layer contains at
most 2η+1d J3d log i neurons.
We now turn to the second layer. The result of the first two layers will be e/3-approximations of
φl,i for all (l, i) ∈ Un(i). Recall that Un(i) contains the indices for the functions forming a basis of
the approximation space Vn(i). To do so, for each indexes (l, i) ∈ Un(i) we construct a 1-layer neural
network approximating the function exp, which will compute an approximation of exp(giι ,汨 T-+
gid,id). The approximation of exp is constructed in the same way as for Lemma B.3. Consider a
regular subdivision of the interval [log(<≡∕3), 0] with step ,2("3), i.e. xo := log(<≡∕3) ≤ xi ≤
…≤ Xm ≤ Xm+ι = 0 where m
[ʌ/ɪlog i], such that Xk = log e + k√2∣,	0 ≤ k ≤ m.
Construct the piece-wise affine function h on the subdivision xo ≤ ∙∙∙ ≤ xm+i which coincides
with exp on xo,…，Xm+i and is constant on (-∞, xo]. Lemma B.3 shows that h approximates
exp on R- within e for the infinity norm. Again, Lemma B.1 gives a representation of h as a 1-layer
neural network with as many neurons as pieces in h i.e. 1 + ∣" ^J~2ι log ∣"∣. The second layer is
the union of 1-layer neural networks approximating exp within e/3, for each indexes (L i) ∈ Un1).
Therefore, the second layer contains ∣ UnI)I，+ [ jɪlog ∣"∣) neurons. As shown in Bungartz &
Griebel (2004),
WI= X 2i∙(d -- + i)=(-1)"X (n + d - 1)(-2尸-i = 2n.(，+O(nd-2))
17
Published as a conference paper at ICLR 2022
Therefore, the second layer has O(2n(n-i； e-2 log ∙∣) neurons. Finally, the output layer computes
the weighted sum of the basis functions to approximate /31). Denote by f(ŋ the corresponding
function of the constructed neural network (see Figure 5), i.e.
fn1)=	χ a-h(χg(χj)
(l,i)∈U	n(1)	j=1
Let us analyze the approximation error of our neural network. The proof of Proposition 3.2 shows
that the output of the two first layers h(P；=1 g(∙j)) approximates φι,i within e. Therefore, We
obtain IIfnI)- f3I)I∣∞ ≤ e P(i i)∈u(1)∣vι,i∣. We now use approximation bounds from Theorem 2.2
on fn(1) .
kf-fn1)k∞ ≤kf-fn1)k∞+kfn1)-fn1)k∞ ≤ 2f∞ ∙2-2n ∙A(d,n)+2w∞
|vl,i|,
(l,i)∈Un(1)
where
X	∣vι,iI ≤ If∣2,∞2-dX2-i ∙ dd -- + i) ≤∣f∣2,∞.
(l,i)∈U	n(1)	i≥0
Let us now take ne = min {n : 2lf82,∞ 2-2nA(d, n) ≤ f }. Then, using the above inequality shows
that the neural network fn1 approximates f within E for the infinity norm. We will now estimate
the number of neurons in each layer of this network. Note that
…2⅛log1∙ and 2"≤
8 2 (2log2) ⅛1 (d — 1)! 2
|f l2,∞
d-1
∙(1 + O(1)).
(3)
We can use the above estimates to show that the constructed neural network has at most N1 (resp.
N2 ) neurons on the first (resp. second) layer where
4
N	4√3d2	If∣2,∞ L 八」+1
2 e→0 8d∕2(2log2)3⅛ud!2	e 1°gJ
This proves the bound the number of neurons. Finally, to prove the bound on the number of training
parameters of the network, notice that the only parameters of the network that depend on the function
f are the parameters corresponding to the weighs vl,i of the sparse grid decomposition. This number
is |琮?| = O(2n«ng-1) = O(e- 1 (log ɪ)3⅛u).
B.4 Proof of Theorem 3.4: Generalization to general activation functions
We start by formalizing the intuition that a sigmoid-like (resp. ReLU-like) function is a function that
resembles the Heaviside (resp. ReLU) function by zooming out along the x (resp. x and y) axis.
Lemma B.6. Let σ be a sigmoid-like activation with limit a (resp. b) in -∞ (resp.+∞). For
any δ > 0 and error tolerance e > 0, there exists a scaling M > 0 such that X → σb-MX) — a
approximates the Heaviside function within E outside of (-δ, δ) for the infinity norm. Furthermore,
this function has values in [0, 1].
Let σ be a ReLU-Uke activation with asymptote b ∙ X + C in +∞. For any δ > 0 and error tolerance
E > 0, there exists a scaling M > 0 such that X → σ(MMC) approximates the ReLUfunction within E
for the infinity norm.
18
Published as a conference paper at ICLR 2022
Proof. Let δ, > 0 and σ a sigmoid-like activation with limit a (resp. b) in -∞ (resp. +∞). There
exists xo > 0 sufficiently large such that (b-a)∣σ(x)-a| ≤ E for X ≤ -x0 and (b-a)∣σ(x)-b∣ ≤ E
for x ≥ X0. It now suffices to take M := χ0∕δ to obtain the desired result.
Now let σ be a ReLU-like	activation with oblique	asymptote	bx	in	+∞	where b	>	0.	Let	M	such
that ∣σ∣ ≤ MbE for X ≤	0 and ∣σ(x) — bx|	≤	MbE for X	≥	0.	One	can	check	that	|	a，M：X |	≤	E	for
x ≤ 0, and | σ(Mx) — x|	≤	E for X ≥ 0.	□
一 ' IMb	1	—	—
Using this approximation, we reduce the analysis of sigmoid-like (resp. ReLU-like) activations to
the case of a Heaviside (resp ReLU) activation to prove the desired theorem.
Proof of Theorem 3.4 We start by the class of ReLU-like activations. Let σ be a ReLU-like
activation function. Lemma B.6 shows that one can approximate arbitrarily well the ReLU activation
with a linear map σ. Take the neural network approximator f of a target function f given by Theorem
3.1. At each node, we can add the linear map corresponding to x → σ(Mx) with no additional neuron
nor parameter. Because the approximation is continuous, we can take M > 0 arbitrarily large in
order to approximate f with arbitrary precision on the compact [0, 1]d.
The same argument holds for sigmoid-like activation functions in order to reduce the problem to
Heaviside activation functions. Although quadratic approximations for univariate functions similar
to Lemma B.3 are not valid for general sigmoid-like activations - in particular the Heaviside —— we
can obtain an analog to Lemma B.2 as Lemma B.7 given in the Appendix B.4.1. This results is an
increased number of neurons. In order to approximate a target function f ∈ X2,∞(Ω), we use the
same structure as the neural network constructed for ReLU activations and use the same notations
as in the proof of Theorem 3.1. The first difference lies in the approximation of log φlj,ij in the
first layer. Instead of using Corollary B.4, we use Lemma B.7. Therefore, 1∣d log ∣ neurons are
needed to compute a E/(3d)-approximation of max(log φj,j, log(<≡∕3)). The second difference is
in the approximation of the exponential in the second layer. Again, we use Lemma B.7 to construct
a e/3-approximation of the exponential on R- with ∣ neurons for the second layer. As a result,
the first layer contains at most 2n+2 3d2 log ∣ neurons for E sufficiently small, and the second layer
contains IUnI)I ∣ neurons. Using the same estimates as in the proof of Theorem 3.1 shows that the
constructed neural network has at most N1 (resp. N2) neurons on the first (resp. second) layer where
,	..3	d+1
N-	J ∙ 25∙ d∖ ≡∞ (log1「，
e→0 8d(2log2)d-1 d!2 E2 k E)
3	3	3(d-
N2_________24 ∙ d	∙ f⅛∞ flog 1)一
e→0 8d(2log2)3⅛ud!2	E2	∖ 2 E)
This ends the proof.
B.4.1	Proof of Lemma B.7
Lemma B.7. Let σ be a sigmoid-like activation. Let f : I -→ [c, d] be a right-continuous in-
creasing function where I is an interval, and let E > 0. There exists a shallow neural network
with activation σ, with at most 2 d-c neurons on a single layer that approximates f within E for the
infinity norm.
Proof. The proof is analog to that of Lemma B.2. Let m = b d-c C . We define a regular subdivision
of the image interval c ≤ y1 ≤ . . . ≤ ym ≤ d where yk = c + kE for k = 1, . . . , m, then
using the monotony of f, we can define a subdivision of I, X1 ≤ . . . ≤ Xm such that Xk :=
sup{X ∈ I, f(X) ≤ yk}. Let us first construct an approximation neural network f with the Heaviside
activation. Consider
f (X) := yι +E X 1 (X - Xi +2xi+1 ≥ 0).
19
Published as a conference paper at ICLR 2022
Let x ∈ [c, d] and k such that x ∈ [xk, xk+1]. We have by monotony yk ≤ f(x) ≤ yk+1 and
yk = y1 + (k - 1) ≤ f(x) ≤ y1 + k = yk+1. Hence, f approximates f within in infinity norm.
Let δ < mini=1,...,m(xi+1 - xi)/4 and σ a general sigmoid-like activation with limits a in -∞ and
b in +∞. Take M given by Lemma B.7 such that σ(Mx) - a approximates the Heaviside function
within 1/m outside of (-δ, δ) and has values in [0, 1]. Using the same arguments as above, the
function
八	m-1 σ (Mx - Mxi+χi+1 )
f(x) := yι + e 72 ------------------a
b-a
i=1
approximates f within 2 for the infinity norm. The proof follows.
□
C Proofs of Section 4
C.1 Proof of Theorem 4.1: Approximating Korobov functions with deep neural
NETWORKS
Let > 0. We construct a similar structure to the network defined in Theorem 3.1 by using the
sparse grid approximation of Subsection 2.2. For a given n, let fn(1) be the projection of f in the
approximation space Vn(1) (defined in Subsection 2.2) and Un(1) (defined in equation 2) the set of
indices (l, i) of basis functions present in Vn(1). Recall fn(1) can be uniquely decomposed as
fn(1) (x) =	X	vl,iφl,i(x).
(l,i)∈Un(1)
where φl,i = Qjd=1 φlj ,ij are the basis functions defined in Subsection 2.2. In the first layer,
we compute exactly the piece-wise linear hat functions φlj,ij , then in the next set of layers,
we use the product-approximating neural network given by Proposition 4.2 to compute the ba-
sis functions φl,i = Qjd=1 φlj ,ij (see Figure 3). The output layer computes the weighted sum
P(l,i)∈U(1) vl,iφl,i(x) and outputs fn(1). Because the approximation has arbitrary precision, we can
chose the network of Proposition 4.2 such that the resulting network f verifies ∣∣∕ - f^nι)k∞ ≤ e/2.
More precisely, as φlj ,ij is piece-wise linear with four pieces, we can compute it exactly with four
neurons with ReLU activation on a single layer (Lemma B.1). Our first layer is composed of the
union of all these ReLU neurons, for the d(2n - 1) indices lj , ij such that 1 ≤ j ≤ d, 1 ≤ lj ≤ n,
1 ≤ ij ≤ 2lj and ij is odd. Therefore, it contains at most d2n+2 neurons with ReLU activation. The
second set of layers is composed of the union of product-approximating neural networks to compute
φl,i for all (l, i) ∈ Un(1). This set of layers contains dlog2 de layers with activation σ and at most
| UnI) | ∙ 8d neurons. The output of these two sets of layers is an approximation of the basis functions
φl,i with arbitrary precision. Consequently, the final output of the complete neural network is an
approximation of fn(1) with arbitrary precision. Similarly to the proof of Theorem 3.1, we can chose
the smallest n such that ∣f - fn(1) ∣∞ ≤ /2 (see equation 3 for details). Finally, the network has
depth at most log2 d + 2 and N neurons where
N = 8d|Un(1)|
〜
→0
25 ∙ d5/2
82 (2log2) 3(d- 1) d!2
The parameters of the network depending on the function are exactly the coefficients vl,i of the
1	1	3(d-1)
sparse grid approximation. Hence, the network has O(E-2 (log 亳)-2 -) training parameters.
20
Published as a conference paper at ICLR 2022
-3	-2	-1	0	1	2	3
Figure 6: Deslaurier-Dubuc interpolets of degree 1, 2 and 3.
D Proofs of Section 5
D.1 Proof of Theorem 5.2: Near-Optimality of Neural Networks for Korobov
FUNCTIONS
Our goal is to define an appropriate subspace XN+1 in order to get a good lower bound on the
Bernstein width bN (K)X, defined in equation 5, which in turn provides a lower bound on the ap-
proximation error (Theorem 5.1).
To do so, we introduce the Deslaurier-Dubuc interpolet φ(L) : R → R. The construction of this
function uses an interpolating scheme on binary rationals. First, φ(L) is defined on all integers
φ(L) (k) = 1k=o for k ∈ Z. Then, We define the function on half integers 2k++1 by fitting a polyno-
mial of degree 2L - 1 interpolating the standard hat function φ at k - L + 1,…，k + L. Iteratively,
we define the interpolet on binary rationals of the form j+1 from the value of the interpolet on ratio-
nals With denominator 2j. Specifically, let Pj(,Lk) be the unique polynomial of degree 2L interpolating
φ(L) at points 2j for k0 ∈ {k - L + 1,…，k + L}. We set
φ(L) (j1) ：= PjL) (2k2+1).
For example, for L = 2 we get φ(2)( j+1):= 得φ(2)(2j) + 得φ(2)(kj)- 表φ(2)(kj)-
16Φ(2)(k+j3). This process defines the interpolet on binary rationals. We then extend the func-
tion to the real line by continuity. See Figure 6 for an illustration. Deslauriers & Dubuc (1989)
proved that the regularity of the interpolet is an increasing function of L the degree of interpolation.
We now prove results in the case L = 2.
Lemma D.1. The interpolet of degree 2, φ(2) is C2 and has support Supp φ(2) = [-3, 3].
Proof. See Appendix D.2.1.
□
We will now use the interpolet φ(2) to construct the subspace XN +1. Using the sparse grids ap-
proach, we can construct a hierarchical basis in X2,∞(Ω) using the interpolet φ(2) as mother func-
tion. In the remaining of the proof, we will write φ instead of φ(2) for simplicity, and use the
notations and definitions related to sparse grids, introduced in Subsection 2.2. Because EN(K) is
decreasing in N, it suffices to show the result for Nn when we define our space XN +1 to be exactly
the approximating space Vn(1) of sparse grids XNn +1 := Vn(1). The following equation establishes
the relation between n and Nn. In the following, for simplicity, we will write N instead of Nn.
N = dim(Vn(1))-1
X 2 …-1 = X 2i-d(d -- + i)-1=2n∙(鼻
∣l∣ι≤n+d-1	i=0	×	7	)
+O(nd-2)
21
Published as a conference paper at ICLR 2022
First, let us give some properties about the subspace XN+1.
Proposition D.2. Let U ∈ XN +1 and write it in decomposed form U = Eli vι,i ∙ φι,i, where the
sum is taken over all multi-indices corresponding to basis functions of XN+1. The coefficients vl,i
can be computed in the following way.
(d ∖
vl,i =	Ilj ,ij U =: Il,iU
j=1
where Ij,jU = u(j) — 16u( j-))一得u( j+r~) + 7⅛u(^⅛τ^) + 16u( j++τ~)∙ Here, Il,i denotes the
d-dimensional stencil which gives a linear combination of values of U at 5d nodal points.
Proof. See Appendix D.2.2.
□
Note that the stencil representation of the coefficients gives directly |vl,i | ≤ 5d kUk∞ . We are now
ready to make our estimates. The goal is to compute sup{ρ : ρU(XN+1) ⊂ K}, which will lead
to a bound on bN(K)X . To do so, it suffices to upper bound the Korobov norm by the L∞ norm
for elements of XN +1. In fact, if Γd > 0 satisfies for all U ∈ U(XN+1), ∣u∣χ2,∞ ≤ Γd∣∣uk∞,then
bN(K)X ≥ 1∕Γd.
Now take U ∈ XN +1 and let us write U = Pli vl,i ∙ φl,i. Note that basis functions in the same
hierarchical class Wl are almost disjoint. More precisely, at each point X ∈ Ω, at most 3d basis
functions φl,i have non-zero contribution to u(x). Therefore, for any 0 ≤ α ≤ 2 ∙ 1,
kDαuk∞ ≤	3d max
∞	1≤i≤2l-1, iodd
∣l∣ι≤n+d-1,
n-1
∣vl,i∣∙ 2hα,likDαφk∞
d-1+i
d-1
60d∣φ∣χ2,∞(Ω)kuk∞∙ £22
i=0
60d
7-r-1τ!IΦ∣X2,∞(Ω) ∙ 22n (nd-1 + O(nd-2)) kuk∞.
(d - 1)!
Finally, denoting by Cd the constant 谭飞 ∣Φ∣χ2,∞(Ω), We get for n sufficiently large
bN(K)X ≥ 2Cd ∙ 22n ∙1nd-1.
Furthermore, recall N =(&-])!2n ∙ nd-1 ∙(1 + O (ɪ)) . Therefore, n 〜IogN, and 2n 〜(d 一
1)!(log2)d-1 ∙ (logN)- . Finally we obtain for some constant Cd > -,
bN(K)X ≥ Cd+(log N)d-1.
N2
We conclude by analyzing the minimum number of parameters in order to get an -approximation of
the Korobov unit ball K. Define ne := min{n : C^n，— ≤ e}. This yields ne 〜21t1g2 log ɪ,
and 2n 〜 弋 C (2log 2)d-1--------1--3-v. The number of needed parameters to obtain an E approx-
V Cd	√^∙(iog ⅛)—
imation K is therefore
d-1
for some constant Cd > -.
Interestingly, the subspace XN+1 our proof uses to show the lower bound is essentially the same as
the subspace we use to approximate Koborov functions in our proof of the upper bounds (Theorem
3.1, 4.1). The difference is in the choice of the interpolate φ to construct the basis functions, degree
1 for the former, and 2 for the later.
22
Published as a conference paper at ICLR 2022
D.2 Missing proofs of Subsection D.1
D.2.1 Proof of Lemma D.1
Lemma D.3. The interpolet of degree 2, φ(2) is C2 and has support Supp φ(2) = [-3, 3].
Proof. To analyze the regularity of the interpolet, we introduce the trigonometric polynomial
P(2)(θ) := X φ⑵(2) eikθ = 1 + 196(eiθ + e-iθ) - 116(e3iθ + e-3iθ).
k∈Z
We can write P(2) (θ) as
4
P"篙∙ s(θ),
sin⑸
where S(θ) = 4 -表(eiθ + e-iθ) is a trigonometric polynomial of degree 1. Deslaurier and DUbUc
(Deslauriers & Dubuc, 1989, Theorem 7.11) showed that the interpolet φ(2) has regularity
Hlogr I
log2_|
where r is the spectral radiUs of the matrix B := [sj-2k]-1≤j,k≤1 where sj are the coefficients of
the trigonometric polynomial S(θ). In oUr case, matrix B writes
-1/16 -1/16	0
B =	0	1/4	0
0	-1/16 -1/16
and has spectral radiUs r
1/4. Therefore, the regUlarity of φ(2) is at least
log r
lθg2
sUpport, one can check that more generally Supp(φ(L)) = [-2L + 1,2L - 1].
2. For the
□
D.2.2 Proof of Proposition D.2
Proposition D.4. Let U ∈ XN +ι and write it in decomposed form U = Eli vι,i ∙ φι,i, where the
sum is taken over all multi-indices corresponding to basis functions of XN+1 . The coefficients vl,i
can be computed in the following way.
(d ∖
vl,i =	Ilj ,ij	U =: Il,iU
j=1
where lj,jU = u(j) — 16u(j2-r)) — 16u(~j++^) + ι⅛u(j2-τ~) + 16u(j+τ~)∙ Here, Il,i denotes the
d-dimensional stencil which gives a linear combination of values of U at 5d nodal points.
Proof. We start by looking at the 1-dimensional case. One can check that Iι,iΦn = 1』Indeed,
7 i' ∙ i' ^l	7	1 ~ / ■ .1	I	∙ 11 1	. .1	1 1 1	，、 T	1 ■ . <	∙ ∕' T	7 . 1
if l > l of if l = l and i = i, then φ^ will be zero at the nodal values of I/ Further, if l < l the
iterative constrUction of φ gives directly.
φ,i (j = ⅛φ,i (⅛1) + ⅛φ,i (⅛1) - 116φ,i (i⅛3) - 16φ,i (⅛3).
Therefore, We obtain Iι,%φ币 =0. Finally, Iι,iU = P不 Vg Iι,iφ巧 =vι,i ∙ Iι,iφι,i = vι,i. This
proves the stencil representation of vl,i for dimension 1. Finally, using the tensor product approach
of the stencil operator Il,i we can generalize the formula to general dimensions.	□
23