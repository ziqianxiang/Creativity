Published as a conference paper at ICLR 2022
How Low Can We Go: Trading Memory
for Error in Low-Precision Training
Chengrun Yang*, Ziyang Wu*, Jerry Chee, Christopher De Sa, Madeleine Udell
Cornell University
{cy438,zw287,jc3464,cmd353,udell}@cornell.edu
Ab stract
Low-precision arithmetic trains deep learning models using less energy, less mem-
ory and less time. However, we pay a price for the savings: lower precision may
yield larger round-off error and hence larger prediction error. As applications
proliferate, users must choose which precision to use to train a new model, and
chip manufacturers must decide which precisions to manufacture. We view these
precision choices as a hyperparameter tuning problem, and borrow ideas from
meta-learning to learn the tradeoff between memory and error. In this paper, we
introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix
factorization to find non-dominated configurations (the Pareto frontier) with a lim-
ited number of network evaluations. For any given memory budget, the precision
that minimizes error is a point on this frontier. Practitioners can use the frontier to
trade memory for error and choose the best precision for their goals.
1 Introduction
Training modern-day neural networks is becoming increasingly expensive as task and model sizes
increase. The energy consumption of the corresponding computation has increased alarmingly,
and raises doubts about the sustainability of modern training practices (Schwartz et al., 2019).
Low-precision training can reduce consumption of both computation (De Sa et al., 2018) and
memory (Sohoni et al., 2019a), thus minimizing the cost and energy to train larger models and
making deep learning more accessible to resource-limited users.
Low-precision training replaces 32-bit or
64-bit floating point numbers with fixed
or floating point numbers that allocate
fewer bits for the activations, optimiz-
ers, and weights. A rich variety of meth-
ods appear in recent literature, includ-
ing bit-centering (De Sa et al., 2018),
loss scaling (Micikevicius et al., 2017)
and mixed-precision training (Zhou et al.,
2016). There is however a fundamental
tradeoff: lowering the number of bits of
precision increases the quantization er-
ror, which may disrupt convergence and
increase downstream error (Zhang et al.,
2016; Courbariaux et al., 2014; Gupta
et al., 2015). The low-precision configura-
tion is thus a hyperparameter to be chosen
according to the resource constraints of
the practitioner.
memory (MB)
(a)	error vs memory on
CIFAR-10 (Krizhevsky
et al., 2009)
(b)	# activation bits in non-
dominated configurations
Figure 1: Test error vs memory for ResNet-18 across 99
low-precision floating point configurations. Figure (a)
shows the tradeoff on CIFAR-10. (Non-dominated points
are blue circles.) Figure (b) shows that the best precision
to use varies depending on the memory budget, on 87
image datasets. See Section 4 for experimental details.
How should we choose this hyperparameter? Many believe the highest allowable precision (given a
memory budget) generally produces the lowest error model. However, there are typically many ways
*Equal contribution.
1
Published as a conference paper at ICLR 2022
to use the same amount of memory. As shown in Figure 1(a), some of these configurations produce
much lower error than others! Figure 1(b) shows that no one configuration dominates all others. We
might consult previous literature to choose a precision; but this approach fails for new applications.
Our goal is to efficiently pick the best low-precision configuration under a memory budget.
Efficiency is especially important for resource-constrained practitioners, such as individual users or
early-stage startups. To promote efficiency, we use a meta-learning (Lemke et al., 2015; Vanschoren,
2018; Hospedales et al., 2020) approach: we train a small number of cheap very-low-precision models
on the dataset to choose the perfect precision. The gains from choosing the right low-precision format
can offset the cost of this extra training — but each precision must be chosen carefully to realize the
benefits of low-precision training.
We use ideas from multi-objective optimization to characterize the tradeoff between memory and error
and identify the Pareto frontier: the set of non-dominated solutions. Users will want to understand
the tradeoff between error and memory so they can determine the resources needed to adequately
train a model for a given machine learning task. This tradeoff may also influence the design of
application-specific low-precision hardware, with profound implications for the future (Hooker, 2020).
For example, among all 99 low-precision configurations we tried, we identified some configurations
that are Pareto optimal across many different tasks (listed in Appendix D). These results could help
hardware manufacturers decide which precision settings are worth manufacturing.
Computing the Pareto frontier by training models for all low-precision configurations is expensive
and unnecessary. Cloud computing platforms like Google Cloud and Amazon EC2 charge more for
machines with the same CPU and double memory: 25% more on Google Cloud* and 100% more on
Amazon EC2* as of mid-September 2021. We use techniques from meta-learning to leverage the
information from other low-precision training runs on related datasets. This approach allows us to
estimate the Pareto frontier without evaluating all of the low-precision configurations.
Our system, Pareto Estimation to Pick the Perfect Precision (PEPPP), has two goals. The first goal
is to find the Pareto frontiers of a collection of related tasks, and is called meta-training in the
meta-learning literature. Meta-training requires a set of measurements, each collected by training and
testing a neural network with a given precision on a dataset. This information can be gathered offline
with a relatively large resource budget, or by crowdsourcing amongst the academic or open-source
community. Still, it is absurdly expensive to exhaustively evaluate all measurements: that is, every
possible low-precision configuration on every task. Instead, we study how to choose a subset of
the possible measurements to achieve the best estimate. The second goal we call meta-test: using
the information learned on previous tasks, how can we transfer that information to a new task to
efficiently estimate its Pareto frontier? This goal corresponds to a resource-constrained individual or
startup who wants to determine the best low-precision configuration for a new dataset.
Both meta-training and meta-test rely on matrix completion and active learning techniques to avoid
exhaustive search: we make a subset of all possible measurements and predict the rest. We then
estimate the Pareto frontier to help the user make an informed choice of the best configuration.
We consider two sampling schemes: uniform and non-uniform sampling. Uniform sampling is
straightforward. Non-uniform sampling estimates the Pareto frontier more efficiently by making
fewer or even no high-memory measurements.
To the best of our knowledge, PEPPP is the first to study the error-memory tradeoff in low-precision
training and inference without exhaustive search. Some previous works show the benefit of low-
precision arithmetic in significant energy reduction at the cost of a small accuracy decrease (Hashemi
et al., 2017), and propose hardware-software codesign frameworks to select the desired model (Lan-
groudi et al., 2019). Many other papers focus on low-precision inference only and use it for model
compression (Wu et al., 2018; Cai & Vasconcelos, 2020). Another line of work seeks state-of-the-art
(SOTA) performance with low-precision training, using carefully designed learning schedules (Sun
et al., 2019; 2020). Our work imagines low-precision as a way to reduce training or deployment
costs of customized models trained on proprietary datasets (not SOTA models on ML benchmarks).
Even so, we show in Section 4 that our work picks promising models for every memory budget,
which enables near-SOTA results on CIFAR-10.
*https://cloud.google.com/ai-platform/training/pricing
*https://aws.amazon.com/ec2/pricing/on-demand
2
Published as a conference paper at ICLR 2022
meta-training
Figure 2: The PEPPP workflow. We begin with a collection of (meta-) training datasets and low
precision configurations. In the meta-training phase, we sample dataset-configuration pairs to train,
and compute the misclassification error. We use matrix factorization to compute a low dimensional
embedding of every configuration. In the meta-test phase, our goal is to pick the perfect precision
(within our memory budget) for the meta-test dataset. We compute the memory required for each
configuration, and we select a subset of fast, informative configurations to evaluate. By regressing
the errors of these configurations on the configuration embeddings, we find an embedding for the
meta-test dataset, which we use to predict the error of every other configuration (including more
expensive ones) and select the best subject to our memory budget.
Figure 2 shows a flowchart of PEPPP. The rest of this paper is organized as follows. Section 2
introduces notation and terminology. Section 3 describes the main ideas we use to actively sample
configurations and approximate Pareto frontiers. Section 4 shows experimental results.
2	Notation and terminology
Low-precision formats. We use floating point numbers for low-precision training in this paper.
As an example, Figure 3 shows an 8-bit floating point
number with 3 exponent bits and 4 mantissa bits. A
specific low-precision representation with a certain
number of bits for each part is called a low-precision
format. We may use different low-precision formats
for the weights, activations, and optimizer. In the
case of using two formats to train and represent a
I7I6Ri4 3 2 i0
sign 3-bit exponent 4-bit mantissa
Figure 3: An 8-bit floating point number rep-
resenting (-1)sign ∙ 2exponent-7 ∙ Lb3b2bM
neural network (as discussed in Section 3.1), these two formats are called Format A and B. A specific
combination of these two formats is a hyperparameter setting of the neural network, and we call it a
low-precision configuration.
Math basics. We define [n] = {1, . . . , n} for a positive integer n. With a Boolean variable X, the
indicator function 1(X) equals 1 if X is true, and 0 otherwise. With a scalar variable x, we use x+ to
denote max{x, 0}. ⊆ and ⊂ denote subset and strict subset, respectively.
Linear algebra. We denote vector and matrix variables respectively by lowercase letters (a) and
capital letters (A). All vectors are column vectors. The Euclidean norm of a vector a ∈ Rn is
∣∣ak := /Pn= 1 *. To denote a part of a matrix A ∈ Rn×d, We use a colon to denote the varying
dimension: Ai,: and A:,j (or aj) denote the ith row and jth column of A, respectively, and Aij
denotes the (i,j)-th entry. With an ordered index set S = {s1, . . . , sk} Where s1 < . . . < sk ∈ [d],
we denote A：S := [A：,si •- A:,sj ∈ Rn×k. Given two vectors x,y ∈ Rn, X W y means Xi ≤ yi
3
Published as a conference paper at ICLR 2022
for each i ∈ [n]. Given a matrix A ∈ Rn×d and a set of observed indices as Ω ⊆ [n] X [d], the partially
observed matrix Pω(a) ∈ Rn×d has entries (PsJA))ij = Aij if (i,j) ∈ Ω, and 0 otherwise.
Pareto frontier. Multi-objective optimization simultaneously minimizes n costs {ci}i∈[n]. A feasible
point c(1) = (c(11), . . . , c(n1)) is Pareto optimal if for any other feasible point c(2) = (c(12), . . . , c(n2)),
c(2) c(1) implies c(2) = c(1) (Boyd & Vandenberghe, 2004). The set of Pareto optimal points is the
Pareto frontier.
Tasks, datasets, measurements and evaluations. A task carries out a process (classification,
regression, image segmentation, etc.) on a dataset. Given a deep learning model and a dataset, the
training and testing of the model on the dataset is called a measurement. In our low-precision context,
we evaluate a configuration on a dataset to make a measurement.
Meta-learning. Meta-learning transfers knowledge from past (meta-training) tasks to better under-
stand the new (meta-test) task. To evaluate the error of meta-training, we use meta-leave-one-out
cross-validation (meta-LOOCV): on a collection of tasks, each time we treat one task as meta-test
and the others as meta-training, and average the results over all splits.
Error matrix and memory matrix. Given
a neural network, the errors of different
low-precision configurations on meta-training
datasets form an error matrix, whose (i, j)-th
entry Eij is the test error of the j th configura-
tion on the ith dataset. To compute the error
matrix, we split the i-th dataset into training
and test subsets (or use a given split), train
the neural network at the j -th configuration on
the training subset, and evaluate the test error
on the test subset. The memory required for
each measurement forms a memory matrix M ,
which has the same shape as the correspond-
ing error matrix. Example error and memory
matrices are shown in Figure 4.
low-precision configurations
ab cd
low-precision configurations
ab cd
1	0.85	0.71	0.32	0.28	datasets	1	181	269	272	295
2	0.80	0.80	0.80	0.69		2	180	269	272	295
3	0.80	0.68	0.46	0.45		3	590	924	927	1032
(a) error matrix E (b) memory matrix M (MB)
Figure 4: Example error and memory matrices
for some datasets and low-precision configurations.
Dataset 1: CIFAR-10, 2: CIFAR-100 (fruit and veg-
etables), 3: ImageNet-stick (Deng et al., 2009). Con-
figuration Format A (exponent bits, mantissa bits),
Format B (exponent bits, mantissa bits). a: (3, 1), (6,
7); b: (3, 4), (7, 7); c: (4, 3), (8, 7); d: (5, 3), (6, 7).
3	Methodology
PEPPP operates in two phases: meta-training and meta-test. First in meta-training, we learn configu-
ration embeddings by training a neural network of a specific architecture at different low-precision
configurations on different datasets. As listed in Appendix A, we study image classification tasks and
a range of low-precision formats that vary in the number of bits for the exponent and mantissa for the
activations, optimizer, and weights. Our hope is to avoid enumerating every possible configuration
since exhaustive search is prohibitive in practice. To this end, we make a few measurements and then
predict the other test errors by active learning techniques. We also compute the memory matrix to
understand the memory consumption of each measurement.
Then in meta-test, we assume that we have completed meta-training, and hence know the configuration
embeddings. The meta-test goal is to predict the tradeoff between memory usage and test error
on a new (meta-test) dataset. This step corresponds to inference in traditional machine learning; it
must be quick and cheap to satisfy the needs of resource-constrained practitioners. To select the
configuration that has smallest test error and takes less than the memory limit, we measure a few
selected (informative and cheap) configurations on the meta-test dataset, and use the information of
their test errors to predict the rest. The problem of choosing measurements is known as the “cold-start”
problem in the literature on recommender systems. Then we use the model built on the meta-training
datasets to estimate values for the other measurements on the meta-test dataset.
3.1	Meta-training
On the n meta-training datasets, we first theoretically compute the memory needed to evaluate each
of the d low-precision configurations to obtain the full memory matrix M ∈ Rn×d . The total memory
consists of memory needed for model weights, activations, and the optimizer (gradients, gradient
4
Published as a conference paper at ICLR 2022
accumulators, etc.) (Sohoni et al., 2019b). Among these three types, activations typically dominate, as
shown in Figure 5. Thus using lower precision for activations drastically reduces memory usage. Addi-
tionally, empirical studies (Zhou et al., 2016) have shown that adopting higher precision formats for the
optimizer can substantially improve the accuracy of the trained model. Since the optimizer usually re-
quires a relatively small memory, itis often the best to use a higher precision format for this component.
Thus for each low-precision configuration, it is typical to use a lower precision for network weights
and activations, and a higher precision for the optimizer, resulting in combinatorially many choices
in total. An example of the memory usage in this low-precision scenario is also shown in Figure 5.
In practice, ML tasks are often correlated: for example, meta-training
datasets might be subsets of a large dataset like CIFAR-100 or Im-
ageNet. The ranking of different configurations tends to be simi-
lar on similar tasks. For example, we computed the test errors of
99 low-precision configurations on 87 datasets (both listed in Ap-
pendix A) to form a performance vector in R99 for each dataset.
We use the Kendall tau correlation to characterize the alignment be-
tween the ranking of errors incurred by different configurations on two
datasets: the Kendall tau correlation is 1 if the order is the same and -1
if the order is reversed. As shown in Figure 6(a), similar datasets have
larger correlations: for example, datasets with indices 38-87 correspond
to ImageNet subproblems such as distinguishing types of fruit or boat.
Notice also that some dataset pairs have configuration performance
rankings that are negatively correlated. The corresponding error matrix
fUHB full precision
KwΛ low precision
'activations optimizer weights
Figure 5: Memory us-
age under two training
paradigms. Both train a
ResNet-18 on CIFAR-10
with batch size 32.
E concatenates the performance vector for each dataset. It is not low rank, but its singular values
decay rapidly, as shown in Figure 6(b). Hence we expect low rank approximation of this matrix from
a few measurements to work well: it is not necessary to measure every configuration.
In the uniform sampling scheme, we sample mea-
surements uniformly at random to obtain a partially
observed error matrix Pq(E). We then estimate the
full error matrix E using a low rank matrix comple-
tion method to form estimate Eb . In this paper, we
use SoftImpute (Mazumder et al., 2010; Hastie
et al., 2015) (Algorithm 1, Appendix B). Using the
estimated error matrix Eb and computed memory
matrix M, we compute the Pareto frontier for each
dataset to understand the (estimated) error-memory
tradeoff. Section 4 Figure 10 shows an example.
In the non-uniform sampling scheme, we sample
measurements with non-uniform probabilities, and
use a weighted variant of SoftImpute, weighting
by the inverse sampling probability, to complete the
error matrix (Ma & Chen, 2019). Then we estimate
(a) dataset correlation
(b) singular value decay
Figure 6: Kendall tau correlation of test er-
ror of all configurations between all pairs of
datasets, and singular value decay of corre-
sponding error matrix. Strong correlations al-
low PEPPP to succeed with a few measure-
ments. Details in Appendix A.
the Pareto frontier in the same way as above. Non-uniform sampling is useful to reduce the memory
needed for measurements. To this end, we construct a probability matrix P by entry-wise transforma-
tion Pij = σ (1/Mij), in which the monotonically increasing σ : R → [0, 1] maps inverse memory
usage to a sampling probability. In this way, we make more low-memory measurements, and thus
reduce the total memory usage.
3.2	Meta-test
Having estimated the embedding of each configuration, our goal is to quickly compute the error-
memory tradeoff on the meta-test dataset, avoiding the exhaustive search of all possible measurements.
We first compute the memory usage of each low-precision configuration on the meta-test dataset.
With this information and guided by meta-training, we evaluate only a few (cheap but informative)
configurations and predict the rest. Users then finally select the non-dominated configuration with
highest allowable memory. An example of the process is shown in Figure 7.
5
Published as a conference paper at ICLR 2022
PEPPP uses Experiment Design with Matrix Factorization (ED-MF) described below to choose
informative configurations. With more time, ED-MF evaluates more configurations, improving our
estimates. We may also set a hard cutoff on memory: we do not evaluate low-precision configurations
exceeding the memory limit. This setting has been studied in different contexts, and is called active
learning or sequential decision making.
ED-MF picks measurements to minimize the variance of the resulting estimate. Specifically, we
d
factorize the true (or estimated) error matrix E ∈ Rn×d (or E) into its best rank-k approximation, and
get dataset embeddings X ∈ Rn×k and configuration embeddings Y ∈ Rd×k as E ≈ X>Y (Fusi
et al., 2018; Yang et al., 2019). On a meta-test dataset, we denote the error and memory vectors
of the configurations as enew ∈ Rd and mnew ∈ Rd, respectively. We model the error on the new
dataset as enew = Y >xnew + ∈ Rd, where xnew ∈ Rk is the embedding of the new dataset and
∈ Rd accounts for the errors from both measurement and low rank decomposition. We estimate the
embedding xnew from a few measurements (entries) of enew by least squares. Note that this requires
at least k measurements on the meta-test dataset to make meaningful estimations, in which k is the
rank for matrix factorization.
If e 〜 N(0, σ2I), the variance of the es-
timator is Pj∈S yj yj> - , in which yj
is the jth column of Y . D-optimal experi-
ment design selects measurements on the
new dataset by minimizing (a scalarization
of) the variance,
minimize log det Pj∈S yjyj>-
subject to |S | ≤ l
S⊆ [d],
(1)
to find S, the set of indices of config-
urations to evaluate. The positive inte-
ger l bounds the number of measurements.
Given a memory cap mmax , we replace
the second constraint above by S ⊆ T , in
which T = {j ∈ [d] | mjnew ≤ mmax}
is the set of feasible configurations. Since
Problem 1 is combinatorially hard, we may
either relax it to a convex optimization
Figure 7: Meta-test on CIFAR-10. After meta-training
on all other datasets in Appendix A Table 2, we use
ED-MF to choose six informative measurements (or-
ange squares) with a 275MB memory limit for each
measurement on CIFAR-10. Then we estimate test er-
rors of other configurations by ED-MF, and restrict
our attention to configurations that we estimate to be
non-dominated (red x’s). Note some of these are in fact
dominated, since we plot true (not estimated) test error!
Finally we select the estimated non-dominated configu-
ration with highest allowable memory (blue square).
problem by allowing decision variables to have non-integer values between 0 and 1, or use a
greedy method to incrementally choose measurements (initialized by measurements chosen by the
column-pivoted QR decomposition (Gu & Eisenstat, 1996; Golub & Van Loan, 2012)). We compare
these two approaches in Appendix C.
In Section 4.2, we compare ED-MF with competing techniques and show it works the best to estimate
the Pareto frontier and select the final configuration.
4 Experiments and discussions
The code for PEPPP and experiments is in the GitHub repository at https://github.com/
chengrunyang/peppp. We use QPyTorch (Zhang et al., 2019) to simulate low-precision formats
on standard hardware. The 87 datasets and 99 low-precision configurations in experiments are listed
in Appendix A. The datasets consist of natural and medical images from various domains. Apart from
CIFAR-10, the datasets include 20 CIFAR-100 partitions from mutually exclusive subsets, based on
the superclass labels. They also include 50 subsets of ImageNet (Deng et al., 2009), which contains
over 20,000 classes grouped into multiple major hierarchical categories like fungus and amphibian;
each of the 50 datasets come from different hierarchies. Finally, we use 16 datasets from the visual
domain benchmark (Wallace & Hariharan, 2020) to increase the diversity of domains.
The 99 low-precision configurations use mixed-precision as described in Section 3.1. Format A
(for the activations and weights) uses 5 to 9 bits; Format B (for the optimizer) ranges from 14 to
6
Published as a conference paper at ICLR 2022
20 bits. For each, these bits may be split arbitrarily between the exponent and mantissa. Across all
configurations, we use ResNet-18, ResNet-34 and VGG (Simonyan & Zisserman, 2014) variants (11,
13, 16, 19) with learning rate 0.001, momentum 0.9, weight decay 0.0005 and batch size 32. Each
training uses only 10 epochs as an early stopping strategy (Yao et al., 2007) to prevent overfitting.
All the measurements take 35 GPU days on NVIDIA® GeForce® RTX 3090.
Developing a method to select the above hyperparameters at the same time as the low-precision format
is an important topic for future research, but not our focus here. In Section 4.3 we demonstrate how
PEPPP can naturally extend to efficiently select both optimization and low-precision hyperparameters.
A number of metrics may be used to evaluate the quality of the obtained Pareto frontier in a multi-
objective optimization problem (Wu & Azarm, 2001; Li et al., 2014; Audet et al., 2020): the distance
between approximated and true frontiers (convergence), the uniformity of the distances between
neighboring Pareto optimal points (uniformity), how well the frontier is covered by the approximated
points (spread), etc. In our setting, the memory is accurate and the test error is estimated, so we do
not have full control of the uniformity and spread of the 2-dimensional frontiers between test error
and memory. As illustrated in Figure 8, we evaluate the quality of our estimated Pareto frontiers by
the following two metrics:
Convergence (Deb et al., 2002) between the
sets of true and estimated Pareto optimal points
P, P is |P1 Pv∈p dist(v, P), where the dis-
tance between point v and set P is dist(v, P) =
min{kv - wk : w ∈ P}. This is a surrogate for
the distance between Pareto frontiers.
Hypervolume difference (HyperDiff) (Zitzler
& Thiele, 1998; Coello & Sierra, 1999; Wu &
Azarm, 2001) is the absolute difference between
the volumes of solution spaces dominated by the
true and estimated Pareto frontiers. This metric
improves with better convergence, uniformity,
and spread. Its computation requires an upper
boundary for each resource.
memory
(a) convergence
upper boundary
g'°T-'……]
2 I «
S &* 二
自 i⅛
memory
(b) HyPerDiff
O true non-dominated
X estimated non-dominated
Figure 8: Illustration of Pareto frontier metrics.
(a) Convergence is the average distance from each
estimated Pareto oPtimal Point to its closest true
Point: average(d1, d2, d3). (b) HyPerDiff is the
absolute difference in area of feasible regions given
by the true and estimated Pareto oPtimal Points: the
shaded area between Pareto frontiers.
When comPuting these metrics, we normalize the memories by ProPortionally scaling them to
between 0 and 1. To evaluate the matrix comPletion Performance, we use the relative error defined as
kvb - vk/kvk, in which vbis the Predicted vector and v is the true vector.
4.1	Meta-training
We study the effectiveness of uniform and non-uniform samPling schemes. For simPlicity, we regard
all 87 available datasets as meta-training in this section. In SOFTIMPUTE, we use λ = 0.1 (chosen by
cross-validation from a logarithmic scale); and we choose a rank 5 aPProximation, which accounts
for 78% of the variance in the error matrix (as shown in Figure 6(b)).
In the uniform samPling scheme, we investigate how many samPles we need for accurate estimates:
we samPle the error matrix at a number of different ratios, ranging from 5% to 50%, and comPlete the
error matrix from each set of samPled measurements. In uniform samPling, the samPling ratio is also
the Percentage of memory we need to make the measurements in Parallel, comPared to exhaustive
search. Hence we show the relationshiP between Pareto frontier estimation Performance and samPling
ratio in Figure 9. We can see the estimates are more accurate at larger samPling ratios, but samPling
20% of the entries already suffices for good Performance.
As a more intuitive examPle, Figure 10 shows the error-memory tradeoff on CIFAR-10. ComPared
to the estimated Pareto oPtimal Points at samPling ratio 5%, the ones at 20% are closer to the true
Pareto frontier (better convergence), lie more evenly (better uniformity) and cover the true Pareto
frontier better (better sPread), as shown by the resPective convergence and HyPerDiff values.
The non-uniform samPling scheme has a similar trend and is shown in APPendix E.2.
7
Published as a conference paper at ICLR 2022
⅛ 2 C
■ ■
U O C
DU.IAUOD
sampling ratio
(a) relative error
CIFAR-10
WUU U
由αJ0dxH
.	7 W 8
sampling ratio
(b) convergence
CIFAR-10
sampling ratio
(c) HyperDiff
884 2 O
-----
Ooooo
IoIl①
200 250 300
memory (MB)
884 2 加
-----
Ooooo
IoIl①
200 250 300
memory (MB)
Figure 9: Pareto frontier estimation in PEPPP meta-
training, with uniform sampling of configurations.
The violins show the distribution of the performance
on individual datasets, and the error bars (blue) show
the range. The red error bars show the standard de-
viation of the error on CIFAR-10 across 100 random
samples of the error matrix. Figure (a) shows the
matrix completion error for each dataset; Figure (b)
and (c) show the performance of the Pareto frontier es-
timates. Modest sampling ratios (around 0.1) already
yield good performance.
(a) sampling ratio 5% (b) sampling ratio 20%
-I- dominated O true non-dominated X estimated non-dominated
Figure 10: Error vs memory on CIFAR-10 with
true and estimated Pareto frontiers from uni-
form sampling in PEPPP meta-training. A 20%
uniform sample of entries yields a better esti-
mate of the Pareto frontier (convergence 0.03
and HyperDiff 0.02) compared to a 5% sample
(convergence 0.09 and HyperDiff 0.16).
4.2	Meta-leave-one-out cross-validation (meta-LOOCV)
Now suppose that we have already collected measurements on the meta-training datasets to form a
meta-training error matrix E (or its low rank approximation E). On the meta-test dataset, PEPPP
estimates the Pareto frontier by the active learning technique ED-MF. We compare ED-MF with a
few other active learning techniques: Random selection with matrix factorization (Random-MF),
QR decomposition with column pivoting and matrix factorization (QR-MF) and two Bayesian
optimization techniques (BO-MF and B O-full), to understand whether the strong assumptions in
ED-MF (low rank and Gaussian errors) are a hindrance or a help. An introduction to these techniques
can be found in Appendix E.1.
We use rank 3 for matrix factorization: Y:,j ∈
R3 for each j ∈ [d]. In BO, we tune hyperpa-
rameters on a logarithmic scale and choose
the RBF kernel with length scale 20, white
noise with variance 1, and ξ = 0.01. Ta-
ble 1 shows the meta-LOOCV settings for
each acquisition technique. We compare the
techniques at a range of number of configura-
tions to measure in each meta-LOOCV split,
Table 1: Meta-LOOCV experiment settings
meta-training error matrix	memory cap on meta-test?	
	no	yes
full	I	II
uniformly sampled	III	IV
non-uniformly sampled	V	VI
resembling what practitioners do in hyperparameter tuning: evaluate an informative subset and infer
the rest. Setting I is the most basic, Setting IV and VI are the most practical. We only show results of
Setting I and IV in the main paper, and defer the rest to Appendix E.
In Setting I, we do meta-LOOCV with the full meta-training error matrix in each split and do not cap
the memory for meta-test. This means we evaluate every configuration on the meta-training datasets.
We can see from Figure 11(a) and 11(b) that:
•	ED-MF stably outperforms under both metrics, especially with fewer measurements.
•	QR-MF overall matches the performance of ED-MF.
•	BO-MF, BO-full and random-MF underperform ED-MF and QR-MF at lower memory
usage, but often match their performance with higher memory.
Practitioners may have only collected part of the meta-training performance, and desire or are
limited by a memory cap when they do meta-test on the new dataset. In Setting IV, we cap the
single-configuration memory usage for meta-test at 816MB, the median memory of all possible mea-
surements across configurations and datasets. Additionally, we uniformly sample 20% configurations
from the meta-training error matrix in each split. In Figure 11(c) and 11(d), we can see similar trends
as Setting I, except that QR-MF is slightly worse.
8
Published as a conference paper at ICLR 2022
O(əs =F UT3ΘIU)
əɔuəɪəAUoɔ
relative memory usage
%
Λ O
O(əs =F u^əlu)
αjθdXH
5%	10%
relative memory usage
0^φ∞ =F u03ΘU1)
əɔuəɪəAUoɔ
20%	40%
relative memory usage
0^φ∞ =F u03ΘU1)
sα,I9dKH
20%	40%
relative memory usage
(a) Setting I convergence (b) Setting I HyperDiff
(c)	Setting IV convergence
(d)	Setting IV HyperDiff
E ED-MF (PEPPP) Q QR-MF
BO-MF ♦ Bθ-fuιι
* RANDoM-MF
0
0
Figure 11: Pareto frontier estimates in meta-LOOCV Setting I and IV (with a 20% meta-training
sampling ratio and an 816MB meta-test memory cap). Each error bar is the standard error across
datasets. The x axis measures the memory usage relative to exhaustively searching the permissible
configurations. ED-MF consistently picks the configurations that give the best PF estimates.
Ultimately, users would want to select a configuration that both achieves a small error and takes
lower memory than the limit. As shown in Figure 2, PEPPP offers users the predicted Pareto frontier
and chooses the non-dominated configuration with the highest allowable memory and hence the
lowest error. We compare the acquisition techniques with the “random high-memory” baseline that
randomly chooses a configuration that takes the highest allowable memory: an approach that follows
the “higher memory, lower error” intuition. Figure 12 shows an example.
Overall, among all approaches and across all memory us-
age, ED-MF outperforms, especially at a smaller number
of measurements. Compared to BO techniques, ED-MF
also enjoys the benefit of having less hyperparameters to
tune. Although techniques like QR-MF and random-
MF are easier to implement, the additional cost of ED-MF
is much smaller than the cost of making measurements:
neural network training and testing.
4.3 Tuning optimization hyperparameters
Previously, we have shown that PEPPP can estimate the
error-memory tradeoff and select a promising configura-
tion with other hyperparameters fixed. In practice, users
may also want to tune hyperparameters like learning rate
to achieve the lowest error. In Appendix E.4, we tune
the number of epochs and learning rate in addition to pre-
cision, and show that the methodology can be used in
broader settings.
卜	1ape
shorebird
housing
cartilaginous fish
foodstuff
substance
relation
spider
percussion
new world monkey
1.2
1.4
error on dataset
(relative to ED-MF)
t BO-MF	■ RaNdom-MF
BO-fUll	< random highest-memory
Figure 12: Relative performance with re-
spect to ED-MF in meta-test Setting IV
when making 3 measurements (memory
usage 〜10%) on 10 ImageNet partitions.
ED-MF outperforms in most cases.


.
♦
1
4.4 Meta-learning across architectures
The meta-learning in previous sections were conducted on ResNet-18. In Appendix E.5, we show
that on 10 ImageNet partitions, PEPPP is also capable of estimating the error-memory tradeoff of
ResNet-34, VGG-11, VGG-13, VGG-16 and VGG-19 competitively. Moreover, the meta-learning
across architectures works better than considering each architecture separately.
5 Conclusion
This paper proposes PEPPP, a meta-learning system to select low-precision configurations that
leverages training information from related tasks to efficiently pick the perfect precision given
a memory budget. Built on low rank matrix completion with active learning, PEPPP estimates
the Pareto frontier between memory usage and model performance to find the best low-precision
configuration at each memory level. By reducing the cost of hyperparameter tuning in low-precision
training, PEPPP allows practitioners to efficiently train accurate models.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Madeleine Udell and Chengrun Yang acknowledge support from NSF Award IIS-1943131, the ONR
Young Investigator Program, and the Alfred P. Sloan Foundation. Christopher De Sa acknowledges a
gift from SambaNova Systems, Inc., and Jerry Chee acknowledges support from the NSF Research
Traineeship Program (Award # 1922551). The authors thank Tianyi Zhang for helpful discussions,
and thank several anonymous reviewers for useful comments.
References
Charles Audet, Jean Bigeon, DominiqUe Cartier, Sebastien Le DigabeL and LUdovic Salomon.
Performance indicators in multiobjective optimization. European journal of operational research,
2020.
Stephen R Becker, Emmanuel J Candes, and Michael C Grant. Templates for convex cone problems
with applications to sparse signal recovery. Mathematical programming computation, 3(3):165,
2011.
C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, 2006. ISBN 9780387310732.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Leon Bungert, Tim Roith, Daniel Tenbrinck, and Martin Burger. A bregman learning framework for
sparse neural networks. arXiv preprint arXiv:2105.04319, 2021.
Zhaowei Cai and Nuno Vasconcelos. Rethinking differentiable search for mixed-precision neural net-
works. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 2349-2358, 2020.
Carlos A Coello Coello and Margarita Reyes Sierra. Multiobjective evolutionary algorithms: classifi-
cations, analyses, and new innovations. In Evolutionary Computation. Citeseer, 1999.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with
low precision multiplications. arXiv preprint arXiv:1412.7024, 2014.
Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger,
Kunle Olukotun, and Christopher Re. High-accuracy low-precision training. arXiv preprint
arXiv:1803.03383, 2018.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjec-
tive genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182-197,
2002.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 248-255. IEEE, 2009.
Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.
Nicolo Fusi, Rishit Sheth, and Melih Elibol. Probabilistic matrix factorization for automated machine
learning. In Advances in neural information processing systems, pp. 3348-3357, 2018.
Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.
Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank-revealing qr
factorization. SIAM Journal on Scientific Computing, 17(4):848-869, 1996.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International conference on machine learning, pp. 1737-1746.
PMLR, 2015.
David A Harville. Matrix algebra from a statistician’s perspective, 1998.
10
Published as a conference paper at ICLR 2022
Soheil Hashemi, Nicholas Anthony, Hokchhay Tann, R Iris Bahar, and Sherief Reda. Understanding
the impact of precision quantization on the accuracy and energy of neural networks. In Design,
Automation & Test in Europe Conference & Exhibition (DATE), 2017, pp. 1474-1479. IEEE, 2017.
Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and low-rank svd
via fast alternating least squares. The Journal of Machine Learning Research, 16(1):3367-3402,
2015.
Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey. arXiv preprint arXiv:2004.05439, 2020.
Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455-492, 1998.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah:
Mixed low-precision hardware & software co-design framework for dnns on the edge. arXiv
preprint arXiv:1908.02386, 2019.
Christiane Lemke, Marcin Budka, and Bogdan Gabrys. Metalearning: a survey of trends and
technologies. Artificial intelligence review, 44(1):117-130, 2015.
Miqing Li, Shengxiang Yang, and Xiaohui Liu. Diversity comparison of Pareto front approximations
in many-objective optimization. IEEE Transactions on Cybernetics, 44(12):2568-2584, 2014.
Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of
estimating missingness probabilities under a low nuclear norm assumption. In Advances in Neural
Information Processing Systems, volume 32, pp. 14900-14909, 2019.
Vivek Madan, Mohit Singh, Uthaipon Tantipongpipat, and Weijun Xie. Combinatorial algorithms for
optimal design. In Conference on Learning Theory, pp. 2210-2258. PMLR, 2019.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322,
2010.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740, 2017.
Jonas Mockus. On Bayesian methods for seeking the extremum. In Optimization techniques IFIP
technical conference, pp. 400-404. Springer, 1975.
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. arXiv preprint
arXiv:1907.10597, 2019.
Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change
in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christopher
R6. Low-memory neural network training: A technical report. arXiv preprint arXiv:1904.10631,
2019a.
Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christopher
R6. Low-memory neural network training: A technical report, 2019b.
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalak-
shmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit
floating point (hfp8) training and inference for deep neural networks. Advances in Neural Informa-
tion Processing Systems, 32, 2019.
11
Published as a conference paper at ICLR 2022
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath
Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan.
Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information
Processing Systems, 33, 2020.
Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018.
Bram Wallace and Bharath Hariharan. Extending and analyzing self-supervised learning across
domains. In European Conference on Computer Vision, pp. 717-734. Springer, 2020.
Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed
precision quantization of convnets via differentiable neural architecture search. arXiv preprint
arXiv:1812.00090, 2018.
Jin Wu and Shapour Azarm. Metrics for quality assessment of a multiobjective design optimization
solution set. J. Mech. Des., 123(1):18-25, 2001.
Chengrun Yang, Yuji Akimoto, Dae Won Kim, and Madeleine Udell. Oboe: Collaborative filtering
for automl model selection. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1173-1183, 2019.
Chengrun Yang, Jicong Fan, Ziyang Wu, and Madeleine Udell. Automl pipeline selection: Efficiently
navigating the combinatorial space. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 1446-1456, 2020.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning.
Constructive Approximation, 26(2):289-315, 2007.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. The zipml framework
for training models with end-to-end low precision: The cans, the cannots, and a little bit of deep
learning. arXiv preprint arXiv:1611.05402, 2016.
Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. QPyTorch: A low-precision
arithmetic simulation framework, 2019.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms—a
comparative case study. In International conference on parallel problem solving from nature, pp.
292-301. Springer, 1998.
12
Published as a conference paper at ICLR 2022
All the tables in this appendix are shown in the last page.
A Datasets and Low-Precision Formats
The 87 datasets chosen in our experiment are listed in Table 2. Note that the datasets contain images
of two different resolutions: 32 denotes resolution 3 × 32 × 32, and 64 denotes 3 × 64 × 64. We list
our choices of Format A and B in Table 3 and 4, respectively. In each evaluation, the low-precision
configuration is composed of one Format A (for the activations and weights) and one Format B (for
the optimizer), as detailed in Section 3.1. Therefore, the total number of low-precision configurations
used in our experiment is 99.
B The SoftImpute Algorithm
There are several versions of SoftImpute; (Hastie et al., 2015) gives a nice overview. We use the
version in (Mazumder et al., 2010).
At a high level, S OFTIMPUTE iteratively applies a soft-thresholding operator Sλ on the partially
observed error matrix with a series of decreasing λ values. Each Sλ replaces the singular values {σi}
with {(σi - λ)+}. The regularization parameter λ can be set in advance or ad hoc, by convergence
dynamics.
The pseudocode for the general SOFTIMPUTE algorithm is shown as Algorithm 1, in which Pω(E)
is a matrix with the same shape as E, and has the (i,j)-th entry being Eij if (i,j) ∈ Ω, and 0
otherwise. In our implementation, λi = λti for each step i, and ti is the step size from TFOCS
backtracking (Becker et al., 2011).
Algorithm 1 SOFTIMPUTE
Input: a partially observed matrix Pω(E) ∈ Rn×d, number of iterations I, a series of decreasing λ
values {λi }iI=1
Output: an estimate E
1	for i = 1 to I do
二 _ _____________ _ , ^.
2	E - Pω(E) + Pqc (E)
3	U, Σ, V 一 SVd(E)
4	E 一 USλ(Σ)V>
C Algorithms for Experiment Design
As mentioned in Section 3.2, there are mainly two algorithms to solve Problem 1, the D-optimal
experiment design problem: convexification and greedy.
The convexification approach relaxes the combinatorial optimization problem to the convex optimiza-
tion problem
minimize log det Pjd=1 vjyjyj>-
d
subject to P vj ≤ l
j=1
vj ∈ [0, 1], ∀j ∈ [d]
(2)
that can be solved by a convex solver (like SLSQP). Then we sort the entries in the optimal solution
v* ∈ Rd and set the largest l entries to 1 and the rest to 0.
The greedy approach (Madan et al., 2019; Yang et al., 2020) maximizes the submodular objective
function by first choosing an initial set of configurations by column-pivoted QR decomposition, and
then greedily adding new configuration to the solution set S in each step until |S | = l. The greedy
stepwise selection algorithm is shown as Algorithm 2, in which the new configuration is chosen by
13
Published as a conference paper at ICLR 2022
the Matrix Determinant Lemma* * (Harville, 1998), and Xt-1 is updated by the close form from the
Sherman-Morrison Formula* (Sherman & Morrison, 1950). The column-pivoted QR decomposition
selects top k pivot columns of Y ∈ Rk×d to get the index set S0, ensuring that X0 = Pj∈S yj yj> is
non-singular.
Algorithm 2 Greedy algorithm for D-optimal experiment design
Input: design vectors {yj}jd=1, in which yj ∈ Rk; maximum number of selected configurations l;
initial set of configurations S0 ⊆ [d], s.t. X0 = Pj∈S yj yj> is non-singular
Output: the selected set of designs S ⊆ [d]
1	S J So
2	while |S | ≤ l do
3	i J argmaχj∈[d]∖sy>X-1yj
4	SJS∪{i}
5	Xt+1 J Xt+ yiyi>
The ConveXification approach empirically works because most entries of the optimal solution V are
close to either 0 or 1. The histogram in Figure 13 shows an example when we use rank k = 5 to
factorize the entire error matrix and set l = 20.
In terms of solution quality, the relative
error plot in Figure 13 shows that the
greedy approach consistently outperforms
in terms of the relative matrix completion
error for each dataset in ED-MF solutions.
Since the greedy approach is also more
than 10× faster than convexification (im-
plemented by scipy), we use the greedy
approach throughout all experiments for
the rest of this paper.
entry values of v*
(a)	histogram of v* entries
0 0.1-
gl
φ H 0.08-
琮-H 0.06-
Convexification
greed greedy
⅛⅛≠H=
密	5	10	15
且	l
(b)	relative error
Figure 13: Convexification vs greedy for ED.
D	Information to hardware design
Among all 87 datasets (each has its own error-memory tradeoff), the promising configurations that
we identify among all 99 configurations are as below (number of sign bits - number of exponent bits
-number of mantissa bits).
•	Format A:	1-4-1, Format B:	1-6-7	(appears on 60	out of	87 Pareto	frontiers)
•	Format A:	1-4-1, Format B:	1-7-7	(appears on 33	out of	87 Pareto	frontiers)
•	Format A:	1-3-1, Format B:	1-7-7	(appears on 30	out of	87 Pareto	frontiers)
•	Format A:	1-4-2, Format B:	1-6-7	(appears on 24	out of	87 Pareto	frontiers)
•	Format A:	1-5-2, Format B:	1-6-7	(appears on 21	out of	87 Pareto	frontiers)
E More details on experiments
We first show the plot of explained variances of top principal components in Figure 14: how much
variance in our data do the first several principal components account for (Bishop, 2006). This
quantity is computed by the ratio of sum of squares of the first k singular values to that of all singular
values. In Figure 14, we vary k from 1 to 30, corresponding to the decay of singular values shown in
Figure 6(b). We can see the first singular value already accounts for 99.0% of the total variance, and
the first two singular values account for more than 99.5%. This means we can pick a small rank for
PCA in meta-training and still keep the most information in our meta-training data.
*The Matrix Determinant Lemma states that for any invertible matrix A ∈ Rk×k and a, b ∈ Rk, det(A +
ab>) = det(A)(1+ b>A-1a), Thus argmaXj∈w∖s det(Xt + yjy>) = argmaxj∈[d]∖sy>X-1yj.
*The Sherman-Morrison Formula states that for any invertible matrix A ∈ Rk×k and a, b ∈ Rk, (A +
ab>)-1 = A-1 -
A-1ab>A-1
1 + b> A-1a .
14
Published as a conference paper at ICLR 2022
① ωu∙s,IEA
denialpx
100.0%-
99.0%
99.5%∙
0
10 20 30
# first several singular values
Figure 14: Explained variance of the first several singular values in Figure 6(b).
Figure 15: Histograms of error and memory. The
dashed lines are the respective medians.
memory (MB)
50
^40
①
瑞30
鼠
计10
0
30 40 50 60 70 80 90 100
# configurations below median memory
Figure 16: Histogram of datasets by the
number of configurations that take memo-
ries less than the overall median of 816MB.
We use the ratio of incorrectly classified images as our error metric. Figure 15 shows histograms of
error and memory values in our error and memory matrices from evaluating 99 configurations on 87
datasets. The vertical dashed lines show the respective medians: 0.78 for test error and 816MB for
memory. We can see both the test error and memory values span a wide range. The errors come from
training a wide range of low-precision configurations with optimization hyperparameters not fine-
tuned, and are thus larger than SOTA results. In Section 4.3 of the main paper and Section E.4 here, we
show that training for a larger number of epochs and with some other optimization hyperparameters
yield the same error-memory tradeoff as Figure 1(a) in the main paper.
In meta-LOOCV settings with a meta-test memory cap at the median memory 816MB, Figure 16
shows a histogram of number of feasible configurations on each of the 87 datasets. There are “cheap”
(resolution 32) datasets on which each of the 99 configurations takes less than the cap, and “expensive”
(resolution 64) datasets on which the feasible configurations are far less than 99.
E.1 Introduction to Random-MF, QR-MF and BO
•	Random selection with matrix factorization (Random-MF). Same as ED-MF, RANDOM-
MF predicts the unevaluated configurations by linear regression, except that it selects the
configurations to evaluate by random sampling.
•	QR decomposition with column pivoting and matrix factorization (QR-MF). QR-MF first
selects the configurations to evaluate by QR decomposition with column pivoting: EP = QR,
in which the permutation matrix P gives the most informative configurations. Then it predicts
unevaluated configurations in the same way as ED-MF and Random-MF.
•	Bayesian optimization (BO). Bayesian optimization is a sequential decision making framework
that learns and optimizes a black-box function by incrementally building surrogate models and
choosing new measurements (Frazier, 2018). It works best for black-box functions that are
expensive to evaluate and lack special structures or derivatives. We compare ED-MF with two
BO techniques. The first technique, BO-MF, applies BO to the function f1 : Rk → R that maps
low-dimensional configuration embeddings {Y:,j }jd=1 to the test errors of the configurations
on the meta-test dataset {ejnew}jd=1
(Fusi et al., 2018). The embeddings come from the same
low-rank factorization of the error matrix E as in ED-MF. The second, BO-FULL, applies
BO to the function f2 : Rn → R that directly maps columns of the error matrix {E:,j }jd=1
to {ejnew}jd=1. To learn either of these black-box functions, we start by evaluating a subset of
configurations S ⊆ [d] and then incrementally select new configurations that maximize the
expected improvement (MockUs, 1975; Jones et al., 1998).
15
Published as a conference paper at ICLR 2022
E.2 Additional meta-training results: non-uniform sampling
In non-uniform sampling, we
sample each entry of the er-
ror matrix E with probabilities
Pij = σ(1∕Wij), in which σ
maps {1/Wij} into an interval
[0, pmax] ⊆ [0, 1] according to
the cumulative distribution func-
tion of {1/Wij }. By varying
pmax , we change how we ag-
gressively sample the configu-
rations, how different the sam-
pling probabilities are between
large and small memory configu-
j }	CIFAR-10
■
-06
ρ
⅛0.4-
ω
∣0.2-
φ
J 0.0	20%	40%
relative memory usage
(a) relative error
convergence 5 0 0 0 III ,246	卜	CIFAR-10
0.0	20%	40%
relative memory usage
(b) convergence
0-6Π	t—CIFAR-10
0.4-
：0.2.| I
0.0	20%	40%
relative memory usage
(c) HyperDiff
Figure 17: Pareto frontier estimation performance in PEPPP meta-
training with non-uniform sampling of configurations. The violins
and scatters have the same meaning as Figure 9. The x axis
measures the memory usage relative to exhaustive search.
rations, and also the percentage of memory needed. In Figure 17, we vary pmax from 0.1 to 1 and see
that the quality of the estimated Pareto frontier improves with more memory.
E.3 Additional Meta-LOOCV Results
In the main paper, we have shown the performance of Pareto frontier estimates and configuration
selection for Setting I and IV. For the rest of the settings in Table 1, we conduct meta-LOOCV in the
same way and show the results of Pareto frontier estimates in Figure 18.
Setting II. We have the 816MB memory cap for meta-test, and have the full meta-training error
matrix in each split.
05O
(3S 中 Ueag 8uJ3AU0D
20%	40%
relative memory usage
(3S 中 UB3日)ffisJ9dAH
号“HX*v⅛**
(a)	Setting II convergence
relative memory usage
(b)	Setting II HyperDiff
(3S-HIre3日)
5%
relative memory usage
(c)	Setting III convergence
(3S =I= uea日)3DU3a,3AUCO
relative memory usage
I I
(3S =I= uea日)β≡,I3dAH
relative memory usage
05O
(3S 中 UB3日)aou:®AUeO
≡
20%
relative memory usage
J	Ilk
(3S =I= u∙i3日)«sJ3dAH
relative memory usage
(d) Setting In HyPerDiff
%
I I
GS 干 UB3日)aαJ3dAH
relative memory usage
> 4 & W*西电 *35


(e) Setting V convergence (f) Setting V HyPerDiff (g) Setting VI convergence (h) Setting VI HyPerDiff
E ED-MF (PEPPP) Q QR-MF B BO-MF Q Bθ-fuιι “ rANDom-MF
Figure 18: Pareto frontier estimates in meta-LOOCV Setting II (full meta-training error matrix, a
816MB memory cap), Setting III (uniformly sample 20% meta-training measurements, no meta-test
memory cap), Setting V (non-uniformly sample 20% meta-training measurements, no meta-test
memory cap), and Setting VI (non-uniformly sample 20% meta-training measurements, an 816MB
meta-test memory cap). Each error bar is the standard error across datasets. ED-MF is among the
best in every setting and under both metrics.
Setting III. We have no memory cap for meta-test, and uniformly sample 20% configurations for
meta-training in each split.
Setting V. We have no memory cap for meta-test, and non-uniformly sample 20% configurations for
meta-training in each split. The sampling method is the same as in meta-training (Figure 17).
Setting VI. We have the 816MB memory cap for meta-test, and non-uniformly sample 20% meta-
training configurations in the same way as Setting V above.
16
Published as a conference paper at ICLR 2022
shcope 001
Figure 19:	Errors of 99 configurations trained for different numbers of epochs.
- - - -
8 6 4 2
- - - -
Oooo
,ɪo,nə
0.050 200	250	300
memory (MB)
(a) fixed learning rate
8 6 4 2
- - - -
Oooo
,ɪo,nə
150	200	250	300
memory (MB)
(b) tuned learning rate
+ dominated (ɔ true non-dominated
Figure 20:	CIFAR-10 error-memory tradeoff. Figure (a) has learning rate 0.001 for all low-precision
configurations. Figure (b) shows the tradeoff with tuned learning rates: at each low-precision
configuration, the lowest test error achieved by learning rates {0.01, 0.001, 0.0001} is selected.
E.4 Tuning Optimization Hyperparameters
E.4. 1 Number of epochs
The low-precision networks still underfit after 10 epochs of training. This situation is typical:
underfitting due to budget constraints is unfortunately common in deep learning. Luckily, meta-
learning the best precision will succeed so long as the validation errors are correctly ordered, even
if they all overestimate the error of the corresponding fully trained model. Indeed, our validation
errors correlate well with the errors achieved after further training: on CIFAR-10, the Kendall tau
correlation between ResNet-18 errors at 10 epochs and 100 epochs is 0.73, shown in Figure 19. The
lowest error at 100 epochs is 9.7%, only approximately 2% higher than SOTA (Bungert et al., 2021).
E.4.2 Learning rate
Previously, we have shown that PEPPP can estimate the error-memory tradeoff and select a promising
configuration with other hyperparameters fixed. In practice, users may also want to tune hyperparam-
eters like learning rate to achieve the lowest error. Here, we tune learning rate in addition to precision,
and show that the methodology can be used in broader settings of hyperparameter tuning.
Figure 20 shows that the error-memory tradeoff still exists with a fine-tuned learning rate for each
configuration. With the learning rate tuned across {0.01, 0.001, 0.0001}, the test errors are in general
smaller, but high-memory configurations still achieve lower errors in general. Thus the need to
efficiently select a low-precision configuration persists.
Our approach can naturally extend to efficiently selecting optimization and low-precision hyperpa-
rameters. We perform the meta-training and meta-LOOCV experiments on a subset of CIFAR-100
partitions with multiple learning rates {0.01, 0.001, 0.0001}. The error and memory matrices we use
here have 45 rows and 99 columns, respectively. Learning rate and low-precision configuration are
collapsed into the same dimension: each row corresponds to a combination of one CIFAR-100 subset
and one of the learning rates {0.01, 0.001, 0.0001}, as shown in Table 5. We say that these error and
memory matrices are LR-tuned. Figure 21 shows the LR-tuned error matrix also has a fast singular
value decay. The other hyperparameters are the same as in LR-fixed experiments, except that we use
17
Published as a conference paper at ICLR 2022
104p----------------------
「02'
100- ∙.
0	5	10	15
index i
Figure 21:	Singular value decay of the LR-tuned error matrix.
①A=-0,I
0.3」
0.2
0.1	E-
CC	a塞2 T
0.0	10%	30%
relative memory usage
(a)	relative error
0.4- ɪ
0.2、	I
o o∣__'王」壬 t
0.0 10%	30%
relative memory usage
(b)	convergence
0.3
0.2
0.1
0.0
basdxH
relative memory usage
(c) HyperDiff
。011出。ALlOo
I CIFAR-100 (aquatic mammals) with learning rate 0.01
Figure 22:	The Pareto frontier estimation performance in meta-training, with uniform sampling of
configurations on the LR-tuned error and memory matrices. Similar to Figure 9, the violins show the
distribution of the performance on individual datasets, and the error bars (blue) show the range. The
red error bars show the standard deviation of the error on CIFAR-100 aquatic mammals and learning
rate 0.01, across 100 random samples of the error matrix. Figure (a) shows the matrix completion
error for each dataset; Figure (b) and (c) show the performance of the Pareto frontier estimates in
convergence and HyperDiff.
batch size 128 and train for 100 epochs. The meta-training and meta-LOOCV results are consistent
with those in Sections 4.1 and 4.2, respectively:
•	In meta-training, we first uniformly samPle the error matrix and study the PerformanCe of
matrix ComPletion and Pareto frontier estimation. Figure 22 shows the matrix ComPletion
error and Pareto frontier estimation metriCs. Then we do non-uniformly samPling and get
Figure 23, the LR-tuned version of Figure 17 in the main PaPer.
•	In meta-test, we evaluate settings in Table 1 in the main PaPer. We get the PerformanCe of
Pareto frontier estimates in Figure 24, the LR-tuned version of Figure 11 in the main PaPer.
ED-MF steadily outPerforms.

06
04
02
0.0-
relative memory usage
(a) relative error
20%	40%
0.4
0.2
0.0
①Oumjoauoo
relative memory usage
(b) ConvergenCe
0.2
0.1
0.0	20%	40%
relative memory usage
(C) HyPerDiff

I CIFAR-100 (aquatic mammals) with learning rate 0.01
Figure 23:	The Pareto frontier estimation performance in meta-training, with non-uniform sampling
of configurations on the LR-tuned error and memory matrices. The violins and scatters have the same
meaning as Figure 17 in the main paper.
18
Published as a conference paper at ICLR 2022
O(əs =F UT3ΘIU)
əɔuəɪəAUoɔ
relative memory usage
%
Λ O
O(əs =F u^əlu)
αjθdXH
5%	10%
relative memory usage
7 0
Aəs =F u^əlu)
əɔuəɪəAUoɔ
10%	20%	30%
relative memory usage
0^φ∞ =F u03ΘU1)
sα,I9dKH
0' 10%	20%	30%
relative memory usage

(a) Setting I convergence
(b) Setting I HyperDiff (c) Setting IV convergence (d) Setting IV HyperDiff
E ED-MF (PEPPP) Q QR-MF
BO-MF <> BO-Fuii * RANdom-MF
Figure 24:	Pareto frontier estimates in meta-LOOCV settings on the LR-tuned error and memory
matrices. Each error bar is the standard error across datasets.
1 O
0)es ± naem(
ecnegrevnoc
.1 0
0)es ± naem(
ffiDrepyH
%
relative memory usage
* *K*⅛ * ⅛+ H*μ y I
5%	10%
relative memory usage
.1
0)es ± naem(
ecnegrevnoc
5%	10%
relative memory usage
Λ O
O(əs =F u^əlu)
α,I9dXH
5%	10%
relative memory usage
%
5
(a) ResNet-34	(b) ResNet-34	(c) VGG variants	(d) VGG variants
E ED-MF (PEPPP) Q QR-MF B BO-MF B Bθ-fuιι / RANdom-MF
Figure 25:	Pareto frontier estimates in meta-LOOCV Setting I when learning across architectures:
from ResNet-18 to either ResNet-34, or to VGG variants. Each error bar is the standard error across
datasets. The x axis measures the memory usage relative to exhaustively searching the permissible
configurations. ED-MF consistently picks the configurations that give the best PF estimates.
E.5 Learning across architectures
We show that on 10 ImageNet partitions, PEPPP with ED-MF is able to estimate the error-memory
tradeoff of the low-precision configurations on of ResNet-34, VGG-11, VGG-13, VGG-16 and VGG-
19. The 10 ImageNet partitions have WordNet IDs {n02470899, n01482071, n02022684, n03546340,
n07566340, n00019613, n01772222, n03915437, n02489589, n02127808} and are randomly selected
from the 50 ImageNet subsets on which we collected the error matrix. On these ImageNet partitions,
we use the performance of ResNet-18 as meta-training data, and either the performance of ResNet-18
or VGG variants as meta-test data. In Figure 25, we can see that ED-MF is steadily among the best
in Pareto frontier estimation, and there is no statistical difference between the estimation performance
on ResNet-34 and VGG variants.
Next, we compare the performance of the following two cases:
i Meta-learning across datasets with performance from the same architecture: For example,
to learn the error-memory tradeoff of ResNet-18 on n02470899, we only use the tradeoffs of
ResNet-18 on 9 other ImageNet partitions as the meta-training data.
ii Meta-learning across datasets with performance from both the same and other architectures:
For example, the error-memory tradeoff of ResNet-18 on n02470899, we not only use the
tradeoffs of ResNet-18 on 9 other ImageNet partitions, but also use those of ResNet-34,
VGG-11, VGG-13, VGG-16 and VGG-19 on the 9 partitions as the meta-training data.
Figure 26	shows that Case ii outperforms Case i in better estimating the error-memory tradeoffs on
different architectures and datasets.
19
Published as a conference paper at ICLR 2022
ecnegrevno
。ED-MF (Case i)
靠 ED-MF (Case ii)
■0.02
5%	10%
relative memory usage
Figure 26: Benefit of meta-learning across architectures. Each error bar is the standard error across
architecture-dataset combinations (e.g., ResNet-18 + n02470899 is a combination). The x axis
measures the memory usage relative to exhaustively searching the permissible configurations.
20
Published as a conference paper at ICLR 2022
Table 2: Datasets
Table 3: Format A (for activations and weights)
index dataset name		resolution	# points	index # exponent bits # mantissa Bits		total bit width	
1	CIFAR10	32	60000	1	31	5	
2	CIFAR100 (aquatic mammals)	32	3000	2	32	6	
3	CIFAR100 (fish)	32	3000	3	33	7	
4	CIFAR100 (flowers)	32	3000	4	34	8	
5	CIFAR100 (food containers)	32	3000	5	41	6	
6	CIFAR100 (fruit and vegetables)	32	3000	6	42	7	
7	CIFAR100 (household electrical devices)	32	3000	7	43	8	
8	CIFAR100 (household furniture)	32	3000	8	44	9	
9	CIFAR100 (insects)	32	3000	9	51	7	
10	CIFAR100 (large carnivores)	32	3000	10	52	8	
11 12	CIFAR100 (large man-made outdoor things) CIFAR100 (large natural outdoor scenes)	32 32	3000 3000	11	53	9	
							
13	CIFAR100 (large omnivores and herbivores)	32	3000				
14	CIFAR100 (medium-sized mammals)	32	3000				
15	CIFAR100 (non-insect invertebrates)	32	3000				
16	CIFAR100 (people)	32	3000				
17 18	CIFAR100 (reptiles) CIFAR100 (small mammals)	32 32	3000 3000		Table 4: Format B (for optimizer)		
19	CIFAR100 (trees)	32	3000				
							
20	CIFAR100 (vehicles 1)	32	3000	index # exponent bits # mantissa Bits		total bit width	
21	CIFAR100 (vehicles 2)	32	3000				
					6	7	14	
22	aircraft	64	6667	1			
23	cub	64	10649	2	69	16	
24	dtd	64	3760	3	6	11	18	
25	isic	64	22802	4	77	15	
26	merced	64	1890	5	79	17	
27	scenes	64	14088	6	7	11	19	
28	ucf101	64	12024	7	87	16	
29	daimlerpedcls	64	29400	8	89	18	
30	gtsrb	64	26640	9	8	11	20	
31	kather	64	4000				
32	omniglot	64	25968				
33	svhn	64	73257				
34	vgg-flowers	64	2040				
35	bach	64	320				
36 37	protein atlas minc	64 64	12113 51750	Table 5	: Datasets and learning rates in Section 4.3		
38	ImageNet (bag)	64	6519				
39	ImageNet (retriever)	64	6668	index dataset name			learning rate
40 41 42 43 44 45 46	ImageNet (domestic cat) ImageNet (stick) ImageNet (turtle) ImageNet (finch) ImageNet (watchdog) ImageNet (footwear) ImageNet (salamander)	64 64 64 64 64 64 64	6750 6750 6750 6750 6404 6587 6750	1 2 3 4 5 6 7	CIFAR100 (aquatic mammals) CIFAR100 (fish) CIFAR100 (flowers) CIFAR100 (food containers) CIFAR100 (fruit and vegetables) CIFAR100 (household electrical devices) CIFAR100 (household furniture)		0.01 0.01 0.01 0.01 0.01 0.01 0.01
47	ImageNet (anthropoid ape)	64	6750	8	CIFAR100 (insects)		0.01
48 49	ImageNet (elasmobranch) ImageNet (shorebird)	64 64	6750 6750	9 10	CIFAR100 (large carnivores) CIFAR100 (large man-made outdoor things)		0.01 0.01
50 51	ImageNet (housing) ImageNet (foodstuff)	64 64	6605 6750	11 12	CIFAR100 (large natural outdoor scenes) CIFAR100 (large omnivores and herbivores)		0.01 0.01
52 53 54 55 56 57 58 59 60 61 62 63 64 65	ImageNet (substance) ImageNet (spider) ImageNet (percussion instrument) ImageNet (New World monkey) ImageNet (big cat) ImageNet (box) ImageNet (fabric) ImageNet (kitchen appliance) ImageNet (mollusk) ImageNet (hand tool) ImageNet (butterfly) ImageNet (stringed instrument) ImageNet (boat) ImageNet (rodent)	64 64 64 64 64 64 64 64 64 64 64 64 64 64	6643 8100 8082 8100 8100 7829 7862 7773 8100 8054 8100 8100 8006 8006	13 14 15 16 17 18 19 20 21 22 23 24 25 26	CIFAR100 (medium-sized mammals) CIFAR100 (non-insect invertebrates) CIFAR100 (people) CIFAR100 (reptiles) CIFAR100 (aquatic mammals) CIFAR100 (fish) CIFAR100 (flowers) CIFAR100 (food containers) CIFAR100 (fruit and vegetables) CIFAR100 (household electrical devices) CIFAR100 (household furniture) CIFAR100 (insects) CIFAR100 (large carnivores) CIFAR100 (large man-made outdoor things)		0.01 0.01 0.01 0.01 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
66	ImageNet (toiletry)	64	7522	27	CIFAR100 (large natural outdoor scenes)		0.001
67	ImageNet (computer)	64	7696	28	CIFAR100 (large omnivores and herbivores)		0.001
68	ImageNet (shop)	64	9400	29	CIFAR100 (medium-sized mammals)		0.001
69	ImageNet (musteline mammal)	64	9450	30	CIFAR100 (non-insect invertebrates)		0.001
70 71 72 73 74 75 76 77 78 79 80 81 82	ImageNet (Old World monkey) ImageNet (bottle) ImageNet (fungus) ImageNet (truck) ImageNet (spaniel) ImageNet (sports equipment) ImageNet (game bird) ImageNet (seat) ImageNet (fruit) ImageNet (weapon) ImageNet (beetle) ImageNet (toy dog) ImageNet (decapod crustacean)	64 64 64 64 64 64 64 64 64 64 64 64 64	9450 9205 9450 9309 9119 9450 9450 9126 9450 9450 10800 9832 10800	31 32 33 34 35 36 37 38 39 40 41 42 43	CIFAR100 (people) CIFAR100 (reptiles) CIFAR100 (aquatic mammals) CIFAR100 (fish) CIFAR100 (flowers) CIFAR100 (food containers) CIFAR100 (fruit and vegetables) CIFAR100 (household electrical devices) CIFAR100 (household furniture) CIFAR100 (insects) CIFAR100 (large carnivores) CIFAR100 (large man-made outdoor things) CIFAR100 (large natural outdoor scenes)		0.001 0.001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001
83	ImageNet (fastener)	64	10675	44	CIFAR100 (large omnivores and herbivores)		0.0001
84 85	ImageNet (timepiece) ImageNet (dish)	64 64	10164 10556	45	CIFAR100 (medium-sized mammals)		0.0001
86	ImageNet (mechanical device)	64	10617				
87	ImageNet (colubrid snake)	64	12150				
21