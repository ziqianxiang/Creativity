Published as a conference paper at ICLR 2022
Learning Distributionally Robust Models at
Scale via Composite Optimization
Farzin Haddadpour
Yale Institute for Network Science
Yale University
farzin.haddadpour@yale.edu
Mehrdad Mahdavi
Department of Computer Science & Engineering
The Pennsylvania State University
mzm616@psu.edu
Mohammad Mahdi Kamani
Wyze Labs Inc.
mmkamani@alumni.psu.edu
Amin Karbasi
Yale Institute for Network Science
Yale University
amin.karbasi@yale.edu
Ab stract
To train machine learning models that are robust to distribution shifts in the data,
distributionally robust optimization (DRO) has been proven very effective. How-
ever, the existing approaches to learning a distributionally robust model either
require solving complex optimization problems such as semidefinite programming
or a first-order method whose convergence scales linearly with the number of data
samples- which hinders their scalability to large datasets. In this paper, We show
how different variants of DRO are simply instances of a finite-sum composite
optimization for which we provide scalable methods. We also provide empirical
results that demonstrate the effectiveness of our proposed algorithm with respect to
the prior art in order to learn robust models from very large datasets.
1 Introduction
Conventional machine learning problem aims at learning a model based on the assumption that
training data and test data come from same data distribution. However, this assumption may not hold
in various practical learning problems where there is label shift (Zhang et al., 2020a), distribution shift
(Sagawa et al., 2019), fairness constraints (Hashimoto et al., 2018), and adversarial examples (Sinha
et al., 2017), to name a few. Distributionally robust optimization (DRO), which has recently attracted
remarkable attention from the machine learning community, is a common approach to deal with the
aforementioned uncertainties (Chen et al., 2017; Duchi & Namkoong, 2016; Rahimian & Mehrotra,
2019). Defining the empirical distribution of the training data of size m by Pm，* Pm=I δ^. where
δ is the Dirac delta function, the goal of DRO is to solve the following optimization problem
inf [Ψ(x)，suPξ∈Q Eq ['(x; ξ)]],
(1)
where ξ is a data sample randomly drawn from distribution Q, '(x; ξ) is the corresponding loss
function and EQ ['(x, ξ)] is the expected loss over distribution Q which belongs to uncertainty set IUm.
The uncertainty set Um is defined as Um , {Q : d(Q, Pm) ≤ } indicates the ball of a distribution
with center Pm and also d(P, Q) is a distance measure between probability distribution P and Q.
We note this uncertainty set captures the distribution shift hence Eq. (1) minimizes the worse data
distribution. Prior studies (Ben-Tal et al., 2013; Bertsimas et al., 2018; Blanchet et al., 2019; Esfahani
& Kuhn, 2018; Pourbabaee, 2020) considered different uncertainty sets (see Definition 3.1 in Esfahani
& Kuhn (2018)) for which they proposed equivalent reformulations of Eq. (1) based on the specific
choice of Um .
To solve the above min-max optimization problems, majority of prior studies heavily rely on either
semidefinite programming (Esfahani & Kuhn, 2018) or stochastic primal-dual methods both for
convex (Deng et al., 2021; Nemirovski et al., 2009; Juditsky et al., 2011; Yan et al., 2019; 2020;
Namkoong & Duchi, 2016) and non-convex (deep learning) objectives (Yan et al., 2020). While
1
Published as a conference paper at ICLR 2022
primal-dual methods can be used as an approach to solve min-max optimization problems, it suffers
from a few downsides. First and foremost, they need to store a probability distribution of constrained
violation of dimension m corresponding to dual variables. Additionally, available primal-dual
methods often demand data sampling that corresponds to the probability distribution over m data
samples which introduces additional cost over uniform sampling. Finally, while majority of prior
studies are limited to DRO problems with convex objectives, establishing tight convergence rate for
DRO problems with penalty with non-convex objectives is still lacking.
To overcome these issues, we consider three different reformulations of Eq. (1), corresponding to three
different choices of uncertainty sets Um namely, (1) DRO with Wasserstein metrics, (2) DRO with
χ2 divergence metrics, and (3) DRO with regularized entropy metrics (also known as KL) and show
in Section 2 that all aforementioned DRO notions are indeed different instances of a deterministic
composite optimization and can be solved by reducing to an instances of the following problem:
min ψ(X), r(x)+—χm hi(X)+f 1—χm gi(X)),	⑵
x	m	i=1	m	i=1
where we suppose r(X) is convex and a relatively simple function, f(X) : Rp → R and hi (X) :
Rd → R for1 ≤ i ≤ m are scalar-valued functions, and gi(X) : Rd → Rp for1 ≤ i ≤ m are
vector-valued functions. On the road to solve problem (2) at scale, we also develop a novel algorithm
for heavily constrained optimization problems (Narasimhan et al., 2020b; Wang & Bertsekas, 2015;
2016) that rather surprisingly invokes a single projection through the course of optimization. This
algorithm is of independent interest and addresses the scalability issues raised in applications such as
fairness (Donini et al., 2018; Zafar et al., 2019).
We summarize the main contributions of our paper below:
•	We provide a large-scale analysis of DRO with Wasserstein distance and heavily constrained
reformulation when the objective function is strongly-convex. Our result relies on a novel
mini-batch constraint sampling for handling heavily-constrained optimization problems. As
summarized in Table 1, our convergence analysis improves the state-of-the-art both in terms
of the dependence on the convergence error as well as the number of constraints m.
•	We represent a large-scale analysis of DRO with non-convex objectives and χ2 or KL diver-
gences and propose a distributed varaint to further improve scalability of DRO problems.
•	We verify our theoretical results through various extensive experiments on different datasets.
In particular, we show that our proposed method outperforms recent methods in DRO for
heavily constrained problems with a great reduction in time complexity over them.
The proofs of all the theorems are provided in the appendix.
1.1 Related Work
DRO and connections to heavily constrained optimization. As mentioned earlier, DRO has many
different formulations, depending on the divergence metrics used (e.g., Wasserstein , χ2 or KL).
While Duchi & Namkoong (2021); Namkoong & Duchi (2016); Shapiro (2017) consider constrained
or penalized DRO formulation, Levy et al. (2020); Sinha et al. (2017) formulate the underlying
optimization problem as unconstrained. One of the contributions of our paper is to provide a unifying
framework through the language of composite optimization and treat all these variants similarly.
In particular, when the objective function is convex, Levy et al. (2020) recently proposed scalable
algorithms for different variants of the DRO problems with , e.g., χ2 or KL divergence metrics. Our
unifying approach readily extends those results to the more challenging non-convex setting for which
we are unaware of any prior work with convergence guarantees (for instance, Hashimoto et al. (2018)
studied DRO with χ2-divergence but did not provide any convergence guarantee). Similarly, Esfahani
& Kuhn (2018); Kuhn et al. (2019) formulated DRO with Wasserstein distance as an instance of
constrained optimization. Notably, they require ti impose one constraint per training data point
and to solve such a constrained problem they proposed a semi-definite program. Even though the
formulation is very novel, it cannot scale. We, in contrast, consider such a heavily constrained
optimization as an instance of a composite optimization for which we provide a scalable solution.
What is rather surprising about our method is that it only checks a batch of constraints per iteration,
inspired by Cotter et al. (2016), and performs a single projection at the final stage of the algorithm
in order to provide an -optimal solution in the case of strongly convex objectives. Moreover, in
2
Published as a conference paper at ICLR 2022
contrast to Cotter et al. (2016), we do not keep a probability distribution over the set of constraints.
We should also remark that our convergence guarantees achieve the known lower bounds in terms
of accuracy and the number of constraints m. Finally, we should highlight the difference of our
algorithm and Frank-Wolfe (FW) (Frank et al., 1956; Jaggi, 2013; Zhang et al., 2020b). While FW
does not require a projection oracle, it performs a linear program over the set of constraints at each
iteration. In contrast, our heavily- constrained optimization solution performs a single projection
without the overhead of running a linear program at each iteration.
Stochastic composite optimization. The general stochastic composite optimization minx Ψ(x) ,
r(x) + f (Eξ [gξ(x))] has recently received a lot of attentions (Qi et al., 2020b;a; Wang et al., 2017;
Kalogerias & Powell, 2019). Our reformulation of DRO variants is a finite-sum instance of this general
problem. More concretely, Huo et al. (2018); Lian et al. (2017); Zhang & Xiao (2019a) aimed to
solve the following finite-sum problem minx
[ψ(x) , r(X) + 1 Pn=Ifj (m1 Pm=I gi(X))], using
SVRG or SAGA (Defazio et al., 2014). In contrast, our proposed algorithm is inspired by Zhang
& Xiao (2019a) and generalizes their method to the case where the extra terms hi (X) in Eq. (2) are
non-zero. We should also note that Qi et al. (2020a) proposed a similar idea in the context of online
learning for DRO problems with KL divergence. Our work in contrast provides guarantees for DRO
with both constraints or penalty terms.
2 DRO via Finite-Sum Composite Optimization
In this section, we discuss in detail how a finite-sum composite optimization (2) can unify various
notions of distributionally robust learning, where some of which rely on heavily constrained optimiza-
tion subroutines. While much research effort has been devoted to develop a specialized algorithm for
each notion, our reduction paves the way to developing a scalable algorithm, discussed in Section 3.
DRO with Wasserstein distance. An equivalent and tractable reformulation of Eq. (1) is provided
in Esfahani & Kuhn (2018); Kuhn et al. (2019), which can be regarded as a heavily constrained
optimization problem as follows:
1m
min r(x)，一	fi(x)	subject to	gi(x) ≤ 0, ∀i ∈ [m].	(3)
xm
i=1
where gi (x) are functions related to loss function as well as slack variables (please see Appendix A for
more details). Naively solving optimization problem (3) suffers from the computational complexity
due to the large number of constraints m. To efficiently solve the optimization problem (3), inspired by
Mahdavi et al. (2012) and Cotter et al. (2016), we pursue the smoothed constrained reduction approach
and introduce the augmented optimization problem (see Appendix B) of the form min Ψ(X) ,
x
[r(x) + Ylη(g(x))] where gi(x)，exp (αgγ(x)) and g(x) = m+1 [1 + Pm=I gi(x)]. We can
see that this optimization problem is a special case of the optimization problem Eq. (2) where
r(x) = f (x), f (mm Pmm=I gm(x)) = Ylng(x), and h(x) = 0. In contrast to Cotter et al. (2016)
that requires an extra storage cost of probability distribution of dimension m, and relatively poor
convergence rate in terms of m and accuracy , we propose an algorithm that simply checks a batch
of constraints and achieves the optimum dependency in terms of m and .
DRO with χ2 -divergence.
divergence metric as follows:
min
x
The second type of DRO problem we consider utilizes the χ2-
m
0≤pi≤1m,Paxim=1pi=1Xi=1pifi(Xi)-YDχ2(p).
(4)
where the χ2 divergence is defined as the distance between the uniform distribution and an arbitrary
probability distribution p, i.e., Dχ2(P)，m Pm=I (Pi - m1 )2. Levy et al. (2020) studied this
problem only for the case of convex objectives. In this paper, we allow objective functions fi for
1 ≤ i ≤ m to be both non-convex or strongly-convex. The following claim derives the equivalent
finite-sum composite optimization.
3
Published as a conference paper at ICLR 2022
Reference	DRO type	# Constraint/Sample Checks to achieve error	objective
Midtouch Cotter et al. (2016)	Wasserstein	O (ln m + m1∙3 * 5(ln m)1.5 + m(ln m)2/3 + m2 log m) ，,4	≡ 3	Strongly convex
SEVR Yu et al. (2021)	General Linearized Wasserstein	O (m ln (ɪ) + DL + ('+κ+l"u )	Strongly convex-concave min-max
Theorem 4.1	Wasserstein	O ((m + κ√m) ln 1)	Optimally strongly convex
Theorem 4.3	χ2 or KL	O (min{√m, j⅛})	Non-convex
Theorem D.1	χ2 or KL	O ((m + κ√m) ln 1)	Optimally strongly convex
Table 1: Comparison of our results with prior approached. All three approaches are using variance
reduction techniques. Du and DL respectively denotes the upper bound on the distance of initial
model from optimal model and initial optimality gap. Please see Yu et al. (2021) for more details.
Finally, we note that while Midtouch approach in Cotter et al. (2016) requires additional storage of
probability distribution of dimension m, our approach does not.
Claim 2.1. The optimization problem (4) is equivalent to the following composite problem:
1 m	1	1m
min	ψ(X), 1 - 2rm X [fi(χ)] + 2- m Xfi(x)	⑸
γ i=1	γ i=1
We note that the optimization problem (5) fits into the formulation of finite-sum composite optimiza-
tion Q) by choosing r(X) = 1, h(X) = m1 Pm=I- (fi(γ)) and Ag(X)) = 2γ [ m1 Pm=I fi(X)]2
With hi(X) = - f2χ, gm(X) = fi(X) and f(x) = 2γ.
DRO with KL divergence. Finally, for DRO With KL-divergence, usually considered in online
settings (Qi et al., 2020a), We consider solving the folloWing optimization problem:
min	max
x	0≤pi≤1, Pim=1 pi=1
m
pifi(Xi) +γH(p1, . . . ,pm ) ,
i=1
(6)
Where H(p1, . . . ,pm ) = - Pim=1 pi log pi is the entropy function. To solve problem (6), it is
straightforWard to convert it to the folloWing equivalent stochastic composite optimization problem:
min
x
Ψ(X) , ln
m
—X eχp
m
i=1
(7)
As it can be seen, the optimization problem (7) fits into the composite optimization (2) by choosing
r(X) = h(X) = 0 and f (g(X)) = ln (m1 Pm=I eχp (fχ)).
3 Our Proposed Algorithm
Having reduced the different notions of DRO to an instance of the composite optimization, in this
section We describe our scalable approach for minimizing the objective Ψ(X) = r(X) + Φ(X) Where
Φ(x) = A Pm=I hm(X) + f (m Pm=I gm(X)). We note that the compositional structure in Φ(∙) leads
to more challenges in optimization compared With the non-compositional finite-sum problem, since
the stochastic gradient of the loss function is not an unbiased estimation of the full gradient. To
overcome this issue, and by building on incremental variance reduction (Zhang & Xiao, 2019b), We
propose a more general algorithm With a neW ingredient in Which We also employ variance reduction
on the extra term h(X) = (1/m) Pm=I hm(X). To handle r(∙), similar to Zhang & Xiao (2019b), we
assume r is convex and a relatively simple function. We folloW the proximal gradient iterates (Beck,
2017; Nesterov, 2013) as follows:
X(t+1) = Πη (X⑴ - ηVΦ(X㈤))	(8)
where we apply the proximal operator of r(X) with the learning rate η as Πrη(X) ,
argminy [r(y) + 品 ∣∣y 一 x∣∣2 ]. By defining the proximal gradient mapping of Ψ as Gn(x)，
4
Published as a conference paper at ICLR 2022
Algorithm 1: Generalized Composite Incremental Variance Reduction (GCIVR (x(0)))
Inputs: Number of iterations t = 1, . . . , T, learning rate η, initial global model x(0) , the size of
epoch length τt , and mini-batch sizes of Bt and St at time t.
for t = 1, . . . , T do
Sample a mini-batch B(t) with size Bt uniformly over [m] and compute
y(t)= B Xξ∈B(t) g(XTt)； ξ), z(t)= B Xξ∈B(t) Vg(XTt)； ξ), WOt) = B Xξ∈B(t) Vh(XTt)； ξ)
Compute Vφ(x0t)) = (z(t))	(f0(y(t))) + WOt)
Update the model as follows: XIt) = ∏η (XOt) — VΦ(x0t))))
for j = 1, . . . , τt - 1 do in parallel
Sample a mini-batch Sj(t) with size St uniformly over [m], and form the estimates
yjt) = yj-ι + ： Xξ∈s(t) [g(Xjt)；ξ)- g(Xj-i； ξ)i	⑼
Zjt) = zj-ι+ s1 Xξ∈s(t) hVg(Xjt)；ξ)- Vg(Xj-1； ξ)i	(IO)
Wjt) = Wj-I+ s1 Xξ∈s(t) hVh(Xjt)；8-Vh(Xj-1；ξ)i	(II)
Compute VΦ(Xjt)) = (Zjt))	(f(yj")) + Wjt)
Update the model as follows: Xj+)ι = ∏η (Xjt) — VΦ(Xjt))))
end
end
Output: Return a randomly selected solution from {Xtj }tj==1O,,.,,TT * X
1 [x 一 ∏η (x 一 ηVΦ(X))], the updating rule in Eq. (8) can be equivalently written as X(t+1)=
X(t) -ηGη(X(t)). Given any y as an output of randomized algorithm, we say y achieves the stationary
point of problem in Eq. (2) in expectation if E kGη(y)k2 ≤ holds. Our goal is to achieve an
stationary point with the least number of calls to a (mini-batch) stochastic oracle.
Focusing on Φ(∙),as detailed in Algorithm 1, we apply three time-scale variance-reduced estimators
forgi(X) and its gradient Vgi(X), as well as hi(X). For DRO with Wasserstein divergence metric with
optimally strongly objective, at the beginning of each epoch t we compute a full-batch gradient over
the entire data samples Bt = m, i.e., y(t) = m Pm=I gi(xTt)), z(t) = A Pm=I Vgi(XTt)), WOt)=
ml Pm=I Vhi(XTt)). In contrast, for the case of DRO with χ2 or KL divergence metrics and non-
convex objectives we incrementally increase the size of the mini-batch Bt ≤ m at the beginning of
each epoch until it reaches the point where we need to compute the full-batch of samples. We should
also highlight that a mini-batch in case of Wasserstein DRO indicates a batch of constraints and in
DRO with χ2 or KL divergence metrics represents the number of data sample accessed. We denote the
length of each epoch and the mini-batch size within each epoch with τt and St, respectively. In each
epoch t and iteration j, we estimate mini-batch gradients from Eqs. (9), (10), and (11) in Algorithm 1,
with some corrections term applied from the previous iteration. We note that the variance-reduced
term corresponding to the correction is inspired by SARAH (Nguyen et al., 2017) and SPIDER (Fang
et al., 2018). Finally, the algorithm returns a randomly selected solution from the iterates.
Distributed variant of Algorithm 1. As mentioned before, efficient training of the stochastic
composite optimization problem has attracted increasing attention in recent years. Despite much
progress, all of existing methods including Algorithm 1 only focus on the single-machine setting. To
employ Algorithm 1 in a distributed setting with p machines, in Appendix E we propose a distributed
variant of proposed algorithm and establish its convergence rate for convex and non-convex objectives
which enjoys a speedup in terms of number of machines.
5
Published as a conference paper at ICLR 2022
4	Convergence Analysis
In this section we establish the convergence of proposed algorithm for different DRO notions
discussed in Section 2. We start by stating the general assumptions and then discuss the obtained
rates. Due to lack of space we only include the rates on the convergence of DRO with Wasserstein
metric for strongly convex, and χ2 and KL divergence metrics for non-convex objectives. We defer
the analysis of χ2 and KL divergence metrics with strongly objectives to Appendix D. To establish
the convergence rates, we first introduce some standard assumptions.
Assumption 1. We make the following assumptions on the components of objective Ψ(x) = r(x) +
m1 Pm=ι hi(X) + f( m1 Pm=ι gi(X)):
1)	∣∣Vh(xι)	—	Vh(x2)k	≤	Lh	∣∣xι 一 X2k where	xι, X2	∈	Rd.	We also assume that
kh(X1) - h(X2)k ≤ `h kX1- X2k.
2)	∣f0(X1) 一 f0(X2)∣	≤ Lf ∣X1 一 X2 ∣ where X1, X2 ∈ R.	We also assume that
∣f(X1) 一 f(X2 )∣ ≤ `f ∣X1 一 X2 ∣.
3)	∣∣Vg(x1; ξ) — Vg(x2;ξ)∣ ≤ Lg ∣∣xι — x2k and Ilg(X1；ξ) — g(x2; ξ)k ≤ 'g ∣∣xι — X2k
for ∀X1, X2 ∈ Rd.
4)	We suppose that Ψ(x) in Eq. (2) is boundedfrom below that Ψ* = infX Ψ(x) > —g.
5)	We assume r(X) ∈ R ∪ {∞} is a convex and lower-semicontinues function.
An immediate implication of above assumption is that f (g(x))，f (ml Pm=I gi(x)) is smooth with
module ('2Lf + 'f Lg), hence Ψ(x) is smooth with module Lφ，[('gLf + 'f Lg) + Lh (Wang
et al., 2016; Zhang & Xiao, 2019b).
Assumption 2 (Unbiased estimation). We assume that the mini-batch of samples ξ over functions gi
for i = 1, 2, . . . , m is unbiased, that is E [g(X; ξ)] = g(X) andE [Vg(X; ξ)] = Vg(X).
4.1	Optimally S trongly Convex Objectives
We here establish the convergence for optimally strongly convex objectives under Wasserstein metric.
Definition 1 (Optimally strongly convex). We say that the objective function Ψ(X) is optimally
StrOngly convex objective with module μ if Ψ(x) — Ψ(Pχ* (x)) ≥ μ ∣∣x — Pχ* (x)∣2.
According to Definition 4 of Necoara et al. (2019) optimally strong objectives or quadratic functional
growth property is the generalization of strongly convex condition. According to Karimi et al. (2016)
below, the PL condition implies an optimally strongly convex condition but not vice versa. Therefore,
optimally strongly convex generalizes PL condition as well.
Following Cotter et al. (2016) and Mahdavi et al. (2012), we make the following assumption.
Assumption 3. There exists a constant P such that if we define g(x) = maxι≤i≤m gi(x) we have
min ∣Vg(X)∣2 ≥ ρ.
g(x)=0
Assumption 4. Function r(X) is smooth with module G.
Theorem 4.1. Assume Ψ is μ-optimally strongly convex and set Tt = St = √Bt = √m, T = √m5μη,
n < L-+√L2+36Gcι where Go , 3('4 Lf + '2 Lg + 'h), α > G, and Y = eχp(-K)). Let us denote
X(k+1) = GCIVR X(k) for k = 0, . . . , K — 1 (using Algorithm 1). Under Assumptions 1-4 and
by letting K = ln (1/), the solution of DRO with Wasserstein divergence is obtained by projecting
X(K) onto the constraint set K = {x∣gi(x) ≤ 0, i = 1, 2,..., m}, i.e., X(K) = ∏κ(χ(K)). In order
to achieve r(X(K)) — r(X(*)) ≤ e, we require an O ((m + κ√m) ln ɪ) calls to the stochastic oracle.
Comparison with previous results. Compared to the MidTouch approach by Cotter et al. (2016),
our obtained rate improves both on the dependency on the number of constraints m, as well as
convergence error e. To better understand the intuition behind achieving such a double folded
improvement in terms ofm and e, we note that unlike Cotter et al. (2016) which utilizes a primal-dual
approach in order to obtain an approximate to the optimal distribution over the constraints, we directly
find the optimal distribution exactly. Second, while Cotter et al. (2016) does not apply any variance
reduction technique to primal variable X, our algorithm benefits from variance reduction over X too.
6
Published as a conference paper at ICLR 2022
Furthermore, Theorem 4.1 entails tighter rate in terms of final accuracy compared to Yu et al. (2021).
These results are summarized in Table. 1.
In Algorithm 1 for optimally strongly convex objectives, we need to do a single projection at the end.
The following corollary bounds the error between the solution before and after the projection.
Corollary 4.2. Under the assumptions made in Theorem 4.1, the error between the solution with
and without projection is bounded by
r(x(K)) - r(x(K)) ≤ G Y ln(m +° +--------^―—O (exp(-K)) + Y ln(m + 1).	(12)
αρ - G αρ - G
Therefore, with a proper choice of Y we can establish convergence rate obtained in Theorem 4.1.
Remark 1. It is worthy to highlight that through reduction of heavily-constrained optimization to
composite optimization, GCIVR algorithm can be considered as an alternative method to solving
constrained optimization problems via mini-batch sampling of constraints. In particular, Theorem 4.1
shows that under certain conditions, we can solve any heavily-constrained optimization problem with
sampling a mini-batch of constraints and achieve similar guarantees compared to projection-based
counterparts while avoiding the heavy dependency on the number of constraints. Another implication
of Theorem 4.1 is that we can solve heavily constrained optimization with the sample complexity
similar to that of an unconstrained optimization problem.
minχ [ψ(X) , r(X) + m1 Pm=I gi(x)]
Remark 2. To understand the tightness of our obtained rate, consider
which is an instance of our general optimization
problem (2). Clearly, any lower bound to solve this instance also holds for the original optimization
problem (2). Xie et al. (2019) provides a lower-bound of O ((m + √κm)ln (1/))) for above
instance, matching our upper bound in terms of the dependency on the number of data samples, while
the dependency on κ can still be improved. Furthermore, for the DRO problem with Wasserstein
divergence metric, rather than solving constrained optimization problem we solve unconstrained
compositional optimization problem. Thus, since solving constrained optimization is more complex
than unconstrained optimization problem, we do not expect to obtain any better bound regarding m
even for DRO with Wasserstein metric.
Remark 3. In a distributed setting, we are able to improve the sample complexity to
O ((m + m + κ√m] ln ∙∣) with P devices. The proof can befound in Appendix F3.
4.2 Non-Convex Objectives
We now turn to establishing the convergence of DRO problems with χ2 and KL divergence metrics for
non-convex objectives by making an additional assumption on the variance of stochastic mini-batches.
Assumption 5 (Bounded variance). The mini-batch sampling has bounded variance that is
E[ kVg(x; ξ) 一 Vg(x)k2 ] ≤ *, where B indicates the batch size.
Theorem 4.3. Under Assumptions 1,2, and 5 using Algorithm 1 for some positive constants β and
0 ≤ Z < √m, denote To，v^∣~ζ = O (√m). For t ≤ To, let us set Tt = St = √Bt = βt + Z and
for t > TO Set Tt = St = √m. Then, if η <
after T = (O (min(1∕√e, 1∕√me))
______ 4
LΦ + √ LΦ + 12G0
iterations, with O (min{√m∕e, 1∕e1∙5}) number of calls to the stochastic oracle it holds that
E [ 11 Gη (X(T)) ∣∣2 ] ≤ e, where we used O(.) notation to hide terms with logarithmic dependency.
We also remark that Assumption 5 is only needed for convergence until TO = We-Z and after t>To
we will not need this assumption. We also note that this assumption is not required in optimally
strongly convex setting as we utilize a full batch at the beginning of each epoch.
[ψ(X) , r(X) + m1 Pm=I gi(X)]
Discussion on lower bound. As we discussed in Remark 2, any impossibility result for optimizing
minx
also holds for general compositional optimization problem
in Eq. (2). For former problem, Fang et al. (2018) shows that to achieve the stationary point with
error e we require at least O (min{ √m,圭}) (almost optimal) gradient oracle calls for strongly
optimal objectives. As a consequence, for the DRO with χ2 or KL divergence metrics we do not
expect less number of constraint checks or sample complexity, respectively.
7
Published as a conference paper at ICLR 2022
Remark 4. In distributed setting with p devices, we can improve the convergence rate of DRO with
χ2 and KL metrics to O (min{ √m + √√m, ^lɪ + &}). For details, please refer to Appendix F.3.
5	Experiments
In this section, we empirically examine the efficacy of the proposed algorithm in different use cases.
The main algorithm that we compare against is Heavily-Constrained algorithm that uses a parametric
multiplier model proposed in Narasimhan et al. (2020b). We call this algorithm Heavily-constrained,
where they learn the Lagrange multiplier values using a parametric model such as a neural network.
The tasks we apply the algorithms are mainly focused on fairness constraints since they fit greatly
to the problem definition in this paper due to the large number of constraints they add to the main
learning problem. In addition, similar to distribution robust optimization approaches, the main goal
of fairness constraint is to find a solution that is agnostic to the distribution of protected groups. The
code for the experiments is available at this repository.
Distributionally robust optimization for fairness constraints. The first experiment is based
on Narasimhan et al. (2020b); Wang et al. (2020), where we want to enforce equality of opportu-
nity (Hardt et al., 2016) constraints for different groups while the group membership is noisy and
changing during the training. Hence, the problem is to make the solution distributionally robust
among different protected groups in the problem. Based on the setup in Narasimhan et al. (2020b),
We assume We have access to the marginal probability of the true groups (P (gi = j |gi = k), where
gi is the true group membership and gi is the noisy group membership). Hence, to enforce fairness
constraints, We consider all possible proxy groups using this marginal probability, Which can increase
the number of constraints greatly. The goal in this case for equal opportunity is to have the true
positive rate of each group in the vicinity of the true positive rate (tpr) of the overall data, that is:
tpr(g = j) ≥ tpr(ALL) - for every proxy group We define.
In this experiment, We use the Adult dataset (Dua & Graff, 2017), and consider race groups of
“White”, “black”, and “other” as protected groups. We train a linear classifier With logistic regression,
and report the overall error rate of the classifier, as Well as the maximum violation of the fairness
constraints (equal opportunity) over true group memberships. We set = 0.05 and the noise level to
0.3. Again, We compare With unconstrained optimization and heavily-constrained algorithm With
a linear model as its multiplier model. First roW of Figure 1 shoWs the results for this experiment,
Where the proposed GCIVR algorithm can achieve the same level of constraint violation on true group
memberships as the heavily constraint While outperforms it in terms of overall error rate. Hence, the
solution found by the GCIVR dominates heavily-constrained solution. In terms of runtime speed, the
first roW of Figure 1(c) clearly shoWs the advantage of GCIVR over heavily-constrained With much
loWer overhead to the unconstrained optimization.
Fairness Constraints on intersectional groups. In this task We ought to learn a linear classifier
With logistic regression to predict the crime rate for a community on Communities and Crime
dataset (Dua & Graff, 2017). This dataset contains 1994 instances of communities each With 128
features to predict the per capita crime rate for each community in the US. The labels are high
and loW crime rates to represent if a community is above the 70th percentile of the data or not.
To add fairness constraints, We first determine different communities based on the percentages of
the Black, Hispanic and Asian population in each community, as discussed in Cotter et al. (2019);
Narasimhan et al. (2020b). Similar to Narasimhan et al. (2020b), We generate 1000 thresholds of the
form (τ1, τ2, τ3) ∈ [0, 1]3 to define each group. Consider the population of each race as (p1, p2, p3),
then a community belongs to group gτ1,τ2,τ3 ifpi ≥ τi ∀i ∈ [3]. Then the fairness constraints enforce
that the error rate of each group should be in the neighborhood of the overall error by margin of .
That is err (gτ1,τ2,τ3) ≤ err (ALL) + . Similar to Narasimhan et al. (2020b), We set = 0.01 and
not consider groups With less than 1% of data.
We compare With the unconstrained optimization and Heavily-constrained algorithm With a 2-layer
neural netWork, each With 100 nodes as their Lagrange model, as described in their paper. We
compare the trade-off betWeen the error rate and maximum violation of fairness constraints among
groups. The second roW of Figure 1 portrays this comparison for the test dataset. As it can be
inferred, the proposed GCIVR algorithm achieves the same error rate as Heavily-constrained, but
both higher than the unconstrained optimization. HoWever, comparing the maximum violation of
constraints, it is clear that GCIVR outperforms both optimization methods. Also, the third figure
shoWs the runtime of different algorithms. It is clear that GCIVR adding a minimal overhead to the
unconstrained optimization, While heavily-constrained increases the runtime more than 100 times.
8
Published as a conference paper at ICLR 2022
ss°UllE 工 OXel
. . . . . .
OooooO
UoO-> dno-9 xe
-:-Unconstrained
-=-Heavily Constrained
—≡- GCIVR
0	10 00	20000	30000	40000	50000
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
Wall-clock Time (min)
. . . . . . .
Ooooooo
e-0」由
ssenriaF lanoitcesretn
Unconstrained
Heavlly Constrained
GCIVR
Iterations
-∙- Unconstrained
—>— Heavily Constrained
—∙— GCIVR
Iterations
Unconstrained
Heavily Constrained
GCIVR
Iterations
,46
,44
,42
,40
,38
-S- Unconstrained
- HeaviIy Constrained
—=—GCIVR
ssenriaF gnikna
0.46
B 0.44
2 0.42
ɪ
0.40
0.38
20000	40000	60000	80000 IOOOOO
Iterations
(a) Error Rate
0	20 00	40 00	60000	80000	100000
Uo-4--0-> dn05xew
Iterations
(b) Max Constraint Violation
0.
0.
0.
0.
0.
—u- Unconstrained
—≈∙- Heavily Constrained
—a- GCIVR
%....
ιrg¾≡∣Pi
0	50	100	150	200	250	3 0
Wall-clock Time (min)
(c) Error Rate (Runtime)
Figure 1: Comparison of the proposed GCIVR algorithm with unconstrained optimization and heavily-
constrained algorithm Narasimhan et al. (2020b) in three different tasks of fairness and DRO. Each
row shows the result for one task based on the error rate, max constraint violations and the runtime of
the codes. In all the cases solutions learned by GCIVR dominates the heavily-constrained solution
and it converges way faster.
Per-query fairness constraints in ranking. In the third problem we evaluate our algorithm on the
fairness in the ranking problem, where we intend to impose per-query constraints on the learning to
rank problem as defined in Narasimhan et al. (2020a;b). We divide document-query pairs into two
groups based on the 40th percentile of the their QualityScore features. Hence, in this problem, we
want to learn a ranking function f : D × Q → R, which maps a pair of document-query features to a
real number as the score. Consider the groups of g0 and g1 as mentioned before, we want the error
rate for these groups to be close to each other. In other terms, if the pairwise error rate is defined as:
erri,j (q) = E [1 {f (d, q) < f (d0, q)} | y > y0, d ∈ gi, d0 ∈ gj], where y and y0 are the respective
binary labels. Then, the fairness constraints can be satisfied as |err0,1 (q) - err1,0(q)| ≤ ∀q ∈ Q.
For this experiment, we use Microsoft Learning to Rank Dataset (MSLR-WEB10K) (Qin & Liu,
2013), which contains 10K queries and 136 features. For this experiment we use a non-convex
objective, where the model is a two-layer neural network each with 128 nodes and cross-entropy as
the loss function. We compare against the unconstrained optimization and the heavily-constrained
algorithm with an one-layer neural network with 64 nodes as its multiplier model. We use 1000 queries
in the training and 100 queries in the test datasets. The third row in Figure 1 shows the error rate and
maximum violation of groups constraints. As it is clear GCIVR outperforms heavily-constrained in
both error rate and maximum violations of groups by a large margin. In terms of runtime, GCIVR is
very close to the unconstrained optimization, and about 2× faster than heavily-constrained.
6	Conclusion
In this paper, we showed that many DRO problems or heavily constrained optimization problems can
be cast into a general framework based on finite-sum composite optimization. To solve this composite
finite-sum optimization, we introduced centralized and distributed algorithms. We theoretically
illustrated that our algorithm converges with an almost optimal number of constraint checks (for
Wasserstein distance) or gradient calls (for χ2 or KL divergence metrics). Finally, we validated our
theory with a number of experiments.
9
Published as a conference paper at ICLR 2022
Acknowledgements
Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032) and ONR
(N00014-19-1-2406). The work of Mehrdad Mahdavi was supported in part by NSF (CNS-1956276).
References
Amir Beck. First-order methods in optimization. SIAM, 2017.
Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
Science, 59(2):341-357, 2013.
Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical
Programming, 167(2):235-292, 2018.
Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applica-
tions to machine learning. Journal of Applied Probability, 56(3):830-857, 2019.
Robert Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-
convex objectives. arXiv preprint arXiv:1707.01047, 2017.
Andrew Cotter, Maya Gupta, and Jan Pfeifer. A light touch for heavily constrained sgd. In Conference
on Learning Theory, pp. 729-771. PMLR, 2016.
Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for efficient non-convex
constrained optimization. In Algorithmic Learning Theory, pp. 300-332. PMLR, 2019.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning. arXiv preprint arXiv:2003.13461, 2020.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated
averaging. arXiv preprint arXiv:2102.12660, 2021.
Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Empiri-
cal risk minimization under fairness constraints. arXiv preprint arXiv:1802.08626, 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. arXiv
preprint arXiv:1610.02581, 2016.
John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-
tionally robust optimization. The Annals of Statistics, 49(3):1378-1406, 2021.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1):115-166, 2018.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex
optimization via stochastic path integrated differential estimator. arXiv preprint arXiv:1807.01695,
2018.
Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated
learning. arXiv preprint arXiv:1910.14425, 2019.
10
Published as a conference paper at ICLR 2022
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local
sgd with periodic averaging: Tighter analysis and adaptive synchronization. Advances in Neural
Information Processing Systems, 2019a.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Trading
redundancy for communication: Speeding up distributed sgd for non-convex optimization. In
ICML,pp. 2545-2554, 2019b.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Fed-
erated learning with compression: Unified analysis and sharp guarantees. arXiv preprint
arXiv:2007.01154, 2020.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing systems, 29:3315-3323, 2016.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without
demographics in repeated loss minimization. In International Conference on Machine Learning,
pp. 1929-1938. PMLR, 2018.
Zhouyuan Huo, Bin Gu, Ji Liu, and Heng Huang. Accelerated method for stochastic composition
optimization with nonsmooth regularization. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
Conference on Machine Learning, pp. 427-435. PMLR, 2013.
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with
stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
Dionysios S Kalogerias and Warren B Powell. Zeroth-order stochastic compositional algorithms for
risk-aware learning. arXiv preprint arXiv:1912.09484, 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pp. 130-166. INFORMS,
2019.
Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally
robust optimization. arXiv preprint arXiv:2010.05893, 2020.
Xiangru Lian, Mengdi Wang, and Ji Liu. Finite-sum composition optimization via variance reduced
gradient descent. In Artificial Intelligence and Statistics, pp. 1159-1167. PMLR, 2017.
Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, and Jinfeng Yi. Stochastic gradient
descent With only one projection. In Advances in Neural Information Processing Systems 25
(NIPS), 2012.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep netWorks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. arXiv
preprint arXiv:1902.00146, 2019.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust
optimization With f-divergences. In NIPS, volume 29, pp. 2208-2216, 2016.
Harikrishna Narasimhan, AndreW Cotter, Maya Gupta, and Serena Wang. PairWise fairness for ranking
and regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
5248-5255, 2020a.
11
Published as a conference paper at ICLR 2022
Harikrishna Narasimhan, Andrew Cotter, Yichen Zhou, Serena Wang, and Wenshuo Guo. Ap-
proximate heavily-constrained learning with lagrange multiplier models. Advances in Neural
Information Processing Systems, 33, 2020b.
Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, 175(1):69-107, 2019.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-
1609, 2009.
Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming,
140(1):125-161, 2013.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takdc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, pp. 2613-2621. PMLR, 2017.
Farzad Pourbabaee. Robust experimentation in the continuous time bandit problem. Economic
Theory, pp. 1-31, 2020.
Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. A practical online method for distribution-
ally deep robust optimization. arXiv preprint arXiv:2006.10138, 2020a.
Qi Qi, Yan Yan, Zixuan Wu, Xiaoyu Wang, and Tianbao Yang. A simple and effective framework
for pairwise deep metric learning. In Computer Vision-ECCV 2020: 16th European Conference,
Glasgow, UK, August 23-28, 2020, Proceedings, PartXXVII16,pp. 375-391. Springer, 2020b.
Tao Qin and Tie-Yan Liu. Introducing letor 4.0 datasets. arXiv preprint arXiv:1306.2597, 2013.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659, 2019.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
arXiv preprint arXiv:1911.08731, 2019.
Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally
robust logistic regression. arXiv preprint arXiv:1509.09259, 2015.
Alexander Shapiro. Distributionally robust stochastic programming. SIAM Journal on Optimization,
27(4):2258-2275, 2017.
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying some distributional
robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Mengdi Wang and Dimitri P Bertsekas. Incremental constraint projection methods for variational
inequalities. Mathematical Programming, 150(2):321-363, 2015.
Mengdi Wang and Dimitri P Bertsekas. Stochastic first-order methods with random constraint
projection. SIAM Journal on Optimization, 26(1):681-717, 2016.
Mengdi Wang, Ji Liu, and Ethan X Fang. Accelerating stochastic composition optimization. arXiv
preprint arXiv:1607.07329, 2016.
Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):
419-449, 2017.
Serena Wang, Wenshuo Guo, Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Michael I
Jordan. Robust optimization for fairness with noisy protected groups. arXiv preprint
arXiv:2002.09343, 2020.
Guangzeng Xie, Luo Luo, and Zhihua Zhang. A general analysis framework of lower complexity
bounds for finite-sum optimization. arXiv preprint arXiv:1908.08394, 2019.
12
Published as a conference paper at ICLR 2022
Yan Yan, Yi Xu, Qihang Lin, LijUn Zhang, and Tianbao Yang. Stochastic primal-dual algorithms
with faster convergence than o(1∕√T) for problems without bilinear structure. arXiv preprint
arXiv:1904.10112, 2019.
Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Sharp analysis of epoch stochastic gradient
descent ascent methods for min-max optimization. 2020.
Yaodong Yu, Tianyi Lin, Eric Mazumdar, and Michael I Jordan. Fast distributionally robust learning
with variance reduced min-max optimization. arXiv preprint arXiv:2104.13326, 2021.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P Gummadi. Fairness
constraints: A flexible approach for fair classification. The Journal of Machine Learning Research,
20(1):2737-2778, 2019.
Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra.
Coping with label shift via distributionally robust optimisation. arXiv preprint arXiv:2010.12230,
2020a.
Junyu Zhang and Lin Xiao. A composite randomized incremental gradient method. In International
Conference on Machine Learning, pp. 7454-7462. PMLR, 2019a.
Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance
reduction. arXiv preprint arXiv:1906.10186, 2019b.
Mingrui Zhang, Zebang Shen, Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. One sample
stochastic frank-wolfe. In International Conference on Artificial Intelligence and Statistics, pp.
4012-4023. PMLR, 2020b.
13
Published as a conference paper at ICLR 2022
Appendix
In the appendix, we provide the missing proofs and derivations from the main manuscript, as well as
proposing the distributed version of Algorithm 1 to further improve the convergence speed.
Table of Contents
A Example of Equivalent Wessterstein Reformulation	15
B Obtaining Augmented Optimization Problem	15
C Proof of Claim 2.1	15
D Strongly-convex convergence for DRO with KL and χ2 metrics	16
E Distributed Stochastic Composite Algorithm	16
F Proof of Theorems 4.1 and F.2	17
F.1 Computational complexity of Theorem 4.1 and Theorem D.1 ................. 19
F.2 Computational Complexity of strongly-convex objectives Corresponding to Theo-
rems 4.3 ............................................................... 20
F.3	Convergence analysis .................................................. 20
G Proof of the Distributed Algorithm	22
G.1	General Lemmas ........................................................ 22
H Approximate Approach For DRO with Wesserstein Metric with Non-convex Objectives 25
14
Published as a conference paper at ICLR 2022
A Example of Equivalent Wessterstein Reformulation
A tractable reformulation of distributionally robust logistic regression problem by reduction to a
constrained optimization problem is provided in Shafieezadeh-Abadeh et al. (2015) as follows:
1m
mm	r(x)，λe +---- Σ si
x	m i=1
subject to gi(x)	=	('β(z, y) - Si) ≤ 0, ∀i	∈	[1： m]
gj (x)	=	('β(z, -y) - λκ - Sj)	≤	0, ∀j ∈	[1 : m]
g'(x) = (k∖β)∖∖l - λ2) ≤ 0
(13)
where 'β(z, y) = log (1 + exp (-y hβ, Zi)) is the associate loss with parameter β and the data
sample (z, y) and we define x = (z, S1, . . . , Sm, λ, β).
B Obtaining Augmented Optimization Problem
To derive the smoothed constrained variant of original optimization problem we follow the reduction
method originally introduced in Mahdavi et al. (2012) to get:
min Ψ(x) = min
r(x) +	max
0≤pi ≤α, Pim=+11
m
pi =α
m
X .(x) + YH (",丝,…,pm ,pm+1, ɪ)
α α α α m+1
min
x
min
x
min
x
i=1
r(x) + Y ln (m⅛I
Ψ(x) + γln (m + 1)
'--------------V------}
Constant
r(x) + γ ln 1 + exp
+ γ ln (m + 1)
1 + exp
i=1
(14)
where in Eq. (14) We used the definitions Ψ(x) = r(x) + Y ln (g(x)), gi(x)，exp (αgγ(x)) and
g(x) = m+ι [1 + Pm=I gi (x)]. Since Y ln (m + 1) is a constant, the minimizer of both Ψ(x) and
Ψ(x) is the same, and consequently we can solve the optimization problem for Ψ via finite-sum
composite optimization.
C Proof of Claim 2.1
We use Lagrangian multiplier for the purpose of the proof. The Lagrangian of the optimization
problem in Eq. (4) is
L(x, Y; λ)
max
0≤pi ≤1, Pim=1 pi =1
By setting Npi L = 0 we obtain
m	1m
fpifi(Xi) - Y 丁ImPi -1)2
2m
i=1	i=1
(15)
Pi」(巫3 + 1).
mY
(16)
Then from the condition i Pi = 1, it follows that:
m
λ=m X fi(X)
i=1
(17)
15
Published as a conference paper at ICLR 2022
Plugging the obtained values for λ and pi into Eq. (4) yields
m1
ψ(x; Y) = ι + E一
i=1 γm
1m
=1- E X U(X)-
1m	1
=1- 2γm X[fi(x)] +2γ
m
fi(x)- m X fi(x)
i=1
m
—X fi
m
i=1
2
m2
Xfi(x)
i=1
1m
X fi(x)
m
i=1
(18)
which completes the proof.
D S TRONGLY- C ONVEX CONVERGENCE FOR DRO WITH KL AND χ2 METRICS
Theorem D.1. Suppose Assumptions 1 and 2 hold, and also assume Ψ is μ-optimally Strongly
convex. If we set Tt = St = Bt = √m and T
√m5μη, Ietting η <
LΦ+√ lΦ+36go
, by letring
x(k+1) = GCIVR (x(k)) for k = 0,..., K — 1 using Algorithm 1 after K = ln(1∕e) repetition
solves the optimization problem in Eq. (2) with convergence error Ψ(X(T)) — Ψ(x(*)) ≤ e with
O ((m + κ√m) ln ɪ) number ofgradient calls. Therefore, depending on the objective function Ψ,
this sample complexity corresponds to both DRO problem with χ2 and regularized KL metrics.
2
Based on TheoremD.1, for DRO problem with χ2 divergence metric, We can achieve Ψ(x(K)) ∣γ=ι —
Ψ(x3)lγ=ι ≤ γe and Ψ(x(κ))∣γ>ι — Ψ(x(*))∣γ>ι ≤ e with O ((m + κ√m) ln ɪ) number of
gradient oracle calls.
E Distributed Stochastic Composite Algorithm
In the main body, we mainly focused on studying the sample complexity of solving DRO problems
in a centralized setting. The question we are interested is that “Can we further improve the sample
complexity?" In this section, we give an affirmative answer to this question via studying distributed
version of DRO problem. We suppose data samples are distributed among p clients 1. For distribution-
ally robust optimization problem via Wasserstein , χ2 and KL divergence metrics in the distributed
fashion like federated learning setups McMahan et al. (2016); Mohri et al. (2019); Haddadpour et al.
(2019b); Haddadpour & Mahdavi (2019); Haddadpour et al. (2019a); Deng et al. (2020); Haddadpour
et al. (2020).
1We suppose that each device has access to at least m/p number of data points.
16
Published as a conference paper at ICLR 2022
We first introduce our distributed finite-sum compositional algorithm as detailed in Algorithm 2.
Algorithm 2: Distributed Generalized Composite Incremental Variance Reduction
(DistGCIVR(x(0)))
Inputs: Number of iterations t = 1, . . . , T, learning rate η, initial global model x(0), and a set of
triples {τt, Bt, St} where τt indicating the size of epoch length and mini-batch sizes of Bt and
St at time t.
for t = 1, . . . , T do
for i = 1, . . . , p do in parallel
Sample minibatches B(t,i) with size Bt uniformly over [m] and compute
y(t,i) = B X gi(XTt);ξ),	z(t,i) = B X Vgi(XTt);ξ),	w0t,i) = B X Vhi(XT);ξ)
t ξ∈B(t,i)	t ξ∈B(t,i)	t ξ∈B(t,i)
Server computes y0t) = P Pp=I y(t,i), z(t) = P Pp=ι z(t,i) and w0t) = P Pp=I w0t,i)
Server computes Vφ(x0t)) = (z(t)) (f0(y(t))) + w0t)
Update the model as follows and broadcasts it to devices
x1t)=∏η (x0t) -VΦ(x0t))))
for j = 1, . . . , τt - 1 do in parallel
Sample a set Sj(t,i) with size St over [m], and form the estimates
yjt,i) = yjt-1 + s1 X Igi(Xjt);ξ)-gi(Xjt-1；ξ)i
t ξ∈Sj(t,i)
Zifi= zj-iι) + SI X [Vgi(Xjt); ξ)-Vgi(Xj%; ξ)i	(19)
t ξ∈Sj(t,i)
a,,(t,i) — a,,(t,i)	_1_ 1	∖	Γ∖-7L，一(t)∙ C ∖7k, ∕‰(t) ∙	C∖∖	/ɔnʌ
Wj = Wj-I	+ S	匚	[Vhi(Xj , ξ) 一 Vhi(Xj-1;ξ)]	(20)
t ξ∈Sj(t,i)
and send y(t,i) and z(t,i) back to server
Server computes yjt) = ɪ Pp=I yjt,i), Zjt) = P Pp=I zjt,i) and
Wjt) = P Pp=I wjt,i)	T
Compute VΦ(Xjt)) = (Zjt)) ff 0(yjt))) + Wjt)
Update the model as follows: Xjt)ι = ∏η (Xjt) — VΦ(Xjt))))
end
end
end
Output: Return a randomly selected solution from {Xtj }tj==10,,.,,TT
We emphasize that our algorithm is developed based on Algorithm 1, so we only describe the main
differences. In the the distributed setting, in epoch t, i-th device has local version of global model,
i.e., yj(t,i), Zj(t,i), Wj(t,i) and at the beginning and during the epochs all devices send back their local
models back to the server to be averaged and global model X to be updated. Then, server broadcast
global model to the devices.
F	Proof of Theorems 4.1 and F.2
Before proceeding to the proof of this theorem, we need to mention that for Wasserstein DRO problem
the main objective r(X) is an affine function of input data. This property naturally satisfies condition
(5) in Assumption 1 and will be useful in distributed setting. For an illustrative example please see
Appendix A.
17
Published as a conference paper at ICLR 2022
Proof of Theorem 4.1: Suppose that x(*) is the solution for the following optimization problem:
1m
minXmize r(X) , m X fi(xi)	QI)
subject to gi(x) ≤ 0, ∀i ∈ {1,…，m}.
Defining Ψ(x) , r(x) + Yln (m+1 [l + Pm=I exp (αgγ(x))]), wehave
Ψ(x) , Ψ(χ) + γln(m + 1).
Noting that X(K) and X(K) are the output of algorithm with and without projection to the constraints
set
K = {x : gi(x) ≤ 0,∀i ∈ [1 : m]}	(22)
respectively. Our proof is based on following two steps:
(1)	We first show that E [Ψ(x(K))] ≤ Ψ(x(*)) + O (exp(-aK)), which indicates the conver-
gence rate of stochastic compositional optimization problem depends on used algorithm
without any projection.
(2)	Second step involves showing that E Ir(X(K))] ≤ r(x(*))+ O (exp(-aK)); in other words
the final projected solution converges to optimal solution of augmented objective function
and a is some positive constant depends on condition number κ.
We note step (1) follows directly from Theorem 8 in Zhang & Xiao (2019b). In the following, we
prove step (2). First note that as ]i(x*) ≤ 0 for all 1 ≤ i ≤ m, we have:
Ψ(χ3)= r(x(*)) + γln (-ɪ 1 + Xexp (辿笠!
1
m+1
=r(x(*))	(23)
This leads to
Ψ(x(*)) ≤ r(x(*)) + γln(m + 1)	(24)
Therefore, using item (1) we have:
E [Ψ(x(K))] ≤ r(x(*))+ O (exp(-aK))	(25)
On the other hand, due to the definition of Ψ(X) and smoothed max or log-sum property, we have:
Ψ(x(K)) ≥ r(x(K)) + max 仪,αgi(X(K))) .	(26)
For the purpose of lower bounding the second term in Eq. (26) we need the following Lemma:
LemmaE1. If XT = XT, by defining g(X(K)) , max g% (X(K)) for all i ∈ [1 : m] we have:
1≤i≤m
g(X(K)) ≥ P 卜(K)- X(K)H2	(27)
The proof of this lemma can be found in Mahdavi et al. (2012) but for the sake of completeness we
also include the proof.
Proof. As X(K) is the projection of X(K) into K; i.e., X(K) = arg min HX — X(K) ∣∣2, then due to
g(x(K ) )≤0
first order optimality condition, there exists a positive constant k > 0 such that
g(X(K)) = 0 s.t., X(K) — X(K) = kVg(X(K))	(28)
1+Xi=m1(1)
gi(x(*))≤0	，、
≤	r(X(*)) + γ ln
18
Published as a conference paper at ICLR 2022
As a result we have:
g(x(K)) = g(x(K)) — g(x(κ)) ≥ (X(K) — X(K)) Vg(x(K)) = Kx(K) — X(K))I UVg(x(K))∣∣
管 P U(X(K)- X(K))U	(29)
where (0)follows from Assumption 3.	□
Hence, we have:
Ψ(x(K))≥r(x(K)) + αρ ∣∣(x(K)- X(K))U	(30)
Moreover, note that we have:
r(X(K)) ≤ r(X(*)) + Y ln(m +1) + O (exp(-aK)) — max(0, q%(x(K)))
1≤i≤m
≤ r(X(*)) + γ ln(m +1) + O (exp(-aK))
Next, we can write:
r(X(*)) — r(X(K)) = r(X(*)) — r(X(K)) + r(X(K)) — r(X(K))
≤ r(X(K)) — r(X(K))
≤ GU(X(K)- X(K))U	(31)
Eqs. (24), (30) and (31) lead to the bound:
αρ U(x(K) — X(K))U ≤ r(X(*)) — r(X(K)) + Y ln(m + 1) + O (exp(-aK))
≤ G U (x(k) — X(K))U + γln(m + 1) + O (exp(-aK))	(32)
which allows us to the following:
Ux(K) — X(K) u ≤ Ylnm + —1-O (exp(-aK))	(33)
αρ — G αρ — G
Finally, we have:
r(X(K)) = r(X(K)) — r(X(K)) + r(X(K))
≤ G ∣∣X(κ) — X(K) ∣∣ + r(X(K))
≤ G Y ln(m +> +-----------O-O (exp(-aK)) + Y ln(m + 1) +r(X(*)) + O (exp(-aK))
αρ — G αρ — G
、-----------------------------{z----------------------------}
Cost of violating constraints
(34)
Therefore, by setting Y
exp(-aK)
ln(m+1)
we achieve the desired result.
Proof of Corollary 4.2: The proof follows directly from Eq. (34).
Proof of Theorem D.1: The proof follows directly from Theorem 8 in Zhang & Xiao (2019b).
F.1 Computational complexity of Theorem 4.1 and Theorem D. 1
Given the GCIVR algorithm, the sample complexity for Theorem 4.1 (where St, Bt and τt are fixed)
is
5	、	,	5	「「「「、，，、
O (max{TB, 2TτS}) X K) = O ( max{m, -~i=——m} + max{2-— √m√m, √m√m} ln(1∕e)
∖	√mμη	√mμη
=O ( (max{m,	—m} + max{ — √m, m} ) ln(1∕e))
mμη	μη
=O ([m + κ√m] ln(1∕e))
(35)
19
Published as a conference paper at ICLR 2022
Proof of Theorem 4.3: The proof follows directly from Theorem 4 in Zhang & Xiao (2019b).
F.2 Computational Complexity of strongly-convex objectives Corresponding
to Theorems 4.3
From Theorem 4 in Zhang & Xiao (2019b) for non-convex objectives we have:
E
if T ≤ T0 ,
else T > T0 ,
(36)
Considering the GCIVR algorithm and the bound in Eq. (36), the sample complexity for Theorem 4.3
(where St and τt are fixed) can be expressed as follows:
ʃ PT=0 (Bt + StTt) if T = O (e-1∕2) ≤ To = O (√m),
IT X (B + ST) T = O (1∕√me) > To = O (√m),
P PT=o 2(γt + β)2 if T = O (e-1∕2) ≤ To = O (√m),
√m √m X (m + √m√m) T = O (1∕√me) > To = O (√m),
O o(y2t3)
I O (F)
O O (e-3∕2)
t O ( F )
if T = O (e-1∕2) ≤ To = O (√m),
T = O (1∕√me) > To = O (√m),
if T = O (e-1∕2) ≤ To = O (√m),
T = O (1/√me) > To = O (√m),
(37)
Therefore, depending on how large √m is and also desired accuracy level e, We can decide to choose
an adaptive or a non-adaptive approach.
F.3 Convergence analysis
In this section, we extend assumptions used for centralized setting to distributed counterpart as
follows:
Assumption 6. We have the following assumptions:
1)	kf0(x1) - f0(x2)k ≤ Lh kx1 - x2 k where x1, x2 ∈ R. We also assume that
kf(x1) - f(x2)k ≤ `h kx1 - x2k
2)	∣∣Vh7-(xι) — Vh7-(x2)k ≤ Lh ∣∣xι 一 x2k where xι, x2 ∈ Rd. We also assume that
khj(x1) - hj(x2)k ≤ `h kx1 - x2k
3)	∣∣Vgj(Xi；ξ)-Vgj(X2；ξ)k	≤	LgIlxI-x2k	and	IIgj(xi；ξ)	— gj(x2;ξ)k	≤
`g ∣x1 一 x2∣ for∀x1, x2 ∈ Rd and 1 ≤ j ≤ p.
4)	We suppose that Ψ(x) in Eq. (2) is boundedfrom below that Ψ* = infX Ψ(x) > -∞.
5)	We assume r(x) ∈ R ∪ {∞} is a convex and lower-semicontinues function.
Assumption 7 (Unbiased estimation). The mini-batch sampling is unbiased that is E [gj (x; ξ)] =
gj(x) and E [Vgj (x; ξ)] = Vgj (x) for 1 ≤ j ≤ P.
Convergence Analysis for Strongly Convex Objectives:
Assumption 8. We additionally for the DRO problem with Wessterstein metric suppose that the
function r(x) is smooth with module G.
Theorem F.2. Under Assumptions 3 and 6 to 8, when Ψ is μ-optimally StrOngIy convex, if we
SetTt = St = √Bt = √m and T = √5μη, and Y = ⅛K) Ietting η < 刀一石”。
and α > G, if we let x(k+1) = DistBCO (Xik)) for k = 0,..., K — 1 using Algorithm 2
after K = ln (1∕e) stages and returning the final solution after projecting onto the constraint set
K = {x∣gi(x) ≤ 0, i = 1, 2,..., m}, i.e., X(K) = ∏κ(x(K)), solves the optimization problem in
Eq. (3) with convergence error r(X(τ)) — r(x(*)) ≤ e with
O ( (m + m + κ√m) ln U)	(38)
20
Published as a conference paper at ICLR 2022
per device number of constraint checks.
Remark 5 (Computational Complexity). The sample complexity for Theorem F.2 (where St, Bt and
τt are fixed) is
O (max{TB, 2TτS}) X K) = O (max{m,	——m} + max{2-— √m√m, √m√m} ln(1∕e)
P √mμη	√mμη
O ( (max{m,	—m} + max{ — √m, m} ) ln(1∕e)
11	√mμη	μη
O
m + m + κ√m ln(1∕e)
p
(39)
Convergence Analysis for Non-Convex Objectives: For the non-convex case we also need the
following extra assumption.
Assumption 9 (Bounded variance). The mini-batch sampling has bounded variance that is
E [kVgj(x； Z) — Vgj(x)k2i ≤ σ2
for 1 ≤ j ≤ m.
Theorem F.3. Under Assumptions 6 to 9 using Algorithm 2, for some positive constants β and
0 ≤ Z <	√m;	denoting To，'m-	= O	(√m),	if t ≤	To,	parameters are chosen to be
Tt = St = √Bt = βt + Z; and when t > To, set Tt = St = √m. Then, if η <
it
holds that
E
_____4
LΦ+√, lΦ+12go
O ClTT)
O (_____ln£____
√∕m(T-To+I)
if T ≤ To ,
T >To,
(40)
This leads to the per device sample complexity of O (min{ √∕m + √m, ^lɪ + 3}) where O(.)
notation hides logarithmic factors. Therefore, depending on the objective function Ψ this sample
complexity corresponds to both DRO problem with χ2 and regularized KL metrics.
We highlight that while in distributed setting we can not reduce computational complexity in terms
of order of magnitude, we can reduce computational complexity partially linearly with number of
devices.
21
Published as a conference paper at ICLR 2022
G	Proof of the Distributed Algorithm
For the convince, we define the following notations
ξ(t) = {ξ(t)	ξ(t)}
ξ = {ξ1 , . . . , ξp },
to denote the set of local solutions and sampled mini-batches at iteration t at different machines,
respectively.
We use notation EH to denote the conditional expectation Eξ(t) [∙].
G.1 General Lemmas
Before proceeding to the proof of main theorems, we first review a few properties of our algorithm
that will be useful in our convergence proof:
E yj(t,i)|x(jt) = yj(t-,i1) +gi(x(jt)) - gi(x(jt-)1)
E hr(t,i) ⑴]—r(t,i)	▽	⑴、	▽	(t) ∖
E Zj	|xj	= zj-1 + Vgi(Xj) - ^gi(xj-i)
E l^w(t,i) lx(t)^l — w(t,i) + Vh ∙(x(t)) — Vh ∙(x(t) )	(41)
E wj |xj = wj-1 + Vhi (xj ) - Vhi(xj-1 )	(41)
which due to linearity and taking average over the models of all devices, leads to
E hyj(t)|x(jt)i = yj(t-)1 +g(x(jt)) - g(x(jt-)1)	(42)
E h (t) (t)i	(t)	V (t) V	(t)
E zj |xj = zj-1 + Vg(xj ) - Vg(xj-1)
E w(t) |x(t) w(t) + Vh(x(t)) Vh(x(t) )	(43)
E wj |xj = wj-1 + Vh(xj ) - Vh(xj -1 )	(43)
Additionally, we have equivalent update rule as follows:
yjt) = yj-ι+ s⅛ X [g(Xjt)；ξ)- g(Xj-1； ξ)i	(44)
ξ∈Sj(t)
Zjt) = Zj-I + s1t)X IVg(Xjt);--Vg(Xjt-i；ξ)i
ξ∈Sj(t)
Wjt)= Wj-I+ SIty X hVh(Xjt); S-Vh(X(-i；ξ)i	(45)
ξinSj(t)
Lemma G.1. For any 1 ≤ j ≤ τt we have:
E ]M)	- g	(Xjt)):]≤ E ]M)	- g	(X0t))[]+ X JyE	]M)	-	Xrt-fl	(46)
E 收)-g0 (Xjt))H ≤ E gt) - g0 (X0t))H + X Sg)E ]M) - Xrtfl
r=1
E [Jwjt) - h0 (Xjt) )]≤ E ]Mt) - h0 (X0t 用 + X S2y E ]M)-X 川	(47)
We note that Lemma G.1 is the generalization of Lemma 1 in Zhang & Xiao (2019b), and we provide
the proof for the sake of completeness.
Proof. Our proof can be considered as a generalization of the proof in Zhang & Xiao (2019b) to
distributed setting. For this end, we use the following equation for every fix vector u:
Var(X) = E IkX - uk2i - kE [X] - uk2
(48)
22
Published as a conference paper at ICLR 2022
Therefore, letting u = g(x(jt) ) we obtain:
E	yj(t)	-g(x(jt))2x(jt)	= E	hyj(t)x(jt)i	-g(x(jt))2+Varyj(t)x(jt)
(=) E hyj(t-)1x(jt-)1i - g(x(jt-)1)	+ Var yj(t)x(jt)	(49)
The final step of proof involves bounding the term Var yj(t) x(jt) as follows:
Var (yjt) Wt))= Var yj-ι + s1ty X [g(Xjt);ξ) - g(Xj-1；司卜jj
ξ∈Sj(t)
=s1t)Var (g(Xjt)； ξ)- g(xjt-ι; SlXjt))
(≤) s1t)E [∣∣g(χjt);ξ)- g(χjt-ι; ξ)∣∣2∣xjt)
≤) SfcE IXjt)-Xj-ι∣∣2∣xjt)[	(50)
where (≤) and (≤) follows from the definition of Var(.) and Assumption 6.
The proof for Eq. (47) follows similarly.
The rest of the proof is similar to Zhang & Xiao (2019b) but for the sake of completeness we
add the rest of the proof. For this purpose, we use the notation Φ(X) , h(X) + f (g(X)) and
VΦ(Xjt)) = (zjt)) f(yjt))+wjt)
Lemma G.2. Under Assumption 6 we have:
E [∣vφ(Xjt)) -vφ(Xjt))∣2l ≤ SG)xE [∣X(t) -X(t)∣2l + Bσ⅛	(51)
with definitions G0 , 3 gg4L2f + gf2 Lg2 and σ02 , 3 gg2Lf2 σg2 + gf2 σg20 +
Proof. Using Assumption 6, we obtain:
E
VΦ(Xjt)) -VΦ(Xjt))∣∣2
=E ∣∣∣(zj(t))Tf0(yj(t))+wj(t)-g0(X(jt))f0(g(X(jt)))-h0(X(jt))∣∣∣2
= E ∣∣∣(zj(t)) f0(yj(t)) - g0(X(jt))f 0(yj(t)) +g0(X(jt))f0(yj(t)) - g0(X(jt))f 0(g(X(jt))) + wj(t) - h0(X(jt))
≤ 3E	(Zjt))Tf0(yjt))- g0(Xjt))f0(yjt))∣0 +3E ∣∣g0(Xjt))f0(yjt)) -g0(Xjt))f0(g(Xjt)))∣∣2]
+ 3E [∣∣∣wj(t) - h0(X(jt))∣∣∣
Assum≤ption 6 3gf2E [∣∣∣zj(t) - g(X(jt))∣∣∣2] + 3gg2Lf2 E [∣∣∣yj(t) - X(jt)∣∣∣2] + 3E [∣∣∣wj(t) - h0(X(jt))∣∣∣2]
(52)
23
Published as a conference paper at ICLR 2022
Next step of proof is to utilize Lemma G.1 in Eq. (52), which leads to
E
∣∣VΦ(xjt)) - VΦ(xjt))
2
(53)
∣∣w0(t) - h0
where (a) comes from algorithm and computing initial full batch in the beginning of each epoch, and
using Assumption 9 as well as following bounds:
E ∣∣y(t)- g(XOt))I] ≤ Bσgy, E ]∣∣z(t) — g0(XOk))I] ≤ 需 and E ∣∣w0t) — h0(XOk))
(54)
□
The rest of the proof is to show that E ∣∣Gη (Xj2 )∣∣2
≤ e where Gn (Xjt))，1 (Xjt) — xj+j and
xj+ι = πf (Xjt)-η[vΦ(Xjt))D
However, Algorithm 2 produces approximate proximal gradient mapping:
Gn(Xjt)) , η (Xjt)-Xj+1)	(55)
where Xjj+ι = Πf (Xjt) - ηVΦ(Xjt))). So the following Lemmas connect two gradient approXima-
tions:
Lemma G.3. We have:
E h∣∣Gη(x2)∣∣2i ≤ 2E ∣∣Gn(x2)∣∣2 +2E ∣∣VΦ(Xjt))-VΦ(Xjt))∣∣2]	(56)
Proof. Using triangle inequality ∣∣Xjt) - Xj+ι∣∣ ≤ 2 ∣∣X(t) - Xj+ι∣∣ +2 ∣∣Xj+ι 一 Xj+ι∣∣ and the
definition of Gn(Xjt)) and Gn(x]), we have
E h∣∣Gη(x2)∣∣2i ≤ 2E ]∣∣Gn(x2)∣∣[ + η ∣∣Xjt)- xj+ι∣∣2
=2E	∣∣Gn(x2)∣∣2 +	η22	∣∣πf	(xjt)	- η hvφ(Xjt))D-	πf	(Xjt)-η hvφ(Xjt))D ∣∣2
=2E ∣∣Gn(x2)∣∣2 +2 ∣∣[VΦ(Xjt))-VΦ(xjt))]∣∣2	(57)
□
Based on the definition Ψ(X) , F (g(X)) + h(X) + r(X) we have the following lemma:
Lemma G.4. With definition LΦ = LF + Lh, we have:
E [ψ(χj∙+ι)] ≤ E [ψ(χjt))] - 2 (η -Lψη2) E [∣Gn(χ2)∣∣2] + 2 ∣∣vφ(Xjt))-VΦ(Xjt))∣∣2
(58)
24
Published as a conference paper at ICLR 2022
and
E [Ψ(xj+ι)] ≤ E [Ψ(xjt))] - 8 E IlGn(X2)1+ ? E ' Φ(Xjt))-十中(Xjt)“j
-(4η — l2φ ) E [MT/	(59)
Proof. Using Lφ-Lipschitz continuity of Ψ and the optimality of the ɪ -strongly convex of SubProb-
lem (r(X)), we obtain
Ψ(X(jt+)1) = F(g(X(jt+)1)) + h(X(jt+)1) + r(X(jt+)1)
≤ F(g(Xjt))) + h(Xjt)) + ([VF(g(Xjt))) + Vh(Xjt))]
=F (g(Xjt))) + h(Xjt)) + DhV F (g(X(t))) + V h(Xjt))i
, X(jt+)1 - X(jt)
, X(jt+)1 - X(jt)
LF+Lh
(t)
Xj+1
+ 1 Il/) T(t)
+ 而 IIXj+1- Xj
- X(jt)
II2 + r
〉
E
+
2
+ ( [vf(g(Xjt))) + Vh(Xjt))] - [VF(g(Xjt))) + Vh(Xjt))] , Xj+ι - j
+ S

LF+Lh
X(jt+)1 -X(jt)III2
2
≤ F(g(Xjt))) + h(Xjt))+r(Xjt)) - ɪ IIX(t) - Xj+JI -1 ɪ
j	j	j 2η j	j+	2η
LF+Lh
(t)	(t) II2
Xj+1 - Xj I

2
+ η「VF(g(Xjt))) + Vh(Xjt))- [VF(g(Xjt))) + Vh(Xjt))ii II2 + ɪ IH+ι - Xjt)II2
2	j	j	j	j	2η j	j
ψ(x(% - ɪ ||X(t) - X(t) Il2 - (L
ψ(Xj) 2η I|Xj Xj+1ll 12η
LF+Lh

2
Xj+ι-X("2 + 2 ∣∣vφ(Xjt))-Vφ(Xjt)))∣∣2
(60)
Next, we complete the proof by taking expectation from both sides of Eq. (60) concludes the proof of
Eq. (58).
Next, using Lemma G.2, we have
-4E [IIGn(x2):]≤ -8E	[IIGn(x2)II2i	+ 4E ]IIvφ(Xjt))- vφ(Xjt))IIj	(61)
We can prove Eq. (59), by simply adding Eq. (61) to both sides of Eq. (58).
□
We note that Lemmas G.3 and G.4 are an extension of Lemma 3 and 4 in Appendix Section of Zhang
& Xiao (2019b) with difference that here the function Ψ(X) includes extra function h(X), which
leads to LΦ = LF + Lh which is bigger than LF in Zhang & Xiao (2019b).
The rest of the proof is same as the proof of Theorems 4 and 8 in Zhang & Xiao (2019b) which is
based on Lemmas G.3 and G.4, and for more details we refer the reader to the Appendix section of
the reference Zhang & Xiao (2019b).
□
H Approximate Approach For DRO with Wesserstein Metric with
Non-convex Objectives
For the non-convex objectives, the optimization problem in Eq. (3) can be upper-bounded with the
optimization problem as follows:
1m
inf sup Eq [h(x; ξ)] ≤ minimize r(x)，一	fi(xi)	(62)
x∈X	x	m
Q∈DN	i=1
subject to gi(x) ≤ 0, ∀i ∈ {1,…，m}
25
Published as a conference paper at ICLR 2022
We note that for the case of convex objective in optimum solution inequality holds with equality;
however, in case of non-convex cost functions we do not have equality necessarily. We note that
Eq. (62) with non-convex constraint can not be easily solved via augmented function approach in
previous section. In order to solve the upper bound optimization problem in Eq. (62) efficiently via
our suggested composite approach. For this end, we suggest to solve the following optimization
problem where in constrained are modified to be convex:
1m
minimize f(x)，	X fi(xi)	(63)
xm
i=1
subject to gi，gi(x) + μi ||x『≤ 0, ∀i ∈{1,…，m}	(64)
where We choose gi such that gi are strongly convex. Therefore, We have the following relationship
between optimization problem:
1m
inf sup EQ [h(x; ξ)] ≤ minimize r(x)，	^X^ fi(xi)	(65)
χ∈x CF	X	m
Q∈DN	i=1
subject to gi(x) ≤ 0, ∀i ∈ {1,…，m}
1m
≤ minimize r(x)，一	fi(xi)	(66)
xm
i=1
subject to gi(x) + μi ||x『≤ 0, ∀i ∈ {1,…，m}	(67)
26