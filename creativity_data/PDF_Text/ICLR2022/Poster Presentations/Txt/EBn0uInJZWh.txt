Published as a conference paper at ICLR 2022
model-based	offline meta-reinforcement
LEARNING WITH REGULARIZATION
Sen Lin1 , Jialin Wan1 , Tengyu Xu2, Yingbin Liang2, Junshan Zhang1,3
1School of ECEE, Arizona State University 2Department of ECE, The Ohio State University
3Department of ECE, University of California, Davis
{slin70, jwan20}@asu.edu, {xu.3260, liang.889}@osu.edu, jazh@ucdavis.edu
Abstract
Existing offline reinforcement learning (RL) methods face a few major challenges,
particularly the distributional shift between the learned policy and the behavior
policy. Offline Meta-RL is emerging as a promising approach to address these
challenges, aiming to learn an informative meta-policy from a collection of tasks.
Nevertheless, as shown in our empirical studies, offline Meta-RL could be outper-
formed by offline single-task RL methods on tasks with good quality of datasets,
indicating that a right balance has to be delicately calibrated between “exploring”
the out-of-distribution state-actions by following the meta-policy and “exploiting”
the offline dataset by staying close to the behavior policy. Motivated by such em-
pirical analysis, we propose model-based offline Meta-RL with regularized Policy
Optimization (MerPO), which learns a meta-model for efficient task structure in-
ference and an informative meta-policy for safe exploration of out-of-distribution
state-actions. In particular, we devise a new meta-Regularized model-based Actor-
Critic (RAC) method for within-task policy optimization, as a key building block
of MerPO, using both conservative policy evaluation and regularized policy im-
provement; and the intrinsic tradeoff therein is achieved via striking the right bal-
ance between two regularizers, one based on the behavior policy and the other on
the meta-policy. We theoretically show that the learnt policy offers guaranteed
improvement over both the behavior policy and the meta-policy, thus ensuring the
performance improvement on new tasks via offline Meta-RL. Experiments corrob-
orate the superior performance of MerPO over existing offline Meta-RL methods.
1 Introduction
Figure 1: FOCAL vs. COMBO.
Offline reinforcement learning (a.k.a., batch RL) has recently attracted extensive attention by learn-
ing from offline datasets previously collected via some behavior policy (Kumar et al., 2020). How-
ever, the performance of existing offline RL methods could degrade significantly due to the following
issues: 1) the possibly poor quality of offline datasets (Levine et al., 2020) and 2) the inability to
generalize to different environments (Li et al., 2020b). To tackle these challenges, offline Meta-RL
(Li et al., 2020a; Dorfman & Tamar, 2020; Mitchell et al., 2020; Li et al., 2020b) has emerged very
recently by leveraging the knowledge of similar offline RL tasks (Yu et al., 2021a). The main aim
of these studies is to enable quick policy adaptation for new offline tasks, by learning a meta-policy
with robust task structure inference that captures the structural properties across training tasks.
Because tasks are trained on offline datasets, value
overestimation (Fujimoto et al., 2019) inevitably oc-
curs in dynamic programming based offline Meta-RL,
resulted from the distribution shift between the be-
havior policy and the learnt task-specific policy. To
guarantee the learning performance on new offline
tasks, a right balance has to be carefully calibrated be-
tween “exploring” the out-of-distribution state-actions
by following the meta-policy, and “exploiting” the of-
fline dataset by staying close to the behavior policy.
However, such a unique “exploration-exploitation” tradeoff has not been considered in existing of-
fline Meta-RL approaches, which would likely limit their ability to handle diverse offline datasets
1
Published as a conference paper at ICLR 2022
particularly towards those with good behavior policies. To illustrate this issue more concretely, we
compare the performance between a state-of-the-art offline Meta-RL algorithm FOCAL (Li et al.,
2020b) and an offline single-task RL method COMBO (Yu et al., 2021b) in two new offline tasks.
As illustrated in Figure 1, while FOCAL performs better than COMBO on the task with a bad-quality
dataset (left plot in Figure 1), it is outperformed by COMBO on the task with a good-quality dataset
(right plot in Figure 1). Clearly, existing offline Meta-RL fails in several standard environments (see
Figure 1 and Figure 11) to generalize universally well over datasets with varied quality. In order to
fill such a substantial gap, we seek to answer the following key question in offline Meta-RL:
How to design an efficient offline Meta-RL algorithm to strike the right balance between exploring
with the meta-policy and exploiting the offline dataset?
To this end, we propose MerPO, a model-based offline Meta-RL approach with regularized Policy
Optimization, which learns a meta-model for efficient task structure inference and an informa-
tive meta-policy for safe exploration of out-of-distribution state-actions. Compared to existing ap-
proaches, MerPO achieves: (1) safe policy improvement: performance improvement can be guar-
anteed for offline tasks regardless of the quality of the dataset, by strike the right balance between
exploring with the meta-policy and exploiting the offline dataset; and (2) better generalization ca-
pability: through a conservative utilization of the learnt model to generate synthetic data, MerPO
aligns well with a recently emerging trend in supervised meta-learning to improve the generaliza-
tion ability by augmenting the tasks with “more data” (Rajendran et al., 2020; Yao et al., 2021). Our
main contributions can be summarized as follows:
(1)	Learnt dynamics models not only serve as a natural remedy for task structure inference in of-
fline Meta-RL, but also facilitate better exploration of out-of-distribution state-actions by generating
synthetic rollouts. With this insight, we develop a model-based approach, where an offline meta-
model is learnt to enable efficient task model learning for each offline task. More importantly,
we propose a meta-regularized model-based actor-critic method (RAC) for within-task policy opti-
mization, where a novel regularized policy improvement module is devised to calibrate the unique
“exploration-exploitation” tradeoff by using an interpolation between two regularizers, one based
on the behavior policy and the other on the meta-policy. Intuitively, RAC generalizes COMBO to
the multi-task setting, with introduction of a novel regularized policy improvement module to strike
a right balance between the impacts of the meta-policy and the behavior policy.
(2)	We theoretically show that under mild conditions, the learnt task-specific policy based on MerPO
offers safe performance improvement over both the behavior policy and the meta-policy with high
probability. Our results also provide a guidance for the algorithm design in terms of how to ap-
propriately select the weights in the interpolation, such that the performance improvement can be
guaranteed for new offline RL tasks.
(3)	We conduct extensive experiments to evaluate the performance of MerPO. More specifically, the
experiments clearly show the safe policy improvement offered in MerPO, corroborating our theo-
retical results. Further, the superior performance of MerPO over existing offline Meta-RL methods
suggests that model-based approaches can be more beneficial in offline Meta-RL.
2	Related Work
Offline single-task RL. Many existing model-free offline RL methods regularize the learnt policy
to be close to the behavior policy by, e.g., distributional matching (Fujimoto et al., 2019), support
matching (Kumar et al., 2019), importance sampling (Nachum et al., 2019; Liu et al., 2020), learning
lower bounds of true Q-values (Kumar et al., 2020). Along a different avenue, model-based algo-
rithms learn policies by leveraging a dynamics model obtained with the offline dataset. (Matsushima
et al., 2020) directly constrains the learnt policy to the behavior policy as in model-free algorithms.
To penalize the policy for visiting states where the learnt model is likely to be incorrect, MOPO
(Yu et al., 2020) and MoREL (Kidambi et al., 2020) modify the learnt dynamics such that the value
estimates are conservative when the model uncertainty is above a threshold. To remove the need
of uncertainty quantification, COMBO (Yu et al., 2021b) is proposed by combining model-based
policy optimization (Janner et al., 2019) and conservative policy evaluation (Kumar et al., 2020).
Offline Meta-RL. A few very recent studies have explored the offline Meta-RL. Particularly, (Li
et al., 2020a) considers a special scenario where the task identity is spuriously inferred due to biased
2
Published as a conference paper at ICLR 2022
datasets, and applies the triplet loss to robustify the task inference with reward relabelling. (Dorfman
& Tamar, 2020) extends an online Meta-RL method VariBAD (Zintgraf et al., 2019) to the offline
setup, and assumes known reward functions and shared dynamics across tasks. Based on MAML
(Finn et al., 2017), (Mitchell et al., 2020) proposes an offline Meta-RL algorithm with advantage
weighting loss, and learns initializations for both the value function and the policy, where they
consider the offline dataset in the format of full trajectories in order to evaluate the advantage. Based
on the off-policy Meta-RL method PEARL (Rakelly et al., 2019), (Li et al., 2020b) combines the idea
of deterministic context encoder and behavior regularization, under the assumption of deterministic
MDP. Different from the above works, we study a more general offline Meta-RL problem. More
importantly, MerPO strikes a right balance between exploring with the meta-policy and exploiting
the offline dataset, which guarantees safe performance improvement for new offline tasks.
3	Preliminaries
Consider a Markov decision process (MDP) M = (S, A, T, r, μo,γ) with state space S, action
space A, the environment dynamics T (s0 |s, a), reward function r(s, a), initial state distribution
μo, and Y ∈ (0,1) is the discount factor. Without loss of generality, We assume that |r(s, a)| ≤
Rmax. Given a policy ∏, let dM(s) := (1 - Y) P∞=o YtPM(St = s∣∏) denote the discounted
marginal state distribution, where PM(St = s∣∏) denotes the probability of being in state S at
time t by rolling out π in M. Accordingly, let dM(s, a) := d∏M(s)π(a∣s) denote the discounted
marginal state-action distribution, and J(M, ∏) := ι-1γE(s,a)〜d∏4(s,a)[r(s, a)] denote the expected
discounted return. The goal of RL is to find the optimal policy that maximizes J(M, π). In offline
RL, no interactions with the environment are allowed, and we only have access to a fixed dataset
D = {(S, a, r, S0)} collected by some unknown behavior policy πβ. Let dπMβ (S) be the discounted
marginal state distribution of ∏β. The dataset D is indeed sampled from dM(s,a) = dM(s)∏β(a|s).
Denote M as the empirical MDP induced by D and d(s, a) as a sample-based version of dM(s, a).
In offline Meta-RL, consider a distribution of RL tasks p(M) as in standard Meta-RL (Finn et al.,
2017; Rakelly et al., 2019), where each task Mn is an MDP, i.e., Mn = (S, A, Tn, rn, μo,n, γ), with
task-shared state and action spaces, and unknown task-specific dynamics and reward function.For
each task Mn , no interactions with the environment are allowed and we only have access to an
offline dataset Dn, collected by some unknown behavior policy πβ,n. The main objective is to learn
a meta-policy based on a set of offline training tasks {Mn}nN=1.
Conservative Offline Model-Based Policy Optimization (COMBO). Recent model-based offline
RL algorithms, e.g., COMBO (Yu et al., 2021b), have demonstrated promising performance on
a single offline RL task by combining model-based policy optimization (Janner et al., 2019) and
conservative policy evaluation (CQL (Kumar et al., 2020)). Simply put, COMBO first trains a
dynamics model Tq(s0∣s, a) parameterized by θ, Via supervised learning on the offline dataset D.
The learnt MDP is constructed as M := (S, A, T, r, μo, Y). Then, the policy is learnt using D and
model-generated rollouts. Specifically, define the action-value function (Q-function) as Qπ (S, a) :=
E [Pt∞=0 Ytr(St, at)|S0 = S, a0 = a], and the empirical Bellman operator as: BbπQ(S, a) = r(S, a)+
γE(s,a,s0)〜D[Q(s0, a0)], for a0 〜 π(∙∣s0). To penalize the Q functions in out-of-distribution state-
action tuples, COMBO employs conservative policy evaluation based on CQL:
Qk+1 — arg min β(Es,a〜ρ[Q(s,a)] - Es,a〜D[Q(s,a)]) + 1 Es,a,s，〜df [(Q(s,a) - BnQk(S,a))2] (1)
Q(s,a)	2	f
where ρ(s, a) := dπc(s)π(a∣s) is the discounted marginal distribution when rolling out π in Mc,
and df (S, a) = f dπMβ (S, a) + (1 - f)ρ(S, a) for f ∈ [0, 1]. The Bellman backup Bbπ over df can
be interpreted as an f -interpolation of the backup operators under the empirical MDP (denoted by
BM) and the learnt MDP (denoted by BπC). Given the Q-estimation Qn, the policy can be learnt by:
∏0 — arg max Es 〜ρ(s),a 〜∏(∙∣s)[(bπ(s,a)].	(2)
π
4	MerPO: Model-Based Offline Meta-RL with Regularized
Policy Optimization
Learnt dynamics models not only serves as a natural remedy for task structure inference in offline
Meta-RL, but also facilitates better exploration of out-of-distribution state-actions by generating
3
Published as a conference paper at ICLR 2022
Figure 2: Model-based offline Meta-RL with learning of offline meta-model and offline meta-policy.
synthetic rollouts (Yu et al., 2021b). Thus motivated, we propose a general framework of model-
based offline Meta-RL, as depicted in Figure 2. More specifically, the offline meta-model is first
learnt by using supervised meta-learning, based on which the task-specific model can be quickly
adapted. Then, the main attention of this study is devoted to the learning of an informative meta-
policy via bi-level optimization, where 1) a model-based policy optimization approach is leveraged
in the inner loop for each task to learn a task-specific policy; and 2) the meta-policy is then updated
in the outer loop based on the learnt task-specific policies.
4.1	Offline Meta-Model Learning
Learning a meta-model based on the set of offline dataset {Dn}nN=1 can be carried out via supervised
meta-learning. Many gradient-based meta-learning techniques can be applied here, e.g., MAML
(Finn et al., 2017) and Reptile (Nichol et al., 2018). In what follows, we outline the basic idea to
leverage the higher-order information of the meta-objective function. Specifically, we consider a
proximal meta-learning approach, following the same line as in (Zhou et al., 2019):
min Lmodel (φm) = EMn
φm
{mmin hE(s,a,s0)〜Dn [log Tθη (Sls,a)] + ηkθn - φm ∣∣2
(3)
where the learnt dynamics for each task Mn is parameterized by θn and the meta-model is parame-
terized by φm . Solving Eq. (3) leads to an offline meta-model.
Given the learnt meta-model Tφ惠,the dynamics model for an individual offline task j can be found
by solving the following problem via gradient descent with initialization Tφ惠 using Dj, i.e.,
min E(s,a,s0)~d「[log Tbj (s0∣s,a)]+ η∣θj — φ" ∣∣2.	(4)
θj
Compared to learning the dynamics model from scratch, adapting from Tφ惠 can quickly generate
a dynamics model for task identity inference by leveraging the knowledge from similar tasks, and
hence improve the sample efficiency (Finn et al., 2017; Zhou et al., 2019).
4.2	Offline Meta-Policy Learning
In this section, we turn attention to tackle one main challenge in this study: How to learn an infor-
mative offline meta-policy in order to achieve the optimal tradeoff between “exploring” the out-of-
distribution state-actions by following the meta-policy and “exploiting” the offline dataset by staying
close to the behavior policy? Clearly, it is highly desirable for the meta-policy to safely ‘explore’
out-of-distribution state-action pairs, and for each task to utilize the meta-policy to mitigate the issue
of value overestimation.
4.2.1	How Do Existing Proximal Meta-RL Approaches Perform?
Proximal Meta-RL approaches have demonstrated remarkable performance in the online setting
(e.g., (Wang et al., 2020)), by explicitly regularizing the task-specific policy close to the meta-policy.
We first consider the approach that applies the online Proximal Meta-RL method directly to devise
offline Meta-RL, which would lead to:
max EMn max
where πc is the offline meta-policy, πn is the task-specific policy, ρn is the state marginal of ρn (s, a)
for task n and D(∙, ∙) is some distance measure between two probability distributions. To alleviate
value overestimation, conservative policy evaluation can be applied to learn Qπn by using Eq. (1).
E S 〜Pn,	Q n (s, a) — λD(∏n ,∏c)
a〜∏n (Ts) L
(5)
4
Published as a conference paper at ICLR 2022
Intuitively, Eq. (5) corresponds to generalizing COMBO to the multi-task setting, where a meta
policy πc is learned to regularize the within-task policy optimization.
To get a sense of how the meta-policy learnt using Eq. (5) performs,
we evaluate its performance in an offline variant of standard Meta-RL
benchmark Walker-2D-Params with good-quality datasets, and evalu-
ate the testing performance of the task-specific policy after fine-tuning
based on the learnt meta-policy, with respect to the meta-training
steps. As can be seen in Figure 3, the proximal Meta-RL algorithm
Eq. (5) performs surprisingly poorly and fails to learn an informative
meta-policy, despite conservative policy evaluation being applied in
within-task policy optimization to deal with the value overestimation.
In particular, the testing performance degrades along with the meta-
training process, implying that the quality of the learnt meta-policy is
in fact decreasing.
3 le2 Walker-2D-Par^ams
0.0	1.0	2.0 3.0 4.0	5.0
Gradient Steps (xlθ5)
一 Proximal Meta-RL Eq.(5)
Figure 3: Performance of
proximal Meta-RL Eq. (5).
Why does the proximal Meta-RL method in Eq. (5) perform poorly in offline Meta-RL, even with
conservative policy evaluation? To answer this, it is worth to take a closer look at the within-task
policy optimization in Eq. (5), which is given as follows:
π
∏n J arg max Es~ρn ,a~∏n (∙∣s) [Qn (s,a)] - λD(∏n ,∏c).	(6)
Clearly, the performance of Eq. (6) depends heavily on the quality of the meta-policy πc . A poor
meta-policy may have negative impact on the performance and result in a task-specific policy πn that
is even outperformed by the behaviour policy πβ,n. Without online exploration, the quality of πn
could not be improved, which in turn leads to a worse meta-policy πc through Eq. (5). The iterative
meta-training process would eventually result in the performance degradation in Figure 3.
In a nutshell, simply following the meta-policy may lead to worse performance of offline tasks when
πβ is a better policy than πc . Since it is infeasible to guarantee the superiority of the meta-policy a
priori, it is necessary to balance the tradeoff between exploring with the meta-policy and exploiting
the offline dataset, in order to guarantee the performance improvement of new offline tasks.
4.2.2	Safe Policy Improvement with Meta-Regularization
To tackle the above challenge, we next devise a novel regularized policy improvement for within-
task policy optimization of task n, through a weighted interpolation of two different regularizers
based on the behavior policy πβ,n and the meta-policy πc, given as follows:
π
∏n J arg max Es~p〃，a~∏n(心)[Qn (s,a)] — λaD(∏n ,∏β,n) - λ(1 - 0)D(∏n ,∏c),	(7)
πn
for some α ∈ [0, 1]. Here, α controls the trade-off between staying close to the behavior policy and
following the meta-policy to “explore” out-of-distribution state-actions. Intuitively, as α is closer
to 0, the policy improvement is less conservative and tends to improve the task-specific policy πn
towards the actions in πc that have highest estimated Q-values. Compared to Eq. (6), the exploration
penalty induced by D(πn, πβ,n) serves as a safeguard and stops πn following πc over-optimistically.
Safe Policy Improvement Guarantee. Based on con-
servative policy evaluation Eq. (1) and regularized pol-
icy improvement Eq. (7), we have the meta-regularized
model-based actor-critic method (RAC), as outlined in
Algorithm 1. Note that different distribution distance
measures can be used in Eq. (7). In this work, we theo-
retically show that the policy πn(a∣s) learnt by RAC is
a safe improvement over both the behavior policy πβ,n
and the meta-policy πc on the underlying MDP Mn,
when using the maximum total-variation distance for
D(∏1,∏2), i.e., D(∏1 ,∏2) ：= maxs DTV(∏1∣∣∏2).
Algorithm 1 RAC
1:	Train dynamics model Tbθn using Dn ;
2:	for k = 1, 2, ... do
3:	Perform model rollouts starting from
states in Dn and add into Dmodel,n ;
4:	Policy evaluation by recursively solv-
ing Eq. (1) using Dn ∪ Dmodel,n;
5:	Improve policy by solving Eq. (7);
6:	end for
For convenience, define νn(ρ, f) = Eρ [(ρ(s, a) - dn(s, a))/df,n(s, a)], and let δ ∈ (0, 1/2). We
have the following important result on the safe policy improvement achieved by πn (a|s).
Theorem L (a) Let e ="^^^二二；%(：：：)f. If Vn(ρπn ,f) - Vn(ρπβ,n ,f) > 0 and α ∈
(max{1 - e, 0}, 1), then J(Mn,∏n) ≥ max{J(Mn,∏) + ξι, J(Mn,∏βn) + ξ2} holds with
probability at least 1 - 2δ, where both ξ1 and ξ2 are positive for large enough β and λ;
5
Published as a conference paper at ICLR 2022
(b) More generally, we have that J(Mn, πn) ≥ max{J (Mn, πc) + ξ1, J(Mn, πβ,n) + ξ2} holds
with probability at least 1 - 2δ, when α ∈ (0, 1/2), where ξ1 is positive for large enough λ.
Remark 1. The expressions of ξ1 and ξ2 are involved and can be found in Eq. (14) and Eq. (15) in
the appendix. In part (a) of Theorem 1, both ξ1 and ξ2 are positive for large enough β and λ, pointing
to guaranteed improvements over πc and πβ,n. Due to the fact that the dynamics TMd learnt via
supervised learning is close to the true dynamics TMn on the states visited by the behavior policy
∏β,n, dπd,n(s, a) is close to dMβ,n(s, a) and 产,n is close to dn(s, a), indicating that the condition
νn (ρπn, f) - νn(ρπβ,n, f) > 0 is expected to hold in practical scenarios (Yu et al., 2021b). For more
general cases, a slightly weaker result can be obtained in part (b) of Theorem 1, where ξ1 is positive
for large enough λ and ξ2 can be negative.
Remark 2. Intuitively, the selection of α balances the impact of πβ,n and πc, while delicately
leaning toward the meta-policy πc because πβ,n has played an important role in policy evaluation
to find a lower bound of Q-value. As a result, Eq. (7) maximizes the true Q-value while implic-
itly regularized by a weighted combination, instead of α-interpolation, between D(πn, πβ,n) and
D(πn, πc), where the weights are carefully balanced through α. In particular, in the tabular setting,
the conservative policy evaluation in Eq. (1) corresponds to penalizing the Q estimation (Yu et al.,
2021b):
Qn+1(s,a) = BbπQn(s,a) - β[ρ(sy -dn(S,a)].	(8)
n	n	df,n (s, a)
Clearly, increases with the value of the penalty term in Eq. (8). As a result, when the policy
evaluation Eq. (1) is overly conservative, the lower bound of α will be close to 0, and hence the
regularizer based on the meta-policy πc can play a bigger role so as to encourage the “exploration”
of out-of-distribution state-actions following the guidance of πc. On the other hand, when the policy
evaluation Eq. (1) is less conservative, the lower bound of α will be close to 1, and the regularizer
based on πβ,n will have more impact, leaning towards “exploiting” the offline dataset. In fact,
the introduction of 1) behavior policy-based regularizer and 2) the interpolation for modeling the
interaction between the behavior policy and the meta-policy, is the key to prove Theorem 1.
Practical Implementation. In practice, We can use the KL divergence to replace the total variation
distance between policies, based on Pinsker,s Inequality: k∏ι-∏2k ≤ '2Dkl(∏i∣∣∏2). Moreover,
since the behavior policy πβ,n is typically unknown, we can use the reverse KL-divergence between
πn and πβ,n to circumvent the estimation ofπβ,n, following the same line as in (Fakoor et al., 2021):
DκL(∏β,n∣∣∏n) = Ea 〜∏β,n[lθg ∏β,n(a∣s)] - Ea 〜∏β,n [lθg ∏n(a∣s)]
X -Ea 〜∏β,n [lθg ∏n(a∣s)] ≈ -E(s,a)-Dn[lθg ∏n(a∣s)].
Then, the task-specific policy can be learnt by solving the following problem:
max Es〜ρn,a〜∏n(∙∣s) [(bπ(s,a)] + λαE(s,a)〜Dn[log∏n(a∣s)] - λ(1 - α)DκL(∏n∖∖∏c).	(9)
4.2.3	Offline Meta-Policy Update
Built on RAC, the offline meta-policy πc is updated by taking the following two steps, in an iterative
manner: 1) (inner loop) given the meta-policy πc, RAC is run for each training task to obtain the
task-specific policy πn ; 2) (outer loop) based on {πn}n, πc is updated by solving:
max EMn E E s~ρn	Qπ(s,a) + λαE(s,a)〜Dn[log∏n(a∖s)] - λ(1 - α)DκL(∏n∖∖∏c)卜 (10)
πc	[ a〜∏n(∙∣s) L	」	J
where both ρn and Qπn are from the last iteration of the inner loop for each training task. By
using RAC in the inner loop for within-task policy optimization, the learnt task-specific policy πn
and the meta-policy πc work in concert to regularize the policy search for each other, and improve
akin to ‘positive feedback’. Here the regularizer based on the behavior policy serves an important
initial force to boost the policy optimization against the ground: RAC in the inner loop aims to
improve the task-specific policy over the behavior policy at the outset and the improved task-specific
policy consequently regularizes the meta-policy search as in Eq. (10), leading to a better meta-policy
eventually. Noted that a meta-Q network is learnt using first-order meta-learning to initialize task-
specific Q networks. It is worth noting that different tasks can have different values of α to capture
the heterogeneity of dataset qualities across tasks.
6
Published as a conference paper at ICLR 2022
Figure 4: Performance comparison among COMBO, COMBO-3 and RAC, with good-quality meta-
policy (two figures on the left) and poor-quality meta-policy (two figures on the right).
In a nutshell, the proposed model-based offline Meta-RL with regularized Policy Optimization
(MerPO) is built on two key steps: 1) learning the offline meta-model via Eq. (3) and 2) learning the
offline meta-policy via Eq. (10). The details are presented in Algorithm 2 in the appendix.
4.3	MerPO-based Policy Optimization for New Offline RL Task
Let Tφκ and π* be the offline meta-model and the offline meta-policy learnt by MerPO. For a new
offline RL task, the task model can be quickly adapted based on Eq. (4), and the task-specific policy
can be obtained based on π* using the Within-task policy optimization module RAC. Appealing to
Theorem 1, we have the following result on MerPO-based policy learning on a new task.
Proposition 1. Consider a new offline RL task with the true MDP M. Suppose πo is the
MerPO-based task-specific policy, learnt by running RAC over the meta-policy π*. If C =
β[ν(ρπo,f)-ν(ρπβ f )]
2λ(1-γ )D(∏o,∏β)
≥ 0 and α ∈ (max{ɪ - e, 0}, 2), then πo achieves the safe performance
improvement over both π* and ∏β, i.e., J(M,πo) > max{J(M, π*), J(M,∏β)} holds withprob-
ability at least 1 - 2δ, for large enough β and λ.
Proposition 1 indicates that MerPO-based policy optimization for learning task-specific policy guar-
antees a policy with higher rewards than both the behavior policy and the meta-policy. This is
particularly useful in the following two scenarios: 1) the offline dataset is collected by some poor
behavior policy, but the meta-policy is a good policy; and 2) the meta-policy is inferior to a good
behavior policy.
5	Experiments
In what follows, we first evaluate the performance of RAC for within-task policy optimization on
an offline RL task to validate the safe policy improvement, and then examine how MerPO performs
when compared to state-of-art offline Meta-RL algorithms. Due to the space limit, we relegate
additional experiments to the appendix.
5.1	Performance Evaluation of RAC
Setup. We evaluate RAC on several continuous control tasks in the D4RL benchmark (Fu et al.,
2020) from the Open AI Gym (Brockman et al., 2016), and compare its performance to 1) COMBO
(where no meta-policy is leveraged) and 2) COMBO with policy improvement Eq. (6) (namely,
COMBO-3), under different qualities of offline datasets and different qualities of meta-policy (good
and poor). For illustrative purpose, we use a random policy as a poor-quality meta-policy, and
choose the learnt policy after 200 episodes as a better-quality meta-policy. We evaluate the average
return over 4 random seeds after each episode with 1000 gradient steps.
Results. As shown in Figure 4, RAC can achieve comparable performance with COMBO-3 given
a good-quality meta-policy, and both clearly outperform COMBO. Besides, the training procedure
is also more stable and converges more quickly as expected when regularized with the meta-policy.
When regularized by a poor-quality meta-policy, that is significantly worse than the behavior policy
in all environments, the performance of COMBO-3 degrades dramatically. However, RAC outper-
forms COMBO even when the meta-policy is a random policy. In a nutshell, RAC consistently
7
Published as a conference paper at ICLR 2022
achieves the best performance in various setups and demonstrates compelling robustness against the
quality of the meta-policy, for suitable parameter selections (α = 0.4 in Figure 4).
Impact of α. As shown in Theorem
1, the selection of α is important to
guarantee the safe policy improvement
property of RAC. Therefore, we next
examine the impact of α on the per-
formance of RAC under different qual-
ities of datasets and meta-policy. More
specifically, we consider four choices
of α: α = 0, 0.4, 0.7, 1. Here, α =
0 corresponds to COMBO-3, i.e., reg-
ularized by the meta-policy only, and
the policy improvement step is regular-
ized by the behavior policy only when
Figure 5: Impact of α on the performance of RAC under
different qualities of offline datasets.
α = 1. Figure 5 shows the average return of RAC over different qualities of meta-policies under
different qualities of the offline datasets. It is clear that RAC achieves the best performance when
α = 0.4 among the four selections of α, corroborating the result in Theorem 1. In general, the
performance of RAC is stable for α ∈ [0.3, 0.5] in our experiments.
5.2	Performance Evaluation of MerPO
Setup. To evaluate the performance of MerPO, we follow the setups in the literature (Rakelly et al.,
2019; Li et al., 2020b) and consider continuous control meta-environments of robotic locomotion.
More specifically, tasks has different transition dynamics in Walker-2D-Params and Point-Robot-
Wind, and different reward functions in Half-Cheetah-Fwd-Back and Ant-Fwd-Back. We collect
the offline dataset for each task by following the same line as in (Li et al., 2020b). We consider the
following baselines: (1) FOCAL (Li et al., 2020b), a model-free offline Meta-RL approach based
on a deterministic context encoder that achieves the state-of-the-art performance; (2) MBML (Li
et al., 2020a), an offline multi-task RL approach with metric learning; (3) Batch PEARL, which
modifies PEARL (Rakelly et al., 2019) to train and test from offline datasets without exploration;
(4) Contextual BCQ (CBCQ), which is a task-augmented variant of the offline RL algorithm BCQ
(Fujimoto et al., 2019) by integrating a task latent variable into the state information. We train on
a set of offline RL tasks, and evaluate the performance of the learnt meta-policy during the training
process on a set of unseen testing offline RL tasks.
Fixed α vs Adaptive α. We consider two implementations of MerPO based on the selection of α.
1) MerPO: α is fixed as 0.4 for all tasks; 2) MerPO-Adp: at each iteration k, given the task-policy
πnk for task n and the meta-policy πck at iteration k, we update αkn using one-step gradient descent to
minimize the following problem.
min (1 - αkn)(D(πnk, πβ,n) - D(πnk, πck)), s.t. αkn ∈ [0.1, 0.5].	(11)
αk
αn
The idea is to adapt αnk in order to balance between D(πnk , πβ,n) and D(πnk , πck), because Theorem
1 implies that the safe policy improvement can be achieved when the impacts of the meta-policy and
the behavior policy are well balanced. Specifically, at iteration k for each task n, αnk is increased
when the task-policy πnk is closer to the meta-policy πck, and is decreased when πnk is closer to the
behavior policy. Note that αkn is constrained in the range [0.1, 0.5] as suggested by Theorem 1.
Results. As illustrated in Figure 6, MerPO-Adp yields the best performance, and both MerPO-
Adp and MerPO achieve better or comparable performance in contrast to existing offline Meta-RL
approaches. Since the meta-policy changes during the learning process and the qualities of the
behavior policies vary across different tasks, MerPO-Adp adapts α across different iterations and
tasks so as to achieve a ‘local’ balance between the impacts of the meta-policy and the behavior
policy. As expected, MerPO-Adp can perform better than MerPO with a fixed α. Here the best
testing performance for the baseline algorithms is selected over different qualities of offline datasets.
Ablation Study. We next provide ablation studies by answering the following questions.
(1)	Is RAC important for within-task policy optimization? To answer this question, we compare
MerPO with the approach Eq. (5) where the within-task policy optimization is only regularized by
8
Published as a conference paper at ICLR 2022
le2 Wa lker-2 D-Pa rams
0.0 1.0 2.0 3.0 4.0 5.0
Gradient Steps (×105)
-MerPO Adp ——MerPO ——CBCQ
-Batch PEARL ——FOCAL ——MBML
0.0 1.0 2.0 3.0 4.0 5.0
Gradient Steps (xlθ5)
-MerPO Adp ——MerPO ——CBCQ
-Batch PEARL ——FOCAL ——MBML
Point-Robot-Wind
0.0 1.0 2.0 3.0 4.0 5.0	^¾,0	0.2	0.5	0.8	1.0
Gradient Steps (×105)	Gradient Steps (xlθ5)
-MerPO Adp ——MerPO ——CBCQ	——MerPO Adp ——MerPO ——CBCQ
-Batch PEARL ——FOCAL ——MBML	——Batch PEARL ——FOCAL ——MBML
Figure 6:	Performance comparison in terms of the average return in different environments. Clearly,
MerPO Adp and MerPO achieve better or comparable performance than the baselines.
3 2 1
IUm①=①6(u」①><
(a) Impact of RAC mod- (b) Impact of model uti- (c) Performance under dif- (d) Testing performance
ule.	lization.	ferent data qualities. for expert dataset.
Figure 7:	Ablation study of MerPO in Walker-2D-Params.
the meta-policy. As shown in Figure 7(a), with the regularization based on the behavior policy in
RAC, MerPO performs significantly better than Eq. (5), implying that the safe policy improvement
property of RAC enables MerPO to continuously improve the meta-policy.
(2)	Is learning the dynamics model important? Without the utilization of models, the within-task
policy optimization degenerates to CQL (Kumar et al., 2020) and the Meta-RL algorithm becomes a
model-free approach. Figure 7(b) shows the performance comparison between the cases whether the
dynamics model is utilized. It can be seen that the performance without model utilization is much
worse than that of MerPO. This indeed makes sense because the task identity inference (Dorfman
& Tamar, 2020; Li et al., 2020a;b) is a critical problem in Meta-RL. Such a result also aligns well
with a recently emerging trend in supervised meta-learning to improve the generalization ability by
augmenting the tasks with “more data” (Rajendran et al., 2020; Yao et al., 2021).
(3)	How does MerPO perform in unseen offline tasks under different data qualities? We eval-
uate the average return in unseen offline tasks with different data qualities, and compare the per-
formance between (1) MerPO with α = 0.4 (“With meta”) and (2) Run a variant of COMBO with
behavior-regularized policy improvement, i.e., α = 1 (“With beha only”). For a fair comparison, we
initialize the policy network with the meta-policy in both cases. As shown in Figure 7(c), the aver-
age performance of “With meta” over different data qualities is much better than that of “With beha
only”. More importantly, for a new task with expert data, MerPO (“With meta”) clearly outperforms
COMBO as illustrated in Figure 7(d), whereas the performance of FOCAL is worse than COMBO.
6 Conclusion
In this work, we study offline Meta-RL aiming to strike a right balance between “exploring” the
out-of-distribution state-actions by following the meta-policy and “exploiting” the offline dataset
by staying close to the behavior policy. To this end, we propose a model-based offline Meta-RL
approach, namely MerPO, which learns a meta-model to enable efficient task model learning and a
meta-policy to facilitate safe exploration of out-of-distribution state-actions. Particularly, we devise
RAC, a meta-regularized model-based actor-critic method for within-task policy optimization, by
using a weighted interpolation between two regularizers, one based on the behavior policy and
the other on the meta-policy. We theoretically show that the learnt task-policy via MerPO offers
safe policy improvement over both the behavior policy and the meta-policy. Compared to existing
offline Meta-RL methods, MerPO demonstrates superior performance on several benchmarks, which
suggests a more prominent role of model-based approaches in offline Meta-RL.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work is supported in part by NSF Grants CNS-2003081, CNS-2203239, CPS-1739344, and
CCSS-2121222.
Reproducibility S tatement
For the theoretical results presented in the main text, we state the full set of assumptions of all
theoretical results in Appendix B, and include the complete proofs of all theoretical results in Ap-
pendix C. For the experimental results presented in the main text, we include the code in the supple-
mental material, and specify all the training details in Appendix A. For the datasets used in the main
text, we also give a clear explanation in Appendix A.
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ron Dorfman and Aviv Tamar. Offline meta reinforcement learning. arXiv preprint
arXiv:2008.02598, 2020.
Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70,pp. 1126-1135. JMLR. org, 2017.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Jiachen Li, Quan Vuong, Shuang Liu, Minghua Liu, Kamil Ciosek, Henrik Christensen, and Hao Su.
Multi-task batch reinforcement learning with metric learning. Advances in Neural Information
Processing Systems, 33, 2020a.
Lanqing Li, Rui Yang, and Dijun Luo. Efficient fully-offline meta-reinforcement learning via dis-
tance metric learning and behavior regularization. arXiv preprint arXiv:2010.01112, 2020b.
10
Published as a conference paper at ICLR 2022
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient
with stationary distribution correction. In Uncertainty in Artificial Intelligence, pp. ll80-1190.
PMLR, 2020.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. arXiv preprint
arXiv:2006.03647, 2020.
Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-
reinforcement learning with advantage weighting. arXiv preprint arXiv:2008.06043, 2020.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Janarthanan Rajendran, Alex Irpan, and Eric Jang. Meta-learning requires meta-augmentation. arXiv
preprint arXiv:2007.05549, 2020.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. arXiv preprint arXiv:1903.08254,
2019.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-
agnostic meta-learning. In International Conference on Machine Learning, pp. 9837-9846.
PMLR, 2020.
Huaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, et al.
Improving generalization in meta-learning via task augmentation. In International Conference on
Machine Learning, pp. 11887-11897. PMLR, 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea
Finn. Conservative data sharing for multi-task offline reinforcement learning. arXiv preprint
arXiv:2109.08128, 2021a.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021b.
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via
minibatch proximal update. Advances in Neural Information Processing Systems, 32:1534-1544,
2019.
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-
learning. arXiv preprint arXiv:1910.08348, 2019.
11
Published as a conference paper at ICLR 2022
Table 1: Hyperparameters for RAC.
Hyperparameters	Halfcheetah	Hopper	Walker2d
Discount factor	099	0.99	-0:99-
Sample batch size	256	256	256
Real data ratio	0.5	0.5	0.5
Model rollout length	5	5	1
Critic lr	3e-4	3e-4	1e-4
Actor lr	1e-4	1e-4	1e-5
Model lr	1e-3	1e-3	1e-3
Optimizer	Adam	Adam	Adam
β	1	1	10
Max entropy	True	True	True
λ		1	1	1
A Experimental Details
A. 1 Meta Environment Description
•	Walker-2D-Params: Train an agent to move forward. Different tasks correspond to different
randomized dynamcis parameters.
•	Half-Cheeta-Fwd-Back: Train a Cheetah robot to move forward or backward, and the re-
ward function depends on the moving direction. All tasks have the same dynamics model
but different reward functions.
•	Ant-Fwd-Back: Train an Ant robot to move forward or backward, and the reward function
depends on the moving direction. All tasks have the same dynamics model but different
reward functions.
•	Poing-Robot-Wind: Point-Robot-Wind is a variant of Sparse-Point-Robot (Li et al., 2020b),
a 2D navigation problem introduced in (Rakelly et al., 2019), where each task is to guide
a point robot to navigate to a specific goal location on the edge of a semi-circle from the
origin. In Point-Robot-Wind, each task is affected by a distinct “wind” uniformly sampled
from [-0.05, 0.05]2, and hence differs in the transition dynamics.
A.2 Implementation Details and More Experiments
A.2.1 Evaluation of RAC
Model learning. Following the same line as in (Janner et al., 2019; Yu et al., 2020; 2021b), the
dynamics model for each task is represented as a probabilistic neural network that takes the current
state-action as input and outputs a Gaussian distribution over the next state and reward:
Tθ(st+ι, r|s, a) = N(μθ(st, at), ∑θ(St, at)).
An ensemble of 7 models is trained independently using maximum likelihood estimation, and the
best 5 models are picked based on the validation prediction error using a held-our set of the offline
dataset. Each model is represented by a 4-layer feedforward neural network with 256 hidden units.
And one model will be randomly selected from the best 5 models for model rollout.
Policy optimization. We represent both Q-network and policy network as a 4-layer feedforward
neural network with 256 hidden units, and use clipped double Q-learning (Fujimoto et al., 2018) for
Q backup update. A max entropy term is also included to the value function for computing the target
Q value as in SAC (Haarnoja et al., 2018). The hyperparameters used for evaluating the performance
of RAC are described in Table 1.
Additional experiments. We also evaluate the performance of RAC in Walker2d under different
qualities of the meta-policy. As shown in Figure 8(a) and 8(b), RAC achieves the best performance
in both scenarios, compared to COMBO and COMBO-3. Particularly, the performance of COMBO-
3 in Figure 8(a) degrades in the later stage of training because the meta-policy is not superior over
the behavior policy in this case. In stark contrast, the performance of RAC is consistently better, as
it provides a safe policy improvement guarantee over both the behavior policy and the meta-policy.
12
Published as a conference paper at ICLR 2022
400
1.2
0.9
0.6
0.3
0.0
le4 HaIfCheetah-Expert
0	100	200	300	400
Episode
---alpha=0.0 ------- alpha=O.7
---alpha=0.4 ------- alpha=1.0
0	100	200	300
Episode
——COMBO ——COMBO-3
(a) Performance
Walker2d with a
meta-policy.
in (b) Performance in (c) Performance
good Walker2d with a random HalfCheetah with
meta-policy.	offline dataset.
in (d) Average return over
expert different qualities of
meta-policies under ex-
pert dataset for different
choices of α.
Figure 8: Performance evaluation of RAC.
Beside, we also compare the performance of these three algorithms under an expert behavior policy
in Figure 8(c), where a meta-policy usually interferes the policy optimization and drags the learnt
policy away from the expert policy. As expected, RAC can still achieve comparable performance
with COMBO, as a result of safe policy improvement over the behavior policy for suitable parameter
selections.
We examine the impact of α on the performance of RAC under different qualities of the meta-policy
for HalfCheetah with expert data. In this case, the meta-policy is a worse policy compared to the
behavior policy. As shown in Figure 8(d), the performance α = 0.4 is comparable to the case of
α = 1 where the policy improvement step is only regularized based on the behavior policy, and
clearly better than the other two cases.
A.2.2 Evaluation of MerPO
Data collection. We collect the offline dataset for each task by training a stochastic policy network
using SAC (Haarnoja et al., 2018) for that task and rolling out the policies saved at each checkpoint
to collect trajectories. Different checkpoints correspond to different qualities of the offline datasets.
When training with MerPO, we break the trajectories into independent tuples {si , ai , ri , s0i } and
store in a replay buffer. Therefore, the offline dataset for each task does not contain full trajectories
over entire episodes, but merely individual transitions collected by some behavior policy.
Setup. For each testing task, we obtain the task-specific policy through quick adaptation using the
within-task policy optimization method RAC, based on its own offline dataset and the learnt meta-
policy, and evaluate the average return of the adapted policy over 4 random seeds. As shown earlier,
we take α = 0.4 for all experiments about MerPO. In MerPO-Adp, we initialize α with 0.4 and
update with a learning rate of 1e - 4.
Meta-model learning. Similar as in section A.2.1, for each task we quickly adapt from the meta-
model to obtain an ensemble of 7 models and pick the best 5 models based on the validation error.
The neural network used for representing the dynamics model is same with that in section A.2.1.
Meta-policy learning. As in RAC, we represent the task Q-network, the task policy network and
the meta-policy network as a 5-layer feedforward neural network with 300 hidden units, and use
clipped double Q-learning (Fujimoto et al., 2018) for within task Q backup update. For each task,
we also use dual gradient descent to automatically tune both the parameter β for conservative policy
evaluation and the parameter λ for regularized policy improvement:
• Tune β . Before optimizing the Q-network in policy evaluation, we first optimize β by
solving the following problem:
min max β (Es,a~ρ[Q(s, a)] - Es,a~D [Q(s,a)] - T) + 1 Es,a,s∕ ~dJ(Q(s,a) -Bn 承(s,a))2 ].
Q β≥0	2	f
Intuitively, the value of β will be increased to penalty the Q-values for out-of-distribution
state-actions if the difference Es,a~ρ [Q(s, a)] - Es,a~D [Q(s, a)] is larger than some thresh-
old value τ .
13
Published as a conference paper at ICLR 2022
Table 2: Hyperparameters for MerPO.
Hyperparameters	Walker-2D-Params	Half-Cheetah-FWd-BaCk	Ant-Fwd-Back	Point-Robot-Wind
Discount factor	099	0.99	0.99	09
Sample batch size	256	256	256	256
Task batch size	8	2	2	8
Real data ratio	0.5	0.5	0.5	0.5
Model rollout length	1	1	1	1
Inner critic lr	1e-3	1e-3	8e-4	1e-3
Inner actor lr	1e-3	5e-4	5e-4	1e-3
Inner steps	10	10	10	10
Outer critic lr	1e-3	1e-3	1e-3	1e-3
Outer actor lr	1e-3	1e-3	1e-3	1e-3
Meta-q lr	1e-3	1e-3	1e-3	1e-3
Task model lr	1e-4	1e-4	1e-4	1e-4
Meta-model lr	5e-2	5e-2	5e-2	5e-2
Model adaptation steps	25	25	25	25
Optimizer	Adam	Adam	Adam	Adam
Auto-tune λ	True	True	True	True
λ lr	1	1	1	1
λ initial	5	100	100	5
Target divergence	0.05	0.05	0.05	0.05
Auto-tune β	True	True	True	True
log β lr	1e-3	1e-3	1e-3	1e-3
log β initial	0	0	0	0
Q difference threshold	5	10	10	10
Max entropy	True	True	True	True
α	0.4	0.4	0.4	0.4
Testing adaptation steps	100	100	100	100
# training tasks	20	2	2	40
# testing tasks		5			2			2			10	
Tune λ. Similarly, we can optimize λ in policy improvement by solving the following
problem:
π
maxmin Es 〜ρ(s) ,a 〜∏(∙∣s)[Q (s,a)] - λα[D(∏,∏β ) - Dtarget] - λ(l - α)[D(∏,∏c) - Dtarget].
π λ≥0
Intuitively, the value of λ will be increased so as to have stronger regularizations if the
divergence is larger than some threshold Dtarget, and decreased if the divergence is smaller
than Dtarget .
Besides, we also build a meta-Q network Qmeta over the training process as an initialization of
the task Q networks to facilitate the within task policy optimization. At the k-th meta-iteration for
meta-policy update, the meta-Q network is also updated using the average Q-values of current batch
B of training tasks with meta-q learning rate ξq, i.e.,
Qkm+e1ta
—
ta
a
n∈B
Therefore, we initialize the task Q networks and the task policy with the meta-Q network and the
meta-policy, respectively, for within task policy optimization during both meta-training and meta-
testing. The hyperparameters used in evaluation of MerPO are listed in Table 2.
For evaluating the performance improvement in a single new offline task, we use a smaller learning
rate of 8e - 5 for the Q network and the policy network update.
A.2.3 More experiments.
We also evaluate the impact of the utilization extent of the learnt model, by comparing the perfor-
mance of MerPO under different cases of real data ratio, i.e., the ratio of the data from the offline
dataset in the data batch for training. As shown in Figure 9(a), the performance of MerPO can be
further boosted with a more conservative utilization of the model.
To understand how much benefit MerPO can bring for policy learning in unseen offline RL tasks,
we compare the performance of the following cases with respect to the gradient steps taken for
learning in unseen offline RL tasks: (1) Initialize the task policy network with the meta-policy and
run RAC (“With meta”); (2) Run RAC using the meta-policy without network initialization (“With
meta (no init)”); (3) Run RAC with a single regularization based on behavior policy without network
initialization, i.e., α = 1 (“With beha only”); (4) Run COMBO (“No regul.”). As shown in Figure
9(b), “With meta” achieves the best performance and improves significantly over “No regul.” and
“With beha only”, i.e., learning alone without any guidance of meta-policy, which implies that the
14
Published as a conference paper at ICLR 2022
Gradient Steps (xlθ5)
Real data ratio=0.95
Real data ratio=0.5
1.0
(a)	Impact of real data ratio.
(b)	Performance comparison for (c) Training sample efficiency
unseen tasks.	Point-Robot-Wind.
in
Figure 9: Ablation study of MerPO.
------.-----,------.-----.-----1
0.0	1.0	2.0 3.0 4.0	5.0
Gradient Steps (xlθ5)
0.0	0.2	0.4 0.6	0.8	1.0
Gradient Steps (xlθ3)
---- 5 trajectories ---- 20 trajectories
— IO trajectories----- 40 trajectories
(b)	Testing efficiency.
——500 — 300 ——200
(a)	Training efficiency.
Figure 10: Sample efficiency.
learnt meta-policy can efficiently guide the exploration of out-of-distribution state-actions. Without
network initialization, “With meta (no init)” and “With beha only” achieve similar performance
because good offline dataset is considered here. Such a result is also consistent with Figure 8(d).
We evaluate the testing performance of MerPO, by changing sample size of all tasks. Figure 10(a)
shows that the performance of MerPO is stable even if we decrease the number of trajectories for
each task to be around 200. In contrast, the number of trajectories collected in other baselines is
of the order 103. Figure 10(b) illustrates the testing sample efficiency of MerPO, by evaluating the
performance at new offline tasks under different sample sizes. Clearly, a good task-specific policy
can be quickly adapted at a new task even with 5 trajectories (1000 samples) of offline data. We also
evaluate the training sample efficiency of MerPO in Point-Robot-Wind. As shown in Figure 9(c) the
performance of MerPO is stable even if we decrease the number of trajectories for each task to be
around 200.
A.2.4 More comparison between FOCAL and COMBO
Following the setup as in Figure 1, we compare the performance between FOCAL and COMBO
in two more environments: Half-Cheetah-Fwd-Back and Ant-Fwd-Back. As shown in Figure 11,
although FOCAL performs better than COMBO on the task with a bad-quality dataset, it is outper-
formed by COMBO on the task with a good-quality dataset. This further confirms the observation
made in Figure 1.
A.3 Algorithms
We include the details of MerPO in Algorithm 2.
15
Published as a conference paper at ICLR 2022
—COMBO ——FOCAL
(a) Performance comparison in Half-Cheetah-Fwd-
Back.
le2 Ant (Bad dataset)	le2 Ant (Good dataset)
8.0 2.0 4.0 6.0 8.0 10.0 8.0 2.0 4.0 6.0 8.0 10.0
Gradient Steps (xlθ3)	Gradient Steps (xlθ3)
——COMBO - FOCAL
(b) Performance comparison in Ant-Fwd-Back.
Figure 11: FOCAL
vs. COMBO.
Algorithm 2 Regularized policy optimization for model-based offline Meta-RL (MerPO)
1:	Initialize the dynamics, actor and critic for each task, and initialize the meta-model and the
meta-policy;
2:	for k = 1, 2, ... do
3:	for each training task Mn do
4:	Solve the following problem with gradient descent for h steps to compute the dynamics
model Tθk based on the offline dataset Di :
min E(s,a,s0)〜Dn[logTbθn(s0∣s,a)] + η∣∣θn - φm(k)k2；
θn
5:	end for
6:	UPdate φm (k + I) = φm (k) -ξ1 [φm (k) - N Pn=I θk]；
7:	end for
8:	Quickly obtain the estimated dynamics model Tbn for each training task by solving Eq. (4) with
t stePs gradient descent;
9:	for k = 1, 2, ... do
10:	for each training task Mn do
11:	for j = 1, ..., J do
12:	Perform model rollouts with Tbn starting from states in Dn and add model rollouts to
Dn ;
model;
13:	Policy evaluation by recursively solving Eq. (1) using data from Dn ∪ Dmn odel ;
14:	Given the meta-Policy πck , imProve Policy πnk by solving Eq. (7);
15:	end for
16:	end for
17:	Given the learnt Policy πnk for each task, uPdate the meta-Policy πck+1 by solving Eq. (10)
with one steP gradient descent;
18:	end for
B Preliminaries
For ease of exposition, let TM and rM denote the dynamics and reward function of the_Underlying
MDP M, TM and rM∙ denote the dynamics and reward function of the empirical MDP M induced
by the dataset D, and TMc and rMc denote the dynamics and reward function of the learnt MDP Mc.
To prevent any trivial bound with ∞ values, we assume that the cardinality of a state-action pair in
the dataset D, i.e., |D(s, a)|, in the denominator, is non-zero, by setting |D(s, a)| to be a small value
less than 1 when (s, a) ∈/ D.
Following the same line as in (Kumar et al., 2020; Yu et al., 2021b), We make the following standard
assumption on the concentration properties of the reward and dynamics for the empirical MDP M
to characterize the sampling error.
Assumption 1. For any (s, a) ∈ M, the following inequalities hold with probability 1 - δ:
IItM (Sls,a) - TM (Sls,a)ki ≤ ∕CT,δ	； |rM(S,a) - rM | ≤ /Cr,δ
|D (s,a)∣	√∣D(s,a)l
where Cτ,δ and Cr,δ are some constants depending on δ via a ʌ/log(l/b) dependency.
16
Published as a conference paper at ICLR 2022
Based on Assumption 1, we can bound the estimation error induced by the empirical Bellman backup
operator for any (s, a) ∈ M:
BMQk(s,a) -BMQk(s,a)∣
=F(s, a) - rM(s, a) + Y E(Tm(s |s, a) -TM (s |s, a))Eπ(a0 |s0) [Q (s , a )]∣∣
∣	s0	∣
≤lrM (S,a) -rMGa)I + Y E(TM(SlS,a) - TM(Sl* * * * S,⑼Eπ(a0∣s0) [Qk(S0,a0)]
∣ s0
≤ /IC1 * + Y kF(SOBa) - TM (SlS,aO∣∣1∣∣Eπ(a0∣s0)[Qk(S0,aO)]k∞
|D(S, a)|
≤ Cr,δ + YCT,δRmaxlQ - Y)
≤	PDo
_ ((I - Y )Cr,δ∕Rmax + γCT,δ ) Rmax
(I - Y)PID(S,a)|
≤ (Cr,δ/Rmax + CT,δ )Rmax ,	Cr,T,δ Rmax
--(1 - Y)P∣DM∣- = (T-YpDo.
Similarly, we can bound the difference between the Bellman backup induced by the learnt MDP Mc
and the underlying Bellman backup:
∣∣∣BMπcQbk(S, a) -BMπ Qbk(S,a)∣∣∣
S|rMC(S, a) - rM(S,a)| + ∖ _m； Dtv (TMc, TM)
whereDtv(TMc, TM ) is the total-variation distance between TMc and TM .
For any two MDPs, M1 and M2 , with the same state space, action space and discount factor Y,
and a given fraction f ∈ (0, 1), define the f -interpolant MDP Mf as the MDP with dynamics:
TMf = fTM1 + (1 - f)TM2 and reward function: rMf = frM1 + (1 - f)rM2 , which has the
same state space, action space and discount factor with M1 and M2. Let Tπ be the transition matrix
on state-action pairs induced by a stationary policy π, i.e.,
Tπ = T(SO∣S, a)π(aO∣SO).
To prove the main result, we first restate the following lemma from (Yu et al., 2021b) to be used
later.
Lemma 1. For any policy π, its returns in any MDP M, denoted by J(M, π), and in Mf, denoted
by J(M1, M2, f, π), satisfy the following:
J(M, π) - η ≤ J(M1,M2,f,π) ≤ J(M, π) +η
where
η =(1 - Y)2 RmaxDtv (TM2 ,TM ) + 1 Y- Y IEdM [(TM - TM I)QM]|
+
1
f
-Y
1 - f
1 - Y
Es,a~dM [|rMi (S,a) - rM(S,a)|] +
Es,a~dM [|rM2 (S,a)-
rM (S, a)I].
Lemma 1 characterizes the relationship between policy returns in different MDPs in terms of the
corresponding reward difference and dynamics difference.
C Proof of Theorem 1
Let d(S, a) := dπMβ (S, a). In the setting without function approximation, by setting the derivation of
Equation Eq. (1) to 0, we have that
Qk+1(S, a) = BQk(S, a) - ep(S，a) -d(S,a).
df (S, a)
17
Published as a conference paper at ICLR 2022
Denote ν(ρ, f) = EP [ρ(Sda)-d(s,a)] as the expected penalty on the Q-ValUe. It can be shown (YU
et al., 2021b) that ν(ρ, f) ≥ 0 and increases with f, for any ρ and f ∈ (0, 1). Then, RAC optimizes
the return of a policy in a f -interpolant MDP induced by the empirical MDP M and the learnt MDP
Mc, which is regUlarized by both the behavior policy πβ and the meta-policy πc :
max J (M, M, f, π) — β"(' ,f) — λaD(π, ∏β) — λ(1 — a)D(π, ∏c).
π	1-γ
(12)
Denote πo as the solution to the above optimization problem. Based on Lemma 1, we can first
characterize the return of the learnt policy πo in the underlying MDP M in terms of its return in the
f -interpolant MDP:
J(M,∏o) + ηι ≥ J(M, M,f,∏o)	(13)
where
ηι=筌-f RmaxDtv(Tc,TM) + 占IEdM[(TMo -TM)QM]∣
f	1-f
+ ι-γEs,a〜dM [|rM(Ja) - rM(S,a川 + ι-γEs,α〜dM [|rc(Ja) - rMGa)|]
j 2γ (I - f) R ∩ (rp γ YfC fCT,δ Rmaχ ŋ	Γ 1A1 ∕~ΓΛ	ZZ―T 、小 _Ll
≤ (1 - C)2 RmaxDtv (T M，TM) + 一(1 - C )2— Es 〜dMM (S) V -^^ VDCQL(πo,πβ )(S) + 1
Cr,δ
+ 匚Y Es,α~dM
1
P∣DMI
+ 1-CEs,a〜dM [|rM(S，a) - TM(S, a)|]
,ηc.	_
Note that the inequality above holds because the following is true for the empirical MDP M (Kumar
et al., 2020):
IEdM [(TM - TM )QM]∣≤
CCτ,δ Rm ax
for DCQL(π1, π2)(S) :=	aπ1 (a|S)
1-C
(πι SIs)
(n2(aIs)
Es 〜dM(s) I ∣D ∣D(S)∣ qDCQL(π,πβ )(s)+1
-1 .
C.1 SAFE IMPROVEMENT OVER πc
We first show that the learnt policy offers safe improvement over the meta-policy πc . Following the
same line as in Eq. (13), we next bound the return of the meta-policy πc in the underlying MDP M
from above, in terms of its return in the f -interpolant MDP:
_ ,—：—- — ' . , _ , _ ,
J(M, M, f,∏c) ≥ J(Μ,∏c) - η2
where
η2
2C(1 - f)
(1-C)
RmaxDtv(TM, TM) + C fT-SY) max Es-rfM (s) j ∣DD(∣)∣ qDCQL(πc,πβ )(S) + 1
Cr,δ
+ 1 -C Es,a~dM
1
P∣DMI
+ 1~ZC Es,α 〜dM [|rM(S,a) - rM (S,a)|]
≤
2
,η2c.
It follows that
J(M,∏o) + ηc - βν(p—虫-λαD(∏o,∏β) - λ(1 - α)D(∏o,∏c)
1	1-C
≥J(M, M, f,∏o) - β
≥J(M, Mc,f,∏c) - β
V (ρπo,f)
1 - C
ν(ρπc,f)
- λαD(πo, πβ) - λ(1 - α)D(πo, πc)
≥J(M,πc)-η2c-βν
1-C
3。,f)
1 - C
- λαD(πc, πβ)
- λαD(πc, πβ),
18
Published as a conference paper at ICLR 2022
where the second inequality is true because π° is the solution to Eq. (12). This gives us a lower
bound on J(M, πo) in terms of J(M, ∏c):
、	C	C	β 「一 、	，-	「
J(M,∏o) ≥ J(M,∏c)-ηι - η2 + Tl-----[ν(ρ o,f) - V(P C,f)]
1 - Y
+ λαD(∏o,∏β) + λ(1 - α)D(∏o,∏c) - λαDg,∏β)∙
It is clear that ηC and ηC are independent to β and λ. To show the performance improvement of ∏o
over the meta-policy ∏c, it suffices to guarantee that for appropriate choices of β and λ,
β
∆c = λαD(∏o,∏β) + λ(1 - α)D(∏o,∏c) - λαD(∏c,∏β) +  -----[ν(ρ o,f) - V(ρ C, f)] > 0.
1 - Y
To this end, the following lemma first provides an upper bound on ∣ν(ρπ o, f) - V(PC, f )|:
Lemma 2. There exist some positive constants Li and L? such that
∣V (Po ,f) - V (PC ,f )∣ ≤ 2(L1 + L2)Dtv (Po (s, α)∣∣ρπ C (s, a)).
Proof. First, we have that
∣V(ρπo,f) - V(ρπC,f)∣
ρπ0 (s, a) — d(s, a)
.fd(s, a) + (1 - f)ρπo(s, a)_
ρπc(s, a) — d(s, a)
_fd(s, a) + (1 - f)ρπc (s, a)_
—
X Γ π o, ρ ρ"o(s,α) - d(s,a)	_ π c ) ρ"c (S,a) - d(s,a) 一
~ P (Sa) fd(s,a) + (1- f )ρ"o (s,a) - P (S,叼 fd(s,a) + (1- f W (s, a)
(s,a)
≤
X Γ πo∕ ) ρ"o(s,a) - d(s,a)	_ c ρ ρπo(s, a) - d(s,a) 一
~ P (S，a) fd(s,a) + (1- f )ρ"o (s,a) - P (S，a) fd(s,a) + (1- f )ρ"o (s,a)
(s,a)
+
Σ
(s,a)
ρπ0 (s, a) — d(s, a)
fd(s, a) + (1 - f )ρπo(s, a)
ρπc(s, a) — d(s, a)
fd(s, a) + (1 - f )ρπc(s, a)
∑[ρπo (s,a)
(s,a)
ρπ0 (s, a) — d(s, a)
fd(s, a) + (1 - f )ρπo(s, a)
+
E Pπc(S,a)
(s,a)
ρπ0 (s, a) — d(S, a)
fd(S, a) + (1 - f )ρπo (s, a)
PKC (s, a) — d(S, a)
fd(S,a) + (1 - f )ρπc (s, a)
(s,a)
ρπo (s, a) — d(S, a)
fd(S, a) + (1 - f )ρπo (s, a)
+ E ρπc(S,a)
(s,a)
ρπo (s, a) — d(S, a)	ρπc (s, a) — d(S, a)
---:---：---:-----：----:----- ----:---：---:-----：---:-----
fd(S,a) + (1 - f)ρπo(S,a) fd(S,a) + (1 - f)ρπc(S,a)
FirSt CbSfTvp that fcr thp tp∏n I P。(S,a)—d(s,a) I
FirSI, ObServe Ihal forthe IermI fd(s,a) + (i-f )ρ∏o (s,a) ∣,
• If ρπo (s, a) ≥ d(S, a), then
ρπ0 (s, a) — d(S, a)
fd(S, a) + (1 - f )ρπo (s, a)
ρπo (s, a)	ρπo (s, a)	1
'fd(S, a) + (1— f )ρπo (s, a) W (1 - f)ρπo(s,a)	1 - f
• If ρπo (s, a) < d(S, a), then
ρπo (s, a) — d(S, a)
fd(S, a) + (1 - f )ρπo (s, a)
d(S, a)
一fd(S,a) + (1 - f)ρπo(S,a)
≤ ∣ d(S,a) I = 1
Tfd(S, a) ∣	f
19
Published as a conference paper at ICLR 2022
Therefore,
ρπoGa) - d(s,a	11	1 I , L
fd(s,a) + (1- f )ρπo (s,a) ≤ max! f, 1—7 厂 1
Next，for the term I fd(ρ,aο⅞-fdP∏,a)s,a) -	⅛⅛⅞⅛y∣, consider the function g(X)=
fd+(-df )χ for x ∈ [0,1]. Clearly, when d(s, a) = 0,
I	ρπo (s, a) - d(s, a)	ρπc (s, a) - d(s, a)	I
=0.
I7d(s, a) + (1 -7)ρπo(s, a)	7 d(s, a) + (1 -7)ρπc (s, a) I
For any (s, a) that d(s, a) > 0, it can be shown that g(x) is continuous and has bounded gradient,
i.e., ∣Vg(χ)∣ ≤ f2d，L2. Hence, it follows that
I ρπo (s, a) - d(s, a)	ρπc (s, a) - d(s, a)
—
I7d(s, a) + (1 -7)ρπo(s, a)	7 d(s, a) + (1 -7)ρπc (s, a)
≤ L2∣ρπo (s,a) — ρπc (s,a)∣.
Therefore, we can conclude that
∣ν (ρπo7) — V (PKC 7 )|
≤L1 X |pno(s,a) - Pπc(S,a)| + L2 X Pπc(S,a)|pno(s,a) - ρπc(S,a)|
(s,a)	(s,a)
≤(LI + L2) X |pKo (S, a) — pc (s, a)|
s,a
=2(L1 + L2)Dtv(ρπo (s, a) ∣∣ρπc (s, a)).
□
Recall that
ρπo(S,a) = dM(s)πo(a∣s), ρπc(s)。)= dM(s)πc(a∣s),
which denote the marginal state-action distributions by rolling out πo and πc in the learnt model
Mc, respectively. Lemma 2 gives an upper bound on the difference between the expected penalties
induced under πo and πc, with regard to the difference between the marginal state-action distribu-
tions. Next, we need to characterize the relationship between the marginal state-action distribution
difference and the corresponding policy distance, which is captured in the following lemma.
Lemma 3. Let D(∏1∣∣∏2) = maxs Dtv (∏1∣∣∏2) denote the maximum total-variation distance be-
tween two policies π1 and π2. Then, we can have that
Dtv(ρπo(S,a)∣∣ρπc(S,a)) ≤ γ-γ maxDtv(∏o(a∣S)∣∣∏c(a∣S)).
Proof. Note that
∞
Dtv(PnO(S, a)Mρπc(S,a)) ≤ (I-Y) X YtDtv(PnO(S,。)即『(s, a)).
t=0
It then suffices to bound the state-action marginal difference at time t. Since both state-action
marginals here correspond to rolling out πo and πc in the same MDP Mc, based on Lemma B.1
and B.2 in (Janner et al., 2019), we can obtain that
Dtv (PnO (S,。川夕广(S,a))
≤Dtv (ρfo (S)∣∣Pnc (s)) + max Dtv (∏o(a∣S)∣∣∏c(a∣S))
≤t max Dtv (∏o (a∣S)∣∣∏c(a∣S)) + max Dtv (∏o (a∣S)∣∣∏c(a∣S))
=(t + 1)max Dtv (∏o(a∣S)∣∣∏c(a∣S)),
s
which indicates that
∞
Dtv(PnO(S,a)∣∣ρnc(s, a)) ≤ (1 — Y) X Yt(t + 1)maxDtv(∏o(a∣S)∣∣∏c(a∣S))
t=0	s
=1 1 max Dtv (∏o(a∣S)∣∣∏c (a∣S)).
1—Y s
□
20
Published as a conference paper at ICLR 2022
Building on Lemma 2 and Lemma 3, we can show that
∣ν (PFf) - V (ρπc ,f )| ≤ 2(L1 + L2) max Dtv (∏o(a∣s)∣∣∏c(a∣s))
1-γ s
，C max Dtv (∏o(a∣s)∣∣∏c(a∣s)).
s
Let D(∙, ∙) = maxs Dtv(∙∣∣∙). It is clear that for λ ≥ λo where λo >(1,『一ɑ)and α < 11,
β
△c =λαD(∏o,∏β) + λ(1 - α)D(∏o,∏c) - λαD(∏c,∏β) +  ---[ν(ρ o,f) - V(ρ C,f)]
1-γ
=λαD(πo, πβ) + λαD(πo, πc) - λαD(πc, πβ) + λ(1 - 2α)D(πo, πc)
+ ɪ[ν(ρπo,f) - V(PnC,f)]
1-γ
β
≥λ(1 - 2α)D(∏o, πc) + i—— [ν (ρ o ,f) - V (ρ C ,f)]
≥λ(1 - 2α)D(∏o, ∏c) - Cβ D(∏o, ∏c)
1-γ
=(λ - λ0)(1 - 2α)D(πo, πc) + λ0(1 - 2α) - ：：~D D(πo,πc) > 0.
1-γ
In a nutshell, we can conclude that with probability 1 - δ
J(M, πo) ≥	J(M, πc)	-η1-	η2 +(λ - λo)(1 -	2α)D(πo, πc) +	λo(1 -	2α) -	-~~—	D(πo, πc),
`—{z—'、---------------{z------------} L	1 - γj
(a)	(b)	S----------------{--------------}
(c)
where (a) depends on δ but is independent to λ, (b) is positive and increases with λ, and (c) is
positive. This implies that an appropriate choice of λ will make term (b) large enough to counteract
term (a) and lead to the performance improvement over the meta-policy πc :
J(M,πo) ≥J(M,πc)+ξ1
where ξ1 ≥ 0.
C.2 SAFE IMPROVEMENT OVER πβ
Next, we show that the learnt policy πo achieves safe improvement over the behavior policy πβ .
Based on Lemma 1, we have
J(M1, M2, f, πβ) ≥J(M,πβ)-η3
where
% =2(⅛-F RmaxDtv (Tc,TM) + 1fγ IEdMe [(TM - TM )QM ]1
f	1-f
+ 1-^Es,a~dMβ [|rM(s，G- rM(s, a)|] + 1-^Es,a~dMβ [|rC(s，G- rM(s, a)|]
2γ(1 - f )	γ fCT ,δ Rmax
≤ (1 - Y)2 RmaxDtv(IMjM) +	(1 - Y)2 - Es~dMβ
+ 1⅛^Es,a~dMβ [|rc(s，a) - rM(s, a) |]
1
Cr δ
+ L Es,a~dMβ

21
Published as a conference paper at ICLR 2022
Therefore, it follows that
J(M, ∏o) + η1 - βν(p—f) - λaD(∏o,∏β) - λ(1 - a)D(∏o,∏c)
1-γ
≥J(M, M,f, ∏o) - β”(ρ，f)- λαD(∏o,∏β) - λ(1 - α)D(∏o,∏c)
1-γ
≥ J(M, M,f, ∏β) - βν(Pne ,f) - λ(1 - α)D(∏β, ∏c)
1-γ
≥J(M, πβ ) - ηβ - β~^Γ^^,ɪɪ - λ(I - α)D(πβ, πC),
1-γ
which indicates that with probability 1 - δ
J(M, πc) ≥ J(M, πβ) - η1c - η3β +λαD(πo, πβ) + λ(1 - α)D(πo, πc) - λ(1 - α)D(πβ, πc)
+ ɪ [ν (ρπo,f) - ν (Pn ,f)],
1-γ
where η3β is some constant that depends on δ but is independent to β and λ.
To conclude, we can have that with probability 1 - 2δ
J(M, πo) ≥ max{J (M, πc) + ξ1, J(M,πβ) + ξ2}
where
ξι = -η1 -	ηc	+	(λ - λo)(I - 2α)D(πo,	πc)	+	λo(1 - 2α)	-	-~~—	D(πo, πc)	(14)
1-γ
and
ξ2 = -η1c - η3β + λαD(πo, πβ)+λ(1 - α)D(πo, πc) - λ(1 - α)D(πβ, πc)	(15)
+ ɪ [ν(ρπo,f )-ν(ρπβ ,f)].	(16)
1-γ
Moreover, as We noted earlier, ξ1 > 0 for a suitably selected λ and α < 1. For the term V (ρπo, f)-
ν(ρπβ, f) in ξ2 where V(Pn, f) is defined as Eρ∏ [P (Sda)-d(s,a) ], as noted in (YU et al., 2021b),
ν(ρπβ , f) is expected to be smaller than ν(ρπo, f) in practical scenarios, due to the fact that the
dynamics TMc learnt via supervised learning is close to the underlying dynamics TM on the states
visited by the behavior policy ∏β. This directly indicates that dπC(s, a) is close to dπβ(s, a) and ρπβ
is close to d(s, a). In this case, let
=β[ν(ρπo ,f) - V(PKe ,f)]
€	2λ(1 - γ)D(∏o,∏β).
We can show that for a > 1 - e,
β
∆β =λαD(∏o,∏β)	+	λ(1 -	α)D(∏o,∏c)	- λ(1 -	α)D(∏c,∏β) +	γ-γ [ν(ρ o ,f)	- ν(ρ β, f)]
=λαD(πo, πβ)	+	λ(1 -	α)D(πo, πc)	- λ(1 -	α)D(πc, πβ) +	2λD(πo, πβ)
=λ [(2 + α)D(πo, πβ) + (1 - α)D(πo, πc) - (1 - α)D(πc, πβ)]
>λ(1 - α)[D(πo, πβ) + D(πo, πc) - D(πc, πβ)]
>0,
and ∆β increases with λ, which implies that
J(M, πo) ≥ J(M, πβ) + ξ2 = J(M,πβ) - η1c - η3β + ∆β > J(M,πβ)
for an appropriate choice of λ.
22