Published as a conference paper at ICLR 2022
On the Certified Robustness for Ensemble
Models and Beyond
Zhuolin Yang1 * Linyi Li1 * Xiaojun Xu1 Bhavya Kailkhura2 Tao Xie3 Bo Li1
1	University of Illinois Urbana-Champaign
2	Lawrence Livermore National Laboratory 3Peking University
{zhuolin5,linyi2,xiaojun3,lbo}@illinois.edu
kailkhura1@llnl.gov taoxie@pku.edu.cn
* Equal contribution
Ab stract
Recent studies show that deep neural networks (DNN) are vulnerable to adver-
sarial examples, which aim to mislead DNNs by adding perturbations with small
magnitude. To defend against such attacks, both empirical and theoretical defense
approaches have been extensively studied for a single ML model. In this work,
we aim to analyze and provide the certified robustness for ensemble ML models,
together with the sufficient and necessary conditions of robustness for different en-
semble protocols. Although ensemble models are shown more robust than a single
model empirically; surprisingly, we find that in terms of the certified robustness
the standard ensemble models only achieve marginal improvement compared to a
single model. Thus, to explore the conditions that guarantee to provide certifiably
robust ensemble ML models, we first prove that diversified gradient and large confi-
dence margin are sufficient and necessary conditions for certifiably robust ensemble
models under the model-smoothness assumption. We then provide the bounded
model-smoothness analysis based on the proposed Ensemble-before-Smoothing
strategy. We also prove that an ensemble model can always achieve higher certified
robustness than a single base model under mild conditions. Inspired by the theoret-
ical findings, we propose the lightweight Diversity Regularized Training (DRT)
to train certifiably robust ensemble ML models. Extensive experiments show that
our DRT enhanced ensembles can consistently achieve higher certified robustness
than existing single and ensemble ML models, demonstrating the state-of-the-art
certified L2-robustness on MNIST, CIFAR-10, and ImageNet datasets.
1	Introduction
Deep neural networks (DNN) have been widely applied in various applications, such as image
classification (Krizhevsky, 2012; He et al., 2016), face recognition (Sun et al., 2014), and natural
language processing (Vaswani et al., 2017; Devlin et al., 2019). However, it is well-known that DNNs
are vulnerable to adversarial examples (Szegedy et al., 2013; Carlini & Wagner, 2017; Xiao et al.,
2018a;b; Bhattad et al., 2020; Bulusu et al., 2020), and it has raised great concerns especially when
DNNs are deployed in safety-critical applications such as autonomous driving and facial recognition.
To defend against such attacks, several empirical defenses have been proposed (Papernot et al., 2016b;
Madry et al., 2018); however, many of them have been attacked again by strong adaptive attack-
ers (Athalye et al., 2018; Tramer et al., 2020). To end such repeated game between the attackers and
defenders, certified defenses (Wong & Kolter, 2018; Cohen et al., 2019) have been proposed to provide
the robustness guarantees for given ML models, so that no additional attack can break the model under
certain adversarial constraints. For instance, randomized smoothing has been proposed as an effective
defense providing certified robustness (Lecuyer et al., 2019; Cohen et al., 2019; Yang et al., 2020a).
Among different certified robustness approaches (Weng et al., 2018; Xu et al., 2020; Li et al., 2020a;
Zhang et al., 2022), randomized smoothing provides a model-independent way to smooth a given
ML model and achieves state-of-the-art certified robustness on large-scale datasets such as ImageNet.
Currently, all the existing certified defense approaches focus on the robustness of a single ML model.
Given the observations that ensemble ML models are able to bring additional benefits in standard
1
Published as a conference paper at ICLR 2022
learning (Opitz & Maclin, 1999; Rokach, 2010), in this work we aim to ask: Can an ensemble ML
model provide additional benefits in terms of the certified robustness compared with a single model?
If so, what are the sufficient and necessary conditions to guarantee such certified robustness gain?
Empirically, we first find that standard ensemble models only achieve marginally higher certified
robustness by directly appling randomized smoothing: with L2 perturbation radius 1.5, a single
model achieves certified accuracy as 21.9%, while the average aggregation based ensemble of three
models achieves certified accuracy as 24.2% on CIFAR-10 (Table 2). Given such observations, next
we aim to answer: How to improve the certified robustness of ensemble ML models? What types of
conditions are required to improve the certified robustness for ML ensembles?
In particular, from the theoretical perspective, we analyze the standard Weighted Ensemble (WE) and
Max-Margin Ensemble (MME) protocols, and prove the sufficient and necessary conditions for the
certifiably robust ensemble models under model-smoothness assumption. Specifically, we prove that:
(1) an ensemble ML model is more certifiably robust than each single base model; (2) diversified
gradients and large confidence margins of base models are the sufficient and necessary conditions
for the certifiably robust ML ensembles. We show that these two key factors would lead to higher
certified robustness for ML ensembles. We further propose Ensemble-before-Smoothing as the model
smoothing strategy and prove the bounded model-smoothness with such strategy, which realizes our
model-smoothness assumption.
Inspired by our theoretical analysis, we propose
Diversity-Regularized Training (DRT), a lightweight
regularization-based ensemble training approach.
DRT is composed of two simple yet effective and gen-
eral regularizers to promote the diversified gradients
and large confidence margins respectively. DRT can
be easily combined with existing ML approaches for
training smoothed models, such as Gaussian augmen-
tation (Cohen et al., 2019) and adversarial smoothed
Figure 1: Illustration of a robust ensemble.
training (Salman et al., 2019), with negligible training time overhead while achieves significantly
higher certified robustness than state-of-the-art approaches consistently.
We conduct extensive experiments on a wide range of datasets including MNIST, CIFAR-10, and Im-
ageNet. The experimental results show that DRT can achieve significantly higher certified robustness
compared to baselines with similar training cost as training a single model. Furthermore, as DRT
is flexible to integrate any base models, by using the pretrained robust single ML models as base
models, DRT achieves the highest certified robustness so far to our best knowledge. For instance,
on CIFAR-10 under L2 radius 1.5, the DRT-trained ensemble with three base models improves the
certified accuracy from SOTA 24.2% to 30.3%; and under L2 radius 2.0, DRT improves the certified
accuracy from SOTA 16.0% to 20.3%.
Technical Contributions. In this paper, We conduct the first study for the sufficient and necessary
conditions of certifiably robust ML ensembles and propose an efficient training algorithm DRT to
achieve the state-of-the-art certified robustness. We make contributions on both theoretical and
empirical fronts.
•	We provide the necessary and sufficient conditions for robust ensemble ML models including
Weighted Ensemble (WE) and Max-Margin Ensemble (MME) under the model-smoothness
assumption. In particular, We prove that the diversified gradients and large confidence margins
of base models are the sufficient and necessary conditions of certifiably robust ensembles. We
also prove the bounded model-smoothness via proposed Ensemble-before-Smoothing strategy,
Which realizes our model-smoothness assumption.
•	To analyze different ensembles, We prove that When the adversarial transferability among base
models is loW, WE is more robust than MME. We also prove that the ML ensemble is more
robust than a single base model under the model-smoothness assumption.
•	Based on the theoretical analysis of the sufficient and necessary conditions, We propose DRT, a
lightWeight regularization-based training approach that can be easily combined With different
training approaches and ensemble protocols With small training cost overhead.
•	We conduct extensive experiments to evaluate the effectiveness of DRT on various datasets, and
We shoW that to the best of your knoWledge, DRT can achieve the highest certified robustness,
outperforming all existing baselines.
2
Published as a conference paper at ICLR 2022
Related work. DNNs are known vulnerable to adversarial examples (Szegedy et al., 2013).
To defend against such attacks, several empirical defenses have been proposed (Papernot et al.,
2016b; Madry et al., 2018). For ensemble models, existing work mainly focuses on empirical
robustness (Pang et al., 2019; Li et al., 2020b; Cheng et al., 2021) where the robustness is measured
by accuracy under existing attacks and no certified robustness guarantee could be provided or
enhanced; or certify the robustness for a standard weighted ensemble (Zhang et al., 2019; Liu et al.,
2020) using either LP-based (Zhang et al., 2018) verification or randomized smoothing without
considering the model diversity (Liu et al., 2020) to boost their certified robustness. In this paper, we
aim to prove that the diversified gradient and large confidence margin are the sufficient and necessary
conditions for certifiably robust ensemble ML models. Moreover, to our best knowledge, we propose
the first training approach to boost the certified robustness of ensemble ML models.
Randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019) has been proposed to provide
certified robustness for a single ML model. It achieved the state-of-the-art certified robustness on
large-scale dataset such as ImageNet and CIFAR-10 under L2 norm. Several approaches have been
proposed to further improve it by: (1) choosing different smoothing distributions for different Lp
norms (Dvijotham et al., 2019; Zhang et al., 2020; Yang et al., 2020a), and (2) training more robust
smoothed classifiers, using data augmentation (Cohen et al., 2019), unlabeled data (Carmon et al.,
2019), adversarial training (Salman et al., 2019), regularization (Li et al., 2019; Zhai et al., 2019),
and denoising (Salman et al., 2020). In this paper, we compare and propose a suitable smoothing
strategy to improve the certified robustness of ML ensembles.
2	Characterizing ML Ensemble Robustness
In this section, we prove the sufficient and necessary robustness conditions for both general and
smoothed ML ensemble models. Based on these robustness conditions, we discuss the key factors for
improving the certified robustness of an ensemble, compare the robustness of ensemble models with
single models, and outline several findings based on additional theoretical analysis.
2.1	Preliminaries
Notations. Throughout the paper, we consider the classification task with C classes. We first
define the classification scoring function f : Rd → ∆C , which maps the input to a confidence
vector, and f (x)i represents the confidence for the ith class. We mainly focus on the confidence
after normalization, i.e., f(x) ∈ ∆C = {p ∈ RC≥0 : kpk1 = 1} in the probability simplex. To
characterize the confidence margin between two classes, we define fy1/y2 (x) := f (x)y1 - f (x)y2 .
The corresponding prediction F : Rd → [C] is defined by F(x) := arg maxi∈[C] f(x)i. We are
also interested in the runner-up prediction F(2)(x) := argmaXi∈[c]j=F®)f (x)i.
r-Robustness. For brevity, we consider the model’s certified robustness, against the L2-bounded
perturbations as defined below. Our analysis can be generalizable for L1 and L∞ perturbations,
leveraging existing work (Li et al., 2019; Yang et al., 2020a; Levine & Feizi, 2021).
Definition 1 (r-Robustness). For a prediction function F : Rd → [C] and input x0 , if all instance
x ∈ {x0 + δ : kδk2<r} satisfies F(x) = F(x0), we say model F is r-robust (at point x0).
Ensemble Protocols. An ensemble model contains N base models {Fi}iN=1, where Fi(x) and
Fi(2) (x) are their top and runner-up predictions for given input x respectively. The ensemble
prediction is denoted by M : Rd → [C], which is computed based on outputs of base models
following certain ensemble protocols. In this paper, we consider both Weighted Ensemble (WE) and
Maximum Margin Ensemble (MME).
Definition 2 (Weighted Ensemble (WE)). Given N base models {Fi}iN=1, and the weight vector
{wi }iN=1 ∈ R+N, the weighted ensemble MWE : Rd → [C] is defined by
N
MWE(x0) := arg max	wjfj(x0)i.	(1)
i∈[C] j=1
Definition 3 (Max-Margin Ensemble (MME)). Given N base models {Fi}iN=1, for input x0, the
max-margin ensemble model MMME : Rd → [C] is defined by
MMME(x0) := Fc(x0) where c = arg max fi(x0)Fi(x0) - fi(x0)F(2)(	) .	(2)
i∈[N]	Fi (x0)
3
Published as a conference paper at ICLR 2022
The commonly-used WE (Zhang et al., 2019; Liu et al., 2020) sums up the weighted confidence of
base models {Fi}iN=1 with weight vector {wi}iN=1, and predicts the class with the highest weighted
confidence. The standard average ensemble can be viewed as a special case of WE (where all wi ’s
are equal). MME chooses the base model with the largest confidence margin between the top and the
runner-up classes, which is a direct extension from max-margin training (Huang et al., 2008).
Randomized Smoothing. Randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019)
provides certified robustness by constructing a smoothed model from a given model. Formally, let
ε 〜N(0, σ2Id) be a Gaussian random variable, for any given model F : Rd → [C] (can be an
ensemble), we define smoothed confidence function gFε : Rd → ∆C such that
gFε (x)j :=	E	I[F (x + ε) = j] = Pr	(F(x + ε) = j).	(3)
j ε 〜N (0,σ2Id J	J ε 〜N (0,σ2Id)'	，八
Intuitively, gFε (x)j is the probability of base model F’s prediction on the jth class given Gaussian
smoothed input. The smoothed classifier GεF : Rd → [C] outputs the class with highest smoothed
confidence: GεF (x) := arg maxj∈[C] gFε (x)j. Let cA be the predicted class for input x0, i.e.,
cA := GεF (x0). Cohen et al. show that GεF is (σΦ-1 (gFε (x0)cA ))-robust at input x0, i.e., the
certified radius is σΦ-1 (gFε (x0)cA) where Φ-1 is the inverse cumulative distribution function of
standard normal distribution. In practice, we will leverage the smoothing strategy together with
Monte-Carlo sampling to certify ensemble robustness. More details can be found in Appendix A.
2.2	Robustness Conditions for General Ensemble Models
We will first provide sufficient and necessary conditions for robust ensembles under the model-
smoothness assumption.
Definition 4 (β-Smoothness). A differentiable function f : Rd 7→ RC is β-smooth, if for any
xι, x2 ∈ Rd and any output dimension j ∈ [C], k^x1 "X^j-^x2""2)"2 ≤ β∙
The definition of β-smoothness is inherited from optimization theory literature, and it is equivalent
to the curvature bound in certified robustness literature (Singla & Feizi, 2020). β quantifies the
non-linearity of function f, where higher β indicates more rigid functions/models and smaller β
indicates smoother ones. When β = 0 the function/model is linear.
For Weighted Ensemble (WE), we have the following robustness conditions.
Theorem 1 (Gradient and Confidence Margin Conditions for WE Robustness). Given input x0 ∈ Rd
with ground-truth label y0 ∈ [C], and MWE as a WE defined over base models {Fi}iN=1 with weights
{wi}iN=1. MWE(x0) = y0. All base models Fi ’s are β-smooth.
•	(Sufficient Condition) The MWE is r-robust at point x0 if for any yi 6= y0,
N	NN
∣∣X Wj Vxfy0/yi (XO)IL ≤ r χ Wj fy0∕yi (XO) - βr X Wj,	⑷
j=1	2	j=1	j=1
•	(Necessary Condition) If MWE is r-robust at point x0, for any yi 6= y0,
N	NN
∣∣XWjVxfyO/yi(xo)∣∣ ≤ rXWjfyo/yi(XO)+βrXwj.	⑸
j=1	2	j=1	j=1
The proof follows from Taylor expansion at x0 and we leave the detailed proof in Appendix B.2.
When it comes to Max-Margin Ensemble (MME), the derivation of robust conditions is more involved.
In Theorem 3 (Appendix B.1.1) we derive the robustness conditions for MME composed of two base
models. The robustness conditions have highly similar forms as those for WE in Theorem 1. Thus,
for brevity, we focus on discussing Theorem 1 for WE hereinafter and similar conclusions can be
drawn for MME (details are in Appendix B.1.1).
To analyze Theorem 1, we define Ensemble Robustness Indicator (ERI) as such:
N	1N
Iyi := ∣∣X WjVxfy0/yi (XO)∣∣2∕ kwkι- rkɪ X W fy0/yi (XO).	⑹
j=1	2	r kwk1 j=1
ERI appears in both sufficient (Equation (4)) and necessary (Equation (5)) conditions. In both
conditions, smaller ERI means more certifiably robust ensemble. Note that we can analyze the
4
Published as a conference paper at ICLR 2022
robustness under different attack radius r by directly varying r in Equations (4) and (5). When r
becomes larger, the gap between the RHS of two inequalities (2βr PjN=1 wj ) also becomes larger,
and thus it becomes harder to determine robustness via Theorem 1. This is because the first-order
condition implied by Theorem 1 becomes coarse when r is large. However, due to bounded β as we
will show, the training approach motivated by the theorem still empirically works well under large r.
Diversified Gradients. The core of first term in ERI is the magnitude of the vector sum
of gradients: ∣∣ PN=I WjPXfj(Oyi(x0)k2. According to the law of cosines: ∣∣a + b∣∣2 =
√kak2 + kbk2 +2kak2kbk2cosha, bi, to reduce this term, we could either reduce the base models’
gradient magnitude or diversify their gradients (in terms of cosine similarity). Since simply reduc-
ing base models’ gradient magnitude would hurt model expressivity (Huster et al., 2018), during
regularization the main functionality of this term would be promoting diversified gradients.
Large Confidence Margins. The core of second term in ERI is the confidence margin:
PjN=1 wjfjy0/yi (x0). Due to the negative sign of second term in ERI, we need to increase this
term, i.e., we need to increase confidence margins to achieve higher ensemble robustness.
In summary, the diversified gradients and large confidence margins are the sufficient and necessary
conditions for high certified robustness of ensembles. In Section 3, we will directly regularize these
two key factors to promote certified robustness of ensembles.
Impact of Model-Smoothness Bound β. From Theorem 1, we observe that: (1) if minyi 6=y0 Iyi ≤
-βr, MWE is guaranteed to be r-robust (sufficient condition); and (2) if minyi 6=y0 Iyi > βr, MWE
cannot be r-robust (necessary condition). However, if minyi6=y0 Iyi ∈ (-βr, βr], we only know
MWE is possibly r-robust. As a result, the model-smoothness bound β decides the correlation
strength between minyi 6=y0 Iyi and the robustness of MWE : if β becomes larger, minyi 6=y0 Iyi is
more likely to fall in (-βr, βr], inducing an undetermined robustness status from Theorem 1, vice
versa. Specifically, when β = 0, i.e., all base models are linear, the gap is closed and we can always
certify the robustness of MWE via comparing minyi 6=y0 with 0. Similar observations can be drawn
for MME. Therefore, to strengthen the correlation between Iyi and ensemble robustness, we would
need model-smoothness bound β to be small.
2.3	Robustness Conditions for Smoothed Ensemble Models
Typically neural networks are nonsmooth or admit only coarse smoothness bounds (Sinha et al.,
2018), i.e., β is large. Therefore, applying Theorem 1 for normal nonsmooth models would lead
to near-zero certified radius. Therefore, we propose soft smoothing to enforce the smoothness of
base models. However, with the soft smoothed base models, directly applying Theorem 1 to certify
robustness is still practically challenging, since the LHS of Equations (4) and (5) involves gradient
of the soft smoothed confidence. A precise computation of such gradient requires high-confidence
estimation of high-dimensional vectors via sampling, which requires linear number of samples with
respect to input dimension (Mohapatra et al., 2020; Salman et al., 2019) and is thus too expensive in
practice. To solve this issue, we then propose Ensemble-before-Smoothing as the practical smoothing
protocol, which serves as an approximation of soft smoothing, so as to leverage the randomized
smoothing based techniques for certification.
Soft Smoothing. To impose base models’ smoothness, we now introduce soft smoothing (Kumar
et al., 2020), which applies randomized smoothing over the confidence scores. Given base model’s
confidence function f : Rd → ∆C (see Section 2.1), we define soft smoothed confidence by
gf : x → Eεf (X + ε). Note that soft smoothed confidence is different from smoothed confidence gF
defined in Equation (3). We consider soft smoothing instead of classical smoothing in Equation (3)
since soft smoothing reveals differentiable and thus practically regularizable training objectives. The
following theorem shows the smoothness bound for gf.
Theorem 2 (Model-Smoothness Upper Bound for gf). Let ε 〜N(0, σ2Id) be a Gaussian random
variable, then the soft smoothed confidence function gf is (2∕σ2)-smooth.
We defer the proof to Appendix B.4. The proof views the Gaussian smoothing as the Weierstrass
transform (Weierstrass, 1885) of a function from Rd to [0, 1]C, leverages the symmetry property, and
bounds the absolute value of diagonal elements of the Hessian matrix. Note that a Lipschitz constant
,2∕(∏σ2) is derived for smoothed confidence in previous work (Salman et al., 2019, Lemma 1),
5
Published as a conference paper at ICLR 2022
which characterizes only the first-order smoothness property; while our bound in addition shows the
second-order smoothness property. In Appendix B.4, we further show that our smoothness bound in
Theorem 2 is tight up to a constant factor.
Now, We apply WE and MME protocols with these sof[smoothness Confidence {gε(x0)}N=1 as base
models, confidence scores, and obtain soft ensemble GMWE and GMMME respectively. Since each
gɛ is (2∕σ2)-smooth, take WE as an example, we can study the ensemble robustness with Theorem 1.
We state the full statement in Corollary 2 (and in Corollary 3 for MME) in Appendix B.1.3. From the
corollary, we observe that the corresponding ERI for the soft smoothed WE can be written as
Iyi := ∣∣EεVχ X Wjfjwyi(X0 + ε)∣∣2.kwk1 - rɪEε X Wjfy0∕yi(X0 + ε).	⑺
We have following observations: (1) unlike for standard models with unbounded β, for the smoothed
ensemble models, this ERI (Equation (7)) would have guaranteed correlation with the model robust-
ness since β = Θ(1∕σ2) is bounded and can be controlled by tuning σ for smoothing. (2) we can
still control ERI by diversifying gradients and ensuring large confidence margins as discussed in
Section 2.2, but need to compute on the noise augmented input x0 + ε instead of original input x0 .
Towards Practical Certification. As outlined at the beginning of this subsection, even with
smoothed base models, certifying robustness using Theorem 1 is practically difficult. Therefore,
we introduce Ensemble-before-Smoothing strategy as below to construct GεM and GεM	as
approximations of soft ensemble GMWE and GMMME respectively.
Definition 5 (Ensemble-before-Smoothing (EBS)). Let M be an ensemble model over base models
{Fi}iN=1 and ε be a random variable. The EBS strategy construct smoothed classifier GεM : Rd → [C]
that picks the class with highest smoothed confidence ofM: GεM(x) := arg maxj ∈[C] gMε (x)j.
Here M could be either MWE or MMME . EBS aims to approximate the soft smoothed ensemble.
PjN 1 wj fj
Formally, use WE as an example, we let ∕mwe :=	j=W ∣∣-to be WE ensemble S confidence, then
ε	PjN=1 Wj (gIfεj )i	ε
gMwe (x)i = EεI[MWE(X + ε) = i] ≈ EεfMwE (X + ε)i = ——N-------j— = IMWE (x)i	(8)
j=1 Wj
where LHS is the smoothed confidence of EBS ensemble and RHS is the soft smoothed ensemble’s
confidence. Such approximation is also adopted in existing work (Salman et al., 2019; Zhai et al.,
2019; Kumar et al., 2020) and shown effective and useful. Therefore, our robustness analysis of soft
smoothed ensemble still applies with EBS and we can control ERI in Equation (7) to improve the
certified robustness of EBS ensemble. For EBS ensemble, we can leverage randomized smoothing
based techniques to compute the robustness certification (see Proposition C.1 in Appendix C).
2.4	Additional Properties of ML Ensembles
Comparison between Ensemble and Single-Model Robustness. In Appendix B.1, we show
Corollary 1, a corollary of Theorem 1, which indicates that when the base models are smooth
enough, both WE and MME ensemble models are more certifiably robust than the base models. This
aligns with our empirical observations (see Table 1 and Table 2), though without advanced training
approaches such as DRT, the improvement of robustness brought by ensemble itself is marginal. In
Appendix B.1, we also show larger number of base models N can lead to better certified robustness.
Comparison between WE and MME Robustness. Since in actual computing, the certified radius
of a smoothed model is directly correlated with the probability of correct prediction under smoothed
input (see Equation (11) in Appendix A), we study the robustness of both WE and MME along with
single models from the statistical robustness perspective in Appendix D. From the study, we have the
following theoretical observations verified by numerical experiments: (1) MME is more robust when
the adversarial transferability is high; while WE is more robust when the adversarial transferability is
low. (2) If we further assume that fi(x0 + ε)y0 follows marginally uniform distribution, when the
number of base models N is sufficiently large, MME is always more certifiably robust. Appendix D.5
entails the numerical evaluations that verify our theoretical conclusions.
3	DIVERSITY-REGULARIZED TRAINING
Inspired by the above key factors in the sufficient and necessary conditions for the certifiably robust
ensembles, we propose the Diversity-Regularized Training (DRT). In particular, let x0 be a training
sample, DRT contains the following two regularization terms in the objective function to minimize:
6
Published as a conference paper at ICLR 2022
•	Gradient Diversity Loss (GD Loss): LGD(xο)ij = ∣∣Vχ fy0/y(2)(X0)+Vx/y0/y()(x0)∣∣2. (9)
•	Confidence Margin Loss (CM Loss): LCM(x0)ij = fiyi()/y0 (x0) + fjyj /y0 (x0).	(10)
In Equations (9) and (10), y0 is the ground-truth label of x0, and yi(2) (or yj(2)) is the runner-up class
of base model Fi (or Fj). Intuitively, for each model pair (Fi, Fj) where i, j ∈ [N] and i 6= j, the
GD loss promotes the diversity of gradients between the base model Fi and Fj. Note that the gradient
computed here is actually the gradient difference between different labels. As our theorem reveals, it
is the gradient difference between different labels instead of pure gradient itself that matters, which
improves existing understanding of gradient diversity (Pang et al., 2019; Demontis et al., 2019).
Specifically, the GD loss encourages both large gradient diversity and small base models’ gradient
magnitude in a naturally balanced way, and encodes the interplay between gradient magnitude and
direction diversity. In contrast, solely regularizing the base models’ gradient would hurt the model’s
benign accuracy, and solely regularizing gradient diversity is hard to realize due to the boundedness
of cosine similarity. The CM loss encourages the large margin between the true and runner-up classes
for base models. Both regularization terms are directly motivated by theoretical analysis in Section 2.
For each input xο with ground truth yο, We use xο + ε with ε 〜 N(0, σ2Id) as training input for
each base model (i.e., Gaussian augmentation). We call two base models Fi , Fj a valid model pair
at (x0, y0) if both Fi(x0 + ε) and Fj (x0 + ε) equal to y0. For every valid model pair, we apply DRT:
GD Loss and CM Loss with ρ1 and ρ2 as the weight hyperparameters as below.
Ltrain =	Lstd (x0 + ε, y0 )i + ρ1	LGD (x0 + ε)ij + ρ2	LCM (x0 + ε)ij .
i∈[N]	i,j∈[N],i6=j	i,j∈[N],i6=j
(Fi, Fj ) is valid	(Fi, Fj ) is valid
The standard training loss Lstd (x0 + ε, y0)i of each base model Fi is either cross-entropy loss
(Cohen et al., 2019), or adversarial training loss (Salman et al., 2019). This standard training loss will
help to produce sufficient valid model pairs with high benign accuracy for robustness regularization.
Specifically, as discussed in Section 2.3, we compute LGD and LCM on the noise augmented inputs
(x0 + ε) instead of x0 to improve the certified robustness for the smoothed ensemble.
Discussion. To our best knowledge, this is the first training approach that is able to promote the
certified robustness of ML ensembles, while existing work either only provide empirical robustness
without guarantees (Pang et al., 2019; Kariyappa & Qureshi, 2019; Yang et al., 2020b; 2021), or tries
to only optimize the weights of Weighted Ensemble (Zhang et al., 2019; Liu et al., 2020). We should
notice that, though concepts similar with the gradient diversity have been explored in empirically
robust ensemble training (e.g., ADP (Pang et al., 2019), GAL (Kariyappa & Qureshi, 2019)), directly
applying these regularizers cannot train models with high certified robustness due to the lack of
theoretical guarantees in their design. We indicate this through ablation studies in Appendix G.4. For
the design of DRT, we also find that there exist some variations. We analyze them and show that the
current design is usually better based on the analysis in Appendix E. Our approach is generalizable
for other Lp-bounded perturbations such as L1 and L∞ leveraging existing work (Li et al., 2019;
Lecuyer et al., 2019; Yang et al., 2020a; Levine & Feizi, 2021).
4	Experimental Evaluation
To make a thorough comparison with existing certified robustness approaches, we evaluate DRT
on different datasets including MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky, 2012), and
ImageNet (Deng et al., 2009), based on both MME and WE protocols. Overall, we show that the DRT
enabled ensemble outperforms all baselines in terms of certified robustness under different settings.
4.1	Experimental Setup
Baselines. We consider the following state-of-the-art baselines for certified robustness: Gaussian
smoothing (Cohen et al., 2019), SmoothAdv (Salman et al., 2019), MACER (Zhai et al., 2019),
Stability (Li et al., 2019), and SWEEN (Liu et al., 2020). Detail description of these baselines can be
found in Appendix F. We follow the configurations of baselines, and compare DRT-based ensemble
with Gaussian Smoothing, SmoothAdv, and MACER on all datasets, and in addition compare it
with other baselines on MNIST and CIFAR-10 considering the training efficiency. There are other
baselines, e.g., (Jeong & Shin, 2020). However, SmoothAdv performs consistently better across
different datasets, so we mainly consider SmoothAdv as our strong baseline.
7
Published as a conference paper at ICLR 2022
Train Ensemble Models Aggregate with WE or
with GD loss & CM loss MME protocol
Smooth via EBS
Strategy
Eq. (9)	Eq. (1) and (2)	Definition 5
-----------y------------4---------------------------------------
Robust EnsembIeTraining	Smoothed Ensemble Construction
Compute Certified
Radius
SeC 2.1 and Appendix A
Robustness Certification
Figure 2: Pipeline for DRT-based ensemble.
Table 1: Certified accuracy under different radii on MNIST dataset. The grey rows present the performance of
the proposed DRT approach. The brackets show the base models we use.
Radius r	∣ 0.00 ∣ 0.25 ∣ 0.50 ∣ 0.75 ∣ 1.00 ∣ 1.25 ∣ 1.50 ∣ 1.75 ∣ 2.00 ∣ 2.25 ∣ 2.50
Gaussian (Cohen et al., 2019)	99.1	97.9	96.6	94.7	90.0	83.0	68.2	46.6	33.0	20.5	11.5
SmoothAdv (Salman et al., 2019)	99.1	98.4	97.0	96.3	93.0	87.7	80.2	66.3	43.2	34.3	24.0
MACER (Zhai et al., 2019)	99.2	98.5	97.4	94.6	90.2	83.5	72.4	54.4	36.6	26.4	16.5
Stability (Li et al., 2019)	99.3	98.6	97.1	93.8	90.7	83.2	69.2	46.8	33.1	20.0	11.2
SWEEN (Gaussian) (Liu et al., 2020)	99.2	98.4	96.9	94.9	90.5	84.4	71.1	48.9	35.3	23.7	12.8
SWEEN (SmoothAdv) (Liu et al., 2020)	99.2	98.2	97.4	96.3	93.4	88.1	81.0	67.2	44.5	34.9	25.0
MME (GaUSSian)	99.2	98.4	96.8	94.9	90.5	84.3	69.8	48.8	34.7	23.4	12.7
DRT + MME (Gaussian)	99.5	98.6	97.5	95.5	92.6	86.8	76.5	60.2	43.9	36.0	29.1
MME (SmoothAdv)	99.2	98.2	97.3	96.4	93.2	88.1	80.6	67.9	44.8	35.0	25.2
DRT + MME (SmoothAdv)		99.2	98.4	97.6	96.7	93.1	88.5	83.2	68.9	48.2	40.3	34.7
WE (Gaussian)	99.2	98.4	96.9	94.9	90.6	84.5	70.4	49.0	35.2	23.7	12.9
DRT + WE (Gaussian)	99.5	98.6	97.4	95.6	92.6	86.7	76.7	60.2	43.9	35.8	29.0
WE (SmoothAdv)	99.1	98.2	97.4	96.4	93.4	88.2	81.1	67.9	44.7	35.2	24.9
DRT + WE (SmoothAdv)	99.1	98.4	97.6	96.7	93.4	88.5	83.3	69.6	48.3	40.2	34.8
Models. For base models in our ensemble, we follow the configurations used in baselines: LeNet (Le-
Cun et al., 1998), ResNet-110, and ResNet-50 (He et al., 2016) for MNIST, CIFAR-10, and ImageNet
datasets respectively. Throughout the experiments, we use N = 3 base models to construct the
ensemble for demonstration. We expect more base models would yield higher ensemble robustness.
Training Details. We follow Section 3 to train the base models. We combine DRT with Gaussian
smoothing and SmoothAdv (i.e., instantiating Lstd by either cross-entropy loss (Cohen et al., 2019;
Yang et al., 2020a) or adversarial training loss (Salman et al., 2019)). We leave training details along
with hyperparametes in Appendix F.
Pipeline. After the base models are trained with DRT, we aggregate them to form the ensemble M,
using either WE or MME protocol (see Definitions 2 and 3). If we use WE, to filter out the effect of
different weights, we adopt the average ensemble where all weights are equal. We also studied how
optimizing weights can further improve the certified robustness in Appendix G.3. Then, we leverage
Ensemble-before-Smoothing strategy to form a smoothed ensemble (see Definition 5). Finally, we
compute the certified robustness for the smoothed ensemble based on Monte-Carlo sampling with
high-confidence (99.9%). The training pipeline is shown in Figure 2.
Evaluation Metric. We report the standard certified accuracy under different L2 radii r’s as our
evaluation metric following existing work (Cohen et al., 2019; Yang et al., 2020b; Zhai et al., 2019;
Jeong & Shin, 2020). More evaluation details are in Appendix F.
4.2 Experimental Results
Here we consider ensemble models consisting of three base models. We show that 1) DRT-based
ensembles outperform the SOTA baselines significantly especially under large perturbation radii;
2) smoothed ensembles are always more certifiably robust than each base model (Corollary 1 in
Appendix B.1); 3) applying DRT for either MME or WE ensemble protocols achieves similar and
consistent improvements on certified robustness.
Certified Robustness of DRT with Different Ensemble Protocols. The evaluation results on
MNIST, CIFAR-10, ImageNet are shown in Tables 1, 2, 3 respectively. It is clear that though the
certified accuracy of a single model can be improved by directly applying either MME or WE
ensemble training (proved in Corollary 1), such improvements are usually negligible (usually less
than 2%). In contrast, in all tables we find DRT provides significant gains on certified robustness for
both MME and WE (up to over 16% as Table 1 shows).
From Tables 1 and 2 on MNIST and CIFAR-10, we find that compared with all baselines, DRT-based
ensemble achieves the highest robust accuracy, and the performance gap is more pronounced on large
radii (over 8% for r = 2.50 on MNIST and 6% for r = 1.50 on CIFAR-10). We also demonstrate
the scalability of DRT by training on ImageNet, and Table 3 shows that DRT achieves the highest
certified robustness under large radii. It is clear that DRT can be easily combined with existing
training approaches (e.g. Gaussian smoothing or SmoothAdv), boost their certified robustness, and
set the state-of-the-art results to the best of our knowledge.
8
Published as a conference paper at ICLR 2022
Table 2: Certified accuracy under different radii on CIFAR-10 dataset. The grey rows present the performance of the proposed DRT approach. The brackets show the base models we use.									
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Gaussian (Cohen et al., 2019)	78.9	64.4	47.4	33.7	23.1	18.3	13.6	10.5	7.3
SmoothAdv (Salman et al., 2019)	68.9	61.0	54.4	45.7	34.8	28.5	21.9	18.2	15.7
MACER (Zhai et al., 2019)	79.5	68.8	55.6	42.3	35.0	27.5	23.4	20.4	17.5
Stability (Li et al., 2019)	72.4	58.2	43.4	27.5	23.9	16.0	15.6	11.4	7.8
SWEEN (Gaussian) (Liu et al., 2020)	81.2	68.7	54.4	38.1	28.3	19.6	15.2	11.5	8.6
SWEEN (SmoothAdv) (Liu et al., 2020)	69.5	62.3	55.0	46.2	35.2	29.5	22.4	19.3	16.6
MME (Gaussian)	80.8	68.2	53.4	38.4	29.0	19.6	15.6	11.6	8.8
DRT + MME (Gaussian)	81.4	70.4	57.8	43.8	34.4	29.6	24.9	20.9	16.6
MME (SmoothAdv)	71.4	64.5	57.6	48.4	36.2	29.8	23.9	19.5	16.2
DRT + MME (SmoothAdv)	72.6	67.2	60.2	50.4	39.4	35.8	30.4	24.0	20.1
WE (Gaussian)	80.8	68.4	53.6	38.4	29.2	19.7	15.9	11.8	8.9
DRT + WE (Gaussian)	81.5	70.4	57.9	44.0	34.2	29.6	24.9	20.8	16.4
WE (SmoothAdv)	71.8	64.6	57.8	48.5	36.2	29.6	24.2	19.6	16.0
DRT + WE (SmoothAdv)	72.6	67.0	60.2	50.5	39.5	36.0	30.3	24.1	20.3
To evaluate the computational cost of DRT, we analyze the theoretical complexity in Appendix E
and compare the efficiency of different methods in practice in Appendices F.1 and F.2. In particular,
we show that DRT with Gaussian Smoothing base models even achieves around two times speedup
compared with SmoothAdv with comparable or even higher certified robustness, since DRT does
not require adversarial training. More discussions about hyper-parameters settings for DRT can
be found in Appendix F. In Appendix G.4, we also show that our proposed DRT approach could
achieve 6% 〜10% higher certified accuracy compared to adapted ADP (Pang et al., 2019) and
GAL (Kariyappa & Qureshi, 2019) training on large radii for both MNIST and CIFAR-10 datasets.
Certified Accuracy with Different Perturbation Radius. We visualize the trend of certified accuracy along with dif- ferent perturbation radii in Figure 3. For	Table 3: Certified accuracy under different radii on ImageNet dataset. The grey rows present the performance of the proposed DRT approach. The brackets show the base models we use.							
	Radius r	0.00	0.50	1.00	1.50	2.00	2.50	3.00
each radius r, we present the best certi- fied accuracy among different smoothing parameters σ ∈ {0.25, 0.50, 1.00}. We notice that while simply applying MME	Gaussian (Cohen et al., 2019) SmoothAdv (Salman et al., 2019) MACER (Zhai et al., 2019) SWEEN (Gaussian) (Liu et al., 2020) SWEEN (SmoothAdv) (Liu et al., 2020)	57.2 54.6 68.0 58.4 55.2	46.2 49.0 57.0 47.0 50.0	37.0 43.8 43.0 37.4 44.2	29.2 37.2 31.0 29.8 37.8	19.6 27.0 25.0 20.2 27.6	15.2 25.2 18.0 15.8 26.6	12.4 20.4 14.0 12.8 21.6
	MME (Gaussian)	58.0	47.2	38.8	31.2	21.4	16.4	14.2
or WE protocol could slightly improve	DRT + MME (Gaussian)	52.2	46.8	42.4	34.2	24.0	19.6	18.0
	MME (SmoothAdv)	55.0	50.2	44.2	38.6	27.4	26.4	21.6
the certified accuracy, DRT could signif- icantly boost the certified accuracy un-	DRT + MME (SmoothAdv)	49.8	46.8	44.4	39.8	30.2	28.2	23.4
	WE (Gaussian)	58.2	47.2	38.6	31.2	21.6	17.0	14.4
	DRT + WE (Gaussian)	52.2	46.8	41.8	33.6	24.2	19.8	18.4
der different radii. We also present the	WE (SmoothAdv)	55.2	50.2	44.4	38.6	28.2	26.2	22.0
	DRT + WE (SmoothAdv)	49.8	46.6	44.4	38.8	30.4	29.0	23.2
trends of different smoothing parameters								
separately in Appendix F which lead to
similar conclusions.
Effects of GD and CM Losses in DRT.
To explore the effects of individual Gra-
dient Diversity and Confidence Margin
Losses in DRT, we set ρ1 or ρ2 to 0 sepa-
rately and tune the other for evaluation on
MNIST and CIFAR-10. The full results
are shown in Appendix G.1. We observe
that both GD and CM losses have positive
effects on improving the certified accu-
racy, and GD plays a major role on larger
radii. By combining these two regular-
ization losses as DRT does, the ensemble
(a) MNIST	(b) CIFAR-10
Figure 3: Certified accuracy for ML ensembles with Gaus-
sian smoothed base models, under smoothing parameter σ ∈
{0.25, 0.50, 1.00} on (Left) MNIST; (Right) CIFAR-10.
model achieves the highest certified accuracy under all radii.
5 Conclusion
In this paper, we explored and characterized the robustness conditions for certifiably robust ensemble
ML models theoretically, and proposed DRT for training a robust ensemble. Our analysis provided the
justification of the regularization-based training approach DRT. Extensive experiments showed that
DRT-enhanced ensembles achieve the highest certified robustness compared with existing baselines.
9
Published as a conference paper at ICLR 2022
Ethics Statement. In this paper, we characterized the robustness conditions for certifying ML en-
semble robustness. Based on the analysis, we propose DRT to train a certifiably robust ensemble. On
the one hand, the training approach boosts the certified robustness of ML ensemble, thus significantly
reducing the security vulnerabilities of ML ensemble. On the other hand, the trained ML ensemble
can only guarantee its robustness under specific conditions of the attack. Specifically, we evaluate
the trained ML ensemble on the held-out test set and constrain the attack to be within predefined L2
distance from the original input. We cannot provide robustness guarantee for all possible real-world
inputs. Therefore, users should be aware of such limitations of DRT-trained ensembles, and should
not blindly rely on the ensembles when the attack can cause large deviations measured by L2 distance.
As a result, we encourage researchers to understand the potential risks, and evaluate whether our
attack constraints align with their usage scenarios when applying our DRT approach to real-world
applications. We do not expect any ethics issues raised by our work.
Reproducibility Statement. All the theorem statements are substantiated with rigorous proofs
in Appendices B to D. In Appendix F, we list the details and hyperparameters for reproducing
all experimental results. Our evaluation is conducted on commonly accessible MNIST, CIFAR-
10, and ImageNet datasets. Finally, we upload the source code as the supplementary material for
reproducibility purpose.
Acknowledgements
This work was performed under the auspices of the U.S. Department of Energy by the Lawrence
Livermore National Laboratory under Contract No. DE-AC52-07NA27344 and LLNL LDRD
Program Project No. 20-ER-014. This work is partially supported by the NSF grant No.1910100,
NSF CNS 20-46726 CAR, Alfred P. Sloan Fellowship, and Amazon Research Award.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning,pp. 274-283, 2018.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David A Forsyth. Unrestricted adversarial
examples via semantic manipulation. In International Conference on Learning Representations,
2020.
Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. Anomalous
example detection in deep learning: A survey. IEEE Access, 8:132330-132347, 2020.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11192-11203, 2019.
Hao Cheng, Kaidi Xu, Chenan Wang, Xue Lin, Bhavya Kailkhura, and Ryan Goldhahn. Mixture
of robust experts (more): A flexible defense against multiple perturbations. arXiv preprint
arXiv:2104.10586, 2021.
Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case
of the binomial. Biometrika, 26(4):404-413, 1934.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320, 2019.
Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea,
Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks transfer? explaining transferability
of evasion and poisoning attacks. In 28th {USENIX} Security Symposium ({USENIX} Security
19), pp. 321-338, 2019.
10
Published as a conference paper at ICLR 2022
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT (1), 2019.
Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy,
Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed
classifiers using f-divergences. In International Conference on Learning Representations, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Kaizhu Huang, Haiqin Yang, Irwin King, and Michael R Lyu. Maxi-min margin machine: learning
large margin classifiers locally and globally. IEEE Transactions on Neural Networks, 19(2):
260-272, 2008.
Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the lipschitz constant as a
defense against adversarial examples. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 16-29. Springer, 2018.
Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed
classifiers. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 10558-10570. Curran Associates, Inc.,
2020.
Sanjay Kariyappa and Moinuddin K Qureshi. Improving adversarial robustness of ensembles with
diversity training. arXiv preprint arXiv:1901.09981, 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05
2012.
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via
randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5165-5177. Curran
Associates, Inc., 2020.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Alexander J Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certified robustness.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 6254-6264.
PMLR, 18-24 Jul 2021.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems, pp. 9464-9474, 2019.
Linyi Li, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks. arXiv preprint
arXiv:2009.04131, 2020a.
Yueqiao Li, Hang Su, Jun Zhu, and Jun Zhou. Boosting the robustness of capsule networks with
diverse ensemble. In 2020 10th International Conference on Information Science and Technology
(ICIST), pp. 247-251. IEEE, 2020b.
11
Published as a conference paper at ICLR 2022
Chizhou Liu, Yunzhen Feng, Ranran Wang, and Bin Dong. Enhancing certified robustness of
smoothed classifiers via weighted model ensembling. arXiv preprint arXiv:2005.09363, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-
order certification for randomized smoothing. Advances in Neural Information Processing Systems,
33, 2020.
Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efficient tests of statistical
hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers ofa Mathematical or Physical Character, 231(694-706):289-337,1933.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
artificial intelligence research, 11:169-198, 1999.
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via
promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970-4979.
PMLR, 2019.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016b.
Lior Rokach. Ensemble-based classifiers. Artificial intelligence review, 33(1):1-39, 2010.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems, pp. 11292-11303, 2019.
Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Black-box smoothing: A
provable defense for pretrained classifiers. arXiv preprint arXiv:2003.01908, 2020.
Saeed Saremi and Rupesh Srivastava. Provable robust classification via learned smoothed densities.
arXiv preprint arXiv:2005.04504, 2020.
Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks. In
International Conference on Machine Learning, 2020.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predicting 10,000
classes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
1891-1898, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Karl Weierstrass. Uber die analytische darstellbarkeit sogenannter willkurlicher functionen einer
reellen VerdnderliChen. Sitzungsberichte der Koniglich Preufiischen Akademie der Wissenschaften
zu Berlin, 2:633-639, 1885.
12
Published as a conference paper at ICLR 2022
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning, pp. 5276-5285, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295, 2018.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018a.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. In International Conference on Learning Representations, 2018b. URL
https://openreview.net/forum?id=HyydRMZC-.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified
robustness and beyond. In Advances in Neural Information Processing Systems, 2020.
Greg Yang, Tony Duan, Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, 2020a.
Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew
Touchet, Wesley Wilkes, Heath Berry, and Hai Li. Dverge: Diversifying vulnerabilities for
enhanced robust generation of ensembles. Advances in Neural Information Processing Systems,
33, 2020b.
Zhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Pan Zhou, Benjamin I. P. Rubinstein,
Ce Zhang, and Bo Li. Trs: Transferability reduced ensemble via promoting gradient diversity and
model smoothness. In Advances in Neural Information Processing Systems, 2021.
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius.
In International Conference on Learning Representations, 2019.
Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Boosting the certified robustness of l-infinity
distance nets. In International Conference on Learning Representations, 2022. URL https:
//openreview.net/forum?id=Q76Y7wkiji.
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Black-box certifica-
tion with randomized smoothing: A functional optimization based framework. arXiv preprint
arXiv:2002.09169, 2020.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in neural information
processing systems, pp. 4939-4948, 2018.
Huan Zhang, Minhao Cheng, and Cho-Jui Hsieh. Enhancing certifiable robustness via a deep model
ensemble. arXiv preprint arXiv:1910.14655, 2019.
13
Published as a conference paper at ICLR 2022
In Appendix A, we provide more background knowledge about randomized smoothing. In Ap-
pendix B, we first discuss the direct connection between the definition of r-robustness and the
robustness certification of randomized smoothing, then prove the robustness conditions and the
comparison results presented in Section 2. In Appendix C, we formally define, discuss, and the-
oretically compare the smoothing strategies for ensembles. In Appendix D, we characterize the
robustness of smoothed ML ensembles from statistical robustness perspective which is directly related
to the robustness certification of randomized smoothing. In Appendix E, we present and analyze
some alternative designs of DRT. In Appendix F, we show the detailed experimental setup and full
experiment results. Finally, in Appendix G, we conduct abalation studies on the effects of Gradient
Diversity and Confidence Margin Losses in DRT in Appendix G.1, certified robustness of single base
model within DRT-trained ensemble in Appendix G.2, and investigate how optimizing ensemble
weights can further improve the certified robustness of DRT-trained ensemble in Appendix G.3. We
also analyze other gradient diversity promoted regularizers’ performance and compare them with
DRT in Appendix G.4.
A	Background: Randomized Smoothing
Cohen et al. (Cohen et al., 2019) leverage Neyman-Pearson Lemma Neyman & Pearson (1933) to
provide a computable robustness certification for the smoothed classifier.
Lemma A.1 (Robustness Certificate of Randomized Smoothing; (Cohen et al., 2019)). At point x0,
let random variable ε 〜N(0, σ2Id), a smoothed model GF is r-robust where
r = σ (φ-1 (gF (XO)CA)- φ-1 (gF (XO)CB 力,	(II)
where cA = GεF(x0) and cB = GεF(2)(x0) are the top and runner-up class respectively, and Φ-1 is
the inverse cumulative distribution function (CDF) of standard normal distribution.
In practice, for ease of sampling, the standard way of computing the certified radius is using the lower
bound of Equation (11): r = σΦ-1(gFε (XO)CA ). Now we only need to figure out gFε (XO)CA . The
common way is to use Monte-Carlo sampling together with binomial confidence interval Cohen et al.
(2019); Yang et al. (2020a); Zhai et al. (2019); Jeong & Shin (2020). Concretely, from definition
gF (xo )ca = Pjn (0,σ2id)(F (xo + ε) = ca), We sample n Gaussian noises: ε1,ε2,...,εn 〜
N (0,σ2Id) and compute the empirical mean: gF (XO)CA = n Pin=ι I [F (xo + εɪ) = ca]. The
binomial testing Clopper & Pearson (1934) then gives a high-confidence loWer bound of gFε (XO)CA
based on gF(XO)CA. We follow the setting in the literature: sample n = 105 samples and set
confidence level be 99.9% Cohen et al. (2019); Yang et al. (2020a); Zhai et al. (2019); Jeong & Shin
(2020). More details are available in Appendix F. Note that F could be either single model or any
ensemble models with Ensemble-before-Smoothing strategy.
B	Detailed Analysis and Proofs in Section 2
In this appendix, we first show the omitted theoretical results in Section 2, which are the robustness
conditions for Max-Margin Ensemble (MME) and the comparison between the robustness of ensemble
model and single model. We then present all proofs for these theoretical results.
B.1	Detailed Theoretical Results and Discussion
Here we present the theoretical results omitted from Section 2 along with some discussions.
B.1.1	Robustness Condition of MME
For MME, we have the following robustness condition.
Theorem 3 (Gradient and Confidence Margin Condition for MME Robustness). Given input XO ∈ Rd
with ground-truth label yO ∈ [C], and MMME as an MME defined over base models {F1, F2}.
MMME(XO) = yO. Both F1 and F2 are β-smooth.
14
Published as a conference paper at ICLR 2022
•	(Sufficient Condition) If for any y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
kVχ∕y"y1 (xo) + Vxfy0/y2 (xo) k2 ≤ 1(fyo/yi (xo) + 削y (xo)) - 2βr,	(12)
r
then MMME is r-robust at point x0.
•	(Necessary Condition) Supposefor any X ∈ {xo + δ : ∣∣δ∣∣2 ≤ r}, for any i ∈ {1, 2}, either
Fi(x) = y0 or Fi(2)(x) = y0. If MMME is r-robust at point x0, then for any y1, y2 ∈ [C]
such that y1 6= y0 and y2 6= y0,
kVxfyo/yi(xo)+ Vxfy0/y2 (χ0)k2 ≤ 1(fy0∕yι(χo) + fyo/y2 (χ0)) + 2βr.	(13)
r
Comparing with the robustness conditions of MME (Theorem 1), the conditions for MME have highly
similar forms. Thus, the discussion for ERI in main text (Equation (6)) still applies here, including
the positive impact of diversified gradients and large confidence margins towards MME ensemble
robustness in both sufficient and necessary conditions and the implication of small model-smoothness
bound β . A major distinction is that the condition for MME is limited to two base models. This
is because the “maximum” operator in MME protocol poses difficulties for expressing the robust
conditions in succinct continuous functions of base models’ confidence. Therefore, Taylor expansion
cannot be applied. We leave the extension to N > 2 base models as future work, and we conjecture
the tendency would be similar as Equation (5). The theorem is proved in Appendix B.2.
B.1.2	Comparison between Ensemble Robustness and Single-Model Robustness
To compare the robustness of ensemble models and single models, we have the following corollary
that is extended from Theorem 1 and Theorem 3.
Corollary 1 (Comparison of Ensemble and Single-Model Robustness). Given an input x0 ∈ Rd
with ground-truth label y0 ∈ [C]. Suppose we have two β-smooth base models {F1, F2}, which are
both r-robust at point x0. For any ∆ ∈ [0, 1):
•	(Weighted Ensemble) Define Weighted Ensemble MWE with base models {F1, F2}. Sup-
pose MWE (x0) = y0. If for any label yi 6= y0, the base models’ smooth-
ness β ≤ ∆ ∙ min{fy0∕yi(xo),fyo/yi(x0)}∕(c2r2), and the gradient cosine similarity
coshVx f1y0 /yi (x0), Vx f2y0 /yi (x0)i ≤ cos θ, then the MWE with weights {w1 , w2} is at least
R-robust at point x0 with
R = r ∙ ] +	(1 — Cwe(1 — Cos θ)) 1/2, where	(14)
C — rnirι	2w1 w2f10y (χo)fy0∕yi (xo)
CWE = yiiyi=yo (w1fyo∕yi(χo)+w2fyo∕yi(χ0))2
C = max{I-δ (1 — Cwe(1 - Cos θ))-1/2, 1}.
•	(Max-Margin Ensemble) Define Max-Margin Ensemble MMME with the base models {F1, F2}.
Suppose MMME (x0) =	y0.	If for any label	y1	6=	y0	and	y2	6=	y0,	the base models’
SmoothneSS β ≤ ∆ ∙ min{fy0/y1 (xο), fy0/y2(x0)}∕(c2r2), and the gradient cosine similarity
CoshVxf1y0/y1 (x0), Vxf2y0/y2 (x0)i ≤ Cos θ, then the MMME is at least R-robust at point x0
with
1 — ∆
1 + ∆
R = r ∙
(1 — CMME(I — cos θ))T" , where
(15)
2f1y0/y1(x0)f2y0/y2(x0)
MME =	mn： (fy0∕y1 (xο)+fy0∕y2(xο))2
y1 ,y2 6=y0
C = max{ 1+δ (1 ― CMME(I ― cos θ)) 2,1}.
The proof is given in Appendix B.3.
Optimizing Weighted Ensemble. As we can observe from Corollary 1, we can adjust the weights
{w1 , w2} for Weighted Ensemble to change CwE and the certified robust radius (Equation (14)).
15
Published as a conference paper at ICLR 2022
Then comes the problem of which set of weights can achieve the highest certified robust radius. Since
larger CWE results in higher radius, we need to choose
(w1OP T , w2OPT) = arg max min
w1 ,w2 yi :yi 6=y0
2wιw2f10y" (xo)f20y (xo)
(Wfyo/yi(xo)+ w2fyo/yi (χo))2
Since this quantity is scale-invariant, we can fix w1 and optimize over w2 to get the optimal weights.
In particular, if there are only two classes, we have a closed-form solution
w2OP T ) = arg max
w1,w2
2w1w2 fyo/yi(xo)fyo/yi(xo)
(wifyo/yi(xo)+W2fyo/yi(xo))2
{k ∙ fyo/yi (xo),k ∙ fyo/yi(xo) : k ∈ R+},
and corresponding CWE achieves the maximum 1/2.
For a special case—average weighted ensemble, we get the corresponding certified robust radius by
setting w1 = w2 and plug the yielded
CWE
min	2fyo/yi W: (XO)
yi:yi6=yo (f1yo/yi(x0) +f2yo/yi(x0))2
∈ (0, 1/2].
into Equation (14).
Comparison between ensemble and single-model robustness. The similar forms of R in the
corollary allow us to discuss the Weighted Ensemble and Max-Margin Ensemble together. Specifically,
we let C be either CWE or CMME , then
R = r ∙后(1 - C(1-8S O))-"2.
Since when R > r, both ensembles have higher certified robustness than the base models, we solve
this condition for cos θ:
1 - ∆ 2	4∆
R >r ^⇒ ( ι + δ ) > 1 - C(1 - Cosθ) ^⇒ Cos θ ≤ 1 - c(i + ∆)2 .
Notice that C ∈ (0, 1/2]. From this condition, we can easily observe that when the gradient cosine
similarity is smaller, it is more likely that the ensemble has higher certified robustness than the base
models. When the model is smooth enough, according to the condition on β, we can notice that ∆
could be close to zero. As a result, 1 - C(4^产 is close to 1. Thus, unless the gradient of base
models is (or close to) colinear, it always holds that the ensemble (either WE or MME) has higher
certified robustness than the base models.
Larger certified radius with larger number of base models N. Following the same methodol-
ogy, we can further observe that larger number of base models N can lead to larger certified radius as
the following proposition shows.
Proposition B.1 (More Base Models Lead to Higher Certified Robustness of Weighted Ensemble).
At clean input x0 ∈ Rd with ground-truth label y0 ∈ [C], suppose all base models {fi}iN=+1M are
β -smooth. Suppose the Weighted Ensemble M1 of base models {fi}iN=1 and M2 of base models
{fi}iN=+NM+1 are both r-robust according to the sufficient condition in Theorem 1, and for any yi 6= y0
the Mi and M?'s ensemble gradients (PN=I WjPxfjOyi(xo) and PNN+ι WjNxf；o/yi(x0))
are non-zero and not colinear, then the Weighted Ensemble M of {fi}iN=+1M is r0-robustfor some
r0 > r.
Proof of Proposition B.1. For any yi 6= y0, since both Mi and M2 are r-robust according to the
sufficient condition of Theorem 1, we have
N	NN
Il X Wj Nxfyo/yi (XO)IL ≤ r X fyo/yi (XO)-βr X Wj,	(16)
j=i	j=i	j=i
16
Published as a conference paper at ICLR 2022
N+M
Il X WjVx琛/yi(X0)
j=N+1
N+M	N+M
≤ 1 X 琛/yi(x0)-βr X Wj.
j=N+1	j=N+1
(17)
Adding above two inequalties we get
N	N+M	N+M	N+M
IlXWjVxfjy0/yi(xo)∣∣2 + ∣∣ X WjNxf"(xo)∣∣2 ≤ r X 琛/yi(X0)-βr X Wj.
j=1	j=N+1	j=1	j=1
(18)
Since gradients of ensemble are not colinear and non-zero, from the triangle inequality,
N+M	N
∣∣ X WjVx琛/yi(χ0)∣∣2 < ∣∣XWjVxfjy0/yi(χ0)∣∣2 +
j=1	2 j=1	2
N+M
∣∣	X WjVxfjy0/yi (x0)∣∣2
j=N+1
and thus
N+M
∣∣ X WjVxfjy0/yi (x0)
j=1
N+M	N+M
<1 X fj0y (xo)-βr X Wj，
j=1	j=1
(19)
(20)
which means We can increase r to r0 and still keep the inequality hold with "≤", and in turn certify a
larger radius r0 according to Theorem 1.	口
Since DRT imposes diversified gradients via GD Loss, the “not colinear” condition easily holds for
DRT ensemble, and therefore the proposition implies larger number of base models N lead to higher
certified robustness of WE. For MME, we empirically observe similar trends.
B.1.3 Robustness Conditions for Smoothed Ensemble Models
Following the discussion in Section 2.3, for smoothed WE and MME, with the model-smoothness
bound (Theorem 2), we can concretize the general robustness conditions in this way.
We define the soft smoothed confidence function gf (x) := Eεf (X + ε). This definition is also used
in the literature (Salman et al., 2019; Zhai et al., 2019; Kumar et al., 2020). As a result, we revise the
ensemble protocols of WE and MME by replacing the original confidences {fi (x0)}iN=1 with these
soft smoothed confidences {gε(x0)}N=1∙ These protocols then choose the predicted class by treating
these {uε(xo)}N=ι as the base models, confidence scores. In the experiments, we did not actually
evaluate these revised protocols since their robustness performance are expected to be similar as
original ones (Salman et al., 2019; Zhai et al., 2019; Kumar et al., 2020). The derived results connect
smoothed ensemble robustness with the confidence scores.
Corollary 2 (Gradient and Confidence Margin Conditions for Smoothed WE Robustness). Given
input Xo ∈ Rd with ground-truth label yo ∈ [C]. Let ε 〜N(0,σ2Id) be a Gaussian random
variable. Define soft smoothed confidence gf (x) := Eεfi(x + ε) for each base model Fi (1 ≤ i ≤
N). The GMWE is a WE defined over soft smoothed base models {gf }N=ι with weights {Wi}N=ι.
GεMWE (x0) = y0.
•	(Sufficient Condition) The G MW E is r-robust at point Xo if for any yi, = yo,
N	1N	2 N
∣∣X WjVx⑹y0/yi(xo)∣∣2 ≤ r X Wj(gjr/y (Xo) - σ X Wj,	(21)
•	(Necessary Condition) If G MW E is r-robust at point Xo ,for any yi, = yo,
N	1N	2 N
∣∣X WjVx(般y0∕yi(xo)∣∣2 ≤ r XWj(般y0∕yi(Xo) + σr2 XWj.	(22)
j=1	j=1	j=1
Corollary 3 (Gradient and Confidence Margin Condition for Smoothed MME Robustness). Given
input Xo ∈ Rd with ground-truth label yo ∈ [C]. Let ε 〜N(0,σ2Id) be a Gaussian random
variable. Define SOft smoothed confidence gf (x) := Effi(X + ε) for either base model Fi or F2.
The GMMME is a MME defined over soft smoothed base models {gf, gf }. GMMME(XO) = yo.
17
Published as a conference paper at ICLR 2022
• (Sufficient Condition) If for any y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
1	4r
kVχ(gε)y"y1(χ0)+Vx 喷严0/y (χ0)k2 ≤ -((gε)y"y1(χo) +(g2)y0/y2 (χo)) - -2,
r	σ2
(23)
then GMMME is r-robust at point xo.
• (Necessary Condition) Suppose for any X ∈ {xo + δ : ∣∣δ∣∣2 ≤ r}, for any i ∈ {1, 2},
either GFi (x) = yo or G^F^(x) = y0. If GMMME is r-robust at point x°, then for any
y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
1	-r
kVx(gε)y/y1(x0) + VxGIrOny(X0)k2 ≤ -((gε)y"y1(xo) + (g2TOny(xo)) + -2.
r	σ2
(24)
Remark. The above two corollaries are extended from Theorem 1 and Theorem 3 respectively, and
correspond to our discussion in Section 2.3. We defer the proofs to Appendix B.5. From these
two corollaries, we can explicit see that the Ensemble-before-Smoothing (see Definition 5) provides
smoothed classifiers GMWE and GMMME with bounded model-smoothness; and We can see the
correlation between the robustness conditions and gradient diversity/confidence margin for smoothed
ensembles.
B.2 Proofs of Robustness Conditions for General Ensemble Models
This subsection contains the proofs of robustness conditions. First, we connect the prediction of
ensemble models with the arithmetic relations of confidence scores of base models. This connection
is straightforward to establish for Weighted Ensemble (shown in Proposition B.2), but nontrivial for
Max-Margin Ensemble (shown in Theorem 4). Then, we prove the desired robustness conditions
using Taylor expansion with Lagrange reminder.
Proposition B.2 (Robustness Condition for WE). Consider an input x0 ∈ Rd with ground-truth
label y0 ∈ [C], and an ensemble model MWE constructed by base models {Fi}iN=1 with weights
{wi}iN=1. Suppose MWE(x0) = y0. Then, the ensemble MWE is r-robust at point x0 if and only if
for any x ∈ {x0 + δ : kδk2 ≤ r},
min
yi∈[C];yi=yo
N
X wjfjn0Oni (x) ≥ 0.
j=1
(25)
Proof of Proposition B.2. According the the definition of r-robust, we know MWE is r-robust if and
only if for any point x := x0 + δ where kδk2 ≤ r, MWE(x0 + δ) = y0, which means that for any
other label yi 6= y0, the confidence score for label y0 is larger or equal than the confidence score for
label yi . It means that
NN
wjfj (x)n0 ≥	wjfj (x)ni
j=1	j=1
for any x ∈ {x0 + δ : kδk2 ≤ r}. Since this should hold for any yi 6= y0, we have the sufficient and
necessary condition
N
min	X wjfn0Oni (x) ≥ 0.	(25)
yi∈[c]%=yo J jjj	)-
j=1
□
Theorem 4 (Robustness Condition for MME). Consider an input x0 ∈ Rd with ground-truth label
y0 ∈ [C]. Let MMME be an MME defined over base models {Fi}iN=1. Suppose: (1) MMME(x0) =
y0; (2) for any x ∈ {x0 + δ : kδk2 ≤ r}, given any base model i ∈ [N], either Fi(x) = y0
or Fi(2) (x) = y0. Then, the ensemble MMME is r-robust at point x0 if and only if for any
x ∈ {x0 +δ : kδk2 ≤ r},
max min	fin0Oni(x)
≥ max min	finiOn0 (x).	(26)
i∈[N] yi∈[C]∙∙yi=yo	i∈[N] y0∈[CY-yi=yo
18
Published as a conference paper at ICLR 2022
The theorem states the sufficient and necessary robustness condition for MME. We divide the two
directions into the following two lemmas and prove them separately. We mainly use the alternative
form of Equation (26) as such in the following lemmas and their proofs:
max min	fiy0/yi (x) + min min	fiy0/yi (x) ≥ 0.	(26)
i∈[N] yi∈[CT∙yi=yo	i∈[N] y0∈[C]∙∙yi=yo
Lemma B.1 (Sufficient Condition for MME). Let MMME be an MME defined over base models
{Fi}iN=1. For any input x0 ∈ Rd, the Max-Margin Ensemble MMME predicts MMME(x0) = y0 if
max min	fiy0/yi (x0) + min min	fiy0/yi (x0) ≥ 0.	(26)
i∈[N] yi∈[CY∙yi=yo	i∈[N] y0∈[C]∙∙yi=yo
Proof of Lemma B.1. For brevity, for i ∈ [N], we denote yi := Fi (x0), yi0 := Fi(2) (x0) for each
base model’s top class and runner-up class at point x0 .
Suppose MMME(x0) 6= y0, then according to ensemble definition (see Definition 3), there exists
c ∈ [N], such that MMME(x0) = Fc(x0) = yc, and
∀i ∈ [N], i 6= c, fc(x0)yc/yc0 > fi (x0)yi/yi0 .	(27)
Because yc 6= y0, we have fc(x0)y0 ≤ fc(x0)yc0, so that fc(x0)yc/y0 ≥ fc(x0)yc/yc0. Now consider
any model Fi where i ∈ [N], we would like to show that there exists y* = y0, such that fi(x0)yi/y0 ≥
fi(xo)y"y*:
•	If yi = yo, let y* := yi, trivially fi(xo)yi/yi = fi(xo)yo/y* ；
•	Ifyi 6= y0, and yi0 6= y0, we let y* := yi0, then fi(x0)yi/yi0 = fi(x0)yi/y* ≥ fi(x0)y0/y*;
•	Ifyi 6= y0, but yi0 = y0, we let y* := yi, then fi(x0)yi/yi0 = fi(x0)yi/y0 ≥ fi(x0)y0/yi =
fi(x0)y0/y*.
Combine the above findings with Equation 27, we have:
∀i∈ [N], i 6=c, ∃yc* ∈ [C]andyc* 6= y0, ∃yi* ∈ [C]andyi* 6= y0, fc(x0)yc*/y0 >fi(x0)y0/yi*.
Therefore, its negation
∃i∈ [N], i 6= c, ∀yc* ∈ [C]andyc* 6= y0, ∀yi* ∈ [C]andyi* 6= y0, fc(x0)y0/yc* + fi(x0)y0/yi* ≥0
(28)
implies M(x0) = y0. Since Equation (28) holds for any yc* and yi*, the equation is equivalent to
∃i ∈	[N],	i	6= c,	min	fc(x0)y0/yc (x0) +	min	fi(x0)y0/yi0 (x0)	≥	0.
ycE[C]：yc=yo	y0 ∈[CY-y0=yo
The existence qualifier over i can be replaced by maximum:
min	fc(x0)y0/yc (x0) + max min	fi(x0)y0/yi0 (x0) ≥ 0.
yc∈[C]∙∙yc=yo	i∈[N] y0∈[CY-y0=yo
It is implied by
max min	fiy0/yi (x0) + min min	fiy0/yi (x0) ≥ 0.	(26)
i∈[N] yi∈[CY∙yi=yo	i∈[N] y0∈[C]∙∙yi=yo
Thus, Equation (26) is a sufficient condition for MMME(X0) = yo.	□
Lemma B.2 (Necessary Condition for MME). For any input x0 ∈ Rd, if for any base model
i ∈ [N], either Fi(x0) = y0 or Fi(2) (x0) = y0, then Max-Margin Ensemble MMME predicting
MMME(x0) = y0 implies
max min	fiy0/yi (x0) + min min	fiy0/yi (x0) ≥ 0.	(26)
i∈[N] yi∈[C]%=yo i	i∈[N] y0∈[C]"=yo i
19
Published as a conference paper at ICLR 2022
Proof of Lemma B.2. Similar as before, for brevity, for i ∈ [N], we denote yi := Fi(x0), yi0 :=
Fi(2)(x0) for each base model’s top class and runner-up class at point x0.
Suppose Equation (26) is not satisfied, it means that
∃c ∈ [N ], ∃yC ∈ [C ] and yC =y 0, ∀i ∈ [N ], ∃y* ∈ [C ] and y=yo, f""° (xo) > 于叫第(xo).
•	If yc = y0, then fyc/y0 (x0) ≤ 0, which implies that fy0/% (χ0) < 0, and hence Fi(x0)=
yo. Moreover, we know that f;"Vy (xo) = f"y0 (xo) ≥ fiy“y0(xo) > fy0/y"xo) ≥
fcy0 /yc(x0) = fcyc /yc(x0) so M(x0) 6= Fc(x0) = y0.
•	If yc = yo, i.e., yC = yo, then fC"y" (xo) ≥ fC"y"(xo) > £：期(xo). If Fi(xo)=
y0, then fiV0/Vi (x0) ≥ fiV0/Vi(x0) = fiVi/Vi(x0). Thus, fcVc/Vc(x0) = fcVc/V0(x0) >
fiyi/yi(xo). As the result, M(xo) = Fc(xo) 6= yo.
For both cases, we show that MMME(xo) 6= yo, i.e., Equation (26) is a necessary condition for
M(xo)= yo.	□
Proof of Theorem 4. Lemmas B.1 and B.2 are exactly the two directions (necessary and sufficient
condition) of MMME predicting label yo at point x. Therefore, if the condition (Equation (26)) holds
for any X ∈ {x0 + δ : kδ∣∣2 ≤ r}, the ensemble MMME is r-robust at point x°; vice versa. □
For comparison, here we list the trivial robustness condition for single model.
Fact B.1 (Robustness Condition for Single Model). Consider an input xo ∈ Rd with ground-truth
label yo ∈ [C]. Suppose a model F satisfies F(xo) = yo. Then, the model F is r-robust at point xo
if and only if for any x ∈ {xo + δ : kδ k2 ≤ r},
min	fy0/yi (x) ≥ 0.
Vi∈[C]iVi = V0
The fact is apparent given that the model predicts the class with the highest confidence.
Now we are ready to apply Taylor expansion to derive the robustness conditions shown in main text.
Theorem 1 (Gradient and Confidence Margin Condition for WE Robustness). Given input xo ∈ Rd
with ground-truth label yo ∈ [C], and MWE as a WE defined over base models {Fi}iN=1 with weights
{wi}iN=1. MWE(xo) = yo. All base model Fi’s are β-smooth.
•	(Sufficient Condition) MWE is r-robust at point xo if for any yi 6= yo,
N	NN
∣∣X Wj Vχfjy0∕yy (xo)∣∣2 ≤ r X Wj fjVo/Vy(XO)- βr X wj.	(4)
j=1	j=1	j=1
•	(Necessary Condition) If MWE is r-robust at point Xo, then for any yi 6= yo,
N	NN
∣∣X wj vxfjV0/Vy (XO) ∣L ≤ r X wj fjVo/Vy(XO)+βr χ wj.	⑸
j=1	2	j=1	j=1
Proof of Theorem 1. From Taylor expansion with Lagrange remainder and the β-smoothness assump-
tion on the base models, we have
NN	N	N
Xwjf"(xo)-r∣∣XwjVxfjy0/Vy(χo)∣∣2 - 2TrX(2βwj) ≤ 嘈I ≤rXwjfj"0y'(X)
j=1	j=1	2	j=1	0 2 j=1
NN	N
≤ X wj fjV0/Vy (Xo) - r∣∣X wj Vxfjy0/Vy (xo)∣L + 2 r2 X(2βwj ),
2
(29)
20
Published as a conference paper at ICLR 2022
where the term -∣r2 PN=ι(2βwj) and ∣r2 PN=ι(2βwj) are bounded from Lagrange remainder.
Note that the difference fjy0/yi is (2β)-smooth instead of β-smooth since it is the difference of two
β-smooth function, and thus PjN=1 wjfjy0/yi isPjN=1(2βwj)-smooth. From Proposition B.2, the
sufficient and necessary condition of WE’s r-robustness is PjN=1 wjfjy0/yi (x) ≥ 0 for any yi ∈ [C]
such that yi 6= y0, and any x = x0 + δ where kδk2 ≤ r. Plugging this term into Equation (29) we
get the theorem.	□
Theorem 3 (Gradient and Confidence Margin Condition for MME Robustness). Given input x0 ∈ Rd
with ground-truth label y0 ∈ [C], and MMME as an MME defined over base models {F1, F2}.
MMME(x0) = y0. Both F1 and F2 are β-smooth.
•	(Sufficient Condition) If for any y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
kVχ∕y"y1 (χo) + Vχ∕y"y2 (χo) k2 ≤ ：(fyo/y1 (χo) + fyo/y2(xo))- 2βr,	(12)
then MMME is r-robust at point x0.
•	(Necessary Condition) Supposefor any X ∈ {x0 + δ : ∣∣δ∣∣2 ≤ r}, for any i ∈ {1, 2}, either
Fi(x) = y0 or Fi(2)(x) = y0. If MMME is r-robust at point x0, then for any y1, y2 ∈ [C]
such that y1 6= y0 and y2 6= y0 ,
kVxfyo/yi(xo)+ Vχfy0∕y2 (χ0)k2 ≤ r(fyo/yi(xo)+ fyo/y2 (χ0)) + 2βr.	(13)
Proof of Theorem 3. We prove the sufficient condition and necessary condition separately.
•	(Sufficient Condition)
From Lemma B.1, since there are only two base models, we can simplify the sufficient
condition for MMME(x) = y0 as
min	f1y0/yi (x) + min	f2y0/yi0 (x) ≥ 0.
yi∈[CT∙yi=yo	y0 E[C]：yi=yo
In other words, for any y1 6= y0 and y2 6= y0,
f1y0/y1(x)+f2y0/y2(x) ≥0.	(30)
With Taylor expansion and model-smoothness assumption, we have
min	f1y0/y1(x)+f2y0/y2(x)
x：kx-x0k2≤r
≥fy"y1(x0)+ fy0/y2(xo) - rkVxfy0/y1 (xo) + Vxfy0/y2(X0)k2 - 1 ∙ 4βr2.
Plugging this into Equation (30) yields the sufficient condition.
In the above equation, the term - ɪ ∙ 4βr2 is bounded from Lagrange remainder. Here, the
4β term comes from the fact that f1y0/y1 (x) + f2y0/y2 (x) is (4β)-smooth since it is the sum
of difference of β-smooth function.
•	(Necessary Condition)
From Lemma B.2, similarly, the necessary condition for MMME(x) = yo is simplified to:
for any y1 6= yo and y2 6= yo,
f1y0/y1(x)+f2y0/y2(x) ≥ 0.	(30)
Again, from Taylor expansion, we have
min	f1y0/y1(x)+f2y0/y2(x)
x：kx-X0k2≤r
≤fy0∕y1(xo)+ fy0/y2(xo)-rkVxfy0/y1 (xo) + Vxfy0/y2(x0)k2 + J ∙ 4βr2.
21
Published as a conference paper at ICLR 2022
Plugging this into Equation (30) yields the necessary condition.
In the above equation, the term + 2 ∙ 4βr2 is bounded from Lagrange remainder. The 4β
term appears because of the same reason as before.
□
Since we will compare the robustness of ensemble models and the single model, we show the
corresponding conditions for single-model robustness.
Proposition B.3 (Gradient and Confidence Margin Conditions for Single-Model Robustness). Given
input x0 ∈ Rd with ground-truth label y0 ∈ [C]. Model F(x0) = y0, and it is β-smooth.
•	(Sufficient Condition) If for any y1 ∈ [C] such that y1 6= y0,
kVχfy"y1(x0)k2 ≤ 1 fy"y1(x0) - βr,	(31)
r
F is r-robust at point x0.
•	(Necessary Condition) If F is r-robust at point x0, for any y1 ∈ [C] such that y1 6= y0,
kVχfyo/yi(xo)k2 ≤ 1 fyo/yi(X0)+ βr.	(32)
r
Proof of Proposition B.3. This proposition is apparent given the following inequality from Taylor
expansion
fy0/yi (χo)-rkVχfyo/yi (x0)k2-βr2 ≤ min	fyo/yi (x) ≤ fyo/yi (xo)-rkVχfyo/yi (x0)k2+βr2
x：kx-X0k2≤r
and the sufficient and necessary robust condition in Fact B.1.	□
B.3 Proof of Robustness Comparison Results between Ensemble Models and
Single Models
Corollary 1 (Comparison of Ensemble and Single-Model Robustness). Given an input x0 ∈ Rd
with ground-truth label y0 ∈ [C]. Suppose we have two β-smooth base models {F1, F2}, which are
both r-robust at point x0. For any ∆ ∈ [0, 1):
•	(Weighted Ensemble) Define Weighted Ensemble MWE with base models {F1, F2}. Sup-
pose MWE (x0) = y0. If for any label yi 6= y0, the base models’ smooth-
ness β ≤ ∆ ∙ min{fy0∕yi(xo),fyo/yi(xo)}∕(c2r2), and the gradient cosine similarity
coshVx f1y0 /yi (x0), Vx f2y0 /yi (x0)i ≤ cos θ, then the MWE with weights {w1 , w2} is at least
R-robust at point x0 with
R = r ∙ ] +	(1 — Cwe(1 — Cos θ)) 1/2, where	(14)
CWE
min	2w1 w2f10y (χo)fy0∕yi (χo)
yi：yi=yo (WIfwy(XO)+w2fy0/yi(XO))2
C = max{ɪ+-δ (1 — Cwe(1 - Cos θ))-1/2, 1}.
•	(Max-Margin Ensemble) Define Max-Margin Ensemble MMME With the base models {Fι, F2}.
Suppose MMME(Xo) = yo. If for any label yι = yo and y = yo, the base models'
smoothness β ≤ ∆ ∙ min{fyo/y1 (xo), fyo/y2(xo)}∕(c2r2), and the gradient cosine similarity
CoshVXfyo/y1 (xo), VXfy0/y2 (xo)i ≤ Cos θ, then the MMME is at least R-robust at point x0
with
CMME = ym1,iyn2:
y1 ,y2 6=yO
R = r ∙ 1 + ∆ (1 — Cmme(1 — cos θ))-1/2, where
2f10/y1 (xο)f2wy2(xο)	c.rn>,r1-∆ 门 C 门	「“A、「1/2 1
(fyο∕y1 (x )+f yo/”的。))2，c = max{ i+δ (1 - CMME(I - cos θ))	,1}∙
(15)
22
Published as a conference paper at ICLR 2022
Proof of Corollary 1. We first prove the theorem for Weighted Ensemble. For arbitrary yi 6= y0, we
have
||wiVxfy0/yi(X0)+ W2Vxfyo/yi (x0)k2
=Jw2kVχfy"yi (χ0)k2+w2kVxfyo/yi(X0)k2 + 2wiw2〈vxfy0/yi (χ0), fyo/yi (χ0)i
≤qw2kVχfy0/yi (xo)k2 + W2kVxfy0/yi (xo)k2 + 2wiW2kVxfy0/yi (x0)k2kVχfy0/yi (x0)k2 cos θ
(i.)
≤
fy0∕yi(xo) + βr)2 + w2 (1 fy0∕yi(xo) +
+ 2w1w2 (1 fy0∕yi (χo) + βr) (1 fy0∕yi (xo) + βr) cos θ
=-W201 ^fyo/yi (xo) + βr2^~+ w22 ^fy 0/yi (xo) + βr2^~+ 2w]W? (fyo/yi (xo) + βr2) (fyo/yi (xo) + βr2) Cos θ
(i≤) 1 ∙ (1 + Cl) qw2fy0/yi(χ0)2 + w2fy0/yi(χ0)2 + 2wιw2fy0∕yi(xo)fy0/yi(χ°)cosθ
=1 ∙ (1 + Cl) J (wιf”yi (xo) + W2 f"yi (x。))2 - 2(1 - cos θ)wιf" (x0)w2 f"yi (xo)
(≤'1 ∙ (1 + Cl) pl - (1 - cos Θ)Cwe (wιfy0∕yi (x。)+ W2fy0/yi (xo))
where (i.) follows from the necessary condition in Proposition B.3; (ii.) uses the condition on β; and
(iii.) replaces 2w1w2f1y0/yi(x0)f2y0/yi(x0) leveraging CWE. Now, we define
K := 1 + ∆ (I - CWE(I - Cos θ)) /2 .
All we need to do is to prove that MWE is robust within radius Kr. To do so, from Equation (4), we
upper bound ||wiVxfyo/yi (xo) + w^xf2yo/vi (x0)k2 by K (wιfy"yi (xo) + w2 fyo/yi (xo))-
βKr(w1 + w2):
|w1Vxf1y0/yi(x0)+w2Vxf2y0/yi(x0)|2
≤ r ∙ (1 + C) P1-(I—Cos CWE (wιfy0∕yi (XO)+w fy0∕yi (XO))
≤ r(I+∆)P1 — (1 — cos θ)CwE (wιfy0∕yi (XO)+w2fy0∕yi (XO))
=r ∙ I』「- △»	…(wιfy0∕yi(χο)+w2fy"yi(χο))
r 1+∆ (1 — (1 — cos θ)CWE)
=Kr (1 — ∆) (wι fy0/yi (xo)+ WfyM (xo))
≤Kr (wιfy0∕yi(xo)+ W2fy0∕yi(xo) — ∆min{fy0∕yi(xο),fy0∕yi(x0)}(w1 + W2)).
Notice that ∆min{fy0∕yi(xo), fy0/yi(xo)} ≥ βc2r2 from β's condition, so
kwi Vxfy0/yi (xo)+ W2Vχf20y (xo)∣2
≤Kr (wιfy0∕yi(xo) + W2fy0∕yi(xo) — βc2r2(w1 + w2))
=J (wιf1“/yi(xo) + W2fy0∕yi(xo)) — βKr(wι + W2) ∙餐
Kr	K2
≤Kr (wιf10yi(xo) + W2fy"yi(xo)) — βKr(wι + W2).
From Equation (4), the theorem for Weighted Ensemble is proved.
23
Published as a conference paper at ICLR 2022
Now we prove the theorem for Max-Margin Ensemble. Similarly, for any arbitrary y1, y2 such that
y1 6= y0 , y2 6= y0, we have
kVχfy0/y1 (χo) + Vxfyo/y2 (χ0)k2
≤r ∙ (l + C2) p1 - (1 - Cos Θ)Cmme (fy"y1(x0) + fy0/y2 (xo)).
Now we define
K0 := J + △ (1 - CMME(I - cos θ)) /2 .
Again, from β's condition We have ∆min{fy0∕y1 (xo), f2y0/y2 (xo)} ≥ βc2r2 and
kVxfy0/yi(xo)+ Vχ∕y"y2 (χ0)k2 ≤ K0r (fyo/yi (χo) + fyo/yi (χ°)) - 2βK 0r.
From Equation (12), the ensemble is (K0r)-robust at point x0, i.e., the theorem for Max-Margin
Ensemble is proved.	□
B.4 Proofs of Model-Smoothness Bounds for Randomized Smoothing
Theorem 2 (Model-Smoothness Upper Bound for gf). Let ε 〜N(0, σ2Id) be a Gaussian random
variable, then the soft smoothed confidence function gf is (2∕σ2)-smooth.
ProofofTheorem 2. Recall that gf (xj = Ef 〜N(。b2 id)f (X + εj, where f (x + εj is a function
from Rd to {0, 1}. Therefore, to prove that gM is (2∕σ2)-smooth, we only need to show that for any
function f : Rd → [0, 1], the function f := f * N(0, σ2Id) is (2∕σ2)-smooth.
According to (Salman et al., 2019, Lemma 1), we have
f(x) =(2 1 )d∕2 Z f (t) exP (-kx2-F2) dt,
(2πσ2)d/2 Rκa	2	2σ2	)
vf(x) = 73-21d/2 2 Z f(t)(x -teχp (-kx-tk2) dt.
(2πσ2)d/2σ2 √Rd	∖	2σ2 )
(33)
(34)
To show f is (2∕σ2)-smooth, we only need to show that Vf is (2∕σ2)-Lipschitz. Let Hf(X) be the
Hessian matrix of f. Thus, we only need to show that for any unit vector u, ∣uτHj(x)u∣ ≤ 2∕σ2.
By the isotropy of Hf(X), it is sufficient to consider U = (1,0,0,..., 0)t, where UTHf(X)U =
Hf (x) 11. Now we only need to bound the absolute value of H/(x) 11:
IHf(X)ι1l=I ②σp2σ2
1
∂
Lf ⑻•丽(X - t)exp
) dt
—
一 (2πσ2)d/
1
exp
(2πσ2)d/
1
—
f(t) ∙ 1 -
(X1 - t1)2
kX - tk
2σ2
σ2
22)dt
exp
—
丁）dt
十 (2πσ2)d/
ɪ + ^^ . 2/'
σ	√2πσ2σ2	Jo
(X1 - t1)
d	σ2
∞ x2
F exp
σ2
2
exp
—
⅛t≡) dt
2σ2)dx
1	22 1 ∞ 2	/ 2 …
σ2+ Vπ∙σ2Λ	exp(	/2)d
(35)
Let Γ(∙) be the Gamma function, we note that
t t2 exp(-t2∕2)dt = ( t exp(-t2∕2)d(-t2∕2) = √2 i √t exp(-t)dt = √2Γ(3∕2) = pπ∕2,
oo	o
24
Published as a conference paper at ICLR 2022
and thus
which concludes the proof.
1	21
IHf(X)11ι≤ σ2 + Vπ ∙ σ2
(36)
□
Remark. The model-smoothness upper bound Theorem 2 is not limited to the ensemble model with
Ensemble-before-Smoothing strategy. Indeed, for arbitrary classification models, since the confidence
score is in range [0, 1], the theorem still holds. If the confidence score is bounded in [a, b], simple
scaling yields the model-smoothness upper bound β = 2(b-a).
Proposition B.4 (Model-Smoothness Lower Bound for gf). There exists a Smoothed confidence
function gf that is β-smooth ifand only if β ≥ √-^J=~^).
Proof of Proposition B.4. We prove by construction. Consider the single dimensional input space R,
and a model f that has confidence 1 if and only if input x ≥ 0. As a result,
gfε(x)y0
Thus,
dgf(X)yo
dx
1
i- exp
2∏σσ
and
∣d⅞⅛H = √2πσ2 ∙ Ixlexp (-2σ2).
By symmetry, we study the function h(x) = x exp(-x2/2) for x ≥ 0. We have h0(x) = (1 -
x) exp(-x2/2). Thus, h(x) obtains its maximum at x0 = 1: h(x0) = exp(-1/2), which implies that
max∣dgε (x)20∣ = exP(-1/2)=	1
ιmax =
∣	d2 x	∣	Λ∕2πσ2	V2πeσ2
which implies β ≥ -； 2 for this gf per smoothness definition (Definition 4).	口
B.5 Proofs of Robustness Conditions for Smoothed Ensemble Models
Corollary 2 (Gradient and Confidence Margin Conditions for Smoothed WE Robustness). Given
input xo ∈ Rd with ground-truth label yo ∈ [C]. Let ε 〜N(0,σ2Id) be a Gaussian random
variable. Define soft smoothed confidence gɛ (x) := Eεfi(x + ε) for each base model Fi (1 ≤ i ≤
N). The GMWE is a WE defined over soft smoothed base models {gε}N=ι With weights {wi}N=ι.
GεMWE (x0) = y0.
•	(Sufficient Condition) The G MW E is r-robust at point Xo if for any yi = yo,
N	1 N	2r N
IlX WjVx(薛严3 (xo)∣∣2 ≤ r X Wj (gjy"y (xo) - σ X Wj,	(21)
•	(Necessary Condition) If G MW E is r-robust at point Xo ,for any yi = yo,
N	1N	2 N
IIX WjVx⑹y0/yi(xo)∣L ≤ r X Wj(加0/yi(Xo) + σ X Wj.	(22)
ProofofCorollary 2. Since GMWE is a WE defined over {gε}N=ι, we apply Theorem 1 directly
for GMWe. Notice that each gi has model-smoothness bound β = 2∕σ2 from Theorem 2 and the
corollary statement follows.	口
Corollary 3 (Gradient and Confidence Margin Condition for Smoothed MME Robustness). Given
input xo ∈ Rd with ground-truth label yo ∈ [C]. Let ε 〜N(0,σ2Id) be a Gaussian random
variable. Define soft smoothed confidence gε(x) := Eεfi(x + ε) for either base model Fi or F2.
The GMMME is a MME defined over soft smoothed base models {无，gε}. GMMME(XO) = yo.
25
Published as a conference paper at ICLR 2022
•	(Sufficient Condition) If for any y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
1	4r
kVχ(gε)y0∕y1(χ0)+Vx 喷严0/y (χ0)k2 ≤ -((gε)y0∕y1(χ0) + (g2 )y0∕y2 (χo)) - -2,
r	σ2
(23)
then GMMME is r-robust at point xo.
• (Necessary Condition) Suppose for any X ∈ {xo + δ : ∣∣δ∣∣2 ≤ r}, for any i ∈ {1, 2},
either GFi (x) = yo or G^F^(x) = y0. If GMMME is r-robust at point x°, then for any
y1, y2 ∈ [C] such that y1 6= y0 and y2 6= y0,
1	4r
kvx(gε)y0∕yι(χ0)+vx 喷严 0/y (χ0)k2 ≤ -((gε)y0∕yι(χo) +(g2 )y0∕y2 (χo)) + -2.
r	σ2
(24)
Proofof Corollary 3. Since GMMME is constructed over confidences g1 and gf, We can directly
apply Theorem 1. Again, with the model-smoothness bound β = 2∕σ2 we can easily derive the
corollary statement.	口
C Analysis of Ensemble Smoothing Strategies
In main text we mainly use the adapted randomized model smoothing strategy which is named
Ensemble-before-Smoothing (EBS). We also consider Ensemble-after-Smoothing (Ensemble-after-
Smoothing). Through the following analysis, we will show Ensemble-before-Smoothing generally
provides higher certified robust radius than Ensemble-after-Smoothing which justifies our choice of
the strategy.
The Ensemble-before-Smoothing strategy is defined in Definition 5. The Ensemble-after-Smoothing
strategy is defined as such.
Definition 6 (Ensemble-after-Smoothing (EAS)). Let M be an ensemble model over base models
{Fi}iN=1. Let ε be a random variable. The EAS ensemble HMε : Rd 7→ [C] at input x0 ∈ Rd is
defined as:
HMε (x0) := GεFc (x0) where c = argmaxgFεi(x0)GεF (x0).	(37)
c	i∈[N]	i	Fi
Here, c is the index of the smoothed base model selected.
Remark. In EBS, we first construct a model ensemble M based on base models using WE or MME
protocol, then apply randomized smoothing on top of the ensemble. The resulting smoothed ensemble
predicts the most frequent class of M when the input follows distribution x0 + ε.
In EAS, we use ε to construct smoothed classifiers for base models respectively. Then, for given input
x0, the ensemble agrees on the base model which has the highest probability for its predicted class.
C.1 Certified Robustness
In this subsection, we characterize the certified robustness when using both strategies.
C.1.1 Ensemble-before-Smoothing
The following theorem gives an explicit method (first compute gMε (x0)Gε (x0) via sampling then
compute r) to compute the certified robust radius r for EBS protocol. This method is used for
computing the certified robust radius in our paper. All other baselines appeared in our paper also use
this method.
Proposition C.1 (Certified Robustness for Ensemble-before-Smoothing). Let GεM be an ensemble
constructed by EBS strategy. The random variable ε 〜N(0,σ2Id). Then the ensemble GM is
r-robust at point x0 where
r := σΦ-1 gMε (x0)GεM(x0) .	(38)
Here, gMε (x0)j = Pr (M(x0 + ε) = j).
The proposition is a direct application of Lemma A.1.
26
Published as a conference paper at ICLR 2022
C.1.2 Ensemble-after-Smoothing
The following theorem gives an explicit method to compute the certified robust radius r for EAS
protocol.
Theorem 5 (Certified robustness for Ensemble-after-Smoothing). Let HMε be an ensemble con-
StruCted by EAS Strategy over base models {Fi}N=ι. The random variable E 〜 N(0, σ2Id). Let
y0 = HMε (x0). For each i ∈ [N], define
ri
if GεFi (x0) = y0
if GεFi (x0) 6= y0
Then the enSemble HMε iS r-robuSt at point x0 where
r := maxi∈[N] ri + mini∈[N] ri
(39)
2
Remark. The theorem appears to be a bit counter-intuitive — picking the best smoothed model in
terms of certified robustness cannot give strong certified robustness for the ensemble. As long as the
base models have different certified robust radius (i.e., ri ’s are different), the r, certified robust radius
for the ensemble, is strictly inferior to that of the best base model (i.e., maxri). Furthermore, if there
exists a base model with wrong prediction (i.e., ri ≤ 0), the certified robust radius r is strictly smaller
than half of the best base model.
ProofofTheorem 5. Without loss of generality, we assume ri > r > •… > rN. Let the perturbation
added to x0 has L2 length δ.
When δ ≤ rN , since picking any model always gives the right prediction, the ensemble is robust.
When rN < δ ≤ r1+rN, the highest robust radius with wrong prediction is δ - rN, and We can
still guarantee that model F1 has robust radius at least r1 - δ from the smoothness of function
X → gFι (x)Gε (χo) (Salman et al., 2019). Since ri — δ ≥ r1-rN ≥ δ - rN, the ensemble will agree
on F1 or other base model with correct prediction and still gives the right prediction.
When δ > r1+rN, suppose /n is a linear model and only predicts two labels (which achieves the
tight robust radius bound according to Cohen et al. (2019)), then fN can have robust radius δ - rN for
the wrong prediction. At the same time, for any other model Fi which is linear and predicts correctly,
the robust radius is at most ri - δ. Since ri - δ < ri - δ < r1-rN < δ - rN, the ensemble can
probably give wrong prediction.
In summary, as we have shown, the certified robust radius can be at most r. For any radius δ > r,
there exist base models which lead the ensemble HM (xo + δe) to predict the label other than yo. □
C.2 Comparison of Two Strategies
In this subsection, we compare the two ensemble strategies when the ensembles are constructed from
two base models.
Corollary 4 (Smoothing Strategy Comparison). Given MMME, a Max-Margin EnSemble ConStruCted
from base models {fa, fb}. Let ε 〜N(0, σ2Id). Let GMMME be the EBS ensemble, and HMMME
be the EAS enSemble. SuppoSe at point x0 with ground-truth label y0, GεF (x0) = GεF (x0) = y0,
gFεa (x0) > 0.5, gFεb (x0) > 0.5.
Let δ be their probability differencefor class yo, i.e, δ := ∣gFα (xo)y° — gFjxo)yol,. Let Pmin be the
smaller probability for Class y0 between them, i.e., pmin := min{gFε (x0)y0 , gFε (x0)y0 }. We denote
p to the probability of choosing the correct class when the base models disagree with each other;
denote pab to the probability of both base models agreeing on the correct class:
p := Pr (MMME(xo + ε) = yo | Fa(xo + ε) 6= Fb(xo + ε) and (Fa(xo + ε) = yo orFb(xo + ε) = yo)) ,
ε
pab := Pr (Fa (xo + ε) = Fb (xo + ε) = yo ) .
ε
We have:
27
Published as a conference paper at ICLR 2022
1.	If P > 1/2 +(2 + 4(pmin - Pab)∕δ)-1, Tg > rH.
2.	Ifp≤ 1/2, rH ≥rG.
Here, rG is the certified robust radius of GεM	computed from Equation (38); and rH is the
certified robust radius of HMε	computed from Equation (39).
Remark. Since p is the probability where the ensemble chooses the correct prediction between two
base model predictions, with Max-Margin Ensemble, we think p > 1/2 with non-trivial margin.
The quantity pmin - pab and δ both measure the base model’s diversity in terms of predicted label
distribution, and generally they should be close. As a result, 1/2 +(2 + 4(pmin - Pab)∕δ)-1 ≈
1/2 + 1/6 = 2/3, and case (1) should be much more likely to happen than case (2). Therefore, EBS
usually yields higher robustness guarantee. We remark that the similar tendency also holds with
multiple base models.
Proof of Corollary 4. For convenience, define pa := gFε (x0)y0,pb := gFε (x0)y0, where pa = pb+δ
and pmin = pb .
From Proposition C.1 and Theorem 5, we have
Tg := 2 ∙ 2Φ-1 (}r(MMME(X0 + E)= y。)) ,	IrH := 2 (Φ-1(Pa) + Φ-1(Pb)).
Notice that Pr(MMME(x0 + ) = y0) = pab + p(pa +pb - 2pab), we can rewrite rG as
Tg = 2 ∙ 2Φ-1(pab + p(pa + Pb - 2pab))∙
1.	WhenP > 1/2 +(2 + 4(Pmin - Pab)∕δ)-1,
since
> 1 ,________1_______ = 1 ,_________δ_______ = Pa + Pb + δ - 2pab =	Pa - Pab
P 2	2+ 4(pminδ-pab)	2	2δ + 4(Pb - Pab)	2(Pa + Pb - 2pab)	Pa + Pb - 2Pab,
We have Pab + P(Pa + Pb - 2pab) > Pa∙ Therefore, TG > σΦ-1(Pa). Whereas, TH ≤ σ∕2 ∙
2Φ-1(Pa) = σ Φ-1(Pa). So TG > TH.
2.	When P ≤ 1/2,
Pab + P(Pa + Pb - 2Pab) ≤ Pab + 1/2 ∙ (Pa + Pb - ?Pab) = (Pa + Pb)∕2.
Therefore, TG ≤ σΦ-1((Pa + Pb)/2). Notice that Φ-1 is convex in [1/2, +∞), so Φ-1(Pa) +
Φ-1(Pb) ≥ 2Φ-1((Pa + Pb)/2), i.e., TH ≥ TG.
□
D	Robustness for Smoothed ML Ensemble: Statistical Robustness
Perspective
In this appendix, We study the robustness of ensemble models from the statistical robustness perspec-
tive. This perspective is motivated from Lemma A.1, Where the certified robust radius of a model
smoothed with GaUSSian distribution ε 〜N(0, σ2Id) is directly proportional to the probability of
the original (unsmoothed) model predicting the correct class under such noise.
We first define the notation of statistical robustness in Appendix D.1; then we show and prove the
certified robustness guarantees of WE, MME, and single models respectively in Appendix D.2; next
we use these results to compare these ensembles under both general assumptions (Appendix D.3)
and more specific uniform distribution assumptions (Appendix D.4) where several findings are
also discussed; finally, we conduct extensive numerical experiments to verify all these findings in
Appendix D.5.
28
Published as a conference paper at ICLR 2022
D. 1 Definitions of Statistical Robustness
Definition 7 ((ε, p)-Statistical Robust). Given a random variable ε and model F : Rd 7→ [C], at
point x0 with ground truth label y0, we call F is (ε, p)-statistical robust if Prε (F (x0 +ε) = y0) ≥ p.
Remark. Note that based on Lemma A.1, when ε 〜N(0, σ2Id), if F is (ε, p)-statistical robust at
point x0, the smoothed model GεF over F is (σΦ-1 (p))-robust at point x0.
The following three definitions are used in the theorem statements in the following subsections. They
can be viewed as the “confidence margins” under noised inputs x0 + ε for single model and ensemble
respectively.
Definition 8 ((ε, λ, p)-Single Confident). Given a classification model F. If at point x0 with ground-
truth label y0 and the random variable ε, we have
Pr max	f (x0 + ε)yj ≤ λ(1 - f(x0 + ε)y0 ) = 1 - p,
ε ∖yj∈[C∖-yj=yo	)
we call F (ε, λ, p)-single confident at point x0.
Definition 9 ((ε, λ, p)-WE Confident). Let MWE be a weighted ensemble defined over base models
{Fi}iN=1 with weights {wi}iN=1. If at point x0 with ground-truth y0 and random variable ε, we have
Pr max Xwifi(x0 + )yj ≤ λXwi (1 - fi(x0 + )y0)	= 1 -p, (40)
we call weighted ensemble MWE (ε, λ, p)-WE confident at point x0.
Definition 10 ((ε, λ, p)-MME Confident). Let MMME be a max-margin ensemble over {Fi}iN=1. If
at point x0 with ground-truth y0 and random variable ε, we have
Pr ^ max	fi(x0 + ε)yj ≤ λ(1 - fi(x0 + ε)y0)	= 1 - p,	(41)
ε v∈[N] '"[Cm0	))
we call max-margin ensemble MMME (ε, λ, p)-MME confident at point x0.
Note that the confidence of every single model lies in the probability simplex, and λ reflects the
confidence portion that a wrong prediction class can take beyond the true class (1 - fi (x0 + ε)).
To reduce ambiguity, we usualy use λ1 in WE Confident, λ2 in MME Confident, and λ3 in Single
Confident. Note that given λι is the weighted average and λ? the maximum over λ's of all base
models, under the same p, λ1∕λ2 ≤ 1. Furthermore, under the same P, λ∖∕λ2 reflects the adver-
sarial transferability (Papernot et al., 2016a) among base models: If the transferability is high, the
confidence scores of base models are similar (λ's are similar), and thus λι is large resulting in large
λ1∕λ2. On the other hand, when the transferability is low, the confidence scores are diverse (λ's are
diverse), and thus λ1 is small resulting in small λ1 /λ2 .
The following lemma is frequently used in our following proofs:
Lemma D.1. Suppose the random variable X satisfies EX > 0, Var(X) < ∞ and for any x ∈ R+,
Pr(X ≥ EX + x) = Pr(X ≤ EX - x), then
Pr(X ≤0) ≤ 2Vw.
Proof of Lemma D.1. Apply Chebyshev’s inequality on random variable X and notice that X is
symmetric, then we can easily observe this lemma.	□
Now we are ready to present the certified robustness for different ensemble models.
D.2 Statistical Certified Robustness Guarantees
The main results in this subsection are Theorem 6 and Theorem 7.
29
Published as a conference paper at ICLR 2022
D.2.1 Certified Robustness for Single Model
As the start point, we first show a direct proposition stating the certified robustness guarantee of the
single model.
Proposition D.1 (Certified Robustness for Single Model). Let ε be a random variable. Let F be a
classification model, which is (ε, λ3, p)-single confident. Let x0 ∈ Rd be the input with ground-truth
yo ∈ [C]. Suppose f (x。+ ε)y° follows Symmetric distribution with mean μ and variance s2, where
μ > (1 + λ-1)-1. We have
s2
Pr(F(xo + ε) = y。) ≥ 1 — P-----------------；-----.
ε ((O + ) y0)≥ P	2(μ — (ι + λ-1)-1)2
Proof of Proposition D.1. We consider the distribution of quantity Y := f(x。 + ε)y0 - λ3 (1 -
f(x。 + ε)y0 ). Since the model F is (ε, λ3,P)-single confident, with probability 1 - P, Y ≤
f (x0 + ε)yo - maxyj∈[cM=yο f (x。+ ε)yj. Wenote that since
EY =(1 + λ3)μ - λ3, Var(Y) = (1 + λ3)2s2,
from Lemma D.1,
s2
Pr(Y ≤ 0) ≤ -------------1----
'—2(μ - (1 + λ-1)-1 )2
Thus,
Pr(F (x。 + ε) = y。) = 1
- Pr(F (x。 + ε) 6= y。)
1
≥1
≥1
-	Pr f(x。 + ε)y0 - max	f(x。 + ε)yj < 0
∖	yj ∈[Cv-yj=yo	.
-	P - Pr(Y ≤ 0)
s2
—	P -..
2(μ - (1 + λ-1)-1)2
□
D.2.2 Certified Robustness for Ensembles
Now we are ready to prove the certified robustness of the Weighted Ensemble and Max-Margin
Ensemble (Theorems 6 and 7).
In the following text, we first define statistical margins for both WE and MME, and point out their
connections to the notion of (ε, P)-Statistical Robust. Then, we reason about the expectation, variance,
and tail bounds of the statistical margins. Finally, we derive the certified robustness from the statistical
margins.
Definition D.1 (X1 ; Statistical Margin for WE MWE). Let MWE be Weighted Ensemble defined
over base models {Fi}iN=1 with weights {wi}iN=1. Suppose MWE is (ε, λ1, P)-WE-confident. We
define random variable X1 which is depended by random variable ε:
N
Xι(w) := (1 + λι) ɪ2 wjfj(XO + e)yο - λιkwkι.
j=1
(42)
■ -¼ C ♦，♦	■ -¼ A Z ST C∖ . . ∙ . ∙ I > 1	♦ c∙ XC « 1 ' A/	∖ T . * 2	Λ > 1	-» r	- 1 -	Fl
Definition D.2 (X2; Statistical Margin for MME MMME). Let MMME be Max-Margin Ensemble
defined over base models {Fi}iN=1. Suppose MMME is (ε, λ2, P)-MME-confident. We define random
variable X? which is depended by random variable ε:
X2(e) :=(1 + λ2) max fi(xO + )y0 + min fi(xO + )y0 - 2λ2.
i∈[N]	i∈[N]
(43)
We have the following observation:
30
Published as a conference paper at ICLR 2022
Lemma D.2. For Weighted Ensemble,
一 ,■ . , . . 一 ， ，、
Pr (MWE(xo + ε) = yo) ≥ 1 - P - Pr (Xι(ε) < 0).
εε
For Max-Margin Ensemble,
Pr (MMME(xo + ε) = yo) ≥ 1 - P - Pr (X2(ε) < 0).
εε
Proof of Lemma D.2. (1) For Weighted Ensemble, we define the random variable X1:
N
X1() := min	Xwjfjy0/yi(x0 +).
yi∈[C];yi=yo L
j=1
Since MWE is (ε, λ1, P)-WE-confident, from Definition 9, with probability 1 - P, we have
N
X1(ε) ≥	wj (fj(x0+ε)y0 -λ2(1 - fj(x0 +ε)y0))
j=1
N
= (1 + λ2)	wjfj (x0 + ε)y0 - λι IHk = Xι(ε).
j=1
Therefore,
Pr(MWE(xo + ε) = yo) = Pr(X1(ε) ≥ 0) ≥ 1 -P - Pr(X2(ε) < 0).
(2) For Max-Margin Ensemble, we define the random variable X2 :
X2 () := max min	fiy0/yi (xo + ) + min min	fiy0/yi (xo + ).
i∈[N] yiE[c]：yi=yo	i∈N] yi∈[c]%=y0
Similarly, since MMME is (ε, λ2, P)-MME-confident, from Definition 10, with probability 1 - P, we
have
X2() ≥ max(fi(xo + ε)y0 -λ2(1 - fi(xo+ε)y0)) + min (fi(xo + ε)y0 -λ2(1 - fi(xo +ε)y0))
i∈[N]	i∈[N]
=(1+λ2) (maχ fi(χo+e)yo+minfi(χo+加)一?人?=X2(ε).
Moreover, from Lemma B.1, we know
Pr(M(XO + ε) = yo) ≥ Pr(X2(ε) ≥ 0) ≥ 1 -P - Pr(JX2(ε) < 0).
□
As the result, to quantify the statistical robustness of two types of ensembles, we can analyze the
distribution of statistical margins X1 and X2 .
Lemma D.3 (Expectation and variance of X1 and X2). Let X1 and X2 be defined by Definition D.1
and Definition D.2 respectively. Assume {fi(xo+ε)y0}iN=1 are i.i.d. and follow symmetric distribution
with mean μ and variance S. Define sf = Var(mini∈[N] fi(xo + ε)y0). We have
EXι(ε) =(1 + λ1)∣∣H∣1μ - λι∣wkι, VarXι(ε) = (1 + λι)2s2kwk2,
EX2(ε) =2(1 + λ2)μ — 2λ2,	VarX2(ε) ≤ 4(1 + λ2)2sf.
Proof of Lemma D.3.
N
EX1(ε) = (1 + λι) XEwjfj(xo + e)y0 - λι∣w∣ι = (1 + λι)∣w∣ιμ - λι∣w∣ι;
j=1
N
VarX1(ε) = (1 + λ1)2 X w2Var(fj(xo + e)y0) = (1 + λι)2s2kwk2.
j=1
31
Published as a conference paper at ICLR 2022
According to the symmetric distribution property of {fi(x0 + ε)y0 }iN=1, we have
E X2(ε)=e(i+12)(maχ fi(χo+e)yo+minfi(χo+加)一?入?
2(1 + λ2)μ - 2λ2.
Also, due the symmetry, we have
Var min fi(x0 + ε)y0 = Var max fi(x0 + ε)y0 = sf2 .
As a result,
Var X2(ε) ≤ (1 + λ2)2 ∙ 4sf.
□
From Lemma D.3, now with Lemma D.1, we are ready to derive the statistical robustness lower
bound for WE and MME.
Theorem 6 (Certified Robustness for WE). Let ε be a random variable supported on Rd. Let MWE
be a Weighted Ensemble defined over {Fi}iN=1 with weights {wi}iN=1. The MWE is (ε, λ1 , p)-WE
confident. Let x0 ∈ Rd be the input with ground-truth label y0 ∈ [C]. Assume {fi(x0 + ε)y0 }iN=1,
the confidence scores across base models for label y0, are i.i.d. and follow symmetric distribution
with mean μ and variance s2, where μ > (1 + λ-1)-1. We have
kwk2	s2
Pr(MWE(XO + ε) = y0) ≥ 1 - P - M—22 ∙ 一；--------ɪ.	(44)
ε	kwki 2 (μ -(1 + λ-1)-1)
Theorem 7 (Certified Robustness for MME). Let ε be a random variable. Let MMME be a Max-
Margin Ensemble defined over {Fi}iN=1. The MMME is (ε, λ2,p)-MME confident. Let x0 ∈ Rd
be the input with ground-truth label y0 ∈ [C]. Assume {fi(x0 + ε)y0}iN=1, the confidence scores
across base models for label yo, are i.i.d. and follow SymmetriC distribution with mean μ where
μ > (1 + λ-1)-1. Define Sf = Var(mini∈[N] fi(xo + ε)y0). We have
Pr(MMME(xo + ε) = yo) ≥ 1 - P-----------f------K.	(45)
ε	2 (μ - (1 + λ-1)-1)
Proof of Theorems 6 and 7. Combining Lemmas D.1 to D.3, we get the theorem.
□
Remark. Theorems 6 and 7 provide two statistical robustness lower bounds for both types of
ensembles, which is shown to be able to translate to certified robustness.
For the Weighted Ensemble, noticing that Xi is the weighted sum of several independent variables,
we can further apply McDiarmid’s Inequality to get another bound
Pr(MWE(xo + ε) = yo) ≥ 1 -P - exp
ε
kwki
kwk2
(μ - (1 + λ-i
which is tighter than Equation (44) when kwk2i /kwk22 is large. For average weighted ensemble,
kwk2i/kwk22 = N . Thus, when N is large, this bound is tighter.
Both theorems are applicable under the i.i.d. assumption of confidence scores. The another assumption
μ > max{(1 + λ- )-i, (1 + λ-1)-1} insures that both ensembles have higher probability of
predicting the true class rather than other classes, i.e., the ensembles have non-trivial clean accuracy.
D.3 Comparison of Certified Robustness
We first show and prove an important lemma. Then, based on the lemma and Theorems 6 and 7, we
derive the comparison corollary.
32
Published as a conference paper at ICLR 2022
Lemma D.4. For μ,λ1,λ2,C > 0, when max{λ1∕(1 + λ1 ),λ2∕(l + λ2)} < μ ≤ 1, and C < 1,
we have
μ 一 (λ21 + 1)T
μ - (λι1 + 1)-1
λ1 < λ-1
λ2 <λ2
μ -
λ2	)
1 + λ2 )
+1-
(46)
< C Q
ProofofLemma D.4.
μ - (% 1 + I)T < C
μ -(XII + I)T
^⇒
1
C
^⇒
% 1 + 1 ʌl 1 + 1
> μ(1 - C)
λ1∕λ2
%1 + λ1∕λ2	λ-1 +1
-μ(C-1 - I)
—
C T
1
^⇒
<λ-1(Cτ(λ⅛ -μ) + μ
^⇒
^⇒
μ - λ-1 + 1
λ1
λ2
¥ (1-μ+C-1
λ2 ∖
λ1
λ2
□
Now we can show and prove the comparison corollary.
Corollary 5 (Comparison of Certified Robustness). Let ε be a random variable SUPPOrted on
Rd. Over base models {Fi}N=1, let MMME be Max-Margin Ensemble, and MWE the Weighted
Ensemble with weights {wi}N=1. Let x0 ∈ Rd be the input with ground-truth label yo ∈ [C]. Assume
{力(X0 + ε)yo }N=1, the confidence scores across base models for label y0, are i.i.d, and follow
symmetric distribution with mean μ and variance s2, where μ > max{(1 + λ[1)-1, (1 + λ2^1)-1}.
Define Sf = Var(mini∈[N] fi(x0 + ε)yo) and assume Sf < s.
• When
λ1
λ2
<λ-1
—(μ - (1 + λ-1) 1)+1 - μ)	-1
(47)
for any weights {wi}N=1, MWE has higher certified robustness than Mmme.
• When
λ1
λ2
(48)
for any weights {wi}N=1, MMME has higher certified robustness than Mwe.
Here, the certified robustness is given by Theorems 6 and 7.
33
Published as a conference paper at ICLR 2022
Proof of Corollary 5. (1) According to Lemma D.4, we have
λ1 < λ-1
λ2 <λ2
(1 + λ2 1)	) + 1 - μ
μ - (λ2 1 + I)-I	sf
<
μ — (λ-1 + 1)-1 S
/Ef μ -(λ-1 + i)-1
V kwk2 μ -(λ-1 + 1)-1
<Sf
s
Ml
kwk1
s2
s2
sf
2 (μ —(1 + λ
2.
According to Theorems 6 and 7, we know the RHS in Equation (44) is larger than the RHS in
Equation (45), i.e., MWE has higher certified robustnesss than MMME.
(2) According to Lemma D.4, we have
λ2 >λ-1((√NNSf (μ- (1 + λ-1)-1) + 1 -μ)	- 1)
⇒μ - (λ-1 + 1)-1 > √Nsf
μ — (λ-1 + 1)-1
/MJ μ -(λ-1 + 1)-1 > Sf
Vkwk2 μ - (λ-1 + 1)-1 S
心	SS	>	Sf
kwk2 2 (μ - (1 + λ-1)-1)2	2 (μ - (1 + λ-1)-1)2
According to Theorems 6 and 7, we know the RHS in Equation (45) is larger than the RHS in
Equation(44), i.e., MMME has higher certified robustnesss than Mwe.	□
Remark. (1) Given that λ1 /λ2 reflects the adversarial transferability among base models Ap-
pendix D.1, the corollary implies that, MME is more robust when the transferability is high; WE is
more robust when the transferability is low.
(2)	As we can observe in the proof, there is a gap between Equation (47) and Equation (48) — when
λ1 /λ2 lies in between RHS of Equation (47) and RHS of Equation (48), it is undetermined which
ensemble protocol has higher robustness. Indeed, this uncertainty is caused by the adjustable weights
{wi }iN=1 of the Weighted Ensemble. If we only consider the average ensemble, then this gap is
closed:
λ1
λ2
MMME more robust
Z
MWE more robust
(3)	Note that we assume that Sf < S, where S2 is the variance of single variable and Sf2 is the variance
of minimum ofN i.i.d. variables. For common symmetry distributions, along with the increase ofN ,
Sf shrinks in the order of O(1/N B) where B ∈ (0, 2]. Thus, as long as N is large, the assumption
Sf < S will always hold. An exception is that when these random variables follow the exponential
distribution, where Sf does not shrink along with the increase of N . However, since these random
variables are confidence scores which are in [0, 1], they cannot obey exponential distribution.
D.4 A Concrete Case: Uniform Distribution
As shown by Saremi & Srivastava (2020) (Remark 2.1), when the input dimension d is large, the
Gaussian noise ε 〜 N(0, σ2Id) ≈ Unif(σ√dSd-1), i.e., x0 + ε is highly uniformly distributed on
34
Published as a conference paper at ICLR 2022
the (d - 1)-sphere centered at x0 . Motivated by this, we study the case where the confidence scores
{fi(x0 +ε)y0}iN=1 are also uniformly distributed.
Under this additional assumption, we can further make the certified robustness for the single model
and both ensembles more concrete.
D.4.1 Certified Robustness for Single Model
Proposition D.2 (Certified Robustness for Single Model under Uniform Distribution). Let ε be
a random variable supported on Rd. Let F be a classification model, which is (ε, λ3, p)-single
confident. Let x0 ∈ Rd be the input with ground-truth y0 ∈ [C]. Suppose f(x0 + ε)y0 is uniformly
distributed in [a, b]. We have
Pr(F (x0 + ε) = y0) ≥ 1 - p - clip
ε
1/(1 + λ3 1) - a
b-a
where
clip(x) = max(min(x, 1), 0).
Proof of Proposition D.2. We consider the distribution of quantity Y := f (x0 + ε)y0 - λ3 (1 -
f (x0 + ε)y0 ). Since the model F is (ε, λ3, p)-single confident, with probability 1 - p, Y ≤
f (X0 + ε)yo 一 maxyj三[。]：#；=#。 f (x0 + ε)yj. At the same time, because f (xo + e)#。 follows the
distribution U ([a, b]),
Y = (1 + λ3)f (x0 + ε)y0 一 λ3
follows the distribution U ([(1 + λ3)a 一 λ3, (1 + λ3)b 一 λ3]). Therefore,
λ 一 ι+∖ λ λ λ3 - (1 + λ3)a ʌ
Pr(Y ≤ 0) =Chp ((1 + λ3)(b- a) )-
As the result,
Pr (f(x0	+ ε)yo	一 max f(x0	+ ε)y,	≤ 0}	≤ P + cliP f λ3	λ∖+bλ3)a"∣	,
∖	yj∈[CLyj=yo	)	∖ (1 +	λ3)(b 一 a))
which is exactly
Pr(F(xo + ε) = yo) ≥ 1 - P - CliP ( ：3[[；+ λ3)a ) = 1 - P - Clip ( 1/(1 + % ) - a).
(1 + λ3)(b 一 a)	b 一 a
□
D.4.2 Certified Robustness for Ensembles
<-t . ∙11	1 r`	/	∖	1	/	∖	1 ∙	1 ʌ r` ∙ . ∙	1 ʌ	ι	ι 1 ʌ	C T τ ι	. ι	∙ ,`	ι ∙	, ∙ι , ∙
Still, we define X1(ε) andX2(ε) according to Definitions D.1 and D.2. Under the uniform distribution
assumption, we have the following lemma.
Lemma D.5 (Expectation and Variance of X1 and X2 under Uniform Distribution). Let X1 and X2
be defined by Definition D.1 and Definition D.2 respectively. Assume that under the distribution of ε,
the base models’ confidence scores for true class {fi(x0 + ε)y0 }iN=1 are pairwise i.i.d and uniformly
distributed in range [a, b]. We have
1
EX1(ε) = 2(1 + λ1 )kwkι(a + b) 一 λ1 IIwII 1,
E X2(ε) =(1 + λ2)(a + b) — 2λ2,
Var X1(ε) = 12(1 + λ1)2kwk2(b - a)2,
VarX2(ε) ≤ (1 + λ2)22 —............Ll (b 一 a)2.
2( ) ≤ ( + 2) N + 1 ∖N + 2 N + 17(	)
ProofofLemma D.5. We start from analyzing X1. From the definition
N
X1(W) := (1 + λ1) X Wj fj (x0 + e)yo - λ1kwk1	(42)
j=1
35
Published as a conference paper at ICLR 2022
where {fi(x0 + )y0}iN=1 are i.i.d. variables obeying uniform distribution U ([a, b]),
a+b	1
EXι9 = (1 + λ1)kwk1 -+- - λ1kwk1 = -(1 + λ1)kwk1(a + b) - λ1∣∣w∣∣1,
N1	1
Var Xι(e) = (1 + λι)2 fwj-(- - -)2 =再(1 + λι)2kw∣∣2(-- -)2.
12	12
j=1
Now analyze the expectation of X2. By the symmetry of uniform distribution, we know
_ ʌ , .	, , -、________ ， . 一- ,. ., -、 一一
EX2(w) =	(1	+ λ2) ∙ 2E fi(xo	+ β)yo	— 2λ2	= (1	+ λ2)(a + -)	— 2λ2.
To reason about the variance, we need the following fact:
Fact D.1. Let x1, x2, . . . , xn be uniformly distributed and independent random variables. Specifi-
cally, for each 1 ≤ i ≤ n, Xi 〜U([-, -]). Then we have
Var ( min Xil = Var ( max Xil = -^―-
1≤i≤n	1≤i≤n	n + 1
n+1
( - )2 .
Observing that each i.i.d. fi(x0 + ε)y0 is eXactly identical to Xi in Fact D.1, we have
Var (maxfi(x0+ e)yo+ mN]fi(x0+ e)yo) ≤ N4Γ (n⅛ - N+ι)(- - -)2.
Therefore,
VarX2(ε) ≤ (1 + λ2)22 —2------------—(- - -)2.
2() ≤ ( + 2) N + 1 IN + 2 N +11(	)
□
Proof of Fact D.1. From symmetry of uniform distribution, we know Var (min1≤i≤n Xi) =
Var (max1≤i≤n Xi). So here we only consider Y := min1≤i≤n Xi. Its CDF F and PDF f can
be easily computed:
F(y) = 1-Pr min Xi ≥ y
(- - y)n-1
f(y) = F (y) = n'(- - -)n , where y ∈ [-,-].
Hence,
EY =Zbyf(y)dy
a
y(- - y)n + (n + 1)-1 (- - y)n+1 a - - -
b = - + E，
(- -)n
E Y2 = Z y2f(y)dy = Z ny2 (--y))n dy
a	a	--
—
(n⅛
—
1
-	(产)ny2∣b + ɪ (-	y + Z	dy) Ib
-	- - a n + 1	(- - -)n	(- - -)n	a
-	(--y)n 2lb	,	2	(-(b- y)n+1	-	1	(b- y)n+2	) Ib
I- - -) y Ia	n +11	(- - -)n y n + 2 (- - -)n	) Ia
22
-+ n∏(- - -)- + (n +1)(n + 2)(- - -) .
As the result, Var Y = EY2 -
( - -)2.
□
Now, similarly, we use Lemma D.1 to derive the statistical robustness lower bound for WE and MME.
We omit the proofs since they are direct applications of Lemma D.5, Lemma D.1, and Lemma D.2.
36
Published as a conference paper at ICLR 2022
Theorem 8 (Certified Robustness for WE under Uniform Distribution). Let MWE be a Weighted
Ensemble defined over {Fi}iN=1 with weights {wi}iN=1. Let x0 ∈ Rd be the input with ground-truth
label y0 ∈ [C]. Let ε be a random variable supported on Rd. Under the distribution of ε, suppose
{fi(x0 + ε)y0}iN=1 are i.i.d. and uniformly distributed in [a, b]. The MWE is (ε, λ1,p)-WEconfident.
Assume a+b > -1 -ι. We have
2	1+λ
d K2
Pr(MWE(xo + ε) = yO) ≥ 1 - P--——,
ε	12
kwk22	b - a
where dw = kWk2，K1 = a+b	1 .
1	2	1+λ1-1
(49)
Theorem 9 (Certified Robustness for MME under Uniform Distribution). Let MMME be a Max-
Margin Ensemble over {Fi}iN=1. Let x0 ∈ Rd be the input with ground-truth label y0 ∈ [C]. Let ε be
a random variable supported on Rd. Under the distribution of ε, suppose {fi(x0 + ε)y0}iN=1 are i.i.d.
and uniformly distributed in [a, b]. MMME is (ε, λ2,p)-MME confident. Assume a+b > 1—口. We
2	1+λ2
have
Pr(MMME(XO + ε) = yO) ≥ 1 - P-N 2 ,
ε4
2	2	1	b- a	(50)
where CN = n+i N+τ- - n+i) ,K2 = 亍F.
1+λ-1
D.4.3 Comparison of Certified Robustness for Ensembles
Now under the uniform distribution, we can also have the certified robustness comparison.
Corollary 6 (Comparison of Certified Robustness under Uniform Distribution). Over base models
{Fi }iN=1, let MMME be Max-Margin Ensemble, and MWE the Weighted Ensemble with weights
{wi }iN=1 . Let xO ∈ Rd be the input with ground-truth label yO ∈ [C]. Let ε be a random variable
supported on Rd. Under the distribution of ε, suppose {fi(xO + ε)y0}iN=1 are i.i.d. and uniformly
distributed with mean μ. Suppose MWE is (ε, λι,p)-WE confident, and MMME is (ε, λ2,p)-MME
confident Assume μ > max
• When
(51)
(
.
1
1
1+λ-1, 1+λ-1
MWE has higher certified robustness than MMME.
• When
λ1
λ2
> λ2-1
1
1 + λ-1
(52)
+ 1 - μ )
MMME has higher certified robustness than MWE.
• When
μ∏⅛ Γ
- 2,
(53)
N > 6 1 -
for any λ1, MMME has higher or equal certified robustness than MWE.
Here, the certified robustness is given by Theorems 8 and 9.
ProofofCorollary 6. First, We notice that a uniform distribution with mean μ can be any distribution
U([a, b]) where (a + b)/2 = μ. We replace μ by (a + b)/2.
Then (1) and (2) follow from Lemma D.4 similar to the proof of Corollary 5.
37
Published as a conference paper at ICLR 2022
(3) Since

N>6 1-
- 2 =⇒
μ)	<1
+ 1 - μ )
< 1,
the RHS of Equation (52) is smaller than 0. Thus, for any λ1, since λ1∕λ2 > 0, the Equation (48) is
satisfied. According to (2), MMME has higher certified robustnesss than Mwe.	□
Remark. Comparing to the general corollary (Corollary 5), under the uniform distribution, we
have an additional finding that when N is sufficiently large, we will always have higher certified
robustness for Max-Margin Ensemble than Weighted Ensemble. This is due to the more efficient
variance reduction of Max-Margin Ensemble than Weighted Ensemble. As shown in Lemma D.5, the
quantity VarX(ε"(EX(ε))2 for Weighted Ensemble is Ω(1∕N), while for Max-Margin Ensemble
is O(1/N2). As the result, when N becomes larger, Max-Margin Ensemble has higher certified
robustness.
We use uniform assumption here to give an illustration in a specific regime. We think it would be
an interesting future direction to generalize the analysis to other distributions such as the Gaussian
distribution that corresponds to locally linear classifiers. The result from these distribution may be
derived from their specific concentration bound for maximum/minimum i.i.d. random variables as
discussed at the end of Appendix D.3.
D.5 Numerical Experiments
To validate and give more intuitive explanations for our theorems, we present some numerical
experiments.
D.5.1 Ensemble Comparison from Numerical Sampling
As discussed in Appendix D.1, λ1∕λ2 reflects the transferability across base models. It is challenging
to get enough amount of different ensembles of various transferability levels while keeping all
other variables controlled. Therefore, we simulate the transferability of ensembles numerically
by varying λ1∕λ2 (see the definitions of λι and λ2 in Definitions 9 and 10), and sampling the
confidence scores {fi(xo + ε)y0} and {maxj∈[c]j=yo fi(xo + ε)} under determined λι and λ2.
For each level of λ1∕λ2, with the samples, we compute the certified robust radius r using randomized
smoothing (Lemma A.1) and compare the radius difference of Weighted Ensemble and Max-Margin
Ensemble. According to Corollary 5, we should observe the tendency that along with the increase of
transferability λ1∕λ2 , Max-Margin Ensemble would gradually become more certifiably robust than
Weighted Ensemble.
Figure 4 verifies the trends: with the increase of λ1∕λ2, MME model tends to achieve higher certified
radius than WE model. Moreover, we notice that under the sameλ1∕λ2, with the larger number of
base models N, the MME tends to be relatively more certifiably robust compared with WE. This is
because we sample the confidence score uniformly and under the uniform distribution, MME tends to
be more certifiably robust than WE when the number of base models N becomes large, according to
Corollary 6.
The concrete number settings ofλ1,λ2, and the sampling interval of confidence scores are entailed in
the caption of Figure 4.
D.5.2 Ensemble Comparison from Certified Robustness Plotting
In Corollary 6, we derive the concrete certified robustness for both ensembles and the single model
under i.i.d. and uniform distribution assumption. In fact, from the corollary, we can directly compute
the certified robust radius without sampling, as long as we assume the added noise ε is Gaussian. In
Figure 5, we plot out such certified robust radius for the single model, the WE, and the MME.
38
Published as a conference paper at ICLR 2022
(a) # of base models N = 3
(b) # of base models N = 10
(c) # of base models N = 20
Figure 4: Signed certified robust radius difference between MME and WE by λ∖∕λ2 under different
numbers of base models N. Here we fix λ2 to be 0.95 and uniformly sample λ1 ∈ [0.8, 0.95). The
confidence score for the true class on each base model is uniformly sampled from [a, b], where a
is sampled from [0.3, 1.0) and b is sampled from [a, 1.0) uniformly for each instance. Blue points
correspond to the negative radius difference (i.e., WE has larger radius than MME) and Red points
correspond to the positive radius difference (i.e., MME has larger radius than WE).
Concretely, in the figure, we assume that the true class confidence score for each base model is
i.i.d. and uniformly distributed in [a, b]. The Weighted Ensemble is (ε, λ1, 0.01)-WE confident; the
Max-Margin Ensemble is (ε, λ2, 0.01)-MME confident; and the single model is (ε, λ3, 0.01)-MME
confident. We guarantee that λ1 ≤ λ3 ≤ λ2 to simulate the scenario that ensembles are based
on the same set of base models to make a fair comparison. We directly apply the results from
our analysis (Theorem 8, Theorem 9, Proposition D.2) to get the statistical robustness for single
model and both ensembles. Then, we leverage Lemma A.1 to get the certified robust radius (with
σ = 1.0, N = 100000 and failing probability α = 0.001 which are aligned with realistic setting).
The x-axis is the number of base models N and the y-axis is the certified robustness. We note that N
is not applicable to the single model, so we plot the single model’s curve by a horizontal red dashed
line.
From the figure, we observe that when the number of base models N becomes larger, both ensembles
perform much better than the single model. We remark that when N is small, the ensembles have
0 certified robustness mainly because our theoretical bounds for ensembles are not tight enough
with the small N . Furthermore, we observe that the Max-Margin Ensemble gradually surpasses
Weighted Ensemble when N is large, which conforms to our Corollary 6. Note that the left sub-figure
has smaller transferability λ∖∕λ2 and the right subfigure has larger transferability λ1∕λ2, it again
conforms to our Corollary 5 and its following remarks in Appendix D.3 that in the left subfigure the
Weighted Ensemble is relatively more robust than the Max-Margin Ensemble.
D.5.3 Ensemble Comparison from Realistic Data
We study the correlation between transferability λ1∕λ2 and whether Weighted Ensemble or Max-
Margin Ensemble is more certifiably robust using realistic data.
By varying the hyper-parameters of DRT, we find out a setting where over the same set of base
models, Weighted Ensemble and Max-Margin Ensemble have similar certified robustness, i.e., for
about half of the test set samples, WE is more robust; for another half, MME is more robust. We
collect 1, 000 test set samples in total. Then, for each test set sample, we compute the transferability
λ1 /λ2 and whether WE or MME has the higher certified robust radius. We remark that λ1 and λ2 are
difficult to be practically estimated so we use the average confidence portion as the proxy:
• For WE,
—	maχyj∈[CM=y° Pill Wifi(X0 + Oyj
λ 1 — Eε	RT	.
Pi=1 wi(1 - fi(x0 + ε)y0 )
• For MME,
λ2 — Eε max maXyj：[C]：yj=y0fi(x0 + ε)yj .
i∈[N ]	(1 - fi(x0 + ε)y0 )
39
Published as a conference paper at ICLR 2022
SSafSnqOH P9ci=!pθο
Certified Robustness Comparison
(a = 0.2, b = 0.3fΛ1 = 0.29, Λ2 = 0.31fΛ3 = 0.30)
1.0
WE
MME
Single Model
5	10	15	20	25	30
SSafSnqOH P9ci=!pθο
Certified Robustness Comparison
WE
MME
Single Model
5	10	15	20	25	30
0.2
0.0
ON
0.0
# of Base Models (Af)
# of Base Models (Af)
(a)	[a, b] = [0.2, 0.3], λ1 = 0.29, λ2
0.31, λ3 = 0.30.
(b)	[a, b] = [0.3, 0.4], λ1 = 0.48, λ2
0.50, λ3 = 0.49.
Figure 5: Comparison of certified robustness (in terms of certified robust radius) of Max-Margin
Ensemble, Weighted Ensemble, and single model under concrete numerical settings. The y-axis
is the certified robustness and the x-axis is the number of base models. The confidence score for
the true class is uniformly distributed in [a, b]. The Weighted Ensemble (shown by blue line) is
(ε, λ1, 0.01)-WE confident; the Max-Margin Ensemble (shown by green line) is (ε, λ2, 0.01)-MME
confident; and the single model (shown by red line) is (ε, λ3, 0.01)-MME confident.
Figure 6: ROC curve of the I[MME has higher certified robustness] classification task with the
threshold variable X .
Now we study the correlation between
X := λ1∕λ2 - RHS of Equation (48) and Y := I [MME has higher certified robustness].
To do so, we draw the ROC curve where the threshold on X does binary classification on Y. The
curve and the AUC score is shown in Figure 6. From the ROC curve, we find that X and Y are
apparently positively correlated since AUC = 0.66 > 0.5, which again verifies Corollary 5. We
remark that besides X, other factors such as non-symmetric or non-i.i.d. confidence score distribution
may also play a role.
Closing Remarks. The analysis in this appendix mainly shows two major findings theoretically
and empirically: (1) MME is more robust when the adversarial transferability is high; while WE is
more robust when the adversarial transferability is low; (2) If each fi (x0 + ε)y0 follows uniform
distribution, when number of base models N is sufficiently large, the MME is always more certifiably
robust. Our analysis does have limitations: we assume the symmetric and i.i.d. distribution of
fi(x0 + ε)y0 or even more strict uniform distribution to derive these findings. Though they model the
real-world scenario in some extent as our realistic data results show, they are not perfect considering
the transferability among base models and boundedness of confidence scores. We hope current
40
Published as a conference paper at ICLR 2022
analysis can open an angle of theoretical analysis of ensembles and leave a more general analysis as
the future work.
E Analysis of Alternative Design of DRT
In the main text, we design our DRT based on GD Loss
Lgd(x0 )ij = IIVxfy0/yi2)(X0)+ Vxf7/yj2) (xo)∣∣2	⑼
and CM Loss
LCM(x0)ij = fiyi()/y0(x0) + fjyj /y0(x0).	(10)
Following the convention, we apply Gaussian augmentation to train the models, i.e., replacing x0 by
xo + ε where ε 〜N(0, σ2Id) in Equation (9) and Equation (10). We apply these two regularizers to
every valid base model pair (Fi, Fj), where the valid pair means both base models predict the ground
truth label y0: Fi (x0 + ε) = Fj (x0 + ε) = y0.
One may concern that in the worst case, there could be O(N2) valid pairs, i.e., O(N 2) regularization
terms in the training loss. However, we should notice that each base model Fi only appears in
O(N) valid pairs. Therefore, when N is large, we can optimize DRT by training iteratively, i.e., by
regularizing each base model one by one to save the computational cost.
An alternative design inspired from the theorems (e.g., Theorem 1) is to use overall summation
instead of pairwise summation, which directly correlates with Iyi (Equation (6)):
L0GD(x0) = ∣∣∣ XN Vxfiy0/yi(2) (x0)∣∣∣2,	(54)
i=1
L0CM(x0) = XN fiyi(2)/y0 (x0).	(55)
i=1
Although this design appears to be more aligned with the theorem and more efficient with O(N)
regularization terms, it also requires all base models Fi to have the same runner-up prediction yi(2) as
observed from both theorem and intuition (otherwise diversified gradients and confidence margins are
for different and independent labels that are meaningless to jointly optimize). It is less likely to have
all base models having the same runner-up prediction than a pair of base models having the same
runner-up prediction especially in the initial training phase. Therefore, this alternative design will
cause fewer chances of meaningful optimization than the previous design and we use the previous
design for our DRT in practice.
F Experiment Details
Baselines. We consider the following state-of-the-art baselines for certified robustness: (1) Gaussian
smoothing (Cohen et al., 2019) trains a smoothed classifier by applying Gaussian augmentation.
2. MACER (Zhai et al., 2019): Adding the regularization term to maximize the certified radius
R = 2 (PA - PB) on training instances. (2) SmoothAdv (Salman et al., 2019) combines adver-
sarial training with Gaussian augmentation. (3) MACER (Zhai et al., 2019) improves a single
model’s certified robustness by adding regularization terms to minimize the Negative Log Likelihood
(NLL) between smoothed classifier’s output gF (x) and label y, and maximize the certified radius
R = 2(Φ-1(gF(x)y) — Φ-1(maXyo=y gF(x)yo)), Where ε 〜N(0,σ2Id) and gF is as defined
in Equation (3). (4) Stability (Li et al., 2019) maintains the stability of the smoothed classifier
gF by minimizing the Renyi Divergence between gF(x) and gF(x + ε) where ε 〜 N(0,σ2Id).
(5) SWEEN (Liu et al., 2020) builds smoothed Weighted Ensemble (WE), which is the only prior
work computing certified robustness for ensemble to our knowledge.
Evaluation Metric. We report the standard certified accuracy under different L2 radii r’s as our
evaluation metric following Cohen et al. (2019), which is defined as the fraction of the test set
samples that the smoothed classifier can certify the robustness within the L2 ball of radius r. Since
the computation of the accurate value of this metric is intractable, we report the approximate certified
41
Published as a conference paper at ICLR 2022
test accuracy (Cohen et al., 2019) sampled through the Monte Carlo procedure. For each sample,
the robustness certification holds with probability at least 1 - α. Following the literature, we choose
α = 0.001, n0 = 100 for Monte Carlo sampling during prediction phase, and n = 105 for Monte
Carlo sampling during certification phase. On MNIST and CIFAR-10 we evaluated every 10-th
image in the test set, for 1, 000 images total. On ImageNet we evaluated every 100-th image in the
validation set, for 500 images total. This evaluation protocol is the same as prior work (Cohen et al.,
2019; Salman et al., 2019).
F.1 MNIST
Baseline Configuration. Following the literature (Salman et al., 2019; Jeong & Shin, 2020; Zhai
et al., 2019), in each batch, each training sample is Gaussian augmented twice (augmenting more
times yields negligible difference as Salman et al. (2019) show). We choose Gaussian smoothing
variance σ ∈ {0.25, 0.5, 1.0} for training and evaluation for all methods. For SmoothAdv, we
consider the attack to be 10-step L2 PGD attack with perturbation scale δ = 1.0 without pretraining
and unlabelled data augmentation. We reproduced results similar to their paper by using their
open-sourced code1.
Training Details. First, we use LeNet architecture and train each base model for 90 epochs. For the
training optimizer, we use the SGD-momentum with the initial learning rate α = 0.01. The learning
rate is decayed for every 30 epochs with decay ratio γ = 0.1 and the batch size equals to 256. Then,
we apply DRT to finetune our model with small learning rate α for another 90 epochs. We explore
different DRT hyper-parameters ρ1, ρ2 together with the initial learning rate α, and report the best
certified accuracy on each radius r among all the trained ensemble models.
Table 4: Certified accuracy of DRT-(ρ1, ρ2) under different radii r on MNIST dataset. Smoothing
parameter σ = 0.25. The grey rows present the performance of the proposed DRT approach. The
brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75
Gaussian (Cohen et al., 2019)	-	-	ɪr	^979^	^966^	93.0
SmoothAdv (Salman et al., 2019)	-	-	99.1	98.4	97.0	96.3
MME (GaUSSian)	-	-		^984^	-96.8-	93.6
	0.1	-0^	99.4	98.3	97.5	95.1
DRT + MME (Gaussian)	0.1	0.5	99.5	98.6	97.1	94.8
	0.2	0.5	99.5	98.5	97.4	95.1
MME (SmoothAdv)	-	-	~92Γ		-97.3-	96.4
	0.1	^0^	99.1	98.4	97.5	96.4
DRT + MME (SmoothAdv)	0.1	0.5	99.1	98.3	97.6	96.7
	0.2	0.5	99.1	98.4	97.5	96.6
WE (Gaussian)	-	-	~92Γ	^984^	^969^	93.7
	0.1	^0^	99.5	98.4	97.3	95.1
DRT + WE (Gaussian)	0.1	0.5	99.5	98.6	97.1	94.9
	0.2	0.5	99.5	98.5	97.3	95.3
WE (SmoothAdv)	-	-	~92Γ		^974^	96.4
	0.1	^0^	99.1	98.4	97.5	96.5
DRT + WE (SmoothAdv)	0.1	0.5	99.1	98.2	97.6	96.6
	0.2	0.5	99.0	98.4	97.5	96.7
Trend of Certified Accuracy with Perturbation Radius. We visualize the trend of certified accu-
racy along with different perturbation radii on different smoothing parameters separately in Figure 7
and Figure 8. For each radius r, we present the best certified accuracy among all the trained models.
We can notice that while simply applying MME or WE protocol could slightly improve the certified
accuracy, DRT could significantly boost the certified accuracy on different radii.
Average Certified Radius. We report the Average Certified Radius (ACR) (Zhai et al., 2019): ACR
=|s1j P(X y)∈stest R(x, y), where Stest refers to the test set and R(χ, y) the Certifed radius on testing
1https://github.com/Hadisalman/smoothing-adversarial/
42
Published as a conference paper at ICLR 2022
Table 5: Certified accuracy of DRT-(ρ1, ρ2) under different radii r on MNIST dataset. Smoothing
parameter σ = 0.50. The grey rows present the performance of the proposed DRT approach. The
brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75
Gaussian (Cohen et al., 2019)	-	-	99.0	97.7	96.4	94.7	90.0	83.0	68.2	43.5
SmoothAdv (Salman et al., 2019)	-	-	98.6	98.0	97.0	95.4	93.0	87.7	80.2	66.3
MME (GaUSSian)	-	-	99.0		97.7	96.8	^949^	90.5	84.3	69.8	48.5
		2.0	99.1	98.4	97.2	95.2	92.6	86.5	74.3	54.1
		5.0	99.1	98.6	97.1	95.3	92.6	86.2	74.0	54.3
		2.0	99.2	98.3	97.4	95.5	92.1	86.4	74.7	55.6
	0.5	5.0	99.0	98.2	97.3	95.1	91.6	84.8	73.7	52.4
		10.0	99.1	98.1	97.1	95.0	91.8	85.7	73.3	51.4
	1.0	5.0	99.1	98.2	97.2	95.2	92.2	85.8	74.4	54.4
		0.1	98.8	98.0	96.8	94.7	91.5	86.5	75.5	59.1
DRT + MME (Gaussian)		0.2	98.9	98.1	96.9	95.1	92.1	85.8	76.1	56.4
		0.5	98.7	98.1	96.8	95.2	92.1	85.8	76.0	56.9
	10.0	2.5	99.0	98.3	97.0	95.1	92.4	85.8	75.7	57.0
		5.0	99.0	98.1	96.8	95.0	91.9	85.5	74.4	54.6
		10.0	99.0	98.2	96.9	95.1	91.9	85.5	74.6	54.5
		2.5	98.7	98.0	96.7	95.1	91.7	86.4	75.6	59.8
	80.0	5.0	98.5	97.7	96.5	94.9	91.9	85.9	76.1	59.3
		25.0	98.9	98.0	96.9	94.9	92.2	85.7	76.5	58.3
MME (SmoothAdv)	-	-I 98.6		98.0	97.0 I	-95.5-	93.2	88.1	80.6	67.8
		0.5	98.4	97.8	97.0	-95.5-	92.7	87.7	80.9	67.9
	0.1	1.0	98.4	97.9	97.0	95.5	92.9	88.1	80.8	67.2
		5.0	98.5	98.2	97.0	95.4	93.1	88.4	81.2	68.3
		0.5	98.4	97.7	97.2	95.3	92.3	87.7	79.3	68.4
		2.0	98.4	97.6	97.1	95.3	92.3	87.8	80.2	67.7
	0.2	5.0	98.4	97.8	97.1	95.2	93.0	87.9	80.3	68.3
		10.0	98.4	97.8	97.1	95.3	92.9	88.5	81.0	67.6
		5.0	98.4	97.5	97.1	95.0	92.4	87.7	79.7	68.3
	0.3	10.0	98.5	97.7	97.0	95.2	92.6	88.5	81.1	68.1
	∩ c	2.0	98.5	97.3	96.6	94.3	91.6	86.7	79.5	68.6
DRT + MME (SmoothAdv)	0.5	5.0	98.4	97.5	96.9	94.6	92.0	87.5	80.1	67.8
	ι ∩	0.5	97.7	96.8	95.5	92.3	89.6	84.1	76.7	66.3
	1.0	1.0	97.9	96.6	95.7	92.6	89.7	84.6	77.5	66.2
		0.1	95.4	93.3	91.2	88.1	83.8	76.8	68.3	59.9
		0.2	95.5	93.7	90.9	87.7	82.0	75.7	68.7	59.6
		0.5	95.0	93.3	91.1	87.8	82.6	76.3	68.2	59.7
		2.5	94.6	92.9	90.1	86.3	81.6	76.0	69.6	62.5
		5.0	94.3	93.1	90.0	86.1	81.9	76.3	70.0	63.6
		10.0	94.9	93.4	91.3	87.3	83.2	78.2	71.8	65.9
	Q∩ ∩	2.5	87.7	84.4	79.9	75.0	70.5	65.5	58.9	50.5
	80.0	5.0	88.5	85.1	81.0	76.8	71.4	67.4	60.6	52.1
WE (Gaussian)	-	-I 99.0		97.8	96.8 I 94.9		90.6	84.5	70.4	48.2
		2.0	99.2	98.4	97.2	95.2	92.5	86.2	74.3	53.5
		5.0	99.1	98.6	97.1	95.3	92.6	86.4	74.2	54.4
		2.0	99.2	98.3	97.4	95.6	92.1	86.5	74.7	55.3
	0.5	5.0	99.0	98.1	97.4	95.1	91.4	84.8	73.7	52.5
		10.0	99.1	98.2	97.1	95.1	91.7	85.4	73.5	51.0
	1.0	5.0	99.1	98.2	97.2	95.2	92.2	85.9	75.1	55.3
		0.1	98.8	98.0	96.8	94.8	91.6	86.7	76.3	59.0
DRT + WE (Gaussian)		0.2	98.8	98.1	97.0	95.0	92.1	86.0	75.7	56.8
		0.5	98.8	98.1	96.9	95.2	92.2	86.0	76.2	57.0
	10.0	2.5	98.9	98.3	97.0	95.1	92.4	85.9	76.2	56.3
		5.0	99.0	98.1	96.9	95.0	91.8	85.5	74.5	55.0
		10.0	99.0	98.1	96.9	95.1	91.9	85.7	74.3	54.4
		2.5	98.7	97.9	96.7	95.1	91.8	86.2	75.5	60.1
	80.0	5.0	98.4	97.8	96.8	95.0	91.9	86.2	75.6	60.2
		25.0	99.0	98.1	96.9	94.9	92.1	85.9	76.7	58.4
WE (SmoothAdv)	-	-I 98.7		98.0	97.0 I	-95.5-	93.4	88.2	81.1	67.9
		0.5	98.4	97.8	97.0	-95.5-	92.7	87.8	80.6	68.1
	0.1	1.0	98.5	97.9	97.0	95.5	93.1	88.0	81.2	67.7
		5.0	98.5	98.2	97.0	95.4	93.3	88.5	81.4	68.6
		0.5	98.4	97.7	97.2	95.4	92.3	87.6	79.7	68.0
		2.0	98.4	97.6	97.1	95.3	92.3	87.8	80.6	68.1
		5.0	98.4	97.9	97.1	95.1	93.0	88.2	80.4	69.1
		10.0	98.3	97.8	97.1	95.3	92.9	88.4	80.7	68.1
		5.0	98.4	97.5	97.1	95.0	92.4	87.9	79.9	69.3
	0.3	10.0	98.4	97.7	97.0	95.2	92.6	88.4	81.1	68.2
	∩ c	2.0	98.4	97.3	96.6	94.3	91.8	86.7	79.6	68.1
DRT + WE (SmoothAdv)	0.5	5.0	98.4	97.5	96.9	94.7	92.0	87.7	79.7	67.7
		0.5	97.8	96.8	95.4	92.3	89.7	84.1	77.0	65.9
	1.0	1.0	97.9	96.6	95.6	92.7	89.8	84.4	77.4	66.2
		0.1	95.3	93.5	91.2	88.7	83.8	76.8	68.9	60.1
		0.2	95.4	93.8	90.9	88.1	83.2	76.6	69.1	59.9
		0.5	95.1	93.5	90.9	87.7	83.6	76.6	69.1	59.8
	10.0	2.5	94.8	93.0	90.5	86.8	82.1	75.1	69.1	62.0
		5.0	94.4	93.3	90.1	86.6	82.0	75.8	70.0	63.2
		10.0	94.7	93.3	90.5	86.8	82.5	77.2	71.8	65.6
	Q∩ ∩	2.5	87.8	83.1	78.5	74.0	67.7	62.3	54.9	47.0
	80.0	5.0	88.4	84.2	79.9	75.3	69.3	63.7	56.5	48.7
43
Published as a conference paper at ICLR 2022
Table 6: Certified accuracy of DRT-(ρ1, ρ2) under different radii r on MNIST dataset. Smoothing
parameter σ = 1.00. The grey rows present the performance of the proposed DRT approach. The
brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	2.50
Gaussian (Cohen et al., 2019)	-	-	96.5	94.3	~9LΓ~	87.0	80.2	71.8	60.1	46.6	33.0	20.5	11.5
SmoothAdv (Salman et al., 2019)	-	-	95.3	93.5	89.3	85.6	80.4	72.8	63.9	54.6	43.2	34.3	24.0
MME (GaUssian)	-		-	96.4	94.8	91.3	87.7	80.8	73.5	61.0	48.8	34.7	23.4	12.7
	0.5	2.0	96.0	^939^	90.1	86.3	80.7	73.2	63.0	52.0	38.9	26.9	15.6
		5.0	95.8	94.1	90.0	86.6	80.4	72.9	62.4	51.3	40.0	27.8	16.5
DRT + MME (Gaussian)	1.0	5.0	95.3	93.1	89.7	85.8	80.0	72.7	62.9	52.0	39.8	28.5	17.6
		0.5	91.3	89.7	85.6	78.8	73.3	65.8	59.1	52.2	43.9	36.0	29.1
	5.0	2.5	92.5	90.2	87.7	82.0	76.3	69.6	60.7	52.8	43.4	35.4	26.0
		5.0	93.2	90.6	88.1	82.9	78.1	70.6	62.3	52.5	43.3	34.4	23.8
MME (SmoothAdv)	-		-F95：4		93.4	89.3	86.1	80.7	73.1	65.0	55.0	44.8	35.0	25.2
	0.2	2.0	94.1	^91T9^	88.6	84.5	79.4	72.4	63.4	54.0	45.0	36.6	27.3
		5.0	94.1	91.6	88.9	84.4	79.3	72.3	63.2	54.2	46.1	36.9	28.5
	0.5 1.0	2.0	92.8	91.3	87.7	83.2	77.3	71.2	62.2	53.3	45.5	37.0	29.7
DRT + MME (SmoothAdv)		5.0 5.0	92.5 92.1	91.2 90.0	88.0 86.3	83.5 81.3	78.5 76.2	71.2 69.4	62.2 61.1	53.8 54.0	45.2 46.4	37.7 38.6	29.2 31.1
	5.0	1.0	89.3	86.5	82.2	76.5	70.5	62.8	54.6	48.5	41.4	35.2	29.2
		5.0	87.6	83.3	78.8	73.1	67.4	61.8	56.2	50.5	44.9	38.4	32.8
	10.0	20.0	82.7	79.6	75.3	72.0	67.9	63.3	58.6	51.1	46.6	40.3	34.7
WE (Gaussian)	Γ^^	-	~96T~	94.9	91.3	87.7	80.7	73.5	61.1	49.0	35.2	23.7	12.9
	0.5	2.0	95.9	93.9	90.2	86.3	80.7	73.2	63.2	51.9	38.6	27.0	15.5
		5.0	95.9	94.1	90.0	86.4	80.4	73.1	62.3	51.7	39.8	27.5	16.4
DRT+WE (Gaussian)	1.0	5.0	95.4	93.1	89.7	85.8	80.0	72.7	62.9	52.1	39.9	28.5	17.8
		0.5	91.3	89.8	85.9	79.0	73.4	65.5	59.2	52.2	43.9	35.4	28.8
	5.0	2.5	92.4	90.2	87.8	81.7	76.2	69.5	60.5	52.5	43.5	35.8	26.8
		5.0	92.9	90.7	88.0	82.7	78.1	70.5	62.3	52.6	43.1	34.5	24.4
WE (SmoothAdv)	-		-F95：2		93.4	89.4	86.2	80.8	73.3	64.8	55.1	44.7	35.2	24.9
	0.2	2.0	94.2	^91T9^	88.6	84.5	79.6	72.5	63.7	53.9	44.9	36.4	27.3
		5.0	94.2	91.6	88.9	84.4	79.3	72.5	63.3	54.3	45.9	36.9	28.7
	0.5 1.0	2.0	92.6	91.3	87.7	83.1	77.5	71.1	62.4	53.3	45.3	36.7	29.3
DRT + WE (SmoothAdv)		5.0 5.0	92.5 92.1	91.2 90.0	88.0 86.4	83.4 81.4	78.5 76.3	71.1 69.7	62.3 61.1	53.7 54.0	45.3 46.4	37.8 38.4	29.5 31.0
	5.0	1.0	89.1	86.5	82.5	76.7	70.5	63.0	54.8	48.4	41.5	35.3	29.1
		5.0	87.9	83.4	78.8	73.0	67.5	61.6	56.2	50.4	44.8	38.5	32.7
	10.0	20.0	82.0	79.1	75.2	71.8	67.6	63.4	58.6	51.2	46.7	40.2	34.7
Radius
AUeJnUUq PaEEU
Radius
(a)	σ = 0.25
(b)	σ = 0.50
(c)	σ = 1.00
Figure 7:	Certified accuracy for ML ensembles with Gaussian smoothed base models, under smooth-
ing parameter σ ∈ {0.25, 0.50, 1.00} separately on MNIST.
X.00	l∙0 _	_	1-0
.0	0.2	0.4	0.6	0.8
Radius
(a)	σ = 0.25
酒9
e
§0.8
W
B0-7
!o.6
O
0.5
---SmoothAdv
MME (SmoothAdv)
—— DRT+MME (SmoothAdv)
WE (SmoothAdv)
——DRT+WE (SmoothAdv)
AUaJnUUq P3g-t3u
0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Radius
(c)	σ = 1.00
ɪ-θ 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Radius
(b)	σ = 0.50
Figure 8:	Certified accuracy for ML ensembles with SmoothAdv base models, under smoothing
parameter σ ∈ {0.25, 0.50, 1.00} separately on MNIST.
44
Published as a conference paper at ICLR 2022
sample (x, y). We evaluate ACR of our DRT-trained ensemble trained with σ ∈ {0.25, 0.5, 1.0}
smoothing parameter and compare it with other baselines. Results are shown in Table 7.
We can clearly see that our DRT-trained ensemble could still achieve the highest ACR on all the
smoothing parameter settings. Especially on σ = 1.00, our improvement is significant.
Table 7: Average Certified Radius (ACR) of DRT-trained ensemble trained with different smoothing
parameter σ ∈ {0.25, 0.50, 1.00} on MNIST dataset, compared with other baselines. The grey rows
present the performance of the proposed DRT approach. The brackets shows the base models we use.
Radius r	σ = 0.25	σ = 0.50	σ = 1.00
Gaussian (Cohen et al., 2019)	-0.912-	-1.565-	-1.633-
SmoothAdv (Salman et al., 2019)	0.920	1.629	1.734
MACER (Zhai et al., 2019)	0.918	1.583	1.520
MME/WE(GaUssian)	-09Γ5-	-1.585-	-1.669-
DRT + MME / WE (Gaussian)	0.923	1.637	1.745
MME / WE (SmoothAdv)	0.926	1.678	1.765
DRT + MME / WE (SmoothAdv)	0.929	1.689	1.812
Effects of ρ1 and ρ2 . We investigate the DRT hyper-parameters ρ1 and ρ2 corresponding to different
smoothing parameter σ ∈ {0.25, 0.5, 1.0}. Here we put the detailed results for various hyper-
parameter settings in Tables 4 to 6 and bold the numbers with the highest certified accuracy on each
radius r. From the experiments, we find that the GD loss’s weight ρ1 can have the major influence on
the ensemble model’s functionality: if we choose larger ρ1, the model will achieve slightly lower
certified accuracy on small radii, but higher certified accuracy on large radii. We also can not choose
too large ρ1 on small σ cases (e.g., σ = 0.25). Otherwise, model’s functionality will collapse. Here
we show DRT-based models’ certified accuracy by applying different ρ1 in Figure 9.
Alternatively, we find that the CM loss’s weight ρ2 can also have positive influence on model’s
performance: the larger ρ2 we choose, the higher certified accuracy we could get. Choosing larger
and larger ρ2 does not harm model’s functionality too much, but the improvement on certified
accuracy will become more and more marginal.
Efficiency Analysis. We regard the execution time per mini-batch as our efficiency criterion. For
MNIST with batch size equals to 256, DRT with the Gaussian smoothing base model only requires
1.04s to finish one mini-batch training to achieve the comparable results to the SmoothAdv method
which requires 1.86s. Moreover, DRT with the SmoothAdv base model requires 2.52s per training
batch but achieves much better results. The evaluation is on single NVIDIA GeForce GTX 1080 Ti
GPU.
>U23UU< pωM-_t① ɔ
Radius
1.50	1.75	2.00
(a) σ = 0.5
>U23UU< pωM-_t① ɔ
0.5	1.0	1.5	2.0	2.5	3.0	3.5
Radius
(b) σ = 1.0
Figure 9: Effect of ρ1 : Certified accuracy of DRT-based models with MME protocol trained by
different GD Loss’s weight ρ1 on MNIST. Smoothing parameter σ ∈ {0.50, 1.00}. Training with
large ρ1 will lead to lower certified accuracy on small radii but higher certified accuracy on large
radii.
0.∞	0.25	0.50

45
Published as a conference paper at ICLR 2022
Table 8: Certified accuracy of DRT-(ρ1, ρ2) under different radii r on CIFAR-10 dataset. Smoothing
parameter σ = 0.25. The grey rows present the performance of the proposed DRT approach. The
brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75
Gaussian (Cohen et al., 2019)	-	-	78.9	^644^	^47.^	30.6
SmoothAdv (Salman et al., 2019)	-	-	68.9	61.0	54.4	45.7
MME (GaUSSian)	-	-	80.8	^2Γ		37.4
	0.1	0.5	81.4	70.4	57.6	43.4
DRT + MME (Gaussian)	0.2	0.5	78.8	69.2	57.8	43.8
	0.5	2.0	73.3	61.7	51.0	39.3
	0.5	5.0	66.2	57.1	46.2	34.4
MME (SmoothAdv)	-	-	71.4	^64^	ɪs-	48.4
	0.1	0.5	72.6	67.2	60.2	50.3
DRT + MME (SmoothAdv)	0.2	0.5	71.8	66.5	59.3	50.4
	0.5	0.5	68.2	64.3	58.2	48.9
WE (Gaussian)	-	-	80.7	-68.3-	ɪs-	37.5
	0.1	0.5	-8TF	70.4	57.7	43.4
DRT + WE (Gaussian)	0.2	0.5	78.8	69.3	57.9	44.0
	0.5	2.0	73.4	61.7	51.0	39.2
	0.5	5.0	66.2	57.1	46.1	34.5
WE (SmoothAdv)	-	-	71.8	^646^	ɪs-	48.5
	0.1	0.5	72.6	67.0	60.2	50.3
DRT + WE (SmoothAdv)	0.2	0.5	71.9	66.5	59.4	50.5
	0.5	0.5	68.2	64.3	58.4	49.1
F.2 CIFAR- 1 0
Baseline Configuration. Following the literature (Salman et al., 2019; Jeong & Shin, 2020; Zhai
et al., 2019), in each batch, each training sample is Gaussian augmented twice (augmenting more
times yields negligible difference as Salman et al. (2019) show). We choose Gaussian smoothing
variance σ ∈ {0.25, 0.5, 1.0} for training and evaluation for all methods. For SmoothAdv, we
consider the attack to be 10-step L2 PGD attack with perturbation scale δ = 1.0 without pretraining
and unlabelled data augmentation. We also reproduced the similar results mentioned in baseline
papers.
Training Details. First, we use ResNet-110 architecture and train each base model for 150 epochs.
For the training optimizer, we use the SGD-momentum with the initial learning rate α = 0.1. The
learning rate is decayed for every 50-epochs with decay ratio γ = 0.1. Then, we use DRT to
finetune our model with small learning rate α for another 150 epochs. We also explore different DRT
hyper-parameters ρ1 , ρ2 together with the initial learning rate α, and report the best certified accuracy
on each radius r among all the trained ensemble models.
Trend of Certified Accuracy with Perturbation Radius. We visualize the trend of certified accu-
racy along with different perturbation radii on different smoothing parameters separately in Figure 10
and Figure 11. For each radius r, we present the best certified accuracy among all the trained models.
We can see the similar trends: Applying either MME or WE ensemble protocol will only give slight
improvement while DRT can help make this improvement significant.
Average Certified Radius. We report the Average Certified Radius (ACR) (Zhai et al., 2019): ACR
=|s1j P(X y)∈stest R(x, y), where Stest refers to the test set and R(χ, y) the Certifed radius on testing
sample (x, y). We evaluate ACR of our DRT-trained ensemble trained with σ ∈ {0.25, 0.5, 1.0}
smoothing parameter and compare it with other baselines. Results are shown in Table 11.
Results shows that, DRT-trained ensemble has the highest ACR on almost all the settings. Especially
on σ = 1.00, our improvement is significant.
Effects of ρ1 and ρ2. We study the DRT hyper-parameter ρ1 and ρ2 corresponding to different
smoothing parameters σ ∈ {0.25, 0.5, 1.0} and put the detailed results in Tables 8 to 10. We bold the
46
Published as a conference paper at ICLR 2022
Table 9: Certified accuracy of DRT-(ρ1, ρ2) under different radii r on CIFAR-10 dataset. Smoothing
parameter σ = 0.50. The grey rows present the performance of the proposed DRT approach. The
brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75
Gaussian (Cohen et al., 2019)	-	-	68.2	57.1	44.9	33.7	23.1	16.3	10.0	5.4
SmoothAdv (Salman et al., 2019)	-	-	60.6	54.2	47.9	41.2	34.8	28.5	21.9	17.1
MME (Gaussian)	-	-	69.5	59.6	47.3	38.4	29.0	19.6	13.3	7.6
		2.0	69.7	61.0	50.9	40.3	30.8	22.5	15.8	10.0
	0.2	5.0	68.0	59.9	50.0	40.8	30.1	22.1	15.2	9.6
	∩ c	2.0	67.8	58.5	49.0	39.9	31.6	23.4	16.1	10.2
	0.5	5.0	65.5	58.4	49.0	40.1	31.2	23.6	16.5	10.2
	ι n	2.0	64.5	55.8	47.5	39.4	31.1	23.6	14.8	9.3
	1.0	5.0	62.2	54.1	46.5	38.8	29.7	22.8	16.6	11.0
DRT + MME (Gaussian)	1.5	5.0	59.2	52.8	44.1	35.6	27.8	22.3	15.0	10.2
		2.5	58.4	51.0	44.2	39.2	33.4	27.6	23.4	20.6
	5.0	5.0	56.2	49.6	45.8	40.4	34.4	29.6	24.4	20.8
		2.5	52.0	46.8	42.0	36.2	32.4	27.8	23.4	19.7
	10.0	5.0	51.2	47.5	42.5	38.1	33.7	28.9	24.9	20.9
	15.0	20.0	54.5	49.8	44.7	34.9	30.2	23.0	18.7	11.1
	20.0	30.0	52.2	46.2	40.2	34.4	29.4	22.6	17.8	12.8
MME (SmoothAdv)	-	-	61.0	54.8	48.7	42.2	36.2	29.8	23.9	19.1
	0.2	5.0	62.2	56.4	50.3	43.4	37.5	26.7	24.6	19.4
	0.5	5.0	61.9	56.2	50.3	43.5	37.6	31.8	24.8	19.6
DRT + MME (SmoothAdv)	1.0	5.0	56.4	52.6	48.2	44.4	39.6	35.8	30.4	23.6
	1.5	5.0	56.0	50.8	47.2	44.2	39.8	35.0	29.4	24.0
WE (Gaussian)	-	-	69.4	59.7	47.5	38.4	29.2	19.7	13.3	7.5
		2.0	69.7	61.2	50.8	40.2	30.8	22.4	15.9	10.0
	0.2	5.0	68.0	59.9	50.1	40.8	30.1	22.1	15.4	9.7
	∩ c	2.0	67.8	58.5	49.2	39.8	31.7	23.5	16.2	10.4
	0.5	5.0	65.5	58.4	49.1	40.3	31.3	24.2	16.4	10.3
		2.0	64.6	55.9	47.5	39.6	31.0	24.0	14.8	9.4
	1.0	5.0	62.3	54.2	46.6	38.8	29.8	22.9	16.6	10.9
DRT + WE (Gaussian)	1.5	5.0	59.2	52.8	44.2	35.8	27.8	22.4	15.0	10.3
		2.5	58.4	51.1	44.2	39.2	33.3	27.8	23.2	20.6
	5.0	5.0	56.2	49.7	45.8	40.3	34.2	29.6	24.5	20.8
		2.5	52.0	46.9	42.0	36.4	32.5	27.8	23.5	19.7
	10.0	5.0	51.2	47.6	42.4	38.1	33.6	28.9	24.9	20.8
	15.0	20.0	54.3	49.8	44.6	35.0	30.3	23.0	18.8	11.3
	20.0	30.0	52.2	46.2	40.2	34.5	29.2	22.6	17.9	12.8
WE (SmoothAdv)	-	-	61.1	54.8	48.8	42.3	36.2	29.6	24.2	19.0
	0.2	5.0	62.2	56.3	50.3	43.4	37.5	26.9	24.7	19.3
	0.5	5.0	61.9	56.2	50.2	43.4	37.9	31.8	25.0	19.6
DRT + WE (SmoothAdv)	1.0	5.0	56.4	52.6	48.2	44.4	39.5	36.0	30.3	23.6
	1.5	5.0	56.1	50.9	47.2	44.1	39.8	35.1	29.4	24.1
numbers with the highest certified accuracy on each radius r. The results show similar conclusion to
our understanding from MNIST.
Efficiency Analysis. We also use the execution time per mini-batch as our efficiency criterion. For
CIFAR-10 with batch size equals to 256, DRT with the Gaussian smoothing base model requires
3.82s to finish one mini-batch training to achieve the competitive results to 10-step PGD attack
based SmoothAdv method which requires 6.39s. All the models are trained in parallel on 4 NVIDIA
GeForce GTX 1080 Ti GPUs.
F.3 ImageNet
For ImageNet, we utilize ResNet-50 architecture and train each base model for 90 epochs using
SGD-momentum optimizer. The initial learning rate α is set to 0.1. During training, the learning
rate is decayed for every 30-epochs with decay ratio γ = 0.1. We tried different Gaussian smoothing
parameter σ ∈ {0.50, 1.00}, and consider the best hyper-parameter configuration for each σ. Then,
we use DRT to finetune base models with the learning rate α = 5 × 10-3 for another 90 epochs. Due
47
Published as a conference paper at ICLR 2022
Table 10: DRT-(ρ1, ρ2) model’s certified accuracy under different radii r on CIFAR-10 dataset.
Smoothing parameter σ = 1.00. The grey rows present the performance of the proposed DRT
approach. The brackets show the base models we use.
Radius r	Pi	P2	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Gaussian (Cohen et al., 2019)	-	-	48.9	42.7	35.4	28.7	22.8	18.3	13.6	10.5	7.3
SmoothAdv (Salman et al., 2019)	-	-	47.8	43.3	39.5	34.6	30.3	25.0	21.2	18.2	15.7
MME (Gaussian)	-	-	50.2	44.0	37.5	30.9	24.1	19.3	15.6	11.6	8.8
	0.5	5.0	49.4	44.2	37.8	31.6	25.4	22.6	18.2	14.4	12.4
	1.0	5.0	49.8	44.4	39.0	31.6	25.6	22.6	18.2	15.0	12.0
	1.5	5.0	48.0	42.4	36.4	30.4	26.2	22.0	18.4	15.4	12.8
		0.5	44.6	38.6	34.6	29.2	25.6	21.8	19.4	17.0	15.6
DRT + MME (Gaussian)	5.0	2.5	44.8	39.6	35.2	31.0	27.8	23.4	20.6	18.2	16.6
		10.0	45.4	40.4	36.8	30.4	26.0	21.8	19.0	15.8	13.6
	10.0	20.0	44.4	40.8	36.2	31.2	27.4	21.2	18.8	17.2	13.6
	15.0	20.0	42.2	39.6	34.8	30.8	26.2	22.4	18.0	16.6	15.4
	20.0	30.0	33.8	30.2	26.8	22.6	18.6	16.8	15.0	12.8	11.4
MME (SmoothAdv)	-	-	48.2	43.7	40.1	35.4	31.3	26.2	22.6	19.5	16.2
	0.2	5.0	48.2	43.9	40.1	35.4	31.5	26.7	22.9	19.8	16.8
DRT + MME (SmoothAdV)	0.5	5.0	48.1	43.8	40.3	35.7	31.8	26.9	23.1	20.1	17.5
	1.0	5.0	46.2	43.4	40.8	37.0	34.2	30.0	26.8	23.8	20.1
	1.5	5.0	47.8	43.4	39.5	35.4	31.6	26.7	23.1	20.4	18.1
WE (Gaussian)	-	-	50.4	44.1	37.5	30.9	24.2	19.2	15.9	11.8	8.9
	0.5	5.0	49.5	44.3	37.8	31.8	25.6	22.5	18.2	14.4	12.3
	1.0	5.0	49.8	44.4	39.1	31.7	25.6	22.8	18.4	15.1	12.1
	1.5	5.0	48.2	42.5	36.6	30.4	26.1	22.1	18.2	15.7	12.6
		0.5	44.6	38.6	34.7	29.1	25.8	21.8	19.6	17.1	15.6
DRT + WE (Gaussian)	5.0	2.5	44.8	39.6	35.4	31.0	27.9	23.4	20.6	18.1	16.4
		10.0	45.4	40.3	36.8	30.4	26.2	21.8	19.1	15.8	13.6
	10.0	20.0	44.5	40.8	36.2	31.3	27.4	21.2	18.9	17.2	13.5
	15.0	20.0	42.2	39.7	34.8	30.8	26.1	22.4	18.0	16.7	15.4
	20.0	30.0	33.8	30.4	26.8	22.8	18.6	16.9	15.0	12.7	11.2
WE (SmoothAdv)	-	-	48.2	43.7	40.2	35.4	31.5	26.2	22.7	19.6	16.0
	0.2	5.0	48.2	43.8	40.2	35.4	31.5	26.8	23.0	19.9	16.7
DRT + WE (SmoothAdv)	0.5	5.0	48.2	43.8	40.5	35.7	31.9	26.8	23.3	20.2	17.5
	1.0	5.0	46.2	43.4	40.6	37.0	34.2	30.1	26.8	23.9	20.3
	1.5	5.0	47.8	43.4	39.6	35.4	31.4	26.7	23.0	20.4	18.1
Table 11: Average Certified Radius (ACR) of DRT-trained ensemble trained with different smoothing
parameter σ ∈ {0.25, 0.50, 1.00} on CIFAR-10 dataset, compared with other baselines. The grey
rows present the performance of the proposed DRT approach. The brackets shows the base models
we use.
Radius r	σ = 0.25	σ = 0.50	σ = 1.00
Gaussian	-0.484-	-0.595-	-0.559
SmoothAdv	0.539	0.662	0.730
MACER	0.556	0.726	0.792
MME/WE(Gaussian)	-0513-	-0.621-	-0.579
DRT + MME / WE (Gaussian)	0.551	0.687	0.744
MME / WE (SmoothAdv)	0.542	0.692	0.689
DRT + MME / WE (SmoothAdv)	0.545	0.760	0.868
to the consideration of achieving high certified accuracy on large radii, we choose large DRT training
hyper-parameter ρ1 and ρ2 in practice, which lead to relatively low benign accuracy.
G Ablation studies
G. 1 The Effects of Gradient Diversity Loss and Confidence Margin Loss
To explore the effects of individual Gradient Diversity and Confidence Margin Losses in DRT, we set
ρ1 or ρ2 to 0 and tune the other for evaluation on MNIST and CIFAR-10. The results are shown in
Table 12 and 13. We observe that both GD and CM losses have positive effects on improving the
48
Published as a conference paper at ICLR 2022
(a) σ = 0.25
Radius
(b) σ = 0.50
Radius
(c) σ = 1.00
Figure 10:	Certified accuracy for ML ensembles with Gaussian smoothed base models, under
smoothing parameter σ ∈ {0.25, 0.50, 1.00} separately on CIFAR-10.
141
C 04.
⅛
ð
0Λ
0.2 g 0Λ t>Λ
SmoothAdv
——MME (SmoothAdv)
——DRT+MME (SmoothAdv)
——WE (SmoothAdv)
——DRT+WE (SmoothAdv)
Radius
(a) σ = 0.25
(c) σ = 1.00



Figure 11:	Certified accuracy for ML ensembles with SmoothAdv base models, under smoothing
parameter σ ∈ {0.25, 0.50, 1.00} separately on CIFAR-10.
certified accuracy while GD plays a major role on larger radii. By combining these two regularization
losses together in DRT, the ensemble model achieves the highest certified accuracy under all radii.
Table 12: Certified accuracy achieved by training with GD Loss (GDL) or Confidence Margin Loss
(CML) only on MNIST dataset.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	2.50
MME (Gaussian)	99.2	98.4	96.8	94.9	90.5	84.3	69.8	48.8	34.7	23.4	12.7
GDL + MME (Gaussian)	99.2	98.4	96.9	95.3	92.3	86.2	76.0	60.2	43.3	35.5	28.7
CML + MME (Gaussian)	99.3	98.4	97.0	95.0	90.8	84.0	71.1	50.0	36.7	24.5	13.7
DRT + MME (Gaussian)	99.5	98.6	97.5	95.5	92.6	86.8	76.5	60.2	43.9	36.0	29.1
WE (Gaussian)	99.2	98.4	96.9	94.9	90.6	84.5	70.4	49.0	35.2	23.7	12.9
GDL + WE (Gaussian)	99.3	98.5	97.1	95.3	92.3	86.3	76.3	59.8	43.4	35.1	29.0
CML + WE (Gaussian)	99.3	98.4	97.0	95.0	90.8	84.1	71.1	50.3	37.0	24.6	13.7
DRT + WE (Gaussian)	99.5	98.6	97.4	95.6	92.6	86.7	76.7	60.2	43.9	35.8	29.0
G.2 Certified Robustness of single base model within DRT-trained ensemble
We also conduct ablation study on how the single base models’ certified accuracy can be improved
after applying DRT to the whole ensemble for both MNIST and CIFAR-10 datasets. Results are
shown in in Table 14 and 15. We are surprised to find that the single base model within our DRT-
trained ensemble are more robust compared to single base model within other baseline ensembles.
Also, integrating them together could achieve higher robustness.
G.3 Optimizing the Weights of DRT-trained Ensemble
While we adapt average weights in our WE ensemble protocol in our experiments, we are also
interested in how tuning the optimal weights could further improve the certified accuracy of our
DRT-trained ensemble. We conduct this ablation study on both MNIST and CIFAR-10 datasets
by grid-searching all the possible weights combination with step size as 0.1. Results are shown in
Table 16 and 17. (AE here refers to the average ensemble protocol and WE the weighted ensemble
protocol by adapting the tuned optimal weights)
49
Published as a conference paper at ICLR 2022
Table 13: Certified accuracy achieved by training with GD Loss (GDL) or Confidence Margin Loss
(CML) only on CIFAR-10 dataset.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
MME (Gaussian)	80.8	68.2	53.4	38.4	29.0	19.6	15.6	11.6	8.8
GDL + MME (Gaussian)	81.0	69.0	55.6	41.9	30.4	24.8	20.1	16.9	14.7
CML + MME (Gaussian)	81.2	69.4	54.4	39.6	29.2	21.6	17.0	13.1	12.8
DRT + MME (Gaussian)	81.4	70.4	57.8	43.8	34.4	29.6	24.9	20.9	16.6
WE (Gaussian)	80.8	68.4	53.6	38.4	29.2	19.7	15.9	11.8	8.9
GDL + WE (Gaussian)	81.0	69.1	55.6	41.8	30.6	25.2	20.2	16.9	14.9
CML + WE (Gaussian)	81.1	69.4	54.6	39.7	29.4	21.7	17.2	13.2	12.8
DRT + WE (Gaussian)	81.5	70.4	57.9	44.0	34.2	29.6	24.9	20.8	16.4
Table 14: Certified accuracy of single base model within DRT-trained ensemble on MNIST dataset.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	2.50
Single (Gaussian)	99.1	97.9	96.6	94.7	90.0	83.0	68.2	46.6	33.0	20.5	11.5
DRT Single (Gaussian)	99.0	98.6	97.2	95.4	92.0	85.6	74.9	59.8	43.4	35.2	28.6
DRT + MME (Gaussian)	99.5	98.6	97.5	95.5	92.6	86.8	76.5	60.2	43.9	36.0	29.1
DRT + WE (Gaussian)	99.5	98.6	97.4	95.6	92.6	86.7	76.7	60.2	43.9	35.8	29.0
Single (SmoothAdv)	99.1	98.4	97.0	96.3	93.0	87.7	80.2	66.3	43.2	34.3	24.0
DRT Single (SmoothAdv)	99.2	98.4	97.6	96.6	92.9	88.1	80.4	68.0	46.4	39.2	34.1
DRT + MME (SmoothAdv)	99.2	98.4	97.6	96.7	93.1	88.5	83.2	68.9	48.2	40.3	34.7
DRT + WE (SmoothAdv)	99.1	98.4	97.6	96.7	93.4	88.5	83.3	69.6	48.3	40.2	34.8
Table 15: Certified accuracy of single base model within DRT-trained ensemble on CIFAR-10 dataset.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
Single (Gaussian)	78.9	64.4	47.4	33.7	23.1	18.3	13.6	10.5	7.3
DRT Single (Gaussian)	81.4	69.8	56.2	42.5	33.6	27.6	24.2	20.4	15.4
DRT + MME (Gaussian)	81.4	70.4	57.8	43.8	34.4	29.6	24.9	20.9	16.6
DRT + WE (Gaussian)	81.5	70.4	57.9	44.0	34.2	29.6	24.9	20.8	16.4
Single (SmoothAdv)	68.9	61.0	54.4	45.7	34.8	28.5	21.9	18.2	15.7
DRT Single (SmoothAdv)	72.4	66.8	57.8	48.2	38.1	33.4	28.6	22.2	19.6
DRT + MME (SmoothAdv)	72.6	67.2	60.2	50.4	39.4	35.8	30.4	24.0	20.1
DRT + WE (SmoothAdv)	72.6	67.0	60.2	50.5	39.5	36.0	30.3	24.1	20.3
We can see that, by learning the optimal weights, the certified accuracy could be only slightly
improved compared to the average weights setting, which indicates that, average weights can be a
good choice in practice.
Table 16: Comparison of the certified accuracy between Average Ensemble (AE) protocol and
Weighted Ensemble (WE) protocol on MNIST dataset. Cells with bold numbers indicate learning
optimal weights could achieve higher certified accuracy on corresponding radius r.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00	2.25	2.50
DRT + AE (Gaussian)	99.5	98.6	97.4	95.6	92.6	86.7	76.7	60.2	43.9	35.8	29.0
DRT + WE (Gaussian)	99.5	98.6	97.6	95.6	92.7	86.8	76.7	60.3	44.0	36.0	29.3
DRT + AE (SmoothAdv)	99.1	98.4	97.6	96.7	93.4	88.5	83.3	69.6	48.3	40.2	34.8
DRT + WE (SmoothAdv)	99.1	98.4	97.6	96.8	93.5	88.5	83.3	69.7	48.5	40.2	34.8
G.4 Comparison with Other Gradient Diversity Regularizers
We notice that out of the certifiably robust ensemble field, there exist two representatives of gradient
diversity promoting regularizers: ADP (Pang et al., 2019) and GAL (Kariyappa & Qureshi, 2019).
They achieved notable improvements on empirical ensemble robustness. For an ensemble consisting
of base models {Fi}iN=1 and input x and ground truth label y, the ADP regularizer is defined as
N
LADP(x, y) = α ∙ X H(mean({fi(x)})) + β ∙ log(ED)
i=1
50
Published as a conference paper at ICLR 2022
Table 17: Comparison of the certified accuracy between Average Ensemble (AE) protocol and
Weighted Ensemble (WE) protocol on CIFAR-10 dataset. Cells with bold numbers indicate learning
optimal weights could achieve higher certified accuracy on corresponding radius r.
Radius r	0.00	0.25	0.50	0.75	1.00	1.25	1.50	1.75	2.00
DRT + AE (Gaussian)	81.5	70.4	57.9	44.0	34.2	29.6	24.9	20.8	16.4
DRT + WE (Gaussian)	81.5	70.4	57.9	44.0	34.3	29.6	25.0	20.9	16.5
DRT + AE (SmoothAdv)	72.6	67.0	60.2	50.5	39.5	36.0	30.3	24.1	20.3
DRT + WE (SmoothAdv)	72.6	67.1	60.2	50.5	39.5	36.1	30.3	24.1	20.4
Table 18: Certified accuracy of {ADP, GAL, DRT}-based Gaussian smoothed ensemble under
different radii with WE protocol.
MNIST r	0.00 I	0.25	0.50	0.75	1.00 I	1.25	1.50 I	1.75 I	2.00 I	2.25	2.50
ADP	99.5	98.2	97.2	95.2	92.2	85.8	73.4	53.2	36.9	24.7	13.3
GAL	99.5	98.3	97.2	95.1	92.4	86.1	73.2	54.4	36.2	24.7	13.9
DRT	99.5	98.6	97.4	95.6	92.6	86.7	76.7	60.2	43.9	35.8	29.0
CIFAR-10 r		0.00	I 0.25	I 0.50	I 0.75	1.00	I 1∙25	1.50	ZK	2.00	
ADP		83.0	68.0	52.2	38.2	28.8	20.0	16.8	14.2	11.0	
GAL		82.2	67.6	53.6	38.8	27.6	20.2	15.4	13.6	10.6	
DRT		81.5	70.4	57.9	44.0	34.2	29.6	24.9	20.8	16.4	
where H(∙) refers to the Shannon Entropy Loss function and ED the square of the spanned volume
of base models’ logit vectors.
GAL regularizer minimizes the cosine similarity value between base models’ loss gradient vectors,
which is defined as:
LGAL(x, y) = log	eχp exp (coshVχ'Fi, j`f)).
1≤i<j≤N
Under the smoothed ensemble training setting, the final training loss is represented by
Ltrain(x, y) =	Lstd(x + ε, y)i + {LADP(x + ε, y) or LGAL(x + ε, y)}
i∈[N]
where we consider standard training loss Lstd(x0 + ε, y0)i of each base model Fi to be the standard
cross-entropy loss.
Table 18 shows the certified accuracy of {ADP, GAL, DRT}-trained ensemble under different radii
with WE protocol on MNIST and CIFAR-10 dataset. We notice that DRT outperforms both ADP and
GAL significantly in terms of the certified accuracy on different datasets.
51