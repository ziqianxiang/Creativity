Actor-critic is implicitly biased
TOWARDS HIGH ENTROPY OPTIMAL POLICIES
Yuzheng Hu, Ziwei Ji, Matus Telgarsky
University of Illinois, Urbana-Champaign
<{yh46,ziweiji2,mjt}@illinois.edu>
Ab stract
We show that the simplest actor-critic method — a linear softmax policy updated
with TD through interaction with a linear MDP, but featuring no explicit regular-
ization or exploration — does not merely find an optimal policy, but moreover
prefers high entropy optimal policies. To demonstrate the strength of this bias, the
algorithm not only has no regularization, no projections, and no exploration like
-greedy, but is moreover trained on a single trajectory with no resets. The key
consequence of the high entropy bias is that uniform mixing assumptions on the
MDP, which exist in some form in all prior work, can be dropped: the implicit
regularization of the high entropy bias is enough to ensure that all chains mix and
an optimal policy is reached with high probability. As auxiliary contributions,
this work decouples concerns between the actor and critic by writing the actor up-
date as an explicit mirror descent, provides tools to uniformly bound mixing times
within KL balls of policy space, and provides a projection-free TD analysis with
its own implicit bias which can be run from an unmixed starting distribution.
1 Overview
Reinforcement learning methods navigate an environment and seek to maximize their reward (Sut-
ton & Barto, 2018). A key tension is the tradeoff between exploration and exploitation: does a
learner (also called an agent or policy) explore for new high-reward states, or does it exploit the
best states it has already found? This is a sensitive part of RL algorithm design, as it is easy for
methods to become blind to parts of the state space; to combat this, many methods have an explicit
exploration component, for instance the -greedy method, which forces exploration in all states with
probability (Sutton & Barto, 2018; Tokic, 2010). Similarly, many methods must use projections
and regularization to smooth their estimates (Williams & Peng, 1991; Mnih et al., 2016; Cen et al.,
2020).
This work considers actor-critic methods, where a policy (or actor) is updated via the suggestions of
a critic. In this setting, prior work invokes a combination of explicit regularization and exploration
to avoid getting stuck, and makes various fast mixing assumptions to help accurate exploration.
For example, recent work with a single trajectory in the tabular case used both an explicit -greedy
component and uniform mixing assumptions (Khodadadian et al., 2021), neural actor-critic methods
use a combination of projections and regularization together with various assumptions on mixing
and on the path followed through policy space (Cai et al., 2019; Wang et al., 2019), and even direct
analyses of the TD subroutine in our linear MDP setting make use of both projection steps and an
assumption of starting from the stationary distribution (Bhandari et al., 2018).
Contribution. This work shows that a simple linear actor-critic (cf. Algorithm 1) in a linear MDP
(cf. Assumption 1.3) with a finite but non-tabular state space (cf. Assumption 1.1) finds an -optimal
policy in poly(1/) samples, without any explicit exploration or projections in the algorithm and
without any uniform mixing assumptions on the policy space (cf. Theorem 1.4). The algorithm and
analysis avoid both via an implicit bias towards high entropy policies: the actor-critic policy path
never leaves a Kullback-Leibler (KL) divergence ball of the maximum entropy optimal policy, and
this firstly ensures implicit exploration, and secondly ensures fast mixing. In more detail:
1
1.	Actor analysis via mirror descent. We write the actor update as an explicit mirror descent.
While on the surface this does not change the method (e.g., in the tabular case, the method
is identical to natural policy gradient (Agarwal et al., 2021b)), it gives a clean optimization
guarantee which carries a KL-based implicit bias consequence for free, and decouples concerns
between the actor and critic.
2.	Critic analysis via projection-free sampling tools within KL balls. The preceding mirror
descent component guarantees that we stay within a small KL ball, if the statistical error of the
critic is controlled. Concordantly, our sampling tools guarantee this statistical error is small, if
we stay within a small KL ball. Concretely, we provide useful lemmas that every policy in a KL
ball around the high entropy policy has uniformly upper bounded mixing times, and separately
give a projection-free (implicitly regularized!) analysis of the standard temporal-difference
(TD) update from any starting state (Sutton, 1988), whereas the closest TD analysis in the
literature uses projections and requires the sampling process to be started from the stationary
distribution (Bhandari et al., 2018). The mixing assumptions here contrast in general with prior
work, which either makes explicit use of stationary distributions (Cai et al., 2019; Wang et al.,
2019; Bhandari et al., 2018), or makes uniform mixing assumptions on all policies (Xu et al.,
2020; Khodadadian et al., 2021).
In addition to the preceding major contributions, the paper comes with many technical lemmas (e.g.,
mixing time lemmas) which we hope are useful in other work.
1.1 Setting and main results
We will now give the setting, main result, and algorithm in full. Further details on MDPs can be
found in Section 1.3, but the actor-critic method appears in Algorithm 1. To start, the environment
and policies are as follows.
Assumption 1.1. The Markov Decision Process (MDP) has states s ∈ Rd and finitely many actions
a ∈ A := {e1, . . . , ek}, and finite rewards r ∈ [0, 1]. States are observed in some feature encoding
s ∈ Rd, but the state space S ⊆ {s ∈ Rd : * 1/2 ≤ ksk ≤ 1} is assumed finite: |S| < ∞.
Policies are linear softmax policies: a policy is parameterized by a weight matrix W ∈ Rd×k, and
given a state s ∈ Rd, uses a per-state softmax to sample a new action a:
a 〜φ(sTW∙),
where φ(sT W a)
exp(sTWa)
Pb∈A exp(sTWb)
(1.1)
Let As denote the set of optimal actions for a given state s. It is assumed that As is nonempty for
every s ∈ S, and that there exists at least one optimal policy which is irreducible (Levin et al., 2006,
Chapter 1).	♦
The choice of linear policies simplifies presentation and analysis, but the tools here should be appli-
cable to other settings. This choice also allows direct comparison to the widely-studied implicit bias
Algorithm 1 Single-trajectory linear actor-critic.
Inputs: actor iterations t and step size θ; critic iterations N and step size η.
Initialize: actor weights W0 = 0 ∈ Rd×k, pre-softmax mapping p0 (s, a) := sTW0a, policy
π0 := φ(p0) (cf. eq. (1.1)); sample initial state/action/reward triple (s0,0, a0,0, r0,0).
fori = 0, 1,2, . . .,t - 1 do
Critic update: use πi to interact with the MDP, obtaining state/action/reward triples
(si,j, ai,j, ri,j)j≤N by continuing the existing trajectory, and form TD estimates
Ui,j+1 := Ui,j - ηsi,j siT,j Ui,j ai,j - γsiT,j+1Ui,jai,j+1 - ri,j aiT,j,
1
With initial condition Ui,o = 0. Set Ui := N ∑j<N Ui,j and Qi(s, a) ：= sTUia, and also
(si+1,0, ai+1,0, ri+1,0) := (si,N, ai,N, ri,N) to continue the existing trajectory in next iteration.
Actor update: set Wi+1 := Wi + θUi and pi+1 := pi + θQi and πi+1 := φ(pi+1).
end for
2
of gradient descent in linear classification settings (Soudry et al., 2017; Ji & Telgarsky, 2018), as will
be discussed further in Section 1.2. The choice of finite state space is to remove measure-theoretic
concerns and to allow a simple characterization of the maximum entropy optimal policy.
Lemma 1.2 (simplification of Lemma A.1). Under Assumption 1.1, there exists a unique maximum
entropy policy π, which satisfies π(s, ∙) = Uniform(As) for every state S, and moreover has a
stationary distribution s∏.
To round out this introductory presentation of the actor, the last component is the update: pi+1 :
pi + Qi and πi+1 :
φ(pi+1), where Qbi is the TD estimate of the Q function, to be discussed
shortly. This update is explicitly a mirror descent or dual averaging update of the policy, where
we use a mirror mapping φ to obtain the policy πi+1 from pre-softmax values pi+1. As mentioned
before, this update appears in prior work in the tabular setting with natural policy gradient and
actor-critic (Agarwal et al., 2021b; Khodadadian et al., 2021), and will be related to other methods
in Section 1.2. We will further motivate this update in Section 2.
The final assumption and description of the critic are as follows. As will be discussed in Section 2,
the policy becomes optimal if Qi is an accurate estimate of the true Q function. We employ a
standard TD update with no projections or constraints. To guarantee that this linear model of Qi
is accurate, we make a standard linear MDP assumption (Bradtke & Barto, 1996; Melo & Ribeiro,
2007; Jin et al., 2020).
Assumption 1.3. In words, the linear MDP assumption is that the MDP rewards and transitions
are modeled by linear functions. In more detail, for convenience first fix a canonical vector form
for state/action pairs (s, a) ∈ Rd×k: let xsa ∈ Rdk denote the vector obtained via unrolling the
matrix saT row-wise (whereby vector inner products with xsa match matrix inner products with
saT). The linear MDP assumption is then that there exists a fixed vector y ∈ Rdk and a fixed matrix
M ∈ Rd×dk so that for any state/action pair xsa and any subsequent state s0 ∈ Rd,
E[r | (s, a)] = xTsay,	and	E[s0 | (s, a)] = Mxsa.
Lastly, suppose 1/2 ≤ ksk ≤ 1 for all s ∈ S.	♦
Though a strong assumption, it is not only common, but note also that since TD must continually
interact with the MDP, then it would have little hope of accuracy if it can not model short-term
MDP dynamics. Indeed, as is shown in Lemma C.3 (but appears in various forms throughout the
literature), Assumption 1.3 implies that the fixed point of the TD update is the true Q function. This
assumption and the closely related compatible linear function approximation assumption will be
discussed in Section 1.2.
We now state our main result, which bounds not just the value function V (cf. Section 1.3) but
also the KL divergence Kvs(∏,∏i) = Es，〜Vs Pa ∏(s0,a)ln 累；：)), where v∏ is the visitation
distribution of the maximum entropy optimal policy π when run from state S (cf. Section 1.3).
Theorem 1.4. Suppose Assumptions 1.1 and 1.3 (which imply the (unique) maximum entropy opti-
mal policy π is well-defined and has a stationary distribution). Given iteration budget t, choose
θ = Θ I /	— [/d I , N = Θ (t2 ln t) , η = Θ (	二),
\t13/16 ln(t)1/4 广	、尸 I	√ NlTNN)
where the constants hidden inside each Θ(∙) depend only on π and the MDP, but not on t. With these
parameters in place, invoke Algorithm 1, and let (πi)i<t be the resulting sequence of policies. Then
with probability at least 1 - 1/t1/8, simultaneously for every state S ∈ Rd and every i ≤ t,
Kvn(∏,∏i) + θ(1 — Y)E (Vn(S)-Vj(S)) ≤ lnk +
j<i
1
(1-Y2.
Before outlining the proof structure and organization of the rest of the paper, a few comments on the
interpretation of Theorem 1.4 are as follows.
Remark 1.5 (Discussion of Theorem 1.4).
1.	Implicit bias. Since ∏ is optimal, the second term can be deleted, and the bound implies
maxKvs(π, ∏i) ≤ lnk +
s∈S
1
(1 — Y)2 ;
3
since this holds for all i ≤ t, it controls the optimization path. This term is a direct consequence
of our mirror descent setup, and is used to control the TD errors at every iteration. This implicit
bias of the policy path stands therefore in stark contrast to the worst-case KL divergence be-
tween arbitrary Softmax policies and π, which is infinite: e.g., a sequence of policies (∏i)i≥ι
which place vanishing probability on a pair (s, a) which in turn receives positive probability
under ∏ will have Kvs(∏, ∏i) → ∞.
2.	Mixing time constants. The critic iterations N and step size η hide mixing time constants;
these mixing time constants depend only on the KL bound ln k + 1/(1 - γ)2, and in particular
there is no hidden growth in these terms with t. That is to say, mixing times are uniformly
controlled over a fixed KL ball that does not depend on t; prior work by contrast makes strong
mixing assumptions (Wang et al., 2019; Xu et al., 2020; Khodadadian et al., 2021).
3.	High probability guarantee. Though prior work focuses on bounds in expectation, we chose
a high probability guarantee to emphasize that the bound does not blow up, despite an arguably
more strenuous setting.
4.	Single trajectory. A single trajectory through the MDP is used to remove the option of the
algorithm escaping from poor choices with resets; only the implicit bias can save it.
5.	Rate. To reach a policy which whose value function is -close to optimal, a trajectory length
(number of samples) of 1/16 is sufficient, ignoring log factors (to obtain this from Theo-
rem 1.4, it suffices to divide both sides of the bound by tθ(1 - γ), set t = 1/16/3, and note the
trajectory length is tN). This is slower than the 1/6 given in the only other single-trajectory
analysis in the literature Khodadadian et al. (2021), but by contrast that work makes uniform
mixing assumptions (cf. Khodadadian et al. (2021, Lemma C.1)), requires the tabular setting,
and uses -greedy for explicit exploration in each iteration.	♦
The proof of Theorem 1.4 and organization of the paper are as follows. After overviews of related
work and notation in Sections 1.2 and 1.3, the first proof component, discussed in Section 2, is the
outer loop of Algorithm 1, namely the update to the policy πi . As discussed before, this part of the
analysis writes the policy update as a mirror descent, and conveniently decouples the suboptimality
error into an actor error, which is handled by standard mirror descent tools and provides the implicit
bias towards high entropy policies, and a critic error, namely the error of the estimated Q function
Qbi . The second component, discussed in Section 3, is therefore the TD analysis establishing that
the estimate Qbi is accurate, which not only requires an abstract TD guarantee (which, as mentioned,
is projection-free and run from an arbitrary starting state, unlike prior work), but also requires tools
to explicitly bound mixing times, rather than assuming mixing times are bounded.
This culminates in the proof of Theorem 1.4, which is sketched at the end of Section 3, and uses
an induction combining the guarantees from the preceding two proof components at all times: it is
established inductively that the next policy πi+1 has high entropy and low suboptimality because
the previous Q function estimates (Qj)j≤i were accurate, and simultaneously that the next estimate
Qi+1 is accurate because πi+1 has high entropy, which guarantees fast mixing. Section 4 concludes
with some discussion and open problems, and the appendices contain the full proofs.
1.2	Further related work
For the standard background in reinforcement learning, see the book by Sutton & Barto (2018).
Standard concepts and notation choices are presented below in Section 1.3.
Standard algorithms: PG/NPG and AC/NAC. The [natural] policy gradient ([N]PG) and nat-
ural actor-critic ([N]AC) are widely used in practice, and summarized briefly as follows. Policy
gradient methods update the actor parameters W with gradient ascent on the value function Vn (μ)
for some state distribution μ (Williams, 1992; Sutton et al., 2000; Bagnell & Schneider, 2003; Liu
et al., 2020; Fazel et al., 2018), whereas natural policy gradient multiplies VwVn (μ) by an inverse
Fisher matrix with the goal of improved convergence via a more relevant geometry (Kakade, 2001;
Agarwal et al., 2021b). What policy gradient leaves open is how to estimate VWVn (μ); actor-critic
methods go one step further and suggest updating the actor with policy gradient as above, but noting
that VW Vn (μ) can be written as a function of Qn, or rather an estimate thereof, and making this
4
estimation the job of a separate subroutine, called the critic Konda & Tsitsiklis (2000). (Natural
actor-critic uses natural policy gradient in the actor update (Peters & Schaal, 2008).) Actor-critic
methods are perhaps the most widely-used instances of policy gradient, and come in many forms; the
use of TD for the critic step is common (Williams, 1992; Sutton et al., 2000; Bagnell & Schneider,
2003; Liu et al., 2020; Fazel et al., 2018).
Linear MDPs and compatible function approximation. The linear MDP assumption (cf. As-
sumption 1.3) is used here to ensure that the TD step accurately estimates the true Q function, and is
a somewhat common assumption in the literature, even when TD is not used (Bradtke & Barto, 1996;
Melo & Ribeiro, 2007; Jin et al., 2020). As an example, the tabular setting satisfies Assumption 1.3,
simply by encoding states as distinct standard basis vectors, namely S := {e1, . . . , e|S| } (Jin et al.,
2020, Example 2.1); moreover, in this tabular setting, the actor update of Algorithm 1 agrees with
NPG (Agarwal et al., 2021b). Interestingly, another common assumption, compatible linear function
approximation, also guarantees our analysis goes through and that Algorithm 1 agrees with NPG,
while being non-tabular in general. In detail, the compatible function approximation setting firstly
requires that the actor
update agrees with Qbi in a certain
sense (which holds in our setting by con-
struction), and secondly that there exists a choice of critic parameters U so that the exact Q function
Qπ can be represented with these parameters (Silver, 2015). If this assumption holds for every pol-
icy πi (and corresponding Qi) encountered in the algorithm, then the policy update of Algorithm 1
agrees with NPG and NAC (this is a standard fact; see for instance Silver (2015), or Agarwal et al.
(2021a, Lemma 13.1)). Additionally, the proofs here also go through under this arguably weaker
assumption: Assumption 1.3 is only used to ensure that TD finds not just a fixed point but the true
Q function (cf. Lemma C.4), which is also guaranteed by the compatibility assumption. However,
since this is an assumption on the trajectory and thus harder to interpret, we prefer Assumption 1.3
which explicitly holds for all possible policies.
Regularization and constraints. It is standard with neural policies to explicitly maintain a con-
straint on the network weights (Wang et al., 2019; Cai et al., 2019). Relatedly, many works both
in theory and practice use explicit entropy regularization to prevent small probabilities (Williams &
Peng, 1991; Mnih et al., 2016; Abdolmaleki et al., 2018), and which can seem to yield convergence
rate improvements (Cen et al., 2020).
NPG and mirror descent. (For background on mirror descent, see Section 2 and appendix B.)
The original and recent analyses of NPG had a mirror descent flavor, though mirror descent and its
analysis were not explicitly invoked (Kakade, 2001; Agarwal et al., 2021b). Further connections
to mirror descent have appeared many times (Geist et al., 2019; Shani et al., 2020), though with
a focus on the design of new algorithms, and not for any implicit regularization effect or proof.
Mirror descent is used heavily throughout the online learning literature (Shalev-Shwartz, 2011), and
in work handling adversarial MDP settings (Zimin & Neu, 2013).
Temporal-difference update (TD). As discussed before, the TD update, originally presented by
(Sutton, 1988), is standard in the actor-critic literature (Cai et al., 2019; Wang et al., 2019), and also
appears in many other works cited in this section. As was mentioned, prior work requires various
projections and initial state assumptions (Bhandari et al., 2018), or positive eigenvalue assumptions
(Zou et al., 2019; Srikant & Ying, 2019; Bhandari et al., 2018).
Implicit regularization in supervised learning. A pervasive topic in supervised learning is the
implicit regularization effect of common descent methods; concretely, standard descent methods
prefer low or even minimum norm solutions, which can be converted into generalization bounds.
The present work makes use of a weak implicit bias, which only prefers smaller norms and does not
necessarily lead to minimal norms; arguably this idea was used in the classical perceptron method
(Novikoff, 1962), but was then shown in linear and shallow network cases of SGD applied to logistic
regression (Ji & Telgarsky, 2018; 2019), which was then generalized to other losses (Shamir, 2020),
and also applied to other settings (Chen et al., 2019). The more well-known strong implicit bias,
namely the convergence to minimum norm solutions, has been observed with exponentially-tailed
losses together with coordinate descent with linear predictors (Zhang & Yu, 2005; Telgarsky, 2013),
gradient descent with linear predictors (Soudry et al., 2017; Ji & Telgarsky, 2018), and deep learning
in various settings (Lyu & Li, 2019; Chizat & Bach, 2020), just to name a few.
5
1.3	Notation
This brief notation section summarizes various concepts and notation used throughout; modulo a
few inventions, the presentation mostly matches standard ones in RL (Sutton & Barto, 2018) and
policy gradient (Agarwal et al., 2021b). A policy π : Rd × Rk → R maps state-action pairs to
reals, and ∏(s, ∙) will always be a probability distribution. Given a state, the agent samples an action
from a 〜 ∏(s, ∙), the environment returns some random reward (which has a fixed distribution
conditioned on the observed (s, a) pair), and then uses a transition kernel to choose a new state
given (s, a).
Taking τ to denote a random trajectory followed by a policy π interacting with the MDP from an
arbitrary initial state distribution μ, the value V and Q functions are respectively
Vn(M) :=	E	X γrt,
so 〜μ	z—
T=s0,a0,r0,s1,∙∙∙ t≥0
Qn (s, a) := S =SEa =& X Yrt= ErO〜(s,a) " + YEsI〜(s,a) Vn(SI)),
τ=so,ao,ro,sι,∙∙∙ t≥0
where the simplified notation Vn(s) = Vn(δS) for Dirac distribution δS on state s will often be used,
as well as the shorthand Vi = Vni and Qi = Qni. Additionally, let An(s, a) := Qn(s, a) - Vn(s)
denote the advantage function; note that the natural policy gradient update could interchangeably use
Ai or Qi since they only differ by an action-independent constant, namely Vn(s), which the softmax
normalizes out. As in Assumption 1.1, the state space S is finite but a subset of Rd, specifically
S ⊆ {s ∈ Rd : 1/2 ≤ ksk ≤ 1}, and the action space A is just the k standard basis vectors
{e1, . . . , ek}. The other MDP assumption, namely of a linear MDP (cf. Assumption 1.3), will
be used whenever TD guarantees are needed. Lastly, the discount factor γ ∈ (0, 1) has not been
highlighted, but is standard in the RL literature, and will be treated as given and fixed throughout
the present work.
A common tool in RL is the performance difference lemma (Kakade & Langford, 2002): letting v∏
denote the visitation distribution corresponding to policy π starting from μ, meaning
vμ := ɪ	ESO〜μ X:YiPr[st = s|so =S/],
1 - γ	t≥0
the performance difference lemma can be written as
Vn(μ) - Vn0(μ) = 1-γES〜vμ0 X Qn(S,a)(π(s,a) - π0(s,a)) =: ι-γ(Qn,π - π'>v”,,
a	(1.2)
where the final inner product notation will often be employed for convenience.
In a few places, we need the Markov chain on states, Pn , which is induced by a policy π: that is,
the chain where given a state s, We sample a 〜∏(s, ∙), and then transition to s0 〜(s, a), where the
latter sampling is via the MDP’s transition kernel.
As mentioned above, sn will denote the stationary distribution of a policy π whenever it exists.
The only relevant assumption we make here, namely Assumption 1.1, is that the maximum entropy
optimal policy π is aperiodic and irreducible, which implies it has a stationary distribution with
positive mass on every state (Levin et al., 2006, Chapter 1). Via Lemma 3.1, it follows that all
policies in a KL ball around ∏ also have stationary distributions with positive mass on every state.
The max entropy OPtimaI卫olicy π is complemented by a (unique) optimal Q function Q and op-
timal advantage function A. The optimal Q function Q dominates all other Q functions, meaning
Q(s,a) ≥ Qn (s, a) for any policy π; for details and a proof, see Lemma A.1.
Weuse kμ-v∣∣tv = SuPU ⊆s ∣μ(U)一ν (U) | to denote the total variation distance, which is pervasive
in mixing time analyses (Levin et al., 2006).
2 Mirror descent tools
To see how nicely mirror descent and its guarantees fit with the NPG/NAC setup, first recall our
updates: pi+1 := pi + Qbi, and πi+1 := φ(pi+1) (e.g., matching NPG in the tabular case (Kakade,
6
2001; Agarwal et al., 2021b)). In the online learning literature (Shalev-Shwartz, 2011; Lattimore &
Szepesvari, 2020), the basic mirror ascent (or dual averaging) guarantee is of the form
X DQbi, π - a=O (√),
i<t
where notably Qbi
can be an arbitrary matrix. The most common results are stated when Qi is the
gradient of some convex function, but here instead we can use the performance difference lemma
(cf. eq. (1.2)): recalling the inner product and visitation distribution notation from Section 1.3,
hQi, πi - πivμ +(Qi - Qi, πi - π∣ μ
vπ
(I- Y)(Vi(M)- Vn (M))+DQi- Qi,πi—πE u ∙
The term Qi - Qi is exactly what we will control with the TD analysis, and thus the mirror descent
approach has neatly decoupled concerns into an actor term, and a critic term.
In order to apply the mirror descent framework, we need to choose a mirror mapping. Rather than
using φ, for technical reasons we bake the measure vμ into the mirror mapping and corresponding
dual objects (cf. Appendix B and the proof of Lemma 2.1). This may seem strange, but it does not
change the induced policy (it scales the dual object for each state by a constant), and thus is a degree
of freedom, and allows us to state guarantees for all possible starting distributions for free.
Our full mirror descent setup is detailed in Appendix B, but culminates in the following guarantee.
Lemma 2.1. Consider Step size θ > 0, any reference policy π, any starting measure μ, and two
treatments of the error Qbi - Qi.
1.	(Simplified bound.) Define Ci := sups,a |Qi(s, a)| for all i <t. Then
Kvμ(π,πt) + θ(I- γ) X(V∏(μ) - Vi(μ)) ≤ Kvμ(π,πo) + θ2 XCi2
i<t	i<t
+ θXDQi- Qi,πi -πEv”.
2.	(Refinedbound.) Define ^ := sups,a ∣Qi(s, a) — Qi(s, a)|. Then
θ
Kvμ(π,πt) + θ(I - Y)E (Vn(μ) - Vi(μ)) ≤ κvμ(π,πo) + ɪ---
i<t	1 - γ
+θ X (芒γ + a + a+1
i<t	Y
and additionally Vi and Qbi
are approximately monotone: for any state s and action a,
Vi+1(S) ≥Vi(S)-占
and
A (	∖、 A /	∖
Qi+1(S,a) ≥ Qi(S,a) -
2γ^i
1 - Y
2	)
- i - i+1 .
Remark 2.2 (Regarding the mirror descent setup, Lemma 2.1).
1. Two rates. For the refined bound, it is most natural to set θ = 1, which requires O(1/) itera-
tions to reach accuracy > 0; by contrast, the simplified guarantee requires O(1/2) iterations
for the same e > 0 with step size θ = 1/√t. We use the simplified form to prove Theorem 1.4,
since its TD error term is less stringent; indeed, the TD analysis we provide in Section 3 will
not be able to give the uniform control needed for the refined bound. Still, we feel the refined
bound is promising, and include it for sake of completeness, future work, and comparison to
prior work.
2. Comparison to standard rates. Comparing the refined bound (with all ^ terms set to zero) to
the standard NPG rate in the literature (Agarwal et al., 2021b), the rate is exactly recovered; as
such, this mirror descent setup at the very least has not paid a price in rates.
7
3.	Implicit regularization term. A conspicuous difference between these bounds and both the
standard NPG bounds (Agarwal et al., 2021b, Theorem 5.3), but also many mirror descent
treatments, is the term Kvn (∏, ∏t); one could argue that this term is nonnegative and moreover
we care more about the value function, so why not drop it, as is usual? It is precisely this term
that gives our implicit regularization effect: instead, we can drop the value function term and
uniformly upper bound the right hand side to get Kvμ(π, ∏t) ≤ ln k + 1/(1 - Y)2, which is
how we control the entropy of the policy path, and prove Theorem 1.4.	♦
3 Sampling tools
Via Lemma 2.1 above, our mirror descent black box analysis gives us a KL bound and a value
function bound: what remains, and is the job of this section, is to control the Q function estimation
error, namely terms of the form Qi - Qbi.
Our analysis here has two parts. The first part, as follows immediately, is that any bounded KL
ball in policy space has uniformly controlled mixing times; the second part, which comes shortly
thereafter, is our TD guarantees.
Lemma 3.1. Let policy π be given, and suppose the induced transition kernel on states Pn is irre-
ducible and aperiodic (Levin et al., 2006, Section 1.3). Then π has a stationary distribution s∏, and
moreover for any c > 0 and any measure ν which is positive on all states and a corresponding set
of policies
Pc := {∏ : KV (∏,∏) ≤ c},
there exist constants C, m1 , m2 so that mixing is uniform over Pc, meaning for any t, and any
π ∈ Pc with induced transition probabilities Pπ,
sup IlPt(s, ∙) -s∏IITV ≤ mιe-m2t,
s
and for any state S and any π ∈ Pc, and any action a with π(s, a) > 0,
ɪ ≤ π(s,a) ≤ c
C ∏ π(s, a)一
and
1 ≤ s∏(s)
C s s∏(s)
≤ C.
Remark 3.2 (Implicit vs explicit exploration). On the surface, Lemma 3.1 might seem quite nice.
Worrying about it a little more, and especially after inspecting the proof, it is clear that the constants
C, m1 , and m2 can be quite bad. On the one hand, one may argue that this is inherent to implicit
exploration, and something like -greedy is preferable, as it arguably gives an explicit control on all
these quantities.
Some aspects of this situation are unavoidable, however. Consider a combination lock MDP, where
a precise, hard-to-find sequence of actions must be followed to arrive at some good reward. Suppose
this sequence has length n and We have a reference policy ∏ which takes each of these good actions
with probability 1 - 1/n, whereby the probability of the sequence is (1 - 1/n)n ≈ 1/e; a policy
π ∈ Pc with π(s, a)∕∏(s, a) ≤ 1/2 for all actions a can drop the probability of this good sequence
of actions all the way down to 1/2n !	♦
Next we present our TD analysis. As discussed in Section 1, by contrast with prior work, our TD
method does not make use of any projections, and does not require eigenvalue assumptions. The
following guarantee is specialized to Algorithm 1; it is a corollary ofa more general TD guarantee,
given in Appendix C, which is stated without reference to Algorithm 1, and can be applied in other
settings.
Lemma 3.3 (See also Lemma C.4). Suppose the MDP and linear MDP assumptions (cf. Assump-
tions 1.1 and 1.3). Consider a policy πi in some iteration i of Algorithm 1, and suppose there exist
mixing constants m ≥ 1 and c > 0 so that the induced transition kernel Pπi on S satisfies
sup IlPni (s, ∙) - S∏i∣∣TV ≤ me-ct.
s
Suppose the TD iterations N and step size η satisfy
N ≥k,
1
η ≤----
—400VkN
lnN + ln m
where k =-----------
c
8
Then letting Ei denote expectation over the trajectory (Sij,ai,j)j≤N and letting Ui denote the
expected TD fixed point given in Lemma C.3 (which satisfies ∣∣Uik ≤ 2/(1 一 Y), the average TD
1
iterate Ui .= N Ej<N Uij satisfies
Ei∣∣Ubi - Ui [I + ηN EiE(s,a)〜(sπ ,π) (saT, Ui - UiE ≤ @ 一 Y 尸
where
DsaT, Ui- UiE
sTUia 一 sTUia = Qi(s, a) 一 Qi(s, a) for almost every (s, a).
^

^
The proof is intricate owing mainly to issues of statistical dependency. It is not merely an issue that
the chain is not started from the stationary distribution; dropping the subscript i for convenience
and letting xj+1 and xj denote the vectorized forms of sj+1ajT+1 and sjajT, and similarly letting uj
denote the vectorized form of Uj, noticethatxj+1, xj, uj are all statistically dependent. Indeed, even
if xj is sampled from the stationary distribution (which also means xj+1 is distributed according to
the stationary distribution as well), the conditional distribution of xj +1 given xj is not the same as
that of xj ! To deal with such issues, the proof chooses a very small step size which ensures the TD
estimate evolves much more slowly than the mixing time of the chain. On a more technical level,
whenever the proof encounters an inner product of the form xj , uj , it introduces a gap and instead
considers uj-k, xj , where uj-k 一 uj is small due to the small step size, and these two are nearly
independent due to fast mixing and the corresponding choice of k.
A second component of the proof, which removes projection steps from prior work (Bhandari et al.,
2018), is an implicit bias of TD, detailed as follows. Mirroring the MD statement in Lemma 2.1, the
left hand side here has not only a Qi -Qi term as promised, but also a norm ControlkUi — Uik2; in
fact, this norm control holds for all intermediate TD iterations, and is used throughout the proof to
control many error terms. Just like in the MD analysis, this term is an implicit regularization, and is
how this work avoids the projection step needed in prior work (Bhandari et al., 2018).
All the pieces are now in place to sketch the proof of Theorem 1.4, which is presented in full in
Appendix D. To start, instantiate Lemma 3.1 with KL divergence upper bound ln k + 1/(1 - Y)2,
which gives the various mixing constants used throughout the proof (which we need to instantiate
now, before seeing the sequence of policies, to avoid any dependence). With that out of the way,
consider some iteration i, and suppose that for all iterations j < i, we have a handle both on the
TD error, and also a guarantee that we are in a small KL ball around ∏ (specifically, of radius
ln k + 1/(1 - Y)2). The right hand side of the simplified mirror descent bound in Lemma 2.1 only
needs a control on all previous TD errors, therefore it implies both a bound on Pj<i Vj (s) and on
Kvs(∏, ∏i). But this KL control on ∏ means that the mixing and other constants We assumed at the
start will hold for πi, and thus we can invoke Lemma 3.3 to bound the error on Qbi - Qi, which we
will use in the next loop of the induction. In this way, the actor and critic analyses complement each
other and work together in each step of the induction.
4 Discussion and open problems
This work, in contrast to prior work in natural actor-critic and natural policy gradient methods,
dropped many assumptions from the analysis, and many components from the algorithms. The
analysis was meant to be fairly general purpose and unoptimized. As such, there are many open
problems.
Implicit vs explicit regularization/exploration. What are some situations where one is better
than the other, and vice versa? The analysis here only says you can get away with doing everything
implicitly, but not necessarily that this is the best option.
More general settings. The paper here is for linear MDPs, linear softmax policies, finite state
and action spaces. How much does the implicit bias phenomenon (and this analysis) help in more
general settings?
9
Acknowledgments
MT thanks Alekh Agarwal, Nan Jiang, Haipeng Luo, Gergely Neu, and Tor Lattimore for valuable
discussions, as well as the detailed comments from the ICLR 2022 reviewers. The authors are
grateful to the NSF for support under grant IIS-1750051.
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018.
Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and al-
gorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf, 2021a.
Version: November 11, 2021.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021b.
J Andrew Bagnell and Jeff Schneider. Covariant policy search. 2003.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine learning, 22(1):33-57, 1996.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 2015.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference and q-learning
provably converge to global optima. arXiv preprint arXiv:1905.10027, 2019.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suffi-
cient to learn deep relu networks? arXiv preprint arXiv:1911.12360, 2019.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradi-
ent methods for the linear quadratic regulator. In International Conference on Machine Learning,
pp. 1467-1476. PMLR, 2018.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In International Conference on Machine Learning, pp. 2160-2169. PMLR, 2019.
Mark Jerrum and Alistair Sinclair. Conductance and the rapid mixing property for markov chains:
The approximation of permanent resolved. In STOC, pp. 235-244, 1988.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300v2, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks. 2019. arXiv:1909.12292 [cs.LG].
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
10
Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001.
Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In ICML, 2002.
Sajad Khodadadian, Thinh T Doan, Siva Theja Maguluri, and Justin Romberg. Finite sample anal-
ysis of two-time-scale natural actor-critic algorithm. arXiv preprint arXiv:2101.10506, 2021.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2020. doi:
10.1017/9781108571401.
David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov chains and mixing times. American
Mathematical Society, 2006.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. In NeurIPS, 2020.
L. Lovasz and M. Simonovits. The mixing rate of markov chains, an isoperimetric inequality, and
computing the volume. In FOCS, pp. 346-354, 1990.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In Interna-
tional Conference on Computational Learning Theory, pp. 308-322. Springer, 2007.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on
the Mathematical Theory of Automata, 12:615-622, 1962.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and trends in
Machine Learning, 4(2):107-194, 2011.
Ohad Shamir. Gradient methods never overfit on separable data. arXiv:2007.00028
[cs.LG], 2020.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 5668-5675, 2020.
David Silver. Introduction to reinforcement learning: Lecture 7. https://www.
davidsilver.uk/wp-content/uploads/2020/03/pg.pdf, 2015. Accessed:
November 13, 2021.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.
R. Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learn-
ing. In COLT, 2019.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
11
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing Systems,pp. 1057-1063, 2000.
Matus Telgarsky. Margins, shrinkage, and boosting. In ICML, 2013.
Michel Tokic. Adaptive ε-greedy exploration in reinforcement learning based on value differences.
In Annual Conference on Artificial Intelligence, pp. 203-210. Springer, 2010.
Cedric Villani. Optimal Transport: Old and New. Springer Science & Business Media, 2008.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229-256, 1992.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241-268, 1991.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale
(natural) actor-critic algorithms. arXiv preprint arXiv:2005.03557, 2020.
Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals
of Statistics, 33:1538-1579, 2005.
Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by
relative entropy policy search. In NIPS, 2013.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. Advances in neural information processing systems, 32, 2019.
12
A Background proof： existence of π
The only thing in this section is the expanded version of Lemma 1.2, namely giving the unique
maximum entropy optimal policy, and some key properties.
Lemma A.1. If |S| < ∞, then there exists a unique maximum entropy optimal policy π and corre-
sponding Q and A which satisfy the following properties.
1.	For any state s, let As denote the set of actions taken by optimal policies. Define π(s, ∙):=
Uniform(As), which is unique; then π is also an optimal policy, and let A and Q denote its
advantage and Q functions.
2.	For ^very state S and every action a, then Q(s, a) = max∏ Qn(s, a), where the maximum is
taken over all policies. Moreover, maXa∈As Q(s,a) > maXa∈As Q(s, a).
3.	π 二 limr→∞ φ(rA).
4.	Ifthere exists an irreducible optimal policy, then π is irreducible as well, and moreover has a
stationary distribution s∏.
Proof of Lemmas 1.2 and A.1.	1. Let (s1, . . . , s|S| ) denote any enumeration of the state space S.
This proof will inductively construct a sequence of optimal policies (∏i )i=0, where each ∏ is
optimal and satisfies ∏i(sj, ∙) = Uniform(Asj) for j ≤ i. For the base case, let ∏0 denote
any optimal policy, which satisfies the desired conditions since the indexing on states starts
from 1. For the inductive step, define ∏i+ι(s, ∙) = ∏i(s, ∙) for S = Si+ι and ∏i+ι(si+ι. ∙)=
Uniform(Asi+1 ). By the performance difference lemma, for any state s,
Vni (4 S) - V∏i+1 (S) = 1 - Y Es0 〜v∏ 十 ɪ <Q∏i(s0, ∙),πi(s0, ∙) - πi+1(s0, .)).
By construction, the inner product is 0 for any s0 = si+ι. When s0 = si+ι, since ∏ is
optimal, then Vni (s00) must be optimal for every state s00, which in turn means Qni (s0, ∙) must
be maximized at each a ∈ As, and therefore the inner product is 0 in this case as well. This
completes the induction, and the desired claim follows by noting ∏ = ∏∣s∣.
2.	For any π with corresponding Q function Qn and value function Vn, and any (s, a), then
Q(s,a) - Qn(s,a) = Er,s0〜(s,a) [r(s,a) + YV(s0) - r(s,a) - YVn(S0)]
=YEs0〜(s,a) hV(s0) - Vn (s0)i
≥ 0.
It follows that Q(s, a) ≥ sup∏ Qn(s, a), and since Q = Qn, then in fact Q(s, a)
maxn Qn(s, a).
3.	By the previous point, for any state S and any a ∈ As, then A(s, a) = Q(s, a) - V(s)=
0 whereas for any b ∈ As, then A(s, b) = Q(s, b) - V(s) ≤ maxb∈As Q(s, b)-
mina∈As Q(s, a) < 0. It follows that
lim φ(rA(s, ∙)) = Uniform(As) = π(s, ∙).
r→∞
4. Let π denote an arbitrary irreducible optimal policy. Since π is uniform on the set of optimal
actions in any state, then for any pair (s, s0) and time t > 0 with Pn (s, s0) > 0, then Pn(s, s0) >
0 as well. Since ∏ is irreducible, this holds for all pairs (s, s0), which means ∏ is irreducible as
well (Levin et al., 2006, Proposition 1.14), and has a stationary distribution s∏.
□
13
B Full mirror descent setup and proofs
This section first gives a basic mirror descent setup. This characterization is somewhat standard
(Bubeck, 2015), though written with extra flexibility and with equalities to preserve implicit biases
terms, which are dropped in most treatments.
First, here is the basic notation (which, unlike the paper body, will allow a subscripted step size θi
which can differ between iterations):
pi+1 := pi - θigi ,	θi > 0,
qi := Vψ(pi),	closed proper convex ψ,
《∙,∙》,	bilinear pairing,
D(p,Pi):= ψ(p) — [Ψ(Pi) +《qi,P - Pi》],	primal Bregman divergence,
D*(q, qi) := ψ*(q) — [ψ*(qi) +《Pi,q — qi》],	dualBregmandivergence.
One nonstandard choice here is that the Bregman divergence bakes in a conjugate element, rather
than using Vψ and Vψ*; this gives an easy way to handle certain settings (like the boundary of the
simplex) which run into non-uniqueness issues. Secondly, ≪∙, •》is just a bilinear form, and does not
need to be interpreted as a standard inner product.
The standard Bregman identities used in mirror descent proofs are as follows:
D*(q,qi) ― D*(q,qi+1) ― D*(qi+ι,qi) =《Pi ―Pi+ι,qi+ι ― q》,	(B.I)
D*(qi+ι ,qi) = D(Pi,Pi+1),	(Bz
D*(qi+ι,qi) + D*(qi ,qi+1)=《Pi — Pi+ι,qi — qi+i》,	(B.3)
D*(q,qi) = ψ*(q) + ψ(Pi)-《Pi, q》≥ 0.	(B.4)
With these in hand, the core mirror descent guarantee is as follows. The bound is written with
equalities to allow for careful handling of error terms. Note that this version of mirror descent does
not interpret the “gradient” gi in any way, and treats it as a vector and no more.
Lemma B.1. Suppose θi > 0. For any t and q where q ∈ dom(ψ*),
Eθi《gi, qi - q》=D*(q,qo) — D*(q,qt) + ED(Pi+1 ,Pi)
i<t	i<t
=D*(q,qo) — D*(q,qt) + £ KPi — Pi+1,qi — qi+1》一D*(qi+1,qi)]
i<t
=D*(q,	q0)	+ θ0《g0,	q0》一D*(q,	qt)	—	θt-1《gt, qt》一D*(qi+1,	qi)
i<t
t-1
+	《gi, qi》 (θi — θi-1) +	θi 《gi+1 — gi, qi+1》 .
i=1	i<t
Moreover, for any i, 《gi, qi+1》 ≤ 《gi, qi》.
Proof. For any fixed iterate i < t, by eqs. (B.1) to (B.3),
θi 《gi,qi — q》 = 《Pi —Pi+1,qi — q》
= 《Pi —Pi+1,qi — qi+1》 + 《Pi —Pi+1,qi+1 — q》
=《Pi — Pi+1,qi — qi+1》+ D∙*(q,qi) — D*S,qi+1) — D*(qi+1,qi) v eq. (B.1)
=D*(q,qi) — D*(q,qi+1) + D*(qi,qi+1)	V eq.(B.3)
=D*(q,qi) — D*(q,qi+1) + D(Pi+1,Pi),	V eq.(B.2)
and additionally note
《《Pi — Pi+1, qi — qi+1》=《gi, qi — qi+1》=《gi+1 — gi, qi+1》—《gi+1, qi+1》+《gi, qi》∙
θi
The first equalities now follow by applying Pi<t to both sides, telescoping, and using the various
earlier Bregman identities (cf. eqs. (B.1) to (B.4)).
14
For the second part, for any i, by convexity of ψ ,
0 ≤《Pi - Pi+1, Vψ(Pi) - Vψ(pi+1)^ = θi《gi,qi - qi+1》,
which rearranges to give《gi, qi+i》≤《gi, qi》since θi > 0.	□
All that remains is to instantiate the various mirror descent objects to match Algorithm 1, and control
the resulting terms. This culminates in Lemma 2.1; its proof is as follows.
Proof of Lemma 2.1. The core of both parts of the proof is to apply the mirror descent guarantees
from Lemma B.1, using the following choices. To start, the primal update is given by
gi := -Qbi ,
Pi+1 := Pi - θgi = Pi + θQi .
The mirror mapping and corresponding dual variables (with vμ baked in) are
ψ(p) := ES〜vπ ln E exp(p(s, a)) = E Vμ(s) ln E exp(p(s, a)),
a∈A
、	vμ(S)exP(P(S,a))
vψ(p)](s,a)= Pb∈A exp(p(s,b)),
s∈S
a∈A
qi := Vψ(Pi),
qi(s,a) := Vμ(s)∏i(s,a).
The primal Bregman divergence, which uses a dual iterate rather than the mirror map, is
D(p, pi) := ψ(p) - ψ(pi) - 《qi, p - pi》 .
The inner product is the standard one, meaning
《p, q》 := hp, qi =	p(S, a)q(S, a).
s∈S a∈A
Lastly, the dual Bregman divergence is given by
《q,ln喻一∑s Vn(S)InVn(S), q ∈ δs×a,
ψ*(q) = {
∞
o.w.,
D*(q,qi)∙:= ψ ④-[ψf (q)-《Pi, q - qi》
= 《q, lnq》 - 《qi, lnqi》
-E ln(qi(s,a)) + ln Vμ(s) + ln£exp(pi(s,b)	(q(s, a) - qi(s,a)
s,a
Mn 小=Kvn (∏,∏i).
b
A key consequence of these constructions is that qi , treated for any fixed S as an unnormalized
policy, agrees with πi := φ(Pi) after normalization; that is to say, it gives the same policy, and the
choice of vμ baked into the definition is not needed by the algorithm, is only used in the analysis;
the “gradient” gi = -Qbi makes no use of it.
Plugging this notation in to Lemma B.1 but making use of two of its equalities, and the performance
difference lemma (cf. eq. (1.2)), then for any μ,
θ(I- Y) X(Vi(μ) - V∏3)) = Xθ hQi,πi- πivπ
i<t	i<t
=X θ	DQi - Qi, πi	- πEvμ	+ X	DθQi, πi - πEvμ
=X θ	DQi - Qi,πi	- πE晦-X	iθigi, qi - q∏》.(B.5)
The proof now splits into the two different settings.
15
1.	(Simplified bound.) By the above definitions and the first equality in Lemma B.1,
-E《仇gi,qi - q∏》=D*(q, qt) - D*(q, qO)- ED(Pi+ι,pi),
i<t	i<t
where the last term may be bounded in a way common in the online learning literature (Shalev-
Shwartz, 2011): since ez ≤ 1 + z + z2 when z ≤ 1, setting Z(s, a) := Qbi(s, a) - Ci for
convenience (whereby Z(s, a) ≤ 0 ≤ 1 as needed by the preceding inequality),
D(pi+1, pi)
ln	exp(pi+1(s, a)) - ln	exp(pi(s, a)) -	πi(s, a)(pi+1(s, a) -pi(s, a))
=Es 〜v∏
=Es 〜v∏
=Es 〜v∏
≤ Es〜v∏
≤ Es〜v∏
a
a
πi(s, a) exp θQbi(s, a)	- θ Xπi(s, a)Qbi(s, a)
a
∏i(s, a) exp (θZ(s, a)) J — θ ^X πi(s, a)Z(s, a)
πi (s, a)(1 + θZ (s, a) + θ2Z(s, a)2) - θ X πi(s, a)Z (s, a)
Xπi(s, a)(1 + θZ(s, a) + θ2Z(s, a)2) - 1 - θ Xπi(s, a)Z (s, a)
a
=Es〜v∏ Eni(s, a)θ2Z(s, a)2 ≤ θ2C2,
a
which together with the preceding as well as eq. (B.5) gives
θ(1 — Y) X(Vi(μ)-V∏ (μ))
i<t
=X “Qi — Qi,πi - π吟—X iθigi, qi - q∏》.
≥ ^X θ DQi -Qi, πi — πE μ + Kvn (π,πt) — Kvn (π,π0) — θ2 ^X C2,
i<t	vπ	i<t
which gives the desired bound after rearranging.
2.	(Refined bound.) By Lemma B.1 with the above choices and any measure vμ, then
(Qi, πi+l) ^μ = -《gi, qi+1》≥ -《gi, qi》=(Qi, πi)U ,	(BS)
and also
-	≪θigi, qi - q∏》=-D*(q, q0) - θ《g0, q0》+ D*(q, qt) + θ《gt, qt》+	DKqi+1, qi)
i<t	i<t
-	θ 《gi+1 - gi, qi+1》
≥ Kvn (n, πt) - Kvn (n, π0) - ɪ---+ ^X θ DQi+1 - Qi,πi+1)	.
1 - Y <
To simplify these further, first note by eq. (B.6) with measure vδπsi+1 for any state s and the
performance difference lemma that
0≤ Qbi, πi+1 - πi vδπsi+1
= Qbi - Qi , πi+1 - πi
vδπsi+1
+ hQi, πi+1 - πiivδπs
≤ 2^i + (1 - Y)(Vi+ι(δs)-Vi(δs)),
16
which rearranges to give
2^
Vi+ι(δs) ≥Vi(δs)- -	∀s,
1 - γ
which itself in turn implies
Qbi+1 - Qbi , πi+1	≥
∖	/ v∏
hQi+1 - Qi, πi+1ivμ - ,≡i - ^i+1
YE	S 〜vμ	(Vi+1(δs0)
a 〜∏i+ι(s,∙)
s0〜(s,a)
Vi(δs0)) - Wi - <≡i+l
2γ^i八 八
------Ci - 6i+1.
1 - γ
(The preceding derivation works if vμ is replaced with any measure on states.) Plugging this
all back in to eq. (B.5) gives
Vt-ι(μ) - v∏(μ) ≥ t X (Vi(μ) - v∏(μ)) - X
i<t
≥ Kvn (π,πt) - Kvn (π,π0)
_	tθ(1 - Y)
2(1 + i)ci
-< w-ɪ,
i<t
2(1 + i)ci
t(1 - γ)
t(1 - γ)2	t(1 - γ)
Σ
i<t
+ i + i+1
≥
—
—
—
1
1
—
where the last term may be omitted for the summation version, and these expressions rearrange
to give the final bounds.
□
C S ampling proofs
As in the body, this section both provides tools to control mixing times, and also a generalized TD
analysis.
C.1 Mixing time controls within KL balls
To control mixing times, these lemmas will use the notion of conductance:
吊*	•	吊 /6	V 吊 /6	Ps∈S Ps0∈S Sn(S)P∏(s,s0)
Φ∏ :=	min	Φ∏(S),	where Φ∏(S) ：=--------------------------------.
π	S⊆∣S∣=s∏(S)≤1∕2	Sn (S)
This quantity was shown to control mixing times of reversible chains by Jerrum & Sinclair (1988)
(for more discussion, see Levin et al. (2006, eq. (7.7), Theorem 12.4, Theorem 13.10)), but a
later proof due to Lovasz & Simonovits (1990) requires chains to only be lazy, and does not need
reversibility. Our chains can be made lazy by flipping a coin before each step, but in our setting we
can avoid this and directly use the mixing time of the lazy chains to control the mixing time of the
original chains.
As a first tool, note that if policies are similar, then their stationary distributions and conductances
are also similar.
Lemma C.1. Let constant c > 0 and an aperiodic irreducible policy π be given，and define a Set of
policies whose action probabilities are similar:
P := Ππ	: ∀s,	a, π(s, a) > 0 ■工 ≤ Q[	≤ c].
[	c -	π(s, a)	- J
Then there exists a constant C > 0 so that the stationary distributions and conductances are also
similar: for any π ∈ P and any state s,
ɪ ≤ SΦ4 ≤ C,	Φ∏ ≥ 却∏.
C	s∏ (s, a)	C
17
Proof. First note that since ∏ is aperiodic and irreducible, it has a stationary distribution s∏ where
necessarily s∏ (S) > 0 for all states S. Next, recall that the stationary distribution can be characterized
in terms of hitting times (Levin et al., 2006, Proposition 1.19), meaning
*π(S)	E[min{t > 0 : St = s}∣so = s].
As discussed in proofs that the denominator is finite (Levin et al., 2006, proof of Lemma 1.13), let-
ting P∏ and P∏ corresponding to the state transitions inducedby taking a step by ∏ and ∏ respectively
and then using the MDP dynamics to get to a state, there exist r > 0 and e > 0 so that P∏ (s, s0) > E
for any states S, S0 and any j ≥ r, therefore Pπj (S, S0) ≥ c-j. In fact, for a fixed S, letting js ≥ 1
denote the smallest exponent so that PnS (s, S) > 0, then En[min{t > 0 : St = s}∣s0 = s] ≥ j§,
meanwhile, as in Levin et al. (2006, proof of Lemma 1.13), defining τπ+ := min{t > 0 : St = S},
E[τ+ls0 = s] = EPr[τ+ > t|s0 = s]
t≥0
≤ XjsPr[τn+ > kjs |S0 = S]
k≥0
≤ js X(i-c-js Pns (s,s))k
≤ js
_「js Pns (s,s)
≤ E[τ+ls0 = s]
-c-jsP∏s (s,s),
where the denominator does not depend on π and thus the ratio is uniformly bounded over P . (We
can produce a bound in the reverse direction trivially, by using Sn(s)∕s∏(S) ≤ 1∕s∏(s).) Let Co
denote the maximum of this ratio and c.
Bounding the conductance is an easy consequence of the definition of C0: using C0 to swap various
terms depending on ∏ with terms depending on ∏, it follows that
φΠ ≥ C4 φn.
The proof is now complete by taking C := C4 as the chosen constant.	□
Next, we use the preceding fact to obtain mixing times; the proof will need to convert to lazy
chains to invoke the mixing time bound due to Lovasz & Simonovits (1990), but then will use a
characterization of mixing times via coupling times to reason about the original chain (Levin et al.,
2006, Theorem 5.4).
Lemma C.2. Let constant c > 0 and an aperiodic irreducible policy π be given, and define a Set of
policies
P := Ππ : ∀s, a, ∏(s, a) > 0 ■工 ≤ (I[ ≤ cl .
[	c - π(s, a) - J
Then there exist constants m1, m2 > 0 so that for every π ∈ P and every t,
sup IIPt(s, ∙) -Sn∣∣TV ≤ mιe-m2t.
s
Proof. For any policy π, let νn denote the corresponding lazy chain: that is, in any state, νn does
nothing with probability 1∕2 (it stays in the same state but does not interact with the MDP to receive
any reward or transition), and otherwise with probability 1∕2 uses π to interact with the MDP in
its current state. Equivalently, if S	6=	S0, then	Pνπ (S, S0)	=	Pn (S,	S0)∕2,	whereas	Pνπ (S, S)	=
(1 + Pn (s,s))∕2. Define V = Vn for convenience, and a new set of nearby policies:
PV := { ∏ : ∀S, s0, Pν(s, s0) > 0 . ɪ ≤
PVn(S, s') ≤ c0]
Pν(s,s0) 一 ʃ
18
where it will be shown that C can be chosen so that PV ⊇ P. Indeed, by definition of P, since ∏∕Π
is bounded, then so is Pn ∕P∏, and in particular pick any C > 0 so that for any ∏ and any pair of
states (s, s0) with Pn (s, s0) > 0, it holds that
1 ≤ Pn(S,sO) ≤ c0
C0 — Pn (s,s0)一.
To show that this choice ofC0 suffices, first consider the case of a pair of different states states s 6= s0:
then
ɪ ≤ Pn U') = 2Pν∏(s,S0) = PVnM = Pn (s,S0) ≤ /
c0 — Pn(s, s0) 2Pν(s, s0)	PV(s, S)	Pn(s, S) ~ C ,
whereas in the case S = s0, if Pn (s, S) ≥ Pn (s, s), then
0 ≥ Pn(s,s) ≥ (1+ Pn (S,S))∕2 = PVn(S,s) ≥ I
≥ Pn(s,s) ≥ (1+ Pn(s,s))∕2 = Pv(s,s) ≥ ,
with an analogous relationship when Pn(S, S) ≤ PnV (S, S), which implies
C ≥ PVn (s, S) ≥ ɪ.
-Pν(s,s) - c0
Consequently, it follows that PV ⊇ P, and by Lemma C.1, that there exists a constant C so that
every π ∈ P satisfies
1 ≤ SVn(S,a) ≤ C	Φ* ≥ -1φt
C ≤ Sv(S,a) ≤ C,	φ"∏ ≥ Cφ”.
(One reason for the prevalence of lazy chains is that they always have unique stationary distribu-
tions (see, e.g., (Levin et al., 2006; Lovasz & Simonovits, 1990)), but as in the preceding, in our
setting we always have stationary distributions automatically; thus we did not need to use laziness
algorithmically, and can use it analytically.)
Since the conductance is uniformly bounded for every element ofPV, there exist positive constants
m3, m4 so that for every π ∈ PV, (Lovasz & Simonovits, 1990),
sup kPVtn (s, ∙) - SVnlITV ≤ m3 exp(-m4t).
s
Now define pmin := mins SVV(S)∕C > 0, whereby it holds by the above that pmin ≤
inf n∈Pν mins SVn (S). As such, by the preceding mixing bound, there exists a t0 so that, simul-
taneously for every π ∈ PV, for all t ≥ t0, by the definition of total variation (instantiated on
singletons), for every S0, sups PVtn (S, S0) ≥ pmin∕2 > 0. Now consider some fixed π ∈ P, which
also satisfies π ∈ PV since PV ⊇ P as above, and consider the coupling time of two sequences
(x0, x1, . . .) and (y0, y1, . . .) which start from some arbitrary pair of states (x0, y0), and thereafter
each steps according to Pn (Levin et al., 2006, Section 5.2). Instead of running Pn directly, simulate
it as follows: use PVn , but discard from the sequence any steps which invoke the lazy option. Then,
for t ≥ t0 , the probability of both chains not choosing the lazy option and jumping to the same state
is at least p := Ps(pmin∕2)2∕4 > 0. As such, for any t ≥ t0, the probability that the two chains did
not land on the same state somewhere between t0 and t is at most
(1 - p)t-t0 ≤ (1 - p)-t0 exp(-pt).
But this is exactly an upper bound on the coupling time, meaning by (Levin et al., 2006, Theorem
5.4) that
SUp l∣P5t(x0, ∙) - Pn(yo, ∙)∣TV ≤ (1 -p)-t0 exp(-pt),
x0,y0
which in turn directly bounds the mixing time (Levin et al., 2006, Corollary 5.5), indeed with con-
stants m1 := (1- p)-t0 and m2 = p. Since these constants do not depend on the specific policy π
in any way, the mixing time has been uniformly controlled as desired.	□
With these tools in hand, we may now control mixing times uniformly over KL balls.
19
Proof of Lemma 3.1. By definition of Kν and Pc, for any π ∈ Pc, letting (s0, a0) denote any pair
which maximizes π(s, a)∕π(s, a) (which implies ∏(s0,a0) > 0), and since Pa q° ln q0 ≥ 一 ln k for
any probability vector q over actions,
C ≥ KV(∏, ∏) = ^X V(S) ^X ∏(s,a)ln
s∈S	a
π(s, a)
π(s, a)
≥ v(s0)∏(s0,
a0) ln
π(s0, a0)
π(s0, a0)
v(s)∏(s, a) ln
(s,a)6=(s0 ,a0 )
π(s, a)
π(s, a)
+
Σ
≥ VISW a)ln :；：/：) + V(SO) X π(S0, aln πs14
π(s , a )	a6=a0	π (s , a)
,	, ,	∏(s0 a0)	,
≥ V(s0)∏(s0,a0)ln ∏⅛a) — V(s0)ln k,
π(S0, a0)
then
π(s0, a0)
π(S0, a0)
≤ 昨(V(s0)∏(s0,ao
ι ln k
0) + ∏(s0,a0)
≤ max
exp
c
a00) mins V(S)
ln k
+ ∏(s0, a00)
:∏(s00,a00) > θ},
c
where the final expression does not depend on π; it follows that ∏(s, a)∕π(s, a) is uniformly upper
bounded over Pc. On the other hand, ∏(s, a)∕π(s, a) ≥ ∏(s, a), so the ratio uniformly bounded in
both directions over Pc, and let C0 denote this ratio.
This in turn completes the proof: via Lemma C.1, the ratio of stationary distributions is also uni-
formly controlled, and via Lemma C.2, the mixing times are uniformly controlled. To obtain the
final constants, it suffices to take the maximum of the relevant constants given by the preceding. □
C.2 TD guarantees
The first step is to characterize the fixed points of the TD update, which in turn motivates the linear
MDP assumption (cf. Assumption 1.3), and is fairly standard (Bhandari et al., 2018).
Lemma C.3. Let any policy π be given, and suppose X 〜(s∏ ,π) is SamPkdfrom the stationary
distribution (in vectorized state/action form), and x0 is a subsequent sample. Then, letting (ExxT)+
denote the pseudoinverse of ExxT,
U := ^X (Y [ExxT]+ Ex(x0)T) [ExxT] + Exr
t≥0
is a fixed point of the expected TD update, meaning
U = E U — ηx
一 γx0,
Moreover under the linear MDP assumption (cf. Assumption 1.3), then XTaU = Qn (s, a) for almost
every (s, a). Lastly, kuk ≤ 2/(1 — Y).
Proof. Let X denote the span of the support ofx according to its stationary distribution; since ExxT
is symmetric and real, then ExxT(ExxT)+ = ΠX, where ΠX denotes orthogonal projection onto X.
For the form of the fixed point, it suffices to show
Ex (〈x — γx0, U〉— r) = 0.
20
To this end, first note that
[ExxT] U = [ExxT] ^X (Y [ExxT] + Ex(x0)T) [ExxT]+ Exr
t≥0
= [ExxT] [ExxT]+ Exr + [ExxT] X (Y [ExxT]+ Ex(x0)T [ExxT]+ Exr
t≥1
ΠX Exr + [ExxT γ [ExxT+ [Ex(x0)T X γ [ExxT+ Ex(x0)T [ExxT+ Exr
t≥0
=ΠχExr + YΠχ [Ex(x0)T] U.
=Exr + γ [Ex(x0)T] U.
This completes the fixed point claim, since
Ex (〈x — γx, Ui- r) = [ExxT] U — Y [Ex(x0)T] U — Exr = 0.
For the claims under the linear MDP assumption, using the notation from Assumption 1.3, letting
Xπ denote the transition mapping induced by π, note by the tower property that
E (x(x0)T) = E (XE [(x0)T∣x]) = E (XE [(Xπv0)T∣x]) = E (X(XnMX)T) = ExxTMTXn,
and therefore, for any xsa in the support of π (which implies xsa ∈ X), and since x0 ∈ X as well,
XsaU = xTa ^X (Y [ExxT]+ Ex(x0)T) [ExxT]+ Exr
t≥0
= xTsa X (Y [ExxT]+ ExxTMTXnT [ExxT]+ ExxTy
t≥0
=xTa X (YnXMTXn)t πXy
t≥0
=xTa X(YMTXn)t y
t≥0
= Qn (s, a).
Lastly, since U ∈ X,, and since ∣Q∏(s,a)∣ ≤ 1/(1 - Y) due to rewards lying within [0,1], and since
1/2 ≤ ksk ≤ 1, then
kUk
IUTXsaI
SUp	Il Il
xsa∈X	ksk
sup
xsa ∈X
Qn (s, a)
2
≤----
—1 - Y
□
Now comes the core TD guarantee. In comparison with prior work (Bhandari et al., 2018), the need
for projections and starting from the stationary distribution are both dropped.
Lemma C.4. Let a policy π be given which interacts with an MDP whose states s ∈ Rd satisfy
ksk ≤ 1, and whose action set is finite and represented as {e1, . . . , ek}; for convenience, let x =
vec(seTk) denote a canonical vectorization of state/action pairs, as in the body of the paper. Let Pn
be the Markov chain on states induced by policy π, and assume it satisfies the following mixing time
bound:
sup IlPn(S, ∙) -SnIITV ≤ me-ct.
s
Let state/action/reward triples (sj, aj, rj)j<N be sampled via interaction of π with the MDP, where
the initial state s0 is arbitrary, the random rewards satisfy rj ∈ [0, 1] almost surely, and write
xj = vec(sj ajT), and let E~x,~r denote the expectation over this trajectory.
Consider the stochastic TD updates defined recursively via U0 = 0 and thereafter
Uj+1 := Uj - ηxj UTxj - YUTxj+1 - rj ,
21
and let U be a fixed point ofthe corresponding expected TD updates with stationary samples, mean-
ing
Eχ,r〜(s∏,∏)x (〈x - γx0, U - r) = O,
X0〜X
where x is sampled from the stationary distribution and x0 is a subsequent sample, and assume
kuk≤ 2/(1 - γ)∙
If the TD parameters N and η are chosen according to
N≥k,
1	ln N + ln m
η ≤ ---1--, where k =-----------
—400VkN	C
then the average TD iterate U := N Pj<N Uj satisfies
E~,~ (ku - uk2 + nN EXsa 〜(s∏ ,π) hxsa,u - ui2) ≤ (1 -4γ)2 .
Proof∙ The structure and primary concerns of the proof are as follows. The main issue is that
Uj, xj, xj+1 are statistically dependent; in fact, even in the unlikely but favorable situation that xj is
distributed according to the stationary distribution (sπ, π), the conditional distribution of xj+1 given
xj can still be far from stationary, even though xj +1 without conditioning is again stationary. The
main trick used here is that η is so small relative to the mixing time that Uj evolves much more slowly
than xj , and thus any interaction between xj and Uj can be replaced with an interaction between xj
and Uj-k, which are approximately independent. The structure of the proof then is to first establish
a few deterministic worst-case estimates on the behavior in any consecutive k iterations, and then to
perform an induction from k to N .
For notational convenience, since π is fixed in this proof, s is written for sπ . Additionally, E≤N
denotes the expectation over ((xj, rj))j≤N, replacing the E~X,~r from the statement and allowing
further flexibility by allowing the subscript to change.
Worst case control between any Uj and Uj-k. Proceeding with this first part of the proof, define
Tj(U) := xj 〈xj -γxj+1,U - xjrj,
whereby
Uj+1 = Uj - ηTj (Uj ),
kuj+ι - UjIl = nkxj〈xj - Yxj,uj〉- XjTjIl ≤ n ((I + Y)kujIl + 1),
j-1	j-1
IUj -Uj-kI ≤	IUi+1 - UiI ≤ kη+η(1 +Y) IUj I.	(C.1)
i=j-k	i=j-k
These inequalities will be useful in the induction as well, but now consider the first k iterations.
Controlling the first k iterations. Specifically, for any i ≤ k, it will be established via induction
that
+ n X(1 + 1∕(200k))l
The base case follows since uo = 0 and ∣∣U∣ ≤ 2/(1 - Y). For the inductive step, since n(1 + Y) ≤
1/(200k),
∣∣Ui+1 - Uk = ∣∣Ui - u - nxi hxi - γxi+ι,Ui - u + Ui + nxiTik
≤ (I + n(1 + γ))k% - Uk + n(1 + Y)kUk + n
≤ (1 + 1∕(200k))i+1 (1-^) + nX(1 + 1∕(200k))l+1
≤ (1 + 1∕(200k))i+1 (---) + n X (1 + 1∕(200k))l
Y	l<i+1
∣Ui- Uk ≤ (1 + 1∕(200k))i (ɪ
1-Y
4	2+2Y
+-- ) + n ( 1 + -,-
1-Y	1-Y
4
1 - Y
22
Since (1 + 1/(200k))i ≤ 2 for all i ≤ k, then
4+8kη 5
kui - Uk ≤ 1------≤ ；-----.	(Cz
1-γ	1-γ
This concludes the proof for the initial k iterations.
Controlling the remaining iterations via induction. The rest of the proof now proceeds via
induction on on iterations k and higher: specifically, given i ∈ {k - 1, . . . , N - 1}, it will be shown
that
i
E≤N∣∣Ui+ι - Uk2 + ηE≤N EEx〜(s,n)(x,Uj- U
j=k
/	26
≤ (1 - γ)2 .
(C.3)
The base case i = k - 1 follows from eq. (C.2) (since the second term in the left hand side here
is an empty sum), thus consider i + 1 with i ≥ k - 1. The remainder of this inductive step will
first introduce a variety of inequalities which need to hold for all j ∈ {k, . . . , i}, before returning to
consideration of i at the end.
Before continuing with the core argument for a fixed j , there are a few useful inequalities to estab-
lish, which will be used many times.
•	Combining the inductive hypothesis with eq. (C.1),
j-1
EkUj - Uj-kk2 ≤ 2k2η2 + 2η2(1 + γ)2k X EkUj — U + U∣2.
i=j-k
≤ 2k2 η2 + 2η2(1 + γ)2kX ((⅛ +(Γ⅛ )
500k2η2
≤ (1 - Y)2.
(C.4)
This is the explicit expression that will appear when moving between Uj and Uj-k to introduce
(approximate) statistical independence.
•	Next come a variety of bounds on Tj . First, for any j ≤ i, using the inductive hypothesis and
∣∣U∣∣ ≤ 2/(1 - Y), for any (x, x0, r) and using the notation Tx,xo,r (u) = x(x - Yx, U - xr,
E≤nkTX,xo,r(Uj)k2 = E≤N∣∣x〈x - γx0,Uj - U + U - xr∣∣
≤ E≤n4 ((1+ γ)2kUj - Uk2 + (1 + Y)2l∣Uk2 + 1)
4(26(1 + γ)2 +4(1 + γ)2 + (1- γ)2)
≤	^-Y2
,	500
≤ (1-.
Separately, making use of eq. (C.4),
E≤N kTj (Uj) - Tj(Uj-k)k2 = E≤N xj 〈xj - Yxj+1, Uj - Uj-k
≤ E≤Nkxjk2 ∙ kxj - Yxj+1 k2 ∙ kUj - Uj-kk2
2000k2η2
≤ (1 - y)2 .
(C.5)
(C.6)
(C.7)
•	Lastly, the convenience inequality which abstracts the application of mixing. Let E|j-k denote
the expectation of (xj, xj+1, rj) conditioned on all information up through time j -k, meaning
(xl, rl)l≤j-k. By the coupling characterization of total variation distance (Villani, 2008, Equa-
tion 6.11), there exists a joint distribution ρ over two triples (xj, xj+1, rj) and (x, x0, r) where
23
the marginal distribution of (xj, xj+ι,r7-) is Ej-k, and the marginal distribution of (z, z∖ S) is
(s,π), and crucially the choice of k implies
sup Prρ[(xj,xj+ι,rj) = (x,x',r)] ≤ sup ∣∣Pk(Sj-, ∙) - SllTV ≤ me-ck ≤ ɪ.
Sj-k	sj-k	N
(Note that (Xj)j≤Ν inherit the mixing time for (Sj)j≤N since Xj = VeC(Sjaj), and (07-)j≤N
are conditionally independent given (Sj)j≤N.) Using this inequality, and moreover making use
OfkUk ≤ 2/(1 - Y) and eqs. (C.5) and (C.6), and defining Ts :=旧%M/〜(s,π)7X,x-r to denote
the update at stationarity,
E≤N(uj-k - u, TS (uj-k ) - Tj (uj-k ))
=E≤j-k (Uj-k - u, TS (uj-k ) - E| j-kTj (uj-k ))
=E≤j-k u uj - k - u, EPTxH,r (Uj-k) - TXj ,Xj+ι ,rj (Uj-k))
≤ E≤j-k l l uj-k - U 11 l l EPTrxj,Xj 十ι,rj (Uj-k) -TX,x0 ,r (Uj-k )1]
≤比
≤ S
E≤j-k I I uj-k - Ull JE≤j-k EP I I TXj ,Xj+ι,rj (uj-k) - Tχ,x0,r (uj-k )||
500k2η2 *
(T-^)2
・ ʌ/E≤j-k Eρ1 [(xj ,xj + 1,rj ) = (X,x∖r)] .llTxj,Xj 十ι,rj (uj-k ) - TX,x0,r (uj-k 升
≤ y1600(1 - Y)2
- JE≤j-k 16 (4∣Uj-k - u∣2 + 4∣uk2 + 1) Eρ1[(xj,xj+ι,rj) = (x,x',r)]
≤;
≤s
1600(5 - Y )2 ∖∣ Nn E≤j-k 16 (4∣uj-k - u∣2 + 4ku∣2 +1)
5
2~2000
1600(1 - Y)2 V N(1 - Y)
2
3
≤
(1 - γ)2√N.
(C.8)
Now comes the main part of the inductive step. Expanding the square and making one appeal to
eq. (C.6),
E≤N∣∣Uj+ι - u∣2 = E≤N∣∣Uj - u∣2 - 2ηE≤N〈uj - u,Tj(uj)〉+ η2E≤NIlTj(Uj)∣2
_	..	,,c	_ /	一，、∖	500η2
≤ E≤N∣∣Uj - u∣2 - 2ηE(Uj - u, Tj(uj))+ (J _ Y)2 .	(C.9)
To lower bound the middle term, making extensive use of eqs. (C.4), (C.6) and (C.7) combined with
the Cauchy-Schwarz inequality, and the inductive hypothesis to control E≤NIlUj-k - U∣2,
E≤N〈uj -	u, Tj (uj ))≥ E≤N〈uj-k	- u, Tj (uj ))- E≤N 11u j - k - uj k ∙ HTj" (uj ) k
≥ E≤N〈uj-k	- u, Tj (uj-k ))- E≤N 11u j - k - Uj I ∙ IITj (uj )k
-E≤N lluj-k - Ull，lTj(uj) - Tj(uj-k) l
≥ E≤N〈Uj-k - U, Tj- (Uj-k )〉- ʌ/E≤N ∣∣Uj-k - Uj Il2 y E≤N ∣∣Tj∙ (Uj )k2
-ʌ/E≤Nluj-k - u| Y E≤N llTj (uj) - Tj (uj-k ) l
≥ E≤N uj-k
≥ E≤N uj-k
500kη	250kη
—
(1 - Y)2	(1 - Y)2
750kη
(1-Y2.
24
Now comes the key step of the proof, which uses the mixing time and introduced gap of size k to
replace Tj(uj-k) with Ts (uj-k); this reasoning is captured in eq.(C.8), which gives
E≤N(uj —k - u, Tj (Uj— k ))= E≤N(uj —k - u, TS (Uj— k ))- E≤N ^uj-k - u, TS (Uj— k ) - Tj (Uj— k ))
Ll	一、∖	3
≥ E≤N {uj-k - U, TS (Uj-k ))— -	、2 B.
(i — Yy VN
Again continuing with the non-constant term, and using the general inequality 2ab ≤ a2 + b2 which
holds for any reals a, b, letting x 〜(s,π) and x0 〜x and r a random reward (i.e., all the random
quantities used to construct TS, though r will cancel), and lastly introducing TS (U) via the fixed
point property in the form TS (U) = 0, and the fact that χ0 has the same distribution as X when it
appears alone,
(Uj — k — u, TS (Uj— k)) =(Uj —k — u, TS (Uj— k) — TS (U))
=ExbH〈x，Uj—k 一 U〈x 一 γx0, Uj—k 一 U
=Ex,r,x0〈x, Uj — k 一 U)2 一 Y〈x, Uj — k 一 U)〈x', Uj — k 一 U)
≥ ExbH〈x,Uj -k — u)2 — y/2 (〈x,Uj -k — U)2 +〈x,Uj-k — U)2)
=(1 — Y)Ex〈x, Uj—k — u)2 ,
where
E≤nEx(x, Uj—k — u)2 = E≤nEx(x, Uj — u)2 + 2E≤nEx(x, Uj — u)(x, Uj—k — Uj)
+ e≤NEx (x, Uj — k 一 uj)
≥ E≤nEx (x,Uj — U)2 — 2 Je≤n∣∣Uj — U∣∣2 Je≤n ||uj-k 一 Uj ∣∣2
≥ E≤nEx (x, Uj — u)2 —
2√26 ∙ 500k2η2
(i—γ)2
≥ E≤nEx (x, Uj — u)2 —
800kη
(1-Y2.
Plugging all of this back in to eq. (C.9) gives
E≤n∣∣Uj+ι — U∣2 ≤ E≤n∣∣Uj — U∣2 — η(1 — Y)E≤nEx (x,Uj — u)2 +
i600kη2 + 3η∕√N
(1-^)2
which after summing over all j ∈ {k,..., i} and telescoping and re-arranging gives
i
E≤n∣∣Ui+ι — U∣2 + £n(1 — y)E≤nEx(x,Uj — U)
j = k
_ 2 , XX 1600kη2 + 3^ √N
≤ e≤nIlUk - UIl +	-----(1 一 γ)2----
j=k
25	Z 1600kη2 + 3η∕√N
≤ Ly +	( —LP—
26
≤ (1—^7,
which establishes the inductive hypothesis stated in eq.(C.3).
Final cleanup. To finish the proof, a tiny amount of cleanup is needed. Adding the missing prefix
of the sum from the conclusion of the induction gives
E≤n∣∣un — u∣2 + £ η(1 — y)E≤nEx (x,Uj — u)2 ≤
j<N
≤
≤
26
(1-Y2
+ £n(1 — y)E≤nEx (x, Uj — u)2
j<k
26 ι 25kη(1 — γ)
Ly + (1 — γ)2
27
(1-Y2.
25
The final bound now follows by using Jensen's inequality to introduce UN in the summation term,
and introducing UN within the norm term by noting the bound held for all i < N and thus the
triangle inequality implies ∣∣U - Uk ≤ Pi<N IlUi - U∣∣∕N ≤ √27∕(1 - Y), whose square can be
added to both sides to give the final bound.	□
These proofs immediately imply Lemma 3.3.
Proof of Lemma 3.3. Lemma 3.3 is a restatement of Lemma C.4, but using a combination of As-
sumption 1.3 and Lemma C.3 (and in particular the fixed point U defined in the latter) to simplify
Lemma C.4.	□
D Proof of Theorem 1.4
Combining the mirror descent and sampling tools, we can finally prove Theorem 1.4.
Proof of Theorem 1.4. Throughout this proof, let δ > 0 denote a unit of failure probability; the final
bound will use the choice δ := t-9/8/2, although most of the proof will simply write δ for sake of
interpretation.
1
(1-Y2
Define the following KL-bounded subset of policy space:
P := \ Ps,	where Ps := < ∏ : Kvs(∏,∏) ≤ ln k +
s∈S
By |S| applications of Lemma 3.1 (one for each Ps) and taking maxima/minima of the resulting
constants, since Sn is positive for every state, then there exist constants Pmin > 0, Ci > 0, and
C2 ≥ 1 so that, for any π ∈ P and any state s and any optimal action a ∈ As,
/一 /、	/	、、π(S, a)
Pmin ≤ sπ(S),	π(s, a) ≥ 万 ,
C2
and letting Pπ denote the transition matrix on the induced chain on S, for any time q,
max kPπq (S) -sπkTV ≤ C2 exp(-C1q).	(D.1)
s∈S
Lastly, the fully specified parameters N and η are
N :	107t2C4 ln C2 ln 1107t2C4 ln。2、	.	1
:=	PminCI	[	PminCI	' n ∙=400√N,
which after expanding the choice of θ satisfy
N = Θ (t2 ln t) , n = Θ ( / 1	),
V N	WN ln N)
where k :
ln N + ln C2
C
1	≤ Pmin
Nn ≤ 4tc2,
where the first two match the desired statement, and the last inequality is used below.
The proof establishes the following inequalities inductively: defining εj for convenience as
εj := SUp (Qi(s, a) - Qi(s, a)) + nNE(s,a)〜(s∏,π)
s,a
2
Qi (S, a) - Qi (S,
then with probability at least 1 - 2iδ,
Kvn(π, πi) + θ(I- Y) X(Vj(S)-Vn(S))
j<i
EQbj εj
≤ lnk +0⅛,
/	54
≤	(1-^F,
/	54
≤ δ(1- γ)2 ,
∀S,
∀j ≤ i,
∀j ≤ i,
(IH.MD)
(IH.TD.1)
(IH.TD.2)
26
where EQb denotes the expectation over the new N examples used to construct Qbj but conditions on
the prior samples. The first inequality, eq. (IH.MD), implies πi ∈ P directly, and moreover implies
the final statement when i = t after plugging in δ = t-9/8/2 and rearranging.
To establish the inductive claim, consider some i > 0 (the base case i = 0 comes for free), and
suppose the inductive hypothesis holds for i - 1; namely, discard its 2(i - 1)δ failure probability,
and suppose the three inequalities hold. This induction will first handle the mirror descent guarantee
in eq. (IH.MD), and then establish the TD guarantees in eqs. (IH.TD.1) and (IH.TD.2) together.
The first inequality, eq. (IH.MD), is established via the simplified mirror descent bound in
Lemma 2.1. To start, since KV (∏, ∏o) ≤ ln k for any measure ν, then instantiating the simplified
bound in Lemma 2.1 for the first i iterations for any starting state s gives
Kvn(Gni) + θ(I- Y)£ (Vn(S)-Vj(S))
j<i
≤ lnk + θ X DQj- Qj,∏j - ∏E s + θ2 Xsup Qj(s, a)2.
j<i	vπ	j<i s,a
(D.2)
Upper bounding the second two terms will make use of upper bounds on (εj)j<i, but rather than
using only eq. (IH.TD.2), here is a more refined approach. For each j < i, define an indicator
random variable
Fj :=1
54 √t
εj> ∏f
which by eq. (IH.TD.1) and Markov’s inequality (since εj ≥ 0) satisfies
PrQj(Fj) ≤ PrQjhεj > √tEQjεji ≤ √t..
By Azuma’s inequality applied to the i binary random variables (Fj)j<i (where Fj -EQb Fj forms a
Martingale difference sequence since EQb conditions on the old sequence and takes the expectation
over the N new samples), with probability at least 1 - δ, and plugging in the choice δ = t-9/8/2,
Fj ≤	Pr(Fj)+
≤ ʤ ln t;
henceforth discard this failure probability, bringing the total failure probability to (2i - 1)δ. Com-
bining this with eq. (IH.TD.2) gives
εj ≤	εj+	εj
j<i	Fjj<=i1	Fjj<=i0
'Xl	54	ιX^ 54√t
一乙 δ(1 - γ)2 + 乙(1 - γ)2
j<i	j<i
Fj=1	Fj =0
≤	54	(√tn +13∕2∖
-(I-Y)2 I δ J
/ 270t136√ln7 /	1
≤	(1 - γ)2≤ 8θ2(1 - γ)2 .
27
Turning back to eq.(D.2),this gives a way to control the middle term: since π(s, b) = 0 for b ∈ As,
X (Qj- Qj ,πj- π) Vs ≤ X max (Qj- Qj ,πj- π) δ
j<i	π j<i ≤ pmn X DQj-Qj,πj- πL =	1X (	E	(Qj(S,a) -Qj(S,a)) Pmin M '(s,a)〜(Sj,∏j ) + E X ∏(s,a)(Qj(s, a)-Qj(s,a)f) S 〜Sj z  	) a∈As 2C2 ≤	E	|Qj(S, a) -Qj(S,a)| Pmin M (s,a)〜(Sj,∏j ) ≤ 2C2√i JXf E	(Qj (S,a) -Qj(S,a))2 pmin V j-<i (S⑷〜(Sj ,πj ) ≤	2C2√L x p Pmin √Nηy £ 1 ≤ 2θ(1 - Y).
Meanwhile, for the last term in eq. (D.2), since sups,a Qj (s, a) ≤ 1/(1 - γ),
sup Qbj(s, a)2 ≤ 2 sup Qj (s, a)2 + Qbj (s, a) - Qj (s, a)
s,a	s,a
j<i	j<i
2t
≤ (1 -Yy +2∑ej
j<i
1
≤ 2θ2(i - Yy.
Plugging all of this back in to eq. (D.2) gives
≤
≤
Kvn(π,πi) + θ(I- Y)E (Vn(S)-Vj(S))
j<i
ln k + θ XDQj - Qj,∏j -亓)S + θ2 X sup Qj (s,a)2
ln k + (1 J Y)2 ,
thus concluding the proof of eq. (IH.MD) and the mirror descent part of the inductive step.
The TD part of the inductive step is now direct: by eq. (IH.MD), then πi ∈ P, and thus eq. (IH.TD.1)
follows directly from Lemma 3.3, and eq. (IH.TD.2) follows via Markov’s inequality after discard-
ing another δ failure probability, bringing the total failure probability to 2iδ, and completing the
inductive step and overall proof.	□
28