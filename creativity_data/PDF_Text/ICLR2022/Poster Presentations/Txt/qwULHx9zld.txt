Published as a conference paper at ICLR 2022
Random matrices in service of ML footprint:
ternary random features with no perfor-
MANCE LOSS
Hafiz Tiomoko Ali	Zhenyu Liao
Huawei Noah’s Ark Lab (London)	Huazhong University of Science & Technology, China
hafiz.tiomoko.ali@huawei.com zhenyu_liao@hust.edu.cn
Romain Couillet
University Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, 38000 Grenoble, France
romain.couillet@univ-grenoble-alpes.fr
Ab stract
In this article, we investigate the spectral behavior of random features kernel
matrices of the type K = Ew[σ(wTxi)σ(wTxj)]in,j=1, with nonlinear function
σ(∙), data xι,..., Xn ∈ Rp, and random projection vector W ∈ Rp having i.i.d.
entries. In a high-dimensional setting where the number of data n and their
dimension p are both large and comparable, we show, under a Gaussian mixture
model for the data, that the eigenspectrum of K is independent of the distribution
of the i.i.d. (zero-mean and unit-variance) entries of w, and only depends on σ(∙)
via its (generalized) Gaussian moments Ez〜N(o,i)[σ0(z)] and Ez〜N(o,i)[σ00(z)].
As a result, for any kernel matrix K of the form above, we propose a novel random
features technique, called Ternary Random Feature (TRF), that (i) asymptotically
yields the same limiting kernel as the original K in a spectral sense and (ii) can be
computed and stored much more efficiently, by wisely tuning (in a data-dependent
manner) the function σ and the random vector w, both taking values in {-1, 0, 1}.
The computation of the proposed random features requires no multiplication, and a
factor of b times less bits for storage compared to classical random features such as
random Fourier features, with b the number of bits to store full precision values.
Besides, it appears in our experiments on real data that the substantial gains in
computation and storage are accompanied with somewhat improved performances
compared to state-of-the-art random features compression/quantization methods.
1 Introduction
Kernel methods are among the most powerful machine learning approaches with a wide range of
successful applications (Scholkopf & Smola, 2018) which, however, suffer from scalability issues
in large-scale problems, due to their high space and time complexities (with respect to the number
of data n). To address this key limitation, a myriad of random features based kernel approximation
techniques have been proposed (Rahimi & Recht, 2008; Liu et al., 2021a): random features methods
randomly project the data to obtain low-dimensional nonlinear representations that approximate the
original kernel features. This allows practitioners to apply them, with a large saving in both time and
space, to various kernel-based downstream tasks such as kernel spectral clustering (Von Luxburg,
2007), kernel principal component analysis (Scholkopf et al., 1997), kernel canonical correlation
analysis (Lai & Fyfe, 2000), kernel ridge regression (Vovk, 2013), to name a few. A wide variety of
these kernels can be written, for data points Xi , Xj ∈ Rp , in the form
κ(Xi, Xj) = Ew σ wTXi σ wTXj	(1)
with w ∈ Rp having i.i.d. entries, which can be “well approximated” by a sample mean
ml Pm=I σ (WTXi) σ (WTXj) over m independent random features for m sufficiently large. For
instance, taking σ(x) = [cos(x), sin(x)] and w with i.i.d. standard Gaussian entries, one ob-
tains the popular Random Fourier Features (RFFs) that approximate the Gaussian kernel (and the
1
Published as a conference paper at ICLR 2022
Laplacian kernel for Cauchy distributed w with the same choice of σ) (Rahimi & Recht, 2008);
for σ(x) = max(x, 0), one approximates the first order Arc-cosine kernel; and the zeroth order
Arc-cosine kernel (Cho, 2012) with σ(x) = (1 + sign(x))/2, etc.
As shall be seen subsequently, (random) neural networks are, to a large extent, connected to kernel
matrices of the form (1). More specifically, the classification or regression performance at the output
of random neural networks are functionals of random matrices that fall into the wide class of kernel
random matrices. Perhaps more surprisingly, this connection still exists for deep neural networks
which are (i) randomly initialized and (ii) trained with gradient descent, as testified by the recent
works on neural tangent kernels (Jacot et al., 2018), by considering the “infinitely many neurons”
limit, that is, the limit where the network widths of all layers go to infinity simultaneously. This close
connection between neural networks and kernels has triggered a renewed interest for the theoretical
investigation of deep neural networks from various perspectives, including optimization (Du et al.,
2019; Chizat et al., 2019), generalization (Allen-Zhu et al., 2018; Arora et al., 2019; Bietti & Mairal,
2019), and learning dynamics (Lee et al., 2019; Advani et al., 2020; Liao & Couillet, 2018a). These
works shed new light on the theoretical understanding of deep neural network models and specifically
demonstrate the significance of studying networks with random weights and their associated kernels
to assess the mechanisms underlying more elaborate deep networks.
In this article, we consider the random feature kernel of the type (1), which can also be seen as the
limiting kernel of a single-hidden-layer neural network with a random first layer. By assuming a
high-dimensional Gaussian Mixture Model (GMM) for the data {xi}in=1 with xi ∈ Rp, we show that
the centered kernel matrix1
K , P{κ(Xi, Xj)}nj=ιP, P , In - 11n1T,	⑵
is asymptotically (as n,p → ∞ with p/n → c ∈ (0, ∞)) equivalent, in a spectral sense, to another
random kernel matrix K which depends on the GMM data statistics and the generalized Gaussian
moments E[σ0(z)], E[σ00(z)] of the activation function σ(∙), but is independent of the specific law of
the i.i.d. entries of the random vector w, as long as they are normalized to have zero mean and unit
variance. As such, one can design novel random features schemes with limiting kernels asymptotically
equivalent to the original K. For instance, define
Kter(Xi, Xj) , Ewter [σter (XTWter) σter (XJWter)]	(3)
with wter ∈ Rp having i.i.d. entries taking value witer = 0 (with probability ) and value
Wter ∈ {-(I-E)-1, (I-E)-2 O	(4)
each with probability 1/2 - /2, where ∈ [0, 1) represents the level of sparsity of W, and
σter (t) = -1 ∙ δt<s- +1 ∙ δt>s+	(5)
for some thresholds s- < s+ chosen to match the generalized Gaussian moments E[σ0(z)], E[σ00(z)]
of any σ function (e.g., ReLU, cos, sin) widely used in random features or neural network contexts.
The proposed Ternary Random Features (TRFs, with limiting kernel matrices defined in (3) asymp-
totically “matching” any random features kernel matrices in a spectral sense) have the computational
advantage of being sparse and not requiring multiplications but only additions, as well as the storage
advantage of being composed of only a finite set of words, e.g., {-1, 0, 1} for E = 0.
Given the urgent need for environmentally-friendly, but still efficient, neural networks such as binary
neural networks (Hubara et al., 2016; Lin et al., 2015; Zhu et al., 2016; Qin et al., 2020; Hubara
et al., 2016), pruned neural networks (Liu et al., 2015; Han et al., 2015a;b), weights-quantized neural
networks (Gupta et al., 2015; Gong et al., 2014), we hope that our analysis will open a new door to a
random matrix-improved framework of computationally efficient methods for machine learning and
neural network models more generally.
1Left- and right-multiplying the kernel matrices by P is equivalent to centering the data in the kernel feature
space, which is a common practice in kernel learning and plays a crucial role in multidimensional scaling (Joseph
& Myron, 1978) and kernel PCA (Schlkopf et al., 1998). In the remainder of this paper, whenever we use kernel
matrices, they are considered to have been centered in this way.
2
Published as a conference paper at ICLR 2022
Figure 1: Test accuracy of logistic regression on quantized random features for different number of
features m ∈ {102, 103,5.103, 104, 5.104}, with LP-RFF (8-bit and 1-bit, in black) (Zhang et al.,
2019), NystrOm approximation (32 bits in red, 16 bits in green) (Williams & Seeger, 2001), versus
the proposed TRF approach (in blue), on the two-class Cov-Type dataset from UCI ML repo, with
n = 418 000 training samples, ntest = 116 000 test samples, and data dimension p = 54.
1.1	Contributions
Our main results are summarized as follows.
1.	By considering a high-dimensional Gaussian mixture model for the data, we show (Theo-
rem 1) that for K defined in (2), kK - Kk → 0 as n, p → ∞, where K is a random matrix
independent of the law of w, and depends on the nonlinear σ(∙) only via its generalized
Gaussian moments E[σ0(z)] and E[σ00(z)] for Z 〜 N (0, 1).
2.	We exploit this result to propose a computationally efficient random features approach,
called Ternary Random Features (TRFs), with asymptotically the same limiting kernel as
any random features-type kernel matrices K of the form (2), while inducing no multiplication
and b times less memory storage for its computation, with b the number of bits to store full
precision values, e.g., b = 32 in the case ofa single-precision floating-point format.
3.	We provide empirical evidence on various random-features based algorithms, showing
the computational and storage advantages of the proposed TRF method, while achieving
competitive performances compared to state-of-the-art random features techniques. As a
first telling example, in Figure 1 on the Cov-Type dataset from the UCI ML repository,TRFs
achieve similar logistic regression performance as the LP-RFF method proposed by Zhang
et al. (2019), with 8 times less memory, and 32 or 16 times less memory than the NystrOm
approximation of the Gaussian kernel (using full precision of 32 or 16 bits) (Williams &
Seeger, 2001); see the shift in the x-axis (memory) of Figure 1.
1.2	Related work
Random features kernel and random neural networks. Random features methods were first
proposed to relieve the computational and storage burden of kernel methods in large-scale problems
when the number of training samples n is large (SchOlkopf & Smola, 2018; Rahimi & Recht, 2008;
Liu et al., 2021a). For instance, Random Fourier Features can be used to approximate the popular
Gaussian kernel, when the number of random features m is sufficiently large (Rahimi & Recht,
2008). Since (deep) modern neural networks are routinely trained with random initialization, random
features method is also considered a stylish model to analyze neural networks (Neal, 1996; Williams,
1997; Novak et al., 2018; Matthews et al., 2018; Lee et al., 2017; Louart et al., 2018). In particular,
by focusing on the large n, m regime, the analysis of such models led to the so-called double descent
theory (Advani et al., 2020; Mei & Montanari, 2019; Liao et al., 2020) for neural nets.
Computationally efficient random features methods. In an effort to further reduce the computa-
tion and storage costs of random features models, various quantization and binarization methods were
proposed (Goemans & Williamson, 1995; Charikar, 2002; Li & Slawski, 2017; Li & Li, 2019; 2021;
3
Published as a conference paper at ICLR 2022
Agrawal et al., 2019; Zhang et al., 2019; Liao et al., 2021; Couillet et al., 2021). More precisely,
Agrawal et al. (2019) combined RFFs with a data-dependent feature selection approach to reduce the
computational cost, while preserving the statistical guarantees of (using the original set of) RFFs.
Zhang et al. (2019) proposed a low-precision approximation of RFFs to significantly reduce the
storage while generalizing as well as full-precision RFFs. Li & Li (2021) designed quantization
schemes of RFFs for arbitrary choice of the Gaussian kernel parameter. Our work improves these
previous efforts by (i) considering a broader family of random features kernels beyond RFFs and by
(ii) proposing the TRF approach that is both sparse and quantized, while asymptotically yielding the
same limiting kernel spectral structure, and thus algorithmic performances (Cortes et al.).
Random matrix theory and neural networks. Random matrix theory (RMT), as a powerful and
flexible tool to investigate the (asymptotic) behavior of large-scale systems, is recently gaining
popularity in the analysis of (deep) neural networks (Martin & Mahoney, 2018; 2019). In this respect,
Pennington & Worah (2017) derived the eigenvalue distribution of the Conjugate Kernel (CK) in a
single-hidden-layer random neural network model. This result was then generalized to a broader
class of data distributions (Louart et al., 2018) and to a multi-layer scenario (Benigni & P6ch6, 2019;
Pastur, 2020). Fan & Wang (2020) went beyond the general i.i.d. assumption (on the entries of
data vectors) and studied the spectral properties of the CK and neural tangent kernel for data that
are approximately “pairwise orthogonal.” Our work improves (Fan & Wang, 2020) by studying the
random features kernel for more structured GMM data, and is thus more adapted to machine learning
applications such as classification. As far as the study of random features kernels under GMM data is
concerned, the closest work to ours is (Liao & Couillet, 2018b) where the kernel matrix K defined in
(2) is studied for GMM data, but only for a few specific activation functions and Gaussian w, see
Footnote 5 below for a detailed account of the technical differences between this work and (Liao
& Couillet, 2018b). Here, we provide a universal result with respect to the (much broader class of)
activation functions and random w, and propose a computation and storage efficient random features
technique well tuned to match the performances of any commonly used random features kernels.
1.3	Notations and Organization of the article
In this article, we denote scalars by lowercase letters, vectors by bold lowercase, and matrices by
bold uppercase. We denote the transpose operator by (∙)T, We use ∣∣ ∙ ∣∣ to denote the Euclidean
norm for vectors and spectral/operator norm for matrices. For a random variable z, E[z] denotes
the expectation of z. The notation δx∈A is the Kronecker delta taking value 1 When x ∈ A and 0
otherWise. Finally, 1p and Ip are respectively the vector of all one’s of dimension p and the identity
matrix of dimension p × p.
The remainder of the article is structured as folloWs. In Section 2, We describe the random features
model under study along With our Working assumptions. We then present our main technical results
in Section 3 on the spectral characterization of random features kernel matrices K and its practical
consequences, in particular the design of cost-efficient ternary random features (TRFs) leading
asymptotically to the same kernel as any generic random features. We provide empirical evidence
in Section 4 shoWing the computational and storage advantage along With competitive performance
of TRFs compared to state-of-the-art random features approaches. Conclusion and perspective are
placed in Section 5.
2 System settings
Let W ∈ Rm×p be a random matrix having i.i.d. entries With zero mean and unit variance. The
random features matrix Σ ∈ Rm×n of some data X ∈ Rp×n is defined as Σ , σ(WX) for some
nonlinear activation function σ : R → R applied entry-Wise on WX. We denote G the associated
random features Gram matrix
1	1	1m
G，—ΣTΣ = —σ(WX)Tσ(WX) = — Vσ (XTWt) σ (WTX)∈ Rn×n (6)
mm	m	t
t=1
which is a sample mean of the expected kernel defined in (1). With P，In — 11n1T, we consider
the expected and centered random features kernel matrix K defined in (2), Which plays a fundamental
4
Published as a conference paper at ICLR 2022
role in various random features kernel-based learning methods such as kernel ridge regression, logistic
regression, support vector machines, principal component analysis, or spectral clustering.
Let xι ,…，Xn ∈ Rp be n independent data vectors belonging to one of K distributional classes
Ci, ∙∙∙ , CK, with class Ca having cardinality n°. We assume that Xi follows a GaUSSian Mixture
Model (GMM), that is, for Xi ∈ Ca,
Xi = μa∕√p + Zi	(7)
with Zi 〜N(0p, Ca/p) for some mean μ° ∈ Rp and covariance Ca ∈ Rp×p associated to class C0.
In high dimensions, under “non triviality assumptions”,2 the data vectors drawn from (7) can be
shown to be neither very “close” nor very “far” from each other, irrespective of the class they belong
to, see (Couillet et al., 2016). We place ourselves under such non-trivial conditions, by imposing, as
in (Couillet et al., 2016; 2018), the following growth rate conditions.
Assumption 1 (High-dimensional asymptotics) As n → ∞, we have (i) p/n → c ∈ (0, ∞) and
na/n →	Ca ∈	(0,1);	(ii) kμak = O(1);(m)for Co =	PK= naCa and	Ca = Ca - Co,
IlCaIl =	O(1),	tr(Ca)	= O(√ρ) and tr(CaCb) = O(P)	for a, b ∈ {1,…，K}. Wedenote
τ , tr(Co)/p that is assumed to converge in (0, ∞).
Remark 1 (Beyond Gaussian mixture data) While the theoretical results in this paper are derived
for Gaussian mixture data in (7), under the non-trivial setting of Assumption 1, we conjecture that
they can be extended to a much broader family of data distributions beyond the Gaussian setting, e.g.,
to the so-called concentrated random vector family (Seddik et al., 2020; Louart & Couillet, 2018),
under similar non triviality assumptions. See Section A.1 in the appendix for more discussions.
To cover a large family of random features, we assume that the random projector matrix W has i.i.d.
entries of zero mean, unit variance and bounded fourth-order moment, with no restriction on their
particular distribution.
Assumption 2 (On random projection matrix) The random matrix W has i.i.d. entries such that
E[Wij] = 0, E[Wi2j] = 1 and E[Wi4j] < λW for some constant λW < ∞.
We consider the family of activation functions σ(∙) satisfying the following assumption.
Assumption 3 (On activation function) The function σ is at least twice differentiable (in the sense
of distributions when applied to a random variable having a non-degenerate distribution function, see
Remark 3 in Section A.1 of the appendix), with max{E∣σ(z)∣, E∣σ2(z)∣, E∣σ0(z)∣, E∣σ00(z)∣} < λσ
for some constant λσ < ∞ and Z 〜N(0,1).
3	Main res ult
Our objective is to characterize the high-dimensional spectral behavior of the centered and expected
random features kernel matrix K defined in (2). It turns out, somewhat surprisingly, that under the
non-trivial setting of Assumption 1, one may mentally picture the high-dimensional data vectors as
(i) being asymptotically pairwise orthogonal (i.e., XiTXj → 0 for i 6= j as p → ∞) and (ii) having
asymptotically equal Euclidean norms (i.e., IXiI2 → τ), independently of the underlying class they
belong to. As we shall see, this high-dimensional “concentration” of XTXj → T ∙ δi= plays a crucial
role in “decoupling” the two dependent but asymptotically Gaussian random variables σ(wTXi)
and σ(wTXj) in the definition (1). This, up to some careful control on the higher-order (but well
“concentrated”) terms, leads to the following result on the asymptotic behavior of K, the proof of
which is given in Section A.3 of the appendix.
Theorem 1 (Asymptotic equivalent of K) Under Assumption 1-3, for K defined in (2), as n → ∞,
~ ..
kK - Kk → 0,
2That is, when classification is neither too hard nor too easy.
5
Published as a conference paper at ICLR 2022
almost surely with P = In - 1n1Tn/n,
K = P (dι∙(z + M√Tp)	(Z	+ M√P) +	d2	∙ VAVt	+	do	∙ In)	P,	(8)
and
V = [J∕√p, φ] ∈ Rn×(K+1),	A = ttTtT 2T 1 ∈ R(K+I)X(K+1),
Where,for Z 〜N (0,1),
do = E[σ2(√τz)] - E[σ(√τz)]2 - TE[σ0(√τz)]2,	di = E[σ0(√τz)]2, d? = 4E[σ00(√τz)]2
and “first-order” random matrix Z = [zi, •…,Zn] ∈ Rp×n as defined in (7), “second-order^ random
(fluctuation) vector φ = kzik2 - E[kzik2] in=1 ∈ Rn, and GMM data statistics
M = [μι,…，μκ] ∈ Rp×K, t = [ tr(Ca) ]	∈ RK, T = [ trCaCb ]	∈ Rk×k (9)
I √p Ja=1	Ip	a α,b=1
as well as the class label vectors J = [ji, •…，jκ]∈ Rn×K with [ja]i = δxi∈Ca.
First note that the “asymptotic equivalence” established in Theorem 1 holds for the expected kernel
K, not on the empirical random feature kernel G defined in (6), and is thus independent of the
number of random features m. On closer inspection of Theorem 1, we see that the data statistics
for classification, i.e., the means (M) and covariances (ttT, T) are respectively weighted by the
generalized Gaussian moments of first (d1) and second order (d2), while the coefficient do merely
acts as a regularization term to shift all the eigenvalues,3 and has thus asymptotically no impact on the
performance of, e.g., kernel spectral clustering for which only eigenvector structures are exploited.4
Theorem 1 unveils a surprising universal behavior of the expected kernel K in the large n, p regime.
Specifically, the expression of K (and thus the spectral behavior of K, see our discussion in the
paragraph that follows) is universal with respect to the distribution of W and the activation function
σ, when they are “normalized” to satisfy Assumptions 2 and 3. This technically improves previous
efforts such as (Liao & Couillet, 2018b),5 and allows us to design computationally more efficient
random features approach by wisely choosing W and σ.
As a direct consequence of Theorem 1, one has, by Weyl's inequality and Davis-Kahan theorem
that (i) the difference between each corresponding pair of eigenvalues and (ii) the distance between
the “isolated” eigenvectors or the “angle” between the “isolated subspaces” of K and K vanish
asymptotically as n, p → ∞. This is numerically confirmed in Figure 2, where one observes a
close match between the spectra (eigenvalue “bulk” and isolated eigenvalue-eigenvector pairs) of
K and those of its asymptotic equivalent K given in Theorem 1, already for n, p only in hundreds.
In particular, we compare, in Figure 2, the eigenspectra of K and K for Gaussian versus Student-t
distributed W, on GMM data. A close match of the spectra and the isolated eigen-pairs is observed,
irrespective of the distribution of W, as predicted by our theory.
In the following corollary, we exploit the universal result in Theorem 1 to design computationally
efficient Ternary Random Features (TRFs) by specifying the distribution of (the entries of) W to
symmetric Bernoulli and σ to a ternary function, so as to obtain a limiting kernel Kter that is
asymptotically equivalent to any random features kernel matrix K of the form (2), for the high-
dimensional GMM data under study. This is proved in Section A.4 of the appendix.
3This can be seen as another manifestation of the implicit regularization in high-dimensional kernel and
random features (Jacot et al., 2020; Derezinski et al., 2020; Liu et al., 2021b).
4We provide in Table 1 (Section A.5 of the appendix) the corresponding Gaussian moments d0, d1, d2 for
various commonly used activation functions in random features and neural network contexts.
5 From a technical perspective, Theorem 1 improves (Liao & Couillet, 2018b, Theorem 1) in the fact that
the latter relies on the explicit forms of the expectation K in (2), and is thus limited to (i) a few nonlinear σ for
which K can be computed explicitly, see (Liao & Couillet, 2018b, Table 1); and (ii) Gaussian distributed W in
which case the p-dimensional integral can be easily reduced to a two-dimensional one. Here, Theorem 1 holds
for a much broader family of random W and nonlinear σ as long as Assumptions 2 and 3 hold.
6
Published as a conference paper at ICLR 2022
Eigenv Eigenvalues of K	Eigenvalues of K
Figure 2: Eigenvalue distribution (TOP) and eigenvector associated to the largest eigenvalue
(BOTTOM) of the expected and centered kernel matrix K (blue) versus its asymptotic equiva-
lent K (red) in Theorem 1, with σ(t) = max(t, 0). (LEFT) W having Gaussian entries and
(RIGHT) W having Student-t entries with 7 degrees of freedom, for two-class GMM data with
μa = [0a-i; 4; 0p-a], Ca = (1 + 4(a - 1)∕√P)Ip, P = 512 and n = 2048.
Corollary 1 (Ternary Random Features) For a given random features kernel matrix K of the form
(2) with W and nonlinear σ satisfying Assumptions 1-3, with associated generalized Gaussian
moments do, di, d2 defined in Theorem 1, let σter be defined in (5) with S- = S-, s+ = S+, and s-,
^+ satisfying thefollowing equations
di = ∏2 9-s+/T + e-S-/TJ, d2 = ■ (^+e-s+∕τ + S-e-s—4)1	(10)
Define the Ternary Random Features matrix Σter = σ ter (Wter X) with Wter defined in (4) having
sparsity level e,the associated Gram matrix Gter = ^^ (Σter )T∑ter as in (6), and the limiting kernel
Kter , P{κter(xi, xj)}in,j=iP
(11)
for κter defined in (3). Then, there exists λ ∈ R such that6 kK - Kter - λPk→ 0 almost surely as
n → ∞.
Note that the system of equations in (10) defining S- and ^+ is a function of the key parameter
T = trC°∕p defined in Assumption 1, which can be consistently estimated from the data; see
Algorithm 1 below and a proof in Lemma 1 of the appendix (the intuition of which follows from the
fact that kxik2 = kμak2∕p + 2μTZi/√p + ∣∣Zik2 = E[tr(zizτ)] + O(p-i/2) according to ⑺ and
Assumption 1). This makes the proposed TRFs and its limiting kernel Kter data-statistics-dependent.
Yet, the system of equations (10) does not have a closed-form solution and might have multiple
solutions on the real line; we practically solve it using a numerical least squares method, by gradually
enlarging the search range (from say [-1, 1]) until a solution is found (the time complexity of which
is independent of the dimension n, p and it is observed in the experiments in Section 4 to converge in
a few iterations). The details of the proposed TRF approach are described in Algorithm 1.
Algorithm 1 Ternary Random Features
Input: Data X and level of sparsity ∈ [0, 1).
Output: Ternary Random Features Σter and Gram matrix Gter .
Estimate T as T = 1 Pn=IIlXi∣∣2.
Solve for thresholds S-, ^+ using (10), which defines σter via (5).
Construct a random matrix Wter ∈ Rm×p having i.i.d. entries distributed according to (4).
Compute Σter = σter(WterX) and then TRFs Gram matrix Gter = ^^(Σter )T∑ter as in (6).
6The parameter λ characterizes the possibly different d0 between K and Kter, see details in Appendix A.4.
7
Published as a conference paper at ICLR 2022
Computational and storage complexity For W ∈ Rm×p a random matrix with i.i.d. N (0, 1)
entries and Wter ∈ Rm×p with i.i.d. entries satisfying (4) with sparsity level ∈ [0, 1), let G =
mlσ(WX)Tσ(WX) for some given smooth function σ (e.g., sine and cosine in the case of random
Fourier features) and Gter = mmσter (WterX)Tσter (WterX) with data matrix X ∈ Rp×n. Itisthen
beneficial to use Gter instead of G as the computation of σ(WX) requires O(mnp) multiplications
and O(mnp) additions, while the computation of σter(WterX) requires no multiplication and only
O((1 - )mnp) additions. In terms of storage, it requires a factor of b = 32 times more bits to store
σ(WX) when computing G compared to storing σter (WterX) when computing Gter (assuming
full precision numbers are stored using b = 32 bits).
The computationally and storage efficient TRFs σter (WterX) can then be used instead of the
“expensive” random features σ(WX) and lead (asymptotically) to the same performance as the latter
on downstream tasks, at least for GMM data, according to Theorem 1. In the following section,
we provide empirical results showing that (i) this “performance match” between TRFs and random
features of the form (1) is not limited to Gaussian data and empirically holds when applied on popular
real-world datasets such as MNIST (LeCun et al., 1998), some UCI ML datasets, as well as DNN-
features of CIFAR10 data in Section A.6 of the appendix; and (ii) due to the competitive performance
of TRFs with respect to standard random features approaches, when compared to state-of-the-art
random feature compression/quantization techniques (for which a “performance-complexity tradeoff”
generally arises, that is, as one compresses more the original random features, the performance
decays), TRFs yield significantly better performances for a given storage/computational budget.
4	Experiments
The experiments in this section and Section A.6 of the appendix are performed on a Ubuntu 18.04
machine with Intel(R) Core(TM) i9-9900X CPU @ 3.50GHz and 64 GB of RAM.
4.1	TRFs match the performance of popular kernels with less budget
We first consider ridge regression with random features Gram matrix G on MNIST data in Figure 3.
We compare RFFs with σ(t) = [cos(t), sin(t)] and Gaussian Wij 〜N(0,1) to the proposed TRF
method with σter (t) in (5) and ternary random projection matrix Wter defined in (4), with different
sparsity levels . The thresholds s- , s+ of σter are tuned in such away that the generalized Gaussian
moments d1 and d2 match with those of RFFs,7 as described in Corollary 1 and Algorithm 1. Figure 3
displays the test mean squared errors as a function of the regularization parameter γ for our TRF
method with different sparsity levels compared to the RFF method, as well as to the baseline
of Kernel Ridge Regression (KRR) using Gaussian kernel. Note that despite 90% sparsity in the
projection matrix W, virtually no performance loss is incurred compared with RFFs. This is further
confirmed in the right hand side of Figure 3 which shows the gains in running time8 when using TRFs.
These experiments show that the proposed TRF approach yields similar performance as popular
random features such as RFFs, with a significant reduction in terms of computation and storage.
4.2	Computational and storage gains — Comparisons to state-of-the-art
In this section, we compare TRFs with state-of-the-art quantized or non-quantized random features
methods, for both random features-based logistic and ridge regressions. specifically, in Figure 1, we
compare logistic regression performance of TRFs versus the Low precision Random Fourier Features
(Lp-RFFs) proposed by Zhang et al. (2019), on the Cov-Type dataset from UCi ML repo. As in
section 4.1, the TRFs are tuned to match the limiting Gaussian kernel of RFFs. For a single datum
x ∈ Rp, the associated TRFs σter(Wterx) ∈ Rm use m bits for storage while Lp-RFFs use 8m bits.
We follow the same protocol as in (Zhang et al., 2019) and use sGD with mini-batch size 250 in
training logistic regressor and {1, 8} bits precision for the Lp-RFF approach. Figure 1 compares the
7For given xi, xj, we use [cos(Wxi), sin(Wxj)] as the random Fourier features so that the (i, j) entry of
the corresponding random features Gram matrix is cos(Wxi)T cos(Wxj) +sin(Wxi)T sin(Wxj) (Rahimi &
Recht, 2008). The generalized Gaussian moments of the RFFs are thus the sum of the d1’s and d2’s corresponding
to sin and cos functions. Note that the d0 ’s of RFFs and TRFs may be different.
8The running time is taken as the total clock-time of the whole ridge regression solver including the random
features calculation.
8
Published as a conference paper at ICLR 2022
g
——Baseline (KRR)
2,000  _____________ RFF
-EF TRF
1,00乜
0.1	0.3	0.5	0.7	0.9
γ
Figure 3: Testing mean squared errors (MSEs with ±1 std, LEFT) and running time (RIGHT)
of random features ridge regression as a function of regularization parameter γ, p = 512, n =
1024, ntest = 512, m = 5.104. With W distributed according to (4) and = [0.1, 0.5, 0.9], versus
RFFs and KRR on a 2-class MNIST dataset - digits (7, 9). We find that do = 0.44 for TRFs and
d0 = 0.39 for RFFs, making λ in Corollary 1 small. Results averaged over 5 independent runs.
logistic regression test accuracy as a function of the total memory budget for LP-RFF (1 bit and 8
bits) and the Nystrom approximation of Gaussian kernel matrices (using 32 and 16 bits) (Williams
& Seeger, 2001) (see also Table 1 in (Zhang et al., 2019)), versus the proposed TRFs approach. As
seen from the shift in the x-axis (memory), by using 8× less memory than LP-RFFs and 32× or 16×
less memory than the Nystrom method, TRFs achieve a superior generalization performance on the
logistic regression task. Similar comparisons are performed in Figure 4 on a random features ridge
regression task for the Census dataset (Rahimi & Recht, 2008), confirming that TRFs outperform
alternative approaches in terms of test mean square errors, with a significant save in memory.
Figure 4: Test mean square errors of ridge regression on quantized random features for different
number of features m ∈ {5.102, 103, 5.103, 104, 5.104}, using LP-RFF (Zhang et al., 2019), Nystrom
approximation (Williams & Seeger, 2001), versus the proposed TRF approach, on the Census dataset,
with n = 16 000 training samples, ntest = 2 000 test samples, and data dimension p = 119.
5	Conclusion
Our large dimensional spectral analysis of the random features kernel matrix K reveals that its
spectral properties only depend on the nonlinear activation through the corresponding generalized
Gaussian moments and are universal with respect to zero-mean and unit-variance random projection
vectors. This allows us to design the new TRF approach which turns both the random weights W and
the activations σ(WX) into ternary integers, thereby allowing to only perform addition operations
and to only store 1 bit for the activations. This drastically saves the storage and computation
of random features while preserving the performances on downstream tasks with respect to their
counterpart expensive kernels. Our article comes along with (Couillet et al., 2021) as first steps in
re-designing machine learning algorithms using Random Matrix Theory, in order to be able to perform
computations on massive data using desktop computers instead of relying on energy consuming giant
servers.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Z L would like to acknowledge the National Natural Science Foundation of China (NSFC-12141107),
the Fundamental Research Funds for the Central Universities of China (2021XXJS110), and CCF-
Hikvision Open Fund (20210008) for providing partial support. R C would like to acknowledge the
MIAI LargeDATA chair (ANR-19-P3IA-0003) at University Grenobles-Alpes, the HUAWEI LarDist
project, as well as the ANR DARLING project for providing partial support of this work.
References
Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional dynamics of
generalization error in neural networks. Neural Networks,132:428-446, 2020. ISSN 0893-6080.
doi: 10.1016/j.neunet.2020.08.022.
Raj Agrawal, Trevor Campbell, Jonathan Huggins, and Tamara Broderick. Data-dependent com-
pression of random features for large-scale kernel approximation. In The 22nd International
Conference on Artificial Intelligence and Statistics, pp. 1822-1831. PMLR, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Zhidong Bai and Jack W. Silverstein. Spectral Analysis of Large Dimensional Random Matrices,
volume 20 of Springer Series in Statistics. Springer-Verlag New York, 2 edition, 2010. ISBN
9781441906601. doi: 10.1007/978-1-4419-0661-8.
Lucas Benigni and Sandrine P6ch6. Eigenvalue distribution of nonlinear models of random matrices.
arXiv preprint arXiv:1904.03090, 2019.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173, 2019.
Patrick Billingsley. Probability and Measure. Wiley Series in Probability and Statistics. John Wiley &
Sons, Ltd, 3 edition, 2012. ISBN 9781118122372. URLhttps://www.wiley.com/en-us/
Probability+and+Measure%2C+Anniversary+Edition-p-9781118122372.
Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the
thiry-fourth annual ACM symposium on Theory of computing, pp. 380-388, 2002.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Programming.
In Advances in Neural Information Processing Systems, volume 32 of NIPS’19, pp. 2937-2947. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/ae614c557843b1df326cb29c57225459-Paper.pdf.
Youngmin Cho. Kernel methods for deep learning. University of California, San Diego, 2012.
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the Impact of Kernel Approximation
on Learning Accuracy. In Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 113-120,
Chia Laguna Resort, Sardinia, Italy. PMLR. URL https://proceedings.mlr.press/
v9/cortes10a.html.
Couillet, Cinar Romain, Gaussier Y., and Imran M. E. Word representations concentrate and this is
good news! In Proceedings of the 24th Conference on Computational Natural Language Learning,
pp. 325-334, 2020, November.
Romain Couillet, Florent Benaych-Georges, et al. Kernel spectral clustering of large dimensional
data. Electronic Journal of Statistics, 10(1):1393-1454, 2016.
10
Published as a conference paper at ICLR 2022
Romain Couillet, Zhenyu Liao, and Xiaoyi Mai. Classification asymptotics in the random matrix
regime. In 2018 26th European Signal Processing Conference (EUSIPCO),pp. 1875-1879. IEEE,
2018.
Romain Couillet, Florent Chatelain, and Nicolas Le Bihan. Two-way kernel matrix puncturing:
towards resource-efficient pca and spectral clustering. arXiv preprint arXiv:2102.12293, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Michal Derezinski, Feynman T Liang, and Michael W Mahoney. Exact expressions for
double descent and implicit regularization via surrogate random design. In Advances
in Neural Information Processing Systems, volume 33, pp. 5152-5164. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685.
PMLR, 2019.
Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-
width neural networks. arXiv preprint arXiv:2005.11879, 2020.
Friedrich Gerard Friedlander, G Friedlander, Mark Suresh Joshi, M Joshi, and Mohan C Joshi.
Introduction to the Theory of Distributions. Cambridge University Press, 1998.
Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):
1115-1145, 1995.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional
networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International conference on machine learning, pp. 1737-1746.
PMLR, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both Weights and Connections for
Efficient Neural Network. In Advances in Neural Information Processing Systems, volume 28
of NIPS’15. Curran Associates, Inc., 2015b. URL https://proceedings.neurips.cc/
paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
Neural Networks. In Advances in Neural Information Processing Systems, volume 29 of NIPS‘16,
pp. 4107-4115. Curran Associates, Inc., 2016. URL https://proceedings.neurips.
cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4631-4640.
PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/jacot20a.
html.
11
Published as a conference paper at ICLR 2022
Kruskal Joseph and Wish Myron. Multidimensional Scaling. 1978. ISBN 9780803909403.
doi:	10.4135/9781412985130. URL https://methods.sagepub.com/book/
multidimensional-scaling.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Pei Ling Lai and Colin Fyfe. Kernel and nonlinear canonical correlation analysis. International
Journal of Neural Systems ,10(05):365-377, 2000.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. ISSN 0018-9219. doi:
10.1109/5.726791.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. arXiv preprint arXiv:1902.06720, 2019.
Ping Li and Martin Slawski. Simple strategies for recovering inner products from coarsely quantized
random projections. Advances in Neural Information Processing Systems, 30:4567-4576, 2017.
Xiaoyun Li and Ping Li. Random projections with asymmetric quantization. Advances in Neural
Information Processing Systems, 32:10858-10867, 2019.
Xiaoyun Li and Ping Li. Quantization algorithms for random fourier features. arXiv preprint
arXiv:2102.13079, 2021.
Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random
fourier features. Journal of Machine Learning Research, 22(108):1-51, 2021. URL http:
//jmlr.org/papers/v22/20-1369.html.
Zhenyu Liao and Romain Couillet. The dynamics of learning: A random matrix approach. In
International Conference on Machine Learning, pp. 3072-3081. PMLR, 2018a.
Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional
data. In International Conference on Machine Learning, pp. 3063-3071. PMLR, 2018b.
Zhenyu Liao and Romain Couillet. Inner-product Kernels are Asymptotically Equivalent to Binary
Discrete Kernels. arXiv, 2019. URL https://arxiv.org/abs/1909.06788.
Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random fourier
features: beyond the gaussian kernel, a precise phase transition, and the corresponding double
descent. 33:13939-13950, 2020. URL https://proceedings.neurips.cc/paper/
2020/file/a03fa30821986dff10fc66647c84c9c3-Paper.pdf.
Zhenyu Liao, Romain Couillet, and Michael W. Mahoney. Sparse quantized spectral clustering. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=pBqLS-7KYAF.
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with
few multiplications. arXiv preprint arXiv:1510.03009, 2015.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolu-
tional neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 806-814, 2015.
Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan A. K. Suykens. Random Features for Kernel
Approximation: A Survey on Algorithms, Theory, and Beyond. IEEE Transactions on Pattern
Analysis and Machine Intelligence, PP(99):1-1, 2021a. ISSN 0162-8828. doi: 10.1109/tpami.
2021.3097011.
12
Published as a conference paper at ICLR 2022
Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Refined
analysis beyond double descent. In Proceedings of The 24th International Conference on Artificial
Intelligence and Statistics, volume 130 of Proceedings ofMachine Learning Research, pp. 649-657.
PMLR, 13-15 Apr 2021b. URL https://Proceedings .mlr.press∕v130∕liu21b.
html.
Cosme Louart and Romain Couillet. Concentration of measure and large random matrices with an
application to sample covariance matrices. arXiv preprint arXiv:1805.08295, 2018.
Cosme Louart, Zhenyu Liao, Romain Couillet, et al. A random matrix approach to neural networks.
The Annals of Applied Probability, 28(2):1190-1248, 2018.
Yueming Lyu. Spherical structured feature maps for kernel approximation. In International Confer-
ence on Machine Learning, pp. 2256-2264. PMLR, 2017.
Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Ev-
idence from random matrix theory and implications for learning. arXiv preprint arXiv:1810.01075,
2018.
Charles H Martin and Michael W Mahoney. Traditional and heavy-tailed self regularization in neural
network models. arXiv preprint arXiv:1901.08276, 2019.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Leonid Pastur. On random matrices arising in deep neural networks. gaussian case. arXiv preprint
arXiv:2001.06188, 2020.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. 2017.
Danil Prokhorov. Ijcnn 2001 neural network competition. 2001.
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural
networks: A survey. Pattern Recognition, 105:107281, 2020.
Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In Advances
in Neural Information Processing Systems, volume 20 of NIPS‘08, pp. 1177-1184. Curran As-
sociates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2007/file/
013a006f03dbc5392effeb8f18fda755- Paper.pdf.
Bernhard Schlkopf, Alexander Smola, and Klaus-Robert Mller. Nonlinear Component Analysis as a
Kernel Eigenvalue Problem. Neural Computation, 10(5):1299-1319, 1998. ISSN 0899-7667. doi:
10.1162/089976698300017467.
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Kernel principal component
analysis. In International conference on artificial neural networks, pp. 583-588. Springer, 1997.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. The MIT Press, 2018. ISBN 9780262256933. doi:
10.7551/mitpress/4175.001.0001.
Mohamed El Amine Seddik, Mohamed Tamaazousti, and Romain Couillet. A kernel random matrix-
based approach for sparse pca. In International Conference on Learning Representations (ICLR),
2019.
13
Published as a conference paper at ICLR 2022
Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain Couillet. Random
matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures.
In International Conference on Machine Learning, pp. 8573-8582. PMLR, 2020.
Elias M Stein and Rami Shakarchi. Functional Analysis, Introduction to Further Topics in Analysis.
2012. doi: 10.1515/9781400840557-005.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Vladimir Vovk. Kernel ridge regression. In Empirical inference, pp. 105-116. Springer, 2013.
Christopher Williams and Matthias Seeger. Using the nystrom method to speed UP kernel machines.
In Proceedings of the 14th annual conference on neural information processing systems, number
CONF, pp. 682-688, 2001.
Christopher KI Williams. CompUting with infinite networks. Advances in neural information
processing systems, pp. 295-301, 1997.
Felix Xinnan X YU, Ananda Theertha SUresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice,
and Sanjiv KUmar. Orthogonal random featUres. Advances in neural information processing
systems, 29:1975-1983, 2016.
Jian Zhang, Avner May, Tri Dao, and Christopher R6. Low-precision random fourier features for
memory-constrained kernel approximation. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1264-1274. PMLR, 2019.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
A	Appendix
We provide detailed discussions on our working assumptions in Section A.1, a consistent estimator of
the key parameter τ in Section A.2, the proof of Theorem 1 in Section A.3 and that of Corollary 1 in
Section A.4, as well as the generalized Gaussian moments for standard kernels arising from random
features and neural net contexts in Section A.5. Additional numerical experiments are placed in
Section A.6.
A.1 Notes on the working Assumptions
For completeness, let us restate the working settings of the paper.
Let xι,…,Xn ∈ Rp be n independent vectors belonging to one of K distributional classes
Ci, ∙∙∙ , CK. Class a has cardinality n&, and We assume that Xi follows a Gaussian Mixture Model
(GMM), i.e., for Xi ∈ Ca,
Xi = μa∕√P + Zi	(12)
with Zi 〜N(0p, Ca/p) for some mean μ° ∈ Rp and covariance Ca ∈ Rp×p associated to class Ca.
We position ourselves in the non-trivial regime of high-dimensional classification as described by the
following growth rate conditions.
Assumption 4 (High-dimensional asymptotics) As n → ∞, we have (i) p/n → c ∈ (0, ∞) and
na/n → Ca ∈ (0,1)； (ii) kμak =。⑴;(iii)for C° = Pa=I naCa and Ca = Ca - C°, then
IlCaIl = O(1), tr(Ca) = O(√ρ) and tr(CaCb) = O(P) for a, b ∈ {1,…，K}. WedenOte
T，tr(C°)∕ρ that is assumed to converge in (0, ∞).
14
Published as a conference paper at ICLR 2022
Beyond Gaussian mixtures While the theoretical results in this paper are derived for Gaussian
mixture data under the non-trivial setting, we conjecture that they can be extended to a much broader
family of data distributions beyond the Gaussian setting, e.g., to the so-called concentrated random
vector family (Seddik et al., 2020), under similar non triviality assumptions.
Definition 1 (Concentrated vector) Given a normed vector space (X, ∣∙∣), and q > 0, a random
vector x ∈ X is said to be q-exponentially concentrated if for any 1-Lipschitz function φ : X → R,
there exists C > 0 independent of dim(X) and σ > 0 such that for all t ≥ 0,
P (∣φ(x)- E[φ(x)])l >t) ≤ Ce-Eσ)q.
Multivariate Gaussian distributed X 〜N(0, Ip) can be checked to belong to the family of Con-
centrated random vectors. The major advantage of using concentrated random vectors for data
modeling is that this concentration property is stable under Lipschitz transformation, that is, for
any 1-Lipschitz map f : Rp → Rq, if x ∈ Rp is concentrated, so is x0 = f (x). Among the broad
family of concentrated random vectors, it has been particularly shown in (Seddik et al., 2020) that
artificial images generated by a Generative Adversarial Network (GAN) (Seddik et al., 2020), which
look extremely close to real-world images (as they are designed to), are Lipshitz transformations of
Gaussian vectors and can thus be, by definition, concentrated random vectors.
As a result, concentrated random vectors are more appropriate (than GMM for instance) in modeling
realistic data, at least for those “close” to data generated by GANs.
The same was shown experimentally for CNN representations of real images (Seddik et al., 2019;
2020) as well as words embeddings in Natural Language Processing (Couillet et al., 2020, November).
As for the random projector matrix, we assume that the matrix W has i.i.d. entries of zero mean,
unit variance and bounded fourth-order moment, with no restriction on their particular distribution as
follows.
Assumption 5 (On random projection matrix) The random matrix W has i.i.d. entries such that
E[Wij] = 0, E[Wi2j] = 1 and E[Wi4j] < λW for some constant λW < ∞.
Remark 2 (On the i.i.d. assumption of entries of W) In Assumption 5 we consider the setting
where the entries of W are independently and identically distributed: this is the case for the
popular vanilla random Fourier features in (Rahimi & Recht, 2008) and the arc-cosine kernels in a
neural network context; but not the case for, e.g., data dependent random features approaches such
as leverage score based methods (Li et al., 2021).
We consider the family of activation functions σ(∙) satisfying the following assumption.
Assumption 6 (On activation function) The function σ is at least twice differentiable (in the sense
of distributions when applied to a random variable having a non-degenerate distribution func-
tion), with max{E∣σ(z)∣, E∣σ2(z)|, E∣σ0(z)∣, E∣σ00(z)∣} ‹ λσ for some constant λσ < ∞ and
Z 〜N(0,1).
Remark 3 (Activation functions not differentiable everywhere) Some popular activation func-
tions used in machine learning such as ReLu, Sign, Absolute value, etc., are not differentiable
everywhere. For those functions, we will use a derivative in the sense of distributions (Friedlander
et al., 1998). A distribution g is a continuous linear functional on the set D of infinitely differentiable
functions with bounded support
g : D → R
Z+∞
g(x)φ(x) dx.
∞
The distributional derivative g0 (φ) is defined such that g0 (φ) = -g(φ0). In particular, we will be
interested here in the expectation of the derivatives of the activation function with respect to the
Gaussian measure i.e., R+∞ σ0(x)e-x2/2 dx. Following the previous definition, we have in this
particular case
∖+∞ σ0(x)e-x2/2 dx
-∞
Z+∞
xσ(x)e-x /2 dx
∞
15
Published as a conference paper at ICLR 2022
which can be evaluated by some integration by parts. This is also refereed to the as “weak derivative”
in the functional analysis literature (Stein & Shakarchi, 2012).
A.2 Auxiliary results and proofs
Lemma 1 (Consistent estimation of T) Let Assumption 4 hold and define T，trC°∕p. Then as
n → ∞, with probability 1,
1n
n X kχik2 -T → 0.
i=1
Proof 1 (Proof of Lemma 1) From equation 7, we have that
n	Kn	n
nXkxik = nXXPkμak -√pμaZi + nXkzik.	(13)
i=1	a=1 i=1	i=1
From Assumption 4, we have that 1 PK=I PZi P kμak2 = O(PT). The second term ofequation 13
√2p μTZi is a weighted sum of independent zero mean random variables; it thus vanishes with
probability 1 as n, P → ∞ by a mere application of Chebyshev’s inequality and the Borell Cantelli
lemma. Finally, using the strong law of large numbers on the last term of equation 13, we have
almost surely,
nK
n X kzi k2 = X ~ntrCa + O(I)
i=1
a=1
K1
X na 1trC。+ o(1)
nP
a=1
where in the last line we use trCa = O(√p) from Assumption 4, and thus 1 En=I l∣Zik2 一 T → 0
almost surely. This concludes the proof.
A.3 Proof of Theorem 1
In the sequel, We will make use of Bachmann-LandaU notations but specific for random variables
in the asymptotic regime where n → ∞. For x ≡ xn a random variable and un ≥ 0, we write
xn = O(un) if for any η and D > 0, nDP(x ≥ nηun) → 0 as n → ∞. For a vector v or a diagonal
matrix with random entries, v = O(un) means that the maximal entry of v in absolute value is
O(un) in the sense defined above. When M is a square matrix, M = O(un) means that the operator
norm of M is O(un). For x a random variable, x = o(un) means that there exists κ > 0 such that
X = O(n-κun). And the same definition for the o(∙) notation applies for vectors and matrices having
random entries.
Let us define X = [xp,…，Xn] ∈ Rp×n and define
Σ = σ(WX)
where W = [wp,…，Wm]τ ∈ Rm×p with W satisfying Assumption 5, and σ a function satisfying
Assumption 6. We consider the gram matrix
G = — ΣτΣ
m
whose (i, j) entry is given by
1m
Gij = mEσ(wTXi)σ(wTXj).
k=1
We are interested in computing
κ(xi,xj) = Ew[σ(wTxi)σ(wTxj)]
(14)
16
Published as a conference paper at ICLR 2022
under Assumptions 4-6.
Under Assumption 4, we have
XTXj = ZTZj + μlμbp + μTwj/√p + μTw"√p
|{z}	|--------------{---------------}
O(P- 1 )	O(PT)
and
kXi k2
>√*	{z
。⑴	O(P-1)	O(PT)
(15)
(16)
where φi , (kZik2 - E[kZik2]).
It can be checked that, for random vector w ∈ RP having i.i.d. entries with E[wi] = 0 and E[wi2] = 1,
we have, conditioned on Xi, that Ew [(wTXi)2] = kXik2 and
Ew[(wTXi)4] = (m4 - 3)kXik2 + 2kXik4.	(17)
with m4 = E[wj ] < λW according to Assumption 5. From the trace lemma, (Bai & Silverstein,
2010, Lemma B.26), one has that
xT xi----trCa → 0	(18)
almost surely as p → ∞, for Xi ∈ Ca. Thus, under Assumption 4 we have in particular
XTxT - trC°∕p → 0 holds almost surely, regardless of the class of xT, see the proof of Lemma 1.
It then follows from Lyapunov CLT (see, e.g., (Billingsley, 2012, Theorem 27.3)) that, under As-
sumption 4 and 5, (wTxT , wTxj ) is asymptotically bivariate Gaussian. We can thus perform, in the
large p limit, a Gram-Schmidt orthogonalization procedure for some standard Gaussian variables ξa,
ξb 〜N(0,1). By construction, ξa, ξb are uncorrelated and thus independent by Gaussianity. Let us
denote the shortcuts ζa = wTxT and ζb = wTxj . We thus have
ζa ≡ wTxT = uaξa + o(1)
ζb ≡ wTxj = vbξa + ubξb + o(1)
with
ua
vb
ub
kxi k
XTXj
kxik
SkXjk2-	.
Since We have that kXik = √τ + o⑴ and ∣∣xj∙k = √τ + o⑴(from equation 16), We can perform
a Taylor expansion of σ(Za) (respectively of σ(Zb)) around √τξa (resp. √Tξb) giving
σ(Za) = σ(√τξα) + σ0(√τξa)(ζa - √τξa) + $σ00(Sa)(Za - √τξa)2 + O((Za - √τξa)2)
σ(Zb) = σ(√τξb) + σ0 (√τξb)(G - √τξb) + 2 σ00(√τξb )(G - √τξb)2 + o((G - √τξb)2).
We then have
κ(xT, Xj) = E[σ(Za)σ(Zb)]
=E[σ(√τξa)σ(√τξb)] + E[σ(√τξa)σ0(√τξb)(ζb - √τξb)]
+ 2E[σ(√τξa)σ00(√τξb)(ζb - √τξb)2]
+ E[σ0(√τξa)(ζa - √τξa)σ(√τξb)] + E[σ0 (√τξa )(Za - √τξa )σ0 (√τξb)(G - √τξb)]
+ 2E]σ0(ξa)(Za- √τξa)σ00(ξb)(ζb - λ∕τξb)2] + 2E]σ"(ξa)(Za- λ∕τξa)2σ(√τξb)]
+ 2E]σ"(√τξa)(Za- /&)2d(6&)(金一√τξb)]
+ 4E]σ"(√τξa)(Za- /&)2。00("&)(α一√τξb)2] + O((Za - √τξa)2).
17
Published as a conference paper at ICLR 2022
Zb - √τξb
-1) √Γξb
where the expectation is with respect to W as in the definition in (14).
Using the independence between ξα, ξb along with the following
ζa - √τξα = ( √τ - l)√τξα	(19)
+ √= √τξα	(20)
we have for ξ 〜N(0,1),
E[σ(√τξα)σ(√τξb)] = (E[σ(√rξ)])2	QI)
E[σ(√τξa)σ'(√τξb)(Zb - √rξ,b)] = (E[σ(√τξ)]E[√τξσ0(√τξ)]) (√T — 1)
+ (E[σ0(√Tξ)]E[√Tξσ(√Tξ)]) √	(22)
Similarly writing down the products (Za - √τξa)2, (ζb - √τξb)2, (ζa - √τξa)(ζb - √τξb), (ζa -
√Tξa)(Zb - √τξb)2, (Za - √τξa)2(Zb - √τξb), (Za - √τξa)2(Zb - √τξb)2 as in Equations 21- 22,
and using Equations 19- 20, we obtain a complete expression of all the terms in K(Xi, Xj) as follows
κ(xi, Xj) = (E[σ(√7ξ)])2 + (E[σ(√7ξ)]E[√7ξσ,(√Γξ)]) (√τ - 1) + (E[σ,(√Tξ)]E[√Tξσ(√Γξ)]) √
+ 旧听”(员)]2) ((√τ -1)( √τ -1)) + (Ev 阳;"(A)D W -1)2)
+ (E[√Tξσ'(√Tξ)]E[τξ2σ"(√Γξ)]) (√L - 1)(2 - ^2
+ (E[√Tξσ'(√Tξ)]E[τξ2σ"(√Γξ)])(号 - ^(√ - I)
+ 闺2%；(VT'K) (√τ - 1)2 (√ - 1)2 + (E[√Tξσ(√Tξ)]E[√Tξσ00(√Tξ)]) √ (√ - 1)
(W(VTξW'd) (√)2 + (E[σ(√7ξ)]E[√7ξσ0(√7ξ)]) (√T - 1)
+ (E[√Tξσ00(√Tξ)]E[τ√Tξ3σ00(√Tξ)]) VbT (√ - 1)(√ - 1)[
3
+ O(PF).
(23)
18
Published as a conference paper at ICLR 2022
Since |xiTxj | ≤ for a sufficiently small , (following from equation 15), using a Taylor approxima-
tion of JkXjk2 - (XxxjI? We obtain
We thus have
O(P-2)
=τ12 (ZTZj )2 + O(P- 2).
(24)
3
All other terms in equation 23 are of order at most O(P-2) and thus vanish asymptotically. We thus
get
κ(xi, Xj) = (E[σ(√Tξ)])2 + (E[σ(√Tξ)]E[√Tξσ0(√Tξ)]) ^√u= - 1)+ (E[σ0(√Tξ)]E[√Tξσ(√Tξ)]) √ +
+ 5ξσ0(√W) ((√ - 1)(√τ - 1)) + (ger%0])(目2
+ (E[σ(√7ξ)]E[√7ξσ0(√Tξ)]) (√ - ] + O(P-2)1 .	(25)
Plugging the terms in equation 24 into equation 25 and rearranging in matrix form, We obtain for
K = P{κ(xi, xj)}in,j=1P and K defined in Theorem 1, that kK - Kk → 0, as expected, Where We
used the fact that kAk ≤ nkAk∞ for A ∈ Rn×n and kAk∞ = maxin,j=1 |Aij |. This concludes the
proof of Theorem 1.
A.4 Proof of Corollary 1
Define the Ternary Random Features matrix Σter = σter (Wter X) With Wter defined in (4) having
sparsity level e, the associated Gram matrix Gter = ml (Σter )TΣter as in (6), and the limiting kernel
Kter , P{κter(xi, xj)}in,j=1P	(26)
19
Published as a conference paper at ICLR 2022
for κter defined in (3). After calculation similar to (Liao & Couillet, 2019, Section 4) and (Liao et al.,
2021), we obtain
E[(σter)0 (√Tz)]2
E[(σter)00 (√Tz)]2
s2	s2	2
------+ I	...- ∖
s+e T + s-e T ∖
T ∖∕2πτ	I
(27)
Thus, according to Theorem 1, for random features kernel matrix K of the form (2) with W and
nonlinear σ satisfying Assumption 1-3, with associated generalized Gaussian moments d0, d1, d2
defined in Theorem 1, by choosing s-, and s+ such that d1 and d2 are respectively equal to the first
and second term of equation 27, we have that
kK-Kter-λPk →0,
almost surely with
λ = do - (E[(σter)2 (√Tz)] - E[(σter) (√Tz)]2 - τE[(σter)0 (√Tz)]2)
where
20
Published as a conference paper at ICLR 2022
A.5 Gaussian moments of popular activation functions
We provide in Table 1 the calculation of the generalized Gaussian moments d0, d1, d2 for popular
activation functions used in random features and neural network contexts. Note that our Table 1
matches the results in (Liao & Couillet, 2018b, Table 2).
Table 1: Values of d0, d1, d2 for different activation functions.
σ⑴	do	di	(d2
	∖t∖		T(1-2)	0	1 2πτ
max(0, t)	2 (2- ∏)	1 	4		1 8πτ
-1, t < s- +1, t > s+ 0, otherwise	(1-erf (√τ)) 2 1 e e-s+ +e-S- !2 -Y π l	(e-争+e-s- !2	C - s++	- S- λ2 I s+ e T +s- e T I
			T	τ √2πτ	I
a+ max(0, t) + a- max(0, —t)	T (a+ + a-)2 (π4∏2)	(a+-a- )2 4	(a+ + a- )2 8πτ
02t2 + a`t + ao	2τ 2a2	al	a2
exp(t)	11 √2Γ+Γ	T +1	0	1 4(τ +1)3
cos(t)	1+e-2τ - e-τ 2	e	0	e-τ ~T~
sin(t)	1-e-2τ	-τ JeTe	e-τ	0
t	0	1	0
sign(t)	1 — 2 π	2 πτ	0
1t>0	1 _ ɪ 4	2π	1 2πτ	0
A.6 Additional experiments
In this section, we complement Section 4 by providing additional experiments on ridge regression
and support vector machine, to support the robustness our TRF method compared to state-of-the-art
approaches.
Remark 4 (On the empirical and expected random features kernels) It is worth noting that our
main results in Theorem 1 and Corollary 1 characterize the behavior of the expected/ limiting
random features kernel K instead of the empirical Gram kernel matrix G defined in (6) obtained by
averaging over m random features. The operator norm difference kK - Gk is then bound to vanish
as m, p, n → ∞ for a sufficiently large m (for example with m/ max(n, p) → ∞).
Random features based Ridge regression To complement the experiments in Section 4.1, we
provide in Figures 5-9, the test mean square error of random features kernel ridge regression with
increasing number of random features m ∈ {512, 4096, 104} for GMM data in Figure 5-7, and
m ∈ {512, 104} for MNIST data in Figure 8 and 9, as a function of the regularization parameter γ
and for different choices of sparsity levels ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. It is interesting to note that,
for both datasets, the performance gap between RFFs and TRFs significantly decreases as m the
number of random features grows large: this is in agreement with our Theorem 1 in which guarantee
is provided only for the expected kernel matrix (that corresponds to m → ∞), not the empirical
Gram matrix as the sample mean over m random features.
21
Published as a conference paper at ICLR 2022
1
0.8
0.6
0.4
0.2
1
0 C
10-2	10
102
γ
20
0.1
000
086
1
)s( emit gninnuR
—Baseline (KRR) 一
——	RFF
-E=F	TRF
0.3
耳I	口
0.5	0.7	0.9
Figure 5: Testing MSE of kernel ridge regression as a function of regularization parameter γ,
p = 512, n = 1024, ntest = 512, m = 512. Ternary function (with thresholds s-, s+ chosen to
match gaussian moments of [cos, sin] function) with W distributed according to equation 4 with
e ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR and RFF. GMM dataset with 从。=[0。-。4; 0p-a], Ca =
(1 + 4(a - 1)/√p)Ip, P = 512, n = 2048. Results averaged over 5 independent runs.
6
.
0
sESM
——Baseline (KRR)
—— RFF
——e = 0.1 (TRF)
——e = 0.5 (TRF)
——e = 0.9 (TRF)
150
——Baseline (KRR)
——	RFF
-E=P	TRF
0.2
0
10-2 10-1	1
102
γ
)s( emit gninnu
100
50
104
0.1	0.3	0.5	0.7	0.9
⅞----------ES---------⅛
Figure 6: Testing MSE of kernel ridge regression as a function of regularization parameter γ,
p = 512, n = 1024, ntest = 512, m = 4096. Ternary function (with thresholds s- , s+ chosen to
match gaussian moments of [cos, sin] function) with W distributed according to equation 4 with
e ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR and RFF. GMM dataset with 从。=[0。-1；4; 0p-a], Ca =
(1 + 4(a - 1)/√p)Ip, p = 512, n = 2048. Results averaged over 5 independent runs.
1
86
..
00
sESM
500
——Baseline (KRR)
—— RFF
——e = 0.1 (TRF)
——e = 0.5 (TRF)
——e = 0.9 (TRF)
0.2
0
10-2
10-1	1
_I__
102
γ
400
§
300
OO
g
S
200
100
——Baseline (KRR)
——	RFF
-E=F	TRF
104
0.3
0.5	0.7	0.9
e
0.1
Figure 7: Testing MSE of kernel ridge regression as a function of regularization parameter γ,
p = 512, n = 1024, ntest = 512, m = 104. Ternary function (with thresholds s-, s+ cho-
sen to match gaussian moments of [cos, sin] function) with W distributed according to equa-
tion 4 with e ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR baseline and RFF. GMM dataset with
μa = [0。-1；4；0p-a], Ca = (1 +4(a - 1)∕√p)Ip, P = 512,n = 2048.). Results averaged
over 5 independent runs.
22
Published as a conference paper at ICLR 2022
1 .8 .6 4. .2 010
0. 0. 0. 0.
sESM
10
4
10
一 - 一 U
O O QIOO
8642
)s( emit gninnuR
印
Figure 8: Testing mean squared errors (MSEs, LEFT) and running time (RIGHT) of kernel ridge
regression as a function of regularization parameter γ, p = 512, n = 1024, ntest = 512, m = 512.
Ternary function (with thresholds s-, s+ chosen to match the Gaussian moments d1, d2 of [cos, sin]
function) with W distributed according to (4) with ∈ {0.1, 0.3, 0.5, 0.7, 0.9}, versus KRR and
RFFs on MNIST dataset 2 classes - digits (7, 9). Results averaged over 5 independent runs.
1 .8 .6 .4 .2 0
0000
sESM
Figure 9: Testing mean squared errors (MSEs, LEFT) and running time (RIGHT) of kernel ridge
regression as a function of regularization parameter γ, p = 512, n = 1024, ntest = 512, m = 104.
Ternary function (with thresholds s-, s+ chosen to match the Gaussian moments d1, d2 of [cos, sin]
function) with W distributed according to (4) with ∈ {0.1, 0.3, 0.5, 0.7, 0.9}, versus KRR and
RFFs on MNIST dataset 2 classes - digits (7, 9). Results averaged over 5 independent runs.
)s( emit gninnu
0.1	0.3	0.5	0.7	0.9
23
Published as a conference paper at ICLR 2022
2468
log(m/p)
Figure 10: Test accuracy using libsvm on different state-of-the-art random features kernels. a8a (UCI)
dataset. Number of training samples n = 22696 - number of test samples nt = 9865, varying ratio
log m/p number of random features over dimension p = 123. Note that the y-axis is zoomed in to
better distinguish the performance of different methods.
log(m/p)
*SSF
-O-RFF
-0-ORF
*TRF
Figure 11: Test accuracy using libsvm on different state-of-the-art random features kernels.
IJCNN1 (Prokhorov, 2001) dataset. Number of training samples n = 49990 - number of test
samples nt = 91701, varying ratio log m/p number of random features over dimension p = 22.
Note that the y-axis is zoomed in to better distinguish the performance of different methods.
Random features based Support Vector Machine We empirically evaluate the classification
performance of various random features approximation algorithms, on several benchmark datasets.
We compared the different algorithms (RFF (Rahimi & Recht, 2008), ORF (Yu et al., 2016), SSF (Lyu,
2017)) on 3 datasets IJCNN1 (Prokhorov, 2001), Cov-Type, and a8a from the UCI ML repository
considered in (Liu et al., 2021a), with our TRF method where we choose the thresholds coefficients
s- , s+ according to Algorithm 1 (to match the generalized Gaussian moments of Gaussian kernel).
Figure 10 shows the results for the a8a dataset, Figure 11 for the IJCNN1 dataset and Figure 12 for
the Cov-Type dataset. The lower running time along with higher SVM test accuracy indicates the
superiority of our RMT-inspired TRF method over the other Gaussian kernel approximation methods.
Comparison of TRF with equivalent 0th order Arc-cosine kernel (with ReLU function) We
consider Support Vector Machine (SVM) classification with random features Gram matrix G on
Fashion MNIST data (LeCun et al., 1998) and on VGG16 embeddings of CIFAR10 (Krizhevsky et al.,
2009) and Imagenet (Deng et al., 2009) datasets in Figures 14 -13 -15 respectively. We use VGG16
with batch normalization (Ioffe & Szegedy, 2015) pre-trained on ImageNet (Deng et al., 2009) as a
feature extractor. We fine-tune this model on the CIFAR10 dataset with 240 epochs and a mini-batch
size 64 with a SGD optimizer with momentum 0.9 and an initial learning rate of 0.1. We then extract
the output of the first fully connected layer of the classifier as our features. For Imagenet, we directly
extract the features from the pretrained model. We compare (i) RF with σ(t) = max(t, 0) (ReLU)
and Gaussian Wij 〜N(0,1) (known as the 0th order Arc-cosine kernel) to (ii) the proposed TRF
24
Published as a conference paper at ICLR 2022
362
4208
4443
)%( erocs ycarucca MVS raeniL
46
log(m/p)
)s( emit gninnuR
2
46
log(m/p)
*SSF
-O-RFF
-0-ORF
*TRF
90
Figure 12: Test accuracy using libsvm on different state-of-the-art random features kernels. Cov-Type
dataset. Number of training samples n = 49990 - number of test samples nt = 91701, varying ratio
log m/p number of random features over dimension p = 22.
-≈⅛-Arc-Cosine RF (σ(t) = ReLU(t))
-O-	TRF (σ(t) = σter(t))
)%( ycaruccA MVS tseT
50
10	210	410	710	910
m
070	210	350	490	630
m
Figure 13: SVM test accuracy using different kernels. VGG-16 Embeddings of CIFAR10 dataset
(Number of features p = 4096). Number of samples n = 1024 fixed, varying number of random
features from m = 10 to m = 1800.
8
method with σter (t) in (3) and ternary random projection matrix Wter defined in (4). The thresholds
s- , s+ of σter are tuned in such away that the generalized Gaussian moments d1 and d2 are matched
with those of ReLU (see Table 1), as described in Corollary 1 and Algorithm 1. Figures 14 -13 -15
display the test SVM accuracy as a function of the number of random features, for our TRF method
compared to the random features corresponding to the Arc-Cosine kernel. We observe similar test
performances with the two kernels while having a computation and storage gains for the ternary
kernel.
25
Published as a conference paper at ICLR 2022
yCaruCCA MVS tseT
0.8
J^ArC-Cosine RF (σ(t) = ReLU(t))
-O-	TRF(σ(t) = σter(t))
)s( emit gninnuR
070	210	350	490	630
m
0.6
. 70
210	350
m
490	630
Figure 14: SVM test aCCuraCy using different kernels. Raw Fashion-MNIST dataset (Number of
features p = 784). Number of samples n = 1024 fixed, varying number of random features from
m = 70 to m = 700. Note that the y-axis is zoomed in to better distinguish the performanCe of
different methods.
432
...
000
yCaruCCA MVS tseT
210
350	490
m
630
Figure 15: SVM test aCCuraCy using different kernels. LeNet-64 Embeddings of Imagenet dataset
(Number of features p = 2018). Number of samples n = 1024 fixed, varying number of random
features from m = 10 to m = 1800.
10
0
1
)s( emit gninnuR
70	210	350	490	630
m
26