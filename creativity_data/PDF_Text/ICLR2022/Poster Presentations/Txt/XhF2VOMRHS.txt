Published as a conference paper at ICLR 2022
A Unified Contrastive Energy-based Model
for Understanding the Generative Ability of
Adversarial Training
Yifei Wang1 Yisen Wang2,3* Jiansheng Yang1 Zhouchen Lin2,3,4
1	School of Mathematical Sciences, Peking University
2	Key Lab. of Machine Perception (MoE), School of Artificial Intelligence, Peking University
3	Institute for Artificial Intelligence, Peking University
4	Pazhou Lab, Guangzhou, 510330, China
Ab stract
Adversarial Training (AT) is known as an effective approach to enhance the ro-
bustness of deep neural networks. Recently researchers notice that robust mod-
els with AT have good generative ability and can synthesize realistic images,
while the reason behind it is yet under-explored. In this paper, we demystify this
phenomenon by developing a unified probabilistic framework, called Contrastive
Energy-based Models (CEM). On the one hand, we provide the first probabilistic
characterization of AT through a unified understanding of robustness and gener-
ative ability. On the other hand, our unified framework can be extended to the
unsupervised scenario, which interprets unsupervised contrastive learning as an
important sampling of CEM. Based on these, we propose a principled method
to develop adversarial learning and sampling methods. Experiments show that
the sampling methods derived from our framework improve the sample quality
in both supervised and unsupervised learning. Notably, our unsupervised adver-
sarial sampling method achieves an Inception score of 9.61 on CIFAR-10, which
is superior to previous energy-based models and comparable to state-of-the-art
generative models.
1	Introduction
Adversarial Training (AT) is one of the most effective approaches developed so far to improve the
robustness of deep neural networks (DNNs) (Madry et al., 2018). AT solves a minimax optimization
problem, with the inner maximization generating adversarial examples by maximizing the classifica-
tion loss, and the outer minimization finding model parameters by minimizing the loss on adversarial
examples generated from the inner maximization (Wang et al., 2019). Recently, researchers have
noticed that such robust classifiers obtained by AT are able to extract features that are perceptually
aligned with humans (Engstrom et al., 2019). Furthermore, they are able to synthesize realistic im-
ages on par with state-of-the-art generative models (Santurkar et al., 2019). Nevertheless, it is still
a mystery why AT is able to learn more semantically meaningful features and turn classifiers into
generators. Besides, AT needs the labeled data {(xi, yi)} for training while canonical deep genera-
tive models do not, e.g., VAE (Kingma & Welling, 2014) and GAN (Goodfellow et al., 2015) only
require {xi}. Thus, it is worth exploring ifit is possible to train a robust model without labeled data.
Several recent works (Jiang et al., 2020; Kim et al., 2020; Ho & Vasconcelos, 2020) have proposed
unsupervised AT by adversarially attacking the InfoNCE loss (Oord et al., 2018) (a widely used
objective in unsupervised contrastive learning), which indeed improves the robustness of contrastive
encoders. However, a depth investigation and understanding for unsupervised AT is still missing.
To address the above issues, in this work, we propose a unified probabilistic framework, Contrastive
Energy-based Models (CEM), that provides a principled understanding on the robustness and the
generative ability of different training paradigms. Specifically, we make the following contributions:
•	Demystifying adversarial training and sampling. We firstly propose a probabilistic in-
terpretation for AT, that is, it is inherently a (biased) maximum likelihood training of the
* Corresponding author: YisenWang (yisen.wang@pku.edu.cn).
1
Published as a conference paper at ICLR 2022
corresponding energy-based model, which explains the generative ability of robust models
learned by AT. Inspired by this, we propose some novel sampling algorithms with better
sample quality than previous methods.
•	A unified probabilistic framework. Based on the understanding above, we propose Con-
trastive Energy-based Model (CEM) that incorporates both supervised and unsupervised
learning paradigms. Our CEM provides a unified probabilistic understanding of previous
standard and adversarial training methods in both supervised and unsupervised learning.
•	Principled unsupervised adversarial training and sampling. Specifically, under our
proposed CEM framework, we establish the equivalence between the importance sampling
of CEM and the InfoNCE loss of contrastive learning, which enables us to design principled
adversarial sampling for unsupervised learning.
Notably, we show that the sampling methods derived from our framework achieve state-of-the-art
sample quality (9.61 Inception score) with unsupervised robust models, which is comparable to both
the supervised counterparts and other state-of-the-art generative models.
2	Related Work
Robust generative models. Researchers recently notice that features extracted by robust classifiers
are perceptually aligned with humans, while standard classifiers are not (Engstrom et al., 2019;
Kaur et al., 2019; Bai et al., 2021). Santurkar et al. (2019) show that we can also generate images
of high quality with robust classifiers by iterative updating from a randomly sampled noise, where
the resulting sample quality is comparable to the state-of-the-art generative models like BigGAN
(Brock et al., 2018).
Contrastive learning. Oord et al. (2018) firstly propose unsupervised contrastive learning by max-
imizing a tractable lower bound on mutual information (MI), i.e., the negative InfoNCE loss. How-
ever, later works find that the lower bounds degrade a lot with a large MI, and the success of these
methods cannot be attributed to the properties of MI alone (Poole et al., 2019; Tschannen et al.,
2020). Our work provides an alternative understanding of unsupervised contrastive learning as im-
portance sampling of an energy-based model, which also enables us to characterize the limitations of
existing methods from a new perspective. In fact, contrastive learning can also be seen as a general
learning framework beyond the unsupervised scenarios. For example, SupContrast (Khosla et al.,
2020) extends contrastive learning to supervised scenarios. Our work further bridges supervised,
unsupervised and adversarial contrastive learning with a unified probabilistic framework.
3	CEM: A Unified Probabilistic Framework
Inspired by previous work that bridges discriminative models with energy-based models (Grathwohl
et al., 2019), in this work, we propose a unified framework, called Contrastive Energy-based Model
(CEM), that incorporates both supervised and unsupervised scenarios.
Our proposed CEM is a special Energy-based Model (EBM) that models the joint distribution
pθ(u, v) over two variables (u, v) with a similarity function fθ(u, v) defined in a contrastive form,
exp(fθ (u, v))
pθ (U, V)=	Z (θ)
(1)
where Z(θ) = exp (fθ (u, v)) dudv is the corresponding partition function. In other words, in
CEM, a pair of samples (u, v) has higher probability if they are more alike. In particular, it can be
instantiated into the two following variants under different learning scenarios.
Parametric CEM. In the supervised scenario, we specify the Parametric CEM (P-CEM) that models
the joint distribution pθ (x, y) of data x and label y in the following form,
pθ(x,y)
exp(fθ (x, y))
-Zθ-
exp(gθ (x)>wy)
-Zθ-
(2)
where gθ : Rn → Rm denotes the encoder, g(x) ∈ Rm is the representation of x, and wk ∈ Rm
refers to the parametric cluster center of the k-th class. Denote the linear classification weight as
2
Published as a conference paper at ICLR 2022
W = [wι,…，WK] and the logit vector as h(x) = g(x)>W, We can see the equivalence between
P-CEM and JEM (Grathwohl et al., 2019) as
fθ(x,y) = gθ(x)>wy = hθ(x)[y].
(3)
Non-Parametric CEM. In the unsupervised scenario, we do not have access to labels, thus we
instead model the joint distribution between two natural samples (x, x0) as
pθ (x, x0)
exp(fθ (x, x0))
-Zθ-
exp (gθ(x)>gθ(x0))
Zθ
and the corresponding likelihood gradient of this Non-Parametric CEM (NP-CEM) is
VθEpd(x,χ0) logPθ(x, x0) = Epd(χ,χ,)Vθfθ(x, x0)-Epθ(χ,χo)Vθfθ(x, x0).
(4)
(5)
In contrastive to P-CEM that incorporates parametric cluster centers, the joint distribution of NP-
CEM is directly defined based on the feature-level similarity between the two instances (x, x0). We
define the joint data distribution pd(x, x0) = pd(x)pd(x0|x) through re-parameterization,
x0 = fθ(t(x)),	t UTr T,	X 〜Pd(x),
(6)
where u.a.r. denotes sampling uniformly at random and T refers to a set of predefined data aug-
mentation operators T = {t : Rn → Rn}. For the ease of exposition, we assume the empirical
data distribution pd(x) is uniformly distributed over a finite (but can be exponentially large) set of
natural samples X .
4	Supervised Scenario: Rediscovering Adversarial Training as
Maximum Likelihood Training
In this section, we investigate why robust models have a good generative ability. The objective of
AT is to solve the following minimax optimization problem:
min Epd(x,y)
θ
max
llχ-χkp≤ε
'ce(x,y; θ) , where 'ce(x,y; θ) = — logpθ(y∣x).
(7)
The inner maximization problem is to find an adversarial example X within the 'p-norm ε-ball around
the natural example x that maximizes the CE loss. While the outer minimization problem is to find
model parameters that minimize the loss on the adversarial examples x.
4.1	Maximization Process
For the inner maximization problem, Projected Gradient Descent (PGD) (Madry et al., 2018) is the
commonly used method, which generates the adversarial example X by maximizing the CE loss1
(i.e., minimizing the log conditional probability) starting from x° = x:
Xn+1 = Xn + αVχn'(xn,y; θ) = Xn - αV^n logPθ(y∣xn)
K
=Xn + αVχn log X exp(fθ (Xn,k)) — αVχn fθ (Xn,y),	(8)
k=1
while the Langevin dynamics for sampling P-CEM starts from random noise Xo = δ and updates
with
Xn+1 = Xn + αVχ log Pθ (Xn) + √2α ∙ ε	(9)
K
=xn + αVχn log£exp(fe(Xn,k)) + √2α ∙ ε.
k=1
Eqns. 8 and 9 both have a positive logsumexp gradient (the second term) to push up the marginal
probability pθ(X). As for the third term, PGD starts from a data point (x, y) such that it requires
the repulsive gradient to be away from the original data point and do the exploration in a local
1 Note that we omit the projection operation and the gradient re-normalization steps.
3
Published as a conference paper at ICLR 2022
region. Langevin dynamics instead starts from a random noise and an additive noise ε is injected
for exploration.
Comparing PGD and Langevin. Following the above analysis, the maximization process in AT
can be seen as a (biased) sampling method that draws samples from the corresponding probabilistic
model pθ (X). Compared to Langevin dynamics, PGD imposes specific inductive bias for sampling.
With the additional repulsive gradient and ε-ball constraint, it explicitly encourages the samples tobe
misclassified around the original data points. In practice, adversarial training with such adversarial
examples is generally more stable than training JEM with Langevin samples, which indicates that
PGD attack is a competitive alternative for the negative sampling method for JEM training.
4.2	Minimization Process
To begin with, the gradient of the joint log likelihood for P-CEM can be written as follows:
Vθ Epd(x,y) log Pθ (x,y)
=Epd(X,y)Vθ fθ (x, y)-Epθ (^,y)Vθ fθ (x, y)	(10)
=Epd(X,y)Vθ fθ (x,y)-Epθ (X)pθ (y∣^)Vθ fθ (x,y),
where (x, y)〜Pd(x, y) denotes the positive data pair, and (x, y)〜pθ(x, y) denotes the negative
sample pair. As discussed above, the adversarial examples X generated by the maximization process
can be regarded as negative samples, and y 〜pθ(y∣x) denotes the predicted label of x. To see how
the maximum likelihood training of P-CEM is related to the minimization process of AT, we add an
interpolated adversarial pair (x, y) into Eq. 10 and decompose it as the consistency gradient and the
contrastive gradient:
νθEpd(χ,y) logPθ(X,y) = Epd(χ,y) Npθ(X,y) [vθfθ(x,y)-vθfθ(X, y)]
=Epd(X,y) N pθ (X,y)[ Vθ fθ (x,y)-Vθ fθ (x,y) + Vθ fθ (x,y)-Vθ fθ (X ,^ ].	(II)
|^^^^^™^^^^^^{^^^^^^™^^^^^^}	∙^^^^^^"^^^^^^{^^^^^^β^^^^^^}
consistency gradient	contrastive gradient
Next, we show that the two parts correspond to two effective mechanisms developed in the adver-
sarial training literature.
AT loss. As the two sample pairs in the contrastive gradient share the same input x, We can see that
the contrastive gradient can be written equivalently as
Epd(χ,y) N pθ (X,y) [Vθ fθ (x,y)-Vθ fθ (x ,y)]
=Epd(X,y) Npθ(X) [vθfθ(x,y) - Epθ(y|X)Vefθ(X,y)]
=Epd(X,y) N pθ (x)Vθ log pθ (y|X),	(12)
which is exactly the negative gradient of the robust CE loss (AT loss) in Eq. 7, in other words,
gradient ascent with the contrastive gradient is equivalent to gradient descent w.r.t. the AT loss.
Regularization. As for the consistency gradient, original AT (Madry et al., 2018) simply ignores it.
Its variant TRADES (Zhang et al., 2019) instead proposes the KL regularization KL(p(∙∣X)kp(∙∣x))
that regularizes the consistency of the predicted probabilities on all classes, whose optimum implies
thatp(∙∣X) = p(∙∣x) → fθ(x,y) = fθ(X,y).
Comparing AT and JEM training paradigms. The above analysis indicates that the minimization
objective of AT is closely related to the maximum likelihood training of JEM (Grathwohl et al.,
2019). Compared to JEM that decomposes the joint likelihood into an unconditional model pθ (x)
and a discriminative model pθ (y|x), the decomposition ofAT in Eq. 10 instead stabilizes training by
introducing an intermediate adversarial pair (X, y) that bridges the positive pair (x, y) and the nega-
tive pair (X, y). Besides, it can inject the adversarial robustness bias by regularizing the consistency
gradient. Together with our analysis on the maximization process, we show that AT is a competitive
alternative for training JEM (a generative model) with more stable training behaviors. That explains
why robust models with AT are also generative.
4.3	Proposed Supervised Adversarial Sampling Algorithms
Our interpretation also inspires principled designs of sampling algorithms for robust classifiers.
4
Published as a conference paper at ICLR 2022
Targeted Attack (TA). Previously, to draw samples from a robust classifier, Santurkar et al. (2019)
utilize targeted attack that optimizes an random initialized input X0 towards a specific class y:
K
Xn+1 = Xn + αVχn logPθ(y∣Xn) = Xn + αVχf (Xn,y) - αVχn logXexp(fθ(Xn,k)) .	(13)
k=1
Compared to PGD attack in Eq. 8, while pushing X towards y, TA has a negative logsumexp gradient
that decreases the marginal probability pθ(X). This could explain why TA is less powerful for
adversarial attack and is rarely used for adversarial training.
Conditional Sampling (CS). To overcome the drawback of targeted attack, a natural idea would be
dropping the negative logsumexp gradient. In fact, we can show that this is equivalent to sampling
from the conditional distribution:
Pθ HO) = expf(χy), Zχ∣y(θ) = Z expf (x,y))dx,
Zχ∣y(θ)	Jx
and its Langevin dynamics takes the form:
Xn+1 = Xn + αVxn logPθ(Xn|y) + √2ɑ ∙ ε = Xn + αV^n fθ(Xn) y) + √2α ∙ ε.	(14)
Samples drawn this way essentially follow an approximated model distribution, pθ (X, y) ≈
Pd(y)Pθ(X|y). Thus, CS can be seen as a debiased targeted attack algorithm.
Reinforced Conditional Sampling (RCS). Inspired by the above analysis, we can design a biased
sampling method that deliberately injects a positive logsumexp gradient:
K
Xn+1 =Xn + αVxnfθ(Xn, ^ + (OVxn log X exp(fθ(Xn, k)) + √2α ∙ ε.	(15)
k=1
With our designed bias, RCS will sample towards the target class y by maximizing pθ (X|y) (with the
fθ(Xn, y) term), and at the same time improve the marginal probability pθ(X) (with the logsumexp
term). As shown in our experiment, RCS indeed obtains improved sample quality.
4.4	Discussion on Standard Training
In the above discussion, we have explained why adversarial training is generative from the perspec-
tive of CEM. In fact, it can also help characterize why classifiers with Standard Training (ST) are
not generative (i.e., poor sample quality). A key insight is that if we replace the model distribution
Pθ(X) with the data distribution Pd(X) in Eq. 10, we have
VθEpd(x,y) logPθ(X, y) = Epd(x,y) Vθfθ(x, y)-Ep°(x)p°(y|x)Vθfθ(X, y)
≈Epd(x,y) Vθfθ(χ, y) — Epd(x)pθ(y∣x)Vθfθ(χ, y) = VθEpd(x,y) logpθ(y∣χ),	(16)
which is the negative gradient of the CE loss in Eq. 7. Thus, ST is equivalent to training CEM by
simply replacing model-based negative samples X 〜pθ(χ) with data samples X 〜Pd(X). This
approximation makes ST computationally efficient with good accuracy on natural data, but signifi-
cantly limits its robustness on adversarial examples (as model-based negative samples). Similarly,
because ST ignores exploring negative samples while training, standard classifiers also fail to gen-
erate realistic samples.
5	Extension of Adversarial Training to Unsupervised S cenario
In this section, we show that with our unified framework, we can naturally extend the interpretation
developed for supervised adversarial training to the unsupervised scenario.
5.1	Understanding Unsupervised S tandard Training through CEM
InfoNCE. Recently, the following InfoNCE loss is widely adopted for unsupervised contrastive
learning of data representations (Oord et al., 2018; Chen et al., 2020; He et al., 2020),
'nce(x, x0,{Xj}K=1； θ) = - log
exp(fθ (x, X0))
PK=j eχp(fθ (χ, χj)),
(17)
5
Published as a conference paper at ICLR 2022
where fθ(x, X) = gθ(x)>gθ(X) calculates the similarity between the representations of the two
data samples, x, x0 are generated by two random augmentations (drawn from T) of the same data
example, and {Xj}K=ι denotes K independently drawn negative samples. In practice, one of the
K negative samples is chosen to be the positive sample x0 . Therefore, InfoNCE can be seen as an
instance-wise K-class cross entropy loss for non-parametric classification.
Perhaps surprisingly, we show that the InfoNCE loss is equivalent to the importance sampling esti-
mate of our NP-CEM (Eq. 4) by approximating the negative samples from pθ(x) with data samples
from pd(x), as what we have done in standard supervised training (Section 4.4):
Epd(χ,χ0)Vθfθ(x, x0) - Ep03x，)Vefθ (X, X0)
—E	V f 0“ C _E	exP(fθ(X,x'))
=Epd(X,xO)Vθ fθ(X, X )-Epθ⑻Pd(XO) Epd(X) eχp(fθ (X ,X))
≈e	v f(χ χ0) _E	eχp(fθ(X,x'))
≈Epd(XKO)Vθfθ(X, X ) - Epd(X)Pd(XO) Epd(X) eχp(fθ(X,X))
Vθ fθ (X, X0)
Vθ fθ (X, X0)
(18)
eχp(fθ(X, X0))
=Epd(Xκo)vθ log Epd(XO) eχρ(fθ(χ, χ0))
≈ ɪ X Vθ log	Kexp(fθX,Xi)),
Ni=I	Pk=I eχp(fθ(Xi, Xik))
which is exactly the negative gradient of the InfoNCE loss. In the above analysis, for an empirical
estimate, we draw N positive pairs (Xi, Xi)〜Pd(x, x0), and for each anchor Xi, we further draw K
negative samples {Xik} independently frompd(X0).
Remark. As pθ(X, X0) = pθ(X)pθ(X0∣X), the negative phase OfNP-CEM is supposed to sample X0
from pθ(X0∣X), where samples semantically close to the anchor sample X, a.k.a. hard negative sam-
ples, should have high probabilities. However, InfoNCE adopts a non-informative uniform proposal
Pd(X0) for importance sampling, which is very sample inefficient because most samples are useless
(Kalantidis et al., 2020). This observation motivates us to design more efficient sampling scheme for
contrastive learning by mining hard negatives. For example, Robinson et al. (2021) directly replace
the plain proposal with pθ(X∣X0) = eχp(βfθ(X, X0))∕Zβ(θ) while keeping the reweighing term.
From the perspective of CEM, the temperature β introduces bias that should be treated carefully. In
all, CEM provides a principled framework to develop efficient contrastive learning algorithms.
5.2	Proposed Unsupervised Adversarial Training
AT is initially designed for supervised learning, where adversarial examples can be clearly defined
by misclassification. However, it remain unclear what is the right way to do Unsupervised Adver-
sarial Training (UAT) without access to any labels. Previous works (Jiang et al., 2020; Ho & Vas-
concelos, 2020; Kim et al., 2020) have carried out UAT with the adversarial InfoNCE loss, which
works well but lacks theoretical justification. Our unified CEM framework offers a principled way
to generalize adversarial training from supervised to unsupervised scenarios.
Maximization Process. Sampling frompθ(X) can be more difficult than that in supervised scenarios
because it does not admit a closed form for variable X0 . Thus, we perform Langevin dynamics with
K negative samples {Xk} drawn fromPd(X0),
Xn+1 = Xn + αVXn logPθ (Xn) + √2α ∙ ε
(19)
≈ Xn + αVXn
=Xn + αVXn
1、八、	__
log K fpe(Xn, Xk) + √2α ∙ ε
k=1
K
log X eχp(fθ(Xn, Xk)) + √2α ∙ ε.
k=1
While the PGD attack of the InfoNCE loss (Eq. 31),
Xn+1
Xn + αVXn log
eχp(fθ (Xn, X0))
Pk=I eχP(fθ(xn, Xk ))
(20)
K
=Xn + αV^n log X eχp(fθ (Xn, X k)) — αVθ fθ (Xn, x0),
k=1
6
Published as a conference paper at ICLR 2022
resembles the Langevin dynamics as they both share the positive logsumexp gradient that pushes
UP pθ(X), and differs by a repulse negative gradient -fθ(X, x0) away from the anchor x0, which is
a direct analogy of the PGD attack in supervised learning (Section 4.1). Therefore, we believe that
the PGD attack of InfoNCE is a proper way to craft adversarial examples by sampling from pθ (x).
Minimization Process. Following the same routine in Section 4.2, with the adversarial example
X 〜pθ(X), we can insert an interpolated adversarial pair (X, x0) and decompose the gradient of
NP-CEM into the consistency gradient and the contrastive gradient,
VθEpd(x,χ0) logPθ(x, X0) = Epd(x,χ0) NPθ(^,χo) [Vθfθ(x, x0)-Vθfθ(X, X0)]
=Epd(X,χ0) NPθ(x,χo) [ Vθfθ(x, x0)-Vθfθ(X, X0) + Vθfθ(X, x0)-Vθfθ(X, X0)].
X--------------{z--------------} X----------------{z-------------}
consistency gradient	contrastive gradient
(21)
In this way, we can directly develop the unsupervised analogy of AT loss and regularization
(Sec. 4.2). Following Eq. 30, it is easy to see that the contrastive gradient is equivalent to the gradi-
ent of the Adversarial InfoNCE loss utilized in previous work (Jiang et al., 2020; Ho & Vasconcelos,
2020; Kim et al., 2020) with adversarial example X,
Epd(χ,χ0)Npθ(x,χo) [Vθfθ(X,x0)-Vθfθ(X, X0)]
=Epd(X,χ0) Npθ(X) vθfθ(X,x) - Epθ(x0)pθ(X) Pe(X)) vθ f (X, x')
=Epd(X,x0) N pθ(X) vθ fθ (X, x') - Epθ(X0) E vθ fθ (x, W
≈ɪ X Vθ log	Kexρ(fθ(Xi,Xi)),
N i=ι	Pk=I eχp(fθ(Xi, Xik))
(22)
where {Xik} denotes the adversarial negative samples frompθ(X0).
5.3	Proposed Unsupervised Adversarial Sampling
In supervised learning, a natural method to draw a sample X from a robust classifier is to maximize
its conditional probability w.r.t. a given class y 〜 pd(y), i.e., maxχ p(y∣X), by targeted attack
(Santurkar et al., 2019). However, in the unsupervised scenarios, we do not have access to labels,
and this approach is not applicable anymore. Meanwhile, Langevin dynamics is also not directly
applicable (Eq. 19) because it requires access to real data samples.
MaxEnt. Nevertheless, we still find an effective algorithm for drawing samples from an unsuper-
vised robust model. We first initialize a batch of N samples B = {Xi}iN=1 from a prior distribution
p0 (X) (e.g., Gaussian). Next, we update the batch jointly by maximizing the empirical estimate of
entropy, where we simply take the generated samples at B = {Xi}iN=1 as samples from pθ(X)
1N	1N 1N
H(Pθ) = -Epθ(x) logPθ(x) ≈ -N ∑^Pθ(Xi) ≈ -N ∑^log N ∑^exp(fθM,Xj)) + logZ(θ). (23)
Specifically, we update each sample Xi by maximizing the empirical entropy (named MaxEnt)
N	N
Xi = Xi + αVχiH(pθ) + √2α ∙ ε ≈ Xi- aVχi ElogEexp(fθ(xi, Xj)) + √2α ∙ ε.	(24)
i=1	j=1
As a result, the generated samples {Xi}iN=1 are encouraged to distribute uniformly in the feature
space with maximum entropy, and thus cover the overall model distribution with diverse semantics.
6	Experiments
In this section, we evaluate the adversarial sampling methods derived from our CEM framework,
showing that they can bring significant improvement to the sample quality of previous work. Be-
sides, in Appendix A, we also conduct a range of experiments on adversarial robustness to verify
our probabilistic understandings of AT. We show adversarial training objectives derived from our
7
Published as a conference paper at ICLR 2022
Table 1: Inception Scores (IS) and Frechet Inception Distance (FID) of different generative models.
Results marked with ? are taken from Shmelkov et al. (2018).
Method	IS (↑)	FID Q)
Auto-regressive		
PiXelCNN++? (Salimans et al., 2017)	5.36	119.5
GAN-based		
DCGAN? (Radford et al., 2016)	6.69	35.6
WGAN-GP (Gulrajani et al., 2017)	7.86	36.4
PresGAN (Dieng et al., 2019)	-	52.2
StyleGAN2-ADA (Karras et al., 2020)	10.02	-
Score-based		
NCSN (Song & Ermon, 2019)	8.87	25.32
DDPM (Ho et al., 2020)	9.46	3.17
NCSN++ (Song et al., 2020)	9.89	2.20
EBM-based		
JEM (Grathwohl et al., 2019)	8.76	38.4
DRL (Gao et al., 2021)	8.58	9.60
AT-based		
TA (Santurkar et al., 2019) (w/ ResNet50)	7.5	-
Supervised CEM (w/ ResNet50)	9.80	55.91
Unsupervised CEM (w/ ResNet18) (ours)	8.68	36.4
Unsupervised CEM (w/ ResNet50) (ours)	9.61	40.25
CEM can indeed significantly improve the performance of AT in both supervised and unsupervised
scenarios, which helps justify our interpretations and our framework.
Models. For supervised robust models, we adopt the same pretrained ResNet50 checkpoint2 on
CIFAR-10 as Santurkar et al. (2019) for a fair comparison. The model is adversarially trained with
'2-norm PGD attack with random start, maximal perturbation norm 0.5, step size 0.1 and 7 steps. As
for the unsupervised case, we are the first to consider sampling from unsupervised robust models.
We train ResNet18 and ResNet50 (He et al., 2016) following the setup ofan existing unsupervised
adversarial training method ACL (Jiang et al., 2020). The training attack is kept the same as that of
the supervised case for a fair comparison. More details are provided in Appendix.
Sampling protocol. In practice, our adversarial sampling methods take the following general form
as a mixture of the PGD and Langevin dynamics,
Xn+1 = ∏kχn-χ0k2≤β [xn + αgk + ηεk ], X0 = δ, εk 〜N (x0ero, 1),k = 0,1,...,K,
where gk is the update gradient, εk is the diffusion noise, ΠS is the projector operator, and δ is the
(conditional) initial seeds drawn from the multivariate normal distribution whose mean and covari-
ance are calculated from the CIFAR-10 test set following Santurkar et al. (2019). We evaluate the
sample quality quantitatively with Inception Score (IS) (Salimans et al., 2016) and FreChet Inception
Distance (FID) (Heusel et al., 2017). More details can be found in in Appendix C.1.
Comparison with other generative models. In Table 1, we compare the sample quality of adver-
sarial sampling methods with different kinds of generative models. We analyze the results in terms
of the following aspects:
•	Our adversarial sampling methods outperform many deep generative models like Pixel-
CNN++, WGAN-GP and PresGAN, and obtain state-of-the-art Inception scores on par
with StyleGAN2 (Karras et al., 2020).
•	Comparing our AT-based methods with previous methods for training EBMs (Grathwohl
et al., 2019; Gao et al., 2021), we see that it obtains state-of-the-art Inception scores among
2We download the checkpoint from the repository https://github.com/MadryLab/
robustness_applications.
8
Published as a conference paper at ICLR 2022
the EBM-based methods. Remarkably, our unsupervised CEM with ResNet18 obtains
both better IS and FID scores than the original JEM, which adopts WideReSNet-28-10
(ZagorUyko & Komodakis, 2016) with even more parameters.
•	Compared with previous AT-based method (Santurkar et al., 2019), our CEM-based meth-
ods also improve IS by a large margin (even with the unsupervised ResNet18). Remarkably,
the supervised and unsupervised methods obtain similar sample quality, and the supervised
methods are better (higher) at IS while the unsupervised methods are better (lower) at FID.
•	We obtain similar Inception scores as state-of-the-art score-based models like NCSN++,
while failto match their FID scores. Nevertheless, a significant drawback of these methods
is that they typically require more than 1,000 steps to draw a sample, while ours only
require less than 50 steps.
Table 2: Inception Scores (IS) and FreChet Incep-
tion Distance (FID) of different sampling methods
for adversarially robust models. Cond: conditional.
Uncond: unconditional.
Training	Sampling	Method	IS (↑)	FID Q)
		TA	9.26	56.72
Supervised	Cond	Langevin CS	9.65 9.77	63.34 56.26
		RCS	9.80	55.91
	Uncond	PGD	535	74.27
Unsupervised		MaxEnt	8.24	41.80
(w/ ResNet18)	Cond	PGD	5.85	68.54
		MaxEnt	8.68	36.44
Unsupervised	Uncond	PGD MaxEnt	524 9.57	141.54 44.86
(w/ ResNet50)	Cond	PGD	5.37	137.68
		MaxEnt	9.61	40.25
Seed	I 7,T &"Jk、-H*
TA	肉套，、爵抬萼郡■霞制
LangeVin	
CS	睇福？电四离
	
PGD MaXEnt	日等八r铮£歌底底芯 “口能•、牌糕•二阿，
PGD MaxEnt	
Figure 1	: Four groups of random sam-
ples (top to bottom): initial, supervised
(ReSNet50), unsupervised (ReSNet18), unsu-
pervised (ReSNet50).
Comparison among adversarial sampling methods. In Table 2, we further compare the sample
quality of different adversarial sampling methods discussed in Sections 4.3 & 5.3. For supervised
models, we can see that indeed TA obtains the lowest IS, while CS can significantly refine the sample
quality, and RCS can further improve the sample quality by the injected bias. For unsupervised
models, we can see that MaxEnt outperforms PGD consistently by a large margin. In particular,
conditional sampling initialized with class-wise noise can improve a little on the sample quality
compared to unconditional sampling. The average visual sample quality in Figure 1 is roughly
consistent with the quantitative results.
The mismatch between IS and FID. A notable issue of adversarial sampling methods is the mis-
match between the IS and FID scores. For example, in Table 1, DDPM and our unsupervised CEM
(w/ ResNet50) have similar Inception scores, but the FID of DDPM (2.20) is significantly smaller
than ours (40.25), a phenomenon also widely observed in previous methods (Santurkar et al., 2019;
Grathwohl et al., 2019; Song & Ermon, 2019). Through a visual inspection of the samples in Figure
1, we can see that the samples have realistic global structure, but as for the local structure, we can
find some common artifacts, which could be the reason why it has a relatively large distance (FID)
to the real data.
7 Conclusion
In this paper, we proposed a unified probabilistic framework, named Contrastive Energy-based
Model (CEM), which not only explains the generative ability of adversarial training, but also pro-
vides a unified perspective of adversarial training and sampling in both supervised and unsupervised
paradigms. Extensive experiments show that adversarial sampling methods derived from our frame-
work indeed demonstrate better sample quality than state-of-the-art methods.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Yisen Wang is partially supported by the National Natural Science Foundation of China under Grant
62006153, Project 2020BD006 supported by PKU-Baidu Fund, and Open Research Projects of
Zhejiang Lab (No. 2022RC0AB05). Jiansheng Yang is supported by the National Science Founda-
tion of China under Grant No. 11961141007. Zhouchen Lin is supported by the NSF China (No.
61731018), NSFC Tianyuan Fund for Mathematics (No. 12026606), Project 2020BD006 supported
by PKU-Baidu Fund, and Qualcomm.
References
Yang Bai, Xin Yan, Yong Jiang, Shu-Tao Xia, and Yisen Wang. Clustering effect of adversarial
robust models. In NeurIPS, 2021.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. ICML, 2020.
Adji B Dieng, Francisco JR Ruiz, David M Blei, and Michalis K Titsias. Prescribed generative
adversarial networks. arXiv preprint arXiv:1910.04302, 2019.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019.
Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based
models by diffusion recovery likelihood. ICLR, 2021.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015.
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. In ICLR, 2019.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of Wasserstein GANs. NeurIPS, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. NeurIPS,
2017.
Chih-Hui Ho and Nuno Vasconcelos. Contrastive learning with adversarial examples. NeurIPS,
2020.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS,
2020.
Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial
contrastive learning. NeurIPS, 2020.
Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard
negative mixing for contrastive learning. NeurIPS, 2020.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
10
Published as a conference paper at ICLR 2022
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. NeurIPS, 2020.
Simran Kaur, Jeremy Cohen, and Zachary C Lipton. Are perceptually-aligned gradients a general
property of robust classifiers? arXiv preprint arXiv:1910.08640, 2019.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning.
NeurIPS, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR, 2014.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On varia-
tional bounds of mutual information. ICML, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. ICLR, 2016.
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. ICLR, 2021.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. NeurIPS, 2016.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the
PixelCNN with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017.
Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Image synthesis with a single (robust) classifier. In NeurIPS, 2019.
Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my GAN? ECCV, 2018.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456, 2020.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. ICLR, 2020.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. ICML, 2019.
11
Published as a conference paper at ICLR 2022
A	Evaluating Adversarial Robustness
In Section 6, we have shown that the new adversarial sampling algorithms derived from our CEM
framework indeed obtain improved sample quality, which helps justifies our interpretation of AT
from a generative aspect. In this section, we further take a complementary way to verify our inter-
pretation by studying its discriminative part. In particular, we develop new variants of AT regular-
ization and verify their effectiveness on improving the adversarial robustness of AT. As CEM has
both supervised and unsupervised variants, we develop AT variants for each scenario respectively
while following the same spirit.
A. 1 Supervised Adversarial Training
A.1.1 Proposed Method
In Section 4.2, we have mentioned that for the consistency gradient, original AT (Madry et al., 2018)
simply ignores it. Denoting X and X as the natural and adversarial inputs, the state-of-the-art AT
variant, TRADES (Zhang et al., 2019), instead adopts the KL regularization KL(p(∙∣X) ∣∣p(∙∣x)) that
explicitly regularizes the consistency of the predicted probabilities on all classes, whose optimum
implies thatp(∙∣X) = p(∙∣x) → fθ(x,y) = fθ(X,y).
Inspired by this discussion, alternatively, we can directly regularize the consistency gradient to zero.
We achieve this with the following Consistency Regularization (CR) with least square loss:
LCR(θ) = Epd(X,y) N Pθ (x) (fθ(x,y) - fθ (x,y))2 .	(25)
We note the our proposed AT+CR differs to ALP (Kannan et al., 2018) as we only minimizes the
gap between the logits of the label class fθ(x, y). ALP was shown to be ineffective for improving
AT, while in the experiments below, we show that our AT+CR objective indeed achieves comparable
(even slightly better) results as TRADES.
A.1.2 Empirical Evaluation
Experimental setup. Following the conventions, we compare different AT methods with Pre-
activated ResNet18 (He et al., 2016) and WideResNet34 (Zagoruyko & Komodakis, 2016) on
CIFAR-10. The maximum perturbation is bounded by ε = 8/255 under '∞ norm. The training
attack is PGD10 (Madry et al., 2018) with random start and step size ε∕4. The test attack is PGD20
with random start and step size ε∕4. We evaluate both the final epoch and the early stopped epoch
with the best robust accuracy.
Table 3: Robustness (accuracy (%) on adversarial attacks) of supervised adversarial training methods
on CIFAR-10. R18: ResNet18. W34: WideResNet34.
Model	Training	Natural Acc (%)	Adversarial Acc (%)
	Ar(Madry etal.,2018)	83.7	52.2
ResNet18	TRADES (Zhang et al., 2019)	82.5	54.3
	AT+CR (ours)	81.5	55.2
	AT(Madry etal.,2018)	86.8	53.6
WideResNet34	TRADES (Zhang et al., 2019)	83.4	57.0
	AT+CR (ours)	86.6	57.0
Result analysis. From Table 3, we can see that the AT+CR objective derived from our framework
indeed enjoy much better robustness than vanilla AT. Compared to the state-of-the-art AT vari-
ant TRADES, we can see that AT+CR is comparable, and sometimes slightly better at robustness.
When the two have similar robustness (for WideResNet34), AT+CR obtains better natural accuracy
than TRADES (86.6 v.s. 83.4). These results empirically justify that our interpretation of AT and
TRADES from a probabilistic perspective.
A.2 Unsupervised Adversarial Training
Similarly, we can develop the same regularization technique for unsupervised adversarial training
through a unified perspective of supervised and unsupervised AT offered by our CEM.
12
Published as a conference paper at ICLR 2022
A.2.1 Proposed Method
In Section 5.2, we have developed a principled unsupervised adversarial training routine by an anal-
ogy with the supervised AT (Section 4.2). Besides, we can also consider an unsupervised version of
the consistency regularization above, namely, the Unsupervised Consistency Regularization (UCR),
Lucr(Θ) = Epd(x,x，)N pθ(x)kfθ (x, x0) — fθ (X, x0)k2,	(26)
which encourages the consistency between the similarity between the natural pair (x, x0) and the
adversarial pair (X, x0).
A.2.2 Empirical Evaluation
Experimental setup. Among the many variants of contrastive learning, we adopt SimCLR as our
baseline method and take the recently proposed ACL (Jiang et al., 2020) as our Unsupervised Ad-
versarial Training (UAT) following the same default setup as in Section C.1. The training attack
configuration is kept the same as the supervised case, while after training, we freeze the encoder and
fine-tune a linear classification layer (standard training) on top with labeled data for evaluating the
learned features. In particular, we evaluate the composed model on the test data with two different
attack methods, FGSM (Goodfellow et al., 2015) (one-step attack) and PGD20 (multi-step attack),
both with ε = 8/255, and report their natural and adversarial accuracy, respectively.
Table 4: Robustness (accuracy (%) on adversarial attacks) of unsupervised contrastive learning
methods on CIFAR-10 with ResNet-18 backbone and two different attack methods: FGSM (Good-
fellow et al., 2015) and PGD (Madry et al., 2018).
Training	Natural Acc (%)	Adversarial Acc (%) FGSM PGD20	
Standard Training (Chen et al., 2020)	91.5	25.6	0.8
UAT (Jiang et al., 2020)	66.6	26.2	21.4
UAT+UCR (ours)	72.0	30.7	24.6
Result analysis. As shown in Table 4, features learned by UAT is indeed more robust than standard
training, e.g., 0.8% to 21.4% under PGD attack. Moreover, with our proposed UCR regularizer
(Eq. 26), we not only effectively improve both natural accuracy (66.6% to 72.0%), and also improve
adversarial robustness: 26.2% to 30.7% under FGSM attack and 21.4% to 24.6% under PGD attack.
This also helps justify the connection between contrastive learning and our CEM.
A.3 Concluding Remark
With the above experiments on adversarial robustness, we empirically verify that our framework
could serve as a valid probabilistic understanding of adversarial training and can be used to develop
new effective adversarial training objectives.
B	Omitted Technical Details
For a concise presentation, we have omitted several technical details in the main text. Here we
present a complete description of the derivation process.
B.1	Log Likelihood Gradient of EBM
In Section 3, we have introduced Energy-based Models (EBM) and the gradient of their log likeli-
hood. We now show how it can be derived out.
For a EBM in the following form,
exp (-Eθ(x))
Z(θ)
pθ(x)
(27)
13
Published as a conference paper at ICLR 2022
the gradient of the log likelihood can be derived as follows:
VθEpd(X) logPθ(x)
=-Epd(X) VeEθ(X)- Relog Z(θ)
Ve R exρ(-Ee(x))
=Epd(X)VeEe(x) +	θ Jx Z('θ) 外〃
=-Epd(X)VeEe(x)+ Z exp,-E；(X)) VeEe(X)
Jx	Z(θ)
=-Epd(X)VeEe(x)+ Epθ(X)VeEe(X).
、-----{z-----} 、-----V-----}
positive phase	negative phase
(28)
B.2	Connection between Standard Training and JEM
In Section 4.4, We have claimed that if We replace the model distribution Pe (X) With the data distri-
bution pd(x) in Eqn. 10, the log likelihood gradient of JEM is equivalent to the negative gradient of
the CE loss. Here We give a detailed proof as folloWs:
VeEpd(X,y) log Pe (x, y)
=Epd(X,y)Ve fe(x,y)-Epθ(^)pθ(y∣^) Vefe(x, y)
≈Epd(X,y)Vefe(x,y) - Epd(x)pθ(y∣X)Vefe(x,y)
=Epd(X,y)
=Epd(X,y)
=Epd(X,y)
=Epd(X,y)
=Epd(X,y)
=Epd(X,y)
[Vefe(x,y) - Epθ(y∣X)Ve fe(x,y)]
K
Vefe(x,y) - XPe(k|x)Vefe(x, k)
k=1
K
v7 , e e	VeXp(fe(x,k))Vefe(x,k)
Ve fe(x,啦-工 qκ--------7 / »
k=1	j=1 exp(fe(x, j))
K
Vefe(x,y) - χP⅛χp≡⅛
k=1 j=1 exp(fe (x, j))
"v；fe(x,y) -VeP3exp(fe(XM#
e	PK=1 exp(fe (x,j)) J
K
Vefe(x,y) - VelogXexp(fe(x, k))
k=1
exp(fe(X,y))
=Epd(X,y) Ve log SK------
k=1 exp(fe (X, k)
=VeEpd(X,y) logpe(y|X).
(29)
B.3 Equivalence between AT Loss and Contrastive Gradient in Supervised
Learning
In Section 4.3, we have claimed that the contrastive gradient equals to the negative gradient of the
robust CE loss (AT loss) following the same deduction in Eqn. 29,
Epd(X,y)Npθ(X,y) [Vefe(x,y)-Vefe(x,y)]
=Epd(X,y) N pθ (X)
=Epd(X,y)Ve log
[Vefe(x,y) - Epθ(y∣X)Ve fe(x,y)]
exp(fe (X ,y))
PK=I exp(fe(x ,k)
=Epd(X,y) N pθ (X)Ve log Pe (y|x),
which is exactly the negative gradient of the canonical AT loss (Madry et al., 2018).
(30)
14
Published as a conference paper at ICLR 2022
B.4	Equivalence between InfoNCE Loss and Non-parametric CEM
In Section 6.1, we have claimed that the the log likelihood gradient of NP-CEM equals to exactly
the negative gradient of the InfoNCE loss when We approximate pθ(X) with Pd(X). The derivation
is presented as follows:
Epd(χ,χ0)Vθfθ(x, x0) - Epθ(χ,χo)Vθfθ (X, x0)
=Epd(χ,χ0)Vθfθ(x,x0) — Epθ(χ)Pθ(X∣X0)Vθfθ (X,X0)
=Epd(X,χ0)Vθfθ(x, x0) — Epθ(x) Pp(XxxPVθfθ (X, X0)
= Epd(X,X0)vθ fθ (X, x0 ) -Epθ (X)Z Rxixppffθ^XX)) vθ fθ (X, XO)
=Epd(X.S fθ(% x0) -Epθ (X)JX RXpdXXexppffθ^XX)) vθ fθ (x,XO) (as pd(x)=pd (X)=⅛)
=Epd(X,xO)Vθ fθ (x, x0) - Epθ(X)Pd(XO) Epdixp ex%;；Xx),K vθ fθ (x, XO)
≈Epd(X，XO)Vθ fθ (x, x0) - Epd(X)Pd(XO) Epdixp exp(fXxl,)x)) vθ fθ (x, XO)
(31)
=Epd(X,χ0)Vθfθ(x, X0) — Epd(X)Vθ log Epd(χo) exρ(fθ(X, X0))
=Epd(X,χ0) [Vθfθ(x, x0) — Vθ log Epd(X0)exρ(fθ(X, X0))] (merge pdg with pd(X))
—旧	v7	exP(fθ(X,XO))	~ 1 XNyj	exP(fθ(Xi,Xi))
=Epd(X，X0)V。log Epd(^o) exρ(fθ(x, X,)) ≈ N i=l vθ log P屋 exp(f。g, Xik)),
where (x, Xiys are positive samples from Pd(x, x0) and Xik's are negative samples independently
drawn from Pd(X0).
B.5	Equivalence between Adversarial InfoNCE and NP-CEM
In Section 6.2, we have developed the unsupervised analogy of AT loss and regularization. In
particular, we have claimed that contrastive gradient is equivalent to the gradient of the Adversarial
InfoNCE loss (i.e., the InfoNCE loss of the adversarial example X) utilized in previous work (Jiang
et al., 2020; Ho & Vasconcelos, 2020; Kim et al., 2020). It can be derived following Eqn. 31:
Epd(X,xO)Npθ(X,^0) [Vθfθ(X,xo)-Vθfθ(X,Xo)]
=Epd(X,X0)Npθ(X) [Vθfθ(x,x0) — Epθ(χo∣χ)Vθfθ(X,X0)]
=Epd(χ,χ0) N p。⑻ 卜；f； (x,XO) - Epd(χ0) Epdex)pefp (f； x⅛)) v； f； (x,XO)
=Epd(χ,χ0) N pθ(X)V； log
exp(f； (X, x0))
Epd(χo) exp(f；(X, X0)
1 Gv7 ]	exp(f；(Xi,Xi))
N i=1 θ θg P=I exp(fθ(xi, Xik))
(32)
where (x^ Xi) are positive samples drawn fromPd(x, x0), Xi's are adversarial samples drawn from
Pθ(X), and XiJs are negative samples independently drawn from Pd(X0).
C More Details and Results on Adversarial S ampling
C.1 Detailed Experimental Setup
Supervised Adversarial Sampling. For supervised robust models, we adopt the same pretrained
ResNet50 checkpoint on CIFAR-10 as Santurkar et al. (2019) 3 for a fair comparison. As described
3We download the checkpoint from the repository https://github.com/MadryLab/
robustness_applications.
15
Published as a conference paper at ICLR 2022
10	20	30	40	50
Numberofeampllng steps
aj∞β U6d8u_
9.5
9.0
aj∞s U6d8u-
2.0
1-5
1-0
0
0X01	0-01	0-1
Nolee ratio
1.0	1	23456789	10
Maximal ⅛ norm
Figure 2: Algorithmic analysis of our proposed supervised adversarial sampling algorithm (RCS).
Left: Inception score with increasing sampling steps N . Middle: Inception score with increasing
diffusion noise scale. Right: Inception score with increasing '2-norm bound β.
9.55
aj∞β UoAd∞u-
20	30	40	50	60	70	00	90	100
Numberofeampllng steps
aj∞s UOAdeOU-
2.0
1-5
1-0
O
7.0
0.001	0.01	0-1	1.0	3-7
Nolee ratio
65 /
6.0
7 S 9	10	11	12	13	14
Maximal ∕3 norm
Figure 3: Algorithmic analysis of our proposed unsupervised adversarial sampling algorithm (Max-
Ent). Left: Inception score with increasing sampling steps N . Middle: Inception score with increas-
ing diffusion noise scale. Right: Inception score with increasing '2-norm bound β.
in Santurkar et al. (2019), the ResNet-50 model is adversarially trained for 350 epochs with learning
rate 0.01 and batch size 256. The learning rate is dropped by 10 at epoch 150 and 200. The training
attack is '2-norm PGD attack with random start, maximal perturbation norm 0.5, step size 0.1 and 7
steps.
Unsupervised Adversarial Sampling. As far as we know, we are the first to consider sampling
from unsupervised robust models. Therefore, to obtain an unsupervised robust model for sampling,
we adopt ACL (Jiang et al., 2020) as the baseline method and follow their default hyper-parameters
4 to train it. The official implementation of ACL is built upon the SimCLR (Chen et al., 2020)
framework for contrastive learning, while they choose a specific hyper-parameter configuration for
adversarial training. In particular, they choose 512 for batch size and train for 1000 epochs with
ResNet-18 backbone (He et al., 2016). The base learning rate is 1.0, where they use a linear warm
up strategy for the first 10 epochs, and apply cosine annealing scheduler after that. The training
attack is kept the same as that of the supervised case for a fair comparison. For ResNet-18, we
adopt the ACL(A2A) setting, which adopts the normal ResNet with only one Batch Normalization
module. Instead, for ResNet-50, we notice that the ACL(A2S) setting yields slightly better results.
The ResNet variants in the ACL(A2S) setting contains two BN modules, where we assign natural
and adversarial examples to different modules. We refer more details to the original paper (Jiang
et al., 2020).
Evaluation of sample quality. Note that there are four hyper-parameters in our sampling protocol:
step size α, '2-ball size β, noise scale η, and iteration steps K, for which we list our choice in Table
5. We evaluate sample quality quantitatively w.r.t. the Inception Score (IS) (Salimans et al., 2016)
and Frechet Inception Distance (FID) (HeUSeI et al., 2017) with 50,000 samples, where the standard
variation ofIS is around 0.1.
C.2 Additional Analysis
Besides the results presented in the main text, we also conduct more experiments to analyze the
behavior of our proposed adversarial sampling algorithms, both quantitatively and qualitatively. We
conduct a detailed analysis of our proposed sampling algorithms and present the results of supervised
4Official code: https://github.com/VITA- Group/Adversarial- Contrastive- Learning.
16
Published as a conference paper at ICLR 2022
Table 5: Sampling hyper-parameters in each scenario.
Scenario	Model	α	β	η	K
Supervised	ResNet50	1	6	0.01	20
Unsupervised	ResNet18	~T~	∞	0.0	10
	ResNet50	7	∞	0.0	50
adversarial sampling (with RCS) in Figure 2 and the results of unsupervised adversarial sampling
(with MaxEnt) in Figure 3. Note that in both cases, we adopt the ResNet-50 backbone and use the
default hyper-parameters unless specified.
C.2.1 Chain Length
From the left panels of Figure 2 and Figure 3, we can see the two adversarial algorithms both have
a sweet spot of sampling steps N (length of the sampling Markov chains) at around 30 to 40 steps,
before and after which the results will be slightly worse.
C.2.2 Noise ratio
In the proper Langevin dynamics, the scale of the noise is determined by the step size, η = √2α.
However, in practice, this would lead to a catastrophically degraded sample quality as the noise
takes over the gradient information. Therefore, following Song & Ermon (2019) and Grathwohl
et al. (2019), we also anneal the noise ratio η to a smaller value for better sample quality. As shown
in the middle panels of Figure 2 and Figure 3, the optimal noise ratio is around 0.01 for both cases.
C.2.3 Maximal Norm
An apparent difference of our adversarial sampling algorithms to the canonical Langevin dynamics
is that ours have a projection step that limits the distance between the samples and the initial seeds.
In the right panels of Figure 2 and Figure 3, We show the impact of the scale of the '2-norm ball
for the sample quality. We can see that generally speaking, as the ball grows larger, the samples get
refined. In the supervised case, the sample quality gets slightly worse with a large norm, which does
not happen in the unsupervised case.
C.2.4 Sampling Trajectory
Aside from the qualitative inspection of the proposed sampling algorithms, we also demonstrate
the sampling trajectory of our supervised (RCS) and unsupervised (MaxEnt) adversarial sampling
methods in Figure 4 & 5. We can see that the samples get gradually refined in terms both low-level
textures and high-level semantics.
17
Published as a conference paper at ICLR 2022
Figure 4: Sampling trajectory (the first 20 steps) of our proposed supervised adversarial sampling
algorithm (RCS). Each row represents the refinement progress of a single sample.
Figure 5: Sampling trajectory (the first 20 steps) of our proposed unsupervised adversarial sampling
algorithm (MaxEnt). Each row represents the refinement progress of a single sample.
18