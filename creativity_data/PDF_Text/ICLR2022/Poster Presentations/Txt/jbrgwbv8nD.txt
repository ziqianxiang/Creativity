Published as a conference paper at ICLR 2022
Constraining Linear-chain CRFs to Regular
Languages
Sean Papay, Roman Klinger; & Sebastian Pado
University of Stuttgart
(sean.papay|klinger|pado)@ims.uni-stuttgart.de
Ab stract
A major challenge in structured prediction is to represent the interdependencies
within output structures. When outputs are structured as sequences, linear-chain
conditional random fields (CRFs) are a widely used model class which can learn
local dependencies in the output. However, the CRF’s Markov assumption makes
it impossible for CRFs to represent distributions with nonlocal dependencies, and
standard CRFs are unable to respect nonlocal constraints of the data (such as
global arity constraints on output labels). We present a generalization of CRFs
that can enforce a broad class of constraints, including nonlocal ones, by specify-
ing the space of possible output structures as a regular language L. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in L. Notably, RegC-
CRFs can incorporate their constraints during training, while related models only
enforce constraints during decoding. We prove that constrained training is never
worse than constrained decoding, and show empirically that it can be substantially
better in practice. Finally, we demonstrate a practical benefit on downstream tasks
by incorporating a RegCCRF into a deep neural model for semantic role labeling,
exceeding state-of-the-art results on a standard dataset.
1	Introduction
Structured prediction is a field of machine learning where outputs are expected to obey some pre-
defined discrete structure. Instances of structured prediction with various output structures occur in
many applications, including computer vision (e.g., scene graph generation (Johnson et al., 2015)
with graph-structured output), natural language processing (e.g., linguistic parsing (Niculae et al.,
2018) with tree-structured output, relation extraction (Roth & Yih, 2004) with tuple-structured out-
put) or modeling the spatial structure of physical entities and processes (Jiang, 2020).
A key difficulty faced by all models is to tractably model interdependencies between different parts
of the output. As output spaces tend to be combinatorially large, special techniques, approximations,
and independence assumptions must be used to work with the associated probability distributions.
Considerable research has investigated specific structures for which such approaches make machine
learning tractable. For instance, when outputs are trees over a fixed set of nodes, maximal arbores-
cence algorithms allow for exact inference (Chu, 1965; Edmonds, 1967); when outputs are graph-
structured, loopy belief propagation can provide approximate inference (Murphy et al., 1999).
If the output forms a linear sequence, a Markov assumption is often made: model outputs depend
only on their immediate neighbors, but not (directly) on more distant ones. A common model that
uses this assumption is the linear-chain conditional random field (CRF) (Lafferty et al., 2001), which
has found ubiquitous use for sequence labeling tasks, including part-of-speech tagging (Gimpel
et al., 2011) and named entity recognition (Lample et al., 2016). This model became popular with
the use of hand-crafted feature vectors in the 2000s, and is nowadays commonly used as an output
layer in neural networks to encourage the learning of structural properties of the output sequence.
The Markov assumption makes training tractable, but also limits the CRF’s expressive power, which
can hamper performance, especially for long sequences (Scheible et al., 2016). Semi-Markov CRFs
(Sarawagi & Cohen, 2004) and skip-chain CRFs (Sutton & McCallum, 2004) are techniques for
relaxing the Markov assumption, but both come with drawbacks in performance and expressiveness.
1
Published as a conference paper at ICLR 2022
In this work, we propose a new method to tractably relax the Markov assumption in CRFs. Specif-
ically, we show how to constrain the output of a CRF to a regular language, such that the resulting
regular-constrained CRF (RegCCRF) is guaranteed to output label sequences from that language.
Since regular languages can encode long-distance dependencies between the symbols in their strings,
RegCCRFs provide a simple model for structured prediction guaranteed to respect these constraints.
The domain knowledge specifying these constraints is defined via regular expressions, a straightfor-
ward, well understood formalism. We show that our method is distinct from approaches that enforce
constraints at decoding time, and that it better approximates the true data distribution. We evaluate
our model empirically as the output layer of a neural network and attain state-of-the-art performance
for semantic role labeling (Weischedel et al., 2011; Pradhan et al., 2012). Our PyTorch RegCCRF
implementation can be used as a drop-in replacement for standard CRFs.
2	Related work
We identify three areas of structured prediction that are relevant to our work: constrained decod-
ing, which can enforce output constraints at decoding time, techniques for weakening the Markov
assumption of CRFs to learn long-distance dependencies, and weight-learning in finite-state trans-
ducers.
Constrained decoding. A common approach to enforcing constraints in model output is con-
strained decoding: Models are trained in a standard fashion, and decoding ensures that the model
output satisfies the constraints. This almost always corresponds to finding or approximating a ver-
sion of the model’s distribution conditionalized on the output obeying the specified constraints. This
approach is useful if constraints are not available at training time, such as in the interactive infor-
mation extraction task of Kristjansson et al. (2004). They present constrained conditional random
fields, which can enforce that particular tokens are or are not assigned particular labels (positive and
negative constraints, respectively). Formally, our work is a strict generalization of this approach, as
position-wise constraints can be formulated as a regular language, but regular languages go beyond
position-wise constraints. Other studies treat decoding with constraints as a search problem, search-
ing for the most probable decoding path which satisfies all constraints. For example, He et al. (2017)
train a neural network to predict token-wise output probabilities for semantic role labeling following
the BIO label-alphabet (Ramshaw & Marcus, 1999), and then use exact A* search to ensure that the
output forms a valid BIO sequence and that particular task-specific constraints are satisfied. For au-
toregressive models, constrained beam search (Hokamp & Liu, 2017; Anderson et al., 2017; Hasler
et al., 2018) can enforce regular-language constraints during search. We further discuss constrained
decoding as it relates to RegCCRFs in Section 5.
Markov relaxations. While our approach can relax the Markov assumption of CRFs through non-
local hard constraints, another strand of work has developed models which can directly learn nonlo-
cal dependencies in CRFs: Semi-Markov CRFs (Sarawagi & Cohen, 2004) relax the Markov prop-
erty to the semi-Markov property. In this setting, CRFs are tasked with segmentation, where individ-
ual segments may depend only on their immediate neighbors, but model behavior within a particu-
lar segment need not be Markovian. As such, semi-Markov CRFs are capable of capturing nonlocal
dependencies between output variables, but only to a range of one segment and inside of a segment.
Skip-chain CRFs (Sutton & McCallum, 2004) change the expressiveness of CRFs by relaxing the as-
sumption that only the linear structure of the input matters: they add explicit dependencies between
distant nodes in an otherwise linear-chain CRF. These dependencies are picked based on particular
properties, e.g., input variables of the same value or which share other properties. In doing so, they
add loops to the model’s factor graph, which makes exact training and inference intractable, and
leads to the use of approximation techniques such as loopy belief propagation and Gibbs sampling.
Weight learning for finite-state transducers. While our approach focuses on the task of con-
straining the CRF distribution to a known regular language, a related task is that of learning a
weighted regular language from data. This task is usually formalized as learning the weights of a
weighted finite-state transducer (FST), as in e.g. Eisner (2002) with directly parameterized weights
and Rastogi et al. (2016) with weights parameterized by a neural network. Despite the difference
in task-setting, this task is quite similar to ours in the formal sense, and in fact our proposal can be
2
Published as a conference paper at ICLR 2022
viewed as a particularly well-behaved special case of FST weight learning for an appropriately cho-
sen transducer architecture and parameterization. We discuss this connection further in Section 4.3.
3	Preliminaries and notation
As our construction involves finite-state automata and conditional random fields, we define these
here and specify the notation we will use in the remainder of this work.
Finite-state automata. All automata are taken to be nondeterministic finite-state automata (NFAs)
without epsilon transitions. Let such an NFA be defined as a 5-tuple (Σ, Q, q1, F, E), where
Σ = {a1,a2,…，a∣∑∣} is a finite alphabet of symbols, Q = {qι,(邕,…,q∣Q∣} is a finite set of states,
q1 ∈ Q is the sole starting state, F ⊆ Q is a set of accepting states, and E ⊆ Q × Σ × Q is a
set of directed, symbol-labeled edges between states. The edges define the NFA’s transition func-
tion △: Q X Σ → 2q, with r ∈ ∆(q, a)什(q, a, r) ∈ E. An automaton is said to accept a string
S ∈ Σ* iff there exists a contiguous path of edges from qι to some accepting state whose edge labels
are exactly the symbols of s. The language defined by an automaton is the set of all such accepted
strings. A language is regular if and only if it is the language of some NFA.
Linear-chain conditional random fields. Linear-chain conditional random fields (CRFs) Lafferty
et al. (2001) are a sequence labeling model. Parameterized by θ, they use a global exponential model
to represent the conditional distribution over label sequences y = hy1, y2, ..., yti conditioned on
input sequences X = hx1,x2,…,x∕: Pθ(y | x) α expPj fj(x, y), with individual observations
xi coming from some observation space X, and outputs yi coming from some finite alphabet Y . In
this work, we use CRFs for sequence labeling problems, but the dataset labels do not correspond
directly to the CRF’s outputs yi . In order to avoid ambiguity, and since the term “state” already has
a meaning for NFAs, we call y the CRF’s tag sequence, and each yi a tag. The terms label sequence
and label will thus unambiguously refer to the original dataset labels.
Each fθj is a potential function of x and y, parameterized by θ. Importantly, in a linear-chain CRF,
these potential functions are limited to two kinds: The transition function gθ(yi, yi+1) assigns a
potential to each pair (yi, yi+1) of adjacent tags in y, and the emission function hθ(yi | x, i) assigns
a potential to each possible output tag yi given the observation sequence x and its position i. With
these, the distribution defined by a CRF is
Pθ (y I χ) B exp fgθ (yi,yi+ι) + £h (χ,y ,i).	⑴
i=1	i=1
Limiting our potential functions in this way imposes a Markov assumption on our model, as potential
functions can only depend on a single tag or a single pair of adjacent tags. This makes learning and
inference tractable: the forward algorithm (Jurafsky & Martin, 2009) can calculate negative log-
likelihood (NLL) loss during training, and the Viterbi algorithm (Viterbi, 1967; Jurafsky & Martin,
2009) can be used for inference. Both are linear in t, and quadratic in |Y | in both time and space.
4	Regular-constrained CRFs
Given a regular language L, we would like to constrain a CRF to L. We formalize this notion
of constraint with conditional probabilities - a CRF constrained to L is described by a (further)
conditionalized version of that CRF’s distribution Pθ (y | x), conditioned on the event that the tag
sequence y is in L. We write this distribution as
Pθ (y I X, L) = ［； ∙ PP (y | x) ify ∈L	(2)
0	otherwise,
with α ≥ 1 defined as α-1 =	y∈LPθ(y I X).
In order to utilize this distribution for machine learning, we need to be able to compute NLL losses
and perform MAP inference. As discussed in Section 3, both of these are efficiently computable
for CRFs. Thus, if we can construct a separate CRF whose output distribution can be interpreted as
P(y I X, L), both of these operations will be available. We do this in the next section.
3
Published as a conference paper at ICLR 2022
Y0 = {(qι→qι), (qι→q2 , (q2→q2), (q2→q3), (q2→q4 , (q3→qι), (q3→q2), (q3→q3), (q4→q3), (q4→q4)}
Figure 1: Example for a RegCCRF, showing NFA and unrolled factor graph. L describes the lan-
guage (O | BI*O*BI*)*, the language of valid BIO sequences for an even number of spans. We
would like to calculate Pθ(y | x, L) for y = hB, O, B, Ii. We show an unambiguous automaton M
for L (left), and a factor graph (right) for the auxiliary CRF computing Pθ (y0 | x), where y0 ∈ Y0*
corresponds to the sole accepting path of y through M (marked).
4.1	Construction
Let M := (Σ, Q, F, E) be an NFA that describes L. We assume that M is unambiguous 一 i.e.,
every string in L is accepted by exactly one path through M . As every NFA can be transformed
into an equivalent unambiguous NFA (Mohri, 2012), this assumption involves no loss of generality.
Our plan is to represent Pθ(y | x, L) by constructing a separate CRF with a distinct tag set, whose
output sequences can be interpreted directly as paths through M. As M is unambiguous, each label
sequence in L corresponds to exactly one such path. We parameterize this auxiliary CRF identically
to our original CRF - that is, with label-wise (not tag-wise) transition and emission functions. Thus,
for all parameterizations θ, both distributions Pθ(y | x) and Pθ(y | x, L) are well defined.
There are many ways to construct such a CRF. As CRF training and inference are quadratic in the
size of the tag set Y, we would prefer a construction which minimizes |Y |. However, for clarity,
we will first present a conceptually simple construction, and discuss approaches to reduce |Y | in
Section 4.2. We start with our original CRF, parameterized by θ, with tag set Y = Σ, transition
function gθ, and emission function hθ, describing the probability distribution Pθ(y | x), y ∈ Σ*.
From this, we construct a new CRF, also parameterized by the same θ, but with tag set Y0, transition
function gθ0 , and emission function h0θ. This auxiliary CRF describes the distribution Pθ0 (y0 | x)
(with y0 ∈ Y0*), which we will be able to interpret as Pθ(y | x, L). The construction is as follows:
gθ0 ((q, a, r), (q0, a0, r0)) = g-θ∞(a,a0)
if r = q0
otherwise
-∞ ifi = 1,q 6= q1
h0θ (x, (q, a, r), i) =	-∞ if i = t, r 6∈ F
I hθ (x,a,i) otherwise.
(3)
(4)
(5)
This means that the tags of our new CRF are the edges of M, the transition function assigns zero
probability to transitions between edges which do not pass through a shared NFA state, and the
emission function assigns zero probability to tag sequences which do not begin at the starting state
or end at an accepting state. Apart from these constraints, the transition and emission functions
depend only on edge labels, and not on the edges themselves, and agree with the standard CRF’s gθ
and hθ when no constraints are violated.
As M is unambiguous, every tag sequence y corresponds to a single path through M, representable
as an edge sequence π = hπ1, π2, ..., πti, πi ∈ E. Since this path is a tag sequence for our auxiliary
CRF, we can directly calculate the auxiliary CRF’s Pθ0(π | x). From the construction of gθ0 and h0θ,
this must be equal to Pθ(y | x, L), as it is proportional to Pθ(y | x) for y ∈ L, and zero (or, more
correctly, undefined) otherwise. Figure 1 illustrates this construction with a concrete example.
4
Published as a conference paper at ICLR 2022
4.2	Time and space efficiency
As the Viterbi and forward algorithms are quadratic in |Y |, very large tag sets can lead to perfor-
mance issues, possibly making training or inference intractible in extreme cases. Thus, we would
like to characterize under which conditions a RegCCRF can be used tractibly, and identify tech-
niques for improving performance. As Y corresponds to the edges of M, we would like to select our
unambiguous automaton M to have as few edges as possible. For arbitrary languages, this problem
is NP-complete (Jiang & Ravikumar, 1991), and, assuming P 6= NP, is not even efficiently approx-
imable (Gruber & Holzer, 2007). Nonetheless, for many common classes of languages, there exist
approaches to obtain a tractably small automaton.
One straightforward method is to construct M directly from a short unambiguous regular expression.
Bruggemann-Klein & Wood (1992) present a simple algorithm for constructing an unambiguous
automaton from an unambiguous regular expression, with |Q| linear in the length of the expression.
Using this method to construct M, the time- and space-complexity of Viterbi are polynomial in
the length of our regular expression, with a worst-case of quartic complexity when the connectivity
graph of M is dense.
For many other tasks, a reasonable approach is to leverage domain knowledge about the constraints
to manually construct a small unambiguous automaton. For example, if the constraints require that
a particular label occurs exactly n times in the output sequence, an automaton could be constructed
manually to count ocurrences of that label. Multiple constraints of this type can then be composed
via automaton union and intersection.
Without making changes to M, we can also reduce the size of |Y | by adjusting our construction. In-
stead of making each edge of M a tag, we can adopt equivalence classes of edges. Reminiscent of
standard NFA minimization, We define (q, a, r)〜(q0, a0, r0)什(q, a) = (q0, a0). When construct-
ing our CRF, whenever a transition would have been allowed between two edges, we allow a tran-
sition betWeen their corresponding equivalence classes. We do the same to determine Which classes
are alloWed as initial or final tags. As each equivalence class corresponds (non-uniquely) to a single
symbol a, We can translate betWeen tag sequences and strings of L just as before.
4.3	Interpretation as a weighted finite-state transducer
While We present our model as a variation of a standard CRF Which enforces regular-language
constraints, an alternate characterization is as a Weighted finite-state transducer With the transducer
topology and Weight parameterization chosen so as to yield the distribution Pθ (y | x, L). In order
to accommodate CRF transition Weights, such an approach involves Weight-learning in an auxiliary
automaton whose edges correspond to edge-pairs in M -we give a full construction in Appendix C.
This interpretation enables direct comparison to studies on Weight learning in finite-state transducers,
such as Rastogi et al. (2016). While RegCCRFs can be viewed as special cases of neural-weighted
FSTs, they inherit a number of useful properties from CRFs not possessed by neural-weighted au-
tomata in general. Firstly, as |y| is necessarily equal to |x|, the partition function Py∈L Pθ(y | x, L)
is guaranteed to be finite, and Pθ(y | x, L) is a well-defined probability distribution for all θ, which
is not true for weighted transducers in general, which may admit paths with unbounded lengths and
weights. Secondly, as M is assumed to be unambiguous, string probabilities correspond exactly to
path probabilities, allowing for exact MAP inference with the Viterbi algorithm. In contrast, finding
the most probable string in the highly ambiguous automata used when learning edge weights for an
unknown language is NP-Hard (Casacuberta & de la Higuera, 1999), necessitating approximation
methods such as crunching (May & Knight, 2006). Finally, as each RegCCRF can be expressed as
a CRF with a particular parameterization, the convexity guarantees of standard CRFs carry over, in
that the loss is convex with respect to emission and transition scores. In contrast, training losses in
general weighted finite-state transducers are usually nonconvex (Rastogi et al., 2016).
5	Constrained training vs. constrained decoding
Our construction suggests two possible use cases for a RegCCRF: constrained decoding, where a
CRF is trained unconstrained, and the learned weights are then used in a RegCCRF at decoding
time, and constrained training, where a RegCCRF is both trained and decoded with constraints. In
5
Published as a conference paper at ICLR 2022
this section, we compare between these two approaches and a standard, unconstrained CRF. We
assume a machine learning setting where we have access to samples from some data distribution
P(x, y), with each X ∈ X*, and each y of matching length in some regular language L ⊆ Σ*. We
wish to model the conditional distribution Pe(y | x) with either a CRF or a RegCCRF, by way of
maximizing the model’s (log) likelihood given the data distribution.
The unconstrained CRF corresponds to a CRF that has been trained, without constraints, on data
from P (x, y), and is used directly for inference: It makes no use of the language L. The model’s
output distribution is Pθu (y | x), with parameter vector θu minimizing the NLL objective:
θu = argmin Ex,y〜P [- lnPθ(y | x)]	(6)
In constrained decoding, a CRF is trained unconstrained, but its weights are used in a RegCCRF at
decoding time. The output distribution of such a model is Pθu (y | x, L). It is parameterized by the
same parameter vector θu as the unconstrained CRF, as the training procedure is identical, but the
output distribution is conditioned on y ∈ L.
Constrained training involves directly optimizing a RegCCRF’s output distribution, avoiding any
asymmetry between training and decoding time. In this case, the output distribution of the model is
Pθc (y | x, L), where
θc = argmin Ex,y〜P [-ln Pθ(y | x, L)]	(7)
is the parameter vector which minimizes the NLL of the RegCCRF’s constrained distribution.
These three approaches form a hierarchy in terms of their ability to match the data distribution:
Lunconstrained ≥ Lconstrained decoding ≥ Lconstrained training , with each L corresponding to the negative log-
likelihood assigned by each model to the data; see Appendix B for a proof. This suggests we should
prefer the constrained training regimen.
6	S ynthetic Data Experiments
While constrained training cannot underperform constrained decoding, the conditions where it is
strictly better depend on exactly how the transition and emission functions are parameterized, and
are not easily stated in general terms. We now empirically show two simple experiments on synthetic
data where the two are not equivalent.
The procedure is similar for both experiments. We specify a regular language L, an observation
alphabet X, and a joint data distribution P(x, y) over observation sequences in X* and label Se-
quences in L. We then train two models, one with a RegCCRF, parameterized by θc, and one with
an unconstrained CRF, parameterized by θu . For each model, we initialize parameters randomly,
then use stochastic gradient descent to minimize NLL with P. We directly generate samples from
P to use as training data. After optimizing θc and θu , construct a RegCCRF with θu for use as a
constrained-decoding model, and we compare the constrained-training distribution Pθc (y | x, L)
with the constrained-decoding distribution Pθu (y | x, L).
We use a simple architecture for our models, with both the transition functions gθ and emission
functions hθ represented as parameter matrices. We list training hyperparameters in Appendix A.
6.1	Arbitrarily large differences in likelihood
We would like to demonstrate that, when comparing constrained training to constrained decoding
in terms of likelihood, constrained training can outperform constrained decoding by an arbitrary
margin. We choose L = (ac)* | (bc)* to make conditional independence particularly relevant - as
every even-indexed label is c, an unconstrained CRF must model odd-indexed labels independently,
while a constrained CRF can use its constraints to account for nonlocal dependencies. For simplicity,
we hold the input sequence constant, with X = {o}.
Our approach is to construct sequences of various lengths. For k ∈ N, we let our data distribution be
31
P(o2k, (ac)k) = - and P(o2k, (bc)k) = -.	(8)
6
Published as a conference paper at ICLR 2022
1
0.8 -
0.6
0.4
0.2
—≡- Constrained decoding
—e— Constrained training
....Data distribution
doohilekil-gol evitage
5
4
3
2
1
-a- Constrained decoding
con Constrained training
....Data distribution entropy
5	10	15	20
5	10	15	20
k
0
k
Figure 2: Model output probabilities, and NLL losses, plotted against sequence length k. As k in-
creases, constrained decoding becomes a progressively worse approximation for the data distribu-
tion, while constrained training is consistently able to match the data distribution.
Table 1: Output distributions for constrained decoding (Pθu (y | x, L)) and constrained training
(Pθc (y | x, L)), compared to the data distribution P(y | x). Constrained decoding cannot learn the
data distribution exactly, and yields a mode which disagrees with that of the data distribution.
y_	P(y I χ)	Pθu (y | χ, L	pθc(y | χ,
acd	~04~	0.32	0.40
bcd	0.3	0.48	0.30
bce	0.3	0.20	0.30
As the marginal distributions for odd-indexed characters are not independent, an unconstrained CRF
cannot exactly represent the distribution P . We train and evaluate individual models for each se-
quence length k. Figure 2 plots model probabilities and NLL losses for various k. We see that, re-
gardless of k, Pθc (y | x, L) is able to match P(y | x) almost exactly, with only small deviations due
to sampling noise in SGD. On the other hand, as sequence length increases, Pθu (y | x, L) becomes
progressively “lop-sided”, assigning almost all of its probability mass to the string (ac)k . This be-
havior is reflected in the models, likelihoods - constrained training stays at near-constant likelihood
for all k, while the negative log-likelihood of constrained decoding grows linearly with k.
6.2	Differences in MAP inference
We show here that constrained training and constrained decoding can disagree about which label se-
quence they deem most likely. Furthermore, in this case, MAP inference agrees with the data distri-
bution’s mode for constrained training, but not for constrained decoding. To do this, we construct a
fixed-length output language L = acd | bcd | bce, where an unconstrained CRF is limited by the
Markov property to predict y’s prefix and suffix independently, and choose a data distribution which
violates this independence assumption. We select our data distribution,
K/	八 C “ F K/ ,八
P (ooo, acd) = 0.4 and P (ooo, bcd)
0.3 and Pe(ooo, bce) = 0.3,
(9)
to be close to uniform, but with one label sequence holding the slight majority, and we ensure that
the majority label sequence is not the label sequence with both the majority prefix and the majority
suffix (i.e. bcd). As before, we hold the observation sequence as a constant (ooo). We train a
constrained and an unconstrained CRF to convergence, and compare Pθu (y | x, L) to Pθc (y | x, L).
Table 1 shows Pθu (y | x, L) and Pθc (y | x, L) as they compare to P (y | x). We find that, while
the mode of P(y | x) is acd, with probability of 0.4, the mode of constrained decoding distribution
Pθu (y | x, L) is bcd, the string with the majority prefix and the majority suffix, to which the
model assigns a probability of 0.48. Conversely, the constrained training distribution Pθc (y | x, L)
matches the data distribution almost exactly, and predicts the correct mode.
7
Published as a conference paper at ICLR 2022
7	Real-world data experiment: semantic role labeling
Task. As a final experiment, we apply our RegCCRF to the NLP task of semantic role labeling
(SRL) in the popular PropBank framework (Palmer et al., 2005). In line with previous work, we
adopt the known-predicate setting, where events are given and the task is to mark token spans as
(types of) event participants. PropBank assumes 7 semantic core roles (ARG0 through ARG5 plus
ARGA) plus 21 non-core roles for modifiers such as times or locations. For example, in [ARG0 Peter]
saw [ARG1 Paul] [ARGM-TMP yesterday], the argument labels inform us who does the seeing (ARG0),
who is seen (ARG1), and when the event took place (ARGM-TMP). In addition, role spans may be
labeled as continuations of previous role spans, or as references to another role span in the sentence.
SRL can be framed naturally as a sequence labeling task (He et al., 2017). However, the task comes
with a number of hard constraints that are not automatically satisfied by standard CRFs, namely: (1)
Each core role may occur at most once per event; (2) for continuations, the span type must occur
previously in the sentence; (3) for references, the span type must occur elsewhere in the sentence.
Data. In line with previous work (Ouchi et al., 2018), we work with the OntoNotes corpus as used
in the CoNLL 2012 shared task1 (Weischedel et al., 2011; Pradhan et al., 2012), whose training set
comprises 66 roles (7 core roles, 21 non-core roles, 19 continuation types, and 19 reference types).
RegCCRF Models. To encode the three constraints listed above in a RegCCRF, we define a regu-
lar language describing valid BIO sequences (Ramshaw & Marcus, 1999) over the 66 roles. A min-
imal unambiguous NFA for this language has more than 22 ∙ 319 states, which is too large to run the
Viterbi algorithm on our hardware. However, as many labels are very rare, we can shrink our au-
tomaton by discarding them at the cost of imperfect recall. We achieve further reductions in size
by ignoring constraints on reference roles, treating them identically to non-core roles. Our final au-
tomaton recognizes 5 core role types (ARG0 through ARG4), 17 non-core / reference roles, and one
continuation role type (for ARG1). This automaton has 672 states, yielding a RegCCRF with 2592
tags. A description of our procedure for constructing this automaton can be found in Appendix D.
Our model architecture is given by this RegCCRF, with emission scores provided by a linear projec-
tion of the output of a pretrained RoBERTa network Liu et al. (2019). In order to provide the model
with event information, the given predicates are prefixed by a special reserved token in the input se-
quence. RoBERTa parameters are fine-tuned during the learning of transition scores and projection
weights. We perform experiments with both constrained training and constrained decoding settings
一 We will refer to these as ConstrTr and ConstrDec respectively. A full description of the training
procedure, including training times, is provided in Appendix A. As RegCCRF loss is only finite for
label sequences in L, we must ensure that our training data do not violate our constraints. We dis-
card some roles, as described above, by simply removing the offending labels from the training data.
In six cases, training instances directly conflict with the constraints specified — all cases involve
continuation roles missing a valid preceding role. We discard these instances for ConstrTr.
CRF Baselines. As baseline models, we use the same architecture, but with a standard CRF re-
placing the RegCCRF. Since we are not limited by GPU memory for CRFs, we are optionally able
to include all role types present in the training set, using the complete training set. We present two
CRF baseline models: CRF-full, which is trained on all role-types from the training set, and CRF-
reduced, which includes the same subset of roles as the RegCCRF models. For CRF-reduced, we
use the same learned weights as for CD, but we decode without constraints.
Results and analysis. We evaluate our models on the evaluation partition, and measure perfor-
mance using F1 score for exact span matches. For comparability with prior work, we use the evalu-
ation script2 for the CoNLL-20θ5 shared task (Carreras & Marquez, 2005). These results, averaged
over five trials, are presented in Table 2. Excepting CRF-reduced, all of our models outperform the
existing state-of-the-art ensemble model Ouchi et al. (2018). We ascribe this improvement over the
existing literature to our use of ROBERTa — prior work in SRL relies on ELMo (Peters et al., 2018),
which tends to underperform transformer-based models on downstream tasks (Devlin et al., 2019).
1As downloaded from https://catalog.ldc.upenn.edu/LDC2013T19, and preprocessed ac-
cording to https://cemantix.org/data/ontonotes.html
2As available from https://www.cs.upc.edu/' Srlconll/soft.html.
8
Published as a conference paper at ICLR 2022
Table 2: Results from our experiments (averaged over 5 trials), along with selected reported results
from recent literature. We rank of our models by precision, recall, and Fi score - rankings differ if
and only if the difference is significant at p < 0.05 (two-tailed), as measured by a permutation test.
	Model	Precision(rank)	ReCan(rank)	F1 (rank)
Our experiments	RoBERTa + CRF (CRF-full)	86.82⑵	87.73(I)	87.27⑵
	RoBERTa + CRF (CRF-reduced)	87.33(1)	85.95(3)	86.63(3)
	RoBERTa + RegCCRF (ConstrDec)	87.28(1)	87.13(2)	87.20(2)
	RoBERTa + RegCCRF (ConstrTr)	87.22(1)	87.79(1)	87.51⑴
Results from literature	He et al. (2017)	—	—	85.5
	Ouchi et al. (2018)	87.1	85.3	86.2
	Ouchi et al. (2018) (ensemble)	88.5	85.5	87.0
	Li et al. (2019)	85.7	86.3	86.0
Of our models, ConstrTr significantly3 outperforms the others in F1 score and yields a new SOTA
for SRL on OntoNotes, in line with expectations from theoretical analysis and on synthetic data.
Our other three models show trade-offs between precision and recall, wherein CRF-full outperforms
the other two in recall and underperforms them in precision. This is not surprising, as CRF-full
is the only model capable of predicting rare role types. The other models, which use the reduced
tag sets, have a theoretical maximum of 99% recall. Interestingly, when comparing between the
three models that use the reduced tag sets, we find a statistically significant interaction between the
constraint setting and model recall, but not between constraints and model precision: ConstrTr has
significantly higher recall than ConstrDec, which in turn significantly beats CRF-reduced in recall,
but there are no statistically significant differences between these models’ precisions.
For our unconstrained models, CRF-full and CRF-reduced, the constraints specified in our automa-
ton are violated in 0.81% and 0.84% of all output sequences respectively.4 While this number is
small, it should not be interpreted to mean that constraints are useless for almost all instances - as
shown in Section 6.2, constraints during training can affect MAP inference even when none of the
alternatives violate constraints.
8	Conclusion and Future Work
We have presented a method to constrain the output of CRFs to a regular language. Our construc-
tion permits constraints to be used at training or prediction time; both theoretically and empirically,
training-time constraining better captures the data distribution. Conceptually, our approach consti-
tutes a novel bridge between constrained CRFs and neural-weighted FSTs.
Future work could target enhancing the model’s expressibility, either by allowing constraints to de-
pend explicitly on the input as regular relations, or by investigating non-binary constraints, i.e., reg-
ular language-based constraints with learnable weights. Additionally, regular language induction
(e.g. Dunay et al. (1994); Bartoli et al. (2016)) could be used to learn languages automatically, re-
ducing manual specification and identifying non-obvious constraints. Another avenue for continu-
ing research lies in identifying further applications for RegCCRFs. The NLP task of relation extrac-
tion could be a fruitful target - RegCCRFs offer a mechanism to make the proposal of a relation
conditional on the presence of the right number and type of arguments. While our construction can-
not be lifted directly to context-free languages due to the unbounded state space of the correspond-
ing pushdown automata, context-free language can be approximated by regular languages (Mohri &
Nederhof, 2001). On this basis, for example, a RegCCRF backed by a regular language describing
trees of a limited depth could also be applied to tasks with context-free constraints.
To encourage the use of RegCCRFs, we provide an implementation as a Python library under the
Apache 2.0 license which can be used as a drop-in replacement for standard CRFs in PyTorch.5
3All significance results are at the p < 0.05 level (two-tailed), as measured by a permutation test over the
five trials of each model.
4For CRF-full, we only count violations of constraints for those roles that our automaton accounts for.
5Available at www.ims.uni- stuttgart.de/en/research/resources/tools/regccrf/
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is supported by IBM Research AI through the IBM AI Horizons Network.
Reproducibility statement
To ensure reproducibility, we have released all code for the RegCCRF model as an open-source
Python 3 library under the Apache 2.0 license, which is included in the supplementary materials.
Additionally, we include Python scripts for reproducing all experiments presented in the paper, de-
tailed descriptions of our datasets and preprocessing steps, and training logs. Furthermore, Ap-
pendix A lists all model hyperparameters, details the preprocessing steps taken for our experiments,
and specifies the hardware used for our experiments along with average training and inference times
for each experiment.
Ethics statement
The research in this paper is fundamental in the sense that it enables machine learning models to
better represent data and limit the search space at inference and learning time. It therefore does not
in and of itself represent additional ethical risks on top of the previous work we build upon.
References
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary im-
age captioning with constrained beam search. In Proceedings of the 2017 Conference on Empir-
ical Methods in Natural Language Processing, pp. 936-945, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1098. URL https:
//www.aclweb.org/anthology/D17-1098.
Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, and Fabiano Tarlao. Inference of regular expres-
sions for text extraction from examples. IEEE Transactions on Knowledge and Data Engineer-
ing, 28(5):1217-1230, 2016. doi: 10.1109/TKDE.2016.2515587.
Anne Bruggemann-Klein and Derick Wood. Deterministic regular languages. In Alain Finkel and
Matthias Jantzen (eds.), STACS 92, pp. 173-184, Berlin, Heidelberg, 1992. Springer Berlin Hei-
delberg. ISBN 978-3-540-46775-5.
Xavier Carreras and Lluis Marquez. Introduction to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning
(CoNLL-2005), pp. 152-164, Ann Arbor, Michigan, June 2005. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/W05-0620.
F. Casacuberta and C. de la Higuera. Optimal linguistic decoding is a difficult computational prob-
lem. Pattern Recognition Letters, 20(8):813-821, January 1999. doi: 10.1016/S0167-8655(99)
00045-8.
Yoeng-Jin Chu. On the shortest arborescence of a directed graph. Scientia Sinica, 14:1396-1400,
1965.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423.
B.D. Dunay, F.E. Petry, and B.P. Buckles. Regular language induction with genetic programming. In
Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress
on Computational Intelligence, pp. 396-400 vol.1, 1994. doi: 10.1109/ICEC.1994.349918.
10
Published as a conference paper at ICLR 2022
Jack Edmonds. Optimal branchings. Journal of Research of the National Bureau of Standards,
71B(4)：233-240, 1967. URL https://nvlpubs.nist.gov/nistpubs/jres/71B/
jresv71Bn4p233_A1b.pdf.
Jason Eisner. Parameter estimation for probabilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics, pp. 1-8, Philadel-
phia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi： 10.3115/
1073083.1073085. URL https://aclanthology.org/P02- 1001.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisen-
stein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah Smith. Part-of-speech tag-
ging for twitter： Annotation, features, and experiments. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics, pp. 42-47, 2011. URL https://
aclanthology.org/P11-2008.pdf.
Hermann Gruber and Markus Holzer. Inapproximability of nondeterministic state and transition
complexity assuming P6= NP. In International Conference on Developments in Language Theory,
pp. 205-216. Springer, 2007.
Eva Hasler, Adria de Gispert, Gonzalo Iglesias, and Bill Byrne. Neural machine translation decod-
ing with terminology constraints. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-
ume 2 (Short Papers), pp. 506-512, New Orleans, Louisiana, June 2018. Association for Com-
putational Linguistics. doi： 10.18653/v1/N18-2081. URL https://www.aclweb.org/
anthology/N18-2081.
Luheng He, Kenton Lee, Mike Lewis, and Luke Zettlemoyer. Deep semantic role labeling： What
works and what’s next. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 473-483, Vancouver, Canada, July
2017. Association for Computational Linguistics. doi： 10.18653/v1/P17-1044. URL https:
//aclanthology.org/P17-1044.
Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid
beam search. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1535-1546, Vancouver, Canada, July 2017. Association
for Computational Linguistics. doi： 10.18653/v1/P17-1141. URL https://www.aclweb.
org/anthology/P17-1141.
Tao Jiang and Bala Ravikumar. Minimal NFA problems are hard. In International Colloquium on
Automata, Languages, and Programming, pp. 629-640. Springer, 1991.
Zhe Jiang. Spatial structured prediction models： Applications, challenges, and techniques. IEEE
Access, 8：38714-38727, 2020. doi： 10.1109/ACCESS.2020.2975584.
Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David A. Shamma, Michael S. Bernstein,
and Li Fei-Fei. Image retrieval using scene graphs. In 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 3668-3678, 2015. doi： 10.1109/CVPR.2015.7298990.
Daniel Jurafsky and James H. Martin. Speech and Language Processing (2nd Edition). Prentice-
Hall, Inc., USA, 2009. ISBN 0131873210.
Diederik P. Kingma and Jimmy Ba. Adam： A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Trausti Kristjansson, Aron Culotta, Paul Viola, and Andrew McCallum. Interactive information
extraction with constrained conditional random fields. In Proceedings of the AAAI Conference on
Artificial Intelligence, pp. 412-418, 2004.
John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields： Probabilis-
tic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pp. 282-289, San Francisco, CA, USA, 2001. Morgan
Kaufmann Publishers Inc.
11
Published as a conference paper at ICLR 2022
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 260-270, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1030. URL https://www.aclweb.org/anthology/
N16-1030.
Zuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. De-
pendency or span, end-to-end uniform semantic role labeling. Proceedings of the AAAI Confer-
ence on Artificial Intelligence, 33(01):6730-6737, Jul. 2019. doi: 10.1609/aaai.v33i01.33016730.
URL https://ojs.aaai.org/index.php/AAAI/article/view/4645.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretrain-
ing approach. arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/abs/
1907.11692.
Jonathan May and Kevin Knight. A better n-best list: Practical determinization of weighted finite
tree automata. In Proceedings of the Human Language Technology Conference of the NAACL,
Main Conference, pp. 351-358, New York City, USA, June 2006. Association for Computational
Linguistics. URL https://aclanthology.org/N06-1045.
Mehryar Mohri. A disambiguation algorithm for finite automata and functional transducers. In In-
ternational Conference on Implementation and Application of Automata, pp. 265-277. Springer,
2012.
Mehryar Mohri and Mark-Jan Nederhof. Regular approximation of context-free grammars through
transformation. In Robustness in language and speech technology, pp. 153-163. Springer, 2001.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate
inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in
Artificial Intelligence, pp. 467-475, Stockholm, Sweden, 1999. Morgan Kaufmann Publishers
Inc. ISBN 1558606149.
Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie. Sparsemap: Differentiable sparse
structured inference. In International Conference on Machine Learning, pp. 3799-3808. PMLR,
2018.
Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. A span selection model for semantic role
labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 1630-1642, Brussels, Belgium, October-November 2018. Association for Com-
putational Linguistics. doi: 10.18653/v1/D18-1191. URL https://www.aclweb.org/
anthology/D18-1191.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. The Proposition Bank: An annotated
corpus of semantic roles. Computational Linguistics, 31(1):71-106, 2005. doi: 10.1162/
0891201053630264. URL https://www.aclweb.org/anthology/J05-1004.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, New Orleans, Louisiana, June
2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:
//www.aclweb.org/anthology/N18-1202.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. CoNLL-
2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Joint Con-
ference on EMNLP and CoNLL - Shared Task, pp. 1-40, Jeju Island, Korea, July 2012. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
W12-4501.
12
Published as a conference paper at ICLR 2022
Lance Ramshaw and Mitchell Marcus. Text chunking using transformation-based learning. In Nat-
Ural Language Processing Using Very Large Corpora, pp. 157-176. Springer Netherlands, Dor-
drecht, 1999. ISBN 978-94-017-2390-9. doi: 10.1007/978-94-017-2390-9」0. URL https:
//doi.org/10.1007/978-94-017-2390-9_10.
Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner. Weighting finite-state transductions with
neural context. In Proceedings of the 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 623-633, San
Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/
N16-1076. URL https://aclanthology.org/N16-1076.
Dan Roth and Wen-tau Yih. A linear programming formulation for global inference in natural
language tasks. In Proceedings of the Eighth Conference on Computational Natural Language
Learning (CoNLL-2004) at HLT-NAACL 2004, pp. 1-8, Boston, Massachusetts, USA, May 6 -
May 7 2004. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W04-2401.
Sunita Sarawagi and William W Cohen. Semi-markov conditional random fields for information
extraction. Advances in neural information processing systems, 17:1185-1192, 2004.
Christian Scheible, Roman Klinger, and Sebastian Pado. Model architectures for quotation detec-
tion. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 1736-1745, Berlin, Germany, August 2016. Association for Com-
putational Linguistics. doi: 10.18653/v1/P16-1164. URL https://aclanthology.org/
P16-1164.
Charles Sutton and Andrew McCallum. Collective segmentation and labeling of distant entities
in information extraction. In Proceedings of the ICML 2004 Workshop on Statistical Relational
Learning, 2004.
Andrew Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13(2):260-269, 1967. doi: 10.1109/TIT.
1967.1054010.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. Ontonotes: A large training corpus for enhanced process-
ing. In Handbook of Natural Language Processing and Machine Translation: DARPA Global Au-
tonomous Language Exploitation. Springer, 2011.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp- demos.6.
A Experimental Design
This appendix details the training procedures and hyperparameter choices for our experiments.
These are summarized in Table 3. Full code for all experiments, along with training logs, are also
included in the supplementary materials.
A.1 CRFS
For all CRFs and RegCCRFs, transition potentials were initialized randomly from a normal distribu-
tion with mean zero and standard deviation 0.1. No CRFs or RegCCRFs employed special start- or
end-transitions - that is, we did not insert any additional beginning-of-sequence or end-of-sequence
tags for the Viterbi or forward algorithms.
13
Published as a conference paper at ICLR 2022
Table 3: Summary of hyperparameters for our models and experiments.
CRFs	Transition score initialization	N (0, 0.1)
Synthetic data experiments	Emission score initialization Optimizer Training iterations Batch size initial learning rate Learning rate decay	pyTorch default SGD 5000 50 1.0 10% every 100 steps
SRL experiments	RoBERTa weights projection weight and bias initialization Optimizer Learning rate Batch size Gradient accumulation	roberta-base pyTorch default Adam 10-5 2 4 batches
A.2 Synthetic data experiments - training procedure
For both synthetic data experiments, the emission potentials were represented explicitly for each
position as trainable parameters - since the observation sequence was constant in all experiments,
these did not depend on x.
parameters were initialized randomly using pyTorch default initialization, and optimized using
stochastic gradient descent. To ensure fast convergence to a stable distribution, we employed learn-
ing rate decay - learning rate was initially set to 1.0, and reduced by 10% every 100 training steps.
We trained all models for a total of 5000 steps with a batch size of 50. All models were trained on
CPUs. For the experiment described in Section 6.1, We trained separate models for each k - the
total training time for this experiment was approximately 35 minutes. The experiment described in
Section 6.2 completed training in approximately 30 seconds.
A.3 Semantic ROLE labeling — training procedure
in the semantic role labeling (SRL) experiments, we incorporated a pretrained RoBERTa net-
work (Liu et al., 2019) - the implementation and weights for this model were obtained using the
roberta-base model from the Hugging Face transformers library (Wolf et al., 2020). RoBERTa
embeddings were projected down to transmission scores using a linear layer with a bias - projection
weights and biases were initialized using the pyTorch default initialization.
input tokens were sub-tokenized using RoBERTa’s tokenizer. The marked predicate in each sentence
was prefixed by a special <unk> token. During training, for efficiency reasons, we excluded all
sentences with 120 or more subtokens - this amounted to 0.23% of all training instances. We
nonetheless predicted on all instances, regardless of length.
We optimized models using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 10-5.
We fine-tune RoBERTa parameters and learn the projection and RegccRF weights jointly. For
performance reasons, batch size was set to 2, but we utilized gradient accumulation over groups of
4 batches to simulate a batch size of 8.
We utilized early stopping to avoid overfitting. Every 5000 training steps, we approximated our
model’s F1 score against a subset of the provided development partition, using a simplified reimple-
mentation of the official evaluation script. Each time we exceeded the previous best F1 score for a
model, we saved all model weights to disk. After 50 such evaluations with no improvement, we ter-
minated training, and used the last saved copy of model weights for final evaluation.
We performed all SRL experiments on GeForce GTX 1080 Ti Gpus. Each experiment used a single
Gpu. Training took an average of 88 hours for RegccRF models with constrained training, 23
hours for RegccRF with constrained decoding, and 24 hours for cRF baseline models. inference
on the complete test set took an average of 18 minutes 55 seconds for cT and cD, and an average
14
Published as a conference paper at ICLR 2022
of 55 seconds for CRF-full and CRF-reduced. All training logs with timestamps are included in the
supplementary materials.
B Proof of constrained training inequality
In this appendix, we prove the inequality presented in section 5: when compared by NLL against the
data distribution, Lunconstrained ≥ Lconstrained decoding ≥ Lconstrained training , with each L corresponding to
that model’s negative log-likelihood. We first prove the left side of this inequality, comparing an
unconstrained CRF to constrained decoding, and then prove the right side, comparing constrained
decoding to constrained training. We use the notation introduced in Section 5.
Theorem 1. For arbitrary θ: Ex	P [-ln Pθ(y | x)] ≥ Ex	P [-ln Pθ(y | x,L)]
x ,y i	x ,y i
Here we compare the distributions Pθ(y | x) and Pθ(y | x, L). We wish to demonstrate that
Pθ(y | x) can never achieve lower NLL than Pθ(y | x, L), and that the two distributions achieve
identical NLL only when Pθ(y | x) = Pθ(y | x, L) i.e. when constraints have no effect. Of note,
this proof is valid for all parameterizations θ, and not just for θu .
Proof. Since every y in P is in L,
Pθ(y | X, L) = α ∙ Pθ(y | x),	(10)
with α ≥ 1. Thus, the NLL of the regular-constrained CRF is
Ex,y 〜P [-ln Pθ (y 1 x, L)] = Ex,y 〜P [-ln Pθ (y 1 x)] - ln a.	(II)
This differs from the NLL of the unconstrained CRF only by the term - ln α. As α ≥ 1, the regular-
constrained CRF’s NLL is less than or equal to that of the unconstrained CRF, with equality only
when α = 1 and therefore Pθ(y | x) = Pθ(y | x, L).
Theorem2. Ex,y〜P [-lnPθu(y | x,L)] ≥ Ex,y〜P [-lnPθcIy | χ,L)]
In this case, we compare the distributions Pθu (y | x, L) and Pθc (y | x, L). We will demonstrate
that the former cannot achieve a lower NLL against the data distribution than the latter.
Proof. This follows directly from our definitions, as we define θc to minimize the NLL of
Pθ(y | x, L) against the data distribution. Thus, Pθu (y | x, L) could never yield a lower NLL than
Pθc (y | x, L), as that would contradict our definition of θc.
C Construction as weighted FST
In this appendix, we present a construction of the RegCCRF as a weighted finite-state transducer
with weight sharing. We do this by first specifying the transducer topology used, and then specifying
how edge weights are parameterized in terms of θ. The resulting transducer yields an identical
distribution to that of the CRF-based construction, Pθ (y | x, L).
C.1 Transducer topology
Starting from L, we define L to be the regular language of bigram sequences for the strings in L, i.e.,
L= {<(s1,s2),(s2,s3), …，(s∣s-1∣, s|s|), (s∣s∣, $)〉| S ∈ L} ,	(12)
with $ acting as a end-of-string symbol. We let M be an unambiguous FSA for the language L, and
choose to interpret this automaton as a finite-state transducer by stipulating that all edges should
accept any symbol in the input language (but only one symbol per transition, and without allowing
epsilon transitions). This unweighted transducer will be used as the topology for our weighted finite-
state transducer.
15
Published as a conference paper at ICLR 2022
C.2 Edge weights
In line with Rastogi et al. (2016), we would like to assign weights to the edges of our transducer M
with a neural network. In order to obtain the same distribution as from our CRF-based construction,
these weights must be parameterized in terms of our transition function gθ and emission function
hθ . For each edge in M, the weight depends only on the emitted bigram, the input sequence, and
the index of the current input symbol - the weight does not depend on the FST states. For a symbol
bigram (a, b), input sequence x, and index i, the edge weight is equal to
W =	gθ (a, b) + hθ (x, a, i)
a,b	hθ(x, a, i)
b 6= $
otherwise
(13)
Each string in L corresponds bijectively to exactly one bigram sequence in L, which corresponds
bijectively to exactly one accepting path in M - this path’s weight (in the Log semiring) is equal to
the unscaled probability produced by our CRF construction, and so the weighted FST, interpreted as
a probability distribution, yields the distribution Pθ (y | x, L).
D Automaton construction for semantic role labeling
In this appendix, we describe how we generate the automaton architecture for our semantic role la-
beling experiments. While our experiments used 5 core-roles, 17 non-core roles, and one continu-
ation role, we discuss here a generalized setting with arbitrary sets of core, noncore, and continua-
tions of core roles.
Algorithm 1 provides pseudocode for our construction. The core idea is to use subsets of core roles
as NFA states, so that we can keep track of which core roles have already ocurred. Additional states
are used in order to ensure all strings are valid BIO sequences.
16
Published as a conference paper at ICLR 2022
Data: Sets Rcore, Rnoncore, and Rcontinuation, of core, noncore, and continuation roles, respectively
Result: A finite-state automaton M = (Σ, Q, q1, F, E), parameterized as described in Section 3
ς4-{OUTSIDE} ∪ ({BEGIN, INSIDE} × (Rcore ∪ Rnoncore ∪ Rcontinuation));
Q — 0;
qι — 0;
F — 0;
E — 0;
for p ∈ 2Rcore do
Q — Q ∪ {p};
F — F ∪ {p};
E — E ∪ {(p, OUTSIDE,p)};
for r ∈ Rn
oncore do
S — (r,p);
Q J Q ∪{s};
E — E ∪ {(p, (BEGIN, r),p),(p, (BEGIN,r), s)};
E J E ∪ {(r, (INSIDE, r), s), (r, (INSIDE, r), p)};
end
for r ∈ Rcontinuation do
if The core role corresponding to r is in p then
s J (r, p);
Q J Q ∪ {s};
E J E ∪ {(p, (BEGIN, r),p), (p, (BEGIN, r), s)};
E J E ∪ {(r, (INSIDE, r), s), (r, (INSIDE, r), p)};
end
end
for r ∈ (Rcore \ p) do
s J (r, p);
t J p ∪ {r};
Q J Q ∪ {s};
E J E ∪ {(p, (BEGIN, r), s), (p, (BEGIN, r), t)};
E J E ∪ {(s, (INSIDE, r), s), (s, (INSIDE, r), t)};
end
end
return (Σ, Q, q1, F, E)
Algorithm 1: Construction of an FSA from given sets of core, noncore, and continuation roles.
To represent BIO labels, we use tuples of the form (BEGIN, <roleType>) for B labels, tuples
of the form (INSIDE, <roleType>) for I labels, and the symbol OUTS IDE for the sole O label.
17