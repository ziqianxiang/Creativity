Published as a conference paper at ICLR 2022
Path Integral Sampler:
A STOCHASTIC CONTROL APPROACH FOR SAMPLING
Qinsheng Zhang
Center for Machine Learning
Georgia Institute of Technology
qzhang419@gatech.edu
Yongxin Chen
School of Aerospace Engineering
Georgia Institute of Technology
yongchen@gatech.edu
Ab stract
We present Path Integral Sampler (PIS), a novel algorithm to draw samples from
unnormalized probability density functions. The PIS is built on the Schrodinger
bridge problem which aims to recover the most likely evolution of a diffusion
process given its initial distribution and terminal distribution. The PIS draws
samples from the initial distribution and then propagates the samples through the
Schrodinger bridge to reach the terminal distribution. Applying the Girsanov the-
orem, with a simple prior diffusion, we formulate the PIS as a stochastic optimal
control problem whose running cost is the control energy and terminal cost is
chosen according to the target distribution. By modeling the control as a neural
network, we establish a sampling algorithm that can be trained end-to-end. We
provide theoretical justification of the sampling quality of PIS in terms of Wasser-
stein distance when sub-optimal control is used. Moreover, the path integrals
theory is used to compute importance weights of the samples to compensate for
the bias induced by the sub-optimality of the controller and time-discretization.
We experimentally demonstrate the advantages of PIS compared with other start-
of-the-art sampling methods on a variety of tasks.
1	Introduction
We are interested in drawing samples from a target density μ = Zμ known UP to a normalizing
constant Z. Although it has been widely studied in machine learning and statistics, generating
asymptotically unbiased samples from such unnormalized distribution can still be challenging (Tal-
war, 2019). In practice, variational inference (VI) and Monte Carlo (MC) methods are two popular
frameworks for sampling.
Variational inference employs a density model q, from which samples are easy and efficient to draw,
to approximate the target density (Rezende & Mohamed, 2015; Wu et al., 2020). Two important
ingredients for variational inference sampling include a distance metric between q and μ to identify
good q and the importance weight to account for the mismatch between the two distributions. Thus,
in variational inference, one needs to access the explicit density of q, which restricts the possible
parameterization of q. Indeed, explicit density models that provide samples and probability density
such as Autoregressive models and normalizing flow are widely used in density estimation (Gao
et al., 2020a; Nicoli et al., 2020). However, such models impose special structural constraints on the
representation ofq. For instance, the expressive power of normalizing flows (Rezende & Mohamed,
2015) is constrained by the requirements that the induced map has to be bijective and its Jacobian
needs to be easy-to-compute (Cornish et al., 2020; Grathwohl et al., 2018; Zhang & Chen, 2021).
Most MC methods generate samples by iteratively simulating a well-designed Markov
chain (MCMC) or sampling ancestrally (MacKay, 2003). Among them, Sequential Monte Carlo
and its variants augmented with annealing trick are regarded as state-of-the-art in certain sampling
tasks (Del Moral et al., 2006). Despite its popularity, MCMC methods may suffer from long mixing
time. The short-run performance of MCMC can be difficult to analyze and samples often get stuck
in local minima (Nijkamp et al., 2019; Gao et al., 2020b). There are some recent works exploring the
possibility of incorporating neural networks to improve MCMC (Spanbauer et al., 2020; Li et al.,
2020b). However, evaluating existing MCMC empirically, not to say designing an objective loss
1
Published as a conference paper at ICLR 2022
Optimal control
Terminal Cost Ψ
Figure 1: Illustration of Path Integral Sampler (PIS). The optimal policy of a specific stochastic
control problem where a terminal cost function is chosen according to the given target density μ,
can generate unbiased samples over a finite time horizon.
function to train network-powered MCMC, is difficult (Liu et al., 2016; Gorham & Mackey, 2017).
Most existing works in this direction focus only on designing data-aware proposals (Song et al.,
2017; Titsias & Dellaportas, 2019) and training such networks can be challenging without expertise
knowledge in sampling.
In this work, we propose an efficient sampler termed Path Integral Sampler (PIS) to generate samples
by simulating a stochastic differential equation (SDE) in finite steps. Our algorithm is built on the
Schrodinger bridge problem (PaVOn, 1989; Dai Pra, 1991; Leonard, 2014; Chen et al., 2021) whose
original goal was to infer the most likely evolution of a diffusion given its marginal distributions at
two time points. With a proper prior diffusion model, this Schrodinger bridge framework can be
adopted for the sampling task. Moreover, it can be reformulated as a stochastic control problem
(Chen et al., 2016) whose terminal cost depends on the target density ^ so that the diffusion under
optimal control has terminal distribution μ. We model the control policy with a network and de-
velop a method to train it gradually and efficiently. The discrepancy of the learned policy from the
optimal policy also provides an evaluation metric for sampling performance. Furthermore, PIS can
be made unbiased even with sub-optimal control policy via the path integral theorem to compute
the importance weights of samples. Compared with VI that uses explicit density models, PIS uses
an implicit model and has the advantage of free-form network design. The explicit density mod-
els have weaker expressive power and flexibility compared with implicit models, both theoretically
and empirically (Cornish et al., 2020; Chen et al., 2019; Kingma & Welling, 2013; Mohamed &
Lakshminarayanan, 2016). Compared with MCMC, PIS is more efficient and is able to generate
high-quality samples with fewer steps. Besides, the behavior of MCMC over finite steps can be
analyzed and quantified. We provide explicit sampling quality guarantee in terms of Wasserstein
distance to the target density for any given sub-optimal policy.
Our algorithm is based on Tzen & Raginsky (2019), where the authors establish the connections be-
tween generative models with latent diffusion and stochastic control and justify the expressiveness
of such models theoretically. How to realize this model with networks and how the method performs
on real datasets are unclear in Tzen & Raginsky (2019). Another closely related work is Wu et al.
(2020); Arbel et al. (2021), which extends Sequential Monte Carlo (SMC) by combining determin-
istic normalizing flow blocks with stochastic MCMC blocks. To be able to evaluate the importance
weights efficiently, MCMC blocks need to be chosen based on annealed target distributions care-
fully. In contrast, in PIS one can design expressive architecture freely and train the model end-to-end
without the burden of tuning MCMC kernels, resampling or annealing scheduling. We summarize
our contributions as follows. 1). We propose Path Integral Sampler, a generic sampler that generates
samples through simulating a target-dependent SDE which can be trained with free-form architec-
ture network design. We derive performance guarantee in terms of the Wasserstein distance to the
target density based on the optimality of the learned SDE. 2). An evaluation metric is provided to
quantify the performance of learned PIS. By minimizing such evaluation metric, PIS can be trained
end-to-end. This metric also provides an estimation of the normalization constants of target dis-
tributions. 3). PIS can generate samples without bias even with sub-optimal SDEs by assigning
importance weights using path integral theory. 4). Empirically, PIS achieves the state-of-the-art
sampling performance in several sampling tasks.
2
Published as a conference paper at ICLR 2022
2	Sampling and stochastic control problems
We begin with a brief introduction to the sampling problem and the stochastic control problem.
Throughout, we denote by τ = {xt , 0 ≤ t ≤ T } a continuous-time stochastic trajectory.
2.1	Sampling problems
We are interested in drawing samples from a target distribution μ(x) = μ(x)∕Z in Rd where Z is
the normalization constant. Many sampling algorithms rely on constructing a stochastic process that
drives the random particles from an initial distribution ν that is easy to sample from, to the target
distribution μ.
In the variational inference framework, one seeks to construct a parameterized stochastic process to
achieve this goal. Denote by Ω = C ([0,T ]; Rd) the path space consisting of all possible trajectories
and by P the measure over Ω induced by a stochastic process with terminal distribution μ at time T.
Let Q be the measure induced by a parameterized stochastic and denote its marginal distribution at
T by μQ. Then, by the data processing inequality, the Kullback-Leibler divergence (KL) between
marginal distributions μQ and μ can be bounded by
DκL(μQkμ) ≤
DKL(QkP) := d dQIOgdQ.
Jω	dP
(1)
Thus, DKL (QkP) serves as a performance metric for the sampler, and a small DKL(QkP) value
corresponds to a good sampler.
2.2	Stochastic control
Consider a model characterized by a special stochastic differential equation (SDE) (Sarkka & Solin,
2019)
dxt = Utdt + dwt, xo 〜ν,	(2)
where xt, ut denote state and control input respectively, and wt denotes standard Brownian motion.
In stochastic control, the goal is to find an feedback control strategy that minimizes a certain given
cost function.
The standard stochastic control problem can be associated with any cost and any dynamics. In this
work, we only consider cost of the form
E
ZT
0
1 ∣∣Utk2 dt + Ψ(xτ) | X0
(3)
〜V
where Ψ represents the terminal cost. The corresponding optimal control problem can be solved
via dynamic programming (Bertsekas et al., 2000), which amounts to solving the Hamilton-Jacobi-
Bellman (HJB) equation (Evans, 1998)
^∂tt - 2VVtzWt + 2δVt = 0, vtS = ψ(∙).	(4)
The space-time function Vt(x) is known as cost-to-go function or value function. The optimal policy
can be computed from Vt (x) as (Pavon, 1989)
U； (X) = -VVt (x).	(5)
3	Path Integral Sampler
It turns out that, with a proper choice of initial distribution ν and terminal loss function Ψ, the
stochastic control problem coincides with sampling problem, and the optimal policy drives samples
from v to μ perfectly. The process under optimal control can be viewed as the posterior of uncon-
trolled dynamics conditioned on target distribution as illustrated in Fig 1. Throughout, we denote
by Qu the path measure associated with control policy u. We also denote by μ0 the terminal distri-
bution of the uncontrolled process Q0. For the ease of presentation, we begin with sampling from a
normalized density μ, and then generalize the results to unnormalized μ in Section 3.4.
3
Published as a conference paper at ICLR 2022
3.1	Path Integral and value function
Thanks to the special cost structure, the nonlinear HJB eq (4) can be transformed into a linear partial
differential equation (PDE)
^φtt + 2δ皿 = 0, φT (∙) = exp{-ψ(∙)}	⑹
by logarithmic transformation (Sarkka & Solin, 2019) Vt(X) = -logφt(x). By the celebrated
Feynman-Kac formula (0ksendal, 2003), the above has solution
φt(x) = Eqo [exp(-Ψ(xτ ))∣xt = x].	(7)
We remark that eq (7) implies that the optimal value function can be evaluated without knowing
the optimal policy since the above expectation is with respect to the uncontrolled process Q0 . This
is exactly the Path Integral control theory (Theodorou et al., 2010; Theodorou & Todorov, 2012;
Thijssen & Kappen, 2015). Furthermore, the optimal control at (t, X) is
*，、	V71	入，、1∙	EQ0 {exP{-ψ(χτ)} Rts dwt | χt = x}
Ut(X) = ▽ log Φt(x) = lim 7——-t——r-τ-t~~VTi-----------F,	⑻
t	s&t (s - t)EQ0 {exp{-Ψ(XT)} | Xt = X}
meaning that U (x) can also be estimated by uncontrolled trajectories.
3.2	Sampling as a stochastic optimal control problem
There are infinite choices of control strategy U such that eq (2) has terminal distribution μ. We
are interested in the one that minimizes the KL divergence to the prior uncontrolled process. This
is exactly the Schrodinger bridge problem (Pavon, 1989; Dai Pra, 1991; Chen et al., 2016; 2021),
which has been shown to have a stochastic control formulation with cost being control efforts. In
cases where ν is a Dirac distribution, it is the same as the stochastic control problem in Section 2.2
with a proper terminal cost as characterized in the following result (Tzen & Raginsky, 2019).
Theorem 1 (Proof in appendix A). When ν is a Dirac distribution and terminal loss is chosen as
Ψ(xt) = log '(H), the distribution Q* induced by the optimal control policy is
Q*(τ) = Q0 (τ |xt )μ(xτ).
(9)
Moreover Q*(xt) = μ(xτ).
To gain more insight, consider the KL divergence
DKL(Qu(T)kQ0(τ|xt)μ(xτ))= DKL(Qu(T)kQ0(τ)4χ4) = DKL(QukQ0) + EQu [log μ0].
μ0(χτ)	μ
(10)
Thanks to the Girsanov theorem (Sarkka & Solin, 2019),
dQu	T 1	2
dQo =eχP(∕ 2 kutk dt + UtdWt).	(11)
It follows that
DKL(QukQ0) =EQu[
(V kutk2 dt].
(12)
Plugging eq (12) into eq (10) yields
Dkl(Qu(t)kQ0(τ|xt)μ(χτ)) = EQu [广 1 kutk2 dt + log μ0(χT)],	(13)
Jo 2	μ(xτ)
which is exactly the cost defined in eq (3) with Ψ = log μ0. Theorem 1 implies that once the optimal
control policy that minimizes this cost is found, it can also drive particles from χo 〜V to XT 〜μ.
4
Published as a conference paper at ICLR 2022
Algorithm 1 Training
Input: Vector: x0 = 0, Scalar: y0 = 0
Output: ut (x) parameterized by θ
Define： SDEdrift f(t, [xt,yt]) = [uθt(Xt), 1 ∣∣uθt(Xt)『],diffusion g(t, [xt,yt]) = [1,0]
loop epoches
XT, yT = sdeint(f, g, [X0, y0], [0, T])	# Integrate SDE from 0 to T with Neural SDE
Gradient descent step Vθ [yτ + log \(£)]	# Optimize control policy
done
3.3	Optimal control policy and sampler
Optimal Policy Representation: Consider the sampling strategy from a given target density by
simulating SDE in eq (2) under optimal control. Even though the optimal policy is characterized by
eq (8), only in rare case (Gaussian target distribution) it has an analytic closed-form.
For more general target distributions, we can instead evaluate the value function eq (7) via empir-
ical samples using Monte Carlo. The approach is essentially importance sampling whose proposal
distribution is the uncontrolled dynamics. However, this approach has two drawbacks. First, it is
known that the estimation variance can be intolerably high when the proposal distribution is not
close enough to the target distribution (MacKay, 2003). Second, even if the variance is acceptable,
without a good proposal, the required samples size increases exponentially with dimension, which
prevents the algorithm from being used in high or even medium dimension settings (Neal, 2001).
To overcome the above shortcomings, we parameterize the control policy with a neural network uθ .
We seek a control policy that minimizes the cost
u* = arg minEQu Z 1 Ilut『dt + log μ-(xτ)- .	(14)
U	0Jo 2	μ(xτ)
The formula eq (14) also serves as distance metric between uθ and U as in eq (13).
Gradient-informed Policy Representation: It is believed that proper prior information can signifi-
cantly boost the performance of neural network (Goodfellow et al., 2016). The score V log μ(x) has
been used widely to improve the proposal distribution in MCMC (Li et al., 2020b; Hoffman & Gel-
man, 2014) and often leads to better results compared with proposals without gradient information.
In the same spirit, We incorporate V log μ(x) and parameterize the policy as
ut(x) = NNι(t, x) + NN2(t) X Vlogμ(x),	(15)
where NN1 and NN2 are two neural networks. Empirically, we also found that the gradient informa-
tion leads to faster convergence and smaller discrepancy DKL(Qu∣∣Q*)∙ We remark that PIS with
policy eq (15) can be viewed as a modulated Langevin dynamics (MacKay, 2003) that achieves μ
within finite time T instead of infinite time.
Optimize Policy: Optimizing uθ requires the gradient of loss in eq (14), which involves ut and the
terminal state xT . To calculate gradients, we rely on backpropagation through trajectories. We train
the control policy with recent techniques of Neural SDEs (Li et al., 2020a; Kidger et al., 2021), which
greatly reduce memory consumption during training. The gradient computation for Neural SDE is
based on stochastic adjoint sensitivity, which generalizes the adjoint sensitivity method for Neural
ODE (Chen et al., 2018). Therefore, the backpropagation in Neural SDE is another SDE associated
with adjoint states. Unlike the training of traditional deep MLPs which often runs into gradient
vanishing/exploding issues, the training of Neural SDE/ODE is more stable and not sensitive the
number of discretization steps (Chen et al., 2018; Kidger et al., 2021). We augment the origin SDE
with state R0 1 ∣∣us ∣∣2 ds such that the whole training can be conducted end to end. The full training
procedure in provided in Algorithm 1.
Wasserstein distance bound: The PIS trained by Algorithm 1 can not generate unbiased samples
from the target distribution μ for two reasons. First, due to the non-convexity of networks and
randomness of stochastic gradient descent, there is no guarantee that the learned policy is optimal.
Second, even if the learned policy is optimal, the time-discretization error in simulating SDEs is in-
5
Published as a conference paper at ICLR 2022
evitable. Fortunately, the following theorem quantifies the Wasserstein distance between the sampler
and the target density. (More details and a formal statement can be found in appendix C)
Theorem 2 (Informal). Under mild condition, with sampling step size ∆t, if ∣∣ujc 一 口『 ≤ de for
any t, then
W2(Qu(xτ ),μ(xτ)) = O(pTd(∆t + e)).	(16)
3.4	Importance Sampling
The training procedure for PIS does not guarantee its optimality. To compensate for the mismatch
between the trained policy and the optimal policy, we introduce importance weight to calibrate
generated samples. The importance weight can be calculated by (more details in appendix B)
Wu(T)=dQu(τ)=exp(Z0 一2 kutk2dt 一 UtdWt 一 ψ(χτ)).
(17)
Algorithm 2 Sampling
We note eq (17) resembles training objective eq (14).
Indeed, eq (14) is the average of logarithm of eq (17).
If the trained policy is optimal, that is, Qu = Q*, all
the particles share the same weight. We summarize the
sampling algorithm in Algorithm 2.
Effective Sample Size: The Effective Sample
Size (ESS), ESSu = EQu [1Wu)2], is a popular metric to
measure the variance of importance weights. ESS is of-
ten accompanied by resampling trick (Tokdar & Kass,
2010) to mitigate deterioration of sample quality. ESS
Input: Vector: χ0 = 0, Scalar: y0 = 0
Output: Samples with weights
for i — 1 to N do
∆t = ti — ti-ι, ∆w 〜N(0, ∆tI),
χi = χi-1 + U∆t + ∆W
y = yi-1 + u0∆w + 2 ∣∣u∣2 ∆t
end for
Outputs:
χN, eχp(-yN - log ⅞XN?)
is also regarded as a metric for quantifying goodness of sampler based on importance sampling.
Low ESS means that estimation or downstream tasks based on such sampling methods may suffer
from a high variance. ESS of most importance samplers is decreasing along the time. Thanks to the
adaptive control policy in PIS, we can quantify the ESS of PIS based on the optimality of learned
policy. For the sake of completeness, the proof in provided in appendix D.
Theorem 3	(Corollary 7 (Thijssen & Kappen, 2015)). If maxt,χ IIUt(X) — UJc(x)∣∣2 ≤ T, then
—ɪ	≥ 1 — e.
EQu [(wu)2] ≥
Estimation of normalization constants: In most sampling problems we only have access to the
target density UP to a normalization constant, denoted by μ = Zμ. PIS can still generate samples
following the same protocol with new terminal cost Ψ = log μ = Ψ — log Z. The additional con-
stant 一 log Z is independent of xT and thus does not affect the optimal policy and the optimization
of Uθ . As a byproduct, we can estimate the normalization constants (more details in appendix E).
Theorem 4.	For any given policy U, the logarithm of normalization constant is bounded below by
ʌ
EiQu [-Su(τ)] ≤ log Z,
(18)
where Su(T) = R0T 1 ∣∣Ut(xt)∣2 dt + Ut(Xt)dwt + Ψ(xT). The equality holds only when U = u*.
Moreover, for any sub-optimal policy, an unbiased estimation of Z using importance sampling is
ʌ
Z = ET 〜Qu 怕xp(-Su(τ))].
(19)
4 Experiments
In this section we present empirical evaluations of PIS and the comparisons to several baselines.
We also provide details of practical implementations. Inspired by Arbel et al. (2021), we conduct
experiments for tasks of Bayesian inference and normalization constant estimation.
We consider three types of relevant methods. The first category is gradient-guided MCMC methods
without the annealing trick. It includes the Hamiltonian Monte Carlo (HMC) (MacKay, 2003) and
6
Published as a conference paper at ICLR 2022
μ
AFT
SMC
NUTS
HMC
VI-NF
PIS-NN
PIS-Grad
Figure 2: Sampling performance on rings-shape density function with 100 steps. The gradient
information can help PIS-Grad and MCMC algorithm improve sampling performance.
No-U-Turn Sampler (NUTS) (Hoffman & Gelman, 2014). The second is Sequential Monte Carlo
with annealing trick (SMC), which is regarded as state-of-the-art sampling algorithm (Del Moral
et al., 2006) in terms of sampling quality. We choose a standard instance of SMC samplers and the
recently proposed Annealed Flow Transport Monte Carlo (AFT) (Arbel et al., 2021). Both use a
default 10 temperature levels with a linear annealing scheme. We note that there are optimized SMC
variants that achieve better performance (Del Moral et al., 2006; Chopin & Papaspiliopoulos, 2020;
Zhou et al., 2016). Since the introduction of advanced tricks, we exclude the comparison with those
variants for fair comparison purpose. We note PIS can also be augmented with annealing trick, pos-
sible improvement for PIS can be explored in the future. Last, the variational normalizing flow (VI-
NF) (Rezende & Mohamed, 2015) is also included for comparison. We note that another popular
line of sampling algorithms use Stein-Variational Gradient Descent (SVGD) or other particle-based
variational inference approaches (Liu & Wang, 2016; Liu et al., 2019). We include the comparison
and more discussions on SGVD in appendix F.3 due to its significant difference. In our experiments,
the number of steps N of MCMC algorithms and the number of SDE time-discretization steps for
PIS work as a proxy for benchmarking computation times.
We also investigate the effects of two different network architectures for Path Integral Sampler. The
first one is a time-conditioned neural network without any prior information, which we denote as
PIS-NN, while the second one incorporates the gradient information of the given energy function
as in eq (15), denoted as PIS-Grad. When we have an analytical form for the ground truth optimal
policy, the policy is denoted as PIS-GT. The subscript RW is to distinguish PIS with path integral
importance weights eq (17) that use eq (19) to estimate normalization constants from the ones with-
out importance weights that use the bound in eq (18) to estimate Z . For approaches without the
annealing trick, we take default N = 100 unless otherwise stated. With annealing, N steps are
the default for each temperature level, thus AFT and SMC rougly use 10 times more steps com-
pared with HMC and PIS. We include more details about hyperparameters, training time, sampling
efficiency, and more experiments with large N in appendices F and G.
4.1	PIS-Grad vs PIS-NN: Importance of gradient guidance
We observed that the advantage of PIS-Grad over PIS-NN is clearer when the target density has
multiple modes as in the toy example shown in Fig 2. The objective DKL(QI回)is known to have
zero forcing. In particular, when the modes of the density are well separated and Q is not expressive
enough, minimizing DKL(QkQ*) Can drive Q(T) to zero on some area, even if Q*(τ) > 0 (Fox &
Roberts, 2012). PIS-NN and VI-NF generate very similar samples that almost cover half the inner
ring. The training objective function of VI-NF can also be viewed as minimizing KL divergence
between two trajectory distributions (Wu et al., 2020). The added noise during the process can
encourage exploration but it is unlikely such noise only can overcome the local minima. On the
other hand, the gradient information can help cover more modes and provide exploring directions.
4.2	Benchmarking datasets
Mode-separated mixture of Gaussian: We consider the mixture of Gaussian in 2-dimension. We
notice that when the Gaussian modes are not far away from each other, all methods work well.
However, when we reduce the variances of the Gaussian distributions and separate the modes of
Gaussian, the advantage of PIS becomes clear even in this low dimension task. We generate 2000
samples from each method and plot their kernel density estimate (KDE) in Fig 4. PIS generates
samples that are visually indistinguishable from the target density.
7
Published as a conference paper at ICLR 2022
Funnel distribution: We consider the popular testing distribution in MCMC literature (Hoffman &
Gelman, 2014; Hoffman et al., 2019), the 10-dimensional Funnel distribution charaterized by
xo ~ N(0, 9),	xi：9|xo ~ N(0, exp(xo)I).
This distribution can be pictured as a funnel - with x0 wide at the mouth of funnel, getting smaller
as the funnel narrows.
MG (d = 2)	FUnnel (d = 10)	LGCP (d = 1600)
	B	S	A	B	S	A	B	S	A
PISRW -GT	-0.012	0.013	0.018	-	-	-	-	-	-
PIS-NN	-1.691	0.370	1.731	-0.098	5e-3	0.098	-92.4	6.4	92.62
PIS-Grad	-0.440	0.024	0.441	-0.103	9e-3	0.104	-13.2	3.21	13.58
PISRW -NN	-1.192	0.482	1.285	-0.018	7e-3	0.02	-60.8	4.81	60.99
PISRW -Grad	-0.021	0.030	0.037	-0.008	9e-3	0.012	-1.94	0.91	2.14
AFT	-0.509	0.24	0.562	-0.208	0.193	0.284	-3.08	1.59	3.46
SMC	-0.362	0.293	0.466	-0.216	0.157	0.267	-435	14.7	436
NUTS	-1.871	0.527	1.943	-0.835	0.257	0.874	-1.3e3	8.01	1.3e3
HMC	-1.876	0.527	1.948	-0.835	0.257	0.874	-1.3e3	8.01	1.3e3
VI-NF	-1.632	0.965	1.896	-0.236	0.0591	0.243	-77.9	5.6	78.2
Table 1: Benchmarking on mode separated mixture of Gaussian (MG), Funnel distribution and
Log GaUssian Cox Process (LGCP) for estimation log normalization constants. B and S stand for
estimation bias and standard deviation among 100 rUns and A2= B2 + S2.
Log Gaussian Cox Process: We fUrther investigate the normalization constant estimation problem
for the challenging log GaUssian Cox process (LGCP), which is designed for modeling the posi-
tions of Finland pine saplings. In LGCP (Salvatier et al., 2016), an Underlying field λ of positive
real valUes is modeled Using an exponentially-transformed GaUssian process. Then λ is Used to
parameterize Poisson points process to model locations of pine saplings. The posterior density is
、/、	/ (X - μ)T K I(X - μ) ʌ TT /	、	小八、
λ(x)〜exp(---------------2-----------)[]eXp(Xiyi - aexpXi),	(20)
where d denotes the size of discretized grid and yi denotes observation information. The modeling
parameters, inclUding normal distribUtion and α, follow Arbel et al. (2021) (See appendix F).
Tab 1 clearly shows the advantages of PIS for the above three datasets, and sUpports the claim
that importance weight helps improve the estimation of log normalization constants, based on the
comparison between PISRW and PIS. We also foUnd that PIS-Grad trained with gradient information
oUtperforms PIS-NN. The difference is more obvioUs in datasets that have well-separated modes,
sUch as MG and LGCP, and less obvioUs on Unimodal distribUtions like FUnnel.
In all cases, PISRW -Grad is better than AFT and SMC. Interestingly, even without annealing and
gradient information of target density, PISRW -NN can oUtperform SMC with annealing trick and
HMC kernel for the FUnnel distribUtion.
4.3	Advantage of the specialized sampling algorithm
From the perspective of particles dynamics, most existing MCMC algorithms are invariant to the
target distribUtion. Therefore, particles are driven by gradient and random noise in a way that is
independent of the given target distribUtion. In contrast, PIS learns different strategies to combine
gradient information and noise for different target densities. The specialized sampling algorithm
can generate samples more efficiently and shows better performance empirically in oUr experiments.
The advantage can be showed in varioUs datasets, from Unimodal distribUtions like the FUnnel dis-
tribUtion to mUltimodal distribUtions. The benefits and efficiency of PIS are more obvioUs in high
dimensional settings as we have shown.
4.4	Alanine dipeptide
BUilding on the sUccess achieved by flow models in the generation of asymptotically Unbiased sam-
ples from physics models (LeCUn, 1998), we investigate the applications in the sampling of molec-
8
Published as a conference paper at ICLR 2022
KL*	μ	φ	ηι	ψ	η2	η3
VI-NF	175.6 ±4.5	24.2± 4.1	3.1 ± 0.05	14.6± 6.4	7e-2±5e-3	8.5e-2±3.5e-3
SMC	183.3 ±2.3	18.3± 2.1	0.32 ± 0.08	9.6 ± 1.2	0.12±0.05	0.15 ± 9e-3
SNF	181.8 ±0.75	6.3± 0.71	0.17±0.05	1.58 ± 0.36	0.11± 0.03	8.8e-2 ±8e-3
PIS-NN	171.3 ±0.61	5.2± 0.35	0.32±0.03	1.03 ± 0.23	5e-2±5e-3	8.7e-2±3e-3
Table 2: KL-divergences comparison among variational approaches of generated density with target
density in overall atom states distribution and five multimodal torsion angles. We emphasis KL*
denote the KL divergence between unnormalized distribution due to lack of ground truth normaliza-
tion constants. Mean and standard deviation are conducted with five different random seeds.
ular structure from a simulation of Alanine dipeptide as introduced in Wu et al. (2020). The target
density of molecule is μ = exp(-E(x∣065]) - 2 版做⑶]∣∣ ).
We compare PIS with popular variational approaches used in generating samples from the above
model. More specifically, we consider VI-NF, and Stochastic Normalizing Flow (SNF) (Wu
et al., 2020). SNF is very close to AFT (Arbel et al., 2021). Both of them cou-
ple deterministic normalizing flow layers and MCMC blocks except SNF uses an amor-
tized structure. We include more details of MCMC kernel and modification in appendix F.
We show a generated molecular in Fig 3 and quantitative comparison Figure 3: Sampled Alanine
in terms of KL divergence in Tab 2, including overall atom states dis- dipeptide molecules
tribution and five multimodal torsion angles (backbone angles φ, ψ
In this experiment we investigate sampling in the latent space of a trained Variational Autoen-
coder (VAE). VAE aims to minimize DκL(q(x)qφ(z∣x)kp(z)pθ(x∣z)), where qφ(z∣x) represents
encoder and pθ for a decoder with latent variable z and data x. We investigate the posterior distri-
bution
Z 〜p(z)pθ(x|z).	(21)
The normalization constant of such target unnormalized density function p(z)pθ (x|z) is exactly the
likelihood of data points pθ(x), which serves as an evaluation metric for the trained VAE.
We investigate a vanilla VAE model trained with
plateau loss on the binary MNIST (LeCun, 1998)
dataset. For each distribution, we regard the aver-
age estimation from 10 long-run SMC with 1000
temperature levels as the ground truth normal-
ization constant. We choose 100 images ran-
domly and run the various approaches on estimat-
ing normalization of those posterior distributions
in eq (21) and report the average performance in
Tab 3. PIS has a lower bias and variance.
5 Conclusion
Table 3: Estimation of logpθ(x) of a trained
VAE.
	B	S	√B2 + S2
VI-NF	-2.3	0.76	2.42
AFT	-1.7	0.95	1.96
SMC	-10.6	2.01	10.79
PISRW -NN	-1.9	0.81	2.06
PISRW -Grad	-0.87	0.31	0.92
Contributions. In this work, we proposed a new sampling algorithm, Path Integral Sampler, based
on the connections between sampling and stochastic control. The control can drive particles from
a simple initial distribution to a target density perfectly when the policy is optimal for an optimal
control problem whose terminal cost depends on the target distribution. Furthermore, we provide a
calibration based on importance weights, ensuring sampling quality even with sub-optimal policies.
Limitations. Compared with most popular non-learnable MCMC algorithms, PIS requires training
neural networks for the given distributions, which adds additional computational overhead, though
this can be mitigated with amortization. Besides, the sampling quality of PIS in finite steps depends
on the optimality of trained network. Improper choices of hyperparameters may lead to numerical
issues and failure modes as discussed in appendix G.1.
9
Published as a conference paper at ICLR 2022
6 Reproducibility Statement
The detailed discussions on assumptions and proofs of theorems presented in the main paper are
included in appendices A and C to E. The training settings and implementation tips of the algorithms
are included in appendices F and G. An implementation based on PyTorch (Paszke et al., 2019) of
PIS can be found in https://github.com/qsh-zh/pis.
Acknowledgments
The authors would like to thank the anonymous reviewers for useful comments. This work is par-
tially supported by NSF ECCS-1942523 and NSF CCF-2008513.
References
Michael Arbel, Alexander GDG Matthews, and Arnaud Doucet. Annealed flow transport Monte
Carlo. arXiv Preprint arXiv:2102.07501, 2021.
Dimitri P Bertsekas et al. Dynamic programming and optimal control: VoL 1. Athena scientific
Belmont, 2000.
Ricky T. Q. Chen, Jens Behrmann, David Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for
invertible generative modeling. In AdvanCes in NeuraI InfOrmatiOn PrOCessing Systems, 2019.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differ-
ential equations. arXiv preprint arXiv:1806.07366, 2018.
Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. On the relation between optimal transport
and Schrodinger bridges: A stochastic control viewpoint. JOUrnaI of OptimizatiOn TheOry and
AppIiCations, 169(2):671-691, 2016.
Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Stochastic control liaisons: Richard
Sinkhorn meets Gaspard Monge on a SchrOdinger bridge. SIAM Review, 63(2):249-313, 2021.
Nicolas Chopin and Omiros Papaspiliopoulos. An introduction to sequential Monte Carlo. Springer,
2020.
Rob Cornish, Anthony Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity
constraints with continuously indexed normalising flows. In International COnferenCe on MaChine
Learning, pp. 2133-2143. PMLR, 2020.
Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. AppIied mathematics
and Optimization, 23(1):313-329, 1991.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. JOUrnaI of the
Royal StatistiCaI Society: Series B (StatistiCaI Methodology), 68(3):411436, 2006.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A
Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al.
Openmm 7: Rapid development of high performance algorithms for molecular dynamics. PLoS
computational biology, 13(7):e1005659, 2017.
Bradley Efron. Tweedie,s formula and selection bias. JOUrnaI of the AmeriCan StatistiCaI
Association, 106(496):1602-1614, 2011.
Ronen Eldan, Joseph Lehec, and Yair Shenfeld. Stability of the logarithmic Sobolev inequality
via the FOllmer process. In AnnaIes de l,InstitUt Henri POinCare, PrObabiIites et Statistiques,
volume 56, pp. 2253-2269. Institut Henri Poincare, 2020.
Lawrence C Evans. Partial differential equations. GradUate studies in mathematics, 19(4):7, 1998.
10
Published as a conference paper at ICLR 2022
Charles W Fox and StePhen J Roberts. A tutorial on variational Bayesian inference. ArtificiaI
intelligence review, 38(2):85-95, 2012.
Christina Gao, Joshua Isaacson, and Claudius Krause. i-flow: High-dimensional integration and
sampling with normalizing flows. Machine Learning: Science and Technology, 1(4):045023,
2020a.
Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based
models by diffusion recovery likelihood. arXiv PrePrint arXiv:2012.08125, 2020b.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. DeeP Iearning, volume 1.
MIT Press,2016.
Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In International
Conference on Machine Learning, pp. 1292-1301. PMLR, 2017.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD:
Free-form continuous dynamics for scalable reversible generative models. arXiv PrePrint
arXiv:1810.01367, 2018.
Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, and Srinivas
Vasudevan. Neutra-lizing bad geometry in Hamiltonian Monte Carlo using neural transport. arXiv
PrePrint arXiv:1903.03704, 2019.
Matthew D Hoffman and Andrew Gelman. The No-U-Turn samPler: adaPtively setting Path lengths
in Hamiltonian Monte Carlo. JoUrnal OfMachine Learning ReSearch, 15(1):1593-1623, 2014.
Jian Huang, Yuling Jiao, Lican Kang, Xu Liao, Jin Liu, and Yanyan Liu. SchrOdinger-Follmer
sampler: Sampling without ergodicity. arXiv PrePrint arXiv:2106.10880, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv PrePrint arXiv:1806.07572, 2018.
Patrick Kidger, James Foster, Xuechen Li, and Terry Lyons. Efficient and accurate gradients for
neural sdes. arXiv PrePrint arXiv:2105.13493, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv PrePrint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv PrePrint
arXiv:1312.6114, 2013.
Yann LeCun. The mnist database of handwritten digits. http://yann.lecun. com/exdb/mnist/, 1998.
Christian Leonard. A survey of the Schrodinger problem and some of its connections with optimal
transport. DiScrete & ContinUoUS Dynamical Systems, 34(4):1533, 2014.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. In InternationaI Conference on ArtificiaI Intelligence and
StatiStics, pp. 3870-3882. PMLR, 2020a.
Zengyi Li, Yubei Chen, and Friedrich T Sommer. A neural network mcmc samPler that maximizes
proposal entropy. arXiv PrePrint arXiv:2010.03587, 2020b.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and acceler-
ating particle-based variational inference. In InternationaI Conference on Machine Learning, pp.
40824092. PMLR, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general PurPose Bayesian inference
algorithm. arXiv PrePrint arXiv:1608.04471, 2016.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrePancy for goodness-of-fit tests.
In International conference on machine Iearning, pp. 276-284. PMLR, 2016.
11
Published as a conference paper at ICLR 2022
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
press, 2003.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
Preprint arXiv:1610.03483, 2016.
Jesper M0ller, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen. Log Gaussian cox
processes. SCandinaVian joUrnal of StatiStics, 25(3):451—482, 1998.
Radford M Neal. Annealed importance sampling. StatiStiCS and computing, 11(2):125-139, 2001.
Kim A Nicoli, Shinichi Nakajima, Nils Strodthoff, Wcjciech Samek, Klaus-Robert Muller, and
Pan Kessel. Asymptotically unbiased estimation of physical observables with neural samplers.
PhySiCal ReVieW E, 101(2):023304, 2020.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. arXiv PrePrint arXiv:1904.09770, 2019.
Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457), 2019.
Bernt 0ksendal. Stochastic differential equations. In StOChaStiC differential equations, pp. 65-84.
Springer, 2003.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. AdVanCeS in neural information PrOCeSSing systems, 32:
8026-8037, 2019.
Michele Pavon. Stochastic control and nonequilibrium thermodynamical systems. APPIied
MathematiCS and Optimization, 19(1):187-202, 1989.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
InternatiOnal COnferenCe on MaChine Learning, pp. 1530-1538. PMLR, 2015.
John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming in
python using pymc3. PeerJ COmPUter Science, 2:e55, 2016.
Simo Sarkka and Arno Solin. APPIied StOChaStiC differential equations, volume 10. Cambridge
University Press, 2019.
Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-MC: Adversarial training for MCMC.
arXiv PrePrint arXiv:1706.07561, 2017.
Span Spanbauer, Cameron Freer, and Vikash Mansinghka. Deep involutive generative models for
neural mcmc. arXiv PrePrint arXiv:2006.15167, 2020.
Kunal Talwar. Computational separations between sampling and optimization. arXiv PrePrint
arXiv:1911.02074, 2019.
Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimensional domains. arXiv PrePrint
arXiv:2006.10739, 2020.
Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control ap-
proach to reinforcement learning. The JOUmaI of MaChine Learning ReSearch, 11:3137-3181,
2010.
Evangelos A Theodorou and Emanuel Todorov. Relative entropy and free energy dualities: Connec-
tions to path integral and KL control. In 2012 ieee 51st ieee conference on decision and control
(cdc), pp. 1466-1473. IEEE, 2012.
Sep Thijssen and HJ Kappen. Path integral control and state-dependent feedback. PhySiCal ReVieW
E, 91(3):032104, 2015.
12
Published as a conference paper at ICLR 2022
Michalis Titsias and Petros Dellaportas. Gradient-based adaptive Markov chain Monte Carlo.
Advances in NeUral Information Processing Systems, 32:15730-15739, 2019.
Surya T Tokdar and Robert E Kass. Importance sampling: a review. Wiley Interdisciplinary
Reviews: CompUtational Statistics, 2(1):54-60, 2010.
Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative
models with latent diffusions. In Conference on Learning Theory, pp. 3084-3114. PMLR, 2019.
Hao Wu, Jonas Kohler, and Frank Noe. Stochastic normalizing flows. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in NeUraI Information Processing Systems,
volume 33, pp. 5933-5944. Curran Associates, Inc., 2020.
Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in NeUral Information
Processing Systems, 34, 2021.
Yan Zhou, Adam M Johansen, and John AD Aston. Toward automatic model comparison: an
adaptive sequential Monte Carlo approach. JoUrnaI of ComPUtational and GraPhicaI Statistics, 25
(3):701-726, 2016.
13
Published as a conference paper at ICLR 2022
A Proof of Theorem 1
Before proving our main theorem, we introduce the following important lemma.
Lemma 4.1 (Dai Pra (1991); Pavon (1989)). The transition density associated with optimal control
policy u* for eq (2) and eq (3) is
QS,t(χ, y) = Q0,t(χ, y)窸,	(22)
where Qsu,t(x, y) denotes the transition probability from state x at time s to state y at time t.
Proof of Theorem 1: We denote the initial Dirac distribution by V = δχ0. Combining eq (7) and
Vt(χ) = - log φt (χ) we obtain
V0(X0) = - log Eqo [exp(-Ψ(xτ))]
-Mz μ dμ0)=
0.
Therefore, We can evaluate the KL divergence between Q* and Q0 (T∣xτ)μ(xτ) as
DKL(Q*(τ)kQ0(τ|XT)μ(xτ)) = Eτ~Q*[/2 ku*k2
dt + Ψ(xT)]
V0(X0) = 0.
The first equality is based on eq (13). Next, we show that XT 〜μ. The above equations imply
Q*,t(X0, y)dy = exp(-Ψ(y))μ0(dy). Itfollows that
P[xT ∈ A] = Q Q0,t(X0, y)dy = [ eχp(-Ψ(y))μ0(dy) = μ(A).
AA
B Proof of importance weights
By definition
Wu(T )= dQ*(τ) = dQ*(τ) dQ0(τ)
dQu(τ)	dQ0(τ) dQu(τ).
Plugging eq (11) and eq (22) into the above we obtain
(23)
wu(τ) = exp( Z -1 ∣∣u⅛k2 dt-u；dwt- log
02
μ0(xτ)
μ(XT)
)=exp(∕ -2 I∣utk2 dt-utdwt-Ψ(xτ)).
C Proof of Theorem 2
C.1 Lipchitz conditon and Preliminary lemma
To ease the burden of notations, we assume x0 〜δ0. Our conclusion and proof can be generalized to
other Dirac distributions easily. We start by assuming some conditions on the Lipschitz of optimal
policy u*. It promises the existence of a unique strong solution with XT 〜μ. The conditions
and properties are studied in Schrodinger-Follmer process, which dates back to the Schrodinger
problem. For proof of existence of a unique strong solution and detailed discussion, we refer the
reader to Dai Pra (1991); Leonard (2014); Eldan et al. (2020); Huang et al. (2021).
Condition 1.
||ut*(x)||22 ≤ C0(1 + ∣x∣2)	(24)
and
IlU*1 (XI)- u*2 (X2)|| ≤ CI(IlXI- x2k + |t1 - t2| 2 ).	(25)
Below is the formal statement of Theorem 2.
Theorem 5. Under Condition 1, with sampling step size ∆t, if Iut* - utI2 ≤ d for any t, then
W2(Qu(xτ ),μ(xτ)) = O(pTd(∆t + e)).	(26)
14
Published as a conference paper at ICLR 2022
We introduce the following lemma before stating the proof for Theorem 2.
Lemma 5.1. (Huang et al., 2021, Lemma A.2.) Assume Condition 1 holds, then the following
inequality holds for Xt generatedfrom u*,
E[kxt2 - xt1 k ] ≤ 4C0 exp(2C0)(C0 + d)(t2 - t1)2 + 2C0(t2 - t1)2 + 2d|t2 - t1 |, t1, t2 ∈ [0, T].
(27)
C.2 Proof of Theorem 2
We denote by x：。？)the trajectory controlled by the optimal policy u*, and by {xt0, xtχ,…,XtN }
the discrete time process with sub-optimal policy u over discrete time {tk} such that t0 = 0, tN = T
and tk - tk-1 = ∆t. The process {xtk} can be extended to continuous time setting as
xtk
k-1
tk
tk-1
utk-1 (xtk-1)ds + dws.
The key of our proof is the bound of xtk - xt* 2 as
xtk - xt*k2
tk
xtk-1 +
tk-1
tk
utk-1(xtk-1)ds +dws - [xt*k-1 +	us*(xs*)ds+dws]
tk-1
≤ ∣∣∣xtk-1 -xt*k-1∣∣∣2+
Ztk
[us*(xs*) - utk-1 (xtk-1)]ds
k-1
+ 2 xtk-1 - xt*k-1
≤ ∣∣xtk-1
tk
[us*(xs*) - utk-1 (xtk-1)]ds
-1
tk
∣∣us*(xs*) - utk-1 (xtk-1)∣∣ ds)2
+2 ∣∣∣Xtk-1 -Xt*k-1∣∣∣ (Z k ∣∣us*(Xs*) - utk-1 (Xtk-1)∣∣ dS)
≤ (1 + α) xtk-1
X*k-ι∣∣2 + (1 + α1)(∕tk ∣∣u*(χ*)-utk-1 (xtk-1)∣∣ ds)2
α	tk-1
≤ (1 + α) xtk-1 - xt*k-1
+ (I +	)(tk - tk-1)( Z	∣∣u*(x*) - utk-ι (Xtk-I) ∣∣2 dS),
tk-1
(28)
where the first and second inequalities are based on the triangle inequality, the third inequality is
based 2ab ≤ αa2 + 1 b2 for any α > 0, and the forth inequality is based on the CaUchy-SchWarz
inequality.
In the folloWing We bound the second term in eq (28) as
∣∣us*(Xs*) - utk-1 (Xtk-1)∣∣2
= ∣∣∣us*(Xs*) -ut*k-1(Xtk-1) +ut*k-1(Xtk-1) - utk-1 (Xtk-1)∣∣∣2
≤ (I + β) ∣∣u*(x*) - u*k-ι (Xtk-I)∣∣ +(I + ?) ∣∣u*k-ι (χtk-ι) - Utk-I (χtk-ι)∣∣
≤ 2C2(1 + β)[∣∣x* - xtk-ι ∣∣2 + |s - tk-1|] + (1 + β ) ∣∣u*k-ι (Xtk-I)- Utk-I (xtk-ι )∣∣ ,
	
	
2
2
15
Published as a conference paper at ICLR 2022
where the first inequality uses 2ab ≤ βa2 + 1 b2 for an arbitrary β > 0 and the second one is based
on eq (25). It follows that
Z	||u；X)- utk-ι (Xtk-I)『ds
tk-1
tk	tk
2C2(I + β) ∣∣xs - Xtk-Ill ds +	2C2 (1 + β)(s - tk-1)ds
tk-1	tk-1
+ /	(1 + I)IIuk-1 (Xtk-I) - utk-1 (Xtk-I )ll ds.
Thus, for stepsize ∆t = tk - tk-1, we establish
Z I|u；X)- utk-ι (Xtk-J∣∣2 ds
tk-1
tk
2C2(1 + β) ||X： - Xtk-ι∣∣2 ds
tk-1
+ C2 (I + βDt2 + (I + β) Hutk-1 (Xtk-I)- Utk-I (Xtk-I) H δ%.
(29)
Next we bound ∣∣X* - Xtk-ι ∣∣2 in eq (29) as
∣∣x; - Xtk-11∣2 ≤ (1 + η) h - X；k-11∣2 + (1 + ：) Kk-1 - Xtk-11∣2,	(30)
where the inequality is based on (a + b)2 ≤ (1 + η)a2 + (1 + 1 )b2 for an arbitrary η > 0.
Plugging eq (30) into eq (29) yields
Z UuKx;)- utk-1 (Xtk-j∣∣2ds
tk-1
≤ (I + β) ||u；k-i (Xtk-I)- utk-1 (Xtk-I)Il ∆t + C2 (I + e)4t2
+2C2(1+β)(1+η) ||X；k-i-Xtk-il∣	∆t+2C2(1+β)(1+n)/	IXs;	- Xt;k-1	∣∣∣	ds.
k-1	(31)
Plugging eq (29) and (31) into eq (28) yields
LHS ≤[1+α+2C2 (1+α )(1+β )(1+万壮阴卜;…一Xtk-i ∣∣
+ 2C2(1 + α)(1+e)(I+ηN/	||x； - X；k-i ∣∣ ds
+ (1 + α)(1 + β) ||u；k-i(X；k-i)-utk-1 (xtk-ι)∣∣ △" +(1 + α)C2(1+βot3.
(32)
Invoking lemma 5.1, we obtain
tk
E[
tk-1
Xs; - Xt; ∣∣∣2 ds] ≤ 4C0 exp(2C0)(C0 + d)∆t3 + 2C0∆t3 + 2d∆t2.
Taking the expectation ofeq (32), in view of the above and the assumption on control, we establish
E[∣∣Xtk - x；k l∣2] ≤ C3E[∣∣Xtk-1 - x；k-11|2] + C4,
16
Published as a conference paper at ICLR 2022
where C3 = [1 + α + 2C2(1 + 1 )(1 + β)(1 + 1 )∆t2], and
C4 =(1 + -)(1 + 1)de∆t2 + (1 + -)C2(1 + β)∆t3
αβ	α
+ 2C2(1 + 1)(1 + β )(1+ η)[4Co exp(2C0)(C0 + d)∆t3 + 2Co∆t3 + 2d∆t2]∆t.
α
Finally, in view of the fact x° = x0 and fixed step size ∆t, We conclude that by the choice a =
C1∆t, β = η = 1
T
C	.e	CF - 1	...........
E[kxτ — XTk2] ≤ C~~rC4 = O(dT(∆t + *,	(33)
C3 - 1
T
where the last inequality is based on ignoring the high order terms, C∆t - 1 ≤ O(T) and CCL1 ≤
O(d(∆t + )).
D Proof of Theorem 3
The proof is a natural extension of Corollary 7 in Thijssen & Kappen (2015).
We define random variable
Su(t) = Zt kuskɪds + UsdWS + Ψ(XT),
02
(34)
and
Φ(t) =exp(-Su(0)+Su(t)).	(35)
Lemma 5.2. (Thijssen & Kappen, 2015, Lemma 4) For any feasible control policy U for stochastic
optimal control problem,
Φ(T )φt(xτ) - Φ(t)φt(xt) = / Φ(s)φt (Xs)(u* - US )0dWs.
(36)
Corollary 1.
M k2
2
φt(x) = Eqo[exp(-Ψ(xτ))∣xt = x] = EQu[exp(-Ψ(xτ)-
ZtT
ds)|xt = x]
(37)
Proof. This follows importance sampling with density ratio from eq (11).	□
Proof of Theorem 3: We denote the important weight by wu; note itis a random variable. It follows
that
Wu =	exp(-ψ(XT) - R(T(kuf dt + UOdW))	=	exp(-Su(t))	(38)
EQu [exp(-Ψ(xτ) — RT(ku22k-dt + u0dw))]	EQu [exp(-Su(t))]
Dividing the LHS ofeq (36) by φ( (x( ) we obtain
Φ(T)φt(xτ) - Φ(t)φt(xt) = Φ(T)φt(xτ) - φo(xo)
Φ0(x0)	EQu [exp(-S u(0))]
Wu - EQu [Wu].
(39)
Therefore, the variance of Wu equals
EQu [(wu - EQu [wu])2] = EQu [( ZT φ7φs(XS) (U - Us)0dw)2]
( φ( (x( )
=EQu [ ZT φ2(s)φ2(XS) (U； - Us)0(u； - Us)ds]
( φ( (x( )
T
= EQu [	(wuφS(xS) exp(Su(s)))2(US； - US)0(US； - US)ds].
(
(40)
17
Published as a conference paper at ICLR 2022
By Jensen’s inequality
φ(s, xs)2 = (EQu[exp(-Su(s))|xs])2 ≤ EQu[exp(-2Su(s))|xs].
Plugging the above inequality into eq (40), we reach the upper bound of variance
EQu [(wu — EQu [wu])2] ≤ Z EQu [(u； - us)0(u* - us)(wu)2]ds.
0
In view of the fact Var(wu) + 1 = EQu [(wu)2], we arrive at
1 + EQu [(wu)2] ≤ EQu [(wu)2] Z EQu [(us； - us)0(us； - us)]ds.
0
(41)
If we consider the near optimal policy such that maxt,χ Ilut(X) - Ujc(x)∣∣2 ≤ T, then it follows that
1
(42)
-----------≥ 1 - €.
EQu [(wu)2] ≥
E Proof of Theorem 4
We consider the KL divergence between trajectory distribution resulted from the policy u and the
one from optimal policy u； :
DKL(Qu(T )∣∣Q*(τ)) = DKL(Qu(T )kQ0(τ) μ0g))
Ju [∕0 T 2
EiQu [∕0	2
IutI2 dt + u0tdwt + Ψ(XT)]
..9 ,	,-	ʌ ,	、 一	一 r
kutk dt + ujdwt + ψ(xτ) + logZ]
ʌ
ET〜Qu [Su(τ) + log Z]
≥ 0.
The last inequality is based on the fact DKL (Qu (T)IQ； (T)) ≥ 0 and the equality holds only when
u = u；, pointing to
ʌ
0 = ET 〜Q*[Su(τ ) + log Z].
Therefore, we can estimate the normalization constant by
ʌ ʌ
Z ≥ exp(-Et〜Qu [Su(τ)]), Z = exp(-Et〜Q* [Su(τ)]).	(43)
Next we provide an unbiased estimation with sub-optimal policy u based on importance sampling
as
1
E [Q* (T )1
ET ~Qu[ QU(T)]
Q； (T)
ET〜Qu [eχp(log Qu(T))]
ʌ
ET〜Qu [exp(-Su(t) — log Z)].
The last equality is based on the fact ET〜Qu [ Qu(T) ] = RT Q*(t)dT = 1. Hence, We obtain an
unbiased estimation of the normalization constant as
ʌ
Z = ET 〜Qu 怕xp(-Su(T))].
F Experiment details and discussions
18
Published as a conference paper at ICLR 2022
log μ
PIS
AFT
SMC
NUTS
HMC
VI-NF
•by
Figure 4:	Sampling performance on a challenging 2D unnormalized density model with well-
separated modes. Kernel density estimation plots are compared with 2k samples. AFT and SMC
use annealing trick with 10 decreasing temperate levels and HMC kernel following (Arbel et al.,
2021). Even without annealing trick and resampling, Path Integral Sampler (PIS) generates visually
indistinguishable samples from target density with 100 steps. PIS starts x0 from origin point while
others start from a standard Gaussian. The underlying distribution is chosen deliberately to distin-
guish the performance of different methods. In particular, 100 steps are not sufficient for general
MCMC to converge to the stationary distribution. We also note performance of compared methods
can be further improved with tuning temperature scheduling, samples initialization. Our generic
algorithm can explore more modes with similar initialization and less tuning parameters.
F.1 Training time, memory requirements and sampling efficiency
The PIS can be trained once before sampling and did not contribute to the runtime in sampling
computation. Most MCMC methods do not have learnable parameters. However, PIS policy is
trained once and used everywhere. Thus, the training time can be amortized when PIS policy is
deployed in generating a large number of samples. The training time of PIS highly depends on
efficiency of training NeuralSDEs. One future direction is to investigate additional regularizations
and structured SDEs to speed up the training. The PIS algorithm is implemented in PyTorch (Paszke
et al., 2019). We use Adam optimizer (Kingma & Ba, 2014) in all experiments to learn optimal
policy with learning rates 5 × 10-3 and other default hyperparameters. All experiments are trained
with 30 epochs and 15000 points datasets. Loss in most experiments plateau after 3 epochs, some
even 1 epoch. Experiments are conducted using an NVIDIA A6000 GPU. Training one epoch on
2d example takes around 15 seconds for PIS-NN and 30 seconds for PIS-Grad, 1.6 minutes and 1.8
minutes respectively on Funnel (d = 10), and 7 minutes and 9 minutes on LGCP (d = 1600). We
note both PIS and AFT can be trained once and used everywhere. Therefore, the training time can
be amortized when the PIS policy is deployed in generating a large number of samples. We further
compare empirical sampling time for various approaches in Tab 4.
In the high dimensional data, thanks to the efficient adjoint SDE solver, we do not need to cache the
whole computational graph and the required memory is approximately the cost associated with one
forward and backward pass of ut (x) network. In our experiments, the total consumed memory is
around 1.5GB for the toy and Funnel example, and around 5GB for the LGCP.
method	Sampling Time	Training Time
AFT	^^352.2ms	711.2ms
SMC	110.8 ms	
PIS-NN	16.8 ms	30.3 ms
PIS-Grad	34.3 ms	61.2 ms
SNF	130.6 ms	256.1 ms
Table 4: Sampling and training efficiency comparison. For sampling, we measure the consumed
time for generating 2000 particles with each method for 100 times in lower dimensional data, 2-D
points datasets, and report average time for each method. We also include one batch training time
for AFT and PIS.
F.2 Network initialization
19
Published as a conference paper at ICLR 2022
In most experiments in the paper, we found PIS with the default setting, T = 1 and zero control
initialization, gives reasonable performance. One exception is LGCP, where we found training with
T = 1 sometimes suffers from numerical issues and gives NAN loss. We sweep T = 1, 2, 5 and
found T = 5 gives encouraging results. As We discussed in appendix G, the optimal policy U
depends on T and large T not only results in a large cover area but also a “smoother" u*. For neural
netWork Weight initialization, We use the zero initialization for the last layer of parameterized policy
ut (x) and other Weights folloW default initialization in PyTorch. We note there is no guarantee
that PIS can cover all modes initially With only the given unnormalized distribution. HoWever,
the initialization problem exists for most sampling algorithms and MCMC algorithms suffer longer
mixing times compared With the ones With proper initialization (Chopin & Papaspiliopoulos, 2020).
PIS also suffers from improper initialization and We report some failure mode in appendix G.
F.3 Details for compared methods
For all trained PIS and its variants, We use uniform 100 time-discretization steps for the SDEs. Gra-
dient clipping With value 1 is used. A Fourier feature augmentation (Tancik et al., 2020) is employed
for time condition. Throughout all experiments, We use the same netWork With the modified first
layer and last layer for different input data shape. In one pass ut(x), We augmented scalar t With
Fourier feature to 128 dimension, folloWed by a 2 linear layers to extract 64 dimension signal fea-
ture. We use 2 layer linear layers to extract 64 dimension feature for x and the concatenate x, t
features before feeding into another 3 linear layer With 64 hidden neurons to get ut (x). We found
that the training is stable With our simple MLP parametrization in our experiments.
For HMC, We use 10 iterations of Hamiltonian Monte Carlo With 10 leapfrog steps per iterations,
totaling 100 leapfrog steps. For NUTS, We set the maximum depth of the tree built as 5. Note
that samples of HMC and NUTS used in our experiments are from separate trajectories instead of
from one trajectory at different timestamps. We observed that the latter is more likely to generate
samples that concentrate on one single mode. For SMC and AFT, We use 10 transitions With each
transition using the same amount computation as HMC. The settings of SMC and AFT folloW the
official implementation (Arbel et al., 2021) in the released codebase 1. In the Alanine Dipeptide
experiments, for AFT We sWeep adam learning rate lr = 1 × 10-4, 5 × 10-4, 1 × 10-3, 5 × 10-3, 1 ×
10-2 and select 5 × 10-3. Other settings folloW the default setup from the official codebase.
F.4 Comparison with SVGD algorithm
In this section, We present some comparison betWeen celebrated SVGD sampling algorithm (Liu
et al., 2016; 2019) and PIS sampler. SVGD is based on collections of interacting particle, and We
found its performance increase With more samples in a batch as it shoWn in Fig 5. Even With 5000
particles in a bach, sample quality of SVGD is still Worse than PIS-Grad. MeanWhile, the kernel-
based approach pays much more computation results as the number of active particles increases as
We shoW in Tab 5 compared With PIS. In this perspective, PIS enjoy much better scalibility.
method	Time (mean ± std)
SVGD 100 particles SVGD 1000 particles SVGD 5000 particles PIS-NN 5000 particles PIS-Grad 5000 particles	19.3 ms ± 1.2 ms 29.4 ms ± 1.8 ms 434 ms ± 2.43 ms 223 μs ± 14.2 μs 412 μs ± 15.1μs	
Table 5: Consumed time for one iteration of SVGD and one step in PIS in 2D points example.
Sampling With SVGD is less efficient compared With PIS.
1https://github.com/deepmind/annealed_flow_transport
20
Published as a conference paper at ICLR 2022
100 particles	1000 particles	5000 particles
Figure 5:	Generated samples from SVGD (Liu & Wang, 2016) with 100 steps. We generated samples
with batch size 100, 1000, 5000. We find with more particles, samples generated are more closed to
the ground truth data.
F.5 Choice of prior SDE:
We can use a more general SDE
dxt = f (t,xt)dt + g(t, Xt)(utdt + dwt), x° 〜V	(44)
instead of eq (2) for sampling. As discuss in Section 3.3, we prefer use a linear function for f, g
to promise a closed-form μ0. The choice of f, g encodes prior knowledge into dynamics without
control and Q* is determined based on the prior Q0. Intuitively, the ideal Q0 should drive particles
from v to Q0 (xT) that is close to μ. In PIS, our training objective is to fit Q* with parameterized
Qu . Thus training can be easier and faster if Q* and Q0 are close since we use zero control as
initialization for training policy. However, there is no general approach to choose f , g such that
Q0 (XT) is close to μ and Q0 (XT) has a closed form. In this work, We adopt the general form with
f = 0, g = I. It would be interesting to explore other prior dynamics or data-variant f, g in the
future work, e.g., underdamped Langevin.
F.6 Estimation of normalization constants
As discussed in Chopin & Papaspiliopoulos (2020), normalization constants estimation of SMC and
its variants AFT can be achieved with incremental importance sampling weights.
In our experiments we treat HMC and NUTS as special cases of SMC with only two different tem-
perature levels. One corresponds to a standard Gaussian distribution and the other one corresponds
to the target density. Since the initial distribution ν for SMC and NUTs is chosen as standard Gaus-
sian, we can omit the MCMC steps for it and the total computation efforts required for the specific
SMC are for the transitions in HMC and NUTS.
For VI-NF, we use importance sampling
/ μ(x)dx = /
q(x)
IidX = Eq [
where q is the normalized distribution represented by normalizing flows, to provide an unbiased esti-
mation of normalization constants. We use the ELBO in eq (18) for PIS and the unbiased estimation
eq (19) for PISRW.
F.7	2 Dimensional Rings example
The ring-shape density function
min((kxk - 1)2,(kxk - 3)2, (kxk - 5)2)
log μ =-
100
21
Published as a conference paper at ICLR 2022
Consider the special case of gradient informed SDE, which can be viewed as PIS-Grad with a spe-
cific group of parameters,
dxt = V log μ(xt)dt + √2dwt.
This is exactly the Langevin dynamics used widely in sampling (MacKay, 2003). As a special
case of MCMC, Langevin sampling can generate high quality samples given large enough time
interval (MacKay, 2003). From this perspective, PIS-Grad can be viewed as a modulated Langevin
dynamics that is adjusted and represented by neural networks.
F.8 Benchmarking datasets
	B	MG(d=2) S	A	FUnnel(d=10)			LGCP(d=1600)		
				B	S	A	B	S	A
AFT-103	-0.509	0.24	0.562	-0.249	0.0758	0.261	-3.08	1.59	3.46
SMC-103	-0.362	0.293	0.466	-0.338	0.136	0.364	-440	14.7	441
AFT-2 × 103	-0.371	0.477	0.604	-0.249	0.0758	0.261	-1.23	0.826	1.48
SMC-2 × 103	-0.398	0.198	0.444	-0.338	0.136	0.364	-197	5.21	197
AFT-3 × 103	-0.316	0.365	0.483	-0.281	0.0839	0.293	-1.05	0.514	1.17
SMC-3 × 103	-0.137	0.62	0.635	-0.323	0.064	0.329	-109	5.58	109
AFT-5 × 103	-0.194	0.319	0.373	-0.253	0.0397	0.256	-0.949	0.439	1.05
SMC-5 × 103	-0.129	0.246	0.278	-0.298	0.0564	0.303	-37.5	5.04	37.8
AFT-104	-0.03	0.515	0.515	-0.194	0.0554	0.202	-0.827	0.356	0.901
SMC-104	-0.171	0.446	0.477	-0.239	0.0412	0.243	-6.47	1.95	6.76
PIS-102	-0.021	0.03	0.037	-0.008	9e-3	0.012	-1.94	0.91	2.14
Table 6: Long-run MCMC on mode separated mixture of Gaussian (MG), Funel distribution and
Log Gaussian Cox Process (LGCP) for estimating log normalization constants. The suffix denotes
the total number of discrete-time steps for each method, which equals the number of layers multiply
steps per layer. We experiments 10, 20, 30, 50, 100 layers for annealing and 100 leapfrog steps per
layer. As the number of steps increases, the performance of AFT and SMC gradually improves. PIS
denotes the PISRW -Grad. B and S stand for estimation bias and standard deviation among 100 runs
and A2= B2 + S2 .
For mixture of Gaussian, we choose nine centers over the grid {-5, 0, 5} × {-5, 0, 5}, and each
Gaussian has variance 0.3. The small variance is selected deliberately to distinguish the performance
of the different methods. We use 2000 samples for estimating the log normalization constant Z. We
use the standard MLP network to parameterize the control drift ut(x), where the time signal is
augmented by Fourier feature using 64 different frequencies. We use 2 layer (64 hidden neurons in
each layer) MLP to extract features from the augmented time signal and x separately, and another 2
layer MLP to map the summation of features to the policy command. We note that all these methods
for comparison, including HMC, NUTS, SMC, AFT, can reach reasonably good results given large
enough iterations. However, with small finite number of steps, PIS achieves the best performance.
We include more results for long-run MCMC methods in Tab 6.
In the experiment with Funnel distribution, Arbel et al. (2021) suggests to use a slice sampler kernel
for AFT and SMC, which includes 1000 steps of slice sampling per temperature. In Tab 1, we still
use HMC for comparing performance with the same number of integral steps. We also include the
results with slice sampler in Tab 7. We use 6000 particles for the estimation of log normalization
constants. The network architecture of PIS is exactly the same as that in the experiments with
mixture of Gaussian.
In the example with Cox process, the covariance K is chosen as
K(Mv) = 1.91 X exP(-kUv-βvk),
22
Published as a conference paper at ICLR 2022
	B	Funnel(d=10) S	A
AFT-10	0.128	0.376	0.398
SMC-10	-0.193	0.067	0.204
AFT-20	0.0134	0.173	0.174
SMC-20	-0.113	0.0878	0.143
AFT-30	0.074	0.309	0.318
SMC-30	-0.006	0.188	0.188
PISRW -Grad	-0.008	0.009	0.012
Table 7: AFT and SMC with slice sampler kernel. The suffix denotes the number of temperature
levels for annealing. 1000 slicing sampling steps are used for each temperature. Though there is no
annealing and only 100 steps are used, the performance of PIS is competitive.
and the mean vector equals log(126) - σ2 and α = 1/M2. We note that this setting follows Arbel
et al. (2021); M0ller et al. (1998). Totally 2000 samples are used to evaluate the log normalization
constant. We treat the mean of estimation results from 100 repetitions of SMC with 1000 tempera-
tures as ground truth normalization constants. In this experiment, we found clipping gradient from
target density function help stabilize and speed up the training of PIS-Grad. This example is the
most challenging task among the three. One major reason is the high dimensionality of the task; the
PIS needs to find optimal policy u : (t, Rd ) → Rd in high dimensional space. In addition, there
is no prior information that can be used to shrink the search space, which makes the training of PIS
with MLP more difficult. We use 2000 particles for estimation of log normalization constants. We
also include more experiment results in Tab 8.
	B	LGCP(d=1600) S	A
AFT-104	-0.827~~	0.356	0.901
PISRW -Grad-1 × 102	-194	0.91	2.14
PISRW -Grad-5 × 102	-1.25	0.57	1.373
PISRW -Grad-10 × 102	-0.832	0.214	0.859
Table 8: PIS with large number of integral step. The suffix number is the total integral steps. For
AFT-104, we use 100 annealing layers and run 100 leapfrog steps per each annealing layer.
F.9 Alanine dipeptide
The setup for target density distribution and the comparison method are adopted from Wu et al.
(2020). Following Noe et al. (2019), an invertible transformation between Cartesian coordinates
and the internal coordinates is deployed before output the final samples. Then we normalize the
coordinates by removing means and dividing them by the standard deviation of train data. To setup
the target distribution, we simulate Alanine dipeptide in vacuum using OpenMMTools (Eastman
et al., 2017) 2. Total 105 atoms data points are generated as training data. Situation parameters,
including time-step and temperature setting are the same as Wu et al. (2020). We refer the reader to
official codebase for more details of setting target density function 3.
Following the setup in Wu et al. (2020), we use unweighted samples to compute the metrics. KL
divergence of VI-NF on μ is calculated based on ELBO instead of importance sampling as in nor-
malizing constants tasks. We use Metropolis random walk MCMC block for SMC, SNF and AFT
and RealNVP blocks for SNF and AFT (Dinh et al., 2016). For a fair comparison, we use PIS-NN
instead of PIS-Grad since none of the approaches in this example uses the gradient information. We
note that SNF is originally trained with maximizing data likelihood where some empirical samples
2https://github.com/choderalab/openmmtools
3https://github.com/noegroup/stochastic_normalizing_flows
23
Published as a conference paper at ICLR 2022
are assumed to be available. We modify the training objective function by reversing the original KL
divergence as in eq (1).
F.10 More details on sampling in Variational Autoencoder latent space

SVgyS 夕夕
Figure 6:	Origin data images and their reconstructions from trained vanilla VAE. It can be seen that
reconstruction images are smoother compared with the original images.
We use a vanilla VAE architecture to train on binary MNIST data. The encoder uses a standard 3
layer MLP networks with 1024 hidden neurons, and maps an image to the mean and standard devi-
ation of 50 dimension diagonal Normal distribution. The decoder employs 3 layer MLP networks
to decode images from latent states. ReLU nonlinearity is used for hidden layers. For training,
we use the Adam optimizer with learning rate 5 × 10-4, batch size 128. With reparameterization
trick (Kingma & Welling, 2013) and closed-form KL divergence between approximated normal dis-
tribution and standard normalization distribution, we train networks for totally 100 epochs. We show
performance of vanilla in Fig 6.
We parameterize distribution of decoder pθ (x|z) as
logPθ(x|z) = logp(x∣Dθ(Z)) = XlogDθ(z) + (1 - x)log(1 - Dθ(z)).
For PIS, we use the same network and training protocol as that in the experiment for mixture of
Gaussian and Funnel distributions. We also use gradient clip to prevent the magnitude of control
drift from being too large.
G	Technical details and failure modes
G.1 TIPS
Here we provide a list of observations and failure cases we encountered when we trained PIS. We
found such evidences through some experiments, though there is no way we are certain the following
claims are correct and general for different target densities.
•	We notice that smaller T may result in control strategy with large Lipchastiz constants,
which is within expectation since large control is required to drive particles to destina-
tion with less amount of time. It is reported that it is more difficult to approximate large
Lipchastiz functions with neural networks (Jacot et al., 2018; Tancik et al., 2020). We
thus recommend to increase T or constraint the magnitude of u to stablize training when
encountering numeric issue or when results are not satisfactory.
•	We found batch normalization can help stablize and speed up training, and the choice of
nonlinear activation (ReLU and its variants) does not make much difference.
•	We also notice that if the control ut (X) has large Lipchastiz constants in time dimension,
the discretized error would also increase. For calculating the weights based on path integral,
we suggest to decrease time stepsize and increase N when the number of integral steps is
small and discretization error is high.
•	We obtained more stable and smaller training loss when training with Tweedie’s for-
mula (Efron, 2011), but we found no obvious improvements on testing the trained sampler
or estimating normalization constants.
•	Regardless the accuracy and memory advantages of Reversible Heun claimed by
torchsde (Li et al., 2020a; Kidger et al., 2021) , we found this integration approach is
24
Published as a conference paper at ICLR 2022
Figure 7: Generated 5000 uncurated samples with T = 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0.
PIS with small T may miss some modes.
less stable compared with simple Euler integration without adjoint and results in numer-
ical issues occasionally. We empirically found that methods without adjoint are more
stable and lower loss compared with adjoint ones, even in low dimensional data like 2d
points. We use Euler-Maruyama without adjoint for low dimension data and Reversible
Heun method (Kidger et al., 2021) in datasets when required memory is overwhelming to
cache the whole computational graph. We recommend readers to use Ito Euler integra-
tion when memory permits or conduct training with a small ∆t. The more steps in PIS
and smaller ∆t not only reduce policy discretization error Theorem 2, but also reduce the
Euler-Maruyama SDE integration errors.
G.2 failure modes
In this section, we show some failure modes of PIS. With an improper initialization and an extremely
small T , PIS suffers from missing mode. The failure can be resulted from following factors. First,
the untrained and initial p(xT) = N(0, TI) may be far away from the target the distribution modes.
Thus it is extremely difficult for training PIS to cover region of high probability mass under μ.
Second, small T results in policies with large Lipchastic constants that are challenging to train
networks as we discussed in appendix G.1. Third, the policy with limited representation power
tends to miss modes due to the proposed KL training scheme. We show some uncurated samples
trained with different T and other failure cases.
Thoughout our experiments, we also find that large T leads to more stable training and PIS sampler
covers more modes. However large T with large δt deteriorates sample quality due to discretization
error but it can be eased with increasing number of steps.
We note PIS is not a perfect sampler and failure modes exists for all compared methods, experiments
results in Section 4 we reported are based on uncurated experiments.
25
Published as a conference paper at ICLR 2022
(a)T = 10,∆t= 0.1, N = 100
(b)T = 10, ∆t = 0.01, N = 103
Figure 8: Large T with large δt deteriorates sample quality due to discretization error in Fig 8a but
it can be eased with increasing number of steps in Fig 8b.
Figure 9: Another failure case with T = 2, ∆t = 0.02, N = 100 due to randomness of training
networks.
26