Published as a conference paper at ICLR 2022
The Rich Get Richer:
Disparate Impact of Semi-Supervised Learning
ZhaoWei Zhu*,Tianyi Luo*, and Yang Liu
Computer Science and Engineering, University of California, Santa Cruz
{zwzhu,tluo6,yangliu}@ucsc.edu
Ab stract
Semi-supervised learning (SSL) has demonstrated its potential to improve the
model accuracy for a variety of learning tasks when the high-quality supervised
data is severely limited. Although it is often established that the average accuracy
for the entire population of data is improved, it is unclear how SSL fares with
different sub-populations. Understanding the above question has substantial fair-
ness implications when different sub-populations are defined by the demographic
groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of
deploying SSL: the sub-population who has a higher baseline accuracy without us-
ing SSL (the “rich” one) tends to benefit more from SSL; while the sub-population
who suffers from a low baseline accuracy (the “poor” one) might even observe a
performance drop after adding the SSL module. We theoretically and empirically
establish the above observation for a broad family of SSL algorithms, which either
explicitly or implicitly use an auxiliary “pseudo-label”. Experiments on a set of
image and text classification tasks confirm our claims. We introduce a new met-
ric, Benefit Ratio, and promote the evaluation of the fairness of SSL (Equalized
Benefit Ratio). We further discuss how the disparate impact can be mitigated.
We hope our paper will alarm the potential pitfall of using SSL and encourage a
multifaceted evaluation of future SSL algorithms.
1	Introduction
The success of deep neural networks benefits from large-size datasets with high-quality supervisions,
while the collection of them may be difficult due to the high cost of data labeling process (Agarwal
et al., 2016; Wei et al., 2022). A much cheaper and easier solution is to obtain a small labeled
dataset and a large unlabeled dataset, and apply semi-supervised learning (SSL). Although the global
model performance for the entire population of data is almost always improved by SSL, it is unclear
how the improvements fare for different sub-populations, e.g., a multiaccuracy metric (Kim et al.,
2019). Analyzing and understanding the above question will have substantial fairness implications
especially when sub-populations are defined by the demographic groups e.g., race and gender.
Motivating Example Figure 1 shows the change of test accuracy for each class during SSL. Each
class denotes one sub-population and representative sub-populations are highlighted. Figure 1 deliv-
ers two important messages: in SSL, even with some state-of-the-art algorithms: 1) the observation
“rich getting richer” is common, and 2) the observation “poor getting poorer” possibly happens.
Specifically, the “rich” sub-population, such as automobile that has a high baseline accuracy at the
beginning of SSL, tends to be consistently benefiting from SSL. But the “poor” sub-population,
such as dog that has a low baseline accuracy, will remain a low-level performance as Figure 1(a) or
even get worse performance as Figure 1(b). This example shows disparate impact of accuracies for
different sub-populations are common in SSL. In special cases such as Figure 1(b), we observe the
Matthew effect: the rich get richer and the poor get poorer. See more discussions in Appendix B.
In this paper, we aim to understand the disparate impact of SSL from both theoretical and em-
pirical aspects, and propose to evaluate SSL from a different dimension. Specifically, based on
classifications tasks, we study the disparate impact of model accuracies with respect to different
sub-populations (such as label classes, feature groups and demographic groups) after applying SSL.
* Equal contributions. Corresponding author: Yang Liu <yangliu@ucsc.edu>.
1
Published as a conference paper at ICLR 2022
Figure 1: Disparate impacts in the model accuracy of SSL. MixMatch (Berthelot et al., 2019b) is
applied on CIFAR-10 with (a) 250 clean labels in the balanced case (25 labeled instances per class)
and (b) 185 clean labels in the unbalanced case (25 labeled instances in each of the first 5 classes,
12 labeled instances in each of the remaining classes). Other instances are used as unlabeled data.
Different from traditional group fairness (Zafar et al., 2017; Bellamy et al., 2018; Jiang et al., 2022b)
defined over one model, our study focuses on comparing the gap between two models (before and
after SSL). To this end, we first theoretically analyze why and how disparate impact are generated.
The theoretical results motivate us to further propose a new metric, benefit ratio, which evaluates
the normalized improvement of model accuracy using SSL methods for different sub-populations.
The benefit ratio helps reveal the “Matthew effect” of SSL: a high baseline accuracy tends to reach a
high benefit ratio that may even be larger than 1 (the rich get richer), and a sufficiently low baseline
accuracy may return a negative benefit ratio (the poor get poorer). The above revealed Matthew ef-
fect indicates that existing and popular SSL algorithms can be unfair. Aligning with recent literature
on fair machine learning (Hardt et al., 2016; Corbett-Davies & Goel, 2018; Verma & Rubin, 2018),
we promote that a fair SSL algorithm should benefit different sub-populations equally, i.e., achiev-
ing Equalized Benefit Ratio which we will formally define in Definition 1. We then evaluate SSL
using benefit ratios and discuss how two possible treatments, i.e., balancing the data and collecting
more labeled data, might mitigate the disparate impact. We hope our analyses and discussions could
encourage future contributions to promote the fairness of SSL.
Our main contributions and findings are summarized as follows: 1) We propose an analytical frame-
work that unifies a broad family of SSL algorithms, which either explicitly or implicitly use an aux-
iliary “pseudo-label”. The framework provides a novel perspective for analyzing SSL by connecting
consistency regularization to learning with noisy labels. 2) Based on our framework, we further
prove an upper bound for the generalization error of SSL and theoretically show the heterogeneous
error of learning with the smaller scale supervised dataset is one primary reason for disparate im-
pacts. This observation reveals a “Matthew effect” of SSL. 3) We contribute a novel metric called
benefit ratio to measure the disparate impact in SSL, the effectiveness of which is guaranteed by
our proved generalization bounds. 4) We conduct experiments on both image and text classification
tasks to demonstrate the ubiquitous disparate impacts in SSL. 5) We also discuss how the disparate
impact could be mitigated. Code is available at github.com/UCSC-REAL/Disparate-SSL.
1.1	Related Works
Semi-supervised learning SSL is popular in various communities (Dai & Le, 2015; Howard &
Ruder, 2018; Clark et al., 2018; Yang et al., 2019; Sachan et al., 2019; Guo et al., 2020; Cheng et al.,
2021; Wang et al., 2021c; Luo et al., 2021; Huang et al., 2021; Bai et al., 2021; Wang et al., 2020; Liu
et al., 2020). We briefly review recent advances in SSL. See comprehensive overviews by (Chapelle
et al., 2006; Zhu et al., 2003) for traditional methods. Recent works focus on assigning pseudo-
labels generated by the supervised model to unlabeled dataset (Lee et al., 2013; Iscen et al., 2019;
Berthelot et al., 2019b;a), where the pseudo-labels are often confident or with low-entropy (Sohn
et al., 2020; Zhou, 2018; Meng et al., 2018). There are also many works on minimizing entropy of
predictions on unsupervised data (Grandvalet et al., 2005) or regularizing the model consistency on
the same feature with different data augmentations (Tarvainen & Valpola, 2017; Miyato et al., 2018;
Sajjadi et al., 2016; Zhang et al., 2018; Miyato et al., 2016; Xie et al., 2019). In addition to network
inputs, augmentations can also be applied on hidden layers (Chen et al., 2020). Besides, some works
(Peters et al., 2018; Devlin et al., 2018; Gururangan et al., 2019; Chen et al., 2019; Yang et al., 2017)
2
Published as a conference paper at ICLR 2022
first conduct pre-training on the unlabeled dataset then fine-tune on the labeled dataset, or use ladder
networks to combine unsupervised learning with supervised learning (Rasmus et al., 2015).
Disparate impact Even models developed with the best intentions may introduce discriminatory bi-
ases (Raab & Liu, 2021). Researchers in various fields have found the unfairness issues, e.g., vision-
and-language representations (Wang et al., 2022), model compression (Bagdasaryan et al., 2019),
differential privacy (Hooker et al., 2019; 2020), recommendation system (Gomez et al., 2021), in-
formation retrieval (Gao & Shah, 2021), image search (Wang et al., 2021b), machine translation
(Khan et al., 2021), message-passing (Jiang et al., 2022a), and learning with noisy labels (Liu, 2021;
Zhu et al., 2021a; Liu & Wang, 2021). There are also some treatments considering fairness without
demographics (Lahoti et al., 2020; Diana et al., 2021; Hashimoto et al., 2018), minimax Pareto fair-
ness (Martinez et al., 2020), multiaccuracy boosting (Kim et al., 2019), and fair classification with
label noise (Wang et al., 2021a). Most of these works focus on supervised learning. To our best
knowledge, the unfairness of SSL has not been sufficiently explored.
2	Preliminaries
We summarize the key concepts and notations as follows.
2.1	Supervised Classification Tasks
Consider a K-class classification task given a set of NL labeled training examples denoted by
DL ：= {(xn,yn)}n∈[NL], Where [Nl] := {1, 2,…,Nl}, Xn is an input feature, yn ∈ [K] is a la-
bel. The clean data distribution with full supervision is denoted by D. Examples (xn, yn) are drawn
according to random variables (X, Y)〜D. The classification task aims to identify a classifier f
that maps X to Y accurately. Let l{∙} be the indicator function taking value 1 when the speci-
fied condition is satisfied and 0 otherWise. Define the 0-1 loss as 1(f (X), Y) := 1{f(X) 6= Y}.
The optimal f is denoted by the Bayes classifier f * = arg min f ED [1(f (X), Y)]. One com-
mon choice is training a deep neural network (DNN) by minimizing the empirical risk: f =
argminf N PN=I '(f (Xn), yn). Notation '(∙) stands for the cross-entropy (CE) loss '(f (x), y):=
- ln(fx[y]), y ∈ [K], where fx[y] denotes the y-th component of f(x). Notations f and f stand
for the same model but different outputs. Specifically, vector f(X) denotes the probability of each
class that model f predicts given feature X. The predicted label f(X) is the class with maximal
probability, i.e., f(X) := arg maxi∈[K] fx[i]. We use notation f if we only refer to a model.
2.2	Semi-Supervised Classification Tasks
In the semi-supervised learning (SSL) task, there is also an unlabeled (a.k.a. unsupervised) dataset
DU := {(xn+NL, ∙)}n∈[Nu] drawn from D, while the labels are missing or unobservable. Let N :=
NL + NU. Denote the corresponding unobservable supervised dataset by D := {(Xn, yn)}n∈[N] .
Compared with the supervised learning tasks, it is critical to leverage the unsupervised information
in semi-supervised learning. To improve the model generalization ability, many recent SSL meth-
ods build consistency regularization with unlabeled data to ensure that the model output remains
unchanged with randomly augmented inputs (Berthelot et al., 2019b; Xie et al., 2019; Berthelot
et al., 2019a; Sohn et al., 2020; Xu et al., 2021). To proceed, we introduce soft/pseudo-labels y.
Soft-labels Note the one-hot encoding of yn can be written as yn , where each element writes
as yn [i] = 1{i = yn}. More generally, we can extend the one-hot encoding to soft labels by
requiring each element y[i] ∈ [0, 1] and Pi∈[K] y[i] = 1. The CE loss with soft y writes as
`(f (X), y) := - Pi∈[K] y[i] ln(fx[i]). If we interpret y[i] = P(Y = i) as a probability (Zhu et al.,
2022a) and denote by Dy the corresponding label distribution, the above CE loss with soft labels
can be interpreted as the expected loss with respect to a stochastic label Y, i.e.,
队f (x), y) := X P(Y = i)'(f(x), i) = EY〜Dy h'(f(x), Y)i.	(1)
i∈[K]
Pseudo-labels In consistency regularization, by using model predictions, the unlabeled data will
be assigned pseudo-labels either explicitly (Berthelot et al., 2019b) or implicitly (Xie et al., 2019),
where the pseudo-labels can be modeled as soft-labels. In the following, we review both the explicit
and the implicit approaches and unify them in our analytical framework.
3
Published as a conference paper at ICLR 2022
Consistency regularization with explicit pseudo-labels For each unlabeled feature xn , pseudo-
labels can be explicitly generated based on model predictions (Berthelot et al., 2019b). The pseudo-
label is later used to evaluate the model predictions. To avoid trivial solutions where model pre-
dictions and pseudo-labels are always identical, independent data augmentations of feature xn are
often generated for M rounds. The augmented feature is denoted by x0n,m := Augment(xn), m ∈
[M ]. Then the pseudo-label yn in epoch-t can be determined based on M model predictions as
y(t) = Sharpen (吉 PMM=I f ⑴(x；m)) , Where model f(t') is a copy of the DNN at the beginning
of epoch-t but without gradients. The function Sharpen(∙) reduces the entropy of a pseudo-label,
e.g., setting to one-hot encoding ej, j = arg maxi∈[K] yn(t) (Sohn et al., 2020). In epoch-t, With
some consistency regularization loss 'cr(∙), the empirical risk using pseudo-labels is:
1	NL
L1(f,DL, DU) = N X
NL n=1
1 NL +NU
'(f(Xn Mn) + 〒 X 'CR(f(Xn), y^).
NU n=NL+1
Consistency regularization with implicit pseudo-labels Consistency regularization can also be
applied without specifying particular pseudo-labels, where a divergence metric between predictions
on the original feature and the augmented feature is minimized to make predictions consistent. For
example, the KL-divergence could be applied and the data augmentation could be domain-specific
(Xie et al., 2019) or adversarial (Miyato et al., 2018). In epoch-t, the total loss is:
L2(f, DL,DU) =	X '(f(xn), yn) + λ
NL	NU
NL +NU
X 'cr(∕ (xn), fw(x,n)),
n=NL+1
where λ balances the supervised loss and the unsupervised loss, x0n := Augment(xn) stands for one-
round data augmentation (m = 1 following the previous notation x0n,m ). Without loss of generality,
we use λ = 1 in our analytical framework.
Consistency regularization loss 'cr(∙) In the above two lines of works, there are different choices
of 'cr(∙), such as mean-squared error 'cr(∕(x), y) := kf (x) - y∣∣2∕K (Berthelot et al., 2019b;
Laine & Aila, 2016) or CE loss (Miyato et al., 2018; Xie et al., 2019) defined in Eq. (1). For a clean
analytical framework, we consider the case when both supervised loss and unsupervised loss are the
same, i.e., 'cR(f (x), y) = '(f (x), y). Note L2 implies the entropy minimization (Grandvalet et al.,
2005) when both loss functions are cE and there is no augmentation, i.e., x0n = xn .
2.3	Analytical Framework
We propose an analytical framework to unify both the explicit and the implicit approaches. With
Eqn. (1), the unsupervised loss in the above two methods can be respectively written as:
1	NL+NU	1 NL+NU
N X	EY~d “J'(f(Xn),y)] and N X EY-Dm*) K(f(Xn),Y)].
U n=NL+1	yn	U n=NL+1	n
Both unsupervised loss terms inform us that, for each feature xn, we compute the loss with respect
to the reference label Y, which is a random variable following distribution D (t) or Df(t)(χ0). Com-
pared with the original clean label Y, the unsupervised reference label Y contains label noise, and
the noise transition (Zhu et al., 2021b) depends on feature xn. Specifically, we have
P(Y = i|X = Xn) = ynt)[i] (Explicit) or P(Y = i|X = Xn) = fXt)[i] (Implicit).
Then with model f(t'), we can define a new dataset with instance-dependent noisy reference labels:
D = {(xn, yn)}n∈[N ] ,where yfn = yn,∀n ∈ [Nl], and yfn = ynt) or f? ∀n ∈ {N + 1,…，N}.
The unified loss is:
1N	1N
L(f, D ) = N X '(f (xn), yn) = N X EY ~DyJ'(f (xn),Y)].	⑵
n=1 n=1
Therefore, with the pseudo-label as a bridge, we can model the popular SSL solution with consis-
tency regularization as a problem of learning with noisy labels (Natarajan et al., 2013). But different
from the traditional class-dependent assumption (Liu & Tao, 2015; Wei & Liu, 2021; Zhu et al.,
2022b), the instance-dependent pseudo-labels are more challenging (Liu, 2022).
4
Published as a conference paper at ICLR 2022
3	Disparate Impacts of SSL
To understand the disparate impact of SSL, we study how supervised learning with DL affects the
performance of SSL with both DL and DU. In this section, the analyses are for one and an arbitrary
sub-population, thus we did not explicitly distinguish sub-populations in notation.
Intuition The rich sub-population with a higher baseline accuracy (from supervised learning with
DL) will have higher-quality pseudo-labels for consistency regularization, which helps to further
improve the performance. In contrast, the poorer sub-population with a lower baseline accuracy
(again from supervised learning with DL) can only have lower-quality pseudo-labels to regularize
the consistency of unsupervised features. With a wrong regularization direction, the unsupervised
feature will have its augmented copies reach consensus on a wrong label class, which leads to a
performance drop. Therefore, when the baseline accuracy is getting worse, there will be more and
more unsupervised features that are wrongly regularized, resulting in disparate impacts of model
accuracies as shown in Figure 1.
In the following, we first analyze the generalization error for supervised learning with labeled data,
then extend the analyses to semi-supervised learning with the help of pseudo-labels. Without loss
of generality, we consider minimizing 0-1 loss 1(f (X), Y ) with infinite search space. Our analyses
can be generalized to bounded loss '(∙) and finite function space F following the generalization
bounds that can be introduced using Rademacher complexity (Bartlett & Mendelson, 2002).
3.1	Learning with Clean Data
Denote the expected error rate of classifier f on distribution D by RD(f) := ED [1(f (X), Y )].
Let fd denote the classifier trained by minimizing 0-1 loss with clean dataset D, i.e.,
fD	：=	argmmf	RD(f),	where RD(f)	:=	N ∑k∈[n]	1(f (xn),yn).	Denote by	Y*|X	:=
arg maxi∈[K] P(Y|X) the Bayes optimal label on clean distribution D. Theorem 1 shows the gen-
eralization bound in the clean case. Replacing D and N with DL and NL we have:
Theorem 1 (Supervised error). With probability at least 1 一 δ ,the generalization error ofsupervised
learning on clean dataset DL is upper-bounded by RD (/。工)≤ 22loNj∕δ) + P(Y* = Y).
3.2	Learning with Semi-supervised Data
We further derive generalization bounds for learning with semi-supervised data. Following our
analytical framework in Section 2.3, in each epoch-t, we can transform the semi-supervised data
to supervised data with noisy supervisions by assigning pseudo-labels, where the semi-supervised
dataset DL ∪ DU is converted to the dataset De given the model learned from the previous epoch.
Two-iteration scenario In our theoretical analyses, to find a clean-structured performance bound
for learning with semi-supervised data and highlight the impact of the model learned from super-
vised data, we consider a particular two-iteration scenario where the model is first trained to con-
vergence with the small labeled dataset DL and get /。工,then trained on the pseudo noisy dataset
~	.	O
D labeled by /。工.
Noting assigning pseudo labels iteratively may improve the performance as
suggested by most SSL algorithms (Berthelot et al., 2019b; Xie et al., 2019), our considered two-
iteration scenario can be seen as a worst case for an SSL algorithm.
Independence of samples in D Recall x0n denotes the augmented copy of xn . The N instances
in D may not be independent since pseudo-label yen comes from fDL (x0n), which depends on xn .
Namely, the number of independent instances N0 should be in the range of [NL, N]. Intuitively, with
appropriate noise injection or data augmentation (Xie et al., 2019; Miyato et al., 2018) to xn such
that x0n could be treated as independent of xn , the number of independent samples in D could be
improved to N. We consider the ideal case where all N instances are i.i.d. in the following analyses.
By minimizing the unified loss defined in Eq. (2), we can get classifier fDe := arg minf RDe (f),
where RD (f) := N Pn∈[N ](£记的 y[i]∙ 1(f (xn), i)). The expected error given classifier f is
5
Published as a conference paper at ICLR 2022
1	.	1 1	7-> /八	ττn l^F∕,∕"∖	1	.1	FFrj 1	♦, C J	‹rζ
denoted by RDe(f) := EDe [1(f (X), Y )], where the probability density function of distribution D
1	1 i' 1	TΓΛ	/ PT-	∙∖	TΓΛ	/ PT-	∖	~ Γ ■!
can be defined as P(X γe)〜D (X =	xn, Y	= Z)= P(X,Υ)〜D (X	= Xn )	∙ yn [i] .
Decomposition With the above definitions, the generalization error (on the clean distribution) of
1	∙ r∙ 2	111 1	1	I > / p ∖	∕c∕2∖ I > ∕p∖∖.7-⅛ /P \	1	m	Y
classifier fDe could be decomposed as RD (fDe) = (RD (fDe) - RDe(fDe)) + RDe (fDe), where Term-1
|
}
'—{z
Term-1	Term-2
transforms the evaluation of fDe from clean distribution D to the pseudo noisy distribution D. Term-
2 is similar to the generalization error in Theorem 1 but the model is trained and evaluated on noisy
distribution D. Both terms are analyzed as follows.
_	____ -1	_ ._ , ~	___ ________ __________
Upper and Lower Bounds for Term-1 Let η(X) := 1 £髭的 |P(Y = i|X) - P(Y = i|X)|,
_____ _________ , ~_______ 一 一 _ 一 一 ≈ __________________________________ _, _________ ~_________ 一
e(X) := P(Y 6= Ye|X) be the feature-dependent error rate, Aef(X) := P(f (X) = Ye|X) be
the accuracy of prediction f (X) on noisy dataset D. Denote their expectations (over X) by η :=
EX[η(X)], e := EX [e(X)], Af = EX [Af (X)]. To highlight that η and e depends on the noisy
dataset D labeled by /。工,We denote them as η(f°L) and e(fDL). Then We have:
_	一 一	一-	, . ≈	, O	—	，个、 ―「、	, ʌ
Lemma 1 (Bounds for Term-1). (2Aff - 1)e(f0L) ≤ RD(fD) - RD(fD) ≤ η(f0L).
Note the upper bound builds on ηe(fDL ) While the loWer bound relates to ee(fDL ). To compare tWo
bounds and shoW the tightness, We consider the case Where Y |X is confident, i.e., each feature X
belongs to one particular true class Y With probability 1, Which is generally held in classification
problems (Liu & Tao, 2015). Lemma 2 shoWs η(X) = e(X) in this particular case.
Lemma 2 (η vs. e). For any feature X, if Y|X is confident, η(X) is the error rate of the model
prediction on X, i.e., ∃i ∈ [K] : P(Y = i|X) = 1 ⇒ η(X) = P(Ye 6= Y|X) = e(X).
Upper bound for Term-2 Denote by Y*|X := argmaxi∈[κ] P(Y = i|X) the Bayes optimal label
on noisy distribution D. FolloWing the proof for Theorem 1, We have:
Lemma 3 (Bound for Term-2). W. p. at least 1 - δ, RD(fDD) ≤ ^2logN2/61 + P(Y = Y*).
ɪɪr	τ	<1	E	ι ∙	∙	.1	r- r/rʌ V	∖	— / Ρ ∖T τ	C *c
Wrap-up Lemma 1 showsTerm-1 Isintherangeof [(2Af〜-1)e(f0L),η(f0L)]. Lemma2informs
us ηe(fDL ) = ee(fDL ) in classification problems Where Y|X, ∀X are confident. With a well-trained
model fD that learns the noisy distribution D, we have Af〜=1 - E and E → 0+, thus Term-1 is in
the range of [(1 - 2E)ηe(fDL ), ηe(fDL )], indicating our bounds for Term-1 are tight. Besides, noting
Lemma 3 is derived following the same techniques as Theorem 1, we know both bounds have similar
tightness. Therefore, by adding upper bounds for Term-1 and Term-2, we can upper bound the error
of semi-supervised learning in Theorem 2, which has similar tightness to that in Theorem 1.
Theorem 2 (Semi-supervised learning error). Suppose the model trained with only DL has gener-
alization error ηe0(fDL ). With probability at least 1 - δ, the generalization error of semi-supervised
learning on datasets DL ∪ DU is upper-bounded by
Rd (fDL∪Du) ≤	η(fDL)	+ P(Y = Y*)	+J2*0 ,
S{~*}	S-----7----} J	JN	,
Disparity due to baseline	Sharpness of pseudo labels	{z
Data dependency
where η(f0L) := η0(f0L) ∙ NU/N is the expected label error in the pseudo noisy dataset D.
Takeaways Theorem 2 explains how disparate impacts in SSL are generated.
Supervised error ηe0(fDL ): the major source of disparity. The sub-population that generalizes well
before SSL tends to have a lower SSL error. Namely, the rich get richer.
CI	r∙ ♦ ιιι πτι∕^Γ^^ /	∖	♦
Sharpness of noisy labels P(Y 6= Y*): a minor
source of disparity depends on how one processes
pseudo-labels. This term is negligible if we sharpen the pseudo-label.
Sample complexity，2 log(2∕δ)∕N: disparity depends on the number of instances N and their
independence. Recall we assume ideal data augmentations to get N in this term. There will be
much less than N i.i.d. instances with poor data augmentations. It would be a major source of
disparity if data augmentations are poor.
6
Published as a conference paper at ICLR 2022
4 B enefit Ratio: An Evaluation Metric
Figure 1 demonstrates that SSL leads to disparate impacts of different sub-populations’ accuracies,
but it is still not clear how much that SSL benefits a certain sub-population. To quantify the disparate
impacts of SSL, we propose a new metric called benefit ratio.
Benefit Ratio The benefit ratio BR(P) captures the normalized accuracy improvement of sub-
population P after SSL, which depends on three classifiers, i.e., /。工：(baseline) supervised learning
only with a small labeled data Dl, fd: (ideal) supervised learning if the whole dataset D has
ground-truth labels, and /0工」0小 SSL with both labeled dataset DL and unlabeled datasetDu. The
test/validation accuracy of the above classifiers are abaseline(P), aideal(P), and asemi(P), respectively.
As a posterior evaluation of SSL algorithms, the benefit ratio BR(P) is defined as:
BR(P)
asemi (P) -
abaseline (P)
aideal (P) - abaseline (P)
(3)
Let P◊ := {Pι, P2,…} be the set of all the concerned sub-populations. We formally define the
Equalized Benefit Ratio as follows.
Definition 1 (Equalized Benefit Ratio). We call an algorithm achieving equalized benefit ratio if all
the concerned sub-populations have the same benefit ratio: BR(P) = BR(P 0), ∀P, P0 ∈ P.
Intuitively, a larger benefit ratio indicates more benefits from SSL. We have BR(P) = 1 when SSL
performs as well as the corresponding fully-supervised learning. A negative benefit ratio indicates
the poor population is hurt by SSL such that asemi(P) < abaseline(P), i.e., the poor get poorer as
shown in Figure 1(b) (sub-population dog). It has the potential of providing guidance and intuitions
for designing fair SSL algorithms on standard datasets with full ground-truth labels. Whether a fair
SSL algorithm on one dataset is still fair on another dataset would be an interesting future work. In
real scenarios without full supervisions, we may use some extra knowledge to estimate the highest
achievable accuracy of each sub-population and set it as a proxy of the ideal accuracy aideal(P).
Theoretical Explanation Recall we have error upper bounds for both supervised learning (Theo-
rem 1) and semi-supervised learning (Theorem 2). Both bounds have similar tightness thus we can
..	「 一 ..	G /C
compare them and get a proxy of benefit ratio as BR(P)
SUp (RD (fDL∪Du |P »-SUp (RD f Dl∣P »
SUP(RD (fD∣P ))—SUp(RD (/口工|尸))
where sup(∙) denotes the upper bound derived in Theorem 1 and Theorem 2, P is a sub-population,
and D|P denotes the set of i.i.d. instances in D that affect model generalization on P. By assuming
P(Y = Y*) = P(Y = Y*) (both distributions have the same sharpness), We have:
Corollary 1. The benefit ratio proxy for P is BR(P) = 1 一 k(&；LNP )), where ∆(Np, NPL)
∕2log(2∕δ)
V	NPL
J2⅛≡, Np and Npl are the effective numbers of instances in D|P and DL|P.
Corollary 1 shows the benefit ratio is negatively related to the error rate of baseline models and pos-
itively related to the number of i.i.d. instances after in SSL, which is consistent with our takeaways
from Section 3. Note Np and NpL may be larger than the corresponding sub-populations sizes if
P shares information with other sub-population P0 during training, e.g., a better classification of
P0 helps classify P. It also informs us that SSL may have a negative effect on sub-population P if
△(n；NF)> 1, i.e., the benefit from getting more effective i.i.d. instances is less than the harm
from wrong pseudo-labels. This negative effect indicates “the poor get poorer”.
5	Experiments
In this section, we first show the existence of disparate impact on several representative SSL methods
and then discuss the possible treatment methods to mitigate this disparate impact.
Settings Three representative SSL methods, i.e., MixMatch (Berthelot et al., 2019b), UDA (Xie
et al., 2019), and MixText (Chen et al., 2020), are tested on several image and text classification
tasks. For image classification, we experiment on CIFAR-10 and CIFAR-100 datasets (Krizhevsky
et al., 2009). We adopt the coarse labels (20 classes) in CIFAR-100 for training and test the per-
formance for each fine label (100 classes). Thus our training on CIFAR-100 is a 20-class classi-
fication task and each coarse class contains 5 sub-populations. For text classification, we employ
7
Published as a conference paper at ICLR 2022
(a) Benefit ratios (y-axis) versus baseline accuracies before SSL (x-axis) on CIFAR-10
Figure 2: Benefit ratios across explicit sub-populations. Dot: Result of each label class. Line: Best
linear fit of dots.
three datasets: Yahoo! Answers (Chang et al., 2008), AG News (Zhang et al., 2015) and Jigsaw
Toxicity (Kaggle, 2018). Jigsaw Toxicity dataset contains both classification labels (one text com-
ment is toxic or non-toxic) and a variety of sensitive attributes (e.g., race and gender information)
of the identities that are mentioned in the text comment, which are fairness concerns in real-world
applications. We defer more experimental details in Appendix C.
5.1	Disparate Impact Exists in Popular SSL Methods
We show that, even though the size of each sub-population is equal, disparate impacts exist in the
model accuracy of different sub-populations, i.e., 1) explicit sub-populations such as classification
labels in Figure 2 (and Figure 5 in Appendix C.2), and 2) implicit sub-populations such as fine-
categories under coarse classification labels in Figure 3a, demographic groups including race in
Figure 3b and gender (Figure 6 in Appendix C.3). All the experiments in this subsection adopt both
a balanced labeled dataset and a balanced unlabeled dataset. Note we sample a balanced subset from
the raw (unbalanced) Jigsaw dataset. Other datasets are originally balanced.
Disparate impact across explicit sub-populations In this part, we show the disparate impact on
model accuracy across different classification labels on CIFAR-10, Yahoo! Answers, and AG News
datasets. Detailed results on AG News can be found in Appendix C.2. In Figure 2a and 2b, we show
the benefit ratios (y-axis) versus baseline accuracies before SSL (x-axis). From left to right, we
show results with different sizes of labeled data: 25 per class to 50 on CIFAR-10 and 100 per class
to 200 on Yahoo! Answers. Figure 2a and 2b utilized two SSL methods respectively (CIFAR-10:
MixMatch & UDA; Yahoo! Answers: MixText & UDA).We statistically observe that the class labels
with higher baseline accuracies have higher benefit ratios on both CIFAR-10 and Yahoo! Answers
datasets. It means that “richer” classes benefit more from applying SSL methods than the “poorer”
ones. We also observe that for some models with low baseline accuracy (left side of the x-axis),
applying SSL results in rather low benefit ratios that are close to 0.
Disparate impact across implicit sub-populations We demonstrate the disparate impacts on
model accuracy across different sub-populations on CIFAR-100 (fine-labels) and Jigsaw (race and
gender) datasets. In Figure 3, we can statistically observe the disparate impact across different sub-
populations on both datasets for three baseline SSL methods. Detailed results on Jigsaw with gender
are shown in Appendix C.3. We again observe very similar disparate improvements as presented in
Figure 2a and 2b - for some classes in CIFAR-100, this ratio can even go negative. Note the disparate
impact on the demographic groups in Jigsaw raises fairness concerns in real-world applications.
5.2	Mitigating Disparate Impact
Our analyses in Section 3 and Section 4 indicate the disparate impacts may be mitigated by balancing
the supervised error η(fDL ∣p) and the number of effective i.i.d. instances for different populations.
8
Published as a conference paper at ICLR 2022
0=eκl,c3u38
0=eκl,c3u38
Accuracy with IOOO clean labels
(a) Benefit ratios of fine-labels (y-axis) versus baseline accuracies before SSL (x-axis) on CIFAR-100
0.4
0.4
0.4
,2⅛c≤e3u3m
0.4
O⅛α I⅞UΦ8
.3.2，
0.0.0.
。一⅛c≤4->M=φu<υs
(b) Benefit ratios of different race attributes (y-axis) versus baseline accuracies before SSL on Jigsaw.
Figure 3: Benefit ratios across implicit sub-populations. Experiments on CIFAR-100 are ran 5 times
for stability. Mean (dashed line) and standard deviation (shaded area) are plotted in (a), where points
in the shaded area indicate the converged local optima with a non-negligible probability.
Table 1: Comparison on the benefit ratio (%) of all race identities & standard deviation (%) between
different settings. t_asian (nt-asian) is the benefit ratio of asian identity With “toxic” labels (“non-
toxic” labels). SD denotes standard deviation. Negative benefit ratios are highlighted in red colors.
[Jigsaw] Settings	t_asian	t Jblack	t_latin	t_white	nt_asian	nt_black	nt Jatin	ntjwhite	SD
Unbalanced (400)	24.71	21.76	28.21	-9.57	27.27	-8.33	-13.82	29.47	19.28
Balance labeled (400)	28.18	20.00	25.23	33.93	14.33	-11.29	-10.37	3.70	17.29
Balance both (400)	28.57	0.00	11.11	40.63	15.38	14.29	6.90	-7.41	15.29
Balance both (800)	13.04	25.00	7.69	24.00	3.85	10.71	16.67	20.00	7.64
We perform preliminary tests to check the effectiveness of the above findings in this subsection. We
hope our analyses and experiments could inspire future contributions to disparity mitigation in SSL.
Balancing and Collecting more labeled data We firstly sample 400 i.i.d. instances from the raW
(Unbalanced) Jigsaw dataset and get the setting of Unbalanced (400). To balance the supervised error
and effective instances for different sub-populations, an intuitive method is to balance the labeled
data by reweighting, e.g., if the size of two sub-populations is 1:2, we simply sample instances
from two sub-populations with a probability ratio of 2:1 to ensure all sub-population have the same
weights in each epoch during training. Balance labeled (400) denotes only 400 labeled instances are
reweighted. Balance both (400) means both labeled and unlabeled instances are balanced. Table 1
shows a detailed comparison on the benefit ratios between different race identities of the Jigsaw
dataset and their standard deviations, where the first three rows denote the above three settings.
We can observe that both the standard deviation and the number of negative benefit ratios become
lower with more balanced settings, which demonstrates the effectiveness of the balancing treatment
strategy, although there still exists a sub-population that has a negative benefit ratio, indicating an
unfair learning result. To further improve fairness, as suggested in our analyses, we “collect” (rather
add) another 400 i.i.d. labeled instances (800 in total) from the Jigsaw dataset, and show the result
after balancing both labeled and unlabeled data in the last row of Table 3 (Balance both (800)). Both
the standard deviation and the number of negative benefit ratios can be further reduced with more
labeled data. More experimental results are on CIFAR-100 and Jigsaw (gender) datasets shown in
Appendix C.4 (balancing) and Appendix C.5 (more data).
6	Conclusions
We have theoretically and empirically shown that the disparate impact (the “rich” sub-populations
get richer and the “poor” ones get poorer) exists universally for a broad family of SSL algorithms.
We have also proposed and studied a new metric benefit ratio to facilitate the evaluation of SSL. We
hope our work could inspire more efforts towards mitigating the disparate impact and encourage a
multifaceted evaluation of existing and future SSL algorithms.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is partially supported by the National Science Foundation (NSF) under grant IIS-2007951,
IIS-2143895, and the NSF FAI program in collaboration with Amazon under grant IIS-2040800.
Ethics S tatement
In recent years, machine learning methods are widely applied in the real world across a broad variety
of fields such as face recognition, recommendation system, information retrieval, medical diagnosis,
and marketing decision. The trend is likely to continue growing. However, it is noted that these
machine learning methods are susceptible to introducing unfair treatments and can lead to systematic
discrimination against some demographic groups. Our paper echoes this effort and looks into the
disparate model improvement resulted from deploying a semi-supervised learning (SSL) algorithm.
Our work uncovers the unfairness issues in semi-supervised learning methods. More specifically,
we theoretically and empirically show the disparate impact exists in the recent SSL methods on
several public image and text classification datasets. Our results provide understandings of the
potential disparate impacts ofan SSL algorithm and help raise awareness from a fairness perspective
when deploying an SSL model in real-world applications. Our newly proposed metric benefit ratio
contributes to the literature a new measure to evaluate the fairness of SSL. Furthermore, we discuss
two possible treatment methods: balancing and collecting more labeled data to mitigate the disparate
impact in SSL methods. We have carefully studied and presented their effectiveness in mitigating
unfairness in the paper. Our experimental results, such as Table 1, demonstrate the disparate impacts
across demographic groups (e.g., gender, race) naturally exist in SSL. We are not aware of the misuse
of our proposals, but we are open to discussions.
References
Vibhu Agarwal, Tanya Podchiyska, Juan M Banda, Veena Goel, Tiffany I Leung, Evan P Minty,
Timothy E Sweeney, Elsie Gyang, and Nigam H Shah. Learning statistical models of phenotypes
using noisy labeled training data. Journal of the American Medical Informatics Association, 23
(6):1166-1173,2016.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate im-
pact on model accuracy. Advances in Neural Information Processing Systems, 32:15479-15488,
2019.
Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang
Liu. Understanding and improving early stopping for learning with noisy labels. Advances in
Neural Information Processing Systems, 34, 2021.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya
Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. Ai fair-
ness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic
bias. arXiv preprint arXiv:1810.01943, 2018.
David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmenta-
tion anchoring. arXiv preprint arXiv:1911.09785, 2019a.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019b.
Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and Vivek Srikumar. Importance of semantic repre-
sentation: Dataless classification. In Aaai, volume 2, pp. 830-835, 2008.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning. MIT Press,
2006.
10
Published as a conference paper at ICLR 2022
Jiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation of hidden
space for semi-supervised text classification. arXiv preprint arXiv:2004.12239, 2020.
Mingda Chen, Qingming Tang, Karen Livescu, and Kevin Gimpel. Variational sequential labelers
for semi-supervised learning. arXiv preprint arXiv:1906.09535, 2019.
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-
dependent label noise: A sample sieve approach. In International Conference on Learning Rep-
resentations, 2021.
Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised se-
quence modeling with cross-view training. arXiv preprint arXiv:1809.08370, 2018.
Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review
of fair machine learning. arXiv preprint arXiv:1808.00023, 2018.
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. Advances in neural information
processing systems, 28:3079-3087, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron Roth, and Saeed Sharifi-
Malvajerdi. Multiaccurate proxies for downstream fairness. arXiv preprint arXiv:2107.04423,
2021.
Ruoyuan Gao and Chirag Shah. Addressing bias and fairness in search systems. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development in Information
Retrieval,pp. 2643-2646, 2021.
Elizabeth Gomez, LUdovico Boratto, and Maria Salamo. Disparate impact in item recommendation:
A case of geographic imbalance. In European Conference on Information Retrieval, pp. 190-206.
Springer, 2021.
Yves Grandvalet, YoshUa Bengio, et al. Semi-sUpervised learning by entropy minimization. CAP,
367:281-296, 2005.
Lan-Zhe GUo, Zhen-YU Zhang, YUan Jiang, YU-Feng Li, and Zhi-HUa ZhoU. Safe deep semi-
sUpervised learning for Unseen-class Unlabeled data. In International Conference on Machine
Learning,pp. 3897-3906. PMLR, 2020.
SUchin GUrUrangan, Tam Dang, Dallas Card, and Noah A Smith. Variational pretraining for semi-
sUpervised text classification. arXiv preprint arXiv:1906.02242, 2019.
Moritz Hardt, Eric Price, and Nati Srebro. EqUality of opportUnity in sUpervised learning. Advances
in neural information processing systems, 29:3315-3323, 2016.
TatsUnori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness withoUt
demographics in repeated loss minimization. In International Conference on Machine Learning,
pp. 1929-1938. PMLR, 2018.
Sara Hooker, Aaron CoUrville, Gregory Clark, Yann DaUphin, and Andrea Frome. What do com-
pressed deep neUral networks forget? arXiv preprint arXiv:1911.05248, 2019.
Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. Characterising
bias in compressed models. arXiv preprint arXiv:2010.03058, 2020.
Jeremy Howard and Sebastian RUder. Universal langUage model fine-tUning for text classification.
arXiv preprint arXiv:1801.06146, 2018.
ZhUo HUang, Chao XUe, Bo Han, Jian Yang, and Chen Gong. Universal semi-sUpervised learning.
Advances in Neural Information Processing Systems, 34, 2021.
11
Published as a conference paper at ICLR 2022
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-
supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp. 5070-5079, 2019.
Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and Xia Hu. Fmp:
Toward fair graph message passing against topology bias, 2022a.
Zhimeng Jiang, Xiaotian Han, Chao Fan, Fan Yang, Ali Mostafavi, and Xia Hu. Generalized de-
mographic parity for group fairness. In International Conference on Learning Representations,
2022b. URL https://openreview.net/forum?id=YigKlMJwjye.
Kaggle. Jigsaw Toxicity dataset: Toxic comment classification challenge. https://www.
kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification,
2018. Accessed: 2021-11-15.
Falaah Arif Khan, Eleni Manis, and Julia Stoyanovich. Translation tutorial: Fairness and friends. In
Proceedings of the ACM Conference on Fairness, Accountability, and Transparency, 2021.
Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for
fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and
Society, pp. 247-254, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang,
and Ed H Chi. Fairness without demographics through adversarially reweighted learning. arXiv
preprint arXiv:2006.13114, 2020.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for
deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3,
pp. 896, 2013.
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in neural information processing
systems, 33:20331-20342, 2020.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
Yang Liu. Understanding instance-level label noise: Disparate impacts and treatments. In Interna-
tional Conference on Machine Learning, pp. 6725-6735. PMLR, 2021.
Yang Liu. Identifiability of label noise transition matrix. arXiv e-prints, pp. arXiv-2202, 2022.
Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise
rates. In Proceedings of the 37th International Conference on Machine Learning, ICML ’20,
2020.
Yang Liu and Jialu Wang. Can less be more? when increasing-to-balancing label noise rates con-
sidered beneficial. Advances in Neural Information Processing Systems, 34, 2021.
Huixiang Luo, Hao Cheng, Yuting Gao, Ke Li, Mengdan Zhang, Fanxu Meng, Xiaowei Guo, Feiyue
Huang, and Xing Sun. On the consistency training for open-set semi-supervised learning. arXiv
preprint arXiv:2101.08237, 2021.
Natalia Martinez, Martin Bertran, and Guillermo Sapiro. Minimax pareto fairness: A multi objective
perspective. In International Conference on Machine Learning, pp. 6755-6764. PMLR, 2020.
Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. Weakly-supervised neural text classifica-
tion. In Proceedings of the 27th ACM International Conference on Information and Knowledge
Management, pp. 983-992, 2018.
12
Published as a conference paper at ICLR 2022
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi-
supervised text classification. arXiv preprint arXiv:1605.07725, 2016.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,
2018.
Reilly Raab and Yang Liu. Unintended selection: Persistent qualification rate disparities and inter-
ventions. Advances in Neural Information Processing Systems, 34, 2021.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-
supervised learning with ladder networks. arXiv preprint arXiv:1507.02672, 2015.
Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for
semi-supervised text classification via mixed objective function. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 6940-6948, 2019.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. Advances in neural information
processing systems, 29:1163-1171, 2016.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780,
2017.
Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 ieee/acm international work-
shop on software fairness (fairware), pp. 1-7. IEEE, 2018.
Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In
Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp.
526-536, 2021a.
Jialu Wang, Yang Liu, and Xin Eric Wang. Are gender-neutral queries really gender-neutral? miti-
gating gender bias in image search. arXiv preprint arXiv:2109.05433, 2021b.
Jialu Wang, Yang Liu, and Xin Eric Wang. Assessing multilingual fairness in pre-trained multi-
modal representations. In the Findings of ACL 2022, Dublin, Ireland, May 2022. Association for
Computational Linguistics.
Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, and Nong Sang. Self-
supervised learning for semi-supervised temporal action proposal. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1905-1914, 2021c.
Zhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, and Guodong Long. Seminll: A
framework of noisy-label learning by semi-supervised learning. arXiv preprint arXiv:2012.00925,
2020.
Jiaheng Wei and Yang Liu. When optimizing $f$-divergence is robust with label noise. In Interna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=WesiCoRVQ15.
13
Published as a conference paper at ICLR 2022
Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with
noisy labels revisited: A study using real-world human annotations. In International Confer-
ence on Learning Representations, 2022. URL https://openreview.net/forum?id=
TBWA6PLJZQm.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data
augmentation. arXiv preprint arXiv:1904.12848, 2019.
Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash:
Semi-supervised learning with dynamic thresholding. In International Conference on Machine
Learning ,pp.11525-11536. PMLR, 2021.
Diyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, and Eduard Hovy. Let’s make your request
more persuasive: Modeling persuasive strategies via semi-supervised neural nets on crowdfund-
ing platforms. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 3620-3630, 2019.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved varia-
tional autoencoders for text modeling using dilated convolutions. In International conference on
machine learning, pp. 3881-3890. PMLR, 2017.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962-970. PMLR, 2017.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. Advances in neural information processing systems, 28:649-657, 2015.
Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):
44-53, 2018.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pp. 912-919, 2003.
Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-
dependent label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 10113-10123, 2021a.
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when
learning with noisy labels. In International Conference on Machine Learning, pp. 12912-12923.
PMLR, 2021b.
Zhaowei Zhu, Zihao Dong, and Yang Liu. Detecting corrupted labels without training a model to
predict, 2022a.
Zhaowei Zhu, Jialu Wang, and Yang Liu. Beyond images: Label noise transition matrix estimation
for tasks with lower-quality features. arXiv preprint arXiv:2202.01273, 2022b.
14
Published as a conference paper at ICLR 2022
Appendix
The Appendix is organized as follows.
• Section A presents the detailed derivations for generalization bounds.
-	Section A.1 proves the upper bound for Term-1.
-	Section A.2 proves the lower bound for Term-1.
-	Section A.2 proves the upper bound for Term-2 (also for Theorem 1).
-	Section A.4 proves Lemma 2.
-	Section A.5 proves Corollary 1.
• Section C presents more experimental settings and results.
A Theoretical Results
A.1 Term- 1 Upper Bound
RD(f) - RDe(f)
P(f(X) 6=Y|X)-P(f(X) 6=Ye|X) P(X) dX
≤ 2IX
P(f(X) = Ye|X) -P(f(X) = Y|X) P(X) dX
〜
〜
P(f(X)	=Y|X)-P(f(X)	=Y|X)+	P(f(X)	6=Y|X)-P(f(X)	6=Y|X)	P(X)	dX
(=a)	TD(Yef(X)||Yf(X))P(X) dX
X
(b)
≤	TD(Ye(X)||Y(X))P(X) dX
X
=1 / XwY = i∣χ) - P(Y = iX)∣p(x) dχ
X i∈[K]
=	η(X)P(X) dX
X
=η
where in equality (a), given model f and feature X, we can treat Yf (X) as a Bernoulli random
variable such that
〜
〜
〜
〜
P(Yf(X) = +) = P(f(X) = Y|X) and P(Yf(X) = -) = P(f(X) 6=Y|X).
Then according to the definition of total variation of two distributions, i.e.,
TD(P |1Q) ：=2 Zu dP - dQ ιdu,
we can summarize the integrand as the total variation between Yf(X) and Yf(X).
Tr	∕ι ∖ 1 ιι F , .ι ι ,	∙ ∙	ι ∙ , ∙	.ι	ι ι ∙ι ∙ , ∙ Γτm /-χ~7^
Inequality (b) holds due to the data processing inequality since the probabilities [P(Y
〜
〜
f(X)), P(Y	6=	f(X))] are generated by	[P(Y	= i),	∀i	∈	[K]],	and the probabilities	[P(Y
f(X)),P(Y 6= f(X))] are generated by [P(Y = i), ∀i ∈ [K]].
η(X) is the accuracy of labels on feature X (in the cases specified in Lemma 2).
A.2 Term- 1 Lower Bound
Let e(X) := P(Y 6= Ye|X) be the feature-dependent error rate, Aef(X) := P(f(X) = Ye|X) be the
〜
〜
accuracy of prediction f(X) on noisy dataset D. Note e(X) is independent ofAf(X). Denote their
expectations (over X) by e := EX[e(X)], Af = EX[Af (X)]. We have:
15
Published as a conference paper at ICLR 2022
RD(f) -RDe(f)
P(f(X) 6=Y|X)-P(f(X) 6=Ye|X) P(X)
P(f(X) =Ye|X)-P(f(X) =Y|X) P(X)
/ ∖
P(f(x)
∖
Y If (X ) = Y ,X) - P(f (X ) = Y If (X ) = Y, X)
-'{z--------------} 、--------------- ----------------}
w.p. 1	P(Ye =Y |X)
P(f(X) = YeIX)P(X)
+X
( ∖
P(f(X)= Yf(X)= Y,X)-P(f(X) = YIf(X) = Y,X) I P(f(X) = Y|X)P(X)
w.p. 0
/(1 - P(Y = Y|X)) P(f (X) = Y|X) P(X)
denoted by e(X)	denoted by Af(X)
+	0 - P(f(X) = YIf(X) 6=Ye,
P(f(X) 6= YeIX)P(X)
e(X)Aef(X)P(X)
P(f(X) =	YIf(X)	6=Ye,Y=Ye,X)P(Y=YeIf(X)	6=Ye,X)	P(f(X)	6=	YeIX)P(X)
r ( ∖
/ P(f (X) = Y If (X) = Y,Y = Y ,X) P(Y = Y If (X) = Y,X) I P(f (X) = YIX )P(X)
JX X---------------{z-------------}
≤1
≥X
e(X)Aef(X)P(X)
IX(0 ∙ P(Y = Y If (X) = Y ,X)) P(f (X) = YIX )P(X)
-X
/ ∖
1 ∙ P(Y = Y If (X ) = Y ,X)
--------1-----{z-------------}
=P(Y 6=Ye |X ) due to independency
P(f(X) 6= YeIX)P(X)
e(X)Af(X)P(X)-
c
P(Y 6= YeIX)P(f(X) 6= YeIX) P(X)
JX x---------'、------------/
denoted by e(X) denoted by 1 - Aef(X)
e(X)(2Aef(X)
- 1)P(X)
X
X
X
X

z
z
=(2Aef - 1)e.
A.3 Term-2 Upper Bound
We adopt the same technique to prove Theorem 1 and Lemma 3. The proof follows Liu & Guo
(2020). We prove Theorem 1 as follows.
Denote the expected error rate of classifier f on distribution D by
RD(f) :=ED[1(f(X),Y)].
16
Published as a conference paper at ICLR 2022
Let fd denote the classifier trained by minimizing 0-1 loss with clean dataset D, i.e.,
fD := arg min RD(f),
f
where
1
RD (f) := N E 1(f (xn),yn).
n∈[N]
The Bayes optimal classifier is denoted by
fD := arg min RD (f).
f
The expected error given by fD is written as
R* := RD(fD).
Denote by Y * |X := arg maxi∈[κ] P(Y |X) the Bayes optimal label on clean distribution D. With
probability at least 1 - δ, we have:
,ʌ ..
RD (fD) - RD (fD)
=RD (fD ) - RD (fD ) + (RD (fD ) - RD (fD )) + (RD (fD ) - RD (fD ))
(a)
≤0 + 2max |Rd(f) - RD(f )|
f∈F
(b)
≤
2log(2∕δ)
-N
1	♦	1 ∙ . Z ∖ < 11	\	< ∙	.1	♦♦	∙ ∙ 1 ∙ 1	1∙	.
where inequality (a) holds since 1) RD(fD) achieves the minimum empirical risk according to its
1 C ∙ . ∙	. 1	τ∖ / P ∖	A / ，士 ∖	,	l-∖ C∖	1 C ,1 C 11	♦	.	.	∙11 Λ	.
definition, thus RD (fD ) - RD (fD* ) ≤ 0; 2) each of the following two terms will be no greater
than the corresponding maximal gap maxf∈F |RD(f) - RD (f)|. Inequality (b) holds due to the
Hoeffding’s inequality, i.e., with probability at least 1 - δ,
max ∣Rd(f) — Rd(f)| ≤ Jlog(Nδ).
Noting RD(fD* ) = P(Y 6= Y*), we can prove Theorem 1. But substituting D for D, we can also
prove Lemma 3.
A.4 Proof for Lemma 2
Proof. Note
P(Ye 6= Y|X) = 1 - P(Ye = Y|X) = 1 -	P(Ye = i|X)P(Y = i|X).
i∈[K]
η(x) := 2 X ∣P(Y = i∣x) - P(Y = i∣x)|.
i∈[K]
AssumeP(Y= i0|x) =1. We have P(Ye 6= Y|x) =1 - P(Ye = i0|x) and
1
η(x):=-1 -p(y = i0∣x) +	E p(y = i∣x) = 1 -p(y = i0∣x).
i∈[K],i6=i0
□
A.5 Proof for Corollary 1
—
BcR(P)
η + √2⅛δ) + P(Y = Y *) - q 2⅛≡ - P(Y = Y *)
/2log(2∕δ) _	/2log(2∕δ)
VNP	-V	NPL
△(NP, NPL) - η
δ(NP, nPl )	.
17
Published as a conference paper at ICLR 2022
B Matthew Effect
We further illustrate the Matthew effect when the initial labeled dataset is small and unbalanced. In
Figure 4, we show the change of test accuracy due to SSL. We test two SSL algorithms on CIFAR-
10: MixMatch (Berthelot et al., 2019b) (left panel) and UDA (Xie et al., 2019) (right panel), where
each label class is treated as a sub-population. There are 20 labeled instances in each of the first
5 classes (20 × 5), 10 labeled instances in each of the remaining classes (10 × 5). The remaining
instances in the first 5 classes and half of the remaining instances in the other 5 classes are used
as unlabeled data, which ensures the unlabeled dataset has the same ratios among sub-populations
as the labeled one. Figure 4 shows two “poor” sub-populations get poorer by using MixMatch and
“four” sub-populations get poorer by using UDA, which consolidate the observation of Matthew
effect of SSL. Note we also find the observation “the poor getting poorer” does not always happen
when there are more labeled instances and a more balanced dataset. See more discussions in the
next subsection.
B.1	More Examples
ISS "!心婚(％) >0E300ratt8h-
MixMatch on CIFAR-IO. Labeled Size = [20x5.10x5]
100
80
60
40
20
0
20	40	60	80
Test accuracy (%) before SSL
Figure 4: Disparate impacts in the model accuracy of SSL. MixMatch (Berthelot et al., 2019b) (left
panel) and UDA (Xie et al., 2019) (right panel) are applied on CIFAR-10 with 150 clean labels.
Dashed line: The threshold indicating the test accuracy does not change before and after SSL. Red
dots: Sub-populations that get poorer after SSL. Green dots: Sub-populations that get richer after
SSL.
UDA on CIFAR-10. Labeled Size = [20x5,10x5]
• Get poorer
• Get richer
20	30	40	50	60
Test accuracy (%) before SSL
O ŋ O O
8 6 4 2
-ISS」心婚(％) >0E300ra8
B.2	Discussions
We do observe some “poor” classes may benefit from SSL, e.g., class “dog” in Figure 1(a), and the
dots that have initial test accuracies smaller than 0.3. We are aware of that, for most cases, we do
only observe “the rich getting richer”. Intuitively, the phenomenon that several poor classes also
benefit from SSL is mainly due to two reasons:
•	In a resource-constrained scenario, “the rich getting richer” is likely to happen with “the poor get-
ting poorer”. However, benefiting from the large capacity of deep neural networks and powerful
SSL algorithms, the available “resource” after SSL is usually greater than before SSL. Thus the
poor may not always get poorer.
•	Current popular SSL algorithms, such as MixMatch (Berthelot et al., 2019b) and UDA (Xie et al.,
2019), are very powerful. It is reasonable to believe the re-labeling procedures in each epoch in
these algorithms could reduce the disparate impact compared to the two-iteration scenario con-
sidered in our theoretical analysis. This may also explain why “some poor classes are adequately
benefited.” Although there are some exceptions, our Figure 2 and Figure 3 can still show that
there are non-negligible disparate impacts including both “the rich get richer” and “the poor get
poorer” in current state-of-the-art (SOTA) SSL algorithms. Besides, Figure 3(a) and Table 1 also
show that some classes have negative benefit ratios, indicating “the poor get poorer”.
18
Published as a conference paper at ICLR 2022
Table 2: Dataset statistics and train/test splitting. The number of train and test samples means the
number of data per class.
Datasets	Label Type	Number of classes	Number of train samples	Number of test samples
CIFAR-10	Image	10	5000	1000
CIFAR-100	Image	100	500	100
Yahoo! Answer	Question Answering	10	140000	5000
AG News	News	4	30000	1900
C More Experiments
C.1 Experiment Settings
Explicit sub-populations: classification labels In this section, we show the statistics of datasets
where we conducted the experiments to demonstrate the disparate impact on the explicit sub-
populations (classification labels) in Table 2. In addition, the train/valid/test splitting in the image
datasets (CIFAR-10 and CIFAR-100) is 45000:5000:10000. As for the splitting in the text datsets
(Yahoo! Answers and AG News), we follow the setting in (Chen et al., 2020).
Implicit sub-populations: fine-categories under coarse classification labels In this section, we
provide a description about coarse labels in CIFAR-100 dataset when we conducted the experiments
to demonstrate the disparate impact on the implicit sub-populations: fine-categories under coarse
classification labels. In CIFAR-100, its 100 classes can be categorized into 20 superclasses. Each
image can have a “fine” label (the original class) and a “coarse” label (the superclass). The list of
superclasses in the CIFAR-100 is as follows: aquatic mammals, fish , flowers orchids, food con-
tainers bottles, fruit and vegetables, household electrical devices, household furniture bed, large
carnivores, large man-made outdoor things, large natural outdoor scenes, large omnivores and her-
bivores, medium-sized mammals, non-insect invertebrates, people, reptiles, small mammals, trees,
vehicles 1, and vehicles 2.
Implicit sub-populations: demographic groups (race & gender) In this section, we describe
the detailed data distribution before and after balancing methods about implicit sub-populations:
demographic groups (race & gender on Jigsaw dataset) . The identities in the race sub-population
includes ‘asian’, ‘black’, ‘latin’, and ‘white’. In each identity, we consider different classification
labels (toxic and non-toxic). Therefore, we have eight identities in the race sub-population. The
numbers of samples on these eight race identities in the unbalanced Jigsaw dataset are ‘asian (toxic):
466; black (toxic): 1673; latin (toxic): 282; white (toxic): 3254; asian (non-toxic): 633; black (non-
toxic): 1879; latin (non-toxic): 511; white (non-toxic): 2652’. The numbers of samples on these
eight race identities in the balanced Jigsaw dataset are ‘asian (toxic): 1419; black (toxic): 1419;
latin (toxic): 1419; white (toxic): 1419; asian (non-toxic): 1419; black (non-toxic): 1419; latin
(non-toxic): 1419; white (non-toxic): 1419’. Similar to race, we also have eight identities in the
gender sub-population by considering four different gender types which are ‘male’, ‘female’, ‘male
femal’ and ‘transgender’ and two classification labels (toxic and non-toxic). The numbers of samples
on these eight gender identities in the unbalanced Jigsaw dataset are ‘male (toxic): 5532; female
(toxic): 4852; male female (toxic): 1739; transgender (toxic): 405; male (non-toxic): 5167; female
(non-toxic): 4985; male female (non-toxic): 2002; transgender (non-toxic): 374’. The numbers
of samples on these eight gender identities in the balanced Jigsaw dataset are ‘male (toxic): 3142;
female (toxic): 3142; male female (toxic): 3142; transgender (toxic): 3142; male (non-toxic): 3142;
female (non-toxic): 3142; male female (non-toxic): 3142; transgender (non-toxic): 3142’. In either
race or gender, Jigsaw dataset contains more than four identities, we set the number of identities to
4 due to the relatively small number of other identities. In addition, the ratio of ‘train:valid:test’ on
either race or gender sub-population case is ‘8:1:1’.
19
Published as a conference paper at ICLR 2022
C.2 More Experimental Results for Disparate Impact across Different Classes
on AG News
In this work, we also conducted experiments on the AG News text classification dataset. Figure 5
demonstrates the disparate impact on model accuracy across different classification labels on the AG
News dataset.
M∣xTe×t. labeled size 100×4
65 70 75 80 85 90
Accuracy with 400 clean labels
。一⅛c≤sɑ)uɑ)g
。一⅛c≤sɑ)uɑ)g
MixText, labeled size 200×4
。一⅛c≤sɑ)uɑ)g
Figure 5: Benefit ratios of different class labels (y-axis) versus baseline accuracies before SSL (x-
axis) on AG News
C.3 More Experimental Results for Disparate Impact across Different
Sub-populations on Jigsaw (gender)
In this work, we also conducted experiments on the Jigsaw Toxicity text classification dataset with
gender sub-populations. Figure 6 shows the disparate impact on model accuracy across different
sub-populations (gender) on the Jigsaw Toxicity dataset.
0.0-∣-∏-5-h——i————i-----M
58 60 62 64 66 68 70
Accuracy with 200 clean labels
D4eɑ⅛JWUWC□
Accuracy with 400 clean labels
Figure 6: Benefit ratios of different gender attributes (y-axis) versus baseline accuracies before SSL
(x-axis) on Jigsaw.
C.4 More Experimental Results of “Balancing” Treatment to Disparate Impact
In this section, we show more experimental results of “Balancing” treatment to disparate impact
described in Section 5.2. In Table 3, we compare the changes of mean and standard deviation on the
model accuracy and our proposed benefit ratio in three settings: (a) SSL with unbalanced data → (b)
SSL with balanced labeled datasets only (by reweighting) → (c) SSL with both balanced labeled &
unlabeled datasets. Setting (c) is not practical due to the lack of labels in unlabeled data but it shows
the best performance that this simple reweighting can achieve. Note that CIFAR-100 1:2 Accuracy
denotes the accuracy of the case CIFAR-100 1:2, where CIFAR-100 1:2 means 40 instances per
class for the first 50 fine classes, 20 instances per class for the next 50 fine classes. Besides, Jigsaw
(1.4:4.5:1:7.5) means that the ratios of different sub-populations’ sizes are 1.4:4.5:1:7.5. Table 3
shows reweighting unbalanced dataset helps improve the overall performance (mean) and reduce the
disparate impacts (standard deviation).
C.5 More Experimental Results of “Collecting More Labeled Data”
Treatment to Disparate Impact
In this subsection, we show more experimental results of “Collecting more labeled data” treatment to
disparate impact described on Section 5.2. Table 4 demonstrates the changes of mean and standard
20
Published as a conference paper at ICLR 2022
Table 3: Balance samples with reweighting. We sample unbalanced CIFAR-100 (1:2 means 40
instances per class for the first 50 fine classes, 20 instances per class for the next 50 fine classes).
Jigsaw is naturally unbalanced. For race, we consider asian, black, latin, and white identities. For
gender, we consider male, female, male female, and transgender identities. The rounded ratios on
the number of samples between different identities in race and gender are listed in the table.
Datasets	Mean	Standard Deviation
CIFAR-100 1:2 Accuracy CIFAR-100 1:5 Accuracy CIFAR-100 1:2 Benefit Ratio CIFAR-100 1:5 Benefit Ratio	69.91 → 69.98 → 69.99 二 61.90 → 62.76 → 63.21 55.16 → 55.19 → 56.09 44.67 → 49.47 → 47.40	22.26 → 21.48 → 21.46 29.50 → 27.73 → 26.97 20.85 → 20.16 → 19.17 37.07 → 33.82 → 32.18
Jigsaw (race) (1.4:4.5:1:7.5) Accuracy Jigsaw (gender) (13.7:12.6:4.8:1) Accuracy Jigsaw (race) (1.4:4.5:1:7.5) Benefit Ratio Jigsaw (gender) (13.7:12.6:4.8:1) Benefit Ratio	66.71 → 67.25 → 67.88 二 66.03 → 66.74 → 67.17 12.46 → 13.09 → 13.67 11.90 — 12.18 — 12.54	25.64 → 21.33 → 17.84 12.90 → 9.75 → 8.68 19.28 → 17.29 → 15.29 88.52 → 50.67 → 34.24
deviation on model accuracy and benefit ratio with the varying number (grow exponentially from
left to right) of labeled data on CIFAR-100 and Jigsaw (race & gender). As shown in Table 4, the
mean accuracy tends to be higher and the standard deviation becomes lower when more labeled data
is utilized to train the model, which shows the effectiveness of collecting more labeled data.
Table 4: Collect more data. a → b → c → d: stands for the change of mean or standard deviation
with different sizes of labeled dataset. For CIFAR-100, the sizes are 5 × 100, 10 × 100, 20 × 100, 40 ×
100. For Jigsaw (both race and gender), the sizes are 25 × 8, 50 × 8, 100 × 8, 150 × 8.
Datasets	Mean	Standard Deviation
CIFAR-100 Accuracy	52.87 → 60.93 → 68.01 → 74.69	32.38 → 28.59 → 23.52 → 16.76
CIFAR-100 Benefit Ratio	48.86 → 54.01 → 56.40 → 66.20	33.39 → 28.62 → 22.11 → 16.87
Jigsaw (race) Accuracy	64.88 → 67.88 → 72.25 → 73.50	29.55 → 17.84 → 9.93 → 6.86
Jigsaw (gender) Accuracy	64.50 → 67.17 → 73.50 → 74.83	20.15 → 8.68 → 6.41 → 5.99
Jigsaw (race) Benefit Ratio	12.09 → 13.67 → 14.52 → 15.12	18.29 → 15.29 → 11.69 → 7.64
Jigsaw (gender) Benefit Ratio	7.80 → 12.54 → 22.46 → 25.81	41.29 → 34.24 → 27.30 → 13.08
21