Published as a conference paper at ICLR 2022
EI EigenGame Unloaded □
When playi ng games is better than opti mi zi ng
Ian Gemp*, Brian McWilliams*, Claire Vernade & Thore Graepel
DeepMind, London UK
{imgemp,bmcw,vernade}@deepmind.com, thoregraepel@gmail.com
Ab stract
We build on the recently proposed EigenGame that views eigendecomposition as a
competitive game. EigenGame’s updates are biased if computed using minibatches
of data, which hinders convergence and more sophisticated parallelism in the
stochastic setting. in this work, we propose an unbiased stochastic update that
is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing
computation on datasets of larger sample sizes, and outperforms EigenGame
in experiments. We present applications to finding the principal components
of massive datasets and performing spectral clustering of graphs. We analyze
and discuss our proposed update in the context of EigenGame and the shift in
perspective from optimization to games.
1	Introduction
Large, high-dimensional datasets containing billions of samples are commonplace. Dimensionality
reduction to extract the most informative features is an important step in the data processing pipeline
which enables faster learning of classifiers and regressors (Dhillon et al., 2013), clustering (Kannan
and Vempala, 2009), and interpretable visualizations. Many dimensionality reduction and clustering
techniques rely on eigendecomposition at their core including principal component analysis (Jolliffe,
2002), locally linear embedding (Roweis and Saul, 2000), multidimensional scaling (Mead, 1992),
isomap (Tenenbaum et al., 2000), and graph spectral clustering (Von Luxburg, 2007).
Numerical solutions to the eigenvalue problem have been approached from a variety of angles for
centuries: Jacobi’s method, Rayleigh quotient, power (von Mises) iteration (Golub and Van der
Vorst, 2000). For large datasets that do not fit in memory, approaches that access only subsets—or
minibatches—of the data at a time have been proposed.
Recently, EigenGame (Gemp et al., 2021) was introduced with the novel perspective of viewing the
set of eigenvectors as the Nash strategy of a suitably defined game. While this work demonstrated an
algorithm that was empirically competitive given access to only subsets of the data, its performance
degraded with smaller minibatch sizes, which are required to fit high dimensional data onto devices.
One path towards circumventing EigenGame’s need for large minibatch sizes is parallelization. in a
data parallel approach, updates are computed in parallel on partitions of the data and then combined
such that the aggregate update is equivalent to a single large-batch update. The technical obstacle
preventing such an approach for EigenGame lies in the bias of its updates, i.e., the divide-and-conquer
EigenGame update is not equivalent to the large-batch update. Biased updates are not just a theoretical
nuisance; they can slow and even prevent convergence to the solution (made obvious in Figure 4).
in this work we introduce a formulation of EigenGame which admits unbiased updates which we
term μ-EigenGame. We will refer to the original formulation of EigenGame as α-EigenGame.1
μ-EigenGame and α-EigenGame are contrasted in Figure 1. Unbiased updates allow us to increase
the effective batch size using data parallelism. Lower variance updates mean that μ-EigenGame
should converge faster and to more accurate solutions than α-EigenGame regardless of batch size.
in Figure 1a (top), the density of the shaded region shows the distribution of steps taken by the
* denotes equal contribution.
1μ signifies unbiased or Unloaded and α denotes original.
1
Published as a conference paper at ICLR 2022
(a)
(b)
Figure 1: (a) Comparing α-EigenGame (GemP et al., 2021) and μ-EigenGame (this work)
over 1000 trials with a batch size of 1. (top) The expected trajectory2 of each algorithm from
initialization () to the true value of the third eigenvector (?). (bottom) The distribution of
distances between stochastic update trajectories and the expected trajectory of each algorithm as a
function of iteration count (bolder lines are later iterations and modes further left are more desirable).
(b) Empirical support for Lemma 2. In the top row, player 3’s utility is given for parents
mis-sPecified by an angular distance along the sphere of ∠(Vj<i,Vj<i) ∈ [-20°, -10°, 10°, 20°]
moving from light to dark. Player 3's mis-sPecification, ∠(vi ,vi), is given by the x-axis (optimum
is at 0 radians). α-EigenGame (i) exhibits slightly lower sensitivity than μ-EigenGame (ii) to
mis-specified parents (see equation (8)). However, when the utilities are estimated using samples
Xt 〜P(X) (faint lines), μ-EigenGame remains accurate (iv), while α-EigenGame (iii) returns a
utility (dotted line) with an optimum that is shifted to the left and down. The downward shift occurs
because of the random variable in the denominator of the penalty terms (see equation (3)).3
2The trajectory when updating with E[Xt>Xt].
3Overestimation is expected by Jensen's: E[Xχ] ≥ E[Xχ].
stochastic variant of each algorithm after 100 burn-in steps. Although the expected path of α-EG is
slightly more direct, its stochastic variant has much larger variance. Figure 1a (bottom) shows that
with increasing iterations, the μ-EG trajectory approaches its expected value whereas α-EG exhibits
larger bias. Figure 1b further supports μ-EigenGame,s reduced bias with details in Sections 3 and 4.
Our contributions: In the rest of the paper, we present our new formulation of EigenGame, analyze
its bias and propose a novel unbiased parallel variant, μ-EigenGame with stochastic convergence
guarantees. μ-EigenGame's utilities are distinct from α-EigenGame and offer an alternative perspec-
tive. We demonstrate its performance with extensive experiments including dimensionality reduction
of massive data sets and clustering a large social network graph. We conclude with discussions of the
algorithm’s design and context within optimization, game theory, and neuroscience.
2	Preliminaries and related work
In this work, we aim to compute the top-k right singular vectors of data X, which is either represented
as a matrix, X ∈ Rn×d , of n d-dimensional samples, or as a d-dimensional random variable. In either
case, we assume we can repeatedly sample a minibatch Xt from the data of size n0 < n, Xt ∈ Rn0×d.
The top-k right singular vectors of the dataset are then given by the top-k eigenvectors of the (sample)
covariance matrix, C = E[熹X>Xt] = E[Ct].
For small datasets, SVD is appropriate. However, the time, O(min{nd2 , n2d}), and space, O(nd),
complexity of SVD prohibit its use for larger datasets (Shamir, 2015) including when X is a ran-
dom variable. For larger datasets, stochastic, randomized, or sketching algorithms are better suited.
Stochastic algorithms such as Oja’s algorithm (Oja, 1982; Allen-Zhu and Li, 2017) perform power
2
Published as a conference paper at ICLR 2022
iteration (Rutishauser, 1971) to iteratively improve an approximation, maintaining orthogonality
of the eigenvectors typically through repeated QR decompositions. Alternatively, randomized algo-
rithms (Halko et al., 2011; Sarlos, 2006; Cohen et al., 2017) first compute a random projection of
the data onto a (k + p)-subspace approximately containing the top-k subspace. This is done using
techniques similar to Krylov subspace iteration methods (Musco and Musco, 2015). After projecting,
a call to SVD is then made on this reduced-dimensionality data matrix. Sketching algorithms (Feld-
man et al., 2020) such as Frequent Directions (Ghashami et al., 2016) also target learning the top-k
subspace by maintaining an overcomplete sketch matrix of size (k + p) × d and maintaining a span
of the top subspace with repeated calls to SVD. In both the randomized and sketching approaches, a
final SVD of the n × (k + p) dataset is required to recover the desired singular vectors. Although the
SVD scales linearly in n, some datasets are too large to fit in memory; in this case, an out-of-memory
SVD may suffice (Haidar et al., 2017). For this reason, the direct approach of stochastic algorithms,
which avoid an SVD call altogether, is appealing when processing very large datasets.
A large literature on distributed approaches to
PCA exists (Liang et al., 2014; Garber et al.,
2017; Fan et al., 2019). These typically fol-
low the pattern of computing solutions locally
and then aggregating them in a single round (or
minimal rounds) of communication. The mod-
ern distributed machine learning setting which
has evolved to meet the needs of deep learning
is fundamentally different. Many accelerators
joined with fast interconnects means the cost of
communication is low compared to the cost of a
single update step, however existing approaches
to distributed PCA cannot take full advantage of
this.
Notation: We follow the same notation as Gemp
et al. (2021). Variables returned by an approxi-
mation algorithm are distinguished from the true
solutions with hats, e.g., the column-wise matrix
of eigenvectors V approximates V. We order
the columns of V such that the ith column, vi,
is the eigenvector with the ith largest eigenvalue
λi . The set of all eigenvectors {Vj } with λj larger than λi, namely Vi ’s parents, will be denoted by
Vj<i . Similarly, sums over subsets of indices may be abbreviated as Pj<i = Pij-=1 111 . The set of all
parents and children of Vi are denoted by V-i . Let the ith eigengap gi = λi — λi+1 . We assume the
standard Euclidean inner product hu, Vi = u>V and denote the unit-sphere and simplex in ambient
space Rd with Sd-1 and ∆d-1 respectively.
Algorithm 1 μ-EigenGameR
1: Given: data stream Xt ∈ Rn0×d, vectors Vi ∈
Sd-1, step sequence ηt, and iterations T.
2: Vi — V0 for all i
3: for t = 1 : T do
4:	parfor i = 1 : k do
5:	rewards J ・X；XtVi
6:	penalties	J
n PjyihXtvi, XtVjiVj
7:	勺μ J rewards — penalties
8:	Vμ,R JVμ — <vμ,ViiVi
9:	Vi J Vi + ηtV μ,R
v0
10:	ViJ π⅜
11: end parfor
12: end for
13: return all Vi
α-EigenGame. We build on the algorithm introduced by Gemp et al. (2021), which we refer to here
as α-EigenGame. This algorithm is derived by formulating the eigendecomposition of a symmetric
positive definite matrix as the Nash equilibrium of a game among k players, each player i owning the
approximate eigenvector Vi ∈ Sd-1. Each player is also assigned a utility function, uα(V∕Vj∙<i), that
they must maximize:
uα(Vi∣Vj<i)
Var
z~}^~{
VjCVi-E
j<i
Align-penalty
z }|	{
hVi,CVj i2
hVj ,CVj i
(1)
These utilities balance two terms, one that rewards a Vi that captures more variance in the data
and a second term that penalizes Vi for failing to be orthogonal to each of its parents Vj< (these
terms are indicated with Var and Align-penalty in equation (1)). In α-EigenGame, each player
simultaneously updates ^ with gradient ascent, and it is shown that this process converges to the
Nash equilibrium. We are interested in extending this approach to the data parallel setting where each
player i may distribute its update computation over multiple devices.
3
Published as a conference paper at ICLR 2022
3	A scalable unbiased algorithm
We present our novel modification to α-EigenGame called μ-EigenGame along with intuition, theory,
and empirical support for critical lemmas. We begin with identifying and systematically removing the
bias that exists in the α-EigenGame updates. We then explain how removing bias allows us to exploit
modern compute architectures culminating in the development of a highly parallelizable algorithm.
3.1	q-EigenGame’s biased updates
Consider partitioning the sample covariance matrix Ct into a sum of m matrices as Ct = n1o X> Xt =
m1 Pm 箸XtmXtm = m1 Pm Ctm. For sake of exposition, We drop the additional subscript t on
C in what follows. We would like α-EigenGame to parallelize over these partitions. However, the
gradient of Ua with respect to Vi does not decompose cleanly over the data partitions:
Align-penalty
Var	Z -、	---------
vα z}> X VICvjC- __ 1 XlrX- X v>cvj r1 ʌ 1	c、
▽i H Cvi - 2. > 二八 Cvj = - Cm C mvi - 7,-> 二八 C mvj ∙	(2)
v> v> Cvj	m L	^> v> Cvj	J
j<i j j	m	j<i j j
We include the superscript α on the EigenGame gradient to differentiate it from the μ-EigenGame
direction later. The nonlinear appearance of C in the penalty terms makes obtaining an unbiased
gradient difficult. The quadratic term in the numerator of equation (2) could be made unbiased by
using two sample estimates ofC, one for each term. But the appearance of the term in the denominator
does not have an easy solution. Cm is likely singular for small n0 (n0 < d) which increases the
likelihood of a small denominator, i.e., a large penalty coefficient (boxed), if we were to estimate the
denominator with samples. The result is an update that emphasizes penalizing orthogonality over
capturing data variance. Techniques exist to reduce the bias of samples of ratios of random variables,
but to our knowledge, techniques to obtain unbiased estimates are not available. This was conjectured
by Gemp et al. (2021) as the reason for why α-EigenGame performed worse with small minibatches.
3.2	Removing q-EigenGame’s bias
it is helpful to rearrange equation (2) to shift perspective from estimating a penalty coefficient (in
red) to estimating a penalty direction (in blue):
Va fX -m X hC mvi - X v>C mvj vCCv i .	(3)
m	j<i	j j
The penalty direction in equation (3) is still difficult to estimate. However, consider the case where
vj is any eigenvector of C with associated (unknown) eigenvalue λ0. In this case, Cvj = λ0vj and
the penalty direction (in blue) simplifies to vj because ||vj || = 1. While this assumption is certainly
not met at initialization, Q-EigenGame leads each vj towards vj, so we can expect this assumption to
be met asymptotically.
This intuition motivates the following μ-EigenGame update direction for vi with inexact parents vj-
(compare orange in equation (4) to blue in equation (3)):
δIi = Cvi - X(V> Cvj )vj = mm X hCmvi - X(V> Cmvj )vji .	(4)
j <i	m	j<i
We use ∆ instead of V because the direction is not a gradient (discussed later). Notice how the
strictly linear appearance of C in μ-EigenGame allows the update to easily decompose over the data
partitions in equation (4). The μ-EigenGame update satisfies two important properties.
Lemma 1 (Asymptotic equivalence). The μ-EigenGame direction, △7, With exact parents (vj =
vj ∀ j < i) is equivalent to Q-EigenGame.
Proof. We start with Q-EigenGame and add a superscript e to its gradient to emphasize this is the
gradient computed with exact parents (vj = vj). Then simplifying, we find
RVa X Cvi - X vl Ivj Cvj = Cvi - X v> f∕vj XVj= Cvi - X(VtCVj )vj = △i. (5)
j<i vj Cvj	j<i vj%vj	j<i
4
Published as a conference paper at ICLR 2022
Therefore, once the first (i - 1) eigenvectors are learned, learning the ith eigenvector with μ-
EigenGame is equivalent to learning with α-EigenGame.	□
Lemma 2 (Zero bias). Unbiased estimates of ∆μ can be obtained with samples from P(X).
Proof. Let X 〜P(X) where X ∈ Rd and P(X) is the uniform distribution over the dataset. Then
E[∆μ] = E[XX>]Vi - X(v>E[XX>]^j)^j = Cvi - X(v>Cvj)^j.	(6)
j<i	j<i
where all expectations are with respect to p(X).	□
These two lemmas provide the foundation for a performant algorithm. The first enables convergence
to the desired solution, while the second facilitates scaling to larger datasets. Algorithm 1 presents
pseudocode for μ-EigenGame where computation is parallelized over the k players.
3.3	Model and data parallelism
In our setting we have a number of connected
devices. Specifically we consider the paral-
lel framework specified by TPUv3 available
in Google Cloud, however our setup is appli-
cable to any multi-host, multi-device system.
The α-EigenGame formulation (Gemp et al.,
2021) considers an extreme form of model par-
allelism (Figure 2a) where each device has its
own unique set of eigenvectors.
(a)	(b)
Figure 2: (a) Extreme model parallelism as pro-
posed in α-EigenGame. (b) Model and data par-
allelism enabled by μ-EigenGame. Squares are
separate devices (here, M = 4). Copies of es-
timates are color-coded. Updates are averaged
across copies for a larger effective batch size.
In this work we further consider a different form
of model and data parallelism which is directly
enabled by having unbiased updates (Figure 2b).
This enables μ-EigenGame to deal with both
high-dimensional problems as well as massive
sample sizes. Here each set of eigenvectors is
copied on M devices. Update directions are
computed on each device individually using a
different data stream and then combined by sum-
ming or averaging. Updates are applied to a sin-
gle copy and this is duplicated across the M - 1 remaining devices. In this way, updates are computed
using an M × larger effective batch size while still allowing device-wise model parallelism. This
setting is particularly useful when the number of samples is very large. This form of parallelism is not
possible using the original EigenGame formulation since it relies on combining unbiased updates. In
this sense, the parallelism discussed in this work generalizes that introduced by Gemp et al. (2021).
Note that we also allow for within-device parallelism. That is, each vi in Figure 2 is a contiguous
collection of eigenvectors which are updated independently, in parallel, on a given device (for
example using vmap in Jax). We provide pseudocode in Algorithm 2 in the appendix which simply
augments Algorithm 1 with an additional parallelized for-loop and aggregation step over available
devices. We also provide detailed Jax pseudo-code for parallel μ-EigenGame in Appendix F. We
compare the empirical scaling performance of μ-EigenGame against α-EigenGame on a 14 billion
sample dataset in section 5.
4	SVD as the solution to a new EigenGame
We theoretically examine the μ-EigenGame algorithm and 1) prove that, using only minibatches
of data, μ-EigenGame converges globally to the true eigenvectors, which 2) comprise the Nash
equilibrium of a novel game formulation we recover through deriving pseudo-utility functions from
update rules. Beyond proving specific theoretical properties of μ-EigenGame, we believe these proof
techniques may be of wider interest to the community.
5
Published as a conference paper at ICLR 2022
4.1	Convergence to SVD
The asymptotic equivalence of μ-EigenGame to α-EigenGame ensures μ-EigenGame is globally,
asymptotically convergent and its unbiased updates ensure it is scalable. Proof in appendix C.
Theorem 1 (Global convergence). Given a positive definite covariance matrix C with the top-k eigen-
gaps positive and a square-summable, not summable step size sequence ηt (e.g., 1/t), Algorithm 1
converges to the top-k eigenvectors asymptotically (limT→∞) with probability 1.
This stochastic asymptotic convergence result is complimentary to the deterministic (full-batch)
finite-sample result in GemP et al. (2021) where each Vi is learned in sequence. In contrast, the
proof above applies when learning all Vi in parallel. We leave finite-sample convergence to future
work (Durmus et al., 2020).
4.2	SVD is NASH of M-EigenGame
We arrived at μ-EigenGame by analyzing and improving properties of the α-EigenGame update.
However, the μ-EigenGame update direction is linear in each V%. This suggests We may be able to
design a pseudo-utility function for it. Rearranging the update direction from equation (4) as
∆ = CVi- X Vj (V> CVi)= [i - X Vj v>i CVi = V μ	(7)
j<i	j<i
reveals that we can reverse-engineer the following utility function
deflation
嘴=V>[Z_Xj{ ]c ∙[Vi]	(8)
j<i
where O is the stop gradient operator commonly used in deep learning packages. As the name implies,
• stops gradients from flowing through its argument so that equation (8) appears linear in Vi instead
μ	i μ μ	μ	ι	ι μ μ~!
of quadratic when	differentiating	the expression.	In light of this, we have renamed ∆	to Vμ	to
emphasize that it is a pseudo-gradient of u。Note that without the stop gradient, the true gradient of
uμμ would be [A + A>]Vi rather than AVi where A = [I - Pj∙<i V>Vj]C. We analyze this alternative
in Appendix H.1 and find it, interestingly, to perform worse than μ-EigenGame empirically.
The utility function 诚 has an intuitive meaning. It is the Rayleigh quotient for the matrix Ci =
[I - Pj∙<i V> Vj]C, which gives the covariance after the subspace spanned by Vj< has been removed.
In other words, player i is directed to find the largest eigenvalue in the orthogonal complement of the
approximate top-(i - 1) subspace. This approach is known as “deflating" the matrix C. Figure 1b
illustrates μ-EigenGame,s reduced bias when estimating the new utility function (and resulting
optimum) from an average over minibatches.
Definition 1 (μ-EigenGame). Let μ-EigenGame be the game with players i ∈ {1,...,k}, their
respective strategy spaces Vi ∈ Sd-1, and their corresponding utilities uμμ as defined in equation (8).
Theorem 2. Top-k SVD is the unique Nash of μ-EigenGame given SymmetriC C with the top-k
eigengaps positive.
Proof. We will show by induction that each Vi is the unique best response to V-i, which implies
they constitute the unique Nash equilibrium. First, consider player 1’s utility. It is the Rayleigh
quotient of C because Vi is constrained to the unit-sphere, i.e., uμ = V> CVi = v1>C^1. Therefore,
we know Vi maximizes uμ and the maximizer is unique because its eigengap gi > 0. In game theory
parlance, Vi is a best response to V-i. The proof continues by induction. The utility of player i is
U = V> [I - Pj∙<i Vjv>]CVi, which is the Rayleigh quotient with the subspace spanned by the
top (i - 1) eigenvectors removed. Therefore, the maximizer of ur is the largest eigenvector in the
remaining subspace, i.e., Vi . As before, gi > 0, so this maximizer is unique. This shows that each Vi
is the unique best response to v-i, therefore, the set of Vi forms the unique Nash.	□
Notice how the induction proof of Theorem 2 relies on a) the hierarchy of vectors (Vi does not
depend on v-ι) and b) the fact that ur need only be a sensible utility when all player i's parents
6
Published as a conference paper at ICLR 2022
168 O00-2
~BaJκJ0te>ue6≡1 W
0u6ugusgdsqns
168 O
BS J0>u6
0u6u
MNIST (Minibatch = 256)
16Cb 5
*S J0>u6
0u6u
MNlST (Minibatch = 32)
T'嬴(297X jy∕↑⅞as (195)]
IKraSUlInaSa99”
guss5gBdsqns
guss5gBdsqns
[碍(2741)
[α-EG (29IH
BJaSg35)) [GHA(2971) [Krasullnasa罚
10	20	30	40	50
Epochs
0	29	58	87	117	1，	0	117 234 351	468 5f	0	937 1875 2812 3750 4687
Iterations (thousands)	Iterations (thousands)	Iterations (thousands)
Figure 3: MNIST Experiment. Runtime (seconds) in legend on CPU (m = 1). Each column
evaluates a different minibatch size ∈ {1024, 256, 32}. Shading indicates ± standard error of the
mean. Learning rates were chosen from {10-3, . . . , 10-6} on 10 held out runs. Solid lines denote
results with the best performing learning rate. All plots show means over 10 trials (randomness
arising from minibatches and initialization). Shaded regions highlight ± standard error of the mean.
are eigenvectors. We revisit this in conjunction with Figure 5b later in discussion section 6.1 to aid
researchers in the design of future approaches.
The Nash property is important because it enables the use of any black-box procedure for computing
best responses. Like prior work, we develop a gradient method for optimizing each utility, however,
that is not a requirement. Any approach suffices if it can efficiently compute a best response.
5	Experiments
As in EigenGame, we omit the projection of gradients onto the tangent space of the sphere; specifically,
we omit line 8 in Algorithm 1. As discussed in Gemp et al. (2021), this has the effect of intelligently
adapting the step size to use smaller learning rates near the fixed point. To ease comparison with
previous work, we count the longest correct eigenvector streak as introduced by Gemp et al. (2021),
which measures the number of eigenvectors that have been learned, in order, to within an angular
threshold (e.g., n/8) of the true eigenvectors. We also measure how well the set of Vi captures the
top-k subspace with a normalized subspace distance: 1 - 1/k ∙ Tr(U *P) ∈ [0,1] where U * = VV t
and P = VVT (Tang, 2019). We provide additional experiments in Appendix A.
MNIST. We compare μ-EigenGame against α-EigenGame, GHA (Sanger, 1989), Matrix Kra-
sulina (Tang, 2019), and Oja’s algorithm (Allen-Zhu and Li, 2017) on the Mnist dataset. We flatten
each image in the training set to obtain a 60, 000 × 784 dimensional matrix X . Figure 3 demonstrates
μ-EigenGame,s robustness to minibatch size. It performs best in the longest streak metric and better
than α-EigenGame in subspace distance. We attribute this improvement to its unbiased updates and
additional acceleration effects which we discuss in detail in section H.2.
Meena conversational model. This dataset consists a subset of the 40 billion words used to train
the transformer-based Meena language model (Adiwardana et al., 2020). The subset was preprocessed
to remove duplicates and then embedded using the trained model.
The dataset consists of n ≈ 14 billion embeddings each with dimensionality d = 2560; its total size
is 131TB. Due to its moderate dimensionality we can exactly compute the ground truth solution by
iteratively accumulating the covariance matrix of the data and computing its eigendecomposition. On
a single machine this takes 1.5 days (but is embarrassingly parallelizable with MapReduce).
We use minibatches of size 4,096 in each TPU. We do model parallelism across 4 TPUs so we see
16,384 samples per iteration. We test two additional degrees of data parallelism with 4× (16 TPUs,
65,536 samples) and 8× (32 TPUs, 131,072 samples) the amount of data per iteration respectively.
We compute and apply updates using SGD with a learning rate of 5 × 10-5 and Nesterov momentum
with a factor of 0.9.
7
Published as a conference paper at ICLR 2022
a(ŋ①匕 S」otj①>u ① 6i∑]
4->。①l0ɔ4-->s ① 6UO-I
Figure 4: Comparison between μ-EigenGame and α-EigenGame with different degrees of data
parallelism (in parentheses) on the Meena dataset.
Figure 4 compares the mean performance of μ-EigenGame against α-EigenGame as a function of
the degree of parallelism in computing the top k = 256 eigenvectors (standard errors computed
over 5 random seeds). Each TPU is tasked with learning 32 contiguous eigenvectors. We see that
increasing the degree of parallelism has no effect on the performance of α-EigenGame. As expected,
it is unable to take advantage of the higher data throughput since its updates are biased and cannot be
meaningfully linearly combined across copies. In contrast, the performance of μ-EigenGame scales
with the effective batch size achieved through parallelism. μ-EigenGame (8×) is able to recover 256
eigenvectors in less than 40,000 iterations in 2 hours 45 minutes (approximately 0.5 epochs).
Spectral clustering on graphs. We conducted an experiment on learning the eigenvectors of the
graph Laplacian of a social network graph (Leskovec and McAuley, 2012) for the purpose of spectral
clustering. The eigenvalues of the graph Laplacian reveal several interesting properties as well such
as the number of connected components, an approximation to the sparsest cut, and the diameter of a
connected graph (Chung et al., 1994).
Given a graph with a set of nodes V and set of edges E , the graph Laplacian can be written as
L = X>X where each row of the incidence matrix X ∈ RlEl×lVl represents a distinct edge;
Xe=(i,j)∈E is a vector containing only 2 nonzero entries, a 1 at index i and a -1 at index j (Horaud,
2009). In this setting, the eigenvectors of primary interest are the bottom-k (λ∣v∣, λ∣v∣-1,...) rather
than the top-k (λ1 , λ2 , . . .), however, a simple algebraic manipulation allows us to reuse a top-
k solver. By defining the matrix L- = λ*I -L with λ* > λ1, We ensure L- 8 0 and the
top-k eigenvectors of L- are the bottom-k of L. The update in equation (4) is transformed into
勺μ = (λ*I — L)Vi — Pj<i (V> (λ*I — L)Vj)Vj. We provide efficient pseudo-code in Appendix G.
The Facebook graph consists of 134, 833 nodes, 1, 380, 293 edges, and 8 connected components,
each formed by a set of Facebook pages belonging to a distinct category, e.g., Government, TV
shows, etc. (Leskovec and Krevl, 2014; Rozemberczki et al., 2019). We add a single edge between
every pair of components to create a connected graph. By projecting this graph onto the bottom 8
eigenvectors of the graph Laplacian using μ-EG (M = 1,n0 = ηt =	) and then running k-means
clustering (Pedregosa et al., 2011), we are able to recover the ground truth clusters (see Figure 5a)
with 99.92% accuracy. The experiment was run on a single CPU.
6	Discussion
6.1	Utilities to updates and back
Figure 5b summarizes the relationships advising the designs of the various EigenGame algorithms.
Starting from the α-EigenGame utility, its update is arrived at by simply following the standard
gradient ascent paradigm. In noticing that stochastic estimates of the gradient are biased, we arrive at
the μ-EigenGame update by considering how to remove this bias in a principled manner.
Sacrificing the exact steepest decent direction for a direction that allows unbiased estimates is a
tradeoff that in this case has benefits. Also, while jμ is not a gradient (except with exact parents), the
new penalties have properties (above) that make them intuitively more desirable than the originals;
they are adaptive to the state of the system (discussed further in section H.2).
8
Published as a conference paper at ICLR 2022
Approx. Eigenvalues (vɪ @ Bottom)
(a)
uμ	Ua
•	Var & Align
q μ <----------------vα
remove bias
(b)
Figure 5: (5a) Facebook Page Networks. (Left) Petals differentiate ground truth clusters; colors
differentiate learned clusters. Petals are ideally colored according to the color bar starting with
the rightmost petal and proceeding counterclockwise. Numbers indicate ground truth cluster size.
Clusters are extracted by running k-means clustering on the learned eigenvectors V ∈ RlVl×k
(samples on rows). (Right) Rayleigh quotient plot reveals a gap between the 8th and 9th eigenvalues
indicating ≈ 8 clusters exist. (5b) Relationships between utilities and updates. An arrow indicates the
endpoint is reasonably derived from the origin; the lack of an arrow indicates the direction is unlikely.
We derive pseudo-utilities with desired theoretical properties by integrating the new updates with
help from the stop gradient operator. However, it is unlikely that this utility would be developed
independently of these steps to solve the problem at hand (see Appendix H for more details). This
suggests an alternative approach to algorithm design complementary to the optimization perspective:
directly designing updates themselves which converge to the desired solution, reminiscent of previous
paradigms that drove neuro-inspired learning rules.
6.2 Bridging Hebbian and optimization approaches
The Generalized Hebbian Algorithm (GHA) (Sanger, 1989; Gang et al., 2019; Chen et al., 2019)
update direction for Vi with inexact parents Vj is similar to μ-EigenGame:
∆gha = CVi- X(V>CVj)Vj.	(9)
j≤i
C appears linearly in this update so GHA can also be parallelized. In contrast to μ-EigenGame, GHA
additionally penalizes the alignment of Vi to itself and removes the unit norm constraint on Vi (not
shown). Without any constraints, GHA overflows in experiments. We take the approach of Gemp et
al. (2021) and constrain Vi to the unit-ball (||Vi|| ≤ 1) rather than the unit-sphere (||Vi|| = 1).
The connection between GHA and μ-EigenGame is interesting because unlike μ-EigenGame, GHA
is a Hebbian learning algorithm inspired by neuroscience and its update rule is not motivated from
the perspective of maximizing of a utility function. Game formulations of classical machine learning
problems may provide a bridge between statistical and biologically inspired viewpoints.
7 Conclusion
We introduced μ-EigenGame, an unbiased, globally convergent, parallelizable algorithm that recovers
the top-k eigenvectors of a symmetric positive definite matrix. We demonstrated the performance of
μ-EigenGame on large scale dimension reduction and clustering problems. We discussed technical
details of μ-EigenGame within the context of game theory, machine learning and neuroscience.
Like its predecessor, μ-EigenGame is a k-player, general-sum game allowing model parallelism
over players; our unbiased reformulation allows even greater parallelism over data. Furthermore, the
hierarchy and Nash property enable the exploration of more sophisticated best responses.
μ-EigenGame,s improved robustness to smaller minibatches makes it more amenable to being used
as part of deep learning, optimization (Krummenacher et al., 2016), and regularization (Miyato et al.,
2018) techniques which leverage spectral information of gradient covariances or Hessians. Graph
spectral methods have also recently shown to be related to state-of-the-art representation learning
algorithms (HaoChen et al., 2021) further cementing the importance of efficient SVD algorithms in
modern machine learning.
9
Published as a conference paper at ICLR 2022
Acknowledgements. We would like to thank Trevor Cai, Rosalia Schneider, Dimitrios Vytiniotis
for invaluable help with optimizing algorithm performance on TPU. We also thank Maribeth Rauh,
Zonglin Li, Daniel Adiwardana and the Meena team for providing us with data and assistance. And
finally, we thank Alexander Novikov for helpful feedback on the manuscript.
References
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton
University Press, 2009.
D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kul-
shreshtha, G. Nemade, Y. Lu, et al. Towards a human-like open-domain chatbot. arXiv preprint
arXiv:2001.09977, 2020.
Z. Allen-Zhu and Y. Li. First efficient convergence for streaming k-PCA: a global, gap-free, and
near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science
(FOCS), pages 487-492. IEEE, 2017.
R. W. Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming
problems. Linear Algebra and its applications, 146:79-91, 1991.
Z. Chen, X. Li, L. Yang, J. Haupt, and T. Zhao. On constrained nonconvex stochastic optimization:
A case study for generalized eigenvalue decomposition. In The 22nd International Conference on
Artificial Intelligence and Statistics, pages 916-925. PMLR, 2019.
F. R. Chung, V. Faber, and T. A. Manteuffel. An upper bound on the diameter of a graph from
eigenvalues associated with its Laplacian. SIAM Journal on Discrete Mathematics, 7(3):443-457,
1994.
M. B. Cohen, C. Musco, and C. Musco. Input sparsity time low-rank approximation via ridge leverage
score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1758-1777. SIAM, 2017.
P. S. Dhillon, D. P. Foster, S. M. Kakade, and L. H. Ungar. A risk comparison of ordinary least
squares vs ridge regression. The Journal of Machine Learning Research, 14(1):1505-1511, 2013.
A. Durmus, P. Jimenez, E. Moulines, S. Said, and H.-T. Wai. Convergence analysis of Riemannian
stochastic approximation schemes. arXiv preprint arXiv:2005.13284, 2020.
J. Fan, D. Wang, K. Wang, and Z. Zhu. Distributed estimation of principal eigenspaces. Annals of
statistics, 47(6):3009, 2019.
D. Feldman, M. Schmidt, and C. Sohler. Turning big data into tiny data: Constant-size coresets for
k-means, PCA, and projective clustering. SIAM Journal on Computing, 49(3):601-657, 2020.
A. Gang, H. Raja, and W. U. Bajwa. Fast and communication-efficient distributed PCA. In ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 7450-7454. IEEE, 2019.
D. Garber, O. Shamir, and N. Srebro. Communication-efficient algorithms for distributed stochastic
principal component analysis. In International Conference on Machine Learning, pages 1203-1212.
PMLR, 2017.
I. Gemp, B. McWilliams, C. Vernade, and T. Graepel. Eigengame: PCA as a Nash equilibrium. In
International Conference for Learning Representations, 2021.
M. Ghashami, E. Liberty, J. M. Phillips, and D. P. Woodruff. Frequent directions: simple and
deterministic matrix sketching. SIAM Journal on Computing, 45(5):1762-1792, 2016.
G. H. Golub and H. A. Van der Vorst. Eigenvalue computation in the 20th century. Journal of
Computational and Applied Mathematics, 123(1-2):35-65, 2000.
A. Haidar, K. Kabir, D. Fayad, S. Tomov, and J. Dongarra. Out of memory SVD solver for big data.
In 2017 IEEE High Performance Extreme Computing Conference (HPEC), pages 1-7. IEEE, 2017.
10
Published as a conference paper at ICLR 2022
N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217-288,
2011.
J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning
with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.
M. Hessel, D. Budden, F. Viola, M. Rosca, E. Sezener, and T. Hennigan. Optax: composable gradient
transformation and optimisation, in JAX!, 2020.
R. Horaud. A short tutorial on graph Laplacians, Laplacian embedding, and spectral clustering, 2009.
I.	T. Jolliffe. Principal components in regression analysis. In Principal Component Analysis. Springer,
2002.
R. Kannan and S. Vempala. Spectral algorithms. Now Publishers Inc, 2009.
G. Krummenacher, B. McWilliams, Y. Kilcher, J. M. Buhmann, and N. Meinshausen. Scalable
adaptive stochastic optimization using random projections. In Advances in Neural Information
Processing Systems, pages 1750-1758, 2016.
J.	Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data, June 2014.
J.	Leskovec and J. McAuley. Learning to discover social circles in ego networks. Advances in Neural
Information Processing Systems, 25:539-547, 2012.
Y. Liang, M.-F. Balcan, V. Kanchanapally, and D. P. Woodruff. Improved distributed principal
component analysis. In NIPS, 2014.
A. Mead. Review of the development of multidimensional scaling methods. Journal of the Royal
Statistical Society: Series D (The Statistician), 41(1):27-39, 1992.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. arXiv preprint arXiv:1802.05957, 2018.
C. Musco and C. Musco. Randomized block Krylov methods for stronger and faster approximate
singular value decomposition. In Advances in Neural Information Processing Systems, 2015.
E.	Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology,
15(3):267-273, 1982.
F.	Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
S.	T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323-2326, 2000.
B. Rozemberczki, R. Davies, R. Sarkar, and C. Sutton. Gemsec: Graph embedding with self clustering.
In Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining 2019, pages 65-72. ACM, 2019.
H. Rutishauser. Simultaneous iteration method for symmetric matrices. In Handbook for Automatic
Computation, pages 284-302. Springer, 1971.
T.	D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network.
NeuralNetworks, 2(6):459-473, 1989.
T.	Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006
47th Annual IEEE Symposium on Foundations ofComputer Science (FOCS'06), pages 143-152.
IEEE, 2006.
11
Published as a conference paper at ICLR 2022
S. M. Shah. Stochastic approximation on Riemannian manifolds. Applied Mathematics & Optimiza-
tion, pages 1-29, 20l9.
O. Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In Proceed-
ings of the International Conference on Machine Learning, pages 144-152, 2015.
C. Tang. Exponentially convergent stochastic k-PCA without variance reduction. In Advances in
Neural Information Processing Systems, pages 12393-12404, 2019.
J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319-2323, 2000.
U. Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416, 2007.
Y. Wang, N. Xiu, and J. Han. On cone of nonsymmetric positive semidefinite matrices. Linear
Algebra and its Applications, 433(4):718-736, 2010.
12
Published as a conference paper at ICLR 2022
A Experiments on synthetic data
Linearly DeCaying SPeCtrUm
Ooo
4 2
*eφ-J∞JOΦAUΘ6i
φ,uouφ6uo
O 200	400	600	800 IOOO
# of Training Iterations
+J ⅛
t tn
o J
u o
ω ω 20
触
3⅛
Exponentially Decaying Spectrum
0
0	200	400	600	800 1000
# of Training Iterations
Figure 6: Synthetic Experiment. Runtime (milliseconds) in legend.
We validate μ-EigenGame in a full-batch setting on two synthetic datasets: one with exponentially
decaying spectrum; the other with a linearly decaying spectrum. Figure 6 shows μ-EigenGame
outperforms α-EigenGame on the former and matches its performance on the latter. We discuss
possible reasons for this gap in the discussion in Section 6.
B Parallelized Algorithm
Riemannian Manifolds. Before introducing an algorithm for μ-EigenGame, We first briefly review
necessary terminology for learning on Riemannian manifolds Absil et al. (2009), specifically for
the sphere. The notation Tvi Sd-1 denotes the set of vectors tangent to the sphere at a point ^ (i.e.,
any vector orthogonal to V/ Rvi(Z) = ∣∣^i+Z∣∣ is the commonly used restriction of the retraction on
Sd-1 to the tangent bundle at Vi (i.e., step in tangent direction Z and then unit-normalize the result).
The operator ∏a (y) = (I - V>Vi)y projects the direction y onto TviSd-1. Combining these tools
together results in a movement along the Riemannian manifold: V(t+1) - R√∏Vi(y)).
We present pseudocode for μ-EigenGame below where computation is parallelized both over the k
players and over M machines per player.
Algorithm 2 μ-EigenGameR
1: Given: data stream Xt ∈ Rn0 ×d, number of parallel machines M per player (minibatch size per
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
machine n00 = M), initial vectors V0 ∈ Sd-1, step size sequence ηt, and number of iterations T.
Vi J V0 for all i
for t = 1 : T do
parfor i = 1 : k do
parfor m = 1 : M do
rewards J X鼠XtmVi
penalties J PjyihXtmVi,XtmVj〉Vj
Vμmn J (rewards — penalties)/"00
V 紫 JV I * * * V Vitm-h μm,α iVi
end parfor
Vμ,R j___L P 6μ,R^ι
V i	J- M ʌ/m [ V im ]
Vi J Vi + ηtVμ,R
0
ViJ尚
end parfor
end for
return all Vi
13
Published as a conference paper at ICLR 2022
C Global Stochastic Convergence
Theorem 1 (Global Convergence). Given a positive definite covariance matrix C with the top-k eigen-
gaps positive and a square-summable, not summable step size sequence ηt (e.g., 1/t), Algorithm 1
converges to the top-k eigenvectors asymptotically (limT→∞) with probability 1.
Proof. Assume none of the Vi are initialized to an angle exactly 90° away from the true eigenvector:
hVi, Vii = O. The set of vectors {Vi :hVi, Vii = 0} has Lebesgue measure 0, therefore, the above
assumption holds w.p.1. The update direction for the top eigenvector Vi is exactly equal to that of
μ	μ-7
α-EigenGame (Vɪ = ▽?), therefore, they have the same limit points for Vi. The proof then proceeds
by induction. As Vj<i approach their limit points, the update for the ith eigenvector Vi approaches
μ	μ-7∣J1
that of α-EigenGame (Vμ = Va) and, by Lemma 3, the stable region of μ-EigenGame also shrinks
to a point around the top-k eigenvectors.
Denote the “update field” H(V) to match the work of Shah (2019). H(V) is simply the concatenation
of all players’ Riemannian update rules, i.e., all players updating in parallel using their Riemannian
updates:
H(V) = [(I - V1V>)∆μ,..., (I - VkV>)∆μ] : Rkd → Rkd	(10)
where △? is defined in equation (7) and (I 一 pip> )△? projects △? onto the tangent space of player
i’s unit sphere.
The result is then obtained by applying Theorem 7 of Shah (2019) with the following information:
a) the unit-sphere is a compact manifold with an injectivity radius of π, b) the update field is a
polynomial in {Vi} and therefore smooth (analytic), and c) by Lemma 4 (see Appendix E) the update
noise constitutes a bounded martingale difference sequence.	□
While the convergence proof for α-EG provides finite-sample rates, it only applies to the algorithm
applied sequentially (not parallelized over eigenvectors) and in the deterministic setting (minibatch
contains the entire data set). The experiments in Gemp et al. (2021) apply the algorithm in parallel
and with mini batch sizes, meaning the α-EG theorem does not actually apply to their experimental
setting. That is to say, the α-EG paper proposes updating eigenvectors in parallel in practice despite
the lack of convergence guarantee.
In contrast, our convergence theorem applies to μ-EG when applied in parallel (over the eigenvectors)
and in the stochastic setting (with mini batch sizes), which is what we examine empirically in our
experiments. The downside is that we do not provide finite-sample convergence rates.
Although we do not provide convergence rates, Lemma 1 proves that the μ-EG update converges
to the α-EG update for each eigenvector, so intuitively, we expect the convergence rates to be
relatively similar given that the algorithms are equivalent in the limit. Note that in the full batch
setting where stochasticity does not conflate the differences between the two algorithms, Figure 6
in Appendix A empirically supports the similarity of the convergence rates for the two algorithms.
Figure 3 (minibatch of 1024) which looks at a large (but not full) minibatch size, also shows a small
difference between convergence for the two algorithms.
In summary, the α-EG convergence theorem is impractical—it provides convergence rates for a
(non-parallel) algorithm in the (non-stochastic) setting, which is a combination that α-EG paper
does not suggest be applied in practice. In contrast, our μ-EG convergence theorem is practical—it
provides asymptotic convergence for a parallel algorithm in the stochastic setting.
Difficulties Obtaining Finite Sample Rates In consideration ofa finite sample convergence result,
we consulted Durmus et al. (2020). The primary obstacle to applying their convergence theorem is
the construction of a suitable Lyapunov function to satisfy their Assumption A.2 stated on page 4.
Constructing Lyapunov functions is typically a tedious, unpredictable process. The work in Durmus
et al. (2020) is very recent and finite sample convergence of Riemannian stochastic approximation
(i.e., update directions are not gradients of any function) schemes is cutting edge, highly technical
research. This is in contrast to Riemannian optimization (i.e., update directions are the gradient of a
function), which is much more mature. We hope theory advances in the near future to a point where
we can more easily provide convergence rates for algorithms like α-EigenGame.
14
Published as a conference paper at ICLR 2022
D Error Propagation / Sensitivity Analysis
Lemma 3. An O(E) angular error ofparent Vj< implies an O(E) angular error in the location of
the solutionfor Vi.
Proof. The proof proceeds in three steps:
1.	O(E) angular error of parent =⇒ O(E) Euclidean error of parent
2.	O(E) Euclidean error of parent =⇒ O(E) Euclidean error of norm of child gradient
3.	O(E) Euclidean error of norm child gradient + instability of minimum at ±∏2 =⇒ O(E)
angular error of child’s solution.
Angular error in the parent can be converted to Euclidean error by considering the chord length
between the mis-specified parent and the true parent direction. The two vectors plus the chord form
an isoceles triangle with the relation that chord length l = 2sin( f) is O(E) for E《1.
Next, write the mis-specified parents as Vj = Vj + Wj where ||wj || is O(E) as we have just shown.
μ μ μ	μ ∙ μ∙	μ	Uji
Let b equal the difference between the Riemannian update direction ▽? with approximate parents
and that with exact parents. All directions we consider here are the Riemannian directions, i.e., they
have been projected onto the tangent space of the sphere. Then
b = Vμ,e -∖7Ii = (	I - Viv>	) Xh(V>CvjIVj-(V>Cvj)vji	(II)
..	, j<i
projection onto sphere
and the norm of the difference is
(12)
|	|b|| = ||(I- Viv>) X h(VirCvj )Vj - (V> Cv j )Vj i||	(13)
j<i
≤	I∣I - v∣v>I∣∙∣I X[(v>CVj)Vj - (v>CVj)Vji∣l	(14)
j<i
≤	Il X[(V>CVj)Vj-(V>CVj)Vji∣∣.	(15)
j<i
We can further bound the summands with
ll(v>Cvj)vj - (v>Cvj)vjll = ll(Vjv> -Vjv>)Cvil1	(16)
≤ IlVjV> - VjV>IIIICViIl	(17)
≤ λιIIVjV> - VjV>II	(18)
= λ1II(Vj +wj)(Vj +wj)r -VjVjrII	(19)
= λ1 IIwj Vjr + Vj wjr + wj wjr II	(20)
≤ λ1(IIwjVjrII + IIVjwjrII + IIwjwjrII)	(21)
= O(E).	(22)
This upper bound on the norm of the difference between the two directions translates to a lower
μ.e	7U.e	μ	μ-7LL.G μ7LL∖
bound on the inner product of the two directions wherever IIV∣x, II > e, specificallyhV7, , VQ > 0
(see Figure 7a). And recall that the direction with exact parents is equivalent to the gradient of
α-EigenGame with exact parents, Viα,e.
Therefore, by a Lyapunov argument, the Vμ direction is an ascent direction on the α-EigenGame
utility where it forms an acute angle (positive inner product) with Viα,e. Furthermore, Viα,e is the
gradient of a utility function that is sinusoidal along the sphere manifold; specifically, it is a cosine
with period π and positive amplitude dependent on the spectrum of C (c.f. equation (8) of Gemp
15
Published as a conference paper at ICLR 2022
(a)
(b)
Figure 7: (a) Close in Euclidean distance can imply close in angular distance if the vectors are long
enough. (b) The stable region for μ-EigenGame consists of an O(e) ball around the true optimum as
→ 0.
μ	μ	μ μ	μ μ	μ	μ μ ∙ μ μι
et al. (2021)). We can derive an upper bound on the Size of the angular region for which W is not
necessarily an ascent direction (the “?" marks in Figure 7). This region is defined as the set of angles
for which the norm of the utility,s derivative is small, i.e., ∣∣Vα,e∣∣ ≤ e. The derivative of cosine
is sine, which depends linearly on its argument (angle) for small values, therefore, ∣θ∣ ≤ O(e) or
12 一 θ∣ ≤ O(e). As long as Vi does not lie within the |π∏ 一 O(e)∣ region, μ-EigenGame will ascend
the utility landscape to within O(e) angular error of the true eigenvector vi. In the limit as e → 0, the
size ofthe | ∏ 一 O(e) | region vanishes to a point, v⊥. To understand the stability of this point, we can
again appeal to the analysis from Gemp et al. (2021)—see equation (8) on page 7 of that work. The
Jacobian of Vμμ and the HeSSian of Ua are equal with exact parents, and we know that its Riemannian
Hessian is positive definite if the ith eigengap is positive: HR [uj]占(λi 一 λi+ι)I. This means that
the point vi⊥ is a repeller for α-EigenGame. Similarly to before, we can show more formally that an
O(e) perturbation to parents results in an O(e) perturbation to the Jacobian of Vμ from H[uj]:
J =[I- ∑VjV>]C	(23)
j<i
=[I - X(Vj + Wj)(vj + Wj)T]C	(24)
= [I 一 X vjvj>]C 一 X[wj vj> + vj wj> + wj wj>]C	(25)
j<i	j<i
=[I 一 X Vjv>]C -O(e)W	(26)
j<i
= H[uiα] 一 O(e)W	(27)
where W is some matrix with O(1) entries (w.r.t. e). For the sphere, the Riemannian Jacobian
is a linear function of the Jacobian (JR = (I 一 ViVT)J 一 (V> JVi)I = HR[uj] — O(e)) and
therefore the error remains O(e). The set of (non)symmetric, positive semidefinite matrices (A is
p.s.d. iff yTAy ≥ 0 ∀ y) forms a closed convex cone, the interior of which contains positive definite
matrices Wang et al. (2010). Therefore, JR remains in this set after a small enough O(e) perturbation.
Therefore, in the limit e → 0, the spectrum of the Jacobian will also be positive definite indicating
the point Vi⊥ is a repeller. This is indicated by the blue arrows in Figure 7b.
Figure 7b summarizes the results that the stable region for α-EigenGame consists of an O(e) ball
around the true optimum for parents with O(e) angular error.	□
16
Published as a conference paper at ICLR 2022
E Noise is Martingale Difference Sequence
Let ∆μ,t = [l - Pj<i Vjt)Vjt)>] Cvitt be the μ-EigenGame update direction computed using the
full expected covariance matrix. Let ∆μ,t = [l - Pj∙<i vjt)vjt)>] Ct^t be the update direction
computed using a minibatch estimate of the covariance matrix where minibatches are unbiased
because they are formed from data sampled uniformly at random from the dataset. Define MiV,t(+t)1 =
∆μ,t	∆μ,t	口nd la	MV(t)	— ΓMV(t)	MV(t) ]>	Wherv V(t) _	f7.(t)	]
∆i -	∆i	and let	Mt+1	= [M1,t+1,	. . . , Mk,t+1]	where V =	{Vi∈[k]}.
Lemma 4. {MtV+(1t) } is a bounded martingale difference sequence with respect to the increasing
σ-fields
Ft= σ({(Vi)(∈t[k],…,(Vi)(∈t[k]}, {C⑴…,C(t)})	(28)
Proof. Given the filtration Ft, we find
EMV+)1 |Ft] = [l - X Vjt)Vjt)[ E[Ct - C]V(t) = 0	(29)
j<i
where the first equality holds because each Ct is formed from a minibatch sampled i.i.d. from the
dataset and therefore independent of the filtration. This result holds for all i ∈ [k], therefore
E[MtV+(1t)|Ft] =0.	(30)
Furthermore,
supE[||MiV,t(+t)1||2|Ft]	(31)
P>	P
z	^	>{z------^-------{
= SUp E[v(tt>(Ct- C )> [l - X vjt)vjt)>i	[l- X vjt)vjt)>i (Ct - C)v(t)∣Ft]	(32)
t	j<i	j<i
=SUpE[v(tt>(Ct - C)>P>P(Ct - C)v(t)∣Ft]	(33)
≤ SUpE[λ2v(t)>(Ct - C)>I(Ct - C)v(t)∣Ft]	(34)
≤ sup λ2v(t)>E[(Ct - C)>(Ct - C) ∣Ft]v(t)	(35)
≤ max{1, (i - 2)2}2ξ2	(36)
where λi ≤ max{1, (i - 2)2} is the max singular value of P1 and ξ2 is the maximum eigenvalue of
E[(Ct - C)>(Ct - C)] over all t. Summing over i we find
SUpE[||MiV,t(+t)1||2|Ft] ≤(Xλi)ξ2	(37)
ti
k-2
≤ (2+Xa2)ξ2	(38)
a=1
= (2+1(k - 2)(k - 1)(2k - 3))ξ2	(39)
6
≤ (2+1 k3)ξ2.	(40)
□
11n the worst case, each subspace subtracted off by Vjv> subtracts a 1 from the eigenvalue of 1 of the identity
matrix.
17
Published as a conference paper at ICLR 2022
This is a worst case bound. As the parent eigenvectors converge, the max singular value of P
converges to 1 so that the variance of the magnitude of the martingale difference is upper bounded by
kξ2.
Note: For a finite dataset of n samples or for a distribution with bounded moments like the Gaussian
distribution, ξ will be finite. However, for other distributions like the Cauchy distribution, ξ may be
unbounded, so care should be taken When running μ-EigenGame in different stochastic settings.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
F Jax pseudocode
For the sake of reproducibility We have included pseudocode in Jax. We use the Optax2 optimiza-
tion library Hessel et al. (2020) and the Jaxline training frameWork3. Our graph algorithm is a
straightforWard modification of the provided pseudo-code. See section G for details.
"""
Copyright 2020 The EigenGame Unloaded Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
https://www. apache .org/liCenSeS/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
import jax
import optax
import jax.numpy as jnp
def eg_grads(vi: jnp.ndarray,
weights: jnp.ndarray,
eigs: jnp.ndarray,
data: jnp.ndarray) -> jnp.ndarray:
Args :
vi: shape (d,),
weights: shape
eigs: shape (k,
data: shape (N,
eigenvector to be updated
(k,), mask for penalty coefficients,
d) , i.e., vectors on rows
d), minibatch X_t
Returns :
grads: shape (d,), gradient for Vi
weights_ij = (jnp.sign(weights +0.5) - 1.) / 2.	# maps -1 to -1 else
to 0
data_vi = jnp.dot(data, Vi)
data_eigs = jnp.transpose(jnp.dot(data,
jnp.transpose(eigs))) # XVj on row j
vi_m_vj = jnp.dot(data_eigs, data_vi)
penalty_grads = vi_m_vj * jnp.transpose(eigs)
penalty_grads = jnp.dot(penalty_grads, weights_ij)
grads = jnp.dot(jnp.transpose(data), data_vi) + penalty_grads
return grads
def utility(vi, weights, eigs, data):
2https://github.com/deepmind/optax
3https://github.com/deepmind/jaxline
18
Published as a conference paper at ICLR 2022
47
48
49
50
51
52
53
54
55
56
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
1
2
3
4
5
6
7
8
9
10
"""Compute Eigengame utilities.
util: shape (1,), utility for vi
"""
data_vi = jnp.dot(data, Vi)
data_eigs = jnp.transpose(jnp.dot(data, jnp.transpose(eigs))) # Xvj on
row j
vi_m_vj2 = jnp.dot(data_eigs, data_vi)**2.
vj_m_vj = jnp.sum(data_eigs * data_eigs, axis=1)
r_ij = vi_m_vj2 / vj_m_vj
Util = jnp.dot(jnp.array(r_ij), weights)
return util
Listing 1: Gradient and utility functions.
def _grads_and_update(vi, weights, eigs, input, opt_state,
axis_index_groups):
"""Compute utilities and update directions, PSUm and apply.
Args :
vi: shape (d,), eigenvector to be updated
weights: shape (k_Per device, k,), mask for penalty coefficients,
eigs: shape (k, d), i.e., vectors on rows
input: shape (N, d), minibatch X_t
opt_state: optax state
axis_index_groups: For multi-host parallelism https ://jax.
readthedocs.io/en/latest/_modules/jax/_src/lax/parallel.html
Returns :
vi_new: shape (d,), eigenvector to be updated
opt_state: new optax state
utilities: shape (1,), utilities
"""
grads, utilities = _grads_and_utils(vi, weights, V, input)
avg_grads = jax.lax.psum(
grads, axis_name='i’, axis_index_groups=axis_index_groups)
vi_new, opt_state, lr = _UPdate_with_grads(vi, avg_grads, opt_state)
return vi_new, opt_state, utilities
def _grads_and_utils(vi, weights, V, inputs):
"""Compute utiltiies and update directions ("grads").
Wrap in jax.vmap for k_per device dimension."""
utilities = utility(vi, weights, V, inputs)
grads = eg_grads(vi, weights, V, inputs)
return grads, utilities
def _UPdate_with_grads(vi, grads, opt_state):
"""Compute and apply updates with optax optimizer.
Wrap in jax.vmap for k_per device dimension."""
updates, opt_state = self._OPtimizer.update(-grads, opt_state)
vi_new = optax.apply_UPdates(vi, updates)
vi_new /= jnp.linalg.norm(vi_new)
return vi_new, opt_state
Listing 2: EigenGame Update functions.
def init(self, *):
"""Initialization function for a Jaxline experiment."""
weights = np.eye(self._total_k) * 2 - np.ones((self._total_k, self.
_total_k))
WeightS[np.triu_indiceS(SeIf._total_k, 1)] = 0.
self._weights = jnp.reshape(weights, [self._num_devices,
self. k_per device,
self._total_k])
local_rng = jax.random.fold_in(jax.random.PRNGkey(seed), jax.host_id
())
keys = jax.random.split(local_rng, self._num_devices)
19
Published as a conference paper at ICLR 2022
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
V = jax.pmap(lambda key: jax.random.normal(key, (self. k_Per device,
self._dims)))(keys)
self._V = jax.pmap(lambda V: V / jnp.linalg.norm(V, axis=1, keepdims=
True))(V)
# Define parallel update function. If k_Per device is not None, wrap
individual functions with vmap here.
self._Partial grad update = functools.partial(
self._grads_and_update, axis_groups=self._axis_index_groups)
self._Par grad update = jax.pmap(
, self._Partial grad update, in_axes=(0, 0, None, 0, 0, 0),
self._OPtimizer = optax.sgd(learning_rate=1e-4, momentum=0.9,
nesterov=True)
def
step(self, *):
"""Step function for a Jaxline experiment"""
inputs = next(input_data_iterator)
self._local_V = jnp.reshape(self._V, (self._total_k, self._dims))
self._V, self._opt_state, utilities, lr = self._par grad update(
self._V, self._WeightS_jnp, self._local_V, inputs, self.
_opt_state,
global_SteP)
Listing 3: Skeleton for Jaxline experiment.
G μ-ElGENGAME ON GRAPHS
Algorithm 3 receives a stream of edges represented as a matrix with edges on the rows and
outgoing node id (out) and incoming node id (in) as nonegative integers on the columns. The
method zeros_like(z) returns an array of zeros with the same dimensions as z. The method
index_add(z, idx, val) adds the values in array val to z at the corresponding indices in array idx
with threadsafe locking so that indices in idx may be duplicated. Both methods are available in JAX.
The largest eigenvector ^ι is learned to estimate λι and may be discarded. The bottom-k eigenvectors
are returned by the algorithm in increasing order. Algorithm 3 expects k + 1 random unit vectors as
input rather than k in order to additionally estimate the top eigenvector necessary for the computation;
otherwise, the inputs are the same as Algorithm 1.
H Algorithm Design Process
In section 4.1, We presented 戒 as the Rayleigh quotient of a deflated matrix (repeated in equation (8)
for convencience):
	deflation z	ʌ	{
	=v> I-- ∑Vj-v>]c•同	(41) j<i =v>C∙[Vi] - X(v>Cvj)(∙[Vi]τVj)	(42) j<i ⊥-penalty Var	z			{
uiα	三二	hvi,cvji2 =viCvi-g KW.	(43)
Alternatively, we can consider ur as equation (42) in light of the derivation for Ua by GemP et al.
(2021). In that case, utilities are constructed from entries in the matrix
20
Published as a conference paper at ICLR 2022
Algorithm 3 μ-EigenGame for Graphs (w/o Riemannian gradient projection)
1	: Given: Edge stream Et ∈ Rn0×2, number of parallel machines M per player (minibatch size per partition n00 = M), initial vectors v0 ∈ Sd-1, step size sequence ηt, and iterations T.
2	vi — v0 for all i ∈ {1,...,k + 1]
3	λι J 2|V| *upper bound on top eigenvalue*
4	: for t = 1 : T do
5	: parfor i = 1 : k + 1 do
6	:	parfor m = 1 : M do
7	[Xv]i = vi (Outtm) - vi(intm) [X>Xv]i J zeros_like(vi) :	[X>Xv]i J index_add([X>Xv], outtm, [Xv]i)
8	
9	
10	:	[X>Xv]i J index_add([X>Xv], intm, -[Xv]i)
11	:	if i = 1 then
12	:	λ1 J ||[Xv]i||2
13	Vμ J [X>Xv]i
14	:	else
15	Vm J λ1[vi - P1<j<i (VJvj )vj]
16	[Xv] j = vj (Outtm) - vj (intm) for al∙l j
17	Vw -= [χ>xv]i - pι<j<i(XvIJXvlj)vj
18	:	end if
19	:	end parfor
20	Vμ J n Ptl[Vμ ]
21	^i j vi + ηtV Vi
22	0 viJ π⅜
23	: end parfor
24	: end for
25	return {v∕i ∈ {2,...,k + 1]]
ʌ -T ʌ
V >C^ =
ΓhV1,CV1i
hV2,CV1i
hV1,CV2i
hV2,CV2i
LhVd,cVιi	hVd,cV2i
hV1,CVdiΛ
hV2,CVdi
.
.
.
hVd,CVdi
(44)
It is argued that if V diagonalizes M and captures maximum variance, then the diagonal hvi, Cvii
terms must be maximized and the off-diagonal hvi, Cvji terms must be zero. As the latter mixed
terms may be negative, the authors square the mixed terms to form “minimizable utilities” and divide
them by hvj, Cvj〉so that they have similar “units" to the terms hvi,Cvii ofthe first type. In contrast,
the U utilities could be arrived at by instead multiplying the mixed terms by hvi, ^). While this
ensures the mixed terms are positive with exact parents (because hvi, Cvji = λj<vi, vji), it does
not ensure they are always positive in general4. In other words, ur is defined in way such that the
⊥-penalties actually encourage vectors to align at times when they should in fact do the opposite! We
therefore consider it unlikely that anyone would pose equation (42) as a utility if coming from the
perspective of α-EigenGame.
We could have extended the diagram in Figure 5b to include this dead end link. We have also included
the true gradient of U as a logical endpoint. We present these extensions in Figure 8.
4e.g., let C = 1	1 and place Vι at —30° and ^2 at 90°.
21
Published as a conference paper at ICLR 2022
U	Eq. (42)
uiα
Var & ⊥
remove	bias
Va
Figure 8:	This diagram presents the relationships between utilities and updates. An arrow indicates
the endpoint is reasonably derived from the origin; the lack of an arrow indicates the direction is
unlikely. The link from equation (42) is explicitly crossed out with a hard stop for emphasis.
H.1 Gradient Ascent on 诚
If we remove the stop gradient O from equation (41), we are left with equation (45):
deflation
Z -、
uμ = v> [I - EVj v> ]Cvi.	(45)
j<i
If we then differentiate this utility, we find its gradient is
vμ = CVi- 1 X[(V> CVj )Vj + (v> Vj )CVj].	(46)
j<i
We also reran experiments with this update direction, vμ on the synthetic and MNIST domains. The
update is unbiased, so it would be expected to scale well, however, it (in orange) appears to scale
more poorly than μ-EigenGame with smaller minibatches.
EXPonentiaHy DXCaying SPVCtnJm	Linearly Decaying Spectrum
O
Ooo
4 2
*eφ-JJapoAue。山
φouφ6uo
4J -∏
ω220
6 ≥
0
200	400	600	800 1000	0	200	400	600	800 1000
# of Training Iterations	# of Training Iterations
Figure 9:	Synthetic Experiment. Runtime (milliseconds) in legend.
In Figure 10, μ-EG appears to converge in terms of subspace error but slows in terms of longest
eigenvector streak. μ-EG updates are also unbiased so we would expect it is convergent globally, but
it underperforms relative to μ-EG.In contrast, α-EG stalls in terms of subspace error likely due to
bias.
Note that with exact parents, mu-EG and mu-tilde-EG have the same update (plug Cvj = λj vj into
equation (46)), so the difference must come from when the parents are still inaccurate.
In Figure 11, we have plotted the norm of the difference between subsequent values of the eigenvectors
over training, i.e., how “far” vi moves after every update. Note all algorithms were run with the same
fixed step size of 10-3, which was optimal for each algorithm in this setting. Clearly, the gradient
version of ɑ-EigenGame (μ-EG) shown in Figure 11c exhibits higher norms overall.
We believe this is due to the higher variance penalty terms (all methods maximize the same Rayleigh
quotient term). Note that both α-EG and μ-EG construct their pentalty directions by a weighted
sum of terms. These terms are computed differently, but both compute weights with inner products
between Vi and Vj after projecting onto the samples in the minibatch Xt. For example, μ-EG
22
Published as a conference paper at ICLR 2022
168 O
*3s .!03>u3山
3±lo□36uo~l
MNIST (Minibatch
[7-EG (17)^^^
^^^>^G(16)∣
3uuf-a 3u,°dsqns
168 O00-2
Xe 3s.lo3>u3山 1
3±lo□36uo-l 8cπ⅛-Ω 3udsqns
MNIST (Minibatch =32)
∣4-EG (274)]
[g-EG (291)]
10	20	30	40	50
Epochs
ffG (293)
0	29	58	87	117	0	117	234 351	468	0	937 1875 2812 3750 4687
Iterations (thousands)	Iterations (thousands)	Iterations (thousands)
FigUre 10: MNIST Experiment. RUntime (seconds) in legend. Each colUmn evalUates a different
minibatch size ∈ {1024, 256, 32}.
compUtes hXtvi, Xtvji = vi>Ctvj. WithoUt loss of generality, assUme C is a diagonal matrix (with
the eigenvalUes on its diagonal). Then hvi>Cvji = Pk λkvikvjk. The eigenvectors vi = ei in this
case, and so the inner prodUct measUres alignment between vi and vj in the dimensions that vi and
vj are trained to be orthogonal. DUe to noise in the minibatches Xt , vi and vj may “drift” in the
remaining dimensions. Projecting essentially ignores these thoUgh becaUse they are weighted by
small eigenvalUes.
In contrast, "-EG computes weights as raw inner products between Vi and Vj. Therefore, any drift of
vi and vj dUe to noise in the minibatch samples contribUtes to the inner prodUct: hvi , vji = Pk vikvjk.
We suspect this is the reason μ-EG exhibits higher drift distance.
In summary, α-EG updates are computed as a weighted sum of terms where the weights are computed
using inner products between Vi and its parents after projecting them to a lower dimensional space.
Computing the inner product in this particular space results in lower variance for each inner product.
Unfortunately, α-EG updates are biased, so while they exhibit relatively low “norm of drift”, they
converge to the incorrect solution (parents never converge to precise solution which prohibits children
from learning accurately solutions).
μ-EG updates are unbiased, so they should converge to the correct solution in the limit, but they
exhibit higher variance due to their penalty weights being computed in the original high dimensional
space (i.e., they pick up every little bit of noise).
Finally, μ-EG updates are unbiased and compute their penalty weights in a lower dimensional space,
suppressing the bulk of the noise that appears from drift caused by randomness in the minibatches.
They exhibit the lowest levels of “norm of drift”.
Norm of Drift: μ-EG
Norm of Drift: a-EG
Norm of Drift: μ-EG
Epochs	Epochs	Epochs
(a)	(b)	(c)
Figure 11: MNIST Experiment. SUbfigUres (a-c) correspond to μ-EigenGame, α-EigenGame, and
the gradient version of α-EigenGame discussed above respectively. Each experiment is conducted
with a minibatch size of 32 over five epochs of training and averaged over 10 trials. Each cUrve shows
the Update distance after each iteration for one of the top-16 eigenvectors. Cyan cUrves indicate
eigenvectors higher in the hierarchy (e.g., v1) and magenta cUrves indicate eigenvectors lower in the
hierarchy.
23
Published as a conference paper at ICLR 2022
H.2 Acceleration
We conjecture that μ-EigenGame converges more quickly than α-EigenGame because of the following
two claims.
Claim 1 The penalty terms of 十μ are all within 90° of those of NCa because ^> ^>j ,Vj>= 1 > 0.
Claim 2 The penalty terms of N μ are all smaller in magnitude than those of ▽?: l∣Vj Il ≤ 11 ^Cj 11.
Indeed, consider the direction Cvj. By properties of the vector rejection, we know the rejection of
this direction onto the tangent space of the unit sphere has magnitude less than or equal to that of the
original vector, ∣∣CVj-1∣. The projection is (I 一 VjVj)(Cvj). Therefore, the rejection is VjVj(Cvj)
and, by the preceding argument, we know its magnitude ∣V>CVj |||Vj ∣∣ is less than or equal to ∣∣CVj-1∣.
Rearranging the inequality completes the proof.	□
By Claim 1, the penalty directions of μ-EG and α-EG approximately agree. And by Claim 2, a-EG's
penalty direction is shorter. Consider a scenario where a parent of Vi has not converged and transiently
occupies space along Vi's geodesic to its true endpoint Vi, a strong penalty term will force Gi to
take a roundabout trajectory, thereby slowing its convergence. A weaker penalty term allows Gi to
pass through regions occupied by its parent as long as its parent is not an eigenvector. Recall from
Section 3 that the two utilities are equivalent when the parents are eigenvectors.
24