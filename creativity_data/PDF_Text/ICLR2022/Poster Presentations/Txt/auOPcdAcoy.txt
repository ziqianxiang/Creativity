Published as a conference paper at ICLR 2022
Hybrid Memoised Wake-Sleep: Approximate In-
ference at the Discrete-Continuous Interface
Tuan Anh Le1	Katherine M. Collins1	Luke Hewitt1	Kevin Ellis2
N. Siddharth3	Samuel Gershman4	Joshua B. Tenenbaum1
Ab stract
Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective compositional
scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep
(HMWS), an algorithm for effective inference in such hybrid discrete-continuous
models. Prior approaches to learning suffer as they need to perform repeated
expensive inner-loop discrete inference. We build on a recent approach, Memoised
Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete
variables, and extend it to allow for a principled and effective way to handle
continuous variables by learning a separate recognition model used for importance-
sampling based approximate inference and marginalization. We evaluate HMWS
in the GP-kernel learning and 3D scene understanding domains, and show that it
outperforms current state-of-the-art inference methods.
1 Introduction
We naturally understand the world around us in terms of discrete symbols. When looking at a scene,
we automatically parse what is where, and understand the relationships between objects in the scene.
We understand that there is a book and a table, and that the book is on the table. Such symbolic
representations are often necessary for planning, communication and abstract reasoning. They allow
the specification of goals like “book on shelf” or preconditions like the fact that “book in hand”
must come before “move hand to shelf”, both of which are necessary for high-level planning. In
communication, we’re forced to describe such plans, among many other things we say, in discrete
words. And further, abstract reasoning requires creating new symbols by composing old ones which
allows us to generalize to completely new situations. A “tower” structure made out of books is still
understood as a “tower” when built out of Jenga blocks. How do we represent and learn models that
support such symbolic reasoning while supporting efficient inference?
θ
We focus on a particular class of hybrid generative models pθ (zd, zc, x)
of observed data x with discrete latent variables zd, continuous latent
variables zc and learnable parameters θ with a graphical model structure
shown in Figure 1. In particular, the discrete latent variables zd represent
an underlying structure present in the data, while the remaining continuous
latent variables zc represent non-structural quantities. For example, in the
context of compositional scene understanding, zd can represent a scene
graph comprising object identities and the relationships between them, like
“a small green pyramid is on top of a yellow cube; a blue doughnut leans Figure 1: Hybrid generative
on the yellow cube; the large pyramid is next to the yellow cube” while zc model pθ (zd, zc, x).
represents the continuous poses of these objects. Here, we assume that object identities are discrete,
symbolic variables indexing into a set of learnable primitives, parameterized by a subset of the
generative model parameters θ. The idea is to make these primitives learn to represent concrete
objects like “yellow cube” or “large green pyramid” from data in an unsupervised fashion.
Algorithms suitable for learning such models are based on variational inference or wake-sleep.
However, these algorithms are either inefficient or inapplicable to general settings. First, stochastic
variational inference methods that optimize the evidence lower bound (elb o) using the reparameteri-
zation trick (Kingma & Welling, 2014; Rezende & Mohamed, 2015) are not applicable to discrete
1 MIT, 2Cornell University, 3University of Edinburgh & The Alan Turing Institute, 4Harvard University
1
Published as a conference paper at ICLR 2022
(a)	Trends in time-series: Learning Gaussian process (gp) kernel to fit data (blue). Extrapolation (orange) shown
with inferred kernel expression (below).
Block(“Blue”， Pos(-1.2， 1.0))
Block(“Green”，Pos(1.0, 1.0))
‘ON’ Gnd(Pos(0, 0)),
Block(“Red”，Pos(-1.4, 0.0))
‘ON' Block(“Red”， Pos(-1.6,
0.0))
‘ON' Block(“Blue”，Pos(-1.8,
0.0))
‘ON’ Gnd(Pos(1, 1))
‘ON’ Block(“Red”， Pos(-1.1，
1.0))
‘ON’ Block(“Blue”， Pos(-1.0，
1.0))
‘ON’ Gnd(Pos(0, 1))
Block(“Red”， Pos(0.8， 0.0))
‘ON’ Gnd(Pos(1, 0))，
Block(“Green”， Pos(-1.1， 0.0))
‘ON’ Gnd(Pos(1, 1))
(b)	Compositional scene understanding over primitives， attributes， and spatial arrangements.
Figure 2: Examples highlighting the hybrid discrete-continuous nature of data. Colours indicate samples from
discrete (violet) and continuous (cyan) random variables respectively.
latent variable models. The reinforce gradient estimator (Williams， 1992)， on the other hand， has
high variance and continuous relaxations of discrete variables (Jang et al.， 2017; Maddison et al.，
2017) don’t naturally apply to stochastic control flow models (Le et al.， 2019). Second， wake-sleep
methods (Hinton et al.， 1995; Dayan et al.， 1995) like reweighted wake-sleep (rws) (Bornschein &
Bengio， 2015; Le et al.， 2019) require inferring discrete latent variables at every learning iteration，
without saving previously performed inferences. Memoised wake-sleep (mws) (Hewitt et al.， 2020)
addresses this issue， but is only applicable to purely discrete latent variable models.
We propose hybrid memoised wake-sleep (hmws)—a method for learning and amortized inference
in probabilistic generative models with hybrid discrete-continuous latent variables. hmws combines
the strengths of mws in memoising the discrete latent variables and rws in handling continuous
latent variables. The core idea in HMWS is to memoise discrete latent variables zd and learn a
separate recognition model which is used for importance-sampling based approximate inference and
marginalization of the continuous latent variables zc .
We empirically compare hmws with state-of-the-art (i) stochastic variational inference methods
that use control variates to reduce the reinforce gradient variance， vimco (Mnih & Rezende，
2016)， and (ii) a wake-sleep extension， rws. We show that hmws outperforms these baselines in two
domains: structured time series and compositional 3D scene understanding， respectively.
2 Background
Our goal is to learn the parameters θ of a generative model pθ(z, x) of latent variables z and data x，
and parameters φ of a recognition model q°(z∣x) which acts as an approximation to the posterior
pθ (z |x). This can be achieved by maximizing the evidence lower bound
ELBO (x,pθ(z,x),qφ(z∣x)) = logPθ(x) - KL(qφ(z∣x)∣∣pθ(z∣x))	(1)
which maximizes the evidence logpθ(x) while minimizing the Kullback-Leibler (KL) divergence，
thereby encouraging the recognition model to approximate the posterior.
If the latent variables are discrete， a standard way to estimate the gradients of the elbo with respect
to the recognition model parameters involves using the reinforce (or score function) gradient
estimator (Williams， 1992; Schulman et al.， 2015)
VφELBO (x,pθ(z,x),qφ(z∣x)) ≈ log
Pθ (z,x)
qφ(ZIx)
• Vφ log qφ(z∣x) + Vφ log
Pθ(z,x)
qφ(ZIx)
(2)
2
Published as a conference paper at ICLR 2022
Wake
{Z'd}N=ι 〜qφGd |X)
{Zm} M=ι = Top M ιatents from {Zm} M=ι ∪{Z n} }n= based on p乜d, X)
Figure 3: Learning phases of Hybrid Memoised wake-sleep. The memory stores a set of discrete latents for each
training data point x. The Wake phase updates the memory using samples from the recognition model. The
Sleep: Replay phase updates the generative model and the recognition model using the memory. The Sleep:
Fantasy phase updates the recognition model using samples from the generative model.
where Z 〜qφ(z∣x). However, the first term is often high-variance which makes learning inefficient.
This issue can be addressed by (i) introducing control variates (Mnih & Gregor, 2014; Mnih &
Rezende, 2016; Tucker et al., 2017; Grathwohl et al., 2018) which reduce gradient variance, (ii)
continuous-relaxation of discrete latent variables to allow differentiation (Jang et al., 2017; Maddison
et al., 2017), or (iii) introducing a separate “wake-sleep” objective for learning the recognition
model that is trained in different steps and sidesteps the need to differentiate through discrete latent
variables (Hinton et al., 1995; Dayan et al., 1995; Bornschein & Bengio, 2015; Le et al., 2019).
2.1	Memoised wake-sleep
Both elb o-based and wake-sleep based approaches to learning require re-solving the inference task
by sampling from the recognition model qφ(z∣χ) at every iteration. This repeated sampling can be
wasteful, especially when only a few latent configurations explain the data well. Memoised wake-
sleep (MWS) (Hewitt et al., 2020) extends wake-sleep by introducing a memory—a set of M unique
discrete latent variables {zdm}mM=1 for each data point x which induces a variational distribution
M
qMEM (zd |x) =	ωmδzdm (zd),	(3)
m=1
consisting of a weighted set of delta masses δzm centered on the memory elements (see also
Saeedi et al. (2017)). This variational distribution is proven to improve the evidence lower bound
ELBO(x, pθ(zd, x), qMEM(zd|x)) (see Sec. 3 of (Hewitt et al., 2020)) by a memory-update phase
comprising (i) the proposal of N new values {z0n}N=ι 〜qφ(zd|x), (ii) retaining the best M values
from the union of the old memory elements and newly proposed values {zdm}mM=1 ∪ {z0dn}nN=1 scored
by pθ(zd, x), and (iii) setting the weights toωm = pθ (zm,x)/PM=ipθ (Zd,X).
Mws, however, only works on models with purely discrete latent variables. If we try to use the same
approach for hybrid discrete-continuous latent variable models, all proposed continuous values are
will be unique and the posterior approximation will collapse onto the MAP estimate.
2.2	Importance sampling based approximate inference and marginalization
In our proposed method, we will rely on importance sampling (is) to perform approximate inference
and marginalization. In general, given an unnormalized density γ(Z), and its corresponding normaliz-
ing constant Z = / γ(z)dz and normalized density ∏(z) = Y(z)/Z, we want to estimate Z and the
expectation of an arbitrary function f, Eπ(z) [f (Z)]. To do this, we sample K values {Zk}kK=1 from a
proposal distribution P(Z) and weight each sample by Wk = γ(zk), leading to the estimators
1K	K
Z ≈ K X Wk =: Z,	E∏(z)[f (z)] ≈ X Wk f(zk)=: I,	(4)
where Wk = Wk/(KZ) is the normalized weight. The estimator Z is often used to estimate marginal
distributions, for example p(x), with γ (Z) being the joint distribution p(Z, x). It is unbiased and
its variance decreases linearly with K. The estimator I is often used to estimate the posterior
expectation of gradients, for example the “wake-。" gradient of RWS (Bornschein & Bengio, 2015;
Le et al., 2019), Epθ(^∣χ)[-Vφ log qφ(z∣x)] with Y(Z) = pθ(z,x), π(z) = pθ(z|x) and f(z)=
-Vφ log qφ(z∣χ). This estimator is asymptotically unbiased and its asymptotic variance decreases
linearly with K (Owen, 2013, Eq. 9.8) which means increasing K improves the estimator.
3
Published as a conference paper at ICLR 2022
3 Hybrid Memoised Wake-Sleep
We propose hybrid memoised wake-sleep (hmws) which extends memoised wake-sleep (mws) to
address the issue of memoization of continuous latent variables. In hmws, we learn a generative
model pθ (zd , zc, x) of hybrid discrete (zd) and continuous (zc) latent variables, and a recognition
model qφ(zd, z/x) which factorizes into a discrete recognition model qφ(zd∖χ) and a continuous
recognition model qΦ(zc∖zd, x). Like MWS, HMWS maintains a memory of M discrete latent variables
{zdm}mM=1 per data point x which is updated in the wake phase of every learning iteration. In the
sleep: replay phase, we use the memoized discrete latents to train both the generative model and the
recognition model. In the sleep: fantasy phase, we optionally train the recognition model on data
generated from the generative model as well. We summarize these learning phases in Fig. 3, the
full algorithm in Alg. 1, and describe each learning phase in detail below. For notational clarity, we
present the algorithm for a single training data point x.
3.1	WAKE
Given a set of M memory elements {zdm}mM=1, we define the memory-induced variational posterior
qMEM(zd∖x) = PmM=1 ωmδzm (zd) similarly as in MWS (eq. (3)). If we knew how to evaluate
pθ(zdm, x), the wake-phase for updating the memory of HMWS would be identical to MWS. Here, we
use an IS-based estimator of this quantity based on K samples from the continuous recognition model
pθ(zm,x)=KXWmk, Wmk=p⅛m⅜⅛, zmk-qφ(Zc∖zm,x).	⑸
The IS weights {{Wmk}kK=1}mM=1 are used for marginalizing zc here but they will later be reused for
estimating expected gradients by approximating the continuous posterior pθ (zc ∖zdm, x). Given the
estimation ofpθ(zdm, x), the weights {ωm}mM=1 of the memory-induced posterior qMEM(zd∖x) are
ω —	pθ(Zm，X)	— 表 Pk = 1 Wmk	_	Pk=I Wmk	-
m ^mM i	∖~^∙M 1 ∖-^K	∖-`∖M^ ∖~`∖K .
Ei=I pθ (Zd, x) Ei=I κ Ej=I Wij	Ei=I Ej=I Wij
The resulting wake-phase memory update is identical to that of mws (Sec. 2.1), except that the
evaluation of pθ (Zdm, x) and ωm is done by (5) and (6) instead. For large K , these estimators are
more accurate, and if exact, guaranteed to improve ELBO(x, pθ(Zd, x), qMEM(Zd∖x)) like in MWS.
3.2	Sleep: Replay
In this phase, we use the memoized discrete latent variables {Zdm}mM=1 (“replay from the memory”) to
learn the generative modelpθ(Zd, Zc, x), and the discrete and continuous recognition models qφ(Zd∖x)
and qφ(Zc∖Zd, x). We now derive the gradient estimators used for learning each of these distributions
in turn and summarize how they are used within hmws in the “Sleep: Replay” part of Alg. 1.
Algorithm 1 Hybrid Memoised Wake-Sleep (a single learning iteration)
Input: Generative modelpθ (Zd,Zc,x), recognition model qφ(Zd|x)qφ(zc∣zd,x), data point x, memory elements
{zdm}mM=1, replay factor λ ∈ [0, 1], # of importance samples K, # of proposals N.
Output: Gradient estimators for updating θ and φ: gθ and gφ; updated memory elements {Zdm}mM=1.
1: Propose {z0n}N=ι 〜qφ(zd∣x)	. Wake
2: Select {zd}* l 11=i — Unique({zm}M= ∪{z0n }N=ι) where L ≤ M + N
3: Sample {zik }K=ι 〜qφ(zc∣zd,x) for i = 1,...,L
4: Compute {{wik}k=ι}L=ι, {pθ(Zd,x)}L=ι (eq. (5)), and {ωi}L=ι (eq. (6))
5: Select {zm, {zmk}k=ι, {wmk}K=ι,ωm}M=ι	― best M values from {zd}l=i, {{zCk}3ι}L=ι,
{{wik}k=1}L1 and {ωi}L=ι according to {pθ(zd,x)}L=ι
6: Compute generative model gradient gθ — gθ({wmk,zCllk)m==1 k=ι) (eq. (7))	. Sl
7: COmPUte gφ,replay - gφ,REPLAYHzm,ωm}M=I)(即 ⑼)
8: Compute gφ,replay — gφ,replay({wmk,zmk}M=K,k=ι) (eq.(10))
9: Compute。配丁八$丫 <——-=PK=I Vφ logqφ(zk|xk) from Zk,xk 〜pθ(z,x)	. Sleep: Fantasy
10: Aggregate recognition model gradient gφ 一 λ(gφ,replay + gφ,replay) + (1 — Ng黑TASY
11: Return: gθ, gφ, {Zdm }mM=1.
4
Published as a conference paper at ICLR 2022
Learning the generative model. We want to minimize the negative evidence - logpθ(x). We
express its gradient using Fisher's identity Vθ logpθ(x) = Ep@[Vθ logpθ(z,x)] which We
estimate using a combination of the memory-based approximation of pθ(zd|x) and the is-based
approximation of pθ (zc |x). The resulting estimator reuses the samples Zmk and weights Wmk from
the wake phase (see Appendix A for derivation):
MK
-Vθ logpθ (x) ≈-XX
vmkVθlogpθ(zdm, zcmk,x)
m=1 k=1
gθ({Wmk, zcmk}mM=,K1,k=1),
where vmk
Wmk
PM=1 PK=1 Wij .
(7)
(8)
Learning the discrete recognition model. We want to minimize kl(pθ (zd∣χ)∣∣qφ(zd∣χ)) whose
gradient can be estimated using a memory-based approximation of pθ (zd |x) (eq. (3))
vφKL(Pθ(ZdIx)M(ZdIx)) = Epθ(zd∣x) [-vΦlogqφ(ZdIx)]
M
≈ - X ωmVφlogqφ(ZdmIx) =: gdφ,REPLAY({Zdm, ωm}mM=1),	(9)
m=1
where memory elements {Zdm}mM=1 and weights {ωm}mM=1 are reused from the wake phase (6).
Learning the continuous recognition model. We want to minimize the average of
KL(pθ(ZcIZdm, x)IIqφ(ZcIZdm, x)) over the memory elements Zdm. The gradient of this KL is esti-
mated by an Is-based estimation ofpθ(ZcIZdm, x)
1M	1M
M E vφ KL(Pθ (ZcIZd ,x)∖∖qφ(zc∖zd ,X)) = M E Epθ (zc∣zm,x) [-vφ log qφ (ZcIZd , x)]
MK
≈ - M X X Wmk Vφ log qφ(Zmk IZm,x) =: gφ,REPLAY ({Wmk ,Zmk }MM=K,k=1 ),	(⑼
m=1 k=1
where Wmk = Wmk/ PK=I Wmi. Both weights and samples Zmk are reused from the wake phase (5).
3.3 Sleep: Fantasy
This phase is identical to “sleep” in the original wake-sleep algorithm (Hinton et al., 1995; Dayan
et al., 1995) which minimizes the Monte Carlo estimation of Epθ(zd,zc,x) [-vφ log qφ(Zd, ZcIx)],
equivalent to training the recognition model on samples from the generative model.
Together, the gradient estimators in the Wake and the Sleep: Replay and the Sleep: Fantasy phases are
used to learn the generative model, the recognition model and the memory. We additionally introduce
a model-dependent replay factor λ ∈ [0, 1] which modulates the relative importance of the Sleep:
Replay versus the Sleep: Fantasy gradients for the recognition model. This is similar to modulating
between the “wake-” and “sleep-” updates of the recognition model in rws (Bornschein & Bengio,
2015). We provide a high level overview of hmws in Fig. 3 and its full description in Alg. 1.
Factorization of the recognition model. The memory-induced variational posterior qMEM(ZdIx)
is a weighted sum of delta masses on {Zdm}mM=1 which cannot model the conditional posterior
Pθ(ZdIx, Zc) as it would require a continuous index space Zc. We therefore use qMEM(ZdIx) to
approximate Pθ(ZdIx) and qφ(ZcIZd, x) to approximate Pθ(ZcIZd, x), which allows capturing all
dependencies in the posterior Pθ (Zc, ZdIx). Relaxing the restriction on the factorization ofqφ(Zc, ZdIx)
is possible, but, we won’t be able to model any dependencies of discrete latent variables on preceding
continuous latent variables.
4 Experiments
In our experiments, we compare hmws on hybrid generative models against state-of-the-art wake-
sleep-based rws and variational inference based vimco. To ensure a fair comparison, we match
the number of likelihood evaluations - typically the most time-consuming part of training - across
5
Published as a conference paper at ICLR 2022
Figure 4: Hybrid memoised wake-sleep (HMWS) learns faster than the baselines: reweighted wake-sleep (RWS)
and vimco based on the marginal likelihood, in both the time series model (left), and the scene understanding
models with learning shape and color (middle) and learning shape only (right). HMWS also learns better scene
understanding models. The gradient estimator of vimco was too noisy and failed to learn the time series model.
(Median with the shaded interquartile ranges over 20 runs is shown.)
all models: O(K(N + M)) for HMWS, and O(S) for RWS and VIMCO where S is the number of
particles for RWS and VIMCO. HMWS has a fixed memory cost of O(M) per data point which can
be prohibitive for large datasets. However in practice, hmws is faster and more memory-efficient
than the other algorithms as the total number of likelihood evaluations is typically smaller than the
upper bound of K(N + M) (see App. C). For training, we use the Adam optimizer with default
hyperparameters. We judge the generative-model quality using an Stest = 100 sample importance
weighted autoencoder (IWAE) estimation of the log marginal likelihood logpθ(x). We also tried the
variational-inference based reinforce but found that it underperforms vimco.
We focus on generative models in which (i) there are interpretable, learnable symbols, (ii) the
discrete latent variables represent the composition of these symbols and (iii) the continuous latent
variables represent the non-structural quantitites. While our algorithm is suitable for any hybrid
generative model, we believe that this particular class of neuro-symbolic models is the most naturally
interpretable, which opens up ways to connect to language and symbolic planning.
4.1 Structured time series
Figure 5: The generative model of structured time series data consists of (i) a prior that uses LSTMs to first
sample the discrete Gaussian process (GP) kernel expression and then the continuous kernel parameters, and
(ii) a likelihood which constructs the final GP kernel expression. The recognition model (on gray background)
mirrors the architecture of the prior but additionally conditions on an lstm embedding of data.
We first apply HMWS to the task of finding explainable models for time-series data. We draw
inspiration from (Duvenaud et al., 2013), who frame this problem as GP kernel learning (Rasmussen
& Williams, 2006). They describe a grammar for building kernels compositionally, and demonstrate
that inference in this grammar can produce highly interpretable and generalisable descriptions of the
structure in a time series. We follow a similar approach, but depart by learning a set of gp kernels
jointly for each in time series in a dataset, rather than individually.
For our model, we define the following simple grammar over kernels
k → k + k | k × k | WN | SE | PER | C, where	(11)
• WN is the White Noise kernel, WNσ2 (x1 , x2) = σ2Ix1=x2,
6
Published as a conference paper at ICLR 2022
Per + SE	Per + WN	WN
Per	Per	WN + SE
Figure 6: Learning to model structured time series data with GaUssian processes (GPs) by first inferring the kernel
expression (shown as text in the top-left corner) and the kernel parameters. The blue curve is the 128-dimensional
observed signal, and the orange curve is a GP extrapolation based on the inferred kernel. We show the mean
(dark orange), the ±2 standard deviation range (shaded) and one sample (light orange).
• SE is the Squared Exponential kernel, SEσ2,12 (x1,x2) = σ2 exp(一(xι — x2)2/2/2),
• PER is a Periodic kernel, PERσ-2,p,i2 (xι,x2) = σ2 exp(—2sin2(π∣xι — x2|/p)//2),
• C isa Constant, C。？(xι,x2) = σ2.
We wish to learn a prior distribution over both the symbolic structure of a kernel and its continuous
variables (σ2, l, etc.). To represent the structure of the kernel as zd, we use a symbolic kernel ‘ex-
pression’: a string over the characters {(, ), +, ×, WN, SE, PER1, . . . , PER4, C}. We provide multiple
character types PERi for periodic kernels, corresponding to a learnable coarse-graining of period
(short to long). We then define an LSTM prior pθ (zd) over these kernel expressions, alongside a
conditional LSTM prior pθ(z∕zd) over continuous kernel parameters. The likelihood is the marginal
gp likelihood—a multivariate Gaussian whose covariance matrix is constructed by evaluating the
composite kernel on a fixed set of points. Finally, the recognition model qφ(zd, zc|x) mirrors the
architecture of the prior except that all the lstms additionally take as input an lstm embedding of
the observed signal x. The architecture is summarized in Fig. 5 and described in full in App. B.1.
We construct a synthetic dataset of 100 timeseries of fixed length of 128 drawn from a probabilistic
context free grammar which is constructed by assigning production probabilities to our kernel
grammar in (11). In Fig. 4 (left), we show that hmws learns faster than rws in this domain in terms
of the log evidence logpθ(x). We also trained with VIMCO but due to the high variance of gradients,
they were unable to learn the model well.
Examples of latent programs discovered by our model are displayed in Fig. 6. For each signal, we
infer the latent kernel expression zd by taking the highest scoring memory element zdm according
to the memory-based posterior qMEM(zd|, x) and sample the corresponding kernel parameters from
the continuous recognition model qφ(z∕zd,χ). We show the composite kernel as well as the GP
posterior predictive distribution. These programs describe meaningful compositional structure in the
time series data, and can also be used to make highly plausible extrapolations.
4.2 Compositional scene understanding
Prior P(Zd, Zc	Likelihood p§(X | z6, Zc	Data X
Recognition model qφ (zd, zc | X)
Figure 7: Generative model of compositional scenes. The prior places a stochastic number of blocks into each
cell, where cells form an imaginary grid on the ground plane. For each cell, a stack of blocks is built by both: i)
sampling blocks from a learnable set of primitives and ii) sampling their relative location to the object below (i.e.,
either the ground or the most recently stacked block). The likelihood uses a differentiable renderer to produce
an image. The recognition model (on gray background) mirrors the structure of the prior, but parametrizes
distributions as learnable functions (NN1-NN4) of a CNN-based embedding of the image.
Next, we investigate the ability of HMWS to parse images of simple scenes in terms of stacks of toy
blocks. Here, towers can be built from a large consortium of blocks, where blocks are drawn from a
fixed set of block types. We aim to learn the parameters of the different block types - namely, their
size and optionally their color - and jointly infer a scene parse describing which kinds of blocks live
7
Published as a conference paper at ICLR 2022
where in each world. Such a symbolic representation of scenes increases interpretability, and has
connections to language and symbolic planning.
Our generative and recognition models are illustrated in Fig. 7 (for the full description of the model,
see App. B.2). We divide the ground plane into an N × N (N = 2) grid of square cells and initialize
a set of P = 5 learnable block types, which we refer to as the base set of “primitives” with which
the model builds scenes. We parameterize each block type by its size and color. Within each cell,
we sample the number of blocks in a tower uniformly, from zero to a maximum number of blocks
Bmax = 3. For each position in the tower, we sample an integer ∈ [0, Bmax], which we use to index
into our set of learnable primitives, and for each such primitive, we also sample its continuous
position. The (raw) position of the first block in a tower is sampled from a standard Gaussian and
is constrained to lie in a subset of the corresponding cell, using an affine transformation over a
sigmoid-transformed value. A similar process is used to sample the positions of subsequent blocks,
but now, new blocks are constrained to lie in a subset of the top surface of the blocks below.
Given the absolute spatial locations and identities of all blocks, we render a scene using the PyTorch3D
differentiable renderer (Ravi et al., 2020), which permits taking gradients of the generative model
probability pθ(zd, zc, x) with respect to the learnable block types θ. All training scenes are rendered
from a fixed camera position (a front view); camera position is not explicitly fed into the model. We
use an independent Gaussian likelihood with a fixed standard deviation factorized over all pixels -
similar to using an L2 loss. The discrete latent variables, zd, comprise both the number of blocks and
block type indices, while the continuous latent variables, zc, represent the raw block locations.
The recognition model qφ(zd, zc|x) follows a similar structure to that of the prior p(zd, zc), as shown
inside the gray box of Fig. 7. However, the logits of the categorical distributions for q@(zd|x), as well
as the mean and (log) standard deviation of the Gaussian distributions for qφ(zc∖zd, x), are obtained
by mapping a convolutional neural network (CNN)-based embedding of the image x through small
neural networks NN1-NN4; in our case, these are linear layers.
We train two models: one which infers scene parses from colored blocks and another which reasons
over unicolor blocks. In both settings, the model must perform a non-trivial task of inferring a scene
parse from an exponential space of PBmax'N possible scene parses. Moreover, scenes are replete
with uncertainty: towers in the front of the grid often occlude those in the back. Likewise, in the
second setting, there is additional uncertainty arising from blocks having the same color. For each
model, we generate and train on a dataset of 10k scenes. Both models are trained using HMWS with
K = 5, M = 5, N = 5, and using comparison algorithms with S = K(N + M) = 50.
In Fig. 4 (middle and right), we show that in this domain, hmws learns faster and better models
than RWS ad VIMCO, scored according to the log evidence log pθ (x). We directly compare the
wall-clock time taken by hmws compared to rws in Table 1, highlighting the comparative efficiency
of our algorithm. Further, we uncovered two local minima in our domain; we find that a higher
portion of hmws find the superior minima compared to rws. In Fig. 8, we show posterior samples
from our model. Samples are obtained by taking the highest probability scene parses based on the
memory-based posterior approximation qMEM(zd∖x). These samples illustrate that HMWS-trained
models can capture interesting uncertainties over occluded block towers and recover the underlying
building blocks of the scenes. For instance, in the colored block domain, we see that the model
properly discovers and builds with red, blue, and green cubes of similar sizes to the data.
5	Related work
Our work builds on wake-sleep algorithms (Hinton et al., 1995) and other approaches that jointly train
generative/recognition models, particularly modern versions such variational autoencoders (Kingma
& Welling, 2014) and reweighted wake-sleep (Bornschein & Bengio, 2015). While variational
autoencoders (vaes) have been employed for object-based scene understanding (e.g. Eslami et al.
(2016); Greff et al. (2019)), they attempt to circumvent the issues raised by gradient estimation with
discrete latent variables, either by using continuous relaxation (Maddison et al., 2017) or limited use
of control variates (Mnih & Rezende, 2016). The wake-sleep family of algorithms avoids the need
for such modification and is better suited to models that involve stochastic branching (see Le et al.
(2019) for a discussion), and is thus our primary focus here.
8
Published as a conference paper at ICLR 2022
Inferring scene parse with color
Posterior Samples
Observations
Inferring scene parse without color
Posterior Samples
Observations
19dEes Z 9dEes m 9dEes
19dEes Z 9dEes m 9dEes
Figure 8: Samples from the posterior when inferring scene parses with color (left) and scene parses without
color diversity (right). Conditioned on a single observation of the front view of a scene (left column), hmws
infers a posterior over blocks that make up the scene. Three samples from the posterior are shown per scene,
sorted by log probability under the model; e.g., the first sample is most probable under hmws. Sampled scenes
are rendered from three different camera angles; position of the camera is depicted in the figure insets. We
emphasize that the model has never seen the 3/4-views. The sampled scenes highlight that we are able to handle
occlusion, capturing a distribution over possible worlds that are largely consistent with the observation.
Our most closely related work is Memoised Wake-sleep, which we build directly on top of and
extend to handle both discrete and continuous latents. A contemporary of Memoised Wake-sleep,
DreamCoder (Ellis et al., 2021), is a closely related program synthesis algorithm following a similar
wake-sleep architecture. Like mws, it lacks principled handling of latent continuous parameters;
performing inner-loop gradient descent and heuristically penalizing parameters via the Bayesian
Information Criterion (MacKay, 2002). Other instances of wake-sleep models include those that
perform likelihood-free inference (Brehmer et al., 2020), or develop more complex training schemes
for amortization (Wenliang et al., 2020).
Broadly, our goal of inferring compositionally structured, mixed discrete-continuous objects has
strongest parallels in the program synthesis literature. One family of approaches (e.g. HOU-
DINI (Valkov et al., 2018) and NEAR (Shah et al., 2020)), perform an outer-loop search over discrete
structures and an inner-loop optimization over continuous parameters. Others jointly reason over
continuous and discrete parameters via exact symbolic methods (e.g. Evans et al. (2021)) or by
relaxing the discrete space to allow gradient-guided optimization to run on the whole problem (e.g.
DeepProbLog (Manhaeve et al., 2018)). None of these however, learn-to-learn by amortizing the cost
of inference, with recent attempts needing quantized continuous parameters (Ganin et al., 2021). What
our work contributes to this space is a generic and principled way of applying amortized inference to
hybrid discrete-continuous problems. This gets the speed of a neural recognition model-Which is
valuable for program synthesis-but with Bayesian handling of uncertainty and continuous parameters,
rather than relying on heuristic quantization or expensive inner loops.
6	Discussion
Inference in hybrid generative models is important for building interpretable models that generalize.
However, such a task is difficult due to the need to perform inference in large, discrete spaces. Unlike
deep generative models in which the generative model is less constrained, learning in symbolic
models is prone to getting stuck in local optima. While compared to existing algorithms, hmws
improves learning and inference in these models, it does not fully solve the problem. In particular, our
algorithm struggles with models in which the continuous latent variables require non-trivial inference,
as the quality of continuous inference is directly linked with the quality of the gradient estimators in
hmws. This challenge can potentially be addressed with better gradient estimators. We additionally
plan to extend our algorithm to more complex, realistic neuro-symbolic models; for instance, those
with more non-cuboidal primitive topologies.
9
Published as a conference paper at ICLR 2022
References
Jorg Bornschein and YoshUa Bengio. ReWeighted wake-sleep. In International Conference on
Learning Representations, 2015.
Johann Brehmer, Gilles LoUppe, JUan Pavez, and Kyle Cranmer. Mining gold from implicit models
to improve likelihood-free inference. Proceedings of the National Academy of Sciences, 117(10):
5242-5249, 2020.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The Helmholtz machine.
Neural computation, 7(5):889-904, 1995.
David DUvenaUd, James Lloyd, Roger Grosse, JoshUa TenenbaUm, and Ghahramani ZoUbin. StrUctUre
discovery in nonparametric regression throUgh compositional kernel search. In International
Conference on Machine Learning, 2013.
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Lucas Morales, Luke Hewitt, Luc
Cary, Armando Solar-Lezama, and JoshUa B TenenbaUm. Dreamcoder: Bootstrapping indUctive
program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN
International Conference on Programming Language Design and Implementation, pp. 835-850,
2021.
SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton,
et al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in Neural
Information Processing Systems, 2016.
Richard Evans, Matko Bosnjak, Lars Buesing, Kevin Ellis, David Pfau, Pushmeet Kohli, and Marek
Sergot. Making sense of raw input. Artificial Intelligence, 299:103521, 2021.
Yaroslav Ganin, Sergey Bartunov, Yujia Li, Ethan Keller, and Stefano Saliceti. Computer-aided
design as language. Advances in Neural Information Processing Systems, 34, 2021.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. In International
Conference on Learning Representations, 2018.
Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel
Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation
learning with iterative variational inference. In International Conference on Machine Learning,
2019.
Luke Hewitt, Tuan Anh Le, and Joshua Tenenbaum. Learning to learn generative programs with
memoised wake-sleep. In Uncertainty in Artificial Intelligence, 2020.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The “wake-sleep” algorithm
for unsupervised neural networks. Science, 1995.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In
International Conference on Learning Representations, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations, 2014.
Tuan Anh Le, Adam R. Kosiorek, N. Siddharth, Yee Whye Teh, and Frank Wood. Revisiting
reweighted wake-sleep for models with stochastic control flow. In Uncertainty in Artificial
Intelligence, 2019.
David J. C. MacKay. Information Theory, Inference & Learning Algorithms. Cambridge University
Press, USA, 2002. ISBN 0521642981.
C Maddison, A Mnih, and Y Teh. The Concrete distribution: A continuous relaxation of discrete
random variables. In International Conference on Learning Representations, 2017.
10
Published as a conference paper at ICLR 2022
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.
Deepproblog: Neural probabilistic logic programming. Advances in Neural Information Processing
Systems, 2018.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
International Conference on Machine Learning, 2014.
Andriy Mnih and Danilo Rezende. Variational inference for Monte Carlo objectives. In International
Conference on Machine Learning, 2016.
Art B Owen. Monte Carlo theory, methods and examples. 2013.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.
Adaptive computation and machine learning. MIT Press, 2006. ISBN 026218253X.
Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and
Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning, pp. 1530-1538. PMLR, 2015.
Ardavan Saeedi, Tejas D Kulkarni, Vikash K Mansinghka, and Samuel J Gershman. Variational
particle approximations. The Journal of Machine Learning Research, 18(1):2328-2356, 2017.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, 2015.
Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. Learn-
ing differentiable programs with admissible neural heuristics. Advances in neural information
processing systems, 2020.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. REBAR:
Low-variance, unbiased gradient estimates for discrete latent variable models. In Advances in
Neural Information Processing Systems, 2017.
Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. Houdini:
Lifelong learning as program synthesis. Advances in Neural Information Processing Systems,
2018.
Li Wenliang, Theodore Moskovitz, Heishiro Kanagawa, and Maneesh Sahani. Amortised learning by
wake-sleep. In International Conference on Machine Learning, 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
11
Published as a conference paper at ICLR 2022
A Derivation of the gradient estimator for learning the
GENERATIVE MODEL
Here, we derive the gradient estimator for learning the generative model given in (7).
We assume that the memory-induced variational posterior approximates the discrete posterior
pθ(zd|x) ≈ qMEM(zd|x)
M
=	ωmδzdm (zd)
m=1
(12)
(13)
and that the continuous posterior expectation Ep@(zc|z7,x)[f (zc)] for each m is approximated using a
set of weighted samples (Zmk, Wmk)K=I
K
Epθ(zc |zdm,x) [f (Zc)] ≈ X Wmk f (Zmk ),
k=1
(14)
where Wmk
Wmk
PK=I Wmi
First, we make use of the Fisher’s identity
Vθ logPθ(x) = Epθ(z∣χ) [Vθ logPθ(x)]
=Epθ(z∣χ) [Vθ logPθ(x) + Vθ logPθ(z∣x)]
=epθ(z∣x) [vΘ logPθ(z,x)]
and We continue by using the approximations in (12)-(14)
= Epθ (zc |zd,x) Epθ (zd |x) [Vθ log pθ (z, x)]
≈ Epθ (zc |zd,x) Eq
MEM(zd|x) [Vθ log pθ(z, x)]
M
=Epθ(zc ∣zm,x) EωmvθlθgPθ (Zm,zc,x)
m=1
(Integrand is not a function of z)
(Second term vanishes)
(Product rule of probability)
(Factorize the posterior)
(Use (12))
(Use (13))
K
≈ EWmk
k=1
MK
ωmVθ log Pθ (zm,zmk ,x)
(Use (14))
(15)
(16)
(17)
(18)
(19)
(20)
(21)
=	vmkVθ logpθ(zdm, zcmk, x),
m=1 k=1
where
(Combine sums)
(22)
vmk
Wmk ωm
(23)
Wmk
Pi=1 Wmi
Wmk
Pi=1 Wmi
∖-`∖M^ k^.K
i=1	j=1 Wij
(24)
∖-`∖M^	∖~'∖K
i=1	j=1 Wij
(25)
B Model Architecture and Parameter Settings
B.1	Structured time series
Fixed parameters
•	Vocabulary V = {WN, SE, PER1, . . . , PER4, C, ×, +, (, )}
•	Vocabulary size vocabulary_size = |V| = 11
•	KerneI ParameterS K = {σWz , (σsE, 'se ), (σPERι ,pPERι ,'PeR] ) ,∙∙∙, SPeR4 ,pPER4 , 'pER4 ), σC }
12
Published as a conference paper at ICLR 2022
•	Kernel parameters size kernel_params_size = |K| = 16
•	Hidden size hidden_size = 128
•	Observation embedding size obs_embedding_size = 128
Generative model pθ (zd, zc, x)
•	Kernel expression LSTM (p)
(input size = vocabulary_size, hidden size = hidden_size)
-	This module defines a distribution over the kernel expression pθ (zd).
-	At each time step t, the input is a one-hot encoding of the previous symbol in V (or a
zero vector for the first time step)
-	At each time step t, extract logit probabilities for each element in V or the end-of-
sequence symbol EOS from the hidden state using a “Kernel expression extractor
(p)” (Linear(hidden_size, vocabulary_size + 1)). This defines the
conditional distribution pθ (zdt |zd1:t-1).
-	The full prior over kernel expression is an autoregressive distribution Qtpθ(zdt |zd1:t-1)
whose length is determined by EOS.
• Kernel expression LSTM embedder (p)
(same as kernel expression LSTM (p))
-	The module summarizes the kernel expression zd into an embedding vector e ∈
Rhidden_size
-	We re-use the kernel expression LSTM (p) above and use the last hidden LSTM state
to be the summary embedding vector.
•	Kernel parameters LSTM (p)
(input size = kernel_params_size + hidden_size, hidden size = hidden_size)
-	This module define a distribution over kernel parameters pθ (zc |zd).
-	At each timestep t, the input is a concatenation of the previous kernel parameters
zct-1 ∈ Rkernel_params_size (or zero vector for t = 1) and the embedding vector
e ∈ Rhidden_size .
-	At each timestep t, extract mean and standard deviations for each kernel param-
eter in K using a “Kernel parameters extractor (p)” Linear(hidden_size,
2 * kernel_params_size). This defines the conditional distribution
Pθ(ZtIz1"T, Zd).
Recognition model qφ(zd, zc|x)
• Signal LSTM embedder
(input size = 1, hidden size = obs_embedding_dim = hidden_size)
-	This module summarizes the signal x into an embedding vector ex	∈
Robs_embedding_dim.
-	The embedding vector is taken to be the last hidden state of an LSTM where x is fed as
the input sequence.
•	Kernel expression LSTM (q)
(input size = obs_embedding_dim + vocabulary_size, hidden size =
hidden_size)
-	This module defines a distribution over kernel expression qφ(zd∣χ).
-	At each time step t, the input is a concatentation of the one-hot encoding of the previous
symbol in V (or a zero vector for the first time step) and ex.
-	At each time step t, extract logit probabilities for each element in V or the end-of-
sequence symbol EOS from the hidden state using a “Kernel expression extractor
(q)” (Linear(hidden_size, vocabulary_size + 1)). This defines the
conditional distribution qφ(zdt |zd1:t-1, x).
-	The full distribution over kernel expression is an autoregressive distribution
Qt qφ(zdt |zd1:t-1, x) whose length is determined by EOS.
13
Published as a conference paper at ICLR 2022
• Kernel expression LSTM embedder (q)
(input size = vocabulary_size, hidden size = hidden_size)
-	This module summarizes the kernel expression Zd into an embedding vector 巳之己 ∈
Rhidden_size .
-	The embedding vector ezd is taken to be the last hidden state of an LSTM where zd is
fed as the input sequence.
•	Kernel parameters LSTM (q)
(input size = obs_embedding_dim + kernel_params_size + hidden_size,
hidden size = hidden_size)
-	This module defines a distribution over kernel parameters qφ(zc∖zd ,x).
-	At each timestep t, the input is a concatenation of the previous kernel parameters
zct-1 ∈ Rkernel_params_size (or zero vector for t = 1), the embedding vector ezd ∈
Rhidden_size and the signal embedding vector ex ∈ Robs_embedding_size.
-	At each timestep t, extract mean and standard deviations for each kernel param-
eter in K using a “Kernel parameters extractor (q)” Linear(hidden_size,
2 * kernel_params_size). This defines the conditional distribution
q。(Zc∖z1,τ,zd,X).
B.2 Compositional scene understanding
Fixed parameters
•	Number of allowed primitives to learn P = 5
•	Maximum number of blocks per cell Bmax = 3
•	Number of cells in the x-plane = number of cells in the Z-plane = N = 2
•	Image resolution I = [3, 128, 128]
•	Observation embedding size obs_embedding_size = 676
Recognition model qφ(Zd, Zc∖x). Components are detailed to match those shown in Fig. 7.
•	cnn-based embedding of the image
-	Conv2d(in_channels=4, out_channels=64, kernel_size=3,
padding=2)
-	ReLU
-	MaxPool(kernel_size=3, stride=2)
-	Conv2d(64, 128, 3, 1)
-	ReLU
-	MaxPool(3, 2)
-	Conv2d(128, 128, 3, 0)
-	ReLU-Conv2d(128, 4, 3, 0)
-	ReLU
-	MaxPool(2, 2)
-	Flatten
Note, the dimensionality of the output is obs_embedding_size (in this case, = 676)
•	Neural network for the distribution over blocks (NN1)
Linear(obs_embedding_size, N2 * (1 + Bmax ))
•	Neural network for the distribution over primitives (NN2)
Linear(obs_embedding_size, N2 * Bmax * P)
14
Published as a conference paper at ICLR 2022
•	Neural network that outputs the mean for the raw primitive locations (NN3)
Linear(obs_embedding_size, N2 * Bmax )
•	Neural network that outputs the standard deviation for the raw primitive locations (NN4)
Linear(obs_embedding_size, N2 * Bmax )
C Comparative Efficiency: Wall-Clock Time and GPU Memory
Consumption
We comprehensively investigate the comparative wall-clock time and GPU memory consumption
of hmws and rws by comparing the algorithms over a range of matched number of likelihood
evaluations. Here, we focus on the scene understanding domain, specifically the model wherein
scene parses are inferred with color. We find that hmws consistently results in more time- and
memory-efficient inference, as seen in Tables 1 and 2. The reason for this is that while we match
HMWS’s K(N + M) with RWS’s S likelihood evaluations,
•	The actual number of likelihood evaluations of HMWS is K ∙ L which is typically smaller
than K(N + M) because we only take L ≤ N + M unique memory elements (step 2 of
Algorithm 1).
•	The loss terms in lines 7-9 of Algorithm 1 all contain only K ∙ M < K ∙ L ≤ K(N + M)
IOgPrObabiIitytermS {logpθ(Zm,z；®,x)}M=K,k=ι, {logqφ(Zmk|zm,x)}M=K,k=ι and M
terms {log qφ(zdm ∣x)}M=ι which means differentiating these loss terms is proportionately
cheaper than for RWS which has K(N + M) log probability terms.
•	We draw only N samples from qφ(zd|x) and K ∙ L samples from qφ(zc∖zd, x) in HMWS in
comparison to K ∙ (N + M) in RWS.
SorK(M+N)	HMWS	RWS
8	67 min	82 min
18	136 min	196 min
32	186 min	327 min
50	390 min	582 min
72	499 min	N/A
Table 1: Wall-clock comparison (time to 5000 iterations). hmws is faster in terms of absolute time to reach a
fixed number of iterations regardless of the setting of the number of likelihood evaluations. Note that hmws is
even able to run with higher number of samples (e.g, K(M + N) = 72), unlike RWS - which fails to run due to
computationally prohibitive memory requirements.
SorK(M+N)	HMWS	RWS
8	2277 MB	3467 MB
18	4645 MB	7646 MB
32	8028 MB	13556 MB
50	12485 MB	21155 MB
72	17963 MB	N/A
Table 2: Comparison of GPU memory consumption. hmws is more memory-efficient than rws over a
range of hyperparameter settings. Note that hmws is even able to run with higher number of samples (e.g,
K * (M + N) = 72), unlike rws - which fails to run due to computationally prohibitive memory requirements.
15
Published as a conference paper at ICLR 2022
D Ablation experiments
D. 1 “Interpolating” between HMWS and RWS
We “interpolate” between rws and hmws to better understand which components of hmws are
responsible for the performance increase. In particular, we train two more variants of the algorithm,
•	“hmws-” which trains all the components the same way as hmws except the continuous
recognition model qφ(zc∣zd, x) which is trained using the RWS loss. This method has a
memory.
•	“rws+” which trains all components the same way as rws except the continuous recognition
model qφ(zc|zd, x) which is trained using the HMWS loss in (10).
The learning curves for the time series model in Figure 9 indicate that the memory is primarily
responsible for the performance increase since hmws- almost identically matches the performance of
hmws. Including the loss in (10) in rws training helps learning but alone doesn’t achieve hmws’s
performance.
Figure 9: Augmenting the learning curves of hmws, rws, vimco on the time series domain in Figure 4 (left)
by hmws- and rws+ highlights the importance of hmws’s memory.
D.2 Sensitivity to the difficulty of continuous inference
Since both the marginalization and inference of the continuous latent variable zc is approximated
using importance sampling, we expect the performance of hmws to suffer as the inference task gets
more difficult. We empirically confirm this and that rws and vimco suffer equally if not worse
(Fig. 10) by arbitrarily adjusting the difficulty of continuous inference by changing the range of the
period pPERi of periodic kernels (see App. B.1).
Figure 10: Increasing the difficulty of the continuous inference task results in worse performance in hmws.
Baseline algorithms rws and vimco suffer equally or even more.
D.3 Comparing HMWS and RWS for different compute budgets
We compare hmws and rws for a number of different compute budgets (see Fig 11). Here, we
keep K * (M + N) = S to match the number of likelihood evaluations of HMWS and RWS, and
16
Published as a conference paper at ICLR 2022
keep K = M = N for simplicity. We run a sweep for HMWS with K = M = N in {2, 3, 4, 5, 6}
and a corresponding sweep for RWS with S in {8, 18, 32, 50, 72}. We find that HMWS consistently
overperforms rws.
Figure 11: Comparing hmws and rws for other compute budgets. Increasing the overall compute by increasing
K and keeping K = M = N improves performance while HMWS consistently outperforms RWS with equivalent
compute. Note that RWS didn’t run for S = 72 due to memory constraints.
E Using VIMCO for hybrid generative models
We initially treated the continuous latent variables the same way as the discrete variables. In the
reported results, we used reparameterized sampling of the continuous latent variables for training
vimco which decreases the variance of the estimator of the continuous recognition model gradients.
However, this didn’t seem to improve the performance significantly.
Since applying vimco to hybrid discrete-continuous latent variable settings is not common in the
literature, we include a sketch of the derivation of the gradient estimator that we use for completeness.
First, let Qφ(z1κ) = QK=I qφ(zk) and Qφ(z1κ|z1:K) = Qk=I qφ(zk |zk) be the distributions
of all the discrete and continuous randomness. We also drop the dependence on data x to reduce
notational clutter. Thus, the gradient we’d like to estimate is
VφEQφ(zyK )Qφ(zcfzi:K )[f(θ,φ,z1κ,z1κ)],	(26)
where f is the term inside of the IWAE objective f (...) = log -1 PK=I ；：(Zk,Zk,X).
Next, we apply reparameterized sampling of the continuous variables z1κ. This involves choosing a
simple, parameterless random variable e 〜s(e) and a reparameterization function Zc = r(e, Zd, φ)
such that its output has exactly the same distribution sampling from the original recognition network
zc 〜qφ(zc∣zd). Hence, we can rewrite the gradient we,d like to estimate as
VφEQφ(zyK)s©：K) [f (θ, Φ,z1κ, {r(ek,zd, φ)}3j ,	(27)
where f takes in reparameterized samples of the continuous latent variables instead of z，K.
Now, we follow a similar chain of reasoning to the one for deriving a score-function gradient estimator
VφEQφ(zyK)s©：K)[f (θ, φ, z1κ, {r(ek, z[ ,φ)}k=1)]	(28)
=Z [Vφ(Qφ(z1κ)S(e1:K))] f (...) + Qφ(z1κ)S(e1:K)Vφf(…)dz』:K de1:K	(29)
=Z Qφ(z1κ)S(e1:K)(Vφ log Qφ(zdκ))f (...) + Qφ(zdκ)S(e1:K)Vφf (...)dz1κ de1:K (30)
=E [VφlogQφ(zd∙κ)f (...) + Vφf (...)] .	(31)
The VIMCO control variate is applied to the first term Vφ log Qφ(z^d1κ)f (...), Importantly, this term
does not contain the density of the continuous variables Qφ(z1κ∣z∕K). Hence, this term only
provides gradients for learning the discrete recognition network unlike if we treated the continuous
latent variables the same as the discrete latent variables (in which case a score-function gradient
would be used for learning the continuous recognition network as well). The gradient for learning the
continuous recognition network instead comes from the second term Vφf(...).
17