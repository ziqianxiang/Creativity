Published as a conference paper at ICLR 2022
CROP: Certifying Robust Policies for Re-
inforcement Learning through Functional
Smoothing
Fan Wu1 Linyi Li1 Zijian Huang1 Yevgeniy Vorobeychik2 Ding Zhao3 Bo Li1
1University of Illinois at Urbana-Champaign	2Washington University in St. Louis
3 Carnegie Mellon University
{fanw6,linyi2,zijianh4,lbo}@illinois.edu yvorobeychik@wustl.edu
dingzhao@andrew.cmu.edu
Ab stract
As reinforcement learning (RL) has achieved great success and been even adopted
in safety-critical domains such as autonomous vehicles, a range of empirical studies
have been conducted to improve its robustness against adversarial attacks. However,
how to certify its robustness with theoretical guarantees still remains challenging.
In this paper, We present the first unified framework CROP (Certifying Robust
Policies for RL) to provide robustness certification on both action and reward
levels. In particular, we propose two robustness certification criteria: robustness
of per-state actions and lower bound of cumulative rewards. We then develop a
local smoothing algorithm for policies derived from Q-functions to guarantee the
robustness of actions taken along the trajectory; we also develop a global smoothing
algorithm for certifying the lower bound of a finite-horizon cumulative reward, as
well as a novel local smoothing algorithm to perform adaptive search in order to
obtain tighter reward certification. Empirically, we apply CROP to evaluate several
existing empirically robust RL algorithms, including adversarial training and dif-
ferent robust regularization, in four environments (two representative Atari games,
Highway, and CartPole). Furthermore, by evaluating these algorithms against adver-
sarial attacks, we demonstrate that our certifications are often tight. All experiment
results are available at website https://crop-leaderboard.github.io.
1	Introduction
Reinforcement learning (RL) has been widely applied to different applications, such as robotics (Kober
et al., 2013; Deisenroth et al., 2013; Polydoros & Nalpantidis, 2017), autonomous driving vehi-
cles (Shalev-Shwartz et al., 2016; Sallab et al., 2017), and trading (Deng et al., 2016; Almahdi
& Yang, 2017; Ye et al., 2020). However, recent studies have shown that learning algorithms are
vulnerable to adversarial attacks (Goodfellow et al., 2014; Kurakin et al., 2016; Moosavi-Dezfooli
et al., 2016; Jia & Liang, 2017; Eykholt et al., 2018), and a range of attacks have also been proposed
against the input states and trained policies of RL (Huang et al., 2017; Kos & Song, 2017; Lin et al.,
2017; Behzadan & Munir, 2017a). As more and more safety-critical applications are being deployed
in real-world (Christiano et al., 2016; Fisac et al., 2018; Cheng et al., 2019; eop), how to test and
improve their robustness before massive production is of great importance.
To defend against adversarial attacks in RL, different empirical defenses have been proposed (Man-
dlekar et al., 2017; Behzadan & Munir, 2017b; Pattanaik et al., 2018; Fischer et al., 2019; Zhang et al.,
2020; Oikarinen et al., 2020; Donti et al., 2020; Shen et al., 2020; Eysenbach & Levine, 2021). In
particular, adversarial training (Kos & Song, 2017; Behzadan & Munir, 2017b; Pattanaik et al., 2018)
and regularized RL algorithms by enforcing the smoothness of the trained models (Shen et al., 2020;
Zhang et al., 2020) have been studied to improve the robustness of trained policies. However, several
strong adaptive attacks have been proposed against these empirical defenses (Gleave et al., 2019;
Hussenot et al., 2019; Russo & Proutiere, 2019) and it is important to provide robustness certification
for a given learning algorithm to end such repeated game between attackers and defenders.
1
Published as a conference paper at ICLR 2022
To provide robustness certification, several studies have been conducted on classification. For instance,
both deterministic (Ehlers, 2017; Katz et al., 2017; Cheng et al., 2017; Tjeng et al., 2017; Weng et al.,
2018; Zhang et al., 2018; Singh et al., 2019; Gehr et al., 2018; Wong & Kolter, 2018; Raghunathan
et al., 2018) and probabilistic approaches (Lecuyer et al., 2019; Cohen et al., 2019; Lee et al., 2019;
Salman et al., 2019; Carmon et al., 2019; Jeong & Shin, 2020) have been explored to provide a lower
bound of classification accuracy given bounded adversarial perturbation. Considering the sequential
decision making property of RL, which makes it more challenging to be directly certified compared
to classification, in this paper we ask: How to provide efficient and effective robustness certification
for RL algorithms? What criteria should be used to certify the robustness ofRL algorithms?
Different from classification which involves one-step prediction only, RL algorithms provide both
action prediction and reward feedback, making what to certify and how to certify robustness of RL
challenging. In this paper we focus on Q-learning and propose two certification criteria: per-state
action stability and lower bound of perturbed cumulative reward. In particular, to certify the per-state
action stability, we propose the local smoothing on each input state and therefore derive the certified
radius for perturbation at each state, within which the action prediction will not be altered. To certify
the lower bound of cumulative reward, we propose both global smoothing over the finite trajectory to
obtain the expectation or percentile bounds given trajectories smoothed with sampled noise sequences;
and local smoothing to calculate an absolute lower bound based on our adaptive search algorithm.
We leverage our framework to test nine empirically robust RL algorithms on multiple RL environ-
ments. We show that the certified robustness depends on both the algorithm and the environment
properties. For instance, RadialRL (Oikarinen et al., 2020) is the most certifiably robust method on
Freeway. In addition, based on the per-state certification, we observe that for some environments such
as Pong, some states are more certifiably robust and such pattern is periodic. Given the information
of which states are more vulnerable, it is possible to design robust algorithms to specifically focus on
these vulnerable states. Based on the lower bound of perturbed cumulative reward, we show that our
certification is tight by comparing our bounds with empirical results under adversarial attacks.
Technical Contributions. In this paper, We take an important step towards providing robustness
certification for Q-learning. We make contributions on both theoretical and empirical fronts.
•	We propose a framework for certifying the robustness of Q-learning algorithms, which is notably
the first that provides the robustness certification w.r.t. the cumulative reward.
•	We propose two robustness certification criteria for Q-learning algorithms, together with corre-
sponding certification algorithms based on global and local smoothing strategies.
•	We theoretically prove the certification radius for input state and lower bound of perturbed cumula-
tive reward under bounded adversarial state perturbations.
•	We conduct extensive experiments to provide certification for nine empirically robust RL
algorithms on multiple RL environments. We provide several interesting observations which would
further inspire the development of robust RL algorithms.
2	Preliminaries
Q-learning and Deep Q-Networks (DQNs). Markov decision processes (MDPs) are at the core of
RL. Our focus is on discounted discrete-time MDPs, which are defined by tuple (S, A, R, P, γ, d0),
where S is a set of states (each with dimensionality N), A represents a set of discrete actions,
R : S × A → R is the reward function, and P : S × A → P(S) is the transition function with
P(∙) defining the set of probability measures, Y ∈ [0,1] is the discount factor, and do ∈ P(S) is the
distribution over the initial state. At time step t, the agent is in the state st ∈ S. After choosing action
at ∈ A, the agent transitions to the next state st+ι 〜P (st, at) and receives reward R(st, at). The
goal is to learn a policy π : S → P(A) that maximizes the expected cumulative reward E[Pt γtrt].
Q-learning (Watkins & Dayan, 1992) learns an action-value function (Q-function), Q?(s, a),
which is the maximum expected cumulative reward the agent can achieve after taking ac-
tion a in state s: Q?(s, a) = R(s, a) + γ E	[maxa0 Q?(s0, a0)]. In deep Q-Networks
s0 〜P (s,a)
(DQNs) (Mnih et al., 2013), Q? is approximated using a neural network parametrized by
θ, i.e., Qπ (s, a; θ) ≈ Q?(s, a). Let ρ ∈ P(S × A) be the observed distribution defined
over states s and actions a, the network can be trained via minimizing loss function L(θ) =
E
(s,a)〜ρ,s0 〜P (s,a)
(R(s, a) + γ maxa0 Qπ (s0, a0; θ) - Qπ(s, a; θ))2
The greedy policy π is de-
fined as taking the action with highest Qπ value in each state s: π(s) = arg maxa∈A Qπ(s, a).
2
Published as a conference paper at ICLR 2022
Certified Robustness for Classifiers via Randomized Smoothing. Randomized smoothing (Co-
hen et al., 2019) has been proposed to provide probabilistic certified robustness for classification.
It achieves state-of-the-art certified robustness on large-scale datasets such as ImageNet under `2-
bounded constraints (Salman et al., 2019; Yang et al., 2020). In particular, given a base model and
a test instance, a smoothed model is constructed by outputting the most probable prediction over
different Gaussian perturbed inputs.
3	Robustness Certification in Q-learning
In this section, we first introduce the threat model, followed by two robustness certification criteria
for the Q-learning algorithm: per-state action and cumulative reward. We consider the standard
adversarial setting in Q-learning (Huang et al., 2017; Kos & Song, 2017; Zhang et al., 2020), where the
adversary can apply '2-bounded perturbation Bε = {δ ∈ Rn | k∣δ∣∣2 ≤ ε} to input state observations
of the agent during decision (test) time to cause the policy to select suboptimal actions. The agent
observes the perturbed state and takes action a0 = π(s + δ), following policy π. Following the
Kerckhoff’s principle (Shannon, 1949), we consider a worst-case adversary who applies adversarial
perturbations to every state at decision time. Our analysis and methods are generalizable to other `p
norms following (Yang et al., 2020; Lecuyer et al., 2019).
3.1	Robustness Certification for Q-learning with Different Criteria
To provide the robustness certification for Q-learning, we propose two certification criteria: per-state
action robustness and lower bound of the cumulative reward.
Robustness Certification for Per-State Action. We first aim to explore the robustness (stabil-
ity/consistency) of the per-state action given adversarially perturbed input states.
Definition 1 (Certification for per-state action). Given a trained network Qπ with policy π, we define
the robustness certification for per-state action as the maximum perturbation magnitude ε, such that
for any perturbation δ ∈ Bε, the predicted action under the perturbed state will be the same as the
action taken in the clean environment, i.e., ∏(s + δ) = ∏(s), ∀δ ∈ Bε.
Robustness Certification for Cumulative Reward. Given that the cumulative reward is important
for RL, here in addition to the per-state action, we also define the robustness certification regarding
the cumulative reward under input state perturbation.
Definition 2 (Cumulative reward). Let P : S × A → P(S) be the transition function of the
environment with P (∙) defining the set of probability measures. Let R, d0, γ, Qn ,∏ be the reward
function, initial state distribution, discount factor, a given trained Q-network, and the corresponding
greedy policy as introduced in Section 2. J(π) represents the cumulative reward and Jε(π) represents
the perturbed cumulative reward under perturbations δt ∈ Bε at each time step t:
∞∞
J(π) := X γtR(st, π(st)),	and	Jε(π) := X γtR(st, π(st +δt)),	(1)
where st+1 〜P (st,at),so 〜do,	where st+1 〜P (st, π(st + δt )),so 〜do.
The randomness of J(π) arises from the environment dynamics, while that of Jε(π) includes
additional randomness from the perturbations {δt}. We focus on a finite horizon H in this paper,
where a sufficiently large H can approximate J(π) and Jε(π) to arbitrary precision when γ < 1.
Definition 3 (Robustness certification for cumulative reward). The robustness certification for
cumulative reward is the lower bound of perturbed cumulative reward J such that J ≤ Jε(∏)
under perturbation in Bε = {δ ∈ Rn | kδk2 ≤ ε} applied to all time steps.
We will provide details on the certification of per-state action in Section 4 and the certification of
cumulative reward in Section 5 based on different smoothing strategies and certification methods.
4	Robustness Certification Strategies for Per-State Action
In this section, we discuss the robustness certification for per-state action, aiming to calculate a lower
bound of maximum perturbation magnitude ε in Definition 1.
4.1	Certification for Per- S tate Action via Action-Value Functional Smoothing
Let Qπ be the action-value function given by the trained network Q with policy π. We derive a
smoothed function Qπ through per-state local smoothing. Specifically, at each time step t, for each
action a ∈ A, we draw random noise from a Gaussian distribution N(0, σ2lN) to smooth Qn(∙, a).
3
Published as a conference paper at ICLR 2022
ππ	π
Q (st,a)	:= E Q	(St	+ ∆t,a)	∀st	∈S,a ∈ A, and	∏(st)	:= argmaxQ	(st,a)	∀st ∈ S.
△ t~N (0,σ21N )	a
(2)
Lemma 1 (Lipschitz continuity of the smoothed value function). Given the action-value function
Qπ : S × A → [Vmin , Vmax], the smoothed function Qπ with smoothing parameter σ is L-Lipschitz
continuous with L = Vmax-Vmin a∕2∕∏ w.r.t. the state input.
The proof is given in Appendix A.1. Leveraging the Lipschitz continuity in Lemma 1, we derive the
following theorem for certifying the robustness of per-state action.
Theorem 1. Let Qπ : S × A → [Vmin, Vmax] be a trained value network, Qπ be the smoothed func-
tion with (2). At time step t with state st, we can compute the lower bound rt of maximum perturbation
magnitude 片(St) (i.e., r ≤ 忑(St), ε defined in Definition 1)for locally smoothed policy π:
where Φ-1
rt
ʌ _ , .
Q (St, ai) — Vmin
Vmax - Vmin
Qπ(st,a2) - Vmin!!
Vmax - Vmin
(3)
is the inverse CDF function, a1 is the action with the highest Qπ value at state St, and
a2 is the runner-up action. We name the lower bound rt as certified radius for the state St.
The proof is omitted to Appendix A.2. The theorem provides a certified radius rt for per-state action
given smoothed policy: As long as the perturbation is bounded by rt, i.e., kδtk2 ≤ rt, the action does
not change: ∏(st + δt) = ∏(st). To achieve high certified robustness for Per-State action, Theorem 1
implies a tradeoff between value function smoothness and the margin between the values of top two
actions: If a larger smoothing Parameter σ is aPPlied, the action-value function would be smoother
and therefore more stable; however, it would shrink the margin between the toP two action values
leading to smaller certified radius. Thus, there exists a ProPer smoothing Parameter to balance the
tradeoff, which dePends on the actual environments and algorithms.
4.2	CROP-LoAct: Local Randomized Smoothing for Certifying Per- S tate Action
Next we introduce the algorithm to achieve the certification for Per-state action. Given a Qπ network,
We apply (2) to derive a smoothed network Qn. At each state st, We obtain the greedy action at w.r.t.
Qπ, and then compute the certified radius rt. We present the complete algorithm in Appendix B.1.
There are some additional challenges in smoothing the value function and computing the certified
radius in Q-learning compared with the standard classification task (Cohen et al., 2019). Challenge
1: In classification, the output range of the confidence [0, 1] is known a priori; however, in Q-learning,
for a given Qπ, its range [Vmin , Vmax] is unknown. Challenge 2: In the classification task, the lower
and upper bounds of the top two classes’ prediction probabilities can be directly computed via the
confidence interval base on multinomial proportions (Goodman, 1965). For Q-networks, the outputs
are not probabilities and calculating the multinomial proportions becomes challenging.
Pre-processing. To address Challenge 1, we estimate the output range [Vmin , Vmax] of a given
network Qπ based on a finite set of valid states Ssub ⊆ S. In particular, we craft a sufficiently large
set Ssub to estimate Vmin and Vmax for Qπ on S, which can be used later in per-state smoothing. The
details for estimating Vmin and Vmax are deferred to Appendix B.1.
Certification. To smooth a given state St, we use Monte Carlo sampling (Cohen et al., 2019) to
sample noise applied to St, and then estimate the corresponding smoothed value function Qπ at St
with (2). In particular, we sample m Gaussian noise ∆% 〜N(0, σ2Iw), clip the Q-network output to
ensure that it falls within the range [Vmin, Vmax], and then take the average of the output to obtain the
smoothed action prediction based on Qπ. We then employ Theorem 1 to compute the certified radius
rt. We omit the detailed inference procedure to Appendix B.1. To address Challenge 2, we leverage
Hoeffding’s inequality (Hoeffding, 1994) to compute a lower bound of Qπ(St, a1) and an upper
bound of Qπ(St, a2) with one-sided confidence level parameter α given the top two actions a1 and
a2. When the former is higher than the latter, we can certify a positive radius for the given state St.
5	Robustness Certification Strategies for the Cumulative Reward
In this section, we present robustness certification strategies for the cumulative reward. The goal is
to provide the lower bounds for the perturbed cumulative reward in Definition 2. In particular, we
propose both global smoothing and local smoothing strategies to certify the perturbed cumulative
reward. In the global smoothing, we view the whole state trajectory as a function to smooth, which
4
Published as a conference paper at ICLR 2022
would lead to relatively loose certification bound. We then propose the local smoothing by smoothing
each state individually to obtain the absolute lower bound.
5.1	Certification of Cumulative Reward based on Global Smoothing
In contrast to Section 4 where we perform per-state smoothing to achieve the certification for per-state
action, here, we aim to perform global smoothing on the state trajectory by viewing the entire
trajectory as a function. In particular, we first derive the expectation bound of the cumulative reward
based on global smoothing by estimating the Lipschitz constant for the cumulative reward w.r.t. the
trajectories. Since the Lipschitz estimation in the expectation bound is algorithm agnostic and could
lead to loose estimation bound, we subsequently propose a more practical and tighter percentile bound.
Definition 4 (σ-randomized trajectory and σ-randomized policy). Given a state trajectory
(so, si,..., sH-ι) of length H where st+ι 〜P (st, ∏(st)), so 〜do, with π the greedy policy of the
action-value function Qn, we derive a Q-randomized trajectory as (s0, si,..., SH -J, where s；+i 〜
P(st,∏(st+∆t)), ∆t 〜N(0, Q12In),and s0 = so 〜do. We correspondingly define a Q-randomized
policy π0 based on π in the following form: ∏0(st) := π(st + ∆t) where ∆t 〜N(0, σ2IN).
Let the operator ㊉ concatenates given input states or noise that are added to each state. The sampled
noise sequence is denoted by ∆ = ㊉H-I∆t, where ∆t 〜N(0, q2In).
Definition 5 (Perturbed return function). Let R, P, γ, do be the reward function, transition function,
discount factor, and initial state distribution in Definition 2. We define a bounded perturbed return
function Fπ : RH×N → [Jmin, Jmax] representing cumulative reward with potential perturbation δ:
H
Fn (㊉H-)1δt) := X YtR(St,π(st + δt)), where st+ι 〜P(st,π(st + δt)),s0 〜d0.	(4)
t=0
We can see when there is no perturbation (δt = 0), Fn (㊉H-10) = J(∏); when there are adversarial
perturbations δt ∈ Bε at each time step, Fn (㊉H-1δt) = Jε(∏), i.e., perturbed Cumulative reward.
Mean Smoothing: Expectation bound. Here we propose to sample noise sequences ∆ to perform
global smoothing for the entire state trajectory, and calculate the lower bound of the expected per-
turbed cumulative reward E∆ [Jε(π0)] under all possible `2 -bounded perturbations within magnitude ε.
The expectation is over the noise sequence ∆ involved in the Q-randomized policy π0 in Definition 4.
Lemma 2 (Lipschitz continuity of smoothed perturbed return function). Let F be the perturbed return
function function defined in (4), the smoothed perturbed return function Fn is (Jmax-Jmin) p∕∏ -
Lipschitz continuous, where Fn (㊉H-1δt) ：= E	Fn (㊉H-1(δt + ∆t)).
/ 〜△〜N(o,σ2iH×N)	t -o '
Theorem 2 (Expectation bound). Let JE = Fn (㊉ H-1 0)- Lε√H, where L = (Jmax-Jmin) p/.
Then Je ≤ E [Jε(∏0)].
Proof Sketch. We first derive the equality between expected perturbed cumulative reward E [Jε(π0)]
Hi
and the smoothed perturbed return function Fn(㊉H=01δt). Thus, to lower bound the former, it
suffices to lower bound the latter, which can be calculated leveraging the Lipschitz continuity of F
in Lemma 2 (proved in Appendix A.3), noticing that the distance between ㊉H-10 and the adversarial
perturbations ㊉ H-1δt is bounded by ε√H. The complete proof is omitted to Appendix A.4.
We obtain Jmin and Jmax in Lemma 2 from environment specifications which can be loose in practice.
Thus the Lipschitz constant L estimation is coarse and mean smoothing is usually loose. We next
present a method that circumvents estimating the Lipschitz constant and provides a tight percentile
bound.
Percentile Smoothing: Percentile bound. We now propose to apply percentile smoothing to
smooth the perturbed cumulative reward and obtain the lower bound of the p-th percentile of Jε (π0),
where π0 is a Q-randomized policy defined in Definition 4.
FnGHOLδ) = Supy {y ∈ R | PhFn (φH-)1(δt + ∆t)) ≤ y] ≤ p} .	(5)
Theorem 3 (Percentile bound). Let Jp = Fnz (㊉H-1 0), where p0 := Φ (Φ-1(p) - ε√H∕σ). Then
Jp ≤ theP-thpercentile of Jε(∏0).
The proof is provided in Appendix A.5 based on Chiang et al. (2020). There are several other
advantages of percentile smoothing over mean smoothing. First, the certification given by percentile
5
Published as a conference paper at ICLR 2022
smoothing is among the cumulative rewards of the sampled σ-randomized trajectories and is therefore
achievable by a real-world policy, while the expectation bound is less likely to be achieved in practice
given the loose LiPschitz bound. Second, for a discrete function such as perturbed return function, the
output of mean smoothing is continuous w.r.t. σ, while the results given by percentile smoothing re-
main discrete. Thus, the percentile smoothed function preserves properties of the base function before
smoothing, and shares similar interpretation, e.g., the number of rounds that the agent wins. Third,
taking p = 50% in percentile smoothing leads to the median smoothing which achieves additional
properties such as robustness to outliers (Manikandan, 2011). The detailed algorithm CROP-GRe, in-
cluding the inference procedure, the estimation of Fπ, the calculation of the empirical order statistics,
and the configuration of algorithm parameters Jmin , Jmax, are deferred to Appendix B.2.
5.2 Certification of Cumulative Reward based on Local Smoothing
Though global smoothing provides efficient and practical bounds for the perturbed cumulative reward,
such bounds are still loose as they involve smoothing the entire trajectory at once. In this section, we
aim to provide a tighter lower bound for Jε(∏) by performing local smoothing.
Given a trajectory of H time steps which is guided by the locally smoothed policy ∏, we can
compute the certified radius at each time step according to Theorem 1, which can be denoted as
r0, r1, . . . , rH-1. Recall that when the perturbation magnitude ε < rt, the optimal action at at time
step t will remain unchanged. This implies that when ε < mintH=-01 rt , none of the actions in the
entire trajectory will be changed, and therefore the lower bound of the cumulative reward when
ε < mintH=-01 rt is the return of the current trajectory in a deterministic environment. Increasing ε
has two effects. First, the total number of time steps where the action is susceptible to change will
increase; second, at each time step, the action can change from the best to the runner-up or the rest.
We next introduce an extension of certified radius rt to characterize the two effects.
Theorem 4. Let (rt1, . . . , rt|A|-1) be a sequence of certified radii for state st at time step t, where rtk
denotes the radius such that if ε < rtk, the possible action at time step t will belong to the actions
corresponding to top k action values of Q at state st. The definition of rt in Theorem 1 is equivalent
to rt1 here. The radii can be computed similarly as follows:
σ
2
Qπ (St,aI) - Vmin ʌ - φ-1 Q Q" (St ,ak +1) - Vmin
Vmax - Vmin
Vmax - Vmin
,	1 ≤ k < |A|,
where a1 is the action of the highest Q value at state st and ak+1 is the (k + 1)-th best action. We
additionally define rt0 (st) = 0, which is also compatible with the definition above.
We defer the proof in Appendix A.6. With Theorem 4, for any given ε, we can compute all possible
actions under perturbations in Bε . This allows an exhaustive search to traverse all trajectories
satisfying that all certified radii along the trajectory are smaller than ε. Then, we can conclude that
Jε(∏) is lower bounded by the minimum return over all these possible trajectories.
5.2.1 CROP-LoRe: Local smoothing for Certified Reward
Given a policy π in a deterministic environment, let the initial state be s0, we propose CROP-LORE
to certify the lower bound of Jε(∏). At a high-level, CROP-LORE exhaustively explores new
trajectories leveraging Theorem 4 with priority queue and effectively updates the lower bound of
cumulative reward J by expanding a trajectory tree dynamically. The algorithm returns a collection
of pairs {(εi, Je .)}i=1 sorted in ascending order of εi, where |C | is the length of the collection. For
all ε0, let i be the largest integer such that εi ≤ ε0 < εi+1, then as long as the perturbation magnitude
ε ≤ ε0, the cumulative reward Jε(∏) ≥ J名.The algorithm is shown in Algorithm 3 in Appendix B.3.
Algorithm Description. The method starts from the base case: when perturbation magnitude ε = 0,
the lower bound of cumulative reward J is exactly the benign reward. The method then gradually
increases the perturbation magnitude ε (later we will explain how a new ε is determined). Along the
increase of ε, the perturbation may cause the policy π to take different actions at some time steps,
thus resulting in new trajectories. Thanks to the local smoothing, the method leverages Theorem 4
to figure out the exhaustive list of possible actions under current perturbation magnitude ε, and
effectively explore these new trajectories by formulating them as expanded branches of a trajectory
tree. Once all new trajectories are explored, the method examines all leaf nodes of the tree and figures
out the minimum reward among them, which is the new lower bound of cumulative reward J under
this new ε. To mitigate the explosion of branches, CROP-LORE proposes several optimization tricks.
6
Published as a conference paper at ICLR 2022
GaUSSAUg ——AdvT⅛in ——SA-MDP (PGD) ——SA-MDP (CVX) ——RadiaIRL ——NoisyNet ——GradDQN
-----Std"∏∙ain
d
0 100 200 300 400 500
time step t
0 100 200 300 400 500	0 100 200 300 400 500
time step t	time step t
σ=0.5
σ=0.75
<7=1.0
1.00
1.50
2.00
time step t	time step t	time step t
Figure 1: Robustness certification for per-state action in terms of certified radius r at all time steps. Each
column corresponds to a smoothing variance σ. The shaded area represents the standard deviation which is small.
RadialRL is the most certifiably robust method on Freeway, while SA-MDP (CVX) is the most robust on Pong.
In the following, we will briefly introduce the trajectory exploration and expansion and growth of
perturbation magnitude steps, as well as the optimization tricks. More algorithm details are deferred
to Appendix B.3, where we also provide an analysis on the time complexity of the algorithm.
In trajectory exploration and expansion, CROP-LORE organizes all possible trajectories in the
form of search tree and progressively grows it. For each node (representing a state), leveraging The-
orem 4, we compute a non-decreasing sequence {rk(s)}|kA=|0-1 representing the required perturbation
radii for π to choose each alternative action. Suppose the current ε satisfies ri(s) ≤ ε < ri+1(s), we
can grow (i + 1) branches from current state s corresponding to the original action and i alternative
actions since ε ≥ rj(s) for 1 ≤ j ≤ i. We expand the tree branches using depth-first search (Tarjan,
1972). In perturbation magnitude growth, when all trajectories for perturbation magnitude ε are
explored, we increase ε to seek certification under larger perturbations. This is achieved by preserving
a priority queue (Van Emde Boas, 1977) of the critical ε's that We will expand on. Concretely, along
the trajectory, at each tree node, we search for the possible actions and store actions corresponding to
{rk (s)}|kA=|i-+11 into the priority queue, since these actions are exactly those need to be explored when ε
grows. We repeat the procedure of perturbation magnitude growth (i.e., popping out the head element
from the queue) and trajectory exploration and expansion (i.e., exhaustively expand all trajectories
given the perturbation magnitude) until the priority queue becomes empty or the perturbation magni-
tude ε reaches the predefined threshold. Additionally, we adopt a few optimization tricks commonly
used in search algorithms to reduce the complexity of the algorithm, such as pruning and the mem-
orization technique (Michie, 1968). More potential improvements are discussed in Appendix B.3.
So far, we have presented all our certification methods. In Appendix C, we further discuss the
advantages, limitations, and extensions of these methods, and provide more detailed analysis to help
with understanding.
6	Experiments
In this section, we present evaluation for the proposed robustness certification framework CROP. Con-
cretely, we apply our three certification algorithms (CROP-LoAct, CROP-GRe, and CROP-LoRe)
to certify nine RL methods (StdTrain (Mnih et al., 2013), GaussAug (Kos & Song, 2017), Adv-
Train (Behzadan & Munir, 2017b), SA-MDP (PGD,CVX) (Zhang et al., 2020), RadialRL (Oikari-
nen et al., 2020), CARRL (Everett et al., 2021), NoisyNet (Fortunato et al., 2017), and Grad-
DQN (Pattanaik et al., 2018)) on two high-dimensional Atari games (Pong and Freeway), one low
dimensional control environment (CartPole), and an autonomous driving environment (Highway). As
a summary, we find that (1) SA-MDP (CVX), SA-MDP (PGD), and RadialRL achieve high certified ro-
bustness in different environments; (2) Large smoothing variance can help to improve certified robust-
ness significantly on Freeway, while a more careful selection of the smoothing parameter is needed in
Pong; (3) For methods that demonstrate high certified robustness, our certification of the cumulative re-
ward is tight. We defer the detailed descriptions of the environments to Appendix D.1 and the introduc-
tion to the RL methods and implementation details to Appendix D.2. More interesting results and dis-
cussions are omitted to Appendix E and our leaderboard, including results on CartPole (Appendix E.6)
and Highway (Appendix E.7). As a foundational work providing robustness certification for RL, we
expect more RL algorithms and RL environments will be certified under our framework in future work.
6.1	Evaluation of Robustness Certification for Per-state Action
7
Published as a conference paper at ICLR 2022
In this subsection, we provide the
robustness certification evaluation
for per-state action.
Experimental Setup and Met-
rics. We evaluate the locally
T- StdlFain
-∙ GaussAug
—AdvTfain
-*- SA-MDP (PGD)
—SA-MDP (CVX)
T- RadialRL
—⅜- NoisyNet
-∙- GradDQN
smoothed policy π derived in Q). Figure 2： Benign performance of locally smoothed policy ∏ under
We follow Theorem 1 and report different smoothing variance σ with clean state observations.
the certified radius rt at each step t.
We additionally compute certified ratio η at radius r, representing the percentage of time steps that can
be certified, formally denoted as ηr = 1/H PtH=1 1{rt≥r}. More details are omitted to Appendix D.4.
Evaluation Results of CROP-LoAct. We present the certification comparison in Figure 1 and
the benign performance under different smoothing variances in Figure 2; we omit details of certified
ratio to Appendix E.1.
On Freeway, We evaluate with a large range of smoothing parameter σ up to 1.0, since Freeway can
tolerate large noise as shown in Figure 2. Results under even larger σ are deferred to Appendix E.2,
showing that the certified robustness can be further improved for the empirically robust methods.
From Figure 1, we see that RadialRL consistently achieves the highest certified radius across all σ ’s.
This is because RadialRL explicitly optimizes over the worst-case perturbations. StdTrain, GaussAug,
and AdvTrain are not as robust, and increasing σ will not make a difference. In Pong, SA-MDP (CVX)
is the most certifiably robust, which may be due to the hardness of optimizing the worst-case pertur-
bation in Pong. More interesting, all methods present similar periodic patterns for the certified radius
on different states, which would inspire further robust training methods to take the “confident state”
(e.g., when the ball is flying towards the paddle) into account. We illustrate the frames with varying
certified radius in Appendix E.3. Overall, the certified robustness of these methods largely matches
empirical observations (Behzadan & Munir, 2017b; Oikarinen et al., 2020; Zhang et al., 2020).
6.2	Evaluation of Robustness Certification for Cumulative Reward
Here we will discuss the evaluation for the robustness certification regarding cumulative reward
in Section 5. We show the evaluation results for both CROP-GRe and CROP-LoRe.
Evaluation Setup and Metrics. We evaluate the σ-randomized policy π0 derived in Definition 4
for CROP-GRe and the locally smoothed policy π derived in ⑵ for CROP-LoRe. We compute the
expectation bound JE, percentile bound Jp, and absolute lower bound J following Theorem 2, The-
orem 3, and Section 5.2.1. To validate the tightness of the bounds, we additionally perform empirical
attacks. More rationales see Appendix D.3. The detailed parameters are omitted to Appendix D.4.
Evaluation of CROP-GRe. We present the certification of reward in Figure 3 and mainly focus
on analyzing percentile bound. We present the bound w.r.t. different attack magnitude ε under
different smoothing parameter σ. For each σ, there exists an upper bound of ε that can be certified,
which is positively correlated with σ. Details see Appendix B.2. We observe similar conclusion as in
the per-state action certification that RadialRL is most certifiably robust for Freeway, while SA-MDP
(CVX,PGD) are most robust for Pong although the highest robustness is achieved at different σ .
Evaluation of CROP-LoRe. In Figure 3, we note that in Freeway, RadialRL is the most robust,
followed by SA-MDP (CVX), then SA-MDP (PGD), AdVTrain, and GaussAug. As for Pong, SA-
MDP (CVX) outperforms RadialRL and SA-MDP (PGD). Remaining methods are not clearly ranked.
6.3	Discussion on Evaluation Results
Impact of Smoothing Parameter σ . We draw similar conclusions regarding the impact of smooth-
ing variance from the results given by CROP-LoAct and CROP-GRe. In Freeway, as σ increases,
the robustness of StdTrain, GaussAug, and AdvTrain barely increases, while that of SA-MDP (PGD),
SA-MDP (CVX), and RadialRL steadily increases. In Pong, a σ in the range 0.01-0.03 shall be
suitable for almost all methods. More explanations from the perspective of benign performance and
certified results will be provided in Appendix E.4. For CROP-LoRe, there are a few different con-
clusions. On Freeway, it is clear that under larger attack magnitude ε, larger smoothing parameter σ
always secures higher lower bound J. In Pong, CROP-LoRe can only achieve non-zero certification
with small σ under a small range of attack magnitude, implying the difficulty of establishing non-
trivial certification for these methods on Pong. We defer details on the selection ofσ to Appendix E.4.
8
Published as a conference paper at ICLR 2022
σ = 0.005
σ=
0.05
pJ)labolG( J)lacoL(
0.000	0.005	0.010
attack ε
ie-4
一
4
3
2
le-2
σ = 0.1
2
0.00 0.25 0.50 0.75 1.00
σ = 0.75
fcz=t==l==4t==t
0-5	1-0	1-5
ie-2
stdπaln (empirical)	GaussAuggmpmcel)	-∙- Admin (empirical)	-A- SA-MDP (PGD) (emρlrtcβl)	-*- SA-MDP (CVX) Iemplrtcal)	-T- RedlalRL (emρlrtcβl)	-φ- NolsyNet (empirical)	-∙- Grad tX)N (empirical)
----Stdπain (certified)	GaUSSAUgteertmed)	----Mmin (certified)	----sa-mdp(pgo)(certified)	----sa-mdp (ano (certι∏ed)	----RediaiRL(Certined)	----NolSyNetteertmed)	---GradtX)N (certified)
a
O
le-3
4
2
0.0 0.2 0Λ 0.6 0.8 1.0 1-2
ie-ι
∣-
0-0	0-5	1-0	1-5
le-2
0.0	0.5	1.0	1.5
attack ε
1-5
Le-3
1.5
le-3
0.0	0.5	1.0	1.5
attack ε
0.00	0.05	0.10
attack ε
-β
一一
0.00 0.05 0.10 0.15 0.20	0.0 0.2 Q.4 Q.6 0.8 10
attack ε	attack ε
(a) Freeway
σ = 0.01
J)labolG( pJ)labolG( J)lacoL
ie-4
attack ε
attack ε
1.5
le-4
C
0.000	0.001
attack ε
O
0.000	0.005
attack ε
3
0.000	0.005
attack ε
∖ C
—⅛-	O 1——.
0.010	0.00
attack ε
(b) Pong
Figure 3: Robustness certification for cumulative reward, including expectation bound JE, percentile bound
Jp (p = 50%), and absolute lower bound J. Each column corresponds to one smoothing variance. Solid lines
represent the certified reward bounds, and dashed lines show the empirical performance under PGD.

Tightness of the certification JE, Jp, and J. We compare the empirical cumulative rewards
achieved under PGD attacks with our certified lower bounds. First, the empirical results are consis-
tently lower bounded by our certifications, validating the correctness of our bounds. Regarding the
tightness, the improved Jp is much tighter than the loose JE, supported by discussions in Section 5.1.
The tightness of J can be reflected by the zero gap between the certification and the empirical result
under a wide range of attack magnitude ε. Furthermore, the certification for SA-MDP (CVX,PGD)
are quite tight for Freeway under large attack magnitudes, and RadialRL demonstrates its superiority
on Freeway. Additionally, different methods may achieve the same empirical results under attack, yet
their certifications differ tremendously, indicating the importance of the robustness certification.
Game Properties. The certified robustness of any method on Freeway is much higher than that on
Pong, indicating that Freeway is a more stable game than Pong, also shown in Mnih et al. (2015).
7	Related Work
We briefly review several RL methods that demonstrate empirical robustness. Kos & Song (2017) and
Behzadan & Munir (2017b) show that adversarial training can increase the agent’s resilience. SA-
DQN (Zhang et al., 2020) leverages regularization to encourage the top-1 action to stay unchanged
under perturbation. Radial-RL (Oikarinen et al., 2020) minimizes an adversarial loss function that
incorporates the upper bound of the perturbed loss. CARRL (Everett et al., 2021) computes the lower
bounds of Q values under perturbation for action selection, but it is only suitable for low-dimensional
environments. Most of these works only provide empirical robustness, with Zhang et al. (2020)
and Fischer et al. (2019) additionally provide robustness certificates at state level. To the best of our
knowledge, this is the first work providing robustness certification for the cumulative reward of RL
methods. Broader discussions and comparisons of related work are in Appendix F.
Conclusions. To provide the robustness certification for RL methods, we propose a general frame-
work CROP, aiming to provide certification based on two criteria. Our evaluations show that certain
empirically robust RL methods are certifiably robust for specific RL environments.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, Alfred P.
Sloan Fellowship, and Amazon Research Award.
Ethics Statement. In this paper, we prove the first framework for certifying the robustness of
RL algorithms against evasion attacks. We aim to certify and therefore potentially improve the
trustworthiness of machine learning models, and we do not expect any ethics issues raised by our
work.
Reproducibility Statement. All theorem statements are substantiated with rigorous proofs in our
Appendix. We have upload the source code as the supplementary material for reproducibility purpose.
References
Promoting the use of trustworthy artificial intelligence in the federal government.
https://www.federalregister.gov/documents/2020/12/08/2020-27065/
promoting- the- use- of- trustworthy- artificial- intelligence- in- the-
federal-government. Accessed: 2021-05-20.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
Saud Almahdi and Steve Y Yang. An adaptive portfolio trading system: A risk-return portfolio
optimization using recurrent reinforcement learning with expected maximum drawdown. Expert
Systems with Applications, 87:267-279, 2017.
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition,
pp. 262-275. Springer, 2017a.
Vahid Behzadan and Arslan Munir. Whatever does not kill deep reinforcement learning, makes it
stronger. arXiv preprint arXiv:1712.09344, 2017b.
Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation attacks on deep q-networks
with parameter-space noise. In International Conference on Computer Safety, Reliability, and
Security, pp. 406-417. Springer, 2018.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
jun 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019.
Chih-Hong Cheng, Georg NUhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In International Symposium on Automated Technology for Verification and Analysis, pp.
251-268. Springer, 2017.
Richard Cheng, Gabor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement
learning through barrier functions for safety-critical continuous control tasks. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 3387-3395, 2019.
10
Published as a conference paper at ICLR 2022
Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John Dickerson, and
Tom Goldstein. Detection as regression: Certified object detection with median smooth-
ing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
Vances in Neural Information Processing Systems, volume 33, pp. 1275-1286. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0dd1bc593a91620daecf7723d2235624- Paper.pdf.
Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and trends in Robotics, 2(1-2):388-403, 2013.
Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, and Qionghai Dai. Deep direct reinforcement
learning for financial signal representation and trading. IEEE transactions on neural networks and
learning systems, 28(3):653-664, 2016.
Priya L Donti, Melrose Roderick, Mahyar Fazlyab, and J Zico Kolter. Enforcing robust control
guarantees within neural network policies. arXiv preprint arXiv:2011.08105, 2020.
Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer,
2017.
Michael Everett, Bjorn Lutjens, and Jonathan P How. Certifiable robustness to adversarial state
uncertainty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning
Systems, 2021.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1625-1634, 2018.
Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl
problems. arXiv preprint arXiv:2103.06257, 2021.
Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula, and
Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control, 64(7):2737-2752, 2018.
Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial
policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Leo A Goodman. On simultaneous confidence intervals for multinomial proportions. Technometrics,
7(2):247-254, 1965.
11
Published as a conference paper at ICLR 2022
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works OfWassily Hoefding, pp. 409-426. SPringer,1994.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network Policies. arXiv preprint arXiv:1702.02284, 2017.
Leonard Hussenot, MatthieU Geist, and Olivier PietqUin. Copycat: Taking control of neural policies
with constant attacks. arXiv preprint arXiv:1905.12282, 2019.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257-280, 2005.
Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed
classifiers. arXiv preprint arXiv:2006.04062, 2020.
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328, 2017.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient
smt solver for verifying deep neural networks. In International Conference on Computer Aided
Verification, pp. 97-117. Springer, 2017.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint
arXiv:1705.06452, 2017.
Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via
randomized smoothing. 2020.
Aounon Kumar, Alexander Levine, and Soheil Feizi. Policy smoothing for provably robust reinforce-
ment learning. arXiv preprint arXiv:2106.11420, 2021.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. arXiv preprint arXiv:1906.04948, 2019.
Edouard Leurent. An environment for autonomous driving decision-making. https://
github.com/eleurent/highway-env, 2018.
Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks. arXiv
preprint arXiv:2009.04131, 2020.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics
of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust
policy learning: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 3932-3939. IEEE, 2017.
12
Published as a conference paper at ICLR 2022
S Manikandan. Measures of central tendency: Median and mode. Journal of pharmacology and
pharmacotherapeutics, 2(3):214, 2011.
Donald Michie. “memo” functions and machine learning. Nature, 218(5136):19-22, 1968.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably
robust neural networks. In International Conference on Machine Learning, pp. 3578-3586. PMLR,
2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574-2582, 2016.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780-798, 2005.
Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In 17th International Conference on Au-
tonomous Agents and Multiagent Systems, AAMAS 2018, pp. 2040-2042. International Foundation
for Autonomous Agents and Multiagent Systems (IFAAMAS), 2018.
Athanasios S Polydoros and Lazaros Nalpantidis. Survey of model-based reinforcement learning:
Applications on robotics. Journal of Intelligent & Robotic Systems, 86(2):153-173, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies. arXiv
preprint arXiv:1907.13548, 2019.
Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70-76, 2017.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classi-
fiers. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
3a24b25a7b092a252166a1641ae953e7- Paper.pdf.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Claude E Shannon. Communication theory of secrecy systems. The Bell system technical journal, 28
(4):656-715, 1949.
Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao. Deep reinforcement learning
with robust and smooth policy. In International Conference on Machine Learning, pp. 8707-8718.
PMLR, 2020.
13
Published as a conference paper at ICLR 2022
GagandeeP Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for
certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1-30,
2019.
Charles Stein et al. A bound for the error in the normal aPProximation to the distribution of
a sum of dePendent random variables. In Proceedings of the Sixth Berkeley Symposium on
Mathematical Statistics and Probability, Volume 2: Probability Theory. The Regents of the
University of California, 1972.
Yanchao Sun, Ruijie Zheng, Yongyuan Liang, and Furong Huang. Who is the strongest enemy?
towards oPtimal and efficient evasion attacks in deeP rl. arXiv preprint arXiv:2106.05087, 2021.
Robert Tarjan. DePth-first search and linear graPh algorithms. SIAM journal on computing, 1(2):
146-160, 1972.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer Programming. arXiv preprint arXiv:1711.07356, 2017.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deeP neural networks from simulation to the real world. In 2017
IEEE/RSJ international conference on intelligent robots and systems (IROS), PP. 23-30. IEEE,
2017.
Peter van Emde Boas. Preserving order in a forest in less than logarithmic time and linear sPace.
Information processing letters, 6(3):80-82, 1977.
Hado Van Hasselt, Arthur Guez, and David Silver. DeeP reinforcement learning with double q-
learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.
ChristoPher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast comPutation of certified robustness for relu networks. In
International Conference on Machine Learning, PP. 5276-5285. PMLR, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examPles via the convex outer
adversarial PolytoPe. In International Conference on Machine Learning, PP. 5286-5295. PMLR,
2018.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Minlie Huang, K Chang, Bhavya Kailkhura, Xue Lin, and
C Hsieh. Automatic Perturbation analysis on general comPutational graPhs. Technical rePort,
Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States), 2020.
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shaPes and sizes. In International Conference on Machine Learning, PP. 10693-
10705. PMLR, 2020.
Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Ju Xiao, and Bo Li. Reinforcement-
learning based Portfolio management with augmented asset movement Prediction states. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, PP. 1112-1119, 2020.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural
network robustness certification with general activation functions. Advances in Neural Information
Processing Systems, 31:4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh. Robust deeP reinforcement learning against adversarial Perturbations on state obser-
vations. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, PP. 21024-21037. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f0eb6568ea114ba6e293f903c34d7488- Paper.pdf.
Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned oPtimal adversary. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=sCZbhBvqQaU.
14
Published as a conference paper at ICLR 2022
The appendices are organized as follows:
•	In Appendix A, we present the detailed proofs for lemmas and theorems in Section 4
and Section 5, laying down the basis for the design of our certification strategies.
•	In Appendix B, we present some additional details of our three certification strategies:
CROP-LoAct to certify the per-state action, and CROP-GRe and CROP-LoRe to
certify the cumulative reward. We include the detailed algorithm description and the
complete pseudocode for each algorithm. Our implementation is publicly available at
https://github.com/AI-secure/CROP.
•	In Appendix C, we provide a discussion on the advantages and limitations of different
certification methods, and also present several promising direct extensions for future work.
We also provide more detailed analysis that help with the understanding of our algorithms.
•	In Appendix D, we show the experimental details, including the game environment, details
descriptions of the RL methods we evaluate, rationales for our evaluation settings and
experimental designs, detailed evaluation setup for each of our certification algorithms in
the main paper, as well as the setup for a new game CartPole.
•	In Appendix E, we present additional evaluation results and discussions for the environments
and algorithms evaluated in the main paper, from the perspectives of different metrics and
different ranges of parameters. We also provide the running time statistics and specifically
illustrate the periodic patterns in the Pong game. In addition, we present the evaluation
results on a standard control environment CartPole and another autonomous driving
environment Highway.
•	In Appendix F, we provide a broader discussion of the related works, spanning the evasion
attacks in RL, Robust RL, as well as robustness certification for RL.
A Proofs
A.1 Proof of Lemma 1
We recall Lemma 1:
Lemma 1 (Lipschitz continuity of the smoothed value function). Given the action-value function
Qπ : S × A → [Vmin , Vmax], the smoothed function Qπ with smoothing parameter σ is L-Lipschitz
continuous with L = Vmax-Vmin，2/n w.r.t. the state input.
Proof. To prove Lemma 1, we leverage the technique in the proof for Lemma 1 of Salman et al.
(2019) in their Appendix A.
For each action a ∈ A, our smoothed value function is
Q(s,a) := ∆〜N(E>IN)Qn(S + △⑷=(2π)N∕2σN Ln Q(t,a)exp (- 2σ2 ks - tk2) dt.
Taking the gradient w.r.t. s, we obtain
▽sQn(s,a) = (2∏)N∕2σN ZRn Qn(t,a)σ12 (S- t) exp (- 2⅛2 ks- tk2) dt.
For any unit direction u, we have
U ∙vsQπ(S，a) ≤ (2∏)N∕2σN ZRn VmaxC- Vmin Iu(S - t)| exp (- 2σ2 ks — tk2) dt
=Vmaxσ2 Vmm . ZRn (2Π⅛|si - t| exp (— 2⅛2|si - t|2) dt
• γ ZRn 画1匹 exp(—2σ2 |sj - t|2)dt
σ
Vmax - Vmin
Thus, Qn is L-LiPschitz continuous with L = Vmax-Vmin yf2∕∏ w.r.t. the state input.
□
15
Published as a conference paper at ICLR 2022
A.2 Proof of Theorem 1
We recall Theorem 1:
■~■
Theorem 1. Let Qπ : S × A → [Vmin, Vmax] be a trained value network, Qπ be the smoothed func-
tion with (2). At time step t with state st, we can compute the lower bound rt of maximum perturbation
magnitude 片(St) (i.e., r ≤ 忑(St), ε defined in Definition 1)for locally smoothed policy π:
≈-, .
Qe (st,a2) — Vmin
Vmax - Vmin
σ
2
≈-, .
Qe (St, ai) — Vmin
Vmax - Vmin
rt
))
(6)
where Φ-1 is the inverse CDF function, a1 is the action with the highest Qeπ value at state St, and
a2 is the runner-up action. We name the lower bound rt as certified radius for the state St.
We first present a lemma that can help in the proof of Theorem 1.
Lemma 3. Let Φ be the CDF of a standard normal distribution, the mapping ηa(s) := Q ∙
φ-1 (QV(SaI-Vmn) is 1 -Lipschitz continuous.
The lemma can be proved following the same technique as the proof for Lemma 1 in Appendix A.1.
The detailed proof can be referred to in the proof for Lemma 2 of Salman et al. (2019) in their
Appendix A. We next show how to leverage Lemma 3 to prove Theorem 1.
Proof for Theorem 1. Let the perturbation be δt, based on the Lipschitz continuity of the mapping η,
we have
ηa1 (St) - ηa1 (St +δt) ≤ kδtk2,	(7)
ηa2 (St +δt) - ηa2 (St) ≤ kδtk2.	(8)
Suppose that under perturbation δt, the action selection would be misled in the sense that the smoothed
value for the original action a1 is lower than that of another action a2, i.e., Qπ(St + δt, a1) ≤
Qπ(St + δt, a2). Then, based on the monotonicity of η, we have
ηa1 (St + δ) ≤ ηa2 (St + δ).	(9)
Summing up (7), (8), and (9), we obtain
kδtk2 ≥ 2(ηaι (St) — ηa2 (St))
σ
2
≈-, 、一
Qe (St, a1) - Vmin
Vmax
- Vmin
, 、一
Qe (St , a2 ) - Vmin
Vmax
- Vmin
))
which is a lower bound of the maximum perturbation magnitude ε(st) that can be tolerated at state
St . Hence, when rt takes the value of the computed lower bound, it satisfies the condition that
Irt ≤ ε(st).	口
A.3 Proof of Lemma 2
We recall Lemma 2:
Lemma 2 (Lipschitz continuity of smoothed perturbed return function). LetF be the perturbed return
function function defined in (4), the smoothed perturbed return function F∏ is Jmax-Jmin) p∕2∕∏ -
Lipschitz continuous, where F∏ (㊉H-1δj := E	F∏ (㊉H-1(δt + ∆t)).
、	/	△〜N(0q2Ih×n )	、-0	j
Proof. The proof can be done in a similar fashion as the proof for Lemma 1 in Appendix A.1.
Compared with the smoothed value network where the expectation is taken over the sampled states,
here, similarly, the smoothed perturbed return function is derived by taking the expectation over
sampled σ-randomized trajectories. The difference is that the output range of the Q-network is
[Vmin, Vmax], while the output range of the perturbed return function is [Jmin, Jmax]. Thus, the
smoothed perturbed return function F is Jmax-Jmin) ∕2∕∏-Lipschitz continuous.	口
16
Published as a conference paper at ICLR 2022
A.4 Proof of Theorem 2
We recall the definition of Fπ as well as Theorem 2:
Fe∏ (㊉H-1δt) :=	E	F∏ (㊉H-1(δt + Zt)).
'	Z	ζ~N (0,σ2IH×N)	'	)
Theorem 2 (Expectation bound). Let JE = Fn (㊉ H-10)- Lε√H,, where L = Jmax-Jmin) p∕2∕∏.
Then JE ≤ E [Jε(∏0)].
Proof. We note the following equality
F∏(㊉H-1δt) =) E [f∏ (㊉M1(δt + Zt))i =) E [f∏, (φH√δt)i =) E [JH(π0)i ,	(10)
where (a) comes from the definition of the smoothed perturbed return function Feπ , (b) is due to
the definition of the σ-randomized policy π0, and (c) arises from the definition of the perturbed
cumulative reward JεH. Thus, the expected perturbed cumulative reward E JεH(π0) is equivalent
to the smoothed perturbed return function Fn (㊉HO1δt). Furthermore, since the distance between
the all-zero ㊉ H-10 and the adversarial perturbations φH-1δt is bounded by ε√H, leveraging the
Lipschitz smoothness of F in Lemma 2, we obtain the lower bound of the expected perturbed
cumulative reward E [JH(∏0)] as Fn (㊉H-10) - Lε√H.	口
A.5 Proof of Theorem 3
We recall the definition of Fnp as well as Theorem 3:
Fn (㊉H-1δt) := Supy {y ∈ R | PhFn (㊉M1(δt + Zt)) ≤ y] ≤ p} .	(11)
Theorem 3 (Percentile bound). Let Jp = F∏ (㊉H-10), where p0 := Φ (Φ-1(p) - ε√H∕σ). Then
Jp ≤ theP-thpercentile of Jε(∏0).
Proof. To prove Theorem 3, we leverage the technique in the proof for Lemma 2 of Chiang et al.
(2020) in their Appendix B.
For brevity, we abbreviate δ := φH-1δt and redefine the plus operator such that δ + Z := ΦH-1(δt +
ζt). Then, similar to Lemma 3, we have the conclusion that
δ→ σ ∙ Φ-1(P [F∏ (δ + Z) ≤ F? (0)])
is 1-Lipschitz continuous, where Z 〜N(0,σ2IH×N).
Thus, under the perturbations δt ∈ Bε for t = 0 . . . H - 1, we have
Φ-1(P [Fn(δ + Z) ≤ Fp0(O)]) ≤ Φ-1 (PhFn (Z) ≤ Fp0(O)]) + kδk2
≤ Φ-1 (PhFn(Z) ≤ Fp0(O)]) + 邛	(SinCe ∣∣δ∣∣2 ≤ ε√H)
=Φ-1(p0) + ε^^^	(By definition of Fn)
= Φ-1 (p)	(By definition of p0).
Since Φ-1 monotonically increase, this implies that P Fn(δ + Z) ≤ Fenp0 (0) ≤ p. According to the
0
definition of Fn In (11), we see that Fn (0) ≤ Fn (δ), ι.e., Jp ≤ thep-th percentile of Jε(∏0)∙ Hence,
the theorem is proved.	口
A.6 Proof of Theorem 4
We recall Theorem 4:
17
Published as a conference paper at ICLR 2022
Theorem 4. Let (rt1, . . . , rt|A|-1) be a sequence of certified radii for state st at time step t, where rtk
denotes the radius such that if ε < rtk, the possible action at time step t will belong to the actions
corresponding to top k action values of Q at state st. The definition of rt in Theorem 1 is equivalent
to rt1 here. The radii can be computed similarly as follows:
Qπ(st,aι) - Vmin! - φ-ι QQπ(st,ak+ι) - Vmin!!
Vmax - Vmin	Vmax - Vmin
1 ≤ k < |A|,
where a1 is the action of the highest Q value at state st and ak+1 is the (k + 1)-th best action. We
additionally define rt0 (st) = 0, which is also compatible with the definition above.
Proof. Replacing a2 with ak+1 in the proof for Theorem 1 in Appendix A.2 directly leads to Theo-
rem 4.	□
B Additional Details of Certification S trategies
In this section, we cover the concrete details regarding the implementation of our three certification
strategies, as a complement to the high-level ideas introduced back in Section 4 and Section 5.
B.1 Detailed Algorithm of CROP-LoAct
Algorithm 1: CROP-LOACT: Local smoothing for certifying per-state action
Input: state s, trained value network Qπ with range
[Vmin , Vmax]; parameters for smoothing:
sampling times m, smoothing variance σ2,	5
one-sided confidence parameter α
π
Output: smoothed value network Qπ , selected action 6
a, certification indicator cert, certified
radius r constant L	7
. Step 1: smoothing
8
ι Generate noise samples δi 〜N(0, σ2I) for
1≤i≤m
2	for each action a ∈ A do	9
. clipping and averaging
3	Qπ(s,a) - m P= Clip(Qπ(s +	10
_ δi , a) , min = Vmin, maχ = VmaX)	ɪɪ
4	a1,a2 — best action and runner-up action given by 12
Qπ
. Step 2: certification
δ	= (Vmax - Vmin) q2⅛ ln a1
. confidence interval
if Qeπ (s, a1) ≥ Qeπ (s, a2) + 2∆ then
. certification success
Cert — True
r — σ (φ-1(Qn(S,aI)-δ-Vmin)-
2 ： I	Vmax-Vmin
(R-1 Q Qn (S ,a2 Hδ-Vmin、、
φ ( Vmax-Vmin ))
else
. certification failure
Cert — False
r — undefined
π
return Qπ , a1 , Cert, r
We present the concrete algorithm of CROP-LoAct in Algorithm 1 for the procedures introduced
in Section 4.2. For each given state st, we first perform Monte Carlo sampling (Cohen et al., 2019;
Lecuyer et al., 2019) to achieve local smoothing. Based on the smoothed value function Qπ, we then
compute the robustness certification for per-state action, i.e., the certified radius rt at the given state
st, following Theorem 1.
Detailed Inference Procedure. During inference, we invoke the model with m samples of Gaus-
sian noise at each time step, and use the averaged Q value on these m noisy samples to obtain the
greedy action selection and compute the certification. This procedure is similar to Predict in Cohen
et al. (2019), but since RL involves multiple step decisions, we do not take the “abstain” decision as
in Predict in Cohen et al. (2019); instead, CROP-LoAct will take the greedy action at all steps no
matter whether the action can be certified or not.
Estimation of the Algorithm Parameters Vmin , Vmax. Our estimate is obtained via sampling the
trajectories and calculating the Q values associated with the state-action pairs along these trajectories.
Since we perform clipping using the obtained bounds, the certification is sound. The cost for
estimating Vmin and Vmax is essentially associated with the number of sampled trajectories, the
number of steps in each trajectory, the number of sampled Gaussian noise m per step, and the cost of
doing one forward pass of the Q network; thus, the estimation can be done with low cost. Furthermore,
18
Published as a conference paper at ICLR 2022
Algorithm 2: CROP-GRE: Global smoothing for certifying cumulative reward
Input: initial state distribution d0, trained value
network Qπ , game cumulative reward range
[Jmin, Jmax], number of steps in an episode
H, perturbation magnitude at each state ε,
percentile p; parameters for smoothing:
sampling times m, smoothing variance σ2,
one-sided confidence parameter α
Output: Expectation bound JE, p-th percentile
bound Jp
. Step 1: smoothing
1	for i = 1 to m do
2	so 〜do,JC —0	. initialization
3	Generate macro-state noise δt 〜N(0, σ2I) for
0≤t<H
4	for t = 0 to H - 1 do
5	at J argmaXa Q(St + δt,a)
6	Execute action at and observe reward ret
and next state st+1	. take a step
7	JiC J JiC + ret	. accumulate the
reward
. Step 2.1: certifying expectation
bound
8	Fe J ml Pi=1 JC	. smoothed actual reward
9	∆conf J (Rmax - Rmin) Jln(^
. confidence interval
10	∆iip J Rmax-Rmin J2 ∙ ε√H
. bound given by Lipschitz continuity
ɪ	言 A	Λ
11	JE J F — ∆conf — ∆lip
12
. Step 2.2: certifying percentile
bound
13	k J COMP UT EORD E RS TAT S (ε, σ, p, m, H)
14	Jp J k-th smallest value in {JC }m=1
15
16	return JE, Jp
we can balance the trade-off between the accuracy and efficiency of the estimation, and for any
configuration of Vmin and Vmax , the certification will invariably be sound (reason above).
B.2	Detailed Algorithm of CROP-GRe
We present the concrete algorithm of CROP-GRe in Algorithm 2 for the procedures introduced
in Section 5.1. Similarly to CROP-LoAct, the algorithm also consists of two parts: performing
smoothing and computing certification, where We compute both the expectation bound JE and the
percentile bound Jp.
Step 1: Global smoothing. We adopt Monte Carlo sampling (Cohen et al., 2019; Lecuyer et al.,
2019) to estimate the smoothed perturbed return function F by sampling multiple σ-randomized
trajectories via drawing m noise sequences. For each noise sequence Z 〜N(0, σ2lH×N), We apply
noise ζt to the input state st sequentially, and obtain the sum of the reward JiC = PtH=-01 ret as the
return for this σ-randomized trajectory. We then aggregate the smoothed perturbed return values
{JiC }im=1 via mean smoothing and percentile smoothing.
Step 2: Certification for perturbed cumulative reward. First, we compute the expectation bound
JE using Theorem 2. Since the smoothed perturbed return function FIS obtained based on m sampled
noise sequences, we use Hoeffding,s inequality (Hoeffding, 1994) to compute the lower bound of
the random variable F (㊉H-10)with a confidence level a. We then calculate the lower bound of
F (㊉H-1δt) under all possible '2-bounded perturbations δt ∈ Bε leveraging the smoothness of F.
We then compute the percentile bound JP using Theorem 3. We let JC be sorted increasingly, and
perform normal approximations (Stein et al., 1972) to compute the largest empirical order statistic
JkC such that P [Jp ≥ JC ≥ 1 — α. The empirical order statistic JkC is then used as the proxy of
Jp under α confidence level. We next provide detailed explanations for COMP UTEORDERSTAT S,
which aims to compute the order k using binomial formula plus normal approximation.
Estimation of Fπ (defined in Lemma 2). For computing the expectation bound (i.e., the lower
bound ofE∆ [Jε(π0)]), an intermediate step is to estimate Fπ (as shown in line 8 of Algorithm 2).
We emphasize that the accuracy of the estimation does not influence the soundness of the lower bound
calculation. The reason is given below. The number of sampled randomized trajectories m controls
19
Published as a conference paper at ICLR 2022
the trade-off between the estimation efficiency and accuracy, as well as the tightness of the derived
lower bound. Concretely, a small m would provide high efficiency, low estimation accuracy, and
loose lower bound; but the lower bound is always sound, since our algorithm explicitly accounts for
the inaccuracy associated with m via leveraging the Hoeffding’s inequality in line 9 in Algorithm 2.
Details of COMPUTEORDERSTATS. We consider the sorted sequence JC ≤ JC ≤…≤ Jm .
We additionally set J0C = -∞ and JmC+1 = ∞. Our goal is to find the largest k such that
P [jp ≥ JC ≥ 1 - α. We evaluate the probability explicitly as follows:
m
m
PhJp ≥ Jk i = X PhJC ≤ Jp〈J+』=X ⑺(Py(I- p0)m-i
i=k	i=k ' /
Thus, the condition P JJp ≥ JkC ≥ 1 - α is equivalent to
k-1
X mi (p0)i(1 - p0)m-i ≤ α.
i=0
(12)
Given large enough m, the LHS of (12) can be approximated via a normal distribution with mean
equal to mp0 and variance equal to mp0(1 - p0). Concretely, we perform binary search to find the
largest k that satisfies the constraint.
We finally explain the upper bound of ε that can be certified for each given smoothing variance.
In practical implementation, for a given sampling number m and confidence level parameter α, if
p0 is too small, then the condition (12) may not be satisfied even for k = 1. This implies that the
existence of an upper bound of ε that can be certified for each smoothing parameter σ, recalling that
P= Φ (Φ-1(p) - ε√H∕σ).
Detailed Inference and Certification Procedures. We next provide detailed descriptions of the
inference and certification procedures, respectively.
Inference procedure. We deploy the σ-randomized policy π0 as defined in Definition 4 in Section 5.1.
That is, for each observed state, we sample one time of Gaussian noise ∆t to add to st , and take action
according to this single randomized observation at = π(st + ∆t). Thus, in deployment time, the
∆-randomized policy π0 executes on one rollout—at each step it takes one observation and chooses
one action (following the procedure described above).
Certification procedure. We sample m randomized trajectories in CROP-GRE instead of sampling
m noisy states per time step as in CROP-LOACT. For each sampled randomized trajectory, at each
time step in the trajectory, we invoke the model with 1 sample of Gaussian noise (as shown in (4)).
Given the m randomized trajectories and the cumulative reward for each of them, we compute the
certification using Theorem 2 and Theorem 3.
The Algorithm Parameters Jmin , Jmax . We use the default values of Jmin , Jmax in the game
specifications rather than estimate them, which are independent with the trained models. Thus there
is no computational cost at all for obtaining the two parameters. As mentioned under Theorem 2
in Section 5.1, using these default values may induce a loose bound in practice, so we further propose
the percentile smoothing that aims to eliminate the dependency of our certification on Jmin and Jmax ,
thus achieving a tighter bound.
B.3	Detailed Algorithms of CROP-LoRe
In the following, we will explain the trajectory exploration and expansion, the growth of perturbation
magnitude, and optimization tricks in details.
Trajectory Exploration and Expansion. CROP-LORE organizes all possible trajectories in the
form of a search tree and progressively grows it. Each node of the tree represents a state, and the depth
of the node is equal to the time step of the corresponding state in the trajectory. The root node (at depth
0) represents the initial state s0 . For each node, leveraging Theorem 4, we compute a non-decreasing
sequence {rk (s)}|kA=|0-1 corresponding to required perturbation radii for π to choose each alternative
action (the subscript t is omitted for brevity). Suppose the current ε satisfies ri (s) ≤ ε < ri+1 (s).
We grow (i + 1) branches from current state s corresponding to the original action and i alternative
actions since ε ≥ rj(s) for 1 ≤ j ≤ i. For nodes on the newly expanded branch, we repeat the same
20
Published as a conference paper at ICLR 2022
Algorithm 3: CROP-LORE: Adaptive search for certifying cumulative reward
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
Input: Enivronment E = (S, A, R, Γ, d0), trained value network 24
Qπ with range [Vmin , Vmax]; parameters for randomized 25
smoothing: sampling times m, smoothing variance σ2,	26
one-sided confidence parameter α	27
Output: a map M that maps an attack magnitude ε to the	28
corresponding certified lower bound of reward J	29
. Initialize global variables	30
p_que V— 0	. initialize an empty priority queue 3]
containing tuples of (state s, action a, radius r,
reward J), sorted by increasing r	32
M V 0	33
Jglobal V ∞	. initialize global minimum reward 34
δ = (Vmax - Vmin) ,* ln 1
confidence bound
35
Function GE TACT I ONS (s, εlim , Jcur ):
Generate noise samples δi 〜 N (0, σ2 I ) for 1 ≤ i ≤ m
for each action a ∈ A do
~ _ , . -ɪ _______ _ ,
Q (S, a) J m Pi=Iclip(Q (S +
δi, a), min = Vmin, max = Vmax)
?	Kn
a? J argmaxa∈A Qπ(S, a)
a_liSt J 0
for each action a ∈ A do
if Γ(S, a) = ⊥ then
L continue
if Qeπ (S, a?) ≥ Qeπ (S, a) + 2∆ then
r — σ (φ-1( Qn (s，a?)f-Vmin
r J 2(φ (	Vmax-Vmin
_—~-
)-
else
Φ-1(
Qn (S，a) + ∆-Vmin
Vmax-Vmin
))
36
37
38
39
40
41
42
43
44
45
Procedure EXP AND (S, εlim , Jcur ):
if Jcur ≥ Jglobal then
L return 0	. pruning
a_liSt J GE TAC T I ON S (S, εlim , Jcur )
if a_liSt = 0 then
Jglobal j- min(Jglobal, Jcur)
return 0
for a ∈ a_liSt do
I s0 J Γ(s, a)
ret J EXPAND(S0, εlim, Jcur + R(S, a))
S0 〜 d0	. initialize initial state
EXPAND (S0 , εlim = 0, Jcur = 0)	. expand initial
trajectory
while True do
if p_que = 0 then
L break
. pop out the first element
(S, a, r, J) J p_que.pop()
. examine the next first element
(_, _, r0, _) J p_que.top()
.derive the critical ε's
ε J r, ε0 J r0
. obtain a pair of mapping
M[ε] J Jglobal
. expand the tree from the new node
Expand (Γ(s, a),ε0, J + R(s, a))
L r J 0
if r ≤ εlim then
. take possible actions
a_liSt J a_liSt ∪ {a}
else
. store impossible actions in queue for
later expansion
_ p_que.push((s,a,r,Jcur))
return a_liSt
procedure to expand the tree with depth-first search (Tarjan, 1972) until the terminal state of the game
is reached or the node depth reaches H . As we expand, we keep the record of cumulative reward for
each trajectory and update the lower bound J when reaching the end of the trajectory if necessary.
Perturbation Magnitude Growth. When all trajectories for perturbation magnitude ε are explored,
we need to increase ε to seek for certification under larger perturbations. Luckily, since the action
space is discrete, we do not need to examine every ε ∈ R+ (which is infeasible) but only need to
examine the next ε where the chosen action in some step may change. We leverage priority queue (van
Emde Boas, 1977) to effectively find out such next “critical” ε. Concretely, along the trajectory
exploration, at each tree node, we search for the possible actions and store actions corresponding
to {rk(s)}|kA=|i-+11 into the priority queue, since these actions are exactly those need to be explored
when ε grows. After all trajectories for ε are fully explored, we pop out the head element from the
priority queue as the next node to expand and the next perturbation magnitude ε to grow. We repeat
this process until the priority queue becomes empty or the perturbation magnitude ε reaches the
predefined threshold.
Additional Optimization. We adopt some additional optimization tricks to reduce the complexity
of the algorithm. First, for environments with no negative reward, we perform pruning to limit the
tree size—if the cumulative reward leading to the current node already reaches the recorded lower
bound, we can perform pruning, since the tree that follows will not serve to update the lower bound.
This largely reduces the potential search space. We additionally adopt the memorization (Michie,
1968) technique which is commonly applied in search algorithms.
21
Published as a conference paper at ICLR 2022
We point out a few more potential improvements. First, with more specific knowledge of the game
mechanisms, the search algorithm can be further optimized. Take the Pong game as an example, given
the horizontal speed of the ball, we can compress the time steps where the ball is flying between the
two paddles, thus reducing the computation. Second, empirical attacks may be efficiently incorporated
into the algorithm framework to provide upper bounds that help with pruning.
Time Complexity. The time complexity of CROP-LORE is O(H |Sexplored | × (log |Sexplored| +
|A|T)), where |Sexplored| is the number of explored states throughout the search procedure, which is
no larger than cardinality of state set, H is the horizon length, |A| is the cardinality of action set, and
T is the time complexity of performing local smoothing. The main bottleneck of the algorithm is the
large number of possible states, which is in the worst case exponential to state dimension. However,
to provide a sound worst-case certification agnostic to game properties, exploring all possible states
may be inevitable.
Detailed Inference and Certification Procedures and Important Modules in Algorithm 3. We
next provide detailed descriptions of the inference and certification procedures, respectively, along
with other important modules.
Inference procedure. In CROP-LoRe, We deploy the locally smoothed policy π as defined in (2)
in Section 4.1. Concretely, for each observed state st, we sample a batch of Gaussian noise to add to
st, and take action according to the computed mean Q value on the batch of randomized observations.
(Actually, the inference procedure per step is exactly the same as in CROP-LOACT described in
Appendix B.1.)
Certification procedure. We obtain the certification via Theorem 4 and the adaptive search algorithm,
which outputs a collection of pairs {(εi, Jε J}i=1 sorted in ascending order of 已％, where |C| is the
length of the collection. The interpretation for this collection of pairs is provided beloW. For all
ε0, let i be the largest integer such that εi ≤ ε0 < εi+1, then as long as the perturbation magnitude
ε ≤ ε0, the cumulative reward Jε(∏) ≥ J厂 ThiS is supported by the fact that all certified radii
associated with all nodes in the entire expanded tree compose a discrete set of finite cardinality; and
the perturbation value between two adjacent certified radii in the returned collection will not lead to a
different tree from the tree corresponding to the largest smaller perturbation magnitude. This actually
is also the inspiration for the certification design.
Important modules. The function GE TAC T I ON computes the possible actions at a given state s
under the limit ε, while the procedure EXPAND accomplishes the task of expanding upon a given
node/state. The main part of the algorithm involves a loop that repeatedly selects the next element
from the priority queue, i.e., a node associated with an ε value, to expand upon.
Additional Clarifications. We clarify that the algorithm only requires access to the environment
such that it can obtain the reward and next state via interacting with the environment E (i.e., taking
action a at state s and obtain next action s0 and reward r), but does not require access to an oracle
transition function Γ. We further emphasize that access to the environment that supports back-tracking
is already sufficient for conducting our adaptive search algorithm.
C Discussion on the Certification Methods
We discuss the advantages and limitations of our certification methods, as well as possible direct
extensions, hoping to pave the way for future research along similar directions. We also provide more
detailed analysis that help with the understanding of our algorithms.
CROP-LoAct. The algorithm provides state-wise robustness certification in terms of the stabil-
ity/consistency of the per-state action. It treats each time step independently, smooths the given state
at the given time step, and provides the corresponding certification. Thus, one potential extension is
to expand the time window from one time step to a consecutive sequence of several time steps, and
provide certification for the sequences of actions for the given window of states.
CROP-GRe. AS explained in Section 5,the expectation bound JE is too loose to have any practical
usage. In comparison, percentile bound JP is much tighter and practical. However, one limitation
of Jp is that there exists an upper bound of the attack magnitude ε that can be certified for each σ,
22
Published as a conference paper at ICLR 2022
as explained in Appendix B.2. For attack magnitudes that exceed the upper bound, we can obtain
no useful information via this certification (though the upper bound is usually sufficiently large).
One limitation of CROP-GRE. Lemma 2 requires knowing the noise added to each step beforehand
so as to generate m randomized trajectories given the known noise sequence {δt}tH=-01. Thus, it
cannot be applied against an adaptive attacker. We clarify that our CROP-LoAct and CROP-LoRe
does not have such limitation.
CROP-LoRe. The algorithm provides the absolute lower bound J of the cumulative reward for
any finite-horizon trajectory with a given initial state. The advantage of the algorithm is that J is
an absolute lower bound that bounds the worst-case situation (apart from the exceptions due to the
probabilistic confidence α), rather than the statistical lower bounds JE and Jp that characterize the
statistical properties of the random variable Jε.
One potential pitfall in understanding J. What CROP-LORE certifies is the lower bound for the
trajectory with a given initial state s0 〜d0, rather than all possible states in d°. This is because our
search starts from a root node of the tree, which is set to the fixed given state s0 .
Confidence of CROP-LORE. Regarding the confidence of our probabilistic certification, we clarify
that we consider the independent multiple-test. Concretely, due to the independence of decision
making errors, the confidence is (1 - α)N, where N is the maximum number of possible attacked
states explored by CROP-LoRe. Formally,
Pr[CROP-LORE certification holds] =	Pr[not make error on s|not make error on spre]
all s explored
by CROP-LORE
=(i)	Y (1-α) × Y 1
all attackable s	all unattackable s
(=ii) (1 - α)N.	(13)
In the above equation, we can see that (i) leverages the independence which in turn gives a bound
of form (1 - α)N. This is because in each step, the event of “certification does not hold” is
independent since we sample Gaussian noise independently. Leveraging such independence, we
obtain a confidence lower bound of (1 - α)N. From (ii), we see that the confidence is only related to
the number of possible attacked states N . This is because for unattackable states, the certification
deterministically holds since there is no attack at current step. Therefore, we only need to count the
confidence intervals from all possibly attackable steps instead of all states explored. We remark that
in practice, the attacker usually has the ability to perturb only a limited number of steps, i.e., N is
typically small. Therefore, the confidence is non-trivial.
Main limitation of CROP-LORE. The main limitation is the high time complexity of the algorithm
(details see Appendix B.3). The algorithm has exponential time complexity in worst case despite
the existence of several optimizations. Therefore, it is not suitable for environments with a large
action set, or when the horizon length is set too long.
Extension to Policy-based Methods. Though we specifically study the robustness certification in
Q-learning in this paper, our two certification criteria and three certification strategies can be readily
extended to policy-based methods. The intuition is that, instead of smoothing the value function in
the Q-learning setting, we directly smooth the policy function in policy-based methods. With the
smoothing module replaced and the theorems updated, other technical details in the algorithms for
certifying the per-state action and the cumulative reward would then be similar.
D	Additional Experimental Details
D. 1 Details of the Atari Game Environment
We experiment with two Atari-2600 environments in OpenAI Gym (Brockman et al., 2016) on top of
the Arcade Learning Environment (Bellemare et al., 2013). The states in the environments are high
dimensional color images (210 × 160 × 3) and the actions are discrete actions that control the agent
to accomplish certain tasks. Concretely, we use the NoFrameskip-v4 version for our experiments,
23
Published as a conference paper at ICLR 2022
where the randomness that influences the environment dynamics can be fully controlled by setting
the random seed of the environment at the beginning of an episode.
D.2 Details of the RL Methods
We introduce the details of the nine RL methods we evaluated. Specifically, for each method, we first
discuss the algorithm, and then present the implementation details for it.
StdTrain (Mnih et al., 2013): StdTrain (naturally trained DQN model) is an algorithm based on
Q-learning, in which DQN is used to reprent Q-value function and TD-loss is used to optimize DQN.
Our StdTrain model is implemented with Double DQN (Van Hasselt et al., 2016) and Prioritized
Experience Replay (Schaul et al., 2015).
GaussAug (Behzadan & Munir, 2017b): The algorithm of GaussAug is similar with AtdTrain, in
which appropriate Gaussian random noises added to states during training. As for implementation,
our GaussAug model is based on the same architecture with the same set of basic training techniques
of StdTrain, and adds Gaussian random noise with σ = 0.005, which has the best testing performance
under attacks compared with other σ, to all the frames during training.
AdvTrain (Behzadan & Munir, 2017b): Instead of using the original observation to train Double
DQN (Van Hasselt et al., 2016), AdvTrain generates adversarial perturbations and applies the
perturbations to part of or all of the frames when training. In our case, we add the perturbations
generated by 5-step PGD attack to 50% of the frames to make a balance between between stable
training and effectiveness of adversarial training.
SA-MDP (PGD) and SA-MDP (CVX) (Zhang et al., 2020): Instead of utilizing adversarial noise to
train models directly like AdvTrain, SA-MDP (PGD) and SA-MDP (CVX) regularize the loss function
with the help of adversarial noise during training in order to train empirically robust RL agents. In
our experiment settings, which is the same as SA-MDP (PGD) and SA-MDP (CVX) (Zhang et al.,
2020), models consider adversarial noise with '∞-norm ε = 1∕255 when solving the maximization
for the regularizer and minimize the original TD-loss concurrently.
RadialRL (Oikarinen et al., 2020): With the similar idea of adding regularization during training of
SA-MDP (PGD) and SA-MDP (CVX), RadialRL compute the worst-case loss instead of regularizing
of the bound of the difference of policy distribution under original states and adversarial states. In
our case, the RadialRL model uses a linearly increasing perturbation magnitude ε (from 0 to 1/255) to
compute the adversarial loss.
CARRL (Everett et al., 2021): Unlike other methods which try to improve robustness of RL agents
during training, CARRL enhances RL agents’ robustness during testing time. It computes the
lower bound of each action’s Q value at each step and take the one with the highest lower bound
conservatively. Given that the lower bound is derived via neural network verification methods (Gowal
et al., 2018; Weng et al., 2018), CARRL can only be applied to low dimensional environments (Everett
et al., 2021). Thus, we only evaluate CARRL on CartPole and Highway, where we set the bound
of the `2 norm of the perturbation be ε = 0.1 and ε = 0.05 respectively when computing the lower
bound of Q value, which demonstrate the best empirical performance under noise and empirical
attacks.
NoisyNet (Fortunato et al., 2017): Instead of the conventional exploration heuristics for DQN and
Dueling agents which is ε-greedy algorithms, NoisyNet adds parametric noise to DQN’s weights,
which is claimed to aid efficient exploration and yield higher scores than StdTrain. Concretely, our
NoisyNet shares the same basic architecture with StdTrain, and samples the network weights during
training and testing, where the weights are sampled from a Gaussian distribution with σ = 0.5.
GradDQN (Pattanaik et al., 2018): GradDQN is a variation of AdvTrain, which utilizes conditional
value of risk (CVaR) as optimization criteria for robust control instead standard expected long term
return. This method maximizes the expected return over worst α percentile of returns, which can
prevent the adversary from possible bad states. In our case, we generate the adversarial states with
10-step attack by calculating the direction of gradient and sampling in that direction from beta1, 1
distribution in each step.
24
Published as a conference paper at ICLR 2022
For StdTrain, SA-MDP (PGD), SA-MDP (CVX), RadialRL, we directly evaluate the trained models
provided by the authors1.
D.3 Rationales for Settings and Experimental Designs
Finite Horizon Setting. In this paper, we focus on a finite horizon setting, mainly for the
evaluation purpose—we would need to compute the (certified and empirical) cumulative rewards
from finite rollouts. We clarify that our criteria, theorem, and algorithm are applicable to the infinite
horizon case as well.
The Role of Practical Attacks in Our Evaluation. The goal of this paper is to provide certification
that comes with theoretical guarantees, and therefore following the standard certified robustness
literatures (Cohen et al., 2019; Li et al., 2020; Salman et al., 2019; Jeong & Shin, 2020), there is
no need to evaluate the empirical attacks, since the certification always holds as long as the attacks
satisfy certain conditions regardless of the actual attack algorithms. The attacks we evaluated in our
paper were only for the demonstration purpose and in the hope of providing an example.
D.4 Detailed Evaluation Setup for Atari Games
We introduce the detailed evaluation setups for the three certification methods corresponding to the
experiments in Section 6.
Evaluation Setup for CROP-LoAct. We report results averaged over 10 episodes and set the
length of the horizon H = 500. At each time step, we sample m = 10, 000 noisy states for smoothing.
When applying Hoeffding’s inequality, we adopt the confidence level parameter α = 0.05. Since
the input state observations for the two Atari games are in image space, we rescale the input states
such that each pixel falls into the range [0, 1]. When adding Gaussian noise to the rescaled states,
we sample Gaussian noise of zero mean and different variances. Concretely, the standard deviation
σ is selected among {0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 0.75, 1.0, 1.5, 2.0, 4.0}. We evaluate
different parameters for different environments.
Evaluation Setup for CROP-GRe. We sample m = 10, 000 σ-randomized trajectories, each of
which has length H = 500, and conduct experiments with the same set of smoothing parameters as in
the setup of CROP-LOACT. When accumulating the reward, we set the discount factor γ = 1.0 with
no discounting. We take α = 0.05 as the confidence level when applying Hoeffding’s inequality in
the expectation bound JE, and α = 0.05 as the confidence level for COMPUTEORDERSTATS in the
percentile bound Jp. For the validation of tightness, concretely, we carry out a 10-step PGD attack
with '2 radius within the perturbation bound at all time steps during testing to evaluate the tightness
of our certification.
Evalution Setup for CROP-LoRe. We set the horizon length as H = 200 and sample m =
10, 000 noisy states with the same set of smoothing variance as in the setup of CROP-LOACT.
Similarly, we adopt γ = 1.0 as the discount factor and α = 0.05 as the confidence level when
applying Hoeffding’s inequality. When evaluating Freeway, we modify the reward mechanism so that
losing one ball incurs a zero score rather than a negative score. This modification enables the pruning
operation and therefore facilitates the search algorithm, yet still respects the goal of the game. We
also empirically attack the policy ∏ to validate the tightness of the certification via the same PGD
attack as described above. For each set of experiments, we run the attack one time with the same
initial state as used for computing the lower bound in Algorithm 3.
D.5 Experimental Setup for CartPole
We additionally experiment with CartPole-v0 in OpenAI Gym (Brockman et al., 2016) on top of the
Arcade Learning Environment (Bellemare et al., 2013). We introduce the experimental setup below.
Details of the CartPole Environment. The state in the CartPole game is a low dimensional vector
of length 4 and the action ∈ {move right, move left}. The goal of the game is to balance the rod on
the cart, and one reward point is earned at each timestep when the rod is upright.
1StdTrain, SA-MDP (PGD), SA-MDP (CVX) from https://github.com/chenhongge/SA_DQN and RadialRL from https:
//github.com/tuomaso/radial_rl under Apache-2.0 License
25
Published as a conference paper at ICLR 2022
Implementation Details of the RL Methods on CartPole. In addition to the eight RL Methods
evaluated on Pong and Freeway as shown in Section 6, we evaluate another RL algorithm CARRL (Ev-
erett et al., 2021), which is claimed to be robust in low dimensional games like CartPole. This method
relies on linear bounds output by the Q network and selects the action whose lower bound of the Q
value is the highest. More detailed introduction to the algorithm and description of the implementation
details are provided in Appendix D.2.
Evaluation Setup for CROP-LoAct on CartPole. We report results averaged over 10 episodes
and set the length of the horizon H = 200. At each time step, we sample m = 10, 000 noisy states
for smoothing. When applying Hoeffding’s inequality, we adopt the confidence level parameter
α = 0.05. We do not perform rescaling on the state observations. When adding Gaussian noise to
states, we sample Gaussian noise of zero mean and different variances. Concretely, the standard
deviation σ is selected among {0.001, 0.005, 0.01, 0.03, 0.05, 0.1}.
Evaluation Setup for CROP-GRe on CartPole. We sample m = 10, 000 σ-randomized trajec-
tories, each of which has length H = 200, and conduct experiments with the same set of smoothing
parameters as in the setup of CROP-LoAct. All other experiment settings are the same as Pong and
Freeway in Appendix D.4.
Evalution Setup for CROP-LoRe on CartPole. Since the reward of CartPole game is quite
dense, we set the horizon length as H = 10 and sample m = 10, 000 noisy states with the same set
of smoothing variance as in the setup of CROP-LoAct. All other experiment settings are the same
as Pong and Freeway in Appendix D.4.
D.6 Experimental Setup for Highway
In addition to the previous environments in OpenAI Gym (Brockman et al., 2016) on the Arcade
Learning Environment (Bellemare et al., 2013), we also evaluate our methods in the Highway
environment (Leurent, 2018). Specifically, we test out algorithms in the highway-fast-v0 environment.
Details of the highway Environment. The state in the highway-fast-v0 environment is a 5 × 5
matrix, where each line represents a feature vectors of either the ego vehicle or other vehicles closest
to the ego vehicle. The feature vector for each vehicle has the form of [x, y, vx, vy, 1], corresponding
to the two-dimensional positions and velocities and an additional indicator value. The actions are
high-level control actions among {lane left, idle, lane right, faster, slower}. The reward mechanism
in this environment is associated with the status of the ego vehicle; more concretely, it gives higher
reward for the vehicle when it stays at the rightmost lane, moves at high speed, and does not crash
into other vehicles.
Evaluation Setup for CROP-LoAct on highway. We report results averaged over 10 episodes
and set the length of the horizon H = 30, which is the maximum length of an episode in the
environment configuration. At each time step, we sample m = 10, 000 noisy states for smoothing.
When applying Hoeffding’s inequality, we adopt the confidence level parameter α = 0.05. We do
not perform rescaling on the state observations. When adding Gaussian noise to states, we sample
Gaussian noise of zero mean and different variances. Concretely, the standard deviation σ is selected
among {0.005, 0.05, 0.1, 0.5, 0.75, 1.0}.
Evaluation Setup for CROP-GRe on highway. We sample m = 10, 000 σ-randomized trajec-
tories (each of which has length H = 30) and conduct experiments with the same set of smoothing
parameters as in the setup of CROP-LoAct. All other experiment settings are the same as Pong and
Freeway in Appendix D.4.
Evalution Setup for CROP-LoRe on highway. Since the reward of highway game is also quite
dense, we set the horizon length as H = 10 as in the CartPole environment and sample m = 10, 000
noisy states with the same set of smoothing variance as in the setup of CROP-LoAct. All other
experiment settings are the same as Pong and Freeway in Appendix D.4.
26
Published as a conference paper at ICLR 2022
% oitar % oitar
yaweerF gnoP
StdTram
1.0
0.8
0.6
0.4
0.2
0.0
AdvTrain
GaussAug
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
σ= 0.1
SA-MDP (PGD)
SA-MDP (CVX)
σ=0.5
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
le-2
σ=0∙001
radius r
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
ie-2
0.0 0.2 0.4 0.6 0.8 1.0
σ=0∙03
radius r
RadiaIRL
0=0.75
0.5
0.2 0.4 0.6
σ=1.0
radius r

Figure 4: Robustness certification for per-state action in terms of certified ratio ηr w.r.t. certified radius r. Each
column corresponds to one smoothing parameter σ. The shaded area represents the standard deviation.
----StdTrain
GaussAug ——AdvTrain —— SA-MDP(PGD) ------------ SA-MDP (CVX) —— RadiaIRL
J SnyPUy
σ = 1.5
σ = 2.0
σ = 4.0
σ = 1.5
σ = 2.0
σ = 4.0
160
IZI
120
1X10
0∙β0
0.60
QXO
oao
1.75 ________________________
~	.一	1.50
1.25 ɪɪ
1.Q0
0.75
0.50
—0.25 ----------------------------
------------------------ 0.00
0 100 200 300 «0 500	0 100 200 300 «0 500
2.00
1.50
1.00
0.50
0.00
0 100 200 300 «0 500
time step t	time step t	time step t
0	-----------ɪɪ 0 -	----U------- 0 -----------U----
0-0	0-5	1-0	1-5	0-0 0-5 1-0 1-5	0	1	2	3	4
attack ε	attack ε	attack ε
(a)	Certified radius Rt along time StePs	(b) Absolute lower bound J w.r.t. attack ε
Figure 5: Robustness certification for (a) per-state action in terms of certified radius r at all time steps, and (b)
cumulative reward in terms of absolute lower bound J w.r.t. the attack magnitude ε. The results are reported
under large smoothing parameter σ compared with those evaluated in Figure 1 and Figure 3 in the main paper.
E Additional Evaluation Results and Discussions
E.1 ROBUSTNESS CERTIFICATION FOR PER-STATE ACTION — CERTIFIED RATIO
We present the robustness certification for per-state action in terms of the certified ratio ηr w.r.t. the
certified radius r in Figure 4. From the figure, we see that RadialRL is the most certifiably robust
method on Freeway, followed by SA-MDP (CVX) and SA-MDP (PGD); while on Pong, SA-MDP
(CVX) is the most robust. These conclusions are highly consistent with those in Section 6.1 as
observed from Figure 1. Comparing the curves for Freeway and Pong, we note that Freeway not only
achieves larger certified radius overall, it also more frequently attains large certified radius.
E.2 RES ULTS OF FREEWAY UNDER LARGER SMOOTHING PARAMETER σ
Since the robustness of the methods RadialRL, SA-MDP (CVX), and SA-MDP (PGD) on Freeway
have been shown to improve with the increase of the smoothing parameter σ in Section 6, we
subsequently evaluate their performance under even larger σ values. We present the certification
results in Figure 5 and their benign performance in Figure 6.
First of all, as Figure 6 reveals, RadialRL, SA-MDP
(CVX), and SA-MDP (PGD) can tolerate quite large
noise—the benign performance of these methods does
not drop even for σ up to 4.0. For the remaining three
methods, the magnitude of noise they can tolerate is much
smaller, as already presented in Figure 2 in the main paper.
On the other hand, although all the three methods Radi-
alRL, SA-MDP (CVX), and SA-MDP (PGD) invariably
attain good benign performance as σ grows, their certified
robustness does not equally increase. As Figure 5 shows,
the certified robustness of SA-MDP (PGD) increases the
fastest, while that of RadialRL drops a little. The observa-
f p」eM①」
Freeway
① >+:B-nuln。
5e-3 5e-2 Ie-I 5e-17.5e-l 1.0 1.5
-B- SttfTrain
♦ GaussAug
—Advlrain
-*- SA-MDP (PGD)
→- SA-MDP (CVX)
T- RadiaIRL
Figure 6: Benign performance of locally
smoothed policy ∏ under a larger range of
smoothing parameter σ with clean state ob-
servations.
27
Published as a conference paper at ICLR 2022
Figure 7: Periodic patterns in Pong. The figure shows two periods (left and right), including the certified radius
r w.r.t. the time steps (above), and the selected game frames corresponding to different stages in each period
(below). Different periods are highly similar.
tion corresponds exactly to our discussion regarding the tradeoff between value function smoothness
and the margin between the values to the top two actions in Section 4.
E.3 Periodic Patterns for Per-State Robustness
We specifically study the periodic patterns for per-state robustness in Pong and present the results
in Figure 7. In the game frames, our agent controls the green paddle on the right. Whenever the
opponent (the orange paddle on the left) misses catching a ball, we earn a point. We present two
periods (frame 320-400 and frame 400-480) where our agent each earns a point in the figure above.
The total points increase from 2 to 3, and from 3 to 4, respectively.
We note that Pong is a highly periodic game, with each period consisting of the following four game
stages corresponding to distinct stages of the certified robustness: 1) A ball is initially fired and set
flying towards the opponent. In this stage, the certified radius remains low, since our agent does
not need to take any critical action. 2) The opponent catches the ball and reverses the direction of
the ball. In this stage, the certified radius gradually increases, since the moves of our agent serve as
the preparation to catch the ball. 3) The ball bounces from the ground, flies towards our agent, gets
caught, and bounces back. In this stage, the certified radius increases rapidly, since every move is
critical in determining whether we will be able to catch the ball, i.e., whether our point (action value)
will increase. 4) The opponent misses the ball, and the ball disappears from the game view. In this
stage, the certified radius drastically plummets to a low value, since there is no clear signal (i.e., ball)
in the game for the agent to take any determined action.
The illustration not only helps us understand the semantic meaning of the certified radius, but can
further provide guidance on designing empirically robust RL algorithms that leverage the information
regarding the high/low certified radius of the states.
E.4 DETAILED DISCUSSIONS AND SELECTION OF SMOOTHING PARAMETER σ
We first take Freeway as an example to concretely explain the impact of smoothing variance from
the perspective of benign performance and certified results, and then introduce the guidelines for the
selection of σ based on different certification criteria.
On Freeway, as σ increases, the benign performance of the locally smoothed policy π of StdTrain,
GaussAug, and AdVTrain decrease rapidly in Figure 2. Also, the certified radius of these methods
barely gets improved and remains low even as σ increases. For the percentile bound Jp , these
three methods similarly suffer a drastic decrease of cumulative reward as σ grows. This is however
not the case for SA-MDP (PGD), SA-MDP (CVX), and RadialRL, for which not only the benign
performance do not experience much degradation for σ as large as 1.0, but the certified radius even
steadily increases. In addition, in terms of the percentile bound Jp , as σ grows, even we increase the
attack ε simultaneously, Jp almost does not decrease. This indicates that larger smoothing variance
can bring more robustness to SA-MDP (PGD), SA-MDP (CVX), and RadialRL.
28
Published as a conference paper at ICLR 2022
CROP-LoAct. We discuss how to find the sweet spot that enables high benign performance and
robustness simultaneously. As σ increases, the benign performance of different methods generally
decreases due to the noise added, while not all methods decrease at the same rate. On Freeway,
StdTrain and GaussAug decrease the fastest, while SA-MDP (PGD), SA-MDP (CVX), and RadialRL
do not degrade much even for σ as large as 4.0. Referring back to Figure 1 and Figure 5, we see that
the certified radius of StdTrain and GaussAug remains low even though σ increases, thus the best
parameters for these two methods are both around 0.1.
For SA-MDP (PGD) and SA-MDP (CVX), their certified radius instead steadily increases as σ
increases to 4.0, indicating that larger smoothing variance can bring more robustness to these two
methods without sacrificing benign performance, offering certified radius larger than 2.0. As to
RadialRL, its certified radius reaches the peak at σ = 1.0 and then decreases under larger σ. Thus,
1.0 is the best smoothing parameter for RadialRL which offers a certified radius of 1.6. Generally
speaking, it is feasible to select an appropriate smoothing parameter σ for all methods to increase their
robustness, and robust methods will benefit more from this by admitting larger smoothing variances
and achieving larger certified radius.
CROP-GRe. We next discuss how to leverage Figure 3 to assist with the selection of σ. For a
given ε or a known range of ε, we can compare between the lower bounds corresponding to different
σ and select the one that gives the highest lower bound. For example, if we know that ε is small
in advance, we can go for a relatively small σ which retains a quite high lower bound. If we know
that ε will be large, we will instead choose among the larger σ's, since a larger ε will not fall in the
certifiable ranges of smaller σ ’s. According to this guideline, we are able to obtain lower bounds of
quite high value for RadialRL, SA-MDP (CVX), and SA-MDP (PGD) on Freeway under a larger
range of attack ε.
CROP-LoRe. Apart from the conclusion that larger smoothing parameter σ often secures higher
lower bound J, we point out that smaller smoothing variances may be able to lead to a higher lower
bound than larger smoothing variances for a certain range of ε. This is almost always true for very
small ε since smaller σ is sufficient to deal with the weak attack without sacrificing much empirical
performance, e.g., in Figure 3, when ε = 0.001, SA-MDP (CVX) on Freeway can achieve J = 3 at
σ = 0.005 while only J = 1 for large σ. This can also happen to robust methods at large ε, e.g., when
ε = 1.2, RadialRL on Freeway achieves J = 2 at σ = 0.75 while only J = 1 at σ = 1.0. Another
case is when σ is large enough such that further increasing σ will not bring additional robustness,
e.g., in Figure 5, when ε = 1.5, RadialRL on Freeway achieves J = 1 at σ = 1.5 while only J = 0
at σ = 4.0.
E.5 Computational Cost
Our experiments are conducted on GPU machines, including GeForce RTX 3090, GeForce RTX
2080 Ti, and GeForce RTX 1080 Ti. The running time of m = 10000 forward passes with sampled
states for per-state smoothing ranges from 2 seconds to 9 seconds depending on the server load. Thus,
for one experiment of CROP-LOACT with trajectory length H = 500 and 10 repeated runs, the
running time ranges from 2.5 hours to 12.5 hours. CROP-GRE is a little more time-consuming than
CROP-LOACT, but the running time is still in the same magnitude. For trajectory length H = 500
and sampling number m = 10000, most of our experiments finish within 3 hours. For CROP-LORE
specifically, the running time depends on the eventual size of the search tree, and therefore differs
tremendously for different methods and different smoothing variances, ranging from a few hours to 4
to 5 days.
E.6 Discussion on Evaluation Results of CartPole
We provide the evaluation results for CROP on CartPole, comparing nine RL algorithms: the six
algorithms evaluated in our main paper (StdTrain, GaussAug, AdvTrain, SA-MDP (PGD), SA-MDP
(CVX), and RadialRL), as well as three additional algorithms (CARRL, NoisyNet, and GradDQN).
The detailed descriptions of these algorithms are provided in Appendix D.2. We present the evaluation
results in Figure 8.
Impact of Smoothing Parameter σ. In CartPole game, we draw similar conclusions regarding the
impact of smoothing variance from the results given by CROP-LoAct, CROP-GRe and CROP-
29
Published as a conference paper at ICLR 2022
LORE. For small σ, different methods are indistinguishable. As σ increases, CARRL demonstrates
its advantage as σ increases, but AdvTrain and GradDQN tolerate large noise. One of differences
when evaluating with CROP-GRe and CROP-LoRe is that RadialRL can hold a high performance
and even sometimes can be better than CARRL. Another one is that the lower bound for GradDQN is
lower than all other methods in CROP-LoRe.
Tightness of the certification JE, Jp and J. In CartPole, We also compare the empirical cumula-
tive rewards achieved under PGD attacks with our certified lower bounds JE, Jp and J. Demonstrated
by Figure 8c, first of all, the correctness of our bounds is validated because the empirical results are
consistently lower bounded by our certifications. Compared with the loose expectation bound JE, the
improved percentile bound Jp is much tighter. Compared with these two methods, the absolute lower
bound J is even tighter, especially noticing the zero gap between the certification and the empirical
result when the attack magnitude is not too large. Finally, methods with the same empirical results
under attack may achieve very different certified lower bounds JE, Jp and J, showing the importance
of certification.
Environment Properties. Different from Pong and Freeway, CartPole is an environment with
low dimension states, in contrast to the high dimensional Atari games evaluated in Section 6.
This gives rise to several outcomes. First, StdTrain can tolerate noise well in CartPole. Second,
CARRL demonstrates its advantage in low dimensional game, which is consistent with the empirical
observations in Everett et al. (2021).
E.7 Discussion on Evaluation Results of Highway
We next present the evaluation results for CROP on an autonomous driving environment Highway,
whose state dimension is larger than CartPole but smaller than Atari games. We compare nine RL
algorithms as introduced previously. We present the evaluation results in Figure 9.
Impact of Smoothing Parameter σ. In highway, CARRL demonstrates its advantage when σ is
small, which is supported by results in all three certifications CROP-LoAct, CROP-GRe, and
CROP-LORE. For action certification (i.e., CROP-LOACT), SA-MDP (PGD) can tolerate a large
range of σ, meaning that the action selection of model trained with SA-MDP (PGD) algorithm is
more consistent than other methods. For reward certification (i.e., CROP-GRE and CROP-LORE),
we derive the conclusion that RadialRL outperforms than other methods when σ is large.
Tightness of the certification JE, Jp and J. In highway, we compare the empirical cumulative
rewards achieved under PGD attacks with our certified lower bounds JE, Jp, and J. The correctness
of our bounds is validated because the empirical results are consistently lower bounded by our
certifications, as shown in Figure 9c. As for the tightness of reward certification methods, we draw
similar conclusion as other environments—J is the most tight, followed by Jp; and JE is much
looser than Jp and JE. We also notice the non-negligible gap between certified lower bounds and
the empirical rewards demonstrated by RadialRL and CARRL under large smoothing parameters.
This may imply that there exists large scopes for improving the attack method, or there is potential to
further tighten the certified lower bounds.
F	A B roader Discussion on Related Work
F.1 Evasion Attacks in RL
We consider the adversarial attacks on state observations, where the attacker aims to add perturbations
to the state during test time, so as to achieve certain adversarial goals, such as misleading the action
selection and minimizing the cumulative reward. We discuss the related works that fall into these
two categories below.
Misleading the Action Selection. It has been shown that different methods—applying random
noise to simply interfere with the action selection (Kos & Song, 2017), or adopting adversarial attacks
(e.g., FGSM (Goodfellow et al., 2014) and CW attacks (Carlini & Wagner, 2017)) to deliberately
alter the probability of action selection—are effective to different degrees. More concretely, when
manipulating the action selection probability, some works aim to reduce the probability of selecting
30
Published as a conference paper at ICLR 2022
StdTrain	GaUSSAUg ——AdvTrain ——SA-MDP (PGD) ——SA-MDP (CVX) ——RadiaIRL ——CARRL ——NoisyNet ——GnadDQN
kSnjPEM
eloPtraC
le-3 σ=0.001
1.50
1.00
0.50
0.00
0	50 100 150 200
time step t
B-OO
6.00
4.00
2.00
0.00
le-3 σ=0.005
0	50 100 150 200
time step t
1.50
1.00
0.50
0.00
0	50 100 150 200
Xe-2 σ=0.01
3.00
time step t
le-2 σ=0.03
2.00
1.00
0.00
5.00
4.00
3.00
2.00
1.00
0.00
0	50 100 150 200
B-OQ
6.00
4.0
2-0
0-0
0	50 100 150 200
O 50 IOO 150 200
time step t
time step t	time step t
(a) Robustness certification for per-state action in terms of certified radius r at all time steps. The shaded area
represents the standard deviation.
CartPoIe
T- StdTTaIn
GaussAug
—Advr∏,aln
⅛- SA-MDP (PGD)
-*- SA-MDP (CVX)
→- RadIaIRL
-tt- CARRL
-♦- NolsyNet
→- GradDQN
(b)	Benign performance of locally smoothed policy ∏ under different smoothing variance σ.
-≡- stt∏T⅞ln (empirical)
----SfcmaMtcertified)
GiussAug (emplrleaŋ
GiussAug (CerfflIed)
-*- A<Mtaln (empirical)
MtflUIn KertHted)
SA-MDP (PSD)(eπptrlca Q
SA-MDP(PGD)Icertffled)
* SA-MDP (CVX) (eπptrlcaQ
----SA-MBP {CWC) (CβrtffleΛ
+ IUdIaIHL IemptrIcaD
—RadiamLKertmed)
* CARRL {eπplrlcaD
C*λλl (Certined)
♦ NeIsyNet (∙mplrlc4θ ♦ GradOQN (emplilcaO
NoIsyNettcertffled) --------GradDQM KertHtedJ
J)labolG( pJ)labolG
σ = 0.001
0-0	0-5	1-0	1-5
le-3
σ = 0.01
O
1.5
le-3
σ = 0.03
200
100
O
-IOO
-200
0	12	3	4
200
160
120
BO
40
σ = 0.05
200 J
IOO g∙
o -
-IOO
-200
-300
σ = 0.1
0.0	0-5	1-0	1.5
ie-2
0-0	0-5	1-0	1-5
le-2
0.001
0.000 0.002 0.004 0.006 0.008
0.000 0.005 0.010 0.015
7
0.00	0.02	0.04
attack ε
7	1--------
0.00	0.05	0-10
7
0.000
7
0.00 0.01 0-02 0-03
attack ε
attack ε	attack ε	attack ε
attack ε
(C) Robustness certification as cumulative reward, including expectation bound JE, percentile bound Jp (p =
50%), and absolute lower bound J. Solid lines represent the certified reward bounds of different methods, and
dashed lines show the empirical performance under PGD attacks.
Figure 8:	Robustness certification on CartPole in terms of (a-b): robustness of per-state action and (c): lower
bound of cumulative rewards. In (a) and (c), each column corresponds to one smoothing variance.
31
Published as a conference paper at ICLR 2022
』SnyPEM
yawhgih
σ-0.005
120
ιm
040
0Λ0
040
。2。
OAO
σ-0.05
120
ιm
040
OM
040
。2。
OAO
O SgTSR 253a
time step t
——Std-Π∙ain GaUSSAUg ——AdvT⅛in ——SA-MDP (PGD) ——SA-MDP (CVX) ——RadiaIRL ——CARRL ——NoisyNet ——GnadDQN
σ-0.75
2.00
I-SO
1.00
0.8
0.00
O 5	10 15 20 25 30
time step t
7.W
SAO
S-OO
，耻	SM
ami	*∙oo
2耻	”》
2.00
,λ∙	1«
04。	0.00
O 5	10 U 20 25 30 O 5	10 15 20 25 30 O
time step t	time step t
840
640
440
24。
OAO
5	10 U 20 25 3β O 5 W U 20 25 30
time step t
time step t
30
25
20
15
10
5
O
(a)	Robustness certification for per-state action in terms of certified radius r at all time steps. The shaded area
represents the standard deviation.
Highway
■ StdTrain
GaussAug
—∙— AdvTrain
→- SArMDP(PGD)
→- SArMDP(CVX)
T- RadiaIRL
-W- CARRL
NoisyNet
→- GradDQN
5e-3 5e-2 le-1 5e-l 7.5e-l 1.0
σ
(b)	Benign performance of locally smoothed policy ∏ under different smoothing variance σ.
-⅜- Ad√τtaln (empirical)
Icertined)
SA-MDP (PGD)(eπptrm 0
SA-MDP (PGD) (Certffled)
-*- SA-MDP (CVX) (eπptrlcaθ
-----SA-MDP (CVX) (Certffled)
lUdlalHL IemptrIcaD
—IUdiamncertined)
* CARRL {eπplrlcaQ
CAMLfcertined)
-φ∙ Nβl^Net (∙mplrlC4θ
NoisyNet(Cerfflled)
♦ SradDQN (empirical)
----GradDQNtcertined)
J)labolG(
J)lacoL
~∙~ SMitaln (empirical)
----SMlfsInKertmed)
GaUSSAUg (empirical)
GiussAua (certified)
0.000	0.005	0.010
attack ε
σ = 0.05
OXJO 0.05	0.10
attack ε
8
le-3
Ie-3
σ = 0.1
ie-2
21
18
15
12
0
!
3
0.00 0.05 0.10 0.15 0.20
attack ε
attack ε
σ = 0.75
σ= 1.0
21
18
15
12
0
!
3
21
18
15
12
0
!
3
0.0	0.5	1.0	1.5
attack ε
0.0	0.5 IR 1.5
attack ε
一
C
(c)	Robustness certification as cumulative reward, including expectation bound JE, percentile bound Jp (p =
50%), and absolute lower bound J. Solid lines represent the certified reward bounds of different methods, and
dashed lines show the empirical performance under PGD attacks.
Figure 9:	Robustness certification on highway in terms of (a-b): robustness of per-state action and (c): lower
bound of cumulative rewards. In (a) and (c), each column corresponds to one smoothing variance.
32
Published as a conference paper at ICLR 2022
the optimal action (Huang et al., 2017; Kos & Song, 2017; Behzadan & Munir, 2017b), while others
try to increase the probability of selecting the worst action (Lin et al., 2017; Pattanaik et al., 2018).
Minimizing the Cumulative Reward. ATLA (Zhang et al., 2021) and PA-AD (Sun et al., 2021)
consider an optimal adversary under the SA-MDP framework (Zhang et al., 2020), which aims to lead
to minimal value functions under bounded state perturbations. To find this optimal adversary (i.e.,
the optimal adversarial state perturbation), ATLA (Zhang et al., 2021) proposes to train an adversary
whose action space is the perturbation set in the state space, while PA-AD (Sun et al., 2021) further
decouples the problem of finding state perturbations into finding policy perturbations plus finding the
state that achieves the lowest value policy, thus addressing the challenge of large state space.
F.2 Robust RL
Distributionally Robust RL. Nilim & El Ghaoui (2005) and Iyengar (2005) consider the problem
of distributionally robust RL, where there is normally an uncertainty set with prior information
regarding the distribution of the uncertain parameters. Iyengar (2005) directly put constraints on
environmental dynamics and reward functions by assuming that they take values in an uncertainty set.
Another line of research assumes the environmental dynamics and reward function as the uncertain
parameters and are sampled from an uncertain distribution in the uncertain set. The uncertainty set is
in general state-wise independent, i.e., “s-rectangularity” in Nilim & El Ghaoui (2005). Both types
aim to derive a policy by solving a max-min problem, with the former taking the minimum directly
over the parameters, while the latter taking the minimum over the distribution.
Empirically Robust RL against Evasion Attacks. We have briefly introduced related work on
empirically robust RL in Section 7 in the main paper. We emphasize two main differences between
the contributions of these works and our CROP. First, most of these robust RL methods only provide
empirical robustness against perturbed state inputs during test time, but cannot provide theoretical
guarantees for the performance of the trained models under any bounded perturbations; while our
CROP framework can provide practically computable certified robustness w.r.t. two robustness
certification criteria (per-state action stability and cumulative reward lower bound), i.e., for a given
RL algorithm, we can compute certifications for it that indicate its robustness. (We will discuss
a few more related work that can provide certified robustness in Appendix F.3.) Second, most of
these Robust RL methods focus on only the per-state decision, while we additionally consider the
certification of trajectories to obtain the lower bound of cumulative rewards. Below, we provide a
more comprehensive review of the categories of RL methods that demonstrate empirical robustness
against evasion attacks.
Randomization methods (Tobin et al., 2017; Akkaya et al., 2019) were first proposed to encourage
exploration. This type of method was later systematically studied for its potential to improve model
robustness. NoisyNet (Fortunato et al., 2017) adds parametric noise to the network’s weight during
training, providing better resilience to both training-time and test-time attacks (Behzadan & Munir,
2017b; 2018), also reducing the transferability of adversarial examples, and enabling quicker recovery
with fewer number of transitions during phase transition.
Under the adversarial training framework, Kos & Song (2017) and Behzadan & Munir (2017b) show
that re-training with random noise and FGSM perturbations increases the resilience against adversarial
examples. Pattanaik et al. (2018) leverage attacks using an engineered loss function specifically
designed for RL to significant increase the robustness to parameter variations. RS-DQN (Fischer
et al., 2019) is an imitation learning based approach that trains a robust student-DQN in parallel with
a standard DQN in order to incorporate the constrains such as SOTA adversarial defenses (Madry
et al., 2017; Mirman et al., 2018).
SA-DQN (Zhang et al., 2020) is a regularization based method that adds regularizers to the training
loss function to encourage the top-1 action to stay unchanged under perturbation.
Built on top of the neural network verification algorithms (Gowal et al., 2018; Weng et al., 2018),
Radial-RL (Oikarinen et al., 2020) proposes to minimize an adversarial loss function that incorporates
the upper bound of the perturbed loss, computed using certified bounds from verification algorithms.
CARRL (Everett et al., 2021) aims to compute the lower bounds of action-values under potential
perturbation and select actions according to the worst-case bound, but it relies on linear bounds (Weng
et al., 2018) and is only suitable for low-dimensional environments.
33
Published as a conference paper at ICLR 2022
F.3 Robustness Certification for RL
Despite the abundant literature in robustness certification in supervised learning, there is almost no
work on robustness certification for RL, given the unclear criteria and the intrinsic difficulty of the
task. In this part, we first introduce another two works that can achieve robustness certification at
state-level, and then another concurrent work Kumar et al. (2021) which also aims to provide provable
robustness regarding the cumulative reward for RL.
Robustness Certification on State Level. Fischer et al. (2019) and Zhang et al. (2020) have
discussed the state level robustness certification as well, but the way they obtain the certification
is different from our CROP. Concretely, we achieve robustness by probabilistic certification via
randomized smoothing (Cohen et al., 2019), while Fischer et al. (2019) and Zhang et al. (2020)
directly achieve robustness by deterministic certification via leveraging the neural network verification
techniques (Zhang et al., 2018; Xu et al., 2020; Mirman et al., 2018).
Despite these works on state-level robustness certification for RL, we are the first to provide the
certification of cumulative rewards. We emphasize that the certification of lower bound of the
cumulative reward in RL is more challenging than the per-state certification considering the dynamic
nature of RL. Our main contribution of the paper indeed mainly focuses on providing an efficient
adaptive search based algorithm CROP-LoRe to certify the lower bound of the cumulative reward
together with rigorous analysis of the certification.
Robustness Certification for Cumulative Rewards. We compare our robustness certification for
cumulative rewards with another concurrent work Kumar et al. (2021).
In terms of the threat model, both Kumar et al. (2021) and CROP consider the type of adversary
that can perturb the state observations of the agent during test time. In our CROP, we consider a
perturbation budget per time step following previous works (Huang et al., 2017; Behzadan & Munir,
2017b; Kos & Song, 2017; Pattanaik et al., 2018; Zhang et al., 2020), while they assume a budget for
the entire episode. The two perspectives are closely related.
Regarding the certification criteria, we certify both per-state action stability and cumulative reward
lower bound. Although they also consider the cumulative reward in their certification goal, they
formulate the certification as a classification problem—certifying whether the cumulative reward is
above a threshold or not, in contrast to directly certifying the lower bound as in our case.
For the certification technique, both works are developed based on randomized smoothing proposed
in supervised learning (Cohen et al., 2019). We propose a global smoothing technique (CROP-GRe),
as well as an adaptive search algorithm coupled with local smoothing (CROP-LoRe) to achieve the
certification of cumulative reward; while they propose an adaptive version of the Neyman-Pearson
lemma, which is similar with our global smoothing. Among the three algorithms (CROP-GRe,
CROP-LoRe, and Kumar et al. (2020)), CROP-GRe cannot defend against an adaptive adversary
(as concretely stated in Appendix C) while the other two can; specifically, CROP-LoRe is much
more sophisticated and tight than the global smoothing ones, which is an important contribution in
our work.
34