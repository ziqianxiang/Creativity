Published as a conference paper at ICLR 2022
Pessimistic Model-based Offline Reinforce-
ment Learning under Partial Coverage
Masatoshi Uehara, Wen Sun
Department of Computer Science
Cornell University, Ithaca, NY 14850, USA
{mu223,ws455}@cornell.edu
Ab stract
We study model-based offline Reinforcement Learning with general function ap-
proximation without a full coverage assumption on the offline data distribution.
We present an algorithm named Constrained Pessimistic Policy Optimization
(CPPO) which leverages a general function class and uses a constraint over the
model class to encode pessimism. Under the assumption that the ground truth
model belongs to our function class (i.e., realizability in the function class), CPPO
has a PAC guarantee with offline data only providing partial coverage, i.e., it can
learn a policy that competes against any policy that is covered by the offline data.
We then demonstrate the flexibility of CPPO: it can be seamlessly applied to many
specialized Markov Decision Processes where additional structural assumptions
further refine the concept of partial coverage. Two notable examples are: (1) low-
rank MDP with representation learning where the partial coverage condition is
defined using a relative condition number measured by the unknown ground truth
feature representation; (2) factored MDP where the partial coverage condition is
defined using density ratios associated with individual factors.
1	Introduction
Offline Reinforcement Learning (RL) is one of the important areas of RL where the learner is pre-
sented with a static dataset consisting of transition-related information (state, action, reward, and
next state) collected by some behavior policy, and needs to learn purely from the offline data without
any future online interaction with the environment. Offline RL is used in a number of applications
where online random experimentation is costly or dangerous such as health care (Kosorok & Laber,
2019), digital marketing (Chen et al., 2019) and robotics (Levine et al., 2020).
The performance guarantees of offline RL often rely on two quantities: the coverage of the offline
data and the property of the function approximation used in the algorithms. For instance, for the
classic Fitted-Q-iteration (FQI) algorithm (Ernst et al., 2005; Munos & Szepesvari, 2008), it requires
(a) full coverage in the offline data, i.e., max(s,a)dπ(s, a)∕ρ(s, a) < ∞ for any stochastic policies
π including history-dependent non-Markovian policies, where dπ (s, a) is a state-action occupancy
distribution of a policy π and ρ(s, a) is an offline distribution, (b) realizability in a Q function
class, i.e., the optimal Q function belongs to the function class, and (c) Bellman completeness, i.e.,
applying the Bellman operator on any function in the function class results in a new function that
also belongs to the function class (see the first row in Table 1). Among these three assumptions,
the full coverage and the Bellman completeness are particularly strong. The full coverage means
that the behavior policy needs to be exploratory enough, although figuring out an exploratory policy
itself is an extremely hard problem for large-scale MDPs. The Bellman completeness assumption
does not have a monotonic property, i.e., even starting with a function class that originally permits
Bellman completeness, slightly increasing the capacity of the function class could result in a new
class that does not have Bellman completeness anymore. Thus, we aim to relax the assumptions on
the offline data and the function class. Particularly, we are interested in the following question:
Given a realizable function class and an offline distribution that only provides
partial coverage, can we learn a policy that is able to compete with any policy that
is covered by the offline distribution?
We study this question from a model-based learning perspective and provide an affirmative answer
to the question. More specifically, different from FQI, we start with a realizable model class, i.e.,
the ground truth transition falls into the model class. We further abandon the strong full coverage
assumption, and instead, assume partial coverage which means the offline data distribution only
covers a state-action distribution of some high-quality comparator policy π* (π* is not necessarily
1
Published as a conference paper at ICLR 2022
Methods	TyPe	Coverage	Additional Structures
FQI (Munos & Szepesvari, 2008)	F	Full: maxs,a ；(:六;< ∞, ∀∏	Bellman complete
Minimax Way (Uehara et al., 2020)	Fp-	Full: maxs,a £(：。< ∞, Vn	Realizability in density ratio
DUan et al.(2020)	一	F	Full: Es,a~ρφ(S, a)φ(s, a) > is PSD	Linear Bellman complete
Xie & Jiang (2020)	F	Full: maxs,a,so P P(s0s，a) < ∞	None
Liu et al. (2020)	F	Partial，： maxsa d Js,a) < ∞ 	s,a *ρ(s,a)	、		Bellman / Policy class complete
Rashidinejad et al. (2021)	F	Partial: msxsa d 产。)< ∞ 	s,a ρ(s,a)	、	I		Tabular MDP
Jin et al. (2020b); Zhang et al. (2021b)	F	n ,∙ 1 + +	X >Esa~d∏* φ(s,a)(φ(s,a)) > X / Partial，，： maxχ —>S,a d 人,~τrτ7~τττ— ‹ ∞ 	X X ' Es,a~ρφ(s,α)(φ(s,α)) ' X		Linear MDP (Jin et al., 2020a)
Xie et al. (2021)	F	^"d^zt^	f-Tf∣d∏*' , 	 Partial: maxf f-T琲 < ∞	Bellman complete
Zanette etal. (2021)	F	,	XE	,∏* φ(s,a)(φ(s,a)) X Partial: max^ —'蓝	J	、：'— < ∞ 	X X ' Es,a~ρφ(s,a)(φ(s,a)) ' X		Linear Bellman complete
Batch (Ross & Bagnell, 2012)	Bi-	Full: maxs,a d(ssOa) < ∞, Vn	None
Milo (Chang et al., 2021)	B	x> ,∙ 1	X ' Esa~d∏* φ(s,a)(φ(s,a)) ' X / Partial: max? -TIa d 口~τ7-∑7~ττr— < ∞ X X ' Es,a~ρφ(s,α)(φ(s,α)) ' X	KNR/GP
CPPO (Ours)	B	Partial+++: maxs,。"ρ(V < ∞ D «- 1	XTEsa~d∏* φ(s,a)(φ(s,a))TX Partial: max^ —'Ia	口~~w— < ∞ X X ' Es,a~ρφ(s,a)(φ(s,a)) ' X D «. I	XTEsa~d∏* φ*(s,a)(φ*(s,a))TX Partial: max^ —Ts,a d 从*,_*-r∏~VTT- < ∞ 	X X ' Es,a~ρφ*(s,a)(φ*(s,a)) ' X		None Linear MDP /KNR / GP Low-rank MDP (unknown φ*)
Table 1: Comparison among existing works regarding their type, coverage, and additional structural
assumptions on the function class or MDPs. Type F means model-free and type B means model-
based. Partial coverage means 2that the offline distribution ρ covers a state-action distribution of
a comparator policy ∏*. f means it assumes an accurate density estimator for ρ(s, a). ff means
although the analysis in Jin et al. (2020a) is done under the full coverage for linear MDPs, based on
the argument (Zhang et al., 2021b), we can show the algorithm has the PAC guarantee under partial
coverage in terms of the relative condition number for linear MDPs. fff means that we can refine
it to a more adaptive quantity using the model class (i.e., Definition 1). All the methods in the table
require realizability in the function class.
the optimal policy, and π* could be non-Markovian), i.e., maxs,。dπ* (s, a)∕ρ(s, a) < ∞, We design
an algorithm — Constrained Pessimistic Policy Optimization (CPPO), which can learn a policy that
is as good as any comparator policy ∏* that is covered by the offline data. The fact that CPPO can
learn to compete against history-dependent policies is meaningful in offline RL when the offline
data does not cover the optimal policy.
While one could assume density ratio based concentrability coefficient (maxs,。dπ* (s, a)∕ρ(s, a))
to be under control for small size MDPs, in large-scale MDPs (e.g. continuous state space), the den-
sity ratio could quickly become an extremely large quantity which makes the performance guarantee
vacuous. When applying CPPO to MDPs with additional structural assumptions, we can seamlessly
refine the density ratio based concentrability coefficient to more natural and tighter quantities. No-
tably, we consider the offline representation learning setting where the underlying MDPs permit a
low-rank structure (unlikely linear MDPs (Jin et al., 2020a; Yang & Wang, 2020), we do not assume
the ground truth state-action feature representation φ? is known, and instead we need to learn φ?)
and we show that we can refine the density ratio to a relative condition number that is defined us-
ing the unknown true state-action feature representation φ? . Intuitively this means that as long as
there exists a high-quality comparator policy that only visits the subspace (defined using the true
representation φ) that is covered by the offline data, CPPO can compete against such a policy, even
without knowing the true φ? . Such bounded relative condition number assumption is much weaker
than the bounded density ratio assumption.3 While the concept of relative condition number was
originally introduced in the online RL setting (e.g., Agarwal et al. (2020c;a) with a known linear
feature φ), and later was introduced in offline RL (Zhang et al. (2021b); Chang et al. (2021)), these
prior works all rely on the fact that the feature representation φ is known to the learner a priori
(see Table 1 for the comparison). Another interesting example is factored MDPs (Kearns & Koller,
1999) where we show CPPO refines the density ratios to be density ratio associated with individual
factors, which leverages the factored structure and is provably tighter. We also give examples on
linear MDPs (Yang & Wang, 2020), kernelized nonlinear regulator (KNRs) (Kakade et al., 2020),
where we again show that CPPO enjoys problem specific quantities for measuring the coverage.
Our contributions. Our contributions are two folds, which we summarize below:
3Strictly speaking, in Jin et al. (2020b); Rashidinejad et al. (2021), a comparator policy is restricted to the
optimal policy. In Chang et al. (2021); Zanette et al. (2021); Xie et al. (2021) and CPPO, a comparator policy
can be any policy.
2
Published as a conference paper at ICLR 2022
1.	We show that in the model-based setting, realizability and partial coverage is enough to
learn a high-quality comparator policy (Theorem 1). Notably, (1) this result holds for any
MDPs with realizable model classes, (2) we can compete against even history-dependent
policies. This is in sharp contrast to the state-of-art provable model-free offline RL results:
see Table 1 on page 2 for detailed comparisons to prior works.
2.	Under additional structural assumptions (e.g., KNRs, linear MDPs (Yang & Wang, 2020),
linear mixture MDPs (Ayoub et al., 2020), low-rank MDPs, factored MDPs), we show that
we can seamlessly refine the density ratio based concentrability coefficients to problem spe-
cific quantities. This flexibility to adapt to problem specific coverage measuring quantities
is in sharp contrast to other model-free offline RL algorithms such as minimax based ap-
proaches (Uehara et al., 2020) which, to the best of our knowledge, cannot leverage MDP’s
structures (e.g., linear MDPs) to refine its density ratio based concentrability coefficients.
While we focus on the model-based setting and have demonstrated advantages of our approach over
model-free ones (i.e., no more Bellman completeness assumption on function classes, being able to
compete against a larger pool of policies, and the ability to seamlessly adapt to problem dependent
structures), it is worth noting that realizability in the model-based setting is usually considered
stronger than the one in the model-free setting. On the empirical side, model-based offline RL
algorithms are the state-of-art (e.g., Yu et al. (2020); Kidambi et al. (2020); Matsushima et al. (2020);
Cang et al. (2021); Chang et al. (2021)). Our theoretical results provide a sharp contrast between
model-based and model-free approaches in offline RL.
2	Related work
We discuss two families of related works of offline RL. In Appendix C, we discuss related works
about representation learning in RL.
Insufficient coverage of the dataset due to the lack of online exploration is known as the main
challenge in offline RL (Wang et al., 2020). To deal with this problem, a number of methods have
been recently proposed from both model-free (Wu et al., 2019; Touati et al., 2020; Kumar et al.,
2020; Liu et al., 2020; Rezaeifar et al., 2021; Fujimoto et al., 2019; Fakoor et al., 2021; Ghasemipour
et al., 2021; Buckman et al., 2020) and model-based perspectives (Yu et al., 2020; Kidambi et al.,
2020; Matsushima et al., 2020; Yin et al., 2021). More or less, their methods rely on the idea
of pessimism and its variants in the sense that the learned policy can avoid uncertain regions not
covered by offline data. As a theoretical side, Munos & Szepesvari (2008); Duan et al. (2020;
2021); Fan et al. (2020) proved FQI has a PAC (probably approximately correct) guarantee under
realizability, the global coverage, and Bellman completeness conditions. Other offline model-free
RL methods such as minimax offline RL methods also require realizability and the global coverage
(Chen & Jiang, 2019; Antos et al., 2008; Uehara et al., 2021; Duan et al., 2021; Zhang et al., 2020;
Nachum et al., 2019). Recently, by leveraging the aforementioned the pessimism idea, Jin et al.
(2020a); Rajaraman et al. (2020) showed that pessimistic FQI can be applied to partial coverage
setting for linear and tabular MDPs. Comparing to their works, our analysis focuses on model-
based approaches with general function approximation. The offline model-based method is known
to have a PAC guarantee under the realizability and the global coverage (Ross & Bagnell, 2012;
Chen & Jiang, 2019). As the most closely related work, Chang et al. (2021) proved a model-based
method with an additional penalty term can weaken the assumption from the global coverage to the
partial coverage for structured MDPs such as KNRs and Gaussian Processes models (Deisenroth &
Rasmussen, 2011). In this work, we consider arbitrary MDPs with a realizable model class and aim
for PAC bounds under a partial coverage condition.
3	Preliminaries
We consider a Markov Decision process (MDP) M = {S, A, P, γ, r, d0} where P : S × A → ∆(S)
is the transition, r : S × A → [0, 1] is the reward function, γ ∈ [0, 1) is the discount factor, and
d0 ∈ ∆(S) is the initial state distribution. A policy π maps from state (or history) to distribution
over actions. Given a policy π and a transition distribution P, VPπ denotes the expected cumulative
reward of π under P, d0 and r. Similarly, QπP : S × A → R, AπP : S × A → R are a Q-
function and advantage-function under P and π. Given a transition P, we denote π(P ) as the
optimal policy associated with model P under reward r. We also denote dπP ⊂ ∆(S × A) as the
average state-action distribution of π under the transition model P, i.e, dπP = (1 - γ) Pt∞=0 γtdπP,t,
3
Published as a conference paper at ICLR 2022
where dπP,t ∈ ∆(S × A) is the distribution of (st, at) under π and P at a time-step t. We denote the
true transition distribution as P?, which we do not know in advance. For simplicity, we suppose r
is known. The extension to the unknown reward is straightforward.
In the offline RL setting, we have an offline distribution ρ ∈ ∆(S × A), and an offline dataset
D = {s(i), a(i), r(i), s0(i)}in=1 which is sampled in the following way: s, a 〜ρ,r = r(s, a), s0 〜
P?(∙∣s, a). We hope to obtain π(P?) = argmax∏ V∏π>? from this offline dataset without any further
interaction with the environment. We often denote ED [f (s, a, s0)] = 1/n P(s,a,s0)∈D f(s, a, s0).
Our goal is to construct an offline RL algorithm Alg, which maps from D to π so that the subopti-
mality gap V∏↑ - VAlg(D) for any comparator policy π* ∈ Π is minimized, where Π in this work
can be an unrestricted policy class (e.g., including non-Markovian policies). Hereafter, c,c1,c2,…
are always universal constants.
Partial coverage. Throughout this work, we do not assume ρ has global coverage. The global cov-
erage in this work means that the density ratio based concentrability coefficient dπp? (s, a)∕ρ(s, a)
is upper-bounded by some constant C ∈ R+ for all polices π ∈ Π , or the feature covariance matrix
corresponding to the offline distribution Es,a 〜ρφ(s,a)φ(s,a)> (φ ∈S×A→ R is a feature
representation) is full rank and has a non-zero minimum eigenvalue, which are commonly used
assumptions in offline RL (Munos, 2005; Antos et al., 2008; Chen & Jiang, 2019; Duan et al., 2020).
Under the full coverage, they show the output policy can compete with the globally optimal policy
π(P ?). However, this assumption may not be true in practice as computing an exploratory policy
itself is a challenging task for large-scale RL problems. Instead, we are interested in the partial
*，、，，、
coverage setting such as dπP ? (s, a)∕ρ(s, a) ≤ C, which means the state-action occupancy measure
under some comparator policy ∏* is covered by the offline dataset. We want to design an algorithm
that can compete against any policy ∏* that is covered by the offline data. This assumption is much
weaker than the global coverage.
4	Pessimistic Model-based Offline RL
We first introduce a general model-based algorithm that has a PAC guarantee of the suboptimality
gap under partial coverage defined with a newly introduced concentrability coefficient. The algo-
rithm takes a realizable model class as input and outputs a policy that is as good as any comparator
policy that is covered by the offline data in the sense of the bounded concentrability coefficient.
Our algorithm, Constrained Pessimistic Policy Optimization (CPPO) (Algorithm 1), takes a realiz-
able hypothesis class M (with P? ∈ M) consisting of |M| candidate models as input, computes
the maximum likelihood estimator (MLE) PbMLE using the given offline data D = {s, a, s0}. It then
forms a min-max objective subject to a constraint. The min-max objective introduces pessimism via
searching for the least favorable model P (in terms of its policy’s value VPπ) that is feasible with re-
spect to the constraint. We can also express the constrained optimization procedure using a version
space MD and a policy optimization procedure defined below:
max min VPπ ,
π∈Π P ∈MD P
whereMD = PP | P ∈ M,ED [τV(JbMLE(∙∣s,a),P(∙∣s,a))2] ≤ ξ},
(1)
where TV(P1, P2) is a total variation (TV) distance between two distributions P1 and P2. The
version space MD contains models that are not far away from PMLE in terms of the average TV
distance under D. The version space is constructed such that with high probability P? ∈ MD .
Below we state the algorithm’s performance guarantee. Assuming for now that P? ∈ MD holds
with high probability, then, Vπ := minP ∈MD VPπ is a pessimistic policy evaluation estimator, which
satisfies Vπ ≤ VPπ? for all π ∈ Π. Using the idea of pessimism, we have the following observation:
*	***	**	**
vpπ? -	vp^?	= v∏?	-	Vπ	+ Vπ	-	vp^?	≤	vpπ?	- Vπ	+	Vπ	- v∏?	≤	V∏?	-	Vπ	,
where the first inequality uses π = arg max∏∈∏ Vπ and the second inequality uses Vπ ≤ V∏? for
all ∏ ∈ Π. Thus, the final error only incurs the policy evaluation error for the comparator policy ∏*,
which leads to the error only depending on the concentrability coefficient for the comparator policy.
We define the following new concentrability coefficient that uses the model class M :
4
Published as a conference paper at ICLR 2022
Algorithm 1 Constrained Pessimistic Policy Optimization (CPPO)
1:	Require: Models M, dataset D, parameter ξ, policy class Π (note Π could be unrestricted)
2:	Obtain the estimator PMLE by MLE: PMLE = argmaxp∈m ED[lnP(s0 | s, a)].
3:	Constrained policy optimization:
∏ = arg maxπ∈∏ minp∈m V∏,
4:	Return π
s.t., ED [τV(pMLE(∙∣s,a),P(∙∣s,a))2]
≤ ξ.
Definition 1 (Model-based Concentrability Coefficient). For a comparator policy π* *, we define the
concentrability coefficient C∏* as follows:
t _	%a)~dP? [TV(P(|s，a)，P?G|s，a))2]
C∏* = supp0∈M E(s,a)~ρ[TV(P0(∙∣s,a),P *(∙∣s,a))2]	.
The following theorem shows CPPO learns a policy that competes against π* when C∏* < ∞.
Theorem 1 (PAC Bound for CPPO with general function class). Assume P? ∈ M. We set ξ =
CIln(C2Ml∕δ). Then, with probability 1 一 δ, for any comparator policy π* ∈ Π (Π can be the
unrestricted policy class containing non-Markovian policies),
vr∏: - vp* ≤ C3(i - Y)-2q Cπ* ln(TM/∑.
To the best of our knowledge, this is the first algorithm that achieves a PAC guarantee for any
MDPs under the partial coverage assumption Cπt* < ∞ with only a realizable hypothesis class.
We emphasize that the inequality in the above uniformly holds for all policies with probability
1 - δ including history-dependent non-Markovian policies (see Remark 2). Note that the ability
to compete against non-Markovian policies in offline RL is meaningful when the offline data does
not cover the optimal policy π? (i.e., there could be a high-quality history-dependent policy that
is covered by the offline data against which we want to compete). In model-free approaches, this
type of result generally cannot be obtained. Indeed, the model-free approach from Xie et al. (2021)
requires Π to be a restricted Markovian policy class, since their bound contains poly(ln(∣∏∣))
dependence. For the detailed discussion, refer to Remark 1.
The quantity Cπt * adaptively captures the discrepancy between the offline data and the state-action
occupancy measure under a comparator policy ∏ depending on the model class M. For example,
Cπt * can be reduced to a relative condition number in KNRs. Besides, it is always upper bounded by
the density ratio based concentrability coefficient:
*
dp	.一 Bin	dP* (s，a)
Cπ*,∞ := sup(s,a) ρ(s,a).
One extreme case is that functions in M are all the same, which implies Cπt* = 1 regardless.
Theorem 1 consider the case where the hypothesis class M is finite. When the hypothesis class is
infinite, we can still obtain the PAC guarantee by utilizing the generalized result in Section A for any
realizable model class with valid statistical complexity (e.g., localized Rademacher complexity).
Prior works that achieve PAC guarantees with only realizable model classes rely on much stronger
global coverage sup∏ C∏,∞ < ∞ (Chen & Jiang, 2019). Even when the comparator policy is
the optimal policy ∏(P?), the partial coverage condition Cn(P*),∞ < ∞ is weaker. Existing
pessimistic model-based algorithms and their theoretical results (Chang et al., 2021) often assume
that a point-wise model uncertainty measure is given as a by-product of model fitting, which limits
the applicability to special linear models such as KNRs/GPs. CPPO can work for any MDPs with
the realizable function class having a valid statistical complexity such that the MLE properly works.
Remark 1 (Comparison to the model-free approach from Xie et al. (2021); Zanette et al. (2021)
). Xie et al. (2021) study the model-free setting where the function class Q models Q functions
assumed to be Bellman complete for any Markovian policy in Π. While directly comparing model-
based approaches to model-free approaches is hard as they use different inductive biases in function
classes, we can leverage the approach from Chen & Jiang (2019, Corollary 6) to convert a model
5
Published as a conference paper at ICLR 2022
class M to a pair of Q and Π class. Specifically, we can convert a model class M to a pair of Q
class andΠ class such that Q will be realizable and also Bellman complete with respect to all π ∈ Π.
After such Conversionfrom the model-based setting to the model-free setting, running the algorithm
fromXie etal. (2021) using Q andΠ achieves VPn: — VPL = PC◊ ln(∣M∣∣Π∣∕n),∀π* ∈ Π, where
C is some concentrability coefficient. For the detailed derivation, we refer readers to Appendix
D. Since the suboptimality gap from such conversion incurs log ∣Π∣, a policy class Π cannot be too
large. Especially, unlike our results, it cannot take the unrestricted policy class as Π. This restriction
cannot be fixed even ifwe use natural policy gradient (NPG) algorithms unless models have special
structures (Xie et al., 2021; Zanette et al., 2021). The details are given in Section D.
Thus, our theorem indicates two advantages of model-based approaches: (1) realizability in function
class is enough to ensure a PAC guarantee under a partial coverage condition, (2) it can compete
against a larger pool of candidate policies including history-dependent non-Markovian policies,
which is a meaningful property when the offline data does not cover the globally optimal policy.
Next, we demonstrate another key advantage of our approach which is its flexibility to be seam-
lessly applied to MDPs with special structures.
5 Examples with Refined Concentrability Coefficients
In the previous section, our results apply to any MDP as long as its true transition belongs to a
function class M. In this section, we consider several concrete MDPs with additional structural
conditions. We show that by leveraging the additional structural conditions, we can refine the model-
based concentrability coefficient to more natural quantities. The examples that we discuss here are:
(1) linear mixture MDPs which generalize linear MDPs from Yang & Wang (2020) and tabular
MDPs, (2) KNRs which generalize LQRs, (3) low-rank MDPs, and (4) factored MDPs.
Before proceeding, we clarify CPPO cannot capture linear MDPs in Jin et al. (2020a) that is differ-
ent from the one (Yang & Wang, 2020) we use, and linear Bellman-complete MDPs (Duan et al.,
2020) without any modification since MLE-based model learning is no longer applicable to them.
However, other objective functions for learning models could be applied to these models (e.g., see
the nonparametric model-based learning approach from Lykouris et al. (2021); Neu & Pike-Burke
(2020) in the online setting), which we leave it as a future work.
5.1	Tabular MDPs and Linear Mixture MDPs
Tabular MDPs Tabular MDPs are MDPs where the state and action spaces are finite. Although the
corresponding hypothesis class for tabular MDPs is infinite, we can still run MLE, that is, estimating
P ? by the empirical distribution. Then, Algorithm 1 has the following guarantee.
Corollary 1 (PAC bound for tabular MDP). We set ξ = ci S |A| ln(nlslAlc2∕6. Then with proba-
bility 1 — δ, for all π* ∈ Π,
V∏ * — V∏ ≤ c3 (1 — γ)-2 1 q c∏*,∞lSl2lAl ln(n|S||A|c4/^7}
Here, for tabular MDPs with M = {P : P(∙∣s, a) ∈ ∆(S), ∀s, a}, the model-based concentrability
coefficient in Definition 1 is equal to the density ratio based concentrability coefficient Cπ*,∞ which
is the right quantity for small-size tabular MDPs.
Linear mixture MDPs We define linear mixture MDPs (Ayoub et al., 2020; Modi et al., 2020).
Definition 2 (Linear mixture MDPs). Given a feature vector ψ : (S, A, S) → Rd, a linear mixture
MDP is anMDP where the ground truth transition is P?(s0|s, a) := θ*>ψ(s, a, s0),θ? ∈ Rd.
By setting, ψ(s, a, s0) = μ(s0) N φ(s, a) (0 denotes the Kronecker product), linear mixture MDPs
include the following linear MDPs (Yang & Wang, 2020):
Definition 3 (Linear MDPs). LinearMDP has P?(s0|s, a) := Pd= i Pd= i Mjμi(s0)φj(s, a) with
μ : S → Rd1 and φ : S ×A→ Rd2 are known features, and M ? ⊂ Rd1 ×d2.
We use CPPO to learn on linear mixture MDPs. The corresponding M is
MMiX = {θ>ψ(s, a,s0) | θ ∈ Θ ⊂ Rd, R θ>ψ(s, a, s0)d(s0) = 1 ∀(s, a)}.
6
Published as a conference paper at ICLR 2022
Given a function V : S → R, define the state-action feature indexed by V as ψV (s, a) :=
ψ(s, a, s0)V (s0)d(s0), we have the following PAC guarantee.
Corollary 2 (PAC bound for linear mixture MDPs). Suppose infs,a,s0 P?(s0 | s, a) ≥ c3 > 0,
Θ = {θ : kθk2 ≤ R}, kψV (s, a)k2 ≤ 1,∀V ∈ S → [0, 1] and P? ∈ MMix. We set ξ =
cιdln2(c2nR∕δ)∕n. Then, with probability 1 一 δ,forany π* in Π (again Π can be the unrestricted
policy class), CPPO outputs a policy π such that:
VP： 一 Vp? ≤ C4(l - γ)-2 jmin(dCn*, d2C∏*,miχ)ln2(C5；R,	(2)
where the ConCentrabiIity coefficient C∏*,mix is defined as:
(X>∑n*.ψ _:x\
—>∑———)
x Σρ,ψV π* x
with the localized class ZP? := {P : E(s,@)〜ρ[TV(P(∙ | s,a),P?(• | s,a))2] ≤ ξ}, ∑pψ π* =
,	VP
E(s,a)〜p[ψVP∙* (S, a)ψVPπ* (S, a)>], and ς∏* ,ψV∏* = Es,a〜dp? [ψVPπ* (s, a)ψVPπ* (s, a)>].
When specializing to linear MDPs, the above bound still holds with C∏*,mix being replaced by the
relative condition number C∏*:
x Σπ* x
C∏* := sup —, where Σp = E(s,Ο)〜p[φ(s, a)φ(s, a)>], Σ∏* = E(Sa)〜d∏; [φ(s, a)φ(s, a)>].
x∈Rd x Σp, x	P?
This is the first PAC-guarantee result in the offline setting under partial coverage C∏*,mix < ∞ for
linear mixture MDPs. C∏*,mix is a newly-introduced concentrability coefficient for linear mixture
MDPs. This coefficient is measured on the integrated feature vectors φV (s, a) for V : S → [0, 1].
Note the class of V is localized, i.e., we consider state-value functions VPπ* (s) for all P centered
around P? under data distribution ρ (i.e., P ∈ ZP?). Such localization property ensures that
C∏*,mix ≤ Cn (see Lemma 10 in Section F).
Note that these relative condition number based quantifiers are always tighter than the density ratio
based concentrability coefficients (i.e., max{C∏*, C∏*,mix} ≤ C∏*,∞). For the special case where
φ(s, a) is a one-hot encoding vector, then they are reduced to the density ratio based concentrability
coefficient. In a non-tabular setting, even if when the density ratio is infinite, the relative condition
number can be still finite. Intuitively, the bounded relative condition number implies that the offline
data covers the subspace that the comparator policy ∏* visits.
We remark P?(s0 | s, a) ≥ c3 > 0 in Corollary 2 is a technical condition that allows us to calculate
the entropy integral of the hypothesis class easily. It can be potentially discarded by a more careful
argument following (van de Geer, 2000, Chapter 7). The norm assumption kψV (s, a)k2 < 1 is
commonly assumed in the online setting (Zhou et al., 2021).
5.2 Kernelized Nolinear Regulators
We consider the example of KNRs in this section. A kernelized Nonlinear Regulator (KNR) (Kakade
etal., 2020) is a model where the ground truth transition P ?(s0|s, a) is defined as s0 = W *φ(s, a)+e,
E 〜N(0, Z2I), with φ : S × A → Rd being a possibly nonlinear feature mapping. We denote
the corresponding model on W by P(W). We can apply Algorithm 1 and obtain its guarantee.
Especially, since TV(P(W)(∙ | s,a),P(W?)(• | s, a))2 = Θ(k(W 一 W*)φ(s, a)k2) (Devroye
et al., 2018), C∏* is upper-bounded by the relative condition number C∏*.
Then, we can also recover the result of Chang et al. (2021) which proposes a reward penalty-based
pessimistic offline RL algorithm. The detail is given in Section B. In summary, we can show
Vp: - Vp? ≤ Cι(1 - γ)-2 min(d1∕2,R)√RqdSCn*：n(I+n),.
where R := rank[Σp]{rank[Σp] + ln(c2∕δ)} and ds is the dimension of the state.
7
Published as a conference paper at ICLR 2022
This implies CPPO can learn a policy that can compete against ∏* with partial coverage C∏* < ∞.
Note that the condition C∏* < ∞ does not require Σρ to be full-rank. Also the bound uses rank[Σρ]
instead of d, which means that our bound is distribution dependent and is still valid even when
d = ∞ as long as the offline data only concentrate on a low-dimensional subspace.
5.3	Low-rank MDPs with Representation Learning
We consider the representation learning in offline RL. Following FLAMBE (Agarwal et al., 2020b),
we study low-rank MDPs but in the offline setting. Note that low-rank MDPs here are a more
generalized model of the aforementioned linear MDPs (Yang & Wang, 2020) since the true feature
representation φ? in a low-rank MDP is unknown.
Definition 4 (Low rank MDPs). The ground-truth model P? admits a low rank decomposition with
a dimension d if there exists two embedding functions μ* : S → Rd, φ* : S × A → Rd s.t.
P?(s0 | s,a) = μ*(s0)>φ*(s, a). Neither μ* nor φ* is known to the learner
One interesting special case ofa low-rank MDP is the following latent variable model (see Agarwal
et al. (2020b) for more details).
Definition 5 (Latent variable models). There exists a latent space Z along WithfUnctionS μ* : Z →
∆(S) and φ* : S × A → ∆(Z) s.t. P?(• | s,a) = Pz∈z μ*(∙ | z)φ*(z | s, a).
To tackle representation learning under partial coverage on low-rank MDPs, we setup function
classes as follows: given two function classes Ψ ⊂ S → Rd, Φ ⊂ S × A → Rd (both are re-
alizable in the sense that μ* ∈ Ψ and φ* ∈ Φ), We consider a hypothesis class {μ(s0)>φ(s, a); μ ∈
Ψ, φ ∈ Φ}. Then, CPPO (Algorithm 1) and Theorem 1 still work under this setting. Note that this
function class setup is exactly the same as the one from FLAMBE.
Here we show that by leveraging the low-rankness, we can refine the concentrability coefficient to
a relative condition number defined by the unknown true representation φ*. We emphasize that this
does not depend on the other features. Particularly, given a comparator policy ∏*, we define C∏*,φ?:
Cn* ,φ? =	SUP	>^^	,	ς∏*	:= Esa〜d∏*	φ* (S, a)φ* (S,。尸，	ςP= Es,a〜ρφ* (S,	a)φ* (S, a)ɪ .
x∈Rd x> Σρx	,	P?
We can show CPPO learns a policy that can compete against ∏* as long as C∏*,φ? < ∞.
Theorem 2 (PAC bound for low-rank MDP). We set ξ = cιln(lφllψlc2∕δ). Suppose (a):
∣∣φ(s, a)k2 ≤ 1,∀(s, a) ∈ S×A, ∀φ ∈ Φ, R μ(s0)>φ(s, a)d(s0) = 1 and R ∣∣μ(s)k2ds ≤ √d, ∀μ ∈
Ψ,φ ∈ Φ,(b) ρ(s, a) = dp?(s, a), (c) P?(s0|s, a) = μ*(s0)>φ*(s, a) for some μ* ∈ Ψ, φ* ∈ Φ.
With probability at least 1 一 δ ,for all π* ∈ Π (again Π can be an unrestricted policy class), CPPO
(Algorithm 1) finds π such that:
v∏π* - Vp? ≤ C3'C∏*,φ*ω∏*rank(Σρ严(ψ-φ)c⅞zδ), ω∏* = (max(s,0)：周：))	(3)
To the best of our knowledge, this is the first established PAC result under the partial coverage
condition C∏*,φ? < ∞, ω∏* < ∞ for low-rank MDPs in the offline setting. We also emphasize
that our bound in Theorem 2 is distribution dependent, i.e., it depends on rank(Σρ) rather than
the exact rank d. Note that rank(Σρ) ≤ d, and rank(Σρ) could be much smaller than d when the
offline distribution only concentrates on a low-dimensional subspace (defined using φ*). Note that
the assumption that ωπ* < ∞ does not imply the state-action density ratio Cπ* ,∞ is small. Indeed,
ωπ* < ∞ is much weaker than Cπ* ,∞ < ∞.
5.4	Factored MDPs
The last example we include is the factored MDP (Kearns & Koller, 1999) defined as follows:
Definition 6 (Factored MDPs). Let d ∈ N+ and O being a small finite set. The state space S = Od,
and for each state S, we denote s[i] ∈ O as the i -th variable of the state S. For each i ∈ [1,…，d],
the parents of i, Pai ⊂ [1,…,d], is the subset of state variables that directly influences i, i.e., the
transition is defined as follows:
∀S, a, S0 : P?(S0|S, a) = Qid=1Pi?(S0[i]|S[Pai], a).
We will denote Si = O|pai | , and given S ∈ S, we will have S[Pai] ∈ Si
8
Published as a conference paper at ICLR 2022
Due to the factorization, the transition operator P? can be described with L := Pid=1 |A||O|1+|pai|
many parameters. In contrast, the non-factored transition will need O(|O|d) parameters. When
|pai| d∀i, itis expected that we can learn this model with lower sample complexity by leveraging
the factorization which has been demonstrated in the online setting (Kearns & Koller, 1999). We
remark a factored MDP is an example where model-based approaches are necessary as neither the
optimal policy nor the Q functions are factored (Koller & Parr, 2000).
We will slightly modify Algorithm 1 to take the factorization into consideration. First, we perform
MLE for model learning: each factor Pi? is independently learned via MLE:
∖ / _ r n π	τπ n π/ /r -1 I r ι ∖ ι	A τ~r A
∀i ∈ [d], PMLE,i = arg maxP ED[lnP(s0[i]|s[pai],a)],	P = i PMLE,i.
Next, the constrained policy optimization procedure is defined as
∏ = arg max min	VPn,	SL	ED [TV(Pi(∙	|	s,a),pMLE,i(∙	| s,	a))2] ≤	ξi	(∀i	∈ [1,…，d]).
π P:= iPi
Note that in the above objective, there is no restriction on the policy, i.e., the arg max operator
searches over all possible policies including non-Markovian ones.
To analyze the performance of the above modified CPPO, we introduce a specialized con-
centration coefficient for factored MDPs that utilizes the factored structure. We focus on
density ratio based concentrability coefficients since in a factored MDP with the function class
M := {P = Qi Pi : Pi ∈ Si × A → ∆(O)}, the concentrability coefficient associated with
M in Definition 1 will be reduced to the density ratio. For any ∏*, We define the concentrability
coefficients for the factored MDP as follows:
___* ,	、
dπP ? (sj , a)
C∏* ∞ := max max	-c--^--,
,	j∈[i,…，d] Sj∈Sj,a∈A ρ(sj, a)
where for Sj ∈ Sj, we denote V (sj,a) := Es∈s^[pa ]=§? V (s, a) for any distribution V ∈ ∆(S×A).
Comparing to Cπ*,∞ defined on the original state space S, here Cπ*,∞ is defined over each state
space Sj associated with each factor j. Note that when |paj | = Θ(1), |Sj | is exponentially smaller
than |S|. One can verify that Cπ*,∞ ≤ Cπ*,∞ (see Appendix E.7), where Cπ*,∞ ignores the
factored structure and treat S as a whole single space. This formally demonstrates the benefit of the
factored structure in terms of the coverage condition in offline RL.
With the new definition of the concentrability coefficients, now we are ready to state the PAC bound
of CPPO for factored MDPs. Recall L := Pid=1Li,Li = |A||O|1+|pai|.
Theorem 3 (PAC bound for factored MDP). We set ξi = ciLiIn(Lic2d∕δ). Then with probability
1 一 δ, CPPOfinds a policy π such thatfor all comparator policy π* ∈ Π (Π can be unrestricted),
V∏: - V^? ≤ C3(1- Y)-2
IydC^
∞L∙ln(nLc4d∕δ)
n
Note that our sub-optimality gap scales polynomially with respect to L, i.e., the complexity of the
factored MDP, rather than |S| which can be Ω(exp(d)).
6 Conclusion
We study model-based offline RL with function approximation under partial coverage. We show that
for the model-based setting, realizability in function class and partial coverage together are enough
to learn a policy that is comparable to any policies (including history-dependent policies) covered by
the offline distribution. Our result demonstrates a sharp contrast to model-free offline RL approaches
which often require additional structural conditions in the function class (e.g., Bellman completion)
and have restrictions on the pool of candidate policies that they can compete against.
Some readers might wonder whether CPPO is computationally efficient. The minimax optimiza-
tion problem arg maxπ∈Π minP ∈M VPπ fits into a framework of planning on robust MDPs (Nilim
& El Ghaoui, 2005; Iyengar, 2005). By introducing a robust Bellman equation, they proposed value
iteration and policy iteration algorithms, and showed that algorithms are practically tractable in the
tabular setting. In the non-tabular setting, Lim & Autef (2019); Tamar et al. (2014) propose the ex-
tension using function approximation. Thus, we can apply their methods to approximately solve the
minimax optimization problem in a model-free fashion. We leave the formal theoretical justification
when using these approximation planning algorithms as an important direction for future work.
9
Published as a conference paper at ICLR 2022
Acknowledgement
The authors would like to thank Nan Jiang, Tengyang Xie for valuable feedback.
Masatoshi Ueharra is partially supported by Masason foundation.
References
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed ex-
ploration for provable policy gradient learning. In Advances in Neural Information Processing
Systems, volume 33, pp. 13399-13412, 2020a.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33, pp. 20095-20107, 2020b.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approxima-
tion with policy gradient methods in markov decision processes. In Proceedings of Thirty Third
Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp.
64-66, 2020c.
Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71:89-129, 2008.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463-474. PMLR, 2020.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020.
Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in offline rl. arXiv preprint
arXiv:2106.09119, 2021.
Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via offline data without great coverage. 2021.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 1042-
1051, 2019.
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi. Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on web search and data mining, WSDM ’19, pp. 456-464, 2019.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472. Citeseer, 2011.
Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.
Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear func-
tion approximation. In Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 2701-2709, 2020.
Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. arXiv preprint arXiv:2103.13883, 2021.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 2005.
10
Published as a conference paper at ICLR 2022
Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J Smola. Continuous doubly con-
strained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume
120 of Proceedings of Machine Learning Research ,pp. 486-489, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online rl. In International Conference
on Machine Learning, pp. 3682-3691. PMLR, 2021.
Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesvari, and Mengdi Wang. Sparse feature Selec-
tion makes batch reinforcement learning more sample efficient. In International Conference on
Machine Learning, pp. 4063-4073. PMLR, 2021.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257-280, 2005.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learn-
ing with linear function approximation. In Proceedings of Thirty Third Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2137-2143, 2020a.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? arXiv
preprint arXiv:2012.15085, 2020b.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. In Advances in Neural Information
Processing Systems, volume 33, pp. 15312-15325, 2020.
Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In IJCAI,
volume 16, pp. 740-747, 1999.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810-21823. Curran Associates, Inc., 2020.
Daphne Koller and Ronald Parr. Policy iteration for factored mdps. In Proceedings of the Sixteenth
conference on Uncertainty in artificial intelligence, pp. 326-334, 2000.
Michael R. Kosorok and Eric B. Laber. Precision medicine. 6:263-286, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Shiau Hong Lim and Arnaud Autef. Kernel-based reinforcement learning in robust markov decision
processes. In International Conference on Machine Learning, pp. 3973-3981. PMLR, 2019.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Advances in Neural Information Processing
Systems, volume 33, pp. 1264-1274, 2020.
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun. Corruption-robust exploration
in episodic reinforcement learning. In Conference on Learning Theory, pp. 3242-3245. PMLR,
2021.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. ICLR, 2020.
11
Published as a conference paper at ICLR 2022
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics,pp. 2010-2020. PMLR, 2020.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv: 2102.07035,
2021.
Remi Munos. Error bounds for approximate value iteration. In Proceedings ofthe National Confer-
ence on Artificial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2005.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020.
Chengzhuo Ni, Anru Zhang, Yaqi Duan, and Mengdi Wang. Learning good state and action repre-
sentations via tensor decomposition. arXiv preprint arXiv:2105.01136, 2021.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780-798, 2005.
Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021.
Nived Rajaraman, Lin F Yang, Jiantao Jiao, and Kannan Ramachandran. Toward the fundamental
limits of imitation learning. arXiv preprint arXiv:2009.05990, 2020.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Leonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021.
Stephane Ross and J Andrew Bagnell. Agnostic system identification for model-based reinforcement
learning. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 1905-1912, 2012.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of
Proceedings of Machine Learning Research, pp. 2898-2933, 2019.
Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In
International conference on machine learning, pp. 181-189. PMLR, 2014.
Ahmed Touati, Amy Zhang, Joelle Pineau, and Pascal Vincent. Stable policy optimization via off-
policy divergence regularization. arXiv preprint arXiv:2003.04108, 2020.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-
policy evaluation. In Proceedings of the 37th International Conference on Machine Learning, pp.
9659-9668, 2020.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and
first-order efficiency. arXiv preprint arXiv:2102.02981, 2021.
12
Published as a conference paper at ICLR 2022
S van de Geer. Empirical Processes in M-Estimation. Cambridge Series in Statistical and Proba-
bilistic Mathematics. Cambridge University Press, 2000.
Martin J Wainwright. High-Dimensional Statistics : A Non-Asymptotic Viewpoint. Cambridge
University Press, New York, 2019.
Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of offline rl
with linear function approximation?. arXiv preprint arXiv:2010.11895, 2020.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. arXiv preprint arXiv:2106.06926, 2021.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In Proceedings of the 37th International Conference on Machine Learning, pp.
10746-10756, 2020.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double
variance reduction. arXiv preprint arXiv:2102.01748, 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp.14129-14142, 2020.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic meth-
ods for offline reinforcement learning. arXiv preprint arXiv:2108.08812, 2021.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of
stationary values. In International Conference on Learning Representations, 2020.
Weitong Zhang, Jiafan He, Dongruo Zhou, Amy Zhang, and Quanquan Gu. Provably efficient
representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935,
2021a.
Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust offline reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021b.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. In Conference on Learning Theory, pp. 4532-
4576. PMLR, 2021.
13
Published as a conference paper at ICLR 2022
A Generalization of Theorem 1
We present the generalized version of Theorem 1 when the hypothesis class is infinite. We define
the modified function class of M:
H=(rP+2Pι IP ∈M).
Given a function class F, let N[] (δ, F, d) be the bracketing number of F w.r.t the metric d(a, b)
given by
d(a, b) = E(s,a)〜P
(a(s0 | s, a) - b(s0 | s, a))2d(s0)
1/2
Then, the entropy integral of F is given by
JB (δ, F, d) = max	(log N[] (u, F, d))1/2du, δ
∖∕δ2∕2
(4)
We also define the localized class of H:
H(δ) = {h ∈ H : E(s,a)〜ρ[h2(P(∙ | s, a)kP?(∙ | s, a))] ≤ δ2},
where h(P(∙ ∣ s,a)∣∣P?(• | s, a)) denotes Hellinger distance defined by
(0.5 Z{p(s0 ∣ s,a) - PP?(s0 ∣ s, a)}2d(s0))	.
Based on Theorem 7.4 (van de Geer, 2000), the MLE has the following guarantee.
Theorem 4 (MLE guarantee with general function approximation). We take a function G() :
[0,1] → R s.t. G(E) ≥ JB [e, H(e), d] and G(e)∕e2 is a non-increasing function w.r.t 匕 Then,
ietting ξn be a solution to √ne2 ≥ CG(E) w.r.t 匕 With probability 1 一 δ, we have
E(s,a)〜ρ[kPMLEd | s,a) - Pd | s, a)k2] ≤ c1 {ξn + Plog(C2∕δ"n} ∙
We remark that the original guarantee in (van de Geer, 2000) is given for the estimation of uncondi-
tional distributions. The adaption to the conditional case is straightforward. For more details, refer
to Section A.1. Besides, when we assume the convexity of the function class, the entropy integral
with bracketing (4) can be replaced with the entropy integral with covering number (Wainwright,
2019, Chapter 14).
By using the above notation and the MLE guarantee, we can generalize Theorem 1.
Theorem 5 (Finite sample error bound of CPPO with an infinite hypothesis class). Assume P? ∈
M. Let f (P)(s, a) = TV(P(∙ ∣ s, a), P?(• ∣ s, a))2. Define
Mi = {p : E(s,a)〜P [f (P)(s, a)] ≤ C (ξn + ln∣∕δ)) },
M2 = {p : E(s,a)〜D [f (P )(s, a)] ≤ C (G(MI) + 既 + ln∣∕") } ,
G(M1) = E[ sup |(ED-EP)[f(P)]|], G(M2) = E[ sup |(ED-EP)[f(P)]|].
P∈M1	P∈MD
Here, in G(M1) and G(M2), the expectation is taken over the data. We set ξ = CG(M1) + Cξn2 +
c (皿？6)). Then,for all π* ∈ Π, we have
VP： - Vp? ≤ (1 - Y)-2Ci√C∏? JG(M2) + G(Mi) + 蓝 + lnc/”.
n
14
Published as a conference paper at ICLR 2022
This theorem shows once we can calculate GM1 , GM2 and ξn , we can obtain the tight rate. Im-
portantly, GM1 and GM2 are upper-bounded by the localized versions of Rademacher complexities
based on symmetrization argument. Hence, their rates are faster than the ones of the nonlocalized
versions.
For example, when |M| is finite, We first have ξn = ,ln(∣M∣c∕δ)∕n. Then, from Bernstein's
inequality (Wainwright, 2019, Exercise 2.8) and union bound, G(M1) is upper-bounded by
G(Mi) .	ξn	× Pln(MIc∕δ)∕n . ξ2.
l{z}	|-------{------}
Variance term	Union bound
Similarly, from empirical Bernstein’s inequality,
G(M2) .	ξn	× Pln(MIc∕δ)∕n . ξn.
l{z}	|------{------}
Variance term	Union bound
Then, we can obtain the result of Theorem 1:
vp∏: - vp? ≤(i - Y)-2c1qcrrln(ιMιc∕δ),	∀∏* ∈ ∏.
We stress if we use Hoeffeding’s inequality above, we would immediately get the slower rate
O(n-1/4). To calculate G(M1) and G(M2) in a tight manner, we need to leverage the knowl-
edge that the variance of each element in M1 and M2 is controlled from the restriction P ∈ M1 or
P ∈ M2.
A. 1 Rate of Convergence of Maximum Likelihood Estimation with Infinite
Hypothesis Class
We aim for obtaining a PAC guarantee of MLE following (van de Geer, 2000). We explain how we
should modify the proof of van de Geer (2000) for unconditional density estimation to conditional
density estimation. For simplicity, we assume P? > 0.
We first introduce the notation:
P=(P + P*)∕2, gp = 0.5log J?, (M)1/2 = nppP ∣ P ∈ m}.
Recall
h2(Pi(∙ I s, a),P2(∙ I s, a))
0.5	P11/2 (s0 I s, a) - P1/2 (s0
I s, a)d(s0)
0.5
Here, from Lemma 4.2 (van de Geer, 2000), the following holds:
Lemma 1 (Some property of Hellinger distance).
E(s,a)〜ρ[h2(P1(∙ I s,a),P2(∙ I s,a))] ≤ 0.5E(s,a)〜ρ[h2(P1(∙ ∣ s,a),P2(∙ ∣ s,a))],
E(s,a)〜ρ[h2(P(∙ I s,a),P?(∙ I s,a))] ≤ E(s,a)〜ρ[16h2(P(∙ I s,a),P?(∙ I s,a))].
We also recall Hellinger distance is stronger than TV distance:
Lemma 2 (Relation of Hellinger distance and TV distance ).
TV(Pι(∙ I s,a),P2(∙ I s,a)) ≤ √2h(Pι(∙ I s,a),P2(∙ I s,a)).
The following lemma is useful to connect the log-loss and the Hellinger distance.
Lemma 3 (Basic Inequality for MLE).
E(s,a)〜ρ[h2(PMLEd i s,a),P?(∙ i s,a))] ≤ (ED - E(s,a)〜ρ,s0〜P*(.|s,a))[gP(S,a, SO)].
This is proved by Lemma 4.1 (van de Geer, 2000). To simplify the notation, we define
H2(P1,P2) = E(s,a)〜ρ[h2(P1(∙ ∣ s,a),P2(∙ I s,a))].
15
Published as a conference paper at ICLR 2022
Here, our goal is showing with probability 1 - δ,
H2(PMLE,P?) ≤ {ξn + √lθg(c2∕δ)∕n}2.
This is proved by showing for x ≥ ξn ,
P(H2 (PMLE,P?) ≥ X2) ≤ cexp(-nx2∕c2 ).
This corresponds to the statement in Theorem 7.4 (van de Geer, 2000). To prove the above, we first
use
P(H2(Pmle,P?) ≥ x2) ≤ P(16H2(PMLE,p?) ≥ X2),
from Lemma 1. Then, from Lemma 3, this is upper-bounded by
P(	sup	Vn(gp) - √nH2(P, P?) ≥ 0)	(5)
P ∈M,H2(P,P *)≥x2∕16
where Vn = √π(Ed - E(s,a)~p,s，~p?(∙∣s,a)). To prove the term 5 is less than Cexp(-nδ2∕c2),
we use Theorem 5.11 (van de Geer, 2000), that is, some uniform inequality based on entropy with
bracketing. The rest of the proof is the same as Theorem 7.4 (van de Geer, 2000). In summary, the
only difference is we use the distance H(a, b) tailored to the conditional density estimation instead
of unconditional density estimation.
B More details for KNRs
We explain the algorithm and present the PAC guaranteed for KNRs. Here, we denote the dimension
of S by dS .
We tailor Algorithm 1 to KNRs as follows to obtain a tighter guarantee. First, MLE procedure is
replaced with WMLE by regularized MLE:
WMLE = arg min ED [kW φ(s, a) - s0k22] + λkW k2F,
W ∈RdS ×d
where ∣∣ ∙ ∣∣f is a FrobeniUs norm. Then, the final policy optimization procedure is
∏ = argmaxπ∈∏ minw∈Wd VPn(W), s.t., WD = {W ∈ RdS×d : k(WMLE - W)(∑n)1∕2∣2 ≤ ξ}
where Σn = Pin=1 φ(si, ai)φ>(si, ai). We state the theoretical guarantee for KNRs below.
Corollary 3 (PAC bound for KNRs). Assume ∣φ(s, a)∣2 ≤ 1, ∀(s, a) ∈ S × A. We set
ξ = /2λ∣∣W*k2 +8Z2 (ds ln(5) +ln(1∕δ) + In),	In = ln (det(∑n)/ det(λI)).
Suppose the KNR model is well-specified. By letting ∣W?|2 = O(1),Z2 = O(1), λ = O(1), with
probability 1 一 δ,forall π*, we have
VP? - VP? ≤ CiH2 min(d1∕2, R)PR JdSCn*，PIn(I三,
where R := rank[Σρ]{rank[Σρ] + ln(c2∕δ)}.
The proof is deferred to Section E.5.
C More Related Works
We discuss literature related to representation learning in RL.
Representation learning for low-rank MDPs (ground truth feature representation is unknown) in
online learning is studied from a model-based perspective (Agarwal et al., 2020b) and model-free
perspective (Modi et al., 2021). In the online setting, Zhang et al. (2021a); Papini et al. (2021)
also study representation learning under different model assumptions. Comparing with these works,
since our setting is offline, the algorithm and analysis are totally different.
16
Published as a conference paper at ICLR 2022
In the offline setting, Ni et al. (2021) study dimensionality reduction in a given kernel space, and Hao
et al. (2021) study feature selection in sparse linear MDPs. Their focus is different as they do not
study PAC guarantees under partial coverage. Ni et al. (2021) assumes the transition operator can be
properly embedded into predefined Reproducing Kernel Hilbert Spaces and learns low-dimensional
state-action representations via kernelized embedding and low-rank tensor decomposition. However,
they did not study the errors for policy optimization after using these learned features. Regarding
offline distribution coverage, Ni et al. (2021) assumes that the feature covariance matrix (feature
associated with the pre-defined kernel) of the offline distribution is full rank. Hao et al. (2021) studies
an OPE problem on sparse linear Bellman complete MDPs in the offline learning setting where
they assume all covariance matrices (covariance matrices that correspond to all possible subsets of
features) under the offline distribution are full rank as well. We study policy optimization in low-
rank MDPs (with unknown feature representation), and we do not assume full coverage, i.e., we do
not assume the feature covariance matrix is full rank, and indeed our result is distribution-dependent
since it scales with respect to the rank of the covariance matrix that is defined using the ground truth
feature representation.
D Comparison to Xie et al. (2021)
We compare a result in (Xie et al., 2021) to our result in detail. Let F be a function class for Q-
functions. Here, we consider a more general version of their algorithm by replacing the original
E(f, π; D) in their algorithm with
E(f, ∏; D) ：= L(f,f； ∏, D) — mi* L(g,f； ∏, D).
g∈G
In their original algorithm, they set G = F. Here, we consider the version such that a discriminator
class G can be different from F.
They show the PAC result under partial coverage as follows. Here, TPπ? is a Bellman operator under
π and P ? :
T∏? : {S×A→ R} 3 f → r(s, a) + Ep*(s0∣s,a)[f (s0, ∏)] ∈{S×A→ R}.
Theorem 6 (Extension of Result in (Xie et al., 2021) ). Suppose realizaibility QπP ? ∈ F, ∀π ∈ Π
and closeness maxf∈f mi∏g∈g Es,a〜ρ[(g — T∏? f )2(s, a)] = 0, ∀π ∈ Π. Then, with 1 一 δ, for any
π* ∈ Π, we have
可：-Vp* = O(，CMn(∣Π∣∣F∣∣G∣∕δ)∕n),
C、	E(s,a)〜dp? [(f -Tf )2(s,a)]
=s∈F E(s,a)〜ρ[(f - Tf)2(s,a)]
By combining this result with the conversion from model-free results to model-based results in
(Chen & Jiang, 2019, Corollary 6), we can obtain the following result under partial coverage.
Theorem 7 (PAC guarantee from the direct application of (Xie et al., 2021) to mode-based RL ).
Assume P? ∈ M. Then, there exists an algorithm s.t. with 1 - δ, for any policy π? ∈ Π,
VP： - Vp* =ONCoin(∣Π∣∣M∣∕δ)∕n).
As We mentioned, this is worse than our result since it includes ∣Π∣. Besides, the algorithm can
only compete against policies restricted in Π, while our algorithm works for the unrestricted policy
class Π which could even include history dependent policies. For completeness, we give the proof
as follows.
We remark that their results (Theorem 4.1) with NPG that can possibly compete with any stochastic
policies, are not applicable here. This is because they need an assumption that the comparator policy
π* needs to satisfy Qp? ∈ F and maxf ∈f miηg∈g Es,a〜P [(g - T∏t f )2(s, a)] = 0, which does not
hold for the corresponding Q-function class F after the conversion. As a notable exception, when
the model is a linear Bellman-complete MDP (Zanette et al., 2021), any stochastic policies satisfy
the Bellman completeness for the linear Q-function class; then, their algorithms can learn policies
that can compete with any stochastic policies satisfying partial coverage.
17
Published as a conference paper at ICLR 2022
Proof of Theorem 7. Given a model class M, consider the following reduction. We define a Q-
function class:
F = {qPπ | π ∈ Π, P ∈ M}.
Then, we define a discriminator class G :
G = {TPπ00qPπ | π ∈ Π,π0 ∈ Π,P ∈ M,P0 ∈ M}.
The above satisfies the realizability QπP? ∈ F, ∀π ∈ Π and the closedness TPπ? F ⊂ G, ∀π ∈ Π.
Thus, the assumptions in Theorem 6 are satisfied. Then, we have
VP? - V∏? = O(PCMn(∣Π∣∣F∣∣G∣∕δ)∕n)
=O(PCMn(∣∏∣M∣∕δ)∕n),
noting |F| = ∣Π∣∣M∣ and |G| = ∣Π∣2∣M∣2.
□
E Missing Proofs
Below We Use c, c1, c2, •一to denote universal constants. For a d-dimensional vector a and a matrix
A ∈ Rd×d,
we denote kak2A = a>Aa. Here, a . B means a ≤ cB for some universal constant. c
E.1 Proofs for General Function Approximation
Proof of Theorem 1. From Lemma 6, the MLE guarantee gives us the following generalization
bound: with probability 1 - δ,
Es,a〜ρ[TV(PbMLE(∙ | s,a),P?(• | s,a))2].
ln(MI∕δ)
n
(6)
Letting
A(P) ：= ∣Es,a〜ρ[TV(P(∙ | s,a),P?(• | s,a))2] - ED[TV(P(∙ | s,a),P?(• | s,a))2]∣.
with probability 1 - δ, from union bound and Bernstein’s inequality, we also have
A(P) ≤
civar(s,a)〜ρ[TV(P(∙ | s, a),P?(• | s, a))2] ln(∣M∣∕δ) + c? ln(∣M∣∕δ)
n
n
, ∀P ∈ M.
(7)
Hereafter, we condition on the above two events. Recall that we construct the version space using
D and PMLE as follows:
MD := {p ∈M : ED [TV(P(∙ | s,a),PMLE (∙ | s,a))2] ≤ ξ}.
First Step: Show P? ∈ MD in high-probability. We set ξ = CIn(Ml∕δ). Conditioning on the
above two events equations (6) and (7), we prove P? ∈ MD . This is proved by
Ed[TV(Pmle(∙∣ s,a),P?(∙ I s,a))2]
=ED [TV(PMLE(• | s, a),P ?(∙ | s, a))2] - E(s,a)〜ρ[TV(PMLE(• 1 s, a),P? (∙ |
s, a))2]
+ E(s,a)〜ρ[TV(PMLE(。| s, a), P?(∙ | s,a))2]
=ED [TV(Pmle(∙ I s,a),P ?(∙ I s,a))2] - E(s,a)〜P [TV(Pmle(∙∣ s,a),P?(
•I
cιMM≡
n
.
.
-----^ , , 、 _ ,. .....................
var(s,a)〜p[TV(Pmle(∙ I s, a), P?(∙ I s,a))2]ln(MI∕δ) + ln(MI∕δ)
n
n
(From (7))
— .........-^ , , 、 _ ,. .....................
E(s,a)〜ρ[TV(PMLe(∙ 1 s,a),P?(• ] s,aOyilnQMDO + ln(MI∕δ)
n
n
^
(TV(PMle(∙ I s,a),P?(∙ I s,a))2 ≤ 4)
1ln(IMI∕δ).
n
(Plug in MLE guarantee)
18
Published as a conference paper at ICLR 2022
Second Step: Show Es,a〜ρ[TV(P(∙ | s,a),P?(• | s,a))2] ≤ cξ, ∀P ∈ MD in high probabil-
ity. We show for any P ∈ MD , the distance between P? is sufficiently controlled in terms of TV
distance. More concretely (conditioning on the above two events (6) and (7) ), we show
Es,a〜ρ[TV(P(∙ | s, a), P?(∙ | s, a))2] . ξ,	∀P ∈ Md.
In order to observe this, for any P ∈ MD , we have
ED[TV(P(∙ | s,a),P?(∙ | s,a))2]
≤ 2Ed[TV(Pmle(∙ | s,a),P(∙ | s,a))2]+2Eo[TV(Pmle(∙ | s,a),P?(∙ | s,a))2] ≤ 4ξ
(From (a + b)2 ≤ 2a2 + 2b2.)
Thus, we have:
Es,a〜ρ[TV(P(∙ | s,a),P?(∙ | s,a))2]
=Es,a〜ρ[TV(P(∙ | s,a),P?(∙ | s,a))2] - ED[TV(P(∙ | s,a),P?(∙ | s,a))2] + ED[TV(P(∙ | s,a),P?(∙ | s,a))2]
≤A(P)+cξ.
Here, from (7), we have
(8)
A(P) ≤
cιvar(s,a)〜ρ[TV(P(∙ | s,a),P*(∙ | s,a))2]]ln(∣M∣∕δ) + c ln(∣M∣∕δ) ∀P ∈ MD
n
n
Then, for any P ∈ MD , we have
A(P) ≤
≤r
CiE(s,a)〜ρ[TV(P(∙ | s,a),P*(∙ | s,a))4]ln(∣M∣∕δ) + c? ln(∣M∣∕δ)
n
4cιE(s,a)〜ρ[TV(P(∙ | s,a),P*(∙ | s,a))2]ln(∣M∣∕δ) +
n
n
C2 ln(∣M∣∕δ)
n
([TV(P(∙ | s,a),P?(∙ | s,a))2] ≤ 4.)
≤
4cι(A(P) + cξ)ln(∣M∣∕δ) + c ln(∣M∣∕δ)
n
n
From (a + b)2 ≤ 2a2 + 2b2,
A2 (P)
--------------------- ∖ 2
c(A(P) + ξ)ln(∣M∣∕δ) [ c ln(∣M∣∕δ)∖
n
.(A(P)+ ξ)ln(MI∕δ)
n
n
.(A(P)+ ξ)ln(∣M∣∕δ) + r Cln(∣M∣∕δ) 12
nn
(ξ includes ln(∣M∣∕δ))
.(A(P) + 1∕nln(∣M∣∕δ)) ln(∣M∣∕δ)
n
Then, we have
A2(P) - BιA(P) - B2 ≤ 0, Bi = Cln(∣M∣∕δ)∕n,	B2 = c(1∕n)2 ln(∣M∣∕δ)2.
This concludes
0 ≤ A(P) ≤ BI + 'B2 +4B2 ≤ c(Bi + PB2) ≤ cln≡≡ . ξ.
2n
Thus, by using the above A(P) . ξ(P ∈ MD) and (8), with probability 1 - δ, we have:
Es,a 〜ρ[TV(P(∙ | s,a),P ?(∙ | s,a))2] ≤ A(P)+ cξ . ξ, P ∈ Md.
Third Step: Calculate the final error bound taking the distribution shift into account For
any P ∈ MD , we prove
VP∏1 - V∏* ≤ (1 - γ)-2
-∕ln(∣M∣∕δ)
n
(9)
19
Published as a conference paper at ICLR 2022
For any P ∈ MD , this is proved as follows:
VP： - V∏t ≤ (1 - Y)-2E(s,a)~dP? [TV(P(∙ I s, a), P?(• I s, a))]
(Simulation lemma, Lemma 5)
≤ (1 - Y)-2,E(s,a)~dP? [TV(P(∙ I s,a),P?(∙ I s,a))2]
≤ (1 — Y)-2/C∏*E(s,a)~ρ[TV(P(∙ ∣ s,a),P?(∙ I s,a))2]
≤ c(1 - γ)-2,C∏* JIn(I从1/H. (Based on the consequence of the second step)
Combining all things together, with probability 1 - δ, for any ∏ ∈ Π, we have
：
：
VPn： - V∏?	≤ VP：
min
P∈MD
VPπ
+ min VPπ
P∈MD P
V^?
：
≤ VP?
min
P∈MD
VPπ
+ min V∏
P∈MD
V^?
：
≤ VP?
min
P∈MD
：
VPπ
. (1 - Y)-2c1
ln(MIc2∕δ)
n
(definition of ∏)
(Fist step, P? ∈ MD)
(From (9))
—
—
—
—
：
—
□
Remark 2 (To compete with all history-dependent polices). Consider the case where Π is all
Markovian polices. We want to show we can compete with all history-dependent non-Markovian
polices:
∏=I Yy ∏ I ∏i ∈ [(U s× a) → △(/)
.
：：
We take an element π* from Π. Then, V∏? and dP? are still well-defined. Then, every step in the
proof still holds. The only step we need to check carefully is this line:
：
V∏: - V∏? ≤ V∏:
min
P∈MD
：
VPπ +
min
P∈MD
VPπ
V^?
：
≤ VP?
min
P∈MD
：
VPπ +
min VP
P∈MD
V^?.
—
—
—
—
This is proved by max∏∈∏ V∏ = max∏∈∏ V∏ for any P.
E.2 Proofs for General function approximation with infinite hypothesis class
Proof of Theorem 5. From Theorem 4, the MLE guarantee gives us the following generalization
bound: with probability 1 - δ,
E(s,a)~ρ[TV(pMLE(∙ I s, a), P?(∙ I s, a))2] . ξ^2n + ln⅞∕δ)) .	(10)
We define
Mi = PP e M : E(s,a)~ρ [TV(P(∙ I s, a),P?(∙ I s, a))2] ≤ c^i2 + Jln^) }.
for some large c. From a functional Bernstein’s inequality (Lemma 12), by defining
f (P )(s,a)=TV(P (∙ I s,a),P*(∙ I s,a))2, G(Mi ) = E[ SUp I (Ed - Eρ)[f(P )]I].
P∈M1
20
Published as a conference paper at ICLR 2022
with probability 1 - δ, we have
sup I(Ed — Eρ)[f(P)]| . G(Mi) + {G(Mι)+ SUp Eρ[f(P尸]}1/2\/log(c1/J) + lθg(c1 /δ)
∈M1	P∈M1	n	n
.G(Mi) + {G(Mi)+ sup Eρ[f(P)]}1/2 JIOg(C10 十 log(c10
P∈M1
.G(Mi) + ξn + (ln≡).
Similarly, by defining
M2 = P : E(s,a)〜D[TV(P(∙ | s,a),P?(∙ | s,a))2] ≤ cG(Mi) + *
G(M2) = E[ sup ](Ed - Eρ)[f(P)]|],
P∈MD
Zn = SUP E(s,a)〜P [f (P )(s,a)].
P∈M2
from a functional Bernstein’s inequality, with probability 1 - δ, we have
sup |(Ed - Eρ)[f(P)]| . G(M2) + {G(M2) + Zn}1/2Jlog(c西+ W)
P∈M2	n	n
.g(m2)+r∑E(c应+ln且出
nn
Hereafter, we condition on the above three events:(10), (11) and (12).
First step: Show P * ∈ MD in high probability. From (10), We have
E(s,a)〜ρ[f (Pmle)(s, a)] ≤ ξn + ln(nδ).
Thus, P? ∈ Mi. Then, from (11) and (10),
E(s,a)〜D [f (PMLE) (s, a)]
=|E(s,a)〜D [f (PMLE)(S, a)] - E(s,a)〜ρ [f (PMLE)(s, a)]| + E(s,a)〜ρf (PMLE)(s, a)]
.G(Mi)+ξn+ln(c/δ).
nn
Thus, P? ∈ MD recalling the definition of MD (We set ξ = G(Mi) + ξn2 + ln(nδ)).
Second step: Show the upper bound of E(s,a)^ρ[f (P)(s, a)] for any P ∈ Md.	For any
P ∈ MD, as the proof of Theorem 1, We can prove P ∈ M2. Thus,
sup E(s,a)〜ρ[f(P)(s,a)] ≤ Zn.
P∈MD
Thus, We Will hereafter analyze Zn. Here from (12), for any P ∈ M2, We have
E(s,a)〜ρ[f(P )(s,a)] ≤ sup (∣(E(s,α)〜D - E(s,°)〜ρ)[f (P )(s, a)]|) + sup E(s,a)〜D [f (P )(s, a)]
P∈M2	P∈M2
.G(M2) + Jznlnc© + G(Mi) + ξn + lnc©.
n	nn
Hence,
Zn ≤ G(M2) + ∖∕zn In+ G(Mi)+ ξn +ln网.
n	nn
This shoWs
Zn ≤ G(M2)+ G(Mi) + ξ2 +---( / ).
nn
21
Published as a conference paper at ICLR 2022
Third step: Calculate the final error bound. Following the proof of Theorem 1, we can prove
VPπ? - VPπ? ≤ (1 - γ)-2c1
qC∏ * ʌ/ G(M2)+G(Mi)+ξn +
ln(c∕δ)
n
□
E.3 Proofs for Tabular MDPs
Proof of Corollary 1. We prove in a similar way as Theorem 1.
First step We set ξ = C|S| |A| lnCnSac2/'". Then, from Lemma 7, With probability 1 一 δ, we
can show P? ∈ MD since
Es,a 〜D hTV(PbMLE(∙ Ι s, a), P?(∙ | s, a))2] ≤ ξ.
Hereafter, we condition on the above event.
Second step. Following the second step in the proof of Theorem 1 based on (8), for any P ∈ MD,
we have
Es,a〜P [TV(P(∙ | s, a), P?(• | s, a))2] ≤ cξ + A(P)	(13)
where
A(P):= ∣Es,a〜ρ[TV(P(∙ | s,a),P?(∙ | s,a))2] 一 ED[TV(P(∙ | s,a),P?(∙ | s,a))2]∣.
Our goal here is showing with probability 1 一 δ,
A(P) .ξ,∀P∈MD.	(14)
ToProve (14), consider an e-net {Pι(s, a), …,Pκ(s, a)} covering a simplex in terms of ∣∣∙∣∣ι 4 for
each fixed pair (s, a) ∈ S × A. We take = 1/n. Since the covering number K is upper-bounded
by (c/e)|SI(WainWright, 2019, Lemma 5.7), we can obtain M = {Pi, …, Pk∣s∣×∣a∣ } s.t. for any
possible P ⊂ S × A → ∆(S), there exists Pi s.t.
TV(Pi(∙ | s,a),P(∙ | s,a)) ≤ e,∀(s,a).
This implies for any P ⊂ S ×A→ ∆(S), there exists 马(∙ | s, a) s.t. ∀(s, a),
∣TV(P(∙ | s, a), P?(∙ | s, a))2 - TV(Pi(∙ | s, a), P?(∙ | s, a))2∣
≤ 4∣TV(P(∙	|	s,a),P?(∙	| s,a))	- TV(Pi(∙	| s,a),P?(∙	| s,a))∣	(a2	一 b2 =	(a	一 b)(a + b))
≤ 4TV(P∙ | s,a),Pi(∙ | s,a))	(∣kak-kbk∣ ≤ ka — bk)
≤ 4e.	(15)
We often use this property (15) hereafter.
Next, we define M0 ⊂ M so that it covers MD. Concretely, we define M0:
M0 = {P ∈ M : ∃P00 ∈ Md , TV(P (∙ | s,a),P 00(∙ | s,a)) ≤ e ∀(s,a)}.	(16)
The construction is illustrated in Figure 1. Here, from the definition, for any P ∈ MD , we can also
find P0 ∈ M0 s.t.
TV(P(∙ | s,a),P0(∙ | s,a)) ≤ e,∀(s,a).
This is because from the definition of M, we can always find P ∈ M satisfying the above. Such P
belongs to M0 from the definition of M0. We use this fact later.
Then, from (16) and recalling (13), we have
Es,a〜P [TV(P(∙ | s,a),P?(∙ | s,a))2] . ξ + A(P),	∀P ∈M0.	(17)
4In the tabular setting, since the state space is countable, it is equivalent to L1 distance.
22
Published as a conference paper at ICLR 2022
Figure 1: MD is colored in gray. M0 corresponds to the set of black dots. Orange dots correspond
to M, which do not belong to M0.
because
Es,a~ρ [TV(P(∙ | s,a),P?(∙ | s,a))2]
≤ Es,a~ρ [TV(P(∙ | s, a), P00(∙ | s, a))2] + Es,a~ρ [TV(P00(∙ | s, a), P?(∙ | s, a))2]
≤ Es,a~ρ [TV(P00(∙ | s, a),P*(∙ | s, a))2] + e2	(Take some P00 ∈ Md noting (16))
≤ cξ + A(P).	(From (13))
Then, with probability 1 - δ, from Bernstein’s inequality, we hav J var[TV(P(∙ | s,a),P*(∙ | s, a))2]ln(Klsl×lAl∕δ) A(P) ≤ VC	n Hereafter, we condition on the above event. Based on (17), we ca var[TV(P(∙ | s,a),P?(∙ | s,a))2] . E[TV(P(∙ | s,a),P?(∙ | s, with probability 1 - δ. Following the argument of Theorem 1, for A2(P) - A(P)Bι- B2 ≤ 0,	Bi = ln(κlsl×lAl∕δ), p?= n Then, with probability 1 - δ, we have A(P) ≤ ln(Klsl×lAl∕δ) + rln(Klsl×lAl∕δ)y nn This shows for any P ∈ MD , we have	e + C In(KISl×lAl∕δ), VP ∈m. n n state a))2] .ξ +A(P),	∀P∈M0, P ∈ M0, we have ^ln(KlSl×lAl∕δ)十(ln(KlSl×lAl∕δ))2 nn .ξ, ∀P∈M0.	(18)
|{Ed — E(s,a)~ρ}[TV(P(∙ | s,a),P?(∙ | s,a))2]∣ ≤ |{Ed - E(s,a)~ρ}[TV(P0(∙ | s,a),P(∙ | s,a))2 + TV( ≤ |{Ed — E(s,a)~ρ}[TV(P0(∙ | s,a),P?(∙ | s,a))2]+8e .ξ. Thus, (14) is proved.	P0(∙ | s,a),P?(∙ | s,a))2]∣ (We take P0 ∈ M0 such that (16)) (From the definition of M0) (From (18) and P0 ∈ M0)
Third step. We follow the third step of Theorem 1:
一*
VP? - VP? . (1 - γ)
*
□
23
Published as a conference paper at ICLR 2022
E.4 Proofs for Linear Mixture MDPs
Proof of Corollary 2. Here, letting P(θ) = θ>ψ(s, a, s0), recall
MMix = P(θ) | θ ∈ Θ ⊂ Rd,	θ>ψ(s, a, s0)d(s0) = 1 ∀(s, a) , H
P + P?，一 *,〕
—2— | P ∈ MMiX >.
Upper-bounding E(s,°)〜°[TV(P(θ*)(∙ | s,a),P(Θmle)(∙ | s,α))2].
By invoking Theorem 4, we first show
E(s,a)〜ρ[TV(P(θ*)(∙ I s,a),P(Θmle)(∙ | s,α))2] ≤ c{(d/n)ln2(nR)+ln(c/S)/n}.
To do that, we calculate the entropy integral with bracketing. First, we have
N[] (E, H, d) ≤N[](E,MMix,d0).
where
d0(a, b) = E(s,a)〜P /
1/2
(a(s, a, s0) — b(s, a, s0 ))2d(s0)
Γ 1	__________ _______________ 1 1/2
d(a, b) = E(s,a)〜P	(√α(s,α, s0) — √b(s,a, s0))2d(s0)
Here, we use two observations. The first observation is
(19)
(20)
(21)
d2	∖ PW~
2
due to the mean-value theorem
P (θ00) + P?
≤ c1d02(P(θ0),P(θ00))
,
2
√a — Vb ≤ max(1∕√α, 1∕√b)(a — b)
and assumption P?(s0 | s, a) ≥ C0 > 0. The second observation is when we have P0 < g < P00, we
also have P(P0 + P?)/2 < P(g + P?)/2 < P(P00 + P?)/2. Then, (19) is concluded.
Next, by letting θ(1), ∙∙∙ , θ(K) be an E-Cover of the d-dimensional ball with a radius R, i.e, Bd(R),
we have the brackets {[P (θ(i)) — , P(θ(i)) + ]}iK=1 which cover Mmix . This is because for any
P(θ) ∈ Mmix, we can take θ(i) s.t. ∣θ — θ(i) ∣2 ≤ E, then,
P (θ(i)) —E < P(θ) < P (θ(i)) + E, ∀(s, a, s0)
noting
|P (θ)(s, a, s0) - P (θ(i))(s, a, s0)| ≤ ∣θ - θ(i)∣2 ≤,	∀(s, a, s0)
The last equality is from Lemma 10.
The brackets above are size of . Therefore, we have
N[](e, Mmix, k∙k2) ≤N(e,Bd(cR),∣H∣2),
(22)
where N(g Bd(CR), k∙∣∣2) is a covering number of Bd(CR) w.r.t ∣∣ ∙ ∣∣2. This is upper-bounded by
(cR/)d (Wainwright, 2019, Lemma 5.7). Thus, we can calculate the upper bound of the entropy
integral JB(δ, Mmix, ∣∣∙ ∣∣2):
δδ
I d1/2 ln1/2 (cR∕u)du ≤ I d1/2 ln(1∕u)du + δd1/2 ln(cιR)
00
=cd1∕2(δ + δln(1∕δ)) + δd1/2 ln(cR)
≤ cd*δln(cR∕δ).
By taking G(x) = d1/2 x ln(CR/x) in Theorem 4, δn = (d/n)1/2 ln(nR) satisfies the critical in-
equality
√nδ2b ≥ d1∕2δn ln(cR∕δn).
Finally, with probability 1 - δ
E(s,a)〜ρ[TV(P(θ?)(∙ | s, a), P(Θmle)(∙ | s, a))2] ≤ ξ0, E = {(d/n) ln2(nR) + ln(c∕δ)∕n}.
(23)
Hereafter, we condition on this event.
24
Published as a conference paper at ICLR 2022
Upper bounding ED[TV(P(Θ?)(∙ I s, a), P(Θmle)(∙ I s, a))2]. Wetakean e-cover of the ball
Bd(R) in terms of ∣∣∙ ∣∣2, i.e., M = {θ⑴，…,θ(K)}, where K = (cR∕e)d. Wetake e = 1∕n. Then,
for any θ ∈ Bd(R), there exists θ(i) s.t. ∀(s, a),
ITV(P(θ)(∙ I s,a),P(Θmle)(∙ I s,a))2 - TV(P(θ(i))(∙ I s,a),P(θMLE)(∙ I s,a))2I
≤ 4ITV(P(θ)(∙ I s,a),P(Θmle)(∙ I s,a)) - TV(P(Θ⑺)(∙ I s,a),P(Θmle)(∙ I s,a))I
≤ 4TV(P(θ)(∙∣ s,a),P(θ⑴)(∙ | s,a))
≤ 4kθ - θ(i)k2
≤ 4.
(a2 - b2 = (a - b)(a + b))
(lkak-kbkl≤ka-bk)
(From Lemma 10.)
(24)
Hereafter, we condition on the event:
I(Ed - E(s,a)〜ρ)[TV(P(θ)(∙ | s,a),P(θ*)(∙ | s,a))2]∣
(25)
var(s,a)〜ρ[TV(P(θ)(∙ | s,a),P(θ*)(∙ | s, a))2]ln(K∕δ) + ln(K∕δ)	∀θ ∈ M
This event holds with probability 1 - δ from Bernstein’s inequality.
Here, note for Θmle, We have θ(i) s.t. ∣Θmle — θ(i)∣∣2 ≤ e. Then, following the first step of
Theorem 1,
E(s,a)〜DTV(P(Θ*)(∙ I s,a),P(Θmle)(∙ I s,a))2
.E(s,a)〜DTV(P(θ*)(∙ | s,a),P(θ⑴)(• | s,a))2 + e
(From (24))
.(ED - E(s,a)〜P)TV(P(θ*)(∙ | s,a),P(θ⑴)(∙ | s,a))2 + e + %⑷〜.TV(P(θ*)(∙ | s,a),P(θ(i))(∙ |
s, a))2
var(s,a)〜ρ[TV(P(θ*)(∙ I s,a),P(θ⑴)(∙ | s, a))2]ln(K∕δ) 1 ln(K∕δ)
+ e + E(s,a)〜ρTV(P(θ*)(∙ | s,a),P(θ⑴)(• | s,a))2
(From (25))
E(s,a)〜ρ[TV(P(θ*)(∙ I s,a),P(θ⑴)(∙ | s,a))2]ln(K∕δ) 1 ln(K∕δ)
+ e + E(s,a)〜ρ[TV(P(θ*)(∙ | s,a),P(θ⑴)(∙ | s,a))2].
(TV(P(θ*)(∙ | s,a),P(θ⑴)(∙ | s,a))2 ≤ 4)
Then, we have
ED[TV(P(θ*)(∙ I s,a),P(Θmle)(∙ I s,a))2]
.j{E(s,a)〜ρ[TV(P(θ*)(∙ | s,a.
),P(Θmle)(∙ I s,a))2]+ e} ln(K∕δ) + ln(K∕δ)
+ e + E(s,a)〜PTV(P(θ*)(∙ | s,a),P(θMLE)(∙ | s,a))
U≡δ) + l≡≡ + e+ξ0.
In the end, by taking
n
1/n, we have with probability 1 - δ,
(From (23))
.
.
.
.
n
n
n
n
n
n
n
n
,ʌ
n
2
EDTV(P(θ*)(∙ I s,a),P(Θmle)(∙ I s,a))2 ≤ ξ, ξ = c{(d∕n)ln2(nR)+ln(c∕δ)∕n}.
This also implies with probability 1 - δ, P? ∈ MD .
Show E(s,a)〜ρTV(P(θ*)(∙ | s,a),P(θ)(∙ I s,a))2 . ξ,∀P(θ) ∈ Md.
We show for any P ∈ MD , the distance between P? is controlled in terms of TV distance. Our
goal is showing
E(s,a)〜ρTV(P(θ*)(∙ I s, a), P(θ)(∙ I s, a))2 . ξ, ∀P(θ) ∈ Md.
25
Published as a conference paper at ICLR 2022
First, following the second step of Theorem 1 based on equation 8, we have
E(s,a)~ρTV(P(θ*)(∙ | s, a), P(θ)(∙ | s, a))2 ≤ A(θ) + cξ, ∀P(θ) ∈ MD (26)
where
A(θ) = |(Ed - E(s,a)~ρ)TV(P(θ*)(∙ | s,a),P(θ)(∙ | s,a))2∣.
From now on, We again consider an E-Cover of the ball Bd(R) in terms of ∣∣ ∙ ∣∣2, i.e., M =
{θ(1),…，θ(K)}, where K = (cιR∕e)d (E = 1/n). This also covers the space MD. We take
M0 = {θ(i1), θ(i2) …，}⊂M which covers Md, that is,
M0 = {θ ∈M | ∃θ0s.t.P(θ0) ∈ Md, ∣∣θ — θ0∣∣2 ≤ e}.
Recall Figure 1, which illustrates this definition. Here, for any θ s.t. ∀P (θ) ∈ MD, we can take
θ0 ∈ M0 s.t. ∣∣θ 一 θ0∣2 ≤ 1/n. This is because we can take θ ∈ M s.t. ∣θ 一 θ0∣2 ≤ E noting M is
an E-net, but such θ belongs to M0 from the definition of M0 .
Then, we have
E(s,a)~ρTV(P(θ*)(∙ I s,a),P(θ)(∙ | s,a)) ≤ A(θ) + cξ,	∀θ ∈ M0.	(27)
This is because for any θ(i) ∈ M0, we can take P(θ) ∈ MD such that
E(s,a)~ρTV(P(θ*)(∙ I s,a),P(θ(i))(∙ I s,a))2
≤ E(s,a)~ρ[TV(P(θ*)(∙ ∣ s,a),P(θ⑴)(∙ ∣ s,a))2 — TV(P(θ*)(∙ ∣ s,a),P(θ)(∙ ∣ s,a))2]
+ E(s,a)~ρ[TV(P(θ*)(∙ ∣ s,a),P(θ)(∙ I s,a))2]
≤ 4e + E(s,a)~ρ[TV(P(θ*)(∙ ∣ s,a),P(θ)(∙ ∣ s,a))2]	(∣θ -θ⑴ ∣2 ≤ E and from (24))
. A(θ) +ξ.	( From (26))
Here, from (25), we have
A(θ) ≤
va%,a)~ρ[TV(P(θ*)(∙ I s,a),P(θ)(∙ I s, a))2]ln(K∕δ) +
C ln(K∕δ)
n
∀θ ∈ M0
c
n
Based on the construction of M0 and (27), we have
va%,a)~ρ[TV(P(θ*)(∙ I s,a),P(θ)(∙ I s,a))2] . A(θ) + ξ, ∀θ ∈ M0.
Then, following the second step of Theorem 1, A(θ) satisfies
A2(θ)-A(θ)Bι-B2 ≤ 0,	Bi = 3 B= ξ 4+ (4
n	nn
Then, we have
A ≤ lnKδ) + ξ1/2 JlnK/δ) . ξ,	∀θ ∈m0.
nn
(28)
We combine all steps. Recall for any ∀P (θ) ∈ MD, we can take θ0 ∈ M0 s.t. ∣θ 一 θ0∣2 ≤ 1/n.
Then, for any P(θ) ∈ MD , we have
A(θ) = I (Ed - E(s,a)~ρ)TV(P(θ)(∙ I s, a), P(θ?)(∙ I s,a))2
≤ I (Ed 一 E(s,a)~ρ)[TV(P(θ)(∙ I s,a),P(θ? )(∙ I s,a))2 — TV(P(θ0)(∙ I s, a), P(θ?)(∙ I s,a))2]
+ (ED — E(s,a)~ρ)[TV(P(θ0)(∙ I s, a), P(θ*)(∙ I s, a))2]
≤ 8e + I (Ed - E(s,a)~ρ)[TV(P(θ0)(∙ I s, a),P(θ? )(∙ I s, a))2]	(From equation 24)
. ξ.	(From equation 28 and θ0 ∈ M0)
Then, we have with probability 1 一 δ,
A(θ) . ξ, ∀P (θ) ∈ MD .	(29)
Finally, for any P(θ) ∈ MD, with probability 1 一 δ, we have
E(s,a)~ρ[TV(P(θ*)(∙ I s,a),P(θ)(∙ I s, a))2] ≤ A(θ) + cξ	(Fromequation26)
. ξ.	(From equation 29)
26
Published as a conference paper at ICLR 2022
Distribution shift part Here, for P ∈ MD we prove
VPπ?	-	VP	. (I-	Y) -PdCπ*,mixξ,	(3O)
VP?	-	VP*	. (1 -	Y)-2qC*ξ.	(31)
Following the third step of the proof of Theorem 5, this immediately concludes the bound
一?
VP? - VP? . (I - Y)	PdCπ*,mixξ,
VP? - VP? . (1 - Y)-2qc?ξ.
Since (31) is obvious from simulation lemma, we only prove (30). To prove (30), we take a distri-
bution P(θ) ∈ MD. First, recall for P(θ) ∈ MD, we have
E(s,a)〜ρTV(P(θ^(∙ | s, a),P(θ)(∙ | s, a))2 . ξ
From the third statement of Lemma 10, for any V : S → [0, 1], we have
E(s,a)〜ρ[∣(θ -θ*)>ψv(s,a)l2] . ξ.
Thus,
∀V : S→ [0,1],	(θ -θ*)>∑ρ,v (θ -θ*) . ξ,	Σρ,v = E(s,a)〜ρ[ψv (s,a)ψV(s,a)].
Here, we have
**	*
VP: - VPr ≤ (1 -Y)T %”)〜dP; [/ {P(s0 | s,a) - P?(s0 | s, a)}VPπ* (s0)d(s0)j
(Simulation lemma, Lemma 5)
≤ (1-Y)TE(s,awP; h(θ -θ*)ψvP∏* (s,a)i∣
(Recall ψV = R ψ(s, a, s0)VPπ* (s0)d(s0))
≤
(1
-Y)	llθ - θ klI+ZvP* E(s,a)〜dP:
'---------7----------------------
(a)
lψV π* (s, a)l(Σ Vπ* +λI)-1 .
P	ρ,VP
. _ - /
{z
(b)
(CS inequality)
The first term (a) is upper-bounded by P{(1 - Y)-2ξ + λR2} noting 0 ≤ V∏* ≤ (1 - y)-1. The
term (b) is upper-bounded by
1/2
E(s,α)〜dp? kψVg* (S，a)k(∑ρ,v∏* +λI )-1 ≤ E(s,α)〜dp? kψVg* (S,a) k-£°,号* +λI)-1
(Jensen’s inequality)
=q^Ndp? W (λI + ςp,vs* )T)
≤ qC∏* ,mix Tr(ςp,Vj∏* (λI + ςp,V∏* )-1)
(From Lemma 11)
≤ C C∏* ^[乂玲口^夕。^π* ) ≤ PC∏* ,mixd.
By taking λ s.t. λR2 . (1 - Y)-2ξ, (30) is proved.
For linear MDPs, from the fourth statement of Lemma 10, C∏*,mix ≤ C∏*. Then, the statement is
concluded.	□
E.5 PROOFS FOR KNRS
Proof of Corollary 3. We prove in a similar way as Theorem 1.
27
Published as a conference paper at ICLR 2022
First Step Recall
ξ = /2λ∣∣W?k2 +8Z2 (ds ln(5) +ln(1∕δ) + In), In = ln (det(Σn)/det(λI)).
Thus, from Lemma 8, with probability 1 - δ, we can show W * ∈ WD since
KCMLE — W?) Mn)1/1? ≤ ξ.
Hereafter, we condition on this event.
Second step For any W ∈ WD , with probability 1 - δ, we have
||(W - W?)(∑n)1∕2∣L ≤ ||(W - C) (∑n)1∕2IL + ll(w* — W) Mn)"]? ≤ 匕
Third step Note P? = P (W*). Then,
_ *
VPπ?
**	*
-VP? ≤VP? -Wm∈iWnDVP(W)+Wm∈iWnDVP(W) -VP?
**
≤VP? -Wm∈iWnDVP(W)+Wm∈iWnDVP(W) -VP?
**
≤ VP? - Wm∈iWnD VP(W).
(definition of ∏)
(Fist step, W* ∈ WD)
Then, by setting W0 = arg minW ∈MD VPπ(*W), we have
VPπ* — 库* ≤ (1 — γ)-2E(s,a)〜dP? [kP0(s, a) - P?(s, a)kτv]
(1 - γ)-2
≤ ——Z——E(s,a)〜dP? [k(W — W )φ(s,a)k2]	(Lemma9)
≤ (I-J-2E(s,aWP? h||(W0 — W*)(∑n)"∣∣2kΦ(s,a)k∑-ιi	(CS inequality)
(1 - γ)-2
≤ ——Z-ξE(s,a)〜dp? [kφGa)k∑H	(SecondsteP)
From Chang et al. (2021, Theorem 20), with probability 1 — δ, we have
ξ ≤ Cl q∣∣W *∣∣2 + ds min(rank(Σρ){rank(Σρ) +ln(c2∕δ)},d)ln(l + n).
In addition, from Chang et al. (2021, Theorem 21), with probability 1 — δ, we also have
/ C∏* rankBpHrankNp] + In(C2评)}
E(s,a)〜dp? [kφ(s,a)k∑-1 ] ≤ Cw--------------n----------------.
Finally, by combining all things, we have
Vpπ* — V∏? ≤ Ci(1 — Y)-2 min(d1/2, R)PR j"S"π In(I + n, R = rank[∑p]{rank[∑p] + ln©/®)}.
□
E.6 Proofs for Low-rank MDPs
Proof of Theorem 2. Until the second step, we can perform the same analysis as Theorem 1. More
concretely, with probability 1 — δ, we have P? ∈ MD and
Es,a〜p[TV(P(∙ | s,a),P?(∙ | s,a))2] ≤ ξ,
∀P ∈MD,ξ := cIn(M
n
Hereafter, we condition on the above event.
(32)
28
Published as a conference paper at ICLR 2022
Letting f (s,a) = TV(P(∙ | s,a),P?(• | s,a)), we use Lemma 4. Then,
E(s,a)~dp* [f (s, a)] ≤ E(s,a)~dp* [kφ?(S, a)k∑-1 ? ] JnYωπ Ep[f2(s,a)] +4γ2λd + J(I - Y)ω∏ Eρ[f2(S, a)]
where Σ = nEρ[φ?φ?>] + λI. We consider how to bound E(s,a)~d∏* [∣∣φ？(s,a)k∑-ι *]. This is
upper-bounded by
E(s,a)~dP*[kΦ*(s,α)k∑-1 *] ≤ qtr(E(s,a)~dP*[Φ*Φ*>]∑-,φ*)
≤ /C∏*,φ* tr(E(s,a)~ρ[φ*φ*>]∑-,φ* )	(From Lemma 11)
≤ JCΠ*,φ*rank(Σρ)∕n.
Here, in the last line, by letting the SVD of Σρ = Eρ[φφ>] be UΣPU> where ΣP is a d X d diagonal
matrix and U is a d × d orthogonal matrix , we use
tr (∑ρ∑-,φ*) = tr(U∑PU>{nUΣPU> + λI}-1) = tr(ΣPU>{nUΣPU> + λI}-1U)
=tr(Σ PU >{U {nΣ P + λI }U >}-1U)
=tr(Σ PU >U {nΣ P + λI }-1U >U)
=tr(ΣP{nΣP + λI})-1 ≤ rank(∑P)∕n.
Hence, when P ∈ MD, by setting λ s.t. λd . nωπξ, we have

E(s,a)~dp* [f (S,a)] ≤
YC∏*,φ*rank(∑P)ω∏ ln(∣M∣∕δ)
n

+
We use (32) here.
Finally,
*
vpπ* — v∏*
≤ VPπ** - min VPπ*
P P ∈MD P
(Recall the proof of the third step in the proof of Theorem 1)
≤ (I-Y)-2EssdP*TV(P(S,a),P*(∙ | s,a))
(1 - γ)ω∏ln(∣M∣∕δ)
n
(P0 = arg minP∈MD VPπ*)
.
C∏*,φ*rank(∑P)ω∏* ln(∣M∣∕δ)
n(1 - Y)4
□
The following inequality is an important lemma to connect E(s,a)~d∏* {f (s, a)} with an elliptical
potential旧⑸衿〜壮凝∣∣φ?(3,初卜-1*.
Lemma 4 (One-step back inequality). Take any f ⊂ S × A → R s.t. kfk∞ ≤ B and 0 < λ ∈ R.
Letting ω = maxs,a (π (a | S)∕πb(a | S)), for any policy π, we have
lE(s,a)~dP* {f (s, a)} | ≤ E(g,a)~dP* kφ? (g, a)k∑-ι J {nωπ Y E(s,a)~P [f2(s, a)] } + Y2λdB2
+ J(1 - Y)ωπE(s,a)~P [f2(s, a)].
where Σ = nE(s,a)~P∣φ*(s, a)φ*>(s, a)] + λI.
Proof of Lemma 4. First, we have an equality:
E(s,a)~dP* {f (s, a)} = YE(g,a)~dP* *p*(s,a) {f (s, a)} + (1 - Y)Es~do,a~∏(s0) {f (s, a)}.
(33)
29
Published as a conference paper at ICLR 2022
The second term in (33) is upper-bounded by
Es〜do ,a〜π(so) {f (s, a) } ≤ Es〜do,a〜π(so) {f2(S,a)""= Jωπ E(s,a)〜P [f2(S, a)] IQ - Y) .
Next we consider the first term in (33). By CS inequality, we have
E(E,a)〜dp?,s〜P*(s,a) {f (s, a)}∣
er,a)〜dp* φ*(s,s)> / μ(s)π(a | s)f (S,a)d(s,a)
≤ E(g,a)〜dp*kφ*(s,s)k∑-1 *k∕ μ(s)π(a | s)f(s,a)d(s,a)k∑ρ,φ*.
Then,
Il /μ(s)π(a | s)f (s,a)d(s,a)k∑ρ,φ*
>
μ(s)∏(a | s)f (s, a)d(s, a) } {nE(s,a)〜ρ[φ*φ*>] +
≤n
μ(s)π(a | s)f (s, a)d(s, a)
〜P
J μ(s)>φ? (S, a)π(a | s)f(s,a)d(s,a)
2
+ B2λd
(Use the assumption ∣∣f (s, a)k∞ ≤ B and ∣∣ J μ(s)d(s)k2 ≤ √d)
=n {E(g,a)〜ρ,s〜P*(g,a),a〜∏(s) [f (s, a)]} + B λd
≤ n {E(g,a)〜ρ,s〜P*(s,a),a 〜π(s) f2(S,a) + B2λd.
(Jensen)
Finally, the the first term in (33) is upper-bounded by
n {E(g,a)〜ρ,s〜P*(g,a),a〜∏(s) f2(S,a) + λdB2
≤ nω∏ {E(g,a)〜ρ,s〜P*(g,a),a〜∏b(s) f2(S, a)} +	λdB2
≤ nω∏ Y YE(s,a)〜p f2 (s, a)] } + λdΒ2.
The final statement is immediately concluded.
(Importance sampling)
(Definition of ρ)
□
E.7 Proofs for Factored MDPs
Proof of Theorem 3. We denote the constrained set as MD :
Md = {p = YPi | ED [τV(pMLE,i(∙ | s[pai],a),Pi(∙ | s[pai],a))2] ≤ ξi,市 ∈ [1,…，d]}.
Following the first step in the proof of Corollary 1, with probability 1 - δ, the product	i Pi? is in
MD, i.e.,
Es,a〜D [τV(pMLE,i(∙	|	s[pai],a),P*(∙	|	s[pai],a))2]	≤	ξi, ∀i ∈ [1,…,d],	ξi	= /Liklg―) .
Note d comes from the union bound. Besides, following the second step in the proof of Corollary 1,
for any P ∈ MD , with probability 1 - δ,
Es,a〜PhTV(Pi(∙ | s[pai],a),P*(∙∣ s[pai],a))2i ≤ ξi,∀i ∈ [1, …,d].
30
Published as a conference paper at ICLR 2022
After conditioning on the above two events, then, for any P ∈ MD and π? ∈ Π, we have
*	*
VPπ? - VPn ≤ (1 - Y)-2E(s,a)〜dP? [TV(P(∙ I s, a),P?(∙ I s, a))]
(Simulation lemma, Lemma 5)
≤ (I-Y)-2E(s,a)〜dP? X TV(Pi(∙ | s[pai],a),p*(∙ | s[pai],a))]
≤ (1 - Y)-2 XtE(s,a)〜P CPIUai；a )	E(s,a)〜ρ[TV(Pi(∙ I s"i],a),P*(∙ | s[pai],a))2]
(CS inequality)
≤ (1 - Y)-2 X qC∏*,∞E(s,a)〜ρ[TV(Pi(∙ | s,a),P*(∙ | s,a))2] ≤ (1 - γ)-2 X ,C∏*,∞ξi
ii
≤ (1 - Y)-2
jdCπ* ,∞
(CS inequality)
-	、-2 仄	L In(Lnd/δ)
≤C(I-Y) 2VdC∏*,∞ —n—.
Here, recall
Cπ*,∞ = , max ,E(s,a)〜P
i∈[1,…，d]
dP? (s[pai], a)
ρ(s[pai],a)
Following the third step in the proof of Corollary 1, the statement is concluded.
□
*
Next, We show that C∏*,∞ ≤ C∏*,p? = maxs,a P?：：；?.
Proposition 1 (Comparison of L∞-density-ratio based concentrabiliity coefficient between factored
MDPs and non-faCtored MDPs). For any π*, we have:
a .…	,一、
Cπ* ,∞ ≤ Cπ* ,∞ .	(34)
Proof. From now on, for any i ∈ [1, ∙∙∙ ,d], by defining Si s.t. S = Si X S[, We prove
dπP*? (si, a)	dπP*? (si, s0i, a)
max	-Py------ ≤ max	p--------ɪ i
si∈Si,a∈A ρ(si, a)	s∈Si,s0i∈Si,a∈A ρ(si, s0i, a)
Cπ* ,∞ .
Then, (34) is easily proved.
First, for any si ∈ Si, a ∈ A, we have
max
s0i
dπP*? (si, s0i, a)
ρ(si, s0i, a)
max
s0i
dπP*? (si, a)dπP*? (s0i I si, a)	dπP*? (si, a)
ρ(si, a)ρ(s0i I si, a)
ρ(si,a)
dπp*? (s0i I si, a)	dπp*? (si, a)
max Wn------------「≥ ―，——「.
s0i	ρ(s0i I si, a)	ρ(si, a)
(35)
Here, we use
1 ≤ max dP? (Si 1 si,a),
s0i	ρ(s0i I si, a)
π*	0
which is proved by the contradiction argument, that is, if 1 > max：o 黑|：. ；j , both P and dp?
cannot be probability mass functions since we would get
1
s0i
s0i
dπP*? (s0i I si , a) ≤ max
s0i I si , a)
0i I si , a)
s0i
ρ(s0i I si,a) <	ρ(s0i I si, a).
s0i
Then, by taking the maximum over si ∈ Si, a ∈ A for both sides on (35), we have
max
si,a
*
dp? (si, a)
ρ(si, a)
≤ max
si ,s0i,a
*
dp? (si, Si, a)
P(S1, si,a)
□
31
Published as a conference paper at ICLR 2022
F Auxiliary Lemmas
Lemma 5 (Simulation Lemma). Consider any two transitions P and Pb, and any policy
π : S →
∆(A). We have:
IVPn - VPπ | ≤ I(I-Y) 1Es,a^d^p [Es0〜P(s,a) IVPn (SO)] - EsO〜P(s,a)[Vn (SO)]]|
≤ (1 - γ) 2Es,a〜dp [TV(P(∙∣s,a),P(∙∣s,a))i .
Proof. Such simulation lemma is standard in model-based RL literature and the derivation can be
found, for instance, in the proof of Lemma 10 from SUn et al. (2019).	□
Lemma 6 (MLE guarantee). Given a set of models M = {P : S × A → ∆(S)} with P? ∈ M,
and a dataset D = {si, a%, Si}n=1 with Si, ai 〜P, and Si 〜P?(si, ai), let PMLE be
PbMLE = arg min	-lnP(SOiISi,ai).
P∈M i=1
With probability at least 1 - δ, we have:
Es,a〜PTV(PMLE(∙∣S,a),P*(∙∣S,a))2 . In(Ml").
Proof. Refer to (Agarwal et al., 2020b, Section E)	□
Lemma 7 (MLE guarantee for tabular models).
ED hTV(P(∙∣S,a),PMLE(∙∣S,a))2i ≤ |S1A1{1S| ln2 + ln⑵ S11A1碎.
2n
Proof. From Chang et al. (2021, Lemma 12) , with probability 1 - δ,
TV(P(∙∣S,a),pMLE(∙∣S,a))2 ≤ 1S1 ^2+^⑵S"/10 ∀(S,a) ∈ S ×A,
2N (S, a)
where N(S, a) is the number of visiting times for (S, a). Then,
ED [τV(P(∙∣S,a),pMLE(∙∣S,a)H
≤ ED
≤X
(s,a)
|S| ln2 + ln(2∣S∣∣A∣∕δ)-
2N(s, a)
■ ∣S∣ ln2 + ln(2∣S∣∣A∣∕δ) ^
2n
∣S∣A∣{∣S∣ ln2 + ln(2∣S∣∣A∣∕δ)}
2n
□
Lemma 8 (MLE guarantee for KNRs).
LE - W?) Rn)1/]? ≤ βn.
Proof. The proof directly follows the confidence ball construction and proof from (Kakade et al.,
2020).	□
Lemma 9 (`1 Distance between two Gaussians). Consider two Gaussian distributions P1 :=
N(μι,Z2I) and P2 := N(μ2,Z2I). We have:
TV(P1,p2) ≤ Z kμ1 - μ2k2 .
32
Published as a conference paper at ICLR 2022
Proof. This lemma is proved by Pinsker’s inequality and the closed-form of the KL divergence
between Pi and P2. Refer to (Kakade et al., 2020).	口
Lemma 10 (Property of linear mixture MDPs). Let P(θ) = θ> ψ(s, a, s0). Suppose P(θ) ∈ S ×
A → ∆(S). For any function V ∈ S → [0, 1], letting ψV (s, a) = ψ(s, a, s0)V (s0)d(s0), we
suppose kψV (s, a)k2 ≤ 1. The following theorems hold:
1.	For any (s, a, s0), we have |P (θ)(s, a, s0) - P(θ0)(s, a, s0)| ≤ kθ - θ0k2.
2.	For any (s,a), we have TV(P(θ)(s,a, ∙),P(θ0)(s,a, ∙)) ≤ ∣∣θ 一 θ0∣∣2. Besides, for any
V : S → [0, 1], we have
∣(θ — θ0)ψv(s, a)| ≤ TV(P(θ)(s, a, ∙), P(θ0)(s, a, ∙)).
3.
Cn ?,p ?
SU x>E(s,a)~dP? [ψV(s,a,x) (S, aW>(s,a,χ) G a)]x
X	χ>E(s,a)~ρ[ψV(s,α,χ) (s, a)ψ>(s,a,χ) (s, a)]x
V(s,a,x) = arg max
V ：St[0,1]
x>
φ(S, a, S0)V (S0)d(S0)
4.	In IinearMDPs (i.e., ψ(s, a, s0) = φ(s, a) 0 μ(s0)), we have
x›E(s a)~d∏? [ψV (S, a)ψV(s, a)]x	x>Ed∏? [φ(s, a)φ(s, a)>]x
SUP	SUP --------------~~ΓTr7~~= SUP 一>m 「/一、/一、>>
V∈{S→[0,1]} X X 1 E(s,a)~ρ[ψv(S, a)ψ>(s, a)]x	X X 1 Eρ[φ(s, a)φ(s, a) 1 ]x
Proof. We prove the first statement. This is proved by
∣P(θ) — P(θ0)∣ = ∣(θ 一 θ0)ψ(s, a, S0)∣ ≤ kθ 一 θ0∣2∣ψ(s, a, s0)∣2 ≤ ∣θ 一 θ0∣2,
Here, we use ∣ψ(S, a, S0)∣2 ≤ 1 which is proved by the assumption by setting V (S) = I(S0 = S)
for any S0.
Next, we prove the second statement. For fixed θ ∈ Rd and (S, a) ∈ S × A, we have
TV(P(θ)(s,a,∙),P(θ*)(s,a,∙))=	SUP | [(θ 一 θ*)>ψ(s,a,s0)V(s0)d(s0)∣
V ：St[0,1] J
= Sup	∣(θ — θ*)> ψ ψ(s,a,s0)V(s0)d(s0)∣
V=S→[0,1]	J
=l(θ 一。*)> /ψ(s,a,s0)V(s,a,θ)(s0)d(s0)l
=l(θ 一 θ*9>ψV(s,a,θ) (s,a)|.
In the third line, we define V(s, a,θ) = argmaxy爸一田」]∣(θ - θ*)> J ψ(s, a, s0)V(s0)d(s0)∣.
Then, from CS inequality,
TV(P(θ)(s,a, ∙),P(θ*)(s,a,∙)) ≤ k(θ -。？||2]忖忆")(s,a)∣∣2 ≤ |忸一。？||2.
We use the assumption ∣ψV(s,a,θ) (S, a)∣2 ≤ 1. This concludes the second statement. Besides, for
any V : S → [0, 1], we have
l(θ 一的砂丫 (s,a)l≤l(θ 一。?)>他心。,°)(s,a)l
≤ TV(P(θ)(s,a, ∙),P(θ0)(s,a,∙)).
The third statement is immediately concluded by
E(s,a)~dP? [TV(P(θ)(s, a, ∙), P(θ*)(s, a, ∙))2] _ E(s,a)~dP? [∣(θ 一。?)>砂%,。,°)(s, a)|2]
E(s,a)~ρ[TV(P(θ)(s,a, ∙),P(θ*)(s,a, ∙))2] = E(s,a)~ρ[∣(θ - θ?)>ψV(s,a,θ)(s,a)∣2] . (36)
33
Published as a conference paper at ICLR 2022
Finally, We prove the fourth statement. Suppose ψ(s, a, s0) = φ(s, a) 0 μ(s0)他 denotes kronerker
product). Then, φv(s, a, s0) = φ(s, a) 0 ʃ μ(s0)V(s0)d(s0). Then, by defining a vector μ(V)=
J μ(s0)V(s0)d(s0), we immediately have
x>E(s,a)〜dP? [ψV (S, a)ψV(s, a)]x	x>E(s,a)〜dP? [(φ(s, a) 0 μ(V ))(φ(s, a) 0 μ(V ))>]x
PT==SUP	PTTTT.
X>E(s,a)〜ρ[ψv (s, a)ΨV (s, a)]x	X	x>E(s,a)〜ρ[(O(S, a) 0 μ(V ))(φ(s, a) 0 μ(V ))>]x
(37)
Here, we have
Eρ[(φ(s, a) 0 μ(V))(φ(s, a) 0 μ(V))>] = Eρ[(φ(s, a) 0 μ(V))(φ(s, a)> 0 μ(V)>)]
=Eρ[(φ(s, a)φ(s, a)>)] 0 (μ(V)μ(V)>).
We notice
{Eρ[(Φ(s, a)φ(s, a)>)] 0 (μ(V)μ(V)>)}1/2 = Eρ[Φ(s, a)Φ(s, a)>]1/2 0 (μ(V)μ(V)>)1/2.
This is because the square root of a matrix is unique and we have (A1/2 0 B1/2 )(A1/2 0 B1/2 )
AB for symmetric matrices A and B. Then, by denoting Fρ = Eρ [φ(S, a)φ(S, a)>], Fdπ?
Edπ? [φ(S, a)φ(S, a)>] and denoting the pseudo inverse of F as F+, we can see (37) is equal to
{FJ2 0 (μ(V)μ(V)>)1/2} + {Fdp? 0 (μ(V)μ(V)>)}{F；/2 0 (μ(V)μ(V)>)1/2}+
={F-1/2 0 (μ(V)μ(V)>)T∕2}{Fdp? 0 (μ(V)μ(V)>)}{F-1/2 0 (μ(V)μ(V)>)-1/2}
={F-1∕2Fdp? F-1/2} 0 {(μ(V )μ(V )>)T∕2(μ(V )μ(V )>)(μ(V )μ(V )>)-1/2}
={F-1∕2FdP? F-1/2} 0 Ik (k = rank(μ(V)μ(V)>))∙
Here, Ik is a diagonal matrix s.t. k ∈ N+ values in the diagonal entries are 1 and the rest of
values are 0. Then, the maximum singular value of {Fρ-1/2Fdp?F-1/2} 0 Ik is equal to the one of
{F-1∕2Fdp* F-1/2}. This is equal to
suP
x
x>FdP？X
x> Fρx
Hence, the fourth statement is concluded.
□
Lemma 11 (Distribution shift lemma). Suppose A1, A2, A3 are semipositive definite matrices:
Tr(AIA2) ≤ bmax(A-1/2AiA-1/2) Tr(A3A2).
Note
σmaχ(A-1∕2AIA-1/2)
x> A1 x
X∈Rd x>Aχ.
Proof.
Tr(A1A2) = Tr(A11/2A2A11/2) = Tr(A11/2A3-1/2A13/2A2A31/2A3-1/2A11/2)
= Tr(A3-1/2A1A3-1/2A31/2A2A13/2).
In addition, for any semipositive definite matrices A, B we have
Tr(AB) = Tr(UΛU>B) = Tr(ΛU>BU) ≤ σmax(Λ) Tr(U>BU) = σmax(A) Tr(B),
where UΛU> is the SVD decomoposition ofA. This concludes that
Tr(A1A2) ≤ σmaχ(A-1∕2AιA-1∕2)Tr(A3A2).
□
34
Published as a conference paper at ICLR 2022
The following lemma is useful to obtain the generalized result of Theorem 1. The proof is given in
(Wainwright, 2019, Theorem 3.27). We first define
Z = sup |{ED - Eρ}[f]
f∈F
Σ2 = sup ED [{f (s, a) - Eρ [f (s, a)]}2], σ2 = sup var[f (s, a)].
f∈F	f∈F
Lemma 12 (Functional Bernstein’s inequality: Talagrand concentration inequality for empirical
process). Suppose kfk∞ ≤ B. With probability 1 - δ,
|Z - E[Z]∣≤ ∑2r∕lθg(C0 + BIOg(C0 .
nn
As an immediate corollary,
|Z - E[Z ]| ≤ {σ2 + B E[Z ]}Jlog(c/J) + B IOg(CM).
nn
35