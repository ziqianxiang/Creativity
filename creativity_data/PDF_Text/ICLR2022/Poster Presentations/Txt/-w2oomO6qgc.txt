Published as a conference paper at ICLR 2022
GeneDisco:	A Benchmark for Experimental
Design in Drug Discovery
Arash Mehrjou1 , Ashkan Soleymani2, Andrew Jesson3, Pascal Notin3,
Yarin Gal3, Stefan Bauer1, Patrick Schwab1
1	GlaxoSmithKline, Artificial Intelligence & Machine Learning
2	MIT, 3 Department of Computer Science, University of Oxford
Ab stract
In vitro cellular experimentation with genetic interventions, using for example
CRISPR technologies, is an essential step in early-stage drug discovery and target
validation that serves to assess initial hypotheses about causal associations be-
tween biological mechanisms and disease pathologies. With billions of potential
hypotheses to test, the experimental design space for in vitro genetic experiments
is extremely vast, and the available experimental capacity - even at the largest
research institutions in the world - pales in relation to the size of this biological
hypothesis space. Machine learning methods, such as active and reinforcement
learning, could aid in optimally exploring the vast biological space by integrating
prior knowledge from various information sources as well as extrapolating to yet
unexplored areas of the experimental design space based on available data. How-
ever, there exist no standardised benchmarks and data sets for this challenging
task and little research has been conducted in this area to date. Here, we introduce
GeneDisco, a benchmark suite for evaluating active learning algorithms for exper-
imental design in drug discovery. GeneDisco contains a curated set of multiple
publicly available experimental data sets as well as open-source implementations
of state-of-the-art active learning policies for experimental design and exploration.
1	Introduction
The discovery and development of new therapeutics is one of the most challenging human endeav-
ours with success rates of around 5% (Hay et al., 2014; Wong et al., 2019), timelines that span on
average over a decade (Dickson & Gagnon, 2009; 2004), and monetary costs exceeding two billion
United States (US) dollars (DiMasi et al., 2016; Berdigaliyev & Aljofan, 2020). The successful
discovery of drugs at an accelerated pace is critical to satisfy current unmet medical needs (Rawl-
ins, 2004; Ringel et al., 2020), and, with thousands of potential treatments currently in development
(informa PLC, 2018), increasing the probability of success of new medicines by establishing causal
links between drug targets and diseases (Nelson et al., 2015) could introduce an additional hundreds
of new and potentially life-changing therapeutic options for patients every year.
However, given the current estimate of around 20 000 protein-coding genes (Pertea et al., 2018), a
continuum of potentially thousands of cell types and states under a multitude of environmental con-
ditions (Trapnell, 2015; MacLean et al., 2018; Worzfeld et al., 2017), and tens of thousands of cel-
lular measurements that could be taken (Hasin et al., 2017; Chappell et al., 2018), the combinatorial
space of biological exploration spans hundreds of billions of potential experimental configurations,
and vastly exceeds the experimental capacity of even the world’s largest research institutes. Machine
learning methods, such as active and reinforcement learning, could potentially aid in optimally ex-
ploring the space of genetic interventions by prioritising experiments that are more likely to yield
mechanistic insights of therapeutic relevance (Figure 1), but, given the lack of openly accessible
curated experimental benchmarks, there does not yet exist to date a concerted effort to leverage the
machine learning community for advancing research in this important domain.
To bridge the gap between machine learning researchers versed in causal inference and the challeng-
ing task of biological exploration, we introduce GeneDisco, an open benchmark suite for evaluating
batch active learning algorithms for experimental design in drug discovery. GeneDisco consists of
1
Published as a conference paper at ICLR 2022
OBUild
COUnterfactUal Estimators
CovariateS X ‹χχ	Experimental
do(t = T)-θζy0~ outcome y
COUnterfaCtUaI
Experimental
Predictions
Design
[2, Generate Hypotheses
control
interventi
Experimental
Data
Figure 1: In the setting considered in this work, Counterfactual estimators of experimental out-
comes (step 1, left) are used to propose experimental hypotheses (step 2, center) for validation in
in vitro experiments with genetic interventions (step 3, right), such as CRISPR knockouts, in order
to discover potential causal associations between biological entities that could be relevant for the
development of novel therapeutics. The trained counterfactual estimators can be used to direct the
experimental search towards the space of biological interest, and thus more efficiently explore the
vast space of genetic interventions. After every cycle, experimental data are generated that could
lead to mechanistic scientific discoveries forming the basis for new therapeutics development, and
guide subsequent experiment cycles with enhanced counterfactual estimators.
several curated datasets, tasks and associated performance metrics, open implementations of state-
of-the-art active learning algorithms for experimental design, and an accessible open-source code
base for evaluating and comparing new batch active learning methods for biological discovery.
Concretely, the contributions presented in this work are as follows:
•	We introduce GeneDisco, an open benchmark suite for batch active learning for drug dis-
covery that provides curated datasets, tasks, performance evaluation and open source im-
plementations of state-of-the-art algorithms for experimental exploration.
•	We perform an extensive experimental baseline evaluation that establishes the relative per-
formance of existing state-of-the-art methods on all the developed benchmark settings using
a total of more than 20 000 central processing unit (CPU) hours of compute time.
•	We survey and analyse the current state-of-the-art of active learning for biological explo-
ration in the context of the generated experimental results, and present avenues of height-
ened potential for future research based on the developed benchmark.
2	Related Work
Background. Drug discovery is a challenging endeavour with (i) historically low probabilities of
successful development into clinical-stage therapeutics (Hay et al., 2014; Wong et al., 2019), and,
for many decades until recently (Ringel et al., 2020), (ii) declining industry productivity commonly
referred to as “Eroom’s law” (Scannell et al., 2012). Seminal studies by Nelson et al. (2015) and
King et al. (2019) respectively first reported and later independently confirmed that the probability
of clinical success of novel therapeutics increases up to three-fold if a medicine’s molecular target
is substantiated by high-confidence causal evidence from genome-wide association studies (GWAS)
Visscher et al. (2017). With the advent of molecular technologies for genetic perturbation, such as
Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) (Jehuda et al., 2018), there
now exist molecular tools for establishing causal evidence supporting the putative mechanism of
a potential therapeutic hypothesis by means of in vitro experimentation beyond GWAS early on
during the target identification and target validation stages of drug discovery (Rubin et al., 2019;
Itokawa et al., 2016; Harrer et al., 2019; Vamathevan et al., 2019). Among other applications (Chen
et al., 2018; Ekins et al., 2019; Mak & Pichika, 2019), machine learning methods, such as active and
reinforcement learning, could potentially aid in discovering the molecular targets with the highest
therapeutic potential faster.
Machine Learning for Drug Discovery. There exist a number of studies that proposed, advanced or
evaluated the use of machine learning algorithms for drug discovery: Costa et al. (2010) used deci-
sion tree meta-classifier to identify genes associated with morbidity using protein-protein, metabolic,
transcriptional, and sub-cellular interactions as input. Jeon et al. (2014) used a support vector ma-
chine (SVM) on protein expressions to predict target or non-target for breast pancreatic or ovarian
cancers, and Ament et al. (2018) used least absolute shrinkage and selection operator (LASSO)
2
Published as a conference paper at ICLR 2022
regularised linear regression on transcription factor binding sites and transcriptome profiling to pre-
dict transcriptional changes associated with Huntington’s disease. In the domain of human muscle
ageing, Mamoshina et al. (2018) used a SVM on deep features extracted from gene expression sig-
natures in tissue samples from young and old subjects to discover molecular targets with putative
involvement in muscle ageing. More recently, Stokes et al. (2020) utilised deep learning to discover
Halicin as a repurposed molecule with antibiotic activity in mice.
Reinforcement and Active Learning. The use of deep reinforcement learning for de novo molec-
ular design has been extensively studied (Olivecrona et al., 2017; Popova et al., 2018; Putin et al.,
2018; Lim et al., 2018; Blaschke et al., 2020; Gottipati et al., 2020; Horwood & Noutahi, 2020).
Active learning for de novo molecular design has seen less attention (Dixit et al., 2016; Green et al.,
2020), however, active learning for causal inference has seen increasing application toward causal-
effect estimation (Deng et al., 2011; Schwab et al., 2018; Sundin et al., 2019; Schwab et al., 2020;
Bhattacharyya et al., 2020; Parbhoo et al., 2021; Qin et al., 2021; Jesson et al., 2021; Chau et al.,
2021), and causal graph discovery (Ke et al., 2019; Tong & Koller, 2001; Murphy, 2001; Hauser &
Buhlmann, 2014; Ghassami et al., 2018; Ness et al., 2017; AgraWaI et al., 20l9; Lorch et al., 2021;
Annadani et al., 2021; Scherrer et al., 2021).
Benchmarks. Benchmark datasets play an important role in developing machine learning method-
ologies. Examples include ImageNet (Deng et al., 2009) or MSCOCO (Lin et al., 2014) for com-
puter vision, as Well as cart-pole (Barto et al., 1983) or reinforcement learning (Ahmed et al., 2020).
Validation of active learning for causal inference methods depends largely on synthetic data exper-
iments due to the difficulty or impossibility of obtaining real World counterfactual outcomes. For
causal-effect active learning, real World data With synthetic outcomes such as IHDP (Hill, 2011) or
ACIC2016 Dorie et al. (2019) are used. For active causal discovery, in silico data such as DREAM4
(Prill et al., 2011) or the gene regulatory netWorks proposed by Marbach et al. (2009) are used. Non
synthetic data has been limited to protein signalling netWorks (Sachs et al., 2005) thus far. Several
benchmarks focus on some aspects of drug discovery, for instance BroWn et al. (2019) focusing on
de novo molecular screening. HoWever, to our knoWledge, no other benchmark addressing the target
identification part of the drug discovery pipeline, Which is What our Work focuses on.
In contrast to existing Works, We develop an open benchmark to evaluate the use of machine learning
for efficient experimental exploration in an iterative batch active learning setting. To the best of our
knoWledge, this is the first study (i) to introduce a curated open benchmark for the challenging
task of biological discovery, and (ii) to comprehensively survey and evaluate state-of-the-art active
learning algorithms in this setting.
3	Methodology
Problem Setting. We consider the setting in Which We are given a dataset consisting of covariates
X ∈ Rp With input feature dimensionality p ∈ N and treatment descriptors T ∈ Rq With treatment
descriptor dimensionality q ∈ N+ that indicate similarity betWeen interventions. Our aim is to
estimate the expectation of the conditional distribution ofan unseen counterfactual outcome Yt ∈ R
given observed covariates X = X and intervention dο(T = t), yt = g(X = x,do(T = t)) ≈
E[Y | X = x, do(T = t)]. This setting corresponds to the Rubin-Neyman potential outcomes
frameWork (Rubin, 2005) adapted to the context of genetic interventions With a larger number of
parametric interventions. In the context of a biological experiment With genetic interventions, yt is
the experimental outcome relative to a non-interventional control (e.g., change in pro-inflammatory
effect) that is measured upon perturbation of the cellular system With intervention t, x is a descriptor
of the properties of the model system and/or cell donor (e.g., the immuno-phenotype of the cell
donor), and t is a descriptor of the genetic intervention (e.g., a CRISPR knockout on gene STAT1)
that indicates similarity to other genetic interventions that could potentially be applied. In general,
certain ranges of yt may be preferable for further pursuit of an intervention T = t that inhibits
a given molecular target - often, but not necessarily alWays, larger absolute values that move the
experimental result more are of higher mechanistic interest. We note that the use of an empty
covariate vector X = x0 With p = 0 is permissible if all experiments are performed in the same
model system With the same donor properties. In in vitro experimentation, the set of all possible
genetic interventions Dpool = {ti}in=po1ol is typically knoWn a-priori and of finite size (e.g., knockout
interventions on all 20 000 protein-coding genes).
3
Published as a conference paper at ICLR 2022
Batch Active Learning. In the considered setting, reading out additional values for yet unexplored
interventions t requires a lab experiment and can be costly and time-consuming. Lab experiments
are typically conducted in parallelised manner, for example by performing a batch of multiple inter-
ventions at once in an experimental plate. Our overall aim is to leverage the counterfactual estimator
gb trained on the available dataset to simulate future experiments with the aim of maximising an
objective function L in the next iteration with as few interventions as possible. For the purpose
of this benchmark, we consider the counterfactual mean squared error (MSE) of gb in predicting
experimentally observed true outcomes yt from predicted outcomes yt as the optimisation objec-
tive LMSE = MSE(yt, yt). Depending on context, other objective functions, such as for example
maximising the number of discovered molecular targets with certain properties of interest (e.g.,
druggability (Keller et al., 2006)) may also be sensible in the context of biological exploration. At
every time point, a new counterfactual estimator gb is trained with the entire available experimental
dataset, and used to propose the batch of b interventions to be explored in the next iteration with
the batch size b ∈ N+ . When using LMSE, this setting corresponds to batch active learning with the
optimisation objective of maximally improving the counterfactual estimator gb.
Acquisition Function. An acquisition function Dk = α(g(t), Dakvail) takes the model and the set
of all available interventions Dakvail in cycle k as input and outputs the set of interventions Dk that
are most informative after the kth experimental cycle with cycle index k ∈ [K] = [0 . . K] where
K ∈ N+ is the maximum number of cycles that can be run. Formally speaking, the acquisition
function α : P(Davail) × G → P(Davail) takes a subset of all possible interventions that have not been
tried so far (Davail), together with the trained model (gb) derived from the cumulative data collected
over the previous cycles, and outputs a subset of the available interventions Dk that are likely to be
most useful under L to obtain gb ∈ G as a better estimate of E[Y | X = x, do(T = t)] with G being
the space of the models which can be, e.g. the space of models and (hyper-)parameters.
4	Datasets, Metrics & Baselines
The GeneDisco benchmark curates and standardizes two types of datasets: three standardized feature
sets describing interventions t (inputs to counterfactual estimators; Section 4.1), and four different in
vitro genome-wide CRISPR experimental assays (predicted counterfactual outcomes; Section 4.2),
each measuring a specific outcome yt following possible interventions T . We perform an extensive
evaluation across these datasets, leveraging two different model types (Section 4.3) and nine differ-
ent acquisition functions (Section 4.4). Since all curated assay datasets contained only outcomes for
only one model system, we used the empty covariate set X = x0 for all evaluated benchmark con-
figurations. The metrics used to evaluate the various experimental conditions (acquisition functions
and model types) include model performance (Figure 2) and the ratio of discovered interesting hits
(Figure 3) as a function of number of samples queried.
4.1	Treatment Descriptors
The treatment descriptors T characterize a genetic intervention and generally should correspond to
data sources that are informative as to a genes’ functional similarity - i.e. defining which genes if
acted upon, would potentially respond similarly to perturbation. The treatment descriptor T is the
input to the model described in Section 3 for a fixed vector of covariates x. Any dataset considered
for use as a treatment descriptor must be available for all potentially available interventions Dpool in
the considered experimental setting. In GeneDisco, we provide three standardised gene descriptor
sets for genetic interventions, and furthermore enable users to provide custom treatment descriptors
via a standardised programming interface:
Achilles. The Achilles project generated dependency scores across cancer cell lines by assaying 808
cell lines covering a broad range of tissue types and cancer types (Dempster et al., 2019). The genetic
intervention effects are based on interventional CRISPR screens performed across the included cell
lines. When using the Achilles treatment descriptors, each genetic intervention is summarized using
a gene representation T with q = 808 corresponding to the dependency scores measured in each
cell line. In Achilles, after processing and normalisation (see Dempster et al. (2019)), the final
dependency scores provided are such that the median negative control (non-essential) gene effect
for each cell line is 0, and the median positive control (essential) gene effect for each cell line
is -1. The rationale for using treatment descriptors based on the Achilles dataset is that genetic
4
Published as a conference paper at ICLR 2022
effects measured across the various tissues and cancer types in the 808 cell line assays included in
(Dempster et al., 2019) could serve as a similarity vector in functional space that may extrapolate to
other biological contexts due to its breadth.
Search Tool for the Retrieval of Interacting Genes/Proteins (STRING) Network Embeddings.
The STRING (Szklarczyk et al., 2021) database collates known and predicted protein-protein in-
teractions (PPIs) for both physical as well as for functional associations. In order to derive a vec-
tor representation suitable to serve as a genetic intervention descriptor T , we utilised the network
embeddings of the PPIs contained in STRING as provided by Cho et al. (2016; 2015) with dimen-
sionality q = 799. PPI network embeddings could be an informative descriptor of functional gene
similarity since proteins that functionally interact with the same network partners may serve similar
biological functions (Vazquez et al., 2003; Saha et al., 2014).
Cancer Cell Line Encyclopedia (CCLE). The CCLE (Nusinow et al., 2020) project collected quan-
titative proteomics data from thousands of proteins by mass spectrometry across 375 diverse cancer
cell lines. The generated protein quantification profiles with dimensionality q = 420 could indicate
similarity of genetic interventions since similar expression profiles across a broad range of biological
contexts may indicate functional similarity.
Custom Treatment Descriptors. Additional, user-defined treatment descriptors can be evaluated
in GeneDisco by implementing the standardised dataset interface provided within.
4.2	Assays
As ground-truth interventional outcome datasets, we leverage various genome-wide CRISPR
screens, primarily from the domain of immunology, that evaluated the causal effect of intervening
on a large number of genes in cellular model systems in order to identify the genetic perturbations
that induce a desired phenotype of interest. In connection with the problem formulation in Sec-
tion 3, the CRISPR screens are the random variables whose realized values are the outcome of the
interventional experiments. In the following, we add more details about four CRISPR screens.
4.2.1	Regulation of Human T cells proliferation
Experimental setting. This assay is based on Shifrut et al. (2018). After isolating CD8+ T cells
from human donors, Shifrut et al. (2018) performed a genome-wide loss-of-function screen to iden-
tify genes that impact the proliferation of T cells following stimulation with T cell receptors.
Measurement. The measured outcome is the proliferation of T cells in response to T cell receptor
stimulation. Cells were labeled before stimulation with CFSE (a fluorescent cell staining dye).
Proliferation of cells is measured 4 days following stimulation by FACS sorting (a flow cytometry
technique to sort cells based on their fluorescence).
Importance. Human T cells play a central role in immunity and cancer immunotherapies. The
identification of genes driving T cell proliferation could serve as the basis for new preclinical drug
candidates or adoptive T cell therapies that help eliminate cancerous tumors.
4.2.2	Interleukin-2 production in primary human T cells
Experimental setting. This dataset is based on a genome-wide CRISPR interference (CRISPRi)
screen in primary human T cells to uncover the genes regulating the production of Interleukin-2
(IL-2). CRISPRi screens test for loss-of-function genes by reducing their expression levels. IL-2
is a cytokine produced by CD4+ T cells and is a major driver of T cell expansion during adaptive
immune responses. Assays were performed on primary T cells from 2 different donors. The detailed
experimental protocol is described in Schmidt et al. (2021).
Measurement. Log fold change (high/low sorting bins) in IL-2 normalized read counts (averaged
across the two biological replicates for robustness). Sorting was done via flow cytometry after
intracellular cytokine staining.
Importance. IL-2 is central to several immunotherapies against cancer and autoimmunity.
5
Published as a conference paper at ICLR 2022
4.2.3	INTERFERON-γ PRODUCTION IN PRIMARY HUMAN T CELLS
Experimental setting. This data is also based on Schmidt et al. (2021), except that this experiment
was performed to understand genes driving production of Interferon-γ (IFN-γ). IFN-γ is a cytokine
produced by CD4+ and CD8+ T cells that induces additional T cells.
Measurement. Log fold change (high/low sorting bins) in IFN-γ normalized read counts (aver-
aged across the two biological replicates for robustness).
Importance. IFN-γ is critical to cancerous tumor killing and resistance to IFN-γ is one escape
mechanism for malignant cells.
4.2.4	Vulnerability of Leukemia cells to NK cells
Experimental setting. This genome-wide CRISPR screen was performed in the K562 cell line to
identify genes regulating the sensitivity of leukemia cells to cytotoxic activity of primary human NK
cells. Detailed protocol is described in Zhuang et al. (2019).
Measurement. Log fold counts of gRNAs in surviving K562 cells (after exposition to NK cells)
compared to control (no exposition to NK cells). Gene scores are normalized fold changes for all
gRNAs targeting this gene.
Importance. Better understanding and control over the genes that drive the vulnerability of
leukemia cells to NK cells will help improve anti-cancer treatment efficacy for leukemia patients,
for example by preventing relapse during hematopoeitic stem cell transplantation.
4.3	Models
Parametric or non-parametric models can be used to model the conditional expected outcomes,
E[Y | X = x, do(T = t)]. Parametric models assume that the outcome Y has density f(y | t, ω)
conditioned on the intervention t and the parameters of the model ω (we drop x0 for compact-
ness). A common assumption for continuous outcomes is a Gaussian distribution with density
f(y | t, ω) = N(y | gb(t; ω), σ2), which assumes that y is a deterministic function of gb(t; ω) with ad-
ditive Gaussian noise scaled by σ2 . Bayesian methods treat the model parameters ω as instances of
the random variable Ω ∈ W and aim to model the posterior density of the parameters given the data,
f (ω | D). For high-dimensional, large-sample data, such as we explore here, a variational approxi-
mation to the posterior is often used, q(ω | D) (MacKay, 1992; Hinton & Van Camp, 1993; Barber
& Bishop, 1998; Gal & Ghahramani, 2016). In this work we use Bayesian Neural Networks (BNNs)
to approximate the posterior over model parameters. A BNN gives bk-1(t) = ml Pjm=I b(t; ωj-1),
where gb(t; ωjk-1) is a unique functional estimator of E[Y | X = x, do(T = t)] induced by
ωj-1 〜 q(ω | DkmI1): a sample from the approximate posterior over parameters given the cu-
mulative data at acquisition step k - 1. We also use non-parametric, non-Bayesian, Random Forest
Regression (Breiman, 2001). A Random Forest gives gk-1(t) = mm Pjm=I gj-1(t), where gjk-1(t)
is a unique functional estimator of E[Y | X = x, do(T = t)] indexed by the jth sample in the
ensemble of m trees trained on Dcku-m1).
We emphasize two main assumptions behind the above modeling choices for the problem formu-
lation of Section 3: (1) A causal link between from the gene intervention to the phenotype screen
exists. (2) Gaussian noise is a reasonable choice for the output noise as the interventional outcome
is a continuous variable. We leave it to future work to explore heterogeneity in output noise and
other likelihood functions.
In the following, we will define our acquisition functions in terms of parametric models, but the
definitions are easily adapted for non-parametric models as well.
4.4	Acquisition Functions
We included a diverse set of the existing active learning methods to serve as an informative set of
baselines for comparison and intuition activation for future improvements. Most of the Different
acquisition function methods can be categorized in the following way:
6
Published as a conference paper at ICLR 2022
•	Random: Random acquisition function is an integral part of whatever baseline set in active
learning’s performance criteria.
•	Uncertainty-based acquisition functions: These categories are popular because of
their easy-to-develop and low-computational properties while having great performance.
Uncertainty-based functions usually are employed in the shallow models that some sense
of intrinsic uncertainty exists for them, e.g., random forests. Here, we adapted some of
them to our notion of uncertainty (based on our regression task). These functions include
topuncertain and Margin Sample.
•	Deep Bayesian Active Learning acquisition functions: One way to bridge between classical
uncertainty-based algorithms and deep active learning methods is to leverage deep bayesian
active learning. Bayesian Active Learning by Disagreement (BALD) calculates mutual
information between samples and model parameters as a definition for uncertainty.
•	Density-based acquisition functions: These functions try to select a sample that is the best
representative of the whole dataset in terms of diversity. Based on different repesentativee
power measures various methods exist that here Coreset and kmeansdata are covered.
•	Adversarial learning attacks: By choosing adversarial samples as nominated samples for
active learning, one may use most of the adversarial learning algorithms as acquisition
functions. We implemented AdvBIM as a compelling example of this category.
•	Hybrid acquisition functions: These methods take into account both sample diversity and
also prediction uncertainty of the model. In other words, they increase the diversity of
data samples according to model properties. BADGE considers the gradient of the loss
function w.r.t weights of the final layer while kmeansembed looks at the data from their
embeddings in the last layer of the deep learning model.
Each one of these acquisition functions (Random, BADGE, BALD (topuncertain and
softuncertain)), Coreset, Margin Sample (Margin), Adversarial Basic Iteractive Method
(AdvBIM), k-means Sampling (kmeansdata and kmeansembed)) included in the benchmark
are described in detail in Appendix B.
5	Experimental Evaluation
Setup. In order to assess current state-of-the-art methods on the GeneDisco benchmark, we per-
form an extensive baseline evaluation of 9 acquisition functions, 6 acquisition batch sizes and 4
experimental assays using in excess of 20 000 CPU hours of compute time. Due to the space limit,
we include the results for 3 batch sizes in the main text and present the results for all batch sizes
in the appendix. The employed CoUnterfactual estimator g is a multi-layer perceptron (MLP) that
has one hidden layer with ReLU activation and a linear output layer. The size of the hidden layer
is determined at each active learning cycle by k-fold cross validation against 20% of the acquired
batch. At each cycle, the model is trained for at most 100 epochs but early stopping may interrupt
training earlier if the validation error does not decrease. Each experiment is repeated with 5 random
seeds to assess experimental variance. To choose the number of active learning cycles, we use the
following strategy: the number of cycles are bounded to 40 for the acquisition batches of sizes 16,
32 and 64 due to the computational limits. For larger batch sizes, the number of cycles are reduced
proportionally so that the same total number of data points are acquired throughout the cycles. At
each cycle, the model is trained from scratch using the data collected up to that cycle, i.e. a trained
model is not transferred to the future cycles. The test data is a random 20% subset of the whole data
that is left aside before the active learning process initiates, and is kept fixed across all experimental
settings (i.e., for different datasets and different batch sizes) to enable a consistent comparison of
the various acquisition functions, counterfactual estimator and treatment descriptor configurations.
Results. The model performance based on the STRING treatment descriptors for different acquisi-
tion functions, acquisition batch sizes and datasets are presented in Figure 2. The same metrics in
the Achilles and CCLE treatment descriptors are provided in Appendix C. To showcase the effect
of the model class, we additionally repeat the experiments using a random forest model as an un-
certainty aware ensemble model with a reduced set of acquisition functions that are compatible with
non-differentiable models (Appendix C). The random forest model is also used in another set of
experiments with other datasets (Sanchez et al., 2021; Zhu et al., 2021) which are fully postponed to
7
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg) Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 256	batch size = 64	batch size = 16	batch size = 256 batch size = 64	batch size = 16
OnofAcqiXelton RmCeone (Shlhitetei.2018)
0期
Q 40
CIaf
。於
Oa
α.ιe
→- stfU>ιowW∏
kπwιsdata
* kmwιwπb0d
• ErBh
- 口 t
ad*βM
040
。就
Oa)
UJ
OB
asa
0.1«
ðθ IeO 38	480	6«
Nwnber of Swπ"e∙ (N)
ConpwlSWI OfActuIeIIai Fuictlwis (S⅛ιrrtdtβ,⅛. 2021 (FNg))
ICfWnrtakI
-*- ∙CΛ>ιoβ<⅛ln
ma⅛n
n⅛e
Bd>eM
oɪ
Numwrof Sarτφ(eβ{N)
Corrparleon of AcqiJtftIon Fmclone (Sc⅛rtcftβtai.2021 (IL∙2 )
._ J__________ , →
-→- Immsdata
■— ɪ--------1"
046
(MO
036
03。
C£6
Oz)
门厂-「TT
M ιeo asα «0 e«
Nurτ⅛er of Sendee (N)
320 64Q	128	19t0 tW>
Nwnbsrof Swrf>lw (N)
1.76
1.70
1£3
1Λ0
1«S
1疑
Compwleon OfAcqUeHon Fsctlorw (Zhueng et d. 2019)
Figure 2: The evaluation of the model trained with STRING treatment descriptors at each active
learning cycle for 4 datasets and 3 acquisition batch sizes. In each plot, the x-axis is the active
learning cycles multiplied by the acquisition bath size that gives the total number of data points
collected so far. The y-axis is the test MSE error evaluated on the test data.
Shifrut et al. 2018
D.17β
0.1∞
0.1a
f O-W
somβ
OMO
0«M
OOX
Corrparisoi OtAcquIatkxi Functkxis (Sritut MSJ- 2018)
320 MO	1290	1920 2ββ0
Nι>rt>βr of Ssmee (N)
O? S
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
Om
0»
(W
Oa)
M 1βQ 320	4β0	640
H*nber of Swτ∣>le< (N)
ICfmoelMrI
-X- ⅝Λu3t⅛,
-*- 3，EBlJa⅛
―*— kjn⅞uuaιiAj
—*—
Oorwet
badge
KhSM
_ corrφβrtson OfAcqUsNon FUnClOnS (Schmidt β,⅛. 2021 (IFNg))
0.200
o.oso -5^^***.
0.026	彦X
0.000
320 WO	1290	1920
Nι›τ⅛βr of SarrBlee (N)
2«0
Comperteon OfAC<M01on F⅛κ⅛onβ (Sc⅛nlββtβi.2021 (IU2))
O.Qβ
0.∞
(MM
i
≈0∞
ɪ
0.8
0.01
0.∞
80 IeO 3≡0	«0 TO
NUrber of Sarafce(N)
NumJxrof
f-X
Figure 3: The hit ratio of different acquisition for BNN model, different target datasets, and different
acquisition batch sizes. We use STRING treatment descriptors here. The x-axis shows the number
of data points collected so far during the active learning cycles. The y-axis shows the ratio of the set
of interesting genes that have been found by the acquisition function up until each cycle.
8
Published as a conference paper at ICLR 2022
the appendix due to the space constraint. To investigate the types of genes chosen by different acqui-
sition functions, we defined a subset of potentially interesting genes as the top 5% with the largest
absolute target value. These are the genes that could potentially be of therapeutic value due to their
outsized causal influence on the phenotype of interest. The hit ratio out of the set of interesting
genes chosen by different acquisition functions are presented in Figure 3 for the STRING treatment
descriptors, and in Appendix C for Achilles and CCLE. Benchmark results of interest include that
model-independent acquisition methods using diversity heuristics (random, kmeansdata) perform
relatively better in terms of model improvement than acquisition functions based on model uncer-
tainty (e.g., topuncertain, softuncertain) when using lower batch acquisition sizes than in regimes
with larger batch acquisition sizes potentially due to diversity being inherently higher in larger batch
acquisition regimes due to the larger set of included interventions in an intervention space with a
limited amount of similar interventions. Notably, while diversity-focused, model-independent ac-
quisition functions, such as random and kmeansdata, perform well in terms of model performance,
they underperform in terms of interesting hits discovered as a function of acquired interventional
samples (Figure 3). Based on these results, there appears to be a trade-off between model improve-
ment and hit discovery in experimental exploration with counterfactual estimators that may warrant
research into approaches to manage this trade-off to maximize long-term discovery rates.
6	Discussion and Conclusion
The ranking of acquisition functions in GeneDisco depends on several confounding factors, such as
the choice of evaluation metric to compare different approaches, the characteristics of the dataset of
interest, and the choice of the model class and its hyperparameters. An extrapolation of results ob-
tained in GeneDisco to new settings may not be possible under significantly different experimental
conditions. There is a subtle interplay between the predictive strength of the model and the acqui-
sition function used to select the next set of interventions, as certain acquisition functions are more
sensitive to the ability of the model to estimate its own epistemic uncertainty. An important message
learned from Figures 2 and 3 is the observation that in active learning, the best acquisition function
is the one that gives a more accurate model with the same budget of experiments. However, in drug
discovery, learning an accurate predictive model has secondary importance. The primary objective
is to find the optimal set of targets for a potential medicine. This objective may be unaligned or even
in the opposite direction of training the predictive model. For example, even though the Kmeans
acquisition function that chooses the centroids in the data domain has a decent performance in the
active learning task (finding the most informative labeled dataset for training the predictive model),
it does not perform well in choosing the most interesting targets (those that moves the phenotype
maximally.) A potential reason could be the intuition that the interesting targets may be close to
each other in terms of the information content they provide to the predictive model. The acquisition
function then considers them as redundant and does not choose in the next iteration. Hence, AL
algorithms must be used with care in drug discovery. We postpone further study of a more suitable
objective function for the acquisition function to future work.
From a practical standpoint, GeneDisco assumes the availability of a labeled set that is sufficiently
representative to train and validate the different models required by the successive active learning
cycles. However, model validation might be challenging when this set is small (e.g., during the
early active learning cycles) or when the labelling process is noisy. Label noise is common in in-
terventional biological experiments, such as the ones considered in GeneDisco. Experimental noise
introduces additional trade-offs for consideration not considered in GeneDisco, such as choosing
the optimal budget allocation between performing experiment replicates (technical and biological)
to mitigate label noise or collecting more data points via additional active learning cycles.
GeneDisco addresses the current lack of standardised benchmarks for developing batch active learn-
ing methods for experimental design in drug discovery. GeneDisco consists of several curated
datasets for experimental outcomes and genetic interventions, provides open source implementa-
tions of state-of-the-art acquisition functions for batch active learning, and includes a thorough
assessment of these methods across a wide range of hyperparameter settings. We aim to attract
the broader active learning community with an interest in causal inference by providing a robust
and user-friendly benchmark that diversifies the benchmark repertoire over standard vision datasets.
New models and acquisition functions for batch active learning in experimental design are of criti-
cal importance to realise the potential of machine learning for improving drug discovery. As future
research, we aim to expand GeneDisco to enable multi-modal learning and support simultaneous
optimization across multiple output phenotypes of interest.
9
Published as a conference paper at ICLR 2022
Acknowledgement
PS, AM and SB are employees of GlaxoSmithKline (GSK) and PS is also a shareholder of GSK.
PN is supported by GSK and the UK Engineering and Physical Sciences Research Council (EPSRC
ICASE award no. 18000077).
Reproducibility S tatement
This work introduces a new curated and standardized benchmark, GeneDisco, for batch active learn-
ing for drug discovery. The benchmark includes four publicly available datasets, which have previ-
ously been published in a peer review process. Using a total of more than 20,000 central processing
unit (CPU) hours of compute time, we perform an extensive evaluation of state-of-the-art acquisition
functions for batch active learning on the GeneDisco benchmark, across a wide range of hyperpa-
rameters. To the best of our knowledge, this is the first comprehensive survey and evaluation of
active learning algorithms on real-world interventional genetic experiment data. Similar to develop-
ments in other fields e.g. for the learning of disentangled representations (Locatello et al., 2019) or
generative adversarial networks (Lucic et al., 2017), we hope that our large scale experiments across
a diverse set of real-world datasets provide an evidence basis to better understand the settings in
which different active learning approaches work or do not work for drug discovery applications.
All used models and acquisition functions are described in detail and referenced in Section 4.4 and
Section 4.3. For all introduced datasets, we include a detailed description and the details on the
train, test and validation splits at the beginning of Section 5.
For all experimental results we report the range of hyper-parameters considered and the methods of
selecting hyper-parameters as well as the exact number of training and evaluation runs (Appendix C).
We additionally provide error bars over multiple random seeds and the code was executed on a cloud
cluster with Intel CPUs. We provide detailed results for all investigated settings in the appendix (C).
References
Raj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and Caroline Uhler. Abcd-
strategy: Budgeted experimental design for targeted causal structure discovery. In The 22nd
International Conference on Artificial Intelligence and Statistics,pp. 3400-3409. PMLR, 2019.
Ossama Ahmed, Frederik Trauble, Anirudh Goyal, Alexander Neitz, YoshUa Bengio, Bernhard
Scholkopf, Manuel Wuthrich, and Stefan Bauer. Causalworld: A robotic manipulation bench-
mark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.
Seth A Ament, Jocelynn R Pearl, Jeffrey P Cantle, Robert M Bragg, Peter J Skene, Sydney R Coffey,
Dani E Bergey, Vanessa C Wheeler, Marcy E MacDonald, Nitin S Baliga, et al. Transcriptional
regulatory networks underlying gene expression changes in huntington’s disease. Molecular Sys-
tems Biology, 14(3):e7435, 2018.
Yashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Ben-
gio, and Stefan Bauer. Variational causal networks: Approximate bayesian inference over causal
structures. arXiv preprint arXiv:2106.07635, 2021.
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.
Deep batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint
arXiv:1906.03671, 2019.
David Barber and Christopher M Bishop. Ensemble learning in bayesian neural networks. Nato ASI
Series F Computer and Systems Sciences, 168:215-238, 1998.
Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,
(5):834-846, 1983.
Nurken Berdigaliyev and Mohamad Aljofan. An overview of drug discovery and development.
Future Medicinal Chemistry, 12(10):939-947, 2020.
10
Published as a conference paper at ICLR 2022
Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Ashwin Maran, and Vinodchandran N
Variyam. Learning and sampling of atomic interventions from observations. In International
Conference on Machine Learning, pp. 842-853. PMLR, 2020.
Thomas Blaschke, Ola Engkvist, Jurgen Bajorath, and Hongming Chen. Memory-assisted rein-
forcement learning for diverse molecular de novo design. Journal of Cheminformatics, 12(1):
1-17, 2020.
Leo Breiman. Random forests. Machine Learning, 45(1):5-32, 2001.
Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking
models for de novo molecular design. Journal of chemical information and modeling, 59(3):
1096-1108, 2019.
Lia Chappell, Andrew JC Russell, and Thierry Voet. Single-cell (multi) omics technologies. Annual
Review of Genomics and Human Genetics, 19:15-41, 2018.
SiU LUn Chau, Jean-FrangoiS Ton, Javier Gonzdlez, Yee Whye Teh, and Dino Sejdinovic. Bayesimp:
Uncertainty quantification for causal data fusion. arXiv preprint arXiv:2106.03477, 2021.
Hongming Chen, Ola Engkvist, Yinhai Wang, MarcUs Olivecrona, and Thomas Blaschke. The rise
of deep learning in drUg discovery. Drug Discovery Today, 23(6):1241-1250, 2018.
HyUnghoon Cho, Bonnie Berger, and Jian Peng. DiffUsion component analysis: Unraveling fUnc-
tional topology in biological networks. In International Conference on Research in Computa-
tional Molecular Biology, pp. 62-64. Springer, 2015.
HyUnghoon Cho, Bonnie Berger, and Jian Peng. Compact integration of mUlti-network topology for
fUnctional analysis of genes. Cell Systems, 3(6):540-548, 2016.
Pedro R Costa, Marcio L Acencio, and Ney Lemke. A machine learning approach for genome-wide
prediction of morbid and drUggable hUman genes based on systems-level data. In BMC Genomics,
volUme 11, pp. 1-15. Springer, 2010.
JoshUa M. Dempster, Jordan Rossen, Mariya Kazachkova, JoshUa Pan, GUillaUme KUgener, David E.
Root, and Aviad Tsherniak. Extracting biological insights from the project achilles genome-
scale crispr screens in cancer cell lines. bioRxiv, 2019. doi: 10.1101/720243. URL https:
//www.biorxiv.org/content/early/2019/07/31/720243.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255. Ieee, 2009.
KUn Deng, Joelle PineaU, and SUsan MUrphy. Active learning for personalizing treatment. In 2011
IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pp.
32-39. IEEE, 2011.
Michael Dickson and Jean PaUl Gagnon. Key factors in the rising cost of new drUg discovery and
development. Nature Reviews Drug Discovery, 3(5):417-429, 2004.
Michael Dickson and Jean PaUl Gagnon. The cost of new drUg discovery and development. Discov-
ery Medicine, 4(22):172-179, 2009.
Joseph A DiMasi, Henry G Grabowski, and Ronald W Hansen. Innovation in the pharmaceUtical
indUstry: new estimates of r&d costs. Journal of Health Economics, 47:20-33, 2016.
Atray Dixit, Oren Parnas, BiyU Li, Jenny Chen, Charles P FUlco, Livnat Jerby-Arnon, Nemanja D
Marjanovic, Danielle Dionne, Tyler BUrks, Raktima RaychowdhUry, et al. PertUrb-seq: dissecting
molecUlar circUits with scalable single-cell rna profiling of pooled genetic screens. Cell, 167(7):
1853-1866, 2016.
Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. AUtomated versUs do-it-
yoUrself methods for caUsal inference: Lessons learned from a data analysis competition. Statis-
tical Science, 34(1):43-68, 2019.
11
Published as a conference paper at ICLR 2022
Sean Ekins, Ana C Puhl, Kimberley M Zorn, Thomas R Lane, Daniel P Russo, Jennifer J Klein,
Anthony J Hickey, and Alex M Clark. Exploiting machine learning for end-to-end drug discovery
and development. Nature Materials,18(5):435-441, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050-1059,
2016.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In International Conference on Machine Learning, pp. 1183-1192. PMLR, 2017.
AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, and Elias Bareinboim. Budgeted ex-
periment design for causal structure learning. In International Conference on Machine Learning,
pp. 1724-1733. PMLR, 2018.
Sai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu,
Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, et al. Learning to navigate the
synthetically accessible chemical space using reinforcement learning. In International Conference
on Machine Learning, pp. 3668-3679. PMLR, 2020.
Darren VS Green, Stephen Pickett, Chris Luscombe, Stefan Senger, David Marcus, Jamel Mes-
lamani, David Brett, Adam Powell, and Jonathan Masson. Bradshaw: a system for automated
molecular design. Journal of Computer-aided Molecular Design, 34(7):747-765, 2020.
Stefan Harrer, Pratik Shah, Bhavna Antony, and Jianying Hu. Artificial intelligence for clinical trial
design. Trends in Pharmacological Sciences, 40(8):577-591, 2019.
Yehudit Hasin, Marcus Seldin, and Aldons Lusis. Multi-omics approaches to disease. Genome
Biology, 18(1):1-15, 2017.
Alain HaUser and Peter Buhlmann. TWo optimal strategies for active learning of causal models from
interventional data. International Journal of Approximate Reasoning, 55(4):926-939, 2014.
Michael Hay, David W Thomas, John L Craighead, Celia Economides, and Jesse Rosenthal. Clinical
development success rates for investigational drugs. Nature Biotechnology, 32(1):40-51, 2014.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217-240, 2011.
Geoffrey E Hinton and DreW Van Camp. Keeping the neural netWorks simple by minimizing the de-
scription length of the Weights. In Proceedings of the Sixth Annual Conference on Computational
Learning Theory, pp. 5-13, 1993.
Julien HorWood and Emmanuel Noutahi. Molecular design in synthetically accessible chemical
space via deep reinforcement learning. ACS omega, 5(51):32984-32994, 2020.
Neil Houlsby, Ferenc Huszdr, Zoubin Ghahramani, and Mgte Lengyel. Bayesian active learning for
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.
informa PLC. Pharma R&D Annual RevieW 2018. 2018. Accessed: 2021-10-03.
Kentaro ItokaWa, Osamu Komagata, Shinji Kasai, Kohei OgaWa, and Takashi Tomita. Testing the
causality betWeen cyp9m10 and pyrethroid resistance using the talen and crispr/cas9 technologies.
Scientific Reports, 6(1):1-10, 2016.
Ronen Ben Jehuda, Yuval Shemer, and Ofer Binah. Genome editing in induced pluripotent stem
cells using crispr/cas9. Stem Cell Reviews and Reports, 14(3):323-336, 2018.
Jouhyun Jeon, Satra Nim, Joan Teyra, Alessandro Datti, Jeffrey L Wrana, Sachdev S Sidhu, Jason
Moffat, and Philip M Kim. A systematic approach to identify novel cancer drug targets using
machine learning, inhibitor design and high-throughput screening. Genome Medicine, 6(7):1-18,
2014.
12
Published as a conference paper at ICLR 2022
Andrew Jesson, Panagiotis Tigas, Joost van Amersfoort, Andreas Kirsch, Uri Shalit, and Yarin
Gal. Causal-bald: Deep bayesian active learning of outcomes to infer treatment-effects from
observational data. Advances in Neural Information Processing Systems, 34, 2021.
Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard
Scholkopf, Michael C Mozer, Chris Pal, and YoshUa Bengio. Learning neural causal models
from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.
Thomas H Keller, Arkadius Pichota, and Zheng Yin. A practical view of ‘druggability’. Current
Opinion in Chemical Biology,10(4):357-361, 2006.
Emily A King, J Wade Davis, and Jacob F Degner. Are drug targets with genetic support twice as
likely to be approved? revised estimates of the impact of genetic support for drug mechanisms on
the probability of drug approval. PLoS Genetics, 15(12):e1008489, 2019.
Andreas Kirsch, Sebastian Farquhar, and Yarin Gal. A simple baseline for batch active learning with
stochastic acquisition functions, 2021.
Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,
2016.
Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based
on conditional variational autoencoder for de novo molecular design. Journal of Cheminformatics,
10(1):1-9, 2018.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolldr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
Conference on Computer Vision, pp. 740-755. Springer, 2014.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114-4124.
PMLR, 2019.
Lars Lorch, Jonas Rothfuss, Bernhard Scholkopf, and Andreas Krause. Dibs: Differentiable
bayesian structure learning. arXiv preprint arXiv:2105.11839, 2021.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural Compu-
tation, 4(3):448-472, 1992.
Adam L. MacLean, Tian Hong, and Qing Nie. Exploring intermediate cell states through the lens
of single cells. Current Opinion in Systems Biology, 9:32-41, 2018. ISSN 2452-3100. doi:
https://doi.org/10.1016/j.coisb.2018.02.009. URL https://www.sciencedirect.com/
science/article/pii/S2452310017302238. Mathematic Modelling.
Kit-Kay Mak and Mallikarjuna Rao Pichika. Artificial intelligence in drug development: present
status and future prospects. Drug Discovery Today, 24(3):773-780, 2019.
Polina Mamoshina, Marina Volosnikova, Ivan V Ozerov, Evgeny Putin, Ekaterina Skibina, Franco
Cortese, and Alex Zhavoronkov. Machine learning on human muscle transcriptomic data for
biomarker discovery and tissue-specific drug target identification. Frontiers in Genetics, 9:242,
2018.
Daniel Marbach, Thomas Schaffter, Claudio Mattiussi, and Dario Floreano. Generating realistic
in silico gene networks for performance assessment of reverse engineering methods. Journal of
Computational Biology, 16(2):229-239, 2009.
Kevin P Murphy. Active learning of causal bayes net structure. 2001.
Matthew R Nelson, Hannah Tipney, Jeffery L Painter, Judong Shen, Paola Nicoletti, Yufeng Shen,
Aris Floratos, Pak Chung Sham, Mulin Jun Li, Junwen Wang, et al. The support of human genetic
evidence for approved drug indications. Nature Genetics, 47(8):856-860, 2015.
13
Published as a conference paper at ICLR 2022
Robert Osazuwa Ness, Karen Sachs, Parag Mallick, and Olga Vitek. A bayesian active learning
experimental design for inferring signaling networks. In International Conference on Research
in Computational Molecular Biology, pp. 134-156. Springer, 2017.
David P Nusinow, John Szpyt, Mahmoud Ghandi, Christopher M Rose, E Robert McDonald III,
Marian Kalocsay, Judit Jane-VaIbUena, Ellen Gelfand, Devin K Schweppe, Mark Jedrychowski,
et al. Quantitative proteomics of the cancer cell line encyclopedia. Cell, 180(2):387-402, 2020.
Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo
design through deep reinforcement learning. Journal of Cheminformatics, 9(1):1-14, 2017.
Sonali Parbhoo, Stefan Bauer, and Patrick Schwab. NCoRE: Neural Counterfactual Representation
Learning for Combinations of Treatments. arXiv preprint arXiv:2103.11175, 2021.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Mihaela Pertea, Alaina Shumate, Geo Pertea, Ales Varabyou, Florian P Breitwieser, Yu-Chi Chang,
Anil K Madugundu, Akhilesh Pandey, and Steven L Salzberg. Chess: a new human gene catalog
curated from thousands of large-scale rna sequencing experiments reveals extensive transcrip-
tional noise. Genome Biology, 19(1):1-14, 2018.
Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo
drug design. Science Advances, 4(7):eaap7885, 2018.
Robert Prill, Julio Saez-Rodriguez, Leonidas Alexopoulos, Peter Sorger, and Gustavo Stolovitzky.
Crowdsourcing network inference: The dream predictive signaling network challenge. Science
Signaling, 4:mr7, 09 2011. doi: 10.1126/scisignal.2002212.
Evgeny Putin, Arip Asadulaev, Yan Ivanenkov, Vladimir Aladinskiy, Benjamin Sanchez-Lengeling,
Aldn Aspuru-Guzik, and Alex Zhavoronkov. Reinforced adversarial neural computer for de novo
molecular design. Journal of Chemical Information and Modeling, 58(6):1194-1204, 2018.
Tian Qin, Tian-Zuo Wang, and Zhi-Hua Zhou. Budgeted heterogeneous treatment effect estimation.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8693-8702.
PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/qin21b.
html.
Michael D Rawlins. Cutting the cost of drug development? Nature Reviews Drug Discovery, 3(4):
360-364, 2004.
Michael S Ringel, Jack W Scannell, Mathias Baedeker, and Ulrik Schulze. Breaking eroom’s law.
Nature Reviews Drug Discovery, 2020.
Dan Roth and Kevin Small. Margin-based active learning for structured output spaces. In European
Conference on Machine Learning, pp. 413-424. Springer, 2006.
Adam J Rubin, Kevin R Parker, Ansuman T Satpathy, Yanyan Qi, Beijing Wu, Alvin J Ong,
Maxwell R Mumbach, Andrew L Ji, Daniel S Kim, Seung Woo Cho, et al. Coupled single-
cell crispr screening and epigenomic profiling reveals causal gene regulatory networks. Cell, 176
(1-2):361-376, 2019.
Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469):322-331, 2005.
Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauffenburger, and Garry P Nolan. Causal
protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721):
523-529, 2005.
14
Published as a conference paper at ICLR 2022
Sovan Saha, Piyali Chatterjee, Subhadip Basu, Mahantapas Kundu, and Mita Nasipuri. Funpred-
1: Protein function prediction from a protein interaction network using neighborhood analysis.
Cellular and Molecular Biology Letters,19(4):675-691, 2014.
Carlos G Sanchez, Christopher M Acker, Audrey Gray, Malini Varadarajan, Cheng Song, Nadire R
Cochran, Steven Paula, Alicia Lindeman, Shaojian An, Gregory McAllister, et al. Genome-wide
crispr screen identifies protein pathways modulating tau protein levels in neurons. Communica-
tions biology, 4(1):1-14, 2021.
Jack W Scannell, Alex Blanckley, Helen Boldon, and Brian Warrington. Diagnosing the decline in
pharmaceutical r&d efficiency. Nature Reviews Drug Discovery, 11(3):191-200, 2012.
Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard
Scholkopf, Michael C Mozer, YoshUa Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning
neural causal models with active interventions. arXiv preprint arXiv:2109.02429, 2021.
Ralf Schmidt, Zachary Steinhart, Madeline Layeghi, Jacob W. Freimer, Vinh Q. Nguyen, Franziska
Blaeschke, and Alexander Marson. Crispr activation and interference screens in primary human t
cells decode cytokine regulation. bioRxiv, 2021. doi: 10.1101/2021.05.11.443701. URL https:
//www.biorxiv.org/content/early/2021/05/12/2021.05.11.443701.
Patrick Schwab, Lorenz Linhardt, and Walter Karlen. Perfect Match: A Simple Method for
Learning Representations For Counterfactual Inference With Neural Networks. arXiv preprint
arXiv:1810.00656, 2018.
Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann, and Walter Karlen. Learn-
ing Counterfactual Representations for Estimating Individual Dose-Response Curves. In AAAI
Conference on Artificial Intelligence, 2020.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.
Eric Shifrut, Julia Carnevale, Victoria Tobin, Theodore L. Roth, Jonathan M. Woo, Christina T. Bui,
P. Jonathan Li, Morgan E. Diolaiti, Alan Ashworth, and Alexander Marson. Genome-wide crispr
screens in primary human t cells reveal key regulators of immune function. Cell, 175(7):1958-
1971.e15, 2018. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.2018.10.024. URL https:
//www.sciencedirect.com/science/article/pii/S0092867418313333.
Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al.
A deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.
Iiris Sundin, Peter Schulam, Eero Siivola, Aki Vehtari, Suchi Saria, and Samuel Kaski. Active
learning for decision-making from imbalanced observational data. In International Conference
on Machine Learning, pp. 6046-6055. PMLR, 2019.
Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca Kirsch, Sampo
Pyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer Bork, et al. The string database in
2021: customizable protein-protein networks, and functional characterization of user-uploaded
gene/measurement sets. Nucleic Acids Research, 49(D1):D605-D612, 2021.
Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In International
Joint Conference on Artificial Intelligence, volume 17, pp. 863-869. Citeseer, 2001.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Cole Trapnell. Defining cell types and states with single-cell genomics. Genome Research, 25(10):
1491-1498, 2015.
Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee,
Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, et al. Applications of machine learn-
ing in drug discovery and development. Nature Reviews Drug Discovery, 18(6):463-477, 2019.
15
Published as a conference paper at ICLR 2022
Alexei Vazquez, Alessandro Flammini, Amos Maritan, and Alessandro Vespignani. Global protein
function prediction from protein-protein interaction networks. Nature biotechnology, 21(6):697-
700, 2003.
Peter M Visscher, Naomi R Wray, Qian Zhang, Pamela Sklar, Mark I McCarthy, Matthew A Brown,
and Jian Yang. 10 years of GWAS discovery: biology, function, and translation. The American
Journal of Human Genetics, 101(1):5-22, 2017.
Chi Heem Wong, Kien Wei Siah, and Andrew W Lo. Estimation of clinical trial success rates and
related parameters. Biostatistics, 20(2):273-286, 2019.
Thomas Worzfeld, Elke Pogge von Strandmann, Magdalena Huber, Till Adhikary, Uwe Wagner,
Silke Reinartz, and Rolf Muller. The unique molecular and cellular microenvironment of ovarian
cancer. Frontiers in Oncology, 7:24, 2017.
Yunkai Zhu, Fei Feng, Gaowei Hu, Yuyan Wang, Yin Yu, Yuanfei Zhu, Wei Xu, Xia Cai, Zhiping
Sun, Wendong Han, et al. A genome-wide crispr screen identifies host factors that regulate sars-
cov-2 entry. Nature communications, 12(1):1-11, 2021.
Xiaoxuan Zhuang, Daniel P. Veltri, and Eric O. Long. Genome-wide crispr screen reveals cancer
cell resistance to nk cells induced by nk-derived ifn-γ. Frontiers in Immunology, 10:2879, 2019.
ISSN 1664-3224. doi: 10.3389/fimmu.2019.02879. URL https://www.frontiersin.
org/article/10.3389/fimmu.2019.02879.
16
Published as a conference paper at ICLR 2022
A	Notations
Here is the list of notations used in this manuscript.
•	Dpool: The pool of unlabeled data.
•	Dakcq: Acquired data at k-th AL cycle.
•	Dtkrain: Cumulative training data after k-th cycle of AL.
•	Dval: Validation data.
•	Dtest: Held-out test data.
•	b: Acquisition batch size.
•	K : Total number of AL cycles.
•	k = [1, 2, . . . , K]: Index of the AL cycle.
•	[K] = [1,2,...,K].
•	e ∈ E: Inherent noise (aleatoric uncertainty).
•	T ∈ T: Treatment variable.
•	E[Y | X = x, do(T = t)]: The conditional expected outcomes.
•	g(t; ω): The model parameterised by ω ∈ Ω to estimate E[Y | X = x, do(T = t)].
•	X: random variable X with distribution X 〜F(X) and density f (x).
B Acquisition Functions cont.
Random. As a baseline we look at random acquisition. Random acquisition at cycle k can be seen
as uniformly sampling data from Dakvail:
1	navail
ɑRandomGbCI⑴，Davail) = {t1,...tb} ^ t ti; ----- .	.	(I)
navail i=1
Here, the acquisition function samples b elements without replacement from the set of navail ele-
ments. The set element (ti) is on the left of the semicolon, and the probability of the element being
acquired (nʒ) is on the right of the semicolon. This convention will be used again below.
BADGE. BADGE looks to maximize the diversity of acquired samples, but, in contrast to Coreset,
it additionally takes the uncertainty of the prediction into account (Ash et al., 2019). If the true
label y were observed, BADGE would proceed by maximizing the diversity of samples based on
the gradient of the loss function l with respect to the weights of the final layer of the most recently
trained model Wk-I ^- l(y, b(t; ω)). Intuitively, it asks how much would our parameters change
if we observed the labeled outcome for this example? However, the true label y is not yet observed.
Ash et al. (2019) explore BADGE in the classification setting. For a two class problem, where f(y |
t, ω) = Bernoulli(y | gb(t; ω)), they propose using the class with the highest predicted probability,
y = argmaxy∈0,1 f (y | t,ω), to approximate the gradient as ∂ωd-τl(b,b(t; ω)). This does not
directly translate to the regression setting, as under our modelling assumptions the y with the highest
predicted likelihood corresponds exactly to gb(t; ω), which would lead to a loss of zero, and gradients
of zero. As a starting point, we instead take yb as a random sample from f(y | t, ω) = N(y |
gb(t; ω), σ2). We then use the same k-means++ algorithm as Ash et al. (2019) to approximate:
αBADGE(gbk-1(t), Dakvail)
. A ∕∂l(b,b(" ωk-1)) ∂l(y,g(tj; ωk-1))∖	(2)
= argmin argmax argmin ∆ -----------------------------------,-------- Z --------
{tι,...tb}∈D3l ti∈D3l j∈Dkail∪Dk- 1	V	dω -	dωk-1	J
where ∆ is again the Euclidean distance.
Bayesian Active Learning by Disagreement (BALD). Given an uncertainty aware model, such as
a BNN or Random Forest we can now take an information theoretic approach to selecting interven-
tions from the pool data. Houlsby et al. (2011) frame active learning as looking to maximize the
17
Published as a conference paper at ICLR 2022
information gain about the model parameters if we observe the outcome Y = y given model inputs.
Formally, the information gain is given by the mutual information between the random variables Y
and Ω given the intervention t and acquired training data Dk-I UP until acquisition step k:
I(Y;C | t,Dkm1) = H (Y | t,Dkm1) - H (Y | Ω,t, Dkm1)
=H (Y | t,Dkm1) - Ef (ωlDcu-I)H(Y | ω,t).
Under the assumed model we have
I(Y;C | t,Dkm1)
1 ]	σ σ2 + Ef (ω∣Dk,m1) [b(t； ω)2] - Ef (ω∣Dk-1) [b(t；
2log (	σ2
which leads to the following estimator setting σ2 = 1
I(Y;C | t, Dku∏1) = 1log
b(t; ωj-1) - mm X b(t; ωj-1)!).
(3)
(4)
(5)
We look at two acquisition functions for BALD. First, we consider the naive batch acquisition αBALD
proposed by Gal et al. (2017) which acquires the the top b examples from Dakvail:
b
αBALD(bk-1(t), DkVaiI) =	argmax XI(Y;Q | ti, DkUm1).	⑹
{t1 ,...tb }∈Dakvail i=1
This method will be referred to as topuncertain in the plots later. And second, we consider
αSoftBALD which randomly samples b interventions from Dakvail weighted by a tempered softmax
function (Kirsch et al., 2021):
… k	f	exp (TempI(Y;C | ti,DkumL))	InaVan
aSoftBALD (b	(t), DaVaiI) = {tl,...tb} ^ t ti;------7-----ʌ------------TW ,	, (7)
[PMlexp (τ⅛pI(FΩ | ti,DkUnI))i=1
where Temp > 0 is a user defined constant. This method will be referred to as softuncertain
in the plots. As Temp → ∞, αSoftBALD will behaVe more like αRandom. And as Temp → 0, αSoftBALD
will behaVe more like αBALD.
Coreset. Coreset acquisition looks to maximize the diVersity of acquired samples. This is done by
finding the data points in DakVail that are furthest from the labelled data points in Dcku-m1. The robust
K-centers algorithm of Sener & SaVarese (2017) approximates a solution to:
αCORESET (gbk-L (t), DakVail) =	argmin argmax argmin	∆(ti , tj).	(8)
{t1 ,...tb}∈DakVail ti ∈DakVail tj ∈DakVail∪Dcku-m 1
Euclidean distances, ∆(ti, tj), are calculated between the output of the penultimate layer of gb(t; ω).
Margin Sample. Margin sampling is designed for classifiers where selection is based on the dis-
tance of a sample from the classifiers decision boundary (Roth & Small, 2006). As a proxy, the
difference between the predicted probability of the most and second most probable classes is used.
The distance between the most probable and the second most probable classes for a multi-class clas-
sification problem can be seen as how confident the model is about the label of that class. HoweVer,
The concept of a decision boundary is ill-defined for regression tasks. One option to approximate
margin sampling could be to model the aleatoric uncertainty of the model by predicting the condi-
tional Variance of the outcome σ2(t; ω) and select data based on the magnitude of this Value. Here,
we instead look at the difference in the maximum and minimum Values of the predicted outcome as
a measure of the model’s confidence and select data based on the magnitude of this Value. Formally,
we haVe
Mc(Y；Ω | ti, Dkm1)=	max (b(t; ωk-1)) - min (b(t; ωj-1)),	(9)
j∈{1,...m}	j∈{1,...m}
and the acquisition function:
b
αMargin(gbk-1(t), DakVail) =	argmax	X Mc(Y；Ω | ti, Dk-1).	(10)
{t1 ,...tb }∈DakVail i=1
Note that this approximation is similar to BALD under the assumption of a uniformly distributed
outcome: f(y | t, ω) = U(y | gb(t; ω)).
18
Published as a conference paper at ICLR 2022
Adversarial Basic Iteractive Method (AdvBIM). Some of the adversarial algorithms can act as
active learning acquisition functions by nominating the adversarial samples. Here, we extended the
famous Adversarial BIM method for our regression task as an example. BIM was introduced by
(Kurakin et al., 2016) to iteratively perturb adversarial samples to maximize the cost function J
subject to an lp norm constraint as
t(0) = t,i(i) = CIiPt,e俨T) + Sign(V^(-i) J (θ∕iT),y)))	(11)
(intermediate results are clipped to stay in e-neighbourhood of the primary data point t). This
teChnique byPasses the intraCtable Problem of finding the distanCe from the deCision boundary by
iteratively perturbing the features until crossing the boundary (Tramer et al., 2017). In our regression
task, we Perturb the features in the gradients’ direCtion to inCrease the Conditional varianCe of the
outcome, i.e.,
£(O) = t,t(i) = clipt,e(t(i-1) + Sign(V^(i-i) VaΓω(g(t,ω)))) for i = {1,..., m},	(12)
where |信 一 t∣∣2 < γ *∣∣t∣∣2 with the hyperparameter γ. After creating adversarial samples for each
data point in Dakvail , αAdversarialBIM acquires the samples by
αAdversarialBIM (gb	(t), Davail ) = U argmin∆dm),tj),	(13)
ti∈Dakvail tj∈Dakvail
where ∆ is the euclidean distance.
k-means Sampling. This method nominates samples by returning the closest sample to each cen-
ter of the unlabeled data clusters. In order to do so, one may run Kmeans++ clustering algorithm
with the number of clusters equal to b over either the unlabeled data points Dakvail or the output
of the penultimate layer of gb(t; ω). We refer to the former as kmeansdata and to the latter as
kmeansembed in the experiments. Assuming {μι,..., μb} are the centers of the clustering, We
have
b
ɑKmeans(bk-1 (t), DkVaiI) = U argmin∆(μi,tj),	(14)
i=1 tj ∈Dakvail
where ∆ is euclidean distance over the data points or the penultimate layer of gb(t; ω).
C Detailed experimental results
C.1 Bayesian Neural Network (BNN) Model
We provide here detailed experimental results across all hyperparameter settings. The result of fig. 2
that was presented for 3 batch sizes are provided for 6 batch sizes in fig. 4. Similarly, the results
of fig. 3 are provided for additional batch sizes in fig. 7. In addition, both fig. 2 and fig. 3 report
the results for the STRING treatment descriptors. All experiments are repeated for two other sets of
input treatment descriptors (Achilles and CCLE) whose results are provided in figs. 5, 6, 8 and 9.
C.2 Random Forest Model
In addition to the BNN model, we carried out thorough analyses for a different model class. The
experiments are repeated for the random forest as an uncertainty aware ensemble model. The un-
certainty in random forests, similar to other ensemble methods, is originated from the prediction
made by each model instance in the ensemble. We use the random forest implementation in the
Scikit-learn package (Pedregosa et al., 2011) with 100 trees and set the option max_depth=None so
that the depth of the trees are determined automatically. The performance of the model trained over
the active learning cycles can be seen in fig. 10 for different acquisition functions, different batch
sizes, different target datasets, and the STRING treatment descriptors. Similarly, the hit ratio of the
interesting genes fora random forest model is reported in fig. 12. The same experiment was repeated
for CCLE treatment descriptors whose results are provided in fig. 11 and fig. 13. Notice that random
forest experiments are done with a reduced set of acquisition functions that could be adjusted to the
random forest model.
19
Published as a conference paper at ICLR 2022
C.3 IN-DEPTH DESCRIPTION OF THE Hit Ratio EXPERIMENT
Here we elaborate more on the purpose and the message of the hit ratio experiment whose results
are reported in figs. 7 to 9, 12 and 13 for various settings. The purpose of these experiments is to
compare the performance of different acquisition functions in different settings of batch sizes and
input/output datasets to hit the gene targets that are known to be interesting by genomics experts.
To choose the set of interesting genes, we sort them based on their absolute target values. Then we
choose the top 5% of this list that corresponds to both extremes of positive and negative values (both
extremes are considered to be good targets by experts.) The experiments are repeated for 5 different
random seeds to obtain the error bars.
20
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
batch size = 512 batch size = 256 batch size = 128	batch size = 64	batch size = 32	batch size = 16
Corrparleon of AcqiJciIon Fkndone (Shl⅛utβt al. 2018)
ðθ 1β0	38	480	640
Mmber of Swndea (N)
dlon FUKlone (Sc⅛rtdt βt d. 2021 (IL-2 )
mβ⅛n
8BWt
badge
ad<SM
UpunowWn
∣πιes9data
IT「丁 STr
2Λ
1«
1«
1.7
1«
1Λ
13
Zhuang et al. 2019
C0π4>0⅛0π OfAcqueifcin Fimcfcirw (Zhuwig et al. 2018)
W 160 sa MO β*>
NUmfcer OfSeeISMN)
NmterofSampIea (N)
OM
OM
a.x
0.8
0.2S
a.ta
8	18	320	4β0	644
N∣Λτ⅛erofSampto((N)
Conpwteori o( Acquleroon
Fundorie (Shltnjt
Eβ0S
owwet
DadeQ
aΛSM
βtβJ.2D1β)
kme≡ι sdat≡
划 ¢40 Iitg 180
Hmbw of SSmNee (N)
04«
04φ
。邠
。3。
。26
020
04«
04φ
,。邠
03。
。26
020
Compeneon otAcqul⅛1on Functksne (Schmidt et aJ. 2021 (∣FN⅛)
ιeo a∞ «« 8o ιao
t*Λ∏bβr of Saπιdw (N)
划 ¢40 Iitg 1«» MW
Hmbw of SSmNee (N)
04«
(Mo
036
03。
026
02。
04«
(Mo
OaS
03。
026
02。
Ccmpwieon OfAcquleltIon Fundone (Schrridt βtβJ.2021 (IL-2))
1(0 32β	¢40	98 Iitg
t*Λ∏bβr of Sample* (N)
040
04«
030
02B
020
D.1C
(MO
03S
03。
。26
Oit
0.16
1A
1«
1.7
13
1∙6
1.4
ι.re
1.K)
1.βs
1.β9
ι.eβ
1.S0
160 320 W	9β0
Nun⅛er of Seiroies(N)
12»
Conpwteon o( Acquisition Fundone (Shlfrutet Si. 201β)
0	10M	204«	3072	408	«1»
NwnberofSanipIw (N)
Nimber of San(JIB (N)
040
04«
0«
02B
020
040
03«
Oat)
OsS
020
0 IOM 20«	3072 4O9β 6120
NuEjerofSaEee(N)
0400
Oae
038
03发
S8
02g
028
02X
028
Corrpwlson OfAcqulJiIcfi Rjndons (Scħrridt et«J. 2021 (IFNg))
0 IOM 20«	3072	40*
Nwnberof Santee (N)
MQ 1289	1«S0	268	3200
Nuni>erofSwπplee(N)
α.aβ
a.3«
o.aa
0.8
10.29
0.26
0.24
022
Corrparleon of ⅛<M0l6 Fι>κlonβ (Sc⅛rtclt et^.2Q21 (IL2))
∏≡tf∏
8B9<
badge
KbSN
ICVUnoVt^n
B0∣U>TCW⅛⅛1
→— tanβ≡wdat≡
ImwMiClQd
urn
Nς>τi>erofSwnpto((N)
Com(>αweπ ofAcqumκ>π Funcbone(Zhuwigeta- 2018)
1.β76
1.βS0
1.βS6
1.ββO
1.576
1.K0
1.S6
Comperieon of ΛcqUdt∣6 Firetlone (Zhueng et al. 2019)


Figure 4:	The evaluation of the model trained with STRING treatment descriptors at each active
learning cycle for 4 datasets and 6 acquisition batch sizes. In each plot, the x-axis is the active
learning cycles multiplied by the acquisition bath size that gives the total number of data points
collected so far. The y-axis is the test MSE error evaluated on the test data.
21
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch size = 256 batch size = 128	batch size = 64	batch size = 32	batch size =16
g
Oa
0.7
0£
≡α4
04
oa
02
Compsrleon of AcqiJeItIon FSetl6« (SHKit βt W. 2018)
ðθ 1β0	3∞	480	640
Nwnber of Swπp∣ea (N)
g
Oa
0.7
0£
a4
04
0»
02
0.1
CorTEeon σf Ac"dl6 HOra (Scħrtdt vtal.2021 (L2))
≡. E∏⅛π
融 nq∙rt*
≡=t
二 kl-
- i≡
8	1β0	320	480	640
NwπbsrofSwπplee(N)
Comperieon OfAcqUdtIon Fsctlone (Zhueng et al. 2019)
3R βM 12ββ 1920 25ββ
Nwter of Sarφee (N)
03e
03。
‘026
020
0.16
ComMrIeon of AcqUeItIon Fsdorw (SHfrut et ∙. 2018)
0	«K4	2M«	3072	«9«
Number ofSβr<ιleβ(N)
0.7
04
04
0»
02
3t0 β40	1290	1920 25ββ
N⅛>rtw of Swnplee (N)
0	1024	20«	3072	409β	6120
Number αfSvn^es (N)
NmterofSampIea (N)
32。 MQ 12ββ 1920	268
Nurrterof Swnplee (N)
MQ 1289	1«S0	268	3200
Nuni>erofSwπplee(N)
0邻。
OM
O加。
84。
OXK
OaaO
0	1β24 tM» 9m 40Sβ
Nwnberof Santee (N)
3«
3A
2«
20
1∙6
1Λ
Comperwon ofAcqu⅛⅛xι F⅛refcιre (ZhUHlg et al. 2019)
-→- random
-∙- toμιncotdn
------→- SOfcrwrtah
-→- kmeaιSdala
→- ImwaiMmbed
-mar⅛n
→- CtreaM
-badge
-→- adve∣M
320 640	1280	1»20	2W0
Nuιr⅛er of Senvies(N)
Comfimor ofAcqu⅛⅛>∏ FUndionB (Zhuwig et W. 2019)
140
1.75
1.70
1Λ5
1£O
1⅝O
640	1280	1920 2S6O SZ»
NumberafSerrftes(N)
1.TO
1鹿
1β)
1牌
CuiiwEnofAcqu⅛t⅛xι Findiore(ZhUalget域.2019)
0 1CB4 2CMβ 3072 4Wβ 5120
Nurr⅛er of Sarnies (N)
CθFr^sr⅛on of AcqUriIon F⅛retlonβ (2husng et al. 2019)
Nuntierof Sanf⅛b (N)


Figure 5:	The evaluation of the model trained with Achilles treatment descriptors at each active
learning cycle for 4 datasets and 6 acquisition batch sizes. In each plot, the x-axis is the active
learning cycles multiplied by the acquisition bath size that gives the total number of data points
collected so far. The y-axis is the test MSE error evaluated on the test data.
22
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
0z∞qosq ZE = 9z∞qoaq
Compsrleon of Acqulellon Funclone (SHtutetsi-2018)
174
160
124
10Λ
7Λ
64
ii
。4
M 1∞	32。	440	644
NlrtwrofSampIee (N)
Corrparleon of AcqiJtiIon Fι>κ1onβ (Sc⅛rtcft M d. 2021«L∙2))
C0τf>ais<rι OfAcqU⅛it⅛n Fmdiane (Zhueng et 域.2019)
2$
rsn⅛m
kφuncβ<t8>ι
• kmeβnsdata
→- Imeantenibed
msrgh
OreSet
Mdge
80	180	320	«0
Nurrter of SernBeS(N)
βw
寸9 = 0z∞qoaq
& H əz-s qaEq
9-= 0z∞qoaq
Zlg = 0z∞qaEq
CorrvsrtetXi of AcqlJdtlon FSCtlone (SH⅜utβtal.2018)
NwnberofSwnpee (N)
NwnberofSwnpee (N)
kmewwd≡⅛
⅛∏w wwwτι bed
ERn
OTW<
badge
IdvBM
OS
Oa
0.7
0£
α4
04
0»
02
0.1
Comperleon of Acquisition Fundone (Shlfrut βtal- 201q
Cσnpτ1eon o( Acquisition Fundone (Shlfrutet Si. 201 β)
Nimber of San(JIB (N)
3R βM 12ββ 1920 25ββ
N⅛>rtw of Swnplee (N)
Canpwteon OfAcquIeItIon Fundone(Sc⅛ricltetβJ.2021 (IL-2))
s
⅛
ω4
≡3
2
Carperison of Acquisiton Fuidnre (Zhuang et ei. 2018)
—Γ9idcm
tojMJ IltWtSill
∙, IoneeiiRlata
→- kmeβnwnbed
msrŋln
armat
hedge
-→- SdvBN
320 BW 12βO 1S»	2580
NlJrnbb of SemNee (N)
QA
OS
Carparison of Acquisiton Funcfcins (Zħueng etei. 2018)
22
2.1
2XI
0.7
0Λ
0Λ
Q4
Q4
Q2
ra∣dcr∣'∣
S
β
1
f
f
Nunbcr of Sanf>eB (N]
Compsrton OfAcqutaUon F⅛re<onβ (SArridtetai. 2021 (Ib2))
rw*m
SlMicatain
kmeanadata
-∙ km<βι sernbed
m∙⅛n
COre
badge
SdvBIM
Nwnbsr Cf Santee (N)
Nwnberof Santee (N)
-→- knwβns⅛ts
→- kmeenwnbed
WrgM
cxretef
Mdge
KIVeM
et el. 2019)
ran⅛m
lopι*ι «rta ⅛ι
kme srsdata
→- kroeanaemlxd
mar⅛n
anβ*t
badge
9dvBIM
Compwieon of⅛qj⅛t
ana
3072
Number of Senviee (N)
P
Figure 6:	The evaluation of the model trained with CCLE treatment descriptors at each active learn-
ing cycle for 4 datasets and 6 acquisition batch sizes. In each plot, the x-axis is the active learning
cycles multiplied by the acquisition bath size that gives the total number of data points collected so
far. The y-axis is the test MSE error evaluated on the test data.
23
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch size = 256 batch size =128	batch size = 64	batch size = 32	batch size =16
08
18 3M MO 98
NunbwofSsraIee (N)
Corrpwleon OtAcquIatkxi FuicIixis {Sħltut etsJ.2018)
320	«0	1280	1920	26β0
Ngrtwrof Swroiee (N)
¢28
QXK
OXM
›0Λ3
QXB
0Λ1
OsM
Compw⅛on of Ac<M⅛tlon F⅛κ⅛OTe (Sc⅛rtdt βt al. 2021 (IL-2))
QAβ
QAS
OXM
i
xOΛ3
002
0A1
04»
Co忤WIeOn OfAcqUI域Ion Finctlone (Zhueng et a. 20l8)
80	1»	320 4W W
Nurter of Sarolee (N)
OXB
OM
18 3M MO tβθ
NunbwofSsraIee (N)
CCTnPWI.on OfAcqUdtIon Functlone (Zhuanget a. 2019)
Q.12
Q.1Q
Q.∞
0.04
a
0.(B
Nunberot
—∙-即Sm
Sanewah
→t- SOfUrartain
-→- km e»i sdata
I—♦- kra∞i9*mbed
—mar⅛n
cor*aet
-∙- badge
ad<BIM
Ctmpertson OfAcqiJaIixi FUnClOnS (Scxirrtdteta.:
320	«0	1280	1920 26βO
N ∣Λτ⅛βr of Swnpiee (N)
taρ∞oertrt∏
-W- βΛrart*ι
---rτwtfπ
→- tβdge
ad>e∣M
Compwleon OfAcqiJdtIon F⅛retlonβ (Sc⅛nldt Ot W. 2021 (L-2))
Ctmpertson OfAcqiJjiIixi Rinclixis (Scħrridt βt sJ. 2021 (FNg)
020
0.1β
∏≡tf∏
Corrpwlson OfAcquteWxi Fuictkxis (SHHlt β,⅛. 2018)
-*— bede«
BΛe∣M
ICfmowMπ
-*- κΛ∙wβt*ι
-IuiievuiMa
0.16
E)∙1β
0.10
W(i
ιxιe
N∣ΛrtJerot 3aπ^ββ (N)
NsrtjerofSanpee(N)
Mmber of SwipM (N)
taρ∞oβrtrt∏
-*- sΛ∙wβt*ι
-4- IuimuiJala
■ πv⅞π
-*— tβdge
■MM
E o.ιo
—∙- rarκfam
-→- WpuTeertaIn
-X- aol⅛∣o∣t*ι
→- ⅛nrans<ta(a
—♦— IoneanSen⅛βd
J—•- nweh
839«
→- badge
KhBM
OW
0∞
MO 1280	1920	2S80	S200
Nurter of Sermles (N)
COrrPartSon OfAcquWIixi Rjndons (Scħrr*ltβtal.2021 (FNg)
O 1024	20«	3072 4O9β 6120
Nwτ*βrof Samplee (N)
Coσ(ιβr1soπ OfACTMM，5 Funrtkxis (Schmidtβ<sJ. 2021 (llr2))
—IVlJuln
ICfWCwMn
→- IUEaJa⅛
---
ocrewt
→- M0e
■MM
0 IeM M4«	3072	4«8 «1MI
Hmberof Swτι>lo* (N)
20« WTZ
Nwτ*βrof Samptee (N)
Ccnifiαweπ of ⅛quBiton Fimcfcire (ZhUmS eta. 2Q18)
Compw⅛on of Ac(μ⅛tlon F⅛κ⅛OTe (Sc⅛rtdt βt al. 2021 (IL∙2))
O-Ef
to p∞ov⅛ri π
-*- stfU>ι owtah
→- kmear^dat≡
• kι I va∣Q0∣ιCmJ
—my≠ι
o*rwΛ
-*~ badge
ad£M
NkmberotSaTOee (N)

。m-H
Figure 7:	The hit ratio of different acquisition for BNN model, different target datasets, and different
acquisition batch sizes. We use STRING treatment descriptors here. The x-axis shows the number
of data points collected so far during the active learning cycles. The y-axis shows the ratio of the set
of interesting genes that have been found by the acquisition function up until each cycle.
24
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch size = 256	batch size = 128	batch size = 64	batch size = 32	batch size =16
Corrparleon of AcquIeItlBi F⅛w1onβ (Shlfrutetal. 2018)
f- WtMnoertah
—*— kmɑrodata
—*— nw≠ι
CWWt
-*— badge
aΛHM
SO W 320	«0
MxnberofSannIee (N)
J.ιy
18 3M MO 98 Iita
NunbwofSsraIee (N)
IOfAcquWtIon Fundone (Sc⅛ridtetaJ.2α21 (IFN⅛)
tcpi>T0«f⅛⅛1
—x- s⅛U>τoe∣W∏
-luivɑŋdala
* Iu I ιes∣M∣ι(ιed
——∏w≠ι
OWMt
" badee
advSN
18 3M MO 98 Iita
NunbwofSsraIee (N)
I of Acquisition FscIOne <SctEdt et N. 2021
tc^u∏oeιt≠ι
→- so‰>τcwW∏
→- kme®WfaIa
-♦— ⅛nwwιιUed
—*— ∏w⅛∏
8B9Λ
badge
a*ew
3M ⅛w ιaα	传 so	2«a
Hmbwof Ssfφββ (N)
OW-H
Cornpwieon of⅛<M⅛tlon F⅛κ⅛OTe (Sc⅛rt⅛tβtal.2021 (IL-2)∣
tvid 所
IapwiovMn
→<- stfU>ιowt≡⅛ι
kmear^dat≡
—*— m<≠ι
→- o*rwΛ
badge
ad£M
04
,oa
04
06
04
loa
T
02
0.1
04
Se =H
Corrpsrleon OfACqUteIt∣6 FscIOne (SHfnιtβt⅛. 2018)
CorrpaJteai OfAcquWtIon Fundons (Sc⅛nWt et Si. 2021 (IFNg))
—•— ∣9Kj⅛aιι
IC^UnOerMn
-*~ stfh>τoβrt⅛ι
→- ⅛∣∣w∣aUa⅛
----mτtf∏
→- OOt set
-*— badge
≡ΛSH
O 10M 2MS a>»	4<»e	¢120
Mmber of Swndea (NI
1W4	20W Wt 40*	6M0
Nurfcerof Ssrrriee (N)
Coiifiaw of Acquwtiwi FuncbwiB (SHftUt et a. 201a)
O 1024	204«	3072 WSe
Nunber of Setroles (N)
CorrpajleonofAcquIeiIon Funclone (Sc⅛nklt βt eɪ. 2021 (∣∣^2))
α.(κ
0.8
Compsrleon of AcqiJRtIon RrelOne (SctmIOtetal. 2021 (≡--2))
06
。2
0.1
g>i8rtah
→- wΛ*wwW∏
-*- kme®Wtata
-♦— ⅛nwwιιUed
—*— nw⅛∏
OWMt
-*— badge
advβN
320 640 12ββ 1«2Q
Mmber of Swndea (N)
CFwlewI OfAcqUdtIwi Functlone (Sdmldtet 域.2021 (IL-2))
random
-∙- IOPInMItain
-×- Wfcjncwtati
→- kmβansdaa
1W4 2MS 30»	40S«
Nι›⅛w of Saπnlw (N)
O 1024	20«	30»	409β
Nuπ⅛er of Swπdea (N)
IeQ 3∞ MO Sββ 1280
Nunber of SonoIee (N)
SEuH
Se =H
Ccnvenscn OfAcquetian FuncborB (Zhuang βt a. 2018)
—ran⅛xn
fcfHjncβrtsh
→- SOfcrcwtsin
-*- kmeaɪsdsts
→- kmeaιsβmbβ<
-→- mar⅛n
corew<
→- badge
8d*BN
Coupmai of AcquwtKm Fwιc⅛mβ (Zhumg eta- 2018)
O 1024	204«	3072	«»6
Nunber of Setndes(N)
Figure 8:	The hit ratio of different acquisition for BNN model, different target datasets, and different
acquisition batch sizes. We use Achilles treatment descriptors here. The x-axis shows the number
of data points collected so far during the active learning cycles. The y-axis shows the ratio of the set
of interesting genes that have been found by the acquisition function up until each cycle.
25
PubliShed aS a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch Size = 256 batch Size =128	batch Size = 64	batch Size = 32	batch Size =16
ɪ of Acquleltlon Fuiclone (Sc⅛rtdt βt⅛. 2021 (IFN8))
---∏1<≠1
→- <xmΛ
badge
—ad>βH
—kmαrodata
QXM
Compw⅛on of Ac<M⅛tlon F⅛κ⅛OTe (Sc⅛rtdt βt al. 2021 (IL-2))
0.10
OΛS
,0Λ
OjaI
08
to p∞ov⅛ri ∏
—*— kmear^dat≡
---∏1<≠1
→- mmΛ
badge
«0
640
«.17«
0.1β0
0.12β
oφ∙1∞
SOffK
0X)∞
OOX
048
CorrpajleonofAcquIeiIon Funclone (Shltvtet Si. 2018)
Oa)
Q”
asa
α.ra
oxκ
OsM
—•— randan
-luivaŋdala
3M ⅛w ιaα	传 so	2«a
Nι∙nbβrof Sarφw (N)
—*— ∏w≠ι
→- vrwΛ
ta∙⅛>
-→- advSH
Corrpwlson OfAcquteWxi Fuictkxis (SHHlt β,⅛. 2018)
0邠
。3。
。26
；020
i0.1S
0.10
。46
0X)0
∣wτdo∏ι
ICfmowM∏
HEBiJala
nwtf∏
Mee
BΛe∣M
»W 1280 Ma) 2BβO 3200
Nwτ*jβrofSa∏∣>iβ<(N)
04
03
0Λ
0.1
04
Corrraleon of⅛quleltlon Fsclone (SHfnitetai. 2018)
0	10M 2MS 3072 dθtβ ¢120
NwibsrofSwnpIee(N)
04
04
02
0.1
1βO 320 »W MO 12S0
Nwτ⅛βrof SMIpie< (N)
3M ⅛w ιaα ma teea
NunbwofSsralee (N)
CorrpaJteai 域 AcquMtIon Fundons (Scfmklt et aJ. 2021 (IFNg))
0 IOM 20W Wt 40*	6M0
Nurfcerof Ssral•« (N)
Nuntw ofSwnpeβ (N)
0.12β
0.1∞
048
Q4β
OXro
Corrparisoi OfACqul蹴ton Functkxis (SdmkltetJJ. 2021 (t2))
«.17«
O.1βO
0X>7β
0X)∞
gts
tcpww⅛⅛ι
→- 3，EBlJa⅛
—•— ∏w⅞∏
—^― αrwet
-»- t∙⅛∙
-→- ad>ew
«0 320 ⅛W MO
Nunberof Swnpiee (N)
1280
CorrpajleonofAcquIeiIon Funclone {Sc⅛nWt βt eɪ. 2021
0.8
0.26
,0.20
£0.1«
α.ra
α.(κ
0.8
—*— ιsι(Xaιι
t*jrtw of Swrpw (N)
-luivɑŋdala
—*— ∏w≠ι
r- QWMt
ta∙⅛>
-→- ad>SH
Compwleon OfACqiJdtlon F⅛retlonβ (Sc⅛nldt Ot al. 2021 (U2))
036
030
026
>ι0i0
0.16
0.10
0X)6
Mmber OfSenipiee(N)
Comperlsori OfAcquIjiIori FuictlBis (S<tιrri(ltβtβ1-2CB1 (∣^2))
• ιακj⅛aιι
tcfm ow⅛ri∏
* lπιeπdata
T- mβ⅛n
→- oorewt
badge
-→- advSN
04
04
03
02
0.1
OtSsrrpee (N)
04
，0a
CWrFw⅛6 of fcqj(ltl∞ Fsctlora (Schrtdt «t al. 2021 (L2))
03
0.1
Ij
1024
20«
3072
409β
-∙- Wpuieeitah
-*- farMrBdala
kmesrBembetl
-*— msr⅛n
-∙- ∞r(9βt
ba⅛e
→- aMH
Nuntw of Swπdn (N)
80	1»	38。 4W W
NurberofSanvIee(N)
Compsfleon OfACqJd 16 F⅛x⅛6β (Zhuang «t W. 2019)
1∞ 320 MQ 9βO 12S0
Nuni>erofSwπplee(N)
Ccnverison OfAcqueitian FunctiorB (Zhuang βt 域.2018)
C0πf>ακ0ι of Acqu⅛t⅛xι Fwιctkmβ (Zhuwig etd. 2018)
O 1024	204«	3072	«»6
Nunber of Setndes(N)
04
Figure 9:	The hit ratio of different acquiSition for BNN model, different target dataSetS, and different
acquiSition batch SizeS. We uSe CCLE treatment deScriptorS here. The x-axiS ShowS the number of
data pointS collected So far during the active learning cycleS. The y-axiS ShowS the ratio of the Set of
intereSting geneS that have been found by the acquiSition function up until each cycle.
26
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch size = 256 batch size = 128	batch size = 64	batch size = 32	batch size = 16
0.24
0.23
122
j.a
Ccmpwieon OfAcquieitIon Fundom« (Schrtdtet al. 2021 (IL3))
tcpι>τow⅛⅛ι
023
Com part eon OfAcquIeiIon Funcβons {SHtut etal. 2Q1⅝
0.17«
0.17«
tθfΛ*wMn
ac∏mw⅛⅛ι
O2βO
β∙17,
024«
0240
023β
i
0230
022β
0.1M
OZZO
«21«
NlΛT⅛βr Ot Swn>iββ (N)
Comparteon OtAcquIeieon Fι*ιcβons (Sdιrri<ltβ, «J. 2021 (IFNg))
t∞L*w^n
924
923
022
In 丁
6120
12W 2ββO 3M0
Nwτ⅛βr of SaEee (N)
126
Corroartson OfAcquWtkxi Functlcfis (S⅛ιrrtdtβt⅛.2021 (t2))
gmotr⅛riπ
1
1
053β
1
)23C
1
1
，22C
1
1
D2M
1
050β
NkmberotSsfrp
l.63
l.ffi
I.β1
l.βQ
l.»
∣.58
l.67
ι.sβ
Compwieon of AcqUdtIon Fsdlorw (Zhuwigetal. 2019)
—*— rand cm
tceιr∞naln
3072
Number of Senviee (N)
IQ24 2Q4β



Figure 10:	The evaluation of the random forest model trained with STRING treatment descriptors
at each active learning cycle for 4 datasets and 6 acquisition batch sizes. In each plot, the x-axis is
the active learning cycles multiplied by the acquisition bath size that gives the total number of data
points collected so far. The y-axis is the test MSE error evaluated on the test data.
27
Published as a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg) Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
0z∞qosq ZE = 9z∞qosq
CorTlPw⅛cn OfAcqilelton Fwidone (Sdvnkit βt ai. 2Q21 (IFNg))
Corrparleon of AcqiJtiIon Fι>κlonβ (Sc⅛rtcft M d. 2021«L∙2))
8	18	320	4β0	644
N∣Λτ⅛erofSampto((N)
Comperieon OfAcqUdtIon Fsctlone (Zhueng et al. 2019)
寸9 = 0z∞qaEq
Canpwteom OfAcquIeItIori Fundone (Shlfrutetal- 2019)
划 ¢40 Iitg 1«» MW
Nwnbsr Cf Saπιp∣w (N)
Conpwteon OfAcquIeItIon Fundone(Sc⅛ricltetaJ.2021 (ILaj)
—■— naι(Xaιι
。27
Corrparleon OfACqUd 16 Fsctlone (ZhgngetW. 2018)
0.19β
0.190
0.1aβ
0.1w
0∙17β
0.170
0.190
0.1as
«.ia«
0∙17β
0.170
Ctmpertson of Acquljilai Functkxis (SHtUt β,JJ.20ia
W) 12W	26β0	3M0	6120
Nwτ⅛βr of SaEee (N)
Ctmpertson of Acqul⅛lαι Funclais (SHfnjte,⅛.2018)
—∣9kJui∣
O IOM 204S	3072	40Se	6120
Nwrtw of SaEee (N)
Corrpajleon OfAcquIrtIon Functlone (Sc⅛ridtβtβ1.2021 (IFN⅛)
O IOM 20«	3072 4O9β 6120
NuEjerofSaEee(N)
OMS
OMO
OJ23$
OJZZS
OJZZO
o∙z,e
O 1024	204β	3072	409β	6120
NumbwotSampies(N)
CuiiwEnofAcqu⅛t⅛xι Fsctiore(Zhuvig etd.2019)
1.TO
148
1Λβ
1∙β+
1耀
1β)
1梯
1j6
O 1CB4 2CMβ 872 4Wβ 512t>
Nurr⅛er of SanVleS (N)
Corroartson OfAcquWtkxi Functlcfis (S⅛ιrrtdtβt⅛.2021 (t2))
831 H ə-s qaEq
9-H ə-s qaEq ZIgH ə-s qaEq

Figure 11:	The evaluation of the random forest model trained with CCLE treatment descriptors at
each active learning cycle for 4 datasets and 6 acquisition batch sizes. In each plot, the x-axis is
the active learning cycles multiplied by the acquisition bath size that gives the total number of data
points collected so far. The y-axis is the test MSE error evaluated on the test data.
28
PubliShed aS a conference paper at ICLR 2022
Schmidt et al. 2021 (IFNg) Schmidt et al. 2021 (IL-2)
batch size = 512 batch Size = 256 batch Size =128	batch Size = 64	batch Size = 32	batch Size =16
Shifrut et al. 2018
CorrpajleonofAcquIeiIon Funclone (ShiHrtet al-201 β)
βtβJ.zoιβl
Companeon oτΛcquιaιon Funcione
Comperleon OfAcqUdtIOn Functlone (SHfruteteI. 2018)
Corrparleon of AcquIeItlBi F⅛w1onβ (Shlfrut βt⅛ 2018)
08
QXK
QXM
Compw⅛on of Ac<M⅛tlon FlncVm (Scħπldtβtβi.2Q21 (IL-2))
to p∞ov⅛ri ∏
→<- stfU>ιowt≡⅛ι
Corrpajleon of AcquWtlon Fundone (Sc⅛ridtβtβ1-2<≡1 (IFN⅛)
320 ⅛w ιaα ma teea
Hrrber of SwtbIw (N)
Zhuang et al. 2019
Compsfleon OfACqJd16 F⅛x⅛6β (Zhusng «t W. 2019)
320 β40	1280	1820	2H0
Nuni>erofSwπplee(N)
0.30
Carpeison of Acquisiion Functors (Zħueng etei. 2019)
一.-rsn<tom
—kφuncβrtaln
—0- Wfcrcertah
MO 12∞	25β0	3β0	5120
Numbw at SanaeS (N)

Figure 12:	The hit ratio of different acquiSition for random foreSt model, different target dataSetS,
and different acquiSition batch SizeS. We uSe STRING treatment deScriptorS here. The x-axiS ShowS
the number of data pointS collected So far during the active learning cycleS. The y-axiS ShowS the
ratio of the Set of intereSting geneS that have been found by the acquiSition function up until each
cycle.
29
PubliShed aS a conference paper at ICLR 2022
Shifrut et al. 2018
Schmidt et al. 2021 (IFNg)
Schmidt et al. 2021 (IL-2)
Zhuang et al. 2019
batch size = 512 batch Size = 256 batch Size =128	batch Size = 64	batch Size = 32	batch Size =16
0Λ6
QXM
Corrparleon of AcquIeItlBi F⅛wlonβ (Shlfrutetal. 2018)
->*- StfbJnWItah
→<- StfbJnWItah
Nureerof
CorrpajleonofAcquIeiIon Funclome (ShiHitetβl.2018)
Compwleon OfAcquMtkxi FuicIixis (Sħltut βtsJ.201⅛
0.17«
0.1∞
tap∞oerM∏
∖-×- sΛ∙w*rt*ι
NgrtRrot Saπ^ee (N)
0.12
α.ra
Corrpajleon of AcquMtIon Fundone (Sctirridt et aJ. 2021
J-*- s⅛U>τoe∣W∏
CorrpajleonofAcquIeIon Funclome (Scrmklt et aJ..
0.14
0.12
0.10
NkJTtWOl
Compwleon OtAoquetIon Functlone (Zhuang et a. 2019)
—∙-即Sm
Sanewah
—WfUrcertairi
Corrpajleon of AcquMtIon Fundone (Sctirridtet aJ.2Q21 {IFN⅛
030
0.16
≡0.10
OXK
J-*- s⅛U>τoe∣Wπ
Ol £
CorrpajleonofAcquIeIon Funclome (Scrmklt et aJ. 2021 (IL-2)
NkJrtWOI
Nuntw ofSwnpeβ (N)
CorrpajleonofAcquleIon Funclome (Shinrtetal-ZOie)
Conpafleon ofAcqu⅛Won FwxSore (3cħjrtclt>tai-202l (∣ft⅛))
6« 12ββ 26ββ SMO ¢120
Mmber of Swndea (N)
640 1290 JaiO 3« 40	6120
Hmbwof Ssfφββ (N)
CorrparleonofAcqulelon Funclone (Srifruteta. 2D1β)
CorrpaJteai OfAcquMtIon Fundons (Scfmklt βtβJ. 2021 (IFNg))
tvιd∣πι
tβpuπOertrin
wΛ∙wβt*ι
OW-H
ConlPK⅛6 of Acquwfcm F5d⅛re (ScHnidt et 域.2021 (L2))
Compeneon OtAcquIaIon FuictlBis (SeimidtetSl. 2021 (L-2))
" ISkJuil
tCfm OWtrin
Q 1024	20«	872	409β
Nuπ⅛er of Swπdea (N)
CwrpaHeon Of ⅛qJ"W∞ Firetlorw (Sctrridtβt⅛. 2021 (L-2))
CcnvaiMn of Acqusiion Functors (Zħuβιgetei. 2019)
if
ConTiwleonotAcquIiIon Finctlone (ZhUW>9 et«- 2018)
-∙- ran⅛m
-∙- Iopuncetah
—Satwcartdn
O 10M	2048	3072 4O9β
Nιn*erofSβτaleβ(N)

π
h

Figure 13: The hit ratio of different acquiSition for random foreSt model, different target dataSetS,
and different acquiSition batch SizeS. We uSe CCLE treatment deScriptorS here. The x-axiS ShowS
the number of data pointS collected So far during the active learning cycleS. The y-axiS ShowS the
ratio of the Set of intereSting geneS that have been found by the acquiSition function up until each
cycle.
30
Published as a conference paper at ICLR 2022
D Additional datasets
We tested the GeneDisco pipeline on two additional datasets which are included in this appendix
due to space constraints. The relative performance of the random forest model with the acquisition
functions that are compatible wit this model class are shown in Figure 14 for the predictive accuracy
task and Figure 15 for the hit ratio task.
D.1 Modulation of Tau proteins in neurons
Experimental setting. This assay is based on Sanchez et al. (2021) in which authors have con-
ducted genome-wide CRISPR screens in two SH-SY5Y neuroblastoma cell lines to identify genes
that, when knocked out, either increased or decreased expression of endogenous tau proteins.
Measurement. After editing (for 21 or 30 days), cells are FACS sorted based on low tau expression
(low quartile 25%) and high tau expression (high quartile 25%). Statistical significance of gRNA
enrichment is determined via a redundant siRNA activity (RSA; log p-value) analysis. RSA up
scores were used to find genes enriched in the 25% of cells with highest tau protein, while RSA
down scores were used to find genes enriched in the 25% of cells with the lowest tau protein.
Importance. While the exact mechanism leading to the buildup of Tau proteins is unknown, their
accumulation is correlated with several neurodegenerative pathologies (eg., Alzheimer’s disease,
progressive supranuclear palsy, and frontotemporal dementia). The genes identified in this screen
may therefore help gaining a better understanding of the underlying disease pathways as well as
leading to potential treatments.
D.2 Regulation of endosomal entry in cells for SARS-CoV-2
Experimental setting. This dataset is based on the genome-wide CRISPR screen described in Zhu
et al. (2021). The assay was designed to identify endosomal entry-specific regulators of SARS-CoV-
2 virions in A549-ACE2 cells.
Measurement. The top candidates from the CRISPR screen were determined according to their
MAGeCK score (- log10).
Importance. The endosomal pathway is one of the two pathways (along with fusion at the plasma
membrane) used by SARS-CoV-2 to infect cells. A better understanding of mechanisms under-
pinning this pathway may help identify targets for the development of new SARS-CoV-2 antiviral
therapeutics.
31
Published as a conference paper at ICLR 2022
Sanchez et al. 2021
Zhu et al. 2021
Sanchez et al. 2021
Zhu et al. 2021
batch size = 256	batch size = 128	batch size = 64	batch size = 32	batch size = 16
Sparsity
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
aaβ
ax
OM
⅛'
≡o⅛a
032
021
Comperleon of AcqiJciIon Fu>donβ (Sc⅛rtclt βt al. 2021 (L∙2))
R—•— IWldWTl
* 3p⅛>τcv⅛⅛ι
-×- StfbJnowWn
go ιβα asα 的。 e«
Nmberof Ssmplee (N)
j3 Comparison Cf Acquisition Functions (Sanchez βt al. 2021)
.random
1.1 ---- IcfKincertaIn
—SaftunaBrtnIn
lβθ 3»	«40	«0	12»
Number «rf Samples (N)
2X>0
1.75
UO
[25
U)O
0.75
050
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
-∙- renΛ<m
b>fxιncβrtaι⅞
∣→- SOftuncertah
18 320	640	960	J280
N u mber of S*mρles (H)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
CCLE features
Corrpajleon OfAcquIrttlBi Functlone (Schrridt βtβ1.2l≡1 (L2))
α%
OM
量Oa
。.丝
0.21
Corrpajleon OfAcquIrttlBi Functlone (Sc⅛ridtβtβ1.2l≡1 (L2))
ato «« IaO 19» weo
NmtwrofSaniplw (N)
Comfert∞tι OfAcquWlOi Fιrdons (ScħrrHtet<i.2Q21 (∣L-2))
STRING features
Compar⅛on of ActjJ∙∣1□n Firetlnne (Zhuan□ Bt al. 20181
320 β40	1280	1«SO	268
Nςmber OfSonflIee (N)
ComPwte6 ofAcqUdlon Fsctlone (Zhuang et al. 2019)
Corrpsrleon of AcqιJ⅛∙∞ F⅛τctl6e (Zhusng * a 2019)
STRING features
Figure 14:	The evaluation of the random forest model trained with CCLE and STRING treatment
descriptors at each active learning cycle for the datasets (Sanchez et al., 2021) and (Zhu et al., 2021)
and also for 5 acquisition batch sizes. In each plot, the x-axis is the active learning cycles multiplied
by the acquisition bath size that gives the total number of data points collected so far. The y-axis is
the test MSE error evaluated on the test data.
32
Published as a conference paper at ICLR 2022
Sanchez et al. 2021
Sanchez et al. 2021
91 = 0z∞qaEq
005
OM
S
量83
0X>2
ZEH ə-s qaEq
寸9 = 0z∞qaEq
Zhu et al. 2021
HO lβθ 320	4β0	640
N u mber of S*mρles (H)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
randan
Wpancertah
Saftuncedar
NumberofSampIes (N)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
-♦- aM⅛n
j-∙- toρuneertah
If- SeftUncWtaH
Corrparleon of Acquleltlon F⅛w1onβ (Scturtdtetei. 2021 (IL-2))
∣ —*— κΛιmΛΛ↑
O-Ef
NkjnberotSaraee (N)
OI ≡
Zhu et al. 2021
CorrTIBIeon of ACqUdt∣6 F5don0 (ZhUWlg et W. 2019)
831 = 0z∞qaEq 9 gZ = ωz∞qaEq
320	640	1280	1920
N u mber of S*mρles (M)
Comparison OfAcquIsItIon Functions {Zhuetal. 2021)
0A4
NUnuWof
Compsfleon of AcqiJeItIon FSCsOne (Zhuang «t W. 2019)
Figure 15:	The hit ratio of different acquisition for random forest model, different target datasets,
and different acquisition batch sizes. We use STRING and CCLE treatment descriptors here. The
x-axis shows the number of data points collected so far during the active learning cycles. The y-axis
shows the ratio of the set of interesting genes that have been found by the acquisition function up
until each cycle.
33