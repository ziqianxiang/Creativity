Published as a conference paper at ICLR 2022
PAC Prediction Sets Under Covariate Shift
Sangdon Park
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
sangdonp@seas.upenn.edu
Insup Lee
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
lee@cis.upenn.edu
Edgar Dobriban
Dept. of Statistics & Data Science
The Wharton School
University of Pennsylvania
dobriban@wharton.upenn.edu
Osbert Bastani
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
obastani@seas.upenn.edu
Ab stract
An important challenge facing modern machine learning is how to rigorously
quantify the uncertainty of model predictions. Conveying uncertainty is espe-
cially important when there are changes to the underlying data distribution that
might invalidate the predictive model. Yet, most existing uncertainty quantifica-
tion algorithms break down in the presence of such shifts. We propose a novel
approach that addresses this challenge by constructing probably approximately
correct (PAC) prediction sets in the presence of covariate shift. Our approach fo-
cuses on the setting where there is a covariate shift from the source distribution
(where we have labeled training examples) to the target distribution (for which we
want to quantify uncertainty). Our algorithm assumes given importance weights
that encode how the probabilities of the training examples change under the co-
variate shift. In practice, importance weights typically need to be estimated; thus,
we extend our algorithm to the setting where we are given confidence intervals
for the importance weights. We demonstrate the effectiveness of our approach on
covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the
PAC constraint, and gives prediction sets with the smallest average normalized
size among approaches that always satisfy the PAC constraint.
1	Introduction
A key challenge in machine learning is quantifying prediction uncertainty. A promising approach is
via prediction sets, where the model predicts a set of labels instead of a single label. For example,
prediction sets can be used by a robot to navigate to avoid regions where the prediction set includes
an obstacle, or in healthcare to notify a doctor if the prediction set includes a problematic diagnosis.
Various methods based on these approaches can provide probabilistic correctness guarantees (i.e.,
that the predicted set contains the true label with high probability) when the training and test dis-
tributions are equal—formally, assuming the observations are exchangeable (Vovk et al., 2005; Pa-
padopoulos et al., 2002; Lei et al., 2015) or i.i.d. (Vovk, 2013; Park et al., 2020a; Bates et al., 2021).
However, this assumption often fails to hold in practice due to covariate shift—i.e., where the in-
put distribution changes but the conditional label distribution remains the same (Sugiyama et al.,
2007; Quinonero-Candela et al., 2009). These shifts can be both natural (e.g., changes in color and
lighting) (HendryckS & Dietterich, 2019) or adversarial (e.g., '∞ attacks) (Szegedy et al., 2014).
We consider unsupervised domain adaptation (Ben-David et al., 2007), where we are given labeled
examples from the source domain, but only unlabeled examples from the target (covariate shifted)
domain. We propose an algorithm that constructs probably approximately correct (PAC) prediction
sets under bounded covariate shifts (Wilks, 1941; Valiant, 1984)—i.e., with high probability over
1
Published as a conference paper at ICLR 2022
A
CPS-W
brain,
fish,
lion,
lollipop,
sea turtle
CPS
(b) size and error tradeoff
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
Og
0	50	100	150	200
size
(a) prediction set example
(c) results across many shifts
Figure 1: (a) An example of a covariate shifted image where an existing approach, PS (Park et al.,
2020a), constructs prediction sets that do not contain the true label (i.e., “sea turtle”); in contrast,
our proposed approach PS-W does. (b) The red curve shows the tradeoff between size and error
achieved by different choices of τ on a single shift; the goal is to be as far to the left as possible
without crossing the desired error bound ε = 0.1 shown as the black dashed line. The existing
approaches PS and WSCI (Tibshirani et al., 2019) fail to satisfy the desired error bound due to
covariate shift, whereas our approach satisfies it. (c) Our approach satisfies the error bound over
nine covariate shifts on DomainNet and ImageNet, whereas existing approaches do not.
the training data (“probably”), the prediction set contains the true label for test instances (“approxi-
mately correct”).
Our algorithm uses importance weights to capture the likelihood ofa source example under the target
domain. When the importance weights are known, it uses rejection sampling (von Neumann, 1951)
to construct the prediction sets. Often, the importance weights must be estimated, in which case we
need to account for estimation error. When only given confidence intervals around the importance
weights, our algorithm constructs prediction sets that are robust to this uncertainty.
We evaluate our approach in two settings. First, we consider rate shift, where the model is trained on
a broad domain, but deployed on a narrow domain—e.g., an autonomous car may be trained on both
daytime and nighttime images but operate at night. Even if the model continues to perform well,
this covariate shift may invalidate the prediction sets. We show that our approach constructs valid
prediction sets under rate shifts constructed from DomainNet (Peng et al., 2019), whereas several
existing approaches do not. Second, we consider support shift, where the model is trained on one
domain, but evaluated on another domain with shifted support. We train ResNet101 (He et al., 2016)
using unsupervised domain adaptation (Ganin et al., 2016) on both ImageNet-C synthetic perturba-
tions (Hendrycks & Dietterich, 2019) and PGD adversarial perturbations (Madry et al., 2017) to
ImageNet (Russakovsky et al., 2015). Our PS-W algorithm satisfies the PAC constraint, and gives
prediction sets with the smallest average normalized size among approaches that always satisfy the
PAC constraint. See Figure 1 for a summary of our approach and results.
Related work. Our work is related to conformal prediction (Vovk et al., 2005; Balasubramanian
et al., 2014) without a shift, where the goal is to construct models that predict sets of labels designed
to include the true label with high probability (instead of predicting individual labels) under the
assumption that the source and target distributions are the same. In particular, our setting is related
to inductive (or split) conformal prediction (Papadopoulos et al., 2002; Vovk, 2013; Lei et al., 2015),
where the training set is split into a proper training set used to train a traditional predictive model,
and a calibration set used to construct the prediction sets. The general approach is to train a model
f : X × Y → R that outputs conformity scores, and then to choose a threshold τ ∈ R that satisfies a
correctness guarantee, where the corresponding prediction sets are C(x) = {y ∈ Y | f(x, y) ≥ τ}
(Park et al., 2020a; Gupta et al., 2021); this notation is defined in Section 2.
Several kinds of correctness guarantees under no shift have been considered. One possibility is input
conditional validity (Vovk, 2013; Barber et al., 2020), which ensures correctness for all future co-
variates x, with high probability only over the conditional label distribution p(y | x). This guarantee
is very strong, and hard to ensure in practice. A weaker notion is approximate (Lei & Wasserman,
2014; Barber et al., 2020) or group (Romano et al., 2019) conditional validity, which ensures cor-
rectness with high probability over p(y | x) as well as some distribution p(x) over a subgroup.
Finally, unconditional validity ensures only correctness over the joint distribution p(x, y). We focus
on unconditional validity, though our approach extends readily to group conditional validity.
2
Published as a conference paper at ICLR 2022
A separate issue, arising in the no shift setting, is how to condition on the calibration set Z. A
conventional goal is unconditional validity, over the distribution p(x, y, Z). We refer to this strategy
as fully unconditional validity. However, the guarantee we consider uses a separate confidence level
for the training data, which is called a training conditional guarantee (Vovk, 2013); this correctness
notion is equivalent to a PAC correctness guarantee (Park et al., 2020a), and is also equivalent to
the standard “content” guarantee with a certain confidence level for tolerance regions (Wilks, 1941;
Fraser, 1956). We build on Park et al. (2020a), which formulates the problem of choosing τ as
learning a binary classifier where the input and parameter spaces are both one-dimensional; thus,
the correctness guarantee corresponds to a PAC generalization bound. This approach is widely
applicable since it can work with a variety of objectives (Bates et al., 2021).
Recent work has extended inductive conformal prediction to a setting with covariate shift (Tibshirani
et al., 2019; Lei & Candes, 2020); similarly, PodkoPaev & Ramdas (2021) considers conformal
prediction under label shift (Lipton et al., 2018), i.e., assuming the conditional probabilities p(x |
y) do not change. These aPProaches start from the assumPtion that the true imPortance weights
(IWs) are known (Tibshirani et al., 2019; PodkoPaev & Ramdas, 2021), or assume some ProPerties
of the estimated IWS (Lei & Candes, 2020), whereas our approach considers a special form of
“ambiguity” in the estimated IWs. Furthermore, they are focused on fully unconditional validity,
whereas we obtain PAC prediction sets. In addition, Cauchois et al. (2020) designs confidence sets
that are robust to all distribution shifts with bounded f -divergence; in contrast, we consider the
unsupervised learning setting where we have unlabeled examples from the target distribution. We
provide additional related work in Appendix A.
2	Background on PAC Prediction Sets
We give background on PAC prediction sets (Park et al., 2020a); we also re-interpret this approach
using Clopper-Pearson confidence intervals (Clopper & Pearson, 1934), which aids our analysis.
2.1	PAC Prediction Sets Algorithm
Let x ∈ X be covariates and y ∈ Y be labels. We consider a source distribution P over X × Y with
probability density function (PDF) p(x, y).1 A prediction set2 is a set-valued function C : X → 2Y.
Inputs. We are given a held-out calibration set Sm of i.i.d. samples (χi,yi)〜P for i ∈ [m]:=
{1,..., m}, written as Sm 〜 Pm, and a score function f : X ×Y → R≥0. For example, f (x, y)
can be a prediction for the probability that y is the label for x. The score function can be arbitrary,
but score functions assigning higher scores to likely labels yield smaller prediction sets.
PAC prediction set. A PAC prediction set is a set-valued function C : X → 2Y satisfying the
following two conditions. First, C is approximately correct in the sense that its expected error
(failing to contain the true label) is bounded by a given ε ∈ (0, 1), i.e.,
LP(C)= W(x,y)〜P [l(y ∈ C(x))] = F(χ,y)〜P[y ∈ C(x)] ≤ ε.
Second, noting that CSm is constructed based on a calibration set Sm 〜Pm, the condition that CSm
is approximately correct must be satisfied with high probability—i.e., for given δ ∈ (0, 1), we have
FSm〜Pm [LP (CSm ) ≤ ε] ≥ 1 - δ.
Our goal is to devise an algorithm for constructing a PAC prediction set C. Letting C(x) = Y for
all x ∈ X always satisfies both conditions above, but this extreme case would be uninformative if
used as a prediction set. Therefore, We additionally want to minimize the expected size E[S(C(x))]
of the prediction sets C(x), where S : 2Y → R≥0 is a size measure, which is application specific
(e.g., the cardinality ofa set in classification); however, we only rely on the monotonicity of the size
measure with respect to the prediction set parameterization in construction.
1All quantities that we define are measurable with respect to a fixed σ-algebra on X × Y ; for instance, p
is the density induced by a fixed σ-finite measure. To be precise, we consider a probability measure P defined
with respect to the base measure M on X × Y ; then, p = dP /dM is the Radon-Nykodym derivative of P with
respect to M . For classification, M is the product of a Lebesgue measure on X and a counting measure on Y .
2We use the term “prediction set” to denote both the set-valued function and a set output by this function.
3
Published as a conference paper at ICLR 2022
Algorithm. To construct C, we first define the search space of possible prediction sets along with
the size measure S. We parameterize C by a scalar τ ∈ T := R≥0 as
Cτ (x) = {y ∈ Y | f(x, y) ≥ τ},
i	.e., τ is the threshold on f (x, y) above which we include y in C(x). Importantly, τ controls the
tradeoff between size and expected error. The reason is that if τ1 ≤ τ2, then Cτ2 (x) ⊆ Cτ1 (x)
for all x ∈ X. Thus, size is monotonically decreasing in τ —i.e., S(Cτ2 (x)) ≤ S(Cτ1 (x)) for all
x ∈ X, and error is monotonically increasing in τ —i.e., LP (Cτ1) ≤ LP (Cτ2). See Figure 1b for
an illustration, and Park et al. (2020a) and Gupta et al. (2021) for details.
As a consequence, a typical goal is to construct Cτ that provably contains the true label with high
probability, while empirically minimizing size (Vovk et al., 2005; Gupta et al., 2021). In our set-
ting, we want to maximize τ (equivalently, minimize expected size) subject to the constraint that
CT is PAC. Let LSm(CT) ：= P(X y)∈sm l(y ∈ CT(x)) be the empirical error count on Sm, and
F (k; m,ε) = Pik=0 (mm )εi(1 - ε)m-i be the cumulative distribution function (CDF) of the binomial
distribution Binom(m, ε) with m trials and success probability ε. In prior work, Park et al. (2020a)
constructs CT by solving the following problem:
τ = max T subj. to LS (CT) ≤ k(m,ε,δ),	(1)
T∈T	m
where k(m, ε, δ) = maxk∈N∪{0} k subj. to F(k; m, ε) ≤ δ. Intuitively, the PAC guarantee via this
construction is related to the Binomial distribution; LSm(C) has distribution Binom(m,Lp(C))
(given a fixed C), since l(y ∈ C(x)) has a BemOUlli(Lp(C)) distribution when (x, y)〜 P. Thus,
k(m,ε,δ) defines a “confidence interval” such that if LSm (C) ≤ k(m,ε,δ), then LP(C) ≤ ε
with probability at least 1 - δ. Below, we formalize this intuition by drawing a connection to the
Clopper-Pearson confidence interval.
2.2	Clopper-Pearson Interpretation
We interpret (1) using the Clopper-Pearson (CP) upper bound θ(k; m,δ) ∈ [0,1] (Clopper &
Pearson, 1934; Park et al., 2O21). This is an upper bound on the true success probability μ,
constructed from a sample k 〜Binom(m, μ), which holds with probability at least 1 - δ, i.e.,
Fk〜Binom(m,μ) [μ ≤ θ(k; m, δ)] ≥ 1 - δ, where
θ(k; m,δ)=inf {θ ∈ [0, 1] | F(k; m,θ) ≤ δ}∪{1}.	(2)
Intuitively, for a fixed C, LSm (C)〜Binom(m,Lp(C)), so the true error LP(C) is contained in
the CP upper bound 4(LSm(C); m; δ) with probability at least 1 - δ. Intuitively, We can therefore
choose T so that this upper bound is ≤ ε. However, we need to account for generalization error of
our estimator. To this end, we have the following (see Appendix D.1 for a proof and Algorithm 2 in
Appendix E for implementation details on the corresponding algorithm):
Theorem 1 Let Ucp(C, Sm, δ) ：= θ(LSm (C); m; δ), where θ is defined in (2). Let T be the solution
of the following problem3 :
T = maxτ Subj. to Ucp(Ct, Sm, δ) ≤ ε.	(3)
T∈T
Then, we have IPSm〜Pm [Lp (CT) ≤ ε] ≥ 1 — δ forany T ≤ T.
3	PAC Prediction Sets Under Covariate S hift
We extend the PAC prediction set algorithm described in Section 2 to the covariate shift setting. Our
novel approach combines rejection sampling (von Neumann, 1951) with Theorem 1.
3We consider a trivial solution τ = 0 when (3) is not feasible, but omitting here to avoid clutter.
4
Published as a conference paper at ICLR 2022
3.1	Problem Formulation
We assume labeled training examples from the source distribution P are given, but want to construct
prediction sets that satisfy the PAC property with respect to a (possibly) shifted target distribution Q.
Both of these are distributions over X × Y; denote their PDFs by p(x, y) and q(x, y), respectively.
The challenge is that we are only given unlabeled examples from Q.
Preliminaries and assumptions. We denote the likelihood ratio of covariate distributions by
w*(x) := q(x)∕p(x), also called the importance weight (IW) of x. Our main assumption is the
covariate shift condition, which says the label distributions are equal (i.e., p(y | x) = q(y | x)),
while the covariate distributions may differ (i.e., we can have p(x) 6= q(x)).
Inputs. We assume a labeled calibration set Sm consisting of i.i.d. samples (χi,yi)〜P (for
i ∈ [m]) is given, and a score function f : X × Y → R≥0. For now, we also assume we have
the true importance weights w* := w*(χj for each (Xi,yi)〜P, as well as an upper bound
b ≥ maxχ∈χ w* (x) on the IWs. In Sections 3.3 and Appendix B, We describe how to estimate these
quantities in a way that provides guarantees under smoothness assumptions on the distributions.4
Problem. Our goal is to construct CSm that is a PAC prediction set under Q—i.e.,
FSm 〜Pm [LQ (CSm ) ≤ ε] ≥ 1 - δ,
where LQ(C) := IP(x,y)〜Q[y ∈ C(x)] is the error of C on Q, while empirically controlling its size.
3.2	Rejection Sampling S trategy
Our strategy is to use rejection sampling to convert Sm consisting of i.i.d. samples from P into a
labeled calibration set consisting of i.i.d. samples from Q.
Rejection sampling. Rejection sampling (von Neumann, 1951; Owen, 2013; Rubinstein & Kroese,
2016) is a technique for generating samples from a target distribution q(x) based on samples from
a proposal distribution p(x). Given importance weights (IWs) w* (x) and an upper bound b ≥
maxx∈X w* (x), it constructs a set of i.i.d. samples from q(x). Typically, rejection sampling is used
when the proposal distribution is easy to sample compared to the target distribution. In contrast, we
use it to convert samples from the source distribution into samples from the target distribution.
In particular, our algorithm takes the proposal distribution to be the source covariate distribution PX,
and the target distribution to be our target covariate distribution QX. Let Vi 〜U := Uniform([0,1])
be i.i.d., V := (V1, . . . , Vm), and w~* := (w1*, . . . , wm* ) with wi* = w* (xi) as defined before. Then,
given Sm, it uses rejection sampling to construct a randomly chosen set ofN samples
TN(Sm, V, w~*, b) := {(xi,yi)∈	Sm | Vi ≤ wi*/b}	(4)
from QX. The expected number of samples is E[N] = m/b; thus, rejection sampling is more
effective when the proposal distribution is similar to the target.
Rejection sampling Clopper-Pearson (RSCP) bound. Given TN, our algorithm uses the CP
bound to construct a PAC prediction set C for Q. Let σi := I(Vi ≤ w*/b) indicate whether
example (xi,yi)∈ Sm is accepted—i.e., TN(Sm, V, w~*, b) = {(xi, yi)∈ Sm | σi = 1},where
|TN (Sm, V, w~*, b)| = Pim=1 σi. Then, it follows that URSCP bounds the error on Q, where
URSCP(C,Sm,V,w~*,b,δ) := UCP(C,TN(Sm,V,w~*,b),δ).	(5)
Thus, we have the following (see Appendix D.3 for the proof, and Algorithm 4 in Appendix E for a
detailed description of the PS-R algorithm that leverages this bound):
Theorem 2 Define URRSCP as in (5). Let T be the solution ofthefollowing problem:
T = maxτ subj. to URRSCP(CT,Sm,V,~*,b,δ) ≤ ε.	(6)
τ∈T
Then, we have IPSm〜Pm,v〜Um [Lq(Cγ) ≤ ε] ≥ 1 一 δ forany T ≤ T.
Note that V is sampled only once for Algorithm 4, so the randomness in V can contribute to the
failure of the correctness guarantee; however, the failure rate is controlled by δ.
4In practice, we can improve stability of importance weights by considering source and target distributions
induced by a feature mapping, e.g., learned using unsupervised domain-adaptation (Park et al., 2020b).
5
Published as a conference paper at ICLR 2022
3.3	Approximate Importance Weights
So far, We have assumed that the true importance weight w* (x) is known. Since in practice, it needs
to be estimated, we relax this assumption to only needing an uncertainty set of possible importance
weights. This allows us to handle estimation error in the weights.
Problem. Let SmX be unlabeled calibration examples from the source distribution (i.e., the covariates
in Sm), TXX be n unlabeled calibration examples from the target distribution (denoted by TXX 〜
QnX), and w~* := (w1*, ..., wm* ) ∈ Rm be the vector of true importance weights wi* := w* (xi), for
(xi , yi) ∈ Sm. Then, we assume an uncertainty set W ⊆ Rm that has w~ * with high probability, i.e.,
FSX〜Pm,τX〜QX [~* ∈w] ≥ 1 - δw,	(7)
where δw ∈ (0, 1). We assume W has the form
W := {w ∈ Rm | ∀i ∈ [m] , wi ≤ Wi ≤ wi},
for some Wi and Wi. See the discussion on our choice of W in Appendix C.1.
Robust Clopper-Pearson bound. To construct a PAC prediction set C for Q, it suffices to bound
the worst-case error over W ∈ W, i.e., we have the following (see Appendix D.4 for the proof):
Theorem 3 Suppose W satisfies (7). Define URRSCP as in (5). Let T be the solution ofthefollowing:
T = maxτ Subj. to max URSCP(Cτ,Sm,V,w,b,δc) ≤ ε.	(8)
τ∈T	w∈W
Then, we have FSm〜Pm,v〜Um,τX〜QX [Lq(Ct) ≤ ε] ≥ 1 - δc - δw forany T ≤ T.
A key challenge applying Theorem 3 is solving the maximum over W ∈ W . We propose a simple
greedy algorithm that achieves the maximum.
Greedy algorithm for robust URSCP. The RSCP bound URSCP satisfies certain monotonicity prop-
erties that enable us to efficiently compute an upper bound to the maximum in (8). In particular,
if C makes an error on (xi, yi) (i.e., yi 6∈ C(xi)), then URSCP is monotonically non-decreasing in
Wi* = W* (xi); intuitively, this holds since a larger Wi* increases the probability that (xi, yi) is
included in TN(Sm, V, w*, b), which in turn increases the empirical error LTN(sm,v,w*,b)(C). Con-
versely, if C does not make an error on (xi, yi) (i.e., yi ∈ C(xi)), URSCP is non-increasing in Wi*.
More formally, we have the following result (see Appendix D.5 for a proof):
Lemma 1 For any i ∈ [m], URSCP (C, Sm, V, W*, b, δ) is monotonically non-decreasing in Wi* if
yi ∈/ C(xi), and monotonically non-increasing in Wi* ifyi ∈ C(xi).
Our greedy algorithm leverages the monotonicity of URSCP. In particular, given W and C, the choice
W = (Wι,...,Wm),	where	Wi = ʃwi if yi ∈ C(Xi)	(∀i ∈ [m])	(9)
IWi if y ∈ C (xi)
is the maximum value over W ∈ W of the constraint URSCP(Cτ, Sm, V, W, b, δC) in (8), i.e.,
max Urscp(C, Sm, V, w, b, δ) = URSCP(C, Sm, V, W, b, δ).	(10)
w∈W
Thus, we have the following, which follows by (10) and the same argument as the Theorem 3:
Theorem 4 Suppose W satisfies (7). Define Wτ,sX ,tX as in (9), making the dependency on T, SmX,
and TnX explicit, and URSCP as in (5). Let T be the solution Ofthefollowing problem:
T = maxT subj. to Urscp(Cr,Sm,,V,Wτ,sχTX,b,δc) ≤ ε.	(11)
mn
Then, we have IPSm〜Pm,v〜Um,τX〜Qn [Lq(Ct) ≤ ε] ≥ 1 — δc — δw forany T ≤ T. 5
Importance weight estimation. In general, to estimate the importance weights (IWs), some as-
sumptions on their structure are required (Kanamori et al., 2009; Cortes et al., 2008; Nguyen et al.,
5We assume b is given for simplicity, but our approach estimates it; see Appendix C.2 for details.
6
Published as a conference paper at ICLR 2022
Algorithm 1 PS-W: an algorithm using the robust RSCP bound in (20)
1:	procedure PS-W(Sm,TX,f,g, T,ε,δw,δc, K, E)
2:	W — ESTIMATEIWS(Sm, TX ,g,δw, K, E)	(.) Compute an uncertainty set W
3:	b — maxi∈[κ] Wi	(.) Compute the maximum IW (see (19) in Appendix C.2)
4:	return PS -ROBUST(Sm , f, T, W, b, ε, δC)
5:	procedure ESTIMATEIWS(SmX , TnX , g, δw , K, E)
6:	Construct bins B1 , . . . ,	BK using g as described in	Appendix B.2
7:	Construct W using B1 ,	.	. . , BK,	SmX , TnX , g, δw and,	E as	described in Appendix B.1
8:	return W
9:	procedure PS -ROBUST(Sm , f, g, T, W, b, ε, δC)
10:	V 〜Uniform([0,1])m
11:	T — 0
12:	for τ ∈ T do	(.) Grid search in ascending order
13:	Construct W using (9) given T, Sm, and W
14:	if Urscp(Cτ,Sm,V,W,b,δc) ≤ ε then
15:	T — max(T, T)
16:	else break
17:	return T
2010; Lipton et al., 2018). We use a cluster-based approach (Cortes et al., 2008). In particular, we
heuristically partition the feature space of the score function f into K bins by using a probabilistic
classifier g that separates source and target examples, and then estimate the source and target covari-
ate distributions p(x) and q(x) based on the smoothness assumption over the distributions, where
the degree of the smoothness is parameterized by E. Then, we can construct the uncertainty set W
that satisfies the specified guarantee in (7). See Appendix B for details.
Algorithm. Our algorithm, called PS-W, is detailed in Algorithm 1; it solves (20) and also performs
importance weight estimation according to (17) in Appendix B. In particular, Algorithm 1 constructs
a prediction set that satisfies the PAC guarantee in Theorem 4. See Section 4.1 for our choice of
parameters (e.g., K, E, and grid search parameters).
4 Experiments
We show the efficacy of our approach on rate and support shifts on DomainNet and ImageNet.
4.1 Experimental Setup
Models. For each source-target distribution pair, we split each the labeled source data and unlabeled
target data into train and calibration sets. We use a separate labeled test set from the target for
evaluation. For each shift from source to target, we use a deep neural network score function f
based on ResNet101 (He et al., 2016), trained using unsupervised domain adaptation based on the
source and target training sets. See Appendix F for details, including data split.
Prediction set construction. To construct our prediction sets, we first estimate IWs by training
a probabilistic classifier g using the source and target training sets. Next, we use g to construct
heuristic IWs w(x) = 1/g(s = 1 | x) - 11, where s = 1 if x is from source. Then, we estimate the
lower and upper bound of the true IWs using Theorem 5 with E = 0.001 and K = 10 bins (chosen
to contain equal numbers of heuristic IWs), where we compute the lower and upper Clopper-Pearson
interval using the source and target calibration sets. Furthermore, given a confidence level δ, we use
δc = δw = δ∕2. For the grid search in line 12 of Algorithm 1, we increase T by 10-7 until the
bound URSCP exceeds 1.5ε. Finally, we evaluate the prediction set error on the labeled target test set.
Baselines. We compare the proposed method in Algorithm 1 (PS-W) with the following:
• PS: The prediction set using Algorithm 2 that satisfies the PAC guarantee from Theorem 1, based
on Park et al. (2020a), which makes the i.i.d. assumption.
• PS-C: A Clopper-Pearson method addressing covariate shift by a conservative upper bound on the
empirical loss (see Appendix E.2 for details), resulting in Algorithm 3 that solves the following:
T = arg max T subj. to UCP(CT,Sm,δ) ≤ ε∕b.
τ∈T
• WSCI: Weighted split conformal inference, proposed in (Tibshirani et al., 2019). Under the ex-
changeability assumption (which is somewhat weaker than our i.i.d. assumption), this approach
7
Published as a conference paper at ICLR 2022
(a) Comparison	(b) Ablation study
Figure 2: The prediction set error and size over 100 random trials under synthetic shift from Ima-
geNet to ImageNet-C13. Parameters are m = 20, 000, ε = 0.1, and δ = 10-5. (a) and (b) shows
the box plots for comparison and ablation study, respectively.
provides correctness guarantees only for a single future test instance, i.e., it includes an ε con-
fidence level (denoted α in their paper) but not δ. Also, it assumes the true IWs are known;
following their experiments, we use the heuristic IWs constructed using g.
•	PS-R: The prediction set using Algorithm 4 that satisfies the PAC guarantee from Theorem 2 with
the heuristic IWs constructed using a probabilistic classifier g.
•	PS-M: The prediction set using Algorithm 5, which is identical to PS-R except that PS-M esti-
mates IWs heuristically using histogram density estimation and a calibration set. In particular, we
partition the IW values into bins, project source and target calibration examples into bins based
on the value of the probabilistic classifier g, and estimate the source density PB and target density
^b for each bin. The estimated importance weight is W(x) = ^b WlpB (x), where PB and ^B are
defined in (14) and (15), respectively.
Metrics. We measure performance via the prediction set error and size on a held-out test set—i.e.,
error is the fraction of (x, y) such that y 6∈ C(x) and size is the average of |C(x)| over x.
Rate shifts on DomainNet. We consider settings where the model is trained on data from a variety
of domains, but is then deployed on a specific domain; we call such a shift rate shift. For instance,
a self-driving car may be trained on both day and night images, but tested during the night time.
While the model should still perform well since the target is a subset of the source, the covariate shift
can nevertheless invalidate prediction set guarantees. We consider rate shifts on DomainNet (Peng
et al., 2019), which consists of images of 345 classes from six domains (sketch, clipart, painting,
quickdraw, real, and infograph); we use all domains as the source and each domain as a target.
Support shifts on ImageNet. Next, we consider support shifts, where the support of the target is
different from the support of the source, but unsupervised domain adaptation is used to learn fea-
ture representations that align the two distributions (Ganin et al., 2016); then, the score function f
is trained on this representation. First, we consider ImageNet-C (Hendrycks & Dietterich, 2019),
which modifies the original ImageNet dataset (Russakovsky et al., 2015) using 15 synthetic pertur-
bations with 5 severity levels. We use 13 perturbations, omitting “snow” and “glass blur”, which
are computationally expensive to run. We consider the original ImageNet dataset as the source,
and the all synthetic perturbations on all of ImageNet (denoted ImageNet-C13) as the target. See
Appendix F.3 for data split. Second, we consider adversarial shifts, where we generate adversarial
examples for ImageNet using the PGD attack (Madry et al., 2017) with 0.01 '∞-norm perturbations
with respect to a pretrained ResNet101. We consider the original ImageNet as the source and the
adversarially perturbed ImageNet as the target.
4.2 Experimental Results
We summarize our results in Table 1, and provide more details in Figure 4 in Appendix G.2. Our
PS-W algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average
normalized size among approaches that always satisfy the PAC constraint.
Rate shifts on DomainNet. We use ε = 0.1 and δ = 10-5. As can be seen in Table 1, the
prediction set error of our approach (PS-W) does not violate the error constraint ε = 0.1, while
all other comparing approaches (except for PS-C) violate it for at least one of the shifts. While
PS-C satisfies the desired bound, it is overly conservative; its prediction sets are significantly larger
than necessary, making it less useful for uncertainty quantification. For the shift to Infograph in
Table 1, PS-W achieves relatively larger average prediction set size compared to other shifts; this is
because the classification error of the score function f over Infograph is large—71.28%, whereas
8
Published as a conference paper at ICLR 2022
Table 1: Average prediction set error and sizes over 100 random trials under rate shifts on Do-
mainNet (first six shifts) and support shifts on ImageNet (last two shifts). We denote an approach
satisfying the ε error constraint by ✓, and X otherwise. The “normalized size" is the size divided
by the total number of classes (i.e., 345 for DomainNet and 1000 for ImageNet). Parameters are
m = 50, 000 for DomainNet, m = 20, 000 for ImageNet, ε = 0.1, and δ = 10-5. Our approach
PS-W satisfies theε constraint, while producing prediction sets with the smallest average normalized
size among approaches that always satisfy the error constraint. See Appendix G.2 for box plots.
Shift	Baselines			Ablations		Ours
	PS	WSCI	PS-C	PS-R	PS-M	PS-W
	error size	error size	error size	error size	error size	error size
An	K) 10∙5	花9)	9∙5	F^~10.7 (0.093)		(0∙W 10∙6	(0J4) 10∙8	-J17.0 (0.070)	
Sketch	流2) 13∙1	命 18∙6	F~141.7 (0.020)		晶282	(0∙x>5) 26∙1	-J^^40.3 (0.078)	
Painting	X 15 4 (0.159)	-X^^30.0 (0.113)		F~125.4 (0.025)		-J~37.7 (0.096)		-X^^34.5 (0.103)		-J~52.8 (0.076)	
Quickdraw	✓	5.9 (0.069)	✓	3.8 (0.097)		✓	23.8 (0.021)		4.3 (0.088)		✓	4.2 (0.087)		✓	6.1 (0.067)	
Real	✓	8.7 (0.079)	✓	7.2 (0.087)		✓	47.8 (0.032)		✓	8.7 (0.080)		✓	7.1 (0.087)		✓	11.8 (0.068)	
Clipart	X 10.2 (0.105)	/1) 10∙9	✓	345.0 (0.000)		✓	19.4 (0.080)		✓	14.8 (0.086)		✓	25.7 (0.060)	
Infograph	X 36.4 (0.363)	B吧	F^^345.0 (0.000)		✓ 202.6 (0.085)	X 177.4 (0.107)		J 216.4 (0.078)	
ImageNet-PGD	^v55 (0.090)	F~47^ (0.096)		J 1000.0 (0.000)		J 1000.0 (0.000)		-J78- (0.074)		F~13.9 (0.049)	
ImageNet-C13	X	9.3 (0.127)	(041) 67.0	J 1000.0 (0.000)	J 1000.0 (0.000)	-J15.9 (0.095)	-J~35.8 (0.061)
mean normalized size	—	—	-0.0338-	-0.0257-	—	0.0047
that over Sketch is 37.16%. Even with the poor score function, our proposed approach still satisfies
the ε = 0.1 error constraint. Finally, while our approach PS-W generally performs best subject
to satisfying the error constraint, we note that our ablations PS-R and PS-M also perform well,
providing different tradeoffs. First, the performance of PS-W is significantly more reliable than
PS-R, but in some cases PS-R performs better (e.g., it produces slightly smaller prediction sets on
rate shifts but significantly larger sets on support shifts). Alternatively, PS-M consistently produces
smaller prediction sets, though it sometimes violates the ε error constraints.
Support shifts on ImageNet. We show results for synthetic and adversarial shifts in Table 1 (and
Figure 4 in Appendix G.2). As can be seen, the error of our approach (PS-W) is below the desired
level. PS-R performs poorly, likely due to the uncalibrated point IW estimation—we find that cal-
ibrated importance weights mitigate these issues, though accounting for uncertainty in the IWs is
necessary for achieving the desired error rate; see Appendix G.3 for details.
For adversarial shifts, the target classification error of the source-trained ResNet101 and the domain-
adapted ResNet101 is 99.97% and 28.05%, respectively. Domain adaptation can significantly de-
crease average error rate, but label predictions still do not have guarantees. However, our prediction
set controls the prediction set error rate as specified by ε. As shown in Figure 4, our prediction set
function outputs a prediction set for a given example that includes the true label at least 90% of the
time. Thus, downstream modules can rely on this guarantee for further decision-making.
Ablation and sensitivity study. We conduct an ablation study on the effect of IW calibration and
the smoothness parameter E for PS-W. We observe that the IW calibration considering uncertainty
intervals around calibrated IWs is required for the PAC guarantee (e.g., Figure 2b). Also, we find
that a broad range of E-s can be used to satisfy the PAC guarantee; we believe this result is due to
the use of a domain adapted score function. See Appendices G.3 & G.5 for details.
5 Conclusion
We propose a novel algorithm for building PAC prediction sets under covariate shift; it leverages re-
jection sampling and the Clopper-Pearson interval, and accounts for uncertain IWs. We demonstrate
the efficacy of our approach on natural, synthetic, and adversarial covariate shifts on DomainNet and
ImageNet. Future work includes providing optimality guarantees on the prediction set size, rigorous
estimation of the hyperparameter E, and incorporating probabilistic IW uncertainty estimates.
9
Published as a conference paper at ICLR 2022
Ethics statement. We do not foresee any significant ethical issues with our work. One possible
issue is that end-users may overly trust the prediction sets if they do not understand the limitations
of our approach—e.g., it is only a high probability guarantee.
Reproducibility statement. Algorithms used for evaluation, including ours, are stated in Algorithm
1, Algorithm 2, Algorithm 3, Algorithm 4, and Algorithm 5, along with hyperparameters for algo-
rithms in Section 4.1 and Appendix F. Related code is released6. The proof of our theorems are
stated in Appendix D; the required assumption is stated in Assumption 1.
Acknowledgments
This work was supported in part by ARO W911NF-20-1-0080, AFRL and DARPA FA8750-18-C-0090, NSF
award CCF 1910769, and NSF award 2031895 on the Mathematical and Scientific Foundations of Deep Learn-
ing (MoDL). Any opinions, findings and conclusions or recommendations expressed in this material are those
of the authors and do not necessarily reflect the views of the Air Force Research Laboratory (AFRL), the Army
Research Office (ARO), the Defense Advanced Research Projects Agency (DARPA), or the Department of
Defense, or the United States Government. The authors are grateful to Art Owen for helpful comments.
References
Anastasios N Angelopoulos, StePhen Bates, Emmanuel J Candes, Michael I Jordan, and LihUa
Lei. Learn then test: Calibrating predictive algorithms to achieve risk control. arXiv preprint
arXiv:2110.01052, 2021.
Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. Conformal prediction for reliable
machine learning: theory, adaptations and applications. Newnes, 2014.
Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. The limits of
distribution-free conditional predictive inference, 2020.
Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I Jordan.
Distribution-free, risk-controlling prediction sets. arXiv preprint arXiv:2101.02703, 2021.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training
and test distributions. In Proceedings of the 24th international conference on Machine learning,
pp. 81-88. ACM, 2007.
Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,
78(1):1-3, 1950.
Emmanuel J. Candes, Lihua Lei, and Zhimei Ren. Conformalized survival analysis, 2021.
Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C. Duchi. Robust validation: Confident
predictions even when distributions shift, 2020.
Victor Chernozhukov, Kaspar Wuthrich, and Zhu Yinchu. Exact and robust conformal inference
methods for predictive machine learning with dependent data. In Conference On Learning Theory,
pp. 732-749. PMLR, 2018.
Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the
case of the binomial. Biometrika, 26(4):404-413, 1934.
Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection
bias correction theory. In International conference on algorithmic learning theory, pp. 38-53.
Springer, 2008.
David R Cox. Two further applications of a model for binary regression. Biometrika, 45(3/4):
562-565, 1958.
6 https://github.com/sangdon/pac- ps- w
10
Published as a conference paper at ICLR 2022
Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal
ofthe Royal Statistical Society: Series D (The Statistician), 32(1-2):12-22,1983.
Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Few-shot conformal prediction
with auxiliary tasks, 2021.
Donald Alexander Stuart Fraser. Nonparametric methods in statistics. John Wiley & Sons Inc, 1956.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research, 17(59):1-35, 2016. URL http://jmlr.org/
papers/v17/15-239.html.
Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift, 2021.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. arXiv preprint arXiv:1706.04599, 2017.
Chirag Gupta, Arun K. Kuchibhotla, and Aaditya K. Ramdas. Nested conformal prediction and
quantile out-of-bag ensemble methods, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770-778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. Proceedings of the International Conference on Learning Represen-
tations, 2019.
Ying Jin, Zhimei Ren, and Emmanuel J Candes. Sensitivity analysis of individual treatment effects:
A robust conformal inference approach. arXiv preprint arXiv:2111.12161, 2021.
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct im-
portance estimation. Journal of Machine Learning Research, 10(Jul):1391-1445, 2009.
Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artificial Intelligence and Statistics,
pp. 4346-4356. PMLR, 2020.
Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in Neural
Information Processing Systems, pp. 3474-3482, 2015.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. arXiv preprint arXiv:1807.00263, 2018.
Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in
Neural Information Processing Systems, pp. 3792-3803, 2019.
Donghwan Lee, Xinmeng Huang, Hamed Hassani, and Edgar Dobriban. T-cal: An optimal test for
the calibration of predictive models. arXiv preprint arXiv:2203.01850, 2022.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71-96, 2014.
Jing Lei, Alessandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore
functional data. Annals of Mathematics and Artificial Intelligence, 74(1):29-43, 2015.
Lihua Lei and Emmanuel J Candes. Conformal inference of counterfactuals and individual treatment
effects. arXiv preprint arXiv:2006.06138, 2020.
Sarah Lichtenstein, Baruch Fischhoff, and Lawrence D Phillips. Calibration of probabilities: The
state of the art. Decision making and change in human affairs, pp. 275-324, 1977.
11
Published as a conference paper at ICLR 2022
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with
black box predictors. In International conference on machine learning, pp. 3122-3130. PMLR,
2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Er-
mon. Calibrated model-based deep reinforcement learning. In International Conference on Ma-
chine Learning, pp. 4314-4323, 2019.
Robert G Miller. Statistical prediction by discriminant analysis. In Statistical Prediction by Dis-
criminant Analysis, pp. 1-54. Springer, 1962.
Allan H Murphy. Scalar and vector partitions of the probability score: Part i. two-state situation.
Journal of Applied Meteorology, 11(2):273-282, 1972.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, Nov 2010. ISSN 1557-9654. doi: 10.1109/tit.2010.2068870. URL http:
//dx.doi.org/10.1109/TIT.2010.2068870.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Artidoro Pagnoni, Stefan Gramatovici, and Samuel Liu. Pac learning guarantees under covariate
shift. arXiv preprint arXiv:1812.06393, 2018.
Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. Inductive confidence
machines for regression. In European Conference on Machine Learning, pp. 345-356. Springer,
2002.
Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural
networks via calibrated prediction. In International Conference on Learning Representations,
2020a. URL https://openreview.net/forum?id=BJxVI04YvB.
Sangdon Park, Osbert Bastani, James Weimer, and Insup Lee. Calibrated prediction with covariate
shift via unsupervised domain adaptation. In The 23rd International Conference on Artificial
Intelligence and Statistics, 2020b.
Sangdon Park, Shuo Li, Insup Lee, and Osbert Bastani. PAC confidence predictions for deep neu-
ral network classifiers. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=Qk-Wq5AIjpq.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1406-1415, 2019.
John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likeli-
hood methods. Advances in large margin classifiers, 1999.
Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classi-
fication under label shift. arXiv preprint arXiv:2103.03323, 2021.
Dimitris N Politis. Model-free prediction in regression. In Model-Free Prediction and Regression,
pp. 57-80. Springer, 2015.
Hongxiang Qiu, Edgar Dobriban, and Eric Tchetgen Tchetgen. Distribution-free prediction sets
adaptive to unknown covariate shift. arXiv preprint arXiv:2203.06126, 2022.
JoaqUin Quinonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer.
Dataset shift in machine learning. Mit Press, 2009.
Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel J. Candes. With malice towards
none: Assessing uncertainty via equalized coverage, 2019.
12
Published as a conference paper at ICLR 2022
Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method, volume 10. John
Wiley & Sons, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal ofcomputer vision, 115(3):211-252, 2015.
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by
importance weighted cross validation. Journal of Machine Learning Research, 8(May):985-1005,
2007.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal predic-
tion under covariate shift. Advances in Neural Information Processing Systems, 32:2530-2540,
2019.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142, 1984.
John von Neumann. Various techniques used in connection with random digits. In A. S. House-
holder, G. E. Forsythe, and H. H. Germond (eds.), Monte Carlo Method, volume 12 of National
Bureau of Standards Applied Mathematics Series, chapter 13, pp. 36-38. US Government Printing
Office, Washington, DC, 1951.
Vladimir Vovk. Conditional validity of inductive conformal predictors. Machine learning, 92(2-3):
349-376, 2013.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.
Ximei Wang, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable calibration with
lower bias and variance in domain adaptation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19212-19223. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf.
Samuel S Wilks. Determination of sample sizes for setting tolerance limits. The Annals of Mathe-
matical Statistics, 12(1):91-96, 1941.
Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In International
Conference on Machine Learning, pp. 11559-11569. PMLR, 2021.
Yachong Yang, Arun Kumar Kuchibhotla, and Eric Tchetgen Tchetgen. Doubly robust calibration
of prediction sets under covariate shift. arXiv preprint arXiv:2203.01761, 2022.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In In Proceedings of the Eighteenth International Conference on
Machine Learning. Citeseer, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pp. 694-699. ACM, 2002.
13
Published as a conference paper at ICLR 2022
A Additional Related Work
Prediction sets under i.i.d. assumption. We built our proposed approach on the known PAC
prediction set approach (Wilks, 1941; Park et al., 2020a) due to its simplicity and sample efficiency.
As other candidates, nested conformal prediction (Vovk et al., 2005; Gupta et al., 2021) reinterprets
multiple known conformal prediction approaches using a scalar parameteriztion of prediction sets;
but different from Wilks (1941) and Park et al. (2020a), it is not known to provide PAC guarantees.
Kivaranovic et al. (2020) provides a PAC style guarantee, but their approach is limited to regression
and is sample-inefficient, i.e., it requires a sample of size O(1/2), while Park et al. (2020a) has a
better sample complexity, as demonstrated in their paper.
Prediction sets under various settings. Prior prediction set algorithms have been considered in
several settings. First, traditional conformal prediction (Vovk et al., 2005) often considers the setting
where labeled examples arrive sequentially from the same distribution; there has also been work
extending conformal prediction to the setting where the distribution is time-varying (Politis, 2015;
ChemozhUkov et al., 2θ18; XU & Xie, 2021; Gibbs & Candes, 2021). Alternatively, there has been
work on constructing risk-controlling prediction sets for supervised learning setting (Vovk, 2013;
Park et al., 2020a; Bates et al., 2021; AngelopoUlos et al., 2021). Finally, there has been recent work
on conformal prediction in the meta-learning setting (Fisch et al., 2021); in particUlar, given a few
labeled examples drawn from the new task, their approach leverages labeled examples from previoUs
tasks to constrUct a conformal predictor for the new task, assUming the tasks are exchangeable.
Developments after our work. After oUr work was made pUblicly available, Jin et al. (2021)
has developed a different, robUst conformal inference approach to constrUcting prediction sets with
estimated weights Under covariate shift. Their algorithm assUmes given Upper and lower boUnds on
the importance weights, and Uses the worst-case qUantile over all weights that satisfy the constraint
to set the critical valUes. FUrther, Yang et al. (2022) have developed a doUbly robUst approach to
constrUct prediction sets satisfying approximate marginal coverage Under covariate shift (which can
be robUst to estimating the weights and per-covariate prediction error), leveraging semiparametric
efficiency theory. QiU et al. (2022) have developed a parallel approach for the PAC case.
Calibration. An alternative way to qUantify Uncertainty is calibrated prediction (e.g., Brier, 1950;
Cox, 1958; Miller, 1962; MUrphy, 1972; Lichtenstein et al., 1977; DeGroot & Fienberg, 1983; GUo
et al., 2017, etc), which aims to ensUre that among instances with a predicted confidence p, the
model is correct a fraction p of the time. TechniqUes have been proposed to re-scale predicted
confidences to improve calibration (Platt, 1999; GUo et al., 2017; Zadrozny & Elkan, 2001; 2002;
KUleshov & Liang, 2015; KUleshov et al., 2018; Malik et al., 2019); inclUding ones with theoretical
gUarantees (KUmar et al., 2019; Park et al., 2021) and ones that handle covariate shift (Park et al.,
2020b; Wang et al., 2020). There are also methods to rigoroUsly test calibration, dating back to Cox
(1958); Miller (1962), see e.g., Lee et al. (2022) for a recent approach. These approaches provide a
qUalitatively different form of Uncertainty qUantification compared to the one we consider.
Rejection sampling. Rejection sampling (sometimes accept-reject sampling) is a well-known tech-
niqUe (Owen, 2013; RUbinstein & Kroese, 2016) dating back at least to von NeUmann (1951). We
highlight that most work on covariate shift relies on importance weighting; oUr rejection sampling
approach is relatively less common and more novel. In fact, to the best of oUr knowledge, only
Pagnoni et al. (2018) has Used rejection sampling for a different problem in this area.
IW estimation. There has been a long line of work stUdying the problem of estimating impor-
tance weights (IWs), also called likelihood ratios, in a way that provides theoretical gUarantees.
For instance, (NgUyen et al., 2010) provides a convergence rate analysis of IW estimators—Under
smoothness assUmptions, they provide a finite sample boUnd on the Hellinger distance between the
trUe IW and estimated IW. Next, (Kanamori et al., 2009) shows a similar finite sample gUarantee,
assUming the trUe IW can be represented as a linear combination of kernels. Finally, (Cortes et al.,
2008) proposes non-parametric IW estimators, modeling the soUrce and target distribUtion by his-
tograms over clUsters in sample space.
The IW estimation approaches can be Used in conjUction with prediction set constrUction. Compared
to Candes et al. (2021), which only guarantees asymptotic validity under certain assumptions on
the estimated IWs, Theorem 4 provides a finite-sample correctness gUarantee. FUrthermore, we
explicitly describe an algorithm for approximate IWs, required by Theorem 4, in Appendix B.
14
Published as a conference paper at ICLR 2022
B Importance Weight Estimation
In general, to estimate the importance weights (IWs), some assumptions on their structure are re-
quired. A number of approaches have been proposed, with varying guarantees under different as-
sumptions (Kanamori et al., 2009; Cortes et al., 2008; Nguyen et al., 2010; Lipton et al., 2018). We
use a cluster-based approach (Cortes et al., 2008); our approach is compatible with any of these
strategies if they can be modified to provide uncertainty estimates of the IWs.
In our approach, given a partition X = SjK=1 Bj into bins, we can estimate the IWs based on the
fractions of source and target samples in each bin. If the partition is sufficiently fine, then we can
obtain confidence intervals around the estimated IWs with finite-sample guarantees. However, this
strategy requires the number of bins in the partition to be exponential in the dimension of X . Thus,
in practice, we use a heuristic to construct the partition. We describe the cluster-based approach and
our partition construction heuristic below.
B.1 Cluster-based approach
We assume given unlabeled calibration sets Sm and TX, where Sm consists of i.i.d. samples Xi 〜
P(x) for i ∈ [m], and TX consists of i.i.d. samples Xi 〜q(x) for i ∈ [n], respectively7. Roughly
speaking, the cluster-based strategy estimates the average IW in each bin Bj; assuming P and q are
roughly constant in each bin, these accurately estimate the true IWs. Let j(X) be the bin containing
X (i.e., X ∈ Bj(x)), and let
PB (X) :=	Pj(x)	s.t.	Pj	=	P(X0)	dX0	and	qB (X)	:=	qj(x)	s.t.	qj	=	q(X0)	dX0
be the (unnormalized) approximations of the densities P and q, respectively, that are constant on
each bin. We assume that PB and qB are accurate approximations:
Assumption 1 Given E ∈ R≥0, the partition satisfies
|P(X) - P(X0)|dX0 ≤ E and	|q(X) - q(X0)|dX0 ≤ E	(j ∈ [K], ∀X ∈ Bj).	(12)
Thus, P and q are roughly constant on the partitions. In general, (12) can hold for any E ∈ R>0 ifP
and q are Lipschitz continuous and each Bj is sufficiently small (see Appendix B.3 for discussion).
Then, under Assumption 1, it can be verified that
|v(x) ∙ p(x) — Pb(x)| ≤ E and |v(x) ∙ q(x) — Qb (x)| ≤ E (∀x ∈ X),	(13)
where v(X) = vj(x), and vj = B dX0 is the volume of bin Bj (see the proof of Theorem 5 for the
validity of (13)). Next, we have the following empirical estimates ofPB and qB, respectively:
PB (X) := Pj(x)
qB (X) := qj(x)
s.t. Pj = — X 1 (x0 ∈ Bj) and
m x0 ∈SmX
s.t. ^j = 1 X 1(x0∈ Bj).
(14)
(15)
Now, IL(XO ∈ Bj) has distribution
x0∈TnX
Bernoulli(Pj) when X 〜P, thus m ∙ Pj has distribution
Binom(m,pj∙), and Pj is contained in a CloPPer-Pearson interval around Pj with high probabil-
ity; in particular, let θ be the Clopper-Pearson lower bound corresponding to the Clopper-Pearson
upper bound defined in Section 2.2, i.e., Fk~Binom(m,μ) [μ ≥ θ(k; m, δ)] ≥ 1 - δ. Then, we have
θ(m ∙ Pj; m, δ0) ≤ pj ≤ θ(m ∙ Pj; m, δ0)
(16)
with probability at least 1 - δ0 with respect to the samples SmX. Combining (13) and (16), we have
the following result (and see Appendix D.6 for a proof):
7We can use the same calibration set to construct IWs and prediction sets due to the union bound in Theo-
rem 3.
15
Published as a conference paper at ICLR 2022
Theorem 5 Letting δ0 = δw /(2K) and [v]+ := max{0, v} for all v ∈ R, we have
[ [ .	[θ (n ∙ qB (x); n,δO)- E]+	V *(/ 一 θ (n ∙ qB (X),n,δO) + E	N	U	M∖
W(X) =	θ (m ∙ PB (x); m,&)+E	≤ W(X)	≤ W(X) = [θ (m ∙ PB (x),m3)-E]+	(∀x	∈	X)
(17)
with probability at least 1 - δw over SmX and TnX.
We use these upper and lower bounds on the IWs as the inputs W and W to our algorithm—i.e.,
W = {w : X → R | ∀x ∈ X, W(X) ≤ w(x) ≤ W(x)},
and use b = maxχ∈χ w(x) = maxj∈[κ] Wj as the maximum IW. If b is known, We need importance
weights associated with source calibration samples SmX; thus we use a simpler form ofW as follows:
W = {w ∈ Rm | ∀i ∈ [m], Wi ≤ Wi ≤ Wi},
where Wi := W(Xi) and Wi := W(Xi) for Xi ∈ SX.
Theorem 5 under Assumption 1 is one way to construct uncertainty set W that contains the true IWs.
Corollary 1 Given K ∈ N, E ∈ R≥0, and δw ∈ (0, 1), suppose Assumption 1 is satisfied and W is
constructed using Theorem 5. Then, we have
FSX〜Pm,TX〜QX [w* ∈w] ≥ 1 - δw.
B.2	Partition construction heuristic
In general, exponentially many bins are needed to guarantee Assumption 1. Instead, we consider
an intuitive heuristic for constructing these bins, so that the importance weights W(X)—rather than
the density functions P(X) and q(X) individually—are roughly constant on each bin, inspired by
(Park et al., 2021; 2020b); a standard heuristic for estimating IWs is to train a probabilistic classifier
g(s | X) to distinguish source and target training examples, and then use these probabilities to
construct the IWs. In particular, define the distribution
g*(X, y) = 21 p(x) ∙ I(S = 1) + 2q(X) ∙ I(S = 0).
Then, letting g*(y | x) be the conditional distribution, we have w*(x) = 1∕g*(s = 1 | x) -1 (Bickel
et al., 2007). Thus, we train g(s | x) ≈ g*(s | x) and construct bins according to W(X) = 1∕g(s =
1 | X) - 1, i.e.,
Bj = {X ∈ X | W(X) ∈ [Wj, Wj+1)},
where 0 = W1 ≤ W2 ≤ ... ≤ WK+1 = ∞. Finally, we describe how to train g . Let SmX0 and TnX0 be
unlabeled training examples from a source and target, respectively; then, the set
RX，,n，= {(x, 1) I X ∈ SX，} ∪ {(χ,0) I X ∈ TX}
consists of i.i.d. samples (x,y) 〜 s*(x,y). Thus, we can train S on Rm，n using supervised
learning. In practice, the corresponding IW estimates W(X) can be inaccurate partly since W is likely
overfit to RXm，,n，, which is why re-estimating the IWs in each bin according to Theorem 5 remains
necessary.
B.3	Density Estimation Satisfying Assumption 1
We consider the following binning strategy that satisfies Assumption 1 for Lipschitz continuous
PDFs. In particular, assume the PDFs p(x) and q(x) are L-Lipschitz continuous for some norm ||』.
Then, for any given E, construct each bin Bj such that
kX - XOk ≤
E
L ∙ Vj
∀X, XO ∈ Bj ,
16
Published as a conference paper at ICLR 2022
where vj is the volume of bin Bj . Then, we know that
|p(x) - p(x0)∣ ≤ Lkx - x0k ≤ E,
vj
so
|p(x) - p(x0)|dx0 ≤
Bj
E.
Similarly, we have
|q(x) - q(x0)|dx0 ≤
Bj
E.
Thus, the bins satisfy Assumption 1. Finally, assuming ∣∣ ∙ ∣∣ is the L∞ norm, We describe one way
to construct bins. In particular, construct bins by taking an ε-net with ε = (E/L)1/(d+1), where d
is the dimension of X. Then, we have
∣x - x0∣ ≤ ε
εd+1
εd
E
L ∙ Vj
as desired.
C Additional Discussion on Importance Weights
C.1 Approximate Importance Weights
In Section 3.3, we assume W has the following form:
W := {w ∈ Rm | Wi ≤ Wi ≤ Wi}.
However, considering the fact that the expected importance weight is one, i.e., IEx〜P [w*(x)] = 1,
the uncertainty set W that contains the true importance weights with high probability can be further
constrained as follows:
W0 := {w ∈ Rm | ∀i ∈ [m] , Wi ≤ Wi ≤ Wi , c ≤ Pm=I Wi ≤ C}
for some C and c. In particular, using the Hoeffding's inequality for example, we can estimate C
and C such that wi, ..., Wm can be the part of the true importance weight w* that satisfies the mass
constraint IEx 〜P [w* (x)] = 1.
Recall that we have to find a maximizer in the uncertainty set W0 as in (8); however, due to the
additional constraint on Pim=1 Wi in W0, it is challenging to solve the maximization problem exactly.
C.2 Maximum Importance Weight Estimation
To generalize our approach to estimate the maximum importance weight b, we redefine the uncer-
tainty set W over importance weights as follows:
W := {w : X → R | ∀x ∈ X, w(x) ≤ w(x) ≤ W(x)}
for some w : X → R and w : X → R such that it contains the true importance weight w* with high
probability—i.e.,
FSX〜Pm,TX〜QX [w* ∈ w] ≥ 1 - δw .	(18)
Given this, the maximum importance weight is obtained as follows:
b = max w(x).
x∈X
Considering that W can be estimated using binning as in Appendix B, the maximum importance
weight is rewritten as follows:
b = max w(x) = max Wj,	(19)
x∈X	j∈[K] j
17
Published as a conference paper at ICLR 2022
where
_ θ (n ∙ ^j, n, δ0) + E
wj	[θ (m ∙ pj, m, δ0) — E]+ .
Here, θ, θ, m, n, δ0, E,pj, and ^j are defined in Theorem 5 and Appendix B.
The guarantee (18) implies the guarantee (7). Thus, for the maximization over the uncertainty set in
(8), We use the original definition W and the greedy algorithm for W in (9). Then, Theorem 4 with
the estimated b using (19) still holds as follows:
Theorem 6 Suppose Assumption 1 is satisfied and W is estimated using Theorem 5. Define
Wτ,sχ ,tx as in (9) and bsχ ,tX as in (19), making the dependency on T, Sm, and Tn explicit,
and URSCP as in (5). Let T be the solution of the following problem:
τ = max τ subj. to URSCP(CT, Sm, V, ^^τ,sxITX, bsXtX ,δc) ≤ ε.	(20)
mn	mn
Then, we have FSm〜Pm,v〜Um,τχ〜QX [Lq(Ct) ≤ ε] ≥ 1 一 δc 一 δw forany T ≤ T.
C.3 Choosing Hyperparameters
In general, there is no systematic way to choose the smoothness parameter E and the number of bins
K ; we briefly discuss strategies for doing so.
Number of bins K. The bins are defined in one dimensional space as described in Appendix B.2, so
we follow the standard practice in the calibration literature for binning (Guo et al., 2017; Park et al.,
2021), where K is between 10 and 20. As we are using equal-mass binning, we choose the number
of bins so that each bin contains sufficiently many source examples (in our case, 5000 examples) for
the length of the Clopper-Pearson interval over IWs of each bin to be below some threshold (in our
case, 10-3), which leads us to K = 10. We provide a sensitivity analysis in Appendix G.6.
Smoothness parameter E. Estimating E (i.e., computing the integral of the difference of source
and target probabilities) is intractable in general as it requires that we perform density estimation
in a high-dimensional space (i.e., 2048 in our case), and then integrate this density over each bin.
To avoid this hyperparameter selection, we choose E equal to zero or close to zero (in our case,
0.001). Intuitively, we find that binning based on the source-discriminator scores is an effective way
to group examples with similar IWs; thus, the main contribution to the uncertainty is the error of
the point estimate. We provide a sensitivity analysis on E in Appendix G.5. Importantly, note that
PS-W even with E = 0 satisfies PAC criterion. One direction for future work is devising better
strategies for choosing E.
C.4 Comparison with WSCI
Guarantee. The main difference between WSCI and PS-W lies in the guarantee provided (rather
than prediction set size); PS-W provides a stronger guarantee than WSCI. In particular, WSCI does
not provide a PAC guarantee. Instead, to satisfy the coverage probability guarantee, it requires a
new calibration set for every new test example. However, in practice, we usually have a single
held-out calibration set. Thus, our approach PS-W provides a guarantee that holds conditioned on
this set. This difference is illustrated in Figure 3a, which compares WSCI and PS-W given the
true importance weight. PS-W produces a larger set size than WSCI, but strictly satisfies the error
constraint. The shortcoming of WSCI can also be observed in Figure 2 in Tibshirani et al. (2019),
which empirically shows that the guarantee only holds on average over examples.
Usage of IWs. WSCI requires the true IWs, whereas our method can use approximate IWs. Also,
IWs are used differently. Given the true importance weights, WSCI uses the importance weights to
reweight examples in the calibration set, and PS-W uses the importance weights to generate a target
labeled calibration set using rejection sampling.
18
Published as a conference paper at ICLR 2022
D Proofs
D.1 Proof of Theorem 1
First, note that the constraint in (1) implies F(LSm(CT); m,ε) ≤ δ; conversely, any value of T
satisfying F(LSm (CT); m,ε) ≤ δ also satisfies LSm (CT) ≤ k(m,ε,δ). Thus, we can rewrite (1) as
T = arg max T subj. to F(LSm (CT); m,ε) ≤ δ.
T∈T
On the other hand, if T satisfies (3), then by definition of UCP(CT, Sm, δ) = θ(k; m, δ), We have
inf{θ ∈ [0,1] ] F(LSm (CT); m,θ) ≤ δ}∪{1}≤ ε,
which implies that F(LSm(CT); m,ε) ≤ δ (since the infimum is obtained within the set since the
binomial CDF F is continuous in ε). Conversely, any value of T satisfying F(LSm (CT); m,ε) ≤ δ
also satisfies UCP(CT, Sm, δ) ≤ ε. Thus, we can rewrite (3) as
T = arg max T subj. to F(LSm (CT); m,ε) ≤ δ,
T∈T
implying (1) and (3) are equal. Thus, the claim follows from Theorem 1 of (Park et al., 2020a) and
the fact that the error LP (CT) is monotonically increasing in T.
D.2 Monotonicity of the Clopper-Pearson B ound
The CP bound UCP enjoys certain monotonicity properties that we will need. Intuitively, the CDF
decreases as the number of observations m increases while holding the number of successes k fixed,
but increases if both m and k are increased by the same amount (i.e., holding the number of failures
m - k fixed). In particular, we have the following:
Lemma 2 We have θ(k; m — 1,δ) ≥ θ(k; m, δ) and θ(k — 1; m — 1,δ) ≤ θ(k; m, δ).
Proof. Recall that F (k ; m, θ ) is the cumulative distribution function of a binomial distribution
Binom (m, θ), or equivalently of the random variable Pm=I Xi, where Xi 〜Bernoulli(θ) are i.i.d.
Decreasing case. If k ≤ m — 1, then we have
m	m-1
X Xi ≤ k ⇒ X Xi ≤ k,
i=1	i=1
hence
m
IP X Xi ≤ k
i=1
m-1
⊆F X Xi ≤ k
i=1
so F(k; m, θ) ≤ F(k; m — 1, θ).
Then, we have
θ(k; m, δ) := inf {θ ∈ [0,1] | F(k; m, θ) ≤ δ}∪ {1}
≤ inf {θ ∈ [0, 1] | F(k; m — 1, θ) ≤ δ} ∪ {1}
=:θ(k; m — 1, δ),
thus θ is monotonically non-increasing in m.
Increasing case. We have
m-1	m
X Xi ≤ k — 1 ⇒ X Xi ≤ k,
i=1	i=1
hence
m-1
F X Xi ≤ k — 1
i=1
m
⊆ IP X Xi ≤ k
i=1
19
Published as a conference paper at ICLR 2022
so F(k - 1; m - 1, θ) ≤ F(k;m, θ).
Then, we have
θ(k; m, δ) := inf {θ ∈ [0,1] | F(k; m, θ) ≤ δ}∪ {1}
≥ inf {θ ∈ [0, 1] | F(k - 1;m - 1,θ) ≤ δ} ∪ {1}
=:θ(k — 1; m — 1, δ),
thus θ is monotonically jointly non-decreasing in (m, k).

D.3 Proof of Theorem 2
The rejection sampling prediction set consists of two steps: (i) generate target samples, using source
samples Sm, importance weights w, and an upper bound on their maximum value b, and (ii) construct
the Clopper-Pearson prediction set using the generated target samples.
From rejection sampling, we choose N := Pim=1 σi samples from Sm, denoting them by TN ; here,
N 〜 Binom (m, 1/b), and 1/b is the acceptance probability (Von Neumann, 1951)——i.e.,
where V0 〜Uniform([0,1]). The samples in TN are independent and identically distributed, con-
ditionally on the random number N of samples being equal to any fixed value n. The reason is that
one can view the rejection sampling algorithm proceeding in stages, iterating through the samples
one by one. The first stage starts at the very beginning, and then each stage ends when a datapoint
is accepted, followed by starting a new stage at the next datapoint. The last stage ends at the last
datapoint.
Based only on the source samples observed in one stage, rejection sampling produces a sample from
the target distribution. Thus, within each stage, we produce one sample from the target distribu-
tion, and because each stage is independent of all the other ones, conditionally on any number of
stages reached, our produced target samples are iid. Thus, we can use the Clopper-Pearson bound
conditionally on each N = n.
To this end, let τ(Sm, V) = T to explicitly denote the dependence on Sm and V, and let
T(Tn) = argmaxT subj. to URSCP(CT,Tn,δ) ≤ ε.
τ∈T
Note that conditioned on obtaining n samples using rejection sampling (i.e., |Tn(Sm, V, w, b)| = n),
We have T(Sm1, V) = T(Tn), where = denotes equality in distribution. Then, We have
FSm〜Pm,V〜Um [LQ(CT(Sm,V)) ≤ ε]
m
=X pSm-Pm,V〜Um [LQ(CT(Sm,V)) ≤ ε | N = n] ∙叫N = n]
n=0
m
=X IPTn〜Qn [LQ (CT(Tn) ) ≤ ε] ∙叫N = n]
n=0
≥ X(1 — δ) ∙ F [N = n]
n=0
= 1 — δ,
where the inequality follows by Theorem 1. The claim follows.
D.4 Proof of Theorem 3
First, let
T = arg maxT subj. to URSCP(CT, Sm, V,~*,b,δ0) ≤ ε,
T∈T
(21)
20
Published as a conference paper at ICLR 2022
which satisfies IPSm〜Pm,v〜Um [Lq(Ct) ≤ ε] ≥ 1 - δc by Theorem 2. Now, with probability at
least 1 - δw, we have ~* ∈ W. Under this event, We have
URSCP(Cτ, Sm, V,溟，b δ°) ≤ mqχ URSCP(Cτ, Sm, V, w, b, δc),
w∈W
so T satisfies the constraint in (21). Thus, we must have T ≤ T. By monotonicity of LQ (CT) in T,
we have LQ(CT) ≤ LQ(CT), which implies that
FSm〜Pm,V〜Um,TX〜QX [LQ(CT) ≤ ε] ≥ FSm〜Pm,V〜Um [LQ(CT) ≤ ε] ≥ 1 - δC,
where the last step follows by Theorem 2. The claim follows by a union bound, since w~* ∈ W with
probability at least 1 - δw .
D.5 Proof of Lemma 1
Let w and v be IWs where w(xi) ≥ v(xi) and w(xj) = v(xj) for j 6= i. Additionally, we use the
following shorthands:
nw = XX Ia ≤ wx)),
i=1
Tnw= I (xi, yi) ∈ Sm Vi ≤ -^bi~ },
kw :=	X 1 (y ∈ C(X)),
(x,y)∈Tnw
nv := XX Ie ≤ 干)，
Tnv := {(xi,yi) ∈ Sm Vi ≤	} , and
kv :=	X 1 (y ∈ C(X)).
(x,y)∈Tnv
Here, nw ≥ nv since w(xi) ≥ v(xi). Finally, recall that F (k; m, θ) be the cumulative distribution
function of a binomial random variable Pm=I Xi, where Xi 〜Bern(θ).
Non-decreasing case. If yi ∈/ C(xi), there are two cases to consider:
1.	If v(bΧi) < Vi ≤ W(Xi), then we can verify that nw = n + 1 and kw = kv + 1.
2.	Otherwise, we can verify that nw = nv and kw = kv .
In both cases, kw ≥ n and nw ≥ n. Since θ is monotonically jointly non-decreasing in (m, k) as
in Lemma 2, we have
URSCP(C, Sm, V, W, b, δ) := UCP(LTnw (C), δ)
:= θ(kw; nw, δ)
≥ θ(kv； nv, δ)
=: UCP(LTnv (C),δ)
=: URSCP(C, Sm, V, v, b, δ),
thus URSCP is monotonically non-decreasing in w(xi).
Non-increasing case. If y ∈ C(xi), then kw = kv Since θ is monotonically non-increasing in m
as in Lemma 2, we have
URSCP(C, Sm, V, W, b, δ) := UCP(LTnw (C), δ)
:= θ(kw； nw, δ)
≤ θ(kv； nv, δ)
=: UCP(LTnv(C ),δ)
=: URSCP (C, Sm, V, v, b, δ),
21
Published as a conference paper at ICLR 2022
thus URSCP is monotonically non-increasing in w(xi).
D.6 Proof of Theorem 5
Recall that
K
p^B (x) := ^X 1 (X ∈ Bj)
j=1
-m X S ∈ Bj)
x0∈SmX
K1
^b (x) :=): I(X ∈ Bj) 一): 1 (Xi ∈ Bj)
j=1	n x0 ∈TnX
K
PB (x) := ɪ2 I(X ∈ Bj) / ρ(x0) dx0,
j=1	Bj
K
qB (x) :=	I(X ∈ Bj) / q(x0) dx0, and
j=1	Bj
v(X) := vj(x) =	dX0.
Bj(x)
Due to the assumption of (12), |v(x) ∙ P(X) - PK(x)| is bounded for any X ∈ Bj as follows:
Similarly,
|v(x) ∙ p(x) — Pb (x)
P(X) dX0 -	P(X0 ) dX
P(X) - P(X0)dX0
j
≤	|P(X) - P(X0)| dX0
Bj
= E.
∣v(x) ∙ q(X)- qB(x)| ≤ E.
(22)
(23)
Observe that mp(X)~ Binom (m, JB p(X0)dX0) for any X ∈ Bj; thus PK is bounded with proba-
bility at least 1 - δ0 as follows due to the Clopper-Pearson interval (θ, θ):
θ(mp(*); m, δ0) ≤ PK(x) ≤ θ(mp(∕); m, δ0).	(24)
Similarly,
θ(nq(X); n,δ0) ≤ qκ(x) ≤ θ(nq(X); n,δ0).	(25)
From (22), (23), (24), and (25), the following holds:
θ(mp(*); m, δ0) — E ≤ V(X) ∙ P(X) ≤ θ(mp(∕); m, δ0) + E and
θ(nq(X); n, δ0) — E ≤ V(X) ∙ q(X) ≤ θ(nq(X); n, δ0) + E.
Therefore, for any X ∈ Bj, w* (x) is bounded as follows:
θ(nq(X); n, δ0) - E ≤ w*(X) = q(X) ≤ θ(nq(X); n, δ0) + E
θ(mp(X); m,δ0) + E - W ʃ p(x) — θ(mp(X); m, δ0) - E,
Since we apply the Clopper-Pearson interval for K partitions for both source and target, the claim
holds due to the union bound.
22
Published as a conference paper at ICLR 2022
E Additional Algorithms
E.1 PS Algorithm
Algorithm 2 PS: an algorithm using the CP bound in (3)
procedure PS(Sm, f, T, ε, δ)
T — 0
for τ ∈ T do	(.) Grid search in ascending order
ifUCP(Cτ,Sm,δ) ≤εthen
T J max(T, τ)
else
break
return T
E.2 PS-C Algorithm
Algorithm 3 PS-C: an algorithm using the CP bound in (3) with ε∕b
procedure PS-C(Sm, f, T, b, ε, δ)
return PS(Sm, f, T, ε∕b, δ)
We describe the PS-C algorithm, which uses a conservative upper bound on the CP interval. Let
LP(C)		:=E	[l(y ∈C(χ))] (X,y)〜P
	LQ(C)	：=E [l(y ∈C(χ))] (X,y)〜Q
	w*(x)	q(x) :=—;~~~ p(x)
	b	:=max w*(x). X∈X
Then, we have	LQ(C) =	E	[l(y ∈C(χ))] (χ,y)〜Q
=IE	[w*(x)l (y ∈ C(x))]
(X,y)~P
≤ -E Jb ∙ l(y ∈C(χ))]
=6(u Ji (y ∈C(χ))]
=b ∙ LP(C).
Thus, LQ(C) ≤ ε if b ∙ LP(C) ≤ ε. Equivalently, LQ(C) ≤ ε if LP(C) ≤ ε∕b. As a consequence,
we can choose C based on the CP bound for the i.i.d. case (i.e., Algorithm 2), except using the
desired error of ε∕b (instead of ε). The algorithm is described in Algorithm 3.
23
Published as a conference paper at ICLR 2022
E.3 PS-R Algorithm
Algorithm 4 PS-R: an algorithm using the RSCP bound in (6)
procedure PS-R(Sm, f, T, w, b, ε, δ)
V 〜Uniform([0,1])m
T — 0
for τ ∈ T do	(.) Grid search in ascending order
if URSCP(Cτ, Sm, V, w, b, δ) ≤εthen
T J max(T, τ)
else
break
return T
E.4 PS-M Algorithm
Algorithm 5 PS-M: an algorithm using the RSCP bound in (6) along with IWs rescaling
procedure PS-M(Sm, TnX, f, T, w, b, ε, δ)
W(x) J PB(X for X ∈ X, wherePB and ^b are defined in (14) and (15), respectively
return PS-R(Sm,f, T, W, b, ε, δ)
F	Experiment Details
F.1 Domain Adaptation
We use a fully-connected network (with two hidden layers, where each layers has 500 neurons
followed by ReLU activations and a 0.5-dropout layer) as the domain classifier (recall that the input
of this domain classifier is the last hidden layer of ResNet101). We use the last hidden layer of
the model as example space X, where its dimension is 2048. For neural network training, we run
stochastic gradient descent (SGD) for 100 epochs with an initial learning rate of 0.1, decaying it by
half once every 20 epochs. The domain adaptation regularizer is gradually increased as in (Ganin
et al., 2016). We use the same hyperparameters for all experiments.
F.2 DomainNet
We split the dataset into 409,832 training, 88,371 calibration, and 88,372 test images.
F.3 ImageNetC - 1 3
We split ImageNet into 1.2M training, 25K calibration, and 25K test images, and ImageNet-C13
into 83M training, 1.6M calibration, and 1.6M test images.
To train a model using domain adaptation, due to the large size of the target training set, we sub-
sample the target training set to be the same size as the source training set on for each random
trials.
G Additional Results
G. 1 Synthetic Rate Shift by Two Gaussians
We demonstrate the efficacy of the proposed approaches (i.e., PS-R with the known IWs and PS-W
with the estimated IWs) using a synthetic dataset consisting of samples from two Gaussian distribu-
tions.
Dataset. We consider two Gaussian distributions N(μ, Σ) and N(μ, Σ0) over 2048-dimensional
covariate space X. Here, μ = 0; Σ and Σ0 are diagonal where ∑1,1 = 52, ∑i,i = 10-1, ∑1,ι = 1,
24
Published as a conference paper at ICLR 2022
(a) With the known true IW
(b) With an estimated IW
Figure 3: Error under the rate shift by Two Gaussians (over 100 random trials). Parameters are
m = 50, 000, ε = 0.01, andδ = 10-5.
and Σi,i = 10-1 for i ∈ {2,..., 2048}. We consider the “flat” Gaussian N(μ, Σ) as the source
and the “tall” Gaussian N(μ, Σ0) as the target. Intuitively, there is a rate shift from the source to the
target—i.e., the target examples are a subset of the source, but occur with higher frequencies. We
use the following labeling function: p(y | x) = σ(5x1), where σ is the sigmoid function. Finally,
we generate 50,000 labeled examples for each training, calibration, and test.
Results. We consider two different setups: 1) the true IW is known, and 2) the true IW is unknown.
In Figure 3a, we demonstrate the prediction set errors given the true IW. As expected PS-R satisfies
the PAC guarantee—i.e., the error is below ε. However, as shown in Figure 3b, when we need to
estimate IWs, using just the point-estimate of the IW results in PS-R performing poorly in terms
of prediction set error; it still satisfies the ε constraint, but the error is close to zero, indicating that
the prediction set size is too large to be useful for an uncertainty quantifier. In contrast, PS-W (i.e.,
rejection sampling based on interval estimates of IWs) produces a larger, more reasonable error rate
while still satisfying the PAC condition. These experiments demonstrate that PS-R works well when
given the true IW, but accounting for IW uncertainty is important when using estimated IWs.
25
Published as a conference paper at ICLR 2022
G.2 Prediction Set Size and Error
error
PS WSCI PS-C PS-R PS-W
φzs4,us UOKP-paJd
(a) All
(b) Sketch
(c) Clipart
乙。.02 I
——ε = 0.10
(d) Painting
PS WSCl PS-C PS-W
175
S 150
前125
§100
75
50
25
WSa Ps-c 晶
(e) Quickdraw	(f) Real
u 0.12
鼠1。
B 0.08
O 0.06
4
y
ŋ 0.04
Φ
θ-0.02
0.00
error
SiZe
——ε = 0.10
-二 ⅛ξ∑  T1 	OJɪ   C—,
PS WSCI PS-C PS-W	PS WSCI PS-C PS-W
(g) Infograph
error
1000
(h) ImageNet-C13
0.06
0.04
0.02
——ε=0.i0
0.00^——i---ɪ----j———-1——
PS WSCI PS-C PS-W
800
600
400
200
0
φzsa,us uoc-p∂Jd
(i) ImageNet + PGD
Figure 4: Prediction set error and size under rate shifts on DomainNet (a-g) and under support shifts
on ImageNet (h, i) (over 100 random trials for error and over a held-out test set for size). Parameters
are m = 50, 000 for DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10-5.
26

•s jgyτssB10 oμs∏τqBqojd oψ UK)JJ SIq既τpM əɔuBxlOdUi] po∣BθSQj-uou
oψ səsn qoτqM 'χ-Sd tmψ əsjom uəaə st jouə sjt 4pbj ut——K)U səop uotsjəa sτψ Inq ə用URlBn既 ɔvj
oψ soysμυs qoBOiddB jno 4sureuιop SnopBA o： IJ邱 oψ uo sjɪnsəj oψ ut uəəs oq ubə sy ,o = 1g^ ψτM
PUB (oQ = u = zu，•"!)∣bλjqiut uosiBOj-JoddoiQ oψ Inoq其丛 ς uɪəioəm 飞工'əwtpsə ：mod b btλ
mq qoBQ ut SIq而əM əɔuBXK)(IUll oψ səɪnɔsəj 4 4pBQism ⅛ uɪəioəm ut Se SMl əsbə-isjom gψ səjougt
jυψ M-Sd jɑ WSd ltm∏BΛ B ɪəpɪsuoɔ əM 'ɪeɪnɔpɪed Ul %_01=p puυ 'r0 = 3 4syτqs kn。既Bull joj
OOO c03 = ιu 'sιj!qs iəNulBUK)Q joj Ooo cog = iuəib SJmuieiBd 飞丛1 Poibosqj PUnoIB s∣bλjqiut səsn
M-Sd PUB 'Sg pəɪnɔsəj Jo səwuɪpsə ：mod səsn W-Sd 'sʌvɪ p51Bθsoι əsn IoU səop X-Sd '(əzɪs joj
jəs isəi InO-PHq B .ɪəʌo PUB ɪoɪɪə joj ∣btij UK)PuBj oθɪ -ɪəʌɑ) uoμBjq∏Bθ uo KPnlS uoμB∣qy :g əm砥H
QDd-IoNo跳ʊɪl (q)	£ID#NO配ʊɪl ⑻
M-Sd N-Sd a-Sd
aision
OOZ
OOfr
009
008
oooτ
M-Sd N-Sd	匕 Sd
Pred-Cf-On Sef error

uαBi30juj-p›jureui0Q Q)
M-Sd口
IHOYroNUwraOQ (ə)
supurej-iə^ureuɪoɑ (ɔ)
丛8∙ip5pi∏0-10NUlUuI。Q (p)
M-Sd W-Sd
O
q乙
Oq
5A
ooτ «2.
N
ςzτ e
051
5AI
OOZ
ΛΛ-Sd	N-Sd	Y-Sd
□	申	甲
MiLNU网IOQ (q)
qoR5[S#NU 曲n。Q ⑻
oτ,o=3
,1
jojjə
NollVXWnVɔ NO 入QnJLS NOIlViav £ D
HOZ mɔl IB JodBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2022
G.4 COMPARISON WITH VARIOUS TOP-K PREDICTION SETS
(a) All	(b) Sketch
(c) Clipart
error
(d) Painting
0.25
ε = 0.10
0.20
τopi Tops τopio Topso ps-w
3 0.20-
-e = 0.10
Topl Top5 ToplO Top50 PS-W
(e) Quickdraw
(f) Real
0.0
error
0.7
ε = 0.10
0.6
0.5
0.4
0.3
0.2
0.1
τoρi Tops TopiO τop50 PS-w
oɑ-2
error
ε = 0.10
Topl Tops ToplQ TOP5Q PS-W
(g) Infograph	(h) ImageNet-C13
(i) ImageNet + PGD
Figure 6:	Prediction set error size size under rate shifts on DomainNet (a-g) and under support shifts
on ImageNet (h, i) (over 100 random trial). Default parameters are m = 50, 000 for DomainNet
and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10-5. A Top-K prediction set is a prediction
set that contains top K labels based on a domain-adapted score function; thus the set size is always
K . As can be seen in the prediction set error plots, Top-K prediction sets do not consistently satisfy
the desired guarantee. Moreover, the prediction set size is worse than PS-W. For example, when
the Top-50 prediction set error rate almost achieves the desired error rate, e.g., (6d), the mean and
medial of the corresponding prediction set size is larger than PS-W.
28
Published as a conference paper at ICLR 2022
G.5 Varying a Smoothness Parameter
'ε = 0.10
W 0.04
⅛0∙02
0.00
(a) All
ε 0.08
ω
B 0.06
175
150
125
8 IOO
75
50
25
O
□ □ □
O 0.001 0.005
(b) Sketch
200
150
50
O
0.10
100
0.00
0.10
0.00
≡ 0.08
Φ
g 0.06
≡ 0.08
Φ
g 0.06
W 0.04
iφ
Zo02
W 0.04
iφ
N 0.02
ε = 0.10
(c) Clipart
(d) Painting
O 0.001 0.005 0.01
(e) Quickdraw
(f) Real
ε = 0.10
(g) Infograph
(h) ImageNet-C13
% 口日口
(i) ImageNet + PGD
Figure 7:	Prediction set error size size under rate shifts on DomainNet (a-g) and under support
shifts on ImageNet (h, i) (over 100 random trial) for varying E. Parameters are m = 50, 000 for
DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10-5. In particular, the parameter
E in Assumption 1 bounds the quality of our estimates of p(x) and q(x); since these errors cannot
be conveniently measured, we have chosen it heuristically as a hyperparameter. In this figure, we
show the error of PS-W as a function of E. As E becomes smaller, the prediction sets become more
optimistic while still satisfying the PAC guarantee. Note that the optimal case is E = 0, since PS-W
still satisfies the PAC guarantee; this result suggests that our IW estimates are reasonably accurate.
29
Published as a conference paper at ICLR 2022
G.6 Varying a Number of Bins
(c) Clipart
0.10
0.06
0.00
0.12
0.08
O 0.06
0.00
2 0.08
φ
g010
W 0.04
N 0.02
S 0.04
φ
θ-0.02
(e) Quickdraw
ε = 0.10
ε = 0.10
(h) ImageNet-C13
(g) Infograph
(i) ImageNet + PGD
Figure 8:	Prediction set error size size under rate shifts on DomainNet (a-g) and under support
shifts on ImageNet (h, i) (over 100 random trial) for varying K. Parameters are m = 50, 000
for DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10-5. In general, K must be
chosen to be small enough so each bin contains sufficiently many source examples to achieve a small
Clopper-Perason interval size (e.g., 10-3), though it also needs to be sufficiently large to satisfy the
smoothness assumption.
30
Published as a conference paper at ICLR 2022
G.7 Prediction SET Visualization
Example x	CPS (x)	CPS-W (x)	I	I Example X	I	CPS(X) I	L	CPS-W (x)
	{raccoon)	ʃ91 raccoon	--∙-⅜ΛL ―4	(鳖l,) harp	<	'angel,、 cello, harp, microphone, piano, 、violin ,
⅛⅛⅛¾∣	{w∖tle}	(bread, grapes, wine bottle, wine glass	W	J shark,1 snorkel	<	dolphin, 、 _		 shark, snorkel, submarine, whale
	感		{campfire}	(、 campfire, ocean, star, 、tent ,	Ifl 	⅞gv∙ /	coffee cup, t	ccP	ʃ	(coffee cup,、 J	Ccp,	I mug, I teapot )
ocean)
'hurricane,、
----
ocean,
square,
、tornado ,
{brain}
brain,
fish,
lion,
lollipop,
sea turtle
{penguin}
fire hydrant,
foot,
------r
penguin,
telephone
)
hot tub
bed,
belt,
birthday cake,
guitar,
hat,
hot tub,
tiny paint can,
pillow,
shoe,
table
asparagus
—^
asparagus,
basket,
bread,
carrot,
harp,
lobster,
toothbrush
baseball,
onion
baseball,
baseball bat,
bread,
light bulb,
onion,
potato
Figure 9: Prediction sets of the DomainNet shift from All to Paint. Parameters are m = 50, 000,
ε = 0.1, and δ = 10-5. The green label is the true label and the label with the hat is the predicted
label. We choose examples where the two approaches differ; in particular, if PS-W is incorrect, then
PS is incorrect as well since the prediction set sizes are monotone in τ.
31
Published as a conference paper at ICLR 2022
Example x	CPS(X)	CPS-W(X)	Il Example X				CPS(X)			CPS-W(X)		
3	{photocopier}	printer, phottocopier			H	timtber wolf, < red wolf, > (dingo J				—. timber wolf, white wolf, red wolf, coyote, dingo	
	a airliner,) J airship, ∖ (WarPIane		aircraft carrier, airliner, airship, container ship, stopwatch, parachute, tank, warplane, wing		IkI		'forklift,	^ golfcart, harvester, ， lawn mower, snowplow, tracktor			• forklift, gokart, golfcart, harvester, lawn mower, pickup, snowplow, thresher, tracktor	
辛州	letopard, ∖ snow leopard, / (cheetah	I leopard, ∣ snow leopard, I jaguar, ∣ (cheetah				diatmondback, sidewinder			fhognose snake,) < diamondback,) I sidewinder J		
■	teltevision	e ent. center, ∣ I monitor, I screen, (television J				■	analog clock, barometer, odometer, stopwatch, —. wall clock			analog clock, barometer, digital watch, mag. compass, odometer, stopwatch, wall clock	
洸	chameleon, Igreen lizard ʃ		chameleon, 一..一 green lizard, green snake, waling stick, mantis		W	(barbell,] I dumbbell, I lens cap, (puck J				barbell, barometer, car wheel, dumbbell, lens cap, power drill, puck, stethoscope	
I w*rq * *.≡ , J	P Pembroke, ɪ Cardigan	Pembroke, Cardigan			*	face powder, lipstick, (paintbrushJ				• ballpoint, ` face powde, —. lipstick, paintbrush, perfume, sunscreen	
Figure 10:	Prediction sets of the shift from ImageNet to ImageNet-C13. Parameters are m =
20,000, ε = 0.1, and δ = 10-5. The green label is the true label and the label with the hat is
the predicted label. We choose examples where the two approaches differ; in particular, if PS-W is
incorrect, then PS is incorrect as well since the prediction set sizes are monotone in T.
32
Published as a conference paper at ICLR 2022
Example x	CPS(X)	CPS-W(X)	I	I Example X	CPS(X)	CPS-W(X)
	bo\x turtle	terrapin, bo\x turtle	W	brain coral, s\tarfish, [sea urchin	brain coral, ` chiton, 一■``■∙⅛. starfish, sea urchin, sea cucumber, coral reef,
stinkhorn
k\it fox
red fox,
kit fox
white terrier,
kuvasz,
komondor
Maltese dog,
white terrier,
kuvasz,
komondor,
Samoyed
逊	la\dybug	leaf beetle, la\dybug	L∙ ，	t\usker	t\usker, Afri. elePhant
	(banjo,] elec. guitar, [stage J	aco. guitar, I banjo, I elec. guitar, I stage	飞1 匕二	I beaker, I I PoP bottle, I I water bottle, I [wine bottle J	(beaker,) IIIbeer bottle,III I Perfume, I I Po\P bottle, I III water bottle, III I wine bottle J
J *M	c\uirass	breastplate, c\uirass		convertible, sP\orts car	car wheel, convertible, I sportscar J
conf
conf
Figure 11:	Prediction sets of the shift from ImageNet to ImageNet-PGD. Parameters are m =
20, 000, ε = 0.1, and δ = 10-5. The green label is the true label and the label with the hat is
the predicted label. We choose examples where the two approaches differ; in particular, if PS-W is
incorrect, then PS is incorrect as well since the prediction set sizes are monotone in τ .
33