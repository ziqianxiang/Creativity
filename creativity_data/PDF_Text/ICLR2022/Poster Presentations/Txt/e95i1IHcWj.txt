Published as a conference paper at ICLR 2022
Equivariant and Stable Positional Encoding
for More Powerful Graph Neural Networks
Haorui Wang* 1*, Haoteng Yin2, Muhan Zhang3,4, Pan Li2
1	School of Computer Science, Wuhan University
2	Department of Computer Science, Purdue University
3	Institute for Artificial Intelligence, Peking University, & 4BIGAI
hr_wang@whu.edu.cn, muhan@pku.edu.cn, {yinht,panli}@purdue.edu
Ab stract
Graph neural networks (GNN) have shown great advantages in many graph-based
learning tasks but often fail to predict accurately for a task based on sets of nodes
such as link/motif prediction and so on. Many works have recently proposed to
address this problem by using random node features or node distance features.
However, they suffer from either slow convergence, inaccurate prediction or high
complexity. In this work, we revisit GNNs that allow using positional features of
nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap,
Deepwalk, etc.. GNNs with PE often get criticized because they are not gener-
alizable to unseen graphs (inductive) or stable. Here, we study these issues in a
principled way and propose a provable solution, a class of GNN layers termed PEG
with rigorous mathematical analysis. PEG uses separate channels to update the
original node features and positional features. PEG imposes permutation equivari-
ance w.r.t. the original node features and rotation equivariance w.r.t. the positional
features simultaneously. Extensive link prediction experiments over 8 real-world
networks demonstrate the advantages of PEG in generalization and scalability.1
1	Introduction
Graph neural networks (GNN), inheriting from the power of neural networks (Hornik et al., 1989),
have recently become the de facto standard for machine learning with graph-structured data (Scarselli
et al., 2008). While GNNs can easily outperform traditional algorithms for single-node tasks (such as
node classification) and whole graph tasks (such as graph classification), GNNs predicting over a set
of nodes often achieve subpar performance. For example, for link prediction, GNN models, such
as GCN (Kipf & Welling, 2017), GAE (Kipf & Welling, 2016) may perform even worse than some
simple heuristics such as common neighbors and Adamic Adar (Liben-Nowell & Kleinberg, 2007)
(see the performance comparison over the networks Collab and PPA in Open Graph Benchmark
(OGB) (Hu et al., 2020)). Similar issues widely appear in node-set-based tasks such as network
motif prediction (Liu et al., 2022; Besta et al., 2021), motif counting (Chen et al., 2020), relation
prediction (Wang et al., 2021a; Teru et al., 2020) and temporal interaction prediction (Wang et al.,
2021b), which posts a big concern for applying GNNs to these relevant real-world applications.
The above failure is essentially induced by the loss of node identities during the intrinsic computation
of GNNs. Nodes that get matched under graph automorphism will be associated with the same
representation by GNNs and thus are indistinguishable (see Fig. 1(i)). A naive way to solve this
problem is to pair GNNs with one-hot encoding as the extra node feature. However, it violates the
fundamental inductive bias, i.e., permutation equivariance which GNNs are designed for, and thus
may lead to poor generalization capability: The obtained GNNs are not transferable (inductive) across
different node sets and different graphs or stable to network perturbation.
Many works have been recently proposed to address such an issue. The key idea is to use augmented
node features, where either random features (RF) or deterministic distance encoding (DE) can
be adopted. Interested readers may refer to the book chapter (Li & Leskovec, 2021) for detailed
discussion. Here we give a brief review. RF by nature distinguishes nodes and guarantees permutation
equivariance if the distribution to generate RF keep invariant across the nodes. Although GNNs
paired with RF have been proved to be more expressive (Murphy et al., 2019; Sato et al., 2021), the
training procedure is often hard to converge and the prediction is noisy and inaccurate due to the
* Wang was an intern at Purdue University when doing this project.
1Code available at https://github.com/Graph-COM/PEG
1
Published as a conference paper at ICLR 2022
original features -
0(-0.39,-0.5)
0 (-0.39,0.5) Q (0.39,0.5)
0 (-0.45,0.0)
Rotation
equivariance
(0.39,-0.5)
0 (0.45,0.
2
(iii)
Figure 1: Illustration of Positional Encoding (PE): (i) GNNs cannot distinguish nodes a,b,c,d because the graph
has automorphism where a is mapped to b, c, d. GNNs fail to predict whether (a,b) or (a,d) is more likely to have
a link; (ii) PE associates each node with extra positional features that may distinguish nodes; (iii) An example
of PE uses the eigenvectors that correspond to the 2nd and 3rd smallest eigenvalues of the graph Laplacian
as positional features (denoted as the 2-dim vectors besides each node). The proposed GNN layer PEG keeps
rotation equivariance when processing these features.
injected randomness (Abboud et al., 2020). On the other hand, DE defines extra features by using
the distance from a node to the node set where the prediction is to be made (Li et al., 2020). This
technique is theoretically sound and empirically performs well (Zhang & Chen, 2018; Li et al., 2020;
Zhang et al., 2020). But it introduces huge memory and time consumption. This is because DE is
specific to each node set sample and no intermediate computational results, e.g., node representations
in the canonical GNN pipeline can be shared across different samples.
To alleviate the computational cost of DE, absolute positions of nodes in the graph may be used
as the extra features. We call this technique as positional encoding (PE). PE may approximate
DE by measuring the distance between positional features and can be shared across different node
set samples. However, the fundamental challenge is how to guarantee the GNNs trained with PE
keep permutation equivariant and stable. Using the idea of RF, previous works randomize PE to
guarantee permutation equivariance. Specifically, You et al. (2019) designs PE as the distances
from a node to some randomly selected anchor nodes. However, the approach suffers from slow
convergence and achieves merely subpar performance. Srinivasan & Ribeiro (2020) states that
PE using the eigenvectors of the randomly permuted graph Laplacian matrix keeps permutation
equivariant. Dwivedi & Bresson (2020); Kreuzer et al. (2021) argue that such eigenvectors are
unique up to their signs and thus propose PE that randomly perturbs the signs of those eigenvectors.
Unfortunately, these methods may have risks. They cannot provide permutation equivariant GNNs
when the matrix has multiple eigenvalues, which thus are dangerous when applying to many practical
networks. For example, large social networks, when not connected, have multiple 0 eigenvalues;
small molecule networks often have non-trivial automorphism that may give multiple eigenvalues.
Even if the eigenvalues are distinct, these methods are unstable. We prove that the sensitivity of node
representations to the graph perturbation depends on the inverse of the smallest gap between two
consecutive eigenvalues, which could be actually large when two eigenvalues are close (Lemma 3.4).
In this work, we propose a principled way of using PE to build more powerful GNNs. The key idea
is to use separate channels to update the original node features and positional features. The GNN
architecture keeps not only permutation equivariant w.r.t. node features but also rotation equivariant
w.r.t. positional features. This idea applies to a broad range of PE techniques that can be formulated
as matrix factorization (Qiu et al., 2018) such as Laplacian Eigenmap (LE) (Belkin & Niyogi, 2003)
and Deepwalk (Perozzi et al., 2014). We design a GNN layer PEG that satisfies such requirements.
PEG is provably stable: In particular, we prove that the sensitivity of node representations learnt by
PEG only depends on the gap between the pth and (p + 1)th eigenvalues of the graph Laplacian if
p-dim LE is adopted as PE, instead of the smallest gap between any two consecutive eigenvalues that
previous works have achieved.
PEG gets evaluated in the most important node-set-based task, link prediction, over 8 real-world
networks. PEG achieves comparable performance with strong baselines based on DE while having
much lower training and inference complexity. PEG achieves significantly better performance than
other baselines without using DE. Such performance gaps get enlarged when we conduct domain-shift
link prediction, where the networks used for training and testing are from different domains, which
effectively demonstrates the strong generalization and transferability of PEG.
1.1	Other related works
As long as GNNs can be explained by a node-feature-refinement procedure (Hamilton et al., 2017;
Gilmer et al., 2017; Morris et al., 2019; VeIiCkOViC et al., 2018; Klicpera et al., 2019; Chien et al.,
2021), they suffer from the aforementioned node ambiguity issue. Some GNNs cannot be explained
as node-feature refinement as they directly track node-set representations (Maron et al., 2019b; Morris
2
Published as a conference paper at ICLR 2022
et al., 2019; Chen et al., 2019; Maron et al., 2019a). However, their complexity is high, as they need
to compute the representation of each node set of certain size. Moreover, they were only studied for
graph-level tasks. EGNN (Satorras et al., 2021) seems to be a relevant work as it studies when the
nodes have physical coordinates given in prior. However, no analysis of PE has been provided.
A few works studied the stability of GNNs by using tools like graph scattering transform (for
graph-level representation) (Gama et al., 2019a;b) and graph signal filtering (for node-level represen-
tations) (Levie et al., 2021; Ruiz et al., 2020; 2021; Gama et al., 2020; Nilsson & Bresson, 2020).
They all focus on justifying the stability of the canonical GNN pipeline, graph convolution layers in
particular. None of them consider positional features let alone the stability of GNNs using PE.
2	Notations and Preliminaries
In this section, we prepare the notations and preliminaries that are useful later. First, we define graphs.
Definition 2.1 (Graph). Unless specified, we always consider undirected graphs of N nodes and
let [N] = {1, 2, ..., N}. One such graph can be denoted as G = (A, X), where A is the adjacency
matrix. X ∈ RN×F denotes the node features, where the vth row, Xv, is the feature vector of node
v. A graph may have self loops, i.e., A has nonzero diagonals. Denote D as the diagonal degree
matrix where Dvv = Pu∈[N] Avu for v ∈ [N]. Let dmax = maxv∈[N] Dvv. Denote the normalized
adjacency matrix as A = D- 1 AD-2 and the normalized Laplacian matrix as L = I - A.
Definition 2.2 (Permutation). An N -dim permutation matrix P is a matrix in {0, 1}N ×N where each
row and each column has only one single 1. All such matrices are collected in Π(N), simplified as Π.
We denote the vector '2-norm as ∣∣∙k, the FrobeniUs norm as k」F and the operator norm as ∣∣∙∣∣op∙
Definition 2.3 (Graph-matching). Given two graphs G(i) = (A(i), X(i)) and their normalized
Laplacian matrices L(i) for i ∈ {1, 2}, their matching can be denoted by a permUtation matrix P ∈ Π
that best aligns the graph strUctUres and the node featUres.
P*(G⑴，G(2))，argmin IlL⑴-PL⑵PTIlF + ∣∣X⑴-PX⑵IlF
P∈Π
Using L instead of A to represent graph strUctUres is for notational simplicity. ActUally for an
Unweighted graph, there is a bijective mapping between L and A. One can rewrite the first term with
A. Later, We use P * by not specifying Gi and G2 if there is no confusion. The distance between the
two graphs can be defined as d(Gi, G2) = ∣L ⑴-P * L(2)P *T ∣f + ∣∣X ⑴-P *X ⑵ ∣∣f.
Next, we review eigenvalue decomposition and summarize arguments on its uniqueness in Lemma 2.6.
Definition 2.4 (Eigenvalue Decomposition (EVD)). For a positive semidefinite (PSD) matrix B ∈
RN ×N, it has eigenvalue decomposition B = UΛUT where Λ is a real diagonal matrix with the
eigenvalues of B, 0 ≤ λ1 ≤ λ2 ≤ ... ≤ λN as its diagonal components. U = [u1, u2, ..., uN] is an
orthogonal matrix where ui ∈ RN is the ith eigenvector, i.e. Bui = λiui .
Definition 2.5 (Orthogonal Group in the Euclidean space). SO(k) = {Q ∈ Rk×k |QQT = QTQ =
I} includes all k-by-k orthogonal matrices. A subgroup of SO(k) includes all diagonal matrices with
±1 as the diagonal components, SN(k) = {Q ∈ Rk×k |Qii ∈ {-1, 1}, Qij = 0, fori 6= j}.
Lemma 2.6. EVD is not unique. If all the eigenvalues are distinct, i.e., λi 6= λj , U is unique up to
the signs of its columns, i.e., replacing ui by -ui also gives EVD. If there are multiple eigenvalues,
say (λi-1 <)λi = λi+1 = ... = λi+k-1 (< λi+k ), then [ui , ui+1 , ..., ui+k-1] lie in an orbit induced
by the orthogonal group SO(k), i.e., replacing [ui, ui+1, ..., ui+k-1] by [ui, ui+1, ..., ui+k-1]Q for
any Q ∈ SO(k) while keeping eigenvalues and other eigenvectors unchanged also gives EVD.
Next, we define Positional Encoding (PE), which associates each node with a vector in a metric space
where the distance between two vectors can represent the distance between the nodes in the graph.
Definition 2.7 (Positional Encoding). Given a graph G = (A, X), PE works on A and gives
Z = PE(A) ∈ RN ×p where each row Zv gives the positional feature of node v .
The absolute values given by PE may not be useful while the distances between the positional features
are more relevant. So, we define PE-matching that allows rotation to best match positional features,
which further defines the distance between two collections of positional features.
Definition 2.8 (PE-matching). Consider two groups of positional features Z(1), Z(2) ∈ RN×p. Their
matching is given by Q*(Z(1), Z(2)) , arg minQ∈SO(p) IZ(1) - Z(2)QIF. Later, Q* is used if it
causes no confusion. Define the distance between them as η(Z(1), Z(2)) = IZ(1) - Z(2)Q* IF.
3
Published as a conference paper at ICLR 2022
Figure 2: Eigenvalues λp and the stability ratio ρp
is far more stable than previous methods with sensitivity maxι≤k≤p ∣λk - λk+1∣-1. Over Citeseer, Pp is
extremely large because there are multiple eigenvalues (ρp is still finite due to the numerical approximation of
the eigenvalues). Over Twitch(PT), even if there are no multiple eigenvalues, ρp is mostly larger than 10.
min]k≤-λp+-λk + ι∣ . PEGWithSenSitiVity lλp - λp+1l-1
3	Equivariant and Stable Positional Encoding for GNN
In this section, we will study the equivariance and stability of a GNN layer using PE .
3.1	Our Goal: Building Permutation Equivariant and Stable GNN layers
A GNN layer is expected to be permutation equivariant and stable. These two properties, if satisfied,
guarantee that the GNN layer is transferable and has better generalization performance. Permutation
equivariance implies that model predictions should be irrelevant to how one indexes the nodes, which
captures the fundamental inductive bias of many graph learning problems: A permutation equivariant
layer can make the same prediction over a new testing graph as that over a training graph if the two
graphs match each other perfectly, i.e. the distance between them is 0. Stability is actually an even
stronger condition than permutation equivariance because it characterizes how much gap between the
predictions of the two graphs is expected when they do not perfectly match each other.
Specifically, given a graph A with node features X, we consider a GNN layer g updating the node
features, denoted as X = g(A, X). We define permutation equivariance and stability as follows.
Definition 3.1 (Permutation Equivariance). A GNN layer g is permutation equivariant, if for any
P ∈ Π and any graph G = (A, X), Pg(A, X) = g(PAPT, PX).
Definition 3.2 (Stability). A GNN layer g is claimed to be stable, if there exists a constant C > 0,
for any two graphs G⑴=(A(I), X(1)) and G(2) = (A(2) ,X(2)), letting P* = P*(G⑴,G(2))
denote their matching, g satisfies kg(A(1) ,X ⑴)—P*g(A ⑵,X ⑵)kF ≤ Cd(G ⑴,G ⑵).By setting
A(I) = PA(2)PT and X⑴=PX⑵ for some P ∈ Π, the RHS becomes zero, so a stable g makes
the LHS zero too. So, stability is a stronger condition than permutation equivariance.
Our goal is to guarantee that the GNN layer that utilizes PE is permutation equivariant and stable. To
distinguish from the GNN layer gg that does not use PE, we use g to denote a GNN layer that uses PE,
which takes the positional features Z as one input and may update both node features X and Z, i.e.,
(X, Z) = g(A, X, Z). Now, we may define PE-equivariance and PE-stability for the GNN layer g.
Definition 3.3 (PE-stability & PE-equivariance). Consider a GNN layer g that uses PE. When it works
on any two graphs G(i) = (A⑶,X⑺),i ∈ {1, 2} and gives (X(i), ZZ(i)) = g(A(i),X⑺,Z(i)), let
P * be the matching between the two graphs. g is PE-stable, if for some constant C > 0 we have
kX ⑴ — P * X ⑵kF + η(zZ(1),P *z>(2) ) ≤ Cd(G ⑴,G ⑵).	(1)
Recall η(∙, ∙) measures the distance between two sets of positional features as defined in Def. 2.8.
Similar as above, a weaker condition of PE-stability is PE-equivariance: If A(1) = PA(2) PT and
X(1) = PX(2) for some P ∈ Π, we expect a perfect match between the updated node features and
positional features, X⑴=PX⑵ and η(Zζ(1), PZζ(2'>) = 0.
Note that previous works also consider g that updates only node features, i.e., X = g(A, X, Z). In
this case, PE-stability can be measured by removing the second term on Z from Eq.1.
3.2	PE-stable GNN layers based on Laplacian Eigenmap as Positional Encoding
To study the requirement of the GNN layer that achieves PE-stability, let us start from a particular
PE technique, i.e., Laplacian eigenmap (LE) (Belkin & Niyogi, 2003) to show the key insight
behind. Later, we generalize the concept to some other PE. Let ZLE denote LE, which includes the
eigenvectors that correspond to the p smallest eigenvalues of the normalized Laplacian matrix L.
4
Published as a conference paper at ICLR 2022
Previous works failed to design PE-equivariant or PE-stable GNN layers. Srinivasan & Ribeiro
(2020) claims that if a GNN layer g(A, X) that does not rely on PE is permutation equivariant,
PE-equivariance may be kept by adding ZLE to node features, i.e., g(A, X, ZLE) = g(A, X +
MLP(ZLE)), where MLP(∙) is a multilayer perceptron that adjusts the dimension properly. This
statement is problematic when ZLE is not unique given the graph structure A. Specifically, though
sharing graph structure A(1) = A(2), if different implementations lead to different LEs ZL(1E) 6= ZL(2E) ,
then g(A(1),X⑴ + MLP(ZLE)) = g(A(2), X⑵ + MLP(ZLE)), which violates PE-equivariance.
Srinivasan & Ribeiro (2020) suggests using graph permutation augmentation to address the issue,
which makes assumptions on an invariant distribution of ZLE that may not be guaranteed empirically.
Dwivedi & Bresson (2020); Kreuzer et al. (2021) claim the uniqueness of ZLE up to the signs and
suggest building a GNN layer that uses random ZLE as g(A, X, ZLE) = g(A, X + MLP(ZLES))
where S is uniformly at random sampled from SN(p). They expect PE-equivariance in the sense of
expectation. However, this statement is generally incorrect because it depends on the condition that
all eigenvalues have to be distinct as stated in Lemma 2.6. Actually, for multiple eigenvalues, there
are infinitely many eigenvectors that lie in the orbit induced by the orthogonal group. Although many
real graphs have multiple eigenvalues such as disconnected graphs or graphs with some non-trivial
automophism, one may argue that the methods work when the eigenvalues are all distinct. However,
the above failure may further yield PE-instability even when the eigenvalues are distinct but have
small gaps due to the following lemma.
Lemma 3.4. For any PSD matrix B ∈ RN ×N without multiple eigenvalues, set positional encoding
PE(B) as the eigenvectors given by the smallest p eigenvalues sorted as 0 = λ1 < λ2 < ... < λp(<
λp+1) of B. For any sufficiently small > 0, there exists a perturbation 4B, k4BkF ≤ such that
min ∣∣PE(B) — PE(B + 4B)S∣∣f ≥ 0.99 max ∣λi+ι — λi∣-1k4B∣∣F + o(e).	(2)
S∈SN(p)	1≤i≤p
Lemma 3.4 implies that small perturbation of graph structures may yield a big change of eigenvectors
if there is a small eigengap. Consider two graphs G(1) = (A(1), X(1)) and G(2) = (A(2), X(2)),
where X(1) = X(2) and A(2) is A(1) with a small perturbation 4A(1). The perturbation is small
enough so that the matching P*(G(1), G(2)) = I. However, the change of PE even after removing
the effect of changing signs minS∈SN(p) ∣PE(A(1)) — PE(A(2))S∣ could be dominated by the largest
inverse eigengap among the first P + 1 eigenvalues maxι≤i≤p ∣λi+ι — λ/-1. In practice, it is hard to
guarantee all these p eigenpairs have large gaps, especially when a large p is used to locate each node
more accurately. Plugging this PE into a GNN layer gives the updated node features X(i) a large
change ∣X⑴—X(2)|f and thus violates PE-stability.
PE-stable GNN layers. Although a particular eigenvector may be not stable, the eigenspace, i.e., the
space spanned by the columns of PE(A) could be much more stable. This motivates our following
design of the PE-stable GNN layers. Formally, we use the following lemma that can characterize the
distance between the eigenspaces spanned by LEs of two graph Laplacians. The error is controlled by
the inverse eigengap between thepth and (P + 1)th eigenvalues mini=ι,2{∣λPi) — λP+∕-1}, which
by properly setting P is typically much smaller than maxι≤k≤p ∣λk — λk+ι∣-1 in Lemma 3.4. We
compute the ratio between these two values over some real-world graphs as shown in Fig. 2.
Lemma 3.5. For two PSD matrices B(1), B(2) ∈ RN×N, set PE(B) as the eigenvectors given by
the P smallest eigenvalues of B. Suppose B(i) has eigenvalues 0 = λ(1i) ≤ λ(2i) ≤ ... ≤ λ(pi) ≤ λ(pi+) 1
and δ = mini=ι,2{∣λPi) — λP+∕-1} < ∞. Then, for any permutation matrix P ∈ Π,
η(PE(B⑴),PPE(B⑵))≤ 23δmin{√P∣B⑴—PB⑵PTkOPjB⑴—PB⑵PT∣f}	(3)
Inspired by the stability of the eigenspace, the idea to achieve PE-stability is to make the GNN layer
invariant to the selection of bases of the eigenspace for the positional features. So, our proposed PE-
stable GNN layer g that uses PE should satisfy two necessary conditions: 1) Permutation equivariance
w.r.t. all features; 2) Rotation equivariance w.r.t. positional features, i.e.,
PE-Stable layer cond.1:	(PX, PZ) = g(PAPT, PX, PZ), ∀P ∈ Π(N),	(4)
PE-Stable layer cond. 2:	(X, ZQ) = g(A, X, ZQ), ∀Q ∈ SO(p).	(5)
5
Published as a conference paper at ICLR 2022
Rotation equivariance reflects the eigenspace instead of a particular selection of eigenvectors and thus
achieves much better stability. Interestingly, these requirements can be satisfied by EGNN recently
proposed (Satorras et al., 2021) as one can view the physical coordinates of objects considered by
EGNN as the positional features. EGNN gets briefly reviewed in Appendix F. Thm. 3.6 proves
PE-equivariance under the conditions Eqs. 4 and 5.
Theorem 3.6.	A GNN layer g(A, X, Z) that uses Z = ZLE and satisfies Eqs. 4,5 is PE-equivariant if
the pth and (p + 1)th eigenvalues of the normalized Laplacian matrix L are different, i.e., λp 6= λp+1.
Note that satisfying Eqs. 4,5 is insufficient to guarantee PE-stability that depends on the form of g.
We implement g in our model PEGN with further simplification which has already achieved good
empirical performance: Use a GCN layer with edge weights according to the distance between the
end nodes of the edge and keep the positional features unchanged. This gives the PEG layer,
PEG: gPEG(A,X,Z) = (ψ [(A Θ ξ) XW] ,z) , where Ξuv = φ (||Z“ - Zv ∣∣), ∀u,v ∈ [N].	(6)
Here ψ is an element-wise activation function, φ is an MLP mapping from R → R and is the
Hadamard product. Note that if A is sparse, only Ξuv for an edge Uv needs to be computed.
We may also prove that the PEG layer gPEG satisfies the even stronger condition PE-stability.
Theorem 3.7.	Consider two graphs G(i) = (A(i), X(i)), i = 1, 2. Denote their normalized Laplacian
matrices’ pth eigenvalue as λ(pi) and (p+ 1)th eigenvalue as λ(pi+) 1. Assume that δ = mini=1,2(λ(pi+) 1 -
λpi))-1 < ∞. Also, assume that ψ and φ in the PEG layer in Eq. 6 are 'ψ, 'φ-Lipschitz continuous
respectively. Then, the PEG layer that uses Z = ZLE satisfies PE-stability with the constant C in
Eq. 1 as C = [(7δ∣∣X⑴∣R + 2d(2ax)'ψ'φ∣∣Wkop + 3δ].
Note that to achieve PE-stability, we need to normalize the node initial features to keep kX (1) kop
bounded, and control ∣∣W∣∣op and 'φ. In practice 'ψ ≤ 1 is typically satisfied, e.g. setting ψ as
ReLU. Here, the most important term is δ. PE-stability may only be achieved when there is an
eigengap between λp and λp+1 and the larger eigengap, the more stable. This observation may be
also instructive to select the p in practice. As previous works may encounter a smaller eigengap
(Lemma 3.4), their models will be generally more unstable.
Also, the simplified form of gPEG is not necessary to achieve PE-stability. However, as it has already
given consistently better empirical performance than baselines, we choose to keep using gPEG.
3.3 Generalized to Other PE techniques: DeepWalk and LINE
It is well known that LE, as to compute the smallest eigenvalues, can be written as a low-rank matrix
optimization form ZLE ∈ arg minZ∈RN ×p tr(LNZZT), s.t. ZTZ = I. Other PE techniques, such as
Deepwalk (Perozzi et al., 2014), Node2vec (Grover & Leskovec, 2016) and LINE (Tang et al., 2015)
can be unified into a similar optimization form, where the positional features Z are given by matrix
factorization M* = Z0ZT (M* may be asymmetric so Z0 ∈ RN×p may not be Z) and M* satisfies
M* = arg min 'pe(A,M)，tr(f+(A)g(M) + f-(D)g(-M)), s.t. rank(M) ≤ P (7)
M∈RN×N
Here f+ (∙) : RN×N → R≥≥×n typically revises A by combining degree normalization and a power
series of A, f-(∙) : RN ×N → RN×N corresponds to edge negative sampling that may be related to
node degrees, and g(∙) isa component-wise log-sigmoid function X — log(1 + exp(x)). E.g., in LINE,
f+(A) = A, f-(O) = c11TD3, for some positive constant c, where 1 is the all-one vector. More
discussion on Eq.7 and the forms of f+ and f- for other PE techniques are given in Appendix G.
According to the above optimization formulation of PE, all the PE techniques generate positional
features Z based on matrix factorization, thus are not unique. Z always lies in the orbit induced by
SO(p), i.e., if Z solves Eq.7, ZQ for Q ∈ SO(p) solves it too. A GNN layer g still needs to satisfy
Eqs. 4,5 to guarantee PE-equivariance. PE-stability actually asks even more.
Theorem 3.8.	Consider a general PE(A) technique that has an optimization objective as Eq.7,
computes its optimal solution M* and decomposes M* = Z0ZT, s.t. ZTZ = I ∈ Rp×p to get
positional features Z. Essentially, Z consists of the right-singular vectors of M*. If Eq.7 has a
unique solution M*, rank(M *) = p and f+, f- therein satisfy f+ (PAPT) = Pf+ (A)PT and
f-(PDPT) = Pf-(D)PT, then a GNN layer g(A, X, Z) that satisfies Eqs. 4,5 is PE-equivariant.
6
Published as a conference paper at ICLR 2022
Note that the conditions on f+ and f- are generally satisfied by Deepwalk, Node2vec and LINE.
However, solving Eq.7 to get the optimal solution may not be guaranteed in practice because the
low-rank constraint is non-convex. One may consider relaxing the low-rank constraint into the nuclear
norm ∣∣Mk* ≤ T for some threshold T (Recht et al., 2010), which reduces the optimization Eq.7 into
a convex optimization and thus satisfies the conditions in Thm. 3.8. Empirically, this step and the
step of computing SVD seem unnecessary according to our experiments. The PE-stability is related
to the value of the smallest non-zero singular value of M*. We leave the full characterization of
PE-equivariance and PE-stability for the general PE techniques for the future study.
4	Experiments
In this work, we use the most important node-set-based task link prediction to evaluate PEG, though
it may apply to more general tasks. Two types of link prediction tasks are investigated: traditional
link prediction (Task 1) and domain-shift link prediction (Task 2). In Task 1, the model gets trained,
validated and tested over the same graphs while using different link sets. In Task 2, the graph used for
training/validation is different from the one used for testing. Both tasks may reflect the effectiveness
of a model while Task 2 may better demonstrate the model’s generalization capability that strongly
depends on permutation equivariance and stability. All the results are based on 10 times random tests.
4.1	The Experimental Pipeline of PEG for Link Prediction
We use PEG to build GNNs. The pipeline contains three steps. First, we adopt certain PE techniques
to compute positional features Z. Second, we stack PEG layers according to Eq. 6. Suppose the final
node representations are denoted by (X, Z). Third, for link prediction over (u, v), we concatenate
(XuXT, ZuZT) denoted as HuV and adopt MLP(Huv) to make final predictions. In the experiments,
we test LE and Deepwalk as PE, and name the models PEG-LE and PEG-DW respectively. To verify
the wide applicability of our theory, we also apply PEG to GraphSAGE Hamilton et al. (2017) by
similarly using the distance between PEs to control the neighbor aggregation according to Eq. 6 and
term the corresponding models PE-SAGE-LE and PE-SAGE-DW respectively.
For some graphs, especially those small ones where the union of link sets for training, validation and
testing cover the entire graph, the model may overfit positional features and hold a large generalization
gap. This is because the links used as labels to supervise the model training are also used to generate
positional features. To avoid this issue, we consider a more elegant way to use the training links. We
adopt a 10-fold partition of the training set. For each epoch, we periodically pick one fold of the links
to supervise the model while using the rest links to compute positional features. Note that PEs for
different folds can be pre-computed by removing every fold of links, which reduces computational
overhead. In practice, the testing stage often corresponds to online service and has stricter time
constraints. Such partition is not needed so there is no computational overhead for testing. We term
the models trained in this way as PEG-LE+ and PEG-DW+ respectively.
4.2	Task 1 — Traditional link prediction
Datasets. We use eight real graphs, including Cora, CiteSeer and Pubmed (Sen et al., 2008), Twitch
(RU), Twitch (PT) and Chameleon (Rozemberczki et al., 2021), DDI and COLLAB (Hu et al., 2021).
Over the first 6 graphs, we utilize 85%, 5%, 10% to partition the link set that gives positive examples
for training, validation and testing and pair them with the same numbers of negative examples
(missing edges). For the two OGB graphs, we adopt the dataset splits in (Hu et al., 2020). The links
for validation and test are removed during the training stage and the links for validation are also not
used in the test stage. All the models are trained till the loss converges and the models with the best
validation performance is used to test.
Baselines. We choose 6 baselines: VGAE (Kipf & Welling, 2016), P-GNN (You et al., 2019), SEAL
(Zhang & Chen, 2018), GNN Trans. (Dwivedi & Bresson, 2020), LE (Belkin & Niyogi, 2003) and
Deepwalk (DW) (Perozzi et al., 2014). VGAE is a variational version of GAE (Kipf & Welling,
2016) that utilizes GCN to encode both the structural information and node feature information, and
then decode the node embeddings to reconstruct the graph adjacency matrix. SEAL is particularly
designed for link prediction by using enclosing subgraph representations of target node-pairs. P-GNN
randomly chooses some anchor nodes and aggregates only from these anchor nodes. GNN Trans.
adopts LE as PE and merges LE into node features and utilizes attention mechanism to aggregate
information from neighbor nodes. For VGAE, P-GNN and GNN Trans., the inner product of two
node embeddings is adopted to represent links. LE and DW are two network embedding methods,
where the obtained node embeddings are already positional features and directly used to predict links.
7
Published as a conference paper at ICLR 2022
Table 1: Performance on the traditional link prediction tasks, measured in ROC AUC (mean±std%).
Method	Feature	Cora	Citeseer	Pubmed	TWitCh-RU	Twitch-PT	Chameleon
	-N-	89.89 ± 0.06	90.11±0.08	94.62 ± 0.02	83.13±0.07	82.89 ± 0.08	97.98±0.01
	C.	55.68 ± 0.05	61.45±0.36	69.03 ± 0.03	85.37 ± 0.02	85.69 ± 0.09	83.13±0.04
	O.	83.97±0.05	77.22±0.04	82.54 ± 0.04	84.76 ± 0.09	87.91 ± 0.05	97.67±0.04
VGAE	P.	83.82±0.12	78.68±0.25	81.74±0.15	85.06±0.14	85.06±0.14	97.91±0.03
	R.	68.43 ± 0.42	71.21±0.78	69.31±0.23	68.42 ± 0.43	68.49 ± 0.73	73.44±0.53
	N. + P.	87.96±0.29	80.04 ± 0.60	85.26±0.17	84.59 ±0.37	88.27±0.19	98.01±0.12
PGNN	N. + P.	86.92 ± 0.02	90.26 ± 0.02	88.12±0.06	83.21 ±0.00	82.37 ± 0.02	94.25 ± 0.01
GNN-Trans.	N. + P.	79.31±0.09	77.49±0.02	81.23±0.12	79.24±0.33	75.44±0.14	86.23 ±0.12
SEAL	N. + D.	91.32±0.91	89.49±0.43	97.16±0.28	92.12±0.10	93.21 ± 0.06t	99.31 ± 0.18t
LE	-P	84.43 ± 0.02	78.36±0.08	84.35±0.04	78.80±0.10	67.56±0.02	88.47±0.03
DW	P.	86.82±0.18	87.93 ±0.11	85.79 ± 0.06	83.10±0.05	83.47 ± 0.03	92.15±0.02
PEG-DW	N. + P.	89.51 ± 0.08	91.67±0.12	87.68 ± 0.29	90.21 ±0.04	89.67 ± 0.03	98.33 ±0.01
PEG-DW	C. + P.	88.36±0.10	88.48±0.10	88.80±0.11	90.32 ± 0.09	90.88 ± 0.05	97.30±0.03
PEG-LE	N. + P.	94.20 ± 0.04t	92.53 ±0.09	87.70±0.31	92.14±0.05	92.28 ± 0.02	98.78±0.02
PEG-LE	C. + P.	86.88 ± 0.03	76.96 ± 0.23	91.65±0.02	90.21 ±0.18	91.15±0.13	98.73 ±0.04
PEG-DW+	N. + P.	93.32±0.08	94.11±0.14	97.88 ± 0.05	91.68±0.01	92.15±0.02	98.20±0.01
PEG-DW+	C. + P.	90.78 ± 0.09	91.22±0.12	93.44±0.05	90.22 ± 0.04	91.37±0.05	97.50±0.03
PEG-LE+	N. + P.	93.78±0.03	95.73 ± 0.09t	97.92 ± 0.1lt	92.29 ±0.11	92.37 ± 0.06	98.18±0.02
PEG-LE+	C. + P.	88.98±0.14	78.61±0.27	94.28 ± 0.05	92.35 ± 0.02t	92.50±0.06	97.79±0.01
Table 2: Performance on OGB datasets, measured in Hit@20 and Hits@50 (mean±std%). Codes are
run on CPU: Intel(R) Xeon(R) Gold 6248R @ 3.00GHz and GPU: NVIDIA QUADRO RTX 6000.
Method		ogbl-ddi(Hits@20(%))						ogbl-collab (HitS@50(%))				
	training time	test time	Validation	test	training time	test time	Validation	test
GCN	29min 27s	0.20s	55.27 ± 0.53	37.11±0.21	1h38min17s	1.38s	52.71 ± 0.10	44.62 ± 0.01
GraphSAGE	14min 26s	0.24s	67.11 ± 1.21	52.81 ± 8.75	38min 10s	0.83s	57.16 ± 0.70	48.45 ± 0.80
SEAL	2h 04min 32s	12.04s	28.29 ± 0.38	30.23 ± 0.24	2h29min05s	51.28s	64.95 ± 0.04	54.71+0.01t
PGNN	9min 49.39s	0.28s	2.66 ± 0.16	1.74 ± 0.19	N/A	N/A	N/A	N/A
GNN-trans.	53min 26s	0.35s	15.63 ± 0.14	9.22±0.21	1h52min22s	1.86s	18.17 ± 0.25	11.19 ± 0.42
DW	36min 41s	0.23s	0.04 ± 0.00	-0.02 ± 0.00-	34min40s	1.08s	53.64 ± 0.03	44.79 ± 0.02
LE	33min 42s	0.29s	0.09 ± 0.00	0.02 ± 0.00	37min22s	1.23s	0.10 ± 0.01	0.12 ± 0.02
PEG-DW	29min 56s	0.27s	56.47 ± 0.35	43.80±0.32	1h42min 05s	1.51s	63.98 ± 0.05	54.33 ± 0.06
PEG-LE	30min 32s	0.29s	57.49 ± 0.47	30.16 ± 0.47	1h42min03s	1.42s	56.52 ± 0.12	48.76 ± 0.92
PE-SAGE-DW	25min 11s	0.31s	68.05 ± 0.96	56.16±5.50t	56min54s	0.97s	63.43 ± 0.48	54.17 ± 0.54
PE-SAGE-LE	26min 19s	0.32s	68.38 ± 0.78	51.49 ± 9.71	55min59s	0.98s	58.66 ± 0.55	49.75 ± 0.67
PEG-DW+	48min 03s	0.28s	59.70 ± 6.87	47.93 ± 0.21	1h37min43s	1.43s	62.31 ± 0.19	53.71 ± 8.02
PEG-LE+	51min 25.35s	0.29s	58.44 ± 1.71	28.32 ±7.34	1h33min29s	1.39s	52.91 ± 1.24	45.96 ± 9.98
Implementation details. For VGAE, we consider six types of features: (1) node feature (N.): original
feature of each node. (2) constant feature (C.): node degree. (3) positional feature (P.): PE extracted
by Deepwalk. (4) one-hot feature (O.): one-hot encoding of node indices. (5) random feature (R.):
random value 九 〜U(0,1) (6) node feature and positional feature (N. + P.): concatenating the node
feature and the positional feature. P-GNN uses node features and the distances from a node to some
randomly selected anchor nodes as positional features (N. + P.). GNN Trans. utilizes node features
and LE as positional features (N. + P.). SEAL adopt Double-Radius Node Labeling (DRNL) to
compute deterministic distance features (N. + D.). For PEG, we consider node features plus positional
features (N. + P.) or constant feature plus positional features (C. + P.).
Results are shown in Table 1 and Table 2. Over the small datasets in Table 1, VGAE with node
features outperforms other features in Cora, Citeseer and Pubmed because the nodes features therein
are mostly informative, while this is not true over the other three datasets. One-hot features and
positional features almost achieve the same performance, which implies that that GNNs naively using
PE makes positional features behave like one-hot features and may have instability issues. Constant
features are not good because of the node ambiguity issues. Random features may introduce heavy
noise that causes trouble in model convergence. Concatenating node features and positional features
gives better performance than only using positional features but is sometimes worse than only using
node features, which is again due to the instability issue by using positional features.
Although PGNN and GNN Trans. utilize positional features, they achieve subpar performance.
SEAL outperforms all of the state-of-art methods, which again demonstrates that the effectiveness of
distance features (Li et al., 2020; Zhang et al., 2020). PEG significantly outperforms all the baselines
except SEAL. PEG+ by better using training sets achieves comparable or even better performance
than SEAL, which demonstrates the contributions of the stable usage of PE. Moreover, PEG can
achieve comparable performance in most cases even only paired with constant features, which benefits
from the more expressive power given by PE (avoids node ambiguity). Note that PE without GNNs
(LE or DW solo) does not perform well, which justifies the benefit by joining GNN with PE.
8
Published as a conference paper at ICLR 2022
Table 3: Performance on the domain-shift link prediction tasks, measured in ROC AUC (mean±std%)
Mehood	Features	Cora→Citeseer	Cora→Pubmed	ES→PT	EN→RU	PPI
	-N	62.74 ± 0.03-	63.53 ± 0.27-	51.52±0.17	60.08 ± 0.02	83.24±0.20
	C.	62.16±0.08	56.89 ± 0.36	82.72 ± 0.26	91.23±0.07	75.27 ±0.89
VGAE	P.	70.59 ± 0.03	79.83 ± 0.27	82.24 ± 0.24	81.42±0.01	77.61±0.47
	R.	68.44 ± 0.63	71.27±0.37	71.26±0.36	69.37 ± 0.35	75.88±0.49
	N. + P.	76.45 ±0.55	65.62±0.42	71.46±0.31	84.00 ± 0.28	84.67 ± 0.22
PGNN	N. + P.	85.02±0.28-	76.88 ± 0.42	70.41±0.07	63.27 ± 0.27	80.84 ± 0.03
GNN-Trans.	N. + P.	61.60±0.52	76.35 ±0.17	63.44 ± 0.34	62.87 ± 0.22	79.82±0.17
SEAL	N. + D.	91.36 ± 0.93t	89.62±0.87	93.37 ± 0.05t	92.34±0.14	88.99±0.12
LE	P	77.62±0.04-	84.03 ± 0.22	67.75 ± 0.09	77.57 ±0.15	72.14±0.82
DW	P.	86.48±0.14	86.97 ± 0.06	83.56 ± 0.03	83.41±0.04	85.18±0.20
PEG-DW	N. + P.	89.91±0.03-	87.23 ± 0.34-	91.82±0.04	91.14±0.02	87.36±0.11
PEG-DW	C. + P.	89.75 ± 0.04	89.58±0.08	91.27±0.04	90.26 ± 0.07	86.42±0.20
PEG-LE	N. + P.	82.57 ± 0.02	92.34 ± 0.28	91.61±0.05	91.93±0.13	85.34±0.14
PEG-LE	C. + P.	79.60 ± 0.04	88.89±0.13	91.38±0.10	92.40±0.10	85.22±0.16
PEG-DW+	N. + P.	91.15±0.06-	90.98 ± 0.03-	91.24±0.16	91.91±0.02	89.92±0.17t
PEG-DW+	C. + P.	91.32±0.01	90.93±0.18	91.22±0.02	92.14±0.02	88.44 ± 0.29
PEG-LE+	N. + P.	86.72±0.05	93∙34±0.1lt	91.67±0.13	92.24 ±0.19	86.77 ± 0.36
PEG-LE+	C. + P.	87.62±0.04	92.21 ± 0.20	91.37±0.19	93.12±0.2"	86.21 ± 0.27
Regarding the OGB datasets, PGNN and GNN Trans. do not perform well either. Besides, PGNN
cannot scale to large graphs such as collab. The results of DW and LE demonstrate that the original
positional features may only provide crude information, so pairing them with GNNs is helpful. PEG
achieves the best results on ddi, and performs competitively with SEAL on collab. The complexity
of PEG is comparable to canonical GNNs. Note that we do not count the time of PE as it relates
to the particular implementation. If PEG is used for online serving when the time complexity is
more important, PE can be computed in prior. For a fair comparison, we also do not count the
pre-processing time of SEAL, PGNN or GNN Trans.. Most importantly, PEG performs significantly
faster than SEAL on test because SEAL needs to compute distance features for every link while PE in
PEG is shared by links. Interestingly, DW seems better than LE as a PE technique for large networks.
4.3	Task 2 —Domain-shift link prediction
Datasets & Baselines. Task 2 better evaluates the generalization capability of models. We consider
3 groups, including citation networks (cora→citeseer and cora→pubmed) (Sen et al., 2008), user-
interaction networks (Twitch (EN)→Twitch (RU) and Twitch (ES)→Twitch (PT)) (Rozemberczki
et al., 2021) and biological networks (PPI) (Hamilton et al., 2017). For citation networks and user-
interaction networks, we utilize 95%, 5% dataset splitting for training and validation over the training
graph, and we use 10% existing links as test positive links in the test graph. For PPI dataset, we
randomly select 3 graphs as training, validation and testing datasets and we sample 10% existing
links in the validation/testing graphs as validation/test positive links.
Baselines & Implementation details. We use the same baselines as Task 1, while we do not use
one-hot features for VGAE, since the training and test graphs have different sizes. As for node
features, we randomly project them into the same dimension and then perform row normalization on
them. Other settings are the same as Task 1. PE is applied to training and testing graphs separately
while PE over testing graphs is computed after removing the testing links.
Results are shown in Table 3. Compared with Table 1, for VGAE, we notice that node features
perform much worse (except PPI) than Task 1, which demonstrates the risks of using node features
when the domain shifts. Positional features, which is not specified for the same graph, is possibly
more generalizable over different graphs. Random features are generalizable while still hard to
converge. PGNN and GNN Trans. do not utilize positional features appropriately and perform far
from ideal. Both SEAL and PEG outperform other baselines significantly, which implies their good
stability and generalization. PEG and SEAL again achieve comparable performance while PEG has
much lower training and testing complexity. Our results successfully demonstrate the significance of
using permutation equivariant and stable PE.
5	Conclusion
In this work, we studied how GNNs should work with PE in principle, and proposed the conditions
that keep GNNs permutation equivariant and stable when PE is used to avoid the node ambiguity issue.
We follow those conditions and propose the PEG layer. Extensive experiments on link prediction
demonstrate the effectiveness of PEG. In the future, we plan to generalize the theory to more general
PE techniques and test PEG over other graph learning tasks.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We greatly thank the actionable suggestions given by reviewers. H. Yin and P. L. are supported by the
2021 JPMorgan Faculty Award and the National Science Foundation (NSF) award HDR-2117997.
References
Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power
of graph neural networks with random node initialization. arXiv preprint arXiv:2010.01179, 2020.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural computation,15(6):1373-1396, 2003.
Maciej Besta, Raphael Grob, Cesare Miglioli, Nicola Bernold, Grzegorz Kwasniewski, Gabriel
Gjini, Raghavendra Kanakagiri, Saleh Ashkboos, Lukas Gianinazzi, Nikoli Dryden, et al. Motif
prediction with graph neural networks. arXiv preprint arXiv:2106.00761, 2021.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In Advances in Neural Information
Processing Systems, pp. 15868-15876, 2019.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? Advances in Neural Information Processing Systems, 2020.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In International Conference on Learning Representations, 2021.
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM
Journal on Numerical Analysis, 7(1):1-46, 1970.
Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.
arXiv preprint arXiv:2012.09699, 2020.
Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Diffusion scattering transforms on graphs. In
International Conference on Learning Representations, 2019a.
Fernando Gama, Alejandro Ribeiro, and Joan Bruna. Stability of graph scattering transforms.
Advances in Neural Information Processing Systems, 32:8038-8048, 2019b.
Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.
IEEE Transactions on Signal Processing, 68:5680-5695, 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263-1272. PMLR, 2017.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social networks, 5(2):109-137, 1983.
Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359-366, 1989.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in
Neural Information Processing Systems, 2020.
10
Published as a conference paper at ICLR 2022
Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A
large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308,
2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019.
Devin Kreuzer, DominiqUe Beaini, Will Hamilton, Vincent LCtourneau, and Prudencio Tossou.
Rethinking graph transformers with spectral attention. Advances in Neural Information Processing
Systems, 34, 2021.
Ron Levie, Wei Huang, Lorenzo Bucci, Michael Bronstein, and Gitta Kutyniok. Transferability of
spectral graph convolutional neural networks. Journal of Machine Learning Research, 22(272):
1-59, 2021.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances
in Neural Information Processing Systems, 27:2177-2185, 2014.
Pan Li and Jure Leskovec. The expressive power of graph neural networks. In Lingfei Wu, Peng Cui,
Jian Pei, and Liang Zhao (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications,
chapter 5, pp. 63-98. Springer, Singapore, 2021.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably
more powerful neural networks for graph representation learning. Advances in Neural Information
Processing Systems, 2020.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal of
the American society for information science and technology, 58(7):1019-1031, 2007.
Yunyu Liu, Jianzhu Ma, and Pan Li. Neural higher-order pattern prediction in temporal networks. In
The International Conference of World Wide Web, 2022.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems, pp. 2153-2164, 2019a.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2019b.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems, pp. 3111-3119, 2013.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602-4609, 2019.
Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for
graph representations. In International Conference on Machine Learning, pp. 4663-4673. PMLR,
2019.
Axel Nilsson and Xavier Bresson. An experimental study of the transferability of spectral graph
networks. arXiv preprint arXiv:2012.10258, 2020.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 701-710, 2014.
11
Published as a conference paper at ICLR 2022
Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as
matrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the Eleventh
ACM International Conference on Web Search and Data Mining, pp. 459-467, 2018.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021.
Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability
of graph neural networks. Advances in Neural Information Processing Systems, 33, 2020.
Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Graph neural networks: Architectures, stability,
and transferability. Proceedings of the IEEE, 109(5):660-682, 2021.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. In Proceedings of the 2021 SIAM International Conference on Data Mining, pp. 333-341.
SIAM, 2021.
VIcctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
networks. In International Conference on Machine Learning, pp. 9323-9332. PMLR, 2021.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node
embeddings and structural graph representations. In International Conference on Learning Repre-
sentations, 2020.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In The International Conference of World Wide Web, pp.
1067-1077, 2015.
Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning.
In International Conference on Machine Learning, pp. 9448-9457. PMLR, 2020.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and YoshUa
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Hongwei Wang, Hongyu Ren, and Jure Leskovec. Relational message passing for knowledge graph
completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, pp. 1697-1707, 2021a.
Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation
learning in temporal networks via causal anonymous walks. In International Conference on
Learning Representations, 2021b.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International Conference on Machine Learning, pp. 40-48. PMLR, 2016.
Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International
Conference on Machine Learning, pp. 7134-7143. PMLR, 2019.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis-kahan theorem for
statisticians. Biometrika, 102(2):315-323, 2015.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in Neural
Information Processing Systems, 31:5165-5175, 2018.
12
Published as a conference paper at ICLR 2022
Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Revisiting graph neural networks for
link prediction. Advances in Neural Information Processing Systems, 2020.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. Bioinformatics, 33(14):i190-i198, 2017.
A Proof of Lemma 3.4
Recall the eigenvalues of the PSD matrix B are 0 = λ1 < λ2 < ... < λp < λp+1 ≤ λp+2 ≤ ... ≤
λN. Suppose one EVD of B = UΛUT. In U = [u1, u2, ..., uN], ui is the eigenvector of λi so
Bui = λiui. Without loss of generality, we set PE(B) = [u1, u2, ..., up].
Suppose k = argminι≤i≤p ∣λi+ι - λ/. Now, we perturb B by slightly perturbing Uk and uk+ι. We
set
u0k =	1 - 2uk + uk+1
u0k+1 = -uk +	1 - 2uk+1
Set u0i = ui for i ∈ [N], i 6= k, k + 1. Note that ku0k k = ku0k+1 k = 1 and u0kT u0k+1 = 0. Then, the
columns of U0 = [u01, u02, ..., u0N] still give a group of orthogonal bases.
Now we denote the above perturbation of B as B + 4B = PiN=1 λiu0iu0iT . Then, PE(B + 4B)
could be [u01, u02, ..., u0p]S0 for any S0 ∈ SN(p). Therefore, for sufficiently small > 0,
S0∈mSiNn(p) kPE(B + 4B)S0 - PE(B)kF2
=k [(，] - e2 - I)Uk + euk+ι, (∖∕l-e2 - I)Uk+1 - euk ]kF
=k(P1 - e2 - I)Uk + euk+ιk2 + k(P1 - e2 - I)Uk+1 - euk∣∣2
=4(1 - Pl - e2)
=2e2 + o(e2).	(8)
Next, we characterize ∣4B ∣F.
NN
∣4B∣2F=∣B+4B-B∣2F=∣ XλiU0iU0iT-XλiUiUiT ∣F2
i=1	i=1
=∣λk (Uk Uk - UkUk ) + λk+1 (Uk+1Uk+1 - Uk+1Uk+1)∣F
=k (λk+1 - λk ) [-e2 (UkUT - Uk+1UT+1) + eP1 - e2 (UkUT+i + Uk+1UT)] IIF
=(λk+1 - λk)2(e2∣UkUkT+1 +Uk+1UkT∣F2 + o(e2))
=2(λk+1 - λk)2 (e2 + o(e2)).	(9)
Combining Eqs. 8,9, we have, for sufficiently small e > 0,
min IlPE(B + 4B)S0 - PE(B)IlF > 0∙99∣λk+ι - λk∣-1∣∣4B∣∣f + o(e),
S0 ∈SN(p)
which concludes the proof.
B Proof of Lemma 3.5
The result of the Lemma 3.5 can be derived from the Davis-Kahan theorem (Davis & Kahan, 1970)
and its variant (Yu et al., 2015) that characterizes the eigenspace perturbation. We apply the Theorem
2 Eq.3 of (Yu et al., 2015) to two PSD matrices.
Theorem B.1 (Theorem 2 (Yu et al., 2015)). Let two PSD matrices B(1), B(2) ∈ RN×N, with
eigenvalues 0 = λ(1i) ≤ λ(2i) ≤ ∙∙∙ ≤ λ(pi) ≤ ∙∙∙ ≤ λ(pN) such that λ(p1+)1 - λ(p1) > 0. For
13
Published as a conference paper at ICLR 2022
i = 1, 2, let U(i) = (u(1i), u(2i), ..., u(pi)) have orthonormal columns satisfying B(i)u(ki) = λku(ki) for
k = 1, 2, ..., p. Then, there exists an orthogonal matrix Q ∈ SO(p) such that
kU⑵Q - U⑴kF ≤ 23/2 mm(P1/2kB⑴-B⑵对田⑴-B⑵忖
λ(p1+)1 - λ(p1)
By symmetry, use the above theorem again and we know there exists Q0 ∈ SO(p)
kU ⑴ Q0 - U ⑵ kF ≤ 23/2 min(P”"B (I - B?,"B(I)- B ⑵kF)
λ(p2+)1 - λ(p2)
Because kU(1)Q0 - U(2) kF = kU(2)Q0T - U(1) kF, then there exists Q ∈ SO(P),
IlU⑵Q — U(I)IlF ≤ 23/26min(p1/2IIB⑴—B(2)||0p, ∣∣B⑴—B(2)||f)	(10)
where δ = min{(λ(p1+)1 - λ(p1))-1, (λ(p2+)1 - λ(p2))-1}.
When we apply a permutation matrix P ∈ Π to permute the rows and columns of B(2), then
(PB(2)PT)(Pu(k2)) = PB(2)u(k2) = λ(k2)Pu(k2) for any k. Moreover, permuting the rows and
columns of a PSD matrix will not change its eigenvalues. This means that B (2) can be replaced by
PB(2)PT in Eq.10 as long as U(2) is replaced by PU(2). Therefore,
IlPU⑵Q — U(I)IlF ≤ 23/26min(p1/2 IlB⑴一PB⑵PT|后，∣B⑴—PB⑵PT∣∣f)
C Proof of Theorem 3.6
To prove PE-equivariance, consider two graphs G(1) = (A(1), X(1)) and G(2) = (A(2), X(2)) that
have perfect matching P*. So, L(I) = P*L(2)P*T, X⑴=P*X(2).
Let Z(i) denote the Laplacian eigenmaps of L(i), i = 1, 2. Set B(i) = L(i), i = 1,2 and P = P *
and use Lemma 3.5. Because L(I) = P*L(2)P*T and λp = λp+ι, then there exists Q ∈ SO(p).
Z(1) = P*Z(2)Q	(11)
Now, We consider a GNN layer g that satisfies Eqs. 4,5. Also denote the output as (X⑴，Z(I))=
g(A(1),X⑴,Z(I)) and (X⑵,Z⑵)=g(A(2),X⑵,Z⑵).
(X ⑴,Z(I))= g(A(1),X ⑴,Z(I))
(=a) g(P*A(2)P*T, P*TX(2), P*Z(2)Q)
(=b) P*g(A(2),X(2),Z(2)Q)
=) (P*X⑵,P*Z(2)Q)
Here (a) is because the perfect matching between G(1) and G(2), and Eq. 11. (b) is due to Eq. 4 and
(c) is due to Eq. 5.
Therefore, X⑴=P*X⑵ and n(Z(1), Z(2)) = 0, which implies that g satisfies PE-equivariance.
D	Proof of Theorem 3.7
To prove PE-stability, consider two graphs G(1) = (A(1), X(1)) and G(2) = (A(2), X(2)).
Let P * denote their matching. We study the PEG layer in Eq. 6 and denote (X (i),Z(i)) =
gPEG(A(i),X(i),Z(i))fori= 1,2.
14
Published as a conference paper at ICLR 2022
Let Us first bound the easier term regarding the positional features η(Z(1),P*Z(2)). Because Z(i)=
Z⑶,η(Z(1),P*Z(2)) = η(Z(1),P*Z⑵),while the bound of the later is given by Lemma 3.5. Set
B(i) = L⑻,i = 1,2 and P = P*. Then, there exists a Q ∈ SO(p),
kP*Z⑵Q - Z⑴kF ≤ 23/26min(p1/2IlL⑴-P*L(2)P*t临，kL(I)- P"⑵P*t∣∣f).	(12)
Next, WeboUndtheharderPartkX⑴—P*X(2)||f. First,
kX⑴-P*X⑵kF = kΨ [(A⑴ © 己⑴)X⑴Wi - P*ψ [(A⑵ Θ Ξ⑵)X⑵Wi ∣∣f∙	(13)
Here, without loss of generality, we set Ξ(UV = φ( k ZUi) — Zvi) k) for u,v ∈ [N] such that Auv) = 0
and otherwise 0. Moreover, ΞUv is bounded because ΞUv = φ(kZ(i) 一 Zvi) k) ≤ 'φkZ(ui) 一 Zvi) k ≤
'φ(kZUi) k + ∣∣Zvi) k) ≤ 2'φ because of the 'φ-Lipschitz continuity of φ and ∣∣Z(i) k ≤ 1.
Compute the difference
kψ [(A⑴ Θ Ξ⑴)Xa)Wi - P*ψ [(A⑵ Θ Ξ⑵)X⑵ Wi ∣∣f
≤) 'ψ k (Aa) Θ Ξ(1)) X⑴ W - P*(A(2) Θ Ξ(2)) X⑵ WIIf
≤ 'ψ ∣∣W kopk (Aa) Θ Ξ(1)) X ⑴-P * (A ⑵ Θ Ξ⑵)X ⑵ If
=)'ψ∣∣Wkopk (A⑴ Θ Ξ⑴-P* (A⑵ Θ Ξ⑵)P*t) X⑴ + P*(A⑵ Θ Ξ⑵)(P*TX⑴-X⑵)∣f
≤ 'ψkWkop [kA⑴ Θ Ξ(1) - P* (A⑵ Θ Ξ(2)) P*T∣∣f∣X⑴ ∣op+	(14)
kP*T (A⑵ Θ Ξ(2)) kopkP*TX⑴-X⑵If]
≤ 'ψ kWkop [kA⑴ Θ Ξ(1) - P* (A⑵ Θ Ξ(2)) P*TkF∣X⑴ kop + kA⑵ Θ Ξ⑵ ∣∣op∣∣X⑴-P*X⑵ ∣f]
(15)
Here, (a) is due to the 'ψ -Lipschitz continuity of the component-wise function ψ. (b) and (d) use that
for any two matrices A, B, kABkF ≤ min{kAkFkBkop, kAkopkBkF} and (d) also uses the triangle
inequality of k ∙ kF. (e) uses ∣∣PA∣∣op = k Akop and ∣∣PA∣IF = k AkF for any P ∈ Π.
Next, we only need to bound ∣∣A⑴ © Ξ(1) 一 P* (A(2) © Ξ(2)) P*TkF and kA⑵ © Ξ⑵ kop.
∣a(I) © Ξ(I)- P* (A⑵ © Ξ⑵)P*TkF
(=) kA(I) © Ξ(1) -(P*A(2)P*t) © (p*Ξ(2)P*T) kF
≤ kA(I) © (Ξ(1) - P*Ξ(2)p*t) kF + k (A(I)- p*,4(2)P*t) © (p*Ξ(2)p*t) kF
≤) maχ ∣AUV)∣kΞ⑴-P*Ξ(2)P*TkF + maχ Ξ2)∣kA⑴-P*A⑵P*TkF
U,v∈[N]	U,v∈[N]
(c)
≤ ∣∣Ξ⑴-P*Ξ(2)P*TkF + 'φ∣∣L⑴一P*L(2)P*TkF
(16)
where (a) is because P(A © B)PT = (PAPT) © (PBPT) for any P ∈ Π . (b) is because for
any two matrices A, B, ∣∣A © BkF ≤ maxu,v ∣Auv∣kB∣∣F. (c) is because maxu,v∈[N] ∣AUV)∣ ≤
1, maxu,v∈[N] ∣ξUv) | ≤ 'φ.
kA⑵ © Ξ(2)kop (≤) kA⑵kopkΞ⑵kop = ρ(A(2))ρ(Ξ⑵)(≤) 2 ∙ dmaχ'φ = 2%«	(17)
where (a) is because for any two matrices A, B, kA© Bkop ≤ kAkopkBkop. (b) is because graphs are
undirected and both A(2) and Ξ(2) are symmetric matrices. Hence, A(2) and Ξ(2) can be diagonalized.
For diagonalizable matrices, their operator norms equal their spectral radius ρ. (c) is because the
15
Published as a conference paper at ICLR 2022
following facts: It is known that a degree normalized adjacency matrix A(2) has eigenvalues between
-1 and 1; And, ρ(Ξ⑵)≤ maXv∈[N] PN=ι 旧百| ≤ 2maXv∈[N] dV2)'φ = 2您乂'@. Here, We use
that Ξ(v2u) is not zero iff vu is an edge in the graph.
Lastly, We need to bound ∣∣Ξ⑴-P*Ξ(2)P*t∣∣f in Eq.16. Let ∏ : [N] → [N] denote the permutation
mapping defined by P*, i.e., Puv = 1 when V = ∏(u) and Puv = 0 otherwise. Pick the Q ∈ SO(P)
that matches the two groups of positional features Z(1) , Z(2).
kΞ(1) - P*Ξ⑵P*TIlF
≤) SX 'φ(kZU1)- ZvI) k-kZ∏2V)- ZΠ2V)k)2
u,v
=SX'φ (kzU1) - zV1)k-kzU1) - Z∏2v)Qk + kzU1) - z∏2V)Qk-kz∏2U)Q - z∏2v)Qk)2
u,v
≤ 'φ∖∣2 X (kzU1) - zV1)k-kzU1) - z∏2V)Qk)2 + (kzU1) - z∏2V)Qk-kz∏2U)Q - z∏2V)Qk)2
u,v
≤) %, X (kzVI)- z∏2V)Qk2 + kzUI)- Z∏2U) Qk2)
u,v
= 2'φkZ⑴-P*Z⑵QkF
=2'φη(z ⑴,p*z ⑵)
(18)
where (a) is due to the 'φ-Lipschitz continuity of the component-wise function φ and (b) is because
of triangle inequalities.
Plugging Eq.18 into Eq.16 and plugging Eqs.16,17 into Eq.15, further using Eqs.13,12, we achieve
HX⑴-P*χ⑵HF
≤'ψ'φ∣∣WHop h(25/2S + 1)IIX⑴IIoPIIL⑴-P*L⑵P*tHf + 2Mm?乂`o恒⑴-P*X⑵IlFi
≤(7δ∣∣X⑴IloP + 2dmaχ)'ψ'φ∣WIoPd(G⑴,G⑵).	(19)
Combining Eq.19 with the bound on positional features in Eq.12, we conclude the proof by
HX⑴-P*χ⑵HF + η(Z⑴,P*Z⑵)
≤ [(7δ∣∣X⑴ Ilop + 2dm"'φI WIop + 3δ]d(G⑴,G⑵).	(20)
E Proof of Theorem 3.8
To prove PE-equivariance, consider two graphs G(1) = (A(1), X(1)) and G(2) = (A(2), X(2)) that
have perfect matching P*, L(1) = P*L(2)P*T, X(1) = P*X(2).
Let Z(i) denote the positional features obtained by decomposing the optimal solution M (i)* to
the optimization problem Eq.7. Because L(1) = P*L(2)P*T, we have A(1) = P*A(2)P*T and
D(1) = P *D(2)P *T. Then,
M (1)* = arg min tr(f+ (A(1))g (M) + f-(D(1))g(-M))
M :rank(M )≤p
(=a)	arg min tr(P*f+(A(2))P*Tg(M) + P *f-(D(2))P *T g(-M))
M :rank(M )≤p
(=b) arg min tr(f+(A(2))P*Tg(M)P* + f-(D(2))P *T g(-M)P *)
M :rank(M )≤p
(=c) arg min tr(f+(A(2))g(P *T M P *) + f-(D(2))g(-P*TMP*))
M :rank(M )≤p
(=d) P* M (2)* P*T
16
Published as a conference paper at ICLR 2022
Here (a) is because A(I) = P*A(2)P*T and the assumptions on f+ and f-. (b) is because for
two squared matrices A, B, tr(AB) = tr(BA). (c) is because g is component-wise function. (d) is
because M*(2) is the unique solution of arg min”^位(M)≤p tr(f+(A(2))g(M) + f-(D(2))g(-M)).
Recall M(I)* = Z0(I)Z(I)T, Z(I)TZ(I) = I and M(2)* = Z0(2)Z(Z)T, Z(Z)TZ(2) = I. Note that
Z(I),Z(2) that satisfy such decompositions are not unique. As rank(M(I)*) = rank(M(2)*) = p, so
Z(1),Z0(1) and P*Z(Z), P*Z0(Z) have full-rank columns.
Since M (1)* = P*M(Z)*P*T, Z0(1)Z(1)T = P*Z0(Z)Z(Z)TP*T. Because Z0(1) has full-rank
columns, Z0(1)TZ0(1) is non-singular. Let Q = Z0(Z)TP*TZ0(1)(Z0T(1)Z0(1))-1. Then, Z(1) =
P* Z(Z) Q. Since, Z(1)TZ(1) = Z(Z)TZ(Z) = I, we have QTQ = I. Q is a squared matrix so
Q ∈ SO(p). That means Z(1) = P*Z(Z)Q for some Q ∈ SO(p).
Now, We consider a GNN layer g that satisfies Eqs. 4,5. Also denote the output as (X⑴，Z(I))=
g(A(1),X(1), Z(I)) and (X(2), Z(2)) = g(A(2),X(2),Z(2)).
(X ⑴,Z(I))= g(A(I),X(I),Z (I))
(=a) g(P*A(Z)P*T, P*TX(Z), P*Z(Z)Q)
(=b) P*g(A(Z),X(Z),Z(Z)Q)
=) (P*X⑵,P*Z(2)Q)
Here (a) is because the perfect matching between G(1) and G(Z), and Z(1) = P*Z(Z)Q. (b) is due to
Eq. 4 and (c) is due to Eq. 5.
Therefore, X(I) = P*X⑵ and n(Z(1), Z(2)) = 0, which implies that g satisfies PE-equivariance.
F Review of the E-GNN layer
Satorras et al. (2021) studies the problem when the nodes of a graph have physical coordinates as
features and proposes E-GNN to deal with this kind of graph data. E-GNN aims to keep permutation
equivariant with respect to the node order, and translation equivariant, rotation equivariant with
respect to the physical coordinate features. As E-GNN asks even more (translation equivariance) than
Eqs. 4,5, E-GNN can be adopted to leverage PE techniques to keep PE-equivariance. The specific
form of E-GNN is as follows.
Given a graph G = (V , E ) with nodes vi ∈ V and edges eij ∈ E . E-GNN layer takes the node
embeddings hl = {hl0, ..., hlM-1}, coordinate embeddings xl = {xl0, ..., xlM-1} and edge information
E = (eij) as input and outputs hl+1 and xl+1, respectively. Thus, the E-GNN layer can be denoted
as: hl+1, xl+1 = EGCL(hl,xl, E). The layer is defined as following:
mij = φe(hli,hlj, ||xli -xlj||Z,aij)
xi(l+1) =xli + X(xli -xlj)φx(mij)
j6=i
mi =	(mij)
j∈Ni
hi(l+1) = φh(hi(l), mi)
Where aij is the edge attributes, φe represents edge operation, φx represents edge embedding
operation and φh represents node operation.
17
Published as a conference paper at ICLR 2022
G	The optimization form for general PE techniques
Given an undirected and weighted network G = (V, E, A) with N nodes, LINE (Tang et al., 2015)
with the second order proximity (aka LINE (2nd)) aims to extract two latent representation matrices
Z, Z0 ∈ RN×p. Let Zi, Zi0 denote the ith row of Z and the ith row of Z0, respectively. The objective
function of LINE (2nd) follows
NN	N
mZ	XXAijg(ZiZjT) + bXEj0〜PVIgl-ZiZj))
where g(x) is the log sigmoid function g(x) = x - log(1 + exp(x)) and b is a positive constant. The
first term corresponds to the positive examples, i.e., links in the graph while the second term is based
on network negative sampling. Also, PV is some distribution defined over the node set. LINE adopts
3
PV (j) a dj4 where dj is the degree of node j. By filling the expectation and using matrix form to
rewrite the objective, we have
NN	N
XXAijg(ZiZj0T)+bXEj0〜PV (g(-ZiZjT))
NN	N
=XXAij g(ZiZjr ) + C ^X d 4 g(-ZiZjT )
i=1 j=1	i=1
=tr(Ar g(ZZ 0)) + tr(cD 411r g(ZZ0))
=tr(Ag(Z 0Z)) + c11r D 4 g(Z0Z))
where C = - b 3/4. So for LINE, f+(A) = A and f-(D) = c11rD3. It is easy to validate
jN=1 dj3/
that for all P ∈ Π, f+(PAP T) = Pf+(A)P T and f-(P DP T) = P f- (D)P T, which satisfies the
condition in Theorem 3.8.
Deepwalk (Perozzi et al., 2014) can be rewritten by following the similar idea. Deepwalk firstly
performs random walks with certain length for many times starting from each node and then treat
each walk as a sequence of node-id strings. Deepwalk trains a skip-gram model on these node-id
strings (Mikolov et al., 2013). Now we consider the skip-gram model with negative sampling (SGNS).
Denote the collection of observed words and their context pairs as D. #(w, C) denotes the number
of times that the pair (w, C) appears in D. #(w) and #(C) indicates the number of times w and C
occurred in D. Let Zw denote the vector representation of w and Zc0 denote the vector representation
of C. According to (Levy & Goldberg, 2014), the objective function of SGNS follows
XX
#(w,c)g(Zw Zc) + b X #(w)Ec0 〜Pc (g(-ZwZC0r))
Deepwalk adopts this objective by viewing each node v in V as a word and viewing any node that gets
sampled simultaneously with v within T hops of the random walk as the context C. Let Φ = D-1A
denote the random walk matrix of the graph. In this case, for a node v as the word and for another
node u as the context, the expected number #(v, u) a Φ0vu = PkT=1(dv(Φk)vu + du (Φk)uv). The
expected number #(v) a dv which uses the stationary distribution of random walk over a connected
graph is proportional to the node degrees. We also set the negative context sampling probability Pc is
proportional to the node degrees, i,e., Pc(u) a du. Then, in Deepwalk, the SGNS objective reduces
to
NN	NN
XXΦ0vug(ZvZu0)+CXXdvdug(-ZvZu0T)
v=1 u=1	v=1 u=1
for some positive constant C. Similar to the derivation for LINE, we can rewrite it into the matrix
form
tr(Φ0g(Z0ZT) + D11T Dg(-Z0ZT)).
18
Published as a conference paper at ICLR 2022
Table 4: Summary of Twitch dataset
	DE	EN	ES	FR	PT	RU
Nodes	9,498	7,126	4,648	6,549	1,912	4,385
Edges	153,138	35,324	59,382	112,666	31,299	37,304
Features	3,170	3,170	3,170	3,170	3,170	3,170
Therefore, for Deepwalk, f+(A) = Φ0 = PkT=1 (DΦk + ΦTkD) and f- (D) = cD11T D, where
Φ = D-1A and c is a positive constant. It is also easy to verify that for all P ∈ Π, f+(PAPT) =
Pf+(A)PT and f-(PDPT) = Pf-(D)PT, which satisfies the conditions in Theorem 3.8.
As for Node2vec (Grover & Leskovec, 2016), it performs a 2nd-order random walk to collect node-id
strings and then train an SGNS model. The optimization form is more involved, interested readers
could check (Qiu et al., 2018; Grover & Leskovec, 2016) for more details.
H Supplement for experiments
We put more specifics of datasets and baselines adopted in Sec. H.1 and Sec. H.2, respectively. We
describe how we tune our model in Sec. H.3. We list further experimental results in Sec. H.4. We
further conduct three supplementary experiments t demonstrate the generalization capability, stability
and wide applicability of our theory and proposed PEG layer in Secs. H.6-H.8.
H. 1 Datasets
The citation networks-Cora, Citeseer and Pubmed are collected by Sen et al. (2008), where nodes
represent documents and edges (undirected) represent citations. Node features are the bag-of-words
representation of documents. The Cora dataset contains 2708 nodes, 5429 edges and 1433 features
per node. The Citeseer dataset contains 3327 nodes, 4732 edges and 3703 features per node. The
Pubmed dataset contains 19717 nodes, 44338 edges and 500 features per node.
Twitch is obtained from Rozemberczki et al. (2021). Twitch is a user-user networks of gamers, where
nodes correspond to users and edges correspond to mutual friendship between them. Node features
correspond to the games they played and liked, users’ location and streaming habits. Twitch contains
7 user networks over different countries, including Germany (DE), England (EN), Spain (ES), France
(FR), Portugal (PT) and Russia (RU). The details of these datasets are shown in Table 4.
Chameleon is used in Chien et al. (2021). Chameleon is a page-page network on topic ‘Chameleon’
in Wikipedia (December 2018), where nodes correspond to articles and edges represent mutual links
between the articles. Node features indicate the presence of several informative nouns in the articles
and the average monthly traffic (October 2017 - November 2018). Chameleon dataset contains 2277
nodes, 36101 edges and 2325 features per node.
Protein-protein interaction (PPI) dataset contains 24 graphs corresponding to different human tissues
(Zitnik & Leskovec, 2017). We adopt the preprocessed data provided by Hamilton et al. (2017) to
construct graphs. The average number of nodes per graph is 2372 and each node has 50 features,
which correspond to positional gene sets, motif gene sets and immunological signatures.
Ogbl-ddi and ogbl-collab are chose from the open graph benchmark (OGB) (Hu et al., 2021), which
adopt more realistic train/validation/test splitting, such as by time (ogbl-collab) and by by drug target
in the body (ogbl-ddi). ogbl-ddi is a drug-drug interaction network, where nodes represent drugs and
edges represent interaction between drugs, where the joint effect of taking the two drugs together
is considerably different from the effect that taking either of them independently. Ogbl-collab is
an author collaboration graph, where nodes represent authors and edges represent the collaboration
between authors. The 128 dimension node features of ogbl-collab is extracted by by averaging the
word embeddings of the authors’ papers. Ogbl-ddi has 4267 nodes and 1.3M edges. Obdl-collab has
0.23M nodes and 1.3M edges.
19
Published as a conference paper at ICLR 2022
Figure 3: Eigenvalues λp and the stability ratio ρp =
minι≤λ≤-⅛+⅛+π.PEGWith SenSitiVity %-4+1厂1
is far more stable than previous methods with sensitivity maxι≤k≤p ∣λk — λk+ι |-1. Over Cora, Pp is extremely
large becauSe there are multiple eigenValueS (ρp iS Still finite due to the numerical approximation of the
eigenvalues). Over Pubmed, Twitch(RU) and Chameleon even if there are no multiple eigenvalues, ρp is mostly
larger than 5.
H.2 Baseline details
We have 4 baselines based on GNNs and 2 baselines based on network embedding techniques, namely,
LE (Belkin & Niyogi, 2003) and Deepwalk (DW) (Perozzi et al., 2014). We will first introduce the
implementation of LE and DW, then discuss other baselines.
Both LE and DW embed the networks in R128 in an unsupervised way. For LE, we factorize of the
graph LaPIaCian matrix: ∆ = I - D- 1 AD-2 = UAUT, where A is the adjacency matrix, D is the
degree matrix, and Λ, U correspond respectively to the eigenvalues and eigenvectors. We use the
128 smallest eigenvectors as LE. For DW, we use the code provide by OpenNE2. Both methods use
the inner product between pairwise node representations as the link representations. Then, the link
representations are fed to an MLP for final predictions.
Regarding to GNN-based baselines, VGAE is implemented according to the code3 (Kipf & Welling,
2016) given by the original paper, with 2 message passing layers with 32 hidden dimensions. P-GNN
is implemented by adopting the code4 provided by the original paper (You et al., 2019), with 2
message passing layers with 32 dimensions, with a tuned dropout ratio in {0, 0.5}. GNN transformer
layer is implemented by adopting the code5 provided by the original paper (Dwivedi & Bresson,
2020) with 2 message passing layers with 128 hidden dimensions. SEAL is implemented by adapting
the code6 provided by the original paper. Notice the ogbl-ddi graph contains no node features, so the
we use free-parameter node embeddings as the input node features and train them together with the
GNN parameters. We slightly tune the hidden dimensions and layers of these baselines and present
the best results.
Moreover, we aim to understand whether PEG is sensitive to the dimension of positional features.
We conduct experiments on Cora, Pubmed and Twitch(RU) to test the sensitivity of the dimension
positional features. In Fig. 4, we compare the performance of PEG-DW, PEG-LE, PEG-DW+ and
PEG-LE+ with PE in different dimensions. All the experiment settings follow the setting to get
Table 1 except for the dimension of positional features.
H.3 Hyperparameters Tuning for PEG
Table 5 lists the most important hyperparameters, which applies to our proposed model PEG. Grid
research is used to find the best hyperparameter combination. Note that our model actually got very
slightly tuned. We believe more extensively hyperparameter tuning may yield even better results.
2https://github.com/thunlp/OpenNE/tree/pytorch
3https://github.com/tkipf/gae
4https://github.com/JiaxuanYou/P-GNN
5https://github.com/graphdeeplearning/graphtransformer
6https://github.com/muhanzhang/SEAL
20
Published as a conference paper at ICLR 2022
Figure 4: The performance (AUC) of PEG with different dimensions of positional encodings for traditional link
prediction (Task 1).
The models are trained until the loss converges and we report the best model by running each for 10
times. We provide our code for reproducing the experimental results to the reviewers and area chairs
in the discussion forums.
Table 5: List of hyperparameters and their value/range
Hyperparameters	Value/Range
batch size	64,128,64×1024(ogb)
learning rate	1e-2, 1e-3
optimizer	Adam
conv. layers	2
conv, hidden dim	128
PE. dim	128	
H.4 Further analysis
As a supplement to the Lemma 3.4 and Lemma 3.5, we compute the ratio between the inverse
eigengap between thepth and (P + 1)th eigenvalues {∣λp - λp+1∣-1} and maxι≤k≤p ∣λk - λk+ι∣-1
over more graphs as shown in Fig. 3.
Fig. 4 shows that different PE techniques have different dimensional sensitivity. In general, DW
seems to be more stable than LE over the three datasets and PEG-DW+ achieves stable performance.
When the dimension increases, PEG are more likely to achieve higher performance, but will be more
time consuming.
H.5 Edge weight visualization
In the PEG layer, we calculate an edge weight according to the distance between the representations
of the end nodes of the edge. To further understand the learnt relationship between edge weights and
the distance between node representations, we visualize the edge weight transformation curves. We
run the experiment traditional link prediction (task 1) and use DW as the node positional embedding,
i.e., the model PEG-DW. For each dataset, we draw the edge weight transformation curves in Fig. 5.
Note that each curve ranges in x-axis from the smallest distance to the largest distance observed from
the corresponding graph, we also randomly select 500 distances from each graph and scatter them on
the curve as shown in Fig. 5. Fig. 5 shows that the edge weight increases monotonically with respect
to the input distance, but the relationship between them is non-linear. Most of the weights are close
to 1 but a few weights are much smaller.
H.6 Link prediction over Random graphs
To further demonstrate the generalization of our model, we conduct inductive link prediction over
random graph models. Specifically, we use stochastic block models (SBM) with two blocks Holland
et al. (1983) to generate random graphs. Each block contains 500 nodes and the probability to form a
link between two nodes within each block is 0.3, while the probability to form a link between two
nodes across different blocks is 0.1. Here, we randomly select links inside the blocks as positive
21
Published as a conference paper at ICLR 2022
Figure 5: The edge weight transformation curve of PEG for each dataset. 500 selected edges are
represented as red points.
the number of graphs for training
Figure 6: The comparison of AUC-ROC between PEG and GAE baselines for link prediction on random
graphs.
samples, and randomly select the missing links (unconnected node pairs) from the generated graph
as negative samples. The model is trained by using various numbers of graphs (e.g. 1, 10, 30, 50),
validated and tested on 10 random graphs respectively. For each graph that is used for training
or testing, we utilize 10% positive links and pair them with the same number of negative missing
links for training or testing respectively. The rest settings are remained the same as in Task 2. We
choose three variants of GAE that use constant features (node degree), random features and positional
features as baselines. All the models are trained until they converge and the models with the best
validation performance are used to report the results (averaged by 10 independent runs), which are
shown in Fig. 6.
For various sizes of input graphs, PEG achieves AUC around 0.98 by just using one training graph and
slightly improves AUC by using more training graphs, which consistently outperforms all three GAE
variants that even use the entire 50 training graphs with large margins. This directly demonstrates the
stability and better generalization of PEG models.
H.7 The study of PE stability by perturbing the input graphs
To further investigate PE stability, We consider two versions of models utilizing PE - one is PEG,
which is PE equivariant and stable, and the others are GAE/VGAE (N./C. + P.), which fail to satisfy
the two equivariant properties and thus may not be stable. These two versions of models are evaluated
on the Cora dataset. All the models are trained over the original graph. Then, we perturb the input
graph by randomly adding or dropping a certain percentage of links during the inference. The rest of
the settings remain the same for link prediction. Specifically, after training the model, we inject a
perturbation on top of the input graph by adding extra links (negative samples) or dropping positive
links with the ratio of 10%, 20% and 30%, respectively. We recompute the positional encodings to
reflect the structural perturbation of the given graph and plug in the perturbed positional encodings
into different models. Other experiment settings remain the same as in Task 1. The results are
summarized and plotted in Fig. 7.
22
Published as a conference paper at ICLR 2022
→- PEG-DW+(C. + P.)
r- PEG-LE+(C. + P.)
→- PEG-DW {C. + R)
*- PEG-LE (C. + R)
-∙- VGAE(C. + LE)
♦ GAE(C. + LE)
Dropping edge (Node feature)
T- PEG-DW+(C. + P.)
-→- VGAE(C. + LE)
→- PEG-DW+(N. + P.)
→- PEG-LE+(N. + M
10
adding edge percent
Figure 7: The AUC-ROC of PEG and GAE/VGAE for link prediction in terms of PE stability (w.
node or constant feature and w. adding or dropping edges). To perturb the input graphs, we can
randomly add or delete some edges with the percentage shown in the X-axis before the inference.
-→- VGAE(N. + LE)
• - GAE(N. + LE)
By comparing the models with different levels of input perturbation, we can observe from Fig. 7 that
PEG achieves consistently and significantly better performance with either node features or constant
features in all four scenarios, which is far more robust than GAE/VGAE-based models that adopt
PE under the same level of perturbations. Compared with adding edges, removing edges is more
destructive for small-size graphs such as Cora, and thereby does more harm to the model performance.
Particularly, all the models using C. + P. achieve subpar performance when considerable number of
edges are removed. However, PEG can still show strong resilience against this type of perturbation
(especially with node features), while GAE/VGAE generally collapses. These results further support
that the performance boost indeed benefits from the equivariance and stability with PEs instead of
simply using PE variants.
H.8 Preliminary results for node classification based on PE
In order to evaluate PEG on a wider category of tasks, we consider node classification on citation
networks - Cora, Citeseer and Pubmed (Sen et al., 2008) and closely follow the transdUctiVe
experimental setup of Yang et al. (2016). Suppose the final readout of PEG is denoted as (X, Z). To
classify the node u, We only utilize its node representation Xu to make the final predictions. The
rest parts and the hyperparameters of the model are kept the same as the one that is good for our task
1 (traditional link prediction). We did not specifically tune the model for node classification. The
results of eValuation on node-leVel are summarized in Table 6. We report the mean accuracy with
standard deViation of classification on the test set of our method after 10 runs.
Both PEG-LE and PEG-DW significantly outperform GCN and proVide comparable results against
GAT, which illustrates the effectiVeness of the proposed PEG on node leVel tasks. Recall that we
almost did not tune our model to achieVe such results. EVen better results may be expected by
further finer tuning the model on each dataset. MoreoVer, GAT uses multi-head attention to aggregate
information from neighbours where the attention weights are based on node features, while PEG only
adopts GCN layers with some edge weights deriVed from positional encodings that merely depend on
the network structures. PEG and GAT essentially utilize orthogonal information source to re-weight
23
Published as a conference paper at ICLR 2022
Table 6: Summary of results in terms of node classification accuracy (mean ± std%), for Cora,
Citeseer and Pubmed.
Method	Cora	Citeseer	PubMed
PEG-DW	82.24±0.02	71.22±0.01	79.83 ± 0.02
PEG-LE	82.16±0.02	71.93±0.01	79.85 ±0.01
GCN	81.50±0.05	70.38 ±0.05	79.03 ± 0.03
GAT	83.03 ±0.07	72.52±0.07	79.06 ± 0.03
the edges. Hence, PEG should be easily further combined with the attention mechanism based on
node features to get even better predictions, which this is out of the scope of this work, so we leave it
for future study.
24