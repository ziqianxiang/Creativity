Under review as a conference paper at ICLR 2022
The Three Stages of Learning Dynamics in
High-dimensional Kernel Methods
Anonymous authors
Paper under double-blind review
Ab stract
To understand how deep learning works, it is crucial to understand the training
dynamics of neural networks. Several interesting hypotheses about these dynam-
ics have been made based on empirically observed phenomena, but there exists a
limited theoretical understanding of when and why such phenomena occur.
In this paper, we consider the training dynamics of gradient flow on kernel least-
squares objectives, which is a limiting dynamics of SGD trained neural networks.
Using precise high-dimensional asymptotics, we characterize the dynamics of the
fitted model in two “worlds”: in the Oracle World the model is trained on the pop-
ulation distribution and in the Empirical World the model is trained on a sampled
dataset. We show that under mild conditions on the kernel and L2 target regression
function the training dynamics undergo three stages characterized by the behav-
iors of the models in the two worlds. Our theoretical results also mathematically
formalize some interesting deep learning phenomena. Specifically, in our setting
we show that SGD progressively learns more complex functions and that there is
a “deep bootstrap” phenomenon: during the second stage, the test error of both
worlds remain close despite the empirical training error being much smaller. Fi-
nally, we give a concrete example comparing the dynamics of two different kernels
which shows that faster training is not necessary for better generalization.
1	Introduction
In order to fundamentally understand how and why deep learning works, there has been much effort
to understand the dynamics of neural networks trained by gradient descent based algorithms. This
effort has led to the discovery of many intriguing empirical phenomena (e.g. Frankle et al. (2020);
Fort et al. (2020); Nakkiran et al. (2019a;b; 2020)) that help shape our conceptual framework for un-
derstanding the learning process in neural networks. Nakkiran et al. (2019b) provides evidence that
SGD starts by first learning a linear classifier and over time learns increasingly complex functions.
Nakkiran et al. (2020) introduces the “deep bootstrap” phenomenon: for some deep learning tasks
the empirical world test error remains close to the oracle world error1 for many SGD iterations, even
if the empirical training and test errors display a large gap. To better understand such phenomena, it
is useful to study training dynamics in related but mathematically tractable settings.
One approach for theoretical investigation is to study kernel methods, which were recently shown to
have a tight connection with over-parameterized neural networks (Jacot et al., 2018; Du et al., 2018).
Indeed, consider a sequence of neural networks (fN (x; θ))N∈N with the widths of the layers going
to infinity as N → ∞. Assuming proper parametrization and initialization, for large N the SGD
dynamics on fN is known to be well approximated by the corresponding dynamics on the first-order
Taylor expansion of fN around its initialization θ0,
fN,lin(X； θ) = fN(X； θ0) + hVθ fN(X； θ0), θ - θ0i
Thus, in the large width limit it suffices to study the dynamics on the linearization fN,lin. When
using the squared loss, these dynamics correspond to optimizing a kernel least-squares objective
with the neural tangent kernel KN(x, x0) = hVθ/n(x; θ0), Vθ/n(x0; θ0)).
1Their paper uses “Ideal World” for “Oracle World” and “Real World” for “Empirical World”.
1
Under review as a conference paper at ICLR 2022
Figure 1: A conceptual drawing of empirical and oracle world learning curves. Stage 1: all curves
are together. Stage 2: training error goes to zero while test and oracle error stay together. Stage 3:
test error remains constant while oracle error decays to the RKHS approximation error. See Section
1.1 for a more detailed discussion. (Dotted lines in stage 3 indicate compressed time interval.)
Over the past few years, researchers have used kernel machines as a tractable model to investigate
many neural network phenomena including benign overfitting, i.e., generalization despite the inter-
polation of noisy data (Bartlett et al., 2020; Liang & Rakhlin, 2020) and double-descent, i.e, risk
curves that are not classically U-shaped (Belkin et al., 2020; Liu et al., 2021). Kernels have also
been studied to better understand certain aspects of neural network architectures such as invariance
and stability (Bietti & Mairal, 2017; Mei et al., 2021b). Although kernel methods cannot be used to
explain some phenomena such as feature learning, they can still be conceptually useful for under-
standing other neural networks properties.
1.1	Three stages of kernel dynamics
Despite much classical work in the study of gradient descent training of kernel machines (e.g. Yao
et al. (2007); Raskutti et al. (2014)) there has been limited work understanding the high-dimensional
setting, which is the setting of interest in this paper. Although solving the linear dynamics of gradient
flow is simple, the statistical analysis of the fitted model requires involved random matrix theory
arguments. In our analysis we study the dynamics of the Oracle World, where training is done on
the (usually inaccessible) population risk, and the Empirical World, where training is done on the
empirical risk (as is done in practice). Associated with the oracle world model ftor and the empirical
world model ft are the following quantities of interest: the empirical training error Rn(ft), the
empirical test error R(ft), and the oracle error R(ftor) defined in Eqs. (1), (2), (3) for which we
derive expressions that are accurate in high dimensions.
Informally, our main results show that under reasonable conditions on the regression function and
the kernel the training dynamics undergo the following three stages:
•	Stage one: the empirical training error, the empirical test error, and the oracle error are all close.
•	Stage two: the empirical training error decays to zero, but the empirical test error and the oracle
error stay close and keep approximately constant.
•	Stage three: the empirical training error is still zero, the empirical test error stays approximately
constant, but the oracle test error decays to the approximation error.
We conceptually illustrate the error curves of the oracle and empirical world in Fig. 1 and provide
intuition for the evolution of the learned models in Fig. 2. The existence of the first and third
stages are not unexpected: at the beginning of training the model has not fit the dataset enough to
distinguish the oracle and empirical world and at the end of training an expressive enough model
with infinite samples will outperform one with finitely many. The most interesting stage is the
second one where the empirical model begins to “overfit” the training set while still remaining close
to the non-interpolating oracle model in the L2 sense (see Fig. 2).
In Section 2 we discuss some related work. In Section 3 we elaborate our description of the three
stages and give a mathematical characterization for two particular settings in Theorems 1 and 2.
2
Under review as a conference paper at ICLR 2022
Stage 2 — end
TΓ-<∙	AA	,11	♦	l' , 1	1	l' , 1	∙ ∙	1	F	1	Fll	F "CT ɪ
Figure 2: A conceptual drawing of the evolution of the empirical and oracle models ft and ftor . In
stage 1, ft and for learn the best linear approximation of fd. At the start of stage 2, ft and for learn
the best quadratic approximation. At the end of stage 2, ft interpolates the training set but is close
to for in the L2 sense. Lastly in stage 3, for learns fd while ft stays the same as the end of stage 2.
Although the three stages arise fairly generally, we remark that certain stages will vanish if the
problem parameters are chosen in a special way (c.f. Remark 1). We connect our theoretical results
to related empirical deep learning phenomena in Remark 3 and discuss the relation to deep learning
in practice in Remark 4. In Section 4 we provide numerical simulations to illustrate the theory more
concretely and in Section 5 we end with a summary and discussion of the results.
2	Related Literature
The generalization error of the kernel ridge regression (KRR) solution has been well-studied in both
the fixed dimension regime (Wainwright, 2019, Chap. 13), (Caponnetto & De Vito, 2007) and the
high-dimensional regime (El Karoui, 2010; Liang & Rakhlin, 2020; Liu et al., 2021; Ghorbani et al.,
2020; 2021; Mei et al., 2021a;b). Most closely related to our results is the setting of (Ghorbani
et al., 2021; Mei et al., 2021a;b). Analysis of the entire KRR training trajectory has also been done
(Yao et al., 2007; Raskutti et al., 2014; Cao et al., 2019) but only for the fixed dimensional setting.
Classical non-parametric rates are often obtained by specifying a strong regularity assumption on
the target function (e.g. the source condition in Fischer & Steinwart (2020)), whereas in our work
the assumption on the target function is mild.
Another line of work directly studies the dynamics of learning in linear neural networks (Saxe et al.,
2013; Li et al., 2018; Arora et al., 2019; Vaskevicius et al., 2019). Similar to us, these works show
that some notion of complexity (typically effective rank or sparsity) increases in the linear network
over the course of optimization.
The relationship between the speed of iterative optimization and gap between population and empir-
ical quantities has been studied before in the context of algorithmic stability (Bousquet & Elisseeff,
2002; Hardt et al., 2016; Chen et al., 2018). These analyses certify good empirical generalization
by using stability in the first few iterations to upper bound the gap between train and test error. In
contrast, our analysis directly computes the errors at an arbitrary time t (c.f. Remark 2). The rela-
tionship between oracle and empirical training dynamics has been considered before in Bottou &
LeCun (2004) and Pillaud-Vivien et al. (2018).
3	Results
In this section we introduce the problem and present a specialization of our results to two concrete
settings: dot product and group invariant kernels on the sphere (Theorems 1 and 2 respectively). The
more general version of our results is described in Appendix A.3.
3.1	Problem setup
We consider the supervised learning problem where we are given i.i.d. data (xi, yi)i≤n. The covari-
ate vectors (xi)i≤n ~ad Unif(Sd-1(√d)) and the real-valued noisy responses y% = fd(xi) + εi for
some unknown target function fd ∈ L2(Sd-1(√d)) and (εi)i≤n ~讥& N(0, σ2). Given a function
3
Under review as a conference paper at ICLR 2022
f ∈ L2(Sd-1(√d)), We define its test error R(f) and its training error Rn(f) as
1n
R(f) ≡ E(Xnew".W ){("底卬-f (xnW ))7	Rbn(f) ≡ n£(Ui- f"	⑴
n i=1
Where (xnew, ynew) is i.i.d. With (xi, yi)i≤n. The test error R(f) measures the fit of f on the
population distribution and the training error Rn(f) measures the fit of f to the training set.
For a kernel function Hd : Sd-1( √d) X Sd-1( √d) → R, we analyse the dynamics of the following
two fitted models indexed by time t: the oracle model ftor and the empirical model ft , which are
given by the gradient flow on R and Rbn over the associated RKHS Hd respectively
ddtfθr(x) = -VR(ftor(x)) = E[Hd(x, z)(fd(z) - fθr(z))],
n
d1
否ft(χ) = -^Rn(ft(χ)) = nfHd(χ, χi)(yi - ft(χi)),
i=1
(2)
(3)
with zero initialization for ≡ f ≡ 0. These dynamics are motivated from the neural tangent kernel
perspective of over-parameterized neural networks (Jacot et al., 2018; Du et al., 2018). A precise
mathematical definition and derivation of these two dynamics are provided in Appendix E.1.
For our results we make some assumptions on the spectral properties of the kernels Hd similar to
those in Mei et al. (2021a) that are discussed in detail in Section A.2. At a high-level we require that
the diagonal elements of the kernel concentrate, that the kernel eigenvalues obey certain spectral gap
conditions, and that the top eigenfunctions obey a hyperconctractivity condition which says they are
“delocalized”. For the specific settings of Theorems 1 and 2 we give more specific conditions on the
kernels that are more easily verified and imply the required spectral properties.
3.2	Dot Product Kernels
In our first example, we consider dot product kernels Hd of the form
Hd(xι, x2) = hd(hxι, x2i∕d),	∀xι, x2 ∈ Sd-1(√d),	(4)
for some function hd : [-1, 1] → R. Our results apply to general dot product kernels under weak
conditions on hd given in Appendix C.2. In particular they apply to the random feature and neural
tangent kernels associated to certain fully connected neural networks (Jacot et al., 2018).
Before presenting our results for this setting we introduce some notation. Denote by P≤' the orthog-
onal projection onto the subspace of L2(Sd-1(√d)) spanned by polynomials of degree less than or
equal to '. The projectors P' and P>' are defined analogously (see Appendix G for details). We
use 0d(∙) for standard little-o relations, where the subscript d emphasizes the asymptotic variable.
The statement f (d) = ωd(g(d)) is equivalent to g(d) = od(f (d)). We use θd,p(∙) in probability
relations. Namely for two sequences of random variables Z1(d) and Z2(d), Z1(d) = od,P(Z2(d)) if
for any ε, Cε > 0 there exists dε ∈ Z>0, such that P(|Z1(d)/Z2(d)| < Cε) ≤ ε for all d ≥ dε. The
asymptotic notations Od, Od,P etc. are defined analogously.
Theorem 1 (Dot Product Kernels). Let {fd ∈ L2 (Sd-1(√d))}d≥1 be a sequence of functions such
that for some η > 0, kfd kL2+η = Od(1), and let {Hd}d≥1 be a sequence of dot product kernels
satisfying Assumption 4. Assume that for some fixed integers j, s ≥ 0 and some δ > 0 that
dj+δ ≤ t ≤ dj+1-δ,	ds+δ ≤ n ≤ ds+1-δ.
Then we have the following characterizations,
(a)	(Oracle World) The oracle model learns every degree component of fd as time progresses
R(for) = ιιP>jfd/+σ+od(i),	IIfor-P≤jfd∕ = od(i).
(b)	(Empirical World - Train) Empirical training error follows oracle error then goes to zero
Rn(ft) = l∣P>jfd∣∣L2 + σ2 + Od,p(1)	if t/n = 0d(1),
Rn(ft) = Od,p(1)	If t/n = ωd(1).
4
Under review as a conference paper at ICLR 2022
Figure 3: Schematic drawings of the conclusions of Theorems 1 and 2 for three different noiseless
settings. Panels (3a) and (3b) illustrate the performance of a dot product kernel Hd for two different
scalings n(d). The full three stages appear in (3a), but the second stages disappears in (3b) since
log n/ log d is nearly an integer (see Remark 1). Panel (3c) compares the performance of a dot
product kernel Hd (red) and corresponding cyclic kernel Hd,inv (green) for a cyclic target function
fd . The cyclic kernel in (3c) generalizes better but optimizes more slowly (see Remark 2).
(c)	(Empirical World - Test) Empirical test errorfollows oracle error until the empirical model
learns the degree-s component of fd
R(Zt)= IIP >min{j,s}fd∣∣L2 + σ2 + od,Ρ ⑴，IIft- P ≤min{j,s} fd |' = od,P ⑴.
The results are conceptually illustrated in an example in Fig. 3a which shows the stair-case phe-
nomenon in high-dimensions and the three learning stages. We see that in both the oracle world and
the empirical world, the prediction model increases in complexity over time. More precisely, the
model learns the best polynomial fit to the target function (in an L2 sense) of increasingly higher
degree. In the empirical world the maximum complexity is determined by the sample size n, which
is in contrast to the oracle world where there are effectively infinite samples.
The results imply that generally (but not always c.f. Remark 1) there will be three stages of learning.
In the first stage the oracle and empirical world models are close in L2 and fit a polynomial with
degree determined by t. The first stage lasts from t = 0 to t = nd-ε n for some small ε > 0.
As t approaches n, there is a phase transition and the empirical world training error goes to zero at
t = ndε n. From time nd-ε till at least ds+1 is the second stage where the empirical and oracle
models remain close in L2 but the gap between test and train error can be large. If the sample size
n is not large enough for ft to learn the target function, then at some large enough t we will enter
a third stage where ftor improves in performance, outperforming ft which remains the same. On
synthetic data in finite dimensions we can see a resemblance of the staircase shape which becomes
sharper with increasing d (c.f. Appendix F).
Remark 1 (Degenerate stages of Learning). For special problem parameters we will not observe
the second and/or third stages. The second stage will disappear if n N dq for some q ∈ N (see Fig.
3b), or if fd is a degree-s polynomial. The third stage will not occur if P>s fd lies in the orthogonal
complement of the RKHS Hd.
3.3	Group Invariant Kernels
We now consider our second setting which concerns the invariant function estimation problem
introduced in Mei et al. (2021b). As before, we are given i.i.d. data (xi, yi)i≤n where the fea-
ture vectors (xi)i≤n ~/d Unif(Sd-1(√d)) and noisy responses y% = fd(x. + £〃 We now
assume that the target function fd satisfies an invariant property. We consider a general type
of invariance, defined by a group Gd that is represented as a subgroup of the orthogonal group
in d dimensions. The group element g ∈ Gd acts on a vector X ∈ Rd via X → g ∙ x.
We say that f? is Gd-invariant if f?(x) = f?(g ∙ x) for all g ∈ Gd. We denote the space
of square integrable Gd-invariant functions by L2(Sd-1(√d), Gd). We focus on groups Gd that
are groups of degeneracy α as defined below. As an example, we consider the cyclic group
5
Under review as a conference paper at ICLR 2022
Cycd = {go, gι,..., gd-ι} where for any X = (xι,..., Xd)T ∈ Sd-1(√d), the group action is
defined by gi ∙ X = (χi+ι, Xi+2,...,Xd, X1,X2,..., Xi)T. The cyclic group has degeneracy 1.
Definition 1 (Groups of degeneracy α). Let Vd,k be the subspace of degree-k polynomials that are
orthogonal to polynomials Ofdegree at most (k 一 1) in L2(Sd-1(√d)), and denote by Vd,k (Gd) the
subspace of Vd,k formed by polynomials that are Gd-invariant. We say that Gd has degeneracy α if
for any integer k ≥ α we have dim(Vd,k /Vd,k (Gd)) dα (i.e., there exists 0 < ck ≤ Ck < +∞
such that ck ≤ dim(Vd,k/Vd,k(Gd)) ≤ Ck for any d≥ 2).
To encode invariance in our kernel we consider Gd-invariant kernels Hd of the form
Hd,inv (X1 , X2 )
h(hX1,g
Gd
• x2i∕d)∏d(dg)
(5)
where πd is the Haar measuare on Gd. Such kernels satisfy the following invariance property: for all
g, g0 ∈ Gd and for Hd(X1, X2) = Hd(g • X1, g0 • X2) for every X1, X2. For the cyclic group, πd is
the uniform measure. We now present our results for the group invariant setting.
Theorem 2 (Group Invariant Kernels). Let Gd be a group of degeneracy α ≤ 1 according to Def-
inition 1. Let {fd ∈ L2(Sd-1(√d), Gd)}d≥ι a sequence of Gd-invariant functions such that for
some η > 0, kfdkL2+η = Od(1), and let {Hd}d≥1 be a sequence of Gd-invariant kernels satisfying
Assumption 5. Assume that for some fixed integers j ≥ 0, s ≥ 1 and some δ > 0 that
dj+δ ≤ t ≤ dj+1-δ,	ds-α+δ ≤ n ≤ ds+1-α-δ .
Then we have the following characterizations,
(a)	(Oracle World) The oracle model learns every degree component of fd as time progresses
R(for) = ∣∣pjd∣∣L2+σε+g⑴，IIfor-PjdIlL2 =知⑴.
(b)	(Empirical World - Train) Empirical training error follows oracle error then goes to zero
Rn(ft) = ∣∣P>j fd∣∣L2 + σ2 + 0d,P ⑴	if t/n = Od(dα),
^	, O .	, .	.	, _
Rbn(ft) = Od,p(1)	ift/n = ωd(d ).
(c)	(Empirical World - Test) Empirical test error follows oracle error until the empirical model
learns the degree-s component of fd
R(ft) = ∣lP>min{j,s}fd∣∣L2 + σ2 + od,P ⑴，||ft - p≤min{j,s}fd∣l l2 = od,P ⑴.
With respect to the dot product kernel setting (c.f. Theorem 1), in this setting the behavior of the
oracle world is unchanged, but the empirical world behaves as if it has dα times as many samples.
This is illustrated graphically in Fig. 3c. It can be shown that using an invariant kernel is equivalent
to using a dot product kernel and augmenting the dataset to {(g • Xi, yi) : g ∈ Gd, i ∈ [n]} (c.f. Ap-
pendix E.2). Hence for the cyclic group which has size dα = d, we arrive at the following intriguing
conclusion: if the target function is invariant, then using a dot product kernel and augmenting n i.i.d
samples to nd many samples is asymptotically equivalent to using nd i.i.d samples.
Remark 2 (Optimization Speed versus Generalization). Interestingly, training with an invariant
kernel is slower than with a dot product kernel and takes longer to interpolate the dataset despite
eventually generalizing better on invariant function estimation tasks (c.f. Fig. 3c). This conclusion
is not an artifact of the continuous time analysis (c.f. Appendix E.3) and is observed empirically in
Section 4.2 for discrete-time SGD. This example highlights the limitation of stability based analyses
(e.g. Hardt et al. (2016)) which argue that faster SGD training leads to better generalization. While
a faster rate leads to better generalization in the first stage when stability can control the gap be-
tween train and test error, our analysis shows that the duration length of the first stage also impacts
the final generalization error.
Remark 3 (Connection with Deep Phenomena). The dynamics of high-dimensional kernel regres-
sion display behaviors that parallel some empirically observed phenomena in deep learning. For
6
Under review as a conference paper at ICLR 2022
kernel regression, we have shown that the complexity of the empirical model, measured as the num-
ber of learned eigenfunctions, depends on the time optimized when t n and the sample size when
t n. At a high-level, we also expect a similar story for neural networks but for some other notion
of complexity. It is believed that neural networks first learn simple functions and then progressively
more complex ones, until the complexity saturates after interpolating at some time proportional to n
(Nakkiran et al., 2019b). We have also shown that in kernel regression there is a non-trivial “deep
boostrap” phenomenon (Nakkiran et al., 2020) during the second learning stage: the gap between
the oracle world and empirical world test errors is negligible whereas the train and test errors ex-
hibit a substantial gap. The gradient flow results for kernel regression can also provide insight into
the deep bootstrap for random feature networks SGD as these results can approximately predict
their behavior (see Section 4.2).
Remark 4 (Connection with Deep Learning Practice). Although we believe our results conceptually
shed light on some of the interesting behaviors observed in the training dynamics of deep learning,
due to our stylized setting we may not exactly see the predicted phenomena in practice. Accurately
observing the three stages of kernel regression requires sufficiently high-dimensional data in order
for the kernel eigenvalues to obey a staircase-like decay and for training to be sufficiently long
as the time axis should be in log-scale. Our results hold for regression whereas for classification
the empirical model may continue improving after classifying the train set correctly. Despite these
caveats, certain conclusions can be observed in some realistic settings (c.f. Appendix E.4).
4	Numerical Simulations
As mentioned previously, Fig. 3 is a “cartoon” of the conclusions stated in Theorems 1 and 2. In
this section, we verify the qualitative predictions of our theorems using synthetic data. Concretely,
throughout this section We take d = 400 and n = d1.5 = 8000, and following our theoretical setup
(c.f. Section 3.1) generate covariates (xi)i≤n ~ad Unif(Sd-1( √d)) and responses y% = f?(xi) + εi
with (εi)i≤n ~iid N(0,σ2), for different choices of target function f?. All simulations in this
section are for the noiseless case σε2 = 0 but a noisy example is given in Appendix F.
In Section 4.1, we simulate the gradient flows of kernel least-squares with dot product kernels and
cyclic kernels (Fig. 4) to reproduce the three stages as shown in Fig. 3. In Section 4.2 we show
that SGD of (dot product and cyclic) random-feature models (Fig. 5) exhibit similar three stages
phenomena, in which the second stage behaviors are consistent with the deep bootstrap phenomena
observed in deep learning experiments (Nakkiran et al., 2020). Empirical quantities are averaged
over 10 trials and the shaded regions indicate one standard deviation from the mean.
4.1	Gradient Flow of Kernel Least-Squares
Under the synthetic data set-up mentioned earlier, we first simulate the oracle world and empirical
world errors curves of gradient flow dynamics using dot product kernels of the form
H(xι, x2) = Ew~sd-ι [σ({w, x0)σ({w, x2i)]	(6)
for some activation function σ. We will examine a few different choices of f? and kernel H, which
are specified in the descriptions of each figure.
The oracle world error is computed analytically but the empirical world errors require sampling
train and test datasets. We compute empirical world errors by averaging over 10 trials. The results
are visualized both in log-scale and linear-scale on the time axis. The log-scale plots allow for
direct comparison with the cartoons in Fig. 3. The linear-scale plots are zoomed into the region
0 < t ≤ nd0.4 since: 1) plotting the full interval squeeze all curves to the left boundary which is
uninformative 2) in practice one would not optimize for very long after interpolation.
In Figs. 4a and 4b we take the target function to be a polynomial of the form
f?(x) = ao He0(x1) + ... + ak Hek(xι),	IlPj/*12 ≈ ajj!	⑺
where Hei(t) is the ith Hermite polynomial (c.f. Appendix G.4) and the approximate equality in Eq.
(7) holds in high-dimensions. In panel (4a) we consider use the ReLU activation function σ(t) =
max(t, 0), and take f? to be a quadratic polynomial with (ao,a1,a2) = (1/2,1/√2,1/√8). With
7
Under review as a conference paper at ICLR 2022
Figure 4: Top row: Log-scale plot of errors versus training time for dot product kernel (4a, 4b) and
{dot product, cyclic} kernels in (4c). In (4a) σ = ReLU and (a0, aι, a2) = (1/2,1/√2,1/√8). In
(4b), σ = ReLU+0.IHe3 and (a0,a1,a2,a3) = (1/2,1∕√2,0,1∕√24). In (4c), f? is Eq. (8) and
σ = ReLU +0.1 He3. Bottom row: Same as the top row but with linear-scale time and zoomed in.
such a choice of parameters, We can see the three stages phenomenon. In panel (4b) We choose f?
to be a cubic polynomial with (a0,a1,a2,a3) = (1/2,1/√2,0,1/√24) and σ(t) = max(t, 0) +
0.1 He3(t) (We need the third Hermite coefficient of σ to be non-zero for stage 3 to occur). This
choice of coefficients for f? is such that ∣∣P>ιf*∣∣L2 ≈ ∣∣P>2 /?|施,so that the second stage in (4b)
is longer compared to (4a).
In Fig. 4c, We take the target function to be a cubic cyclic polynomial
f?(x)
dd	d
=√⅛( ∑Xi + ∑xixi+1 + Σ xixi+1xi+2
(8)
Where the subindex addition in xi+k is understood to be taken modulo d. We compare the per-
formance of the dot product kernel H and its invariant version Hinv (c.f. Eq. (5)) With activation
function σ(t) = max(t, 0) + 0.1 He3 (t). The kernel Hinv With n samples performs equivalently to
H With nd samples (c.f. Remark 2), but is more computationally efficient since the size of the kernel
matrix is still n × n. Using Hinv elongates the first stage by a factor d, delaying the later stages and
ensuring that the empirical World model improves longer.
Although in the simulations, the dimension d is not yet high enough to see a totally sharp staircase
phenomenon as in the illustrations of Fig. 3, even for this d We are still able to clearly see the
three predicted learning stages and deep bootstrap phenomenon across a range of settings. To better
understand the effect of dimension We shoW similar plots With varying d in Appendix F.
4.2	SGD for Two-layer Random-Feature Models
To more closely relate With deep learning practice and the deep bootstrap phenomenon (Nakkiran
et al., 2020), We simulate the error curves of SGD training on random-feature (RF) models (i.e. tWo-
layer netWorks With random first-layer Weights and trainable second-layer Weights), in the same
synthetic data setup as before. In particular, We look at dot product RF models
1N
fot(x; a) = √= faiσ(<wi, x〉)，	Wi 〜iid Unif(Sd-1),
N i=1
and cyclic invariant RF models
1N
fyc(x; a) = √== Eai J σ(hwi,g ∙ x))∏d(dg),	Wi 〜iid Unif(Sd-1).
8
Under review as a conference paper at ICLR 2022
Figure 5: Top Row: SGD dynamics of random-feature models as described in Section 4.2. The tar-
get function and data distribution of (5a), (5b), (5c) are that of (4a), (4b), (4c) respectively. Bottom
Row: The corresponding linear-scale errors of kernel least-squares gradient flow for comparison.
For all following experiments we take the activation σ to be ReLU and N = 4 × 105 ≈ n1.4.
For a given data distribution and RF model we train two fitted functions, one on a finite dataset
(empirical world) and the other on the data distribution (oracle world). More specifically, the training
of the empirical model is done using multi-pass SGD on a finite training set of size n with learning
rate η = 0.1 and batch size b = 50. The training of the oracle model is done using one-pass SGD
with the same learning rate η and batch size b, but at each iteration a fresh batch is sampled from
the population distribution. Both models ft , ftor are initialized with ai = 0 for i ∈ [N]. To speed up
and stabilize optimization we use momentum β = 0.9. Note that if we took N → ∞, η → 0, and
β = 0 we would be exactly in the dot product kernel gradient flow setting.
In Fig. 5, the data generating distributions of panels (5a), (5b), (5c) are respectively the same as
that of panels (4a), (4b), and (4c) from Section 4.1. The top row of Fig. 5 shows SGD for {dot
product, cyclic} RF models, and the bottom row shows the corresponding gradient flow for {dot
product, cyclic} kernel least-squares. We see that the corresponding curves in these two rows exhibit
qualitatively the same behaviors. Additionally, the results in panel (5c) show that as predicted, even
for discrete SGD dynamics the dot product RF optimizes faster but fails to generalize, whereas the
invariant RF optimizes slower but generalizes better.
5	Summary and Discussion
In this paper, we used precise asymptotics to study the oracle world and empirical world dynamics
of gradient flow on kernel least-squares objectives for high-dimensional regression problems. Under
reasonable conditions on the target function and kernel, we showed that in this setting there are three
learning stages based on the behaviors of the empirical and oracle models and also connected our
results to some empirical deep learning phenomena.
Although our setting already captures some interesting aspects of deep learning training dynamics,
there are some limitations which would be interesting to resolve in future work. We require very
high-dimensional data in order for the asymptotics to be accurate, but real data distributions have
low-dimensional structure. We work in a limiting regime of neural network training where the
dynamics are linear and the step-size is infinitesimal. It is an important direction to extend this
analysis to the non-linear feature learning regime and to consider discrete step-size minibatch SGD,
as these are considered important aspects of network training. Our results hold for the square-loss
in regression problems, but many deep learning problems involve classification using cross-entropy
loss. Lastly, our analysis holds specifically for gradient flow, so it would be also interesting to
consider other iterative learning algorithms such as boosting.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32:7413-7424, 2019.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167-1180, 2020.
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of
deep convolutional representations. arXiv preprint arXiv:1706.03078, 2017.
Leon BottoU and Yann LeCun. Large scale online learning. Advances in neural information Pro-
cessing systems, 16:217-224, 2004.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. The Journal OfMachine Learn-
ing Research, 2:499-526, 2002.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv PrePrint arXiv:1912.01198, 2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of ComPutational Mathematics, 7(3):331-368, 2007.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv PrePrint arXiv:1804.01619, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv PrePrint arXiv:1810.02054, 2018.
Noureddine El Karoui. The spectrum of kernel random matrices. The Annals of Statistics, 38(1):
1-50, 2010.
Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algo-
rithms. J. Mach. Learn. Res., 21:205-1, 2020.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. arXiv PrePrint arXiv:2010.15110,
2020.
Jonathan Frankle, David J Schwab, and Ari S Morcos. The early phase of neural network training.
arXiv PrePrint arXiv:2002.10365, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? arXiv PrePrint arXiv:2006.13409, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. The Annals of Statistics, 49(2):1029-1054, 2021.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR,
2016.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv PrePrint arXiv:1806.07572, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47. PMLR, 2018.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv PrePrint arXiv:1911.00809, 2019.
10
Under review as a conference paper at ICLR 2022
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gener-
alize. TheAnnals ofStatistics, 48(3):1329-1347, 2020.
Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Refined anal-
ysis beyond double descent. In International Conference on Artificial Intelligence and Statistics,
pp. 649-657. PMLR, 2021.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random fea-
tures and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
arXiv:2101.10588, 2021a.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019a.
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances
in Neural Information Processing Systems, 32:3496-3506, 2019b.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. arXiv preprint arXiv:2010.08127, 2020.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gradi-
ent descent on hard learning problems through multiple passes. arXiv preprint arXiv:1805.10074,
2018.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1):
335-366, 2014.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems, 32:2972-2983, 2019.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learn-
ing. Constructive Approximation, 26(2):289-315, 2007.
11
Under review as a conference paper at ICLR 2022
A	General Setting
In this section we present our theory for training dynamics of kernel regression in an abstract setting
similar to that of Mei et al. (2021a). We first introduce the setting of interest, then state the relevant
assumptions, and finally we provide our theoretical results. We provide proofs of these results in
Appendix B.
A.1 Problem Setup
Consider a sequence of Polish probability spaces (Xd, νd), where νd is a probability measure on the
configuration space Xd, indexed by an integer d. We denote by L2 (Xd) = L2 (Xd, νd) the space of
square integrable functions on (Xd, νd). ForP ≥ 1, We denote IlfIlLp(Xd)= Eχ~νd[∣f (x)p|]1/p the
Lp norm of f. Let Dd ⊆ L2(Xd) be a closed linear subspace. In some simple applications we will
consider Dd = L2(Xd), but the extra generality Will be useful in certain applications.
We are concerned With a supervised learning problem Where We are given i.i.d data (yi, xi)i≤n. The
feature vectors Xi ~a& Vd are in Xd and the empirical-valued noisy responses yi are given by
yi = fd (xi ) + εi
for some unknoWn target function fd ∈ Dd and εi ~iid N(0, σ2).
We consider a general RKHS defined on (Xd, νd) via the compact self-adjoint positive definite
operator Hd : Dd → Dd Which admits the representation
Hdg(x) =	Hd(x, x0)g(x0)νd(dx0),
Xd
Where Hd ∈ L2(Xd × Xd) With the property that X Hd(x, x0)g(x0)νd(dx0) = 0 for g ∈ Dd⊥.
By the spectral theorem of compact operators, there exists an orthonormal basis (ψj)j≥1,
span(ψj, j ≥ 1) = Dd ⊆ L2 (Xd) and empirical eigenvalues (λd,j)j≥1 With nonincreasing ab-
solute values ∣λd,ι∣ ≥ ∣λd,2∣ ≥ … and Pj≥ι λdj < ∞ such that
∞
Hd(x1,x2) =	λ2d,jψj(x1)ψj(x2)
j=1
Where convergence holds in L2 (Xd × Xd).
For S ⊆ {1, 2, . . .} We denote PS to be the projection operator from L2(Xd) onto Dd,S :=
span(ψj, j ∈ S). We denote Hd,S to be the operator
∞
Hd,S = X λ2d,j ψj ψj?
j=1
and Hd,S the corresponding kernel.
If S = {j ∈ N : j ≤ '} we will write as short-hand Hd, ≤' and analogously for S = {j ∈ N : j > '}.
The trace of this operator is given by
Tr(Hd,s) ≡ X λd,j = Eχ~νdHd,s(x, x)] < ∞.
j∈S
Define the test error R : L2(Xd) → R and training error Rbn : L2(Xd) → R
1n
R(f) ≡ E(Xnew,ynew){(ynew - f (*new))2},	^bn(f) ≡ n Σ>i - f(Xi))2	⑼
i=1
where (X1, y1), . . . , (Xn, yn), (Xnew, ynew) are iid. For a kernel Hd ∈ L2(Xd ×Xd) we will consider
the oracle model ftor and the empirical model ft which satisfy the following gradient flows
ddtfθr(x) = -VR(fOr(x)) = Ez“d [Hd(x, z)(fd(z) — fθr(z))],	(10)
n
d1
dft(x) = ZRn(ft(χ)) = - EHd(x, Xi)(yi - ft(Xi)).	(11)
n i=1
12
Under review as a conference paper at ICLR 2022
A.2 General Assumptions
We now state our assumptions on the kernel and the sequence of probability spaces (Xd, νd).
Assumption 1 ({n(d), m(d)}d≥1-Kernel Concentration Property). We say that the sequence of op-
erators {Hd}d≥1 satisfies the Kernel Concentration Property (KCP) with respect to the sequence
{n(d), m(d)}d≥1 if there exists a sequence of integers {r(d)}d≥1 with r(d) ≥ m(d), such that the
following conditions hold.
(a)
(Hypercontractivity of finite eigenspaces.) For any fixed q ≥ 1, there exists a constant C
such that for any h ∈ Dd,≤r(d) = span(ψs, 1 ≤ s ≤ r(d)), we have
khkL2q ≤ C khkL2 .
(b)	(Properly decaying eigenvalues) There exists fixed δ0 > 0, such that, for all d large enough,
n(d)2+δ0 ≤
n(d)2+δ0 ≤
(P∞=r(d) + 1 λdj)2
P∞	λ8,
j=r(d)+1 λd,j
(P∞=r(d) + 1 λdj)2
P∞	λ4.
j=r(d)+1 λd,j
(c)	(Concentration of diagonal elements ofkernel) For (Xi )i∈[n(d)] ~iid Vd, we have:
max、1∣Ex~Vd [Hd,>m(d) (Xi, x) ] - Ex,x0~νd [Hd,>m(d) (x, X ) ] I = od,p(1) ∙ Ex,x0~νd [Hd,>m(d) (x, X )],
i∈[n(d)]
max, ∣Hd,>m(d)(Xi, Xi) - Eχ[Hd,>m(d)(x, x)]∣ = 0d,p(1) ∙ Ex Hd,>m(d)(x, x)].
i∈[n(d)]
Assumption 1(a) can be interpreted as requiring that the top eigenfunctions of Hd are delocalized.
Assumption 1(b) concerns the tail of eigenvalues ofHd and is a mild assumption in high-dimensions.
Lastly, 1(c) essentially requires that “most points” in Xd behave similarly in the sense of having
similar values of the kernel diagonal Hd(X, X).
Assumption 2 (Eigenvalue condition at level {(n(d), m(d))}d≥1). We say that the sequence of
kernel operators {Hd}d≥1 satisfies the Eigenvalue Condition at level {(n(d), m(d))}d≥1 if the fol-
lowing conditions hold for all d large enough
(a)	There exists a fixed δ0 > 0, such that
∞
n(d)1+δ0 ≤ λ4	X	入鼠,	(12)
d,m(d)+1 k=m(d)+1
∞
n(d)1+δ0 ≤ λ	X	λd,k.	(13)
d,m(d)+1 k=m(d)+1
(b)	There exists a fixed δ0 > 0, such that
∞
n(d)i ≥ Q X	入菰.
d,m(d) k=m(d)+1
(c)	There exists a fixed δ0 > 0, such that
m(d) ≤ n(d)1-δ0 .
Assumptions 2(a) and 2(b) can be seen as a spectral gap assumption. This ensures a clear separa-
tion between the eigenvalues in the subspace Dd,≤m(d) and the subspace Dd,>m(d). The technical
requirement 2(c) is mild.
In the asymptotic setting, we will be interested in understanding the model learned at a time t = t(d)
which scales with the dimension. The next set of assumptions give requirements for a valid scaling.
13
Under review as a conference paper at ICLR 2022
Assumption 3 (Admissible Time at {t(d), u(d), n(d), m(d)}d≥1). We say that the sequence
{t(d), u(d), n(d), m(d)}d≥1 is an Admissible Time for the sequence of kernel operators {Hd}d≥1 if
the following conditions hold
(a)	There exists a fixed δ0 > 0, such that for d large enough
λ∙J ≤ t(d)1-δ0 ≤ t(d)1+δ0 ≤ λ~」—
d,u(d)	d,u(d)+1
(b)	If u(d) < m(d) for infinitely many d, then
nn(d)	X S = od ⑴.
k=m(d)+1
(c)	There exists a constant C such that
m(d)	∞
X λd2,k ≤ C X	λ2d,k
k=1	k=m(d)+1
Assumption 3(a) is similar to the spectral gap condition Assumption 2(a), 2(b) for (t(d), u(d))d≥1.
Assumption 3(b) relates the ordering of the indices u(d), m(d) to the relative growth of t(d), n(d).
Assumption 3(c) requires that the eigenvalue tail does not decay too abruptly.
A.3 Main Results
In this section we give the main theoretical results. Recall the problem set-up and notation from
Appendix A.1. We will characterize the gradient flow dynamics dynamics of the oracle model ftor
Eq. (10) and the empirical model ft Eq. (11) for a general kernel Hd.
Theorem 3 (Oracle World). Let {fd ∈ Dd}d≥1 be a sequence of functions and {Hd}d≥1 be a
sequence of kernel operators such that {(Hd,t(d), u(d))}d≥1 satisfies Assumption 3(a), then
R(fOr) = ∣∣P>u(d)fd/ + σ2 + Od⑴∙kfdkL2 ,
Ilfor- P≤u(d)fd∣∣L2 = Od(I) ∙kfdkL2,
where P≤u(d) and P>u(d) are the projection operators onto the subspace spanned by the top u(d)
kernel eigenfunctions and the orthogonal complement respectively, as defined in Appendix A.1.
The error of the oracle model is determined solely by optimization time t through u(d). Due to the
spectral gap assumption 3(a) learning only occurs along the top u(d) eigenfunctions.
The next results describe the empirical model. First we characterize the training error.
Theorem 4 (Empirical World - Train). Let {fd ∈ Dd}d≥1 be a sequence of functions,
(xi)i∈[n(d)] 〜 Vd independently, and {Hd}d≥ι be a Sequence of kernel operators such that
{(Hd, n(d), m(d), t(d), u(d))}d≥1 satisfies {(n(d), m(d))}d≥1-KPCP (Assumption 1), eigenvalue
condition at level {(n(d), m(d))}d≥1 (Assumption 2), and {(n(d), m(d), t(d), u(d))}d≥1 is a valid
time (Assumption 3). Define κH = Tr Hd,>m(d) and `(d) = min{u(d), m(d)}. Then for any
η > 0 we have,
Rn(ft) = Il P>'(d)fd∣∣L2 + σ + Od,p(1) ∙ (kfdkL2+η + σ2),	if t = Od(n/KH),
Rn(ft) = Od,p(1) ∙ (kfdkL2+η + σ2),	ift = ωd(n∕κH).
In the early-time regime t《 n∕κH the training error may be non-zero and matches the oracle world
error if also u(d) ≤ m(d). In the late-time regime t》n∕κH the training error is negligible and the
model interpolates the training set. The quantity κH arises since the empirical kernel matrix can be
decomposed as H = H≤m + H>m and the second component is approximately a multiple of the
identity: H>m ≈ Tr(H>m) ∙ In = KH ∙ In. This term acts as a self-induced ridge-regUlarizer.
Our final result characterizes the test error of the empirical model.
14
Under review as a conference paper at ICLR 2022
Theorem 5 (Empirical World - Test). Let {fd ∈ Dd}d≥1 be a sequence of functions,
(xi)i∈[n(d)] 〜 Vd independently, and {Hd}d≥ι be a Sequence of kernel operators such that
{(Hd, n(d), m(d), t(d), u(d))}d≥1 satisfies {(n(d), m(d))}d≥1-KPCP (Assumption 1), eigenvalue
condition at level {(n(d), m(d))}d≥1 (Assumption 2), and {(n(d), m(d), t(d), u(d))}d≥1 is a valid
time (Assumption 3). Define `(d) = min{u(d), m(d)}. Then for any η > 0 we have,
R(ft) = IIP>'(d)fd∣∣L2 + σ2 + od,Ρ⑴∙ (kfdkL2+η + σ2),
ft - P≤'(d)fd∣∣2 2= 0d,p(1) ∙ (kfdkL2+η + σ2).
The result shows that the empirical model is essentially the projection of the regression function
onto the first `(d) eigenfunctions. The quantity `(d) controls the complexity of the model which
increases with time t up until u(d) ≥ m(d) after which the complexity is limited by n.
15
Under review as a conference paper at ICLR 2022
B Proof of General Setting
B.1 Oracle World - Proof of Theorem 3
The oracle model ODE Eq. (10) with initialization f0or ≡ 0 can be solved (c.f. Appendix E.1) to
yield the solution
ftor = fd - e-tHd fd.
Therefore the excess risk is given by
R(fOr)-σ2 = Ex 〜Vd (fd (x)-fθr(x))2
L
Xd
∞
fd(X)e-2tHd fd(X)Vdmx) = χ eχp(-2tλd,k )f2.
k=0
where λd k are the kernel eigenvalues and fk := hfd, ψk)工2 are the Fourier coefficients of fd in the
kernel eigenbasis (c.f. Appendix A.1). We can control this quantity as follows,
IRfor)- σ2- kp>ufdkLl/ kfdkLz ≤ max m maχeχp(-2tλd,k), JmaXJ- eχp(-2tλd,k)
k≤u	k≥u+1
≤ max{maxexp(-a(txd,k)), kmaχl 5tλd,k)}
≤ maχ{exp(-Ω(tδ0 )),O(t-δ0)} = od(1)
where the last inequality follows from Assumption 3(a). This shows the first theorem statement,
kfor - P≤ufdkL2 = od(1) ∙ kfdkL2 .
Now observe that
u
P≤ufd - for = X e-tλd,k fkΨk - X (1 — e-tλd,k )fkΨk
k=0	k≥u+1
hence
kfor - p≤ufd∣ιL2/1IfdkL2 ≤ max m maxexp(-2tλd,k), JmaXI(I- e-tλd,k)2 ∖
k≤u	k≥u+1
≤ max < maxexp(-2tλd k), max 1 - exp(-2tλd,k) = Od(I)
k≤u	k≥u+1
where the second inequality follows from the fact that (1 - e-x)2 ≤ 1 - e-2x and the final equality
is from the proof of the first part of the theorem. Thus,
kfor - P≤ufdkL2 = Od⑴∙ kfdkL2
completing the proof.
B.2	EMPIRICAL WORLD - PRELIMINARIES
We will introduce some useful notations and the objects of analysis for studying the empirical world.
B.2.1	Training Dynamics
For the training of the empirical world c.f. Eq. (11), if u(t) = (ft(X1), . . . , ft(Xn)) ∈ Rn then from
Eq. (44) if u(0) = 0 then
u(t) =y-e-tH/ny= (In -e-tH/n)y,	(14)
where H ∈ Rn×n with the (i, j)th element given by Hij = Hd(Xi, Xj) and y = f + ε ∈ Rn with
f = (fd(X1), . . . , fd(Xn))T ∈ Rn and ε = (ε1, . . . , εn)T ∈ Rn.
16
Under review as a conference paper at ICLR 2022
For x ∈ Rd, define h(x) = (Hd(x1, x), . . . , Hd(xn, x))T ∈ Rn. The training and test errors as
defined in Eq. (1) can be written as
Rbn(ft) = 1 ku(t) - yk2 = 1 yTe-2tH/ny,
nn
R(ft) = Ex [(fd(x) - u(t)THTh(X))2].
一. 一，个、	...
Expanding R(ft) yields
R(ft) = Ex [fd(x)2] - 2u(t)THTE + u(t)THTMH-1u(t)	(15)
where E = (E1,...,En)T ∈ Rn, M = (Mij)i,j∈[n] ∈ Rn×n, andH = (Hij)i,j∈[n] ∈ Rn×n
with
Ei = Ex[fd(x)Hd(x, xi)],
Mij = Ex [Hd(x, xi)Hd(x, xj )],
Hij = Hd(xi, xj).
B.2.2	Decompositions and Notations
In this section we recall some useful decompositions of empirical quantities from Mei et al. (2021a).
As mentioned earlier the eigendecomposition of Hd is given by
∞
Hd(x, y) =	λ2d,kψk(x)ψk(y).
k=1
We write the orthogonal decomposition of fd in the basis {ψk }k≥1 as
∞
fd(x) = Efd,k ψk (x).
Define
ψk = (ψk(x1), . . . ,ψk(xn))T ∈ Rn,
D≤m = diag(λd,1, λd,2,..., λd,m) ∈ Rm×m,
Ψ≤m = (ψk (xi))i∈[n],k∈[m] ∈ Rn×m,
^	Z O	O	个、T	一 一
f≤m = (fd,1, fd,2, ..., fd,m)τ ∈ R .
We have the following orthogonal basis decompositions of f, H, E and M
b
f = f≤m + f>m ,	f≤m = Ψ≤m f≤m ,
H = H≤m + H>m ,	H≤m = Ψ≤mD≤2 mΨT≤m,
E = E≤m + E>m,	E≤m = Ψ≤mD≤2 mfb≤m,
M=M≤m+M>m,	M≤m = Ψ≤mD≤4 mΨT≤m,
∞
f>m = E fd,kΨk,
k=m+1
∞
H>m = X λd2,kψkψkT,
k=m+1
∞
E>m =	λd,kfd,kψk,
k=m+1
∞
M>m = X λd4,kψkψkT.
k=m+1
(16)
By Lemma 6 below, under Assumptions 1 and 2(a) the matrices H and M can be written as
H = Ψ≤mD≤2 mΨT≤m + κH (In + ∆H),	(17)
M = Ψ≤mD≤4 mΨT≤m +κM(In + ∆M),	(18)
17
Under review as a conference paper at ICLR 2022
where
∞
κH = Tr(Hd,>m ) = X λd,k ,
k≥m+1
∞
KM = Tr(Hd,>m) = X λd,k，
k≥m+1
and
max{k∆Hkop, k∆Mkop} = od,P(1).
We will Use α as shorthand for the scalar valued dimension dependent quantity e-(t/n)KH and take
K≤m := In - αe-E^ψ≤mD≤mψ≤m.	(19)
We also introduce the shrinkage matrix defined as
S≤m = (Im + κH D≤m )T = diag((sj )j∈H) ∈ Rm×m,	where Sj = λjλjH ∙	Q0)
If unspecified we will typically use ∆, ∆0, etc. to denote matrices with operator norm od,P(1). For
positive integers ' < m, define [', m] = {' +1,..., m}. We will use the following notation
S'm = diag((sj)j∈[',m])	for Sj defined in Eq. (20)
Ψ'm = (Ψk(Xi))i∈[n],k∈[',m] ∈ Rn×(ImT)
f'm = E fd,kψk
k∈ [',m]
B.2.3	Auxiliary Lemmas
Here we collect some lemmas which will be of use to us.
Lemma 1 (Matrix Exponential Perturbation Inequality). For matrix operator norm k ∙ k, if A, B ∈
Rn×n are symmetric then
eA - eB ≤ kA - Bk max{eA, eB}.
For general square matrices A, B ∈ Rn×n we have
eA-eB ≤ kA - BkekAkekBk.
Lemma 2. Let Z ∈ Rm×n,Ψ ∈ Rm×p, D ∈ Rp×p andt ∈ R. Denote A = ZTΨ ∈ Rn×p and
B = ΨTΨ ∈ Rp×p. Then,
ZTetΨDΨTΨ = AetDB.
The notations in Lemmas 3-8 all follow the notations given in Section B.2.2.
Lemma 3 (Lemma 12 from Mei et al. (2021a) with λ = 0). Let Assumptions 1 and 2 hold. Then,
IlnHTMHT- Ψ≤mS≤mΨ≤m∕n∣∣op = Od,p(1).
Lemma 4 (Theorem 6(b) from Mei et al. (2021a)). Let Assumptions 1(a), 2(c) hold. Then,
∣∣ψ≤mψ≤l√n - Im∣∣op = od,P⑴.
Lemma 5 (Lemma 13 from Mei et al. (2021a) with λ = 0). Let Assumptions 1 and 2 hold. Then,
∣∣ΨT≤mH -1Ψ≤mD≤2 m - S≤m∣∣op = od,P(1).
Lemma 6 (Theorem 6 from Mei et al. (2021a)). Let Assumptions 1 and 2(a) hold. Then we can
decompose the kernel matrices as follows
H=Ψ≤mD≤2mΨT≤m + κH (In + ∆H),
M = Ψ≤mD≤4 mΨT≤m + κM (In + ∆M),
18
Under review as a conference paper at ICLR 2022
where
∞
κH = Tr(Hd,>m ) = X λd,k ,
k≥m+1
∞
KM = Tr(Hd,>m) = X λd,k，
k≥m+1
and
max{k∆Hkop, k∆Mkop} = od,P(1).
Lemma 7. Let Assumption 1(a) hold. Let S, T be disjoint subsets of N. Let D ∈ RlSl×lSl be a
diagonal matrix. Then we have that for any η > 0, there exists C(η) (independent of d), such that
E[fTψTΨsDΨSΨtfτ]/n2 ≤ C(η) kPτfdkL2+η Tr(D)/n,
where the expectation is with respect to the randomness in ΨS , ΨT.
Proof. Let ∣: S → [|S|] be the bijection such that Ψs = (Ψ∣-i(k)(xi))i∈[n],k∈[∣s∣] then we have
E[fTΨTΨsDΨSΨtfT]/n2 = XXX D∣(s)∣(s)E[Ψu(xi)Ψs(xi)Ψs(xj)ψv(xj)]fufv/n2
u,v∈τ s∈s i,j∈[n]
=	D∣(s)∣(s)E[ψu(Xi)ψs(Xi)ψs(Xi)ψv (XiXfufv/n2
u,v∈τ s∈s i∈[n]
=1 X D∣(s)ι(s)Eχ[(Pτ fd(x))2ψs (x)2]
n s∈s
≤ n X D∣(s)∣(s) kPτ fdkL2+η kψs kL(4+2η)∕η
n s∈s
n
≤ C(η) kPτfdkL2+η XDii/n,
i=1
where the second to last inequality is by Holder’s inequality and the last inequality used the hyper-
Contractivity assumption as in Assumption 1(a).	□
Lemma 8 (Exponential Kernel Decomposition). Assume the conditions of Lemma 6 hold and as-
sume there exists δ0 > 0, such that (t(d), u(d)) satisfies the condition
t(d)1+δ0
<	1
d,u(d)+1
(21)
Let j (d) satisfy min{u(d), m(d)} ≤ j(d) ≤ m(d) then
∣∣e-tHd∕n - e-(t∕n)(ψ≤jd≤jψ≤j+κHIn) U	= od,P(1)
Proof. First consider the regime t = Od(n∣KH). Recall the decomposition,
H = Ψ≤mD≤2 mΨT≤m + κH (In + ∆H)
where k∆H kop = od,P(1). By Lemma 1 and Lemma 4,
e-tH/n - e-(t∕n)(Ψ≤j D≤' Ψ≤ +khI)U ≤
m
X tλd,kψkψ"n
k≥j+1
+ (t/n)KH k∆hkop
op
≤
(t ∙ Jmax ∙λd,Q llψ≤mψ≤m"llop + 0d,P⑴
k≥j +1
≤ Od,P(t ∙ kmaXι λd,k) + od,P(I)
(=a) Od,P(t-δ0 ) + od,P(1) = od,P(1),
19
Under review as a conference paper at ICLR 2022
where equality (a) holds by assumption Eq.(21). Now if t = ωd(n∕κH), then it is easy to see that
max |||e-tH/n||	JeTt-ψ≤jD≤'ψ≤+κHI)∣∣ } ≤ e-ωd,P(I) = od,P⑴,
and therefore
(e-tH/n - e-(t∕n)(Ψ≤jD≤jΨ≤ + khI)∣∣
≤ 2max {||e-tH/n||	J∣e-(t∕n)(ψ≤jD≤jψ≤j+κHI)∣∣ } = Od,p⑴.
□
B.3	Empirical World - Train
If t = ωd(n∕κH), then by Eq. (17) it is easy to see that ||e-2(t/n)H||op = Od,p(1), hence
^	, ^ .	.......O
Rnft)= od,P⑴∙ kfdkL2 .
From now on we focus on the case that t = od(n∕κH). Let us decompose the training error as
follows
^ ^ , _ _ _
Rbnft) = R1 + R2 + R3
where
Ri = 1 fTe-2(t/n)H f,
n
R2 = 2 eTe-2(t/n)H f,
n
R3 = 1 eTe-2(t/n)H ε.
n
B.3.1	TERM R1
Let us start by analysing R1. We can write
Ri = 1f≤' + f>'Ve-2Wn)H (f≤' + f>')
= T1 + 2T2 + T3
where
Ti = 1 fT'e-2(t/n)H f≤',
n≤
T2 = n fT'e-2(t/n)H f≤',
T3 = 1 fT'e-2(t/n)H f>'.
Let us analyse the term Ti,
T1 ≤ 1 fT'e-2(t∕n)ψ≤'D≤'ψ≤'f≤'.
n≤
By Lemma 2, denoting B = Ψ≤'Ψ≤'∕n,
1 ψT e-2("n)Ψ≤'D≤'Ψ≤'ψ≤' = Be-2tD≤'B .
n —	—
We will show that
∣∣Be-2tD≤'BIIO p = 0d,p(1).	(22)
20
Under review as a conference paper at ICLR 2022
By Lemma 4, since kBkop = Θd,P(1), for d large B has a positive-definite square root B1/2 w.h.p.
Therefore we can write
Be-2tD≤ `b = Bl∕2e-2tB1∕2D≤'B1/2 b1/2
and bound the operator norm
ge-2tD≤'B∣L「'|田总卜-2国/27.2匕
=((Bk	e-2tλmin(B”2D≤' B1/2)
≤ IIBkop e-2tλmaX(B)λminCD≤') = Od,p(l).
since by Assumption 3(a), tλmin(D≤`) = Ω(tδ0). Therefore T = 0d,p(1) ∙ ∣∣P≤'fdkL2.
Now we analyse the term T3. Observe that by the inequality 1 - x ≤ e-x ≤ 1,
nkf>'k2 -(2t/n)nfT5ψ≤'D≤'ψ≤' + KH(In + δh))f>' ≤ T3 ≤ nkf>'k2∙
We have by Lemma 7,
tE[f>'Ψ>'Ψ≤'D≤ 'Ψ≤'Ψ>'f>']∕n2 ≤ C (η) ∣P>' fdkL2+η	X λdj
where the last quantity is 0d,p(1) ∙ ∣∣P>'fdkL2+η by Assumption 3(c). Thus We see that
T = kP>'fdkL2 + Od,p(1) ∙ kP>'fdkL2+η .
Observe that by the Cauchy-Schwarz inequality,
T2 ≤ (TIT3)1/2 ≤ Od,p(1) ∣P≤'fd∣L2 ∣P>'fd∣L2+η .
Putting everything together we see that,
Rl = kP>'fdkL2 + Od,p(1) ∙ (kfdkL2 + kP>'fdkL2+η ).
B.3.2	TERM R2
Turning to R2 , we take the second-moment with respect to ε
ɪ Eε[R2] = -4 Ee[eTe-2(t/n)H ff Te-2(t/n)H ε]∕σ∣
σε2	2	n2	ε
=fτe-4En)Hf ≤ 4(kf∣2∕n)
n2	n
=od,P⑴∙ llfdkL2 .
By Markov,s inequality R = 0d,p(1) ∙ ∣∣fdkL2 σ2.
B.3.3	TERM R3
Now let US analyse R3. Recalling the definition B = Ψ≤Ψ≤'∕n, we can compute the expectation
ɪ (1 - Eε[R3]) = 1τr(In - e-2(t/n)H)
=-e-κHt/n Trkn -e-2C(ψ≤'D≤'ψ≤Q) + od,P(1)
≤ ne-κHt/n Tr(2(t∕n)(Ψ≤'D≤'Ψ≤Q) + 0d,p(1)
=ne-κHt/n Tr(2tBD≤`) + 0d,p(1)
≤ 2te-κHt/n IBkop Tr(D≤`) + 0d,p(1) = 0d,p(1).
21
Under review as a conference paper at ICLR 2022
Equality (a) follows from Lemma 8. For (b) we used the inequality 1 -e-x ≤ x and that trace is the
sum of eigenvalues. Equality (c) uses the cyclic property of trace. In the last equality we used that by
Lemma 4, ∣∣Bkop = Od,p(1) and Assumption 3(c) implies that (t/n) Tr(D≤`) = Od((t∕n)κH)=
0d(1) since by assumption We consider t = Od(n/KH). Turning to the variance
Varε [R3] = Eε [R32 ] - Eε [R3]2
=JEe[3Te-2(t/n)Hε)2] - Eε[R3]2
n2
≤ ^^2 Eε[(ETE)2] - σ4(I + od,P(I))
n2	ε
=O(1/n) ∙ σ4 + σ4 - σ4(I + od,P(I))
=od,P ⑴∙ σ4,
where the first inequality uses that ^e-2(t/n)H IIop ≤ 1 and (a) uses that for εi 〜N(0,σ2), E[εi4]=
3σε4 . Therefore by Chebyshev’s inequality, R3 = σε2 (1 + od,P(1)).
Putting everything together yields
Rnft)= RI + R2 + R3 =IlP>'∣∣L2 + od,P⑴∙ (kfdkL2+η + σ2)∙
B.4 Empirical World - Test
Recalling u(t) from Eq. (14), let
u(t) = v(t) + E(t)
where
v(t) = (In - e-tH/n)f,	(23)
E(t) = (In - e-tH/n)E.	(24)
Recall the expansion of the test error from Eq. (15),
Rft)= Ex [fd(x)2] - 2u(t)THTE + u(t)THTMH-1u(t)
= Ifd IL2 - 2T1 + T2 + T3 - 2T4 + 2T5 ,
where
T1 = v(t)TH-1E,
T2 = v(t)TH-1MH-1v(t),
T3 = E(t)TH-1MH-1E(t),
T4 = E(t)TH-1E,
T5 = E(t)TH-1MH-1v(t).
The proof for the test error is the most involved, but will follow a similar strategy of analysing each
term in the expansion. We first begin by analysing T2 in Section B.4.1, then T1 in Section B.4.2,
and finally terms T3, T4, T5, which are all simpler than the first two, in Section B.4.3. At the end of
this Appendix section we present the proof of Theorem 5.
B.4.1 TERM T2
As before, we will analyze each term separately. We begin with term T2 .
Proposition 1 (Term T2).
T2 = v(t)THTMH-1v(t) = ∣∣S≤'f≤'∣∣2 + 0d,p(1) ∙ kfdkL2+η
where we recall the shrinkage matrix S≤m defined in Eq. (20) and v(t) in Eq. (23).
22
Under review as a conference paper at ICLR 2022
Proofof Proposition 1. To analyze T2 We further decompose it into the following terms
T = (f≤' + f>')T(In - e-H∕n)H-1MHT(In - e-H∣n)(f<e + f>e)
=T21 + T22 + T23,
where
T21 二	二 fT'(In-	e-tH/n)H-1MH-1(In -	e-tH∕n)f≤',	(25)
T22 二	=2f2(In -	-e-tH/n)H-1MH-1(In -	-e-tH/n)f>',	(26)
T23 二	=f>'(In -	e-tH/n)H-1MH-1(In -	e-tH/n)f>.	(27)
Using Lemma 9 and 10 proven below, by the Cauchy-Schwarz inequality,
T22 ≤ (2T2lT23)1/2 = Od,p(1) ∙ kP≤'切 L2 kfd U
and hence
T2 = T2l + T22 + T23 = ∣∣S≤'f≤'ll2 + Od,p(1) ∙ IIfdllL2+η .
□
Lemma 9 (Term T2ι Eq.(25)).
T21 = ∣∣s≤ef≤e∣∣2 + Ogi) ∙∣p≤fd 隹
ProofofLemma 9. Recall the notation α and K≤ from Eq. (19). Define ∆ such that
∆ := (In - e-tH/n) - K≤'
which by Lemma 8 satisfies ∣∣∆∣∣c,P = Od,p(1). Then we can split T2ι into
T21 = T211 + T212 + T2l3 + T214
where
T211 = fT'K≤'Ψ≤'S≤ 'Ψ≤'K≤'f≤'∕n2,
T212 = fT'K≤'Φ'mS2mΦ]mK≤'f≤'∕n2,
T213 = f<iK≤e(H-1MH-1 - Ψ≤mS≤mΨ≤m∕n2)K≤'f≤',
T214 = fT'∆H-1MH-1K≤' f≤ + fT'K≤' H-1MH-1∆f≤ + f≤∆H-1MH-1∆f≤e.
We will show that the dominant term is T2n and the others are of lower order. By Lemma 3,
IlnH-1MH-1 - Ψ≤mS≤mΨ≤m∕n ∣ ∣ op = Od,p(1),
and since K≤i W In, T213 = Od,p(1) ∙ ∣∣P≤'fd∣∣L2. By Lemma 3, Lemma 4 and since S≤m W Im,
IlnH-1MH-1∣∣op ≤ ∣∣Ψ≤mS≤mΨ≤m∕n∣∣op + Od,p(1) ≤ 1 + Od,p(1)
hence it is easy to see that T214 = o√,p(1) ∙ ∣∣ P≤'fd∣∣L2. Turning to T212 We have
T212 ≤ n2fT'(In - ae-(t/n)L吗)Ψ'mΨTm(In - αe-Cψ≤'D2吗)f≤'
=3fT'(I - αe-En)BD≤')Ψ1'Ψ'mΨTmΨ≤'(I - αe-"BD≤' f
n2 ≤	≤	m	≤
≤ ∣∣f≤'∣L ∣∣ψ['ψ'm"∣∣0p = od,P(I) ∙ IIP≤'fd l∣L2 ,
where the first inequality used S'm W Im-', we used Lemma 2 in (a), and the last equality used
Lemma 4 (note that Ψ≤Ψ'm∕n is an off-diagonal block of Ψ≤mΨ≤m∕n which corresponds to an
all zeroes submatrix of‰). Finally we look at the main term T211. Defining the matrices
A := 1 Ψ≤je-("n)ψ≤'D≤'ψ≤'Ψ≤',
n≤
B := 1 Ψ≤'Ψ≤',
23
Under review as a conference paper at ICLR 2022
we can write
T211 =:也Ψ≤'(In - αeY∕*≤'D襄吗)Ψ≤'S≤'Ψ≤'(In - αeY∕*≤'D2吗)Ψ≤ef≤e
=fl'(BS≤`b - 2αAS≤eB + α2AS≤eA)jb≤e.
Now observe that by Lemma 2,
A = 1 ψ≤'eT"n)ψ≤'D≤cψ≤c Ψ≤' = BeTDAB
n ≤
and by the same argument used to show Eq. (22), ∣∣ Akcφ = 0d,p(1). Since B = In + Z and
S≤ W l`, we have
T2ιι = I I s≤ef≤'I I 2 + od,p⑴∙ ∣∣p≤'fd∣∣L2 ,
hence combining terms
T21 = T211 + T212 + T213 + T214 = llS≤'f≤'∣L + Od,p(1) ∙ ∣∣P≤'fd∣L2 .
□
Lemma 10 (Term T23 Eq. (27)).
T23 = fT'(In - eSn)H-1MHT(In- e-H/n)f>' = Od,p(1) ∙ k切"。.
ProofofLemma 10. Let US define the matrix
G = (In - e-H/n)H-1MHT(In - e-tH/n).
Using similar reasoning from Lemma 9 we can write
G = n12 K≤mΨ≤mS≤ mΦ≤mK≤m + ∆
=nK≤'ψ≤mS≤mψ≤mK≤' + Z ∙
for matrices ∆, △/ satisfying max{∣∣∆∣Lp , ∣∣∆ι∣θp} = 0d,p(1). We can split T⅛ into
T23 = - f>'Gf>'
n
=—(f>m + f'm)TG(f>m + f'm)
n
1	2	1
=—f>mGf>m +	f'mGf>m +	f'mGf'm
n	n	n
=T231 + t23,2 + T233 + T234
where
T231 =	二 n12 fLK≤mΨ≤mS≤mΨ%K≤mf>m,
T232	二 ^^2 fTnGf>m, n2 m
T233	二 n12 f*K≤'Ψ≤mS≤mΨ≤mK≤'f'm,
T234	二 od,P⑴∙ Il P>'fd∣∣L2 .
Let us first start with analysing T231, defining B = Ψ≤mΨ≤m∕n we have
T231 = 3fTmΨ≤m(Im - αe-("n)D≤mB )S≤m (Im - ae-(t/n)BD≤m )Ψ≤mf>
n2
(a)............T	t .	, 2
≤ (1 + od,P(1))f>mψ≤mψ≤mf>∣τ7n
=Od,p(1) ∙kP>m 切 L2 + η ,
24
Under review as a conference paper at ICLR 2022
where the first equality is by Lemma 2 and the last line follows from Lemma 7, Markov’s inequality,
and by the fact that m/n = od(1) by Assumption 2(c). To see that inequality (a) holds note that
∣∣Im - αe-(t/ngmB∣∣	= ∣∣B-1∕2(Im - ɑe-(t"nB'1/DmB1/)B叫
≤ B -1/2opB1/2op.
Since S≤2 m	Im,
∣∣(Im - αe"R≤mB)S≤m(Im - αeY"D≤m)∣∣°P ≤ 但/人 |田江=1 + θd,P⑴，
where the last equality is by Lemma 4. Now let us turn to T233 , which we can further split into
T233 = T2331 + T2332	(28)
where
T2331
T2332
n fTmK ≤' ψ≤'s≤ 'ψ≤'K≤' f'm,
n fT∩K≤'ψ'ms2mψTnK≤'f'm.
Redefining B = Ψ≤Ψ≤'∕n and using a similar argument as for T231, the first term can be seen as
T2331 = 3fTnΨTmΨ≤'(I - αeTt∕n)D≤'B)S≤'(I - αe-(t/n)B%)Ψ≤'Ψ'mfm
n2
≤ (1 + Od,p⑴)∣∣fm∣∣2 ∣∣ΨTmΨ≤'/n∣∣2p = Od,p⑴∙ kP'mfdkL2 .	(29)
Turning to the second term T2332, let
A := 1 ΨTm0 - e-(t加乂ψ≤'D≤'ψ≤'+κHIn))Ψ'm,
then we can write this term as
T2332 = fTm AS'mAfm
≤ fTmA2fm ≤ fTnAfm + Od,p(l) ∙ kP'mfdkL2,	(30)
where the last inequality holds since A W ΨTmΨ'm∕n = Im-` + ∆ by Lemma 4 and so
A2 = A1/2AA1/2
W A1/2(I + ∆)A1/2
=A + A1/2AA1/2
=A + ∆.
By the inequality 1 - e-x ≤ x, in the PSD order we see that
A W ^^2 ψTmψ≤'D≤'ψ≤'ψ'm + H ( — ψTnψ'm).
n2	m	n n m
Therefore from Eq. (30) we have
T2332 ≤ n2 fTnψTnψ≤'D≤ 'ψ≤'ψ'mf'm + fTn -H-	ψTm ψ'm) f'm + od,P ⑴，IIP'mfdkLs .
For the second term on the right, by Assumption 3(b) and by Lemma 4,
m )f⅛ = od,P⑴∙ kP'mfd∣∣L2 .
25
Under review as a conference paper at ICLR 2022
For the first term by Lemma 7 and Assumptions 3(b), 3(c),
nt2E[fbmψ]mΨ≤'D≤'Ψ≤'Ψ'mfem] ≤ (t∕n)C(η) ∣RmfdkL2+η Tr(D≤`)
=od,P⑴∙ ∣∣p'mfd∣∣L2+η ,
therefore by Markov’s inequality
T2332 = od,P⑴∙ ∣∣p'mfd∣∣L2+η .
Hence combining terms
T233 = T2331 + T2332 = od,P⑴∙ k P'mfdk L2+η ∙
By the Cauchy-Schwarz inequality
T232 = _2 fTnGf>m
n2
≤ 2( —2 f'mGf'm —2 fTmGf>m)
n2	n2
=2(T231 + od,P⑴∙ ∣∣p>'fd∣∣L2)	(T233 + od,P⑴∙ ∣∣P>'fd∣∣L2)
≤ od,P⑴∙ IIP>'fdkL2+η∣P >mfd ∣L2+η .
Putting everything together we see that
T23 = T231 + T232 + T233 + T234 = 0d,p(1) ∙ Ilfd∣∣L2+η .
□
B.4.2 TERM T1
Now we will analyse term T1 .
Proposition 2 (Term T1 ).
Ti = fT(In-e*n)HTE =k/2&仁 +。&再⑴∙ kfdkL2+η .
Proof of Proposition 2. We break T1 into the following terms
T1 = T11 + T12 + T13
where
Tii = f≤'(I-eTH/n)H-1E≤m,	(31)
T12 = fT'(I - eTH/n)H-1E≤m,	(32)
Ti3 = fT(I-e-tH/n)H -iE>m.	(33)
Using Lemma 11 and recalling Lemma 10,
T12 ≤ (T23)1/2f ≤m( = od,P⑴.IIP≤mfd∣∣L2 IIfdllL2+η ,
where T23 is as given in Eq. (27). From the analysis of Tii in Lemma 12 and Ti3 in Lemma 13 we
combine everything to get Proposition 2
Ti =卜∕2f≤'∣∣2+ 0d,P(1) ∙ kfdkL2+η .
□
Lemma 11. For a vector v ∈ Rn,
VTH-1E≤m ≤ (VTH-1MH-iv)i/2||fbcm||2
1	1	i/2
=(/vTψ≤ms≤mψ≤mv + od,P⑴./Wlb)	/'1 .
26
Under review as a conference paper at ICLR 2022
ProofofLemma 11. Recall that E≤m = Ψ<mD≤mf<m. By the Cauchy-Schwarz inequality,
VTH -1E≤m = VTH-1Ψ≤mD≤ mfb≤m,
≤ (VTH-1Ψ≤mD≤mΨ≤mHTV) 1/2 11 f≤m (,
≤ (vtHTMHTv)1[f≤m(
1/2
=(nɪVTψ≤mS≤mψ≤mv + od,P⑴∙ ∕lvl∣2)	f ≤m (,
where the last equality follows from Lemma 3.	□
Lemma 12 (Term T11 Eq. (31)).
T11 =卜≤/2 Ad2 + °d,P⑴∙ kP≤mfd岛.
Proofof Lemma 12. First note that by Lemma 8 for some ∆ such that ∣∣∆∣∣cιP = 0d,p(1),
T11 = T111 + T112
where
T111 = fT'(I - e-("n)(ψ≤'%ψ≤'+κHIn))H-1E≤m,
T112 = fT'∆H-1E≤m.
By Lemma 11,
T112 ≤ (∣∣△*(Hf/1/") ∣∣nHTMHTL) 1]∣f≤m( = /P⑴∙ ∣∣P≤mfd隹.
We now consider
T111 = T1111 - T1112
where
T1111 = f≤H-1E≤m
T1112 = αfTeeT"n)ψ≤'D≤'ψ≤'H-1E≤m.
Note that by Lemma 5,
∣∣Ψ≤'H-1Ψ≤mD≤m - [S≤'; 0]∣∣op = Od,p(1),
where 0 is a ' × (m - ') matrix of zeros. Hence for the first term Tnn,
T1111 = fT'Ψ≤' H-1ψ≤mD≤ mf≤m = 11 S≤/2 f≤'11 j + 为用⑴ /P≤m 力隹.
Define H≤ := Ψ≤'D≤`Ψ≤'. For the second term T1112, by Lemma 11
T1112 ≤ (S + Od,p(1) ∙∣∣P≤'力隹)1]∣f≤m∣∣2,	(34)
where
S = 3 fTee-("n)H≤' hmS^HmeT'/nE' f≤e∙
n2
Define a ：= 1 ψ≤e-("n)H≤'ψ≤'. WehaVe
S ≤ 3fTee-("n)H≤'Ψ≤mΨ<me-("n)H≤' f≤'
n2	m
=3 fTee-("n)H≤' 亚7亚”一⑵加- f≤ + g 也厂⑵田 Ψ'mψm e-("n)H≤' f≤
n2	n2	m
≤ JfT'e-(t/n)Hg亚人亚基一⑵加七 f≤ + ∣∣f≤'∣∣2 ∙ ∣∣Ψ['Ψ'm∕n喙
=f≤'A2f< + 0d,p(1) ∙∣∣P≤'力隹
=od,P(I) ∙ llp≤'fd∣∣L2,
27
Under review as a conference paper at ICLR 2022
since as noted before in Eq. (22), kAkop = od,P(1). Therefore
T11
T1111 + T1112 + T112 = ∣∣S≤/'2f≤'H2 + od,P(I) ∙ IlP≤mfd∣∣L2
□
Lemma 13 (Term T13 Eq. (33)).
T13 = od,P⑴∙ IlP>mfdkL2 IlfdIlL2 ∙
Proof of Lemma 13. We have
|T13| = |fT(I - e-tH/n)H-1E>m|
≤ IfI2∣∣H-1∣∣opIE>mI2
Note that we have E[kf k2] = n kfd∣∣L2. FurtherbyEq. (17), we have ∣∣H-1∣∣op ≤ 2∕κH with high
probability. Finally, recalling the definition of E>m from Eq. (16), we have
E[IE>m I2]
∞
n X入鼠fk ≤ n
k=m+1
max
k≥m+1
λ4d,k IP>m fd I2L2 .
As a result, we have
|T13| ≤ Od,P(1) -1^^/4^2 ||力卜2 [n2 max λd,kVlKH
k≥m+1
=Od,P ⑴∙ IIP >mfd∣∣ L2 llfdll L2 [n max λd,k] / X λd,k
k≥m+1
k≥m+1
=od,P⑴∙ IlP>mfdkL2 IlfdllL2 ,
where the last equality used Eq. (13) in Assumption 2(a).
□
B.4.3 TERMS T3, T4, T5
To analyse the terms T3 , T4, T5 we can adapt the corresponding steps for the proof of Theorem 4 in
Mei et al. (2021a). For the following analysis we recall the definition of ε(t) from Eq. (24).
Lemma 14 (Term T3).
T3 = ε(t)THTMH-1ε(t) = 0d,p(1) ∙ σ∣
Proof of Lemma 14.
σ12 Eε[T3] = Tr((In — e-tH/n)2H TMH T)
≤ Tr(HTMHT)
=Tr(Ψ≤mS≤ mΨ≤m∕n2) + 0d,p(1)
(b)	1
≤ n2 Tr(ψ≤mψ≤m) + od,P⑴
(c)	1
=n2nm(I + od,P(I)) + od,P(I) = od,P(I),
where (a) used Lemma 3, (b) used S≤m	Im, and (c) used Lemma 4 and Assumption 2(c). The
lemma then follows from Markov's inequality.	口
Lemma 15 (Term T4).
T4 = ε(t)THTE = (I) ∙ (σ2 + |旧店).
28
Under review as a conference paper at ICLR 2022
Proof of Lemma 15.
-1 Eε[T42] = -1 Eε[εT(I - e-tH/n)HTEETHT(I - e-tWn)ε]
σε2	4	σε2
= ETH -1(I- e-tH/n)2H -1E
≤ ETH -2E
Notice that M	Ψ≤LD≤4 LΨT≤L for any L ∈ N, by the decomposition of Eq. (16). Therefore
supD≤2LΨT≤LH -2Ψ≤LD≤2 Lop = sup H -1Ψ≤LD≤4LΨT≤LH -1op
L	L	(35)
≤ H -1MH -1op = od,P(1),
where the last inequality follows from Lemma 3. Hence,
ETH -2E = lim E≤TLH -2E≤TL
L→∞
(=a) lim fb≤TL[D≤2LΨT≤LH -2Ψ≤LD≤2L]fb≤L
L→∞
≤) limsup∣∣D≤lψ≤lh-2ψ≤LD≤L∣∣	∙ lim ∣∣f;Lll
L→∞	op L→∞
2
2
(c)	2
≤ od,P⑴∙ kfdkL2 ,
where (a) follows from the definition of E≤L, (b) follows from the definition of operator norm, and
(c) follows from Eq. (35). Therefore we get
T4 = 0d,p(1) ∙ σε ∙ kfdkL2 = Od,p(1) ∙ (σ2 + IlfdlIL2).
□
Lemma 16 (Term T5).
T = ε(t)THTMH-1v(t) = Od,p(1) ∙冠 + |族隹+。).
Proof of Lemma 16. We can write term T5 as
T5 = T51 + T52
where
T51 = ε(t)THTMHT(In - e-tH/n)f≤',
T52 = ε(t)THTMHT(In - e-tH/n)f>'.
Note as in Eq. (35), that by Lemma 3 and Lemma 4,
∣∣M1/2H-2M 1/2||o P = ∣∣H TMH-1∣∣op = 0d,p(1),
and taking the second moment of T51 yields
3Eε[T∣21] ≤ 3Eε[εTHTMHT(In - e-tH/n)f<'fT'(In - e-tH/n)HTMH-1ε]
σε2	51	σε2	≤
=f<'(In - e-tH/n)[H-1MH-1]2(In - e-tH")f≤'
≤ ∣∣M1/2H-2M/∣∣	∣∣M1/2HT(In - e-tH∕n)f≤'∣∣2
=0d,p(1) ∙ T2ι
=od,p(I) ∙ kp≤'fd∣∣L2,
where T21 is as given in Eq. (25). Similarly we get that
-EEε[T52] = od,P(I) ∙ T23 = od,P(I) ∙ kfd∣∣L2+η ,
σε
where T23 is as given in Eq. (27). By Markov’s inequality we deduce that
T5 = od,P(I) ∙ σε ∙ (IlP≤'fdkL2 + kfdkL2+η ) = od,P⑴∙ (σ2 + llfdkL2+η )∙
□
29
Under review as a conference paper at ICLR 2022
Finally putting Propositions 1, 2 and Lemmas 14, 15, 16 together for terms T2, T1, T3, T4 and T5
respectively leads to the proof of Theorem 5
Proof of Theorem 5.
R(ft) = kfdkL2 — 2Tι + T + T3 - 2T4 + 2T
=∣∣p>fdkL2+a(—2 卜 ≤'f≤42+卜 ≤'∕≤'∣∣2
+ od,P⑴ YkfdkL2 + ll/d∣∣L2+η + σ2)
= I(I- S≤')fτ≤'ll2 + IIP>'∕dkL2 + od,P⑴∙ (k∕dkL2 + ∣∣∕dkL2+η + σ2).
By Assumption 2(b), KH/n = θd(1) ∙ maxj≤' λd j hence
∣∣(I - S≤')f≤'∣∣2 = Od,p(1) ∙ kfdkL2
and as a result we obtain the first part of the theorem
R(ft) = kP>'fdkL2 + Od,p(1) ∙ (kfdkL2 + kfdkL2+η + σ2).	(36)
Now observe that similar to Eq. (15) we have the following decomposition
∣∣ft - P≤'fd∣∣; = kP≤'fdkL2 — 2u(t)TH-1E≤' + u(t)TH TMH-1u(t),
where u(t) is given in Eq. (14). Therefore we can write
∣∣ft - P≤'fd[2 = R(ft) -kP>'fdkL2 + 2u(t)TH-1E>'.	(37)
We now focus on the term
u(t)TH -1E>' = v(t)TH -1E>' + ε(t)TH-1E>'.
By choosing L = ` in the proof of Lemma 15, it follows that
ε(t)THTE≤' = — ∙ (σ2 + kP≤'fdkL2),
hence combining with the bound for T4 in Lemma 15 yields
ε(t)TH-1E>' = T4 - ε(t)TH-1E/
=Od,p(1) ∙ (σ2 + kfdkL2) - 0d,p(1) ∙ (σ2 + kP≤'fdkL2)
=Od,p(1) ∙ (σ2 + kfdkL2).
We will now show that
v(t)TH-1E>' = 0d,p(1) ∙ kP>'fd∣L2 kfd∣L2 .	(38)
If `(d) = m(d), then Eq. (38) follows from Lemma 13. Otherwise, consider the case `(d) = u(d).
Following similar logic to the proof of Lemma 13, because E[kf k22] = n kfdk2L2 and
∞
E[kE>uk2] = n X λd,kf'k ≤ nλd,u+1 kP>ufdkL2 ,
k=u+1
we also get Eq. (38) since
∣v(t)TH-1E>'∣ = IfT(I - e-tH/nH-1)E>u∣
≤ (t/n)kf k2∣∣(I - e-tH/n)(tH/n)-1∣∣oJ∣E>uk2
(a)
≤ Od,Ρ⑴∙ ∣∣P>ufd∣∣L2 ∣∣fdIlL2 tλd,u+1
=) Od,p(1) ∙kP>ufdkL2 kfdkL2 ,
where (a) used the inequality (1 - e-x)/x ≤ 1 and (b) used Assumption 3(a). Therefore
u(t)TH-1E>' = Od,p(1) ∙ (σ2 + ∣∣fdkL2), hence by Eq. (36) and Eq. (37) We obtain the final
part of the theorem
∣∣ft - P≤'fd∣∣22 = Od,p(1) ∙ (kfdkL2+η + σ2).	(39)
□
30
Under review as a conference paper at ICLR 2022
C DOT Product Kernels on Sd-1(√d)
C.1 Setting
We now apply our general theorems to the setting of dot product kernels on the sphere. Concretely
We take Xd = SdT (√d) and Vd = Unif(Sd-1(√d)) and consider dot product kernels Hd which
take the form of Eq. (4). Note that by Eq. (61) any dot product kernel hd can be decomposed as
hd(hxι, X2i∕d) = Ew〜Unif(Sd-i)[σd(hw, xιi)σ.({w, x2i)]
for some activation function σd . We state mild assumptions on σd and show that under these condi-
tions we can apply the results in Section A.3.
C.2 Assumptions
We state our assumptions on ◎& after some definitions. See Appendix G for additional background.
Denote by P≤' the orthogonal projection onto the subspace of L2(Sd-1(√d)) spanned by polyno-
mials of degree less than or equal to '. The projectors P' and P>' are defined analogously. Let us
emphasize that the projectors P≤' are related but distinct from the P≤m: while P≤' projects onto
the eigenspace of polynomials of degree at most `, P≤m projects onto the top m-eigenfunctions.
The assumptions given on the activations are the same as Assumption 3 of Mei et al. (2021a).
Assumption 4 (Assumptions for Dot Product Kernels at level s ∈ N). Let {Hd}d≥1 be a sequence
of dot product kernels with associated activation functions {σd}d≥1 as in Eq. (61). We assume the
following hold
(a)	There exists an integer k and constants ci < 1 and co > 0, such that ∣σd(x)∣ ≤
c0 exp c1x2/(4k) .
(b)	We have
minds-k ∣∣Pkσd(he, •川良=Ωd⑴，
k≤s
∣∣P>2s+1σd(he, ∙i)∣∣L2=Ωd⑴，
where e ∈ Sd-1 is a fixed vector (it is easy to see that these quantities do not depend on e).
Consider t(d), n(d) such that
dj+δ0 ≤ t ≤ dj+1-δ0,	ds+δ0 ≤ n ≤ ds+1-δ0
for some j, s ∈ N and δ0 > 0. We now verify that if {σd}d≥1 satisfies Assumption 4 at level s, then
for an appropriate choice of (u(d), m(d)) the conditions in Appendix A.2 are satisfied and lead to
Theorem 1. We set u(d) and m(d) to be the number of eigenvalues associated to spherical harmonics
of degree less than or equal to j and s respectively
js
u = X B(d, k) = Θd(dj ),	m = X B(d, k) = Θd(ds).
k=0	k=0
The verification of Assumption 1 (Kernel Concentration Property) and Assumption 2 (Eigenvalue
Condition) at level {(n(d), m(d)} is the same as the treatment in Theorem 2 of Mei et al. (2021a).
We only need to verify Assumption 3. To see part 3(a), note that 1∕λd u(d)= Θd(dj) and 1∕λd u(d)=
Θd(dj+1). For part 3(b), the condition holds because u(d) < m(d) for large dif and only ifj < s.
Assumption 3(c) is easily seen to hold since the trace of the kernel operator Tr(Hd) = Θd(1).
31
Under review as a conference paper at ICLR 2022
D Group Invariant Kernels on Sd-1(√d)
D.1 Setting
We now apply our general theorems to the setting of group invariant kernels on the sphere. Con-
cretely We take Xd = Sd-1 (√d) and Vd = Unif(Sd-1(√d)) and consider kernels Hd which take
the form of Eq. (5) for some function h. By Eq. (61), for some activation function σd
Hd(x1,x2)
Gd
Ex〜Unif(SdT) [σd(hx1, wi)σd(hx2,g ∙ w>)]∏d(dg).
(40)
We state mild assumptions on σd and show that under these conditions we can apply the results in
Appendix A.3. For additional technical background refer to Appendix G.
D.2 Assumptions
We will assume that σd = σ for all dand make the following assumptions on σ which are the same
as Assumption 1 in Mei et al. (2021b).
Assumption 5 (Assumption on Group Invariant Kernel at level s). Let {Hd}d≥1 be a sequence
of invariant kernels with associated activation functions σd = σ as in Eq. (40). We assume the
following conditions hold
(a)	For Gd = Cycd, we assume σ to be (s + 1) ∨ 3 differentiable and there exists constants
co > 0 and c1 < 1 such that ∣σ(k)∣ ≤ coec1u2/2 for any 2 ≤ k ≤ (S + 1) ∨ 3.
For general Gd, we assume that σ is a (finite degree) polynomial function.
(b)	The Hermite coefficients μk(σ) (c.f. Appendix) verify μk = 0 for any 0 ≤ k ≤ S.
(c)	We assume that σ is not a polynomial with degrees less than or equal to S.
Consider t(d), n(d) such that
dj+δ0 ≤ t ≤ dj+1-δ0,	ds-α+δ0 ≤ n ≤ ds-α+1-δ0
for some j, S ∈ N and δ0 > 0. We now verify that if σ satisfies Assumption 5 at level S, then the
conditions given in Appendix A.2 are satisfied for an appropriate choice of (u(d), m(d)) which leads
to Theorem 2. We set u and m to be the number of eigenvalues invariant polynomials of degree less
than or equal to j and S respectively
js
u = X D(d, k) = Θd(dj-α), m = X D(d, k) = Θd(ds-α),
where D(d, k) is the dimension of the subspace of invariant polynomials of degree k, c.f. Appendix
G.6. The verification of Assumption 1 (Kernel Concentration Property) and Assumption 2 (Eigen-
value Condition) at level {(n(d), m(d)} is exactly the same as in Theorem 1 in Mei et al. (2021b).
We must verify Assumption 3. To see part 3(a), note that 1∕λd u(d)= Θd(dj) and 1∕λdd u(d)=
Θd(dj+1). For part 3(b), the condition holds because u(d) < m(d) for large dif and only ifj < S in
which case
nd) Tr(Hd,m3))≤ d++δ0 θ(d-α)=O(d-2δ0 dj-s+1)=od(1).
Assumption 3(c) can be seen to hold from the fact that Tr Hd,>m(d) = Θ(ds-α) and
ms
X λ2d,j = X ξd2,k D(d, k) = Θ(Sd-α) = Θ(d-α)
j=0	k=0
from which it follows that for some constant C
m∞
Xλd2,j ≤CXλ2d,j.
j=0	j>m
32
Under review as a conference paper at ICLR 2022
E Auxiliary Results
E.1 Solution to Kernel Dynamics
Recall that we are interested in the following dynamics given in Eqs. (2), (3),
djtfθr(x) = E[Hd(x, z)(fd(z)- fθr(z))],
n
d1
dtft(x) = ~), Hd(X, xi)(yi - ft(Xi))
n i=1
With zero initialization for ≡ f ≡ 0. In this section We clarify the derivation, validity, and solution
of these dynamics. Let us consider the maps R : Hd → R and Rbn : Hd → R defined in Eq. (1).
R(f) =	(f (X) - fd(X))2dνd (X) + σε2,
Xd
1n
Rn(f) = n 工(f(χi) - yi)2.
First, we recall the definition of the Frechet derivative of a functional V : Hd → R at f. The Frechet
derivative DV(f) is the linear functional such that for g ∈ Hd,
lim	IV (f + g)-V(f) - DV ⑺⑼1 =0.
kgkHd →0	kgkHd
The gradient W(f) ∈ Hd is defined such that
(▽V (f ),giHd= DV (f )(g)
exists uniquely by the Riesz representation theorem. The gradients of the risk functionals are
▽R(f) = Hd(f - fd),
1n
▽Rbn(f) = -∑(f(Xi) -yi)Hχi,
n i=1
Where Hxi (X) := Hd(Xi, X) and Hd is the kernel operator as in Section A.1. A proof of this fact
is given in Proposition 2.1 of Yao et al. (2007). Taking ftor(x) as shorthand for for (t, x) Where
for(t, ∙) ∈ Hd is the oracle model at time t and similarly for ft(x), the following gradient flows
With zero initialization are Well-defined for t ≥ 0,
d∣fθr = -VR(fOr) = -Hd(fOr - fd) = Ez[Hd(∙, z)(fd(z) - fθr(z)]
(41)
nn
d-	-
Xtft = -▽Rn(ft) = -- E(ft(xi) - y )HXi = - EHd(∙, xi)(yi - ft(Xi)).	(42)
n i=1	n i=1
The oracle model ODE Eq. (41) is simply a linear differential equation which has the following
∞
solution involving the operator exponential exp(A) := P Ak/k!
k=0
ftor = fd + exp(-tHd)(f0or - fd) = fd - exp(-tHd)fd.	(43)
For the empirical model ODE Eq. (42) we first consider the system of scalar differential equations
induced at the points {(Xi,yi)}i∈[n]. Letting u(t) = (ft(X1), . . . ,ft(Xn))T, y = (y1, . . . , yn)T,
andH = (Hd(Xi, Xj))i,j∈[n] we have
不u(t) =	H(u(t) - y),
dt	n
with initial condition u(0) = 0. As this a linear ODE, the solution is given by
u(t) = y + e-tH/n(u(0) -y) = (In - e-tH/n)y.	(44)
33
Under review as a conference paper at ICLR 2022
For x ∈ Rd, define h(x) = (Hd(x, x1), . . . , Hd (x, xn))T ∈ Rn. Let a(t) = H-1u(t) ∈ Rn. We
will show that the function ft(∙) := (h(∙), a(t)i ∈ Hd, which satisfies (ft(xι),..., ft(xn)) T =
u(t), satisfies the following equation
-dft(x) = 1 hh(x), y - u(t)i = 1 hh(x),e-tH/ny'i,
dt	n	n
which is Eq. (42) at point x. Indeed, by the chain rule
dd
dtft(x) = dt hh(x), a(t)i
=hh(X), ddt a())
=hh(x), HT1He-tH/nyi
n
=1 hh(x),e-tH/nyi
n
which is what we wanted to show.
E.2 Equivalence between Invariant Kernels and Data Augmentation
In this section we will show an equivalence between the (time rescaled) gradient flows for training
invariant kernels and using an augmented dataset. Specifically consider a group G and a kernel H
that is G-equivariant, that is
H(g ∙ Xι,g ∙ X2) = H(xi, X2) ∀g ∈ G, ∀X1, X2 ∈ X.
Given a G-equivariant kernel H, we define a G-invariant kernel Hinv as the group averaged kernel
Hinv = I H(Xl,g ∙ X2)∏(dg)
G
for the Haar measure π on G (c.f. Eq. (5)). Note that any dot product kernel is G-equivariant for G a
subgroup the orthogonal group e.g. the cyclic group Cyc (c.f. Section 3.3).
Given a dataset (X, y) = {(Xi , yi ) : i ∈ [n]} consider the augmented dataset
(XG,yg) = {(g ∙ Xi,yi): g ∈ G,i ∈ [叫.
We consider the (rescaled c.f. Remark 5) empirical dynamics Eq. (3) of the gradient flow on (X, y)
using Hinv which we denote ft,inv
d	mn
击ft,inv(x) = -mVRn(ft,inv) = -— f(ft,inv(Xi) - yi)Hinv(Xi, X)
ni=1
i , 1	∙ ∙ i 1	∙	i' .ι	1 ∙	. n	/ -WT-	∖	∙ ττ ι ∙ ι	F ,
and the empirical dynamics of the gradient flow on (XG , yG) using H which we denote ft,aug
dt ft,aug(x) = - n〉： X ：(ft,aug(g ∙ Xi) - yi)H(g ∙ xi, X)∙
Proposition 3. Let G be a finite group with m elements. Given a G-equivariant kernel H, if π is the
uniform measure on G then


ft,inv ≡ ft,aug ,	∀t ≥ 0.
Proof. Let G = {g1, . . . , gm} where g1 is the identity. Define the output vectors
Tn
uinv (t) := (ft,inv (X1 ), . . . , ft,inv (Xn )) ∈ R ,


Ug ⑴ := (ft,aug (g ∙ XI),...,ft,aug (g ∙
xn))T ∈ Rn,
uaug(t) := (ug1 (t),. ..,ugm(t))T ∈ Rmn
34
Under review as a conference paper at ICLR 2022
Furthermore, define the kernel matrices
Hg,g0 := [H(g ∙ Xi,g0 ∙ Xj)]i,j∈[n] ∈ Rn×n for g,g0 ∈ G,
Haug:= [Hg,g0]g,g0∈G ∈ Rmn×mn,
Hinv = [Hinv(xi, xj )]i,j∈[n] .
m
Note that by definition Hinv = ml P Hgi gj. We will show that
j=1
uaug (t) = (uinv (t), uinv (t), . . . , uinv (t)) for all t ≥ 0.
. .	. . . 一 .	. . .	. . _	O	O
(45)
From this the result follows by Theorem 4.1 in Li et al. (2019) since ft,inv, ft,aug are given by kernel
regressions with targets uinv(t), uaug(t) and kernels Hinv, H respectively.
By Eq. (42), we can write
uinv (t) = (I - exp(-tmHinv /n))y ,
uaug (t) = (I - exp(-tHaug/n))yG ,
where yG = (y, . . . , y) ∈ Rmn . By expanding the matrix exponential series and using linearity, it
suffices to show that
HakugyG = (mkHiknvy, mkHiknvy,..., mkHiknvy) for all k ∈ N.
in order to show Eq. (45) holds. We prove the above by induction on k. For k = 1, observe that
(mHinvy)im=1
where the second equality follows from G-equivariance of H. Assume the inductive hypothesis
holds for k. Then
Haku+g1yG = HaugHakugyG
/ "" V
=	mk	Hgi ,gj Hiknvy
j=1	i=1
(m	/ m	∖k ∖m
=XHgi,gj XHgi,gj0	y∣
j=1	j0=1	i=1
TX Hgj	y∖
i=1
= (mk+1Hikn+v1y)im=1
where the second equality applies the induction hypothesis and the third equality uses equivariance.
Thus the inductive claim is proved and the proof is complete.
□
Remark 5. The scaling factor m in the gradient flow for ft,inv, leads to a natural comparison with
ft,aug as elaborated in Appendix E.3. As argued in that section, in the gradient descent discretiza-
tion, it is natural to take a step-size inversely proportional to the maximum kernel eigenvalue. In the
case of high-dimensional invariant kernels, note that
λmax(Haug) = mλmax(H)
〜mλmaχ(Hinv)
hence the step-size for the invariant kernel flow should be m times larger.
35
Under review as a conference paper at ICLR 2022
E.3 Discretizing Time
Comparing different “speeds” of optimization algorithms only makes sense for discrete-time algo-
rithms. Consider the following gradient descent dynamics with step-size η, obtained as the dis-
cretization of the empirical gradient flow Eq. (42)
1n
1
fk+ι = fk - ηvRbn(fk ) = fk - ηn 工(fk (Xi)- yi)H (∙, χi),	k = 0,1,...	(46)
We will argue that it is natural to take η 〜n∕λmaχ(H) where H is the kernel matrix.
Define the sampling operator S : Hd → Rn by S(f) = (f (xi))n=ι ∈ Rn and let S* : Rn → Hd
n
be its adjoint, defined by S* (y) = n P yiHχi (see Yao et al. (2007) Appendix B for more details).
i=1
Then we can rewrite the gradient descent equation Eq. (46) as
^ ^ ^ ,
fk+1 = fk - η(S*Sft- S*y), k = 0,1,…	(47)
Let T = S*S and define H := SS* = nH to be the normalized kernel matrix. Let
一1
b := S* H y ∈ Hd and note that since Tb = S* y, we can rewrite Eq. (47) as
O	O	_ , O _、	,
fk+1 = fk - ηT(fk - b),	k = 0, 1,...	(48)
Denote the eigenvalues of H as λι ≥ λ2 ≥ ... ≥ λn > 0. By the spectral theorem, there exists
n
a set of orthornomal eigenvectors φ1, . . . , φn ∈ Hd such that T = P λiφiφi*. Define αi (k) =
i=1
hfk - b, φii ∈ R. By taking Eq. (48) then subtracting b and taking the inner product with φi on both
sides, we get the coordinate evolution equations for i = 1, . . . , n
，-	.	.,	_，个	-、	..
αi(k + I)= h(Id - ητ)(fk - b), φii
..^ . .
=h(fk - b), (id - ηT)φii
= (1 - ηλi)αi(k)
where the second equality holds since T is self-adjoint and the last equality is since φi is an eigen-
vector of T. It is easy to see that αi (k) = (1 - ηλi)kαi (0). Therefore we see that gradient descent
Eq. (46) is guaranteed to converge if η < 1∕(2λ1) and may not otherwise. Therefore it is natural to
choose the step-size η to scale asymptotically as η 〜1∕λmaχ(H).
For a dot product kernel H and its corresponding invariant kernel Hinv the kernel matrices have
operator norms of the same order
λmax(H) 〜λmax(Hinv)
hence no time rescaling is need to compare the corresponding optimization speeds asymptotically.
E.4 Similarities with Empirical Phenomena
In this section we elaborate upon Remark 4 and mention some connections with empirical observa-
tions in Nakkiran et al. (2020). Although the metric in our setting is the squared loss, we can still
observe three stages in classification problems when measuring the soft error. In Fig. 6a taken from
Nakkiran et al. (2020) we can observe stage 1 and stage 2. Either training has not continued long
enough to observe stage 3 or n is large enough so that the models have converged to the approxima-
tion error of the neural network class (c.f. Remark 1). In Fig. 6b taken from Nakkiran et al. (2020),
although the train errors are not plotted, by extrapolating from Fig. 6a, presumably for each n stage
1 and stage 2 occur. From the dimmer curves in Fig. 6b we can see that for n < 50000 stage 3
occurs as well.
In Fig. 7a, we see a parallel between the use of cyclic versus dot product kernels and the use data
augmentation versus not for a Resnet-18 trained on CIFAR-5m (note that using a cyclic kernel is
equivalent to using a dot product kernel with data-augmentation c.f. Appendix E.2). In both our
theoretical results and in the empirical results of Nakkiran et al. (2020) we observe that the ideal
world optimization speed of augmented and non-augmented training are the same, but for augmented
training the real world training speed is slowed down, eventually leading to better generalization for
long enough training.
36
Under review as a conference paper at ICLR 2022
Figure 6: Soft-error curves for Resnet-18 trained on CIFAR-5m taken from ref. Nakkiran et al.
(2020). Panel (6a): n = 5 × 104 (Fig. 6 in ref). Panel (6b) Varying n (Fig. 4a in ref).
(a)
(b)
C C Real vs Ideal: Effect of Data Aug
0.9-------------------------------------------
0.8
1.0
0.8
.765432r-Io
0.0.0.0,0.0.0.0.
—No Data Aug
-With Data Aug
---Ideal Worlds
Train
0	5000	10000	15000	20000
SGD Iterations
(a)	(b)
Figure 7: Panel (7a): Data-augmentation for Resnet-18 on CIFAR-5m. (Fig. 5a from Nakkiran et al.
(2020)). Panel (7b): Cyclic versus dot product kernel (Fig. 5c from this work)
37
Under review as a conference paper at ICLR 2022
F	Additional Figures
To see the effects of varying the dimension d, in Fig. 8 we replicate the log-scale plots of kernel gra-
dient flow with dot product kernels from Fig. 4. We take n = d1.5 and vary d ∈ {50, 100, 200, 400}.
Each plot is averaged over 10 runs with the shaded region representing one standard deviation around
the mean. We can see that as d increasing the standard deviation decreases and the curves approach
the theoretical high-dimensional prediction.
—Empirical lest
——Empirical Train
1.2
1.0
0.8
⅛ 0.6
"04
0.2
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
log t/log d
(b) d = 50
1.2
1.0
0.8
3 0,6
0.4
0.2
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
log t/log d
(c) d = 50
0.0 0.5 1.0 1.5 2.0 2.5 3.0
log t/log d
(a) d = 50
(d) d = 100
0.5	1.0	1.5 2.0 2.5	3.0
log t√logd
(g) d = 200
(e) d = 100
1.2
1.0
0.8
⅛ 0.6
0.4
0.2
0.0
(h) d = 200
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
log t/log d
(f) d = 100
Iogt/Iogd
(i) d = 200
(j) d = 400
(k) d = 400
(l) d = 400
Figure 8: Each column replicates a kernel gradient flow experiment from Fig. 4 over d ∈
{50, 100, 200, 400}. Column 1: Replicates Fig. 4a Column 2: Replicates Fig. 4a but with σε2 = 0.2.
Column 3: Replicates Fig. 4b. Averaged over 10 trials.
38
Under review as a conference paper at ICLR 2022
G Technical background
G. 1 Notations
For a positive integer, we denote by [n] the set {1, 2, . . . , n}. For vectors u, v ∈ Rd, we denote
hu, vi = u1v1 + . . . + udvd their scalar product, and kuk2 = hu, ui1/2 the `2 norm. Given a
matrix A ∈ Rn×m, we denote kAkop = maxkuk2=1 kAuk2 its operator norm and by kAkF =
(Pij A2j) 1/2 its FrobeniUs norm. If A ∈ Rn×n is a square matrix, the trace of A is denoted by
Tr(A,)=Pi∈[n]Aii.
We use Od( ∙) (resp. od( ∙)) for the standard big-O (resp. little-o) relations, where the subscript d
emphasizes the asymptotic variable. Furthermore, We write f = Ωd(g) if g(d) = Od(f (d)), and
f = ωd(g) if g(d) = od(f (d)). Finally, f = Θd(g) if we have both f = Od(g) and f = Ωd(g).
We use Od,p( ∙) (resp. θd,p( ∙)) the big-O (resp. little-o) in probability relations. Namely, for h1(d)
and h2(d) two sequences of random variables, h1(d) = Od,P(h2(d)) if for any ε > 0, there exists
Cε > 0 and dε ∈ Z>0, such that
P(|h1(d)/h2(d)| > Cε) ≤ε,	∀d≥ dε,
and respectively: h1(d) = od,P(h2(d)), if h1(d)/h2(d) converges to 0 in probability. Similarly, we
will denote h1(d) = Ωd,p(h2(d)) if h2(d) = Od,p(h1(d)), and h1(d) = ωd,p(h2(d)) if h2(d)=
od,P(h1(d)). Finally, h1(d) = Θd,P(h2(d)) if we have both h1(d) = Od,P(h2(d)) and h1(d) =
Ωd,p(h2(d)).
G.2 Functional spaces over the sphere
For d ≥ 3, we let Sd-1(r) = {x ∈ Rd : kxk2 = r} denote the sphere with radius r in Rd.
We will mostly work with the sphere of radius √d, Sd-1( √d) and will denote by Td the uniform
probability measure on Sd-1(√d). All functions in this section are assumed to be elements of
L2(Sd-1( √d), Td), with scalar product and norm denoted as h ∙, ∙∖l and ∣∣ ∙ ∣∣l2 :
hf, giL2 ≡
/
7sd-1(√d)
f(x) g(x) τd(dx) .
(49)
For ' ∈ Z≥o, let Vd,' be the space of homogeneous harmonic polynomials of degree ' on Rd (i.e.
homogeneous polynomials q(x) satisfying ∆q(x) = 0), and denote by Vd,' the linear space of
functions obtained by restricting the polynomials in Vd,' to Sd-1( √d). With these definitions, we
have the following orthogonal decomposition
∞
L2(Sd-1(√d),Td) = M Vd,' .	(50)
'=0
The dimension of each subspace is given by
dim(Vd,') = B(d,')=2'+-d- 2 (' + d - 3).	(51)
For each ` ∈ Z≥0, the spherical harmonics {Y'(,dj)}1≤j≤B(d,') form an orthonormal basis of Vd,':
hYk(id),Ys(jd)iL2=δijδks.
Note that our convention is different from the more standard one, that defines the spherical harmonics
as functions on Sd-1(1). It is immediate to pass from one convention to the other by a simple scaling.
We will drop the superscript d and write Y',j = Y'(,dj) whenever clear from the context.
We denote by Pk the orthogonal projections to Vd,k in L2 (Sd-1 (√d), Td). This can be written in
terms of spherical harmonics as
B(d,k)
Pkf(x) ≡ X hf,Yki>L2Yki(x).	(52)
l=1
We also define P≤' ≡ P：=。Pk, P>' ≡ I — P≤' = P∞='+1 Pk, and P<' ≡ P≤'-1, P≥' ≡ P>'-1.
39
Under review as a conference paper at ICLR 2022
G.3 Gegenbauer polynomials
The '-th Gegenbauer polynomial Q'd) is a polynomial of degree '. Consistently with our convention
for spherical harmonics, we view Qf) as a function Q'd) : [-d, d] → R. The set {Q'd)}'≥o forms
an orthogonal basis on L2([-d, d],T(I), where Td is the distribution of √dhx, e。when X 〜 t(,
satisfying the normalization condition:
hQk ∖√dheι, ∙i), Qj ∖√dheι, ∙i)iL2(sd-ι(√d)) = B(d k) δjk .
(53)
In particular, these polynomials are normalized so that Q'd)(d) = 1. As above, We will omit the
superscript (d) in Q'd) when clear from the context.
Gegenbauer polynomials are directly related to spherical harmonics as follows. Fix V ∈ SdT (√d)
and consider the subspace of 匕 formed by all functions that are invariant under rotations in Rd that
keep v unchanged. It is not hard to see that this subspace has dimension one, and coincides with the
span of the function Q'd) (hv, •)).
We will use the following properties of Gegenbauer polynomials
1.	For x, y ∈ Sd-1(√d)
hQjd)(hχ, ∙i),Qkd)(hy, ∙i)iL2 = Bdiy δj (Qd(χ yi).
(54)
2.	For x, y ∈ Sd-1(√d)
1	B(d,k)
Qkd)(hx,yi) = Bdiy X YId)(XMid)(y).
(55)
These properties imply that, up to a constant, Q(kd)(hx, yi) is a representation of the projector onto
the subspace of degree-k spherical harmonics
(Pkf )(x) = B(d,k) I	Qkd(hx, yi) f(y)τd(dy).
JSdT(√d)
(56)
For a function σ ∈ L2 ([-√d, √d], τj) (where T(I is the distribution of he1, Xi when X
Unif(Sd-1(√d))), denoting its spherical harmonics coefficients ξd,k(σ) to be
ξd,k (σ) = /	σ(x)Qkd (√d,x)τ1(dx),
J[-√d,√d]
then we have the following equation holds in L2([-√d, √d], τd1) sense
∞
σ(x) = X ξd,k (σ)B(d,k)Qkd)(√dχ).
k=0
(57)
For any rotationally invariant kernel Hd(x1, x2)
hd(hxι, x2i∕d), with hd(√d ∙)	∈
L2([-√d, √d], τd1), we can associate a self adjoint operator Hd : L2(Sd-1(√d)) → L2(Sd-1(√d))
〜
Hdf (x) ≡ [	hd(hx, xii/d) f (xι) Td(dx1).
JSd-1(√d)
(58)
By rotational invariance, the space Vk of homogeneous polynomials of degree k is an eigenspace
of Hd, and we will denote the corresponding eigenvalue by ξd,k (hd). In other words Hdf (x) ≡
P∞=o ξd,k(hd)Pkf. The eigenvalues can be computed via
ξd,k(hd)
(59)
40
Under review as a conference paper at ICLR 2022
For a dot product kernel Hd(x, y) = hd(hx, y〉/d) consider the GegenbaUer expansion of hd in
L2([-√d, √d],τj)
∞
hd(hx, yi/d) = X ξk,d(hd)B(d, k)Q(kd)(hx, yi).	(60)
k=0
Using Eq. (54) we can equivalently write the kernel as an expectation over random features for some
activation σd
hd(hx, yi/d) = Ew〜Unif(Sd-i)[σd(hw, x>)σd((w, y))]	(61)
by taking
∞
σd(x) = X ξd,k (hd)^1 / 2 B (d,k)Q,d (√dx).	(62)
k=0
Note that σd ∈ L2([-√d, √d], τ1) as long as h(1) < ∞.
G. 4 Hermite polynomials
The Hermite polynomials {Hek}k≥0 form an orthogonal basis of L2 (R, γ), where γ(dx) =
e-x /2dχ/λ∕2∏ is the standard Gaussian measure, and Hek has degree k. We will follow the classi-
cal normalization (here and below, expectation is with respect to G 〜N(0,1)):
E{Hej(G)Hek(G)} = k! δjk .	(63)
As a consequence, for any function g ∈ L2 (R, γ), we have the decomposition
g(x) =X μ⅛g2Hek(X),
k=0
μk(g) ≡ E{g(G)Hek(G)}.
(64)
The Hermite polynomials can be obtained as high-dimensional limits of the Gegenbauer polyno-
mials introduced in the previous section. Indeed, the Gegenbauer polynomials (up to a √d scaling
in domain) are constructed by Gram-Schmidt orthogonalization of the monomials {xk}k≥0 with
respect to the measure T(I, while Hermite polynomial are obtained by Gram-Schmidt orthogonaliza-
tion with respect to γ. Since Td ⇒ Y (here ⇒ denotes weak convergence), it is immediate to show
that, for any fixed integer k,
lim Coeff {Qkd)(√dx) B(d,k)1/2} = Coeff ∣-1172 Hek (x)).
d→∞	(k!)1/2
(65)
Here and below, for P a polynomial, Coeff{P (x)} is the vector of the coefficients of P . As a
consequence, for any fixed integer k, we have
μk(σ) = lim ξd,k(σ)(B(d,k)k!)1/2,	(66)
d→
where μk (σ) and ξd,k(σ) are given in Eq. (64) and Eq. (57).
G.5 The invariant function class and the symmetrization operator
Let Gd be a group that is isomorphic to a subgroup of O(d), the orthogonal group in d dimension.
That means, each element of Gd can be identified with a matrix in O(d) ⊆ Rd×d, and the group
addition operation in Gd can be regarded as matrix multiplications in O(d). For any X ∈ Sd-1 (√d)
and g ∈ Gd, we define group action g ∙ X to be the multiplication of matrix representation of g
with the vector x. We equip Gd with a probability measure πd, which is the uniform probability
measure on Gd . More specifically, the Borel sigma algebra on Gd is defined as the Borel sigma
algebra of O(d) restricted on Gd. The uniform probability measure πd satisfies the property that, for
any Borel-measurable set B ⊆ Gd and any g ∈ Gd, we have
πd(B) = πd(gB).
41
Under review as a conference paper at ICLR 2022
Let L2(Sd-1(√d)) be the class of L2 functions on Sd-1(√d) equipped with uniform probability
measure Unif(Sd-1(√d)). We define the invariant function class to be
L2(Sd-1(√d), Gd)	=	{f	∈	L2(Sd-1(√d))	:	f (x) =	f(g	∙x),	∀x	∈ Sd-1(√d),	∀g	∈	Gd}.
We define the symmetrization operator S : L2(Sd-1 (√d)) → L2(Sd-1(√d), Gd) to be
(Sf)(x)= Gd
f(g ∙ x)∏d(dg).
G.6 Orthogonal polynomials on invariant function class
We define Vd,≤k ⊆ L2(Sd-1(√d)) to be the subspace spanned by all the degree ' polynomials,
Vd,>k ≡ V⊥≤k ⊆ L2(Sd-1(√d)) to be the orthogonal complement of Vd,≤k, and Vd,k = Vd,≤k ∩
Vd⊥,≤k-1. In words, Vd,k contains all degree k polynomials that orthogonal to all polynomials of
degree at most k - 1. We further define Vd,<k = Vd,≤k-1 and Vd,≥k = Vd,>k-1.
Let P≤' to be the projection operator on L2(Sd-1(√d), Unif) that project a function onto Vd,≤',
the space spanned by all the degree ' polynomials. Then it is easy to see that P≤' and S operator
commute. This means, for any f ∈ L2(Sd-1(√d)), We have
P≤'[S (f )] = S [P≤'(f)].
Similarly, we can define p`, P<', P>', P≥', which commute with S. We denote Vd,'(Gd) ≡
P'(Sd-1(√d), Gd) to be the space of polynomials in the images of P'S. Then we have
P'(Ad, Gd) = P'(L2(Sd-1(√d), Gd)) = S [P'(L2(Ad))].
We denote D(d; k) = D(SdT(√d); gd； k) ≡ dim(Pk(SdT(√d), Gd)) to be the dimension of
Pk(SdT(√d), Gd). We denote {Ykd)}ι∈[D(sd-i(√d>k)] to be a set of orthonormal polynomial basis
in Pk(Sd-1(√d), Gd). Thatmeans
Ex~Unif(Sd-1(√d))[γkill (X)Yk2l2 (X)] = 1{k1 = k2,l1 = l2},
and
Ykd)(χ) = Ykd)(g ∙χ), ∀X ∈ Sd-1(√d), ∀g ∈Gd.
42