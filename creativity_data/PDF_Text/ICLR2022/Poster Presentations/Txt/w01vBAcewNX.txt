Published as a conference paper at ICLR 2022
On Covariate Shift of Latent Confounders
in Imitation and Reinforcement Learning
Guy Tennenholtz
Nvidia Research
Technion
Shie Mannor
Nvidia Research
Technion
Assaf Hallak
Nvidia Research
Gal Chechik
Nvidia Research
Gal Dalal
Nvidia Research
Uri Shalit
Technion
Ab stract
We consider the problem of using expert data with unobserved confounders for
imitation and reinforcement learning. We begin by defining the problem of learn-
ing from confounded expert data in a contextual MDP setup. We analyze the
limitations of learning from such data with and without external reward, and pro-
pose an adjustment of standard imitation learning algorithms to fit this setup. We
then discuss the problem of distribution shift between the expert data and the on-
line environment when the data is only partially observable. We prove possibility
and impossibility results for imitation learning under arbitrary distribution shift of
the missing covariates. When additional external reward is provided, we propose
a sampling procedure that addresses the unknown shift and prove convergence to
an optimal solution. Finally, we validate our claims empirically on challenging
assistive healthcare and recommender system simulation tasks.
1	Introduction
Reinforcement Learning (RL) is increasingly used across many fields to create agents that learn via
interaction and reward feedback. Imitation Learning (IL, Hussein et al. (2017)) is concerned with
learning via expert demonstrations without access to a reward function. Similarly, RL settings often
use expert data to boost performance, eliminating the need to learn from scratch. In this work we
consider the IL and RL paradigms in the presence of partially observable expert data.
While expert demonstration data is useful, in many realistic settings such data may be prone to
hidden confounding (Gottesman et al., 2019), i.e., there may be features used by the expert which
are not observed by the learning agent. This can occur due to, e.g., privacy constraints, continually
changing features in ongoing production pipelines, or when not all information available to the
human expert was recorded. As we show in our work, covariate shift of unobserved factors between
the expert data and the real world may lead to significant negative impact on performance, frequently
rendering the data useless for imitation (see Figure 1 and Theorem 2).
In this paper we define the tasks of imitation and reinforcement learning using expert data with un-
observed confounders and possible covariate shift. We focus on a contextual MDP setting, where a
context is sampled at every episode from some distribution, affecting both the reward and the transi-
tion between states. We assume that the agent has access to additional expert data, generated by an
optimal policy, for which the sampled context is missing, yet is observed in the online environment.
We begin by analyzing the imitation-learning problem, (i.e., without access to reward) in Section 3.
Under no covariate shift in the unobserved context, we characterize a sufficient and necessary set of
optimal policies. In contrast, we prove that in the presence of a covariate shift, if the true reward
depends on the context, then the imitation-learning problem is non-identifiable and prone to catas-
trophic errors (see Section 3.2 and Theorem 2). We further analyze the RL setting (i.e., with access
to reward and confounded expert data) in Section 4. Figure 1 depicts a possible failure case of using
confounded expert data with unknown covariate shift in a dressing task. Unlike the imitation setting,
1
Published as a conference paper at ICLR 2022
Figure 1: Failure of using confounded expert data under context distribution mismatch between online envi-
ronment and expert data. Caregiver does not learn to perform well in a dressing task when covariate shift of
hidden confounders is present but not accounted for.
we show that optimality can still be achieved in the RL setting while using confounded expert data
with arbitrary covariate shift. We use a corrective data sampling procedure and prove convergence
to an optimal policy.
Our contributions are as follows. (1) We introduce IL and RL with hidden confounding and prove
fundamental characteristics w.r.t. covariate shift and the feasibility of imitation. (2) In the RL
setting, under arbitrary covariate shift, we provide a novel algorithm with convergence guarantees
which uses a corrective sampling technique to account for the unknown context distribution in the
expert data. (3) Finally, we conduct extensive experiments on recommender system (Ie et al., 2019)
and assistive-healthcare (Erickson et al., 2020) environments, demonstrating our theoretical results,
and suggesting that confounded expert data can be used in a controlled manner to improve the
efficiency and performance of RL agents.
2	Preliminaries
Online Environment. We consider a contextual MDP (Hallak
et al., 2015) defined by the tuple M = (S, X, A, P, r, ρo, ν, γ),
where S is the state space, X is the context space, A is the ac-
tion space, P : S × S × A × X 7→ [0, 1] is the context dependent
transition kernel, r : S × A × X 7→ [0, 1] is the context dependent
reward function, and γ ∈ (0, 1) is the discount factor. We assume
an initial distribution over contexts ρo : X 7→ [0, 1] and an initial
state distribution ν : S × X 7→ [0, 1].
The environment initializes at some context X 〜ρ°(∙), and state
so 〜V(∙∣x). At time t, the environment is at state St ∈ S and an
agent selects an action at ∈ A. The agent receives a reward rt =
r(st, at, x) and the environment then transitions to state st+ι 〜
P (∙∣St,at,x).
We define a Markovian stationary policy π as a mapping
∏ : S ×X ×A→ [0,1], such that ∏(∙∣s, x) is the action sampling
probability. We define the value of a policy π by vM (π) =
En[(1 — Y) P∞=0 Ytr(St,at,x) | X 〜po,so 〜V(∙ | x)], where
Figure 2: Causal Diagram Con-
textual MDP
Eπ denotes the expectation induced by the policy π. We denote by Π the set of all Markovian
policies and Πdet the set of deterministic Markovian policies. We define the optimal value and policy
by VM = max∏∈∏ vm(∏), and ∏M ∈ argmax∏∈∏ vm(∏), respectively. Whenever appropriate,
We simplify notation and write v*,∏*. We use ∏M to denote the set of optimal policies in M, i.e.,
∏M = arg max∏∈∏ vm(∏). We also define the set of catastrophic policies ∏M as the set
∏M = arg min vm(∏).
π∈Π
(1)
2
Published as a conference paper at ICLR 2022
We later use this set to show impossibility of imitation under arbitrary covariate shift and a context-
independent transition function.
Expert Data with Unobserved Confounders. We assume additional access to a con-
founded dataset consisting of expert trajectories D* = {(s0,a0,s1,a1,...,sH,a})}n=1,
where aj 〜 ∏* ∈ ∏M.	The trajectories in the dataset were sampled i.i.d.
from the marginalized expert distribution (under possible context covariate shift)
P * (s0,a0, si, aι,..., SH) = Px Pe(X)V (s0∣x) QH-I P (st+ι∣st,at,x)∏* (at∣st,x), where Pe
is some distribution over contexts. Importantly, PeGoeS not necessarily equal Po - the distribution
of contexts in the online environment. Notice that it is assumed that π* that generated the data had
access to the context xi (i.e., π* is context-dependent), though it is missing in the data.
In this work, we consider two settings:
1.	Confounded Imitation Learning (Section 3): The agent has access to confounded ex-
pert data (with context distribution Pe) as well as real environment (S, X, A, P, Po, ν, γ),
without access to reward.
2.	Reinforcement Learning with Confounded Expert Data (Section 4): The agent has ac-
cess to confounded expert data (with context distribution Pe) as well as real environment
M = (S, X, A, P, r, Po, ν, γ), with access to reward.
In both settings we aim to find a context-dependent policy which maximizes the cumulative reward.
The confounding factor here is w.r.t. the unobserved context and distribution Pe in the offline data.
Marginalized State Action Distribution. We denote the state-action frequency ofa policy π ∈ Π
given context X ∈ X by dπ(s, a|x) = (1 一 Y) P∞=0 YtPπ(st = s,at = a|x, so 〜V(∙∣x)), where
Pπ denotes the probability measure induced by π. Similarly, given a distribution over contexts,
we define the marginalized state-action frequency of a policy π under the corresponding context
distribution by
dΠo (s,a) = Ex 〜Pod(S,a | X)]
d∏e (s, a) = Ex〜ρe [dπ* (s, a | x)]
(online environment),
(offline expert data).
A Causal Perspective. Our work sits at an intersection between the fields of RL and Causal In-
ference (CI). We believe it is essential to bridge the gap between these two fields, and include an
interpretation of our model using CI terminology in Appendix D, where we equivalently define our
objective as an intervention over the unknown distribution Pe in a specified Structural Causal Model,
as depicted in Figure 2.
3 Imitation Learning with Unobserved Confounders
In this section, we analyze the problem of confounded imitation learning, namely, learning from ex-
pert trajectories with hidden confounders and without reward. Similar to previous work, we consider
the task of imitation learning from expert data in the setting where the agent is allowed to interact
with the environment (Ho & Ermon, 2016; Fu et al., 2017; Kostrikov et al., 2019; Brantley et al.,
2019). In the first part of this section we assume no covariate shift between the online environment
and the data is present, i.e., Pe = Po. We lift this assumption in the second part, where we focus on
the covariate shift of the hidden confounders.
3.1	NO COVARIATE SHIFT: Po = Pe
We first consider the scenario in which no covariate shift is present between the offline data and the
online environment, i.e., Po = Pe . We begin by defining the marginalized ambiguity set, a central
component of our work.
Definition 1. For π ∈ Π, we define the set of deterministic policies that match the marginalized
state-action frequency of π by	Υπ	=	π0	∈ Πdet :	dρπo0 (s, a)	=	dρπe (s, a)	∀ s ∈	S, a	∈ A .
3
Published as a conference paper at ICLR 2022
Context-Independent
Transition
(Theorem 2)
Bounded Confounding
(Appendix C)
Context-Independent
Reward
(Theorem 3)
Figure 3: A spectrum for the difficulty of confounded imitation with covariate shift. Context-Independent
transitions may result in non-identifiable and catastrophic candidates, whereas context-independent rewards
reduce the problem to a standard imitation learning.
Recall that, in general, ∏ ∈ ∏M may depend on the context X ∈ X. Therefore, the set Υ∏*
corresponds to all deterministic policies that cannot be distinguished from ∏* based on the observed
expert data. The following theorem shows that for any policy π* ∈ ∏M and any policy ∏o ∈ Υ∏*,
one could design a reward function r0, for which π0 is optimal, while the set Υπ* is indiscernible
from Υπ0, i.e., Υπ* = Υπ0 (see Appendix F for proof). In other words, Υπ* is the smallest set of
candidate optimal policies.
Theorem 1.	[Sufficiency of Υ∏*] Assume Pe ≡ ρ0. Let π* ∈ ∏M and let ∏o ∈ Υ∏*. Then,
Υ∏* = Υ∏o. Moreover, if ∏o = π*, then there exists ro such that ∏o ∈ ∏Mo but π* ∈ ∏Mo, where
M0 = (S,A,X,P,r0,Po,ν,γ).
The above theorem shows that any policy in Υπ* is a candidate optimal policy, yet without knowing
the context the expert used. Such ambiguity can result in selection of a suboptimal or even catas-
trophic policy. We provide a practical algorithm in Appendix B which calculates the ambiguity set
Υπ* , and returns an average policy, with computational guarantees. In the next subsection we ana-
lyze a more challenging scenario, for which Po 6= Pe. In this case, Υπ* may not be sufficient for the
imitation problem.
3.2 WITH COVARIATE SHIFT: Po 6= Pe
Next, we assume covariate shift exists between the online environment and the expert data, i.e.,
Po 6= Pe . Particularly, without further assumptions on the extent of covariate shift, we show two
extremes of the problem. In Theorem 2 we prove that whenever the transitions are independent of
the context, the data cannot in general be used for imitation. In contrast, in Theorem 3 we prove that,
whenever the reward is independent of the context, the imitation problem can be efficiently solved.
Clearly, if SUPP(Po) ⊆ SUPP(Pe)1 then there exists X ∈ SUPP(Po) for which π* is not identifiable
from the expert data2. We therefore assume throughout that Supp(Po) ⊆ Supp(Pe). We begin by
defining the set of non-identifiable policies as those that cannot be distingUished from their respective
state-action freqUencies withoUt information on Pe .
Definition 2. We say that {πi}ik=1 are non-identifiable policies if there exist {Pi}ik=1 such that
dρπii (s, a) = dρπjj (s, a) for all i 6= j.
Next, focUsing on catastrophic policies (recall EqUation (1)), we define catastrophic expert policies
as those which coUld be either optimal or catastrophic Under Po for different reward fUnctions.
Definition 3. We say that {πi}ik=1 are catastrophic expert policies if there exist {ri}ik=1 such that for
all i, ∏i ∈ ∏M., and ∃j ∈ [k], j = i such that ∏ ∈ ∏M?, where Mj = (S, X, A, P, rj,ρo, ν, Y).
Using the fact that both Pe and r are Unknown, the following theorem shows that whenever
P (s0 |s, a, X) is independent of X, one coUld find two policies which are non-identifiable, catastrophic
expert policies (see Appendix F for proof). In other words, in the case of context-independent tran-
sitions, withoUt fUrther information on Pe or r the expert data is Useless for imitation. FUrthermore,
attempting to imitate the policy Using the expert data coUld resUlt in a catastrophic policy.
1For a distribUtion P we denote by SUpp(P) the sUpport of P.
2We define non-identifiability in Definition 2. We Use a similar notion of identifiability as in Pearl (2009b)
4
Published as a conference paper at ICLR 2022
Theorem 2.	[Catastrophic Imitation] Assume |X | ≥ |A|, and P (s0|s, a, x) = P(s0|s, a, x0) for all
x, x0 ∈ X. Then ∃πe,1, πe,2 s.t. {πe,1, πe,2} are non-identifiable, catastrophic expert policies.
While Theorem 2 shows the impossibility of imitation for context-free transitions, whenever the
reward is independent of the context, the imitation problem becomes feasible. In fact, as we show
in the following theorem, for context-free rewards, any policy in Υ∏* is an optimal policy.
Theorem 3.	[Sufficiency of Context-Free Reward] Assume Supp(ρo) ⊆ Supp(ρe) and
r(s, a, x) = r(s, a, x0) for all x, x0 ∈ X. Then Υ∏* ⊆ ∏M.
Theorems 2 and 3 suggest that the hardness of the imitation problem under covariate shift lies on a
wide spectrum (as depicted in Figure 3). While dependence of the transition P (s0 |s, a, x) on x pro-
vides us with information to identify x in the expert data, the dependence of the reward r(s, a, x) on
x increases the degree of confounding in the imitation problem. Both of these results are concerned
with arbitrary confounding.
Bounded Confounding: A Sensitivity Perspective. A common approach in causal inference is
to bound the bias of unobserved confounding through sensitivity analysis (Hsu & Small, 2013;
Namkoong et al., 2020; Kallus & Zhou, 2021). In our setting, this confounding bias occurs due to a
covariate shift of the unobserved covariates. As we’ve shown in Theorem 2, though these covariates
are observed in the online environment, their shifted and unobserved distribution in the offline data
can render catastrophic results. Therefore, we consider the odds-ratio bounds of the sensitivity in
distribution between the online environment and the expert data, as stated formally below.
Assumption 1 (Bounded Sensitivity). We assume that Supp(ρe) ⊆ Supp(ρo) and that there exists
some Γ ≥ 1 such that for all x ∈ Supp(ρe), Γ-1
Po(X)(I-Pe(X))	p
—Pe(X)(I-Po(X))一
Next, we define the notion of δ-ambiguity, a generalization of the ambiguity set in Definition 1.
Definition 4 (δ-Ambiguity Set). For a policy π ∈ Π, we define the set of all deterministic policies
that are δ-close to π by Υδπ = π0 ∈ Πdet : dPπo0 (s, a) - dPπe (s, a) < δ, s ∈ S, a ∈ A .
Similar to Definition 1, the δ-ambiguity set considers all deterministic policies with a marginalized
state-action frequency of distance at most δ from π. The following results shows that ΥπΓ*-1 is a
sufficient set of candidate optimal policies, as long as Assumption 2 holds for some Γ ≥ 1.
Theorem 4.	[Sufficiency of Υ∏-1 ] Let Assumption 2 holdfor some Γ ≥ 1. Then π* ∈ Υ∏-1.
For the interested reader, we further analyze the case of bounded confounding in Appendix C. We
also demonstrate the effect of bounded confounding in Section 5. In the following section, we
show that, while arbitrary confounding may result in catastrophic results for the imitation learning
problem, when coupled with reward, one can still make use of the expert data.
4 Using Expert Data with Unob served Confounders for RL
In the previous section we showed sufficient conditions under which imitation is possible, with and
without covariate shift. When covariate shift is present, but unknown, the imitation learning problem
may be hard, or even impossible (see Theorem 2, catastrophic imitation). We ask, had we had access
to the reward function, would the expert data be useful under arbitrary covariate shift? In this section
we show that expert data with unobserved confounders can be used to converge to an expert policy,
even when arbitary covariate shift is present. In our experiments (Section 5) we empirically show
that using our method can also improve overall performance.
We view the confounded expert data as side information to the RL problem. Specifically, we assume
access to the true reward signal in the online environment and wish to leverage the offline expert data
to aid the agent in converging to an optimal policy. To do this, we define an optimization problem
that maximizes the cumulative reward, while minimizing an f -divergence (e.g., KL-divergence, TV-
distance, χ2-divergence) of state-action frequencies in Υπ*,
maX EX〜Po,s,a〜dπ(s,a∣x)[r (S,a, X)] - λDf (dρo (S, a) ||dp® (S, a)) .	(PI)
5
Published as a conference paper at ICLR 2022
Algorithm 1 RL using Expert Data with Unob-
served Confounders (Follow the Leader)
1:	input: Expert data with missing context D*,
λ > 0, policy optimization algorithm ALG-RL
2:	init: Policy π0
3:	for k = 1, . . . do
4:	Ps - argminρ DκL(d7∏o	(s,a)∣∣d∏* (s,a))
5:	gk J 1 ggk-1 + E	,πk-1 -7√*1T )
k k ∖"	s,a〜d∏o	dπp( (s,a)J y
6:	πk J ALG-RL(r(s, a, x) - λgk (s, a))
7:	end for
Algorithm 2 RL using Expert Data with Unob-
served Confounders (Online Gradient Descent)
1:	input: Expert data with missing context,
λ, B, N > 0, policy optimization alg. ALG-RL
2:	init: Policy π0 , bonus reward network gθ
3:	for k = 1, . . . do
4:	Ps J argminρ Df (d^-1 (s,a)∣∣d∏* (s,a))
5:	for e = 1, . . . N do
6:	Sample batch {si,ai}B=ι 〜dPo-1 (s,a)
7:	Sample batch {Se, ae}B=ι 〜d∏* (s, a)
8:	Update gθ according to VθL(θ)	=
⅛ PB=I vθ [f * (gθ (Se,ae )) - gθ (si,ai)]
9:	end for
10:	πk J ALG-RL(r(s, a, x) - λgθ (s, a))
11:	end for
Here, λ > 0 and Df is the f -divergence, where f is a convex function f : (0, ∞) 7→ R. The
solution to Problem (P1) is an optimal policy π* ∈ ∏M as long as Po ≡ ρe. Rewriting Df using its
variational form (see Appendix A for background on the variational form of f -divergences), we get
the following equivalent optimization problem, motivated by Nachum et al. (2019):
max Cminm) Ex〜ρo,s,a〜d∏(s,a∣x)[r(s, a,x) + λg(s,a)] - λEs	d∏*(s )[f *(g(s,a))], (P1b)
π∈Π g=S×A→R	k 1 7	，	ρe ∖，/
where f * is the convex conjugate of f, i.e., f *(y) = SuPx Xy - f (y).
Unfortunately, when covariate shift exists (i.e., ρo 6= ρe), Problems (P1) and (P1b) are not ensured
to converge to an optimal policy (Theorem 2). Instead, we propose to reformulate Problem (P1b)
using a distribution ρs which minimizes the f -divergence, as follows,
max	min	Ex〜ρ0,s,a〜d∏(s,a∣x) [r(s, a,x) + λg(s,	a)]	-	λEs	d∏*(s	)[f *(g(s,	a))].	(P2)
π∈Π g = S×A→R	'	ρs '
ρs∈B(X)
Here, B(X ) denotes the set of probability measures on the Borel sets of X, and
dρS (s, a) = Ex〜ρs [dπ* (s, a | x)]. Indeed, whenever SUPP(Po) ⊆ SUPP(ρe), We have that
(π, ρs) = (π*, ρo) is a solution to Problem (P2). That is, unlike Problems (P1) and (P1b), Prob-
lem (P2) can achieve an optimal solution to the RL problem which still uses the expert data.
Corrective Trajectory Sampling (CTS). Solving Problem (P2) involves an expectation over an
unknown distribution, d∏ (s, a). Fortunately, d∏ (s, a) can be equivalently written as an expectation
over trajectories in D* , rather than expectation over unobserved contexts, as shown by the following
proposition (see Appendix F for proof):
Proposition 1. [Trajectory Sampling Equivalence] Let Ps* which minimizes Problem (P2) for some
π ∈ Π, g : S × A 7→ R, and assume Supp(Po) ⊆ Supp(Pe). Then, there exists pn ∈ ∆n such that
dΠ* (S,a) =nl→∞ Ei〜Pn [(I-Y) P∞=0 Yt1{(Si,ai) = (s,a)}].
Proposition 1 allows us to estimate the inner minimization problem over Ps in Problem (P2) using
trajectory samples. Particularly, we uniformly sample k distributions p1n , . . . pkn , where pjn ∈ ∆n ,
and then estimate
min Dfmno||dn：) ≈
ρs
j∈minfe}Ddf (d∏o(s,a) IlEi〜Pn Xγt1{(St,at) =(S,a)}
t=0
, (2)
∞
which can be estimated by using the variational form ofDf (see Appendix A). We call this procedure
Corrective Trajectory Sampling (CTS), as it uses complete trajectory samples to account for the
unknown context distribution Pe .
Solving Problem (P2). Algorithm 1 provides an iterative procedure for solving the optimization
problem in Problem (P2). It uses alternative updates of a cost player (line 5) and policy player (line
6). In line 5 the gradient of DKL w.r.t. dπ is taken using a Follow the Leader (FTL) cost player
6
Published as a conference paper at ICLR 2022
FeedingSawyer
— With CTS (ours, β = 0.3)
— With CTS (ours, β = 0.8)
-No CTS (β = 0.3)
-No CTS (β = 0.8)
Figure 4: Plots compare training curves of using CTS vs. normal sampling of expert data for small (β = 0.3)
and large (β = 0.8) covariate shift bias in four assistive-healthcare tasks. Dashed black lines show expert and
RL (without data) scores. Runs were averaged over 5 seeds. Legend is shared across all plots.

to estimate the next bonus iterate. Finally, in line 6, an efficient, approximate policy optimization
algorithm ALG-RL is executed using an augmented reward. The following theorem, provides con-
vergence guarantees for Algorithm 1 (see Appendix F for proof based on Zahavy et al. (2021)).
Theorem 5.	Let ALG-RL be an approximate best response player that solves the RL problem in
iteration k to accuracy Wk = √. Then, Algorithm 1 will converge to an ε-optimal solution to
Problem (P2) in O (^4) samples.
Notice that, while Theorem 5 shows Algorithm 1 converges to an optimal policy, it does not deter-
mine whether the expert data improves overall learning efficiency. We leave this theoretical question
for future work. Nevertheless, in the following section we conduct extensive experiments to show
that such data can indeed improve overall performance on various tasks.
A drawback of Algorithm 1 is that it needs to estimate the state-action frequencies. A practical
implementation of Algorithm 1 using online gradient descent (OGD) is provided in Algorithm 2 -
the algorithm does not require approximate estimates of the state-action frequencies, but rather, only
the ability to sample from them. Similar to Algorithm 1, we use CTS (see Equation (2)) to estimate
ρs in line 4 according to some f -divergence. Here, samples are drawn from the current policy as
well as samples from D* (with CTS). We write Df in its variational form, and use a neural network
representation for gθ . We then use the aforementioned samples to minimize the f -divergence using
OGD. Finally, the policy is updated using ALG-RL and an augmented reward.
5	Experiments
We tested our proposed approach for using expert data with hidden confounding in recommender-
system and assistive-healthcare environments. For all our experiments we used χ2-divergence as our
choice of f -divergence, as we found it to work best. Comparison to other divergences is provided
in Figure 5 (left). We used PPO (Schulman et al., 2017) implemented in RLlib (Liang et al., 2018)
for both the imitation as well as RL settings. We include specific choice of hyperparameters and an
exhaustive overview of further implementation details in Appendix E.
7
Published as a conference paper at ICLR 2022
Figure 5: Left plot shows comparison of different choices of f -divergences for pure imitation (without reward
and without covariate shift) on the BedBathingPR2 environment. Middle plot depicts execution of imitation
with hidden confounding (without reward) for different levels of covariate shift. Right plot compares our CTS
correction on the RecSim environment with strong covariate shift bias. All runs were averaged over 5 seeds.
Assistive Healthcare. Consider the challenge of providing physical assistance to disabled persons.
A recently proposed set of tasks for assistive-healthcare, simulating autonomous robots as versatile
caregivers (Erickson et al., 2020). Each task has a unique goal, affected by both the physical world
as well as the patient specific preferences and disabilities.
We tested our algorithm on four tasks: feeding, dressing, bathing, and drinking. In these, we used
the following features to define user context: gender, mass, radius, height, patient impairment, and
patient preferences. The patient’s mass, radius, and height distributions were dependent on gender.
The patient’s impairment was given by either limited movement, weakness, or tremor (with sporadic
movement). Finally, the patient’s preferences were affected by the velocity and pressure of touch
forces applied by the robot. For the context distribution ρo we used the default values as provided
by the original environment. To enforce a distribution shift in the expert data, we shifted each distri-
bution randomly with an additive factor β ∙ dχ, where β ∈ [0,1], and dχ was a random distribution
chosen from a set of shifting distributions (see Appendix E). Here β corresponds to the covariate
shift strength. The expert data was generated by a fixed policy trained using a dense reward function.
A sparse reward signal was used for executing our experiments with the confounded expert data. For
further details, we refer the reader to Appendix E.
Figure 4 depicts results for executing Algorithm 2 on four assistive-gym environments with various
covariate shift strengths. As evident in most of the enviornments, covariate shift strongly affected
overall performance. Particularly in the feeding, drinking, and dressing environments, the success of
reaching the goal (i.e., spoon to mouth, cup to mouth, and sleeve to hand) was highly affected by the
degree of covariate shift. This is due to the changing distribution of size, movement, and preferences
of the patient, and thus of the goal. Nevertheless, in all environments, using the expert data (with
and without CTS) was found to help induce better policies than executing the same RL algorithm
without expert data. This suggests that expert data can assist in improving overall RL performance,
yet correcting for covariate shift may significantly improve it in these domains.
Recommender Systems. In practical recommender systems, sequential interaction with users
presents a great challenge for optimizing user long-term engagement and overall satisfaction (Ie
et al., 2019). Leveraging expert data collected using, e.g., surveys to users, may greatly benefit fu-
ture solutions. Because features are repeatedly added to these systems, full information in the data
is rarely available. Here, we use the recently proposed RecSim Interest Evolution environment (Ie
et al., 2019), simulating sequential user interaction with a slate-based recommender system. The
environment consists of a document model for sampling documents, a user model for defining a
distribution over user context features, and a user choice model, which defines the intent of the user
based on observable document features and the user’s sampled context (e.g., personality, satisfaction,
interests, demographics, and other behavioral features such as session length or visit frequency).
We used a slate of 10 documents and a user context of dimension 20. To test the severity of the
implications of Theorem 2 in the confounded imitation setting, we used a user-model sampled from
a Beta-distribution. Specifically, for the expert data the user context features x = (x0, . . . , x19)
were sampled from a Beta-distribution, where Xi 〜 Beta(αi, 4), and α? = 1.5 + 需i. In
contrast, the online environment features were sampled from a shifted Beta-distribution with
a? = (1 - β)(1.5 + 895i) + β(l0 - 895i), where β ∈ [0,1] defined the shift strength. While the
8
Published as a conference paper at ICLR 2022
original environment used a uniform distribution to generate user contexts, the Beta-distribution let
us analyze severe forms of covariate shifts, testing the limits of our results in Sections 3 and 4.
Figure 5(a) depicts the effect of increased covariate shift on imitation in the RecSim environment
with a dataset of 100 expert trajectories (generated by an optimal policy that had access to the full
context). Without covariate shift (β = 0) an optimal score is achieved, and as β increases, perfor-
mance decreases. Particularly, as the mirrored distribution is reached (β = 1), a catastrophic policy
is reached. While the imitator “believes” to have reached an optimal policy, it has in fact reached a
catastrophic one, as shown by the orange plot. Conversely, Figure 5(b) depicts the benefit of using
confounded expert data in the RL setting (with an online reward signal). Though strong confounding
is present, the agent is capable of leveraging the data to improve overall learning performance.
6	Related Work
Imitation Learning. The imitation learning problem has been extensively studied in both the fully
offline (Pomerleau, 1989; Bratko et al., 1995) as well as online setting (Ho & Ermon, 2016; Fu
et al., 2018; Kim & Park, 2018; Brantley et al., 2019). Specific to our work are GAIL (Ho & Ermon,
2016), AIRL (Fu et al., 2017), and DICE (Kostrikov et al., 2019), which use distribution matching
methods. Our work generalizes these settings to imitation with hidden confounders.
Reinforcement Learning with Expert Data. Much work has revolved on leveraging offline data
for RL. Recently, offline RL (Levine et al., 2020) has shown great improvement over regular offline
imitation techniques (Kumar et al., 2020; Kostrikov et al., 2021; Tennenholtz et al., 2021a; Fujimoto
& Gu, 2021). In the online RL setting, the combination of offline data to improve RL efficiency has
shown great success (Nair et al., 2020). KL-regularized techniques (Peng et al., 2019; Siegel et al.,
2019) as well as DICE-based algorithms (Nachum et al., 2019) have also shown efficient utilization
of offline data. Our work generalizes the latter to the confounded setting.
Intersection of Causal Inference and Imitation Learning. Closely related to our work is that of
Zhang et al. (2020). There, the authors suggest a notion of imitability, showing when observational
data can help identify a policy under some partially observed structural causal model. Our work
provides an alternative perspective on the problem. In contrast to their work, we rely on concurrent
imitation approaches (i.e. state-action frequency matching techniques) and importantly, allow access
to the online environment. Furthermore, we provide guarantees and practical algorithms for both the
imitation and RL settings. We refer the reader to Appendix D for an interpretation of our framework
in CI terminology, from a perspective of stochastic interventions in a Structural Causal Model.
Another intersection with causal inference discusses the problem of causal confusion in imitation
(de Haan et al., 2019). Causal confusion is concerned with the problem of nuisances in observed
confounded data due to an unknown causal structure. These “causal misidentifications” can lead to
spurious correlations and catastrophic failures in generalization. In contrast, our work discusses the
orthogonal problem of hidden confounders with possible covariate shift.
Intersection of Causal Inference and Reinforcement Learning. Previous work has analyzed
the problem of optimal control from logged data with unobserved confounders (Lattimore et al.,
2016), as well as utilizing (non-expert) confounded data for online interactions (Tennenholtz et al.,
2021b). Much work has revolved around the reinforcement learning setup with access to (non-
expert) confounded data (Zhang & Bareinboim, 2019; Wang et al., 2020). Other work has considered
the problem of off-policy evaluation from confounded data (Tennenholtz et al., 2020; Oberst &
Sontag, 2019; Kallus & Zhou, 2020). Our work is focused on leveraging expert data with hidden
confounders and possible covariate shift in both the imitation and the RL settings.
7	Conclusion
This work presented and analyzed the problem of using expert data with hidden confounders for
both the imitation and RL settings. We showed that covariate shift of hidden confounders between
the expert data and the online environment can result in learning catastrophic policies, rendering
imitation learning hard, or even impossible (Theorem 2). In addition, we showed that when a reward
is provided, using the expert data is still possible under arbitrary hidden covariate shift (Theorem 5),
and proposed new algorithms for tackling this problem using corrective trajectory sampling (CTS).
9
Published as a conference paper at ICLR 2022
References
Kiante Brantley, Wen Sun, and Mikael HenafL Disagreement-regularized imitation learning. In
International Conference on Learning Representations, 2019.
Ivan Bratko, Tanja Urbancic, and Claude Sammut. Behavioural cloning: phenomena, results and
problems. IFACProceedings Volumes, 28(21):143-149,1995.
Juan Correa and Elias Bareinboim. A calculus for stochastic interventions: Causal effect identifica-
tion and surrogate experiments. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 10093-10100, 2020.
Imre Csiszar and Paul C Shields. Information theory and statistics: A tutorial. 2004.
Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Ad-
vances in Neural Information Processing Systems, 32:11698-11709, 2019.
Zackory Erickson, Vamsee Gangaram, Ariel Kapusta, C Karen Liu, and Charles C Kemp. Assis-
tive gym: A physics simulation framework for assistive robotics. In 2020 IEEE International
Conference on Robotics and Automation (ICRA), pp. 10169-10176. IEEE, 2020.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse rein-
forcement learning. In International Conference on Learning Representations, 2018.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nature
medicine, 25(1):16-18, 2019.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
preprint arXiv:1502.02259, 2015.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29:4565-4573, 2016.
Jesse Y Hsu and Dylan S Small. Calibrating sensitivity analyses to observed covariates in observa-
tional studies. Biometrics, 69(4):803-811, 2013.
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A
survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1-35, 2017.
Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and
Craig Boutilier. Recsim: A configurable simulation platform for recommender systems. arXiv
preprint arXiv:1909.04847, 2019.
Nathan Kallus and Angela Zhou. Confounding-robust policy evaluation in infinite-horizon rein-
forcement learning. arXiv preprint arXiv:2002.04518, 2020.
Nathan Kallus and Angela Zhou. Minimax-optimal policy learning under unobserved confounding.
Management Science, 67(5):2870-2890, 2021.
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa.
Imitation learning as f-divergence minimization. In International Workshop on the Algorithmic
Foundations of Robotics, pp. 313-329. Springer, 2020.
Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. arXiv preprint arXiv:1912.05032, 2019.
10
Published as a conference paper at ICLR 2022
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Finnian Lattimore, Tor Lattimore, and Mark D Reid. Causal bandits: learning good interventions
via causal inference. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 1189-1197, 2016.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053-3062. PMLR, 2018.
Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.
IEEE Transactions on Information Theory, 52(10):4394-4412, 2006.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill. Off-policy
policy evaluation for sequential decisions under unobserved confounding. arXiv preprint
arXiv:2003.05623, 2020.
Michael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural
causal models. In International Conference on Machine Learning, pp. 4881-4890. PMLR, 2019.
Judea Pearl. Causality: models, reasoning and inference, volume 29. Cambridge University Press,
2000.
Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, New York,
NY, USA, 2nd edition, 2009a. ISBN 052189560X, 9780521895606.
Judea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96-146, 2009b.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSY-
CHOLOGY .. ., 1989.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.
com/FacebookResearch/Nevergrad, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked:
Behavior modelling priors for offline reinforcement learning. In International Conference on
Learning Representations, 2019.
11
Published as a conference paper at ICLR 2022
Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable en-
vironments. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
10276-10283, 2020.
Guy Tennenholtz, Nir Baram, and Shie Mannor. Gelato: Geometrically enriched latent model for
offline reinforcement learning. arXiv preprint arXiv:2102.11327, 2021a.
Guy Tennenholtz, Uri Shalit, Shie Mannor, and Yonathan Efroni. Bandits with partially observable
confounded data. In Conference on Uncertainty in Artificial Intelligence. PMLR, 2021b.
Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably efficient causal reinforcement learning
with confounded observational data. arXiv preprint arXiv:2006.12311, 2020.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex mdps. arXiv preprint arXiv:2106.00661, 2021.
Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment
regimes. Advances in Neural Information Processing Systems, 32:13401-13411, 2019.
Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved
confounders. Advances in neural information processing systems, 33, 2020.
12
Published as a conference paper at ICLR 2022
Distribution Matching	Equivalent Representation	Comments
Distribution Ratio	suPgf×A→(0,1) Es,a 〜d∏0[lOgg(S,a))] + Es,a~dπo [log(1 - g(S,a川	GAIL (Ho & Ermon, 2016)
KL-divergence	suPgf×A→R Es,a 〜d7 [g(s,a)] - log Es,a 〜d∏0 he"',"']	Donsker-Varadhan Representation (Kostrikov et al., 2019)
χ2-divergence	suPgf×A→R 2Es,a~dπo [g(s,a)] - Es,a 〜d∏0 [g2(s,叫	Variational Representation of f -Divergence
TV Distance	suP∣g∣≤ 2 Es,a-dπo [g(s,a)] - Es,a〜d∏0[g(s,a1	Variational Representation of f -Divergence
Table 1: Different distribution matching techniques and their equivalent representations.
Appendix
A Background: Distribution Matching for Imitation Learning
A common approach used in (non-confounded) imitation learning is matching the policy’s
*
state-action frequency dρπo to the offline target distribution dρπe . Consider a source distribu-
tion p ∈ ∆N and target distribution q ∈ ∆N. GAIL (Ho & Ermon, 2016) uses the distribu-
tion ratio objective log(p/q), which can estimated using a GAN-like objective DR(p||q) =
supg：z1(0,i)Ep[log(g(z))] + Eq [log(1 - g(z))], to match the distribution P to q.
This technique can be generalized to f -divergences (Csiszar & Shields, 2004; Liese & Vajda, 2006;
Kostrikov et al., 2019; Ke et al., 2020). Specifically, we wish to minimize a discrepancy measure
from p to q, namely minp∈K D(p||q). For a convex function f : [0 : ∞) 7→ R, the f -divergence of
p from q is defined by Df
(p∣∣q) = %[f (p)]
DICE (Kostrikov et al., 2019) uses the variational
representation of the f -divergence,
Df (p||q) = SUp Ep[g(z)] - Eq[f*(g(z))],
g i Z→R
where f * is the Fenchel conjugate of f defined by f * (y) = SUpx Xy - f (y). The convex conjugate
has closed form solutions for the total variation distance, KL-divergence, χ2-divergence, Squared
Hellinger distance, Le Cam distance, and Jensen-Shannon divergence. Using the variational repre-
sentation of the f -divergence we can estimate Df using samples from p and q. Table 1 presents
examples of various f -divergences and their respective dual formulation. We also add the distribu-
tion ratio for comparison to the table, though it is not an f -divergence.
B Confounded Imitation - Algorithm and Convergence
Guarantees
B.1 A Toy Example
To gain intuition, we start with a simple toy example. Consider the three-state example depicted
in Figure 6. Here, the environment initiates at state A w.p. 1, after which the agent can choose to
(deterministicaly) transition to state B or C. The agent then receives a reward depending on the
context. The optimal policy is given by π*(a∣s, x) = 1{a = 0b, x = xj + 1{a = a0, x = X2}
for s = A, and any action is optimal for s 6= A. Without loss of generality we as-
sume π*(aB |B, x) = π*(aC |C, x) = 1. We turn to analyze the marginalized state-
action frequency, which uniquely defines the set of optimal policies (Puterman, 2014). De-
noting ρe(x1) = ρ, we have that dρπe* (s, a) = ρdπ* (s, a|x1) + (1 - ρ)dπ* (s, a|x2). Then,
dρπe* (s, a) = (1 - γ)1{s = A} + ργ1{s = B, a = aB} + (1 - ρ)γ1{s = C, a = aC}.
13
Published as a conference paper at ICLR 2022
Figure 6: A contextual MDP with state space S = {A, B, C}, action space A = {aB , aC} and
context space X = {xι, x2}. We assume V(A|x) = 1 for all X ∈ X. The actions a，B,a° transition
the agent to states B, C, respectively, after which the agent receives a reward r ∈ {0, 1} depending
on the context. We assume B, C are sink states.
Algorithm 3 Confounded Imitation
1:	input: Expert data with missing context D* 〜dρ[, λ > 0, sensitivity bound δ ≥ 0.
2:	init: Y = 0	ɛ
3:	for n = 1, . . . do
4:	Sample u(s,a)〜U[0,δ],∀s,a
5:	L (n； g0) := Es,a〜% (s,a) [g0(s, a)] — Es,a〜限(s,a)+u(s,a) [g0(s, a)]
6:	Li (π; gi) :=
Ex〜Po,s,a〜dπ (s,a|x) [gi (S, a, X)] ― Ex〜p。,s,a〜dπi (s,a∣x) [gi (S, a, X)] , i ≥ 1
7:	Compute πn by solving
min max	L*(π; go(s,a)) — λmin Li(π; gi(s, a, x))	(4)
π∈πdet ∣go∣≤1 ,∣gi∣≤1	i
8:	if πn ∈ Υ then
9:	Terminate and return π(a∣s,x) =LP1=1d :(Sax)
,	Pin=-11 Pa0 dπi (s,a0,x)
10:	else
11:	Υ=Υ∪{πn}
12:	end if
13:	end for
No Covariate Shift. Suppose ρo = ρe,and P = 1. Trivially d∏ (s,a) = d∏ (s,a). We define the
(suboptimal) policy
∏o(a∣A, x) = 1 — π*(a∣A, x) , a ∈ A,x ∈ X.	(3)
It can be verified that d∏ (s, a) = dp0 (s, a) still holds, yet ∏o is catastrophic (Equation (1)) with
value zero. A question arises: can we show that π0 is a suboptimal policy given access to the expert
data (i.e., access to d∏ (s, a)) and a forward model P(s0∣s, a, x)?
Unfortunately, one cannot prove that π0 is suboptimal. Informally, notice that π0 is an optimal
policy for an alternative reward function, r0 (S, a, x) = 1 — r(S, a, x), yet is catastrophic w.r.t. the
true reward r. Indeed, since r is unknown and dpo (s, a) = %。(s, a), We cannot reject r° (i.e.,
we cannot conclude that r0 is not the true reward). In other words, one cannot use the data to
differentiate which of {∏o,∏*} is the optimal policy.
With Covariate Shift. Next, assume Po 6= Pe, and define π0 as in Equation (3). Let Pee = 1 — Pe
and recall that Pe(x1) = P. Then, we have that
df (s, a) = (1 — ρ)dπ0(s, a|xi) + ρdπ0(s, a∣x2) = (1 — ρ)dπ (s, a∣x2) + Pdn (s, a|xi) = d∏e (s, a).
Indeed, the expert data is incapable of distinguishing ∏0 and ∏*, since df = d^∏*, and Pe is un-
known. Unfortunately, as we’ve shown previously, π0 achieves value zero. Notice that, unlike the
previous section, one cannot distinguish π* from the catastrophic policy ∏0 for any choice of ρ0.
14
Published as a conference paper at ICLR 2022
B.2	A Practical Algorithm
The ambiguity set of Theorem 1 may contain suboptimal policies. Instead of randomly selecting a
policy from this set, we can choose the average policy. The following proposition shows that such a
selection is favorable.
Proposition 2. Define the mean policy ∏(a∣s,x)	=	P "π∈γp ' dn(s，a0 x), and denote
α* = 1nMYTYn*| ∈ [0,1]∙ Then, vm(∏) ≥ α*v* + (1 — ɑ*)min∏∈γπ* VM(π).
Remark 1. Note that π is generally not the average policy ∣τ1 * ∣ E∏∈γ * π(a∣s, x).
Remark 2. In an episodic setting, π can be estimated by uniformly sampling a policy π ∈ Υ∏* at
the beginning of the episode, and playing it until the environment terminates.
Algorithm 3 describes our method for calculating the ambiguity set of Theorem 1, and returns ∏ of
Proposition 2. At every iteration of the algorithm, we find a new policy in the set by minimizing the
total variation distance (written in variational form) between dρπ* (s, a) and dρπ (s, a), while regular-
izing it with the distance between π and all previously collected πi ∈ Υ. Algorithm 3 also uses a
sensitivity parameter δ ≥ 0 (defined formally in Appendix C) whenever bounded covariate shift is
present. For this section we assume δ = 0.
In practice, the functions L and Li in lines 4 and 5 are estimated using samples from trajectories
of ∏, ∏i, and D*. We then solve the min-max problem of Equation (4) using a parametric represen-
tations of gi and online gradient decent. The following proposition states that Algorithm 3 indeed
retrieves the set Υπ* .
Proposition 3. Assume Pe ≡ Po and ∣Υ∏* | < ∞. Then there exists λ* > 0 such that for any
λ ∈ (0, λ*), Algorithm 3 (with δ = 0 SenSitivity) will return π of Proposition 2 after exactly ∣Υ∏* |
iterations.
B.3	Imitation with Context-Free Reward
We tested Algorithm 3 on both the RecSim environment as well as a four-rooms environment with
random instantiations of walls. Experiments for the RecSim environment are readily provided in
Section 5. Here we describe our simple four-rooms environment and show experiments w.r.t. Theo-
rem 3.
The four-rooms environment, as depicted in Figure 7, is a 15 × 15 grid-world in which an agent can
take one of four actions: LEFT, RIGHT, UP, or DOWN. Each action moves the agent in the specified
direction whenever no obstacle is present. The agent (shown in blue) must reach the (green) goal
while avoiding the (red) mine. When the goal is reached the agent receives a reward of +1 and the
episode terminates. In contrast, if the agent reaches the mine, she receives a reward of -1 and the
episode terminates. The state space of the environment consists of the agent’s (row, col) position in
the world. The rest of the information in the environment is defined by the context x. Particularly,
the context is defined by the position of the green goal, the position of the red mine, and the specific
instantiation of walls (two instantiations are depicted in Figure 7).
We trained an agent with full information (i.e., observed context, including goal location, mine
location, and walls). We generated expert data w.r.t. the trained agent. To demonstrate the result
of Theorem 3 we executed Algorithm 3 with both a shifted distribution and the default distribution
of walls. We did not change the distribution of goal and mine. Note that since the distribution of
walls only affects the transition function and not the reward, we expect, by Theorem 3, the optimal
solution to remain the same. Indeed, as shown in Figure 7 after training an agent with no access
to the contextual information of the walls in the expert data, the agent achieved comparable results
both with and without covariate shift on the distribution of walls.
This result seem surprising at first, as the walls are essential for solving the task at hand. Never-
theless, since the distribution of wall is observed in the online environment, the partially observed
expert data suffices to obtain an optimal policy. This settles with Theorem 3 which indeed states that
this information is not needed in the expert data in order to obtain an optimal policy.
15
Published as a conference paper at ICLR 2022
Figure 7: Results for the rooms environment with covariate shift affecting only the distribution of
walls. It is evident that whenever the reward is context-free comparable performance is obtained.
Runs averaged over 5 seeds.
C B ounded Hidden Confounding
In this section we discuss the imitation learning problem under bounded hidden confounders. There
are several ways to define boundness of unobserved confounders. In Section 3 we showed that,
under arbitrary covariate shift and context-free transitions, the imitation learning problem is impos-
sible, i.e., one cannot rule out a catastrophic policy. We begin by considering the effect of bounded
covariate shift, i.e., Po ≤ C. We then consider almost-context-free rewards, showing a tradeoff w.r.t.
ρe
the hardness of the imitation problem.
A Sensitivity Perspective. A common approach in causal inference is to bound the bias of unob-
served confounding through sensitivity analysis (Hsu & Small, 2013; Namkoong et al., 2020; Kallus
& Zhou, 2021). In our setting, this confounding bias occurs due to a covariate shift of the unobserved
covariates. As we’ve shown in Theorem 2, though these covariates are observed in the online envi-
ronment, their shifted and unobserved distribution in the offline data can render catastrophic results.
Therefore, we consider the odds-ratio bounds of the sensitivity in distribution between the online
environment and the expert data, as stated formally below.
16
Published as a conference paper at ICLR 2022
Assumption 2 (Bounded Sensitivity). We assume that Supp(ρe) ⊆ Supp(ρo) and that there exists
some Γ ≥ 1 such that for all x ∈ Supp(ρe)
Γ-1 ≤ Po(X)(I - Pe(X)) ≤ Γ
一Pe(X)(I - Po(X))一.
Next, we define the notion of δ-ambiguity, a generalization of the ambiguity set in Definition 1.
Definition 5 (δ-Ambiguity Set). For a policy π ∈ Π, we define the set of all deterministic policies
that are δ-close to π by
Υδπ = nπ0 ∈Πdet: dρπo0 (s, a) - dρπe (s, a) <δ,s∈S,a∈Ao.
Similar to Definition 1, the δ-ambiguity set considers all deterministic policies with a marginalized
state-action frequency of distance at most δ from ∏. The following results shows that Υ∏-1 is a
sufficient set of candidate optimal policies, as long as Assumption 2 holds for some Γ ≥ 1.
Theorem 6.	[Sufficiency of Υ∏-1 ] Let Assumption 2 holdfor some Γ ≥ 1. Then π* ∈ Υ∏-1.
The above result suggests that Algorithm 3 can be executed over Υ∏-1 by adding δ = Γ - 1
additive uniform noise to d∏ (s, a) (see Line 4 of Algorithm 3), and executing the algorithm for a
finite number of iterations, finally selecting an average policy from the approximate set.
Context Reconstruction. When bounded covariate shift is present, one might attempt to learn an
inverse mapping of contexts from observed trajectories in the data.
We denote by Pρπ the probability measure over contexts X ∈ X and trajectories
τ = (s0, a0, s1, a1, . . . sH) as induced by the policy π and context distribution P. That is,
H-1
PP(x,τ) = P(X)V(s0∣x) ɪɪ P(st+ι∣st,at,x)π(at∣st,x).
t=0
As the true context is observed in the online environment, we can calculate for any π the quan-
tity Pρπ (X, τ). As the expert data distribution was generated by the marginalized distribution
Pρ∏0 (τ) = Pχ∈χ Pρ∏0 (x, T), it is unclear if knowledge of Pn (x, T) is beneficial.
Fortunately, whenever Assumption 2 holds, a high probability of reconstructing a context in the
online environment induces a high probability of reconstructing it in the expert data. To see this,
assume that there exists δ ∈ [0,1] such that for all ∏ ∈ Υ∏-1, T ∈ SUPP(PPe (T)), there exists X ∈ X
such that
Pρπo (X|T) ≥ min{(1 - δ)(Po(X) + Γ(1 - Po(X))), 1}.	(5)
That is, We assume that for any policy that δ-ambiguous to ∏, and any induced trajectory of X ∈ X,
one can with high Probability identify X in the online environment. ImPortantly, this ProPerty can
be verified in the online environment. When AssumPtion 2 and 5 hold, we get that
Po(X)
Pne(XIT )
PPe (T IX)Pe(X) ≥ Pn (T IX)	Po(X)	=	PPO (XIT)	≥ 1 - δ
PPe (T)	— PPe (T) Po(X) +γ(1 - Po(X))	Po(X) +F(I-Po(X))一 .
In other words, we can reconstruct X with Probability 1 - δ for any trajectory T which satisfies the
above. This allows us to deconfound essential Parts of the exPert data, rendering it useful for the
imitation Problem, even when reward is not Provided. We leave further analysis of this direction for
future work.
Context-Dependent Reward. In Theorem 3 we showed that whenever the reward is indePendent
of the context then the imitation problem is easy, in the sense that any policy ∏o ∈ 丫冗* is also an
oPtimal Policy. Here, we relax the assumPtion on the reward, and instead assume bounded dePen-
dence of the reward on the context. The following definition upper bounds the confounding effect
of the reward w.r.t. the context.
17
Published as a conference paper at ICLR 2022
Figure 8: Contextual MDP Causal Diagram.
Definition 6. Let : X 7→ R such that
min	|r(s, a, x) - r0(s, a)| ≤ (x) , s ∈ S, a ∈ A, x ∈ X
roiS×A→R	-
Using the above definition, We can now show that any policy in Υ∏* is still approximately optimal,
as shown by the following result.
Theorem 7.	[Context Dependent Reward] Let : X 7→ R of Definition 6. Denote
e°e = Ex〜ρo(χ)[e(x)]+ Ex〜ρe(χ)k(x)]. Thenforany π* ∈ ∏M, ∏o ∈ Υ∏*
v(∏o) ≥ v(∏*) - eoe.
A direct corollary for the above result states that for e : X 7→ R of Definition 6, if e(x) = e, for
all X ∈ X, then for π0,π* of Theorem 7, it holds that v(∏o) ≥ v(π*) - 2e. That is, ∏o is an
approximately optimal policy.
D Relation to Causal Inference
Our work is focused on the problem of hidden confounders in expert data for imitation and reinforce-
ment learning. We have chosen to write the paper in terminology famililar to the RL community. In
this section we address and formalize the problem in Causal Inference (CI) terminology. We begin
by defining Structural Causal Models (SCM, Pearl (2009a)) 一 a basic building block of our frame-
work. We then show how the confounded imitation problem can be formalized as an intervention
over a specific SCM. Generally speaking, the causal view casts the environment, namely the expert
environment generating the offline data, and the online environment, as confounders.
Definition 7 (Structural Causal Models). A Structural Causal Model (SCM) is a tuple
M = (U, V, F , P (U )) where U is a set of exogenous variables and V is a set of endogenous vari-
ables. F is a set of functions such that fi ∈ F are functions mapping a set of endogenous variables
Pai ⊆ V \{Vi} anda set of exogenous variables Ui ⊆ U to the domain ofVi, i.e., Vi = fi(Pai, Ui).
Finally, P (u) is a probability distribution over the set of exogenous variables U. We assume that
the SCM is recursive, i.e., that the causal diagram associated with it is acyclic.
Every SCM M is associated with a causal diagram G, as depicted in Figure 8. Our framework relies
largely on the formulation of stochastic interventions, as proposed in Correa & Bareinboim (2020).
18
Published as a conference paper at ICLR 2022
We consider stochastic, conditional (non-atomic) interventions, defined by regime indicators σZ
(Pearl, 2000; Correa & Bareinboim, 2020), defined formally below.
Definition 8 (Non-Atomic Interventions). Given a SCM M = (U, V, F, P (U)) and a subset
Z ⊆ V, an intervention QZ = {σzι,..., σzn} defines a new SCM Mσz = (U, V, F*,P(U)) in
which the Set of functions F is changed to F * = {f*}^v ∈{zjJn U{fi}i:v ∈v ∖{zjJn ∙
Non-atomic interventions are a generalization of the classic atomic do(X = x) interventions, de-
fined by the SCM Mz and causal diagram GZ in which all edges incoming into Z are removed. We
have that
P (y|do(Z = z)) = P(y|z; σZ = do(Z = z)).
Atomic interventions replace function in F by constant functions, whereas non-atomic interventions
use general functions. For notational simplicity, when a single intervention is applied to some fi ∈
F, We denote it by σz = d(f J f*), indicating that in the interventional distribution f is used
instead of fi. Next we define the identifiability of a causal effect under an intervention, as follows.
Definition 9 (Identifiability). Let X, Y, Z ⊆ V with Y∩Z = 0 in some SCM with causal diagram G.
Given an intervention σZ = {σZ1 , . . . , σZn}, the causal effect P(y|x, σZ) is said to be identifiable
from V0 ⊆ V ifit can be uniquely computed from P(V0)for every assignment (y, x) in every model
that induces G and P(V0).
Following the model definitions of Section 2, we define the contextual MDP SCM as follows.
Definition 10. A contextual MDP SCM is defined by the causal diagram of Figure 8.
For some horizon H >	0, the SCM is defined by the set of endogenous variables
V = {si}iH=0 ∪ {ai}iH=0 ∪ {x} ∪ {ri}iH=0 (denoting the states, actions, context and rewards, respec-
tively), a set of exogenous variables U, and functions F = {fsi, fai, fri, fρe, fν0}, where fsi corre-
spond to the transition function, fai the expert policy, fri the reward function, fρe the context expert
distribution, and fν0 the initial context-dependent state distribution.
Relating to our formal definition of our model in Section 2, with slight abuse of notations, the
functions {fs, fg, fa, fρe, fν0} adhere to the following relations
P (si+1 = s0|si = s, ai = a, x) = P (fsi (s, a,x, U))
P(ri = r|si = s, ai = a, x) = δ(fri (s, a, x) = r)
π*(ai = a|si = s, x) = P (fai (a, s, x, U)
ρe(x) = P (fρe (x, U))
νo(so∣x) = P (fνo (s0,x,U)),
where δ(∙) indicates the Dirac delta distribution.
We are now ready to define the confounded imitation problem. We define the (non-atomic) interven-
tion σx = do(fρe J fρo ) which replaces fρe with fρo in the contextual MDP SCM defined above.
The goal of imitation learning is then to identify the quantities
P(ai|si,x, σx = do(fρe J fρo)) , 0 ≤ i ≤ H - 1,	(6)
where, importantly, we assume we only have access to P(si, ai), P (si+1 |si, ai, x), P(s0|x),
and P(x∣Qχ = do(fρe J fρo)). Notice that in our setting, P(si+ι∣Si,ai,x), P(so|x), and
P(χ∣Qχ = do(fρe J fρo)) correspond to known quantities of the online environment, whereas
P(si , ai ) corresponds to the (partially observed) offline expert data. We also emphasize that
P (si+1 |si, ai, x) is not dependent on the intervention σZ. That is,
P(si+1 |si, ai,x, σx = do(fρe J fρo)) = P(si+1|si, ai, x).
Remark. Our work studies a slightly different version of the identifiability problem in Equation (6),
as we only wish to identify an optimal policy from the set Π*M, as opposed to the single specific
policy π*. This requirement can be formalized by defining an extended SCM which includes all
optimal policies in Π*M, with the assumption that only one is observed (corresponding to the expert
data).
19
Published as a conference paper at ICLR 2022
Algorithm 4 RL using Expert Data with Unobserved Confounders (Complete Algorithm)
1:	input: Expert data with missing context D*, λ, α, B,N,M > 0, policy optimization algorithm
ALG-RL
2:	init: Policy ∏0, global bonus reward network gθ
3:	for k = 1, . . . do
4:	Generate dataset of rollouts Rk 〜d∏k-1 (s, a)
5:	Initialize local networks gm J gθ, m ∈ [M]
6:	for m = 1, . . . M do
7:	Sample weight vector wm uniformly from ∆n
8:	for e = 1 . . . N do
BU
9:	Sample batch uniformly from Rk, i.e., {si, ai}i=ι 〜 Rk
10:	Sample batch according to weights Wm from D*, i.e., {se, ae}B=ι wm D*
11:	Update gθm according to
1B
VθmLm(θm) = B ]ζVθm [(1 - α)f *(gm (se,ae)) + αf *(gm (si,ai)) - gm (si,ai)]
12:	end for
13:	m* ∈ argminm∈[M] Lm(θm)
*
14:	Update global parameters from the selected local network g* J gm *
15:	πk J ALG-RL(r(s, a, x) - λgθ* (s, a))
16:	end for
17:	end for
Name	Value	Comments
Batch size	128	
Learning rate	5e-5	
Rollout size	19, 200	
Total timesteps	5e6	
Num epochs	50	How many training epochs to do after each rollout
γ	0.95	Discount factor
kl coef	0.2	Initial coefficient for KL divergence
kl target	0.01	Target value for KL divergence
GAE λ	1	The GAE (lambda) parameter
Num workers	40	
Table 2: Hyper-parameters used to train the PPO agent.
E Implementation Details
Our experiments were based off of the recently proposed assistive-gym (Erickson et al., 2020) and
recsim (Ie et al., 2019) environments. In this section we discuss further implementation details,
hyperparameters, context distributions, and generation of the expert data.
Algorithm Details. A complete description of Algorithm 2 is presented in Algorithm 4. Specific
hyperparameters used are shown in Tables 2 and 3. We implemented the algorithm using the RLlib
framework (Liang et al., 2018). We used PPO (Schulman et al., 2017) as our policy-optimization
algorithm. All neural networks consisted of two-layer fully connected MLPs with 100 parameters
in each layer. We used the same rollout buffer (of size 19200 samples) for both our PPO agent as
well as our imitation module, which estimated the augmented reward.
20
Published as a conference paper at ICLR 2022
Name	Value	Comments
Batch size	128	
Learning rate	1e-4	
Imitation Method	χ-divergence	
Num epochs	50	How many training epochs to do after each rollout
α	0.9	Df regularization coefficient
M	10000	Budget for CTS optimizer
Table 3: Hyper-parameters used for imitation and CTS.
Motivated by Kostrikov et al. (2019), we regularized the expert demonstrations with samples from
dπ. Particularly, we let α ∈ (0, 1], such that 1 - α corresponds to the probabiilty of sampling an
expert example and α corresponds to the probability of sampling from the replay. This leads to
minimizing an augmented version of the f -divergence which can be written as
7min 改 Es,a 〜dπo(s,a∣x)[gGa) - αf *3s,a川-(I - a)Es,a 〜d∏* (s,a)[f *(g(s,a))].
g:S ×A7→R	e
Our imitation module consisted of two networks gθ and hθ as proposed in Fu et al. (2018). The
“done” signal was also added to the state for training the imitation module. For training CTS we
used the Nevergrad optimization platform (Rapin & Teytaud, 2018) with a budget of 10000 and one
worker. Here a copied version of the networks gθ and hθ were used to for initialization and then
approximate the minimum Df .
For choosing λ we used an adaptive strategy which ensured λ balanced the RL objective with the
imitation objective. Specifically, we used the following tradeoff between reward r and bonus g
(1 - λadap )r(s, a) + λadapg(s, a),
where λadap = —. Here rmean corresponds to the average reward in the replay buffer and
rmean +gme
an
gmean to the average bonus in the replay buffer. By averaging the two, we maintained a similar scale
to effectively use the expert data in all the evaluated environments without optimizing for λ.
Context Distribution. For each environment we used a varying context distribution in the expert
data, with increasing distance to that of the online environment. The context distribution for the
RecSim environment is formally described in Section 5. For the assistive-gym environment the
context was defined by the following features: gender, mass, radius, height, patient impairment, and
patient preferences. The patient’s mass, radius, and height distributions were dependent on gender.
The patient’s impairment was given by either limited movement, weakness, or tremor (with sporadic
movement). Finally, the patient’s preferences were affected by the velocity and pressure of touch
forces applied by the robot. We used default average values that were provided with the simulator.
Particularly, we used the following distributions for each feature
gender 〜Bem(PmaIe)
mass (gender)〜N (μιmss (gender), σmm1 ass)
radius (gender)〜N (〃^侬(gender), ^2^通)
height(gender)〜N(μheight(gender), ^2加瓦)
velocity weight 〜Unif(['vel, uvel])
force nontarget weight 〜Unif(['target, Utarget])
high forces 〜Unif(['high forces, uhigh forces])
food hit weight 〜Unif(['hit, Uhit])
food velocity weight 〜Unif(['f°odvel, Ufoodvel])
high pressures weight 〜Unif(['highpressure, UhighPressure])
ImPaIrment 〜MUItinOmial(Pnone, plimits, pweakness, ptremor ).
21
Published as a conference paper at ICLR 2022
I Name	Value	I Name	Value	I Name	Value I
I pmale	0.3	I 'vel	0.225	I 'high pressure	0.009 I
I μmass (male)	78.4	I uvel	0.275	I uhigh pressure	0.011 I
I μmass (female)	62.5	I 'target	0.009	I pnone	0.1 I
2 I σmass	10	I utarget	0.011	I PlimitS	0.4 I
I μradius (male)	1	I 'high forces	0.045	I pweakness	0.3 I
I μradius (female)	1	I uhigh forces	0.055	I ʃnɪ I ptremor	0.2 I
I σradius	0.1	I 'hit	0.9	I	
I μheight (male)	1	I uhit	1.1	I	
I μheight (female)	1	'food vel	0.9	I	
I ^2 I σheight	0.1	'food vel	1.1	I	
Table 4: Parameters for context distribution used in assistive-gym					
I Name	Value	I Name	Value	I Name	Value I
I pmale	0.8	I 'vel	0.225	'high pressure	0.009 I
I μmass (male)	88.4	I uvel	0.275	uhigh pressure	0.111 I
I μmass (female)	72.5	'target	0.007	pnone	0.1 I
I 2 I σmass	20	utarget	0.016	I PlimitS	0.1 I
I μradius(male)	0.9	'high forces	0.035	pweakness	0.1 I
I μradius (female)	0.9	uhigh forces	0.06	ptremor	0.7 I
I σradius	0.2	I 'hit	0.4	I	
I μheight (male)	1.1	I uhit	2.1	I	
I μheight (female)	1.1	I 'food vel	0.4	I	
2 I σheight	0.2	'food vel	2.1	I	
Table 5: Parameters for one of the shifted context distribution used in assistive-gym
The values for each distribution are provided in Table 4. For setting covariate shift, we used a
a set of distributions that were shifted w.r.t. the default context distribution. We then sampled a
shifted distribution w.p. β and the default distribution w.p. 1 - β. That is, when β = 1, the user
sampled a context only from the shifted distribution. Table 5 shows an example of one of the shifted
distribution that were used.
Expert Data Generation. For the assistive-gym experiments we a dense reward function for gen-
erating the expert data and a sparse one for our experiments using the expert data. Specifically, the
dense reward function used the environment’s default reward function, defined by
wι ∙ distance to goal + w2 ∙ action + w3 ∙ task specific reward + w4 ∙ preference score,
where the preferences were weighted according to the context features. Specific weights are pro-
vided in the implementation of assistive-gym (Erickson et al., 2020). The sparse reward function
did not use the distance to goal (i.e., w1 = 0).
22
Published as a conference paper at ICLR 2022
F Missing Proofs
F.1 Proofs for Section 3
We begin by proving two auxilary lemmas.
Lemma 1. Let π2 ∈ Υπ1. Then, Υπ1 = Υπ2.
Proof. We show that Υπ1 ⊆ Υπ2 and Υπ2 ⊆ Υπ1.
Let π ∈ Υπ1, then dπ (s, a) = dπ1 (s, a). By our assumption, π2 ∈ Υπ1, then dπ2 (s, a) = dπ1 (s, a).
Hence, dπ(s, a) = dπ2 (s, a). That is, π ∈ Υπ2. This proves Υπ1 ⊆ Υπ2.
Similarly, let π ∈ Υπ2, then dπ(s, a) = dπ2 (s, a). By our assumption, π2 ∈ Υπ1 , then dπ2 (s, a) =
dπ1 (s, a). Hence, dπ(s, a) = dπ1 (s, a). That is, π ∈ Υπ1 . This proves Υπ2 ⊆ Υπ1, completing the
proof.	□
Lemma 2. Let ∏o be a determiniStic policy and let Mo = (S, A, X, P, ro, γ) such that
ro(s, a, x) = 1{a = ∏o(s, x)}. Then ∏o is the unique, optimal policy in Mo.
Proof. By definition ofπ0 and r0,
ro(s, πo(s, x), x) = 1, ∀s ∈ S, x ∈ X.
In particular, Eπ0 [ro(st, at, x)] = 1. Then
∞∞
VMo ≤ (I-Y) XYt = E∏o (I-Y) XYtrO(St,at,x) = VMIO.
t=o	t=o
This proves πo is an optimal policy. To prove uniqueness, assume by contradiction there exists an
optimal policy π1 6= πo . Then,
V ∏1 = Es,a,x 〜dπι (s,a,x)[1{a = π0 (S,X)H = Es,x 〜dπι (s,x) [Ea 〜∏ι (∙∣ s,x)[1{a = π0(s, x)}]] < 1 = VMO .
In contradiction to ∏ι is optimal. Then, ∏o is a unique optimal policy.	□
We are now ready to prove Theorem 1.
Theorem 1.	[Sufficiency of Υ∏*] Assume Pe ≡ ρ0. Let π* ∈ ∏M and let ∏o ∈ Υ∏*. Then,
Υ∏* = Υ∏o. Moreover if ∏o = ∏*, then there exists ro such that ∏o ∈ ∏Mcι but ∏* ∈ ∏Mo, where
Mo = (S, A, X, P, ro, ρo, ν, Y).
Proof. Let ∏ ∈ ∏M and let ∏o ∈ Υ∏*. By Lemma 1, as ∏o ∈ Υ∏*, it holds that Υ∏* = Υ∏0.
Next, choosing ro (S, a, x) = 1{a = πo (S, x)}, by Lemma 2 we get that πo is an optimal policy in
Mo. This proves ∏o ∈ ∏Mo. Finally, by Lemma 2, ∏Mo = {∏o}, proving π* ∈ ∏Mo if and only
if ∏ = ∏o.	0	0	0	□
Proposition 2. Define the mean policy ∏(a∣s,x) =	P Eπ£γP / )亓(§，ɑ0 方),and denote
α* = InMYTYn*| ∈ [0,1]. Then, vm(∏) ≥ α*v* + (1 — ɑ*)min∏∈γπ* VM(π).
Proof. Let ∏ as defined. Then by linearity of expectation
VM = Es,a,x〜dπ Ir(S, a, X)]
1
E Es,a,x〜d∏ [r(s,a,x)]
π∈Υπ*
1
VMπ.
π∈Υπ*
23
Published as a conference paper at ICLR 2022
Denote B* = ∏M ∩ Υ∏*, then
	VM=3 ∏X*VM+⅛∣ YnXB*VM =∣ΥJ∣ VM + 百 YX* VM ≥ JBIV* + ∣γ∏* ∖B*∣	min V∏ ≥ ∣γ∏*∣VM +	∣γ∏*∣ ∏∈γ∏*∖B* M ≥≡ VM + T ∏mn*VM, ∣ π* ∣	∣ π* ∣ π∈Yπ*
completing the proof.	□
Theorem 2.	[Catastrophic Imitation] Assume |X | ≥ |A|, and P (s0|s, a, x) = P(s0|s, a, x0) for all
x, x0 ∈ X. Then ∃πe,1, πe,2 s.t. {πe,1, πe,2} are non-identifiable, catastrophic expert policies.
Proof. We first sketch the proof for the special case X = {x0, x1}, A = {a0, a1} and a singleton
state space S = {s0}. The general proof follows similarly and is given below.
By letting π1, π2 be the determinisic policies which choose opposite actions at opposite contexts,
i.e., π1(xi) = ai, π2(xi) = a1-i, we can choose ρe(x) = d* (π1(x)) and ρee(x) = d* (π2(x)) which
yield
1
dρπe1 (a) =	ρe(xi)1{a = π1 (xi)}
i=0
2
= X d*(π1(xi))1{a = π1(xi)}
i=1
k
= X d* (ai)1{ai = a} := d* (a).
i=1
Similarly, dρπe2 (a) = d* (a).
For the second part of the proof choose r1 (a, x) = 1{x = xi, a = ai} and r2(a, x) =
1{x = xi , a = a1-i }. Notice that πi is optimal for ri under any distribution of contexts, yet πi
achieves zero reward for r1-i.
We now provide a complete proof for the general case.
Let ρo, d* (a). Without loss of generality, let X = {x0, . . . , xm}, A = {a0, . . . , ak} with m ≥ k,
and denote Xk Define	{x1, . . . , xk} ⊆ X. By definition there exists an injective function from A into X. f(x) = ai , x = xi , i = 0, . . . , k a0 , o.w. (x)	ai+1	(mod	k)	, x = xi, i	= 0, .	. . , k g	a0	, o.w.
Then we can select π1, π2, ρe, ρee as follows
∏ι(a∣x) = 1{a = f(x), X ∈ Xk} +
π2 (a|x) = 1{a = g(x), x ∈ Xk} +
1
k + 1
1
k + 1
1{x ∈/ Xk}
1{x ∈/ Xk},
and
ρe(x) = d* (f (x))1{x ∈ Xk},
ρee(x) = d* (g(x))1{x ∈ Xk}.
24
Published as a conference paper at ICLR 2022
We get that
m
d∏e(a) = EPe(Xi)∏ι(a∣Xi)
i=1
k
=X d*(f (Xi))I{a = f (xi)}
i=1
k
=)：d*(ai)1{ai = a} = d*(a).
i=1
Similarly,
k
dp2 (a) = X d*(g(xi))1{a = g(xi)}
i=1
k
=〉：d (ai+1 (mod k)) 1{ai+1 (mod k) = a}
i=1
k
=)：d*(ai)1{ai = a} = d*(a).
i=1
This proves the first part of the theorem. For the other parts, choose r1 , r2 as follows
r1 (a, X) = 1{X = Xi , a = ai, 0 ≤ i ≤ k}
r2(a, X) = 1 X = Xi, a = ai+1 (mod k), 0 ≤ i ≤ k .
Then, by definition, for any P(x) such that SUPP(P) ∩Xk = 0,
Ex〜P(x),a〜∏ι (∙∣x) [r1 (a, X)] = 1 = max Ex〜P(x),a〜π(∙∣x) [r1 (a, X)],
Ex〜P(x),a〜∏ι (Tx) [r2 (a, x)] = 0 = min Ex〜P(x),a〜π(∙∣x) [r2 (a, x)].
And similarly,
Ex〜P(x),a〜∏2(Tx)[r1(a，x)] = 0 = min Ex〜P(x),a〜π(∙∣x)[rl(a, x)],
Ex〜P(x),a〜∏2 (∙∣x) [r2 (a，x)] = 1 = max Ex〜P(x),a〜π(∙∣x) [r2 (a, x)].
The condition on the suPPort holds for ρe, ρee by definition. If, SuPP(ρo) ∩ Xk = 0, then the result
holds trivially as Ex〜ρ0(x),a〜∏(∙∣x)[rι(a,x)] = Ex〜ρ0(x),a〜∏(∙∣x) [r2(a,x)] = 0 for all π ∈ Π. This
completes the proof.	□
Lemma 3. Assume Supp(ρo) ⊆ Supp(ρe). Then
arg maχ Ex〜ρe(x),s,a〜dπ (s,a|x)[r(s,a, x)] ⊆ arg max Ex〜ρo(x),s,a〜dπ (s,a|x)[r(s, a, x)]
Proof. For clarity we denote
πρe = arg max Ex〜ρe(x),s,a〜dπ (s,a| x)[r( s,a, x)]
Πpo = argmax Ex 〜ρo(x),s,a 〜d∏(s,α∣x)[r(s,a,χ)]
∏Supp(ρe) =	×	argmax Es,a 〜d∏(s,α∣x)[r(s,a,x)].
x∈Supp(ρe)
To prove the lemma, We will show Πpe = ∏Supp3) ⊆ Π^o.
We begin by proving ∏^e = ∏supp(ρe). Indeed, let π* ∈ ∏Supp(Pe). Then, for any X ∈ SUPP(Pe)
Es,a〜d∏* (s,a∣x)[r(s, a, x)] = max Es,a〜d∏ (s,a∣x) [r(s, a, x)] ∙
25
Published as a conference paper at ICLR 2022
In particular,
Ex〜Pe(x),s,a〜dπ* (s,a∣x)[r(S, a,
x)] = Ex〜ρe(x) [maxEs,a〜d∏(s,a∣x)[r(s,a,x)]]
≥ m∏ax Ex〜ρe(x),s,a〜dπ* (s,a∣x) Ir(S, a, X)],
where We UsedJensen's inequality. This proves ∏Supp(ρe) ⊆ Π*e.
To see the other direction, let ∏ ∈ Πpe and assume by contradiction that ∏ ∈ πSupp(ρe). Then,
there exists X ∈ SUPP(Pe) SUch that
Es,a〜dπe (s,a|x)[r(s, a, x)] < m∏ɑx Es,a〜dπ (s,a㈤[r(s, a, x)].
Define
∏(∙∣s,x) = 1{x = X}∏χ(∙∣S, X) + 1{x = X}∏e(∙∣S, x),
where πx ∈ argmax∏ E§,a〜d∏(s,a∣χ)[r(s,a,X)]. Then,
v(∏e) = P (X = X)Es,a 〜d∏e (s,a∣X)[r(s, a,X)]+ E	P(X)Es,a 〜d∏e (s,a∣x) [r(s,a, x)]
x∈Supp(ρe)∖{定}
< P(X = X)Es,a 〜d/ (s,a|x)[r(s,a,X)] + E	P(X)Es,a 〜d∏e(s,a∣x) [r(s, a, x)] = v(n),
x∈Supp(ρe)∖{x}
in contradiction to ∏ ∈ Πpe. This proves Πpe ⊆ ∏Supp(ρe). We have thus shown that Π*e =
Π*	e
ΠSupp(ρe).
Finally, it is left to show that ∏Supp(ρe) ⊆ ∏po. Similar to before, let π* ∈ ∏Supp(ρe). Then, for any
X ∈ Supp(ρe), by Jensen’s inequality
Ex〜Po(x),s,a〜dπ* (s,a∣x) [r(s, a,
x)] = Ex〜ρo(x) [maχ Es,a〜dπ (s,a∣x)[r(s, a, x)]]
≥ m∏αχ Ex〜ρo(x),s,a〜dπ* (s,a∣x) [r(s,a,X)].
This completes the proof.	□
Theorem 3.	[Sufficiency of Context-Free Reward] Assume Supp(ρo) ⊆ Supp(ρe) and
r(s, a, x) = r(s, a, X0) for all x, x0 ∈ X. Then Υ∏* ⊆ ∏M.
Proof. Let ∏o ∈ Υ∏*, we will show ∏o ∈ ∏M. Since r(s, a, x) = r(s,a,x0) for all X ∈ X we
denote r(S, a) = r(S, a, X). By definition of Υπ* we have that.
dρπo0 (S, a) = dρπe* (S, a)
Then,
v(πθ) = Ex〜ρo(x),s,a〜dπ0 (s,a∣x)[r(s, a)]
=Ex〜ρo(x)	E dπ0(s,a | x)r(s,a)
s∈S,a∈A
=E r(s, a)Ex〜ρo(x)[dπ0 (S, a । x)]
s∈S,a∈A
=Es,a 〜dπ0 (s,α)[r(s,a)]
=Es,a〜dn* (s,α) [r(s, a)]
=Ex〜ρe(x),s,a〜dπ* (s,α∣x) [r(s, a)]
=max Ex〜ρe(x),s,a〜dπ(s,a∣x) [r(s, a)]
Then, ∏o ∈ argmax∏ Ex〜ρe(x),s,a〜d∏(s,a∣x) [r(s,a)]. Applying Lemma 3
π0 ∈ arg max Ex〜ρo(x),s,a〜dπ (s,a| x)[r(s, a)] = πm ,
completing the proof.
□
26
Published as a conference paper at ICLR 2022
F.2 Proofs for Section 4
Proposition 1. [Trajectory Sampling Equivalence] Let Ps which minimizes Problem (P2)for some
π ∈ Π, g : S × A 7→ R, and assume Supp(ρo) ⊆ Supp(ρe). Then, there exists pn ∈ ∆n such that
dπ (s,a) =nl→∞ Ei〜Pn [(I-Y) P∞=0 Y'1](Si,a^ = (s,a)}].
Proof. We can write
∞
dπ (S, a | x) = (1 - Y)	YtP (St = S, at = a|x)
t=0
∞
= (1 - Y) XX
YtP(st = s, at = a|x, τ)P(τ|x)
∞
=(1-Y)XXYt1{τt = (s, a)}P (τ |x).
Then, denoting 耳(T) = Ex〜PW [P(τ | x) ], We get that
∞
dρπsW (s, a) = (1 - Y)XXYt1{τt = (s, a)}PρπsW (τ)
τ t=0
∞
=ET〜PnJ (I-Y) X Yt1{τt = (S,a)}.
s	t=0
Since, Supp(ρo)	⊆	Supp(ρe), there exists pn	∈	∆n such that
Ei〜Pn [(1 - Y) P∞=o y'1{st, at = (s,a)}] is an unbiased estimator of d% (s, a). The result
follows by the law of large numbers.	□
Theorem 5.	Let ALG-RL be an approximate best response player that solves the RL problem in
iteration k to accuracy Ek = %. Then, Algorithm 1 will converge to an E-optimal solution to
Problem (P2) in O(表)samples.
Proof. We begin by showing that h(P) = minx∈∆n Df(P||Ex[Qx]) is convex in P. We can write
Df in its variational form, rewriting h(P) as
h(P) = min maxEz〜P[g(z)] - Ex,z〜Qχf *(g(z))],
x∈∆n g:Z 7→R
where
fs (w) = sup{yw - f(y)}.
We have that Ez〜P[g(z)] - Ex,z〜qx [f s(g(z))] is affine in g and x. Therefore, strong duality holds,
yielding
h(P) = max min Ez〜P[g(Z)] - Ex,z〜Qx[f*(g(Z))]
giZ→R x∈∆n
max < Ez〜P [g(z)] +
g:Z 7→R
max
x∈∆n
Ex,z 〜Qx[f *(g(ζ))])}
We have that maxx∈∆n Ex,z〜Qx [f s (g(ζ))] is convex in g as a maximum over convex (affine) func-
tions in a compact set. Therefore h(P) is also convex as a maximum over convex functions.
Then, the objective in Problem (P2) is convex in dρπ . Following the meta algorithm framework
for convex RL in Zahavy et al. (2021), we write the gradient of Df (d2(s, 0)|吗；(s, a)). Notice
that for any general f -divergence Df(xi||yi) = Eyi
[f (Xi)]
it holds that
Nxj Df(Xi||yi) = 0, j = i,
27
Published as a conference paper at ICLR 2022
and
Vxi Df(XiIIyi)=VxiEyife) ]= EyiB Vzf ⑶ 1z=yi
Specifically, for the KL-divergence, DKL(Pi\\qi) = -Eqi [log(Pi)]. Then,
VpiDKL(piIIqi) = Eqi
-1
Pi
Applying Lemma 2 of Zahavy et al. (2021) with a Follow the Leader (FTL) cost player completes
the proof.	□
F.3 Proofs of Additional Results in Appendix
Proposition 3. Assume Pe ≡ Po and ∖Υ∏*∖ < ∞. Then there exists λ* > 0 such that for any
λ ∈ (0, λ*), Algorithm 3 (with δ = 0 sensitivity) will return π of Proposition 2 after exactly ∖Υ∏* ∖
iterations.
Proof. Denote
λ;
max
π∈Πdet,π6∈Υπ*,π0∈Υπ*
λ2
min
π∈Πdet,π6∈Υπ*
*
dTV(dρπo(s, a), dρπe (s, a)),
where dτv is the total variation distance. Let λ* = λ** and λ ∈ (0, λ*) and notice that λ* > 0.
To prove the result., we will show that at iteration n of the algorithm πn ∈ Υπ* and that either
n-1
πn ∈/ Υn-1 := {πj }j=1 or Υn-1 = Υπ* .
Base case (n = 1). By the variational representation of thef-divergence,
*
gθ.maA→R Es,a〜d7 (s,a)[g0(S,a)] — Es,a〜d∏* (s,a) f (g。Ga))] = dTV (dp。(S,a) ,dQe(S,a)).
By definition Υπ* = arg minπ∈Πdet dTV (dρπo (s, a)∖∖dρπe* (s, a)). Then, π1 ∈ Υπ* . Finally since
Υo = 0, We have that ∏ι ∈ Υ0.
Induction step. Suppose the claim holds for some n = k. We will show it holds for n = k + 1.
We begin by showing that πk+1 ∈ Υπ* . Assume by contradiction that πk+1 ∈ Πdet , πk+1 ∈/ Υπ* .
Using the variational form of thef-divergence,
max
gi-.S×A×X
Li(∏k+ι; gi) = dτv (d∏k+1 (s,a,x),d∏i (s,a,x)) ≤ λf,
We have that
max
go-.S×A
*
L (∏k+ι; go) = dτv(d∏k+1 (s, a), d∏e (s, a)) ≥ λ.
max	L*(∏k+ι; go) 一 λminLi(∏k+ι; gi) ≥ λ2, — λλ] > λ寸—λ*λj = 0.
go:S×A→R,	i
gi = S×A×X→R
Next, let ∏k+ι ∈ Υ∏*, then
max
g0 :S ×A7→R,
gi :S ×A×X 7→R
L*(∏k+ι;go) — λminLi(∏k+ι;gi) ≤ 0,
i
where we used the fact that L (∏k+ι; go) = 0 by definition of Υ∏*, and Li ≥ 0. We have reached a
contradiction to πk+1 being a solution to Equation (4). This proves that πk+1 ∈ Υπ* .
28
Published as a conference paper at ICLR 2022
Finally, We show that ∏k+ι / Yk if and only if Yk= Υ∏*. First, notice that if Yk = Υ∏*
then Equation (4) will return πk+1 ∈ Υk by definition of the total variation distance. Next, as-
sume Yk 6= Yπ* and assume by contradiction πk+1 ∈ Yk. Then, ∃i : maxgi Li (πk+1; gi) = 0, and
maXgo∙s×A→R L (∏k+ι; go) = 0, by definition of Y∏*. Hence,
max	L^(∏k+ι; go) 一 λ min Li(∏k+ι; gi) = 0.
g0 :S ×A7→R,	i
gi :S ×A×X 7→R
In contrast, since Yk = Y∏*, there exists ∏ / Y∏* such that ∏ / Yk, and
max	L*(∏; go) 一 λmin Liln gi) ≤ X； < 0,
g0 :S ×A7→R,	i
gi=S×A×X→R
in contradiction to ∏k+ι being a solution Equation (4). This completes the proof.	□
Theorem 6.	[Sufficiency of Y∏-1 ] Let Assumption 2 holdfor some Γ ≥ 1. Then n； / Y∏-1.
Proof. Let π ∈ Π. We will show that π ∈ YπΓ-1. By elementary algebra, we have that, under
Assumption 2,
Po(x)(1 一 Γ-1) + Γ-1 ≤ 哈 ≤ Po(x)(1 一 Γ)+Γ.
ρe(x)
Since Supp(ρe) ⊆ Supp(ρo),
d∏o(s, a) = Ex〜ρo(χ)[dπ(s,a 1 x)]
=Ex〜ρe(x) [00∖ d dπ (s, a 1 x)
ρe(x)
≤ Ex 〜Pe(x)[(Pθ (x)(1 — r)+r)dπ(s,a । x)].
Subtracting dρπ from both sides we get that
dπo(s, a) — dρe(s, a) ≤ Ex 〜ρe(x)[(ρo (x)(1 一 γ)+γ — 1)dπ (S, a | x)]
=(r - 1)Ex〜ρe(x)[(1 — PO(Xy)d (s,a | x)]
≤ Γ 一 1.
Similarly,
d∏o ≥ Ex〜Pe(x) [(ρo(x)(1 - Γ-1) + Γ-1)dπ(s, a | x)].
Hence,
dπo (s, a) 一 dπe (s, a) ≥ Ex 〜ρe(x) [(ρo (X)(I- γ 1)+γ 1 — 1)dπ(s,a | x)]
=(γ 1 一 1)Ex〜ρe(x) [(1 一 PO(X))dπ(s, a | x)]
≥ 一(1 一 Γ-1)
≥ 一(「- 1)
where the last two transitions hold since Γ ≥ 1. Then, we have that
dρπo (s, a) 一 dρπe (s, a) ≤ Γ 一 1.
This completes the proof.	□
Theorem 7.	[Context Dependent Reward] Let : X 7→ R of Definition 6. Denote
toe = Ex〜ρ0(x)k(x)]+ Ex〜ρe(x)k(x)]. Thenforany n； / ∏M, ∏o / Y∏*
v(∏o) ≥ v(n；) 一 toe.
29
Published as a conference paper at ICLR 2022
Proof. Let π* ∈ ∏M, ∏o ∈ Υ∏*. The proof follows similar steps to that of Theorem 3.
v(π0) = Ex〜ρo(x),s,a〜dπ0 (s,a∣x)[r(S, a, X)]
=Ex〜ρo(x),s,a〜dπ0 (s,a∣x)[r(s, a, X) - r0(s, a)] + Ex〜Po(x),s,a〜dπ0 (s,a∣x) [r0 (s, a)]
=Ex〜ρo(x),s,a〜dπ0 (s,a∣x)[r (S, a, X) - r0(s, a)] + Ex〜Po(x)	): d ° (s, a | x)r0 (s, a)
s∈S,a∈A
=Ex〜ρo(x),s,a〜d∏0(s,a∣x)[r(s, a, x) - rο(s,a)] + E	rο(s,a)Ex〜ρo(x) [dπ0 (s,a | x)]
s∈S,a∈A
=Ex〜ρo(x),s,a〜d∏ο(s,a∣x)[r(s,a,χ) - r0(s,a)] + Es,a〜d20 (s,α) [r0 (s, a)]
=Ex〜ρo(x),s,a〜d∏0(s,α∣x)[r(s, a,x) - r0(s,a)] + Es,a〜d∏* (s,α) [r0 (s, a)]
=Ex〜ρo(x),s,a〜d∏0(s,α∣x)[r(s, a, x) - rο(s,a)] +Ex〜Pe(x),s,a〜dπ* (s,α∣x) [r0(s, a)].
Then,
v(π*) - v(π0) = Ex〜Pe(x),s,a〜d∏* (s,a∣x) [r(s, a, x) - r0(s, a)] - Ex〜ρo(x),s,a〜d∏0 (s,α∣x) [r(s, a, x) - r0(s, a)]
≤ Ex〜Pe(x) [E(x)] + Ex〜ρo (x) k(x)],
completing the proof.	□
30