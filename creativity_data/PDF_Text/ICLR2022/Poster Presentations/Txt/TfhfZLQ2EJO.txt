Published as a conference paper at ICLR 2022
SURF: Semi-supervised Reward Learning with
Data Augmentation for Feedback-efficient
Preference-based Reinforcement Learning
Jongjin Park1 Younggyo Seo1 Jinwoo Shin1 Honglak Lee2,4 Pieter Abbeel3 Kimin Lee3
1KAIST 2University of Michigan 3UC Berkeley 4LG AI Research
Ab stract
Preference-based reinforcement learning (RL) has shown potential for teaching
agents to perform the target tasks without a costly, pre-defined reward function by
learning the reward with a supervisor’s preference between the two agent behav-
iors. However, preference-based learning often requires a large amount of human
feedback, making it difficult to apply this approach to various applications. This
data-efficiency problem, on the other hand, has been typically addressed by using
unlabeled samples or data augmentation techniques in the context of supervised
learning. Motivated by the recent success of these approaches, we present SURF,
a semi-supervised reward learning framework that utilizes a large amount of un-
labeled samples with data augmentation. In order to leverage unlabeled samples
for reward learning, we infer pseudo-labels of the unlabeled samples based on the
confidence of the preference predictor. To further improve the label-efficiency of
reward learning, we introduce a new data augmentation that temporally crops con-
secutive subsequences from the original behaviors. Our experiments demonstrate
that our approach significantly improves the feedback-efficiency of the state-of-
the-art preference-based method on a variety of locomotion and robotic manipu-
lation tasks.
1	Introduction
Reward function plays a crucial role in reinforcement learning (RL) to convey complex objectives to
agents. For various applications, where we can design an informative reward function, RL with deep
neural networks has been used to solve a variety of sequential decision-making problems, including
board games (Silver et al., 2017; 2018), video games (Mnih et al., 2015; Berner et al., 2019; Vinyals
et al., 2019), autonomous control (Schulman et al., 2015; Bellemare et al., 2020), and robotic ma-
nipulation (Kober & Peters, 2011; Kober et al., 2013; Kalashnikov et al., 2018; Andrychowicz et al.,
2020). However, there are several issues in reward engineering. First, designing a suitable reward
function requires more human effort as the tasks become more complex. For example, defining a
reward function for book summarization (Wu et al., 2021) is non-trivial because it is hard to quan-
tify the quality of summarization in a scale value. Also, it has been observed that RL agents could
achieve high returns by discovering undesirable shortcuts if the hand-engineered reward does fully
specify the desired task (Amodei et al., 2016; Hadfield-Menell et al., 2017; Lee et al., 2021a). Fur-
thermore, there are various domains, where a single ground-truth function does not exist, and thus
personalization is required by modeling different reward functions based on the user’s preference.
Preference-based RL (Akrour et al., 2011; Christiano et al., 2017; Ibarz et al., 2018; Lee et al.,
2021a) provides an attractive alternative to avoid reward engineering. Instead of assuming a hand-
engineered reward function, a (human) teacher provides preferences between the two agent behav-
iors, and an agent learns how to show the desired behavior by learning a reward function, which is
consistent with the teacher’s preferences. Recent progress of preference-based RL has shown that
the teacher can guide the agent to perform novel behaviors (Christiano et al., 2017; Stiennon et al.,
2020; Wu et al., 2021), and mitigate the effects of reward exploitation (Lee et al., 2021a). However,
existing preference-based approaches often suffer from expensive labeling costs, and this makes it
hard to apply preference-based RL to various applications.
1
Published as a conference paper at ICLR 2022
Meanwhile, recent state-of-the-art system in computer vision, the label-efficiency problem has been
successfully addressed through semi-supervised learning (SSL) approaches (Berthelot et al., 2019;
2020; Sohn et al., 2020; Chen et al., 2020b). By leveraging unlabeled dataset, SSL methods have
improved the performance with low cost. Data augmentation also plays a significant role in im-
proving the performance of supervised learning methods (Cubuk et al., 2018; 2019). By using mul-
tiple augmented views of the same data as input, the performance has been improved by learning
augmentation-invariant representations.
Inspired by the impact of semi-supervised learning and data augmentation, we present SURF: a
Semi-sUpervised Reward learning with data augmentation for Feedback-efficient preference-based
RL. To be specific, SURF consists of the following key ingredients:
(a)	Pseudo-labeling (Lee, 2013; Sohn et al., 2020): We leverage unlabeled data by utilizing the
artificial labels generated by learned preference predictor, which makes the reward function
produce a confident prediction (see Figure 1a). We remark that such a SSL approach is par-
ticularly attractive in our setup as an unlimited number of unlabeled data can be obtained with
no additional cost, i.e., from past experiences stored in the buffer.
(b)	Temporal cropping augmentation: We generate slightly shifted or resized behaviors, which are
expected to have the same preferences from a teacher, and utilize them for reward learning (see
Figure 1b). Our data augmentation technique enhances the feedback-efficiency by enforcing
consistencies (Xie et al., 2019; Berthelot et al., 2020; Sohn et al., 2020) to the reward function.
We remark that SURF is not a naive application of these two techniques, but a novel combination
of semi-supervised learning and the proposed data augmentation, which has not been considered or
evaluated in the context of the preference-based RL.
Our experiments demonstrate that SURF significantly improves the preference-based RL
method (Lee et al., 2021a) on complex locomotion and robotic manipulation tasks from DeepMind
Control Suite (Tassa et al., 2018; 2020) and Meta-world (Yu et al., 2020), in terms of feedback-
efficiency. In particular, our framework could make RL agents achieve ~100% of success rate on
complex robotic manipulation task using only a few hundred preference queries, while its baseline
method only achieves ~50% of the success rate under the same condition (see Figure 2). Further-
more, we show that SURF can improve the performance of preference-based RL algorithms when
we operate on high-dimensional and partially-observable inputs.
2	Related work
Preference-based RL. In the preference-based RL framework, a (human) supervisor provides pref-
erences between the two agent behaviors and the agent uses this feedback to perform the task (Chris-
tiano et al., 2017; Ibarz et al., 2018; Leike et al., 2018; Stiennon et al., 2020; Wu et al., 2021; Lee
et al., 2021a;b). Since this approach is only feasible if the feedback is practical for a human to pro-
vide, several strategies have been studied in the literature. Ibarz et al. (2018) initialized the agent’s
policy with imitation learning from the expert demonstrations, while Lee et al. (2021a) utilized un-
supervised pre-training for policy initialization. Several sampling schemes (Sadigh et al., 2017;
Biyik & Sadigh, 2018; Biyik et al., 2020) to select informative queries also have been adopted for
improving the feedback-efficiency. Our approach differs in that we utilize unlabeled samples for
reward learning, and also provide a novel data augmentation technique for the agent behaviors.
Data augmentation for RL. In the context of RL, data augmentation has been widely investigated
for improving data-efficiency (Srinivas et al., 2020; Yarats et al., 2021), or RL generalization (Cobbe
et al., 2019; Lee et al., 2019). For example, RAD (Laskin et al., 2020) demonstrated that data
augmentation, such as random crop, can improve both data-efficiency and generalization of RL
algorithms. While these methods are known to be beneficial to learn policy in the standard RL
setup, they have not been tested for learning rewards. To the best of our knowledge, we present the
first data augmentation method specially designed for learning reward function.
Semi-supervised learning. The goal of semi-supervised learning (SSL) is to leveraging unlabeled
samples to improve a model’s performance when the amount of labeled samples are limited. In
an attempt to leverage the information in the unlabeled dataset, a number of techniques have been
proposed, e.g., entropy minimization (Grandvalet & Bengio, 2004; Lee, 2013) and consistency reg-
ularization (Sajjadi et al., 2016; Miyato et al., 2018; Xie et al., 2019; Sohn et al., 2020). Recently,
2
Published as a conference paper at ICLR 2022
(a) Pseudo-labeling
(b) Temporal cropping
Figure 1: Overview of SURF. (a) We leverage unlabeled experiences by generating pseudo-labels yb
from the preference predictor Pψ in (1). To mitigate the negative effects from this semi-supervised
learning, we only utilize pseudo-labels when the confidence of the predictor is higher than threshold
τ. (b) Given two segments (σ0, σ1), we generate augmented segments (σb0, σb1) by cropping the
subsequence from each segment.
the combination of these two approaches have shown state-of-the-art performance in benchmarks,
e.g., MixMatch (Berthelot et al., 2019), and ReMixMatch (Berthelot et al., 2020), when used with
advanced data augmentation techniques (Zhang et al., 2018; Cubuk et al., 2019). Specifically, Fix-
Match (Sohn et al., 2020) revisits pseudo-labeling technique and demonstrates that joint usage of
pseudo-labels and consistency regularization achieves remarkable performance due to its simplicity.
3	Preliminaries
Reinforcement learning (RL) is a framework where an agent interacts with an environment in dis-
crete time (Sutton & Barto, 2018). At each timestep t, the agent receives a state st from the en-
vironment and chooses an action at based on its policy π(at |st). In conventional RL framework,
the environment gives a reward r(st, at) and the agent transitions to the next state st+1. The return
Rt = Pk∞=0 γkr(st+k , at+k ) is defined as discounted cumulative sum of the reward with discount
factor γ ∈ [0, 1). The goal of the agent is to learn a policy that maximizes the expected return.
Preference-based reinforcement learning. In this paper, we consider a preference-based RL
framework, which does not assume the existence of hand-engineered reward. Instead, a (human)
teacher provides preferences between the agent’s behaviors and the agent uses this feedback to per-
form the task (Christiano et al., 2017; Ibarz et al., 2018; Leike et al., 2018; Stiennon et al., 2020; Lee
et al., 2021a;b; Wu et al., 2021) by learning a reward function, which is consistent with the observed
preferences.
We formulate a reward learning problem as a supervised learning problem (Wilson et al., 2012;
Christiano et al., 2017). Formally, a segment σ is a sequence of observations and actions
{(sk, ak), ..., (sk+H-1, ak+H-1)}. Given a pair of segments (σ0, σ1), a teacher gives a feedback
indicating which segment is preferred, i.e., y ∈ {0, 1, 0.5}, where 1 indicates σ1 σ0 , 0 indicates
σ0 σ1 , and 0.5 implies an equally preferable case. Each feedback is stored in a dataset D as a
triple (σ0, σ1, y). Then, we model a preference predictor using the reward function rbψ following the
Bradley-Terry model (Bradley & Terry, 1952):
P Γτ 1 , 〃0 ] =	eχp(Pt r (S1, aI))
ψ[	] = Pi∈{0,i} exp(Ptb(st,at)),
(1)
where σi σj denotes the event that segment i is preferable to segment j . The underlying assump-
tion of this model is that the teacher’s probability of preferring a segment depends exponentially on
the accumulated sum of the reward over the segment. The reward model is trained through super-
vised learning with teacher’s preferences. Specifically, given a dataset of preferences D, the reward
function is updated by minimizing the binary cross-entropy loss:
LCE= 0	1E	hLRewardi	=- 0	1E	h(1-y)logPψ[σ0σ1]+ylogPψ[σ1σ0]i.
(σ0 ,σ1,y)〜D	(σ0,σ1,y)〜D
The reward function rbψ is usually optimized only using labels from real human, which are expensive
to obtain in practice. Instead, we propose a simple yet effective method based on semi-supervised
learning and data augmentation to improve the feedback-efficiency of preference-based learning.
3
Published as a conference paper at ICLR 2022
Algorithm 1 SURF
Require: Hyperparameters: unlabeled batch ratio μ, threshold parameter T, and loss weight λ
Require: Set of collected labeled data Dl, and unlabeled data Du
1:	for each gradient step do
2:	Sample labeled batch {(σ0 ,σ1,y)(i)}B=ι 〜Dl
3:	Sample unlabeled batch {(σU, σ±)(j) }μ=B1 〜Du
4:	// Data augmentation for labeled data
5:	for i in 1 . . . B do
6:	(b0,b1)(i) - TDA((σ0,σl1)(i)) in Algorithm 2
7:	end for
8:	// Pseudo-labeling and data augmentation for unlab eled data
9:	for j in 1... μB do
10:	Predict pseudo-labels yb((σu0 , σu1)(j))
11:	(bu,b1 )(j) ― TDA((σ0,σu)(j)) in Algorithm 2
12:	end for
13:	Optimize LSSL (3) with respect to ψ
14:	end for
4 SURF
In this section, we present SURF: a Semi-sUpervised Reward learning with data augmentation
for Feedback-efficient preference-based RL, that can be used in conjunction with any existing
preference-based RL methods. Our main idea is to leverage a large number of unlabeled samples
collected from environments for reward learning, by inferring pseudo-labels. To further increase the
effective number of training samples, we propose a new data augmentation that temporally crops the
subsequence of the agent behaviors. The full procedure of our unified framework in Algorithm 1
(See Figure 1 for the overview of our method).
4.1	Semi-supervised reward learning
To improve the feedback efficiency, we propose a semi-supervised learning (SSL) method for lever-
aging unlabeled experiences in the buffer for reward learning. In addition to a labeled dataset
Dl = {(σl0, σl1, y)(i)}iN=l1, we utilize an unlabeled dataset Du = {(σu0, σu1)(i)}iN=u1 to optimize the
reward model rψ.1 Specifically, we generate the artificial labels ybby pseudo-labeling (Lee, 2013;
Sohn et al., 2020) for the unlabeled dataset Du. We infer a preference yb for an unlabeled segment
pair (σu0, σu1) as a class with higher probability as follows:
0 1	0, if Pψ [σu0	σu1] >0.5
yb(σu,σu)=	1, otherwiuse. u	(2)
By generating labels from the prediction model, we can obtain free supervision for optimizing our
reward model. However, pseudo-labels from low-confidence predictions can be inaccurate, and such
noisy feedback can significantly degrade the peformance of preference-based learning (Lee et al.,
2021b). To filter out inaccurate pseudo-labels, we only use unlabeled samples for training when the
confidence of the predictor is higher than a pre-defined threshold (Rosenberg et al., 2005). Then the
reward model rψ is optimized by minimizing the following objective:
LSSL= CIJE	hLReward(σ0,σ1,y) + λ ∙ LReward (σ[ ,σ1 ,y) ∙ l(Pψ [σ∕ A σ1-k*] > τ)],⑶
(σ0,σ1,y)〜Di，L
(σ0 ,σU)~Du
where k = arg maxj∈{0,1} b(j) is an index of the preferred segment from the pseudo-label, λ is a
hyperparameter that balances the losses, and τ is a confidence threshold. Training with the pseudo-
labels encourages the model to output more confident predictions on unlabeled samples. This can
be seen as a form of entropy minimization (Grandvalet & Bengio, 2004), which is essential to the
success of recent SSL methods (Berthelot et al., 2019; 2020). The entropy minimization can improve
the reward learning by forcing the preference predictor to be low-entropy (i.e., high-confidence) on
1The unlabeled dataset Du is not constrained to a fixed size since one can collect those unlabeled samples
flexibly by sampling arbitrary pairs of experiences from the buffer.
4
Published as a conference paper at ICLR 2022
Algorithm 2 TDA: Temporal data augmentation for reward learning
Require: Minimum and maximum length Hmin and Hmax , respectively, for cropping
Require: Pair of segments (σ0, σ1) with length H
1:	σ0 = {(s00, a00), ..., (s0H -1, a0H-1)}
2:	σ1 = {(s10, a10), ..., (s1H -1, a1H-1)}
3:	Sample H0 from a range of [Hmin, Hmax]
4:	Sample k0, k1 from a range of [0, H - H0]
5:	// RANDOMLY CROP A SEQUENCE WITH LENGTH H0
6:	b0 -{(SkO,ako),…,(Sko+H0-ι,ako+H0-ι)}
7:	σ ― {(Sk1, akI),…，(Skι+H0-1, akι+H0-1) }
8:	Return (σb0, σb1)
unlabeled samples. During training, we sample a larger minibatch of unlabeled samples than labeled
ones by a factor of μ following (Sohn et al., 2020), since unlabeled samples with low confidence are
dropped within minibatch.
4.2 Temporal data augmentation for reward learning
To further improve the feedback-efficiency in preference-based RL, we propose a new data aug-
mentation technique specially designed for reward learning. Specifically, for a given two segments
and preference (σ0, σ1, y), we generate augmented segments (σb0, σb1, y) by cropping the subse-
quence from each segment (see Algorithm 2 for more details).2 Then, we utilize augmented sam-
ples (σb0, σb1) to optimize the cross-entropy loss in (3). The intuition behind the augmentation is
that for a given pair of behavior clips, the human teacher may keep their relative preferences for
slightly shifted or resized versions of them. In the context of SSL, data augmentation is also related
to consistency regularization (Xie et al., 2019; Sohn et al., 2020) approaches that train the model
to output similar predictions on augmented versions of the same sample. Namely, this temporal
cropping method enables our framework can also enjoy the benefits of consistency regularization.
5	Experiments
We design our experiments to investigate the following:
◦	How does SURF improve the existing preference-based RL method in terms of feedback effi-
ciency?
◦	What is the contribution of each of the proposed components in SURF?
◦	How does the number of queries affect the performance of SURF?
◦	Is temporal cropping better than existing state-based data augmentation methods in terms of
feedback efficiency?
◦	Can SURF improve the performance of preference-based RL methods when we operate on
high-dimensional and partially observable inputs?
5.1 Setups
We evaluate SURF on several complex robotic manipulation and locomotion tasks from Meta-
world (Yu et al., 2020) and DeepMind Control Suite (DMControl; Tassa et al. 2018; 2020), respec-
tively. Similar to prior works (Christiano et al., 2017; Lee et al., 2021a;b), in order to systemically
evaluate the performance, we consider a scripted teacher that provides preferences between two tra-
jectory segments to the agent according to the underlying reward function.3 Since preferences of
the scripted teacher exactly reflects ground truth reward of the environment, one can evaluate the
algorithms quantitatively by measuring the true return.
2The length of the cropped segment is generated randomly across the batch but the same for segment pairs,
because the preference predictor uses the accumulated sum of the reward over time.
3While utilizing preferences from the human teacher is ideal, this makes hard to evaluate algorithms quan-
titatively and quickly.
5
Published as a conference paper at ICLR 2022
O
0.0
SAC with ground truth reward
PEBBLE (feedback=10000) + SURF
pebble (feedback=10000)
5 0 5
7 5 2
(求)eωωωuυ=ω
0.5	1.0	1.5	2.0
Environment Steps (×106)
50
(％)eωωωuυ=ω
O
0.0	0.25	0.5	0.75
1.0
Environment Steps (×106)
0 5 0 5
17 5 2
(％)eωωωuυ=ω
O
0.0	0.25	0.5	0.75
1.0
Environment Steps (×106)
(a) Hammer	(b) Door Open	(c) Button Press
SAC with ground truth reward
PEBBLE (feedback=10000) + SURF
pebble (feedback=10000)
O
(求)se°:SSBbnS
Environment Steps (×106)
O
(求)se°:SSBbnS
SAC with ground truth reward
PEBBLE (feedback=4000) + SURF
PEBBLE (feedback=4000)
0 5 0 5
0 7 5 2
O - l l
0.0	0.25	0.5	0.75
(求)se°:SSBbnS
On I
0.0 0.1 0.2 0.3 0.4 0.5
Environment Steps (×106)
1.0
Environment Steps (×106)
(d) Sweep Into	(e) Drawer Open	(f) Window Open
Figure 2:	Learning curves on robotic manipulation tasks as measured on the success rate. The solid
line and shaded regions represent the mean and standard deviation, respectively, across five runs.
We remark that SURF can be combined with any preference-based RL algorithms by replacing the
reward learning procedure of its backbone method. In our experiments, we choose state-of-the-
art approach, PEBBLE (Lee et al., 2021a), as our backbone algorithm. Since PEBBLE utilizes
SAC (Haarnoja et al., 2018) algorithm to learn the policy, we also compare to SAC using the ground
truth reward directly, as an upper bound of PEBBLE and our method. We note that our goal is not
to outperform SAC, but rather to perform closely using as few preference queries as possible.
Implementation details of SURF. For all experiments, we use the same hyperparameters used by
the original SAC and PEBBLE algorithms, such as learning rate of neural networks and frequency of
the feedback session. For query selection strategy, we use the disagreement-based sampling scheme,
which selects queries with high uncertainty, i.e., ensemble disagreement (see Appendix B for more
details). At each feedback session, we sample unlabeled samples as 10 times of labeled ones by
uniform sampling scheme, unless otherwise noted. Although we only use such amount of unlabeled
samples for time-efficient training, we note that one can utilize much more unlabeled samples as
needed. For the hyperparameters of SURF, we fix the loss weight λ = 1, and unlabeled batch ratio
μ = 4 for all experiments, and use threshold parameter T = 0.999 for Window Open, Sweep Into,
Cheetah tasks, and τ = 0.99 for the others. We provide more experimental details in Appendix B.
Extension to visual control tasks. To further demonstrate the effectiveness of our method, we
also provide experimental results on visual control tasks, where each observation is an image of
84 × 84 × 3. Specifically, we choose DrQ-v2 (Yarats et al., 2022), a state-of-the-art pixel-based RL
approach on DMControl, as a backbone algorithm for PEBBLE and SURF. Similar to experiments
with state-based inputs, we also compare to DrQ-v2 using ground truth reward as an upper bound.
5.2 Benchmark tasks with scripted teachers
Meta-world experiments. Meta-world consists of 50 robotic manipulation tasks, which are de-
signed for learning diverse manipulation skills. We consider six tasks from Meta-world, to investi-
gate how SURF improves a preference-based learning method on a range of complex robotic ma-
nipulation tasks (see Figure 7 in Appendix B). Figure 2 shows the learning curves of SAC, PEBBLE
6
Published as a conference paper at ICLR 2022
u-InMaa,pos5d山
1,000
750
500
250
SAC with ground truth reward
PEBBLE (feedback=100) + SURF
pebble (feedback=100)
0
0.0 0.1
0.2 0.3 0.4 0.5
Environment Steps (×106)
u-InMaa,pos5d山
1,000
750
500
250
O
0.0 0.25	0.5	0.75
u-ln"0=α,pos5d山
1,000
750
500
250
— SAC with ground truth reward
PEBBLE (feedback=1000) + SURF
PEBBLE (feedback=1000)
O
0.0 0.25
0.5	0.75	1.0
Environment Steps (×106)
1.0
Environment Steps (×106)
(a) Walker
(b) Cheetah
(c) Quadruped
Figure 3:	Learning curves on locomotion tasks as measured on the ground truth reward. The solid
line and shaded regions represent the mean and standard deviation, respectively, across five runs.
and SURF (which combined with PEBBLE) on the manipulation tasks. In each task, PEBBLE and
SURF utilize the same number of preference queries for fair comparison. As shown in the figure,
SURF significantly improves the performance of PEBBLE given the same number of feedback on
all tasks we considered, and matches the performance of SAC using the ground truth reward on four
tasks. For example, we find that when using 400 preference queries, SURF (red) reaches the same
performance as SAC (green) while PEBBLE (blue) is far behind to SAC on Window Open task.
We also observe that SURF achieves similar performance to PEBBLE with much less labels. For
example, to achieve comparable performance to SAC on Window Open task, PEBBLE needs 2,500
queries (reported in (Lee et al., 2021a)), requiring about 6 times more queries than SURF. These
results demonstrate that SURF significantly reduces the feedback required to solve complex tasks.
DMControl experiments. For locomotion tasks, we choose three complex environments from
DMControl: Walker-walk, Cheetah-run, and Quadruped-walk. Figure 3 shows the learning curves
of the algorithms with same number of queries. We find that using a budget of 100 or 1,000 queries
(which takes only few human minutes), SURF (red) could significantly improve the performance of
PEBBLE (blue). These results again demonstrate that that SURF improves the feedback-efficiency
of preference-based RL methods on a variety of complex tasks.
5.3 Ablation study
Component analysis. To evaluate the effect of each technique in SURF individually, we incremen-
tally apply semi-supervised learning (SSL) and temporal cropping (TC) to our backbone algorithm,
PEBBLE. Figure 4a shows the learning curves of SURF on Walker-walk task with 100 queries.
We observe that leveraging unlabeled samples via pseudo-labeling (green) significantly improves
PEBBLE, in terms of both sample-efficiency and asymptotic performance, while standard PEBBLE
(blue) suffers from lack of supervision. In addition, both supervised (blue) and semi-supervised
(green) reward learning are further improved by additionally utilizing temporal cropping (purple
and red, respectively). This implies that our augmentation method improves label-efficiency by gen-
erating diverse behaviors share the same labels. Also, the results show that the key components of
SURF are both effective, and their combination is essential to our method’s success. We also provide
extensive ablation studies on Meta-world, which show similar tendencies in Appendix C.
Effects of query size. To investigate how the number of queries affects the performance of SURF,
we evaluate the performance of SURF with a varying number of queries N ∈ {50, 100, 200, 400}.
As shown in Figure 4b, SURF (solid lines) consistently improves the performance of PEBBLE (dot-
ted lines) across a wide range of query sizes. The gain from SURF becomes even more significant
in the extreme label-scarce scenarios, i.e., N ∈ {50, 100}.
Comparison to other augmentation for state-based inputs. To demonstrate that temporal crop-
ping can enduce significant improvements for reward learning, we compare our method to other
augmentation methods for state-based inputs. We consider random amplitude scaling (RAS) and
adding Gaussian noise (GN) proposed in Laskin et al. (2020) as our baselines. RAS multiplies
an uniform random variable Z to the state, i.e., b = S ∙ z, where Z 〜 Unif[α,β], and GN adds
7
Published as a conference paper at ICLR 2022
Environment Steps (×106)
Environment Steps (×106)
Environment Steps (×106)
(c) Effects of data augmentation
(a) Contributions of each component	(b) Query size
Figure 4:	Ablation study on Walker-walk. (a) Contribution of each technique in SURF, i.e., semi-
supervised learning (SSL) and temporal cropping (TC). (b) Effects of query size. (c) Comparison of
augmentation methods. The results show the mean and standard deviation averaged over five runs.
1,000
,>50
7 5 2
O
0.0 0.1 0.2 0.3 0.4 0.5
Environment Steps (×106)
(c)	Loss weight λ
750
500
250
----PEBBLE + SURF (τ = 0.95)
PEBBLE + SURF (τ = 0.97)
----PEBBLE + SURF (τ = 0.99)
PEBBLE + SURF (τ = 0.999)
0 -∣ I	I	I I
0.0	0.1	0.2	0.3	0.4	0.5
Environment Steps (×106)
(b)	Threshold parameter τ
U」n4*a,pos -d 山
1,000
750
500
250
---PEBBLE + SURF (μ = l)
---PEBBLE + SURF (μ = 2)
PEBBLE + SURF (μ = 4)
1 QQQ — PEBBLE + SURF (μ = 7)
0 -	I I I	I
0.0	0.1	0.2	0.3	0.4	0.5
Environment Steps (×106)
(a) Unlabeled batch ratio μ
U」n4 黑a,pos -d 山
U」n4 黑a,pos -d 山
Figure 5:	Hyperparameter analysis on Walker-walk using 100 preference queries. The results show
the mean and standard deviation averaged over five runs.
a multivariate Gaussian random variable Z to the state, i.e., b = S + z, where Z 〜 N(0,I).
As proposed in Laskin et al. (2020), we apply these methods consistently along the time dimen-
sion, and choose the parameters for RAS as α = 0.8, β = 1.2. Specifically, for a given segment
σ = ({(sk, ak), ..., (sk+H-1, ak+H-1)}), we obtain the augmented sample σb by perturbing each
state along the segment, i.e., σb = ({(bsk, ak), ..., (bsk+H-1, ak+H-1)}). In Figure 4c, we plot the
learning curves of PEBBLE with various data augmentations on Walker-walk task with 100 queries.
We observe that RAS improves the performance of PEBBLE, but temporal cropping still outper-
forms these two methods. GN degrades the performance, possibly due to the noisy inputs. Since
RAS is an orthogonal approach to augment state-based inputs, one can integrate them with our
method to further improve the performance. This may be an interesting future direction for address-
ing feedback-efficiency in preference-based RL (see Appendix C).
Effects of hyperparameters of SURF. We investigate how the hyperparameters of SURF affect
the performance of preference-based RL. In Figure 5, we plot the learning curve of SURF with
different set of hyperparameters: (a) unlabeled batch ratio μ ∈ {1, 2,4,7}, (b) threshold parameter
τ ∈ {0.95, 0.97, 0.99, 0.999}, and (c) loss weight λ ∈ {0.1, 0.5, 1, 2}, respectively. First, we
observe that SURF is quite robust on μ, but the performance slightly drops with a large batch size
μ = 7. We expect that this is because a large batch size makes the reward model overfit to unlabeled
data. We also observe that SURF is also robust on the threshold τ , except for the smallest value
of 0.95. Because there are only two classes in tasks, the optimal threshold could larger than the
value typically used in previous SSL methods (Sohn et al., 2020), i.e., 0.95. In the case of the loss
weight λ, tuning this parameter brings more improvements than other hyperparameters. Although
8
Published as a conference paper at ICLR 2022
u-ln"0=α,pos5d山
1,000
750
500
250
0
0.0 0.25	0.5	0.75
u-InMaa,pos5d 山
—DrQ-v2 with ground truth reward
PEBBLE (feedback=1000) + SURF
PEBBLE (feedback=1。。。)
Oooo
0 5 0 5
0 7 5 2
0.25	0.5	0.75	1.0
u-InMaa,pos5d 山
1.0
0
0.0
00
O
L
gl0g
5 g 5
7 5 2
0
0.0 0.25
0.5	0.75	1.0
Number of Frames (×106)
Number of Frames (×106)
Number of Frames (×106)
(a) Walker
(b) Cheetah
(c) Quadruped
Figure 6:	Learning curves on locomotion tasks with pixel-based inputs as measured on the ground
truth reward. The solid line and shaded regions represent the mean and standard deviation, respec-
tively, across five runs.
we use a simple choice, i.e., λ = 1, in our experiments, more tuning λ would further improve the
performance of our method.
5.4 Experiments on visual control tasks
Figure 6 shows the learning curve of DrQ-v2, PEBBLE, and SURF with the same number of queries.
We observe that SURF (red) significantly improves the performance of PEBBLE (blue). In particu-
lar, SURF achieves comparable performance to DrQ-v2 (green) with ground truth reward in Walker-
walk, only using a budget of 200 queries. These results demonstrate that SURF could also improve
the performance with image observations. We remark that our temporal cropping augmentation can
be combined with any existing image augmentation methods, which would be an interesting future
direction to explore.
6 Discussion
In this work, we present SURF, a semi-supervised reward learning algorithm with data augmentation
for preference-based RL. First, in order to utilize an unlimited number of unlabeled data, we utilize
pseudo-labeling on confident samples. Also, to enforce consistencies to the reward function, we
propose a new data augmentation method called temporal cropping. Our experiments demonstrate
that SURF significantly improves feedback-efficiency of current state-of-the-art method on a variety
of complex robotic manipulation and locomotion tasks. We believe that SURF can scale up deep RL
to more diverse and challenging domains by making preference-based learning more tractable.
An interesting future direction is to extend state-based inputs to partially-observable or high-
dimensional inputs, e.g., pixels. One can expect that representation learning based on unlabeled
samples and data augmentation (Chen et al., 2020a; Grill et al., 2020) is crucial to handle such in-
puts. We think that our investigations on leveraging unlabeled samples and data augmentation would
be useful in representation learning for preference-based RL.
Ethics statement. Preference-based RL can align RL agents with the teacher’s preferences, which
enables us to apply RL to diverse problems and obtain strong AI. However, there could be possible
negative impacts if a malicious user corrupts the preferences to teach the agent harmful behav-
iors. Since we have proposed a method that makes preference-based RL algorithms more feedback-
efficiently, our method may reduce the efforts for teaching not only the desirable behaviors, but also
such bad behaviors. For this reason, in addition to developing algorithms for better performance and
efficiency, it is also important to consider safe adaptation in the real world.
Reproducibility statement. We describe the implementation details of SURF in Appendix B, and
also provide our source code in the supplementary material.
9
Published as a conference paper at ICLR 2022
Acknowledgements and Disclosure of Funding
This work was supported by Samsung Electronics Co., Ltd (IO201211-08107-01), OpenPhilosophy,
and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant
funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School
Program (KAIST)). We would like to thank Junsu Kim and anonymous reviewers for providing
helpful feedbacks and suggestions in improving our paper.
References
Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases, 2011.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man6. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20,
2020.
Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77-82, 2020.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
David Berthelot, Nicholas Carlini, Ian GoodfelloW, Nicolas Papernot, Avital Oliver, and Colin A
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Advances in Neural
Information Processing Systems, 2019.
David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning With distribution matching and augmenta-
tion anchoring. In International Conference on Learning Representations, 2020.
Erdem Biyik and Dorsa Sadigh. Batch active preference-based learning of reWard functions. In
Conference on Robot Learning, 2018.
Erdem Biyik, Nicolas Huynh, Mykel J Kochenderfer, and Dorsa Sadigh. Active preference-based
gaussian process regression for reWard learning. In Robotics: Science and Systems, 2020.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324-345, 1952.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple frameWork for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020a.
Ting Chen, Simon Kornblith, Kevin SWersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.
Paul F Christiano, Jan Leike, Tom BroWn, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. In Advances in Neural Information Processing
Systems, 2017.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-
tion in reinforcement learning. In International Conference on Machine Learning, 2019.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
10
Published as a conference paper at ICLR 2022
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data aug-
mentation with no separate search. arXiv preprint arXiv:1909.13719, 2019.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad-
vances in Neural Information Processing Systems, 2004.
Jean-Bastien Grill, Florian Strub, Florent Altch6, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, 2018.
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca Dragan. Inverse
reward design. In Advances in Neural Information Processing Systems, 2017.
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. In Advances in Neural Information
Processing Systems, 2018.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning,
2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Jens Kober and Jan Peters. Policy search for motor primitives in robotics. Machine learning, 84
(1-2):171-203, 2011.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Rein-
forcement learning with augmented data. In Advances in Neural Information Processing Systems,
2020.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep
neural networks. In ICML Workshop, 2013.
Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique
for generalization in deep reinforcement learning. arXiv preprint arXiv:1910.05396, 2019.
Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement
learning via relabeling experience and unsupervised pre-training. In International Conference on
Machine Learning, 2021a.
Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-
based reinforcement learning. In Thirty-fifth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 1), 2021b. URL https://openreview.net/
forum?id=ps95-mkHF_.
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable
agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,
2018.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
11
Published as a conference paper at ICLR 2022
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286,
2007.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of ob-
ject detection models. In 2005 Seventh IEEE Workshops on Applications of Computer Vision
(WACV/MOTION’05) - Volume 1, volume 1, pp. 29-36, 2005. doi: 10.1109/ACVMOT.2005.107.
Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learn-
ing of reward functions. In Robotics: Science and Systems, 2017.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. In Advances in Neural Information
Processing Systems, 2016.
Jurgen Schmidhuber. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE
Transactions on Autonomous Mental Development, 2(3):230-247, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, 2015.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Artfhur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. In Advances in Neural Information Processing Systems, 2020.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. In International Conference on Machine Learning, 2020.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. arXiv preprint
arXiv:2009.01325, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh
Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm_control: Software and tasks for
continuous control. arXiv preprint arXiv:2006.12983, 2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In International Conference on Intelligent Robots and Systems, 2012.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from
trajectory preference queries. In Advances in Neural Information Processing Systems, 2012.
12
Published as a conference paper at ICLR 2022
Jeff Wu, Long Ouyang, Daniel M Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-
tiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862,
2021.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data
augmentation for consistency training. arXiv preprint arXiv:1904.12848, 2019.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021.
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-
trol: Improved data-augmented reinforcement learning. In ICLR, 2022.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning, 2020.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
13
Published as a conference paper at ICLR 2022
A PEBBLE algorithm
A state-of-the-art preference-based RL algorithm, PEBBLE (Lee et al., 2021a), consists of two main
components: unsupervised pre-training and relabeling experiences. To collect diverse experience,
PEBBLE pre-trains the policy by using intrinsic motivation (Oudeyer et al., 2007; Schmidhuber,
2010) in the beginning of training. Specifically, PEBBLE optimizes the policy to maximize the state
entropy H(S) = -Es〜p(s)[logp(s)] to efficiently explore the environment. Then PEBBLE learns
the policy by using the state-of-the-art off-policy RL algorithm, SAC (Haarnoja et al., 2018). Since
the learning process of off-policy algorithms with a non-stationary reward function can be unstable,
PEBBLE stabilizes the learning process by relabeling all experiences in the buffer when the reward
model is updated.
B	Experimental details
(a) Hammer (b) Door Open (c) Button Press (d) Sweep Into (e) Drawer Open (f) Window Open
Figure 7: Rendered images of robotic manipulation tasks from Meta-world. Our goal is learning
various locomotion and manipulation skills using preferences from a teacher.
Training details. We choose PEBBLE (Lee et al., 2021a) as a backbone algorithm of SURF, and
use the hyperparameters in Table 1 for both PEBBLE and our method. For the reward model, we use
a three-layer MLP with 256 hidden units and leaky ReLU activation. Following the implementation
of PEBBLE (Lee et al., 2021a), we use an ensemble of three reward models and bound the output
to [-1, 1] using tanh function. Each model is trained by minimizing the cross-entropy loss using
ADAM optimizer (Kingma & Ba, 2015) with the learning rate of 0.0003. For semi-supervised
learning and data augmentation of SURF, we use hyperparameters in Table 2.
Sampling schemes. In preference-based RL methods, informative query sampling (Biyik & Sadigh,
2018; Biyik et al., 2020; Sadigh et al., 2017) has been adopted for improving the feedback-efficiency.
For all experiments of PEBBLE and SURF, we use the disagreement-based sampling (Christiano
et al., 2017) to choose queries for labeling: we first uniformly sample the initial batch of segments,
and select Nquery pairs of segments4 with high uncertainty based on the variance across ensemble of
preference predictors {Pψi [σ1 σ0]}iN=en1. Note that we use uniform sampling scheme for unlabeled
samples, because the number of unlabeled samples are not limited. At each feedback session, we
sample unlabeled samples as 10 times of labeled ones if the maximum budget of feedback is equal
or larger than 1,000, and otherwise we sample unlabeled samples as 100 times of labeled ones.
Table 1: Hyperparameters of PEBBLE.
Hyperparameter	Value	HyPerParameter	Value
Initial temperature	0.1	Hidden units per each layer	1024 (DMControl), 256 (Meta-world)
Length of segment	50	# of layers	2 (DMControl), 3 (Meta-world)
Learning rate	0.0003 (Meta-world) 0.0005 (Walker, Cheetah) 0.0001 (Quadruped)	Batch Size Optimizer	1024 (DMControl), 512 (Meta-world) Adam (Kingma & Ba, 2015)
Critic target update freq	2	Critic EMA τ	0.005
(β1, β2)	(0.9, 0.999)	Discount Y	0.99
Frequency of feedback	5000 (Meta-world) 20000 (Walker, Cheetah) 30000 (Quadruped)	Maximum budget / # of queries per session	1000/100, 100/10 (DMControl) 10000/50, 4000/20 (Meta-world) 2000/25, 400/10 (Meta-world)
# of ensemble models Nen	3	# of pre-training steps	10000
4We select 10% of the initial batch.
14
Published as a conference paper at ICLR 2022
Table 2: Hyperparameters of SURF with state-based inputs.
Hyperparameter
Value
Unlabeled batch ratio μ
Threshold τ
Loss weight λ
Min/Max length of cropped segment [Hmin, Hmax]
Segment length before cropping
4
0.999 (Window Open, Sweep Into, Cheetah)
0.99 (others)
1
[45, 55]
60
Hyperparameters of DrQ-v2. We use the same encoder architecture and hyperparameters as in
DrQ-v2 (Yarats et al., 2022). The full list of hyperparameters of DrQ-v2 is presented in Table 3. For
semi-supervised learning and data augmentation of SURF with pixel-based inputs, we use hyperpa-
rameters in Table 4.
Table 3: A set of hyperparameters used in our experiments.
Parameter	Setting
Replay buffer capacity	106
Action repeat	2
Seed frames	4000
Exploration steps	2000
n-step returns	1 (Walker), 3 (Cheetah / Quadruped)
Mini-batch size	512 (Walker), 256 (Cheetah / Quadruped)
Discount γ	0.99
Optimizer	Adam
Learning rate	10-4
Agent update frequency	2
Critic Q-function soft-update rate τ	0.01
Features dim.	50
Hidden dim.	1024
Exploration stddev. clip	0.3
Exploration stddev. schedule	Walker: linear(1.0, 0.1, 100000) Cheetah / Quadruped: linear(1.0,0.1,500000)
Table 4: Hyperparameters of SURF with pixel-based inputs.
Hyperparameter	Value
Unlabeled batch ratio μ	1
Threshold τ	0.99
Loss weight λ	1 (Cheetah), 0.1 (others)
Min/Max length of cropped segment [Hmin, Hmax]	[45, 55] (Cheetah), [48, 52] (others)
Segment length before cropping	60 (Cheetah), 54 (others)
Implementation. We implement SURF using the publicly released implementation repository of the
PEBBLE algorithm (https://github.com/pokaxpoka/B_Pref) with a full list of hyper-
parameters in Table 1. For visual control tasks, we implement PEBBLE and SURF using the official
code of DrQ-v2 (https://github.com/facebookresearch/drqv2) with hyperparam-
eters in Table 3. Note that DMControl environment depends on the MuJoCo simulator (Todorov
et al., 2012), which is a commercial software. We follow the standard evaluation protocol for the
locomotion tasks from DMControl. For robotic manipulation tasks from Meta-world, we measure
the task success rate as defined by the authors. For each run of experiments, we utilize one Nvidia
RTX 2080 Ti GPU and 4 CPU cores for training.
C Additional experimental results
Ablation study on Meta-world. We provide additional experimental results for component analysis
on Meta-world (Yu et al., 2020). To evaluate the effect of each technique in SURF individually, we
incrementally apply semi-supervised learning (SSL) and temporal cropping (TC) to our backbone
15
Published as a conference paper at ICLR 2022
algorithm, PEBBLE. Figure 8a and 8b show the learning curves of SURF on Window Open with
400 queries and Hammer with 10,000 queries, respectively. We observe that both semi-supervised
learning (green) and data augmentation (purple) improve the baseline of PEBBLE (blue). Also,
applying both of them further improves the performance (red). This shows that the key components
of SURF are both effective.
Applying temporal cropping with other augmentations. In Section 5.3, we compared our method
to random amplitude scaling (RAS) and adding Gaussian noise (GN) proposed in Laskin et al.
(2020). To investigate if applying RAS or GN with the temporal cropping (TC) further improve the
performance, we provide experimental results for the joint usage of the augmentations. In Figure 9a,
we plot the learning curves of PEBBLE with various data augmentations on Walker-walk with 100
queries. We observe that the naive combination of two augmentations, i.e., RAS + TC and GN + TC,
do not further improve the performance. Investigating how to combine several data augmentation
methods for reward learning would be an interesting future direction.
Effects of augmentation intensity. To investigate how does augmentation intensity affects the
RL performance, we provide additional experimental results. In Walker-walk with 100 queries, we
apply the temporal cropping to PEBBLE, with a varying cropping range, i.e., (Hmax - Hmin)/2,
from an array of [2, 5, 10, 20]. Figure 9b shows that temporal cropping consistently improves the
performance of the PEBBLE. Although the performance with a large cropping range of 20 slightly
underperforms the performance with 10, these results show that our augmentation method is quite
robust to the choice of hyperparameters.
(东)0φjroa ssəuns
100-
PEBBLE + SURF (SSL: O, TC: O)
PEBBLE + SURF (SSL: O, TC: X)
PEBBLE + SURF (SSL: X, TC: O)
PEBBLE
O
O
- - - -
5 0 5 0
7 5 2
5
0
/
O
3
O
2
O
(东)0φjroa ssəuns
100-
5 0 5 0
7 5 2
O
2
5
ɪ
O
O
0
I 5
Environment Steps (×106)
Environment Steps (×106)
(a) Ablation study on Window Open
(b) Ablation study on Hammer
Figure 8:	Contribution of each technique in SURF, i.e., semi-supervised learning (SSL) and temporal
cropping (TC), in (a) Window Open, and (b) Hammer. The results show the mean and standard
deviation averaged over five runs.
16
Published as a conference paper at ICLR 2022
PEBBLE + TC + RAS
PEBBLE + TC + GN
PEBBLE + TC
PEBBLE + RAS
PEBBLE + GN
PEBBLE
O O
O 5
5 2
u.jmə①PoSS
0.0 0.1 0.2 0.3 0.4 0.5
Environment Steps (×106)
750
O O
O 5
5 2
u.jmə①PoSS
PEBBLE (crop = 20)
PEBBLE (Crop = IO)
PEBBLE (crop = 5)
PEBBLE (crop = 2)
PEBBLE
0 -------1-----1-----1-----------
0.0	0.1	0.2	0.3	0.4 0.5
Environment Steps (×106)
(a)	Joint usage of the augmentations
(b)	Effect of augmentation intensity
0
Figure 9:	Analysis of the augmentation methods on Walker-walk task with 100 queries. (a) Com-
parison of the augmentation methods, i.e., applying random amplitude scaling (RAS) or Gaussian
noise (GN) with temporal cropping (TC). (b) Comparison of the performance of PEBBLE with vary-
ing augmentation intensity, i.e., cropping range. The results show the mean and standard deviation
averaged over five runs.
17