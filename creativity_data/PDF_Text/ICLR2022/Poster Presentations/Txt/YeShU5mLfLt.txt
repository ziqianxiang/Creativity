Published as a conference paper at ICLR 2022
On the Convergence of Certified Robust
Training with Interval B ound Propagation
Yihan Wang*, Zhouxing Shi*, Quanquan Gu, Cho-Jui Hsieh
University of California, Los Angeles
{yihanwang,zshi,qgu,chohsieh}@cs.ucla.edu
*Equal contribution
Ab stract
Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods
for training neural networks with certifiable robustness guarantees when potential
adversarial perturbations present, while the convergence of IBP training remains
unknown in existing literature. In this paper, we present a theoretical analysis
on the convergence of IBP training. With an overparameterized assumption, we
analyze the convergence of IBP robust training. We show that when using IBP
training to train a randomly initialized two-layer ReLU neural network with lo-
gistic loss, gradient descent can linearly converge to zero robust training error
with a high probability if we have sufficiently small perturbation radius and large
network width.
1	Introduction
It has been shown that deep neural networks are vulnerable against adversarial examples (Szegedy
et al., 2014; Goodfellow et al., 2015), where a human imperceptible adversarial perturbation can
easily alter the prediction by neural networks. This poses concerns to safety-critical applications
such as autonomous vehicles, healthcare or finance systems. To combat adversarial examples, many
defense mechanisms have been proposed in the past few years (Kurakin et al., 2016; Madry et al.,
2018; Zhang et al., 2019; Guo et al., 2018; Song et al., 2018; Xiao et al., 2020). However, due to the
lack of reliable measurement on adversarial robustness, many defense methods are later broken by
stronger attacks (Carlini & Wagner, 2017; Athalye et al., 2018; Tramer et al., 2020).
There are recently a line of robust training works, known as certified robust training (certified de-
fense), focusing on training neural networks with certified and provable robustness - the network is
considered robust on an example if and only if the prediction is provably correct for any perturbation
in a predefined set (e.g., a Sman '∞ ball) (Wang et al., 2018b; Bunel et al., 2018; Zhang et al., 2018;
Wang et al., 2018c; Wong & Kolter, 2018; Singh et al., 2018; 2019; Weng et al., 2018; Xu et al.,
2020). Certified defense methods provide provable robustness guarantees without referring to any
specific attack and thus do not rely on the strength of attack algorithms.
To obtain a neural network with certified robustness, a common practice is to derive a neural network
verification method that computes the upper and lower bounds of output neurons given an input
region under perturbation, and then train the model by optimizing the loss defined on the worst-
case output from verification w.r.t. any possible perturbation. Many methods along this line have
been proposed (Wong & Kolter, 2018; Wong et al., 2018; Mirman et al., 2018; Gowal et al., 2018;
Raghunathan et al., 2018a; Zhang et al., 2020a). Among these methods, Interval Bound Propagation
(IBP) (Mirman et al., 2018; Gowal et al., 2018) is a simple but effective and efficient method so
far, which propagates the interval bounds of each neuron through the network to obtain the output
bounds of the network. Most of the latest state-of-the-art certified defense works are at least partly
based on IBP training (Zhang et al., 2020a; Shi et al., 2021; Lyu et al., 2021; Zhang et al., 2021).
However, the convergence properties of IBP training remained unknown. For standard neural net-
work training (without considering adversarial perturbation, aka natural training), it has been shown
that gradient descent for overparameterized networks can provably converge to a global minimizer
with random initialization (Li & Liang, 2018; Du et al., 2019b;a; Jacot et al., 2018; Allen-Zhu
et al., 2019; Zou et al., 2018). Compared to standard training, IBP-based robust training has a very
1
Published as a conference paper at ICLR 2022
different training scheme which requires a different convergence analysis. First, in the robust train-
ing problem, input can contain perturbations and the training objective is defined differently from
standard training. Second, IBP training essentially optimizes a different network augmented with
IBP computation, as illustrated in Zhang et al. (2020a). Third, in IBP training, the activation state
of each neuron depends on the certified bounds rather than the values in standard neural network
computation, which introduces special perturbation-related terms in our analysis.
In this paper, we conduct a theoretical analysis to study the convergence of IBP training. Follow-
ing recent convergence analysis on Stochastic Gradient Descent (SGD) for standard training, we
consider IBP robust training with gradient flow (gradient descent with infinitesimal step size) for a
two-layer overparameterized neural network on a classification task. We summarize our contribu-
tions below:
•	We provide the first convergence analysis for IBP-based certified robust training. On a
two-layer overparameterized ReLU network with logistic loss, with sufficiently small per-
turbation radius and large network width, gradient flow with IBP has a linear convergence
rate, and is guaranteed to converge to zero training error with high probability.
•	This result also implies that IBP converges to a state where the certified robust accuracy
measured by IBP bounds tightly reflects the true robustness of the network.
•	We show additional perturbation-related conditions required to guarantee the convergence
of IBP training and identify particular challenges in the convergence analysis for IBP train-
ing compared to standard training.
Notation We use lowercase letters to denote scalars, and use lower and upper case boldface let-
ters to denote vectors and matrices respectively. 1(∙) stand for the indicator function. For a d-
dimensional vector x ∈ Rd, kxkp is its `p-norm. For two sequences {an} and {bn}, n > 0, we have
a，n = O(bn) if and only if ∃C > 0, ∃N > 0, ∀n > N, an ≤ Cbn,. And We have a，n = Ω(bn) if and
only if ∃C > 0,∃N > 0,∀n > N, an ≥ Cbn.
2	Related Work
2.1	Certified Robust Training
The goal of certified robust training is to maximize the certified robust accuracy of a model evalu-
ated by provable robustness verifiers. Some Works added heuristic regularizations during adversarial
training to improve certified robustness (Xiao et al., 2019; Balunovic & Vechev, 2020). More ef-
fectively, certified defense Works typically optimize a certified robust loss Which is a certified upper
bound of the loss W.r.t. all considered perturbations. Among them, Wong & Kolter (2018); Mirman
et al. (2018); Dvijotham et al. (2018); Wong et al. (2018); Wang et al. (2018a) used verification
With linear relaxation for nonlinear activations, and Raghunathan et al. (2018b) used semi-definite
relaxation. HoWever, IBP (Mirman et al., 2018; GoWal et al., 2018), Which computes and propagates
interval loWer and bounds for each neuron, has been shoWn as efficient and effective and can even
outperform methods using more complicated relaxation (Lee et al., 2021; Jovanovic et al., 2021).
Most of the effective certified defense methods are at least partly based on IBP. For example, Zhang
et al. (2020a) combined IBP With linear relaxation bounds; Lyu et al. (2021) designed a parameter-
ized activation; Zhang et al. (2021) designed a 1-Lipschitz layer with '∞-norm computation before
layers using IBP; Shi et al. (2021) accelerated IBP training With shortened training schedules. As
most state-of-the-art methods so far contain IBP as an important part, we focus on analyzing the
convergence of IBP training in this paper.
On the theoretical analysis for IBP bounds, Baader et al. (2020) analyzed the universal approxima-
tion of IBP verification bounds, and Wang et al. (2020) extended the analysis to other activation
functions beyond ReLU. However, to the best of our knowledge, there is still no existing work
analyzing the convergence of IBP training.
The aforementioned methods for certified robustness target at robustness with deterministic certifi-
cation. There are also some other works on probabilistic certification such as randomized smooth-
ing (Cohen et al., 2019; Li et al., 2019; Salman et al., 2019) which is out of our scope.
2
Published as a conference paper at ICLR 2022
2.2	Convergence of Standard Neural Network Training
There have been many works analyzing the convergence of standard neural network training. For
randomly initialized two-layer ReLU networks with quadratic loss, Du et al. (2019b) proved that
gradient descent can converge to a globally optimum with a large enough network width polynomial
in the data size. Ji & Telgarsky (2019) pushed the requirement of network width to a polylogarithmic
function. For deep neural networks, Allen-Zhu et al. (2019) proved that for deep ReLU networks,
gradient descent has a linear convergence rate for various loss functions with width polynomial in
network depth and data size. Chen et al. (2019) proved that a polylogarithmic width is also sufficient
for deep neural networks to converge. However, they only focus on standard training and cannot be
directly adapted to the robust training settings.
2.3	Convergence of Empirical Adversarial Training
Robust training is essentially a min-max optimization. For a training data distribution X , the objec-
tive for learning a model fθ parameterized by θ can be written as1:
arg min E(χ,y)〜X max '(fθ (X + ∆),y),
θ	∆∈S
where (x, y) is a sample, '(∙,y) is the loss function, S is the space of perturbations. Empirical
adversarial training approximates the inner minimization by adversarial attacks, and some works
analyzed the convergence of adversarial training: Wang et al. (2019) considered a first-order station-
ary condition for the inner maximization problem; Gao et al. (2019); Zhang et al. (2020b) showed
that overparameterized networks with projected gradient descent can converge to a state with robust
loss close to 0 and the the inner maximization by adversarial attack is nearly optimal; and Zou et al.
(2021) showed that adversarial training provably learns robust halfspaces in the presence of noise.
However, there is a significant difference between empirical adversarial training and certified robust
training such as IBP. Adversarial training involves a concrete perturbation ∆, which is an approx-
imate solution for the inner maximization and could lead to a concrete adversarial input x + ∆.
However, in IBP-based training, the inner maximization is computed from certified bounds, where
for each layer, the certified bounds of each neuron are computed independently, and thereby the cer-
tified bounds of the network generally do not correspond to any specific ∆. Due to this significant
difference, prior theoretical analysis on adversarial training, which requires a concrete ∆ for inner
maximization, is not applicable to IBP.
3	Preliminaries
3.1	Neural Networks
Following Du et al. (2019b), we consider a similar two-layer ReLU network. Unlike Du et al.
(2019b) which considered a regression task with the square loss, we consider a classification task
where IBP is usually used, and we consider binary classification for simplicity. On a training dataset
{(xi, yi)}in=1, for every i ∈ [n], (xi, yi) is a training example with d-dimensional input xi(xi ∈Rd)
and label yi(yi ∈ {±1}), and the network output is:
f(W,a,
1m
Xi) = √m ∑arσ(w>Xi),
(1)
where m is the width of hidden layer (the first layer) in the network, W ∈ Rm×d is the weight ma-
trix of the hidden layer, wr(r ∈ [m]) is the r-th row ofW, a ∈ Rm is the weight vector of the second
layer (output layer) with elements aι, ∙∙∙ , am, and σ(∙) is the activation function. We assume the ac-
tivation is ReLU as IBP is typically used with ReLU. For initialization, We set a『〜unif[{1, -1}] and
Wr 〜N(0, I). Only the first layer is trained after initialization. Since We consider binary classifica-
tion, we use a logistic loss. For training example (Xi, yi), we define ui(W, a, Xi) := yif(W, a, Xi),
1Here we use notations to denote the general robust training problem, but in our later analysis, we will have
different notations for a simplified problem setting.
3
Published as a conference paper at ICLR 2022
the loss on this example is computed as l(ui(W, a, xi)) = log(1 + exp(-ui (W, a, xi))), and the
standard training loss on the whole training set is
nn
L = X l(ui(W, a, xi)) = Xlog 1 + exp(-ui(W, a, xi)) .
i=1	i=1
3.2	Certified Robust Training
In the robust training setting, for original input xi (∀i ∈ [n]), we consider that the actual input
may be perturbed into Xi + ∆i by perturbation ∆%. For a widely adopted setting, We consider '∞
perturbations, where ∆% is bounded by an '∞ ball with radius e(0 ≤ E ≤ 1), i.e., ∣∣∆ik∞ ≤ e.
For the convenience of subsequent analysis and without loss of generality, we make the following
assumption on each Xi , which can be easily satisfied by normalizing the training data:
Assumption 1. ∀i ∈ [n], we assume there exists some ξ > 0, such that Xi ∈ [E, 1]d, kXik2 ≥ ξ.
In Du et al. (2019b), they also assume there are no parallel data points, and in our case we assume
this holds under any possible perturbation, formulated as:
Assumption 2. For perturbation radius E, we assume that
∀i,j ∈ [n],i = j, ∀xi ∈ B∞X,e), ∀xj ∈ B∞(xj,e),	Xi # xj,
where B∞(xi, E) Standsfor the '∞-ball with radius E centered at Xi.
IBP training computes and optimizes a robust loss L, which is an upper bound of the standard loss
for any possible perturbation ∆i (∀i ∈ [n]):
n
L ≥ Xmax l log ɑ + exp(-yif(W,a,Xi + ∆i))) | ∣Ak∞ ≤ e}.
i=1	i
To compute L, since log(∙) and exp(∙) are both monotonic, for every i ∈ [n], IBP first computes the
lower bound of Ui (W, a, Xi + ∆j for ∣∣∆i∣∣∞ ≤ e, denoted as ui. Then the IBP robust loss is:
n
L = ɪ2log(l + exp(-Ui)),	where Ui ≤ minui(W, a, Xi + ∆i)(i ∈ [n]).	(2)
∆i
i=1
IBP computes and propagates an interval lower and upper bound for each neuron in the network,
and then Ui is equivalent to the lower bound of the final output neuron. Initially, the interval bound
of the input is [xi - E ∙ 1, X + E ∙ 1] given ∣∣∆i∣∣∞ ≤ e, since Xi - E ∙ 1 ≤ Xi + ∆i ≤ Xi + E ∙ 1
element-wisely holds. Then this interval bound is propagated to the first hidden layer, and we have
the interval bound for each neuron in the first layer:
∀r ∈ [m], σ wr>Xi - Ekwrk1 ≤ σ wr>(Xi + ∆i) ≤ σ wr>Xi + Ekwrk1 .
These bounds are further propagated to the second layer. We focus on the lower bound of Ui , which
can be computed from the bounds of the first layer by considering the sign of multiplier yiar:
1m
ui(W, a, Xi + ∆i) = y √m Σ arσ(wr>(Xi + ∆i))
r=1
1m
≥ √mɪ^ ( 13i ar = 1)σ (w>Xi - EIlWrl∣l)
r=1
+ l(yiθr = -1)σ(w>Xi + EllWrI∣l) } := ui∙	(3)
Then the IBP robust loss can be obtained as Eq. (2). And we define U := (U1,U2,…，Un).
We define certified robust accuracy in IBP training as the percentage of examples that IBP bounds
can successfully certify that the prediction is correct for any concerned perturbation. An example
i(i ∈ [n]) is considered as robustly classified under IBP verification if and only if Ui > 0. Let Ui be
the exact solution of the minimization in Eq. (2) rather than relaxed IBP bounds, we also define the
true robust accuracy, where the robustness requires Ui > 0. The certified robust accuracy by IBP
is a provable lower bound of the true robust accuracy.
4
Published as a conference paper at ICLR 2022
3.3	Gradient Flow
Gradient flow is gradient descent with infinitesimal step size for a continuous time analysis, and it
is adopted in prior works analyzing standard training (Arora et al., 2018; Du et al., 2019a;b). In IBP
training, gradient flow is defined as:
∀r ∈ [m],
dWr (t)	∂L(t)
dt	∂Wr (t)
(4)
where wι (t), w2 (t),…，Wm(t) are rows of the weight matrix at time t, and L(t) is the IBP robust
loss defined as Eq. (2) using weights at time t.
3.4	Gram Matrix
Under the gradient flow setting as Eq. (4), for all i ∈ [n], we analyze the dynamics of Ui during IBP
training, and we use Ui (t) to denote its value at time t:
d (, X	X / dui(t)
,
dwrt≡ ) = X -I超)% (t),
j=1
(5)
where l0(Uj) is the derivative of the loss, H(t) is a Gram matrix and defined as Hij (t) =
Pm=I( ∂Wi(t), ∂∂wj(tt)) (∀1 ≤ i,j ≤ n). We provide a detailed derivation in Appendix B.1.
The dynamic of Ui can be described using H.
From Eq. (3), ∀i ∈ [n], r ∈ [m], derivative 工(?)can be computed as follows:
dUid) _ ɪ
∂Wr (t)	√m “ar
sign(wr(t)) + Ar-i(t) xi + sign(wr (t))
where sign(wr(t)) is element-wise for wr(t), and we define indicators
A+i(t) ：= l(yiθr = 1, Wr (t)TXi - EkWr (t)∣∣ι > 0),
A-i(t) := l(yiθr = —I, Wr (t)TXi + EkWr (t)∣∣ι > 0).
Then elements in H can be written as:
1m
Hij (t) = ~yiyj	ar
m r=1
sign(Wr(t)) +Ar-i(t) xi + Esign(Wr(t))
Ar+j (t)xj - sign(wr (t)) + Ar-j (t)xj + sign(wr (t))
1m	m	m
=m，iy人 XT Xj X arij (t) -e( X(βrij ⑴ Xi +。呼⑴ Xj )T Sign(Wr (t))) + EEd X Yrij (t)),
r=1	r=1	r=1
(6)
where αrij (t), βrij (t), γrij(t) are defined as follows
αrij(t) = (Ar+i(t)+Ar-i(t))(Ar+j(t)+Ar-j(t)),
βrij(t) = (Ar+i(t)+Ar-i(t))(Ar+j(t) -Ar-j(t)),
γrij (t) = (Ar+i (t) - Ar-i (t))(Ar+j (t) - Ar-j (t)).
Further, we define H∞ which is the elementwise expectation of H(0), to characterize H(0) on the
random initialization basis:
∀1 ≤ i, j ≤ n, Hij ：= E∀1≤r≤m,Wr〜N(0,I),ar〜unif[{-1,1}]Hij(0),
where Hij(0) depends on the initialization of weights Wr and ar. We also define λ0 ：= λmin(H∞)
as the least eigenvalue of H∞. We will prove that H(0) is positive definite with high probability, by
showing that H∞ is positive definite and bounding the difference between H(0) and H∞.
5
Published as a conference paper at ICLR 2022
4 Convergence Analysis for IBP Training
We present the following main theorem which shows the convergence of IBP training under certain
conditions on perturbation radius and network width:
Theorem 1 (Convergence of IBP Training). Suppose Assumptions 1 and 2 hold for the train-
ing data, and the '∞ perturbation radius satisfies E ≤ O (min (松森,ɪ~√√Rd^)), where
R = d⅛λ02, C = λ384ξ. For a two-layer ReLU network (Eq. (1)), suppose its width for the
first hidden layer satisfies m ≥ ω((用)需)2) and the network is randomly initialized
as ar 〜 unif[ {1, -1}], Wr 〜 N(0, I), with the Second layer fixed during training. Thenfor any
confidence δ(0 < δ< 1), with probability at least 1 - δ, IBP training with gradient flow can converge
to zero training error.
The theorem contains two findings: First, for a given E, as long as it satisfies the upper bound on E
specified in the theorem, with a sufficiently large width m, convergence of IBP training is guaranteed
with high probability; Second, when E is larger than the upper bound, IBP training is not guaranteed
to converge under our analysis even with arbitrarily large m, which is essentially different from
analysis on standard training and implies a possible limitation of IBP training.
In the following part of this section, we provide the proof sketch for the main theorem.
4.1	Stability of the Gram Matrix during IBP Training
We first analyze the stability of H during training since H can characterize the dynamic of the
training as defined in Eq. (5). We show that when there exists some R such that the change of
Wr(∀r ∈ [m]) is restricted to kWr(t) - Wr(0)k2 ≤ R during training, we can guarantee that
λmin(H(t)) remains positive with high probability. This property will be later used to reach the
conclusion on the convergence. We defer the derivation for the constraint on R to a later part.
For all r ∈ [m], with the aforementioned constraint on Wr (t), we first show that during the IBP
training, most of αrij (t), βrij(t), γrij (t) terms in Eq. (6) remain the same as their initialized values
(t = 0). This is because for for each of αrij (t), βrij (t), γrij (t), the probability that its value changes
during training can be upper bounded by a polynomial in R, and thereby the probability can be made
sufficiently small for a sufficiently small R, as the following lemma shows:
Lemma 1. ∀r ∈ [m], at some time t > 0, suppose kWr (t) - Wr(0)k2 ≤ R holds for some R, and
V2gR_
iog(√ 2Rdξ)
E≤
holds, then for all 1 ≤ i, j ≤ n, we have
.................... .................................. 12 ,	、「一 二
Pr(arij(t) = αrij(0)), PMerij(t) = βrj(0)), Pr(Yrij(t) = Yrij(0)) ≤ ʌ-ʌ(l + E)VdR := R.
2πξ
We provide the full proof in Appendix A.1. Probabilities in Lemma 1 can be bounded as long as the
probability that each of indicator Ar+i (t), Ar-i (t), Ar+j(t), Ar-j (t) changes is upper bounded respec-
tively. When the change ofWr(t) is bounded, the indicators can change during the training only if at
initialization |Wr(0)>xi ±EkWr(0)k1 | is sufficiently small, whose probability can be upper bounded
(notation ± here means the analysis is consistent for both + and - cases). To bound this probability,
while Du et al. (2019b) simply used the anti-concentration of standard Gaussian distribution in their
standard training setting, here our analysis is different due to additional perturbation-related terms
EkWr(0)k1, and we combine anti-concentration and the tail bound of standard Gaussian in our proof.
We can then bound the change of the Gram matrix, i.e., kH(t) - H(0)k2:
Lemma 2. ∀r ∈ [m], at some time t > 0, suppose kWr (t) - Wr (0)k2≤ R holds for some constant
R, for any confidence δ(0 < δ < 1), with probability at least 1 - δ, it holds that
kH(t) - H(0)k2 ≤
12(1 + E)(1 + 2E + E2)d1.5n2
R.
√2∏ξδ
(7)
This can be proved by first upper bounding E[|Hij (t) - Hij (0)|] (∀1 ≤ i, j ≤ n) using Lemma 1,
and then by Markov’s inequality, we can upper bound kH(t) - H(0)k2 with high probability.
6
Published as a conference paper at ICLR 2022
We provide the proof in Appendix A.2. And by triangle inequality, we can also lower bound
λmin(H(t)):
Corollary 1. ∀r ∈ [m], at some time t > 0, suppose kwr (t) - wr (0)k2 ≤ R holds for some
constant R, for any confidence δ(0 < δ < 1), with probability at least 1 - δ, it holds that
λmin(H(t)) ≥ λmin(H(0)) - 12(1 + ''^l；+ 饪)",5"2 R,	(8)
where λ m^ (∙) stands for the minimum eigenvalue.
We also need to lower bound λmin(H(0)) in order to lower bound λmin(H(t)). Given Assumption 2,
we show that the minimum eigenvalue of H∞ is positive:
Lemma 3. When the dataset satisfies Assumption 2, λ0 := λmin(H∞) > 0 holds true.
The lemma can be similarly proved as Theorem 3.1 in Du et al. (2019b), but we have a different
Assumption 2 considering perturbations. We discuss in more detail in Appendix A.3. Then we can
lower bound λmin(H(0)) by Lemma 3.1 from Du et al. (2019b):
Lemma 4 (Lemma 3.1 from Du et al. (2019b)). Ifλ0 > 0, for any confidence δ(0 < δ < 1), take
m = Ω( nλ2 log( δ)) ,then with probability at least 1 一 δ, it holds true that λmin(H(0)) ≥ 3 λ°.
Although we have different values in H(0) for IBP training, we can still adopt their original lemma
because their proof by Hoeffding’s inequality is general regardless of values in H(0). We then
plug in λmin(H(0)) ≥ 4λo to Eq. (8), and We solve the inequality to find a proper R such that
λmin(H)(t) ≥ λ20, as shown in the following lemma (proved in Appendix A.4):
Lemma 5. For any confidence δ(0 < δ < 1), ∀r ∈ [m], suppose kwr (t) - wr (0)k2 ≤ R holds,
where R = dc¾0∙ With C = λ3∣4ξ, then probability at least 1 — δ, λma(H(t)) ≥ λ20 holds.
Therefore, we have shown that with overparameterization (required by Lemma 4), when wr is rel-
atively stable during training for all r ∈ [m], i.e., the maximum change on wr(t) is upper bounded
during training (characterized by the '2-norm of weight change restricted by R), H(t) is also rela-
tively stable and remains positive definite with high probability.
4.2	Convergence of the IBP Robust Loss
Next, we can derive the upper bound of the IBP loss during graining. In the following lemma, we
show that when H(t) remains positive definite, the IBP loss L(t) descends in a linear convergence
rate, and meanwhile we have an upper bound on the change ofwr(t) w.r.t. time t:
Lemma 6. Supposefor 0 ≤ S ≤ t, λma(H(t)) ≥ λ20, we have
L(t) ≤ exp
L(0)exp
kwr(t) - wr(0)k2 ≤
nt
This lemma is proved in Appendix A.5, which follows the proof of Lemma 5.4 in Zou et al. (2018).
To guarantee that λma(H(s)) ≥ λ20 for 0 ≤ S ≤ t, by Lemma 5, we only require √n= ≤ R =
d⅞λ02, which holds sufficiently by
cδλo√m
≤ d1∙5n3
(9)
Meanwhile, for each example i, the model can be certified by IBP on example i with any '∞ per-
turbation within radius e, if and only if Ui > 0, and this condition is equivalent to l(uj < κ, where
κ := log(1 + exp(0)). Therefore, to reach zero training error on the whole training set at time t, we
can require L(t) < κ, which implies that ∀1 ≤ i ≤ n,l(ui) < κ. Then with Lemma 6, we want the
upper bound of L(t) to be less than κ:
L(t) ≤ exp
L(0)exp
< κ,
7
Published as a conference paper at ICLR 2022
which holds sufficiently by
t> 楙(log (LK0))+ L(O))	(10)
To make Eq. (10) reachable at some t, with the constraint in Eq. (9) we require:
λ4θ (log ("))+ L(O)) <%f.	(11)
The left-hand-side of Eq. (11) can be upper bounded by
B (log (^^K^^)+L(O))=B (L(O)+log(L(O)) - log(K)) ≤ T(2L(O)—log(K)).
Therefore, in order to have Eq. (11) hold, it suffices to have
^(2L(O) - log(κ)) < cδ⅛√m =⇒ L(O) + C0 < c0δλ2√m,	(12)
λ0	d1.5 n3	d1.5 n3
where c0 := C and co are positive constants.
Since L(O) has randomness from the randomly initialized weight W, We need to upper bound the
value of L(O) as we show in the following lemma (proved in Appendix A.6 by concentration):
Lemma 7. In natural training, for any confidence δ(O < δ < 1), with probability at least 1 - δ,
L(O) = O( nδ) holds. In IBP training, for any confidence δ(O < δ < 1), with probability at least
1 - δ, L(O) = O( (n√md + nδ) holds.
And this lemma implies that with large n and m, there exist constants c1, c2, c3 such that
L(O) ≤ CInT + c2n + c3.
δδ
Plug Eq. (13) into Eq. (12), then the requirement in Eq. (12) can be relaxed into:
c0δλ2√m	cγn√mde	c2n	( c0δλ2	CInde) √—	CM
d1.5 n3	δ	δ 3	0	d1.5 n3	δ	δ 4
(13)
(14)
where C4 := C3 + C0 is a constant. As long as Eq. (14) holds, Eq. (11) also holds, and thereby IBP
training is guaranteed to converge to zero IBP robust error on the training set.
4.3	Proving the Main Theorem
Finally, we are ready to prove the main theorem. To make Eq. (11) satisfied, we want to make its
relaxed version, Eq. (14) hold by sufficiently enlarging m. This requires that the coefficient of √m
in Eq. (14), d⅛λ°∙ — c1『 to be positive, and we also plug in the constraint on e in Lemma 1:
≡⅛ - T > o, e ≤ log√q⅛.
Combining these two constraints, we can obtain the constraint for e in the main theorem:
C c0δ2λ0	√2dR	、
e< minUd25n3, ^qR^ )
Then by Eq. (14), our requirement on width m is
、c〃	d1.5n4δλ0	vʌ
m ≥ ω(lδ2λ0 - ed2∙5n4J )'
This completes the proof of the main theorem.s In our analysis, we focus on IBP training with e > O.
But IBP with e = 0 can also be viewed as standard training. By setting e = 0, if m ≥ Ω(nλ4d3),
our result implies that for any confidence δ (O < δ < 1), standard training with logistic loss also
converges to zero training error with probability at least 1 - δ. And as e gets larger, the required m
for convergence also becomes larger.
8
Published as a conference paper at ICLR 2022
0.04
---- standard training
IBP training witħ ε = 0.04
---- IBP training witħ ε= 0.001
0.03
O
£
0.02
0.01
0.00
500
2000	5000	10000	80000
Model width m
(b) Final training error of IBP training (on models
with width 2000 and 5000 respectively), when the
perturbation radius is varied.
(a) Final training error of standard training and
IBP (with ∈ {0.001, 0.04}) respectively, when
the width m of the model is varied.
Figure 1: Experimental results.
5	Experiments
We further conduct experiments to compare the convergence of networks with different widths m for
natural training and IBP training respectively. We use the MNIST (LeCun et al., 2010) dataset and
take digit images with label 2 and 5 for binary classification. And we use a two-layer fully-connected
ReLU network with a variable width. We train the model for 70 epochs with SGD, and we keep
fixed throughout the whole training process. We present results in Figure 1. First, compared with
standard training, for the same width m, IBP has higher training errors (Figure 1a). Second, for
relatively large ( = 0.04), even if we enlarge m up to 80,000 limited by the memory of a single
GeForce RTX 2080 GPU, IBP error is still far away from 0 (Figure 1a). This is consistent with
our main theorem that when is too large, simply enlarging m cannot guarantee the convergence.
Moreover, when is even larger, IBP training falls into a local minimum of random guess (with
errors close to 50%) (Figure 1b). We conjecture that this is partly because λ0 can be very small
with a large perturbation, and then the training can be much more difficult, and this difficulty cannot
be alleviated by simply enlarging the network width m. Existing works with IBP-based training
typically use a scheduling on and gradually increase from 0 until the target value for more stable
training. Overall, the empirical observations match our theoretical results.
6	Conclusion
In this paper, we present the first theoretical analysis of IBP-based certified robust training, and
we show that IBP training can converge to zero training error with high probability, under certain
conditions on perturbation radius and network width. Meanwhile, since the IBP robust accuracy
is a lower bound of the true robust accuracy (see Section 3.2), upon convergence the true robust
accuracy also converges to 100% on training data and the certification by IBP accurately reflects
the true robustness. Our results have a condition requiring a small upper bound on , and it will be
interesting for future work to study how to relax this condition, take the effect of scheduling into
consideration, and extend the analysis to deeper networks.
Acknowledgements
We thank the anonymous reviewers for their helpful comments. This work is partially supported
by NSF under IIS-2008173, IIS-2048280 and by Army Research Laboratory under agreement num-
ber W911NF-20-2-0158; QG is partially supported by the National Science Foundation CAREER
Award 1906169 and IIS-2008981. The views and conclusions contained in this paper are those of
the authors and should not be interpreted as representing any funding agencies.
9
Published as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, volume 97 of Proceed-
ingsof Machine Learning Research, pp. 242-252, 2019. URL http://proceedings.mlr.
press/v97/allen-zhu19a.html.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Im-
plicit acceleration by overparameterization. In International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning Research, pp. 244-253, 2018. URL
http://proceedings.mlr.press/v80/arora18a.html.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 274-283, 2018.
URL http://proceedings.mlr.press/v80/athalye18a.html.
Maximilian Baader, Matthew Mirman, and Martin T. Vechev. Universal approximation with certified
networks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=B1gX8kBtPr.
Mislav Balunovic and Martin T. Vechev. Adversarial training and provable defenses: Bridging
the gap. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=SJxSDxrKDr.
Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and Pawan Kumar Mudigonda. A
unified view of piecewise linear neural network verification. In Advances in Neural Information
Processing Systems, pp. 4795-4804, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/be53d253d6bc3258a8160556dda3e9b2-Abstract.html.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pp. 3-14, 2017.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suffi-
cient to learn deep relu networks? CoRR, abs/1911.12360, 2019. URL http://arxiv.org/
abs/1911.12360.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via random-
ized smoothing. In International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 1310-1320, 2019. URL http://proceedings.mlr.
press/v97/cohen19c.html.
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent
finds global minima of deep neural networks. In International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research, pp. 1675-1685, 2019a. URL
http://proceedings.mlr.press/v97/du19c.html.
Simon S. Du, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers. arXiv preprint arXiv:1805.10265, 2018.
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D. Lee. Convergence of
adversarial training in overparametrized neural networks. In Advances in Neural Information Pro-
cessing Systems, pp. 13009-13020, 2019. URL https://proceedings.neurips.cc/
paper/2019/hash/348a38cd25abeab0e440f37510e9b1fa-Abstract.html.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
10
Published as a conference paper at ICLR 2022
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
ChUan Guo, Mayank Rana, MoUstaPha Cisse, and LaUrens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=SyJ7ClWCb.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neUral networks. In Advances in Neural Information Processing Systems, PP.
8580-8589, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
5a4be1fa34e62bb8a6ec6b91d2462f5a- Abstract.html.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. CoRR, abs/1909.12292, 2019. URL
http://arxiv.org/abs/1909.12292.
Nikola Jovanovic, Mislav Balunovic, Maximilian Baader, and Martin Vechev. Certified defenses:
Why tighter relaxations may hurt training? arXiv preprint arXiv:2102.06700, 2021.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examPles in the Physical world.
arXiv preprint arXiv:1607.02533, 2016.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Sungyoon Lee, Woojin Lee, Jinseong Park, and Jaewook Lee. Loss landscaPe matters: Training
certifiably robust models with favorable loss landscaPe. OpenReview, 2021.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems, PP. 9464-9474, 2019.
Yuanzhi Li and Yingyu Liang. Learning overParameterized neural networks via stochastic gra-
dient descent on structured data. In Advances in Neural Information Processing Systems, PP.
8168-8177, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
54fe976ba170c19ebae453679b362263- Abstract.html.
Zhaoyang Lyu, Minghao Guo, Tong Wu, Guodong Xu, Kehuan Zhang, and Dahua Lin. Towards
evaluating and training verifiably robust neural networks. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, PP. 4308-4317, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris TsiPras, and Adrian Vladu. To-
wards deeP learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interPretation for
Provably robust neural networks. In International Conference on Machine Learning, vol-
ume 80 of Proceedings of Machine Learning Research, PP. 3575-3583, 2018. URL http:
//proceedings.mlr.press/v80/mirman18b.html.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examPles. In International Conference on Learning Representations, 2018a. URL https:
//openreview.net/forum?id=Bys4ob- Rb.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying
robustness to adversarial examPles. In Advances in Neural Information Processing Systems,
PP. 10900-10910, 2018b. URL https://proceedings.neurips.cc/paper/2018/
hash/29c0605a3bab4229e46723f89cf59d83-Abstract.html.
Hadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, SebaStien
Bubeck, and Greg Yang. Provably robust deeP learning via adversarially trained
smoothed classifiers. In Advances in Neural Information Processing Systems, PP. 11289-
11300, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
3a24b25a7b092a252166a1641ae953e7- Abstract.html.
11
Published as a conference paper at ICLR 2022
Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certified robust
training via better initialization and shorter warmup. arXiv preprint arXiv:2103.17268, 2021.
GagandeeP Singh, Timon Gehr, Matthew Mirman, Markus PuscheL and Martin T. Vechev. Fast
and effective robustness certification. In Advances in Neural Information Processing Systems,
pp. 10825-10836, 2018. URL https://proceedings.neurips.cc/paper/2 018/
hash/f2f446980d8e971ef3da97af089481c3-Abstract.html.
Gagandeep Singh, Timon Gehr, Markus PuscheL and Martin Vechev. An abstract domain for certi-
fying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):41, 2019.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. In Interna-
tional Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=rJUYGxbCW.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally
robust neural networks. arXiv preprint arXiv:1811.02625, 2018a.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal
safety analysis of neural networks. In Advances in Neural Information Processing Systems, pp.
6369-6379, 2018b. URL https://proceedings.neurips.cc/paper/2018/hash/
2ecd2bd94734e5dd392d8678bc64cdab- Abstract.html.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis
of neural networks using symbolic intervals. In 27th {USENIX} Security Symposium ({USENIX}
Security 18), pp. 1599-1614, 2018c.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6586-6595, 2019. URL
http://proceedings.mlr.press/v97/wang19i.html.
Zi Wang, Aws Albarghouthi, Gautam Prakriya, and Somesh Jha. Interval universal approximation
for neural networks. arXiv preprint arXiv:2007.06093, 2020.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane S.
Boning, and Inderjit S. Dhillon. Towards fast computation of certified robustness for relu net-
works. In International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 5273-5282, 2018. URL http://proceedings.mlr.press/
v80/weng18a.html.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, volume 80 of Proceed-
ings of Machine Learning Research, pp. 5283-5292, 2018. URL http://proceedings.
mlr.press/v80/wong18a.html.
Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling prov-
able adversarial defenses. In Advances in Neural Information Processing Systems, pp.
8410-8419, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
358f9e7be09177c17d0d17ff73584307-Abstract.html.
Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-
take-all. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=Skgvy64tvr.
12
Published as a conference paper at ICLR 2022
Kai Y. Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shafiullah, and Aleksander Madry. Training
for faster adversarial robustness verification via inducing relu stability. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
BJfIVjAcKm.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified
robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.
Bohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying l-infinity ro-
bustness using neural networks with l-inf-dist neurons. In International Conference on Machine
Learning ,pp.12368-12379. PMLR, 2021.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472-7482,
2019. URL http://proceedings.mlr.press/v97/zhang19p.html.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in Neural Information
Processing Systems, pp. 4944-4953, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/d04863f100d59b3eb688a11f95b0ae60-Abstract.html.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane S. Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In
International Conference on Learning Representations, 2020a. URL https://openreview.
net/forum?id=Skxuk1rFwB.
Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality. arXiv
preprint arXiv:2002.06668, 2020b.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
Difan Zou, Spencer Frei, and Quanquan Gu. Provable robustness of adversarial training for learning
halfspaces with noise. arXiv preprint arXiv:2104.09437, 2021.
13
Published as a conference paper at ICLR 2022
A Proof of Lemmas
A.1 Proof of Lemma 1
Proof. For all i ∈ [n],r ∈ [m], we first consider the change of indicator I(Wr (t)>Xi ±d∣Wr (t)kι >
0) during training compared to the value at t = 0 (the notation ± here means the analysis is consis-
tent for both + and - cases). Under the constraint that kWr(t)±Wr(0)k2 ≤ Rand kxik∞ ∈ [0, 1]d,
we have (see Appendix B.2 for details):
IWr (t)>Xi ± EkWr (t)kl - (Wr (0)>Xi ± EkWr (0)kl)| ≤ (1 + ^)√dR.	(15)
Thereby, if sign(Wr(t)>xi ± kWr(t)k1) 6= sign(Wr(0)>xi ± kWr(0)k1), then at initialization,
we must have
∣Wr (0)>Xi ± EkWr (0)∣∣11 ≤ (1 + E) √dR.
(16)
We want to upper bound the probability that Eq. (16) holds. It is easy to show that if the following
two inequalities hold, then Eq. (16) does not hold for sure:
∣Wr (0)>Xi∣ ≥ 2(1 + E)√dR,	(17)
EIlWr (0)kl ≤ (1 + E)√dR.	(18)
Therefore,
Pr (∣Wr (0)>Xi ± EkWr (0)∣ι∣ ≤ (1 + e)√dR
≤I-Pr (|Wr(O)F ≥2(1 + E)√dR and kWr(0)k1 ≤(I + 'HR
For Eq. (17), by anti-concentration inequality for Gaussian, we have
Pr(Wr(0)>Xi∣ ≤ 2(1 + E)√dR) ≤ 4(1 TGR.	(19)
2πξ
In other words, with probability at least 1 一 4(1 + e)√dR∕(√2πξ), Eq. (17) holds. And for Eq.
(18), by the tail bound of standard Gaussian and union bound, we have
Pr(EkWr(O)"I ≤ (1 + E)& ≥ 1 - 2dexp (一 2π⅛dR2).	QO)
Combining Eq. (19) and Eq. (20), Eq. (16) holds with at most the following probability
4(—R + 2d exp (一 2(1 + EW ).	(21)
√2πξ	∖	E)
Here we require E to be sufficiently small such that
(1 + E)√dR
√2∏ξ
≥ d exp
2(1 + E)2dR2
E2
(22)
—
and we can solve the inequality to obtain an upper bound for E (detailed in Appendix B.3):
√2dR
E ≤ iog(q2Rd ξ),
(23)
and in this case Eq. (21) holds with probability at least
6	,	.	r--
√2πξ(1 + MR.
14
Published as a conference paper at ICLR 2022
Therefore, we upper bound the probability:
6	6	6 . 6	6
Pr ( Sign(Wr(t)>Xi ± d∣Wr(t)kl ) = Sign(Wr(0)>Xi ± d∣Wr(0)kι)) ≤ -/=-(1 + e) √dR.
2πξ
Thereby
一一 一 一 「，「..，,一	............ 6	・ L
∀i ∈ [n],r ∈ [m], Pr(Ari(t) = A^(0)), Pr(AMt) = A-i(0)) ≤ √=-(1 + ≡) VdR.
2πξ
Note that at least one of Ar+i (t) and Ar-i (t) always remains zero during training, because condition
yiar = 1 in Ar+i(t) and condition yiar = -1 in Ar-i(t) are mutually exclusive. Then
6
Pr(Ari(t) + A-i(t) = Ari(0) + A-i(0)) ≤ ɪr(1 + 6√dR
2πξ
6
Pr(A+i(t) - Ari(t) = A+i(0) — A-i(0)) ≤ √2∏ξ (1 + ≡)√dR.
Next we can upper bound the probability that each of αrij (t), βrij(t), γrij (t) (∀i, j ∈ [n], r ∈ [m])
changes respectively:
.............................   ，、一	. ..................... 12 ,	, 「一
Prgrij ⑴=αrij (O)), PMerij ⑴=Brij (O)), Pr(Yrij ⑴=Yrij (O)) ≤	(1 + E) VdR.
2πξ
□
A.2 Proof of Lemma 2
Proof. With Lemma 1, we can bound the expectation of the change for each element in H(t) (Eq.
(6)) as:
E[|Hij(t)-Hij(O)|]
≤ mE(mR∣∣Xi∣∣2∣∣Xjk2 + EmR((∣∣Xik2 + IlXjIl2)k sign(wr(t))∣∣2) + E2dmR)
≤ Rd(I + 2e + €2)
12(1 + e)(1 + 2e + e2)d1.5
√2π
(∀i,j∈ [n])
Then by Markov’s inequality, we have that with probability at least 1 - δ,
kH(t) — H(0)k2 ≤ X IHij(t) — Hij(O)I ≤ 12(1 + €)(1晨；+€2)d1.5n2R.
i∈[n],j∈[n]	2πξδ
□
A.3 Proof of Lemma 3
Proof. First for simplicity, we define ρi = —Ar+i(O) + Ar-i(O) (ρi ∈ {—1, O, 1}), and
φ(Xi)(Wr (0)) = yi(Xi + EPi sign(Wr (0))1 (Wr (0)>^Xi + EPi sign(wr (0))) > 0).
To prove that λ0 > O, similar as Theorem 3.1 in Du et al. (2019b), we need to prove that for any
r ∈ [m], if η1, η2, ..., ηn (∀i ∈ [n], ηi ∈ R) satisfy Pin=1 ηiφ(Xi)(Wr(0)) = 0 almost everywhere
(a.e.) for any Wr(0), we have ∀i ∈ [n], ηi = 0.
In Theorem 3.1 in Du et al. (2019b), it is proved that for φ0(Xi)(w) = Xil(w>Xi) (i ∈ [n]), when
∀i = j, Xi # Xj holds, for any ηι,η2,..., ηn(∀i ∈ [n], η ∈ R), if
n
Xηiφ0(Xi)(Wr(0)) = 0,
i=1
15
Published as a conference paper at ICLR 2022
then ∀i	∈	[n], ηi	=	0.	For any r ∈ [m], by taking	∀i	∈	[n], x0i	= xi +	ρi sign(wr (0)),	we
have φ(xi)(wr (0)) = φ0(x0i)(wr (0)), and it holds that x0i ∈ B∞ (xi, ), x0j ∈ B∞ (xj , ). Then if
Xi # Xj, ∀i,j, η1,η2,…,ηn satisfy Pn=I mφ0(χi)(wr(0)) = 0 a.e., we have ∀i ∈ [n],ηi = 0.
Therefore if ∀i,j ∈ [n],i = j, ∀χi ∈ B∞(xi, e), ∀xj ∈ B∞(xj,e), χi # xj, if m，...,ηn satisfy
ηiφ(xi)(wr(0)) = 0,
i
then
ηiφ0(x0i)(wr (0)) = 0
i
also holds, and then ∀i ∈ [n], η = 0.	□
A.4 Proof of Lemma 5
Proof. The lemma can be proved by solving inequality
λmin(H(t)) ≥ λmin(H(0)) - 12(1 + ''^l；+	R ≥ y ∙	(24)
According to Lemma4, λma(H(0)) ≥ 4λ0. And with Eq. (8), in order to ensure λma(H(t)) ≥ λ0,
we can make
12(1 + 6)(1 + 2' + e2)d1-5n2 R1λo
√2∏ξδ	R ≤ T.
This yields
R ≤	√∏ξδλo
― 48(1 + ')(1 + 2' + '2)d1∙5n2 .
Note that 0 ≤ ' ≤ 1, and thus 1 + ' ≤ 2 and 1 + 2' + '2 ≤ 4 can be upper bounded by constants
respectively. Then we can take
R	√2∏ξδλo = cδλo
≤ 384d1∙5n2 = dr5n2,
where c
√2∏ξ
384
and in this case λmin(H(t)) ≥ λ0 w.p. at least 1 - δ probability.
□
A.5 Proof of Lemma 6
Proof. The proof of this lemma is inspired by the proof of Lemma 5.4 in Zou et al. (2018). In our
proof, we define f(x) = (f(x1), f(x2), ..., f(xn)), where f(x) is a scalar function and x is a vector
of length n. When λmm(H)(s) ≥ λ0 holds for 0 ≤ S ≤ t, we can bound the derivative of L(t):
16
Published as a conference paper at ICLR 2022
i=1
n
n
=-∑ l0(Ui)∑ l0(Uj )Hij
=-l0(u)> Hl0(U)
≤ — λ0 X IE
i=1
≤ λ0 X l0(ui)
(iii)
≤
i=1
n
-20 X min(1/2,
i=1
(iv)	λ0	n
≤ --2 min (1∕2,∑
(v)
≤
i=1
— λ200 min(1∕2,L2⅛
λ0	1
-------------.
2 2 + 2∕L(u),
—
where (i) is due to λmin(H) ≥ λ0, (ii) is due to -l0(u) ≤ 1, (iii) holds due to the following property
of cross entropy loss -l0(u) ≥ min(1∕2,竽),(iv) holds due to the function min(1∕2,x) is a
concave function and Jenson,s inequality, (v) holds due to min(α, b) ≥ [/。+]译
Therefore, we have
2 dL(u) . _2 dL(u) ≤ - λ0
dt L L(U) dt — — 2
By integration on both sides from O to t, we have
L(u(t)) - L(u(0)) + log(L(u(t))1 - log Γl(i
λot
Therefore, we have
log (L(U(U)) ≤ -λ4t + L(U(O)) + log (L(U(O)))
which yields
L(U(t)) ≤ exp ( — λ0t) exP (L(U(O)))L(u(0))∙
And we can bound the change of wr .
dwr (t)
dt
2
=II叩
dwr	2
In	1
=	(Ui) -^ar yiσ (<Wr, Xi ± e∣∣WrkIi)(Xi ± e∣∣Wrkl )
W	√m
1n
≤√m X kl0(ui )k2
n
≤ ;
2
17
Published as a conference paper at ICLR 2022
where σ0(∙) stands for the derivative of the ReLU activation. Thus
nt
∣∣wr ⑴-Wr (O)Il2 ≤ -J=
□
A.6 Proof of Lemma 7
Proof. We first prove the standard training part. As we have defined previously that
n
L(0) = X log(1 +exp(-ui(0))),
i=1
where
1m
Ui(0) = y √m £a『σ(Wr (0)>Xi)∙
r=1
For each arσ(Wr (0)>xi), r ∈ [m], i ∈ [n], note that the randomness only comes from random
initialization for Wr, there is 1 possibility that it is equal to 0, and another 2 possibility that it
follows a normal distribution N (0, σi2 ), where σi = ∣xi ∣22 . Therefore, we have
E(ar σ(Wr (0)> xi)) = 0,
Var(ar σ(wr (0)> Xi)) = σi-
m
E	X arσ(Wr(0)>Xi) = 0,
r=1
1m	2
Var (√m X ar σ(Wr(0)>xi)) = ~2
Therefore, by Chebyshev’s inequality, we can bound
Pr(%(0)∣≤ σi2) ≥ 1 - δ.
2δ
And We can bound L(O) = O(n maXi=1 σ ) = O(δ) With probability at least 1 — δ.
For IBP training,
1m
Ui(O) = √m X l(yiar = 1)σ(wr (0)>Xi — EkWr (0)∣∣1) + l(ji ar = 1)σ(Wr (0)>Xi + EkWr (0)∣∣1)∙
r=1
Thus
∣Ui(0) - Ui(O)I ≤ -ɪme∣Wr(0)kι = √me∣Wr(0)kι∙
√m
By E(kWrk1) = O(d) and Markov’s inequality, With probability at least 1 - δ,
.,八 .,, 八/ mιmdc 、
IUi(O) - Ui(O)I ≤ O(一δ一 ).
And we can bound L(O) = O( n√mde + n) with probability at least 1 - δ.	□
18
Published as a conference paper at ICLR 2022
B Detailed Derivation for Other Equations or Inequalities
B .1 derivation on the dynamics of ui (t)
We provide a detailed derivation on the dynamics of Ui (t) presented in Eq. (5), which We use Hi (t)
to describe 第Ui(t):
2X(
r=1 '
m /
=Σ(
r=1
∂Ui(t) dWr (t)∖
∂Wr (t) , dt /
∂Ui(t)	∂L(W(t), a)
—
∂Wr (t) ,	∂Wr (t)
X / ∂Ui(t)
r=1 ∖dwr(t)'
—
XX _//(U ) XX / dui(t)辿⑴
M (Uj ) ⅛ V Wr (t),∂ Wr (t)
n
E -l0(Uj)Hij(t),
j=i
B.2 DERIVATION for EQ. (15)
Eq. (15) basically comes by triangle inequality:
∣Wr(t)TXi - d∣Wr(t)k - (Wr(0)>Xi - E∣∣Wr(0)∣∣ι)∣
=I (Wr (t) - Wr (0))τXi - d∣Wr (t)∣∣1 + E∣∣Wr (0)∣∣1∣
≤ |(Wr (t) - Wr (0))τXi∣ + € ∣ IlWr (t)∣∣ι -IlWr (0)∣∣1]
≤ ∣(Wr (t) - Wr (0))τXi∣ + e∣∣Wr (t) - Wr (0)||i
≤ (1 + e)√dR.
B.3 Derivation for eq. (23)
We solve the inequality in Eq. (22) to derive an upper bound for € in Eq. (23):
-ɪ(l +	e)√dR ≥	-ɪ√dR	≥ dexp	( - 2(I	+ ?)dR
√2πξ	√2πξ	∖	€2
R
2(1 + €)2dR2
€2
≥ exp -
log
2(1 + e)2dR2
€2
and then we can require
2(1 + e)2dR2
€2
√2dR
⇒ € ≤ ---.	—,
log(√ 黑 ξ)
19