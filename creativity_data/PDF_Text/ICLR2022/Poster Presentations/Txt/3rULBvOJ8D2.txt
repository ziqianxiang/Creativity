Published as a conference paper at ICLR 2022
Unraveling Model-Agnostic Meta-Learning
via The Adaptation Learning Rate
Yingtian Zou, Fusheng Liu, Qianxiao Li
National University of Singapore, Singapore
{yingtian, fusheng}@u.nus.edu, qianxiao@nus.edu.sg
Ab stract
Model-Agnostic Meta-Learning (MAML) aims to find initial weights that allow
fast adaptation to new tasks. The adaptation (inner loop) learning rate in MAML
plays a central role in enabling such fast adaptation. However, how to choose
this value in practice and how this choice affects the adaptation error remains
less explored. In this paper, we study the effect of the adaptation learning rate in
meta-learning with mixed linear regression. First, we present a principled way
to estimate optimal adaptation learning rates that minimize the population risk
of MAML. Second, we interpret the underlying dependence between the optimal
adaptation learning rate and the input data. Finally, we prove that compared with
empirical risk minimization (ERM), MAML produces an initialization with a
smaller average distance to the task optima, consistent with previous practical
findings. These results are corroborated with numerical experiments.
1	Introduction
Meta-learning or learning to learn provides a paradigm where a machine learning model aims to
find a general solution that can be quickly adapted to new tasks. Due to its fast adaptability, meta-
learning has been widely applied to challenging tasks such as few-shot learning (Vinyals et al., 2016;
Snell et al., 2017; Rusu et al., 2018), continual learning (Finn et al., 2019; Javed & White, 2019),
and neural architecture search (Zhang et al., 2019; Lian et al., 2019). One promising approach in
meta-learning is Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017), which consists of two
loops of optimization. In the outer loop, MAML aims to learn a good meta-initialization that can
be quickly adapted to new task in the inner loop with limited adaptation (parameter optimization)
steps. The double loops optimization serve as “learning-to-adapt” process, thus enabling the trained
model to adapt to new tasks faster than direct Empirical Risk Minimization (ERM) algorithms (Finn
et al., 2017; Raghu et al., 2020). Recent works (Nichol et al., 2018; Fallah et al., 2020; Collins
et al., 2020; Raghu et al., 2020) attribute the fast adaptability to the phenomenon that the learned
meta-initialization lies in the vicinity of all task solutions. However, the theoretical justification of this
empirical statement, and more generally how fast adaptability of MAML depends on the inner loop
optimization remains unclear. As a key component of MAML, the adaptation (inner loop) learning
rate (hereafter called α) is shown empirically to plays a crucial role in determining the performance
of the learned meta-initialization (Rajeswaran et al., 2019). In particular, the value of α bridges
ERM and MAML, in the sense that the latter reduces to the former when α = 0. However, from a
theoretical viewpoint, the dependence of MAML performance on the choice of α remains unclear,
and furthermore, there lacks a precise practical guideline on how to pick a near-optimal value.
In this paper, we address these issues by answering the following two questions: (1) How to choose
the optimal α that minimizes population risk of MAML? (2)What is the effect of α on fast adaptability
of MAML? To this end, we consider the mixed linear regression problem with random feature models.
For the first question, we derive the optimal α which minimizes the population risk of MAML in the
limit of an infinite number of tasks. This can then be used to estimate an effective α prior to training.
Moreover, we analyze the underlying statistical dependency between the optimal α and the input
data, e.g. relation to the moments of data distribution. This in turn allows the heuristic application of
our results beyond linear models, and we demonstrate this with experiments. To answer the second
question, we compare MAML with an ERM algorithm (without inner loop optimization) in order
to reflect the effect of α in optimization. As stated in many works, like Nichol et al. (2018), that
1
Published as a conference paper at ICLR 2022
meta-initialization learned by MAML in parameter space is close to all training tasks thus contributes
to fast adaptability. We conduct an experiment and observe that MAML with a not too large α yields
a shorter mean distance to task optima than ERM. To justify this empirical finding, we define a
metric measuring the expected geometric distance between the learned meta-initialization and task
optima. We prove that in our setting, the MAML solution indeed possess a smaller value of this
metric compared with that of ERM for small α, providing theoretical evidence for the observed
phenomena. Our contributions can be summarized as follows:
•	We provide a principled way to select the optimal adaptation learning rate a* for MAML
which minimizes population risk (Theorem 1 & Proposition 1). We also interpret the
underlying statistical dependence of a* to input data (Corollary 1) with two examples.
•	We validate the observation that MAML learns a good meta-initialization in the vicinity of
the task optima, which reveals the connection between the adaptation learning rate α and
the fast adaptability in optimization. (Theorem 2)
•	We also extend our result about the choice of a* to more practical regime, including deep
learning. All of our theoretical results are well corroborated with experimental results.
2	Problem formulation
We study the MAML algorithm under the mixed linear regression setting. Suppose we have a task T
that is sampled from the distribution D(T). Each task T corresponds to a linear relationship
X—— χτ,ι ——∖
yτ = Φ(Xt)aτ, XT =	…	,Xt ∈ RK×dχ, Φ(Xt) ∈ RK×d, aτ ∈ Rd.
∖ 一 xτ,κ -
(2.1)
where XT ∈ RK×dx is the input data of task T which has K vector samples {xT,1, ...,xT,K},xT,j ∈
Rdx i.i.d sampled from D(x) 1. For each input data, we have a mapping φ : Rdx → Rd transform
each point of XT from input data space Rdx to a d-dimensional feature space Rd where we denote
the transformation of all data in task T by Φ(XT) = [φ(XT,1), ..., φ(XT,K)]> as the feature of
that task. Then, we assume optimal solution aT ∈ Rd for task T is i.i.d sampled from D(a). The
corresponding label yT ∈ RK can be obtained from (2.1).
Our target is to learn a model to minimize the risk of different tasks across D(T). Note that each task
T is determined by a feature-solution pair (Φ(XT), aT). Therefore, we can formulate this multi-task
problem with parameter space Rd and loss function ` as
mi% ET〜D(T)[' (w; T)] = min Ea〜D(a)Eχ〜D(X) [' (w; Φ(X), a)]	(2.2)
w∈Rd	w∈Rd
To solve this problem, ERM and MAML algorithms yield different iterations. Specifically, ERM uses
all data from all tasks to directly minimize the square error loss `, such that population risk of ERM is
1	2
Lr (w, K) := E E — Φ(X)w - Φ(X)a	(2.3)
a〜D(a) X〜D(X) K	2
As a counterpart, MAML first adapts with an adaptation learning rate α on each task using its training
set - a subset of task data in the inner loop. Then, in the outer loop, MAML minimizes the evaluation
loss for each adapted task-specific solution using a validation set. For simplicity, since data is i.i.d
sampled from the same distribution, we first consider the setting where all data in each task is used as
training set and validation set in our main results. We present later the extension of these results to
the case with a different train-validation split. (Please refer to Appendix H.1)
Thus, the general population risk of one-step MAML is defined by
Lm(w,α,Κ) :=	E E ɪ '( W - aVw'(w;Φ(X), a);Φ(X), a )	(2.4)
a〜D(a) X〜D(X) K	∖|	{z	}	J
Inner Loop
1For simplicity, we denote this sampling and stacking multiple examples to a matrix process as X 〜D(X)
2
Published as a conference paper at ICLR 2022
In practice, we use the empirical objective function as a surrogate objective function. We first sample
N tasks with task optima {a1, ..., aN} from D(a) and then sample K data for each task. Then, the
empirical risk of MAML can be specified as Lm
O ,	一 __
Lm(w,α, N, K)
1 N 2
NK X φ(Xi)Wi - φ(Xi)ai
i=1	2
(2.5)
where wi0 = w - 2αΦ(Xi)> (Φ(Xi)w - Φ(Xi)ai) /K is adapted parameters of task i after inner
loop. Correspondingly, we apply ERM algorithm to the same problem by removing inner loop
(setting α = 0), thus the empirical risk of ERM is denoted as Lr(w, N, K). In addition, we follow
the original MAML (Finn et al., 2017) to use the same α for training and testing.
Notation We denote an optimal adaptation learning rate as α*. Global minima of empirical risk of
MAML and ERM (when they are unique) are denoted by wm, wr. We write {1, ..., N} as [N] and
use k ∙ k to denote the Euclidean norm. We use subscripts to index the matrices/vectors corresponding
to task instances, and bracketed subscripts to index the entries of matrices. Other notations are
summarized in Appendix Table 1.
Assumption 1 (Normalization). For simplicity, we consider a centered parameter space such that
Ea〜D(a) [a] = 0 and Var [a] = σ^2.
Assumption 2 (Bounded features). With probability 1, the covariance matrix of input features
Φ(X)>Φ(X) is positive definite and has uniformly bounded eigenvalues from above by λS > 0 and
below by λI > 0.
3 Main results
In this section, we analyze MAML through the adaptation learning rate α. Our derived insights are
summarized into three theoretical results: (1) The estimation of an optimal adaptation learning rate
a* which minimizes MAML population risk; (2) The statistical meaning of a* in terms of the data
distribution, and (3) The geometric interpretation of the effect of α on fast adaptability of MAML
compared to ERM.
3.1	ON THE OPTIMAL ADAPTATION LEARNING RATE α*
x-r τ i'	,1	1	,	∙	1	/ T√-、	7∖ zɔ ∙	.1	∙ ∙	1	1 ∙	, ∙ i' . ∙	Λ Λ
We focus on the underparameterized case (K ≥ d). Given the empirical objective functions Lr, Lm
defined in (2.5), we can derive the global minima by the first-order optimality condition. We obtain
the global minimum of ERM wr and minimum of MAML wm in the following closed-forms,
Wr= Wr ({1(Xi), ai}i∈[N ]) = (X φ(Xi)>φ(Xi))	( X φ(Xj )>φ(Xj )aj)
i∈[N]	j∈[N]
wm(α) = Wm ({Ci(a), ai}i∈[N])= ( X Ci(α)>Ci(α))-1 ( X Cj(α)>Cj(α)ai
i∈[N]	j∈[N]
(3.1)
where Ci(α) = Φ(Xi) [I - (2ɑ∕K)Φ(Xi)>Φ(Xi)] ,Ci(α) ∈ RK×d can be viewed as the adapted
feature of task i. Observe that Wm (α) (and thus the MAML algorithm) depends on α. If α = 0,
MAML reduces to ERM. For large a, instabilities may occur, thus there may exist an optimum, α*
that minimizes the MAML population risk. The later intuition is worthwhile to be proved, from
which we do not have a principled way to guide the choice of optimal hyperparameter a* for MAML
so far. To this end, we focus on the generalization error by taking the population risk on the global
minimum of empirical risk. In particular, we consider the population risk of the MAML optimizer in
the average sense, where the average population risk is
Lm(α, N, K) = EwmLm(wm, α, K)	(3.2)
whose minimizer we denote as ɑ*(N, K). In this way, we eliminate randomness of the global
minimum Wm learned from sampled tasks. The following theorem gives a precise value of ɑ* (N, K)
in the limit N → ∞.
3
Published as a conference paper at ICLR 2022
Theorem 1.	Under assumptions 1 &2,we have as N → ∞, α*(N, K) → 0^(K), where
*	K tr[Eχ [(Φ(X )>Φ(X ))2]]
αlim( )=2tr[Eχ[(Φ(X)>Φ(X))3]],
Φ(X) ∈ RK×d, K is the sample size per task and N is the number of tasks.
(3.3)
The proof is found in Appendix B. In this theorem, We give the nearly optimum α^ which is an
alternative form for true optimal α, namely a*, to minimize the MAML generalization error. As
dictated in (3.3), the desired a* is determined by the feature covariance matrix in expectation.
Remark. The precise derivation of the case where N is finite is complicated, thus we derive the
limiting case here as an estimator oftrue α*. Our estimation a^ is the unique minimum. We will
show later that this allows us to compute near optimal values efficiently in practice, each of which is
close to the optimal α*(N, K) in corresponding problem.
Remark. The estimator (3.3) can be generalized to different scenarios. For overparameterized
models, we obtain a similar result for the minimum norm solution if the number of tasks N is limited
(NK d). Further, we show a computationally efficient estimator (H.15) in Appendix H.2. For deep
learning, we can compute a range ofeffective α values based on α^. We also give the numerical
form when the training data is different from the test data in each task. These are presented in
Appendix H.4 and H.1 respectively.
In the above we considered the average population risk (3.2). This simplifies the calculations of
finding the a*. Below, we justify this simplification by showing that in the limit of large number of
tasks, the average population risk is a good estimate of the true population risk.
Proposition 1 (Informal). Assume u = C (α)> C (α)a is sub-gaussian random variable with sub-
gaussian norm ku(i) kΨ2 ≤ L, assumption 1 & 2 hold, then with probability at least 1 - δ that
Lm(Wm,α, K) -Lm(α,N,K)
L2	d IdE(α,K) ,~2 ε(α, K) ,	2 1
≤ K max[∖l~λN2λ l°g δ,⅜j l°g δ∫
(3.4)
where ε(α, K) = O(1/(c0 + α)2). Here c0 > 0 is a constant and d is the feature size.
The proof is found in Appendix C. Proposition 1 complements Theorem 1 by guaranteeing that the
gap between the average population risk and population risk with same argument α will disappear
along with N goes to infinity. Large α makes the bound tighter while small α makes ε(α, K)
converge to a positive constant; thus (3.4) provides a non-vacuous bound with regard to α. Hence, it
is justified to make an estimation of a* using the average case. By Theorem 1 and Proposition 1, we
give an explicit form to estimate a* for MAML where this estimation α^ is not too far from the
true α* of a specific case. Later experiments show our estimation α^ is close to true a* in both
underparameterized and overparameterized models (see Section 5.1). This is meaningful for selecting
an a* minimizing MAML risk, instead of randomly choosing it. Previous work (Bernacchia, 2021)
explores on this by giving a range of a* may exist for the linear model. Instead, we show a fine result
that we provide a certain value estimator of α*. (Details refer to Appendix H.5)
Relation to data distribution. After estimating the value of a* through Theorem 1, we are now
interested in the statistical interpretation of α*. In particular, we aim to summarize the dependence
of an estimation of a* on the distribution of the inputs and tasks. This in turn allows us to devise
strategies for choosing near optimal α for MAML beyond the simple settings considered here.
Corollary 1. With a feature mapping φ : Rdx → Rd for each data x ∈ Rdx, the αl*im in Theorem 1
will satisfy the following inequality
1
2dσ2(φ(xj ...,φ(xκ))
≤ (Oiin ≤ 2σ2(φ(xι),…,φ(xκ))
(3.5)
whereσ2(φ(x1), . . . , φ(xK)) is variance of the feature.
See proof in Appendix D. According to Corollary 1, we can see that αl*im is bounded by the statistics
of the input data. These bounds are governed by the standard derivation terms. More specifically, our
estimator (3.3) holds an inverse relationship to higher order moment of data distribution while its
4
Published as a conference paper at ICLR 2022
(a) Visualization of solutions and trajectory
Figure 1: (a) Visualization of trajectory of MAML solution wm (α). Orange dots are task optima
{ai }[N] of sampled tasks, where location of ai is decided by its entries. Red dots highlighted in red
circle are newly coming tasks. Green cross is wr , (α = 0) while the purple trajectory is generated as
a increasing. Red star is Wm(αjim). (b) Average euclidean distances of Wm(α) and {ai}[N] display
corresponding points in left figure. Black arrow is the tangent line. Best viewed in colors.
(b) Mean solution distances
bounds (3.5) have an inverse relationship to data variance. As a consequence, the αjim for different
problems mainly depend on the standard derivations. For example, Ofim and thus α* will shrink to
zero as the variance of data increases and vice versa. In other words, small α is tailored to those
tasks with large data variance when the model size is fixed. To illustrate the insight more clearly,
we present two examples — regression with polynomial basis functions (Example 1) and the case
where Φ(X) is a random matrix with a prescribed distribution (see Appendix E). In this following
example, we narrow the range and get the exact relationship where the expression of αl*im rather
than its bounds depend on data variance and model size d. In later experiments, we also validate this
relationship on various models with different basis functions.
Example 1 (Polynomial basis function). Assume we have K i.i.d samples x1,..., xκ 〜N(0, σ2)
for each task. Consider polynomial basis function φ : R → Rd, where φ(x) = (1, ..., xd-1). Then
value of αl*im has an inverse relationship to σ2 and dimension d (Proof is in Appendix E).
3.2 Geometric Interpretation of MAML Adaptation
In another direction, we aim to investigate geometric properties of the meta-initialization learned by
MAML as α varies. In previous experimental investigations, it is suggested that MAML learns the
near meta-initialization to all tasks Nichol et al. (2018) or trade-offs on easy and hard tasks Fallah
et al. (2020). We can also observe the new phenomena in toy experiments. As shown in the Figure
1 (a), we sampled 500 tasks in R2 parameter space. Specifically, we i.i.d sample and stack data as
Xi ∈ RK×2,〜D(x) and task optima Qi 〜D(a), Qi ∈ R2 (scattered orange dots) for each task i.
Green cross shows the location of MAML solution Wm (α) with α = 0, namely ERM solution Wr.
Since D(x), D(Q) are some symmetric zero-mean distributions, the optimal solution is expected at
the origin. When several new training tasks (with higher penalties) have been added as shown in the
red circle area, then new Wr will be closer to new tasks. Along α increasing, Wm (α) generates a
trajectory shown as the purple curve. The dynamics of global minimum Wm(α) will start from green
cross and move away from the red circle until reach an optimum location of point (red star Wm(αl*im))
which minimizes the total distances to red and orange dots (Other cases shown in Appendix H.4).
It indicates that the effect of α (inner loop optimization) is to help MAML minimize total distances
to all training task optima. Unlike ERM learning a biased solution to dense tasks area, MAML
converges to a distance-aware solution that tries to minimize the distances within one-step adaptation
at stepsize α. The αl*im is the optimum adaptation stepsize to learn the optimum location of point, or
nearest point, to all tasks. Figure 1 (b) displays the mean distance for each point in purple trajectory to
all tasks. As we can see, the distance decreases at beginning as α increases until reach the minimum.
To theoretically prove the insight in Figure 1, we characterize it by measuring the average post-
adaptation distance between the meta-initialization (global minimum) learned by a specific algorithm
and task optima in a task distribution.
5
Published as a conference paper at ICLR 2022
Definition 1 (Average Distance under Fast Adaptation). Given task distribution D(T), meta-
initialization wA0 learned by algorithm A, optimum aT of task T, the average distance under
t-step (t ≥ 0) fast adaptation is defined by
Ft(wA0)
"T ~D(T)kwAt ,T-aTk2,
t
Wta,t = WA - η X Vw'(WA,t , T)
j=1
(3.6)
where WAt ,T is the adapted parameter of task T with t steps, η is the step size, ` is the loss function.
Ft evaluates the distance between adapted parameters and true task optimum for a given meta-
initialization at any adaptation step t. If t is small, Ft describes the fast adaptation error in solution
distance of the meta-initialization learned by an algorithm. Hence, we can measure the fast adaptability
of MAML with Ft(Wm). Observe that for small α, Wm can be linearized as
Wm (α) = Wr + αVαWm(0) + O(α2)	(3.7)
In this regime, the effect of MAML is dictated by the α gradient VαWm(0), which can be visualized
as the tangent of the purple curve at the green cross in Figure 1(b). By comparing Ft of the meta-
initializations Wr , Wm learned by ERM and MAML, we are able to find the connection between
α and the fast adaptability in meta-learning, at least in the small alpha regime. For simplicity, we
assume that the input data features are uncorrelated, thus the covariance matrix is diagonal.
Theorem 2.	Let Wm(α), Wr be the meta-initializations learned from T1, ..., TN by MAML and ERM.
-2λ - λ s 1	—	- -	- 2 s K+	Le -2λS κ+κ√ 4λS+1∙5cλ4 3λS-λ6 ”λS	,	1
With Ft(∙), underAssumption 1 & 2, for any α ∈ 0,------------ λ2S4λ6 -λ6I)——S——I---S atnumber
of step t, we have
ETι,…,Tn~D(T) [Ft(wr) - Ft(wm(α))] ≥ (1 - KλS)	NKλ
K NKλS
(3.8)
where η is the step size in Definition 1, ce > 0 is a constant.
See proof in Appendix G. This theorem prove our insight at small α that MAML has smaller average
solution distance than ERM. As it illustrated in Theorem 2, at any step t ≥ 0, Ft (Wm (α)) ≤ Ft (Wr )
holds if α is smaller than some constant. This means adapting to different tasks with MAML meta-
initialization leads to shorter average solution distance than ERM’s at any number of adaptation steps.
But the gap will disappear along number of steps t increasing to infinity, which is sensible. Note
that even t = 0, this inequality still holds true. Therefore the meta-initialization of MAML Wm
has shorter expected distance to new task than ERM Wr before adaptation. Theorem 2 has revealed
the connection between α and fast adaptability. Even with small α, MAML learns a more adaptive
solution than ERM which is closer to the new tasks in expectation enabling quick approximation.
It benefits from learning a closer meta-initialization for all tasks on average. Thus α plays a role in
learning a distance-aware solution. This result is consistent with our observation in the Figure 1.
Compared to ERM algorithms, the fast adaptability of MAML stems from the learned meta-
initialization determined by the adaptation learning rate α. When facing a multi-task problem,
traditional ERM algorithms bias its learned initialization to minimize the averaged risk. However, this
strategy fails to take the further adaptation into account, and thus learns a solution far from unknown
task optima. On the contrary, MAML learns a distant-aware meta-initialization and converges to the
vicinity of all task optima with a limited adaptation budget (Nichol et al., 2018; Rajeswaran et al.,
2019), or tends to favor “hard tasks” (Fallah et al., 2020; Collins et al., 2020). Hence, before adap-
tation, ERM may have lower population risk than MAML. However, after adaptation, the situation
will reverse since MAML can adapt to most unknown task optima closer (see Figure 5(a)). This
benefit is also illustrated by (Zhou et al., 2020) that the shorter solution distance leads to a better
meta-initialization for fast adaptation. We note that “task hardness” may not always be easy to define,
especially for non-linear cases (Collins et al., 2020). Here, we instead focus on directly analyzing the
geometric distance (Theorem 2), which has substantiated the aforementioned findings in optimization
behavior from different angles.
4	Related Work
Meta learning learns a general solution based on previous experience which can be quickly adapted
to unknown tasks (Finn et al., 2017; Li et al., 2017; Snell et al., 2017; Vinyals et al., 2016; Nichol
6
Published as a conference paper at ICLR 2022
(a) NTK (Uniform initialization)
Figure 2: Loss of overparameterized quadratic regression with regard to α. Triangles in the dash-dot
line is the mean loss across whole tasks. The error bar denotes 95% confidence interval on different
tasks. Red stars are estimations α^.
(b) NTK (Normal initialization)
et al., 2018; Grant et al., 2018; Harrison et al., 2018; Rusu et al., 2018; Rajeswaran et al., 2019;
Finn & Levine, 2018; Rajeswaran et al., 2019; Finn et al., 2018; Yin et al., 2020). One promising
approach to meta-learning is Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) which
learns a meta-initialization such that the model can adapt to a new task via only few steps gradient
descent. Understanding the fast adaptability of meta-learning algorithm, especially on MAML, is
now an important question. As a variant of MAML, (Nichol et al., 2018) attribute fast adaptation
to the shorter solution distance, and devises a first-order approximation algorithm based on this
intuition. Other first-order methods (Denevi et al., 2019; Zhou et al., 2019) try to achieve adaptation
by adding a regularized term to get a distance-aware solution. (Raghu et al., 2020) shows that
even performing inner loop optimization on part of the parameters still leads to fast adaptation. A
shared empirical finding of these results is that MAML produces initial weights that are closer to the
population optimum of individual tasks on average, and it is argued that this partly contributes to its
fast adaptibility. Here, we present a rigorous result that confirms the distance reduction property of
MAML, at least in the considered setting, lending theoretical backing to these empirical observations.
On the theoretical front, analyses of meta-learning mainly focus on generalization error bounds and
convergence rates (Amit & Meir, 2018; Denevi et al., 2019; Finn et al., 2019; Balcan et al., 2019;
Khodak et al., 2019; Zhou et al., 2019; Fallah et al., 2020; Ji et al., 2020b; Zhou et al., 2020; Ji et al.,
2020a). For example, Fallah et al. (2020) studies MAML by recasting it as SGD on a modified loss
function and bound the convergence rate using the batch size and smoothness of the loss function.
Ji et al. (2020b) extend this result to the multi-step version of MAML. Other works (Charles &
Konecny, 2020; Wang et al., 2021; Gao & Sener, 2020; Collins et al., 2020) investigate the MAML
optimization landscape and the trade-off phenomena in terms of task difficulty: e.g. MAML tend to
find meta-initializations that are closer to difficult tasks. However, the effect of inner loop learning
rate α on the MAML dynamics and learned solution are not explored in these works.
Of particular relevance is the work of Bernacchia (2021), which derives, under an ideal setting of
Gaussian inputs and regression coefficients, a range of α values that can help guide its choice. In
this paper, we adopt a more general setting, where we do not assume specific input distributions. We
derive a precise optimal value of α (instead of a range), which can be estimated from input data.
Furthermore, we show using experiments that the optimal values may not be negative (c.f. Bernacchia
(2021)) in the standard meta-learning setting, where the same α is used for training and testing.
5	Experiments
5.1	Estimation of a*
We verify our theorem through Neural Tangent Kernel (NTK) (Jacot et al., 2018) and deep learning
on the omniglot dataset (Lake et al., 2011). in the former setting, we followed the problem setup in
(Bernacchia, 2021) to perform quadratic regression. Different from their model size of 60, we used
a two-layer Neural Tangent Kernel (NTK) (Jacot et al., 2018) with sufficiently wide hidden layers
(size 10,000). Then, We can estimate a* by the neural tangent feature to obtain α% = 1∕(2NKσ2)
7
Published as a conference paper at ICLR 2022
Figure 3: Test loss and accuracy on Omniglot 20-way 1-shot classification. The blue and orange
line represent the test loss (left) and test accuracy (right) of original configuration in ANIL (Raghu
et al., 2020) paper and our online estimation. The shadows are the standard deviation of multiple
experiments with different seeds.
(b) Omniglot 20w1s Test Accuracy
Figure 4: Value of a^ along the data variance, σ2. Different curves are different data distributions.
(a)The feature of Gaussian basis function. These curves can be perfectly fitted by an inverse
proportional function. (b) The feature of uniformly initialized NTK model.
(σ is the variance of NTK feature, whose derivation is found in Appendix H.2). Shown as a vertical
dotted line ending with the red star in Figure 2, we can see our estimation is nearly optimal. To
reduce fortuity, we choose arbitrary values of N, K to compute the estimation aRmr. Furthermore, we
also test our estimation on uniform initialization with other groups of hyper-parameters and obtained
similar results. Then, for deep learning classification, we use online estimation to compute α* for
ANIL Raghu et al. (2020) on the Omniglot dataset Lake et al. (2011). To keep training stable, we
normalize the features before the last layer and compute the corresponding α%. Then, we compare
our estimation scheme with the default selection method where the model and training learning rates
are the same. Test loss and accuracy are reported with mean and variance in Figure 3. Both training
schemes achieve similar results after 4 × 104 iterations. We only plot the first 1.5 × 104 iters (20
iters per scale) to see the differences clearly. As shown, our estimation of a* converges faster than
that in the default configuration. Other experimental parameters and additional results, including
non-central distributions and deep regression experiments, are found in Appendix H.4. Overall, these
experiments suggest that our estimation derived in the idealized linear setting can guide practical
hyper-parameter selection.
5.2	RELATION OF DATA VARIANCE AND OPTIMAL α
In this section, we verified our theoretical results of α^ and its relation to data variance. As
drawn the Figure 4, value of a^ and data variance have an inverse relationship. We first verified
8
Published as a conference paper at ICLR 2022
Figure 5: With different data distributions, Xi 〜U(-5, 5),N(0, 2),Exp(1) (curves in different
colors) (a) the loss difference between MAML and ERM with t steps adaptation on each task
Pi [`it (wm) - `it (wr)] ( `it (wm) is the t-step adaptation loss on task i from the MAML learned
initialization wm) and (b) Average solution distance gap of MAML and ERM after t-step adaptation,
Ft (wm) - Ft (wr).
this with a Gaussian basis function Φ(X)(j)= exp(-(X(j)— 〃j) /2σi). Then, we conducted
experiments on three different data distributions: normal distribution N (0, σ), uniform distribution
U(—√z12σ∕2, √z12σ∕2) and exponential distribution Exp(1∕σ). From in (a), we can see the smooth
curves perfectly fitted with some inverse proportional function e.g. y = 0.35∕σ2. Next, we used
NTK as the basis function to verify our result in overparameterized regime. We used two layers MLP
with width= 10, 240 and uniform initialization to compute the neural tangent feature. As we can see
from Figure 4(b), the diagram also shows the tendency that a^ decreases as σ increasing. As a
consequence, variance, as a part of the statistical property of data, will influence α*.
5.3	Fast adaptation
To understand the effect of α on Ft, we set α = 10-4 to train MAML such that its global minimum
wm is inched from ERM wr . Then, we tracked their adaptation losses and adaptation errors
with growing adaptation steps, shown in the Figure 5. Adaptation loss for task i is defined by
`it(w) = kΦ(Xi)Adapt(w, i, t, η) - yi k2 where Adapt(w, i, t, η) is t-step adaptation parameter with
learning rate η = 1e - 5. The adaptation loss difference between MAML and ERM is described
as Pi5=0010 `it(wm) - `it (wr). From Figure 5 (a) we can see, the loss of MAML is marginally higher
than ERM before adapting. But the difference dramatically decreases to negative values, which
illustrates that MAML has better performance than ERM with only few steps adaptation. Similar
results appear on various data distributions: uniform distribution U (-5, 5), normal distribution
N(0, 2) and exponential distribution Exp(1). It makes sense because wr, wm are the minimizers
of non-adaptation loss and one-step adaptation loss, respectively. Then we plot the difference of
adaptation errors in distance Ft (wm) - Ft (wr) along adaptation step t. In Figure 5(b) we can see,
Ft of MAML is always smaller than ERM’s, including t = 0. Since Ft measures distances of adapted
solution and task optimum solution, this result has substantiated our Theorem 2. Furthermore, it
also demonstrate that the effect of α, even it is small, is acting as the guide to find a distance-aware
meta-initialization for target tasks which possesses faster adaptability compared to ERM.
6	Conclusion
In this paper, we investigated MAML through the lens of adaptation learning rate α. We gave a
principled way to estimate an optimal adaptation learning rate α* minimizing MAML population
risk. We also try to interpret the role of α statistically and geometrically. Further investigation has
revealed the underlying data statistics that α* depends on. This statistical dependency also motivates
us to explore other effect of α, such as the optimization behavior in a geometric context. By studying
the role of α on optimization, we confirmed theoretically that MAML obtains solutions with shorter
average distance to individual task optima than ERM - an empirical observation that was suggested
to contributes to MAML’s fast adaptability. We believe these results are instructive in contributing to
the theoretical understanding of meta-learning and its algorithm design.
9
Published as a conference paper at ICLR 2022
7	Acknowledgement
This research/project is supported by the National Research Foundation, Singapore under its AI
Singapore Programme (AISG Award No: AISG-GC-2019-001-2A). Any opinions, findings and
conclusions or recommendations expressed in this material are those of the author(s) and do not
reflect the views of National Research Foundation, Singapore. Q. Li is supported by the National
Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-2021-0005).
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory. In
ICML, 2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In International Conference on Machine Learning, pp. 424—433. PMLR, 2019.
Alberto Bernacchia. Meta-learning with negative learning rates. In International Conference on
Learning Representations, 2021.
Andrea Braides. A handbook of gamma-convergence. In Handbook of Differential Equations:
stationary partial differential equations, volume 3, pp. 101-213. Elsevier, 2006.
Zachary Charles and Jakub Konecny. On the outsized importance of learning rates in local update
methods. arXiv preprint arXiv:2007.00878, 2020.
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Why does maml outperform erm? an
optimization perspective. arXiv preprint arXiv:2010.14672, 2020.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regularization. In International Conference on Machine Learning,
pp. 1566-1575. PMLR, 2019.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based
model-agnostic meta-learning algorithms. In International Conference on Artificial Intelligence
and Statistics, pp. 1082-1092. PMLR, 2020.
Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradi-
ent descent can approximate any learning algorithm. In International Conference on Learning
Representations, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems, pp. 9516-9527, 2018.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
International Conference on Machine Learning, pp. 1920-1930. PMLR, 2019.
Katelyn Gao and Ozan Sener. Modeling and optimization trade-off in meta-learning. Advances in
Neural Information Processing Systems, 33, 2020.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online
bayesian regression. arXiv preprint arXiv:1807.08912, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.
Khurram Javed and Martha White. Meta-learning representations for continual learning. arXiv
preprint arXiv:1905.12588, 2019.
10
Published as a conference paper at ICLR 2022
Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-specific adaptation over partial parameters. In Advances in Neural Information Processing
Systems, 2020a.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020b.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, pp. 5915-5926, 2019.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of
simple visual concepts. In Proceedings of the annual meeting of the cognitive science society,
volume 33, 2011.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot
learning. arXiv preprint arXiv:1707.09835, 2017.
Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and
Shenghua Gao. Towards fast adaptation of neural architectures with meta learning. In International
Conference on Learning Representations, 2019.
Albert W Marshall, Ingram Olkin, and Barry C Arnold. Inequalities: theory of majorization and its
applications, volume 143. Springer, 1979.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature
reuse? towards understanding the effectiveness of maml. In International Conference on Learning
Representations, 2020.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in Neural Information Processing Systems, pp. 113-124, 2019.
Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18:1-9, 2013.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960, 2018.
Rajesh Sharma, Madhu Gupta, and Girish Kapoor. Some better bounds on the variance with
applications. J. Math. Inequal, 4(3):355-363, 2010.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, 2017.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching
networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.
Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a
learning-to-learn approach. In International Conference on Machine Learning, pp. 10981-10990.
PMLR, 2021.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. In International Conference on Learning Representations, 2020.
Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search.
In International Conference on Learning Representations, 2019.
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via
minibatch proximal update. In Advances in Neural Information Processing Systems, 2019.
Pan Zhou, Yingtian Zou, Xiao-Tong Yuan, Jiashi Feng, Caiming Xiong, and Steven C. H. Hoi. Task
similarity aware meta learning: Theory-inspired improvement on maml. In 4th Workshop on
Meta-Learning at NeurIPS, 2020.
11
Published as a conference paper at ICLR 2022
A Definitions, notations and lemmas
Notation We denote an optimal adaptation learning rate as α*. Global minima of empirical risk of
MAML and ERM (when they are unique) are denoted by wm, wr. We write {1, ..., N} as [N] and
use k ∙ k to denote the Euclidean norm. We use subscripts to index the matrices/vectors corresponding
to task instances, and bracketed subscripts to index the entries of matrices. For function f depends on
a, b, x, we omit other variables by f (..., x) when we discuss with x.
Table 1: High-frequency notation table.
Symbols	Definition	Symbols	Definition
α	Adaptation learning rate	K,k1,k2	All/train/val/ sample size per task
一 * ɑ	Optimal adaptation learning rate	Lm, Lr	Population risk of MAML/ERM
c*	c* αlim, αest	Estimation (limit) of α*	^	^ Lm, Lr	Empirical risk of MAML/ERM
λI, λS	Min/max of eigenvalues	Lm	Average population risk
ai	Task optimum of task i	N	Number of training tasks
d	Feature dimension	Wm, Wr	Global minimum of MAML/ERM
Definition 2 (Gamma Convergence). Let Fn : X → R for each n ∈ N. We say that (Fn)n∈N Γ
-converges to F : X → R, and write Γ - limn→∞ Fn = F or Fn →Γ F, if
•	For every x ∈ X and every (xn)n∈N such that xn → x in X
F (x) ≤ lim inf Fn (xn)
n→∞
•	for every x ∈ X, there exists some (xn)n∈N such that xn → x in X and
F (x) ≥ lim sup Fn (xn)
n→∞
Definition 3 (Lower Semicontinuous Envelope). Given F : X → R, the lower Semicontinuous
envelope (or relaxation) of F is the ”greatest lsc function bounded above by F ”:
Flsc(x) := sup{G(x) | G : X → R is lsc and G ≤ F on X}
inf lim inf F (xn) | (xn)n∈N ⊆ X and xn → x
Lemma 1 (Remark 2.2, (Braides, 2006)). If Fn uniform converge to F, then Fn →Γ Flsc where Flsc
is Lower Semicontinuous Envelope of F.
Lemma 2 (Γ-Convergence, (Braides, 2006)). Let X be a topological space. Let {Fn} be a equi-
coercive family of functions and let Fn Γ -converges to F in X, then
• limn→∞ dn = d where dn = inf x∈X and d = inf x∈X F(x). That is, the minima converges
Fn(x)
• The minimizers of Fn converge to a minimizer of F.
Proposition 2. If both A and B are positive semidefinite, the inequality is true:
tr(AB) ≤ tr(A) tr(B).	(A.1)
and if A is n-by-n symmetric PSD, we have
tr(A2) ≥ tr(A)2 .	(A.2)
n
12
Published as a conference paper at ICLR 2022
Proof. Let a = tr(A). For PSD matrix A, we have A	aI. Then
tr(AB) = tr(B1/2AB1/2) ≤ tr(B1/2(aI)B1/2) = tr(A) tr(B).	(A.3)
For second inequality, we can apply spectral decomposition on A as A = QDQ-1. So we have
tr(A) = tr(QDQ-1) = tr(DQQ-1) = tr(D) =	λi.
i
where {λi}, i ∈ [1, n] is the eigenvalues of matrix A. Then by Cauchy-Schwarz inequality we can get
tr(A)2 = Xλi! ≤ nXλi2! = ntr(A2)
口
Lemma 3 (Hanson-Wright inequality, (Rudelson & Vershynin, 2013)). LetX = (X1, . . . , Xn) ∈ Rn
be a random vector with independent components Xi which satisfy EXi = 0 and kXi kψ ≤ L. Let
A be an n × n matrix. Then, for every t ≥ 0,
P {∣X>AX - EX>AX∣ >t} ≤ 2exp
t2	t )
L4MkHs ,L2MkJ
-c min
where kξkψ2 = supp≥1 p-1/2 (E|X |p)1/p is sub-gaussian norm, kAk = maxx6=0 kAxk2/kxk2 is
operator norm and kAkHS = (Pi,j |ai,j |2)1/2 is Hilbert-Schmidt (or Frobenius) norm.
Definition 4 (Fast Adaptation Error). Given the task distribution D(T), meta-initialization w0, the
optimal solution oftask T is WT, then t-step fast adaptation error is defined by
Ft(w0,D(T)):=	E	kwT - WT∣∣2
T ~D(T)
(A.4)
where WT = w0 一 η Pj Vwj 'i(wj) is the adapted parameter oftask T with t steps.
Lemma 4 (Ruhe’s trace inequality, (Marshall et al., 1979)). IfA, B are n × n positive semidefinite
Hermitian matrices with eigenvalues,
a1 ≥ …≥ an ≥ 0,	b1 ≥ …≥ bn ≥ 0
respectively, then
nn
aibn-i+1 ≤ tr(AB) ≤	aibi
i=1	i=1
Proposition 3. For any positive random variable X ~ D(x), we have following inequality holds true
(Eχ~D(x)(x2))2 — Eχ~D(χ)(x)Eχ~D(χ)(x3) ≤ 0	(A.5)
Proof. With the fact that
(Ex~D(X)(X )) 一Ex~D(x) (X)Ex~D(x) (X )
X2p(X)
xp(x)
(A.6)
where x > 0 and p(x) > 0.
1	. o	1
Let f = (Xp(X))2 > 0 and g = (x3p(x))2 > 0, with CaUchy-SchWarz Inequality,
we have that
Ex~D(x)
dx
=Ex~D(x) (x)Ex~D(x) (x3)
fg dx
(A.7)
(A.8)
□
13
Published as a conference paper at ICLR 2022
B Proof of Theorem 1
Proof Sketch We list our proof steps as follows
1.	Get global minima of ERM and MAML by first order optimality condition.
2.	Let average MAML population risk Cm as the targeLin order to eliminate the randomness
(Proposition 1 guarantees the upper bound between Cm and population risk Cm).
3.	Approximate this target function Cm by another function LmX which is the limit of Cm as
number of tasks N → ∞.
4.	According to positive definiteness, we can get the range of α.
5.	With notion of gamma convergence, the minimizer of Cm will also converge to the minimizer
of LampX.
6.	Our estimation of a* is in the range of α in Step 4.
Notation for this proof: For simplicity, we omit the arguments of the function if its symbol has a
index e.g. Φi = Φ(Xi), Ci = Ci(α). Then we give the full proof on estimation of α*.
Proof. Recall that the global minimum closed form for ERM and MAML are
N	-1 N
Wr (N,K) = Wr ({φ(Xi), ai}i∈[N ])=
Wm (α, N, K) = Wm {Ci (α), ai}i∈[N] where Ci = Φi —竿ΦiΦ>Φi, Ci ∈ Rκ×d. Since Wm depends on random variables a1, ..., in (3.2), where	i=1	i=1 -1	(B.1) )=(X C>c)	(X C>gaj aN. The average population risk of MAML, defined
Cm(α, N, K) = EwmCm(Wm, α, K) of global minimum Wm in (B.1) can be written as CmS N, K) = Eaι,…,aN〜D(a)Cm(Wm, α, K) 1 =	EEE K {ai}N ι 〜D(a) a〜D(a) X〜D(X)	=Eaι,...,aN 〜D(a)Cm (Wm, α, K )	(B.2) kC(α)(Wm - a)k2
i=1 =ɪ	E	E	C (α) I K a,{ai}N=ι〜D(a) χ~D(X) Let Λj = PN Ci>Ci-1Cj>Cj,j∈ [1,N],Λj ∈Rd× Cm(α,N,K)=!	E	E	C(α) I K a,{ai}N=I〜D(a)X~D(x)	\ =O	E	E	(X K a,{ai}NL~D(a)x~D(x) [ ∖i⅛ —a>C(α)>C(α) (^XΛj∙a)— + a>C(α)>C(α)a 14	N N	- -1 ( N	\	1 2 XCi>Ci	XCj>Cjaj	—a i=1	j=1 (B.3) d. The (B.3) can be rewritten as, N N	∖ 2 X Λj aj — a j=1 Q C(α)>C(α) (XAjaJ j=1	(B.4) XΛiai	C(α)>C(α)a
Published as a conference paper at ICLR 2022
Under Assumption 1 and a is independent to X, then We have
Cm(α, N, K)=—
K
Λiaij	C(α)τC(α) I ɪ2 Ajaj
(B.5)
K XM)
Let Lmχ(α) be an approximation function of Lm(α, N, K).
2
Lm)Xm),对X 〜乳产[Cm)TC(a)]
σ2
a E tr
K X〜D(X)
I - KαΦ(X)τΦ(χ)) τ Φ(X)τΦ(X) (I- KαΦ(X)τ
(B.6)
E
E
X"(x)	{©}&]〜D(α)
T
N
Then the approximation error Will be
344",K)-Laa)I
E
X 〜D(X)
N
X tr(AτC(α)τC(α)Aj)
∖j=i
N
(B.7)
E tr Cjτ Cj
X〜D(X) M	j
Ci	C (α)τC (α)
Where
CTCi = Φ[Φi- 4α (Φ[Φi)2 + 4022 + (Φ[Φi)3	(B.8)
K	K2
With Assumption 2, there exists constants 0 < c1 < c2 where
C1 ≤∣∣Φ(Xi)kF ≤ C2,∀i ∈ [N]	(B.9)
With Proposition 2 hold, ∀i ∈ [N] we have
tr(CjCi)= tr (φτ$i - K(φτ小了 + K2(φ[Φi)3)
= tr(Φ⅛) - tr (Ka(φJφi)) +tr (K⅛(φJφi)3)
≤ sup 画kF - 4α inf tr[(ΦτΦi)2] + 4α2- sup 画|根	(BIO)
i∈[N]	K i∈[N]	K2 i∈[N]
=c2- K i∈nN]tr[(φτφi)2]+⅛c3
4α 2	4α2 3
≤c2 - B + 产 c2
15
Published as a conference paper at ICLR 2022
Then by applying multiple times of Proposition 2 we have
XJD(X)j=1tr C>j ∑C>Ci
N
i=1
N
XJED(X)	tr Cj> CjCj> Cj	Ci> Ci
N
≤ XM) H tr[(C>Cj )2] tr
N
≤ E	X tr (C>Cj)2 tr
X〜D(X) j=1	' j '
i=1
N
XCi>Ci
i=1
N	-1
X c>c)
4α 2	4α2 3 3
≤N" - Kdc1 + κ2c2) XJD(X)tr
-1
C(α)>C(α)
N
X Ci> Ci
i=1
C(α)>C(α)
(B.11)
N
N

Next, we need upper bound the last inverse term. With Assumption 2 we know that all eigenvalues
of Φ(X)>Φ(X),X 〜D(X) are bounded by [λι,λs]. Let Call(α) = PN=ι C>Ci, then with
probability 1 the max/min eigenvalues of Call(α) will have following constraints,
λ λmin(Call(α)) ≥ N(λι - 4αλS/K + 4α2λ∣/K2)
λ λmaχ(Call(α)) ≤ N(λs - 4αλ2/K + 4α2λS/K2)
(B.12)
Since the Call (α) is a positive matrix, we need have the constrain on λmin (Call (α)) > 0, which
means
α∈
K(λS - PλS-λ4)	∪	K(λS + pλ∣-λ4)
2λ∣	竭
(B.13)
There exists a positive definite matrix λs (α, N)I where
Call (α)	λmin(Call )I
and the following inequality is easy to get
tr(Call (α)) ≥ tr(λmin(Call)I)
tr(Ca-ll1(α)) ≤tr(λ-m1in(Call)I)
(B.14)
(B.15)
So the last inverse term will be
2
≤
XJD(X)
E tr
i=1
-1
1
—4αλS/K + 4a213/K 2)d
(B.16)
2
Apply these inequalities to (B.7), we can get the upper bound
σ2	N (ci — Kadc2 + Kc3)3	i I
∣Lm(α, n, k) - Lmx(α)∖ ≤ K ∙ N2(λι - 4αλS/K + 4a"3/K2)2d2 = O(N
(B.17)
When α ∈ 0, K(JS-XλSzλ4) ) ∪	+XλS-"), ∞), thelimitwillgotozero,
lim
N →∞
sup
,Kes-√λs-λ4! ∪ K Kes+√λs-λ4
,	2λi	) I	2λi	,1
∣Lm(α,N, K) - Lm)X(α)∣=0
(B.18)
16
Published as a conference paper at ICLR 2022
which means Cm (α, N, K) will uniformly converge to LmX (α) for a belongs to the interval above.
Note that Cm (α, N, K) is a continuous function of a. So with Lemma 1, we have
Γ — lim Lm (α,N,K) = Lm)X (α)
N→∞
(B.19)
So We have estimation of true a*, denoted as a"rm
αlim =argmin LapXm)
α
arg min E tr
α X 〜D(X)
φ(x )>φ(x ) - 4α (φ(x )>φ(x ))2 + 402&(x )>φ(x ))3
K tr[Eχ [(Φ(X )>Φ(X ))2]]
2tr[Eχ [(Φ(X )>Φ(X ))3]]
(B.20)
According to Lemma 2 where a*(N, K) is the minimizer of Lm (α, N, K)
α* (N, K) = arg min Lm(α, N, K),	lim α* (N, K) = a^.	(B.21)
α	N→∞
C Proof of Proposition 1
Proposition 4 (Formal state of Proposition 1). Assume u = C(α)>C(α)a is sub-gaussian ran-
dom variable with sub-gaussian norm ∣∣U(i)∣∣Ψ2 = sup)>ip-1/2(E|u(i)|p)1/p ≤ L. Then with
probability at least 1 - δ that
Lm(Wm, α, K) — Lm (α, N, K)
L2
≤ - max
dε(α, K)
N2
log 2,
δ
R log 2)	(C.1)
□
if α ∈ 0, K(λS-√rλ)) ∪ (κλS+√≡2, ∞ and
ε(α, K) = (λs - 4αλI∕K + 4α2λS/K2)∕(λI - 4αλS/K + 4α2λ∣/K2)2
when assumption 1 & 2 holds. d is the feature size, N is the number of tasks and K is the sample
size per task.
Here, we follow the same proof notation as Theorem 1.
Proof. By definition, we have
Lmlα, N, K) = EaI,…,aN〜D(a)Lm(Wm (a, N, K), α, K)
Similar to (B.3), let Λj = PiN=1 Ci>Ci	Cj>Cj, we have,
LmWm ., K) = Ka〜D(a) XJD(X) k S^m	N K)-叫 2
2
=~	E E	llc(α) I〉' Ajaj- a ∣
K a~Dm) XJD(X) ∣∣	Ij=I	)
=V7	E E X Aiai ) C(α)>C(α)	(X Ajaj)
K	aJD(a) XJD(X) i=1	M
(N	∖>
-a>C(α)>C(α)	Ajaj	Aiai	C (α)> C (α)a
+a> C (α)> C (α)a
(C.2)
(C.3)
17
Published as a conference paper at ICLR 2022
With Assumption 1, second term and third term of (C.3) will be cancelled. So the Lm is
Lm = ⅛ E	XXΛiai ) C(α)>C(α)
K X~D(x)匕
N
X	Λjaj
j=1
As the comparison, Lm(α, N, K) is the averaged function of Lm,
Lm =^T7	E
K X〜D(X)
2
+ ʒa	E	tr[C(α)>C(α)]
K X〜D(X)
(C.4)
which is given by
E	XN Λiai	C(α)>C(α)
{ai}N=ι〜D(a) IM )
N
X	Λjaj
j=1
(C.5)
+ σ tr (C(α)>C(α))
Let A be the common matrix of cross-term of ai and aj, then for each term in (C.5),
N
-1
Λ> C (α)>C (α)Λj = C> Ci £ C>Ck	C(α)>C(α)
k=1
Ci	'-----
-1
>>
Ck0 Ck0	Cj Cj
(C.6)
}
{z
A
〜 .〜
=CeiACej
So cancel their second terms, the difference of Lm(wm,, α) and Lm,(a, N, K) will be
Lm(Wm, α, K) - Lm(α, N, K)
1
K XjD(X)
NN
XX
ai> CeiACej aj
i=1 j=1
2
+KXl"3ɑG≥)2
—
1
K
{ai}N=ι〜D(a) X〜D(X)
NN
XX
ai> CeiACej aj
i=1 j=1
(C.7)
E
E
1
K XJD(X)
NN	NN
X X a>ciACjaj- {3匕⑷ X X a>ciACjaj
1
K XJD(X)
NN	NN
X X ui>Auj - E X X ui>Auj
J 弋	{ai}N「D(a) J 弋
i=1 j=1	i i=1	i=1 j=1
where ui = Ceiai = Ci>Ciai is sub-gaussian random variable and with Assumption 1,
Eui = Eai = 0
(C.8)
Let U[N] = (u1; ...; uN), ∈ RNd, we write the quadratic form into a bilinear form for each product
term ui>Auj
A
U[N]AU[N] = (u>,…,UN).
A
A
.
.
.
A
u1
.J = XXu>AUj
where
A = 1n 1N 乳 AjAk ∈ RNdxNd
(C.9)
(C.10)
•	A T A7^ll 1	, -	1	∙ Tr	1	1	. A 1 < 1	1	C∙ A 1 ^Λ
is a N X N block matrix and 0 is Kronecker product. And the relations of A and A are
kAek = NkAk, kAekHS = N2	A(2i,j)
i,j
(C.11)
18
Published as a conference paper at ICLR 2022
By applying Hanson-Wright inequality we have
Pr(ILm(Wm,α,K) -Lm(α, N, K)1 > t) =Pr
(K UN ]au[n ]- EUN ]yeU[
≤2 exp
t2
[N] II > t
(C.12)
—c min ---------,-----
tL4kA∣∣Hs L2kA∖
t
Further with Cauchy Inequality, we can get the following equation
N
-1
-1
N
kAk
k=1
Ck>Ck	C(α)>C(α)	Ck>Ck
k=1
N	-1
X Ck>Ck
k=1
(C.13)
According to Theorem 1, all eigenvalues of second term falls in [λI - 4αλ2S /K + 4α2 λI3 /K2 , λS -
4αλ2∕K + 4a2λS/K2] and of order 1/N for first term.
k Ak ≤	(λs - 4αλ2∕K + 4a2λSIK2)
k k - N2(λι - 4αλS/K + 4α2λ3/K2)2
Let ε(α, K) = (λs - 4αλ2∕K + 4α2λS/K2)∕(λ/ - 4αλS/K + 4α2λ3∕K2)2
|司=NkAk≤ ε(αNK)
(C.14)
(C.15)
Next, we can bound kAkHs by ||Ak，It,s obvious that kAkHS ≤ drank(A)kAk. So IlAkHS Can be
upper bounded by
kAk2HS ≤ rank(A)kAk2 ≤ rank(A)
2
dε2 (α, K)
N4
(C.16)
rank(A)ε2 (α, K)
N4
Thus, the kAek2HS will no more than
kAkHs ≤ ε2(α,K) N
(C.17)
In summary, we can get the bound
Pr(ILm(wm,α, K) -Lm(α, N, K)∣ > t) ≤ 2exp
t2N2
tN
-Cmin (L4dε2(α,K), L2ε(α, K))
(C.18)
Finally, we rewrite the inequality by eliminating t, we have at least 1 - δ,
∣c / T 八 κ / nr tt、i L2	f I dε(α, K) 1	2 ε(α,K) 1	21
ILm(wm,α,K) -Cm(α,N,K)i ≤ K max J N	N2	log $，-N^ 1°g δ [
(C.19)
≤
≤
≤
□
D Proof of Corollary 1
Proof. Recall our estimation of a* is given by,
*	. aaxx, ʌ	Ktr[Eχ[(Φ(X)>Φ(X))2]]
c*	a	Tapx(CA
αlim = argminLm (a) = 2tr[EX[(φ(χ)>Φ(X))3]]
(D.1)
19
Published as a conference paper at ICLR 2022
For each task, we have K samples with d dimensional features Φ(X) ∈ RK ×d. Since Φ(X)>Φ(X)
is positive definite matrix, by applying spectral decomposition, we have
trEX[(Φ(X)>Φ(X))2] =EX tr[(Φ(X)>Φ(X))2]
=EX tr[(UΣXU>)(UΣXU>)]	(D.2)
= tr EX [Σ2X]
where U is an orthogonal matrix and ΣX is the a diagonal matrix filled by eigenvalues λ1, ..., λd of
the covariance matrix of the feature. It’s easy to prove in Principle Component Analysis (PCA) that
tr E(ΣX) is the variance of features where
σ2(φ(xι),..., φ(xκ)) =1 tr Eχ [(Φ(X) - μ)>(Φ(X) - μ)]
K
1κ	1d
=κ E(O(Xi) -μ) = κ E λi	(D.3)
i=1	i=1
=1tr E(∑x)
K
where φ(xi) ∈ Rd is each row of Φ(X) and μ is zero.
With Jensen’s inequality, we have
d[trE(∑x)]p ≤ trE(∑X) ≤ [trE(∑x)]p, (p ≥ 1)	(D.4)
Thus, we can write the inequalities
K[trEX(∑x)]2 VK[trEχ[(Φ(X)>Φ(X))2]] < Kd[trEχ(Σχ)]2
2d[trEχ(Σχ)]3 ≤ 2tr[Eχ[(Φ(X)>Φ(X))3]] ≤ 2[trEχ(Σχ)]3]
K	K [tr EX [(Φ(X )>Φ(X ))2]]	Kd
2dtrEχ(Σχ) ≤ 2tr[Eχ[(Φ(X)>Φ(X))3]] ≤ 2trEX(Σχ)
thereby
1	≤ *	≤	d
2dσ2(φ(xι),...,φ(xκ)) — Qlim — σ2(φ(xι),...,φ(xκ))
(D.5)
(D.6)
□
E Examples
Example 1 (Normal, Polynomial feature) Assume we have K i.i.dsamples xi,…,χκ 〜N(0, σ2)
and a is a random vector from zero-mean distribution. Consider polynomial basis function φ : R →
Rd, where φ(y) = (1, ..., yd-1).
		(一 φ(XI) 一 ∖	1	x1	. . .	xd-1
	Φ(X) =	I — φ(x k)—)	..	. ..	. ..	. 1 xκ . . .	. . . d-1 xκ
Since we have				
		*	_ Ktr[Eχ[(Φ(X)>Φ(X))2]] αlim = 2tr[Eχ [(Φ(X )>Φ(X ))3]]		
So that
dκ	2	κ	2
tr[EX[(Φ(X)>Φ(X))2]] =ExX X xi(j-1)+0	+...+ Xxi(j-1)+d
j=1 i=1	i=1
dd κ
=ExXX	Xxij+m-2
j=1 m=1	i=1
(E.1)
(E.2)
(E.3)
dd	dd
=KXX
E[x2(j+m-2)] + (K - 1)KXX
E2[x(j+m-2)]
j=1 m=1	j=1 m=1
20
Published as a conference paper at ICLR 2022
Similarly, the denominator is
tr[EX[(Φ(X)>Φ(X))3]]
dd d	K	K	K
=ExXX	X Xxij+m-2	X xij0+l-2	Xxtj+l-2
j=1 l=1	m=1	i=1	i0=1	t=1
dd
=ExXX
x2j+l-3 +	+ xj+d-2xj+l-2
x0	+ . . . + xi	xi0
、----------------{z----------------}
dK2
K
X xtj+l-2
dd
=ExXX
2j+m+l-4
xi
'--------
^Z
dK
+	+ xj +d-2 xj +l-2
+ . . . + xi	xi0
-------}×--------{----
dK(K-1)
K
Xxtj+l-2
t=1
(E.4)
ddd
EEE (KE[x3j+m+2l-6] + 3K(K - 1)E2[x2j+2l-4]E[xj+m-2]
j=1 l=1 m=1
+(K3 - 3K2 + 2K)E3 [x(j+m+l-3)]
If K =1, σ → 0 the optimal a^ will be
*	=	Pd=I PPn= E[x2j+m-叫
lim =2 Pj= Pd=I Pm=I E[χ3j+m+2l-6]
=P2=-2[C(i +1,1) - 2C(i - d + 1,1)]E[x2i]
—	2 P3= 1 g2(d,j)E[xj]
=P2=-2 gι(d,i)σ2i(2i- 1)!! = O (ɪ)
-2 Pj=-3 g2(d,j)σ2j ⑵-1)!「	V27
(E.5)
where C(n, k ) =	kn
is the binomial coefficient and g1(d, i) = C(i + 1, 1) - 2C(i - d + 1, 1).
If K → ∞, σ → 0
*
αlim
(K - 1)K2 P?= Pm=IE2[χ(j+m-2)]
2 Pj=I PP=1 Pm=I(K3 - 3K2 + 2K)E3[χ(j+m+l-3)]
Pd=ι Pm=IE2[χ(j+m-2)]
Pd=ι Pd=ι Pm=IE3[x(j+m+l-3)] L
P2=-2 gι (d,i)E2[xi]
p3=-3g3(d,j)E3[xi l J
Pd=IIgι(d,i)[σ2i(2i- 1)!!]2	O (ɪ)
Pd=d∕2e-1 g3(d,j)[σ2j ⑵-1)!!]3( 1力
(E.6)
We show the coefficients of each moment in the Figure 6. As we can see, denominator becomes
dominant since the coefficient of every moment, number of terms and order of moment are all larger
(higher) than numerator.
So in this case, the αl*im has an inverse relationship with σ2 .
Example 2 (Random Matrices) Assume all elements Yij in feature matrix are independent. Then
let Y be a random matrix we have
)
21
Published as a conference paper at ICLR 2022
Figure 6: Example of d = 50 with coefficients of each moment given by g1 (d, i), g2(d, i), g3(d, i).
(P= Yil	... P= KιKd∖
Φ(X )>Φ(X) =	.	.	. I
Pi=1 YidYi1 . . .	Pi=1 Yi2d
For the numerator in (D.1),
dd K	K
tr[Ex[(Φ(X)>Φ(X))2]] =EYij,i∈[K],j∈[d] XX XYitYis	XYi0tYi0s
=dKE[Y4] + (dK(K - 1) +d(d- 1)K)E2[Y2]
(E.7)
(E.8)
+d(d- 1)K(K - 1)E4[Y]
Here, the row m column n entry of (Φ(X)>Φ(X))2 is
d
(Φ(X)>Φ(X))(2mn) =X
s=1
K
X
j=1
(E.9)
The diagonal entry of (Φ(X)>Φ(X))3 at row m will given by
d
(Φ(X)>Φ(X))(3mm) = X
n=1
Similarly, the denominator is
tr[Ex[(Φ(X)>Φ(X))3]]
dd
=EYij,i∈[K],j∈[d]
m=1 n=1
d
X
s=1
d
X
s=1
K
X
j=1
(E.10)
=KdE[Y6] +Kd(d- 1)(E[Y 5]E[Y] + E[Y 4]E[Y 2] +E2[Y3])
、-------------------------{z--------------------------}
i=i0=j
+b1 +K(K- 1)d(d - 1)(E[Y3]E3 [Y] + E[Y3]E[Y2]E[Y] + E2 [Y2]E2 [Y])
'----------------------------------V----------------------------------}
i=i06=j
+b1 +K(K- 1)d(d - 1)(E[Y3]E3 [Y] + E[Y3]E[Y2]E[Y] + E2 [Y2]E2 [Y])
、----------------------------------V----------------------------------}
i6=i0=j
+b1 +K(K- 1)d(d - 1)(E[Y4]E2 [Y] +E3[Y2] +E2[Y2]E2[Y])
'-----------------------------V------------------------------}
i=j6=i0
(E.11)
+b2 + (K3 -3K2 + 2K)d(d - 1)(E6[Y] + E2 [Y2]E2 [Y] +E[Y2]E4[Y])
、-----------------------------------V-----------------------------------}
i6=i06=j
22
Published as a conference paper at ICLR 2022
where b1 = K(K - 1)dE[Y4]E[Y2],b2 = (K3 - 3K2 + 2K)dE3 [Y 2].
If K = 1, the optimal α 嬴 will be
*	= dE[Y 4] = E[Y 4]
αlim = 2dE[Y6 ] = 2E[Y 6]
(E.12)
If K → ∞ and d → ∞,
*	K(dK(K - 1) + d(d - 1)K)E2[Y2] + d(d - 1)K2(K - 1)E4[Y]
αlim =(K3 - 3K2 + 2K)[dE3[Y2] + d(d - 1)(E6[Y] + E2[Y2]E2[Y] + E[Y2]E4[Y])]
=___________dE2[Y2] + d(d- 1)E4[Y ]__O(I)
=dE3[Y2] + d(d - 1)(E6[Y] + E2[Y2]E2[Y] + E[Y2]E4[Y]) ( )
= ______E4[Y]_O⑴≈“。⑴
E6 [Y ] + E2 [Y2 ]E2[Y ]+ E[Y 2]E4[Y ]	()	E[Y 6]()
(E.13)
Both two examples are related to the high order moments of data distributions. In polynomial feature
example, we focus on the gaussian distributed data and its inverse relationship to data variance. As
for any random matrix, it depends on the fourth moment over sixth moment.
F Proposition for Theorem 2
wm(α) =wr+α XN Φ(Xi)>Φ(Xi)
Proposition 5. ∃ε, α ∈ [-ε, ε] global minimum of MAML wm(α) is given by following equation
N4
X K(Φ(Xj)>Φ(Xj))2 (Ws- aj)
j	(F.1)
+ Za Vawm(ξ) (α -ξ)2dξ
0	2!
where wr is ERM global minimum.
Proof. As for the MAML, the global minimum is,
wm(α) =(X c> Ci)	(X Cj Cja)	(F.2)
With Ci(α) = Φi- KΦiΦ>Φi, Ci(α) ∈ RK×d.
Let Wα = PiN=1 Ci>(α)Ci and να denote PjN=1 Cj>Cjaj, then
N	2α	>	2α
VaWa =Va ^X (φi - K φiφi φi) (φi - K φiφi φi
N
=X-评(φ>Φi )2 + t12 (Φ>Φi)3
K	K2
i=1
Similarly, we have
(F.3)
4	8α
VaVa = X 一五 (φi φi) ai + 记 (φi φi) ai
K	K2
i=1
(F.4)
23
Published as a conference paper at ICLR 2022
For first-order derivative, we have the following form,
VaWm(O) =W-IIa=0 [V a νa |a=0 ― Va^^aIa=0 Wm(0)]
i V	,	i V	,
X -K4(Φ>Φj)2aj + X K4(Φ>Φl)2Wm(0)
j=1	l=1
N 4
X K (φ> φj 产(Wm(O)- aj )
j=1
(F.5)
Recall that
N	-1 N
Wm(O)=Ws =(X Φ>Φi)	[X Φ>Φjaj
So for small α, we have the Taylor expansion at zero that
(F.6)
Wm(α) =Wm(O) + VaWm(O) +
0
a V2aWm0 (ξ)
Wr + α XN Φ(xi)>Φ(xi)
+ [ — (α - ξ)2dξ
2!
-1
(α - ξ)2dξ
N4
X KK(Φ(xj )> Φ(xj ))2 (Ws- aj)
j
(F.7)
□
G Proof of Theorem 2
Proof Sketch We list our proof steps as follows
1.	Based on Definition 1, our target is to illustrate that fast adaptation distance gap between
Wm and Wr is always negative which means MAML has smaller distance to all the tasks at
any adaptation steps in expectation.
2.	We first get the linearized expression of Wm by Proposition 5.
3.	Compute fast adaptation distance gap ∆ = Eτι,…,tn〜D(T)Ft(Wm) 一 Ft(Wr) across same
task distribution D(T) and take expectations with respect to all random variables.
4.	With lemma of trace inequalities and assumptions, we can get the upper bound for the
dominant term of ∆, refer to (G.30).
5.	Bound the reminder terms, we can get the range of α.
Notation for this proof Follow the Theorem 1, we omit the arguments of the function if its symbol
has a index e.g. ΦT = Φ(xT ). Covariance matrix ΦT>ΦT = GT and inverse of sum covariance
matrix V = (Pi∈[N] Φi>Φi)-1 for short.
Proof. For each task T sampled from distribution D(T), gradient descent iteration yields
WTt+1 = W
W
T — nV `t (WT)
T — Φ Φt (ΦtrWT 一 ΦT,aτ )
K
I -%
I -%
WT + —ɪɪ Φt Φt aτ
K
t+1	t+1
W0 + X K (I-今
j=1
j-1
ΦT> ΦT aT
(G.1)
24
Published as a conference paper at ICLR 2022
where w0 is the initialization. Let GT = ΦT>ΦT , the adapted error will be
—
kwTt
GTaT - aT
GT aT - aT
(G.2)
then with Definition 1, we can get t-step fast adaptation error for MAML,
Ft(Wm0 )=	E	QtT Wm0 +ST2
T 〜D(T)
(G.3)
and the ERM fast adaptation error is
Ft(Wr0) =	E	QtT Wr0 + ST 2
T 〜D(T)
(G.4)
Note that the sum of geometric series in ST is
ST = X Kn (l - KnGT)	GTaT - aT = 11
t
- I aT = -QT aT
(G.5)
Now, let’s focus on the error gap of MAML and ERM, denoted as ∆ , then we have
A = Ti,..,tE〜D(T)Ft(Wm) -Ft(w0)
Ti,...,TnET〜D(T) ∖QT (Wm + 叫 + 2ST, QT (Wm - 叫
(G.6)
For small α, we get its linear expansion in Proposition 5
Wm (α) = W0 + αVɑWm (0)+ [
0
2!
(α - ξ)2dξ
Wr0+α(XN Gj
-1
1 V ,
X KG2 (w0 - a)	+ RI
(G.7)
α Ww (ξ)
N
j
2
where Ri = Ra ^αWm(ξ) (α 一 ξ)2dξ is the reminder term. So it would be
A= E E
Wm ,w0 T〜D(T)
(2w0 + aVaW0,(0) + RI) + 2ST, αQTVaWm
(G.8)
Let V = PjN Gj	and the first derivative VαWm0 (0) is split into
αVaWm(0) = V (X KKG2W0)- V (X KGjaj
j
j
thus inner product will be four product terms and a reminder term which is
A =K {aE[N] T〜E(T) [ *QTW0, QTV (X G2W) + - *QTW0, QTV (X G2a
+ *ST, QTV (X G2W0 j + - *ST, QTV (X Gjaj j+} + R2 + R1
(G.9)
(G.10)
25
Published as a conference paper at ICLR 2022
where the remainder terms are
R2 = α2( VaWm (0), VaWm (0) ), R = 02( Ri, VaWm
(G.11)
Now, let’s look at the expectation. Recall that task T is defined by random variables (Φ(XT), aT).
With simultaneously diagonalizable assumption, all feature covariance matrix in tasks can be factor-
ized to
Φ(Xt )>Φ(Xt ) = GT = U Σt U *
(G.12)
where U is the basis of features, ΣT is the only random variable of GT . So the data of each task
depends on eigenvalue diagonal matrix ΣT . So taking expectation over T means the two independent
expectation E∑τ〜d(∑), EaT〜D(a). Similarly, w0 in previous section is expressed by
⇒ ET1 ,...,TN
Wr0
X Gjaj)
}[N] (Wr0) = E{ai}[N]E{Σi}[N] (Wr0)
(G.13)
For each product term in (G.10), we list four main terms as following. First term is
“TNET 〜D(T) (QT W0,QT V X G2 W
E tr
{Gi}[N]
i=i
E
{ai}[N]~D(a) {Gi}[N]
N
GiV QtT QtT V	Gj2	V Gi
N
,KaQT V X GG V
j
))
(G.14)
Similarly, we can get the second term
{Ti}[N]⅞ 〜D(T) (QT W0,QT V X Gj= {aiE"E∕QT W0,QT V X Gj
E tr
{Gi}[N]
N
XGiVQtTQtTVGi2
i=i
For third and fourth terms, EaT〜D(a) is a marginal expectation of ET〜D(T), thus
E	ST, QtTVXN Gj2Wr0
{Ti}[N],T〜D(T) ∖	j	/
Similarly, the fourth term is
(G.15)
E E	-QtT aT, QtTV X Gj2Wr0
aτ~D(α) wr,{Gi}[N] ∖	j	/
=0
a[N]〜D(a) T〜D(T)
E

N
E
j
E
(G.16)
ST,QtTVXN Gj2aj	=0
(G.17)
26
Published as a conference paper at ICLR 2022
So overall, the we care about above four terms as a function of α, N, t, ... denoted as δt(α, N)
δt(α, N) =∆-R2 -R01
8α
E7	E
K {Gi}[N]
tr
NN
XGiVQTQTV(XGj] VGi
——E tr〉、GiVQTQTVG2
K {Gi}[N]	i=1
8α
E7	E tr
K {Gi}[N]
8α
NrE tr
K {Gi}[N]
8α
NrE tr
K {Gi}[N]
N	NN
XGiVQtTQtTV	XGj2	VGi -XGiVQtTQtTVGi2
i=1	j	i=1
XN GiVQtTQtTV ((XN Gj2 ] VGi - Gi2
XN VQtTQtTV ((XN Gj2 ] VGi2-Gi3
(G.18)
By simultaneously diagonalizable assumption, we have
8α
δt(α, N) = E tr
K {Σi}[N]
8α
RE tr
K {Σi}[N]
. N	( N
XVQtTQtTV (X(UΣj2U>)(UΣbNU>)(UΣi2U>) - (UΣi3U>)
i=1	j
NN	N
XX
UΣi2ΣbNΣj2U> - XUΣi3U>
ij	i
(G.19)
where ΣN = (Pk∈N] Σk)	is a PD matrix and QT = (I - (2η∕KK)GT)t is a exponential decay
term w.r.t η. With probability 1, λsI	Σi	λxI
(Nλx)-1I	ΣbN	(Nλs)-1I
(G.20)
Note that VQtTQtTV = (QtT V)>QtT V is a symmetric positive definite matrix where
VQtTQtTV =UΣbNU>Q2TtUΣbNU>
NU>
UΣbNU>
(G.21)
Note that m-th diagonal entry of E{Σi}[N] PiN PjN UΣi2ΣbNΣj2U> - PiN UΣi3U> is
→>	(Eλ2)2	EλEλ3∖ T _ (Eλ2)2 - EλEλ3	2 (C-S)0
e(m) (^ΝEλ	NET~ e(m) = NEλ	ke(m)k	≤
where (C-S) is according to Cauchy-Schwarz inequality for integrals in Proposition 3.
(G.22)
So the above matrix is a negative definite matrix. Now, let us can derive following trace inequality
for NSD and PSD.
Proposition 6. If A is a n-by-n negative definite matrix and B is a n-by-n PSD matrix, we have
tr(AB) ≤ λmin(B)tr(A)
(G.23)
27
Published as a conference paper at ICLR 2022
Proof. By Ruhe’s trace inequality (Lemma 4), we have tr(AB) ≥ i λi(A)λn-i+1 (B). Eigen-
values of A are negative where λi (A) < 0, ∀i ∈ [n]. So we have tr(AB) ≤ λmin (B) Pi λi (A) =
λmin(B)tr(A)	□
So with Proposition 6, we have
δt(α, N) ≤ ~T7 E	λmin (VQTQTV) tr
K {Σi}[N]
NN	N
XX
UΣi2ΣbNΣj2U> - X UΣi3U>
ij	i
(G.24)
with (G.20) and (G.21)
λmin (VQTQTV) = λmin	ΣN
(G.25)
For the second part, we have
N N	N	N N	N	-1	N
XX
UΣi2ΣbNΣj2U>- X UΣi3U>	= tr IXX∑2 X∑k ∑2-X∑3
ij	i	ij k	i
N	-1 N N	N
=tr ∣ X∑k	XX∑2∑2 -X∑3
k	ij	i
N	-1	N N	N N
≤λmin](X Σk)	tr XX ∑2∑2- XX ∑3∑k
N	-1	N N	N N
=占 J(X ∑k)	∖ tr [XX j - XX ∑3∑k
Overall, with probability 1, we have
(G.26)
2t	N N	N N
δt(ɑ,N) ≤N3κλχ (1-κf^} {∑E[N]tr XXZW-XX"k	(G.27)
Specifically,
NN	NN
E tr IXXΣj2Σi2 -XXΣi3Σk
{Σi}[N]	i j	i k
tr NE(Σ4) - NE(Σ4) + (N2 - N)E2(Σ2) - (N2 - N)E(Σ3)E(Σ)	(G28)
7}	}	7}	}
i=j	i=k	i6=j	i6=k
d
= (N2 -N)X[E(Σ2)](2i) - [E(Σ3)](i)[E(Σ)](i)
i=1
With Proposition 3 we know that any eigenvalue λ = Σ(i) > 0 obeys the statistical condition
E2[λ2] - E[λ3]E[λ]. Thus there exists a constant ce > 0 such that
NN	NN
E tr XXΣj2Σi2-XXΣi3Σk	< -ced(N2 - N)
{Σi}[N] i j	i k
Finally, we have
δt(α, N) ≤ -
2η	2t 8αd2ce 1	1
-Kλx	KλX N - N2)
(G.29)
(G.30)
28
Published as a conference paper at ICLR 2022
Now, let’s bound the remainder terms R10 , R2 where
R1 =	E E	/QTRι,αQTV	X -4Gj (w0 - aj)	(G.31)
a[N]〜D(a) T 〜D(T) ∖	Vj K j
R1 is the remainder term in Taylor expansion (G.7). With Integral Mean Value Theorem
R1 = ]： Vawm⑹(α-ξ)2dξ = CaVaWm(ξ0)	(G.32)
We have the locally Lipschitz property for C∞ function wm(α), in a small region with small α such
that	2
Ri ≈ α2- Vawm(0)	(G.33)
Then we have
N
X Gi3ai - Gi3ws0 +K Gi2 Vawm0 (α)
i=1
NN
X X GGGjai- G3Gjws + 4GjGj (w0 — aj) I
i=1 j=1
NN
X X(4GjGjwO - G3Gj ws) + (GGGjai- 4GjGjaj)
i=1 j=1
8
K2
NN
X X(4Gij Gjj - Gi3Gj)(wrs - aj)
i=1 j=1
(G.34)
Recall(F.6) the w0 = (PN Φ>Φi)-i(PN Φ>Φi0ai>) while Ea^〜D(a) w0 — ai = 0. The R in
above equation is
R01
4α3
K T〜E(T)tr
NN
XX(4GijGjj -Gi3Gj)VjQjTt VGjj
i=1 j=1
4α3
K T〜E(T)tr
NN
XX U (4Σjj Σij Σjj - Σjj Σi3Σj )U> UΣbjNQjTt ΣbNU>
4α3
≤ K T〜E(T)tr
NN
XX(4ΣijΣj4-Σi3Σj3)
i=1 j=1
(G.35)
Y ( 4α3d
≤ [n3k3
jt
sup E tr
i,j ∈[N] Σi,Σj
NN
XX(4ΣijΣj4 -Σi3Σj3)
i=1 j=1
And the eigenvalues of Σi are bounded in [λs, λx] where
NN
E tr XX(4ΣijΣj4 - Σi3Σj3) = tr 3N EΣ6 + (Nj - N)E(4ΣjΣ4 - Σ3Σ3)
Σi,Σj	i=1 j=1
≤ d 3Nλ6x + (Nj -N)(4λ6x -λs6)
≤dNj(4λ6x-λ6s)
(G.36)
29
Published as a conference paper at ICLR 2022
In summary,
R01 ≤
NK3
4α3d2 (4λ6x - λs6)
2t
(G.37)
Similar to R01 let’s bound the R2 in (G.10),
(

R2 =α2 E
a[N]〜D(a) T〜D(T)
16α2
K2r T〜E(T)tr
N
XGi2VQ2TtVGi2
i=1
≤ E tr
T 〜D(T)
N
XΣi4 tr hΣb2NQ2Tti
i=1
(G.38)
E
j
j
16α2d2λχ (也 λ yt
λ2NK2 V - K s)
Thus the final constraint of α will be
2t
2t α2
4α3d2 (4λ6x - λs6)	16α2d2λ4x
+r
+ λ2NK2
s
NK3
(4λX - λ6) + 4αλX
K2
λs2K
:2	1
-rλ! (1
8αd2ce 1	1
Kλl" (N - N2/ ≤ 0
1
N2	≤ 0
(G.39)
α2(4λx- λf) + 4aλx
K2
λs2K
-rλe (1-N)≤ 0
where ratio factor ^ = [(1 - 2η∕Kλχ) / (1 - 2η∕Kλs)]2t. We have the extreme points of ɑ
α(N)
4λX ± . 1( 4λx A2 +4
-有 ± V +4
3χR rλe (1-寺)
K2
(4λX-λ6)
K2
(G.40)
2
x
For K ∈ Z+ ≥ 1, small α, t = 0 and large t we have
r = [(1 - 2η∕Kλχ) / (1 - 2η∕Kλs)]2t = O(1)
(G.41)
So finally, the α needs to satisfy
α(N ) ≤ α(2)
-2λXK + K√4λχ + 1.5eλ4(4λχ - λf)∕λχ
λs2(4λ6x-λs6)
(G.42)
□
H Experiments
H.1 Practical form of Theorem 1
For practical use, we show a numerical form to estimate a* for the case where number of tasks is
finite and training data u ∈ Rk1 is different from validation data t ∈ Rk2. The corresponding feature
matrices are Φ(u) and Φ(t). Let’s derive corollary of Theorem 1 for realistic meta-learning setting.
Corollary 2. If training feature Φ(u) ∈ Rk1 ×d is different from validation feature Φ(t) ∈ Rk2×d
for every task, then
* (k、_	kιEχtr[Φ(u)>Φ(u)(Φ(t)>Φ(t)]
αlim( 1) = 2Eχtr[Φ(u)>Φ(u)(Φ(t)>Φ(t)Φ(u)>Φ(u)].
(H.1)
Proof. Similar to proof of Theorem 1, we have
Wm ({xi, ai}i∈[N ],N,k1,k2,α) = (X Cbi(α)>Cbi(α))	(X Ci(α)>Ci(α)a)	(H.2)
30
Published as a conference paper at ICLR 2022
where Cbi(α) = Φ(ti)(I - 2αΦ(ui)>Φ(ui)) ∈ Rk2×d.
The corresponding average population risk will be
Lm(N,kl, k2,α) = Eaι,…,aN〜D(a)Lm(Wm, α,七,k2)
= E	E	l∣Cb(α) (wm({xi, ai}i∈[N],N, k1,k2,α
a,{ai}N=ι~D(a) x~D(X)"
(H.3)
Then we can define similar approximation function Lampx (α) as (B.6) where
Lmr (α) , Ex〜D(X) tr [C(α)>C(α)i	(H.4)
And with same bound for kΦ(ui)k, kΦ(ti)k ∈ [c1, c2], we have
Γ - lim Lm(kι,k2,N,α) = Lmx(α)	(H.5)
N→∞
With Gamma convergence lemma, We can get the final estimation ɑ1mʃ in (H.1).
□
In experiments, we use the following numerical form
α* =	kι Pi=ι tr[Φ(ui)>Φ(ui)(Φ(ti)>Φ(ti)]
alim	2 Pj=I tr[φ(% )>φ(u)(中出厂中出)φ(u )>φ(%)]
to evaluate our estimation.
H.2 Overparameterized setting
Let’s consider overparameterized setting. Thus, we have feature for each task (K
[ψ(x1), ..., ψ (xK)]> ∈ RK×d. Correspondingly, empirical objective of ERM is given by
(H.6)
d), Ψ
<
1N
Lr(W) = NK 工 kψiw - yik.
i=1
Assume meta-initialization is the mean of all task optima that a = mean(aι,…，aN). Then concate-
nate all task features we have
Ψall
Ψ1(1)
ΨN(K)
Ψall ∈ RNK ×d
(H.7)
So MAML objective is given by
12
Lm (w,α,N,K) = NK ∣∣ψallw - yall k
=NK IICall (α)w — al,
(H.8)
where w0 = [w — 2a∕(NK )Ψall (Ψ>n W — Ψ>na) ] is adapted parameters and Call(a) = Ψall(I 一
(2ɑ∕NK)Ψ>uΨall). The minimum norm solution is
Wm (…,N, K, α) = Call(α), (Call(α)Call(α)> ' Call(α)a
(H.9)
Note that, in overparameterized setting, Theorem 1 will not perfectly give the precise form for α*.
But the technique can be easily extend to large d setting where
Lm(α,N, K) =Ea,a,xkC(α)(Wm — a)∣1 2
=Ea,a,x [wmCm)TQa)Wm + aτC(α)τC(α)a]	(HIO)
=Ex tr[Call(α)>CaglrlamCall(α)C (α)>C (α)Call(α)>CaglrlamCall(α)]	.
+ Ex tr[C(α)τC(α)]
31
Published as a conference paper at ICLR 2022
where Caglrlam = (Call (α)Call (α)>)-1.
By Proposition 2, the Cm(α, N, K) will be upper bounded where
Cm(α, N, K) =Ex tr[CgrιamCaiι(α)C(α)>C(α)Caiι(α)>] + Ex tr[C(α)>C(α)]
=Ex tr[Call(α)>CaglrlamCall(α)C(α)>C(α)] + Ex tr[C(α)>C(α)]
≤Ex tr[Call(α)>CaglrlamCall(α)]Ex tr[C(α)>C(α)] + Ex tr[C(α)>C(α)]
(H.11)
=2Ex tr(C(α)>C(α))
Hence, minimizing the upper bound also tells US how to select α*. In another word, we are seeking
an estimation a^st nearly minimize the upper bound.
α*st = arg min Ex tr(C(α)>C(α))
α
(H.12)
and the C(α)>C(α) is a covariance matrix where
Cm)TCm) = ψ>ψ - NK "]ψ)2 + NK (ψ>ψ)3	(H.⑶
We can derive that
αest
NK Ex tr(ΨτΨ)2
2Ex tr(ΨτΨ)3
(H.14)
Since d is large, its computational cost is high. Here we assume second moment of all elements of
ΨτΨ are σ2, which means σ is the variance of all elements of features. Finally we have
C *	1
αest =	二 2NKσ2
Let’s take the Neural Tangent Kernel (NTK) (Jacot et al., 2018) for example,
f(w, x) = f (winit, x) + Vf(Winit, x)τ(W - Winit).
Then we have neural tangent feature
ΨiτW = Vf(Winit,Xi)τ(W - Winit)
⇒ Ψi ≈ f(Winit,Xi)
Next, we stack all the features to Ψall to compute the variance e.g. recall σ = Ψall.std(). After that,
we can compute the estimation a& using (H.15).
(H.15)
(H.16)
(H.17)
H.3 Experimental setup
Estimation of α*, underparameterized model We set hyperparameters dimension d = 20, number
of training/validation samples per task K = 50, number of tasks N = 5000. Each x is i.i.d sampled
from a distribution U (-5, 5) while each a is i.i.d sampled from high dimension Gaussian distribution
N(0, 3I). Then computing the Ordinary Least Square (OLS) solution with different α, we can show
the training loss landscape in terms of a. The true ɑ*(N) = argmina minw C(α, 5000, 50, W) is
the minimizer of the training loss. Our estimation α^mj described in Theorem 1 is evaluated by
comparing the error to true α*.
Estimation of a*, overparameterized model We perform the nonlinear regression on two different
models. The first is quadratic regression using neural tangent feature (see H.2). All hyperparameters
are set to be same as (Bernacchia, 2021) where
W 〜N (wo, V-Ip)	b 〜N(0, σ2) X 〜N (0, Ip) y | x, w,b 〜N ((XTW + b)2, σ2
But to guarantee the overparameterization, we set hidden size with 10, 000, which means the total
dimension will be 30, 001. Then we perform quadratic function regression with ranging α value
to see the test loss. After that, we can evaluate the how accurate our estimation using (H.15) is by
32
Published as a conference paper at ICLR 2022
comparing to optimum of test loss. Second experiment is sine function regression using 3-layer MLPs
activated with ReLU. The data and labels are generated from a stochastic function
y = asin(x + b),a,b ~ U(0,π),x 〜U(—5, 5).
To get a good representation, we pre-train the model with ERM loss and then freeze the first two
layers as the feature extractor. Then we use the output from second layer as the random feature to
train last layer on 1,000 training tasks. At the same time, a^ can be computed from the features of
1, 000 training tasks. Then last layer trained with differnet α will be evaluated on 10, 000 test tasks.
Fast adaptation distance We run experiments with random matrices. For each task, the data
are i.i.d drawn for the prescribed distributions which represent three common types, Uniform
U (—5, 5), Gaussian N(0, 2) and Exponential Exp(1). Specifically, each entry in random matrix
X ∈ RK×d, (K = 50, d = 20) is sampled variable from a distribution Xj ~ D(χ). The feature
map is an identity map Φ(X) = X. First we sample 5000 training tasks to compute the closed-form
meta-initializations for ERM and MAML with a small α (10-4). Then we perform t-step adaptation
on 5000 test tasks and compare the fitting losses and the Average Distance under Fast Adaptation
Ft(wm), Ft(wr). The learning rate η in fast adaptation evaluation is 10-5.
Estimation of a*, deep learning In our experiments, We valid our estimation of a* on sine
regression and few-shot classification.
•	For deep regression, We folloW the (Finn et al., 2017) to perform sine regression With
3-layer MLP Whose hidden size is 40. Then each task is an instance in stochastic function
y = asin(x + b),a,b ~ U(0, π) while the training set is 10 i.i.d sampled data pair from the
corresponding sine function and test set is consists of another sampled 256 points. During
test, we sample 10, 000 tasks to evaluate the learned model.
•	For deep classification, we follow the Omniglot experiments in (Raghu et al., 2020). Here,
we adopt the online estimation scheme to compute the a* for the adaptation learning rate of
last layer. To this end, we apply α* = 1/(2 X Nway X Nshot X σ2) where σ2 is mean of
covariance of the normalized feature F>F, F = (F1/kF1 k, ..., FNway/kFNwayk). Then
abuffer in the buffer is online updated using α,Uffer J 0.9α^uffer + 0.1α*.
H.4 Additional Result
H.4. 1 Optimization behavior
We add more illustrative experiments on visualizing the trajectory of global minima of MAML. We
consider normally distributed task optima with centralized data and uncentralized data. As shown in
7, the MAML minimum still try to balance the distances to different task optima. But the situation
is more complex in uncentralized data (second row). However, α always minimize the geometric
distance at beginning where the shape of mean distance function appears to be convex. This has
confirmed our Theorem 2 where small α always lead to a shorter mean distance to different task
optima than ERM algorithm.
H.4.2 ESTIMATION OF a* ON BASIS FUNCTION FEATURE
Firstly, we used the random matrix for task i as the feature matrix, Φi ∈ RK ×d. All elements of Φi
are i.i.d sampled from U (—5, 5). Secondly, we created the random features using Gaussian basis
function. Gaussian basis function Φ(X )(j = exp(一 (X(j)— μ,)2 /2σf) is a function whose value
depends only on the distance between the input and some fixed point. Thirdly, we used polynomial
feature Φ(X)(i,：)= (co,…，Cnxd-I) which is based on Taylor series. With N tasks, we compute
the one-step adaptation loss. Optimal learning rate minimizing the loss is denoted by a*(N). The
error gap between true optimum and estimation ∣α* (N) — α^ | with three random features are shown
in Figure 8 (a), (b) & (c) respectively. To reduce random errors, there was an average of 10 sampling
trials, shown as the solid lines. The shadow area represents standard deviation. As the number of
tasks N increasing, the estimation error will shrink down to zero and its uncertainty reduces as well.
So our estimation is reliable and accurate when number of tasks becomes large.
33
Published as a conference paper at ICLR 2022
□«ls) *且
0.00000.00250.00500.00750.01000.01250.01500.0175
a
Figure 7: Additional results for optimization behavior with normally distributed task optima. Left
column: Visualization of trajectory of MAML solution. Orange dots are task optima {a}[N] of
sampled tasks, where location of ai is decided by its entries. Red dot highlighted in circle is new
coming task. Green cross is wr , (α = 0) while the purple trajectory is generated as α increasing.
Red star is Wm(a；im,...). Right column: Average euclidean distance of Wm(α,...) and {a}[N] and
corresponding points in left figure. First row: centralized data X 〜N(0, 2), a 〜N(0, 3I). Second
row: uncentralized data X 〜U(0, 5), a ZN(0,3I). Best viewed in colors.
0.000	0.005	0.010	0.015	0.020
a
0.025
0.020
0.015
0.010
0.005
0.000
0.25
0.20
0.15
0.10
0.05
0.00
0.7
0.5
0.4
0.3
0.2
0.1
0.0
21	£3	£5	27	29 2^∙1 2^3	21	23	25	27	29 2^l1 21'	t21	23	25	27	29 2“ 2^3
Number of tasks	Number of tasks	Number of tasks
(a)	(b)	(c)
Figure 8: Estimation error ∣α*(N) — a；im | along task number N increasing (K is fixed). The blue
line in the shadow is mean of the error. The shadow area is the standard deviation of the errors. (a)
Random matrices (b) Gaussian basis function (c) Polynomial basis function.
We use Gaussian basis function as the random feature and conduct the experiments on different
types of distribution to evaluate our estimation quality. Then, we use uniform/normal distribution
with zero mean U, N as the stereotype of central symmetric distribution. In experiments, we set
d = 10, K = 15, N = 3000 and the parameters in Gaussian function depends on the range of data.
As shown in the Figure 9, the estimation a；im is close to true optimum, α* in four different cases:
(a) data is sampled from a central uniform distribution U(—5, 5), task optima are sampled from a
central normal distribution N(0, 32I); (b) data is sampled from a non-central normal distribution
N(0, 22), task optima are sampled from a central normal distributionN(0, 32I); (c) data is sampled
from a central uniform distribution U (—5, 5), task optima are sampled from a non-central normal
distribution N(5, I); (d) data is sampled from a non-central Chi-Sqaure distribution χ2 (7), task
optima are sampled from a imbalanced Zipf distribution P(X = k) = z(S)k-s where Z(S) is the
34
Published as a conference paper at ICLR 2022
α	0.001	0.005	0.01	0.05	0.08	0.1	0.15	0.2	0.3	0.4	0.5
Pre MSE	829	822	822	827	^^829^	^^827"	^^825^	821	^^826^^	^^825""	^^826^
Post MSE	829	820	817	807	IoO-	797T~	^^805^	820	^^86Γ^	^^955""	^^989^
Table 2: Test loss of one-step sinusoid regression with neural network feature. First row is the discrete
test values of α, second row is the Mean Square Error(MSE) loss before adaptation and third row is
the loss after adaptation. All loss values are digits after the decimal point
Riemann Zeta function. Note that results in (c) and (d) are beyond our Assumption 1. So our theorem
can be extended to more general scenarios.
Figure 9: Evaluate estimation a；im on different types of distributions. (a) Central data distribution and
central task optima distribution. (b) Non-central data distribution and central task optima distribution.
(c) Central data distribution and Non-central task optima distribution. (d) Non-symmetric non-central
distributions for data and task optima.
H.4.3 Estimation of a* ON Neural Network feature
We used neural network based feature to verify our theorem in underparameterized (original model
size in Finn et al. (2017)) and overparameterized setting (NK < d). in former setting, we used
3-layer Multilayer Perceptron (MLP) activated with ReLu for sine functions family regression where
each task is to regress an instance in stochastic function y = a sin(x + b),a,b 〜U(0, π). We used
ERM to train and freeze the first 2 layers (as feature extractor) and then only fine-tune last layer
with MAML. Compute αl*im through features of sampled training tasks we got αl*im = 0.10319. As
shown in Table. 2, the optimal α of lowest MSE after adapting is 0.1 which is nearest discrete value
in table to αl*im .
H.4.4 Heuristic estimation range for deep Learning
To make it practical for deep learning, we give the heuristic estimation range where α* it might
be based on our theorem. Previously, we show (3.3) for underparameterized model (K > d) and
(H.15) for overparameterized model (NK	d). Besides, the trace term for covariance matrix
in underparameterized setting can be simplied by Kdσ2 where σ2 is the covariance of the feature
(second moment). So here, the heuristic estimation by merging these two settings, where it derived as
1
*
αlim
2 min(NK, d)σ2
(H.18)
35
Published as a conference paper at ICLR 2022
where N, K are number of training task and its training sample size, d is model size. Next, we show
a simple way to estimate the range of σ2. In general, each element of feature with probability 1
will fall into the [0,1]. Then, given n observations, we have (Popoviciu's inequality on covariance
(Sharma et al., 2010))
-1 ≤ σ ≤ 1
2n σ 4 4
(H.19)
Proof. Assume with probability 1, each element x of Φ(X) has x ∈ [m, M].
Define a function f in terms of random variable x by
f(t) =E(x-t)2
Computing the derivative f0, and solving the minimum
f0 (t) = -2E[x] + 2t = 0 ⇒ f (E[x]) = min f(t)
t∈R
(H.20)
(H.21)
So we have the covariance has following upper bound
2	M+m
σ2 = f(E[x]) ≤ f I —2—
(H.22)
where
f (M+m )= E [(χ-M+m 了
4E [((x - m) + (x - M))2] = (M - m)2
(H.23)
Thus form = 0, M= 1, we have
方2<(1- 0)2	1
σ ≤	=—
一 4	4
for an independent sample of n observations from a bounded probability distribution,
Szokefalvi Nagy inequality shows that
σ2 ≥ (M - m/」
2n	2n
(H.24)
the von
(H.25)
□
Here, we conduct following deep regression experiments to evaluate our heuristic estimation. We plot
the estimated range of a* given by our bounds - the red area between the two star-lines in Figure
10. Then we perform quadratic regression with 2-layer neural network follow the hyperparameter
in (Bernacchia, 2021). From Figure 10(a), we can see, the optimal α for this task is positive and
our estimated range includes suboptimal points. Follow the setting of (Finn et al., 2017) (All
hyperparameters are same), we use 3-layer NN with hidden size 40 to test sine regression tasks. As
shown in the Figure 10(b), our estimated range includes the optimal α and other good α.
H.5 Relation to negative learning rate
As we mentioned before, (Bernacchia, 2021) show negative learning rate minimizing the test loss of
MAML. In this section, we compare their results with ours. Specifically, we follow the setting of
underparameterized experiment (Bernacchia, 2021) where the defined hyperparameters are set to be
same, nt = 5, nv = 25, nr = 10, m = 40, p = 30, σ = 0.2, ν = 0.2. Parameters are sampled from
following distributions
W 〜N ,0, V-Ip)	X 〜N (0,Ip)	y | x, W 〜N (XTw,σ2)
We conduct experiments on numerical fitting loss on meta-learning instead of the closed-forms Ltest
in theorems (Bernacchia, 2021)2. As we can see from the Figure 11, (Bernacchia, 2021) only give the
result on special case where αr = 0.2 is fixed. However, this strategy highly depends on the selection
of αr that may not achieve the minimum of meta-learning loss.
2Test losses are computed on standard meta-learning regression
36
Published as a conference paper at ICLR 2022
0 5 0 5 0 5 0
.52 O 7 5 2 O
4.4.4.3 33.3.
SSoIs 但
(a) Quadratic Regression 2-layer NN
(b) Sine Regression 3-layer NN
Figure 10: Heuristic estimation of α* range of deep learning. (a) Quadratic regression on 2-layer
neural network. (b) Sine regression on 3-layer neural network.
Figure 11: Comparison of our estimation and (Bernacchia, 2021) on underparameterized mixed linear
regression. X -axis is the discrete values of αt and Y -axis is the test loss of MAML. First row, test
loss with respect to αt while the left one shows same α for meta-training and meta-testing and right
one is fixed αt = 0.2 strategy. Second row, comparison of test loss of different strategies and the
suggested range of minimizers given by their paper (pink and green diamonds) and our estimation
(red diamond). Best viewed in color.
In addition, we run deep learning experiments to demonstrate that optimal learning rate α is positive.
All hyperparameters and generating process are set to be same as the non-linear regression experiments
in (Bernacchia, 2021). Furthermore, we train the 2-layer neural network to regress quadratic functions
with 5 adaptation steps and evaluate models on same 10 folds with each fold consists of 1000 test
tasks. The results (with error bar) are shown in the Figure 12. As we can see, the optimal learning
rate for both strategies are positive.
37
Published as a conference paper at ICLR 2022
Figure 12: Test losses with reference to adaptation learning rate in meta-training of deep quadratic
regression in (Bernacchia, 2021). X-axis is the discrete values of αt and Y -axis is the test loss of
MAML. Left: test loss with fixed αt = 0.01 and varying αr. Right: test loss with same αt and αr
38