Published as a conference paper at ICLR 2022
Variational Oracle Guiding for Reinforce-
ment Learning
Dongqi Han** 1, Tadashi Kozuno2, Xufang Luo3, Zhaoyun Chen4,
Kenji Doya1, Yuqing Yang3, and Dongsheng Li3
1	Okinawa Institute of Science and Technology
2	University of Alberta
3Microsoft Research Asia
4Institute of Artificial Intelligence, Hefei Comprehensive National Science Center
Ab stract
How to make intelligent decisions is a central problem in machine learning and
artificial intelligence. Despite recent successes of deep reinforcement learning
(RL) in various decision making problems, an important but under-explored as-
pect is how to leverage oracle observation (the information that is invisible during
online decision making, but is available during offline training) to facilitate learn-
ing. For example, human experts will look at the replay after a Poker game, in
which they can check the opponents’ hands to improve their estimation of the
opponents’ hands from the visible information during playing. In this work, we
study such problems based on Bayesian theory and derive an objective to leverage
oracle observation in RL using variational methods. Our key contribution is to pro-
pose a general learning framework referred to as variational latent oracle guiding
(VLOG) for DRL. VLOG is featured with preferable properties such as its robust
and promising performance and its versatility to incorporate with any value-based
DRL algorithm. We empirically demonstrate the effectiveness of VLOG in online
and offline RL domains with tasks ranging from video games to a challenging tile-
based game Mahjong. Furthermore, we publish the Mahjong environment and an
offline RL dataset as a benchmark to facilitate future research on oracle guiding1.
1	Introduction
Deep reinforcement learning (DRL) has undergone rapid development in recent years (Sutton &
Barto, 2018; Mnih et al., 2015; Vinyals et al., 2019). However, there is a common and important
but under-explored aspect in RL: imagine that after playing a Poker game, a human player may
look at the replay to check opponents’ hands and analyze this information to improve his/her play-
ing strategy (or policy) for the next time. We refer to the information like opponents’ hands as
oracle observation, defined as the information invisible to the agent during online task execution
but available during offline training. By contrast, the information available during task execution
is called executor observation. Such a scenario has been referred to as oracle guiding for RL (Li
et al., 2020; Fang et al., 2021) (see Sec. 3 for a formal definition). Oracle guiding is common in
real life. For example, when taking an examination (the oracle observation is the answers to similar
questions, which are available only during preparing); and training a robot to perform some tasks on
the Moon (when training the robot, we can provide it with the information of the territory, which are
not available during execution). The type of oracle observation can be diverse, including hindsight
information (Harutyunyan et al., 2019; Guez et al., 2020), human feedback (Knox & Stone, 2009;
Loftin et al., 2016; MacGlashan et al., 2017), re-calibrated data with post-processing, and hidden
states in a partially observed setting (Li et al., 2020).
While humans naturally perform oracle guiding when learning to make decisions, it remains chal-
lenging in RL. The difficulties include: (1) how to guarantee that learning with oracle observation
*Work done during an internship in Microsoft Research Asia. Email: dongqi.han@oist.jp
1https://github.com/Agony5757/mahjong
1
Published as a conference paper at ICLR 2022
improves the main decision model using executor observation only, and (2) if introducing an aux-
iliary loss leveraging oracle observation, how to tradeoff between the main loss and auxiliary loss.
While recent studies attempted to model oracle guiding in RL (Guez et al., 2020; Li et al., 2020;
Fang et al., 2021), none of them addressed these difficulties (refer to the Related Work section for
more details). In particular, all these proposed methods are heuristic: although empirical results
showed performance gain with oracle guiding, it is not theoretically guaranteed that the usage of
oracle observation improves execution performance.
In this paper, we propose a fundamentally new idea for oracle guiding based on Bayesian theory.
Taking Poker as an example, we know that the learning of optimal strategy is tractable if knowing
the global, true state of the environment (or simply state2), including all visible or invisible cards,
the opponents’ playing style, etc. (Azar et al., 2017; Jin et al., 2018; 2020). A key part of skill
improvement is learning to estimate the probabilistic distribution of environmental state from ex-
ecutor observation. The common way to do this by human experts is to watch match replays where
the oracle observation (e.g. opponents’ hands) is available, and then use the oracle-estimated state
to correct the executor-estimated state. We interpret this to Bayesian language: executor-estimated
state as the prior distribution, and the oracle-estimated one as the posterior distribution. Thus, the
training objective can be considered two-fold: learning to make decisions based on posterior esti-
mation of state, and learning a prior distribution of state closer to the posterior one.
We formulate this idea by proposing a novel learning framework for general oracle guiding prob-
lems based on variational Bayes (VB) (Kingma & Welling, 2014), referred to as variational latent
oracle guiding (VLOG). VLOG owns several preferable properties. First, VLOG is theoretically
guaranteed to leverage oracle observation for improving the decision model using executor obser-
vation. Second, VLOG is a versatile DRL framework that can be integrated into any value-based
RL algorithm and is agnostic to the type of oracle observation. Third, VLOG does not suffer from
the necessity of tuning additional hyper-parameters. Finally, we empirically show that VLOG con-
tributes to better performance in a variety of decision-making tasks in both online and offline RL
domains. The tasks include a simple maze navigation, video games playing, and a particularly chal-
lenging tile-based game Mahjong in which humans heavily leverage oracle observation in learning
(Li et al., 2020). We also contribute to the community by taking Mahjong as a benchmarking task
for oracle guiding and publishing the RL environment and dataset to facilitate future research.
2	Related work
In the past few years, research interests have grown on DRL and imitating learning (Chen et al.,
2020) with leveraging oracle or hindsight information. For DRL, Guez et al. (2020); Fang et al.
(2021) considered hindsight observation (executor observation at future steps) as the oracle obser-
vation during training. Guez et al. (2020) used hindsight observation to facilitate learning a rep-
resentation of current state. Another method (Fang et al., 2021) was used for stock trading: the
authors trained a teacher (oracle) policy with hindsight information, and employed network distilla-
tion to make the student policy behaves more similar to the teacher policy. Both the methods (Guez
et al., 2020; Fang et al., 2021) are heuristic and focused on making use of future observation for a
better sequential modeling, while VLOG is theoretically guaranteed for any kind of oracle observa-
tion. For applications in imperfect-information games, Suphx (Li et al., 2020), a DRL-based AI for
Mahjong, also introduced a method to leverage oracle observation (opponents’ hand) for stronger
performance. They concatenate oracle observation with executor observation as the input of policy
network, where the oracle observation is timed by a scalar variable which is annealed from 1 to 0
during the training course. However, the method used in Li et al. (2020) is also heuristic and has
only been tested in one task.
Variational Bayes (VB) is a well-established method and has been taken advantage of in RL. For
example, control as probabilistic inference uses VB to connect the objective function of RL and
variational lower bound of a probabilistic inference problem (Furmston & Barber, 2010; Weber et al.,
2015; Levine, 2018). Our idea differs since it frames a value regression problem by the maximum-
likelihood problem, and then, it applies VB to solve it (see Sec. 4). Also, the usage of VBian network
models for DRL has captured attention from researchers recently. For example, Ha & Schmidhuber
(2018) proposed to employ a VAE to reduce the high dimensionality of image observation; Igl et al.
2Generally, oracle observation does not necessarily contain all the information of environmental state.
2
Published as a conference paper at ICLR 2022
(2018); Han et al. (2020); Lee et al. (2020) proposed variational RNNs as the state-transition models
to encode the belief states of the agent; Yin et al. (2021) utilized a variational sequential generative
model for predicting future observation and used the prediction error to infer intrinsic reward to
encourage exploration; and Okada et al. (2020) demonstrated performance gain by using a deep
Bayesian planning model in continuous control tasks. Our study differs from the mentioned works
by focusing on oracle guiding, and VLOG does not involve learning a state transition model.
3	Oracle guiding POMDP
Here we define the problem based on the Partially Observable Markov decision process (POMDP)
(Sondik, 1978). An oracle guiding POMDP distinguishes from the original POMDP by having two
types of observations: executor and oracle. The executor observation x is always available to the
agent, on which the agent's decision making (execution) relies. The oracle observation X is not
accessible during execution, but can be obtained afterward. X is included in X since the former is
always available. Thus X contains no less information of the underlying environment state than x.
A formal definition of oracle guiding POMDP is a tuple(S, A, Po, T, X, X, O, O, γi, where S
and A are the state and action spaces, respectively. P0 specifies the initial state distribution such
that P0 (s) is the probability of a state s ∈ S being an initial state. T specifies the state transition
probability such that T (s0, r|s, a) is the probability of reaching to a new state s0 ∈ S with an
immediate reward r ∈ R after taking an action a ∈ A at a state s ∈ S . X denotes the executor
observation space and X denotes the oracle observation space. O specifies the executor observation
probability such that O(X|s) is the probability of an executor observation X ∈ X at a state s ∈ S.
Similarly, O specifies the oracle observation probability such that O(X|s) is the probability of an
oracle observation X ∈ X ata state S ∈ S. Y ∈ [0,1) is the discount factor. Value functions are the
expected value of return from a particular state or state-action pair. For a policy π, its Q-function is
defined by qπ (s, a) := En [P∞=o γnrt+n∣st = s,at = a], where En indicates the expectation when
the policy ∏ is followed. The state value function Vn is defined by Vn (S) := E∏(a∣s)[qn (s, a)]. The
agent aims to maximize EP0(s)vn(s) with respect to π, and the value-functions play an important
role to this end (Sutton & Barto, 2018).
4	VLOG: varational latent oracle guiding
Let us introduce a latent vector zt, which is a probabilistic variable representing the environmental
state St. From a Bayesian perspective, we consider the prior distribution p(zt|Xt) as the agent’s
estimated probability density function (PDF) for zt based on executor observation Xt . Meanwhile,
the posterior distribution PDF q(zt∣Xt) is modeled based on oracle observation Xt.
In RL, the most basic requirement is to make a good estimation of return by a value function approx-
imator V(Xt) := V(zt)p(zt|Xt)dzt (we denote it by V, but it can also be a Q function by simply
replacing Xt with (Xt, at)) based on available information, i.e. executor observation Xt. The target
of return, denoted by Vttar, can be estimated by any value learning algorithm such as TD(0) (Sutton
& Barto, 2018), Peng’s Q(λ) (Kozuno et al., 2021), etc. (generally, Vtar can always be given by Bell-
man equations. However, one can also use Monte-Carlo return as Vtar if available). In particular, we
employed double Q-learning with dueling architecture (Wang et al., 2016) to compute Vtar due to
its effectiveness and simplicity (Sec. 5 and B.1). We want to maximize the log-likelihood objective
of the estimation of return based on executor observation Xt (i.e., for the executor model)
L := logP V(Xt) = Vttar|Xt = log
/ P (V(Zt) =
Vttar|zt p(zt|Xt)dzt
log/, q⅛⅛P (V(Zt)="zt) p(ztiXt)dzt.
3
Published as a conference paper at ICLR 2022
By Jensen’s inequality, we have
P(V (Z t) = vtar∣zt) p(zt |xt)
L ≥	q(zt|xt)log	~~卢	dzt
Jzt	q(Z t |xt)
= zt
q(zt |xt) logP(V(Zt) = Vtar ∣Zt) - q(zt |xt) log
q (z t |x t )^
p(zt|xt)
dzt
=Eq(zt |X t) logP V(zt) =Vttar|zt	-DKL (q (Zt IXt) kp (Zt |xt)) := LVLOG.
'-----------------{z-----------------} '-------------{z-------------}
oracle prediction error	regularization term
(1)
Thus we can maximize our initial objective L via maximizing LVLOG, which is also known as the vari-
ational lower bound (Kingma & Welling, 2014), but in our oracle-guiding scheme. Since p(zt|xt)
and q(zt |xt) represents the PDF of the latent vector obtained from executor observation and oracle
observation, respectively, the meanings of the two terms in LVLOG appear clear now: the first term,
i.e., oracle prediction error, helps to improve value estimation from posterior latent state distribution
(zt computed with oracle observation); and the second term, i.e., regularization term, helps to shape
the prior representation of zt closer to the posterior one as latent oracle guiding. We would like to
highlight that the VLOG objective is the lower bound of the objective for the prior executor model
V(xt) (the estimation of return using executor observation xt) with the usage of oracle observation
Xt. This lower bound guarantees that the usage of oracle observation facilitates the learning of the
executor model, which is our original motivation.
Remark 1. One may use any shape of the approximate posterior q, depending on which different
instance of VLOG is possible. Furthermore, one may directly use V(xt) instead ofp(zt|xt). These
design choices allow users to incorporate any prior knowledge on oracle observation. For example,
if one knows the range of a state-value at xt is a closed interval [l, u], the approximate posterior
q (vt |X t) can be restricted to a family of probability distributions supported on [l, u].
I Oracle prediction error
φφ KLD Regularization
Figure 1: Implementation of VLOG for deep Q-learning during (A) learning and (B) execution.
Executor observation xt and oracle observation Xt use two different encoders but share one decoder
(MLP for estimating Q function). (C) Baseline model without using oracle observation. A rectangle
means a deterministic layer/variable, and an ellipse means a stochastic one. Here the outputs are
multi-head Q-values corresponding to discrete actions.
4.1	Implementation with neural networks
Inspired by the implementation of variational auto-encoder (VAE, Kingma & Welling (2014)), we
propose the neural network architecture of VLOG (Fig. 1). The executor observation Xt and oracle
observation Xt are processed by two distinct encoder networks to compute the prior and posterior
distribution of the latent vector, respectively. During training, both xt and Xt are available, and all
the network parameters are updated by maximizing the VLOG objective in an end-to-end manner
(Fig. 1A). During execution, the agent computes prior distribution p(Z|Xt) for decision making
(Fig. 1B) without using oracle observation. Zt is computed by parameterized normal distribution
p(Zt∣Xt) = N(μp, exp (log σp)), (μp, log σp) = PriOrenCOder(Xt),
q(Zt∣Xt) = N(μq, exp (log σq)), (μ2, log σqt) = posterior encoder(Xt).
For computing P(V(Zt) = Vttar|Zt) in Eq. 1, we simply assume it follows normal distribution, and
estimate it with mean square error between V(Zt) and Vttar in practice. The reparameterization trick
is used to perform end-to-end training as in VAE (Kingma & Welling, 2014). Then the output of the
decoder (value function) can be obtained by V(Zt) = decoder(Zt). Note that Zt is obtained using
the posterior encoder during training, and using the prior encoder during execution (Fig. 1A, B).
4
Published as a conference paper at ICLR 2022
4.2	Task-agnostic latent bottleneck control
To learn a better representation, we borrow the idea from β-VAE (Higgins et al., 2016) to multiply
a coefficient β to the regularization term. Thus we have the loss function (negative lower bound) as
JVeLOG = -Eq(ZtIx t) [lθg P (v(Zt) = Vtar ∣Zt)] + β□κL S 3|X t)kP (ZtM)) ∙	⑵
The hyper-parameter β controls the capacity of the latent information bottleneck (Tishby & Za-
slavsky, 2015; Alemi et al., 2017). We found the choice of β is important for the performance of
VLOG in RL (see Appendix B.3). However, having extra hyper-parameters are not desired. Inspired
by the method used in Burgess et al. (2017) for controlling the scale of KL divergence in β-VAE,
we propose a task-agnostic method to automatically adjust β by setting a target of KL divergence
DKtaLr . In particular, we minimize the auxiliary loss function (β as the being optimized parameter)
Jβ = (log 10 DtKL - log10 Dkl (q (zt|Xt)kp (zt∣xt))) log(β).	⑶
The intuition here is to strengthen the regularization by increasing β when the divergence between
prior and posterior is too large, and vice versa. This method is similar to that used in soft actor-critic
for automatically adjusting the entropy coefficient (Haarnoja et al., 2019), while we used this for KL
divergence coefficient. Importantly, we found a well-performing value DKtaLr = 50 agnostic to other
design choices. It worked well in a range of different tasks and networks (Sec. 5). Therefore, we do
not need to tune β. We provide more discussion about this method in Appendix D.
5	Experiments
How does VLOG perform in practice? We investigated the empirical performance of VLOG in three
types of tasks using online or offline RL, from simple to difficult. In the following experiments, we
used double DQN with dueling network architecture (van Hasselt et al., 2016; Wang et al., 2016) as
the base RL algorithm, the model and loss functions for RL are defined in Appendix. B.1. As DRL is
susceptible to the choice of hyper-parameters, introducing any new hyper-parameters might obscure
the effect of oracle guiding. Double DQN and dueling architecture are preferable for the base algo-
rithm since they require no additional hyper-parameters, in contrast to other DQN variants (Hessel
et al., 2018), such as prioritized experience replay (Schaul et al., 2016), noisy network (Fortunato
et al., 2018), categorical DQN (Bellemare et al., 2017), and distributed RL (Kapturowski et al.,
2018). Importantly, we used the same hyper-parameter setting for all methods and environments as
much as possible (see Appendix B.2).
5.1	MAZE
We first demonstrate how VLOG helps to shape the latent representation by leveraging oracle obser-
vation in learning. The testbed is a maze navigation task3 (Fig. 2A) with 10×10 grids. The executor
observation is the (x, y) position, where x, y are continuous values randomly sampled within each
grid of the maze (thus the observations in two adjacent but wall-separated grids may be very close).
At each step, the agent selects an action (going up, down, right, or left) and moves to another grid
if not blocked by wall. We provided the VLOG agent with oracle observation (xc, yc, dg) during
training, where xc , yc are coordinates of the center of the current grid, and dg is the (shortest) path
distance to goal from the current grid. It is intuitive that although the raw observation is (x, y), dg
matters more in a maze navigation task. We empirically investigated how much such oracle observa-
tion used in learning of VLOG could help to shape the latent representation of z w.r.t. dg rather than
position. The encoder and decoder were both 2-layers multi-layer perceptrons (MLP) with width
256 and ReLU activation. The size of latent vector zt for VLOG was 128 since we computed both
μ and σ (Appendix C).
Experiments show that the baseline agent struggled to reach the goal (Fig. 2B), while VLOG agents
stably solved the task after learning. To check how the usage of VLOG affected the learned la-
tent representation, we visualize the latent representation of both VLOG and baseline models with
principal component analysis (PCA, Pearson F.R.S. (1901)). In Fig. 2C, we map path distance to
goal dg to color and plot the scores of the first 2 PCs of z (computed using executor observation)
3https://github.com/MattChanTK/gym-maze
5
Published as a conference paper at ICLR 2022
Figure 2: Learning latent representations by oracle guiding in a simple maze. (A) Illustration of the
task. (B) Success rate of using VLOG and not using VLOG (baseline), each using 8 random seeds.
(C) PCA of z in VLOG and the corresponding hidden state in baseline model (z collected during
execution, not using oracle observation). Color indicates normalized path distance to goal. The
latent state of VLOG, trained with oracle observation, showed a more sequential and interpretable
representation w.r.t. path distance to goal. We circled the latent regions corresponding to some
places in the maze that the (x, y) positions are close but distances to goal distinguish. (D) The
learned latent representation w.r.t position in the maze, where the green and black component of the
color corresponds to the score of the first 2 PCs of the latent state, respectively, as shown by the
colormap (similar colors indicate similar representations).
for VLOG and the corresponding latent state for baseline using successful trials (“latent layer” in
Fig. 1C). The latent state of VLOG showed a relatively smoother and more interpretable represen-
tation of distance-to-goal comparing to that of baseline. We then plot the latent representations for
different positions in the maze in Fig. 2D. The latent state of VLOG more clearly represented dg ,
consistent with the result (Fig. 2C). In particular, we looked into a rectangle region (denoted by rect-
angles in Fig. 2D) inside which the left 2 grids and right 2 grids are segregated by a wall. We found
the corresponding areas in the latent PC space and circled them in Fig. 2C. While these 4 grids are
close in (x, y) (executor observation), their distances-to-goal (oracle observation) are highly distin-
guished. By leveraging oracle guiding using VLOG, the agents can clearly differentiate the left 2
grids and the right 2 grids in latent space as shown in Fig. 2C, D, left (note that the latent state z here
of VLOG was computed using executor observation only). By contrast, the latent representations
of these grids were overlapped for the baseline model, which did not utilize the oracle observation
(Fig. 2C, D, right). In sum, we demonstrated with a toy example that VLOG effectively helped the
latent space to couple with oracle state useful for the task. The following sections will transfer to
experiments on more complicated tasks and discuss how VLOG can improve practical performance.
5.2	Noisy MinAtar
To evaluate how VLOG scales with more high-dimensional state space, we tested it on a set of
MinAtar videos games. MinAtar (Young & Tian, 2019) is a test platform for AI agents, which
implements 5 miniaturized Atari 2600 games with discrete actions (Seaquest, Breakout, Space In-
vaders, Freeway and Asterix). MinAtar is inspired by Arcade Learning Environment (Bellemare
et al., 2013) but simplifies the environments for efficiency. The observation is 10×10 pixels with
multiple channels indicating different objects. In real-world, the observation usually contains some
noise. Thus it is natural to consider the noisy observation as the partially-observable executor obser-
vation, and the original, non-noisy observation as the oracle one. Suppose that at each frame, each
pixel may “break” randomly with an independent probability of 1/8 (Fig. 3A). The original obser-
vation at a broken pixel is erased and is replaced by a different value in all channels. We consider
such noisy MinAtar environments with the noisy pixels as the executor observation and the original
pixels as the oracle observation.
The network structure was the same as that for Maze, but the encoder was replaced by a CNN
(Appendix C). We ran experiments on all the 5 environments of MinAtar with VLOG as well as
6
Published as a conference paper at ICLR 2022
million frames	million frames
Figure 3: (A) Illustration of the MinAtar environments with randomly broken pixels. (B) Learning
curves showing normalized average return (divided by the average return of trained oracle model)
of VLOG and alternative methods each using 8 random seeds.
baseline, oracle and alternative oracle guiding methods (see Appendix A for details). Baseline
model always uses executor observation as network input (Fig. 1C). Oracle is the same as baseline
except that it always receives oracle observation (i.e., cheating, we ran the experiments of oracle
for reference). VLOG-no oracle is an ablation of VLOG where we use the executor observation
as the input to posterior encoder in VLOG (oracle observation is not used). Suphx-style oracle
guiding is the oracle guiding method used in the Mahjong AI Suphx (Li et al., 2020), in which the
executor observation and the dropout-ed oracle observation (with dropout probability pdropout) are
dropout
concatenated as the input to the network. With training proceeds, p is gradually increased
from 0 to 1, thus the trained network does not need oracle observation for input (Appendix A).
OPD-style oracle guiding is the oracle guiding method used in oracle policy distillation (OPD)
(Fang et al., 2021). OPD-style oracle guiding first trains a teacher model using oracle observation
as input, and then trains the executor model using an auxiliary loss, which is the error between the
executor and the teacher models’ estimation for value function (Appendix A).
The results show that oracle performed usually the best, as expected. We normalized the perfor-
mance of non-oracle models using oracle model as reference, for more clear comparison (Fig. 3B).
Among all the oracle guiding methods (VLOG, OPD-style and Suphx-style), VLOG consistently
performed the best. It is notable that VLOG and VLOG-no oracle performed surprisingly well in
Seaquest. This can be explained by that Seaquest is a task with a local optimum (see Appendix E),
while the stochasticity in hidden states of VLOG helped exploration in latent space to escape from
the local optimum (a similar idea is Fortunato et al. (2018), but their noise was added to the network
weights). Except in Seaquest, VLOG-no oracle did not show significant performance different with
baseline, showing that the performance gain of VLOG in this task set mainly came from leveraging
oracle observation for shaping latent distribution; and the usage of variational Bayesian model was,
at least not harming the performance when there is no helpful oracle information.
5.3	Offline learning on Mahjong
Mahjong is a popular tile-based game with hundreds of millions of players worldwide (here we
consider the Japanese variant). The game is like many other card games (but using tiles instead of
cards), in which multiple (usually four) players draw and discard tiles (totally 136 tiles) alternatively
to satisfy winning conditions. It is a highly challenging game characterized with (1) imperfect
information in executor observation (a player cannot see opponents’ private tiles and the remaining
tiles to be drawn), (2) stochastic state transitions as in many card games, and (3) extremely high
game complexity (i.e., the number of distinguished, legal game states). Complexity of Mahjong is
much larger than 10166 (Appendix F.1). For reference, complexity of Go is 〜10172 (Silver et al.,
2016) and complexity of no-limit Poker is 〜10162 (Johanson, 2013).
In Mahjong, it is hard to make optimal decisions based on executor observation because the out-
comes heavily depend on invisible information, and complexity of invisible state space is as high
as 1048 on average (Li et al., 2020). In response to this challenge, Li et al. (2020) introduced
suphx-style oracle guiding and demonstrated a performance gain. Thus, we consider Mahjong as a
promising test platform for oracle guiding methods. Since the number of possible states in Mahjong
is extremely large. It is costly to explore with random actions in an online RL manner, and no pre-
7
Published as a conference paper at ICLR 2022
vious work could train a strong Mahjong AI with purely online RL. Also, we would like to examine
the effectiveness of VLOG in offline RL settings. For these reasons, we transferred to offline RL
(Levine et al., 2020) for the Mahjong task using expert demonstrations.
We processed about 23M steps of human experts’ plays from the online Mahjong game platform
Tenhou (https://tenhou.net/mjlog.html) to a dataset for offline RL (data were augmented using the
symmetry in Mahjong, see Appendix F). Also, we created a simulator of Mahjong as the testing
environment. Though there are sophisticated ways to encodes the state and action space of Mahjong
(Li et al., 2020), we attempts to make simplifications with reasonable amounts of approximations
since our goal is to not create an strong Mahjong AI, but to use Mahjong as a platform to study
oracle guiding problems. In our case, the action space is composed of 47 discrete actions covering
all decisions in Mahjong. An executor observation is a matrix encoding public information and
the current player’s private hand; An oracle observation concatenates the executor observation with
the information of the opponents’ private hands. (see Appendix F). We used an 1-D CNN as the
encoder as done in Mahjong AIs commonly (Li et al., 2020), and the size of zt and the decoder
network width was increased to 512 and 1024, respectively (Appendix C).
Note that although Mahjong is a 4-player game, using offline RL data to train an agent does not
involve multi-agent RL (Zhang et al., 2021) because the offline dataset is fixed: the opponents have
fixed policies and thus can be considered as parts of the environment. Our experiments focused on
single-agent RL to avoid the complexity caused by considering multi-agent RL. We investigated two
kinds of offline RL settings. The first is conservative Q-learning (CQL) (Kumar et al., 2020). Our
CQL setting differed from the online RL setting in previous sections by adding auxiliary CQL loss
(Kumar et al., 2020) to the Q-learning loss function (Appendix B.1.2). The other is behavior cloning
(BC). Although VLOG was designed for value-based RL, we could straightforwardly incorporate
VLOG with BCby letting the network predict action instead Q function. The learning was conducted
by minimizing the cross entropy between the output and the target action (demonstration) as in a
classification problem. Note that we did not test OPD-style oracle guiding in BC setting because
it was equal to the baseline since we can directly use demonstration actions as oracle policy for
distillation.
being evaluated model ∣		CQL	I		BC
vs. baseline model	avg. payoff	match win rate(%)	avg. payoff	match Win rate(%)
Oracle (cheating)	224 ± 13	53.6 ± 0.4	15 ± 12	50.5 ± 0.4
SuPhx - style	59 ± 13	51.2 ± 0.4	37 ± 12	50.9 ± 0.4
OPD - style	-9 ± 13	50.0 ± 0.4	-	-
VLOG	233 ± 13	55.7 ± 0.4	41 ± 12	51.4 ± 0.4
VLOG-no oracle	61 ± 13	52.0 ± 0.4	67 ± 12	52.2 ± 0.4
Table 1: Performance of each model after training vs. the trained baseline model on Mahjong, each
using 4 random seeds. Each match was played by four independent players (agents) for a series
of 8 games. A player wins a match if it has the highest points among all players when finishing
8 games. Each player played for itself and only consider to maximize its own payoff and ranking.
Among the 4 players, two were being tested agents and the other two were the baseline models
(no communication or team cooperation between agents). The baseline model was trained with the
same offline RL process, but without oracle guiding (Fig. 1C)). To decrease the variance, we sum up
the results of two players with the same model, and use it as the performance of the corresponding
method. Average payoff is the mean of points change per game. Each type of match was repeated
12,500 times (100,000 games). Data are Mean ± S.E.M..
Because Mahjong is a zero-sum, four-player game, we tested the performance of trained models
in two scenarios: playing with the (trained) baseline model (Table 1) and playing with each other
(fight each other, Table 2). In the first scenario, four agents were playing the same matches on the
game table, where two of them were being tested agents and the other two were the baseline models.
Although each agent played for itself and there was no communication between players, we simply
added up the payoff of the two being tested agents, and consider they won a match if one of them
ranked top (so the match win rate will be 50% if equally strong as baseline), for statistics (Table 1).
8
Published as a conference paper at ICLR 2022
Fight each other (CQL)	I avg. payoff	match win rate(%)	I game win rate(%)	deal-in rate(%)
Oracle (cheating)	168 ± 11	28.0 ± 0.4	18.45	4.48
Suphx - style	-75 ± 12	23.2 ± 0.4	18.32	8.84
OPD - style	-209 ± 12	20.1 ± 0.4	14.39	8.06
VLOG	116 ± 12	28.7 ± 0.4	19.36	8.28
I Fight each other (BC) ∣	avg. payoff	match win rate(%) ∣	game win rate(%)	deal-in rate(%) ∣
Oracle (cheating)	-27 ± 11	23.9 ± 0.4	20.69	7.96
SuPhx - style	-10 ± 11	24.6 ± 0.4	20.51	8.03
VLOG	26 ± 11	25.6 ± 0.4	20.82	8.00
VLOG-no oracle	11±11	25.9 ± 0.4	20.76	8.15
Table 2: Trained models playing with each other on the same table. Deal-in means the player
discards a tile and another player wins the game by picking up this tile to compose a winning hand.
Dealing-in results a punishment to the player, thus it is better to be avoided. Each type of match was
repeated for 12,500 times (i.e. 100,000 games). Data are Mean ± S.E.M..
For CQL, the results (Table 1 left and 2 upper) show that VLOG substantially outperformed the
baseline and alternative methods (because Mahjong is a highly random game, 55.7% match win
rate indicates a large skill gap). Interestingly, VLOG was even comparable to oracle. This can be
explained by that VLOG also benefited from its Bayesian property, which is consistent with that
VLOG-no oracle showed a significant performance gain over the baseline model (Table 1 left). Still,
the oracle model learned to reduce deal-ins (i.e., player discards a tile and another player wins the
game by picking up this tile to compose a winning hand) since it could explicitly see the opponents’
private tiles, showing a much lower deal-in rate than other non-cheating models (Table 2 upper).
In BC setting, the agents did not learn a value function, but tried to predict human experts’ actions.
Therefore, the training procedure did not involve reasoning the relationship between playing out-
come and oracle observation, but just imitating human behaviors. This can be seen from the results
that oracle did not substantially outperform baseline in BC (Table 1 right and 2 lower). However,
VLOG and VLOG-no oracle still showed performance gain, thanks to the stochastic modeling.
6	Summary
We have proposed VLOG - a variational Bayesian learning framework for leveraging oracle obser-
vation to facilitate DRL especially in partially observable environments. VLOG is available for any
RL problem in which there is oracle observation that may help the executor to make decisions.
We first introduced a latent vector z to represent the environmental state. The prior and posterior
distribution of z is modeled using executor and oracle observation, respectively. Then, we derived
a variational lower bound (Eq. 2) by maximizing which we can optimize the executor model us-
ing the oracle observation. We developed the corresponding methodology for DRL, which can be
incorporated with most RL algorithms that need to estimate a value function.
If oracle observation contains more information to retrieval the true environmental state (or, it is the
true environmental state), VLOG’s oracle guiding in latent space helps to shape a latent representa-
tion in neural networks closer to the true one. We demonstrated this advantage of VLOG using the
maze task. Then, we scaled VLOG up to solve image-based videos games, and compared it with
alternative oracle-guiding methods. Though all oracle-guiding methods showed performance gain
over the baseline model, VLOG performed consistently the best. Finally, we transferred to offline
RL domain using a challenging tile-based game Mahjong in which an executor plays with hidden
information and random state transitions, and observed VLOG achieved best overall performance.
We also conducted an ablation study of VLOG (VLOG-no oracle) in which posterior model did not
receive oracle observation, but executor one. VLOG-no oracle demonstrated performance gain in
the tasks that may benefit from the stochasticity; otherwise, it performs similar to the determinis-
tic baseline. This clarified that the source of VLOG’s promising performance is two-fold: oracle
guiding and stochastic modeling. Finally, we publish the dataset of Mahjong for offline RL and the
corresponding RL environment so as to facilitate future research on oracle guiding.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work was supported by Microsoft Research Asia. Kenji Doya was supported by Japan So-
ciety for the Promotion of Science KAKENHI Grant Numbers JP16K21738, JP16H06561 and
JP16H06563, as well as by Okinawa Institute of Science and Technology.
Reproducibility statement
The source code of VLOG can be found in Supplementary Material.
Ethics statement
We declare no conflict of interests. We tried to use friendly colors to people with color recognition
disabilities (Fig. 2C, D) and distinguishable markers for performance curves of different models
(Fig. 2B, Fig. 3 and Fig6). Our Mahjong dataset was generated using downloadable, public game
replay data from Tenhou.net with post-processing. The dataset contains no private information about
players. Since VLOG is a general framework for leveraging the oracle information, we cannot
foresee any direct application of VLOG to malicious purposes. However, any new RL algorithm
might confer increased autonomy on an agent, and eventually lead to a completely autonomous
agent, which can be used for malicious purposes, e.g., fully autonomous soldiers.
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep Variational Information
Bottleneck. In International Conference on Learning Representations, 2017.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, 2017.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Marc G. Bellemare, Will Dabney, and Remi Munos. A Distributional Perspective on Reinforcement
Learning. In International Conference on Machine Learning, 2017.
Christopher P. Burgess, Irina Higgins, Arka Pal, Lolc Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-VAE. 2017.
Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp KrahenbUhL Learning by cheating. In Con-
ference on Robot Learning, pp. 66-75. PMLR, 2020.
Yuchen Fang, Kan Ren, Weiqing Liu, Dong Zhou, Weinan Zhang, Jiang Bian, Yong Yu, and Tie-Yan
Liu. Universal Trading for Order Execution with Oracle Policy Distillation. In AAAI Conference
on Artificial Intelligence, 2021.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg. Noisy Networks for Exploration. In International Conference on
Learning Representations, 2018.
Thomas Furmston and David Barber. Variational methods for reinforcement learning. In Pro-
ceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp.
241-248. JMLR Workshop and Conference Proceedings, 2010.
Arthur Guez, Fabio Viola, Theophane Weber, Lars Buesing, Steven Kapturowski, Doina Precup,
David Silver, and Nicolas Heess. Value-driven Hindsight Modelling. In Advances in Neural
Information Processing Systems, 2020.
10
Published as a conference paper at ICLR 2022
David Ha and Jurgen Schmidhuber. Recurrent World Models Facilitate Policy Evolution. In Ad-
vances in Neural Information Processing Systems, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1856-1865, 2018.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic Algo-
rithms and Applications. arXiv, abs/1812.05905, 2019.
Dongqi Han, Kenji Doya, and Jun Tani. Variational Recurrent Models for Solving Partially Observ-
able Control Tasks. In International Conference on Learning Representations, 2020.
Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi Azar, Bilal Piot, Nico-
las Heess, Hado van Hasselt, Gregory Wayne, Satinder Singh, Doina Precup, and Remi Munos.
Hindsight Credit Assignment. In Advances in Neural Information Processing Systems, 2019.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining Improvements in
Deep Reinforcement Learning. In AAAI Conference on Artificial Intelligence, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning Basic Visual Concepts with a Con-
strained Variational Framework. In International Conference on Learning Representations, 2016.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep Variational
Reinforcement Learning for POMDPs. In International Conference on Machine Learning, 2018.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, 2020.
Michael Johanson. Measuring the size of large no-limit poker games. arXiv preprint
arXiv:1302.7008, 2013.
Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent Expe-
rience Replay in Distributed Reinforcement Learning. In International Conference on Learning
Representations, 2018.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Confer-
ence on Learning Representations, 2014.
W. Bradley Knox and Peter Stone. Interactively Shaping Agents via Human Reinforcement: The
TAMER Framework. In International Conference on Knowledge Capture, 2009.
Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney,
Michal Valko, and David Abel. Revisiting Peng’s Q(λ) for Modern Reinforcement Learning. In
International Conference on Machine Learning, 2021.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-Learning for Of-
fline Reinforcement Learning. In Advances in Neural Information Processing Systems, 2020.
Alex Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep
reinforcement learning with a latent variable model. Advances in Neural Information Processing
Systems, 33, 2020.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review,
2018.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
11
Published as a conference paper at ICLR 2022
Junjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao
Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering Mahjong with Deep Reinforcement
Learning. arXiv, abs/2003.13590, 2020.
Robert Loftin, Bei Peng, James MacGlashan, Michael L. Littman, Matthew E. Taylor, Jeff Huang,
and David L. Roberts. Learning behaviors via human-delivered discrete feedback: modeling
implicit feedback strategies to speed up learning. Autonomous agents and multi-agent systems,
30(1):30-59, 2016.
James MacGlashan, Mark K. Ho, Robert Loftin, Bei Peng, Guan Wang, David L. Roberts,
Matthew E. Taylor, and Michael L Littman. Interactive Learning from Policy-Dependent Hu-
man Feedback. In International Conference on Machine Learning, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529, 2015.
Masashi Okada, Norio Kosaka, and Tadahiro Taniguchi. PlaNet of the Bayesians: Reconsidering
and Improving Deep Planning Network by Incorporating Bayesian Inference. In International
Conference on Intelligent Robots and Systems, 2020.
Karl Pearson F.R.S. LIII. On lines and planes of closest fit to systems of points in space. The
London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559-572,
1901.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience Replay. In
International Conference on Learning Representations, 2016.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Edward J. Sondik. The optimal control of partially observable markov processes over the infinite
horizon: Discounted costs. Oper. Res., 26(2):282-304, 1978.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
2nd edition, 2018.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
IEEE Information Theory Workshop. IEEE, 2015.
Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double
Q-learning. In AAAI conference on artificial intelligence, 2016.
Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan
Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou,
Max Jaderberg, Alexander Sasha Vezhnevets, Remi Leblond, Tobias Pohlen, Valentin Dalibard,
David Budden, Yury Sulsky, James Molloy, Tom Le Paine, CagIar GUICehre, Ziyu Wang, To-
bias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver
Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and
David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature,
575(7782):350-354, 2019.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando Freitas. Du-
eling Network Architectures for Deep Reinforcement Learning. In International conference on
machine learning, 2016.
Theophane Weber, Nicolas Heess, Ali Eslami, John Schulman, David Wingate, and David Silver.
Reinforced variational inference. In Advances in Neural Information Processing Systems (NIPS)
Workshops, 2015.
12
Published as a conference paper at ICLR 2022
Haiyan Yin, Jianda Chen, Sinno Jialin Pan, and Sebastian Tschiatschek. Sequential generative
exploration model for partially observable reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35,pp. 10700-10708, 2021.
Kenny Young and Tian Tian. MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible
Reinforcement Learning Experiments. arXiv preprint arXiv:1903.03176, 2019.
Kaiqing Zhang, ZhUoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp.
321-384, 2021.
13
Published as a conference paper at ICLR 2022
A Implementation of alternative oracle guiding methods
Figure 4: Diagram of our implementation for Suphx-style and OPD-style oracle guiding. For OPD-
style oracle guiding, the teacher model is trained and then fixed in prior to training the student model
(executor model).
In the original works of Suphx (Li et al., 2020) and OPD (Fang et al., 2021), the authors were per-
forming oracle guiding to different RL objectives to ours. For comparison, we re-implemented
Suphx-style and OPD-style oracle guiding in our settings using similar network structures (see
Fig. 4) and used the same hyper-parameters wherever possible. However, the key methodology
of Suphx-style and OPD-style oracle guiding remains the same as those in their original works.
As shown in Fig. 4 left, suppose executor observation is Xt and oracle observation is Xt, Suphx-Style
model receives input
XtuPhX = concatenate(xt, δt Θ Xt),
where δt is the dropout matrix whose elements are Bernoulli variables with P (δt (i, j) = 1) =
1 - pdropout, and Θ denotes element-wise multiplication. During training, pdropout was linearly
increased from 0 to 1 in the first 2/3 training epochs, and kept to be 1 in the remaining 1/3 epochs.
The other parts of training were the same as the baseline model.
For OPD-style oracle guiding, we directly used the trained oracle model as the teacher model (Fig. 4,
right). The loss function (in Q-learning) was
Jopd = MSE (Qtar, Q(xt, at) + μMSE (QteaCher(Xt), Q(xt, at)),
where the first term MSE (Qttar, Q(Xt, at)) was the same as the loss function for the baseline model.
And the second term is the distillation loss with a coefficient μ (note that the teacher model is fixed
during training). We selected the best-performing μ for each task using grid search (see Table 3).
B RL algorithms and hyper-parameters
B.1	RL algorithms
B.1.1	Dueling Double DQN for Maze and MinAtar tasks
As we discussed in Sec. 5, we used double DQN with dueling network architecture (van Hasselt
et al., 2016; Wang et al., 2016) as the base RL algorithm, because it work relatively well (Hessel
et al., 2018) without introducing additional hyper-parameter.
The Dueling architecture of DQN (Wang et al., 2016) is defined as follows (see Appendix C for
hidden layer size):
14
Published as a conference paper at ICLR 2022
class DuelingQNetwork ( nn . Module) :
def __init__ ( self , input_size , action_num , hidden_layers ):
super (DuelingQNetwork, self ). __init__ ()
self . input_size = input_Size
self . action_num = action_num
self . hidden_layers = hidden JayerS
self . network_modules = nn . ModuleList ()
last_layer_Size = input_Size
for layer_Size in hidden JayerS :
self . network_modules . append (nn . Linear ( last_layer_Size , layer_Size ))
self . network_modules . append (nn .ReLU())
last_layer_size = layer_size
self . value_layer = nn. Linear ( last_layer_size , 1)
self . advantage_layer = nn. Linear (last-layer_size , action_num)
self . main .network = nn . Sequential (* self . network_modules )
def forward ( self , x):
h = self . main_network(x)
V = self. value_layer(h). repeat_interleave( self. action _num , dim=-1)
q0 = self .advantage_layer(h)
a = q0 - torch .mean(q0 , dim=-1, keepdim=True ). repeat_interleave(
self . action_num , dim=-1)
q=v+a
return q
Double deep Q-learning (van Hasselt et al., 2016) was used to compute Qtarget in Fig. 1 A (one can
use any other algorithm to compute Qtarget without changing other parts). In particular, as in Wang
et al. (2016), we have
Qttarget = rt + γQ(zt, arg max Q(zt+1, a0; θ); θ-),
a0
where rt is the reward at step t, γ is the discount factor (Table 3), θ denotes the parameters of
the Q network (MLP decoder) for computing the Q-function (Appendix C). Note that z is given
by the posterior decoder with oracle observation x as input, since the oracle prediction error term
Eq(ζt∣Xt) [log P (V(Zt) = vtar |zt)] in Eq. 1 is the expectation over the posterior distribution q(z∣x).
Following deep RL normals (Mnih et al., 2015; Wang et al., 2016; van Hasselt et al., 2016), we used
a target Q network with the same structure as the original Q network, of which the parameters are
denoted as θ- (Table 3). Every 1,000 steps, the target Q network copies the parameters from the
original Q network (Table 3). Then the first term of the VLOG loss function (Eq. 2) is simply given
by the mean square error between Qtarget and the output of Q network (MLP decoder) for the Maze
and MinAtar tasks.
B.1.2	Dueling Double DQN with conservative Q learning for Mahjong
In Mahjong, as we transfer to offline RL domain (Sec. 5.3), directly using off-policy RL algorithm
usually results to very unsatisfying performance (Levine et al., 2020).
Therefore, we complement the loss function of VLOG (Eq. 2) with an auxiliary conservative Q-
learning (CQL) loss (Kumar et al., 2020),
Jcql = αEχ,0〜D log X exp(Q(z(X), a0, θ)) - [Q(z(X), a, θ)],
a0∈A
where D is the offline dataset we used for Mahjong and α = 1. The combined loss function used
for Mahjong (CQL) is JVβLOG + JCQL.
B.2	Hyper-parameter selection
We summarize the hyper-parameters in Table 3.
15
Published as a conference paper at ICLR 2022
Explanation	Symbol	Value	Comment
Common			
Discount factor Learning rate Batch size coefficient of e-greedy target network update interval environment steps per gradient step Optimizer	Y e T	1 (Mahjong) 0.995 (others) 0.0001 1024 (Mahjong) 128 (others) 0.1 (learning) 0 (evaluation) 1000 4 Adam	For all loss functions For online RL For online RL For all loss functions
VLOG			
KL divergence target value Initial value of β	tar DKL	50 0.00001	Fixed in all experiments
OPD - style			
coefficient of policy distillation loss	μ	0.01 (Mahjong) 10 (others)	By grid search
Table 3: Hyper-parameters used in this paper. For OPD-style, we did grid search for the additional
hyper-parameter μ to select the best-performing value among [0.001, 0.01, 0.1, 1, 10].
B.3	SENSITIVITY ANALYSIS FOR β
As we mentioned in Sec. 4.2, the coefficient β is important to the learning of VLOG agents. If we
fix the value of β throughout training, a too large or too small β will result in worse performance.
The corresponding results are shown in Fig. 6.
C Network structures
For simplicity, We use the same hiddenJayerdize for all the fully connected layers, where the hid-
den-layersize is 256 for maze and MinAtar, and 1024 for Mahjong.
C.1 Encoder
Since we targeted at various tasks, we used different encoder network for each type of environment.
The prior and posterior encoder has the same structure except different sizes of input features/chan-
nels.
For the maze task, the encoder was a 2-layers MLP with ReLU activation. The output size is also
equal to hidden JayerSize.
For the MinAtar tasks, we used 2-D CNN encoder defined as follows:
import torch . nn as nn
cnn_module_list = nn . ModuleList ()
cnn_module_list. append(nn . Conv2d( n.channels , 16, 3 , 1, 0))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Conv2d(16 , 32, 3 , 1, 0))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Conv2d(32 , 128, 4, 2, 0))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Conv2d(128 , 256, 2, 1, 0))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Flatten ())
cnn_minatar = nn. Sequential (*cnn_module_list)
where n_ChannelS is the number of channels of executor or oracle observation. The output size is
equal to hidden JayerSize.
16
Published as a conference paper at ICLR 2022
For Mahjong, because the second dimension of observation (tile ID) has local contextual relationship
(Li et al., 2020), we used 1-D CNN (convolution along the tile ID dimension) as the encoders,
defined as follows:
cnn_module_list = nn . ModuleList ()
cnn_module_list. append(nn . Conv1d( n .channels , 64, 3, 1, 1))
cnn_module_list. append(nn.Conv1d(64 , 64, 3 , 1, 1))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Conv1d(64 , 64, 3 , 1, 1))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Conv1d(64 , 32, 3 , 1, 1))
cnn_module_list. append(nn .ReLU())
cnn_module_list. append(nn . Flatten ())
cnn_mahjong = nn. Sequential (* cnn_module_list)
The output size is 1088, close to hiddendayersize.
C.2 Latent layer
For VLOG and VLOG (no-oracle), the size of Z layer is half of hiddenJayersize because We need to
estimate both the mean and the variance of z. For all the other models (baseline, oracle, OPD-style,
Suphx-style), the latent layer is one fully connected layer with size hiddenJayersize and ReLU
activation.
C.3 Decoder
The decoders for all models were 2-layers MLPs with size hiddenJayerdize. Except BC on
Mahjong, the input of decoder was the output of the latent layer concatenated with the action, and
we used the dueling Q-network structure (Wang et al., 2016) to output a scalar Q value. ReLU
activation was used except for the output.
For BC on Mahjong, the input of decoder was the output of the latent layer. The outputs of decoder
was logit of actions, and the actions could be obtained using softmax.
D Task-agnostic latent b ottleneck control
As we discussed in Sec. 4.2, the coefficient β of the regularization term in the VLOG loss function
(Eq. 2) is adaptively regularized by Eq. 3, provided with DKtaLr , which is another hyper-parameter.
While our experiments demonstrated the effectiveness of this approach, the follows discuss more
thoughts behind this design choice.
In principle, replacing a hyper-parameter with another does not always make training easier. How-
ever, in practice (especially deep RL), the performance will be highly sensitive to some hyper-
parameters (e.g., the entropy coefficient α in the original soft-actor critic algorithm (Haarnoja et al.,
2018) need to be tuned in each robotic task. This is because the reward magnitude is different among
tasks.). As a result, it will be beneficial to replace a sensitive hyper-parameter with another one that
does not need fine tuning. For example, in the follow-up paper of soft-actor critic, the authors used
adaptive entropy coefficient α by introducing another hyper-parameter, the entropy target (Haarnoja
et al., 2019). They empirically found that it is good to set the entropy target equal to the negative of
the degree of freedom of the agent, thus to avoid tuning α.
Our idea of replacing β with DKtaLr is due to similar reasons. One obvious problem is that the
magnitude of the “oracle prediction error” term (Eq. 2) relies on the reward magnitude of the task.
Therefore β should also be adjusted to match with the magnitude of task reward. However, DKtaLr
is only relevant to the magnitude of prior z and posterior z, which does not differ too much among
tasks (usually at the order of 1). In practice, we found that DKtaLr = 50 works well for all the tasks
including Maze, MinAtar and Mahjong.
Another way of regularizing β is to employ a linear or exponential scheduler. For example, in
Burgess et al. (2017), the authors used a linear scheduler for the target KL-divergence and got good
17
Published as a conference paper at ICLR 2022
results. However, using a scheduler introducing more hyper-parameters (at least two: initial β and
final β), which is against our intention to reduce the impact of hyper-parameters.
E	Seaquest local optimum
In Seaquest, the agent drives a submarine diving into the sea to shoot at enemies and rescue divers
to earn scores. However, the submarine has limited oxygen. It must surface to replenish the oxygen
before running out for surviving, by doing which it temporarily cannot earn scores. A typical local
optimum is to use the last remaining oxygen for diving instead of surfacing.
F Mahjong
F.1 Estimation of game complexity of Mahjong
We consider 4-players Japanese Mahjong4. Although there are minor variants of rules, the following
estimation applies to general cases.
For easier computation, we make two major simplifications (i.e., we estimate a lower bound of the
game complexity): (1) melding from discard5 is not considered and (2) the information other than
tiles, such as results of contextual games, points of players, is not considered.
There are 34 types of tiles, each with 4 duplicates (so totally 136 tiles). We further restrict our
estimation to the last turn (i.e., the last tile is drawn). Among 136 tiles, 53 tiles are in someone’s
hand (the 4 players have 14, 13, 13, 13 tiles in hands, respectively). Permutation of tiles in one’s
hand does not make a different, while the permutation matters if not in one’s hand. The number of
distinguishable configurations of 136 tiles thus can be computed as (13!)3×136!!×(4!)34 〜10145
Meanwhile, for each discarded tile, it is important to know whether it was discarded immediately
after being drawn or not. For 70 discarded tiles, the number of possibilities is simply 270 〜1021.
Therefore, the lower bound of game complexity of Mahjong is estimated as
10145 X 1021 〜10166.
If considering the other being simplified information, the state space could be much larger. For
example, let’s consider the current points of each player. The most common rule is that each player
starts with 25,000 points, with 100 points as the minimal unit (totally 1,000 units), and the game
terminates if someone got negative points. So the number of possibilities can be converted to the
answer of “how many ways to distribute 1000 candies to 4 kids”, which is
(1000+1)(4T)
-(4-1)
〜
108.
F.2 Details of observation space and action space encoding
In our Mahjong environment6 *, the action space is composed of 47 discrete actions (Table 5). Because
not all actions are available at a certain step, we also provide the set of valid actions among all 47
actions at each step according to the rules, which can be used during playing and learning.
The oracle observation has the shape of 111 × 34 (executor observation is a part of oracle obser-
vation, with shape 93 × 34) (Fig. 5). The first dimension corresponds to 111 features (channels).
The second dimension of observation (with size 34) corresponds to 34 Mahjong tiles (the order is
Character 1-9, Dot 1-9, Bamboo 1-9, East, South, West, North, White, Green, Red). We used 1-D
CNNs with convolution along the second dimension for the encoders. Suppose the current player is
player 0, and other players are numbered 1, 2, 3 counter-clockwise. The value of any element in an
observation is 1 or 0, explained in Table 4.
4https://en.WikiPedia.org/wiki/JaPaneSe_Mahjong
5A meld is a specific pattern of three or four tiles. A player can pick up a discarded tile from others to form
a meld by displaying the meld to public, if certain condition is satisfied.
6The Mahjong environment we used in this papers is available on https://github.com/pymahjong/pymahjong
for reproducibility. However, we recommend to use the newer version https://github.com/Agony5757/mahjong
which is better-supported by the authors and much faster.
18
Published as a conference paper at ICLR 2022
Rough meaning	Feature index	Meaning (t is the corresponding tile)
Hand of player 0	0 1 2 3 4 5	If player 0 has ≥1 tin hand If player 0 has ≥2 tin hand If player 0 has ≥3 tin hand If player 0 has 4 tin hand If player 0 has dicarded t in this game If player 0 has Red Dora t in hand
Callings of player 0 (melding from discarded tiles)	6 7 8 9 10 11	If player 0 has ≥1 tin callings If player 0 has ≥2 tin callings If player 0 has ≥3 tin callings If player 0 has 4 tin callings If player 0 has ≥1 tin callings and t is from other,s discarded tiles If player 0 has Red Dora t in callings
Callings of player 1 Callings of player 2 Callings of player 3	12to 17 18 to 23 24 to 29	Same as 5 to 11, but for player 1 Same as 5 to 11, but for player 2 Same as 5 to 11, but for player 3
Discarded tiles from player 0	30 31 32 33 34 35 36 37 38 39	If player 0 has discarded ≥1 t If player 0 has discarded ≥2 t If player 0 has discarded ≥3 t If player 0 has 4 tin calling If player 0's first dicarded t is Tegiri (if applicable) If player 0's second dicarded t is Tegiri (if applicable) If player 0's third dicarded t is Tegiri (if applicable) If player 0's fourth dicarded t is Tegiri (if applicable) If player 0 has discarded Red Dora t If player 0 has discarded t to annouce Riichi
Discarded tiles from player 1 Discarded tiles from player 2 Discarded tiles from player 3	-40 to 49 50 to 59 60 to 69	Same as 30-39, but for player 1 Same as 30-39, but for player 2 Same as 30-39, but for player 3
Other public information	70 71 72 73 74 75 76 77 78 79	If t is Dora indicator (≥ 1 repeats) If t is Dora indicator (≥ 2 repeats) If t is Dora indicator (≥ 3 repeats) If t is Dora indicator (4 repeats) If t is Dora (≥ 1 repeats) If t is Dora (≥ 2 repeats) If t is Dora (≥ 3 repeats) If t is Dora (4 repeats) If t is wind of the table If t is wind of self
The tile of the latest action	80	―	If t is the tile corresponding to the latest action
Information for available actions	ICIgy∙C9∣>86θιC∣ 888888888666	If at t is in player 0's hand If at t can be Chi, and is the smallest in the meld If at t can be Chi, and is the middle in the meld If at t can be Chi, and is the largest in the meld If at t can be Pong If at t can be An-Kan If at t can be Kan If at t can be Ka-Kan If Riichi is possible by discarding t If t is the latest discarded tile from others enabling RonAgari If t is the latest drawn tile enabling Tsumo If t is Kyuhai and is in player 0's hand
Hand of player 1 (only for oracle) Hand of player 2 (only for oracle) Hand of player 3 (only for oracle)	93 to 98 99to 104 105 to 110	Same as 0 to 5, but for player 1 Same as 0 to 5, but for player 2 Same as 0 to 5, but for player 3
Table 4: Explanation of the 111 features of oracle observation encoding (the first 93 features are
available to the executor) of our Mahjong environment. Player 0 is the current player who is making
decision and player 1, 2, 3 are opponents counterclockwise. Tegiri (“discard from hand”) means the
tile is not discarded immediately after drawing it.
19
Published as a conference paper at ICLR 2022
Action index	Explanation
0	Discard Character 1
1	Discard Character 2
2	Discard Character 3
3	Discard Character 4
4	Discard Character 5 (non-Red Dora with higher priority)
5	Discard Character 6
6	Discard Character 7
7	Discard Character 8
8	Discard Character 9
9	Discard Dot 1
10	Discard Dot 2
11	Discard Dot 3
12	Discard Dot 4
13	Discard Dot 5 (non-Red Dora with higher priority)
14	Discard Dot 6
15	Discard Dot 7
16	Discard Dot 8
17	Discard Dot 9
18	Discard Bamboo 1
19	Discard Bamboo 2
20	Discard Bamboo 3
21	Discard Bamboo 4
22	Discard Bamboo 5 (non-Red Dora with higher priority)
23	Discard Bamboo 6
24	Discard Bamboo 7
25	Discard Bamboo 8
26	Discard Bamboo 9
27	Discard East Wind
28	Discard South Wind
29	Discard West Wind
30	Discard North Wind
31	Discard White Dragon Tile (HakU)
32	Discard Green Dragon Tile (Hatsu)
33	Discard Red Dragon Tile (Chu)
34	Chi (the picked up tile is the smallest in the meld)
35	Chi (the picked up tile is the middle in the meld)
36	Chi (the picked up tile is the largest in the meld)
37	Pon
38	An-Kan
39	Kan
40	Ka-Kan
41	Riichi
42	Ron
43	Tsumo
44	Restart the game with Kyushukyuhai
45	Not to response (when Chi, Pon, Kan, Ron, etc. is possible)
46	Not to Riichi (When Riichi is possible)
Table 5: Action encoding of our Mahjong environment.
20
Published as a conference paper at ICLR 2022
Figure 5: Picture of a Mahjong game. The executor observation includes the publicly visible infor-
mation and the player’s private hand tiles. The oracle observation includes the executor observation
and additional information about the opponents’ private tiles.
F.3 Data augmentation
In (Japanese) Mahjong, there are 3 suit sets of (characters, dots, bamboos) tiles, 4 wind tiles and 3
dragon tiles. The 3 suit sets are symmetric and thus exchangeable with each other7. So are the 4
wind tiles and the 3 dragon tiles. According to such symmetry, we augmented the offline RL dataset
by randomly exchanging the 3 suit sets, the 4 wind tiles and the 3 dragon tiles, respectively.
7There is one exemption of winning hand pattern “all green”. Since it is an extremely rare case, we simply
ignore it.
21
Published as a conference paper at ICLR 2022
breakout
SeaqUeSt
million frames	million frames
SpaceJnvaders
million frames
freeway
12	3	4
million frames
I 1°gιo^ = ~9∣
O ɪogɪo β = -7
f-IogioO = -5
ʌ l°gιo β = -3
ι°gιo / = -1∣
∣τ-log；o/3 = l I
Figure 6: Sensitivity analysis of VLOG in noisy MinAtar environments w.r.t. the selection of β if β
is fixed.
22