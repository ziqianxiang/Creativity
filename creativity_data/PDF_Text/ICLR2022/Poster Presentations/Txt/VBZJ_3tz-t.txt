Published as a conference paper at ICLR 2022
The Unreasonable Effectiveness of Random
Pruning: Return of the Most Naive Baseline
for Sparse Training
Shiwei Liu1, Tianlong Chen2, Xiaohan Chen2, Li Shen3
Decebal Constantin Mocanu1,4, Zhangyang Wang2, Mykola Pechenizkiy1,
1	Eindhoven University of Technology, 2University of Texas at Austin
3JD Explore Academy,4University of Twente,
{s.liu3,m.pechenizkiy}@tue.nl,
{tianlong.chen,xiaohan.chen,atlaswang}@utexas.edu,
d.c.mocanu@utwente.nl, mathshenli@gmail.com
Ab stract
Random pruning is arguably the most naive way to attain sparsity in neural net-
works, but has been deemed uncompetitive by either post-training pruning or
sparse training. In this paper, we focus on sparse training and highlight a per-
haps counter-intuitive finding, that random pruning at initialization can be quite
powerful for the sparse training of modern neural networks. Without any delicate
pruning criteria or carefully pursued sparsity structures, we empirically demon-
strate that sparsely training a randomly pruned network from scratch can match
the performance of its dense equivalent. There are two key factors that con-
tribute to this revival: (i) the network sizes matter: as the original dense networks
grow wider and deeper, the performance of training a randomly pruned sparse
network will quickly grow to matching that of its dense equivalent, even at high
sparsity ratios; (ii) appropriate layer-wise sparsity ratios can be pre-chosen for
sparse training, which shows to be another important performance booster. Simple
as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely
trained to outperforming a dense Wide ResNet-50, on ImageNet. We also ob-
served such randomly pruned networks outperform dense counterparts in other
favorable aspects, such as out-of-distribution detection, uncertainty estimation, and
adversarial robustness. Overall, our results strongly suggest there is larger-than-
expected room for sparse training at scale, and the benefits of sparsity might be
more universal beyond carefully designed pruning. Our source code can be found
at https://github.com/VITA-Group/Random_Pruning.
1	Introduction
Most recent breakthroughs in deep learning are fairly achieved with the increased complexity of
over-parameterized networks (Brown et al., 2020; Raffel et al., 2020; Dosovitskiy et al., 2021; Fedus
et al., 2021. arXiv:2101.03961; Jumper et al., 2021; Berner et al., 2019). It is well-known that large
models train better (Neyshabur et al., 2019; Novak et al., 2018; Allen-Zhu et al., 2019), generalize
better (Hendrycks & Dietterich, 2019; Xie & Yuille, 2020; Zhao et al., 2018), and transfer better (Chen
et al., 2020b;a; 2021b). However, the upsurge of large models exacerbates the gap between research
and practice since many real-life applications demand compact and efficient networks.
Neural network pruning, since proposed by (Mozer & Smolensky, 1989; Janowsky, 1989), has evolved
as the most common technique in literature to reduce the computational and memory requirements of
neural networks. Over the past few years, numerous pruning criteria have been proposed, including
magnitude (Mozer & Smolensky, 1989; Han et al., 2015; Frankle & Carbin, 2019; Mocanu et al.,
2018), Hessian (LeCun et al., 1990; Hassibi & Stork, 1993), mutual information (Dai et al., 2018),
Taylor expansion (Molchanov et al., 2016), movement (Sanh et al., 2020), connection sensitivity (Lee
et al., 2019), etc. Motivated for different scenarios, pruning can occur after training (Han et al., 2015;
Frankle & Carbin, 2019; Molchanov et al., 2016; Lee et al., 2021), during training (Zhu & Gupta,
1
Published as a conference paper at ICLR 2022
2017; Gale et al., 2019; Louizos et al., 2018; You et al., 2020; Chen et al., 2021c;a), and even before
training (Mocanu et al., 2018; Lee et al., 2019; Gale et al., 2019; Wang et al., 2020; Tanaka et al.,
2020). The last regime can be further categorized into “static sparse training” (Mocanu et al., 2016;
Gale et al., 2019; Lee et al., 2019; Wang et al., 2020) and “dynamic sparse training” (Mocanu et al.,
2018; Bellec et al., 2018; Evci et al., 2020a; Liu et al., 2021b;a).
While random pruning is a universal method that can happen at any stage of training, training a
randomly pruned network from scratch is arguably the most appealing way, owing to its “end-to-end”
saving potential for the entire training process besides the inference. Due to this reason, we focus
on random pruning for sparse training in this paper. When new pruning approaches bloom, random
pruning naturally becomes their performance’s empirical “lower bound” since the connections are
randomly chosen without any good reasoning. Likely due to the same reason, the results of sparse
training with random pruning (as “easy to beat” baselines to support fancier new pruning methods)
reported in the literature are unfortunately vague, often inconsistent, and sometimes casual. For
instance, it is found in Liu et al. (2020b) that randomly pruned sparse networks can be trained from
scratch to match the full accuracy of dense networks with only 20% parameters, whereas around 80%
parameters are required to do so in Frankle et al. (2021). The differences may arise from architecture
choices, training recipes, distribution hyperparameters/layer-wise ratios, and so on.
In most pruning literature (Gale et al., 2019; Lee et al., 2019; Frankle et al., 2021; Tanaka et al.,
2020), random pruning usually refers to randomly removing the same proportion of parameters
per layer, ending up with uniform layer-wise sparsities. Nevertheless, researchers have explored
other pre-defined layer-wise sparsities, e.g., uniform+ (Gale et al., 2019), Erdos-Renyi random
graph (ER)(MOcanU et al., 2018), and Erdos-Renyi-Kernel (ERK)(Evci et al., 2020a). These layer-
wise sparsities also fit the category of random pruning, as they require no training to obtain the
corresponding sparsity ratios. We assess random pruning for sparse training with these layer-wise
sparsity ratios, in terms of various perspectives besides the predictive accuracy.
Our main findings during this course of study are summarized below:
•	We find that the network size matters for the effectiveness of sparse training with random
pruning. With small networks, random pruning hardly matches the full accuracy even at mild
sparsities (10%, 20%). However, as the networks grow wider and deeper, the performance
of training a randomly pruned sparse network will quickly grow to matching that of its dense
equivalent, even at high sparsity ratios.
•	We further identify that appropriate layer-wise sparsity ratios can be an important booster
for training a randomly pruned network from scratch, particularly for large networks. We
investigate several options to pre-define layer-wise sparsity ratios before any training; one
of them is able to push the performance of a completely random sparse Wide ResNet-50
over the densely trained Wide ResNet-50 on ImageNet.
•	We systematically assess the performance of sparse training with random pruning, and
observe surprisingly good accuracy and robustness. The accuracy achieved by ERK ratio
can even surpass the ones learned by complex criteria, e.g., SNIP or GraSP. In addition,
randomly pruned and sparsely trained networks are found to outperform conventional dense
networks in other favorable aspects, such as out-of-distribution (OoD) detection, adversarial
robustness, and uncertainty estimation.
2	Related Work
2.1	Static Sparse Training
Static sparse training represents a class of methods that aim to train a sparse subnetwork with a fixed
sparse connectivity pattern during the course of training. We divide the static sparse training into
random pruning and non-random pruning according to whether the connection is randomly selected.
Random Pruning. Static sparse training with random pruning samples masks within each layer in
a random fashion based on pre-defined layer-wise sparsities. The most naive approach is pruning
each layer uniformly with the same pruning ratio, i.e., uniform pruning (Mariet & Sra, 2016; He
et al., 2017; Suau et al., 2019; Gale et al., 2019). Mocanu et al. (2016) proposed a non-uniform and
scale-free topology, showing better performance than the dense counterpart when applied to restricted
2
Published as a conference paper at ICLR 2022
Boltzmann machines (RBMs). Later, expander graphs were introduced to build sparse CNNs and
showed comparable performance against the corresponding dense CNNs (Prabhu et al., 2018; Kepner
& Robinett, 2019). While not initially designed for static sparse training, ER (Mocanu et al., 2018)
and ERK (Evci et al., 2020a) are two advanced layer-wise sparsities introduced from the field of
graph theory with strong results.
Non-Random Pruning. Instead of pre-choosing a sparsity ratio for each layer, many works utilize
the proposed saliency criteria to learn the layer-wise sparsity ratios before training, also termed
as pruning at initialization (PaI). Lee et al. (2019) first introduced SNIP that chooses structurally
important connections at initialization via the proposed connection sensitivity. Following SNIP,
many efficient criteria have been proposed to improve the performance of non-random pruning
at initialization, including but not limited to gradient flow (GraSP; Wang et al. (2020)), synaptic
strengths (SynFlow; Tanaka et al. (2020)), neural tangent kernel (Liu & Zenke, 2020), and iterative
SNIP (de Jorge et al., 2021; Verdenius et al., 2020). Su et al. (2020); Frankle et al. (2021) uncovered
that the existing PaI methods hardly exploit any information from the training data and are very
robust to mask shuffling, whereas magnitude pruning after training learns both, reflecting a broader
challenge inherent to pruning at initialization.
2.2	Dynamic Sparse Training
In contrast to static sparse training, dynamic sparse training stems from randomly initialized sparse
subnetworks, and meanwhile dynamically explores new sparse connectivity during training. Dynamic
sparse training starts from Sparse Evolutionary Training (SET) (Mocanu et al., 2018; Liu et al., 2020a)
which initializes the sparse connectivity with Erdos-Renyi (Erdos & Renyi, 1959) topology and Peri-
odically explores the sparse connectivity via a prune-and-grow scheme during the course of training.
While there exist numerous pruning criteria in the literature, simple magnitude pruning typically
performs well in the field of dynamic sparse training. On the other hand, the criteria used to grow
weights back differs from method to method, including randomness (Mocanu et al., 2018; Mostafa &
Wang, 2019), momentum (Dettmers & Zettlemoyer, 2019), gradient (Evci et al., 2020a; Jayakumar
et al., 2020; Liu et al., 2021b). Besides the prune-and-grow scheme, layer-wise sparsities are vital
to achieving high accuracy. (Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019) reallocates
weights across layers during training based on reasonable heuristics, demonstrating performance
improvement. Evci et al. (2020a) extended ER to CNNs and showed considerable performance
gains to sparse CNN training with the Erdoos-Reenyi-Kernel (ERK) ratio. Very recently, Liu et al.
(2021a) started from a subnetwork at a smaller sparsity and gradually pruned it the target sparsity
during training. The denser initial subnetwork provides a larger exploration space for DST at the
early training phase and thus leads to a performance improvement, especially for extreme sparsities.
Even though dynamic sparse training achieves promising sparse training performance, it changes the
sparse connectivity during training and is thus out of the scope of random pruning.
While prior works have observed that random pruning can be more competitive in certain cases (Mo-
canu et al., 2018; Liu et al., 2020b; Su et al., 2020; Frankle et al., 2021), they did not give principled
guidelines on when and how it can become that good; nor do they show it can match the performance
of dense networks on ImageNet. Standing on the shoulders of those giants, our work summarized
principles by more extensive and rigorous studies, and demonstrate the strongest result so far, that
randomly pruned sparse Wide ResNet-50 can be sparsely trained to outperform a dense Wide ResNet-
50, on ImageNet. Moreover, compared with the ad-hoc sparsity ratios used in (Su et al., 2020), we
show that ERK (Evci et al., 2020a) and our modified ERK+ are more generally applicable sparsity
ratios that consistently demonstrate competitive performance without careful layer-wise sparsity
design for every architecture. Specifically, ERK+ ratio achieves similar accuracy with the dense Wide
ResNet-50 on ImageNet while being data free, feedforward free, and dense initialization free.
3	Methodology
We conduct extensive experiments to systematically evaluate the performance of random pruning.
The experimental settings are described below.
3.1	Layer-wise Sparsities Ratios
Denote sl as the sparsity of layer l. Random pruning, namely, removes weights or filters in each layer
randomly to the target sparsity sl. Different from works that learn layer-wise sparsities and model
3
Published as a conference paper at ICLR 2022
weights together during training using iterative global pruning techniques (Frankle & Carbin, 2019),
soft threshold (Kusupati et al., 2020), and dynamic reparameterization (Mostafa & Wang, 2019),
etc., the layer-wise sparsities of random pruning is pre-defined before training. Many pre-defined
layer-wise sparsity ratios in the literature are suited for random pruning, while they may not initially
be designed for random pruning. We choose the following 6 layer-wise sparsity ratios to study. SNIP
ratio and GraSP ratio are two layer-wise ratios that we borrow from PaI.
ERK. Introduced by Mocanu et al. (2018), Erdos-Renyi (ER) SParsifies the Multilayer Perceptron
(MLP) with a random topology in which larger layers are allocated with higher sparsity then smaller
layers. Evci et al. (2020a) further proposed a convolutional variant (ErdoS-Renyi-KerneI (ERK)) that
takes the convolutional kernel into consideration. Specifically, the sparsity of the convolutional layer
is scaled proportional to 1 一 nl_1+nl×wl+hl where nl refers to the number of neurons/channels in
layer l; wl and hl are the corresponding width and height.
ERK+. We modify ERK by forcing the last fully-connected layer as dense if it is not, while keeping
the overall parameter count the same. Doing so improves the test accuracy of Wide ResNet-50 on
ImageNet1 as shown in Appendix F.
Uniform. Each layer is pruned with the same pruning rate so that the pruned network ends up with a
uniform sparsity distribution, e.g., Zhu & Gupta (2017).
Uniform+. Instead of using a completely uniform sparsity ratio, Gale et al. (2019) keep the first
convolutional layer dense and maintain at least 20% parameters in the last fully-connected layer.
SNIP ratio. SNIP is a PaI method that selects important weights based on the connection sensitivity
score |g w|, where w and g is the network weight and gradient, respectively. The weights with the
lowest scores in one iteration are pruned before training. While not initially designed for random
pruning, We adjust SNIP for random pruning by only keeping its layer-wise sparsity ratios, while
discarding its mask positions. New mask positions (non-zero elements) are re-sampled through a
uniform distribution 〜Uniform(0,1) with a probability of 1 一 Sl. Such layer-wise sparsities are
obtained in a (slightly) data-driven way, yet very efficiently before training without any weight update.
The SNIP ratio is then treated as another pre-defined (i.e., before the training starts) sampling ratio
for random pruning.
GraSP ratio. GraSP is another state-of-the-art method seeking to pruning at initialization. Specif-
ically, GraSP removes weights that have the least effect on the gradient norm based on the score
of -W Θ Hg, where H is the Hessian matrix and g is the gradient. Same with SNIP, we keep the
layer-wise sparsity ratios of GraSP and re-sample its mask positions.
3.2	Experimental Settings
Table 1: Summary of the architectures, datasets and hyperparameters used in this paper.
Model	Mode	Data	#Epoch	Batch Size	LR	Momentum	LR Decay, Epoch	Weight Decay
ResNets	Dense	CIFAR-10/100	160	128	0.1	0.9	10×, [80, 120]	0.0005
	Sparse	CIFAR-10/100	160	128	0.1	0.9	10×, [80, 120]	0.0005
Wide ResNets	Dense	ImageNet	90	192*4	0.4	0.9	10×, [30, 60, 80]	0.0001
	Sparse	ImageNet	100	192*4	0.4	0.9	10×, [30, 60, 90]	0.0001
Architectures and Datasets. Our main experiments are conducted with the CIFAR version of
ResNet (He et al., 2016) with varying depths and widths on CIFAR-10/100 (Krizhevsky et al., 2009),
the batch normalization version of VGG (Simonyan & Zisserman, 2014) with varying depths on
CIFAR-10/100, and the ImageNet version of ResNet and Wide ResNet-50 (Zagoruyko & Komodakis,
2016) on ImageNet (Deng et al., 2009). For ImageNet, we follow the common setting in sparse
training (Dettmers & Zettlemoyer, 2019; Evci et al., 2020b; Liu et al., 2021b) and train sparse models
for 100 epochs. All models are trained with stochastic gradient descent (SGD) with momentum. We
share the summary of the architectures, datasets, and hyperparameters in Table 1.
Measurement Metrics. In most pruning literature, test accuracy is the core quality that researchers
consider. However, the evaluation of other perspectives is also important for academia and industry
1For datasets with fewer classes, e.g, MNIST, CIFAR-10, ERK+ scales back to ERK as the last fully-
connected layer is already dense.
4
Published as a conference paper at ICLR 2022
Figure 1: From shallow to deep. Test accuracy of training randomly pruned subnetworks from
scratch with different depth on CIFAR-10. ResNet-A refers to a ResNet model with A layers in total.
Figure 2: From narrow to wide. Test accuracy of training randomly pruned subnetworks from
scratch with different width on CIFAR-10. ResNet-A-B refers to a ResNet model with A layers in
total and B filters in the first convolutional layer.
before replacing dense networks with sparse networks on a large scope. Therefore, we thoroughly
evaluate sparse training with random pruning from a broader perspective. Specifically, we assess
random pruning from perspectives of OoD performance (Hendrycks et al., 2021), adversarial robust-
ness (Goodfellow et al., 2014), and uncertainty estimation (Lakshminarayanan et al., 2016). See
Appendix A for full details of the measurements used in this work.
4	Experimental Results on CIFAR
In this section, we report the results of sparse training with random pruning on CIFAR-10/100 under
various evaluation measurements. For each measurement, the trade-off between sparsity and the
corresponding metric is reported. Moreover, we also alter the depth and width of the model to check
how the performance alters as the model size alters. The results are averaged over 3 runs. We report
the results on CIFAR-10 of ResNet in the main body of this paper. The results on CIFAR-100 of
5
Published as a conference paper at ICLR 2022
ResNet, and CIFAR-10/100 of VGG are shown in Appendix D and Appendix E, respectively. Unless
otherwise stated, all the results are qualitatively similar.
4.1	Predictive Accuracy of Random Pruning on CIFAR- 1 0
We first demonstrate the performance of random pruning on the most common metric - test accuracy.
To avoid overlapping of multiple curves, we share the results of GraSP in Appendix C. The main
observations are as follows:
①	Performance of random pruning improves with the size of the network. We vary the depth
and width of ResNet and report the test accuracy in Figure 1. When operating on small networks,
e.g., ResNet-20 and ResNet-32, we can hardly find matching subnetworks even at mild sparsities, i.e.,
10%, 20%. With larger networks, e.g., ResNet-56 and ResNet-110, random pruning can match the
dense performance at 60% 〜70% sparsity. Similar behavior can also be observed when We increase
the width of ResNet-20 in Figure 2. See Appendix B for results on deeper models.
②	The performance difference between different pruning methods becomes indistinct as the
model size increases. Even though uniform sparsities fail to match the accuracy achieved by non-
uniform sparsities (ERK and SNIP) with small models, their test accuracy raises to a comparable
level as non-uniform sparsities with large models, e.g., ResNet-110 and ResNet-20-56.
③	Random pruning with ERK even outperforms pruning with the well-versed methods (SNIP,
GraSP). Without using any information, e.g., gradient and magnitude, training a randomly pruned
subnetwork with ERK topology leads to expressive accuracy, even better than the ones trained with
the delicately designed sparsity ratios, i.e., SNIP, and GraSP (shown in Appendix C).
4.2	Broader Evaluation of Random Pruning on CIFAR- 1 0
In general, sparse training with random pruning achieves quite strong results on CIFAR-10 in terms
of uncertainty estimation, OoD robustness, and adversarial robustness without any fancy techniques.
We summary main observations as below:
①	Randomly pruned networks enjoy better uncertainty estimation than their dense counter-
parts. Figure 3 shows that randomly pruned ResNet-20 matches or even improves the uncertainty
estimation of dense networks with a full range of sparsities. ECE of random pruning grows with the
model size, in line with the finding of dense networks in Guo et al. (2017). Still, random pruning
is capable of sampling matching subnetworks with high sparsities (e.g., 80%) except for the largest
model, ResNet-110. Results of NLL are presented in Appendix G.2, where randomly pruned networks
also match NLL of dense networks at extremely high sparsities.
②	Random pruning produces extremely sparse yet robust subnetworks on OoD. Figure 4 plots
the results of networks trained on CIFAR-10, tested on CIFAR-100. The results tested on SVHN
are reported in Appendix G.1. Large network sizes significantly improve the OoD performance of
random pruning. As the model size increases, random pruning can match the OoD performance of
dense networks with only 20% parameters. Again, SNIP ratio and ERK outperform uniform sparsities
in this setting.
③	Random pruning improves the adversarial robustness of large models. The adversarial
robustness of large models (e.g., ResNet-56 and ResNet-110) improves significantly with mild
sparsities in Appendix G.3. One explanation here is that, while achieving high clean accuracy,
these over-parameterized large models are highly overfitted on CIFAR-10, and thus suffer from poor
performance on adversarial examples (shown by Tsipras et al. (2019); Zhang et al. (2019) as well).
Sparsity induced by random pruning serves as a cheap type of regularization which possibly mitigates
this overfitting problem.
5	Experimental Results On ImageNet
We have learned from Section 4 that larger networks result in stronger random subnetworks on
CIFAR-10/100. We are also interested in how far we can go with random pruning on ImageNet (Deng
et al., 2009), a non-saturated dataset on which deep neural networks are less over-parameterized than
on CIFAR-10/100. In this section, we provide a large-scale experiment on ImageNet with various
ResNets from ResNet-18 to ResNet-101 and Wide ResNet-50.
6
Published as a conference paper at ICLR 2022
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6
Sparsity	Sparsity
ResNet-44
Sparsity
ResNet-56
0.045
0.040
0.7 0.8 0.9
ResNet-IlO
0.035
0.030
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity	Sparsity
—ERK —Uniform — Uniform+ —SNIP Ratio —Dense

Figure 3: Uncertainty estimation (ECE). The experiments are conducted with various models on
CIFAR-10. Lower ECE values represent better uncertainty estimation.
ResNet-20	ResNet-32	ResNet-44
0.87
Sparsity
ResNet-56
[0.86
ɔ
∂ 0.85
0C
0.84
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity	Sparsity
—ERK — Uniform —Uniform+ —SNIP Ratio —Dense
Figure 4:	Out-of-distribution performance (ROC-AUC). Experiments are conducted with models
trained on CIFAR-10, tested on CIFAR-100. Higher ROC-AUC refers to better OoD performance.
Same with CIFAR, we evaluate sparse training with random pruning from various perspectives,
including the predictive accuracy, OoD detection, adversarial robustness, and uncertainty estimation.
We choose SNIP and ERK+ sparsity ratios for Wide ResNet-50, and SNIP ratios for the rest of the
architectures. The sparsity of randomly pruned subnetworks is set as [0.7, 0.5, 0.3]. We first show the
trade-off between the parameter count and the test accuracy for all architectures in Figure 5-top-left.
Moreover, to better understand the computational benefits brought by sparsity, we report the trade-off
between test FLOPs and each measurement metric in the rest of Figure 5.
Predictive Accuracy. The parameter count-accuracy trade-off and the FLOPs-accuracy trade-off
is reported in Figure 5-top-left and Figure 5-top-middle, respectively. Overall, we observe a very
similar pattern as the results of CIFAR reported in Section 4. On smaller models like ResNet-18
and ResNet-34, random pruning can not find matching subnetworks. When the model size gets
considerably larger (ResNet-101 and Wide ResNet-50), the test accuracy of random pruning quickly
improves and matches the corresponding dense models (not only the small-dense models) at 3θ% 〜
50% sparsities. While the performance difference between SNIP ratio and ERK on CIFAR-10/100 is
7
Published as a conference paper at ICLR 2022
Accuracy (#Param)
#Parameter Count [106]
OoD Performance
ι
5
Adversarial Robustness
SNIP Ratio ResNet-18
—SNIP Ratio ResNet-34
SNIP Ratio ResNet-50 --- ERK+ Wide ResNet-50
#FLOPS [109]
Uncertainty Estimation (ECE)
#FLOPS [109]
— SNIP Ratio ResNet-IOl
—SNIP Ratio Wide ResNet-50
1
4
3
2
1
1
1
O
→ i
d i
N
Uncertainty Estimation (NLL)
* Dense ResNet-IOl
★ Dense Wide ResNet-50
Dense ResNet-18
★ Dense ResNet-34
Dense ResNet-50
Figure 5:	Summary of evaluation on ImageNet. Various Evaluation of ResNets on ImageNet,
including predictive accuracy on the original ImageNet, adversarial robustness with FGSM, OoD
performance on ImageNet-O, and uncertainty (ECE and NLL). The sparsity of randomly pruned
subnetworks is set as [0.7, 0.5, 0.3] from left to right for each line.
vague, SNIP ratio consistently outperforms ERK+ with Wide Resnet-50 on ImageNet with the same
number of parameters (see Appendix F for more details). Besides, we observe that random pruning
receives increasingly larger efficiency gains (the difference between x-axis values of sparse models
and dense models) with the increased model size on ImageNet.
While in Figure 5-top-left, a randomly pruned Wide ResNet-50 with SNIP ratios (purple line)
can easily outperform the dense ResNet-50 (blue star) by 2% accuracy with the same number of
parameters, the former requires twice as much FLOPs as the latter in Figure 5-top-middle. This
observation highlights the importance of reporting the required FLOPs together with the sparsity
when comparing two pruning methods.
Broader Evaluation of Random Pruning on ImageNet. As shown in the rest of Figure 5, the
performance of the broader evaluation on ImageNet is extremely similar to the test accuracy. Random
pruning can not discover matching subnetworks with small models. However, as the model size
increases, it receives large performance gains in terms of other important evaluations, including
uncertainty estimation, OoD detection performance, and adversarial robustness.
5.1	Understanding Random Pruning via Gradient Flow
The performance gap between the ERK and SNIP ratio on ImageNet raises the question - What
benefits do the layer-wise sparsities of SNIP provide to sparse training? Since SNIP takes gradients
into account when determines the layer-wise sparsities, we turn to gradient flow in the hope of
finding some insights on this question. The form of gradient norm we choose is the effective gradient
flow (Tessera et al., 2021) which only calculates the gradient norm of active weights. We measure the
gradient norm of sparse networks generated by SNIP and ERK2 during the early training phase. The
results are depicted in Figure 6.
Overall, we see that the SNIP ratio indeed brings benefits to the gradient norm at the initial stage
compared with ERK. Considering only SNIP (green lines) and ERK (orange lines), the ones with
higher gradient norm at the beginning always achieve higher final accuracy on both CIFAR-10 and
ImageNet. This result is on par with prior work (Wang et al., 2020), which claims that increasing the
gradient norm at the beginning likely leads to higher accuracy. However, this observation does not
2The gradient norm patterns of ERK and ERK+ are too similar to distinguish. After specifically zooming in
the initial training phase, we find that the gradient norm of ERK+ is slightly higher than ERK.
8
Published as a conference paper at ICLR 2022
Gradient Norm	Gradient Norm
ResNet-56, Sparsity=50%
ResNet-IlO, Sparsity=50%
-ERK, Top-I Acc = 93.46%
——SNIP, Top-I Acc = 93.27%
---Dense1Top-I Acc = 93.34%
O 5000 IOOOO 15000 20000
0.100
0.075
0.050
0.025
0.000
Wide ResNet-50, Sparsity=85%	Wide ResNet-50, Sparsity=70% Wide ResNet-50, Sparsity=60%
Figure 6: Gradient norm of randomly pruned networks during training. Top: Comparison
between gradient norm of SNIP and ERK with 50% sparse ResNet-20, ResNet-56, and ResNet-110
on CIFAR-10. Bottom: Comparison between gradient norm of SNIP and ERK with Wide ResNet-50
on ImageNet at various sparsities.
hold for dense Wide ResNet-50 on ImageNet (blue lines). Even though dense models have a lower
gradient norm than SNIP, they consistently outperform SNIP in accuracy by a large margin.
Instead of focusing on the initial phase, we note that the gradient norm early in the training (the flat
phase after gradient norm drops) could better understand the behaviour of networks trained under
different scenarios. We empirically find that final performance gaps between sparse networks and
dense networks is highly correlated with gaps of gradient norm early in training. Training with
small networks (e.g., ResNet-20) on CIFAR-10 results in large performance and gradient norm gaps
between sparse networks and dense networks, whereas these two gaps simultaneously vanish when
trained with large networks, e.g., ResNet-56 and ResNet-110. Similar behavior can be observed in
Wide ResNet-50 on ImageNet in terms of sparsity. Both the performance and gradient norm gaps
decrease gradually as the sparsity level drops from 85% to 60%. This actually makes sense, since
large networks (either large model size or large number of parameters) are likely to still be over-
parameterized after pruning, so that all pruning methods (including random pruning) can preserve
gradient flow along with test accuracy equally well.
Our findings suggest that only considering gradient flow at initialization might not be sufficient for
sparse training. More efforts should be invested to study the properties that sparse network training
misses after initialization, especially the early stage of training (Liu et al., 2021a). Techniques
that change the sparse pattern during training (Mocanu et al., 2018; Evci et al., 2020a) and weight
rewinding (Frankle et al., 2020; Renda et al., 2020) could serve as good starting points.
6	Conclusion
In this work, We systematically revisit the underrated baseline of sparse training - random pruning.
Our results highlight a counter-intuitive finding, that is, training a randomly pruned network from
scratch without any delicate pruning criteria can be quite performant. With proper network sizes and
layer-wise sparsity ratios, random pruning can match the performance of dense networks even at
extreme sparsities. Impressively, a randomly pruned subnetwork of Wide ResNet-50 can be trained
to outperforming a strong benchmark, dense Wide ResNet-50 on ImageNet. Moreover, training with
random pruning intrinsically brings significant benefits to other desirable aspects, such as out-of-
distribution detection, uncertainty estimation and adversarial robustness. Our paper indicates that in
addition to appealing performance, large models also enjoy strong robustness to pruning. Even if we
pruning with complete randomness, large models can preserve their performance well.
9
Published as a conference paper at ICLR 2022
7	Reproducibility
We have shared the architectures, datasets, and hyperparameters used in this paper in Section 3.2.
Besides, the different measurements and metrics used in this paper are shared in Appendix A. We
have released our code at https://github.com/VITA-Group/Random_Pruning.
8	Acknowledgement
This work have been done when Shiwei Liu worked as an intern at JD Explore Academy. This work
is partially supported by Science and Technology Innovation 2030 - “Brain Science and Brain-like
Research” Major Project (No. 2021ZD0201402 and No. 2021ZD0201405). Z. Wang is in part
supported by the NSF AI Institute for Foundations of Machine Learning (IFML).
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on in-and out-
distribution improves explainability. In European Conference on Computer Vision, pp. 228-245.
Springer, 2020.
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=BJ_wN01C-.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Dkebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural
information processing systems, 33:15834-15846, 2020a.
Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity
in vision transformers: An end-to-end exploration. Advances in Neural Information Processing
Systems, 34, 2021a.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training in
computer vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 16306-16316, 2021b.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-
supervised models are strong semi-supervised learners. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 22243-22255. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf.
10
Published as a conference paper at ICLR 2022
Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. Earlybert:
Efficient bert training via early-bird lottery tickets. In Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), pp. 2195-2207, 2021c.
Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational
information bottleneck. In International Conference on Machine Learning, pp. 1135-1144. PMLR,
2018.
PaU de Jorge, Amartya SanyaL Harkirat BehL Philip Torr, Gregory Rogez, and PUneet K. Dokania.
Progressive skeletonization: Trimming more fat from a network at initialization. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=9GsFOUyUPi.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Tim Dettmers and LUke Zettlemoyer. Sparse networks from scratch: Faster training withoUt losing
performance. arXiv preprint arXiv:1907.04840, 2019.
Alexey Dosovitskiy, LUcas Beyer, Alexander Kolesnikov, Dirk Weissenborn, XiaohUa Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil HoUlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=YicbFdNTTy.
Paul Erdos and Alfred Renyi. On random graphs i. Publicationes Mathematicae (Debrecen), 6:
290-297, 1959.
UtkU Evci, Trevor Gale, Jacob Menick, Pablo SamUel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pp. 2943-2952.
PMLR, 2020a.
UtkU Evci, Yani A IoannoU, Cem Keskin, and Yann DaUphin. Gradient flow in sparse neUral networks
and how lottery tickets win. arXiv preprint arXiv:2010.03533, 2020b.
William FedUs, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. Advances in Neural Information Processing Systems.,
2021. arXiv:2101.03961.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neUral networks. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJl-b3RcF7.
Jonathan Frankle, Gintare Karolina DziUgaite, Daniel Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning,
pp. 3259-3269. PMLR, 2020.
Jonathan Frankle, Gintare Karolina DziUgaite, Daniel Roy, and Michael Carbin. PrUning neUral
networks at initialization: Why are we missing the mark? In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Ig-VyQc-MLK.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volUme 1. Springer series in statistics New York, 2001.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neUral networks. arXiv
preprint arXiv:1902.09574, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2014.
11
Published as a conference paper at ICLR 2022
Shupeng Gui, Haotao Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, and Ji Liu. Model
compression with adversarial robustness: A unified optimization framework. Advances in Neural
Information Processing Systems, 32, 2019.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. Sparse dnns with improved adver-
sarial robustness. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
4c5bde74a8f110656874902f07378009-Paper.pdf.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. International Conference on Learning
Representations, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. Morgan Kaufmann, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.
In Proceedings of the IEEE international conference on computer vision, pp. 1389-1397, 2017.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 41-50,
2019.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. International Conference on Learning Representations, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, ICCV., 2021.
Ting-Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang. Triple wins: Boosting ac-
curacy, robustness and efficiency together by enabling input-adaptive inference. arXiv preprint
arXiv:2002.10025, 2020.
Steven A Janowsky. Pruning versus clipping in neural networks. Physical Review A, 39(12):6600,
1989.
Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Top-k
always sparse training. Advances in Neural Information Processing Systems, 33, 2020.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn TUnyaSUVUnakooL Russ Bates, AUgUStm Zidek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
Jeremy Kepner and Ryan Robinett. Radix-net: StrUctUred sparse matrices for deep neUral networks.
In 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),
pp. 268-274. IEEE, 2019.
Alex KrizheVsky, Geoffrey Hinton, et al. Learning mUltiple layers of featUres from tiny images. 2009.
Aditya KUsUpati, ViVek RamanUjan, RaghaV Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade,
and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International
Conference on Machine Learning, pp. 5544-5555. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, pp. 598-605, 1990.
Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for
the magnitude-based pruning. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=H6ATjJ0TKdf.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=B1VZqjAcYX.
Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei, and
Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artificial neurons on
commodity hardware. Neural Computing and Applications, 2020a.
Shiwei Liu, TT van der Lee, Anil Yaman, Zahra Atashgahi, D Ferrar, Ghada Sokar, Mykola Pech-
enizkiy, and DC Mocanu. Topological insights into sparse neural networks. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, ECMLPKDD, 2020b.
Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola
Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Sparse training via boosting
pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems
(NeurIPs)., 2021a.
Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need
dense over-parameterization? in-time over-parameterization in sparse training. In Proceedings of
the 39th International Conference on Machine Learning, pp. 6989-7000. PMLR, 2021b.
Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer.
In International Conference on Machine Learning, pp. 6336-6347. PMLR, 2020.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l_0 regularization. International Conference on Learning Representations, 2018.
Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression using determinantal
point processes. In International Conference on Learning Representations, 2016.
Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they
don’t know. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ByxGkySKwH.
Decebal Constantin Mocanu, Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu, and Antonio
Liotta. A topological insight into restricted boltzmann machines. Machine Learning, 104(2):
243-270, Sep 2016. ISSN 1573-0565. doi: 10.1007/s10994-016-5570-z. URL https://doi.
org/10.1007/s10994-016-5570-z.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity
inspired by network science. Nature communications, 9(1):2383, 2018.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. International Conference on Learning Represen-
tations, 2016.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. In International Conference on Machine Learning, pp.
4646-4655. PMLR, 2019.
Michael C Mozer and Paul Smolensky. Using relevance to reduce network size automatically.
Connection Science, 1(1):3-16, 1989.
13
Published as a conference paper at ICLR 2022
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/
housenumbers/nips2011_housenumbers.pdf.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role of
over-parametrization in generalization of neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=BygfghAcYX.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJC2SzZCW.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium
on security and privacy (EuroS&P), pp. 372-387. IEEE, 2016.
Ameya Prabhu, Girish Varma, and Anoop Namboodiri. Deep expander networks: Efficient deep
networks from graph theory. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 20-35, 2018.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL
http://jmlr.org/papers/v21/20-074.html.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in
neural network pruning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=S1gSj0NKvB.
Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by
fine-tuning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 20378-20389. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
eae15aabaa768ae4a5993a8a4f4fa6e4- Paper.pdf.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556. ICLR., 2014.
Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee.
Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural
Information Processing Systems. arXiv:2009.11094, 2020.
Xavier Suau, Luca Zappella, and Nicholas Apostoloff. NETWORK COMPRESSION USING
CORRELATION ANALYSIS OF LAYER RESPONSES, 2019. URL https://openreview.
net/forum?id=rkl42iA5t7.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. Advances in Neural Information Process-
ing Systems. arXiv:2006.05467, 2020.
Kale-ab Tessera, Sara Hooker, and Benjamin Rosman. Keep the gradients flowing: Using gradient
flow to study sparse network optimization. arXiv preprint arXiv:2102.01670, 2021.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-
bustness may be at odds with accuracy. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=SyxAb30cY7.
Stijn Verdenius, Maarten Stol, and Patrick Forre. Pruning via iterative ranking of sensitivity statistics.
arXiv preprint arXiv:2006.00896, 2020.
14
Published as a conference paper at ICLR 2022
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkgsACVKPH.
Cihang Xie and Alan Yuille. Intriguing properties of adversarial training at scale. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=HyxJhCEFDS.
Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou,
Kaisheng Ma, Yanzhi Wang, and Xue Lin. Adversarial robustness vs. model compression, or both?
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 111-120,
2019.
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Zhangyang Wang,
Richard G Baraniuk, and Yingyan Lin. Drawing early-bird tickets: Towards more efficient training
of deep networks. In International Conference on Learning Representations 2020 (ICLR 2020),
2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC), pp. 87.1-87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.
30.87. URL https://dx.doi.org/10.5244/C.30.87.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482. PMLR, 2019.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=H1BLjgZCb.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model
compression. arXiv preprint arXiv:1710.01878, 2017.
15
Published as a conference paper at ICLR 2022
A	Measurements and Metrics
Formally, let’s denote a (sparse) network f : X → Y trained on samples from distribution D. f(x, θ)
and f(x, θ m) refers to the dense network and the pruned network, respectively. The test accuracy
of the pruned subnetworks is typically reported as their accuracy on test queries drawn from D, i.e.,
P(χ,y)〜D (f (x, θ Θ m) = y). In addition to the test accuracy, We also evaluate random pruning from
the perspective of adversarial robustness (Goodfellow et al., 2014), OoD performance (Hendrycks
et al., 2021), and uncertainty estimation (Lakshminarayanan et al., 2016).
Adversarial Robustness. Despite of the remarkable ability solving classification problems, the
predictions of deep neural netWorks can often be fooled by small adversarial perturbations (Szegedy
et al., 2013; Papernot et al., 2016). Prior Works have shoWn the possibility of finding the sWeet point
of sparsity and adversarial robustness (Guo et al., 2018; Ye et al., 2019; Gui et al., 2019; Hu et al.,
2020). As the arguably most naive method of inducing sparsity, We are also interested in if training a
randomly pruned subnetWork can improve the adversarial robustness of deep netWorks. We folloW
the classical method proposed in GoodfelloW et al. (2014) and generate adversarial examples With
Fast Gradient Sign Method (FGSM). Specifically, input data is perturbed with esign(VχL(θ, x, y)),
where e refers to the perturbation strength, which is chosen as 蔡 in our paper.
Out-of-distribution performance. The investigation of out-of-distribution (OoD) generalization is
of importance for machine learning in both academic and industry fields. Since the i.i.d. assumption
can hardly be satisfied, especially those high-risk scenarios such as healthcare, and military. We
evaluate whether random pruning brings benefits to OoD. Following the classic routines (Augustin
et al., 2020; Meinke & Hein, 2020), SVHN (Netzer et al., 2011), CIFAR-100, and CIFAR-10 with
random Gaussian noise (Hein et al., 2019) are adopted for models trained on CIFAR-10; ImageNet-O
as the OoD dataset for models trained on ImageNet.
Uncertainty estimation. In the security-critical scenarios, e.g., self-driving, the classifiers not only
must be accurate but also should indicate when they are likely to be incorrect (Guo et al., 2017). To
test effects of the induced sparsity on uncertainty estimation, we choose two widely-used metrics,
expected calibration error (ECE) (Guo et al., 2017) and negative log likelihood (NLL) (Friedman
et al., 2001).
B	Predictive Accuracy of ResNet with Varying Width
Figure 7: Test accuracy of training randomly pruned ResNet-20, ResNet-32, and ResNet-44 from
scratch with ERK ratios when model width varies on CIFAR-10.
16
Published as a conference paper at ICLR 2022
C Predictive Accuracy of ResNet on CIFAR- 1 0 Including GraSP
To avoid multiple lines overlap with each other, we report the results of GraSP ratio on CIFAR-10
separately in this Appendix. It is surprising to find that random pruning with GraSP ratio (red lines)
consistently have lower accuracy than ERK and SNIP, except for extremely high sparsites, i.e., 0.8
and 0.9.
Figure 8: From shallow to deep. Test accuracy of training randomly pruned subnetworks from
scratch with different depth on CIFAR-10. ResNet-A refers to a ResNet model with A layers in total.
Figure 9: From narrow to wide. Test accuracy of training randomly pruned subnetworks from
scratch with different width on CIFAR-10. ResNet-A-B refers to a ResNet model with A layers in
total and B filters in the first convolutional layer.
17
Published as a conference paper at ICLR 2022
D Predictive Accuracy of ResNet on CIFAR- 1 00
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity	Sparsity
ERK —Uniform — Uniform+ — SNIP Ratio —Dense
Figure 10: From shallow to deep. Test accuracy of training randomly pruned subnetworks from
scratch with different depth on CIFAR-100. ResNet-A refers to a ResNet model with A layers in
total.
Ooo
6 5 4
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity	Sparsity	Sparsity
—ERK —Uniform —Uniform+ -SNIP Ratio —Dense
Figure 11:	From narrow to wide. Test accuracy of training randomly pruned subnetworks from
scratch with different width on CIFAR-100. ResNet-A-B refers to a ResNet model with A layers in
total and B filters in the first convolutional layer.
18
Published as a conference paper at ICLR 2022
E Predictive Accuracy of VGG on CIFAR-10/100
94.0
93.5
93.0
VGG-13
92.75
94.00
93.75
93.50
l 93.25
93.00
VGG-16
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
-S
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity	Sparsity
-→- ERK -→- SNIP Ratio -→- Dense
Figure 12:	From shallow to deep. Test accuracy of training randomly pruned VGGs from scratch
with different depth on CIFAR-10.
Figure 13: From shallow to deep. Test accuracy of training randomly pruned VGGs from scratch
with different depth on CIFAR-100.
19
Published as a conference paper at ICLR 2022
F Comparing Random Pruning with Its Dense Equivalents with
Various Ratios.
ERK τ- SNIP Ratio τ- ERK+
Figure 14: Test accuracy of Wide ResNet-50 on ImageNet. Left: Performance of ERK with
various sparsity of the last fully-connected layer. We vary the sparsity of the last fully-connected
layer while keeping the overall sparsity fixed as 70%. Results of the original ERK are indicated with
dashed red lines. Right: Performance comparison between randomly pruned Wide ResNet-50 and the
corresponding dense equivalents with a similar parameter count.
ERK+. We demonstrate the performance improvement caused by ERK+ on ImageNet. As mentioned
earlier, ERK naturally allocates higher sparsities to the larger layers while allocating lower sparsities
to the smaller ones. As a consequence, the last fully-connected layer is very likely to be dense for the
datasets with only a few classes, e.g., CIFAR-10. However, for the datasets with a larger number of
classes, e.g., ImageNet, the fully-connected layer is usually sparse. We empirically find that allocating
more parameters to the last fully-connected layer while keeping the overall parameter count fixed
leads to higher accuracy for ERK with Wide ResNet-50 on ImageNet.
We vary the last layer’s sparsity of ERK while maintaining the overall sparsity fixed and report the
test accuracy achieved by the corresponding sparse Wide ResNet-50 on ImageNet in Figure 14-left.
We can observe that the test accuracy consistently increases as the last layer’s sparsity decreases from
0.8 to 0. Consequently, we keep the last fully-connected layer of ERK dense for ImageNet and term
this modified variant as ERK+.
Compare random pruning with its dense equivalents. To draw a more solid conclusion, we train
large, randomly pruned Wide ResNet-50 on ImageNet and compare it to the dense equivalents with
the same number of parameters on ImageNet. As shown in Figure14-right, all randomly pruned
networks outperform the dense ResNet-34 with the same number of parameters. ERK+ consistently
achieves higher accuracy than ERK, even closely approaching the strong baseline - dense ResNet-50.
More interestingly, the layer-wise sparsities discovered by SNIP boost the accuracy of sparse Wide
ResNet-50 over dense ResNet-50, highlighting the importance of layer-wise sparsity ratios on sparse
training . Given the fact that the performance gap between SNIP ratio and ERK on CIFAR-10 is
somehow vague, our results highlight the necessity of evaluating any proposed pruning methods with
large-scale models and datasets, e.g., ResNet-50 on ImageNet.
20
Published as a conference paper at ICLR 2022
G B roader Evaluation of Random Pruning on CIFAR- 1 0
G. 1 Out-of-distribution performance (ROC-AUC) .
u⊃<60tf
0.850
0.925
0.900
0.875
ResNet-20
0.950]
0.950
0.925
0.900
0.875
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
ResNet-IlO
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
ResNet-56
ERK	Uniform
0.96
0.94
0.92
0.90
0.88
Uniform+
Figure 15: Out-of-distribution performance (ROC-AUC). The experiments are conducted with
various models trained on CIFAR-10, tested on SVHN. Higher ROC-AUC refers to better OoD
performance.
21
Published as a conference paper at ICLR 2022
G.2 Uncertainty Estimation (NLL)
0.34
0.32
0.30
0.28
0.26
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
Sparsity
ERK —Uniform — Uniform+ -→- SNIP Ratio -→- Dense
Figure 16: Uncertainty estimation (NLL). The experiments are conducted with various models on
CIFAR-10. Lower NLL values represent better uncertainty estimation.
G.3 Adversarial Robustness
ResNet-20
ResNet-32
ResNet-44
Sparsity	Sparsity
ERK —Uniform — Uniform+ —SNIP Ratio -→- Dense
Figure 17: Adversarial robustness. The experiments are conducted with various models on CIFAR-
10. Higher values represent better adversarial robustness.
22