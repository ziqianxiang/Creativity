Published as a conference paper at ICLR 2022
On the Learning and Learnability of Quasimetrics
Tongzhou Wang
MIT CSAIL
Phillip Isola
MIT CSAIL
Ab stract
Our world is full of asymmetries. Gravity and wind can make reaching a place
easier than coming back. Social artifacts such as genealogy charts and citation
graphs are inherently directed. In reinforcement learning and control, optimal
goal-reaching strategies are rarely reversible (symmetrical). Distance functions
supported on these asymmetrical structures are called quasimetrics. Despite their
common appearance, little research has been done on the learning of quasimet-
rics. Our theoretical analysis reveals that a common class of learning algorithms,
including unconstrained multilayer perceptrons (MLPs), provably fails to learn
a quasimetric consistent with training data. In contrast, our proposed Poisson
Quasimetric Embedding (PQE) is the first quasimetric learning formulation that
both is learnable with gradient-based optimization and enjoys strong performance
guarantees. Experiments on random graphs, social graphs, and offline Q-learning
demonstrate its effectiveness over many common baselines.
Project Page:	ssnl.github.io/quasimetric.
Code:	github.com/SsnL/poisson_quasimetric_embedding.
1	Introduction
Learned symmetrical metrics have been proven useful for innumerable tasks including dimensionality
reduction (Tenenbaum et al., 2000), clustering (Xing et al., 2002), classification (Weinberger et al.,
2006; Hoffer & Ailon, 2015), and information retrieval (Wang et al., 2014). However, the real world
is largely asymmetrical, and symmetrical metrics can only capture a small fraction of it.
Generalizing metrics, quasimetrics (Defn. 2.1) allow for asymmetrical distances and can be found in
a wide range of domains (see Fig. 1). Ubiquitous physical forces, such as gravity and wind, as well as
human-defined rules, such as one-way roads, make the traveling time between places a quasimetric.
Furthermore, many of our social artifacts are directed graphs— genealogy charts, follow-relation on
Twitter (Leskovec & Krevl, 2014), citation graphs (Price, 2011), hyperlinks over the Internet, etc.
Shortest paths on these graphs naturally induce quasimetric spaces. In fact, we can generalize to
Markov Decision Processes (MDPs) and observe that optimal goal-reaching plan costs (i.e., universal
value/Q-functions (Schaul et al., 2015; Sutton et al., 2011)) always form a quasimetric (Bertsekas &
Tsitsiklis, 1991; Tian et al., 2020). Moving onto more abstract structures, quasimetrics can also be
found as expected hitting times in Markov chains, and as conditional Shannon entropy H(∙ | ∙) in
information theory. (See the appendix for proofs and discussions of these quasimetrics.)
In this work, we study the task of quasimetric learning. Given a sampled training set of pairs
and their quasimetric distances, We ask: how well can we learn a quasimetric that fits the training
data? We define quasimetric learning in analogy to metric learning: whereas metric learning is the
problem of learning a metric function, quasimetric learning is the problem of learning a quasimetric
function. This may involve searching over a hypothesis space constrained to only include quasimetric
functions (which is what our method does) or it could involve searching for approximately quasimetric
functions (we compare to and analyze such approaches). Successful formulations have many potential
applications, such as structural priors in reinforcement learning (Schaul et al., 2015; Tian et al., 2020),
graph learning (Rizi et al., 2018) and causal relation learning (Balashankar & Subramanian, 2021).
Towards this goal, our contributions are
•	We study the quasimetric learning task with two goals: (1) fitting training data well and (2)
respecting quasimetric constraints (Sec. 3);
1
Published as a conference paper at ICLR 2022
QuaSimetricS
Shortest Paths on
Directed Graphs ,
Metrics
Euclidean
Distance
Distance
Any Normed Space J
H χ-y ∏y
Time to Target Location
Under Gravity
Conditional Entropy
H(' | ') Optimal Goal-Reaching
Plan Costs in MD
GridWorld with One-way Doors
Figure 1: Examples of quasimetric spaces. The car drawing is borrowed from Sutton & Barto (2018).
•	We prove that a large family of algorithms, including unconstrained networks trained in the
Neural Tangent Kernel (NTK) regime (Jacot et al., 2018), fail at this task, while a learned
embedding into a latent quasimetric space can potentially succeed (Sec. 4);
•	We propose Poisson Quasimetric Embeddings (PQEs), the first quasimetric embedding formula-
tion learnable with gradient-based optimization that also enjoys strong theoretical guarantees on
approximating arbitrary quasimetrics (Sec. 5);
•	Our experiments complement the theory and demonstrate the benefits of PQEs on random graphs,
social graphs and offline Q-learning (Sec. 6).
2	Preliminaries on Quasimetrics and Poisson Processes
Quasimetric space is a generalization of metric space where all requirements of metrics are satisfied,
except that the distances can be asymmetrical.
Definition 2.1 (Quasimetric Space). A quasimetric space is a pair (X , d), where X is a set of
points and d : X × X → [0, ∞] is the quasimetric, satisfying the following conditions:
∀x,y ∈ X, X = y ^⇒ d(x,y) = 0, (Identity of Indiscernibles)
∀x, y, z ∈ X,	d(x, y) + d(y, z) ≥ d(x, z).	(Triangle Inequality)
Being asymmetric, quasimetrics are often thought of as (shortest-path) distances of some (possibly
infinite) weighted directed graph. A natural way to quantify the complexity of a quasimetric is to
consider that of its underlying graph. Quasimetric treewidth is an instantiation of this idea.
Definition 2.2 (Treewidth of Quasimetric Spaces (Memoli et al., 2018)). Consider a quasimetric
space M as shortest-path distances on a positively-weighted directed graph. Treewidth of M is the
minimum over all such graphs’ treewidths.
Poisson processes are commonly used to model events (or points) randomly occurring across a set A
(Kingman, 2005) , e.g., raindrops hitting a windshield, photons captured by a camera. The number
of such events within a subset of A is modeled as a Poisson distribution, whose mean is given by a
measure μ of A that determines how “frequently the events happen at each location”.
Definition 2.3 (Poisson Process). For nonatomic measure μ on set A, a Poisson process on A with
mean measure μ is a random countable subset P ⊂ A (i.e., the random events / points) such that
•	for any disjoint measurable subsets A1, . . . , An of A, the random variables N(A1), . . . , N(An)
are independent, where N(B) , #{P ∩ B} is the number of points of P in B, and
•	N(B) has the Poisson distribution with mean μ(B), denoted as Pois(μ(B)).
Fact 2.4 (Differentiability ofP [N(A1) ≤ N(A2)]). For two measurable subsets A1, A2,
P [N(Ai) ≤ N(A2)]=叫pois(μ(Aι \ A2)) ≤ P0is(μ(A2 \ Ai))].	⑴
S-----------------{z-----------------}
two independent Poissons
Furthermore, for independent X 〜Pois(μ1), Y 〜Pois(μ2), the probability P [X ≤ Y] is differen-
tiable w.r.t. μ1 and μ2. In the special case where μ1 or μ2 is zero, we can simply compute
P [x ≤Y] = {P [X1Y]]=P [X=0]=e-μι	if μ=0	(POiS⑼is always0)
=exp (-(μι - μ2)+) ,	(2)
where x+，max(0, x). For general μ1, μ2, this probability and its gradients can be obtained via a
connection to noncentral χ2 distribution (Johnson, 1959). We derive the formulas in the appendix.
Therefore, if A1 and A2 are parametrized by some θ such that μ(A1 \ A2) and μ(A2 \ A1) are
differentiable w.r.t. θ, so is P N(Ai) ≤ N(A2)].
2
Published as a conference paper at ICLR 2022
Euclidean Space Embedding
(Training ∕mse = 58.83 ± 0.00)
0
10	20	30
Poisson Quasimetric Embedding
(Training EMSE = 0.02 ± 0.07)
ιoo
80
60
40
20
0
0	10	20	30
Figure 2: Quasimetric learning on a 3-element space. Leftmost: Training set contains all pairs except
for (a, c). Arrow labels show quasimetric distances (rather than edge weights). A quasimetric d
should predict d(a, C) ∈ [28,30]. Right three: Different formulations are trained to fit training pairs
distances, and then predict on the test pair. Plots show distribution of the prediction over 100 runs.
3	Quasimetric Learning
Consider a quasimetric space (X, d). The quasimetric learning task aims to infer a quasimetric from
observing a training set {(xi, yi, d(xi, yi))}i ⊂ X × X × [0, ∞]. Naturally, our goals for a learned
predictor d: X ×X → R are: respecting the quasimetric COnStraintS and fitting training distances.
Crucially, we are not simply aiming for the usual sense of generalization, i.e., low population error.
Knowing that true distances have a quasimetric structure, we can better evaluate predictors and desire
ones that fit the training data and are (approximately) quasimetrics. These objectives also indirectly
capture generalization because a predictor failing either requirement must have large error on some
pairs, whose true distances follow quasimetric constraints. We formalize this relation in Thm. 4.3.
3.1	Learning Algorithms and Hypothesis Spaces
Ideally, the learning should scale well with data, potentially generalize to unseen samples, and support
integration with other deep learning systems (e.g., via differentiation).
Relaxed hypothesis spaces. One can simply learn a generic function approximator that maps the
(concatenated) input pair to a scalar as the prediction of the pair’s distance, or its transformed version
(e.g., log distance). This approach has been adopted in learning graph distances (Rizi et al., 2018) and
plan costs in MDPs (Tian et al., 2020). When the function approximator is a deep neural network, we
refer to such methods as unconstrained networks. While they are known to fit training data well (Jacot
et al., 2018), in this paper we also investigate whether they learn to be (approximately) quasimetrics.
Restricted hypothesis spaces. Alternatively, we can encode each input to a latent space Z, where a
latent quasimetric dz gives the distance prediction. This guarantees learning a quasimetric over data
space X. Often dz is restricted to a subset unable to approximate all quasimetrics, i.e., an overly
restricted hypothesis space, such as metric embeddings and the recently proposed DeepNorm and
WideNorm (Pitis et al., 2020). While our proposed Poisson Quasimetric Embedding (PQE) (specified
in Sec. 5) is also a latent quasimetric, it can approximate arbitrary quasimetrics (and is differentiable).
PQE thus searches in a space that approximates all quasimetrics and only quasimetrics.
3.2	A Toy Example
To build up intuition on how various algorithms perform according to our two goals, we consider a
toy quasimetric space with only 3 elements in Fig. 2. The space has a total of 9 pairs, 8 of which
form the training set. Due to quasimetric requirements (esp. triangle inequality), knowing distances
of these 8 pairs restricts valid values for the heldout pair to a particular range (which is [28, 31] in
this case). If a model approximates 8 training pairs well and respects quasimetric constraints well, its
prediction on that heldout pair should fall into this range.
We train three models w.r.t. mean squared error (MSE) over the training set using gradient descent:
•	Unconstrained deep network that predicts distance,
•	Metric embedding into a latent Euclidean space with a deep encoder,
•	Quasimetric embedding into a latent PQE space with a deep encoder (our method from Sec. 5).
The three approaches exhibit interesting qualitative differences. Euclidean embedding, unable to
model asymmetries in training data, fails to attain a low training error. While both other methods
approximate training distances well, unconstrained networks greatly violate quasimetric constraints;
only PQEs respect the constraints and consistently predicts within the valid range.
3
Published as a conference paper at ICLR 2022
Here, the structural prior of embedding into a quasimetric latent space appears important to successful
learning. Without any such prior, unconstrained networks fail badly. In the next section, we present a
rigorous theoretical study of the quasimetric learning task, which confirms this intuition.
4	Theoretical Analysis of Various Learning Algorithms
In this section, we define concrete metrics for the two quasimetric learning objectives stated above,
and present positive and negative theoretical findings for various learning algorithms.
Overview. Our analysis focuses on data-agnostic bounds, which are often of great interests in
machine learning (e.g., VC-dimension (Vapnik & Chervonenkis, 2015)). We prove a strong negative
result for a general family of learning algorithms (including unconstrained MLPs trained in NTK
regime, k-nearest neighbor, and min-norm linear regression): they can arbitrarily badly fail to fit
training data or respect quasimetric constraints (Thm. 4.6). Our informative construction reveals the
core reason of their failure. Quasimetric embeddings, however, enjoy nice properties as long as they
can approximate arbitrary quasimetrics, which motivates searching for “universal quasimetrics”. The
next section presents PQEs as such universal approximators and states their theoretical guarantees.
Assumptions. We consider quasimetric spaces (X, d) with X ⊂ Rd, finite size n = |X| < ∞, and
finite distances (i.e., d has range [0, ∞)). It allows discussing deep networks which can’t handle
infinities well. This mild assumption can be satisfied by simply capping max distances in quasimetrics.
For training, m < n2 pairs are uniformly sampled as training pairs S ⊂ X × X without replacement.
In the appendix, we provide all full proofs, further discussions of our assumptions and presented
results, as well as additional results concerning specific learning algorithms and settings.
4.1	Distortion and Violation Metrics for Quasimetric Learning
We use distortion as a measure of how well the distance is preserved, as is standard in embedding
analyses (e.g., Bourgain (1985)). In this work, we especially consider distortion over a subset of
pairs, to quantify how well a predictor d approximates distances over the training subset S.
Definition 4.1 (Distortion). Distortion of d over a subset of pairs S ⊂ X × X is disS(d) ,
^
(max(x,y)∈s,x=y d(χy))(max(x,y)∈s,x=y d^χy)), and its overall distortion is dιs(d)，disχ×χ(d).
For measuring consistency w.r.t. quasimetric constraints, we define the (quasimetric) violation metric.
Violation focuses on triangle inequality, which can often be more complex (e.g., in Fig. 2), compared
to the relatively simple non-negativity and Identity of Indiscernibles.
Definition 4.2 (Quasimetric Violation). Quasimetric violation (violation for short) of d is vio(d) ，
maxA1,A2,A3∈x 心 dA⅛A⅜ Z、, where we define 0 = 1 for notation simplicity.
3	d(A1 ,A2 )+d(A2 ,A3 )	0
Both distortion and violation are nicely agnostic to scaling. Furthermore, assuming non-negativity
and Identity of Indiscernibles, vio(d) ≥ 1 always, with equality iff d is a quasimetric.
Distortion and violation also capture generalization. Because the true distance d has optimal training
distortion (on S) and violation, a predictor d that does badly on either must also be far from truth.
Theorem 4.3 (Distortion and Violation Lower-Bound Generalization Error). For non-negative
d, dis(d) ≥ max(disS (d), vio(d)), where dis(d) captures generalization over the entire X space.
4.2	Learning Algorithms Equivariant to Orthogonal Transforms
For quasimetric space (X , d), X ⊂ Rd, we consider applying general learning algorithms by
concatenating pairs to form inputs ∈ R2d (e.g., unconstrained networks). While straightforward, this
approach means that the algorithms are generally unable to relate the same element appearing as 1st
or 2nd input. As we will show, this is sufficient for a wide family of learning algorithms to fail badly-
ones equivariant to orthogonal transforms, which we refer to as OrEq algorithms (Defn. 4.4).
For an OrEq algorithm, training on orthogonally transformed data does not affect its prediction, as
long as test data is identically transformed. Many standard learning algorithms are OrEq (Lemma 4.5).
4
Published as a conference paper at ICLR 2022
vio(d) ≥ rdxz——
d(x, y) + d(y, z)
c
. .ʌ . . . ʌ ʌ . ..
diss (d)(diss (d) + d(y, Z))
Training (——►) : d(x,z)= c, d(w,z) = 1,
d(x, y) = 1, d(y, w0) = 1.
_ ʌ,
Test (--■>) : d(y, z) = ?
d(x, y0) = 1, d(y, w) = 1.
Test (-->): d(y,z) = ?
Figure 3: Two training sets pose incompatible constraints ( ) for the test pair distance d(y, z).
With one-hot features, an orthogonal transform can exchange (*, y) — (*, y0) and (*, W) — (*, w0),
leaving the test pair (y, z) unchanged, but transforming the training pairs from one scenario to the
other. Given either training set, an OrEq algorithm must attain same training distortion and predict
identically on (y, z). For appropriate c, this implies large distortion or violation in one of these cases.
Definition 4.4 (Equivariant Learning Algorithms). Given training set D = {(zi, yi)}i ⊂ Z × Y,
where zi are inputs and yi are targets, a learning algorithm Alg produces a function Alg(D) : Z → Y
such that Alg(D)(z0) is the function’s prediction on sample z0. Consider T a set of transformations
Z → Z. Alg is equivariant to T iff for all transform T ∈ T, training set D, Alg(D) = Alg(T D) ◦ T,
where TD = {(Tz, y) : (z, y) ∈ D} is the training set with transformed inputs.
Lemma 4.5 (Examples of OrEq Algorithms). k-nearest-neighbor with Euclidean distance, MLP
trained with squared loss in NTK regime, and min-norm least-squares linear regression are OrEq.
Failure case. The algorithms treats the concatenated inputs as generic vectors. If a transform
fundamentally changes the quasimetric structure but is not fully reflected in the learned function (e.g.,
due to equivariance), learning must fail. The two training sets in Fig. 3 are sampled from two different
quasimetrics over the same 6 elements An orthogonal transform links both training sets without
affecting the test pair, which is constrained differently in two quasimetrics. An OrEq algorithm,
necessarily predicting the test pair identically seeing either training set, must thus fail on one. In the
appendix, we empirically verify that unconstrained MLPs indeed do fail on this construction.
Extending to larger quasimetric spaces, we consider graphs containing many copies of both patterns in
Fig. 3. With high probability, our sampled training set fails in the same way—the learning algorithm
can not distinguish it from another training set with different quasimetric constraints.
Theorem 4.6 (Failure of OrEq Algorithms). Let (fn)n be an arbitrary sequence of large values.
There is an infinite sequence of quasimetric spaces ((Xn, dn))n with |Xn| = n, Xn ⊂ Rn such that,
over the random training set S of size m, any OrEq algorithm must output a predictor d that satisfies
•	d fails non-negativity, or
•	max(disS(d), vio(d)) ≥ fn (i.e., d approximates training S badly or is far from a quasimetric),
with probability 1/2 - o(1), as long as S does not contain almost all pairs 1 - m/n2 = ω(n-1/3),
and does not only include few pairs m/n2 = ω(n-1/2).
Furthermore, standard NTK results show that unconstrained MLPs trained in NTK regime converge
to a function with zero training loss. By the above theorem, the limiting function is not a quasimetric
with nontrivial probability. In the appendix, we formally state this result. Despite their empirical
usages, these results suggest that unconstrained networks are likely not suited for quasimetric learning.
4.3	Quasimetric Embeddings
A quasimetric embedding consists of a mapping f from data space X to a latent quasimetric space
(Z, dz), and predicts d(x, y) , dz (f (x), f(y)). Therefore, they always respect all quasimetric
constraints and attain optimal violation of value 1, regardless of training data.
However, unlike deep networks, their distortion (approximation) properties depend on the specific
latent quasimetrics. If the latent quasimetric can generally approximate any quasimetric (with flexible
learned encoders such as deep networks), we have nice guarantees for both distortion and violation.
In the section below, we present Poisson Quasimetric Embedding (PQE) as such a latent quasimetric,
along with its theoretical distortion and violation guarantees.
5
Published as a conference paper at ICLR 2022
5	Poisson Quasimetric Embeddings (PQEs)
Motivated by above theoretical findings, we aim to find a latent quasimetric space (Rd, dz) with a
deep network encoder f : X → Rd , and a quasimetric dz that is both universal and differentiable:
•	for any data quasimetric (X, d), there exists an encoder f such that dz (f (x), f(y)) ≈ d(x, y);
•	dz is differentiable (for optimizing f and possible integration with other gradient-based systems).
Notation 5.1. We use x, y for elements of the data space X, u, v for elements of the latent space Rd,
upper-case letters for random variables, and (∙)z for indicating functions in latent space (e.g., dz).
An existing line of machine learning research learns quasipartitions, or partial orders, via Order
Embeddings (Vendrov et al., 2015). Quasipartitions are in fact special cases of quasimetrics whose
distances are restricted to be binary, denoted as π. An Order Embedding is a representation of a
quasipartition, where πOE(x, y) = 0 (i.e., x is related to y) iff f(x) ≤ f(y) coordinate-wise:
πOE(x, y) , πzOE(f(x),f(y)) , 1 - Y 1f(x)j-f(y)j≤0.	(3)
j
Order Embedding is universal and can model any quasipartition (see appendix and Hiraguchi (1951)).
Can We extend this discrete idea to general continuous quasimetrics? Quite naively, one may attempt
a straightforward soft modification of Order Embedding:
πSoftOE(U, V) , 1 - Yeχp ( -(Uj-Vj)+) = 1 -eχp (- X(Uj-Vj)+),	(4)
jj
Which equals 0 if U ≤ V coordinate-Wise, and increases to 1 as some coordinates violate this condition
more. HoWever, it is unclear Whether this gives a quasimetric.
A more principled Way is to parametrize a (scaled) distribution of latent quasipartitions Πz, Whose
expectation naturally gives a continuous-valued quasimetric:
dz(u, v; Πz, α) , α ∙ E∏,〜人[∏z(u, v)], α ≥ 0.	(5)
Poisson Quasimetric Embedding (PQE) gives a general recipe for constructing such Πz distributions
so that dz is universal and differentiable. Within this frameWork, We Will see that πzSoftOE is actually
a quasimetric based on such a distribution and is (almost) sufficient for our needs.
5.1	Distributions of Latent Quasipartitions
A random latent quasipartition πz : Rd ×Rd → {0, 1} is a difficult object to model, due to complicated
quasipartition constraints. Fortunately, the Order Embedding representation (Eq. (3)) is Without such
constraints. If, instead of fixed latents U, V, We have random latents R(U), R(V), We can compute:
Eπz[πz(U,V)] = ER(u),R(v)πzOE(R(U), R(V)) = 1 - P [R(U) ≤ R(V) coordinate-Wise] .	(6)
In this vieW, We represent a random πz via a joint distribution of random vectors1 {R(U)}u∈Rd , i.e.,
a stochastic process. To easily compute the probability of this coordinate-Wise event, We assume that
each dimension of random vectors is from an independent process, and obtain
Eπz [πz(U,V)] = 1 - YP [Rj(U) ≤ Rj (V)] .	(7)
j
The choice of stochastic process is flexible. Using Poisson processes (With Lebesgue mean measure;
Defn. 2.3) that count random points on half-lines2 (-∞, a], We can have Rj (U) = Nj ((∞, Uj]), the
(random) count of events in (∞, Uj ] from j-th Poisson process:
Enz〜∏z [∏z(u,v)] = 1 - Y P [Nj ((-∞,uj]) ≤ Nj((-∞,Vj])]	(8)
j
=1 - Y exp ( - (Uj-Vj)+) = ∏SoftOE(u,v),	(9)
j
1In general, these random vectors R(u) do not have to be of the same dimension as u ∈ Rd, although the
dimensions do match in the PQE variants We experiment With.
2Half-lines has Lebesgue measure ∞. More rigorously, consider using a small value as the loWer bounds of
these intervals, Which leads to same result.
6
Published as a conference paper at ICLR 2022
where we used Fact 2.4 and the observation that one half-line is either subset or superset of an-
other.Indeed, πzSoftOE is an expected quasipartition (and thus a quasimetric), and is differentiable.
Considering a mixture of such distributions for expressiveness, the full latent quasimetric formula is
dPQE-LH(u, v; α)，X αi ∙(1 - exp ( - X(Uij- Vi,j)+)) ,	(10)
where we slightly abuse notation and consider latents u and v as (reshaped to) 2-dimensional. We will
see that this is a special PQE case with Lebesgue measure and half-lines, and thus denoted PQE-LH.
5.2	General PQE Formulation
We can easily generalize the above idea to independent Poisson processes of general mean measures
μj and (sub)set parametrizations U → Aj(u), and obtain an expected quasipartition as:
Enz 〜∏ZQE(μ,AM(U,V)I , 1 - Y P [Nj (Aj (U)) ≤ Nj (Aj (V))]	(II)
j
=1 - YPhPOis(μj∙(Aj(U) \ Aj(V))) ≤ Pois(μj∙(Aj(v) \ Aj(u)))], (12)
j	|	{z	}
Poisson rate of points landing only in Aj (u)
which is differentiable as long as the measures and set parametrizations are (after set differences).
Similarly, considering a mixture gives us an expressive latent quasimetric.
A general PQE latent quasimetric is defined with {(μi,j,Ai,j)}i,j and weights α ≥ 0 as:
dPQE(U, v; μ, A,α) , Eai ∙ Enz〜∏PQl‰,Ai) [πz (U，v)]	(13)
=X ai(1 - Y PhPOis(〃i,j(Ai,j(u) \ Ai,j(V))) ≤ pois(μi,j(Ai,j(v) \ Ai,j(")))]}
whose optimizable parameters include {ai}i, possible ones from {(μi,j,Ai,j)}i,j (and encoder f).
This general recipe can be instantiated in many ways. Setting Ai,j (U) → (-∞, Ui,j] and Lebesgue
μi,j, recovers pQe-LH. In the appendix, we consider a form with Gaussian-based measures and
Gaussian-shapes, denoted as PQE-GG. Unlike PQE-LH, PQE-GG always gives nonzero gradients.
The appendix also includes several implementation techniques that empirically improve stability,
including learning αi ’s with deep linear networks, a formulation that outputs discounted distance, etc.
5.3	Continuous -valued Stochastic Processes
But why Poisson processes over more common choices such as Gaussian processes? It turns out that
common continuous-value processes fail to give a differentiable formula.
Consider a non-degenerate process {R(U)}u, where (R(U), R(V)) has bounded density if U 6= V.
Perturbing U → U + δ leaves P [R(U) = R(U + δ)] = 0. Then one of PR(U) ≤ R(U + δ) and
P R(U + δ) ≤ R(U) must be far away from 1 (as they sum to 1), breaking differentiability at
P [R(U) ≤ R(U)] = 1. (This argument is formalized in the appendix.) Discrete-valued processes,
however, can leave most probability mass on R(U) = R(U + δ) and thus remain differentiable.
5.4	Theoretical Guarantees
Our PQEs bear similarity with the algorithmic quasimetric embedding construction in M6moli et al.
(2018). Extending their analysis to PQEs, we obtain the following distortion and violation guarantees.
Theorem 5.2 (Distortion and violation of PQEs). Under the assumptions of Sec. 4, any quasimetric
space with size n and treewidth t admits a PQE-LH and a PQE-GG with distortion O(t lOg2 n) and
violation 1, with an expressive encoder (e.g., a ReLU network with ≥ 3 layers and polynomial width).
In fact, these guarantees apply to any PQE formulation that satisfies a mild condition. Informally, any
PQE with h × k Poisson processes (i.e., h mixtures) enjoys the above guarantees ifit can approximate
the discrete counterpart: mixtures of h Order Embeddings, each specified with k dimensions. In the
appendix, we make this condition precise and provide a full proof of the above theorem.
7
Published as a conference paper at ICLR 2022

lθ-ɪ
IOd
---PQE
Unconstrained Net.
---- Asym. Dot Product
---Metric Embedding
---DeepNorm
---WideNorm
IOd
0.0	0.2	0.4	Q.6
Training Set Fraction
(a) A dense graph.
Groundtrath
Distance Matrix
o
20 φ
ɪ
10
10-2
10-3
(C) A sparse graph With block structure.
(b) A sparse graph.
Figure 4: Comparison of PQE and baselines on quasimetric learning in random directed graphs.
6 Experiments
Our experiments are designed to (1) confirm our theoretical findings and (2) compare PQEs against a
Wider range of baselines, across different types of tasks. In all experiments, We optimize γ-discounted
distances (With γ ∈ {0.9, 0.95}), and compare the folloWing five families of methods:
•	PQEs (2 formulations): PQE-LH and PQE-GG With techniques mentioned in Sec. 5.2.
•	Unconstrained networks (20 formulations): Predict raW distance (directly, With exp
transform, and with (∙)2 transform) or γ-discounted distance (directly, and with a
sigmoid-transform). Each variant is run With a possible triangle inequality regularizer
Ex,y,z max(0, γ d(x,y)+d(y,z) - γd(x,z))2 for each of 4 weights ∈ {0, 0.3, 1, 3}.
•	Asymmetrical dot products (20 formulations): On input pair (x, y), encode each into a feature
vector with a different network, and take the dot product. Identical to unconstrained networks,
the output is used in the same 5 ways, with the same 4 triangle inequality regularizer options.
•	Metric encoders (4 formulations): Embed into Euclidean space, `1 space, hypersphere with
(scaled) spherical distance, or a mixture of all three.
•	DeepNorm (2 formulations) and WideNorm (3 formulations): Quasimetric embedding meth-
ods that often require significantly more parameters than PQES (often on the order of 106 〜107
more effective parameters; see the appendix for detailed comparisons) but can only approximate
a subset of all possible quasimetrics (Pitis et al., 2020).
We show average results from 5 runs. The appendix provides experimental details, full results
(including standard deviations), additional experiments, and ablation studies.
Random directed graphs. We start with randomly generated directed graphs of 300 nodes, with
64-dimensional node features given by randomly initialized neural networks. After training with MSE
on discounted distances, we test the models’ prediction error on the unseen pairs (i.e., generalization),
measured also by MSE on discounted distances. On three graphs with distinct structures, PQEs
significantly outperform baselines across almost all training set sizes (see Fig. 4). Notably, while
DeepNorm and WideNorm do well on the dense graph quasimetric, they struggle on the other two,
attaining both high test MSE (Fig. 4) and train MSE (not shown). This is consistent with the fact that
they can only approximate a subset of all quasimetrics, while PQEs can approximate all quasimetrics.
Large-scale social graph. We choose the Berkeley-Stanford Web Graph (Leskovec & Krevl, 2014)
as the real-wold social graph for evaluation. This graph consists of 685,230 pages as nodes, and
7,600,595 hyperlinks as directed edges. We use 128-dimensional node2vec features (Grover &
Leskovec, 2016) and the landmark method (Rizi et al., 2018) to construct a training set of 2,500,000
pairs, and a test set of 150,000 pairs. PQEs generally perform better than other methods, accurately
predicting finite distances while predicting high values for infinite distances (see Table 1). DeepNorms
and WideNorms learn finite distances less accurately here, and also do much worse than PQEs on
learning the (quasi)metric of an undirected social graph (shown in the appendix).
Offline Q-learning. Optimal goal-reaching plan costs in MDPs are quasimetrics (Bertsekas &
Tsitsiklis, 1991; Tian et al., 2020) (see also the appendix). In practice, optimizing deep Q-functions
often suffers from stability and sample efficiency issues (Henderson et al., 2018; Fujimoto et al.,
2018). As a proof of concept, we use PQEs as goal-conditional Q-functions in offline Q-learning,
on the grid-world environment with one-way doors built upon gym-minigrid (Chevalier-Boisvert
et al., 2018) (see Fig. 1 right), following the algorithm and data sampling procedure described in Tian
et al. (2020). Adding strong quasimetric structures greatly improves sample efficiency and greedy
planning success rates over popular existing approaches such as unconstrained networks used in Tian
et al. (2020) and asymmetrical dot products used in Schaul et al. (2015) (see Fig. 5). As an interesting
observation, some metric embedding formulations work comparably well.
8
Published as a conference paper at ICLR 2022
	Triangle	MSE w.r.t.	L1 Error	Prediction d
	inequality	γ-discounted	when true	when true
	regularizer	distances (× 10-3) J	d<∞J	d=∞↑
PQE-LH	X	3.043	1.626	69.942
PQE-GG	X	3.909	1.895	101.824
BeSt UnconstrainedNet.	X	3.086	2.115	59.524
	✓	2.813	2.211	61.371
Best Asym. Dot Product	X	48.106	2.520 ×1011	2.679 ×1011
	✓	48.102	2.299 ×1011	2.500 ×1011
Best Metric Embedding	X	17.595	7.540	53.850
BeSt DeepNorm	X	5.071	2.085	120.045
BeSt WideNorm	X	3.533	1.769	124.658
Table 1: Quasimetric learning on large-scale web graph.
“Best” is selected by test MSE w.r.t. γ-discounted distances.
Number OfTraining Trajectories
Figure 5: Offline Q-learning results.
7	Related Work
Metric learning. Metric learning aims to approximate a target metric/similarity function, often via
a learned embedding into a metric space. This idea has successful applications in dimensionality
reduction (Tenenbaum et al., 2000), information retrieval (Wang et al., 2014), clustering (Xing et al.,
2002), classification (Weinberger et al., 2006; Hoffer & Ailon, 2015), etc. While asymmetrical
formulations have been explored, they either ignore quasimetric constraints (Oord et al., 2018;
Logeswaran & Lee, 2018; Schaul et al., 2015), or are not general enough to approximate arbitrary
quasimetric (Balashankar & Subramanian, 2021), which is the focus of the present paper.
Isometric embeddings. Isometric (distance-preserving) embeddings is a highly influential and
well-studied topic in mathematics and statistics. Fundamental results, such as Bourgain’s random
embedding theorem (Bourgain, 1985), laid important ground work in understanding and constructing
(approximately) isometric embeddings. While most such researches concern metric spaces, M6moli
et al. (2018) study an algorithmic construction of a quasimetric embedding via basic blocks called
quasipartitions. Their approach requires knowledge of quasimetric distances between all pairs and
thus is not suitable for learning. Our formulation takes inspiration from the form of their embedding,
but is fully learnable with gradient-based optimization over a training subset.
Quasimetrics and partial orders. Partial orders (quasipartitions) are special cases of quasimetrics
(see Sec. 5). A line of machine learning research studies embedding partial order structures into latent
spaces for tasks such as relation discovery and information retrieval (Vendrov et al., 2015; Suzuki
et al., 2019; Hata et al., 2020; Ganea et al., 2018). Unfortunately, unlike PQEs, such formulations
do not straightforwardly generalize to arbitrary quasimetrics, which are more than binary relations.
Similar to PQEs, DeepNorm and WideNorm are quasimetric embedding approaches learnable with
gradient-based optimization (Pitis et al., 2020). Theoreically, they universally approximates a subset
of quasimetrics (ones induced by asymmetrical norms). Despite often using many more parameters,
they are restricted to this subset and unable to approximate general quasimetrics like PQEs do (Fig. 4).
8	Implications
In this work, we study quasimetric learning via both theoretical analysis and empirical evaluations.
Theoretically, we show strong negative results for a common family of learning algorithms, and
positive guarantees for our proposed Poisson Quasimetric Embedding (PQE). Our results introduce the
novel concept of equivariant learning algorithms, which may potentially be used for other learnability
analyses with algorithms such as deep neural networks. Additionally, a thorough average-case or
data-dependent analysis would nicely complement our results, and may shed light on conditions
where algorithms like deep networks can learn decent approximations to quasimetrics in practice.
PQEs are the first quasimetric embedding formulation that can be learned via gradient-based opti-
mization. Empirically, PQEs show promising performance in various tasks. Furthermore, PQEs are
fully differentiable, and (implicitly) enforce a quasimetric structure in any latent space. They are
particularly suited for integration in large deep learning systems, as we explore in the Q-learning
experiments. This can potentially open the gate to many practical applications such as better em-
bedding for planning with MDPs, efficient shortest path finding via learned quasimetric heuristics,
representation learning with quasimetric similarities, causal relation learning, etc.
9
Published as a conference paper at ICLR 2022
References
Noga Alon and Joel H Spencer. The probabilistic method. John Wiley & Sons, 2004.
Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Conference
on Machine Learning,pp. 146-155. PMLR, 2017.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Ananth Balashankar and Lakshminarayanan Subramanian. Learning faithful representations of
causal graphs. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pp. 839-850, 2021.
Umberto Bertele and Francesco Brioschi. On non-serial dynamic programming. J. Comb. Theory,
Ser. A, 14(2):137-148, 1973.
Dimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-
matics of Operations Research, 16(3):580-595, 1991.
Kenneth P Bogart. Maximal dimensional partially ordered sets i. hiraguchi’s theorem. Discrete
Mathematics, 5(1):21-31, 1973.
Be山 Bollobgs and Bollobgs B6la. Random graphs. Number 73. Cambridge university press, 2001.
Jean Bourgain. On lipschitz embedding of finite metric spaces in hilbert space. Israel Journal of
Mathematics, 52(1-2):46-52, 1985.
Barry Brown, James Lovato, and Kathy Russell. CDFLIB: library of fortran routines for cumulative
distribution functions, inverses, and other parameters, 1994.
Yury Brychkov. On some properties of the marcum q function. Integral Transforms and Special
Functions, 23:177-182, 03 2012. doi: 10.1080/10652469.2011.573184.
John Burkardt. C++ source code for CDFLIB. https://people.sc.fsu.edu/~jburkardt/cpp_
src/cdflib/cdflib.html, 2021.
Moses Charikar, Konstantin Makarychev, and Yury Makarychev. Directed metrics and directed graph
partitioning problems. In SODA, volume 6, pp. 51-60. Citeseer, 2006.
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym-minigrid, 2018.
Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the
cross-entropy method. Annals of operations research, 134(1):19-67, 2005.
Paul ErdoS and Alfred R6nyi. On random graphs. i. Publicationes Mathematicae Debrecen, 6:
290-297, 1959.
Stefan Felsner, Ching Man Li, and William T. Trotter. Adjacency posets of planar graphs. Discrete
Mathematics, 310(5):1097-1104, 2010. ISSN 0012-365X.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning
hierarchical embeddings. In International Conference on Machine Learning, pp. 1646-1655.
PMLR, 2018.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On the
similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580, 2020.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
855-864, 2016.
10
Published as a conference paper at ICLR 2022
Peter GrUnWald and Paul Vitdnyi. Shannon information and kolmogorov complexity. arXiv preprint
cs/0410002, 2004.
Matthieu Guillot and Gautier Stauffer. The stochastic shortest path problem: a polyhedral combina-
torics perspective. European Journal OfOperational Research, 2*5(1):148—158, 2020.
Nozomi Hata, Shizuo Kaji, Akihiro Yoshida, and Katsuki FujisaWa. Nested subspace arrangement for
representation of relational data. In International Conference on Machine Learning, pp. 4127-4137.
PMLR, 2020.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Toshio Hiraguchi. On the dimension of partially ordered sets. The science reports of the Kanazawa
University, 1(2):77-94, 1951.
Elad Hoffer and Nir Ailon. Deep metric learning using triplet netWork. In International workshop on
similarity-based pattern recognition, pp. 84-92. Springer, 2015.
Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit AgraWal, and Phillip Isola.
The loW-rank simplicity bias in deep netWorks. arXiv preprint arXiv:2103.10427, 2021.
Piotr Indyk. Algorithmic applications of loW-distortion geometric embeddings. In Proceedings 42nd
IEEE Symposium on Foundations of Computer Science, pp. 10-33. IEEE, 2001.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep netWork training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural netWorks. arXiv preprint arXiv:1806.07572, 2018.
N. L. Johnson. On an extension of the connexion betWeen poisson and χ2 distributions. Biometrika,
46(3/4):352-363, 1959. ISSN 00063444.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
John Frank Charles Kingman. Poisson processes. Encyclopedia of biostatistics, 6, 2005.
Andrei N Kolmogorov. On tables of random numbers. Sankhya： The Indian Journal of Statistics,
Series A, pp. 369-376, 1963.
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large netWork dataset collection. http:
//snap.stanford.edu/data, June 2014.
Ming Li, Paul Vitdnyi, et al. An introduction to Kolmogorov complexity and its applications, volume 3.
Springer, 2008.
Lajanugen LogesWaran and Honglak Lee. An efficient frameWork for learning sentence representa-
tions. In International Conference on Learning Representations, 2018.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent With Warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
J. I. Marcum. Table of Q Functions. RAND Corporation, Santa Monica, CA, 1950.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999.
Facundo Memoli, Anastasios Sidiropoulos, and Vijay Sridhar. Quasimetric embeddings and their
applications. Algorithmica, 80(12):3803-3824, 2018.
11
Published as a conference paper at ICLR 2022
Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel, and Bobby Bhattacharjee.
Measurement and Analysis of Online Social Networks. In Proceedings of the 5th ACM/Usenix
Internet Measurement Conference (IMC’07), San Diego, CA, October 2007.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Giacomo Ortali and Ioannis G Tollis. Multidimensional dominance drawings. arXiv preprint
arXiv:1906.09224, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8026-8037. 2019.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral universality in
deep networks. In International Conference on Artificial Intelligence and Statistics, pp. 1924-1932.
PMLR, 2018.
Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba. An inductive bias for distances: Neural
nets that respect the triangle inequality. arXiv preprint arXiv:2002.05825, 2020.
DJ de S Price. Networks of scientific papers. Princeton University Press, 2011.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779.
Fatemeh Salehi Rizi, Joerg Schloetterer, and Michael Granitzer. Shortest path distance approximation
using deep learning techniques. In 2018 IEEE/ACM International Conference on Advances in
Social Networks Analysis and Mining (ASONAM), pp. 1007-1014. IEEE, 2018.
Neil Robertson and Paul D Seymour. Graph minors. iii. planar tree-width. Journal of Combinatorial
Theory, Series B, 36(1):49-64, 1984.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In International conference on machine learning, pp. 1312-1320. PMLR, 2015.
J. G. Skellam. The frequency distribution of the difference between two poisson variates belonging
to different populations. Journal of the Royal Statistical Society. Series A (General), 109(Pt 3):
296-296, 1946.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and
Multiagent Systems-Volume 2, pp. 761-768, 2011.
Ryota Suzuki, Ryusuke Takahama, and Shun Onoda. Hyperbolic disk embeddings for directed
acyclic graphs. In International Conference on Machine Learning, pp. 6066-6075. PMLR, 2019.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, and
Sergey Levine. Model-based visual planning with self-supervised functional distances. arXiv
preprint arXiv:2012.15373, 2020.
William T Trotter. Partially ordered sets. Handbook of combinatorics, 1:433-480, 1995.
12
Published as a conference paper at ICLR 2022
Leslie G Valiant. A theory of the learnable. Communicationsofthe ACM, 27(11):1134-1142, 1984.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. In Measures of complexity, pp. 11-30. Springer, 2015.
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and
language. arXiv preprint arXiv:1511.06361, 2015.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau,
Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt,
Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric
Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,
Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris,
Anne M. Archibald, Ant6nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0
Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature
Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen,
and Ying Wu. Learning fine-grained image similarity with deep ranking. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 1386-1393, 2014.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929-9939. PMLR, 2020.
Kilian Q Weinberger, John Blitzer, and Lawrence K Saul. Distance metric learning for large margin
nearest neighbor classification. In Advances in neural information processing systems, pp. 1473-
1480, 2006.
Eric P Xing, Andrew Y Ng, Michael I Jordan, and Stuart Russell. Distance metric learning with
application to clustering with side-information. In NIPS, volume 15, pp. 12. Citeseer, 2002.
13
Published as a conference paper at ICLR 2022
Appendix
A Discussions for Sec. 2: Preliminaries on Quasimetrics and Poisson Processes	15
A.1	Quasimetric Spaces ......................................................... 15
A.2	Poisson Processes .......................................................... 18
B Proofs, Discussions and Additional Results for Sec. 4: Theoretical Analysis of Various
Learning Algorithms	20
B.1	Thm. 4.3: Distortion and Violation Lower-Bound Generalization Error ........ 20
B.2	Lemma 4.5: Examples of OrEq Algorithms ..................................... 21
B.3	Thm. 4.6: Failure of OrEq Algorithms ....................................... 23
C Proofs and Discussions for Sec. 5: Poisson Quasimetric Embeddings (PQEs)	33
C.1	Non-differentiability of Continuous-Valued Stochastic Processes ............ 33
C.2	PQE-GG: Gaussian-based Measure and Gaussian Shapes ......................... 34
C.3	Theoretical Guarantees for PQEs ............................................ 35
C.4	Implementation of Poisson Quasimetric Embeddings (PQEs) .................... 40
D Experiment Settings and Additional Results	43
D.1	Experiments from Sec. 3.2: A Toy Example ................................... 43
D.2	Experiments from Sec. 6: Experiments ....................................... 45
14
Published as a conference paper at ICLR 2022
A Discussions for Sec. 2: Preliminaries on Quasimetrics and
Poisson Processes
A.1 Quasimetric Spaces
Definition 2.1 (Quasimetric Space). A quasimetric space is a pair (X , d), where X is a set of
points and d : X × X → [0, ∞] is the quasimetric, satisfying the following conditions:
∀x, y ∈ X,	X = y ^⇒ d(x, y) = 0, (Identity of Indiscernibles)
∀x, y, z ∈ X,	d(x, y) + d(y, z) ≥ d(x, z).	(Triangle Inequality)
Definition A.1 (Quasipseudometric Space). As a further generalization, we say (X, d) is a
quasipseudometric space if the Identity of Indiscernibles requirement is only satisfied in one direction:
∀x, y ∈ X, x = y =⇒ d(x, y) = 0,	(Identity of Indiscernibles)
∀x, y, z ∈ X,	d(x, y) + d(y, z) ≥ d(x, z).	(Triangle Inequality)
A.1.1 Examples of Quasimetric Spaces
Proposition A.2 (Expected Hitting Time of a Markov Chain). Let random variables (Xt)t be a
Markov Chain with support X. Then (X, dhitting) is a quasimetric space, where
dhitting(s, t) , E [time to hit t | start from s] ,	(14)
where we define the hitting time of s starting from s to be 0.
Proof of Proposition A.2. Obviously dhitting is non-negative. We then verify the following quasimet-
ric space properties:
•	Identity of Indiscernibles. By definition, we have, ∀x, y ∈ X, x 6= y,
dhitting(x, x) = 0	(15)
dhitting(x, y) ≥ 1.	(16)
•	Triangle Inequality. For any x, y, z ∈ X, we have
dhitting(x, y) + dhitting (y, z) = E [time to hit y then hit z | start from x]	(17)
≥ E [time to hit z | start from x]	(18)
= dhitting(x, z).	(19)
Hence, (X, dotting) is a quasimetric space.	□
Proposition A.3 (Conditional Shannon Entropy). Let X be the set of random variables (of some
probability space). Then (X, dH) is a quasipseudometric space, where
dH(X,Y) ,H(Y |X).	(20)
If for all distinct (X, Y ) ∈ X × X, X can not be written as (almost surely) a deterministic function
of Y , then (X , dH ) is a quasimetric space.
Proof of Proposition A.3. Obviously dH is non-negative. We then verify the following quasipseudo-
metric space properties:
•	Identity of Indiscernibles. By definition, we have, ∀X, Y ∈ X,
dH (X, X) = H(X | X) = 0	(21)
dH(Y,X)=H(Y |X) ≥0,	(22)
where ≤ is = iffY is a (almost surely) deterministic function ofX.
•	Triangle Inequality. For any X, Y, Z ∈ X, we have
dH(X,Y)+dH(Y,Z) =H(Y | X)+H(Z | Y)	(23)
≥ H(Y | X) +H(Z | XY)	(24)
= H(YZ | X)	(25)
≥ H(Z | X)	(26)
= dH (X, Z).	(27)
15
Published as a conference paper at ICLR 2022
Hence, (X , dH) is a quasipseudometric space, and a quasimetric space when the last condition is
satisfied.	□
Conditional Kolmogorov Complexity From algorithmic information theory, the conditional Kol-
mogorov complexity K(y | x) also similarly measures “the bits needed to create y given x as
input” (Kolmogorov, 1963). It is also almost a quasimetric, but the exact definition affects some
constant/log terms that may make the quasimetric constraints non-exact. For instance, when defined
with the prefix-free version, conditional Kolmogorov complexity is always strictly positive, even
for K(x | x) > 0 (Li et al., 2008). One may remedy this with a definition using a universal Turing
machine (UTM) that simply outputs the input on empty program. But to make triangle inequality
work, one needs to reason about how the input and output parts work on the tape(s) of the UTM.
Nonetheless, regardless of the definition details, conditional Kolmogorov complexity do satisfy a
triangle inequality UP to log terms (Grunwald & Vitdnyi, 2004). So intuitively, it behaves roughly
like a quasimetric defined on the space of binary strings.
Optimal Goal-Reaching Plan Costs in Markov Decision Processes (MDPs) We define MDPs
in the standard manner: M = (S, A, R, P, γ) (Puterman, 1994), where S is the state sPace, A is the
action sPace, R: S × A → R is the reward function, P : S × A → ∆(S) is the transition function
(where ∆(S) is the set of all distributions over S), and γ ∈ (0, 1) is the discount factor.
We define Π as the collection of all stationary Policies π : S → ∆(A) on M. For a Particular Policy
π ∈ Π, it induces random trajectories:
•	Trajectory starting from state s ∈ S is the random variable
ξπ(s) = (s1, a1,r1, s2, a2, r2, . . . ),	(28)
distributed as
s1 = s	(29)
a，i 〜π(si),	∀i ≥ 1	(30)
Si+1 〜P(si,ai),	∀i ≥ 1.	(31)
•	Trajectory starting from state-action Pair (s, a) ∈ S × A is the random variable
ξπ(s,a) = (s1, a1, r1, s2, a2,r2, . . . ),	(32)
distributed as
s1 = s	(33)
a1 = a	(34)
ai 〜 π(si),	∀i ≥ 2	(35)
si+1 〜 P(si, ai),	∀i ≥ 1.	(36)
Proposition A.4 (Optimal Goal-Reaching Plan Costs in MDPs). Consider an MDP M =
(S, A, R, P, γ). WLOG, assume that R : S × A → (-∞, 0] has only non-Positive rewards (i.e.,
negated costs). Let X = S ∪ (S × A). Then (X, dsum) and (X, dγ) are quasiPseudometric sPaces,
where
dsum(x, y) , minE [total costs from x to y under π]	(37)
π∈Π
'min∏∈∏ E(sι ,aι,rι,... )=ξ∏ (x) [ - PtrtIs，星也口回" ]	if '9 = S ∈ S,
'---{z-----}	S----V------}
not reached s0 yet	goal is a state
min∏∈∏ E(sι ,αι,rι,…)=ξ∏ (χ) [ 一 Pt rt 1(s0,a0)∈{(si,ai)}i∈ I]]if y = (S0,aO) ∈S×A,
'----------V----------}	'-----------V--------}
not reached s0 and performed a0 yet	goal is a state-action pair
一	(38)
and
dγ(x, y) , logγ max E [γtotal costs from x to y under π	(39)
is defined similarly.
If the reward function is always negative, (X, dsum) and (X, dγ) are quasimetric spaces.
16
Published as a conference paper at ICLR 2022
Proof of Proposition A.4. Obviously both dsum and dγ are non-negative, and satisfy Identity of
Indiscernibles (for quasipseudometric spaces). For triangle inequality, note that for each y, we can
instead consider alternative MDPs:
•	If y = s0 ∈ S, modify the original MDP to make s0 a sink state, where performing any action
yields 0 reward (i.e., 0 cost);
•	If y = (s0 , a0) ∈ S × A, modify the original MDP such that performing action a0 in state s0
surely transitions to a new sink state, where performing any action yields 0 reward (i.e., 0 cost).
Obviously, both are Markovian. Furthermore, they are Stochastic Shortest Path problems with no
negative costs (Guillot & Stauffer, 2020), implying that there are Markovian (i.e., stationary) optimal
policies (respectively w.r.t. either minimizing expected total cost or maximizing expected γtotalcost).
Thus optimizing over the set of stationary policies, Π, gives the optimal quantity over all possible
policies, including concatenation of two stationary policies. Thus the triangle inequality is satisfied
by both.
Hence, (X , dsum) and (X , dγ) are quasipseudometric spaces.
Finally, if the reward function is always negative, x 6= y =⇒ dsum (x, y) > 0 and dγ (x, y) > 0, so
(X, dsum) and (X, dγ) are quasimetric spaces.	□
Remark A.5. We make a couple remarks:
•	Any MDP with a bounded reward function can be modified to have only non-positive rewards by
subtracting the maximum reward (or larger);
•	We have
dsum(s, (s, a)) = dγ (s, (s, a)) = -R(s, a).	(40)
•	When the dynamics is deterministic, dsum ≡ dγ, ∀γ ∈ (0, 1).
•	Unless y is reachable from x with probability 1 under some policy, dsum (x, y) = ∞.
•	Unless y is unreachable from x with probability 1 under all policies, dsum (x, y) < ∞. Therefore,
it is often favorable to consider dγ types.
•	In certain MDP formulations, the reward is stochastic and/or dependent on the reached next state.
The above definitions readily extend to those cases.
•	γ dγ ((s,a),y) is very similar to Q-functions except that Q-function applies discount based on time,
and γdγ ((s,a),y) applies discount based on costs. We note that a Q-learning-like recurrence can
also be found for γdγ ((s,a),y) .
If the cost is constant in the sense for some fixed c < 0, R(s, a) = c, ∀(s, a) ∈ S × A, then
time and cost are equivalent up to a scale. Therefore, γ dγ ((s,a),y) coincides with the optimal
Q-functions for the MDPs described in proof, and γdγ (s,y) coincides with the optimal value
functions for the respective MDPs.
A.1.2 Quasimetric Treewidth and Graph Treewidth
Definition 2.2 (Treewidth of Quasimetric Spaces (Memoli et al., 2018)). Consider representa-
tions of a quasimetric space M as shortest-path distances on a positively-weighted directed graph.
Treewidth of M is the minimum over all such graphs’ treewidths. (Recall that the treewidth of a graph
(after replacing directed edges with undirected ones) is a measure of its complexity.)
Graph treewidth is a standard complexity measure of how “similar” a graph is to a tree (Robertson
& Seymour, 1984). Informally speaking, if a graph has low treewidth, we can represent it as a tree,
preserving all connected paths between vertices, except that in each tree node, we store a small
number of vertices (from the original graph) rather than just 1.
Graph treewidth is widely used by the Theoretical Computer Science and Graph Theory communities,
since many NP problems are solvable in polynomial time for graphs with bounded treewidth (Bertele
& Brioschi, 1973).
17
Published as a conference paper at ICLR 2022
A.2 Poisson Processes
Definition 2.3 (Poisson Process). For nonatomic measure μ on set A, a Poisson process on A with
mean measure μ is a random countable subset P ⊂ A (i.e., the random events / points) such that
•	for any disjoint measurable subsets A1 , . . . , An of A, the random variables N(A1), . . . , N(An)
are independent, where N(B) , #{P ∩ B} is the number of points of P in B, and
•	N(B) has the Poisson distribution with mean μ(B), denoted as Pois(μ(B)).
Poisson processes are usually used to model events that randomly happens “with no clear pattern”,
e.g., visible stars in a patch of the sky, arrival times of Internet packages to a data center. These events
may randomly happen all over the sky / time. To an extent, we can say that their characteristic feature
is a property of statistical independence (Kingman, 2005).
To understand this, imagine raindrops hitting the windshield of a car. Suppose that we already
know that the rain is heavy, knowing the exact pattern of the raindrops hitting on the left side of the
windshield tells you little about the hitting pattern on the right side. Then, we may assume that, as
long as we look at regions that are disjoint on the windshield, the number of raindrops in each region
are independent.
This is the fundamental motivation of Poisson processes. In a sense, from this characterization,
Poisson processes are inevitable (see Sec. 1.4 of (Kingman, 2005)).
A.2.1 POISSON RACE PROBABILITY P [Pθis(μι) ≤ P0is(μ2)] AND ITS GRADIENT FORMULAS
In Fact 2.4 we made several remarks on the Poisson race probability, i.e., for independent X 〜
Pois(μι), Y 〜P0is(μ2), the quantity P [X ≤ Y]. In this section, we detailedly describe how we
arrived at those conclusions, and provide the exact gradient formulas for differentiating P [X ≤ Y]
w.r.t. μι and μ2.
From Skellam distribution CDF to Non-Central χ2 distribution CDF. Distribution of the differ-
ence of two independent Poisson random variables is called the Skellam distribution (Skellam, 1946),
with its parameter being the rate of the two Poissons. That is, X - Y 〜Skellam(μι, μ2). Therefore,
P [X ≤ Y] is essentially the cumulative distribution function (CDF) of this Skellam at 0. In Eq. (4)
of (Johnson, 1959), a connection is made between the CDF of Skellam(μ1,μ2) distribution, and the
CDF of a non-central χ2 distribution (which is a non-centered generalization of χ2 distribution) with
two parameters k > 0 degree(s) of freedom and non-centrality parameter λ ≥ 0): for integer n > 0,
P [Skellam(μ1,μ2) ≥ n] = P [NonCentralχ2 ( 2n , 2μ2 ) < 2μ4,
degree(s) of freedom non-centrality parameter
(41)
which can be evaluated using statistical computing packages such as SciPy (Virtanen et al., 2020)
and CDFLIB (Burkardt, 2021; Brown et al., 1994).
Marcum-Q-Function and gradient formulas. To differentiate through Eq. (41), we consider
representing the non-central χ2 CDF as a Marcum-Q-function (Marcum, 1950). One definition of the
Marcum-Q-function QM : R × R → R in statistics is
QM (a, b)
∞ x M-1
Jb xU exp
IM-1 (ax) dx,
(42)
where IM-1 is the modified Bessel function of order M - 1. (When M is non-integer, we refer
readers to (Brychkov, 2012; Marcum, 1950) for definitions, which are not relevant to the discussion
below.) When used in CDF of non-central χ2, we have
P [NonCentralχ2(k, λ) < x] = 1 — Q k (√λ, √x)
(43)
18
Published as a conference paper at ICLR 2022
Combining with Eq.(41), and using the symmetry Skellam(μι, μ2) == -Skellam(μ2,μ1), We have,
for integer n,
P [X ≤ Y + n] = P [Skellam(μ1,μ2) ≤ n]
ʃP [NonCentralχ2(-2n, 2μι) < 2μ2]	if n < 0
11 — P [NonCentralχ2(2(n + 1), 2μ2) < 2μ" if n ≥ 0
=ʃ 1 - Q-n(√2μi, √2μ2) if n < 0
[Qn+1 (√2μ2, √2μι)	if n ≥ 0.
Prior work (Brychkov, 2012) provides several derivative formula for the Marcum-Q-Function:
(44)
(45)
(46)
•	For n < 0, we have
∂	∂
P-PX ≤ Y + n] = 4— ( 1 - Q-n(A∕2μi, λ∕2μ2) )	(47)
∂μι	∂μι ∖
=Q-n(√2μ7, √2μ?) - Q-n+1(√2μ7, √2μ2)
(Eq. (16) of (Brychkov, 2012))
=-eTμι+μGl-n(2√μ^) (Eq. (2) of (Brychkov, 2012))
_ n
=-(μ2) 2 e-(√μl-√μ2)2I-n(2√μκ	(48)
where Iv(e) (x) , e-|x|Iv(x) is the exponentially-scaled version of Iv that computing libraries
often provide due to its superior numerical precision (e.g., SciPy (Virtanen et al., 2020)),
∂	∂
P-P [X ≤ Y + n] = p- (1 - Q-n(A∕2μi, λ∕2μ2) )	(49)
∂μ2	∂μ2 ∖
_ n+1
=(μ∣)	2 eτμι+“2kn-ι(2√μ^)
(Eq. (19) of (Brychkov, 2012))
n+1
也 y 丁
μι)
e-(√μΓ-√μ2)2 l-n-i(2√μiμ2),	(50)
•	For n ≥ 0, we have
∂∂
P [X ≤ Y + n] =	Qn+1 Cχ∕2μ2, V2μι)	(51)
∂μι	∂μι
=-(μ1) e-(μι+μ2)in(2√μ^)	(Eq. (19) of (Brychkov, 2012))
=-(μ) e-(√μ1-√μ2)2Ine)(2√μμ2),	(52)
and,
∂_______ —	.	∂ -	.,—'—.
P-PX ≤ Y + n] = ——Qn+1 Cχ∕2μ2, ʌ/2.I)	(53)
∂μ2	∂μ2
=Qn+2(√2μZ, √2μΓ) - Qn+ι(v‰, √2μΓ)
(Eq. (16) of (Brychkov, 2012))
n+1
=(μ1) 2 eTμι+μGIn+ι(2√μ^) (Eq. (2) of (Brychkov, 2012))
n+1
=(μθ 2 e-(√μr-√μ2)2In+1 (2√μiμ2).	(54)
Setting n = 0 gives the proper forward and backward formulas for P [X ≤ Y].
19
Published as a conference paper at ICLR 2022
B	Proofs, Discussions and Additional Results for Sec. 4:
Theoretical Analysis of Various Learning Algorithms
Assumptions. Recall that we assumed a quasimetric space, which is stronger than a quasipseu-
dometric space (Defn. A.1), with finite distances. These are rather mild assumptions, since any
quasipseudometric with infinities can always be modified to obey these assumptions by (1) adding
a small metric (e.g., d(x, y) , 1x6=y with small > 0) and (2) capping the infinite distances to a
large value higher than any finite distance.
Worst-case analysis. In this work we focus on the worst-case scenario, as is common in standard
(quasi)metric embedding analyses (Bourgain, 1985; Johnson & Lindenstrauss, 1984; Indyk, 2001;
Memoli et al., 2018). Such results are important because embeddings are often used as heuristics in
downstream tasks (e.g., planning) which are sensitive to any error. While our negative result readily
extends to the average-case scenario (since the error (distortion or violation) is arbitrary), we leave a
thorough average-case analysis as future work.
Data-independent bounds. We analyze possible data-independent bounds for various algorithms.
In this sense, the positive result for PQEs (Thm. C.4) is really strong, showing good guarantees
regardless data quasimetric. The negative result (Thm. 4.6) is also revealing, indicating that a family
of algorithms should probably not be used, unless we know something more about data. Data-
independent bounds are often of great interest in machine learning (e.g., concepts of VC-dimension
(Vapnik & Chervonenkis, 2015) and PAC learning (Valiant, 1984)). An important future work is to
explore data-dependent results, possibly via defining a quasimetric complexity metric that is both
friendly for machine learning analysis, and connects well with combinatorics measures such as
quasimetric treewidth.
Violation and distortion metrics. The optimal violation has value 1. Specifically, it is 1 iff d is a
quasimetric on X (assuming non-negativity). Distortion (over training set) and violation together
quantify how well d learns a quasimetric consistent with the training data. A predictor can fit training
data well (low distortion), but ignores basic quasimetric constraints on heldout data (high violation).
Conversely, a predictor can perfectly obey the training data constraints (low violation), but doesn’t
actually fit training data well (high distortion). Indeed, (assuming non-negativity and Identity of
Indiscernibles), perfect distortion (value 1) and violation (value 1) imply that d is a quasimetric
consistent with training data.
Relation with classical in-distribution generalization studies. Classical generalization studies
the prediction error over the underlying data distribution, and often involves complexity of the
hypothesis class and/or training data (Vapnik & Chervonenkis, 2015; McAllester, 1999). Our focus
on quasimetric constraints violation is, in fact, not an orthogonal problem, but potentially a core
part of in-distribution generalization for this setting. Here, the underlying distribution is supported
on all pairs of X × X . Indeed, if a learning algorithm has large distortion, it must attain large
prediction error on S ⊂ X × X; if it has large violation, it must violates the quasimetric constraints
and necessarily admits bad prediction error on some pairs (whose true distances obey the quasimetric
constraints). Thm. 4.3 (proved below) formalizes this idea, where we characterize generalization with
the distortion over all possible pairs in X × X.
B.1 Thm. 4.3: Distortion and Violation Lower-Bound Generalization Error
Theorem 4.3 (Distortion and Violation Lower-Bound Generalization Error). For non-negative
d, dis(d) ≥ max(disS (d), vio(d)), where dis(d) captures generalization over the entire X space.
B.1.1 Proof
Proof of Thm. 4.3. It is obvious that
. ,^ . , ^
dis(d) ≥ diss(d).	(55)
Therefore, it remains to show that dis(d) ≥	vio(d).
20
Published as a conference paper at ICLR 2022
WLOG, say vio(d) > 1. Otherwise, the statement is trivially true.
ɪʌ ,1 FC ∙ , ∙	l' ∙	1	, ∙	Z ɪʌ C A z-»\	1	C	_ T? ∙ , 1 G/	∖ .	ZA
By the definition	of violation	(see Defn. 4.2),	we have, for some x,	y, z ∈ X,	with d(x, z) >	0,
.	d(xz—= vio(d).
d(x, y) + d(y, z)
(56)
ɪʃ- 9Z	∖ , 9/	∖ zʌ ,1	. 1	l' , 1 l' 11
If d(x, y)	+ d(y, z) = 0, then we must have one of the following two cases:
•	If d(x,	y)	>	0 or d(y, z) > 0, the statement is true because dis(d) = ∞.
•	If d(x,	y)	=	d(y, Z) = 0, then d(x, z) = 0 and the statement is true since dis(2)	≥ d⅛⅛	= ∞.
It is sufficient to prove the case that d(x, y) + d(y, z) > 0. We can derive
T/ ∖	■ / 7^∖ I T/ ∖ , 7^∕	∖ ∖	/Lr'
d(x, z) = vio(d) d(x, y) + d(y, z)	(57)
^
≥ ʌ (d(χ, y) + d(y, Z))	(58)
dis(d)
≥ ^Sd(X,z).	(59)
_ dis(d)
If d(x, Z) = 0, then dis(d) = ∞ and the statement is trivially true.
If d(x, Z) > 0, above Eq. (59) implies
dis(∕) ≥ J , [ ≥ —=⇒ =⇒ dis(2) ≥ jvio(..	(60)
’ 一d(x,z) — dis(d)
Combining Eqs. (55) and (60) gives the desired statement.
□
B.2 LEMMA 4.5: EXAMPLES OF OrEq ALGORITHMS
Lemma 4.5 (Examples of OrEq Algorithms). k-nearest-neighbor with Euclidean distance, MLP
trained with squared loss in NTK regime, and min-norm least-squares linear regression are OrEq.
Recall the definition of Equivariant Learning Transforms.
Definition 4.4 (Equivariant Learning Algorithms). Given training set D = {(Zi, yi)}i ⊂ Z × Y,
where Zi are inputs and yi are targets, a learning algorithm Alg produces a function Alg(D) : Z → Y
such that Alg(D)(Z0) is the function’s prediction on sample Z0. Consider T a set of transformations
Z → Z. Alg is equivariant to T iff for all transform T ∈ T, training set D, Alg(D) = Alg(T D) ◦T,
where TD = {(TZ, y) : (Z, y) ∈ D} is the training set with transformed inputs.
B.2.1	PROOF
Proof of Lemma 4.5. We consider the three algorithms individually:
•	k-nearest neighbor with Euclidean distance.
It is evident that if a learning algorithm only depend on pairwise dot products (or distances), it is
equivariant to orthogonal transforms, which preserve dot products (and distances). k-nearest-
neighbor with Euclidean distance only depends on pairwise distances, which can be written in
terms of dot products:
kx - yk22 = xTx + yTy - 2xTy.	(61)
Therefore, it is equivariant to orthogonal transforms.
•	Min-norm least-squares linear regression.
Recall that the solution to min-norm least-squares linear regression Ax = b is given by
Moore-Penrose pseudo-inverse X = A+b. For any matrix A ∈ Rm×n with SVD UΣV* = A,
21
Published as a conference paper at ICLR 2022
and T ∈ O(n) (where O(n) is the orthogonal group in dimension n), we have
(AT t)+ = (U ΣV *T t)+ = TV Σ+U * = TA+,	(62)
where We used T* = TT for T ∈ O(n). The solution for the transformed data ATT and b is
thus
(ATT)+b = TA+b.	(63)
Thus, for any new data point X ∈ Rn and its transformed version Tx ∈ Rn,
(Tx)T (AT T)+b = xτT TTA+ = XA+ .	(64)
、	{z	'	1{Z}
transformed problem prediction	original problem prediction
Hence, min-norm least-squares linear regression is equivariant to orthogonal transforms.
• MLP trained with squared loss in NTK regime.
We first recall the NTK recursive formula from (Jacot et al., 2018).
Denote the NTK for a MLP with L layers with the scalar kernel Θ(L) : Rd × Rd → R. Let
β > 0 be the (fixed) parameter for the bias strength in the network model, and σ be the activation
function. Given x, z ∈ Rd, it can be recursively defined as following. For h ∈ [L],
Θ(h)(x, Z) , Θ(h-1)(x, z)Σ(h)(x, Z) + Σ(h)(x, z),	(65)
where
Σ(0)(x, Z) = 1xτz + β2,	(66)
d
Λ(h-1)	Σ(h-1)(x,x) Σ(h-1)(x, Z)	(67)
Λ (x,Z)= Σ(h-1)(Z,x)	Σ(h-1) (Z, Z) ,	(67)
Σ(h)(x, Z) = C ∙ E(u,v)〜N(0,Λ(h-i)) [σ(u)σ(v)] + β2,	(68)
ς㈤(X, Z) = C ∙ E(u,v)〜N(o,Λ(h-i)) [σ(U)σ(V)],	(69)
for some constant c.
It is evident from the recursive formula, that Θ(h) (x, Z) only depends on xTx, ZTZ and xTZ.
Therefore, the NTK is invariant to orthogonal transforms.
Furthermore, training an MLP in NTK regime is the same as kernel regression with the NTK
(Jacot et al., 2018), which has a unique solution only depending on the kernel matrix on training
set, denoted as Ktrain ∈ Rn×n , where n is the training set size. Specifically, for training data
{(xi, yi)}i∈[n], the solution fN*TK : R → R can be written as
f*TK(X) = (θ(L)(x,xι) Θ(L)(x,x2)… Θ(L)(x,xn)) K-a1ny,	(70)
where y = (y1 y2 . . . yn ) is the vector of training labels.
Consider any orthogonal transform T ∈ O(d), and the NTK regression trained on the transformed
data {(TXi, yi)}i∈[n] . Denote the solution as fN*TK,T : R → R. As we have shown, Kt-ra1in is
invariant to such transforms, and remains the same. Therefore,
fNτκ,τ(Tx) = (Θ(L)(Tx,Txi) Θ(L)(TX,Tx2)…Θ(L)(TX,Txn)) K-^y	(71)
=(Θ(L) (x, Xi) Θ(L)(X,X2)…Θ(L)(X,Xn)) K-a1ny	(72)
= fN*Tκ(X).	(73)
Hence, MLPs trained (with squared loss) in NTK regime is equivariant to orthogonal transforms.
Furthermore, we note that there are many variants of MLP NTK formulas depending on details
such as the particular initialization scheme and bias settings. However, they usually only lead
to slight changes that do not affect our results. For example, while the above recursive NTK
formula are derived assuming that the bias terms are initialized with a normal distribution (Jacot
et al., 2018), the formulas for initializing bias as zeros (Geifman et al., 2020) does not affect the
dependency only on dot product, and thus our results still hold true.
These cases conclude the proof.
□
22
Published as a conference paper at ICLR 2022
c
≥ ~	~一	ʌ7 ^-
diss(d)(diss(d) + d(y, Z))
vio(d) ≥ ʌ	d(x,z)—
d(x, y) + d(y, z)
Training (——►) : d(x,z)= c, d(w,z) = 1,
d(x, y) = 1, d(y, w0) = 1.
_ ʌ,
Test (--■>) : d(y, z) = ?
d(x, y0) = 1, d(y, w) = 1.
Test (-->): d(y,z) = ?
Figure 6: Two training sets pose incompatible constraints ( ) for the test pair distance d(y, z).
With one-hot features, an orthogonal transform can exchange (*, y) — (*, y0) and (*, W) — (*, w0),
leaving the test pair (y, z) unchanged, but transforming the training pairs from one scenario to the
other. Given either training set, an OrEq algorithm must attain same training distortion and predict
identically on (y, z). For appropriate c, this implies large distortion or violation in one of these cases.
B.3 THM. 4.6: FAILURE OF OrEq ALGORITHMS
We start with a more precise statement of Thm. 4.6 that takes into consideration divergent m/n2 :
Theorem 4.6 (Failure of OrEq Algorithms). Let (fn)n be an arbitrary sequence of large values.
There is an infinite sequence of quasimetric spaces ((Xn, dn))n with |Xn| = n, Xn ⊂ Rn such that,
over the random training set S of size m, any OrEq algorithm must output a predictor d that satisfies
•	d fails non-negativity, or
•	max(disS(d), vio(d)) ≥ fn (i.e., d approximates training S badly or is far from a quasimetric),
with probability 1/2 - o(1), as long as S does not contain almost all pairs 1 - m/n2 = ω(n-1/3),
and does not only include few pairs m/n2 = ω(n-1/2).
Recall that the little-Omega notation means f = ω(g) ^⇒ g = o(f).
B.3.1	Proof
Proof strategy. In our proof below, we will extend the construction discussed in Sec. 4.2 to large
quasimetric spaces (reproduced here as Fig. 6). To do so, we
1.	Construct large quasimetric spaces containing many copies of the (potentially failing)
structure in Fig. 6, where we can consider training sets of certain properties such that
•	we can pair up such training sets,
•	an algorithm equivariant to orthogonal transforms must fail on one of them,
•	for each pair, the two training sets has equal probability of being sampled;
Then, it remains to show that with probability 1 - o(1) we end up with a training set of such
properties.
2.	Consider sampling training set as individually collecting each pair with a certain probability
p, and carefully analyze the conditions to sample a training set with the special properties
with high probability 1 - o(1).
3.	Extend to fixed-size training sets and show that, under similar conditions, we sample a
training set with the special properties with high probability 1 - o(1).
In the discussion below and the proof, we will freely speak of infinite distances between two elements
of X , but really mean a very large value (possibly finite). This allows us to make the argument clearer
and less verbose. Therefore, we are not restricting the applicable settings of Thm. 4.6 to quasimetrics
with (or without) infinite distances.
In Sec. 4.2, we showed how orthogonal-transform-equivariant algorithms can not predict d(y, z)
differently for the two particular quasimetric spaces and their training sets shown in Fig. 6.
But are these the only bad training sets? Before the proof, let us consider what kinds of training
sets are bad for these two quasimetric spaces. Consider the quasimetrics dleft and dright over X ,
23
Published as a conference paper at ICLR 2022
{x, y, y0 , z, w, w0 }, with distances as shown in the left and right parts of Fig. 6, where we assume
that the unlabeled pairs have infinite distances except in the left Pattern d(x, w0) ≤ 2, and in the both
Patterns d(y, Z) has some appropriate value consistent with the respective triangle inequality.
Specifically, we ask:
•	For what training sets Sle任 ⊂ X ×X can we interchange y - y0 and W - w0 on 2nd input to
obtain a valid training set for dright, regardless of c?
•	For what training sets Sright ⊂ X ×X can we interchange y - y0 and W - w0 on 2nd input to
obtain a valid training set for dleft, regardless of c?
Note that if Sleft (or Sright) satisfies its condition, the predictor d from an algorithm equivariant
to orthogonal transforms must (1) predict d(y, z) identically and (2) attain the same training set
distortion on it and its transformed training set. As we will see in the proof for Thm. 4.6, this implies
large distortion or violation for appropriate c.
Intuitively, all we need is that the transformed data do not break quasimetric constraints. However, its
conditions are actually nontrivial as we want to set c to arbitrary:
•	We can’t have (x, W) ∈ Sright because it would be transformed into (x, W) which has
dleft(x, W) ≤ 2. Then dright(x, W) ≤ 2 and then restricts the possible values of c due to tri-
angle inequality with dright(W, z) = 1. For similar reasons, we can’t have (x, W0) ∈ Sleft. In fact,
we can’t have a path of finite total distance from x to W (or W0) in Sright (or Sleft).
•	We can not have (y0, y0) ∈ S(.)(which has distance 0), which would get transformed into (y0, y)
with distance 0, which (on the right pattern) would restrict the possible values of c due to triangle
inequality, and break our assumption of d(.)not being a quasipseudometric. For similar reasons
(W0, W0), and cycles containing y0 or W0 with finite total distance, should be avoided.
We note that having (y, y) or (W, W) would also break the non-quasipseudometric assumptions,
and thus should avoid them as well (although cycles are okay here since they do not restrict
values of c). In fact, with metrics more friendly to zero distances (than distortion and violation,
which are based on distance ratios), it might be possible to allow them and obtain better bounds
in the second-moment argument below in the proof for Thm. 4.6.
•	Similarly, we can,t have (y, y0), (y0, y), (w, w0), or (w0, W) in S(.), as they will be mapped to
(y, y),(y0, y0), (W, W), or (W0, W0).
With these understandings of the pattern shown in Fig. 6, we are ready to discuss the constructed
quasimetric space and training sets.
Proof of Thm. 4.6. Our proof follows the outline listed above.
1.	Construct large quasimetric spaces containing many copies of the (potentially failing)
structure in Fig. 6.
For any n > 0, consider the following quasimetric space (Xn, dn) of size n, with one-hot
features. WLOG, assume n = 12k is a multiple of 12. If it is not, set at most 11 elements
to have infinite distance with every other node. This won’t affect the asymptotics. Let the
n = 12k elements of the space be
X {xleft xleft xright xright Wleft Wleft Wright Wright
n = x1 , . . . , xk , x1 , . . . , xk ,W1 , . . . , Wk , W1 , . . . , Wk ,
y1left,...,ykleft, y1right,...,ykright,W10left,...,Wk0left,Wk0r+igh1t,...,W20rkight,
y10left, . . . , yk0left,yk0r+ig1ht, . . .y20rkight,z1, . . . , zk,	zk+1, . . . , z2k},	(74)
24
Published as a conference paper at ICLR 2022
with quasimetric distances, ∀i, j ,
dn (xieft, zj) =	dn(xriight, zj)	=	c	(75)
dn (wieft, zj) =	dn(wiright, zj)	=	1	(76)
dn(xieft, yieft) =	dn(xriight, yiright)	=	1	(77)
dn(yieft, wileft) =	dn(yiright, wiright)	=1	(78)
dn(xieft, wileft) =2	(79)
dn (yiet, zj) = c	(80)
dn(yiright, zj) = 2,	(81)
where subscripts are colored to better show when they are the same (or different), unlisted
distances are infinite (except that dn(u, u) = 0, ∀u ∈ X). Essentially, we equally divide
the 12k nodes into 6 “types”, {x, y, w, z, w0, y0}, corresponding to the 6 nodes from Fig. 6,
where each type has half of its nodes corresponding to the left Pattern (of Fig. 6), and the
other half corresponding to the right pattern, except for the Z types.
Furthermore,
•	Among the Ieft-Pattem nodes, each set with the same subscript are bundled together in
the sense that Xieft only has finite distance to y：f which only has finite distance to Wileft
(instead of other yjleft’s or wk0left’s). However, since distance to/from yieft and wieft are
infinite anyways, we can pair
(xieft, yieft, wileft, yjleft, wleft, zh)	(82)
for any i,j, l, h, to obtain a left pattern.
•	Among the right-pattern nodes, each set with the same subscript are bundled together
in the sense that xriight only has finite distance to yiright, and yjright which only has finite
distance to wjright (instead of other yj0right’s or wkright’s). However, since are distances are
infinite anyways, we can pair
(xriight, yiright, yjright, wjright, wlright, zh)	(83)
for any i,j,l,h, to obtain a right pattern.
We can see that (X, d) indeed satisfies all quasimetric space requirements (Defn. 2.1),
including triangle inequalities (e.g., by, for each (a, b) with finite distance dn (a, b) < ∞,
enumerating finite-length paths from a to b).
Now consider the sampled training set S.
•	We say S is bad on a left Pattern specified by ileft,jleft,lleft, hleft, if
s ⊃ {(χieft, ZhleJ(Xieeft, yieft),碉;,Wift), Weft-)}	削)
0 = S ∩ {(yieft ,Zhleft), (yieft, yieft), (WIeft, W 吗,(yj岩,yjf), (Wift, Wift),
(Xleft W0left)	( left 0left)	(Wleft W0left)	( 0left left)	(W0left Wleft)}	(85)
(Xileft, Wileft ),	(yileft, yjleft ),	(Wlleft, Wileft ),	(yjleft , yileft),	(Wileft , Wlleft)}	(85)
• We say S is bad on a right Pattern specified by *ght, jright,lright, hright, if
S ⊃ {(Xright Z ) (Xright	0right) ( 0right Wright) (Wright Z )}
S ⊃ {(Xiright , Zhright), (Xiright , yiright ), (yjright , Wjright), (Wjright, Zhright)}
0 = S ∩ {(yjrirgighhtt
( right
(Xiright
Z ) (yright yright) (Wright Wright) (y0right y0right) (W0right
, hright , jright , jright , jright ,	jright , iright , iright , lright
(86)
Wlrriigghhtt),
W0right) ( right 0right)	(Wright W0right)	( 0right right)	(W0right	Wright)}
, Wjright ), (yjright , yiright ),	(Wjright , Wlright ),	(yiright ,	yjright ),	(Wlright ,	Wjright )}
(87)
Most importantly,
• If S is bad on a left Pattern specified by ileft, jleft,lleft, hleft, consider the orthogonal
transform that interchanges y/ft - yjeft and Wleft - Wileft on 2nd input. In S, the
25
Published as a conference paper at ICLR 2022
possible transformed pairs are					
d(xielefftt,yielefftt)	1	-→	d(xielefftt,yjleefftt)	= 1,	(known in S)
d(yielefftt,Willeefftt)	1	-→	d(yielefftt,Wlelefftt)	= 1,	(known in S)
d(u, yielefftt)	∞	-→	d(u, yjleefftt)	= ∞,	(poissble in S for some u 6= xieft)
d(u, yjleefftt)	∞	-→	d(u, yielefftt)	= ∞,	(poissble in S for some u)
d(u, Willeefftt )	∞	-→	d(u, Wlelefftt)	= ∞,	
				(poissble in S for some u ∈/ {xielefftt, yielefftt})	
d(u, Wlelefftt)	∞	-→	d(u,Willeefftt)	= ∞.	(poissble in S for some u)
The crucial observation is that the transformed training set just look like one sampled
from a quasimetric space where
-	the quasimetric space has one less set of left-pattern elements,
-	the quasimetric space has one more set of right-pattern elements, and
-	transformed training set is bad on that extra right Pattern (given by the extra set of
right-pattern elements),
which can be easily verified by comparing the transformed training set with the require-
ments in Eqs. (86) and (87).
• Similarly, if S is bad on a right pattern specified by i=ght,jright, lright, hright, consider
the orthogonal transform that interchanges yj'ght - yiright and Wjight - W nght on 2nd
input. In S the possible transformed pairs are
d(xriirgighhtt,yirriigghhtt)	1	-→	d(xriirgighhtt, yjrirgighhtt) = 1,	(known in S)
d(yjrirgighhtt, Wjrirgighhtt)	1	-→	d(yjrirgighhtt,Wlrriigghhtt) = 1,	(known in S)
d(u, yjrirgighhtt)	∞	-→	d(u, yirriigghht t) =∞,	(poissble in S for some u)
d(u, yirriigghht t)	∞	-→	d(u, yjrirgighhtt) =∞, (poissble in S for some u 6= xriight)	
d(u,Wlrriigghhtt)	∞	-→	d(u, Wjrirgighhtt) =∞,	(poissble in S for some u)
d(u, Wjrirgighhtt)	∞	-→	d(u,Wlrriigghhtt) = ∞. (poissble in S for some u ∈/ {xright, yright}) iright , jright	
Again, the crucial observation is that the transformed training set just look like one
sampled from a quasimetric space where
-	the quasimetric space has one less set of right-pattern elements,
-	the quasimetric space has one more set of Ieft-Pattem elements, and
-	transformed training set is bad on that extra left Pattern (given by the extra set of
Ieft-Pattern elements),
which can be easily verified by comparing the transformed training set with the require-
ments in Eqs. (84) and (85).
Therefore, when S is bad on both a lef Pattern and a right Pattern (necessarily on disjoint
sets of pairs), we consider the following orthogonal transform composed of:
(a)	both transforms specified above (which only transforms 2nd inputs),
(so that after this we obtain another Possible training set of same size from the quasi-
metric sPace that is only different uP to some Permutation of X)
(b)	a permutation of X (on both inputs) so that the bad Ieft-Pattern nodes and the bad
right-pattern nodes exchange features,
This transforms gives another possible training set ofsame sizefrom the same quasimetric
space, also is bad on a lef Pattern and a right Pattern. Moreover, with a particular way of
select bad patterns (e.g., by the order of the subscripts), this process is reversible. Therefore,
we have defined a way to pair up all such bad training sets.
26
Published as a conference paper at ICLR 2022
zɔ ∙ 1	,1	1 ∙	1	1 9	, ∙	1	. 1	,	,	, /1 l'	FC,
Consider the predictors dbefore and dafter trained on these two training sets (before and after
transform) with an learning algorithm equivariant to orthogonal transforms. Assuming that
they satisfy non-negativity and Identity of Indiscernibles, we have,
•	The predictors have the same distortion over respective training sets.
Therefore we denote this distortion as disS (d) without specifying the predictor d or
training set S.
•	the predictors must predict the same on heldout pairs in the sense that
left	right
dbefore(yileft, zhleft) = dafter (yjright , zhright)	(88)
right	left
dbefore (yjright , zhright) = dafter (yileft , zhleft).	(89)
Focusing on the first, we denote
d(y, z) , dbefore (yieleftt , zhleft) = dafter(yjrgight , zhright)	(90)
without specifying the predictor d or the specific y and z.
However, the quasimetric constraints on heldout pairs (yieft, zhleft) and (yjright, zhright) are
completely different (see the left vs. right part of Fig. 6). Therefore, as shown in Fig. 6,
assuming non-negativity, one of the two predictors must have total violation at least
vio(才)≥ max ∣ ----------X------二------X-------	'(y, ZL | .	(91)
一	∖diss(d)(diss(d) + d(y,z))	2 ∙ diss(d))
Fixing a large enough c, two terms in the max of Eq. (91) can equal for some d(y, z), and
are respectively decreasing and increasing in d(y, z). In that case, we have
for δ > 0 such that	δ vio(d) ≥	,	(92) 2 ∙ diss(d) cδ 	X	X	=	-.	(93) diss(d)(diss(d) + δ)	2 ∙ diss(d)
Solving the above quadratic equation gives
leading to	-diss (d) + J diss (d)2 + 8c δ = 	%	,	(94) -	-1 + ∖∣∖ + 8c∕diss (d)2 vio(d) ≥	Y—4	.	(95)
Therefore, choosing c ≥ fn2(4fn + 1)2 gives
	diss(d) ≤ fn	(96) -	-1 +，1 + 8c∕diss (d) ⇒ vio(d) ≥ 	'-4	 (97) 、-1 + Pι + 8f2(4fn + i)2∕fn ≥	4	 (98) =-1+ P1 + 8(4fn + I)2	(99) ≥ -1 + 4fn + 1	(100) = fn.	(101)
Hence, for training sets that are bad on both a lef Pattern and a right Pattern, We have
shown a way to pair them UP such that
•	each pair of training sets have the same size, and
•	the algorithm fail on one of each pair by producing a distance predictor that
27
Published as a conference paper at ICLR 2022
-has either distortion over training set ≥ fn, or violation ≥ fn, and
-has test MSE ≥ fn.
Remark B.1. Note that all training sets of size m has equal probability of being sampled.
Therefore, to prove the theorem, it suffices to show that with probability 1 - o(1), we can
sample a training set of size m that is bad on both a left Pattern and a right Pattern.
2. Consider sampling training set as individually collecting each pair with a certain prob-
ability p, and carefully analyze the conditions to sample a training set with the special
properties with high probability 1 - o(1).
In probabilistic methods, it is often much easier to work with independent random variables.
Therefore, instead of considering uniform sampling a training set S of fixed size m, we
consider including each pair in S with probability p, chosen independently. We will first
show result based on this sampling procedure via a second moment argument, and later
extend to the case with a fixed-size training set.
First, let’s define some notations that ignore constants:
f 〜g ^⇒ f =(I + O(I))g	(102)
f《g ^⇒ f = o(g).	(103)
We start with stating a standard result from the second moment method (Alon & Spencer,
2004).
Corollary B.2 (Corollary 4.3.5 of (Alon & Spencer, 2004)). Consider random variable
X = Xi + Xz +------------+ Xn, where Xi is the indicator random variable for event Ai. Write
i 〜j if i = j and the pair of events (Ai, Aj) are not independent. Suppose the following
quantity does not depend on i:
△* , X P [Aj | Ai].	(104)
j〜i
If E [X] → ∞ and △《E [X], then X 〜E [X] with probability 1 一 o(1).
We will apply this corollary to obtain conditions on p such that S with probability 1 - o(1)
is bad on some left pattern, and conditions such that S with probability 1 一 o(1) is bad on
some right pattern. A union bound would then give the desired result.
•	S is bad on some left pattern.
Recall that a left Pattern is specified by iieft, jieft,lieft, hieft all ∈ [k]:
(Xieeft ,yieft ,WiftjtMft ,Zhleft)	(105)
Therefore, we consider k4 =(合)4 events of the form
Aileftjleft,iieft,hieft 与{S is bad on the left Pattern at iieft,jieft,lieft, hieft}.	(106)
Obviously, these events are symmetrical, and the △* in Eq. (104) does not depend on i.
By the quasimetric space construction and the requirement for S to be bad on a left Pat-
tern in Eqs. (84) and (85), we can see that (ileft,jleft,lleft,hleft)〜(i0eft,j 强匕(仕，hf
only if ile任=ileft or jleft = j∖任 or lleft = lleft or hle任=hleft∙
28
Published as a conference paper at ICLR 2022
Therefore, we have
EX]〜n4p4(1 - p)10	(include 4 pairs & exclude 10 pairs)
△ ≪ n3p4 (1 一 p)9	(share jleft)
+ n3p2 (1 一 p)7	(share ileft)
+ n3p4 (1 一 p)9	(share lleft)
+ n3p4 (1 一 p)10	(share hleft)
+ n2p2 (1 一 p)4	(share jleft, ileft)
+ n2p4 (1 一 p)8	(share jleft, lleft)
+ n2p4 (1 一 p)9	(share jleft, hleft)
+ n2p2 (1 一 p)4	(share ileft , lleft)
+ n2p(1 一 p)6	(share ileft, hleft)
+ n2p3 (1 一 p)9	(share lleft, hleft)
+ n(1 一 p)3	(share ileft, lleft, hleft)
+ np3 (1 一 p)8	(share jleft, lleft, hleft)
+ np(1 一 p)3	(share jleft , ileft, hleft)
+ np2 (1 一 p)	(share jleft, ileft, lleft)
〜n3p2(1 一p)7 + n2(p2(1 一p)4 + p(1 一p)6)	(107)
+ n((1 -p)3 +p2(1 - p)).	(108)
Therefore, to apply Corollary B.2, we need to have
n4p4(1 一 p)10	→	∞	(109)
n3p2 (1 一 p)7	n4p4(1	一 p)10	(110)
n2(p2 (1 一 p)4 + p(1 一 p)6)	n4p4(1	一 p)10	(111)
n((1 — p)3 + P(1 一P))	≪	n4p4(1	— p)10,	(112)
which gives
p	n-1/2	(113)
1 ― p》n-1/3	(114)
as a sufficient condition to for S to be bad on some left Pattern with probability 1 - o(1).
•	S is bad on some right pattern.
Recall that a right Pattem is specified by iright, jright, lright, hright all ∈ [k]:
(xright 0right right wright w0right z )
(xiright , yiright , yjright , wjright , wlright , zhright)
(115)
Similarly, we consider k4 =(巷)4 events of the form
Airight,jright,lright,hright , {S is bad On the Ieft Pattem at iright, jright, lright, hright}∙ (116)
Again, these events are symmetrical, and △* in Eq. (104) does not depend on i.
29
Published as a conference paper at ICLR 2022
Similarly, we have
E [X]〜n4p4(1 一 p)10	(include 4 pairs & exclude 10 pairs)
∆*《n3p3(1 - p)9	(share iright)
+ n3P3 (1 一 P)8	(share jright)
+ n3P4 (1 一 P)10	(share hright)
+ n3P4 (1 一 P)9	(share lright)
+ n2P2 (1 一 P)4	(share iright , jright)
+ n2P2 (1 一 P)9	(share iright , hright)
+ n2P3 (1 一 P)8	(share iright , lright)
+ n2P2 (1 一 P)7	(share jright , hright)
+ n2P3 (1 一 P)5	(share jright , lright)
+ n2P4 (1 一 P)9	(share hright , lright)
+ nP2 (1 一 P)4	(share jright , hright , lright)
+ nP2 (1 一 P)8	(share iright , hright , lright)
+ nP2 (1 一 P)	(share iright , jright , lright)
+ n(1 一 P)	(share iright, jright, hright)
〜n3p3(1 — p)8 + n2p2(1 — p)4	(117)
+ n(1 一 P).	(118)
Therefore, to apply Corollary B.2, we need to have	
n4P4(1 一 P)10 → ∞	(119)
n3P3 (1 一 P)8	n4P4 (1 一 P)10	(120)
n2P2 (1 一 P)4	n4P4 (1 一 P)10	(121)
n(1 一 P)	n4P4 (1 一 P)10,	(122)
which gives	
P	n-3/4	(123)
1 一 P	n-1/3	(124)
as a sufficient condition to for S to be bad on some right Pattern with probability
1 一 o⑴.
So, by union bound, as long as
p	n-1/2	(125)
1 -P》n-1/3,	(126)
S is bad on some left Pattem and some right Pattem with probability 1 一 o(1).
3.	Extend to fixed-size training sets and show that, under similar conditions, we sample
a training set with the special properties with high probability 1 一 o(1).
To extend to fixed-size training sets, we consider the following alteration procedure:
(a)	Sample training set S by independently include each pair with probability P，mn++δ,
for some δ > 0.
(b)	Show that with high probability 1 一 o(1), we end up with [m, m + 2δ] pairs in S.
(c)	Make sure that P satisfy Eq. (125) and Eq. (126) so that S is bad on some left Pattern
and some right Pattern with high probability 1 一 o(1).
(d)	Randomly discard the additional pairs, and show that with high probability 1 一 o(1)
this won,t affect that S is bad on some left Pattern and some right pattern.
We now consider each step in details:
30
Published as a conference paper at ICLR 2022
(a)	Sample training set S by independently include each pair with probability p ,
m++δ, for some δ > 0.
n2 ,
For P，mn++δ, the number of pairs in the training set is distributed as
m+δ
Binomial(n2,----—).	(127)
n2
(b)	Show that with high probability 1 - o(1), we end up with [m, m + 2δ] pairs in S.
Standard Binomial concentration tells us that,
mm +	+。、，.
δ》nʌ/p(l — P) =⇒ P Binomial(n2,-------------) ∈ [m, m + 2δ] → 0,	(128)
n2
which can be satisfied if
δ n.
(129)
(c)	Make sure that p satisfy Eq. (125) and Eq. (126) so that S is bad on some left
Pattern and some right pattern with high probability 1 一 o(1).
Therefore, we want
m+δ》n-1/2	(130)
n2
m	+ δ
1-----2-》n-1/3.	(131)
n2
(d)	Randomly discard the additional pairs, and show that with high probability 1 一
o(1) this won't affect that S is bad on some left pattern and some right pattern.
Consider any specific bad left Pattern and a right Pattern in S. It is sufficient that We
don't break these two patterns during discarding.
Since We only discard pairs, it suffices to only consider the pairs We Want to preserve,
which are a total of 8 pairs across two patterns.
Each such pair is discarded the probability ≤ Im, since we remove at most 2δ pairs. By
union bound,
P [all 8 pairs are preserved] ≥ 1 - 16δ.	(132)
m
Hence, it suffices to make sure that
δ m.	(133)
Collecting all requirements, we have
Assume that	δ n	(134) m+-δ》n-1/2	(135) n2 1 - m+-δ》n-1/3	(136) n2 δm.	(137) m2》n-1/2	(138) n2 1 -	m2》n-1/3.	(139) n2
It can be easily verified that using δ , n1.1 satisfies all conditions.
Hence, for a uniformly randomly sampled training set S with size m, S is bad on some left
Pattern and some right Pattern with high probability 1 一 o(1), as long as
m2》n-1/2	(140)
n2
1 一 m2》n-1/3.	(141)
n2
This is exactly the condition we need to prove the theorem (see Remark B.1).
This concludes the proof.
□
31
Published as a conference paper at ICLR 2022
B.3.2	Discussions
Training set size dependency. Intuitively, when the training set has almost all pairs, violation can
be lowered by simply fitting training set well; when it is small and sparse, the learning algorithm
may have an easier job finding some consistent quasimetric. Thm. 4.6 shows that, outside these two
cases, algorithms equivariant to orthogonal transforms can fail. Note that for the latter case, Thm. 4.6
requires the training fraction to decrease slower than n-1/2, which rules out training sizes that is
linear in n. We leave improving this result as future work. Nonetheless, Thm. 4.6 still covers common
scenarios such as a fixed fraction of all pairs, and highlights that a training-data-agnostic result (such
as the ones for PQEs) is not possible for these algorithms.
Proof techniques. In embedding theory, it is quite standard to analyze quasimetrics as directed
graphs due to their lack of nice metric structure. In the proof for Thm. 4.6, we used abundant
techniques from the probabilistic method, which are commonly used for analyzing graph properties
in the asymptotic case, including Corollary B.2 from the second moment technique, and the alteration
technique to extend to fixed-size training sets. While such techniques may be new in learning theory,
they are standard for characterizing asymptotic probabilities on graphs, which quasimetrics are often
analyzed as (Charikar et al., 2006; Memoli et al., 2018).
To provide more intuition on why these techniques are useful here, we note that the construction of
a training set of pairs is essentially like constructing an ErdoS-Renyi random graph on n2 vertices.
Erdos-Renyi (undirected) random graphs come in two kinds:
•	Uniformly sampling a fixed number of m edges;
•	Adding an edge between each pair with probability p, decided independently.
The latter, due to its independent decisions, is often much easy to analyze and preferred by many. The
alteration technique (that we used in the proof) is also a standard way to transfer a result on a random
graph of the latter type, to a random graph of the former type (BollobgS & Bela, 2001). Readers can
refer to (Alon & Spencer, 2004; Bollobgs & Bela, 2001; Erdos & R6nyi, 1959) for more in-depth
treatment of these topics.
Generalization to other transforms. The core of this construction only relies on the ability to
swap (concatenated) inputs between (x, y) — (x, y0) and between (y, W) — (y, w0) via a transform.
For instance, here the orthogonal transforms satisfy this requirement on one-hot features. Therefore,
the result can also be generalized to other transforms and features with the same property. Our stated
theorem focuses on orthogonal transforms because they correspond to several common learning
algorithms (see Lemma 4.5). If a learning algorithm is equivariant to some other transform family, it
would be meaningful to generalize this result to that transform family, and obtain a similar negative
result. We leave such extensions as future work.
B.3.3	Corollary of Distortion and Violation for Unconstrained MLPs
Corollary B.3 (Distortion and Violation of Unconstrained MLPs). Let (fn)n be an arbitrary
sequence of desired violation values. There is an infinite collection of quasimetric spaces
((Xn, dn))n=1,2,... with |Xn| = n, Xn ⊂ Rn such that MLP trained with squared loss in NTK
regime converges to a function d that either
•	fails non-negativity, or
•	Viθ(d) ≥ fn,
with probability 1/2 - o(1) over the random training set S of size m, as long as S does not contain
almost all pairs 1 - m/n2 = ω(n-1/3), and does not only include few pairs m/n2 = ω(n-1/2).
Proof of Corollary B.3. This follows directly from Thm. 4.6 and standard NTK convergence results
obtained from the kernel regression optimality and the positive-definiteness of the NTK. In particular,
Proposition 2 of (Jacot et al., 2018) claims that the NTK is positive-definite when restricted to a
hypersphere. Since the construction in proof of Thm. 4.6 uses one-hot features, the input (concatena-
tion of two features) lie on the hypersphere with radius √2. Hence, the NTK is guaranteed positive
definite.	□
32
Published as a conference paper at ICLR 2022
5-
4-
3-
2-
1-
O-
6-1=-≡
c
(a) Training losses for varying c. Note the scale of the
vertical axis.
(b) Prediction on heldout pair d(y, z) for varying c.
Figure 7: Training unconstrained MLPs on the toy failure construction discussed in Sec. 4.2 (re-
produced as Fig. 6). Two patterns in the construction have different constraints on distance of the
heldout pair (y, z). Plots show mean and standard deviations over 5 runs. Left: All training conclude
with small training error. Right: Trained MLPs predict identically for both patterns. Here standard
deviation is small compared to mean and thus not very visible.
B.3.4	Empirical Verification of the Failure Construction
We train unconstrained MLPs on the toy failure construction discussed in Sec. 4.2 (reproduced as
Fig. 6). The MLP uses 12-1024-1 architecture with ReLU activations, takes in the concatenated
one-hot features, and directly outputs predicted distances. Varying c ∈ {1, 10, 100, 1000}, we train
the above MLP 5 times on each of the two patterns in Fig. 6, by regressing towards the training
distances via MSE loss.
In Fig. 7, we can see that all training runs conclude with small training error, and indeed the trained
MLPs predict very similarly on the heldout pair, regardless whether it is trained on the left or right
pattern of Fig. 6, which restricts the heldout pair distance differently.
This verifies our theory (Thm. 4.6 and Corollary B.3) that algorithms equivariant to orthogonal
transforms (including MLPs in NTK regime) cannot distinguish these two cases and thus must fail on
one of them.
C	Proofs and Discussions for Sec. 5: Poisson Quasimetric
Embeddings (PQEs)
C.1 Non-differentiability of Continuous-Valued Stochastic Processes
In this section we formalize the argument presented in Sec. 5.3 to show why continuous-valued
stochastic processes lead to non-differentiability. Fig. 8 also provides a graphical illustration of the
general idea.
Proposition C.1 (Quasimetric Embeddings with Continuous-Valued Stochastic Processes are
not Differentiable). Consider any Rk-valued stochastic process {R(u)}u∈Rd such that u 6= u0 =⇒
P [R(u) = R(u0)] < c for some universal constant c < 1. Then P [R(u) ≤ R(u0)] is not differen-
tiable at any u = u0 .
Proof of Proposition C.1. Assume that the quantity is differentiable. Then it must be continuous in
u and v.
We will use the (, δ)-definition of continuity.
At any U ∈ Rd, consider small e ∈ (0,1-c). By continuity, since
P [R(u) ≤ R(u)] = P[R(u+δ) ≤ R(u + δ)] = 1	(142)
33
Published as a conference paper at ICLR 2022
P[ R (U) = R (U)] = 1
Most probability still on R(U) = R(U + δ)
(b) Discrete-valued stochastic process.
(R(U), R(U + δ)) has bounded density on R2
PR(U) = R(u)] = 1	=⇒ p[R(U) = R(U + δ)]=0
(a) Continuous-valued stochastic process.
Figure 8: Bivariate distributions from different stochastic processes. Left: In a continuous-valued
process (where (Nθ, Nθ0) has bounded density if θ 6= θ0), perturbing one θ → θ + leaves
P [Nθ = Nθ+] = 0. Then one of PNθ ≤ Nθ+ and PNθ+ ≤ Nθ must be far away from
1 (as they sum to 1), breaking differentiability at either P [Nθ ≤ Nθ] = 1 or P [Nθ+ ≤ Nθ+] = 1.
Right: For discrete-valued processes, most probability can still be left on Nθ = Nθ+ and thus do
not break differentiability.
we can find ∈ Rd such that
P [R(u) ≤ R(u + δ)] ≥ 1 -	(143)
P[R(u+δ) ≤ R(u)] ≥ 1 -	.	(144)
However, by assumption, P [R(u) = R(u + δ)] < c. Therefore,
P [R(u) ≤ R(u + δ)] ≥ 1 -	(145)
P [R(u + δ) < R(u)] ≥ 1 - - c,	(146)
which implies
1 = P [R(u) ≤ R(U + δ)] + P [R(u + δ) < R(u)] ≥ 2 - 2e - C ≥ 5 - 2c > 1.	(147)
By contradiction, the quantity must not be differentiable at any U = u0.	□
C.2 PQE-GG: Gaussian-based Measure and Gaussian Shapes
In Sec. 5.1, we presented the following PQE-LH formulation for Lebesgue measures and half-lines:
dzPQE-LH (U, v) ,
^X αi ∙(1 — exp(- X(Uij -Vij 广)).	(I0)
ij
Here, Ui,j and Vi,j receive zero gradient when Ui,j ≤ Vi,j.
Gaussian shapes parametrization. We therefore consider a set parametrization where no one set
is entirely contained in a different set— the regions regions ⊂ R2 between an axis and a 1D Gaussian
density function of fixed variance σs2hape = 1. That is, for each given U ∈ R, we consider sets
an(μ) , {(a, b):b ∈ [0,fN(a;μ, I)]},	(148)
where fN(b; μ, σ2) denotes the density of 1D Gaussian N(μ, σ2) with mean μ and variance σ2
evaluated at b. Since the Gaussian density function have unbounded support, these sets, which are
translated versions of each other, never have one set fully contained in another. For latent U ∈ Rh×k
reshaped as 2D, our set parametrizations are,
U → Ai,j(U) , AN (Ui,j),	i ∈ [h], j ∈ [k].	(149)
A Gaussian-based measure. These subsets of R2 always have Lebesgue measure 1, which would
make PQE symmetrical (if used with a (scaled) Lebesgue measure). Thus, we use an alternative
R2 measure given by the product of a R Lebesgue measure on the b-dimension (i.e., dimension
of the function value of the Gaussian density) and a R Gaussian measure on the a-dimension (i.e.,
dimension on the input of the Gaussian density) centered at 0 with learnable variances (σm2 easure)i,j.
To avoid being constrained by the bounded total measure of 1, we also optimize learnable positive
scales ci,j > 0. Hence, the each Poisson process has a mean measure as the product of a R Lebesgue
measure and a R Gaussian with learnable standard deviation, then scaled with a learnable scale.
34
Published as a conference paper at ICLR 2022
Note that the Gaussian measure should not be confused with the Gaussian shape. Their parameters
also are fully independent with one another.
Computing measures of Gaussian shapes and their intersections. The intersection of two such
Gaussian shapes is formed by two Gaussian tail shapes, reflected around the middle point of the two
Gaussian means (since they have the same standard deviation σshape = 1). Hence, it is sufficient
to describe how to integrate a Gaussian density on a Gaussian measure over an interval. Applying
this with different intervals would give the measure of the intersection, and the measures of the
two Gaussian shapes. Omit indices i, j for clarity. Formally, we integrate the Gaussian density
fN (a; u, σs2hape) over the centered Gaussian measure with variance σm2 easure, which has density
fN (a; 0, σm2
easure):
/ C ∙ fN(a; u,σ2haPe)fN(囚0,湍easUre) da,	(15O)
which is also another Gaussian integral (e.g., considered as integrating the product measure along the
a line of the form y = x + u). After standard algebraic manipulations (omitted here), we obtain
/ C ∙ fN (a； u,σ2haPe)fN (见0,feasUre) da	(151)
C ∙ exp I-U /σtθtal)	σmeasure σshapeσmeasure
=--------&	2	----- fN	a； U -2----,--------2	da,	(152)
√2∏σ2otal	σ2otal	σ2otal
for
σ2	, σ2	+ σ2
σtotal	σshaPe + σmeasure .
(153)
This can be easily evaluated using statistical computing packages that supports computing the error
function and/or Gaussian CDF. Moreover, this final form is also readily differentiable with standard
gradient formulas. To summarize,
each set A(u) has total measure
C
P2πσtθtal
eχp (-u=σ2otal)；
the intersection of A(v) and A(u2), for v ≤ u2 has measure
√2πσ2otal
√2πσ2otal
• exP (-v2O2otal)
C • exP (-u2 02otal)
fN a； u2
fN	a； v
22
σmeasure σshaPe
Ξ2
σtotal
σ2
σmeasure
-2	da
t2otal
22
σmeasure σshaPe
σtotal
σ2
σmeasure
-2	da.
t2otal
(154)
(155)
(156)
C
+
σ
σ
Interpretation and representing any total order. Consider two Gaussian shapes A(v) and A(u2).
Note that the Gaussian-based measure μGaussian is symmetric around and centered at 0. Therefore,
|v| < |u2 |	⇒ μGaussian(A(V)) > μGaussian(A(u2))	(157)
=⇒ MGaussian (A(v) \ A(U2 )) > MGaussian (A(〃2 ) \ A(v))∙	(158)
Moreover, scaling the rates of a Poisson makes it more concentrated (as a Poisson’s mean grows
as the square of its standard deviation) so that limc→∞ P [Pois(cμι) ≤ Pois(Cμ2)] = Iμ1<μ2 for
μι = μ2. Then any total order can be represented as the limit of a Poisson process with Gaussian
shapes, with the shapes’ having their means arranged according to the total order, as the scale on the
Gaussian-based measure grows to infinity.
C.3 Theoretical Guarantees for PQEs
Theorem 5.2 (Distortion and violation of PQEs). Under the assumptions of Sec. 4, any quasimetric
space with size n and treewidth t admits a PQE-LH and a PQE-GG with distortion O(t log2 n) and
violation 1, with an expressive encoder (e.g., a ReLU network with ≥ 3 layers and polynomial width).
In Sec. 5.4, we presented the above theoretical distortion and violation guarantees for PQE-LH and
PQE-GG. Furthermore, we commented that the same guarantees apply to more generally to PQEs
satisfying a mild condition. Here, we first precisely describe this condition, show that PQE-LH and
35
Published as a conference paper at ICLR 2022
PQE-GG do satisfy it, state and prove the general result, and then show the above as a straightforward
corollary.
C.3. 1 The Concentration Property
Recall that PQEs are generally defined with measures μ and set parametrizations A as
dPQE(U, v； μ, A,α) , Eai ∙ E∏z〜ΠPQE(μi,Ai) [πz (u, V)] ,	(13)
where
Enz 〜∏ZQE(μ,A)[πZ(U, V)I , 1 - Y P[Nj (Aj (U)) ≤ Nj (Aj (V))].	(12)
j
Because the measures μ and set parametrizations A themselves may have parameters (e.g., as in
PQE-GG), we consider them as classes of PQEs. E.g., PQE-GG is a class of PQEs such that the μ is
the specific Gaussian-based form, and A is the specific Guassian-shape.
Definition C.2 (Concetration Property of PQEs). Consider a PQE class with h mixtures of quasi-
partition distributions, each from k Poisson processes. We say that it has concentration property if it
satisfies the following. Consider any finite subset of X0 ⊂ X, and arbitrary function g : X → Rh×k.
There exists a sequence of ((f (n), μ(n), A(n))n such that
•	f(n) : X0 → Rd,
•	μ(n), A(n) are valid members of this PQE,
•	Enz〜πzQE(μi,Ai) [πζ(f(n)(x0),f (n)(y0))] uniformly converges to 1 - QjIg(X)i,j≤g(y)i,j, over
all mixtures i and pairs x, y ∈ X0.
A sufficient condition. It suffices to make the probabilities
(x,y,i,j) →P[Nj(Aj(U)) ≤ Nj(Aj(V))],	(159)
along some PQE sequence uniformly converge to the indicators
(x,y,i,j) → 1g(x0)i,j≤g(y0)i,j.	(160)
This is sufficient since product of bounded functions is uniformly convergent, if each function is.
Both statements below together form a sufficient condition for Eq. (159) to uniformly converge to
Eq. (160):
1.	For any g, there exists a specific PQE of this class satisfying
•	Measures (of set differences) are consistent with g with some margin > 0: ∀i ∈
[h],j∈[k],x∈X0,y∈X0,
g(x)i,j < g(y)i,j ^⇒ μi,j(Aij(J(Xy) ∖ Ai,j(f (y))) + e < 〃ij(Aij(f(y)) \ Aij(f (X)))
g(X)i,j = g(y)i,j ^⇒ 〃ij(Aij J(X)) \ Aij(Jy))) = 〃，j(Aij(f(y))∖ Aij J(X))) = 0.
•	Either of the following:
- One side must be zero: ∀i ∈ [h], j ∈ [k], X ∈ X , y ∈ X,
(μi,j (Ai,j(f(X)) \ Ai,j (f(y))))(μi,j(Ai,j (f(y)) \ Aij J(X)))) = 0,	(161)
- Max measure is bounded by some constant c > 0:
maχ.μi,j(Aij(f(X)) \ Ai,j(f(y))) ≤ c.
x,y,i,j
(162)
2.	For any given specific PQE of this class, for any positive scale d > 0, there is another PQE
(with same formulation) whose measures (of set differences) equal exactly those of the given
PQE scaled by d.
We now show that this is a sufficient condition. Note that a Poisson distribution has standard deviation
equal to square root of its mean. This means that as we scale the rate of a Poisson, it becomes more
concentrated. Applying to Poisson race probability, we have, for 0 ≤ μι + e < μ2,
36
Published as a conference paper at ICLR 2022
• one direction of Poisson race probability:
P [Pois(d ∙ μι) ≤ Pois(d ∙ μ2)]
≥	P [|Pois(d ∙ μ2) — Pois(d ∙ μι) — d(μ2 — μι) | ≤ d(μ2 — μι)]
≥	1 -	μι + μ2
一	d(μ2 一 μι)2
≥	J1-2 ifμι=0
I1 - dc2 if μ2 < c;
• the other direction of Poisson race probability:
P [Pois(d ∙ μ2) ≤ Pois(d ∙ μι)]
≤	P [|Pois(d ∙ μ2) - Pois(d ∙ μι) - d(μ2 - μι) | ≥ d(μ2 - μι)]
≤	μι + μ2
—	d(μ2 - μι)2
≤	J 京	if μι = 0
-	ld⅛	ifμ2 < c.
(163)
(164)
(165)
(166)
(167)
(168)
(169)
(170)
Therefore, applying to scaled versions of the PQE from Item 1 above, we have thus obtained the
desired sequence, where Eq. (159) uniformly converges to Eq. (160) with rate O(1/d).
Lemma C.3. PQE-LH and PQE-GG both have the concentration property.
Proof of Lemma C.3. We show that both classes satisfy the above sufficient condition.
•	PQE-LH: Lebesgue measure λ and half-lines.
WLOG, since X is countable, we assume that g satisfies
g(x)i,j 6= g(y)i,j =⇒ |g(x)i,j - g(y)i,j| > 1,	∀i ∈ [h], j ∈ [k], x ∈ X0,y ∈ X0.
(171)
The encoder in Item 1 above f : X → Rh×k can simply be g. We then have
μi,j(Aij(f(y))∖Aij(f(X))) = LebK-∞,g⑻N-∞,g(X)D = (g(y)i,j-g(χ)i,j)+. (172)
This ensures that one side is always zero. Furthermore, scaling can be done by simply scaling
the encoder f. Hence, PQE-LH satisfies this constraint.
•	PQE-GG: Gaussian-based measure and Gaussian shapes (see Appendix C.2).
Because X0 is finit, we can have positive constant margin for the PQE requirements in Item 1.
(Infinite X0 does not work because the total measure is finite (for a specific PQE-GG with specific
values of the scaling).) Concretely, we satisfy both requirements via
一 in descending order of g(∙)i,j We assign Gaussian shapes increasingly further from the
origin;
一 scaling comes from that We allow scaling the Gaussian-based measure.
Hence, PQE-GG satisfies this constraint for finite X.
□
C.3.2 A General S tatement
We noW state the general theorem for PQEs With the above concentration property.
Theorem C.4 (Distortion and violation of PQEs (General)). Consider any PQE class With the
concentration property. Under the assumptions of Sec. 4, any quasimetric space With size n and
treeWidth t admits such a PQE With distortion O(t log2 n) and violation 1, With an expressive
encoder (e.g., a ReLU netWork With ≥ 3 hidden layers, O(n) hidden Width, and O(n2) quasipartition
distributions, each With O(n) Poisson processes.).
Before proving this more general theorem, let us extend a result from M6moli et al. (2018).
Lemma C.5 (Quasimetric Embeddings with Low Distortion; Adapted from Corollary 2 in
Memoli et al. (2018)). Let M = (X, d) be a quasipseudometric space With treeWidth t, and n =
|X|. Then M admits an embedding into a convex combination (i.e., scaled mixture) of O(n2 )
quasipartitions With distortion O(t log2 n).
37
Published as a conference paper at ICLR 2022
ProofofLemma C.5. The distortion bound is proved in Corollary 2 in (Memoli et al., 2018), which
states that any quasipseudometric space with n elements and t treewidth admits an embedding into a
convex combination of quasipartitions with distortion O(t log2 n).
To see that n2 quasipartitions suffice, we scrutinize their construction of quasipartitions in Algorithm 2
of (Memoli et al., 2018), reproduced below as Algorithm 1.
Algorithm 1 Random quasipartition of a bounded treewidth graph. Algorithm 2 of (M6moli et al.,
2018).____________________________________________________________________________________________________
Input: A digraph G of treewidth t, a hierarchical tree of separators of G (H, f) with width t, and r > 0.
Output: A random r-bounded quasipartition R.
Initialization: Set G* = G, H * = H and R = E(G). Perform the following recursive algorithm on G*
and H *.
Step 1. Pick z ∈ [0, r/2] uniformly at random.
Step 2. If |V (G*)| ≤ 1,terminate the current recursive call. Otherwise pick the set of vertices K = G* .
Let H1, . . . , Hm be the sub-trees of H* below root(H*) that are hierarchical trees of separators of
C1 , . . . , Cm respectively.
Step 3. For all (u, v) ∈ E(G*) remove (u, v) from R if one of the following holds:
(a)	dG(u, x)	> z and dG(v,	x)	≤ z for some vertex x ∈	K.
(b)	dG(x, v)	> z and dG(x,	u)	≤ z for some vertex x ∈	K.
Step 4. For all i ∈ {1, . . . , m} perform a recursive call of Steps 2-4 setting G* = G* [Ci] and H* = Hi.
Step 5. Once all branches of the recursive terminate, enforce transitivity on R: For all u, v, w ∈ V (G) if
(u, v) ∈ R and (v, w) ∈ R, add (u, w) to R.
Many concepts used in Algorithm 1 are not relevant for our purpose (e.g., r-bounded quasipartition).
Importantly, we observe that for a given quasimetric space, the produced quasipartition is entirely
determined by the random choice of z in Step 1, which is only used to compare with distance values
between node pairs. Note that there are n2 node pairs, whose minimum distance is exactly 0 (i.e.,
distance from a node to itself). Since z ≥ 0, there are at most n2 choices of z that lead to at most n2
different quasipartitions, for all possible values of r.
The construction used to prove Corollary 2 of (Memoli et al., 2018) uses exactly quasipartitions given
by this algorithm. Therefore, the lemma is proved.	□
Lemma C.5 essentially proves the first half of Thm. C.4. Before proving the full Thm. C.4, we restate
the following result from (Hiraguchi, 1951), which gives us a bound on how many total orders are
needed to represent a general partial order (i.e., quasipartition).
Theorem C.6 (Hiraguchi’s Theorem (Hiraguchi, 1951; Bogart, 1973)). Let (X, P) be a partially
ordered set such that |X| ≥ 4. Then there exists a mapping f : X → Rb|X|/2c such that
∀x,y ∈ X,	XPy ^⇒ f(x) ≤ f(y) coordinate-wise.	(173)
Proof of Thm. C.4. It immediately follows from Lemma C.5 and Thm. C.6 that any quasimetric
space with n elements and treewidth t admits an embedding with distortion O(t log2 n) into a convex
combination of n2 quasipartitions, each represented with an intersection of O(n) total orders.
Because the PQE class has concentration property, for any finite quasimetric space, we can simply
select a PQE that is close enough to the desired convex combination of n2 quasipartitions, to
obtain distortion O(t log2 n). Since each Poisson process in PQE takes a constant number of
latent dimensions, we can have such a PQE with O(n3)-dimensional latents and n2 quasipartition
distributions.
It remains only to prove that we can compute such required latents using the described architecture.
Consider any x ∈ X ⊂ Rd . Since X is finite, we can always find direction ux ∈ Rd such that
∀y ∈ X \ {x}, yTux 6= xTux. That is, x has a unique projection onto ux. Therefore, we can have
c, b+ , b- ∈ R such that
C ∙ uTx + b+ = 1	(174)
-	C ∙ UTx + b- = 1,	(175)
38
Published as a conference paper at ICLR 2022
but for y ∈ X \ {x}, we have, for some a > 0, either
C ∙ uTy + b+	= —a	(176)
-	C ∙ UTy + b-	= a +	2,	(177)
or
c ∙ UTy + b+	= a +	2	(178)
—	c ∙ uTy + b-	= —a.	(179)
Then, consider computing two of the first layer features as, on input z ,
[ReLU(c ∙ UTz + b+) ReLU(-c ∙ UTz + b-)],	(180)
which, if z = x, is [1, 1]; if z 6= x, is either [0, 2 + a] or [2 + a, 0], for some a > 0.
Then, one of the second layer features may sum these two features and threshold it properly would
single out x, i.e., activate only when input is x.
After doing this for all x ∈ X, we obtain an n-dimensional second layer feature space that is just
one-hot features.
The third layer can then just be a simple embedding look up, able to represent any embedding,
including the one allowing a PQE to have distortion O(t log n), as described above.
Because quasimetric embeddings naturally have violation 1, this concludes the proof.	□
C.3.3 Proof of Thm. 5.2: Distortion and violation of PQEs
Proof of Thm. 5.2. Lemma C.3 and Thm. C.4 imply the result. To see that polynomial width is
sufficient, note that the hidden width are polynomial by Thm. C.4, and that the embedding dimensions
needed to represent each of the O(n3) Poisson processes is constant 1 in both PQE-LH and PQE-GG.
Hence the latent space is also polynomial. This concludes the result.	□
C.3.4 Discussions
Dependency on log n. log n dependency frequently occurs in distortion results. Perhaps the
most well-known ones are Bourgain’s Embedding Theorem (Bourgain, 1985) and the Johnson-
Lindenstrauss Lemma (Johnson & Lindenstrauss, 1984), which concern metric embeddings into
Euclidean spaces.
Dependency on treewidth t. Treewidth t here works as a complexity measure of the quasimetric.
We will use a simple example to illustrate why low-treewidth is easy. Consider the extreme case
where the quasimetric is the shortest-path distance on a tree, whose each edge is converted into two
opposing directed ones and assigned arbitrary non-negative weights. Such a quasimetric space has
treewidth 1 (see Defn. 2.2). On a tree,
1. the shortest path between two points is fixed, regardless of the weights assigned,
2. for each internal node U and one of its child C, the followings are quasipartitions:
d01 (x, y) , 1shortest path from
x to y passes (u, c)
d01 (x, y) , 1shortest path from
x to y passes (c, u) .
Hence it can be exactly represented as a convex combination of quasipartitions. However, both of
observations becomes false when the graph structure becomes more complex (higher treewidth) and
the shortest paths can are less well represented as tree paths of the tree composition.
Comparison with unconstrained MLPs. Thm. C.4 requires a poly-width encoder to achieve low
distortion. This is comparable with deep unconstrained MLPs trained in NTK regime, which can
reach 0 training error (distortion 1 on training set) in the limit but also requires polynomial width
(Arora et al., 2019).
Quasipseudometrics and infinite distances. Thm. C.4 relies on our assumptions that (X , d) is
not a quasipseudometric space and has all finite distances. In fact, if we allow a PQE to have infinite
convex combination weights, it can readily represent quasipseudometric spaces with infinite distances.
39
Published as a conference paper at ICLR 2022
Additionally, PQE can still well approximate the quasimetric space with infinities replaced with
any sufficiently large finite value (e.g., larger than the maximum finite distance). Thus, this limit is
generally not important in practice (e.g., learning γ-discounted distances), where a large value and
infinity are usually not treated much differently.
Optimizing quasimetric embeddings. From Thm. C.4, we know that optimizing PQEs over the
training set S w.r.t. distortion achieves low distortion (and optimal violation by definition). While
directly optimizing distortion (or error on log distance or distance ratios, equivalently) seems a valid
choice, such objectives do not always train stably in practice, with possible infinities and zeros.
Often more stable losses are used, such as MSE over raw distances or γ-discounted distances γd,
for γ ∈ (0, 1). These objectives do not directly relate to distortion, except for some elementary
loose bounds. To better theoretically characterize their behavior, an alternative approach with an
average-case analysis might be necessary.
C.4 Implementation of Poisson Quasimetric Embeddings (PQEs)
Sec. 5.2 mentioned a couple implementation techniques for PQEs. In this section, we present them in
full details.
C.4.1 Normalized Measures
Consider a PQE whose each of j expected quasipartitions is defined via k Poisson processes, with set
parametrizations u → Ai,j (u), i ∈ [h], j ∈ [k]. To be robust to the choice of k, we instead use the
normalized set parametrizations A0i,j :
A0i,j (u) , Ai,j(u)/k,	i ∈ [h], j ∈ [k].	(181)
This does not change the PQE’s concentration property (Defn. C.2) or its theoretical guarantees (e.g.,
Thms. 5.2 and C.4).
C.4.2 OUTPUTTING γ-DISCOUNTED DISTANCES
Recall the PQE quasimetric formulation in Eq. (13), for αi ≥ 0, and encoder f : X → Rd :
d(x,y)，Eai
i
(1-YP[Pois(μi,j(Ai,j(f(x))∖ Ai,j(f(y)))) ≤ Pois(μ,,jʤ(f(y)) \、(f(x))))])
j	(13)
With discount factor γ ∈ (0, 1), we can write the γ-discounted PQE distance as
Y d(x,y) = Y(Yai)I-Qj P[Pois(μi,j (Ai,j (f(x))∖Ai,j (f (y))))≤Pois(μi,j (A- (f (y))∖Ai,j (f (x))))].	(182)
i {z
a scalar that can take value in any (0, 1)
Therefore, instead of learning αi ∈ [0, ∞), we can learn bases βi ∈ (0, 1) such and define the
Y-discounted PQE distance as
d(x,y) , π 储-Qj P[Pois(μi,j(Ai,j(f(χ))∖Ai,j(f(y))))≤Pois(μi,j(Ai,j(f(y))∖Ai,j(f(χ))))]
Y	βi	.	(183)
i
These bases βi ∈ (0, 1) can be parametrized via a sigmoid transform. Consider quasimetric learning
w.r.t. errors on Y-discounted distances (e.g., MSE). Unlike the parametrization with directly learning
the convex combination weights αi ’s, such a parametrization (that learns the bases βi ’s) does not
explicitly include Y and thus can potentially be more stable for a wider range of Y choices.
Initialization. Consider learning bases βi ’s via a sigmoid transform: learning bi and defining
βi，σ(bi). We must take care in initializing these b/s so that σ(bi)'s are not too close to 0 or 1, since
we take a product of powers with these bases. To be robust to different h numbers of quasipartition
distributions, we initialize the each bi to be from the uniform distribution
U[°T(0.52/h ),σ-1(0.752")],	(184)
40
Published as a conference paper at ICLR 2022
which means that, at initialization,
Y βi0.5 = Y σ(bi)0.5 ∈ [0.5, 0.75],	(185)
i∈[h]	i∈[h]
providing a good range of initial outputs, assuming that the exponents (expected outputs of quasipar-
tition distributions) are close to 0.5. Alternatively, bi’s maybe parametrized by a deep linear network,
a similar initialization is employed. See Appendix C.4.3 below for details.
C.4.3 Learning Linear/Convex Combinations with Deep Linear Networks
Deep linear networks have the same expressive power as regular linear models, but enjoy many
empirical and theoretical benefits in optimization (Saxe et al., 2013; Pennington et al., 2018; Huh
et al., 2021). Specifically, instead of directly learning a matrix ∈ Rm×n, a deep linear network (with
bias) of l layers learns a sequence of matrices
M1 ∈ Rm1×n	(186)
M2 ∈ Rm2×m1	(187)
..
..	..	(188)
Ml-1 ∈ Rml-1×ml-2	(189)
Ml ∈ Rm×ml-1	(190)
B ∈ Rm×n,	(191)
where the linear matrix can be obtained with
Ml Ml-1 ...M2 M1 +B,	(192)
and we require
min(m1, m2, . . . , ml-1) ≥ min(m, n).	(193)
In our case, the convex combination weights for the quasipartition distributions often need to be large,
in order to represent large quasimetric distances; in Poisson process mean measures with learnable
scales (e.g., the Gaussian-based measure described in Appendix C.2), the scales may also need to be
large to approximate particular quasipartitions (see Appendix C.3.1).
Therefore, we choose to use deep linear networks to optimize these parameters. In particular,
• For the convex combination weights for h quasipartition distributions,
-	When learning the convex combination weights {αi}i∈[h], We use a deep linear network to
parametrize a matrix ∈ R1×h (i.e., a linear map from Rh to R), which is then viewed as a
vector ∈ Rh and applied an element-wise square transform a → a2 to obtain non-negative
weights α ∈ [0, ∞)h;
-	When learning the bases for discounted quasimetric distances βi ’s (see Appendix C.4.2),
we use a deep linear network to parametrize a matrix ∈ Rh×1 , which is then viewed as
a vector ∈ Rh and applied an element-wise sigmoid transform a → σ(a) to obtain bases
β∈(0,1)h.
Note that here we parametrize a matrix ∈ Rh×1 rather than R1×h as above for αi ’s. The
reason for this choice is entirely specific to the initialization scheme we use (i.e., (fully-
connected layer weight matrix initialization, as discussed below). Here the interpretation
of a linear map is no longer true. If we use R1×h, the initialization method would lead to
the entries distributed with variance roughly 1/n, which only makes sense if they are then
added together. Therefore, we use Rh×1, which would lead to constant variance.
• For scales of the Poisson process mean measure, such as PQE-GG, we consider a slightly
different strategy.
Consider a PQE formulation with h × k independent Poisson processes, from which we form h
quasipartition distributions, each from k total orders parametrized by k Poisson processes. The
Poisson processes are defined on sets
{Ai,j}i∈[h],j∈[k],	(194)
use mean measures
{μi,j }i∈[h],j∈[k],	(195)
41
Published as a conference paper at ICLR 2022
and set parametrizations
{u → Ai,j (u)}i∈[h],j∈[k],	(196)
to compute quantities
μi,j(Aij(U) \ Aij(V))	forU ∈ Rd, v ∈ Rd, i ∈ [h],j ∈ [k].	(197)
Scaling each mean measure independently. Essentially, adding learnable scales (of mean
measures) w ∈ [0, ∞)h×k (or, equivalently, {wi,j ∈ [0, ∞)}i,j) gives a scaled set of measures
{wi,j ∙ μi,j }i∈[h] ,j∈[k].	(198)
This means that the quantities in Eq. (197) becomes respectively scaled as
Wi,j ∙ μi,j(Aij(u) \ Aij(v))	for U ∈ Rd,v ∈ Rd, i ∈ [h],j ∈ [k].	(199)
Convex combinations of all measures. However, we can be more flexible here, and allow not
just scaling each measure independently, but also convex combinations of all measures. Instead
of having w as a collection of h × k scalar numbers ∈ [0, ∞), we have a collection of (h × k)
vectors each having length (h × k) (or h × k-shape tensors)
{wi,j ∈ [0, ∞)h×k}i∈[h],j∈[k],	(200)
and have the quantities in Eq. (197) respectively scaled and combined as
X Wij,i0j0	∙	μi0j0 (Ai0,j0(u)	\ Ai0j0 (v))	for U ∈	Rd,v	∈ Rd, i ∈	[h],j ∈	[k].	(201)
i0,j0
Note that these still are valid Poisson processes for a PQE. Specifically, the new Poisson processes
now all use the same set parametrization (as the collection of original ones), with different
measures (as different weighted combinations of the original measures). This generalizes the
case where each mean measure is scaled independently (as w can be diagonal).
Therefore, we will apply this more general strategy using convex combinations of all measures.
Similarly to learning the convex combination weights of quasipartition distributions, we collapse
a deep linear network into a tensor ∈ Rh×k×h×k, and apply an element-wise square a → a2,
result of which is used as the convex combination weights w to ensure non-negativity.
Initialization. For initializing the matrices (M1, M2, . . . , Ml) of a deep linear network (Eq. (190)),
we use the standard weight matrix initialization of fully-connected layers in PyTorch (Paszke et al.,
2019). The bias matrix B (Eq. (191)) is initialized to all zeros.
When used for learning the bases for discounted quasimetric distances βi ’s (as described in Ap-
pendix C.4.2), we have a deep linear network parametrizing a matrix ∈ Rh×1, initialized in the same
way as above (including initializing B as all zeros). Consider the matrix up to before the last one:
M*，Mι-ι ∙ M2 Mi ∈ Rml-1×1.	(202)
M* is essentially a projection to be applied on each row of the last matrix Ml ∈ Rh×ml-1, to obtain
bi (which is then used to obtain bases βi , σ(bi)). Therefore, we simply rescale the M* subspace for
each row of Ml and keep the orthogonal space intact, such that the projections would be distributed
according to the distribution specified in Eq. (184):
UQT(0.52/h),GT(0.752/h)],,	(184)
which has good initial value properties, as shown in Appendix C.4.2.
C.4.4 CHOOSING h THE NUMBER OF QUASIPARTITION DISTRIBUTIONS AND k THE NUMBER
of Poisson Processes for Each Quasipartition Distribution
A PQE (class) is defined with h X k independent Poisson processes with means {μi,j }i∈[h],j∈[k] along
with h × k set parametrizations {Ai,j}i∈[h],j∈[k] . For k pairs of means and set parametrizations, we
obtain a random quasipartition. A mixture (convex combination) of the resulting h random quasipar-
titions gives the quasimetric. The choices of μ and A are flexible. In this work we explore PQE-LH
and PQE-GG as two options, both using essentially the same measure and parametrization across all
i, j (up to individual learnable scales). These two instantiations both perform well empirically. In
this section we aim to provide some intuition on choosing these two hyperparameters h and k.
h the Number of Quasipartition Distributions Theoretical result Thm. C.4 suggest thats, for a
quasimetric space with n elements, n2 quasipartition distributions suffice to learn a low distortion
42
Published as a conference paper at ICLR 2022
Triangle inequality =⇒
? ≤ d(a, b) + d(b, c) = 31
? ≥ d(a, b) - d(c, b) = 28
Figure 9:	The 3-element quasimetric space, and the training pairs.Training set contains all pairs
except for (a, c). Arrows show quasimetric distances (rather than edge weights of some graph).
embedding. Since this is a worst-case result, the practical scenario may require much fewer quasipar-
titions. For instance, Appendix C.3.4 shows that O(n) quasipartitions is sufficient for any quasimetric
space with a tree structure. In our experiments, h ∈ [8, 128] quasipartition distributions are used.
k the Number of Poisson Processes for Each Quasipartition Distribution (Random Partial
Order) It is well-known that such intersection of sufficiently many total orders can represent
any partial order (Trotter, 1995; Hiraguchi, 1951). This idea is equivalent with the dominance
drawing dimension of directed graphs (Ortali & Tollis, 2019), which concerns an order embedding of
the vertices to preserve the poset specified by the reachability relation. In this graph theoretical view,
several results are known. (Felsner et al., 2010) prove that planar graphs have at most 8 dimension.
(Ortali & Tollis, 2019) show that the dimension of any graph with n vertices is at most min(wp, 2),
where wP the maximum size of a set of incomparable vertices. A simpler and more fundamental
result can be traced to Hiraguchi from 1951:
Theorem C.6 (Hiraguchi’s Theorem (Hiraguchi, 1951; Bogart, 1973)). Let (X, P) be a partially
ordered set such that |X| ≥ 4. Then there exists a mapping f : X → Rb|X|/2c such that
∀x,y ∈ X,	XPy ^⇒ f(x) ≤ f(y) coordinate-wise.	(173)
Thm. C.6 states that 2 dimensions generally suffice for any poset of size n ≥ 4.
In our formulation, this means that using k = 2 Poisson processes (giving 2 random total orders)
will be maximally expressive. In practice, this is likely unnecessary and sometimes impractical. In
our experiments, we choose a small fixed number k = 4.
D	Experiment Settings and Additional Results
Computation power. All our experiments run on a single GPU and finish within 3 hours. GPUs
we used include NVIDIA 1080, NVIDIA 2080 Ti, NVIDIA 3080 Ti, NVIDIA Titan Xp, NVIDIA
Titan RTX, and NVIDIA Titan V.
D.1 Experiments from Sec. 3.2: A Toy Example
In Sec. 3.2 and Fig. 2, we show experiment results on a simple 3-element quasimetric space.
Quasimetric space. The quasimetric space has 3 elements with one-hot features ∈ R3 . Thequasi-
metric and training pairs are shown in Fig. 9.
Unconstrained network. The unconstrained network has architecture 6-128-128-32-1, with ReLU
activations.
43
Published as a conference paper at ICLR 2022
Unconstrained Network
(Output Distance)
(Training EMSE = 0.02 ± 0.06)
(a) Unconstrained network that di-
rectly predicts distance.
Unconstrained Network
(Output Square Root of Distance)
(Training ∕mse = 0.00 ± 0.02)
Asym. Dot Product
(Output Square Root of Distance)
(Training £MSE = 0.00 ± 0.00)
(b) Unconstrained network that pre-
dicts distance with a square a → a2
transform.
(c) Asymmetrical dot product that
predicts distance with a square a →
a2 transform.
∕ι Space Embedding
(Training £MSE = 58.83 ± 0.00)
c; I
Valid ,-----
Range l∣	1-
0	10
(d)	Metric embedding into an `1
space.
Euclidean Space Embedding
(Training £MSE = 58.83 ± 0.00)
0	10	20
Poisson Quasimetric Embedding
(TrainingfMSE = 0.02 ±0.07)
0	10	20
(e)	Metric embedding into an Eu-
clidean space.
(f)	Poisson Quasimetric Embedding
specified in Appendix D.1.
Figure 10:	Training different formulations to fit training pairs distances via MSE, and using them to
predict on the test pair. Plots show distribution of the prediction over 100 runs. Standard deviations
of the training error are shown.
Metric embedding. The embedding space is 32-dimensional, upon which corresponding metric is
applied. The encoder network has architecture 6-128-128-32, with ReLU activations.
Asymmetric dot products. The embedding space is 32-dimensional. The two inputs are encoded
with a different encoder of architecture 6-128-128-32, with ReLU activations. Then the dot product
of the two 32-dimensional vector is taken, which parametrizes a distance estimate
Poisson Quasimetric Embeddings. The embedding space is 32-dimensional, which parametrizes
8 quasimetric distributions, each from 4 independent Poisson processes using (scaled) Lebesgue
measure and half-lines. We use deep linear networks, as described in Appendix C.4.3. A deep
linear network (without bias) of architecture 8-32-32-1 parametrizes the convex combination weights
{αi}i∈[8] . Another deep linear network (without bias) of architecture 32-64-64-32 parametrizes
convex combination weights of the mean measures d ∈ [0, ∞)32×32. Note that these do not give
many more effective parameters to PQEs as they are equivalent with simple linear transforms.
Optimization. All models are trained w.r.t. MSE on distances with the Adam optimizer (Kingma
& Ba, 2014) with learning rate 0.0003 for 1000 iterations (without mini-batching since the training
set has size 8).
Additional results. Results with additional formulations (together with the ones presented in Fig. 2)
are shown in Fig. 10.
44
Published as a conference paper at ICLR 2022
D.2 Experiments from Sec. 6: Experiments
Triangle inequality regularizer. For methods that do not inherently respect triangle inequalities
(e.g., unconstrained networks and asymmetrical dot products), we explore training with a regularizer
that encourages following these inequalities. By sampling random triplets uniformly over the training
set, the regularizer is formulated as,
Ex,y,z max(0, γd(x,y)+d(y,z) - γd(x,z))2 ,	(203)
where the γ-discounted terms and the squared form allows easier balancing with the training loss,
which, across all experiments, are MSEs on some γ-discounted distances.
PQE settings. Across all experiments of this section, when given an encoder architecture mapping
input to an Rd latent space, we construct PQEs according to the following general recipe, to obtain the
two PQEs settings used across all experiments: PQE-LH (PQE with Lebesgue measure and half-lines)
and PQE-GG (PQE with Gaussian-based measure and Gaussian shapes, see see Appendix C.2):
•	(Assuming d is a multiple of 4,) We use h ,= d/4 quasipartition distributions, each given by
k , 4 Poisson processes;
•	A deep linear network (see Appendix C.4.3), is used for parametrizing the convex combination
weights α ∈ Rd/4 or the bases β ∈ Rd/4 (see Appendix C.4.2), we follow the initialization and
parametrization described in Appendix C.4.3, with hidden sizes [nhidden , nhidden , nhidden] (i.e., 4
matrices/layers), where nhidden , max(64, 21+dlog2 (d/4)e ).
•	For PQE-GG,
-	The learnable σ21 easure ∈ (0, ∞)d (one for each Poisson Process) is achieved by optimizing
the log variance, which is initialized as all zeros.
-	The Gaussian-based measures need learnable scales. We use a deep linear network to
parametrize the [0, ∞)d×d weights for the convex combinations of measures, as described in
Appendix C.4.3. Similarly, it has hidden sizes [nhidden, nhidden , nhidden] (i.e., 4 matrices/lay-
ers), where nhidden , max(64, 21+dlog2 de ).
Note that the PQEs add only a few extra effective parameters on top of the encoder (d for PQE-LH,
and d + d2 for PQE-GG), as the deep linear networks do not add extra effective parameters.
Mixed space metric embedding settings. Across all experiments of this section, when given an
encoder architecture mapping input to an Rd latent space, we construct the metric embedding into
mixed space as follows:
•	(Assuming d is a multiple of 4,) We use (1) a (d/2)-dimensional Euclidean space (2) a (d/4)-
dimensional `1 space, and a (d/4)-dimensional spherical distance space (without scale).
•	Additionally, we optimize three scalar values representing the log weights of the convex combi-
nation to mix these spaces.
DeepNorm and WideNorm method overview and parameter count comparison with PQEs.
Both DeepNorm and WideNorm parametrize asymmetrical norms. When used to approximate
quasimetrics, they are applied as d(x, y) , fAsymNorm(fEnc(x) - fEnc(y)), where fEnc is the en-
coder mapping from data space to an Rd latent space and fAsymNorm is either the DeepNorm or the
WideNorm predictor on that latent space (Pitis et al., 2020).
•	DeepNorm is a modification from Input Convex Neural Network (ICNN; Amos et al. (2017)),
with restricted weight matrices and activation functions for positive homogeneity (a requirement
of asymmetrical norms), and additional concave function for expressivity.
For an input latent space of Rd, consider an n-layer DeepNorm with width w (i.e., ICNN output
size) and the suggested intermediate MaxReLU activation and MaxMean final aggregation (see
(Pitis et al., 2020) for details of these functions). This DeepNorm predictor fDeepNorm (on latent
45
Published as a conference paper at ICLR 2022
space) has
#parmaters of fDeepNorm
= n × (d × w)
、-----------------{z------}
U matrices from input to each layer
+	(n — 1) X w2
W matrices between neighboring layer activations
+	n×w
1—^^}
intermediate MaxReLU activations
+	w × (4+5)
|----{------}
concave function (with 5 components) parameters
+ final MaxM|e{a1zn}aggregation,
which is on the order of O(nw max(d, w)). In the common case where the hidden size w is
chosen to be on the same magnitude as d, this becomes O(nd2).
• WideNorm is based on the observation that
x → kW ReLU(x :: —x)k2	(204)
is an asymmetric norm when W is non-negative, where :: denotes vector concatenation. Wide-
Norm then learns many such norms each with a different W matrix parameter, before (again)
feeding the norm values into a concave function and aggregating them together with MaxMean.
For an input latent space of Rd, consider a WideNorm with c such learned norms with W
matrices of shape R(≥2>d)0×w. This WideNorm predictor fWideNorm (on latent space), has
#parmaters of fWideNorm = c × (2d × w)
|------V-----}
W matrices
+	c× (4+5)
1	7-----}
concave function (with 5 components) parameters
+ final MaxM|e{a1zn}aggregation,
which is on the order of O(cdw). In the common case where both the number of components c
and the output size of each component (before applying the l2-norm) are chosen to be on the
same magnitude as d, this becomes O(d3).
For both DeepNorm and WideNorm, their parameter counts are much larger than the number of
effective parameters of PQEs (d for PQE-LH and d + d2 for PQE-GG). For a 256-dimensional latent
space, this difference can be on the order of 106 〜107.
DeepNorm and WideNorm settings. Across all experiments of this section, we evaluate 2 Deep-
Norm settings and 3 WideNorm settings, all derived from the experiment setting of the original paper
(Pitis et al., 2020). For both DeepNorm and WideNorm, we use MaxReLU activations, MaxMean
aggregation, and concave function of 5 components. For DeepNorm, we use 3-layer networks
with 2 different hidden sizes: 48 and 128 for the 48-dimensional latent space in random directed
graphs experiments, 512 and 128 for the 512-dimensional latent space in the large-scale social graph
experiments, 128 and 64 for the 128-dimensional latent space in offline Q-learning experiments. For
WideNorm, we components of size 32 and experiment with 3 different numbers of components: 32,
48, and 128.
Error range. Results are gathered across 5 random seeds, showing both averages and population
standard deviations.
D.2.1 Random Directed Graphs Quasimetric Learning
Graph generation. The random graph generation is controlled by three parameters d, ρun and ρdi.
d is the dimension of the vertex features. ρun specifies the fraction of pairs that should have at least
one (directed) edge between them. ρdi specifies the fraction of such pairs that should only have one
(directed) edge between them. Therefore, if ρun = 1, ρdi = 0, we have a fully connected graph; if
46
Published as a conference paper at ICLR 2022
ρun = 0.5, ρdi = 1, we have a graph where half of the vertex pairs have exactly one (directed) edge
between them, and the other half are not connected. For completeness, the exact generation procedure
for a graph of n vertices is the following:
1.	randomly add PUn ∙ n2 undirected edges, each represented as two opposite directed edges;
2.	optimize Rn×d vertex feature matrix using Adam (Kingma & Ba, 2014) w.r.t. Lalign (α =
2)+ 0.3 ∙ LUniform(t = 3) from (Wang & Isola, 2020), where each two node is considered a
positive pair if they are connected;
3.	randomly initialize a network f of architecture d-4096-4096-4096-4096-1 with tanh activa-
tions;
4.	for each connected vertex pair (u, v), obtain du→v , f (feature(u)) - f (feature(v)) and
dv→u = -du
→v;
5.	for each (u, v) such that du→v is among the top 1-Pdi/2 of such values (which is guaranteed
to not include both directions of the same pair due to symmetry of du→v), make v → u the
only directed edge between u and v.
We experiment with three graphs of 300 vertices and 64-dimensional vertex features:
•	Fig. 11: A graph generated with PUn = 0.15, Pdi = 0.85;
•	Fig. 12: A sparser graph generated with PUn = 0.05, Pdi = 0.85;
•	Fig. 13: A sparse graph with block structure by
1.	generating 10 small dense graphs of 30 vertices and 32-dimensional vertex features, using
PUn = 0.18, Pdi = 0.15,
2.	generating a sparse 10-vertex “supergraph” with 32-dimensional vertex features, using
PUn = 0.22, Pdi = 0.925,
3.	for each supergraph vertex
(a)	associating it with a different small graph,
(b)	for all vertices of the small graph, concatenate the supergraph vertex’s feature to the
existing feature, forming 64-dimensional vertex features for the small graph vertices,
(c)	picking a random representative vertex from the small graph,
4.	connecting all 10 representative vertices in the same way as their respective supergraph
vertices are connected in the supergraph.
Architecture. All encoder based methods (PQEs, metric embeddings, dot products) use 64-128-
128-128-48 network with ReLU activations, mapping 64-dimensional inputs to a 48-dimensional
latent space. Unconstrained networks use a similar 128-128-128-128-48-1 network, mapping con-
catenated the 128-dimensional input to a scalar output.
Data. For each graph, we solve the groundtruth distance matrix and obtain 3002 pairs, from which
we randomly sample the training set, and use the rest as the test set. We run on 5 training fractions
evenly spaced on the logarithm scale, from 0.01 to 0.7.
Training. We use 2048 batch size with the Adam optimizer (Kingma & Ba, 2014), with learning
rate decaying according to the cosine schedule without restarting (Loshchilov & Hutter, 2016) starting
from 10-4 to 0 over 3000 epochs. All models are optimized w.r.t. MSE on the γ-discounted distances,
with γ = 0.9. When running with the triangle inequality regularizer, 683 ≈ 2048/3 triplets are
uniformly sampled at each iteration.
Full results and ablation studies. Figs. 11 to 13 show full results of all methods running on all
three graphs. In Fig. 14, we perform ablation studies on the implementation techniques for PQEs
mentioned in Appendix C.4: outputting discounted distance and deep linear networks. On the simple
directed graphs such as the dense graph, the basic PQE-LH without theses techniques works really
well, even surpassing the results with both techniques. However, on graphs with more complex
structures (e.g., the sparse graph and the sparse graph with block structure), basic versions of PQE-
LH and PQE-GG starts to perform badly and show large variance, while the versions with both
techniques stably trains to the best results. Therefore, for robustness, we use both techniques in other
experiments.
47
Published as a conference paper at ICLR 2022
Groundtruth Distance Matrix
(3-layer 128-width)
PQE-GG
IoT
0.0
0.5
10-2
IO-3
0.0
0.5
10-2
IO-3
WideNorm
(32-component)
DeepNorm
(3-layer 48-width)
0.0
0.5
0.0
0.5
WideNorm
(48-component)
10-2
IO-3
0.0	0.5
WideNorm
(128-component)
10-2
IO-3
0.0	0.5
10-2
IO-3
Unconstrained Net
(Output Distance)
0.0	0.5
10-2
IO-3
Unconstrained Net
(Output Distance
via exp)
0.0	0.5
Unconstrained Net
(Output Distance
via Square)
10-2
IO-3
0.0	0.5
Unconstrained Net
Unconstrained Net (Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
0.0	0.5
0.0	0.5
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight =0.3
Unconstrained Net
Heldout MSE
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight =0.3
0.0	0.5	0.0	0.5
(Output Distance
via Square)
Δ-ineq. Reg. Weight =
Unconstrained Net
Unconstrained Net (Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
0.3 Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
0.0	0.5
0.0	0.5
0.0	0.5
Unconstrained Net
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
Unconstrained Net
Unconstrained Net (Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight =
IO-2
10-3
0.0	0.5
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight =
IO-2
10-3
0.0	0.5
10^2
10-3
0.0	0.5
10^2
10-3
10^2
10-3
0.0	0.5	0.0	0.5
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
Unconstrained Net
Unconstrained Net (Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 3 Δ-ineq. Reg. Weight = 3
IO-2
10-3
IO-2
10-3
0.0	0.5
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
0.0	0.5
10^2
10-3
0.0	0.5
10^2
10-3
0.0	0.5
0.0	0.5
Euclidean Metric Embedding
0.0	0.5
0.0	0.5
10-2
10-3
Spherical Distance
Metric Embedding
10-2
10-3
Mixed Space
Metric Embedding
0.0	0.5	0.0	0.5
Asym. Dot Product
Asym. Dot Product
(Output Distance)
0.0	0.5
10-2
10-3
Asym. Dot Product
(Output Distance
via exp)
0.0	0.5
(Output Distance
via Square)
10-2
10-3
0.0	0.5
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
0.0	0.5	0.0	0.5
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
10-2
10-3
0.0
0.5
10-2
10-3
0.0	0.5
10-2
10-3
0.0	0.5
10-2
10-3
0.0	0.5
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 1
0.0	0.5	0.0	0.5
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
0.0	0.5	0.0	0.5	0.0	0.5
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 3
10-2
10-3
0.0	0.5
10-2
10-3
0.0	0.5
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 3 Δ-ineq. Reg. Weight = 3
0.0	0.5	0.0	0.5	0.0	0.5
Figure 11: A dense graph. Individual plots on the right show standard deviations.
48
Published as a conference paper at ICLR 2022
lθ-ɪ
Heldout MSE
Groundtruth Distance Matrix
(67.23% pairs are unreachable)
PQE-LH
10-2
0.0
0.5
PQE-GG
10-2
0.0
0.5
10-2
WideNorm
(32-component)
WideNorm
(48-component)
WideNorm
(128-component)
DeepNorm
(3-layer 48-width)
DeepNorm
(3-layer 128-width)
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
■			
	10^2 -		
Unconstrained Net
(Output y-Discounted Distance
10-2
Unconstrained Net
(Output Distance)
0.0
0.5
10-2
Unconstrained Net
(Output Distance
via exp)
0.0
0.5
10-2
Unconstrained Net
(Output Distance
via Square)
0.0
0.5
Unconstrained Net
(Output /-Discounted Distance)
10-2
0.0
0.5
via Sigmoid σ)
10-2
0.0
0.5
10-2
10-3
10-2
0.7
0.0	0.1	0.2	0.3	0.4	0.5	0.6
Training Set Fraction
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight =0.3
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight =0.3
Unconstrained Net
(Output Distance
via Square)
Unconstrained Net
Unconstrained Net
(Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
IO-2 -		IO-2 -		10^2 -	
					
0.0
0.5
0.0
0.5
0.0
0.5
Δ-ineq. Reg. Weight =0.3 Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
10-2
	10^2 -	
		
0.5
0.0
0.5
0.0
Unconstrained Net
Unconstrained Net
(Output y-Discounted Distance
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight = 1
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 1
Unconstrained Net
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
	IO-2 -		10^2 -		10^2 -		10^2 -	
								
-一						~~≡ :				-一
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
Unconstrained Net
Unconstrained Net
(Output y-Discounted Distance
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight = 3
(Output y-Discounted Distance)
Δ-ineq. Reg. Weight = 3
via Sigmoid σ)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
0.0
0.5
0.5
0.0
0.5
0.0
0.5
0.0
0.5
IO-2 -		IO-2 -		10^2 -		10^2 -	
0.0
∕ι Metric Embedding
Euclidean Metric Embedding
Spherical Distance
Metric Embedding
Mixed Space
Metric Embedding
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
IO-2 -		IO-2 -		10^2 -	
Asym. Dot Product
(Output y-Discounted Distance
Asym. Dot Product
(Output /-Discounted Distance)
via Sigmoid σ)
Dot Product
ut Distance
via exp)
Asym. Dot Product
(Output Distance)
Asym. Dot Product
(Output Distance
via Square)
0.5
0.5
0.0
0.5
0.0
0.5
Asym. Dot Product
(Output y-Discounted Distance)
via Sigmoid σ)
Asym. Dot Product
(Output Distance
via Square)
Asym. Dot Product
(Output y-Discounted Distance
IO-2 -	
	
0.0
	10^2 -		10^2 -	
				
Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 0.3
Dot Product
ut Distance
via exp)
Δ-ineq. Reg. Weight = 0.3
10^2 -		IO-2 -		10^2 -		Io-2 -	
							
0.0
0.5
0.0
0.5
0.0
0.5
0.0
IO-2
0.5
0.0
0.5
Asym. Dot Product
Asym. Dot Product
(Output y-Discounted Distance
10-2
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 1
Dot Product
ut Distance
via exp)
Δ-ineq. Reg. Weight = 1
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
	IO-2 -		10^2 -		10^2 -		10^2 -	
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
Asym. Dot Product
Asym. Dot Product
(Output y-Discounted Distance
10-2
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 3
Dot Product
ut Distance
via exp)
Δ-ineq. Reg. Weight = 3
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 3 Δ-ineq. Reg. Weight = 3
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
0.5
Figure 12: A sparse graph. Individual plots on the right show standard deviations.
	10^2 -		10^2 -		IO-2 -	
49
Published as a conference paper at ICLR 2022
Heldout MSE
Groundtruth Distance Matrix
(67.13% pairs are unreachable)
8
6
4
2
O
10-1
10-2
10-3
10-4
	
	——PQE 	 Unconstrained Network 	 Metric Embedding 	Asym. Dot Product
I I I	
Wy	'	 WideNorm	
	
	
	
0.7
0.0	0.1	0.2	0.3	0.4	0.5	0.6
Training Set Fraction
10
(3-layer 128-width)
10-1
10-1
10-3
0.0
0.5
10-1
10-3
Unconstrained Net
(Output Distance)
0.0
0.5
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight =0.3
10-3
0.0
0.5
10-1
10-3
WideNorm
(32-component)
0.0
0.5
10-1
10-3
WideNorm
(48-component)
0.0
0.5
10-1
10-3
WideNorm
(128-component)
0.0
0.5
Unconstrained Net
(Output /-Discounted Distance
Unconstrained Net
(Output /-Discounted Distance)
via Sigmoid σ)
10-1
10-3
Unconstrained Net
(Output Distance
via Square)
Unconstrained Net
(Output Distance
via exp)
	10^1 -		10^1 -		10^1 -	
	10^3 -		10^3 -		10^3 -	口
0.5
0.0
0.5
0.0
0.5
0.0
0.5
0.0
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight =0.3
Unconstrained Net
(Output Distance
via Square)
Unconstrained Net
Unconstrained Net
(Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
lθ-ɪ -		lθ-ɪ -				10^1 -	
IO-3 .		IO-3 .			匚	10^3 -	
0.0
0.5
0.0
0.5
0.0
0.5
Δ-ineq. Reg. Weight =0.3 Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
10-1
10-3
	10^1 -	
	10^3 -	
		
0.5
0.0
0.5
0.0
Unconstrained Net
Unconstrained Net
(Output /-Discounted Distance
10-1
10-3
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight = 1
0.0
0.5
10-1
10-3
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 1
10-1
10-3
0.0
0.5
Unconstrained Net
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
(Output /-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
	10^1 -		10^1 -	
□	10^3 -		10^3 -	匚
0.5
0.0
0.5
0.0
0.5
0.0
Unconstrained Net
Unconstrained Net
(Output Distance)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 3
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
Unconstrained Net (Output /-Discounted Distance
(Output /-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 3 Δ-ineq. Reg. Weight = 3
lθ-ɪ -		lθ-ɪ -		10^1 -		10^1 -		10^1 -	
IO-3 -		IO-3 -		10^3 -		10^3 -		10^3 -	
0.0	0.5		0.0	0.5		0.0	0.5		0.0	0.5		0.0	0.5	
Spherical Distance
Metric Embedding
Euclidean Metric Embedding
lθ-ɪ -	一 一
Io-3 .	
0.0
0.5
∕ι Metric Embedding
lθ-ɪ -	k——	—
Io-3 .	
0.0	0.5
10-1
10-3
Mixed Space
Metric Embedding
ιo^1 -	
10^3 -	
0.0	0.5
0.0	0.5
Asym. Dot Product
(Output Distance)
10-1
10-3
Asym. Dot Product	Asym. Dot Product	Asym. Dot Product
(Output Distance	(Output Distance	Asym. Dot Product (Output /-Discounted Distance
via exp)	via Square) (Output /-Discounted Distance) via Sigmoid σ)
lθ-ɪ - Io-3 .		ιo^1 - 10^3 -		ιo^1 - 10^3 -		ιo^1 - 10^3 -	
0.0	0.5	0.0	0.5	0.0	0.5	0.0	0.5
0.0	0.5
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 0.3
lθ-ɪ -	
Io-3 .	
0.0	0.5
10-1
10-3
10-1
10-3
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 0.3 Δ-ineq. Reg. Weight = 0.3
ιo^1 -		ιo^1 -	
10^3 -		10^3 -	
0.0	0.5	0.0	0.5
0.0	0.5	0.0	0.5
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 1 Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 1
lθ-ɪ -		lθ-ɪ -		ιo^1 -	
Io-3 .	匚	Io-3 .		10^3 -	匚
0.0	0.5
0.0	0.5	0.0	0.5
10-1
10-3
0.0	0.5
0.0	0.5
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
(Output Distance
via exp)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
(Output Distance
via Square)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
Asym. Dot Product (Output y-Discounted Distance
(Output y-Discounted Distance) via Sigmoid σ)
Δ-ineq. Reg. Weight = 3 Δ-ineq. Reg. Weight = 3
10-1
10-3
0.0	0.5
10-1
10-3
0.0	0.5
10-1
10-3
0.0	0.5
10-1
10-3
0.0	0.5
10-1
10-3
0.0	0.5
Figure 13: A sparse graph with block structure. Individual plots on the right show standard deviations.
50
Published as a conference paper at ICLR 2022
Ablation Study of PQE-LH on the Dense Graph
Ablation Study of PQE-GG on the Dense Graph
Heldout MSE	Heldout MSE	Heldout MSE
——PQE-LH
wsw mop"H
Ablation Study of PQE-LH on the Sparse Graph
——PQE-LH
---PQE-LH - Discounted Dist.
——PQE-LH - Discounted Dist. - Deep Linear Net
Groundtruth Distance Matrix
30
20
10
O
wsw mop"H
Ablation Study of PQE-LH on the Sparse Graph with Block Structure
——PQE-LH
---PQE-LH - Discounted Dist.
——PQE-LH - Discounted Dist. - Deep Linear Net
Groundtruth Distance Matrix
——PQE-GG
——PQE-GG - Discounted Dist
—— PQE-GG - Discounted Dist - Deep Linear Net
Groundtnjth Distance Matrix
20
10
O
Ablation Study of PQE-GG on the Sparse Graph
——PQE-GG
——PQE-GG - Discounted Dist.
—— PQE-GG - Discounted Dist. - Deep Linear Net
Graundtnjth Distance Matrix
30
20
10
O
0.0	0.2 0Λ 0.6
Training Fraction
Ablation Study of PQE-GG on the Sparse Graph with Block Structure
wsw mop"H
Figure 14: Ablation studies of PQE-LH and PQE-GG on three random graphs.
D.2.2 Large-Scale Social Graphs Quasimetric Learning
Data source. We choose the Berkeley-StanfordWebGraph (Leskovec & Krevl, 2014) as the large-
scale directed social graph, which consists of 685,230 pages as nodes, and 7,600,595 hyperlinks
as directed edges. Additionally, we also use the Youtube social network (Leskovec & Krevl, 2014;
Mislove et al., 2007) as a undirected social graph, which consists of 1,134,890 users as nodes, and
2,987,624 friendship relations as undirected edges. Both datasets are available from the SNAP
website (Leskovec & Krevl, 2014) under the BSD license.
Data processing. For each graph, we use node2vec to obtain 128-dimensional node features
(Grover & Leskovec, 2016). Since the graph is large, we use the landmark method (Rizi et al., 2018)
to construct training and test sets. Specifically, we randomly choose 150 nodes, called landmarks,
and compute the distances between these landmarks and all nodes. For directed graph, this means
computing distances of both directions. From the obtained pairs and distances, we randomly sample
2,500,000 pairs to form the training set. Similarly, we form a test set of 150,000 from a disjoint set of
50 landmarks. For the undirected graph, we double the size of each set by reversing the pairs, since
the distance is symmetrical.
Architecture. All encoder based methods (PQEs, metric embeddings, dot products) use 128-
2048-2048-2048-512 network with ReLU activations and Batch Normalization (Ioffe & Szegedy,
2015) after each activation, mapping 128-dimensional inputs to a 512-dimensional latent space.
Unconstrained networks use a similar 256-2048-2048-2048-512-1 network, mapping concatenated
the 256-dimensional input to a scalar output.
Training. We use 1024 batch size with the Adam optimizer (Kingma & Ba, 2014), with learning
rate decaying according to the cosine schedule without restarting (Loshchilov & Hutter, 2016) starting
from 10-4 to 0 over 80 epochs. All models are optimized w.r.t. MSE on the γ-discounted distances,
with γ = 0.9. When running with the triangle inequality regularizer, 342 ≈ 1024/3 triplets are
uniformly sampled at each iteration.
51
Published as a conference paper at ICLR 2022
				MSE w.r.t.	L1 Error		Prediction d	
Method Family	Formulation			Y-discounted	when true		when true	
				distances (×10-3) J	d<∞J		d=∞↑	
PQEs	PQE-LH			3.0427 ± 0.1527	1.6263 ± 0.0550		69.9424 ± 0.4930	
	PQE-GG			3.9085 ± 0.1258	1.8951 ± 0.0336		101.8240 ± 10.3970	
	Output distance	d(x, y)	，f (X,y)	3.0862 ± 0.0392	2.1151 ± 0.0241		59.5243 ± 0.3700	
Unconstrained Nets	Output distance via exp(∙)	d(X,y)	, exp(f(X, y))	3.3541 ± 0.1759	1.0090 × 1023 ± 2.0179 ×	1023	5.3583 × 105 ± 1.0582 ×	106
(without Triangle Inequality	Output distance via squaring a → a2	d(x,y	, (f(X, y))2	4.5663 ± 0.2294	3.3459 ± 0.2494		68.2613 ± 11.6061	
Regularizer)	Output γ -discounted distance	Yd(X,y)	, f(X, y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X,y))	3.1823 ± 0.1133	∞ ± NaN		65.8630 ± 0.4287	
	Output distance	d(X,y)	，f (X,y)	2.8128 ± 0.0625	2.2109 ± 0.0341		61.3709 ± 0.3936	
Unconstrained Nets	Output distance via exp(∙)	d(X,y)	, exp(f(X, y))	2.9344 ± 0.0455	∞ ± NaN		∞ ± NaN	
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y	, (f(X, y))2	4.9947 ± 0.4198	16.5445 ± 29.3175		58.9205 ± 6.4216	
Regularizer Weight = 0.3)	Output γ -discounted distance	Yd(X,y)	, f(X, y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X,y))	2.9178 ± 0.1351	∞ ± NaN		∞ ± NaN	
	Output distance	d(X,y)	，f (X,y)	3.0481 ± 0.1272	2.3729 ± 0.1378		60.4040 ± 0.1890	
Unconstrained Nets	Output distance via exp(∙)	d(X,y)	, exp(f(X, y))	3.0161 ± 0.0718	∞ ± NaN		3.1289 × 1016 ± 6.2579 ×	1016
(Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X, y))2	4.4921 ± 0.3534	3.6930 ± 0.4896		90.6206 ± 66.5704	
Regularizer Weight = 1)	Output γ -discounted distance	Yd(X,y)	, f(X, y)	4.4046 ± 0.5167	2.7873 ± 0.0770		31.3195 ± 0.9929	
	Output γ-discounted distance via sigmoid σ(∙)	γd(χ,y)	,σ(f (X,y))	2.9314 ± 0.1022	2.2634 ± 0.1147		∞ ± NaN	
	Output distance	d(X,y)	，f (X,y)	5.2955 ± 0.5279	3.8060 ± 0.2908		58.1193 ± 0.4383	
Unconstrained Nets	Output distance via exp(∙)	d(X, y)	, exp(f(X, y))	3.5713 ± 0.2002	212.5421 ± 416.9256		∞ ± NaN	
(Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X, y))2	4.3745 ± 0.3709	2.9491 ± 0.2228		53.1119 ± 5.5452	
Regularizer Weight = 3)	Output γ -discounted distance	Yd(X,y)	, f(X, y)	7.3416 ± 0.6486	3.5232 ± 0.1352		26.9200 ± 0.4697	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X,y))	3.5818 ± 0.3565	∞ ± NaN		65.7709 ± 0.8646	
	Output distance	d(X,y)	，f(X)Tg(y)	3.1622 × 1019 ±NaN	23.4270 ± NaN		0.1529 ± NaN	
Asym. Dot Products	Output distance via exp(∙)	d(X, y)	, exp(f(X)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN	
(without Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X)Tg(y))2	48.1056 ± 0.0056	2.5195 × 1011 ± 2.1751 ×	1011	2.6794 × 1011 ± 2.5398 ×	1011
Regularizer)	Output γ -discounted distance	Yd(X,y)	, f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X)Tg(y))	48.1073 ± 0.0112	∞ ± NaN		∞ ± NaN	
	Output distance	d(X,y)	，f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
Asym. Dot Products	Output distance via exp(∙)	d(X,y)	, exp(f(X)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN	
(Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X)Tg(y))2	48.1041 ± 0.0035	1.9498 × 1011 ± 7.9641 ×	1010	1.6049 × 1011 ± 3.7099 ×	1010
Regularizer Weight = 0.3)	Output γ -discounted distance	,d(X,y)	, f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X)Tg(y))	48.1103 ± 0.0110	∞ ± NaN		∞ ± NaN	
	Output distance	d(X, y)	，f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
Asym. Dot Products	Output distance via exp(∙)	d(X,y)	, exp(f(X)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN	
(Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X)Tg(y))2	48.1021 ± 0.0002	2.2986 × 1011 ± 9.1970 ×	1010	2.5002 × 1011 ± 1.4464 ×	1011
Regularizer Weight = 1)	Output γ -discounted distance	Yd(X,y)	, f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X)Tg(y))	58.4894 ± 23.2224	∞ ± NaN		∞ ± NaN	
	Output distance	~1^, Γ d(X, y)	，f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
Asym. Dot Products	Output distance via exp(∙)	d(X,y)	, exp(f(X)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN	
(Triangle Inequality	Output distance via squaring a → a2	d(X,y)	, (f(X)Tg(y))2	48.1031 ± 0.0020	2.3522 × 1011 ± 2.6429 ×	1011	1.7025 × 1011 ± 1.0700 ×	1011
Regularizer Weight = 3)	Output γ -discounted distance	Yd(X,y)	, f(X)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN	
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)	,σ(f (X)Tg(y))	48.3034 ± 0.4485	∞ ± NaN		∞ ± NaN	
	Euclidean space	d(X,y)	，kf (x) - f(y)k2	17.5952 ± 0.2667	7.5399 ± 0.0742		53.8500 ± 3.8430	
Metric Embeddings	`1 space	d(X,y)	, kf(X) - f(y)k1	18.0521 ± 0.3546	7.1154 ± 0.1835		66.2507 ± 3.3308	
	Spherical distance space w/ learnable scale α	d(X,y)	,α ∙ arccos( JTftf(Sk)	19.2990 ± 0.2032	6.9545 ± 0.0887		32.1458 ± 0.4562	
	Mixing above three spaces w/ learnable weights			17.8312 ± 0.3099	7.3493 ± 0.1086		51.7481 ± 3.6248	
DeepNorms	3-layer 128-width			7.0862 ± 0.3170	2.4498 ± 0.0617		111.2209 ± 2.5045	
	3-layer 512-width			5.0715 ± 0.1348	2.0853 ± 0.0633		120.0452 ± 4.3525	
	32-component (each of size 32)			3.5328 ± 0.2120	1.7694 ± 0.0213		124.6580 ± 2.8678	
WideNorms	48-component (each of size 32)			3.6842 ± 0.2385	1.8081 ± 0.0680		122.6833 ± 5.5026	
	128-component (each of size 32)			3.8125 ± 0.2331	1.8096 ± 0.0765		128.5427 ± 5.1412	
Table 2: Quasimetric learning on the large-scale directed Berkeley-StanfordWebGraph.
52
Published as a conference paper at ICLR 2022
			MSE w.r.t.	L1 Error		Prediction d
Method Family	Formulation		Y -discounted	when true		when true
			distances (× 10-3) J	d<∞J		d=∞↑
PQEs	PQE-LH		2.4400 ± 0.0695	0.6480 ± 0.0119		NaN ± NaN
	PQE-GG		2.5895 ± 0.0318	0.6697 ± 0.0042		NaN ± NaN
	Output distance	d(χ,y)，f(χ,y)	1.4883 ± 0.0168	0.5084 ± 0.0029		NaN ± NaN
Unconstrained Nets	Output distance Via exp(∙)	d(x,y)，exp(f(x,y))	1.5223 ± 0.0160	0.4910 ± 0.0151		NaN ± NaN
(without Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f(χ,y))2	2.2955 ± 1.1674	0.6185 ± 0.1409		NaN ± NaN
Regularizer)	Output γ-discounted distance	Yd(X,y)，f(x,y)	1.5069 ± 0.0228	0.4975 ± 0.0211		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x,y))	1.4802 ± 0.0197	0.5082 ± 0.0036		NaN ± NaN
	Output distance	d(χ,y)，f(χ,y)	1.5009 ± 0.0208	0.5107 ± 0.0032		NaN ± NaN
Unconstrained Nets	Output distance via exp(∙)	d(x,y)，exp(f(x,y))	1.5206 ± 0.0444	0.4935 ± 0.0098		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f(χ,y))2	1.7398 ± 0.3896	0.5488 ± 0.0600		NaN ± NaN
Regularizer Weight = 0.3)	Output γ-discounted distance	Yd(X,y)，f(x,y)	1.5005 ± 0.0148	0.4986 ± 0.0121		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x,y))	1.4851 ± 0.0168	0.5089 ± 0.0026		NaN ± NaN
	Output distance	d(χ,y)，f (χ,y)	1.4999 ± 0.0243	0.5107 ± 0.0046		NaN ± NaN
Unconstrained Nets	Output distance via exp(∙)	d(x,y)，exp(f(x,y))	1.5224 ± 0.0376	0.4948 ± 0.0169		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f(χ,y))2	1.8875 ± 0.5078	0.5692 ± 0.0683		NaN ± NaN
Regularizer Weight = 1)	Output γ-discounted distance	Yd(X,y)，f(x,y)	1.4769 ± 0.0176	0.4919 ± 0.0128		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x,y))	1.4846 ± 0.0115	0.5088 ± 0.0021		NaN ± NaN
	Output distance	d(χ,y)，f (χ,y)	1.4939 ± 0.0110	0.5099 ± 0.0018		NaN ± NaN
Unconstrained Nets	Output distance via exp(∙)	d(x,y)，exp(f(x,y))	1.5154 ± 0.0389	0.4871 ± 0.0174		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f(χ,y))2	2.4747 ± 1.0850	0.6505 ± 0.1357		NaN ± NaN
Regularizer Weight = 3)	Output γ-discounted distance	Yd(X,y)，f(x,y)	1.4915 ± 0.0127	0.4983 ± 0.0160		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x,y))	1.4829 ± 0.0153	0.5084 ± 0.0029		NaN ± NaN
	Output distance	d(χ,y)，f (χ)Tg(y)	2633.7907 ± NaN	11.3879 ±NaN		NaN ± NaN
Asym. Dot Products	Output distance via exp(∙)	d(x,y)，exp(f (x)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN
(without Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f (χ)Tg(y))2	339.1550 ± 0.0022	7.8948 × 1011 ± 7.4010 ×	1011	NaN ± NaN
Regularizer)	Output γ-discounted distance	Yd(X,y) , f (x)Tg(y)	2.6920 ± 1.2655	0.7062 ± 0.2156		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x)Tg(y))	182.2068 ± 1.2382	∞ ± NaN		NaN ± NaN
	Output distance	d(χ,y)，f (χ)Tg(y)	9.9748 × 105 ± NaN	8.1867 ± NaN		NaN ± NaN
Asym. Dot Products	Output distance via exp(∙)	d(x,y)，eχp(f (x)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f (χ)Tg(y))2	339.1560 ± 0.0010	6.8658 × 1011 ± 3.4985 ×	1011	NaN ± NaN
Regularizer Weight = 0.3)	Output γ-discounted distance	Yd(X,y) , f (x)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x)Tg(y))	183.3337 ± 1.0384	∞ ± NaN		NaN ± NaN
	Output distance	d(χ,y)，f (χ)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN
Asym. Dot Products	Output distance via exp(∙)	d(x,y)，eχp(f (x)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f(χ)Tg(y))2	339.1552 ± 0.0021	7.4588 × 1011 ± 3.7277 ×	1011	NaN ± NaN
Regularizer Weight = 1)	Output γ-discounted distance	Yd(X,y) , f (x)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x)Tg(y))	191.0928 ± 9.7137	∞ ± NaN		NaN ± NaN
	Output distance	d(χ,y)，f (χ)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN
Asym. Dot Products	Output distance via exp(∙)	d(x,y)，eχp(f (x)Tg(y))	NaN ± NaN	NaN ± NaN		NaN ± NaN
(Triangle Inequality	Output distance via squaring a → a2	d(χ,y)，(f (χ)Tg(y))2	339.1556 ± 0.0020	9.0283 × 1011 ± 6.0203 ×	1011	NaN ± NaN
Regularizer Weight = 3)	Output γ-discounted distance	Yd(X,y) , f (x)Tg(y)	NaN ± NaN	NaN ± NaN		NaN ± NaN
	Output γ-discounted distance via sigmoid σ(∙)	Yd(X,y)，σ(f(x)Tg(y))	228.0300 ± 37.0632	∞ ± NaN		NaN ± NaN
	Euclidean space	d(χ,y) , kf(X) - f(y)k2	1.3131 ± 0.0671	0.4833 ± 0.0128		NaN ± NaN
Metric Embeddings	`1 space Spherical distance space w/ learnable scale α	d(X,y) , kf(X) - f(y)kι d(X，y) , α ∙ arcc0s( ffXf)k2 )	3.5993 ± 1.5986 6.7731 ± 0.1915	0.7787 ± 0.1842 1.0829 ± 0.0177		NaN ± NaN NaN ± NaN
	Mixing above three spaces w/ learnable weights		2.1014 ± 0.0685	0.5923 ± 0.0109		NaN ± NaN
DeepNorms	3-layer 128-width		8.0192 ± 0.2476	1.1834 ± 0.0213		NaN ± NaN
	3-layer 512-width		5.4366 ± 0.0855	0.9666 ± 0.0072		NaN ± NaN
	32-component (each of size 32)		3.0841 ± 0.0667	0.7272 ± 0.0068		NaN ± NaN
WideNorms	48-component (each of size 32)		3.0438 ± 0.1322	0.7247 ± 0.0173		NaN ± NaN
	128-component (each of size 32)		2.9964 ± 0.1363	0.7173 ± 0.0166		NaN ± NaN
Table 3: Metric learning on the large-scale undirected Youtube graph. This graph does not have
unreachable pairs so the last column is always NaN.
53
Published as a conference paper at ICLR 2022
Full results. Tables 2 and 3 show full results of distance learning on these two graphs. On the
directed Berkeley-StanfordWebGraph, PQE-LH performs the best (w.r.t. discounted distance MSE).
While PQE-GG has larger discounted distance MSE than some other baselines, it accurately predicts
finite distances and outputs large values for unreachable pairs. On the undirected Youtube graph,
perhaps as expected, metric embedding methods have an upper hand, with the best performing method
being an Euclidean space embedding. Notably, DeepNorms and WideNorms do much worse than
PQEs on this symmetric graph.
D.2.3 Offline Q-Learning
As shown in Proposition A.4 and Remark A.5, we know that a quasimetric is formed with the optimal
goal-reaching plan costs in a MDP M = (S, A, R, P, γ) where each action has unit cost (i.e.,
negated reward). The quasimetric is defined on X , S ∪ (S × A).
Similarly, Tian et al. (2020) also make this observation and propose to optimize a distance function
by Q-learning on a collected set of trajectories. The optimized distance function (i.e., Q-function) is
then used with standard planning algorithms such as the Cross Entropy Method (CEM) (De Boer
et al., 2005). The specific model they used is an unconstrained network f : (s, a, s0) → R, outputting
discounted distances (Q-values).
Due to the existing quasimetric structure, we explore using PQEs as the distance function formulation.
We mostly follow the algorithm in Tian et al. (2020) except for the following minor differences:
•	Tian et al. (2020) propose to sample half of the goal from future steps of the same trajectory, and
half of the goal from similar states across the entire dataset, defined by a nearest neighbor search.
For simplicity, in the latter case, we instead sample a random state across the entire dataset.
•	In Tian et al. (2020), target goals are defined as single states, and the Q-learning formulation
only uses quantities distances from state-action pairs (s, a) ∈ S × A to states s0: d((s, a), s0).
However, if we only train on d((s, a), s0), quasimetric embeddings might not learn much about
the distance to state-action pairs, or from states, because it may simply only assign finite distances
to d((s, a), s0), and set everything else to infinite. To prevent such issues, we choose to use
state-action pairs as target goals, by adding a random action. Then, the embedding methods only
need to embed state-action pairs.
In planning when the target is actually a single goal s0 ∈ S, we use the following distance/Q-
function
d((S, a), SO), -1+∣A∣ x d((S, a), (S0, aO)).	(205)
|A| a0∈A
Such a modification is used for all embedding methods (PQEs, metric embeddings, asymmetrical
dot products). For unconstrained networks, we test both the original formulation (of using single
state as goals) and this modification.
Environment. The environment is a grid-world with one-way doors, as shown in of Fig. 15, which
is built upon gym-minigrid (Chevalier-Boisvert et al., 2018) (a project under Apache 2.0 License).
The agent has 4 actions corresponding to moving towards 4 directions. When it moves toward a
direction that is blocked by a wall or an one-way door, it does not move. States are represented as
18-dimensional vectors, containing the 2D location of the agent (normalized to be within [-1, 1]2).
The other dimensions are always constant in our enviroment as they refer to information that can not
be changed in this particular environment (e.g., the state of the doors). The agent always starts at
a random location in the center room (e.g., the initial position of the red triangle in Fig. 15). The
environment also defines a goal sampling distribution as a random location in one of the rooms on
the left or right side. Note that this goal distribution is only used for data collection and evaluation.
In training, we train goal-conditional policies using the goal sampling mechanism adapted from Tian
et al. (2020), as described above.
Training trajectories. To collect the training trajectories, we use an -greedy planner with
groundtruth distance toward the environment goal, with a large = 0.6. Each trajectory is capped to
have at most 200 steps.
54
Published as a conference paper at ICLR 2022
[success]
Figure 15: Grid-world offline Q-learning average planning success rates. Right shows the environ-
ment.
Architecture. All encoder based methods (PQEs, metric embeddings, dot products) use 18-2048-
2048-2048-1024 network with ReLU activations and Batch Normalization (Ioffe & Szegedy, 2015)
after each activation, mapping a 18-dimensional state to four 256-dimensional latent vectors, cor-
responding to the embeddings for all four state-action pairs. Unconstrained networks use a similar
architecture and take in concatenated 36-dimensional inputs. With the original formulation with
states as goals, we use a 36-2048-2048-2048-256-4 network to obtain a R|A| output, representing
the distance/Q-values from each state-action pair to the goal; with the modified formulation with
state-action pairs as goals, We use a 36-2048-2048-2048-256-16 network to obtain a RlAl×lAl output.
Training. We use 1024 batch size with the Adam optimizer (Kingma & Ba, 2014), with learning
rate decaying according to the cosine schedule without restarting (Loshchilov & Hutter, 2016) starting
from 10-4 to 0 over 1000 epochs. Since we are running Q-learning, all models are optimized w.r.t.
MSE on the γ-discounted distances, with γ = 0.95. When running with the triangle inequality
regularizer, 341 ≈ 1024/3 triplets are uniformly sampled at each iteration.
Planning details. To use the learned distance/Q-function for planning towards a given goal, we
perform greedy 1-step planning, where we always select the best action in A according to the learned
model, without any lookahead. In each of 50 runs, the planner is asked to reach a goal given by the
environment within 300 steps. The set of 50 initial location and goal states is entirely decided by the
seed used, regardless of the model. We run each method 5 times using the same set of 5 seeds.
Full results. Average results across 5 runs are shown in Fig. 15, with full results (with standard
deviations) shown in Fig. 16. Planning performance across the formulations vary a lot, with PQEs and
the Euclidean metric embedding being the best and most data-efficient ones. Using either formulation
(states vs. state-action pairs as goals) does not seem to affect the performance of unconstrained
networks. We note that the the asymmetrical dot product formulation outputting discounted distance
is similar to Universal Value Function Approximators (UVFA) formulation (Schaul et al., 2015); the
unconstrained network outputting discounted distance with states as goals is the same formulation as
the method from Tian et al. (2020).
55
Published as a conference paper at ICLR 2022
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
Unconstrained Net
State as Goals
(Output Distance)
250 500 750 1000
1.00-	
0.75-	
0.50-	
0.25-	
0.00-	-一一 二
0	250 500 750 1000
1.00-
0.75-
0.50-
0.25-
0.00-.
WideNorm
(128-component)
		1.00
	0.75
	0.50
..一 一		0.25
	0.00
0	250 500 750 1000
WideNorm
""component)
250 500 750 1000
WideNorm
1.00
0.75
0.50
0.25
Unconstrained Net
State as Goals
(Output Distance via exp)
Unconstrained Net
State as Goals
(Output Distance via Square)
1.00-	-
0.75-	
0.50-	
0.25-	-	
0.00-	
	
Unconstrained Net
State as Goals
(Output ^Discounted Distance)
1.00-		1.00
0.75-	/	0.75
0.50-		0.50
0.25-	L 一 一.	0.25
0.00-	二	0.00
Unconstrained Net
State as Goals
(Output y-Discounted Distance
via Sigmoid σ)
0.00
0.00
	0	250 500 750 1000		0	250 500 750 1000 Unconstrained Net State as Goals (Output Distance via exp) Δ-ineq. Reg. Weight = 0.3		0	250 500 750 1000 Unconstrained Net State as Goals (Output Distance via Square) Δ-ineq. Reg. Weight = 0.3	0	250 500 750 1000 Unconstrained Net State as Goals (Output『DiscOUnted Distance)		0	250 500 750 1000 Unconstrained Net State as Goals (Output y-Discounted Distance	
	Unconstrained Net State as Goals (Output Distance) Δ-ineq. Reg. Weight =0.3								
									via Sigmoid σ) Δ-ineq. Reg. Weight = 0.3
							Δ-ineq. Reg. Weight =0.3		
1.00-	ʃθ - -Ii ^,~	1.00-		1.00-		1.00-	,_	1.00-	
0.75-		0.75-		0.75-		0.75-		0.75-	
0.50-		0.50-		0.50-		0.50-	匕	0.50-	匕
0.25		0.25-		0.25-	一	0.25		0.25-	
0.00-	UA I' h _ _-			- —	o.oo-		o.oo-		o.oo-		o.oo-	
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
1.00
0.75
0.50
0.25
Unconstrained Net
State as Goals
(Output Distance)
Δ-ineq. Reg. Weight =1
Unconstrained Net
State as Goals
(Output Distance via exp)
Δ-ineq. Reg. Weight =1
1.00-	
0.75-	
0.50-	
	
0.25-	
o.oo-	
Unconstrained Net
State as Goals
(Output Distance via Square)
Δ-ineq. Reg. Weight = 1
1.00- 0.75-	
0.50-	F/
0.25-	
	!∣ . -≤^一二一--	
0.00-	二	 _
	
Unconstrained Net
State as Goals
(Output /-Discounted Distance)
Δ-ineq. Reg. Weight =1
1.00-	
0.75-	
0.50-	
0.25-	
	:∙一 一
0.00-	
Unconstrained Net
State as Goals
(Output y-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight = 1
1.00- 0.75- 0.50- 0.25- o.oo-	/
0.00
	0	250 500 750 1000 Unconstrained Net State as Goals (Output Distance) Δ-ineq. Reg. Weight =3		0	250 500 750 1000 Unconstrained Net State as Goals (Output Distance via exp) Δ-ineq. Reg. Weight = 3		0	250 500 750 1000	0	250 500 750 1000 Unconstrained Net	Unconstrained Net State as Goals	State as Goals (Output Distance via Square)	(Output /-Discounted Distance) ∆-ineq. Reg. Weight =3	∆-ineq. Reg. Weight =3				0	250 500 750 1000 Unconstrained Net State as Goals Output y-Discounted Distance via Sigmoid σ) Δ-ineq. Reg. Weight = 3
1.00- 0.75- 0.50-		1.00- 0.75- 0.50-		1.00- 0.75- 0.50-		1.00- 0.75- 0.50-	y/	1.00- 0.75- 0.50-	
0.25-		0.25-		0.25-	⅜≈-' - 一：		0.25-		0.25-	
0.00-		o.oo-		o.oo-		o.oo-		o.oo-	
	0	250 500 750 1000 Unconstrained Net State-Action as Goals (Output Distance)			0	250 500 750 1000 Unconstrained Net State-Action as Goals (Output Distance via exp)		0	250 500 750 1000	0	250 500 750 1000 Unconstrained Net	Unconstrained Net State-Action as Goals	State-Action as Goals (Output Distance via Square)	(Output /-Discounted Distance)				0	250 500 750 1000 Unconstrained Net State-Action as Goals Output y-Discounted Distance via Sigmoid σ)
1.00- 0.75-			1.00- 0.75-	一	1.00- 0.75-		1.00- 0.75-		1.00- 0.75-	
0.50- 0.25-			0.50- 0.25-		0.50- 0.25-		0.50- 0.25-	. L一二二-三三 一 一 _	0.50- 0.25-	
o.oo-			0.00-		0.00-		o.oo-		o.oo-	
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
0	250 500 750 1000
Figure 16: Grid-world offline Q-learning full results. Individual plots on show standard deviations.
56
Published as a conference paper at ICLR 2022
1.0
0.5
0.0
Unconstrained Net
State-Action as Goals
(Output Distance)
Δ-ineq. Reg. Weight = 0.3
0	250 500 750 1000
Unconstrained Net
State-Action as Goals
(Output Distance via exp)
Unconstrained Net
State-Action as Goals
(Output Distance via Square)
Δ-ineq. Reg. Weight =0.3
Unconstrained Net
State-Action as Goals
(Output /-Discounted Distance)
Unconstrained Net
State-Action as Goals
(Output /-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight = 0.3
750 1000
Unconstrained Net
State-Action as Goals
(Output Distance)
Unconstrained Net
State-Action as Goals
(Output Distance via exp)
Δ-ineq. Reg. Weight = 1
Unconstrained Net
State-Action as Goals
(Output Distance via Square)
Δ-ineq. Reg. Weight =1
Unconstrained Net
State-Action as Goals
(Output y-Discounted Distance)
Δ-ineq. Reg. Weight =1
0.5
Unconstrained Net
State-Action as Goals
(Output /-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight =1
1.0-I
Unconstrained Net	Unconstrained Net
State-Action as Goals	State-Action as Goals
(Output Distance)	(Output Distance via exp)
OQL——，	，	，	，
0	250 500 750 1000
Unconstrained Net
State-Action as Goals
(Output /-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight = 3
Unconstrained Net
State-Action as Goals
(Output Distance via Square)
Δ-ineq. Reg. Weight =3
Unconstrained Net
State-Action as Goals
(Output /-Discounted Distance)
Δ-ineq. Reg. Weight =3
0	250 500 750 1000
Spherical Distance
Metric Embedding
Mixed Space
Metric Embedding
Asym. Dot Product
(Output Distance)
Asym. Dot Product
(Output Distance via exp)
Asym. Dot Product
(Output Distance via Square)
750 1000
750 1000
750 1000
Asym. Dot Product
(Output /-Discounted Distance)
Asym. Dot Product
(Output /-Discounted Distance
via Sigmoid σ)
Asym. Dot Product
(Output Distance)
Asym. Dot Product
(Output Distance via exp)
Δ-ineq. Reg. Weight =0.3
0	250 500 750 1000
Asym. Dot Product
(Output Distance via Square)
Δ-ineq. Reg. Weight =0.3
Asym. Dot Product
(OutputrDiscounted Distance)
Δ-ineq. Reg. Weight = 0.3
Asym. Dot Product
(Output /-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight =0.3
Asym. Dot Product
(Output Distance)
0	250 500 750 1000
Asym. Dot Product
(Output /-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight = 1
Asym. Dot Product
(Output Distance via exp)
Δ-ineq. Reg. Weight = 1
750 1000
Asym. Dot Product
(Output Distance via Square)
Δ-ineq. Reg. Weight =1
Asym. Dot Product
(Output /-Discounted Distance)
Δ-ineq. Reg. Weight =1
0	250 500 750 1000
0	250 500 750 1000
Asym. Dot Product
(Output y-Discounted Distance
via Sigmoid σ)
Δ-ineq. Reg. Weight = 3
Asym. Dot Product
(Output Distance)
Δ-ineq. Reg. Weight = 3
1.0
0.5
0.0
0	250 500 750 1000
1.0
0.5
0.0
Asym. Dot Product
(Output Distance via exp)
Δ-ineq. Reg. Weight = 3
0	250 500 750 1000
Asym. Dot Product
(Output Distance via Square)
Δ-ineq. Reg. Weight =3
Asym. Dot Product
(Output y-Discounted Distance)
Δ-ineq. Reg. Weight = 3
0	250 500 750 1000
0	250 500 750 1000
Figure 16: Grid-world offline Q-learning full results (cont.). Individual plots on show standard
deviations.
57