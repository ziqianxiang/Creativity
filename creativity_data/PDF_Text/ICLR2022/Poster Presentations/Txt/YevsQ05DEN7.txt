Published as a conference paper at ICLR 2022
Understanding Dimensional Collapse in Con-
trastive Self-supervised Learning
Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian
Facebook AI Research
{ljng, pascal, yann, yuandong}@fb.com
Ab stract
Self-supervised visual representation learning aims to learn useful representations
without relying on human annotations. Joint embedding approach bases on max-
imizing the agreement between embedding vectors from different views of the
same image. Various methods have been proposed to solve the collapsing problem
where all embedding vectors collapse to a trivial constant solution. Among these
methods, contrastive learning prevents collapse via negative sample pairs. It has
been shown that non-contrastive methods suffer from a lesser collapse problem of
a different nature: dimensional collapse, whereby the embedding vectors end up
spanning a lower-dimensional subspace instead of the entire available embedding
space. Here, we show that dimensional collapse also happens in contrastive learn-
ing. In this paper, we shed light on the dynamics at play in contrastive learning
that leads to dimensional collapse. Inspired by our theory, we propose a novel
contrastive learning method, called DirectCLR, which directly optimizes the rep-
resentation space without relying on a trainable projector. Experiments show that
DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.
1	Introduction
Self-supervised learning aims to learn useful representations of the input data without relying on hu-
man annotations. Recent advances in self-supervised visual representation learning based on joint
embedding methods (Misra & Maaten, 2020b; He et al., 2020; Chen et al., 2020a; Chen & He, 2020;
Grill et al., 2020; Zbontar et al., 2021; Bardes et al., 2021; Chen et al., 2020b; Dwibedi et al., 2021;
Li et al., 2021; Misra & Maaten, 2020a; HaoChen et al., 2021; Assran et al., 2021; Caron et al., 2021)
show that self-supervised representations have competitive performances compared with supervised
ones. These methods generally aim to learn representations invariant to data augmentations by max-
imizing the agreement between embedding vectors from different distortions of the same images.
As there are trivial solutions where the model maps all input to the same constant vector, known
as the collapsing problem, various methods have been proposed to solve this problem that rely on
different mechanisms. Contrastive methods like Chen et al. (2020a) and He et al. (2016) define ‘pos-
itive’ and ‘negative’ sample pairs which are treated differently in the loss function. Non-contrastive
methbods like Grill et al. (2020) and Chen & He (2020) use stop-gradient, and an extra predictor to
prevent collapse without negative pairs; Caron et al. (2018; 2020) use an additional clustering step;
and Zbontar et al. (2021) minimize the redundant information between two branches.
These self-supervised learning methods are successful in preventing complete collapse whereby all
representation vectors shrink into a single point. However, it has been observed empirically in non-
contrastive learning methods (Hua et al., 2021; Tian et al., 2021) that while embedding vectors do not
completely collapse; they collapse along certain dimensions. This is known as dimensional collapse
(Hua et al., 2021), whereby the embedding vectors only span a lower-dimensional subspace.
In contrastive methods that explicitly use positive and negative pairs in the loss function, it seems
intuitive to speculate that the repulsive effect of negative examples should prevent this kind of di-
mensional collapse and make full use of all dimensions. However, contrary to intuition, contrastive
learning methods still suffer from dimensional collapse (See Fig. 7). In this work, we theoretically
study the dynamics behind this phenomenon. We show there are two different mechanisms that
1
Published as a conference paper at ICLR 2022
cause collapsing: (1) along the feature direction where the variance caused by the data augmenta-
tion is larger than the variance caused by the data distribution, the weight collapses. Moreover, (2)
even if the covariance of data augmentation has a smaller magnitude than the data variance along
all dimensions, the weight will still collapse due to the interplay of weight matrices at different lay-
ers known as implicit regularization. This kind of collapsing happens only in networks where the
network has more than one layer.
Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which
directly optimizes the encoder (i.e., representation space) without relying on a trainable projector.
DirectCLR outperforms SimCLR with a linear trainable projector on ImageNet.
We summarize our contributions as follows:
•	We empirically show that contrastive self-supervised learning suffers from dimensional
collapse whereby all the embedding vectors fall into a lower-dimensional subspace instead
of the entire available embedding space.
•	We showed that there are two mechanisms causing the dimensional collapse in contrastive
learning: (1) strong augmentation along feature dimensions (2) implicit regularization driv-
ing models toward low-rank solutions.
•	We propose DirectCLR, a novel contrastive learning method that directly optimizes the rep-
resentation space without relying on a trainable projector. DirectCLR outperforms SimCLR
with a linear trainable projector.
2	Related Works
Self-supervised Learning Methods Joint embedding methods are a promising approach in self-
supervised learning, whose principle is to match the embedding vectors of augmented views of a
training instance. Contrastive methods (Chen et al., 2020a; He et al., 2016) directly compare train-
ing samples by effectively viewing each sample as its own class, typically based on the InfoNCE
contrastive loss (van den Oord et al., 2018) which encourages representations from positive pairs of
examples to be close in the embedding space while representations from negative pairs are pushed
away from each other. In practice, contrastive methods are known to require a large number of
negative samples. Non-contrastive methods do not directly rely on explicit negative samples. These
include clustering-based methods (Caron et al., 2018; 2020), redundancy reduction methods (Zbon-
tar et al., 2021; Bardes et al., 2021) and methods using special architecture design (Grill et al., 2020;
Chen & He, 2020).
Theoretical Understanding of Self-supervised Learning Although self-supervised learning mod-
els have shown success in learning useful representations and have outperformed their supervised
counterpart in several downstream transfer learning benchmarks (Chen et al., 2020a), the under-
lying dynamics of these methods remains somewhat mysterious and poorly understood. Several
theoretical works have attempted to understand it. Arora et al. (2019b); Lee et al. (2020); Tosh
et al. (2021) theoretically proved that the learned representations via contrastive learning are use-
ful for downstream tasks. Tian et al. (2021) explained why non-contrastive learning methods like
BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) work: the dynamics of the alignment
of eigenspaces between the predictor and its input correlation matrix play a key role in preventing
complete collapse.
Implicit Regularization It has been theoretically explained that gradient descent will drive adjacent
matrices aligned in a linear neural network setting (Ji & Telgarsky, 2019). Under the aligned matrix
assumption, Gunasekar et al. (2018) prove that gradient descent can derive minimal nuclear norm
solution. Arora et al. (2019a) extend this concept to the deep linear network case by theoretically
and empirically demonstrating that a deep linear network can derive low-rank solutions. In general,
over-parametrized neural networks tend to find flatter local minima (Saxe et al., 2019; Neyshabur
et al., 2019; Soudry et al., 2018; Barrett & Dherin, 2021).
3	Dimensional Collapse
Self-supervised learning methods learn useful representation by minimizing the distances between
embedding vectors from augmented images (Figure 1a). On its own, this would result in a collapsed
2
Published as a conference paper at ICLR 2022
Embeddings
InfONCE
loss
(a) embedding space
Figure 1: Illustration of the collapsing problem. For complete collapse, the embedding vectors collapse to
same point. For dimensional collapse, the embedding vectors only span a lower dimensional space.
(b) complete collapse (c) dimensional collapse
solution where the produced representation becomes constant (Figure 1b). Contrastive methods
prevent complete collapse via the negative term that pushes embedding vectors of different input
images away from each other. In this section, we show that while they prevent complete collapse,
contrastive methods still experience a dimensional collapse in which the embedding vectors occupy
a lower-dimensional subspace than their dimension (Figure 1c).
We train a SimCLR model (Chen et al. (2020a))
with a two-layer MLP projector. We followed
the standard recipe and trained the model on
ImageNet for 100 epoch. We evaluate the di-
mensionality by collecting the embedding vec-
tors on the validation set. Each embedding vec-
tor has a size of d = 128. We compute the co-
variance matrix C ∈ Rd×d of the embedding
layer (here Z := PN=I Zi/N and N is the total
number of samples):
1N
C = N E(Zi - Z)(Zi - Z)T (I)
N i=1
Figure 2 shows singular value decomposition
on this matrix (C = USV T, S = diag(σk)). in
sorted order and logarithmic scale ({log(σk)}).
We observe that a number of singular values
collapse to zero, thus representing collapsed di-
mensions.
5101520
_ - -
sn-e>,Jn6SJ0
O 20	40	60	80 IOO 120
Singular Value Rank: Index
Figure 2: Singular value spectrum of the embedding
space. The embedding vectors are computed from a
pretrained SimCLR model on the validation set of Ima-
geNet. Each embedding vector has a dimension of 128.
The spectrum contains the singular values of the covari-
ance matrix of these embedding vectors in sorted or-
der and logarithmic scale. A number of singular values
drop to zero, indicating collapsed dimensions.
4 Dimensional Collapse caused by S trong Augmentation
4.1	Linear Model
In this section, we explain one scenario for contrastive learning to have collapsed embedding dimen-
sions, where the augmentation surpasses the input information. We focus on a simple linear network
setting. We denote the input vector as x and the augmentation is an additive noise. The network is a
single linear layer with weight matrix is W. Hence, the embedding vector is Z = Wx. We focus on
a typical contrastive loss, InfoNCE (van den Oord et al., 2018):
N
- X log
i=1
L
___________eχp(TZi - Zi|2/2)________
Pj=ieχp(TZi - Zj|2/2) + eχp(TZi - Zi|2/2)
(2)
where Zi and Z0i are a pair of embedding vectors from the two branches, Zj indicates the negative
samples within the minibatch. When all Zi and Z0i are normalized to be unit vector, the negative
distance -|Zi - Z0i|2/2 can be replaced by inner products ZiTZ0i. The model is trained with a basic
stochastic gradient descent without momentum or weight decay.
3
Published as a conference paper at ICLR 2022
4.2	Gradient Flow Dynamics
We study the dynamics via gradient flow, i.e., gradient descent with an infinitesimally small learning
rate.
Lemma 1. The weight matrix in a linear contrastive self-supervised learning model evolves by:
W= -G	(3)
where G = i(gz xiT + gz0 x0iT), and gz is the gradient on the embedding vector zi (similarly gz0).
This can be easily proven based on the chain rule. See proof in Appendix B.1. For InfoNCE loss
defined in Eqn 2, the gradient of the embedding vector for each branch can be written as
gzi =	αij (zj -z0i)	+	αji(zj	-zi),	gz0i	=	αij(z0i	-zi)	(4)
j6=i	j 6=i	j 6=i
where {αij} are the softmax of similarity of between zi and {zj}, defined by αij = exp(-|zi -
zj |2/2)/Zi, αii = exp(-|zi-z0i|2/2)/Zi, and Zi = Pj6=i exp(-|zi-zj|2/2)+exp(-|zi-z0i|2/2).
Hence, Pj αij = 1. Since zi = Wxi, we have
G= -WX	(5)
where
X := -∑ I E ɑij (Xi	-Xj) + E αji(Xi-Xj)卜：-£(1-	αii)(x0i	-	xi)x0iT	(6)
i	j6=i	j 6=i	i
Lemma 2. X is a difference of two PSD matrices:
__	ʌ	ʌ
X = ∑ 0 - ∑ 1	⑺
Here ∑o = Pij αij (xi - xj )(xi - xj )T is a weighted data distribution covariance matrix and
Σι = Pi(1 一 αii)(xi 一 xi)(xi 一 xi)T is a weighted augmentation distribution Covariance matrix.
See proof in Appendix B.2. Therefore, the amplitude of augmentation determines whether X is a
positive definite matrix. Similar to Theorem 3-4 in Tian et al. (2020), Lemma 2 also models the
time derivative of weight W as a product of W and a symmetric and/or PSD matrices. However,
Lemma 2 is much more general: it applies to InfoNCE with multiple negative contrastive terms,
remains true when αij varies with sample pair (i, j), and holds with finite batch size N. In contrast,
Theorem 4 in Tian et al. (2020) only works for one negative term in InfoNCE, holds only in the
population sense (i.e., N → +∞), and the formulation has residual terms, if αij are not constants.
Next, we look into the dynamics of weight matrix W given property of X .
Theorem 1. With fixed matrix X (defined in Eqn 6) and strong augmentation such that X has
negative eigenvalues, the weight matrix W has vanishing singular values.
See proof in Appendix B.3.
Corollary 1 (Dimensional Collapse Caused by Strong Augmentation). With strong augmentation,
the embedding space covariance matrix becomes low-rank.
The embedding space is identified by the singular value spectrum of the covariance matrix on the
embedding (Eqn. 1), C = Pi(Zi — z)(zi — Z)T/N = Pi W(Xi 一 X)(Xi — X)TWT/N. Since W
has vanishing singular values, C is also low-rank, indicating collapsed dimensions.
Numerical simulation verifies our theory. We choice input data as isotropic Gaussian with covari-
ance matrix Pi,j (Xi - Xj )(Xi - Xj )T/N = I. We set the augmentation as additive Gaussian with
covariance matrix equal to Pi(Xi - Xi)(Xi — Xi)T/N = block .diagonal(0 k * I), where the block
has the size of 8x8. We plot the weight matrix singular value spectrum in Figure 3 with various
augmentation amplitude k. This proves that under linear network setting, strong augmentation leads
to dimensional collapse in embedding space.
4
Published as a conference paper at ICLR 2022
Our theory in this section is limited to linear network
settings. For more complex nonlinear networks,
the collapsing condition will still depend on “strong
augmentation” but interpreted differently. A strong
augmentation will be determined by more compli-
cated properties of the augmentation (higher-order
statistics of augmentation, manifold property of aug-
mentation vs. data distribution) conditioned on the
capacity of the networks.
Figure 3: Weight matrix singular value spectrum
with different augmentation amplitude k. The set-
ting is a single layer linear toy model with each
weight matrix of the size of 16x16, where the
block has the size of 8x8. Strong augmentation
results in vanishing singular values in weight ma-
trices.
0	2	4	6	8	IOI2	14
Singular Value Rank Index
1.6
1.4
n ι.2
ω
> i-o
0.2
5 Dimensional Collapse
caused by Implicit Regularization
5.1	Two-layer linear model
With strong augmentation, a linear model under In-
foNCE loss will have dimensional collapse. How-
ever, such scenarios rely on the condition that the
network has a limited capacity which may not hold
for real cases. On the other hand, when there is no
strong augmentation (∑ι Y ∑o) and thus X matrix remains PSD, a single linear model won t have
dimensional collapsing. However, interestingly, for deep networks, dimensional collapsing still
happens in practice. In the following, We will show that it stems from a different nature: implicit
regularization, where over-parametrized linear networks tend to find low-rank solutions.
To understand this counter-intuitive phenomena, we
start with the simplest over-parametrized setting
by choosing the network as a two-layer linear MLP
without bias. The weight matrices of these two lay-
ers are denoted by Wi ∈ Rd×d and W2 ∈ Rd×d.
Similar to the setting in Sec 4, the input vector is
denoted as X and the augmentation is an additive
noise. The embedding vector from each branch is
Z = W2W1x, hence Z ∈ Rn. We do not normalize
z. See Figure 4. We use InfoNCE loss defined in
Eqn 2. The model is trained with a basic stochastic
gradient descent without momentum or weight decay.
Figure 4: Two-layer Linear Model
5.2	Gradient Flow Dynamics
Similar to Lemma 1, we derive the gradient flow on the two weight matrices W1 and W2.
Lemma 3. The weight matrices of the two layer linear contrastive self-supervised learning model
evolves by (G =	i (gzi xiT + gz0 x0iT) is defined in Lemma 1):
W1 = -W2T G,	W2 = -GWT	(8)
This can be easily proven based on the chain rule. See proof in Appendix B.4. For the two layer
case, similar to Eqn 5, we have the specific form of G:
G = -W2W1X	(9)
where X is defined in Eqn 6. According to Lemma 2, we know that with small augmentation,
X = Σ0 - Σ1	0 is a positive-definite matrix.
5.3	Weight Alignment
Since we have two matrices W1 and W2, the first question is how they interact with each other.
We apply singular value decomposition on both matrices W1 and W2, i.e., W1 = U1S1V1T, W2 =
U2 S2 V2T and S1 = diag([σ1k]), S2 = diag([σ2k]). The alignment is now governed by the interaction
5
Published as a conference paper at ICLR 2022
between the adjacent orthonormal matrices V2 := [v2k] and U1 = [u1k]. This can be characterized
by the alignment matrix A = V2TU1, whose (k, k0)-entry represents the alignment between the k-th
right singular vector v2k of W2 and the k0-th left singular vector u1k0 of W1. The following shows
that indeed W1 and W2 aligns.
Theorem 2 (Weight matrices align). If for all t, W2(t)W1(t) 6= 0, X(t) is positive-definite and
W1(+∞), W2(+∞) have distinctive singular values, then the alignment matrix A = V2T U1 → I.
See proof in Appendix B.5. Here, we also empirically demonstrate that under InfoNCE loss, the
absolute value of the alignment matrix A converges to an identity matrix. See Figure 5.
The alignment effect has been studied in other scenarios (Ji & Telgarsky, 2019; Radhakrishnan et al.,
2020). In the real case, when some of our assumptions are not satisfied, e.g., there are degenerate
singular values in weight matrices, we will not observe a perfect alignment. This can be easily un-
derstood by the fact that the singular decomposition is no longer unique given degenerate singular
values. In our toy experiment, we specifically initialize the weight matrices to have non-degenerate
singular values. In real scenario, when weight matrices are randomly initialized, we will only ob-
serve the alignment matrix to converge to a block-diagonal matrix, with each block representing a
group of degenerate singular values.
Given the fact that singular vectors corresponding to
the same singular value align, we can now study the
dynamics of the singular values of each weight ma-
trix W1 and W2 .
Theorem 3. If W2 and W1 are aligned (i.e., V2 =
U1T), then the singular values of the weight matrices
W1 and W2 under InfoNCE loss evolve by:
σ k = σk (σ2)2(v kT X V k)	(10)
σ 2 = σk (σk)2(v kT X v k)	(11)
Figure 5: Visualization of the alignment matrix
A = V2T U1 after training. The setting is a 2-layer
linear toy model with each weight matrix of the
See proof in Appendix B.6. According to Eqn. 10, size of 16x16. The alignment matrix converges to
(σ1k)2 = (σ2k)2 + C. We solve the singular an identity matrix.
value dynamics analytically: σ1k = σ1k ((σ1k )2 +
C)(v1kTXv1k). This shows that a pair of singular values (singular values with same ranking from
the other matrix) have gradients proportional to themselves. Notice that X is a positive definite ma-
trix, the term v1k TXv1k is always non-negative. This explains why we observe that the smallest group
of singular values grow significantly slower. See demonstrative experiment results in Figure 6a and
6b.
Wl Spectrum
(a) W1
O 500 IOOO 1500 2000 25∞ 3∞0 35∞ 40∞
iterations
W2 Spectrum
O 500 l∞0 UOO 2∞0 25∞ 30∞ 3500 40∞
iterations
(b) W2
Embedding Space Spectrum
τ-1°-u-2°
san-> ∙l-n6U-S 6。-
O 5∞ IOOO 1500 2000 2500 3∞0 35∞ 4∞0
iterations
(c) Embedding Space
Figure 6: Evolution of the singular values of the weight matrices and the embedding space covariance matrix.
The setting is a 2-layer linear toy model with each weight matrix of the size of 16x16. The lowest few singular
values of each weight matrix remain significantly smaller.
Corollary 2 (Dimensional Collapse Caused by Implicit Regularization). With small augmentation
and over-parametrized linear networks, the embedding space covariance matrix becomes low-rank.
The embedding space is identified by the singular value spectrum of the covariance matrix on the
embedding vectors, C = P(Z - z)(z - Z)T/N = P W2W1(x - x)(x - X)TWTWT/N. As
6
Published as a conference paper at ICLR 2022
W2W1 evolves to be low-rank, C is low-rank, indicating collapsed dimensions. See Figure 6c for
experimental verification.
Our theory can also be extended to multilayer networks and nonlinear setting. Please see Appendix C
6	DirectCLR
6.1	Motivation
We now leverage our theoretical finding to design novel algorithms. Here we are targeting the
projector component in contrastive learning.
Empirically, adding a projector substantially improves the quality of the learned representation and
downstream performance (Chen et al., 2020a). Checking the spectrum of the representation layer
also reveals a difference with/without a projector. To see this, We train two SimCLR models with and
without a projector. The representation space spectrum are shown in Figure 7b. The dimensional
collapse in representation space happens when the model is trained without a projector. Thus, the
projector prevents the collapse in the representation space.
Representations Embeddings
Encoder Projector
(a) representation and embedding
(b) Representation space spectrum
Figure 7: (a) Definition of representation and the embedding space; (b) Singular value spectrums of the
representation space of pretrained contrastive learning models (pretrained with or without a projector). The
representation vectors are the output from the ResNet50 encoder and directly used for downstream tasks. Each
representation vector has a dimension of 2048. Without a projector, SimCLR suffers from dimensional collapse
in the representation space.
The projector in contrastive learning is essential to prevent dimensional collapse in the representa-
tion space. We claim the following propositions regarding a linear projector in contrastive learning
models.
Proposition 1. A linear projector weight matrix only needs to be diagonal.
Proposition 2. A linear projector weight matrix only needs to be low-rank.
Based on our theory on implicit regularization dynamics, we expect to see adjacent layers W1 (=
U1S1V1T ) and W2 (= U2S2V2T ) to be aligned such that the overall dynamics is only governed by
their singular values S1 and S2 . And the orthogonal matrices V2T and U1 are redundant as they will
evolve to V2T U1 = I, given S1 and S2 .
Now, let’s consider the linear projector SimCLR model and only focus on the channel dimension.
W1 is the last layer in the encoder, and W2 is the projector weight matrix. Our propositions claim
that for this projector matrix W2, the orthogonal component V2 can be omitted. Because the previous
layer W1 is fully trainable, its orthogonal component (U1) will always evolve to satisfy V2T U1 = I.
Therefore, the final behavior of the projector is only determined by the singular values (S2 ) of the
projector weight matrix. This motivates Proposition 1: the orthogonal component of the weight
matrix doesn’t matter. So we can set the projector matrix as a diagonal matrix.
Also, according to our theory, the weight matrix will always converge to the low-rank. The singular
value diagonal matrix naturally becomes low-rank, so why not just set it low-rank directly? This is
the motivation of Proposition 2.
7
Published as a conference paper at ICLR 2022
These propositions are verified via ablation studies in Sec 6.3. Given these two propositions, We
propose DirectCLR, which is effectively using a low-rank diagonal projector.
6.2	Main Idea
We propose to remove the projector in contrastive
learning by directly sending a sub-vector of the
representation vector to the loss function. We
call our method DirectCLR. In contrast to all re-
cent state-of-the-art self-supervised learning meth-
ods, our method directly optimizes the representa-
tion space. See Figure 8, DirectCLR picks a subvec-
tor of the representation Z = r[0 : do], where do
is a hyperparameter. Then, it applies a standard In-
foNCE loss on this normalized subvector Z = z∕∣z∣,
L=Pilog
Pj eXp(z=Zj ).
We train DirectCLR with a standard recipe of Sim-
CLR for 100 epochs on ImageNet. The backbone
encoder is a ResNet50. More implementation details
Representations
Encoder
Figure 8: DirectCLR: no trainable projector, sim-
ply apply InfoNCE loss on the a fixed sub-vector
of the representations
can be found in the Appendix D. DirectCLR demonstrates better performance compared to SimCLR
with a trainable linear projector on ImageNet. The linear probe accuracies for each model are listed
in Table 1.
Loss function	Projector	Accuracy
SimCLR	2-layer nonlinear projector	66.5
SimCLR	1-layer linear projector	61.1
SimCLR	no projector	51.5
DirectCLR	no projector	62.7
Table 1: Linear probe accuracy on ImageNet. Each model is trained on ImageNet for 100 epochs with standard
training recipe. The backbone encoder is a ResNet50. DirectCLR outperforms SimCLR with 1-layer linear
projector.
We visualize the learnt representation space spectrum in Figure 9. DirectCLR prevents dimensional
collapse in the representation space similar to the functionality of a trainable projector in SimCLR.
S①n-e>」e-n6u-s Jo 6oη
Figure 9: Representation space spectrum of Di-
rectCLR compared to SimCLR (a) with a 2-layer
nonlinear projector (b) with a 1-layer linear pro-
jector (c) without projector. The spectrums are
computed based on the output from the backbone,
using ImgaeNet validation set. Similar to Sim-
CLR with projectors, DirectCLR is able to prevent
dimensional collapse in the representation space.
residual connection
hidden layer	nonlinear	representations
(full-rank)	conv block
Figure 10: Why is the whole representation vector r
meaningful in DirectCLR while only part of it receives
gradient? It takes advantage of the residual connection
in the backbone. Thus, the gradient passing through the
representation vector is low-rank where only the first do
channel dimensions are non-zero. When the gradient
enters the ResNet backbone and passes through the last
nonlinear conv block, it becomes full rank. Therefore,
this hidden layer h receives gradients on all channels.
During forward pass, h is directly fed to the representa-
tion vectors via the residual connection. Therefore, the
entire representation vector r is meaningful.
8
Published as a conference paper at ICLR 2022
One may suspect that the contrastive loss in DirectCLR does not apply a gradient on the rest part of
the representation vector r[d0 :], then why these dimensions would contain useful information?
Here, we show that the entire representation vector r contains useful information. See Figure 10.
First, the gradient backpropagating through the representation vector is low-rank, where only the
first d0 channel dimensions are non-zero. When the gradient enters the ResNet backbone and passes
through the last nonlinear conv block, it becomes full rank. Therefore, this hidden layer h receives
gradients on all channels. Note that h and r have a same channel dimension of 2048. Next, we
consider the forward pass. This hidden layer h is directly fed to the representation vectors via the
residual connection. As a result, the rest part of the representation vector r[d0 :] is not trivial. In
addition, we run an ablation study in Sec F to test the linear probe accuracy based only on the
“directly” optimized vector. This verifies that the whole representation vector is meaningful.
6.3	Ablation Study
Projector	diagonal	low-rank	Top-1 Accuracy
no projector			51.5
orthogonal projector			52.2
trainable projector			61.1
trainable diagonal projector	X		60.2
fixed low-rank projector		X	62.3
fixed low-rank diagonal projector	X	X	62.7
Table 2: Ablation study: top-1 accuracies on ImageNet by SimCLR model with different projector settings.
To further verify our hypothesis, we have perform ablation studies.
Proposition 1 matches the fact that: (a) an orthogonal constrained projector performs the same as the
non-projector setting; (b) fixed low-rank projector performs the same as a fixed diagonal projector;
(c) trainable linear projector performs the same as a trainable diagonal projector.
Proposition 2 matches the observation that a low-rank projector has the highest accuracy.
Please see more detailed ablation study discuss and additional ablation experiments in Appendix F.
7	Conclusions
In this work, we showed that contrastive self-supervised learning suffers from dimensional collapse,
where the embedding vectors only span a lower-dimensional subspace. We provided the theoretical
understanding of this phenomenon and showed that there are two mechanisms causing dimensional
collapse: strong augmentation and implicit regularization. Inspired by our theory, we proposed a
novel contrastive self-supervised learning method DirectCLR that directly optimizes the representa-
tion space without relying on a trainable projector. DirectCLR outperforms SimCLR with a linear
projector on ImageNet.
Acknowledgement
We thank Yubei Chen, Jiachen Zhu, Adrien Bardes, Nicolas Ballas, Randall Balestriero, Quentin
Garrido for useful discussions.
Reproducibility S tatement
We provide detailed proof for all the lemmas and theorems in the Appendices. Code (in PyTorch) is
available at https://github.com/facebookresearch/directclr
9
Published as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, W. Hu, and Yuping Luo. Implicit regularization in deep matrix fac-
torization. In NeurIPS, 2019a.
Sanjeev Arora, H. Khandeparkar, M. Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical
analysis of contrastive unsupervised representation learning. In ICML, 2019b.
Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Bal-
las, and Michael G. Rabbat. Semi-supervised learning of visual features by non-parametrically
predicting view assignments with support samples. ArXiv, abs/2104.13963, 2021.
Adrien Bardes, J. Ponce, and Y. LeCun. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. ArXiv, abs/2105.04906, 2021.
D. Barrett and B. Dherin. Implicit gradient regularization. ArXiv, abs/2009.11162, 2021.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and M. Douze. Deep clustering for unsupervised
learning of visual features. In ECCV, 2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv’e J’egou, J. Mairal, Piotr Bojanowski, and Ar-
mand Joulin. Emerging properties in self-supervised vision transformers. ArXiv, abs/2104.14294,
2021.
Mario Lezcano Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural net-
works: A simple parametrization of the orthogonal and unitary group. ArXiv, abs/1901.08428,
2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. 2020a.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2020.
Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. ArXiv, abs/2003.04297, 2020b.
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With
a little help from my friends: Nearest-neighbor contrastive learning of visual representations.
ArXiv, abs/2104.14548, 2021.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, Koray KavukCuoglu, Remi Munos, and Michal Valko. Bootstrap your own
latent: A new approach to self-supervised learning. In NeurIPS, 2020.
Suriya Gunasekar, Blake E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan
Srebro. Implicit regularization in matrix factorization. 2018 Information Theory and Applications
Workshop (ITA),pp.1-10, 2018.
Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-
supervised deep learning with spectral contrastive loss. ArXiv, abs/2106.04156, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 9726-9735, 2020.
Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ArXiv, abs/2105.00470, 2021.
10
Published as a conference paper at ICLR 2022
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. ArXiv,
abs/1810.02032, 2019.
L. Jing, J. Zbontar, and Y. LeCun. Implicit rank-minimizing autoencoder. ArXiv, abs/2010.00679,
2020.
J. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Prov-
able self-supervised learning. ArXiv, abs/2008.01064, 2020.
Junnan Li, Pan Zhou, Caiming Xiong, R. Socher, and S. Hoi. Prototypical contrastive learning of
unsupervised representations. ArXiv, abs/2005.04966, 2021.
Ishan Misra and L. V. D. Maaten. Self-supervised learning of pretext-invariant representations. 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6706-6716,
2020a.
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-
tions. In CVPR, 2020b.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Y. LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks. ArXiv,
abs/1805.12076, 2019.
Adityanarayanan Radhakrishnan, Eshaan Nichani, D. Bernstein, and Caroline Uhler. On alignment
in deep linear neural networks. arXiv: Learning, 2020.
Andrew M. Saxe, James L. McClelland, and S. Ganguli. A mathematical theory of semantic devel-
opment in deep neural networks. Proceedings of the National Academy of Sciences, 116:11537 -
11546, 2019.
Daniel Soudry, E. Hoffer, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient
descent on separable data. ArXiv, abs/1710.10345, 2018.
Yuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning
with dual deep networks. arXiv preprint arXiv:2010.00578, 2020.
Yuandong Tian, Xinlei Chen, and S. Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. ArXiv, abs/2102.06810, 2021.
Christopher Tosh, A. Krishnamurthy, and Daniel J. Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. ArXiv, abs/2008.10150, 2021.
Aaron van den Oord, Y. Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. ArXiv, abs/1807.03748, 2018.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. arXiv preprint arxiv:2103.03230, 2021.
A	Useful Lemmas
We adapt two useful lemmas from Arora et al. (2019a).
Lemma 4. Given a matrix W and the dynamics that W evolves by W, the singular values of this
matrix evolve by:
σk = U kT W Vk	(12)
where uk and vk are singular value σk ’s corresponding left and right singular vectors. i.e. the k-th
column of matrices U and V respectively.
11
Published as a conference paper at ICLR 2022
Proof. Given a matrix W and its singular value decomposition W = USVT. We have the dynamics
of the matrix
W = U SV T + US V T + USV T
Multiplying U T from the left and multiplying V from the right, considering U and V are orthogonal
matrices, we have
U T W V = U T U S + S + SV T V
Since S = diag (σk) is a diagonal matrix, we have
σk = UkTWVk - UkTukσk - σkVkTVk
k	k	kT	T k
Again, considering Uk and Vk have unit-norm, we have Uk Uk = 0 and Vk Vk = 0. Therefore, we
derive
σk = UkT W Vk
□
Lemma 5. Given a matrix W and the dynamics that W evolves by W, the singular vectors of this
matrix evolve by:
U = U (H Θ (U T W VS + SV T W T U))	(13)
V = V(H Θ (VTWTUS + SUTWV))	(14)
where Θ represents Hadamard element-wise multiplication. H is a skew-symmetric matrix
Hk,k0 = (1/(bk2 - σk02) ifk = k	(15)
0	if k = k0
Proof. Same as proof for Lemma 1, we start from the following equation
U T W V = U T U S + S + SV T V
Considering the fact that UT U and VT V are skew-symmetric matrices, whose diagonal terms are
all zero, We Hadamard-multiPly I to both sides of the equation. Here, I has all diagonal values equal
zeros and all off-diagonal values equal to one, we have
IΘ UT WV = UT US + SVT V	(16)
Taking transPose, we have
IΘ VT WU = -SUTU — VTVS	(17)
Right-multiPlying S to Eqn 16 and left-multiPlying S to Eqn 17, then adding them uP, we have
UTUS2 — S2UTU = IΘ (UTWVS + SVTWU)
Therefore, we have
U = U (H Θ (U T W VS + SV T W T U))
where	H= (l∕(σk2 - σk02) if k = k0
0	if k = k0
Similar proof applies to Eqn 14.	□
Lemma 6 (Alignment matrix dynamics). The alignment matrix A, defined by A = V2TU1, evolves
by:
A = -A(Hι Θ (ATF + FTA)) + (H2 Θ (AFT + FAT))A	(18)
where Θ represents Hadamard (element-wise) multiplication. Hl is a skew-symmetric matrix, whose
(k, k0 )-entry is given by
Hk,k0 = (1/(d2 - σlk02) ifk = k0	(19)
and F is defined by
F = S2U2TGV1S1	(20)
12
Published as a conference paper at ICLR 2022
Proof. According to Lemma. 5, we have
Ui = U1(H1 Θ (UTW1V1S1 + SiVTWTUι))
V2 = V2(H2 Θ CVTWTU2S2 + S2UTW2½))
Plugging the above two equations and Eqn 8, the dynamics of the alignment matrix A = V2T Ui can
be written as
A = VT U1 + VT U1
= VTUi(Hi Θ (UTW1V1S1 + SIVTWTUI)) + (H Θ (VTWTU2S2 + S2UW2V2))tVTUi
= -A(Hi Θ (UiTW2TGViSi +SiViTGTW2Ui)) + (H2 Θ (S2U2TGWiTV2 + V2T WiGT U2S2))A
=	-A(Hi Θ (UiTV2S2U2TGViSi +SiViTGTU2S2V2TUi))
+(H2 Θ (S2U2TGViSiUiTV2 +V2TUiSiViTGTU2S2))A
= -A(Hi Θ (ATS2U2TGViSi + SiViTGTU2S2 A)
+(H2 Θ (S2U2TGViSi AT+ ASiViTGTU2S2))A
= -A(Hi Θ (ATF +FT A)) +(H2 Θ(AFT+F AT))A
where
F = S2U2TGViSi
□
Lemma 7 (Singular value dynamics). The singular values of the weight matrices Wi and W2 evolve
by:
σk = - X(yVf uk)σk0(uk0TGVk)	(21)
k0
σk = - X( UkT v k W (u FG Vkj	(22)
k0
Proof. According to Lemma 4,
σ r = UrT Wivr
Plugging in Eqn 8, we have
σk =	-UkT WT Gvk
= -UikTV2S2 U2T Gvik
=	- X(v2k0TUik)σ2k0 (U2k0TGvik)
k0
Similar proof applies to Eqn 22.	□
B	Delayed Proofs
B.1	Proof of Lemma 1
The gradient on matrix W is
dL _X ∂L ∂zi	∂L ∂Zi
而=乙(∂Zi ∂W + ∂zi ∂W)
ii
We denote the gradient on Zi and Zi as 8名名 and gz, respectively. Since 箫=Xi and 繇=xi, we
get	i
W = T dW)T = - X(g"xT + gzi XiT)
i
13
Published as a conference paper at ICLR 2022
B.2	Proof of Lemma 2
Proof. X is defined in Eqn 6.
X=	(	αij(x0i	-xj)	+	αji(xi	-xj))xiT	-	(1 -	αii)(x0i	-xi)x0iT
i j 6=i	j6=i	i
=	αijx0ixiT -	αijxjxiT + ΣΣαji(xi - xj)(xi - xj)T
i j 6=i	i j 6=i	i j 6=i
+	αji(xi	-	xj)xjT	-	(1 - αii)(x0i	-xi)(x0i	- xi)T	-	(1 -	αii)(x0i -xi)xiT
i j 6=i	i	i
Given the fact that Pj6=i αij = 1 - αii, we have Pi Pj6=i αijx0ixiT = Pi(1 - αii)x0ixiT . Also,
since Pi Pj 6=i iterates all pairs of i, j, we can replace the index between i and j , we have
Pi Pj6=i αij xj xiT = Pi Pj6=i αjixixjT.
Therefore
X=ΣΣαji(xi - xj)(xi - xj)T -	(1 - αii)(x0i - xi)(x0i - xi)T
□
B.3 Proof of Theorem 1
Proof. According to Lemma 1, we have
d
-W = WX	(23)
dt
For a fixed X , we solve this equation analyically,
W(t) = W (0) exp(X t)
Apply eigen-decomposition on X, X = UΛUT . Then we have exp(X t) = U exp(Λt)U T . There-
fore,
W(t) = W(0)U exp(Λt)UT
Because X has negative eigenvalues, i.e., Λ has negative terms, we have for t → ∞, exp(Λt) is rank
deficient. Therefore, we know that W(∞) is also rank deficient, the weight matrix W has vanishing
singular values.
□
B.4 Proof of Lemma 3
Proof. The gradient on matrix W2 is
dL	∂L ∂zi	∂L ∂zi
dW2 =，( ∂zi ∂W2, + 西 ∂W2)
ii
(24)
We denote the gradient on Zi and Zi as gzi and %, respectively. Since ∂∂Wz^ = WιXi and ∂W- =
W1 x0i , we get
W2 = -(dW )T = - X(gziXT + gzi XiT) WT	(25)
dW	i
Similar proof applies to W1.
□
14
Published as a conference paper at ICLR 2022
B.5 Proof of Theorem 2
Here, we prove that under the assumption that singular values are non-degenerate, the alignment
matrix A = V2T U1 converges to identity matrix.
Proof. According to Lemma 3, we have
-d (W1WT) = -WIGTW2 - WTGWT
-d (WT W2) = -WT GWT - WIGT W2
therefore,
-d (WIWT - WT w2) = 0
or
W1 W1T - W2T W2 = C
Next, we show that the Frobenius norm of each weight matrix grow to infinitely.
当∣Wι∣∣F = d^cr(WWT) = -tr(WTGWT) - tr(WιGTW2)
dt	dt
According to Eqn 9, G = -W2W1X, we have
-tr(W2T GW1T) = tr(W2T W2W1XW1T)
= tr(W2W1XW1TW2T)
Because X is a positive definite matrix and for all t, W2 (t)W1(t) 6= 0, we know B :=
W2W1XW1T W2T is positive semi-definite and B 6= 0. Therefore, tr(B) = Pk λk (B) > 0 since
not all eigenvalues of B are zero.
Therefore, we know ||W1 ||2F → +∞ (similarly ||W2 ||2F → +∞). In the limit t- > +∞, we have
W1W1T = W2T W2
Plug in the singular value decomposition of W1 and W2, we have U1S12U1T = V2S22V2T. Assuming
W1 and W2 have non-degenerate singular values, due to the uniqueness of eigen-decomposition, we
have
U1 = V2
therefore,
V2T U1 = I
□
Remark. Note that when the non-degenerate singular value assumption does not hold, the corre-
sponding singular vectors are not unique and we will not observe the corresponding dimensions
becoming aligned.
B.6 Proof of Theorem 3
Proof. According to Theorem 2, for σ1k and σ2k with same index, the corresponding singular vector
pairs v2k and u1k will get aligned, i.e., v2k0Tu1k → δi,j. Therefore, Eqn 21 and Eqn 22 can be simplified
to
T
σ1 → -σ2 (u2 Gv1 )
σk → -σk(UkTGvk)
Insert Eqn 9 and considering the alignment, we derive
σ 1 → σk (σk产(VkTXv1)
σ2 → σk(σ1 )2(vkTXv1)
□
15
Published as a conference paper at ICLR 2022
C Effect of More Layers and Nonlinearity
In our toy model, we focused on a two-layer linear MLP setting. Here, we empirically show that our
theory extends to multilayer and nonlinear cases, as shown in Figure 11a.
Stronger over-parametrization leads to a stronger collapsing effect, which has been shown theoreti-
cally (Arora et al., 2019a; Barrett & Dherin, 2021) and empirically (Jing et al., 2020). This can be
explained by the fact that more adjacent matrices getting aligned, and the collapsing in the product
matrix gets amplified. Note that for a single-layer case, L = 1, there is no dimensional collapse in
the embedding space, which is consistent with our analysis.
S①n-e>」e-n6u-s Jo 6oη
-25 -.	.	.	.	.	.	.
O	2	4	6	8	10	12	14
SinguIarVaIue Rank Index
(a) multiple layers
S①n-e>」e-n6u-s Jo 6oη
-25-.	.	.	.	.	.	.
O	2	4	6	8	10	12	14
SinguIarVaIue Rank Index
(b) nonlinear
Figure 11: Embedding space singular value spectrum with different layers on (a) linear and (b) nonlinear
networks. All models use weight matrices with a size of 16x16. Adding more layers in the network leads to
more collapsed dimensions. Adding nonlinearity leads to a similar collapsing effect.
We empirically show that the collapsing effect also applies to the nonlinear scenario. We insert
ReLU between linear layers and observe a similar singular value collapse compared to the linear
case. See Figure 11b.
D Implementation Detail
D. 1 Augmentations
Each input image is transformed twice to produce the two distorted views for contrastive loss. The
image augmentation pipeline includes random cropping, resizing to 224x224, random horizontal
flipping, color jittering, grayscale, Gaussian blurring, and solarization.
D.2 Network
Throughout the ImageNet experiments in this paper, we use a ResNet-50 (He et al., 2016) as an
encoder. This network has an output of dimension 2048, which is called a representation vector.
D.3 Optimization
We use a LARS optimizer and train all models for 100 epochs. The batch size is 4096, which fits
into 32 GPUs during training. The learning rate is 4.8 as in SimCLR (Chen et al., 2020a), which
goes through a 10 epoch of warming up and then a cosine decay schedule.
E	HYPERPARAMETER TUNING ON d0
Here, we list the ImageNet accuracy with various d0 value in Figure 12. It’s easy to see that when
d0 → 0, there’s too little gradient information coming from the loss, the performance drops. When
d0 → 2048, the model converges to standard SimCLR without a projector, which we know suffers
from dimensional collapse in representation space.
16
Published as a conference paper at ICLR 2022
Figure 12: Hyperparameter tuning on d0 based on ImageNet linear probe Top-1 accuracy.
F Ablation S tudy Detail
Fixed low-rank projector vs Fixed low-rank diagonal projector: DirectCLR is equivalent to
SimCLR with a fixed low-rank diagoanl projector. It performs the same as a SimCLR with fixed
low-rank projector, which achieves 62.3% linear probe accuracy. Specifically, the singular values of
this low-rank matrix are set to have d0 numbers of 1 and 0 for the rest, then left- and right- multiply
a fixed orthogonal matrix. Therefore, their only difference is that this fixed projector has an extra
fixed orthogonal matrix in between.
Trainable projector vs trainable diagonal projector: We trained a SimCLR model with a trainable
projector that is constrained be diagonal. The model achieves 60.2% linear probe accuracy on
ImageNet, which is close to a SimCLR with a 1-layer linear projector.
Orthogonal projector vs no projector: We train a single layer projector SimCLR model with
orthogonal constraint using ExPM parametrization (Casado & Martlnez-Rubio, 2019). Therefore,
the projector weight matrix has all singular values fixed to be 1. This model reaches 52.2% accuracy
on ImageNet which is close to a SimCLR without projector.
These ablation studies verify the propostion 1 that the SimCLR projector only needs to be diago-
nal. Also, according to Table 2, we find that low-rank projector setting consistently improves the
performance, which verifies proposition 2.
Linear probe on subvector instead of the entire vector: For DirectCLR, we perform a linear
probe only on the sub-vector z and get 47.9% accuracy on ImageNet. This shows that the rest of r
still contains useful information even though it does not see gradient directly coming from the loss
function.
Random dropout instead of fixed subvector: Since DirectCLR drops out a number of dimensions
for the loss function, it would be natural to ask whether random dropping out can reach the same
performance. We train a SimCLR model without a projector and randomly feed d0 number of
features to InfoNCE loss every iteration. This model reaches only 43.0% accuracy on ImageNet.
This demonstrates the importance of applying a fixed subvector, which allows the alignment effect
to happen.
17