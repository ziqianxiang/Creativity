Published as a conference paper at ICLR 2022
Tuformer: Data-driven Design of Transform-
ers for Improved Generalization or Efficiency
Xiaoyu Liu
Department of Computer Science
University of Maryland, College Park
MD 20740, USA
xliu1231@umd.edu
Jiahao Su
Department of Computer Science
University of Maryland, College Park
MD 20740, USA
jiahaosu@umd.edu
Furong Huang
Department of Computer Science
University of Maryland, College Park
MD 20740, USA
furongh@umd.edu
Ab stract
Transformers are neural network architectures that achieve remarkable performance
in many areas. However, the core component of Transformers, multi-head self-
attention (MHSA), is mainly derived from heuristics, and the interactions across its
components are not well understood. To address the problem, we first introduce a
mathematically rigorous and yet intuitive tensor diagram representation of MHSA.
Guided by tensor diagram representations, we propose a novel design, namely
Tunable Transformers (Tuformers), by allowing data-driven weights across heads,
whereas MHSA adopts pre-defined and fixed weights across heads, as will be
explained in our paper. Tuformers naturally reveal a flexible design space that
a user, depending on the needs, can choose a structure that has either improved
performance (generalization error) or higher model efficiency. Any pre-trained
Transformer can be an initialization of the corresponding Tuformer with trainable
number of heads for efficient training and fine-tuning. Tuformers universally
outperform Transformers on various tasks across multiple domains under a wide
range of model sizes.1
1	Introduction
Transformer models are first introduced by Vaswani et al. (2017) in the context of sequence modelling.
They have demonstrated impressive results on a wide variety of tasks in many fields, such as
language model pre-training (Sanh et al., 2019), speech recognition (Moritz et al., 2020), image
classification (Dosovitskiy et al., 2020), and generation (Parmar et al., 2018). The core component of
Transformer models is the multi-head self-attention (MHSA) which is extended from the standard
attention mechanism (Bahdanau et al., 2014). Each attention head in MHSA has a global receptive
field, i.e., each token’s representation is updated by attending to all other tokens, and H attention
heads are computed in parallel and concatenated together.
The current MHSA design is mainly derived from empirical studies or heuristics, leaving some
unresolved challenges. (1) Lack of solid theoretical understanding. The theory behind MHSA is
only starting to catch up with practice. The role of the components and their interactions in MHSA
are not well understood rigorously from a theoretical perspective, which may lead to inefficiencies in
the design. For example, Michel et al. (2019) show that most attention heads can be removed in the
testing phase without much performance compromise, while Cordonnier et al. (2020) find empirical
evidence of redundancy in key/query projections and propose a re-parameterization scheme. These
works focus more on practical solutions, leading to questions about whether theoretical patterns exist
in these designs. (2) The number of heads is not trainable. Intuitively, the heads in multi-heads
1code available at : https://github.com/umd-huang-lab/tuformer
1
Published as a conference paper at ICLR 2022
are expected to capture different context information through each head. However, the number of
heads is fixed in training. Thus, although we could tune the hyper-parameter, an exhaustive search
would be time-consuming or not practical for large-scale tasks. (3) Hard to analyze the expressive
power. Analyzing the expressive power (as will be defined in Definition 3) of a neural network, i.e.,
proving that some architectures are more expressive than others, is a non-trivial and challenging task.
Such analysis has been done in Convolutional Neural Networks (CNNs) (LeCun et al., 1995; Cohen
et al., 2016), Recurrent Neural Networks (RNNs) (Mikolov et al., 2010; Khrulkov et al., 2018), but
no such work exists for interpreting MHSA or guiding the structural design.
In response to the above challenges, we first interpret MHSA from a tensor representation perspective
using the intuitive graphical tool, the tensor diagram. Current prevalent descriptions of MHSA use
flow charts to convey high-level intuitions, which could cause ambiguities. Therefore, it is inevitable
to pair those flow charts with mathematical formulas to understand the mechanism precisely. However,
these two separated descriptions create difficulties for interpretations and inspections of the operations
implemented in MHSA. To address this issue, we propose a graphical representation of MHSA,
which is both semantically intuitive and mathematically rigorous. Specifically, we modify and
extend the vanilla tensor diagrams (Penrose, 1971), which conveniently allow for rigorous graphical
representation of multi-linear operations between multi-dimensional arrays (i.e., higher-order tensors)
to represent nonlinear operations.
Using tensor diagram representation, we project the current design of MHSA into its tensor form,
which renders a holistic view of the weights in MHSA for better interpretation of the information flow
and exchange among the components of MHSA. We then propose a novel data-driven structure,
namely Tunable-Head Self-Attention (THSA), which is a re-parameterization of the weight matrices
to allow learnable weight across heads. Transformers with THSA, named Tunable Transformers
(Tuformers), have several advantages compared against vanilla Transformers: (1) A guaranteed higher
expressive power. We prove that MHSA is a special case of THSA. (2) The number of heads is
trainable. The concept of the number of heads in THSA generalizes to the stable rank of the core
matrix, allowing data-driven implicit training. (3) Tuformers allow initialization from pre-trained
Transformers such as BERT (Devlin et al., 2019) and its variants.
More importantly, we formulate a flexible design space for MHSA where we explore the trade-off
between generalization and efficiency, which can be adjusted by the expressive power if there is no
over-fitting. Users can choose structures with either higher model efficiency or better generalization
that satisfy their own needs. The proposed design space further allows improving the expressive power
of the network by finding better tensor representations of the weights through tensor representation
theory in future followup works.
We experiment Tuformers with several tasks across multiple domains, from language modeling, ma-
chine translation to image generation under a wide range of model sizes. We demonstrate competitive
results not only on Tuformers but also in cases where Tuformers are initialized with pre-trained
Transformers for other downstream tasks, when combined with Linear Transformer (Katharopoulos
et al., 2020) on the image generation task and their efficient variants in the design space.
Summary of Contributions:
(1)	We propose a mathematically rigorous and semantically intuitive tensor diagram representation
of the multi-head self-attention, introducing a new tool to the ML community for future studies on
interpretation and improvements of Transformers.
(2)	We propose a novel design of the MHSA, Tunable-Head Self-Attention (THSA), resulting in
Tuformers. Tuformers are structurally more expressive and are showing improved generalization
error experimentally.
(3)	We formulate a flexible design space for attention unit design where users can choose to design
structures, depending on the needs, that have better generalization or efficiency.
2	Tensor Diagram Representation
Notations. We use lower case letters (e.g., v) to denote vectors, upper case letters (e.g., M) to
denote matrices, and curly letters (e.g., T) to denote general tensors. For a tensor T ∈ RI1 × IM,
we refer to the number of indices as order, each individual index as mode, and the length of one mode
as dimension. For instance, T is an Mth order tensor that has dimension Im at its mth mode. We
reserve superscripts to distinguish similar arrays (e.g., WQ, WK, WV are query/key/value weight
2
Published as a conference paper at ICLR 2022
matrices), and subscripts to index the elements in an array (e.g., Wij is the (i, j)th element of W).
We use colon : to slice an array (e.g., Wi,: denotes the ith row of W).
We propose to use tensor diagrams (Penrose, 1971), a commonly used rigorous/precise and intuitive
graphical representation for multi-linear operations among higher-order arrays (i.e., tensors), to
represent multi-head self-attention (MHSA). Since MHSA consists of multilinear operations and a
nonlinear softmax function, we will introduce tensor diagrams and our novel design extending tensor
diagrams to denote the composition of nonlinear and multilinear operations in MHSA.
2.1	Tensor Diagram Basics
Arrays denoted as nodes with legs. An array
is represented as a node with leg(s) in a tensor
diagram as shown in Figure 1. We denote the
order (the number of dimensions) of the array
by the number of legs extending from the node.
Each labeled leg represents one mode of a tensor.
Every mode of the tensor needs to be uniquely
labeled. We usually use the dimension of the
mode as the label (i.e., an associated positive
integer written on top of each leg). The legs
Vector	Matrix 3 dimensional array
order 1 tensor order 2 tensor order 3 tensor
Figure 1: Arrays in tensor diagram. A vector is de-
noted as a node With 1 leg, a matrix as a node With 2
legs and an N -dimensional array as a node with N legs.
do not need to be straight lines, and their orientations do not matter. Matrices M ∈ Ra×b and
M> ∈ RB×A
can be represented via the same tensor diagram as long as the M node has tWo legs
(to denote that it is a matrix, Wherever the legs extend to), labeled as A and B (to denote its size).
K
Amr Bnr = Cnmr
EAmk Bk
k=1
Cmn
exp(α ∙ Amn)	_ C
PN=ι exp(α ∙ Amn) = Cmn
n
Figure 2: Tensor diagrams for atomic operations. (1) (left) Contraction is an operation that generalizes
the matrix multiplication. It sums element-Wise products on a mode in object A and a corresponding mode
(With the same dimension) in object B (i.e., along a leg in node A and a corresponding leg in node B). In the
tensor diagram, multiplying tWo matrices (or higher-order tensors With more than 2 legs) corresponds to “gluing”
their corresponding legs (on a certain mode). (2) (middle) Softmax is an element-Wise exponential function
normalized along a certain mode. We propose to denote the α-scaled softmax function softmax(αA) on A as a
dotted box with a labeled filled ball (to distinguish itself from tensor objects, i.e., nodes Which are blank circles)
attached to one leg. (3) (right) Batch multiplication is an element-Wise product along the connected legs.
Operations in tensor diagrams. There are three types of operations in the calculation of MHSA:,
contraction, softmax, and batch multiplication, as shoWn in Figure 2 and explained in its caption.
Evaluation of tensor diagrams: (1) Evaluation order. We can evaluate a tensor diagram in any
pair-Wise order except for the nodes in the softmax box. Since the softmax function is nonlinear, We
must first evaluate the nodes in the softmax box With arbitrary order before those outside the box.
(2) Reading a tensor diagram. We can easily identify the output shape by the dangling edges. For
instant, in Section 2.1, there are three dangling legs M, N, R. Thus the output is a 3rd order tensor
With dimensions M, N, R. Note that the softmax function does not change the shape of the tensor.
Advantages of tensor diagrams: (1) Tensor diagram is orientation invariant, meaning that We can
represent X and X > using the same tensor diagram. Thus in multi-head, We obtain a universal
graphical representation regardless of Whether We represent an embedding of each token in the
sequence as roWs or columns of the input embedding matrix X . (2) Multi-linear operations are
concisely represented and interpreted. (3) The representation is both precise in math and intuitive,
and the information floW is clear, making analysis of netWork structure more accessible. In addition,
With the labels of the legs, We can read the model complexity explicitly.
We include a comprehensive introduction to tensor diagram representation in Appendix A.
3
Published as a conference paper at ICLR 2022
2.2	Tensor Diagram Representation of multi-head self-attention
The core of Transformer models is a multi-head self-attention (MHSA) module which allows the
model to jointly attend to information from different representation sub-spaces at different posi-
tions (Vaswani et al., 2017). To distinguish inputs for query, key, value matrices, we use XQ ∈ RN×F,
XK, XV ∈ RM×F respectively. The MHSA module outputs a matrix M ∈ RN×F as
Q[h] =XQW[Qh]; K[h] =XKW[Kh]; V[h] =XVW[Vh],	(1a)
head®] = SoftmaX (Q[h]K>∕√D) V⅛	(1b)
M = [head[i], head.],…，head[H ]] W O.	(1c)
In the above equations, we see how MHSA computes its output in three steps:
(1a) Linear transformation on input embedding to get latent features. For each h, the query/key/value
matrices Q[h] , K[h] , V[h] are linear transformations, parameterized by W[Qh] , W[Kh], and W[Vh] ∈
RF ×D respectively, of the input embedding.
(1b) Scaled dot-product attention. Each head[h] ∈ RN×D computes a scaled dot-product be-
tween a latent feature matrix Q[h] ∈ RN×D (i.e., a query matrix) and another latent feature matrix
K[h] ∈ RM×D (i.e., a key matrix) along D, and applies a softmax function to obtain the weights to
be multiplied with a third latent feature matrix V[h] ∈ RM ×D (i.e., a value matrix).
(1c) Concatenation and contraction for MHSA. After concatenation of the H heads, a linear trans-
formation of the concatenated heads, parameterized by WO ∈ RHD×F, is implemented to get the
(a) Single-head Self-Attention	(b) Multi-Head Self-Attention
Multi-head self-attention in tensor diagram. The concatenation of H matrices {head[h] }hH=1 can
be viewed as constructing a three dimensional array with the third mode being the number of heads,
i.e., three-legged nodes in tensor diagram notation. This way, the weight matrices {W[Qh] }hH=1 are
stacked to be an order 3 tensor WQ . It is the same with other weight matrices. Since in MHSA,
the attention calculation among key, query and value matrices are done separately within each head,
batch multiplication along mode H is a proper way to denote these “head-wise” operations. Finally,
we connect the resultant tensor with W O to obtain MHSA as in Figure 3b. See Appendix B for more
details and the proof.
3	Tuformers: Data-Driven Design with Higher Expressive Power
3.1	Motivation: Interpretation of MHSA
In MHSA, Q[h] and K[h] can only contribute to head[h] , ∀h. In order to characterize the information
flow in MHSA, we stack the weight matrices W[Qh], ∀h together (similarly for W[Kh] and W[Vh]) and
introduce a matrix C to interpret MHSA. Guided by its tensor diagram representation in Figure 3b,
C is clearly characterized as in Proposition 1.
Proposition 1 (Interpretation of MHSA). With weight matrices Stacked as WQ :=
[W[Q,…，W[H]], WK := [W[K …，W[H]], WV := [W[V，…,W[H]] ∈ RF×DH, and a
core matrix C := IH 0 (1d 1]) ∈ Rdh×dh ( i.e., a Kronecker product of an all-ones matrix
1D 1>D ∈ RD×D and an identity matrix IH ∈ RH×H), MHSA is equivalent to
4
Published as a conference paper at ICLR 2022
Q = X QW Q; K = X K WK ； V = X V W V,
DH
headr = Softmax	Crs QsKs	Vr,
M = [head1, head2,…，headDH] W O.
(2a)
(2b)
(2c)
In tensor diagram notation, MHSA is equivalent to Figure 4a. Here, we use Qr, Kr, Vr ∈ RF,
headr ∈ RN to denote the rth column of the corresponding matrices, and Crs to denote the (r, s)th
element of C.
Proposition 1 is proved in Appendix D.1. Note that {headι,…，head。} correspond to head[i],
{headD+ι, ∙∙∙ , head?。} correspond to head[2] and so forth. The intuition of interpreting MHSA
using Proposition 1 comes from the tensor diagram graphical representation of MHSA structure.
From the tensor diagram of MHSA as shown in Figure 3b, we see two types of contractions: latent
contractions along D and head contractions along H. The former corresponds to (1D1>D) part of C
and the latter corresponds to IH part of C as shown in Figure 4a.
Motivation for an improved design: from MHSA (a preset C) to THSA (a learnable C). In
MHSA, with the preset specific C = IH 0 (1D1>D), it restricts the possible weight sharing. In
other words, Q[h] and K[h] can only contribute to head[h] , ∀h. However, there is no motivation why
such a preset and fixed C is desirable or why we do not allow Q[h] or K[h] to contribute to other
heads. Our motivation for an improved design is simple: rather than presetting the core matrix as
C = IH 0 (1D1>D), we allow a data-driven learnable C, and we call such a design Tunable-head
self-attention (THSA). Now with the learnable C in THSA, we allow Q[h] and K[h] to contribute to
other heads head[h0], where h0 6= h. In other words, THSA allows learnable weights across heads.
(a) Multi-head Self-Attention
(MHSA)
c = IH ⑥(ιD Id)
(b) Tunable-Head Self-Attention
(THSA)
C fully trainable
(c) C as a design space: an
example.
C = Cl *(C>C3).
Figure 4: MHSA as a special case of THSA. Figure 4a (left) represents MHSA. Figure 4b (middle) represents
THSA. As shown in Proposition 1, weight matrices in MHSA collaborate in a very specific way. When
C = IH ⑥(1> 1d ) in Proposition 1, THSA degrades to to MHSA. Therefore, it is natural to make C fully
trainable for higher expressive power. THSA also reveals a design space. For example, we can extend MHSA by
allowing trainable components C1 , C2 and C3 as in Figure 4c.
3.2	A Novel Design: Tunable-Head Self-Attention (THSA)
As shown in Figure 4b, a Tunable-head self-attention (THSA) 2 module has the same input and
output domains as an MHSA, i.e., it takes the input embedding XQ ∈ RN ×F and XK, XV ∈
RM ×F as inputs and returns a matrix T ∈ RN ×F as output. A THSA, with five weight matrices
WQ, WK,WV ∈ RF×R, WO ∈ RR×F, andC ∈ RR×R, is mathematically described as
Q= XQW Q; K= XKWK; V= XVWV,	(3a)
Ωr = Softmax (XX Crs QsKsr∕√r) Vr,	(3b)
T = ΩW O = [Ωι, Ω2,…，Ωr] W O.	(3c)
The intermediate result Ωr∙ ∈ RN is the rth column of the matrix Ω. In MHSA, the softmax function
is scaled, which normalizes its input by √D, i.e., the square root of the latent dimension. The scaling
2THSA improves MHSA and only modifies the intra-layer structure. It does not change the inter-layer
structure outside MHSA.
5
Published as a conference paper at ICLR 2022
ensures that the distribution of the attention matrix approximates a standard Gaussian distribution. To
achieve a similar effect, in THSA, We change the constant to ʌ/R accordingly.
THSA generalizes MHSA. THSA has learnable C. If C = IH 0 (ID lD), i.e., a Kronecker product
of an all-ones matrix 1D 1>D ∈ RD×D and an identity matrix IH ∈ RH×H, the THSA reduces to an
MHSA With H heads and latent dimension D , as illustrated in Figure 4a. We call a Transformer With
THSA module as Tunable Transformers (Tuformers). The tensor diagram representation of THSA is
given in Figure 4b. It has an learnable core matrix C compared to MHSA in Figure 4a Which has a
preset core matrix.
Tuformers can be initialized with Pre-trained Transformers. Although different in the structure,
Tuformers can alWays be initialized With pre-trained Transformer models through re-parameterization.
MHSA is updated by Weight matrices {W[Kh]}hH=1, {W[Qh]}hH=1, {W[Vh]}hH=1 and WO While in THSA
We have WQ, WK, WV , WO and C. To initialize THSA With MHSA, We folloW the scheme
proposed in Proposition 1. C is the kronecker product betWeen a D × D all-one matrix and an
H × H identity matrix, Where D is the latent dimension and H is the number of heads in MHSA.
WO remains the same as that of MHSA. As for WQ, WK and WV ∈ RF×R, We concatenate
{W[Kh]}hH=1,{W[Qh]}hH=1,{W[Vh]}hH=1 ∈RF×DalongmodeD.
The notion of heads in THSA. Due to the learnable C and data-dependent Weight sharing mecha-
nism, the notion of heads in THSA is not as obvious as in MHSA. Although We have an upper bound
R of the number of heads, Which is achieved When the learned C decides not to share any Weights
across heads, the actual effective number of heads depends on C. As a result, We define the number
of heads in THSA as the stable rank of the core matrix C, Which is i σi2 maxi σi , Where σi is the
singular value of C. We use the stable rank because it is largely unaffected by tiny singular values.
THSA as a flexible design space: C in THSA ( Figure 4b ) naturally reveals a design space. In
MHSA, C is completely fixed and in THSA C is fully trainable. We can also have flexible C designs
for user-specific needs of either increasing generalization or improving efficiency.
X Q
WK
C = Ih ® CC3)
M
V
F
D
C2
1
(a) skip-left
C = IH ⑤(C>C3)
(b) skip-right
C = Cl ⑤(1D 1d )
(c) trainable heads-only
C= C1
(d) heads-only
C=IH
D
Figure 5: C naturally reveals a design space. We can obtain better generalization or better efficiency by alloWing
fully trainable C, partially trainable C, pre-set fixed C and etc. We demonstrate some of them extending from
the structure of MHSA Where C takes the form of a Kronecker product as shoWn in Figure 4c. The relationship
betWeen the generalization and efficiency of these designs is summarized in Figure 6.
Better generalization. Better generalization can be
obtained by increasing the expressive poWer if there is
no over-fitting. A naive Way to increase the expressive
poWer is to add more parameters in the Weight matrices
in MHSA, such as increasing the latent dimension D or
the number of heads H . We find that a more effective
Way to increase the expressive poWer for better gen-
eralization is to find more expressive structures. We
demonstrate a feW of them in Figure 5 and Figure 4b.
In MHSA, C is pre-set fixed Where C = IH 0 (1>D 1D).
In Figure 5a (skip-left C = IH 0 (C2>C3) ) and Fig-
ure 5b (skip-right C = IH 0 (C2>C3) ), We maintain
More Efficiency
Figure 6: We can divide all structures listed
in Figure 4 and Figure 5 into different groups
by their expressive poWer. The higher group
a structure is in, the higher expressive it has.
Better generalization can be obtained in more
expressive structures. We can also go for very
efficient ones if We are Willing to sacrifice some
generalization. More details in Section 4.
the form of a Kronecker product but alloW either C1 or
C2 , C3 trainable. We can think of skip-left and skip-
right as skip-connection designs in C. In THSA, C is
fully trainable. It is easy to observe an increasing in
expressive poWer from MHSA to skip-left, skip-right,
and finally THSA, together With an increasing in generalization in the same order. Our empirical study
6
Published as a conference paper at ICLR 2022
also show that these structurally more expressive models more effectively improve the generalization
than naively increase the number of parameters.
More Efficiency. We can obtain different levels of efficient structures in the design space by adjusting
the expressive power. Compared to MHSA, skip-left and skip-right have more generalization in
various tasks but almost the same number of parameters. Being special cases of skip-right, Figure 5c
( trainable heads-only C = C1 ) and Figure 5d ( heads-only C = IH ) are less expressive. However,
trainable heads-only only has around 6% of the parameters compared to MHSA while heads-only
only has 1.5% with most performance maintained.
There is a trade-off between generalization and efficiency in the design space C as demonstrated in
Figure 6. We empirically verify that without over-fitting, generalization and efficiency have similar
hierarchy as expressive power. Thus we can design structures in different level of expressive power,
tailoring needs for either better generalization or more efficiency. THSA has the best generalization,
but 125% parameters compared to MHSA. Since skip-left and skip-right are special cases of THSA,
it is natural that their expressive power and generalization are lower. However, they are more efficient.
With almost the same number of parameters, they have better generalization than MHSA. Heads-only
and trainable heads-only are very efficiency sacrificing an acceptable amount generalization. More
details can be found in Section 4.
4	Experiments
Datasets, tasks and evaluation metrics. We evaluate Tuformers on 7 datasets for 5 tasks: word-level
Language Modeling (LM) on Penn Treebank (PTB) (Marcus et al., 1993), Neural Machine Translation
(NMT) on WMT16 ‘English-German’ (Sennrich et al., 2016), Automatic Speech Recognition (ASR)
on LibriSpeech (Panayotov et al., 2015), Natural Language Inference (NLI) on MNLI (Williams
et al., 2018) and QNLI (Wang et al., 2018) and Image Generation on CIFAR10 (Krizhevsky, 2009)
and MNIST (Deng, 2012) datasets.
Baselines. We compare the performance of MHSA module with THSA module in the following 5
backbone architectures: (1) a vanilla Transformer model (Vaswani et al., 2017), (2) an Transformer-
XL model (Dai et al., 2019), (3) a BERT-large model (Devlin et al., 2019), (4) a RoBERTa-large
model (Liu et al., 2019), and (5) an ALBERT model (Lan et al., 2019). In all baselines, we adopt
all default settings used in the original paper (hyperparameters specified in Appendix E), and only
replace/re-parameterize the MHSA to THSA.
4.1	Empirical Results of Tuformers.
In this part, we first show that THSA (used in Tuformers) universally outperforms MHSA (used in
Transformers) diverse tasks under different model scales and THSA is simple to use in practice.
(1)	THSA consistently outperforms MHSA on diverse datasets for a variety of tasks under different
model scales. In LM, NMT and ASR tasks, we compare THSA against the MHSA models under
different model scales. MHSA is parameterized by four weight matrices WQ, WK, WV and WO
whereas THSA adds an additional core matrix C of the same size. Adopting the size of the four
original weight matrices in MHSA to THSA incurs an 25% increase in the number of parameters.
For a fair comparison, we first compare MHSA against THSA with the same number of parameters
(thus with smaller query/key/value/output weight matrices). To get a full picture, we also compare
MHSA against THSA with the same sized four original weight matrices. As shown in Figure 7,
under the same number of parameters, our THSA (blue curve) universally outperforms MHSA (red
curve) under all model scales. By increasing the number of parameters of the attention units by 25%
3 (i.e., adding the core matrix C while keeping the size of the query/key/value/output weight matrices
the same), our THSA (purple curve) further improves the performance universally under all model
scales.
(2)	Tuformers initialized with pre-trained strong models further improve the performance.
In NLI task, we initialize Tuformers with pre-trained BERT-large, RoBERTa-large and ALBERT
models, and perform the same fine-tuning tasks. As shown in Table 1, THSA is compatible with
3As will be discussed later, the overall model size is not drastically increased.
7
Published as a conference paper at ICLR 2022
Language Modeling on Penn Treebank
10%	20%	50%	100%	150% 200%
model scale
10%	20%	50%	100%	150% 200%
model scale
10%	20%	50%	100%	150% 200%
model scale
Figure 7: Performance comparison of THSA and MHSA in (left) language modeling on Penn Treebank
dataset on TransformerXL, (middle) neural machine translation on WMT16 English-German dataset and (right)
automatic speech recognition on Librispeech on Transformers. Under the same number of parameters, our THSA
universally outperforms MHSA under all model sizes. The error bars displayed in shades are generated using 10
random runs in LM and NMT and 3 random runs in ASR.
Table 1: (Left) THSA initialized with pre-trained models. Comparison of accuracies obtained for language
inference task on MNLI and QNLI datasets, the higher the better. Results show that Tuformers can be
conveniently initialized with pre-trained models for downstream tasks, and fine-tuning obtains impressive
performance. (Right) THSA +kernels. Comparison of bits per dimension (BPD) for image generation task on
MNIST and CIFAR-10 datasets, the lower the better. Results show that Tuformers can be extended to image
generation tasks and improve the performance of other efficient designs with linear computational and memory
complexities.
Models	MNLI	QNLI	Models	MNIST	CIFAR 10
BERT (Devlin et al., 2019) BERT + THSA	84.1% 84.9%	92.1% 92.8%			
			ELU kernel (Katharopoulos et al., 2020)	0.72	3.51
RoBERTa (Liu et al., 2019)	89.4%	94.3%	ELU kernel + THSA	0.65	3.42
RoBERTa + THSA	90.2%	94.7%	Polynomial kernel (Tsai et al., 2019)	0.64	3.47
ALBERT (Lan et al., 2019) ALBERT + THSA	89.1% 89.5%	97.9% 98.1%	Polynomial kernel THSA	0.57	3.37
existing pre-trained Transformer models and there is no need to train Tuformers from scratch: fine-
tuning THSA initialized from pre-trained BERT and its variants further improves their performance.
(3)	Combing THSA with kernels to obtain linear computation and memory complexities in sequence
length. Tuformers can also be incorporated into efficient variants of Transformers for lower computa-
tion and memory complexities. We experiment on image generation tasks on CIFAR10 (Krizhevsky,
2009) and MNIST (Deng, 2012) datasets as in Table 1 and show that Tuformers obtain strong
performance and linear space complexity when combined with kernels.
4.2	Flexible design space
(1)	THSA reveals a flexible design space C for Transformer. We empirically demonstrate that there
exists a trade-off between generalization and efficiency in this design space, which could be adjusted
by the expressive power. By making C fully-trainable, partially trainable or pre-set fixed as shown in
Figure 5 and Figure 4, we demonstrate that some structures in C improves the generalization while
some other designs improve the efficiency of MHSA.
(2)	Models with higher expressive power in the de-
sign space have better generalization. As shown in
Table 2 and Figure 7, we compare two ways that in-
crease the expressive power for better generalization:
naively increasing the number of parameters and de-
signing more expressive structures. Both approaches
increase generalization. However, naively increas-
ing the number of parameters is not as effective as
adopting more expressive structures. With the same
number of parameters, more expressive structures,
such as THSA, skip-left and skip-right, universally
outperform MHSA.
Table 2: Performance in the flexible design space.
Models	NMT (BLEU)	# params	Train	Test
MHSA	289	100%	10.6	3.2
MHSA with 125% params	29.0	125%	11.2	3.6
MHSA with 1.5% params	20.2	1.5%	6.4	2.5
THSA	29.4	125%	11.5	3.9
skip-right C = Ci ® (iDid)	29.3	101%	10.6	3.2
skip-left C = IH ® (C> C3)	29.2	100%	10.6	3.2
trainable heads-only C = C1	27.4	6%	7.4	2.8
heads-only C = IH	27.1	1.5%	6.1	2.3
8
Published as a conference paper at ICLR 2022
(3)	Models can also be made very efficient. Moreover, in Figure 5c ( trainable heads-only C = C1 ),
we trade generalization for efficiency. With 6% parameters, we maintain 95% performance in the
NMT task. In a more extreme case, if we only maintain the head contraction as shown in Figure 5d,
the number of parameters goes down to 1.5% with a 42% faster training and 30% faster testing.
Surprisingly, almost 94% performance is obtained with this light-weighted model.
4.3	Ablation study: check-pointing to alleviate memory overhead without
kernels.
Although an effective design with guaranteed higher expressive power, Tuformers come with an extra
memory overhead when calculating values within the softmax. In Tuformers, if done naively, the
memory overhead is MNR where N, M denote the sequence length and R = DH is the rank of the
core tensor, incurring a D times larger memory overhead than vanilla Transformers in calculating
softmax. Combining Tuformers with some state-of-the-art kernel-based efficient Transformers such
as Linear Transformers using ELU (Katharopoulos et al., 2020) and polynomial (Tsai et al., 2019)
kernels, we reduce the computation and memory complexities of Tuformers to be linear in the
sequence length by removing the nonlinearity constrain in the softmax.
We can also alleviate the D times larger memory overhead compared with vanilla Transformers
in calculating softmax without using kernels.Specifically, we use check-pointing to eliminate the
memory overhead in THSA. Since no intermediate results are saved in the forward pass using check-
pointing, we recalculate the intermediate results of the softmax box in the back-propagation, which
introduces some computational overhead. We observe that applying check-pointing to Tuformers
leads to only a slight increase in training time while relieving the memory overhead. We can reduce
or even avoid the repetitive calculations by developing a proper backward module, although such
development is beyond the scope of this paper and deferred for future works. See Figure 12 for more
details.
5	Related Works
Analysis of MHSA. Given the popularity of Transformers in a wide range of domains, a line of
works focuses on understanding and improving MHSA. Voita et al. (2019) proposed a practical
scheme to prune less informative heads. Michel et al. (2019) also showed that most attention heads
are removable in testing without much performance compromise. Cordonnier et al. (2020) found
empirical evidence of redundancy in key/query projections and proposed a re-parameterization
scheme. These works bring valuable insights into MHSA, but a consistent theoretical analysis for
these findings is missing. Our work differs from these works in that we focus on the theoretical
understanding of MHSA in terms of expressive power, and we provide theoretical guarantees for a
novel design, Tuformers, with higher expressive power and data-driven trainable heads.
Efficient Transformers. MHSA has a global receptive field, i.e., each token’s representation is
updated by attending to all other tokens, therefore incurring a quadratic memory and computation
complexities concerning the sequence length. An extensive line of works focusing on reliving such the
dependencies on the sequence length, such as Performer (Choromanski et al., 2020), Reformer (Kitaev
et al., 2020), Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020).
Our proposed model, Tuformers, can be incorporated into some kernel-based efficient Transformers,
which applies kernel-based methods to MHSA to remove the nonlinearity constraint brought by the
softmax function, achieving the same state-of-the-art linear computation and memory complexities in
the sequence length.
6	Conclusion
This paper introduces a mathematically rigorous yet intuitive tensor diagram representation of MHSA,
formulates a design space where we can analyze the expressive power of MHSA and its variants and
proposes Tuformers, a novel model design with a guaranteed higher expressive power. Furthermore,
Tuformers have a data-driven structure where heads can be trained implicitly and initialized with
pre-trained Transformer models. The introduction of the tensor diagram representations and the
theory presented in this paper provide new tools and open new directions for future research searching
for expressive and efficient architectures.
9
Published as a conference paper at ICLR 2022
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al.
Rethinking attention with performers. In International Conference on Learning Representations,
2020.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on learning theory, pp. 698-728. PMLR, 2016.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate
instead of concatenate. arXiv preprint arXiv:2006.16362, 2020.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, 2019.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141-142, 2012.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2020.
AngeIos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Frangois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International Conference on Machine
Learning, pp. 5156-5165. PMLR, 2020.
Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural
networks. In International Conference on Learning Representations, 2018.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
International Conference on Learning Representations, 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The
handbook of brain theory and neural networks, 3361(10):1995, 1995.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Jerry Ma and Denis Yarats. On the adequacy of untuned warmup for adaptive optimization. In
Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Computational Linguistics, 19(2):313-330, 1993.
10
Published as a conference paper at ICLR 2022
Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in
Neural Information Processing Systems, 32:14014-14024, 2019.
Tomas Mikolov, M. Karafidt, L. BUrgeL J. Cernocky, and S. KhUdanpur. Recurrent neural network
based language model. In INTERSPEECH, 2010.
Niko Moritz, Takaaki Hori, and Jonathan Le. Streaming automatic speech recognition with the
transformer model. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 6074-6078. IEEE, 2020.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48-53, 2019.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In 2015 IEEE international conference on acoustics, speech
and signal processing (ICASSP), pp. 5206-5210. IEEE, 2015.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055-4064.
PMLR, 2018.
Roger Penrose. Applications of negative dimensional tensors. Combinatorial mathematics and its
applications, 1:221-244, 1971.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems
for wmt 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared
Task Papers, pp. 371-376, 2016.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Transformer dissection: An unified understanding for transformer’s attention via the lens of
kernel. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 5797-5808, 2019.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018.
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101.
11
Published as a conference paper at ICLR 2022
A Supplementary Material for Tensor Diagrams
In this section, we provide a comprehensive introduction to the tensor diagram representation.
A.1 Components of Tensor Diagram: Arrays as Nodes with Leg(s)
Arrays denoted as Nodes with Legs. An array is represented as a node with leg(s) in a tensor
diagram as shown in Figure 1. The order (the number of dimensions) of the array is denoted by the
number of legs extending from the node. The legs do not need to be straight lines ,and the orientation
does not matter. For example, matrices A and A> are equivalent in this notation. Each labeled leg
represents one mode of a tensor. Every mode of the tensor needs to be uniquely labeled. We usually
use the dimension of the mode as the label (i.e., an associated positive integer written on top of each
leg). If multiple modes of the same tensor have the same length, we use subscripts to differentiate
them.
A.2 Operations in Tensor Diagram
K
EAmk Bk
k=1
Cmn
n
Figure 8: (1) Contraction as shown in Appendix A.2 is an operation that generalizes the matrix multiplication.
It does summation on element-wise products along a mode in object 1 and a corresponding mode (with the same
dimension) in object 2 (i.e., in tensor diagram language, along a leg in node 1 and a corresponding leg in node 2).
In tensor diagram, multiplying two matrices (or higher-order tensors with more than 2 legs) corresponds
to “gluing” their corresponding legs (along a certain mode). (2) Softmax as shown in Appendix A.2 is an
element-wise exponential function normalized along a certain mode. Tensor diagram has a convention to denote
the contraction, but not the softmax. We propose to denote the α-scaled softmax function softmax(αA) on A
as a dotted box with a labeled filled ball (to distinguish itself from tensor objects, i.e., nodes which are blank
circles) attached to one leg. (3) Batch Multiplication, as shown in appendix A.2, is an elementwise product
along the connected legs.
There are three types of operations:contraction, softmax and batch multiplication. Contraction
is an operation that does summation on element-wise products along a mode in object 1 and a
corresponding mode (with the same dimension) in object 2 (i.e., in tensor diagram language, along a
leg in node 1 and a corresponding leg in node 2), whereas softmax is an element-wise exponential
function normalized along a certain mode. Tensor diagram has a convention to denote the contraction,
but not the softmax. We will first introduce contraction in tensor diagram notation and then propose
our design of softmax in tensor diagram language.
Contractions denoted as edges connecting the node legs. In tensor diagram, multiplying two
matrices (or higher-order tensors with more than 2 legs) corresponds to “gluing” their corresponding
legs (along a certain mode) as shown in Appendix A.2, and it is called tensor contraction. Since
the representation of arrays in the tensor diagram is orientation invariant, contractions are also
orientation invariant. This is especially useful in representing the self-attention unit. In some cases,
the data object are represented as row vectors while in some other scenarios they are denoted as
column vectors. If we use mathematical formula, we need to take care of the ordering of the matrix
multiplication because A>B, AB> , BA> , B>A and more are all different. In contrast, tensor
diagram provides a universal graphical representation as long as the corresponding legs are connected
correctly.
Nonlinear activation denoted as dotted box with a labelled filled ball attached. We propose to
denote the α-scaled softmax function softmax(αA) on A as a dotted box with a labeled filled ball
(to distinguish itself from tensor objects, i.e., nodes which are blank circles) attached to one leg. The
label right above the filled ball denote the scaling parameter α, and the leg that the ball is attached to
12
Published as a conference paper at ICLR 2022
indicate the mode where normalization is implemented along in the softmax operation. As shown in
Appendix A.2,let A denote the resultant of some operations (for example Q[h]K>]), the √= -scaled
softmax on A which normalize along leg N is depicted. Softmax does not change the shape of the
input. That is why we introduce a dotted box, which does not change the shape either, to denote it.
Batch Multiplication Given A and B, the batch multiplication is an element-wise product along
the connected legs. The resultant tensor C is obtained by merging connected nodes, maintaining the
dangling legs M, N and R. Since in MHSA, the attention calculation among key, query and value
matrices are done separately within each head, batch multiplication is a proper way to denote these
“head-wise” operations.
A.3 Advantages of Tensor Diagram
Tensor diagram representations enjoy a few advantages compared with using flow charts together
with math formulas.
(1)	The notation is orientation invariant. The input embedding matrix X can be represented as
either RN ×F or RF ×N with rows or columns being the embedding of each token in the sequence
respectively. Although we take the former convention in our mathematical notations in Equation B.1,
there are papers that use the latter, creating discrepancies between notations. However, in tensor
diagram, since the node legs are orientation invariant, we obtain a universal graphical representation
of multi-head self-attention, irrespective of how the input embedding matrix is represented, as shown
in Figure 9c.
(2)	The representation is both precise in math and intuitive, making analysis easier. The tensor
diagram itself is mathematically rigorous.
With the labels of the legs, the model complexity (number of parameters) is explicitly displayed,
requiring no supplementary information such as the size of the parameters WQ, WK and WV as in
mathematical formulas. In addition,the resultant from an operation or a sequence of operations (i.e.,
a connected sub-graph of the entire tensor diagram graph) can be treated as “merging” any connected
nodes in the sub-graph. This is particularly convenient to obtain the size of the resultant: any the
dangling leg becomes a leg of the resultant. 3 4 * * *
(3) Multi-linear operations are concisely represented and interpreted. Using tensor diagram, we
can concisely represent the multi-head self-attention rigorously in one diagram as shown in Figure 9c.
More important, the multi-linear interaction between the latent features Q[h] , K[h] and V[h] , which
essentially attribute to the multi-linear interaction between the weight tensors WQ, WK and WB,
is clearly illustrated in the tensor diagram. This illustration, which is not achieved by any existing
representations, is crucial for our principled understanding of the multi-head self-attention.
(4) The information flow is clear. For example, in Figure 9a there is no direct information pass from
W[Kh] , W[Qh] to W[Vh] while in Figure 9c, such exchange is done through edge Ds and H s. It is such
observation (there are multiple paths that connect the weight parameter matrices) that leads to our
idea of comparing expressive power between different structures.
13
Published as a conference paper at ICLR 2022
B S upplementary Material for Multi-Head Self-Attention
In the section, we prove the equivalence between the tensor diagram and the equations for multi-head
self-attention (MHSA). To establish such an equivalence, we will need to show that both tensor
diagram and the matrix equations lead to the same result in element-wise notation.
(a) Single-Head Self-Attention
(b) Single-Head to Multi-Head
(c) Multi-head self-attention
Figure 9: Tensor diagrams from single-head self-attention to multi-head self-attention. Figure 9a is the
tensor diagram representation of a single-head self-attention. Figure 9c is the tensor-diagram representation of a
multi-head self-attention.
For convenience of reference, we recap both representations in Equation (B.1) and Figure 9.
Q[h] = XQW[Qh]; K[h] = XKW[Kh]; V[h] = XV W[Vh].	(B.1a)
head[h] = SoftmaX (Q[h]K>]∕√D) V⅛	(B.1b)
M = [head[i], head.],…，head[H ]] W O,	(B.1c)
Note that the query weight tensor WQ comes from a concatenation of H query weight matrices
{W[Qh]}hH=1 such that WhQ,:,: = W[Qh] . Similarly, WK, WV are concatenated from {W[Kh]}hH=1,
{WV [h]}hH=1 such that W hK,:,: = W[Kh] , WhV,:,: = W[Vh] . Furthermore, we denote Q, K, V as the
reshaped tensors from {Q[h]}hH=1, {K[h]}hH=1, {V[h] }hH=1. Finally, WO is reshaped from WO.
To differentiate between equations and tensor diagrams in the proof, we add an over-line to every
object in the tensor diagram throughout this section.
B.1 Single-Head Self-Attention (SHSA)
We will prove that the tensor diagram in Figure 9a is equivalent to Equations (B.1a) and (B.1b).
Proof. For simplicity, we omit the subscript [h] in this subsection.
We first show that a contraction of the nodes within the softmax box is equivalent to QK> . Let
A = QK> = XQW Q(W KXK)>, whereXQ ∈ RN×F, XK ∈ RM×F, WQ, WK ∈ RF×D.
We use subscripts in the element-wise notation to differentiate edges when multiple edges with same
dimension appear in the same equation.
F
Qnd = X XnQf1WfQ1d,	(B.2a)
f1=1
F
Kmd = X XmKf2 WfK2d,	(B.2b)
f2=1
D	DF F
Anm = X QndKdm = X X X XnQf1 WfQ1dWfK2dXmKf2 .	(B.2c)
d=1	d=1 f1=1 f2=1
Denote be the result of the subset of nodes in the softmax box as A.
DFF
Anm = XXX X QfIW QidW KdX ff2 .	(B.3)
d=1 f1=1 f2=1
14
Published as a conference paper at ICLR 2022
Comparing Equations (B.2c) and (B.3), We have A = A. Let B = Softmax(A) and B =
Softmax(A), we further have B = Softmax(A) = Softmax(A) = B.
Let M be the result of the tensor diagram and M be the matrix representation of the single-head
attention. We have
NF
Mnd = X XBnmXmVfWfVd.	(B.4)
m=1 f=1
NF
Mnd = XX BnmX∖ W f	(B.5)
m=1f =1
Comparing Equations (B.4) and (B.5), we have C is equivalent to C, which completes the proof. □
B.2 Multi-head Self-Attention (MHSA)
We now prove that tensor diagram in Figure 9c is equivalent to Equation (1c).
Proof. Let H be the number ofheads. Let A ∈ Rh×n×m be the result of a contraction of the nodes
within the softmax box. From the last section, we know that Ah,：,： = A[h] = Q[h]K>]. Applying
the softmax function B = Softmax(A), we have Bh,：,： = Softmax(Q[h]K>]).
Let T be result of the tensor diagram and T be that of Equation (B.1c).
Tnf4 =	Softmax(Q[h],nd1 Q[h],md1 )V[h],md2 W[h],d2 f4.	(B.6)
Tn1f4 =	X Softmax(QhndI KhmdI)V hmd2 W 0md2 .	(B.7)
Comparing T and T, we complete the proof.
□
C Closer Investigation of MHSA and its Comparison to THSA
Rethinking MHSA. There are two types contractions in MHSA as shown in Figure 4a: Head-
Contraction, a global contraction among all weights WQ , WK, WV and WO along mode H, and
Latent-Contraction, a local contraction between WQ and WK as well as between WV and WO
both along mode D. If we remove the Head-Contraction and maintain the Latent-Contraction only,
multi-head is reduced to single-head with an additional contraction with the weight matrix WO. To
understand the role of the two types of contractions, we propose ablation studies by removing one
of them and study the changes in the structure and the expressive power. It turns out that in both
situation, MHSA falls into a special case of THSA.
Repeated Proposition 1 with two special cases. Given R = DH and other hyperparameters being
the same, THSA module includes MHSA as a special case. Specifically,
(1)	If C = 1D1>D, i.e., an D × D all-ones matrix, the THSA module reduces to an MHSA with a
single head and latent dimension D (i.e., a single-head self-attention with latent dimension D).
(2)	If C = IH, i.e., an H × H identity matrix, the THSA reduces to a MHSA with H heads and latent
dimension 1 (i.e., a heads-only self-attention with H heads).
(3)IfC=IH0 (1d ID), i.e., a Kronecker product of an all-ones matrix 1d lD ∈ RD×D and an
identity matrix IH ∈ RH ×H, the THSA reduces to an MHSA with H heads and latent dimension D.
The proof of the above propositions is given in Appendix D.
Definition 2 (Stable Rank). Let A ∈ RN×M, the stable rank of A of is defined as Pi σi2 maxi σi
where σi is the singular value of A.
15
Published as a conference paper at ICLR 2022
Figure 10: (Left) THSA includes a single-head self-attention. In this case, the stable rank of the core tensor
C is 1. It is interesting to observe that this is a single-head self-attention and this structure can be obtained
by removing the head contraction from Figure 4a. (Right) THSA includes a MHSA with H heads and latent
dimension = 1. In this case, the stable rank equals to the rank of the identical matrix, which is H, corresponding
to the number of heads of MHSA. Also, the structure on the right can be obtained by removing the latent
contraction edges D from Figure 4a.
Figure 11: This figure shows how to initialize THSA with MHSA. MHSA is a special case of THSA when the
core tensor C takes the Kronecker product of a D × D all-one matrix and a H × H identity matrix. Note that
in this case, the number of heads H in MHSA is equivalent to the stable rank of C .
The notion of heads in THSA. The concept of the number of heads in THSA generalizes to the
stable rank of the core matrix, allowing a data-driven implicit training. The number of heads H in
MHSA corresponds to the stable rank of the core matrix C, defined as Pi σi2 maxi σi where σi is
the singular value. It is a useful surrogate for the rank because it is largely unaffected by tiny singular
values.
As shown in Appendix C when C is constructed by the outer product of two vectors, the stable
rank of C is 1, while in this case, the whole structure is equivalent to a single-head self-attention.
When C is an identical matrix as shown in appendix C, the stable rank of C equals to the rank of the
identical matrix, which is H, corresponding to the number of heads of MHSA. Also, the structure on
the right can be obtained by removing the latent contraction edges D from Figure 4a. When C is
the Kronecker product of the all-one matrix and the identical matrix, which corresponds to MHSA,
the stable rank of C is H , and this number does not change in the training because that C is a fixed
matrix in MHSA.
D	Supplementary Material for Tuformer and Proofs
We first formally describe how to compare the expressive power between two models.
Definition 3 (Expressive Power). Suppose we have two function classes F, G with the same source
domain X and target domain Y, i.e., each function f ∈ F (or g ∈ G) is a mapping from X to
Y. We say G is more expressive than F if G ⊇ F : for any f ∈ F, there exists g ∈ G such that
g(x) = f (x), ∀x ∈ X. Furthermore, we say G is strictly more expressive than F if G ⊃ F: besides
G ⊇ F, there exists g ∈ G such that given any f there exists x ∈ X and g(x) 6= f (x).
16
Published as a conference paper at ICLR 2022
According to the definition, expressive power is a partial order set. Therefore, expressive power of
different models is usually not comparable as two sets might have overlap and non-overlap regions.
However, when one is a superset or subset of the other, we can unambiguously compare their
expressive power. In the following theorem, we formally show that a Tunable-head self-attention
(THSA) module has a higher expressive than multi-head self-attention (MHSA) if the rank R in
THSA is equal to the product of the number of heads H and the latent dimension D in MHSA.
Theorem 4 (THSA is more expressive than MHSA). Given R = DH and other hyper-parameters
being the same, a Tunable-head self-attention (THSA) module with rank R is more expressive than a
multi-head self-attention (MHSA) module with H heads and latent dimension D.
Theorem 4, proved in Appendix D.2, shows that any function realizable by MHSA can be realized by
THSA.
In this section, we will prove Proposition 1 and Theorem 4 in Section 3. In both theorems, we assume
the rank in Tunable-head self-attention (THSA) equals to the product of the number of heads H and
the latent dimension D in multi-head self-attention (MHSA), i.e., R = DH.
For convenience of reference, we recap the mathematical expressions and tensor diagrams for both
multi-head self-attention (MHSA) and Tunable-head self-attention (THSA) in the following.
Multi-head Self-Attention (MHSA). An MHSA module has its learnable parameters as H sets
of (query, key, value) weight matrices {W[Qh],W[Kh],W[Vh]}hH=1 (with W[Qh], W[Kh], W[Vh] ∈ RF×D
for each h) and an output weight matrix WO ∈ RHD×F. The module maps three input matrices
XQ ∈ RN×F, XK,XV ∈ RM×F into an output matrix M ∈ RN×F.
Q[h] =	XQW[Qh];	K[h]	=	XKW[Kh];	V[h]	=	XV	W[Vh],	(D.1a)
head[h] = softmax Q[h]K[>h] V[h] ,	(D.1b)
M = [head[i], head[2],…，head[H ]] W O.	(D.1c)
In Equation (D.1a), Q[h] ∈ RN×D, K[h] , V[h] ∈ RM×D are query, key, value matrices respectively.
In Equation (D.1b), We omit the scaling factor 1∕√D for the Softmax function (the scalar can be
merged into the learnable parameters), and head[h] ∈ RN ×D is the resulted matrix for head h.
Tunable-Head Self-Attention. A THSA module is parameterized by five matrices: a query Weight
matrix W Q ∈ RF ×R, a key weight matrix W K ∈ RF ×R, a value weight matrix W V ∈ RF ×R,
an output weight matrix W ∈ RR×F, and an additional core matrix C ∈ Rr×r. The module
has the same input and output domains as in MHSA, i.e., it takes three matrices XQ ∈ RN×F,
XK, XV ∈ RM×F as inputs and returns a matrix T ∈ RN×F as output.
Q = X QW Q; K = X K W K; V = X V W V,	(D.2a)
headr = Softmax (^X
__ O-----T__O ι^τ-----T z----T	z---T -∣	_ 一 .
T = head W = [headι, head2,…，headκ] .	(D.2c)
To distinguish THSA from MHSA, we use over-scored symbols whenever needed. In Equation (D.2a),
Q ∈ RN×r, K, V ∈ RM×r are query, key, value matrices respectively. In Equation (D.2b), we
again omit the scaling factor 1∕√R in the Softmax function. Equation (D.2b) leads to a head matrix
head ∈ RF×N, where headr ∈ RF denotes the rth column of the matrix head.
17
Published as a conference paper at ICLR 2022
D.1 Proof of Proposition 1
Proof of Proposition 1. We constructively prove that THSA reduces to three cases of MHSA if the
query/key/value weight matrices in THSA concatenate the corresponding matrices in MHSA:
W Q =	=M WQ], ∙∙	•, W[H]],	(D.3a)
W K =	=hW[K, WK, ∙	,•, WH ]i,	(D.3b)
W V =	=hW[V, WV, ∙∙	• , W[VH]i .	(D.3c)
In addition, We set W O = W O. We have W Q, W K, W V ∈ RF ×R, and W O ∈ RR×F.
(1) For C = 1R1R>, i.e. an R × R all-ones matrix, we aim to prove that the THSA module reduces
to a single-head self-attention with H = 1 and D	= R. Since there is only one head in the module,
we have W Q = WQ, WK = WK, and W V =	W[V1] . As an immediate result,
Q = X QW Q =	XQ W[Q1] = Q[1] ,	(D.4a)
K = X K W K=	XKW[K1] = K[1] ,	(D.4b)
V = X V W V =	XVW[V1] = V[1] .	(D.4c)
Since C is an all-ones matrix, i.e., Crs = 1, ∀r, s, We reWrite Equation (D.2b) as:
headr
softmax
softmax
(D.5)
Notice that the equation holds for each column r, We further Write all R equations jointly as:
head = SoftmaX (Q K>) V = SoftmaX (Q[1]K>]) V[1] = head[1].
Finally, we express the output matrix as:
T = head W O = head[i] W O = M,
(D.6)
(D.7)
which concludes the reduction for the special case (1).
(2)	For C = IR, an R × R identity matrix, we aim to prove that the THSA module reduces to a
heads-only self-attention with H = R and D = 1. Since the latent dimension D = 1, i.e., each W[Qh]
(or WK, W-V, )is a vector, we have W Q = WQ, WK = WK, and W V = WV. Therefore,
[h]	[h]	r	[r]	r	[r]	r	[r]
Qr = X QW Q = X QWQ = Q[r],
Kr = XK WK = X K WK = Κ[r],
Vr = X V WV = X V WV = V[r].
(D.8a)
(D.8b)
(D.8c)
Since C is an identity matrix, i.e., Crr = 1 and Crs = 0, ∀r 6= s, We reWrite Equation (D.2b) as:
headr = SoftmaX (QrK>) Vr = softmax(Q[r]K>J V[r] = head^.
Finally, we express the output matrix as:
T = head W O = [head[i], head [2],…；head[R] ] W O = M,
(D.9)
(D.10)
Which concludes the reduction for the special case (2).
(3)	For C = IH 0 (1dlD), i.e., a Kronecker product between an identity matrix IH ∈ RH×H and
an all-ones matrix 1D1>D ∈ RD×D, We aim to prove that the THSA module reduces to a multi-head
18
Published as a conference paper at ICLR 2022
self-attention (MHSA) module with H heads and latent dimension D. In this general case, we have
Q =	XQWQ = XQ	[W[Q],	W[Q,…，W[¾]	=	[Q[1], Q[2],…，Q[H]↑	(D.11)
K =	XKWK = XK	hW[K,	WK，…，W[H]i	=	[K[1], K[2],…，K[H]]	(D.12)
V =	X V W V = X VhWV，W[V,…，W[H]i	=	[V[1], V[2],…，V[H]]	(D.13)
Notice that C is a block diagonal matrix blkdiag(10 1~D,…，1d 1~D), where each blockis an all-ones
matrix. Therefore we can rewrite Equation (D.2b) for r = (h - 1)D + d as:
____	R R
head(h-i)D+d = SoftmaX I E l(h-i)D<s≤hD
s=1
softmaX X Q[h],dK[>h],d V[h],d
softmaX
Since Equation (D.16) holds for each column d, we write all D equations jointly as:
[head(h-i)D+d,…，headhD] = SoftmaX(Q[h]K>，V[h] = head®].
Again, since Equation (D.17) holds for each block h, we combine all H equations as:
head = [head[i], head[2],…，head[H]].
Finally, we express the output matrix as:
T = head W O = [head[i], head [2],…，head[H]] W O = M,
which concludes the reduction for the case (3).
(D.14)
(D.15)
(D.16)
(D.17)
(D.18)
(D.19)
□
D.2 Proof of Theorem 4
Proof of Theorem 4. We have shown that THSA reduces to MHSA when C takes specific forms.
This shows that THSA is more expressive than MHSA — any mapping by an MHSA module can
also be realized by a THSA module.
To prove that THSA is strictly more expressive than MHSA, we need find a mapping by a THSA
module that can not be realized by an MHSA module. It suffices to show that the matrix M is
rank-deficient, while the matrix T can be full-rank. We assume MH < min(N, F).
For convenience, we divide the matrix WO for MHSA into H blocks.
W O = [W[O>, W[O>,…，W[H]i >，	(D.20)
where W[Oh] ∈ RD×F is the matrix for the hth head. We now rewrite the output matrix M as:
H
M = X SoftmaX Q[h]K[>h]	V[h] W[Oh] .
h=1S--------{z---------'、---{-----}
RN ×M	RM ×F
(D.21)
Since the matrix inside summation is a product of two matrices of size RN×M and RM×F, its rank is
at most M. Use the property rank(A + B) ≤ rank(A) + rank(B), we have rank(M) ≤ MH <
min(N, F). Similarly, We divide the matrix W O for THSA into R rows and rewrite the output
matrix T as:
R
T = SoftmaX
r=1
^^^{^^^≡
RN ×M
/
， RM ×F
(D.22)
Using the same argument, we have rank(T) ≤ MR. Since H < R (in fact, R = DH), we can
always find an example such that rank(T) > rank(M). This completes the proof.	□
19
Published as a conference paper at ICLR 2022
E S upplementary Material for Experiments
All the experiments are run on computing nodes with 4 NVIDIA GeForce GPUs.
(1)	Language Modeling on Penn Treebank. Language modeling is the task of computing the
probability of a sentence or a sequence of words. The model performance is measured by per-word
Perplexity (PPL) which is the lower the better.
Penn Treebank (Marcus et al., 1993) is under the LDC User Agreement for Non-Members. We adopt
the default settings as in the vanilla Transformer models(Vaswani et al., 2017). Specifically, we have
N (length of the sequence) to be 2048, F (length of the initial embedding) to be 512, 8 number
of heads, 6 layers encoder-decoder structure, D (latent feature dimension) to be 64. We use Adam
optimizer and have 4000 warm-up steps. We also use label smoothing(0.1). The model is trained for
50 epochs.
(2)	Neural Machine Translation on WMT16. The goal of NMT is to generate a corresponding
sequence in one language given a sequence in another. The performance is measured by BLEU scores.
(the higher, the better).
WMT16 English-German dataset (Sennrich et al., 2016) is under the MIT license. In this experiment,
we use a Transformer model with 8 layers and we have F = 512 and R = 1024. The learning rate is
1e-5 and 4000 warmups. We also set label-smooting to be 0.1. To speed up the experiment, we use
the mix-precision trick with an optimization level being O1.
(3)	Image Generation on MNIST and CIFAR-10. The image generation task is to predict an image
pixel by pixel. The performance is evaluated by bits per dimension (BPD). The image generation task
is chosen to demonstrate, firstly, that Tuformers work beyond the language domain, and secondly,
Tuformers can improve other efficient designs.
CIFAR10 (Krizhevsky, 2009) dataset is under the MIT license (MIT), MNIST(LeCun et al., 2010)
is under the Creative Commons Attribution-Share Alike 3.0 license. The reason why we choose
to evaluate Tuformer in image generation tasks on these two datasets is that we want to show the
compatibility of our model to other efficient designs. Katharopoulos et al. (2020) introduces a linear
transformer that has a state-of-the-art complexity. As a result, we evaluate our design on the same
experiments as they do in their paper. We adopt basically the same settings except for GPU resources.
Concretely, we use a 8 layer transformer model with 8 heads. The embedding size F is 256 an D is 32.
N is set to be 1024. We also use “RAdam” (Ma & Yarats, 2021) optimizer for a state-of-the-art result.
The model is trained for 250 epochs for MNIST dataset and 50 epochs for CIFAR10 dataset. The
batch size is 10 in MNIST training and 4 in CIFAR training. We additionally use he mix-precision
trick with an optimization level set to be O1 to speed up the training.
(4)	Automatic Speech Recognition on LibriSpeech Dataset ASR consists of transcribing audio
speech segments into text. The performance is measured by Word Error Rate (WER). (the lower, the
better). We adopt the default settings as in Fairseq (Ott et al., 2019), except that we replace MHSA
with THSA.
(5)	Natural Language Inference on MNLI and QNLI. NLI is a task of determining whether the
given “hypothesis” and “premise” logically follow (entailment) or unfollow (contradiction) or are
undetermined (neutral) to each other. The results are evaluated by accuracy. Using MNLI dataset, the
model tries to predict whether sentence A entails or contradicts B while in QNLI dataset, the model
is trained to answer whether sentence B contains answers to the question in sentence A. We use a
batch size of 32 for 10 epochs, with a learning rate 1e-5.
Memory Overhead in THSA. As mentioned in Section 4.3, naive Tuformers come with an extra
memory overhead when calculating values within the softmax. In Tuformers, if done naively, the
memory overhead is MNR where N, M denote the sequence length and R = DH is the rank of the
core tensor, incurring a D times larger memory overhead than vanilla Transformers in calculating
softmax. We argue that combining Tuformers with kernel-based efficient variants such as Linear
Transformers (Katharopoulos et al., 2020) and polynomial kernels (Tsai et al., 2019) kernels could not
only cancel the memory overhead but also achieve the same linear memory complexity by removing
the nonlinear constrain in the softmax.
20
Published as a conference paper at ICLR 2022
Furthermore, the extra memory overhead can be eliminated from the engineering side using the
check-pointing technique: intermediate values are not saved in the memory but recalculated When
needed. We compare the runtime in both training and inference phase With check-pointing. As shoWn
in fig. 12, under the same number of parameters, there is only slight increase in the training and
basically the same runtime in the inference phase. With a direct transformation from Transformer
to Tuformer (for example, When initialized With pre-trained models), the runtime is still under a
reasonable range.
Language Modeling on Penn Treebank
noitareti rep dnoce
Language Modeling on Penn Treebank
.4 .2 .0 .8 .6
11100
noitareti rep dnoces
-Θ-MHSA
-Θ-THSA
-θ- THSA With checkpointing
1.5
0.2
10%	20%	50%	100%	150% 200%
10%	20%	50%	100%	150%	200%
model scale
model scale
Figure 12: (Left) Training and (Right) inference run-time comparison betWeen MHSA, THSA With the same
number of parameters and THSA With the same number of parameters With check-pointing. The batch size is
256.
21