Published as a conference paper at ICLR 2022
Doubly Adaptive Scaled Algorithm for
Machine Learning Using 2ndORDER Information
Majid Jahani1, Sergey Rusakov1, Zheng Shi1, Peter Richtarik2, Michael W. Mahoney3,
Martin Takac4,1
1Lehigh University 2KAUST 3University of California, Berkeley, USA
4 Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)
majidjahani89@gmail.com, ser318@lehigh.edu,
shi.zheng.tfls@gmail.com, peter.richtarik@kaust.edu.sa,
mmahoney@stat.berkeley.edu, takac.MT@gmail.com
Ab stract
We present a novel adaptive optimization algorithm for large-scale machine learning
problems. Equipped with a low-cost estimate of local curvature and Lipschitz
smoothness, our method dynamically adapts the search direction and step-size.
The search direction contains gradient information preconditioned by a well-scaled
diagonal preconditioning matrix that captures the local curvature information.
Our methodology does not require the tedious task of learning rate tuning, as the
learning rate is updated automatically without adding an extra hyperparameter. We
provide convergence guarantees on a comprehensive collection of optimization
problems, including convex, strongly convex, and nonconvex problems, in both
deterministic and stochastic regimes. We also conduct an extensive empirical
evaluation on standard machine learning problems, justifying our algorithm’s
versatility and demonstrating its strong performance compared to other start-of-the-
art first-order and second-order methods.
1 Introduction
This paper presents an algorithm for solving empirical risk minimization problems of the form:
minw∈Rd F (W) := n Pn=I f (w; xi, yi) = 1 pn=J(w),	(1)
where w is the model parameter/weight vector, {(xi, yi)}in=1 are the training samples, and
fi : Rd → R is the loss function. Usually, the number of training samples, n, and dimension, d, are
large, and the loss function F is potentially nonconvex, making (1) difficult to solve.
In the past decades, significant effort has been devoted to developing optimization algorithms for
machine learning. Due to easy implementation and low per-iteration cost, (stochastic) first-order
methods (Robbins & Monro, 1951; Duchi et al., 2011; Schmidt et al., 2017; Johnson & Zhang,
2013; Nguyen et al., 2017; 2019; Kingma & Ba, 2014; Jahani et al., 2021a; Recht et al., 2011) have
become prevalent approaches for many machine learning applications. However, these methods have
several drawbacks: (i) they are highly sensitive to the choices of hyperparameters, especially learning
rate; (ii) they suffer from ill-conditioning that often arises in large-scale machine learning; and (iii)
they offer limited opportunities in distributed computing environments since these methods usually
spend more time on “communication” instead of the true “computation.” The main reasons for the
aforementioned issues come from the fact that first-order methods only use the gradient information
for their updates.
On the other hand, going beyond first-order methods, Newton-type and quasi-Newton methods
(Nocedal & Wright, 2006; Dennis & More, 1977; Fletcher, 1987)are considered to be a strong
family of optimizers due to their judicious use of the curvature information in order to scale
the gradient. By exploiting the curvature information of the objective function, these methods
mitigate many of the issues inherent in first-order methods. In the deterministic regime, it is
known that these methods are relatively insensitive to the choices of the hyperparameters, and
they handle ill-conditioned problems with a fast convergence rate. Clearly, this does not come
for free, and these methods can have memory requirements up to O(d2 ) with computational
1
Published as a conference paper at ICLR 2022
complexity up to O(d3) (e.g., with a naive use of the Newton method). There are, of course,
efficient ways to solve the Newton system with significantly lower costs (e.g., see Nocedal &
Wright (2006)). Moreover, quasi-Newton methods require lower memory and computational
complexities than Newton-type methods. Recently, there has been shifted attention towards
stochastic second-order (Roosta-Khorasani & Mahoney, 2018; Byrd et al., 2011; Martens, 2010;
Jahani et al., 2020a; Xu et al., 2017; Roosta et al., 2018; Yao et al., 2018) and quasi-Newton
methods (Curtis, 2016; Berahas et al., 2016; Mokhtari & Ribeiro, 2015; Jahani et al., 2021b; Bera-
has et al., 2019; Jahani et al., 2020b) in order to approximately capture the local curvature information.
These methods have shown good results for sev-
eral machine learning tasks (Xu et al., 2020;
Berahas et al., 2020; Yao et al., 2019). In some
cases, however, due to the noise in the Hessian
approximation, their performance is still on par
with the first-order variants. One avenue for re-
ducing the computational and memory require-
ments for capturing curvature information is to
consider just the diagonal of the Hessian. Since
the Hessian diagonal can be represented as a
vector, it is affordable to store its moving aver-
age, which is useful for reducing the impact of
noise in the stochastic regime. To exemplify this,
AdaHessian Algorithm (Yao et al., 2020) uses
Hutchinson’s method (Bekas et al., 2007) to ap-
proximate the Hessian diagonal, and it uses a
second moment of the Hessian diagonal approx-
imation for preconditioning the gradient. Ada-
Hessian achieves impressive results on a wide
range of state-of-the-art tasks. However, its pre-
conditioning matrix approximates the Hessian
diagonal only very approximately, suggesting
that improvements are possible if one can better
approximate the Hessian diagonal.
Figure 1: Comparison of the diagonal approxima-
tion by AdaHessian and OASIS over a random
symmetric matrix A (100 × 100). left: Relative
error (in Euclidean norm) between the true diag-
onal of matrix A and the diagonal approximation
by AdaHessian, Hutchinson’s method, and OASIS
(the x-axis shows the number of random vectors
sampled from the Rademacher distribution, see
Section 2. Moreover, this plot can be considered as
the representation of the error of the Hessian diag-
onal approximation’s evolution over the iterations
of minimizing wT Aw, since A is fixed and sym-
metric.); right: Diagonal approximation scale for
AdaHessian and OASIS (y-axis), in comparison
to the true diagonal of matrix A (x-axis).
In this paper, we propose the dOubly Adaptive Scaled algorIthm for machine learning using Second-
order information (OASIS). OASIS approximates the Hessian diagonal in an efficient way, providing
an estimate whose scale much more closely approximates the scale of the true Hessian diagonal (see
Figure 1). Due to this improved scaling, the search direction in OASIS contains gradient information,
in which the components are well-scaled by the novel preconditioning matrix. Therefore, every
gradient component in each dimension is adaptively scaled based on the approximated curvature for
that dimension. For this reason, there is no need to tune the learning rate, as it would be updated auto-
matically based on a local approximation of the Lipschitz smoothness parameter (see Figure 2). The
well-scaled preconditioning matrix coupled with the adaptive learning rate results in a fully adaptive
step for updating the parameters.Here, we provide a brief summary of our main contributions:
•	Novel Optimization Algorithm. We propose OASIS as a fully adaptive method that preconditions
the gradient information by a well-scaled Hessian diagonal approximation. The gradient component
in each dimension is adaptively scaled by the corresponding curvature approximation.
•	Adaptive Learning Rate. Our methodology does not require us to tune the learning rate, as it
is updated automatically via an adaptive rule. The rule approximates the Lipschitz smoothness
parameter, and it updates the learning rate accordingly.
•	Comprehensive Theoretical Analysis. We derive convergence guarantees for OASIS with respect
to different settings of learning rates, namely the case with adaptive learning rate for convex and
strongly convex cases. We also provide the convergence guarantees with respect to fixed learning
rate and line search for both strongly convex and nonconvex settings.
•	Competitive Numerical Results. We investigate the empirical performance of OASIS on a variety
of standard machine learning tasks, including logistic regression, nonlinear least squares prob-
lems, and image classification. Our proposed method consistently shows competitive or superior
performance in comparison to many first- and second-order state-of-the-art methods.
Notation. By considering the positive definite matrix D, we define the weighted Euclidean norm of
vector X ∈ Rd with IIxkD = XTDx. Its corresponding dual norm is shown as ∣∣ ∙ ∣∣D. The operator
2
Published as a conference paper at ICLR 2022
AdaHessian (lr=0.03125)	AdaHessian (lr=0.125)	AdaHessian (lr=0.5)	AdaHessian (lr=2)	— OASIS
AdaHessian (lr=0.0625)	AdaHessian (lr=0.25)	AdaHessian (lr=1)	AdaHessian (lr=4)
Figure 2: Adaptive Learning Rate (Logistic Regression with strong-convexity parameter 1
over rcv1 dataset). left and middle: Comparison of optimality gap for AdaHessian Algorithm with
multiple learning-rate choices vs. OASIS Algorithm with adaptive learning rate (dashed-blue line);
right: Comparison of the best optimality gap and test accuracy for AdaHessian Algorithm w.r.t. each
learning rate shown on x-axis after 40 iterations vs. the optimality gap and test accuracy for our
OASIS Algorithm with adaptive learning rate after 40 iteration (dashed-blue line).
is used as a component-wise product between two vectors. Given a vector v, we represent the
corresponding diagonal matrix of v with diag(v).
2	Related Work
In this paper, we analyze algorithms with the generic iterate updates:
Wk+1 = Wk — ηk D k Tmk,	(2)
where DDk is the preconditioning matrix, mk is either gk (the true gradient or the gradient approxi-
mation) or the first moment of the gradient with momentum parameter β1 or the bias corrected first
moment of the gradient, and ηk is the learning rate. The simple interpretation is that, in order to
update the iterates, the vector mk would be rotated and scaled by the inverse of preconditioning
matrix DDk, and the transformed information would be considered as the search direction. Due to
limited space, here we consider only some of the related studies with a diagonal preconditioner.
For more general preconditioning, see Nocedal & Wright (2006). Clearly, one of the benefits of a
well-defined diagonal preconditioner is the easy calculation of its inverse.
There are many optimization algorithms that follow the update in (2). A well-known method is
stochastic gradient descent (SGD).The idea behind SGD is simple yet effective: the preconditioning
matrix is set to be DDk = Id, for all k ≥ 0. There are variants of SGD with and without momentum.
The advantage of using momentum is to smooth the gradient (approximation) over the past iterations,
and it can be useful in the noisy settings. In order to converge to the stationary point(s), the learning
rate in SGD needs to decay. Therefore, there are many important hyperparameters that need to
be tuned, e.g., learning rate, learning-rate decay, batch size, and momentum. Among all of them,
tuning the learning rate is particularly important and cumbersome since the learning rate in SGD is
considered to be the same for all dimensions. To address this issue, one idea is to use an adaptive
diagonal preconditioning matrix, where its elements are based on the local information of the iterates.
One of the initial methods with a non-identity preconditioning matrix is Adagrad (Duchi et al., 2011;
McMahan & Streeter, 2010). In Adagrad, the momentum parameter is set to be zero (mk = gk), and
the preconditioning matrix is defined as:	___________
DDk = diag(,Pk=ιgk。gk).	⑶
In the preconditioning matrix DDk in (3), every gradient component is scaled with the accumulated
information of all the past squared gradients. It is advantageous in the sense that every component
is scaled adaptively. However, a significant drawback of Dk in (3) has to do with the progressive
increase of its elements, which leads to rapid decrease of the learning rate. To prevent Adagrad’s
aggressive, monotonically decreasing learning rate, several approaches, including Adadelta (Zeiler,
2012) and RMSProp (Tieleman & Hinton, 2012), have been developed. Specifically, in RMSProp,
the momentum parameter βι is zero (or mk = gk) and the preconditioning matrix is as follows:
DDk = Jβ2DDk-12 + (1 — β2)diag(gk Θ gk),	(4)
where β2 is the momentum parameter used in the preconditioning matrix. As we can see from
the difference between the preconditioning matrices in (3) and (4), in RMSProp an exponentially
decaying average of squared gradients is used, which prevents rapid increase of preconditioning
3
Published as a conference paper at ICLR 2022
components in (3).
Another approach for computing the adaptive scaling for each parameter is Adam (Kingma & Ba,
2014). Besides storing an exponentially decaying average of past squared gradients like Adadelta and
RMSprop, Adam also keeps first moment estimate of gradient, similar to SGD with momentum. In
Adam, the bias-corrected first and second moment estimates, i.e., mk and DDk in (2), are as follows:
mk = 1≡ββkPk=Iek-igi,	DDk = q 1≡ββkPk=1βk-idiag(gi ® gi) ∙	(5)
There have been many other first-order methods with adaptive scaling (Loshchilov & Hutter, 2017;
Chaudhari et al., 2019; Loshchilov & Hutter, 2016; Shazeer & Stern, 2018).
The methods described so far have only used the information of the gradient for preconditioning
mk in (2). The main difference of second-order methods is to employ higher order information for
scaling and rotating the mk in (2). To be precise, besides the gradient information, the (approximated)
curvature information of the objective function is also used. As a textbook example, in Newton’s
method DDk = V1 2F(Wk) and mk = gk with ηk = 1.
Diagonal Approx. Recently, using methods from randomized numerical linear algebra, the AdaHes-
sian method was developed (Yao et al., 2020). AdaHessian approximates the diagonal of the Hessian,
and it uses the second moment of the diagonal Hessian approximation as the preconditioner DDk in
(2). In AdaHessian, Hutchinson’s method1 is used to approximate the Hessian diagonal as follows:
Dk ≈ diag(E[zk	V2F(wk)zk]),	(6)
where zk is a random vector with Rademacher distribution. Needless to say,2 the oracle V2F(wk)zk,
or Hessian-vector product, can be efficiently calculated; in particular, for AdaHessian, it is computed
with two back-propagation rounds without constructing the Hessian explicitly. In a nutshell, the first
momentum for AdaHessian is the same as (5), and its second order momentum is:
DDk=q ⅛∣⅛ pk=1βk-iD2.	⑺
The intuition behind AdaHessian is to have a larger step size for the dimensions with shallow loss
surfaces and smaller step size for the dimensions with sharp loss surfaces. The results provided
by AdaHessian show its strength by using curvature information, in comparison to other adaptive
first-order methods, for a range of state-of-the-art problems in computer vision, natural language
processing, and recommendation systems (Yao et al., 2020). However, even for AdaHessian, the
preconditioning matrix DDk in (7) does not approximate the scale of the actual diagonal of the Hessian
particularly well (see Figure 1). One might hope that a better-scaled preconditioner would enable
better use of curvature information. This is one of the main focuses of this study.
Adaptive Learning Rate. In all of the methods discussed previously, the learning rate ηk in (2)
is still a hyperparameter which needs to be manually tuned, and it is a critical and sensitive hy-
perparameter. It is also necessary to tune the learning rate in methods that use approximation of
curvature information (such as quasi-Newton methods like BFGS/LBFGS, and methods using di-
agonal Hessian approximation like AdaHessian). The studies (Loizou et al., 2020; Vaswani et al.,
2019; Chandra et al., 2019; Baydin et al., 2017; Malitsky & Mishchenko, 2020) have tackled the
issue regarding tuning learning rate, and have developed methodologies with adaptive learning rate,
ηk , for first-order methods. Specifically, the work (Malitsky & Mishchenko, 2020) finds the learning
rate by approximating the Lipschitz smoothness parameter in an affordable way without adding a
tunable hyperparameter which is used for GD-type methods (with identity norm). Extending the latter
approach to the weighted-Euclidean norm is not straightforward. In the next section, we describe how
we can extend the work (Malitsky & Mishchenko, 2020) for the case with weighted Euclidean norm.
This is another main focus of this study. In fact, while we focus on AdaHessian, any method with a
positive-definite preconditioning matrix and bounded eigenvalues can benefit from our approach.
3	OASIS
In this section, we present our proposed methodology. First, we focus on the deterministic regime,
and then we describe the stochastic variant of our method.
1For a general symmetric matrix A, E[z	Az] equals the diagonal of A (Bekas et al., 2007).
2Actually, it needs to be said: many within the machine learning community still maintain the incorrect belief
that extracting second order information “requires inverting a matrix.” It does not.
4
Published as a conference paper at ICLR 2022
3.1	Deterministic OASIS
Similar to the methods described in the previous section, our OASIS Algorithm generates iterates
according to (2).
Motivated by AdaHessian, and by the fact
that the loss surface curvature is different
across different dimensions, we use the cur-
vature information for preconditioning the
gradient. We now describe how the pre-
conditioning matrix DD k can be adaptively
updated at each iteration as well as how to
update the learning rate ηk automatically
for performing the step. To capture the cur-
vature information, we also use Hutchin-
son’s method and update the diagonal ap-
proximation as follows:
Algorithm 1 OASIS
Input:w0, η0, D0, θ0 = +∞
1
2
3
4
5
6
7
wι = wo 一 ηoD0-1VF(w0)
for k = 1, 2, . . . do
Form Dk via (8) and Dk via (9)
Update ηk based on (10)
Set Wk+1 = Wk 一 ηkDk-1VF(wk)
Set θk = η≡-1
end for
Dk = β2Dk-1 + (1 - β2) diag(vk), where Vk := Zk Θ V2F(Wk)zk.
(8)
Before we proceed, we make a few more comments about the Hessian diagonal Dk in (8). As is clear
from (8), a decaying exponential average of Hessian diagonal is used, which can be very useful in
the noisy settings for smoothing out the Hessian noise over iterations. Moreover, it approximates
the scale of the Hessian diagonal with a satisfactory precision, unlike AdaHessian Algorithm (see
Figure 1). More importantly, the modification is simple yet very effective. Similar simple and
efficient modification happens in the evolution of adaptive first-order methods (see Section 2).
Further, motivated by (Paternain et al., 2019; Jahani et al., 2021b), in order to find a well-defined
preconditioning matrix Dk, we truncate the elements of Dk by a positive truncation value α. To be
more precise:
(Dk)i,i = max{∣Dk∣i,i,α}, ∀i ∈ [d].
(9)
The goal of the truncation described above is to have a well-defined preconditioning matrix that
results in a descent search direction; note the parameter α is equivalent to in Adam and AdaHessian).
Next, we discuss the adaptive strategy for updating the learning rate ηk in (2). By extending the
adaptive rule in (Malitsky & Mishchenko, 2020) and by defining θk := -ηk-, ∀k ≥ 1, our learning
ηk-1
rate needs to satisfy the inequalities: i) ηk2 ≤ (1 + θk-1)ηk2-1, and ii) ηk ≤
kwk-wk-1kD^k
2kVF (wfc)-VF (wk-ι)kD .
Dk
(These inequalities come from the theoretical results.) Thus, the learning rate can be adaptively
calculated as follows:
ηk = min{vz1 + Θk-1ηk-ι, ∣∣Wk ― Wk-ikDJ(2∣∣VF(wk) - VF(Wk-I)∣∣DJ}.	(10)
As is clear from (10), it is only required to store the previous iterate with its corresponding gradient
and the previous learning rate (a scalar) in order to update the learning rate. Moreover, the learning
rate in (10) is controlled by the gradient and curvature information. As we will see later in the
theoretical results, ηk ≥ £ where L is the Lipschitz smoothness parameter of the loss function. It is
noteworthy to highlight that due to usage of weighted-Euclidean norms the required analysis for the
cases with adaptive learning rate is non-trivial (see Section 4). Also, by setting β2 = 1, α = 1, and
D0 = Id, our OASIS algorithm covers the algorithm in (Malitsky & Mishchenko, 2020).
3.2	Stochastic OASIS
In every iteration of OASIS, as presented in the previous section, it is required to evaluate the
gradient and Hessian-vector product on the whole training dataset. However, these computations are
prohibitive in the large-scale setting, i.e., when n and d are large. To address this issue, we present a
stochastic variant of OASIS (see Appendix B for details) that only considers a small mini-batch of
training data in each iteration.
The Stochastic OASIS chooses sets Ik, Jk ⊂ [n] randomly and independently, and the new iterate
is computed as: Wk+1 = Wk - ηkDkTVFIk (wk), where VFIk (Wk)=册 Pi∈τk VFi(wk)
and Dk is the truncated variant of Dk = β2Dk-1 + (1 — β2) diag(zk Θ V2FJfc(Wk)zk) with
VFJk (Wk) =册 Pj∈jk VFj(Wk)∙
5
Published as a conference paper at ICLR 2022
3.3	Warmstarting and Complexity of OASIS
Both deterministic and stochastic variants of our methodology share the need to obtain an initial
estimate of D0 . The importance of this is illustrated by the rule (10), which regulates the choice of
ηk, which is highly dependent on Dk. In order to have a better approximation of D0, we propose to
sample some predefined number of Hutchinson’s estimates before the training process.
The main overhead of our methodology is the Hessian-vector product used in Hutchinson’s method
for approximating the Hessian diagonal. With the current advanced hardware and packages, this
computation is not a bottleneck anymore. To be more specific, the Hessian-vector product can be
efficiently calculated by two rounds of back-propagation. We also present how to calculate the
Hessian-vector product efficiently for various well-known machine learning tasks in Appendix B.
4	Theoretical Analysis
In this section, we present our theoretical results for OASIS. We show convergence guarantees for
different settings of learning rates, i.e., (i) adaptive learning rate, (ii) fixed learning rate, and (iii)
with line search. Before the main theorems are presented, we state the following assumptions and
lemmas that are used throughout this section. For brevity, we present the theoretical results using line
search in Appendix A. The proofs and other auxiliary lemmas are in Appendix A.
Assumption 4.1. (Convex). The function F is convex.
Assumption 4.2. (L-smooth). The gradients of F are L-Lipschitz continuous for all w ∈ Rd, i.e.,
∃L > 0 such that ∀w, w0 ∈ Rd,F(W) ≤ F(w0) + OF(W),w — w0i + L ∣∣w — w0k2.
Assumption 4.3. The function F is twice continuously differentiable.
Assumption 4.4. (μ-StrongIy convex). The function F is μ-StrongIy convex, i.e., there exists a
constant μ > 0 such that ∀w, w0 ∈ Rd, F(W) ≥ F(w0) + NF(w0), W — w0i + 2 ∣∣w — w0∣2.
Here is a lemma regarding the bounds for Hutchinson’s approximation and the diagonal differences.
Lemma 4.5. (Bound on change of Dk). Suppose that Assumption 4.2 holds, then i) |(vk)i| ≤ Γ ≤
√dL, where Vk = Zk Θ V2F(Wk)zk;ii) ∃ δ ≤ 2(1 — β2)Γ such that ∀k : ∣Dk+ι — Dk ∣∣∞ ≤ δ.
4.1	Adaptive Learning Rate
Here, we present theoretical convergence results for the case with adaptive learning rate using (10).
Theorem 4.6. Suppose that Assumptions 4.1, 4.2 and 4.3 hold. Let {Wk} be the iterates gen-
erated by Algorithm OASIS,	then we have:	F(Wk)	-	F*	≤	LC	+	2L(1	—	β2)ΓQk,	where
C = 2刖-"员”叫20员 +2η1θ1(F (W0) — F (W*)), and Qk = £?曰(2⅛+α ∣Wi-ι — Wi ∣2 +
Lno+ ∣Wi — W*k2).
Remark 4.7. The following remarks are made regarding Theorem 4.6:
1.	Ifβ2 = 1, α = 1, and D0 = Id, the results in Theorem 4.6 completely match with (Malitsky &
Mishchenko, 2020).
2.	By considering an extra assumption as in (Reddi et al., 2019; Duchi et al., 2011) regarding
bounded iterates, i.e., ∣Wk — W* ∣2 ≤ B ∀k ≥ 0, one can easily show the convergence of OASIS
to the neighborhood of optimal solution(s).
The following lemma provides the bounds for the adaptive learning rate for smooth and strongly-
convex loss functions. The next theorem provides the linear convergence rate for the latter setting.
Lemma 4.8. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, then ηk ∈ [2L, 2μ ].
Theorem 4.9. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, and let W* be the unique solution
for (1). Let {Wk} be the iterates generated by Algorithm 1. Then, for all k ≥ 0 and β2 ≥
maχ{1-4 L2「公2/2+q2), 1—4Lr(2α2μ2+L3r2)} we have:ψk+l ≤(I-2τ⅛ )ψk,Where ψk+1 =
∣∣Wk+ι — W*∣Dfc + 1 ∣∣Wk+ι — Wk IIDk + 2ηk(1 + θk)(F(Wk) — F (w*)).
4.2	Fixed Learning Rate
Here, we provide theoretical results for fixed learning rate for deterministic and stochastic regimes.
Remark 4.10. For any k ≥ 0, we have aI W Dk W ΓI where 0 < α ≤ Γ.
6
Published as a conference paper at ICLR 2022
4.2.1	Deterministic Regime
Strongly Convex. The following theorem provides the linear convergence for the smooth and
strongly-convex loss functions with fixed learning rate.
Theorem 4.11. Suppose that Assumptions 4.2, 4.3, and 4.4 hold, and let F * = F (w*), where w* is
α2
the unique minimizer Let {wk } be the iterates generated by Algorithm 1, where 0 < ηk = η ≤ ——,
LΓ
and wo is a starting point. Then,forall k ≥ 0 we have: F(Wk) 一 F * ≤ (1 一 ημ )k[F (w°) — F *].
Nonconvex. The following theorem provides the convergence to the stationary points for the
nonconvex setting with fixed learning rate.
Assumption 4.12. Thefunction F(.) is bounded below by a scalar F.
Theorem 4.13. Suppose that Assumptions 4.2, 4.3, and 4.12 hold. Let {wk} be the iterates generated
α2
by Algorithm, where 0 < ηk = η ≤ —, and w° is a starting point. Then, for all T > 1 we have:
TPT=IkVF(Wk)k2 ≤ 2r[F(ηT)-F] -→∞ 0.	(11)
4.2.2	Stochastic Regime
Here, we use EIk [.] to denote conditional expectation given wk, and E[.] to denote the full expectation
over the full history. The following standard assumptions as in (Bollapragada et al., 2019; Berahas
et al., 2016) are considered for this section.
Assumption 4.14. There exist a constant γ such that EI [kVFI (w) 一 VF(w)k2] ≤ γ2.
Assumption 4.15. There exist a constant σ2 < ∞ such that EI [kVFI (w*)k2] ≤ σ2.
Assumption 4.16. VFI (w) is an unbiased estimator of the gradient, i.e., EI [VFI (w)] = VF (w).
Strongly Convex. The following theorem presents the convergence to the neighborhood of the
optimal solution for the smooth and strongly-convex case in the stochastic setting.
Theorem 4.17.	Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let {wk} be the iterates
generated by Algorithm 1 with ηk = η ∈ (0, Γαjμ), then, for all k ≥ 0,
E[F(Wk) — F*] ≤ (1 — c)k (F(wo) — F*) + ⅛2,	(12)
where C = 2Γμ 一 2ηαj ∈ (°, 1). Moreover, if ηk = η ∈ (0, 2Γj2) then
E[F(wk) — F*] ≤ (1 — ημ)k (F(wo) — F*) + *∙	(13)
Nonconvex. The next theorem provides the convergence to the stationary point for the nonconvex
loss functions in the stochastic regime.
Theorem 4.18.	Suppose that Assumptions 4.2, 4.3, 4.12, 4.14 and 4.16 hold. Let {wk} be the iterates
2
generated by Algorithm 1, where 0 < ηk = η ≤ j^, and w° is the starting point. Then, for all k ≥ 0,
E h 1 PT-1∣∣ V F7?”j Ml 21< 2r[F(WO)-F] I ηΓγ2 j T→∞ ηΓγ2 j
E [τ2^k=o IIvF (Wk川]≤---------ητ------1	0--------> α ∙
The previous two theorems provide the convergence to the neighborhood of the stationary points. One
can easily use either a variance reduced gradient approximation or a decaying learning rate strategy to
show the convergence to the stationary points (in expectation). OASIS’s analyses are similar to those
of limited-memory quasi-Newton approaches which depends on λmax and λmin (largest and smallest
eigenvalues) of the preconditioning matrix (while in (S)GD λmax= λmin = 1). These methods in
theory are not better than GD-type methods. In practice, however, they have shown their strength.
5	Empirical Results
In this section, we present empirical results for several machine learning problems to show that our
OASIS methodology outperforms state-of-the-art first- and second-order methods in both determin-
istic and stochastic regimes. We considered: (1) deterministic '2-regularized logistic regression
(strongly convex); (2) deterministic nonlinear least squares (nonconvex), and we report results on
2 standard machine learning datasets rcv1 and ijcnn13; and (3) image classification tasks on
3https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
7
Published as a conference paper at ICLR 2022
X3gn 弋 jsR
Ilcnnl
lθ-ɪ
10^3
10^5
10^7
10^9
lθ-ɪɪ
IlCnnl
AdGD
AdaHessian
OASIS
0	400
>sses
MGD
AdeHessien
OASS
100	200	300	400
Number OfEffectlve Passes
100	200	300	400
Number of Effective Passes
AdGD
——AdaHessien
——OASIS
100	200	300	400
Number OfEftectlve Passes
100	200	300	400
Numberof Effective Passes
( ( I
X总己Uy⅛3κ
MGD
MeHessien
OASIS
IlCnnl_______________
AdGD
—AdeHessien
—OASIS
lJjtβι------------
100	200	300	400
Number OfEffectIve Passes
rcvl
Ioo 200
Number OfEffec
rcvl
Figure 3: Comparison of optimality gap and Test Figure 4: Comparison of objective function
Accuracy for different algorithms on Logistic Re- (F (w)) and Test Accuracy for different algo-
gression Problems.	rithms on Non-linear Least Square Problems.
MNIST, CIFAR10, and CIFAR100 datasets on standard network structures.In the interest of space,
we report only a subset of the results in this section. The rest can be found in Appendix C.
To be clear, we compared the empirical performance of OASIS with algorithms with diagonal pre-
conditioners. In the deterministic regime, we compared the performance of OASIS with AdGD
(Malitsky & Mishchenko, 2020) and AdaHessian (Yao et al., 2020). Further, for the stochastic regime,
we provide experiments comparing SGD (Robbins & Monro, 1951), Adam (Kingma & Ba, 2014),
AdamW (Loshchilov & Hutter, 2017), and AdaHessian. For the logistic regression problems, the
regularization parameter was chosen from the set λ ∈ {击,1, 10}. It is worth highlighting that
we ran each method for each of the following experiments from 10 different random initial points.
Moreover, we separately tuned the hyperparameters for each algorithm, if needed. See Appendix C
for details. The proposed OASIS is robust with respect to different choices of hyperparameters, and
it has a narrow spectrum of changes (see Appendix C).
Logistic Regression. We considered '2-regularized logistic regression problems, F(W) =
1 pn=ι log(1 + e-yi%w) + λ ∣∣w∣∣2. Figure 3 shows the performance of the methods in terms
of optimality gap and test accuracy versus number of effective passes (number of gradient and
Hessian-vector evaluations). As is clear, the performance of OASIS (with adaptive learning rate and
without any hyperparameter tuning) is on par or better than that of the other methods.
Non-linear Least Square. We considered non-linear least squares problems (described in Xu et al.
(2020)): F(w) = npn=ι(yi - 1/(1 + e-xTW))2. Figure 4 shows that our OASIS Algorithm always
outperforms the other methods in terms of training loss function and test accuracy. Moreover, the
behaviour of OASIS is robust with respect to the different initial points.
Image Classification. We illustrate the performance of OASIS on standard bench-marking neural
network training tasks: MNIST, CIFAR10, and CIFAR100. The results for MNIST and the details
of the problems are given in Appendix C. We present the results regarding CIFAR10/CIFAR100.
CIFAR10. We use standard ResNet-20 and ResNet-32 (He et al., 2015) architectures for com-
paring the performance of OASIS with SGD, Adam, AdamW and AdaHessian4. Specifically, we
report 3 variants of OASIS: (i) adaptive learning rate, (ii) fixed learning rate (without first moment)
and (iii) fixed learning rate with gradient momentum tagged with “Adaptive LR,” “Fixed LR,” and
“Momentum,” respectively. For settings with fixed learning rate, no warmstarting is used in order to
obtain an initial D0 approximation. For the case with adaptive learning case, we used the warmstart-
ing strategy to approximate the initial Hessian diagonal. More details regarding the exact parameter
values and hyperparameter search can be found in the Appendix C. The results on CIFAR10 are
shown in the Figure 5 (the left and middle columns) and Table 1. As is clear, the simplest variant of
OASIS with fixed learning rate achieves significantly better results, as compared to Adam, and a per-
formance comparable to SGD. For the variant with an added momentum, we get similar accuracy as
AdaHessian, while getting better or the same loss values, highlighting the advantage of using different
preconditioning schema. Another important observation is that OASIS-Adaptive LR, without too
much tuning efforts, has better performance than Adam with sensitive hyperparameters. All in all, the
performance of OASIS variants is on par or better than the other state-of-the-art methods especially
4Note that we follow the same Experiment Setup as in (Yao et al., 2020), and the codes for other algorithms
and structures are brought from https://github.com/amirgholami/adahessian.
8
Published as a conference paper at ICLR 2022
ResNet20 on CIFAR10 With Welqht Decay
ResNet32 on CIFAR10 With Welqht Decay
Figure 5: Performance of SGD, Adam, AdamW, Adehessian and different variants of OASIS on
CIFAR10 (left and middle columns) and CIFAR100 (right column) problems on ResNet-20 (left
column), ResNet-32 (middle column) and ResNet-18 (right column).
Epochs
SGD. As we see from Figure 5 (left and middle columns), the lack of momentum produces a slow,
noisy training curve in the initial stages of training, while OASIS with momentum works better than
the other two variants in the early stages. All three variants of OASIS get satisfactory results in the
end of training. More results and discussion regarding CIFAR10 dataset are in Appendix C.
Table 1: ResUlts of ResNet-20/32 on CIFAR10 Table 2: Results of ResNet-18 on
Setting	ResNet-20 ResNet-32 CIFAR100.
			Setting	ResNet-18
SGD	92.02 ± 0.22	92.85 ± 0.12		
Adam	90.46 ± 0.22	91.30' ± 0.15	SGD	76.57 ± 0.24
AdamW	91.99 ± 0.17	92.58 ± 0.25	Adam	73.40 ± 0.31
AdaHessian	92.03 ± 0.10	92.71 ± 0.26	AdamW AdaHessian	72.51 ± 0.76 75.71 ± 0.47
OASIS- Adaptive LR OASIS- Fixed LR	91.20 ± 0.20	92.61 ± 0.22		
	91.96 ± 0.21	93.01 ± 0.09	OASIS- Adaptive LR	76.93 ± 0.22
OASIS- Momentum	92.01 ± 0.19	92.77 ± 0.18	OASIS- Fixed LR	76.28 ± 0.21
OASIS- Momentum	76.89 ± 0.34
CIFAR-100. We use the hyperparameter settings obtained by training on CIFAR10 on
ResNet-20/32 to train CIFAR100 on ResNet-18 network structure.5 We similarly compare
the performance of our method and its variants with SGD, Adam, AdamW and AdaHessian. The
results are shown in Figure 5 (right column) and Table 2. In this setting, without any hyperparam-
eter tuning, fully adaptive version of our algorithm immediately produces results surpassing other
state-of-the-art methods especially SGD.
6	Final Remarks
This paper presents a fully adaptive optimization algorithm for empirical risk minimization. The
search direction uses the gradient information, well-scaled with a novel Hessian diagonal approx-
imation, which itself can be calculated and stored efficiently. In addition, we do not need to tune
the learning rate, which instead is automatically updated based on a low-cost approximation of the
Lipschitz smoothness parameter. We provide comprehensive theoretical results covering standard
optimization settings, including convex, strongly convex and nonconvex; and our empirical results
highlight the efficiency of OASIS in large-scale machine learning problems.
Future research avenues include: (1) deriving the theoretical results for stochastic regime with
adaptive learning rate; (2) employing variance reduction schemes in order to reduce further the
noise in the gradient and Hessian diagonal estimates; and (3) providing a more extensive empirical
investigation on other demanding machine learning problems such as those from natural language
processing and recommendation system (such as those from the original AdaHessian paper (Yao
et al., 2020)).
5https://github.com/uoguelph-mlrg/Cutout.
9
Published as a conference paper at ICLR 2022
Acknowledgements
MT was partially supported by the NSF, under award numbers CCF:1618717/CCF:1740796. PR
was supported by the KAUST Baseline Research Funding Scheme. MM would like to acknowledge
the US NSF and ONR via its BRC on RandNLA for providing partial support of this work. Our
conclusions do not necessarily reflect the position or the policy of our sponsors, and no official
endorsement should be inferred.
Ethics S tatement
This work presents a new algorithm for training machine learning models. We do not foresee any
ethical concerns. All datasets used in this work are from the public domain and are commonly used
benchmarks in ML papers.
Reproducibility S tatement
We uploaded all the codes used to make all the experiments presented in this paper. We have used
random seeds to ensure that one can start optimizing the ML models from the same initial starting
point as was used in the experiments. We have used only datasets that are in the public domain,
and one can download them from the following website https://www.csie.ntu.edu.tw/
~cjlin∕libsvmtools∕datasets∕. After acceptance, We will include a link to the GitHUb
repository where we will host the source codes.
References
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood.
Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782,
2017.
C. Bekas, E. Kokiopoulou, and Y. Saad. An estimator for the diagonal of a matrix. Applied
NumericalMathematics, 57(11):1214-1229, 2007. ISSN0168-9274. doi: https://doi.org/10.1016/
j.apnum.2007.01.003. URL https://www.sciencedirect.com/science/article/
pii∕S0168927407000244. Numerical Algorithms, Parallelism and Applications (2).
Albert S Berahas, Jorge Nocedal, and Martin Takac. A multi-batch l-bfgs method for machine
learning. In Advances in Neural Information Processing Systems, pp. 1055-1063, 2016.
Albert S Berahas, Majid Jahani, Peter Richtarik, and Martin Takac. Quasi-newton methods for deep
learning: Forget the past, just sample. arXiv preprint arXiv:1901.09997, 2019.
Albert S. Berahas, Raghu Bollapragada, and Jorge Nocedal. An investigation of Newton-Sketch and
subsampled Newton methods. Optimization Methods and Software, 35(4):661-680, 2020.
Raghu Bollapragada, Richard H Byrd, and Jorge Nocedal. Exact and inexact subsampled newton
methods for optimization. IMA Journal of Numerical Analysis, 39(2):545-578, 2019.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):
977-995, 2011.
Kartik Chandra, Erik Meijer, Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George,
Melissa Grueter, Basil Hosmer, Steffi Stumpos, Alanna Tempest, et al. Gradient descent: The
ultimate optimizer. arXiv preprint arXiv:1909.13371, 2019.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019.
10
Published as a conference paper at ICLR 2022
Frank E. Curtis. A self-correcting variable-metric algorithm for stochastic optimization. In Interna-
tional Conference on Machine Learning, pp. 632-641, 2016.
John E Dennis, Jr and Jorge J More. Quasi-newton methods, motivation and theory. SIAM review, 19
(1):46-89, 1977.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Roger Fletcher. Practical Methods of Optimization. John Wiley & Sons, New York, 2 edition, 1987.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtarik. Sgd: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200-5209. PMLR, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
Majid Jahani, Xi He, Chenxin Ma, Aryan Mokhtari, Dheevatsa Mudigere, Alejandro Ribeiro, and
Martin Takac. Efficient distributed hessian free algorithm for large-scale empirical risk minimiza-
tion via accumulating sample strategy. In International Conference on Artificial Intelligence and
Statistics, pp. 2634-2644. PMLR, 2020a.
Majid Jahani, Mohammadreza Nazari, Sergey Rusakov, Albert S Berahas, and Martin Takac. Scaling
up quasi-newton algorithms: Communication efficient distributed sr1. In International Conference
on Machine Learning, Optimization, and Data Science, pp. 41-54. Springer, 2020b.
Majid Jahani, Naga Venkata C Gudapati, Chenxin Ma, Rachael Tappenden, and Martin Takac. Fast
and safe: accelerated gradient methods with optimality certificates and underestimate sequences.
Computational Optimization and Applications, 79(2):369-404, 2021a.
Majid Jahani, Mohammadreza Nazari, Rachael Tappenden, Albert Berahas, and Martin Takac. Sonia:
A symmetric blockwise truncated optimization algorithm. In International Conference on Artificial
Intelligence and Statistics, pp. 487-495. PMLR, 2021b.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for sgd: An adaptive learning rate for fast convergence. arXiv preprint arXiv:2002.10542,
2020.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Yura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. In Interna-
tional Conference on Machine Learning, pp. 6702-6712. PMLR, 2020.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151-3181, 2015.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
11
Published as a conference paper at ICLR 2022
Yurii Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018.
L. M. Nguyen, J. Liu, K Scheinberg, and M. Takac. SARAH: a novel method for machine learning
problems using stochastic recursive gradient. In Advances in neural information processing
systems, volume 70,pp. 2613-2621, 2017.
Lam Nguyen, PhUong Ha Nguyen, Marten Dijk, Peter Richtarik, Katya Scheinberg, and Martin
Takac. Sgdand hogwild! convergence without the bounded gradients assumption. In International
Conference on Machine Learning, pp. 3750-3758. PMLR, 2018.
Lam M Nguyen, Phuong Ha Nguyen, Peter Richtarik, Katya Scheinberg, Martin Takac, and Marten
van Dijk. New convergence aspects of stochastic gradient algorithms. J. Mach. Learn. Res., 20:
176-1, 2019.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Series in Operations
Research. Springer, second edition, 2006.
Santiago Paternain, Aryan Mokhtari, and Alejandro Ribeiro. A newton-based method for nonconvex
optimization with fast evasion of saddle points. SIAM Journal on Optimization, 29(1):343-368,
2019.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in neural information processing systems,
pp. 693-701, 2011.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
F. Roosta, Y. Liu, P. Xu, and M. W. Mahoney. Newton-MR: Newton’s method without smoothness or
convexity. Technical Report Preprint: arXiv:1810.00303, 2018.
Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled newton methods. Mathematical
Programming, 2018.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 1195-1204. PMLR, 2019.
P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-type methods for non-convex optimization
under inexact Hessian information. Technical Report Preprint: arXiv:1708.07164, 2017.
Peng Xu, Fred Roosta, and Michael W Mahoney. Second-order optimization for non-convex machine
learning: An empirical study. In Proceedings of the 2020 SIAM International Conference on Data
Mining, pp. 199-207. SIAM, 2020.
Z. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact non-convex Newton-type methods.
Technical Report Preprint: arXiv:1802.06925, 2018.
Z. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney. PyHessian: Neural networks through the lens
of the Hessian. Technical Report Preprint: arXiv:1912.07145, 2019.
12
Published as a conference paper at ICLR 2022
Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. Adahessian: An
adaptive second order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
13
Published as a conference paper at ICLR 2022
A	Theoretical Results and Proofs
A.1 Assumptions
Assumption 4.1. (Convex). The function F is convex, i.e., ∀w, w0 ∈ Rd,
F(W) ≥ F(w0) + hVF(w0), w — w0i.	(14)
Assumption 4.2. (L-smooth). The gradients of F are L-Lipschitz continuous for all w ∈ Rd, i.e.,
there exists a constant L > 0 such that ∀w, w0 ∈ Rd,
F(W) ≤ F(w0) + hVF(w0),w — w0i + kww— — w0k2.	(15)
Assumption 4.3. The function F is twice continuously differentiable.
Assumption 4.4. (μ-strongly convex). The function F is μ-StrongIy convex, i.e., there exists a
constant μ > 0 such that ∀w, —0 ∈ Rd,
F(—) ≥ F(—0) + hVF(w0), — — —0i + μ||— — ―0k2.	(16)
Assumption 4.12. Thefunction F(.) is bounded below by a scalar F.
Assumption 4.14. There exist a constant γ such that EI [|VFI (—) — VF (—)|2] ≤ γ2.
Assumption 4.16. VFI (—) is an unbiased estimator of the gradient, i.e., EI [VFI (—)] = VF (—),
where the samples I are drawn independently.
A.2 Proof of Lemma 4.5
Lemma 4.5. (Boundon change of Dk). Suppose thatAssumption 4.2 holds, i.e., ∀— : V2f (—) W LI,
then
1.	|(Vk)il ≤ Γ ≤ dL,L, where Vk = Zk Θ V2F(—k)zk.
2.	there ∃δ ≤ 2(1 — β2 )Γ such that
|Dk+i— Dkk∞ ≤ δ,	∀k.
Proof. By Assumption 4.2, we have that |V2F (—)|2 ≤ L and hence
Mk∞ ≤ |V2F(—)∣∣∞ ≤ √d∣V2F(—)∣2 ≤ √dL,
which finishes the proof of case 1.
Now, from (8) we can derive
Dk+1 — Dk = (β2 — 1)Dk + (1 — β2) zk Θ V2F(—k)zk
and hence
∣∣Dk+1 — Dk ∣∣∞ = (I — β2 )|Dk — Zk θ V2F (Wk )zk ∣∣∞ ≤ (1 — β2)kDk — Vk ∣∣∞
≤ (1 — β2)(∣Dk∣∞ + IlVkk∞) ≤ 2(1 — β2)Γ.	口
A.3 Lemma Regarding Smoothness with Weighted Norm
In the following, we present a lemma for smoothness with weighted norm. Theorem 2.1.5 in (Nesterov
et al., 2018) provides the same analysis for any norm, and the following lemma can be seen as a
special case of Theorem 2.1.5 (Nesterov et al., 2018) with respect to the weighted Euclidean norm.
In the following, we provide the proof for completeness.
Lemma A.1. (Smoothness with norm D) Suppose that Assumptions 4.1 and 4.2 hold, and D 0,
then we have:
.... ~ ..
∣VF(x) — VF(y)∣D ≤ Lkx — y∣D,	(17)
r f	L
where L =-----
λmin(D)
14
Published as a conference paper at ICLR 2022
L
〜
Proof. By the equality L
-----7--, We conclude that LD A LI which results in:
λmin(D)	一
亍
F(y) ≤ F(x) + EF(x),y - X + -IIx - y∣∣D.
(18)
Extending proof ofTheorem 2.1.5. in (Nesterov, 2013) we define φ(y) = F(y) - "F(x), y). Then,
clearly, X ∈ arg min φ(y) and (18) is still valid for φ(y).
Therefore
Φ(χ) ≤ Φ(y -
DT
~r~ Vφ(y)),
L
(18)	DT	L DT
φ(x) ≤ Φ⑻ + (Vφ(y),--rVφ(y)i + -Il- KVφ(y)∣∣D,
L	- L
FIX)-NF(x),xi ≤ F(V)-EF(x),yi - 1 (Vφ(y),D-1Vφ(y)} + JIIDTVφ(y)∣∣D,
L	-L
F(X) ≤ F(V) + (VF(x),x -Vi- J(HVφ(v)HD)2,
-L
F(x) ≤ F(y) + (VF(x),x -Vi- J(HVF(V)-VF(X)IlD)2.
-L
Thus
F(x) + (VF(x), V - Xi + J(HVF(v) - VF(X)IID)2 ≤ F(v).
-L
(19)
Adding (19) with itself with X swapped with V we obtain
1(IVF(v) - VF(X)IID)2 ≤ (VF(v) - VF(x),V - Xi
L
=(D-1(VF(v) - VF(x)),D(v - x)i
≤ I∣vf(V)- vf(X))IIDIIv - x∣∣d,
which implies that
........................................ ~ ..
IlVF(X)-VF(V)IID ≤ LllX - VlID.	(20)
□
A.4 PROOF OF THEOREM 4.6
Lemma A.2. Let f : Rd → R be a convex function, and x* is one of the optimal solutions for (1).
Then,for the sequence of {wk} generated by Algorithm 1 we have:
Ilwk+ 1 - w*lDj-∣∣wk - wk+1∣∣Dk + -Ik (1 + θk )(F (Wk ) - F(W*))
≤ IlWk - w*∣D…+ -l∣Wk - Wk-IIlD…+ 2ηkθk(F(wk-1) - F(w*))+
2(i-e2)「((ηkαk+ 2)|wk-1- wk∣∣2 + (L *k +1)∣IWk- w*ιι2). (21)
Proof. We extend the proof in (Malitsky & Mishchenko, 2020). We have
l∣Wk+1 - W*∣∣Dk = ∣∣Wk+1 - Wk + Wk - W*∣∣Dk
=∣∣Wk+1 - Wk IlDk + IlWk - W*∣∣Dk + 2(Wk+1 - Wk ,D k (Wk - W*)i
=∣∣Wk+1 - WkIlDk + ∣∣Wk - W*∣∣Dk +2ηk(VF(Wk), W* - Wki
≤ ∣∣Wk+1 - WkIlDk + ∣∣Wk - W*∣∣Dk + 2ηk(F(w*) - F(Wk))	(22)
=|Wk+1 - Wk∣∣Dk + IlWk- w*∣∣Dk - 2ηk(F(Wk) - F(W*)),
15
Published as a conference paper at ICLR 2022
where the third equality comes from the OASiS,s step and the inequality follows from convexity of
F(w). Now, let,s focus on ∣∣wfc+1 - Wk∣∣D . We have
Ilwk+ι- wk IID卜=2∣wk+ι- WkIID卜-IlWk+ι- wk IIDk
=2{-ηkDk 1VF(wk),Dk(Wk+1 - Wk)>-∣∣Wk+ι - WkIlDk
=-2ηkhVF(Wk),Wk+ι - Wk 一 ∣∣Wk+ι 一 WkIIDk
=-2ηk〈VF(Wk) - VF(Wk-1),Wk+1 - Wk)一 2ηk NF(Wk-1),Wk+1 - Wk-
iiWk+i- Wk ∣∣D k
=2ηk〈VF(Wk) - VF(Wk-I),Wk - Wk+1〉+ 2ηk〈VF(Wk-I),Wk - Wk+1)-
iiWk+1- Wk ∣∣D k.
Now,
2ηk〈VF(Wk)- VF(Wk-I),Wk - Wk+1〉≤ 2ηkIlVF(Wk) - VF(Wk-I)IlD/∣Wk - Wk+1∣Dfc
(10)
≤ ∣∣Wk - Wk-IkDk∣Wk - Wk+1∣Dk
≤ 2 llWk - Wk-IIlDk + 2 llWk - Wk+1 iDk ,
where the first inequality comes from Cauchy-Schwarz and the third one follows Young,s inequality.
Further,
1
〈VF(Wk-1),Wk - Wk+1)=-------〈Dk-1(Wk-1 - Wk),Wk - Wk+1)
ηk-1
=-----〈Dk-1(Wk-1 - Wk), ηkDk IVF(Wk))
ηk-1
= 决(Wk-I - Wk )T D k-1D k IVF (Wk )
ηk-1
=决(Wk-I - Wk )T VF(Wk) +
ηk-1
决(Wk-I - Wk)T (Dk-IDk 1 - 1) VF (Wk),	(23)
ηk-1
where the first two qualities are due the OASiS,s update step. The second term in the above equality
can be upperbounded as follows:
(Wk-I- Wk )T (D k-1D k-1 - I) VF (Wk) ≤ (1 -e2 ) ^ɑ , IlWk-I- Wk ∣∣ , IlVF (Wk )∣∣,
(24)
where multiplier on the left is obtained via the bound on ∣∣ ∙ ∣∣∞ norm of the diagonal matrix
Dk-1Dk-1 -1：
IlDk-IDk 1 - ∕∣∣∞ = max |(Dk-IDk 1 - I) , I= max I (Dk-1 - Dk)i | | (Dk 1)i | ≤ (1 -e2)出,
which also represents a bound on the operator norm of the same matrix difference. Next we can use
the Young,s inequality and VF(w*) = 0 to get
(1 -e2)--- ∙ ∣∣wk-1 - wk Il ∙ IlVF(Wk )∣∣ ≤ (1 -e2)-(∣∣wk-1 - wk ∣∣2 + L2∣∣wk - w*∣∣2)∙ (25)
Q	Q
Therefore, we have
〈VF (Wk-1), Wk - Wk+1) ≤ ~^k~ (Wk-1 - Wk )T VF (Wk ) +
ηk-1
.Γ,..	..c	c..	...n
(1 -e2)一(IlWk-I - wk Il + L ∣∣wk - W Il ).
Q
16
Published as a conference paper at ICLR 2022
Also,
I∣wk+1 — wk IIDk ≤ 2 IlWk — Wk-IIIDk + 2 1wk — Wk + 1 1Dfc + 2ηkθk (Wk-I — wk )T PF (wk ) +
Γ
2ηk θk (1 — β2) a (IlWk-I — wk Il + L ∣∣wk — W ∣∣ ) — ||wk+1 — wk IlDk
≤ 2 IlWk — Wk-IIIDk +1 ∣∣Wk — Wk+ι∣∣Dk + 2ηkθk (F (Wk-I) — F (Wk))+
Γ
2ηk θk (1 一e2)α (IlWk-I — wk Il + L IlWk — W ∣∣ ) -∣∣wk+1 — wk IlDk -
Finally, We have
∣∣wk+1 — w*∣∣Dk ≤ 2 IlWk — Wk-IllDk + 2 IlWk - wk + 1∣∣Dk + 2ηkθk (F(Wk-1) - F(Wk )) -
一	Γ	..c	_n..	-C.	..	..c
2ηk θk (1 一e2)α (IlWk-I — Wkll - L ∣∣wk — W ∣∣ ) -∣∣wk+1 — wk IlDk
+ IlWk — w*∣∣Dk — 2ηk (F (wk) — F (w*)).
By simplifying the above inequality, We have:
llwk+1 — w*∣∣D k + 2 llwk — wk+1∣∣D k + 2ηk (1 + θk )(F(Wk ) — F (w*))
≤ IlWk — w* l∣Dk + 2 llwk — Wk-IkDk + 2ηkθk (F(Wk-I) — F (w*)) +
Γ
2ηk θk (1 一e2)α (llwk-1 — wk Il + L IlWk — W || )
=IlWk — w*l∣Dk 1 + 2 llwk — Wk-IkDk 1 + 2ηkθk (F(Wk-I) — F(w*)) +
Γ
2ηk θk (1 一e2)一 (IlWk-I — wk k + L IlWk — W || ) +
a
kwk— w*kD k-D k-ι+ 2 kwk— WkTkD k-D k-
≤ ∣∣Wk — w*∣∣D…+ 2l∣Wk — Wk-IkD…+ 2ηkθk(F(Wk-1) — F(w*))+
Γ
2ηk θk (1 一e2)一(∣∣wk-1 — wk k + L ∣∣wk — W || ) +
a
2(1 — β2)r(∣∣wk — w*k2 + 2 ∣∣wk — Wk-Ik2)
=kwk ― w*kD k-1 + 2 kwk ― Wk-IkD k_1 + 2ηk θk (F (Wk-I) ― F (w*)) +
2(1 — β2)r(η^∣∣Wk-i — Wkk2 + L Okok ∣∣Wk — w*k2 + ∣∣Wk — w*k2 +
2 I∣wk — Wk-Ik2).
□
Theorem 4.6. Suppose thatAssumptions 4.1, 4.2 and 4.3 hold. Let {wk} be the iterates generated
by Algorithm 1, then we have:
F (Wk) — F * ≤ -- -+ 2L(1 — β2)Γ ɪ,
k	k
where
C = Iw1 ― w*iDo + 1 ∣∣wι — WOkD0 -2ηιθι(F(W0)— F(W*))
Qk = X ((^oɪ + 2)Iwi-i — Wik2 + (-O^ + 1)Hwi — w*12)∙
i=1
17
Published as a conference paper at ICLR 2022
Proof. By telescoping inequality (21) in Lemma A.2 we have:
Ilwk+1 - w*kD k + 2 Ilwk- wk + 1kD k + 2ηk(1 + θk )(F(Wk ) - F (W))
k-1
+ 2^χ[ηi(1 + θi) - ηi+ιθi+ι](F(Wk)- F(W*))
i=1
≤ kw1 — w*kD 0 + 1 kw1 — wOkD 0 +2ηιθι(F(WO)- F (w* )) +
'----------------------{------------------------}
C
2(1 - β2)r X(( (ηαL+2)kWiT - Wik2+(~~αα~ι+力限，- W*k2).
i=1	α	α
X------------------------{-----------------------}
Qk
Moreover, by the rule for adaptive learning rule we know ηi(1 + θi) - ηi+1θi+1 ≥ 0, ∀i. Therefore,
we have
k-1
2ηk (1 + θk)(F(Wk) - F(W*))+2 X[ηi (1 + θi) - ηi+1 θi+1](F (Wk) - F (W*))
i=1
≤ C + 2(1 - β2)ΓQk.
ηk(1 + θk)Wk + Pik=-11(ηi(1 + θi) - ηi+1θi+1)Wi
By setting W =   ----------------- JI -----------------+~~——, where Sk
Sk
Pik=-11(ηi(1 + θi) - ηi+1θi+1), and by using Jensens’s inequality, we have:
F(Wk ) - F* ≤ TTTT + (I - β2)r些.
ηk (1 + θk ) +
By the fact that ηk ≥ ɪ thus ɪ ≤ 半,we have
2L	Sk	k
F (Wk) - F * ≤ F- -+ 2L(1 - β2)Γ ɪ.
kk
□
A.5 Proof of Lemma 4.8
Lemma 4.8. Suppose that Assumptions 4.2, 4.3 and 4.4 hold, then ηk ∈
h α Γ
∣2L, 2μ
L
Proof. By (17) and the point that L =-X-, we conclude that:
λmin(Dk)
kWk - Wk-1kD%	≥ ɪ = Amin(Dfe)
2kvF(Wk) -VF(Wk-ι)kD ≥ 21 - —2L-
α
≥ 2L.
Now, in order to find the upperbound for
which comes from Assumption 4.4:
kwk-wk-1kD k
2kVF (wk )-VF (wk-1 )kD
, we use the following inequality
F(Wk) ≥ F(Wk-I) + hVF(Wk-I), (Wk - Wk-i)i + 2IlW - Wk-1∣∣2∙	(26)
μ
By setting μ =----X-, we conclude that μDk W μI which results in:
λmax (Dk )
F(Wk) ≥ F(Wk-I) + hVF(Wk-i), (Wk — Wk-i)i + 2kW — Wk-IkDj	(27)
18
Published as a conference paper at ICLR 2022
The above inequality results in:
“∣∣w - Wk-i∣∣Dχ ≤ EF(Wk) - VF(wk-1), (wk - Wk-1))
=hDk-1(VF(wk) - VF(Wk-1)), Dk(wk - Wk-1))
≤ IlVF(Wk) - VF(Wk-I))IlD/I Wk - Wk-IllDk.	(28)
Therefore, we obtain that
kWk - Wk-IllDk	(≤ 1 = λmax(Dk) ≤ 匚
2∣∣VF(Wk) -VF(Wk-1)∣ID	_ 2μ — —2μ ―-我
Dk
and therefore, by the update rule for ηk, We conclude that ηk ∈ [*, ；].	口
A.6 Proof OF Theorem 4.9
Lemma A.3. . Suppose Assumptions 4.2, 4.3 and 4.4 hold and let w* be the unique solution of(1).
Thenfor (Wk) generated by Algorithm 1 we have:
l∣wk+1 - w*I∣D k + 2 l∣wk+1 - wk IlD k + 2ηk (I + θk)(F (Wk)- F(W*))
≤ ∣∣Wk - w*∣∣D…+ 2∣lWk- Wk-1 IlD…+ 2ηkθk(F(Wk-1) - F(w*))
+ ((1 - β2)r(l + 2?ηk ) - μηkθk) ∣∣Wk - Wk-11∣2
+ ((1 - β2)r(2+ 2L2fηk ) - μηk) ∣∣Wk - w*I2.
Proof. By the update rule in Algorithm 1 we have:
l∣wk+1 - w*∣∣Dχ = ∣∣wk+1 - Wk + Wk - w*∣∣Dχ
=∣∣Wk+1 - WkIlDk + ∣∣Wk - w*∣∣D^ +2(Wk+1 - Wk,Dk(wk - w*))
=∣∣Wk+1 - WkIIDk + ∣∣Wk - w*∣∣D^ + 2ηkhVF(wk),w* - Wk)
≤ ∣∣Wk+1 -	WkIlDk + ∣∣Wk	- w*∣∣D^	+	2ηk(F(w*)	- F(Wk))	(29)
= IlWk+1 -	WkIlDk + ∣∣Wk	- w*∣∣D^	-	2ηk(F(Wk)	- f(w*)),
where the inequality follows from convexity of F(w). By strong convexity of F(.) we have:
F (w*) ≥ F (wk) + (VF (wk ),w* - Wk) + μ ∣∣Wk - w*∣2.	(30)
In the lights of the strongly convex inequality we can change (29) as follows
(29),(30)	μ
ι∣wk+1-w*i∣dk ≤	ι∣wk+1-wkIIDk+ ι∣wk-w*iDk+ 2ηk(F(W*)-F(Wk)- 2IlWk-w*『)
=∣∣Wk+1 - WkIlDk + ∣∣Wk - w*∣∣Dχ - 2ηk(F(Wk) - F(w*)) - μηkIlWk - w*∣2.
(31)
19
Published as a conference paper at ICLR 2022
Now, let,s focus on ∣∣wfc+1 - WkllDk. Wehave
Ilwk+ι- wk IlD卜=2∣wk+ι- WkIlD卜-Ilwk+ι- wk IlDk
=2{-ηkDk 1VF(wk),Dk(wk+ι - wk)iTlwk+ι - wkIIDk
=-2ηkhVF(wk),wk+ι - wki - llwk+ι - wkIIDk
=-2ηk EF(wk) - VF(wk-1),wk+1 - wk〉- 2ηk EF(wk-1),wk+1 — wk)一
ιιwk+ι- wk ∣∣D k
=2ηk〈VF(wk) - VF (wk-1),wk - wk+1) +2ηk EF (wk-1),wk - wk+1)-
llwk+1 - wk ∣∣Dk.	(32)
Now,
2ηkNF(wk) - VF(wk-1),wk - wk+1) ≤ 2ηk∣∣VF(wk) - VF(wk-1)∣∣D/Iwk - wk+1∣∣Dfc
≤ Ilwk - wk-1∣∣Dk∣∣wk - wk+1∣∣Dk
≤ 2Ilwk - wk-1∣∣Dk + 2Ilwk - wk+1∣∣Dk,	(33)
where the first inequality is due to Cauchy-Schwarz inequality, and the second inequality comes from
.	Ci	.	∣∣Wk-Wk-1 k_D,	TA 1	. /CC、.	/CC、
the choice of learning rate such that ηk ≤ 2∣rf(Wk)tf(wk-1)∣区∙ By plugging (33) into (32) we
obtain:
(32) 1	1
llwk+1 - wkIIDk ≤ 2Ilwk - wk-1l∣Dk + 2Ilwk - wk+1IDfc + 2ηk〈vf(wk-1),wk - wk+1)-
llwk + 1 - wk IlDk .	(34)
Now, we can summarize that
(31)
∣∣wk+1 - w*lDfc ≤ llwk+1 - wkIIDk + Ilwk - w*lDfc - 2ηk(F(wk) - F(w*)) - μηk∣wk - w*∣∣2
(34)1	1
≤ 2Ilwk -wk-1lDfc + 2Ilwk -wk+1lDfc +2ηkhVF(wk-1),wk -wk+1)-
llwk+1 - wkIIDk + Ilwk - w*lDfc - 2ηk(F(wk) - F(w*)) - μηklwk - w*ll2
=2 Ilwk - wk-1 Il Dk - 2 Ilwk - wk + 1 lDfc + 2ηkhVF(wk-1), wk - wk⅛1i +
Ilwk - w*lDfc - 2ηk(F(wk) - F(w*)) - μηkIlwk - w*∣∣2
=2 Ilwk - wk-1∣∣D k-1 + Ilwk - w*lD k-1 - 2 lwk - wk+1 lD k +
2ηkhVF(wk-1 ),wk - wk+1) - 2ηk(F(wk) - F(w*))+
2 Ilwk - wk-1∣∣D k-D k-1 + ιιwk - w"∣D k-D k-1
Next, let us bound〈VF(wk-1), wk - wk+1).
-μηkIlwk - w*ll2∙
(35)
By the OASiS,s update rule, wk+1 = wk - ηkDk-1VF(wk), we have,
1
〈VF(wk-1),wk - wk+1) =--〈Dk-1(wk-1 - wk),wk - wk+1)
ηk-1
=----〈Dk-1(wk-1 - wk), ηkDk-1 VF(wk))
ηk-1
=决(wk-1 - wk )T D k-1D k IVF (wk )
ηk-1
= 决(wk-1 - wk )T VF(wk ) +
ηk-1
“k (wk-1 - wk )T (D k-1D k 1 - I) VF (wk).	(36)
ηk-1	∖	)
20
Published as a conference paper at ICLR 2022
The second term in the above equality can be bounded from above as follows:
(wk-1 - wk )T (力 k-1 力 k-1 - I) VF (Wk) ≤ (1 —万2)^a , Ilwk-I- Wk ∣∣ ∙ ∣∣VF (Wk )∣∣,	(37)
where multiplier on the left is obtained via the bound on ∣∣ ∙ ∣∣∞ norm of the diagonal matrix
力k-1 /k-1 - I:
IIDd k-1DD k-1 - III ∞ = max ∣(dd k-1DD k-1 - 1) , I= max I (DD k-1 - DD k )i I I (DD k-1)i I ≤ (1 - β2) ~^,
which also represents a bound on the operator norm of the same matrix difference. Next we can use
the Young,s inequality and VF(w*) = 0 to get
(1 -e2)  ∙ Ilwk-I- wk k ∙ IlVF(Wk )∣∣ ≤ (1 -e2)-( ∣∣wk-1 - wk ∣∣2 + L2∣∣wk - w*∣∣2). (38)
α	α
By the inequalities (72), (36), (37) and (38), and the definition θk = "k We have:
ηk-ι
llwk+ι - w*l∣Dk + 2llwk+ι - wkIlDk + 2ηk(1 + θk)(F(Wk) - F(W*))
≤ Ilwk 一 w*∣∣D…+ 2∣∣wk 一 wk-ι∣D…+ 2ηkθk(F(wk-ι) 一 F(w*))
+ (2∣wk - wk-1 IlDk-DLl + (1 - β2)2r∣ηk Ilwk - wk-i∣2 - μηkθkIlwk - wk-i∣∣2)
+ (kwk - w*l∣Dk-Dk- + (I -伪)—Q knk Ilwk - w*∣∣2 - μηkIlwk - w*ll2)
≤ Ilwk ― w*∣∣D…+ 2Ilwk ― wk-i∣∣D…+ 2ηkθk(F(wk-1) ― F(w*))
+	((1 -e2)「(1+----Qp-) - μηkθk) ∣∣wk - wk-1 Il2
+	((1 -e2)「(2 + 2L Qkηk) - μηk) Ilwk - w*∣∣2∙
□
Theorem 4.9. Suppose that Assumptions 4.2, 4.4, and 4.3 hold and let w* be the unique solution
for (1). Let {wk} be the iterates generated by Algorithm 1. Then, for all k ≥ 0 we have: If
β2 ≥ max(1 —
α4μ4	α3μ3	ɪɪ
4L2Γ2(α2μ2 + LΓ2), - 4LΓ(2α2μ2 + L3Γ2) }
2
Ψk+1 ≤ (1 -	)Ψk,	(39)
2Γ 2κ2
where
ψk+1 = ∣wk+1- w*∣Dfc+ 2 ∣∣wk+1 - wkkDfc + 2ηk (1 + θk)(F (wk ) - F(W*))∙
21
Published as a conference paper at ICLR 2022
Proof. By Lemma A.3, we have:
Ilwk+ι- w*kD k + 2 Ilwk+ι — wk kD k + 2ηk(I + θk)(F (Wk)- F(W))
≤ kwk - w*kD k-1 + 2 kwk - wk-1 kD k-1 + 2ηk θk (F (Wk-I) - F (w* ))
+ ((1 一 β2)r(1+ 2?nk) 一 μηkθk) kwk 一 wk-1 k2
+ ((1 - β2)r (2+ 2Laknk) - μnk) kwk - w*k2.
Lemma 4.8 gives Us nk ∈ [*, ；], so We can choose large enough β2 ∈ (0,1) such that
((1 - β2)r (1 + 2θαnk) - μnkθk) ≤ 0,
2L	2 Lvkn-L- 2L2θknk∖	∖vn
I(I - β2)r I 2 +-a— ) - μnk) ≤ 0.
In other Words, We have the folloWing bound for β2 :
c 、 a	a4μ4
β2 ≥ m {1 - 2L2Γ2(a2μ2 + LΓ2),
a3μ3
2LΓ(2a2μ2 + L3Γ2)
(40)
HoWever, We can push it one step further, by requiring a stricter inequality to hold, to get a recursion
Which can alloWs us to derive a linear convergence rate as folloWs
((1 - β2)r(1 + 2θank) - μnkθk)
((I- β2)r(2 +2^0>) - μnk)
≤ - 2 μnk θk ,
1
≤ 一 2 μnk,
Which requires a correspondingly stronger bound on β2:
β. ≥ maχ{1_________________________a4μ________ 1__________a3μ_________}
β2 ≥	{	4L2Γ2(a2μ2 + LΓ2),	4LΓ(2a2μ2 + L3Γ2) }
(41)
Combining this With the condition for nk and definition for θk , We obtain
(1 - β2)r(1 + 2θank
-μnkθk) kwk - wk-1k2 ≤ -2μnkθkkwk 一wk-ιk2
2
≤ -4ΓK2kwk -wk-lkDk-1,
and
((1 - β2)r(2+2L2ak nk)
-μnk kwk - w*k2 ≤
-1 μnkIlwk - w*k2
≤
-念 kwk - w*kD k-1
where K = μ. Using it to supplement the main statement of the lemma, we get
kwk + 1 - w*l∣Dk + £ kwk+1 - wk kDk + 2nk (1 + θk)(F(Wk) - F(W*))
≤ (1 - 4ΓK )kwk - w*kD k-1 + 2(1 - 2Γ2^2 )kwk - wk-1 kD k-1 + '2Hk θk(F (wk-1)- F (w*)"
22
Published as a conference paper at ICLR 2022
Mirroring the result in (Malitsky & Mishchenko, 2020) we get a contraction in all terms, since for
function value differences we also can show
ηkθk	- -	ηk	≤ ι -
ηk(i + θk)	ηk (1 + θk) —	2Γκ
□
A.7 Proof of Theorem 4.11
Theorem 4.11. Suppose that Assumptions 4.2, 4.4, and 4.3 hold, and let F * = F (w*) where w* is
α2
the unique minimizer Let {wk } be the iterates generated by Algorithm 1, where 0 < ηk = η ≤ -—,
LΓ
and w0 is a starting point. Then, for all k ≥ 0 we have:
F (Wk)- F * ≤ (1 - 半)k [F (wo) - F *].
(42)
Proof. By smoothness of F(.) we have:
F(wk+ι) = F(Wk - ηD-Nf(Wk))
≤ F(Wk) + VF(Wk)T(-ηD-1 Vf(Wk)) + 2kηD-IVf(Wk)k2
≤ F(Wk)- £kVF(Wk)k2 + η⅞kVF(Wk)k2
Γ	2α2
=F(Wk) - n(ɪ - 2α)kVF(Wk)k2	(43)
≤ F(Wk) - η2ΓkVF(Wk)k2,	(44)
where the first inequality comes from Assumption 4.2, and the second inequality is due to Remark
4.10, and finally, the last inequality is by the choice of η. Since F(.) is strongly convex, we have
2μ(F(Wk) 一 F*) ≤ ∣∣VF(Wk)k2, and therefore,
F(Wk+ι) ≤ F(Wk) - 半(F(Wk)- F*).
Also, we have:
F (Wk+ι) - F * ≤ (1 - ημ )(F (Wk) - F *).
□
A.8 Proof of Theorem 4.13
Theorem 4.13. Suppose that Assumptions 4.3, 4.12 and 4.2 hold. Let {Wk} be the iterates generated
α2
by Algorithm 1, where 0 < ηk = η ≤ —-, and w° is a starting point. Then, for all T > 1 we have:
LΓ
ɪXX kVF(Wk)k2 ≤ 2r[F(WO)- F] -→→ 0.
T	ηT
k=1	η
Proof. By starting from (44), we have:
F(Wk+ι ≤ F(Wk) - 2ΓkVF(Wk)k2.
By summing both sides of the above inequality from k = 0 to T - 1 we have:
T-1	T-1
X (F (Wk+1) - F (Wk)) ≤-X 2Γ ∣VF (Wk )k2.
k=0	k=0
(45)
(46)
23
Published as a conference paper at ICLR 2022
By simplifying the left-hand-side of the above inequality, we have
T-1
X [F (wk+1) - F(wk)] = F(wT) - F(w0) ≥ Fb - F(w0),
k=0
where the inequality is due to 户 ≤ F(WT) (Assumption 4.12). Using the above, We have
X1 kVF(Wk)k2 ≤ T[F(WO)- F].
η
k=0
(47)
□
A.9 Proof of Theorem 4.17
Lemma A.4 (see Lemma 1 (Nguyen et al., 2018) or Lemma 2.4 (Gower et al., 2019)). Assume that
∀i the function Fi(W) is convex and L-smooth. Then, ∀W ∈ Rd the following hold:
EI[kVFι(w)k2] ≤ 4L(F(W) — F(w*)) + 2σ2.	(48)
Theorem A.5. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let parameters α > 0,
η > 0, β2 ∈ (0,1] are chosen such that a < 2ΓL, η ≤ ɪ, β2 > 1 一 2rη-07μ) > 0∙ Let {wk} be
the iterates generated by Algorithm 1, then, for all k ≥ 0,
E[kwk — w*kDk] ≤ (1 一 c)kMkD0 + (1 + 2(⅛β2κ)2⅛2，	(49)
where C = η"αTr一警(1-β2)r ∈ (0,1).
Remark A.6. Ifwe choose β2 :二1 一 4rη-07μ) > 0 then
E[kwk - w*kDk] ≤ (1 一 等)k kw0 - w*kD0 +4(Γ¾Γσ2η.
Proof. In order to bound krk+11∣2^ by krk+1 k2^ we will use that
0 Y DDk+1 = DDk + DDk+1 — DDk W DDk + IlDDk+1 — DDk k∞I W DDk + IlDDk+1 — DDk k∞I
ʌ .. _ _ _ ʌ , _ ._____________
W Dk + kDk + 1 - DkI∣∞I W Dk + 2(1 一 β2)rI
W Dk + 2(1 一 β2)Γ 1Dk W (1+ 2(I-β2)r )Dk.	(50)
Then
krk+1kDk+1(<) (1 + ≡β2κ)krk+1 kDk = (1 + 一ɪ)krk 一 ηD^k-1VFIk (Wk)|隹k
二(1 + %呼)krk kDk - 2η(1 + 月呼 Wk, VFIk (Wk)i
+ (1 + 2⅛κ)η2 (kVFrk(Wk)kDJ2
≤ (1 + 空守)krk kDk - 2η(1 + 空守 Wk, VFIk (Wk)i
2
+ (1 + 一江)-kVFIk (Wk )k2.	(51)
αα
24
Published as a conference paper at ICLR 2022
NoW, taking an expectation With respect to Ik conditioned on the past, We obtain
E[krk+ιkDk+1] (≤) (1 + 一ɪ)krkkDk + 2η(1 + %呼)(F* - F(Wk)- 2kwk - w*k2
+(1+
2
2(1-β2)r) — (4L(F(Wk) - F*) + 2σ2)
α
—(1 + 月产)krkkDk + 2η(1 + 月产)(F* - F(Wk)-今相kDk)
+(1+
2
2(1-β2)r) — (4L(F(Wk) - F*) + 2σ2)
α
(1 + 月产)(1- 2η2Γ) krkkDk
+(1+
2(1-β2)r ) (4L 心-
((F(Wk) - F*)) + (1 + 组守)η22σ2
≤ (1 + 2(i-β2)r
—(J^ + a
1 - ηΓ) krkkD k +(1 + 月产注 2σ2,
(52)
where We used the fact that 2L αη - 1 ≤ 0 → η ≤ 畀.In order to achieve a convergence We need to
have
(1 + 归守)(1-竿)=：1-c< 1.
We have
(1 + 2(i-β2)r
(ɪ + ɑ
Id) = I-
ημα - (Γ - ημ)2(1 - β2)Γ
Γα
{z^
c
Now, we need to choose β2 such that
1 > β2 > 1 -
ημα
2Γ(Γ - ημ)
> 0.
α
α
|
}
With such a choice We can conclude that
(52)
E[krkkDk] ≤ (1 - c)krk-ikDk-1 +(1 + 2⅛β2)r)%2σ2,
≤
≤
≤
(1-c)kkrokD0 + X(1 - c)i(1 +
i=0
(1-c)k krokD 0 + X (1-c)i(1 +
2(i-β2)r)栏 2σ2
α ɑ ,
2(i-β2)r)栏 2σ2
a Ja ,
i=0
(1-c)k krokD 0 +(1 +
2(1-β2)Γ)2σ2η
αc
α
α
2
□
Theorem 4.17. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold. Let {Wk} be the iterates
generated by Algorithm 1 with ηk = η ∈ (0, 0L2), then, for all k ≥ 0,
E[F(Wk) - F*] — (1 - c)k (F(wo) - F*) + η2⅛2,	(53)
cα2
where C =	2Γμ	一	2ηαL	∈ (0,1)∙	Moreover	if ηk	= η ∈ (0,	^TL2)	then
E[F(Wk) - F*] —(1 -竿)k (F(wo) - F*) + ^^∙	(54)
Proof. First, We Will upper-bound the F (Wk+1) using smoothness of F(W)
F(Wk+ι) = F(Wk - ηDk-1VF⅛k(Wk))
—F (Wk)+ VF (Wk)T (-ηDD k - 1VFIk (Wk)) + 2 旧仇八叫(Wk )k2
η2 L
—F(Wk) - ηVF(Wk)TDk-1VFIk (Wk) + η-2 kVFIk (Wk)k2,
2α2
25
Published as a conference paper at ICLR 2022
where the first inequality is because of Assumptions 4.2 and 4.4, and the second inequality is due to
Remark 4.10. By taking the expectation over the sample Ik, we have
η2 L
EIk [F(Wk+1)] ≤ F(Wk) - ηEIk [VF(Wk)TDk IVFrk (Wk力 + 2α2EIk [kvF!k (Wk)k2]
η2 L
=	F(Wk) - ηVF(Wk)TDk-1VF(Wk) + η-2EIk[kVFik(Wk)k2]
2α2	k k
≤	F (Wk) — η kVF (Wk )k2 + η2L (4L(F (Wk) - F *)+2σ2)
Γ	2α2
≤	F(Wk) + n2μ (F* - F(Wk)) + η2L (4L(F(Wk) — F*) + 2σ2)
Γ	2α2
Subtracting F* from both sides, we obtain
(55) η	η2 L	η2 Lσ2
Eik[F(Wk+1) — F ] ≤ (1 一 p2μ +	4L) (F(Wk) - F ) +-----
Γ	2α2	α2
/
(55)
1-
(F (Wk)- F *)+ η⅛2
α2
≤
(56)
\
By taking the total expectation over all batches I0, I1, I2,... and all history starting with W0, we have
E[F(Wk) - F*] (<) (1 — c)k (F(wo) — F*) + X (1 — c)i η2L2σ2
i=0	α
≤ (1 — c)k (F(wo) — F*) + X (1 — c)i η2Lσ2
i=0	α
=(1 — c)k (F (wo) — F *) + η⅛σ!
cα2
and the (53) is obtained. Now, if η ≤ ^TL2 then
2ημ	2η2L2	μ μ
C=^-------h ≥2η (γ
and (54) follows.
α2 μ L2
2Γl2 α2
—
□
A.10 Proof of Theorem 4.18
Theorem 4.18. Suppose that Assumptions 4.3, 4.12, 4.2, 4.14 and 4.16 hold. Let {Wk} be the iterates
2
η
generated by Algorithm 1, where 0 < ηk = η ≤ ―, and wo is the starting point. Then, for all
LΓ
k ≥ 0,
1 T -1
E T EkVF(Wk)k2
k=o
2Γ[F(wo) - F]	ηΓγ2L T→→ ηΓγ2L
ηT	a2	) α2
Proof. We have that
F (wk+ι) = F (wk — ηD k-1VFik (wk))
≤ F (Wk) + VF (Wk)T (-n,D k -1VFIk (Wk)) + 2 IInDkTVFIk (Wk )k2
η2 L
≤ F(Wk) — nVF(Wk)tDk-1VFIk (Wk) + A kVFIk (Wk)k2,
2α2
26
Published as a conference paper at ICLR 2022
where the first inequality is because of Assumptions 4.2 and 4.4, and the second inequality is due to
Remark 4.10. By taking the expectation over the sample Ik, we have
η2 L
EIk [F (Wk+1)] ≤ F (wk ) - ηEIk [VF (wk )TD k * I * * *VFrk (wk 力 + 2α2 EIk [kvF!k (wk )k2]
η2L
=F (wk) - ηVF (wk)T D k-1VF (wk) + 2α2 EIk [kVFik (wk )k2]
≤ F (wk) - η (1 - 2L) kVF (wk )k2 + η2α2L	(57)
22
≤ F (wk) - 2Γ kVF (wk )k2 + ¾-,	(58)
where the second inequality is due to Remark 4.10 and Assumption 4.14, and the third inequality is
due to the choice of the step length.
By inequality (58) and taking the total expectation over all batches I0, I1, I2,... and all history
starting with w0 , we have
E[F (wk+1) - F (wk)] ≤ - ɪ E[kVF (wk )k2] + 'ηγL.
2Γ	2α2
By summing both sides of the above inequality from k = 0 to T - 1,
T -1	T -1	2 2LT
E[F (wk+1) - F (wk)] ≤ - 2Γ	E[kVF (wk)k2] +
k=0	k=0
T -1
-2ΓE X kVF(wk)k2
k=0
η2γ2LT
+ F^
By simplifying the left-hand-side of the above inequality, we have
T -1
X E [F (wk+1) - F(wk)] = E[F (wT)] - F(w0) ≥ Fb - F(w0),
k=0
where the inequality is due to F ≤ F(wτ) (Assumption 4.12). Using the above, We have
E 区 kVF(wk)k2# ≤ 2r[F(w0) - F] + ⅛LT.
η	α2
k=0
□
A.11 Proofs with Line Search
Given the current iterate wk , the steplength is chosen to satisfy the following sufficient decrease
condition
F (wk + ηk Pk) ≤ F (wk) - cιηkVF (wk )T D k-1VF (wk),	(59)
where Pk = -Dk-1VF(wk) and ci ∈ (0,1). The mechanism works as follows. Given an initial
steplength (say ηk = 1), the function is evaluated at the trial point wk + αkpk and condition (59) is
checked. If the trial point satisfies (59), then the step is accepted. If the trial point does not satisfy
(59), the steplength is reduced (e.g., ηk = τηk for τ ∈ (0, 1)). This process is repeated until a
steplength that satisfies (59) is found.
Algorithm 2 Backtracking Armijo Linesearch (Nocedal & Wright, 2006)
Input： Wk, Pk
1: Selectηinitial, c1 ∈ (0, 1) and τ ∈ (0, 1)
2: η = ηinitial , j = 0
3: while F(Wk + ηkPk) > F(Wk) + CinKF(Wk)tPk do
4:	Set ηj+1 = τηj
5:	Setj =j+1
6: end while
Output: ηk = ηj
27
Published as a conference paper at ICLR 2022
By following from the study (Berahas et al., 2019), we have the following theorems.
A.11.1 Determinitic Regime - S trongly Convex
Theorem A.7. Suppose that Assumptions 4.3 and 4.2 and 4.4 hold. Let {wk } be the iterates
generated by Algorithm 1, where ηk is the maximum value in {τ-j : j = 0, 1, . . . } satisfying (59)
with 0 < c1 < 1, and w0 is the starting point. Then for all k ≥ 0,
F(Wk) — F? ≤ (l — 4”02T2L- c1)τ) " [F(wo) — F?].
Proof. Starting with (43) we have
F(Wk 一 ηkDDk-1VF(Wk)) ≤ F(Wk) — ηk fɪ 一 ηkτ⅛) IVF(Wk)k2∙
Γ	2α2
From the Armijo backtracking condition (59), we have
F(Wk — ηkDDk-1VF(Wk)) ≤ F(Wk)- Cink VF(Wk)TDDk-1VF(Wk)
≤ F(Wk) — 竿kVF(Wk)k2.
Looking at (60) and (61), it is clear that the Armijo condition is satisfied when
2α2(1 — c1)
Ink ≤ —FT-------.
(60)
(61)
(62)
Thus, any nk that satisfies (62) is guaranteed to satisfy the Armijo condition (59). Since we find nk
using a constant backtracking factor of τ < 1, we have that
nk ≥ 2a2(1 -CI)T.	(63)
ΓL
Therefore, from (60) and by (62) and (63) we have
F(Wk+i) ≤ F(Wk) — nk (1 - n⅛) kVF(Wk)k2
Γ 2α2
≤ F(Wk) — 罕kVF(Wk)k2
≤ F(Wk) — 2α2CR - CI)T kVF(Wk)k2.	(64)
Γ2 L
By strong convexity, We have 2μ(F(w) - F?) ≤ ∣∣VF(w)∣∣2, and thus
F (wk+ι) ≤ F (Wk) — Wo2；：1 — cI)T (F (w) — F ?).	(65)
Γ2 L
Subtracting F? from both sides, and applying (65) recursively yields the desired result.	□
A.11.2 Deterministic Regime - Nonconvex
Theorem A.8. Suppose that Assumptions 4.3, 4.12 and 4.2 hold. Let {Wk} be the iterates generated
by Algorithm 1, where nk is the maximum value in {T-j : j = 0, 1, . . . } satisfying (59) with
0 < C1 < 1, and where W0 is the starting point. Then,
lim kVF(Wk)k =0,
k→∞
(66)
and, moreover, for any T > 1,
1 T-1
T EkVF(Wk)k2
k=0
Proof. We start With (64)
F(Wk+1) ≤ F(Wk)
≤ Γ2L[F(wo) — F] τ→→ 0
-2α2cι(1 — CI)TT
2α2 C1 (1 — C1)T
Γ2L
kVF(Wk)k2.
—
Summing both sides of the above inequality from k = 0 to T — 1,
T-1	T-1	2
X (F (Wk+ι) — F (Wk)) ≤-X 2α 即 - CI,T kVF (Wk )k2.
ΓL
k=o	k=o
28
Published as a conference paper at ICLR 2022
The left-hand-side of the above inequality is a telescopic sum and thus,
T-1
X [F (wk+1) - F(wk)] = F(wT) - F(w0) ≥ Fb - F(w0),
k=0
where the inequality is due to 户 ≤ F(WT) (Assumption 4.12). Using the above, We have
T-1
X kVF(Wk)k2
k=0
≤ Γ2L[F(wo) - F]
-2α2cι(1 — cι)τ
(67)
Taking limits we obtain,
τ-1
VF(Wk)k2 < ∞,
k=0
which implies (66). Dividing (67) by T we conclude
T χ1 kVF(Wk)k2 ≤
k=0
Γ2L[F (wo) — F]
2α2cι(1 — cι)τT
□
A.12 Proofs with decaying step-size
In this section, we provide the analysis with decaying stepsize for both strongly convex and nonconvex
cases in the stochastic regime. The proofs for the following theorems follows from the proofs in
Theorems 4.17 and 4.18, and Theorems 4.7 and 4.9 in Bottou et al. (2018).
A.12.1 Stochastic Regime - Strongly Convex
Theorem A.9. Suppose that Assumptions 4.2, 4.3, 4.4, 4.15 and 4.16 hold., and let F? = F(W?),
where W? is the minimizer of F. Let {Wk} be the iterates generated by Algorithm 1, where ηk is a
sequence of stepsize such that, for all k ≥ 0,
ηk = . Φ 7 for some φ > — and Z > 0 such that no ≤ 二占
Z + k	μ	2Γ L2
Then, for all k ≥ 0, the expected optimality gap satisfies
E[F(Wk) — F?] ≤ -ɪ-
Z+k
where
ν := max n J °0-------, Z(F(WO) — F?
1( φμ — 1)ɑ2
(68)
(69)
(70)
Proof. By taking total expectation from (56), and replacing η with ηk we have:
E[F(Wk+ι) — F*] ≤
(71)
By the assumption η0 ≤ 2αΓLμ2, and the way that ηk is designed, i.e., in a decaying way, we have
2
ηk < ηo ≤ 2rL:2, therefore
C=2—2ηlL2 ≥ 2n J μ —色μL ∖ = 2n jμ —上 ) =ημ
=Γ α2	≥ 2ηk Ir	2ΓL2 α2) η Ir 2Γ>, = Γ .
Thus, we have:
E[F(Wk+ι) — F*] ≤(1 —半)E[F(Wk) — F*] + ηkL0σ2.	(72)
29
Published as a conference paper at ICLR 2022
We do the rest of the analysis of (69) with induction. First, for k = 0, (69) holds by the definition of
ν. Next, assuming (69) holds for k ≥ 0, by (72) we have:
E[F (wk+1) - F ?] ≤
/	φμ ∖
1 -工 I ν + φ2Lσ2
k j k	k2ɑ2
(k - φμ
k2
\
φ2Lσ2
V +—X----
k2α2
ν-
V
≤ 7-
k + 1
(LI V + 1
k2 j	k202
_ - /
{^^^^^^^^^^^^^^^^^^
nonpositive by definition of ν
(73)
□
A.12.2 Stochastic Regime - NONCONVEX
Theorem A.10. Suppose that Assumptions 4.3, 4.12, 4.2, 4.14 and 4.16 hold. Let {wk} be the
iterates generated by Algorithm 1, where ηk is a steplength sequence satisfying
∞∞
ηk = ∞ and	ηk2 < ∞.	(74)
k=0	k=0
and w0 is the starting point. Then, with AT := PkT=-01 ηk,
ιim E∖∑τ=1ηkI∣VF(Wk)k2] < ∞,	(75)
T→∞
and therefore E[-1-PT-(Inkk VF(Wk)k2] -→→ 0.	(76)
AT
Proof. The steplength sequence {ηk} goes to zero due to the second condition in (74). meaning that
2α2
w.l.o.g., We may assume that nk ≤ —-- for all k ≥ 0. By starting from and taking total expectation
2ΓL
from (57), we have
E[F(wk+ι)] — E[F(Wk)] ≤ —nk (J — 骷)E[kVF(Wk)k2] + ^^
≤-翁 E[kVF (wk)k2] + n2γ2L.
2Γ	2α2
Summing both sides of this inequality for k ∈ {0, . . . , T — 1} gives:
F - E[F(wo)] ≤ E[F(WT)] — E[F(wo)] ≤ -2ΓPT=⅛E[∣VF(Wk)∣2] + Yo2P=O喉.
(77)
Multiplying by 2Γ, and rearranging we get
PT=-OLnkE[kVF(Wk)k2] ≤ 2Γ(E[F(Wo)] - F) + mP=0脸.	(78)
The second condition in (74) guarantees that the right-hand side of the above inequality converges to
a finite limit when T increases, therefore, it implies (75). By the first condition in (74), we conclude
that AT → ∞ as T → ∞, which results in (76).	□
30
Published as a conference paper at ICLR 2022
A.13 Convergence Bounds Interpretation
In this section, we provide the interpretation behind the convergence bounds in our theoretical results.
•	In Theorem 4.6 there is the term (1 - β2)ΓQk, which essentially highlights that the
k
parameter β2 controls the size of the neighborhood of optimal solution(s). In other words, if
β2 is very close to 1 (which is the case for our algorithm), the convergence neighborhood
would be very small. Meaning that, with the assumption regarding bounded iterates, our
algorithm converges to a very small neighborhood of optimal solution as β2 is very close to
1 (while in SGD the step-size controls the size of this neighborhood and accordingly, the
step-size needs to go zero in order to converge to the small neighborhood of the optimal
solution(s)). Note this convergence guarantee is for smooth and convex case with adaptive
learning rate.
•	Theorem 4.9 provides a linear convergence rate for the smooth and strongly convex case
with adaptive learning rate. The analysis behind this theorem is based on the Lyapunov
energy that decreases linearly.
•	Remark 4.10 guarantees that the OASIS preconditioning matrix is positive definite and its
eigenvalues are uniformly bounded above and below.
•	Theorems 4.11 and 4.13 provides linear and sublinear rate to the stationary point(s) for
deterministinc strongly convex and nonconvex cases, respectively. The learning rate is fixed
in both mentioned cases, and the convergence rates are similar to those of limited-memory
quasi-Newton approaches (e.g., L-BFGS).
•	Theorems 4.17 and 4.18 provides linear and sublinear rate to the neighborhood of sta-
tionary point(s) (in expectation) for deterministinc strongly convex and nonconvex cases ,
respectively. Please note that we provided another convergence guarantee for the stochastic
strongly convex case (please see Theorem A.5). Again, as the deterministic cases, the
provided convergence guarantees completely match with those of the classical quasi-Newton
methods (such as L-BFGS); the learning rate is fixed in the latter cases. Please note that
these methods (such as OASIS and classical limited-memory quasi-Newton methods) in
theory are not better than GD-type methods. In practice, however, they have shown their
strength.
•	Theorems A.9 and A.10 provides convergence guarantees for the stochastic setting with
decaying learning rate. The mentioned theorems provides the convergence guarantees to the
stationary point(s) (in expectation) for the stochastic strongly convex and nonconvex cases.
•	Theorems A.7 and A.8 provides linear and sublinear convergence rates to the stationary
point(s) for the deterministic strongly convex and nonconvex cases with utilizing linesearch.
B Additional Algorithm Details
B.1 Related Work
As was mentioned in Section 2, we follow the generic iterate update:
Wk+1 = Wk - ηkDkTmk.
Table 3 summarizes the methodologies discussed in Section 2.
B.2 Stochastic OASIS
Here we describe a stochastic variant of OASIS in more detail. As mentioned in Section 3, to estimate
gradient and Hessian diagonal, the choices of sets Ik, Jk ⊂ [n], are independent and correspond to
only a fraction of data. This change results in Algorithm 3.
31
Published as a conference paper at ICLR 2022
Table 3: Summary of Algorithms Discussed in Section 2
Algorithm	mk	Dk		
SGD (Robbins &Monro,1951)	Bimt—i + (I -	-βi)gk	1
Adagrad (Duchi et al., 2011)	gk		yzPk=ι diag(gi θ gi)
RMSProp (Tieleman & Hinton, 2012)	gk		ʌ/ lβ2D k—12 + (1 - β2)diag(gk θ gk )
Adam (Kingma & Ba, 2014)	(i-βi)P3 1-βk	βk-igi	,S(1 - β2) P3 βk-idiag(gi Θ gi) V	1 - βk
AdaHessian (Yao et al., 2020)	(i-βι) Pk=i 1 - β1	βk-igi	S (i-β2 )Pk=ι βk-Vr V	1- βk	
OASIS	gk		∣β2Dk-1 + (1 - β2 )vk Ia
* Vi = diag(zi Θ V2F(Wi)Zi) and	zi 〜RademaCher (0.5) ∀i		≥ 1,
**(∣A∣α)ii = max{∣A∣ii,α}
***Dk = β2Dk-i + (1 - β2)vk
Algorithm 3 Stochastic OASIS
Input: w0, η0, Ik, D0, θ0 = +∞, β2, α
1:	w1 = w0 - no DD 0-1VF (wo)ik
2:	for k = 1, 2, . . . do
3:	Calculate Dk = β2Dk-1 + (1 - β2) diag(zk	V2FJk (wk)zk)
4:	Calculate Dk by setting (Dk)i,i = max{|Dk |i,i, α}, ∀i ∈ [d]
5:	Update nk = min{ P1 + θk-1nk-1, 2kVFik (wk)-VFik (Dkk-ι)k^^ }
6：	Set Wk + 1 = Wk - nkDkTVFIk (Wk)
7:	Set θk = nk-
ηk-1
8:	end for
Additionally, in order to compare the performance of our preconditioner schema independent of
the adaptive learning-rate rule, we also consider variants of OASIS with fixed η. We explore two
modifications, denoted as “Fixed LR” and “Momentum” in Section 5. “Fixed LR” is obtained
from Algorithm 3 by simply having a fixed scalar η for all iterations, which results in Algorithm 4.
“Momentum” is obtained from “Fixed LR” by considering a simple form of first-order momentum
with a parameter β1, and this results in Algorithm 5. In Section C.5, we show that OASIS is robust
with respect to the different choices of learning rate, obtaining a narrow spectrum of changes.
Algorithm 4 OASIS- Fixed LR
Input: wo, n, Ik, Do, β2, α
1： Wi = Wo - nDOTVFIk (wo)
2: for k = 1, 2, . . . do
3： Calculate Dk = β2Dk-1 +(1 -β2) diag(zk V2 FJk (Wk)zk)
4： Calculate Dk by setting (Dk)i,i = max{|Dk|i,i, α}, ∀i ∈ [d]
5：	Set Wk+1 = Wk - n-Dk-1VFIk (Wk)
6： end for
32
Published as a conference paper at ICLR 2022
Algorithm 5 OASIS- Momentum
Input： wo, η, Ik, Do, βι, β2 α
1:	Set mo = VFIk (wo)
2:	w1 = wo - ηDo-1mo
3:	for k = 1, 2, . . . do
4:	Calculate Dk = β2Dk-1 +(1-β2) diag(zk V2 FJk (wk)zk)
5:	Calculate Dk by setting (Dk)i,i = max{|Dk|i,i, α}, ∀i ∈ [d]
6:	Calculate mk = β1mk-1 + (1 - β1)VFIk (wk)
7:	Set Wk+1 = Wk - ηDk-1mk
8:	end for
In our experiments, we used a biased version of the algorithm with Ik = Jk. One of the benefits of
this choice is to compute the Hessian-vector product efficiently. By reusing the computed gradient,
the overhead of computing gradients with respect to different samples is significantly reduced (see
Section B.3).
The final remark is related to a strategy to obtain Do, which is required at the start of OASIS
Algorithm. One option is to do warmstarting, i.e., spend some time before training in order to sample
some number of Hutchinson’s estimates. The second option is to use bias corrected rule for Dk
similar to the one used in Adam and AdaHessian
Dk = β2Dk-1 + (1 - β2)diag(zk	V2FJk (wk)zk),
Dk
CCOr —	jk
Dk = 1 - βk+1,
which allows to obtain Do by defining D-1 to be a zero diagonal.
B.3 Efficient Hessian-vector Computation
In the OASIS Algorithm, similar to AdaHessian (Yao et al., 2020) methodology, the calculation of
the Hessian-vector product in Hutchinson’s method is the main overhead. In this section, we present
how this product can be calculated efficiently. First, lets focus on two popular machine learning
problems: (i) logistic regression; and (ii) non-linear least squares problems. Then, we show the
efficient calculation of this product in deep learning problems.
Logistic Regression. One can note that We can show the '2-regularized logistic regression as:
F(W) = 1 (1 n * log(1 + e-YOXTW)) + 2∣∣w∣∣2,	(79)
where In is the vector of ones with size 1 X n, * is the standard multiplication operator between two
matrices, and X is the feature matrix and Y is the label matrix. Further, the Hessian-vector product
for logistic regression problems can be calculated as follows (for any vector v ∈ Rd):
(
∖
V2F(w) * V =—
n
xt *
Y O Y Oe-Y0XTw
(1 + e-Y ΘXTw)2
+ λv.
(80)
O X * V
F
|
I
∖
3
}
/
The above calculation shows the order of computations in order to compute the Hessian-vector
product without constructing the Hessian explicitly.
Non-linear Least Squares. The non-linear least squares problems can be written as following:
F (W) = 2> - φ(XT W) K
(81)
33
Published as a conference paper at ICLR 2022
The Hessian-vector product for the above problem can be efficiently calculated as:
▽2F(W) * V =
/
、
1
n
-XT * hφ(XTw)(1-φ(XTw))(Y -2(1+Y)φ(XTw)+3φ(XTw)φ(XTw))i
Θ X * v
ɪ
j,
(82)
I
-^{2z
L
I
\
}
/
Deep Learning. In general, the Hessian-vector product can be efficiently calculated by:
▽2F(w) * V
∂2F (W)
∂w∂w
*v
∂ ∂F(w)T
菰 (
(83)
As is clear from (83), two rounds of back-propagation are needed. In fact, the round regarding the
gradient evaluation is already calculated in the corresponding iteration; and thus, one extra round of
back-propagation is needed in order to evaluate the Hessian-vector product. This means that the extra
cost for the above calculation is almost equivalent to one gradient evaluation.
34
Published as a conference paper at ICLR 2022
Table 5: Deep Neural Networks used in the experiments.
Data	Network	# Train	# Test	# Classes	d
MNIST	Net DNN	60K	10K	10	21.8K
CIFAR10	ReSNet20	50K	10K	10		272K
	ResNet32	50K	10K	10	467K
CIFAR10O	ReSNet18	50K	10K	100		11.22M
C Details of Experiments
C.1 Table of Algorithms
Table 4 summarizes the algorithms implemented in Section 5.
Algorithm	DeScription and Reference
SGD	Stochastic gradient method (Robbins &Monro, 1951)
Adam	Adam method (Kingma & Ba, 2014)
AdamW	Adam with decoupled weight decay (Loshchilov & Hutter, 2017)
AdGD	Adaptive Gradient Descent (Malitsky & Mishchenko, 2020)
AdaHessian	AdaHessian method (Yao et al., 2020)
OASIS-Adaptive LR	Our proposed method with adaptive learning rate
OASIS-Fixed LR	Our proposed method with fixed learning rate
OASIS-Momentum	Our proposed method with momentum
Table 4: Description of implemented algorithms
To display the optimality gap for logistic regression problems, we used Trust Region (TR) New-
ton Conjugate Gradient (Newton-CG) method (Nocedal & Wright, 2006) to find a w such that
∣∣VF(w)k2 < 10-19. Hereafter, we denote F(W) - F(w*) the optimality gap, where we refer w* to
the solution found by TR Newton-CG.
C.2 Problem Details
Some metrics for the image classification problems are given in Table 5.
The number of parameters for ResNet architectures is particularly important, showcasing that OASIS
is able to operate in very high-dimensional problems, contrary to the widespread belief that methods
using second-order information are fundamentally limited by the dimensionality of the parame-
ter space.
C.3 B inary Classification
In Section 5 of the main paper, we studied the empirical performance of OASIS on binary classifi-
cation problems in a deterministic setting, and compared it with AdGD and AdaHessian. Here, we
present the extended details on the experiment.
C.3. 1 Problem and Data
As a common practice for the empirical research of the optimization algorithms, LIBSVM datasets6
are chosen for the exercise. Specifically, we chose 5 popular binary class datasets, ijcnn1, rcv1,
news20, covtype and real-sim. Table 6 summarizes the basic statistics of the datasets.
6Datasets are available at https://www.csie.ntu.edu.tw/ ~cjlin∕libsvmtools∕
datasets/.
35
Published as a conference paper at ICLR 2022
Table 6: SummaryofDatasets.
Dataset	# feature	n (# Train)	# Test	% Sparsity
ijcnn11	22	^^49,990^^	91,701	-40.91
rcv11	47,236	20,242	677,399	99.85
news20 2	1,355,191	14,997	4,999	99.97
Covtype2	54		435,759	145,253	77.88
real-sim 2	20,958	-54,231…	18,078	99.76
1 dataset has default training/testing samples.
2 dataset is randomly split by 75%-training & 25%-testing.
Let (xi, yi) be a training sample indexed by i ∈ [n] := {1, 2, ..., n}, where xi ∈ Rd is a feature
vector and yi ∈ {-1, +1} is a label. The loss functions are defined in the forms
Tλ
fi(w) = log(1 + e-yi%w) + 2 kwk2,	(84)
fi(w) = "- 1 + e1-xTw )，	(85)
where (84) is a regularized logistic regression of a particular choice of λ > 0 (and we used λ ∈
{0.1/n, 1/n, 10/n} in the experiment), and hence a strongly convex function; and (85) is a non-linear
least square loss, which is apparently non-convex.
The problem we aimed to solve is then defined in the form
min I F(W) ：= - X fi(w) J ,
w∈Rd	n
and We denote w* the global optimizer of (86) for logistic regression.
(86)
C.3.2 Configuration of Algorithm
To better evaluate the performance of the algorithms, We configured OASIS, AdGD and AdaHessian
With different choices of parameters.
•	OASIS Was configured With different values of β2 and α, Where β2 can be any values in
{0.95, 0.99, 0.995, 0.999} and α can be any value in the set {10-3, 10-5, 10-7} (see Section
C.5 Where OASIS has a narroW spectrum of changes With respect to different values of α and β2).
In addition, We adopted a Warmstarting approach to evaluate the diagonal of Hessian at the starting
point w0 .
•	AdGD Was configured With 12 different values of the initial learning rate, i.e., η0 ∈
{10-11, 10-10,..., 0.1, 1.0}.
•	AdaHessian Was configured With different values of the fixed learning rate: for logistic regression,
We used 12 values betWeen 0.1 and 5; for non-linear least square, We used 0.01, 0.05, 0.1, 0.5, 1.0
and 2.0.
To take into account the randomness of the performance, We used 10 distinct random seeds to initialize
w0 for each algorithm, dataset and problem.
C.3.3 Extended Experimental Results
This section presents the extended results on our numerical experiments. For the Whole experiments
in this paper, We ran each method 10 times starting from different initial points.
For logistic regression, We shoW the evolution of the optimality gap in Figure 6 and the ending gap
in Figure 7; the evolution of the testing accuracy and the maximum achieved ones are shoWn in Figure
8 and Figure 9 respectively.
For non-linear least square, the evolution of the objective and its ending values are shoWn in
Figure 10; the evolution of the testing accuracy along With the maximum achived ones are shoWn in
Figure 11.
36
Published as a conference paper at ICLR 2022
klɪir
10°
io-9
10-*
ICr9.
10-12
O IOO 200 300 400
Number OfEffective Passes
0	100 200 300 400
Number OfEffectIve Passes
0	100 200 300 400
Number OfEffective Passes
0 IOO 200 300 400
Number OfEffective Passes
0	100 200 300 400
Number OfEffectIve Passes
Figure 6: Evolution of the optimality gaps of OASIS, AdGD and AdaHessian for '2-regularized
Logistic regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to
right: ijcnn1, rcv1, news20, covtype and real-sim.
rcvl
ijcnnl
new≡20
covtype
real-sim
AdG □ AdaHesS⅛∏ OAStS
AdGD Ad. HessAn OAStS
AdGD AdaHess∣4∏ OASIS
AdGD AdaHcss∣4∏ OASIS
AdG □ AdaHess ⅛∏ OAStS
AdG □ AdaHesS⅛∏ OAStS
AdGD Ad. HessAn OAStS
AdGD AdaHess∣4∏ OASIS
AdGD ASHCsSI" OASIS
AdG □ AdaHess ⅛∏ OASts
AdGD AdaHessIan OAStS
AdGD AdaHesslan OAStS
AdGO AdsHesslan OAStS
AdGD AdsHessIsn OASIS
AdGD AdaHessIan OASIS
Figure 7: Ending optimality gaps of OASIS, AdGD and AdaHessian for '2-regularized Logistic
regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to right:
ijcnn1, rcv1, news20, covtype and real-sim.
37
Published as a conference paper at ICLR 2022
0.9-
0.8-
⅛ɑ8
¥
6 0.7
0.5
D.S
0.€
D.S
10.7
I- 0.6
100	200	300	400
Number OfEffectIve Passes
⅛ɑ8
0
7-
0.6-
D.5
Number of Effective Passes
0.85-
0.80-
0.75-
covtype
0.75
0.70-
0.65-
0.60-
0.55-
0.50
0.75-
0.70-
0.65-
0.60-
0.55-
0.7。-
0.65-
0.60-
0.55-
0.50
0.50
0	100 200 300 400
Number OfEffectIve Passes
Number of Effective Passes
Figure 8: Evolution of the testing accuracy of OASIS, AdGD and AdaHessian for '2-regularized
Logistic regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to
right: ijcnn1, rcv1, news20, covtype and real-sim.
Figure 9: Maximum testing accuracy of OASIS, AdGD and AdaHessian for '2-regularized Logistic
regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to right:
ijcnn1, rcv1, news20, covtype and real-sim.
covtype
0.75
0.74
0.73
0.72
Q.70
38
Published as a conference paper at ICLR 2022
Number of Effective Passes
Number of Effective Passes
Number of Effective Passes
Number of Effective Passes
rea∣-sim
1.00 ---------
0.95
0.90
0.85
0.80
0.75
0.70	.
MGD AdsHessIsn OASIS
Figure 10: Evolution of the objective F(w) (top row) and the ending F(w) (bottom row) of OASIS,
AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20, covtype
and real-sim.
ijcnnl
δ, 0.900-
自 0.825-
I 0.875-
g, 0.850-
AdG 口
----Mal-Iesslan
——OASIS
0 100 200 300 400
Number of Effective Passes
1.0
0.4
real-sim
0.8
0.6
6 100 200 300 400
Number OfEffective Passes
ijcnnl
rcvl
news20
real-sim
covtype
AdGO AdaHessIan OASB
AdGD AdaHesstan OASS
AdGD AdaHessIan OASIS
AdGD AdaHesstan OASB
AdGD AdaHessIan OASIS
Figure 11: Evolution of the testing accuracy (top row) and the maximum accuracy (bottom row) of
OASIS, AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20,
covtype and real-sim.
39
Published as a conference paper at ICLR 2022
ResNet20 on
Ooooooo
9 8 7 6 5 4 3
ResNet20 on CIFAR10 Without Weight Decay
Ooooooo
9 8 7 6 5 4 3
Auejnuofsφl
O 20 40 60 80 IOO 120 140 160 O 20 40 60 80 IOO 120 140 160
Figure 12: ResNet20 on CIFAR10 with and without weight decay. Final accuracy results can be
found in Table 7.
C.4 Image Classification
In the following sections, we provide the results on standard bench-marking neural network training
tasks: CIFAR10, CIFAR100, and MNIST.
C.4.1 CIFAR 1 0
In our experiments, we compared the performance of the algorithm described in Table 4 in 2 settings
- with and without weight decay. The procedure for choosing parameter values differs slightly for
these two cases, so they are presented separately. Implementations of ResNet20/32 architectures are
taken from AdaHessian repository.7
Analogously to (Malitsky & Mishchenko, 2020), we also modify terms in the update formula for ηk
kwk-wk-1 kD k	]
ηk = min{ V1 + θk-ιηk-1, 2gF(wk)7F(wk-ι)k*, }.
Dk
Specifically, We incorporate parameter Y into，1 + γθk-ι and use more optimistic bound of 1/Lk
instead of 1/2Lk, which results in a slightly modified rule
一b、一λ------------	kwk-wk-1kDl.	】
ηk = min{ V1 + γθk-ιηk-1, kVF(wk)-VF(wk-ι)k*. }.
Dk
In order to find the best value for γ, We include it into hyperparameter tuning procedure, ranging the
values in the set {1, 0.1, 0.05, 0.02, 0.01} (see Section C.5 Where OASIS has a narroW spectrum of
changes With respect to different values of γ).
Moreover, We used a learning-rate decaying strategy as considered in (Yao et al., 2020) to have the
same and consistent settings. In the aforementioned setting, ηk Would be decreased by a multiplier
on some specific epochs common for every algorithms (the epochs 80 and 120).
7https://github.com/amirgholami/adahessian
40
Published as a conference paper at ICLR 2022
ReSNet32 On ClFARlO WithOUt W函ht Deeay
0	20 40 60 80 100 120 140 160
Epochs	Epochs
Epochs	Epochs
Figure 13: ResNet32 on CIFAR10 with and without weight decay. Final accuracy results can be
found in Table 7.
Table 7: Results of ResNet20/32 on CIFAR10 with and without weight decay. Variant of our method
with fixed learning rate beats or is on par with others in the weight decay setting; while OASIS with
adaptive learning rate produces consistent results showing a close second best performance without
any learning rate tuning (for without weight decay setting).
Setting	ResNet20, WD	ResNet20, no WD	ResNet32, WD	ResNet32, no WD
SGD	92.02 ± 0.22	89.92 ± 0.16	92.85 ± 0.12	90.55 ± 0.21
Adam	90.46 ± 0.22	90.10 ± 0.19	91.30 ± 0.15	91.03 ± 0.37
AdamW	91.99 ± 0.17	90.25 ± 0.14	92.58 ± 0.25	91.10 ± 0.16
AdaHessian	92.03 ± 0.10	91.22 ± 0.24	92.71 ± 0.26	92.19 ± 0.14
OASIS-Adaptive LR	91.20 ± 0.20	91.19 ± 0.24	92.61 ± 0.22	91.97 ± 0.14
OASIS-Fixed LR	91.96 ± 0.21	89.94 ± 0.16	93.01 ± 0.09	90.88 ± 0.21
OASIS-Momentum	92.01 ± 0.19	90.23 ± 0.14	92.77 ± 0.18	91.11 ± 0.25
WD := Weight decay
41
Published as a conference paper at ICLR 2022
With weight decay: It is important to mention that due to variance in stochastic setting, the learning
rate needs to decay (regardless of the learning rate rule is adaptive or fixed) to get convergence.
One can apply adaptive learning rate rule without decaying by using variance-reduced methods
(future work). Experimental setup is generally taken to be similar to the one used in (Yao et al.,
2020). Parameter values for SGD, Adam, AdamW and AdaHessian are taken from the same
source, meaning that learning rate is set to be 0.1/0.001/0.01/0.15 and, where they are used,
β1 and β2 are set to be 0.9 and 0.999. For OASIS, due to a significantly different form of the
preconditioner aggregation, we conducted a small scale search for the best value of β2 , choosing
from the set {0.999, 0.99, 0.98, 0.97, 0.96, 0.95}, which produced values of 0.99/0.95/0.98 for
“Fixed LR,” “Momentum” and adaptive variants correspondingly for ResNet20 and 0.99/0.99/0.98
for “Fixed LR,” “Momentum” and adaptive variants correspondingly for ResNet32, while β1 for
momentum variant is still set to 0.9. Additionally, in the experiments with fixed learning rate, it is
tuned for each architecture, producing values 0.1/0.1 for ResNet20 and 0.1/0.1 for ResNet32 for
“Fixed LR” and “Momentum” variants of OASIS correspondingly. For the adaptive variant, γ is set to
0.1/1.0 for ResNet20/32. α is set to 0.1 for all OASIS variants. We train all methods for 160 epochs
and for all methods. With fixed learning rate we employ identical scheduling, reducing step size by
a factor of 10 at epochs 80 and 120. For the adaptive variant scheduler analogue is implemented,
multiplying step size by ρ at epochs 80 and 120, where ρ is set to be 0.1/0.5 for ResNet20/32. Weight
decay value for all optimizers with fixed learning rate is 0.0005 and decoupling for OASIS is done
similarly to AdamW and AdaHessian. For adaptive OASIS weight decay is set to 0.001 without
decoupling. Batch size for all optimizers is 256.
Without weight decay: In this setting we tune learning rate for SGD, Adam, AdamW and AdaHessian,
obtaining the values 0.15/0.005/0.005/0.25 and 0.125/0.01/0.01/0.25 for ResNet20/32 accord-
ingly. Where relevant, β1, β2 are taken to be 0.9, 0.999. For OASIS we still try to tune β2, which
produces values of 0.999 for ResNet20 and adaptive case of ResNet32 and 0.99 for “Fixed LR,”
“Momentum” in the case of ResNet32; for the momentum variant β1 = 0.9. Learning rates are
0.025/0.05 for ResNet20 and 0.025/0.1 for ResNet32 for “Fixed LR” and “Momentum” variants of
OASIS correspondingly. For adaptive variant γ is set to 0.01/0.01 for ResNet20/32. α is set to 0.1
for adaptive OASIS variant, while hyperoptimization showed, that for “Fixed LR” and “Momentum”
a value of 0.01 can be used. We train all methods for 160 epochs and for all methods with fixed
learning rate we employ identical scheduling, reducing step size by a factor of 10 at epochs 80 and
120. For the adaptive variant scheduler analogue is implemented, multiplying step size by ρ at epochs
80 and 120, where ρ is set to be 0.1/0.1 for ResNet20/32. Batch size for all optimizers is 256.
Finally, all of the results presented are based on 10 runs with different seeds, where parameters are
chosen amongst the best runs produced at the preceding tuning phase.
All available results can be seen in Table 7 and Figures 12 and 13. We ran our experiments on an
NVIDIA V100 GPU.
C.4.2 CIFAR 1 00
Analogously to CIFAR10, we do experiments in settings with and without weight decay. In both cases
we took best learning rates from the corresponding CIFAR10 experiment with ResNet20 architecture
without any additional tuning, which results in 0.1 for all. β2 is set to 0.99 in the case with weight
decay and to 0.999 in the case without it. ResNet18 architecture implementation is taken from a
github repository.8 We train all methods for 200 epochs. For all optimizers learning rate is decreased
(or effectively decreased in case of fully adaptive OASIS) by a factor of 5 at epochs 60, 120 and 160.
All of the results presented are based off of 10 runs with different seeds. All available results can be
seen in Table 8 and Figure 14. We ran our experiments on an NVIDIA V100 GPU.
C.4.3 MNIST
Similar to the previous results, we ran the methods with 10 different random seeds. We considered
Net DNN which has 2 convolutional layers and 2 fully connected layers with ReLu non-linearity,
mentioned in the Table 5. In order to tune the hyperparameters for other algorithms, we considered the
set of learning rates {100, 10-1, 10-2, 10-3}. For OASIS, we used the set of {10-1, 10-2} and the
8https://github.com/uoguelph-mlrg/Cutout
42
Published as a conference paper at ICLR 2022
ResNetl8 on CIFAR100 With Weight Decay
Ooooooo
7 6 5 4 3 2 1
0^^25^^50^^75 100 125 150 175 200
Epochs
ResNetlB on CIFAR100 Without Weight Decay
Ooooooo
7 6 5 4 3 2 1
O 25 50 75 IOO 125 150 175 200
Epochs
Figure 14: ResNet18 on CIFAR100 with and without weight decay. Final accuracy results can be
found in Table 8.
Epochs
Table 8: Results of ResNet18 on CIFAR100. Simply transferring parameter values from similar
task with ResNet20 on CIFAR10 predictably damages performance of optimizers compared to their
heavily tuned versions. Notably, in the setting without weight decay performance of SGD and Adam
became unstable for different initializations, while adaptive variant of OASIS produces behaviour
robust to the choice of the random seed.___________________________________________________
Setting	ReSNet18, WD	ResNet18, no WD
SGD	76.57 ± 0.24	70.50 ± 1.51
Adam		73.40 ± 0.31	67.40 ± 0.91
AdamW	72.51 ±' 0.76	67.96 ± 0.69
AdaHessian	75.71 ±' 0.47	70.16 ± 0.82
OASIS-Adaptive LR	76.93 ± 0.22	74.13 ± 0.20
OASIS-Fixed LR	76.28 ± 0.21	70.18 ± 0.76
OASIS-Momentum	76.89 ± 0.34	70.93 ± 0.77
WD := Weight decay
43
Published as a conference paper at ICLR 2022
95
90
85
80
75
70
Net DNN on MNIST With Weight Decay
2	4	6	8	10
86
Net DNN on MNIST Without Weight Decay
8 6 4 2 0 8
9 9 9 9 9 8
2	4	6	8	10
Epochs
Epochs
Figure 15: Net DNN on MNIST with and without weight decay. Final accuracy results can be found
in Table 9.
Table 9: Results of Net DNN on MNIST. Gradient momentum usage seems improve results on short
trajectories for all methods, including OASIS.
Setting	NetDNN,WD	Net DNN, no WD
SGD	98.37 ± 0.45	98.86 ± 0.18
Adam		98.92 ± 0.14	98.90 ± 0.13
AdamW	98.76 ± 0.13	98.92 ± 0.13
AdaHessian	98.82 ± 0.16	98.86 ± 0.16
OASIS-Adaptive LR	97.93 ± 0.27	97.95 ± 0.29
OASIS-Fixed LR	97.24 ± 3.97	98.09 ± 1.36
OASIS-Momentum	98.78 ± 0.22	98.89 ± 0.10
WD := Weight decay
set for truncation parameter α ∈ {10-1, 10-2}. As is clear from the results shown in Figure 15 and
Table 9, OASIS with momentum has the best performance in terms of training (lowest loss function),
and and it is comparable with the best test accuracy for the cases with and without weight decay.
It is worth mentioning that OASIS with adaptive learning rate got satisfactory results with lower
number of parameters, which require tuning, which is vividly important, especially in comparison
to first-order methods that are sensitive to the choice of learning rate. We ran our experiments on a
Tesla K80 GPU.
C.5 Sensitivity Analysis of OASIS
It is worth noting that we designed and analyzed OASIS for the deterministic setting with adaptive
learning rate. It is safe to state that no sensitive tuning is required in the deterministic setting (the
learning rate is updated adaptively, and the performance of OASIS is completely robust even if the
rest of the hyperparameters are not perfectly hand-tuned); see Figures 16 and 17.
44
Published as a conference paper at ICLR 2022
IO0
10-1
10-2
1 IO-3
I
ɪlθ-4
10-5
10-6
10-7
Number OfEffective Passes
LI (SL
10-1
10~2
10-3
10-4
10-5
10-6
10-7
covtype
Figure 16: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Logistic regression.
45
Published as a conference paper at ICLR 2022
(M)d	(明
O IOO 200	300	400
Number OfEffective Passes
rcvl
Figure 17: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Non-linear Least Square.
Number OfEffective Passes
46
Published as a conference paper at ICLR 2022
100
Ooooo
8 6 4 2
ADeJnDDfSR
Sensitivity Analysis (LR) - OASIS
OASIS l∏0.005
OASIS I∏O.O1
OASIS I∏O.O25
OASIS I∏O.O5
OASIS lr:0.0625
OASIS I∏O.1
OASIS I∏O.125
OASIS l∏0.2
OAS∣SJ∏0.3
OASIS l∏0.4
OASIS l∏0.5
OASIS lπl.0
OASIS lr:2.0
100
Sensitivity Analysis (LR)- SGD
'O
O
5
'O
O
5
Ooooo
8 6 4 2
Aejn4 as ①一
sgdJπ0.005
sgdjπθ.θl
sgd_lr:0.025
—sgdJπ0.05
sgd_lr:0.0625
≡^≡ sgdjπθ.l
sgdJr=0.125
sgdjπθ.2
sgdjπθ.3
sgd_lr:0.4
—" sgdjπθ.5
sgdJπl.0
sgd_lr:2.0
O
50
B
Oo
O
5
Epochs	Epochs
Figure 18:	Sensitivity of OASIS vs. SGD w.r.t. learning rate η, CIFAR10 on ResNet20.
We also provide sensitivity analysis for the image classification tasks. These problems are in stochastic
setting, and we need to tune some of the hyperparameters in OASIS. As is clear from the following
figures, OASIS is robust with respect to different settings of hyperparameters, and the spectrum of
changes is narrow enough; implying that even if the hyperparameters aren’t properly tuned, we still
get acceptable results that are comparable to other state-of-the-art first- and second-order approaches
with less tuning efforts. Figure 18 shows that OASIS is robust for different values of learning rate (left
figure), while SGD is completely sensitive with respect to learning rate choices. One possible reason
for this is because OASIS uses well-scaled preconditioning, which scales each gradient component
with regard to the local curvature at that dimension, whereas SGD treats all components equally.
Furthermore, in order to use an adaptive learning rate in the stochastic setting, an extra hyperparameter,
γ, is used in OASIS (similar to (Malitsky & Mishchenko, 2020)). Figure 19 shows that OASIS
is also robust with respect to different values of γ (unlike the study in (Malitsky & Mishchenko,
2020)). The aforementioned figures are for CIFAR10 dataset on the ResNet20 architecture; the
same behaviour is observed for the other network architectures.
Finally, we show here that the performance of OASIS is also robust with respect to different values
of β2. Figure 20 shows the robustness of OASIS across a range of β2 values. This figure is for
CIFAR100 dataset on the ResNet18 architecture.
47
Published as a conference paper at ICLR 2022
IOO
Sensitivity Analysis (y)- OASIS

80
60
40
20
0
----OASIS_y:0.01
OASIS_y:0.02
----OASIS_y:0.05
----OASIS_y:0.1
----OASIS_y:1.0
0
50	100
Epochs
150
Figure 19:	Sensitivity of OASIS w.r.t. γ, CIFAR10 on ResNet20.

ClFARlOO-ResNetlS
100
80
60
40
20
00	100	200
Epochs
Figure 20:	Sensitivity of OASIS w.r.t. β2, CIFAR100 on ResNet18.
48