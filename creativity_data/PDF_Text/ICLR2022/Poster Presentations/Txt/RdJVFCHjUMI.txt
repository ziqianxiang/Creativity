Published as a conference paper at ICLR 2022
An Explanation of In-context Learning as
Implicit Bayesian Inference
Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma
Stanford University
{xie,aditir,pliang,tengyuma}@cs.stanford.edu
Ab stract
Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, itis unclear
what enables in-context learning. In this paper, we study how in-context learning
can emerge when pretraining documents have long-range coherence. Here, the
LM must infer a latent document-level concept to generate coherent next tokens
during pretraining. At test time, in-context learning occurs when the LM also
infers a shared latent concept between examples in a prompt. We prove when this
occurs despite a distribution mismatch between prompts and pretraining data in
a setting where the pretraining distribution is a mixture of HMMs. In contrast to
messy large-scale datasets used to train LMs capable of in-context learning, we
generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs
both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit
large-scale real-world phenomena including improved in-context performance with
model scaling (despite the same pretraining loss), sensitivity to example order, and
instances where zero-shot is better than few-shot in-context learning.
1	Introduction
Large language models (LMs) such as GPT-3 (Brown et al., 2020; Lieber et al., 2021; Wang &
Komatsuzaki, 2021; Radford et al., 2019) are pretrained on massive text corpora to predict the next
word given previous words. They demonstrate the surprising ability to do in-context learning, where an
LM “learns” to do a task simply by conditioning on a prompt containing input-output pairs, achieving
SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18%
and 3% over previous SOTA (Brown et al., 2020)). For example, consider the task of predicting
nationalities from names. A prompt (Figure 1) is constructed by concatenating independent “training”
examples (e.g., “Albert Einstein was German”) followed by a “test example” (“Marie Curie was”).
Conditioning on this prompt, GPT-3 places the largest probability on the correct output
p(“Polish” | “Albert Einstein was German \n Mahatma Gandhi was Indian \n Marie Curie was”)
by inferring the task from examples. Intruigingly, GPT-3 was not explicitly pretrained to learn from
examples, and the distribution of prompts (which concatenate independent examples) is quite different
from natural language. Our understanding of in-context learning is limited since (i) real pretraining
data is messy and (ii) in-context learning has so far required large-scale datasets and models.
In this paper, we introduce a simple pretraining distribution where in-context learning emerges.
To generate a document, we first draw a latent concept θ, which parameterizes the transitions of a
Hidden Markov Model (HMM) (Baum & Petrie, 1966), then sample a sequence of tokens from the
HMM (Figure 9). This latent variable structure is common in topic models such as LDA (Blei et al.,
2003; Gruber et al., 2007). During pretraining, the LM must infer the latent concept across multiple
sentences to generate coherent continuations. When conditioning on a prompt, in-context learning
occurs when the LM also infers a shared prompt concept across examples to make a prediction. We
assume the LM fits the pretraining distribution p exactly with enough data and expressivity, so that
the question of in-context learning becomes characterizing the conditional distribution of completions
1The code, data, and experiments are located on GitHub and CodaLab.
1
Published as a conference paper at ICLR 2022
Figure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data.
During pretraining, the language model (LM) implicitly learns to infer a latent concept (e.g., wiki
bios, which typically transition between name (Albert Einstein) → nationality (German) → occupation
(physicist) → ...) shared across sentences in a document. Although prompts are unnatural sequences
that concatenate independent examples, in-context learning occurs if the LM can still infer the shared
concept across examples to do the task (name → nationality, which is part of wiki bios).
given prompts p(output|prompt) under the pretraining distribution, where the prompt is generated
from a different distribution pprompt. This conditional distribution, which is the posterior predictive
distribution, marginalizes out the latent concepts:
p(output|prompt) =
/
concept
p(output|concept,prompt)p(concept|prompt)d(concept).
(1)
If p(concept|prompt) concentrates on the prompt concept with more examples, then the LM learns
via marginalization by “selecting” the prompt concept. Thus, in-context learning can be viewed as
the LM implicitly performing Bayesian inference.
The main challenge is that prompts are sampled from a different distribution than the pretraining
distribution. The canonical Bayesian asymptotic tool is the Bernstein-von Mises theorem (van der
Vaart, 1998; Kleijn & van der Vaart, 2012; Gunst & Shcherbakova, 2008), which asserts (under regularity
conditions) that the posterior distribution of a latent variable concentrates on the maximum likelihood
estimate. However, Bernstein-von Mises typically assumes observations are independent and/or drawn
from the same distribution as the model, both of which are not satisfied. We prove that despite the
distribution mismatch, the asymptotic prediction error of in-context learning is optimal when the signal
about the latent concept in each prompt example is larger than the error due to the distribution mismatch.
Additionally, we prove that the in-context learning error decreases with the length of each example—
thus, information in the inputs, not just the input-output mapping, can be useful for in-context learning.
As a companion to this theory, we created the Generative IN-Context learning dataset (GINC), which
is a small-scale synthetic dataset for studying in-context learning. We find that both Transform-
ers (Vaswani et al., 2017) and LSTMs (Hochreiter & Schmidhuber, 1997) trained on GINC exhibit in-
context learning. We verify intuitions from the theory, showing that the accuracy of in-context learning
improves with the number of examples and example length. Ablations of the GINC dataset show that the
latent concept structure in the pretraining distribution is crucial to the emergence of in-context learning.
The experiments also bring up open questions which go beyond our theory, which only studies the
pretraining distribution. We find that scaling up the number of model parameters steadily improves
the in-context accuracy despite achieving the same pretraining loss, showing that larger models may
improve in-context learning beyond increasing the capacity for memorizing the training data better.
Previously observed in-context learning phenomena such as sensitivity to example ordering (Zhao
et al., 2021) and the existence of settings where zero-shot is better than one/few-shot learning (Brown
et al., 2020) are also mirrored in GINC.
2	In-context learning setting
Pretraining distribution. In our framework, a latent concept θ from a family of concepts Θ defines
a distribution over observed tokens o from a vocabulary O. To generate a document, we first sample a
2
Published as a conference paper at ICLR 2022
concept from a prior p(θ) and then sample the document given the concept. Each pretraining document
is a length T sequence:
p(o1,...
,oT)=
θ∈Θ
p(oι,...,oτ ∣θ)p(θ)dθ.
(2)
We assumep(oι,…,oτ|θ) is defined by a Hidden Markov Model (HMM). The concept θ determines
the transition probability matrix of the HMM hidden states h1,...,hT from a hidden state set H.
Prompt distribution. The prompt distribution pprompt generates prompts for in-context learning.
A prompt is a concatenation ofn independent training examples and 1 test input xtest, which are all
conditioned on a shared prompt concept θ*. The goal is to predict the test output ytest by predicting
the next token conditioned on the prompt.
A prompt example is composed of an input token sequence x (e.g., Albert Einstein was) followed
by an output token y (e.g., German). In particular, the i-th training example Oi consists ofan input
xi =Oi[1 : k- 1] (the first k- 1 tokens) followed by an output token yi = Oi [k] at the end2. The i-th
training example is independently generated as follows:
1.	Generate a start hidden state hsitart from a prompt start distribution pprompt.
2.	Given hstart, generate the example sequence Oi = [χi,yi] from p(θ∕hstart,θ*), the pretraining
distribution conditioned on a prompt concept θ*.
The test input xtest = xn+1 is sampled similarly. Between each example, there is a special delimiter token
odelim. The prompt consists ofa sequence of training examples (Sn) followed by the test example xtest:
[Sn,xtest] = [x1,y1,o	,x2,y2,o ' ,…,xn,yn,o ' ,xtest] 〜pprompt.	(3)
Mismatch between prompt and pretraining distributions. Since transitions between independent
examples can be unnatural, the prompts are low probability sequences under the pretraining distribution.
We provide a simple illustration using the names to nationalities example. Suppose that wiki bio
documents in the pretraining data typically transition between name → nationality → occupation
→ .... In the prompt, the examples transition between name → nationality → name → nationality → ...,
which contains low-probability transitions such as “German” → “Mahatma Gandhi”. The prompt
formatting (e.g., choice of delimiter) can also be a source of mismatch. We aim to show that despite
this mismatch, large LMs can infer the prompt concept from examples.
In-context predictor and task. For in-context learning, the output target y for each example x is
sampled according to pprompt(y|x):
ytest r^ PnTomnt (y | XteSt) — E 力 Start 〜T) ，力 start I <τ. .、]P (y | XteSt ,ht∣=>ct ,θ ) 1 .	(4)
? IeSt LpTompIʌ? I l-csι-√ htest 〜Pprompt (htest I Xtest) LL、?∣ IeSι, test , . _|	∖ /
where htsetastrt denotes the hidden state corresponding to the first token of Xtest. We analyze the in-context
predictor fn(Xtest) = argmaxy P(y|Sn,Xtest), which outputs the most likely prediction over the pre-
training distribution conditioned on the prompt from the prompt distribution3. We study the in-context
predictorandits expected0-1 errorwithnexamples Lo-ι(fn) = Extest,ytest〜Pprompt[1[fn(xtest) = ytest]].
2.1	Assumptions
We detail the assumptions in our framework, including the structure of delimiters and regularity
assumptions. We first assume that there exists a subset of delimiter hidden states D which generates
the special delimiter token odelim deterministically.
Assumption 1 (Delimiter hidden states). Let the delimiter hidden states D be a subset of H. For any
hdelim ∈D and θ ∈ Θ, p(odelim|hdelim,θ) — 1 andfor any h / D, p(odelim∣h,θ) — 0.
Thus, observing the delimiter odelim reveals that the corresponding hidden state is in D, but does not
reveal which element of D it is. The delimiter is usually a token that can appear in a broad range of
contexts (e.g., newline). The delimiter ideally does not distract from the examples — for example,
an adversarial delimiter could look like part of the input X. To mitigate these scenarios, we assume
that no delimiter (e.g., newline) is significantly more likely under one concept rather than another.
2The example length k is fixed for simplicity — we leave extending our analysis to variable k as future work.
3 In practice, greedy decoding or nucleus sampling (Holtzman et al., 2o2o) are used for likely completions.
3
Published as a conference paper at ICLR 2022
Assumption 2 (Bound on delimiter transitions). For any delimiter state hdelim ∈ D and any hidden
state h ∈ H, the probability of transitioning to a delimiter hidden state under θ is upper bounded
p(hdelim∣h,θ) <c2 forany θ ∈ Θ∖{θ*}, and is Iowerboundedp(hdelim∣h,θ*) >cι > 0 for θ*. Addition-
ally, the start hidden state distributionfor delimiter hidden states is bounded asp(hdelim ∣θ) ∈ [c3,c4].
The prompt start distribution is a source of distribution shift that is separate from the shift from
concatenating independent examples. We make an assumption that limits how much distribution shift
is introduced by the prompt start distribution.
Assumption 3 (Distribution shift from prompt start distribution). We assume that the prompt
start distribution Pprompt is close in TV distance to all hidden transition distributions (under θ*)
starting from a delimiter hidden state: maxh delim ∈d TV (Pprompt (h)kp(h∣hdelim, θ*)) < ∆/4. Here,
∆ = pprompt(ymax|xtest) - maxy6=ymax pprompt(y|xtest) is the margin between the most likely label
ymax =argmaxyPprompt(y|xtest) and the second most likely label.
Even if the maximum TV distance is 0, there is still distribution shift from concatenating independent
examples. We also assume the prompt concept θ* is in the family Θ, a broad set of concepts.
Assumption 4 (Well-specification). The prompt concept θ* is in Θ.
Even though the pretraining distribution is broad, the prompt is still low probability under the
pretraining distribution since it concatenates independent examples. Finally, if the prompt has zero
probability under the prompt concept θ*,then Bayesian inference will not be able to infer the prompt
concept as in Section 3.1. The following are regularity assumptions which mainly ensure that the
prompt is not zero probability under θ*.
Assumption 5 (Regularity). The pretraining distribution P satisfies: 1) Lower bound on transition
probabilityfor the prompt concept θ*: for any pair ofhidden states h,h0 ∈ H, p(h∣h0,θ*) >c5 > 0.
2)	Start hidden state is lower bounded: for any h ∈H, p(h∣θ*) ≥ c8 > 0. 3)All tokens can be emitted:
for every symbol o, there is some hidden state h ∈ H such that p(o∣h,θ*) >c6 > 0, 4) The prior p(θ)
has support over the entire concept family Θ and is bounded above everywhere.
3 Theoretical analysis
We prove that in the limit of infinite examples, the error of the in-context predictor is optimal if a
distinguishability condition holds——the prompt concept θ* is distinct enough fromthe other concepts in
Θ (e.g., when Θ is a discrete set). When distinguishability does not hold (e.g, Θ is continuous-valued),
we show that the expected error still decreases with the length of each example, showing that
information in both the inputs and the input-output mapping contribute to in-context learning.
3.1	High-level approach
Our goal is to show that argmaxyP(y|Sn,xtest) → argmaxyPprompt(y|xtest) as the number of examples
n grows. In the following, assume that the prompt has non-zero probability under the pretraining
distributionP given θ*,meaning thatp(Sn,χtest∣θ*) > 0. We expandp(y∣Sn,χtest) to analyze its limit:
P(y|Sn,xtest)=
θ
P(y∣Sn,Xtest,θ)p(θ∣Sn,Xtest)dθ
H ∕p(y∣Sn,χtest,θ)p(Sn,xtest∣θ)p(θ)dθ (Bayes, rule, drop the constant --—1——-)
θ	P(Sn,xtest)
=ZX p(y∣xtest,h鬻t,θ)p(h鬻t∣Sn,Xtest,θ) P(Sn沟吸、p(θ)dθ	⑸
θhhrt∈HH	p(Sn,xtestlθ )
(Law of total prob, Markov property, divide by p(Sn,χtest ∣θ*) (a constant))
=/ X P(y∣Xtest,hreart,θ)p(hreart|Sn,Xtest,θ)exp(n∙rn(θ))p(θ)dθ	(6)
θhtsetastrt∈H
where rn(θ)=n log P(Sn,xee混. In Theorem 1, we prove that under a distinguishability condition,
exp(n∙rn(θ)) → 0 for all concepts θ except the prompt concept θ*, where exp(n∙ rn(θ*)) = 1. The
only nonzero term in the integral is when θ = θ*, and thus the prompt concept is “selected” as a
consequence of Bayesian inference4. Lemma 1 shows that the argmax after restricting to θ* is the
4We can exchange limits and integrals since the probabilities are bounded (dominated convergence).
4
Published as a conference paper at ICLR 2022
same as the most likely label under pprompt (y |xtest) (using Assumption 3). Putting these together with
Equation 6, the in-context predictor infers the prompt concept θ*:
argmax p(y|Sn ,xtest) → argmax pprompt(y |xtest)	(7)
yy
Thus, the in-context predictor is optimal as the number of in-context examples increases.
3.2	Heuristic derivation
Recall from Section 3.1 that if exp(n∙rn(θ)) → 0 for all θ = θ*,then Bayesian inference “selects” the
prompt concept through marginalization. To do this, we focus on showing that rn (θ), the average
log-likelihood ratio between θ and θ*, converges to a negative constant, and thus nrn goes to -∞.
The main technical challenge is to handle the sequence-of-examples structure of the prompt, which
makes all the examples dependent with respect to the pretraining distribution. Our approach uses
properties of delimiter tokens to approximately factorize the examples, with constant error per example.
We let Oiex = [oid-eli1m,Oi] be the i-th input-output pair and the previous delimiter together for i > 1 and
define O1ex=O1. Expanding the likelihood term inside rn(θ), our goal is to show
n
P(Sn ,xtest∣θ)= P(XteSt∣Sn,θ)p(Sn∣θ) ≈ Yθ(.1)p(.θi∣θ)	(8)
i=1
To show this, we expandp(Sn∣θ) with the chain rule, and with Assumption 5 (to boundP(XteStISn,θ)
by O(1)) it can be shown that
n
P(XteSt ∣Sn,θ)p(Sn∣θ) ≈ YO⑴P(OeXQeXi-1 ,θ).	⑼
i=1
We then marginalize P(OeXIOeXi-ι,θ) over the hidden state hd-im corresponding to the delimiter in
Oiex=[oid-eli1m,Oi]:
nn	n
YO⑴P(OeXIOeXi-ι,θ) = YO⑴ X P(OiIhd-im,θ)P(hdelimIOeXi-ι,θ)≈YO⑴P(OM
i=1	i=1	hide-lim1 ∈D	i=1
(10)
While summing over H above would be a trivial equality, we can replace H with the set of delimiter
hidden states D since P(hQexi-ι,θ) =0 for non-delimiter hidden states h∈ D (Assumption 1). We
used in the first equality that O1eX:i-1 → hid-eli1m → OieX forms a Markov chain and P(oid-eli1m Ihid-eli1m) = 1
(Assumption 1) to change OieX to Oi. Finally, we can show using properties of delimiter hidden states
(Assumption 2) thatP(hd-imIOeXi-ι,θ)=O⑴ and Phde吧三。P(OiIhd-im,θ) ≈ O(I)P(OiIθ) in the
second step. Therefore, we can upperboundrn(θ) as
rn(θ) ≤ - (θ(n) + Xlog P(OiIQ ! →O(1)+ EO〜P	Jlog P(OIQ].	(11)
nJ —n1 '，+W gP(OiIθ*) I ' L O〜PPrompt[ gp(OIΘ)	' '
The eXpectation term can be written as the difference of two KL divergences,
KL(PPrOmPt(O)∣∣p(OIΘ*)) - KL(PPrOmPt(O)∣∣p(OIΘ)). We bound the first KL term by a con-
stant using Assumption 5 ——intuitively for one example, PPrOmPt and p("Θ*) are close. We break the
second term into a sum of negative KL divergences over k tokens. There are O(k) KL terms and
only O(-) other error terms, which come from the distribution mismatch between the prompt and
pretraining distributions. If the KL terms are larger than the error terms, thenrn(θ) has a negative limit.
If this holds for all θ = θ*, then we have exp(n ∙ rn (θ)) → 0 for all θ = θ*, enabling in-context learning.
3.3	Formal results
3.3.1	In-context learning under distinguishability
We define a distinguishability condition which formalizes when in-conteXt learning occurs. Letting
Pjθ(o) := P(O[j] = oIO[- : j --],θ) be the output distribution of the j-th token given the previous
tokens and Pjprompt (o) := Pprompt(O[j] =oIO[-:j--]) be the analogous distribution under the prompt
5
Published as a conference paper at ICLR 2022
OOD low-prob transitions
between examples
nn AA
Albert Einstein was German ∖n Mahatma Gandhi was Indian ∖n Marie Curie was
VAΛ∕ VAΛ∕
In-distribution transitions
reveal information about θ*
Figure 2: When the signal about the prompt concept within each example (green) is greater than the
error from low-probability transitions between examples, in-context learning succeeds in our latent
concept setting (Theorem 1). Increasing the example length k increases the signal. The signal for
in-context learning comes from tokens in both the inputs and the input-output mapping.
distribution, the distinguishability condition depends on the KL divergence between pjprompt (which
represents θ*) and pjθ as Wen as error terms eθtart and eθelim coming from the distribution mismatch
between the prompt and pretraining distributions at the start and delimiter token for each example:
KLj (θ *kθ ):= Eθ[Lj-1]~Ppro1JKL(PpromptkPθ )]	(12)
dθelim :=2(log(c2) -log(c1))+log(c4) -log(c3),	sθtart :=log(1/c8).	(13)
Condition 1 (Distinguishability). We define θ* to be distinguishable if for all θ ∈ Θ,θ 6=θ*,
k
XKLj(θ*kθ) >sθtart+dθelim.	(14)
j=2
When the signal from KL divergence (LHS) is larger than the error terms, Equation 14 is satisfied
(Figure 2). For larger example lengths k, the LHS increases, improving distinguishability. Intuitively,
larger example lengths increase the proportion of the prompt sampled from the pretraining distribution
by providing more evidence for Bayesian inference. Under Condition 1, the in-context predictor
asymptotically achieves the optimal expected error.
Theorem 1.	Assume the assumptions in Section 2.1 hold. If Condition 1 holds, then as n → ∞ the
prediction according to the pretraining distribution is
argmaxP(y|Sn,xtest) →argmaxPprompt(y|xtest)∙	(15)
yy
Thus, the in-context predictor fn achieves the optimal 0-1 risk: limn→∞L0-1(fn) =inff L0-1(f).
3.3.2 Non-distinguishable case
The distinguishability condition (Condition 1) fails When there is some θ 6= θ * for Which the KL
divergence betWeen θ and θ * is less than the error terms. HoWever, this also means that the output
distributions ofθ and θ* are close in KL. We leverage this to prove that the expected 0-1 error decreases
With the example length k under tWo different settings Where distinguishability does not hold.
Continuity. Our first result relies on a continuity assumption betWeen the concept parameter and
its corresponding output distribution. Our assumption is based on prior Works (Kleijn & van der Vaart,
2012), Where the KL divergence is assumed to have a 2nd-order Taylor expansion.
Theorem 2.	Let the set of θ which does not satisfy Equation 14 in Condition 1 to be B. Assume that
KL divergences have a 2nd-order Taylor expansion around θ*:
∀j> 1, KLj(θ*kθ) = 2(θ-θ*)>Ij,θ*(θ-θ*)+O(kθ-θ*k3)	(16)
where Ij,θ* is the Fisher information matrix of the j-th token distribution with respect to θ*. Let
γθ* = maXj ： ma，；)：*) where λ max ,λ min return the largest and smallest eigenvalues. Thenfor k> 1 and
as n→∞, the 0-1 risk of the in-context learning predictor fn is bounded as
nl→∞Lo-ι(fn) ≤infLo-ι(f)+ gT(O (γθ*supθ∈k-T+"θelim ))	(17)
where g (δ) = 2 ((1 - δ )log(1 - δ ) + (1 + δ )log(1 + δ)) is a calibration function (SteinWart, 2007; AVila
Pires & Szepesvari, 2016) for the multiclass logistic loss for δ ∈ [0,1).
6
Published as a conference paper at ICLR 2022
k=3
k=5
k=8
k=10
Figure 3:	In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINC
dataset. Accuracy increases with number of examples n and length of each example k.
k=3
k=5
k=8
k=10
-k=3
-k=5
-k=8
-→- k=10
0	20	40	60
Num examples
k=3
k=5
k=8
k=10
Figure 4:	Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left)
When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining data
has random transitions, the model sees all token transitions but in-context learning fails. (Right) When
prompts are from random unseen concepts, in-context learning fails to extrapolate.
Since the inverse calibration function g-1 is roughly linear in for ≤ 0.7, the excess risk roughly
decreases as OQlkY When the “worst-case condition number" γe* of the Fisher information matrices
is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction
to vary θ* in which the output distribution will sharply change. As a consequence, the concepts θ that
are not distinguishable from the prompt concept θ* parameterize distributions that produce similar
outputs to the prompt concept and thus achieve a small error.
Varying-length test examples. In the setting where the length of xtest is random (uniformly from
2 to k), we can give a similar error guarantee without continuity.
Theorem 3. Let the setofθ which does not satisfy Equation 14 in Condition 1 to be B. Let the length
of the test example xtest be uniformly distributed between 2 and k, for k ≥ 2. Then for k ≥ 2 and as
n→∞, the 0-1 risk of the in-context learning predictor fn is bounded as
nl→m∞L 0-f) ≤ in fL 0-1 (f )+gT (O(Wk s-1+ ' "dei '
(18)
Instead of measuring only the error at the k-th token, we average the prediction error on the 2nd to k-th
tokens. However, we leave bridging the mismatch between training examples, which are consistently
length k, and test examples, which have random length, to future work.
4 Simulations
We generate the GINC dataset and show that Transformers (Vaswani et al., 2017) and LSTMs (Hochre-
iter & Schmidhuber, 1997) trained on GINC exhibit in-context learning. In the theory, we assumed
that the pretrained LM fits the pretraining distribution exactly. Here, we pretrain LMs to approximate
the pretraining distribution and find that the in-context learning properties transfer to the LM.
GINC dataset. We construct the GINC dataset according to our theory (see Appendix F.1). For
pretraining, we define a uniform mixture of HMMs over a family Θ of 5 concepts to generate 1000
pretraining documents with 〜10 million tokens total. For prompting, We generate prompts with 0
to 64 training examples and example lengths k ∈ {3,5,8,10} (2500 prompts for each setting). The
target token ytest is taken to be the most likely output instead of sampling so that the intrinsic error is 0.
Main result. We train GPT-2-based Transformers (Radford et al., 2019) and LSTMs on three
versions of the GINC dataset with vocabulary sizes 50, 100, and 150, then evaluate the in-context
7
Published as a conference paper at ICLR 2022
Figure 5: In-context accuracy (95%
intervals) of Transformers improves
as model size increases on the
GINC dataset for vocabulary sizes
50, 100, and 150.
Model	# Params	Train loss (pretraining)	Val loss (pretraining)	In-context Acc
Vocab size 50, k= 10,n = 64				
Transformer (4 layer)	29M	1.49	1.50	60.2 ± 5.7
Transformer (12 layer)	85M	1.31	1.33	81.2 ± 7.1
Transformer (16 layer)	115M	1.31	1.33	84.7 ± 3.4
LSTM	28M	1.31	1.35	95.8 ± 1.11
Vocab size 100, k = 10,n = 64				
Transformer (4 layer)	29M	1.58	1.59	67.4 ± 4.7
Transformer (12 layer)	85M	1.40	1.42	84.6 ± 3.0
Transformer (16 layer)	115M	1.41	1.43	88.7 ± 1.6
LSTM	28M	1.43	1.44	95.8 ± 1.54
Vocab size 150, k = 10,n = 64				
Transformer (4 layer)	29M	1.44	1.45	92.8 ± 1.9
Transformer (12 layer)	85M	1.27	1.28	98.4 ± 0.4
Transformer (16 layer)	115M	1.27	1.28	98.1 ± 0.5
LSTM	28M	1.26	1.31	99.2 ± 1.06
Figure 6: In-context accuracies (95% intervals)
on GINC with vocab sizes (50, 100, 150) for
Transformers and LSTMs. Accuracy improves
with scale even though the pretraining loss may
be the same.
accuracy (see Appendix F.2, F.3). We average all results over 5 pretraining runs. Figure 3 shows that
for both Transformer and LSTMs, in-context accuracy improves as the number of prompt examples
n and the example length k increase, verifying our theory.
Ablations on the latent concept structure. We ablate the role of the mixture-of-concepts structure
in GINC. In Figure 4 (left), we pretrain a4 layer Transformer on data with only one concept (removing
the prior) from Θ, resulting in flat in-context learning curves. Figure 4 (middle) shows that pretraining
on random pretraining data, which contains all possible token transitions, in-context learning also fails.
Therefore, the mixture-of-concepts structure is important and simply seeing diverse token transitions
does not enable in-context learning.
Extrapolation to unseen concepts. Full generative control of GINC allows for experimentation with
latent variables in the pretraining distribution. For example, in large-scale datasets, itis difficult to test
whether a concept or task is in the pretraining data. We test this in GINC by testing the in-context accu-
racy ofa4 layer Transformer on prompts generated from 5 random concepts that are not in the pretrain-
ing family of concepts. Figure 4 (right) shows that in-context learning also fails for these novel concepts.
Effect of model size and architecture. Figure 5 shows that increasing the size of the Transformer
(4, 12, 16 layers) steadily increases the in-context accuracy, corroborating the results of Brown et al.
(2020). Table 6 shows that even though larger Transformers may have the same pretraining loss
(e.g., 12 and 16 layer Transformers both get 1.33 validation loss for vocab size 50), the in-context
accuracy still improves (81% to 85% from 12 to 16 layers), suggesting that larger models can improve
in-context learning beyond improving pretraining perplexity. This may be related to phenomena from
overparameterization and overtraining (Zhang et al., 2017; Power et al., 2021). Finally, the model
architecture also plays a role — LSTMs consistently outperform Transformers on GINC despite having
fewer parameters, perhaps due to the similarity between HMMs and LSTMs. We leave analysis of
the effect of model scaling and model architecture as open questions.
Sensitivity to example ordering. In Figure 7 (left), we test the sensitivity of in-context accuracy
on GINC to the ordering of the prompt examples, following Zhao et al. (2021). For this experiment,
we consider prompts generated from a single concept and prompt start distribution. We sample 10
different sets (leading to 10 training set IDs) of4 examples and generate all 24 possible permutations
for each example set. We consider the in-context accuracy ofthe4 layer Transformer trained on GINC
with vocabulary size 50. Similarly to the behavior of GPT-3 (Zhao et al., 2021), there is a significant
variation (10-40% difference) between permutations of the same set of examples.
Zero-shot is sometimes better than few-shot. In some settings in GINC, we find that zero-shot
performance can be better than few-shot performance. This mirrors GPT-3 on some datasets (e.g., LAM-
BADA, HellaSwag, PhysicalQA, RACE-m, CoQA/SAT analogies for smaller models (Brown et al.,
2020)). This occurs especially when the transition probabilities in GINC are lower entropy (controlled
via a temperature parameter). For this experiment, we consider GINC with transition matrix temper-
ature parameter 0.01 (instead of 0.1), 12 concepts, and vocabulary size 100. Figure 7 (right) shows that
here, few-shot accuracy is initially worse than zero-shot accuracy, but can recover with more examples.
We hypothesize that the distracting prompt structure initially decreases the accuracy in this setting.
8
Published as a conference paper at ICLR 2022
80
40
30
70
&
⅛ 60
⅛50
0123456789
Training set ID
Num examples
k=3
k=5
k=8
k=10
Figure 7: (Left) In-context accuracy varies widely with example ordering. Each training ID refers
to a set of training examples. Each dot refers to the in-context learning accuracy of one permutation
of the training examples for that particular training ID. (Right) Zero-shot performance can be higher
than one/few-shot performance in some settings in GINC, mirroring the behavior of GPT-3 on some
datasets such as LAMBADA (Brown et al., 2020). The few-shot setting introduces the distracting
prompt structure, which can initially lower accuracy.
5	Discussion and related work
Learning via Bayesian inference and extrapolation. The canonical Bernstein-von Mises theo-
rem (van der Vaart, 1998) does not apply for in-context learning since the prompt examples are not
independent under the pretraining distribution. Gunst & Shcherbakova (2008) show a Bernstein-von
Mises-type result for observations from an HMM, but do not handle observations from a different dis-
tribution. Future directions include more precise asymptotic results about the posterior distribution and
results under misspecification/extrapolation (Kleijn & van der Vaart, 2012). A possible avenue for ex-
trapolation to some types of unseen concepts is to factorize the latent concept into semantics and syntax.
While the pretraining data may contain only some semantics-syntax pairs, the language model could
generalize to unseen pairs if it learns generalizable syntactical operations such as copying or reordering.
Topic models and HMMs. Topic models such as LDA (Blei et al., 2003) also have document-level
latent variables, but learning is typically relies on algorithms such as EM (Dempster et al., 1977),
variational inference (Jordan et al., 1999), or MCMC (Metropolis et al., 1953; Hastings, 1970). We
focus on learning as a natural result of Bayesian inference without an explicit inference algorithm.
Wei et al. (2021a) also use an HMM model in their pretraining analysis. However, they analyze how
pre-trained representations learned with masked LMs (Devlin et al., 2019; Liu et al., 2019; Lewis
et al., 2020; Clark et al., 2020) can improve optimization-based downstream learning (Li & Liang,
2021; Lester et al., 2021) rather than in-context learning.
Bridging the mismatch between pretraining and prompting. Prior works support our theoretical
intuitions that reducing the prompt distribution mismatch would improve in-context learning. Finetun-
ing LMs on text with a prompting format improves its zero-shot performance (Wei et al., 2021b; Sanh
et al., 2021) and optimizing prompt templates improves few-shot finetuning (Jiang et al., 2020; Schick
&SchUtze,2021;ShinetaL,2020;GaoetaL,202l). ZhaoetaL (2021);HoltzmanetaL (2021)improve
in-context accuracy via calibration or renormalization, a form of adaptation to the prompt distribution.
Meta-learning. Meta-learning methods can also train a sequence model to learn from examples (Ravi
& Larochelle, 2017). However, meta-learning models are trained to learn, while in-context learning
emerges from LM pretraining.
Studying large-scale phenomena at a small scale. We can study in-context learning, a large scale
phenomenon, at a small scale in GINC because the complexity of the pretraining distribution (HMM
hidden state size, number of latent concepts) is small, such that the data and models are relatively larger.
Since GINC is synthetic, we can also control the latent data properties (e.g., unseen concepts) to make
predictions about large LMs while working at a small scale.
6	Conclusion
We cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers
a concept when making a prediction. We show that in-context learning occurs when the pre-training
distribution is a mixture of HMMs. Our work provides a first step towards understanding in-context
learning, which we hope will provide insight for improving pretraining and prompting.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank Tianyi Zhang, Frieda Rong, Lisa Li, Colin Wei, Shibani Santurkar, Tri Dao, Ananya
Kumar, and Shivam Garg for helpful discussions and feedback. SMX is supported by an NDSEG
Fellowship. The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL
at Stanford University. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the
Sloan Fellowship, and JD.com. Toyota Research Institute provided funds to support this work.
References
Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov
chains. The annals Ofmathematical statistics, 37(6):1554-1563,1966.
D. Blei, Andrew Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research (JMLR), 3:993-1022, 2003.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations (ICLR), 2020.
A. P. Dempster, Laird N. M., and Rubin D. B. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1-38, 1977.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Association for Computational Linguistics
(ACL), pp. 4171-4186, 2019.
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. arXiv, 2021.
Zoubin Ghahramani and Michael Jordan. Factorial hidden Markov models. Machine Learning, 29:
245-273, 1997.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. Hidden topic Markov models. In Artificial
Intelligence and Statistics (AISTATS), 2007.
M. Gunst and O. Shcherbakova. Asymptotic behavior of Bayes estimators for hidden Markov models
with application to ion channels. Mathematical Methods of Statistics, 17, 2008.
Keith W. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57(1):97-109, 1970.
SePP HoChreiter and Jurgen SChmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of neural text
degeneration. In International Conference on Learning Representations (ICLR), 2020.
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. SurfaCe form
ComPetition: Why the highest Probability answer isn’t always right, 2021.
Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How Can we know what language
models know? In Association for Computational Linguistics (ACL), 2020.
MiChael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and LawrenCe K. Saul. An introduCtion
to variational methods for graPhiCal models. Machine Learning, 37:183-233, 1999.
10
Published as a conference paper at ICLR 2022
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension. In Association for Computational
Linguistics (ACL), 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
B.J.K. Kleijn and A.W. van der Vaart. The Bernstein-von mises theorem under misspecification.
Electronic Journal of Statistics, 6, 2012.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. In Association for Computational Linguistics
(ACL), 2020.
Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
Association for Computational Linguistics (ACL), 2021.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation.
Technical report, AI21 Labs, August 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference
on Learning Representations (ICLR), 2019.
Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward
Teller. Equation of state calculations by fast computing machines. The journal of chemical physics,
21(6):1087-1092,1953.
Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset:
Word prediction requiring a broad discourse context. In Association for Computational Linguistics
(ACL), 2016.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking:
Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH AI Workshop, 2021.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations (ICLR), 2017.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,
Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal
Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng
Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman,
Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables
zero-shot task generalization, 2021.
Timo Schick and Hinrich Schutze. Exploiting cloze questions for few shot text classification and
natural language inference. In European Association for Computational Linguistics (EACL), 2021.
Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Eliciting
knowledge from language models using automatically generated prompts. In Empirical Methods
in Natural Language Processing (EMNLP), 2020.
11
Published as a conference paper at ICLR 2022
Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation,
26, 2007.
A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream
tasks? an analysis of head and prompt tuning. arXiv, 2021a.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv, 2021b.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace’s
transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017.
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. In International Conference on Machine Learning
(ICML), 2021.
BernardoAVilaPireSandCSabaSZePeSVari. Multiclass classification calibration functions. arXiv, 2016.
12
Published as a conference paper at ICLR 2022
A Framework details
Prompt distribution details. For in-context learning, we sample a prompt from a new distribution
pprompt, which consists of n independent training examples and 1 test example. We first sample
n hidden segments H of length k by sampling the first element hstart = H [1] from a prompt start
distribution pprompt. Then, we sample the rest of the segment Hseg =H[2 : k] from the hidden transition
distribution of the pretraining distribution P corresponding to a particular concept θ*:
H1,...,Hn, Hi= [hi,1,...,hi,k]	(19)
hstart = Hi [1]〜Pprompt, Hseg = Hi [2: k]〜P(Hseg | hstart,θ*).	(20)
To end each example (except the test example), we sample n delimiters hdelim ∈ D from ppdreolimmpt:
delim delim delim delim
h1	,...,hn ,	hi	〜 Pprompt.	(21)
Conditioned on hidden variables Hi and hidelim, we sample the observed tokens Oi = [oi,1,...,oi,k] and
oidelim respectively from the pre-training distribution:
Oi ,...,On,Oi 〜p(Oi∣Hi)	(22)
CdeIim	CdeIim	CdeIim /Welimi 入delim q*∖
o1 ,...,on , oi	〜 P(oi	|hi	,θ )
(23)
The “input” for each example is xi = Oi[1 : k- 1] and the “output” is yi = Oi[k]. Taking S to be the
sequence of training examples (without the test example), the resulting prompt sequence is
[Sn,Xtest] = [Oi ,θ1elim,...,On,onelim,Xtest] = [x1,y1,θlelim,X2,y2,θdelim,...,Xn,yn,Onelim,Xtest]〜Pprompt
(24)
where xtest = xn+i = On+i [1 : k- 1] is sampled via the same process but with k- 1 elements.
B Propositions for Theorem 1
The following propositions, which lower bound the probability ofa delimiter token and probability
of an example under θ*, are direct corollaries of the assumptions.
Proposition 1. Forall i, we havep(hdeelm|Oi,o职im,...,Oi,θ*) >ci andp(hddeliim∣Oι,o沪m,...,Oi,θ) <
c2.
Proof. By Assumption 2,
p(hdelim∣Oi,odelim ,...,Oi,θ)= Xp(hdelim∣hi,k )p(hi,k ∣Oi,o1elim,...,Oi,θ)	(25)
hi,k
<Xc2P(hi,k|Oi,odielim,...,Oi,θ)=c2.	(26)
hi,k
Similarly,
p(hdelim∣Oι ,o1elim,...,Oi ,θ*)= Xp(hdelim∣hi,k )p(hi,k ∣Oι,o1elim,...,Oi,θ*)	(27)
hi,k
>XciP(hi,k|Oi,odielim,...,Oi,θ*)=ci.	(28)
hi,k
□
Proposition 2. The probability ofan example is lower bounded for θ*: there is some c7 >0 such that
p(O∕h itαrt ,hj,ι,θ*) >c7 forall i andfuture hidden states hj,ι ,forany l and j >i.
Proof. By Assumption 5, we have
p(Oi Ihstart,hj,ι,θ*) = EP(OiIHi)P(Hi Ihstart,hj,ι,θ*)> (C6)k	(29)
Hi
for some Hi . We have
P(HiIhistart,hj,l,θ*)=
P(hj,ι∣H,hstart,θ*)P(H∣hstart,θ*)
P(hj,ι Ihitart,θ*)
> c25
(30)
which lower bounds the terms in the numerator by c5 (marginalizing over previous hidden states),
and upper bounding the denominator by 1. Setting c7 = (c6)kc25 finishes the proof.	□
13
Published as a conference paper at ICLR 2022
C Convergence of the in-context predictor
Under Assumption 3, we show that the in-context predictor fn(xtest) = argmaxy p(y|Sn, xtest)
converges when abstracting away the Bayesian inference component (the selection of θ* from Θ) of
the in-context predictor. We will complete the argument for the convergence of the in-context predictor
in the proof of Theorem 1.
Lemma 1. Suppose the prompt Sn and the test input xtest are given. Under Assumption 3, we show
that the argmax ofthe averaged predictive distribution conditioned on θ* and a prompt Sn is the same
as the argmax of the prompt predictive distribution:
argmax E p(y∣xtest,h^M"[p(h鬻t∖Sn,xtest,θ*) = argmaxpPrOmpt(y|xtest).	(31)
y	htsetastrt∈H	y
Proof. First, we note by definition that
pprompt(y |xtest) =	p(y|xtest,htest ,θ )pprompt (htest |xtest).
htsetastrt∈H
Expanding the last term, we have
Pprompt(h≡tEeSt) YP(XteSthe*,θ*)Pprompt(h鬻t).
which is proportional to a constant in xtest.
(32)
(33)
On the other hand, analyzing one term inside the LHS of the lemma statement, we have
p(hstart∣Sn,Xtest,θ*) YP(XteStIh鬻t,θ* )p(h鬻t∣Sn,θ* )	(34)
which is proportional to a constant in xtest and Sn . The quantities differ in the last term, which we
expand below and put in matrix form. Let T ∈ RlHl×lDl be the matrix that represents the transition
probabilities starting from a delimiter state: P(htsetastrt|hdelim) for htsetastrt ∈H and hdelim ∈D. As a result,
p(h 鬻t∣Sn,θ* )= ∑>(h 鬻t∣hnelim,θ*)pMelim∣Sn,θ*)
hdelim
=Tv
(35)
(36)
where hdnelim is the delimiter hidden state before htsetastrt.
Let W ∈ RlYl×lHl be the matrix that represents the probabilities p(y∣χtest,hsesrt,θ*)p(xtest∣hsesrt,θ*) for
all the possible y ∈Y and htsetastrt ∈ H. Overall, we can write
X p(∙∣Xtest,h鬻t,θ* )p(h鬻t∣Sn,Xtest,θ*) = WTv	(37)
htsetastrt∈H
PPrOmPt (∙∣xteSt) = WU	(38)
where u ∈ R|H| is the vector of probabilities that corresponds to the prompt start distribution Pprompt.
Bounding the difference between the two predictive distributions,
kWTv-Wuk∞≤kWTv-Wuk1	(39)
|Y|
=X|Wi>(Tv-u)|i	(40)
i=1
|Y| |H|
=XXWij(Tv-u)j	(41)
i=1 j=1
|Y| |H|
≤ XXWij I (Tv-u)j I (Wij ≥ O)	(42)
i=1j=1
|H|	|Y|
=X(XWij)I(Tv-u)jI	(43)
j=1 i=1
= kTv-uk1.	(44)
14
Published as a conference paper at ICLR 2022
Using Assumption 3, We can further bound this by ∆∕2:
|D|
kTv-ukι = 2TV (pprompt(∙)kXviP(∙∣hdelim = i,θ*))
i=1
≤ 2XviTV (pprompt(∙) kp( ∙∣ hdelim = W)) (convexity of TV distance)
i=1
≤2 max TV(Pprompt(∙)kp(∙∣hdelim,θ*)) <∆∕2.
hdelim ∈D
(45)
(46)
(47)
Since the probability of any output does not change by more than ∆∕2 and the margin betWeen the most
likely label and the second most likely label is ∆, the argmax,s are the same, showing the result. □
D Proof of Theorem 1
Proof. We analyze the most likely prediction over the pretraining distribution conditioned on the
prompt argmaxyp(y|Sn,xtest).
p(y∣Sn,Xtest)= / p(y∣Sn,Xtest,θ)p(θ∣Sn,Xtest)dθ
θ
H / p(y∣Sn,Xtest,θ)p(Sn,Xtest∣θ)p(θ)dθ
θ
H /P(ylSn,xtest,θ) P(Sn,xtes' p(θ)dθ
θθ	P(Sn,xtest∣θ")
=ZX p(y∣xtest,h≡t,θ)p(h鬻t∣Sn,Xtest,θ) P(Sn,xtes：W p(θ)dθ
Jθhst∈H	P(Sn,xtes"θ )
(48)
(49)
(50)
(51)
Defining the following quantity,
/小 1I	P(Sn,xtestlθ)	O
rn(θ) = nlogp(Sn,Xtest∣θ*) .	(52)
we will show that under distinguishability for all θ = θ*, rn(θ) converges toanegative constant such that
P(Sn ,xtestlθ)	/	∕Q∖∖ `n	∕;a∖
p(Sn,Xtest∣θ*)=exp(n∙rnI")) →0	(53)
for θ = θ*, whereas this ratio is always 1 for θ = θ*. This will then “select” the desired prompt concept
through marginalization.
Supposing that Equation 53 holds, we show that the theorem statement holds. Let
△0= max TV(PPrompt(∙)kP(∙∣hdelim,θ*)) <∆∕2,
hdelim∈D
and let e< (∆∕2-Δ0)p(Θ* ). Then for n large enough (due to Equation 53),
Z X P(y∣xtest,h鬻t,θ)P(hSt∣Sn,Xtest,θ) P(Sn四曜 P(θ)dθ
,θ⅛lt∈H	P(Sn,xtest|" )
=X P(y∣Xtest,hsesrt,θ*)P(hsesrt∣Sn,Xtest,θ*)P(θ*)+ [	eθ(y)P(θ)dθ
Cf∈H	'θ=θ*
H X P(y∣xtest,hsesrt,θ*)P(hsSt∣Sn,xtest,θ*)+7l7τ I	S(y)P(θ)dθ
htsetastrt∈H	P	θ6=θ*
where θ(y) ≤∕2 for all y ∈ Y.
(54)
(55)
(56)
(57)
By Lemma 1, the argmax of the first term of Equation 57 is the same as argmaxyPprompt(y|xtest), where
the margin between the most likely label and the second most likely is at least ∆∕2 — △. Since
PFy Jθ=θ* eθ ⑹ P⑼ ≤ 2P⅛y
< (∆∕2-∆0)∕2
(58)
15
Published as a conference paper at ICLR 2022
for all y∈ Y, the argmax of Equation 57 is also the same as argmaxpprompt (y|xtest).
Now it remains to show that rn (θ) converges to a negative constant for θ = θ*. Let Oex =同-罕,。/
be the i-th observation segment and the previous delimiter together for i > 1 and define O1ex = O1.
Expanding the numerator of the ratio in rn(θ), we have
p(Sn,Xtest∣θ)= P(XteSt∣Sn,θ)p(Sn∣θ)
n
=Xp(Xtest∣hsesrt ,θ)p(hSesrt i Sn,θ)p(oneiim∣oexn,θ)∏p(Oex∣Oex"i,θ)
start	i=1
htest	i=1
Ep(Xtest∣hSt,θ)p(h鬻tI Sn,θ)
start
htest
(59)
(60)
(61)
n
X p(onelim∣hnelim)p(hnelim∣oexn,θ)∏	X p(Oi∣hdehm,θ)p(hdehm∣Oexi-便)
hdnelim ∈D	i=1hide-lim1∈D
(62)
Ep(Xtest∣hSt,θ)p(h鬻tI Sn,θ)	(63)
htsetastrt
n
X p(hnelim∣oexn,θ)∏ X p(θi∣hdTθ)p(hdeum∣oexi-ι,θ)	碎
hdnelim ∈D	i=1hdie-lim1∈D
n
Xp(XteStIhSt,θ)p(h鬻t∣Sn,θ)∏ X p(Oi∣hd吧θ)p(hdeUmIOexI,。)	◎)
htsetastrt	i=1hide-lim1∈D
Note that in the last line, the inner sum is over the set of delimiter states D by using the assumption
that observing a delimiter odelim implies that the corresponding hidden state hdelim must be in D. We
also see that PhdeIimp(hnelim∣θexn,θ) = 1.
We restrict our attention to θ wherep(Sn,XtestIθ) > 0, since otherwise θ does not affect the prediction.
Expanding rn(θ), we have the following upper bound:
rn(θ) = 1 (log
n
1
n
p(Sn,Xtest∣θ) ʌ
p(Sn,Xtest∣θ*) J
h^p(Xtest∣h鬻t,θ)p(h鬻tI Sn,θ)	X
StP(XteStIhSt,θ*M(hsSt | Sn,θ*) +i= ɑg
(66)
Phd-m∈D p(OiIhdelim,θ)p(hdelimIoexi
Phd-m∈D PQI hdelim,θ* )p( hdelim Ioexi
1,θ)
1
≤ —
n
(67)
hssrt 1∙p(h 辔 I %,” 一、、、，£ Phd-'m∈D P(OiIhdTθ 八
StC7∙P(h鬻tISn,θ*)	( g(c2)	g(c1))+工l gPhdeIim∈DP(OiIhdeum,θ*J
(68)
1	n	Phdielim1∈Dp(OiIhid-eli1m,θ)
n Cog(C7)+n(log(⑹-log(CI))+Xlog Phd” p(Oi∣em,θ*)
i=1	i1
(69)
In the above steps, we used both Propositions 1 and 2 in the terms involving C2,C1 (bounding the
probability of hdelim hidden states) and C7 (bounding the probability of Xtest). Note that in the second
line, the sum can must be over the set of delimiter states D by using the assumption that observing
a delimiter odelim implies that the corresponding hidden state hdelim must be in D.
16
Published as a conference paper at ICLR 2022
Focusing on the numerator of the ratio term and summing over the start hidden state for the i-th example,
X p(θi∣hd-im,θ)= χ Xp(OiIhstart,θ)p(hitart∣hd-im,θ))	(70)
hdie-lim1∈D	hdie-lim1 ∈Dhsitart start delim =Xp(OiIhstart,θ)p(hstart∣θ) X Pph J⅛1,θ)	(71) 尔	hi-m∈D	p(hs	lθ) delim start =Xp(OiIhaθ)ρ(hstart∣θ) X p(h⅛l¾θ),θ)	(72) hsitart	hdie-lim1 ∈D	p i-1
where the last step applies Bayes’ rule. We can lower and upper bound the following quantity for any
θ using Assumption 2:	p(hdelim∣hstart,θ)	p(hdelim∣hstart,θ) p⅞⅛)	≤	c3i	(73) p(hd-im∣hstart,θ)	p(hd-im∣hstart,θ) p(hd-im∣θ)	≥	C4	.	()
This implies that	X p(hd-im∣hstart,θ) < i	(75) h⅛D	p(hd-imιθ)	F hi-1∈D X p(hd-im∣hstart,θ) ≥ ɪ h⅛D p(hd-imιθ) -C4. hi-1∈D
Plugging in these bounds, we have
1 (	"	Ehstart p(Oi∣hstart,θ)p(hitart∣θ)
rn⑻ ≤ n (-log(C7)+2n(IOg(C2)-log(CI))+n(logQ)-IOg(C/+Xlog P irt p(Oi∣hStart,θ)p(hit叫 θ*)
i=1	hi	i	i
(77)
-log(C7)+2n(log(C2)-log(C1))+n(log(C4
n
)-log(C3))+	log
i=1
p(Oi∣θ)
p(θi∣θ*)
→n→∞ EO〜Pprompt
1	P(O∣Θ).
g p(O∣θ*).
+ delim
where we set
dθelim = 2(log(C2)-log(C1))+log(C4)-log(C3).
(78)
(79)
(80)
1
n
Next, we convert the expectation in the bound into a KL divergence. We have
p(Olθ) _ IRI	] p(Olθ)	] ppromPt(O)	zon
O~pprom4logp(O∣θ*)] = O~pρromρt [logpprompt(O) + og p(O∣θ*) J	(8 )
=KL(pprompt∣∣p(∙∣θ*))- KL(ppromptkp(∙∣θ)).	(82)
We will upper bound the first KL term:
KL(ppromptkp(∙∣θ*))= EO〜PpromptlOg pp(OPθ(O) .	(83)
Expanding the numerator and denominator of the ratio inside, we have
k
Pprompt(O) = XpPromPt(H[1])p(O[1]∣H[1],θ*)Yp(O[j]∣H[j],θ*)p(H[j]∣Hj-1],θ*)	(84)
H	j=2
k
p(O∣θ*) = Xp(H[1]∣θ*)p(O[1]∣H[1],θ*)Y p(O[j]∣H[j],θ*)p(H[j]∣H [j-1],θ*)	(85)
H	j=2
17
Published as a conference paper at ICLR 2022
which differ in only the hidden start distribution. Using Assumption 5, We have thatp(h∣θ*) ≥ c8 for
any h ∈ H, which implies that
pprompt (h)	1
—’一一~:- ≤ —
p(h∣θ*) — C8
=⇒ Pprompt(O) ≤ -1 p(O∣θ*).
c8
Finally, this implies that the KL term is bounded as
KL(PPromPt∣∣p(∙∣θ*)) ≤-lθg(c8).
This term is non-negative since c8 ≤ 1.
(86)
(87)
(88)
Aiming to decompose the second KL term into a sum over the k tokens, we write
Pj(o)= P(Oj]=o∣O[1: j-1],θ) andPjromPt(o)= PPromPt(Oj] = o∣O[1: j-1]). Wehave
-KL(PPromPtkP(∙∣θ)) = -XPPromPt(O)lθg Ppmp(O)	(89)
— Xr)(O)Xlo PPromPt(O[j]|O[1：j-1D)	(90
=- ⅛PPromPt(O)Mlog P(O[j]∣O[1: j-1],θ)	(90)
_ X X	∕nυ	PPromPt(O [j]|O[1: j-1D)	z0n
=??PromPt(O)log P(Oj]|O[1：j-1],θ)	(91)
k
=-XEΟ[1j-1]〜PSt IKL(PjromPtkPj)]	(92)
j=1
Then we have that
k
lim rn(θ) < -	EO[1:j
-1]~Pprompt[KL(PPromPt kPj )] + EStart+ EdeIim	(93)
j=1
The second term (set eθtart = log( * ))isan error term that depends on how different the starting prompt
distribution PPromPt (which is Part of PPromPt) is to the Pretraining distribution. The third term is an error
term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is
larger in magnitude than the error terms. Note that as k becomes larger, the number of observations
of θ* “overpowers” the distracting transitions in the prompt distribution. This condition is equivalent
to the disinguishability condition (Condition 1).
By assumption, for θ = θ* the Condition 1 holds, and thus
nl→∞ P((S,Xteest%)= nl→∞exp(n ∙ rn(θ))=0
(94)
since rn(θ) has a negative, constant limit. Note that exp(n ∙rn(θ*)) = 1 for θ*.
□
E Non-distinguishable case
When Condition 1 is unsatisfied, Equation 14), gives an uPPer bound on the sum ofKL divergences
for the next token distributions given different-length histories. In contrast, the in-context task only
measures the accuracy of the last (k-th) token. The main challenge is to relate the different-length
histories to each other to give a more Precise bound for the error on the in-context task (last token).
Before addressing this challenge, we give the following lemma, which leverages the result of Avila
Pires & Szepesvari (2016); Steinwart (2007) to relateabound on the KL divergence to 0-1 loss.
18
Published as a conference paper at ICLR 2022
Lemma 2. Let the set of θ which does not satisfy Condition 1 to be B. Assume that
KL(Pprompt(ytest|xtest)kp(ytest|xtest, θ) is bounded above for all θ and that θ* minimizes the
multiclass logistic risk LCE(θ) = -Extest~pprompt [pprompt(ytest|xtest)logp(ytest|xtest,θ)]∙ If
Ex test 2 ppn)mpt[KL(Pprompt(y test lx test) kp(y test|x test ,θ))] ≤ ^θ for all θ ∈B,	(95)
then
lim L0-1(fn) ≤infL0-1(f)+g-1 supθ	(96)
n→∞	f	θ∈B
where
g(δ) = 1((1-δ)log(1 -δ) + (1+δ)log(1 + δ))	(97)
is a calibration function for the multiclass logistic loss for δ ∈ [0,1].
Proof. First, we note that we can study the 0-1 risk of the limiting predictor:
lim L0-1(fn)= lim
Extest ,ytest 2pprompt[1[fn(xtest) 6=ytest]]	(98)
→∞	n→∞	pp
Extest,ytest2pprompt[ lim 1[fn(xtest) 6=ytest]] (dominated convergence, boundedness of indicator)
(99)
Extest ,ytest 2pprompt[1[nl→im∞fn (xtest) 6= ytest]]
(100)
where in the last step we use that since the output space of fn is discrete and the probabili-
ties that the in-context predictor takes an argmax over converges, then for N large enough,
fN (xtest) = limn→∞fn (xtest).
Note that for every input xtest, the limiting in-context learning predictor outputs the argmax of a
predictive distribution which can be a mixture of predictive distributions over B:
lim fn(xtest) = argmaxEθ~q[p(y∣xtest,θ)]	(101)
n→∞	y
for some distribution q over B. The KL divergence between this mixture and the prompt concept is
bounded by the KL divergence of any one θ ∈ B , due to the convexity of KL:
Extest~Pprompt[KL (PPrOmPt(y |Xtest) 11 Eθ~q [p(y| Xtest ,θ)]]	(102)
≤ Extest2pprompt [Eθ~q[KL(pprompt(y |xtest) kp(y |xtest,θ))]]	(103)
= Eθ2q [Extest2pPrOmPt [KL(pprompt(y|xtest) kp(y lxtest,θ))]]	(104)
≤ SUPExtest~Pprompt[KL(PPrOmpt(y |xtest) kp(y|xtest,θ))]	(105)
θ∈B
where we can exchange the order of expectations since the KL is bounded (dominated convergence).
From the KL bound KL(PPrOmPt (ytest |xtest)kp(ytest Ixtest,θ), We thus have
Extest2pprompt[KL(pprompt(ytest|xtest) kp(ytest|xtest,θ))] = LCE(θ) -LCE(θ* ) ≤ supeθ	(106)
θ∈B
where Lce(θ) = -ExISst~pprompt[Pprompt(yteMxtest)logP(yteMxtest,θ)] is the multiclass logistic risk, and
Lce(Θ*) is the optimal risk over θ ∈ Θ by assumption. Applying Theorem 2.2 and 5.11 of AVi山
Pires & Szepesvari (2016), g is a calibration function for the multiclass logistic loss, and allows us
to convert the surrogate risk bound to a bound on the 0-1 loss, giving the result. Note that we have
zero approximation error here, since θ* ∈ Θ.	口
Note thatg-1 is roughly linear in for smaller than 0.7, where the bound is non-vacuous.
19
Published as a conference paper at ICLR 2022
E.1 Proof of Theorem 2
Proof. By the continuity assumption, we have for any θ in B that
kk
XKLj(θ*kθ) ≥ 3X(θ-θ*)>Ij,θ*(θ-θ*) + (k-1)O(kθ-θ*k3)	(107)
j=2	2j=2
≥ 2(k - 1)λmin(Ij,θ* )kθ - θ* k2	(108)
=⇒kθ-θ*k2≤ ι工『+am	.	(109)
1 (k-1)(mιnj λmin(Ij,θ*))
We use this to bound the last KL term by plugging it in below:
KLk (θ*kθ) = 2(θ-θ*)>Ij,θ*(θ-θ*) + O(kθ-θ*k3)	(110)
≤ 1(max λmaχ(Ij,θ* ))kθ-θ*k2+O(kθ - θ*k2)	(111)
2j
≤ (⅛rt + Eθelim)(maXj λmax(Ij,θ* ) + O(1))	(口?)
一	(k -1)minj λmin(Ij,θ*)	^
Rearranging and noting that KLk(θ*∣∣θ) = ExteSt〜Ppro1mJKL(Pprompt(ytest山est)kp(ytest∣xtest,θ))], We
have
ExteSt〜Ppro1JKL(Pprompt(ytest∣Xtest)kp(ytestEest,θ))] ≤ ʤgim*%，*仍)+O(I))
(k-1)minj λmin (Ij,θ* )
(113)
Plugging into Lemma 2 gives the result.	□
E.2 Proof of Theorem 3
Note that Condition 1 ensures that the sum of KL divergences betWeen positions Within a k-length
input is bounded. This means that We have a bound over not only the last-position KL divergence, but
also for all the intermediate tokens. Intuitively, the random length test example alloWs the in-context
predictor to “take credit” for fitting the intermediate tokens. The proof is immediate given the KL
bound and Lemma 2, given that the length of xtest is uniformly random betWeen 2 to k.
Proof. Let the set of θ that does not satisfy Condition 1 tobe B. We have for any θ in B that
Ex	test~Pprompt[KL(PPrompt (ytest | xtest) ∣∣p(ytest | xtest,θ))]	(114) 1k ≤ k-1 EEO[1j-1]~pprompt KL(PPrOmPt(O [j]∣O[1: j - 1]) ∣p(O[j] |O[1: j - 1],θ)) k-1j=2 (115) ≤ SUpθ (eθart + eθelim)	(116) k-1
by Theorem 1 and Condition 1. Plugging this into Lemma 2 gives the result.	□
F Experimental details
F.1 GINC dataset
Pretraining distribution. We consider a pretraining distribution from a mixture of HMMs With an
interpretable hidden state structure and emission distribution. The HMM hidden state ht = [st,vt] at time
t is composed of an entity vt ∈ {1,...,|V |} (e.g., Einstein) and a property st ∈ {1,...,|S|} (e.g., nationality,
first name, last name, other grammatical tokens). We model the entities and properties as independent
Markov chains (i.e., a factorial HMM (Ghahramani & Jordan, 1997)), While the emissions depend on
both. In pretraining documents, We expect that the entities (e.g., Einstein) change sloWly over time While
and the properties of the entity (e.g., their nationality) change quickly With some pattern to generate
20
Published as a conference paper at ICLR 2022
Pretraining document
f / h x ax o a k au ap /
a o u au ae f ao an / ah
UyaSakaUj w ax 1
aw r ae au g au ap / / u
aj ae d a h x af u aj i
rjwjasyxniap
In-context Prompt
1 aw ac / ax aj ae / ac j
Properties (S)				
Newline	First name	Last name	Nationality	Linking verb
∖n	Albert	Einstein	German	was
∖n	Mahatma	Gandhi	Indian	was
Figure 8: Example pretraining document snippet (Left) and example prompt with 3 training examples,
1 test example, and example length 3 (Right). The delimiter token is the backslash.
Figure 9: The GINC dataset generates sequences from a mixture of HMMs. The HMM hidden states
consist of entities (v) and properties (s), which index into a memory matrix to produce the observed
token. The entity and property sequences are sampled from independent Markov chains. The concept
parameter θ is the transition matrix for properties, which defines relations between properties. In this
example, the sequence of properties [2,3,5,4] relates names to nationalities, defining the in-context
task. The blue color represents hidden states/observations sampled from the prompt distribution, and
the purple color represents hidden states/observations sampled from the pretraining distribution.
natural sentences. We implement this by ensuring that the probability of transitioning to the same entity
index in the next step is at least 0.9. The emission distribution depends on a memory matrix M with |V|
rows and |S| columns (Figure 9). At step t, we use the entity vt and property st to index into the memory
matrix. In particular, the observed tokens are deterministic withp(ot|ht) = 1 ifot = M[vt,st]. This
construction satisfies the structure on delimiter states (Assumption 1). We ensure that all the transitions
have nonzero probability and use a uniform prior over concepts, satisfying Assumptions 2 and 5.
Concept parameter. The concept parameter is the property transition matrix, while the entity
transition matrix is fixed for all concepts. The prompt start distribution and the concept together
determine the in-context task. We define a uniform mixture of HMMs over a family Θ of5 concepts
to generate 1000 documents with 〜10 million tokens total.
Vocabulary. The GINC dataset is generated from a mixture of HMMs. These HMMs output tokens
from a vocabulary of size in {50,100,150}. The vocabulary contains a special delimiter token (backslash
21
Published as a conference paper at ICLR 2022
35
30
25
20
0	20	40	60
Num examples
k=3
k=5
k=8
k=10
<
90
80
70
60
50
40
0	20	40	60
Num examples
-k=3
- k=5
-k=8
-k=10
Figure 10: In-context accuracy curve ofthe4 layer Transformer on the GINC dataset when the entity
transition matrix does not have an additional identity component, for vocabulary sizes 50 (left), 100
(middle), and 150 (right). In-context learning is still generally successful.
-See Figure 8, designated to be index 1. The vocabulary is generated as combinations of letters starting
from a to z, then aa to az, and so on. All sequences are tokenized by splitting on whitespaces.
Memory matrix. The shared memory matrix has 10 entities and 10 properties, totaling 100 entries
(corresponding to 100 hidden states). The first column of the memory matrix is fixed to be the delimiter
token, while each remaining entry of the shared memory matrix is populated with a token sampled
uniformly from the vocabulary.
Transition matrix for properties. We generate 5 property transition matrices, one for each
component of the HMM mixture. We generate each transition matrix via a convex combination of
100 random permutation matrices. The weights of the convex combination are randomly generated as
softmax((u-0.5)/t)	(117)
where u∈R100 has uniform random entries in [0,1] and t is a temperature parameter, set to 0.1.
Transition matrix for entities. The entity transition matrix is shared between all the HMMs that
consistute the mixture. The entity transition matrix is generated in the same way as the property
transition matrices, except with one additional step. Letting T be a transition matrix sampled in the
same way as a property transition matrix,
In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while
and the properties of the entity (e.g., their occupation) change quickly with some pattern to generate
natural sentences. We implement this by ensuring that the probability of transitioning to the same entity
index in the next step is at least 0.9. The final entity transition matrix is then 0.1T + 0.9I where I is
the identity matrix. Although we add the diagonal component for added realism, we also consider not
adding this component. Figure 10 shows in-context learning curves for a small (4 layer) Transformer
trained on data that does not add the diagonal component (we check this for vocabulary sizes 50, 100,
and 150). In-context learning still works in this case, although not as well for the 50 vocab size case.
Start distribution. The starting distribution for the hidden states in all HMMs in the mixture are
close to uniform. We generate the start distribution as softmax((u-0.5)/t) for random vector u with
entries uniformly from [0,1] and temperature t = 10. In the pretraining documents, we only sample
from the start distribution in the beginning of the document.
Prompt distribution. To generate the prompts, we first sample a concept θ uniformly at random
from Θ (well-specification, Assumption 4), then use it to generate all the prompt examples. The prompt
start distribution is chosen to be uniform over entities but with a fixed starting property that is chosen
randomly for each prompt, for consistency in the task. This may not satisfy Assumption 3, but we
found this to still work empirically and is simpler. Given the starting property, we sample k tokens from
the HMM defined by the concept θ. Finally, we append the delimiter token for the example. We repeat
this process for each example in the prompt, concatenating all examples. The label is generated as
argmax pprompt(y|xtest)	(118)
y
under the prompt concept θ*. This differs from the theory, which samples ytest instead of taking it to be
the most likely token. However, there can be a large amount of intrinsic error that sampling introduces.
We define the label this way in the simulations to remove the intrinsic error from sampling.
22
Published as a conference paper at ICLR 2022
Example of prompt generation. In the example in Figure 8 (right), the starting property is fixed to be
5 (for example). The first token (l) is generated by sampling a random entity index (3), and indexing into
the memory matrix returns l. Running the hidden state chain of the HMM forward gives the next pair
of property and entity. Since the entity Markov chain changes slowly, the entity is still 3 in the next step
-however, the property has changed to 4, and indexing into the memory matrix outputs the next token
(aw). Following this same process to generate the third token (the output for the first example), we finish
generating one example. To end the example, we append a delimiter (backslash). We repeat this example
generation process for all the examples, except for the test example at the end, where we do not generate
the last token. We condition the HMM on the generated prompt to compute the posterior distribution
over the next token pprompt(y|xtest). We take the argmax of this distribution to be the ground truth label.
Dataset details. The dataset contains 1000 training documents and 100 validation documents, where
training documents have 10240 tokens and validation documents have 1024 tokens. Each document
is generated by first selecting one of the HMMs from the mixture uniformly at random, then generating
10240 tokens from the HMM.
We also generate 2500 in-context prompts for each (example length,number of examples) pair, for
example lengths k = [3,5,8,10] and number of examples n = [0,1,2,4,8,16,32,64]. Each prompt is
generated using a random HMM in the mixture.
F.2 Transformer details
Our Transformer models are based on the GPT-2 architectures with 4, 12, and 16 layers respectively,
with 12 attention heads, 768 dimensional embeddings, residual/embedding/attention dropout set to 0.1,
and a context window of 1024. Other than the number of layers, the other parameters are the default
settings from the HuggingFace library (Wolf et al., 2019). We train for 5 epochs using the AdamW
optimizer (Loshchilov & Hutter, 2019; Kingma & Ba, 2015) with a batch size of8 and a linear learning
rate schedule (with 1000 step warmup) up to a learning rate of 8e-4 for the 4 layer and 12 layer model,
while for the 16 layer model we start with a constant learning rate of 8e-4 and reduce by a factor of
0.25 whenever the best validation loss does not improve. We tried both learning rate strategies for
all models and take the most consistent. We tuned these models so that the training loss curves between
seeds have smaller variability between the runs in terms of the curve shape and when the loss decreases
-we found that this is an important indication of stable results. The models took 50 minutes, 2 hours,
3 hours to train respectively. The hardware was mainly Titan Xp GPUs, trained and evaluated using
16-bit precision. All the results are reported with 5 pretraining runs (5 different seeds).
F.3 LSTM DETAILS
We train an LSTM language model with embedding size 768, hidden layer size 768, and 6 layers. We
use dropout 0.2 and weight decay 1e-5. The optimizer is AdamW starting with a learning rate of 1e-3,
then reducing by a factor of 0.25 whenever the best validation loss does not go down. We train for
a total of 10 epochs, with gradient clipping at norm 1.0. We use a batch size of 8 and backpropagate
through time for 1024 steps (each pretraining data segment is also 1024 tokens). Each model takes
roughly 2 hours to train on Titan Xp GPUs.
F.4 Varying the vocabulary size
To do well on the in-context learning task, the model must both infer the prompt concept and the last
HMM hidden state. In general, increasing the number of observable symbols makes the in-context task
easier by making the inference of the HMM hidden state easier. With more symbols, each hidden state
is more likely to output a different symbol, making the inference problem easier. This improvement
comes despite the number of output classes in the problem (same as the vocabulary size) increasing.
Figures 11, 12, 13, 14 show in-context learning curves for vocabulary sizes 50, 100, and 150, keeping
other hyperparmeters of the dataset the same.
F.5 Experiment on GPT- 3
We conduct an additional experiment which shows that longer examples improve in-context learning
in GPT-3 on the LAMBADA (Paperno et al., 2016) completion task.
Data. In this experiment, we define a short version of the LAMBADA test dataset (LAMBADA
test-short) which contains only test examples with up to 200-300 characters in length. We also
define two “training” datasets from which to sample examples for the in-context prompts from.
The short training dataset (LAMBADA train-short) contains examples from the training set that are
23
Published as a conference paper at ICLR 2022
llʌ	I
ικ¾^-^^Mmsg^ME⅜⅛^
O 20	40	60	O 20	40	60
Num examples	Num examples
k=3
k=5
k=8
k=10
Figure 11:	In-context accuracy ofthe4 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
k=3
k=5
k=8
k=10
k=3
k=5
k=8
k=10
-k=3
一 k=5
—«— k=8
-→- k=10
Figure 12:	In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
k=3
k=5
k=8
k=10
k=3
k=5
k=8
k=10
Figure 13:	In-context accuracy of the 16 layer Transformer on the GINC dataset for vocabulary sizes
50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
0	20	40	60
Num examples
0	20	40	60
Num examples
-k=3
-k=5
-∙- k=8
-→- k=10
Figure 14:	In-context accuracy of the LSTM on the GINC dataset for vocabulary sizes 50 (left), 100
(middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
200-300 characters in length, which matches the distribution of test-short. The long training dataset
(LAMBADA train-long) contains training examples that are 500-600 characters long. We cut the
number of examples in the larger of the two training datasets so that the two training datasets are equally
sized (47 examples). For each test example, we sample 5 random training examples (5-shot learning).
We also consider equalizing the total length of the prompts in two ways. First, we consider duplicating
the 5 short examples (if the examples are [1,2,3,4,5], duplicating refers to [1,2,3,4,5,1,2,3,4,5]).
This allows for equalizing the total length without increasing the number of examples. As a skyline
comparison, we also consider sampling 10 independent short examples, which contains more
input-output pairs for the task.
Result. Table 1 shows that when evaluating only on LAMBADA test-short, 5-shot in-context
learning using LAMBADA train-long improves the test accuracy by almost 1% compared to
LAMBADA train-short, despite the long/short distribution mismatch between train and test. This
supports intuitions from our theory.
24
Published as a conference paper at ICLR 2022
Prompt example length	Test Acc (200-300 chars)
5 examples	
Short (200-300 chars)	69.8
Long (500-600 chars)	70.7
10 examples	
Short, duplicated examples	69.6
Short, independent examples	71.4
Table 1: Accuracies for 5-shot in-context learning of GPT-3 on a filtered LAMBADA test set with short
examples (200-300 characters). Even though there is distribution mismatch with the test set, having
longer examples improves the accuracy, supporting theoretical intuitions. The first two rows use 5 train-
ing examples in the prompt, while the last two rows use 10 training examples to equalize the total length.
In comparison, simply increasing the total prompt length by duplicating the short examples does not
improve the accuracy. Intuitively, the longer examples have additional information that is not directly
related to mapping between the input and output, but can be leveraged to improve in-context learning
by helping the model infer the latent concept. Using 5 long examples (as opposed to 5 short examples)
closes about 56% of the gap between using 5 short examples and 10 independent short examples
despite not adding additional examples or task-related information.
25