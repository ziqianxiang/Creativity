Published as a conference paper at ICLR 2022
OTTER: Data Efficient Language- Supervised
Zero-Shot Recognition with Optimal Trans-
port Distillation
Bichen Wu1 ∖ Ruizhe Cheng2 ∖ Peizhao Zhang1, Peter Vajda1, Joseph E. Gonzalez2
1Meta Reality Labs, 2UC Berkeley
{wbc,stzpz,vajdap}@fb.com,{chengruizhe,jegonzal}@berkeley.edu
Ab stract
Traditional computer vision models are trained to predict a fixed set of predefined
categories. Recently, natural language has been shown to be a broader and richer
source of supervision that provides finer descriptions to visual concepts than
supervised "gold" labels. Previous works, such as CLIP, use InfoNCE loss to train
a model to predict the pairing between images and text captions. CLIP, however,
is data hungry and requires more than 400M image-text pairs for training. The
inefficiency can be partially attributed to the fact that the image-text pairs are
noisy. To address this, we propose OTTER (Optimal TransporT distillation for
Efficient zero-shot Recognition), which uses online entropic optimal transport to
find a soft image-text match as labels for contrastive learning. Based on pretrained
image and text encoders, models trained with OTTER achieve strong performance
with only 3M image text pairs. Compared with InfoNCE loss, label smoothing,
and knowledge distillation, OTTER consistently outperforms these baselines in
zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled
ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on
7 different dataset/architecture settings x 6 metrics, OTTER outperforms (32) or
ties (2) all baselines in 34 of them. Our source code is open sourced at https:
//github.com/facebookresearch/OTTER.
1	Introduction
In real-world image recognition tasks, input images come from a broad range of distributions,
spanning tens of thousands of object categories unknown during training. It is thus important for
computer vision models to generalize to a large number of visual concepts that may or may not be
present in the training data. This problem is called zero-shot learning (ZSL), which aims to transfer
knowledge from some known classes with training data to a much larger number of unfamiliar classes.
Previous works on ZSL have explored using attributes (Romera-Paredes & Torr, 2015; Akata et al.,
2015; 2013), class hierarchy (Wang et al., 2018; Kampffmeyer et al., 2019), and pretrained word
embeddings (Frome et al., 2013; Norouzi et al., 2014) to transfer knowledge from pretrained image
representations to recognize new classes. Recently, natural language has been used as a powerful
source of supervision for visual representation learning. (Desai & Johnson, 2020; Sariyildiz et al.,
2020; Zhang et al., 2020; Jia et al., 2020) demonstrate the effectiveness of pretraining on image-text
data. Among them, CLIP (Radford et al., 2021) applies natural language supervision to zero-shot
image recognition. It collects an enormous dataset with over 400M image caption pairs from the
Internet, and trains an image encoder and a text encoder jointly with a contrastive loss to maximize
the cosine similarity of paired image and text embeddings. CLIP demonstrates good zero-shot
classification results on a wide range of downstream image classification datasets. However, a main
constraint of CLIP is that it requires over 400M image-text pairs for training. Collecting and training
on such a huge dataset is very expensive. The inefficiency can be partially attributed to the fact that
the training labels from image-text pairs are noisy. As shown in Figure 1, in a typical image-text
dataset, we observe that images and captions are loosely correlated. It is very common that one
caption (image) can potentially match several other images (captions), and the ground-truth pairing is
* Equal contribution
1
Published as a conference paper at ICLR 2022
one of the	aerial:
Original	most	woman
dramatic	waving her
CaPtiOnS	mountain	arms on the
ranges I have	rock
seen
CinematiC View of a Iake
aerial shot of and pine
the dramatic forest
coastline at
the cliffs
a forest of
stunted trees
that stand in
sharp
contrast _
landscape
with mown
grass and a
haystack
newly built
small house
next to the
sea and the
beach
the public
house
traditional
pub in old
building on
corner
a cottage in
the
picturesque
village
cinematic
Permuted aerial shot of
Captions
the dramatic
coastline at
the cliffs
one of the
most
dramatic
mountain
ranges I have
seen
aerial:
woman
waving her
arms on the
rock
landscape
with mown
grass and a
haystack
view of a lake
and pine
forest
a forest of
stunted trees
that stand in
sharp
contrast _
a cottage in
the
picturesque
village
newly built
small house
next to the
sea and the
beach
the public
house
traditional
pub in old
building on
corner
Figure 1:	Images and captions are only loosely correlated in many image-text datasets. The ground-
truth pairing is not the only sensible match between texts and images. In the example above, we can
find permutations of text captions that can still match with original images.
not the only sensible match. Note that examples in Figure 1 are not hand-picked special cases. In
fact, such noisy image-text matching is prevalent in image-text datasets.
To quantitatively analyze this, we use a CLIP(Radford et al., 2021) VIT-B/32 pretrained on OpenAI’s
400M dataset to estimate the matching probabilities between a batch of paired image-text samples.
Specifically, we randomly sample 1000 batches from the CC3M (Sharma et al., 2018) and YFCC15M
(subset of YFCC100M (Thomee et al., 2016)) datasets, and use the pretrained CLIP model to compute
the image-to-text matching probabilities by taking the dot-product of the feature embeddings and
taking a softmax along each row. For each batch, we compute three statistics (averaged across rows):
default probability, non-default max probability, and non-default average probability. Note in both
datasets, the matching probability between paired samples are far smaller than 1.0, and the probability
decreases with the batch size. This indicates that there exist image and text samples that are not
paired, but have nontrivial matching probabilities. This is further confirmed by the max matching
probabilities between unpaired samples. In the extreme cases (CC 3M, 2048 batch size), the average
of max matching probability between unpaired image-text samples is very close to the average of
probability of paired samples. Despite prevalent noisy matching between images and texts, CLIP uses
the InfoNCE loss (Hadsell et al., 2006) for training and uses the ground-truth pairings as hard labels.
This ignores the many-to-many relationship within a batch of images and text captions, leading to
noisy training signals and lower data efficiency.
Table 1: Matching probabilities estimated by CLIP on Conceptual Captions and YFCC
Dataset	Batch Size	Paired	Unpaired Avg	Unpaired Max
	512	0.565	0.001	0.215
CC 3M	1024	0.480	0.001	0.230
	2048	0.398	0.000	0.238
	512	0.628	0.001	0.197
YFCC 15M	1024	0.551	0.000	0.219
	2048	0.469	0.000	0.239
To address this, we propose OTTER, or Optimal TransporT distillation for Efficient zero-shot
Recognition. We improve InfoNCE to consider the many-to-many relationship between unpaired
images and texts. Specifically, given a batch of image and text tuples {(vi, ti)}i=1:N, we first use
image/text encoders to estimate a similarity matrix whose elements denotes similarity from image
vi to text caption tj . Based on the similarity matrix, we use optimal transport to find a matching
probability between each possible image-text combination. To model the many-to-many relationship,
we add an entropic regularization to the optimal transport so that the match is softly assigned.
Entropic-regularized optimal transport can be solved efficiently with the iterative Sinkhorn-Knopp
algorithm (Cuturi, 2013). Finally, we use the match as soft label to train the image and text encoders.
Based on pretrained image and text models, we use OTTER to train zero-shot models on the
Conceptual Captions (CC) (Sharma et al., 2018), (subset of) Wikipedia-based Image Text (Srinivasan
et al., 2021), and YFCC 15M (Thomee et al., 2016) datasets, which contain 3M, 5M, and 15M
image-caption pairs, respectively. We evaluate the image encoder’s zero-shot recognition of common
visual concepts on Google Open Images (GOI) (Kuznetsova et al., 2020) (19,958 categories) and
2
Published as a conference paper at ICLR 2022
multi-labeled ImageNet 10K (10032 categories) from Tencent-ML-Images (Wu et al., 2019a). Over
42 evaluations on 7 different dataset-architecture settings × 6 metrics, OTTER outperforms (32) or
ties (2) all baselines in 34 of them. We also propose a quantitative vision-language compositionality
benchmark and show comparable results to CLIP in Appendix D.
2 Related Works
Zero-Shot Learning in Computer Vision: Zero-shot learning (ZSL) studies the generalization of
knowledge to unseen classes. Previous methods for zero-shot recognition in computer vision mainly
follow three paradigms. The first type, including DeViSE (Frome et al., 2013) and ConSE (Norouzi
et al., 2014), uses pretrained word embedding vectors to represent different categories and implicitly
model their relationships. However, word embedding is a preliminary and limited representation
of class relationships, which hurts performance. The second paradigm, including GCNZ (Wang
et al., 2018), DPGZ (Kampffmeyer et al., 2019), and HZSL (Liu et al., 2020), explicitly models
class relationships as a graph, and uses a graph convolutional network (GCN), or a predefined class
hierarchy, such as WordNet (Feinerer & Hornik, 2020), to learn the knowledge propagation between
classes. However, real-world class relationships are complicated and simple graph structures such as
WordNet are too limited to model such relationships. Lastly, (Romera-Paredes & Torr, 2015; Akata
et al., 2015; 2013) rely on human-labeled attributes to model semantics of classes. The scalability of
these methods are limited by the need for attribute annotations. More recently, CLIP (Radford et al.,
2021) applies language-supervision to ZSL by training on image caption pairs. Our work is based on
CLIP and we generalize the InfoNCE loss to improve its data efficiency.
Vision and Language: Natural language has long been used as a source of supervision in fields
like image-text retrieval Hironobu et al. (1999), object classification Wang et al. (2009), and video
understanding (Ramanathan et al., 2013). Socher et al. (2014); Karpathy et al. (2014); Li et al.
(2019); Chen et al. (2021a) have proposed methods of learning visual and language representations
in a joint embedding space. More recently, (Lu et al., 2019; Chen et al., 2020c; Qi et al., 2020)
propose using a cross-modal attention mechanism to increase performance in image-text matching.
In addition, (Joulin* et al., 2016; Li et al., 2017; Desai & Johnson, 2020; Sariyildiz et al., 2020)
demonstrate that good visual representations can be learned by predicting image captions. To scale
up vision-language joint training, CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2020) both
collect their own image-text datasets with 400M and 1B image-caption pairs.
Optimal transport (OT) is a theory that enables comparison of two probability distributions whose
supports may not overlap. OT has been applied to many areas such as domain adaptation (Courty
et al., 2016), generative models (Salimans et al., 2018), and self-supervised vision representation
learning (Caron et al., 2020; Asano et al., 2019). In vision and language, (Chen et al., 2020a) uses
OT to align objects in images and words in texts. The problem formulation of our work is similar to
(Damodaran et al., 2018), where OT is used to mitigate the label noise in remote-sensing data under
supervised learning. In our paper, we extend the method from supervised learning to contrastive
learning, where OT is a natural way to estimate pairings between images and texts. In another related
work, (Chen et al., 2021b) adds an additional OT-based Wasserstein loss to contrastive representation
distillation (Tian et al., 2019). The loss matches student representations to teacher representations in a
batch. (Chen et al., 2021b) is different from our method since it directly minimizes the Wassertein loss
between two models’ representations, while our method uses OT to estimate the pairing probability
and use the probability for knowledge distillation. Directly minimizing Wasserstein loss between
image/text embeddings in our case will lead to collapsed representations, where models generate
constant output regardless of inputs.
Other related works: Our work is also related to areas including learning with noisy labels,
contrastive learning, and knowledge distillation. Our method uses OT to estimate the matching
probability of unpaired images and texts. This is reminiscent to estimating label transition probability
under noisy labels (Song et al., 2020). Our method is based on contrastive learning, which is
commonly used in self-supervised visual representation learning (Chen et al., 2020b; He et al., 2020).
For vision representation learning, (Robinson et al., 2020) argues that sampling hard negative pairs
can improve learning efficiency. For language-supervised representation learning, however, it is
important to mitigate the noise of widely spread hard negative samples, since positive image-text
pairs are usually only loosely correlated. Our method is also an extension to knowledge distillation
(KD) (Hinton et al., 2015). Typical KD directly relies on a teacher model to directly generate a
target distribution (Xie et al., 2020; Bagherinezhad et al., 2018; Caron et al., 2021). Our method is
3
Published as a conference paper at ICLR 2022
different since our target distribution is computed by OT based on the pairwise similarity estimated
by a teacher model. Experiments show that this works better for image-text contrastive learning.
Fly over sea
EMA update
Image encoder
Image Text
similarity	similarity

Dog on boat
Mount & sea
Cliff & sea
Text encoder
EMA update
Predicted
distribution
Cross
Entropy
Image-text^
similarity
Optimal
Transport
Target
distribution
Mυ*


Figure 2:	Architecture of OTTER. We use image and text embeddings to compute similarity matrices
Sv (and St), which is then used to solve for matching probabilities Mv* (and Mt*) as targets.
3	Methods
We introduce OTTER in this section. Let {(vi, ti)}i=1:N be a batch of paired image-text tuples
sampled from data distributionp(v, t). Our model contains an image encoder fv (∙) and a text encoder
ft(∙) that map image Vi and text ti to '2-normalized embeddings Zv and Zi respectively.
3.1	Contrastive Learning with InfoNCE Loss
CLIP (Radford et al., 2021) trains the image and text encoders with contrastive learning to pull the
paired image and text embeddings closer, and push the unpaired embeddings farther. This is achieved
by miminizing the InfoNCE loss LInfoNCE = Lv + Lt . Lv is the loss for matching images to text
captions, and Lt is for text-to-image matching. Lv is defined as
1 ££	t	1 ££	exp((zv>zj)∕τ)
Lv = -NXXIijlogPv(Zv，zj；T) = -NXXIijlOgPN--------------------	> t ,	⑴
i=1 j=1	i=1 j=1	kN=1 exp((ziv>ztk)∕τ)
where (zv>zj) is the cosine similarity between two '2-normalized embedding vectors. T is a
(trainable) temperature parameter. Iij is the element of an identity matrix IN with Iii = 1, ∀i
and Iij = 0, ∀i = j. Note Pv is normalized across Zk for k = 1, ∙∙∙ ,N in the denominator.
Symmetrically, we define Lt and pt in the same way as Equation (1) except we normalize across zvk .
Equation (1) is a rather redundant way of writing the InfoNCE loss, as Iij are all zeros for unpaired
image-text samples. However, this shows that InfoNCE is essentially the cross entropy between a
one-hot distribution Iij and the estimated probability Pv(ziv, ztj； T). One-hot distribution assumes
that within a batch of images and text captions, the only match for image vi is its paired text caption
ti. However, as shown in Figure 1 and Table 1, this assumption is not true. Paired images and text
captions are only loosely correlated. It is common that one image can match with several other texts
and vice versa. The ground-truth match provided by the dataset is not the only sensible match between
images and texts. One-hot labels are therefore noisy, leading to degraded learning performance.
3.2	Modeling the Probability of Unpaired Image-Text Matching
To better capture the many-to-many relationship in image-text datasets, we modify InfoNCE in
Equation (1) to consider the matching probability of unpaired images and texts. For a batch of N
image-text pairs, we define Yi ∈ {1, . . . , N} as a random variable, and let qv(Yi = j|v1:N, t1:N) be
the probability that image vi should be matched with text caption tj in the batch. We model this as
qv (Yi = j|v1:N, t1:N) = qv(Yi = i)Iij + qv(Yi 6= i)Mij = αIij + (1 - α)Mij (2)
4
Published as a conference paper at ICLR 2022
where Mij := qv(Yi = j|Yi 6= i, v1:N, t1:N), Mii = 0 ∀i is the conditional probability of image vi
being matched to tj given that it is not matched to text ti. For simplicity, we write qiv (j) := qv (Yi =
j|v1:N, t1:N). α ∈ [0, 1] is the prior probability that image vi is matched with its paired text caption
ti. α reflects the noise level in the dataset. In an ideal noiseless dataset, α = 1, so qiv (j) = Iij.
This is the case where we should use the one-hot labels Iij for contrastive learning. However, in
image-text datasets, it is common for an unpaired text caption tj to be a better match for image vi , as
shown in Table 1. In this case, α < 1 and using Iij as the target distribution is no longer accurate. So
we generalize the InfoNCE loss in Equation (1) by replacing Iij with the more generic qiv(j) as
NN
Lv = - N XX[αIij +(I- α)Mivj ]log Pv (Zv, Zj;τ),	⑶
N i=1 j=1
αIij provides supervision on paired image-text samples and (1 - α)Mivj supervises unpaired samples.
The question is how do we estimate Mivj. A simple estimation is to let Mivj = (1 - Iij)/(N - 1) ∀i, j
be a uniform distribution. This is equivalent to the label smoothing method proposed in (Szegedy
et al., 2016). However, this completely ignores the contents of images v1:N and texts t1:N.
3.3	Modeling with Optimal Transport
To design a better method of estimating Mivj , we start from two intuitions: first, in a reasonable
image-text dataset, there are no bad images or texts. We assume all the images and texts are equally
matchable so they should have equal matching probabilities. Second, the matching probability from
image vi to caption tj should depend on their similarity estimation Sij . A relatively higher similarity
Sij should lead to higher matching probability Mivj . An estimation for Mivj that satisfies the two
intuitions can be obtained by solving the following entropic optimal transport problem (Cuturi, 2013)
Mv* = argmaxhM, Sv〉f + λH(M).	(4)
M∈M
Sv ∈ RN×N is a similarity matrix whose elements Sivj denotes the similarity from vi to tj. We
discuss how to compute Sv in Section 3.4. hM, SviF = Pij Mij Sivj is the Frobenius inner product
between the similarity matrix Sv and the matching plan M. Maximizing this term ensures M is
similar to Sv , i.e., larger Sivj leads to larger Mij and vice versa. Meanwhile, we add an entropy
regularization on M as H(M) = - Pij Mij log Mij . This ensures that M does not over concentrate
on a few elements. We constrain the solution of Equation (4) to be a transportation polytope
M = {M ∈ RNXN | M1n = N 1n, M>1n = N 1n}.	(5)
This constraint ensures that the solution Mv* satisfies the first intuition - all images and texts are
equally important and should be matched with equal probabilities. Moreover, as proven in (Cuturi,
2013), the solution to Equation (4) takes the form of a normalized exponential matrix
Mv* = Diag(r) exp(Sv/λ)Diag(c),	(6)
where r, c ∈ RN are row and column normalization vectors and can be calculated through the
iterative Sinkhorn-Knopp algorithm (Cuturi, 2013). The Sinkhorn-Knopp algorithm can be efficiently
implemented on GPU and we provide a pseudo-code implementation in Appendix B.
From Equation (6), it is clear that Mv * satisfies our second intuition that a similarity Sij leads to
higher matching probability since MVj 〜exp(Sj/λ). The role of the entropic regularization is also
clear. A larger λ or higher entropy regularization and leads to "softer" distribution for Mivj*. On the
other hand, a smaller λ or lower entropy regularization leads to "harder" distribution for Mivj*.
3.4	Computing the Similarity Matrix
To compute the similarity from image Vi to text tj-, We can use a Pair of teacher encoders fv (∙) ,ft (∙)
to compute '2-normalized embeddings Zv, Zj. Denoting Zv, Zt ∈ Rd×N as matrcies whose columns
are Zv：N, Z；：N respectively, we compute the similarity matrix as
Sv = γv z v> z v + γtZt>Zt + z v>Zt - ηiN.	(7)
The first term Zv>Zv ∈ RN×n compares the image similarities, as (Zv>Zv)j = Zv>Zv is the
cosine similarity between image embeddings. Intuitively, it assumes that for a pair of similar images,
it is likely that we can exchange their text captions. Similarly, Zzt>Zzt compares the text similarities.
5
Published as a conference paper at ICLR 2022
It assumes that if a pair of text captions are similar, it is more likely that one text caption can match
the other image. The term Zv>Zt considers the similarity between the image and text embeddings.
Finally, ηIN with η → ∞ ensures the diagonal terms of Sv are infinitely small. This effectively sets
the diagonal terms of Mv* to 0, which is necessary since Mij is conditioned on 匕=i.
There are several options to instantiate fv(∙) and ft(∙). The simplest option is to use the original
image and text encoder fv(∙),ft(∙) as fv(∙), ft(∙). Alternatively, following recent works (He et al.,
2020; Caron et al., 2021; LiU et al., 2021), fv(∙), ft(∙) can share the same model architecture with
fv (∙), ft(∙), but their weights are updated as an exponential moving average as θ  mθ + (1 - m)θ,
where θ is the weight for fv(∙), ft(∙), θ isthe weight for fv (∙), ft(∙), and m is a momentum parameter
set to 0.999. Of course, We can also use trained image and text encoders such as CLIP for fv (∙) and
ft(∙). We adopt the first two options in our paper, since we want to avoid using extra image-text pairs.
3.5	Relationship with Knowledge Distillation
OTTER is an extension of conventional knowledge distillation (KD) (Hinton et al., 2015). Equation
(3) computes the cross entropy H(qiv, piv) between qiv (j) and piv (j) := pv (ziv, ztj; τ), where qiv(j) is
the teacher distribution solved by OT and pv (j) is the student distribution with logits (ziv>zjt)∕τ
computed by f (∙)v, f (∙)t. A more conventional way to compute KD's teacher distribution is
qv (zv, Zj; τ)
eχp((zv>zj"τ )
PN=I exp((Zv>Zk)∕τ)
(8)
where zv, Zj are computed by the teacher fv (∙), ft(∙). We can re-write Equation (8) in the matrix form
as Qv = Diag(r) exp(Zv>Zt∕τ)Diag(c), where r = 1, and Ci = 1/ PN=I exp((Zv>Zk)∕τ). Note
this teacher distribution has the same form as OTTER in Equation (6), but with two differences. First,
OTTER’s similarity matrix Sv in Equation (7) have three more terms: γvZZv>ZZv, γtZZt>ZZt, ηIN . In
comparison, KD ignores image-image, text-text similarities and does not exclude diagonal terms. By
setting γv = γt = η = 0, their similarity matrices are equivalent. Second, OTTER’s normalization
vectors r, c in Equation (6) are solved with Sinkhorn-Knopp while for KD r, c are computed by a
Softmax function. In fact, if we set the #iteration to 0 in Algorithm 2 (Appendix B), Sinkhorn-Knopp
is equivalent to Softmax, as also noted by (Caron et al., 2021).
4	Experiments
In this section, we discuss our experiments validating the effectiveness of OTTER. We open-sourced
our code at https://github.com/facebookresearch/OTTER. To setup a baseline, we
follow CLIP (Radford et al., 2021) to train an image and a text encoder to predict the pairing of
image and text samples using the infoNCE loss. Since the dataset used by CLIP is not released,
we train on three publicly available datasets, Conceptual Captions 3M (CC) (Sharma et al., 2018),
Wikipedia-base Image-Text Dataset (WIT), and YFCC 15M (Thomee et al., 2016). We only train on
images with English captions in all 10 partitions of the WIT dataset, resulting in 5M image-text pairs
in total. Since the datasets we use are small (〜100x smaller than the one used by CLIP), we have to
use pre-trained models to initialize the image and text encoders. Also, due to the datasets’ limited
scale and concept coverage, models trained on CC or WIT do not perform well on domain-specific
datasets such as Stanford Cars (Krause et al., 2013) and FGVC Aircraft (Maji et al., 2013). To test
zero-shot recognition on common visual concepts, we evaluate our models on the test set of Google
Open Image (GOI) (Kuznetsova et al., 2020), which contains 19,958 classes. We also evaluate on
the test set of multi-labeled ImageNet 10K (10032 classes) dataset whose labels come from Tencent
ML-Images (Wu et al., 2019a). Each image in ImageNet 10K is auto-labeled with highly-correlated
class labels from GOI, alleviating the single-label issue of ImageNet 21K and 1K. To compare with
previous ZSL methods (Norouzi et al., 2014; Frome et al., 2013; Wang et al., 2018; Liu et al., 2020),
we report the ZSL performance of one of our models on ImageNet21K+1K.
Training: We adopt a training recipe similar to BiT’s finetuning strategy (Kolesnikov et al., 2020):
We use SGD with an initial learning rate of 3e-3, a cosine annealing scheduler, momentum 0.9, and
no weight decay. Input images are resized to 256x256 and randomly cropped to 224x224 while test
images are resized to 256x256 and center-cropped to 224x224. We train on 8 V100 GPUs using
Pytorch (Paszke et al., 2019) distributed data parallel with a total batch size of 512 (64 per GPU) for
6
Published as a conference paper at ICLR 2022
10 epochs. While CLIP (Radford et al., 2021) computes InfoNCE using sub-batches on each GPU,
we gather logits from all GPUs for OTTER and baselines.
Inference: For inference, we follow CLIP to compute the text embeddings for the target classes
using the trained text encoder, and we use a prompt template of “a photo of {label}" to augment the
label texts. Next, we fit a KNN using the text embeddings. Given an image, we find the top K nearest
label embedding neighbors to the image embedding based on cosine similarity.
Evaluation: GOI (Kuznetsova et al., 2020) and ImageNet 10K from Tencent-ML-Images (Wu et al.,
2019a) are multi-labeled. Following previous work on ZSL (Norouzi et al., 2014; Frome et al., 2013;
Wang et al., 2018; Liu et al., 2020), we use flat hit @ k (FH@K) for evaluation. FH@K is the
percentage of test images such that the top K predictions of the model overlap with true labels and is
formally defined as N Pi=1 i({{f(vi)}κ ∩ Li = 0}), where {f (vi)}κ is the top K predictions
for the i-th image and Li is the set of true labels.
Table 2: FH@K on test sets of Google Open Images and ImageNet10K from Tencent-ML-Images.
Data	Image encoder	Text encoder	Method	GOI FH@K(%)			IN10KFH@K(%)		
				1	5	10	1	5	10
CLIP	ResNet50	CLIP	InfoNCE	26.5	54.0	64.3	20.1	44.8	56.4
(400M)	ViT-B/32	Transformer		27.5	55.3	65.4	22.5	49.1	60.7
	Wide ReSNet50x2		InfoNCE	28.6	58.6	69.8	11.0	29.9	40.6
			InfoNCE	26.8	55.1	66.4	10.9	29.4	40.5
	ReSNet50		LS	26.3	55.9	67.5	10.1	29.6	39.8
			KD	26.7	55.3	67.1	10.0	27.5	38.5
			OTTER	29.1	59.6	70.9	12.0	31.8	42.1
			InfoNCE	22.8	50.0	61.5	7.9	23.7	33.0
	ResNet34	DeCLUTR -Sci-base	LS	19.8	46.9	59.2	6.7	21.9	31.9
			KD	21.1	47.9	59.8	7.3	23.0	32.5
CC			OTTER	24.2	52.6	64.4	9.0	25.6	35.4
(3M)			InfoNCE	27.2	57.0	69.0	10.0	27.9	38.5
	FBNetV3-A		LS	24.2	53.9	65.7	8.9	26.7	38.0
			KD	26.9	56.7	68.4	10.7	28.9	39.7
			OTTER	27.5	57.2	69.0	10.4	29.4	39.9
			InfoNCE	25.7	54.3	66.1	8.7	25.8	35.8
	FBNetV3-C		LS	24.8	54.0	66.1	9.7	26.8	37.6
			KD	26.6	55.8	67.6	10.5	28.2	38.9
			OTTER	27.5	57.6	69.1	10.4	28.7	39.4
			InfoNCE	25.5	52.2	62.8	9.5	26.1	35.9
	ResNet50	Sentence	LS	24.5	50.8	61.6	9.3	26.7	37.0
		-BERT-base	KD	25.6	52.3	62.4	9.8	26.2	36.0
			OTTER	26.1	53.1	63.4	9.9	26.6	36.6
			InfoNCE	13.5	34.0	44.8	6.3	19.2	27.8
WIT	ResNet50	DeCLUTR	LS	14.3	35.5	46.2	6.4	19.8	28.9
(5M)		-Sci-base	KD	14.4	35.0	45.9	6.2	19.3	28.0
			OTTER	14.5	36.4	47.7	6.2	19.8	29.0
			InfoNCE	18.8	42.9	53.6	8.9	26.3	36.9
YFCC	ResNet50	DeCLUTR	LS	19.6	44.9	55.7	9.8	28.2	38.8
(15M)		-Sci-base	KD	19.5	43.5	54.2	8.9	26.0	36.7
			OTTER	20.6	45.4	55.9	9.3	27.4	38.1
4.1 Comparing OTTER with Baselines
To compare with OTTER, we include three baselines: 1) InfoNCE with hard labels; 2) InfoNCE with
label-smoothing (LS) (Szegedy et al., 2016), as described in Section 3.2; 3) InfoNCE with knowledge
distillation (KD) (Hinton et al., 2015), as described in Section 3.5. In addition to the experimental
setting described above, we use the following OTTER hyper-parameters: we set the loss coefficient
α = 0.5, set γv = γt = 1 for the similarity matrix. We use the exponential-moving average (EMA)
of the image/text encoders as teachers and set the EMA decay to 0.999. For Sinkhorn-Knopp, we set
7
Published as a conference paper at ICLR 2022
Table 3: Flat hit @K on ImageNet 21K+1K.
Dataset	Image Encoder	Text Encoder	Method	Flat Hit@k(%)			
				rɪl	2	rɪl	rɪθ
		skip-gram	DeViSE	0.3	0.9	2.2	3.6
IN1k	ResNet50	skip-gram	ConSE	0.1	1.5	3.5	4.9
(1.2M)		GloVe	GCNZ	1.0	2.3	5.3	8.1
		GloVe	HZSL	2.2	4.6	9.2	12.7
CC	FBNetV3-C	DeCLUTR-SCi-base	InfoNCE LS	3.2 3.4	4.8 5.1	8.8 9.4	12.9 13.7
(3M)			KD	3.6	5.4	9.7	14.0
			OTTER	3.7	5.5	9.9	14.3
CLIP	ResNet50	CLIP	CLIP	13.5	19.7	30.5	39.4
(400M)	ViT-B/32	Transformer		15.3	22.2	33.9	43.3
λ = 0.15 and the number of iterations to 5. For the KD baseline, we also use EMA teacher and set
α = 0.5. For the label-smoothing baseline, we set α = 0.9, which yields better results than α = 0.5.
On CC, we train the image-text models based on four different pretrained image encoders: ResNet-
{50, 34} (He et al., 2016), FBNetV3-{A, C} (Wu et al., 2019b; Wan et al., 2020; Dai et al., 2020),
and two pretrained text encoders: DeCLUTR-Sci-base (Giorgi et al., 2020) pretrained on S2ORC
(Lo et al., 2020) and Sentence BERT (Reimers & Gurevych, 2019) pretrained on SNLI (Bowman
et al., 2015) and MultiNLI (Williams et al., 2018). We also train ResNet50 + DeCLUTR-Sci-base
on the (partial) WIT (Srinivasan et al., 2021) and the YFCC15M (subset of YFCC 100M) (Thomee
et al., 2016) datasets. We report FH@K=1, 5, 10 on the test sets of both GOI and multi-labeled
ImageNet 10K (Wu et al., 2019a). As shown in Table 2, over the 42 evaluations on 7 different
dataset-architecture settings x 6 metrics, OTTER outperforms (32) or ties (2) all other baselines
in 34 of them. Compared with CLIP’s performance on the GOI test set, a ResNet50 trained by
OTTER outperforms CLIP-RN50 by 2.6 pts FH@1 and by 6.6 pts FH@10. To further illustrate the
significance of the performance gain, we show that a ResNet50 (25.6M params) trained with OTTER
outperforms a Wide ResNet50x2 (68.4M params) trained with InfoNCE under the same setting.
For reference, to put OTTER in the context of traditional ZSL methods, we present FH@K results on
zero-shot transfer to the ImageNet 21K+1K (Deng et al., 2009) dataset, which contains 21,841 classes
in total. The result is reported in Table 3. With 400M image-text pairs, CLIP (Radford et al., 2021)
vastly outperforms all other methods. ImageNet22K’s classes contain many uncommon words, such
as scientific names of animals or medical terms. While not directly comparable with traditional ZSL
methods due to differences in datasets used and model architectures, OTTER is significantly better
than previous ZSL methods, beating the previous SotA, HZSL(Liu et al., 2020), by 68% relatively.
4.2	Visualizing OTTER
In order to check if the image/text matching found by OTTER is sensible, we provide visualizations
of OTTER’s matching results. In Figure 3, we visualize the matching results on a small batch of 9
image-text pairs. We set α = 0.5 for paired image-text samples, as shown in the diagonal elements
in Figure 3. The off-diagonal elements are estimated by OTTER. Since the interpretation of the
matching results are highly subjective, we leave the interpretation to readers.
Next, we use OTTER to process a larger batch of 512 image-text pairs. This is our batch size for
training. We pick the top-8 largest off-diagonal pairs from the optimal tranport result and show them
in Figure 4. As we can see, in a large batch, we can easily find unpaired images and captions that turn
out to be good matches. InfoNCE will simply regard these pairs as negative examples and push them
away from each other while OTTER can better handle this by treating them as semi-positive pairs.
4.3	Importance of Similarity Matrix and EMA
In Equation 7, we design the similarity matrix Sv as the composition of image, text, and image-text
similarity matrices. In Table 4, we show experiments to validate the effectiveness of this composition
and the necessity of using EMA. There are various levels of performance drop when we don’t use
the image or text similarities, or when EMA is turned off. Note that our baseline hyper-parameters
are different from Table 2, so the accuracy is also different. We compare different settings using
FH@K=1 on the GOI test set. More in-depth ablation studies are shown in Appendix A.
8
Published as a conference paper at ICLR 2022

Bh 函	H	0.5	0.003	0.0094		0.065	0.1		0.0044	0.32	l.le-08 6.2e-07	
		0.0005 0.003	0.5	0.48 0.47	0.5		0.0086 0.00028 0.0016	0.0082		0.00057 0.011	0.0054 0.0041	4.4e-08	8e-06 1.le-07 8.le-07	
										
										
	J	0.025	0.003	0.0001	0.5	0.12	0.35	0.0017	3.9e-08	1.3e-05
r		0.29	0.00039	0.0032	0.11	0.5	0.074	0.028	2.8e-07	6.7e-06
Hr		0.0074	0.0012	0.021	0.16	0.23	0.5	0.084	l.le-05	3.1e-06
		0.19	0.022	0.013	0.15	0.017	0.066	0.5	0.0039	0.031
		le-07	2.6e-07	1.2e-06	4.9e-06	1.2e-05	2e-05	0.023	0.5	0.48
■ 一 ∙∙		4.2e-07	3.3e-05	4.1e-07	0.00049	1.4e-05	3.4e-06	0.0052	0.49	0.5
		one of the most dramatic mountain ranges I have seen	aerial: woman waving her arms on the rock	cinematic aerial shot of the dramatic coastline at the cliffs	view of a loch and pine forest	a forest of stunted trees that stand in sharp contrast …	landscape with mown grass and a haystack	newly built small house next to the sea and the beach	the public house traditional PUb in old building on corner	a cottage in the picturesque village
Figure 3: Visualization of OTTER's matching on a batch of 9 image/text pairs.
ice hockey
player # of the
skates against
sports team
american
football player
celebrates his
touchdown run
ice hockey vector illustration a Cute Cat
player blocks a	with a CuP of	starring with
shot during the	coffee and hand	sharp eyes
second period … drawn …
seamless sketch
of a cup of hot
coffee and
steam
close-up of a
calico cat
playing with a
toy
profession cuts
the hair of the
client with
clipper
Figure 4: Visualization of top-8 image-text pairs matched by OTTER in a batch of 512 samples.
These image/text pairs are regarded as negative samples by InfoNCE.
Table 4: Validation of Similarity Matrix and EMA.
	α	γv	γt	EMA	λ	#iter	batch	FH@K=I
	 baseline	0.5	1.0	1.0	✓	0.1	4	512	31.0
similarity matrix	0.5	0.0	1.0	✓	0.1	4	512	28.8 (J 2.2)
	0.5	1.0	0.0	✓	0.1	4	512	27.8 (J 3.2)
	0.5	0.0	0.0	✓	0.1	4	512	26.1 (J 4.9)
EMA	0.5	1.0	1.0	X	0.1	4	512	30.4 (J 0.6)
5	Conclusion
Image-text datasets collected from the Internet are noisy, and the InfoNCE loss used by previous
works such as CLIP fails to recognize the potential matches between unpaired images and captions in
a batch. As a solution, OTTER extends the InfoNCE loss to consider the many-to-many relationship
between unpaired images and texts by computing a pair-wise similarity matrix and using entropic
optimal transport to solve for the off-diagonal matching probabilities. OTTER outperforms (32) or
ties (2) all other baselines on Google Open Images and ImageNet 10K in 34 out of 42 comparisons.
In future research, we want to test the effectiveness of OTTER on larger datasets, such as CLIP 400M.
9
Published as a conference paper at ICLR 2022
References
Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for
attribute-based classification. CVPR, 2013.
Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output
embeddings for fine-grained image classification. CVPR, 2015.
Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous
clustering and representation learning. arXiv preprint arXiv:1911.05371, 2019.
Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery:
Improving ima- genet classification through label progression. arXiv preprint arXiv:1805.02641,
2018.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated
corpus for learning natural language inference. 2015.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve J6gou, Julien MairaL Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
arXiv:2104.14294, 2021.
Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, and Changhu Wang. Learning the best pooling
strategy for visual semantic embedding. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2021a.
Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. Graph optimal
transport for cross-domain alignment. ICML, 2020a.
Liqun Chen, Zhe Gan, Dong Wang, Jingjing Liu, Ricardo Henao, and Lawrence Carin. Wasserstein
contrastive representation distillation. CVPR, 2021b.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. ICML, 2020b.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020c.
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain
adaptation. arXiv preprint arXiv:1507.00504v2, 2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. NeurIPS, 2013.
Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen, Yuandong
Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using neural
acquisition function. arXiv preprint arXiv:2006.02049, 2020.
Bharath Bhushan Damodaran, Remi Flamary, Vivien Seguy, and Nicolas Courty. An entropic optimal
transport loss for learning deep neural networks under label noise in remote sensing images. arXiv
preprint arXiv:1810.01163, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. pp. 248-255, 2009.
Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations.
arXiv preprint arXiv:2006.06666, 2020.
Ingo Feinerer and Kurt Hornik. wordnet: WordNet Interface, 2020. URL https://CRAN.
R-project.org/package=wordnet. R package version 0.1-15.
10
Published as a conference paper at ICLR 2022
Andrea Frome, Greg S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. Devise: A deep visual-semantic embedding model. NIPS, 2013.
John M Giorgi, Osvald Nitski, Gary D. Bader, and Bo Wang. Declutr: Deep contrastive learning for
unsupervised textual representations. arXiv preprint arXiv:2006.03659, 2020.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. CVPR, 2006.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CVPR, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. CVPR, 2020.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Yasuhide Mori Hironobu, Hironobu Takahashi, and Ryuichi Oka. Image-to-word transformation
based on dividing and vector quantizing images with words. In in Boltzmann machines”, Neural
Networks, pp. 405409, 1999.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. arXiv preprint arXiv:2102.05918, 2020.
Armand Joulin*, Laurens van der Maaten*, Allan Jabri, and Nicolas Vasilache. Learning visual
features from large weakly supervised data. In ECCV, 2016.
Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P. Xing.
Rethinking knowledge graph propagation for zero-shot learning. CVPR, 2019.
Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep fragment embeddings for bidirectional image
sentence mapping. In Proceedings of the 27th International Conference on Neural Information
Processing Systems - Volume 2, NIPS'14,pp.1889-1897, Cambridge, MA, USA, 2014. MIT Press.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 2020.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab
Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.
The open images dataset v4: Unified image classification, object detection, and visual relationship
detection at scale. IJCV, 2020.
Ang Li, Allan Jabri, Armand Joulin, and Laurens van der Maaten. Learning visual n-grams from web
data. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 4193-4202, 2017.
doi: 10.1109/ICCV.2017.449.
Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for
image-text matching. In ICCV, 2019.
Shaoteng Liu, Jingjing Chen, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua, and Yu-Gang Jiang.
Hyperbolic visual embedding learning for zero-shot recognition. CVPR, 2020.
Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu,
Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. arXiv preprint
arXiv:2102.09480, 2021.
11
Published as a conference paper at ICLR 2022
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic
scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 4969-4983, Online, July 2020. Association for ComPUta-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/
anthology/2020.acl-main.447.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.
S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of
aircraft. Technical report, 2013.
Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg S. Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic
embeddings. ICLR, 2014.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal
pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966,
2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint
arXiv:2103.00020, 2021.
Vignesh Ramanathan, Percy Liang, and Li Fei-Fei. Video event understanding using natural language
descriptions. In 2013 IEEE International Conference on Computer Vision, pp. 905-912, 2013. doi:
10.1109/ICCV.2013.117.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.
10084.
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. arXiv preprint arXiv:2010.04592, 2020.
Bernardino Romera-Paredes and Philip H. S. Torr. An embarrassingly simple approach to zero-shot
learning. ICML, 2015.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris N. Metaxas. Improving gans using optimal
transport. ICLR, 2018.
Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption
annotations. arXiv preprint arXiv:2008.01392, 2020.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. Grounded
compositional semantics for finding and describing images with sentences. Transactions of the
Association for Computational Linguistics, 2:207-218, 2014. doi: 10.1162/tacl_a_00177. URL
https://www.aclweb.org/anthology/Q14-1017.
12
Published as a conference paper at ICLR 2022
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy
labels with deep neural networks: A survey. arXiv preprint arXiv:2007.08199, 2020.
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:
Wikipedia-based image text dataset for multimodal multilingual machine learning. arXiv preprint
arXiv:2103.01913, 2021.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Commun. ACM,
59(2):64-73, jan 2016. ISSN 0001-0782. doi: 10.1145/2812802. URL https://doi.org/
10.1145/2812802.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
preprint arXiv:1910.10699, 2019.
Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu,
Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural architecture search for
spatial and channel dimensions. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 12965-12974, 2020.
Josiah Wang, Katja Markert, and Mark Everingham. Learning models for object recognition from
natural language descriptions. In Proceedings of the British Machine Vision Conference, 2009.
Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and
knowledge graphs. CVPR, 2018.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pp. 1112-1122. Association for Computational Linguistics, 2018.
URL http://aclweb.org/anthology/N18-1101.
Baoyuan Wu, Weidong Chen, Yanbo Fan, Yong Zhang, Jinlong Hou, Jie Liu, and Tong Zhang.
Tencent ml-images: A large-scale multi-label image database for visual representation learning.
IEEE Access, 7, 2019a.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via
differentiable neural architecture search. CVPR, 2019b.
Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self-training with noisy student
improves imagenet classification. CVPR, 2020.
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Con-
trastive learning of medical visual representations from paired images and texts. arXiv preprint
arXiv:2010.00747, 2020.
13
Published as a conference paper at ICLR 2022
A Ablation Studies
In this section, we analyze the impact of hyper-parameters on the performance of OTTER in Table 5.
Note that our baseline hyper-parameters are different from Table 2, so the accuracy is also different.
We compare different settings using FH@K=1 on the GOI test set.
Table 5: Ablation studies. ResNet50 + DeCLUTR-Sci-base evaluated on GOI test set.
	α	γv	γt	EMA	λ	#iter	batch	FH@K=I
	 baseline	0.5	1.0	1.0	✓	0.1	4	512	31.0
α	0QT~	1.0	1.0	✓	0.1	4	512	29.9Q 1.1)
	0.9	1.0	1.0	✓	0.1	4	512	28.4 Q 2.6)
similarity matrix	0.5	0.0	1.0	✓	0.1	4	512	28.8 Q 2.2)
	0.5	1.0	0.0	✓	0.1	4	512	27.8 Q 3.2)
	0.5	0.0	0.0	✓	0.1	4	512	26.1 Q 4.9)
EMA		1.0	1.0	X	0.1	4	512	30.4 Q 0.6)
	0.5	1.0	1.0	✓	0.05	4	512	29.0 Q 2.0)
	0.5	1.0	1.0	✓	0.3	4	512	28.2 Q 2.8)
Sinkhorn	0.5	1.0	1.0	✓	0.1	0	512	29.1 Q 1.9)
	0.5	1.0	1.0	✓	0.1	2	512	29.3Q 1.7)
	0.5	1.0	1.0	✓	0.1	6	512	30.0Q 1.0)
batch	0.5	1.0	1.0	✓	0.1	4	256	25.6 Q 5.4)
size	0.5	1.0	1.0	✓	0.1	4	768	28.1 Q 2.9)
Confidence in the image-text pairs: In Section 3.2, we define α = qiv (i) as the probability that the
paired text caption is the correct match with the image. This reflects the confidence, or the noise
level, in the ground truth pairs. We set α = 0.1, 0.5, 0.9 in our experiment, and found that both lack
of confidence (0.1) or over-confidence (0.9) can hurt the performance. Relatively, α = 0.9 leads to
worse performance, validating the necessity of mitigating label noise.
Image-to-image, text-to-text similarity: We included the image and text similarity when computing
the pair-wise similarities for OT. The assumption is that samples with similar images or text captions
are likely to share labels. To test this, we set γv , γt to 0 in the experiments. We found that both
image-to-image and text-to-text similarity are helpful. Relatively, text similarity seems to be more
important than image similarity, as removing text similarity leads to a larger performance drop.
Do we need EMA teacher? In the default setting, we used the exponential moving average of the
image/text encoders to compute the similarity estimation. We test the alternative option of using the
image/text encoders themselves, and found that this leads to a small accuracy drop (0.6 points).
Impact of optimal transport: One key component of OTTER is to use optimal transport to match
images with text captions within a batch. However, do we really need optimal transport? To compute
the teacher distillation, a simple alternative is to use a Softmax function. As we discussed in Section
3.5, Softmax is equivalent to our Sinkhorn-Knopp implementation when we set the number of
iterations to 0. So we validate the necessity of optimal transport by setting the #iteration to 0, 2, 4, 6.
Experiments show that using Softmax (0 iteration of Sinkhorn) leads to the worst performance (-1.9).
This validates the necessity of using optimal transport to ensure all images and texts within a batch
are matchable. Besides, using 2 (fewer) and 6 (more) iterations also lead to accuracy drops (-1.7,
-1.0). Using more iterations of Sinkhorn leads to a more converged solution to the optimal transport
problem, but this does not seem to be positively correlated with better performance. We also explored
the impact of entropy regularization controlled by λ in Equation (6). Experiments show that the target
distribution being too "hard" (λ = 0.05) or too "soft" (λ = 0.30) can hurt the performance.
Batch size: Previous works on contrastive learning show that a larger batch size raises the lower
bound of mutual information (Hadsell et al., 2006) and leads to better performance (Chen et al.,
2020b; He et al., 2020). However, for noisy image-text pairs, larger batch sizes can potentially bring
more unpaired matches. We study the impact of batch sizes by setting it to 256, 512, 768 in the
experiments. We find that both smaller (256) and larger (768) batch sizes lead to worse performance.
We hypothesize that batch size needs to be co-adapted with hyper-parameter settings of α and λ.
14
Published as a conference paper at ICLR 2022
α estimates the noise level, which is positively correlated with the batch size. λ yields different
"softness" with different batch sizes. However, further investigation is to required to validate this.
B	Pseudocode for OTTER
Algorithm 1: PyTorch Pseudocode for OTTER
#	fs, ft: student and teacher model.
#	tpi, tpd: learnable inverse temperature.
#	eta: a large constant, e.g., 100.
#	alpha: loss coefficient.
#	I_N: NxN identity matrix.
#	xent: cross entropy function.
for img, txt in loader:
#	Regular InfoNCE loss
emb_v, emb_t = fs(img, txt) # normalized embeddings.
logits = emb_v @ emb_t.T
prob_v = Softmax(logits * tpi) # normalize over t.
prob_t = Softmax(logits.T * tpi) # normalize over v.
L_infoNCE = xent(prob_v, I_N) + xent(prob_t, I_N)
#	Similarity estimation
emb_v_t, emb_t_t = ft(img, txt).detach() # stop gradient.
sim_vv, sim_tt = emb_v_t @ emb_v_t.T, emb_t_t @ emb_t_t.T
sim_vt, sim_tv = emb_v_t @ emb_t_t.T, emb_t_t @ emb_v_t.T
S_v = sim_vv + sim_tt + sim_vt - eta * I_N
S_t = sim_tt + sim_vv + sim_tv - eta * I_N
#	Optimal Transport Distillation
M_v = sinkhorn(S_v)
M_t = sinkhorn(S_t)
L_d = xent(prob_v, M_v) + xent(prob_t, M_t)
#	Final loss
loss = alpha * L_infoNCE + (1-alpha) * L_d
update(fs, ft, tpi, tpd)
Algorithm 2: PyTorch Pseudocode for Sinkhorn-Knopp
def sinkhorn(S, lambda=0.15, niter=5):
T = exp(S / lambda)
T = T / T.sum()
N = T.shape[0]
#	iterative row/column normalization
for _ in range(niter):
T /= (T.sum(dim=1, keepdim=True) * N) # row normalization
T /= (T.sum(dim=0, keepdim=True) * N) # column normalization
#	Note if niter=0, this is equivalent to Softmax
return T /= T.sum(dim=1, keepdim=True) # row normalization
C Variance Analysis
In our experiments, we noticed variance of experimental results with identical settings. To study this,
we repeat the experiments in Table 2 with a ResNet50 image encoder and a DeCLUTR-Sci-base text
encoder for 3 times each using different random seed to analyze the variance of the experiments. We
noticed higher variance on GOI experiments. For example, for the FH@10 metric, the variance can be
up to 1.88 pts. Note that the mean accuracy’s gap between OTTER and baselines are all significantly
larger than the standard deviation, indicating that the performance improvement of OTTER is not a
15
Published as a conference paper at ICLR 2022
result of randomness. However, such high variance is worth noting and requires future investigation
on how to reduce it.
Table 6: Flat hit @K on test sets of Google Open Images and ImageNet10K from Tencent-ML-
Images.
Data	Method		GOI FH@K(%)					IN10KFH@K(%)		
		1	5	10	1	5	10
	InfoNCE	27.1 ± 0.23	56.1 ± 0.92	66.4 ± 1.05	10.9 ± 0.38	29.4 ± 0.75	40.5 ± 0.76
CC	LS	26.7 ± 1.00	55.9 ± 1.31	67.5 ± 1.31	10.1 ± 0.67	29.6 ± 0.81	39.8 ± 1.03
(3M)	KD	26.7 ± 0.81	55.3 ± 1.67	67.1 ± 1.71	10.0 ± 0.75	27.5 ± 1.42	38.5 ± 1.14
	OTTER	28.6 ± 1.17	59.6 ± 1.71	70.9 ± 1.88	12.0 ± 0.31	31.8 ± 0.40	42.1 ± 0.26
D Quantitative Analysis on the Image-Text Compositionality on
CUB
ALIGN Jia et al. (2020) presents an interesting demonstration of the compositionality of image and
text embeddings generated by language supervised vision models. On an image retrieval task, a
query is formed by adding an image embedding vector to a text embedding vector. The returned
image is expected to be similar to the image query and the text query. ALIGN demonstrates
the compositionality by qualitatively showing several retrieval results, but does not provide any
quantitative evaluations. In this paper, we design a preliminary benchmark based on the CUB dataset
(Welinder et al. (2010)) to evaluate the image-text compositionality.
Image + text queries
Query embeddings
Dataset image embedding
Image query emb
■
yellow forehead color
black eye color
■■■
white breast color
Text query emb
Text query emb
■■■
Text query emb
QUery embedding
Image embedding
Image embedding
Image embedding
...
Image embedding
Cosine
Similarity
0.3
0.9
0.4
0.1
O
O
Figure 5:	Illustration of the image + text -> image retrieval.
CUB (WeIinder et al. (2010)) consists of 6033 bird images, and each image-i is annotated with a
set of bird attributes, which we denote as Ai. In this dataset, there are in total 288 unique attributes.
Given an image and several CUB bird attributes in text, we generate image and text embeddings
using our pretrained models and add the embeddings together to form a query embedding vector.
We use the query vector to match images in CUB, and choose the nearest neighbor, based on cosine
similarity, as the retrieved image. This process is illustrated in Figure 5. To evaluate the retrieval
quality, we compare the overlap between the retrieved image’s attributes with the image-text query’s
attributes.
We now describe how we generate text queries in addition to an image query. Randomly adding
text attributes to image queries may result in unmatchable queries. To ensure that the added text
query is sensible and the combined image-text query can be matched to an image in the dataset,
we obtain a text query with the following approach: We first select a pair of images, denoted as
image-i and image-j. We use image-i as the image query, and let Qv = Ai be the image query’s
attribute set. Then, to form the text query, we compare the differences of image-j and i, and let
Qt = Aj - Ai be the text query’s attribute set. The combined image-text query should contain
attributes Q = Qv S Qt. We show an example of image query, text query, and combined query in
Figure 6. Following this process, we generate 1,000,000 image-text queries from randomly sampled
image pairs from CUB where each image pair shares at least 10 common attributes. The image-text
queries used in our experiment are provided in the attached file image_text_query_list.txt
in the supplementary material.
16
Published as a conference paper at ICLR 2022
image-i
image-j
Image Query
'grey wing color',
'white upperparts color,,
'white forehead color,,
'white primary color1,
Text Query
'orange belly color',
'orange underparts color',
'orange breast color,,
'blue nape color',
Ai	Aj
Figure 6:	Image and text query example.
Table 7: Quantitative Vision-Language Compositionality Benchmark. OR represents Overlapping
Rate, IOR represents Image Overlapping Rate, and TOR represents Text Overlapping Rate.
Model	Image Encoder	Text Encoder	Method	OR(%)	IOR (%)	TOR (%)
Baseline	-		-		-	27.7	31.3	23.2
CLIP	ResNet50	CLIP Transformer	InfoNCE	35.7	36.2	34.5
Ours	ResNet50	DeCLUTR -Sci-base	InfoNCE LS KD OTTER	34.5 34.2 33.2 34.7	33.6 33.6 32.9 33.9	-36.8- 35.8 34.4 36.7
Our evaluation measures the overlap between the retrieved image’s attribute set Rk with the image-
text query set Qk, image query set Qvk, text query set Qtk. Specifically, we compute: 1) the attributes
overlapping rate (OR) N PN=I IQkQRk|. This measures the retrieval quality for the combined
image-text query. 2) the average image attributes overlapping rate (IOR) N PN=I lQ∣k∩VRk |. This
evaluates if the returned image hits/misses attributes in the image query. 3) the average text attributes
overlapping rate (TOR) N PN=I IQkQtRk |. This evaluates if the retrieved image hits/misses attributes
of text queries. An ideal match should achieve higher scores in OR, IOR, and TOR simultaneously.
We report our results comparing with CLIP and other baselines in Table 7. First, we report the
measurement of a random baseline. With this random baseline, for any image-text query, we return a
random image from the dataset. We can see that this gives a non-trivial OR (27.7%), IOR (31.3%),
and TOR (23.2%). This is not surprising because images in CUB have many overalapping attributes.
However, although both CLIP and our models are never directly trained on CUB nor trained to
predict bird attributes, their overlapping rates are significantly higher than the random baseline. CLIP
achieves comparable accuracy with our models, with 1% higher OR, 2.3% higher IOR, and -2.2%
lower TOR. Among different training methods, OTTER outperforms all baselines in OR and IOR, and
achieves slightly worse (-0.1%) TOR than the InfoNCE baseline. Also note that our models achieve
similar IOR and TOR, which shows both image and text queries are considered. This demonstrates
good compositionality.
In Figure 7, we show qualitative results of our model trained by OTTER. For example, in the first
row, we add yellow forehead color, black eye color, yellow upper-parts color and yellow breast
17
Published as a conference paper at ICLR 2022
-----'black wing color,
-----'black leg color*
'yellow forehead color',
'black eye color*, --V
'yellow upperparts color1，
'white breast color"	,
'striped wing pattern'
-'black eye color' -
'black forehead color',
'yellow nape color,
'yellow primary color*
Figure 7: Qualitative results of text + image retrieval. Blue lines indicate texts attributes, and red
lines indicate image attributes. For the demonstration purpose, we only label parts of the attributes
that are easy to recognize.
'black throat color,,
'white breast color：
'black wing color',
'black under tail color1
'grey forehead color1
'black eye color'
(solid tail pattern'
color as text attributes to an image of a black bird. The retrieved bird image contains attributes
from both image and text queries - it has yellow body colors while maintains black leg and black
wing colors. From both quantitative and qualitative results, our model demonstrates good image-text
compositionality.
18
Published as a conference paper at ICLR 2022
E Image Attributions
1.	Figure 1, Paul Bica, Coast of Kauai, Hawaii, CC BY 2.0
2.	Figure 1, James St. John, Columnar-jointed rhyolitic obsidian lava flow, CC BY 2.0
3.	Figure 1, Mordaka, QK9A1397, CC BY-SA 4.0
4.	Figure 1, Alan Reid, Heathery moor on the flank of Stone Saul, CC BY-SA 2.0
5.	Figure 1, Jimmy Emmerson, The Tormented Valley, CC BY-NC-ND 2.0
6.	Figure 1, Roman Boed from The Netherlands, Black Forest- Meadow (10561897306), CC
BY 2.0
7.	Figure 1, Daniel Clerc / CC-BY-SA-3.0, 2013 bois herpin 013, CC BY-SA 3.0
8.	Figure 1, Dave Bevis, 22 and 24 High Street, Newcastle-under-Lyme, CC BY-SA 2.0
9.	Figure 1, Nilfanion, Thatched cottages in Coverack (8379), CC BY-SA 4.0
10.	Figure 4, TheAHL, Chuck Kobasew (cropped), CC BY 2.0
11.	Figure 4, Mark Mauno, Jasper Fitzi, CC BY 2.0
12.	Figure 4, Famartin, 2020-04-27 18 55 23 A Calico cat looking for food in a kitchen in the
Franklin Farm section of Oak Hill, Fairfax County, Virginia, CC BY-SA 4.0
13.	Figure 4, Beth, Haircut-4, CC BY 2.0
19