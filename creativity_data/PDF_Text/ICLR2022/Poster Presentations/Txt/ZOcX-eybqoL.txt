Published as a conference paper at ICLR 2022
Generalisation in Lifelong Reinforcement
Learning through Logical Composition
Geraud Nangue Tasse, Steven James & Benjamin Rosman
School of Computer Science and Applied Mathematics
University of the Witwatersrand
Johannesburg, South Africa
geraudnt@gmail.com, {steven.james, benjamin.rosman1}@wits.ac.za
Ab stract
We leverage logical composition in reinforcement learning to create a framework
that enables an agent to autonomously determine whether a new task can be
immediately solved using its existing abilities, or whether a task-specific skill
should be learned. In the latter case, the proposed algorithm also enables the
agent to learn the new task faster by generating an estimate of the optimal policy.
Importantly, we provide two main theoretical results: we bound the performance
of the transferred policy on a new task, and we give bounds on the necessary and
sufficient number of tasks that need to be learned throughout an agent’s lifetime to
generalise over a distribution. We verify our approach in a series of experiments,
where we perform transfer learning both after learning a set of base tasks, and
after learning an arbitrary set of tasks. We also demonstrate that, as a side effect
of our transfer learning approach, an agent can produce an interpretable Boolean
expression of its understanding of the current task. Finally, we demonstrate our
approach in the full lifelong setting where an agent receives tasks from an unknown
distribution. Starting from scratch, an agent is able to quickly generalise over the
task distribution after learning only a few tasks, which are sub-logarithmic in the
size of the task space.
1 Introduction
Reinforcement learning (RL) is a framework that enables agents to learn desired behaviours by
maximising the rewards received through interaction with an environment (Sutton et al., 1998). While
RL has achieved recent success in several difficult, high-dimensional domains (Mnih et al., 2015;
Levine et al., 2016; Lillicrap et al., 2016; Silver et al., 2017), these methods require millions of samples
from the environment to learn optimal behaviours. This is ultimately a fatal flaw, since learning to
solve complex, real-world tasks from scratch for every task of interest is typically infeasible. Hence a
major challenge in RL is building general-purpose agents that are able to use existing knowledge to
quickly solve new tasks in the environment. The question of interest is then: after learning n tasks
sampled from some distribution, how can an agent transfer or leverage the skills learned from those n
tasks to improve its starting performance or learning speed in task n + 1?
This problem setting is formalised by lifelong RL (Thrun, 1996; Abel et al., 2018). One approach to
transfer in lifelong RL is composition (Todorov, 2009), which allows an agent to leverage its existing
skills to build complex, novel behaviours that can then be used to solve or speed up learning of a new
task (Todorov, 2009; Saxe et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al.,
2019; Peng et al., 2019). Recently, Nangue Tasse et al. (2020) proposed a framework for defining a
Boolean algebra over the space of tasks and their optimal value functions. This allowed for tasks and
value functions to be composed using the union, intersection and negation operators in a principled
manner to yield optimal skills zero-shot.
In this work, we propose a framework for lifelong RL that focuses not only on transfer between tasks
for faster RL, but also provides guarantees on the generalisation of an agent’s skills over an unknown
task distribution. We first extend the logical composition framework of Nangue Tasse et al. (2020) to
discounted and stochastic tasks. We provide theoretical bounds for our approach in stochastic settings,
1
Published as a conference paper at ICLR 2022
and also compare them to previous work in the discounted setting. We then show how our framework
leverages logical composition to tackle the lifelong RL problem. The framework enables agents to
iteratively solve tasks as they are given, while at the same time constructing a library of skills that
can be composed to obtain behaviours for solving future tasks faster, or even without further learning.
We empirically verify our framework in a series of experiments, where an agent is i) pretrained on
a set of base tasks provided by the Boolean algebra framework, and ii) when the pretrained tasks
are not base tasks. We show that agents here are able to achieve significant jumpstarts on new tasks.
Finally, we demonstrate our framework in the lifelong RL setting where an agent receives tasks from
an unknown (possibly non-stationary) distribution and must determine what skills to learn and add to
its library, and how to combine its current skills to solve new tasks. Results demonstrate that this
framework enables agents to quickly learn a set of skills, resulting in a combinatorial explosion in
their abilities. Consequently, even when tasks are sampled randomly from an unknown distribution, an
agent can leverage its existing skills to solve new tasks without further learning, thereby generalising
over task distributions.
2 Background
We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is defined by the
tuple (S, A,p, r, γ), where (i) S is the state space, (ii) A is the action space, (iii) p(s0|s, a) is a
Markov transition probability, (iv) r is the real-valued reward function bounded by [rMIN, rMAX], and
(v) γ ∈ [0, 1) is the discount factor. In this work, we focus on tasks where an agent is required to
reach a set of desirable goals in a goal space G ⊆ S (a set of boundary states). Here, termination in
G is modelled similarly to van Niekerk et al. (2019) by augmenting the state space with a virtual
state, ω, such that p(ω∣s, a) = 1 ∀(s, a) ∈ (GX A) and the rewards are zero after reaching ω. We
hence consider the set of tasks M such that the tasks are in the same environment—described by a
background MDP (S, A,p, γ, r0)—and each task can be uniquely specified by a set of desirable and
undesirable goals:
M(S, A, p, γ, r0) := {(S, A, p, γ, r) | ∀a ∈ A, r(s, a) = r0 (s, a) ∀s ∈ S \ G;
r(g, a) = rg ∈ {rMIN, rMAX} ∀g ∈ G} (1)
The goal of the agent is to compute a Markov policy π from S to A that optimally solves a given task.
A given policy π is characterised by a value function V π (s) = Eπ [Pt∞=0 γtr(st, at)], specifying
the expected return obtained under ∏ starting from state s. The optimal policy ∏* is the policy that
obtains the greatest expected return at each state: Vπ* (S) = V * (S) = max∏ Vπ (S) for all S in S. A
related quantity is the Q-value function, Qπ (s, a), which defines the expected return obtained by
executing a from S, and thereafter following π. Similarly, the optimal Q-value function is given by
Q*(S, a) = maxπ Qπ (S, a) for all S in S and a in A.
2.1 Logical composition
Nangue Tasse et al. (2020) recently proposed the notion of a Boolean task algebra, which allows
an agent to perform logical operations—conjunction (∧), disjunction (∨) and negation (-)—over
the space of tasks and value functions. While they only considered deterministic shortest path
tasks (γ = 1 with deterministic dynamics), we summarise their approach here and later extend it to
discounted stochastic tasks (Section 3.1).
To achieve zero-shot logical composition, Nangue Tasse et al. (2020) extend the standard rewards
and value functions used by an agent to define goal-oriented versions as follows:
Definition 1. The extended rewardfunction r : S×G×A→ R is given by the mapping
(S, g, a) 7→
(亍MIN
r(S, a)
if g 6= S and S ∈ G
otherwise,
(2)
where rMIN ≤ min{rMIN, (rMIN 一 rMAX)D}, and D is the diameter ofthe MDP (JakSch et al.,2010).
Definition 2. The extended Q-valuefunction Q: S ×G ×A→ R is given by the mapping
(S,g,a) → r(s,g, a)+Y Ep(SlS,a)Vπ (s0,g),	(3)
s0∈S
2
Published as a conference paper at ICLR 2022
where Vπ(s,g) = En [P∞=o r(st,g, at)].
By penalising the agent for achieving goals different from those it wanted to reach (TMIN if g =
s and s ∈ G), the extended reward function has the effect of driving the agent to learn how
to separately achieve all desirable goals. Importantly, the standard reward and value func-
tions can be recovered from their extended versions by simply maximising over goals. As
such, the agent can also recover the task policy by maximising over both goals and actions:
π(s) ∈ arg maxa∈A maxg∈G QT (s, g, a).
The logic operators over tasks and extended action-value functions are then defined as follows:
Definition 3. Let M be a set of tasks with bounds MMIN , MMAX ∈ M such that,
rMMAX(s,a) := max rM (s, a)	rMMIN (s, a) := min rM (s, a)
M∈M	M∈M
Define the , ∨, and ∧ operators over M as
-(M) := (S, A,P,r-M), where r-M(s,a) := (γmmax (s, a) + ^Mmin (s,a)) -「m(s,a)
∨(M1, M2) := (S,A,p,rM1∨M2), where rM1∨M2(s, a) := max{rM1 (s, a), rM2 (s, a)}
∧(M1, M2) := (S,A,p,rM1∧M2), where rM1∧M2(s, a) := min{rM1 (s, a), rM2 (s, a)}
Definition 4. Let Q* be the set of optimal extended Q-VaIue functions for tasks in M, with
bounds QMIN,QMax ∈ Q* which are respectively the optimal Q-functions for the tasks
MMIN, MMAX ∈ M. Define the , ∨, and ∧ operators oVer QT * as,
-(Q*)(s,g,a) := (QMIN(s,g,a) + QMAX(s,g, a)) - Q*(s,g,a)
∨(QT*1, QT2*)(s, g, a) := max{QT1*(s, g, a), QT2*(s, g, a)}
∧(QT*1, QT2*)(s, g, a) := min{QT1*(s,g,a), QT2*(s, g, a)}
Using the definitions for the logical operations over M and QT * given above, Nangue Tasse et al.
(2020) construct a Boolean algebra over tasks and extended value functions. Furthermore, by
leveraging the goal-oriented definition of extended value functions, they also show that M and QT *
are homomorphic. As a result, if a task can be expressed using the Boolean algebra, the optimal value
function for the task can immediately be computed. This enables agents to solve any new task that is
given as the logical combination of learned ones.
3	Lifelong transfer through composition
In lifelong RL, an agent is presented with a series of tasks sampled from some distribution D. The
agent then needs to not only transfer knowledge learned from previous tasks to solve new but related
tasks quickly, but it also should not forget learned knowledge in the process. We formalise this
lifelong learning problem as follows:
Definition 5. Let D be an unknown, possibly non-stationary, distribution oVer a set of tasks
M(S, A,p, γ, r0). The lifelong learning problem consists of the repetition of the following steps for
t∈N:
1.	The agent is given a task Mt 〜D(t),
2.	The agent interacts with the MDP Mt until it is -optimal in M0, ..., Mt.
This formulation of lifelong RL is similar to that of Abel et al. (2018); the main difference is that we
do not assume that D is stationary, and we explicitly require an agent to retain learned skills.
As discussed in the introduction, one of the main goals in this setting is that of transfer (Taylor &
Stone, 2009). We add an important question to this setting: how many tasks should an agent learn
3
Published as a conference paper at ICLR 2022
during its lifetime in order to generalise over the task distribution? In other words, how many tasks
should it learn to be able to solve any new task immediately? While most approaches focus on the
goal of transfer, the question of the number of tasks is often neglected by simply assuming the case
where the agent has already learned n tasks (Abel et al., 2018; Barreto et al., 2018). Consider, for
example, a task space with only |G| = 40 goals. Then, given the combination of all possible goals,
the size of the task space is |M| = 2|G| ≈ 1012. If D is a uniform distribution over |M|, then for
most transfer learning methods an agent will have to learn most of the tasks it is presented with, since
the probability of observing the same task will be approximately zero. This is clearly impractical for
a setting like RL, where learning methods often have a high sample complexity even with transfer
learning. It is also extremely memory inefficient, since the learned skills of most tasks must be stored.
3.1 Extending the Boolean algebra framework
In this section, we show how logical composition can be leveraged to learn a subset of tasks
that is sufficient to generalise over the task distribution. Since the logical composition results of
Nangue Tasse et al. (2020) were only shown for deterministic shortest path tasks (where γ = 1), we
extend the framework to discounted and stochastic tasks M (Equation 1). To achieve this, we first
redefine the extended_reward function (Definition 1) to use the simpler penalty 尸MIN = γmin. We
also redefine - over Q* as follows:
-(Q *)(.)：= [Q Max ʃ.) ifM.)- Q MIN (.)1 ≤ ©(J- Q Max (J1 , ∀(.) ∈S×G×A.
Q*MIN(.)	otherwise,
The intuition behind this re-definition of the negation operator is as follows: since each goal is
either desirable or not, the optimal extended value function Q*(s, g, a) is either QMAX(s, g, a)
or QMIN(s, g, a). Hence, if Q*(s, g, a) is closer to QMIN(s, g, a), then its negation should be
QMax(s, g,a), and vice versa. For tasks in M, this is equivalent to the previous definition of -
for optimal Q-VaIUe functions, but it will give us tight bounds when composing E-OPtimaI Q-value
functions (see Theorem 1).
We now show that the Boolean algebra and zero-shot composition results of Nangue Tasse et al.
(2020) also hold for tasks in M.
Proposition 1. Let Q* be the set ofoptimal Q-ValuefUnctionsfor tasks in M. Let A : M → Q*
be any map from M to Q* such that A (M) = QM for all M in M. Then,
(i)	M and Q* respectively form a Boolean task algebra (M, ∨, ∧, —, MMAX, MMIN) and
a Boolean extended value functions algebra (Q*, ∨, ∧,—, Q*MAx , Q*MIN),
(ii)	A is a homomorphism between M and Q*.
We can now solve any new task in M zero-shot if we are given the correct Boolean expression that
informs the agent how to compose its optimal skills. This is essential for the following results.
3.2 Transfer between tasks
In this section, we leverage the logical composition results to address the following question of interest:
given an arbitrary set of learned tasks, can we transfer their skills to solve new tasks faster? As we
will show in Theorem 1, we answer this question in the affirmative. To achieve this, we first note that
each task M ∈ M can be associated with a binary vector T ∈ {0, 1}|G| which represents its set of
desirable goals, as illustrated by the tasks in Table 1. The approximation T of this task representation
can be learned just from task rewards (rM (s, a)) by simply computing T (s) = 1rM (s,a)=rM AX at
each terminal state s that the agent reaches. We can then use any generic method, such as the sum-of-
products (SOP), to determine a candidate Boolean expression (BExP) in terms of the learned binary
representations Tn = {T1, ..., Tn} ofa set of past tasks M = {M1, ..., Mn} ⊆ M. An estimate of
the optimal Q-value function of M can then be obtained by composing the learned Q-value functions
Qn = {Q*,…,Qn} according to BEXP. Theorem 1 shows the OPtimaIity of this process.1
1See Appendix A for proofs of theorems and Appendix B for a brief description of the SOP method.
4
Published as a conference paper at ICLR 2022
Theorem 1. Let M ∈ M be a task with reward function r, binary representation T and optimal
extended action-value function Q *. Given E-approximations of the binary representations Tll 二
{Tι,…，Tn} and optimal Q-functions Qn = {Q*,…，Q∣} forn tasks M = {Mι,…，M∣} ⊆ M, let
~
TF = BEXP(Tn) and QF = BEXP (Qn),
where BEXP is derived from Tn and T using a generic method F. Define π(s) ∈
argmaXa∈A QF where QF :二 maXg∈G QF(s, g, a). Then,
(I)	kQ* - Qπk∞ ≤ T22γ ((I T=TF + 1 r∈{rg }|G| )r∆ + E),
(ii)	if the dynamics are deterministic,
kQ* - QFk∞ ≤ (1T 6=TF )r∆ + E,
where 1 is the indicator function, rg (s, a) := r(s, g, a), τ δ ：= rmax 一 rmin, and kf 一 h∣∣∞ :=
maxs,g,a |f(s, g, a) - h(s, g, a)|.
Theorem 1(i) states that if QF is close to optimal, then acting greedily with respect to it is also close
to optimal. Interestingly, this is similar to the bound obtained by Barreto et al. (2018) (Proposition 1)
for transfer learning using generalised policy improvement (GPI), but stronger.2 This is unsurprising,
since π(s) ∈ arg max°∈A maXg∈G QF(s, g, a) can be interpreted as generalised policy improvement
on the set of goal policies of the extended value function QF. Importantly, if the environment is
deterministic, then we obtain a strong bound on the composed value functions (Theorem 1(ii)). This
bound shows that transfer learning using logical composition is E-optimal—that is, there is no loss
in optimality—when the new task is expressible as a logical combination of past ones. With the
exponential nature of logical combination, this gives agents a strong generalisation ability over the
task space—and hence over any task distribution—as we will show in Theorem 2.
3.3 Generalisation over a task distribution
We leverage Theorem 1 to design an algorithm that combines the SOP approach with goal-oriented
learning to achieve fast transfer in lifelong RL. Given an off-policy RL algorithm A , the agent
~
♦ ∙, ∙ 1 ∙	∙ ,	,	1 1	1 c∙ . ∙ K , 1 , 1 1 ∙	, m	ι	1 ι ɪ`ɪ` a . . ι ι ♦ ♦
initializes its extended value function Q, the task binary vector T, and a goal buffer. At the beginning
of each episode, the agent computes TSOP and QSOP for T using the SOP method and its library of
learned task vectors and extended Q-functions. It then acts using the behaviour policy (E-greedy for
~
example) of A with QSOP for the action-value function if TSOP = T, and QSOP ∨ Q otherwise.3
~
τ , ` m	/ m	, ι	,	1	ι	K	, `	ι	1 ∙	, ι	1 ι ɪ`ɪ`	♦	/	* ι	ι ∙ , ∙	11	ι
If TSOP 6= T, the agent also updates Q for each goal in the goal buffer using A . Additionally, when
the agent reaches a terminal state s, it adds it to the goal buffer and updates T(S) using the reward it
receives (T (s) = 1rM (s,a)=rM AX). Training stops when the agent has reached the desired level of
~
.∙	1 ∙ .	/	t`.	∙	1	. ∙	∖	t`.	•>♦>>,•>	. Ii . 1	1	1 m	1 K .	♦.
optimality (or after n episodes in practice), after which the agent adds the learned T and Q to its
library if TSOP 6= T. The full algorithm is included in Appendix B. We refer to this algorithm as
S OPGOL (Sum Of Products with Goal-Oriented Learning).
When G is finite, we show in Theorem 2 that SOPGOL generalises over any unknown task distribution
after learning only a number of tasks logarithmic in the size of the task space. The lower bound is
dlog|G|e, since this is the minimum number of tasks that span the task space, as can be seen in Table
1 (top) for example. The upper bound is |G | because that is the dimensionality of the task binary
representations {0, 1}|G| . Since the number of tasks is |M| = 2|G|, we have that the upper bound
|G | = log|M| is logarithmic in the size of the task space.
2See Section 1.4 of the appendix for a detailed discussion of this with the simplification of the bound in
Proposition 1 (Barreto et al., 2018) to the same form as Theorem 1(i).
3
3Since QSOP ∨ Q = max{QSOP, Q}, it is equivalent to GPI and hence is guaranteed to be equal or
more optimal than the individual value functions. Hence using QSOP ∨ Q in the behaviour policy gives a
straightforward way of leveraging Q SOP to learn Q faster.
5
Published as a conference paper at ICLR 2022
Theorem 2. Let D be an unknown, possibly non-stationary, distribution over a set of tasks
M(S, A,p,γ, ro) with finite G. Let A : M → Q* be any map from M to Q* such that
A(M) = QM forall M in M. Let
ʌz	ʌz	ʌz
~ — , . __ ~ — _ , . . ~ — , __________________________________________
Tt+1, Q*+ι = SOPGOL(A, Mt, Tt, Qt) where Mt ~ D(t) and To = Q0 = 0 ∀t ∈ N.
Then,
dlog ∣G∣e ≤ lim Nt ≤ |G| where Nt := |T| = |Q*|.
t→∞
Interestingly, Theorem 2 holds even in the case where a new task is expressible in terms of past tasks
(TSOP = T), but we wish to solve it to a higher degree of optimality than past tasks. In this case, we
can pretend TSOP 6= T and learn a new Q-function to the desired degree of optimality. We can then
add it to our library, and remove any other skill from our library (the least optimal for example).
4 Experiments
Goals	1	■	S		*	B	f	•	≡		•	≡	% O		≡
Ta	1	0	1	0	1	0	1	0	1	0	1	0	1	0	1
Tb	0	1	1	0	0	1	1	0	0	1	1	0	0	1	1
Tc	0	0	0	1	1	1	1	0	0	0	0	1	1	1	1
Td	0	0	0	0	0	0	0	1	1	1	1	1	1	1	1
															
Goals		■	B	t	•	B	1	•	B	∖	•	B	端	O	≡
T1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1
T2	0	0	1	0	1	1	1	0	1	1	0	0	1	0	0
T3	1	1	0	1	0	0	0	1	0	1	1	0	0	1	1
Figure 1: PickUpObj domain.
The red triangle represents the
agent.
Table 1: Binary representation for base (top) and test (bottom)
tasks. 0 or 1 corresponds to a goal reward of rMIN or rMAX.
4.1 Transfer after pretraining on a set of tasks
We consider the PickUpObj domain from the MiniGrid environment (Chevalier-Boisvert et al.,
2018), illustrated by Figure 1, where an agent must navigate in a 2D room to pick up objects of
various shapes and colours from pixel observations.4 This type of domain is prototypical in the
literature (Nangue Tasse et al., 2020; Barreto et al., 2020; van Niekerk et al., 2019; Abel et al., 2018),
because it allows for easy demonstration of transfer learning in many-goal tasks. In this domain,
there are |G| = 15 goals each corresponding to picking up objects of 3 possible types—box, ball,
key—and 5 possible colours—red, blue, green, purple, and yellow. Hence a set of dlog2 |G|e = 4
base tasks can be selected that can be used to solve all 2|G| = 32768 possible tasks under a Boolean
composition of goals. The agent receives a reward of 2 when it picks up desired objects, and -0.1
otherwise. For all of our experiments in this section, we use deep Q-learning (Mnih et al., 2015)
as the RL method for SOPGOL and as the performance baseline. We also compare SOPGOL to
S OPGOL-transfer, or to SOPGOL-continual. S OPGOL-transfer refers to when no new skill is
learned and S OPGOL-continual refers to when a new skill is always learned using the SOP Q
estimate to speed up learning, even if the new task could be solved zero shot. Since SOPGOL
determines automatically which one to use, we compare whichever one it chooses with the other one
in each of our experiments.
We first demonstrate transfer learning after pretraining on a set of base tasks—a minimal set of tasks
that span the task space. This can be done if the set of goals is known upfront, by first assigning a
Boolean label to each goal in a table and then using the rows of the table as base tasks. These are
illustrated in Table 1 (top). Having learned the -optimal extended value functions for our base tasks,
4Further environment details are given in Appendix D.
6
Published as a conference paper at ICLR 2022
2	3	4	5
Episodes	le2
(a) M1
(b) M2
(c) M3
Figure 2: Episodic returns (top) and learned binary representations (bottom) for test tasks M1 , M2
and M3 after pretraining on the base set of tasks Ma , Mb, Mc and Md. The shaded regions on the
episodic returns indicate one standard deviation over 4 runs. The learned binary representations are
similarly averaged over 4 runs, and reported for the first 500 episodes. The initial drop in DQN
performance is as a result of the initial exploration phase where the exploration constant decays from
0.5 to 0.05. The Boolean expressions generated by SOPGOL during training for the respective test
tasks are:
M1	=	Ma ∧ Mb ∧ Mc ∧ Md ,
M2	=	(Ma ∧ -Mb ∧ -Md) ∨ (Ma ∧ Mc ∧ Md) ∨ (-Ma ∧ Mb ∧ -MC ∧ -Md)	∨	(-Ma	∧
—Mb ∧ —Mc ∧ Md),
M3	=	(Ma ∧ Mb ∧ Mc) ∨ (Ma ∧ -Mb ∧ -Md)	∨ (Ma ∧ MC ∧ Md) ∨ (-Ma ∧ Mb ∧ -MC	∧
—Md ) ∨ (—Ma ∧ —Mb ∧ —Mc ∧ Md ) ∨	(—Mb ∧ Mc ∧ —Md ).
we can now leverage logical composition for transfer learning on test tasks. We consider the three test
tasks shown in Table 1 (bottom). For each, we run SOPGOL, SOPGOL-continual, and a standard
DQN. Figure 2 illustrates the results where, as predicted by our theoretical results in Section 3.2,
SOPGOL correctly determines that the current test tasks are solvable from the logical combinations
of the learned base tasks. Its performance from the start of training is hence the best.
Now that we have demonstrated how SOPGOL enables an agent to solve any new task in an
environment after training on base tasks, we consider the more practical case where new tasks are
not fully expressible as a Boolean expression of previously learned tasks. The agent in this case
2	2	2
sωOD
SOPGOL
SOPGOL transfer
DQN
0.25	0.50	0.75	1.00	1.25	1.50	1.75
Episodes	le4
6	0.8	1.0
Episodes
—SOPGOL
——SOPGOLtransfer
——DQN
2	3	4	5 O
Episodes	le2
(b)	M2
SIumBa
sωOD
sωOD
2	3
Episodes
(c) M3
4	5
le2
O	1	2	3	4	5 O
Episodes	le2
(a)	M1
SEmBa
Figure 3: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2
and M3 after pretraining on the non-base set of tasks ■, ,■ and 鬟 The shaded regions on the episodic
returns indicate one standard deviation over 4 runs. The learned binary representations are similarly
averaged over 4 runs, and reported for the first 500 episodes. The initial drop in DQN performance is
a result of the initial exploration phase where the exploration constant decays from 0.5 to 0.05. The
Boolean expressions generated by SOPGOL for the respective test tasks are:
二 一 一 -
Mi	= —■ ∧ - ∧ ■ ∧ ―喀，
二	Z	一	、	， 一	一	小，一	一	小，一	八
M2	= (	∧ —■ ∧	— ) ∨	(—■ ∧ ■ ∧ —■ ∧ )) ∨ (— ∧ —■ ∧	■ ∧，) ∨ (—■	∧ - ∧ - %),
/ — — 一、， 一 、, 一 小
M3	= (—■ ∧ - ∧ —/)∨ (—■ ∧ — ) ∨ (—■ ∧ — ∧ —/).
7
Published as a conference paper at ICLR 2022
is pretrained on a set of tasks that do not span the task space, { ,■, ■,/}, corresponding to the
tasks of picking up green objects, blue objects, yellow objects, and keys. We then train the agent
with SOPGOL, SOPGOL-transfer, and a standard DQN on the same set of test tasks considered
previously (Table 1 (bottom)). The results in Figure 3 demonstrate how SOPGOL now chooses to
learn a task-specific skill after transfer, and hence outperforms S OPGOL-transfer since the test tasks
are not entirely expressible in terms of the pretrained ones. Consider Figure 3a, for example. The
test task is to pick up a yellow box, but the agent has only learned how to pick up red objects, blue
objects, yellow objects, and keys. It has not learned how to pick up boxes. However, we note from
the inferred Boolean expression (M1) that the agent correctly identifies that the desired objects are, at
the very least, yellow. Without further improvements to this transferred policy (S OPGOL-transfer),
we can see that this approach outperforms DQN from the start. This is due to two main factors: (i) the
transferred policy navigates to objects more reliably, so takes fewer random actions; and (ii) although
the transferred policy does not have a complete understanding of which are the desirable objects, it at
least navigates to yellow objects, which are sometimes yellow boxes.
Finally, since SOPGOL is able to determine that the current task is not entirely expressible in terms
of its previous tasks (by checking whether TSOP = T ), it is able to learn a new Q-value function
that improves on the transferred policy. Additionally, its returns are strictly higher than those of
SOPGOL-transfer because SOPGOL learns the new Q-value function faster by using QSOP ∨ Q In
the behaviour policy.
4.2 Lifelong transfer
In this section, we consider the more general setting where the agent is not necessarily given pretrained
skills upfront, but is rather presented with tasks sampled from some unknown distribution. We revisit
the example given in Section 3, but now more concretely by using a stochastic Four Rooms domain
(Sutton et al., 1999), with a goal space of size |G| = 40 and a task space of size |M| = 2|G| ≈
1012.
Complete environment details are given in Appendix C.
We demonstrate the ability of SOPGOL to generalise over task distributions by evaluating the
approach with the following distributions: (i) Dsampled : the goals for each task are chosen uniformly
at random over G; (ii) Dbest: the first dlog2 |G|e tasks are the base tasks, while the rest follow
Dsampled . This distribution gives the agent the minimum number of tasks to learn and store, since
the agent learns the base tasks first before being presented with any other task. (iii) Dworst : the first
|G | tasks are each defined by a single goal that differs from the previous tasks, while the rest follow
Dsampled . This distribution forces the agent to learn and store the maximum number of tasks, since
none of the |G| tasks can be expressed as a logical combination of the others. We use Q-learning
(Watkins, 1989) as the RL method for SOPGOL, and Q-learning with maxQ initialisation as a
baseline. This has been shown by previous work (Abel et al., 2018) to be a practical method of
initialising value functions with a theoretically optimal optimism criterion that speeds-up convergence
during training. Our results (Figure 4) show that SOPGOL enables a lifelong agent to quickly
generalise over an unknown task distribution. Interestingly, both graphs show that the convergence
(a) Number of policies learned and stored after
solving n tasks.
(b) Number of samples required to learn -
optimal policies for each task.
Figure 4: Number of policies learned and samples required for the first 50 tasks of an agent’s lifetime
in the Four Rooms domain. The shaded regions represent standard deviations over 25 runs.
8
Published as a conference paper at ICLR 2022
speed during a randomly sampled task distribution Dsampled is very close to that of the best task
distribution Dbest . This suggests that there is room to make the bound in Theorem 2 even tighter by
making some assumptions on the task distribution—an interesting avenue for future work.
5	Related work
There have been several approaches in recent years for tackling the problem of transfer in lifelong
RL. Most closely related is the line of work on concurrent skill composition (Todorov, 2009; Saxe
et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al., 2019). These methods
usually focus on multi-goal tasks, where they address the combinatorial amount of desirable goals by
composing learned skills to create new ones. Given a reward function that is well approximated by
a linear function, Barreto et al. (2020) propose a scheme for few-shot transfer in RL by combining
GPI and successor features (SF) (Barreto et al., 2017). In general, approaches based on GPI with SFs
(Barreto et al., 2021) are suitable for tasks defined by linear preferences over features (latent goal
states). Given the set of features for an environment, Alver & Precup (2022) shows that a base set of
successor features can be learned, which is sufficient to span the task space. While these approaches
also support tasks where goals are not terminal, the smallest number of successor features that must
be learned to span the task space is |G | (the upper-bound in Theorem 2). Our work is similar to these
approaches in that it can be interpreted as performing GPI with the logical composition of extended
value functions, which leads to stronger theoretical bounds than GPI with the linear composition of
successor features (see Appendix A.4). Finally, none of these works consider the lifelong RL setting
where an agent starts with no skill and receives tasks sampled from an unknown distribution (without
additional knowledge like base features or true task representations). In contrast, SOPGOL is able to
handle this setting with logarithmic bounds on the number of skills needed to generalise over the task
distribution (Theorem 2).
Other approaches like options (Sutton et al., 1999) and hierarchical RL (Barto & Mahadevan, 2003)
address the lifelong RL problem via temporal compositions. These methods are usually focused on
single-goal tasks, where they address the potentially long trajectories needed to reach a desired goal
by composing sub-goal skills sequentially (Levy et al., 2017; Bagaria & Konidaris, 2019). While they
do not consider the multi-goal setting, they can be used in conjunction with concurrent composition
to learn how to achieve a combinatorial amount of desirable long horizon goals. Finally, there are
also non-compositional approaches (Finn et al., 2017; Abel et al., 2018; Singh et al., 2021), which
usually aim to learn the policy for a new task faster by initializing the networks with some pre-training
procedure. These can be used in combination with SOPGOL to learn new skills faster.
6	Conclusion
In this work, we proposed an approach for efficient transfer learning in RL. Our framework, SOPGOL,
leverages the Boolean algebra framework of Nangue Tasse et al. (2020) to determine which skills
should be reused in a new task. We demonstrated that, if a new task is solvable using existing skills,
an agent is able to solve it with no further learning. However, even if this is not the case, an estimate
of the optimal value function can still be obtained to speed up training. This allows agents in a lifelong
learning setting to quickly generalise over any unknown (possibly non-stationary) task distribution.
The main limitation of this work is that it only consider tasks with binary goal rewards—where goals
are either desirable or not. Although this covers a vast number of many-goal tasks, combining our
framework with works on weighted composition (van Niekerk et al., 2019; Barreto et al., 2020) could
enable a similar level of generalisation over tasks with arbitrary goal rewards. Another exciting
avenue for future work would be to extend our transfer learning and generalisation results to include
temporal tasks by leveraging temporal composition approaches like options. Finally, we note that
just like previous work, we rely on the existence of an off-the-shelf RL method that is able to learn
goal-reaching tasks in a given environment. Since that is traditionally very sample inefficient, our
framework can be complemented with other transfer learning methods like MaxQInit (Abel et al.,
2018) to speed up the learning of new skills (over and above the transfer learning and task space
generalisation shown here). Our approach is a step towards the goal of truly general, long-lived
agents, which are able to generalise both within tasks, as well as over the distribution of possible
tasks it may encounter.
9
Published as a conference paper at ICLR 2022
Acknowledgements
GNT is supported by an IBM PhD Fellowship. This research was supported, in part, by the National
Research Foundation (NRF) of South Africa under grant number 117808. The content is solely the
responsibility of the authors and does not necessarily represent the official views of the NRF.
The authors acknowledge the Centre for High Performance Computing (CHPC), South Africa, for
providing computational resources to this research project. Computations were also performed using
High Performance Computing Infrastructure provided by the Mathematical Sciences Support unit at
the University of the Witwatersrand.
References
D. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman. Policy and value transfer in lifelong
reinforcement learning. In International Conference on Machine Learning, pp. 20-29, 2018.
S.	Alver and D. Precup. Constructing a good behavior basis for transfer using generalized policy
updates. In International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=7IWGzQ6gZ1D.
A. Bagaria and G. Konidaris. Option discovery using deep skill chaining. In International Conference
on Learning Representations, 2019.
A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 4055-4065, 2017.
A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy
improvement. In International Conference on Machine Learning, pp. 501-510. PMLR, 2018.
A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with generalized
policy updates. Proceedings of the National Academy of Sciences, 117(48):30079-30087, 2020.
A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Aygun, P Hamel, D. Toyama, J. Hunt, S. Mourad,
D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. arXiv preprint
arXiv:2106.13105, 2021.
A. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event
dynamic systems, 13(1):41-77, 2003.
M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym.
https://github.com/maximecb/gym-minigrid, 2018.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning, pp. 1126-1135. PMLR, 2017.
T.	Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement
learning for robotic manipulation. In 2018 IEEE International Conference on Robotics and
Automation, pp. 6244-6251, 2018.
J. Hunt, A. Barreto, T. Lillicrap, and N. Heess. Composing entropic policies using divergence
correction. In International Conference on Machine Learning, pp. 2911-2920, 2019.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of
Machine Learning Research, 11(Apr):1563-1600, 2010.
S.	Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The
Journal of Machine Learning Research, 17(1):1334-1373, 2016.
A. Levy, R. Platt, and K. Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 12,
2017.
10
Published as a conference paper at ICLR 2022
T.	. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In International Conference on Learning Representations,
2016.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,
A. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature,
518(7540):529, 2015.
G. Nangue Tasse, S. James, and B. Rosman. A Boolean task algebra for reinforcement learning.
Advances in Neural Information Processing Systems, 33, 2020.
X. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. MCP: Learning composable hierarchical
control with multiplicative compositional policies. In Advances in Neural Information Processing
Systems,pp. 3686-3697, 2019.
A. Saxe, A. Earle, and B. Rosman. Hierarchy through composition with multitask LMDPs. Interna-
tional Conference on Machine Learning, pp. 3017-3026, 2017.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):
354, 2017.
A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine. Parrot: Data-driven behavioral priors
for reinforcement learning. In International Conference on Learning Representations, 2021.
N. Subrahmanyam. Boolean vector spaces. Mathematische Zeitschrift, 83(5):422-433, 1964.
R.	Sutton, A. Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge,
1998.
R. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181-211, 1999.
M. Taylor and P. Stone. Transfer learning for reinforcement learning domains: a survey. Journal of
Machine Learning Research, 10:1633-1685, 2009.
S.	Thrun. Is learning the n-th thing any easier than learning the first? In Advances in neural
information processing systems, pp. 640-646, 1996.
E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing
Systems, pp. 1856-1864, 2009.
B.	van Niekerk, S. James, A. Earle, and B. Rosman. Composing value functions in reinforcement
learning. In International Conference on Machine Learning, pp. 6401-6409, 2019.
C.	Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge, 1989.
11
Published as a conference paper at ICLR 2022
A	Proofs of theoretical results
A. 1 B o olean Algebra Definition
Definition 6. A Boolean algebra is a set B equipped with the binary operators ∨ (disjunction) and ∧
(conjunction), and the unary operator  (negation), which satisfies the following Boolean algebra
axioms for a, b, c in B:
(i)	Idempotence: a ∧ a = a ∨ a = a.
(ii)	Commutativity: a ∧ b = b ∧ a and a ∨	b = b ∨ a.
(iii)	Associativity: a ∧ (b ∧ c) = (a ∧ b) ∧	c and a ∧ (b ∨ c)	=	(a ∨ b) ∨	c.
(iv)	Absorption: a ∧ (a ∨ b) = a ∨ (a ∧ b)	= a.
(v)	Distributivity: a ∧ (b ∨ c) = (a ∧ b) ∨	(a ∧ c) and a ∨ (b ∧	c)	= (a	∨ b) ∧ (a ∨ c).
(vi)	Identity: there exists 0, 1 in B such that
0∧a=0
0∨a=a
1∧a=a
1∨a=1
(vii)	Complements: for every a in B, there exists an element a0 in B such that a ∧ a0 = 0 and
a ∨ a0 = 1.
A.2 Proofs for Proposition 2
Lemma 1. Let M be a set of tasks. Then (M, ∨, ∧, , MMAX, MMIN) is a Boolean algebra.
Proof. Let M1,M2 ∈ M. We show that -, ∨, ∧ satisfy the Boolean properties (i) - (vii).
(i)-(v): These easily follow from the fact that the min and max functions satisfy the idempotent,
commutative, associative, absorption and distributive laws.
(vi): Let rMM AX ∧M1 and rM1 be the reward functions for MMAX ∧ M1 and M1 respectively.
Then for all (s, a) in S × A,
r	(s a)	min{rMAX,rM1(s,a)},	ifs ∈ G
MM AX ∧M1 ,	min{r0 (s, a), r0 (s, a)}, otherwise.
= rM1 (s, a), ifs ∈ G
r0 (s, a),	otherwise.
= rM1 (s, a).
(rM1 (s, a) ∈ {rMIN, rMAX} for s ∈ G)
Thus MMAX ∧ M1 = M1 . Similarly MMAX ∨ M1 = MMAX, MMIN ∧ M1 = MMIN,
and MMIN ∨ M1 = M1 . Hence MMIN and MMAX are the universal bounds ofM.
(vii): Let rM1∧-M1 be the reward function for Mi ∧ -Mi. Then for all (s, a) in SX A,
rM1∧-M1 (s,a)=
min{rM1 (s, a), (rMAX + rMIN) - rM1 (s, a)},
min{r0(s, a), (r0(s,a) + r0(s, a)) - r0(s, a)},
ifs ∈ G
otherwise.
{rMIN,	if S ∈ G and vm` (s, a) = rMAX
rMAX,	if s ∈ G and rM1 (s, a) = rMIN
r0 (s, a), otherwise.
rMMIN (s, a).
Thus Mi ∧ -Mi = MMIN, and similarly Mi ∨ -Mi = MMAX .
12
Published as a conference paper at ICLR 2022
□
Lemma 2. Let Q* be the set of optimal Q-value functions for tasks in M.	Then
(<2*, ∨, ∧,-, QMAX, QMIN) is a BooleanAlgebra.
Proof. Let QMl, fQM2 ∈ Q* be the optimal Q-value functions for tasks M、, M2 ∈ M with reward
functions vm1 and vm2 . We show that」,∨,八 satisfy the Boolean properties (i) - (vii).
(i)-(v): These follow directly from the properties of the min and max functions.
(vi): For all (s, g, a) in S ×G ×A,
(Qmax 八 QMI)(S,g,a) = min{QMAX(s, g, a), QMi (s, g, a)}
=Jmin{QMAX (S,g,a), QMAX (s,g, a)}, if rMi (g,a0) = rMAX ∀a0 ∈ A
lmin{QMAx(s, g, a), QMIN(s, g, a)},	otherwise.
=JQMAX(s,g, a), if rMi (g, a) = rMAX ∀a0 ∈ A
IQMIN (s, g, a), otherwise.
=QMi (s,g, a) (since rMi (g, a0) ∈ {rMiN,rMAx} ∀a0 ∈ A).
Similarly, QMAX ∨ QMI = QMAX, QMIN 八 QMI = qMin，and qMin ∨ QMI = QMi.
(vii): For all (.) in S ×G × A,
(QMi ∧-QMi)(.) = min{QMi(.),-QMi(.)}
=jmin{QMin(.),QMax(.)} if∣Q*(.)- QMin(.)1 ≤ IQ*(.) - QMAX(.)1
∖min{Q Max (∙),Q Min (∙)} otherwise,
=QMIN (J.
Similarly, QMIV -IQMI = QMAX
□
Lemma 3. Let Q* be the set of optimal extended JQ-value functions for tasks in M. Then
for all M1,M2 ∈ M, we have (i) Q-m1 = -QMi, (ii) QM1vm2 = QMi ∨ QM2, and
(叫 QMi λm2 = QMi 八 QM2.
Proof. Let M1, M2 ∈ M. Then for all (s, g, a) in S ×G × A,
(i):
Q-MI(S,g, a)
=J qMax (s, g,a),
IQMIN(s, g, a),
=J qMax (s, g,a),
IQMIN(s, g, a),
=J QMAX (s, g, a),
IQMIN(s, g, a),
= JQ*M AX (S, g, a),
IQMIN(s, g, a),
=-IQ Mi(S,g, a).
if r-Mi (g, a0) = rMAX ∀a0 ∈ A
otherwise.
if rMi (g, a0) = rMiN ∀a0 ∈ A
otherwise.
if Q Mi (s, g, a) = Q Min (s, g, a)
otherwise.
ifIQMi(S,g,a) - QMin(S,g, a)I ≤ IQMi(S,g,a)- QMAX(S,g, a)I
otherwise.
13
Published as a conference paper at ICLR 2022
(ii):
QMIVM2 (s,g,0)=
JqMax(Si g,a),
QqMin(S, g, α),
if rM1VM2 (g,a0) = γmax ∀a0 ∈ A
otherwise.
JQMAX (s,g,a), if max{rM1 (g,"rM2 (g, a0)} = γmax ∀a0 ∈ A
〔QMin(s, g, a),	otherwise.
JQMAX(S, g, a), if maχ{QMI(S,g, a), QM2(S,g, a)} = QMAX(S,g, a)
IQMIN(s, g, a),	otherwise.
max{QM1 (s, g, a), QM2 (s, g, a)}
(QM1∨ QM2)(s,g,a).
(iii): Follows similarly to (ii).
□
Proposition 2. Let Q* be the set ofoptimal Q-VgIUefUnCtiOnSfOr tasks in M. Let A : M → Q*
be any map from M to Q* such that A (M) = QM for all M in M. Then,
(i)	M and Q* respectively form a Boolean task algebra (M, V, ∧, -, MMAX, MMIN) and
a Boolean extended value functions algebra (Q*, V, ∧,—, qMax , qMin ),
(ii)	A is a homomorphism between M and Q*.
Proof. (i): Follows from Lemma 1 and 2.
(ii): Follows from Lemma 3.
□
A.3 Proofs for Theorem 1
Lemma 4. Let Q* be the Set of optimal Q-value functions for tasks Ln M. Denote QM as the
E-optimal (Q-value functionfor a task M ∈ M such that
**
IQM(s,g, a) - QM(s,g, a)| ≤ E forall (s,g, a) ∈S ×G× A.
Thenfor all Mi, M2 in M and (s, g, a) in S ×G × A,
(i)	∣[QM1 vQM2](s,g,a)-[QM1 vQM2](s,g,a)∣ ≤ E
(Ii)NQMI ∧QM2](s,g,a) - [QM1 ∧QM2](s,g,a) ∣ ≤ E
(Iii)卜QM1 (s,g,a)-rQM1 (s,g, a) I ≤ E
Proof. (i):
∣[QM1 V QM2](s, g, a) - [QM1 V QM2](s, g, a) ∣
* *
= maχ	QM(s,g,a) - maχ	QM(s,g,a)
M ∈{M1,M2}	M ∈{M1,M2}
≤ max 」QM(S,g,a) - QM(S,g,a) ∣
M ∈{M1,M2}∣ M	M	I
≤ E.
14
Published as a conference paper at ICLR 2022
(ii):
kQM1 八 QM2](s,g,α) - [QM1 八 ^5m2](s, g, a) I
∕s*	A*
= min	Q)m(s,g,a) -	min	Q)m(s,g,a)
M ∈{M1,M2}	M ∈{M1,M2}
≤ min -, 1 QM(s,g,α) - QM(s,g,a) ∣
M∈{M1,M2} I	I
≤ €.
(iii):
卜QMI (S,g, a) - -QMI (S,g, a)
1IQMAX (s, g,a) - -QMIN (s,g,a) I,
[IqMin(s,g,a) - -QMAX(S,g,a)|,
J 1qMax(S, g, a) - QMAX(S, g, a)I，
IIQMIN(s, g, a) - qMin(s, g, a)|,
if qM1 = qMin(s,g, a)
otherwise.
if qM1 = qMin (s,g,a)
otherwise.
≤ €.
□
Lemma 5. Let M ∈ M be a task with reward function r, binary representation T and optimal
extended action-value function Q *. Given €-approximations of the binary representations Tn =
{Tι,..., Tn} and optimal Q-functions Qn = {Q1,..., Qn} forn tasks M = {Mι,..., Mn} ⊆ M, let
~
TF = BEXP(Tn) and QF = BEXP(Qn),
where BEXP is derived from Tn and T using a generic method F. Define π(s) ∈
argmaxa∈A QF where QF := maxg∈g QF(s, g, a). Then,
IlQ* - QF∣∣∞ ≤ (1 T=TF)r∆ +€,
where 1 is the indicator function, r∆ ：= rmax 一 rmn, and ∣∣∕ 一 h∣∞ := maxs,3,a If (s,g,a) 一
h(s,g, a)I.
Proof.	_	_	_	_
IQ*(s,g, a) - QF(s,g, a)I = IQ*(s,g,a) - QF(s,g,a) + QF(s,g,a) - QF(s,g, a)I
≤ IQ*(s,g,a) - QF(s,g,a)I + IQF(s,g,a) - QF(s,g, a)I
≤ IQ*(s,g,a) - QF(s,g,a)I + €.	(Using Lemma4)
If T = TF ,then Q *(s, g, a) = Q F (s, g, a), and We aredone. Let T = TF. Without loss of generality,
let Q*(s, g, a) = QMAX(s, g, a) and qF(s, g, a) = qMin(s, g, a)∙ Then,
IQ*(s, g, a) - QF(s, g, a)I ≤ IQMax (s, g, a) - QMIN(s, g, a) i
≤ r∆.
□
Lemma 6. Let Q* and Q* be the optimal Q-value function and optimal extended Q-value function
respectivelyfor a deterministic task in M. Thenfor all (s, a) in S × A, we have
Q*(s, a) = maxQ*(s,g, a).
g∈G
15
Published as a conference paper at ICLR 2022
Proof. We first note that
Now define
{
max r(s, g, a)
g∈G
max{rMIN, r(s, a)},
max r(s, a),
g∈G
if s ∈ G
otherwise. = r(s, a).
(4)
Qmax(S'a) = max Q*(s,g, a).
Then it follows that
[TQmax] Ga) = r(s,a) + Y Ep(SlS,a)maA Qmax(S0,aO)
s0∈S	a ∈
=r(s, a) + γ	p(s0∣s, a) max max Q*(s0, g, a0)
s0∈S
=r(s, a) + γ	p(s0∣s, a) max max Q*(s0, g, a0)
s0∈S
=r(s, a) + max Y ɪ2 p(s0∣s, a) maxQ*(s0, g, a0)
s0∈S
(Since p is deterministic)
maxf(s, g, a) + max γ ɪ2 p(s0∣s, a) max Q*(s0,g,a0)	(Using Equation 4)
s0∈S
=max 厂(s,g, a)+ γ	p(s0∣s, a) max Q* (s0,g,a0),
g	s0∈S	a
since 尸(s, g, a) = r0(s, a) ∀s ∈ G andp(s, a, ω) = 1 with Q*(ω, g, a0) = 0 ∀s ∈ G.
=max Q*(s, g, a)
g∈G
= Qmax(s, a).
Hence Qmax is a fixed point of the Bellman optimality operator.
If s ∈ G, then
Q Max(S,a) = max Q*(S,g,a) = max 厂(S,g, a) = r(S,a) = Q*(S,a)∙
g∈G	g∈G
Since Qm^x = Q* holds in G and Qm^x is a fixed point of the Bellman operator, then Qmax = Q*
holds everywhere.
□
Theorem 1. Let M ∈ M be a task with reward function r, binary representation T and optimal
extended action-value function Q *. Given E-approximations of the binary representations T =
〜〜〜
{T1, ..., Tn} and optimal Q-functions Q*n = {Q1*, ..., Q*n} forn tasks M = {M1, ..., Mn} ⊆ M, let
TF = BEXP(Tn) and QF = BEXP(Qn),
where BEXP is derived from Tn and T using a generic method F. Define π(s) ∈
argmaxa∈A QF where QF := maxg∈g QF(s, g, a). Then,
⑴ kQ* - Qπk∞ ≤ 1-2Y ((I T=TF + 1 r∈{rg }|G| )r∆ + E),
(ii) if the dynamics are deterministic,
kQ* - QFk∞ ≤ (1T 6=TF )r∆ + E,
16
Published as a conference paper at ICLR 2022
where 1 is the indicator function, rg (s,a) ：= r(s, g, a), r δ ：= rmax 一 rmin, and Ilf 一 h∣∣∞ ：：
maxs,g,a |f(s, g, a) - h(s, g, a)|.
Proof. (i): We first note that each g in G can be thought of as defining an MDP Mg ：=
(S, A,p,rg,γ) with reward function rg(s,a) ：= r(s, g, a), optimal policy ∏g(s) = π*(s,g)
and optimal Q-value function Qng (s, a) = Q *(s, g, a). Then this proof follows similarly to that
of Barreto et al. (2017) Theorem 2,
Q*(s,a) — Qπ(s,a)
….	_*.	,	2	…
≤ Q (s, a) 一 Q g (s, a) +-((IT=TF)r∆ + C) (Barreto et al. (2017) Theorem 1)
1	一 Y
22
≤----max |r(s, a) 一 rq(s, a)| +-((1T=TF)r4 + c) (Barreto etal. (2017) Lemma 1)
1	一 γ s,a	1 一 γ F
22
≤ 1 一 Y (Ir=rg)r∆ + 1 一 Y ((IT=TF )r∆ + E)
(Since rewards only differ in G where r(s, a), rg(s, a) ∈ {rMIN, rMAX} for s ∈ G)
2
≤ γ-γ ((IT=TF + 1r=rg )r∆ + c)∙
Hence,
2
IlQ 一 Q k∞ ≤ 1---Y ((IT=TF + min 1r=rg )r∆ + E)
2
≤ 二"((IT=TF + 1r∕{rg }∣G∣)r∆ + e)
(Since min 1r6=rg = 0 only when r ∈ {rg}|G| ).
(ii):
∣Q*(s, a) — Qf(s, a)| = | maxQ*(s, g, a) — maxQF(s, g, a)| (Lemma 6)
gg
≤ max ∣Q"(s,g,a) — QF(s,g, a)|
g
≤ (1T 6=TF )r∆ + E. (Lemma 5)
□
A.4 Comparing the bounds of Theorem 1 with that of GPI in Barreto et al.
(2018)
We first restate Proposition 1 (Barreto et al., 2018) here.
πj*
Proposition 3 ((Barreto et al., 2018)). Let M ∈ M and let Qi j be the action value function of an
ππ
optimal policy of Mj ∈ M when executed in Mi ∈ M. Given approximations {Qiπ1 , ..., Qiπn } such
that ∣Qπj — Qπj | ≤ E for all s, a ∈ S ×A, and j ∈ {1,…，n}, let
π(s) ∈ arg max max Q∏j (s, a).
aj
then,
2
kQ 一 Q k∞ ≤ Y-y(Ilr — rik∞ +min Ilri — rjk∞ + e),
where Q* is the optimal value function of M, Qπ is the value function of π in M, and ∣∣f — h∣∞ ：
maxs,g,a |f(s, g, a) 一 h(s, g, a)|.
17
Published as a conference paper at ICLR 2022
We can simplify the bound in Proposition 3 as follows:
2
IlQ - Q k∞ ≤ ι-γ(Ilr - rik∞ +min Ilri - rjk∞ + E)
2
≤ 3---((1r=n)r∆ +mm Ilri — rjk∞ + E)
1-	γ	j
(Since rewards only differ in G where r(s, a), ri(s, a) ∈ {rMIN, rMAX} for s ∈ G)
2
≤ ---
一I - Y
2
≤----
一I - Y
((1r6=ri)r∆ + (mjin1ri6=rj)r∆ +E)
((Ir=Ti)r∆ + (1ri∈{rj }n )r∆ + E)
(Since min 1ri6=rj = 0 only when ri ∈ {rj}n )
2
≤ 1 - γ ((Ir=Ti + 1Ti∈{rj}n )r∆ + E).
where 1 is the indicator function, and r∆ := rMAX - rMIN. We can see that this bound is similar to
that of Theorem 1(i) but weaker. This because:
(i)	The first term of this bound (1r6=ri) requires that reward function of the current task (r) be
identical to that of a reference task (ri). In Barreto et al. (2018), ri is taken as the best linear
approximation of r. In contrast, the first term of Theorem 1(i) (1T 6=TF) only requires the
current task to be expressible as a Boolean composition of past tasks.
(ii)	The second term of this bound (1%/{彩}九)requires that the reference task (the best linear
approximation to the current task) is exactly one of the past tasks. In contrast, the second
term of Theorem 1(i) (1丁弑%}©) only requires the current task to have a single desirable
goal.
This suggests that we can can think of the logical composition approach as an efficient way of doing
GPI, one which leads to tight performance bounds on the transferred policy (Theorem 1(ii)).
A.5 Proofs for Theorem 2
Theorem 2. Let D be an unknown distribution, possibly non-stationary, over a set of tasks
M(S, A,p, γ, ro). Let A : M → Q* be any map from M to Q* such that A(M) = QM
for all M in M. Let
〜〜 〜
~. —, ~. —,. , . ~. —,
Tt+1, Q*+ι = SOPGOL(A, Mt, T, Q*) where Mt 〜D(t) and T = Q* = 0 ∀t ∈ N.
Then,
dlog ∣G∣e ≤ lim Nt ≤ |G|	where Nt ：= |T| = |Qt l∙
t→∞
Proof. Let Tt be the approximate binary representation of task Mt learned by SOPGOL. We first
note that SOPGOL returns Tt ∪ {Tt} only if Tt is not in the span of Tt. That is,
T+1 = T ∪{Tt} iff Tt = BEXP(T) Where BEXP = SOP(T, Tt).
Hence, it is sufficient to shoW that the number, N, of linearly independent binary vectors, T ∈
{0, 1}|G|, that span the Boolean vector space (Subrahmanyam, 1964), GF (2)|G|,5 is bounded by
dlog|G|e ≤N≤ |G|.
This folloWs from the fact that dlog |G|e is the size of a minimal set of generators for GF(2)|G| (as
can easily be seen With a Boolean table), and |G | is its dimensionality.
□
5GF(2) is the Galois field With tWo elements, ({0, 1}, +, .), Where + := XOR and . := AND.
18
Published as a conference paper at ICLR 2022
B Sum Of Products with Goal Oriented Learning
Algorithm 1 shows the full pseudo-code for SOPGOL. Here, SOP(T, T) is the classical sum
of products method in Boolean logic. Given a list of binary vectors T = [T1, ..., Tn], a Boolean
expression for a new binary vector T is obtained as follows:
1.	Identify all rows of T with a 1.
2.	For each such row: Make a product (conjunction) of all the input variables and make the
negation of each variable with a 0 in this row.
3.	Take the sum (disjunction) of all these product terms.
The output of SOP is a function BEXP that takes in |T| variables, and applies disjunctions, Con-
junctions and negations to them according to the Boolean expression obtained above.
Algorithm 1: SOPGOL
Input :off-policy RL algorithm A ,	/* e.g DQN */
task MDP M,
set of E-OPtimal task binary representations T,
set of E-OPtimal Q-value functions Q.
Initialise T : G → {0,1}
Initialise Q: S×G×A→ R according to A
Initialise goal buffer G with terminal states observed from a random policy
while Q is not converged do
Initialise state s from M
,~. ~.
BEXP 一 SOP(T,T)
ʃ—"
__ — _ _ , . _ ,—.,
Tsop, fQSOP -Bexp(T), BEXP(Q )
~
— — .. ~ ___________ — —
Q J QSOP if T = TsOP else Q ∨ QSOP
g — arg max I max Q(s,g0, a))
g∈G V∈a	)
while s is not terminal do
Select action a using the behaviour policy from A: a J π(s, g) / * e.g E-greedy
*/
Take action a, observe reward r and next state s0 in M
._ ɪ .—
if T = Tsop then
foreach g0 ∈ G do
r J rMIN if g = S ∈ G else r
Update Q With (s, g , a, r, s0) according to A
end
if s is terminal then
T(s) J 1r=rMAX
G JG ∪{s}
else
s J s0
end
end
end
,~. ~.
Bexp J SOP(T,T)
〜〜 〜〜
~. —	, ~. —. ~ , ~.. , ~. -~- -
T, Q J (T, Q) if T = Bexp(T) else (T ∪ {T}, Q ∪ {Q})
return T, Q
19
Published as a conference paper at ICLR 2022
C	Four Rooms Environment
We use the Four Rooms domain (Sutton et al., 1999), where an agent must navigate in a grid world to
particular locations. The goal locations are placed along the sides of the walls and at the centre of
rooms (Figure 5). This gives a goal space of size |G| = 40 and a task space of size |M| = 2|G| ≈
1012.
The agent can move in any of the four cardinal directions at each timestep, but colliding with a wall
leaves the agent in the same location. We add a 5th action for “stay” that the agent chooses to achieve
goals. A goal location only becomes terminal if the agent chooses to stay in it. All rewards are 0 at
non-terminal states, and 1 at the desirable goals. The transition dynamics are stochastic with a slip
probability (sp = 0.1). That is, with probability 1-sp the agent moves in the direction it chooses, and
with probability sp it moves in one of the other three chosen uniformly at random.
Figure 5: 40 goals Four Rooms domain with goals in green and the agent in red.
20
Published as a conference paper at ICLR 2022
D	Function Approximation Experiment Details
D.1 Environment
The PICKUPOBJ environment is fully observable, where each state observation is a 56 * 56 * 3 RGB
image (Figure 1). The agent has 7 actions it can take in this environment corresponding to: 1 - rotate
left, 2 - rotate right, 3 - move one step forward if there is no wall or object in front, 4 - pickup object
if there is an object in front and no object has been picked, 5 - drop the object in front if an object has
been picked and there is no wall or object in front, 6 - open the door in front if there is a closed-door
in front, and 7 - close the door in front if there is an opened door in front.
For each task, each episode starts with 1 desirable object and 4 other randomly chosen objects placed
randomly in the environment. The agent is also placed at a random position with a random orientation
at the start of each episode. The agent receives a reward of -0.1 at every timestep, and a reward
of 2 when it picks up a desirable object. The environment transitions to a terminal state once the
agent picks up any object and the agent observes the picked object. There are 15 types of objects
(illustrated in Table 1) resulting in 15 possible goal states. Hence, the dimension of the state space is
|S| = 56 × 56 × 3, the goal space is |G| = 15, and the action space is |A| = 7.
D.2 Network Architecture and Hyperparameters
In our function approximation experiments, We represent each extended value function Q* With a list
~
of |G| DQNs, such that the value function for each goal Qg* (s, a) := Q*(s, g, a) is approximated With
a separate DQN. The DQNs used have the folloWing architecture, With the CNN part being identical
to that used by Mnih et al. (2015):
1.	Three convolutional layers:
(a)	Layer 1 has 3 input channels, 32 output channels, a kernel size of 8 and a stride of 4.
(b)	Layer 2 has 32 input channels, 64 output channels, a kernel size of 4 and a stride of 2.
(c)	Layer 3 has 64 input channels, 64 output channels, a kernel size of 3 and a stride of 1.
2.	TWo fully-connected linear layers:
(a)	Layer 1 has input size 3136 and output size 512 and uses a ReLU activation function.
(b)	Layer 2 has input size 512 and output size 7 With no activation function.
We used the ADAM optimiser With batch size 256 and a learning rate of 10-3. We started training
after 1000 steps of random exploration and updated the target Q-netWork every 1000 steps. Finally,
We used -greedy exploration, annealing from 0.5 to 0.05 over 100000 timesteps.
Finally, We used the same DQN architecture and training hyperparameters for the baseline in all
experiments.
21