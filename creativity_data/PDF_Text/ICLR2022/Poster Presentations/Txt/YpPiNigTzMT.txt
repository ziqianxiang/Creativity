Published as a conference paper at ICLR 2022
Universalizing Weak Supervision
Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, Frederic Sala
Department of Computer Sciences
University of Wisconsin-Madison
{cshin23, wli525, hvishwakarma, ncroberts2, fsala}@wisc.edu
Ab stract
Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large
datasets for training data-hungry models. These approaches synthesize multiple noisy
but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for down-
stream training. However, the synthesis technique is specific to a particular kind of label,
such as binary labels or sequences, and each new label type requires manually designing
a new synthesis algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties, including practical
flexibility, computational efficiency, and theoretical guarantees. We apply this technique to
important problems previously not tackled by WS frameworks including learning to rank,
regression, and learning in hyperbolic space. Theoretically, our synthesis approach pro-
duces a consistent estimators for learning some challenging but important generalizations
of the exponential family model. Experimentally, we validate our framework and show
improvement over baselines in diverse settings including real-world learning-to-rank and
regression problems along with learning on hyperbolic manifolds.
1	Introduction
Weak supervision (WS) frameworks help overcome the labeling bottleneck: the challenge of building a large
dataset for use in training data-hungry deep models. WS approaches replace hand-labeling with synthesizing
multiple noisy but cheap sources, called labeling functions, applied to unlabeled data. As these sources may
vary in quality and be dependent, a crucial step is to model their accuracies and correlations. Informed by
this model, high-quality pseudolabels are produced and used to train a downstream model. This simple yet
flexible approach is highly successful in research and industry settings (Bach et al., 2019; Re et al., 2020).
WS frameworks offer three advantages: they are (i) flexible and subsume many existing ways to integrate
side information, (ii) computationally efficient, and (iii) they offer theoretical guarantees, including estimator
consistency. Unfortunately, these benefits come at the cost of being particular to very specific problem
settings: categorical, usually binary, labels. Extensions, e.g., to time-series data (Safranchik et al., 2020),
or to segmentation masks (Hooper et al., 2021), require a new model for source synthesis, a new algorithm,
and more. We seek to side-step this expensive one-at-a time process via a universal approach, enabling WS
to work in any problem setting while still providing the three advantageous properties above.
The main technical challenge for universal WS is the diversity of label settings: classification, structured
prediction, regression, rankings, and more. Each of these settings seemingly demands a different approach
for learning the source synthesis model, which we refer to as the label model. For example, Ratner et al.
(2019) assumed that the distribution of the sources and latent true label is an Ising model and relied on a
property of such distributions: that the inverse covariance matrix is graph-structured. It is not clear how to
lift such a property to spaces of permutations or to Riemannian manifolds.
1
Published as a conference paper at ICLR 2022
Label Embeddings
0V↦(T+l}d
or
I	9：y↦^d
I______
I__E[dg(λa,λb)]-
I Parameter Estimation
I Estimate parameters using
■	triplet method
，辐：θ
I ・•日♦
g[dg(λa,y)]
Universal Weak Supervision Framework
Label Inference
L
Label Model
Figure 1: Applications enabled by our approach (left) and weak supervision pipeline (right).
We propose a general recipe to handle any type of label. The data generation process for the weak sources
is modeled with an exponential family distribution that can represent a label from any metric space (Y, dY).
We embed labels from Y into two tractable choices of space: the Boolean hypercube {±1}d and Euclidean
space Rd . The label model (used for source synthesis) is learned with an efficient method-of-moments
approach in the embedding space. It only requires solving a number of scalar linear or quadratic equations.
Better yet, for certain cases, we show this estimator is consistent via finite sample bounds.
Experimentally, we demonstrate our approach on five choices of problems never before tackled in WS:
•	Learning rankings: on two real-world rankings tasks, our approach with as few as five sources performs
better than supervised learning with a smaller number of true labels. In contrast, an adaptation of the
Snorkel (Ratner et al., 2018) framework cannot reach this performance with as many as 18 sources.
•	Regression: on two real-world regression datasets, when using 6 or more labeling function, the perfor-
mance of our approach is comparable to fully-supervised models.
•	Learning in hyperbolic spaces: on a geodesic regression task in hyperbolic space, we consistently out-
perform fully-supervised learning, even when using only 3 labeling functions (LFs).
•	Estimation in generic metric spaces: in a synthetic setting of metric spaces induced by random graphs,
we demonstrate that our method handles LF heterogeneity better than the majority vote baseline.
•	Learning parse trees: in semantic dependency parsing, we outperform strong baseline models.
In each experiment, we confirm the following desirable behaviors of weak supervision: (i) more high-quality
and independent sources yield better pseudolabels, (ii) more pseudolabels can yield better performance
compared to training on fewer clean labels, (iii) when source accuracy varies, our approach outperforms
generalizations of majority vote.
2	Problem Formulation and Label Model
We give background on weak supervision frameworks, provide the problem formulation, describe our choice
of universal label model, special cases, the embedding technique we use to reduce our problem to two easily-
handled cases, and discuss end model training.
Background: WS frameworks are a principled way to integrate weak or noisy information to produce label
estimates. These weak sources include small pieces of code expressing heuristic principles, crowdworkers,
lookups in external knowledge bases, pretrained models, and many more (Karger et al., 2011; Mintz et al.,
2009; Gupta & Manning, 2014; Dehghani et al., 2017; Ratner et al., 2018). Given an unlabeled dataset, users
construct a set of labeling functions (LFs) based on weak sources and apply them to the data. The estimates
produced by each LF are synthesized to produce pseudolabels that can be used to train a downstream model.
2
Published as a conference paper at ICLR 2022
Problem Formulation: Let x1 , x2 , . . . , xn be a dataset of unlabeled datapoints from X . Associated with
these are labels from an arbitrary metric space Y with metric dY1. In conventional supervised learning,
we would have pairs (x1 , y1), . . . , (xn, yn); however, in WS, we do not have access to the labels. Instead,
we have estimates of y from m labeling functions (LFs). Each such LF s : X → Y produces an estimate
of the true label y from a datapoint x. We write λa (i) ∈ Y for the output of the ath labeling function
sa applied to the ith sample. Our goal is to obtain an estimate of the true label yi using the LF outputs
λ1(i),..., λm(i). This estimate y, is used to train a downstream model. To produce it, We learn the label
model P(λ1,λ2,..., λm∣y). The main challenge is that we never observe samples of y; it is latent.
We can summarize the weak supervision procedure in two steps:
•	Learning the label model: use the samples λa(i) to learn the label model P(λ1, λ2, . . . , λm |y),
•	Perform inference: compute yi, or P(yi∣λ1(i),..., λm(i)), or a related quantity.
Modeling the sources Previous approaches, e.g., Ratner et al. (2019), select a particular parametric choice
to model p(λ1, . . . , λm |y) that balances two goals: (i) model richness that captures differing LF accuracies
and correlations, and (ii) properties that permit efficient learning. Our setting demands greater generality.
However, we still wish to exploit the properties of exponential family models. The natural choice is
m
p(λ1,...,λm∣y) =彳 exp (X -θadγ (λa,y)+ X -θa,bdγ (λa ,λb)).
a=1	(a,b)∈E
、 一{z '	’ |	V	」
Accuracy Potentials	Correlation Potentials
(1)
Here, the set E is a set of correlations corresponding to the graphical representation of the model (Figure 1,
right). Observe how source quality is modeled in (1). If the value of θa is very large, any disagreement
between the estimate λa and y is penalized through the distance dY(λa, y) and so has low probability. If θa
is very small, such disagreements will be common; the source is inaccurate.
We also consider a more general version of (1). We replace -θadY(λa, y) with a per-source distance dθa .
For example, for Y = {±1}d, dθa (λa, y) = -θT ∣λa - y|, with θa ∈ Rd, generalizes the Hamming distance.
Similarly, for Y = Rd, we can generalize -θakλa - yk2 with dθa (λa, y) = -(λa - y)Tθa (λa - y), with
θa ∈ Rd×d p.d., so that LF errors are not necessarily isotropic. In the Appendix B, we detail variations and
special cases of such models along with relationships to existing weak supervision work. Below, we give a
selection of examples, noting that the last three cannot be tackled with existing methods.
•	Binary classification: Y = {±1}, dY is the Hamming distance: this yields a shifted Ising model for
standard binary classification, as in Fu et al. (2020).
•	Sequence learning: Y = {±1}d, dY is the Hamming distance: this yields an Ising model for sequences,
as in Sala et al. (2019) and Safranchik et al. (2020).
•	Ranking: Y = Sρ, the permutation group on {1, . . . , ρ}, dY is the Kendall tau distance. This is a
heterogenous Mallows model, where rankings are produced from varying-quality sources. If m = 1, we
obtain a variant of the conventional Mallows model (Mallows, 1957).
•	Regression: Y = R, dY is the squared `2 distance: it produces sources in Rd with Gaussian errors.
•	Learning on Riemannian manifolds: Y = M, a Riemannian manifold (e.g., hyperbolic space), dY is the
Riemannian distance dM induced by the space’s Riemannian metric.
Majority Vote (MV) and its Relatives A simplifying approach often used as a baseline in weak supervision
is the majority vote. Assume that the sources are conditionally independent (i.e. E is empty in (1)) and all
accuracies are identical. In this case, there is no need to learn the model (1); instead, the most “popular”
1We slightly abuse notation by allowing dY to be dc, where d is some base metric and c is an exponent. This permits
us to use, for example, the squared Euclidean distance—not itself a metric—without repeatedly writing the exponent c.
3
Published as a conference paper at ICLR 2022
Problem	Set	Distance	MV Equivalent	Im(g)	End Model	
Binary Classification	π±ιτ	'ι	Majority Vote	{±1},.	Binary Classifier
Ranking	SP_	Kendall tau	Kemeny Rule	{±1}(2)	Learning to Rank
Regression	^R	Squared-'2	Arithmetic Mean	R	Linear Regression
Riemannian Manifold	^M	Riemannian	FreChet Mean	Rd ,	Geodesic Regression
Dependency Parsing	T	'2	FreChet Mean	Rd×d	Parsing Model
Table 1: A variety of problems enabled by universal WS, with specifications for sets, distances, and models.
label is used. For binary labels, this is the majority label. In the universal setting, a natural generalization is
yMV = arg min — X IdY(λa,z).	(2)
z∈Y m	a=1
Special cases of (2) have their own name; for Sρ, itis the Kemeny rule (Kemeny, 1959). For dM, the squared
Riemannian manifold distance, yMV is called the Frechet or Karcher mean.
Majority vote, however, is insufficient in cases where there is variation in the source qualities. We must
learn the label model. However, generically learning (1) is an ambitious goal. Even cases that specialize
(1) in multiple ways have only recently been fully solved, e.g., the permutation case for identical θa ’s was
fully characterized by Mukherjee (2016). To overcome the challenge of generality, we opt for an embedding
approach that reduces our problem to two tractable cases.
Universality via Embeddings To deal with the very high level of generality, we reduce the problem to
just two metric spaces: the boolean hypercube {-1, +1}d and Euclidean space Rd. To do so, let g :
Y → {±1}d (or g : Y → Rd) be an injective embedding function. The advantage of this approach is
that if g is isometric—distance preserving—then probabilities are preserved under g. This is because the
sufficient statistics are preserved: for example, for g : Y → Rd, -θadY(λa, y) = -θad(g(λa), g(y)) =
-θa kg(λa) - g(y)kc , so that if g is a bijection, we obtain a multivariate normal for c = 2. If g is not
isometric, there is a rich literature on low-distortion embeddings, with Bourgain’s theorem as a cornerstone
result (Bourgain, 1985). This can be used to bound the error in recovering the parameters for any label type.
End Model Once we have produced pseudolabels—either by applying generalized majority vote or by
learning the label model and performing inference—we can use the labels to train an end model. Table 2
summarizes examples of problem types, explaining the underlying label set, the metric, the generalization
of majority vote, the embedding space Im(g), and an example ofan end model.
3 Universal Label Model Learning
Now that we have a specification of the label model distribution (1) (or its generalized form with per-source
distances), we must learn the distribution from the observed LFs λ1, . . . , λm . Afterwards, we can perform
inference to compute y or p(y∣λ1,..., λm) or a related quantity, and use these to train a downstream model.
A simplified model with an intuitive explanation for the isotropic Gaussian case is given in Appendix D.
Learning the Universal Label Model Our general approach is described in Algorithm 1; its steps can be
seen in the pipeline of Figure 1. It involves first computing an embedding g(λa) into {±1}d or Rd; we
use multidimensional scaling into Rd as our standard. Next, we learn the per-source mean parameters,
then finally compute the canonical parameters θa. The mean parameters are E[dG (g(λa)g(y))]; which
reduce to moments like E[g(λa)ig(y)i]. Here dG is some distance function associated with the embedding
space. To estimate the mean parameters without observing y (and thus not knowing g(y)), we exploit
observable quantities and conditional independence. As long as we can find, for each LF, two others that are
mutually conditionally independent, we can produce a simple non-linear system over the three sources (in
each component of the moment). Solving this system recovers the mean parameters up to sign. We recover
these as long as the LFs are better than random on average (see Ratner et al. (2019)).
4
Published as a conference paper at ICLR 2022
Algorithm 1: Universal Label Model Learning
Input: Output of labeling functions λa(i), correlation set E, prior p for Y , optionally embedding function g.
Embedding: If g is not given, use Multidimensional Scaling (MDS) to obtain embeddings g(λa) ∈ Rd ∀a
for a ∈ {1, 2, . . . , m} do
For b : (a,b) ∈ E EstimateCorrelations"i,j, E [g(λa)ig(λb)j] = n Pn=1 g(λa(t))ig(λb(t))j
Estimate Accuracy: Pick b, c : (a, b) 6∈ E, (a, c) 6∈ E, (b, c) 6∈ E.
if Im(g) = {±1}d
∀i, Estimate Oa,b = P(g(λ )i = 1,g(λlb)i = 1), Oa,c ,Ob,c, Estimate 'a = P(g(λ )i = 1), 'b,'c
Accuracies - QUADRATICTRIPLETS(Oa,b,Oa,c,Ob,c,'a,'b,'c,p,i)
else ∀i, Estimate ^a,b := E [g(λa)ig(λb)i] = * 1 P：=1 g(λa(t))ig(λb(t))i, ea,c, ^b,c
Accuracies J CONTINUOUSTRIPLETS(^a,b,^a,c, eb,c,p,i)
Recover accuracy signs (Fu et al., 2020)
end for
return θa , θa,b by running the backward mapping on accuracies and correlations
The systems formed by the conditional independence relations differ based on whether g maps to the Boolean
hypercube {±1}d or Euclidean space Rd. In the latter case we obtain a system that has a simple closed
form solution, detailed in Algorithm 2. In the discrete case (Algorithm 4, Appendix E), we need to use
the quadratic formula to solve the system. We require an estimate of the prior p on the label y; there are
techniques do so (Anandkumar et al., 2014; Ratner et al., 2019); we tacitly assume we have access to it.
The final step uses the backward mapping from mean to canonical parameters (Wainwright & Jordan, 2008).
This approach is general, but it is easy in special cases: for Gaussians the canonical θ parameters are the
inverse of the mean parameters.
Performing Inference: Maximum Likelihood Estimator Having estimated the label model canonical
. 久 ι A	r∙ ιι . 1
parameters θa and θa,b for all the
sources, we use the maximum-likelihood estimator
y = argmin — Xm 1 d^ (λa(i),z)
z∈Y m	a=1 θa
(3)
Compare this approach to majority vote (2), observing that MV can produce arbitrarily bad outcomes. For
example, suppose that we have m LFs, one of which has a very high accuracy (e.g., large θ1) and the others
very low accuracy (e.g., θ2, θ3, . . . , θm = 0). Then, λ1 is nearly always correct, while the other LFs are
nearly random. However, (2) weights them all equally, which will wash out the sole source of signal λ1. On
the other hand, (3) resolves the issue of equal weights on bad LFs by directly downweighting them.
Simplifications While the above models can handle very general scenarios, special cases are dramati-
cally simpler. In particular, in the case of isotropic Gaussian errors, where dθa (λa, y) = -θa kλa -
yk2, there is no need to perform an embedding, since we can directly rely on empirical averages like
1 Pn=1 dθa (λa (i), λb(i)). The continuous triplet step simplifies to directly estimating the covariance entries
θ-1; the backward map is simply inverting this. More details can be found in Appendix D.
4 Theoretical Analysis: Estimation Error & Consistency
We show that, under certain conditions, Algorithm 1 produces consistent estimators of the mean parameters.
We provide convergence rates for the estimators. As corollaries, we apply these to the settings of rankings
and regression, where isometric embeddings are available. Finally, we give a bound on the inconsistency
due to embedding distortion in the non-isometric case.
Boolean Hypercube Case We introduce the following finite-sample estimation error bound. It demonstrates
consistency for mean parameter estimation for a triplet of LFs when using Algorithm 1. To keep our pre-
sentation simple, we assume: (i) the class balance P (Y = y) are known, (ii) there are two possible values
5
Published as a conference paper at ICLR 2022
of Y , y1 and y2, (iii) we can correctly recover signs (see Ratner et al. (2019)), (iv) we can find at least three
conditionally independent labeling functions that can form a triplet, (v) the embeddings are isometric.
Theorem 4.1.	For any δ > 0, for some y1 and y2 with known class probabilities p = P (Y = y1), the
quadratic triplet method recovers αi = P (g(λa)i = 1|Y = y), βi = P (g(λb)i = 1|Y = y), γi =
P(g(λc)i = 1|Y = y) UP to error O((皿2//6 )1/4) With probability at least 1 — δ for any conditionally
independent label functions λa , λb , λc and for all i ∈ [d].
Algorithm 2: CONTINUOUSTRIPLETS
Input: Estimates 20m ^a,c, ^b,c, prior p, index i
Obtain variance Eg(Y)i2 from prior p
E[g(λa)g(y)] ~ p∣ea,b ∣∙∣^a,c∣∙ E[Y 2] ∕∣eb,c∣
E[g(λb)g(y)] ~ p∣ea,b∏eb,cl ∙ E[Y2] /瓦,/
E[g(λc)g(y)] _ P∣ea,c∣∙∣^b,c∣∙ E [Y2] / ∣ea,b∣
return Accuracies
E[g(Xa)g(y)], E[g(Xb)g(y)], E[g(T)g(y)]
We state our result via terms like αi =
P (g(Xa)i|Y = y); these can be used to obtain
E [g(λa)i∣y] and so recover E [dγ(λa, y)].
To showcase the power of this result, we apply it
to rankings from the symmetric group Sρ equipped
with the Kendall tau distance dτ (Kendall, 1938).
This estimation problem is more challenging than
learning the conventional Mallows model (Mal-
lows, 1957)—and the standard Kemeny rule (Ke-
meny, 1959) used for rank aggregation will fail if
applied to it. Our result yields consistent estimation
when coupled with the aggregation step (3).
We use the isometric embedding g into {±1}(2ρ): For π
∈ Sρ, each entry in g(π) corresponds to a pair (i, j)
with i < j , and this entry is 1 if in π we have i < j and —1 otherwise. We can show for π, γ ∈ Sρ that
Pi=1 g(∏)ig(γ)i = (2) — 2dτ(π,γ), and so recover E [g(λa)ig(y)i], and thus E [dτ(λa, y)]. Then,
Corollary 4.1.1. For any δ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet Xa , Xb, Xc, with parameters
U > θa, θb, θc > 4ln(2), we can recover θ0,θb, θc up to error O(g-1(θ + (log(2ρ2)∕(2δn))1/4) — θ) with
probability at least 1 一 δ, where g2(U) = (—ρe-U)/((1 一 e-U)2) + Pj=1(j2e-Uj)∕((1 - e-Uj)2).
Note that the error goes to zero as O(n-1/4). This is an outcome of using the quadratic system, which
is needed for generality. In the easier cases, where the quadratic approach is not needed (including the
regression case), the rate is O(n-1/2). Next, note that the scaling in terms of the embedding dimension d is
O((log(d))1/4)—this is efficient. The g2 function captures the cost of the backward mapping.
Euclidean Space Rd Case The following is an estimation error bound for the continuous triplets method in
regression, where we use the squared Euclidean distance. The result is for d = 1 but can be easily extended.
Theorem 4.2.	Let E [g(λa)g(y)] be an estimate of the accuracies E [g(Xa)g(y)] using n samples, where all
LFs are conditionally independent given Y. If the signs of a are recoverable, then with high probability
E[||E[g(Xa)g(y)] - E [g(λa)g(y)] ||2] = O ((a-X∣ + a-6in∣) PmaX(emax,emax
Here, a|min| = mini |E g(Xi)g(y) | and emax = maxj,k ej,k.
The error tends to zero with rate O(n-1/2 ) as expected.
Distortion & Inconsistency Bound Next, we show how to control the inconsistency in parameter estima-
tion as a function of the distortion. We write θ to be the vector of the canonical parameters, θ0 for their
distorted counterparts (obtained with a consistent estimator on the embedded distances), and μ, μ0 be the
corresponding mean parameters. Let 1 — ε ≤ dg (g(y), g(y0))/dY(y, y0) ≤ 1 for all y, y0 ∈ Y × Y. Then,
for a constant emin (the value of which we characterize in the Appendix).
6
Published as a conference paper at ICLR 2022
Theorem 4.3.	The inconsistency in estimating θ is boundedas llθ -θ0k ≤ εkμVemin.
5 Experiments
We evaluated our universal approach with four sample applications, all new to WS: learning to rank, regres-
sion, learning in hyperbolic space, and estimation in generic metric spaces given by random graphs.
Our hypothesis is that the universal approach is capable of learning a label model and producing high-quality
pseudolabels with the following properties:
•	The quality of the pseudolabels is a function of the number and quality of the available sources, with more
high-quality, independent sources yielding greater pseudolabel quality,
•	Despite pseudolabels being noisy, an end model trained on more pseudolabels can perform as well or
better than a fully supervised model trained on fewer true labels,
•	The label model and inference procedure (3) improves on the majority vote equivalent—but only when
LFs are of varying quality; for LFs of similar quality, the two will have similar performance.
Additionally, we expect to improve on naive applications of existing approaches that do not take structure
into account (such as using Snorkel (Ratner et al., 2018) by mapping permutations to integer classes).
Application I: Rankings We applied our approach to obtain pseudorankings to train a downstream rank-
ing model. We hypothesize that given enough signal, the produced pseudorankings can train a higher-quality
model than using a smaller proportion of true labels. We expect our method produces better performance
than the Snorkel baseline where permutations are converted into multi-class classification. We also anticipate
that our inference procedure (3) improves on the MV baseline (2) when LFs have differing accuracies.
Approach, Datasets, Labeling Functions, and End Model We used the isotropic simplification of Algo-
rithm 1. For inference, we applied (3). We compared against baseline fully-supervised models with only a
proportion of the true labels (e.g., 20%, 50%, ...), the Snorkel framework (Ratner et al., 2018) converting
rankings into classes, and majority vote (2). We used real-world datasets compatible with multiple label
types, including a movies dataset and the BoardGameGeek dataset (2017) (BGG), along with synthetic data.
For our movies dataset, we combined IMDb, TMDb, Rotten Tomatoes, and MovieLens movie review data
to obtain features and weak labels. In Movies dataset, rankings were generated by picking d = 5 film items
and producing a ranking based on their tMDb average rating. In BGG, we used the available rankings.
We created both real and simulated LFs. For simulated LFs, we sampled 1/3 of LFs from less noisy Mallows
model, 2/3 of LFs from very noisy Mallows model. Details are in the Appendix H. For real LFs, we built
labeling functions using external KBs as WS sources based on alternative movie ratings along with popular-
ity, revenue, and vote count-based LFs. For the end model, we used PTRanking (Yu, 2020), with ListMLE
(Xia et al., 2008) loss. We report the Kendall tau distance (dτ ).
Results Figure 2 reports end model performance for the two datasets with varying numbers of simulated LFs.
We observe that (i) as few as 12 LFs are sufficient to improve on a fully-supervised model trained on less data
(as much as 20% of the dataset) and that (ii) as more LFs are added, and signal improves, performance also
improves—as expected. Crucially, the Snorkel baseline, where rankings are mapped into classes, cannot
perform as well as the universal approach; it is not meant to be effective general label settings. Table 2
shows the results when using real LFs, some good, some bad, constructed from alternative ratings and simple
heuristics. Alternative ratings are quite accurate: MV and our method perform similarly. However, when
poorer-quality LFs are added, MV rule tends to degrade more than our proposed model, as we anticipated.
Application II: Regression We used universal WS in the regression setting. We expect that with more
LFs, we can obtain increasingly high-quality pseudolabels, eventually matching fully-supervised baselines.
Approach, Datasets, Labeling Functions, End Model, Results We used Algorithm 1, which uses the
continuous triplets approach 2. For inference, we used the Gaussians simplification. As before, we compared
7
Published as a conference paper at ICLR 2022
0.35
00.30
I
0.25
0.40
0.35
巧 0.30
3
Mo.25
0.20
- WS (Majority Vote)
-WS (Ours)
Fully supervised (20%)
—Fully supervised (50%)
----Fully supervised (100%)
-WS (Snorkel)
μ estimation error
r ■ r ■
ZIuau。luEed n1
r ■
Ziuau。UJSed --QL
Σ estimation error
Wsw U£9 9-e>A
Label estimation error
3	6	9	12 15 18	3	6	9	12	15 18
Num LFs	Num LFs
ιo3 ið5	io5 iδs	ιo3 ι∂5
Samples	Samples	Samples
(a) Movies dataset
(b) BGG dataset
(a) Parameter Estimation Error
Figure 2: End model performance with ranking LFs
(Left: Movies, Right: BGG). Training a model on pseu-
dolabels is compared to fully-supervised baselines on vary-
ing proportions of the dataset along with the Snorkel base-
line. Metric is the Kendall tau distance; lower is better.
Figure 3: Regression: parameter estimation error
(left two plots) and label estimation error comparing
to majority vote baseline (rightmost) with increasing
number of samples.
Setting	dτ	MSE	Setting	dτ	MSE
Fully supervised (10%)	0.2731	0.3357	WS (One LF, Rotten tomatoes)	0.2495	0.4272
Fully supervised (25%)	0.2465	0.2705	WS (One LF, IMDb score)	0.2289	0.2990
Fully supervised (50%)	0.2313	0.2399	WS (One LF, MovieLens score)	0.2358	0.2690
Fully supervised (100%)	0.2282	0.2106			
-WS (3 LFs, MV (2))-	0.2273	0.2754	WS (3 scores + 3 bad LFs, MV (2))	0.2504	-
WS (3 LFs, OUrS)	0.2274	0.2451	WS (3 good + 3 bad LFs, Ours)	0.2437	-
Table 2: End model performance With real-world rankings and regression LFs on movies. WS (3 scores, ∙) shows the
result of our algorithm combining 3 LFs. In ranking, high-quality LFs perform well (and better than fewer clean labels),
but mixing in lower-quality LFs hurts majority vote (2) more than our proposed approach. In regression, our method
yields performance similar to fully-supervised with 50% data, while outperforming MV.
against baseline fully-supervised models with a fraction of the true labels (e.g., 20%, 50%, ...) and MV (2).
We used the Movies rating datasets with the label being the average rating of TMDb review scores across
users and the BGG dataset with the label being the average rating of board games. We split data into 75%
for training set, and 25% for the test set. For real-world LFs, we used other movie ratings in the movies
dataset. Details are in the Appendix H for our synthetic LF generation procedure.For the end model, we
used gradient boosting (Friedman, 2001). The performance metric is MSE.
Figure 9 (Appendix) shows the end model performance with WS compared to fully supervised on the movie
reviews and board game reviews datasets. We also show parameter estimation error in Figure 3. As expected,
our parameter estimation error goes down in the amount of available data. Similarly, our label estimator is
consistent, while majority vote is not. Table 2 also reports the result of using real LFs for movies. Here, MV
shows even worse performance than the best individual LF - Movie Lens Score. On the other side, our label
model lower MSE than the best individual LF, giving similar performance with fully supervised learning
with 50% training data.
Application III: Geodesic Regression in Hyperbolic Space Next, we evaluate our approach on the prob-
lem of geodesic regression on a Riemannian manifold M , specifically, a hyperbolic space with curvature
K = -50. The goal of this task is analogous to Euclidean linear regression, except the dependent variable
lies on a geodesic in hyperbolic space. Further background is in the Appendix H.
Approach, Datasets, LFs, Results We generate y* by taking points along a geodesic (a generalization of
a line) parametrized by a tangent vector β starting at p, further affecting them by noise. The objective of
the end-model is to recover parameters p and β, which is done using Riemannian gradient descent (RGD)
to minimize the least-squares objective: p,β = arg minq,α Pn=ι d(expq(xiɑ),y.2 where d(∙, ∙) is the
hyperbolic distance. To generate each LF λj, we use noisier estimates of y*, where the distribution is
N(0,σ2) and σj 〜 U[i.5+(i5zj),4.5+(i5zj)], Zj 〜 Bernoulli(0.5) to simulate heterogeneity across LFs;
8
Published as a conference paper at ICLR 2022
Fully supervised (80%)
Fully supervised (90%)
Fully supervised (100%)
WS (Ours)
WS (Majority Vote)
u
2
□
Theta average
(a) Heterogeneous LFs
Figure 5: Comparison between our label model and ma-
jority voting in generic metric space. Metric is accuracy;
higher is better.
Theta average
(b) Homogenous LFs
Figure 4: Comparison between our approach, (2),
and fully-supervised in geodesic regression. Metric
is least-squares objective; lower is better.
the noise vectors are parallel transported to the appropriate space. For label model learning, we used the
isotropic Gaussian simplification. Finally, inference used RGD to compute (3). We include MV (2) as a
baseline. We compare fully supervised end models each with n ∈ {80, 90, 100} labeled examples to weak
supervision using only Weak labels. In Figure 4 We see that the Frechet mean baseline and our approach both
outperform fully supervised geodesic regression despite the total lack of labeled examples. Intuitively, this
is because With multiple noisier labels, We can produce a better pseudolabel than a single (but less noisy)
label. As expected, our approach yields consistently lower test loss than the Frechet mean baseline.
Dataset/UAS	Y	λ1	λ2	λ3
cs_pdt-ud	0.873	0.861	0.758	0.842
en_ewt-ud	0.795	0.792	0.733	0.792
en_lines-ud	0.850	0.833	0.847	0.825
en_partut-ud	0.866	0.869	0.866	0.817
Table 3: UAS scores for semantic dependency pars-
ing. Y is synthesized from off-the-shelf parsing LFs.
Application IV: Generic Metric Spaces We also
evaluated our approach on a structureless problem—
a generic metric space generated by selecting random
graphs G With a fixed number of nodes and edges. The
metric is the shortest-hop distance betWeen a pair of
nodes. We sampled nodes uniformly and obtain LF val-
ues With (1). Despite the lack of structure, We still an-
ticipate that our approach Will succeed in recovering the
latent nodes y When LFs have sufficiently high quality.
We expect that the LM improves on MV (2) When LFs
have heterogeneous quality, While the tWo Will have similar performance on homogeneous quality LFs.
Approach, Datasets, LFs, Results We generated random graphs and computed the distance matrix yield-
ing the metric. We used Algorithm (1) With isotropic Gaussian embedding and continuous triplets. For
label model inference, We used (3). Figure 5 shoWs results on our generic metric space experiment. As ex-
pected, When LFs have a heterogeneous quality, LM yield better accuracy than MV. HoWever, When labeling
functions are of similar quality, LM and MV give similar accuracies.
Application V: Semantic Dependency Parsing We ran our technique on semantic dependency parsing
tasks, using datasets in English and Czech from the Universal Dependencies collection (Nivre et al., 2020).
The LFs are off-the-shelf parsers from Stanza (Qi et al., 2020) trained on different datasets in the same
language. We model a space of trees With a metric given by the `2 norm on the difference betWeen the
adjacency matrices. We measure the quality of the synthesized tree Y with the unlabeled attachment scores
(UAS). Our results are shoWn in Table 3. As expected, When the parsers are of different quality, We can
obtain a better result.
6 Conclusion
Weak supervision approaches allow users to overcome the manual labeling bottleneck for dataset construc-
tion. While successful, such methods do not apply to each potential label type. We proposed an approach to
universally apply WS, demonstrating it for three applications new to WS: rankings, regression, and learning
in hyperbolic space. We hope our proposed technique encourages applying WS to many more applications.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision)
and the Wisconsin Alumni Research Foundation (WARF).
References
Imdb movie dataset. https://www.imdb.com/interfaces/.
MSLR-WEB10K. https://www.microsoft.com/en-us/research/project/mslr/.
Tmdb 5k movie dataset version 2. https://www.kaggle.com/tmdb/tmdb-movie-metadata.
BoardGameGeek Reviews Version 2.	https://www.kaggle.com/jvanelteren/
boardgamegeek-reviews, 2017.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decom-
positions for learning latent variable models. Journal of Machine Learning Research, 15:2773-2832,
2014.
Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in deploying weak
supervision at industrial scale. In Proceedings of the 2019 International Conference on Management of
Data,pp. 362-375, 2019.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proc. of the
eleventh annual conference on Computational learning theory, pp. 92-100. ACM, 1998.
J. Bourgain. On lipschitz embedding of finite metric spaces in hilbert space. Israel Journal of Mathematics,
52(1-2):46-52, 1985.
Robert Busa-Fekete, Dimitris Fotakis, Balazs Szorenyi, and Manolis Zampetakis. Optimal learning of mal-
lows block model. In Alina Beygelzimer and Daniel Hsu (eds.), Conference on Learning Theory, COLT
2019, 2019.
Ioannis Caragiannis, Ariel D. Procaccia, and Nisarg Shah. When do noisy votes reveal the truth? ACM
Trans. Econ. Comput., 4(3), March 2016. ISSN 2167-8375. doi: 10.1145/2892565. URL https:
//doi.org/10.1145/2892565.
Arun Tejasvi Chaganty and Percy Liang. Estimating latent-variable graphical models using moments and
likelihoods. In International Conference on Machine Learning, pp. 1872-1880, 2014.
Mayee F. Chen, Benjamin Cohen-Wang, Stephen Mussmann, Frederic Sala, and Christopher Re. Comparing
the value of labeled and unlabeled data in method-of-moments latent variable estimation. In International
Conference on Artificial Intelligence and Statistics (AISTATS), 2021.
Flavio Chierichetti, Anirban Dasgupta, Ravi Kumar, and Silvio Lattanzi. On reconstructing a hidden permu-
tation. In 17th Int’l Workshop on Approximation Algorithms for Combinatorial Optimization Problems
(APPROX’14), 2014.
Andrew Davenport and Jayant Kalagnanam. A computational study of the kemeny rule for preference
aggregation. In Proceedings of the 19th national conference on Artifical intelligence (AAAI 04), 2004.
Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using
the em algorithm. Applied statistics, pp. 20-28, 1979.
10
Published as a conference paper at ICLR 2022
Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. Neural ranking
models with weak supervision. In Proceedings of the 40th International ACM SIGIR Conferenceon Re-
search and Development in Information Retrieval, 2017.
M.A. Fligner and J.S. Verducci. Distance based ranking models. J.R. Statist. Soc. B, 48(3), 1986.
URL https://www-jstor-org.ezproxy.library.wisc.edu/stable/2345433?seq=
1#metadata_info_tab_contents.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pp.
1189-1232, 2001.
Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher Re.
Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of the 37th
International Conference on Machine Learning (ICML 2020), 2020.
Sonal Gupta and Christopher D Manning. Improved pattern learning for bootstrapped entity extraction. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pp. 98-108,
2014.
Sarah Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis Langlotz,
and Christopher Re. Cut out the annotator, keep the cutout: better segmentation with weak supervision.
In Proceedings of the International Conference on Learning Representations (ICLR 2021), 2021.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning, pp. 448-456. PMLR, 2015.
Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran. Evaluating the crowd with confidence.
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 686-694, 2013.
Manas Joglekar, Hector Garcia-Molina, and Aditya Parameswaran. Comprehensive and reliable crowd
assessment algorithms. In Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pp.
195-206. IEEE, 2015.
David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In
Advances in neural information processing systems, pp. 1953-1961, 2011.
J. Kemeny. Mathematics without numbers. Daedalus, 88(4):577-591, 1959.
Maurice Kendall. A new measure of rank correlation. Biometrika, pp. 81-89, 1938.
Claire Kenyon-Mathieu and Warren Schudy. How to rank with few errors. In Proceedings of the thirty-ninth
annual ACM symposium on Theory of computing (STOC ’07), 2007.
C. L. Mallows. Non-Null Ranking Models. I. Biometrika, 44, 1957. doi: 10.1093/biomet/44.1-2.114. URL
https://doi.org/10.1093/biomet/44.1-2.114.
J. Marden. Analyzing and modeling rank data. Chapman and Hall/CRC, 2014.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pp.
1003-1011. Association for Computational Linguistics, 2009.
Sumit Mukherjee. Estimation in exponential families on permutations. The Annals of Statistics, 44(2):
853-875, 2016.
11
Published as a conference paper at ICLR 2022
Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D. Manning, Sampo
Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An ever-
growing multilingual treebank collection. In Proceedings of the 12th Language Resources and Evalu-
ation Conference, pp. 4034-4043, Marseille, France, May 2020. URL https://aClanthology.
org/2020.lrec-1.497.
R. L. Plackett. The analysis of permutations. J. Applied Statistics, 24:193-202, 1975.
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: A Python natural
language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics: System Demonstrations, 2020. URL https://nlp.
stanford.edu/pubs/qi2020stanza.pdf.
Aditi Raghunathan, Roy Frostig, John Duchi, and Percy Liang. Estimation from indirect supervision with
linear moments. In International conference on machine learning, pp. 2568-2577, 2016.
A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. Re. Data programming: Creating large
training sets, quickly. In Proceedings of the 29th Conference on Neural Information Processing Systems
(NIPS 2016), Barcelona, Spain, 2016.
A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Re. Training complex models with
multi-task weak supervision. In Proceedings of the AAAI Conference on Artificial Intelligence, Honolulu,
Hawaii, 2019.
Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. Snorkel:
Rapid training data creation with weak supervision. In Proceedings of the 44th International Conference
on Very Large Data Bases (VLDB), Rio de Janeiro, Brazil, 2018.
Christopher Re, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. Overton: A data system for
monitoring and improving machine-learned products. In Proceedings of the 10th Annual Conference on
Innovative Data Systems Research, 2020.
Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from noisy rules.
In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 5570-5578, Apr. 2020.
Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa, Saelig Khattar, Ashwini Ramamoor-
thy, Ke Xiao, Kayvon Fatahalian, James Priest, and Christopher Re. Multi-resolution weak supervision
for sequential data. In Advances in Neural Information Processing Systems 32, pp. 192-203, 2019.
Joel A. Tropp. An Introduction to Matrix Concentration Inequalities. 2014.
P. Varma, F. Sala, A. He, A. J. Ratner, and C. Re. Learning dependency structures for weak supervision
models. In Proceedings of the 36th International Conference on Machine Learning (ICML 2019), 2019.
Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational infer-
ence. Foundations and Trends® in Machine Learning, 1(1-2):1-305, 2008.
Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank:
theory and algorithm. In Proceedings of the 25th international conference on Machine learning, pp.
1192-1199, 2008.
Hai-Tao Yu. Pt-ranking: A benchmarking platform for neural learning-to-rank. arXiv preprint
arXiv:2008.13368, 2020.
12
Published as a conference paper at ICLR 2022
The appendix contains additional details, proofs, and experimental results. The glossary contains a conve-
nient reminder of our terminology (Appendix A). We provide a detailed related work section, explaining
the context for our work (Appendix B). Next, we give the statement of the quadratic triplets algorithm (Ap-
pendix E). Afterwards, we give additional theoretical details: more details on the backward mapping, along
with an extended discussion on the rankings problem. We continue with proofs of Theorem 4.1.1 and Theo-
rem 4.2 (Appendix F). Finally, we give more details on the experimental setup (Appendix H) and additional
experimental results including partial rankings (Appendix I, J).
A Glossary
The glossary is given in Table 4 below.
Symbol	Definition
X Y dY x1 , x2, . . . , xn y1,y2,. . . ,yn s1, s2, . . . , sm 12	m λ ,λ ,...,λ n m λa(i) θa, θa,b E[da(λa, y)], E[da(λa, λb)] g ρ ea,b Oa,b la Sρ π dτ (∙, I	feature space label metric space label metric unlabeled datapoints from X latent (unobserved) labels from Y labeling functions / sources output of labeling functions number of data points number of LFs output of ath labeling function applied to ith sample xi canonical parameters in model (1) mean parameters in (1) injective mapping g : Y → Rd or {±1}d number of items in ranking setting E[g(λa)ig(λb)i] P(g(λa)i=1,g(λb)i=1) P(g(λa)i = 1) symmetric group on {1, ..., ρ} permutation ∈ Sρ Kendall tau distance on permutations (Kendall, 1938)
Table 4: Glossary of variables and symbols used in this paper.
B	Related Work
Weak Supervision Existing weak supervision frameworks, starting with Ratner et al. (2016), select a
particular model for the joint distribution among the sources and the latent true label, and then use the
properties of the distribution to select an algorithm to learn the parameters. In Ratner et al. (2016), a factor
model is used and a Gibbs-sampling based optimizer is the algorithm of choice. In Ratner et al. (2018), the
model is a discrete Markov random field (MRF), and in particular, an Ising model. The algorithm used to
learn the parameters solves a linear system obtained from a component of the inverse covariance matrix. In
Sala et al. (2019) and Fu et al. (2020), the requirement to use the inverse covariance matrix is removed, and
a set of systems among as few as three sources are used instead. These systems have closed-form solutions.
All of these models could be represented within our framework. The quadratic triplets idea is described, but
13
Published as a conference paper at ICLR 2022
not analyzed, in the appendix of Fu et al. (2020), applied to just the particular case of Ising models with
singleton potentials. Our work uses these ideas and their extensions to the general setting, and provides
theoretical analyses for the two sample applications we are interested in.
All of these papers refer to weak supervision frameworks; all of them permit the use of various types of
information as labeling functions. For example, labeling functions might include crowdworkers Karger
et al. (2011), distant supervision Mintz et al. (2009), co-training Blum & Mitchell (1998), and many others.
Note that works like Dehghani et al. (2017) provide one type of weak supervision signal for rankings and
can be used as a high-quality labeling function in our framework for the rankings case. That is, our work
can integrate such sources, but does not directly compete with them.
The idea of learning notions like source accuracy and correlations, despite the presence of the latent label, is
central to weak supervision. It has been shown up in other problems as well, such as crowdsourcing Dawid
& Skene (1979); Karger et al. (2011) or topic modeling Anandkumar et al. (2014). Early approaches use ex-
pectation maximization (EM), but in the last decade, a number of exciting approaches have been introduced
based on the method of moments. These include the tensor power method approach of Anandkumar et al.
(2014) and its follow-on work, the explicitly crowdsourcing setting of Joglekar et al. (2013; 2015), and the
graphical model learning procedures of Chaganty & Liang (2014); Raghunathan et al. (2016). Our approach
can be thought of as an extension of some of these approaches to the more general setting we consider.
Comparison With Existing Label Models There are several existing label models; these closely resemble
(1) under particular specializations. For example, one of the models used for binary labels in Ratner et al.
(2019); Varma et al. (2019) is the Ising model for λ1 , . . . , y ∈ {±1}
1m
p(λ1,...,λm,y) = 7exp (X θaλay + X θa,bλaλb + θγ y).	(4)
a=1	(a,b)∈E
A difference is that this is a joint model; it assumes y is part of the model and adds a singleton potential prior
term to it. Note that this model promotes agreement λay rather than penalizes disagreement -λadY(λa, y),
but this is nearly equivalent.
Rankings and Permutations One of our chief applications is to rankings. There are several classes of
distributions over permutations, including the Mallows model (Mallows, 1957) and the Plackett-Luce model
(Plackett, 1975). We are particularly concerned with the Mallows model in this work, as it can be extended
naturally to include labeling functions and the latent true label. Other generalizations include the gener-
alized Mallows model (Marden, 2014) and the Mallows block model (Busa-Fekete et al., 2019). These
generalizations, however, do not cover the heterogenous accuracy setting we are interested in.
A common goal is to learn the parameters of the model (the single parameter θ in the conventional Mallows
model) and then to learn the central permutation that the samples are drawn from. A number of works in this
direction include Caragiannis et al. (2016); Mukherjee (2016); Busa-Fekete et al. (2019). Our work extends
results of this flavor to the generalization on the Mallows case that we consider. In order to learn the center
permutation, estimators like the Kemeny rule (the procedure we generalize in this work) are used. Studies
of the Kemeny rule include Kenyon-Mathieu & Schudy (2007); Caragiannis et al. (2016).
C Extension to mixed label types
In the body of the paper, we discussed models for tasks where only one label type is considered. As a simple
extension, we can operate with multiple label types. For example, this might include a classification task
where weak label sources give their output as class labels and also might provide confidence values; that is, a
pair of label types that include a discrete value and a continuous value. The main idea is to construct a finite
14
Published as a conference paper at ICLR 2022
product space with individual label types. Suppose there are k possible label types, i.e. Yι, Y2,…，Yk. We
construct the label space by the Cartesian product
Y = Yi ×Y2 X …XYk.
All that is left is to define the distance:
k
d2Y(y1,y2) =	d2Yi(proji(y1),proji(y2)),
i=1
where proji is projection onto the i-th factor space. Then, using this combination, we extend the exponential
family model (1), yielding
mk
p(λ1,…，λm Iy) = z eχp (XX -θai)dYi (Proji(Xa), Proji(U))
k
+ X X -θa(i,)bdYi(proji(λa), proji(λb)).
(a,b)∈E i=1
We can learn the parameters θa(i) , θa(i,)b by Algorithm 1 using the same approach. Similarly, the inference
procedure (3) can be extended to
mk
yj = argminE £ dθ(i) (PrOji(λa(j)), PrOji(Z)).
z∈Y a=1 i=1	a
There are two additional aspects worth mentioning. First, the user may wish to consider the scale of distances
in each label space, since the scale of one of the factor spaces’ distance might be dominant. To weight each
label space properly, we can normalize each label space’s distance. Second, we may wish to consider the
abstention symbol as an additional element in each space. This permits LFs to output one or another type of
label without necessarily emitting a full vector. This can also be handled; the underlying algorithm is simply
modified to permit abstains as in Fu et al. (2020).
D	Universal Label Model For Isotropic Gaus sian Embedding
We illustrate the isotropic Gaussian version of the label model. While simple, it captures all of the challenges
involved in label model learning, and it performs well in practice. The steps are shown in Algorithm 3.
Why does Algorithm 3 obtain the estimates of θ without observing the true label y? To see this, first, note
that post-embedding, the model we are working with is given by
m
p(λ1,...,λm∣y) = -exp(X -θakg(λa) - g(y)k2 + X -θa,b kg(λa) - g(λb)∣∣2).
a=1	(a,b)∈E
If the embedding is a bijection, the resulting model is indeed is a multivariate Gaussian. Note that the term
“isotropic” here refers to the fact that for d > 1, the covariance term for the random vector λa is a multiple
of I.
Now, observe that if (a, b) 6∈ E, we have that E kλa - λbk2	= E k(λa - y) - (λb - y)k2	=
E kλa - yk2 +E kλb - yk2. Note that we can estimate the left-hand side E kλa - λbk2 from samples,
15
Published as a conference paper at ICLR 2022
Algorithm 3: Isotropic Gaussian Label Model Learning
1:	Input: Output of labeling functions λa(i), correlation set E
2:	for a ∈ {1, 2, . . . , m} do
3:	for b ∈ {1, 2, . . . , m} \ a do
4:	EstimateCorrelationsNij, E [d(λa, λb)] - 1 Pn=I d(λa(i), λb(i))
5:	end for
6:	Estimate Accuracy: Pick b, c : (a, b) 6∈ E, (a, c) 6∈ E, (b, c) 6∈ E
E[d(λa,y)] - 1∕2(E [d(λa,λb)] + E [d(λa,λc)] - E jd(λb,λc)])
7:	Form estimated covariance matrix Σ from accuracies and correlations; Compute θ J Σ 1
8:	end for
9:	return θa, θa,b
while the right-hand side contains two of our accuracy terms. We can then form two more equations of this
type (involving the pairs a, c and b, c) and solve the resulting linear system. Concretely, we have that
E [kg(λa) - g(λb)k2] = E [kg(λa) - yk2] + E [kg(λb) - yk2]
E [kg(λa) - g(λc)k2] = E [kg(λa) - yk2] + E [kg(λc) - yk2]
E [kg(λb) - g(λc)k2] =E[kg(λb)-yk2]+E[kg(λc)-yk2].
To obtain the estimate of E kg(λa) - yk2 , we add the first two equations, subtract the third, and divide
by two.This produces the accuracy estimation step in Algorithm 3. To obtain the estimate of the canonical
parameters θ, it is sufficient to invert our estimate of the covariance matrix. In practice, note that we need not
actually compute an embedding; we can directly work with the original metric space distances by implicitly
assuming that there exists an isometric embedding.
E	Additional Algorithmic Details
Quadratic Triplets We give the full statement of Algorithm 4, which is the accuracy estimation algorithms
for the case Im(g) = {±1}d.
Inference simplification in R For the simplification of inference in the isotropic Gaussian embedding, we
do not even need to recover the canonical parameters; it is enough to use the mean parameters estimated
in the label model learning step. For example, if d = 1, we can write these accuracy mean parameters as
E [g(λi)g(y)] and PUt them into a vector E [∑]Λy. The correlations can be placed into the sample covariance
matrix Σg(λ1),...,g((λm) . Then,
y(i) ：= E [y∣λ1(i),...λm(i)] =ΣTyΣ-1[λ1(i),...,λm(i)].	⑸
This is simply the mean of a conditional (scalar) Gaussian.
F	Additional Theory Details
We discuss further details for several theoretical notions. The first provides more details on how to obtain the
canonical accuracy parameters θa from the mean parameters E [dτ (λa, y)] in the rankings case. The second
involves a discussion of learning to rank problems, with additional details on the weighted estimator (3).
16
Published as a conference paper at ICLR 2022
Algorithm 4: QUADRATICTRIPLETS
Input： Estimates Oa,b, Oa,c, Ob,c,'a,'b, 'c, prior p, index i
for y in Y do
Obtain probability p0 = P (Y = y) from prior p
Set β J (Oa,b(1 - P0) + ('a - pz)'b)/(p0Z - P0'α)
Set Y J (Oa,c(1 - P) + ('a - pz)'c)∕(p0Z 一 P0'α)
Solve quadratic (pβγ + 'b'c — pβ'c — pγ'b — Oa,b(1 — P))(POa — p0'a)2 =0 in Z
ʌ , , . .
P,(g(λa^i∖Y = y) J Z
Py(g(λb)i[Y = y) J (Oa,b(1 — P0) + ('a — pPy(g(λa)i∖Y = y))'b)∕(p0P)(g(λa)i[Y = y) — p0'a)
ʌ , , . . 一 ʌ , , . . . . , . ʌ , , . . .
Py(g(λc)i∖Y = y) J (Oa,c(1 — P0) + ('a — pP,(g(λa)i∖Y = y))'c)∕(p0Py(g(λa)i∖Y = y) — P0'a)
end for
return Accuracies P,(g(λa)i∖Y = y),P,(g(λb)i∣Y = y), P(g(λc)i∣Y = y)
More on the backward mapping for rankings We note, first, that the backward mapping is quite simple
for the Gaussian cases in Rd : it just involves inverting the mean parameters. As an intuitive example,
note that the canonical parameter in the interior of the multivariate Gaussian density is Σ-1, the inverse
covariance matrix. The more challenging aspect is to deal with the discrete setting, and, in particular, its
application to rankings. We do so below.
We describe how we can recover the canonical accuracy parameter θa for a label function λa given accuracy
estimates P(g(λa)i = 1|Y = y) for all y ∈ Y. By equation (1), the marginal distribution of λa is specified
by
P(λa ) = eχp(-g-λa>Tθτ θag(y)).	(6)
Z
Since this is an exponential model, it follows from Fligner & Verducci (1986) that
ED] = T
(7)
t=-θa
where D = Ei l{g(λa)i = g(y)i} and M(t) is the moment generating function of D under (5). E[D] can
be easily estimated from the accuracy parameters obtained from the triplet algorithm, and the inverse of (6)
can then be solved for. For instance, in the rankings case, it can be shown (Fligner & Verducci, 1986) that
M (-θ) = ɪ Z (θ).
Additionally, we have that the partition function satisfies (Chierichetti et al., 2014)
1 — eθj
z ⑻=Y τ-eτ.
j≤k
It follows that
M (t = 1Y Fj.
k!	1 — eθ
j≤k
(8)
Using this, we can then solve for (6) numerically.
Rank aggregation and the weighted Kemeny estimator Next, we provide some additional details on the
learning to rank problem. The model (1) without correlations can be written
P(λ* 1,...,λm∣y)
1
Z exp
(X -θadτ(λa,y)
(9)
17
Published as a conference paper at ICLR 2022
Thus, ifwe only have one labeling function, we obtain the Mallows model (Mallows, 1957) for permutations,
whose standard form is p(λ1 |y) = 1/Z exp(-θ1dτ(λ1, y)). Permutation learning problems often use the
Mallows model and its relatives. The permutation y (called the center) is fixed and n samples of λ1 are
drawn from the model. The goal is to (1) estimate the parameter θ1 and (2) estimate the center y. This neatly
mirrors our setting, where (1) is label model learning and (2) is the inference procedure.
However, our problem is harder in two ways. While we do have n samples from each marginal model with
parameter θi, these are for different centers y1, . . . , yn—so we cannot aggregate them or directly estimate
θi. That is, for LF a, we get to observe λa(1), λa(2), . . . , λa(n). However, these all come from different
conditional models P(∙∣yi), with a different yi for each draw. In the uninteresting case where the yi,s are
identical, we obtain the standard setting. On the other hand, we do get to observe m views (from the LFs)
of the same permutation yi . But, unlike in standard rank aggregation, these do not come from the same
model—the θa accuracy parameters differ in (9). However, if θa is identical (same accuracy) for all LFs, we
get back to the standard case. Thus we can recover the standard permutation learning problem in the special
cases of identical labels or identical accuracies—but such assumptions are unlikely to hold in practice.
Note that our inference procedure (3) is the weighted version of the standard Kemeny procedure used for
rank aggregation. Observe that this is the maximum likelihood estimator on the model (1), specialized
to permutations. This is nearly immediate: maximizing the linear combination (where the parameters are
weights) produces the smallest negative term on the inside of the exponent above.
Next, we note that it is possible to show that the sample complexity of learning the permutation y using this
estimator is still Θ(log(m∕ε)) by using the pairwise majority result in Caragiannis et al. (2016).
Finally, a brief comment on computational complexity: itis known that finding the minimizer of the Kemeny
rule (or our weighted variant) is NP-hard (Davenport & Kalagnanam, 2004). However, there are PTAS
available for it (Kenyon-Mathieu & Schudy, 2007). In practice, our permutations are likely to be reasonably
short (for example, in our experiments, we typically use length 5) so that we can directly perform the
minimization. In cases with longer permutations, we can rely on the PTAS.
G	Proof of Theorems
Next we give the proofs of the proposition and our two theorems, restated for convenience.
G.1 Distortion Bound
Our result that captures the impact of distortion on parameter error is
Theorem 4.3. The inconsistency in estimating θ is boundedas llθ -θ0k ≤ εkμVemin.
Before We begin the proof, We restate, in greater detail, some of our notation. We write Pθ^γ for the true
model
m
pθ[dγ d∙.,λmIy) = Z exp ( X -θadγ (λa, y) + X -θa,bdγ (λa, λb))
a=1	(a,b)∈E
and PS，( for models that rely on distances in the embedding space
m
P9;dg (λ1,…，λm∣y) = -exp (X-£ dg (g(λa), g(y)) + X Y 必(g(λa), g(λb))).
a=1	(a,b)∈E
We also write θ to be the vector of the canonical parameters θa and θa,b in ps;;dy and θ0 to be its counter-
part for pe，；dg. Let μ be the vector of mean parameters whose terms are E[dγ(λa, y)] and E[dγ(λa, λb)].
18
Published as a conference paper at ICLR 2022
Similarly, We write μ0 for the version given by 夕6，应.Let Θ be a subspace so that θ, θ0 ∈ Θ. Since our
exponential family is minimal, we have that the log partition function A(θ) has the property that V2A(θ) is
.1	♦	. ∙	1 ∙	∙ . ∙	ICJC	,1,,1	11, ♦	1	Γ∙ X—z9 Λ / X∖ Γ∙ ∕⅛ _ 厂、 ∙
the covariance matrix and is positive definite. Suppose that the smallest eigenvalue of V2A(θ) for θ ∈ Θ is
emin.
For our embedding, suppose we first normalize (ie, divide by a constant) our embedding function so that
dg Igly),glyY) ≤ 1
dγ (y,y0)	一
for all y, y0 ∈ Y × Y . In other words, our embedding function g never expands the distances in the original
metric space, but, of course, it may compress them. The distortion measures how bad the compression can
be, Let ε be the smallest value so that
1 - ε ≤ dg(g(y),g(y0)) ≤ 1
一dγ (y,y0)一
for all y, y0 ∈ Y × Y . If g is isometric, then we obtain ε = 0. On the other hand, as ε approaches 1, the
amount of compression can be arbitrarily bad.
Proof. We use Lemma 8 from Fu et al. (2020). It states that
kθ 一 θ0k ≤ ——kμ 一 μ0k
emin
≤ ^—kμk.
emin
To see the latter, we note that kμ 一 μ0k ≤ ∣∣μ(1 一 μμ)k ≤ ∣∣μ∣∣(1 — (1 一 ε)) = ∣∣μ∣∣ε, where μ is the
element-wise ratios, and where we applied our distortion bound.	□
G.2 B inary Hypercube Case
Theorem 4.1. For any δ > 0, for some y1 and y2 with known class probabilities p = P (Y = y1), the
quadratic triplet method recovers αi = P (g(λa)i = 1|Y = y), βi = P (g(λb)i = 1|Y = y), γi =
P(g(λc)i = 1|Y = y) UP to error O((皿2//6 )1/4) With probability at least 1 一 δ for any conditionally
independent label functions λa, λb, λc and for all i ∈ [d].
Proof. Define
P (g(λa)i = 1|Y=y)	P(g(λa)i = 1|Y 6=y)
P(g(λa)i = -1|Y = y) P(g(λa)i = -1|Y = y)
and
Oab	P(g(λa)i = 1, g(λb)i = 1)
Oi = P(g(λa)i = 一1, g(λb)i = 1)
P(g(λa)i = 1, g(λb)i = 一1)
P(g(λa)i = 一1, g(λb)i = 一1)
By conditional independence, we have that
μap (μb)τ = oab.
P= P(Y0=y)
0
P(Y 6=y)
(10)
Note that we can express
P(g(λa)i = 1|Y 6=y)
P(g(λa)i = i)
p (y = y
P(g(λa)i = 1∣Y = y)P(Y = y)
P(Y = y)
19
Published as a conference paper at ICLR 2022
We can therefore rewrite the top row of μ? as [ai, qi — ra∕ where qi = P(Pdi=I) and r = p(Y=y). After
=y	=y
estimating the entries of Oi’s and qi’s, we consider the top-left entry of (10) for every pair of a, b and c, to
get the following system of equations
taiβi + qaqb — qarβi — qbrai
taiYi + qaqc - qarγi - GCrai
tβiYi + qbqc — qim — ^crβi
Oab
1 — p,
OaC
1 — p,
ObC
1 — p，
where βi and Yi are the top-left entries of μ9 and μC, respectively, P = P(Y = y), and t = (Idppp.
For ease of notation, we write qi as qi and so on. Rearranging the first and third equations gives Us expres-
sions for α and γ in terms of β .
α
γ
Oab + qarβ — ^a ^b
tβ — qbr
Obcp + qcrβ — qbqc
tβ — ^br
Substituting these expressions into the second equation of the system gives
qirβ +	Oab	—	^a^b	qcrβ +	Obc	— qb^c	ʌ ʌ ʌ	q"β	+	Obc	—	^b^c	q	qirβ + Oab	—	^i^b	OaC
-p	-p	-p	-p	iC
t-----T7,~3------------------Γ7,~+--------+ ^a^c	—	^ar----Γ7,~-----------ficr------T7>~=--------- = ；-----.
tβ — qbr	tβ — qbr	tβ — qbr	tβ — qbr	1 — p
We can multiply the equation by (tβ — ^br)2 to get
^
t(^arβ + 丁'--qaqb) ∙ (^crβ + -—— — ^b^c) + ^a^c((tβ — ^br)2)
1—p	1—p
A	^ .
—qar(qcre + τ一C---^b^c) ∙ (tβ 一 ^br) 一 ^cr(^arβ + 丁上-^a^b) ∙ (tβ 一 ^br)
1—p	1—p
=& ∙ (tβ - qbr)2
1—p
^	^
2 2	Oab	Obc	Oab	Obc
⇒ t ^a^cr β + (；------^a^b)^crβ + (；----qb^c)qarβ + (；-----^a^b)(z-------^b^c)
1—p	1—p	1—p	1—p
+qa^c 卜β2 — 2^brtβ + ^2r2)
/	A	A	\
一^ar(^crtβ2 + (j—cp — ^b^c)tβ — qb^cr2β - Gbr(j-j - GbGc))
^	^
—qcr(q0rtβ2 + (j-p — (^aqb)-tβ — ^a^br2β —血r(1—p — ^a^b)∖
=OP •仆2 — 2^brtβ + q2r2)
20
Published as a conference paper at ICLR 2022
⇒ β2[qaqct2 — ^aqcr2e — 4•产
1-p
r / A .	A	∖
Oab	Obc
+β t (τ------^a^b)^cr + (τ.----^b^c)^ar 一 2^a^c^brt
1—p	1—p
Obc	2	Oab	2
一q°r I(1--p — ^b^c)t — ^b^cr ) - qcr ((1一不 一 ^a^b)t 一 ^a^br ) +
△
Oac
ac
1—p
^
Oab	Obc	2 2
+ t(；------qaqb)(;---------(ib(ic) + qa qcqb r
1一 p	1一 p
A	^ ,	^	1
2 Obc	2 Oab	Oac	2 2
+qa^br (；-------^b^c) + ^b^cr (；------(ia(ib) 一 ；- ∙ f^bt	= 0.
1一 p	1一 p	1一 p
The only sources of error are from estimating c's and O's. Let εco denote the error for the c's. Let εo denote
the error from O's. Applying the quadratic formula We obtain β =(一卜 土，(b0)2 - 4a0c0)∕(2a0) where the
coefficients are
0	2	2	ac 2
a = qaqct 一 qaqCr t 一 τ------ ∙ t
1一 p
^ ^ .	A	∖
0	Oab	Obc
b = t (3------^a^b)^cr +(3------^b^c)^ar 一 2^a^c^brt
1一 p	1一 p
/O,	一、 / O ,
Obc	2	Oab
一^ar I (ɪɪp — ^b^c)t — qb^cr J- qcr I (ɪɪp
一 qaGb)t 一
c0
t(修
1一 p
Obc	2 2	2	Obc
一 ^a^b)(τ.------qbqc) + qaqc%r + qaqb r ( ：；-----qbqc)
1一 p	1一 p
2 Oab
+qbqcr (；-----qaqb) 一
1一 p
ac
1一 p
Let εa0 ,εbo ,εco denote the error for each coefficient. Thatis, e。，= |(a0)* — a0∣, where (a0)* is the population-
level coefficient, and similarly for b0 , c0 . Let εc and εO indicate the estimation error for the c and O terms.
Then
ε°o = O (t2pεc + 1-^εo),
εb0 = O (I-∙p (εc + εo)),
εc0 = o ((UF + 1⅛ + r2) εc+ ((⅛ + 1⅛) εO).
Because d and e are functions of p, if we ignore the dependence on p, we get that
εa0 = εb0 = εc0 = O(εc + εO ).
Furthermore,
ε(b0)2 = O(εb), εa0c0 = O(εa + εc).
O
21
Published as a conference paper at ICLR 2022
It follows that	_________
εβ = O(√ εc0 + εo).
Next, note that O and c are both the averages of indicator variables, where the ci’s involve P (g(λa)i = 1 and
the Oiab’s upper-left corners compute P (g(λa)i = 1, g(λb)i = 1). Thu we can apply Hoeffding’s inequality
and combine this with the union bound to bound the above terms. We have with probability at least 1 一 δ,
εci ≤ Jlog(2d∕δ) and similarly with probability at least 1 一 2, WOi ≤ Jlog(2d∕δ) for all i ∈ [d]. It follows
that with probability at least 1 一 δ, εɑ = εβ = εγ = O(( log2d∕δ) )1/4).	□
We can now prove the main theorem in the rankings case.
Corollary 4.1.1. For any δ > 0, U > 0, a prior over y1 and y2 with known class probability p, and
using Algorithm 1 and Algorithm 4, for any conditionally independent triplet λa , λb, λc, with parameters
U > θa, θb, θc > 4ln(2), we can recover θ0,θb, θc up to error O(g-1(θ + (log(2ρ2)∕(2δn))1/4) 一 θ) with
probability at least 1 — δ, where g2(U) = (—ρe-U)/((1 — e-U)2) + Pj=、(j2e-Uj)/((1 - e-Uj)2).
Proof. Consider a pair of items (a, b). Without loss of generality, suppose a Yyi b and a >y? b. Define
αa,b = P (a Yλ b|yi), &&,卜=P (a f b∣y2) then our estimate for P (λa,b)〜Y(a,b)) where λaɑ^)〜
Y(a,b) denotes the event that label function i ranks (a, b) correctly would be
ʌ ,. _ ______ 、 , , , ʌ
p(λ(a,b)〜γ(a,b)) = Pαa,b + (I- P)(I- α'a,b)
which has error O(∈α). Then, note that E[d(λa, Y)] = Pa b P(λaa	〜 Y(a,b)). Therefore, we can compute
the estimate E[d(λa, Y)] = Pa b E[λi 〜Y]αb which has error O((j)( ln2)/6 )1/4).
Recall from (7) we have that
W mi d[log(M⑴)]
Eθ [D] = d—
t=-θ
where M(t) is the moment generating function, and recall from (8) that
It follows that
Eθ[D]
M ⑴=k! Y
ke-θ	je-θj
1 - e-θ 一 工 1 - e-θj
j≤k
(11)
⇒ ddθ Eθ[D]
dθ
-ke-θ	j 2e-θj
(1 - e-θ)2 +幺(1 - e-θj)2 .
-θ	j 2e-θj
Let gk(θ) = (1-e-θ)2 + Ej≤k (1-e-θj)2 . By the lemma below, gk is non positive and increasing in θ for
θ > 0. This means we can numerically compute the inverse function of (7), with the stated error. □
Lemma G.1. gk is non-positive and increasing in θfor θ > 4 ln(2). Additionally, gk is decreasing in k.
Proof. We first show non-positivity. For this, it is sufficient to show
2θ	≤ e-θ
(1 - e-θj)2 ≤ (1 - e-θ)2 .
(12)
22
Published as a conference paper at ICLR 2022
holds for all 1 ≤ j and θ > 0. This clearly holds for j = 1. Rearranging gives
j…)≤ (T )2.
The right-hand term is greater than or equal to 1, so it suffices to choose θ such that
j2e-θ(j-1) ≤ 1
⇒ θ ≥ 叫.
j -1
It can be shown that the right-hand term decreases with j for j > 1, so it suffices to take j = 2, which
implies θ ≥ 2 ln(2). By choice of θ this is clearly true.
To show that gk is increasing in θ, we consider
d ⑷ ke-θ (1-e-2θ),
dθgk ⑻=(1-e-)4 +
Similarly, it suffices to show
j3e-θj(1 - e-2θj)
-(1 - e-θj )4- ≤
Rearranging, we have
j3e-θj-i) ≤ 1 - e-2
j e	≤ 1 - e-2
Note that
1 - e-2θ (1 - e-θj'
1 - e-2θj 1 1 - e-θ ,
The term on the right is greater than 2 for θ > 0 and j ≥
j3e-θ(j-1)
X j3e-θj (e-2θj - 1)
⅛	(1-e-θj )4
j ≤k
e-θ(1 - e-2θ)
(1 - e-θ)4 .
? ( 1-e-θj∖4
jV 1 - e-θ ).
4	1 - e-θj
≥ 1 - e-2θj .
1.	Therefore, it suffices to choose θ such that
1
≤ —
一2
⇒θ≥
ln(2j3)
j -1
Once again, the term on the right is decreasing in j, so we can take j = 2, giving θ ≥ 4 ln(2), which is
satisfied by our choice of θ.
Finally, the fact that gk is decreasing in k follows from (12).
□
G.3 Euclidean Embedding Case
Theorem 4.2. Let E [g(λa)g(y)] be an estimate of the accuracies E [g(λa)g(y)] using n samples, where all
LFs are conditionally independent given Y. If the signs of a are recoverable, then with high probability
E[||E[g(Y)g⑻] - E[g(Xa)g⑻] ||2] = O ((a-1in∣ + a-6in∣) PmaX(emax,emax)/n).
Here, a|min| = mini |E g(λi)g(y) | and emax = maxj,k ej,k.
23
Published as a conference paper at ICLR 2022
Proof. For three conditionally independent label functions λα, λb, λc, our estimate of Eg(λα)g(y)] is
ιE")g(y)h( i^a⅛cF1 )2∙
Furthermore, if We define Xab = ⅛a4, We can write the ratio of elements of a to a as
,	leα,b∣
lE[g(λα)g(y)U	= (l¾l	,怕a,d	. I eb,/A5 = (xa,bxa,cA5
I E[g(λa)g(y)] I IM | | ea,c| MCU 【xb,c	)
(and the other definitions are symmetric for kb and kb). Now note that because we assume that signs are
一	一	一一	.ʌ . ，一 一、	, 、r	_ 一 ，一 一、	,	、r，	,ʌ . ，一 一、	,	、r，	_-- ，一-、	,、、，
completely recoverable, ∣E[g(λa)g(y)] - E[g(λa)g(y)]∣ = ∣E[g(λa)g(y)]∣ - ∣E[g(λa)g(y)]∣ .
Next note that
∣E[g(λa)g(y)]2 - E[g(λa)g(y)]2∣ = ∣E[g(λa)g(y)] - E[g(λa)g(y)]∣∣E[g(λa)g(y)]+ E[g(λa)g(y)]∣.
By the reverse triangle inequality,
(∣E[g(λa)g(y)]∣ - ∣E[g(λa)g(y)]∣)2 =∣∣E[g(λa)g(y)]∣ - ∣E[g(λa)g(y)]∣∣2
≤ ∣E[g(λa)g(y)] - E[g(λa)g(y)]∣2
=(∣E[g(λa)g(y)]2 - E[g(λa)g(y)]2∣!2
[∣E[g(λa)g(y)] + E[g(λa)g(y)]∣ J
≤ c12∣E[g(λa)g(y)]2 - E[g(λa)g(y)]2∣2,
where we define C as mm0 ∣E[g(λa)g(y)]+ E[g(λa)g(y)] ∣. (With high probability C = a∣min∣∣ + ∣a∣min∣,but
there is a chance that E[g(λa)g(y)] and E[g(λa)g(y)] have opposite signs.) For ease of notation, suppose
we examine a particular (a, b,c) = (1, 2, 3). Then,
(∣E[g(λ1)g(y)]∣-∣E[g(λ1)g(y)]∣)2
≤ C⅛(λ1)g(y)i2 -耽gg^2 = C2
2
∣ e121 ∣ ei31	∣e12 ∣∣ei3 ∣
—：---：———：-------：—
∣^23∣	归23∣
2
1 ∣e12 ∣∣ei3 ∣ - ∣^12 ∣∣ei3 ∣ + ∣e12 ∣∣e13 ∣ - ∣e12 ∣∣e13 ∣ + ∣e12 ∣∣e13 ∣ - ∣e12 ∣∣e13 ∣
C	怕23〔	归 23〔	归 23〔	归23〔	归23〔	归231
≤ C2 ( 1 ≡≡ 卜怕23 J e231 ∣+1 ≡ 卜同3 J e13 "+1 s 卜同21 Tei2 ∣ ∣ A
3 I ∣ e23 - e231 + 1 辿 I ∣ e13 - e131 +
e23e23 I	Ie23I
With high probability, all elements of e and e must be less than emax = maxj,k ej,k. We further know that
elements of ∣ e∣ are at least a2min∣∕E[Y2]. Now suppose (with high probability) that elements of ∣ e∣ are at
24
Published as a conference paper at ICLR 2022
least ^∣min∣ > 0, and define 4a,b = ^a,b - e0,b. Then,
(∣E[g(λ1 )g(y)]2∣-∣E[g(λ1 )g(y)]∣)2
maXemax,emaX) (___________∖________∣	∣
≤	c2	[a2	.	l^2	.	lE[Y2]2	|423| +	a；
∣min∣ ∣min∣	∣
≤ max(emax,emaχ)(423 + 423 + 422)
c2	23	13	12
2
厂画Y2]|413| + a2 . lE[Y2] |412| )
|min|	|min|
12
O⅛^hY平 + a4min∣E[Y2]2 J
The original expression is now
.ʌ _ . _ _ , _ .
∣E[g(λa)g(y)] - E[g(λa)g(y)]∣
/ maχ(emax, emaχ)
≤	C2
asn⅛nEYψ+a4min∣E[Y 2]2
(42a,b + 42a,c + 4b2,c)
1
2
1
2
To bound the maximum absolute value between elements of e and e, note that
(2 (4j + 4a,c + 42,c)) 2 ≤ llea,b,c - ea,b,c∖∖F,
where ea,b,c isa3 X 3 matrix with ei,j inthe (i,j)-th position. Moreover, it is a fact that ∖∖ea,b,c 一 ea,b,c∖∣F ≤
√r∖∖ea,b,c - ea,b,c∖∖2 ≤ √3∖∖^a,b,c-ea,b,c∖∖2 where r is the rank of e°,b,c - ©。也「Putting everything together,
.ʌ _ . _ _ , _ .
∖E[g(λa)g(y)] - E[g(λa)g(y)]∖
max(emax, em0χ) (__1_______,____2____
c2	"4m,nMmin∣E[Y+ afminlEY2]2
maχ(emαx, em0χ) (________1_________,______2_____
c2	IamKnEW + a4min∣E[Y 2 ]2
1	Y
• 2∖∖"-ea,b,c 叼
1
3	∖ 2
• £ ∖∖ea,b,c - ea,b,c∖∖2 )
Lastly, to compute E[∖∖E[g(λa)g(y)] - E[g(λa)g(y)]∖∖2], we use Jensen’s inequality (concave version, due
to the square root) and linearity of expectation:
E[∖E[g(λa)g(y)] - E[g(λa)g(y)]∖]
1	2 ʌ 3	2Λ	2
a4	. l^4 .	lE[Y2]4	+ a4 . lE[Y2]2 ) . 2E[∖∖ea,b,c -	ea,b,c^2] ) .
∣min∣ ∣min∣	∣min∣
We use the covariance matrix inequality as described in Tropp (2014), which states that
P(∖∖e - e∖∖2 ≥ Y) ≤ max
where σ = maxa eaa and C > 0 is a universal constant.
To get the probability distribution over1岛屁-ea,b,c∖∖2, wejust note that P(∖∖ea,b,c - ea,b,c∖∖2 ≥ Y)
P(∖∖e - e∖∖2 ≥ Y2)to get
P(∖∖ea,b,c - ea,b,c∖∖2 ≥ Y) ≤ 2e3 max (e-σ√CY,e-σnC2).
max(emax, emax)

25
Published as a conference paper at ICLR 2022
From this we can integrate to get
∞	σ4C2	σ4C2	σ4
E[∣l^a,b,c - ea,b,c||2] =	P(l∣eα,b,c - ea,b,c∣l2 ≥ γ)dγ ≤ 2e3max -, 2 —— =。(一).
0	nn	n
Substituting this back in, we get
E[∣E[g(λa)g(y)] - E[g(λa)g(y)]∣]
1
≤ ( max(emax,emaX) (__________1_______ +	2	∖ . o (、) 2
vɑlminl + a∣minI)2 ( ajmin∣ a4min∣ E[Y 2]4	a4min|E[Y 平 J	In"
1
≤ ( max(emax,emaX) . (________1_______ +	2	∖ . o (V∖ 2
l(6|min| + a|minI)2 "jminlajminlEY 2]4	a4min|E[Y 2]2 J	In”
_________max(emax,emaX)	(	1	+	2	) .。(σ4)) 2
(6∣min∣ + a∣minI)2 min(E[Y 2]4, E[Y 2]2)______________ajmin∣ 6jmin∣	a4min∣ )	∖n J )
with high probability. Finally, We clean UP the a∣min∣ and a∣min∣ terms. The terms involving a and a can be
rewritten as
1 + 2a4 .,
IminI
a6 -	∣^4	- I +	2a5	.	∣a5	. I + a4	.	∣^6	. ∣
IminI IminI	IminI IminI	IminI IminI
We suppose that 叫min| ∈ [1 - 3 1 + e] for some e > 0 with high probability. Then, this becomes less than
a|min|
1 + 2^4 .
IminI
(1-e)4aimin∣+2(1-e)5aimin∣ +(1 - e)6aim
/	1+2a4min∣
≤ 4(1-e)6aimin|
=O (θi0^ + 广
aIminI	aImin
Therefore, with high probability, the sampling error for the accuracy is bounded by
-..人- , _ . ,.,
E[∣∣E[g(λa)g(y)]-
□
Note that if all label functions are conditionally independent, we only need to know the sign
of one accuracy to recover the rest. For example, if we know if E[g(λ1)g(y)] is positive
or negative, we can use E[g(λ1)g(y)]E[g(λ2)g(y)] = e1,2E[Y2]2, E[g(λ1)g(y)]E[g(λ3)g(y)] =
e1,3E[Y2]2, . . . , E[g(λ1)g(y)]E[g(λm)g(y)] = e1,mE[Y2]2 to recover all other signs.
26
Published as a conference paper at ICLR 2022
H	Extended Experimental Details
In this section, we provide some additional experimental results and details. All experiments were conducted
on a machine with Intel Broadwell 2.7GHz CPU and NVIDIA GK210 GPU. Each experiment takes from 30
minutes up to a few hours depending on the experiment conditions.
Hyperbolic Space and Geodesic Regression The following is basic background useful for understanding
our hyperbolic space models. Hyperbolic space is a Riemannian manifold. Unlike Euclidean space, it does
not have a vector space structure. Therefore it is not possible to directly add points. However, geometric
notions like length, distance, and angles do exist; these are obtained through the Riemannian metric for the
space. Points p in Smooth manifolds M are equipped with a tangent space denoted TpM . The elements
(tangent vectors v ∈ TpM) in this space are used for linear approximations of a function at a point.
The shortest paths in a Riemannian manifold are called geodesics. Each tangent vector x ∈ Tp M is equiv-
alent to a particular geodesic that takes p to a point q. Specifically, the exponential map is the operation
exp : TpM → M that takes p to the point q = expp (v) that the tangent vector v points to. In addition, the
length kvk of the tangent vector is also the distance d(p, q) on the manifold. This operation can be reversed
with the log map, which, given q ∈ M, provides the tangent vector v = logp(q).
The geodesic regression model is the following. Set some intercept p ∈ M . Scalars xi ∈ R are selected
according to some distribution. Without noise, the output points are selected along a geodesic through p
parametrized by β: yi = expp(xi × β), where β is the weight vector, a tangent vector in Tp(M) that is not
known. This is a noiseless model; the more interesting problem allows for noise, generalizing the typical
situation in linear regression. This noise is added using the following approach. For notational convenience,
we write expp(v) as exp(p, v). Then, the noisy y are yi = exp(exp(p, xi × β), εi), where εi ∈ Texp(p,xi) M.
This notion generalizes adding zero-mean Gaussian noise to the y’s in conventional linear regression.
The equivalent of least squares estimation is then given by p,β = arg minq,α Pn=ι d(exp(q, αxi), yi)2.
Semantic Dependency Parsing We used datasets on Czech and English taken from the Universal De-
pendencies Nivre et al. (2020) repository. The labeling functions were Stanza Qi et al. (2020) pre-trained
semantic dependency parsing models. These pre-trained models were trained over other datasets from the
same language. For the Czech experiments, these were the models cs.cltt, cs.fictree, cs.pdt,
while for English, they were taken from en.gum, en.lines, en.partut, en.ewt. The metric is
the standard unlabeled attachment score (UAS) metric used for unlabeled dependency parsing. Finally, we
used access to a subset of labels to compute expected distances, as in the label model variant in Chen et al.
(2021).
Movies dataset processing In order to have access to ample features for learning to rank and regression
tasks, we combined IMDb (imd) and TMDb(tmd) datasets based on movie id. In the IMDb dataset, we
mainly used movie metadata, which has information chiefly about the indirect index of the popularity (e.g.
the number of director facebook likes) and movie (e.g. genres). The TMDb dataset gives information
mainly about the movie, such as runtime and production country. For ranking and regression, we chose
voteaverage column from TMDb dataset as a target feature. In rankings, We converted the voteaverage
column into rankings so the movie with the highest review has a top ranking. We excluded the movie scores
in IMDb dataset from input features since it is directly related to TMDb vote average. But we later included
IMDb, Rotten Tomatoes, MovieLens ratings as Weak labels in Table 2.
After merging the two datasets, we performed feature engineering as follows.
1.	One-hot encoding: color, content .rating, original-language, genres, status
27
Published as a conference paper at ICLR 2022
2.	ToP (frequency) 0.5% one-hot encoding: plot-keywords, keywords, production-companies, actors
3.	Count: production-countries, SPoken-languages
4.	Merge (add) actor」RCebOOk-Ukes, actor^2facebookJikes, actor二负cebookJikeS into ac-
tor facebook-likes
5.	Make a binary feature regarding whether the movie’s homePage is missing or not.
6.	Transform date into one-hot features such as month, the day of week.
By adding the features above, we were able to enhance Performance significantly in the fully-suPervised
regression task used as a baseline.
In the ranking exPeriments, we randomly samPled 5000 sets of movies as the training set, and 1000 sets of
movies as the test set. The number of items in an item set (ranking set) was 5. Note that the movies in the
training set and test set are strictly seParated, i.e. if a movie aPPears in the training set, it is not included in
the test set.
Model and hyperparameters In the ranking setuP, we used 4-layer MLP with ReLU activations. Each
hidden layer had 30 units and batch normalization(Ioffe & Szegedy, 2015) was aPPlied for all hidden layers.
We used the SGD oPtimizer with ListMLE loss (Xia et al., 2008); the learning rate was 0.01.
In the regression exPeriments, we used gradient boosting regression imPlemented in sklearn with
^estimators=250. Other than ^estimators, we used the default hyperparameters in sklearn,s implemen-
tation.
BoardGameGeek data processing In the BoardGameGeek dataset (boa, 2017), we used metadata of
board games only. Since this dataset showed enough model performance without additional feature engi-
neering, we used the existing features: yearpublished, minplayers, maxplayers, playingtime, minplaytime,
maxplaytime, minage, owned, trading, wanting, wishing, numcomments, numweights, averageweight. The
target variable was average ratings (average) for regression, and board game ranking (Board Game Rank).
Note that ‘Board Game Rank‘ cannot be directly calculated from average ratings. BoardGameGeek has its
own internal formula to determine the ranking of board games. In the ranking setup, we randomly sampled
5000 board game sets as the training set, and 1000 board game sets as the test set.
Simulated LFs generation in ranking and regression In rankings, We sampled from λa(i) ~
Zre-βadτ(π,Yi) with βa. For more realistic heterogeneous LFs, 1/3 (good) LFS βa are sampled from
U nif orm(0.2, 1), and 2/3 (bad) LFs’ βa are sampled from U nif orm(0.001, 0.01). Note that higher value
of βa yields less noisy weak labels. In regression, we used the conditional distribution Λ∣Y to generate
samples of Λ. Specifically, where Λ∣Y 〜 N(μ, Σ), where μ = ∑λy∑-1y and Σ = ∑λ - ∑λy∑-1∑yλ,
from the assumption (Λ, Y)〜N(0, Σ).
I Additional S ynthetic Experiments
We present several additional synthetic experiments, including results for partial ranking labeling functions
and for parameter recovery in regression settings.
I.1 Ranking
To check whether our algorithm works well under different conditions, we performed additional experiments
with varied parameters. In addition, we performed a partial ranking experiment.
28
Published as a conference paper at ICLR 2022
① uu--0 t un3①z
Num LFs (m)
二
C
(ŋ
①
(a) d = 10, n = 250
0.6
0.4
0.2
0.0
1.2
(L)
1 1.0
(ŋ
益0.8
Q
(b) d = 20, n = 250
Figure 6: Inference via weighted and standard Kemeny rule over full rankings (top) with permutations of
size d = 10, 20. Error metric is Kendall tau distance (lower is better). Proposed weighted Kemeny rule is
nearly optimal on full rankings.
Ranking synthetic data generation First, n true rankings Y are sampled uniformly at random. In the full
ranking setup, each LF ∏ is sampled from the Mallows λa(i)〜 1 e-βadτ(π,Yi) with parameters βa,匕 and
in the partial ranking setup it is sampled from a selective Mallows with parameters βa , Yi , Si where each
Si ⊆ [d] is chosen randomly while ensuring that each x ∈ [d] appears in atleastap fraction of these subsets.
Higher p corresponds to dense partial rankings and smaller p leads to sparse partial rankings. We generate
18 LFs with 10 of then having /。〜U niform(0.1,0.2) and rest have /。〜U niform(2, 5). This was done
in order to model LFs of different quality. These LFs are then randomly shuffled so that the order in which
they are added is not a factor. For the partial rankings setup we use the same process to get βa and randomly
generate Si according to the sparsity parameter p. For a set of LFs parameters we run the experiments for 5
random trials and record the mean and standard deviation.
Full Ranking Experiments Figure 6 shows synthetic data results without an end model, i.e., just using the
inference procedure as an estimate of the label. We report the ‘Unweighted’ Kemeny baseline that ignores
differing accuracies. ‘Estimated Weights’ uses our approach, while ‘Optimal Weights’ is based on an oracle
with access to the true θa parameters. As expected, synthesis with estimated parameters improves on the
standard Kemeny baseline. The improvement for the full rankings case (top) is higher for fewer LFs; this is
intuitive, as adding more LFs globally improves estimation even when accuracies differ.
Partial Ranking Experiments In the synthetic data partial ranking setup, we vary the value of p (ob-
servation probability) from 0.9 (dense) to 0.2 (sparse) and apply our extension of the inference method to
partial rankings. Figure 7 shows the results obtained. Our observations in terms of unweighted vs weighted
aggregation remain consistent here with the full rankings setup. This suggests that the universal approach
can provide the same type of gains in the partial rankings.
I.2 Regression
Similarly, we performed a synthetic experiment to show how our algorithm performs in parameter recovery.
Regression synthetic data generation The data generation model is linear Y = βTX, where our data is
given by (X, Y ) with X ∈ Rq and Y ∈ R. We generate n such samples. Note that we did not add a noise
variable ε 〜 N(0, σ2) here since We do not directly interact with Y; the noise exists instead in the labeling
functions (i.e., the weak labels).
29
Published as a conference paper at ICLR 2022
0.05
U
C
ω
0.04
Q
0.03
c
0.02
W
0.01
—Unweighted
—Optimal Weights
Estimated Weights
Num LFs (m)
(b) d= 10,n = 250,p= 0.7
(a) d = 10,n = 250, p = 0.9
Num LFs (m)
(d) d= 10,n = 250,p= 0.2
Figure 7: Inference via weighted and standard majority vote over partial rankings with permutations of size
d = 10. Error metric is Kendall tau distance (lower is better). Proposed inference rule is nearly optimal on
full rankings.
Parameter estimation in synthetic regression experiments Figure 8 reports results on synthetic data
capturing label model estimation error for the accuracy and correlation parameters (μ, σ) and for directly
estimating the label Y . As expected, estimation improves as the number of samples increases. The top-right
plot is particularly intuitive: here, our improved inference procedure vastly improves over naive averaging
as it accesses sufficiently many samples to estimate the label model itself. On the bottom, we observe, as
expected, that label estimation significantly improves with access to more labels.
J	Additional Real LF Experiments and Results
We present a few more experiments with different types of labeling functions.
J.1 Board Game Geek dataset
In the board games dataset, we built labeling functions using simple programs expressing heuristics. For
regression, we picked several continuous features and scaled them to a range and removed outliers. Similarly,
for rankings, we picked what we expected to be meaningful features and produced rankings based on them.
The selected features were [’owned’, ’trading’, ’wanting’, ’wishing’, ’numcomments’].
30
Published as a conference paper at ICLR 2022
μ estimation error
2 3
- -
O O
1 1
3~∣ 二0」」E2πn~ 3Ξ
IO3 IO5
Samples
2 3
- -
O O
1 1
z,loE©」©d dot
Σ estimation error
IO3 IO5
Samples
(a) Number of samples
■-
-
W
山 s0-H2> a
IO0
Label estimation error
Baseline
OurApproach
IO3 IO5
Samples
Σ estimation error Label estimation error
10-3
10-3
10-3
,μ estimation errors
6x10-3
× × Y
4 3 7
Z~∣ 二0」」E©」©d 3Ξ
23	25
Num LFs (m)
W
Z~∣ -」0」」9 Ee-IedlQlL
23	25
Num LFs (m)
(b) Number of LFs
山 SWc0t°,°,-2> >
23	25
Num LFs (m)
Figure 8: Parameter and label estimation with varying the number of samples (top) and the number of
labeling functions (bottom)
	# of training examples	Kendall Tau distance (mean ± Std)
Fully supervised (5%)	250	0.1921 ± 0.0094
Fully supervised (10%)	500	0.1829 ± 0.0068
WS (HeUriSticS)	5000	0.1915 ± 0.0011
Table 5: End model performance with true ranking LFs in BGG dataset.
In this case, we observed that despite not having access to any labels, we can produce performance similar to
fully-supervised training on 5-10% of the true labels. We expect that further LF development will produce
even better performance.
J.2 MSLR-WEB 1 0K
To further illustrate the strength of our approach we ran an experiment using unsupervised learning methods
in information retrieval (such as BM25) as weak supervision sources. The task is information retrieval, the
dataset is MSLR-WEB10Kmsl, and the model and training details are identical to the other experiments.
We used several labeling functions including BM25 and relied on our framework to integrate these. The
labeling functions were written over BM25 and features such as covered query term number, covered query
31
Published as a conference paper at ICLR 2022
	# of training examples	MSE (mean ± std)
Fully supervised (1%)	144	0.4605 ± 0.0438
Fully supervised With “bad” subset (10%)	1442	0.8043 ± 0.0013
Fully supervised With “bad” subset (25%)	3605	0.4628 ± 0.0006
WS (Heuristics)	14422	0.8824 ± 0.0005
Table 6: End model performance with true regression LFs in BGG dataset. The training data was picked
based on the residuals in linear regression (resulting in a “bad” subset scenario for a challenging dataset).
We obtain comparable performance.
0.375
0.350
0.325
(a) Movies dataset
WS (Majority Vote)
WS (Ours)
Fully supervised (25%)
Fully supervised (50%)
(-----Fullysupervised (100%)
0.300
5	10	15
Num LFs
(b) BoardGameGeek dataset
Figure 9: End model performance with regression LFs (Left: Movies dataset, Right: BGG Dataset). Results
from training a model on pseudolabels are compared to fully-supervised baselines on varying proportions of
the dataset. Baseline is the averaging of weak labels. Metric is (MSE); lower is better.
term ratio, boolean model, vector space model, LMIR.ABS, LMIR.DIR, LMIR.JM, and query-url click
count.
As expected, the synthesis of multiple sources produced better performance than BM25 Dehghani et al.
(2017) alone. Despite not using any labels, we outperform training on 10% of the data with true labels. This
suggests that our framework for integrating multiple sources is a better choice than either hand-labeling or
using a single source of weak supervision to provide weak labels. Below, for Kendall tau, lower is better.
	Kendall tau distance	NDCG@1
Fully supervised (10%)	0.4003 ± 0.0151	0.7000 ± 0.0200-
Fully supervised (25%)	^^0.3736 ± 0.0090^^	0.7288 ± 0.0077
WS (Dehghani et al.(2017))	0.4001 ± 0.0063^^	0.7288 ± 0.0077
WS (OUrS)	一	0.3929 ± 0.0052	0.7402 ± 0.0∏9~
Table 7: End model performance with true ranking LFs in MSLR-WEB10K dataset. Since the dataset has a
lot of tie scores and the number of items is not uniform across examples, we sampled the examples with five
unique scores (0, 1, 2, 3, 4). Also, in each example, items are randomly chosen so that each score occurs
only once in each item set.
32