Published as a conference paper at ICLR 2022
How does unlabeled data improve generaliza-
tion in self-training? A one-hidden-layer the-
ORETICAL ANALYSIS
Shuai Zhang
Rensselaer Polytechnic Institute
Troy, NY, USA 12180
zhangs29@rpi.edu
Sijia Liu
Michigan State University
East Lansing, MI, USA 48824
MIT-IBM Watson AI Lab, IBM Research
liusiji5@msu.edu
Jinjun Xiong
University at Buffalo
Buffalo NY, USA 14260
jinjun@buffalo.edu
Meng Wang
Rensselaer Polytechnic Institute
Troy, NY, USA 12180
wangm7@rpi.edu
Pin-Yu Chen
IBM Research
Yorktown Heights, NY, USA 10562
Pin-Yu.Chen@ibm.com
Ab stract
Self-training, a semi-supervised learning algorithm, leverages a large amount of
unlabeled data to improve learning when the labeled data are limited. Despite em-
pirical successes, its theoretical characterization remains elusive. To the best of
our knowledge, this work establishes the first theoretical analysis for the known
iterative self-training paradigm and proves the benefits of unlabeled data in both
training convergence and generalization ability. To make our theoretical analysis
feasible, we focus on the case of one-hidden-layer neural networks. However,
theoretical understanding of iterative self-training is non-trivial even for a shal-
low neural network. One of the key challenges is that existing neural network
landscape analysis built upon supervised learning no longer holds in the (semi-
supervised) self-training paradigm. We address this challenge and prove that itera-
tive self-training converges linearly with both convergence rate and generalization
accuracy improved in the order of 1 / √M, where M is the number of unlabeled
samples. Experiments from shallow neural networks to deep neural networks are
also provided to justify the correctness of our established theoretical insights on
self-training.
1 Introduction
Self-training (Scudder, 1965; Yarowsky, 1995; Lee et al., 2013; Han et al., 2019), one of the most
powerful Semi-SUperVised learning (SemiSL) algorithms, augments a limited number of labeled data
with unlabeled data so as to achieve improved generalization performance on test data, compared
with the model trained by supervised learning using the labeled data only. Self-training has shown
empirical success in diversified applications such as few-shot image classification (Su et al., 2020;
Xie et al., 2020; Chen et al., 2020a; Yalniz et al., 2019; Zoph et al., 2020), objective detection
(Rosenberg et al., 2005), robustness-aware model training against adversarial attacks (Carmon et al.,
2019), continual lifelong learning (Lee et al., 2019), and natural language processing (He et al.,
2019; Kahn et al., 2020). The terminology “self-training” has been used to describe various SemiSL
1
Published as a conference paper at ICLR 2022
algorithms in the literature, while this paper is centered on the commonly-used iterative self-training
method in particular. In this setup, an initial teacher model (learned from the labeled data) is applied
to the unlabeled data to generate pseudo labels. One then trains a student model by minimizing the
weighted empirical risk of both the labeled and unlabeled data. The student model is then used as
the new teacher to update the pseudo labels of the unlabeled data. This process is repeated multiple
times to improve the eventual student model. We refer readers to Section 2 for algorithmic details.
Despite the empirical achievement of self-training methods with neural networks, the theoretical
justification of such success is very limited, even in the field of SemiSL. The majority of the theo-
retical results on general SemiSL are limited to linear networks (Chen et al., 2020b; Raghunathan
et al., 2020; Oymak & Gulcu, 2020; Oneto et al., 2011). The authors in (Balcan & Blum, 2010)
show that unlabeled data can improve the generalization bound if the unlabeled data distribution and
target model are compatible. For instance, the unlabeled data need to be well-chosen such that the
target function for labeled data can separate the unlabeled data clusters, which, however, may not be
able to be verified ahead. Moreover, (Rigollet, 2007; Singh et al., 2008) proves that unlabeled data
can improve the convergence rate and generalization error under a similar clustering assumption,
where the data contains clusters that have homogeneous labels. A recent work by Wei et al. (2020)
analyzes SemiSL on nonlinear neural networks and proves that an infinite number of unlabeled data
can improve the generalization compared with training with labeled data only. However, Wei et al.
(2020) considers single shot rather than iterative SemiSL, and the training problem aims to minimize
the consistency regularization rather than the risk function in the conventional self-training method
(Lee et al., 2013). Moreover, Wei et al. (2020) directly analyzes the global optimum of the noncon-
vex training problem without any discussion about how to achieve the global optimum. To the best
of our knowledge, there exists no analytical characterization of how the unlabeled data affect the
generalization of the learned model by iterative self-training on nonlinear neural networks.
Contributions. This paper provides the first theoretical
study of iterative self-training on nonlinear neural net-
works. Focusing on one-hidden-layer neural networks,
this paper provides a quantitative analysis of the gen-
eralization performance of iterative self-training as a
function of the number of labeled and unlabeled sam-
ples. Specifically, our contributions include
1.	Quantitative justification of generalization im-
provement by unlabeled data. Assuming the exis-
tence of a ground-truth model with weights W * that
maps the features to the corresponding labels, we prove
that the learned model via iterative self-training moves
closer to W* as the number M of unlabeled data in-
creases, indicating a better testing performance. Specif-
ically, we prove that the Frobenius distance to W*,
which is approximately linear in the generalization er-
ror, decreases in the order of 1 / √M. As an exam-
ple, Figure 1 shows that the proposed theoretical bound
matches the empirical self-training performance versus
sification; see details in Section 4.2.
Fitted curve of θ(^^)
by Theorem 1
100K 200K 300K 400K 500K
Number of unlabeled data
Figure 1: The trend of test accuracy
improvement (%) on CIFAR-10 by self-
training on CIFAR-10 (labeled) with dif-
ferent amount of unlabeled data from 80
Million Tiny Images matches our theoret-
ical prediction.
the number of unlabeled data for image clas-
6 5
瓮)1uəluəAOjdIUl
Aɔpjnɔɔp -səɪ
米 Self-training
2.	Analytical justification of iterative self-training over single shot alternative. We prove that
the student models returned by the iterative self-training method converges linearly to a model close
to W*, with the rate improvement in the order of 1∕√M.
3.	Sample complexity analysis of labeled and unlabeled data for learning a proper model.
We quantify the impact of labeled and unlabeled data on the generalization of the learned model.
In particular, we prove that the sample complexity of labeled data can be reduced compared with
supervised learning.
1.1 Related works
Semi-supervised learning. Besides self-training, many recent SemiSL algorithms exploit either
consistency regularization or entropy minimization. Consistency regularization is based on the as-
2
Published as a conference paper at ICLR 2022
sumption that the learned model will return same or similar output when the input is perturbed
(Laine & Aila, 2016; Bachman et al., 2014; Sajjadi et al., 2016; Tarvainen & Valpola, 2017; Reed
et al., 2015). (Grandvalet & Bengio, 2005) claims that the unlabeled data are more informative if the
pseudo labels of the unlabeled data have lower entropy. Therefore, a line of works (Grandvalet &
Bengio, 2005; Miyato et al., 2018) adds a regularization term that minimizes the entropy of the out-
puts of the unlabeled data. In addition, hybrid algorithms that unify both the above regularizations
have been developed like (Berthelot et al., 2019a;b; Sohn et al., 2020).
Domain adaptation. Domain adaptation exploits abundant data in the source domain to learn a
model for the target domain, where only limited training data are available (Liebelt & Schmid,
2010; Vazquez et al., 2013; Zhang et al., 2013; Long et al., 2015; Tzeng et al., 2014). Source and
target domain are related but different. Unsupervised domain adaptation (Ganin & Lempitsky, 2015;
Ganin et al., 2016; Gong et al., 2013; Bousmalis et al., 2016), where training data in target domain
are unlabeled, is similar to SemiSL, and self-training methods have been used for analysis (Zou
et al., 2018; Tang et al., 2012; French et al., 2018). However, self-training and unsupervised domain
adaptation are fundamentally different. The former learns a model for the domain where there is
limited labeled data, with the help of a large number of unlabeled data from a different domain. The
latter learns a model for the domain where the training data are unlabeled, with the help of sufficient
labeled data from a different domain.
Generalization analysis of supervised learning. In theory, the testing error is upper bounded by the
training error plus the generalization gap between training and testing. These two quantities are often
analyzed separately and cannot be proved to be small simultaneously for deep neural networks. For
example, neural tangent kernel (NTK) method (Jacot et al., 2018; Du et al., 2018; Lee et al., 2018)
shows the training error can be zero, and the Rademacher complexity in (Bartlett & Mendelson,
2002) bounds the generalization gap (Arora et al., 2019a). For one-hidden-layer neural networks
(Safran & Shamir, 2018), the testing error can be proved to be zero under mild conditions. One
common assumption is that the input data belongs to the Gaussian distribution (Zhong et al., 2017;
Ge et al., 2018; Kalai et al., 2008; Bakshi et al., 2019; Zhang et al., 2016; Brutzkus & Globerson,
2017; Li & Yuan, 2017; Soltanolkotabi et al., 2018). Another line of approaches (Brutzkus et al.,
2018; Li & Liang, 2018; Wang et al., 2019) consider linearly separable data.
The rest of this paper is organized as follows. Section 2 introduces the problem formulation and
self-training algorithm. Major results are summarized in Section 3, and empirical evaluations are
presented in Section 4. Section 5 concludes the whole paper. All the proofs are in the Appendix.
2 Formalizing Self-Training: Notation, Formulation, and
Algorithm
Problem formulation. Given N labeled data sampled from distribution Pl , denoted by D =
{xn, yn}nN=1, and M unlabeled data drawn from distribution Pu, denoted by D = {xem}mM=1. The
aim is to find a neural network model g(W), where W denotes the trainable weights, that minimizes
the testing error on data sampled from Pl .
 Table 1: Iterative Self-Training 
(S1)	Initialize iteration ' = 0 and obtain a model W(') as the teacher using labeled data D
only;
(S2)	Use the teacher model to obtain pseudo labels ym, of unlabeled data in D;
(S3)	Train the neural network by minimizing (1) via T-step mini-batch gradient descent
method using disjoint subsets {Dt}T01 and {Dt}To1 of D. Let W('+1) denote the obtained
student model;
(S4)	Use W('+1) as the current teacher model. Let' - ' + 1 and go back to step (S2);
Iterative self-training. In each iteration, given the current teacher predictor g(W(`)), the pseudo-
labels for the unlabeled data in D are computed as ym = g(W ('); Sm). The method then minimizes
the weighted empirical risk fD,De (W) of both labeled and unlabeled data through stochastic gradient
3
Published as a conference paper at ICLR 2022
descent, where
N	eM
fD,D (W) = 2N £ 3 - g(W ； Xn))2 + 2M £ 回m - g(W ；花情))2,	⑴
2N	2M
n=1	m=1
and λ + λ = 1. The learned student model g( W('+1)) is used as the teacher model in the next iter-
ation. The initial model g(W(0)) is learned from labeled data. The formal algorithm is summarized
as in Table 1.
Model and assumptions. This paper considers regression1, where g is a one-hidden-layer fully
connected neural network equipped with K neurons. Namely, given the input x ∈ Rd and weights
W = [wι, w2,…，WK] ∈ Rd×K, we have
1K
g(w; χ) ：= K ΣJ Φ(wTx)，	⑵
where φ is the ReLU activation function2, and φ(z) = max{z, 0} for any input z ∈ R. Here, we fix
the top layer weights as 1 for simplicity, and the equivalence of such a simplification is discussed in
Appendix K.
Moreover, We assume an unknown ground-truth model with weights W * that maps all the features to
the corresponding labels drawn from Pl, i.e., y = g(W*; x), where (x, y)〜Pi. The generalization
function (GF) with respect to g(W) is defined as
I(g(W)) = E(x,y)〜Pl (y - g(W； x))2 = E(x,y)〜Pl (g(W*； x) - g(W； x))2.	⑶
By definition I g(W*) is zero. Clearly, W* is not unique because any column permutation of
W*, which corresponds to permuting neurons, represents the same function as W* and minimizes
GF in (3) too. To simplify the representation, we follow the convention and abuse the notation that
the distance from W to W*, denoted by kW - W* kF, means the smallest distance from W to
any permutation of W*. Additionally, some important notations are summarized in Table 2.
We assume the inputs of both the labeled and unlabeled data belong to the zero mean Gaussian
distribution, i.e., X 〜 N(0,δ2Id), and e 〜 N(0,δ2Id). The Gaussian assumption is motivated
by the data whitening (LeCun et al., 2012) and batch normalization techniques (Ioffe & Szegedy,
2015) that are commonly used in practice to improve learning performance. Moreover, training one-
hidden-layer neural network with multiple neurons is NP-Complete (Blum & Rivest, 1992) without
any assumption.
The focus of this paper. This paper will analyze three aspects about self-training: (1) the gener-
alization performance of W(L), the returned model by self-training after L iterations, measured by
kW(L)-W*kF3;
(2) the influence of parameter λ in (1) on the training performance; and (3) the
impact of unlabeled data on the training and generalization performance.
Table 2: Some Important Notations
D=	{xn , yn }n=1	Labeled dataset with N number of samples;
De	{xem}mM=1	Unlabeled dataset with M number of samples;
d		Dimension of the input X or e;
K		Number of neurons in the hidden layer;
κ		Conditional number (the ratio of the largest and smallest singular values) of W *;
W'		Model returned by self-training after ' iterations; W(0) is the initial model;
W*		Weights of the ground truth model;
Wλ		W闪=^印* + (1 — ^W⑼；
1The results can be extended to binary classification with a cross-entropy loss function. Please see
Appendix-I.
2Because ReLU is non-linear and non-smooth, (1) is non-convex and non-smooth, which poses analytical
challenges. The results can be easily extended to smooth functions with bounded gradients, e.g., Sigmoid.
3We use this metric because I (g(W)) is shown to be linear in ∣∣ W(L) — W * ∣∣f numerically when W(L)
is close to W*, see Figure 4.
4
Published as a conference paper at ICLR 2022
3 Theoretical results
Beyond supervised learning: Challenge of self-training. The existing theoretical works
such as (Zhong et al., 2017; Zhang et al., 2020a;b;c) verify that for one-hidden-layer neu-
ral networks, if only labeled data are available, and x are drawn from the standard Gaus-
sian distribution, then supervised learning by minimizing (1) with λ = 1 can return a
model with ground-truth weights W * (UP to column permutation), as long as the num-
ber of labeled data N is at least N*, which depends on κ, K and d. In contrast, this
paper focuses on the low labeled-data regime when N is less than N*. Specifically,
N*/4 < N≤ N*.	(4)
Intuitively, ifN < N*, the landscape of the empir-
ical risk of the labeled data becomes highly non-
convex, even in a neighborhood of W*, thus, the
existing analyses for supervised learning do not
hold in this region. With additional unlabeled data,
the landscape of the weighted empirical risk be-
comes smoother near W*. Moreover, as M in-
creases, and starting from a nearby initialization,
the returned model W(L) by iterative self-training
can converge to a local minimum that is closer to
W* (see illustration in Figure 2).
Figure 2: Adding unlabeled data in the em-
pirical risk function drives its local minimum
closer to W*, which minimizes the generaliza-
tion function.
Compared with supervised learning, the formal analyses of self-training need to handle new techni-
cal challenges from two aspects. First, the existing analyses of supervised learning exploit the fact
that the GF and the empirical risk have the same minimizer, i.e., W*. This property does not hold
for self-training as W* no longer minimizes the weighted empirical risk in (1). Second, the iterative
manner of self-training complicates the analyses. Specifically, the empirical risk in each iteration is
different and depends on the model trained in the previous iteration through the pseudo labels.
In what follows, we provide theoretical insights and the formal theorems. Some important quantities
λ and μ are defined below
λδ
λ := 77；	≈Ξ7, and μ 一 μ(δ, δ) := ∖ '
λδ2 + λδ2
C	~~c
λδ2 + eδ2
,
λP(δ) + eρ(δ)
(5)
where ρ is a positive function defined in (73). λ is an increasing function of λ. Also, from Lemma
11 (in Appendix), ρ(δ) is in the order of δ2 when δ ≤ 1 for ReLU activation functions. Thus, μ is a
〜
〜
fixed constant, denoted by μ*, for all δ,δ ≤ 1. When δ and δ are large, μ increases as they increase.
The formal definition of N * in (4) is c(κ)μ*2K 3d log q, where c(κ) is some polynomial function of
κ and can be viewed as constant.
3.1 Informal key theoretical findings
To the best of our knowledge, Theorems 1 and
2 provide the first theoretical characterization of
iterative self-training on nonlinear neural net-
works. Before formally presenting them, we
summarize the highlights as follows.
1. Linear convergence of the learned models.
The learned models converge linearly to a model
close to W*. Thus, the iterative approach re-
turns a model with better generalization than that
by the single-shot method. Moreover, the con-
vergence rate is a constant term plus a term in
the order of 1∕√M (see ∆ι in Figure 3), indi-
cating a faster convergence with more unlabeled
data.
Figure 3: Illustration of the (1) ground truth W*,
(2) iterations {W(')}L=o, (3) convergent point
W(L), and (4) WN =、W* + (1 -川 W⑼.
5
Published as a conference paper at ICLR 2022
2.	Returning a model with guaranteed generalization in the low labeled-data regime. Even
when the number of labeled data is much less than the required sample complexity to obtain W *
in supervised learning, we prove that with the help of unlabeled data, the iterative self-training can
return a model in the neighborhood of W [λ], where W [λ] is in the line segment of W ⑼(W = 0)
and ground truth W * (W = 1). Moreover, W is upper bounded by YN/N *. Thus W (L) moves
closer to W* as N increases (E0 in Figure 3), indicating a better generalization performance with
more labeled data.
3.	Guaranteed generalization improvement by unlabeled data. The distance between W(L)
and W[λ] (Ei in Figure 3) scales in the order of 1∕√M. With a larger number of unlabeled data
M, W(L) moves closer to W[λ] and thus W*, indicating an improved generalization performance
(Theorem 1). When N is close to N* but still smaller as defined in (12), both W(L) and W[λ]
converge to W*, and thus the learned model achieves zero generalization error (Theorem 2).
3.2 Formal theory in low labeled-data regime
Takeaways of Theorem 1: Theorem 1 characterizes the convergence rate of the proposed algorithm
and the accuracy of the learned model W(L) in a low labeled-data regime. Specifically, the iterates
converge linearly, and the learned model is close to W [λ] and guaranteed to outperform the initial
model W(0).
Theorem 1.	Suppose the initialization W(0) and the number of labeled data satisfy
kW(0)-W*kF≤PjC(KW⅛	with p∈(2』	⑹
and max { ɪ,p 一 2p√-^ } ∙ N* ≤ N ≤ N*.	(7)
If the value of λ in (5) and unlabeled data amount M satisfy
(8)
and M ≥ (2p 一 1)-2c(κ)μ2(l — λ)2K3dlogq.	(9)
Then, when the number T of SGD iterations is large enough in each loop `, with probability at least
1 — q-d, the iterates {W(')}L=o converge to W[λ] as
∣∣W(L)- W四IF ≤ (, + Θ(μ√M^)) ∙ T) ∙ ∣W(0) — W叫∣? + 0 + θ(μ(√M^))) ∙∣w* - W叫IF,
(10)
where W[λ] = λW* + (1 — λ) W((J). Typically, when the iteration number L is sufficient large, we
have
∣∣W(L) — W*∣∣F ≤(1 + Θ(μ√―λ))) ∙ 2(1 —	∙ kW* — W(0)kF.
(11)
The accuracy of the learned model W(L) with respect to W* is characterized as (10), and the
learning model is better than initial model as in (11) if the following conditions hold. First, the
weights λ in (1) are properly chosen as in (8). Second, the number of unlabeled data is sufficiently
large as in (9).
Selection of λ in self-training algorithms. When λ increases, the required number of unlabeled
data is reduced from (9), and the convergence point W(L) becomes closer to W* from (11), which
indicates a smaller generalization error. Thus, a large λ within its feasible range (8) is desirable.
When the initial model W(0) is closer to W* (corresponding to a larger p), and the number of
labeled data N increases, the upper bound in (8) increases, and thus, one can select a larger λ.
The initial model W(0). The tensor initialization from (Zhong et al., 2017) can return a W(0) that
satisfies (6) when the number of labeled data is N = p2N* (see Lemma 3 in Appendix). Combining
with the requirement in (7), Theorem 1 applies to the case that N is at least N*/4.
6
Published as a conference paper at ICLR 2022
3.3 Formal theory of achieving zero generalization error
Takeaways of Theorem 2: Theorem 2 indicates the model returned by the proposed algorithm con-
verges linearly to the ground truth W*. Thus the distance between the learned model and the ground
truth can be arbitrarily small with the ability to achieve zero generalization error. The required sam-
ple complexity is reduced by a constant factor compared with supervised learning.
Theorem 2.	Consider the number of unlabeled data satisfies
(1 - 1∕(μ√K ))2 ∙ N * ≤ N ≤ N *,	(12)
we choose λ such that
1 - 1∕(μ√K) ≤ λ ≤ √N∕N*.	(13)
Suppose the initial model W(0) and the number of unlabeled data M satisfy
kW⑼一W*kF ≤ JW⅛∕2	and M ≥ c(κ)μ2(1 - *)2K3dlogq,	(14)
c(κ)μ2K 3/2
the iterates {W (')}L=o converge to the ground truth W * asfollows,
kW(L)- W*kF ≤ [(1 + -√N + ( √M- λ)) ∙μ√K(1- λ)i ∙kW⑼-W*kF.
(15)
The models W Ss converge linearly to the ground truth W * as (15) when the number of labeled
data satisfies (12). In contrast, supervised learning requires at least N* labeled samples to estimate
W * accurately without unlabeled data, which suggests self-training at least saves a constant fraction
of labeled data.
3.4 The main proof idea
Our proof builds upon and extends one recent line of works on supervised learning such as (Zhong
et al., 2017; Zhang et al., 2020b;c; 2021). The standard framework of these works is first to show
that the generalization function I(g(W)) in (3) is locally convex near W*, which is its global
minimizer. Then, when M = 0 and N is sufficiently large, the empirical risk function using labeled
data only can approximate I(g(W)) well in the neighborhood of W*. Thus, if initialized in this
local convex region, the iterations, returned by applying gradient descent approach on the empirical
risk function, converge to W* linearly.
The technical challenge here is that in self-training, when unlabeled data are paired with pseudo
labels, W* is no longer a global minimizer of the empirical risk fD,De in (1), and fD,De does not
approach I(g(W)) even when M and N increase to infinity. Our new idea is to design a population
risk function f(W; λ) in (17) (see appendix), which is a lower bound of fD,De when M and N are
infinity. f (W; W) is locally convex around its minimizer W [λ], and W [λ] approaches W * as λ
increases. Then we show the iterates generated by fD,De stay close to f(W; λ), and the returned
model W(L) is close to W[^. New technical tools are developed to bound the distance between the
functions fD,De and f(W; λ).
4	Empirical results
4.1	Synthetic data experiments
We generate a ground-truth neural network with the width K = 10. Each entry of W* is uniformly
selected from [-2.5, 2.5]. The input of labeled data xn are generated from Gaussian distribution
N(0, Id) independently, and the corresponding label yn is generated through (2) using W*. The
2
unlabeled data xem are generated from N(0, δ2Id) independently with δ = 1 except in Figure 7.
d is set as 50 except in Figure 9. The value of λ is selected as ,N∕(2Kd) except in Figure 8.
We consider one-hidden-layer except in Figure 4. The initial teacher model W(0) in self-training
is randomly selected from {W |kW - W* kF∕kW* kF ≤ 0.5} to reduce the computation. In
7
Published as a conference paper at ICLR 2022
each iteration, the maximum number of SGD steps T is 10. Self-training terminates if ∣∣ W('+1) -
W⑶∣F/∣W⑶kF ≤ 10-4 or reaching 1000 iterations. In Figures 5 to 8, all the points on the
curves are averaged over 1000 independent trials, and the regions in lower transparency indicate
the corresponding one-standard-deviation error bars. Our empirical observations are summarized
below.
(a)	GF (testing performance) proportional to ∣∣ W - W*∣f. Figure 4 illustrates the GF in (3)
against the distance to the ground truth W*. To visualize results for different networks together,
GF is normalized in [0, 1], divided by its largest value for each network architecture. All the results
are averaged over 100 independent choice of W. One can see that for one-hidden-layer neural
networks, in a large region near W*, GF is almost linear in ∣∣ W - W*∣f. When the number of
hidden layers increases, this region decreases, but the linear dependence still holds locally. This is
an empirical justification of using ∣∣ W - W *∣f to evaluate the GF and, thus, the testing error in
Theorems 1 and 2.
(b)	k W(L) - W*∣f as a linear function of 1∕√M. Figure 5 shows the relative error ∣∣ W(L) -
W *∣F ∕∣W *∣∣F when the number of unlabeled data and labeled data changes. One can see that the
relative error decreases when either M or N increases. Additionally, the dash-dotted lines represent
the best fitting of the linear functions of 1/√M using the least square method. Therefore, the relative
error is indeed a linear function of 1/√M, as predicted by our results in (11) and (15).
4.3
3.5
IlW - W*IlF∕∣∣w*IIF
Figure 4: The generalization
function against the distance to
the ground truth neural network
Fitted curve of θ(^j)
fey Theorem 1
3
Number of unlabeled data (M)
200	400	600	800	1000
Figure 6: The convergence rate
with different M when N <
N *.
Figure 5: The relative error
against the number of unlabeled
data.
(c)	Convergence rate as a linear function of 1/√M. Figure 6 illustrates the convergence rate
when M and N change. We can see that the convergence rate is a linear function of 1/√M, as
predicted by our results (11) and (15). When M increases, the convergence rate is improved, and
the method converges faster.
(d)	Increase of δ slows down convergence. Figure 7 shows that the convergence rate becomes
worse when the variance of the unlabeled data δ increases from 1. When δ is less than 1, the
convergence rate almost remains the same, which is consistent with our characterization in (10) that
the convergence rate is linear in μ. From the discussion after (5), μ increases as δ increases from 1
and stays constant when δ is less than 1.
(e)	∣ W (L)-W * ∣ F /1W * ∣ F is improved as a linear function of 工 Figure 8 shows that the relative
errors of W (L) with respect to W * decrease almost linearly when λ increases, which is consistent
with the theoretical result in (11). Moreover, when λ exceeds a certain threshold positively correlated
with N, the relative error increases rather than decreases. That is consistent with the analysis in (8)
that λ has an upper limit, and such a limit increases as N increases.
(f)	Unlabeled data reduce the sample complexity to learn W*. Figure 9 depicts the phase tran-
sition of returning W(L). For every pair of d and N, we construct 100 independent trials, and each
trial is said to be successful if ∣∣ W(L) - W*∣f/∣ W*∣f ≤ 10-2. The white blocks correspond to
the successful trials, while the block in black indicates all failures. When d increases, the required
number of labeled data to learn W* is linear in d. Thus, the sample complexity bound in (12) is
order-wise optimal for d. Moreover, the phase transition line when M = 1000 is below the one
when M = 0. Therefore, with unlabeled data, the required sample complexity of N is reduced.
8
Published as a conference paper at ICLR 2022
1	2	3 J 5
The value of S
Figure 7: Convergence rate
with different δ.
0.1	0.2	0.3	0.4	0.5
Mdp P313qdl 30 JqqUInN
M = O
680
620
560
500
440
380
320
260
200
20 26 32 38 44 50
Dimension of input (d)
Figure 9: Empirical phase transition of
the curves with (a) M = 0 and (b) M =
1000.
M = 1000
680
620
560
500
440
380
320
260
20 26 32 38 44 50
Dimension of input (d)
kW (L)-W *k
Figure 8:	tw*> -
when λ and N change.
4.2	Image classification on augmented CIFAR- 1 0 dataset
We evaluate self-training on the augmented CIFAR-10 dataset, which has 50K labeled data. The
unlabeled data are mined from 80 Million Tiny Images following the setup in (Carmon et al., 2019)4,
and additional 50K images are selected for each class, which is a total of 500K images, to form the
unlabeled data. The self-training method is the same implementation as that in (Carmon et al., 2019).
λ and λ is selected as N/(M + N) and M/(N + M), respectively, and the algorithm stops after
200 epochs. In Figure 10, the dash lines stand for the best fitting of the linear functions of 1/√M
via the least square method. One can see that the test accuracy is improved by up to 7% using
unlabeled data, and the empirical evaluations match the theoretical predictions. Figure 11 shows the
convergence rate calculated based on the first 50 epochs, and the convergence rate is almost a linear
function of 1/√M, as predicted by (10).
-)∣(-N=15K -θ-N=30K
I~~I IkT KW Fitted curve of Q(l∕y∕M)
—	—	....by Theorem 1
Figure 10: The test accuracy against the
number of unlabeled data
1∕√M	×1Q-3
Figure 11: The convergence rate against the
number of unlabeled data
5	Conclusion
This paper provides new theoretical insights into understanding the influence of unlabeled data in
the iterative self-training algorithm. We show that the improved generalization error and conver-
gence rate is a linear function of 1/√M, where M is the number of unlabeled data. Moreover,
compared with supervised learning, using unlabeled data reduces the required sample complexity of
labeled data for achieving zero generalization error. Future directions include generalizing the anal-
ysis to multi-layer neural networks and other semi-supervised learning problems such as domain
adaptation.
Acknowledgement
This work was supported by AFOSR FA9550-20-1-0122, ARO W911NF-21-1-0255, NSF 1932196
and the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part of the IBM AI Hori-
zons Network (http://ibm.biz/AIHorizons).
4The codes are downloaded from https://github.com/yaircarmon/semisup-adv
9
Published as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems,pp. 6158-6169, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In 36th In-
ternational Conference on Machine Learning, ICML 2019, pp. 477-502. International Machine
Learning Society (IMLS), 2019b.
Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in
neural information processing systems, 2014.
Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory, pp. 195-268. PMLR, 2019.
Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Jour-
nal of the ACM (JACM), 57(3):1-46, 2010.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmenta-
tion anchoring. In International Conference on Learning Representations, 2019a.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint
arXiv:1905.02249, 2019b.
Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. Neural
Networks, 5(1):117-127, 1992.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 343-351, 2016.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605-614. JMLR. org, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. Advances in Neural Information Processing Systems, 32:
11192-11203, 2019.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervised models are strong semi-supervised learners. Advances in Neural Information
Processing Systems, 33:22243-22255, 2020a.
Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious
features under domain shift. Advances in Neural Information Processing Systems, 33, 2020b.
10
Published as a conference paper at ICLR 2022
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta-
tion. In International Conference on Learning Representations, number 6, 2018.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Guaranteed recovery of one-hidden-layer neural networks
via cross entropy. IEEE Transactions on Signal Processing, 68:3225-3235, 2020.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International conference on machine learning, pp. 1180-1189. PMLR, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The journal of machine learning research, 17(1):2096-2030, 2016.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with land-
scape design. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BkwHObbRZ.
Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discrimina-
tively learning domain-invariant features for unsupervised domain adaptation. In International
Conference on Machine Learning, pp. 222-230. PMLR, 2013.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Con-
ference d’apprentissage CAp, pp. 281, 2005.
Jiangfan Han, Ping Luo, and Xiaogang Wang. Deep self-learning from noisy labels. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 5138-5147, 2019.
Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. Revisiting self-training for neural
sequence generation. In International Conference on Learning Representations, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. volume 37 of Proceedings of Machine Learning Research, pp.
448-456, Lille, France, 07-09 Jul 2015. PMLR.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, 2018.
Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 7084-7088. IEEE, 2020.
Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically
learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805, 2008.
Volodymyr Kuleshov, Arun Chaganty, and Percy Liang. Tensor factorization via matrix factoriza-
tion. In Artificial Intelligence and Statistics, pp. 507-516, 2015.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Yann A LeCun, L6on Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for
deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3,
2013.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018.
11
Published as a conference paper at ICLR 2022
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with un-
labeled data in the wild. In Proceedings of the IEEE/CVF International Conference on Computer
Vision ,pp.312-321,2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-
tion. In Advances in Neural Information Processing Systems, pp. 597-607. 2017.
Joerg Liebelt and Cordelia Schmid. Multi-view object class detection with a 3d geometric model.
In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp.
1688-1695. IEEE, 2010.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97-105. PMLR,
2015.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Luca Oneto, Davide Anguita, Alessandro Ghio, and Sandro Ridella. The impact of unlabeled pat-
terns in rademacher complexity theory for kernel classifiers. Advances in neural information
processing systems, 24:585-593, 2011.
Samet Oymak and Talha Cihad Gulcu. Statistical and algorithmic insights for semi-supervised
learning with self-training. arXiv preprint arXiv:2006.11006, 2020.
Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via
deep tensor decomposition. arXiv preprint arXiv: 1805.06523, 2018.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. In International Conference on
Machine Learning, pp. 7909-7919. PMLR, 2020.
Scott E Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR (Work-
shop), 2015.
Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster
assumption. Journal of Machine Learning Research, 8(7), 2007.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object
detection models. In Proceedings of the Seventh IEEE Workshops on Application of Computer
Vision (WACV/MOTION’05)-Volume 1-Volume 01, pp. 29-36, 2005.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4430-4438, 2018.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. Advances in neural information
processing systems, 29:1163-1171, 2016.
Henry Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac-
tions on Information Theory, 11(3):363-371, 1965.
Aarti Singh, Robert Nowak, and Jerry Zhu. Unlabeled data: Now it helps, now it doesn’t. Advances
in neural information processing systems, 21:1513-1520, 2008.
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel,
Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised
learning with consistency and confidence. Advances in Neural Information Processing Systems,
33, 2020.
12
Published as a conference paper at ICLR 2022
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan. When does self-supervision improve few-
shot learning? In European Conference on Computer Vision, pp. 645-666. Springer, 2020.
Kevin Tang, Vignesh Ramanathan, Fei-Fei Li, and Daphne Koller. Shifting weights: Adapting
object detectors from image to video. In Advances in Neural Information Processing Systems, pp.
647-655, 2012.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning results. In Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, pp. 1195-1204, 2017.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
David Vazquez, Antonio M Lopez, Javier Marin, Daniel Ponsa, and David Geronimo. Virtual and
real world adaptation for pedestrian detection. IEEE transactions on pattern analysis and machine
intelligence, 36(4):797-809, 2013.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357-2370, 2019.
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training
with deep networks on unlabeled data. In International Conference on Learning Representations,
2020.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10687-10698, 2020.
I Zeki Yalniz, Herve J6gou, Kan Chen, Manohar Paluri, and DhrUv Mahajan. Billion-scale Semi-
supervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019.
David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd
annual meeting of the association for computational linguistics, pp. 189-196, 1995.
Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, pp. 819-827.
PMLR, 2013.
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Guaranteed convergence
of training convolutional neural networks via accelerated gradient descent. In 2020 54th An-
nual Conference on Information Sciences and Systems (CISS), 2020a. URL doi:10.1109/
CISS48834.2020.1570627111.
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph neural
networks with guaranteed generalizability:one-hidden-layer case. In 2020 International Confer-
ence on Machine Learning (ICML), 2020b.
Shuai Zhang, Meng Wang, Jinjun Xiong, Sijia Liu, and Pin-Yu Chen. Improved linear convergence
of training cnns with generalizability guarantees: A one-hidden-layer case. IEEE Transactions
on Neural Networks and Learning Systems, 32(6):2622-2635, 2020c.
13
Published as a conference paper at ICLR 2022
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Why lottery ticket wins? a the-
oretical perspective of sample complexity on pruned neural networks. In Thirty-fifth Conference
on Neural Information Processing Systems (NeurIPS), 2021.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence
and Statistics ,pp.1524-1534. PMLR, 2019.
Yuchen Zhang, Jason D. Lee, and Michael I. Jordan. L1-regularized neural networks are improperly
learnable in polynomial time. In Proceedings of The 33rd International Conference on Machine
Learning, volume 48, pp. 993-1001, 2016.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guaran-
tees for one-hidden-layer neural networks. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 4140-4149. JMLR. org, https://arxiv.org/abs/1706.03175,
2017.
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.
Rethinking pre-training and self-training. Advances in Neural Information Processing Systems,
33, 2020.
Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for se-
mantic segmentation via class-balanced self-training. In Proceedings of the European conference
on computer vision (ECCV), pp. 289-305, 2018.
14
Published as a conference paper at ICLR 2022
Appendix
A Overview of the proof techniques
We first provide an overview of the techniques used in proving Theorems 1 and 2.
1.	Characterization of a proper population risk function. To characterize the performance of the
iterative self-training algorithm via the stochastic gradient descent method, we need first to define
a population risk function such that the following two properties hold. First, the landscape of the
population risk function should be analyzable near {W(`)}". Second, the distance between the
empirical risk function in (1) and the population risk function should be bounded near {W(')}L=0.
The generalization function defined in (3), which is widely used in the supervised learning problem
with a sufficient number of samples, failed the second requirement. To this end, we turn to find a
new population risk function defined in (17), and the illustrations of the population risk function and
objection function are included in Figure 12.
2.	Local convex region of the population risk function. The purpose is to characterize the it-
erations via the stochastic gradient descent method in the population risk function. To obtain the
local convex region of the population risk function, we first bound the Hessian of the population risk
function at its global optimal. Then, we utilize Lemma 12 in Appendix H.1 to obtain the Hessian of
the population risk function near the global optimal. The local convex region of the population risk
function is summarized in Lemma 1, and the proof of Lemma 1 is included in Appendix H.1.
3.	Bound between the population risk and empirical risk functions. After the characterization of
the iterations via the stochastic gradient descent method in the population risk function, we need to
bound the distance between the population risk function and empirical risk function. Therefore, the
behaviors of the iterations via the stochastic gradient descent method in the empirical risk function
can be described by the ones in the population risk function and the distance between these two. The
key lemma is summarized in Lemma 2 (see Appendix H.2), and the proof is included in Appendix
H.2.
Figure 12: The landscapes of the objection function and population risk function.
In the following contexts, the details of the iterative self-training algorithm are included in Appendix
B. We then first provide the proof of Theorem 2 in Appendix E, which can be viewed as a special
case of Theorem 1. Then, with the preliminary knowledge from proving Theorem 2, we turn to
present the full proof of a more general statement summarized in Theorem 3 (see Appendix F),
which is related to Theorem 1. The definition and relative proofs of μ and P are all included in
Appendix G. The proofs of preliminary lemmas are included in Appendix H.
B Iterative self-training algorithm
In this section, we implement the details of the mini-batch stochastic gradient descent used in each
stage of the iterative self-training algorithm. After t number of iterations via mini-batch stochastic
15
Published as a conference paper at ICLR 2022
gradient descent at `-th stage of self-training algorithm, the learned model is denoted as W (`,t). One
can easily check that W(') in the main context is denoted as W(',0) in this section and the following
proofs. Last, the pseudo-code of the iterative self-training algorithm is summarized in Algorithm 1.
Algorithm 1 Iterative Self-Training Algorithm
Input: labeled D = {(xn, yn)}nN=1, unlabeled data D = {xem}mM=1, and gradient step size η;
Initialization: preliminary teacher model with weights W(0,0);
Partition: randomly and independently pick data from D and De to form T subsets {Dt }tT=-01 and
{Det}tT=-01, respectively;
for ' = 0,1, ∙∙∙ ,L 一 1 do
ym = g(W(',0)； em) for m =1, 2,…，M
for t = 0,1,…，T - 1 do
w(',t+1) = wWt) - η ∙ ▽力 e (wWt)) + β. (WWt) - W(',tT))
end for
W ('+1,0) = W (',τ)
end for
C Notations
In this section, we first introduce some important notations that will be used in the following proofs,
and the notations are summarized in Table 1.
As shown in Algorithm 1, W (`,t) denotes the learned model after t number of iterations via mini-
batch stochastic gradient descent at `-th stage of the iterative self-training algorithm. Given a student
model W, the pseudo label for xe ∈ D is generated as
,r≥c 一八
y = g(W ； x).	(16)
Further, let W[p] = pW* + (1 - p)W(O，0), we then define the population risk function as
2e	2
f (W; p) =	2Eχ(y*(p)-	g(W; x))	+ 2Ex	(y* (p)	-	g(W;	e))	,	(17)
where y*(p) = g(W[p]; x) with x 〜N(0,δ2I) and e*(p) = g(W[p]; e) with e 〜N(0, δ2I).
When p = 1, we have W[p] = W* and y*(p) = y for data in D.
Moreover, We use σi to denote the i-th largest singular value of W*. Then, K is defined as σ∖∕σκ,
and Y = QK=I σjσκ. Additionally, to avoid high dimensional tensors, the first order derivative of
the empirical risk function is defined in the form of vectorized W as
V∕(W )=芹 τ,f T,…,f T iτ ∈ RdK	(18)
∂w1	∂w2	∂wK
with W = [wι, w2,…,WK] ∈ Rd×K. Therefore, the second order derivative of the empiri-
cal risk function is in Rdk×dk. Similar to (18), the high order derivatives of the population risk
functions are defined based on vectorized W as well. In addition, without special descriptions,
α= [αT, αT,… , ατK]τ stands for any unit vector that in RdK with αj ∈ Rd. Therefore, we have
/K ^ ^、2
∣∣V2f∣∣2 = max ∣∣ατV2fα∣∣2 = max (X ατ--)
α	α	∂wj
(19)
16
Published as a conference paper at ICLR 2022
Finally, since we focus on order-wise analysis, some constant numbers will be ignored in the major-
ity of the steps. In particular, we use h1 (z) & (or ., h)h2(z) to denote there exists some positive
constant C such that hι(z) ≥ (or ≤, =)C ∙ h2(z) when Z ∈ R is sufficiently large.
Table 3: Some Important Notations
D=	{xn , yn }n=1	Labeled dataset with N number of samples;
De	= {xem}mM=1	Unlabeled dataset with M number of samples;
Dt	{xn , yn }n=t 1	a subset of D with Nt number of labeled data;
■— Det	= {xem}mM=t 1	a subset of D With Mt number of unlabeled data;
d		Dimension of the input X or e;
K		Number of neurons in the hidden layer;
W *		Weights of the ground truth model;
W[p]		W[p] = pW* + (1 - P)W(0,0);
W (`,t)		Model returned by iterative self-training after t step mini-batch stochastic gradient de- scent at stage `; W (0,0) is the initial model;
^ , ^. fD,D ( or f)		The empirical risk function defined in (1);
f(W;p)		The population risk function defined in (17);
^ ^		The value of λδ2∕(λδ2 + eδ2);
μ		The value of 旷苴吸 ; 	λρ(δ)+eρ(δ);	
σi		The i-th largest singular value of W *;
κ		The value of σι /σκ;
γ		The value of QK=I σ⅛∕σκ;
q		Some large constant in R+;
D Preliminary Lemmas
We will first start with some preliminary lemmas. As outlined at the beginning of the supplementary
material, Lemma 1 illustrates the local convex region of the population risk function, and Lemma
2 explains the error bound between the population risk and empirical risk functions. Then, Lemma
3 describes the returned initial model W (0,0) via tensor initialization method (Zhong et al., 2017)
purely using labeled data. Next, Lemma 4 is the well known Weyl’s inequality in the matrix setting.
Moreover, Lemma 5 is the concentration theorem for independent random matrices. The definitions
of the sub-Gaussian and sub-exponential variables are summarized in Definitions 1 and 2. Lemmas
6 and 7 serve as the technical tools in bounding matrix norms under the framework of the confidence
interval.
Lemma 1. Given any W ∈ Rd×K, let p satisfy
P . μ2κ ∙ kw - w*kF
(20)
Then, we have
,, ~ , ~.
λρ(δ) + λρ(S)
12κ2γK 2
M(W; P)W 空尸.
K
(21)
17
Published as a conference paper at ICLR 2022
Lemma 2. Let f and f be the functions defined in (17) and (1), respectively. Suppose the pseudo
label is generated through (16) with weights W. Then, we have
kv〃w)- Vf(W)k2. λδ2 rdlogq ∙kw - w *∣∣+λδ2 rdogq ∙ kw—w k2
KN	KM
∣∣λδ2 ∙ (W — w[p]) + λδ2 ∙ (W* — w[p])∣∣2
+	2K
(22)
with probability at least 1 - q-d.
Lemma 3 (Initialization, (Zhong et al., 2017)). Assuming the number of labeled data satisfies
N ≥ p2N*	(23)
for some large constant q and P ∈ [-K, 1], the tensor initialization method, which is summarized in
Appendix I, outputs w (0,0) such that
kw(0,0) - W * kF ≤ P ∙ cσKμ2κ	(24)
with probability at least 1 - q-d.
Lemma 4 (Weyl’s inequality, (Bhatia, 2013)). Let B = A + E be a matrix with dimension m × m.
Letλi(B) and λi(A) be the i-th largest eigenvalues ofB and A, respectively. Then, we have
∣%(B)- λi(A)∣≤∣∣E∣∣2, ∀ i ∈ [m].	(25)
Lemma 5 ((Tropp, 2012), Theorem 1.6). Consider a finite sequence {Zk} of independent, random
matrices with dimensions d1 × d2. Assume that such random matrix satisfies
E(Zk ) = 0 and kZk k ≤ R almost surely.
Define
δ2 := max n∣∣∣ XE(ZkZk*)∣∣∣,∣∣∣ XE(Zk*Zk)∣∣∣o.
kk
Then for all t ≥ 0, we have
Prob ∣∣∣ kZk
≥ t} ≤ (d1+d2)exp (δ⅛‰).
Definition 1 (Definition 5.7, (Vershynin, 2010)). A random variable X is called a sub-Gaussian
random variable if it satisfies
(E|X|p)1/p ≤ C1 √P	(26)
for all P ≥ 1 and some constant c1 > 0. In addition, we have
Ees(X-EX) ≤ ec2kXk2ψ2s2	(27)
for all s ∈ R and some constant c2 > 0, where kX kφ2 is the sub-Gaussian norm of X defined as
kX kψ2 =suPp≥ι PT/2(EXIp)1/p.
Moreover, a random vector X ∈ Rd belongs to the sub-Gaussian distribution if one-dimensional
marginal αT X is sub-Gaussian for any α ∈ Rd, and the sub-Gaussian norm of X is defined as
kX kψ2 = supkαk2=1kαTXkψ2.
Definition 2 (Definition 5.13, (Vershynin, 2010)). A random variable X is called a sub-exponential
random variable if it satisfies
(E|X|p)1/p ≤c3P
for all P ≥ 1 and some constant c3 > 0. In addition, we have
Ees(X-EX) ≤ ec4kXk2ψ1s2
(28)
(29)
for s ≤ 1/kX kψ1 and some constant c4 > 0, where kX kψ1 is the sub-exponential norm of X
defined as ∣∣X∣∣ψι = supp≥1 PT(EX∣p)1/p.
18
Published as a conference paper at ICLR 2022
Lemma 6 (Lemma 5.2, (Vershynin, 2010)). Let B(0, 1) ∈ {αkαk2 = 1, α ∈ Rd} denote a
unit ball in Rd. Then, a subset Sξ is called a ξ-net of B(0, 1) if every point z ∈ B(0, 1) can be
approximated to within ξ by some point α ∈ B(0, 1), i.e., kz - αk2 ≤ ξ. Then the minimal
cardinality of a ξ-net Sξ satisfies
∣Sξ| ≤ (1 + 2∕ξ)d.	(30)
Lemma 7 (Lemma 5.3, (Vershynin, 2010)). Let A be an d1 × d2 matrix, and let Sξ(d) be a ξ-net
of B(0, 1) in Rd for some ξ ∈ (0, 1). Then
IIAk2 ≤ (IT)T	. ,max ＜，“、|aTAa2|.	(31)
α1∈Sξ (d1),α2∈Sξ(d2)
Lemma 8 (Mean Value Theorem). Let U ⊂ Rn1 be open and f : U -→ Rn2 be continuously
differentiable, and x ∈ U, h ∈ Rn1 vectors such that the line segment x + th, 0 ≤ t ≤ 1 remains
in U. Then we have:
f(x + h) - f(x)
Vf (x + th)dt
. h,
where Vf denotes the Jacobian matrix of f.
E Proof of Theorem 2
With p = 1 in (17), the population risk function is reduced as
f (W) = λ2Eχ(y - g(W； x)) + 2Eχ(e* - g(W； e)),	(32)
where y = g(W*; x) with X 〜N(0,δ2I) and 谟 =g(W*; e) with e 〜N(0, δ2I). In fact,
(32) can be viewed as the expectation of the empirical risk function in (1) given ym, = g(W *; Xm).
Moreover, the ground-truth model W* is the global optimal to (32) as well. Lemmas 9 and 10
are the special case of Lemmas 1 and 2 with p = 1. The proof of Theorem 2 is followed by the
presentation of the two lemmas.
The main idea in proving Theorem 2 is to characterize the gradient descent term by the MVT in
Lemma 8 as shown in (36) and (37). The IVT is not directly applied in the empirical risk func-
tion because of its non-smoothness. However, the population risk functions defined in (17) and
(32), which are the expectations over the Gaussian variables, are smooth. Then, as the distance
IVf(W) - Vf(W*)IF is upper bounded by a linear function of IW - W*IF as shown in (47),
We can establish the connection between ∣∣ W(',t+1) 一 W*∣f and ∣∣ W(',t) 一 W*∣f as shown in
(50). Finally, by mathematical induction over ` and t, one can characterize IW (L,0) - W* IF by
∣W (0,0) - W*∣
F as shown in (52), which completes the whole proof.
ɪ .   ∕⅛ /ɪ	1 ∙ .1_	r∖ τ . c 1 P	. 1 r . ■	Ir 1 ∙ ∕cc∖ J /1 ∖	. ■ ι
Lemma 9 (Lemma 1 with p = 1). Let f and f are the functions defined in (32) and (1), respectively.
Then, for any W that satisfies,
∣w 一 W *∣f ≤ μσκK,	(33)
we have
λρ≡⅛≡ 7(W)W 7(λδ⅛^)
12κ2 γ K2	K
(34)
ɪ .   λ /ɪ	∙. i_	r∖ τ . c ιfiι . 1 r . ■ Ir ι ∙ ∕cc∖ ι ∕λ∖	. ■ ι
Lemma 10 (Lemma 2 with p = 1). Let f and f be the functions defined in (32) and (1), respectively.
Suppose the pseudo label is generated through (16) with weights W. Then, we have
IVf(W) 一 Vf(W)∣2 .(λK2 Jd^Nq + (I M2 JdMq) ∙ ∣W - W*∣2
,(1 - λ)δ2 ( Idlog q 1∖
+ K wɪ- + 2J
..-~~^	...
∣Wf 一 W* ∣2
with probability at least 1 一 q-d.
(35)
19
Published as a conference paper at ICLR 2022
Proof of Theorem 2. From Algorithm 1, in the `-th outer loop, we have
W(',t+1) =W(',t) - NfD Dt(W(',t)) + β(W('㈤-W('，tT))
=W('㈤-ηVf(W(',力 + β(W(Kt) - W('，tT))	(36)
+ η ∙ (Vf(W(',t)) - VfDt,Det(W(',t))).
Since Vf is a smooth function and W * is a local (global) optimal to f, then We have
Vf(W(',t)) =Vf(W(',t)) -Vf(W*)
=Zo V2f (W('㈤ + U ∙ (W('，t) - W*))du ∙ (W('，t) - W*),
(37)
Where the last equality comes from MVT in Lemma 8. For notational convenience, We use H (`,t)
to denote the integration as
H('㈤:
( V2f(W('，t)
+ u ∙ (W('，t) - W*))du.
(38)
Then, We have
W(',t+1) - W*		I-	ηH('，t)	βι	W (`,t) - W*	
W (`,t) - W*		+η	I0 Vf(W('，t))-	W (',tT) - W * VfDt,Dt (W (',t))# 0	(39) .
Let H(',t) = S AST be the eigen-decomposition of H(',t). Then, we define
			A(β) :=	Sτ	0	A(β)	S	0		I-ηΛ+βI βI			(40)
								S	=		I0	.	
				0	Sτ		0						
													
	S0		Sτ 0		I0						I-ηΛ+βI βI		
Since				=		, we know A(β) and							share the same
	0S		0 Sτ		0I						I	0	
eigenvalues. Let γi(Λ) be the i-th eigenvalue of V2f(wb(t)), then the corresponding i-th eigenvalue
of (40), denoted by γi(A), satisfies
(Yi(A)(β))2 - (1 - ηYi(Λ) + β)Yi(A)(β) + β = 0.
By simple calculation, we have
(41)
IYi(A) (β)l
1
2
(1 - ηYi(Λ) + β) +	(1 - ηYi(Λ) + β)2 - 4β , otherwise.
(42)
Specifically, we have
Y(A)(0) >Yi(A)(β),	for ∀β ∈ (0,(1-ηγ(A))2),
(43)
and Yi(A) achieves the minimum Yi(A)* =
9, for any a ∈ Rd with kak2 = 1, we have
when β
2
. From Lemma
aτ Vf (W ('",a = / aτ V2 f(W ('，t) + U ∙ (W ('，t) - W *))adu ≤ / Ymaxkak2du
aτVf(W('，t))a = / aτV2f(W('，t) + U ∙ (W('，t) - W*))adu ≥ / Yminkak2du
γmax,
γmin ,
(44)
20
Published as a conference paper at ICLR 2022
2( 2 X2 I T R2∖	∖c∕ 八 ιVc∕E∖
where YmaX = -(—K+-), and Ymin = U+γK2). Therefore, We have
..	~■	^C ~'~c.
(Λ) = λρ(δ) + eρ(δ)	and 勺(A) = 7(λδ + eδ )
Ymin	12κ2γK2 ,	YmaX	K
Thus, we can select η = (	1 ∩= )2, and ∣A(β) ∣∣2 can be bounded by
，	'√γ 您X+√ 魔M..............
(45)
嗽…∣2 <1 _ √(岑丁)/(2 •
1	μ电 S)
=1--.	—,
,168κ2γK
7(λδ2 + eδ2)
K
(46)
where μ(δ, δ)=
λo(δ)+To(S) )1/2
λδ2 + λ(52 )
From Lemma 10, we have
.ʌ √∙2	r-n~~.	TA2	In....
∣v∕(W('* - v∕(W(',t))∣∣2 =(Irʌ/ɪNgq + λKr∖pM^) ∙ ∣W('，')_ W*∣2
~「〜C	l------
+ λK (V dMr+1 )∙∣w (',0) - w*∣2.
Given ε > 0 and ε > 0 with ε + ε < 1, let
λδ2 /d log q	εμ(δ, S)
K K N Nt √ √168κ2γK,
--	-TT-- , _	≈,
dlog q V	εμ(δ, δ)
Mt √ √168κ2 YK，
and
where we need
Nt ≥ ε-2μ-2( ∖χ2λδ ∖δ2 )2κ2YK3dlog q,
λδ2 + λδ2
eδ2
and Mt ≥ ε-2μ-2(--------
—	vλδ2 + λδ2
)2κ2γK 3d log q.
(47)
(48)
(49)
Therefore, from (46), (47) and (48), we have
∣∣W('，t+1) _ W*∣2
.	(-I	_	~∖ / C CA .
< (1 _ ( _pi-^ ) k W^)- W* 12 + 小
'	M168κ2γK	J
dMF+1 )•“
(2,0)
—
W *∣∣2
/ 门 L c ,/X 入、	e^2
<(1 _ (	_ 2”(Z)) ∣W(M- W*∣∣2 + η ∙ ɪIW(',0) - W*∣∣2
'	yz168κ2γK	J	K
(50)
when M ≥ 4d log q. By mathematical induction on (50) over t, we have
∣∣W(',t) _ W*∣∣2
<fι _ (I-ε —可〃Y
+
,168κ2γK
√168κ2γK
.kW(40) _ W*∣∣2
√K
(I — ε 一 ε)μ 14(λδ2 + λJ2)
<〕(-¾⅛ )t + (1T
•
K
√K¼λδ2
，('，0) _ W*∣∣2
ε - ε)μ(λδ2 + λδ2)J
.kW(40) _ W*∣∣2
(51)
21
Published as a conference paper at ICLR 2022
By mathematical induction on (51) over `, we have
∣∣W(',T) - W*∣∣2
≤[(ι-(I-ε-分μ)T +------------P2γe	~~ ]'∙∣W(0,0)-W*∣2	(52)
['	,168κ2γK)	(1 — ε — ε)μ(λδ2 + λδ2)J
□
F Proof of Theorem 1
Instead of proving Theorem 1, we turn to prove a stronger version, as shown in Theorem 3. One can
verify that Theorem 1 is a special case of Theorem 3 by selecting λin the order of p and εeis in the
order of (2p — 1).
The major idea in proving Theorem 3 is similar to that of Theorem 2. The first step is to characterize
the gradient descent term on the population risk function by the MVT in Lemma 8 as shown in
(58) and (59). Then, the connection between ∣∣ W('+1,0) 一 W[p] ∣f and ∣∣ W(',0) 一 W[p] ∣f are
characterized in (64). Compared with proving Theorem 2, where the induction over ` holds naturally
with large size of labeled data, the induction over ` requires a proper value ofp as shown in (69). By
induction over ` on (64), the relative error ∣W (L,0) 一 W[p] ∣F can be characterized by ∣W (0,0) 一
W[p] ∣F as shown in (71).
Theorem 3. Suppose the initialization W (0,0) satisfies with
. ʌ.
|p 一 λ∣ ≤
2(1 - e)p 一 ι
~μ√K -
for some constant εe∈ (0, 1/2), where
λδ2	_( N
λδ2 + λδ2	κ2γK 3μ2d log q
and
μ = μ(δ, δ)=
λδ2 + eδ2
λρ(δ) + λρ(δ)
Then, if the number of samples in D further satisfies
M & ε-2κ2γμ2(l — *)2K3dlogq,
(53)
(54)
(55)
(56)
the iterates {W(',t)}Lt=O converge to W[p] with P satisfies (53) as
lim ∣∣W(',T) — W[p]∣2
T→∞
≤ι-ε ∙(1 — P* + μ√KI(λ — p*)∣) ∙ ∣W(0,0) - W*∣∣2 + (ʌɔ ∙∣W(',0) — W[p]∣2,
with probability at least 1 — q-d.
Proof of Theorem 3. From Algorithm 1, in the `-th outer loop, we have
W(',t+1) =W(',t) — NfDt τet(W(',t)) + β(W(',t) — W(',t-D)
=W(',t) 一 ηVf(W(',t)) + β(W(',t) 一 W(',t-1))
+ η ∙ (Vf(W(',t)) — VfDt,Det(W(',t)))
Since Vf is a smooth function and W [p] is a local (global) optimal to f, then we have
Vf (W (',t)) =Vf (W (',t)) — Vf (W [p])
=/1 V2f (w(',t) + U ∙ (W(',t) — W[p]))du ∙ (W(',t) — W[p]),
(57)
(58)
(59)
22
Published as a conference paper at ICLR 2022
where the last equality comes from Lemma 8.
Similar to the proof of Theorem 2, we have
kW('H1)-W叫2 ≤ kA(β)l∣2∙∣∣W('，t)-w叫2+η∙∣∣v∕(w('%-vft,Dt(W化力心.(60)
From Lemma 2, we have
	∣∣v∕(W(',t)) -Vf(W(',t))∣2 ʌ c 2	In...	TA 2	Γ~H~~ <irʌ/ɪNgq ∙ ∣W(e,t) - W*k +	Mgq ∙ ∣W('，t)- W('，0)||2	(61) ι ∣λδ2 ∙ (W(O，0)- W叫-eδ2 ∙ (W* - W叫 ∣ +	K
When '	=0, following the similar steps from (41) to (46), we have ∣∣V∕(W(',t)) - Vf(W(',t))∣2 ʌ c 2	Γ-H~~	TA 2	In... < IrENgq ∙ ∣ W(M- W[pi k + λK■启F ∙ k WC,t) - W叫 2 + λδ2 rdl0gq ∙ ∣W* - W[叫 + / Jd^q ∙ k W(O，0) - W叫12	(62) K	Nt	K	Mt ∣λδ2 ∙ (1 — p) — eδ2 ∙ p ∣	m∩∖ + J			U	∙ ∣∣W(0,0) - W*∣∣2 K
and	∣∣W('，t+1)- W㈤∣∣2 ≤(1 - M 乃1 -1 2 N) ∙ ∣W(',t) - W叫∣2 '	μ(δ, δ) ʌ/ 154κ2γK) + η∙(λδ⅛J^ + ∣ λδ2. (I-K)-赌.p∣ ) ∙ ∣wa。)- w*∣2	(63) + η ∙ IeP ∙ Jd^q ∣W (0,0)-W*∣2. K	Mt
Therefore, we have
lim ∣∣W('，T) - W叫2
T →∞
μ -∖∕154κ2 γK
≤	1-ε ∙η∙
[(λδ2(1 - p)
Id logq
I λδ2 ∙ (1 — p) — λδ2 ∙ PI
∙ ∣w(0,0)- W*∣2
+
K
+ ελδ2∙jp ∙ Jdl0gq∙∣w(OQ- w*∣∣2]
K V Mt	」
μp154κ2γK	K	Iyλδ2(1 -p) Idlogq	∣λδ2 ∙ (1 -p) - λδ2 ∙P∣∖
≤	1 - ε ∙ 14(λδ2 + λδ2) .K K N Nt +	K	)
∙ kW(O，0)- W*∣2 + ελδ2∙^ ∙ I∣d用∙ ∣W(O，0)- W*∣∣2]
K	Mt
'三∙ (1 - P + √K ∙ ∣ (1 - p)μλ - pμ(1 - λ) ∣) ∙∣W (OQ- W *∣2
+产W ∙∣w(OQ- W*∣∣2
(1 - ε)
=士 ∙ (1 -P + μ√K∣ λ -P∣ ) ∙ ∣W(O,0) - W*∣2 +	∙ ∣W(O,0) - W*∣2,
1 — ε ∖	)	(1 — ε)
(64)
where λ
λδ2
λδ2+eS2.
23
Published as a conference paper at ICLR 2022
To guarantee the convergence in the outer loop, we require
lim ∣∣W('，T) - W[p]∣∣2 ≤ ∣∣W(0,0) - W[p]k2 = PkW(OQ - W*∣∣2,
T→∞
and lim ∣∣W('，T) - W*∣2 ≤ ∣W(O，0) - W*∣2.
T→∞
Since we have
∣∣W('，T) - W[p]∣2 ≤∣W('，T) - W*∣2 + ∣∣W* - W[p]∣∣2
=∣W('，T) - W*∣2 + (1 -p) ∙ ∣W* - W(0,0)∣2,
it is clear that (65) holds if and only if
ɪ-ɪ .(1 - P + εp + μ√KR - Pl) + 1 - P ≤ 1.
To guarantee the iterates strictly converges to the desired point, we let
1 〜∙(1 - P + eP + μ√KR - P∣) + 1 - P ≤ 1 - ɪ
1-ε	C
for some larger constant C, which is equivalent to
. ʌ .
|p - N ≤
2(1 - εe)p - 1
μ√K
To make the bound in (69) meaningful, we need
1
P ≥ 2(1 - e).
When ` > 1, following similar steps in (64), we have
lim IIW('，T) - W[p]∣2
T→∞
≤ ɪ ∙(1 - P + μ√K ∣(λ - p)∣) ∙ ∣W (0,0) - W *∣2 + ɪ ∙ ∣W ('，0) - W [p]∣2,
1-ε	1-ε
Given (69) holds, from (71), we have
lim ∣W(L,T) - W[p] ∣2
L→∞,T →∞
≤ 14^ ∙(1 - P + μ√K ∣λ - P∣) ∙∣W(0,0)- W *∣2
≤ 14^ ∙(1 - P + μ√K ∣λ - p∣) ∙∣W (0,0) - W *∣2.
(65)
(66)
(67)
(68)
(69)
(70)
(71)
(72)
□
G DEFINITION AND RELATIVE PROOFS OF ρ
In this section, the formal definition ofρ is included in Definition 3, and a corresponding claim about
ρ is summarized in Lemma 11. One can quickly check that the ReLU activation function satisfies
the conditions in Lemma 11.
The major idea in proving Lemma 11 is to show Hr(δ) and Jr(δ) in Definition 3 are in the order of
δr when δ is small.
Definition 3. Let Hr(δ) = Ez〜N(0炉)(φ0(σκz)zr) and Jr(δ) = Ez〜N(0炉)(φ02(σκz)zr). Then,
ρ = ρ(δ) is defined as
ρ(δ) = min { J0(δ) - H2(δ) - Hf(δ), J2(δ) - Hf(δ) - H2(δ), H0(δ) ∙ H2(δ) - H2(δ)}, (73)
where σK is the minimal singular value of W*.
24
Published as a conference paper at ICLR 2022
Lemma 11 (Order analysis of ρ). If ρ(δ) > 0 for δ ∈ (0, ξ) for some positive constant ξ and the
sub-gradient of ρ(δ) at 0 can be non-zero, then ρ(δ) = Θ(δ2) when δ → 0+. Typically, for ReLU
activation function, μ in (5) is a fixed Constantfor all δ,δ ≤ L
ProofofLemma 11. From Definition 3, We know that Hr(δ) = Ez〜N(。炉)φ0(σκz)zr. Suppose
we have Hr(δ) = Θ(δr) and Jr(δ) = Θ(δr), then from (73) we have
J0(δ) - H02(δ) - H12(δ) ∈ Θ(1) - Θ(δ2),
J2(δ) - H12(δ) - H22(δ) ∈ Θ(δ2),	(74)
Ho(δ) ∙ H2(δ) - H2(δ) ∈ Θ(δ2) - Θ(δ4).
Because ρ is a continuous function with ρ(z) > 0 for some z > 0. Therefore, ρ 6= J0(δ) - H02(δ) -
H12(δ) when δ → 0+, otherwise ρ(z) < 0 for any z > 0. When δ → 0+, both J2(δ) - H12(δ) -
H2(δ) and Ho(δ) ∙ H2(δ) - H2(δ) are in the order of δ2, which indicates that μ is a fixed constant
when both δ and δ are close to 0. In addition, J2(δ) - H2 (δ) - H2(δ) goes to +∞ while both
J0(δ) - H2(δ) - HIM and Ho(δ) ∙ H2(δ) - H2(δ) go to -∞ when δ → +∞. Therefore, with a
large enough δ, we have
ρ(δ) ∈ Θ(δ2) - Θ(δ4)	or Θ(1) - Θ(δ2),	(75)
which indicates that μ is a strictly decreasing function when δ and δ are large enough.
Next, we provide the conditions that guarantee Hr(δ) = Θ(δr) hold, and the relative proof for Jr(δ)
can be derived accordingly following the similar steps as well. From Definition 3, we have
lim增
δ→0+ δr
lim
δ→0+
(=a) lim
δ→0+
Z+∞	1	2
φ(σκz)(ʌ) kXe~δ2 dz
∞	δ 2πδ
「 φ0(σκδt);e-t2dt
-∞	2π
0-	r	+∞	r	(76)
= lim	φ0(σκδt)-=e~t dt + lim	φ0(σκδt)-=e~t dt
δ→o+ J-∞	√2∏	δ→o+ √o+	'	' √2∏
Z0- tr	+∞ tr
√= e-t dt + φo(0+) J	√= e-t dt,
where equality (a) holds by letting t = Z. It is easy to verify that
+∞
0+
(-I)' / -tr e-t2 dt,
-∞	2π
and both are bounded for a fixed r. Thus, as long as either φ0(0-) or φ0(0+) is non-zero, we have
Hr(δ) = Θ(δr) when δ → 0+.
If φ has bounded gradient as ∣φ0∣ ≤ Cφ for some positive constant Cφ. Then, we have
H^=IZr
1	- z2 ，
I__ e δ2 dz
√2∏δ
φ0(σκδt) I-e-t dt
2π
≤Cφ ∙
(77)
Z+∞
∞
Therefore, we have Hr(δ) = O(δr) for all δ > 0 when φ has bounded gradient.
Typiclly, for ReLU function, one can directly calculate that Hr(δ) = δr for δ ∈ R, and ρ(δ) = Cδ2
when δ ≤ 1 for some constant C = 0.091. Then, it is easy to check that μ is a constant when
δ,δ ≤ 1.	□
25
Published as a conference paper at ICLR 2022
H Proof of preliminary lemmas
H.1 Proof of Lemma 1
The eigenvalues of V2f (∙;P) at any fixed point W can be bounded in the form of (80) by Weyrs
inequality (Lemma 4). Therefore, the primary technical challenge lies in bounding ∣∣V2f (W;P) -
V2f(W[p];p)k2, which is summarized in Lemma 12. Lemma 13 provides the exact calulation of
the lower bound of Ex PjK=1 αjTxφ0(wj[p]T x) when x belongs to Gaussian distribution with
zero mean, which is used in proving the lower bound of the Hessian matrix in (81).
Lemma 12. Let f(W; P) be the population risk function defined in (17) with P and W satisfying
(20). Then, we have
kV2f(W[p];p)-V2f(W;p)∣2 . λδ2 +(1 - λ)δ2 ∙ kW[p] - Wk2.
K	σK
Lemma 13 (Lemma D.6, (Zhong et al., 2017)). For any {wj}jK=1 ∈ Rd, let α ∈ RdK be the unit
vector defined in (19). When the φ is ReLU function, we have
K2
min]Ex〜N(0,σ2) (X αjχφ0(WTX)) & p(σ),	(79)
kαk2 =1	j=1
where ρ(σ) is defined in Definition 3.
Proof of Lemma 1. Let λmax(W) and λmin(W) denote the largest and smallest eigenvalues of
V2f(W; P) at point W, respectively. Then, from Lemma 4, we have
λmax(W) ≤ λmax(W[p]) + ∣V2f (W ； P)- V2f (W [p] ； p)∣2,
λmin(W ) ≥ λmin(W [p])-∣V2 f (W ； P)- V2f(W [p] ； p)∣2.
Then, we provide the lower bound of the Hessian matrix of the population function at W[p] . For
any α ∈ RdK defined in (19) with ∣α∣2 = 1, we have
min^aτ V2 f (W [p]; p)α
K	2K	2
=-^2 min	λEx XαjTxφ0(wj[p]Tx) 2 + λeExe X αjT xeφ0(wj[p]T xe) 2
kαk2 =1	j=1	j=1
1K	2	K	2
≥ K kmi=1λEχ( X α xφ0(Wjp x)) + kmi=1eEχ( X α eφ0 (Wpe))
2	j=1	2	j=1
≥ λρ(δ) + λρ(δ)
—11κ2γK2	，
where the last inequality comes from Lemma 13.
Next, the upper bound can be bounded as
(81)
max αTV2f(W[p] ; P)α
1	K	2K	2
K max [λEχ (X ατxφ0(wjp x)) + λEχ( X ατeφ0(wjp ef)]
kαk2=1	j=1	j=1
1K	2	K	2
≤ K maxJEχ (X ατ xφ0 (W y x)) + maxJEχ (X ατ eφ0(w y e))
kαk2=1	j=1	kαk2=1	j=1
(82)
26
Published as a conference paper at ICLR 2022
For Ex PjK=1 αjTxφ0(wj[p]Tx) , we have
K2
Ex X αjTxφ0(wjp x)
j=1
KK
=Ex X X αjT1xφ0(wj[p1]Tx)αjT2xφ0(wj[p]Tx)
j1=1 j2=1
KK
= X X ExαjT1xφ0(wj[p1]Tx)αjT2xφ0(wj[p2]Tx)
j1=1 j2=1
K K	1/4
≤XX Ex(αjT1x)4Ex (φ0(wj[p1]Tx))4Ex (αjT2x)4Ex (φ0(wj[p2]Tx))4
j1=1 j2=1
KK
≤XX3δ2kαj1k2kαj2k2
j1=1 j2=1
KK
≤6δ2 XX 1(kɑjιk2 + |d2 k2)
j1=1 j2=1
=6Kδ2
Therefore, we have
max∣αT V2f(W [p]; p)α
1	K	[ ]T 2
≤K kma=1λEχ(XαTxφ0(Wjp	X))
2	j=1
(83)
(84)
<6(λδ2 + eJ2)
K
Then, given (20), we have
kW'nW叫F= PkW(nW**.尚∙
(85)
Combining (85) and Lemma 12, we have
kV2f (W ； P) —V2 f (W [p]; P)k2 . W 2 /2 ) .
κγ
(86)
Therefore, (86) and (80) completes the whole proof.
□
H.2 Proof of Lemma 2
The task of bounding of the quantity between ∣∣V∕ -Vf k2 is dividing into bounding Iι, I2, I3 and
I4 as shown in (89). I1 and I3 represent the deviation of the mean of several random variables to
their expectation, which can be bounded through concentration inequality, i.e, Chernoff bound. I2
and I4 come from the inconsistency of the output label y and pseudo label ye in the empirical risk
function in (1) and population risk function in (17). The major challenge lies in characterizing the
upper bound ofI2 and I4 as the linear function of Wf - W[p] and W[p] - W*, which is summarized
in (96).
27
Published as a conference paper at ICLR 2022
Proof of Lemma 2. From (1), we know that
∂f	λ三八三 一	、 一λ M /、三 —	、
∂Wk (W )= N X ( K X φwj χn) - yn) χn + ɪ X (K X φwj Xm)- Q Xm
k	n=1 j=1	m=1 j=1
λ NK
=K2N X X (°(WTχn) - °(W；TXn)) χn
n=1 j=1
1 λMK
+ KM X X (°(wTem) - °(WTXm)) em.
m=1 j=1
(87)
From (32), we know that
KK
∂W(W)=K2 Ex X (φ(wτ χ)- φ(w;TX))X+-Kr Ex X (φ(wτ e)- φ(wτ e))χ.
wk	j=1	j=1
(88)
Then, from (17), we have
_ ʌ _
∂f 皿 ∂f
∂Wk(W)-∂Wk(W; Pp
KN
=K2N X [X (Φ(wTXn)- φ(wjτXn))Xn - Eχ (φ(wTX)- φ(wjp]Tx))x]
j=1
1 - λ
+ K1 2M
λK1N
K2 X [n X (Φ(wtXn) - Φ(Wj Xn))Xn - Ex (φ(wjf x) - φ(wj x))x]
j=1	n=1
λK
+ K2 X Ex[(φ(wJTX)- φ(wjp X))Xl
j=1
1	λK 1M
+ ~K~ X [M X (φ(w^τXm) - φ(wjτXm))Xm - Eχ(φ(wjfx) - φ(wjx))x]
(89)
j=1	m=1
1 λK
+ -K~ XEx [(Φ(wTx) - φ(wjp (p)x))x]
K j=1
:=I1 + I2 + I3 + I4 .
For any αj ∈ Rd with ∣∣αj ∣∣2 ≤ 1, we define a random variable Z (j) = (φ(wT x)-φ(w*τ x)) OT X
and Zn(j) = (φ(wjT Xn) - φ(wjτ Xn)) O Xn as the realization of Z (j) for n = 1,2 …，N. Then,
for any P ∈ N+ , we have
(E|Z|p)1/p =(E∣φ(wTx) - φ(w*τx)∣p ∙ ∣αTx∣p) /
≤(E∣(wj- w；)TXIp ∙ |aTx∣p) /「
(90)
≤C ∙ δ2k Wj - w*k2 ∙ P,
where C is a positive constant and the last inequality holds since X 〜N(0, δ2). From Definition 2,
we know that Z belongs to sub-exponential distribution with ∣∣Z∣∣ψι . δ2∣∣Wj - wj∣2. Therefore,
by Chernoff inequality, we have
Zn(j) - EZ (j) < t ≤ 1 -
eNst
(91)
28
Published as a conference paper at ICLR 2022
for some positive constant C and any s ∈ R.
Let t = δ2kWj - w；k2 JdlNq and S = 062口_2__*心∙ t for some large constant q > 0, we have
I： XX Zn(j) - EZ(j)| . δ2kWj - Wjk2 ∙ rdNgq	(92)
n=1
with probability at least 1- q-d. From Lemma 7, we have
1N
U N X (Φ(wTXn)- φ(wjτXn))xn - Eχ (φ(wTX)- 0(W；Tx))x(
n=1	2	(93)
≤2δ2kwj - wjjk2 ∙ rdNgq
with probability at least 1 - (q/5)-d. Since q is a large constant, we release the probability as
1- q-d for simplification. Similar to Z, we have
U1M
U M X (Φ(wTXm) - Φ(wTXm))Xm - Eχ(φ(WTx) - Φ(wTx))x
m=1
2
.δ2kwj - Wjk 2 ∙ ↑jdMgq
(94)
with probability at least 1- q-d.
For term Ex
Figure 13: The subspace spanned by Wj； and Wj[p]
h(φ(w*τ x) - φ(w/]Tx))Xi
[p]
, let us define the angle between Wj； and Wj
as θ1 . Figure
13 shows the subspace spanned by the vector Wj； and Wej. We divide the subspace by 4 pieces, where
the gray region denotes area I, and the blue area denotes area II. Areas III and IV are the symmetries
of II and I from the origin, respectively. Hence, we have
Ex [(φ(w*τx) - φ(w?]TX))x]
=Eχ∈ areaI [(φ(w)Tx) - φ(w/TX))Xi + Eχ∈ areaII [(φ(w)Tx) - φ(w/]TX))Xi
+ Ex∈ areaIII [(φ(WjTX) - φ(w∕ X))Xi + Ex∈ area IV[(0(w；TX)- φ(Wjp]TX))Xi
=Eχ∈ areaI [(0(w；Tx) - Φ(w∕Tx))x] + Eχ∈ areaII [w；Tee] - Eχ∈ areaIII [w?]Txx]
=Ex∈ area I (Wj； - Wj[p])TXX + Ex∈ area II (Wj； - Wj[p])TXX
(95)
29
Published as a conference paper at ICLR 2022
Therefore, we have
Kλ2Eχ[(φ(w;Tx) - φ(wjp]τx))x] + 1--2λEx [(φ(WTe) - φ(wjp]τe))ei U
=2Kκ2Ex [(w； - wjp] )Txx] + 12-λEx [(Wj- wjp])Txx]	(96)
2K	2K	2
Jlλδ2 ∙ (Wj- wjp] ) + (I- λ)δ2 ∙ (W； - Wjp)U2
=	2K2	.
From (93), (94) and (96), we have
Uf (W;P)- f (W)U
∂Wk	∂Wk 2
KN
≤ K X Il N X (φ(wj Xn)-
j =1 n=1
'jTXn))Xn - Eχ(φ(wTX)- Φ(W尸X))x||2
1- λ
+-----
+ K2
-EX(Φ(wTe) - Φ(WTX))e∣∣2
K
+X
j=1
Kλ2 Eχ[(Φ(w;T χ) - Φ(wjp]T X))Xi + 1κ2λ Ex[(Φ(wT e) - Φ(wjp]T e))e
(97)
≤Kδ2rdNp ∙ X kwj- wjk2 +
j=1
1Kλ ∙ δ2rdMl ∙ X IM- Wjk2
j=1
K
+2κ2 ∙ X ∣lλδ2 ∙ (Wj- wjp])+eδ2 ∙ (w;- wjp]) ∣∣2
j=1
≤k⅛δF^ ∙kW - W*k2+⅛2 ∙ δ2rdMq ∙kW -Wk
+ 2⅛∣∣λδ2 ∙ (W - Wm) + (1- λ)32 ∙ (w *-W[p]) ∣∣2
with probability at least 1 - q-d.
In conclusion, let α ∈ RKd and αj ∈ Rd with a = [αT, αT,…，aTK]t, We have
kVf(W) -V∕(W )k2 =kT(Vf(W )-Vf(W ))∣
K
≤ XIaT (∂wk(W)-菽(W))l
^^
.Xn ∂wk (W)- ∂wk (W "2 ∙kak k2
."J VF ∙ kW - W *k2 + ⅛λ ∙ δ2rd0gq ∙kW - W k2
KN	K	M
+ 2K∣∣λδ2 ∙ (W - W[p]) + (1 - λ)δ2 ∙ (W* - W叫 ∣∣2
(98)
with probability at least 1 - q-d.
□
H.3 Proof of Lemma 12
The distance of the second order derivatives of the population risk function f (∙; P) at point W and
W[p] can be converted into bounding P1, P2, P3 and P4, which are defined in (101). The major
30
Published as a conference paper at ICLR 2022
idea in proving P1 is to connect the error bound to the angle between W and W [p] . Similar ideas
apply in bounding the other three items as well.
Proof of Lemma 12. From (17), we have
ɑ d f— (W[p];P) = λ2EEχφ0(wjp]Tx)φ0(wjp]TX)XxT + 1-2λEχφ0(wjp]Te)φ0(wjp]τe)eeτ,
∂wj1 ∂wj2	K2	j1	j2	K2	j1	j2
(99)
and
∂2f	λ
∂W-∂W-(W; Pp= K2 Eχφ (WIx)φ (W2X)Xx +
1--2λ Ex φ0(wτe)φ0(wT2 e)eeτ,
(100)
where Wj[p] is the j-th column of W[p] . Then, we have
∂2f	∂2f
f(W *) - f(W)
∂Wj1∂Wj2	∂Wj1∂Wj2
=KEx [φ0(wjP]TX)φ0(Wjp]Tx) - Φ0(wTx)Φ0(WTx)]xxt
+ 1κ2λEχ∣φ0(wjp]τe)φ0(WjP]Te) - Φ'(wJ1X)Φ'(wTe)] eeT
=Kλ2 ExhO'Mp" x)(Φ' (Wjp]T x) — Φ'(wTx)) + Φ'(wT x)(Φ0(wjp]T x) — φ0(wTx))ixxτ
+ 1κ-2λEχ[φ0(wjp]τe)(φ0(wjp]τX) - φ'(wTe)) + φ0(wTe)(φ'(Wjp]Te) - Φ0(wTe))ieeτ
=K ∣Eχφ0(wjp]τx)(φ0(Wjp]Tx) - φ0(wTx))xxt + Exφ0(wT2x)(φ0(wjρ]τx) - φ0(wTx))XxTi
+ 1κ-2λ 忸xφ0(Wjp]Te)(φ0(Wjp]TX) - Φ,(WT2e))eeτ + Exφ0(WTe)(φ'(Wjp]TX) - Φ0(WTe))eeτi
：=K (Pi + P2) + ⅛λ (P3 + P4).
K2	K2
(101)
For any a ∈ Rd with kak2 = 1, we have
aTPia =Exφ0(Wjp]τχ)(φ0(w"Tx)- φ0(WTx))(aτx)2	(102)
where a ∈ Rd. Let I = φ0(Wjp]T x)(φ0(Wjp]T x) - φ0(WT x)) ∙ (aT x)2. Itis easy to verify there ex-
ists a group of orthonormal vectors such that B = {a, b, c, a⊥,…，a⊥} with {a, b, c} spans a sub-
space that contains a, Wj2 and w*2 . Then, for any x, We have a unique Z = 卜， z2, …， zd]
such that
x = zia + z2b + z3c +-----+ zda⊥.
Also, since X 〜N(0, δ2Id), we have Z 〜N(0, δ2Id). Then, we have
I =Ezi,z2,z3|”(WTx) - ”(Wjp]Tx)|・ IaTx|2
/ ∣φ0(WTx) - φ0(Wjp]Tx)| ∙ IaTx∣2 ∙ fz(z1,z2,z3)dz1dz2dz3,
where x = zia + z2b + z3c andfZ(zi, z2, z3) is probability density function of (zi, z2, z3). Next,
we consider spherical coordinates with zi = Rcosφi, z2 = Rsinφisinφ2, z3 = Rsinφicosφ2.
Hence,
I=J WWTx)
—Φ0(Wjp]Tx) I ∙ |Rcos φι ∣2 ∙ fz(R, φι, φ2)R2 Sin φ1dRdφ1dφ2.
(103)
It is easy to verify that φ0 WjT x only depends on the direction of x and
fz (R, φi, φ2)
1	- z2+z2+z2
-------eβ	2δ2
(2πδ2) 2
1	- R2
-------eC 2δ2
(2πδ2) 2
31
Published as a conference paper at ICLR 2022
only depends on R. Then, we have
I(i2, j2)
=/ IΦ0(wjT(x/R)) - φ0(wjp]τ(χ∕R))∣∙ |Rcosφιl2 ∙ fz(R)R2sin φ1 dRdφ1 dφ2
∞
=	R4fz (R)dR
0
(a)	∞
≤ 3δ2 ∙ /	R2fz(R
0
= 3δ2 ∙ Ezι,z2,z3∣φ0(wTχ) - φ0(wjp]Tχ)1
≤3δ2 ∙ Eχ∣φ0(WTx) - Φ'(wjP]Tx)|,
where the inequality (a) is derived from the fact that ∣cosφι∣ ≤ 1 and
—
广 R4，e-悬dR = Γ -Hζd(e-R2)
Jo	(2πδ2) 2	Jo	(2πδ2) 2
∞ -R2 R R3δ2
= e 2δ2 d-------3
Jo	(2πδ2) 2
∞	1 R2
=3δ2	R2------3 e-2δ2 dR.
Jo	(2πδ2) 2
(104)
(105)
Define a set A1 = {x|(wj[p]Tx)(wjTx) < 0}. If x ∈ A1, then wj[p]T x and wjTx have different
signs, which means the value of φ0(wjT x) and φ0(wj[p]Tx) are different. This is equivalent to say
that
lφ0 (WT2X)- φ0(Wp]Tx)1 = { 1, if X ∈ AC.	(106)
, x∈	1
Moreover, if x ∈ A1 , then we have
Wjp]Tx| ≤Wjp]Tχ - wTx| ≤ l∣wjp] - Wj21∣2 ∙ kχ∣∣2.	(107)
Let us define a set A2 such that
δ _( ∣ lwjp]Tx|	Vk%-wj21∣2∖ _ rθ ∣l θ Ivkwjp] - W ∣∣2∖ π08
A2 ={ xlkw*2 k2kxk2 ≤	kw*2 k2	} = {θx,w"1 cos θx,wjp]1 ≤	kwjp]k2	}
Hence, we have that
Eχ∣φ0(wT2X)- φ0(wjp]τx)∣2 =Eχ∣φ0(wT2X)- φ0(wjp]τx)|
=Prob(x ∈ A1)	(109)
≤Prob(X ∈ A2 ).
Since X 〜N(0,δ2kak22I),θx [p] belongs to the uniform distribution on [-π, π], we have
x,wj2
π - arccos
Prob(X ∈ A2 )
kwjp]-wj2 k2
kwjp] k2
≤1tan(π - arccos M⅛业)
F	kwjp]k2
π
arccos
π
kwj[p] - wj2 k2 )
kwjp]k2
≤ 2 kwj[p] - wj2k2
F	kwjp]k2
(110)
32
Published as a conference paper at ICLR 2022
Hence, (104) and (110) suggest that
I≤
6δI 2 kwj2 - Wjp]k 2
π σK
kak22.
(111)
The same bound that shown in (111) holds for P2 as well.
P3 and 马 satisfy (111) except for changing δ2 to δ2.
Therefore, we have
kV2f(W[p]; p)-v2f (W; p)k2
= max IaT (V2f(W [p]; P) — V2f(W; p))ɑ∣
kαk2≤1
≤ XX XX aT (Irf (W [p]; P)- Irf (W; p))a,2
j =1 j =1 ∣ j1 ∂wj1∂wj2	∂wj1∂wj2	2 ∣
KK
≤K XX (λ∣∣Pι + P2 k2 + (1- λ)∣∣P3 + P4k2) kaji k2gj∙2 k2
j1=1 j2=1
≤K XX XX 4(λδ2 + (1 - λ)δ2) kwjp]- Wjk2 gj1k2kaj2k2
j1=1 j2=1	σK
≤ɪ M + (1- λ)δ2) .nWk2 ,
K	σK
where a ∈ RKd and aj ∈ Rd with a = [aT, OT,…,aTK]t.
(112)
I Initialization via tensor method
In this section, we briefly summarize the tensor initialization in (Zhong et al., 2017) by studying the
target function class as
1K
y = KfVj Φ(wjTχ),	(113)
Kj=1
where Vj ∈ R. Note that for ReLU function, We have vjφ(wjτx) = Sign(Vj)φ(∣vj∣wjτx).
Without loss of generalization, we can assume Vjj ∈ {+1, -1}. Additionally, it is clear that the
function studied in (2) is the special case of (113) when Vjj = 1 for all j. In addition, Theorem 5.6
in (Zhong et al., 2017) show that the sign of Vjj can be directly recovered using tensor initialization,
which indicates the the equivalence of (2) and (113) when using tensor initialization.
We first define some high order momenta in the following way:
M1 = Ex{yx} ∈ Rd,	(114)
M2 = Ex [y(x 乳 X - δ2I)i ∈ Rd×d,	(115)
M3 = Exhy(X183 - x0δ2I)i ∈ Rd×d×d,	(116)
where Ex is the expectation over X and z83 := Z 0 Z 0 z. The operator 0 is defined as
d2
v0eZ =	(v	0	zi	0 zi	+	zi	0 v 0 zi +	zi	0 zi 0	v),	(117)
i=1
for any vector v ∈ Rd1 and Z ∈ Rd1 ×d2 .
33
Published as a conference paper at ICLR 2022
Following the same calculation formulas in the Claim 5.2 (Zhong et al., 2017), there exist some
known constants ψi , i = 1, 2, 3, such that
K
Ml = XΨι ∙kw*k2 ∙ wj,	(118)
j=1
K
M2 = X Ψ2 ∙ ∣∣wj∣2 ∙ WjWjT,	(119)
j=1
K
M3 = X ψ3 ∙kWj k2 ∙ Wj03,	(120)
j=1
where Wj = wj∕∣∣wj ∣∣2 in (114)-(116) is the normalization of wj. Therefore, we can see that the
information of {Wjj}jK=1 are separated as the direction of Wj and the magnitude of Wj in M1, M2
and M3 .
Mi, M2 and M3 can be estimated through the samples {(xn,yn)}N=j, and let Mi, M2, M3
denote the corresponding estimates. First, we will decompose the rank-K tensor M3 and obtain the
{Wj }K=ι. By applying the tensor decomposition method (Kuleshov et al., 2015) to M3,the outputs,
j . j 1	-≤^-j
denoted by Wj, are the estimations of {sjWj}储，where SjiS an unknown sign. Second, We will
estimate sj, vjj and ∣Wjj ∣2 through Mi and M2. Note that M2 does not contain the information of
sj because sj2 is always 1. Then, through solving the following two optimization problem:
^ ∙
αi = arg min
α1 ∈RK
^ ∙
α2 = arg min :
α2 ∈RK
K
IMI- Xψ1α1,jwwjl,
K
I M2 — X ψ2α2jWjW；t
j=i
(121)
The estimation of sj can be given as
ʌ	J___/ ^	/ ^	、
Sj = sign (αι,j ∕α2j).
Also, we know that |bij | is the estimation of ∣∣w* ∣∣ and
Vj = sign(bι,j /sj) = sign(b2,j )∙
Thus, W(0) is given as
[sign(S2,1)S1,1Wb；,	…，sign(S2,κ)aι,κWW%].
Subroutine 1 Tensor Initialization Method
1:
2:
3:
4:
5:
6:
7:
8:
Input: labeled data D = {(xn, yn)}nN=i;
Partition D into three disjoint subsets Di, D2, D3;
一一 一 ^ ^ .............. ................. . — — .
Calculate Mi, M2 following (114), (115) using Di, D2, respectively;
Obtain the estimate subspace V of M2;
一 一 ≤^≥ ,ʌ ʌ ʌ. 一 一 _
Calculate Mc3 (Vb , Vb , Vb ) through D3;
K
Obtain {sbj}jK=i via tensor decomposition method (Kuleshov et al., 2015) on M3(V , V , V );
Obtain αbi, αb2 by solving optimization problem (121);
Return: Wj(0) = sign(αb2,j)αbi,j Vb ubj and vj(0) = sign(αb2,j), j = 1, ..., K.
To reduce the computational complexity of tensor decomposition, one can
project Mc3 to a lower-
dimensional tensor (Zhong et al., 2017). The idea is to first estimate the subspace spanned by
{Wjj }jK=i , and let Vb denote the estimated subspace. Moreover, we have
M3(V,亿 V) = Exly((VTx)03 — (VTx)0Ex(VTX)(VTX)T)] ∈ Rk×k×k,	(122)
34
Published as a conference paper at ICLR 2022
Then, one can decompose the estimate Mc3(Vb, Vb, Vb) to obtain unit vectors {Sj}j=ι ∈ RK. Since
W* lies in the subspace V, We have VVTWj = wj. Then, VbSj is an estimate of wj. The
initialization process is summarized in Subroutine 1.
J Classification Problems
The frameWork in this paper is extendable to binary classification problem. For binary classification
problem, the output y given input x is defined as
Prob{y = 1} = g(W*; x)
(123)
With some ground truth parameter W *. To guarantee the output is Within [0, 1], the activation
function is often used as sigmoid. For classification, the loss function is cross-entropy, and the
objective function over labeled data D is defined as
fD (W ) = NN X	-yn log g(W ； Xn) - (1 -yn)lθg(1 -g(W ； Xn)).	(124)
(xn,yn)∈D
The expectation of objective function can be Written as
EDfD(W) =E(x,y) - y log(g(W; xn)) - (1 - y)log(1 - g(W; x))
=ExE(y|x) - y log(g(W; xn)) - (1 - y)log(1 - g(W; x))	(125)
=Ex h - g(W *; x) log(g(W; xn)) - (1 - g(W*; x)) log(1 - g(W; xn))i
Please note that (125) is exactly the same as (32) With λ = 1 When the loss function is squared loss.
For cross entropy loss function, the second order derivative of (125) is calculated as
∂ fD (W )	1	yn	1 - yn	0 T 0 T T
∂Wj∂Wk = N [+ (1- g(W; x))2 ] ∙ φ (Wjx)φ (Wk x)xx .	(126)
When j 6= k. Refer to (88) in (Fu et al., 2020) or (132) in (Zhang et al., 2020b), We have
yn(φ0(WjTx)φ0(WkTx))	φ0(WjTx)φ0(WkTx)	2
卜-----g2(WX)--------口2 引	g2(W； X)— 八2 ≤ K .	(127)
Following similar steps in (90), from Defintion 2, we know that OT ∂W∙∂W) ak belongs to the SUb-
exponential distribution. Therefore, similiar results for objective function With cross-entropy loss
can be established as well. One can check (Fu et al., 2020) or (Zhang et al., 2020b) for details.
K	One-hidden layer neural network with top layer weights
For a general one-hidden layer neural network, the output of the neural network is defined as
1K
g(W, v； x) = Kflvj Φ(wT x),	(128)
K j=1
where V = [v1,v2,…，VK ] ∈ RK. Then, the target function can be defined as
1K
y = g(W *, v* ； x) = KEvjΦ(w*Tx)	(129)
K j=1
for some unknown weights W* and v*.
In the following paragraphs, we will provide a short description for the equivalence of (129) and (2)
in theoretical analysis. Note that for ReLU functions, we have Vjφ(wjx) = Sign(Vj)φ(∣Vj WTx).
35
Published as a conference paper at ICLR 2022
Without loss of generalization, We can assume vj,vj ∈ {+1, -1} for all j ∈ [K]5. From Appendix
I, we know that the sign of v* can exactly estimated through tensor initialization. There, we can
focus on analysis the neural netWork in the form as
(130)
1K
g(W; χ) = K 工 VjΦ(wjχ).
Considering the objective function in (1) and population risk function in (17), we have
ʌ
2
λM
+ KM [X(φ(Wj Xm
m=1
)-Φ(Wj^m))^m - Eχ(φ(wTx) - φ(wjp]τe))ʃi H2
KN
X H K2N [X (φ(wTXn)- φ(wjTxn))Xn- Ex (φ(WTX)- φ(Wjp]TX))Xl
j=1	n=1
1λM	H
+ K2M [ X (Φ(WT Xm) - Φ(wj Xm))Xm - Eχ(φ(WT x) - φ(wj回 x))x] ||2 ,
m=1
(131)
which is exact the same as (89). Similar results can be derived for Lemma 12. Therefore, the
conclusions and proofs of Lemma 1 and Lemma 2 does not change at all.
Additionally, fixing the second-layer weights and only training the hidden layer is the state-of-the-
art practice in analyzing two-layer neural networks (Arora et al., 2019b;a; Allen-Zhu et al., 2019;
Safran & Shamir, 2018; Li & Liang, 2018; Brutzkus & Globerson, 2017; Oymak & Soltanolkotabi,
2018; Zhang et al., 2019). Additionally, as indicated in (Safran & Shamir, 2018), training a one-
hidden-layer neural network with all vj fixed as 1 has intractable many spurious local minima, which
indicates that training problem is not trivial.
5To see this, one can view |v*|w* as the new ground truth weights, and the goal for this paper is to recover
the new ground truth weights.
36