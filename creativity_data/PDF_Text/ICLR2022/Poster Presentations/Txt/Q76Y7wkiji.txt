Published as a conference paper at ICLR 2022
Boosting the Certified Robustness of
L-infinity Distance Nets
Bohang Zhang1	Du Jiang1	Di He1,2	Liwei Wang1,3
1 Key Laboratory of Machine Perception, MOE, School of Artificial Intelligence, Peking University
2 Microsoft Research 3International Center for Machine Learning Research, Peking University
zhangbohang@pku.edu.cn 1800013027@pku.edu.cn
di_he@pku.edu.cn wanglw@cis.pku.edu.cn
Ab stract
Recently, Zhang et al. (2021) developed a new neural network architecture based
on '∞-distance functions, which naturally possesses certified '∞ robustness by its
construction. Despite the novel design and theoretical foundation, so far the model
only achieved comparable performance to conventional networks. In this paper,
we make the following two contributions: (i) We demonstrate that '∞-distance
nets enjoy a fundamental advantage in certified robustness over conventional net-
works (under typical certification approaches); (ii) With an improved training pro-
cess We are able to significantly boost the certified accuracy of '∞-distance nets.
Our training approach largely alleviates the optimization problem that arose in the
previous training scheme, in particular, the unexpected large Lipschitz constant
due to the use of a crucial trick called `p -relaxation. The core of our training ap-
proach is a novel objective function that combines scaled cross-entropy loss and
clipped hinge loss with a decaying mixing coefficient. Experiments show that us-
ing the proposed training strategy, the certified accuracy of '∞-distance net can be
dramatically improved from 33.30% to 40.06% on CIFAR-10 ( = 8/255), mean-
while outperforming other approaches in this area by a large margin. Our results
clearly demonstrate the effectiveness and potential of '∞ -distance net for certified
robustness. Codes are available at https://github.com/zbh2047/L_inf-dist-net-v2.
1	Introduction
Modern neural networks, while achieving high accuracy on various tasks, are found to be vulnerable
to small, adversarially-chosen perturbations of the inputs (Szegedy et al., 2013; Biggio et al., 2013).
Given an image x correctly classified by a neural network, there often exists a small adversarial per-
turbation δ, such that the perturbed image x + δ looks indistinguishable to x, but fools the network
to predict an incorrect class with high confidence. Such vulnerability creates security concerns in
many real-world applications.
A large body of works has been developed to obtain robust classifiers. One line of works proposed
heuristic approaches that are empirically robust to particular attack methods, among which adver-
sarial training is the most successful approach (Goodfellow et al., 2014; Madry et al., 2017; Zhang
et al., 2019a). However, a variety of these heuristics have been subsequently broken by stronger and
adaptive attacks (Carlini & Wagner, 2017; Athalye et al., 2018; Uesato et al., 2018; Tramer et al.,
2020; Croce & Hein, 2020), and there are no formal guarantees whether the resulting model is truly
robust. This motivates another line of works that seeks certifiably robust classifiers whose prediction
is guaranteed to remain the same under all allowed perturbations. Representatives of this field use
convex relaxation (Wong & Kolter, 2018; Mirman et al., 2018; Gowal et al., 2018; Zhang et al.,
2020b) or randomized smoothing (Cohen et al., 2019; Salman et al., 2019a; Zhai et al., 2020; Yang
et al., 2020a). However, these approaches typically suffer from high computational cost, yet still
cannot achieve satisfactory results for commonly used '∞-norm perturbation scenario.
Recently, Zhang et al. (2021) proposed a fundamentally different approach by designing a new net-
work architecture called '∞-distance net, a name coming from its construction that the basic neuron
is defined as the '∞-distance function. Using the fact that any '∞-distance net is inherently a 1-
Lipschitz mapping, one can easily check whether the prediction is certifiably robust for a given data
1
Published as a conference paper at ICLR 2022
point according to the output margin. The whole procedure only requires a forward pass without
any additional computation. The authors further showed that the model family has strong expres-
SiVe power, e.g., a large enough '∞-distance net can approximate any I-LiPschitz function on a
bounded domain. Unfortunately, however, the empirical model performance did not well reflect
the theoretical adVantages. As shown in Zhang et al. (2021), it is necessary to use a conVentional
multi-layer perception (MLP)I on top of an '∞-distance net backbone to achieve better performance
compared to the baseline methods. It makes both the training and the certification procedure compli-
cated. More importantly, it calls into question whether the '∞-distance net is really a better model
configuration than conventional architectures in the regime of certified robustness.
In this paper, We give an affirmative answer by showing that '∞ -distance net itself suffices for good
performance and can be well learned using an improved training strategy. We first mathematically
prove that undermild assumptions of the dataset, there exists an '∞-distance net with reasonable size
by construction that achieves perfect certified robustness. This result indicates the strong expressive
power of '∞-distance nets in robustness certification, and shows a fundamental advantage over
conventional networks under typical certification approaches (which do not possess such expressive
power according to Mirman et al. (2021)). However, it seems to contradict the previous empirical
observations, suggesting that the model may fail to find an optimal solution and further motivating
us to revisit the optimization process designed in Zhang et al. (2021).
Due to the non-smoothness of the '∞-distance function, Zhang et al. (2021) developed several train-
ing tricks to overcome the optimization difficulty. A notable trick is called the `p -relaxation, in which
'p-distance neurons are used during optimization to give a smooth approximation of '∞-distance.
However, we find that the relaxation on neurons unexpectedly relaxes the Lipschitz constant of the
network to an exponentially large value, making the objective function no longer maximize the
robust accuracy and leading to sub-optimal solutions.
We develop a novel modification of the objective function to bypass the problem mentioned above.
The objective function is a linear combination of a scaled cross-entropy term and a modified clipped
hinge term. The cross-entropy loss maximizes the output margin regardless of the model’s Lips-
chitzness and makes optimization sufficient at the early training stage when p is small. The clipped
hinge loss then focuses on robustness for correctly classified samples at the late training phase when
p approaches infinity. The switch from cross-entropy loss to clipped hinge loss is reflected in the
mixing coefficient, which decays to zero as p grows to infinity throughout the training procedure.
Despite its simplicity, our experimental results show significant performance gains on various
datasets. In particular, an '∞-distance net backbone can achieve 40.06% certified robust accuracy
on CIFAR-10 ( = 8/255). This goes far beyond the previous results, which achieved 33.30% cer-
tified accuracy on CIFAR-10 using the same architecture (Zhang et al., 2021). Besides, it surpasses
the relaxation-based certification approaches by at least 5 points (Shi et al., 2021; Lyu et al., 2021),
establishing a new state-of-the-art result.
To summarize, both the theoretical finding and empirical results in this paper demonstrate the merit
of '∞-distance net for certified robustness. Considering the simplicity of the architecture and train-
ing strategy used in this paper, we believe there are still many potentials for future research of
'∞-distance nets, and more generally, the class of Lipschitz architectures.
2	Preliminary
In this section, we briefly introduce the '∞-distance net and its training strategy. An '∞-distance net
is constructed using '∞-distance neurons as the basic component. The '∞-distance neuron U takes
vector X as the input and calculates the '∞-norm distance between X and parameter W with a bias
term b. The neuron can be written as
u(X, {w, b}) = kX - wk∞ +b.	(1)
Based on the neuron definition, a fully-connected '∞-distance net can then be constructed. Formally,
an L layer network g takes X(0) = X as the input, and the lth layer X(l) is calculated by
Xil)= u(x°τ), {w(l,i), b(l)}) = kx(l-1) - w(l,i)k∞ + b(l, l ∈ [L],i ∈ [nι].	(2)
1Without any confusion, in this paper, a conventional neural network model is referred to as a network
composed of linear transformations with non-linear activations.
2
Published as a conference paper at ICLR 2022
Here nl is the number of neurons in the lth layer. For K-class classification problems, nL = K.
The network outputs g(x) = x(L) as logits and predicts the class arg maxi∈[K] [g(x)]i.
An important property of '∞-distance net is its LiPschitz continuity, as is stated below.
Definition 2.1. A mapping f (z) : Rm → Rn is called λ-Lipschitz with respect to 'p-norm ∣∣ ∙ ∣∣p,
if for any z1 , z2, the following holds:
∣f(z1) - f (z2)∣p ≤ λ∣z1 -z2∣p.	(3)
Proposition 2.2. The mapping ofan '∞-distance layer is1-Lipschitz with respect to '∞-norm. Thus
by composition, any '∞-distance net g(∙) is I-LiPschitz with respect to '∞-norm.
'∞-distance nets naturally possess certified robustness using the Lipschitz property. In detail, for
any data point x with label y , denote the output margin of network g as
margin(x, y; g) = [g(x)]y - max[g(x)]j.	(4)
j6=y
If x is correctly classified by g, then the prediction of a perturbed input x + δ will remain the
same as x if ∣δ∣∞ < margin(x, y; g)/2. In other words, we can obtain the certified robustness
for a given perturbation level e according to I(margin(x, y; g)/2 > e), where I(∙) is the indicator
function. We call this margin-based certification. Given this certification approach, a corresponding
training approach can then be developed, where one simply learns a large margin classifier using
standard loss functions, e.g., hinge loss, without resorting to adversarial training. Therefore the
whole training procedure is as efficient as training standard networks with no additional cost.
Zhang et al. (2021) further show that '∞-distance nets are Lipschitz-universal approximators. In
detail, a large enough '∞-distance net can approximate any 1-Lipschitz function with respect to
'∞-norm on a bounded domain arbitrarily well.
Training '∞-distance nets. One major challenge in training '∞-distance net is that the '∞-distance
operation is highly non-smooth, and the gradients (i.e. Vχ∣x-w∣∞ and Rw ∣∣x 一 w∣∞) are sparse.
To mitigate the problem, Zhang et al. (2021) used 'p-distance neurons instead of '∞-distance ones
during training, resulting in approximate and non-sparse gradients. Typically p is set toa small value
(e.g., 8) in the beginning and increases throughout training until it reaches a large number (e.g.,
1000). The authors also designed several other tricks to further address the optimization difficulty.
However, even with the help of tricks, '∞-distance nets only perform competitively to previous
works. The authors thus considered using a hybrid model architecture, in which the '∞-distance net
serves as a robust feature extractor, and an additional conventional multi-layer perceptron is used as
the prediction head. This architecture achieves the best performance, but both the training and the
certification approach become complicated again due to the presence of non-Lipschitz MLP layers.
3	Expressive Power of '∞-distance Nets in Robust Classification
In this section, we challenge the conclusion of previous work by proving that simple '∞-distance
nets (without the top MLP) suffice for achieving perfect certified robustness in classification. Recall
that Zhang et al. (2021) already provides a universal approximation theorem, showing the expressive
power of '∞-distance nets to represent Lipschitz functions. However, their result focuses on real-
valued function approximation and is not directly helpful for certified robustness in classification.
One may ask: Does a certifiably robust '∞-distance net exist given a dataset? If so, how large
does the network need to be? We will answer these questions and show that one can explicitly
construct an '∞-distance net that achieves perfect certified robustness as long as the dataset satisfies
the following (weak) condition called r-separation (Yang et al., 2020b).
Definition 3.1. (r-separation) consider a labeled dataset D = {(xi, yi)} where yi ∈ [K] is the label
of xi. We say D is r-separated with respect to `p-norm if for any pair of samples (xi, yi), (xj, yj),
as long as yi 6= yj , one has ∣xi 一 xj ∣p > 2r.
Table 1: The r-separation property of commonly used datasets, taken from Yang et al. (2020b).
Dataset r commonly used e
-MNIST	0.369	03
ciFAR-10	0.106	8/255
3
Published as a conference paper at ICLR 2022
It is easy to see that r-separation is a necessary condition for robustness under `p-norm perturbation
= r. In fact, the condition holds for all commonly used datasets (e.g., MNIST, CIFAR-10): the
value of r in each dataset is much greater than the allowed perturbation level as is demonstrated
in Yang et al. (2020b) (see Table 1 above). The authors took a further step and showed there must
exist a classifier that achieves perfect robust accuracy if the condition holds. We now prove that
even if We restrict the classifier to be the network function class represented by '∞-distance nets,
the conclusion is still correct: a simple two-layer '∞-distance net with hidden size O(n) can already
achieve perfect robustness for r-separated datasets.
Theorem 3.2. Let D be a dataset with n elements satisfying the r-separation condition with respect
to '∞ -norm. Then there exists a two-layer '∞-distance net with hidden size n, such that when using
margin-based certification, the certified '∞ robustaCcuracy is 100% on D under perturbation e = r.
Proof sketch. Consider a two layer '∞-distance net g defined in Equation (2). Let its parameters be
assigned by
w(1,i) = xi, bi() = 0	for i ∈ [n]
w(2,j) = C ∙ I(yi = j), bj2) = -C for i ∈ [n],j ∈ [K]
where C = 4 maxi∈[n] ∣∣xik∞ is a constant, andI(∙) is the indicator function. For the above assign-
ment, it can be proved that the network outputs
[g(x)]j = x(j2) = - min ∣x - xi∣∞ .	(5)
i∈[n],yi=j
From Equation (5) the network g can represent a nearest neighbor classifier, in that it outputs the
negative of the nearest neighbor distance between input x and the samples of each class. Therefore,
given data x = xi in dataset D, the output margin of g(x) is at least 2r due to the r-separation
condition. In other words, g achieves 100% certified robust accuracy on D.	□
Remark 3.3. The above result can be extended to multi-layer networks. In general, we can prove
the existence of such networks with L layers and no more than O(n/L + K + d) hidden neurons
for each hidden layer where d is the input dimension. See Appendix A for details of the proof.
The significance of Theorem 3.2 can be reflected in the following two aspects. Firstly, our result
explicitly shows the strong expressive power of '∞-distance nets in robust classification, which
complements the universal approximation theorem in Zhang et al. (2021). Moreover, Theorem 3.2
gives an upper bound of O(n) on the required network size which is close to practical applications.
It is much smaller than the size needed for function approximation (O(1∕εd) under approximation
error ε, proved in Zhang et al. (2021)), which scales exponentially in the input dimension d.
Secondly, our result justifies that for well-designed architectures, using only the global Lipschitz
property is sufficient for robustness certification. It contrasts to the prior view that suggests lever-
aging the local Lipschitz constant is necessary (Huster et al., 2018), which typically needs sophisti-
cated calculations (Wong et al., 2018; Zhang et al., 2018; 2020b). More importantly, as a compar-
ison, Mirman et al. (2021) very recently proved that for any conventional network, the commonly-
used interval bound propagation (IBP) (Mirman et al., 2018; Gowal et al., 2018) intrinsically cannot
achieve perfect certified robustness on a simple r-separation dataset containing only three data points
(under e = r). In other words, '∞-distance nets certified using the global Lipschitz property have a
fundamental advantage over conventional networks certified using interval bound propagation.
4	Investigating THE Training of '∞-distance Nets
Since robust '∞-distance nets exist in principle, the remaining thing is understanding why the cur-
rent training strategy cannot find a robust solution. In this section, we first provide evidence that
the training method in Zhang et al. (2021) is indeed problematic and cannot achieve good certified
robustness, then provide a novel way to address the issue.
4.1	Problems of the Current Training Strategy
As shown in section 2, given any perturbation level e, the certified accuracy of data point x can
be calculated according to I(margin(x, y; g)/2 > e) for 1-Lipschitz functions. Then the hinge loss
becomes standard to learn a robust '∞-distance net:
Lhinge(g, D; θ) = E(xi,yi)∈D [max {θ - margin(xi, yi; g), 0}] ,	(6)
4
Published as a conference paper at ICLR 2022
O Q OO
4 3 2 1
(次)36u3N3d
p=8 p=20	p=100	p=1000
5 0 5 0
3 3 2 2
) &E3uun 芸仁
4	6	8	10	12
hinge threshold (θ∕ε)
O IOOO 2000	3000	4000
epoch
(a)
(b)
Figure 1: Experiments of '∞-distance net training on CIFAR-10 dataset using the hinge loss function. Training
details can be found in Section 5.1. (a) The percentage of training samples with output margin greater than θ
throughout training. We use very long training epochs, and the final percentage is still below 2.5%. The dashed
lines indicate different p values of `p -distance neurons. (b) The certified accuracy on training dataset and test
dataset respectively, trained using different hinge threshold θ. Training gets worse when θ ≤ 6.
where θ is the hinge threshold which should be larger than 2. Hinge loss aims at making the output
margin for any sample greater than θ, and Lhinge (g, D; θ) = 0 if and only if the network g achieves
perfect certified robustness on training dataset under perturbation e = θ∕2.
Hinge loss fails to learn robust classifiers. We first start with some empirical observations. Con-
sider a plain '∞-distance net trained on CIFAR-10 dataset using the same approach and hyper-
parameters provided in Zhang et al. (2021). When the training finishes, we count the percentage of
training samples whose margin is greater than θ, i.e., achieving zero loss. We expect the value to be
large if the optimization is successful. However, the result surprisingly reveals that only 1.62% of the
training samples are classified correctly with a margin greater than θ. Even if we use much longer
training epochs (e.g. 4000 epochs on CIFAR-10), the percentage is still below 2.5% (see Figure
1(a)). Thus we conclude that hinge loss fails to optimize well for most of the training samples.
Since the output margins of the vast majority of training samples are less than θ, the loss approxi-
mately degenerates to a linear function without the maximum operation:
C	/	z^ι∖ . . %	/	z^ι∖ τττ>	「八	■	/	∖ 1
Lhinge (g, D; θ) ≈ Lhinge (g, D； θ) = E(xi,yi)∈D [θ - margin®, y, g)]
= θ - E(xi,yi)∈D [margin(xi, yi; g)] ,
(7)
where θ becomes irrelevant, and training becomes to optimize the average margin over the dataset
regardless of the allowed perturbation which is definitely problematic.
After checking the hyper-parameters used in Zhang et al. (2021), we find that the hinge threshold θ
is set to 80/255, which is much larger than the perturbation level = 8/255. This partly explains
the above degeneration phenomenon, but it is still unclear why such a large value has to be taken.
We then conduct experiments to see the performance with different chosen hinge thresholds θ. The
results are illustrated in Figure 1(b). As one can see, a smaller hinge threshold not only reduces
certified test accuracy but even makes training worse.
Why does this happen? We find that the reason for the loss degeneration stems from the `p-
relaxation used in training. While 'p-relaxation alleviates the sparse gradient problem, it destroys
the Lipschitz property of the '∞ -distance neuron, as stated in Proposition 4.1:
Proposition 4.1. A layer constructed using 'p-distance neurons
up (x, {w, b}) = kx -wkp+b	(8)
is d1/p Lipschitz with respect to '∞-norm, where d is the dimension of x. Thus by composition, an
L layer 'p-distance net is dL/p Lipschitz with respect to '∞-norm.
Zhang et al. (2021) uses a small p in the beginning and increases its value gradually during training.
From Proposition 4.1, the Lipschitz constant of the smoothed network can be significantly large2 at
2For a 6-layer 'p-distance net (p = 8) with hidden size 5120 used in Zhang et al. (2021), Proposition 4.1
approximately gives a Lipschitz constant of 568. We also run experiments to validate such upper bound is
relatively tight. See Appendix C for more details.
5
Published as a conference paper at ICLR 2022
the early training stage. Note that the robustness certification I(margin(x, y; g)/2 ≤ ) holds for
1-Lipschitz functions. When the Lipschitz constant is large, even if the margin of a data point passes
the threshold, the data point can still lie near to classification boundary. This makes the training
using hinge loss ineffective at the early stage and converge to a wrong solution far from the real
optima. We argue that the early-stage training is important as when p rises to a large value, the
optimization becomes intrinsically difficult to push the parameters back to the correct solution due
to sparse gradients.
Such argument can be verified from Figure 1(a), in which we plot the percentage of training samples
with margins greater than θ throughout a long training process. After p starts to rise, the margin
decreases drastically, and the percentage sharply drops and never increases again during the whole
'p-relaxation procedure. It is also clear Why the value of θ must be chosen to be much larger than 2e.
For small θ, the margin optimization becomes insufficient at early training stages when the Lipschitz
constant is exponentially large, leading to Worse performance even on the training dataset.
4.2	Our Solution
In the previous section, one can see that the hinge loss and the `p -relaxation are incompatible. As the
'p-relaxation is essential to overcome the sparse gradient problem, we focus on developing better
loss functions. We shoW in this section that a simple change of the objective function can address
the above problem, leading to non-trivial improvements.
We approach the issue by investigating the commonly used cross-entropy loss. For conventional
non-Lipschitz networks, cross-entropy loss aims at increasing the logit of the true class while de-
creasing the other logits as much as possible, therefore enlarges the output margin without a thresh-
old constraint. This makes the optimization of output margin sufficient regradless of the Lipschitz
constant, which largely alleviates the problem in Section 4.1 for training 'p-distance nets with small
p. Such findings thus motivate us to replace hinge loss with cross-entropy. However, on the other
hand, cross-entropy loss only coarsely enlarges the margin, rather than exactly optimizing the sur-
rogate of certified accuracy I(margin(x, y; g)/2 ≤ e) that depends on e. When the model is almost
1-Lipschitz (i.e. p approaches infinity), hinge loss can still be better than cross-entropy. In other
words, cross-entropy loss and hinge loss are complementary.
Based on the above argument, we propose to simply combine cross-entropy loss and hinge loss to
obtain the following objective function:
L(g,D; θ) = E(χi,yi)∈D[λ'CE(s ∙ g(xi),yi)+min{'hinge(g(xi)∕θ,yi), 1}]	(9)
'------------------} 、-------------{------------}
scaled cross-entropy loss	clipped hinge loss
where 'CE(z, y) = log(Pi exp(zi)) - zy and 'hinge(z, y) = max{maxi6=y zi - zy + 1, 0}. We now
make detailed explanations about each term in Equation (9).
Scaled cross-entropy loss 'ce(s ∙ g(xi), yi). Cross-entropy loss deals with the optimization issue
when p is small. Here a slight difference is the introduced scaling s as is explained below. We know
cross-entropy loss is invariant to the shift operation (adding a constant to each output logit) but not
scaling (multiplying a constant). For conventional networks, the output logits are produced through
the last linear layer, and the scaling factor can be implicitly learned in the parameters of the linear
layer to match the cross-entropy loss. However, '∞-distance net is strictly 1-Lipschitz and does not
have any scaling operation. We thus introduce a learnable scalar (temperature) s that multiplies the
network output g(xi) before taking cross-entropy loss. We simply initialize it to be 1. Note that s
does not influence the classification results and can be removed once the training finishes.
Clipped hinge loss min{'hinge(g(xi)∕θ, yi), 1}. Clipped hinge loss is designed to achieve robust-
ness when p approaches infinity. Unlike the standard hinge loss, the clipped version plateaus if the
output margin is negative (i.e., misclassified). In other words, clipped hinge loss is equivalent to
applying hinge loss only on correctly-classified samples. The reason for applying such a clipping
is three-fold. (i) Scaled cross-entropy loss already focuses on learning a model with high (clean)
accuracy, thus there is no need to optimize on wrongly-classified samples duplicatively using hinge
loss. Moreover, cross-entropy is better than hinge loss when used in classification, as the gradient
of hinge loss makes optimization harder3. (ii) In the late training phase, the optimization becomes
3The gradient of hinge loss w.r.t. output logits is sparse (only two non-zero elements) and does not make
full use of the information provided by the logit.
6
Published as a conference paper at ICLR 2022
intrinsically difficult (p approaching infinity). As a consequence, wrongly classified samples may
have little chance to be robust. Clipped hinge loss thus ignores these samples and concentrates on
easier ones to increase their potential to be robust after training. (iii) The clipped hinge loss is a bet-
ter surrogate for 0-1 robust error I(margin(x, y; g)/2 ≤ ). Compared with hinge loss, the clipped
version is closer to our goal and more likely to achieve better certified accuracy. Finally, due to the
presence of cross-entropy loss, we will show in Section 5.3 that the hinge threshold θ can be set to
a much smaller value unlike Figure 1(b), which thus avoids the loss degeneration problem.
The mixing coefficient λ. The coefficient λ in loss (9) plays a role in the trade-off between cross-
entropy and hinge loss. Based on the above motivation, we use a decaying λ that attenuates from
λo to zero throughout the process of 'p-relaxation (λ0 is a hyper-parameter). When P is small at the
early training stage, we focus more on cross-entropy loss. After p grows large, λ vanishes, and a
surrogate of 0-1 robust error is optimized.
We point out that objective functions similar to (9) also appeared in previous literature. In particular,
the TRADES loss (Zhang et al., 2019a) and MMA loss (Ding et al., 2020) are both composed of
a mixture of the cross-entropy loss and a form of robust loss. Nevertheless, the motivations of
these methods are quite different. For example, TRADES was proposed based on the theoretical
results suggesting robustness may be at odds with accuracy (Tsipras et al., 2019), while our training
approach is mainly motivated by the optimization issue. Furthermore, the implementations of these
methods also vary a lot. We use clipped hinge loss to achieve robustness due to its simplicity, and
uses a decaying λ correlate to P in 'p-relaxation due to the optimization problem in Section 4.1.
5	Experiments
5.1	Experimental Setting
In this section, we evaluate the proposed training strategy on benchmark datasets MNIST and
CIFAR-10 to show the effectiveness of '∞-distance net.
Model details. We use exactly the same model as Zhang et al. (2021) for a fair comparison. Con-
cretely, We consider the simple fully-connect '∞-distance nets defined in Equation (2). All hidden
layers have 5120 neurons. We use a 5-layer network for MNIST and a 6-layer one for CIFAR-10.
Training details. In all experiments, we choose the Adam optimizer with a batch size of 512. The
learning rate is set to 0.03 initially and decayed using a simple cosine annealing throughout the whole
training process. We use padding and random crop data augmentation for MNIST and CIFAR-10,
and also use random horizontal flip for CIFAR-10. The `p -relaxation starts at P = 8 and ends at P =
1000 with P increasing exponentially. Accordingly, the mixing coefficient λ decays exponentially
during the `p -relaxation process from λ0 to a vanishing value λend. We do not use further tricks that
are used in Zhang et al. (2021), e.g. the `p weight decay or a warmup over perturbation , to keep
our training strategy clean and simple. The dataset dependent hyper-parameters, including θ, λ0,
λend and the number of epochs T, can be found in Appendix B. All experiments are run for 8 times
on a single NVIDIA Tesla-V100 GPU, and the median of the performance number is reported.
Evaluation. We test the robustness of the trained models under e-bounded '∞-norm perturbations.
Following the common practice (Madry et al., 2017), we mainly use = 0.3 for MNIST dataset
and 8/255 for CIFAR-10 dataset. We also provide results under other perturbation magnitudes, e.g.
e = 0.1 for MNIST and e = 2/255, e = 16/255 for CIFAR-10. We first evaluate the robust test
accuracy under the Projected Gradient Descent (PGD) attack (Madry et al., 2017). The number of
iterations of the PGD attack is set to a large number of 100. We then calculate the certified robust
accuracy based on the output margin.
5.2	Experimental Results
Results are presented in Table 2. For each method in the table, we report the clean test accuracy
without perturbation (denoted as Clean), the robust test accuracy under PGD attack (denoted as
PGD), and the certified robust test accuracy (denoted as Certified). We also compare with random-
ized smoothing (see Appendix D), despite these methods provides probabilistic certified guarantee
and usually take thousands of times more time than other approaches for robustness certification.
7
Published as a conference paper at ICLR 2022
Table 2: Comparison of our results with existing methods.						
Dataset	e	Method	Reference	Clean	PGD	Certified
		CAP	(Wong etal.,2018)	98.92	-	96.33
		IBP*	(Gowal et al., 2018)	98.92	97.98	97.25
		CROWN-IBP	(Zhang et al., 2020b)	98.83	98.19	97.76
	0.1	IBP	(Shi etal.,2021)	98.84	-	97.95
		COLT	(Balunovic & Vechev, 2020)	99.2	-	97.1k
		'∞-distance Nett	(Zhang et al., 2021)	98.66	97.79聿	97.70
MNIST		'∞ - distance Net	This paper	98.93	98.03	97.95
		IBP*	(Gowaletal., 2018)	97.88	93.22	91.79
		CROWN-IBP	(Zhang et al., 2020b)	98.18	93.95	92.98
		IBP	(Shi etal.,2021)	97.67	-	93.10
	0.3	COLT	(Balunovic & Vechev, 2020)	97.3	-	85.7k
		'∞-distance Net+MLP	(Zhang et al., 2021)	98.56	95.28^	93.09
		'∞ - distance Net	(Zhang et al., 2021)	98.54	94.71*	92.64
		'∞ - distance Net	This paper	98.56	94.73	93.20
		CAP	(Wong etal.,2018)	68.28	-	53.89
		IBP*	(Gowal et al., 2018)	61.46	50.28	44.79
		CROWN-IBP	(Zhang et al., 2020b)	71.52	59.72	53.97
	2/255	IBP	(Shi etal.,2021)	66.84	-	52.85
		COLT	(Balunovic & Vechev, 2020)	78.4	-	60.5k
		Randomized Smoothing	(Blum et al., 2020)	78.8	-	62.6§k
		'∞-distance Nett	(Zhang et al., 2021)	60.33	51.45*	50.94
		'∞ - distance Net	This paper	60.61	54.28	54.12
		IBP*	(Gowaletal., 2018)	50.99	31.27	29.19
		CROWN-IBP	(Zhang et al., 2020b)	45.98	34.58	33.06
		CROWN-IBP	(XU et al., 2020)	46.29	35.69	33.38
CIFAR-10		IBP	(Shi etal.,2021)	48.94	-	34.97
		CROWN-LBP	(Lyu et al., 2021)	48.06	37.95	34.92
	8/255	COLT	(Balunovic & Vechev, 2020)	51.7	-	27.5k
		Randomized Smoothing	(Salman et al., 2019a)	53.0	-	24.0§k
		Randomized Smoothing	(Jeong & Shin, 2020)	52.3	-	25.2§k
		'∞-distance Net+MLP	(Zhang et al., 2021)	50.80	37.06*	35.42
		'∞ - distance Net	(Zhang et al., 2021)	56.80	37.46*	33.30
		'∞ - distance Net	This paper	54.30	41.84	40.06
		IBP*	(Gowal et al., 2018)	31.03	23.34	21.88
		CROWN-IBP	(Zhang et al., 2020b)	33.94	24.77	23.20
	16/255	IBP	(Shi etal.,2021)	36.65	-	24.48
		'∞-distance Nett	(Zhang et al., 2021)	55.05	26.02*	19.28
		'∞ - distance Net	This paper	48.50	32.73	29.04
* The IBP results are obtained from Zhang et al. (2020b).
t These results are obtained by running the code in the authors' github. See Appendix B for details.
■ The number of PGD steps is chosen as 20 in Zhang et al. (2021).
§ These methods provide probabilistic certified guarantees.
k Calculating certified accuracy for these methods takes several days on a single GPU, which is 4 to 6 orders of magnitude
slower than other methods.
Comparing with Zhang et al. (2021). It can be seen that for all perturbation levels and datasets,
our proposed training strategy improves the performance of '∞-distance nets. In particular, We
boost the certified accuracy on CIFAR-10 from 33.30% to 40.06% under = 8/255, and from
19.28% to 29.04% under a larger = 16/255. Note that We use exactly the same architecture as
Zhang et al. (2021), and a larger netWork With better architecture may further improve the results.
Another observation from Table 2 is that the improvement of our proposed training strategy gets
more prominent With the increase of . This is consistent With our finding in Section 4.1, in that the
optimization is particularly insufficient for large using hinge loss, and in this case our proposed
objective function can significantly alleviate the problem.
Comparing with other certification methods. For most settings in Table 2, our results establish
neW state-of-the-arts over previous baselines, despite We use the margin-based certification Which is
much simpler. The gap is most noticeable for = 8/255 on CIFAR-10, Where We surpass recent
relaxation-based approaches by more than 5 points (Shi et al., 2021; Lyu et al., 2021). It can also be
observed that '∞-distance net is most suitable for the case when '∞ perturbation is relatively large.
This is not surprising since Lipschitz property is Well exhibited in this case. If is vanishingly small
(e.g. 2/255), the advantage of the Lipschitz property will not be well-exploited and '∞-distance net
will face more optimization and generalization problems compared with conventional networks.
8
Published as a conference paper at ICLR 2022
end
(a)	(b)	(c)
Figure 2: Experiments of '∞-distance net training on CIFAR-10 dataset (e = 8/255) using the proposed
objective function (9). (a) The percentage of training samples with output margin greater than θ (blue), less
than 0 (orange), or between 0 and θ (green) throughout training. The dashed lines indicate different p values of
`p -relaxation. (b) The final performance of the trained network using different hinge threshold θ. (c) Heatmap
of certified accuracy with different hyper-parameters λ0 and λend. Each grid shows the certified accuracy for a
pair of hyper-parameters.
5.3 Investigating the Proposed Training Strategy
We finally demonstrate by experiments that the proposed training strategy indeed addresses the opti-
mization problem in Section 4.1. We first trace the output margin of the training samples throughout
training on CIFAR-10 dataset ( = 8/255), and plot the percentage of samples with a margin greater
than θ, less than 0, or between 0 and θ, shown in Figure 2(a). In contrast to Figure 1(a), it can be
seen that the loss does not degenerate in the whole training process. We then demonstrate in Figure
2(b) that the accuracy curve regarding different choices of hinge threshold θ is well-behaved com-
pared with Figure 1(b). In particular, the best θ that maximizes the certified accuracy on training
dataset approaches 2 (while for original hinge loss the value is 6). The peak certified accuracy on
training dataset also improves by 10 points (see blue lines in the two figures). Such evidence clearly
demonstrates the effectiveness of the proposed training strategy.
Sensitivity analysis. We perform sensitivity analysis on CIFAR-10 dataset ( = 8/255) over hyper-
parameters including the hinge threshold θ and the mixing coefficient λ. Results are shown in
Figure 2(b) and 2(c). It can be seen that (i) the certified accuracy is above 39% for a wide range of
θ (between 5 and 8); (ii) among the 20 hyper-parameter combinations of (λ0 , λend), all certified
accuracy results surpass 39%, and half of the results can achieve a certified accuracy of more than
39.75%. In summary, the performance is not sensitive to the choice of the hyper-parameters θ and
λ. See Appendix F for more details on other hyper-parameters.
Ablation studies. We also conduct ablation experiments for the proposed loss function on CIFAR-
10 dataset ( = 8/255). Due to the space limit, we put results in Appendix E. In summary, both
the cross-entropy loss and clipped hinge loss are crucial to boost the certified accuracy. Using a
decaying mixing coefficient λ can further improve the performance and stabilize the training.
6	Related Work
In recent years substantial efforts have been taken to obtain robust classifiers. Existing approaches
mainly fall into two categories: adversarial training and certified defenses.
Adversarial training. Adversarial training methods first leverage attack algorithms to generate ad-
versarial examples of the inputs on the fly, then update the model’s parameters using these perturbed
inputs together with the original labels (Goodfellow et al., 2014; Kurakin et al., 2016; Madry et al.,
2017). In particular, Madry et al. (2017) suggested using Projected Gradient Descent (PGD) as
the universal attacker to find a perturbation that maximizes the standard cross-entropy loss, which
achieves decent empirical robustness. Some recent works considered other training objectives that
combine cross-entropy loss and a carefully designed robust surrogate loss (Zhang et al., 2019a; Ding
et al., 2020; Wang et al., 2020), which show similarities to this paper. However, all methods above
are evaluated empirically using first-order attacks such as PGD, and there is no formal guarantee
whether the learned model is truly robust. This motivates researchers to study a new type of method
9
Published as a conference paper at ICLR 2022
called certified defenses, in which the prediction is guaranteed to remain the same under all allowed
perturbations, thus provably resists against all potential attacks.
Relaxation-based certified defenses. These methods adopt convex relaxation to calculate a convex
region containing all possible network outputs for a given input under perturbation (Wong & Kolter,
2018; Wong et al., 2018; Dvijotham et al., 2018; 2020; Raghunathan et al., 2018a;b; Weng et al.,
2018; Singh et al., 2018; Mirman et al., 2018; Gehr et al., 2018; Wang et al., 2018; 2021; Gowal
et al., 2018; Zhang et al., 2018; 2020b; Xiao et al., 2019; Croce et al., 2019; Balunovic & Vechev,
2020; Lee et al., 2020; Dathathri et al., 2020; Xu et al., 2020; Lyu et al., 2021; Shi et al., 2021). If all
points in this region correspond to the correct prediction, then the network is provably robust. How-
ever, the relaxation procedure is usually complicated and computationally expensive. Furthermore,
Salman et al. (2019b); Mirman et al. (2021) indicated that there might be an inherent barrier to tight
relaxation for a large class of convex relaxation approaches. This is also reflected in experiments,
where the trained model often suffers from severe accuracy drop even on training data.
Randomized smoothing for certified robustness. Randomized smoothing provides another way
to calculate a probabilistic certification under `2 perturbations (Lecuyer et al., 2019; Li et al., 2019a;
Cohen et al., 2019; Salman et al., 2019a; Zhai et al., 2020; Jeong & Shin, 2020; Zhang et al., 2020a;
Yang et al., 2022; Horvath et al., 2022). For any classifier, if a Gaussian random noise is added to
the input, the resulting “smoothed” classifier then possesses a certified guarantee under `2 perturba-
tions. Randomized smoothing has been scaled up to ImageNet and achieves state-of-the-art certified
accuracy for `2 perturbations. However, recent studies imply that it cannot achieve nontrivial certi-
fied accuracy for 'p perturbations under e = Ω(d"PT/2) when P > 2 which depends on the input
dimension d (Yang et al., 2020a; Blum et al., 2020; Kumar et al., 2020; Wu et al., 2021). Therefore
it is not suitable for '∞ perturbation scenario if e is not very small.
Lipschitz networks. An even simpler way for certified robustness is to use Lipschitz networks,
which directly possess margin-based certification. Earlier works in this area mainly regard the Lip-
schitz property as a kind of regularization and penalize (or constrain) the Lipschitz constant of a
conventional ReLU network based on the spectral norms of its weight matrices (Cisse et al., 2017;
Yoshida & Miyato, 2017; Gouk et al., 2018; Tsuzuku et al., 2018; Farnia et al., 2019; Qian & Weg-
man, 2019; Pauli et al., 2021). However, these methods either can not provide certified guarantees
or provide a vanishingly small certified radius. Anil et al. (2019) figured out that current Lipschitz
networks intrinsically lack expressivity to some simple Lipschitz functions, and designed the first
Lipschitz-universal approximator called GroupSort network. Li et al. (2019b); Trockman & Kolter
(2021); Singla & Feizi (2021) studied Lipschitz networks for convolutional architectures. Recent
studies (Leino et al., 2021; Singla et al., 2022) achieved the state-of-the-art certified robustness
using GroupSort network under `2 perturbations. However, none of these approaches above can
provide good certified results for '∞ robustness. The most relevant work to this paper is Zhang et al.
(2021), in which the author designed a novel Lipschitz network with respect to '∞-norm. We show
such architecture can establish new state-of-the-art results in the '∞ perturbation scenario.
7	Conclusion
In this paper, we demonstrate that a simple '∞-distance net suffices for good certified robustness
from both theoretical and experimental perspectives. Theoretically, we prove the strong expressive
power of '∞-distance nets in robust classification. Combining with Mirman et al. (2021), this re-
sult may indicate that '∞-distance nets have inherent advantages over conventional networks for
certified robustness. Experimentally, despite simplicity, our approach yields a large gain over pre-
vious (possibly more complicated) certification approaches and the trained models establish new
state-of-the-art certified robustness.
Despite these promising results, there are still many aspects that remain unexplored. Firstly, in the
case when e is very small, '∞-distance nets may have a lot of room for improvement. Secondly, it is
important to design better architectures suitable for image classification tasks than the simple fully
connected network used in this paper. Finally, it might be interesting to design better optimization
algorithms for '∞-distance nets to further handle the model,s non-smoothness and gradient sparsity.
We hope this work can make promising the study of '∞-distance nets, and more generally, the global
Lipschitz architectures for certified robustness.
10
Published as a conference paper at ICLR 2022
Acknowledgement
This work was supported by National Key R&D Program of China (2018YFB1402600), BJNSF
(L172037). Project 2020BD006 supported by PKUBaidu Fund.
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In Inter-
national Conference on Machine Learning, pp. 291-301, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International conference on machine
learning, pp. 274-283. PMLR, 2018.
Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.
In International Conference on Learning Representations, 2020.
Battista Biggio, Igmo Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases, pp. 387-402.
Springer, 2013.
Avrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable
to certify '∞ robustness for high-dimensional images. arXiv preprint arXiv:2002.03517, 2020.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten de-
tection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and security,
pp. 3-14, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning, pp. 854-863, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International conference on machine learning, pp. 2206-
2216. PMLR, 2020.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu net-
works via maximization of linear regions. In the 22nd International Conference on Artificial
Intelligence and Statistics, pp. 2057-2066. PMLR, 2019.
Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Ue-
sato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, and
Pushmeet Kohli. Enabling certification of verification-agnostic networks via memory-efficient
semidefinite programming. In Advances in Neural Information Processing Systems, volume 33,
pp. 5318-5331, 2020.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct
input space margin maximization through adversarial training. In International Conference on
Learning Representations, 2020.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A
dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018.
Krishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Push-
meet Kohli. Efficient neural network verification with exactness characterization. In Uncertainty
in Artificial Intelligence, pp. 497-507. PMLR, 2020.
Farzan Farnia, Jesse Zhang, and David Tse. Generalizable adversarial training via spectral normal-
ization. In International Conference on Learning Representations, 2019.
11
Published as a conference paper at ICLR 2022
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy (SP),pp. 3-18. IEEE, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Miklos Z. Horvath, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. Boosting randomized
smoothing with variance reduced classifiers. In International Conference on Learning Represen-
tations, 2022.
Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the lipschitz constant as a
defense against adversarial examples. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 16-29. Springer, 2018.
Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed
classifiers. Advances in Neural Information Processing Systems, 33, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on
randomized smoothing for certifiable robustness. In International Conference on Machine Learn-
ing, pp. 5458-5467. PMLR, 2020.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Sungyoon Lee, Jaewook Lee, and Saerom Park. Lipschitz-certifiable training with a tight outer
bound. Advances in Neural Information Processing Systems, 33, 2020.
Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks. In International
Conference on Machine Learning (ICML), 2021.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. Advances in Neural Information Processing Systems, 32:9464-9474, 2019a.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Joern-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. Advances in
Neural Information Processing Systems, 32:15390-15402, 2019b.
Zhaoyang Lyu, Minghao Guo, Tong Wu, Guodong Xu, Kehuan Zhang, and Dahua Lin. Towards
evaluating and training verifiably robust neural networks. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 4308-4317, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3578-3586.
PMLR, 2018.
12
Published as a conference paper at ICLR 2022
Matthew Mirman, Maximilian Baader, and Martin Vechev. The fundamental limits of interval arith-
metic for neural networks. arXiv preprint arXiv:2112.05235, 2021.
Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower. Training robust
neural networks using lipschitz bounds. IEEE Control Systems Letters, 2021.
Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International Conference
on Learning Representations, 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018a.
Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10900-10910, 2018b.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. In Pro-
ceedings of the 33rd International Conference on Neural Information Processing Systems, pp.
11292-11303, 2019a.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relax-
ation barrier to tight robustness verification of neural networks. Advances in Neural Information
Processing Systems, 32:9835-9846, 2019b.
Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Fast certified robust
training with short warmup. In ICML 2021 Workshop on Adversarial Machine Learning, 2021.
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In International Conference on Ma-
chine Learning, volume 139, pp. 9756-9766. PMLR, 2021.
Sahil Singla, Surbhi Singla, and Soheil Feizi. Improved deterministic l2 robustness on CIFAR-10
and CIFAR-100. In International Conference on Learning Representations, 2022.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020.
Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
In International Conference on Learning Representations, 2021.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in neural information
processing systems, pp. 6541-6550, 2018.
Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the
dangers of evaluating against weak attacks. In International Conference on Machine Learning,
pp. 5025-5034. PMLR, 2018.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. Advances in Neural Information Processing Systems, 31, 2018.
Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network
robustness verification. Advances in Neural Information Processing Systems, 34, 2021.
13
Published as a conference paper at ICLR 2022
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In International Conference on
Learning Representations, 2020.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning, pp. 5276-5285. PMLR, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8410-8419, 2018.
Yihan Wu, Aleksandar Bojchevski, Aleksei Kuvshinov, and StePhan Gunnemann. Completing the
picture: Randomized smoothing suffers from the curse of dimensionality for a large family of
distributions. In International Conference on Artificial Intelligence and Statistics, pp. 3763-3771.
PMLR, 2021.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Mahi Shafiullah, and Aleksander Madry. Training for
faster adversarial robustness verification via inducing relu stability. In International Conference
on Learning Representations, 2019.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified
robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693-
10705. PMLR, 2020a.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. In Proceedings of the 34rd International Confer-
ence on Neural Information Processing Systems, 2020b.
Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, and Bo Li. On the certified robust-
ness for ensemble models and beyond. In International Conference on Learning Representations,
2022.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius.
In International Conference on Learning Representations, 2020.
Bohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying l-infinity ro-
bustness using neural networks with l-inf-dist neurons. In International Conference on Machine
Learning, pp. 12368-12379. PMLR, 2021.
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Black-box certification
with randomized smoothing: A functional optimization based framework. Advances in Neural
Information Processing Systems, 2020a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482. PMLR, 2019a.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In International Conference on Learning Representations, 2019b.
14
Published as a conference paper at ICLR 2022
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. Advances in Neural Information
Processing Systems, 31:4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In
International Conference on Learning Representations, 2020b.
15
Published as a conference paper at ICLR 2022
A	Proof of Theorem 3.2 and B eyond
Theorem A.1. LetD be a dataset with n elements satisfying the r-separation condition with respect
to '∞-norm. Then there exists a two-layer '∞-distance net with hidden size n, such that the certified
'∞ robust accuracy is 100% on D under perturbation e = r.
Proof. Consider a two layer '∞-distance net g defined in Equation (2). Let its parameters be as-
signed by
w(1,i) = xi, bi(1) = 0	for i ∈ [n]	(10)
w(2,j) = C∙I(yi= j),bj2) = -C for i ∈ [n],j ∈ [K]	()
where C = 4 maxi∈[n] ∣∣Xi∣∣∞ is a constant, and I(∙) is the indicator function. The chosen of C is
large enough so that the following holds:
kxi - xjk ≤ C/2	∀(xi, yi), (xj,yj) ∈ D	(11)
For the above assignment, the first layer simply calculates the '∞-distance between X and each
sample in dataset D. We now derive the output of the second layer. We have
[g(x)]j = x(j2) = kx(1) - w(2,j) k∞ + b(2,j)
= b(2,j) + max |xi(1) - wi(2,j) |
i∈[n] i	i
= -C + max	max	|xi(1) - C|,	max	|xi(1) |
i∈[n],yi=j	i∈[n],yi 6=j
= -C + max	max	|kx - xik	- C| ,	max	kx - xik∞	(12)
i∈[n],yi=j	i ∞	i∈[n],yi 6=j	i ∞
= -C + max (C - kx - xik∞)	(13)
i∈[n],yi=j
= - min kx - xik∞.	(14)
i∈[n],yi=j
Here a core step is Equation (13) which follows by using Inequality (11) when the image x is in
dataset D.
From Equation (14) the network g can represent a nearest neighbor classifier, in that it outputs the
negative of the nearest neighbor distance between input x and the samples of each class. Therefore,
given data x = xi in dataset D, the output [g(x)]j is either 0 or less than -2r depending on whether
j = yi, due to the r-separation condition. Therefore the output margin is at least 2r. In other words,
g achieves 100% certified robust accuracy on D.	□
We now give a general result which shows that any L layer (L ≥ 2) '∞-distance net with hidden size
O(n/L + K + d) can achieve perfect certified robustness. In this general setting, the total number
of neurons in the network is thus O(n + KL + dL) which is still close to real practice.
Theorem A.2. Let D be a dataset with n elements satisfying the r-separation condition with respect
to '∞-norm. Then there exists an L-layer '∞-distance net with hidden size no more than d L-i] +
K + 2d where d is the input dimension, such that the certified '∞ robust accuracy is 100% on D
under perturbation e = r.
Proof. The basic idea is to rearrange the computation process of the two-layer network in the above
proof by order so as to satisfy the width constraint. To formulate the proof below, we first define
some notations. Define K prefix arrays hj (j ∈ [K]) as follows:
hj,k = - min kx - xik∞.	(15)
i∈[k],yi=j
Note that we want the network output to be the negative of the nearest neighbor distance of all
samples in a class j, i.e. - mini∈[n],yi=j kx - xi k∞, which corresponds to hj,n. For any hidden
layer x(l) (l ∈ [L - 1]), we separate it into four sets: I(l) = {xi(l) : i ∈ [d]}, Ie(l) = {xi(l) : d < i ≤
16
Published as a conference paper at ICLR 2022
2d}, O(l) = {Xil) : 2d < i ≤ 2d + K} and S(l) containing the rest dl-ie neurons. We also denote
O(L) = {xi(L) : i ∈ [K]} for ease of presentation.
We first make a construction in which the neurons of I(l) in each layer exactly represent the input x,
e.g. X(I) = Xi (1 ≤ i ≤ d). This is feasible since an '∞-distance neuron can represent the operation
that fetches an element of the neuron input on a bounded domain, e.g. the following operation
u(z, {w, b}) = kz - wk∞ + b = zj ∀z ∈ K	(16)
if we assign wj = -C, b = -C, wk = 0(k 6= j) where C is larger than twice the diameter of
domain K. In this way, all hidden neurons in the network can have access to the network input x.
We similarly let the neurons of Ie(l) represent the input x again, e.g. Xi(+l)d = Xi (d < i ≤ 2d).
Next, we aim at designing the following computation pattern for O(l):
O(l) = {hj d n(l — 1) e ： j ∈ K} e.g. x2d+j = hj d nd-。e .	(17)
j, I L-1 I	+j j, I L-1 I
In this way O(L) = {hj,n : j ∈ [K]}, and the network exactly represents a nearest neighbor classi-
fier which is desired. To represent O(l+1) in (17), we use the following recursive relation
X2d+lj = hj d卫e = max hj dn(i-i)e,	max	-Ilx - Xik∞ .	(18)
L-I	j,1 L-1	1 d nL-1Q e<i≤d ⅛ e,yi=j
Note that hj dn(i-i)e = χ2ld+j is already calculated in O(II in the previous layer. The left thing is to
calculate Ix - xi I∞ for all i ∈
{d 喂^ e + ι,∙∙
•, d l⅛ e}
, which can be done by the neurons of
the set S(l) in the previous layer (which will be proven later). Assume S(l) represents
S (l) = {x2d+Κ+i ： i ∈ hd L-I eiθ，	x2d+K+i = Ilx - xd n(l-1) e+ill	，	(19)
L-1	∞
then the neuron X(2ld++1j) merges the information of neuron X(2ld)+j and part of neurons X(2ld)+K+i in S(l)
depending on whether yi = j, using the construction similar to (10). In detail,
x2ld+1j) = ιιx(l)- w(l+1,2d+j)k∞+b2l+1j) = hj,d l⅛ e
(20)
w
w
(l+1,2d+j)
k
(l+1,2d+j)
2d+K+i
b(2ld++1j)
holds by assigning
-C • I(k = 2d + j ) for k ∈ [2d + K]
C • I(yd n(l-1) e+i = j) for i ∈ d L—1 ei
-C
where C is a sufficiently large constant.
Now it remains to represent S(l) in (19). We first consider the simplest case when l = 1. In this case
we can directly calculate X(21d)+K+i = ιx - xiι∞ by assigning proper weights and zero bias. Now
assume l ≥ 2. In this case, We cannot calculate the '∞-distance directly since the previous layer
has irrelavant neurons, e.g. the neurons in sets O(l-1) and S(l-1). We want to only use the sets of
neurons I(l-1) and Ie(l-1) in the previous layer.
Suppose the objective is to represent ιx - xi ι∞ for some i. Note that
ιx - xiι∞ = max max{Xk - [xi]k, [xi]k - Xk}.
k∈[d]
We assign the parameters of the '∞-distance neuron Xjl) = ∣∣x(l-1) - w(l,j)k∞ + bjI) for some j
as folloWs:
wk(l,j)	= [xi]k - C	for k	∈	d]
wd(l+,jk)	= [xi]k + C	for k	∈	d]
w2(ld,+j)k	=0	for k	∈	K + d L-1 ei
b(jl)	= -C			
17
Published as a conference paper at ICLR 2022
where C is a sufficiently large constant. In this way
xj(l) = kx(l-1) - w(l,j) k∞ + b(jl)
b(l) +	| (l-1)	(l,j) |	| (l-1)	(l,j) |	| (l-1)	(l,j) |
= bj + max max |xk - wk |, max |xd+k - wd+k |,	max	|x2d+k - w2d+k|
[k∈[d]	k∈[d]	+	+	k∈hκ+d L-i ei	+	+
= -C+max km∈a[dx](x(kl-1) - [xi]k +C),mk∈a[dx](-x(dl+-k1) + [xi]k +C)
= max max(xk - [xi]k), max(-xk + [xi]k)
= kx - xik∞
which is desired. Proof completes.	口
B Additional Experimental Details and Hyper-parameters
Our experiments are implemented using the Pytorch framework. We run all experiments in this
paper using a single NVIDIA Tesla-V100 GPU. The CUDA version is 11.2.
The learnable scalar in Equation (9) is initialized to be one and trained using a smaller learning
rate that is one-fifth of the base learning rate. This is mainly to make training stable as suggested
in Zhang et al. (2019b) since the scalar scales the whole network output. The final performance
is not sensitive to the scalar learning rate as long as it is set to a small value. For random crop
data augmentation, we use padding = 1 for MNIST and padding = 3 for CIFAR-10. The model is
initialized using identity-map initialization (see Section 5.3 in Zhang et al. (2021)), and mean-shift
batch normalization is used for all intermediate layers. The training procedure is as follows:
•	In the first e1 epochs, We set P = 8 in 'p-relaxation and use λ = λo as the mixing Coeffi-
cient;
•	In the next e2 epochs, p exponentially increases from 8 to 1000. Accordingly, λ exponen-
tially decreases from λ0 to a vanishing small value λend;
•	In the final e3 epochs, p is set to infinity and λ is set to 0.
All hyper-parameters are provided in Table 3. Most hyper-parameters are directly borrow from
Zhang et al. (2021), e.g. hyper-parameters of the optimizer, the batch size, and the value p in `p-
relaxation. The only searched hyper-parameters are the hinge threshold θ and the mixing coefficient
λ0 , λend. These hyper-parameters are obtained using a course grid search.
Table 3: Hyper-parameters used in this paper.
Dataset		MNIST				CIFAR-10			
	0.1	0.3	2/255	8/255	16/255
Optimizer		Adam(βι = 0.9, β2 = 0.99, e		= 10-i0)	
Learning rate			0.03		
Batch size			512		
pstart			8		
pend			1000		
Epochs	ei = 25, e2	二375, e3 = 50	ei = 100, e2 = 1150		e3 = 50
Total Epochs		450		1300	
Hinge threshold θ	06	09	20/255^^	48/255	80/255
Mixing coefficient λ0	0.05	0.05	0.05	0.1	0.1
Mixing coefficient λend	2 × 10-4	2 X 10-4	2 X 10-3	5 X 10-4	2 X 10-4
We also run additional experiments using the training strategy in Zhang et al. (2021) for performance
comparison when the original paper does not present the corresponding results. This mainly includes
the case = 0.1 on MNIST and = 2/255, = 16/255 on CIFAR-10, as shown in Table 2. We
use the same hyper-parameters in Zhang et al. (2021), except for the hinge threshold θ where we
perform a careful grid search. The choice of θ is listed in Table 4.
18
Published as a conference paper at ICLR 2022
Table 4: Best hinge thresholds for different settings using the training strategy in Zhang et al. (2021).
Dataset	MNIST			CIFAR-10			
	0.1	0.3	2/255	8/255	16/255
Hinge threshold θ	0.8	0.9	32/255	80/255	128/255
Loss
Zhang et al. (2021)
This paper
Lipschitz
Average Max
123.5	168.4
121.5	183.4
Figure 3: Approximating the LiPschitz constant of 'p-distance net when P = 8, trained using differ-
ent loss functions on CIFAR-10 dataset. The left figure plots the calculated value of (21) over the
test set at each quantile. The right table provides the statistical information.
C THE LIPSCHITZ Constant of 'p-distance Net
We have shown in Section 4.1 that an L layer 'p-distance net with d neurons in each hidden layer is
dL/p Lipschitz with respect to '∞-norm. The value becomes quite large if P is small. For example,
Zhang et al. (2021) uses a 6-layer `p-dist net with d = 5120. This gives a Lipschitz constant of
approximate 568 when P = 8 at the beginning of training.
one may ask whether such upper bound of Lipschitz constant (Proposition 4.1) is tight and reflects
the true Lipschitz property in practice. To validate the tightness of the bound, we run the following
experiments. Consider the '∞-distance net used in Zhang et al. (2021). We train this architecture
following the training strategy either in Zhang et al. (2021) or in this paper. After training finishes,
we then set P = 8 without changing the model parameters. We approximate the Lipschitz constant
of the model using Projected Gradient Descent (PGD), which provides a lower bound estimate. In
detail, for each image x in the test dataset, we estimate the quantity
1 m 隆kg(X+δ)- g(x)k∞
(21)
where g is the network and is a small constant taken to be 1/255. The expression (21) is clearly a
lower bound of the Lipschitz constant (can be seen as the “local Lipschitz constant” near point x).
It can be further lower bounded by using the PGD solution δ = δPGD. We run PGD for each target
label j and optimize
:ma≤j[g(x+皿 Tg(X) j1
(22)
using 20 PGD steps with step size /4.
Results are shown in Figure 3. It can be observed that the “local Lipschitz constant” around real data
points is indeed far larger than one. The average value exceeds 100 which is close to the theoretical
upper bound.
D Randomized Smoothing for '∞ Perturbations
Randomized smoothing approaches typically provide probabilistic certified guarantees for `2 per-
turbations. To apply these methods in the '∞ perturbation scenario, most of works convert the result
of '2 perturbation into '∞ perturbation using norm inequalities (Salman et al., 2019a; Blum et al.,
2020). Specifically, to certify the robustness under e-bounded '∞ perturbations, one can certify the
robustness under (e√d)-bounded '2 perturbations to obtain a lower bound estimate where d is the
input dimension. on CiFAR-10 dataset, the input dimension d = 3072. This corresponds to an
'2 perturbation radius e = 0.4347 for '∞ perturbation radius e = 2/255, and corresponds to an '2
perturbation radius e = 1.739 for '∞ perturbation radius e = 8/255.
19
Published as a conference paper at ICLR 2022
For the case = 2/255, Blum et al. (2020) directly reported a certified accuracy of 62.6% using
randomized smoothing which is currently state-of-the-art. For the case = 8/255, there are no
literature results that directly report '∞ robustness, so We use the results of '2 robustness from
representative papers (Salman et al., 2019a; Jeong & Shin, 2020). Salman et al. (2019a) reported a
certified accuracy of 24% and a clean accuracy of 53% under `2 perturbation = 1.75 (Table 17 in
their paper). Jeong & Shin (2020) reported a certified accuracy of 25.2% and a clean accuracy of
52.3% under `2 perturbation = 1.75 (Table 1 in their paper, σ = 0.5). For the case = 16/255,
all randomized smoothing methods fail and only achieve a trivial certified accuracy of 10%.
E Ablation Studies
In this section We conduct ablation experiments to the proposed loss. Let a training sample be (x, y)
where y is the label of x, and denote g(x) as the output of an '∞-distance net for input x. Let
'hinge(z,y) = max{maxi=y Zi - Zy + 1,0} and 'ce(z,y) = Iog(PiexP(Zi)) - Zy represent the
hinge loss and cross-entropy loss, respectively. We would like to justify that (i) Cross-entropy loss
can alleviate the optimization issue in 'p-relaxation (which is a better substitute over hinge loss),
but the threshold of hinge loss is also crucial as it explicitly optimizes the certified accuracy; (ii)
Combining cross-entropy loss and clipped hinge loss leads to a much better performance; (ii) Using
a decaying mixing coefficient λ can further boost the performance and stabilize the training.
We consider the following objective functions:
(1)	The baseline hinge loss: 'hinge(g(x)∕θ,y) with hinge threshold θ. This loss is used in
Zhang et al. (2021).
(2)	The cross-entropy loss: 'ce(s ∙ g(x), y) where S is a scalar (temperature). Note that the
information of the allowed perturbation radius is not encoded in the loss, and the loss only
coarsely enlarges the output margin (see Section 4.2). Therefore it may not achieve desired
certified robustness.
(3)	A variant of cross-entropy loss with threshold: 'ce(s ∙ g(x - θ1y),y) where S is a scalar
(temperature), θ is the threshold hyper-parameter and 1y is the one-hot vector with the yth
element being one. Intuitively speaking, we subtract the yth output logit by θ before taking
cross-entropy loss. Compared to the above loss (2), now the information is encoded
in the threshold hyper-parameter θ. We point out that this loss can be seen as a smooth
approximation of the hinge loss.
(4)	The combination of cross-entropy loss and clipped hinge loss: X'ce(s ∙ g(x),y) +
min('hinge(g(x)∕θ,y), 1) with a fixed mixing coefficient λ.
(5)	The combination of cross-entropy loss and clipped hinge loss: X'ce(s ∙ g(x),y) +
min('hinge(g(x)∕θ, y), 1) with a decaying λ. The loss is used in this paper.
We keep the training procedure the same for the different objective functions above. The hyper-
parameters such as θ and λ are independently tuned for each objective function to achieve the best
certified accuracy. The scalar S is a learnable parameter in each loss except for objective function
(2) where we tune the value of S. For other hyper-parameters, we use the values in Table 3. We
independently run 5 experiments for each setting and the median of the performance is reported.
Results are listed in Table 5, and the bracket in Table 5(b) shows the standard deviation over 5 runs.
= 8∕255 on CIFAR-10).
(b) Performance using objective function (4)
with different mixing coefficient λ.
Table 5: Performance of ablation studies (
(a) Performance of different objective functions with best
hyper-parameters.
Loss	Clean	Certified	Hyper-parameters	λ	Clean	Certified
(1)	56.80	33.30	θ = 80/255	0.1	58.99(±0.35)	37.67(±0.25)
(2)	55.58	33.23	s = 1.0	0.05	56.50(±0.25)	38.82(±0.14)
(3)	53.37	34.91	θ = 32/255	0.02	53.51(±0.40)	39.24(±0.37)
(4)	53.51	39.24	θ = 48/255, λ = 0.02	0.01	50.48(±1.83)	38.05(±1.31)
(5)	54.30	40.06	θ = 48/255, λ = 0.1 1 0	0.005	47.51(±5.03)	37.03(±2.64)
				0	10.0	10.0
20
Published as a conference paper at ICLR 2022
We can draw the following conclusions from Table 5:
•	Hinge loss and cross-entropy loss are complementary. Cross-entropy is better in the early
training phase when the Lipschitz constant is large, while hinge loss is better for certified
robustness when the model is almost 1-Lipschitz in the later training phase. This can be
seen from the results of objective functions (1-3) in Table 5(a), where (3) incorporates
cross-entropy loss and the threshold in hinge loss, and outperforms both (1) and (2) by a
comparable margin.
•	Combining cross-entropy loss and clipped hinge loss leads to much better performance.
This can be seen from the result of the objective function (4), which significantly outper-
forms (1-3). However, this loss is very sensitive to the hyper-parameter λ as is demonstrated
in Table 5(b). If λ is too large, the certified accuracy gets worse. If λ is too small, the train-
ing becomes unstable and the clean accuracy drops significantly. In the extreme case when
λ = 0, the loss (4) reduces to the clipped hinge loss and the optimization fails because
clipped hinge loss does not optimize for wrongly-classified samples.
•	Using a decaying mixing coefficient λ can further boost the performance and stabilize the
training. In contrast to the loss (4), we will show in Appendix F that the proposed objective
function (5) in this paper is not sensitive to hyper-parameter λ.
F	Sensitivity Analysis
In this section we provide sensitive analysis of the proposed objective function (9) with respect to
hyper-parameters. We consider the setting = 8/255 on CIFAR-10 dataset. For each choice of
hyper-parameters, we independently run 3 experiments and the median of the certified accuracy is
reported.
The hinge threshold θ. The results are already plotted in Figure 2(b). We list the concrete numbers
below.
Table 6: Sensitive Analysis over hyper-parameter θ.
θ	3e	4	5	6e	7	8	9	10
Certified	35.23	38.47	39.55	40.06	39.46	39.05	38.68	38.31
The mixing coefficients λ0 and λend . The results are already shown in Figure 2(c).
The number of epochs. Our best result reported in Table 2 is trained for 1300 epochs, which
is longer than Zhang et al. (2021). We also consider using the same training budget by setting
e1 = 100, e2 = 650, e3 = 50 in Table 3. This yields a total of 800 training epochs. In this way we
can achieve 54.52 clean accuracy and 39.61 certified accuracy.
Adam hyper-parameters. While we use the same Adam hyper-parameters as Zhang et al. (2021),
note that the values are different from the default numbers in Pytorch (e,g. β2 = 0.999 and =
10-8). Instead, we use β2 = 0.99 and = 10-10 in all experiments.
For e, We find the value is essential to be small, because for the 'p-distance function, the gradients of
most of the elements are close to zero when p is large. For Adam optimizer (Kingma & Ba, 2015),
the update is Written as
mt
Wt+1 = Wt - lr ∙ L ,-.	(23)
√Vt + e
Then a large e Will dominate the denominator in Adam if vt is close to zero, Which severely Weakens
the parameters’ update and leads to Worse performance. We find the value of e is not sensitive if it
is small enough, i.e. We can obtain almost identical performance for e = 10-10, 10-11 or 10-12.
The use of a smaller β2 is mainly because of the `p relaxation. Since p increases exponentially
during training, the netWork function changes through time, therefore the long-time-ago historical
gradient information Will become meaningless and even do harm to the training. This is Why a
smaller β2 is used, so that the second-order momentum (the term vt) in Adam only depends on
recent gradient information. We find it is also OK to choice β2 =0.98 or 0.995, and the certified
accuracy can reach 39.5%. HoWever, the default value of 0.999 is too large, as it corresponds to 10
times more historical information than the value of 0.99.
21