Published as a conference paper at ICLR 2022
Provable Learning-based Algorithm
For Sparse Recovery
Xinshi Chen & Haoran Sun
School of Mathematics
Georgia Institute of Technology
Atlanta, USA
{xinshi.chen,haoransun}@gatech.edu
Le Song
Machine Learning Department
MBZUAI & BioMap
UAE & China
songle@biomap.com
Abstract
Recovering sparse parameters from observational data is a fundamental problem
in machine learning with wide applications. Many classic algorithms can solve
this problem with theoretical guarantees, but their performances rely on choosing
the correct hyperparameters. Besides, hand-designed algorithms do not fully ex-
ploit the particular problem distribution of interest. In this work, we propose a
deep learning method for algorithm learning called PLISA (Provable Learning-
based Iterative Sparse recovery Algorithm). PLISA is designed by unrolling a
classic path-following algorithm for sparse recovery, with some components be-
ing more flexible and learnable. We theoretically show the improved recovery ac-
curacy achievable by PLISA. Furthermore, we analyze the empirical Rademacher
complexity of PLISA to characterize its generalization ability to solve new prob-
lems outside the training set. This paper contains novel theoretical contributions to
the area of learning-based algorithms in the sense that (i) PLISA is generically ap-
plicable to a broad class of sparse estimation problems, (ii) generalization analysis
has received less attention so far, and (iii) our analysis makes novel connections
between the generalization ability and algorithmic properties such as stability and
convergence of the unrolled algorithm, which leads to a tighter bound that can
explain the empirical observations. The techniques could potentially be applied to
analyze other learning-based algorithms in the literature.
observations Z1.n	true parameter β *
1	Introduction
The problem of recovering a sparse vector β* from finite ob-
servations ZLn 〜(Pq* )n is fundamental in machine learning,
covering a broad family of problems including compressed
sensing, sparse regression analysis, graphical model estima-
tion, etc. It has also found applications in various domains.
For example, in magnetic resonance imaging, sparse signals
need to be reconstructed from measurements taken by a scan-
ner. In computational biology, estimating a sparse graph struc-
ture from gene expression data is important for understanding
gene regulatory networks.
Figure 1: Sparse recovery problems.
gene regulatory network
Various classic algorithms are available for solving sparse recovery problems. Many of them come
with theoretical guarantees for the recovery accuracy. However, the theoretical performance often
relies on choosing the correct hyperparameters, such as regularization parameters and the learning
rate, which may depend on unknown constants. Furthermore, in practice, similar problems may
need to be solved repeatedly, but it is hard for classic algorithms to fully utilize this information.
To alleviate these limitations, we consider the approach of learning-to-learn and propose a neural
algorithm, called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm). PLISA
is a deep learning model that takes the observations Z1:n as the input and outputs an estimation for
β* . To make use of classic techniques developed by domain experts, we design the architecture
of PLISA by unrolling and modifying a classic path-following algorithm proposed by Wang et al.
1
Published as a conference paper at ICLR 2022
(2014). To benefit from learning, some components in this classic algorithm are made more flexible
with careful design and treated as learnable parameters in PLISA. These parameters can be learned
by optimizing the performances on a set of training problems. The learned PLISA can then be used
for solving other problems in the target distribution.
With the algorithm design problem converted to a deep learning problem, we ask the two fundamen-
tal questions in learning theory:
1.	Capacity: What’s the recovery accuracy achievable by PLISA? Can the flexible components in
PLISA lead to an algorithm which effectively improves the recovery performance?
2.	Generalization: How well can the learned PLISA solve new problems outside the training set?
Is the generalization behavior related to the algorithmic properties of PLISA?
Aiming at supplying rigorous answers to these questions, we conduct theoretical analysis for PLISA
to provide guarantees for its representation and generalization ability. The results and the techniques
in our analysis can distinguish our work from existing studies on algorithm learning. We summarize
our novel contributions into the following three aspects.
1.	Theoretical understanding. In contrast to the plethora of empirical studies on algorithm learn-
ing, there have been relatively few studies devoted to the theoretical understanding. Existing the-
oretical efforts primarily focus on analyzing the convergence rate achievable by the neural algo-
rithm (Chen et al., 2018; Liu et al., 2019a; Zhang & Ghanem, 2018; Wu et al., 2020), but the
generalization error bound has received less attention so far. A substantial body of works only argue
intuitively that algorithm unrolling architectures can generalize well because they contain a small
number of parameters. In comparison, we provide theoretical guarantees for both the capacity and
the generalization ability of PLISA, which are more solid arguments.
2.	General setting. The problem setting in this paper is new and more challenging. Existing
works mainly focus on a specific problem. For example, the compressed sensing problem with a
fixed design matrix is the mostly investigated one. PLISA, however, is generic and is applicable to
various sparse recovery problems as long as they satisfy certain conditions in Assumption C.1.
3.	Novel connection. The algorithmic structure in PLISA can make it behaves differently from
conventional neural networks. Therefore, we largely utilize the analysis techniques in classic algo-
rithms to derive its generalization bound. By combining the analysis tools of deep learning theory
and optimization algorithms, our result reveals a novel connection between the generalization ability
of PLISA and the algorithmic properties including the convergence rate and stability of the unrolled
algorithm. Benefit from this connection, our generalization bound is tight in the sense that it matches
the interesting behavior of PLISA observed in experiments - the generalization gap could decrease
in the number of layers, which is rarely observed in conventional neural networks.
2 PLISA: Learning To Solve Sparse Estimation Problems
A sparse estimation problem is to recover β* from finite observations Zi：n sampled from Pq* .
As a concrete example, in a sparse linear regression problem, n observations {Zi = (xi, yi)}in=1
are sampled from a linear model y = x>β* + e, and an algorithm needs to estimate β* from n
observations. Classic algorithms recover β* by minimizing a regularized empirical loss:
βbλ ∈ argminLn(Z1:n, β) + P(λ, β),	(1)
where Ln is an empirical loss that measures the “fit” between the parameter β and observations
Z1:n, and P (λ, β) is a sparsity regularization with coefficient λ. When Ln is the least square loss
and P (λ, β) is λkβ k1 , the optimization is known as LASSO and can be solved by the well-known
algorithm ISTA (Daubechies et al., 2004). Based on the idea of algorithm unrolling, Gregor &
LeCun (2010) proposed LISTA, a neural algorithm that interprets ISTA as layers of neural networks.
It has been demonstrated that LISTA outperforms ISTA thanks to its learnable components. Since
then, designing neural algorithms by unrolling ISTA has become an active research topic. However,
existing works mostly focus on the compressed sensing problem with a fixed design matrix only.
To enable for more general applicability, we design the architecture of PLISA by unrolling a clas-
sic path-following algorithm called APF (Wang et al., 2014) instead of ISTA. APF is applicable
to nonconvex losses and nonconvex penalty functions, covering a considerably larger range of ob-
jectives than LASSO. Designing the architecture based on APF allows PLISA to be applicable to
2
Published as a conference paper at ICLR 2022
a broader class of problems such as nonlinear sparse regression, graphical model estimation, etc.
Furthermore, employing nonconvexity can potentially lead to better statistical properties (Fan & Li,
2001; Fan et al., 2009; Loh & Wainwright, 2015), for which we will explain more in Section 3.
In the following, we will introduce APF and the architecture of our proposed PLISA. After that, we
will describe how to optimize the parameters in PLISA under the learning-to-learn setting.
2.1	A Brief Introduction to APF
We briefly introduce the classic algorithm APF (Wang et al., 2014), and its details are presented in
Algorithm 3 in Appendix I. The key idea of path-following algorithms is creating a sequence of T
many sub-objectives to gradually approach the target objective that is supposed to be more difficult
to solve. More specifically, APF approximates the local minimizers of a sequence of sub-objectives:
βt ≈ βλt ∈ arg min Ln(Zi：n, β) + P(λt, β), for t = 1,…，T,	(2)
β
where λι > λ2 > •- > λτ is a decreasing sequence of regularization parameters. The last
parameter λT is the target regularization parameter. As a result, APF contains T blocks, and each
block contains an iterative algorithm that minimizes one sub-objective in Eq. 2. The output of the
0
(t - 1)-th block, denoted by βt-1, is used as the initialization of the t-th block, i.e., βt0 = βt-1.
Then the t-th block minimizes the t-th sub-objective by the modified proximal gradient algorithm:
for k = 1,…，K,	βk	一 T. (βk-1 -	α(VβLn(ZI：n βk-1)	+ VβQ(%, βk-1)))	.	(3)
output of t-th block:	βt	= βetK .	(4)
The notation Tδ(β) := sign(β) max {∣β∣ 一 δ, 0} is the soft-thresholding function, and the function
Q is the concave component ofP defined as Q(λ, β) := P(λ, β) - λkβk1. The number of steps K
in each block is determined by certain stopping criteria. It can be seem that the update steps in each
block is similar to the ISTA algorithm, but it is modified in order to incorporate nonconvexity.
2.2	Architecture OF PLISA
The architecture of PLISA is designed by unrolling the APF algo-
rithm, and augmenting some learnable parameters θ. Therefore, the
architecture of PLISAθ contains T blocks and each block contains K
layers defined by the K-step algorithm in Eq. 3 (See Figure 2). Note
that in PLISAθ, both K and T are pre-defined. The architecture of
PLISAθ is different from APF as summarized below:
1.	Element-wise and learnable regularization parameters. Most
classic algorithms including APF employ a uniform regulariza-
tion parameter λt across all entries of β, but PLISAθ uses a d-
dimensional vector λt = [λt,ι,…，λt,d]> to enforce different
levels of sparsity to different entries in β. Furthermore, the reg-
ularization parameters in PLISAθ are learnable, which will be op-
timized during the training.
2.	Learnable penalty function: classic algorithms use a pre-defined
sparse penalty function P, but PLiSAθ parameterizes it as a com-
bination of q different penalty functions and learns the weights of each penalty. In other words,
PLISAθ can learn to select a specific combination of the penalty functions from training data.
3. Learnable step size: The step sizes in APF are selected by line-search but they are learnable in
PLISAθ. Experimentally, we find the learned step sizes lead to a much faster algorithm.
In later sections of this paper, we will show how such differences can make PLISAθ perform better
than APF both empirically and theoretically.
Algorithm 1 and 2 present the mathematical details of the architecture, follow which we explain
some notations and definitions. Red-colored symbols indicate learnable parameters in PLISAθ.
CellK"
algorithm steps
VLn(ZI：“ 0)
↑
00 = 0
↑
β2
↑
cellw,α
algorithm steps
↑
βl
Celka
algorithm steps
^l:n
input
λτ
^2
η,λ*
η, 2*
↑
Figure 2: Architecture.
η, 2*

3
Published as a conference paper at ICLR 2022
Algorithm 1: PLISAθ architecture #blocks: T , #layers per block: K Parameters: θ = {η, λ*, w, α}	Algorithm 2: Layers in each block BlOCkw,α
	Input: Zi：n, βt-ι, λt βet0 J βt-1
Input: samples Z1:n	
β0 - 0,	λ0 - PeLn(ZI:n, 0)	For k = 1, . . . , K do
1 For t = 1, . . . , T do	j	~八	-1 .	~八	-1 . 1	gk JVeLn(ZI:n,βk-1) + VeQw(λt,βk-1)
λt — max {σ(η) ◦ λt-i,λ*} βt J BlOCkw,a(Zl：n, βt-1,λt) return βT	βk jTɑ∙λt (βk-1- α ∙ gk) return βt = βetK
Regularzation parameters. In PLISAθ, the element-wise regularization parameters are initialized
by a vector λ0 := PeLn(Zin, 0), and then updated sequentially by
λt — max {σ(η) ◦ λt-i,λ*} ,	(5)
where σ(∙), ◦, and max{∙, ∙} are element-wise sigmoid function, multiplication, and maximization.
{η, λ*} are both d-dimensional learnable parameters. Eq. 5 creates a sequence λι,…，λτ through
the decrease ratio σ(η), until they reach the target regularization parameters λ*.
Penalty function. PLISAθ parameterizes the penalty function as follows,
Pw (λ, β) = pq=ι f ∙ P⑴(λ, β), where f = Pqeχ⅛wi)w0).	⑹
In other words, Pw is a learnable convex combination of q penalty functions (P⑴，…，P(q)). The
weights of these functions are determined by learnable parameters W = [wι, •一，Wq]. In this paper,
we focus on learning the combination of three well-known penalty functions:
P(1)(λ,β) = kλ ◦ βk1, P(2) (λ, β) =Pjd=1MCP(λj,βj), P(3) (λ, β) =Pjd=1SCAD(λj,βj),
where P(1) is convex, and MCP (Zhang, 2010a) and SCAD (Fan & Li, 2001) are nonconvex penal-
ties whose analytical forms are given in Appendix B. One can include any other penalty functions as
long as they satisfy a set of conditions specified in Appendix B. Qw (λ, β) := Pw (λ, β) 一∣∣λ ◦ βkι
represents the concave component ofPw. The analytical form of PβQw(λt, β) are in Appendix B.
2.3 Learning-to-learn Setting
Now we describe how to train the parameters θ in PLISAθ under the learning-to-learn setting.
Training set. Similar to other works in this domain, we assume the access to m problems from the
target problem-space P, and use them as the training set:
Dm = {(Z([, β* ⑴),…，(ZImm, β*(m))} with (Z(in i, β*(i)) ∈P.
Here each estimation problem is represented by a pair of observations and the corresponding true
parameter to be recovered. A different problem i can contain a different number ni of observations.
Training loss. Since the intermediate outputs βt(Z±n; θ) of plisaθ are also estimates of β*, a
common design of the training loss is the weighted sum of the intermediate estimation errors (Chen
et al., 2021). More specifically, we employ the following training loss:
mT	2
LYrain(Dm； θ) ：= mm XX YTTB(Zan. ； θ) —♦* (II 2,	⑺
i=1 t=1
where γ < 1 is a discounting factor If γ = 0 then the loss is only estimated at the last layer.
Generalization error. The ultimate goal of algorithm learning is to minimize the estimation error
on expectation over all problems in the target problem distribution:
Lgen(P(P)； θ) ：= E(ZLn,β*)〜P(P) 1叔h(％" 0) 一 β*k2 ,	⑻
where P(P) is a distribution in the target problem-space P. Let θ* ∈ arg min LYr=0n(Dm； θ) be a
minimizer of the training loss. It is well-known that the generalization error can be bounded by:
Lgen (P(P )； θ*) ≤ Lgen(P(P )；- L=n6m； θ*)+ LMOn(Dm； θ*) .	(9)
x---------------{---------------} X--------{-------}
generalization gap: Theorem 4.1	training error: Theorem 3.1
We will theoretically characterize these two terms in Theorem 3.1 and Theorem 4.1.
4
Published as a conference paper at ICLR 2022
3 Capacity of PLISA
Can PLISAθ achieve a small training error without using too many layers? How can designs of
element-wise regularization and learnable penalty functions help PLISAθ to achieve a smaller train-
ing error compared to classic algorithms? We answer this question theorectically in this section.
3.1	First Main Result: Capacity
Let et(Zi：n； θ) be the output of the t-th block in PLiSAθ. Let X ∨ a denote entry-wise maximal
value max{x, a}. Let (x)S denote the sub-vector of x with entries indexed by the set S.
Theorem 3.1 (Capacity). Assume the problem space P satisfies Assumption C.1 and Dm ⊆ P. Let
T be the number of blocks in PLISAθ and let K be the number of layers in each block. For any
ε > 0, there exists a set of parameters θ = {η, λ*, w, α} such that the estimation error of every
problem (Zi：n, β*) ∈ Dm is bounded as follows, ∀T > to,
IleT(Zi：n； θ) — β"∣2 ≤ ε-1cθs* exp(-CθK(T - to))	optimization error	(10)
+ cθKmk (VβLn(Zi：n, β*) ∨ £)5* ∣2, Statisticalerror	(11)
where S* := supp(β*) is the support indices of β*, cθ, C§, and Cθ are some positive values de-
pending on the chosen θ, and κm is a condition number which reveals the similarity of the problems
in Dm. Note that K and to are required to be larger than certain values, but we will elaborate in
Appendix E that the required lower bounds are small. See Appendix Efor the proof of this theorem.
This estimation error can be interpreted as a combination of the optimization error (in Eq. 10) and the
statistical error (in Eq. 11). The optimization error decreases linearly in both K andT. The statistical
erroroCcUrs because of the randomness in Zi：n. The gradient at the true parameter Ve Ln (Zi：n, β*)
characterizes how well the finite samples Z1:n can represent the distribution Pβ* .
A direct consequence of Theorem 3.1 is that the training error can be small without using too many
layers and blocks in PLISAθ. We will also elaborate on how the entry-wise regularization and
learnable penalty function can effectively reduce the training error in the following.
(i)	Impact of entry-wise regularization. Restricting the regularization to be uniform across entries
will lead to an error bound that replaces the statistical error ∣∣ (VβLn(Z[n, β*) ∨ ε)s* ∣∖ in Eq. 11
by √S* (∣∣VβLn(ZLn, β*)k∞ ∨ ε). To understand how the former has improved the latter, We
can consider the sparse linear regression problem. If the design matrix is normalized such that
maxι≤j≤d k([xιj,…，[xnj )∣∣2 ≤ √n, then ∣∣ (Vβ Ln(ZI：n, β*) ∨ ε%* ∣∣2 ≤ C√s*∕n with high
probability. In comparison, √77∣∣VβLn(Zi：n, β*)k∞ ≤ Cy∕s*logd∕n with high probability is a
slower statistical rate due to the term log d.
(ii)	Impact of learnable penalty function. To explain the the benefit of using learnable penalty
function, we give a more refined bound for the statistical error in Eq. 11 in the following lemma.
Lemma 3.1 (Refined bound). Assume the same conditions and parameters θ in Theorem 3.1. As-
sume T → ∞ so that the optimization error can be ignored. For simplicity, assume wf3 = 0 and only
consider the weights wf1 and wf2 for `1 penalty and MCP. Then for every problem (Z1:n, β*) ∈ Dm:
kβ∞(Zm; θ) - β*k2 ≤ 1+8f)Km k(Vβ Ln(ZI:n, β*) ∨ ε)s* ∣2	(S*: Small ∖βj∖'s)	(12)
+ 1+8fKm k(Vβ Ln(ZI:n, β*) ∨ ε)s* ∣2	(S*: Large ∖βj∖'s), (13)
where b > 1 is a hyperparameter in MCP, and the index sets S1* and S2* are defined as S1* := {j ∈
S* : ∖∖βj* ∖∖ ≤ bλj*} and S2* := {j ∈ S* : ∖∖βj* ∖∖ > bλj*}. See Appendix E for the proof.
This refined bound reveals the benefit of learning the penalty function because:
1.	According to Lemma 3.1, the optimal penalty function is problem-dependent. For example, if
(8(bρ- + 1)κm + 1)∣(Vβ Ln (Z1:n, β*) ∨ ε)S1* ∣2 > (8(bρ- - 1)κm - 1)∣(Vβ Ln (Z1:n, β*) ∨
ε)S* ∣2, choosing wf2 = 0 can induce a smaller error bound. Otherwise, wf2 = 1 is better.
Therefore, learning is a more suitable way of choosing the penalty function.
2.	The convergence speed Cθ in Eq. 10 is also affected by the weights, monotonely decreasing
in wf2 . Through gradient-based training, we can automatically find the optimal combination of
penalty functions to strike a nice balance between the statistical error and convergence speed.
5
Published as a conference paper at ICLR 2022
4 Generalization Analysis
How well can the learned PLISAθ solve new problems outside the training set? In this section, we
conduct the generalization analysis in a novel way to focus on answering the questions:
How is the generalization bound of PLISAθ related to its algorithmic properties?
And how is it different from conventional neural networks?
4.1 Second Main Result: Generalization Bound
To analyze the generalization properties of neural networks, many works have adopted the analysis
framework of Bartlett & Mendelson (2002) to bound the Rademacher complexity via Dudley’s in-
tegral (Bartlett et al., 2017; Chen et al., 2019; Garg et al., 2020; Joukovsky et al., 2021). A key step
in this analysis framework is deriving the robustness of the training loss to the small perturbation in
model parameters θ. Since we can view PLISAθ as an iterative algorithm, we borrow the analysis
tools of classic optimization algorithms to derive its robustness in θ. The following lemma states this
key intermediate result, which connects the Lipschitz constant to algorithmic properties of PLISAθ.
Lemma 4.1 (Robustness to θ). Assume P satisfies Assumption C.1 and Dm 〜P(P)m. Assume
PLISAθ contains T > t0 blocks and K layers. Consider a parameter space Θ in which the param-
eters SatiSfy (i) α ∈ 际皿 ρ+ ] ,(ii) % ∈ [σ-1(0.9),ηmaχ] ,(iii) fɪ + wɜ a-i ≤ ξmax < P-, and
(iv) λj ∈ [8sup(Zinβ* )∈Dm ∣Vβ Ln(Zi:n, β*)]j I ∨ ε,λmaχ] With Some positive constants amin,
ηmax, ξmax, and λmaχ. ThenfOr any θ = {η, λ*, w,α} and θ0 = {η0, λ*0, w0,a0} in Θ, and for
any recovery problem (Zin, β*) ∈ Dm, thefollowing inequality holds,
kβτ(Zi：n； θ) — βτ(Zi：n； θ0)∣∣2 ≤ c*(T - to)√S* ∣α - α01 exp(—C©K(T - to))
|
}
{^^^^^^^^^^^^^^^
convergence rate
+ (c2∣lη - η0k2 + c3∣∣λ* - λ*0ll2 + c4√dkw - wlQ (I- eχpJCθKT)),
(14)
(15)
X-----------------{----------------}
stability rate
where c1, c2, c3, c4 and CΘ are some positive constants. Note that similar to Theorem 3.1, K and to
are required to be larger than certain small values. See Appendix F.1 for the proof.
Convergence rate & step size perturbation. In Eq. 14, the Lipschitz constant in the step size α
scales at the same rate as the convergence rate of PLISAθ, decreasing exponentially in T and K
(See Fig. 3 for a visualization). To understand this, consider when both step sizes α and α0 are within
the convergence region (i.e., (0, ρ+-1]). After infinitely many steps, their induced outputs will both
converge to the same optimal point. This intuitively explains why the output perturbation caused by
α-perturbation has the same decrease rate as the optimization error.
Stability rate & regularization perturbation.
In the literature of optimization, stability of an
algorithm expresses its robustness to small per-
turbation in the optimization objective. This
is clearly related to the robustness of PLISAθ
to the perturbation in η, λ*, w, because these
parameters jointly determine the regularization
Pw (λt, β), which is a part of the optimiza-
tion objective. Therefore, we exploit the analy-
sis techniques for algorithmic stability to derive
the robustness in (η, λ*, w)-perturbation and ob-
tain the Lipschitz constant in Eq. 15, which is
bounded but increasing in T and K (See Fig. 3
for a visualization).
Convergence Rate
Stability Generalization Bound
Figure 3: Visualization of convergence, stability,
and generalization bound in Theorem 4.1. The two
sets of visualizations are obtained by choosing differ-
ent speeds CΘ in the convergence rate and stability.
Based on the key result in Lemma 4.1, we can apply Dudley’s integral to measure the empirical
Rademachar complexity which immediately yields the following generalization bound.
6
Published as a conference paper at ICLR 2022
Theorem 4.1 (Generalization gap). Assume the assumptions in Lemma 4.1. For any > 0, with
probability at least 1 - , the generalization gap is bounded by
Lgen(P(P)； θ) — L；在n(Dm； θ) ≤ Q PmTlog(4「1) +	(16)
c2 m-1 log (√mKTexp(-CθK(T — t0)) ∨ 1)+ c3dm-1 log (√m(1 — exp(-CθKT))),
S----------{z-------}	'---------{z------}
convergence rate	stability
where c1, c2, c3, CΘ are constants independent of d, m, K and T. See Appendix Ffor the proof.
Fig. 3 visualizes how the generalization bound in Theorem 4.1 grows when KT increases. The two
sets of plots look slightly different by picking different constants CΘ . We have also tried varying
the values of c2, c3, d, m in Theorem 4.1. Overall, they lead to the two types of behaviors in Fig. 3.
An important observation in Theorem 4.1 and Figure 3 is that the generalization gap could decrease
in the number of layers, and we will see in Section 7 that this matches the empirical observations.
It also distinguishes algorithm-unrolling based architectures from conventional neural networks,
whose generalization gaps rarely decrease in the number of layers.
Remark. The above generalization results are conducted on a constrained parameter space (as de-
scribed in Lemma 4.1) so that we can utilize the algorithmic properties of PLISAθ. We focus on
this space because the analysis contains more interesting and new ingredients. For parameters out-
side this space, the analysis procedure is similar to other conventional recurrent networks. Since the
bound in Theorem 4.1 has matched the empirical observations, it is reasonable to believe that after
training, the learned parameters are likely to be in this ‘nice’ constrained space.
5 Extension To Unsupervised Learning-to-learn Setting
Real-world datasets may not contain the ground-truth parameters β*, but only contain the samples
from each task, Dm = {Z(1),…，Z(m)}. In this setting, we can construct an unsupervised training
loss to minimize the empirical loss function Ln (e.g., the likelihood function) on the samples.
1m
UnSUPerviSedtrainingloSS： LUrain(Dm； θ) :=—xLn2(Z(：n2,βτ(Z(：n 1 ； θ))	(17)
m i=1
In this loss, both Z1(:in) and Z1(:in) are subsets of Z1(:in) . The samples Z1(:in) are used as the input to
PLISAθ and the samples Z1(:in) are used for evaluating the output βT from PLISAθ.
Let θU ∈ arg min Lrain (Dm; θ) be a minimizer to this unsupervised loss. Theoretically, to bound
the generalization error of θU, We can show that
Lgen(P(P )您)≤ C Pi:1 (Ln2(Z(in 2 , βτ (呢[「斯))- Ln2 (Z(i[ 2 ,。*(”)	(18)
、------------------------------V-------------------------------}
unsupervised training error
+ Lgen(P(P )； θU ) - LtUn(Dm； θ1 ) + C PmjIVe Ln (喇 2 ,。*(力||2 .	(19)
、------------------{-----------------} 、-----------------V-------------------}
generalization gap: Theorem 4.1	statistical error
Compared to Eq. 9, this upper bound contains an additional statistical error, which appears because
of the gap between the unsupervised loss and the true error that we aim to optimize. Clearly, in this
upper bound, the generalization gap can be bounded by Theorem 4.1. Furthermore, the unsupervised
training error in Eq. 18 can be bounded by combining Theorem 3.1 with the following in equality.
Ln2 (Zl:n2 , βτ(Zln ； θU)) — Ln2 (Zl:n2 , β*)	(20)
≤	Ln2 (Z1:n2 , βT (Z1:ni ； θ")) - Ln2 (Z1:n2 , β*)
≤	kVβ Ln2 (Zl:n2 , β*)k2 kβτ (Z-; θ*)- β*∣∣2	+ ρ+	kβτ (Zl:n ;	θ*)- β*k2.	(21)
s------------{----------} s------------------{------------}
bounded by Theorem 3.1	bounded by Theorem 3.1
More details of the extension to unsupervised setting can be found in Appendix H.
7
Published as a conference paper at ICLR 2022
6	Related Work
Learning-to-learn has become an active research direction in recent years (Bora et al., 2017;
Franceschi et al., 2017; Niculae et al., 2018; DeneVi et al., 2018; Pogancic et al., 2019; LiU et al.,
2019b; Berthet et al., 2020). Many works share the idea of unrolling or differentiating through al-
gorithms to design the architecture (Yang et al., 2017; Borgerding et al., 2017; Corbineau et al.,
2019; Xie et al., 2019; ShriVastaVa et al., 2020; Chen et al., 2020a; Wei et al., 2020; Indyk et al.,
2019; GroVer et al., 2019; Wu et al., 2019). A well-known example of learning-based algorithm is
LISTA (Gregor & LeCun, 2010) which interprets ISTA (Daubechies et al., 2004) as layers of neu-
ral networks and has been an actiVe research topic (Zhang & Ghanem, 2018; KamiloV & Mansour,
2016; Chen et al., 2018; Liu et al., 2019a; Wu et al., 2020; Kim & Park, 2020).
HoweVer, the generalization of algorithm learning has receiVed less attention. The only exceptions
are seVeral works. HoweVer, Chen et al. (2020b) and Wang et al. (2021) only consider learning
to optimize quadratic losses. Behboodi et al. (2020); JoukoVsky et al. (2021) do not connect the
generalization analysis with algorithmic properties to proVide tighter bounds as in our work. Unlike
our work that analyzes the Lipschitz continuity (Lemma 4.1), the work of Gupta & Roughgarden
(2017); Balcan et al. (2021) studied the generalization of learning-based algorithms with a focus
on scenarios when the Lipschitz continuity is unaVailable. We will refer the audience to Shlezinger
et al. (2020); Chen et al. (2021) for a more comprehensiVe summary of related works.
7	Experiments
7.1	Synthetic Experiments
In synthetic datasets, we consider sparse linear regression problems and sparse precision matrix
estimation problems for Gaussian graphical models. Specifically, We recover target vectors β* ∈
Rd, where d = {256,1024} in SLR, and estimate precision matrices Θ* ∈ Rd×d, where d =
{50, 100} in SPE. See Appendix I.2 for descriptions of the dataset and data preparation.
7.1.1	Sparse Linear Regression (SLR)
In this experiment, we compare PLISA with several
baselines and also verify the theorems.
Performance comparison. We consider baselines
including APF (Wang et al., 2014), ALISTA (Liu
et al., 2019a), RNN (Andrychowicz et al., 2016), and
RNN-'ι. APF is the path-following algorithm that
is used as the basis of our architecture. ALISTA is
a representative of algorithm unrolling based archi-
tectures, which is an advanced variant of LISTA. We
have tried the vanilla LISTA, but it performs worse
than ALISTA on our tasks so it is not reported. RNN
refers to the LSTM-based model in Andrychowicz et al. (2016). Besides, we add a soft-thresholding
operator to this model to enforce sparsity, and include this variant as a baseline, called RNN-'ι.
Except for APF, all methods are trained on the same set of training problems and selected by the
validation problems. For APF, we perform grid-search to choose its hyperparameters, which is also
selected by the validation set. The detailed specification of each model can be found in Appendix I.3.
dim=256	dim=1024
30
25
20
15
10
5
0
Figure 4: Convergence of recovery error. Since
APF takes a long time to converge, its curve are
outside the range of these plots. We use a dash-
line to represent the final `2 error it achieves.
Fig. 4 shows the convergence of kβt - β* ∣∣2 for
problems in the test set. The x-axis indicates
the wall-clock time. In terms of the final re-
covery accuracy, PLISA outperforms all base-
line methods. In the more difficult setting (i.e,
d = 1024), its advantage is obvious. Although
PLISA is slightly slower than other deep learn-
ing based models due to the computations of
KT
Figure 5: Generalization gap of PLISA with varying
KT , for two different experimental settings.
MCP and SCAD, PLISA achieves a better accuracy and it has been converging much faster than the
classic algorithm APF. APF is very slow mainly due to the use of line-search for selecting step sizes.
8
Published as a conference paper at ICLR 2022
Generalization gap. We are interested in the generalization behavior of PLISA. As this experiment
is conducted for theoretical interest, we do not use the validation set to select the model. We vary
the number of layers (K) and blocks (T) in PLISA to create a set of models with different depths.
For each depth, we train the model with 2000 training problems, and then test it on a separate set of
100 problems to approximate the generalization gap. In Fig. 5, we observe the interesting behavior
of the generalization gap, where the left one increases in KT at the beginning and then decrease to
a constant, and the right one increases fast and then decrease very slowly. This surprisingly matches
the two different visualizations in Fig. 3 of the predicted generalization gap given by Theorem 4.1.
7.1.2	Sparse Precision Matrix Estimation (SPE)
We compare PLISA with APF, GLASSO (Friedman et al., 2008), GISTA (Guillot et al., 2012), and
GGM (Belilovsky et al., 2017) on sparse precision estimation tasks in Gaussian graphical models.
GLASSO estimates the precision matrix by block-coordinate decent methods. GISTA is a proximal
gradient method for precision matrix estimation. GGM utilizes convolutional neural network to
estimate the precision matrix. Details of each model and the training can be found in Appendix I.4.
Table 1 reports the Frobenius error kΘ -
Θ*kF between the estimation Θ and the
true precision matrix Θ*, averaged over
100 test problems. PLISA achieves con-
sistent improvements. Classic algorithm
are slower because they perform line-
search. GLASSO is faster than other clas-
sic algorithm because we use the sklearn
package (Pedregosa et al., 2011) in which
the implementations are optimized.
Table 1: Recovery error in SPE. The reported time is the av-
erage wall-clock time for solving each instance in seconds.
Sizes	p =50		p= 100	
Methods	kθ - θ*∣∣f	Time	kθ - θ*∣∣f	Time
PLISA	119.47 ± 12.23	0.117	142.70 ± 13.38	0.132
GLASSO	169.63 ± 17.99	1.66	237.95 ± 27.49	3.12
GISTA	186.96 ± 25.48	53.47	373.66 ± 41.72	36.02
APF	269.51 ± 32.28	46.02	485.94 ± 60.33	86.82
GGM	194.26 ± 10.73	0.007	445.00 ± 58.89	0.008
7.1.3	Discussion on Training-Testing Time
Test time. Fig. 4 and Table 1 shows the wall-clock time for solving test problems. Overall, classic
algorithms are slower because they need to perform line-search. It is noteworthy that learning-based
methods can solve a batch of problems parallelly but most classic algorithms cannot. To allow more
advantages for classic algorithms, the test problems are solved without batching in all methods.
Train time. As metioned earlier, we perform grid-search to select the hyperparameters in classic
algorithms using validation sets. The training time comparison is summarized in Table 3 and Table 4
in Appendix I.4. We can see that training time is not a bottleneck of this problem. Moreover, In
SPE, classic algorithms even require a longer training time than learning-based methods.
7.2	Unsupervised Learning on Real-world Datasets
We conduct experiments of unsupervised algorithm learning on 3 datasets: Gene expression
dataset (Kouno et al., 2013), Parkinsons patient dataset (Tsanas et al., 2009), and School exam
score dataset (Zhou et al., 2011). See Appendix I.6 for details of datasets and the configuration.
The goal of the algorithm is to estima-
tion the sparse linear regression parame-
ters β* for each problem. Recovery ac-
curacy is estimated by the least-square er-
ror on a set of held-out samples for each
problem. Table 2 shows the effectiveness
Table 2: Recovery accuracy on real-world datasets.
I PLISA I APF I RNN ∣ RNN-'ι ∣ ALISTA
Gene	1.177	1.289	1.639	1.349	1.289
Parkinsons	11.63	11.86	11.91	13.05	34.843
School	296.6	367.9	561.5	310.3	884.2
of PLISA. Note that these real-world datasets may not satisfy the assumptions in this paper. This
set of experiments are conducted only to demonstrate the robustness of the proposed method.
8	Discussion
We proposed PLISA for learning to solve sparse parameter recovery problems. We analyze its
capacity and generalization ability. The techniques could be used to derive guarantees for other
algorithm-unrolling based architectures. The model PLISA can be improved by using a more flexi-
ble penalty function (e.g., a conventional neural network) as long as it satisfies Assumption B.1.
9
Published as a conference paper at ICLR 2022
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural Information Processing Systems,pp. 3981-3989, 2016.
Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen
Vitercik. How much data is sufficient to learn high-performing algorithms? generalization guar-
antees for data-driven algorithm design. In Proceedings of the 53rd Annual ACM SIGACT Sym-
posium on Theory of Computing, pp. 919-932, 2021.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Arash Behboodi, Holger Rauhut, and Ekkehard Schnoor. Compressive sensing and neural networks
from a statistical learning perspective. arXiv preprint arXiv:2010.15658, 2020.
Eugene Belilovsky, Kyle Kastner, Gael Varoquaux, and Matthew B Blaschko. Learning to discover
sparse graphical models. In International Conference on Machine Learning, pp. 440-448. PMLR,
2017.
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis
Bach. Learning with differentiable perturbed optimizers. arXiv preprint arXiv:2002.08676, 2020.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In International Conference on Machine Learning, pp. 537-546. PMLR, 2017.
Mark Borgerding, Philip Schniter, and Sundeep Rangan. Amp-inspired deep networks for sparse
linear inverse problems. IEEE Transactions on Signal Processing, 65(16):4293-4308, 2017.
SebaStien Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint
arXiv:1405.4980, 2014.
Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on
information theory, 51(12):4203-4215, 2005.
Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural
networks. arXiv preprint arXiv:1910.12947, 2019.
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and
Wotao Yin. Learning to optimize: A primer and a benchmark. arXiv preprint arXiv:2103.12828,
2021.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of un-
folded ista and its practical weights and thresholds. In Advances in Neural Information Processing
Systems, pp. 9061-9071, 2018.
Xinshi Chen, Yu Li, Ramzan Umarov, Xin Gao, and Le Song. Rna secondary structure prediction
by learning unrolled algorithms. arXiv preprint arXiv:2002.05810, 2020a.
Xinshi Chen, Yufei Zhang, Christoph Reisinger, and Le Song. Understanding deep architecture with
reasoning layer. Advances in Neural Information Processing Systems, 33, 2020b.
M-C Corbineau, Carla Bertocchi, Emilie Chouzenoux, Marco Prato, and J-C Pesquet. Learned
image deblurring by unfolding a proximal interior point algorithm. In 2019 IEEE International
Conference on Image Processing (ICIP), pp. 4664-4668. IEEE, 2019.
Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint. Communications on Pure and Applied
Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 57(11):1413-
1457, 2004.
10
Published as a conference paper at ICLR 2022
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around
a common mean. Advances in Neural Information Processing Systems, 31:10169-10179, 2018.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American statistical Association, 96(456):1348-1360, 2001.
Jianqing Fan, Yang Feng, and Yichao Wu. Network exploration via the adaptive lasso and scad
penalties. The annals of applied statistics, 3(2):521, 2009.
Luca Franceschi, Paolo Frasconi, Michele Donini, and Massimiliano Pontil. A bridge between
hyperparameter optimization and larning-to-learn. stat, 1050:18, 2017.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with
the graphical lasso. Biostatistics, 9(3):432-441, 2008.
Vikas K Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. arXiv preprint arXiv:2002.06157, 2020.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the
27th International Conference on International Conference on Machine Learning, pp. 399-406.
Omnipress, 2010.
Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting
networks via continuous relaxations. arXiv preprint arXiv:1903.08850, 2019.
Dominique Guillot, Bala Rajaratnam, Benjamin T Rolfs, Arian Maleki, and Ian Wong. Iterative
thresholding algorithm for sparse inverse covariance estimation. arXiv preprint arXiv:1211.2532,
2012.
Rishi Gupta and Tim Roughgarden. A pac approach to application-specific algorithm selection.
SIAM Journal on Computing, 46(3):992-1017, 2017.
Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. arXiv preprint
arXiv:1910.13984, 2019.
Boris Joseph Joukovsky, Tanmoy Mukherjee, Nikos Deligiannis, et al. Generalization error bounds
for deep unfolding rnns. In Proceedings of Machine Learning Research. Journal of Machine
Learning Research, 2021.
Ulugbek S Kamilov and Hassan Mansour. Learning optimal nonlinearities for iterative thresholding
algorithms. IEEE Signal Processing Letters, 23(5):747-751, 2016.
Dohyun Kim and Daeyoung Park. Element-wise adaptive thresholds for learned iterative shrinkage
thresholding algorithms. IEEE Access, 8:45874-45886, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tsukasa Kouno, Michiel de Hoon, Jessica C Mar, Yasuhiro Tomaru, Mitsuoki Kawano, Piero Carn-
inci, Harukazu Suzuki, Yoshihide Hayashizaki, and Jay W Shin. Temporal dynamics and tran-
scriptional control using single-cell gene expression analysis. Genome biology, 14(10):1-12,
2013.
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic weights are as good
as learned weights in LISTA. In International Conference on Learning Representations, 2019a.
URL https://openreview.net/forum?id=B1lnzn0ctQ.
Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, and Zhongxuan Luo. On the conver-
gence of learning-based iterative methods for nonconvex inverse problems. IEEE transactions on
pattern analysis and machine intelligence, 42(12):3027-3039, 2019b.
Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical and
algorithmic theory for local optima. The Journal of Machine Learning Research, 16(1):559-616,
2015.
11
Published as a conference paper at ICLR 2022
Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie. Sparsemap: Differentiable sparse
structured inference. In International Conference on Machine Learning, pp. 3799-3808, 2018.
Edouard Ollier and Vivian Viallon. Regression modelling on stratified data with the lasso.
Biometrika, 104(1):83-96, 2017.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Marin Vlastelica Pogancic, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differ-
entiation of blackbox combinatorial solvers. In International Conference on Learning Represen-
tations, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Nir Shlezinger, Jay Whang, Yonina C Eldar, and Alexandros G Dimakis. Model-based deep learn-
ing. arXiv preprint arXiv:2012.08405, 2020.
Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinivas Aluru, Han Liu, and
Le Song. GLAD: Learning sparse graph recovery. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=BkxpMTEtPB.
Athanasios Tsanas, Max Little, Patrick McSharry, and Lorraine Ramig. Accurate telemonitoring of
parkinson’s disease progression by non-invasive speech tests. Nature Precedings, pp. 1-1, 2009.
Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a
learning-to-learn approach. In International Conference on Machine Learning, pp. 10981-10990.
PMLR, 2021.
Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of conver-
gence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014.
Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane Schonlieb, and HUa
Huang. Tuning-free plug-and-play proximal algorithm for inverse imaging problems. In Interna-
tional Conference on Machine Learning, pp. 10158-10169. PMLR, 2020.
Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated learned ista. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BygPO2VKPH.
Shanshan Wu, Alex Dimakis, Sujay Sanghavi, Felix Yu, Daniel Holtmann-Rice, Dmitry Storcheus,
Afshin Rostamizadeh, and Sanjiv Kumar. Learning a compressed sensing measurement matrix
via gradient unrolling. In International Conference on Machine Learning, pp. 6828-6839. PMLR,
2019.
Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, and Zhouchen Lin. Differentiable lin-
earized admm. In International Conference on Machine Learning, pp. 6902-6911. PMLR, 2019.
Y Yang, J Sun, H Li, and Z Xu. Admm-net: A deep learning approach for compressive sensing mri.
corr. arXiv preprint arXiv:1705.06869, 2017.
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of
statistics, 38(2):894-942, 2010a.
Jian Zhang and Bernard Ghanem. Ista-net: Interpretable optimization-inspired deep network for
image compressive sensing. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1828-1837, 2018.
Tong Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine
Learning Research, 11(3), 2010b.
Jiayu Zhou, Jianhui Chen, and Jieping Ye. Malsar: Multi-task learning via structural regularization.
Arizona State University, 21, 2011.
12
Published as a conference paper at ICLR 2022
A List of definitions and notations
For the convenience of the reader, we summarize a list of notations blow.
1.	∣∣βkλ,ι ：= Pd=ι λj ∣βj| denotes element-wise '1 norm.
2.	(x)S is the sub-vector of x with entries in the index set S.
3.	X ∨ e = max{x, e} = [max{xι, c},…，max{xd, e}]>.
4.	Element-wise soft-threshold:
TSλ(β) = (∣β∣- sλ)+ ◦ sign(β)	(22)
(lβι∣ - sλι)+ sign(βι) -
=	.	(23)
.
(lβd∣ - sλd)+ sign(βd) _
5.	Denote a single iteration of the modified proximal gradient step as
βk = MPG(βk-1; λ, w, α)
:=τα∙λ (βk-1- α (VLλ,w(βk-1)))
B	Penalty Functions
We provide detailed descriptions of the penalty functions used in PLISA. We focus on learning the
combination of three well-known penalty functions:
P(1)(λ,β)= ∣λ ◦ β∣1, P(2)(λ,β)=Pjp=1MCP(λj,βj), P(3)(λ,β)=Pjp=1SCAD(λj,βj),
where MCP (Zhang, 2010a) and SCAD (Fan & Li, 2001) are nonconvex penalties whose analytical
forms are given below.
B.1 Analytical forms
The MCP penalty can be written as
β2	bλ2
MCP(λ,β) = (λ |e| -砺)∙1 (IeI ≤ bλ)+-2 ∙1 (Ie| > bλ),
where b > 0 is some hyperparameter.
The SCAD penalty can be written as
SCAD(λ,β) = λ ∣β∣∙ 1 (∣β∣ ≤ λ) - β2-；a R+ λ2 ∙ 1 (λ< 网 ≤ aλ)
2(a- 1)
+ (a +21)λ2 ∙ 1 (IβI >aλ),
where a > 2 is some hyperparameter.
PLISA uses the concave components of these penalty functions. Their analytical forms are as
follows.
q⑵(λ,β) = MCP(λ,β)-	λ	∣β∣	= -β2 ∙ 1 (∣β∣ ≤ bλ)+ (b22 - λ ∣β∣)	∙ 1 (∣β∣	>bλ)
q⑶(λ, β) = SCAD(λ,β) -	λ	∣β∣	= 2λ与l - β2- λ2 ∙ 1 (λ< ∣β∣ ≤ aλ)
2(a - 1)
+ (a +I)λ2- "βl ∙ 1 (∣β∣>aλ)
13
Published as a conference paper at ICLR 2022
B.2 Implications
Based on the definitions of these penalty functions, it is easy to check that their convex combination
satisfies a set of conditions as stated in the following Assumption B.1.
Assumption B.1 (Regularization). Let P(λ, β) = Q(λ, β) + kβkλ,1 where Q is concave in β, and
that Q(λ, β) = Pj=I q(λj ,βj). Denote the partial gradient of q as q](β) := d%,e). Assume the
following regularization conditions.
(a)	There exists constants ξ ≥ 0 such that for any β0 > β,
q q qλ Ie)- qλ(β) ,
-ξ ≤	β-β- ≤ 0.
(b)	q(λ, -β) = q(λ, β).
(c)	qλ0 (0) = q(λ, 0) = 0.
(d)	|q](β)| ≤ λ
(e)	|q] Je)- qλ2 ⑶ | ≤ lλ1 - λ2 |.
Note that the penalty function Pw (λ, β) in PLISAθ satisfies all these conditions, with the constant
ξ in condition (a) being
11
ξw = w2 b + w3	1,
where a and b are the hyperparameters in SCAD and MCP.
Next, we present an important lemma which states the restricted strongly convexity and smoothness
of the modified loss
Lλ,w(β) := Ln(Z1:n, β) + Qw(λ, β).
Lemma B.1 (Restricted Strongly Convex and Restricted Strongly Smooth). Assume P satisfies
Assumption C.1 and (Zn, β*) ∈ P. If Q(λ, β) satisfies the regularization conditions in Assump-
tion B.1 with a constant ξ < ρ-. Then
Lλ(β) := Ln(Z1:n, β) + Q(λ, β)
is strongly convex and smooth in the subspace {kβ0 一 βko ≤ s* + 23}. That is,for any β, β0 such
that kβ0 — βko ≤ s* + 23, the following inequalities hold
Lλ(β0) ≥ Lλ(β) + VLλ(β)>(β0 — β) + p-2-ξkβ0 — βk2	(24)
Lλ(β0) ≤ Lλ(β) + VLλ(β)>(β0 — β) + ρ+ kβ0 — βk2	(25)
Proof. The proof is straightforward by using the condition (a) in Assumption B.1 and the sparse-
eigenvalue condition in Assumption C.1.	□
14
Published as a conference paper at ICLR 2022
C Problem Space Assumptions
We follow the notations in Wang et al. (2014); Loh & Wainwright (2015) to describe some classic
assumptions on the estimation problems.
Assumption C.1 (Problem Space). Let s*, sS be positive integers and ρ-, ρ+ be positive constants
such that s > (121(ρ+∕ρ-) + 144(ρ+∕ρ-)2)s*. Assumefor every estimation problem (ZLn, β*)
in the space P, the following conditions are satisfied.
(a)	∣β*∣0 ≤ s* and ∣β*∣∞ ≤B1;
(b)	For any nonzero V ∈ Rd with sparsity ∣∣vko ≤ s* + 2S, it holds
v>^β Ln(ZLn,β)v
(c)	8 l[VβLn(ZI:n, β*)]j | ≤ l[VβLn(ZI:n, 0)] j | ≤ B2, ∀j = 1, …，d.
kvk2
∈ [ρ-, ρ+];
Condition (a) assumes β* is s*-sparse and B1-bounded. Condition (b) is commonly referred to as
‘sparse eigenvalue condition’ (Zhang, 2010b; Wang et al., 2014), which is weaker than the well-
known restricted isometry property (RIP) in compressed sensing (Candes & Tao, 2005). Note that
the class of functions satisfying conditions of this type is much larger than the class of convex losses.
In the special case when Ln(Zin, β) is strongly convex in β, condition (b) holds with S → ∞. The
last condition bounds the gradient of the empirical loss Ln at the true parameter β* and 0.
15
Published as a conference paper at ICLR 2022
D Recovery Guarantee For Parameters In Constrained Space
Our generalization analysis is conducted on a constrained parameter space. In this section, we
provide theoretical guarantees for the estimation error of PLISAθ when θ is in the constrained
space. We first formally define the parameter space as follows.
Definition D.1 (Constrained parameter space). Assume P satisfies Assumption C.1 and Dm ⊆ P.
Given some positive values αmin, ηmax, ξmax, and λmax. We definite the constrained parameter
space Θ(Dm) as Θ(D,) := {θ = {η, λ*, w,α} : conditions (i)(ii) (iii)(iv) are satisfied}, where
the conditions are:
(i)	α ∈ [αmin, ρ+ ]∙
(ii)	ηj ∈ [ηmin := σ-1(0.9),ηmaχ],forall j = 1,…，d.
(iii)	Wfb + Wf3 a-i ≤ ξmaχ < ρ-, where b is the hyperparameter in MCP and a is the hyperparam-
eter in SCAD.
(iv)	λ* ∈ [8sup(z1:n,e* )∈Dm ∣[VβLn(ZI:n, β*)]j∣∨ ε,λmaχ],forall j = 1,…，d.
For parameters in this constrained space, we can show the performance guarantee for the outputs of
PLISAθ, as stated in the following Theorem D.1.
Theorem D.1 (Recovery guarantee). Assume P satisfies Assumption C.1 and Dm ⊆ P. Let Θ(Dm)
be the constrained parameter space defined in Definition D.1 and assume θ = {η, λ*, w, ɑ} ∈
Θ(Dm). Assume
K ≥ log
ρ- - ξmax
(1+
1	) k(B2 ∨ λ*Qs* k2	/ log
αminρ+	ε
+1,
where δ := 1 一 αmin(ρ— — ξmaχ). Then for any problem (Zi：n, β*) in the training set Dm, all
intermediate outputs of PLISAθ have thefollowing error bounds for all t = 1,…，T.
kβk (Z1:n； θ) — β*k2 ≤	15/2	k(λt(Zm; θ))s* k2,
ρ- 一 ξmax
IIet(ZI：n；θ) — β*∣∣2 ≤	19/8	k(λt(Ziw θ))s*k2.
∀k = 1,…，K,
Proof. The proof of this theorem is based on a series of key lemmas for the properties of the modified
proximal gradient steps in each cell of PLISA. We state these key lemmas and their proofs in
Section G. With these lemmas, the proof of this theorem is straightforward, and we state the proof
below.
Given a fixed problem (Zi：n, β*) ∈ Dm and a fixed set of parameters θ = {η, λ*, w,a} ∈ Θ(Dm,),
we simplify some notations in this proof by removing the dependency on Z1:n and θ. For example,
We denote βt = βt(Zι.n θ), βk = βk(Zi：n； θ), λt = λt(Zrn θ), etc.
Notations. Furthermore, we will use the following simplified notations and definitions.
empirical loss
modified loss
regularized loss
local optimal
sub-optimality
Ln (β) := Ln (Z1:n, β)
Lλ(β) := Ln(Z1:n, β) + Qw(λ, β)
φλ(β) := Ln(Z1:n, β) + Pw(λ, β) = Lλ(β) + ∣β∣λ,1
βbλ ∈	arg min	φλ (β)
β∈Rd"∣βs* kο≤s
ωλ(β) := ξ,miβkι mβax {∣(β-^∕ββ0∣λ>1 (VLEe)+λ ◦
Now we are ready to state the proof. We first show the following statement holds true for all t ≤ T
by mathematical induction.
Statement(t): 3入七(β0) ≤ 1, and k(βk )s* ∣∣0 &S ∀k = 1,…，K.
16
Published as a conference paper at ICLR 2022
Statement⑴.First We verify that Statement⑴ holds true. Recall that λo = |VLn(0)|. In the
following, we prove that βe10 is a local solution of φλ0 (β) := Lλ0 (β) + kβkλ0,1.
VLλ0Ge0) + λo ◦ ∂kβ0kι
= VLn(0) + VβQw(λ0, 0) + λ0 ◦ ∂k0k1
= sign(VLn(0)) ◦ |VLn(0)| + 0 + λ0 ◦ ∂k0k1
=Sign(VLn(0)) ◦ λo + λo ◦ ∂∣∣0kι.
Since - sign(VLn(0)) ∈ ∂k0k1, then We have 0 ∈ VLλ0 (β10) + λ0 ◦ ∂kβ10k1. Therefore,
ωλ0 (βe10) = 0. Since λ1 = λ0 ◦ η1, by Lemma G.6, We have
ωλι 例 ≤ ωλ0(o+0.2 = 2 ≤ 1/2.
By Lemma G.5, we have φλι (β0) - φλι (β*) ≤ 21∕2J(λξ1)s* k2, which implies that the conditions
in Lemma G.4 are satisfied. Therefore, We have proved that
k(βk)广ko ≤ & Vk =1,...,K.
This finishes the proof of Statement(1).
Statement(t). Now we assume Statement(t - 1) is true and prove Statement(t). First, we prove that
K
ωλt-1 (βtK-1) ≤ 1/4. By Lemma G.2 and Lemma G.5,
ωλt-1(βetK-1)≤
& + αm⅛+ M
min(λt-1)
(1 - αmin (ρ- - ξmax))
K-1 29∕2k(λt-i)s*k2
≤ι I 29^	(1 +
ρ- - ξmax
αmin ρ+
)网 ∨ λ*)S*k2 P(1 - ɑmin(ρ- - ξmaχ))KT
ASSUme K ≥ log (4 Jρz¾χ (1 + αm⅛+)
k(λ0∨λ*)s*k
2) / log (δ-1∕2) + 1, where
1
ε
—
ε
δ := 1 - αmin (ρ- - ξmax ).
Then we have
ωλt-1(βetK-1) ≤ 1/4.
0K
Since βt0 = βtK-1, Lemma G.6 implies that
ωλt (βet0) ≤ 1/2.
Furthermore, by Lemma G.5, ωλt (βet0) ≤ 1/2 implies
φλt(β0)- φλt(β*) ≤ 2≡λMi,
ρ- - ξmax
which in turns implies that the conditions in Lemma G.4 are satisfied. Therefore, we have
k(βkLko ≤ S Vk =1,...,K,
which completes the proof of Statement(t).
(26)
(27)
Now we derive the error bounds. Similar to how we derive Eq. 26 and Eq. 27, it is easy to show that
3λt (βκ) ≤ 1/4, Vt = 1,…，T,
and φλt (β0) - Φλt (β*) ≤ 21/2kC)s*k2 Vt = 1,…，T.
ρ- - ξmax
Combining φλt (β0) 一 φλt (β*) ≤ 21 片(λt)S* k2 with Lemma G.3, we have
kβk - β*k2 ≤ 15∕*)S*k2,	∀k =1,…，K.	(28)
ρ- - ξmax
Combining 3λt (βetK) ≤ 1/4 with Lemma G.5, we have
UeK	V (1 J7∖ k(λt)s*k2	19/8k("s*k2
kβt - β k2 ≤ b + Tj P-^-^X = P-Tmax .
□
17
Published as a conference paper at ICLR 2022
E Capacity Analysis: Proof of Theorem 3.1
Proof. This theorem is a direct result of Theorem D.1, by taking a suitable set of parameters θ.
Similar to the proof of Theorem D.1, some notations are simplified. Please refer to the proof of
Theorem D.1 for detailed illustrations.
Denote the union support set of the true parameters β* by Sm := {j : j ∈ supp(β*), (Zi：n, β*) ∈
Dm}. Then We specify a set of parameters θ = {η, λ*, w, α} for plisaθ as follows.
・ α
•w
1
=—
P+	—	—	—
:The weights w can take any values as long as they satisfy Wfb + f a--ɪ < ρ-. Since Wf and
wf3 and be arbitrary close to 0, there must exists a set of weights w that satisfy this constraint. In
particular, we denote this value by ξw := Wfb + f a-ɪ.
•	λ*:	For each j ∈	Sm,	we take	λ*	:=	8max(zn,β*)∈Dm	∣VLn(Zn,β*)j∣	∨ ε as the target
regularization parameter. For each j ∈ Smm, we take λj = B2, which is the upper bound of
kVβ Ln (Z1:n, 0)k∞ by Assumption C.1.
•	η: For all j, take ηj = log 9 so that the decrease ratio is σ(ηj) = 0.9.
Clearly, this set of parameters satisfy the conditions in Theorem D.1, so we can apply its result in
this proof.
In the following, we will show that, with this specification of the parameters, PLISAθ can achieve
the recovery accuracy stated in Theorem 3.1.
Let to = min {t : t ≥ 0, λ* = max{0.9t VLn(Zi：n, 0)|, λ*} , Zi：n ∈ Dm,,λ* ∈ θ ∈ Θ(Dm)}
be the number of blocks after which λt(Z1∙n; θ) = λ*. We know to is a small number because
the decrease rate is linear. To be more clear, it is easy to show that to ≤ log (Bε2) ∕log(10∕9).
Therefore, after to cells, the regularization parameters do not change anymore. That is,
%(Zi：n； θ)= λ*, Vt ≥ to.
Since the specified parameters satisfy the conditions in Theorem D.1, we can follow the same way
as how we derive Eq. 26 to obtain that
ωλt-ι (βt-ι) ≤ 1/4,	Vt = 1,…，T.
Since we have λt = λ* for t ≥ to. The last (T 一 to) cells can be viewed as a single cell with
K(T - to) many steps. Therefore, we can apply Lemma G.2 and Lemma G.5 to obtain
ωλt (βK) ≤ 2√2p+ S(U /j I29/"/，*k2
t t	ρ- 一 ξw
≤ ↑P^+Γ(1 + ɪ) "λ° ∨ λ*)S*k2 P(1-α(ρ--ξw))K""1
ρ- 一 ξw	αρ+	ε
≤ \。(1 + ɪ)-p(1-α(ρ--ξw))K(IO)T
ρ- 一 ξw	αρ+	ε
=Cθ λ∕s*ε-1 exp(-Cθ K (t — to)),
where Cθ
一1 log(1 - α(P- - ξw)), cθ
kβt — β*k2 ≤
ωλt (βetK) +
_	/ 29ρ+ (1
= V ρ--ξw(I
187) kλs* k2
+——),—— ----—
十 αP+ 1 1-α(ρ--ξw)
B2
By Lemma G.5,
ds*∣∣2
ρ-
C ε
cθε-1√s* exp(—CθK(t — to)) +
1s* exp( —CθK(t 一 to)) + cθ kλS* k2
187 kλs* k2
ρ- 一 ξw
(29)
≤
≤
—
—
where 团=?*B cθ
ρ--ξ
w
condition number as
and cθ = 8(p 1-ξ). What remains is bounding ∣∣λS* ∣∣2. First, we define a
κm
max ∫ 8max(Zn,β*)∈Dm VeLn(Zn, β*)jI
j∈sm t 8min(Zn,β*)∈Dm VeLn(Zn, β*)jI∨ ε
18
Published as a conference paper at ICLR 2022
In fact, Km ≤ Bε2 .Then recall the specification of λ* at the beginning of this proof. We have
kλS*∣∣2 ≤ X 8Km ∣Vβ Ln (Zn, β*)j ∣∨ ε
j∈S*
≤ 8Kmk(Vβ Ln(Zn, β* ) ∨ ε)s*∣∣2∙
Plugging this back to Eq. 29, we have
kβt - β* k2 ≤ ɑθε-1 s* exp(—CθK(t - tO)) + cθ8κmk (VeLn(Zn, β*) ∨ ε)S* k2.
This is equivalent to the statement to be proved.	□
19
Published as a conference paper at ICLR 2022
F Generalization Analysis: Proof of Theorem 4.1
We first state a well-known inequality that bounds the generalization gap by empirical Rademacher
complexity.
Theorem F.1 (Adapted from Theorem 26.5 (Shalev-Shwartz & Ben-David, 2014)). Assume that for
all (Zi：n, β*) ∈ P and all θ ∈ Θ we have that '(βτ(Zi：n； θ), β*) ≤ c. Then with probability at
least 1 - , for all θ ∈ Θ,
Lgen(P(P)； θ) - LMOn(Dm； θ) ≤ 2Rm('θ) + 4c^gm^,
where 'θ := {(Zi：n, β*) → ∣∣βτ (Zi：n； θ) 一 β*∣∣2 : θ ∈ Θ} is the space of loss functions of our
model PLISAθ, and Rm,('θ) is its empirical Rademacher complexity. It is defined as
Rm('θ) := Eσ SUPΘ∈Θ m∙ Pm=I σi ∣∣βτ(Z(：ni； θ) - β*⑺ ∣∣2 ,
where {σi}im=1 are m independent Rademacher random variables.
Therefore, it resorts to bound the empirical Rademacher complexity, which can be bounded via
Dudley’s integral. The following theorem states the bound for the empirical Rademacher complexity
and its proof follows.
Theorem F.2 (Empirical Rademacher complexity bound). Assume the assumptions in Theorem 4.1.
Let Θ = Θ(Dm1) be the constrained space defined in Definition D.1. Let 'θ := {(Zi：n, β*) →
∣∣βτ (Zi：n； θ) — β*∣∣2 ： θ ∈ Θ} be the space of loss functions of our model PLISAg. Then the
empirical Rademacher COmPIexity is bounded by Rm'θ ≤
√= qiog (c2√mK(T - to) exp(-CθK(T - to)) ∨ 1)+ c3dlog (c4√m (1 - exp(-CθKT)))
where c1, c2, c3, c4 are some constants independent ofd, m, K and T.
Proof. The classical Dudley’s entropy integral bound gives us an upper bound for the empirical
Rademacher complexity in terms of the covering number.
Rm'θ
d
The notation Pm means the empirical measure defined based on the samples in Dm , and
k'θ∣∣Pm,∞ ：= sup Pm('θ)
θ∈Θ
=SUP — X	kβτ(Zl:n； θ) - β*k2
θ∈Θ m
θ∈θ	(ZLn,β*)∈Dm
by Theorem D.1	≤ sup —	^X	——
θ∈Θ m	ρ
θ∈θ	(Zhn,β*)∈Dm -
≤ sup — X 一
θ∈Θ m	ρ
θ∈θ	(ZLn,β*)∈Dm —
19/8
k(λt(Zi皿 θ))s*∣∣2
19/8
√s* max{B2, λmaχ} ≤ C.
—
—
Here we use a constant C as the upper bound because we only care about the scales in d, m, K and
T. Next, We bound the covering number N (√m,'θ, L2 (Pm)). By the key result in Lemma 4.1, it
20
Published as a conference paper at ICLR 2022
is easy to derive that
I囱-'θ0 ∣∣L2(Pm) ≤ 2C(c1K(T - t0)√F ∣α - α0∣ exp(-CθK(T - t0))
+ k2旧-η0∣∣2 + C3∣∣λ* - λ*1∣2 + C4√d∣∣w - w0∣∣2) (1 - exp(-CθKT)))
≤ cK(T — to) ∣α — α0∣ exp(-CθK(T — to))
+ C (∣∣η - η0k2 + ∣∣λ* - λ*1∣2 + √d∣∣w - w0∣∣2) (1 - exp(-CθKT)) /
From now on, we abuse the symbol c to generally represent some constant that does not depend on
d, m, K, T.
N(" L2(Pm)) ≤N (CK(T - to)exp(-CθK(T - t0)), αmin, ^^ _ , ∣'∣)
X N (c (1 - exp(-CθKT)), [ηmin, ηmax]d,'2)
× N (C (1 - exp(-CeKT)), Y X %min, V2)
X N (Cvd (1-exp(-CeKT)), [w" wmax]3, '2) .
For the covering number of a d-dimensional compact vector space T ⊆ Rd, We can bound it by
the E-PaCking number. That is, N(T, 3 '2) ≤ M(T, e, '2) ≤ C Qd=I (Tj,max-τj,min + 1) for some
constant C. Applying this fact, We have
N(6,'θ,L2(Pm)) ≤
CK(T - to)exp(-CeK(T - t0)) ∨ ]
€
d /
×∏(
j=i ∖
C (1 - exp(-CeKT)) +1
d
×∏(
j=i ∖
C (1 - exp(-CeKT)) +1
€
€
X 33C√d(1 - exp(-CθKT))!3
€
Which implies
log N	,'θ,L2(Pm))
≤ log (C√mK(T - to) exp(-CeK(T - to)) V 1)
+ (2d) log (C√m (1 — exp(-CθKT)) + 1) + 3 log (c√dm (1 — exp(-CθKT)))
≤ log (C√mK(T — to) exp(-CθK(T — to)) V 1) + Cdlog (C√m (1 — exp(-CθKT)) + 1).
Therefore, Rm'& ≤
ʌ/log (C√mK (T — to) exp(-CθK(T — to)) V 1) + Cd log (C√m (1 — exp(-CθKT))).
□
21
Published as a conference paper at ICLR 2022
F.1 Robustness to Parameter Perturbation: Proof of Lemma 4.1
We split the parameters θ = {λ*, w,η,α} into two sets θ = θι ∪ θ2, where θι = {a} and
θ2 = {λ*, w, η}. Clearly, the robustness can be bounded by
kβT (Z1:n; θ1 ∪ θ2) - βT (Z1:n; θ10 ∪ θ2)k2 + kβT (Z1:n; θ10 ∪ θ2) - βT (Z1:n; θ10 ∪ θ20 )k2.
We derive the upper bounds of these two terms in Lemma F.1 and Lemma F.2, which imply
kβτ(Zi：n； θ) - βτ(Zin θ0)∣∣2 ≤ C√X (T - to) δK(T-tO) ∣α - α0∣
+ (CIlIn - η0k2 + C2∣∣λ* - λ*0∣∣2 + C4√d∣∣w - w0∣∣2) 11 - δ ,
where δ :=，1 一 ɑmin(ρ— — ξmaχ) < 1. Lemma 4.1 is equivalent to this inequality by taking
CΘ = - log(δ) > 0. Therefore, the key derivation steps are in the proof of Lemma F.1 and
Lemma F.2, as stated below.
Lemma F.1. Assume the assumptions in Lemma 4.1 are satisfied. Let
θ = {λ*, w, n,α}	and θ0 = {λ*, w, n,a0}
be parameters in the constrained space Θ. Let {βt}tT=1 = PLISAθ (Z1:n) and {βt0}tT=1 =
PLISAθ0 (Z1:n) be the intermediate outputs of PLISA with different parameters θ and θ0. Assume
K ≥ log
(4，29ρ+	(1 +
ρ- - ξmax
1	)√s^(B2 + λmaX)
αmin ρ+	ε
/log (δ-1) + 1,
where δ := √1 - αmin(ρ- - ξmaχ) < 1. Then
kβτ - βτ0k2 ≤ C√K (T - to) δκ(T-t0) ∣α - α0∣ .
Proof. By triangular inequality,
βetk-βetk0	= MPG(βetk-1; λt, w, α) - MPG(βetk-10; λt, w, α0)
≤ MPG(βetk-1; λt, w, α) - MPG(βetk-1; λt, w, α0)
+ MPG(βetk-1; λt, w, α0) - MPG(βetk-10; λt, w, α0)
WLOG, assume α > α0. Applying Lemma F.3 and Lemma F.4 to the above two terms on the right
hand side, we obtain
IIek -俄］∣2 ≤ δkβk-1-βk-l0k2 + r≡ IIek - βn
≤ δkβk-1 - ek-l0k2 + C ∣α - α0∣ |同-8^1(	(30)
where c is the Lipschtiz constant. c is a finite number because α is bounded and positive. Applying
it recursively, we have
K
∣∣eκ - eκ 0∣∣2 ≤ δK ∣eo - e00k2+c ∣α - αo∣ X δK-k ∣∣βk - βk-1∣∣2.	(3i)
22
Published as a conference paper at ICLR 2022
Now we bound the term ∣∣∕3k - ∕3k-1∣∣ . By Lemma G.1 and Lemma G.2,
—
where c0 :
B2δ-1. However, we can have a tighter bound for this term if t is
larger than a certain integer. More precisely, recall the definition that λt = max {λ0 0 σ(η)t, λ*}.
Since σ(ηmax) < 1, the entries of λ0 0σ(η)t will decrease exponentially to reach the entry values of
λ*. Let to = min {t : t ≥ 0, λ* = max{λ0 0 σ(nmaχ)t, λ*} , Vλ* ∈ θ ∈ Θ(Dm)} be the number
of blocks after which λt = λ*. We know to is a small number because the decrease rate is linear.
Now, the regularization parameters satisfy λt = λ*, Vt ≥ t0. Therefore, we can apply Lemma G.1
and Lemma G.2 again to obtain the following bound for all t ≥ t0:
Combining the two different bounds for ∣∣∕3k - QkT ∣∣ with Eq. 31, we have
Mt-助2 ≤ 巧”小十{C; - ；］. $ ；二")+k
= δκkβ	-Λ' k + ∕C∣ α - aI √FKδκ	if t<t0
=kβtτ1	QtTk2 + ∣C∣ α-α0∣ √FKδκ(I。+1) ift ≥ t0
if t < t0
if t ≥ t0
where C = cd. We apply the above inequality recursively and obtain that
∕t0-1	T	∖
∣∣qt - βτ0k2 ≤ c√s*Kδκ I X δκ(TTt) ∣ α - α∣ + X δκ(TTt)δκ(t-t0) ∣ α - α∣ ɪ
∖ t=1	t=t0	)
=C√FKδκ 卜K(TTt0+1) 1 -那；K I) +(T - t0)δK(TTt0)) ∣ α - α∣
=c√Kδκ ((δK(I-Ri)) + (T - t0)) δK(TTt0)) ∣ α - α0∣
≤ C√s*K (T - t0 + 1) δκ(τTt0+1) ∣ α - α0∣
≤ C√K (T - t0) δκ(τTt0) ∣ α - α0∣
□
23
Published as a conference paper at ICLR 2022
Lemma F.2. Assume the assumptions in Lemma 4.1 are satisfied. Let
θ = {λ*, w, η,α} and θ0 = {λ*0, w0, η0,α}
be parameters in the constrained space Θ. Let {βt}tT=1 = PLISAθ (Z1:n) and {βt0}tT=1 =
PLISAθ0 (Z1:n) be the intermediate outputs of PLISA with different parameters θ and θ0. Then
kβT (Z1:n; θ) - βT (Z1:n, θ0)k2 ≤
1	δKT
(cιkη — η0∣∣2 + C2∣∣λ* — λ*0∣∣2 + C4√d∣∣w — w0∣∣2)———,
where δ :=，1 一 ɑmin(ρ- — ξmaχ) < 1 and C1,2,4 are some constants.
.	。一、E	r ：八、E r/	一 一 -.、E	。 Y . , ʌ zn τ√_	_	一.	.
Proof. Let {βt}tT=1 ∪ {βtk}tT=1kK=1 and {βt0}tT=1 ∪ {βtk0}tT=1kK=1 be the intermediate outputs of
PLiSAθ(Zi：n) and plisaθo(ZLn) respectively. By triangle inequality,
Iek — iek0∣∣2 = ∣∣MPGGek-1; λt, w, α) — MPG(βk-10; %0, w0, α0)(
≤ MPG(βetk-1; λt, w, α) — MPG(βetk-10; λt, w, α)
+ ∣∣∣MPG(βetk-10; λt, w, α) — MPG(βetk-10; λt0, w, α)∣∣∣
+ ∣∣∣MPG(βetk-10; λt0, w, α) — MPG(βetk-10; λt0, w0, α)∣∣∣ .
Applying Lemma F.3, Lemma F.5, and Lemma F.6 to the above 3 terms on the right hand side, we
obtain
∣∣βk - βk % ≤ δkβk-1—βk-i0k2 + P2- kλt - %∣∣2+Ckw-w0k2kλtk2,	(32)
l+-{iZ}	篙
where C is some absolute constant.
Term (i). By definition and triangular inequality,
∣∣λt — λt0k2 ≤ k max {λo ◦ σ(η)t, λ*} — max {λ° ◦ σ(η0)t, λ*} ∣∣2
+ k max {λo ◦ σ(η0)t, λ*} — max {λ° ◦ σ(η0)t, λ*0} ∣2
≤ kλok∞tσ(ηmaχ)t-1tkσ(η) — σ(η0)k2 + kλ* - λ*0k2
≤ ^42σ(nmaX)ttkη - η0k2 + ∣∣λ* - "0∣∣2
The last inequality holds by Assumption C.1 and the bounded first derivative of σ(∙). Therefore, for
some constants C2 =看 and Ci = C2 B2,
2
⑴=------kλt - λtk2 ≤ C1σ(ηmaX) tkη - η ∣∣2 + C2kλ - λ ∣∣2
ρ+	t
≤ Cι∣∣η — η0∣∣2 + C2∣∣λ* — λ*[∣2.
The last inequality holds because the value σ(ηmaX)tt is bounded by some constant.
Term (ii). Since ∣∣λt∣2 ≤ ∣∣ max {σ(ηmaχ)tλ°, λ*0}∣∣2 ≤ ∣∣ max {σ(ηmaχ)tλo, λ0}∣∣2 = ∣∣λ0∣∣2 ≤
B √d,
(ii) = C∣∣w —如0||2|八||2 ≤ C4√d∣w — w0∣2.
Plugging the bounds for (i)-(iii) into Eq. 32, we have
∣∣βk — βk0∣∣2 ≤ 8|同-1 — βkτ∣∣2 + Ci∣η — η0∣2 + C2∣∣λ* — λ*0∣∣2 + C4√d∣∣w — w0∣∣2.
24
Published as a conference paper at ICLR 2022
By direct computation,
.-湃IL
K-1
≤ δκ 网-β00I I 2 + (Cι∣∣n - n0∣∣2 + C2kλ* - λ*1∣2 + C4√d∣w - w0∣∣2) X δk
k=0
1 — δκ
=δκ I∣β0 -∕30o∣∣2 + (Cιkn -柏∣2 + C2kλ* - λ*0k2 + C4√d∣w - w0∣∣2) I-=T.
κ0
Recall that βK = βt and β0 = βt-ι. We can apply the above inequality recursively and obtain that
IleT - βT 1∣2
≤ δκ ∣∣βτ-1 - βτ-1l∣2 +(Cikn - n0k2 + C2∣∣λ* - λ*0∣∣2 + C4√d∣w - wl∣2) Ir-K
≤(C11忻-η0∣∣2 + GD* - TlI2 + C4√d∣∣w - w0∣∣2
=(CI Iln - η°∣∣2 + C2kλ* - λ*0∣∣2 + c4√dkw - W°k2
1	δK T-1
⅛ X (δK )t
t=0
1 - δKT
1 - δ .
□
Lemma F.3 (Robustness to β). Assume (Zi：n, β*) ∈ P and P satisfies Assumption C.1. Assume
β, β0 satisfy ∣∣β — β0∣∣0 ≤ S = s* + 2s and α ≤ 工.Then
kMPG(β； λ, w,°) - "β λ, w,α)∣2 ≤ j1- 2⅛⅛⅛ kβ-β0∣2
≤ P1 - α (P- - ξw) llβ - β'l∣2
(33)
(34)
Proof.
l∣MPG(β; λ, w, α) - MPG(β0; λ, w, α) ∣∣2
=∣%.λ (β - α (VLλ,w(β))) - TG (β0 - α (VLλ,w(β0)))∣2
≤ ∣β - α (VLλ,w(β)) - β0 + α (VLλ,w(β0))∣2
=∣β - β0∣2 + α2 ∣VLλ,w(β) - VLλ,w(β0)∣2 - 2αhβ - β0, VLλ,w(β) - VLλ,w(β')).
(35)
Since on the restricted subspace {β : ∣β - β* ∣o ≤ s* + 2s}, Lλ is (P- - ξw)-strongly convex and
P+-smooth, by Lemma G.7,
hβ - β0, VLAw(β) - VLλ,w(β0)i
〉
∣β - β0∣2 + kV% (β) -VL- (β')k2
ρ- - ζw + ρ+	ρ- - ζw + ρ+
Combining this inequality with Eq. 35, we have
∣∣MPG(β; λ, w, α) - MPG(β0； λ, w, α)∣∣2
≤
1 - 2α
(P- - ξw)ρ+
ρ- - ξw + ρ+
∣β-β0k2
+ α (α------------———) IlVLλ,w (β)- VLλ,w (β0)∣ι2
∖ P- - ξw + ρ+√
By the assumption that α ≤	, the second term in the above inequality is non-positive. Further-
more, this assumption also implies that 1 - 2α PP--ξwjρ+ ≥ 0. Therefore,

∣∣MPG(β; λ, w,α) - MPG(β0; λ, w,α)∣∣2 ≤
(P- - ξw)ρ+
ρ- - ξw + ρ+
∣β - β0∣2
□
25
Published as a conference paper at ICLR 2022
Lemma F.4 (Robustness to α). With out loss of generalization, assume α ≥ α0. Denote
β+ := MPG(β; λ, w, α) and β+0 := MPG(β; λ, w, α0).
Then
kβ+0-β+k2 ≤ Vα+-α7 忖—现
Proof. Define the quadratic approximation function as
12
ψα,λ,w (Z, β) := Lλ,w (e) + hVLλ,w (e), Z - βi + 2α Ilz - βk2 + kzkλ,1 .	(36)
Clearly,
MPG(β; λ,w,α) = arg min ψα,λ,w (Z, β).
z
Since ψɑ,λ,w(z, β) is 1 -strongly convex in z, and that β+ := MPG(β; λ, w, a) is its optimal point,
ψα,λ,w(z, β) ≥ Ψα,λ,w (β+, β) + 2^ ∣∣Z — β+∣∣2	∀Z.	(37)
Similarly,
Ψα0,λ,w (Z, β) ≥ Ψα0,λ,w (β+7, β) + 207∣∣Z - β+0∣∣2 ∀Z.	(38)
Taking Z = β+7 in Eq. 37 and Z = β+ in Eq. 38 yields
2α ∣∣β+ - β+0∣∣2 ≤ ψα,λ,w (β+0, β) - ψα,λ,w (β+ , β),
2ɑ ∣∣β+ - β+0∣∣2 ≤ ψα0,λ,w (β+, β) - ψα0,λ,w(β+0, β).
Summing the above two inequalities, we have
+ +---1--θ) ∣∣β+ - β+0∣∣2 ≤ (ψα,λ,w (β+0, β) - ψɑ0,λ,w (β+0, B))
2α α
+ (Ψɑ0,λ,w (β+, β) - Ψα,λ,w (β+, β))
(by α ≥ α7)	≤ ψα0,λ,w (β+ , β) - ψα,λ,w (β+, β)
=ι( α - α)∣∣β+ - β∣∣2.
αα
□
Lemma F.5 (Robustness to λ). Let β+ = MPG(β; λ, w, α) and β+7 = MPG(β; λ7, w, α). Then
Iβ+ - β+7I2 ≤2αI(λ-λ7)SI2,	(39)
where S := supp(β+) ∪ supp(β+7).
Proof. Recall the quadratic approximation in Eq. 51 and the definition that MPG(β; λ, w, α) =
arg mi□z ψɑ,λ,w (z, β). By the 1 -strongly convexity of ψα,λ,w in z, it holds true for all Z that
12
2α ∣∣z - β ∣∣2 ≤ ψα,λ,w (z, β) - ψα,λ,w (β , β)
="Lλ,w(β), Z - β+i + 2α (kz - βk2 -∣∣β+ - β俏)+ (kzkλ,ι - ∣∣β+∣∣λ,ι)	(40)
Similarly,
(41)
≤ (VLλ0,w(β), Z - β+0i + 2α (kz - βk2 -∣∣β+0 - β∣∣2) + (kzkλ0,ι - ∣∣β+ 0∣∣λ0,ι).	(42)
26
Published as a conference paper at ICLR 2022
Taking Z = β+ in Eq. 40, taking Z = β+ in Eq. 42, and summing up the two inequalities, We have
1∣∣β+ -β+,∣∣2 ≤ "Lλ,w(β) - VLk,W(β),β+0 -β+i
+ kβ+ ◦ (λ0 - λ) kι-kβ+'o(λ0 - λ) kι
=hVQλ,w(β) - VQλ,w(β), β+ - β+i + kβ+ - β+1∣∣λ-λ∣,ι
d
≤ X∖qw(λj,βj) - qw(λj,βj)∖∖β+' - β+∖ + kβ+ -β+0k∣ λ-λ ∣ ,1
j=i
d
(by Assumption B.1) ≤ X ∖λj - λj∖∖β+' - β+∖ + ∣∣β+ - β+'∣∣∣ λ-λ∕ ∣,1
j=ι
= 2∣β + - β+'∣∣ λ-λ∣ ,1
≤ 2∣(λ - λ')s∣2∣∣β+'- β+∣∣2
where S := supp(β+) Usupp(β+'). Dividing both side by ∣∣β+' - β+∣∣2 draws the conclusion. □
Lemma F.6 (Robustness to w). Let β+ = MPG(β; λ, w, α) and β+' = MPG(β; λ, w', α). Then
∣β+ - β+'∣2 ≤ C∣w - w'∣2∣(λ)s ∣2,	(43)
where S := supp(β+) U supp(β+') and C is some absolute constant.
Proof. Recall the quadratic approximation in Eq. 51 and the definition that MPG(β; λ, w,α) =
argminz ψα,λ,w(z, β). By the 1 -strongly convexity of ψa,λ,w in β, it holds true for all Z in the
restricted subspace that
1	2
次 ∣ ∣ Z - β ∣ ∣ 2 ≤ ψα,λ,w (z, β) - ψα,λ,w (β , β)
=hVLλ,w(β), Z - β+i + 2α (IlZ - β∣2 -∣∣β+ - β俏)+ (∣Z∣∣λ,ι - ∣∣β+∣∣λ,ι)	(44)
Similarly,
2aHZ - β+0∣∣2
≤ hVLλ,w(β), Z - β+'i + ( (IlZ - β∣2 - ∣∣β+' - β∣∣2) + (IlZb,1 - ∣∣β+ '∣∣λ,ι).	(45)
Taking Z = β+' in Eq. 44, taking Z = β+ in Eq. 45, and summing up the two inequalities, we have
1 ∣∣β+ - β+'∣∣2 ≤ hVLλ,w(β) - VLλ,w(β), β+' - β+i
=hVQλ,w(β) - VQλ,w(β), β+' - β+i
d
≤ X ∖qw(λj,βj) - qw，(λj,βj)∖∖β+' - β+∖.
j=i
Note that qw(λj, βj) = P3=1 eχpiwy q⑻Qj, βj) where Z(W) = P3=1 exp(wj. Then
∣√ fλ B ' J λ B )∣ <χ∣eXP(Wi) eχp(wi)
∖qw(λj，βj) - qw(λj，βj)∖ ≤ "苕W)-Z(w')
(by Assumption B1) ≤ X ∣exp(Wi) - exp(Wi)
(byAssumptionB.I) ≤2^∣ z(w)	Z(w')
Pi)'(%, βj )∣
λj.
27
Published as a conference paper at ICLR 2022
Therefore,
1 ∣g+	叶产 <X exp(wi)	exp(wi)∣XX	+0	+，
α"β -β A≤⅛ 3Wr-NwT ∖fβ jβ -βj1
Ve eχp(wi)	eχp(wi )∖∣八、U ∣g+0	h+∣∣
≤⅛ ^wr-^wT ∖k(λ)Sk2"β -β 卜
(By Lipschitz continuity) ≤ C∣∣w - w0∣∣2∣∣(λ)s∣∣2 ∣∣β+0 - β+∣∣2,
for some constant C.	□
28
Published as a conference paper at ICLR 2022
G Key Lemmas
This section supplies some fundamental results for the modified proximal gradient algorithm. All
the results in this section are based on the following assumption and notations.
Assumption G.1. Assume P satisfies Assumption C.1 and consider a problem (Zn, β*) ∈ P. As-
sume we have a penaltyfunction P (λ, β) whose concave component Q(λ, β) = P (λ, β)-∣∣λ 0β∣∣ι
satisfies the regularization conditions in Assumption B.1 with a Lipschitz constant ξfor the condition
(a). We adopt the following notations for the statements in this sections.
empirical loss
modified loss
regularized loss
local optimal
quadratic approximation
sub-optimality
Ln(β) := Ln(Z1:n, β)
Lλ(β) := Ln(ZI:n, β) + Q(λ, β)
φλ(β) := Ln(Z1:n, β) + P (λ, β) = Lλ(β) + kβkλ,1
βbλ ∈	arg min	φλ (β)
β∈Rd:IU ko≤s
ψɑ,λ(z, β) := Lλ(β) + hVLλ(β), Z - βi + ( kz - βk2 + kzkλ,1
ωλ(β) := ξ,miβkι mβax {kβ-β‰ι (vLλ(β)+λ ◦ ξ,)}
Lemma G.1 (Contraction of MPG). Assume Assumption G.1. Let β0, •一，βk be a SeqUenCe of
vectors obtained by the modified proximal gradient updates:
βk = MPG(βk-1; λ, w,α)
:= Tα∙λ (βk-1 - α (VLn(βk-1) + VβQ(λ, βk-1))).
Assume k(βk)s^ko ≤ s. Then
(1 - ρ+) ∣∣βk - βk-1ll2 ≤ φλ(βk-1) - Φλ(βk).	(46)
Proof. Note that
βk = βk-1 - αδα(βk-1), where δɑ(β) := α-1 (β -Tɑλ (β - αVLλ(β))).
It is easy to observe that δα(β) ∈ VLλ(β) + λ ◦ ∂ kβ - αδα(β)k1. By RSS in Lemma B.1, it
holds that
Lλ (βk) ≤ Lλ(βk-1) - αVLλ(βk-1)>δα(βk-1) + p+ α2∣∣δα(βk-1)∣∣2 ∙
Furthermore, by RSC in Lemma B.1, for any Z in the space {∣∣z - βk-1∣∣0 ≤ s* + 2s},
Lλ (βk) ≤ Lλ(z) + VLλ(βk-1)>(β - Z)- αVLλ(βk-1)τδα(βk-1)	(47)
+ ρ+α2 ∣∣δɑ(βk-1)∣∣2 - — ∣∣βk-1 - z∣∣2.	(48)
Denote v = δα(βk-1) - VLλ(βk-1). We have v ∈ λ ◦ ∂ ∣∣βk-1 - αδα(βk-1)∣∣1 since
δα(βk-1) ∈ VLλ(βk-1) + λ ◦ ∂∣∣βk-1 - αδα(βk-1)∣∣1. By the convexity of ∣∣∙kλ,1,
∣∣βk-1 - αδα(βk-1)∣∣λ,1 ≤ kzkλ,ι + v> (βk-1 - αδα(βk-1) - z).
Combining this inequality with Eq. 48, we have
Lλ(βk) + kβkkλ,1 ≤ Lλ(Z) + λ kZkλ,1 + δα(βk-1)τ(βk-1 - Z)	(49)
-α (I- αρ+) ∣∣δα(βk-1)∣∣2 - ɪ ∣∣βk-1 -z∣∣2.	(50)
Taking Z = βk-1 in Eq. 50 implies that
Lλ(βk)+kβk kλ,ι ≤ Lλ(βk-1)+∣∣βk-1∣∣λ,1 -1 (1 - αρ+) ∣∣βk - βk-1∣∣2.
29
Published as a conference paper at ICLR 2022
Therefore,
(1 - ρ+) ∣∣βk - βk-1ll2 ≤ φλ(βk-1) - Φλ(βk).
□
Lemma G.2 (Convergence of modified proximal gradient). Assume Assumption G.1. Let
β0, ∙∙∙ , βk be a Sequence ofvectors obtained by the modified proximal gradient updates:
βk = MPG(βk-1; λ, w, α)
:=Tα∙λ (βk-1 - α (VLn(βk-1) + VβQ(λ, βk-1)))
with a SteP size a in (0, pɪ-]. Assume ∣∣(βk)广∣∣o ≤ s. Then
φλ(βk) - φλ(βbλ) ≤ (1 - α(ρ- - ξw))	φλ(β0) - φλ(βbλ) ,
kβk - βλ∣2 ≤ (1 - α(ρ- - ξw))k ∣∣β0 - βλ∣2,
ωλ(βk) ≤ 遮1 + 奔'5r(1 - α(ρ- - ξw))k-1 (φλ(β0) - Φλ(βλ)].
min(λ)
Proof. Consider the quadratic approximation function
ψα,λ(z, β) ：= Lλ(β) + hVLλ(β), Z - βi + 2α ∣z - β∣2 + kz∣k,ι .	(51)
By definition,
MPG(βk-1; λ, w, α) = arg min ψα,λ(z, βk-1).
z
Since ψa,λ(z, βk-1) is 1 -strongly convex with respect to z, and that βk is its optimal point, We
can obtain
12
ψα,λ(z, βk-1) ≥ Ψα,λ(βk, β	) + 2αl∣Z - βkll2 , ∀Z.	(52)
Note that by Lemma B.1,
ψα,λ(βk, βk-1) = Lλ(βk-1) + VLλ(βk-1 )> (βk - βk-1) + (I，- βk-1∣2
+∣βk∣λ,1
≥ Lλ(βk) + α-ρ+ kβk - βk-1k2 + kβkkλ,ι
=Φλ(βk) + +-ρ+ kβk - βk-1k2.
Similarly, by Lemma B.1,
ψα,λ(z, βk-1) = Lλ(βk-1) + VLλ(βk-1)> (z - βk-1) + ɪkz - βk-1k2
2α
+ ∣z∣λ,1
≤ Φλ(z)+ α -(P- - ξ) kz - βk-1∣2.
Combining the above two results with Eq. 52, we have
φλ(βk) ≤ φλ(z) + α -(P- - ξ) kz - βk-1k2 - α-ρ+ kβk - βk-1k2
-2α I|z - βkl2	(53)
≤ φλ(z) + α -(P--ξ) kz - βk-1k2 - α-ρ+ kβk - βk-1k2.
30
Published as a conference paper at ICLR 2022
Taking Z = cβλ + (1 - c)βk-1 for some C ∈ [0,1], then
φλ(βk)
≤ cφλ(βX) + (1 - c)φλ(βk-1) +(1 - (p 2 - ξ))c2	kβk-1 -	βλk2	-	i-ρ+ kβk	-	βk-1k2
≤ cφx(βx) + (1 - c)φλ(βk-1) +C 尾-2°- -ξ))	kβk-1 -	βλk2	-	α-2ρ+ kβk	-	βk-1k2
Taking C = α (P- - ξ), it implies
φλ(βk) - Φλ(βλ) ≤ (1 - α (P- - ξ)) (φλ(βk-1) - Φx(βλ)) - α--P+-∣∣βk - βk-1∣∣2.
Assume α ≤ ɪ. Then
p+
φλ(βk) - φλ(βλ) ≤ (1 - α (P- - ξ)) (φλ(βk-1) - φλ(βλ)).
Taking Z = βχ in Eq. 53 implies
φλ(βk) ≤ φχ(βλ) + α -(Z2--ξ) I∣βλ - βk-1k2 - 21α I∣βλ - βk k2.
By the optimality of βχ, We have
1 -(Pi--∙ kβχ - βk-ιk2 - 2α 麻-βk ∣∣2 ≥ 0,
which implies
kβλ - βkk2 ≤ (1 - α (P- - ξ)) kβλ - βk-1k2.
Finally, we derive the upper bound for ωχ(βk). Since βk = argminz ψ%χ(z, βk-1), by the
optimality condition, for each j = 1, ∙∙∙ ,d, there exists a subgradient ξj ∈ ∂ Iek ∣ such that
VLλ(βk-1) + 1(βk - βk-1) + λ o ξ = 0.
α
By the definition of ωχ and combining it with the above equality,
ωχ(βk) = min max ʃ -τ(β——β)— (VLχ(βk) + λ o ξz)∣
' ξ g∈∂0kι β∈Ω[∣∣βk-β0∣∣χJ Xw ' F
≤ max {kβ≡A⅛ (VLX就)+λ o ξ)}
=max {Im Sk)- VLX(βk-1)+1(βk-1 - βk))}
f Λ (ek- βj)λj (VLX(βk) -VLχ(βk-1) + ɪ(βk-1 - βk))j]
=max
β∈ω 1⅛ kβk - β'kX,ι	λj
≤ —‰kVLX(βk) - VLX(βk-1) + 1(βk-1 - βk)k∞
min(λ)	α
≤	‰kVLX(βk) - VLX(βk-1) + 1(βk-1 - βk)∣∣2
min(λ)	α
≤	kVLX(βk) - VLX(βk-1)k2 + 1 k(βk-1 - βk)∣∣2
Inin(λ)	Q
(byRSS) ≤ -‰(p+ + 1)k(βk-1- βk)∣2.
min(λ)	α
31
Published as a conference paper at ICLR 2022
ByLemmaG.1, k(βk-1 - βk)∣∣2 ≤ J21 -(P+)(φλ(βk-1) - φλ(βk)). Therefore,
3入0) ≤ m⅛s + I)S2α⅛y(φλ(βI)- "β))
1	1	2	-
≤E(P+ + ZN15 )-φ"k))
≤ m⅛λ)(ρ+ + I)S P+ (φ"kT)-。入肉)
≤ m⅛)(ρ+ + I)S P+(1 - α (P-T))k-1 (公⑶-φλ(云)).
□
Lemma G.3 (Statistical '2 error). Assume Assumption G.1. Ifthefollowing conditions are satisfied
kβ - β*∣∣0 ≤ s* + 25,	(54)
φλ(β)- φλ(β*) ≤ 2^-∣∣λs*∣∣2,	(55)
P- - ξ
λj ≥ 8∣[VLn(β*)]j∣,	(56)
then ∣∣β - β*∣∣2 ≤ mino>0max {,21 + 学a, ⅛ + 导} kp--ξ2 ≤ 1"S*k2.
Proof. By the restricted strong convexity of Lx in Lemma B.1,
P-fɪ kβ - β*k2
≤ Lχ(β) - Lχ(β*) - VLχ(β*)[(β - β*)
=φλ(β) - φλ(β*) + (kβ*∣∣χ,ι -kβkλ,ι) - VLλ(β*)T(β - β*)
≤ ^1A kλs* k2 + (闭x,ι -网x,ι) -VLχ(β*)>(β - β*).	(57)
P- - ξ	'-------V-------'、-------V---------Z
⑴	(ii)
For the term (i),
∣∣β*∣∣x,ι -∣∣β∣∣x,ι = ∣∣βS*l∣λs*,1 -l∣βs*1lx.*,1+ (l∣β⅛*Ilx尹,1 - kβs*llʌs*,1)
=kβS*kx.* ,1 -∣∣βS*∣∣Xs* ,1 - kβs*IlX>,1
≤ Il(e - β")s* l∣Xs* ,1 - kβs*l∣Xs*,1
=k(β - β*)s* ∣∣Xs* ,1 -∣∣(β - β*)s*IlX/,1.
For the term (ii),
-VLχ(β*)>(β - β*) = -VLn(β*)>(β - β*) - VQχ(β*)>(β - β*)
(By Eq. 56) ≤ 8∣β - β*∣∣χ,1 - VQχ(β*)>(β - β*)
(since q'x, (0) = 0) = ：∣β - β*∣∣χ,1 - VQχ(βS* )τ(β - β*)s*
(since ∣ qλj(e) ∣ ≤ λj) ≤ 8llβ - β*Hχ,1+ Il(β - β*)s* ∣∣xs*,1
9	1
=W I(β - β*)s*∣∣Xs* ,1 + 81(β - β*)s*l∣χ尹,1.
32
Published as a conference paper at ICLR 2022
Combining the upper bounds of term (i) and term (ii) with Eq. 57, We have
kβ-β*ki
21	...	17	.......... 7	..........
≤	(P- -	ξ)i	kλs*ki + 4(ρ-	- ξ) Il(e - β	)s*kλs*，1 -	4(ρ- -	ξ)	I∣(β	- β)S* kλ›，1，(58)
1	21	17
≤ ρ^^-ξ (P^^-ξ llλS* ki + W k(β - β)S* kλs* ,1) ∙
Let a ≥ 0 beaconstant.If k (β - β*)s*bs* ,ι ≤ PZa-ξ ∣∣λs* ∣∣i, then
kβ-β*ki ≤ ,21 + .aR.
4 P- - ξ
IflKe - β")s* lλs*,ι > ρ-a-ξl∣λs* ∣i,then
kβ -β*ki ≤ PJ=^ (21 +17) ∣(β -β)s*尻*,1 ≤ (21 + 147) ∣λ∈^k(β -β*)ki,
which implies ∣β - β* ∣i ≤ (i1 + f) 噎-ξ2 ∙ Combining the two cases, we know that
kβ-β*ki ≤ minmax (,21 +17 α, 21 + 1r) ∣λS⅛.
Taking a = 7 it is easy to derive that
kβ-β*∣i ≤15/2/ki.
P- - ξ
□
Lemma G.4 (Retain in the restricted space). Assume Assumption G.1. Let β0, ∙∙∙ , βk be a se-
quence of vectors obtained by the following update:
βk = MPG(βk-1; λ, w, α)
with a SteP sizes α in [ami∏,寺].Assume thefollowing conditions:
k(β0)s*∣0 ≤ S
φλ(β0) - φλ(β*) ≤	∣λs* Ili,
P- - ξ
N ≥ 8∣[VLn(β*)]j∣,
(L⅛144 (H (∖)工 S,
"min(P- - ξ)	IP- - ξ) 八 mm(λ下｝)
where K := ρρ+-ξ. Thenforall k = 1, ∙∙∙ ,K,
and
l(βk)s*∣0 ≤ S,
φλ (βk ) - φλ(β*) ≤ --ʒ∑IλS* ∣i.
Proof. Recall that βk = Tαλ (βk) where
βk = βk-1 - αVLλ(βk-1)
=βk-1 - αVLλ(β*) + α (VLλ(β*) - VLλ(βk-1)).
33
Published as a conference paper at ICLR 2022
To show ∣∣(βk)5∣∣0 ≤ s, we need to prove that, for j ∈ S*,thenumberof j,s such that ∣∣3j ∣ > aλj
is no more than s. Note that
∣βk | ≤ ∖βk-1∖ + α I (VLλ(β*) - VLλ(βfc-1))j ∣+ α∣ [VL„ (β*)j ∣
≤ ∣βk-1∣ + α I (VLλ(β*) - VLλ(βkT))j ∣ + 1 ∙ αλj.
Therefore, ∀c ∈ [0,1],
∣∣就y∣∣0 ≤ Card ({j ∈ 炉：∣βk-1∣ > c7ɑλj})
+ Card ({j ∈ 尹:∣ (VLλ(β*) - VLλ(βfc-1)), ∣ > (1 - c) ∙ 8λj}
where Card () represents the size of the set. For the term in Eq. 59,
(59)
(60)
Card ({j ∈ 京：牌] > C73}) ≤ X Cy = $ X
j∈S* 8 J	j∈S*
8	, T
≤ C ∙ 7ɑmin(λs*)2 k(β	)s*kλS*'1
8
=C ∙ 7αmin(λs*)2 k(β	- β)S*kλS*,1
Bounding ∣∣(βk-1 - β*)s* ∣∣ι. Following the derivation of Eq. 58, We can obtain
(ρ--ξ)M1- β*∣2
21	C 17	—	7	, T
≤ (ρ^^-ξj∣λs*∣∣2+ ɪIKe - β )s*∣∣λs* ,ι - 4k(e	- β )s* llʌs*,ι,
which implies
一	12	C 17	—
k(βkτ- β*)s* Iy,1 ≤ ^7-ξ ∣λs*∣∣2 + ∙y k(βkτ- β*)s*kλs* ,1.
Let a ≥ 0 beaconstant.If ∣(βk-1 - β*)s* ∣∣λs* ,1 ≤ P--ξ ∣∣λs* ∣∣2, then
k(βkτ - β*)s* ∣λ尹,1 ≤ (12 + 17α) JPMl.
IfiKekT- β*)S* ∣λs* ,1 > ρ-a-ξ llλs* ∣2, then
IKekT- β*)S* llʌs*,1 ≤ (-a + 了) ∣(.T - β*)S* ∣λs* ,1
≤ (5 + -7^) ∣λS* ∣2∣∣(ekτ - β*)∣2
(LemnaG3) ≤ ⅞7 竺+ J ) g.
2 ∖α 7 J ρ- - ξ
Combining the above two cases yields
∣(βk-1 - e*)s* ∣λs* ,1 ≤ minmax {12+T, 125 (1+⅛7
≤ (423) ∣λs* ∣2
≤ I 14 J P--ξ,
)) "12
34
Published as a conference paper at ICLR 2022
where the last inequality is obtained by taking a
bounded by
255
34 .
Therefore, the term in Eq. 59 can be
Card ({j ∈ S* ： ∣βk-1∣ > C7αλj}) <
Now We bound the term in Eq. 60 as follows.
≤
≤
8∣∣(βk-1- β*)s* IIy,1
C ∙ 7a min(λs*)2
(	1692	、∣λs* ∣2
49ca min(λs* )2	ρ- - ξ
1692	∣λs* ∣∣2) 2
49cα(ρ- — ξ) ∖min(λ文)J '
Denote the set as S0 := {j ∈ S* :
(VLχ(β*) - VLλ(βk-1))j∣ > (1 - c) ∙ 7λj} and its size as S = ∣S0∣. Then
s = Card (j∈ ∈ S* : ∣(VLλ(β*) - VLx(βk-1)).∣ > (1 - c) ∙ 8λj
-1— X
(I-C) ∙ 7 j⅛
≤
(VLλ(β*)-VLλ(βk-1)) j
≤
≤
1
(I-C) ∙ 7
1
Σ
(VLλ(β*)-VLλ(βk-1)) j
j∈S0
√s0
min(λ尹)
(1 - c) ∙ 7 min(λSΓ)
∣∣VLλ(β*)-VLλ(βk-1)∣∣2
<
1
(1 - c) ∙ 7 min(⅛τ)
(ρ+)∣∣β*- "∣∣2.
The last inequality holds because Lx is (ρ+)-smooth in the restricted subspace by Lemma B.1.
Applying Lemma G.3, we have
s < 49(1-7Pmi)[λ尹)2 kβ*-βk-ιk2
<	64(ρ+)2	(15∕2∣λs* ∣2 Y <	3600
― 49(1 — c)2 min(λs*)2 ( ρ- — ξ ) ~ 49(1 — c)2
p+	2 llλs*l∣2	2
ρ- - ξ) ∖min(λ炉)
Combining the above bounds for the terms in Eq. 59 and Eq. 60, we have
Il 就)Eo < mɪɪl)
c∈(U,))
1692	1
49cα
121
3600
P- - ξ + 49(1 - c)2
2
∣∣λs*∣∣2	2
< Iamm(P- -ξ) +144
< S
P+
P- - ξ
(P+ ʌ2
P-- - ξ) ) ∖min(λ广)
∣∣λs*∣∣2	2
min(λ广)
The second last inequality is obtained by taking C = 7.
Finally, if we can show φχ(βk) - φλ(β*) < W ∣∣λ5* ∣∣2, then we can show by math induction
that ∣∣(βk)s*∣∣0 < s for all k.
Taking N = βk-1 in Eq. 53, we have
Φλ(βk) < Φλ(βk-1) -2 S -ρ+ ∣∣βk - βk-ι∣∣2 < Φλ(βk-1).
Therefore,
φλ(βk) - φλ(β*) < φλ(βk-1) - φλ(β*) < ^/⅛∣λs*∣2.
P- - ξ
□
35
Published as a conference paper at ICLR 2022
Lemma G.5 (Statistical error). Assume Assumption G.1. If
ωλ(β) ≤ e, with e ≤ 1/2,
IIe - β*∣∣0 ≤ s* + 23,
N ≥ 8 ∣[Ln(β*)]j∣ ,
then thefollowing relations hold true.
kβ -"
φλ(β)- φλ(β*)
.. .^ .
φλ (β)- φλ(βλ)
≤ (e + 17) ∣∣λs* ∣∣2 ≤	21/8
ρ- - ξ 一
3e(e + 17/8) ∣∣λs* ∣∣
7/8 - e
llʌs* l∣2,
2 ≤ J1∕2
ρ- - ξ — P- - ξ
DU
≤ e (3⅛≡+51") R ≤ 弋 Nk2.
∖ 7/8 - e	P- - ξ P- - ξ
ρ- - ξ
≤
Proof. Since ∣β - β* ∣0 ≤ s* + 23, then by the RSC and RSS in Lemma B.1,
(P- - ξ)kβ* - βkl ≤ (β - β*)τVLλ(β) - (β - β*)TVLλ(β*).	(61)
Let ξ ∈ ∂∣∣β∣1 be the subgradient that attains the minimum in ωλ(β). Adding and subtracting
(β - β* )τ(λ o ξ) to the right-hand side of Eq. 61 yields
(P- - ξ)∣β* - β股 ≤ (β - β*)τ(VLλ(β) + λ o ξ) - (β - β*)τVLλ(β*)
×-----------V-----------} X-------V--------}
(i)	(ii)
-(β - β*)τ(λ o ξ).
(iii)
Now we bound the terms (i)-(iii).
Bound (i) using ωλ(β). Since ξ attains the minimum in ωλ(β), by definition,
ωλ(β)=步第{Im(VL入网+又)}.
Therefore,
(i) = (β - β*)τ(VLλ(β) + λξ) ≤ ωλ(β)∣β - β*∣λ,ι ≤ e∣β - β*∣λj.
Bound (ii). Note that L∖ = Ln + Q∖. Thus,
∣(ii)∣ = ∣(β - β*)τVLn(β*) + (β - β*)τVQλ(β*)∣
≤ I (β - β*)τVLn(β*)∣ + ∣(β - β*)τVQλ(β*)∣
Since λj ≥ 8 ∣[Ln(β*)]j∣, We have
∣(β - β*)τVLn(β*)∣ ≤ 1∣β - β*kλ,ι.
8
Since q]. (0) = 0 and q1, (β) ≤ λj (Assumption B.1), it holds true that
∣(β - β*)τVQλ(β*)∣ = ∣(β - β*)τ* (VQλ(β*))s* ∣
≤ k(β - β*)s* kλs* ,1.
Therefore,
∣(ii)∣ ≤ 1 ∣β - β*k2 + k(β -β*)s*kλs*,ι∙
8
Bound (iii) by the definition of subgradient.
(iii) = (β - β*)τ(λ o ξ)
=hλS* o (β - β*)s* , ξS* i + hλs* o (β - β*)s*, ξs* i.
36
Published as a conference paper at ICLR 2022
By Holder,s inequality and the fact ∣∣ξ∣∣∞ ≤ 1,
hʌs* o (β - β*)s*, ξs* i ≥ -∣∣(β - β*)s* ∣∣λs* ,1.
Since ξ ∈ 训β∣∣ι and that βp = 0,
hλS* o (B	- β*)s*, ξS*i	=	hλS* o	βS*,	ξS*i	= l∣βS*l∣λs*,1	= IKe -	β*)s*l∣λs*,1.	(62)
Combining the above three equations, it holds true that
(iii) ≥ -Il(B - β*)s*∣∣λ.*,ι + Il(B - β*)s*∣∣λ尹,1.
Combine (i)-(iii). Combining the bounds for (i)-(iii), it holds true that
(P- -ξ)kβ*一例2
≤ (e + 8 + 1 + 1) k(B - β*)s* ∣∣λs* ,1 - (1 - e - w) k(B - β*)s*l∣λs*,1
=(e + ɪ) ||(B - b*)s* ∣∣λs*,1 - Q - e) k(B - β*)s*∣∣λs*,1	(63)
'---V---}
≥0
≤ (e + ɪ) k(B - β*)s* kλs* ,1.
Using the fact that ∣ (B - B*)s* ∣∣λs* ,1 ≤ ∣∣λs* ∣∣2∣∣(β — B*)s* ∣∣2, it implies the first conclusion:
∣∣β∙-β∣∣2 ≤ & + 17 "W .	(64)
P- - ξ
Now we prove the objective value. Since L∖ is convex on B, B* and ∣∙∣ 1 is convex,
Φλ(B*) ≥ Φλ(B) + (B* - B)T(VL入(B) + λ oξ),
which implies
Φλ(B) - Φλ(B*) ≤ (B - B*)τ(VLλ(B) + λoξ)
≤ IB - B*kλ,1ωλ(B) ≤ e∣B - B*kλ,1	(65)
Now we bound the norm ∣∣B - B* ∣λ,1 as follows. By triangle inequality,
IB -	B*kλ,1	≤	k(B	-	B*)s*	I— ,1 + k(B	-	BWkλ/,1.	(66)
To bound ∣∣(B 一 B*)s*llʌ›,1, moving the term ∣∣(B 一 B*)s*llʌ尹,1 in Eq. 63 to the left hand side
yields
(7/8 - e)∣(B - B*)s*l』,1 ≤ (e + 17∕8)∣(B - B*)s* ∣Λs*,1,
which implies
∣(B - B*)s*∣λ尹,1 ≤ ⅛1z/8∣(B - B*)S* ∣λs*,1.	(67)
/∕8 — e
Plugging it into Eq. 66, we have
llλs*l∣2IIB - B*l∣2
(68)
Combining it with Eq. 65, we have
Φλ(B)- Φλ(B*)
≤ 3e(e +17/8) Ds* ∣∣2
7/8 - e	P- - ξ
37
Published as a conference paper at ICLR 2022
τ-<∙	11	1	∙ ,ιι	1 I' ,ι	I / c∖ I /言 ∖ n ∙	Ue za Il ，	*	, rʌ ~ ι ι ∙
Finally, We derive the bound for the term φλ(β) - φλ(βλ). Since ∣∣β - βλ∣∣0 ≤ s* + 2s and φλ is
convex on the restricted space, then
.	.	.^	.	,	,	. ~Γ .	^ .
Φλ(β) - Φλ(βλ) ≤ (VLλ(β) + λ 0 ξ)>(β - βλ)
≤ ωλ(β)∣β - βλ∣λ,ι
≤ e (∣∣β - β*∣∣λ,ι + ∣∣β* - ∕3λ∣∣λ,ι)
(by Eq. 68) ≤ e
(3(e +17/8) ∣∣λs* ∣2
k 7/8 - e P- - ξ
+ l∣β* - ∕bλ∣∣λ,ι) ∙
(69)
Now we only need to bound the term ∣∣β* - /3入||入」.We can derive its bound following the same
,	1 ∙ 1-1 ,C 1 ♦	,1 i' . .1 .	/介 ∖ C 1 ∙ 1	∙11 ∙	1 ,1
steps as we derive Eq. 68 and using the fact that ωλ(βλ) = 0, which will imply that
kβλ -β*∣1 ≤
3(0+17/8) ∣∣λs*∣∣∣
7/8 - 0 ρ- - ξ
51∕7∣λs* ∣∣
P- - ξ
Plugging it into Eq. 69, we have
. . .^ .
φλ(β)- φλ (βλ)
≤ ( 3(e +17/8)
≤ I 7/8 - e
+
生∣∣
P- - ξ
Assuming ≤ 1/2, then
.. .^ .
φλ (P)- φλ(BX)
≤ 29/2ps*|||
—P- - ξ
□
Lemma G.6 (Optimality when λ decreases). Assume Assumption G.1. If
a%一 (β) ≤ e-,
λt = η o λt-ι	with	ηj ∈ [0.9,1],
kβ∣0 ≤ 3,
(70)
(71)
(72)
then
ωλt (B) ≤
et-1 + 0.2
-09-
，一、 , ^
φλt(p) - φλt (Bλt) ≤
3 (et-i + 17/8)
0.9(7/8 - et-i)
et-1 + 0.2 Il (λt)s* ∣∣∣
09	P- - ξ
If et-ι ≤ 1/4, then
ωλt (B) ≤ 2
，一、 , ^
φλt(B) - φλt (pλt) ≤
10∣(λt)s*k∣
P- - ξ
Proof. Recall the definition of ωλt-1 (β).
"ʌt-ɪ(B)=ξ 图嬴 ι 卿{∕βkι(VLU(B)+λt-ιξ/)}
Let ξ ∈ 训Bk ι be the subgradient that attains the minimum in ω%-ι (B). Then
ωλt-1 (B) = mΩ { IIB(BB7ι,ι(VLλt-1 (B) + λt-1ξ)}.
38
Published as a conference paper at ICLR 2022
Now we consider λt = η o A1. By definition,
Note that
3% 网=，圜嬴1 mΩ {m S ⑶+λtξ')}
≤ max {∣β-j∣lι(VLλt (β)+ λtξ)} ∙
VLλt (β) + λtξ = (VLλt-1 (β) + λt-1ξ)
+ (At- λ" o ξ
+ (VQλt (β)-VQλ-ι (β)),
which implies
3λt 网 ≤ maΩ {∣β- β∣[ι (VLλt-1 (⑶ + λt-1ξ)}
、------------------------：--V----------------'
⑴
((β - β)>....................
+ max < -r-----,-----(At - λt-i) o ξ
…Ike - βkλt,1
X--------------V-------------
(ii)
J
+ 步第{Ie-a],1 (vq%M- vq%t (β))} ∙
|
{z^
(iii)
J
Using the fact that ∣β 一 β 1∣λ%ι ≥ 0∙9∣β 一 β,kλt-ι,ι, term (i) can be bounded as follows.
⑴ ≤ 019 p⅜βt3J (VLλt-1 网 + λt-1ξ) ≤ 焉3λt-1 M ≤ ¾⅛ ∙
Using the fact ∣ξj | ≤ 1 and the fact that [λt-1 - λt]j- = g - 1) [λt]j∙ ≤ (焉-1) [λt]j-, term (ii)
can be bounded as follows.
d
(ii) ≤ X
j=ι
同一闻
kβ - βkλt,1
(L - 1、[%].= kβ-β 111 (ɪ - I- - 1
[0.9	八 tj kβ-β∣∣λt,ι (0.9 J	0∙9
Term (iii) is bounded similarly as term (ii). Since Iqlj (β) - qλ∕. (β) ∣ ≤ ∣ λj - λj ∣ , we have
d
(iii) ≤ X
j=ι
I βj-βj∣
kβ - βkλt,1
[λt-1
d
-λtj ≤ X
j=ι
I βj-βj∣
kβ - βkλt,1
Combining the bounds for (i)-(iii), we have
3λt (β) ≤
et-1 + 2	∈t-1 + 0∙2
---——	2 =	——-
0∙9-----------------------0∙9
EI r'	∕' .<	∕' ∙ r' ∙ 1	1 TL ɪ	F	1 .1	.	I ∕C∖	I /6、	〜	Ue
The first part of the proof is finished. Now we bound the term φ%t (β) - φ%t (β%). Since ∣∣β -
βλ ∣∣0 ≤ s* + 23 and φ% is convex on the restricted space, then
,, O
φλt(e) - φλt (∕bλt)
≤ (VL%t (β) + λt o ξ)>(β - βλj
≤ 3λt (β)∣β - βλt∣λt,ι
≤ I +0∙2 (∣∖β - β*kλt,ι + kβ* - βλt kλt,ι)	(73)
0∙9
39
Published as a conference paper at ICLR 2022
Following the same way we derive Eq. 67, we can obtain that
	∣(β -β*)mg)尹,ι ≤ e7∕8+：[8 ∣(β -β*)s*∣(λt)s* ,ι.
Therefore,	∣β - β*kλt,ι = k(β - β*)炉k(λt)尹,1 + k(β - β*)s* k(λt)s*,1	(74) 3 ≤ 7/8 - et 1 k(β - β )s*∣(λt)s*,1 ≤ 3k(λ⅛⅛∣β-β*∣2 7/8 — et-i 3∣∣ (X)S* 12 (et-1 + 17) ∣∣(λt-I)S* ∣∣2 (By Lemma G.5) ≤ -⅛⅛~-~~- 	-			 —7/8 - et-i	ρ- - ξ =3 (et-i + 17/8) Il(X)S* ∣∣2	(75) 0.9(7/8 — et-i) ρ- — ξ
ɪ T ∙ ,ι	,ι ,	II c+ za	II	ι	.ι i` . .ι .	/ G^ ∖ -C
Using the same arguments for the term	∣∣β*	- β%k%,ι	and	using the fact that ωλt (β%) ≤ 0 ∙
we obtain that
	Hβλ - β∕≤ 3(0+17/8) I"* I2 =51"∣∣(λt)s*∣∣2 t	—	7/8 - 0	ρ- - ξ	P- - ξ	.
Combining this inequality with Eq. 73 and Eq. 75, we have
	，一、 , ^ Φ×t M- Φ×t (βλt) ≤ ( 3 (et-i + 17/8) +51/7、et-i + 0∙2 Mλt)s* 112 一 10.9(7/8 - et-i)	/ J	09	P- - ξ .
Assuming e- ι ≤ 1/4, then
	φλt (β) - φλt (βλt) ≤ 1≡≠. P- - ξ
□
Lemma G.7 (Coercivity of the gradient). [Lemma 3.11 in Bubeck (2014)] Letfbe a-strongly convex
b-smooth and on B ⊆ Rd. Thenfor all β, βf ∈ B , one has
"f(β) - Vf(β'),β -βi ≥ -αbvkβ -β0∣2 + ɪ IlVf(β) - Vf(β)∣∣2.
a + b	a + b
40
Published as a conference paper at ICLR 2022
H Details of Section 5: Extension To Unsupervised
Learning-to-learn Setting
In addition to the assumptions in the supervised setting, in this unsupervised setting, we assume
for each estimation problem, both Ln1 (Z1:n1) and Ln2 (Z1:n2) satisfy condition (b) and (c) in As-
sumption C.1. Similar to the supervised setting, the generalization error of the optimizer θU can be
bounded by
Lgen(P(P )； OU )
1 m	2
≤ E(ZLn,β*)〜P(P) IleT(Z1:ni; θu ) - β*k2 - m X IeT (Z(in 1 ; θu ) - β*(i)H2
i=1
generalization gap: Theorem 4.1
m2
+m XM(Z(in i； θu )-β 叫 ∣2.
i=1	2
(76)
(77)
supervised training error
However, in the unsupervised setting, the minimizer θU only guarantees that unsupervised training
loss function ^^ Pmm=I Ln (ZI；, βτ(Z(2< ； θU)) is small. It requires further derivations to show
that the corresponding supervised training loss in Eq. 77 is small, too.
Therefore, in the following, we bridge the gap between the supervised training loss and the unsuper-
vised training loss. Based on the proof of Theorem D.1, We know that kβτ(Z(i∖; θU)∣s*ko ≤ S.
Therefore, we can apply the restricted strong convexity of Ln2 to obtain the following inequality for
βτ = βτ(Z(in 1 ； θU), β* = β*(i), and Z = Z(22 for each i = 1,…，m.
kβτ - β*k2 ≤ 看(Ln2 (Z, βτ) - Ln2 (Z, β*) + VLn2 (Z, β* )> (β* - βτ))
2
≤	—	(Ln2	(Z, Bτ)	- Ln2	(Z, β	)	+	IlVLn2	(Z, β	)	k2k	(β - Bτ) k2)
ρ-	2	2	2
⇒ kβτ - β"∣∣2	≤	一	(Ln2	(Z, 3τ)	- Ln2	(Z, β*))+-2 IlVLn2	(Z,	β*)	k2
2	ρ-	2	2	ρ2-	2	2
Aggregating this inequality for i = 1, ∙∙∙ ,m, We can obtain the following inequality that bounds
the supervised training loss using the unsupervised loss.
m
m XMT (ZIin ι ； θu )-β* CI ≤ Tm
i=1	-
m
X kVLn2 (Z(in 2, β*⑴ )k
i=1
{^^^^^^^
statistical error
+
2(Z(in 2, βτ(Z(in ι ； θu ))-Ln2(Z(in 2, e*(i)
2
2
}
unsupervised training error
The right-hand-side of this inequality consists of a statistical error and the unsupervised training
error. Finally, to characterize how small the unsupervised training error can be, we can combine the
following inequality (by restricted strongly smooth) with the result of Theorem 3.1.
Ln2 (Zlin2 , βτ(Zlinι ； θU)) - Ln2 (Zlin2 , β*)
≤ Ln2 (Zlin2 , βτ(Zlinι ； θ*)) - Ln2 (Zlin2 , β*)
≤ kVβ Ln2 (Zlin2 , β*)∣∣2 kβτ(Zlinι ；。*)- β*∣∣2 + 然 kβτ(Zlin ；。*)- β*k2 ∙	(78)
1--------------{--------------'
bounded by Theorem 3.1
1-------------{-----------
bounded by Theorem 3.1
41
Published as a conference paper at ICLR 2022
Overall, the generalization error can be bounded by
Lgen(P(P )； θU )
m
≤ E(ZLn,β*)〜P(P) kβT (Z1：ni; θU ) - β*k2 - m X IeT(Zfl ι; θU ) - β*(i)
i=1
2
*
2
|
generalization gap: Theorem 4.1
m
/
4
+------
ρ- m
X	kvβLn(ZIn2,β*(i))k2 kβτ(z(：ni；θ*)-β*(i)k2+ρ+ kβτ(z(：n「。*)-。*叫
i=1
1--------------------{----------
statistical error
}|
一 {一一	,
bounded by Theorem 3.1
1---------------{-----------
bounded by Theorem 3.1
2
16
ρ2- m
、
m
X kVLn2 (Z(：n 2, β*(i))k2.
i=1
{^^^^^^^
statistical error
}
42
Published as a conference paper at ICLR 2022
I Details of Synthetic Experiments
I.1	A Gentle Review of Linear Regression and Precision Estimation
I.1.1	Sparse Linear Regression
A sparse linear regression model reads
y = x> β* + e
where β* is a sparse vector and e is Gaussian noise. Given n observations Zi：n
{(xι, yι), ∙∙∙ , (xn, yn)}, the goal is to estimate the vector β*.
In our method, we use the the least-square error to define the empirical loss function.
j=1
I.1.2	Sparse Precision Estimation
The sparse precision matrix estimation problem in Gaussian graphical models assumes the observa-
tion of n samples from a distribution N(0, Σ). Given the n samples, Xi：n := {xι,…,Xn} from
N(0, Σ), the goal is to estimate the precision matrix Θ* := Σ-1 which is the inverse covariance
matrix. This precision matrix represents the conditional independency among random variables.
A commonly used objective for estimating the precision matrix is the `1 -penalized log determinant
divergence:
.. ^ ..
-logdet(Θ) + hΘ, Σni + λkΘkι
where Σn is the sample covariance matrix. In our method, We use the likelihood term as the empiri-
cal loss.
_ , _. - - ，一、 ^ .
Ln(XI:n, Θ) = - log det(Θ) + hΘ, Σni
I.2	Data Preparation
For the sparse linear recovery problem, we follow the setting in Wang et al. (2014). We create the
synthetic data by sampling a set of estimation problems {((X(i),Y(i)), β*(i))}. In each problem,
the design matrix X (i) ∈ Rn×d contains n = 64 independent realizations of a random vector
x ∈ Rp with p = 256, 1024 in easy or difficult setting, respectively. x follows a zero mean Gaussian
distribution with covariance matrix (Σ)i,j = 0.9 ∙ 1{i=j} + 1 ∙ 1{i=j}. The true parameter vector
β*(i) has a sparsity s* = kβ*(i)∣∣o = 16 and its nonzero entries take values uniformly sampled
from (-2, -1) ∪ (ɪ, 2). The support set of each β*(i) is independently sampled from a union
support set S, with |S| = 128 to allow some similarity among the problems. The observation Y(i) is
sampled such that Y(i) - X(i)β*(i) is a n-dimensional Gaussian random vector with zero mean and
covariance matrix In . In all the experiments, 2000 such problems are used for training, 200 such
problems are used for validation, and 100 such problems are used for test.
In sparse precision matrix estimation problem, we follow the setting in Guillot et al. (2012). We
create the synthetic data by sampling a set of estimation problems {(Σni), Θ(i))}. In each problem,
the ground truth precision matrix is generated in the following ways: the lower diagonal part (lower
triangular parting, exluding diagonal) of true precision matrix Θ has a sparsity s*, that are uniformly
selected from a union support with size S. After selecting the nonzero entries, we assign them values
uniformly from (-2, -ɪ) ∪ (ɪ, 2) and let the upper diagonal part has the same value as lower di-
agonal part. Finally, a multiple of the identity was added to the resulting matrix so that the smallest
eigenvalue was equal to 1. In this way, Θ was insured to be sparse, positive definite, and well-
conditioned. After the precision matrix is generated, we generate the observational samples for each
precision matrix, by generating n independent samples of the random vector X 〜 N(0, (Θ(i))-1).
The samples are denoted by X(i) ∈ Rn×d . Using the samples, we can compute the sample covari-
ancematrix Σni) = n Pn=I(X(i))TX(i).
43
Published as a conference paper at ICLR 2022
In all the experiments, 2000 such problems are used for training, 200 such problems are used for val-
idation, and 100 such problems are used for test. Weuse (n, S, s*) = (100,375, 75), (200, 600,150)
for d = 50, 200, respectively.
I.3	Baseline Implementation
ALISTA: We follow the implementation in Liu et al. (2019a).
RNN: The RNN is designed following Andrychowicz et al. (2016). Initialize the β0 as 0. At every
step, we compute the gradient NeL(βt) w.r.t. to objective as the input to a LSTM with hidden
dimension equals to 128. Then we use the output of the LSTM as the increment:
βt+1 = βt + LSTM(NβL(βt))	(79)
We repeat this iteration 20 steps and use β20 as our final output.
RNN-'i： RNN-'ι use the same architecture as RNN. The only difference is we add an extra learn-
able parameter λt for soft thresholding at each step:
βt+1 = βt + LSTM(NβL(βt))	(80)
βt+1 = ηλt (βt+1)	(81)
where the softthresholding ηλ is an elementwise operator maps ηλ(x) = 0 is |x| < λ and ηλ(x) =
sign(x)(|x| - λ).
GLASSO: we use the implementation in the sklearn package.
GGM: We follow the implementation in Belilovsky et al. (2017).
APF: We follow the implementation in Wang et al. (2014). The specific algorithm steps are sum-
marized in Algorithm 3.
Algorithm 3: The Approximate Path Following Method
Input: λtgt > 0, eopt > 0 (Here we assume e∩pt《λtgt∕4), η ∈ [0.9,1)
i Initialize βo J 0, Lo J Lmi∏, λo = ∣∣NL(0)∣∣∞,N J (λ°∕λtgt)/log(η-1).
2	for t = 1, ..., N - 1 do
3	λt J ηtλ0
4	et J λt∕4
5	{βt, Lt} J Proximal-Gradient(λt, et, βt-1, Lt-1, R) as in Algorithm 4.
6	end
7	λN J λtgt
8	eN J eopt
9	{βN, LN} J Proximal-Gradient(λN, eN, βN-1, LN-1, R).
io	return {∕3t}N=ι.
Algorithm 4: The Proximal Gradient Method
Input: λt > 0, et > 0, β0 ∈ Rp, L0 > 0
i	Initialize k J 0.
2	repeat
3	kJk+1
4	Linit J max{Lmin, Ltk-1/2}
5	βtk, Ltk J Line-Search(λt, βtk-1, Linit) as in Algorithm 5.
6	until ωλt (βtk) ≤ et as defined in Equation 82;
7	βt J βk
8	Lt J Ltk
,	C T 1
9	return {βt, Lt }.
44
Published as a conference paper at ICLR 2022
Algorithm 5: The Line Search Method
Input: λt > 0, βk-1 ∈ Rd, Linit > 0,R > 0
1	repeat
2	βt J TLk,λt (βk-1) as defined in Equation 83
3	if φλt(βk) >ΨLk,λt(βk； βk-1) then
4	I Lk J Lk.
5	end
6	until φλt (βtk) ≤ ψLk,λt (βtk, βtk-1) as defined in Equations 84 and 85;
7	βtJβk	"
8	Lt J Ltk
, c τ ι
9	return {βt, Lt }.
min max < %——g ； (VLλ(β) + λξ0) >
ξ0∈∂kβkι m∈Ω[kβ-β0kι I	λ(β)+ ξ7∫
L (βk-d =/0	if lβj I ≤ XtlL
Lt ,λt t % Isign (βj )(∣β∕-λt∕Lk) if∣βj∣>λt∕Lk
. . ~ . 、 ..
φλ(β) = Leλ(β) +λkβk1
(82)
(83)
(84)
T	Lk	2
ΨLk,λt (β; βk-1) = L (βk-1) + VL (βk-1) (β - βk-1) + 于 Ile - βk-1∣∣2 + Pλt (β) (85)
I.4	Training and Evaluation
We trained the learning-based methods to minimize the weighted loss. That’s to say, for each sparse
linear recovery problem, given the sequence of outputs (β1, ..., βT), the loss is computed as:
T
L = X YT-ikβi- β*k2	(86)
i=1
where γ decreasing from 0.9 to 0.1 during the training process. The procedure is similar in the
sparse precision matrix estimation problem. We use optimizer Adam (Kingma & Ba, 2014). For
sparse linear recovery problem, we use batch size 10, we train 500 epochs with learning rate 1e-
4 and select the model based on the l2 loss on valid data. For sparse precision matrix estimation
problem, we use batch size 40, we train 200 epochs with learning rate 1e-3 and select the model
based on Frobenius loss on valid data.
For classical algorithms, we select their parameters, λ for APF and ρ for GISTA, based on their
performance on the validation set. Specifically, we evaluate APF or GISTA with ρ or λ from
{0.01, 0.025, 0.05, 0.1, 0.2} and use one with the best recovery error in test.
Table 4: Training time for SPE (minutes)
Table 3: Training time for SLR (minutes)
Sizes	d = 256	d= 1024	Sizes	d = 50	d= 100
PLISA	393	462	PLISA	35	39
ALISTA	176	271	GGM	14	43
RNN	96	99	GISTA	176	116
RNN'1	101	106	APF	316	331
APF	214	426	GLASSO	42	57
We report the total training time for learning based methods as well as the parameter tuning time for
classical algorithms. From Table 3 and Table 4, we can see that training a learning-based method is
45
Published as a conference paper at ICLR 2022
cheap in our experiments, as 1) A single forward for PLISA or GGM is very fast as stated in Table 1.
2) We can easily parallel the computations to handle a batch of problems during the training time,
as the learning-based methods do not require line-search.
The evaluation is performed on a server with CPU: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz,
GPU: Nvidia GTX 2080TI, Memory 264G, in single thread.
I.5	Ablation Study
Table 5: Ablation study of PLISA (p = 1024). TPR
is the true positive rate of recovering the nonzero en-
tries of β*. FPS is the cardinality of false positive
entries. Note that the true sparsity level is s* =
16. Standard deviations over 100 test problems are
present in the parantheses.
P PLISA	IPLISA-SingIel PLISA-'1
'2 error	1.34 (2.28)	18.25 (6.06)	2.20 (2.76)
TPR	0.99 (0.01)	0.62 (0.19)	0.99 (0.02)
FPS	16.65 (13.60)	51.07 (6.67)	25.11 (13.30)
Figure 6: Ablation study.
We consider two variants of PLISA to perform the ablation study. One is PLISA-single which
employs a single regularization parameter across different entries, i.e., η1 = …=ηd and λ;=
…=λ]. The other is PLISA-'1 which does not learn the penalty function but uses the '1 norm,
i.e., Pw(λ, β) = kλ ◦ βk1. Fig. 6 and Table 5 show the vanilla PLISA performs better than
alternatives. Especially, it has a much better accuracy than PLISA-single. Therefore, this ablation
study has validated the effectiveness of using entry-wise regularization parameters and learning the
penalty function.
I.6	Real-world Experiments
We use the following 3 real-world datasets.
(1)	Gene - a single-cell gene expression dataset that contains expression levels of 45 transcription
factors measured at different time-points. We follow Ollier & Viallon (2017) to pick the transcription
factor, EGR2, as the response variable and the other 44 factors as the covariates. In this dataset, each
time-point is considered as an estimation problem. In each estimation problem, the goal is learning
the regression weights β* on the 44 factors to predict the expression level of the target transcription
factor, EGR2. Therefore, in this problem, p = 44. This dataset contains the gene expression data for
120 single cells at 8 different time points. For each time point, we randomly split the 120 samples
into 6 sets, so that each set contains 20 samples. By doing this, we construct 48 estimation problems
each of which contains 20 samples. 10 samples are used for recovering the parameters β* and the
other 10 samples are used for evaluate it by computing the least-square error. We use 36 problems
for training, 6 problems for validation, and 6 problems for testing.
(2)	Parkinsons - a disease dataset that contains symptom scores of Parkinson for different patients.
Each patient is considered as an estimation problem. In each estimation problem, the goal is learning
the regression weights β* on 19 bio-medical features to predict the symptom score. This dataset
contains 42 patients, so there are 42 estimation problems in total. Each patient is examined at
different time-point and each time a sample is generated. For each patient we randomly select 100
samples, so that eventually our dataset contains 42 estimation problems each of which contains 100
samples. 50 are used for recovering the parameter β* and 50 are used for evaluation. We use 28
problems for training, 7 problems for validation, and 7 problems for testing.
(3)	School - an examination score dataset of students from 139 secondary schools in London. Each
school is considered as an estimation problem. In each estimation problem, the goal is learning the
regression weights β* on 28-dimensional school and student features to present the exam scores
for all students. We use the dataset from Malsar package (Zhou et al., 2011). For each school, we
randomly select 40 students as the samples. Since some schools contain less than 40 students, we
finally obtain 125 estimation problems (schools) each of wich contains 40 samples. 20 are used for
46
Published as a conference paper at ICLR 2022
recovering the parameter β* and 20 are used for evaluation. We use 100 problems for training, 10
problems for validation, and 15 problems for testing.
On each dataset, we train each learning-based algorithm for 200 epochs using Adam with learning
rata 1e-3. The batch size is set to be 6.
47