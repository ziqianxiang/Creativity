Published as a conference paper at ICLR 2022
Data-Driven Offline Optimization for
Architecting Hardware Accelerators
Aviral Kumart, * Amir Yazdanbakhsht
Milad Hashemi Kevin SWersky Sergey Levine*
Google Research * UC Berkeley (t Equal Contribution)
aviralk@berkeley.edu, ayazdan@google.com
Ab stract
To attain higher efficiency, the industry has gradually reformed towards application-
specific hardware accelerators. While such a paradigm shift is already starting to
show promising results, designers need to spend considerable manual effort and
perform large number of time-consuming simulations to find accelerators that can
accelerate multiple target applications while obeying design constraints. Moreover,
such a “simulation-driven” approach must be re-run from scratch every time the set
of target applications or design constraints change. An alternative paradigm is to use
a “data-driven”, offline approach that utilizes logged simulation data, to architect
hardware accelerators, without needing any form of simulations. Such an approach
not only alleviates the need to run time-consuming simulation, but also enables
data reuse and applies even when set of target applications changes. In this paper,
we develop such a data-driven offline optimization method for designing hardware
accelerators, dubbed Prime, that enjoys all of these properties. Our approach learns
a conservative, robust estimate of the desired cost function, utilizes infeasible points
and optimizes the design against this estimate without any additional simulator
queries during optimization. Prime architects accelerators—tailored towards
both single- and multi-applications—improving performance upon stat-of-the-
art simulation-driven methods by about 1.54× and 1.20×, while considerably
reducing the required total simulation time by 93% and 99%, respectively. In
addition, Prime also architects effective accelerators for unseen applications in a
zero-shot setting, outperforming simulation-based methods by 1.26×.
1	Introduction
The death of Moore’s Law [11] and its spiraling effect on the semiconductor industry have driven the
growth of specialized hardware accelerators. These specialized accelerators are tailored to specific
applications [64, 47, 41, 53]. To design specialized accelerators, designers first spend considerable
amounts of time developing simulators that closely model the real accelerator performance, and then
optimize the accelerator using the simulator. While such simulators can automate accelerator design,
this requires a large number of simulator queries for each new design, both in terms of simulation
time and compute requirements, and this cost increases with the size of the design space [65, 53, 25].
Moreover, most of the accelerators in the design space are typically infeasible [25, 64] because of
build errors in silicon or compilation/mapping failures. When the target applications change or a new
application is added, the complete simulation-driven procedure is generally repeated. To make such
approaches efficient and practically viable, designers typically “bake-in” constraints or otherwise
narrow the search space, but such constraints can leave out high-performing solutions [9, 44, 7].
An alternate approach, proposed in this work, is to devise a data-driven optimization method that only
utilizes a database of previously tested accelerator designs, annotated with measured performance
metrics, to produce new optimized designs without additional active queries to an explicit silicon
or a cycle-accurate simulator. Such a data-driven approach provides three key benefits: (1) it
significantly shortens the recurring cost of running large-scale simulation sweeps, (2) it alleviates
the need to explicitly bake in domain knowledge or search space pruning, and (3) it enables data
re-use by empowering the designer to optimize accelerators for new unseen applications, by the
virtue of effective generalization. While data-driven approaches have shown promising results in
1
Published as a conference paper at ICLR 2022
One-Time
Collected Dataset
(TPU, NVDLA, ShiDianNao)
;Conservative ∣
:Learned Model：
Optimizers
Optimized
Hardware Accelerators
Figure 1: Overview of Prime. We use a one-time collected dataset of prior accelerator designs, including
TPU-style [65], NVDLA-style [42], and ShiDianNao-style [10] accelerators to train a conservative surrogate
model, which is used to design accelerators to meet desired goals and constraints.
biology [14, 5, 57], using offline optimization methods to design accelerators has been challenging
primarily due to the abundance of infeasible design points [64, 25].
The key contribution of this paper is a data-driven approach, Prime , to automatically architect high-
performing application-specific accelerators by using only previously collected offline data. Prime
learns a robust surrogate model of the task objective function from an existing offline dataset, and
finds high-performing application-specific accelerators by optimizing the architectural parameters
against this learned surrogate function, as shown in Figure 1. While naively learned surrogate
functions usually produces poor-performing, out-of-distribution designs that appear quite optimistic
under the learned surrogate [35, 5, 57]. The robust surrogate in Prime is explicitly trained to prevent
overestimation on “adversarial” designs that would be found during optimization. Furthermore,
in contrast to prior works that discard infeasible points [25, 57], our proposed method instead
incorporates infeasible points when learning the conservative surrogate by treating them as additional
negative samples. During evaluation, Prime optimizes the learned conservative surrogate.
Our results show that Prime architects hardware accelerators that improve over the best design in the
training dataset, on average, by 2.46× (up to 6.7×) when specializing for a single application. In
this case, Prime also improves over the best conventional simulator-driven optimization methods
by 1.54× (up to 6.6×). These performance improvements are obtained while reducing the total
simulation time to merely 7% and 1% of that of the simulator-driven methods for single-task
and multi-task optimization, respectively. More importantly, a contextual version of Prime can
design accelerators that are jointly optimal for a set of nine applications without requiring any
additional domain information. In this challenging setting, Prime improves over simulator-driven
methods, which tend to scale poorly as more applications are added, by 1.38×. Finally, we show
that the surrogates trained with Prime on a set of training applications can be readily used to obtain
accelerators for unseen target applications, without any retraining on the new application. Even in this
zero-shot optimization scenario, PRIME outperforms simulator-based methods that require re-training
and active simulation queries by up to 1.67×. In summary, PRIME allows us to effectively address
the shortcomings of simulation-driven approaches, (1) significantly reduces the simulation time, (2)
enables data reuse and enjoys generalization properties, and (3) does not require domain-specific
engineering or search space pruning.
2	Background on Hardware Accelerators
The goal of specialized hardware accelerators—Google TPUs [29, 23], Nvidia GPUs [43], Graph-
Core [21]—is to improve the performance of specific applications, such as machine learning models.
To design such accelerators, architects typically create a parameterized design and sweep over
parameters using simulation.
Target hardware accelerators. Our primary evaluation uses an industry-grade and highly param-
eterized template-based accelerator following prior work [65]. This template enables architects to
determine the organization of various components, such as compute units, memory cells, memory,
etc., by searching for these configurations in a discrete design space. Some ML applications may have
large memory requirements (e.g., large language models [6]) demanding sufficient on-chip memory
resources, while others may benefit from more compute blocks. The hardware design workflow
directly selects the values of these parameters. In addition to this accelerator and to further show
the generality of our method to other accelerator design problems, we evaluate two distinct dataflow
accelerators with different search spaces, namely NVDLA-style [42] and ShiDianNao-style [10]
from Kao et al. [30] (See Section 6 and Appendix C for a detailed discussion; See Table 6 for results).
2
Published as a conference paper at ICLR 2022
Table 1: The accelerator design space parameters for the primary accelerator search space targeted in this work.
The maximum possible number of accelerator designs (including feasible and infeasible designs) is 452,760,000.
Prime only uses a small randomly sampled subset of the search space.
Accelerator Parameter ∣ # Discrete Values ∣∣ Accelerator Parameter ∣ # Discrete Values
# of PEs-X	10	# of PEs-Y	10 7
PE Memory	~1	# of Cores	
Core Memory		# of Compute Lanes	10
Instruction Memory	-4	Parameter Memory	5
Activation Memory	~1	DRAM Bandwidth	6
How does an accelerator work? We briefly explain the
computation flow on our template-based accelerators (Fig-
ure 2) and refer the readers to Appendix C for details on
other accelerators. This template-based accelerator is a 2D
array of processing elements (PEs). Each PE is capable
of performing matrix multiplications in a single instruc-
tion multiple data (SIMD) paradigm [20]. A controller
orchestrates the data transfer (both activations and model
Figure 2: An industry-level machine learn-
ing accelerator [65].
parameters) between off-chip DRAM memory and the on-chip buffers and also reads in and manages
the instructions (e.g. convolution, pooling, etc.) for execution. The computation stages on such
accelerators start by sending a set of activations to the compute lanes, executing them in SIMD
manner, and either storing the partial computation results or offloading them back into off-chip
memory. Compared to prior works [25, 10, 30], this parameterization is unique—it includes multiple
compute lanes per each PE and enables SIMD execution model within each compute lane—and
yields a distinct accelerator search space accompanied by an end-to-end simulation framework. More
details in Appendix C.
3	Problem Statement, Training Data and Evaluation Protocol
Our template-based parameterization maps the accelerator, denoted as x, to a discrete design space,
X = [x1, x2, ∙ ∙ ∙ , XK], and each Xi is a discrete-valued variable representing one component of
the microarchitectural template, as shown in Table 1 (See Appendix C for the description of other
accelerator search spaces studied in our work). A design maybe be infeasible due to various reasons,
such as a compilation failure or the limitations of physical implementation, and we denote the set
of all such feasibility criterion as Feasible(x). The feasibility criterion depends on both the target
software and the underlying hardware, and it is not easy to identify if a given x is infeasible without
explicit simulation. We will require our optimization procedure to not only learn the value of the
objective function but also to learn to navigate through a sea of infeasible solutions to high-performing
feasible solutions x* satisfying FeaSible(x*) = 1.
Our training dataset D consists of a modest set of accelerators xi that are randomly sampled from the
design space and evaluated by the hardware simulator. We partition the dataset D into two subsets,
Dfeasible and Dinfeasible. Let f (x) denote the desired objective (e.g., latency, power, etc.) we intend
to optimize over the space of accelerators x. We do not possess functional access to f (x), and
the optimizer can only access f (x) values for accelerators x in the feasible partition of the data,
Dfeasible. For all infeasible accelerators, the simulator does not provide any value of f (x). In addition
to satisfying feasibility, the optimizer must handle explicit constraints on parameters such as area
and power [13]. In our applications, we impose an explicit area constraint, Area(x) ≤ α0, though
additional explicit constraints are also possible. To account for different constraints, we formulate
this task as a constrained optimization problem. Formally:
min f (x) s.t. Area(x) ≤ α0, FeaSible(x) = 1
x	0	0	(1)
on D = Dfeasible ∪ Dinfeasible = {(x1, y 1), ∙ ∙ ∙ , (XN, yN)} ∪ {xι, ∙ ∙ ∙ , xN0 }
While Equation 1 may appear similar to other standard black-box optimization problems, solving it
over the space of accelerator designs is challenging due to the large number of infeasible points, the
need to handle explicit design constraints, and the difficulty in navigating the non-smooth landscape
(See Figure 3 and Figure 11 in the Appendix) of the objective function.
What makes optimization over accelerators challenging? Compared to other domains where
model-based optimization methods have been applied [5, 57], optimizing accelerators introduces a
3
Published as a conference paper at ICLR 2022
Figure 3: Left: histogram of infeasible (right orange bar with large score values) and feasible (left cluster of
bars) data points for MobileNetEdgeTPU; Right: zoomed-in histogram (different number of bins) focused on
feasible points highlighting the variable latencies.
Table 2: The description of the applications, their domains, number of (convolutions, depth-wise convolutions,
feed-forward) XLA ops, model parameter size, instruction sizes in bytes, number of compute operations.
Name	∣ Domain ∣ # of XLA Ops(Conv, D/W, FF) ∣ Model Param ∣ Instr. Size ∣ # of Compute Ops.
MobileNetEdgeTPU	Image Class.	(45,13,1)	3.87 MB	476,736	1,989,811,168
MobileNetV2	Image Class.	(35,17,1)	3.31 MB	-416,032	609,353,376
MobileNetV3	Image Class.	(32,15,17)	5.20 MB	1,331,360	449,219,600
M4	Object Det.	(32,13, 2)	6.23 MB	-317,600	3,471,920,128
M5	Object Det.	(47, 27, 0)	2.16MB	-328,672	939,752,960
M6	Object Det.	(53,33,2)	0.41 MB	-369,952	228,146,848
U-Net	Image Seg.	(35, 0, 0)	3.69 MB	-224,992	13,707,214,848
t-RNN Dec	Speech Rec.	(0, 0,19)	19 MB	-915,008	40,116,224
t-RNN Enc	Speech Rec.	(0,0,18)	一	21.62 MB	-909,696	45,621,248
number of practical challenges. First, accelerator design spaces typically feature a narrow manifold
of feasible accelerators within a sea of infeasible points [41, 53, 17], as visualized in Figure 3 and
Appendix (Figure 12). While some of these infeasible points can be identified via simple rules (e.g.
estimating chip area usage), most infeasible points correspond to failures during compilation or
hardware simulation. These infeasible points are generally not straightforward to formulate into the
optimization problem and requires simulation [53, 44, 64].
Second, the optimization objective can exhibit high sensitivity to small variations in some architecture
parameters (Figure 11b) in some regions of the design space, but remain relatively insensitive in other
parts, resulting in a complex optimization landscape. This suggests that optimization algorithms based
on local parameter updates (e.g., gradient ascent, evolutionary schemes, etc.) may have a challenging
task traversing the nearly flat landscape of the objective, which can lead to poor performance.
Training dataset. We used an offline dataset D of (accelerator parameters, latency) via random
sampling from the space of 452M possible accelerator configurations. Our method is only provided
with a relatively modest set of feasible points (≤ 8000 points) for training, and these points are the
worst-performing feasible points across the pool of randomly sampled data. This dataset is meant to
reflect an easily obtainable and an application-agnostic dataset of accelerators that could have been
generated once and stored to disk, or might come from real physical experiments. We emphasize
that no assumptions or domain knowledge about the application use case was made during dataset
collection. Table 2 depicts the list of target applications, evaluated in this work, includes three
variations of MobileNet [23, 50, 27], three in-house industry-level models for object detection (M4,
M5, M6; names redacted to prevent anonymity violation), a U-net model [48], and two RNN-based
encoder-decoder language models [22, 24, 49, 38]. These applications span the gamut from small
models, such as M6, with only 0.4 MB model parameters that demands less on-chip memory, to the
medium-sized models (≥ 5 MB), such as MobileNetV3 and M4 models, and large models (≥ 19 MB),
such as t-RNNs, hence requiring larger on-chip memory.
Evaluation protocol. To compare state-of-the-art simulator-driven methods and our data-driven
method, we limit the number of feasible points (costly to evaluate) that can be used by any algorithm
to equal amounts. We still provide infeasible points to any method and leave it up to the optimization
method to use it or not. This ensures our comparisons are fair in terms of the amount of data
available to each method. However, it is worthwhile to note that in contrast to our method where
worse-quality data points from small offline dataset are used, the simulator-driven methods have an
inherent advantage because they can steer the query process towards the points that are more likely
to be better in terms of performance. Following prior work [5, 57, 58], we evaluate each run of a
4
Published as a conference paper at ICLR 2022
method by first sampling the top n = 256 design candidates according to the algorithm’s predictions,
evaluating all of these under the ground truth objective function and recording the performance of
the best accelerator design. The final reported results is the median of ground truth objective values
across five independent runs.
4	Prime: Architecting Accelerators via Conservative Surrogates
As shown in Figure 4, our method first learns a conservative surrogate model of the optimization
objective using the offline dataset. Then, it optimizes the learned surrogate using a discrete optimizer.
The optimization process does not require access to a simulator, nor to real-world experiments beyond
the initial dataset, except when evaluating the final top-performing n = 256 designs (Section 3).
Learning conservative surrogates using logged
offline data. Our goal is to utilize a logged dataset
of feasible accelerator designs labeled with the de-
sired performance metric (e.g., latency), Dfeasible,
and infeasible designs, Dinfeasible to learn a map-
ping fθ : X → R, that maps the accelerator con-
figuration x to its corresponding metric y . This
learned surrogate can then be optimized by the
optimizer. While a straightforward approach for
learning such a mapping is to train it via supervised
regression, by minimizing the mean-squared error
：回中{河yJi=n f4.
Feasible	/***
ifgf {MM -
Infeasible
∙fθ(xi)
fθ	fθ (Xj)
f⅛* PPtmizerf {χ-}k=/
O>	Opt =
'fθ	fθ(x-))
L3 = fθ (X-)
L2 = fθ (Xj)
A
Li = (fθ(χ⅛) - y
X1	Xc
Accelerator Configuration
Figure 4: Overview of PRIME which trains a con-
servative surrogate fθ (xi) using Equation 3. Our
neural net model for fθ (x) utilizes two transformer
layers [59], and a multi-headed architecture which is
pooled via a soft-attention layer.
Eχi,yi~d[(fθ(Xi) - yi)2], prior work [35, 36, 57] has shown that such predictive models can arbitrar-
ily overestimate the value of an unseen input Xi. This can cause the optimizer to find a solution x*
that performs poorly in the simulator but looks promising under the learned model. We empirically
validate this overestimation hypothesis and find it to confound the optimizer in on our problem
domain as well (See Figure 13 in Appendix). To prevent overestimated values at unseen inputs from
confounding the optimizer, we build on COMs [57] and train fθ (X) with an additional term that
explicitly maximizes the function value fθ(X) at unseen X values. Such unseen designs X, where the
learned function fθ (X) is likely to be overestimated, are “negative mined” by running a few iterations
of an approximate stochastic optimization procedure that aims to maximize fθ in the inner loop. This
procedure is analogous to adversarial training [19]. Equation 2 formalizes this objective:
θ* := argmin L⑻ := EXi,yi~Dfeasible [(fθ (Xi)- yi^] - aEx—~Opt(%) [fθ (X-∙⑵
X- denotes the negative samples produced from an optimizer Opt(∙) that attempts to maximize the
current learned model, fθ . We will discuss our choice of Opt in the Appendix Section B.
Incorporating design constraints via infeasible points. While prior work [57] simply optimizes
Equation 2 to learn a surrogate, this is not enough when optimizing over accelerators, as we will
also show empirically (Appendix A.1). This is because explicit negative mining does not provide
any information about accelerator design constraints. Fortunately, this information is provided by
infeasible points, Dinfeasible. The training procedure in Equation 2 provides a simple way to do
incorporate such infeasible points: We simply incorporate Xi 〜DinfeaSibIe as additional negative
samples and maximize the prediction at these points. This gives rise to our final objective:
min Linf(θ) := L(θ) — βEχgDinfeasible [fθ (W)]	⑶
θ
Multi-model optimization and zero-shot generalization. One of the central benefits of a data-
driven approach is that it enables learning powerful surrogates that generalize over the space of
applications, potentially being effective for new unseen application domains. In our experiments, we
evaluate Prime on designing accelerators for multiple applications denoted as k = 1, ∙∙∙ ,K, jointly
or for a novel unseen application. In this case, we utilized a dataset D = {Dι,…，DK}, where
each Dk consists of a set of accelerator designs, annotated with the latency value and the feasibility
criterion for a given application k. While there are a few overlapping designs in different parts of
the dataset annotated for different applications, most of the designs only appear in one part. To train
a single conservative surrogate fθ(X) for multiple applications, we extend the training procedure
in Equation 3 to incorporate context vectors ck ∈ Rd for various applications driven by a list of
application properties in Table 2. The learned function in this setting is now conditioned on the
context fθ(X, ck). We train fθ via the objective in Equation 3, but in expectation over all the contexts
5
Published as a conference paper at ICLR 2022
and their corresponding datasets: minθ Ek〜的 [L严(θ)]. Once such a contextual surrogate is learned,
We can either optimize the average surrogate across a set of contexts {ci, c2,…，Cn} to obtain an
accelerator that is optimal for multiple applications simultaneously on an average (“multi-model”
optimization), or optimize this contextual surrogate for a novel context vector, corresponding to
an unseen application (“zero-shot” generalization). In this case, Prime is not alloWed to train on
any data corresponding to this neW unseen application. While such zero-shot generalization might
appear surprising at first, note that the context vectors are not simply one-hot vectors, but consist of
parameters With semantic information, Which the surrogate can generalize over.
Learned conservative surrogate optimization. Prior Work [64] has shoWn that the most effective
optimizers for accelerator design are meta-heuristic/evolutionary optimizers. We therefore choose to
utilize, firefly [62, 63, 39] to optimize our conservative surrogate. This algorithm maintains a set of
optimization candidates (a.k.a. “fireflies”) and jointly update them toWards regions of loW objective
value, While adjusting their relative distances appropriately to ensure multiple high-performing, but
diverse solutions. We discuss additional details in Appendix B.1.
Cross validation: which model and checkpoint should we evaluate? Similarly to supervised
learning, models trained via Equation 3 can overfit, leading to poor solutions. Thus, We require a
procedure to select Which hyperparameters and checkpoints should actually be used for the design.
This is crucial, because We cannot arbitrarily evaluate as many models as We Want against the simulator.
While effective methods for model selection have been hard to develop in offline optimization [57, 58],
We devised a simple scheme using a validation set for choosing the values of α and β (Equation 3),
as Well as Which checkpoint to utilize for generating the design. For each training run, We hold out
the best 20% of the points out of the training set and use them only for cross-validation as folloWs.
Typical cross-validation strategies in supervised learning involve tracking validation error (or risk),
but since our model is trained conservatively, its predictions may not match the ground truth, making
such validation risk values unsuitable for our use case. Instead, We track Kendall’s ranking correlation
betWeen the predictions of the learned model fθ(xi) and the ground truth values yi (Appendix B)
for the held-out points for each run. We pick values of α, β and the checkpoint that attain the
highest validation ranking correlation. We present the pseudo-code for Prime (Algorithm 1) and
implementation details in Appendix B.1.
5	Related Work
Optimizing hardWare accelerators has become more important recently. Prior Works [45, 28, 56,
41, 8, 34, 4, 3, 25, 60, 61] mainly rely on expensive-to-query hardWare simulators to navigate
the search space and/or target single-application accelerators. For example, HyperMapper [41]
targets compiler optimization for FPGAs by continuously interacting With the simulator in a design
space With relatively feW infeasible points. Mind Mappings [25], optimizes softWare mappings to
a fixed hardWare provided access to millions of feasible points and throWs aWay infeasible points
during learning. MAGNet [60] uses a combination of pruning heuristics and online Bayesian
optimization to generate accelerators for image classification models in a single-application setting.
AutoDNNChip [61] uses tWo-level online optimization to generate customized accelerators for ASIC
and FPAG platforms. In contrast, Prime , does not only learn a surrogate using offline data but can
also leverage information from infeasible points and can Work With just a feW thousand feasible points.
In addition, We devise a contextual version of Prime that is effective in designing accelerators that
are jointly optimized for multiple applications, different from prior Work. Finally, to our knoWledge,
our Work, is the first to demonstrate generalization to unseen applications for accelerator design,
outperforming state-of-the-art online methods.
A popular approach for solving black-box optimization problems is model-based optimization
(MBO) [55, 51, 54]. Most of these methods fail to scale to high-dimensions, and have been extended
With neural netWorks [55, 54, 31, 16, 15, 2, 1, 40]. While these methods Work Well in the active
setting, they are susceptible to out-of-distribution inputs [58] in the offline, data-driven setting. To
prevent this, offline MBO methods that constrain the optimizer to the manifold of valid, in-distribution
inputs have been developed Brookes et al. [5], Fannjiang & Listgarten [12], Kumar & Levine [35].
HoWever, modeling the manifold of valid inputs can be challenging for accelerators. Prime dispenses
With the need for generative modeling, While still avoiding out-of-distribution inputs. Prime builds
on “conservative” offline RL and offline MBO methods that train robust surrogates [36, 57]. HoWever,
unlike these approaches, Prime can handle constraints by learning from infeasible data and utilizes a
better optimizer (See Appendix Table 7 for a comparison). In addition, While prior Works area mostly
6
Published as a conference paper at ICLR 2022
Table 3: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the task
of learning accelerators specialized to a given application. Lower latency is better. From left to right: our
method, online Bayesian optimization (“Bayes Opt”), online evolutionary algorithm (“Evolutionary”), and the
best design in the training dataset. On average (last row), PRIME improves over the best in the dataset by 2.46×
(up to 6.69× in t-RNN Dec) and outperforms best online optimization methods by 1.54× (up to 6.62× in t-RNN
Enc). The best accelerator configurations identified is highlighted in bold.
Application	Prime	Online Optimization			D (Best in Training)
		Bayes Opt	I Evolutionary ∣	MBO	
MobileNetEdgeTPU	298.50	319.00	320.28	332.97	354.13
MobileNetV2	207.43	240.56	238.58	244.98	410.83
MobileNetV3	454.30	534.15	501.27	535.34	938.41
M4	370.45	396.36	383.58	405.60	779.98
M5	208.21	201.59	198.86	219.53	449.38
M6	131.46	121.83	120.49	119.56	369.85
U-Net	740.27	872.23	791.64	888.16	1333.18
t-RNN Dec	132.88	771.11	770.93	771.70	890.22
t-RNN Enc	130.67	865.07	865.07	866.28	584.70
Geomean of Prime's Improvement	1.0×-	1.58×	1.54×	1.61×	2.46×
restricted to a single application, we show that Prime is effective in multi-task optimization and
zero-shot generalization.
6	Experimental Evaluation
Our evaluations aim to answer the following questions: Q(1) Can PRIME design accelerators tailored
for a given application that are better than the best observed configuration in the training dataset, and
comparable to or better than state-of-the-art simulation-driven methods under a given simulator-query
budget? Q(2) Does PRIME reduce the total simulation time compared to other methods? Q(3)
Can PRIME produce hardware accelerators for a family of different applications? Q(4) Can PRIME
trained for a family of applications extrapolate to designing a high-performing accelerator for a new,
unseen application, thereby enabling data reuse? Additionally, we ablate various properties of Prime
(Appendix A.6) and evaluate its efficacy in designing accelerators with distinct dataflow architectures,
with a larger search space (up to 2.5×10114 possible candidates).
Baselines and comparisons. We compare PRIME against three
online optimization methods that actively query the simulator:
(1) evolutionary search with the firefly optimizer [64] (“Evolu-
tionary”), which is the shown to outperform other online meth-
ods for accelerator design; (2) Bayesian Optimization (“Bayes
Opt”) [18], (3) MBO [1]. In all the experiments, we grant all
the methods the same number of feasible points. Note that
our method do not get to select these points, and use the same
exact offline points across all the runs, while the online methods
can actively select which points to query, and therefore require
new queries for every run. “D(Best in Training)” denotes the
best latency value in the training dataset used in Prime. We
also present ablation results with different components of our
method removed in Appendix A.6, where we observe that utiliz-
ing both infeasible points and negative sampling are generally
important for attaining good results. Appendix A.1 presents
additional comparisons to COMs Trabucco et al. [57]—which
Optimization Iterations
Figure 5: Comparing the total simu-
lation time of Prime (for Prime this
is the total time for a forward-pass
through the trained surrogate on a CPU)
and evolutionary method on MobileNet-
EdgeTPU. Prime only requires about
7% of the total simulation time of the
online method.
only obtains negative samples via gradient ascent on the learned surrogate and does not utilize
infeasible points—and P3BO Angermueller et al. [2]—an state-of-the-art online method in biology.
Architecting application-specific accelerators. We first evaluate PRIME in designing specialized
accelerators for each of the applications in Table 2. We train a conservative surrogate using the
method in Section 4 on the logged dataset for each application separately. The area constraint α
(Equation 1) is set to α = 29 mm2 , a realistic budget for accelerators [64]. Table 3 summarizes
the results. On average, the best accelerators designed by Prime outperforms the best accelerator
configuration in the training dataset (last row Table 3), by 2.46×. PRIME also outperforms the
accelerators in the best online method by 1.54× (up to 5.80× and 6.62× in t-RNN Dec and t-RNN
7
Published as a conference paper at ICLR 2022
Table 4: Optimized average latency (the lower, the better) across multiple applications (up to ten applications)
from diverse domains by Prime and best online algorithms (Evolutionary and MBO) under different area
constraints. Each row show the (Best, Median) of average latency across five runs. The geometric mean of
Prime’s improvement over other methods (last row) indicates that Prime is at least 21% better.
Applications	∣ Area ∣ Prime (Ours) ∣ Evolutionary (Online) ∣ MBO (Online)
MobileNet (EdgeTPU, V2, V3)	29 mm2	(310.21, 334.70)	(315.72, 325.69)	(342.02, 351.92)
MobileNet (V2, V3), M5, M6	29 mm2	(268.47, 271.25)	(288.67, 288.68)	(295.21, 307.09)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6	29 mm2	(311.39,313.76)	(314.31,316.65)	(321.48, 339.27)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6, U-Net, t-RNN-Enc	29 mm2	(305.47,310.09)	(404.06, 404.59)	(404.06,412.90)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN-Enc	100 mm2	(286.45, 287.98)	(404.25, 404.59)	(404.06, 404.94)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN (Dec, Enc)	29 mm2-	(426.65, 426.65)	(586.55, 586.55)	(626.62, 692.61)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6, U-Net, t-RNN (Dec, Enc)	100 mm2	(383.57,385.56)	(518.58,519.37)	(526.37, 530.99)
Geomean of Prime’s Improvement	一	(1.0×, 1.0×)	(1.21×, 1.20×)~I	I^^(1.24×, 1.27×)
Enc, respectively). Moreover, perhaps surprisingly, Prime generates accelerators that are better than
all the online optimization methods in 7/9 domains, and performs on par in several other scenarios
(on average only 6.8% slowdown compared to the best accelerator with online methods in M5 and
M 6). These results indicates that offline optimization of accelerators using Prime can be more
data-efficient compared to online methods with active simulation queries. To answer Q(2), we
compare the total simulation time of Prime and the best evolutionary approach from Table 3 on the
MobileNetEdgeTPU domain. On average, not only that Prime outperforms the best online method
that we evaluate, but also considerably reduces the total simulation time by 93%, as shown in Figure 5.
Even the total simulation time to the first occurrence of the final design that is eventually returned
by the online methods is about 11× what PRIME requires to fine a better design. This indicates
that data-driven Prime is much more preferred in terms of the performance-time trade-off. The
fact that our offline approach Prime outperforms the online evolutionary method (and also other
state-of-the-art online MBO methods; see Table 8) is surprising, and we suspect this is because online
methods get stuck early-on during optimization, while utilizing offline data allows us Prime to find
better solutions via generalization (see Appendix B.1.1).
Architecting accelerators for multiple applications. To an-
swer Q(3), we evaluate the efficacy of the contextual version
of Prime in designing an accelerator that attains the lowest la-
tency averaged over a set of application domains. As discussed
previously, the training data used does not label a given accel-
erator with latency values corresponding to each application,
and thus, Prime must extrapolate accurately to estimate the
latency of an accelerator for a context it is not paired with in
the training dataset. This also means that Prime cannot simply
return the accelerator with the best average latency and must
run non-trivial optimization. We evaluate our method in seven
different scenarios (Table 4), comprising various combinations
of models from Table 2 and under different area constraints,
where the smallest set consists of the three MobileNet variants
and the largest set consists of nine models from image clas-
sification, object detection, image segmentation, and speech
Optimization Iterations
Figure 6: Comparing the total Simu-
lation time needed by Prime and on-
line methodS on Seven modelS (Area
≤ 100mm2 ) . PRIME only requireS
about 1%, 6%, and 0.9% of the to-
tal Simulation time of Evolutionary,
MBO, and BayeS Opt, reSpectively, al-
though Prime outperformS the beSt on-
line method by 41%.
recognition. ThiS Scenario iS alSo eSpecially challenging for online methodS Since the number of
jointly feaSible deSignS iS expected to drop Significantly aS more applicationS are added. For inStance,
for the caSe of the MobileNet variantS, the training dataSet only conSiStS of a few (20-30) accelerator
configurationS that are jointly feaSible and high-performing (Appendix B.2—Figure 9).
Table 4 ShowS that, on average, Prime findS acceleratorS that outperform the beSt online method
by 1.2× (up to 41%). While PRIME performS Similar to online methodS in the SmalleSt three-model
Scenario (firSt row), it outperformS online methodS aS the number of applicationS increaSeS and
the Set of applicationS become more diverSe. In addition, comparing with the beSt jointly feaSible
deSign point acroSS the target applicationS, PRIME findS Significantly better acceleratorS (3.95×).
Finally, aS the number of model increaSeS the total Simulation time difference between online methodS
and Prime further widenS (Figure 6). TheSe reSultS indicate that Prime iS effective in deSigning
acceleratorS jointly optimized acroSS multiple applicationS while reuSing the Same dataSet aS for
the Single-taSk, and ScaleS more favorably than itS Simulation-driven counterpartS. Appendix A.4
expoundS the detailS of the deSigned acceleratorS for nine applicationS, comparing our method and
the beSt online method.
8
Published as a conference paper at ICLR 2022
Table 5: Optimized objective values (i.e., latency in milliseconds) under zero-shot setting. Lower latency is
better. From left to right: the applications used to train the surrogate model in Prime the target applications for
which the accelerator is optimized for, the area constraint of the accelerator, Prime’s (best, median) latency, and
best online method’s (best, median) latency. Prime does not use any additional data from the target applications.
On average (last row), Prime yields optimized accelerator for target applications (with zero query to the target
applications’ dataset) with 1.26× (up to 1.66×) lower latency over the best online method. The best accelerator
configurations identified is highlighted in bold.
Train Applications	∣ Test Applications ∣ Area ∣ Prime (Ours) ∣ Evolutionary (Online)
MobileNet (EdgeTPU, V3)	MobileNetV2	29 mm2	(311.39, 313.76)	(314.31, 316.65)
MobileNet (V2, V3), M5, M6	MobileNetEdge, M4	29 mm2-	(357.05, 364.92)	(354.59, 357.29)
MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN Enc	U-Net, t-RNN Dec	29 mm2-	(745.87, 745.91)	(1075.91, 1127.64)
MobileNet (EdgeTPU, V2, V3),M4, M5, M6, t-RNN Enc	U-Net, t-RNN DeC-	100mm2	(517.76, 517.89)	(859.76, 861.69)
Geomean of Prime's Improvement	—	—	(1.0×, 1.0×)		(1.24×, 1.26×)
Table 6: Optimized objective values (i.e. total number of cycles) for two different dataflow architectures,
NVDLA-style [42] and ShiDianNao-style [10], across three classes of applications. The maximum search space
for the studied accelerators are ≈ 2.5×10114. PRIME generalizes to other classes of accelerators with larger
search space and outperforms the best online method by 1.06× and the best data seen in training by 3.75× (last
column). The best accelerator configurations is highlighted in bold.
Applications	∣ Dataflow ∣ PRIME	∣ Evolutionary (Online) ∣ D (Best in Training)
MobileNetV2	NVDLA	2.51 ×107	2.70×107	1.32×108
MobileNetV2	ShiDianNao	2.65 ×107	2.84 ×107	1.27 ×108
ResNet50	NVDLA	2.83 ×108	3.13×108	1.63 ×109
ResNet50	ShiDianNao	3.44 ×108	3.74×108	2.05 ×109
Transformer	NVDLA	7.8×108-	7.8×108	1.3×109
Transformer	ShiDianNao	7.8×108-	7.8×108	1.5×109
Geomean of PRIME's Improvement	—	1.0×	1.06×	3.75×
Accelerating previously unseen applications (“zero-shot” optimization). Finally, we answer
Q(4) by demonstrating that our data-driven offline method, PRIME enables effective data reuse by
using logged accelerator data from a set of applications to design an accelerator for an unseen new
application, without requiring any training on data from the new unseen application(s). We train a
contextual version of Prime using a set of “training applications” and then optimize an accelerator
using the learned surrogate with different contexts corresponding to “test applications,” without any
additional query to the test application dataset. Table 5 shows, on average, Prime outperforms
the best online method by 1.26× (up to 66%) and only 2% slowdown in 1/4 cases. Note that the
difference in performance increases as the number of training applications increases. These results
show the effectiveness of Prime in the zero-shot setting (more results in Appendix A.5).
Applying Prime on other accelerator architectures and dataflows. Finally, to assess the the
generalizability of Prime to other accelerator architectures Kao et al. [30], we evaluate Prime to
optimize latency of two style of dataflow accelerators—NVDLA-style and ShiDianNao-style—across
three applications (Appendix C details the methodology). As shown in Table 6, Prime outperforms
the online evolutionary method by 6% and improves over the best point in the training dataset by
3.75×. This demonstrates the efficacy of PRIME with different dataflows and large design spaces.
7	Discussion
In this work, we present a data-driven offline optimization method, Prime to automatically architect
hardware accelerators. Our method learns a conservative surrogate of the objective function by
leveraging infeasible data points to better model the desired objective function of the accelerator
using a one-time collected dataset of accelerators, thereby alleviating the need for time-consuming
simulation. Our results show that, on average, our method outperforms the best designs observed
in the logged data by 2.46× and improves over the best simulator-driven approach by about 1.54×.
In the more challenging setting of designing accelerators jointly optimal for multiple applications
or for new, unseen applications, zero-shot, PRIME outperforms simulator-driven methods by 1.2×,
while reducing the total simulation time by 99%. The efficacy of Prime highlights the potential for
utilizing the logged offline data in an accelerator design pipeline. While Prime outperforms the
online methods we utilize, in principle, a strong online method can be devised by running Prime in
the inner loop. Our goal is to not advocate that offline methods must replace online methods, but that
training a strong offline optimization algorithm on offline datasets of low-performing designs can be
a highly effective ingredient in hardware accelerator design.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank the “Learn to Design Accelerators” team at Google Research and the Google EdgeTPU
team for their invaluable feedback and suggestions. In addition, we extend our gratitude to the Vizier
team, Christof Angermueller, Sheng-Chun Kao, Samira Khan, Stella Aslibekyan, and Xinyang Geng
for their help with experiment setups and insightful comments.
References
[1]	Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and
Lucy Colwell. Model-based Reinforcement Learning for Biological Sequence Design. In ICLR,
2019.
[2]	Christof Angermueller, David Belanger, Andreea Gane, Zelda Mariet, David Dohan, Kevin
Murphy, Lucy Colwell, andD Sculley. Population-Based Black-Box Optimization for Biological
Sequence Design. In ICML, 2020.
[3]	Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-Kelley, Jeffrey Bosboom,
Una-May O’Reilly, and Saman Amarasinghe. OpenTuner: An Extensible Framework for
Program Autotuning. In PACT, 2014.
[4]	Prasanna Balaprakash, Ananta Tiwari, Stefan M Wild, Laura Carrington, and Paul D Hovland.
AutoMOMML: Automatic Multi-Objective Modeling with Machine Learning. In HiPC, 2016.
[5]	David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by Adaptive Sampling
for Robust Design. In ICML, 2019.
[6]	Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are
Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020.
[7]	Prasanth Chatarasi, Hyoukjun Kwon, Natesh Raina, Saurabh Malik, Vaisakh Haridas, Tushar
Krishna, and Vivek Sarkar. MARVEL: A Decoupled Model-driven Approach for Efficiently
Mapping Convolutions on Spatial DNN Accelerators. arXiv preprint arXiv:2002.07752, 2020.
[8]	Jason Cong, Peng Wei, Cody Hao Yu, and Peng Zhang. Automated Accelerator Generation and
Optimization with Composable, Parallel and Pipeline Architecture. In DAC, 2018.
[9]	Shail Dave, Youngbin Kim, Sasikanth Avancha, Kyoungwoo Lee, and Aviral Shrivastava.
DMazeRunner: Executing Perfectly Nested Loops on Dataflow Accelerators. TECS, 2019.
[10]	Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng,
Yunji Chen, and Olivier Temam. ShiDianNao: Shifting Vision Processing Closer to the Sensor.
In ISCA, 2015.
[11]	Hadi Esmaeilzadeh, Emily Blem, Renee St Amant, Karthikeyan Sankaralingam, and Doug
Burger. Dark Silicon and the End of Multicore Scaling. In ISCA, 2011.
[12]	Clara Fannjiang and Jennifer Listgarten. Autofocused Oracles for Model-Based Design. arXiv
preprint arXiv:2006.08052, 2020.
[13]	Michael J Flynn and Wayne Luk. Computer System Design: System-on-Chip. John Wiley &
Sons, 2011.
[14]	Justin Fu and Sergey Levine. Offline Model-Based Optimization via Normalized Maximum
Likelihood Estimation. In ICLR, 2021.
[15]	Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional Neural Processes.
In ICML, 2018.
[16]	Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali
Eslami, and Yee Whye Teh. Neural Processes. arXiv preprint arXiv:1807.01622, 2018.
10
Published as a conference paper at ICLR 2022
[17]	Michael A Gelbart, Jasper Snoek, and Ryan P Adams. Bayesian Optimization with Unknown
Constraints. arXiv preprint arXiv:1403.5607, 2014.
[18]	Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and
D Sculley. Google Vizier: A Service for Black-box Optimization. In SIGKDD, 2017.
[19]	Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adver-
sarial Examples. ICLR, 2015.
[20]	Robert J Gove, Keith Balmer, Nicholas K Ing-Simmons, and Karl M Guttag. Multi-Processor
Reconfigurable in Single Instruction Multiple Data (SIMD) and Multiple Instruction Multiple
Data (MIMD) Modes and Method of Operation, 1993. US Patent 5,212,777.
[21]	GraphCore. GraphCore. https://www.graphcore.ai/, 2021. Accessed: 2021-05-16.
[22]	Alex Graves. Sequence Transduction with Recurrent Neural Networks. arXiv preprint
arXiv:1211.3711, 2012.
[23]	Suyog Gupta and Berkin Akin. Accelerator-Aware Neural Network Design using AutoML.
arXiv preprint arXiv:2003.02838, 2020.
[24]	Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao,
David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al. Streaming End-to-End
Speech Recognition for Mobile Devices. In ICASSP, 2019.
[25]	Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman Parashar, and Christo-
pher W Fletcher. Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space
Search. In ASPLOS, 2021.
[26]	Andrew Howard and Suyog Gupta. Introducing the Next Generation of On-Device Vision Mod-
els: MobileNetV3 and MobileNetEdgeTPU. https://ai.googleblog.com/2019/
11/introducing-next-generation-on-device.html, 2020.
[27]	Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for MobileNetV3.
In CVPR, 2019.
[28]	Md Shahriar Iqbal, Jianhai Su, Lars Kotthoff, and Pooyan Jamshidi. FlexiBO: Cost-Aware
Multi-Objective Optimization of Deep Neural Networks. arXiv preprint arXiv:2001.06588,
2020.
[29]	Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder
Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-Datacenter Performance
Analysis of a Tensor Processing Unit. In ISCA, 2017.
[30]	Sheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. ConfuciuX: Autonomous Hardware
Resource Assignment for DNN Accelerators using Reinforcement Learning. In MICRO, 2020.
[31]	Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum,
Oriol Vinyals, and Yee Whye Teh. Attentive Neural Processes. In ICLR, 2019.
[32]	Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR,
2015.
[33]	Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
[34]	David Koeplinger, Matthew Feldman, Raghu Prabhakar, Yaqi Zhang, Stefan Hadjis, Ruben
Fiszel, Tian Zhao, Luigi Nardi, Ardavan Pedram, Christos Kozyrakis, et al. Spatial: A Language
and Compiler for Application Accelerators. In PLDI, 2018.
[35]	Aviral Kumar and Sergey Levine. Model Inversion Networks for Model-Based Optimization.
NeurIPS, 2020.
11
Published as a conference paper at ICLR 2022
[36]	Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-Learning for
Offline Reinforcement Learning. NeurIPS, 2020.
[37]	Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman Parashar, Vivek Sarkar,
and Tushar Krishna. Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow:
A Data-Centric Approach. In MICRO, 2019.
[38]	Bo Li, Anmol Gulati, Jiahui Yu, Tara N Sainath, Chung-Cheng Chiu, Arun Narayanan, Shuo-
Yiin Chang, Ruoming Pang, Yanzhang He, James Qin, et al. A Better and Faster end-to-end
Model for Streaming ASR. In ICASSP, 2021.
[39]	Changnian Liu, Yafei Tian, Qiang Zhang, Jie Yuan, and Binbin Xue. Adaptive Firefly Optimiza-
tion Algorithm Based on Stochastic Inertia Weight. In ISCID, 2013.
[40]	Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen
Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Azade Nazi, et al. A Graph Placement
Methodology for Fast Chip Design. Nature, 2021.
[41]	Luigi Nardi, David Koeplinger, and Kunle Olukotun. Practical Design Space Exploration. In
MASCOTS, 2019.
[42]	Nvidia. NVDLA deep learning accelerator. http://nvdla.org, 2021. Accessed: 2021-
10-01.
[43]	Nvidia. Nvidia. https://www.nvidia.com/en-us/, 2021. Accessed: 2021-05-16.
[44]	Angshuman Parashar, Priyanka Raina, Yakun Sophia Shao, Yu-Hsin Chen, Victor A Ying,
Anurag Mukkara, Rangharajan Venkatesan, Brucek Khailany, Stephen W Keckler, and Joel
Emer. Timeloop: A Systematic Approach to DNN Accelerator Evaluation. In ISPASS. IEEE,
2019.
[45]	Maryam Parsa, John P Mitchell, Catherine D Schuman, Robert M Patton, Thomas E Potok, and
Kaushik Roy. Bayesian Multi-objective Hyperparameter Optimization for Accurate, Fast, and
Efficient Neural Network Accelerator Design. Frontiers in Neuroscience, 14:667, 2020.
[46]	Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing Posterior Collapse
with Delta-VAEs. In ICLR, 2018.
[47]	Brandon Reagen,Jos6 Miguel Herndndez-Lobato, Robert Adolf, Michael Gelbart, Paul What-
mough, Gu-Yeon Wei, and David Brooks. A Case for Efficient Accelerator Design Space
Exploration via Bayesian Optimization. In ISLPED, 2017.
[48]	Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for
Biomedical Image Segmentation. In MICCAI. Springer, 2015.
[49]	Tara N Sainath, Yanzhang He, Bo Li, Arun Narayanan, Ruoming Pang, Antoine Bruguier, Shuo-
yiin Chang, Wei Li, Raziel Alvarez, Zhifeng Chen, et al. A Streaming On-Device End-to-End
Model Surpassing Server-Side Conventional Model Quality and Latency. In ICASSP, 2020.
[50]	Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
MobileNetV2: Inverted Residuals and Linear Bottlenecks. In CVPR, 2018.
[51]	Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the
Human Out of the Loop: A Review of Bayesian Optimization. Proceedings of the IEEE, 2016.
[52]	Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer. arXiv preprint arXiv:1701.06538, 2017.
[53]	Zhan Shi, Chirag Sakhuja, Milad Hashemi, Kevin Swersky, and Calvin Lin. Learned Hard-
ware/Software Co-Design of Neural Accelerators. arXiv preprint arXiv:2010.02075, 2020.
[54]	Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian Optimization of
Machine Learning Algorithms. In NIPS, 2012.
12
Published as a conference paper at ICLR 2022
[55]	Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian Optimization Using Deep
Neural Networks. In ICML, 2015.
[56]	Yanan Sun, Bing Xue, Mengjie Zhang, Gary G Yen, and Jiancheng Lv. Automatically Designing
CNN Architectures Using the Genetic Algorithm for Image Classification. IEEE Transactions
on Cybernetics, 2020.
[57]	Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative Objective
Models for Effective Offline Model-Based Optimization. In ICML, 2021.
[58]	Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Design-Bench:
Benchmarks for Data-Driven Offline Model-Based Optimization, 2021. URL https:
//openreview.net/forum?id=cQzf26aA3vM.
[59]	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. arXiv preprint
arXiv:1706.03762, 2017.
[60]	Rangharajan Venkatesan, Yakun Sophia Shao, Miaorong Wang, Jason Clemons, Steve Dai,
Matthew Fojtik, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al.
MAGNet: A Modular Accelerator Generator for Neural Networks. In ICCAD, 2019.
[61]	Pengfei Xu, Xiaofan Zhang, Cong Hao, Yang Zhao, Yongan Zhang, Yue Wang, Chaojian Li,
Zetong Guan, Deming Chen, and Yingyan Lin. AutoDNNchip: An Automated DNN Chip
Predictor and Builder for both FPGAs and ASICs. In FPGA, 2020.
[62]	Xin-She Yang. Nature-Inspired Metaheuristic Algorithms. Luniver press, 2010.
[63]	Xin-She Yang and SUash Deb. Eagle Strategy Using LeVy Walk and Firefly Algorithms for
Stochastic Optimization. In NICSO. Springer, 2010.
[64]	Amir Yazdanbakhsh, Christof AngermUeller, Berkin Akin, Yanqi ZhoU, Albin Jones, Milad
Hashemi, KeVin Swersky, Satrajit Chatterjee, RaVi Narayanaswami, and James LaUdon. Apollo:
Transferable ArchitectUre Exploration. arXiv preprint arXiv:2102.01723, 2021.
[65]	Amir Yazdanbakhsh, Kiran Seshadri, Berkin Akin, James LaUdon, and RaVi Narayanaswami.
An EValUation of Edge TPU Accelerators for ConVolUtional NeUral Networks. arXiv preprint
arXiv:2102.10423, 2021.
13
Published as a conference paper at ICLR 2022
Appendices
A	Additional Experiments
In this section, we present additional experiments compared to the method of Trabucco et al. [57]),
present some additional results obtained by jointly optimizing multiple applications (Appendix A.2),
provide an analysis of the designed accelerators (Appendix A.4) and finally, discuss how our trained
conservative surrogate can be used with a different evaluation time constraint (Appendix A.3).
A.1 Comparison to Other Baseline Methods
Comparison to COMs. In this section, we perform a comparative evaluation of PRIME to the
COMs method Trabucco et al. [57]. Like several offline reinforcement learning algorithms [36], our
method, Prime and COMs are based on the key idea of learning a conservative surrogate of the
desired objective function, such that it does not overestimate the value of unseen data points, which
prevents the optimizer from finding accelerators that appear promising under the learned model but
are not actually promising under the actual objective. The key differences between our method and
COMS are: (1) Prime USeS an evolutionary optimizer (Opt(∙)) for negative sampling compared to
gradient ascent of COMs, which can be vastly beneficial in discrete design spaces as our results show
empirically, (2) PRIME can explicitly learn from infeaSible data pointS provided to the algorithm,
while COMS doeS not have a mechaniSm to incorporate the infeaSible pointS into the learning of
Surrogate. To further aSSeSS the importance of theSe differenceS in practice, we run COMS on three
taSkS from Table 3, and preSent a compariSon our method, COMS, and Standard method in Table 7.
The “Standard” method repreSentS a Surrogate model without utilizing any infeaSible pointS. On
average, PRIME outperformS COMS by 1.17× (up to 1.24× in M6).
Table 7: Optimized objective valueS (i.e., latency in milliSecondS) obtained by PRIME and COMS [57] when
optimizing over Single applicationS (MobilenetV2, MobilenetV3 and M6), extending Table 3. Note that Prime
outperformS COMS. However, COMS improveS over baSeline “Standard” method (laSt column).
Application	∣∣ Prime (OUrS)Il COMS ∣∣ Standard
MObileNetV2	207.43	251.58	374.52
MObileNetV3	454.30	485.66-	575.75
""M6	131.46	163.94-	180.24
Geomean of Prime's Improvement	1.0×	1.17×	1.46×~
Comparison to generative offline MBO methods. We provide a compariSon between PRIME
and prior offline MBO methodS baSed on generative modelS [35]. We evaluate model inverSion
networkS (MINS) [35] on our accelerator data. However, we were unable to train a diScrete
objective-conditioned GAN model to 0.5 diScriminator accuracy on
our offline dataSet, and often obServed a collapSe of the diScriminator.
AS a reSult, we trained a δ-VAE [46], conditioned on the objective
function (i.e., latency). A Standard VAE [33] Suffered from poSterior
collapSe and thuS informed our choice of utilizing a δ-VAE. The la-
tent Space of a trained objective-conditioned δ-VAE correSponding
to acceleratorS on a held-out validation dataSet (not uSed for training)
iS viSualized in the t-SNE plot in the figure on the right. ThiS iS a
2D t-SNE of the acceleratorS configurationS (§Table 1). The color
of a point denoteS the latency value of the correSponding acceler-
ator configuration, partitioned into three binS. ObServe that while
we would expect theSe objective conditioned modelS to diSentangle
acceleratorS with different objective valueS in the latent Space, the
modelS we trained did not exhibit Such a Structure, which will hamper optimization. While our
method Prime could alSo benefit from a generative optimizer (i.e., by uSing a generative optimizer
in place of Opt(∙) with a conservative surrogate), we leave it for future work to design effective
generative optimizerS on the accelerator manifold.
14
Published as a conference paper at ICLR 2022
(。① S)① ELL Uo=B-ΓIE∞-5QL
Figure 7: Comparing the total simulation time needed by the P3BO and Evolutionary method on MobileNet-
EdgeTPU. Note that, not only the total simulation time of P3BO is around 3.1× higher than the Evolutionary
method, but also the latency of final optimized accelerator is around 18% for MobileNetEdgeTPU. The total
simulation time of our method is around 7% of the Evolutionary method (See Figure 5).
Comparison to P3BO. We perform a comparison against P3BO method, the state-of-the-arts online
method in biology [2]. On average, PRIME outperforms the P3BO method by 2.5× (up to 8.7× in
U-Net). In addition, we present the comparison between the total simulation runtime of the P3BO
and Evolutionary methods in Figure 7. Note that, not only the total simulation time of P3BO is
around 3.1× higher than the Evolutionary method, but also the latency of final optimized accelerator
is around 18% for MobileNetEdgeTPU. On the other hand, the total simulation time of Prime for
the task of accelerator design for MobileNetEdgeTPU is lower than both methods (only 7% of the
Evolutionary method as shown in Figure 5).
Table 8: Optimized objective values (i.e., latency in milliseconds) obtained by PRIME and P3BO [2] when
optimizing over single applications (MobileNetEdgeTPU, M4, t-RNN Dec, t-RNN Enc, and U-Net). On average,
PRIME outperforms P3BO by 2.5×.
Application	∣∣ Prime (Ours) ∣∣ P3BO
MobileNetEdgeTPU	298.50	376.02
^^4	370.45	一 483.39
U-Net	740.27	一 771.70
t-RNN DeC	132.88	865.12
t-RNN EnC	130.67	1139.48
Geomean of Prime's Improvement	-1.0×	∙~2.5×-
A.2 Learned Surrogate Model Reuse for Accelerator Design
Extending our results in Table 4, we present another variant of optimizing accelerators jointly
for multiple applications. In that scenario, the learned surrogate model is reused to architect an
accelerator for a subset of applications used for training. We train a contextual conservative surrogate
on the variants of MobileNet (Table 2) as discussed in Section 4, but generated optimized designs
by only optimizing the average surrogate on only two variants of MobileNet (MobileNetEdgeTPU
and MobileNetV2). This tests the ability of our approach Prime to provide a general contextual
conservative surrogates, that can be trained only once and optimized multiple times with respect to
different subsets of applications. Observe in Table 9, Prime architects high-performing accelerator
configurations (better than the best point in the dataset by 3.29× - last column) while outperforming
the online optimization methods by 7%.
A.3 Learned Surrogate Model Reuse under Different Design Constraint
We also test the robustness of our approach in handling variable constraints at test-time such as
different chip area budget. We evaluate the learned conservative surrogate trained via Prime under
a reduced value of the area threshold, α, in Equation 1. To do so, we utilize a variant of rejection
sampling - we take the learned model trained for a default area constraint α = 29 mm2 and then reject
15
Published as a conference paper at ICLR 2022
Table 9: Optimized objective values (i.e., latency in milliseconds) obtained by our PRIME when using the
jointly optimized model on three variants of MobileNets and use for MobileNetEdgeTPU and MobileNetV2 for
different dataset configurations. PRIME outperforms the best online method by 7% and finds an accelerator that
is 3.29× better than the best accelerator in the training dataset (last row). The best accelerator configuration is
highlighted in bold.
Prime	Online Optimization
Applications	An ∣∣ -Opt ∣ -Infeasible Standard Bayes Opt ∣ Evolutionary D (Best in Training)
(MobileNetEdgeTPU,MobileNetV2)	253.85	297.36 ∣ 264.85	341.12	∣ 275.21	∣ 271.71	834.68
Table 10: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the task
of learning accelerators specialized to MobileNetEdgeTPU under chip area budget constraint 18 mm2 reusing
the already learned model by our method for MobileNetEdgeTPU (shown in Table 3). Lower latency/runtime
is better. From left to right: our method, our method without negative sampling (“PRIME-Opt”) and without
utilizing infeasible points (“Prime-Infeasible”), standard surrogate (“Standard”), online Bayesian optimization
(“Bayes Opt”), online evolutionary algorithm (“Evolutionary”) and the best design in the training dataset. Note
that Prime improves over the best in the dataset by 12%, outperforms the best online optimization method by
4.4%. The best accelerator configuration is highlighted in bold.
Applications	Prime	Standard	Online Optimization	D (Best in Training)
	An	Il -Opt I -Infeasible		Bayes Opt ∣ Evolutionary	
MobileNetEdgeTPU, Area ≤ 18 mm2	315.15 II 433.81 ∣ 351.22	470.09	331.05	I 329.13	354.13
Table 11: Per application latency for the best accelerator design suggested by PRIME and the Evolutionary
method according to Table 4 for multi-task accelerator design (nine applications and area constraint 100 mm2).
PRIME outperforms the Evolutionary method by 1.35×.
Applications	Latency (ms)		Improvement of Prime over Evolutionary
	Prime ∣ Evolutionary(Online)		
MobileNetEdgeTPU	288.38	319.98	1.10×
MobileNetV2	-216.27	255.95	1.18×
MobileNetV3	-487.46	573.57	1.17×
M4	-400.88	406.28	1.01 ×
M5	248.18	239.18	0.96×
M6	164.98	148.83	0.90×
U-Net	~1268.73	908.86	0.71 ×
t-RNN Dec	~191.83	862.14	5.13×
t-RNN Enc	-185.41	952.44	4.49 ×
Average (Latency in ms)	383.57	518.58	1.35 ×
all optimized accelerator configurations which do not satisfy a reduces area constraint: Area(x) ≤
α0 = 18 mm2. Table 10 summarizes the results for this scenario for the MobileNetEdgeTPU [23]
application under the new area constraint (α = 18 mm2). A method that produces diverse designs
which are both high-performing and are spread across diverse values of the area constraint are
expected to perform better. As shown in Table 10, Prime provides better accelerator than the best
online optimization from scratch with the new constraint value by 4.4%, even when Prime does
not train its conservative surrogate with this unseen test-time design constraint. Note that, when
the design constraint changes, online methods generally need to restart the optimization process
from scratch and undergo costly queries to the simulator. This would impose additional overhead in
terms of total simulation time (§ Figure 5 and Figure 6). However, the results in Table 10 shows that
our learned surrogate model can be reused under different test-time design constraint eliminating
additional queries to the simulator.
A.4 Analysis of Designed Accelerators
In this section, we overview the best accelerator configurations that Prime and the Evolutionary
method identified for multi-task accelerator design (See Table 4), when the number of target applica-
tions are nine and the area constraint is set to 100 mm2. The average latencies of the best accelerators
found by PRIME and the Evolutionary method across nine target applications are 383.57 ms and
518.58 ms, respectively. In this setting, our method outperforms the best online method by 1.35×.
16
Published as a conference paper at ICLR 2022
Table 12: Optimized accelerator configurations (See Table 1) found by PRIME and the Evolutionary method
for multi-task accelerator design (nine applications and area constraint 100 mm2). Last row shows the accel-
erator area in mm2. PRIME reduces the overall chip area usage by 1.97×. The difference in the accelerator
configurations are shaded in gray.
Accelerator Parameter	Parameter Value	
	Prime	Evolutionary (Online)
#ofPEs-X	4	4
#ofPEs-Y	6	8
# of Cores	64-	128
# of Compute Lanes	F	6""
PE Memory	2,097,152	1,048,576
Core Memory	-131,072	131,072
Instruction Memory	32,768	8,192
Parameter Memory	4,096	4,096
Activation Memory	512	2,048
DRAM Bandwidth (Gbps)	30^	30^
Chip Area (mm2)	46.78	92.05
Table 11 shows per application latencies for the accelerator suggested by our method and the Evolu-
tionary method. The last column shows the latency improvement of Prime over the Evolutionary
method. Interestingly, while the latency of the accelerator found by our method for MobileNetEd-
geTPU, MobileNetV2, MobileNetV3, M4, t-RNN Dec, and t-RNN Enc are better, the accelerator
identified by the online method yields lower latency in M5, M6, and U-Net.
To better understand the trade-off in design of each accelerator designed by our method and the
Evolutionary method, we present all the accelerator parameters (See Table 1) in Table 12. The
accelerator parameters that are different between each of the designed accelerator are shaded in
gray (e.g. # of PEs-Y, # of Cores, # of Compute Lanes, PE Memory, Instruction Memory, and
Activation Memory). Last row of Table 12 depicts the overall chip area usage in mm2. Prime not
only outperforms the Evolutionary algorithm in reducing the average latency across the set of target
applications, but also reduces the overall chip area usage by 1.97 X. Studying the identified accelerator
configuration, We observe that Prime trade-offs compute ( 64 cores vs. 128 cores ) for larger PE
memory size ( 2,097,152 vs. 1,048,576 ). These results show that Prime favors PE memory size to
accommodate for the larger memory requirements in t-RNN Dec and t-RNN Enc (See Table 2 Model
Parameters) where large gains lie. Favoring larger on-chip memory comes at the expense of lower
compute power in the accelerator. This reduction in the accelerator’s compute power leads to higher
latency for the models with large number of compute operations, namely M5, M6, and U-Net (See
last row in Table 2). M4 is an interesting case where both compute power and on-chip memory is
favored by the model (6.23 MB model parameters and 3,471,920,128 number of compute operations).
This is the reason that the latency of this model on both accelerators, designed by our method and the
Evolutionary method, are comparable (400.88 ms in Prime vs. 406.28 ms in the online method).
A.5 Comparison with Online Methods in Zero-Shot Setting
We evaluated the Evolutionary (online) method under two protocols for the last two rows of Table 5:
first, we picked the best designs (top-performing 256 designs similar to the Prime setting in Section 4)
found by the evolutionary algorithm on the training set of applications and evaluated them on the target
applications and second, we let the evolutionary algorithm continue simulator-driven optimization
on the target applications. The latter is unfair, in that the online approach is allowed access to
querying more designs in the simulator. Nevertheless, we found that in either configuration, the
evolutionary approach performed worse than Prime which does not access training data from the
target application domain. For the area constraint 29 mm2 and 100 mm2 , the Evolutionary algorithm
reduces the latency from 1127.64 → 820.11 and 861.69 → 552.64, respectively, although still worse
than PRiME. in the second experiment in which we unfairly allow the evolutionary algorithm to
continue optimizing on the target application, the Evolutionary algorithm suggests worse designs
than Table 5 (e.g. 29mm2: 1127.64 → 1181.66 and 100mm2: 861.69 → 861.66).
17
Published as a conference paper at ICLR 2022
Table 13: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the task of
learning accelerators specialized to a given application. Lower latency/runtime is better. From left to right: our
method, our method without negative sampling (“PRIME-Opt”) and without utilizing infeasible points (“PRIME-
Infeasible”), standard surrogate (“Standard”), online Bayesian optimization (“Bayes Opt”), online evolutionary
algorithm (“Evolutionary”) and the best design in the training dataset. Note that, in all the applications Prime
improves over the best in the dataset, outperforms online optimization methods in 7/9 applications and the
complete version of Prime generally performs best. The best accelerator designs are in bold.
Application	Prime			Standard	Online Optimization		D (Best in Training)
	An	-Opt	-Infeasible		Bayes Opt	Evolutionary	
MobileNetEdge	298.50	435.40	322.20	411.12	319.00	320.28	354.13
MobileNetV2-	207.43	281.01	214.71	374.52	240.56	238.58	410.83
MobileNetV3-	454.30	489.45	483.96	575.75	534.15	501.27	938.41
^"M4	370.45	478.32	432.78	-1139.76	396.36	383.58	779.98
M5	208.21	319.61	246.80	307.57	201.59	198.86	449.38
^"M6	131.46	197.70	162.12	180.24	121.83	120.49	369.85
U-Net	740.27	740.27	765.59	763.10	872.23	791.64	1333.18
t-RNN Dec	132.88	172.06	135.47	136.20	771.11	770.93	890.22
t-RNN Enc	130.67	134.84	137.28	150.21	865.07	865.07	584.70
Table 14: The comparison between the accelerator designs suggested by PRIME and EdgeTPU [65, 23] for
single model specialization. On average (last row), with single-model specialization our method reduces the
latency by 2.69× while minimizes the chip area usage by 1.50×.
Application	Latency (milliseconds)			Chip Area (mm2)		
	Prime	EdgeTPU	Improvement	Prime	EdgeTPU	Improvement
MobileNetEdgeTPU	294.34	523.48	1.78×	18.03	27	1.50×
MobileNetV2	208.72	408.24	1.96 ×	-17.11	27^	1.58×
MobileNetV3	459.59	831.80	1.81×	-11.86	27^	2.28 ×
M4	370.45	675.53	1.82×	-19.12	27^	1.41 ×
M5	208.42	377.32	1.81×	-22.84	27^	1.18×
M6	132.98	234.88	1.77 ×	-16.93	27^	1.59×
U-Net	1465.70	-2409.73	1.64 ×	-25.27	27^	1.07×
t-RNN Dec	132.43	-1384.44	10.45 ×	-14.82	27^	1.82×
t-RNN Enc	130.45	-1545.07	11.84×	-19.87	27^	1.36×
Average Improvement	—	—	2.69 ×	—	—	1.50 ×
A.6 Prime Ablation Study
Here we ablate over variants of our method: (1) Opt was not used for negative sampling (“PRIME-
Opt” in Table 13) (2) infeasible points were not used (“PRIME-Infeasible” in Table 13). As shown
in Table 13, the variants of our method generally performs worse compared to the case when both
negative sampling and infeasible data points are utilized in training the surrogate model.
A.7 Comparison with Human-Engineered Accelerators
In this section, we compare the optimized accelerator design found by Prime that is targeted towards
single applications to the manually optimized EdgeTPU design [65, 23]. EdgeTPU accelerators are
primarily optimized towards running applications in image classification, particularly, MobileNetV2,
MobileNetV3 and MobileNetEdgeTPU. The goal of this comparison is to present the potential benefit
of Primefor a dedicated application when compared to human designs. For this comparison, we
utilize an area constraint of 27 mm2 and a DRAM bandwidth of 25 Gbps, to match the specifications
of the EdgeTPU accelerator.
Table 14 shows the summary of results in two sections, namely “Latency” and “Chip Area”. The
first and second under each section show the results for Prime and EdgeTPU, respectively. The final
column for each section shows the improvement of the design suggested by Prime over EdgeTPU.
On average (as shown in the last row), PRIME finds accelerator designs that are 2.69× (up to 11.84×
in t-RNN Enc) better than EdgeTPU in terms of latency. Our method achieves this improvement
while, on average, reducing the chip area usage by 1.50× (up to 2.28× in MobileNetV3). Even on
the MobileNet image-classification domains, we attain an average improvement of 1.85×.
18
Published as a conference paper at ICLR 2022
Table 15: Optimized objective values (i.e., latency in milliseconds) under zero-shot setting when the test
applications include all the nine evaluated models (e.g. MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN
Dec, t-RNN Enc, U-Net). Lower latency is better. From left to right: the applications used to train the surrogate
model in Prime the target applications for which the accelerator is optimized for, the area constraint of the
accelerator, Prime’s (best, median) latency, and best online method’s (best, median) latency. The best accelerator
configurations identified is highlighted in bold.
Train Applications	Area	Prime	Evolutionary (Online)
MobileNet (EdgeTPU,V2,V3), M4, M5, M6,t-RNN EnC	29 mm2	(426.65, 427.94)	(586.55, 586.55)
MobileNet (EdgeTPU,V2,V3), M4, M5, M6,t-RNN EnC	100 mm2	(365.95, 366.64)	(518.58,519.37)
Geomean of PRIME’S Improvement	—	(1.0×, 1.0×)	(1.40×, 1.39×)
A.8 Zero-Shot Results on All Applications
In this section, we present the results of zero-shot optimization from Table 5 on all the nine appli-
cations we study in the paper (i.e., test applications = all nine models: MobileNet (EdgeTPU, V2,
V3), M6, M5, M4, t-RNN (Enc and Dec), and U-Net). We investigate this for two sets of training
applications and two different area budgets. As shown in Table 15, we find that Prime does perform
well compared to the online evolutionary method.
A.9 Different Train and validation Splits
In the main paper, we used the worst 80% of the feasible points in the training dataset for training
and used the remaining 20% of the points for cross-validation using our strategy based on Kendall’s
rank correlation. In this section, we explore some alternative training-validation split strategies to see
how they impact the results. To do so, we consider two alternative strategies: (1) training on 95% of
the worst designs, validation on top 5% of the designs, and (2) training on the top 80% of the designs
and validation on the worst 20% of the designs. We apply these strategies to MobileNetEdgeTPU,
M6 and t-RNN Enc models from Table 3, and present a comparative evaluation in Table 16 below.
Results. As shown in Table 16, we find that cross-validating using the best 5% of the points in
the dataset led to a reduced latency (298.50 → 273.30) on MobileNetEdgeTPU, and retained the
same performance on M6. However, it increased the latency on t-RNN Enc (130.67 → 137.45).
This indicates at the possibility that while top 5% of the datapoints can provide a better signal for
cross-validation in some cases, this might also hurt performance if the size of the 5% dataset becomes
extremely small (as in the case of t-RNN Enc, the total dataset size is much smaller than either
MobileNetEdgeTPU or M6).
The strategy of cross-validating using the worst 20% of the points hurt performance on M6 and
t-RNN Enc, which is perhaps as expected, since the worst 20% of the points may not be indicative
of the best points found during optimization. However, while it improves performance on the
MobileNetEdgeTPU application compared to the split used in the main paper but it is still worse than
using the top 5% of the points for validation.
Table 16: Performance of PRIME (as measured by median latency of the accelerator found across five runs)
under various train-test splits on three applications studied in Table 3.
ApplicationS	Best 5% Validation	Best 20% Validation (Table 3)	Worst 20% Validation
MobileNetEdgeTPU	273.30	298.50	286.53
M6	131.46	131.46	142.68
t-RNN EnC	137.45	130.67	13571
B	Details of Prime
In this section, we provide training details of our method Prime including hyperparameters and
compute requirements and details of different tasks.
B.1	Hyperparameter and Training Details
Algorithm 1 outlines our overall system for accelerator design. Prime parameterizes the function
fθ(x) as a deep neural network as shown in Figure 4. The architecture of fθ (x) first embeds the
19
Published as a conference paper at ICLR 2022
discrete-valued accelerator configuration x into a continuous-valued 640-dimensional embedding via
two layers of a self-attention transformer [59]. Rather than directly converting this 640-dimensional
embedding into a scalar output via a simple feed-forward network, which we found a bit unstable to
train with Equation 3, possibly due to the presence of competing objectives for a comparison), we
pass the 640-dimensional embedding into M different networks that map it to M different scalar
predictions (fθi(x))iM=1. Finally, akin to attention [59] and mixture of experts [52], we train an
additional head to predict weights (wi)iM=1 ≥ 0 of a linear combination of the predictions at different
heads that would be equal to the final prediction: fθ(x) = PiK=1 wifθi (x). Such an architecture
allows the model to use different predictions fθi(x), depending upon the input, which allows for more
stable training. To train fθ(x), we utilize the Adam [32] optimizer. Equation 3 utilizes a procedure
Opt that maximizes the learned function approximately. We utilize the same technique as Section 4
(“optimizing the learned surrogate”) to obtain these negative samples. We periodically refresh Opt,
once in every 20K gradient steps on fθ (x) over training.
Algorithm 1 Training the conservative surrogate in PRIME
1:	Initialize a neural model fθ0 (x) and a set M = 23 of negative particles to be updated by the
firefly optimizer {x-(0),…，x-(0), XM (0)} to random configurations from the design space.
2:	for iteration i = 0, 1, 2, 3, . . . until convergence do
3:	for firefly update step t = 0, 1, . . . , 4	. Inner loop do
4:	Update the M fireflies according to the firefly update rule in Equation 5,
5:	towards maximizing fθi (X) according to:	. Negative mining
6:	Xi(ti + 1) = Xi(ti) +β(Xi(ti) - Xj (ti)) + ηti
7:	end for
8:	Find the best firefly found in these steps to be used as the negative sample:
9:	X- =argmin{fθi(x-(ti)),…，fθi(XM(ti))}	. Find negative sample
10:	Run one gradient step on θi using Equation 3 with Xi- as the negative sample
11:	if i%p == 0, (p = 20000), then:	. Periodically reinitialize the optimizer
12:	Reinitialize firefly particles {x-(0), •一，x-(0), XM(0)} to random designs.
13:	end for
14:	Return the final model fθ* *(x)
The hyperparameters for training the conservative surrogate in Equations 3 and its contextual version
are as follows:
• Architecture of fθ(X). As indicated in Figure 4, our architecture takes in list of categor-
ical (one-hot) values of different accelerator parameters (listed in Table 1), converts each
parameter into 64-dimensional embedding, thus obtaining a 10 × 64 sized matrix for each
accelerator, and then runs two layers of self-attention [59] on it. The resulting 10 × 64
output is flattened to a vector in R640 and fed into M = 7 different prediction networks that
give rise to f1(x), ∙∙∙ , fM(x), and an additional attention 2-layer feed-forward network
(layer sizes = [256, 256]) that determines weights wι,…，wm, such that Wi ≥ 0 and
PiM=1 wi = 1. Finally the output is simply fθ(X) = Pi wifθi(X).
• Optimizer/learning rate for training fθ(X). Adam, 1e - 4, default β1 = 0.9, β2 = 0.999.
• Validation set split. Top 20% high scoring points in the training dataset are used to provide
a validation set for deciding coefficients α, β and the checkpoint to evaluate.
• Ranges of α, β. We trained several fθ(X) models with α ∈ [0.0, 0.01, 0.1, 0.5, 1.0, 5.0]
and β ∈ [0.0, 0.01, 5.0, 0.1, 1.0]. Then we selected the best values of α and β based on the
highest Kendall’s ranking correlation on the validation set. Kendall’s ranking correlation
between two sets of objective values: S = {y1,y2,…，yN} corresponding to ground
truth latency values on the validation set and S0 = {y1, y2,…，y0N} corresponding to the
predicted latency values on the validation set is given by τ equal to:
τ
PiN,j,NI[(yi-yj)(yi0-yj0)>0]-PiN,j,NI[(yi-yj)(yi0-yj0)≤0]
N ∙ (N - 1)
(4)
20
Published as a conference paper at ICLR 2022
Table 17: Dataset sizes for various applications that we study in this paper. Observe that all of the datasets are
smaller than 8000.
Application	∣∣ Datasetsize
MobileNetEdgeTPU	7697
MobileNetV2	7620-
MobileNetV3	5687-
^^4	3763-
^^5	5735-
^^6	7529-
U-Net	557-
t-RNN Dec	121^
I-RNNEnc	1240-
•	Clipping fθ (x) during training. Equation 3 increases the value of the learned function
fθ(x) at X = xo ∈ Dinfeasible and X-〜Opt(fθ). We found that with the small dataset,
these linear objectives can run into numerical instability, and produce +∞ predictions. To
avoid this, we clip the predicted function value both above and below by ±10000.0, where
the valid range of ground-truth values is O(1000).
•	Negative sampling with Opt(∙). As discussed in Section 4, We utilize the firefly optimizer
for both the negative sampling step and the final optimization of the learned conservative
surrogate. When used during negative sampling, we refresh (i.e., reinitialize) the firefly
parameters after every p = 20000 gradient steps of training the conservative surrogate, and
run t = 5 steps of firefly optimization per gradient step taken on the conservative surrogate.
•	Details of firefly: The initial population of fireflies depends on the number of accelerator
configurations (C) following the formula 10 + b(C1.2 + C) × 0.5e. In our setting with ten
accelerator parameters (See Table 1), the initial population of fireflies is 23. We use the
same hyperparameters: γ = 1.0, β0 = 1.0, for the optimizer in all the experiments and
never modify it. The update to a particular optimization particle (i.e., a firefly) Xi, at the t-th
step of optimization is given by:
Xi(t + 1) = Xi(t) + β(Xi(t) - Xj (t)) + i.i.d. Gaussian noise,	(5)
where Xj (t), j 6= i is a different firefly that achieves a better objective value compared to Xi
and the function β is given by: β(r) = β0e-γr2 .
•	Training set details: The training dataset sizes for the studied applications are shown
in Table 17. To recap, to generate the dataset, we first randomly sampled accelerators
from the deign space, and evaluated them for the target application, and constituted the
training set from the worst-performing feasible accelerators for the given application. Since
different applications admit different feasibility criteria (differences in compilation, hardware
realization, and etc.), the dataset sizes for each application are different, as the number of
feasible points is different. Note however that as mentioned in the main text, these datasets
all contain ≤ 8000 feasible points.
Discussion on data quality: In the cases of t-RNN Dec, t-RNN Enc, and U-Net, we find
that the number of feasible points is much smaller compared to other applications, and we
suspect this is because our random sampling procedure does not find enough feasible points.
This is a limitation of our data collection strategy and we intentionally chose this naive
strategy to keep data collection simple. Other techniques for improving data collection and
making sure that the data does not consist of only infeasible points includes strategies such
as utilizing logged data from past runs of online evolutionary methods, mixed with some
data collected via random sampling to improve coverage of the design space.
21
Published as a conference paper at ICLR 2022
Table 18: Comparing the latency of the accelerators designed by the evolutionary approach for variable number
of simulator access budgets (8k and 32k). Even with 4× as much allowed simulator interaction, online methods
are unable to perform that well in our case.
Application	Area	Prime	Evolutionary (Online)	
			8k data points	32k data points
MobileNetEdgeTPU	29 mm2	298.50	320.28	311.35
t-RNN Dec	29 mm2	132.88	770.93	770.63
t-RNN Enc	29 mm2	130.67	865.07	865.07
Geomean of PRIME'S Improvement	—	^^(1.0×, 1.0×)	3.45×	3.42×
B.1.1	Details of Firefly Used for Our Online Evolutionary Method
In this section, we discuss some details for firefly optimization used in the online evolutionary method.
Stopping criterion: We stopped the firefly optimization when the latency of the best design found
did not improve over the previous 1000 iterations, but we also made sure to run firefly optimization
for at least 8000 iterations, to make sure that both the online and offline methods match in terms
of the data budget. We also provide the convergence curves for firefly optimization on various
single-application problems from Table 3 in Figure 8.
What happens if we run firefly optimization for longer? We also experimented with running
the evolutionary methods for longer (i.e., 32k simulator accesses compared to 8k), to check if this
improves the performance of the evolutionary approach. As shown in Table 18, we find that while
this procedure does improve performance in some cases, the performance does not improve much
beyond 8k steps. This indicates that there is a possibility that online methods can perform better than
Prime if they are run for many more optimization iterations against the simulator, but they may not
be as data-efficient as Prime.
Hyperparameter tuning for firefly: Since the online optimization algorithms we run have access
to querying the simulator over the course of training, we can simply utilize the value of the latest
proposed design as a way to perform early stopping and hyperparameter tuning. A naive way to
perform hyperparameter tuning for such evolutionary methods is to run the algorithm for multiple
rounds with multiple hyperparameters, however this is compute and time intensive. Therefore, we
adopted a dynamic hyperparameter tuning strategy. Our implementation of the firefly optimizer
tunes hyperparameters by scoring a set of hyperparameters based on its best performance over a
sliding window of T data points. This allows us to adapt to the best hyperparameters on the fly,
within the course of optimization, effectively balancing the number of runs that need to be run in the
simulator and hyperparameter tuning. This dynamic hyperparameter tuning strategy requires some
initial coverage of the hyperparameter space before hyperparameter tuning begins, and therefore,
this tuning begins only after 750 datapoints. After this initial phase, every T = 50 iterations, the
parameters γ and β0 are updated via an evolutionary scoring strategy towards their best value.
Discussion of t-RNN Enc and t-RNN Dec. Finally, we discuss the results of the evolutionary
approach on the t-RNN Enc and t-RNN Dec tasks, for which the convergence plots are shown in
Figures 8h and 8i. Observe that the best solution found by this optimization procedure converges quite
quickly in this case (with about 1000 iterations) and the evolutionary method, despite the dynamic
hyperparameter tuning is unable to find a better solution. We hypothesize that this is because the
performance of a local optimization method may suffer heavily due to the poor landscape of the
objective function, and it may get stuck if it continuously observes only infeasible points over the
course of optimization.
B.1.2	Exact Hyperparameters Found By Our Cross-Validation S trategy
In this section, we present the exact hyperparameters found by our cross-validation strategy discussed
in Section 4. To recap, our offline cross-validation strategy finds the early stopping checkpoint and
selects the values of α and β in Equation 3 that attain the highest rank correlation on a held-out
validation set consisting of top 20% of the dataset feasible samples. The values of α, β and checkpoint
selected for the experiments in Table 3, Table 4, Table 5 and 6 are shown in Table 19.
22
Published as a conference paper at ICLR 2022
400
300
0	2000	4000	6000	8000
# of Data Points
! 1 I 1
AUUaIe-J
(b) MobileNetV2
(c) MobileNetV3
(a) MobileNetEdgeTPU
O IOOO 2000	3000	4000	5000
# of Data Points
(d) M4
0	2000	4000	6000	8000
# of Data Points
(e) M5
150
125 --------------------------------------------
0	1000	2000	3000	4000	5000
# of Data Points
(f) M6
0	2000	4000	6000	8000	0	2000	4000	6000	8000	0	2000	4000	6000	8000
# of Data Points	# of Data Points	# of Data Points
(g) U-Net	(h) t-RNN Dec	(i) t-RNN Enc
Figure 8: Optimization behavior of Firefly optimizer (Online Evolutionary). Observe that the optimization
procedure converges and plateaus very quickly (at least 1000 iterations in advance) and hence we stop at 8000
iterations. In the case of t-RNN Enc and t-RNN Dec, we find that the evolutionary algorithm performs poorly
and we suspect this is because it saturates quite quickly to a suboptimal solution and is unable to escape. This is
also evident from Figures 8h and 8i, where we observe that online optimization plateaus the fastest for these
RNN applications.
AUUeae-I
♦
AUUeae-I
B.2	Details of Architecting Accelerators for Multiple Applications
Simultaneously
Now we will provide details of the tasks from Table 4 where the goal is to architect an accelerator
which is jointly optimized for multiple application models. For such tasks, we augment data-points
for each model with the context vector ck from Table 2 that summarizes certain parameters for each
application. For entries in this context vector that have extremely high magnitudes (e.g., model
parameters and number of compute operations), we normalize the values by the sum of values across
the applications considered to only encode the relative scale, and not the absolute value which is
not required. To better visualize the number of feasible accelerators for joint optimization, Figure 9
show the tSNE plot (raw architecture configurations are used as input) of high-performing accelerator
configurations. The blue-colored dots are the jointly feasible accelerators in the combined dataset,
and note that these data points are no more than 20-30 in total. The highlighted red star presents the
best design suggested by Prime with average latency of 334.70 (Table 4). This indicates that this
contextual, multi-application problem poses a challenge for data-driven methods: these methods need
to produce optimized designs even though very few accelerators are jointly feasible in the combined
dataset. Despite this limitation, Prime successfully finds more efficient accelerator configurations
that attain low latency values on each of the applications jointly, as shown in Table 4.
23
Published as a conference paper at ICLR 2022
Table 19: Hyperparameters α, β and checkpoint index (measured in terms of gradient steps on the learned
conservative model) for PRIME found by our offline cross-validation strategy discussed in Section 4, that is
based on the Kendall’s rank correlation on the validation set (note that no simulator queries were used to
tune hyperparameters). In the case of the multi-task and zero-shot scenarios, when training on more than one
application, the batch size used for training PRIME increases to N -fold, where N is the number of applications
in the training set, therefore we likely find that even a few gradient steps are good enough.
Table	Application	α	β	Checkpoint Index
Table 3	MobileNetEdgeTPU	0.01	5.0	80000
Table 3	MobileNetV2	-50	-50	120000
Table 3	MobileNetV3	-50	0.01	80000
Table 3	"m4	-01	^00	80000
Table 3	"M5	-50	-T.q-	80000
Table 3	^M6	T.q	ɪθ-	60000
Table 3	U-Net	-00	-T.o-	100000
Table 3	t-RNN Dec	^.0	^00	60000
Table 3	t-RNN Enc	-00	^0!	60000
Table 4	MobileNet (EdgeTPU, V2, V3)	-50	0.01	60000
Table 4	MobileNet (V2, V3), M5, M6	-00	-50	30000
Table 4	MobileNet (EdgeTPU, V2, V3), M4, M5, M6	-03	ɪo	100000
Table 4	MobileNet (EdgeTPU, V2, V3), M4, M5, M6,t-RNN EnC (Area 29.0)	-00	ɪo-	20000
Table 4	MobileNet (EdgeTPU, V2, V3), M4, M5, M6,t-RNN EnC (Area 100.0)	-00	^00-	20000
Table 4	MobileNet (EdgeTPU, V2, V3), M4, M5, M6, U-Net, t-RNN (Enc, Dec) (Area 29.0)	0.01	0.01	10000
Table 4	MobileNet (EdgeTPU, V2, V3), M4, M5, M6, U-Net, t-RNN (Enc, Dec) (Area 100.0)	0.01	^0!	20000
Table 5	Train (Zero-Shot): MobileNet (EdgeTPU, V3)	-50	0.01	60000
Table 5	Train (Zero-Shot): MobileNet (V2, V3), M5, M6	-00	-50	30000
Table 5	Train (Zero-Shot): MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN Enc (Area 29.0)-	-00	ɪo	20000
Table 5	Train (Zero-Shot): MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN Enc (Area 100.0)	-0!	-50-	20000
Table 6	MobileNetV2 (NVDLA)	-00	ɪo-	40000
Table 6	MobileNetV2 (ShinDianNao)	-00	ɪo-	40000
Table 6	ResNet 50 (NVDLA)	0.01	ɪo-	40000
Table 6	ResNet 50 (ShinDianNao)	-00	^00-	75000
Table 6	Transformer (NVDLA)	0.01	ɪo	200000
Table 6	Transformer (ShinDianNao)	-00-	^0!-	100000
-15.0	-10.0	-5.0	0.0
5.0	10.0	15.0
tSNE-×
Figure 9: tSNE plot of the joint dataset and randomly sampled infeasible data points. The blue points show the
accelerator configurations that are jointly feasible for all the applications. The highlighted point with red star
shows the best design proposed by PrimeThe rest of the points show the infeasible points.
B.3	Dataset Sensitivity to Accelerator Parameters
We visualize the sensitivity of the objective function (e.g. latency) with respect to the changes
in certain accelerator parameters, such as memory size (Table 1), in Figure 11b, illustrating this
sensitivity. As shown in the Figure, the latency objective that we seek to optimize can exhibit high
sensitivity to small variations in the architecture parameters, making the optimization landscape
particularly ill-behaved. Thus, a small change in one of the discrete parameters, can induce a large
change in the optimization objective. This characteristic of the dataset further makes the optimization
task challenging.
24
Published as a conference paper at ICLR 2022
Figure 10: Overview of ShiDianNao dataflow accelerator. This dataflow accelerators exhibits an output-
stationary dataflow where it keeps the partial results stationary within each processing elements (PEs).
Table 20: The evaluated applications, their model parameter size, number of compute operations, and normalized
compute-to-memory ratio.
Name	∣ Model Param ∣ # of Compute Ops. ∣ Normalized ComPute-to-Memory Ratio
MobileNetEdgeTPU	3.87 MB	1,989,811,168	1.38e-1
MobileNetV2	3.31MB	609,353,376	4.96e-2
MobileNetV3	5.20MB	449,219,600	2.33e-2
M4	-6.23MB	3,471,920,128	1.5e-1
M5	-2.16MB	939,752,960	1.17e-1
M6	-0.41MB	228,146,848	1.5e-1
U-Net	3.69 MB	13,707,214,848	E0-
t-RNN Dec	19MB	40,116,224	5.68e-4
t-RNN Enc	-21.62 MB-	45,621,248	5.68e-4
C Overview of Accelerators and Search Space
This section briefly discuss the additional accelerators (similar to [30]) that we evaluate in this work,
namely NVDLA [43] and ShiDianNao [10], and their corresponding search spaces.
NVDLA: Nvidia Deep Learning Accelerator NVDLA [42] is an open architecture inference accel-
erator designed and maintained by Nvidia. In compared to other inference accelerators, NVDLA is a
weight stationary accelerator. That is, it retains the model parameters on each processing elements and
parallelizes the computations across input and output channels. NVDLA-style dataflow accelerators
generally yield better performance for the computations of layers at the later processing stages. This
is because these layers generally have larger model parameters that could benefit from less data
movement associated to the model parameters.
ShiDianNao: Vision Accelerator Figure 10 shows the high-level schematic of ShiDianNao acceler-
ator [10]. ShiDianNao-style dataflow accelerator is an output-stationary accelerator. That is, it keeps
the partial results inside each PE and instead move the model parameters and input channel data. As
such, in compared to NVDLA-style accelerators, ShiDianNao provides better performance for the
computations of the layers with large output channels (generally first few layers of a model).
Search space of dataflow accelerators. We follow a similar methodology as [30] to evaluate
additional hardware accelerators, discussed in the previous paragraphs. We use MAESTRO [37], an
analytical cost model, that supports the performance modeling of various dataflow accelerators. In
this joint accelerator design and dataflow optimization problem, the total number of parameters to be
optimized is up to 106—the tuple of (# of PEs, Buffers) per per model layer—with each parameter
taking one of 12 discrete values. This makes the hardware search space consist of ≈ 2.5×10114
accelerator configurations. We also note that while the method proposed in [30] treats the accelerator
design problem as a sequential decision making problem, and uses reinforcement learning techniques,
Prime simply designs the whole accelerator in a single step, treating it as a model-based optimization
problem.
D Subset of Applications for Good Zero-Shot Performance
In this section, we present the results of an ablation study with the goal to identify a subset of
applications such that training on data from only these applications yields good zero-shot performance
25
Published as a conference paper at ICLR 2022
×105
4.0
3.5
3.0
U 25
§2.0
UL5
1.0
0.5
0.0
Runtime
əluwnQ;
700.0
650.0
600.0
550.0
500.0
0	12	3
Core Memory Size
(a)
(b)
Figure 11: The (a) histogram of infeasible (orange bar with large score values)/feasible (blue bars) data points
and (b) the sensitivity of runtime to the SiZe of core memory for the MobileNetEdgeTPU [26] dataset.
Figure 12: tSNE plot of the infeasible and feasible hardware accelerator designs. Note that feasible designs
(shown in blue) are embedded in a sea of infeasible designs (shown in red), which makes this a challenging
domain for optimiZation methods.
across all the nine applications studied in this work. Since we cannot train Prime for every subset of
applications because the space of subsets of all applications is exponentially large, we utiliZed some
heuristics in devising the subset of applications we would train on, with the goal to make interesting
observations that allow us to devise rough guidelines for performing application selection.
Our heuristic for devising subsets of applications: Building on the intuition that applications with
very different compute to memory ratios (shown in Table 20) may require different accelerator designs
一 for example, if our goal is to run a compute-intensive application, We likely need an accelerator
design with more compute units - we study two subsets of training applications: (1) MobileNetV2,
MobileNetV3, M6, M5, and (2) MobileNetV2, MobilenetV3, M5, t-RNN Enc. Note that, these tWo
combinations only differ in whether some RNN application was used in training or not. As shown in
Table 20, the t-RNN applications admit a very different compute to memory ratio, for instance, while
this ratio is 5.68e — 4 for t-RNN Enc and t-RNN Dec, it is much different 〜0.01 — 0.2 for other
models MobileNetEdgeTPU, MobileNetV2, MobileNetV3, M5, and M6. This means that likely
t-RNN Enc and Dec will require different kinds of accelerators for good performance compared to
the other applications.
Results: We present the performance of Zero-shot evaluating the designed accelerator obtained by
training on combinations (1) and (2), and also the accelerator obtained by training on (3) seven
applications from Table 5 in Table 21 as reference. We make some key takeaways from the results:
•	The performance of both configuration (1) and training with seven applications ((3), last
row of Table 21) are similar.
26
Published as a conference paper at ICLR 2022
O OOoO
6
AUUe~l p一 p,Jd
∞80
200	400	600	800 IOOO
Actual Latency
PRIME does not overestimate
Figure 14: Plot showing the calibration plot of the predicted (y-axis) and actual latencies (x-axis) of accelerators
found by PRIME. Compared to Figure 13, observe that all the acclerator configurations lie above y = x, meaning
that Prime predicts a higher latency (y-axis) compared to the actual latency. This means that Prime does not
think that accelerators that attain high-latency values under the simulator, are actually good. We also provide a
zoomed-in version of the plot on the right, which shows that there are accelerators do have meaningfully distinct
latency predictions under Prime. Observe in the zoomed-in plot that the designs that attain small predicted
latencies also perform relatively better under the actual latency compared to the designs that attain larger
predicted latency of 〜14000-16000 under the Prime surrogate. Optimizing against Prime is still effective
because optimization just needs relative correctness of values, not absolutely correct latency predictions.
Figure 13: To verify if the overestimation hypothesis-that optimizing an accelerator against a naive standard
surrogate model is likely to find optimizers that appear promising under the learned model, but do not actually
attain low-latency values-in our domain, we plot a calibration plot of the top accelerator designs found by
optimizing a naively trained standard surrogate model. In the scatter plot, we represent each accelerator as a
point with its x-coordinate equal to the actual latency obtained by running this accelerator design in the simulator
and the y-coordinate equal to the predicted latency under the learned surrogate. Note that for a large chunk of
designs, the predicted latency is much smaller than their actual latency (i.e., these designs lie beneath the y = x
line in the plot above). This means that optimizing designs under a naive surrogate model is prone to finding
designs that appear overly promising (i.e., attain lower predicted latency values), but are not actually promising.
This confirms the presence of the overestimation hypothesis on our problem domain.
Actual Latency
O
O
•	in case (2), when the training applications consist of four applications in which one appli-
cation is t-RNN Enc, with drastically different compute to memory ratio (Table 20), the
performance on an average across all applications becomes slightly worse (compare the
performance in (2) vs (3)).
Conclusion and guidance on selecting good applications: The above results indicate that only a
few applications (e.g., four applications in case (1)) can be enough for good performance on all nine
applications. While this set may not be not minimal, it is certainly a much smaller set compared to the
nine applications considered. Adding an RNN application in case (2) increases latency in compared to
case (1), because t-RNN Enc likely admits a very different optimal accelerator compared to the other
applications due to a very different compute/memory ratio, which in turn skews the generalization of
the surrogate learned by Prime when trained only on this limited set of four applications. However,
27
Published as a conference paper at ICLR 2022
Table 21: Additional ablation study under zero-shot setting when the test applications include all the nine
evaluated models (e.g. MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN Dec, t-RNN Enc, U-Net). Lower
latency is better. From left to right: the applications used to train the surrogate model in PRIME, the area
constraint of the accelerator, Prime’s (best, median) latency.
Train Applications	Area	Prime (best, median)
(1) MobileNet (V2, V3), M5, M6	29 mm2	(426.65,427.35)
(2) MobiIeNet(V2, V3), M5,t-RNN EnC	29 mm2	(461.79,464.87)
(3) MobiIeNet(EdgeTPU, V2, V3), M4, M5, M6,t-RNN EnC	29 mm2	(426.65,427.94)
when seven applications are provided in case (3), even when the set of training applications includes
t-RNN, its contribution on the Prime surrogate is reduced since many other compute intensive
applications are also provided in the training set and the resulting accelerator performs well.
Practitioner guidance: The primary practitioner guidance we can conclude here is that the models
used for training must be representative of the overall distribution of the target models that we
want to zero-shot generalize to. Since a number of our applications are compute intensive, we
were able to simply utilize set (1) to attain good performance on all the applications. On the other
hand, in case (2), when the t-RNN Enc application was over-represented - while seven of nine
applications we considered were primarily compute intensive, one out of four applications we used
for training in case (2) were memory intensive - this hurt performance on the overall set. Therefore,
we believe that ensuring that the training subset of applications is adequately aligned with the overall
set of applications in terms of compute/memory ratio statistic is imperative for favorable zero-shot
performance.
For a practitioner deciding between zero-shot generalization and additional data collection, it
may make sense to test if the target application admits a similar value of the compute to memory
ratio as an already existing application. If it does, then the practitioner might be able to utilize
the zero-shot generalization, as is indicated with the good performance of case (1), whereas if the
compute/memory ratio is heavily different from any seen application, zero-shot generalization to the
target application may be worse. Finally, making sure that the training applications adequately reflect
the compute/memory ratio statistic for the overall target set is important.
28