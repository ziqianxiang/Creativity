Published as a conference paper at ICLR 2022
GRAND++:	Graph Neural Diffusion with A
Source Term
Matthew Thorpe*1, Hedi Xia*2, Tan Nguyen*2, Thomas Strohmer3
Andrea L. Bertozzi2, Stanley J. Osher2 & Bao Wang4 *
1Department of Mathematics, University of Manchester, Manchester M13 9PL, UK
2 Department of Mathematics, UCLA, Los Angeles, CA, 90095, USA
3Department of Mathematics, UC Davis, Davis, CA 95616, USA
4Department of Mathematics and Scientific Computing and Imaging (SCI) Institute
University of Utah, Salt Lake City, UT, 84102, USA
Ab stract
We propose GRAph Neural Diffusion with a source term (GRAND++) for graph
deep learning with a limited number of labeled nodes, i.e., low-labeling rate.
GRAND++ is a class of continuous-depth graph deep learning architectures whose
theoretical underpinning is the diffusion process on graphs with a source term.
The source term guarantees two interesting theoretical properties of GRAND++:
(i) the representation of graph nodes, under the dynamics of GRAND++, will not
converge to a constant vector over all nodes even as the time goes to infinity,
which mitigates the over-smoothing issue of graph neural networks and enables
graph learning in very deep architectures. (ii) GRAND++ can provide accurate
classification even when the model is trained with a very limited number of la-
beled training data. We experimentally verify the above two advantages on vari-
ous graph deep learning benchmark tasks, showing a significant improvement over
many existing graph neural networks.
1 Introduction
Graph neural networks (GNNs) are the backbone for deep learning on graphs. Recent GNN ar-
chitectures include graph convolutional networks (GCNs) [30], ChebyNet [16], GraphSAGE [29],
neural graph fingerprints [20], message passing neural networks [28], and graph attention networks
(GATs) [54]. These graph deep networks have achieved success in many applications, including
computational physics and computational chemistry [20, 28, 3], recommender systems [41, 62], and
social networks [63, 47]. Hyperbolic GNNs have also been proposed to enable certain kinds of
data embedding with much smaller distortion [11, 37]. See [6] for some recent advances of GNN
algorithm development and applications.
A well-known problem of GNNs is that increasing the depth of GNNs often results in a significant
drop in performance on various graph learning tasks. This performance degradation has been widely
interpreted as the over-smoothing issue of GNNs [35, 44, 12]. Intuitively, GNN layers update the
node representation by taking a weighted average of its neighbors’ features, making representations
for neighboring nodes to be similar. As the GNN architecture gets deeper, all nodes’ representa-
tion will become indistinguishable resulting in over-smoothing. In Sec. 2, we briefly show that
certain GNNs have a diffusive nature which makes over-smoothing inevitable. Another interesting
interpretation of the GNN performance degradation is via a bottleneck [1], since a GNN tends to
represent exponentially growing information from neighbors with fixed-size vectors. Several algo-
rithms have been proposed to mitigate the over-smoothing of GNNs, including skip connection and
dilated convolution [33], Jumping Knowledge [60], DropEdge [49], PairNorm [64], graph neural
diffusion (GRAND) [10], and wave equation motivated GNNs [21]. Nevertheless, developing deep
GNN architectures is still in its infancy compared to the development of other deep networks.
Besides suffering from over-smoothing, we notice that the accuracy of existing GNNs drops severely
when they are trained with a limited labeled data. As illustrated in Fig. 1, the test accuracy of several
* Correspond to wangbaonj@gmail.com or matthew.thorpe-2@manchester.ac.uk
1
Published as a conference paper at ICLR 2022
celebrated GNN architectures, including GCN, GAT, and GraphSage, drops rapidly when they are
trained with fewer labeled data. Moreover, the variance of classification accuracy grows significantly
as number of labeled nodes drops. Indeed, semi-supervised graph learning with very low-labeling
rates has been studied in the Laplace learning and graph deep learning settings, see, e.g., [36, 9, 23];
one question is can we develop new GNN architectures to improve the performance of graph deep
learning in low-labeling rate regimes?
CiteSeer
#Labeled Nodes Per Class
PUbMed
CORA
Figure 1: Test accuracy of GCN, GAT, and GraphSage vs. the number of labeled nodes per class. All networks
have 2 layers, and each experiment is run with 100 splits and 20 random seeds following [10]. The accuracy
drops rapidly with fewer labeled data for training. CORA, CiteSeer, and PubMed have 2485, 2120, and 19717
nodes in total respectively. Results on more benchmark GNN architectures are in Appendix D.4.
1.1	Our contribution
With the above GNN problems in mind, we focus on developing new continuous-depth GNNs to
overcome over-smoothing and boost the accuracy of GNNs with a limited number of labeled data.
We first present a random walk interpretation of the GRAND model [10], revealing a potentially in-
evitable over-smoothing phenomenon when GRAND is implicitly very deep. Based on the random
walk viewpoint of GRAND, we then propose graph neural diffusion with a source term (GRAND++)
that corrects the bias arising from the diffusion process, see Sec. 5 for details. GRAND++ theoret-
ically guarantees that: (i) under GRAND++ dynamics, the graph node features do not converge to
a constant vector over all nodes even as the time goes to infinity, and (ii) GRAND++ can provide
accurate prediction even when it is trained with a limited number of labeled nodes. Moreover, these
theoretical results resonate with the practical advantages of GRAND++. We summarize the major
practical advantages of GRAND++ below.
•	GRAND++ can effectively overcome the over-smoothing issue; it is remarkably more accurate
than existing GNNs when the architecture is very deep.
•	GRAND++ is suitable for graph deep learning when only a few nodes are labeled as training data.
Moreover, in the low-labeling rates, GRAND++ can be more accurate when the network is deeper.
•	GRAND++ inherits the continuous-depth merit from GRAND, which defines the network depth
implicitly and enables memory-efficient training by using the adjoint method.
1.2	Related work
Diffusion on graphs and continuous-depth graph neural networks. Diffusion has been defined
on graphs, see, e.g., [25, 24], and used in various applications, including data clustering and di-
mension reduction [15, 4], image processing [27, 22, 17, 38], and semi-supervised graph nodes
classification [67, 65]. From the numerical viewpoint, fast algorithms have been proposed for using
diffusion on graphs to solve penalized graph cut problems [26]. The connection between GNNs
and diffusion on graphs has been studied substantially. For instance, GNN has been interpreted as a
diffusion process on graphs, which performs low-pass filtering on the input features [43]. Moreover,
insights from the diffusion process on graphs have been used to improve the performance of GNNs,
see, e.g., [2, 36, 31, 57].
Leveraging neural ordinary differential equations (ODEs) [13], continuous-depth GNNs have been
proposed, see, e.g., [45, 58, 68]. One recent work is GRAND [10], which parameterizes the diffusion
equation on graphs with a neural network. See Sec. 3 for a brief review of GRAND.
Neural ODEs. Neural ODEs [13] are a class of continuous-depth neural networks whose depth is
defined implicitly. Training neural ODEs using the adjoint method [46] is more memory efficient
than training other neural networks using backpropagation. We provide a brief review of neural
ODEs and the adjoint method in Appendix C. GRANDs [10] are a class of neural partial differen-
tial equations (PDEs) on graphs that can also be considered as a coupled system of neural ODEs.
Furthermore, GRANDs are also trained by using the adjoint method.
2
Published as a conference paper at ICLR 2022
Laplace learning and Poisson learning. Laplace learning has been used for semi-supervised data
classification [67, 65, 56], image processing [7, 27], etc. Direct application of Laplace learning
with Gaussian weights [5] or locally linear embedding weights [50] for the above tasks may cause
inference inconsistency when only a limited number of graph nodes are labeled, resulting in poor
performance. Several algorithms address the inference inconsistency at low labeling rate. They
include up-weighting the weights of the labeled data [52] and the p-Laplacian [8, 48, 66]. In [9], the
authors have proposed Poisson learning for improving Laplace learning at extremely low-labeling
rate regimes. Poisson learning augments Laplace learning with a Green’s function at each labeled
data, enabling accurate node classification when only a few labeled data are available. Compared to
Laplace learning, Poisson learning adds Green’s function to the label of each labeled node and then
performs label propagation to predict the label for unlabeled graph nodes. GRAND and GRAND++
both learn graph node representations and perform prediction by activating the node representations,
which are fundamentally different from Laplace and Poisson learning.
1.3	Notation
We denote scalars by lower- or upper-case letters and vectors and matrices by lower- and upper-case
boldface letters, respectively. For a matrix A, we denote its transpose as A> and its Hadamard
product with another matrix B as A B, i.e., the entrywise multiplication of A and B. We write
the set {1,2, ∙ ∙ ∙ , n} as [n]. We denote the probability and expectation of a given random variable
x as P(x) and E[x], respectively. The meaning of other notations can be inferred from the context.
1.4	Organization
The paper is organized as follows: In Sec. 2, we review diffusion equation on graphs and its con-
nection to GNNs. In Secs. 3 and 4, we briefly review GRAND and present a random walk inter-
pretation of GRAND, respectively. Leveraging the random walk viewpoint of GRAND, we propose
GRAND++ for deep graph learning with theoretical guarantees in Sec. 5. We verify the efficacy of
GRAND++ in Sec. 6. Technical proofs and more validations are provided in the appendix.
2	Background
Diffusion equation on graphs. Let G = (X, W) represent an undirected graph with n nodes,
where X = ([x(1)]>, ∙ ∙ ∙ , [x(n)]>)> ∈ Rn×d With each row x(i) ∈ Rd a feature vector and
W := Wij a n × n matrix with Wij representing the similarity (edge weight) between the ith
and jth feature vectors, and we assume Wij = Wji . Consider the following diffusion process that
evolves the feature matrix X on the graph (see Appendix A for a brief review of calculus on graphs):
dXt)=div(G(X(t),t) ΘVX(t)),	(1)
where X(t) = ([x⑴(t)]>,…，[x(n)(t)]>)> ∈ Rn×d with x(i)(0) = x(i), V and div are the
gradient and divergence operators, respectively. The matrix G(X(t), t) is chosen such that W Θ G
is right-stochastic, i.e., each row of W Θ G summing to 1. In the machine learning setting, we can
parameterize G with learnable parameters θ which we denote by G(X(t), t, θ). The initial features
are evolved under the diffusion dynamics (1) from t = 0 to T to learn the final representation X(T)
for further machine learning tasks.
In the simplest case when G(X(t), t) is only dependent on the initial node features X, i.e., G
is time-independent, right-stochasticity implies Pj Wij Gij = 1 for all i, and so we focus on the
particular case when Gij = 1/di with di = Pjn=1 Wij. In this case the right-hand side of (1)
reduces to the negative of the random-walk Laplacian applied to X(t) and (1) becomes
dX(t) = div(G(X(t),t) Θ VX⑴)=-LX⑴，	⑵
where L = I - D-1W := I - A (A := A(X)) is the random walk Laplacian and D is diagonal
with Dii = di. See [14, 25, 24] for more about random walk Laplacian and diffusion on graphs.
Graph neural networks. Applying forward Euler discretization, with step size δt < 1, of (2) gives
, , , , ~ , ,
X(kδt) = X((k - 1)δt)	-	δtLX((k	- 1)δt)	:=	LX((k	-	1)δt),	for k =	1, 2, ∙ ∙ ∙ ,K,	(3)
3
Published as a conference paper at ICLR 2022
T = Kδt, and X (0) = X. Note that the matrix L is the discretization of the diffusion operator,
which is a special low-pass filter. Equation (3) is a prototype for motivating GNNs: by introducing
weights W (k) ∈ Rd×d and a nonlinearity σ, e.g., ReLU, into (3), we have
X ((k + 1)δt) = σ(L X (kδt )W(k)).
(4)
The model in (4) is similar to the well-established GCN architecture proposed in [30]. The diffusive
nature of the GNN architecture in (4) further explains the over-smoothing issue of training deep
GNNs; the deeper the network architecture is, the more the node features diffuse. Eventually, all
nodes share similar features and become indistinguishable. See Sec. 5 for a detailed analysis.
3	A B rief Review of GRAND
GRAND is anew continuous-depth GNN proposed in [10]. It integrates a learnable encoder function
φ and a learnable decoder function ψ with the neural network parameterized graph diffusion process,
resulting in the prediction Y = ψ(X(T)), where X(T) is computed as
X(T) = X(0) + ZOT dXt)dt,
with X(0) = φ(X),
(5)
where ∂X(t)∕∂t is given by the graph diffusion equation (2). From the neural ODE perspective, We
can perform forward propagation of GRAND, i.e., we solve (5), using numerical ODE solvers.
In the simplest case, when G is only dependent on the initial node features, we can rewrite (1) as
-∂t^ = (A(X) - I)x⑴，	⑹
GRAND models the diffusivity A(X) in (6) by the multi-head self-attention mechanism; potential
choices of the attention function include the ones proposed in [53, 54]. More precisely, in GRAND
A(X) = 1 Ph=1 Al (X) with h being the number of heads and the attention matrix Al(X)=
(al(xi, Xj)), for l = 1,…，h,is computed as follows:
l	exp (LeakyReLU(al>[WlXi∣∣WlXj]))
a xi，Xj	Pk∈Ni exp (LeakyReLU(al>[WlXi∣∣ WlXk]))，	⑺
where Wl and al are learned, k is the concatenation operator, and Ni is the index set of the nodes
that are connected to the ith node in the graph. GRAND with the attention in (7) is called GRAND-l,
that is, GRAND-l is a special case of GRAND when the diffusivity is dependent only on the initial
graph node features. Time-dependent attention and graph rewiring can be integrated into GRAND,
resulting in GRAND-nl and GRAND-nl-rw, respectively [10]. From the ODE viewpoint, GRAND
and its variants are a class of coupled neural ODEs defined on an unweighted graph. Their merits
include continuous-depth and memory-efficient training using the adjoint method [46, 13].
4 Random walk viewpoint of GRAND
In this section, we present a random walk interpretation of GRAND. The connection between graph
random walks and the diffusion equation has been extensively studied, but we recap the key idea
here to motivate the new GRAND with a source term architecture. Let {B(i) (k)}k∈N be the random
walk on {X(j)(0)}jn=1 defined by, for δt ∈ [0, 1],
B(i)(0) = X(i)(0)
P(B⑴(k +1) = X⑹(0)|B⑴(k) = Xj)(O)) = { δtWj't if' = j
(8)
where dj = Pn=I Wj' (assume w`' = 0 for all '). Proposition 1 below is well-known, see [67].
We provide the proof of Proposition 1 and all the subsequent theoretical results in Appendix B.
Proposition 1 Let X solve (3) and B(i) be the random walk determined by (8) where δt ∈ [0, 1].
Then
X(i)(δtk) = EB(i)(k).
4
Published as a conference paper at ICLR 2022
Proposition 2 below gives the stationary distribution of the random walk {B(i) (k)}k∈N.
Proposition 2 Assume the graph G
{B(i)(k)}k∈N is
π=
(X, W) is connected. Then, the stationary distribution of
(9)
which is independent of the starting position x(i).
Furthermore, we have the following theoretical result on the asymptotic behavior of graph node
features under the GRAND dynamics given by (3).
Proposition 3 Assume the graph G = (X, W) is connected. Thenfor all i = 1,…,n, we have
n
x(i) (kδt) → xe := X x(j) (0)πj, as k → ∞.
j=1
Hence, for the case of (3), i.e., GRAND-l, we expect the output to be approximately independent of
the input, due to over-smoothing. Of course, once we reintroduce the X(t) dependence back into G
in (1) and (2) or into the operator A in (6) then the above arguments no longer hold. Nevertheless,
the GRAND architectures are built on a principle that is ill-suited to deep networks. In the next
section we introduce a source term and perform a similar random walk analysis that illustrates how
the new architecture can be better suited for deep GNN architectures.
5 GRAND++: Graph Neural Diffusion with A Source Term
5.1	Algorithm and formulation
At the core of GRAND++ is the introduction of a source term into GRAND, leveraging the random
walk viewpoint of the diffusion process. We take a small subset of feature vectors, indexed by
I ⊆ [n], believed to be “trustworthy” for use as a source term. In particular, we use the features of
labeled data. The GRAND++ dynamics are defined by a diffusion equation with a source term (we
use the variable z for GRAND++-related dynamics and x for GRAND dynamics)
∂ z(i)(t)
∂t
div G(Z(t),t) Θ VZ(t)](i) + X δijCj
j∈I
(10)
where Cj is the source at feature vector of node j . Below we motivate a particular choice of Cj .
The key idea is to first characterise the bias that arises from the diffusion and use that to propose
a correction via the choice of source terms Cj . Following the simplifications in (2), our diffusion
equation (without the source term) follows the approximate dynamics when t 1
dx∂() = - [LX⑴产=-X⑴⑴ + ； X Wijx⑶⑴ ≈ 0.
dt	X一{z—} di j = 1 X一{一}
∙-∙w	I	ɪ	--⅛∙
≈x	≈x
For i ∈ I ,it transpires that choosing Ci = x(i) 一 X (where X is defined below) gives rise to
a random walk interpretation that allows us to prove that the oversmoothing seen in the GRAND
model is avoided.
One can in fact choose Xe with a certain degree of freedom. Ifwe initialise X(0) = X then we obtain
Xe = Pjn=1 X(j)πj (as is usual in the GRAND model). However, as the similarities are encoded in
the graph weights, and the diffusion dynamics will drive it towards a non-trivial state, we can choose
a different initialization than X(0) = X. Through connections with random walks we, in the next
subsection, motivate an alternative initialisation
XZ⑴⑼=X x( id-X, Where χ = ∣∣∣ x Xj)
i=1	i∈I	di	|I| j∈I
(11)
5
Published as a conference paper at ICLR 2022
with the dynamics
∂z(i)(t)
∂t
div [G(Z(t),t) Θ VZ(t)](i) + X δij (X⑴一X).
j∈I
(12)
For example, we could choose
Z⑺(0) = ʃ di (x(i)- X) if i ∈I or Z ⑴(0) = X⑴-c,
0	otherwise,
where C = n (Pn=I x(i) - Pj∈ι x(d-x) is chosen such that (11) holds. We do not believe that the
constant c (that shifts by a constant) is particularly important but it is included to provide a random
walk interpretation which helps to understand the deep architecture (when T is big) behaviour of
the model GRAND++. The justification for this choice will be made in Sec. 5.2. To summarize, the
GRAND++ model in (12), with initial condition satisfying (11), simply adds a source term to the
original GRAND model and uses a different initial condition. Therefore, the nonlinear diffusivity
and graph rewiring tricks used by GRAND can be easily integrated into GRAND++. In terms of
implementation, since GRAND++ merely changes the right-hand side of GRAND, which again can
be regarded as a system of coupled first-order neural ODEs; we can leverage neural ODE training,
testing, and inference for GRAND++ similar to GRAND.
In the next subsection we explore the random walk connection of the above model, suggesting that
building a graph neural network based on the diffusion with source model does not suffer from the
same degeneracy as we observed in Sec. 4 and is therefore better suited to build deep GNNs. In
particular, we can write the diffusion with source model as the short time expected behaviour of a
random walk and therefore we do not have the issue of reaching the stationary state (in other words
passing the mixing time). Our experiments in Sec. 6 suggest the formal motivation holds and we are
able to design deep GNNs.
5.2	The random walk perspective of GRAND++
Let us continue to consider the simplified model in the previous subsection, i.e., assume the dynam-
ics are governed by
dz∂() = - [LZ(t)](i) + X m(X⑴-X)	(13)
j ∈I
where the initial condition satisfies (11). Using the forward Euler discretisation of the above dynam-
ics we have
Z⑴(δtk) = Z⑴(δt(k - 1)) - δt [LZ(δt(k - 1))](i) + δt Xδij (X⑴一X),	(14)
j∈I
for k = 1, 2, . . . , K where again T = Kδt .
We use the same random walk as that introduced in Sec. 4, i.e. the random walk defined by (8), but
we will now only consider random walks that are initialised on the nodes indexed by I.
Proposition 4 Let Z solve (14) with the initial condition satisfying (11), and let B(i) be the random
walk determined by (8). Then,
k
Z⑴(kδt) - E X d X (Xj)- X)1b⑸(s)=x(i)
s=0 di j∈I
→ 0 as k → ∞.
Remark 1 In the limit k → ∞ the term
E[X ⅛ X
s=0 i j∈I
X(j)1B(j (s)=x(i
is formally a function of the random walk at all times. Whilst if k is very large (i.e. in comparison to
the mixing time) we still have that
E ʃ X X(j)1Bj)(k)=x(i)
di j∈I
ɪ X Xj) P(Bj)(k) = Xj)) ≈ ? X Xj)	(15)
di j∈I	X-------{-------}	di j∈I
≈πi
6
Published as a conference paper at ICLR 2022
and on the other hand
E d X x1B(j)(k)=χ(i)
i j∈I
X
di
j∈I |
B(j) (k) = x(j) ≈
一■――	,
{z^
≈πi
∏i∣l∣X
di
(16)
From the definition of X we see that (15) and (16) are approximately equal. And therefore we can
understand E[d Pj∈i X^1 B(j) (k)=χ(i) ] as the long time behaviour of E[d Pj∈ι x(j)lB(j)(k)=x(i)]
Very formally we can see that subtracting the long-time behaviour from the all-time behaviour leaves
us with the short time behaviour. This provides one explanation as to why we do not expect the deep
layers to be determined by the stationary state of the random walk (at which point there is little
dependence on the initial layers, causing the deep layers to be approximately constant).
Remark 2 The random walk interpretation
(17)
can be considered to be dual to the random walk interpretation in Sec. 4: in Sec. 4 we released
the random walker from the node of interest, whilst now we release the random walkers from nodes
indexed by I and see how many of them hit the node of interest. We note also that we do not require
a lower bound on the size of the set I. Indeed, if |I| is fixed whilst one takes the number of feature
vectors n → ∞ we still expect many properties of GRAND++, in particular Proposition 5 below, to
hold. This is due to the asymptotic well-posedness of the dual random walk in low labeling rates [9].
Proposition 3 reveals that in the simple setting of (2), GRAND converges to a constant when its
depth goes to infinity. However, this is not true for GRAND++ since the graph node features will
not converge to a constant vector driven by the GRAND++, as shown in Proposition 5 below.
Proposition 5 Assume the graph G = (X, W) is connected. Then z(i) (kδt) that was defined in
(14) does not converge to a constant vector as a function of i as k → ∞. That is, the node features
will not become the same across graph nodes under the GRAND++ dynamics.
Remark 3 Proposition 5 guarantees GRAND++ is less likely to suffer from over-smoothing than
GRAND, and in particular it shows that we have a non-constant deep layer limit, i.e., as t →
∞. Analysing the limit is beyond the scope of the paper but we have seen one characterisation in
Proposition 4. By construction we have ∂z(i)(t)∕∂t ≈ 0 for i ∈ I so one should expect that the
deep layer limit is (close to) a smooth interpolation of the feature vectors labeled by I.
The continuous time model (10) is, in the special case of (13), the mean field limit of the probabilistic
formulation (17). Our proposed algorithm is formulated from the mean-field limit.
6 Experiments
In this section, we compare the performance of GRAND++ with GRAND and several other popu-
lar GNNs on various graph node classification tasks. We aim to show the practical advantages of
GRAND++ in learning with limited labeled data and using deep architectures. Without mentioning
clearly, we use the same hyperparameters that that used for GRAND in [10] for GRAND++. We
provide detailed descriptions of experimental settings and datasets that are omitted in the main text
in Appendix D.1. For all experiments, we run 100 splits for each dataset with 20 random seeds for
each split, which are conducted on a server with four NVIDIA RTX 3090 graphics cards.
We compare the performance of GRAND++ and its nonlinear and graph rewiring variants with sev-
eral popular GNNs on various graph node classification benchmarks. Except for the integration time,
which measures the implicit depth of GRAND and GRAND++, we adopt the experimental settings
of GRAND in [10] for GRAND++ include numerical differential equation solvers. Following [10],
we study seven graph node classification datasets, namely CORA, CiteSeer, PubMed, CoauthorCS,
Computer, Photo, and ogbn-arxiv; we describe these datasets in Appendix D.1.
7
Published as a conference paper at ICLR 2022
6.1	GRAND++ is more resilient to deep architectures
We first show that our introduced source term in (12) can improve the accuracy of GRAND-l when
the architecture is deep, i.e., the integration time T in (5) is big. We denote GRAND-l with the source
term as GRAND++-l. For each node classification task, we train all models using the same number
of labeled nodes as in [10]. Figure 2 contrasts the performance of GRAND-l and GRAND++-l
with different depths, or T , on CORA, CiteSeer, Computer, and Photo datasets. We provide the
detailed results on PubMed and CoauthorCS, together with more comparisons of GRAND++-l with
GRAND-l and several other celebrated GNNs include GCN, GAT, and GraphSage in Table 5 in
Appendix D.2. The results in Fig. 2 and Appendix D.2 confirm that GRAND-l suffers less from
over-smoothing compared to GCN, GAT, and GraphSage. Moreover, GRAND++-l performs on
par with GRAND-l when the depth (T) of the network is small, but GRAND++-l significantly
outperforms GRAND-l when T is large. As T increases, the margin becomes wider, indicating
that GRAND++-l can overcome over-smoothing much more effectively than GRAND-l. Note that
we did not use uniform depth for GRAND-l and GRAND++-l on all datasets because the adaptive
step-size ODE solver fails when T is large for some tasks.
CORA	CiteSeer	Computer	Photo
Figure 2: Test accuracy vs. the “depth” (T in (5)) of GRAND-l and GRAND++-l on the four graph node
classification tasks. We see that GRAND++-l is much more resilient to deep architectures than GRAND-l.
These results show that GRAND++ is better suited for learning with a very deep architecture than GRAND.
Next, we compare GRAND-l and GRAND++-l on the ogbn-arxiv node classification task, which
is a large-scale benchmark. We train two models using labeling rates of 3.0% and 5.0%, re-
spectively; the corresponding test accuracy for GRAND-l/GRAND++-l are 65.26%/66.64% and
67.42%/67.77%, respectively. GRAND++-l outperforms GRAND-l in both labeling rates.We fur-
ther compare GRAND and GRAND++ with different depth on the ogbn-arxiv task in Appendix D.6.
6.2	GRAND++ is more accurate with limited labeled training data
Besides helping to overcome over-smoothing,
our theory shows that the source term can
boost the accuracy of GRAND-l with low-
labeling rates. Table 1 compares the accuracy
of GRAND++-l with GRAND-l, GCN, GAT,
GraphSage, and MoNet, trained with different
numbers of labeled data. Here, we slightly tune
T for GRAND++ based on the optimal value
for GRAND, see Table 4 in the Appendix for
their values. We see that with few labeled data,
in most tasks GRAND++-l is significantly more
accurate than the other GNNs include GRAND-
l, confirming our theoretical insight. For Coau-
Depth (T)
CiteSeer
Depth (T)
CORA
Figure 3: Accuracy of GRAND++-l and GRAND-l for
CORA and CiteSeer, where both models, with different
depth (T), are train with 1 labeled node per class. These
results show that GRAND++ is more effective in learn-
ing with low-labeling rates than GRAND.
thorCS task, both GRAND-l and GRAND++-l are worse than GCN and GraphSage. Moreover,
increasing the depth of GRAND++-l can improve the classification accuracy with limited training
data, but this is not the case for GRAND-l, see Fig. 3. We perform the t-test in Appendix D.5 to
confirm the statistical significance of the accuracy gain of GRAND++ over GRAND in Table 1.
6.3	Time-dependent attention and graph rewiring
The previous experimental results show that GRAND++-l enhances the accuracy of GRAND-l in the
cases when the labeled training data is limited and when the network is deep. Here, we explore the
same strategy for GRAND-nl and GRAND-nl-rw; we name the corresponding models with the new
source term GRAND++-nl and GRAND++-nl-rw, respectively. Table 2 compares GRAND-nl and
8
Published as a conference paper at ICLR 2022
Model #per class CORA
CiteSeer	PubMed CoauthorCS Computer	Photo
GRAND++-l
(ours)
GRAND-l
[10]
GCN [30]
1251020 1251020 12510
54.94 ± 16.09	58.95 ± 9.59	65.94 ± 4.87	60.30 ± 1.50	67.65 ± 0.37	83.12 ± 0.78
66.92 ± 10.04	64.98 ± 8.31	69.31 ± 4.87	76.53 ± 1.85	76.47 ± 1.48	83.71 ± 0.90
77.80 ± 4.46	70.03 ± 3.63	71.99 ± 1.91	84.83 ± 0.84	82.64 ± 0.56	88.33 ± 1.21
80.86 ± 2.99	72.34 ± 2.42	75.13 ± 3.88	86.94 ± 0.46	82.99 ± 0.81	90.65 ± 1.19
82.95 ± 1.37	73.53 ± 3.31	79.16 ± 1.37	90.80 ± 0.34	85.73 ± 0.50	93.55 ± 0.38
52.53 ± 16.40	50.06 ± 17.98	62.11 ± 10.58	59.15 ± 5.73	48.67 ± 1.66	81.25 ± 2.50
64.82 ± 11.16	59.55 ± 10.89	69.00 ± 7.55	73.83 ± 5.58	74.77 ± 1.85	82.13 ± 3.27
76.07 ± 5.08	68.37 ± 5.00	73.98 ± 5.08	85.29 ± 2.19	80.72 ± 1.09	88.27 ± 1.94
80.25 ± 3.40	71.90 ± 7.66	76.33 ± 3.41	87.81 ± 1.36	82.42 ± 1.10	90.98 ± 0.93
82.86 ± 2.39	73.02 ± 5.89	78.76 ± 1.69	91.03 ± 0.47	84.54 ± 0.90	93.53 ± 0.47
47.72 ± 15.33	48.94 ± 10.24	58.61 ± 12.83	65.22 ± 2.25	49.46 ± 1.65	82.94 ± 2.17
60.85 ± 14.01	58.06 ± 9.76	60.45 ± 16.20	83.61 ± 1.49	76.90 ± 1.49	83.61 ± 0.71
73.86 ± 7.97	67.24 ± 4.19	68.69 ± 7.93	86.66 ± 0.43	82.47 ± 0.97	88.86 ± 1.56
78.82 ± 5.38	72.18 ± 3.47	72.59 ± 3.19	88.60 ± 0.50	82.53 ± 0.74	90.41 ± 0.35
82.07 ± 2.03	74.21 ± 2.90	76.89 ± 3.27	91.09 ± 0.35	82.94 ± 1.54	91.95 ± 0.11
	1	47.86 ± 15.38	50.31 ± 14.27	58.84 ± 12.81	51.13 ± 5.24	37.14 ± 7.81	73.58 ± 8.15
	2	58.30 ± 13.55	55.55 ± 9.19	60.24 ± 14.44	63.12 ± 6.09	65.07 ± 8.86	76.89 ± 4.89
GAT [54]	5	71.04 ± 5.74	67.37 ± 5.08	68.54 ± 5.75	71.65 ± 4.53	71.43 ± 7.34	83.01 ± 3.64
	10	76.31 ± 4.87	71.35 ± 4.92	72.44 ± 3.50	74.71 ± 3.35	76.04 ± 0.35	87.42 ± 2.38
	20	79.92 ± 2.28	73.22 ± 2.90	75.55 ± 4.11	79.95 ± 2.88	80.05 ± 1.81	89.38 ± 2.48
	1	43.04 ± 14.01	48.81 ± 11.45	55.53 ± 12.71	61.35 ± 1.35	27.65 ± 2.39	45.36 ± 7.13
	2	53.96 ± 12.18	54.39 ± 11.37	58.97 ± 12.65	76.51 ± 1.31	42.63 ± 4.29	51.93 ± 4.21
GraphSage	5	68.14 ± 6.95	64.79 ± 5.16	66.07 ± 6.16	89.06 ± 0.69	64.83 ± 1.62	78.26 ± 1.93
[29]	10	75.04 ± 5.03	68.90 ± 5.08	70.74 ± 3.11	89.68 ± 0.39	74.66 ± 1.29	84.38 ± 1.75
	20	80.04 ± 2.54	72.02 ± 2.82	74.55 ± 3.09	91.33 ± 0.36	79.98 ± 0.96	91.29 ± 0.67
	1	47.72 ± 15.53	39.13 ± 11.37	56.47 ± 4.67	58.99 ± 5.17	23.78 ± 7.57	34.72 ± 8.18
MoNet	2	60.85 ± 14.01	48.52 ± 9.52	61.03 ± 6.93	76.57 ± 4.06	38.19 ± 3.72	43.03 ± 8.22
[40]	5	73.86 ± 7.97	61.66 ± 6.61	67.92 ± 2.50	87.02 ± 1.67	59.38 ± 4.73	71.80 ± 5.02
	10	78.82 ± 5.38	68.08 ± 6.29	71.24 ± 1.54	88.76 ± 0.49	68.66 ± 3.30	78.66 ± 3.17
	20	82.07 ± 2.03	71.52 ± 4.11	76.49 ± 1.75	90.31 ± 0.41	73.66 ± 2.87	88.61 ± 1.18
Table 1: Classification accuracy of different GNNs trained with different number of labeled data per class
(#per class) on six benchmark graph node classification tasks. The highest accuracy is highlighted in bold for
each number of labeled data per class. These results show that GRAND++ is more effective in learning with
low-labeling rates than GRAND. (Unit: %)
GRAND-nl-rw with the corresponding model with a source term. We see that overall GRAND++-nl
(GRAND++-nl-rw) outperforms GRAND-nl (GRAND-nl-rw) when the network is deep, i.e., T is
big. We further study the low-labeling rate regimes in Appendix D.3.
Model	Depth (T)	GRAND-nl [10]	GRAND-nl-rw [10]	GRAND++-nl (ours)	GRAND++-nl-rw (ours)
CORA	1	79.70 ± 1.88	79.07 ± 3.05	79.24 ± 1.48	79.24 ± 1.48
	4	82.31 ± 0.91	82.47 ± 1.32	82.64 ± 0.89	82.23 ± 1.14
	16	82.11 ± 1.42	82.05 ± 1.31	83.24 ± 0.20	81.48 ± 1.07
	32	79.42 ± 0.64	81.01 ± 0.81	81.21 ± 0.37	82.20 ± 1.15
CiteSeer	1	71.84 ± 2.98	71.84 ± 2.66	70.45 ± 2.12	71.74 ± 1.37
	16	72.65 ± 2.42	73.06 ± 2.98	72.48 ± 1.10	73.29 ± 1.37
	64	70.29 ± 2.58	69.65 ± 2.50	72.64 ± 0.93	73.38 ± 0.95
	128	65.19 ± 6.77	65.45 ± 7.18	74.24 ± 0.70	74.23 ± 0.70
PubMed	1	77.93 ± 1.27	77.93 ± 1.26	78.01 ± 0.68	78.01 ± 0.68
	4	77.95 ± 1.28	78.02 ± 1.14	78.41 ± 0.88	78.17 ± 0.93
	16	76.51 ± 2.73	76.88 ± 2.57	78.43 ± 0.78	78.12 ± 0.87
Table 2: Classification accuracy of GRAND and GRAND++ variants of different depth trained 20 labeled data
per class. The highest accuracy is highlighted in bold for each of the depths T = 1, 4, 16, 32, 64, and 128. We
test T only up to 16 for PubMed and up to 32 for 32 since the neural ODE solver failed for larger T. (Unit: %)
7	Concluding Remarks
We propose GRAND++, which augments graph neural diffusion with a source term. We present
some theory that connects the model to a random walk formulation on graphs. GRAND++ out-
performs many existing GNNs for graph deep learning with very deep architectures and when the
number of labeled data is limited. GRAND++ can be regarded as coupled ODE system in which
each ODE has an external force term. As such, it is natural to consider if advanced techniques in
accelerating training, test, and inference of neural ODEs can be leveraged to improve the efficiency
and accuracy of GRAND++, in particular high-order neural ODEs [19, 61, 42, 59] and noise injec-
tion [55]. It is interesting to note that the second-order neural ODE can be connected to the wave
equation in the graph setting, which can automatically bypass over-smoothing. We leave studying
the second-order neural ODE on graphs as future work.
9
Published as a conference paper at ICLR 2022
8	Acknowledgement
This material is based on research sponsored by NSF grants DMS-1924935, DMS-1952339, DMS-
2027248 and NSF CCF-1934568, DOE grant DE-SC0021142, and ONR grant N00014-18-1-2527
and the MURI grant N00014-20-1-2787. MT would like to thank the Isaac Newton Institute for
Mathematical Sciences for support and hospitality during the programme Mathematics of Deep
Learning when work on this paper was undertaken (EPSRC grant number EP/R014604/1) and ac-
knowledge support from the European Union Horizon 2020 research and innovation programmes
under the Marie SkIodoWska-CUrie grant agreement No. 777826 (NoMADS). MT also holds a TUr-
ing Fellowship at the Alan Turing Institute.
References
[1]	Uri Alon and Eran Yahav. On the bottleneck of graph neUral netWorks and its practical impli-
cations. In International Conference on Learning Representations, 2021.
[2]	James AtWood and Don ToWsley. DiffUsion-convolUtional neUral netWorks. In Advances in
neural information processing systems, pages 1993-2001, 2016.
[3]	Peter Battaglia, Razvan PascanU, MattheW Lai, Danilo Jimenez Rezende, et al. Interaction
netWorks for learning aboUt objects, relations and physics. In Advances in Neural Information
Processing Systems, pages 4502-4510, 2016.
[4]	Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality redUction and data
representation. Neural Computation, 15(6):1373-1396, 2003.
[5]	Mikhail Belkin and Partha Niyogi. Semi-sUpervised learning on Riemannian manifolds. Ma-
chine learning, 56(1-3):209-239, 2004.
[6]	Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar VeliCkovic. Geometric deep learn-
ing: Grids, groUps, graphs, geodesics, and gaUges. arXiv preprint arXiv:2104.13478, 2021.
[7]	Antoni Buades, Bartomeu Coll, and Jean M. Morel. Neighborhood filters and PDE’s. Nu-
merische Mathematik, 105(1):1-34, 2006.
[8]	Jeff Calder. The game theoretic p-Laplacian and semi-supervised learning With feW labels.
Nonlinearity, 32(1), 2018.
[9]	Jeff Calder, Brendan Cook, MattheW Thorpe, and Dejan Slepcev. Poisson learning: Graph
based semi-supervised learning at very low label rates. In Hal DaUme In and Aarti Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume 119
of Proceedings of Machine Learning Research, pages 1306-1316. PMLR, 13-18 Jul 2020.
[10]	Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb,
and Emanuele Rossi. GRAND: Graph neural diffusion. In Marina Meila and Tong Zhang,
editors, Proceedings of the 38th International Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pages 1407-1418. PMLR, 18-24 Jul 2021.
[11]	Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional
neural networks. Advances in neural information processing systems, 32:4868-4879, 2019.
[12]	Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the
over-smoothing problem for graph neural networks from the topological view. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 34, pages 3438-3445, 2020.
[13]	Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-
ran Associates, Inc., 2018.
[14]	Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Math-
ematical Soc., 1997.
10
Published as a conference paper at ICLR 2022
[15]	Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick
Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and struc-
ture definition of data: Diffusion maps. Proceedings of the National Academy of Sciences,
102(21):7426-7431,2005.
[16]	Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. Advances in neural information processing
systems, 29:3844-3852, 2016.
[17]	Xavier Desquesnes, Abderrahim Elmoataz, and Lezoray, Olivier. Eikonal equation adaptation
on weighted graphs: fast geometric diffusion process for local and non-local image and data
processing. Journal of Mathematical Imaging and Vision, 46(2):238-257, 2013.
[18]	John R Dormand and Peter J Prince. A family of embedded Runge-Kutta formulae. Journal
of Computational and Applied Mathematics, 6(1):19-26, 1980.
[19]	Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alche—Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
[20]	David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
molecular fingerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015.
[21]	Moshe Eliasof, Eldad Haber, and Eran Treister. PDE-GCN: Novel architectures for graph
neural networks motivated by partial differential equations. arXiv preprint arXiv:2108.01938,
2021.
[22]	Abderrahim Elmoataz, Olivier Lezoray, and SebaStien Bougleux. Nonlocal discrete regulariza-
tion on weighted graphs: A framework for image and manifold processing. IEEE Transactions
on Image Processing, 17(7):1047-1060, 2008.
[23]	Francesco Farina and Emma Slade. Data efficiency in graph networks through equivariance.
arXiv preprint arXiv:2106.13786, 2021.
[24]	Mark Freidlin and Shuenn-Jyi Sheu. Diffusion processes on graphs: stochastic differential
equations, large deviation principle. Probability theory and related fields, 116(2):181-220,
2000.
[25]	Mark I. Freidlin and Alexander D. Wentzell. Diffusion Processes on Graphs and the Averaging
Principle. The Annals of Probability, 21(4):2215 - 2245, 1993.
[26]	Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, and Al-
lon G. Percus. Multiclass data segmentation using diffuse interface methods on graphs. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 36(8):1600-1613, 2014.
[27]	Guy Gilboa and Stanley Osher. Nonlocal operators with applications to image processing.
Multiscale Modeling & Simulation, 7(3):1005-1028, 2008.
[28]	Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, edi-
tors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages 1263-1272. PMLR, 06-11 Aug 2017.
[29]	Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.
[30]	Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In Proceedings of the 5th International Conference on Learning Representations,
ICLR ’17, 2017.
11
Published as a conference paper at ICLR 2022
[31]	Johannes Klicpera, Stefan WeiBenberger, and StePhan Gunnemann. Diffusion improves graph
learning. Advances in Neural Information Processing Systems, 32:13354-13366, 2019.
[32]	Guohao Li, Matthias Muller, Bernard Ghanem, and Vladlen Koltun. Training graph neural
networks with 1000 layers. arXiv preprint arXiv:2106.07476, 2021.
[33]	Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as
deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 9267-9276, 2019.
[34]	Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. arXiv preprint arXiv:2006.07739, 2020.
[35]	Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional net-
works for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelli-
gence, 2018.
[36]	Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. Lanczosnet: Multi-scale
deep graph convolutional networks. In International Conference on Learning Representations,
2019.
[37]	Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. Advances
in Neural Information Processing Systems, 32:8230-8241, 2019.
[38]	Francois Lozes, Abderrahim Elmoataz, and Olivier Lezoray. Partial difference operators on
weighted graphs for image processing on surfaces and point clouds. IEEE Transactions on
Image Processing, 23(9):3896-3909, 2014.
[39]	Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dis-
secting neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 3952-3963.
Curran Associates, Inc., 2020.
[40]	Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and
Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model
cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
5425-5434, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.
[41]	Federico Monti, Michael M Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 3700-3710, 2017.
[42]	Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Lio. On sec-
ond order behaviour in augmented neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 5911-5921. Curran Associates, Inc., 2020.
[43]	Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass
filters. arXiv preprint arXiv:1905.09550, 2019.
[44]	Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classification. In International Conference on Learning Representations, 2020.
[45]	Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and
Jinkyoo Park. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532,
2019.
[46]	Lev Semenovich Pontryagin. Mathematical theory of optimal processes. CRC press, 1987.
[47]	Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf:
Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pages 2110-2119, 2018.
12
Published as a conference paper at ICLR 2022
[48]	Mauricio Flores Rios, Jeff Calder, and Gilad Lerman. Algorithms for lp-based semi-supervised
learning on graphs. arXiv preprint arXiv:1901.05031, 2019.
[49]	Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep
graph convolutional networks on node classification. In International Conference on Learning
Representations, 2020.
[50]	Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear
embedding. science, 290(5500):2323-2326, 2000.
[51]	Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. Latent ordinary differential
equations for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019.
[52]	Zuoqiang Shi, Stanley J. Osher, and Wei. Zhu. Weighted nonlocal Laplacian on interpolation
from sparse data. Journal of Scientific Computing, 73(2-3):1164-1177, 2017.
[53]	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[54]	Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. In International Conference on Learning Rep-
resentations, 2018.
[55]	Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the Feynman-
Kac formalism to improve natural and robust accuracies. In Advances in Neural Information
Processing Systems, pages 1655-1665, 2019.
[56]	Fei Wang, Changshui Zhang, Helen C Shen, and Jingdong Wang. Semi-supervised classifica-
tion using linear neighborhood propagation. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06), volume 1, pages 160-167. IEEE, 2006.
[57]	Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process
in linear graph convolutional networks. arXiv preprint arXiv:2102.10739, 2021.
[58]	Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In
Hal Daume In and Aarti Singh, editors, Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10432-
10441. PMLR, 13-18 Jul 2020.
[59]	Hedi Xia, Vai Suliafu, Hangjie Ji, Tan Minh Nguyen, Andrea Bertozzi, Stanley Osher, and
Bao Wang. Heavy ball neural ordinary differential equations. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Sys-
tems, 2021.
[60]	Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
International Conference on Machine Learning, pages 5453-5462. PMLR, 2018.
[61]	Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. ODE2VAE: deep generative second
order odes with Bayesian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32. Curran Associates, Inc., 2019.
[62]	Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, pages 974-983, 2018.
13
Published as a conference paper at ICLR 2022
[63]	Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Proceed-
ings of the 32nd International Conference on Neural Information Processing Systems, pages
5171-5181,2018.
[64]	Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling oversmoothing in GNNs. In Interna-
tional Conference on Learning Representations, 2020.
[65]	Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard SchOlkopf.
Learning with local and global consistency. In Advances in neural information processing
systems, pages 321-328, 2004.
[66]	Dengyong Zhou and Bernhard Scholkopf. Regularization on discrete spaces. In 27th DAGM
Conference on Pattern Recognition, pages 361-368, 2005.
[67]	Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaus-
sian fields and harmonic functions. In Proceedings of the 20th International Conference on
Machine learning, pages 912-919, 2003.
[68]	Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, and James S. Duncan. Ordinary differential
equations on graph networks. https://openreview.net/forum?id=SJg9z6VFDr, 2020.
14
Published as a conference paper at ICLR 2022
A Background on Graph Differential Operators
Let (X, W) represent a graph where X = ([x(1)]>, . . . , [x(n)]>)> ∈ Rn×d is the matrix where
each row x(i) ∈ Rd is a feature vector and W = (Wij)in,j=1 is a n × n matrix with Wij representing
the similarity (edge weight) between the ith and jth feature vector. We assume that we are dealing
with an undirected graph, i.e., Wij = Wji. A Rk1 -valued function on the nodes of the graph can
be represented as a matrix U ∈ Rn×k1 by U = ([u(1)]> , . . . , [u(n)]>)> and we define the inner
product
n
(U, Vi = X U⑴∙ V⑺.
i=1
Similarly, a Rk2 -valued function on the edges can be represented as a third-order tensor U ∈
Rn×n×k2 which we write as
/ U(1,1) …U(1,n) \
U =	... I
∖ U(n,1) …U(n,n) )
andU(i,j) ∈ Rk2. On edge functions we use the inner product
1n
(U, Vi = — X WijU(i,j) ∙ V(i,j).
2 i,j=1
Multiplication between a matrix and an edge function is usually defined pointwise and we use the
notation [A U](i,j) = AijU(i,j) ∈ Rk2, for a matrix A ∈ Rn×n and an edge function U ∈
Rn×n×k2 , to make this clear. Similarly, pointwise multiplication between two matrices A, B ∈
Rn×n is defined by [A B]ij = Aij Bij ∈ R. When a matrix is acting as a linear operator on a
node function we use the usual matrix-vector notation and write [AU](i) = Pjn=1 Aiju(j) ∈ Rk1
for a matrix A ∈ Rn×n and node function U = ([u(1)]>, . . . , [u(n)]>)> ∈ Rn×k1 . In the sequel
we will have k1 = k2 = d.
The gradient of a node-function U = ([u(1)]>, . . . , [u(n)]>)> ∈ Rn×d is defined as the edge-
function VU ∈ Rn×n×d With [VU](ij) = u(j) — u(i) ∈ Rd. The divergence divV =
([[divV](1)]>, . . . , [[divV](n)]>)> ∈ Rn×d of an edge-function V ∈ Rn×n×d is defined as
n
[divV](i) = XWijV(i,j)
j=1
for all i = 1, . . . , n. For anti-symmetric edge functions, i.e. V(i,j) = -V (j,i) for all i, j, We have
that the divergence is the negative adjoint to the gradient, i.e.
(divV, Ui = -(V, VUi.
B Technical Proofs
Proof: [Proof of Proposition 1] For notational convenience let us assume that x(i)(0) = x(i).
Clearly
E hB(i)(0)i = x(i) = x(i)(0)
for all i = 1, . . . , n. Assume that
E hB(i) (k)i =x(i)(δtk)
15
Published as a conference paper at ICLR 2022
for alli = 1,...,n.Then,
E [B⑴(k + 1)]
n
=X x(j)P(B(i)(k + 1) = x(j))
j=ι
nn
=XXXCj)P (B⑶(k) = X⑶|B⑴⑴=X⑶)P (B⑺⑴=X⑶)
=X X XCj) ((1 - δt) 1i=' + *) P(B⑺⑴=X('))
n	Cnn
=(1 - δt) X XCj)P (B⑶⑴=X(')) + δ X 用' X XCj)P(B(')(k) = XCj))
j=1	"i '=1 j=1
n
=(1 - δt)E 回)(川 + d X Wi'E 怛⑶(k)]
i '=1
n
=(1 - δt)X⑻(δtk) + δ X Wi'X0δtk)
di
i '=1
n
=XCi)(δtk) + dtXWi'Wi' (Xc')(δtk) -Xci)(δtk))
i '=1
x
(i)(δtk) - δt [LX(δtk)](i)
S(Mk + 1)),
=X
as required.
口
Proof: [Proof of Proposition 2] Let P = (Pij) ∈ Rn×n be the probability transition kernel, so
	p. . = ∫1 - δt if i=j, Pij = I δW ifi= j.
We have
n ∑∏iPij = i=1	=X ρ⅛ ((I- δt1i=j+δW) 二 dj(I - δt) + δt Pn=I Wij Pk = I dk	Pk=I dk 二	dj —P=^ 二πj,
as required.	口
Proof: [Proof of Proposition 3] The proof follows from a simple application of Propositions 1
and2. Namely, for any i ∈ {1,...,n}
nn
XCi)(kδt) = E[BCi)(k)] = X XCj)(0)P(BCi)(k) = XCj)) → X XCj)(0)∏j = X
j=ι	j=ι
as k → ∞.
口
Proof: [Proof of Proposition 4] Let
yCi)(k) = E
-X) 1B(j)Cs)=x⑸
16
Published as a conference paper at ICLR 2022
Notice that
一 k
E EIB⑸(s)=χ(i)
_s=0
k
XP (Bj)(S)= x(i))
s=0
P (Bj) (0) = X⑴)+ X P (Bj)(S) = X⑺
、--------V---------} s = 1
δij
k n
δij + XXP (Bj)(S) = X⑴IBj)(S - 1) = X⑶)P (Bj)(S - 1) = X⑶)
s=1'=1
kn
δj + XX (1 -δt)δ'i +
s=1'=1 '
δWi) P(Bj)(S- 1) = X⑶)
k	n τ^τ^ k
δj + (1 - δt) X P (Bj)(S- 1) = X⑴)+ δt X W X P (Bj)(S - 1) = X
s=1	'=1	' s=1
δj +
k-1
s=0
k-1
E1B(j)(s)=x(i)
_s=0
k-1
E1B(j)(s)=x(')
_s=0
δj + (1 - δt)E
From the definition of Y and the above recursive relationship We have
[
y⑶(k =屋 X (Xj)- x) E X iBj)(s)=χ(i)
k-1
E1B(j)(s)=χ(')
_s=0
k-1
E E1B(j)(s)=χ(')
_s=0
1 L / /.、	、	/.、	δ+ ʌ	，八
d X (Xj)- X)δj + (1 - δt)y(i)(k - 1) + d X W'iy(')(k - 1)
i j∈I '	i '=1
y(i)(k - 1) + X (Xj)- X)δij - δt [LY(k - 1)](i).
i j∈工
NoW We let
W⑴(k) = di (幺i)(kδt) - y⑴(k))
so that W satisfies
W⑶(k) = w(i)(k - 1) - δt [LW(k - 1)](i) = [PW(k - 1)](i).
Hence, W(k) = PkW(0). Since the stationary distribution of the random walk with transition
kernel is π, we have limk→∞ Pk = 1π>. Hence, as k → ∞, w(i)(k) → P；=1 ∏jWj)(0) = 0
since Wj) (0) = 0 by the choice in the initialisation of Z.	□
17
Published as a conference paper at ICLR 2022
Proof: [Proof of Proposition 5] We prove the result by contradiction. Assume that there exists Z
such that z(i)(kδt) → Z for all i = 1,...,n as k → ∞. Then LZ(kδt) → 0. Since We can write
z(i)(kδt) - z(j)(kδt) = z(i)((k - 1)δt) - z(j)((k - 1)δt)
-δt [LZ((k - 1)δt)](i) - [LZ((k - 1)δt)](j)
+ δt X (δi'(x⑴一X)- δj'(x⑶-x)),
'∈I
then taking the limit k → ∞ implies
X δi'(x(i) - X) = X δj'(x(j) - X)
'∈I	'∈I
for all i, j which is clearly not true.
C Neural ODEs and Training Neural ODEs with Adjoint Method
Neural ODEs [13] are a class of continuous-depth (-time) neural networks that are particu-
larly suitable for learning complex dynamics from irregularly sampled sequential data, see, e.g.,
[13, 51, 19, 39, 42]. Mathematically, a neural ODE is the first-order ODE:
粤"=f (h(t),t, θ),
dt
(18)
where f (h(t), t, θ) ∈ Rd is specified by a neural network parameterised by θ, e.g., a two-layer feed-
forward neural network. Starting from the input h(0), neural ODEs learn the representation and
perform prediction by solving (18) from t = 0 to T using a numerical integrator with a given error
tolerance, often with an adaptive step size solver (or adaptive solver for short) [18]. Solving (18)
from t = 0 to T in a single pass with an adaptive solver requires evaluating f (h(t), t, θ) at various
timesteps, with computational complexity counted by the number of forward function evaluations
(forward NFEs) [13].
The adjoint sensitivity method (or adjoint method) [46], is a memory-efficient method for training
neural ODEs. We regard the output h(T) as the prediction and denote the loss between h(T) and
the ground truth as L. Let a(t) := ∂L∕∂h(t) be the adjoint state, then we have (see [13, 46] for
details)
dθ = /'a 吗产 dt，
dθ 0	∂ θ
with a(t) satisfying the following adjoint ODE
da(t)	> ∂
F = -a(t) ∂hf(h⑴,t,θ),
(19)
(20)
which is solved numerically from t = T to 0 and also requires the evaluation of the right-hand side
of (20) at various timestamps, and the backward NFEs measure the computational complexity.
D Experimental Details and More Experimental Results
D. 1 Datasets and experimental settings
Graph node classification dataset. Following [10], we consider the largest connected component
of seven graph node classification datasets, including CORA, CiteSeer, PubMed, coauthor graph
CoauthorCS, and Amazon co-purchasing graphs Computer and Photo, and a large scale ogbn-arxiv
dataset. For completeness, we list the number of classes, the number of features, and the number of
nodes and edges of each dataset in Table 3. More detailed information can be found in [10].
Depth of GRAND and GRAND++ for the results in Table 1. Table 4 lists the fine-tuned T for
the results in Table 1. Due to the limited time, we only search around the value of optimal T for
GRAND with grid spacing 0.1.
18
Published as a conference paper at ICLR 2022
Dataset	Classes	Features	#Nodes	#Edges
CORA	7	1433	2485	5069
CiteSeer	6	3703	2120	3679
PubMed	3	500	19717	44324
CoauthorCS	15	6805	18333	81894
Computer	10	767	13381	245778
Photo	8	745	7487	119043
ogbn-arxiv	40	128	169343	1166243
Table 3: Summary of the graph node classification datasets.
Model	CORA	CiteSeer	PubMed	CoauthorCS	Computer	Photo
GRAND++-l	18.3	8.0	13.0	4.0	3.2	3.6
GRAND-l	18.2948	7.8741	12.9423	3.2490	3.5824	3.6760
Table 4: The value of the fine-tuned T, i.e. depth of the continuous-depth GNNs, for GRAND and GRAND++
in learning with different labeling results, and the corresponding accuracy are reported in Table 1. The values
of T for GRAND++ are adopted from the paper [10].
D.2 Classification accuracy of GNNs with different depths
In this subsection, we provide detailed numbers that correspond to Fig. 1. We further compare
GRAND++-l with several other GNN architectures, including GCN, GAT, and GraphSage, with
different depths on a few benchmark datasets. Table 5 lists the classification accuracy of GRAND++,
GRAND, and three benchmark GNNs with different depths on six graph node classification tasks.
Again, we see that GRAND++ is better than the other GNN models when the networks are deep.
Model	depth	CORA	CiteSeer	PubMed	CoauthorCS	Computer	Photo
	1	77.48 ± 1.43	71.23 ± 3.47	78.11 ± 1.47	90.42 ± 0.76	84.11 ± 0.51	92.93 ± 0.84
	4	81.98 ± 1.42	72.58 ± 3.79	79.20 ± 0.74	90.89 ± 0.36	84.19 ± 0.93	93.54 ± 0.38
	16	82.49 ± 1.37	73.84 ± 2.66	79.49 ± 0.84	90.24 ± 0.30	78.97 ± 2.33	92.69 ± 0.61
GRAND++-l	32	82.48 ± 0.71	73.29 ± 1.29	79.81 ± 1.61	NA	76.01 ± 1.33	92.94 ± 0.90
(ours)	64	80.99 ± 1.76	72.81 ± 2.18	NA	NA	NA	NA
	128	80.29 ± 1.98	NA	NA	NA	NA	NA
	256	79.04 ± 2.94	NA	NA	NA	NA	NA
	1	78.59 ± 1.17	71.96 ± 2.74	77.93 ± 1.26	90.79 ± 0.93	83.41 ± 0.69	92.66 ± 0.42
	4	82.80 ± 1.62	73.87 ± 2.12	78.71 ± 1.19	90.94 ± 0.21	84.23 ± 1.05	92.47 ± 0.53
	16	82.75 ± 1.17	72.61 ± 2.42	78.79 ± 0.93	87.66 ± 1.70	77.67 ± 1.94	92.37 ± 0.27
GRAND-l	32	82.19 ± 1.73	72.65 ± 3.15	78.70 ± 1.08	NA	69.56 ± 2.20	89.61 ± 1.33
[10]	64	80.87 ± 2.28	69.84 ± 2.66	NA	NA	NA	NA
	128	77.22 ± 2.88	NA	NA	NA	NA	NA
	256	67.79 ± 3.10	NA	NA	NA	NA	NA
	1	76.92 ± 0.56	72.80 ± 1.69	72.78 ± 1.80	91.53 ± 0.45	81.44 ± 0.24	91.31 ± 0.19
GCN	4	81.35 ± 1.27	70.54 ± 6.61	77.15 ± 3.00	87.84 ± 0.96	75.73 ± 1.02	90.11 ± 0.66
[30]	16	19.70 ± 7.06	24.78 ± 1.45	41.36 ± 1.77	14.49 ± 0.91	12.86 ± 2.39	23.11 ± 1.76
	32	21.86 ± 6.09	24.23 ± 1.65	40.66 ± 1.86	12.14 ± 1.64	21.15 ± 13.10	24.30 ± 0.73
	1	72.49 ± 2.03	71.83 ± 1.53	77.24 ± 0.72	79.22 ± 0.60	73.97 ± 1.20	87.08 ± 0.37
GAT	4	80.95 ± 2.28	72.31 ± 2.82	77.37 ± 1.32	78.05 ± 1.10	76.67 ± 2.79	87.95 ± 1.76
[54]	16	29.14 ± 1.02	24.84 ± 1.45	39.21 ± 0.43	24.20 ± 2.22	37.07 ± 2.99	29.97 ± 3.68
	32	29.75 ± 1.57	24.83 ± 1.45	39.02 ± 0.12	22.73 ± 2.08	32.53 ± 3.09	25.57 ± 4.03
	1	73.47 ± 1.98	71.94 ± 1.45	72.42 ± 0.61	91.74 ± 0.26	75.95 ± 0.70	88.10 ± 0.87
GraphSage	4	79.83 ± 2.43	50.00 ± 14.27	76.01 ± 2.35	87.94 ± 0.23	75.62 ± 2.85	90.68 ± 2.11
[29]	16	25.52 ± 6.45	24.84 ± 1.45	37.55 ± 3.92	10.12 ± 2.21	22.79 ± 10.77	25.57 ± 3.31
	32	29.14 ± 1.02	28.38 ± 2.54	39.21 ± 4.39	7.91 ± 3.15	37.07 ± 13.22	20.09 ± 5.67
Table 5: Classification accuracy of different GNN models with different depths on six benchmark graph node
classification tasks. NA: neural ODE solver failed. These results show that GRAND++ is better suited for
learning with a very deep architecture than GRAND. (Unit: %)
D.3 More results on time-dependent attention and graph rewiring
We further explore the effects of the source term for GRAND-nl and GRAND-nl-rw in the low-
labeling rate regimes. Table 6 compares GRAND-nl and GRAND-nl-rw with the corresponding
model with a source term. We see that GRAND-nl and GRAND-nl-rw are almost always worse than
the vanilla GRAND-l, consistent with the results reported in [10]. GRAND++-nl and GRAND++-
nl-rw cannot help learning at low labeling rates anymore. However, when the labeling rates are
not low, GRAND++-nl or GRAND++-nl-rw can outperform GRAND-nl and GRAND-nl-rw, even
outperform GRAND++.
19
Published as a conference paper at ICLR 2022
Model	#per class	GRAND-nl [10]	GRAND-nl-rw [10]	GRAND++-nl (ours)	GRAND++-nl-rw (ours)
	1	50.55 ± 15.68	50.63 ± 17.71	48.89 ± 11.51	47.94 ± 11.06
	2	65.06 ± 9.35	61.24 ± 16.19	59.96 ± 7.90	58.25 ± 11.97
CORA	5	76.93 ± 3.10	76.50 ± 3.91	74.01 ± 1.73	74.25 ± 1.99
	10	79.60 ± 2.69	79.38 ± 3.25	80.14 ± 0.69	80.18 ± 0.40
	20	82.22 ± 1.93	82.14 ± 2.49	83.24 ± 0.20	81.48 ± 1.07
	1	50.25 ± 17.66	50.20 ± 17.90	49.65 ± 5.45	53.10 ± 5.51
	2	59.87 ± 10.89	59.95 ± 10.48	59.16 ± 8.13	60.26 ± 5.10
CiteSeer	5	68.21 ± 5.08	68.05 ± 5.48	66.13 ± 2.09	67.81 ± 1.97
	10	71.88 ± 6.94	71.92 ± 7.34	68.84 ± 2.84	71.45 ± 1.64
	20	72.84 ± 6.61	72.72 ± 6.85	72.52 ± 1.24	73.87 ± 1.35
	1	66.97 ± 10.07	67.69 ± 7.89	63.85 ± 4.86	67.45 ± 3.88
	2	69.17 ± 2.46	69.42 ± 2.13	66.98 ± 5.30	69.11 ± 1.80
PubMed	5	72.56 ± 3.36	72.68 ± 2.52	71.49 ± 1.53	72.05 ± 3.67
	10	76.03 ± 3.73	75.32 ± 3.45	74.94 ± 2.15	75.09 ± 2.88
	20	78.55 ± 1.59	78.30 ± 1.43	78.41 ± 0.99	79.44 ± 0.56
Table 6: Classification accuracy of the variants of GRAND and GRAND++ models trained with different
numbers of labeled data per class (#per class) on graph node classification tasks. (Unit: %)
D.4 Classification accuracy of GNNs with fewer number of training data
Besides the results shown in Fig. 1, we further test the classification accuracy of more benchmark
GNN architectures trained with fewer numbers of labeled data per class. Tables 7-9 list the classi-
fication accuracy, on the test set, of different benchmark GNN models when they are trained with
different numbers of labeled nodes per class.
#labeled nodes per class	1	2	5	10	20
GCN [30]	47.72 ± 15.53	60.85 ± 14.01	73.86 ± 7.97	78.82 ± 5.38	82.07 ± 2.03
GAT [54]	47.86 ± 15.38	58.30 ± 13.55	71.04 ± 5.74	76.31 ± 4.87	79.92 ± 2.28
GraphSage [29]	43.04 ± 14.01	53.96 ± 12.18	68.14 ± 6.95	75.04 ± 5.03	80.04 ± 2.54
MoNet [40]	47.72 ± 15.53	60.85 ± 14.01	73.86 ± 7.97	78.82 ± 5.38	82.07 ± 2.03
Lanczos [36]	47.41 ± 11.82	60.94 ± 4.00	74.28 ± 3.07	76.12 ± 0.93	79.85 ± 1.82
AdaLanczos [36]	48.23 ± 11.82	61.46 ± 4.96	74.24 ± 3.25	77.61 ± 1.36	81.03 ± 1.56
GCNN [58]	43.31 ± 11.95	60.28 ± 12.89	72.75 ± 4.21	78.92 ± 1.32	81.89 ± 1.12
GRAND-l [10]	52.53 ± 16.40	64.82 ± 11.16	76.07 ± 5.08	80.25 ± 3.40	82.86 ± 2.39
GRAND-nl [10]	40.97 ± 14.87	50.59 ± 13.25	65.13 ± 9.14	72.55 ± 6.65	77.76 ± 4.21
GRAND-nl-rw (gdc) [10]	52.68 ± 12.48	65.54 ± 10.01	74.94 ± 7.04	80.64 ± 6.19	82.47 ± 1.93
GRAND-nl-rw (two-hop) [10]	53.79 ± 17.72	64.50 ± 11.88	74.33 ± 6.28	79.61 ± 4.47	82.37 ± 1.98
Table 7: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CORA.
#labeled nodes per class	1	2	5	10	20
GCN [30]	48.94 ± 10.24	58.06 ± 9.76	67.24 ± 4.19	72.18 ± 3.47	74.21 ± 2.90
GAT [54]	50.31 ± 14.27	55.55 ± 9.19	67.37 ± 5.08	71.35 ± 4.92	73.22 ± 2.90
GraphSage [29]	48.81 ± 11.45	54.39 ± 11.37	64.79 ± 5.16	68.90 ± 5.08	72.02 ± 2.82
MoNet [40]	39.13 ± 11.37	48.52 ± 9.52	61.66 ± 6.61	68.08 ± 6.29	71.52 ± 4.11
Lanczos [36]	49.16 ± 3.63	57.65 ± 7.60	66.72 ± 9.38	71.01 ± 4.90	72.14 ± 2.00
AdaLanczos [36]	50.32 ± 7.42	58.35 ± 7.97	67.39 ± 8.20	72.15 ± 4.85	74.33 ± 2.83
GCNN [58]	40.58 ± 15.32	51.71 ± 13.87	63.16 ± 12.26	67.06 ± 5.65	69.84 ± 1.77
GRAND-l [10]	50.06 ± 17.98	59.55 ± 10.89	68.37 ± 5.00	71.90 ± 7.66	73.02 ± 5.89
GRAND-nl [10]	49.96 ± 18.62	59.57 ± 11.03	68.21 ± 7.08	71.88 ± 6.94	72.84 ± 6.61
GRAND-nl-rw (gdc) [10]	50.35 ± 17.74	59.98 ± 10.32	68.39 ± 5.81	71.83 ± 7.26	72.81 ± 6.94
GRAND-nl-rw (two-hop) [10]	50.20 ± 17.90	59.95 ± 10.48	68.05 ± 5.49	71.92 ± 7.34	72.72 ± 6.85
Table 8: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CiteSeer.
D.5 t-test of the accuracy improvement of GRAND++ over GRAND
To confirm the statistical significance of the accuracy improvement of GRAND++ over GRAND in
Table 1, in this subsection, we conduct t-test experiments at 0.95 confidence to compare GRAND and
GRAND++ on six different benchmark graph node classification tasks. We first perform unpaired
t-tests to show the improvement of GRAND++ over GRAND on low labeled datasets using the
following t-score
.	μGRAND++ - μGRAND
t-score =	二
./ σGRAND++ + σ2RAND++
VnTn
(21)
20
Published as a conference paper at ICLR 2022
#labeled nodes per class	1	2	5	10	20
GCN [30]	58.61 ± 12.83	60.45 ± 16.20	68.69 ± 7.93	72.59 ± 3.19	76.89 ± 3.27
GAT [54]	58.84 ± 12.81	60.24 ± 14.44	68.54 ± 5.75	72.44 ± 3.50	75.55 ± 4.11
GraphSage [29]	55.53 ± 12.71	58.97 ± 12.65	66.07 ± 6.16	70.74 ± 3.11	74.55 ± 3.09
MoNet [40]	56.47 ± 4.67	61.03 ± 6.93	67.92 ± 2.50	71.24 ± 1.54	76.49 ± 1.75
Lanczos [36]	60.12 ± 6.37	63.65 ± 6.97	70.61 ± 4.50	73.01 ± 3.27	78.35 ± 1.84
AdaLanczos [36]	61.07 ± 5.16	64.11 ± 6.88	69.05 ± 3.00	72.79 ± 2.74	78.10 ± 1.91
GCNN [58]	60.78 ± 20.64	65.14 ± 19.45	72.72 ± 10.82	76.47 ± 6.03	79.24 ± 3.45
GRAND-l [10]	62.11 ± 10.58	69.00 ± 7.55	73.98 ± 5.08	76.33 ± 3.41	78.76 ± 1.69
GRAND-nl [10]	61.75 ± 11.12	69.16 ± 8.46	72.35 ± 5.35	76.03 ± 3.72	78.55 ± 1.59
GRAND-nl-rw (gdc) [10]	61.70 ± 10.74	69.42 ± 8.21	72.39 ± 5.25	75.32 ± 3.45	78.30 ± 1.43
GRAND-nl-rw (two-hop) [10]	61.65 ± 12.09	68.49 ± 8.99	72.68 ± 5.92	75.72 ± 3.50	78.77 ± 1.88
Table 9: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: PubMed.
where μ and σ2 are the mean and variance of the performances of each model, and n is the number
of runs for each model. The t-test score are shown in Table 10.
#per class	CORA	CiteSeer	PubMed	CoauthorCS	Computer	Photo
1	1.05	4.36	3.28	1.95	111.60	45.33
2	1.39	3.96	0.34	4.59	7.18	4.65
5	2.55	2.68	-3.67	-1.96	15.67	0.26
Table 10: Unpaired t-test scores of GRAND++ v.s. GRAND on six different benchmark graph node classifica-
tion tasks. With n = 100, over 0.95 confidence is equivalent to exceed roughly 1.66 t-test scores. Highlighted
are the ones passing the test.
For some entries in Table 10 that are not significant enough, we further conduct paired t-test between
GRAND++ and GRAND on these specific datasets as shown in Table 11. Since a large portion of
variance comes from splitting of the datasets, we pair up tests of GRAND and GRAND++ with the
same splitting in this experiment. In this case, a sample of difference of size n is computed, and
t-test score can be computed using the equation
t-score
Mdif - 0
σdiff/√n
(22)
Dataset	#per class	Accuracy Difference	# splits	t-score	p-score
CORA	1	1.06 ± 6.24	100	1.80	0.044
CORA	2	1.45 ± 5.23	100	2.78	0.003
Table 11: Paired t-test scores of GRAND++ v.s. GRAND on datasets where unpaired t-test scores are not
significant enough.
D.6 Tasks for further evaluating deep GRAND and GRAND++
Open graph benchmark with paper citation network (ogbn-arxiv). Ogbn-arxiv consists of 169,
343 nodes and 1, 166, 243 directed edges. Each node is an arxiv paper represented by a 128-
dimensional features and each directed edge indicates the citation direction. This dataset is used
for node property prediction and has been a popular benchmark to test the advantage of deep graph
neural networks over shallow graph neural networks [34, 32]. Compared to the GRAND model used
in [10], we reduce the hidden dimension from 162 to 81 to fit the model into the GPU in our lab.
Model depth (T ) GRAND-l [10] GRAND++-l (ours) Improvement from GRAND++-l (ours)
OGBN-arXiv
14683264
68.50 ± 0.76	68.79 ± 0.35	0.29
69.53 ± 0.21	69.68 ± 0.38	0.15
69.46 ± 0.43	69.71 ± 0.24	0.25
69.44 ± 0.30	69.61 ± 0.28	0.17
67.44 ± 0.59	69.41 ± 0.53	1.97
63.47 ± 0.28	68.05 ± 0.73	4.58
55.95 ± 1.24	67.26 ± 0.61	11.31
Table 12: Classification accuracy of the linear GRAND and GRAND++ models trained with different depth
on the OGBN-arXiv graph node classification task. Compared to the GRAND model used in [10], we reduce
the hidden dimension from 162 to 81 to fit the model into the GPU in our lab. (Unit: %)
21