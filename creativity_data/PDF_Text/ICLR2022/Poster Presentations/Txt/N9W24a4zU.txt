Published as a conference paper at ICLR 2022
Steerable Partial Differential Operators for
Equivariant Neural Networks
Erik Jenner*
University of Amsterdam
erik@ejenner.com
Maurice Weiler
University of Amsterdam
m.weiler.ml@gmail.com
Ab stract
Recent work in equivariant deep learning bears strong similarities to physics. Fields
over a base space are fundamental entities in both subjects, as are equivariant maps
between these fields. In deep learning, however, these maps are usually defined by
convolutions with a kernel, whereas they are partial differential operators (PDOs) in
physics. Developing the theory of equivariant PDOs in the context of deep learning
could bring these subjects even closer together and lead to a stronger flow of ideas.
In this work, we derive a G-steerability constraint that completely characterizes
when a PDO between feature vector fields is equivariant, for arbitrary symmetry
groups G. We then fully solve this constraint for several important groups. We
use our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant maps
based on Schwartz distributions that unifies classical convolutions and differential
operators and gives insight about the relation between the two.
Figure 1: A vector field (left) can be mapped to a scalar field (right) by applying certain partial differential
operators (PDOs), such as the Laplacian of the divergence and the 2D curl. Such a PDO from a 2D vector to a
scalar field can be represented as a 2 × 1 matrix, where each of the two entries is a one-dimensional PDO that
acts on one of the two components of the vector field. Similarly, matrices of PDOs with different dimensions
map between other types of fields. Our goal is to find all PDOs for which this map becomes equivariant, for
arbitrary types of fields. For the implementation, we will later discretize PDOs as stencils (middle).
1	Introduction
In many machine learning tasks, the data exhibits certain symmetries, such as translation- and
sometimes rotation-invariance in image classification. To exploit those symmetries, equivariant
neural networks have been widely studied and successfully applied in the past years, beginning with
Group convolutional neural networks (Cohen & Welling, 2016; Weiler et al., 2018b). A significant
generalization of Group convolutional networks is given by steerable CNNs (Cohen & Welling, 2017;
Weiler et al., 2018a; Weiler & Cesa, 2019), which unify many different pre-existing equivariant
models (Weiler & Cesa, 2019). They do this by representing features as fields of feature vectors over
a base space, such as R2 in the case of two-dimensional images. Layers are then linear equivariant
maps between these fields.
This is very reminiscent of physics. There, fields are used to model particles and their interactions,
with physical space or spacetime as the base space. The maps between these fields are also equivariant,
with the symmetries being part of fundamental physical laws.
* Work done during an internship at QUVA Lab
1
Published as a conference paper at ICLR 2022
It is also noteworthy that these symmetries are largely ones that appear the most often in deep learning,
such as translation and rotation equivariance. These similarities have already led to ideas from physics
being applied in equivariant deep learning (Lang & Weiler, 2021).
However, one remaining difference is that physics uses equivariant partial differential operators
(PDOs) to define maps between fields, such as the gradient or Laplacian. Therefore, using PDOs
instead of convolutions in deep learning would complete the analogy to physics and could lead to
even more transfer of ideas between subjects.
Equivariant PDO-based networks have already been designed in prior work (Shen et al., 2020; Smets
et al., 2020; Sharp et al., 2020). Most relevant for our work are PDO-eConvs (Shen et al., 2020),
which can be seen as the PDO-analogon of group convolutions. However, PDO-eConvs are only one
instance of equivariant PDOs and do not cover the most common PDOs from physics, such as the
gradient, divergence, etc. Very similarly to how steerable CNNs (Cohen & Welling, 2017; Weiler
et al., 2018a; Weiler & Cesa, 2019) generalize group convolutions, we generalize PDO-eConvs by
characterizing the set of all translation equivariant PDOs between feature fields over Euclidean spaces.
Because of this analogy, we dub these equivariant differential operators steerable PDOs.
These steerable PDOs and their similarity to steerable CNNs also raise the question of how equivariant
PDOs and kernels relate to each other, and whether they can be unified. We present a framework for
equivariant maps that contains both steerable PDOs and convolutions with steerable kernels as special
cases. We then prove that this framework defines the most general set of translation equivariant, linear,
continuous maps between feature fields, complementing recent work (Aronsson, 2021) that describes
when equivariant maps are convolutions. Since formally developing this framework requires the
theory of Schwartz distributions, we cover it mainly in Appendix E, and the main paper can be read
without any knowledge of distributions. However, we reference the main results from this framework
in the paper where appropriate.
In order to make steerable PDOs practically applicable, we describe an approach to find complete
bases for vector spaces of equivariant PDOs and then apply this method to the most important cases.
We have also implemented steerable PDOs for all subgroups of O(2) (https://github.com/
ejnnr/steerable_pdos). Our code extends the E2CNN library1 (Weiler & Cesa, 2019), which
will allow practitioners to easily use both steerable kernels and steerable PDOs within the same
library, and even to combine both inside the same network. Finally, we test our approach empirically
by comparing steerable PDOs to steerable CNNs. In particular, we benchmark different discretization
methods for the numerical implementation.
In summary, our main contributions are as follows:
•	We develop the theory of equivariant PDOs on Euclidean spaces, giving a practical charac-
terization of precisely when a PDO is equivariant under any given symmetry.
•	We unify equivariant PDOs and kernels into one framework that provably contains all
translation equivariant, linear, continuous maps between feature spaces.
•	We describe a method for finding bases of the vector spaces of equivariant PDOs, and
provide explicit bases for many important cases.
•	We benchmark steerable PDOs using different discretization procedures and provide an
implementation of steerable PDOs as an extension of the E2CNN library.
1.1 Related work
Equivariant convolutional networks Our approach to equivariance follows the one taken by
steerable CNNs (Cohen & Welling, 2017; Weiler et al., 2018a; Weiler & Cesa, 2019; Brandstetter
et al., 2021). They represent each feature as a map from the base space, such as Rd, to a fiber Rc that
is equipped with a representation ρ of the point group G. Compared to vanilla CNNs, which have
fiber R, steerable CNNs thus extend the codomain of feature maps.
A different approach is taken by group convolutional networks (Cohen & Welling, 2016; Hoogeboom
et al., 2018; Weiler et al., 2018b). They represent each feature as a map from a group H acting on the
input space to R. Because the input to the network usually does not lie in H , this requires a lifting
1https://quva-lab.github.io/e2cnn/
2
Published as a conference paper at ICLR 2022
map from the input space to H . Compared to vanilla CNNs, group convolutional networks can thus
be understood as extending the domain of feature maps.
When H = Rd o G is the semidirect product of the translation group and a pointwise group G, then
group convolutions on H are equivalent to G-steerable convolutions with regular representations.
For finite G, the group convolution over G simply becomes a finite sum. LieConvs (Finzi et al.,
2020) describe a way of implementing group convolutions even for infinite groups by using a Monte
Carlo approximation for the convolution integral. Steerable CNNs with regular representations would
have to use similar approximations for infinite groups, but they can instead also use (non-regular)
finite-dimensional representations. Both the group convolutional and the steerable approach can be
applied to non-Euclidean input spaces—LieConvs define group convolutions on arbitrary Lie groups
and steerable convolutions can be defined on Riemannian manifolds (Cohen et al., 2019b; Weiler
et al., 2021) and homogeneous spaces (Cohen et al., 2019a).
One practical advantage of the group convolutional approach employed by LieConvs is that it doesn’t
require solving any equivariance constraints, which tends to make implementation of new groups
easier. They also require somewhat less heavy theoretical machinery. On the other hand, steerable
CNNs are much more general. This makes them interesting from a theoretical angle and also has
more practical advantages; for example, they can naturally represent the symmetries of vector field
input or output. Since our focus is developing the theory of equivariant PDOs and the connection to
physics, where vector fields are ubiquitous, we are taking the steerable perspective in this paper.
Equivariant PDO-based networks The work most closely related to ours are PDO-eConvs (Shen
et al., 2020), which apply the group convolutional perspective to PDOs. Unlike LieConvs, they are
not designed to work with infinite groups. The steerable PDOs we introduce generalizes PDO-eConvs,
which are obtained as a special case by using regular representations.
A different approach to equivariant PDO-based networks was taken by Smets et al. (2020). Instead of
applying a differential operator to input features, they use layers that map an initial condition for a
PDE to its solution at a fixed later time. The PDE has a fixed form but several learnable parameters
and constraints on these parameters—combined with the form of the PDE—guarantee equivariance.
Sharp et al. (2020) also use a PDE, namely the diffusion equation, as part of their DiffusionNet model,
which can learn on 3D surfaces. Interestingly, the time evolution operator for the diffusion equation is
exp(t∆), which can be interpreted as an infinite power series in the Laplacian, very reminiscent of the
finite Laplacian polynomials that naturally appear throughout this paper. Studying the equivariance of
such infinite series of PDOs might be an interesting direction for future work. We clarify the relation
between PDO-based and kernel-based networks in some more detail in Appendix F.
2 Steerable PDOs
In this section, we develop the theory of equiv-
ariant PDOs. We will represent all features
as smooth fields f : Rd → Rc that asso-
ciate a feature vector f (x) ∈ Rc, called the
fiber at x, with each point x ∈ Rd . We write
Fi = C∞(Rd, Rci) for the space of these fields
f in layer i. Additionally, we have a group of
transformations acting on the input space Rd ,
which describes under which symmetries we
want the PDOs to be equivariant. We will al-
ways use a group of the form H = (Rd, +) o G,
for some G ≤ GL(d, R). Here, (Rd, +) refers
to the group of translations of Rd , while G is
some group of linear invertible transformations.
Figure 2: Transformation of scalar and vector fields (re-
produced with permission from Weiler & Cesa (2019))
The full group of symmetries H is the semidirect product of these two, meaning that each element
h ∈ H can be uniquely written as h = tg, where t ∈ Rd is a translation and g ∈ G a linear
transformation. For example, if G = {e} is the trivial group, we consider only equivariance un-
der translations, as in classical CNNs, while for G = SO(d) we additionally consider rotational
equivariance.
3
Published as a conference paper at ICLR 2022
Each feature space Fi has an associated group representation ρi : G → GL(ci , R), which determines
how each fiber Rci transforms under transformations of the input space. Briefly, ρi associates an
invertible matrix ρi (g) to each group element g, such that ρi(g)ρi(g0) = ρi(gg0); more details on
representation theory can be found in Appendix B. To see why these representations are necessary,
consider the feature space F = C∞ (R2 , R2) and the group G = SO(2). The two channels could
simply be two independent scalar fields, meaning that rotations of the input move each fiber but
do not transform the fibers themselves. Formally, this would mean using trivial representations
ρ(g) = 1 for both channels. On the other hand, the two channels could together form a vector field,
which means that each fiber would need to itself be rotated in addition to being moved. This would
correspond to the representation ρ(g) = g. These two cases are visualized in Fig. 2.
In general, the transformation of a feature f ∈ Fi under an input transformation tg with t ∈ Rd and
g ∈ G is given by
((tg) Bi f)(x) := ρi(g)f (g-1(x -t)) .	(1)
The g-1(x - t) term moves each fiber spatially, whereas the ρi (g) is responsible for the individual
transformation of each fiber.
For a network, we will need maps between adjacent feature spaces Fi and Fi+1. Since during this
theory section, we only consider single layers in isolation, we will drop the index i and simply denote
the layer map as Φ : Fin → Fout. We are particularly interested in equivariant maps Φ, i.e. maps that
commute with the action of H on the feature spaces:
Φ(hBinf) = hBoutΦ(f) ∀h ∈ H,f ∈ Fin.	(2)
We call Φ translation-equivariant if Eq. (2) holds for h ∈ (Rd, +), i.e. for pure translations. Analo-
gously, Φ is G-equivariant if it holds for linear transformations h ∈ G. Because H is the semidirect
product of Rd and G, a map is H-equivariant if and only if it is both translation- and G-equivariant.
2.1	PDOs as maps between feature spaces
We want to use PDOs for the layer map Φ, so we need to introduce some notation for PDOs between
multi-dimensional feature fields. As shown in Fig. 1, such a multi-dimensional PDO can be interpreted
as a matrix of one-dimensional PDOs. Specifically, a PDO from C∞ (Rd, Rcin ) to C∞ (Rd, Rcout ) is
described by a cout × cin matrix. For example, the 2D divergence operator, which maps from R2 to
R = R1 can be written as the 1 × 2 matrix (∂1 ∂2). This is exactly analogous to convolutional
kernels, which can also be interpreted as cout × cin matrices of scalar-valued kernels.
To work with the one-dimensional PDOs that make up the entries of this matrix, we use multi-index
notation, so for a tuple α = (α1, . . . , αd) ∈ N0d, we write ∂α := ∂1α1 . . . ∂dαd. A general one-
dimensional PDO is a sum Pα cα∂α, where the coefficients cα are smooth functions cα : Rd → R
(so for now no spatial weight sharing is assumed). The sum ranges over all multi-indices α, but we
require all but a finite number of coefficients to be zero everywhere, so the sum is effectively finite.
As described, a PDO between general feature spaces is then a matrix of these one-dimensional PDOs.
2.2	Equivariance constraint for PDOs
We now derive a complete characterization of the PDOs that are H-equivariant in the sense defined by
Eq. (2). Because a map is equivariant under the full symmetries H = (Rd, +) o G if and only if it is
both translation equivariant and G-equivariant, we split up our treatment into these two requirements.
First, we note that translation equivariance corresponds to spatial weight sharing, just like in CNNs
(see Appendix G for the proof):
Proposition 1. A cout × cin -PDO Φ with matrix entries	α ciαj∂α is translation equivariant if and
only if all coefficients ciαj are constants, i.e. ciαj (x) = ciαj (x0) for all x, x0 ∈ Rd.
So from now on, we restrict our attention to PDOs with constant coefficients and ask under which
circumstances they are additionally equivariant under the action of the point group G ≤ GL(d, R).
To answer that, we make use of a duality between polynomials and PDOs that will appear throughout
this paper: for a PDO Pα cα∂α , with cα ∈ R,2 there is an associated polynomial simply given by
2Note that we had cα ∈ C∞ (Rd, R) before, but we now restrict ourselves to constant coefficients and
identify constant functions Rd → R with real numbers.
4
Published as a conference paper at ICLR 2022
α cαxα, where xα := x1α1 . . . xdαd. Conversely, for any polynomial p = α cαxα, we get a PDO
by formally plugging in ∂ = (∂1, . . . , ∂d) for x, yielding p(∂) := Pα cα∂α. We will denote the map
in this direction by D, so we write D(p) := p(∂).
We can extend this duality to PDOs between multi-dimensional fields: for a matrix P of polynomials,
we define D(P) component-wise, so D(P) will be a matrix of PDOs given by D(P)ij := D(Pij).
To avoid confusion, we will always denote polynomials by lowercase letters, such as p, q, and matrices
of polynomials by uppercase letters, like P and Q.
As a simple example of the correspondence between polynomials and PDOs, the Laplacian operator
∆ is given by ∆ = D(|x|2). The gradient (∂1, . . . , ∂d)T is induced by the d × 1 matrix (x1, . . . , xd)T
and the 3D curl (d = 3) is induced by the 3 × 3 matrix
0	-x3	x2
P =	x3	0	-x1
-x2 x1	0
0	-∂3	∂2
D(P) =	∂3	0	-∂1
-∂2	∂1	0
(3)
Finally, note that D is a ring isomorphism, i.e. a bijection with D(p + q) = D(p) + D(q) and
D(pq) = D(p) ◦ D(q), allowing us to switch viewpoints at will.
We are now ready to state our main result on the G-equivariance of PDOs:
Theorem 2. For any matrix of polynomials P, the differential operator D(P) is G-equivariant if
and only if it satisfies the PDO G-steerability constraint,
P ((g-1 )Tχ) = Pout(g)P(χ)ρin(g)-1 ∀g ∈ G,χ ∈ Rd.	(4)
P(x) just means that x is plugged into the polynomials in each entry of P, which results in a
real-valued matrix P(x) ∈ Rcout ×cin, and similarly for P (g-1)Tx . Appendix C provides more
intuition for the action of group elements on polynomials. Because D is an isomorphism, this result
completely characterizes when a PDO with constant coefficients is G-equivariant, and in conjunction
with Proposition 1, when any PDO between feature fields is H-equivariant. The proof of Theorem 2
can again be found in Appendix G.
To avoid confusion, we would like to point out that this description of PDOs as polynomials is only a
useful trick that lets us express certain operations more easily and will later let us connect steerable
PDOs to steerable kernels. Convolving with these polynomials is not a meaningful operation; they
only become useful when ∂ is plugged in for x.
2.3 Examples of equivariant PDOs
To build intuition, we explore the space of equivariant PDOs for two simple cases before covering
the general solution. Consider R2 as a base space with the symmetry group G = SO(2) and trivial
representations ρin and ρout, i.e. maps between two scalar fields. The equivariance condition then
becomes p(gx) = p(x), so the polynomial p has to be rotation invariant. This is the case if and only
if it can be written as a function of |x|2, i.e. p(x) = q(|x|2) for some q : R≥0 → R. Since we want p
to be a polynomial, q needs to be a polynomial in one variable. Because |x|2 = x12 + x22, we get the
PDO D(p) = q(∆), where ∆ := ∂12 + ∂22 is the Laplace operator. So the SO(2)-equivariant PDOs
between two scalar fields are exactly the polynomials in the Laplacian, such as 2∆2 + ∆ + 3.
As a second example, consider PDOs that map from a vector to a scalar field, still with SO(2) as
the symmetry group. There are two such PDOs that often occur in the natural sciences, namely
the divergence, div v := ∂1v1 + ∂2v2, and the 2D curl, curl2D v := ∂1v2 - ∂2v1. Both of these
are SO(2)-equivariant (see Appendix A). We get additional equivariant PDOs by composing with
equivariant scalar-to-scalar maps, i.e. polynomials in the Laplacian. Specifically, q(∆) ◦ div and
q(∆) ◦ curl2D are also equivariant vector-to-scalar PDOs, for any polynomial q. We will omit the ◦
from now on.
We show in Appendix A that these PDOs already span the complete space of SO(2)-equivariant
PDOs from vector to scalar fields. Explicitly, the equivariant PDOs in this setting are all of the form
q1(∆) div + q2(∆) curl2D for polynomials q1 and q2. One example of this is the PDO shown in
Fig. 1.
5
Published as a conference paper at ICLR 2022
In these examples, as well as in other simple cases (see Appendix A), the equivariant PDOs are all
combinations of well-known operators such as the divergence and Laplacian. That the notion of
equivariance can reproduce all the intuitively “natural” differential operators suggests that it captures
the right concept.
3	Bases for s paces of steerable PDOs
For the purposes of deep learning, we need to be able to learn steerable PDOs. To illustrate how to
achieve this, consider again SO(2)-equivariant PDOs mapping from vector to scalar field. We have
seen in Section 2.3 that they all have the form
q1(∆) div + q2(∆) curl2D .	(5)
In practice, we will need to limit the order of the PDOs we consider, so that we can discretize them.
For example, we could consider only polynomials q1 and q2 ofup to first order, i.e. q1(z) = c1z + c2
and q2(z) = c3z + c4. This leads to PDOs ofup to order three (since ∆ is a second order PDO and
div and curl2D are first order). The space of such equivariant PDOs is then
c1∆ div + c2 div + c3∆ curl2D + c4 curl2D ,	ci ∈ R .	(6)
We can now train the real-valued parameters c1, . . . , c4 and thereby learn arbitrary equivariant PDOs
of up to order three.
The general principle is that we need to find a basis of the real vector space of steerable PDOs; then
we learn weights for a linear combination of the basis elements, yielding arbitrary equivariant PDOs.
Different group representations are popular in equivariant deep learning practice; for example PDO-
eConvs and group convolutions correspond to so-called regular representations (see Appendix M) but
quotient representations have also been used very successfully (Weiler & Cesa, 2019). We therefore
want to find bases of steerable PDOs for arbitrary representations ρin and ρout. To do so, we make
use of the existing work on solving the closely related G-steerability constraint for kernels. In this
section, we will first give a brief overview of this kernel steerability constraint and then describe how
to transfer its solutions to ones of the PDO steerability constraint.
3.1	The steerability constraint for kernels
The kernel G-steerability constraint characterizes when convolution with a kernel κ : Rd → Rcout ×cin
is G-equivariant, like the PDO steerability constraint Eq. (4) does for PDOs. Namely,
κ(gx) = |detg∣-1ρout(g)κ(x)ρin(g)-1	∀g ∈ G	(7)
has to hold. This constraint was proven in (Weiler et al., 2018a) for orthogonal G and later in (Weiler
et al., 2021) for general G. It is very similar to the PDO steerability constraint: the only differences
are the determinant and the κ(gx) term on the LHS where we had (g-1)T instead of g.
An explanation for this similarity is that both constraints can be seen as special cases of a more
general steerability constraint for Schwartz distributions, which we derive in Appendix E. Schwartz
distributions are a generalization of classical functions and convolutions with Schwartz distributions
can represent both classical convolutions and PDOs. As we prove in Appendix E, such distributional
convolutions are the most general translation equivariant, continuous, linear maps between feature
spaces, strictly more general than either PDOs or classical kernels. We also show how steerable
PDOs can be interpreted as the Fourier transform of steerable kernels, which we use to explain the
remaining differences between the two steerability constraints.
But for the purposes of this section, we want to draw particular attention to the fact that for G ≤ O(d),
the two constraints become exactly identical: the determinant then becomes 1, and (g-1)T = g. So
for this case, which is by far the most practically important one, we will use existing solutions of the
kernel steerability constraint to find a complete basis of equivariant PDOs.
3.2	Transferring kernel solutions to steerable PDO bases
Solutions of the kernel steerability constraint have been published for subgroups of O(2) (Lang &
Weiler, 2021; Weiler & Cesa, 2019) and O(3) (Lang & Weiler, 2021; Weiler et al., 2018a), and they
6
Published as a conference paper at ICLR 2022
all use a basis of the form
{καβ(x):=Pα(∣x∣)χβ (x/|x|) | α ∈A,β ∈B} ,	(8)
where A and B are index sets for the radial and angular part and the 夕α SPan the entire space of radial
functions. The reason is that the steerability constraint Eq. (7) constrains only the angular part if
G ≤ O(d), because orthogonal groups preserve distances.
The angular functions χβ are only defined on the sphere Sd-1. But crucially, we show in Appendix J
that they can all be canonically extended to polynomials defined on Rd.3 4 Concretely, we define
Xe(x) := ∣χ∣lβXe(x/|x|), where Ie is chosen minimally such that Xe is a polynomial. What We
prove in Appendix J is that such an lβ always exists.
The radial part 夕α in the kernel basis consists of unrestricted radial functions. To get a basis for
the space of polynomial steerable kernels, it is enough to use only powers of |x|2 for the radial part.
Specifically, we show in Appendix H that
{pkβ := ∣x∣2kXe I k ∈ No,β ∈b}	(9)
is a basis for the space of polynomial steerable kernels. As a final step, we interpret each polynomial
as a PDO using the isomorphism D defined in Section 2.2, yielding a complete basis of the space
of steerable PDOs. The |x|2k terms become Laplacian powers ∆k, while in the examples from
Section 2.3, Xe corresponds to divergence and curl.
In Appendix I, we apply this procedure to subgroups of O(2) and O(3) to obtain concrete solutions.
4	Experiments
Implementation We developed the theory of steerable PDOs in a continuous setting, but for imple-
mentation the PDOs need to be discretized, just like steerable kernels. The method of discretization
is completely independent of the steerable PDO basis, so steerable PDOs can be combined with any
discretization procedure. We compare three methods, finite differences (FD), radial basis function
finite differences (RBF-FD) and Gaussian derivatives.
Finite differences are a generalization of the usual central difference approximation and are the
method used by PDO-eConvs (Shen et al., 2020). RBF-FD finds stencils by demanding that the
discretization should become exact when applied to radial basis functions placed on the stencil points.
Its advantage over FD is that it can be applied to structureless point clouds rather than only to regular
grids. Gaussian derivative stencils work by placing a Gaussian on the target point and then evaluating
its derivative on the stencil points. Like RBF-FD, this also works on point clouds, and the Gaussian
also has a slight smoothing effect, which is why this discretization is often used in Computer Vision.
Formal descriptions of all three discretization methods can be found in Appendix L.
In addition to discretization, the infinite basis of steerable PDOs or kernels needs to be restricted to a
finite subspace. For kernels, we use the bandlimiting filters by Weiler & Cesa (2019). For PDOs,
we limit the total derivative order to two for 3 × 3 stencils and to three for 5 × 5 stencils (except for
PDO-eConvs, where we use the original basis that limits the maximum order of partial derivatives).
Finally, steerable PDOs and steerable kernels only replace the convolutional layers in a classical
CNN. To achieve an equivariant network, all the other layers, such as nonlinearities or Batchnorm
also need to be equivariant. Weiler & Cesa (2019) discuss in details how this can be achieved for
various types of layers. In our experiments, we use exactly the same implementation they do. Care
also needs to be taken with biases in the PDO layers. Here, we again follow (Weiler & Cesa, 2019)
by adding a bias only to the trivial irreducible representations that make up ρout.
3This holds for the cases we discuss here, i.e. subgroups of O(2) and O(3). In higher dimensions, the
situation is more complicated, but for those the kernel solutions have not yet been worked out anyway.
4As in the original PDO-eConv paper (Shen et al., 2020). Note that their performance is better, which is
simply caused by their different architecture and hyperparameters.
7
Published as a conference paper at ICLR 2022
Rotated MNIST We first bench-
mark steerable PDOs on rotated
MNIST (Larochelle et al., 2007), which
consists of MNIST images that have
been rotated by different angles, with
12k train and 50k test images. Our
results can be found in Table 1. The
models with 5 × 5 stencils use an
architecture that Weiler & Cesa (2019)
used for steerable CNNs, with six
C16-equivariant layers followed by two
fully connected layers. The first column
gives the representation under which
the six equivariant layers transform
(see Appendix B for their definitions).
PDO-eConvs implicitly use regular
representations (see Appendix M),
but with a slightly different basis than
the one we present, so we test both
bases. We also tested models that are
D16-equivariant in their first layers
and C16-equivariant in their last one
but did not find any improvements,
see Appendix N. For the models with
3 × 3 stencils, we use eight instead of
six C16-equivariant layers, in order to
compensate for the smaller receptive
field and keep the parameter count
comparable. The remaining differences
between kernel and PDO parameter
counts come from the fact that the basis
restrictions necessarily work slightly
Table 1: MNIST-rot results. Test errors ± standard deviations
are averaged over six runs. Vanilla CNN is a solely translation
equivariant model (G = {e}) with the same general architecture.
See main text for details on the models.
Represen- tation	Method	StenCil	Error [%]	Params
	Vanilla	3 × 3	2.001 ± 0.030	
—				1.1M
	CNN	5×5	1.959 ± 0.055	
		3 × 3	0.741 ± 0.036	837K
	Kernels	5×5	0.683 ± 0.021	1.1M
		3×3	1.196 ± 0.062	837K
regular (our	FD	5×5	1.54 ± 0.32	941K
basis)	O TDTT ∏TΛ RBF-FD	3×3	1.313 ± 0.065	837K
		5×5	1.475 ± 0.020	941K
	(_O Iloo	3×3	0.795 ± 0.030	837K
	Gauss	5×5	0.750 ± 0.017	941K
regular	FD4	5 X 5	1.98 ± 0.11	
(PDO-				982K
eConv)	Gauss	5×5	0.831 ± 0.039	
		3 × 3	0.717 ± 0.026	877K
	Kernels	5×5	0.670 ± 0.011	1.1M
		3×3	1.143 ± 0.063	877K
quotient (our	FD	5×5	1.347 ± 0.026	951K
basis)	RR口 ∏ΓΛ RBF-FD	3×3	1.303 ± 0.077	877K
		5×5	1.422 ± 0.040	951K
	(_O Iloo	3×3	0.825 ± 0.053	877K
	Gauss	5×5	0.744 ± 0.040	951K
differently (via bandlimiting filters or derivative order restriction respectively). All models were
trained with 30 epochs and hyperparameters based on those by Weiler & Cesa (2019), though we
changed the learning rate schedule and regularization slightly because this improved performance
for all models, including kernel-based ones. The training data is augmented with random rotations.
Precise descriptions of the architecture and hyperparameters can be found in Appendix O.
STL-10 The rotated MNIST dataset has
global rotational symmetry by design, so it is un-
surprising that equivariant models perform well.
But interestingly, rotation equivariance can also
help for natural images without global rotational
symmetry (Weiler & Cesa, 2019; Shen et al.,
2020). We therefore benchmark steerable PDOs
on STL-10 (Coates et al., 2011), where we only
use the labeled portion of 5000 training images.
The results are shown in Table 2. The model ar-
chitecture and hyperparameters are exactly the
same as in (Weiler & Cesa, 2019), namely a
Wide-ResNet-16-8 trained for 1000 epochs with
random crops, horizontal flips and Cutout (De-
Vries & Taylor, 2017) as data augmentation.
The group column describes the equivariance
group in each of the three residual blocks. For
Table 2: STL-10 results, again over six runs. All models
except the vanilla CNN use regular representations, see
main text for details.
Method	Groups	Error [%]	Params
Vanilla CNN	—	12.7 ± 0.2	11M
Kernels	D8D4D1 D4D4D1	10.7 ± 0.6 10.2 ± 0.4	4.2M
FD	D8D4D1	12.1 ± 0.6	
	D4D4D1	12.1 ± 0.7	
RBF-FD	。8 D4D1	14.3 ± 0.4	3.2M
	D4D4D1	14.3 ± 0.4	
Gauss	。8 D4D1	11.2 ± 0.3	
	D4D4D1	10.6 ± 0.8	
example, D8D4D1 means that the first block is equivariant under reflections and 8 rotations, the
second under 4 rotations and the last one only under reflections. All layers use regular representations.
The D8-equivariant layers use 5 × 5 filters to improve equivariance, whereas the other layers use
3 × 3 filters.
8
Published as a conference paper at ICLR 2022
Fluid flow prediction In the previously described tasks, the input and output representations are
all trivial. To showcase the use of non-trivial output representations, we predict laminar fluid flow
around various objects, following Ribeiro et al. (2020). In this case, the network outputs a vector
field, which behaves differently under rotations than scalar outputs, and whose equivariance cannot
be represented using PDO-eConvs, since they only implement trivial and regular representations. We
find that equivariance significantly improves performance for both kernels and PDOs, compared to
vanilla non-equivariant versions. See Appendix N for details.
Equivariance errors In the continuum, steerable CNNs and steerable PDOs are both exactly
equivariant. But the discretization on a square grid leads to unavoidable equivariance errors for
rotations that aren,t multiples of ∏2. The violation of equivariance in practice is thus closely connected
to the discretization error. For finite differences, the discretization error is particularly easy to bound
asymptotically, and as pointed out by Shen et al. (2020), this places the same asymptotic bound on
the equivariance error. However, our experiments show that empirically, finite differences don’t lead
to a particularly low equivariance error (kernels and all PDO discretizations perform similarly). See
Table 4 in Appendix N for details.
Locality of PDOs While all equivariant models improve significantly over the non-equivariant
CNN, the method of discretization plays an important role for PDOs. The reason that FD and RBF-FD
underperform kernels is that they don’t make full use of the stencil, since PDOs are inherently local
operators. When a 5 × 5 stencil is used, the outermost entries are all very small compared to the
inner ones, and even in 3 × 3 kernels, the four corners tend to be closer to zero (see Appendix N for
images of stencils to illustrate this). Gaussian discretization performs significantly better and almost
as well as kernels because its smoothing effect alleviates these issues. This fits the observation that
kernels and Gaussian methods profit from using 5 × 5 kernels, whereas these do not help for FD and
RBF-FD (and in fact decrease performance because of the smaller number of layers).
5	Conclusion
We have described a general framework for equivariant PDOs acting on feature fields over Euclidean
space. With this framework, we found strong similarities between equivariant PDOs and equivariant
convolutions, even unifying the two using convolutions with Schwartz distributions. We exploited
these similarities to find bases for equivariant PDOs based on existing solutions for steerable kernels.
Our experiments show that the locality of PDOs can be a disadvantage compared to convolutional ker-
nels. However, our approach for equivariance can easily be combined with any discretization method,
and we show that using Gaussian derivatives for discretization alleviates the issue. Equivariant PDOs
could also be very useful in cases where their strong locality is a desideratum rather than a drawback.
The theory developed in this work provides the necessary foundation for applications where equivari-
ant PDOs, rather than kernels, are needed. For example, Probabilistic Numerical CNNs (Finzi et al.,
2021) use PDOs in order to parameterize convolutions on continuous input data. Finzi et al. also
derive a constraint to make these PDOs equivariant, which is a special case of our PDO G-steerability
constraint Eq. (4). The solutions to this constraint presented in this paper are the missing piece for
implementing and empirically evaluating Probabilistic Numerical CNNS — neither of which Finzi
et al. do.
Another promising application of PDOs is an extension to manifolds. Gauge CNNs (Cohen et al.,
2019b; Kicanaoglu et al., 2019; Haan et al., 2021; Weiler et al., 2021) are a rather general framework
for convolutions on manifolds; see Weiler et al. (2021) for a thorough treatment and literature
review. As in the Euclidean case, Gauge CNNs use feature fields that transform according to some
representation ρ. The kernels are defined on the tangent space and are still constrained by the
G-steerability constraint. Because of that, our approach to equivariant Euclidean PDOs is very
well-suited for generalization to manifolds and our steerable PDO solutions will still remain valid.
One advantage of PDOs in this setting is that they require no Riemannian structure and can achieve
equivariance with respect to arbitrary diffeomorphisms, as is common in physics, instead of mere
isometry equivariance.
9
Published as a conference paper at ICLR 2022
Reproducibility Statement
All Propositions and Theorems in the main paper and in the appendix explicitly state all their
assumptions. In cases where theoretical results are stated only informally in the main paper (such as
our results involving Schwartz distributions), the precise claims can be found in the appendix. The
appendix also contains complete proofs for all of our theoretical claims.
The code necessary to reproduce our experiments can be found at https://github.com/
ejnnr/steerable_pdo_experiments. The required datasets are downloaded and prepro-
cessed automatically. The exact hyperparameters we used are available as pre-defined configuration
options in our scripts. We also include a version lockfile for installing precisely the right versions of
all required Python packages. Our implementation of steerable PDOs is easy to adapt to different use
cases and fully documented, allowing other practitioners to test the method on different datasets or
tasks.
Acknowledgments
We would like to thank Gabriele Cesa and Leon Lang for discussions on integrating steerable PDOs
into the E2CNN library and on solutions of the kernel G-steerability constraint. This work was
supported by funding from QUVA Lab.
References
Jimmy Aronsson. Homogeneous vector bundles and G-equivariant convolutional neural networks.
arXiv:2105.05400 [cs, math, stat], 2021.
Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik Bekkers, and Max Welling. Geometric and
physical quantities improve E(3) equivariant message passing, 2021.
Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single-Layer Networks in Unsupervised Feature
Learning. International Conference on Artificial Intelligence and Statistics, 2011.
Taco Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous spaces.
In NeurIPS, 2019a.
Taco S. Cohen and Max Welling. Group Equivariant Convolutional Networks. International Conference on
Machine Learning, 2016.
Taco S. Cohen and Max Welling. Steerable CNNs. International Conference on Learning Representations,
2017.
Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge Equivariant Convolutional
Networks and the Icosahedral CNN. International Conference on Machine Learning, 2019b.
Terrance DeVries and Graham W. Taylor. Improved Regularization of Convolutional Neural Networks with
Cutout. arXiv:1708.04552 [cs], 2017.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Wilson. Generalizing convolutional neural networks
for equivariance to lie groups on arbitrary continuous data. In ICML, 2020.
Marc Finzi, Roberto Bondesan, and Max Welling. Probabilistic Numeric Convolutional Neural Networks.
International Conference on Learning Representations, 2021.
Bengt Fornberg and Natasha Flyer. A Primer on Radial Basis Functions with Applications to the Geosciences.
CBMS-NSF Regional Conference Series in Applied Mathematics, 2015.
Pim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge Equivariant Mesh CNNs: Anisotropic
convolutions on geometric graphs. International Conference on Learning Representations, 2021.
Emiel Hoogeboom, Jorn W. T. Peters, Taco S. Cohen, and Max Welling. HexaConv. International Conference
on Learning Representations, 2018.
Berkay Kicanaoglu, Pim de Haan, and Taco Cohen. Gauge Equivariant Spherical CNNs, 2019. URL https:
//openreview.net/forum?id=HJeYSxHFDS.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on
Learning Representations, 2015.
10
Published as a conference paper at ICLR 2022
Leon Lang and Maurice Weiler. A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels.
International Conference on Learning Representations, 2021.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation
of deep architectures on problems with many factors of variation. International Conference on Machine
Learning, 2007.
Mateus Dias Ribeiro, Abdul Rehman, Sheraz Ahmed, and Andreas R. Dengel. Deepcfd: Efficient steady-state
laminar flow approximation with deep convolutional neural networks, 2020.
Burhan Sadiq and Divakar Viswanath. Finite difference weights, spectral differentiation, and superconvergence.
Mathematics of Computation, 83, 2014.
Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Diffusion is All You Need for Learning
on Surfaces. arXiv:2012.00888 [cs], 2020.
Zhengyang Shen, Lingshen He, Zhouchen Lin, and Jinwen Ma. PDO-eConvs: Partial Differential Operator
Based Equivariant Convolutions. International Conference on Machine Learning, 2020.
Bart Smets, Jim Portegies, Erik Bekkers, and Remco Duits. PDE-based Group Equivariant Convolutional Neural
Networks. arXiv:2001.09046 [cs, math, stat], 2020.
Francois Treves. Topological Vector Spaces, Distributions and Kernels. Academic Press Inc, San Diego, 1967.
DmitriI Aleksandrovich Varshalovich, D. A. Varshalovich, A. N. Moskalev, and Valeril Kel'manovich Kherson-
skii. Quantum Theory of Angular Momentum: Irreducible Tensors, Spherical Harmonics, Vector Coupling
Coefficients, 3nj Symbols. World Scientific Pub., 1988.
Maurice Weiler and Gabriele Cesa. General E(2)-Equivariant Steerable CNNs. Advances in Neural Information
Processing Systems, 2019.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3D Steerable CNNs: Learning
Rotationally Equivariant Features in Volumetric Data. Advances in Neural Information Processing Systems,
2018a.
Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning Steerable Filters for Rotation Equivariant
CNNs. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018b.
Maurice Weiler, Patrick Forr6, Erik Verlinde, and Max Welling. Coordinate Independent Convolutional Networks
一 Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. arXiv:2106.06020 [cs, stat], 2021.
Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. British Machine Vision Conference (BMVC),
2016.
11
Published as a conference paper at ICLR 2022
Supplementary material
A Steerable PDOs between vector and scalar fields
We give bases for the space of steerable PDOs for many important cases in Appendix K. However,
the generality of the description there obscures the connection to well-known PDOs such as the
gradient or divergence. So to complement the general solutions, we discuss a few simple cases in
much more detail in this section. We will see that Laplacian, gradient, divergence, and curl are all
rotation equivariant and, more interestingly, that all rotation equivariant PDOs can be constructed
by combining these (in the simple settings we cover in this section). In many cases, we rederive the
solutions even though all of them would follow immediately from the general case in Appendix K, in
order to provide some intuition on why these are the only equivariant PDOs. Readers who are only
interested in an overview of the results may wish to skip to the end of this section.
A.1 Scalar to scalar PDOs
We have already argued in Section 2.3 that the SO(2)-equivariant PDOs between two scalar fields
are precisely polynomials in the Laplacian, i.e. of the form q(∆) for an arbitrary real polynomial
q ∈ R[x]. The derivation given there applies without changes to SO(d) for d > 2 and to O(d) as
well, so the same holds in these cases.
A.2 Scalar to vector PDOs
We start by considering the case where ρin is trivial (with cin = 1) and ρout is the vector field
representation (i.e. cout = d). We can then represent PDOs by a d × 1 matrix of polynomials, i.e. a
column vector. The PDO steerability constraint becomes
P (gx) = gP (x)	(10)
since ρin is trivial. A good mental model for this subsection is to think of P as a vector field
P : Rd → Rd whose entries happen to be polynomials. The steerability constraint simply states that
this vector field must “look the same” after a rotation, see Fig. 3 for examples.
We begin by discussing the case G = SO(2), which is somewhat different from O(2) and from
d > 2. Any rotation equivariant vector field on R2 is fully determined by its values on the ray
{(x1, 0) | x1 > 0}. Specifically, the rotation equivariant vector fields v are precisely those that can in
polar coordinates be written as
v(r,ψ)= fCOS 中-Sin 中)(f1(r))	(11)
' ')∖ Sin φ	cos q)∖ f2(r))
for arbitrary radial parts f1 , f2 . In Cartesian coordinates, we can write this as
V(X U)= (X -y] (fι(χ2 + y2A	(12)
X 八 f2(χ2 + y2))	()
where fi(z) := fi√Zz). We need v1 and v2 to be polynomials in x, y, which means that the fi need
to be polynomials. If we then apply the D isomorphism, we get:
Proposition 3. The SO(2)-equivariant differential operators from a scalar to a vector field are
exactly those of the form
(∂∂12 -∂∂21(qq21((∆∆)) =q1(∆)(∂∂21+q2(∆)(-∂∂12	(13)
where q1 and q2 are arbitrary polynomials.
In words, the SO(2)-equivariant PDOs are all linear combinations of the gradient and the transpose
of the 2D curl, (-∂2, ∂1)T, with coefficients being polynomials in the Laplacian (rather than just real
numbers).
5Created using https://www.desmos.com/calculator/eijhparfmd
12
Published as a conference paper at ICLR 2022
(b) More complex equivariant vector field (a linear
combination of gradient and transpose curl)
(a) Vector field that induces the gradient
Figure 3: Two examples of SO(2)-equivariant vector fields.5
The gradient is also equivariant under reflections, i.e. O(2)-equivariant, and it easily generalizes to
higher dimensions. However, the transpose 2D curl only appears in this particular setting: it is not
reflection-equivariant, and it does not have an analogon in higher dimensions.6 We summarize this in
the following result:
Proposition 4. Let G = O(d) for d ≥ 2 or G = SO(d) for d > 2. Then the G-equivariant
differential operators from a scalar to a vector field are exactly those of the form
q(∆) grad	(14)
for an arbitrary polynomial q ∈ R[x].
A possible intuition for Why SO(2) is a special case is that SO⑵=S1 whereas SO(d), Sd-1
for d > 2 and O(d) = Sd-1 for d ≥ 2. Our construction above heavily makes use of the fact that
rotations of R2 correspond one-to-one to angles, i.e. points on S1, and this construction thus does not
generalize to any other cases.
Before we prove Proposition 4, we show a helpful lemma:
Lemma 5. Let d > 2. Then for any linearly independent vectors v, w ∈ Rd, there is a rotation
g ∈ SO(d) such that gv = v but gw 6= w. For d = 2, there is such a g ∈ O(2).
Proofoflemma. Since V and W are linearly independent, We can write Rd = Span(V)㊉ W, where
W is a linear subspace containing w. Pick an element g ∈ SO(d - 1) (or Ο(d - 1)=O⑴={±1}
in the d = 2 case) such that gw = w, where g acts on W (this always exists, note that W = 0). Then
there is a g ∈ SO(d) (or O(d)) that restricts to g on W and is the identity on Span(V) (in terms of
matrices with respect to a basis of the form {V, . . .}, g would be block-diagonal, with a 1 × 1 identity
block and a (d — 1) × (d — 1) block for g).	□
Proof of Proposition 4. One direction is clear: the gradient is induced by the matrix of polynomials
P (x) = x, x ∈ Rd , which is clearly equivariant. We have also seen that polynomials of the Laplacian
are equivariant, and we can compose these equivariant PDOs to get another equivariant PDO. So
what remains to show is that no other equivariant PDOs exist.
6To avoid confusion, we remark that the 3D curl of course exists but maps between two vector fields. The 2D
curl is in fact closely related to the 3D curl, and the fact that it does not have a higher-dimensional analogon
corresponds to the fact that the 3D curl, as a vector to vector PDO cannot easily be generalized to higher
dimensions.
13
Published as a conference paper at ICLR 2022
So let P be G-equivariant, with G as in Proposition 4. Let furthermore x ∈ Rd be arbitrary and
g ∈ G be an element in the stabilizer of x, i.e. gx = x. Then we have
gP (x) = P (gx) = P(x) .	(15)
In other words, for any g ∈ G with gx = x, we also have gP (x) = P (x). By the lemma, x and P(x)
are thus linearly dependent, i.e. P(x) = cx for this particular x and some c ∈ R.
We can apply this reasoning to any x ∈ Rd, so there is a function c : Rd → R such that P (x) = c(x)x.
Furthermore,
c(x)gx = gP (x) = P (gx) = c(gx)gx ,	(16)
so c has to be rotation invariant. As we have already argued, this implies that c(x) = q(x12 + . . . + x2d)
for some function q, and since c needs to be a polynomial, so does q. Then
D(P) = q(∆) grad	(17)
as claimed.	□
If d = 1, the lemma also holds and the proof goes through - this is just the scalar case from the
previous section, which is generalized here. But for G = SO(2), this argument does not work because
SO(1) = {1}, so the decisive step in the proof of the lemma fails. That is what allows the additional
equivariant PDOs.
A.3 Vector to scalar PDOs
Equivariant PDOs mapping from vector to scalar fields are simply the transpose of those mapping
from scalar to vector fields (for orthogonal groups G): P is now a 1 × d matrix and the steerability
constraint is
P (gx) = P (x)g-1 = P (x)gT .	(18)
By transposing, we get
PT(gx) = gPT (x) ,	(19)
which is the equivariance condition for scalar to vector layers. As solutions, we get the divergence (as
the transpose of the gradient) and for G = SO(2) also the 2D curl. They are again combined linearly
with polynomials in the Laplacian as coefficients.
A.4 Vector to vector PDOs
For PDOs mapping between two vector fields, the steerability constraint is
P (gx) = gP (x)g-1 .	(20)
Since the solution space in this case is somewhat more complicated, we will only cover G = SO(2)
in these examples; see Appendix K for more solutions. In principle, we could apply the same method
that we already used before for SO(2), choosing the radial components of P freely and using the
steerability constraint to determine the angular components. But since the computations in this case
are more involved and don’t yield much additional insight, we will instead use the solutions from
Appendix K. Vector fields correspond to frequency 1 irreps, and writing out the solutions for those
explicitly, we get that the equivariant PDOs are exactly linear combinations of
1 0	0 -1	∂x2 -∂y2	2∂x∂y	-2∂x∂y	∂x2 -∂y2
0	1 ,	1	0	,	2∂x∂y	∂y2	-	∂x2	,	∂x2	-	∂y2	2∂x∂y
(21)
with polynomials in the Laplacian as coefficients.
The first two operators are simply the identity and a ∏2 rotation matrix, both zeroth order PDOs. Note
that the rotation matrix rotates the fibers of the vector field, it does not act on the base space. The
other two operators are less interpretable, but we can replace them through a change of basis:
1δ (1	θʌ	+ 1	(d2	-	d2	2dχdy	ʌ = (	dX	dχdyʌ
2 V0	1J	+ 2 ∖2dχ dy	dy	-	dX)	Idxdy	d2	J
(22)
14
Published as a conference paper at ICLR 2022
which is the matrix describing the composition grad ◦ div. Similarly,
1δ(0 -Λ + 1 (-2∂χ∂y ∂X - ∂y∖ = (-∂χ∂y	∂X ∖
2δ V1 0 ) + 2 Vd2 - ∂2 2dχdy ) V -dy	dχdy)
(23)
which is the matrix describing grad ◦ curl2D. So if We write R for the ∏ rotation matrix (interpreted
as a PDO), then the equivariant PDOs mapping between vector fields are exactly linear combinations
of
id, R, grad ◦ div, grad ◦ curl2D ,	(24)
as always with polynomials in the Laplacian as coefficients.
We can be even more economical and describe these PDOs with fewer building blocks. For two
vectors P, Q of one-dimensional PDOs, e.g. P = ∂ = (∂ι, ∂2)τ, we write P 0 Q for the 2 X 2
PDO with entries (P 0 Q)j = PiQj. Then we can write for example grad ◦ div = ∂ 0 ∂. We
furthermore note that
R∂= 10 -10	∂∂xy = -∂∂xy = curl2D .	(25)
This means we can write the basis from above as
id, R, ∂ 0 ∂,	∂ 0 R∂ .
(26)
A.5 Summary of results
•	SO(d)- or O(d)-equivariant PDOs between two scalar fields are exactly polynomials in the
Laplacian.
•	G-equivariant PDOs mapping from scalar to vector fields are exactly those of the form
q(∆) grad for G = SO(d) with d > 2 or G = O(d). For PDOs from vector to scalar fields,
we similarly get q(∆) div.
•	SO(2)-equivariant PDOs from vector to scalar fields are exactly those of the form
q1(∆) div + q2(∆) curl2D .	(27)
For PDOs from scalar to vector fields, we get grad instead of div and the transpose of the
2D curl instead.
•	SO(2)-equivariant PDOs between two vector fields are linear combinations of the identity, a
2 rotation, grad ◦ div and grad ◦ curl2D, with polynomials in the Laplacian as coefficients.
B Representation theory primer
This section introduces the fundamental definitions of representation theory that we need.
B.1	Basic definitions
Definition 1. A group representation of a group G, or representation for short, is a group homomor-
phism ρ : G → GL(V ) for some vector space V . This means that ρ(gg0) = ρ(g)ρ(g0) for all g, g0,
so multiplication of group elements in G is represented as matrix multiplication in GL(V ).
For this paper, we only need V = Rc, so we focus on this case. In particular, some of the following
definitions make use of the fact that we only consider finite-dimensional representations.
Given multiple representations of the same group, we can “stack them together” using direct sums:
Definition 2. Let ρ1 : G → GL(Rc1 ) and ρ2 : G → GL(Rc2) be two representations of G. Then
we define the direct sum representation ρι ㊉ ρ2 : G → GL(Rc1+c2) by
(P1 ㊉ P2)(g) = (ρi(g) ρ2(g)) ,	(28)
which acts independently on the subspaces Rc1 and Rc2 of Rc1+c2 .
15
Published as a conference paper at ICLR 2022
This corresponds exactly to stacking feature fields of different types. For example, we can stack a
vector and a scalar field into one four-dimensional field, such that its vector and scalar part transform
independently. The representation of this four-dimensional field will be the direct sum of the vector
and scalar field representations.
Often, two representations are formally different but can be transformed into one another using
a change of basis; they then behave the same in all relevant aspects. For example, SO(2) has a
representation
Pw) =kos 中 - sin 吟	(29)
八" ∖ sin φ CoS φ J	' /
that represents a rotation angle 夕 by a counterclockwise rotation matrix (this representation is the one
used for vector fields). However, we could just as well use
ρ(φ) := ( cos 中 sin °) ,	(30)
八〜	\-sin φ cos φ J ,	∖ /
where the rotation is clockwise. Using one or the other is pure convention and we would like to treat
them as “the same” representation. This is formalized as follows:
Definition 3. Two representations ρ1, ρ2 : G → GL(Rc) are equivalent if there is a matrix Q ∈
GL(Rc) such that
ρ2(g) = Q-1ρ1(g)Q	(31)
for all g ∈ G.
Intuitively, ρ1 and ρ2 differ only by a change of basis, which is given by Q.
B.2	Decomposition into irreducible representations
We can now discuss irreducible representations, which play an important role for solving the kernel
and PDO steerability constraints.
Definition 4. A linear subspace W ⊆ Rc is called invariant under a representation ρ : G → GL(Rc)
if ρ(g)w ∈ W for all g ∈ G and W ∈ W. In this case, We can define the restriction ρ∣w : G →
GL(W), called a subrepresentation of ρ.
Definition 5. A representation ρ is called irreducible if all its subrepresentations are either ρ itself or
representations G → GL({0}), where {0} is the trivial vector space.
For example, ρι and ρ2 are both subrepresentations of ρι ㊉ ρ2, so direct sums are never irreducible.
A natural question is the converse: if a representation is not equivalent to a direct sum, does that mean
that it is irreducible? In other words, can all representations be split into a direct sum of irreducible
ones? In general, this is false, but it holds for the cases that interest us:
Definition 6. A topological group is a group G equipped with a topology, such that the group
multiplication G × G → G and the inverse map G → G are continuous with respect to that topology.
A compact group is a topological group that is compact as a topological space.
Theorem 6. Let G be a compact group. Then every finite-dimensional representation of G is
equivalent to a direct sum of irreducible representations.
Since we only consider compact subgroups of O(2) and O(3), this theorem applies to all the cases
we solve.
So we can always write
ρin = Qi-n 1	ψi Qin
i∈Iin
(32)
ρout
Qo-u1t	ψi Qout
i∈Iout
where the ψi are irreducible representations. It is then easy to show (Weiler & Cesa, 2019) that a
kernel k solves the G-steerability constraint
k(gx) = ρout(g)-1 k(x) ρin(g)-1	(33)
16
Published as a conference paper at ICLR 2022
if and only if κ := Qout k Qi-n 1 solves a block-wise steerability constraint between irreducible
representations. Concretely,
κij (gx) = ψi(g)-1κij(x)ψj(g)-1	∀i,j	(34)
where κij(x) is the submatrix of κ(x) that belongs to ψi and ψj.
The approach to solving the steerability constraint is thus to solve Eq. (34) for arbitrary irreducible
representations ψi and ψj . For general (not necessarily irreducible) ρout and ρin, we then first find the
decompositions into irreducible representations. Each basis element κij of the solution to Eq. (34) is
then padded with zeros to the right size and finally transformed via k = Qo-u1t κQin to get the final
basis elements. See Weiler & Cesa (2019) for a more detailed discussion and visualization.
Clearly, this procedure works just as well for PDOs as it does for kernels: for PDOs, we need to
find the restriction of the kernel solution space to polynomials, and it does not matter whether we
restrict on the level of irreducible representations and then combine them, or first combine irreducible
representations and then restrict.
B.3 Specific representations
We now define all the types of representations that occur in the main paper:
•	As already mentioned in the paper, scalar fields are described by the trivial representation
ρ(g) := 1 and vector fields by the representation ρ(g) := g (for G ≤ GL(Rd)).
•	For a finite group G, the regular representation is ρ : G → GL(R|G|), defined by
ρ(g)eg0 := egg0 ,	(35)
where (eg)g∈G is the canonical basis of R|G| (for some ordering of G). So this representation
associates one basis vector to each group element and then acts by permuting these basis
vectors. ρ(g) is thus always a permutation matrix.
•	Quotient representations are a generalization of regular representations, defined as follows:
let G be a finite group and H ≤ G a subgroup. Then we define the quotient representation
PGoH ： G → GL(RIG1/1HI) by
PGOH(g)eg0H := egg0H ,	(36)
where we now use a basis indexed by the cosets gH ∈ G/H for g ∈ G. For H = {e},
we recover regular representations. Appendix C by Weiler & Cesa (2019) provides some
intuition for these quotient representations in the case where G and H are both cyclic groups
CN and CM.
C Intuition for the group action on polynomials
Our work makes heavy use of terms of the form p(gx), where p is a polynomial p : Rd → R, g ∈ G
is a group element, and x ∈ Rd . We would now like to provide a bit more intuition for this action
of the group G on polynomials. Note that we will only cover scalar-valued polynomials in this
appendix, i.e. using trivial representations. The general case is straight-forward: the group acts on
each polynomial in a matrix of polynomials the way we describe here, while Pin(g) and Pout(g) act
via matrix multiplication.
To prevent confusion, let us reiterate that we can think of polynomials in two different ways. The first
is as a formal expression, where x is a placeholder for things to be plugged in. This is the approach
we take when connecting polynomials to PDOs, where we plug in differential operators for x. The
second is as a specific type of function on Rd—in which case x ∈ Rd is simply the argument of that
function.
This second perspective is the one in which the group action on polynomials is easiest to understand.
Specifically, the action of g on x is simply matrix multiplication. Similar to how x 7→ p(x) defines a
function on Rd, x 7→ p(gx) defines a different function on Rd. We could think of it as composing the
function p with the group action of g on Rd .
17
Published as a conference paper at ICLR 2022
Crucially, this new function is still a polynomial in the components ofx, just with different coefficients.
For purposes of illustration, consider a very simple example with d = 2 and G = SO(2). We will
use the polynomial p(x) = x22 + x1 (this is just meant to be a simple non-trivial polynomial, it does
not have special equivariance properties).
We can represent g as a rotation matrix parameterized by an angle θ. The action on x is then given by
gx
cos θ
sin θ
- sin θ
cos θ
=x1 cos θ - x2 sin θ
x1 sin θ + x2 cos θ
(37)
The left-hand side is what we now plug into our polynomial, which yields
p(gx) = (gx)22 + (gx)1 = (x1 sin θ + x2 cos θ)2 + (x1 cos θ - x2 sin θ) .	(38)
We can expand this and collect the coefficients for each power of x1 and x2, which yields
p(gx) = sin2 (θ)x21 + 2 sin(θ) cos(θ)x1x2 + cos2 (θ)x22 + cos(θ)x1 - sin(θ)x2 .	(39)
If desired, we can now switch to the first perspective, and interpret this polynomial a formal expression
defined by its coefficients. In that perspective, g acts on p by modifying its coefficients, rather than
by composition. Computing the new coefficients is straightforward analytically (given a matrix
representation of g); we just gave a simple example for a polynomial of order two. The general
case proceeds along the same lines, it just requires using the binomial theorem (or the multinomial
theorem for d > 2).
D Background on distributions
In Appendix E, we will describe our framework for equivariant maps using convolutions with
Schwartz distributions. To facilitate that, we now give the necessary background on Schwartz
distributions. We restrict ourselves to what is absolutely necessary for our purposes; for a much more
thorough introduction and for proofs, see e.g. Treves (1967).
D. 1 Basic definitions
Definition 7. For U ⊂ Rd open, D(U) := Cc∞(U) is called the space of test functions on U (Cc∞ (U)
is the space of compactly supported smooth functions U → R). A sequence (Wn) in D(U) is defined
to converge to 0 iff
(i)	there is a compact subset K ⊂ U such that the support of each Wn is contained in K and
(ii)	∂αWn → 0 uniformly for all multi-indices α.
One can define the so-called canonical LF topology on D(U). This topology induces the notion
of convergence just given. However, constructing this topology explicitly is unnecessary for our
purposes since knowing when sequences converge will be enough.
Definition 8. A linear functional T : D(U) → R is defined as continuous if for every sequence Wn
that converges to 0 in D(U), T Wn → 0 (in R).
Such a continuous functional is called a distribution on U. The space of all distributions on U is
written as D0(U).
This notion of continuity is also induced by the canonical LF topology and then D0(U) is the
topological dual of D(U) as the notation suggests.
So intuitively, a distribution is a “reasonable” way of linearly assigning a number in R to each test
function in Cc∞ .
A function f : U → R induces a distribution Tf ∈ D0 (U), defined by
TfW :=	fWdλd .
U
(40)
18
Published as a conference paper at ICLR 2022
Since φ has compact support, We don't need many restrictions on f (for example, any locally
integrable function f ∈ L1,loc (U) works).
We Will also use the duality pairing
(T,0 := T(0	(41)
and under slight abuse of notation also Write
hf,0i :
Tf(0)
f0dλd.
U
(42)
Note that this coincides With the inner product on L2(U), but it alloWs functions f that are not in L2
While in exchange requiring 0to have compact support.
D.2 Convolutions
We define the translation τx : Rd → Rd by τx(y) = y + x and set τxf := f ◦ τ-x for functions f
on Rd. This is exactly the same as the action of (Rd, +) We used in the main paper, just With more
explicit notation. Furthermore, We Write f(x) := f (-x).
Then We can define the convolution betWeen a distribution and a test function: for f ∈ D(U) and
T ∈ D0(U), the convolution T * f ∈ C∞(U) is defined by
__	. . ,	一 , Y,
(T * f )(x) := T(Txf).	(43)
Explicitly, (τxf)(y) = f(x - y). This immediately shoWs that this notion of convolution extends the
classical one, i.e.
Tg*f =g*f.	(44)
D.3 Derivatives
Definition 9. For T ∈ D0(U), We define the distributional derivative ∂αT as the unique distribution
on U for Which
h∂ɑT,0i = (-1)lαlhT,∂α0i.	(45)
The distributional derivative of all orders alWays exists, i.e. “distributions are infinitely differentiable”,
but of course only in this distributional sense. At least for f ∈ C∞(U) (but also under much Weaker
assumptions), this definition also extends the definition of derivatives of functions, in the sense that
T∂αf = ∂αTf.
D.4 Composition with diffeomorphisms
Let F : U → U be a diffeomorphism andT ∈ D0(U). Then We define the composition T ◦F ∈ D0(U)
as
hT ◦ F,0i := hT, I det DF TI夕◦ F Ti	(46)
Where det DF-1 is the inverse Jacobian.
As in the previous constructions, this extends the definition for classical functions in the sense that
Tf ◦f = Tf ◦ F.
This folloWs immediately from the transformation theorem for integrals:
hTf◦F,0i = [ (f ◦ F) ∙中 dλ
U
=f f ∙ (φ ◦ FT)Idet DF-1]dλ
U
= hTf, (0 ◦ F -1)IIdet DF -1IIi
= hTf ◦ F, 0i .
(47)
(48)
(49)
(50)
(51)
19
Published as a conference paper at ICLR 2022
Furthermore, this type of composition is associative:
h(T ◦ F) ◦ G, φi = hT ◦ F, I det DG-1∖φ ◦ GTi	(52)
=hT, ∖det DFT ∖ ∖det DG-1 ◦ F-1∖φ ◦ GTO FTi	(53)
=hT, ∖det(DG-1 ◦ FT)DF-1∖φ ◦ (F ◦ G)Ti	(54)
=hT, ∖det D(F ◦ G)T∖° ◦ (F ◦ G)Ti	(55)
=hT ◦ (F ◦ G),φi.	(56)
We are particularly interested in the case where F is a linear transformation, i.e. F ∈ GL(Rd). Then
we have
hT ◦ F" = ∖det FT∖hT,φ ◦ FT〉	(57)
(note that in this case, we can pull out the determinant because it is just a constant). For translations,
we get
hT ◦ Tx ,0 =工中 O T-X = (T,Tχ0 .	(58)
D.5 The Dirac delta distribution
A very simple but important distribution is the following:
Definition 10. For any x ∈ Rd, the Dirac delta distribution δx ∈ D(Rd) is defined by
δχ[g]:=2(x).	(59)
It is clear from the definition that the distributional derivatives of the delta distribution are given by
(∂αδχ)3 = (-1)lαl∂α双x).	(60)
D.6 Convergence of distributions
Definition 11. We say that a sequence Tn of distributions converges to T, written Tn → T, if it
converges pointwise, i.e.
hTn,0 → hT, φi for n → ∞	(61)
for all 夕 ∈ D(U).
Abusing notation a bit, we also write fn → T for functions fn if Tfn → T .
One important type of function sequences are Dirac sequences, which converge to the Delta distribu-
tion:
Lemma 7. Let fn be a sequence in L1 (Rd) such that
(i)	fn ≥ 0,
(ii)	kfnk1 = 1, and
(iii)	jRd∖Bε(0) fndλd → 0 forall ε > 0.
Then fn → δ0 in the sense of distributions.
We also note that convergence plays together nicely with derivatives and with convolutions:
Lemma 8. IfTn → T, then ∂αTn → ∂αTfor all α.
Proof.
h∂ αTn,0 =(T)lαlhTn,∂α0
→ (-1)lαlhT,∂αg	(62)
=h∂αT,gi.
□
20
Published as a conference paper at ICLR 2022
Lemma 9. If Tn → T, then Tn * f → T * f Pointwisefor all f ∈ D(U).
Proof.
. ,,, ,	—.
(Tn * f )(x)= Tn(Txf)
→ T(Txf)	(63)
= (T*f)(x).
□
D.7 Tempered distributions and the Fourier transform
The Schwartz space S(Rd) is the space of functions for which all derivatives decay very quickly as
|x| → ∞. More precisely, a smooth function f : Rd → R is in S(Rd) iff
sup xβ∂αf(x) < ∞ .
x∈Rd
(64)
Intuitively, all derivatives off must decay more quickly than any polynomial. Examples are compactly
supported functions or Gaussians.
Its dual space S0(Rd) is called the space of tempered distributions and can be continuously embedded
into D0 (Rd). Tempered distributions are still very general; in particular the delta distribution,
distributions induced by functions, and derivatives of tempered distributions are all tempered.
The reason we’re interested in tempered distributions is that there is a Fourier transform defined by
hF{T} ,0:= hT, F{0i	(65)
for tempered distributions T. This is an automorphism on S0(Rd).
We use the following convention for the Fourier transform on functions:
F{f}(ξ):
(2π)-d/2 [
Rd
f (x) exp(-ix ∙ ξ) dx,
which means the inverse is given by
F-1 {f} (x)
(2π)-d/2 /
Rd
f (x) exp(ix ∙ ξ) dξ.
(66)
(67)
We will later need the Fourier transform of derivatives of the Dirac delta distribution:
hF{∂αδo},6=h∂αδo, F{0i
=(-1)lαl∂ αF{φ}∖
0
=(-i)”{xα0∖o
)1a1(2n)-d/2 /
)1a1(2n)-d/2 Z
xαφ(x) exp(-ix ∙ 0)dx
xɑφ(x)dx ,
(68)
so F {∂ɑδ0} a Xa (or more precisely, the distribution induced by the function X → Xa).
D.8 Matrices of distributions
Analogously to how we defined matrices of PDOs or of polynomials, we will need matrices of
distributions. We will write D0(U, Rcout ×cin) for the space of cout × cin dimensional matrices with
entries in D0(U). We get a pairing
h∙, ∙i : D0(U, Rcout×cin) X D(U, Rcin) → Rcout	(69)
21
Published as a conference paper at ICLR 2022
defined by
hT, fii :=XhTij,fji	(70)
j
The convolution of T ∈ D0(U, Rcout ×cin) and f ∈ D(U, Rcin) is defined analogously to the scalar case,
i.e. (T * f )(χ) := hT, τχfi, where We now use the more general duality PairingjUst defined.
We can also define the multiplication of matrices of distributions by matrices over R. For a matrix
A ∈ Rcout×cout and a matrix of distributions T ∈ D0(Rd, Rcout×cin), we write
(AT)ij := XAilTlj ∈ D0(Rd, Rcout×cout)	(71)
l
(multiPlying a distribution by a scalar obviously defines another distribution). Analogously, we define
TB for B ∈ Rcin ×cin . It immediately follows that
AhT,B0 = hATB,φi	(72)
holds.
ComPosition with diffeomorPhisms of Rd can simPly be defined comPonent-wise for matrices of
distributions. This commutes with multiPlication by constant matrices, meaning that
ATB ◦ F = A(T ◦ F)B .	(73)
Finally, we also define the Fourier transform comPonent-wise, and this commutes with multiPlication
by matrices in the same way.
E Distributional framework for equivariant maps
In this section, we develoP two main results: first that all linear continuous translation equivariant
maPs between feature sPaces are convolutions with some distribution, and then the equivariance
constraint for such convolutions. See APPendix D for background on distributions. We equiP D(U)
with the canonical LF toPology throughout this section.
E.1 Translation equivariant maps are convolutions with distributions
We begin by showing that the framework using convolutions with Schwartz distributions encomPasses
all translation equivariant continuous linear maPs between feature sPaces. As PreParation, we Prove a
simPle Lemma on the reflection maP s:
Lemma 10. The map s : D(U) → D(U) given by s(f) := f is linear and continuous.
Proof. Linearity is clear:
s(af + bg)(x) = af (-x) + bg(-x) = as(f)(x) + bs(g)(x) .	(74)
For continuity, we use the fact that a linear maP from D(U) to itself is continuous if and only if it is
sequentially continuous (Treves, 1967, ProPosition 14.7). So take any sequence fn → 0 in D(U).
This means that
(i)	there is a comPact subset K ⊂ U such that the suPPort of each fn is contained in K
(ii)	∂αfn → 0 uniformly for all multi-indices α
We set -K := {-x | x ∈ K}, then the suPPort of s(fn) (which is just the mirror of the suPPort of
fn) is contained in -K, and -K is comPact. Additionally,
∂αs(fn) = (-1)αs(∂αfn) .	(75)
Since ∂αfn → 0 uniformly, the same is true for s (∂αfn). It follows that s is sequentially continuous,
and thus continuous, which concludes the proof.	□
22
Published as a conference paper at ICLR 2022
(77)
(78)
(79)
□
We first prove our main generality result for the special case of one-dimensional fibers:
Lemma 11. Let Φ : D(U) → D(U) be a translation equivariant continuous linear map. Then there
is a distribution T ∈ D0(U) such that Φ(f) = T * f.
Proof. Let Φ : D(U) → D(U) be any continuous linear map, where continuity is understood with
respect to the canonical LF topology. We then define
,, .. ....
T : D(U) → R,	T(f)=Φ(f)(0).	(76)
Equivalently, we can write T = δ0 ◦ Φ ◦ s, understood as normal composition of functions. δ0 and Φ
are linear and continuous, as is s by Lemma 10. Therefore, T is also linear and continuous, and thus
a Schwartz distribution T ∈ D0(U).
We will also need that
τ~-xf (y) = (τ-xf)(-y)
= f(x-y)
=f(y - χ)
,	—., .
=(Txf)(y).
Now using the assumption that Φ is translation equivariant, i.e.
τx ◦ Φ = Φ ◦ τx ,
it follows that convolution with T is given by
(T * f )(χ) = TTxf
= hT, T~-xfi
= Φ(T-xf)(0)
= T-xΦ(f)(0)
= Φ(f)(x) .
This shows that convolution with T is Φ, which concludes the proof.
Finally, we generalize to multi-dimensional feature fields:
Theorem 12. Let Φ : D(U, Rcin) → D(U, Rcout) be a translation equivariant continuous linear map.
Then there is a distribution T ∈ D0(U, Rcout ×cin) such that Φ(f) = T * f.
Proof. We write ej for the j-th canonical basis vector of Rcin. For f ∈ D(U, Rcin), we write fj for
its j -th component. Then we have
(Cin	∖	Cin
Xfjej =XΦ(fjej) .	(80)
j=1	j=1
We now define the components Φij : D(U) → D(U) by
Φij(g) = (Φ(gej))i .	(81)
Here, gej ∈ D(U, RCin) is the map with g in its j-th output component and 0 in the other ones. We
can then write
Cin	Cin
Φ(f)i =XΦ(fjej)i = XΦij(fj).	(82)
j=1	j=1
It is clear that the components Φij of Φ are still translation equivariant, continuous and linear.
Therefore, by the previous Lemma, there are distributions Tij ∈ D0(U) such that Φij (g) = Tij * g.
We then get
Cin
Φ(f)i =	Tij*fj =T*f	(83)
j=1
where T ∈D0(U, Rcout ×cin) is the matrix of distributions with entries Tij.	□
23
Published as a conference paper at ICLR 2022
E.2 Equivariance constraint for distributions
We will now characterize the distributions T ∈ D0(Rd, Rcout ×cin) for which the operator
T* : D(Rd, Rcin) → C∞ (Rd, Rcout)	(84)
given by f → T * f is H-equivariant. The codomain of T may contain functions that are not
compactly supported for some T. We are mostly interested in distributions T for which the codomain
can be restricted to D(Rd, Rcout) but for the discussion of equivariance this does not make any
difference, so we keep the derivation general by not restricting the distribution T.
First, we can note that convolution with a distribution is always translation equivariant:
Proposition 13. The map f 7→ T * f is translation equivariant for any distribution T.
Proof. The general equivariance condition is
(h ∙(T * f ))(χ) = (T * (h ∙ f ))(χ)	(85)
for all f ∈ D(Rd, Rcin), h ∈ H, x ∈ Rd. More explicitly, this means
PoUt(g)hτ,τh-iχf = hT,τx(Pin(g)f - h-1)i ,	(86)
where g is the linear component of h.
Let h = t ∈ Rd, i.e. a pure translation. The condition then becomes
— 「 :----------------------- 一 一 「
hT,T(χ-t)fi = hT,Tχ(f ◦ τ-t)i = (T,TχT-tfi,	(87)
which always holds (since T(x-t)= TχT-t). So T is translation equivariant by construction. □
Therefore, it suffices to consider pure linear transformations g ∈ GL(Rd). For those, we have the
following result, which generalizes the steerability constraint for PDOs and the one for kernels:
Theorem 14. Let T ∈ D0(Rd, Rcout ×cin). Then the map f 7→ T * f is G-equivariant if and only if
T ◦ g = |det g∣-1ρ out (g)Tρ in (g)-1.	(88)
As is shown in Appendix D, the definitions of convolution with distributions and of composition with
a diffeomorphism are compatible with those for classical functions. So if T is a classical kernel, then
the constraint on T is given by the same equation, which already gives us the steerability constraint
for kernels.
(89)
(90)
(91)
(92)
Proof. We start with the constraint Eq. (86) from above with h = g ∈ G and will transform this into
the desired form Eq. (88). First, to simplify notation, We can remove all the reflections ∙ in Eq. (86).
1
This is now possible because f ◦ g-1 = f ◦ g-1, which was not true for translations. Furthermore,
we can pull the translations to the other side of the duality pairing:
Pout(g)hT ◦ Tg-iχ,f i = Ihr ◦ Tχ,Pin(g)f ◦ g-1i
= |det g|hT ◦ τx ◦ g, ρin(g)fi .
Now we’ll use
τx ◦g = g ◦ τg-1x ,
since
x +gy = g(g-1x + y) .
Plugging this into Eq. (89), we get
ρout(g)hT ◦ τg-1x, fi = |det g|hT ◦ g ◦ τg-1x, ρin(g)f i .
We want to have only f on the right side of the duality pairing, so we use the notation we introduced
for multiplying distributions by matrices and get
(Pout(g)T ◦ Tg-iχ,f i = |det g∣(T ◦ g ◦ Tg-iχρin(g), fi .	(93)
24
Published as a conference paper at ICLR 2022
This holds for all f iff we have equality of distributions,
ρout(g)T ◦ τg-1x =! |det g|T ◦g ◦ τg-1xρin(g) .	(94)
Finally, multiplication with matrices and composition with diffeomorphisms commute, so we can
cancel the τg-1x. This means our final constraint on T is
ρout(g)T =! |det g|(T ◦ g)ρin(g) ,	(95)
which we can slightly rewrite as
T ◦ g =Idet g∣-1Pout(g)Tρin(g)-1 .	(96)
This is exactly Eq. (88). All steps of the derivation work equally well in the other direction, which
proves the “if and only if”.	□
E.3 Differential operators as convolutions
It is clear that convolutions with classical kernels are a special case of convolutions with distributions
(see Appendix D). But we have claimed that convolutions with distributions also cover PDOs, which
is what we show now.
We have already seen in Appendix D that derivatives of the delta distribution are closely related to
PDOs. So we calculate the convolution with such derivatives:
((∂αδo) * f)(x) = h∂αδo,Tχf)
=(-1)lαl∂ α(τχf)(0)
=(-1)lαlτχ(∂ αf)(0)
= τx ∂}αf (0)	(97)
= ∂}αf (-x)
= ∂αf(x).
Noting that the map T → T is R-linear, We see that D(p)f = p(∂)δQ * f for polynomials p. It then
also immediately follows that this is true for matrices of polynomials P because
cin
(D(P )f)i =	D(Pij )fj
j=1
cin
XPij(∂)δ0 *fj
j=1
(98)
=P(∂)δ0*f.
This shoWs that PDOs can be interpreted as convolutions With distributions, as claimed.
E.4 The Fourier duality between kernels and PDOs
Interpreting PDOs as convolutions With distributions alloWs us to relate them to classical convolutional
kernels via a Fourier transform. We have already seen in Appendix D that
F{∂αδo} = (2n)-d/2(-i)|a|xa,	(99)
Which by linearity of the Fourier transform immediately implies
F{P(∂)δo}ij = (2π)-d/2 X(-i)1a|cjXa .	(100)
α
for Pij = Pα ciαjxα. So the Fourier transform of derivatives of the delta distribution are polynomials,
and of course vice versa via the inverse Fourier transform. Since PDOs are convolutions With such
delta distribution derivatives, We can also interpret them as convolutions with the (inverse) Fourier
transform of polynomials. We Will use this interpretation to give a derivation of the PDO steerability
constraint that sheds some light on the similarities and differences to the kernel steerability constraint.
We begin by proving a basic fact about the Fourier transform of a composition of functions:
25
Published as a conference paper at ICLR 2022
Lemma 15. Let g ∈ GL(Rd) and 夕 ∈ D(U). Then
F{2。g} = Idet g-1∣F{^} ◦ g-T
where we use the Shorthandg-T := (g-1)T.
(101)
Proof. First we apply the transformation theorem for integrals:
F{P◦ g} (ξ) = (2π)-"2
J 4(gx)exp(-ix ∙ ξ)dx
=I det g-1∣(2π)-"2 /
q(x) exp(-ig-1x ∙ ξ)dx .
(102)
Now we just rewrite g-1χ ∙ ξ = X ∙ g-Tξ, which gives the desired result.
□
This result holds more generally for tempered distributions (a subset of distributions for which the
Fourier transform can be defined, see Appendix D):
Proposition 16. For a tempered distribution T and g ∈ GL(Rd),
F{T ◦g} = IIdet g-1IIF {T} ◦g-T .	(103)
Proof.
hF {T ◦ g} ,0 = hT ◦ g, F {ψ}^i
=∣ det g-1∣hT, F{0}。g-1i
=)hT, F{0。gτ}〉	(104)
=hF{T} ,0。gT i
=1det g-1|hF{T} ◦ g-T,0i,
where We used Lemma 15 for(1).	□
Now note that the equivariance constraint for distributions, Eq. (88), is equivalent to the constraint
we get when we take the Fourier transform on both sides. That’s because the Fourier transform is an
automorphism on the space of tempered distributions. By applying the result we just proved, we then
get the constraint
[det g-1∣F{T} ◦ g- = ∣det g-1|Pout(g)F{T} Pin(g)-1 .	(105)
We can cancel the determinants, which gives the equivariance constraint in Fourier space:
F {T}。g-T = Pout(g)F {T} Pin(g)-1 .	(106)
Now let T = P(∂)δo, so that T * f = D(P)f for some matrix of polynomials P. We,ve seen above
that in this case F {T} = P 。 m-i (up to a constant coefficient), where m-i is multiplication by the
negative imaginary unit -i. So D(P) is equivariant iff
P。m-i。g-T = Pout(g)P。m-iPin(g)-1 ,	(107)
but since m-i is invertible and commutes with the other maps, we can cancel it and get the equivari-
ance condition
P。g-T = Pout(g)Pρin(g)-1 ,	(108)
which is precisely the PDO steerability constraint. The reason that it differs slightly from the kernel
steerability constraint can now be traced back to Lemma 15. Intuitively speaking, since PDOs are
in a sense the Fourier transform of convolutional kernels, they transform differently under GL(Rd),
which leads to superficial differences in the steerability constraints. However, Fourier transforms
commute with rotations and reflections (i.e. transformations from O(d)), which is why for G ≤ O(d),
the two steerability constraints coincide.
26
Published as a conference paper at ICLR 2022
E.5 PDOs as the infinitesimal limit of kernels
Let ψn be any sequence of functions such that ψn → δ0 , for example a Dirac sequence. Then for a
polynomial p = Pα cαxα, we also have
p(∂)ψn → p(∂)δ0	(109)
because of Lemma 8. Then Lemma 9 implies that
p(∂)ψn * f → p(∂)δo * f = D(p)f,	(110)
where the convergence is understood pointwise. Therefore, any sequence of kernels that approximates
the delta distribution (by becoming “increasingly narrow”) can be used to approximate arbitrary
PDOs by convolving with the derivatives of the kernels.
One example is a sequence of Gaussians
ψε(X) = (2∏εp∕2 e-x2∕ε	(111)
(now indexed by ε > 0 instead of natural numbers). For ε → 0, we have ψε → δ0, as is easy to
check with Lemma 7. This naturally leads to the “derivative of Gaussian” discretization used in our
experiments: we can approximate the PDO as convolution with a derivative of a Gaussian kernel and
then simply discretize this Gaussian derivative by sampling it on the grid points.
The discussion in this subsection focused on 1 × 1 PDOs and kernels, i.e. cin = cout = 1, but since
convergence for multi-dimensional PDOs and kernels works component-wise, everything generalizes
immediately to that setting.
F The relation between kernels and PDOs
Convolutional kernels and PDOs are closely related but also differ in some important ways. This
appendix is meant to briefly summarize various aspects of their relation that would otherwise only be
scattered throughout this paper.
First and foremost, we would like to emphasize that, in a continuous setting, PDOs are not just
a special case of convolutions with classical kernels. For example, the gradient operator is not
represented by convolution with any kernel. In fact, even the identity operator (a zeroth-order PDO)
is not the convolution with any function.
However, there are two caveats to the above statement. First, if we allow convolutions with Schwartz
distributions, rather than the usual kernels, then this generalizes both PDOs and convolutions with
functions. In this broader framework, we can thus interpret PDOs as convolutions. But to the best of
our knowledge, convolutions with Schwartz distributions have not been previously discussed in a
deep learning context, so for the common usage of “convolution”, PDOs are distinct operators.
The second caveat is that when we discretize (translation-equivariant) PDOs on a regular grid, they
become convolutions in this discrete setting. Even so, the differences in the continuum can matter in
practice. First, we might want to discretize on point clouds or meshes. Second, even on a regular
grid, there are different methods for discretizing PDOs. While all of them lead to convolutions, they
lead to convolutions with slightly different kernels. This shows that there is no clear one-to-one
correspondence between kernels and PDOs even on discrete regular grids. Finally, the space of
discretizations of equivariant PDOs is not necessarily the same as the space of discretizations of
equivariant kernels.
Finally, we would like to mention a connection that is outside the scope of this paper: infinite series
of differential operators and convolutions can be the same even in the continuous setting. As an
example, consider the diffusion equation
∂tu(x, t) = ∆u(x, t) .	(112)
Its solution can be written as
u(x, t) = exp(t∆)u(x, t = 0) .	(113)
The time evolution operator exp(t∆) is an infinite series of differential operators, but can also be
written as a convolution with a heat kernel. However, these infinite series are not covered by our
work—we restrict ourselves to PDOs of finite order. We mention this connection between infinite
PDOs and convolutions only to avoid confusion for readers familiar with this correspondence.
27
Published as a conference paper at ICLR 2022
G	Proof of the PDO equivariance constraint
In this section, we prove Proposition 1 and Theorem 2, the characterizations of translation- and
G-equivariance for PDOs. Appendix E already contains a proof of Theorem 2 that is arguably
more insightful, but in this section we present an elementary proof that does not rely on Schwartz
distributions.
G. 1 Translation equivariance
We begin by proving that translation equivariance of a PDO is equivalent to spatially constant
coefficients, Proposition 1.
Let Φij = Pα ciαj ∂α, where ciαj ∈ C∞ (Rd) and i, j index the cout × cin matrix describing the PDO.
The equivariance condition Eq. (2) for the special case of translations is
Φ(t Bin f) = t BoutΦ(f) ∀t ∈ Rd,f ∈ Fin.	(114)
Using the definition of the B action in Eq. (1), this becomes
Xcα(x)∂αf(x - t) = Xcα(x - t)∂αf(x - t) ∀x,t ∈ Rd, f ∈ Fin .	(115)
Here, cα are matrix-valued, f is vector valued, and we have a matrix-vector product between the two.
Explicitly, this means
XX
ciαj(x)∂αfj(x -t) = XXciαj(x -t)∂αfj(x - t)	(116)
for all indices i. If f is zero in all but one component, the sum over j reduces to one summand. We
therefore get a simpler scalar constraint
Xciαj(x)∂αg(x -t) = X ciαj (x -t)∂αg(x -t)	(117)
that has to hold for all indices i,j, where g ∈ C∞(Rd) is now scalar-valued.
Since this must hold for all functions g, we get an equality of differential operators,
Xciαj(x)∂α = X ciαj (x - t)∂α	∀x,t ∈ Rd.	(118)
αα
This implies ciαj (x - t) = ciαj(x) for all x, t and α, which in turn means that ciαj must be constants.
From Eq. (115), it is also apparent that constant coefficients are sufficient for translation equivariance,
which proves the converse direction.
G.2 G-EQUIVARIANCE
Secondly we prove Theorem 2, the G-steerability constraint for PDOs. Recall that the G-equivariance
condition for D(P) is
D(P)(g Bin f) = g Bout (D(P)f),	(119)
where f ∈ Fin and g ∈ G. If we write out the definition of the induced action B, this becomes
D(P)(ρin(g)f ◦ g-1) = PoUt⑺(DPf) ◦ g-1 .	(120)
We write P = Pα cαxα, where cα are constant matrix-valued coefficients cα ∈ Rcout×cin. The LHS
of Eq. (120) is then
D(P )(Pin(g)f ◦ g-1)= X CɑPin(g)∂α (f ◦ g-1 ) .	(121)
α
In Lemma 17, we will show that the last term in Eq. (121) is given by
∂α(f ◦ g-1) = ((g-T∂)α f) ◦ g-1,	(122)
28
Published as a conference paper at ICLR 2022
where g-T := (g-1)T and ∂ is understood as a vector that g-T acts on. Plugging this into Eq. (121),
we get
D(P)(Pin(g)f ◦ g_1) = X Ca ((g-TH)" ρin(g)f) ◦ g-1	(123)
α
for the LHS of Eq. (120). For the matrix of polynomials P = Pα cαxα, we now define
g ∙ P ：= X Ca(g-1 x)a,	(124)
α
which is again a matrix of polynomials, each one rotated by g. Then Eq. (123) can be written more
compactly as
D(P)(Pin(g)f ◦ g-1) = (D(gT ∙ P)Pin(g)f) ◦ g-1 .	(125)
Plugging this back into Eq. (120), canceling the g-1 and using the fact that this has to hold for all f,
we get
D(gτ ∙ P)Pin(g) = Pout(g)D(P)	(126)
as an equality of differential operators. We move the ρin(g) to the other side and use the fact that the
map D is bijective, which yields our final constraint
P(g-Tx) = ρout(g)P (x)ρin(g)-1 .	(127)
This concludes the proof of the G-steerability constraint for PDOs.
Finally, we prove the Lemma we just made use of, a higher-dimensional chain rule for the special
case we need:
Lemma 17. Let f ∈ C∞(Rd, Rc) and g ∈ GL(Rd). Then for any multi-index α,
∂a(f ◦ g-1) = ((g-τ∂)a f) ◦ g-1,	(128)
where g-τ := (g-1)T.
Proof. In general, for a linear map A ∈ GL(Rd), we have
∂iAj(x) = Aji ∀x .	(129)
Therefore,
∂i(f ◦ g-1) = X(∂jf) ◦ g-1 ∂i(g-1)j	(130)
j
=X(∂jf) ◦ g-1 ∙ (g-1 )ji	(131)
j
=(((gT)T ∂), f) ◦ g-1.	(132)
We can apply this iteratively to show that
∂a(f ◦ g-1 )= (((g-1 )T ∂)a f) ◦ g-1.	(133)
□
H	Transferring steerable kernel bases to steerable PDO bases
In this section, we develop the method presented in Section 3.2 in more detail and with proofs.
We fix a group G ≤ O(d) and representations ρin and ρout. Then we write K for the space of
G-steerable kernels. Because the steerability constraints for kernels and PDOs are identical in this
setting, the space of equivariant PDOs is the image under the isomorphism D of the intersection
Kpol := R[x1,. ..,xd] ∩K.
(134)
29
Published as a conference paper at ICLR 2022
In words, the space of equivariant PDOs is isomorphic to the space of polynomial steerable kernels.
Both spaces are infinite-dimensional real vector spaces. The question we tackle now is how we can
find a basis of Kpol given a basis of K under certain conditions.
It will vastly simplify our discussion to treat K and Kpol as modules over invariant kernels instead
of as real vector spaces, at least for now. A module is a generalization of a vector space, where the
scalars for scalar multiplication can form a ring instead of a field. Because the radial parts of steerable
kernels are unrestricted, it makes sense to think of K as a module over the ring of radial functions.
Formally:
Lemma 18. K is a C∞(R≥0)-module, with scalar multiplication defined by
(f κ)(x) := f |x|2 κ(x)	(135)
for κ ∈ K and f ∈ C∞(R≥0). Similarly, Kpol is an R[r2]-module, where r2 := x12 + . . . + x2d and
multiplication is simply multiplication of polynomials.
Proof. The steerability constraint
κ(gx) = ρout(g)κ(x)ρin(g)-1	(136)
is clearly R-linear and in particular, K is closed under addition and 0 ∈ K. Furthermore, for κ ∈ K,
(fκ)(gχ) = f(∣gχ∣2)κ(gχ)
=f (∣X∣2)Pout (g)κ(x)ρin(g)-1	(137)
= ρout(g)(f κ)(x)ρin(g)- ,
so K is also closed under the given scalar multiplication. The proof for Kpoi is exactly analogous. □
In Lemma 18 and in the following, we write r2 instead of |x|2 simply to emphasize its role as a
polynomial; so when reading r2, think of it as a polynomial, and when reading |x|2 simply as a
function of x.
A basis ofa module is defined analogously to a basis ofa vector space, as a set of linearly independent
vectors that span the entire module. However, linear combinations now allow coefficients in the ring
of radial functions, instead of only real numbers. This means that fewer vectors are needed to span
the entire space, because the coefficients “do more work”.
In contrast to vector spaces, not every module has a basis. K and Kpol do have a basis in the cases we
consider in the paper but for this section that doesn’t matter to us: we will simply assume that K has
a basis with certain properties and then transfer this basis to Kpol. That the method developed here is
indeed applicable to subgroups of O(2) and O(3) will be the topic of Appendix J.
We roughly proceed in two steps:
1.	We show that a basis of K (over C∞(R≥0)) that consists only of polynomials (and fulfills a
few other technical conditions) is also a basis of Kpol, but this time of course over R[r2].
2.	We then show how to turn this module basis into a vector space basis of Kpol.
The first step is formalized as follows:
Proposition 19. Let B ⊂ R[x1, . . . , xd]cout×cin be a basis of K such that no matrix of polynomials in
B is divisible by r2 and each one is homogeneous. Then B is also a basis of Kpol as an R[r2]-module.
Here, we say that b ∈ R[x1, . . . , xd]cout×cin is divisible by r2 if every component is divisible by r2 (as
a polynomial). We call it homogeneous if all its entries are homogeneous polynomials of the same
degree.
Proof. Linear independence is obvious: R[r2] ⊂ C∞ (R≥0), so if B is linearly independent over
C∞(R≥0), then it is also linearly independent over R[r2].
30
Published as a conference paper at ICLR 2022
To show that B generates Kpol, first let p ∈ Kpol be homogeneous of degree l. Because p ∈ K, there
are fi ∈ C∞(R≥0) and κi ∈ B such that
P(X) = X fi(∣x∣2)κi(x) .	(138)
i
We want to show that fi ∈ R[z], i.e. that fi is a polynomial. Since each κi is homogeneous of degree
li, we get
X fi(λ2∣x∣2)λliKi(x) = λ X fi(∣x∣2)κi(x)	(139)
ii
because of p(λx) = λlp(x) for any λ ≥ 0. Because the κi are linearly independent, we must have
fi(λ2∣x∣2) = λl-lifi(∣x∣2).	(140)
Thus, fi is homogeneous of degree l-li and as We show in Lemma 20 below, this implies
fi(z) = cz(l-li)/2 ,	(141)
or alternatively
fi(|x|2) = ci|x|l-li .	(142)
What remains to show is that l-li is a natural number, i.e. that l - Ii is even and non-negative. To
prove this, we divide all the i into two groups: those for which l - li is even and those for which it is
odd. Then we get an expression of the form
p(x) = |x| X akr2mk κk + X bkr2nk κk .	(143)
Each summand is a rational function (the quotient of two polynomials), so both sums are rational
functions. p(x) is also rational, so the first term on the RHS has to be rational. This is only possible
if ak = 0 for all k, otherwise we could divide by the (rational) sum and should get a rational function,
but |x| is not rational. This shows that l - li is even for all i.
Now let lmax := maxi li. Then
/、	PiciIxllmxτKi(X)	n...
P(X)=—产—-.	(144)
It is not possible to cancel any terms: because κi is not divisible by r2 and for one i, lmax - li = 0,
the enumerator is not divisible by r2 (as a polynomial). Since the denominator is a power of r2 , the
fraction can’t be simplified. But we know that P(x) is a polynomial. Therefore, lmax ≤ l.
In summary, we’ve shown that
fi(z) = ciz(l-li)/2	(145)
where l - li is even and non-negative. Therefore, all fi are polynomials.
Now recall that we assumed P to be homogeneous. But this is no significant restriction: we can write
any polynomial as a sum of homogeneous polynomials, each of which can be written as a linear
combination of the κi , as we just showed. Adding those up leads to a linear combination of the κi for
arbitrary polynomials. This complete the proof.	□
Lemma 20. Let f : R≥0 → R be homogeneous of degree l ∈ R, meaning that f (λx) = λlx for all
λ ≥ 0. Then f(z) = czl for some c ∈ R (and if l ∈ N, then f is a polynomial).
Note that this Lemma does not hold in higher dimensions - in general there are many more homoge-
neous functions than polynomials!
Proof. For any z ≥ 0,
f(z) = zlf(1) ,	(146)
which proves the claim by setting c := f (1).	□
31
Published as a conference paper at ICLR 2022
In Appendix J, we will show that the construction of angular basis elements described in Section 3.2
leads to a basis B of K with the properties required by Proposition 19. It then follows that this also
defines a basis of Kpol as a module. We now come to the second step, turning this module basis into a
vector space basis.
Proposition 21. Let B be a basis of Kpol as an R[r2]-module. Then
{r2kb∣k ∈ N≥o,b ∈ B}	(147)
is a basis of Kpol as a real vector space.
Proof. Let v ∈ Kpol. By assumption, there are then basis vectors b1, . . . , bn ∈ B and coefficients
p1 , . . . , pn ∈ R[r2] such that
n
v = Xpibi .	(148)
i=1
For each i = 1, . . . , n there are also real coefficients a(0i), . . . , a(mi)i such that
mi
pi = X a(ki)r2k .	(149)
k=0
Combining these equations, we get
n mi
v=XXa(ki)r2kbi.	(150)
This is a real linear combination of elements of the form r2kb for b ∈ B. So since v was arbitrary, the
set of such elements does indeed span Kpol.
To prove linear independence, let
n
Xair2kibi=0	(151)
i=0
for some choice of real coefficients ai. Now we only need to note that air2ki ∈ R[r2]. So we can
interpret this expression as a linear combination over R[r2]. Because B is a basis, it follows that
air2ki = 0 for all i, and thus a% = 0. That proves linear independence.	□
As a final note, we show that the condition in Proposition 19 that the basis elements are not divisible
by r2 is purely technical; any basis can easily be transformed into one that fulfills it in a canonical
way, as formalized by the following lemma. We do not formally need this result anywhere but it may
prove useful if this approach is extended to other groups G because it clarifies which parts of the
conditions in Proposition 19 are actually important.
Lemma 22. Let B ⊂ R[x1, . . . , xd]cout ×cin be a basis of K. For b ∈ B, we write b = r2kb0,
where k is chosen maximally such that there is an (automatically unique) polynomial matrix
b0 ∈ R[x1, . . . , xd]cout ×cin. Then B0 := {b0 | b ∈ B} is a basis of K and no b0 is divisible by r2.
Furthermore, if b is homogeneous, then so is b0.
Proof. First, note that b0 is in fact well-defined: For k = 0, b0 = b works, while for k > deg(b),no
fitting b0 exists7. So there is some maximal k ≥ 0 with the desired property. b0 is clearly unique,
namely b0 = |x|-2kb as functions on Rd. Furthermore, b0 is not divisible by r2 because k was chosen
maximally.
B0 is also clearly a generating set: κ = Pi fibi = Pi (fir2ki )b0i.
To prove linear independence, assume that
X fib0i = 0 .	(152)
i
7Here, the degree of a matrix of polynomials is the maximum of the degrees of all components (though in
this case, even if k is larger than the minimum degree, no b0 exists).
32
Published as a conference paper at ICLR 2022
We can write k := maxi ki and then get
0=r2kXfib0i=Xfir2(k-ki)bi.	(153)
ii
This means that
fir2(k-ki) = 0 ∀i,	(154)
which implies fi = 0 for all i, since r2(k-ki) is non-zero everywhere except in the origin and fi is
continuous. Therefore, B0 is linearly independent and hence a basis of K.	□
Now we can formulate Proposition 19 more generally: for any basis B of K consisting only of
homogeneous matrices of polynomials, the corresponding B0 is a basis of Kpol.
I	Solutions for important groups
This and the next two appendices describe the solutions of the PDO equivariance constraint for
subgroups of O(2) and O(3). In this appendix, we describe the general form of these solutions;
we recommend readers who are not interested in all the details focus on this one. Appendix J
contains proofs for some claims we make in this appendix where these proofs do not provide as
much insight. Finally, Appendix K contains tables with concrete solutions, it is mainly relevant to the
implementation of steerable PDOs.
I.1	SOLUTIONS FOR SUBGROUPS OF O(2)
To solve the steerability constraint for all (compact) subgroups G ≤ O(2) and for arbitrary represen-
tations ρin and ρout, Weiler & Cesa (2019) derive explicit bases only for irreducible representations,
since the general case can then easily be computed (see Appendix B for details). This works exactly
the same for PDOs as well.
The angular basis elements χβ that they describe for irreps are all matrices with entries of the form
cos(k夕)and sin(k夕)，where k differs between basis elements but is the same for all entries of one
matrix. For example, for G = SO(2), ρin the frequency n irrep, i.e. ρin (g) = gn, and ρout trivial, the
angular basis elements are
χι = (cos(ng), sin(n夕))	and	χ2 = (- sin(ng), cos(ng)) .	(155)
We will show how to find the steerable PDO basis using this example, but the method works exactly
the same in all cases. Tables with all explicit solutions can be found in Appendix K.
As described in Section 3.2, we now need to multiply these matrices with the smallest power of |x|
such that all entries become polynomials. We show in Appendix J that the necessary coefficient for
cos(n夕)and sin(n夕)is |x|n, so in the example above, we get
Xi = (|x|n cos(n2)，|x|n sin(n2))	and	X2 = (-|x|n sin(n2)，|x|n cos(n2)).
(156)
The entries are written in polar coordinates here, but they are in fact polynomials in the Cartesian
coordinates x1 and x2 . More precisely, we show in Appendix J that they are closely related to
Chebyshev polynomials, based on which we derive the following explicit expressions:
|x|n cos(n夕)= X(-1)i (n)xn-ix2 and |x|n sin(n夕)=X(-1)i+1 (n)xn-iχ2 .(157)
i≤n even	i≤n odd
This Cartesian form then allows us to interpret the polynomial as a differential operator, by applying
the ring isomorphism D, i.e. plugging in ∂1 and ∂2 for x1 and x2.
If we set n = 1 in our example above, which corresponds to a vector field, we get simply |x| cos(夕)=
xi and |x| sin(夕)=χ2, so we recover the angular PDO basis
D(Xi) = (∂ι, ∂2) = div and	D(X2) = (-∂2, ∂ι) = curl2D	(158)
that we already derived in Section 2.3. To get a complete basis, we combine these PDOs with powers
of the Laplacian.
33
Published as a conference paper at ICLR 2022
I.2	SOLUTIONS FOR O(3) AND SO(3)
We now turn to the other cases for which steerable kernel solutions have been published, namely O(3)
and SO(3). Like for O(2), we only need to consider pairs of irreducible representations. As described
in (Weiler et al., 2018a; Lang & Weiler, 2021), we can build the corresponding angular parts χβ out
of real spherical harmonics Ylm using Clebsch-Gordan coefficients. We show in Appendix J that we
can apply this procedure to ∣χ∣lYim instead of Ylm to obtain the corresponding Xβ. We then build the
basis by combining with powers of |x|2, as we described in Section 3.2 and already did for O(2). To
find the corresponding differential operators D(Xβ), We only need a Cartesian representation of the
polynomials |x|lYlm, which is fortunately well-known (Varshalovich et al., 1988). This then again
leads to a complete basis of the space of steerable PDOs, With the same general form containing
poWers of the Laplacian.
J PROOFS FOR SOLUTIONS FOR SUBGROUPS OF O(2) AND O(3)
In this section, We apply the method from Appendix H to subgroups of O(2) and O(3), by describing
bases for the space of steerable kernels that satisfy the conditions of Proposition 19.
J.1 O(2)
As described in Appendix I.1, it is sufficient to consider irreducible representations ρin and ρout and
for these representations, the angular part of the kernel has a basis consisting of matrices With entries
of the form cos(n夕)and sin(n夕)(where n ∈ Z). Crucially, n is the same for all entries of one matrix
(though it may differ betWeen basis elements).
We claim that multiplying each angular basis element with r|n| gives a basis of the space of poly-
nomial steerable kernels Kpol (as a R[r2]-module), as described in Appendix H. This follows from
Proposition 19 if we can prove that r|n| cos(n夕)and r|n| sin(n夕)are
(i)	polynomials (in x, y),
(ii)	homogeneous (of degree n),
(iii)	and not divisible by r2 .
To show that r|n| cos(n夕)and r|n| sin(n夕)are polynomials, we use Chebyshev polynomials, which
can among other things be seen as a generalization of the addition theorems for sin(2夕)and cos(2夕).
Specifically, they are families of polynomials Tn (first kind) and Un (second kind) defined for n ≥ 0
with the property that
cos(n夕)=Tn(Cos 夕)	(159)
sin(n夕)= Un-ι(cos φ) sin 夕.	(160)
We extend the definition to negative n by setting
T-n := Tn	forn ≥ 0	(161)
U-1 := 0	(162)
U-n-1 := -Un-1	forn ≥ 1.	(163)
Then Eq. (159) holds for all n ∈ Z, as follows immediately from the parity of sin and cos.
Motivated by the close relation to Chebyshev polynomials, we then define the following notation for
the matrix entries we are considering:
Tn :二 r|n|Tn(Cos φ) = r|n| cos(nq)	(164)
Un= r|n| sin(2)Un-ι(cosφ) = r|n| sin(n2).	(165)
We now prove claim (i), that Tn and Un are polynomials in the Cartesian coordinates x, y, by using a
few well-known facts about Chebyshev polynomials: First, Tn has degree |n|, so the highest order
term in Tn is r|n| (Cos 夕)|n| = x|n|. Similarly, Un-ι has degree |n| 一 1 and the highest order term
34
Published as a conference paper at ICLR 2022
in Un is thus r|n| (Cos 夕)|n|-1 Sin φ = x|n|-1y. Lower order terms have additional powers of r that
aren’t “matched” by a cosine or sine. But the second fact about Chebyshev polynomials is that they
are either even or odd, so all of these powers of r are even (i.e. powers of r2) and thus themselves
polynomials.
That Tn and Un are homogeneous of degree n - claim (ii) - is immediately clear from their definition.
It remains to show claim (iii), that they are not divisible by r2. For that, we use the following Lemma:
Lemma 23. Let p be a non-zero, harmonic, homogeneous polynomial. Then p is not divisible by r2.
Here, p is harmonic if its Laplacian vanishes.
Proof. For any homogeneous polynomial p, there is a unique decomposition
p = r2q + h ,	(166)
such that h is harmonic and homogeneous. Since q = 0, h = p is one such decomposition for p
harmonic, there is no solution with h = 0 (unless p = 0). So non-zero homogeneous harmonic
polynomials are not divisible by r2.	□
Now we only need to show that Tn and Un are in fact harmonic. This can be done with a brief
calculation in polar coordinates:
∆Tn = (∂2 + ∣∂r + r12 dŋ (rn cos(n2))
=n(n — 1)rn-2 cos(nφ) + nrn-2 cos(nφ) — n2rn-2 cos(nφ)
=0.
EI	1	1 1 Γ∙ τ`τ
The same holds for Un :
△Un = (∂2 + ∣∂r + J∂j (rn sin(n°))
=n(n — 1)rn-2 sin(ng) + nrn-2 sin(ng) — n2rn-2 sin(ng)
=0.
(167)
(168)
In summary, we have shown that Tn and Un are homogeneous polynomials not divisible by r2, which
makes Proposition 19 applicable.
In order to make the basis practically applicable, we also give explicit Cartesian expressions for Tn
and Un. We restrict ourselves to non-negative n to keep the notation less cluttered. Equation (161)
immediately gives the case for n < 0. An explicit formula for the Chebyshev polynomials of the first
kind is
b 2 C
Tn(x) =
k=0
2k	(χ2- ι)kχn-2k
2
(169)
It follows that
= ________ 、
Tn = r Tn(Cos 夕)
=Pχ2 + y2n Tn p 2 2x 2
x2 + y2
b 2C /	∖
x QkM一
k=0
(170)
b 2C /、
X 2nk (—1)ky2kxn-2k
35
Published as a conference paper at ICLR 2022
Similarly, the Chebyshev polynomials of the second kind are given by
b 2 C /一、
Un(X) = X Ck + 1)(x2 - 1)fc xn-2k
k=0
which means that
n
Un = rn Sin φUn-ι(cos φ)
ypx2 + y2n 1 Un-1 ( / ；
x2 + y2
bn-1C /	、
y X 2kn+ι 5"一2
k=0
(171)
(172)
bn-1C /	\
X	2kn+ 1 (-1)ky2k+1xn-(2k+1)
J.2 O(3)
As already mentioned in Appendix I.2, for O(3) and SO(3), the irreps angular basis between
irreducible representation can be built by combining real spherical harmonics Ylm using Clebsch-
Gordan coefficients. We refer to Weiler et al. (2018a); Lang & Weiler (2021) for details on how this
works since it is exactly the same procedure whether one uses kernels or PDOs. The only fact we
need to know here is that each basis element is built from spherical harmonics with the same l.
The necessary steps are now very similar to those for O(2) and its subgroups. We will use rlYlm
where We had Tn and Un for O(2). Here it becomes important that inside one basis element, only
one l appears, because this means we can multiply the entire matrix of polynomials by rl .
We would then need to show that these are homogeneous polynomials not divisible by r2 but for
spherical harmonics, itis already very well known that rlYlm are homogeneous harmonic polynomials
of degree l; Lemma 23 then implies that they are not divisible by r2. For explicit Cartesian expressions,
we refer to e.g. Varshalovich et al. (1988).
K S olution tables
Using the results from Appendices H and J, we now very easily get complete bases for all subgroups
of O(2) and for all irreducible representations by transferring the solutions by Weiler & Cesa (2019).
The appendix of (Weiler & Cesa, 2019) contains tables with all kernel solutions; we simply replace
every term of the form cos(k夕)with Tk and sιn(k夕)with Uk to get the following tables.
For the sake of readability, we only write the polynomials Tk and Uk inside the tables, though the
PDOs themselves should of course be D(Tk) and D(Uk). Explicit formulas for Tk and Uk were
given in Eq. (157).
All bases described here are module bases for the space of steerable PDOs as an R[∆]-module. See
Section 3.2 and Appendix H for details.
K.1 SPECIAL ORTHOGONAL GROUP SO(2)
The irreducible representations of SO(2) are the trivial representation ψ0 and those of the form
ψk : SO(2) → GL(R2),	g7→gk.	(173)
The bases for all the combinations of these irreps are:
36
Published as a conference paper at ICLR 2022
ψo
ψn, n ∈ N>0
ψm,
m ∈ N>o
〜
Um+n
-Tm+n
Um+n
Tm+n
+n
+n
K.2 ORTHOGONAL GROUP O(2)
Elements of O(2) can be written as a tuple (r, s) consisting of a rotation r ∈ SO(2) and a flip
s ∈ {±1}. The irreducible representations with frequency k > 0 are similar to those for SO(2), only
with the additional flip:
ψ1,k : SO(2) → GL(R2),	(r, s) 7→ rk ◦ s .	(174)
Here, s is understood as a map acting on R2 either as the identity or by flipping along a certain axis.
In contrast to SO(2), there are now two irreducible representations for k = 0, namely the trivial
representation ψ0,0(r, s) = 1 and the representation ψ1,0(r, s) = s. The solutions for all possible
combinations of these irreducible representations are as follows:
^∖In Out"∖^	ψ0,0	ψ1,0	Ψ1,n, n	∈ N>o
ψo,0	(1)	0	(-Un	Tn)
ψ1,0 ψ1,m, m ∈ N>o	0 (-Tm) m	(1) Tm Um)	(Tn Tm-n	-Um-n Um-n	Tm-n),	Un ) Tm+n	Um+ n 万	T Um+n	-Tm+n
K. 3 REFLECTION GROUP ±1
The reflection group has only two irreducible representations: the trivial representation ψ0 and the
representation ψ1 (s) = s. Both are one-dimensional, so all the PDOs are only 1 × 1 matrices:
To get the full basis, μ needs to take on all natural numbers (in practice, We use all μ UP to some
maximum value). Note that we assume that the reflection is with respect to the x1-axis, both here and
later for DN. To get other reflection axes, the PDOs simply need to be rotated.
K.4 CYCLIC GROUP CN
The irreducible representations of CN are the same as those of SO(2) but only up to a frequency of
k = b N-1 C. If N is even, there is an additional one-dimensional irreducible representation, namely
Ψn∕2(Θ) = cos
x-r τ . 1	, , 1 1' 11	1	1	I	r-∏ t 个	T∖T
We then get the folloWing solutions, Where t ranges over Z and t over N:
(175)
37
Published as a conference paper at ICLR 2022
Ψn∕2 (if N even)
ψn,	1 ≤ n < N/2
ψo
⑵N ),	(TR+1∕2)n),	(一力n+tN	Tn∙+tN) ,
0N )	UO+1/2)N)	(Tn+tN Gn+tN)
ψι (N even)
%+∖∕2)N 卜	⑵N),
(uu(⅛+1∕2)N )	ON )
(-Un+(t+1∕2)N 亍n+(t+1∕2)N),
(卷+(t+1∕2)N	Un+(t+1∕2)n)
ψn
1 ≤ n <N/2
— Um+tN
,^m+tN,
m+tN
Um+tN
(-Um+(t+1∕2)N
∖ 宁m+(t + 1∕2)N.
(Tm+(t+1∕2)N)
∖Um+(t+1∕2')N
(Tm-n+tN
'Um-n+tN
m+n+tN
m+n+tN
m-n+tN
Tm-n+tN
m-n+tN
m-n+tN
m+n+tN
—Tm+n+tN
— Um+n+tN
Tm+n+tN
m-n+tN
m-n+tN
m+n+tN
m+n+tN
K.5 Dihedral group DN
Similarly to CN, the irreducible representations of DN are the same as those of O(2) up to a frequency
of k = [ N-1 C. If N is even, there are two additional one-dimensional irreducible representations,
namely
(176)
The solutions are (again with t ∈ Z and t ∈ N):
∖.1n Out∖^	ψ0,0	ψ1,0	Ψ0,N∕2 (N even)	Ψ1,N∕2 (N even)	ψι,n,	1 ≤ n<N∕2	
ψ0,0	(TN )	MN )	〔T(£+i/2)n1	(U(t +1/2)Nl	(—Un+tN	Tn+tN )
ψ1,0	(UN )	(TN )	(U(t +1/2)Nl	[t⅛+1∕2)n1	(T+tN	Un+tN)
ψ0,N∕2, (N even)	[T⅛ 十1/2)Nj	(U(£+1/2)Nj	(TN )	(UN )	(一 Un 十(t +1/2)N	T+(t +1/2)N)
ψ1,N∕2, (N even)	(¼⅛+1∕2)n)	(⅛ 十1/2)Nl	(UN )	(TN )	(亍n+(t +1/2)N	Un 十(t +1/2)N)
ψ1,m, 1 ≤ m ≤					( l^m∙-n+tN	一UUm-n+tN A
	—Um+tN	(Tm+tN ʌ	-gm 十(t +1/2)N)	Tm+(t +i/2)n	∖Um-n+tN	Tm-n+tN
N/2	Tm + tN	Um+tN	∖ Tm+(t+1∕2)N	∖Urn⅛(t +1/2)N	Tm+n+tN	Um+n+tN
					Um+n+tN	一Tm∙+n+tN J
L DISCRETIZATION methods for PDOS
L.1 Finite differences
Finite difference methods are common in machine learning; for example, the discretization of 去 as
[—1 1] or [—1 0 1], or of S as [1 —2 1] all use finite difference methods. To understand
where these filters come from, we need the following well-known result:
Proposition 24. Let xι,... ,xn ∈ R be arbitrary but distinct grid points. Thenfor m ≤ N 一 1,
there are unique coefficients Wnm) such that the approximation
1N
f(m)(0) ≈ hm EWnm)f(hxn)	(177)
n=1
38
Published as a conference paper at ICLR 2022
has an error O(hN-m) for any f ∈ CN (R).
The coefficients wn(m) are called finite difference coefficients and approximating derivatives using
Eq. (177) is the finite difference method. We will soon describe how to generalize this to higher
dimensions as well.
We remark that O(hN-m) is an asymptotic upper bound on the error, and it can sometimes be
lower, even for all f. For example, the central difference discretization of 为 as [1 -2 1] uses
N = 3 grid points but still achieves an error of O(h2), rather than O(h). For details on when such a
“boosted” order of accuracy occurs, see (Sadiq & Viswanath, 2014).
Note that Eq. (177) can be generalized to
1N
f (m)(x) ≈ 而 Ewnm)f(hxn + x) .	(178)
hm	n
n=1
This follow immediately because the coefficients wn(m) don’t depend on the function f, so we can
apply Proposition 24 to τ-xf.
Particularly interesting for us is the case of a regular grid. We can use infinitely many grid points
xn ∈ Z as long as we demand that wn(m) is zero for almost all n. Then we get
f ((U)(X) ≈ hm X wnm)f (hn + x).	(179)
n
Fixing h = 1, this is exactly the cross correlation
f(m) ≈ w(m) ?f	(180)
if we interpret w(m) as a function n 7→ wn(m). This is why, in the end, we discretize a derivative by
convolving with some stencil, such as [1	-2 1], at least on a regular 1D grid.
Generalizing Proposition 24 to higher dimensions does not work in a straightforward way. However,
if we restrict ourselves to regular grids, then finite difference methods can be easily applied to PDOs.
The idea is very simple: a PDO such as ∂x∂y2 can be interpreted as first applying ∂y2 and then ∂x (or
the other way around). So we discretize each of these with the one-dimensional finite difference
method described before, and then we convolve with both filters one after the other.8 We can also
combine the two one-dimensional filter into one two-dimensional filter, the outer product of the two.9
The asymptotic error of this discretization will simply be the highest asymptotic error along all the
dimensions, so we get similar guarantees.
L.2 RBF-FD
As mentioned, Proposition 24 does not directly generalize to higher dimensions. So to discretize a
PDO on arbitrary point clouds in higher dimensions, a somewhat different approach is needed.
RBF-FD is one such method and works as follows: we still want to approximate a derivative using
N
∂αf(0) ≈ Xwnαf(xn) ,	(181)
n=1
similar to finite difference methods. Here, xn ∈ Rd are arbitrary (but again distinct) points. The idea
is now that we require this approximation to be exact if f (x)=夕(Ilx - Xnl∣), where 夕 is an arbitrary
but fixed radial basis function. In words, the approximation should become exact for a certain radial
basis function centered on any of the points xn . This leads to a linear system, which is solved for the
coefficients wnα.10 For more details on both finite differences and RBF-FD, see for example Fornberg
& Flyer (2015).
8As we have seen, finite difference methods can most immediately be seen as performing a cross-correlation
rather than a convolution. However, we can easily switch to convolutions by flipping the filter.
9For a simple PDO such as ∂x∂y2 , this may be undesirable for computational reasons. But in practice, we
have PDOs that are sums of such pure terms and thus don’t factorize.
10In practice, one often solves an extended linear system containing additional low-order polynomials, but we
won’t discuss that here.
39
Published as a conference paper at ICLR 2022
L.3 Gaussian derivatives
Discretizing PDOs using derivatives of Gaussians is very simple to describe: given grid points
xn ∈ Rd , we approximate using
N
∂αf(0) ≈ X (∂αG(xn; σ)) f(xn)	(182)
n=1
where G(x; σ) is a Gaussian kernel with standard deviation σ centered around 0. σ is a free parameter;
larger σ will lead to a stronger denoising effect.
On regular grids, this again turns into a cross-correlation, with the filter being the derivative
∂αG(xn; σ) evaluated on the grid coordinates.
In Appendix E we briefly touch on a possible interpretation of this discretization method using the
distributional framework for PDOs.
M	Relation to PDO-eConvs
In this section, we describe how PDO-eConvs (Shen et al., 2020) fit into the framework of steerable
PDOs. We mostly follow the original notation from Shen et al. (2020) when describing PDO-eConvs
but do make some minor changes to avoid clashes and confusion with our own notation.
As in our presentation, Shen et al. (2020) use polynomials to describe PDOs. One difference is that
they never explicitly use matrices of polynomials, because they model the feature space somewhat
differently (which we will discuss in a moment). They write H for the polynomial describing a PDO
(where we would write e.g. p) and write χ(A) for the corresponding PDO transformed by A ∈ O(d).
In our notation,
X(A) := D(A ∙ H)= D(H ◦ AT).	(183)
PDO-eConvs use two types of PDO layers. The first one, Ψ, can be interpreted as a steerable PDO
with ρin trivial and ρout regular. It maps the scalar input to the network to the internally used regular
representation. The second layer type, Φ maps between regular representations and is used for hidden
layers. At the end, pooling is performed to obtain a scalar output again.
The first layer type is defined as
Ψ : C∞(Rd) → C∞(E(d)),	Ψ(f )(x, A) := (X(A)f )(x).	(184)
Here, E(d) := Rd o S with S ≤ O(d); in practice, S needs to be a finite subgroup, i.e. CN or DN.
Elements of E(d) can be uniquely written as (x, A) with X ∈ Rd and A ∈ S.
There is an obvious bijection C∞(E(d)) = C∞(Rd, Rc), where C := |S| is the order of S, i.e. the
number of group elements. Concretely, we define
Θ: C ∞ (E(d)) → C ∞(Rd, Rc), f → (x- (f(x,Aι),...,f(x,Ac)))	(185)
where A1 , . . . , Ac is an enumeration of the group elements of S. We will therefore interpret
C∞(E(d)) as a c-dimensional feature field over Rd, and we will show that using regular repre-
sentations for this field (and trivial ones for the input) makes the PDO-eConv layers equivariant and
thus steerable PDOs.
First, note that under the Θ bijection, the first PDO-eConv layer type Ψ becomes a c × 1 matrix of
PDOs, namely
What we mean by this is that the diagram
40
Published as a conference paper at ICLR 2022
C∞(Rd) -----ψ——> C∞(E(d))
Il h
C∞(Rd) —D(Hψ)	> C∞(Rd, Rc)
commutes. Concretely, we have
Ψ(f)(x, Ai) = (χ(Ai)f)(x) = (D(HΨ)f)(x)i.	(187)
So we need to check whether HΨ satisfies the PDO steerability constraint for trivial to regular PDOs:
HΨ(Ax) = ρregular(A)HΨ(x) .	(188)
Using the definition of ρregular(A), we can rewrite the RHS as
c
ρregular (A)HΨ (x) =	ρregular (A)Hψ (x)k eAk
k=1
c
=X(Ak ∙ H)(X)eAAk
k=c1	(189)
= X H(Ak-1x)eAAk
k=1
c
= X H(Al-1Ax)eAl .
l=1
Here, we use basis vectors eA1 , . . . , eAc for Rc, with the same enumeration A1, . . . , Ac of S used to
define Θ. For the final step, we reparameterized the sum with Al := AAk.
The LHS of Eq. (188) can be written as
HΨ(Ax) = (Ai ∙ H(Ax),	..., Ac ∙ H(Ax))T
=(H(A-1Ax), ..., H(A-IAx))T .
(190)
This is the same as the final term in Eq. (189), which proves that the PDO steerability constraint is
satisfied.
The second PDO-eConv layer type, mapping between regular representations, is defined as
c
Φ : C∞(E(d)) → C∞(E(d)), Φ(e)(x, A) ：= X (χAA)e) (x, AAj).	(⑼)
XAA) are C different PDOs and they act on e ∈ C∞(E(d)) by acting on each of the C components
separately. Under the Θ bijection, Φ becomes
c
Θ(Φ(e))(x)i =X χ(AAji)e (x,AiAj)
j=1
c
= X χ(AAji)Θ(e)AiAj (x)	(192)
j=1
c
= X χ(AAi-i1)AjΘ(e)Aj (x) .
j=1	i
We can therefore represent it as a C × C PDO D(HΦ) with
(HΦ)ij (x) = HAi-1Aj(Ai-1x) =: Hij(x) ,	(193)
where HA-1A is the polynomial that induces χ(AI-) 1A . This makes the diagram
41
Published as a conference paper at ICLR 2022
C∞(E(d))----------φ------> C∞(E(d))
θ	Iθ
C∞(Rd, Rc) —D(H > C∞(Rd, Rc)
commute, similar to the case discussed above. So again, we need to check that HΦ satisfies the PDO
steerability constraint, this time for ρin and ρout both regular:
HΦ(Ax) = ρregular(A)HΦ(x)ρregular(A-1) .	(194)
Writing out HΨ in its components, this becomes
HΨ (Ax) = X ρregular(A)Hij (x)eAi eAj ρregular(A)
i,j
(=1) X Hij (x)eAAi eTAAj
i,j
=	HAi-1Aj (Ai-1x)eAAieTAAj
i,j	(195)
= X HAi-1AA-1Aj (Ai-1Ax)eAieTAj
i,j
= X HAi-1Aj (Ai-1Ax)eAieTAj
i,j
=	Hij (Ax)eAi eTAj .
i,j
The first and the last term are the same, just written out in components on the RHS, so the steerability
constraint is again satisfied. For (1), we used that ρregular(A) is orthogonal, and thus
ej ρregular (A)	= ej ρregular (A) = (ρregular (A)ej ) .	(196)
(2) was again a reparameterization of the sum, with AAi 7→ Ai and AAj 7→ Aj . The other steps are
only simplifications and plugging in definitions.
In conclusion, we have shown that there is a simple bijection between the feature spaces used for
PDO-eConv hidden layers and the feature fields we use, and that under this bijection, PDO-eConvs
correspond to steerable PDOs with regular representations (and trivial representations for the input).
It is relatively easy to adapt the argument we present for the converse direction: every equivariant
PDO between two regular feature fields (or from a scalar to a regular one) can be interpreted as a
PDO-eConv layer.
N	Additional experimental results
N.1 Fluid flow prediction
While our classification experiments on MNIST-rot and STL-10 make use of various representations
inside the networks, the input and output are always trivial. To showcase the flexibility of the steerable
PDO framework, we also conduct a small experiment in fluid flow prediction, where the output
contains a vector field.
We use the dataset provided by Ribeiro et al. (2020). It contains simulated laminar fluid flows
around various objects. The inputs are all scalar fields describing the geometry of the problem. The
outputs are the velocity and pressure field, the former of which is a vector field, and thus requires
vector representations to get the correct equivariance (rather than only trivial and regular ones like
PDO-eConvs support). In the original dataset, the fluid always flows into a tube from the same
direction. To better demonstrate the effects of equivariance, we randomly rotate each sample by
a multiple of 2, making the task somewhat more challenging. During training, the rotations are
42
Published as a conference paper at ICLR 2022
Table 3: Mean squared test error for prediction of velocity and pressure of laminar flow around
different objects.
Method	Equivariance	MSE
Kernel	—	3.20 ± 0.22
	C8	2.26 ± 0.14
PDO	—	2.75 ± 0.21
	C8	2.32 ± 0.08
different in each epoch to avoid disadvantaging the non-equivariant networks (i.e. we effectively use
data augmentation). To ensure all inputs have the same shape, we pad the original samples with zeros
to make them square before rotating them.
Our architecture and hyperparameters follow those of Ribeiro et al. (2020). The only exception is
that we use a single decoder for the vector field (since we treat it as a single vector-valued output
for the purposes of equivariance), whereas Ribeiro et al. (2020) used separate decoders for the two
vector components in some of their experiments. Just as in the STL-10 experiments, we perform
no hyperparameter tuning and use the hyperparameters that Ribeiro et al. (2020) optimized for the
non-equivariant network. To make the network equivariant, we replace the usual convolutions with
C8-equivariant steerable kernels or PDOs. We chose the channel sizes such that all networks had
approximately the same number of parameters (slightly over 800k).
Table 3 shows a clear advantage of the equivariant networks over the non-equivariant ones.11 Steerable
PDOs perform slightly worse than steerable kernels, though the difference is within the error intervals.
They still perform clearly better than any non-equivariant method. The PDO results are based on
Gaussian discretization, since that performed best in our other experiments.
N.2 Equivariance errors
To check the equivariance error—and indirectly the discretization error, since at least equivariant
layers have zero equivariance error in the continuous setting—, we checked how much rotating an
input image changes the output of a layer, compared to what the output should be under perfect
equivariance. The challenge here is that rotating a discrete image itself introduces some errors. To
minimize those, we used a large high-dimensional image, rotated it, and then scaled it down before
passing it into the layer, and scaled down again after that. We compared the result of this procedure to
what we get by first downscaling, then applying a convolutional or PDO layer, then rotating, and then
downscaling again. Effectively, all rotations thus happen at large resolutions, which should minimize
artifacts.
Table 4 shows the relative equivariance errors (as multiples of 1e-6). These errors are for randomly
initialized layers (averaged over 10 initializations). As discussed in the main text, the asymptotic
error bound for finite difference discretization does not lead to a particularly low error in practice.
N.3 Restriction experiments
Table 5 shows additional results on MNIST-rot. The general architecture and hyperparameters are the
same as in the experiments in Section 4 with regular representations, using our basis. However, in the
experiments in this section, the first five layers are D16-equivariant, while the final PDO/convolutional
layer is C16-equivariant. The motivation for this is that while the input images do not have global
reflectional symmetry, such symmetry occurs on smaller scales, so that stronger equivariance in
earlier layers might be helpful.
However, we don’t observe clear improvements over pure C16-equivariance. A reason could be that
even the C16-equivariant networks are already very parameter efficient compared to classical CNNs,
so that parameter efficiency and equivariance are not a bottleneck anymore. It is also possible that the
architecture would need to be adapted slightly to profit from the D16-equivariant layers.
11Note that our non-equivariant performance is significantly worse than the one obtained by Ribeiro et al.
(2020)—this is because we randomly rotated the samples, resulting in a more challenging task.
43
Published as a conference paper at ICLR 2022
Table 4: Relative equivariance errors for C16 on a test image, averaged over 10 random initializations of the
layer. As orientation, we also include non-equivariant (vanilla) convolutions.
Method	StenCil	Error [1e-6]
Vanilla	3 × 3	32716 ± 5484
convolution	5×5	32785 ± 5620
Kernels	3 X 3	5.0 ± 1.0
	5×5	5.0 ± 1.3
FD	3 × 3	4.8 ± 1.1
	5×5	6.6 ± 1.2
RBF-FD	3 × 3	5.9 ± 1.2
	5×5	6.1 ± 1.1
Gauss	3 × 3	4.9 ± 1.7
	5×5	6.4 ± 0.8
Table 5: MNIST-rot results with restriction from D16 to C16 equivariance. Test errors ± standard deviations
are averaged over six runs. See main text for details on the models.
Method	StenCil	Error [%]	Params
Tc am ol c Kernels	3×3	0.717 ± 0.022	709K
	5×5	0.710 ± 0.020	1.1M
	3×3	1.248 ± 0.060	709K
FD	5×5	1.436 ± 0.063	947K
oτjT7 TT r>	3×3	1.396 ± 0.059	709K
RBF-FD	5×5	1.565 ± 0.048	947K
Gauss	3×3	0.806 ± 0.047	709K
	5×5	0.778 ± 0.051	947K
44
Published as a conference paper at ICLR 2022
£ ɑu-lu-mɑ SSneo
(a) 3 × 3 stencils
-φ⊂⅛^- £ QM--M-S SSneo
(b) 5 × 5 stencils
Figure 4: Stencils of basis filters for a trivial to regular layer. Each row is a different method and
contains six arbitrarily selected filters from the basis (which is not the entire basis). All the settings
are those that were actually used for the MNIST-rot experiments.
N.4 Stencil images
Figure 4 contains examples of stencils used during the MNIST-rot experiments. For the 3 × 3 stencil,
the different methods yield qualitatively similar results (though FD and RBF-FD have fewer stencils
that make use of the four corners). But for 5 × 5 stencils, kernels and Gauss discretization make
significantly more use of the outer stencil points than FD and RBF-FD.
45
Published as a conference paper at ICLR 2022
Table 6: Architecture for MNIST-rot experiments
Layer	Output fields
Conv block	16
Conv block	24
Max pooling Conv block	32
Conv block	32
Max pooling Conv block	48
Conv block	64
Group pooling Global average pooling Fully connected	64
Fully connected + Softmax	10
O Details on experiments
O. 1 MNIST-rot experiments
For the MNIST-rot experiments, we use an architecture similar to one from (Weiler & Cesa, 2019).
Table 6 contains a listing of all the layers. Each conv block consists of a 3 × 3 or 5 × 5 steerable layer,
either convolutional or a PDO, followed by batch-normalization and an ELU nonlinearity. The output
fields are the number of C16-regular feature fields that is used; in the case of the Vanilla CNN and for
quotient representations, the number of fields is adjusted so that the parameter count is approximately
preserved.
For the quotient experiments, We use 5ρreguiar ㊉ 2ρC16/C2 ㊉ 2PC16/C4 ㊉ 4P廿所加 as the representation,
where the numbers are scaled to reach the same parameter count as the model with only regular
representations. This combination of representations is the same one used by Weiler & Cesa (2019)
and We refer to their appendix for motivation on Why We need to combine different representations
When using quotients.
We trained all MNIST-rot models for 30 epochs With Adam (Kingma & Ba, 2015) and a batch
size of 64. The training data Was normalized and augmented using random rotations. For the final
training runs, We used the entire set of 12k training plus validation images, as is common practice on
MNIST-rot. The initial learning rate Was 0.05, Which Was decayed exponentially after a burn-in of
5 epochs at a rate of 0.7 per epoch. We used a dropout of 0.5 after the fully connected layer, and a
Weight decay of 1e-7.
These hyperparameters are based on those used in (Weiler & Cesa, 2019); the main difference is that
We use another learning rate schedule, Which Works better than the original one for all models.
For the Gaussian discretization models, We use a standard deviation of σ = 1 for 3 × 3 stencils and
σ = 1.3 for 5 × 5 stencils; We chose these values by visual inspection of the stencils, With the aim
that full use is made of the stencil (see Appendix N). The RBF-FD discretization uses third-order
polyharmonic basis functions, i.e.夕(r) = r3.
O.2 STL-10 experiments
For the STL-10 experiments, We used exactly the same architecture and hyperparameters as Weiler &
Cesa (2019), Which in turn are essentially those of DeVries & Taylor (2017). This means We train
a Wide-ResNet-16-8 (Zagoruyko & Komodakis, 2016) for 1000 epochs, With SGD and Nesterov
momentum of 0.9, a batch size of 128 and Weight decay of 5e-4. We begin With a learning rate
of 0.1 and divide it by 5 after 300, 400, 600 and 800 epochs. For data augmentation, We pad the
image by 12 pixels, then randomly crop a 96 × 96 pixel patch, randomly flip horizontally, and apply
Cutout (DeVries & Taylor, 2017) With a cutout size of 60 × 60 pixels.
46
Published as a conference paper at ICLR 2022
O.3 Computational requirements
We performed our experiments on an internal cluster with a GeForce RTX 2080 Ti and 6 CPU cores.
A single run of an MNIST-rot model took about 12 minutes and a run of the STL-10 model about
5.5h. Training the fluid flow prediction model took about 45 minutes. Multiplying this by the number
of experiments we did and by six runs with different seeds, the MNIST-rot results took about 34
hours to produce, the STL-10 results about 264 hours, and the fluid flow results about 18 hours. The
initial tests we did to debug and find a good learning rate schedule (on MNIST-rot) took much less
time than that. So we estimate that producing this paper took around 350 GPU-hours on the GeForce
RTX 2080 Ti.
47