Published as a conference paper at ICLR 2022
PEARL: Data Synthesis via Private Embeddings
and Adversarial Reconstruction Learning
Seng Pei Liew, Tsubasa Takahashi, Michihiko Ueno
LINE Corporation
{sengpei.liew,tsubasa.takahashi,michihiko.ueno}@linecorp.com
Ab stract
We propose a new framework of synthesizing data using deep generative models in
a differentially private manner. Within our framework, sensitive data are sanitized
with rigorous privacy guarantees in a one-shot fashion, such that training deep
generative models is possible without re-using the original data. Hence, no extra
privacy costs or model constraints are incurred, in contrast to popular gradient
sanitization approaches, which, among other issues, cause degradation in privacy
guarantees as the training iteration increases. We demonstrate a realization of
our framework by making use of the characteristic function and an adversarial
re-weighting objective, which are of independent interest as well. Our proposal
has theoretical guarantees of performance, and empirical evaluations on multiple
datasets show that our approach outperforms other methods at reasonable levels of
privacy.
1	Introduction
Synthesizing data under differential privacy (DP) (Dwork (2006; 2011); Dwork et al. (2014)) enables
us to share the synthetic data and generative model with rigorous privacy guarantees. Particularly,
DP approaches of data synthesis involving the use of deep generative models have received attention
lately (Takagi et al. (2021); Xie et al. (2018); Torkzadehmahani et al. (2019); Frigerio et al. (2019);
Jordon et al. (2019); Chen et al. (2020); Harder et al. (2021)).
Typically, the training of such models utilizes gradient sanitization techniques (Abadi et al. (2016))
that add noises to the gradient updates to preserve privacy. While such methods are conducive to
deep learning, due to composability, each access to data leads to degradation in privacy guarantees,
and as a result, the training iteration is limited by the privacy budget. Recently, Harder et al. (2021)
has proposed DP-MERF, which first represents the sensitive data as random features in a DP manner
and then learns a generator by minimizing the discrepancy between the (fixed) representation and
generated data points. DP-MERF can iterate the learning process of the generator without further
consuming the privacy budget; however, it is limited in the learning and generalization capabilities
due to its fixed representation. In this work, we seek a strategy of training deep generative models
privately that is able to resolve the aforementioned shortcomings, and is practical in terms of privacy
(e.g., usable image data at ' 1.)
We propose a private learning framework called PEARL (Private Embeddings and Adversarial
Reconstruction Learning). In this framework, we have i) no limitation in learning iterations, and
ii) well-reconstruction capability. Towards those preferable properties, our framework first obtains
(1) informative embedding of sensitive data and (2) auxiliary information (e.g., hyperparameters)
useful for training, both in a differentially private manner, then (3) the generative model is trained
implicitly like GANs via the private embedding and auiliary information, where the learning is based
on a stochastic procedure that generates data, and (4) a critic distinguishing between the real and
generated data. The overview of PEARL is illustrated in Fig. 1.
As a concrete realization of PEARL, We first identify that the characteristic function (CF) representa-
tion of data can be sanitized as the private embedding of PEARL. Consequently, it is possible to train
deep generative models using an appropriately defined metric measuring the discrepancy between the
real (but sanitized) and generated data distribution based on the CF without re-using the original data.
As will be explained in detail in later Sections, the generative modelling approach using CFs also
1
Published as a conference paper at ICLR 2022
Figure 1: PEARL is a private learning framework that has i) no limitation in learning iterations, and
ii) well-reconstruction capability. Towards those preferable properties, our framework first obtains (1)
private embedding and (2) auxiliary information from the sensitive data, then (3) trains a generator
while (4) optimizing a critic to distinguish between the real and generated data.
involves sampling “frequencies” from an ad hoc distribution, to project the data to the embedding. It
is desirable to optimize the sampling distribution to better represent the data as an embedding, but
the naive way of optimizing it would require re-accessing the data via sampling, coming at a cost of
privacy budget. Henceforth, we also propose to incorporate a privacy-preserving critic to optimize the
sampling strategy, which, through re-weighting, chooses the best representation from a fixed samples
of frequencies without extra privacy costs.
To this end, we propose the following minimax optimization training objective:
JnfsUP X ：(：) lφ Pr (I)- φ Qθ (ti)|2.	⑴
θ∈θ ω∈Ω i=1 ωo(ti)
See later parts for notations and details. Theoretically, we show that our proposed objective has
properties similar to those that are suited to training GANs, i.e., continuity and differentiability
of the generator’s parameters, and continuity in weak topology. We also prove the consistency of
our privacy-preserving sampling strategy at the asymptotic limit of infinite sampling. Empirical
evaluations show that PEARL is able to high-quality synthetic data at reasonable privacy levels.
Related works. Traditional methods of synthesizing data are mainly concerned with discrete data or
data preprocessed to the discrete form (Zhang et al. (2017); Qardaji et al. (2014); He et al. (2015);
Chen et al. (2015); Cai et al. (2021); Zhang et al. (2021)), whereas we are interested in more general
methods involving continuous data. Deep generative models under the DP setting are suitable for
this type of tasks (Takagi et al. (2021); Xie et al. (2018); Torkzadehmahani et al. (2019); Frigerio
et al. (2019); Jordon et al. (2019); Chen et al. (2020); Harder et al. (2021)). The private training
of deep generative models is usually performed using gradient sanitization methods. An exception
is DP-MERF (Harder et al. (2021)), which is closest to our work. There, random features used
to approximate the maximum mean discrepancy (MMD) objective are privatized and utilized for
training a generator. PEARL, which, as a realization, uses CFs, may be viewed as a generalization of
DP-MERF. Additionally, PEARL has several distinctive features which are lacking in DP-MERF.
The first lies in the introduction of a privacy-preserving critic, which leads to an improvement of
performance. The second is the private selection of the parameter of the sampling distribution, which
is also shown to be vital. Moreover, DP-MERF uses non-characteristic kernels when treating tabular
data, in contrast to ours, which is characteristic and has guarantees in convergence. We finally note
that generative models using CFs but only non-privately have been explored before (Ansari et al.
(2020); Li et al. (2020)) .
Contributions. Our contribution in this paper is three-fold: (i) We propose a general framework
called PEARL, where, unlike gradient sanitization methods, the generator training process and
iteration are unconstrained; reliance on ad-hoc (non-private) hyperparameter tuning is reduced by
extracting hyperparameters (auxiliary information) privately. (ii) We demonstrate a realization of our
framework by making use of the characteristic function and an adversarial re-weighting objective.
(iii) Our proposal has theoretical guarantees of performance, and empirical evaluations show that our
approach outperforms competitors at reasonable levels of privacy ( ' 1).
2
Published as a conference paper at ICLR 2022
2	Preliminaries
This Section gives a brief review of essential preliminaries about differential privacy, characteristic
function and the related notations.
2.1	Differential Privacy
Definition 1 ((, δ)-Differential Privacy). Given privacy parameters ≥ 0 and δ ≥ 0, a randomized
mechanism, M : D → R with domain D and range R satisfies (, δ)-differential privacy (DP) if for
any two adjacent inputs d, d0 ∈ D and for any subset of outputs S ⊆ R, the following holds:
Pr[M(d) ∈ S] ≤ e ∙ Pr[M(d0) ∈ S] + δ.	(2)
We next consider concrete ways of sanitizing certain outputs with DP. A typical paradigm of DP is
applying the randomized mechanism, M, to a certain deterministic function f : D → R such that the
output of f is DP. The noise magnitude added by M is determined by the sensitivity of f, defined as
∆f = suPdQ∈D Ilf (d) - f (d0)k, where ∣∣∙∣∣ is anorm function defined on fs output domain. d and
d0 are any adjacent pairs of dataset. Laplacian and Gaussian mechanisms are the standard randomized
mechanisms. We primarily utilize the Gaussian mechanism in this paper (Dwork et al. (2014)):
Definition 2 (Gaussian Mechanism). Let f : X → R be an arbitrary function with sensitivity ∆f.
The Gaussian Mechanism, Mσ, parameterized by σ, adds noise to the output of f as follows:
Mσ(x) = f(x) +N(0,σ2I).	(3)
One of the most important properties of DP relevant to our work is the post-processing theorem
(Dwork et al. (2014)):
Theorem 1 (Post-processing Theorem). Let M : D → R be (, δ)-DP and let f : R → R0 be an
arbitrary randomized function. Then, f ◦ M : D → R0 is (, δ)-DP.
It ensures that the DP-sanitized data can be re-used without further consuming privacy budgets.
2.2	Characteristic Functions
Characteristic function (CF) is widely utilized in statistics and probability theory, and perhaps is best
known to be used to prove the central limit theorem (Williams (1991)). The definition is as follows.
Definition 3 (Characteristic Function). Given a random variable X ⊆ Rd and P as the probability
measure associated with it, and t ∈ Rd, the corresponding characteristic function (CF) is given by
Φp(t) = Eχ~p[eit∙χ]
/
Rd
eit∙χdP.
(4)
Here, i is the imaginary number. From the signal processing point of view, this mathematical
operation is equivalent to the Fourier transformation, and ΦP(t) is the Fourier transform at frequency
t. It is noted that we deal with the discrete approximation of CFs in practice. That is, given a dataset
with n i.i.d. samples, {xj}n=ι from P, the empirical CF is written as Φp(t) = n Pn=j eit∙xj. We
next introduce characteristic function distance (CFD) (Heathcote (1972); Chwialkowski et al. (2015)):
Definition 4 (Characteristic Function Distance). Given two distributions PandQ of random variables
residing in Rd, andω a sampling distribution on t ∈ Rd, the squared characteristic function distance
(CFD) between P and Q is computed as:
C (P, Q) = Et~ω(t)[ΦP(t) - ΦQ(t)2] = ΦP(t) - ΦQ(t)2ω(t)dt.	(5)
Rd
Notations. Let us make a short note on the notations before continuing. Let k be the num-
ber of t drawn from ω and P be the probability measure of a random variable. We group
the CFs associated to P of different frequencies, (ΦP(t1), . . . , ΦP(tk))> more compactly as
φP(x). To make the dependence of φP(x) on the sampled data explicit, we also use the fol-
1n
lowing notation: φp(x) = 1 ∑j=ι φp(xj). We notice that ∣∣φp(x)∣2 ≡
JPm=I ∣ΦP(tm)l2
dpm=ι∖ Pn=I eitm∙xl /n∣2 ≤ dPm=ι∖ Pn=I 1/n|2 = √k, where the norm is taken over the
z	I 、C	LT」 「一1	c . ..	-	. .	7-	7-	.	..
(complex) frequency space. With a slight abuse of notation, we abbreviate φP as φ when there is no
ambiguity in the underlying probability measure associated with the CF.
3
Published as a conference paper at ICLR 2022
3	Generative Model of PEARL
Let us describe generative modelling under the PEARL framework. The realization is summarized by
the following Proposition:
Proposition 1. Let the real data distribution be Pr, and the output distribution of an implicit
generative model Gθ by Qθ. Let n be the total number of real data instances and consider releasing k
CFs from the dataset. Then, Gθ trained to optimize the empirical CFD, min Cb2 (Pr, Qθ) with the CF
θ∈Θ
sanitized according to the Gaussian mechanism (Defn. 3) with Sensitivity 2√k∕n satisfies (e, δ)-DP,
where σ ≥ p2 log(1.25∕δ)∕c.
Proof. In the following we give a detailed explanation of the above Proposition. The first step of
PEARL is projecting the data to the CF as in Eq. 4, where the number of embeddings is the number
of frequency drawn from ω(t).
We note that CF has several attractive properties. CF is uniformly continuous and bounded, as can be
seen from its expression in Eq. 4. Unlike the density function, the CF of a random variable always
exists, and the uniqueness theorem implies that two distributions are identical if and only if the CFs
of the random variables are equal (Lukacs (1972)).
The CF is sanitized with DP by applying the Gaussian mechanism (Defn. 3) to φ(x):
φe(x) = φb(x) + N (0, ∆2φb(x)σ2I),	(6)
where we write the sanitized CF as φ(x); ∆φb(x) denotes the sensitivity of the CF, σ denotes the noise
scale which is determined by the privacy budget, (, δ).
The calculation of sensitivity is tractable (no ad-hoc clipping commonly required by gradient sanitiza-
tion methods) Without loss of generality, consider two neighboring datasets of size n where only the
last instance differs (xn 6= x0n). The sensitivity of φ(x) may then be calculated as
∆ b = max
φ(x)	D,D0
nn
n X 祗 j) - 1X Φ(xj)
j=1	j=1
=max Il1 B(Xn)-11b(Xn)IL = 2√k,
xn ,xn	2
2
where We have used triangle inequality and ∣∣φ(∙)∣∣2 ≤ √k. It can be seen that the sensitivity is
proportional to the square root of the number of embeddings (releasing more embeddings requires
more DP noises to be added to φb) but inversely proportional to the dataset size, which is important
for controlling the magnitude of noise injection at practical privacy levels, as will be discussed in
later Sections.
We would like to train a generative model where the CFs constructed from the generated data
distribution, Y ⊆ Rd, matches those (sanitized) from the real data distribution, X ⊆ Rd . A natural
way of achieving this is via implicit generative modelling (MacKay (1995); Goodfellow et al. (2014)).
We introduce a generative model parametrized by θ, Gθ : Z → Rd, which takes a low-dimensional
latent vector z ∈ Z sampled from a pre-determined distribution (e.g., Gaussian distribution) as the
input.
In order to quantify the discrepancy between the real and generated data distribution, we use the CFD
defined in Eq. 5. Empirically, when a finite number of frequencies, k, are sampled from ω,C2(P,Q)
is approximated by
1 k	I	I2
Cb2(P, Q) = k SX ∣φP(Ii)- φQ(ti)1 ≡ ∣∣φP(X)- φQ(X)Il ,
i=1	2
(7)
1	ɪ' / . ∖	1 ʃɪ / . ∖	.1	∙ ∙ 1 X 'l -	1 , IC	∙ ∙ 1	1	1∙ . ∙1	TΓΛ	IC
where ΦP(t) and ΦQ(t) are the empirical CFs evaluated from i.i.d. samples of distributions P and Q
respectively. The training objective of the generator is to find the optimal θ ∈ Θ that minimizes the
empirical CFD: min Cb2(Pr, Qθ). It can be shown via uniqueness theorem that as long as ω resides in
θ∈Θ
Rd, C (P, Q) = 0 ^⇒ P = Q (SriPerUmbUdUr et al. (2011)). This makes CFD an ideal distance
metric for training the generator.
4
Published as a conference paper at ICLR 2022
Optimization procedure. The generator parameter, θ, is updated as follows. φPr (x) is first sanitized
to obtain φPr (x), as in Eq. 6. This is performed only for once (one-shot). Then, at each iteration, m
samples of z are drawn to calculate φQθ (x). Gradient updates on θ are performed by minimizing the
CFD, φePr (x) - φbQθ (x) .
We note that only the first term, φPr (x), has access to the real data distribution, X, of which privacy
is of concern. Then, by Thm. 1, Gθ trained with respect to φPr (x) is DP. Furthermore, unlike gradient
sanitization methods, the training of Gθ is not affected by network structure or training iterations.
Once the sanitized CF is released, there is no additional constraints due to privacy on the training
procedure.	□
DP release of auxiliary information. Auxiliary information, e.g., hyperparameters regarding to the
dataset useful for generating data with better quality may be obtained under DP. In our case, one may
extract auxiliary information privately from the dataset to select good parameters for the sampling
distribution ω(t). This will be discussed in more detail in the next Sections.
Another example is the modelling of tabular data. Continuous columns of tabular data consisting of
multiple modes may be modelled using Gaussian mixture models (GMMs) (Xu et al. (2019)). GMMs
are trainable under DP using a DP version of the expectation-maximization algorithm (Park et al.
(2017)). The total privacy budget due to multiple releases of information is accounted for using the
Renyi DP composition. See App. A for the definition of Renyi DP.
4	Adversarial Reconstruction Learning
This Section is devoted to proposing a privacy-preserving critic for optimizing ω(t), while giving
provable guarantees of performance.
Back to Eq. 5 and Eq. 7, we note that choosing a “good” ω(t) or a “good” set of sampled frequencies
is vital at helping to discriminate between P and Q. For example, if the difference between P and Q
lies in the high-frequency region, one should choose to use t with large values to, in the language of
two-sample testing, improve the test power.
Adversarial objective. If the resulting empirical CFD remains small due to under-optimized ω(t),
while the two distributions still differ significantly, the generator cannot be optimally trained to
generate high-quality samples resembling the real data. Hence, we consider training the generator by,
in addition to minimizing the empirical CFD, maximizing the empirical CFD using an adversarial
objective which acts as a critic, where the empirical CFD is maximized by finding the best sampling
distribution. We consider a training objective of the form
2
inf sup Ccω (Pr , Qθ),	(8)
θ∈θ ω∈Ω
where Ω is some set of probability distribution space of which the sampling distribution ω lives in.
Privacy-preserving optimization. It is intractable to directly optimize ω in the integral form as in
Eq. 5. In this work, we opt for a privacy-preserving one-shot sampling strategy where once the
private data is released (by sampling from ω), optimization is performed without further spending the
privacy budget. We believe that such a new formulation of distribution learning is of independent
interest as well.
Effectiveness of the critic. To further motivate why it is preferable to introduce an adversarial
objective as in Eq. 8, we present a simple demonstration through the lens of two-sample testing
(Chwialkowski et al. (2015); Jitkrittum et al. (2016)) using synthetic data generated from Gaussian
distributions. We generate two unit-variance multivariate Gaussian distributions P, Q, where all
dimensions but one have zero mean. We conduct a two-sample test using the CF to distinguish
between the two distributions, which gets more difficult as the dimensionality increases. We test
if the null hypothesis where the samples are drawn from the same distribution is rejected. Higher
rejection rate indicates better test power.
Note that the first dimension (where the distribution differs) of the frequency used to construct CFs
is the most discriminative dimension for distinguishing the distributions. We consider three sets of
5
Published as a conference paper at ICLR 2022
frequencies: “unoptimized”, “normal”, and “optimized”, where the set of “unoptimized” frequencies
is optimized with re-weighting. More details can be found in App. C.
Fig. 2 shows the hypothesis rejection rate versus
the number of dimensions for the three cases
considered above. As can be observed, the “op-
timized” case gives overall the best test power.
While this experiment is somewhat contrived,
it can be understood that although both “unop-
timized” and “optimized” sets of frequencies
contain the discriminative t0 , the re-weighting
procedure of selecting the most discriminative
CF improves the test power significantly. Even
without re-sampling from a “better” ω(t), re-
weighting the existing frequencies can improve
the test power. The fact that re-weighting can im-
prove the test power is crucial privacy-wise be-
cause the altenative method, re-sampling, causes
degradation in privacy.
Figure 2: Increased test power upon optimization
(green) in two-sample test.
Proposal. Recall that the empirical CFD, Cb2 (Pr, Qθ) = 1 Pk=J ΦPr (ti) - Φq0 (ti)∣ ,isobtained
by drawing k frequencies from a base distribution ω0 . Our idea is to find a (weighted) set of
frequencies that gives the best test power from the drawn set. We propose Eq. 1 as the optimization
objective, restated below:
k
jnf sup X 3) lφ Pr (Ii)- φ Qθ (ti)∣2.
θ∈θ ω∈Ω i=1 ωo(ti)
Note that the generator trained with this objective still satisfies DP as given in Prop. 1 due to Thm. 1.
The following Lemma ensures that the discrete approximation of the inner maximization of Eq. 1
approaches the population optimum as the number of sampling frequency increases (k → ∞):
Lemma 1. Let ω0 be any probability distribution defined on Rd, and let f : Rd → R0 be any function.
Also let t ∈ Rd and ω* be the maximal distribution of ω with respect to Eω [f (t)] ≡ J f (t)ω(t)dt.
Assume that the empirical approximation Eb ω [f (t)] → Eω [f (t)] at the asymptotic limit for any ω.
Then, Eωo [f (t) ωθ(t) ] → Eω* [f (t)] at the asymptotic limit as well.
The proof is in App. B, and is based on importance sampling. Empirically, we find that treating ω(ti)
as a free parameter and optimizing it directly does not lead to improvement in performance. This may
be due to the optimization procedure focusing too much on uninformative frequencies that contain
merely noises due to DP or sampling. We perform parametric optimization instead, that is, e.g., we
perform optimization with respect to {μ, σ} if ω is of the Gaussian form, N(μ, σ2).
Performance guarantees. Let us discuss the theoretical properties of Eq. 8. The objective defined
in Eq. 8 shares beneficial properties similar to those required to train good GANs, first formulated
in (Arjovsky et al. (2017)). First, the generator learns from a distance continuous and differentiable
almost everywhere within the generator’s parameters. Second, the distance is continuous in weak
topology, and thus provides informative feedback to the generator (different from, e.g., the Jensen-
Shannon divergence, which does not satisfy this property). We make assumptions similar to those
given in (Arjovsky et al. (2017)), and state the first theorem as follows.
Theorem 2.	Assume that Gθ(z) is locally Lipschitz with respect to (θ, z); there exists L((θ, z) satis-
fying Ez [L(θ,z)] < ∞; and supω∈Ω Eω [|t|] < ∞ for all t. Then, thefunction supω∈Ω C2 (Pr, Qθ)
is continuous in θ ∈ Θ everywhere, and differentiable in θ ∈ Θ almost everywhere.
Note that the local Lipschitz assumptions are satisfied by commonly used neural network components,
such as fully connected layers and ReLU activations. The continuity and differentiability conditions
with respect to θ stated above allow Gθ to be trained via gradient descent. The theorem related to
continuity in weak topology is the following:
Theorem 3.	Let P be a distribution on X and (Pn)n∈N be a sequence of distributions on X. Under
the assumption supω∈Ω Eω(t) [ktk] < ∞, the function supω∈Ω Cω (Pn, P) is continuous in the
6
Published as a conference paper at ICLR 2022
weak topology, i.e., if Pn -→ P, then suPω∈Ω Cω (Pn, P) —→ 0, where -→ denotes convergence in
distribution.
Weakness is desirable as the easier (weaker) it is for the distributions to converge, the easier it will be
for the model to learn to fit the data distribution. The core ideas used to prove both of these theorems
are the fact that the difference of the CFs (which is of the form eia) can be bounded as follows:
|eia — eib| ≤ |a — b|, and showing that the function is locally Lipschitz, which ensures the desired
properties of continuity and differentiability. See App. B for the full proofs.
5	Experiments
5.1	Experimental Setup
To test the efficacy of PEARL, we perform empirical evaluations on three datasets, namely MNIST
(LeCun et al. (2010)), Fashion-MNIST (Xiao et al. (2017)) and Adult (Asuncion & Newman (2007)).
Detailed setups are available in App. H.
Training Procedure. As our training involves minimax optimization (Eq. 1), we perform gradient
descent updates based on the minimization and maximization objectives alternately. We use a zero-
mean diagonal standard-deviation Gaussian distribution, N (0, diag(σ1 2 ) as the sampling distribution,
ω. Maximization is performed with respect to σ . Let us give a high-level description of the
full training procedure: draw k frequencies from ω , calculate the CFs with them and perform
DP sanitization, train a generator with the sanitized CFs using the minimax optimization method.
The pseudo-code of the full algorithm is presented in App. E. We further give the computational
complexity analysis of our algorithm in App. F.
DP release of auxiliary information. We also note that applying maximization (re-weighting) on
a randomly selected set of frequencies would not work well. We initially fix the inverse standard
deviation of the base distribution, ω0 , to be the DP estimate of the mean of the pairwise distance of
the data, to obtain (privately) a spectrum of frequencies that overlaps with the "good" frequencies.
This is motivated by the median heuristic (Garreau et al. (2017)). Mean is estimated instead of median
as its sensitivity is more tractable when considering neighboring datasets. We obtain ∆ =2√d∕n
as its sensitivity, where d and n are the data dimension and the number data instances respectively.
See App. D for the derivation. 1 Using this DP estimate, we also investigate as a ablation study the
generator performance with minimization objective only (w/o critic). The full PEARL framework
includes the use of the critic to find the "best" frequencies among the selected spectrum to distinguish
the distributions (and overall perform a minimax optimization).
Evaluation Metrics. In the main text and App. I, we show qualitative results, i.e., synthetic images
(image dataset) and histograms (tabular dataset) generated with PEARL. Furthermore, for image
datasets, the FreChet Inception Distance (FID) (Heusel et al. (2017)) and Kernel Inception Distance
(KID) (BinkOWSki et al. (2θ18)) are used to evaluate the quantitative performance. For tabular data,
we use the synthetic data as the training data of 10 scikit-learn classifiers (Pedregosa et al. (2011))
and evaluate the classifiers’ performances on real test data. The performance indicates hoW Well
synthetic data generalize to the real data distribution and hoW useful synthetic data are in machine
learning tasks. ROC (area under the receiver operating characteristics curve) and PRC (area under
the precision recall curve) are the evaluation metrics. Definitions of FID and KID are in App. G.
5.2	Evaluation Details
MNIST and Fashion-MNIST. Privacy budget is allocated equally betWeen the sanitization of CFs
and the release of auxiliary information. 2 On a single GPU (Tesla V100-PCIE-32GB), training
MNIST (With 100 epochs) requires less than 10 minutes.
1We note that DP-MERF also uses a sampling distribution. Since privacy budget is not allocated for calculat-
ing the parameters of the sampling distribution there, We fix its sampling distribution to be N (0, diag(1)).
2We expect that CF sanitization requires more privacy allocation as it is involved in doing the heavy lifting
of training the model. We find that allocating around 80-90% of the budget to CF sanitization can increase the
performance by around 15%. Nevertheless, We utilize an equal share of privacy budget for fairness reasons, as
tuning the budget allocation Would require re-using the private data and this can lead to privacy leakage.
7
Published as a conference paper at ICLR 2022
刈Q日。。
0»*l∙生
,⅛--Λ.>■?∙v
■777'刀叮
菽-4⅛∙a⅛
5⅛⅛√∙*⅛∙
“ Y ¥ / W
33T 3 3
2A1.2X
(b) = 1 (MNIST)
(c) = 10 (MNIST)
qze⅛ Q
g∙, f r g
Y 7 7 7 7
G J66 6
5 855 5
∙√q、夕 T
3 3 J 3 3
l,¾2a,Λ
/ / Jr<,5
ono。。
= 0.2 (MNIST)
Figure 3: Generated MNIST and Fashion-MNIST samples for various values of and δ = 10-5.
(g) = 10 (FMNIST)
(d) = ∞ (MNIST)
(h) = ∞ (FMNIST)
Datasets	Metrics	DPGAN	DPCGAN	DP-MERF	Ours (w/o critic)	Ours
MNIST	FID KID (×103)	22.1 ± 1.01 573 ± 41.2	17.3 ± 2.90 286 ± 69.5	49.9 ± 0.22 148 ± 46.2	3.79 ± 0.06 77.8 ± 9.88	3.52 ± 0.06 70.5 ± 10.3
Fashion-MNIST	FID KID (×103)	16.6 ± 1.34 535 ± 61.5	14.61 ± 1.75 425 ± 64.2	37.0 ± 0.15 1220 ± 36.1	1.99 ± 0.04 24.0 ± 6.90	1.92 ± 0.04 26.9 ± 6.80
Table 1: FID and KID (lower is better) on image datasets at (, δ) = (1, 10-5).
Some of the generated images of ours and baseline models are shown in Fig. 3, and more (including
baselines’) can be found in App. I. At the non-private limit, the image quality is worse than other
popular non-private approaches such as GANs due to two reasons. First, projecting data to a lower
dimension causes information loss. Second, our architecture does not have a discriminator-like
network as in the vanilla GAN framework. However, we notice that the quality of the images does
not drop much as the privacy level increases (except at = 0.2, where the quality starts to degrade
visibly) because the noise added to the CFs is small as it scales inversely proportional to the total
sample size. It also indicates that our approach works particularly well at practical levels of privacy.
We now provide quantitative results by performing evaluation at (, δ) = (1, 10-5). Note that we
focus on this high privacy region despite the fact that many previous studies experimented with
' 10, because recent analyses showed that realistic attacks can lead to privacy violations at low
privacy (Jagielski et al. (2020); Nasr et al. (2021)). Particularly, the claim “values of that offer
no meaningful theoretical guarantees” (	1) can be chosen in practice has been “refuted” in
general (Nasr et al. (2021)). This motivates us to perform evaluation at with meaningful theoretical
guarantees ( ' 1).
At this privacy region, we compare PEARL with models using gradient sanitization and DP-MERF
(Harder et al. (2021)) as other methods do not produce usable images at single-digit . 3 We run
the experiment five times (with different random seeds each time), and for each time, 60k samples
are generated to evaluate the FID and KID. In Table 1, the FID and KID (average and error) of
DP-MERF, DPGAN, PEARL without critic, and PEARL are shown. It can be seen that PEARL
outperforms DP-MERF significantly, and the critic leads to improvement in the scores.
3In the gradient sanitized generative model literature, GS-WGAN (Chen et al. (2020)) is known to be the
state-of-the-art, but we are unable to train a stable model at = 1. Thus, we make comparisons with other
standard methods, namely DPGAN (Xie et al. (2018)) and DPCGAN (Torkzadehmahani et al. (2019)).
8
Published as a conference paper at ICLR 2022
Real data
DP-MERF
Ours
	ROC	PRC	ROC	PRC	ROC	PRC
LR	0.788	0.681	0.661 ± 0.059	0.542 ± 0.041	0.752 ± 0.009	0.641 ± 0.015
Gaussian NB	0.629	0.511	0.587 ± 0.079	0.491 ± 0.06	0.661 ± 0.036	0.537 ± 0.028
Bernoulli NB	0.769	0.651	0.588 ± 0.056	0.488 ± 0.04	0.763 ± 0.008	0.644 ± 0.009
Linear SVM	0.781	0.671	0.568 ± 0.091	0.489 ± 0.067	0.752 ± 0.009	0.640 ± 0.015
Decision Tree	0.759	0.646	0.696 ± 0.081	0.576 ± 0.063	0.675 ± 0.03	0.582 ± 0.028
LDA	0.782	0.670	0.634 ± 0.060	0.541 ± 0.048	0.755 ± 0.005	0.640 ± 0.007
Adaboost	0.789	0.682	0.642 ± 0.097	0.546 ± 0.071	0.709 ± 0.031	0.628 ± 0.024
Bagging	0.772	0.667	0.659 ± 0.06	0.538 ± 0.042	0.687 ± 0.041	0.601 ± 0.039
GBM	0.800	0.695	0.706 ± 0.069	0.586 ± 0.047	0.709 ± 0.029	0.635 ± 0.025
MLP	0.776	0.660	0.667 ± 0.088	0.558 ± 0.063	0.744 ± 0.012	0.635 ± 0.015
Average
0.765 0.654
0.721 ± 0.035	0.618 ± 0.033
0.641 ± 0.044	0.536 ± 0.034
Table 2: Quantitative results for the Adult dataset evaluated at (, δ) = (1, 10-5).

α∙α ιa z.o	ta
Catesory
(a) “marital-status”
Oa 1.0 2Λ 34 4« 54 6Λ 7.0 g M IMmZ«134144
cat*gs>y
(b) “occupation”
■ Real
Ours
DP-MERF
(e) "age'
(f) “capital-loss'
«4 1Λ iβ 34 4Λ 54
(C) “relationship”
(g) “capital-gain”
(h) “fnlwgt”
Figure 4: Histogram plots for the the Adult dataset. Evaluation is performed at (e, δ) = (1,10-5).
Adult. The Adult dataset consists of continuous and categorical features. As data pre-processing,
continuous features are scaled to [0,1]. We also compare our results with DP-MERF, as DP-MERF
performs best among other existing methods (e.g., DPCGAN achieves ROC and PRC around 0.5;
see Table 4.) Again, auxiliary information and CFs share equal budget of privacy, and we perform
evaluation at (e, δ) = (1,10-5). 11k synthetic samples are generated for evaluation.
Histogram plots of the attributes comparing the real and synthetic data is shown in Fig. 4. As can be
observed in the Figure, PEARL is able to model the real data better than DP-MERF, e.g., covering
more modes in categorical variables with less discrepancy in the frequency of each mode. PEARL is
also able to better model the descending trend of the continuous variable “age”. The average ROC
and PRC scores (average and error) are shown in Table 2. ROC and PRC scores based on PEARL's
training data are shown to be closer to those based on real training data compared to DP-MERF. More
histogram plots in higher resolution are available in App. I. Also, in App. I, we present experimental
results of another tabular dataset (Credit), as well as evaluations with other metrics/tasks using the
synthetic data. PEARL still performs favorably under these different conditions. Overall, we have
demonstrated that PEARL is able to produce high-quality synthetic data at practical privacy levels.
6	Conclusion
We have developed a DP framework to synthesize data with deep generative models. Our approach
provides synthetic samples at practical privacy levels, and sidesteps difficulties encountered in
gradient sanitization methods. While we have limited ourselves to characteristic functions, it is
interesting to adopt and adapt other paradigms to the PEARL framework as a future direction.
9
Published as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318. ACM, 2016.
Abdul Fatir Ansari, Jonathan Scarlett, and Harold Soh. A characteristic function approach to deep
implicit generative modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 7478-7487, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
Mikolaj Binkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD
GANs. arXiv preprint arXiv:1801.01401, 2018.
Kuntai Cai, Xiaoyu Lei, Jianxin Wei, and Xiaokui Xiao. Data synthesis via differentially private
markov random fields. Proceedings of the VLDB Endowment, 14(11):2190-2202, 2021.
Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach
for learning differentially private generators. Advances in Neural Information Processing Systems,
33:12673-12684, 2020.
Rui Chen, Qian Xiao, Yu Zhang, and Jianliang Xu. Differentially private high-dimensional data
publication via sampling-based inference. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 129-138, 2015.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In Advances in Neural Information
Processing Systems, pp. 1981-1989, 2015.
Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. Calibrating prob-
ability with undersampling for unbalanced classification. In 2015 IEEE Symposium Series on
Computational Intelligence, pp. 159-166. IEEE, 2015.
Cynthia Dwork. Differential privacy. In Proceedings of the 33rd international conference on
Automata, Languages and Programming-Volume Part II, pp. 1-12, 2006.
Cynthia Dwork. A firm foundation for private data analysis. Communications of the ACM, 54(1):
86-95, 2011.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211-407, 2014.
Herbert Federer. Geometric measure theory. Springer, 2014.
Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick Duverger. Differentially
private generative adversarial networks for time series, continuous, and discrete open data. In IFIP
International Conference on ICT Systems Security and Privacy Protection, pp. 151-164. Springer,
2019.
Damien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the median
heuristic. arXiv preprint arXiv:1707.07269, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Frederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean
embeddings with randomfeatures for practical privacy-preserving data generation. In International
conference on artificial intelligence and statistics, pp. 1819-1827. PMLR, 2021.
10
Published as a conference paper at ICLR 2022
Xi He, Graham Cormode, Ashwin Machanavajjhala, Cecilia M Procopiuc, and Divesh Srivastava.
Dpt: differentially private trajectory synthesis using hierarchical reference systems. Proceedings
ofthe VLDB Endowment, 8(11):1154-1165, 2015.
CE Heathcote. A test of goodness of fit for symmetric random variables. Australian Journal of
Statistics, 14(2):172-181, 1972.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural
information processing systems, 30, 2017.
Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine
learning: How private is private sgd? Advances in Neural Information Processing Systems, 33:
22205-22216, 2020.
WittaWat Jitkrittum, Zoltdn Szab6, KacPer P ChWialkoWski, and Arthur Gretton. Interpretable
distribution features with maximum testing power. In Advances in Neural Information Processing
Systems, pp. 181-189, 2016.
James Jordon, Jinsung Yoon, and Mihaela van der Schaar. PATE-GAN: generating synthetic data
With differential privacy guarantees. In International Conference on Learning Representations,
ICLR, 2019.
Achim Klenke. Probability theory: a comprehensive course. Springer Science & Business Media,
2013.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handWritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Shengxi Li, Zeyang Yu, Min Xiang, and Danilo P. Mandic. Reciprocal adversarial learning via
characteristic functions. In Advances in Neural Information Processing Systems, 2020.
Eugene Lukacs. A survey of the theory of characteristic functions. Advances in Applied Probability,
4(1):1-37, 1972.
David J.C MacKay. Bayesian neural netWorks and density netWorks. Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated
Equipment, pp. 73-80, 1995.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Milad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papemoti, and Nicholas Carlin. Adversary
instantiation: LoWer bounds for differentially private machine learning. In 2021 IEEE Symposium
on Security and Privacy (SP), pp. 866-882. IEEE, 2021.
Mijung Park, James R. Foulds, Kamalika Choudhary, and Max Welling. DP-EM: differentially
private expectation maximization. In Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics, pp. 896-904, 2017.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
Wahbeh Qardaji, Weining Yang, and Ninghui Li. PrivieW: practical differentially private release of
marginal contingency tables. In Proceedings of the 2014 ACM SIGMOD international conference
on Management of data, pp. 1435-1446, 2014.
Bharath K Sriperumbudur, Kenji Fukumizu, and Gert RG Lanckriet. Universality, characteristic
kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7), 2011.
Shun Takagi, Tsubasa Takahashi, Yang Cao, and Masatoshi YoshikaWa. P3gm: Private high-
dimensional data release via privacy preserving phased generative model. In 2021 IEEE 37th
International Conference on Data Engineering (ICDE), pp. 169-180. IEEE, 2021.
11
Published as a conference paper at ICLR 2022
Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private
synthetic data and label generation. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, June 2019.
YU-Xiang Wang, Borja Balle, and Shiva Prasad KasivisWanathan. Subsampled r6nyi differential
privacy and analytical moments accountant. In The 22nd International Conference on Artificial
Intelligence and Statistics ,pp.1226-1235. PMLR, 2019.
David Williams. Probability with martingales. Cambridge University Press, 1991.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative
adversarial netWork. arXiv preprint arXiv:1802.06739, 2018.
Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular
data using conditional gan. In Advances in Neural Information Processing Systems, volume 32,
2019.
Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xiaokui Xiao. Privbayes:
Private data release via bayesian netWorks. ACM Transactions on Database Systems (TODS), 42
(4):1-41, 2017.
Zhikun Zhang, Tianhao Wang, Ninghui Li, Jean Honorio, Michael Backes, Shibo He, Jiming Chen,
and Yang Zhang. PrivSyn: Differentially private data synthesis. In 30th USENIX Security
Symposium (USENIX Security 21), pp. 929-946, 2021.
12
Published as a conference paper at ICLR 2022
A	Additional Definitions and Previous Results
Definition 5 (Renyi differential privacy). A randomized mechanism M is said to satisfy ε-Renyi
differential privacy of order λ, when
1	P r[M(d) = x] λ-1
λ	λ 一 Igaj Λ4(d)](ρrMdy^i)	j ≤ε
is satisfiedfor any adjacent datasets d and d0. Here, Dλ(PkQ) = χ⅛ι log Ex〜q[(P(x)∕Q(x))λ] is
the Renyi divergence. Furthermore, a ε -RDP mechanism of order λ is also (ε + IogI/δ, δ) -DP.
Next, We note that the Gaussian mechanism is (λ, λ2f )-RDP (Mironov (2017)). The particular
advantage of using RDP is that it gives a convenient way of tracking the privacy costs when a
sequence of mechanisms is applied. More precisely, the folloWing theorem holds (Mironov (2017)):
Theorem 4 (RDP Composition).For a sequence of mechanisms M1, ..., Mk s.t. Mi is (λ, εi)-RDP
∀i, the composition M1 ◦ ... ◦ Mk is (λ, i εi)-RDP.
We use the autodp package to keep track of the privacy budget (Wang et al. (2019)).
B	Proofs
B.1 Proof of Thm. 2
Proof. Let Pr denote the real data distribution, Qθ denote the data distribution generated by Gθ using
a latent vector, Z sampled from a pre-determined distribution, and ω a sampling distribution. | ∙ |
denotes the modulus of ∙. Then, the CFD between the distributions is
C2 (Pr,Qθ ) = Et 〜ω(t) h∣ΦPr (t) - ΦQθ (t)∣2i ,
where Φp(t) = Ex〜P [eit∙χ] is the CF of P. For notational brevity, we write Φpr (t) as Φr (t), and
ΦQθ (t) as Φθ(t) in the following.
We consider the optimized CFD, supCω2 (Pr, Qθ), and would like to show that it is locally Lips-
ω∈Ω
chitz, 4 which subsequently means that it is continuous. Since the Radamacher’s theorem (Federer
(2014)) implies that any locally Lipschitz function is differentiable almost everywhere, the claim of
differentiability is also justified once the Lipschitz locality is proven.
We first note that the difference of two maximal functions’ values is smaller or equal to the maximal
difference of the two functions. Then, for any θ and θ0,
supCω2(Pr,Qθ) -supCω2(Pr,Qθ0) ≤ supCω2(Pr,Qθ)-Cω2(Pr,Qθ0).	(9)
lω∈Ω	ω∈Ω	1	ω∈Ωl	1
We note that the absolute difference of any two complex values represented in terms of its real-number
amplitude (A, B) and phase (α, β) satisfies ∣Aeiα - Beiβ| ≤ |A| + |B|. Writing the maximal ω as
ω*, the RHS ofEq. 9 can be written as
∣cω * (Pr,Qθ )-Cω * (Pr, Qθ0 )|	(10)
=Et〜ω* (t) [(Cω*(Pr, Qθ) + Cω*(Pr, Qθ0)) (Cω*(Pr, Qθ) -Cω*(Pr,Qθ0))]
=Et 〜ω*(t) [(∣Φr (t) — Φθ (t)| + ∣Φr(t) — ①©，(t) |)( | Φr (t) - Φθ (t) | - ∣Φr (t) - Φθo(t)∣)]
≤ Et〜ω* (t)[(2∣Φr (t)| + ∣Φθ (t)| + ∣Φθ0 (t)∣)(∣Φr (t) - Φθ (t)∣-∣Φr (t)-①Θ，(t)|)]
(a)
≤ 4Et 〜ω*(t) Uφr (t) - φθ (t)| - lφr (t) - φθ0 (t)|]
(b)
≤ 4Et 〜ω*(t) Uφθ (t) - φθ0 (t)|],
4A function f is locally Lipschitz if there exist constants δ ≥ 0 and M ≥ 0 such that |x - y| < δ →
|f(x) - f(y)∣ ≤ M ∙ |x - y| for all x,y.
13
Published as a conference paper at ICLR 2022
where We have used (a) ∣Φp∣ ≤ 1, (b) triangle inequality. By interpreting a complex number as
a vector on a 2D plane, and using trigonometric arguments, one can deduce that |eia - eib | =
2 sin(|a - b|/2) ≤ |a - b|. Then,
4Et〜ω*(t) [∣Φθ(t) - Φθ0(t)∣] =4Et〜ω*(t) h∣Ez[eit∙G"z)] - Ez[eit∙Gθ0(z)]∣i	(11)
(c)
≤ 4Et 〜ω*(t) [Ez[|t ∙ Gθ (Z)- t ∙ Gθ0 (Z)|]]
(d)
≤ 4Et 〜ω*(t)Ez[∣t∣∙∣Gθ (Z)- Gθ0 (z)∣].
In (c), we have also used the Jensen inequality. In (d), the Cauchy-Schwarz inequality has been
applied. As we are assuming that Gθ is a L(θ, Z)-Lipschitz function, we have
Et〜ω*(t)Ez [∣t∣∙ ∣Gθ(Z)- Gθ0(z)∣] ≤ 4Et〜ω*(t)[∣t∣] ∙ Ez[L(θ,z)] ∙ ∣θ - θ]	(12)
Since We are also assuming that Et〜ω*(t)[|t|], Ez[L(θ,z)] ≤ ∞, we have shown that supCω is
ω∈Ω
locally Lipschitz, as required. It is therefore continuous and differentiable almost everywhere, as
discussed above.	□
B.2	Proof of Thm. 3
Proof. We denote Xn 〜Pn and X 〜P, and ω* the maximal function of ω. Notice that
C* (Pn, P) = Et〜ω*(t) [∣Eχn [eit∙xn] - Ex [eit∙χ]∣2i
=Et〜ω* (t) [|Exn [eit∙xn] - Ex [eit∙x] ∣ ∙ |Ex“ [eit,xn] - Ex [eit,x] ∣]
(a)
≤ 2Et〜ω*(t) [∣Exn [eit∙xn] - Ex [eit∙x] ∣]
(b)
≤ 2Et〜ω*(t)Exn,x [∣eit∙xn - eit∙x∣]
(c)
≤ 2Et〜ω*(t) [|t|] Exn,x [∣Xn - x|].	(13)
Here, (a) uses ∣Aeiα - Beiβ| ≤ |A| + |B| as argued above Eq. 10; (b) uses Jensen inequality; (c)
uses the argument |eia - eib| = 2 sin(|a - b|/2) ≤ |a - b|, given above Eq. 11. Then, by weak
convergence equivalence (Klenke (2013)), the RHS of Eq. 13 approaches zero as Pn -D→ P, hence
proving the theorem.	□
B.3	Proof of Lemma. 1
Proof. Recall that for any two distributions, ω(t), ω0(t) and any function f (t),
Eω [f(t)] =	f(t)ω(t)dt
=Zf (t) ^ω(t∩ ωo(t)dt
ω0(t)
=Eωo [f (t) -ω(tτ].
0	ω0(t)
Hence, Eω(t)[f (t)] = Eω0 [f (t)ωω((t)]. Let ω* be the maximal probability distribution. It is then
clear that Eω [f (t)] → Eω [f (t)] implies Eω° [f (t) ω-*((t)] → Eω* [f (t)], as desired.	□
C Experimental Setup of Two-sample Testing on Synthetic Data
Let us describe in more detail the experiment presented in Sec. 4. Data are generated from two
unit-variance multivariate Gaussian distributions P, Q, where all dimensions but one have zero mean
(P 〜N(0d, Id), Q 〜N((1,0,..., 0)>, Id)). We wish to conduct a two-sample test using the CF to
14
Published as a conference paper at ICLR 2022
distinguish between the two distributions, which gets more difficult as the dimensionality increases.
We test if the null hypothesis where the samples are drawn from the same distribution is rejected.
Three sets of frequencies are considered. The number of frequncies in each set is set to 20. The
first set is an “unoptimized” set of frequencies. The first dimension of all but one frequency
has the value of zero. Other dimensions have values generated randomly from a zero-mean unit-
variance multivariate Gaussian distribution. We denote the frequency with non-zero value in the
first dimension by t0 without loss of generality. A “normal” set of frequencies is also considered
for comparison, where the frequencies of all dimensions are sampled randomly from a multivariate
Gaussian distributions. Finally, we consider an “optimized” set of frequencies, where from the
“unoptimized” set of frequencies, only t0 is selected to be used for two-sample testing. In other words,
we re-weight the set of frequencies such that all but t0 has zero weight. 1,000 samples are generated
from each of P and Q. We repeat the problem for 100 trials to obtain the rejection rate (and repeat
the whole experiment 5 times to get the error bar).
D DP Estimate of the Mean of Pairwise Distance
Median heuristic is applied widely in kernel methods applications to determine the bandwidth of the
radial basis function (RBF) kernels (Garreau et al. (2017)). The bandwidth is taken to be the median
of all pairwise distances of data samples. Here we give a DP estimation of the mean instead as the
calculation of mean is more tractable. Let x be samples of a certain data distribution of dimension d
and assume that the values lie in [0, 1]d. Given n samples, there is a total of n(n - 1)/2 pairwise
distance pairs. Then, the mean of the pairwise distance of samples is
2 n
Dn(X) = n(n - 1) Σkxi - xjk2.
where ∣∣ ∙ ∣∣2 indicates the Euclidean norm.
Consider a pair of neighboring datasets, D, D0 . Without loss of generality, let Xn 6= X0n and Xi = X0i
for i = n. Then, the sensitivity of Dn(X) is
△d” (X)=m,aχ
2
n(n - 1)
n
X ∣Xi - Xj ∣2 -
(a)	2
=「-------7τmaχ
n(n - 1) D,D0
i6=j
n-1
∣Xi - Xn ∣2
i=1
2n
-7_π EkXi -Xjk2
n(n - 1)
i6=j
n-1
X ∣Xi -X0n∣2
i=1
—
2
2
=(2 ])∙ (n - 1)√d
n(n - 1)
2√d
----
.
n
In (a), we cancel out all terms unrelated to Xn or X0n .
In (b), we use the fact that ∣Xi - Xn ∣2
and ∣∣Xi 一 xn∣2 lie in [0, √d]. After obtaining the sensitivity, one can then applies the Gaussian
mechanism as in Eq. 6 to obtain the DP estimate of the mean of the pairwise distance of samples.
E Training Algorithm
The pseudo-code of the proposed training algorithm is given in Algorithm 1.
F Computational Complexity Analysis
PEARL is trained based on deep learning methods, which is efficient computation-wise using a
GPU. Nevertheless, let us give a computational complexity analysis in a traditional sense by ignoring
(partially) the parallel computing capability.
15
Published as a conference paper at ICLR 2022
Algorithm 1: PEARL Training
Input: Sensitive data {x}in=1, differential privacy noise scale σDP, number of frequencies k,
base sampling distribution variance σ0, training iterations T , learning rates ηC and
ηG, number of generator iterations per critic iteration ngen, batch size B, latent
distribution Pz
Output: Differentially private generator Gθ
1	Obtain auxiliary information (e.g., base sampling distribution variance σ0);
2	Sample frequencies {t}k=ι With t 〜N(0, diag(σ0));
3	for i in {1, ..., k} do
4	Φ p(ti )= n pn=ι eiti∙xj
5	ΦeP(ti) = ΦbP(ti) +N(0,∆2b( )σD2PI)
φ(x)
6	end
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
Accumulate privacy cost ;
φ(x) - (Φp(tι),..., Φp(tk ))>
Initialize generator Gθ, sampling distribution variance σ ;
for step in {1, ..., T} do
for t in {1, ..., ngen} do
Sample batch {zi}B=ι with Zi 〜 PZ;
for i in {1, ..., k} do
I ΦQ(ti) = ⅛ PB=i eiti∙Gθ(Zj)
end
φ(z) - (ΦQ(tl),..., ΦQ(tk)) 1
2
θ 一 θ - ηG ∙ VθCω (φ(x), b(z))
end
Sample batch {zi}B=ι with Zi 〜 PZ;
for i in {1, ..., k} do
I ΦQ(ti) = ⅛ PB=I eiti∙Gθ(zj)
end
φ(z) - (ΦQ(tl),..., ΦQ(tk)) 1
2
σ J σ + ηc ∙ VσCω (φ(x), φ(z))
end
26 Return Gθ
Time complexity. We sample k frequencies and calculate its inner product with data points (of
dimension d), make a summation with respect to n data points to build k CFs. Since inner product
takes d steps, summation n steps, and we do it for k times, the time complexity of generating CFs is
nkd. We additionally train the generative model, requiring ttr . Note that unlike graphical models
like PrivBayes, which gradually add nodes to the Bayesian network, our method can be parallelized
efficiently with GPU. As we sample n records from the generative model during data generation,
the time complexity of dataset generation is n times the inference time, tinf . The total training and
inference time is O(nkd + ntinf + ttr).
Space complexity. We need to store the k CF values during training. We also need to store the
generative model with m parameters. The space complexity is O(k + m).
G	Evaluation Metrics
We utilize evaluation metrics commonly used to evaluate GAN,s performance, namely FreChet
Inception Distance (FID) (Heusel et al.(2017)) and Kernel Inception Distance (KID) (BinkoWSki
et al. (2018)). FID corresponds to computing the Frechet distance between the Gaussian fits of the
Inception features obtained from real and fake distributions. KID is the calculation of the MMD of
the Inception features between real and fake distributions using a polynomial kernel of degree 3.
16
Published as a conference paper at ICLR 2022
The precision definitions are as follows. Let {xri}in=1 be samples from the real data distribution Pr
and {xθi }im=1 be samples from the generated data distribution Qθ . The corresponding feature vectors
extracted from a pre-trained network (LeNet in our case) are {zri }in=1 and {zθi }im=1 respectively.
The FID and KID are defined as
FID(Pr , Qθ ) =kμr - μθ k2 +TrNr + ςθ - 2(匕 ςθ )1/2),
nn
KID(Pr,Qθ )=-_π X X [κ(zr ,zj)]
n(n-1) i=1 j=1,j6=i	i j
mm
+ —X X [κ(zθW)]
m(m - 1)	j
i=1 j=1,j6=i
nm
-m XX[κ(zr 鸿)]，
i=1 j=1
(14)
(15)
where (μr, ∑r) and (μθ, ∑θ) are the sample mean & covariance matrix of the inception features of
the real and generated data distributions, and κ is a polynomial kernel of degree 3:
κ(x,y) = (dX ∙y+1)
(16)
where d is the dimensionality of the feature vectors. We compute FID with 10 bootstrap resamplings
and KID by sampling 100 elements without replacement from the whole generated dataset.
H Implementation Details
Datasets. For MNIST and Fashion-MNIST, we use the default train subset of the torchvision
5 library for training the generator, and the default subset for evaluation. For Adult, we follow the
preprocessing procedure in Harder et al. (2021) to make the dataset more balanced by downsampling
the class with the most number of samples.
Neural networks. The generator for image datasets has the following network architecture:
•	fc → bn → fc → bn → upsamp → relu → upconv → sigmoid,
where fc, bn, upsamp, relu, upconv, sigmoid refers to fully connected, batch normalization, 2D
bilinear upsampling, ReLU, up-convolution, and Sigmoid layers respectively.
For tabular dataset, we use the following architecture:
•	fc → bn → relu → fc → bn → relu → fc → tanh/softmax,
where tanh and softmax are the Tanh and softmax layers respectively. Network output corresponding
to the continuous attribute is passed through the Tanh layer, whereas network output corresponding
to the categorical attribute is passed through the softmax layer, where the category with the highest
value from the softmax output is set to be the generated value of the categorical attribute. We train
both networks conditioned on the class labels. For DP-MERF, we use the same architectures for fair
comparisons.
Hyperparameters. We use Adam optimizer with learning rates of 0.01 for both the minimization
and maximization objectives. Batch size is 100 (1,100) for the image datasets (tabular dataset). The
number of frequencies is set to 1,000 (3,000) for MNIST and tabular datasets (Fashion-MNIST).
The training iterations are 6,000, 3,000, and 8,000 for MNIST, Fashion-MNIST, and tabular datasets
respectively.
5https://pytorch.org/vision/stable/index.html
17
Published as a conference paper at ICLR 2022
Datasets	Methods	ROC	PRC	Range	Marginal	MMD
Adult	DP-MERF	0.64	0.54	0.10	1.37	2 × 10-3
	Ours	0.72	0.62	0.06	0.71	1 × 10-7
Credit	DP-MERF	0.66	0.26	0.049	0.95	2 × 10-5
	Ours	0.84	0.68	0.026	0.72	1 × 10-8
Table 3: Evaluation with ROC, PRC (higher is better), range query, marginal release and MMD
(lower is better) on tabular datasets at (, δ) = (1, 10-5).
	Real data		DPCGAN		Ours	
	ROC	PRC	ROC	PRC	ROC	PRC
LR	0.788	0.681	0.501 ± 0.001	0.484 ± 0.001	0.752 ± 0.009	0.641 ± 0.015
Gaussian NB	0.629	0.511	0.499 ± 0.005	0.483 ± 0.003	0.661 ± 0.036	0.537 ± 0.028
Bernoulli NB	0.769	0.651	0.482 ± 0.159	0.498 ± 0.091	0.763 ± 0.008	0.644 ± 0.009
Linear SVM	0.781	0.671	0.501 ± 0.001	0.484 ± 0.001	0.752 ± 0.009	0.640 ± 0.015
Decision Tree	0.759	0.646	0.599 ± 0.078	0.546 ± 0.056	0.675 ± 0.03	0.582 ± 0.028
LDA	0.782	0.670	0.501 ± 0.002	0.484 ± 0.002	0.755 ± 0.005	0.640 ± 0.007
Adaboost	0.789	0.682	0.523 ± 0.065	0.5 ± 0.036	0.709 ± 0.031	0.628 ± 0.024
Bagging	0.772	0.667	0.546 ± 0.082	0.516 ± 0.058	0.687 ± 0.041	0.601 ± 0.039
GBM	0.800	0.695	0.544 ± 0.101	0.518 ± 0.066	0.709 ± 0.029	0.635 ± 0.025
MLP	0.776	0.660	0.503 ± 0.004	0.486 ± 0.004	0.744 ± 0.012	0.635 ± 0.015
Table 4: Quantitative results for the Adult dataset evaluated at (, δ) = (1, 10-5).
I Additional Results
I.1	Additional tasks and dataset for tabular data
We here present more results relevant to PEARL’s generative modeling of tabular data. We first
compare PEARL with DPCGAN in Table 4 using the same classifiers as in the main text.
Three additional metrics/tasks are utilized: Range query. 1000 range queries containing three
attributes are sampled randomly. The average l1 error between the original and synthetic datasets is
evaluated. Marginal release. All 2-way marginals are calculated and the average l1 error is evaluated.
MMD. The max mean discrepancy (MMD) between the original and synthetic dataset is evaluated.
We use the MMD because it is a powerful measure that could in principle detect any discrepancy
between two distributions. A Gaussian kernel is used in the evaluation.
Additionally, we perform evaluation on another tabular dataset: Credit (Dal Pozzolo et al. (2015)).
Our results comparing with DP-MERF as shown in Table 3 demonstrate that PEARL is competitive
with various tasks and datasets.
I.2	MNIST AND FASHION-MNIST IMAGES
Fig. 5 and Fig. 6 show more enlarged images of MNIST and Fashion-MNIST at various level of
generated with PEARL. We also show images generated by PEARL but without critic, and DPGAN.
Note that we are unable to generate meaningful images from DP-MERF after taking privacy into
account appropriately (see Sec. 5).
I.3 Adult histograms
We show more histogram plots for continuous and categorical attributes comparing our method with
DP-MERF in Fig. 9 and Fig. 10.
18
Published as a conference paper at ICLR 2022
9∕99Qqg9夕夕
7777777777
4 4 6 *ocf√g 6 66
5srΓgsXΛ 54JS
√u" JV9。√MO
33333333t.3
aaz J2J√J□>i
/∕l∕∕∕z∕//
odooooΛ-ooo
Q 7 ⅜* S .0 q7
了'rr,rτ∙v*r∙f''f
rz-r7?7','r.7 7r
⅛&£ G 七⅛∙- b 6 G♦
K 5Γ'∙"..>■*:.武£ Wr
*√⅛∙sy.>./不“V
3 3 3.，..1,31333
z% 7.2Z.<N⅛'J A
If / 1< Zzf /• 1« Wt If / /*
αd 0 000 CCa&
夕 999gq9 9 9。”
8r8agg8g2z
7777777777
64 ♦/<备$6 SΛ
5 5 sπu∕∕⅛5 Sx
2 W 此 H :,V
3-^33333333
3 NZ12JZ •\ J2
//∕f∕ΛZ∕∕l
(d) = 0.2
(c) = 1
Figure 5: Additional generated MNIST images at various .
19
Published as a conference paper at ICLR 2022
(a) = ∞
(b) = 10
(c) = 1
(d) = 0.2
(a) = 1, MNIST (DPGAN)
Figure 6: Additional generated Fashion-MNIST images at various .
(b) = 1, FMNIST(DPGAN)
Oq4√¾
7αJ2 2
Ft »/ f/ Fi
Coo。力
7 7 7 7 7
夕夕。♦勺
Figure 7: Images generated by DPGAN at = 1.
(b) = 1, FMNIST(Ours w/o critic)
(a) = 1, MNIST (Ours w/o critic)
Figure 8:	Images generated by our proposal (PEARL) but without critic at = 1.
20
Published as a conference paper at ICLR 2022
DP-MERF	Ours
Age
DP-MERF
Ours
(b) fnlwgt
DP-MERF
Ours
(c) Capital Gain
DP-MERF
Ours
(d) Capital loss
Figure 9:	Histograms of various continuous attributes of Adult dataset comparing real and synthetic
data.
21
Published as a conference paper at ICLR 2022
IO1
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.010.011.012.013.014.015.0
category
O2
aUnOU
(a) Education
Real
Ours
DP-MERF
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.011.012.013.014.0
category
(b) Occupation
2.0	3.0	4.0	5.0	6.0	7.0	8.0
category
O3
aUnOU
0.0	1.0	2.0	3.0	4.0	5.0
category
(d) Relationship
4unou
(c) Work class
category
O3
aUnOU
Real
Ours
DP-MERF
2.0
category
(e) Sex
(f) Race
Figure 10:	Histograms of various categorical attributes of Adult dataset comparing real and synthetic
data.
22