Published as a conference paper at ICLR 2022
Implicit Bias of Adverarial Training for Deep
Neural Networks
Bochen Lyu
DataCanvas Lab
DataCanvas, Beijing, China
lvbc@zetyun.com
Zhanxing Zhu
The University of Edinburgh, UK
zhanxing.zhu@gmail.com
Ab stract
We provide theoretical understandings of the implicit bias imposed by adversarial
training for homogeneous deep neural networks without any explicit regularization.
In particular, for deep linear networks adversarially trained by gradient descent on
a linearly separable dataset, we prove that the direction of the product of weight
matrices converges to the direction of the max-margin solution of the original
dataset. Furthermore, we generalize this result to the case of adversarial training
for non-linear homogeneous deep neural networks without the linear separability
of the dataset. We show that, when the neural network is adversarially trained
with '2 or '∞ FGSM, FGM and PGD perturbations, the direction of the limit
point of normalized parameters of the network along the trajectory of the gradient
flow converges to a KKT point of a constrained optimization problem that aims to
maximize the margin for adversarial examples. Our results theoretically justify the
longstanding conjecture that adversarial training modifies the decision boundary
by utilizing adversarial examples to improve robustness, and potentially provides
insights for designing new robust training strategies.
1	Introduction
Deep neural networks (DNNs) have achieved great success in many fields such as computer vision
(Krizhevsky et al., 2012; He et al., 2015) and natural language processing (Collobert & Weston, 2008)
and other applications. These breakthroughs also lead to the importance of research on the robustness
and security issues of DNNs, among which those about adversarial examples are especially prevalent.
Adversarial examples are obtained by adding craftily designed imperceptible perturbations to the
original examples, which can sharply change the predictions of the DNNs with high confidence
(Szegedy et al., 2014; Nguyen et al., 2015). Such vulnerability of DNNs renders security concerns
about deploying them in security-critical systems including vision for autonomous cars and face
recognition. It is therefore crucial to develop defense mechanisms against these adversarial examples
for deep learning.
To this end, many defense strategies have been proposed to make DNNs resistant to adversarial
examples, such as adding a randomization layer before the input to the classifier (Xie et al., 2018),
input transformations (Guo et al., 2018), adversarial training (Madry et al., 2018), etc. However,
Athalye et al. (2018) pointed out that most of the defense techniques are ineffective and give a false
sense of security due to obfuscated gradients except for adversarial training—a method which has
not been comprehensively attacked yet. Given a C-class training dataset {(xi , yi)}in=1 with natural
example xi ∈ Rd and corresponding label yi ∈ {1, ..., C}, adversarial training is formulated as
solving the following minimax optimization problem
1n
minL(W)=min-5^^ max ' (Xi + δi(W),yi； W),
W	W n	δi∈B(0,)
i=1
(1)
where f is the DNN function, ` is the loss function and δi (W) is the adversarial perturbation
generated by some adversary A(xi, yi; W) typically depending on model parameters W, within
the set B(0, ) = {δ : kδkp ≤ } around the natural example xi. Commonly, adversarial training
conducts a two-player game at each iteration: the inner maximization is to attack the model and
1
Published as a conference paper at ICLR 2022
find the corresponding perturbation of the original example that maximizes the classification loss `;
the outer minimization, on the other hand, is to update the model parameters W with the gradient
descent method such that the loss ` is minimized on adversarial examples generated by the inner
maximization (see Algorithm 1 for details).
To have a better theoretical understanding of the fact that adversarial training empirically improves
the robustness of DNNs against adversarial examples, Zhang et al. (2019) decomposed the prediction
error for adversarial examples and identified a trade-off between robustness and accuracy while Li
et al. (2020) studied the inductive bias of gradient descent based adversarial training for logistic
regression on linearly separable data. Faghri et al. (2021) studied adversarial robustness of linear
neural networks by exploring the optimization bias of different methods. Yu et al. (2021) studied
adversarial training through the bias-variance decomposition and showed that its generalization error
on clean examples mainly comes from the bias. However, even for the simplest DNN—the deep linear
network, we notice that there exists no work to theoretically understand such robustness achieved by
adversarial training through exploring its implicit bias.
On the other hand, for the standard training, recent works extensively explored the implicit bias
imposed by gradient descent or its variants for DNNs in different settings. For a simple setting of
linear logistic regression on linearly separable data, Soudry et al. (2018); Ji & Telgarsky (2018);
Nacson et al. (2019b) derived that the direction of the model parameter converges to that of the
max-'2-margin with divergent norm. Shamir (2021) concluded that gradient methods never overfit
when training linear predictors over separable dataset. Viewing the above model as a single-layer
network, a natural but more complicated extension is that for the deep linear network on linearly
separable data: Ji & Telgarsky (2019) proved that the gradient descent aligns the weight matrices
across layers and the product of weight matrices also converges to the direction of the max-'2-margin
solution; Gunasekar et al. (2018) showed the implicit bias of gradient descent on linear convolution
networks. Nacson et al. (2019a); Lyu & Li (2020) further promoted the study of implicit bias of
gradient descent for standard training to the case of more general homogeneous non-linear neural
networks and proved that the limit point of the optimization path is along the direction of a KKT
point of the max-margin problem. Wei et al. (2019) studied the regularization path for homogeneous
DNNs and also proved the convergence to the max-margin direction in this setting. Banburski et al.
(2019) showed that gradient descent induces a dynamics of normalized weights converging to an
equilibrium which corresponds to a minimal norm solution.
It is therefore our goal in this paper to theoretically understand the resistance to adversarial examples
for adversarially trained DNNs, linear and non-linear ones, through the lens of implicit bias imposed
by adversarial training. Due to the inner maximization, adversarial training differs a lot from standard
training and one should be careful to analyze the perturbed training dynamics. For the adversarial
training objective Eq. (1), various approaches have been proposed to solve the inner maximization,
such as fast gradient sign method (FGSM (Goodfellow et al., 2015)) and its stronger version projected
gradient descent (PGD (Madry et al., 2018)). The widely used adversarial training adopts PGD to
attack the model, while recent work (Wong et al., 2020) also suggested that a weaker adversary
can also surprisingly yield a model with satisfying robustness. Thus to conduct a comprehensive
study about the implicit bias of adversarial training for DNNs, we will use '2-fast gradient method
(FGM (Miyato et al., 2016)), '∞ FGSM, '2-PGD and '∞-PGD to solve the inner maximization of
the adversarial training objective.
1.1	Our Contribution
In this paper, we devote to answering two questions. First, is there any implicit bias imposed by
adversarial training for DNNs without explicit regularization? Second, if there exists such an implicit
bias, what are the convergence properties of the model parameters along the adversarial training
trajectory?
To this end, we first investigate the adversarial training with '2 adversarial perturbations for deep
linear networks on linear separable data where the allowed Euclidean distances from the adversarial
examples to their corresponding original examples kx0i - xi k2 are less than the max-'2-margin of
the original dataset. Despite the simplicity of this setting, this problem is meaningful due to its
non-convexity and the introduction of adversarial examples, which heavily depend on the model
parameters, during the training process. We prove that gradient descent for adversarial training
2
Published as a conference paper at ICLR 2022
implicitly aligns weight matrices across layers and the direction of the product of weight matrices
also surprisingly converges to that of the max-'2-margin solution of the original dataset-similar to
that of standard training for deep linear network (Ji & Telgarsky, 2019). Our results significantly
generalize those in Li et al. (2020) for adversarial training of logistic regression. This simple yet
insightful case positively answers our first question but partially answers the second one because it
still remains unclear why such convergence property can improve robustness considering its similarity
to that for the standard training. In fact, adversarial training differs from standard training in the
way how they impose such convergence property for parameters: the first one is to maximize the
margin of adversarial examples while the latter is maximizing that of the original dataset, and these
two optimization problems happen to possess solutions along the same direction.
We then move forward to explore a more general situation, adversarial training for homogeneous
non-linear DNNs without the linear separability of the dataset. We study the limit point of the
normalized model parameters along the adversarial training trajectory and show that
Theorem 1 (Informal). When the deep neural network is adversarially trained with one of the
'2 -FGM, FGSM, '2 -PGD and '∞ -PGD perturbations, the limit point of the normalized model
parameters is along the direction of a KKT point of a constrained optimization problem which aims
to maximize the margin of adversarial examples.
This indicates that adversarial training is implicitly maximizing the margin of adversarial examples
rather than that of original dataset. Thus Theorem 1 provides another view for the high bias error
on clean examples of adversarial training in Yu et al. (2021) since distributions of adversarial and
clean examples are different. To the best of knowledge, these results are the first attempt to analyze
the implicit bias of adversarial training for DNNs. We believe our results provide a theoretical
understanding on the effectiveness of adversarial training for improving robustness against adversarial
examples. It could potentially shed light on how to enhance the robustness of adversarially trained
models or even further inspire more effective defense mechanisms.
Organization. This paper is organized as follows. Section 2 is about notations and settings. Section
3 presents our main results on the implicit bias of adversarial training for DNNs. Section 4 provides
numerical experiments to support our claims. We conclude this work in Section 5 and discuss future
directions. Some technical proofs are deferred to supplementary materials.
Algorithm 1 Adversarial Training
Input: Training set S = {(xi, yi)}in=1, Adversary A to solve the inner maximization, learning
rate η, initialization Wk for k ∈ {1, . . . , L}
for t = 0 to T - 1 do
S 0 (t) = 0
for i = 1 to n do
x0i(t) = A(xi,yi,W(t))
S0(t) = S0(t)2 * * S(x0i(t),yi)
end for
for k = 1 to L do
Wk(t + 1) = Wk (t) - η(t) dL(∂Wk;W)
end for
end for
2 Preliminaries
Notations. For any matrix A ∈ Rm×n, we denote its i-th row j-th column entry by Aij . Let AT
denote the transpose of A. ||』f represents the Frobenius norm and ∣∣ ∙ ∣∣p is the 'p norm. The training
set is {(xi, yi)}in=1 where xi ∈ Rd, kxik2 ≤ 1 and yi ∈ {1, -1}. For a scalar function f : Rd 7→ R,
We denote its gradient by Vf. Furthermore, tr (A) = Pi Aii denotes the trace for the matrix A.
We study the adversarial training for the L-layer positively homogeneous deep neural network
f(x; W) = WL夕L (Wl-1 …23(W222(W1x))…)	(2)
3
Published as a conference paper at ICLR 2022
where Wk is the k-th layer weight matrix and 夕k is the activation function of the k-th layer 1. The
multi-c-homogeneity of the network is defined by
L
f (x; a1W1,…，OlWl) = Y akf(x; W)	(3)
k=1
for any positive constants ak's and C ≥ 1, where W = (Wι,…,WL) is the collection of the
parameters of the network. For example, deep ReLU networks are multi-1-homogeneous. For
convenience, we also adopt the following notations for a multi-c-homogeneous DNN:
Pk = l∣WkkF, ρ = ρ1 …PL	and f(x; W) = Pf (x; c),	(4)
where Wck = Wk/kWkkF with kWckkF = 1 fork ∈ {1, . . . ,L}.
We use δi(W) to represent the adversarial perturbation of the original example xi within the perturba-
tion set B(0, ) : {δ : kδkp ≤ } around the original example xi for f(x; W). Furthermore, we use
the scale invariant adversarial perturbations defined as follows for adversarial training in this paper.
Definition 1 (Scale invariant adversarial perturbation). An adversarial perturbation is said to be a
scale invariant adversarial perturbation for f(xi; W1, . . . , WL) and loss function ` if it satisfies
δi(a1W1, . . . , aLWL) = δi(W1, . . . , WL)	(5)
for any positive constants ak ’s.
We will show in Section 3.2 that FGSM, '2-FGM, '2-PGD and '∞-PGD perturbations for homoge-
neous DNNs are all scale invariant perturbations, which are important for analyzing different types of
perturbation in a unified manner. The empirical adversarial training loss with the perturbation δi (W)
is given by
nn
L(W) = -X '(yi,χi; W) = -X' 3f(χi + 6(w ); W)).	(6)
n i=1	n i=1
For ease of notation, we denote 'i (W) = '(yi, xi; W). The loss function ' is continuously differen-
tiable and satisfies
Assumption 1 (Loss function). ' > 0, '0 < 0, limx→∞ '(x) = 0 and 'x→-∞(x) = ∞.
Many widely used loss functions satisfy the above assumption such as '(x) = e-x and the logistic
loss '(x) = ln(- + e-x). Furthermore, we make the following common assumptions about the
smoothness of f(x; W) and the adversarial perturbation δ(W):
Assumption 2 (Smoothness). With respect to W, yf (x; W) is locally Lipschitz for any fixed x;
yif(xi; W) further have locally Lipschitz gradients and δi(W) are locally Lipschitz for all training
examples xi.
Remark. Our results can also be generalized to non-smooth homogeneous neural networks straight-
forwardly (Appendix B.4). Assuming Lipschitzness about perturbations is because we focus on
popular perturbations such as '2-FGM and PGD perturbations which have explicit forms and depend
on gradients of the network, whose Lipschitzness assumptions are quite common.
3	Main Results
We present our main theoretical results on the implicit bias of the gradient flow/gradient descent for
adversarial training in this section. For the deep linear neural network, a special kind of homogeneous
models, in Section 3.1 we further restrict the dataset to be linearly separable and focus on'2 adversarial
perturbations. We prove that the singular vectors corresponding to the largest singular values of
weight matrices get aligned across layers with the progress of adversarial training1 2 . Based on this key
result, the product of weight matrices converges to the direction of the maximum margin solution
1 夕k is a vector in our notations.
2This phenomenon for standard training has been studied by Ji & Telgarsky (2019) first.
4
Published as a conference paper at ICLR 2022
under the original data. Furthermore, we study a much more general scenario in Section 3.2: without
the assumption of linearly separability of data and norm constraints on adversarial perturbations,
we show that the gradient flow of adversarial training with scale invariant adversarial perturbations
for the homogeneous nonlinear neural networks implicitly performs margin maximization under
the adversarial data. Due to the space limit, some technical proofs of Section 3.1 are deferred to
Appendix A, and we present proofs of Section 3.2 in Appendix B.
3.1 Adversarial Training for Deep Linear Network
In this section we restrict the dataset {(xi, yi)}in=1 to be linearly separable in the sense that there
exists a unit vector u such that ∀i ∈ {1, . . . , n}, yi hu, xii > 0 and we explore the adversarial
training dynamics of gradient descent for deep linear networks, i.e., with identity activation function
ψk (x) = X for all layers,
f (x,W) = WL …WlX.	⑺
Now let
U = arg max min	yi hxi, Ui
kuk=1i∈{1,...,n} i i
be the max-margin solution given by the hard-margin SVM and γm = maxkuk=1 ym hXm, Ui be
the max-margin, where m = argminy i n} yi hxj, u). We only consider the '2-norm adversarial
perturbations in this section, i.e., B(0, ) = {δ : kδk2 ≤ }. The allowed Euclidean distance from an
adversarial example X0 to its original example X is assumed to satisfy
kX0 - Xk2 ≤ ≤ γm .
(8)
We denote the product of all weight matrices by Wn = WL …Wi, which is simply a vector. Our
model is adversarially trained by gradient descent
Art+、
Wk (t + 1) = Wk (t) - η(t) ∂Wk(t).
(9)
The inner maximization of the adversarial training objective Eq. (1) can be solved exactly in this case,
where the adversarial perturbations are taken as
δi(W (t)) = Fe kW((∣2
(10)
for the t-th iteration of adversarial training. For any layer k, the weight matrix can be decomposed as
Wk = UkΣkVkT
where Uk and Vk are the left and right singular matrices, respectively. Furthermore, we use Uk and vk
to represent the left and right singular vectors corresponding to the largest singular value, respectively.
We emphasize that the training dynamics abruptly changes due to this perturbation and one should
be careful to analyze its effects. It can be easily seen that Eq. (10) is a scale invariant adversarial
perturbations by our definition. Note that with this kind of perturbation, the adversarial data during
the training are still linearly separable, since we have required that the perturbation is smaller than
the max-margin of the original dataset described in Eq. (8). We further assume that ` satisfies
Assumption 3. The loss function ' is β-smooth and |'0| ≤ α.
such as logistic loss. We are now ready to explore the implicit bias of the gradient descent for
adversarial training. Inspired by the idea of studying the smoothness of loss function from Ji &
Telgarsky (2019) for standard training, here we first examine whether our adversarial training loss
Eq. (1) possesses smoothness. Let the set S(r) denote the set of weights with bounded norm
S(r) = {w∣kWk∣∣f ≤ r, kWπk2 ≥ r,k = 1,...,L} ,	(II)
where r ≥ 1 and r is a small constant to avoid trivial solution. To simplify the notation, we let
β(r, r, e) = r3-L2
e	αβ
α + β + =L I 2α + β + -；-
rL ∖	r-
(12)
The following lemma provides us a first view of the overall trends of gradient descent for adversarial
training in deep linear network on linear separable dataset.
5
Published as a conference paper at ICLR 2022
Lemma 1 (Overall trends of adversarial training for deep linear network). With adversarial pertur-
bation Eq. (10), under Assumption 1, Assumption 3 and the requirement Eq. (8), for the deep linear
network adversarially trained by gradient descent on linear separable data, the adversarial training
objective (1) is β(r, f, e)-smooth within the given set S(r) for a constant r. However, with constant
step size for gradient descent, the weight matrices will eventually leave the set S(r) if r remains
unchanged during the training
max	kWk (t1 )kF > r for some t1 > 0.	(13)
k∈{1,...,L}
The first part of this lemma implies the smoothness of the adversarial training loss Eq. (1) while also
leading the weight matrices to leave the set S(r). In fact, this is saying that maxk∈{1,...,L} kWk kF is
divergent along the adversarial training trajectory since maxk∈{1,...,L} kWkkF will be larger than r
for any given r. We can keep W(t + 1) inside S(r(t)) at every step t by delicately adjusting the step
size of gradient descent and treating r as a function of the step t.
Lemma 2 (Smoothness of the adversarial training loss). If the learning rate is taken as η (t) =
min{1,1∕β(r, r, e)}, then the adversarial training loss L(W) is β(r, r, e)-smooth where r(t + 1) is
taken as
r(t + 1) = {r(t) + μ(t)
if W (t + 1) ∈S (r(t) - μ(t)),
otherwise
during the training, where μ = rL-1 (1 + e)ɑ∕β(r, r, C).
We show the divergence of maxk∈{1,...,L} kWkkF along the adversarial training trajectory in Lemma
1, and now we can further derive that the Frobenius norms of weight matrices for all layers are
divergent with the smoothness of the adversarial training loss in hand, considering that the differences
between any two layers during the adversarial training are always bounded (Lemma 6). Due to this
divergence of weight norms, only the direction of the product of weight matrices is important for the
model to make predictions.
Now we present our main theorem to provide insights on the theoretical understanding of the implicit
bias of gradient descent for adversarial training of deep linear network.
Theorem 2 (Convergence to the direction of the max-margin solution). With adversarial perturbation
Eq. (10), under Assumption 1, Assumption 3 and requirement (8), for gradient descent of the
adversarial training objective Eq. (6) with logistic loss, the singular vectors of the largest singular
values of adjacent layers get aligned if the learning rate is taken as that in Lemma 2:
∀k ∈ {1, . . . , L} :	|huk, vk+1i| → 1	(14)
as t → ∞. As a result, the direction of the product of weight matrices converges to that of the
max-margin solution
Wn
kW∏k2
→ uf.
(15)
Remark. The alignment phenomenon for standard training has been showed in the previous work
Ji & Telgarsky (2019), here we further extend this result to the case of adversarial training, where the
training objective is different from that of the standard training due to the introduction of adversarial
perturbation dependent on the parameters of the network. Our work is also a significant generalization
of Li et al. (2020) which studied the convergence to max-margin solution of adversarial training for
logistic regression, a single-layer linear network.
Furthermore, although the direction of the solution Eq. (15) given by adversarial training does not
differ from that of standard training, i.e., the direction of uf, these two solutions are not same: the
first is maximizing the margin of adversarial examples while the latter is maximizing that of original
dataset—which happens to be the same one in this setting. The similarity of these two solutions
comes from the fact that the adversarial data are also linearly separable under the requirement Eq. (8),
whose max-margin solution has the same direction as that of uf. The above reasoning will be more
clear in the next section where we show the solution of adversarial training as a max-margin solution
of adversarial examples more explicitly.
6
Published as a conference paper at ICLR 2022
3.2 Adversarial Training for Homogeneous Deep Neural Network
Additional definitions For simplicity, We will use exponential loss '(x) = e-x in this section.
For an original example Xi, the margin for its adversarial example Xi + δi(W) is defined as Y =
yif(Xi + δi(W); W) while for the whole dataset {(Xi, yi)}in=1, the margin for their corresponding
adversarial examples is denoted by YYm where m = arg minm∈{1,...,n} yif(Xi + δi(W); W). We
..1	1∙	1	∙	”	， 「小、G 」	1	..
introduce the normalized margin as Yi = yif (Xi + δi(W); W) to explore the convergence properties,
where Wck
Wk/ kWk kF such that kWk kF = 1 for any layer k . Furthermore, the margin for the
original example Xi is defined to be Yi = y%f (xi； W) ≥ %. To relieve the headaches of picking
suitable learning rates, we consider gradient flow, gradient descent with infinitesimal step, of the
training loss Eq. (6), which leads to
dWk
dt
(16)
—
for k ∈ {1, . . . , L}. Note that the introduction of adversarial perturbations (relying on the network
parameters) abruptly perturbs the training dynamics. In this section, to conduct a comprehensive study
of adversarial training, we do not restrict the perturbation used for solving the inner maximization of
the adversarial training objective in advance while only require it to be a scale invariant adversarial
perturbation. By noting the following lemma, we can easily generalize our results to the commonly
used adversarial attacks:
Lemma 3. Under Assumption 1, '2 -FGM, FGSM, '2 -PGD and '∞ -PGD Perturbationsfor homoge-
neous DNNs are all scale invariant adversarial perturbations.
The RHS of Eq. (16) is different from that of standard training due to the inner maximization. Our
key observation is the following property of adversarial training which can drastically simplify our
analysis despite of the complexity brought by the adversarial examples.
Lemma 4. For the exponential loss, along the gradient flow trajectory of adversarial training with
scale invariant adversarial perturbations for homogeneous DNN f(X; W), we have
(17)
In the case of deep linear network, we adopt linearly separable dataset with the requirement Eq. (8),
yielding the fact that the adversarial data are also linearly separable. In this section we do not impose
such conditions but only apply a weaker assumption in a more general scenario.
Assumption 4. There exists a separability of adversarial examples of the dataset3 : there exists a
time t0 such that yif(Xi + δi(W(t0)); W(t0)) > 0 for all i ∈ {1, . . . , n}.
Theoretically, Zhang et al. (2020); Gao et al. (2019) proved the convergence to low robust training
loss for heavily overparametrized nets. Moreover, adversarial training can typically achieve this
separability in practice, i.e., the model can fit adversarial examples of the training dataset, which
makes the above assumption a reasonable one. In Section 3.1, we obtain divergent weight norms for
adversarial training. Thanks to Lemma 4, we can further generalize it to the case of homogeneous
network without linearly separability of dataset and the requirement Eq. (8) for adversarial examples.
This divergence of model parameters is necessary for the convergence of the adversarial training loss.
To begin with, recalling our definition of ρ = QkL=1 ρck = QkL=1 kWk kcF, we state our first main
theorem regarding the overall trend for gradient flow of adversarial training as follows.
Theorem 3 (Divergence of weight norms). For the adversarial training objective (6) of the binary
classification task with exponential loss and scale invariant adversarial perturbations, under Assump-
tion 2 and Assumption 4, the product of the Frobenius norms of weight matrices diverges to infinity
along the trajectory of the gradient flow
P = Ω (lnt) → ∞,
and, as a result, the training loss L converges to zero as t → ∞.
3Similar assumptions are previously used in Lyu & Li (2020); Ji & Telgarsky (2020)
7
Published as a conference paper at ICLR 2022
Remark. The divergence of weight norms for standard training of homogeneous DNNs has been
studied by, for example, Lyu & Li (2020); Wei et al. (2019). The above theorem considers adversarial
training, which implies that the margin for an adversarial example, Yi for any i ∈ {1,...,n}, also
goes to infinity along the adversarial training trajectory. This makes the normalized margin Yi
necessary to understand the convergence properties of the adversarial training solution as t → ∞.
Built upon this result, we first shed some lights on the implicit bias of the gradient flow for the
adversarial training through the following theorem, which discusses the training dynamics of the
normalized margins for the adversarial examples.
Theorem 4 (Non-decreasing adversarial margin). For adversarial training with scale invariant
adversarial perturbations, when the separability of adversarial examples is achieved at t0, the
weighted sum of the changing rates of normalized margins for the adversarial examples are non-
negative for all t ≥ t0 :
nd
∀t ≥ to ： X e-ρ)i -Yi ≥ 0;	(18)
i=1
there exists a sufficiently large time tι > to such that dγm ≥ 0 if ∀i : Yi 一 Ym = ω(P) for all t ≥ tι.
The above theorem reveals a relation for the normalized margins for adversarial examples. According
to the above lemma, since P → ∞ as t → ∞, the term with weight e-ργm, roughly speaking,
dominates the LHS of(18) after some time tι ≥ to. Then -γm∕-t4 will never be negative for ∀t ≥ tι.
This directly implies that the gradient flow is actually maximizing the margin for the adversarial
examples of the original dataset. Applying Lemma 4 and Theorem 3, in the following theorem, we
further strengthen the above observation for adversarial training for homogeneous DNNs.
Theorem 5 (Convergence to the direction of a KKT point). Under Assumption 2 and 4, for ex-
ponential loss and multi-c-homogeneous DNNs, the limit point of normalized weight parameters
{W/kW k2 : t ≥ 0} of the gradient flow for the adversarial training objective Eq. (6) with scale
invariant adversarial perturbations is along the direction of a KKT point of the constrained norm-
minimization problem
wmin	1 kWk2	S." ≥ 1 ∀i ∈{1,...,n}.	(19)
W1 ,...,WL
Remark 1. According to Lemma 3, the above theorem can be directly applied to '2-FGM, FGSM,
'2 and '∞-PGD perturbations. The requirement Eq. (8) in Section 3.1 is not required since the dataset
in this section is no longer linearly separable. It is well-known that the norm-minimization problem
is closely related to the margin-maximization problem in the sense that
Optimizationproblem(19) y⇒ max	5m,	s.t. ∣∣W∣∣2 = 1.
W1,...,WL	2
Remark 2. Theorem 5 is the key theorem of this section. It shows that, for adversarial training,
the gradient flow is implicitly maximizing the margin of adversarial examples. Therefore, for
perturbations with norm constraints other than '2-norm, (19) is no longer an '2-norm optimization
problem but a mixed-norm one due to the perturbation δ(W) in Ym, which was first observed by
Li et al. (2020) for adversarial training of one-layer neural network while our results are for more
general deep non-linear homogeneous neural networks. This is one of the major differences between
adversarial training and standard training—they are formulated from solving different optimization
problem. Theorem 5 provides theoretical understandings of the folklore that adversarial training can
modify the decision boundary against adversarial examples to improve robustness.
Remark 3. In the context of standard training, Lyu & Li (2020) studied the properties of the nor-
malized margin through its approximation, the smooth-normalized margin. One can try to generalize
the analysis method in Lyu & Li (2020) to adversarial training with various kinds of adversarial
perturbations by further assuming the local smoothness regarding gradients when considering non-
smooth analysis. Generalizinig our results to non-smooth case can be found in Appendix B.4. To
easily adapt to adversarial training, our new strategy in this work is to directly analyze the normalized
margin itself for adversarial examples instead and utilizes the alignment phenomena observed by
Ji & Telgarsky (2020). Most importantly, what we seek to push forward in the current work are the
theoretical understandings of the adversarial training for DNNs through its implicit bias.
4Strictly speaking, this is not well-defined since m may not be unique. However, we can use this definition
in an approximately correct way for t sufficiently large.
8
Published as a conference paper at ICLR 2022
U 一 6」笛UJ P①N一2E」ou
U-proE P ①~-roEJ OU-roμroSJ① >pro
C-EnE pω--Bau OU-里-BS,Jω>PB
(a) FGSM adversarial examples (b) '∞ -PGD adversarial examples (C) FGSM adversarial examples
Figure 1: Adversarial training for the 3-layer neural network on MNIST. Adversarial normalized
margin for (a) FGSM adversarial examples; (b) '∞-PGD adversarial examples; (C) FGSM adversarial
examples after 10000 epoChs.
Trade-off between robustness and accuracy. More signifiCantly, Theorem 5 also implies that
adversarial training leads to the trade-off between robustness and aCCuraCy as in Zhang et al. (2019).
This can be seen as follows. Let f (∙, W) denote the classifier obtained from adversarial training.
Then for a Clean training example (x, y) with y = 1 for simpliCity, we have yf(x + δ(W); W) =
f(x + δ(W); W) > 0 since f(x; W) is a max-margin classifier over adversarial data. This means
that, for any clean test example (x0, y0) around x in the sense that x0 = x + τ with kτkp ≤ , we will
have
f (x0, W) = f(x + T; W) ≥ f(x + δ (W); W) > 0	(20)
due to the definition of δ(W) when y = 1: δ(W) = maxv∈B(x,e) '(yf (X + v; W)) =
minv∈B(x,) f(x + v; W) for `0 < 0. Therefore, as a result of forcing our model to correctly
classify the adversarial example x + δ(W), it will classify any such x0 = x + τ as having labels 1
even there may be some of them having true labels -1, which leads to the drop of accuracy at the cost
of increasing robustness. Besides, since adversarial training maximizes the margin of the specific
adversarial perturbation used for training, the trained model performs worse on other perturbations.
4	Numerical Experiments
In this section, we conduct numerical experiments on MNIST dataset to support our claims. We
adversarially trained a 3-layer neural network using SGD with constant learning rate and batch-size 80.
The model has the architecture of input layer-1024-ReLU-64-ReLU-output layer. We present results
for adversarial training with: (1) FGSM perturbations with e = 16/255; (2) '∞-PGD perturbations,
where the PGD is ran for 5 steps with step size 6/255 and = 16/255. As a comparison, we
also standardly trained a model with the same architecture to evaluate the normalized margin for
adversarial examples by attacking it with FGSM and PGD during its training process.
Fig. 1(a) shows that the normalized margin for FGSM adversarial examples keeps increasing during
the process of adversarial training with FGSM perturbations. Meanwhile, the standardly trained
model has lower normalized margins for FGSM adversarial examples. Similar results exist for
'∞-PGD adversarial examples as showed by Fig. 1(b). To see that adversarial normalized keeps
increasing more clearly, we also plot the adversarial training with FGSM perturbations after 10000
epochs in Fig. 1(c). More detailed experiments are in Appendix C. These empirical findings support
our claims that adversarial training implicitly maximizes the margin of adversarial examples.
5	Conclusion
In this paper, we have studied the implicit bias of adversarial training for the deep linear networks and,
more generally, the homogeneous DNNs. We proved that adversarial training with scale invariant
adversarial perturbations implicitly performs margin-maximization for adversarial data during its
training process. Intuitively, such implicit bias strongly implies that adversarial training encourages
the neural networks to utilize adversarial examples more to improve its robustness. It will be an
interesting future direction to improve the effect of adversarial training by promoting it to enlarge
the margins of adversarial examples more effectively. It is also possible to design new defense
mechanisms by explicitly requiring the standard training to have the bias of maximizing the margin
of adversarial examples. We will leave these potential benefits of our theoretical analysis as future
work.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This project is supported by Beijing Nova Program (No. 202072) from Beijing Municipal Science
Technology Commission.
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In arXiv:1802.06509, 2018.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. In International Conference on Machine learning(ICML),
2018.
A. Banburski, Q. Liao, B. Miranda, T. Poggio, L. Rosasco, F. D. L. Torre, and J. Hidary. Theory iii:
Dynamics and generalization in deep networks. In arXiv:1903.04991, 2019.
Frank H Clarke. Generalized gradients and applications. 1975.
Frank H Clarke. Optimization and nonsmooth analysis. 1983.
R.	Collobert and J. Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine
learning(ICML), 2008.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. In arXiv:1804.07795, 2018.
S.	S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. In arXiv: 1806.00900, 2018.
J.	Dutta, K. Deb, R. Tulshyan, and R. Arora. Approximate kkt points and a proximity measure for
termination. Journal ofGlobal Optimization, pp.1463-1499, 2013.
Fartash Faghri, Sven Gowal, Cristina Vasconcelos, David J. Fleet, Fabian Pedregosa, and Nico-
las Le Roux. Bridging the gap between adversarial robustness and optimization bias. In arXiv:
2102.08868, 2021.
R. Gao, T. Cai, H. Li, L. Wang, C. Hsieh, and J. D. Lee. Convergence of adversarial training in
overparametrized neural networks. In arXiv:1906.07916, 2019.
I.	Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR), 2015.
Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. In arXiv: 1806.00468, 2018.
C. Guo, M. Rana, M. Cisse, and L. van der Maaten. Countering adversarial images using input
transformations. In International Conference on Learning Representations(ICLR), 2018.
K.	He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level per-
formance on imagenet classification. In International conference on computer vision (ICCV),
2015.
Z. Ji and M. Telgarsky. Risk and parameter convergence of logistic regression. In arXiv:1803.07300,
2018.
Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In International
Conference on Learning Representations(ICLR), 2019.
Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. In arXiv:2006.06657,
2020.
10
Published as a conference paper at ICLR 2022
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenent classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems(NeurIPS), 2012.
Y. Li, E. X.Fang, H. Xu, and T. Zhao. Inductive bias of gradient descent based adversarial training on
separable data. In International Conference on Machine Learning(ICLR), 2020.
K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. In
International Conference on Learning Representations(ICLR), 2020.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Representations(ICLR), 2018.
T. Miyato, A. M. Dai, and I. Goodfellow. Adversarialtraining methods for semi-supervised text
classification. In arXiv:1605.07725, 2016.
M. S. Nacson, S. Gunasekar, J. Lee, N. Srebro, and D. Soudry. Lexicographic and depth-sensitive mar-
gins in homogeneous and non-homogeneous deep models. In Proceedings of the 36th International
Conference on Machine Learning, 2019a.
M. S. Nacson, N. Srebro, and D. Soudry. Stochastic gradient descent on separable data: Exact
convergence with a fixed learning rate. In Proceedings of Machine Learning Research, 2019b.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Conference on computer vision and pattern recognition
(CVPR), 2015.
Ohad Shamir. Gradient methods never overfit on separable data. In Journal of Machine Learning
Research, 2021.
D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. Journal ofMachine Learning Research, pp. 1-57, 2018.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014.
C. Wei, J. D Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of neural
nets v.s. their induced kernel. In Advances in Neural Information Processing Systems(NeurIPS),
2019.
E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations (ICLR), 2020.
C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating adversarial effects through randomization.
In International Conference on Learning Representations (ICLR), 2018.
Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, and Yi Ma. Understanding gen-
eralization in adversarial training via the bias-variance decomposition. In arXiv: 2103.09947,
2021.
H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically principled trade-off
between robustness and accuracy. In International Conference on Machine Learning(ICLR), 2019.
Y. Zhang, O. Plevrakis, S. S. Du, X. Li, Z. Song, and S. Arora. Over-parameterized adversarial
training: An analysis overcoming the curse of dimensionality. In arXiv: 2002.06668, 2020.
A Proofs of Section 3.1
In Section A.1, we discuss the smoothness of the adversarial training loss. In Section A.2, we prove
the alignment phenomenon of adversarial training for deep linear networks. We make the common
assumption that L(W(0)) ≤ L(0) = '(0) and ▽£(W(0)) =0 to start adversarial training.
11
Published as a conference paper at ICLR 2022
A.1 Smoothness of the adversarial training loss
A.1.1 Proof of Lemma 1
Proof. We first prove that the adversarial training loss for the deep linear network is smooth within
S(r). For any W, V ∈ S(r), let δW and δV denote the adversarial perturbations of the network
f(x; W) and f(x; V ), respectively. Then we have, by taking derivatives of L w.r.t W and using that
kAkF+kBkF≥kA-BkF,
_ ~	_ ~
∂ L	∂L
---- - T--
∂W1 ∂V1
n
≤ 1 XIlWT …WT XT y 洛(Wn)- VT …VL XT y 必(Vh) L
F i=1
I------------------------------------------'
{^^^^^^^™
(a)
∂ kW∏k 西W、	∂ kV∏k
FrG(Wn)- Fr
~，
~.,
'i(V∏),
F
(21)
n
+n X
{z
(b)
1	.1	.	/I- \	1	.1	1	.1	.	•	•	1	♦	L	♦	•!♦	♦,	1 ^r∣l • . 1	.
where the term (b) abruptly changes the training dynamics. For simplicity, we use X, y and `0 without
the subscript i because the analysis can be done similarly for any i ∈ {1, . . . , n}. We begin with the
term (a) of Eq. (21):
(a) ≤ ∣∣WT …WTxTy'0(W∏) - VTWT …WTXTy^0(W∏)∣∣
+ IIVTWT …WlTxTy'0(W∏) - VT …VTXTyZ0(Vh)II
≤ IW2 - V2k IIWT …WTXTyi0(Wn)∖
+ IMk IIW3T …WTXTy'0(Wn) - VT …VTXTyZ0(Vh)II
≤ kW2 - V2k rL-2α + HIWf …WTXTyz0(Wn) - VT …VTXTyz0(V∏)II
dL
≤ XrL-2α kWk- Vkk + rL-2kVLk "0(Wn) - ZO(Vh)(
k=2
e
≤ rL-2α(L -1) + βLr2L-2 kW - Vk ,	(22)
where c follows from kXk ≤ 1, kWk k ≤ r and `0 ≤ α; repeating the procedure of c until k = L
gives us d; e is because kWk - Vk k ≤ kW - Vk for any k and
IIZ0(Wn) - ZO(Vh)II ≤ βkWn - Vhk ≤ βLrLTkW - Vk	(23)
considering that ` is a β-smooth function. It now remains us to bound the term (b) of Eq. (21). Note
that
∂ IlWnk2 _ WT …WTWL …W1
∂W1	= P⅛
(24)
12
Published as a conference paper at ICLR 2022
then we have
(b) ≤
WT …WT Wl …Wi
kWΠk
40(W∏) -
v2t WT …WT WL …Wi
kWΠk
70(Wn)
VT W3T …WT WL …W1
kWΠ k
(WΠ)
VT VT …VT VL …H
kVΠ k
'0(V∏)
Eαr2L-2
-∣W2 -/I
WT -WT Wl …Wi
rL
(2L-
1)
αr
kWΠ k
2L-2
/(W∏)-
VT…VT VL…匕
kVΠ k
'0(V∏)
rL
kW - Vk +r
2L-2 Il Z0(Wn)
Il IWnk
'0(V⅞)
I^k
(2L-
Eαr2L-2	Er2L-2
1)IW - Vk + -r^ 卜(Wn)
Eαr2L-2
+ —2^ IWn-Wk
c
≤
d
≤
e
≤

+
—
+ r
—
—
r2L-2
f
≤
-1)α + LLrLT + 0βLLL 1 ) IlW - Vk ,
rL	J
(25)
where c follows from kWkk ≤ r and% ≤ α; repeating c for 2L - 1 times gives d; e follows from
kkWnk-kVnkk ≤ kWn - Vnk; invoking Eq. (23) gives us f. Now we can bound Eq. (21) by
combing the above results
_〜 -~
∂L	∂ L
---- - T--
∂Wι	∂V1
F
≤	rL-2
≤ r3LL
r2L-2
α(L - 1) + βr2L-2L +——Tr-
α+β+rL (2α+β+(OL)]
-1)α + LLrL-I + ^LLL-
kW-Vk
kW-Vk,
(26)
where the last inequality is because we choose r ≥ 1. The above analysis is for the first layer. Now
we can apply the same technique to other layers, which finally gives us
∣∣vL(w) - VL(V)k ≤ v3ll2
α + β + rL (2α + β + ~Lj} ] kW - V k,	(27)
thus L is β(r, r, E)-Smooth inside S(r) where
L(r, r,e) = v3lL2
αβ
α + β + 二L ( 2α + β + =L
rL ∖	rL
(28)
We now prove the second statement of Lemma 1, which says that W will leave S(r) with constant r
and learning rate η. Let W(t), W(t + 1) ∈ S(r) and η(t) = min{1,1∕β(r,尸,e)}.
Lemma 5 (Adapted from Lemma 6 in Ji & Telgarsky (2019)). Under Assumption 1 and Assumption
3, suppose gradient descentfor adversarial training is run with a constant step size 1∕L(r, e,尸).Then
there exists a time t when maxk kWk(t)kF > r.
I - ∕cr∖ .	. 1	, U ♦ C/ —	∖	.1	.1	r∙
Eq. (27) states that L is L(r, r, E)-Smooth, therefore
L(W(t + 1)) - L(W(t)) ≤ DVL(W(t)), -η(t)L(w(t)))+ Ly)η(t)2 ∣vL(w(t))∣2
=-粤kVL(W(t))∣2,	(29)
which also implies that L(W (t)) never increases during the training
L(W(t + 1)) ≤ L(W(t))-吟∣∣vL(w(t))∣2.	(30)
By repeatedly applying Lemma 5 when the learning rate is constant, we know W will eventually
leave S(r), which also implies that maxk ∣∣Wk∣∣f is unbounded in this case.	□
13
Published as a conference paper at ICLR 2022
A.1.2 Proof of Lemma 2
Proof. For any step t with the learning rate in the Lemma 2, suppose that W(t) ∈ S(r(t) - μ(t)),
we then have
kWk(t+1)kF ≤ kWk(t)kF+η(t)
~
∂L
∂Wk (t)
≤kWk (t)kF + μ(t),
F
where the last inequality follows from
Ar
-^πτy ≤∣∣Wk+ι …WTXTWT …wk-ιy'0(w∏)∣∣ + e -L∏ɪ'0(W∏)
∂Wk (t)	∂Wk
≤ (1 + ^∑l)αr2L∙	(31)
Therefore kWk (t + 1)kF will always stay in S(r(t)) and the smoothness is preserved. If
kWk (t + 1)kF is still less than r(t) - μ(t), then we keep r(t + 1) = r(t) while we increase
r(t + 1) to
r(t +1)= r(t) + μ(t)
if r(t) - μ(t) ≤ IlWk(t + 1)∣∣F ≤ r(t) such that ∣∣Wk(t + 1)|后 ≤ r(t) ≤ r(t +1) - μ(t + 1),
which means that W(t + 2) will stay inside S(r(t + 1)). The overall smoothness can then be proved
by induction.	□
A.2 Alignment phenomenon
We start with the following lemma for the adversarial training to prove the alignment phenomenon5:
Lemma 6 (Bounded differences between weight norms). The difference of Frobenius norms for any
two layers is bounded during the adversarial training process
∣Wk(t)∣2F-∣Wj(t)∣2F	-	2Lr(W (0)) ≤	∣Wk(0)∣2F-∣Wj(0)∣2F, ∀k,j∈{1,...,L}.
Proof. If there is not adversarial perturbation, then we can easily obtain
T	-L	-L	T
Wk+1 -W+1 = -Wk Wk	(32)
by conducting some algebra, where L stands for the empirical loss without adversarial perturbation.
Then invoking Eq. (24) for k by noting that tr (A) = tr (AT) and tr (ABC) = tr (CAB), We have
y*(W∏)Wk+ι-∣-πk2 =	Wk+1 …WTWL …WiWT …WT
-Wk+1	∣WΠ ∣2
=y*(W∏)喘⅛ WT.
Wk
(33)
Combining Eq. (32), for the adversarial empirical loss, we have
WkT+1
~
∂ L
-Wk+i
_ ~
盖WT,
(34)
which will give us the following relation
WkT+i(t + 1)Wk+i(t + 1) = WkT+i(t)Wk+i(t) + η(t)2
-L !T (	-L
-Wk+ι(t)	1-Wk+ι(t)
-η(t)R+ι(t)( -Wk‰ ) + (占 !T Wk+ι(t)
5This property for standard training has previously been studied by Ji & Telgarsky (2019); Arora et al. (2018).
14
Published as a conference paper at ICLR 2022
which can be easily seen by writing out the gradient descent update explicitly. One can derive a
similar relation for Wk(t + 1)WkT (t + 1). Combing Eq. (32), we have
〜
∂ L
〜
∂ L
WkT+1(t + 1)Wk+1(t + 1) - WkT+1(t)Wk+1(t) - η(t)2
∂Wk+1 (t)
∂Wk+1 (t)
=Wk(t + I)WT(t + I)-Wk⑴WT⑴-η⑴2 (∂WWLt))(∂W⅛! .	(35)
Summing up the above equation from 0 to t will give us
WkT+1(t)Wk+1(t) - WkT+1(0)Wk+1(0) - Ak+1(t) = Wk(t)WkT (t) - Wk(0)WkT (0) - Bk(t)
(36)
where symmetric matrices Ak and Bk are defined by
t-1
Ak(t) = X η(τ)2
τ=0
∂L	!T(	∂L
∂Wk (T) /	∂ ∂Wk (T)
and
Bk ⑻=X η(τ )2 (∂W⅛)) (∂W⅛)
Also notice that
tr(Ak(t)) =tr(Bk(t))
(37)
where
T
tr (Ak(t)) ≤
L	t-1
Xtr(Xη(τ)2
k0=1	τ=0
〜
∂L
∂Wk0 (τ)
〜
∂L
∂Wk0 (T)
L t-1
XXη(T)2
k0=1 τ=0
2
〜
∂ L
∂Wk0 (T)
a t-1	2
≤ Xη(τMVL(W(T)))
τ=0
b
≤2
t-1
Xη(τML(W(T))-L(W(T + 1))]
τ=0
~ , ... ~ , ... ~ , ...
2L(W(0)) — 2L(W(t)) ≤ 2L(W(0)),
(38)
where a is because η < 1; b is due to Eq. (29); summing up from T = 0 to t - 1 and noting that
L > 0 give us c. For any two layers k < j, taking trace of both sides of (36) and summing from k to
j , we have that the differences between the Frobenius norms for any two layers
kWk(t)k2F-kWj(t)k2F= tr(Aj(t))+kWj(0)k2F-tr(Bk(t))-kWk(0)k2F
≤ 2L(W(0)) + ∣kWj(0)kF -kWk(0)kF∣
are always bounded. Since max1≤k≤L kWk kF → ∞, the above result implies that kWk kF → ∞
for all k.	□
A.2.1 Proof of Theorem 2
Intuitively, Wk can be decomposed as
Wk(t) = Uk (t)Σk (t)VkT (t).
15
Published as a conference paper at ICLR 2022
For Wk , denote the first singular value, the corresponding left singular vector and right singular
vector by σk , uk and vk . Since σk → ∞, the initialization becomes negligible as t → ∞. According
to Eq. (36), this will lead to
Uk(t)Σk(t)ΣkT (t)UkT (t) → Vk+1 (t)ΣkT+1 (t)Σk+1 (t)VkT+1 (t)
as t → ∞. On the other hand, since WLWLT has rank 1 for WL being a row vector, all layers have
rank 1 because they get aligned with WL and kwW-------→ Uk VT.
To further elaborate the above reasoning, we provide a more exact proof starting with the definition
of the alignment phenomenon.
Definition 2 (Alignment phenomenon for deep linear networks.). For deep linear network f(x; W) =
WL •… Wιx, the alignment phenomenon is defined as
∀k∈ {1,...,k} : |huk,vk+1i| → 1	(39)
and Wk/kWk kF → uk vkT as t → ∞ along the training trajectory, where uk, vk are the left and
right singular vectors correspond to the largest singular value of Wk.
The proof for the alignment phenomenon of adversarially trained deep linear networks in our setting
is as follows 6.
Proof. Note that
we have
huk, vk+1i σk+1 = uk Wk+1Wk+1uk + uk (vk+1σk+1vk+1 - Wk+1Wk+1)uk,
UTWk+1Wk + 1uk	kWk+lk2 -IlWk+1 IlF
~9.	+	~9.
σk2+1	σk2+1
≤ hUk, vk+1i2 ≤ 1
by utilizing the definition of matrix norm and
IWk+1I2F≥UkTWkT+1Wk+1Uk.
According to Eq. (36), let
Γk = WkT+1(0)Wk+1(0) -Wk(0)Wk(0)T + Ak+1 -Bk,
and replace WkT+1Wk+1 with Γk + Wk WkT in Eq. (41), we have
2
2 σk
huk, vk+1i ≥ ~------+
σk+1
UkTΓkUk + IWk+1 I22 - IWk+1I2F
σ2+1
(40)
(41)
(42)
(43)
(44)
To prove the alignment phenomenon, we now bound the RHS of Eq. (44) with 1 - α where α → 0
when t → ∞. We are going to bound 3 terms, namely UTΓkUk, k Wk ∣∣2 一 ∣∣ WkkF and 睚/睚+、.
1.	Bound UkTΓkUk.
UkTΓkUk ≥ -UkTWk(0)Wk (0)T Uk - UkTBkUk
≥ -∣Wk (0)∣22 - tr (Bk )
≥-∣Wk(0)k2 - 2L(W(0)),	(45)
where we used Eq. (38) in the third inequality.
2.	Bound ∣Wk ∣22 - ∣Wk∣2F. According to the definition of singular values, we have
∣Wk ∣22 = σk2 ≥ vkT+1WkWkT vk+1
= vkT+1(WkT+1Wk+1 - Γk)vk+1
≥ σk2+1 - vkT+1∆Wk(0)vk+1 - tr (Bk)	(46)
6Some parts of the proof is inspired by Ji & Telgarsky (2019)
16
Published as a conference paper at ICLR 2022
where ∆Wk(0) = Wk(0)Wk(0)T - WkT+1(0)Wk+1(0) and we used that tr (Bk) ≥ kBkk2
in the second inequality. On the other hand, by taking trace of both sides of Eq. (36), we
have
kWk k2F = kWk+1 k2F - kWk+1(0)k2F + kWk(0)k2F - tr (Ak+1) + tr (Bk) .	(47)
Combined with Eq. (46), we have
kWkk22-kWkk2F ≥ kWk+1k22 - kWk+1k2F
+ (kWk+1(0)k2F - kWk (0)k2F) - vkT+1 ∆W (0)vk+1
+ (tr (Ak+1) - tr (Bk)) - tr (Bk)
a
≥ kWk+2 k2 - kWk+2 kF
+ (kWk+2(0)k2F - kWk (0)k2F) - (vkT+1∆Wk(0)vk+1 + vkT+2∆Wk+1 (0)vk+2)
+ (tr (Ak+2) - tr (Bk)) - (tr (Bk) + tr (Bk+1))
b	L-1	L-1
≥ kWL(0)k2F - kWk(0)k2F - X vkT0+1∆Wk0 (0)vk0+1 - X tr (Bk0)
k0=k	k0=k
+ tr (AL ) - tr (Bk )
c
≥ M - 2LL(W(0)),	(48)
where a follows from Eq. (37); b follows from summing a from k to L - 1 and that
kWL k2 = kWL kF ; c is because Eq. (38) and that
L-1
M=minkWL(0)k2F - kWk(0)k2F - X vkT0+1∆Wk0(0)vk0+1	(49)
k0=k
is finite.
3.	Bound σk㈤+广 Eq. (46) gives Us that
σ2 、1	vT+ι∆Wk(0)vk+ι-tr(Bk)、，	vf+QW®(0)vk+ι + 2L(W(0))
2	≥ 1	2	≥ 1	2
σk2+1	σk2+1	σk2+1
where vf+ι∆Wk(0)vk+i + 2C(W(0)) is finite.
Putting the bounds obtained from the above 1, 2 and 3 back to Eq. (44) will give us
huk, vk+1i2 ≥ 1 -
IlWk(0)k2 + 2(L + 2)L(W(0)) - M + vf+ι∆Wk(0)vk+i
(51)
Considering that σk → ∞ and the numerator is finite, we thus conclude that the RHS of Eq. (51) will
converge to 1. Therefore, we have that
huk,vk+1i2→1	(52)
as t → ∞. Furthermore, since IWk+1 I2F - IWk+1 I22 is finite (see Eq. (48)) and σk → ∞ (i.e.,
k Wkk2∕∣WkkF → 1), We have 口戊打 → UkVT because other singular values are negligible. We
now have the alignment phenomenon.	□
Furthermore, we have
Wn
kWLkF …IIWIkF
TT
→ ViUi …VLuL → V1.
(53)
We now move forward to prove the alignment phenomenon for the first layer. We assume that the
support vectors span the data space Rd and denote the orthogonal complement of span(u) by U⊥.
Let P⊥ denote the projection onto U⊥. Under this assumption, Ji & Telgarsky (2019) showed the
following lemma7
7This lemma is based on Lemma 12 in Soudry et al. (2018).
17
Published as a conference paper at ICLR 2022
Lemma 7 (Lemma 3 in Ji& Telgarsky (2019)). Let S denote the set ofindices ofsupport vectors,
then with probability 1,
K := min max (ξ,gy侪 > 0	(54)
∣ξ∣ = 1,ξ⊥u i∈S
ifthe data is sampled from absolutely continuous distribution.
To begin with, We first introduce the following lemma.
Lemma 8. For the logistic lossfunction ' = ln(1 + e-x), if (W∏,U> ≥ 0 and∣∣P⊥W∏∣∣ ≥ 2n∕eκ =
夕,then
CW，黑)≥ 0∙	(55)
Proof. We first decompose Eq. (55) as two parts:
卜M ∂⅛ ) = A 以［yi SL")- SL * R,	(56)
where
(p⊥w, dWT )=击〈丸 W, WT ∙∙∙ WT WL …W2〉
=VT1J hP⊥W∏,P⊥W∏i ≥0∙
IIW∏k
Since W < 0, we have
(P⊥W,篇 ) ≥1 ∑'iyi (P±W1，df∂WW)) =1Σ DP⊥W∏, xiyMW∏)> ∙	(57)
1	i=1	1	i=1
Let Xjyj ∈ argmaxi∈s (-P⊥W∏, XiyG which means that
h-P⊥W∏,xjyji≥ κ∣∣P⊥W∏∣∣∙
Then we have
n
-E (P⊥W∏,Xiyi%,(W∏))
n i=1 '
1 G e-<W∏,(g+δi)%i
=n∑ 1 + e-hW∏,(g+δi)yii h-P⊥W∏, Xiyii
i=1
1 S e-<W∏,(g + δi)3
n 工 I + e-hW∏,(χi+δi)3 h-P⊥ w∏, P⊥Xiyii
i=1
1 ʌ	e-<W∏,(xi+δi)3
≥ nΣ I + e-hW∏,(χi+δi)3 h-P⊥w∏, P⊥Xiyii
1 e-hw∏,(χj +δj )yj i	1	e-hW∏,(χk+δk)yQ
≥ n 1 + e-hW∏,(叼 +δjIyj)(-丸丽，丸”仍i + n 工 1十 e-hW∏,(xk+δk)yki h-P⊥w∏, P⊥xyki,
k∈Ck
(58)
where Ck = {k : (-P⊥W∏, P⊥Xk yk i ≤ 0}. Considering that Xj yj is a support vector, the first term
of (58) can be bounded as follows:
1 e-hw∏,(xj+δj )yj i
n I + e-hW∏,(χj+δj)yji h-p⊥w∏, P±Xiyii
≥ K∣∣P⊥W∏He-hw∏,γβieekW∏k
n
e-(w∏ ,χj yj-Yui
1 + e-hW∏,(χj +δj )yj i
K kP⊥W∏∣e-"∏cuieekWnk
e-hP±W∏,P±Xj yj i
1 + eekw∏ke-‹P⊥w∏,P⊥χj yj ie-hw∏,7ui
≥ 在kP⊥W∏∣e-hw∏,γui
2n
eek w∏ k
(59)
18
Published as a conference paper at ICLR 2022
where we use that EkWnk, hW∏,γU), hP⊥W∏, P⊥Xj y7- i > 0. Each term of the second part of (58)
can be bounded by
e-hWΠ,(xk +δk)yk i
1 + e-hw∏,(χk+δk)yki h-P⊥Wn, P⊥xkyki
≥ ekWΠke-hWΠ,xkyki h-P⊥WΠ,P⊥xkyki
=eekwπke-hwπ,γuie-hWπ,χkyk-γui h-P⊥W∏, p⊥χky®)
≥ eekwπke-hwπ,γuie-h-p⊥wπ,p⊥xkyki h-P⊥Wn, P⊥xkyk)
≥ eekw∏ke-hw∏,Yui
where the second inequality follows from
hW∏,χkyk - Yui = hW∏,P⊥χkyki + hW∏,χkyk - P⊥χkyk - Yui
≥ hWΠ, P⊥xkyki = hP⊥WΠ, P⊥xkyki
while the last inequality comes from f(x) = -xe-x ≥ -1/e for x ≥ 0. Finally, we have
DP⊥W∏, (xi + δi)yi',(W∏)E ≥ e-hWncuie'kWnk (KkP⊥Wnk - j)
Therefore, <P⊥W∏, (g + 6力族(Wn)E ≥ 0 if
kP⊥Wnk ≥ 2n = P
(60)
(61)
(62)
□
We are now ready to prove the alignment phenomenon for the first layer. Let P⊥W1 denote the
projection of rows of Wi onto u⊥. We start from exploring the asymptotic behavior of kpWW1JF
when t → ∞. The update ofP⊥W1(t + 1) can be written explicitly as
kP⊥W1(t+ 1)k2F
= kP⊥Wi(t)k2F	- 2η(t)	P⊥Wi(t),P⊥	d L + + η(t)2 ∂Wi(t) / + η	~ ∂L P i	 ⊥∂Wι(t)		2 F	
≤ kP⊥Wi(t)k2F	- 2η(t)	P⊥Wi(t),P⊥	d L + + η(t)2 ∂Wι(t)∕+ η	〜 ∂L ∂Wι(t)	2 F		
≤ kP⊥Wi(t)k2F	-2η(t)*P⊥WI⑴，∂W⅛ + + 2(L(W(t + 1)) -			L(W (t)))		(63)
where the last inequality follows from Eq. (30). We define a large enough step t0 as follows: for any
t ≥ t0, we have
2
kWi(t + i)kF = kWi(t)kF - 2η(t) (wι(t), 1WW=-^ + η(t)2
~
∂L
∂Wι(t)
≥ kWi(t)k2F ,
F
(64)
where ”large enough t0” means that
C	≈ ,
P + L(tO)
kW1 (t0)kF
→0
(65)
as kWi kF → ∞. Suppose that there exists a ti ≥ t0 such that
kP⊥ Wn (ti - 1)kF < P and kP⊥ Wn (ti )kF ≥ P,	(66)
19
Published as a conference paper at ICLR 2022
which is to say (recalling Lemma 8)
rWι ⑴,∂W⅛) ≥ 0
=⇒ kP⊥Wι(t + 1)kF ≤ kP⊥Wι(t)kF + 2 (L(W(t)) - L(W(t + 1))) ,	(67)
for to ≤ t1 ≤ t ≤ t2 and t? = ∞ if We never have ∣∣P⊥W∏(t)∣∣F < 夕 after tι. If there does not
exist such a t1, then we directly conclude that kpWW^JF → 0 since IlWIIIF → ∞. On the other
hand, for any t0 ≤ t1 ≤ t ≤ t2
kP⊥w1(t)∣F	kP⊥Wι(tι)kF + 2 (L(W(tι)) - L(W(t)))
kWι(t)kF ≤	kW1(t1 )kF
≤ kP⊥W1(t1)kF + 2L(W(tι))
-	kWι(tι)kF
≤ 夕2 + 2μ(tι)φ + μ2(tι) + 2L(W(ti))
∣	kW1(t1)kF
=Φ(y,tι) —kWι (tι)kF where the last inequality follows from Eq. (31) and Φ(夕,t1) is defined by φ(2,tι) =22 + 2μ(tι)夕 + μ2(ti) + 2L(W(tι)) ≥ φ(2,t0) for any t0 ≥ t1 because the loss and μ never increase. Therefore we can conclude that kP⊥W1(t)kF ≤ ΦW,t)	≤ Φ(3,t1)	→ 0 kW1(t)kF - kW1(t)kF - kW1(t1)kF for any t ≥ t1 . As a result, | hvi(t), ui | → L	(68) (69) (70) (71)
B Proofs of Section 3.2
We present some useful properties and proofs of Lemma 3 and Lemma 4 in Section B.1. Section
B.2 focuses on the divergences of weight norms. Section B.3 is about the convergence to KKT
points(Theorem 5).
Additional notations We useWij;k to represent the i-th row j-th column entry of the k-th layer
weight martrix.
B.1	Useful properties and proofs of Lemma 3 and Lemma 4
For any m × n matrix A, we denote v(A), an mn-dimensional vector, as the vectorized version of it:
/ Aιι ∖
A21
V(A)=	a'	,	(72)
m1
...
Amn
then the trace operator can be represented by
tr (ATB) = V(A)TV(B) = V(B)Tv(A).	(73)
We first introduce the Euler’s theorem on homogeneous functions.
20
Published as a conference paper at ICLR 2022
Lemma 9 (Euler’s theorem on homogeneous functions). If f(W1, . . . , WL) is a positive multi-ck-
homogeneous function
L
f(ρ1W1,...,ρLWL) = Yρckkf(W1,...,WL)	(74)
k=1
for positive constants ρk ’s and ck ≥ 1, then we have:
tr (df(WdWJWL, Wk) = Ckf(Wi,…，Wl).	(75)
Proof. Taking derivatives of ρk on both sides of Eq. (74) for any given k ∈ {1, . . . , L}, we have
L
Y ρckk00 f(x; W)ckρckk-1.
k0 6=k
Since Pk is arbitrary, we let PL = •…=Pk = •…=ρι = 1, then the above equation becomes
tr (Wkf(亚脑"")=Ckf(Wi, ...,Wl)
for any k ∈ {1, . . . , L}.
(76)
(77)
□
Furthermore, we have
〈W ,W)= X tr ( df(WW2 Wk)
= Kf(x,W)	(78)
where K = PkL=1 Ck. For the deep neural networks defined in Eq. (2) with homogeneous property,
we can then apply the above lemma to them.
We examine whether Assumption 2 can still be applied to adversarial margin under Assumption 1.
Lemma 10 (Assumption 2 for adversarial training). For any fixed x,
•	if yf(x; W) is locally Lipschitz, then so does yf(x + δ(W); W) with perturbation δ(W).
Proof. For fixed x, suppose yf(W) is locally Lipschitz on Y , then for each W ⊂ Y there
is a ZW containing W such that yf(W) is Lipschitz on ZW:
kyf(W)-yf(V)k ≤LWkW-VkforW,V ⊂ ZW.	(79)
For the adversarial perturbation, by definition δ(W) and δ(V ) are solutions of the inner
maximization for the adversarial training, in other words,
yf (X + δ(W); W) ≤ yf(x + δ(V); W)	(80)
yf (X + δ(V); V) ≤ yf (x + δ(W); V)	(81)
because then we will have '(χ + δ(W); W) > '(x + δ(V); W) since ' is non-increasing
under Assumption 1. As a result,
yf (x + δ(W); W) - yf (X + δ(V); V) ≤ yf (x + δ(V); W) - yf (x + δ(V); V)
≤LWkW-Vk	(82)
for yf(X + δ(W); W) > yf(X + δ(V); V). The result for yf(X + δ(W); W) < yf(X +
δ(V); V, is similar.	□
•	if∀i : yif(Xi; W) have locally Lipschitz gradients, then so do yif(Xi + δi(W); W).
21
Published as a conference paper at ICLR 2022
Proof. For fixed x, suppose yVf (W) is locally LiPschitz on Y, then for each W ⊂ Y there
is a ZW containing W such that yVf (W), δ(W) are Lipschitz on ZW, for W, V ⊂ ZW:
kyVf(x; W) - yVf(x; V)k ≤LWkW-Vk	(83)
kyVf(x + δ(W); W) - yVf(x + δ(V); W)k ≤LWxkδ(W)-δ(V)k
≤ LWxLWδ kW - Vk (84)
Then
IIyiVf (Xi + δi(W); W) - yVf(xi + δi(V); V)k
≤ kyiVf (Xi + δi(W); W) - yVf(xi + δi(W); V)k
+ kyiVf (Xi + δi(W); V) - yiVf (Xi + δi(V); V)k
≤ (LW + LWxLWδ) IW - VI.	(85)
□
B.1.1	Proof of Lemma 3
Proof. The proof can be done for multi-ck-homogeneous networks as defined in Lemma 9. We first
note that
L
Vxf (x; ρι Wι,..., PlWl) = Y PkkVxf (x; Wι,..., WL)	(86)
k=1
by taking derivatives with respect to X on both sides of Eq. (74). Therefore Vxf(X; W1, . . . , WL) is
also positive homogeneous. Under Assumption 1, we prove Lemma 3 in the following cases:
1.	'2-FGM perturbation. This perturbations is taken as
δFGM(W)
.
=ey'0Vχf (x; Wi ,...,Wl) = - eyVχf(x; Wi,...,Wl)
∣∣y^0Vχf (x; Wi,..., WL) JI	kVx f (x; Wi,..., WL)k
because M < 0. Invoking Eq. (86), We know that
δFGM(PiWi, . . . , PLWL)
ey QL=I Pkk Vxf (x; Wi,..., WL)
Qk=iPkkkVxf(x; Wi,...,WL)k
EyVxf (x; Wi,..., Wl)
kVxf (x; Wi,...,Wl)∣
δFGM(Wi, . . . , WL).
(87)
(88)
2.	FGSM perturbation.
6fgsm(W) = ESgn (y20Vxf (x; Wi,..., WL)) ,	(89)
where sgn() denotes the sign function. Then we have
δFGSM (piWi,...,plWl) = ESgn (yM Y PkkVxf (x; Wi,..., Wl)
k=i
since Pk > 0 for any k thus will not affect the sign operation.
δFGSM (W)
(90)
3.	'2 and '∞-PGD perturbation. We only prove the case for '2-PGD and the case for '∞-PGD
is similar.
jD(W) = PB(0,e) ]δPGD(W) — ξ IyVfXTWi,..∙,WL))J ,	(91)
k xf(x;	i, . . . , L)k
22
Published as a conference paper at ICLR 2022
where PB(0,) denotes the projection operator, B(0, ) is the perturbation set, j is the step
indices of PGD and ξ is the step size. We prove by induction. For j = 0, we have
m / W	W ∖ Q I" CyVxf (x； WI ,...,WL) "
δPGD(PI W1,...,PLWL) = PB(OQ [-ξ kvxf (x； Wi,...,Wl)∣∣]
=δP1GD(W1,...,WL).
(92)
Ifwe have δPjGD(P1W1, . . . ,PLWL) = δPjGD(W1, . . . , WL), then forj + 1, we have
δPjG+D1 (P1 W1 , . . . , PLWL ) = PB(0,)
δPjGD (PW) - ξ
yQL=1PkVxf(x;Wi1...2WL}
QL=I Pk kvxf (x; Wι,...,WL)k
δPjGD(W) - ξ
PB(0,)
yVx f (x; Wi,...,Wl)
kVxf (x; Wi,...,Wl)∣∣
δPjG+D1(W1,..., WL).
(93)
Therefore, these four adversarial perturbations are all scale invariant adversarial perturbations by our
definition.	□
B.1.2 Proof of Lemma 4
Proof. This can be easily proved by noting that, for any multi-ck-homogeneous functions,
L
f(xi+δi(P1W1,...,PLWL);P1W1,...,PLWL)= YPckkf(xi+δi(W1,...,WL);W1,...,WL)
k=1
(94)
because δ(W) is a scale invariant adversarial perturbations. Then f(x + δ(W); W) is still a homoge-
neous function and Lemma 9 can be applied to f(x + δ(W); W1, . . . , WL) and we have
tr (盖 Wk
1n
1X 'yt
n
i=1
n
∂f(xi + δi(W); Wi,...,Wl )
∂Wk
-Ck X e-yif (Xi + δi(W); Wι,..., WL)
n i=1
n
-n X UYi.
i=1
(95)
Taking ci = … =CL gives Us Lemma 4.
□
B.2 Divergences of Frobenius norms of weight matrices
B.2.1	Divergences of all layers
Lemma 11. The Frobenius norms of all layers grow at approximately the same rate along the
trajectory of gradient flow for adversarial training with scale invariant adversarial perturbations for
multi-Ck -homogeneous DNNs
ɪ dkWLkF = ɪ dkWL-ikF =…=ɪ dkWikF
CL dt CL-1 dt	C1 dt
(96)
Remark. Note that a similar conclUsion as that of Lemma 11 exists for gradient flow of the standard
training for mUlti-1-homogeneoUs networks DU et al. (2018). We generalize this property to the
adversarial training of multi-Ck-homogeneous neural networks. When ci =…=cl, we have that
all layer grow at the same rate.
23
Published as a conference paper at ICLR 2022
Proof. For any Wk,
dkWk IlF _ X d (Wij;kWji;k) _ 9X d dL ʌ	WT
F A —dt - = -2 ⅛ S	Wji；k
where We use dW
(97)
—
and Lemma 4. One can then immediately notice that the above
equation does not depend on any specific k, thus we have
ɪ d∣WL∣F = ɪ d∣WL-ikF = ∙∙∙ = ɪ d∣WιkF
cL dt cL-1 dt	c1 dt
(98)
B.2.2 Proof of Theorem 3
We prove this theorem by exploring the time evolution of ρ.
Proof. We start from a multi-ck-homogeneous function then take ci = •… = ”. Recalling
■—∙.	■—■	... ..
f(x; W) = ρf(x; C) where Wk = Wk/ ∣∣WkkF and
LL
ρ=Yρckk=YkWkkcFk,
k=1	k=1
(99)
the adversarially trained predictor with scale invariant adversarial perturbation is still homogeneous
(see Eq. (94)):
f(x + δ(W); W) = Pf (x + δ(c); W).	(100)
For ρk = kWkkF, we have
dρk
dt
n
& X e-γi Yi
nρk i=1
(101)
by invoking Eq. (97). The dynamical evolution of ρ will then be
dρ
dt
X P11 …埠…PLL= X c⅛2 1X X e-Yi Yi
k⅛	dt L	k=1 Pk In -
(102)
Let t0 denote the time such that all worst case adversarial examples are correctly classified. We study
the trends for P after t0 . On one hand, the empirical adversarial loss does not increase:
dL _	∂ L dW _ dW
dt ∂W dt dt
≥ 0.	(103)
2
On the other hand, recalling our definition of normalized adversarial margin, let
m = arg min Yi = arg min PYi
i∈{1,...,n}	i∈{1,...,n}
and note the definition of t0 , we have
(104)
e-7m ≤ 1 X e-γi = L =⇒ 7m ≥ ln
n
i
≥ ln I - I > 0
—"to"
(105)
because L does not increase and L(t) ≤ L(to) < 1. This also implies that γm, thus Yi for all
i ∈ {1, . . . , n}, can not be arbitrarily close to 0. Otherwise one would conclude that L(t) may
be arbitrarily close to 1 which is a contradiction. Therefore we can immediately conclude that
dP/dt > 0 in Eq. (102), which implies that P may diverge as t → ∞. To have a clearer view
□
24
Published as a conference paper at ICLR 2022
about the convergence property of ρ, we study the following relation regarding the time evolution of
eρyi f (xi +δi (W)) after t0 :
dt
Eeg Yi
i
'----
(a)
The term (a) can be computed by invoking Eq. (102)
+ fIeePY Pyi
i
、
------------ ---
df (Xi + δi(C); C)
{z
(b)
dt
(106)
Σ
i
}
}
(a) = X⅛2Xeργi^i (XiYj)
k=1 nPk i	j
L 22 n 1n
=X ⅛Pτ X Y2 + 2 X ^iYj SYTj) + e-ρdj)
k=1 Pk	i=1	i6=j
≥ XI
j ≥ X PX
k=1 nP2k
(107)
where the first inequality follows from X + 1/x ≥ 2 for X ≥ 0 and We denote the minimum of Ei Yi
for t ≥ t0 as ζ > 0. The existence of such ζ has been discussed below Eq. (105).
The term (b) needs more analysis. For any fixed adversarial example Xi + δ(W), we have
yi
-------------- ----
df(xi + δi(Wc); Wc)
dt
L
yi	tr
k=1
_ ------------- -----
∂f(xi + δi(Wc); Wc)
(108)
，	6 / τ, 1	..
where dWk /dt can be computed as follows
dWjl;k = dWjl;k
dt ∂W dWmn；k
m,n	;
dWmn;k
dt
X	d
∂W dWmn；k
m,n	mn;
ɪ	dWji；k
Pk ∖ dt
Wjl；k
Iqtr(Wk WT)
n
-W^ X y,e-
nPk i=1
dWmn;k
dt
-Yi Yi	.
(109)
On the other hand, we note that
~
∂ L
dWk
dt
- & ∂f(xi + δi(W); W)
∂f(Xi + δi(W); W) T
∂Wk
ɪ X yj e-γj
nρk j
T
∂f(Xj + δj (WC); WC)
(110)
c
because fW ) = F fM ). Combing Eq. (109) andEq.(110), we have
∂Wk	ρk ∂Wk
—
dWk
dt
T
∂f (Xj+ δj(c); C) ʌ
-- ----------- ---
-Ckf (Xj + δj(C); C).
(111)
Therefore
v
y e-γ Ak V
_ — —
∂f(xj+ δj(W); C)
(112)
25
Published as a conference paper at ICLR 2022
where
Ak= I - ”WnT = I-V (CT K (CT )T
(113)
can be seen as a projector operator. Putting Eq. (112) into Eq. (108) will give us the expression
T
AT v "
yi
-- ----
df(xi + δi(Wc); Wc)
_ ----
∂γ (C)
dt
_ ---
∂Wck
(114)
Now the term (b) in Eq. (106) is, by putting Eq. (114) back into it,
(b)
Rearranging eρ6i-%.) + e-ρ(%-γj) as follows
eP(γ伍+γj) + e-ρ(7v+7j) - (ePγj - e-Pγj )(ePγv -已
(115)
eP(γv-%∙) + e-ρ(Aa-AZj)
will allow us to rewrite (c) as
PYi )
(c)
JTiAkv Ak V
i
2
2
-E (eρ% -
i
e-ρ%) AkV
2+IX L
2
2
+ ∣X e-p%Ak
JTiAkv Ak V
i
-£eP%Akv
i
Ce-ρ% Ak v
i
2
2

≥ 0.
Combing Eq. (107) and Eq. (116) with Eq. (106), we will have
(116)
1X
i
dt
≥ X p2(t)z2
—k=1 n2ρk(t).
(117)
According to Eq. (101), we know that ∀k : dρk/dt > 0 after to, therefore ∀k : ρ2∕ρk is lower
bounded by its value at t0 for t ∈ [t0 , ∞) because
d
dt
d (Pkck-2 YY PkckO
k06=k
LL	L
2(ck-1)Pkck-% ∏ Pkck0 + Pkck-2 X a，®，Pkck'-1Pko (	∏ Pkck00) > 0,
k0 6=k	k0 6=k	k00 6=k0,k
(118)
where Pk denotes dPk∕dt > 0 for any k due to Eq. (101). Integrating both sides ofEq. (117) from
t0 to t gives us
1	(eP(t)yvf(xv + δv(c)ic(t)) - eP(to)yvf(χv+δv(c)W(to))
i
Lt
≥X
k=1 t0
n⅛%(t- to),
L
≥X
k=1
(119)
26
Published as a conference paper at ICLR 2022
where the last inequality follows from Eq. (118). Let t → ∞ in the above equation, we will have
P = Ω (ln t) → ∞
(120)
since yif (xi + δi (W ); W) is upper bounded, which can be easily deduced considering that all
weights have norm 1, and Z is lower bounded as discussed before. Taking ci =…=CL gives Us the
conclusion.	□
Note that P = ρC1 •…PLL and Pk for any k grows at the same rate for multi-c-homogeneous functions
(Lemma 11), we immediately have
Corollary 1. ∀k ∈ {1, . . . , L} : Pk → ∞ as t → ∞.
B.3 Convergence to KKT point
B.3.1 Proof of Theorem 4
Proof. Most calculations needed for the proof of this theorem have been done in Section B.2.2.
According to Eq. (114), we have
Σ
i
— —
-Yi	df (X + δi(C); W)
e y---------石-------
L1
X P2 X LY
k=1 k i
H
∂ ∂γ (C) !TAτ J ∂^i(c) !ʌ
V ∂Wk ) k \ ∂ck ))
2
L
k=1
≥ 0.
2
(121)
□
B.3.2 Proof of Theorem 5
The KKT conditions for the optimization problem Eq. (19) are
∀k :
1 巡-X λ* = 0
2 ∂Wk	与 i ∂Wk
i=1
∀i ∈{1,...,n} : λi(Yi — 1) = 0.
(122)
(123)
Following Dutta et al. (2013); Lyu & Li (2020), we define the approximate KKT point in our settings
Definition 3 (Adapted from Definition C.4 in Lyu & Li (2020)). The approximate KKT points of the
optimization problem (19) are those feasible points which satisfy the following condtions:
i	Il 1 ∂∣∣wk2
1∙	II 2 ∂W
≤ χ;
2
2. ∀i ∈{1,...,n}: λi(5i — 1) ≤ ξ,
where χ, ξ > 0 and λi ≥ 0. These points are said to be (χ, ξ)-approximate KKT points.
We now present the proof of Theorem 5.
Proof. Let
kWk
V
(124)
(125)
where K = PkL=1 c = cL. Since
cf (x; W) = cYmf(x; V)
(126)
27
Published as a conference paper at ICLR 2022
according to the definition of the multi-c-homogeneous functions and
Cf (x; W) = (Wk, d(KW) ) , cf (x; V) = M, fV ),
∂Wk	∂Vk
(127)
we have
∂f (x; V) _	1	∂f(x; W)
∂V = YK-。/K ∂W
(128)
Specifically, in the following, we will show that V is a (χ, ξ)-approximate KKT points of the
optimization problem (19) with χ, ξ → 0 along the training trajectory. Because the homogeneous
property of the network trained with scale invariant adversarial perturbations, it satisfies the MFCQ
condition by simply noting that
∀i ∈ {1, . . . , n} : tr
cγYi > 0.
(129)
This makes a point being a KKT point a necessary condition for it to be the optimal solution.
We now show that the limit point of V along the adversarial training trajectory is an (χ, ξ)-approximate
KKT point with χ, ξ → 0 starting with the first condition. Let
λi = ( ∂WY ) kWkYK-2"Ke-γi,
(130)
then we have
X	∂f (Xi + δi(V); V)
V - Xλiyi--------∂V--------
i
WL - X
~1/K	乙
γm	i
λiyi	∂f(xi + δi(W); W)
∂W
2(Pk P)
(Qk PkcWK Y/
I	DW, ∂W E
：kWk∣∣和『
2
a
2
2
2
≤
2Lρk0
ρ200YK
I	DW,磊 E
：kWk∣∣ 驯|
2 I = χ(t)
(131)
where a follows from (128); k0 = arg maxk ρk and k00 = arg mink ρk. This is the first condition
for the limit point of V being an approximate KKT point. We can further show that χ → 0 as t → 0
by noting the following alignment phenomenon which was originally observed by Ji & Telgarsky
(2020) and intended for standard training with fixed training examples:
Lemma 12 (Adapted from Theorem 4.1 in Ji & Telgarsky (2020)). Under Assumption 2 and
Assumption 4 for exponential loss and homogeneous deep neural networks, along the trajectory of
adversarial training with scale invariant adversarial perturbations, we have
∀k ∈{1,...,k} :lim (W,磊(t) =-1.
t→∞ kWk∣∣∂W(川
(132)
The extension of Theorem 4.1 in Ji & Telgarsky (2020), which was intended for fixed training
examples, to our settings is because, by our construction for adversarial training with scale invariant
adversarial perturbations, the adversarial training margin are locally Lipschitz with locally Lipschitz
gradients and the prediction function f(x + δ(W); W) is positively (multi)homogeneous with respect
to W (see Lemma 10 and Eq. (4)). Invoking Lemma 12 and considering that YmL can not be arbitrarily
close to 08 and ∀k : Pk → ∞ at the same rate according to Lemma 11 thus 器 can not be infinite,
we have,
lim χ(t) → 0.
t→∞
8See Eq. (105). In fact, it keeps increasing after some time according to Theorem 4.
(133)
28
Published as a conference paper at ICLR 2022
On the other hand, the second condition for the limit point of V being an approximate KKT point is
X λi(U - 1)= X YWk (I 作 If! Li际-常
≤ Pk Pk X e-5i6⅛ - Xfm)
一ρ/AKKj Dw,需 E
≤ Pk P X _________ne-5^i (Y - fm)_
一P2K 微 K 乙 KPemm Pj y f (xj + δj (W); W)
2n
≤	n Ek Pk	X e-(7i-7m) Y - Im
—Kρ1+2∕K襦K ±	Am
≤	n Pk Pk
—Keρ1+2∕κ Y#2/K
≤	nLP20	= ξ
_ KePPk00Y+2K	'
(134)
where a follows from (w,需〉≤ ∣∣ W∣∣ ∣ ∂L ∣ ； b is due to Lemma 4 and ∀i : Yi ≥ γm; C uses
∀i : γYi ≥ γYm again; d is due to e-xx is upper bounded by its value at x = 1 for x ≥ 0. Since
2
∀k : limt→∞ Pk (t) → ∞ at the same rate, We conclude that limt→∞ 牛 can not be infinite thus
limt→∞ ξ(t) → 0 because Ym can not be arbitrarily close to 0 and limt→∞ P → ∞. Therefore, the
limit point of V is an (χ, ξ)-approximate KKT point of the mini-norm problem along the trajectory
of adversarial training with scale invariant adversarial perturbations where limt→∞ χ(t), ξ(t) → 0.
Restating the theorem in Dutta et al. (2013) regarding the relation between (χ, ξ)-approximate KKT
point and KKT point in our setting:
Theorem 6 (Theorem 3.6 in Dutta et al. (2013) and Theorem C.4 in Lyu & Li (2020)). Let {xt : t ∈
N} be a sequence of feasible point of the optimization problem 19. xt is an (χt, ξt)-approximate
KKT poiint for all t with two sequences {χt > 0 : t ∈ N} and {ξt > 0 : t ∈ N} and χt, ξt → 0. If
xt → x as t → ∞ and MFCQ holds at x, then x is a KKT point of this optimization problem.
We then immediately conclude that the limit point of {W(t)∕∣∣W∣∣ : t ≥ 0} of gradient flow for
the adversarial training objective Eq. (6) is along the direction of a KKT point of the optimization
problem (19).	□
B.4	Generalization to non-smooth case
It is not hard to generalize the current analysis to non-smooth case, which will include the deep ReLU
network, because, in our main steps, there are counterparts of our conclusions for non-smooth case
for that sub-differential (Clarke, 1975) is a generalization of gradient. The non-smooth analysis can
be done by first replacing the gradient flow equation Eq. (16) with its generalization,
dW ∈ -∂L(W(t))
(135)
where ∂ L(W (t)) is the sub-differential. Then one can follow similar procedures as our approach to
make the generalization.
Proof. For simplicity, we only consider multi-1-homogeneous networks here, i.e., ci = •…=CL = 1,
which include the deep ReLU neural networks. Lemma 9 can be generalized to non-smooth case
according to Lemma C.1 in Ji & Telgarsky (2020) or Theorem B.2 in Lyu & Li (2020):
Lemma 13 (Lemma C.1 in Ji & Telgarsky (2020)). Suppose f : Rn → R is locally Lipschitz and
L -positively homogeneous, then for any W ∈ Rn and any W * ∈ ∂f (W),
hW,W *〉= Lf(W).
(136)
29
Published as a conference paper at ICLR 2022
The starting point of the proof for Lemma 3 is the homogeneity of f(x; W) and can be easily
promoted to handle the non-smooth case.
With generalizations of Lemma 3 and Euler’s theorem on homogeneous functions for non-smooth
case, the proof of Lemma 4 can also be generalized to non-smooth case accordingly because it
is based on the Euler’s theorem on homogeneous functions and Lemma 3 for adversarial training.
Specifically, it will become
(dWk ,W) = n X e-γi γi.	(137)
i=1
The proof of Theorem 3 adopts Lemma 4 and can be generalized to non-smooth case by combining
the chain rule (Clarke, 1983) for gradient flow to ensure Eq. (103)
—
〜
dL
dt
dW
-dL(WMF
dW
dt
≥0
2
(138)
with similar approach as in Lemma B.9 in Ji & Telgarsky (2020) or Lemma 5.2 in Davis et al. (2018).
Then generalizing the rest of the proof of Theorem 3 is straightforward.
Finally, the generalization of Theorem 5 to non-smooth case can also be done because it adopts
Theorem 3, Lemma 4, the chain rule, and the gradient alignments Lemma 12 which holds for
non-smooth case.	□
B.5	DEEP LINEAR NETWORKS ADVERSARIALLY TRAINED WITH '∞ PERTURBATIONS
Following the settings of Section 3.1, the '∞-perturbation for deep linear networks f (x; W)=
WL …Wix can be given exactly as
δ(W (t)) = -yisgn(WΠ(t)).	(139)
According to Theorem 5, the adversarial training solution is formulated from solving the following
optimization problem:
max2 l∣W 112	(140)
s.t. min yiWΠxi - kWΠk1 ≥ 1.	(141)
i∈{1,...,n}
It can be seen that this optimization problem, a mixed-norm optimization problem, will have different
solutions when compared to the margin-maximization problem of the original data, which will lead
to a different decision boundary distinguishing adversarial training methods from the standard ones.
Besides, the constraint on (Eq. (8) for `2 perturbation) should also be changed accordingly such
that the adversarial training examples are still linear separable:
≤γm,1	(142)
where γm,ι is the max-'i-norm margin:
γm,1 = max min	yiuxi.	(143)
kuk1=i i∈{i,...,n}
C S upplemented Experiments
C.1 Adversarial training with different loss functions, architectures and
VARYING PERTURBATION SIZES
We present the results for adversarial training with different loss functions, architectures and varying
perturbation sizes in this section to further verify our theorems and assumptions. We use two different
models: one is a 3-layer neural network with the same architecture as that in the Section 4; the other
is a CNN with architecture Input-Conv-ReLU-Pooling-Conv-ReLU-Pooling-FC-FC,
where Conv stands for convolution layer, FC stands for fully connected layer and Pooling is
max-pooling layer.
30
Published as a conference paper at ICLR 2022
U-PBE PaZ--BEJOU -Bk!BS,ltυ>PB
(a) Adversarial normalized margins for varying perturbation sizes
08642086
W9999988
>u2⊃uuro 6lπu.l.lBs.l>PB
O	IOO 200	300	400	500
epochs
(b) Adversarial training accuracy for varying perturbation sizes
Figure 2: Adversarial training for the 3-layer neural network on MNIST to perform binary classi-
fication. The perturbations used for training are FGSM and '∞-PGD perturbations with varying
perturbation sizes = 8/255, 12/255 and 24/255. The adversarial normalized margins and adversar-
ial training accuracies are evaluated with the corresponding perturbations used for training.
For the 3-layer network, We conduct adversarial training using both FGSM and '∞-PGD perturbations
with perturbation sizes = 8/255, 12/255 and 24/255 to perform binary classification where we
choose all examples with labels ”3” and ”8” from MNIST to be our dataset. The loss function is
logistic loss. Fig. 2(a) shows that for both FGSM and '∞-PGD perturbations with varying perturbation
sizes, the adversarial normalized margins during training keep increasing after some step. Fig. 2(b)
reveals that the adversarial training accuracy can achieve 100% (after 〜200 epochs), which supports
Assumption 4.
For the multi-classification task, let f (x; W)[j] denote the j-th output. Then the margin for an
adversarial example xi + δi corresponding to the original example xi is defined by
Yi = f(xi + δi; W)[yi] - max f(xi + δi; W)[j],
j6=yi
31
Published as a conference paper at ICLR 2022
U-PBE PaZ--BEJOU -Bk!BS,ltυ>PB
0.2-
0.0-
-0.2-
-0.4-
-0.6-
-0.8-
——FGSM, ε= 16/255
FGSM, ε = 32/255
——PGD, ε = 16/255
—— PGD, ε = 32/255
0	100	200	300	400	500
epochs
(a)	Adversarial normalized margins for varying perturbation sizes
>u2⊃uuro 6lπu-B.Q--.lBs.ltυ>PB
O
10
6 4 2
9 9 9
90
0	100	200	300	400	500
epochs
(b)	Adversarial normalized margins for varying perturbation sizes
Figure 3:	Adversarial training for the CNN on all training examples of MNIST to perform multi-
classification. The perturbations used for training are FGSM and '∞-PGD perturbations with varying
perturbation sizes = 16/255 and 32/255. The adversarial normalized margins and adversarial
training accuracies are evaluated with the corresponding perturbations used for training.
32
Published as a conference paper at ICLR 2022
and the margin for the adversarial data is defined to be
Ym =	min、Yi.
i∈{1,...,n}
Then for the CNN, We conduct adversarial training using both FGSM and '∞-PGD perturbations with
perturbation sizes = 16/255 and 32/255 to perform multi-classification on all training examples of
MNIST to verify our claims and assumptions when the perturbation sizes are large. The loss function
is cross-entropy loss. It can be seen from Fig. 3(a) that the adversarial normalized margins keep
increasing after about 250 epochs, i.e., after the model fits all adversarial training examples, for all
adversarial perturbations. Fig. 3(b) clearly shows that the adversarial training accuracy can reach
100% for all perturbation types and sizes, even the large size = 32/255.
These experiments show that adversarial normalized margins are different whenever perturbation
types or sizes are different. All models adversarially trained with all perturbations and loss functions
can achieve 100% adversarial training accuracy, which verifies Assumption 4. Moreover, the trends
that adversarial normalized margins all keep increasing are clear, even long after the separation of
adversarial training examples. These results well support the claims of Theorem 5.
C.2 EXPERIMENTS ABOUT `2 -FGM PERTURBATIONS FOR BINARY CLASSIFICATION
Ooooo
0 9 8 7 6
1
AUeJnUUe 6wwgμe3>pe
U-BJeUJ pez=euuou
----adversarial training
standard training
-3.0 .
0	100	200	300	400
epochs
50 .
0	100	200	300	400
epochs
(a) '2 -FGM adversarial examples	(b) Adversarial training accuracy
Figure 4:	Adversarial training for the 3-layer neural network on MNIST. The attack method is
'2-FGM with e = 8. (a) Adversarial normalized margin for '2-FGM adversarial examples during
adversarial training and standard training. (b) Adversarial training accuracy for '2-FGM adversarial
training.
In the MNIST dataset, we adversarially trained a 3-layer neural network with the same architecture as
that in Section 4 using SGD with constant learning rate and batch-size 64. To have a clear view about
the difference between the implicit bias for standard training and adversarial training, the adversarial
perturbation used for training is '2-FGM perturbation with e = 8 since even the standard model can
fit '2-FGM adversarial examples easily in the MNIST dataset when the perturbation size e is not
large. As a comparison, we also standardly trained a model with the same architecture to evaluate the
normalized margin for adversarial examples by attacking it with '2-FGM during its training process.
As showed in Fig.4(b), the adversarilly training accuracy reaches 100% very quickly, which supports
Assumption 4. It can be seen in Fig.4(a) that the adversarial normalized margin keeps increasing dur-
ing the adversarial training process while the standardly trained model maintains a lower adversarial
normalized margin.
C.2.1 Experiments about exponential loss
For completeness, we also conduct experiment about the binary classification where the model is
adversarially trained with exponential loss (Fig.5) for '2-FGM perturbations. We select the examples
with label ”2” and ”8” from MNIST and adversarially trained a 2-layer neural network using SGD
with constant learning rate 10-5. The hidden layer is of size 10000 and the activation function is
ReLU. The normalized margin for adversarial data is defined exactly as in Section 3.2. The adversarial
33
Published as a conference paper at ICLR 2022
adversarial training
0.0005
0.0000
-0.0005
-0.0010
-0.0015
-0.0020
-0.0025
-0∙0030
O 2000	4000	6000	8000 IOOOO
epochs
Figure 5: Adversarial normalized margin during adversarial training for binary classification. The
loss function during training is taken as exponential loss.
(a) 4-layer deep linear network
(b) 3-layer deep linear network
(c) 2-layer deep linear network
Figure 6: Frobenius norms for weights of deep linear networks with different layers during adversarial
training. The perturbations are `2 perturbations.
perturbations during the adversarial training are given by '2-FGM with e = 3. As showed in Fig.5,
although extremely slowly, the normalized margin for adversarial data gradually increases during the
adversarial training.
C.3 Divergence of weight norms for deep linear networks during adversarial
TRAINING
We conduct adversarial training of deep linear networks with `2 perturbation on a linearly separable
dataset. The loss function is logistic loss. Specifically, we use deep linear networks with layers 2, 3
and 4, respectively. Since the divergence is slow (〜lnt), it is hard to observe that ∣∣Wk∣∣f → ∞.
34
Published as a conference paper at ICLR 2022
However, as showed in Fig. 6(a), Fig. 6(b) and Fig. 6(c), the trends that kWk kF for all layers of deep
linear networks with different layers keep increasing are clear. Furthermore, we can see that the ratios
kWk k2/kWkkF for all layers of deep linear networks with different layers also keep increasing, as
showed by Fig. 7(a), Fig. 7(b) and Fig. 7(c).
(a) 4-layer deep linear network
(b) 3-layer deep linear network
(c) 2-layer deep linear network
Figure 7: Ratios of 2-norm and Frobenius norm for weights of deep linear networks with different
layers during adversarial training. The perturbations are `2 perturbations.
35