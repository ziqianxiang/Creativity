Published as a conference paper at ICLR 2022
Out-of-distribution Generalization in the
Presence of Nuisance-Induced Spurious Cor-
RELATIONS
Aahlad Puli1*	Lily H. Zhang Eric K. Oermann Rajesh Ranganath
New York University
Ab stract
In many prediction problems, spurious correlations are induced by a changing re-
lationship between the label and a nuisance variable that is also correlated with the
covariates. For example, in classifying animals in natural images, the background,
which is a nuisance, can predict the type of animal. This nuisance-label relation-
ship does not always hold, and the performance of a model trained under one such
relationship may be poor on data with a different nuisance-label relationship. To
build predictive models that perform well regardless of the nuisance-label rela-
tionship, we develop Nuisance-Randomized Distillation (NuRD). We introduce
the nuisance-randomized distribution, a distribution where the nuisance and the
label are independent. Under this distribution, we define the set of representa-
tions such that conditioning on any member, the nuisance and the label remain
independent. We prove that the representations in this set always perform better
than chance, while representations outside of this set may not. NuRD finds a rep-
resentation from this set that is most informative of the label under the nuisance-
randomized distribution, and we prove that this representation achieves the highest
performance regardless of the nuisance-label relationship. We evaluate NuRD on
several tasks including chest X-ray classification where, using non-lung patches
as the nuisance, NuRD produces models that predict pneumonia under strong
spurious correlations.
1	Introduction
Spurious correlations are relationships between the label and the covariates that are prone to change
between training and test distributions (Gulrajani and Lopez-Paz, 2020). Predictive models that ex-
ploit spurious correlations can perform worse than even predicting without covariates on the test dis-
tribution (Arjovsky et al., 2019). Discovering spurious correlations requires more than the training
distribution because any single distribution has a fixed label-covariate relationship. Often, spurious
correlations are discovered by noticing different relationships across multiple distributions between
the label and nuisance factors correlated with the covariates. We call these nuisance-induced spuri-
ous correlations.
For example, in classifying cows vs. penguins, typical images have cows appear on grasslands and
penguins appear near snow, their respective natural habitats (Arjovsky et al., 2019; Geirhos et al.,
2020), but these animals can be photographed outside their habitats. In classifying hair color from
celebrity faces on CelebA (Liu et al., 2015), gender is correlated with the hair color. This relation-
ship may not hold in different countries (Sagawa et al., 2020). In language, sentiment of a movie
review determines the types of words used in the review to convey attitudes and opinions. However,
directors’ names appear in the reviews and are correlated with positive sentiment in time periods
where directors make movies that are well-liked (Wang and Culotta, 2020a). In X-ray classification,
conditions like pneumonia are spuriously correlated with non-physiological traits of X-ray images
due to the association between the label and hospital X-ray collection protocols (Zech et al., 2018).
Such factors are rarely recorded in datasets but produce subtle differences in X-ray images that
convolutional networks easily learn (Badgeley et al., 2019).
We formalize nuisance-induced spurious correlations in a nuisance-varying family of distributions
where any two distributions are different only due to the differences in the nuisance-label rela-
tionship. As the nuisance is informative of the label, predictive models exploit the nuisance-label
relationship to achieve the best performance on any single member of the family. However, predic-
tive models that perform best on one member can perform even worse than predicting without any
covariates on another member, which may be out-of-distribution (ood). We develop NuRD to use
*1 Corresponding email: aahlad@nyu.edu. The code is available here.
1
Published as a conference paper at ICLR 2022
data collected under one nuisance-label relationship to build predictive models that perform well on
other members of the family regardless of the nuisance-label relationship in that member.
In section 2, we motivate and develop ideas that help guarantee performance on every member of
the family. The first is the nuisance-randomized distribution: a distribution where the nuisance
is independent of the label. An example is the distribution where cows and penguins have equal
chances of appearing on backgrounds of grass or snow. The second is an uncorrelating represen-
tation: a representation of the covariates such that under the nuisance-randomized distribution, the
nuisance remains independent of the label after conditioning on the representation. The set of such
representations is the uncorrelating set. We show that the nuisance-randomized conditional of the
label given an uncorrelating representation has performance guarantees: such conditionals perform
as well or better than predicting without covariates on every member in the family while other con-
ditionals may not. Within the uncorrelating set, we characterize one that is optimal on every member
of the nuisance-varying family simultaneously. We then prove that the same optimal performance
can be realized by uncorrelating representations that are most informative of the label under the
nuisance-randomized distribution.
Following the insights in section 2, we develop Nuisance-Randomized Distillation (NuRD) in sec-
tion 3. NuRD finds an uncorrelating representation that is maximally informative of the label un-
der the nuisance-randomized distribution. NURD’s first step, nuisance-randomization, breaks the
nuisance-label dependence to produce nuisance-randomized data. We provide two nuisance ran-
domization methods based on generative models and reweighting. The second step, distillation,
maximizes the information a representation has with the label on the nuisance-randomized data over
the uncorrelating set. We evaluate NuRD on class-conditional Gaussians, labeling colored MNIST
images (Arjovsky et al., 2019), distinguishing waterbirds from landbirds, and classifying chest X-
rays. In the latter, using the non-lung patches as the nuisance, NuRD produces models that predict
pneumonia under strong spurious correlations.
2	Nuisance-Randomization and Uncorrelating Sets
We formalize nuisance-induced spurious correlations via a family of data generating processes. Let
y be the label, z be the nuisance, and x be the covariates (i.e. features). The family consists of
distributions where the only difference in the members of the family comes from the difference in
their nuisance-label relationships. Let D index a family of distributions F = {pD}D; a member pD
in the nuisance-varying family of distributions F takes the following form:
pD(y,z,x) = p(y)pD(z | y)p(x | z, y),	(1)
where pD (z | y) is positive and bounded for any y where p(y) > 0 and any z in the family’s
nuisance space SF . This family is called the nuisance-varying family. Due to changing nuisance-
label relationships in this family, the conditional distribution of the label y given the covariates x in
one member, e.g. the training distribution, can perform worse than predicting without covariates on
another member of the family, e.g. a test distribution with a different nuisance-label relationship. We
define performance of a model p^(y | x) ona distribution Pte as the negative expected KL-divergence
from the true conditional Pte(V | x): Perfpte(P(y I x)) = -Epte(X)KL [pte(y | x) k p(y | x)].
Higher is better. This performance equals the expected log-likelihood up to a constant, Cpte =
Hpte(y I x), that only depends on the Pte Perfpt.(P(y I x)) = Epte(y,χ) logP(y I x) + Cpte∙
Consider the following example family {qa}a∈R:
y 〜N(0,1) Z 〜N(ay, 0.5) X =[xι 〜N(y - z, 1.5), x2 〜N(y + z, 0.5)].	(2)
Given training distribution Ptr = q1 and test distribution Pte = q-1, the conditional Ptr(y I x)
performs even worse than predicting without covariates, Perfpte (P(y)) ≥ Perfpte (Ptr(y I x));
see appendix A.9 for the proof. The problem is thatPtr(y I x) utilizes label-covariate relationships
that do not hold when the nuisance-label relationships change. When the changing nuisance-label
relationship makes the conditional PD(y I x) of one member unsuitable for another P0D ∈ F, the
family exhibits nuisance-induced spurious correlations.
Next, we identify a conditional distribution with performance guarantees across all members of
the family. We develop two concepts to guarantee performance on every member of the nuisance-
varying family: the nuisance-randomized distribution and uncorrelating representations.
Definition 1. The nuisance-randomized distribution is P (x, y, z) = P(x I y, z)Ptr(z)P(y).1
1Different marginal distributions p (z) produce different distributions where the label and nuisance are
independent. The results are insensitive to the choice as long as p (z) > 0 for any z ∈ SF . One distribution
that satisfies this requirement is p (z) = ptr (z). See lemma 2.
2
Published as a conference paper at ICLR 2022
In the cows vs. penguins example, p is the distribution where either animal has an equal chance to
appear on backgrounds of grass or snow. The motivation behind the nuisance-randomized distribu-
tion is that when the nuisance is independent of the label, (noisy2) functions of only the nuisance are
not predictive of the label. If the covariates only consist of (noisy) functions of either the nuisance
or the label but never a mix of the two (an example of mixing is x1 = y - z + noise), then the
conditional p (y | x) does not vary with the parts ofx that are (noisy) functions of just the nuisance.
Thus, p (y | x) ignores the features which have changing relationships with the label.
How about nuisance-varying families where the covariates contain functions that mix the label and
the nuisance? Equation (2) is one such family, where the covariates x1 and x2 are functions of
both the label and the nuisance. In such nuisance-varying families, the conditional p (y | x) can
use functions that mix the label and the nuisance even though the nuisance is not predictive of the
label by itself. These mixed functions have relationships with the label which change across the
family; for example in eq. (2), the coordinate x1 is correlated positively with the label under q0
but negatively under q-2 . Then, under changes in the nuisance-label relationship, the conditional
p (y | x) can perform worse than predicting without covariates because it utilizes a relationship, via
these mixed features, that no longer holds. See appendix A.9 for details.
We address this performance degradation of p (y | x) by introducing representations that help
avoid reliance on functions that mix the label and the nuisance. We note that when the condi-
tional p (y | x) uses functions that mix the label and the nuisance, knowing the exact value of the
nuisance should improve the prediction of the label, i.e. y p z | x. Therefore, to avoid reliance on
mixed functions, we define uncorrelating representations r(x), where the nuisance does not provide
any extra information about the label given the representation:
Definition 2. An uncorrelating set of representations is R(p ) s.t. ∀r ∈ R(p ), y p z | r(x).
In the example in eq. (2), r(x) = x1 + x2 is an uncorrelating representation because it is purely
a function of the label and the noise. Conditional distributions p (y | r(x)) for any uncorrelating
r(x) only depend on properties that are shared across all distributions in the nuisance-varying family.
Specifically, for r ∈ R(p ), the conditional distribution p (y | r(x)) uses p(r(x) | y, z) and p(y)
which are both shared across all members of the family F. For z0 such that p (z0 | r(x)) > 0,
0 p (y | z0)p (r(x) | y, z0) p(y)p(r(x) | y,z0)
pɪMr(X))=pɪ(y Ir(X)，Z ) = P∑HXΓM— = Ep(y)p(r(x)|y, z0).⑶
This fact helps characterize the performance ofp (y | r(x)) on any member pte ∈ F:
Perfpte (p (y | r(X))) = Perfpte(p(y)) + E KL p(r(X) | y,z) k Ep(y)p(r(X) | y, z) . (4)
pte(y,z)
As KL-divergence is non-negative, for any uncorrelating representation r, the conditional
p (y | r(X)) does at least as well as predicting without covariates for all members pte ∈ F:
Perfpte (p (y | r(X))) ≥ Perfpte (p(y)). See appendix A.3 for the formal derivation. In fact, we
show in appendix A.5 that when the identity representation r(X) = X is uncorrelating, then p (y | X)
is minimax optimal for a family with sufficiently diverse nuisance-label relationships.
Equation (4) lower bounds the performance ofp (y | r(X)) for any representation in the uncorrelat-
ing set across all pte ∈ F . However, it does not specify which of these representations leads to the
best performing conditional. For example, between two uncorrelating representations like the shape
of the animal and whether the animal has horns, which predicts better? Next, we characterize un-
correlating representations that are simultaneously optimal for all test distributions pte ∈ F.
Optimal uncorrelating representations. As we focus on nuisance-randomized conditionals,
henceforth, by performance of r(X), we mean the performance of p (y | r(X)): Perfpte (r(X)) =
Perfpte (p (y | r(X))). Consider two uncorrelating representations r, r2, where the pair (r, r2) is
also uncorrelating. How can r2(X) dominate r(X) in performance across the nuisance-varying fam-
ily? Equation (3) shows that
pɪ(y I [r(χ),r2(χ)]) Xp(y)p(r(χ) I r2(χ),y,z = z)p(r2(χ) I y,z = z).
If r2 (X) blocks the dependence between the label and r(X), i.e. r(X) p y | r2(X), z, then knowing
r does not change the performance when r2 is known, suggesting that blocking relates to perfor-
mance. In theorem 1, we show that the maximally blocking uncorrelating representation is simulta-
neously optimal: its performance is as good or better than every other uncorrelating representation
on every distribution in the nuisance-varying family. We state the theorem first:
2Noisy functions of a variable are functions of that variable and exogenous noise.
3
Published as a conference paper at ICLR 2022
Theorem 1. Let r* ∈ R(pɪ) be maximally blocking: Vr ∈ R(pɪ), ydLpιι r(x) | z, r*(x). Then,
1.	Simultaneous optimality ∀pte ∈ F, ∀r ∈ R(Pj	Perfpte(r*(X)) ≥ Perfpte(NX)).
2.	(InfOrmatiOnmaximaIity ∀r(x) ∈ R(px), lpɪ(y; r*(x)) ≥ lpɪ(y; r(x)).
3.	Information maximality implies simultaneous optimality ∀r0 ∈ R(p ),
IpX (y； r0(X))= IpX (y； r*(X))	=⇒	Vpte ∈ F, Perfpte(r* (X))= PerfpteerO(X)).
The proof is in appendix A.4. The first part of theorem 1, simultaneOus Optimality, says that a maxi-
mally blocking uncorrelating representation r* dominates every other r ∈ R(p ) in performance on
every test distribution in the family. In the cows vs. penguins example, the segmented foreground
that contains only the animal is a maximally blocking representation because the animal blocks the
dependence between the label and any other semantic feature of the animal.
The second and third parts of theorem 1 are useful for algorithm building. The second part proves
that a maximally blocking r* is also maximally informative of the label under p , indicating how to
find a simultaneously optimal uncorrelating representation. What about other information-maximal
uncorrelating representations? The third part shows that if an uncorrelating representation r0 has the
same mutual information with the label (under p ) as the maximally blocking r*, then r0 achieves the
same simultaneously optimal performance as r*. In the cows vs. penguins example, an example of a
maximally informative uncorrelating representation is the number of legs of the animal because the
rest of the body does not give more information about the label. The second and third parts of theo-
rem 1 together show that finding an uncorrelating representation that maximizes information under
the nuisance-randomized distribution finds a simultaneously optimal uncorrelating r(X).
3 Nuisance-Randomized Distillation (NuRD)
Theorem 1 says a representation that maximizes information with the label under the nuisance-
randomized distribution has the best performance within the uncorrelating set. We develop a rep-
resentation learning algorithm to maximize the mutual information between the label and a rep-
resentation in the uncorrelating set under the nuisance-randomized distribution. We call this al-
gorithm Nuisance-Randomized Distillation (NuRD). NuRD has two steps. The first step, called
nuisance randomization, creates an estimate of the nuisance-randomized distribution. The second
step, called distillation, finds a representation in the uncorrelating set with the maximum information
with the label under the estimate of the nuisance-randomized distribution from step one.
Nuisance Randomization. We estimate the nuisance-randomized distribution with generative
models or by reweighting existing data. Generative-NURD uses the fact that p(X | y, z) is
the same for each member of the nuisance-varying family F. With an estimate of this condi-
tional denoted P(x | y, z), generative-NuRD's estimate of the nuisance-randomized distribution
is Z 〜ptr (z), y 〜p(y), X 〜P(x | y, z). For high dimensional x, the estimateP(x | y, z) can be
constructed with deep generative models. Reweighting-NuRD importance weights the data from
ptr by p(y)/ptr(y | z), making it match the nuisance-randomized distribution:
p (X, y, z) =p(y)ptr(z)p(X | y,z) = p(y)ptr (z)
ptr (y | Z)
Ptr (y | z)
p(X | y, z)
pt⅛y⅛ ptr (X, y,z).
Reweighting-N U RD uses a model trained on samples from ptr to estimate 小喀| Z).
Distillation. Distillation seeks to find the representation in the uncorrelating set that maximizes
the information with the label under pɪ, the estimate of the nuisance-randomized distribution. Max-
imizing the information translates to maximizing likelihood because the entropy Hpx (y) is constant
with respect to the representation rγ parameterized by γ:
Ipx (rY(x)； V) — hM (y) = Epx(y,rγ(x)) logpx(y | rγ (X)) = max Epx(y,rγ(x)) logpθ (y | rγ(X)).
Theorem 1 requires the representations be in the uncorrelating set. When conditioning on rep-
resentations in the uncorrelating set, the nuisance has zero mutual information with the label:
Ip (y; z | rγ (X)) = 0. We operationalize this constraint by adding a conditional mutual information
penalty to the maximum likelihood objective with a tunable scalar parameter λ
maxEpx(y,z,χ) logpθ(y | rγ(x)) 一 λI鼠(y; Z | rγ(x)).	(5)
θ,γ
4
Published as a conference paper at ICLR 2022
The objective in eq. (5) can have local optima when the representation is a function of the nuisance
and exogenous noise (noise that generates the covariates given the nuisance and the label). The in-
tuition behind these local optima is that the value of introducing information that predicts the label
does not exceed the cost of the introduced conditional dependence. Appendix A.6 gives a formal dis-
cussion and an example with such local optima. Annealing λ, which controls the cost of conditional
dependence, can mitigate the local optima issue at the cost of setting annealing schedules.
Instead, we restrict the distillation step to search over representations rγ (x) that are also marginally
independent of the nuisance z under p , i.e. z p rγ (x). This additional independence removes
representations that depend on the nuisance but are not predictive of the label; in turn, this removes
local optima that correspond to functions of the nuisance and exogenous noise. In the cows vs. pen-
guins example, representations that are functions of the background only, like the presence of snow,
are uncorrelating but do not satisfy the marginal independence. Together, the conditional indepen-
dence y p z | rγ (x) and marginal independence z p rγ (x) hold if and only if the representation
and the label are jointly independent of the nuisance : (y, rγ (x)) p z. Using mutual information to
penalize joint dependence (instead of the penalty in eq. (5)), the distillation step in NURD is
maxEpχ(y,z,χ) logPθ(y | rγ(x)) - λIpx([y,rγ(x)]；z).	(6)
θ,γ
We show in lemma 4 that within the set of representations that satisfy joint independence, NuRD
learns a representation that is simultaneously optimal in performance on all members of the
nuisance-varying family. To learn representations using gradients, the mutual information needs
to be estimated in a way that is amenable to gradient optimization. To achieve this, we estimate the
mutual information in NuRD via the classification-based density-ratio estimation trick (Sugiyama
et al., 2012). We use a critic model pφ to estimate said density ratio. We describe this technique in
appendix A.1 for completeness. We implement the distillation step as a bi-level optimization where
the outer loop optimizes the predictive model pθ (y | rγ (x)) and the inner loop optimizes the critic
model pφ which helps estimate the mutual information.
Algorithm. We give the full algorithm boxes for both reweighting-NURD and generative-NURD
in appendix A.1. In reweighting-NuRD, to avoid poor weight estimation due to models memorizing
the training data, we use cross-fitting; see algorithm 1. The setup of nuisance-induced spurious
correlations in eq. (1) assumes p(y) is fixed across distributions within the nuisance-varying family
F. This condition can be relaxed when pte(y) is known; see appendix A.1.
4	Related Work
In table 1, we summarize key differences between NuRD and the related work: invariant learning
(Arjovsky et al., 2019; Krueger et al., 2020), distribution matching (Mahajan et al., 2020; Guo et al.,
2021), shift-stable prediction (Subbaswamy et al., 2019a), group-DRO (Sagawa et al., 2019), and
causal regularization (Veitch et al., 2021; Makar et al., 2021). We detail the differences here.
Nuisance versus Environment. In general, an environment is a distribution with a specific spuri-
ous correlation (Sagawa et al., 2019). When the training and test distributions are members of the
same nuisance-varying family, environments denote specific nuisance-label relationships. In con-
trast, nuisances are variables whose changing relationship with the label induces spurious correla-
tions. While obtaining data from diverse environments requires data collection from sufficiently dif-
ferent sources, one can specify nuisances from a single source of data via domain knowledge.
Domain generalization, domain-invariant learning, and subgroup robustness We briefly men-
tion existing methods that aim to generalize to unseen test data and focus on how these methods can
suffer in the presence of nuisance-induced spurious correlations; for a more detailed presentation,
see appendix A.2. Domain generalization and domain-invariant learning methods assume the train-
ing data consists of multiple sufficiently different environments to generalize to unseen test data that
is related to the given environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa
et al., 2019; Mahajan et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al.,
2018b; Goel et al., 2020; Wald et al., 2021; Ganin et al., 2016; Xie et al., 2017; Zhang et al., 2018;
Ghimire et al., 2020; Adeli et al., 2021; Zhang et al., 2021). Due to its focus on nuisances, NuRD
works with data from a single environment. Taking a distributional robustness (Duchi et al., 2021)
approach, Sagawa et al. (2019) used group-DRO to build models that perform well on every one of a
finite set of known subgroups. Other work also aims to minimize worst subgroup error with a finite
number of fixed but unknown subgroups (Lahoti et al., 2020; Martinez et al., 2021); as subgroups
5
Published as a conference paper at ICLR 2022
Table 1: NURD vs. methods that use nuisances or environments. In this work, the training data
comes from a single member of the family F, i.e. a single environment. For methods that require
multiple environments, values of the nuisance can be treated as environment labels. Unlike existing
methods, NuRD works with high-dimensional nuisances without requiring them at test time.
	Invariant	Dist. match	Shift-stable	GrouP-DRO	Causal reg.	NuRD
High-dim z	X	X	✓	X	X	✓
No test-time z	✓	✓	X	✓	✓	✓
are unknown, they only find an approximate minimizer of the worst subgroup error in general even
with infinite data. While these methods (Lahoti et al., 2020; Martinez et al., 2021) were developed
to enforce fairness with respect to a sensitive attribute, they can be applied to ood generalization
with the nuisance treated as the sensitive attribute; see (Creager et al., 2021). Given the nuisance,
existence of a finite number of subgroups maps to an additional discreteness assumption on the
nuisance variable; in contrast, NURD works with general nuisances. Given a high dimensional z,
as in our experiments, defining groups based on the value of the nuisance like in (Sagawa et al.,
2019) typically results in groups with at most one sample; with the resulting groups, methods that
minimize worst subgroup error will encourage memorizing the training data.
Nuisance as the environment label for domain generalization. Domain generalization methods
are inapplicable when the training data consists only of a single environment. In this work, the
training data comes from only one member of the nuisance-varying family, i.e. from a single envi-
ronment. What if one treats groups defined by nuisance values as environments? Using the nuisance
as the environment label can produce non-overlapping supports (over the covariates) between en-
vironments. In Colored-MNIST for example, splitting based on color produces an environment of
green images and an environment of red images. When the covariates do not overlap between envi-
ronments, methods such as Arjovsky et al. (2019); Krueger et al. (2020) will not produce invariant
representations because the model can segment out the covariate space and learn separate func-
tions for each environment. Methods based on conditional distribution matching (Mahajan et al.,
2020; Guo et al., 2021) build representations that are conditionally independent of the environment
variable given the label. When the training data is split into groups based on the nuisance value,
representations built by these methods are independent of the nuisance given the label. However,
splitting on a high-dimensional nuisance like image patches tends to yield many groups with only a
single image. Matching distributions of representations for the same label across all environments
is not possible when some environments only have one label.
Causal learning and shift-stable prediction. Anticausal learning (ScholkoPf et al., 2012) as-
sumes a causal generative process for a class of distributions like the nuisance-varying family in
eq. (1). In such an interPretation, the label y and nuisance z cause the image x, and under indePen-
dence of cause and mechanism, p(x | y, z) is fixed — in other words, indePendent — regardless of
the distribution pD (y, z). A closely related idea to NURD is that of Shift-Stable Prediction (Sub-
baswamy et al., 2019a;b). Subbaswamy et al. (2019a) Perform graph surgery to learn p (y | x, z)
assuming access to z during test time. Shift-stable models are not aPPlicable without nuisances at
test time while NuRD only requires nuisances during training. However, if the nuisance is available
at test time, the combined covariate-set [x, z] is 1) uncorrelating because y p z | [x, z] and 2) max-
imally blocking because r([x, z]) y | z, [x, z], and theorem 1 says p (y | [x, z]) is oPtimal.
Two concurrent works also build models using the idea of a nuisance variable. Makar et al. (2021)
assume that there exists a stochastic function of y but not z, called x*, such that yJLPD X | x*, z; they
use a marginal indePendence Penalty r(x) p z. Veitch et al. (2021) use counterfactual invariance
to derive a conditional indePendence Penalty r(X) ptrz | y. The theory in these works requires
the nuisance to be discrete, and their algorithms require the nuisance to be both discrete and low-
cardinality; NuRD and its theory work with general high-dimensional nuisances. Counterfactual
invariance Promises that a rePresentation will not vary with the nuisance but it does not Produce
oPtimal models in general because it rejects models that dePend on functions of the nuisance. On
the other hand, the uncorrelating ProPerty allows using functions of only the nuisance that are in
the covariates to extract further information about the label from rest of the covariates; this leads
to better Performance in some nuisance-varying families, as we show using the theory of minimal
sufficient statistics (Lehmann and Casella, 2006) in aPPendix A.7.
6
Published as a conference paper at ICLR 2022
5	Experiments
We evaluate the implementations of NuRD on class-conditional Gaussians, Colored-MNIST (Ar-
jovsky et al., 2019), Waterbirds (Sagawa et al., 2019), and chest X-rays (Irvin et al., 2019; Johnson
et al., 2019). See appendix B for implementation details and further evaluations of NuRD.
Model selection, baselines, and metrics. Models in both steps of NURD are selected using held-
out subsets of the training data. We split the training data into training and validation datasets with
an 80 - 20 split. For nuisance-randomization, this selection uses standard measures of held-out per-
formance. Selection in the distillation step picks models that give the best value of the distillation
objective on a held out subset of the nuisance-randomized data from NURD’s first step.
We compare against Empirical Risk minimization (ERM) because, as discussed in section 4, exist-
ing methods that aim to generalize under spurious correlations require assumptions, such as access to
multiple environments or discrete nuisance of small cardinality, that do not hold in the experiments.
When possible, we report the oracle accuracy or build gold standard models using data that does
not have a nuisance-label relationship to exploit. For every method, we report the average accuracy
and standard error across 10 runs each with a different random seed. We report the accuracy of each
model for each experiment on the test data (pte) and on heldout subsets of the original training data
(Ptr) and the estimate of the nuisance-randomized distribution (pɪ). For all experiments, We use
λ = 1 and one or two epochs of critic model updates for every predictive model update.
5.1	Class-Conditional Gaussians
We generate data as folloWs: With B(0.5) as the uniform Bernoulli distribution, qa(y, z, x) is
y 〜B(0.5) Z 〜N(a(2y - 1), 1) X = [xι 〜N(y - z, 9), x2〜N(y + z, 0.01)].	(7)
The training and test sets consist of 10000 samples from ptr = q0.5 and 2000 samples from pte =
q-0.9 respectively. All models in both NURD methods are parameterized With neural netWorks.
Table 2: Accuracy of NURD versus ERM on class conditional Gaussians.
Method	Heldout ptr	Heldout pɪ	pte
ERM	84 ± 0%	-	39 ± 0%
generative-NuRD	71 ± 0%	67 ± 0%	58 ± 0%
reWeighting-NuRD	71 ± 1%	66 ± 0%	58 ± 0%
Results. Table 2 reports results. The test accuracy of predicting With the optimal linear uncor-
relating representation r*(x) = xι + X2, is 62%; appendix B gives the optimality proof. Both
generative-NuRD and reWeighting-NuRD achieve close to this accuracy.
5.2	COLORED-MNIST
We construct a colored-MNIST dataset (Arjovsky et al., 2019; Gulrajani and Lopez-Paz, 2020) With
images of 0s and 1s. In this dataset, the values in each channel for every pixel are either 0 or 1.
We construct tWo environments and use one as the training distribution and the other as the test. In
training, 90% of red images have label 0; 90% of green images have label 1. In test, the relationship
is flipped: 90% of the 0s are green, and 90% of the 1s are red. In both training and test, the digit
determines the label only 75% of the time, meaning that exploiting the nuisance-label relationship
produces better training accuracies. The training and test data consist of 4851 and 4945 samples
respectively. We run NuRD With the most intense pixel as the nuisance.
Table 3: Accuracy of NURD versus ERM on Colored-MNIST. The oracle accuracy is 75%.
Method	Heldout ptr	Heldout pɪ	pte
ERM	90 ± 0%	-	10±0%
generative-NuRD	73 ± 1%	80 ± 1%	68 ± 2%
reWeighting-NuRD	75 ± 0%	74 ± 0%	75 ± 0%
7
Published as a conference paper at ICLR 2022
Results. See table 3 for the results. ERM learns to use color, evidenced by the fact that it achieves
a test accuracy of only 10%. The oracle accuracy of 75% is the highest achievable by models that do
not use color because the digit only predicts the label with 75% accuracy. While generative-NURD
has an average accuracy close to the oracle, reweighting-NURD matches the oracle at 75%.
5.3	Learning to classify Waterbirds and Landbirds
Sagawa et al. (2019) consider the task of detecting the type of bird (water or land) from images where
the background is a nuisance. Unlike Sagawa et al. (2019), we do not assume access to validation
and test sets with independence between the background and the label. So, we split their dataset
differently to create our own training and test datasets with substantially different nuisance-label
relationships. The training data has 90% waterbirds on backgrounds with water and 90% landbirds
on backgrounds with land. The test data has this relationship flipped. We use the original image
size of 224 × 224 × 3. The training and test sets consist of 3510 and 400 samples respectively.
We ensure that p(y = 1) = 0.5 in training and test data. Thus, predicting the most frequent class
achieves an accuracy of 0.5. Cropping out the whole background requires manual segmentation.
Instead, we use the pixels outside the central patch of 196 × 196 pixels as a nuisance in NURD. This
is a high-dimensional nuisance which impacts many existing methods negatively; see section 4. The
covariates are the whole image; see fig. 1.
Method	Ptr	PjL	Pte
ERM	91 ±	0%	-	66 ± 2%
reweighting-NURD	85±	1%	81	± 1%	83±2%
Figure 1:	Table of results and figure showing an example of the nuisance for Waterbirds. On the
left are the accuracies of NURD and ERM on Waterbirds. Gold standard accuracy is 90% (see
the results paragraph below). The figure shows an image and the corresponding border nuisance.
NuRD has both images during training and only the left image at test time.
Results. Figure 1 reports results. We construct a gold standard model on data where waterbirds
and landbirds have equal chances of appearing on backgrounds with water and land; this model
achieves a test accuracy of 90%. ERM uses the background to predict the label, as evidenced
by its test accuracy of 66%. Reweighting-NURD uses the background patches to adjust for the
spurious correlation to achieve an average accuracy close to the gold standard, 83%. We do not
report generative-NuRD’s performance as training on the generated images resulted in classifiers
that predict as poorly as chance on real images. This may be due to the small training dataset.
5.4 Learning to label pneumonia from X-rays
In many problems such as classifying cows versus penguins in natural images, the background,
which is a nuisance, predicts the label. Medical imaging datasets have a similar property, where fac-
tors like the device used to take the measurement are predictive of the label but also leave a signature
on the whole image. Here, we construct a dataset by mixing two chest x-ray datasets, CheXpert and
MIMIC, that have different factors that affect the whole image, with or without pneumonia. The
training data has 90% pneumonia images from MIMIC and 90% healthy images from CheXpert.
The test data has the flipped relationship, with 90% of the pneumonia images from CheXpert and
90% of the healthy images from MIMIC. We resize the X-ray images to 32 × 32. Healthy cases are
downsampled to make sure that in the training and test sets, healthy and pneumonia cases are equally
probable. Thus, predicting the most frequent class achieves an accuracy of 0.5. The training and test
datasets consist of 12446 and 400 samples respectively. In chest X-rays, image segmentation cannot
remove all the nuisances because nuisances like scanners alter the entire image (Pooch et al., 2019;
Zech et al., 2018; Badgeley et al., 2019). However, non-lung patches, i.e. pixels outside the central
patches which contain the lungs, are a nuisance because they do not contain physiological signals
of pneumonia. We use the non-lung patches (4-pixel border) as a nuisance in NURD. This is a high-
dimensional nuisance which impacts existing methods negatively; see section 4. The covariates are
the whole image; see fig. 2.
Results. Figure 2 reports results. Building an oracle model in this experiment requires knowledge
of all factors that correlate the label with all the parts of the X-ray. Such factors also exist within
each hospital but are not recorded in MIMIC and CheXpert; for example, different departments in
the same hospital can have different scanners which correlate the non-lung patches of the X-ray
8
Published as a conference paper at ICLR 2022
Method	Ptr	pɪ	pte
ERM	89 ± 0%	-	37± 1%
generative-NURD	70 ± 3%	90 ± 2	41 ± 2%
reweighting-NuRD	75 ± 1%	68 ± 1%	61 ± 1%
Figure 2:	Table of results and figure showing an example of the nuisance for chest X-rays. The
figure shows an example of a chest X-ray and the corresponding non-lung patches (right). NuRD
has both images during training and only the left image at test time.
with the label (Zech et al., 2018). ERM uses the nuisance to predict pneumonia, as evidenced by
its test accuracy of 37%. Reweighting-NURD uses the non-lung patches to adjust for the spurious
correlation and achieves an accuracy of 61%, a large improvement over ERM.
Generative-NuRD also outperforms ERM’s performance on average. Unlike reweighting-NuRD
which outperforms predicting without covariates, generative-NuRD performs similar to predicting
without covariates on average. The few poor test accuracies may be due to two ways generative
nuisance-randomization can be imperfect: 1) little reliance of x on z with y fixed, 2) insufficient
quality of generation which leads to poor generalization from generated to real images.
6 Discussion
We develop an algorithm for ood generalization in the presence of spurious correlations induced
by a nuisance variable. We formalize nuisance-induced spurious correlations in a nuisance-varying
family, where changing nuisance-label relationships make predictive models built from samples
of one member unsuitable for other members. To identify conditional distributions that have per-
formance guarantees on all members of the nuisance-varying family, we introduce the nuisance-
randomized distribution and uncorrelating representations. We characterize one uncorrelating rep-
resentation that is simultaneously optimal for all members. Then, we show that uncorrelating repre-
sentations most informative of the label under the nuisance-randomized distribution also achieve the
same optimal performance. Following this result, we propose to estimate the nuisance-randomized
distribution and, under this distribution, construct the uncorrelating representation that is most in-
formative of the label. We develop an algorithm called NuRD and show that it outperforms ERM
on synthetic and real data by adjusting for nuisance-induced spurious correlations. Our experiments
show that NuRD can use easy-to-acquire nuisances (like the border of an image) to do this adjust-
ment; therefore, our work suggests that the need for expensive manual segmentation, even if it does
help exclude all the nuisances, could be mitigated.
Limitations and the future. Given groups based on pairs of nuisance-label values, Sagawa et al.
(2020) suggest that subsampling equally from each group produces models more robust to spurious
correlations than reweighting (Byrd and Lipton, 2019; Sagawa et al., 2019); however, subsampling
is ineffective when the nuisance is high-dimensional. Instead, as sufficient statistics of the condi-
tional ptr (y | z) render y, z independent, grouping based on values of sufficient statistics could be
promising. The nuisance-randomization steps in generative-NuRD and reweighting-NuRD model
different distributions in the training distribution: ptr (x | y, z) and ptr (y | z) respectively. Methods
that combine the two approaches to produce better estimates of the nuisance-randomized distribution
would be interesting. The first step in reweighting-NURD is to estimate ptr(y | z). As deep net-
works tend to produce inflated probabilities (Guo et al., 2017), one must take care to build calibrated
models for p(y | z). Adapting either calibration-focused losses (Kumar et al., 2018; Goldstein et al.,
2020) or ensembling (Lakshminarayanan et al., 2016) may produce calibrated probabilities. In our
experiments, the training data contains a single environment. Methods for invariant representation
learning (Arjovsky et al., 2019; Krueger et al., 2020; Mahajan et al., 2020; Guo et al., 2021) typi-
cally require data from multiple different environments. Nuisance-randomized data has a different
nuisance-label relationship from the training data, meaning it is a different environment from the
training data. Following this insight, using nuisance-randomization to produce samples from differ-
ent environments using data from only a single environment would a fruitful direction. The absolute
performance for both ERM which exploits spurious correlations and NuRD which does not, is too
low to be of use in the clinic. Absolute performance could be improved with larger models, more
data, using pretrained models, and multi-task learning over multiple lung conditions, all techniques
that could be incorporated into learning procedures in general, including NuRD.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The authors were partly supported by NIH/NHLBI Award R01HL148248, and by NSF Award
1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science. The
authors would like to thank Mark Goldstein for helpful comments.
References
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard ZemeL Wieland Brendel,
Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings ofthe IEEE international conference on computer vision, pages 3730-3738, 2015.
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why
overparameterization exacerbates spurious correlations. In International Conference on Machine
Learning, pages 8346-8356. PMLR, 2020.
Zhao Wang and Aron Culotta. Identifying spurious correlations for robust text classification. arXiv
preprint arXiv:2010.02458, 2020a.
John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl
Oermann. Variable generalization performance of a deep learning model to detect pneumonia in
chest radiographs: a cross-sectional study. PLoS medicine, 15(11):e1002683, 2018.
Marcus A Badgeley, John R Zech, Luke Oakden-Rayner, Benjamin S Glicksberg, Manway Liu,
William Gale, Michael V McConnell, Bethany Percha, Thomas M Snyder, and Joel T Dudley.
Deep learning predicts hip fracture using confounding patient and healthcare variables. NPJ
digital medicine, 2(1):1-10, 2019.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine
learning. Cambridge University Press, 2012.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai
Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapo-
lation (rex). arXiv preprint arXiv:2003.00688, 2020.
Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching.
arXiv preprint arXiv:2006.07500, 2020.
Ruocheng Guo, Pengchuan Zhang, Hao Liu, and Emre Kiciman. Out-of-distribution prediction with
invariant risk minimization: The limitation and an effective fix. arXiv preprint arXiv:2101.07732,
2021.
Adarsh Subbaswamy, Peter Schulam, and Suchi Saria. Preventing failures due to dataset shift:
Learning predictive models that transport. In The 22nd International Conference on Artificial
Intelligence and Statistics, pages 3118-3127. PMLR, 2019a.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019.
Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invari-
ance to spurious correlations: Why and how to pass stress tests. arXiv preprint arXiv:2106.00545,
2021.
Maggie Makar, Ben Packer, Dan Moldovan, Davis Blalock, Yoni Halpern, and Alexander D’Amour.
Causally-motivated shortcut removal using auxiliary labels. arXiv preprint arXiv:2105.06422,
2021.
10
Published as a conference paper at ICLR 2022
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning
for domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018a.
Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning with ac-
curacy constraint for domain generalization. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pages 315-331. Springer, 2019.
Alexis Bellot and Mihaela van der Schaar. Accounting for unobserved confounding in domain
generalization. arXiv preprint arXiv:2007.10653, 2020.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5400-5409, 2018b.
Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup
performance gap with data augmentation. arXiv preprint arXiv:2008.06775, 2020.
Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain gener-
alization. arXiv preprint arXiv:2102.10395, 2021.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The journal of machine learning research, 17(1):2096-2030, 2016.
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance
through adversarial feature learning. arXiv preprint arXiv:1705.11122, 2017.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,
pages 335-340, 2018.
Sandesh Ghimire, Satyananda Kashyap, Joy T Wu, Alexandros Karargyris, and Mehdi Moradi.
Learning invariant feature representation to improve generalization across chest x-ray datasets.
In International Workshop on Machine Learning in Medical Imaging, pages 644-653. Springer,
2020.
Ehsan Adeli, Qingyu Zhao, Adolf Pfefferbaum, Edith V Sullivan, Li Fei-Fei, Juan Carlos Niebles,
and Kilian M Pohl. Representation learning with statistical independence to mitigate bias. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages
2513-2523, 2021.
Dinghuai Zhang, Kartik Ahuja, Yilun Xu, Yisen Wang, and Aaron Courville. Can subnetwork
structure be the key to out-of-distribution generalization? In International Conference on Machine
Learning, pages 12356-12367. PMLR, 2021.
John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A
generalized empirical likelihood approach. Mathematics of Operations Research, 2021.
Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang,
and Ed H Chi. Fairness without demographics through adversarially reweighted learning. arXiv
preprint arXiv:2006.13114, 2020.
Natalia L Martinez, Martin A Bertran, Afroditi Papadaki, Miguel Rodrigues, and Guillermo Sapiro.
Blind pareto fairness and subgroup robustness. In International Conference on Machine Learning,
pages 7492-7501. PMLR, 2021.
Elliot Creager, JOrn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant
learning. In International Conference on Machine Learning, pages 2189-2200. PMLR, 2021.
Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.
11
Published as a conference paper at ICLR 2022
Adarsh Subbaswamy, Bryant Chen, and Suchi Saria. A universal hierarchy of shift-stable distri-
butions and the tradeoff between stability and performance. arXiv preprint arXiv:1905.11374,
2019b.
Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pages 590-597, 2019.
Alistair EW Johnson, TomJ Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng,
Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a
large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042,
2019.
Eduardo HP Pooch, Pedro L Ballester, and Rodrigo C Barros. Can we trust deep learning mod-
els diagnosis? the impact of domain shift in chest radiograph classification. arXiv preprint
arXiv:1909.01940, 2019.
Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?
In International Conference on Machine Learning, pages 872-881. PMLR, 2019.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pages 1321-1330. PMLR, 2017.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, pages 2805-
2814. PMLR, 2018.
Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, and Rajesh Ranganath. X-cal: Explicit
calibration for survival analysis. Advances in Neural Information Processing Systems, 33, 2020.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In International
Conference on Machine Learning, pages 1448-1458. PMLR, 2020.
Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang, and Brahim Chaib-draa. Domain general-
ization with optimal transport and metric learning. arXiv preprint arXiv:2007.10573, 2020.
Hal Daume III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007.
Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia. A brief review of domain
adaptation. Advances in Data Science and Information Engineering, pages 877-894, 2021.
Zhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automat-
ically generated counterfactuals. arXiv preprint arXiv:2012.10040, 2020b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.
12
Published as a conference paper at ICLR 2022
A	Further details about NuRD and proofs
A.1 Details about NuRD
The algorithm boxes for reweighting-NuRD and generative-NuRD are given in algorithms 1 and
2.
Estimating and using the weights in reweighting-NURD. In learning ptr (y | z) for a high-
dimensional z, flexible models like deep neural networks can have zero training loss when the model
memorizes the training data. For a discrete y, such a model would output Ptr (y | z) = 1 for
every sample in the training data. Then, the model’s weight estimates on the training data are
p(y)∕^tr(y | Z) = p(y). Weighting the training data with such estimates fails to break the nuisance-
label relationship because Ptr (y,z, x) ^r(y Z) = ptr (y, z, x)p(y) Y Ptr (Z | y)p(x | y, z). Toavoid
such poor weight estimation, we employ a cross-fitting procedure: split the data into K disjoint
folds, and the weights for each fold are produced by a model trained and validated on the rest of
the folds. See algorithm 1 for details. In estimating loss for each batch under pɪ during training,
one can either weight the per-sample loss or produce the batches themselves via weighted sampling
from the data with replacement.
Density-ratio trick in Distillation. The density-ratio trick for estimating mutual information
(Sugiyama et al., 2012) involves Monte Carlo estimating the mutual information using a binary
classifier. Let ` = 1 be the pseudolabel for samples from P (y, rγ (x), z), and ` = 0 for samples
from P (y, rγ (x))P (z). Then,
Px(y,rγ(x),z)	p(' = 1 | y,z,rγ(x))
L([rγ(X), y]; z) = EPMy,z,x) log pi(y,r,(x))p,z) = EpWy,z,χ) logi- p(` =1 | y,z,rγ(x)).
With parameters φ, we estimate the conditional probability with a critic model, denoted Pφ .
Accounting for shifts in the marginal label distribution. NURD relies on the assumption in
eq. (1) that distributions in the nuisance-varying family F have the same marginal p(y). What
happens if pte comes from a nuisance-varying family with a different marginal? Formally,
with ptr ∈ F, let pte belong to a nuisance-varying family F0 = {pte(y)/ptr(y)pD (y, z, x) =
pte(y)pD(z | y)p(x | y, z)} where pD ∈ F. Given knowledge of the marginal distribution
pte(y), note that the weighted training distribution ptr 0 = pte(y)/ptr(y)ptr(y, z, x) lives in F0. Run-
ning NURD onptr0 produces predictive models that generalize to pte. To see this, note p0 (y, z, x) =
p0tr(y)p0tr(z)p(x | y, z) is a nuisance-randomized distribution in F0. With R(p0 ) as the uncorrelat-
ing set of representations defined with respect top0 , i.e. r0(x) ∈ R(p0 ) =⇒ y p0 z | r0, lemma 1
and theorem 1 hold. It follows that running NURD on samples from pte(y)/ptr(y)ptr (y, z, x) pro-
duces an estimate of p0 (y | r0(x)) (r0(x) ∈ R(p0 )) with the maximal performance on every
Pte ∈ F0 if a maximally blocking r* (x) ∈ R(Pi).
A.2 Extended related work
Domain generalization methods aim to build models with the goal of generalizing to unseen test
data different from the training data (Gulrajani and Lopez-Paz, 2020). Recent work uses multiple
sufficiently different environments to generalize to unseen test data that lies in the support of the given
environments or subgroups (Li et al., 2018a; Arjovsky et al., 2019; Akuzawa et al., 2019; Mahajan
et al., 2020; Krueger et al., 2020; Bellot and van der Schaar, 2020; Li et al., 2018b; Goel et al., 2020;
Wald et al., 2021). Chang et al. (2020) develop a multi-environment objective to interpret neural
network predictions that are robust to spurious correlations. Similarly, domain-invariant learning
and related methods build representations that are independent of the domain (Ganin et al., 2016;
Xie et al., 2017; Zhang et al., 2018; Ghimire et al., 2020; Adeli et al., 2021; Zhou et al., 2020).
Due to its focus on nuisances, NuRD works with data from a single environment. As in section 4,
to split the data into multiple environments, one can split the data into groups based on the value of
the nuisance. Then, domain-invariant methods build representations that are independent of the nui-
sance and under nuisance-induced spurious correlations these representations may ignore semantic
features because they are correlated with the nuisance. Domain adaptation (DaUme III, 2009; Ben-
David et al., 2007; Farahani et al., 2021) methods assume access to unlabelled test data which NuRD
13
Published as a conference paper at ICLR 2022
does not require. We do not assume access to the test data because nuisance-label relationships can
change over time or geography which, in turn, changes the the target distribution.
Algorithm 1: Reweighting-NURD
Input: Training data D, specification of the weight model pα (y | z) which estimates
ptr(y | z), representation model rγ (x), predictive model pθ(y | rγ (x)) and critic model
Pφ(' | y, z, rγ (x)); regularization coefficient λ, number of iterations for the weight
model Nw, for the predictive model and representation Np, and the number of critic
model steps Nc. Number of folds K.
Result: Return estimate ofp (y | rγ (x)) for rγ ∈ R(p ) with maximal information with y.
Nuisance Randomization step;
Estimate the marginal distribution over the label p(y);
Split data into K equal disjoint folds, D = {Fi}i≤K, for cross-fitting;
for each fold Fi, i ≤ K do // (cross-fitting)
Initialize pα(y | z);
for Nw iterations do
Sample training batch from the rest of the folds (F-i) : B 〜F-;
Compute likelihood P(yi,zi)∈B log pα(yi | zi);
Update α to maximize this likelihood (via Adam for example);
end
Produce weights Wi = p(yi)∕pα(y. | Zi) for each (yi, Zi, Xi) ∈ Fi;
end
Distillation step;
Initialize rγ, pθ , pφ;
for Np iterations do
for Nc iterations do
Sample training batch B 〜D and sample independent copies of Z marginally: z 〜D;
Construct batch B = {yi, Zi, Xi};
Compute likelihood
E	Wi logpφ(' =1 | VWY(Xi))+	E	Wi logPφ(' = 0 | y zi,rγX))∙
Ri,Zi,Xi)∈b	(yi,Zi,Xi)∈B
Update φ to maximize likelihood (via Adam for example);
end
Sample training batch B 〜D;
Compute distillation objective using the density-ratio trick
1
IBl
Wi
(xi,yi,Zi)∈B
log pθ (yi I rγ(Xi)) - λlog
Pφ(' = 1 ∣ Vi,Zi,rγ(Xi)) 一
1 - Pφ(' = 1 I Vi, Zi,rγ (Xi)) 一
Update θ, γ to maximize objective (via Adam for example).
end
Return pθ (y I rγ (X)).
Taking a distributional robustness (Duchi et al., 2021) approach, Sagawa et al. (2019) applied group-
DRO to training data where the relative size of certain groups in the training data results in spurious
correlations. Given these groups, group-DRO optimizes the worst error across distributions formed
by weighted combinations of the groups. With high dimensional Z as in our experiments, defining
groups based on the value of the nuisance typically results in groups with at most one sample;
with such groups, group-DRO will encourage memorizing the training data. Other work aims to
minimize worst subgroup error with a finite number of fixed but unknown subgroups (Lahoti et al.,
2020; Martinez et al., 2021); as subgroups are unknown, they only find an approximate minimizer
of the worst subgroup error in general even with infinite data.
In contrast, NuRD builds predictive models with performance guarantees across all test distribu-
tions (that factorize as eq. (1)) using knowledge of the nuisance. Given the nuisance, existence of
a finite number of subgroups maps to an additional discreteness assumption on the nuisance vari-
able; NuRD works with general high-dimensional nuisances. Wang and Culotta (2020b) focus on
14
Published as a conference paper at ICLR 2022
sentiment analysis of reviews and build a dataset where the nuisance label relationship is destroyed
by swapping words known to be associated with sentiment of the review, with their antonyms. This
is equivalent to using domain-specific knowledge to sample from p(x | y, z) in generative NURD.
NURD requires no domain-specific knowledge about the generative model p(x | y, z).
Algorithm 2: Generative-NURD
Input: Training data D, specification of the generative model pβ (x | y, z) that estimates
ptr(x | y, z), representation model rγ(x), predictive model pθ(y | rγ (x)), and critic
model pφ(' | y, z, r、(x)); regularization coefficient λ, number of iterations for the
weight model Nw, number of iterations for the predictive model and representation Np,
number of critic steps Nc .
Result: Return estimate ofp (y | rγ (x)) for rγ ∈ R(p ) with maximal information with y.
Nuisance Randomization step;
for Nw iterations do
Sample training batch B 〜D;
Compute likelihood P(y ,z ,x )∈B logpβ(xi | zi, yi) (or some generative objective);
Update β to maximize objective above;
end
Estimate the marginal distribution over the label p(y);
Sample independent label and nuisance y 〜D, Zj 〜D, and then sample X 〜pβ(X |、％, Zj);
Construct dataset D using triples {yk = z%, Zk = zj-, Xk = X};
Distillation step;
Initialize rγ, pθ , pφ;
for Np iterations do
for Nc iterations do
Sample training batch B 〜D and sample independent copies of Z marginally: Tzi 〜D;
Construct batch B = {yi, Zi, Xi} using B;
Compute likelihood
E	Wi log Pφ(' = 1 | Yi,Zi,Tγ (Xi))+	E	Wi log Pφ(' = 0 | yi, Zi,rγ (Xi)).
Ri,zi, Xi)∈B	(yi,Zi,Xi)∈B
Update φ to maximize likelihood (via Adam for example);
end
Sample batch from generated training data B 〜D;
Compute distillation objective using the density-ratio trick
1 p I p∖∖ ʌ 1	pφ(' = 1 | yk, Zk, rγ(Xk))
logPθ (yk | rγ(Xk )) - λ log --^—7—rη-——
.	1 - Pφ(' = 1 | yk, Zk,rγ(Xk))」
IBI	X	[l
(xk,yk,zk)∈B
;
Update θ, γ to maximize objective (via Adam for example).
end
Return pθ (y I rγ (X)).
15
Published as a conference paper at ICLR 2022
A.3 KEY LEMMAS FOR UNCORRELATING REPRESENTATIONS r ∈ R(p )
In this lemma, we derive the performance of the nuisance-randomized conditional p (y | r(x))
for any r ∈ R(p ) and show that it is at least as good as predicting without covariates on any
pte ∈ F .
Lemma 1. Let F be a nuisance-varying family (eq. (1)) and p = p(y)ptr (z)p(x | y, z) for some
ptr	∈	F.	Assume	∀pD	∈ F,	pD (z	| y) is bounded. If	r(x)	∈ R(p	),	then	∀pte	∈	F,	the
performance of p (y | r(x)) is
Perfpte(p (y | r(x))) = Perfpte(p(y)) + E KL p(r(x) | y,z) k Ep(y)p(r(x) | y, z) . (8)
pte(y,z)
As the KL-divergence is non-negative, Perfpte (p (y | r(x))) ≥ Perfpte (p(y)).
Proof. (of lemma 1) Note that the identity Ep(x)g ◦ f(x) = Ep(f(x))g ◦ f(x) implies that
EpteHM log pɪ M(y(x))=Epte(W(X)) log pɪ (y∣(y(x)).
As p (z | y) = ptr (z) > 0 on z ∈ SF and y s.t. p(y) > 0 is bounded, lemma 3 implies that
p (y, z, x) > 0 ⇔ pte (y, z, x) > 0. This fact implies the following KL terms and expectations of
log-ratios are all well-defined:
-Perfpte(p (y| r(x))) = Epte(x)KL[pte(y| x) k p (y | r(x))]
Epte(y,x) log
Pte(V | x)pte(y)
pɪ(y | r(x))pte(y)
Epte(y,x) log
Pte (y I χ)
Pte(y)
+ Epte(y,x) log
pte(y)
P (y | r(x))
Epte(y,x) log
Pte (y I χ)
pte(y)
+ Epte(y,r(X)) log pɪ(*)(x))
Epte(X)KL [pte(y | x) k pɪ(y)] + Epte(y,z,r(x)) log P (I；F(X))
Epte(X)KL [pte(y	|	x)	k	pɪ(y)]	+	Epte(y,Z)Epɪ(r(X) 1	y,Z)	log	p (y Ir(x))
Epte(X)KL [Pte(y	|	x)	k	pɪ(y)]	+	Epte(y,z)Epx(r(X) 1	y,z)	log	P (^^∖(x)' Z)
Epte(X)KL [Pte(y	I	x)	k	θɪ (y)]+	Epte(y,z)E以 (r(X) |	y,z)	log	-7]*(Xz)I	Z)
Epte(X)KL [Pte(y	|	x)	k	pɪ(y)]	+	Epte(y,Z)Epɪ(XI y,z) log P	(1\) I y Z)
Epte(X)KL [Pte(y	|	x)	k	pɪ(y)]	-	Epte(y,z)Epɪ(r(X) I	y,z)	log	^p^(T(Xp^zy^
Epte(X)KL[Pte(y I x) k P (y)] - Epte(y,z)KL[P (r (x) I y,z) k P (r(x) I z)]
Here, P (r(x) I y,z) = P(r(x) I y,z) as P (x I y,z) = P(x I y,z) by definition of the nuisance-
varying family. The proof follows by noting that the gap in performance of P (y I r(x)) and P(y)
equals an expected KL term:
-Perfpte(P(y)) + Perfpte (P (y I r (x))) = Epte(y,z)KL [P (r(x) I y,z) k P (r (x) I z)]
= Epte(y,z)KL P(r(x) I y,z) k Ep(y)P(r(x) I y,z) .
(9)
Rearranging these terms completes the proof.	□
Lemma 2 shows that uncorrelating sets are the same for any nuisance-randomized distribution and
that the conditional distribution of the label given an uncorrelating representations is the same for
all nuisance-randomized distributions.
16
Published as a conference paper at ICLR 2022
Lemma 2. Let F be a nuisance-varying family with p(y) and p(x | y, z) and nuisance
space SF. Consider distributions p ,1(y, z, x) = p(y)p ,1(z)p(x | y, z) and p ,2 (y, z, x) =
p(y)p ,2(z)p(x | y,z) such that p ,1(z) > 0,p ,2(z) > 0 for z ∈ SF, and p ,1(y,z,x) >
0 ^⇒ pɪ,2(y, z, x) > 0. Then, the uncorrelating sets are equal R(pɪj) = R(pi,2) and for
any r(x) ∈ R(p ,1),
p ,1(y | r(x)) =p ,2(y | r(x)).
Proof. By the assumption that p ,1(y, z, x) > 0 ⇔ p ,2(y, z, x) > 0, there exist some z such that
p ,1(z | r(x)) > 0 andp ,2(z | r(x)) > 0. With such z, for any r ∈ R(p ,1),
p ,1(y | r(x)) =p ,1(y | r(x), z)
= p(y)
p(r(x) | y,z)
pɪ J(MX) 1 z)
p(r(x) | y,z)
Ep ,1(y | z)[pɪ,Icr(X) 1 z, y)]
p(r(x) | y,z)
Ep(y)p(r(x) | z, y)
p(r(x) | y,z)
Ep ,2(y i z)p(r(X)| z, y)
= p(y)
p(r(x) | y,z)
pɪ,2(r(x) | z)
p ,2(y | r(x), z)
Taking expectation on both sides with respect to p ,2 (z | r(x)),
Ep	,2(z	|	r(x))p	,1(y	|	r(x)) =Ep ,2(z | r(x))p ,2(y | r(x), z)	=p ,2(y | r(x)).	(10)
Note that Ep	,2(z	|	r(x))p	,1(y	|	r(x)) =p ,1(y | r(x)), which implies
p ,1(y | r(x)) =p ,1(y | r(x), z) =p ,2(y | r(x), z) =p ,2(y | r(x)),
completing one part of the proof, p ,1(y | r(x)) = p ,2(y | r(x)).
Further, we showed y p ,1z | r(x) =⇒ y p ,2z | r(x) which means r(x) ∈ R(p ,2). As the
above proof holds with pɪj,pɪ,2 swapped with each other, r(x) ∈ R(pɪ,i) ^⇒ r(x) ∈ R(PJL,2).
□
The next lemma shows that every member of the nuisance-varying family is positive over the same
set ofy, z, x and is used in proposition 1 and lemma 1.
Lemma 3. Let the nuisance-varying family F be defined with p(y), p(x | y, z) and nuisance space
SF. Let distributions pD = p(y)pD(z | y)p(x | z, y) and p0D = p(y)p0D(z | y)p(x | z, y) be
such that pD (z | y), p0D (z | y) > 0 for all y such that p(y) > 0 and z ∈ SF. Further assume
pD(z | y),p0D(z | y) are bounded. Then, pD (y, z, x) > 0 ⇔ p0D (y, z, x) > 0.
Proof For any Z ∈ SF and any y such that p(y) > 0, PD (z | y) > 0 and PD(Z | y) > 0,
p0D (y, z, x) =p(x | y,z)p0D(z | y)p(y) =p(x | y,z)pD(z | y)p(y)
PD (Z | y)
Pd (z | y)
= PD (y, z, x)
PD (Z Iy)
Pd (z | y)
(11)
Thus, for all z in the nuisance space SF and any y such that P(y) > 0,
P0D (y, z, x) > 0 ^⇒ PD (y, z, x) > 0.
As z only takes values in the nuisance space SF, when P(y) = 0,
P0D (y, z, x) = PD (y, z, x) = 0.
Together, the two statements above imply
PD(y,z,x) > 0 ⇔P0D(y,z,x) > 0.
□
17
Published as a conference paper at ICLR 2022
A.4 Optimal uncorrelating representations
Theorem 1. Let r* ∈ R(pɪ) be maximally blocking: Vr ∈ R(pɪ),	yɪpɪr(x) | z, r*(x). Then,
1.	Simultaneous optimality ∀pte ∈ F, ∀r ∈ R(Pɪ), Perfpte(r*(X)) ≥ PerfptemX)).
2.	(InfOrmatiOnmaximaIity ∀r(x) ∈ R(px), lpɪ(y; r*(x)) ≥ lpɪ(y; r(x)).
3.	Information maximality implies simultaneous optimality ∀r0 ∈ R(p ),
IpX (y； r0(X))= IpX (y； r*(X))	=⇒	Vpte ∈ F, Perfpte(r* (X))= PerfpteerO(X)).
PrOOf. (proof for theorem 1)
We first prove that for any pair r, r2 ∈ R(p ) such that r2 blocks r, r(X) p y | z, r2(X), r2 domi-
nates the performance ofr on every pte ∈ F. The simultaneously optimality of the maximally block-
ing representation will follow. For readability, let '(r2) = Epte(X)KL [pte(y | x) ∣∣ pɪ(y | r2(X))].
We will show that
Epte(X)KL [pte(y | X) k pɪ(y | r(X))] ≥ '(r2).
We will use the following identity which follows from the fact that p(X | y, z) does not change
between distributions in the data generating process eq. (1):
pD (r2 (X), r(X) | y, z)
Pd(r2(χ) I y, z,r(χ)) =------rτ~Γ∖-----ʌ-
pD(r(X) | y,z)
=p(r2(χ),r(χ) I y,z)
p(r(χ) I y, z)
= p(r2 (X) I y, z, r(X)).
Next, we will show that
Epte(X)KL[pte(y I X) ∣ p (y I r(X))]
='(r2)+ Epte(y,z,r(x))KL [pɪ(r2(X) | y,z,r(X)) k pɪ(r2 (X) Ir(X),z)].
The steps are similar to lemma 1’s proof
pte(y I X)	p (y I r2(X))
Epte(X)KL [pte(y i X) k pɪ (y i r(X))] = Epte(y,x) log pɪ (y Ir2 (χ)) + Epte(y,x) lθg pɪ (y1 r(χ))
='(r2)+Epte(y,r(χ),r2(χ)) log ⅛y⅛χ∣
p (y I r (X) z)
='(r2)	+ Epte(y,z,r(X),r2(X))	log	p	(y	I	r(χ)	Z)	{as r, r2	∈ R(pi)}
p (y I r2(X), r(X), z)
= '(『2)+ Epte(y,z,r(X),r2(X)) log	pɪ(y "(χ), Z)—	{YXpɪ『(x)| z,r2(X)}
`(r2 ) + Epte(y,z,r(X),r2(X)) log
`(r2 ) + Epte(y,z,r(X),r2(X)) log
pɪ(y,r2(χ) I r(χ),z)
pɪ(y I r(χ), z)pɪ(r2(χ) I r(χ), z)
pɪ(r2(χ) I y,r(χ),z)
pɪ(r2(χ) I r(χ), z)
pɪ(r2(x) | y,r(χ),Z)
'(r2)+ Epte(y,z,r(χ))Epte(r2(χ) | y,z,r(χ)) log pɪ (r2 (χ) I r(χ),z)
'(r2)+ Epte(y,z,r(X))KL [pɪS(χ) ∣y,z,r(X)) k pɪ(r2(X) ∣ r(χ), z)]
Noting that KL is non-negative and that Perf is negative-KL proves the theorem:
Epte(X)KL[pte(y	I χ) k p	(y	I	r2(χ))] ≤	Epte(X)KL[pte(y	I χ)	k p	(y	I	r(χ))] .	(12)
It follows that for a maximally blocking r*
Vr ∈ R(p ) Epte(X)KL [pte(y I χ) k p (y I r*(χ))] ≤ Epte(X)KL [pte(y I χ) k p (y I r(χ))] .
18
Published as a conference paper at ICLR 2022
As performance is negative KL, the proof follows that r* dominates r in performance. This
concludes the first part of the proof.
For the second part, We prove information maximality of a maximally blocking r*(x) ∈ R(PL).
The proof above shows that the model pɪ(y | r*(x)) performs at least as well as PJL(y | r(x)) for
any r(x) ∈ R(PJL) on any Pte ∈ F. We characterize the gap in performance between pɪ(y | r* (x))
andP (y | r(x)) for any r(x) ∈ R(P ) as the following conditional mutual information term:
Ep (y,z,r(x))KL [P (r*(x) | y, z, r(x)) k P (r* (x) | r(x), z)] = Ip (r*(x); y | z, r(x)).
The entropy decomposition of conditional mutual information (with Hq(∙) as the entropy under a
distribution q) gives two mutual information terms.
Ep (y,z,r(x))KL [P (r*(x) | y, z, r(x)) k P (r* (x) | r(x), z)] = Ip (r*(x); y | z, r(x)),
= Hp	(y	|	z, r(x)) -Hp (y | z, r(x), r*(x))
= Hp	(y	|	z, r(x)) -Hp (y | z, r*(x))	{y	p r(x)	| z, r*(x)}
= Hp	(y	| r(x))	-Hp	(y | r*(x))	{r, r*	∈ R(P )}
= Hp	(y	| r(x))	-Hp	(y)+Hp (y)	-Hp	(y | r*(x))
=Ip (y; r*(x)) -Ip (y, r(x)).
This difference is non-negative for any r ∈ R(P ) which proves the second part of the theorem:
Ip (y; r*(x)) -Ip (y,r(x)) =Ip (r*(x);y | z, r(x)) ≥ 0.
For the third part, note that any representation r0 which satisfies Ip (y; r* (x)) = Ip (y, r0(x))
(information-equivalence) also satisfies
Ip (y; r*(x) | z, r0(x)) = 0 =⇒ y p r*(x) | z, r0(x).
Under this condition, eq. (12) implies
Epte(x)KL[Pte(y | x) k P (y | r*(x))] ≥ Epte(x)KL[Pte(y | x) k P (y | r0(x))] .
However, as r0 ∈ R(P ) and that r* (x) is maximally blocking, which (by the proof above) implies
Epte(x)KL[Pte(y | x) k P (y | r*(x))] ≤ Epte(x)KL[Pte(y | x) k P (y | r0(x))] .
The only way both these conditions hold is if
Epte(x)KL[Pte(y	| x)	k P (y | r*(x))] =	Epte(x)KL[Pte(y | x) k P (y | r0(x))] .
This completes the	proof	that	for any r0 ∈ R(P	) that is information-equivalent to r* (x)	under
P , the model P (y | r0(x)) has the same performance as P (y | r* (x)) for every Pte ∈ F, and
consequently, r0 is also optimal.
□
A.5 Minimax optimality
Proposition 1. Consider a nuisance-varying family F (eq. (1)) such that for some Ptr ∈ F there
exists a distribution P ∈ F such that P = P(y)Ptr (z)P(x | y, z) ∈ F. Let F satisfy
y	pDz	=⇒	∃P0D	∈ F s.t.	hEp0D(x)KL[P0D(y	| x) k	PD(y	| x)] -	Ip0D (x; y)i	> 0.	(13)
If y p z | x, then P (y | x) is minimax optimal :
P (y | x) = arg min max Ep0D(x)KL[P0D(y | x) k PD(y | x)] .
PD(y | χ)iPD∈FpD∈F
Proof. (of proposition 1) By lemma 3, as PD(y, z, x) > 0 ⇔ P0D(y, z, x) > 0, performance is well
defined for anyPD(y | x) on any P0D ∈ F. First, lemma 1 with Pte = P0D and r(x) = x gives
Ip0D (x; y) -Ep0D(x)KL[P0D(y | x) k P (y | x)]
= Ep0D(x)KL[P0D(y | x) k P (y)] -Ep0D(x)KL[P0D(y | x) k P (y | x)] (14)
= Ep0D(y,z)KL P(x | y,z) k Ep(y)P(x | y, z) .
= Ep0D(y,z)KL [P (x | y,z) k P (x | z)] ≥ 0.
19
Published as a conference paper at ICLR 2022
Thus, unlike any pD ∈ F such that y p z,
pm0 a∈xF hEp0D(x)KL[p0D(y | x) k p (y | x)] - Ip0D (x; y)i ≤ 0.	(15)
For any pD such that y pDz, let p0D be such that Ep0D (x) KL [p0D(y | x) k pD(y | x)] - Ip0D (x; y) >
0. As eq. (15)impliesEp0D(x)KL[p0D(y | x) k p (y | x)] - Ip0D (x; y) ≤ 0, it follows that ∀pD such
that y pD z,
max Ep0D(x)KL[p0D(y | x) k pD(y | x)] > max Ep0D(x)KL[p0D(y | x) k p (y | x)] .
p0D ∈F	D	p0D∈F	D
By lemma 3, any pD ∈ F is positive over the same set of y, z, x and if y pD z, then pD (y | x) =
p (y | x) (see lemma 2 for proof with instantiation r(x) = x). This means
p (y | x) = arg min max Ep0 (x)KL[p0D(y | x) k pD(y | x)] .
pD∈F p0D ∈F	D	D
See proposition 3 for an example nuisance-varying family where the information criterion in eq. (13)
holds.
A.6 Distillation details and a local optima example for eq. (5)
Figure 3: Landscape of the objective in eq. (5) for the example in eq. (2) for linear representations
ru,v (x) = ux1 + vx2. Local maxima correspond to representations r-u,u and global maxima to
representations ru,u .
The objective in eq. (5) can have local optima when the representation is a function of the nui-
sance and the exogenous noise in the generation of the covariates given the nuisance and the label.
Formally, let the exogenous noise satisfy (, z) p y. Then,
(, z) p y =⇒ (f (, z), z) p y =⇒ z p y | f (z, ).
Such a representation r(x) = f(, z) is both in the uncorrelating set and independent of the label y
under the nuisance-randomized distribution p meaning it does not predict the label.
Local optima example for conditional information regularization eq. (5). Figure 3 plots the
value of the objective in eq. (5) computed analytically for λ = 20, over the class of linear represen-
tations indexed by u, v ∈ R, ru,v (x) = ux1 + vx2, under the data generating process in eq. (2).
Representations of the kind r-u,u(x) = u(x2 - x1) are functions of z and some noise independent
of the label and, as fig. 3 shows, are local maxima on the landscape of the maximization objective
in eq. (5). Global maxima correspond to representations ru,u.
20
Published as a conference paper at ICLR 2022
Performance characterization for jointly independent representations
Lemma 4. Let F be a nuisance varying family. For any jointly independent representation r, i.e.
[r(x), y] p z,
∀pte ∈ F	Perfpte(p (y | r(x))) = Cpte +Ip (r(x); y),
where Cpte is a pte-dependent constant that does not vary with r(x).
NURD maximizes the information term Ip (y; rγ (x)) and, therefore, maximizes performance on
every member of F simultaneously. It follows that within the set of jointly independent represen-
tations, NURD, at optimality, produces a representation that is simultaneously optimal on every
pte ∈ F.
Proof. Lemma 1 says that for any uncorrelating representation r ∈ R(p ) and ∀pte ∈ F,
Perfpte (p (y |	r(x))) = Perfpte(p(y))	+ E KL	p(r(x)	| y,z) k	Ep(y)p(r(x)	| y,	z)	.
pte(y,z)
However, as the joint independence [r(x), y] p z implies both the uncorrelating property and
r(x) p z | y, the second term in the RHS above can be expressed as Ip (r(x); y):
E KL p(r(x) | y, z) k Ep(y)p(r(x) | y, z)
pte (y,z)
= E KL p (r(x) | y, z) k Ep (y)p (r(x) | y, z)
pte(y,z)
= E KL p (r(x) | y) k Ep (y)p (r(x) | y)
pte(y,z)
= E	KL [p (r(x) | y) k p (r(x))]
pte(y,z)
= E KL [p (r(x) | y) k p (r(x))]
pte(y)
= E KL [p (r(x) | y) k p (r(x))]
p (y)
= Ip (r(x); y).
Noting Cpte = Perfpte (p(y)) does not vary With r(x) completes the proof.	□
Performance gaps between jointly independent representations and uncorrelating representa-
tions. The joint independence [r(x), y] p z implies the uncorrelating property but uncorrelating
representations only satisfy this joint independence When they are independent of the nuisance.
Thus, representations that satisfy joint independence form a subset of uncorrelating representations.
This begs a question: is there a loss in performance by restricting NuRD to representations that
satisfy said joint independence? In appendix A.7.1, We use the theory of minimal sufficient statistics
(Lehmann and Casella, 2006) to shoW that there exists a nuisance-varying family Where the best un-
correlating representation dominates every representation that satisfies joint independence on every
member distribution, and is strictly better in at least one.
A.7 Counterfactual invariance vs. the uncorrelating property
We A) shoW that counterfactually invariant representations are a subset of uncorrelating representa-
tions by reducing counterfactual invariance to the joint independence [r(x), y] p z and B) give an
example nuisance-varying family F Where the best uncorrelating representation strictly dominates
every jointly independent representation in performance on every test distribution pte ∈ F: at least
as good on all pte ∈ F and strictly better on at least one.
We shoW A by proving counterfactually invariant representations satisfy joint independence
[r(x), y] p z Which implies the uncorrelating property, but not vice versa. Counterfactual invari-
ance implies that for all pD ∈ F, the conditional independence r(x) pDz | y holds by theorem 3.2
in Veitch et al. (2021). As y p z, it folloWs that [r(x), y] p z; this joint independence implies
the uncorrelating property, y p z | r(x). But, uncorrelating representations only satisfy the said
joint independence When they are independent of the nuisance.
We shoW B in appendix A.7.1 by constructing a nuisance-varying family Where the optimal perfor-
mance is achieved by an uncorrelating representation that is dependent on the nuisance.
21
Published as a conference paper at ICLR 2022
A.7.1 Joint independence vs. the uncorrelating property
Here, we discuss the performance gap between representations that are uncorrelating
(y p z | r(x)) and those that satisfy the joint independence (y, r(x)) p z. We construct a data
generating process where optimal performance on every member of F is achieved only by uncorre-
lating representations that do not satisfy joint independence.
Theorem 2. Define a nuisance-varying family F = {pD (y, z, x) = p(y)pD (z | y)p(x | y, z)}.
Let RJ = {r(x); [r(x), y] p z} and RC = {r(x); y p z | r(x)} be the set of representations
that, under the nuisance-randomized distribution, satisfy joint independence and conditional inde-
pendence respectively. Then there exists a nuisance-varying family F such that
∀pte ∈ F	max Perfpte (r(x)) ≤ max Perfpte (r(x)),	(16)
r∈RJ	r∈RC
and ∃pte ∈ F for which the inequality is strict
max Perfpte (r(x)) < max Perfpte (r(x)),	(17)
r∈RJ	r∈RC
Proof. In this proof we will build a nuisance-varying family F such that p ∈ F and y p z | x.
This makes x a maximally blocking uncorrelating representation because r(x) p y | x, z. Thus it
has optimal performance on every pte ∈ F within the class of uncorrelating representations. We let
y be binary, p ∈ F. The structure of the rest of the proof is as follows:
1.	The representation f(x) = p (y = 1 | x) is optimal in that it performs exactly as well as
x on every member of the family F .
2.	Any representation T (x) that matches the performance of x on every pte ∈ F satisfies
y p x | T (x).
3.	All functions T(x) such that y p x | T(x) determine f (x). This is shown in lemma 5.
4.	We construct a family where f (x) p z which, by the point above, means that every opti-
mal representation T(x) is dependent on z: T(x) p z. But every representation r ∈ RJ
satisfies r(x) p z and, therefore, is strictly worse in performance than f(x) on p , mean-
ing that they perform also strictly worse than x (because Perfpte (f (x)) = Perfpte (x)).
Noting x ∈ RC completes the proof.
For 1, let f(x) = p (y = 1 | x). However, we show here that p (y | f (x)) = p (y | x).
p (y =	1 | f(x)) =Ep	(x	|	f(x))p	(y	= 1 | x, f (x))
= Ep	(x	|	f(x))p	(y	= 1 | x)
= Ep (x | f(x))f(x)
= f(x)
=p (y = 1 | x) (=p (y | x, f (x)))
This means f(x) performs exactly as well as x on every pte ∈ F and x p y | f (x).
For 2, recall Perfpte (r(x)) = -Epte(x)KL [pte(y | x) k p (y | r(x))] and note that 1 implies
Perfp (x) = Perfp (f(x)) = 0.
Let T(x) be any function that performs as well as f(x) on every pte ∈ F. As p ∈ F,
0 = Perfp (f (x)) = Perfp (T (x))
= -Ep	(x)KL[p	(y	|	x) k p (y | T (x)]
= -Ep	(x)KL[p	(y	|	x, T (x)) k p (y |	T (x)]
= -Ip (y;x | T (x))
=⇒ y p x | T (x).
We leave 3 to lemma 5 and show 4 here.
22
Published as a conference paper at ICLR 2022
The example data generating process. We give a data generating process where f(x) = p(y =
1 | x) is dependent on z : f(x) p z. We assume p ∈ F. With a binary y and a normal z, let
p (y, z, x) = p(y)p(z)p(x | y, z) be generated as follows: with ρ : {0, 1} × {0, 1} → (0, 1), let
p(y =1)= 0.5, Z ~N(0,1), p(b = 1 | y = y,z = Z) = ρ(y, 1[z ≥ 0]), X =[b, 1[z ≥ 0]].
We will drop the subscript in p for readability next. Throughout the next part, we use a key
property of independence: [a, b]ɪc y⇒ bɪe | a, aɪe.
As z is a standard normal random variable 1[z ≥ 0] |z| | y, meaning we can write (y, 1[z ≥
0]) |z| because z is generated independently of y. Thus, as the distribution of b only depends on
1[z ≥ 0] and y due to the data generating process, it holds that (b, y, 1[z ≥ 0]) |z|. Then
(b, y, 1[z ≥ 0]) |z| =⇒ y |z| | b, 1[z ≥ 0] =⇒ y z | b, 1[z ≥ 0] =⇒ y z | X
As X only depends on 1[z ≥ 0] and b, for readability, we define a = 1[z ≥ 0]. Then p (y, a, b) =
p (y)p (a)p(b | y, a), where p(b = 1 | y = y,a = a) = ρ(y, a) andX = [b, a].
We overload the notation forf: expanding x = [b, a], we let f(x) = f(b, a) = p(y = 1 | X = [b, a]).
We write f(b, a) for different values of b here,
f(1,a) =p(y = 1 | X = [1, a]) =p(y = 1 | b = 1,a = a)
=p(b = 1, y = 1 | a = a)
p(b = 1 | a = a)
=	p(y = 1)p(b = 1 | y = 1, a = a)
py∈{o,i}p(y = y)p(b = 1 | y = y, a = a)
0.5p(b = 1 | y = 1, a = a)
0.5 Py∈{0,1} (P(b = 1 । y = y, a = a))
=	P(I,a)
ρ(0,a) + ρ(1,α).
f(0,a) =p(y = 1 | X = [0, a]) =p(y = 1 | b = 0,a = a)
=p(b = 0, y = 1 | a = a)
p(b = 0 | a = a)
=	p(y = 1)p(b = 0 | y = 1, a = a)
py∈{o,i} p(y = y)p(b =0 | y = y, a = a)
0.5(1 — p(b = 1 | y = 1, a = a))
0.5 Py∈{0,1} 1 - p(b = 1 | y = y, a = a)
=	1  ρ(1, a)
2 — P(O, a) — P(I, a)
We let P(y, 1) =	0.5 for y ∈ {0, 1}, P(0, 0) = 0.1, and P(1, 0) =			0.9. Then, with a =		1,	
	f(1,a) = f(0, a) =	P(1, 1)	0.5	0.5, 0.5			(18) (19)
		ρ(0,1)+ ρ(1,1) - 0.5 + 0.5 — 1 — P(1, 1)	1 —			0.5,		
		2 — P(0, 1) — P(I, a)	- 2 — 0.5	— 0.5			
and with a = 0,	f(1, a) = f(0, a) =	P(1, 0)	0.9	0.9, 0.9			(20) (21)
		p(0, 0) + p(1, 0) = 0.1+0.9 - 1 — P(1, 0)	1 —			0.1.		
		2 — p(0, 0) — p(1, 0)-	—2 — 0.1	— 0.9			
Thus, the distribution f(X) | a = a changes with a meaning that f (X) a which implies f (X) z
as a = 1[z ≥ a] is a function ofz.
23
Published as a conference paper at ICLR 2022
Note that f (x) z, then f (x) 6∈ RJ as f(x) z is an implication of joint independence. Any
function T (x) that achieves the same performance as p (y | f (x)) (by 3 and lemma 5), determines
f (x). It follows that, T (x) 6∈ RJ because
f (x) p z =⇒ T (x) p z.
So every r ∈ RJ must perform worse than f (x), and consequently x, on p . Finally, the indepen-
dence x p z implies x 6∈ RJ but y p z | x and so x ∈ RC . In this example we constructed, x is
the maximally blocking uncorrelating representation which means that it is optimal in RC on every
pte ∈ F. As RJ is a subset, any r ∈ RJ can at best match the performance of x and we already
showed that every r ∈ RJ is worse than f (x) and consequently x on p . This completes the proof.
□
Lemma 5. Consider a joint distribution p(y, x) with binary y. Assume that p(x | y = y) has the
same support for y ∈ {0, 1}. Then, for any function T (x) such that y x | T (x), the function
f(x) = p(y = 1 | x) is T (x)-measurable (T (x) determines f (x)).
Proof. We use the notion of sufficient statistics from estimation theory, which are defined for a
family of distributions, to define the set of functions T(x) for the joint distribution p(y, x).
Sufficient statistics in estimation theory. Consider a family of distributions P = {pθ (x); θ ∈ Ω }.
Assume θ is discrete and that Ω is finite. A function T(x) is a sufficient statistic of a family of
distributions P if the conditional distribution pθ(x | T(x) = t) does not vary with θ for (almost)
any value oft. A minimal sufficient statistic is a sufficient statistic M(x) such that for any sufficient
statistic T(X) T(x) = T(x0) =⇒ M(x) = M(x0). Any bijective transform of M(x) is also a
minimal sufficient statistic.
The rest of the proof will follow from relying on theorem 6.12 from Lehmann and Casella (2006)
which constructs a minimal sufficient statistic for a finite family of distributions P = {pi; i ∈
{0, K - 1}} as
M(X) = ∫ P1(X)P2(χ)…PKT(X)]
( )= lpo(x) ,P0 (x),	, P0(x) ʃ
Defining the family with conditionals. Now let the family P = {py (x)); y ∈ {0, 1}} where
py(x) = p(x | y = y) which are conditionals of the joint distribution p(x, y) we were given in the
theorem statement. Next, we show that the set of functions T(x) such that y px | T(x) is exactly
the set of the sufficient statistics for the family {p(x | y = y); y ∈ {0, 1}}.
By definition of sufficiency wherepy(x | T(x) = t) does not vary with y for any value oft,
∀t, Pi(χ IT(χ) = t) = po(χ IT(χ) = t) u⇒∀t, p(χ | T(χ) = t, y = 1) = p(χ | T(χ) = t, y = 0),
where the last statement is equivalent to the conditional independence x y | T (x).
Minimality of P(y = 1 I χ). By definition Py (χ) = P(χ I y = y). As this family contains only
two elements, the minimal sufficient statistic is
M(X) =	PI(X)	= P(X	| y =1)	= p(y	= 0) p(y = 1 |χ)
po(χ) p(x	I y = 0) p(y	= 1) 1 - p(y = 11 x) .
Thus, M(X) is a bijective transformation of the function P(y = 1 I X) (when P(y = 1 I X) ∈ (0, 1))
which in turn implies that P(y = 1 I x) is a minimal sufficient statistic for the family P.
Conclusion. We showed that the set of functions that satisfy y x I T(x) are sufficient statis-
tics for the family P. In turn, because only sufficient statistics T(x) of the family P satisfy
y px I T (x), it follows by definition that P(y = 1 I x) is determined by every T (x), complet-
ing the proof.
□
A.8 Gaussian example of the information criterion
Proposition 3. Consider the following family of distributions qa indexed by a ∈ R,
ey,ez -N(0, 1) y 〜N(0, 1) Z 〜N(ay, 1∕2) X = [y + ey,z + P1∕2ez]
In this family, for any PD = qb(y I z) where y p z, there exists aP0D = qa(y I z) such that
hEp0D(x)KL[P0D(y I X) k PD(y I X)] - Ip0D (X; y)i > 0.
24
Published as a conference paper at ICLR 2022
Proof. (of proposition 3) First, write Z = ay + ^/1726 where δ 〜 N(0,1). Let e = 1∕2(δ (δ + ∈z);
this is a normal variable with mean 0 and variance 1. Then, write x = [y+y, ay+] where y, are
Gaussian random variables with joint distribution q(ey)q(e). Therefore, qa(y, x) is a multivariate
Gaussian distribution, with the following covariance matrix (over y, x1 , x2 ):
Σ
1
2
a
=⇒ Σ1,2 = [1, a],	Σ2-,2,
1
a2 + 2
a2 + 1
-a
-a
2
The conditional mean and variance are:
Eqa [y | X = x] = ς1,2ς2,2x = a2 +2 [1, a]x
a2 + 1
σqa (y 1 X) = ς1,1 - ς1,2ς2,2ς2,1 = 1 -镇+2
1
02+2.
Rewrite the quantity in the theorem statement as a single expression:
Eqa(x)KL[qa(y | X) k qb(y | X)] -Iqa(X;y)
= Eqa(x)KL[qa(y | X) k qb(y | X)] -Eqa(x)KL[qa(y | X) k q(y)] .
qa(y | X)	qa(y | X)
=Eqa(X'y) log qbFR - Eqa(X'y) log ~KyΓ.
= Eqa (x,y) (log q(y) - log qb(y | X)) .
Expand (log q(y) - logqb(y | X)) in terms of quantities that vary with y, X and those that do not:
logq(y=y) - log qb(y=y1x)=- y22 - log √2π Jy -Ebqbyy Xχ'+log q2πσqb(y|x)
y2	2	、(y - b2+2 [1,b]χ)	/ 2π
- T - log √2π + (b +2)------2-------+ log Vb2+2
As only the first two terms vary with y, X, compute the expectations Eqa over these:
Eqa(X)qa(y | X)
I y2 +	(b2	+ 2)	(y	- TH ) =	Eq(y)	(-y2)	+ (b2	+	2)Eq(y)qa(χ	|	y)	- T
22	2	2
\ /
―1 ÷(/+» E	((b2+2)y -[1,b]χ)2
=- 2 + (b + 2)Eq(y)qa(x | y)-2(b2 +2)2-----
_	1 , κ	((b2 + 2)y -[1,b]x)2
=- 2 + EqH)qa(XI y)	2(b2 + 2)
_	1	((b2 + 2)y - y - ey - aby - be)2
=- 2 + Eq(y)q(ey)q(e)	2(b2 +2)
_	1	((b2 + 1 - ab)y - ey - be)2
=- 2 + EqH)q(Cy)qW	2(b2 +2)
1 var (b2 + 1 - ab)y + var(ey) + var(be)
=- 2 +	2(b2 +2)
1	(b2 + 1 - ab)2var (y) + var(ey) + b2var(e)
=- 2 +	2(b2 + 2)
_	1	(b2 + 1 - ab)2 + 1 + b2
=- 2 +	2(b2 + 2)
25
Published as a conference paper at ICLR 2022
(b2 + 1 - ab) - 1
2(b2 +2)
The proof follows for any a such that
(b2 + 1 — ab)2 — 1
2(b2 +2)
+ log √1∕b2+2
(b2 + 1 — ab)2 — 1
2(b2 +2)
-1 log (b2 +2)> 0
Let a = b + 1+ν for some scalar V. Then, if |v| > 1 + (b2 + 2) log(b2 + 2),
(b2 + 1 - ab)2 - 1
2(b2 + 2)
-1 log (b2+2)=2⅛⅛- 2 log 俨+2)> 0.
□
A.9 EXAMPLE WHERE ptr(y | x),p (y | x) PERFORM WORSE THAN p(y) UNDER pte
In this section, we motivate nuisance-randomization and the uncorrelating property. Consider the
following data generating process for a family {qa}a∈R and fixed positive scalar σ2:
y 〜N(0,1)	Z 〜N(ay, 0.5)	X =	[xι	〜N(y	- z,σ2	-	0.5), x2〜N(y	+ z, 0.5)] . (23)
Letting σ2 = 2 recovers the example in eq. (2). We keep σ2 for ease of readability. We first derive
the performance of p(y) = qb(y) = q(y) relative to qb(y | x) under qa.
Performance of	q(y)	relative to	qb(y	| x) under qa Rewrite x
[(1 - a) * y + √σ2eι, (1 + a) * y + e2)]
where e1,e2 〜N(0,1).
Let the joint distribution
over y, e1, e2 be q(y)q(e1)q(e2). Then, qa(y, x) is a multivariate Gaussian distribution with the
following covariance matrix (over y, x1 , x2):
1	(1-a)	(1+a)
Σ = (1 - a) (1 - a)2 + σ2	(1 - a2)	,
(1 + a)	(1 - a2)	(1 + a)2 + 1
∑	1 _	1	∑-ι  ______________1___________Λ1 + a)2 +1	-(1 - a2)
%,2 = [1 - a, 1 + a],	%2 = σ2(1 + a)2 + (1- a)2 + σ -(1- a2)	(1 - a)2 + σ2
Eqa [y | x = x] = ς1,2ς-2x = -----ʃɪ------Γ2----2 [(1 - a),σ2(1 + a)]x
σ2 (1 + a) + (1 - a) + σ2
σq2a (y | x) = Σ1,1 - Σ1,2Σ2-,12Σ2,1 = 1 -
σ2(1 + a)2 + (1 - a)2
σ2 (1 + a)2 + (1 — a)2 + σ2
σ2(1 + a)2 + (1 — a)2 + σ2
The performance of q(y) (recall Perf is negative KL) relative to qb(y | x) on qa (y, x) can be
written as
Eqa(x)KL[qa(y | x) k qb(y | x)] - Eqa(x)KL[qa(y | x) k q(y)] = Eqa (x,y) (log q(y) - log qb(y | x)) .
Expand (log q(y) - log qb(y | x)) in terms that vary with y, x and those that do not:
logq (y = y) - logqb(y = y | x)
=-%- log √2π + (y -7E;^y XXD+ log q2πσ2b (y |χ)
2	2σqb (y | x)
2	(y _	[(l-b),b2 (I+b)]x A 2 
=-y2- log √2π+(σ2(1+b)2+(1 - b)2+σ2)—σ"1+b2 σ2(1-b)+"—+log q2πσqt (y |χ)
2	(寸 _	[(I-吐σ2(I+b)]x A2
=-yr + (σ2(1 + b)2 + (1 - b)2 + σ2)匚卫与+=D + log /!C
As σq2b (y | x) does not vary with x, y, we will compute the expectation over the first two
terms:
Eqa(x,y) (log q(y) - logqb(y | x))
26
Published as a conference paper at ICLR 2022
-2 + (σ2(l + b)2 + (1 - b)2 + σ2)Eq(y)qα(x | y)
((σ2(1 + b)2 + (1 - b)2 + σ2)y - [(1 - b),σ2(1 + b)]x)2
2σ2(σ2 (1 + b)2 + (1 — b)2 + σ2)2
-2 + Eq(y)qα(χ I y)
((σ* 2(1 + b)2 + (1 - b)2 + σ2)y - [(1 - b),σ2(1 + b)]x)2
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
(recall X = [(1 - α)y + √σ2e1, (1 + a)y + ⑦
1
-+
2
Eq(y)q(ey)q(e)
1 + b)2 + (1 - b)2 + σ2)y - (1 - b)√σ2e1 - (1 - α)(1 - b)y - σ2(1 + α)(1 + b)y - σ2(1 + b)s)
-2 + Eq(y)q(%)q(e)
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
((σ2(1 + b)2 + (1 - b)2 - σ2(α + b + ab) - (1 - α)(1 - b)) y - (1 - b)√σ2e1 - σ2(1 + b)0)
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
{let C (a, b) = (σ2(1 + b)2 + (1 — b)2 — σ2(a + b + ab) — (1 — a)(1 — b)) }
-1 + Var (C(a, b)y) + var((1 - b)√σ⅝ι) + var(σ2(1 + b)o)
2	2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
-1 + (C (a, b))2 Var (y) + σ2(1 - b)2var(s) + (σ2)2(1 + bfvar(0)
2	2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
1	+ (C(a, b))2 + σ2(1 - b)2 + (σ2)2(1 + b)2
2	2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
C(a,b)2 - (σ2)2
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)
Note that log Jσ2i, (y | X) - log J σ2(I+ b)2；(Ij)2 + σ2 = 2 log S (I+ b) +∙21T) +σ ) . With this,
We have the ability to bound the performance of q(y) relative to qb(y | x) under qa.
The conditional qι(y | x) performs worse than qι(y) = p(y) on q-ι. We show here that
Ptr(y | x) = qι(y | x) does worse than qι(y) = p(y) on Pte = q-ι. Letting a = -1 =⇒
(1 + a) = 0, (1 — a) = 2 and b = 1 =⇒ (1 + b) = 2, (1 — b) = 0 then
C (a, b)	= σ2(1	+ b)2	+ (1 — b)2 — σ2(a +	b + ab) — (1 — a)(1 —	b) = σ2 * 22 + σ2 = 5σ2.
=	C(a,b)2 - (σ2)2_________ɪɪɑ 卜2(1 +	b)2 + (1 - b)2 + σ2)
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)	2	σ2
_ 25(σ2)2 - (σ2)2	1	5 * σ2
2 * σ2(5 * σ2)	2 " σ2
=12 - 1log5 > 0.
52
Predictive failure when X is not uncorrelating Note that q0(y | x) = pɪ (y | x) corresponds
to the nuisance randomized conditional of the label given the covariates. We show that q0(y | x)
27
Published as a conference paper at ICLR 2022
performs worse than p(y) on infinitely many qa. Letting b = 0 in C(a, b) helps compute the
performance of q(y) over q0(y | x) for every qa:
C(a, b) = σ2(1 + b)2 +(1 - b)2-σ2(a+b+ab)-(1-a)(1-b) = σ 2 +1-aσ 2 -(1-a) = σ2+a(1-σ2).
Then, the performance of q(y) relative to p (y | x) is
C(a,b)2 -(σ2)2__________11θ (σ2α + b)2 + (I - b)2 + σ2)
2σ2(σ2(1 + b)2 + (1 — b)2 + σ2)	2 2	σ2
_ (a2(1 — σ2)2 + (σ2)2 + 2σ2(1 — σ2)a) — (σ2)2	1	(2 * σ2 + 1)
=	2 * σ2(2 * σ2 + 1)	2 log 2
{letting σ2 = 2.}
This performance difference between q(y) andp (y | x) is positive for any a > 5 or a < —1. Thus,
for every test distribution qa such that a > 5 or a < —1, p (y | x) performs worse than marginal
prediction.
B Further experimental details
Implementation details In section 5, the label y is a binary variable and, consequently, we use the
Bernoulli likelihood in the predictive model and the weight model. In reweighting-NuRD in prac-
tice, the estimate of the nuisance-randomized distribution px(y, z, x) 8 Ptr(Y)/ptr(y ∣ Z)Ptr (y, z, x)
with an estimatedPtr (y | Z) may have a different marginal distribution px(y) = Ptr(y). To ensure
that ptr (y) = px(y), We weight our preliminary estimate pɪ again as ρSxy)PJy，z，x).
In all the experiments, the distribution Pθ (y | rγ (x)) is a Bernoulli distribution parameterized by rγ
and a scaling parameter θ. In general, when the family ofP (y | rγ (x)) is unknown, learning pre-
dictive models requires a parameterization Pθ(y | rγ (x)). When the family is known, for example
when y is categorical, the parameters θ are not needed because the distribution P(y | rγ (x)) can be
parameterized by the representation itself. For the critic model pφ(' | y, z, rγ(x)) in the distillation
step, we use a two layer neural network with 16 hidden units and ReLU activations that takes as
input y, rγ (x), and a scalar representation sψ(z); the critic model’s parameters are φ, ψ. The repre-
sentation sψ(z) is different in the different experiments and we give these details below.
In generative-NURD, we select models for P(x | y, z) by using the generative objective’s value
on a heldout subset of the training data. For model selection, we use Gaussian likelihood in the
class-conditional Gaussian experiment, binary likelihood in the colored-MNIST experiment, and
squared-loss reconstruction error in the Waterbirds and chest X-ray experiments. In reweighting-
NURD, we use a cross-fitting procedure where the training data is split into K folds, and K models
are trained: for each fold, we produce weights using a model trained and validated on the other
K — 1 folds. Hyperparameter selection for the distillation step is done using the distillation loss
from eq. (6) evaluated on a heldout validation subset of the nuisance-randomized data from the first
step.
In all experiments, we report results with the distillation step optimized with a fixed λ = 1 and
with 1 or 2 epochs worth of critic model updates per every representation update. In setting the
hyperparameter λ, a practitioner should choose the largest λ such that optimization is still stable for
different seeds and the validation loss is bounded away from that of marginal prediction. Next, we
give details about each individual experiment.
Optimal linear uncorrelating representations in Class Conditional Gaussians. Here, we show
that r* (x) = xι + x2 is the best linear uncorrelating representation in terms of performance. First let
the Gaussian noises in the two coordinates of X (given y, Z) be eι 〜N(0,9) and 功 〜N(0,0.01)
respectively. Define ru,v(x) = ux1 + vx2 = (u + v)y + (v — u)z + u1 + v2. We will show that
q0(z | ru,v(x),y = 1) 6= q0(z | ru,v(x),y = 0) when u 6= vandu 6= —v. First, q0(z, ru,v(x) | y =
y ) is a bivariate Gaussian with the following covariance matrix:
Σy = (v —1 u)
(v — u)
(v — u)2 + 9u2 + 0.01v2
--⇒ ςj],2 = V ― u,
Σ-1
Ly;2,2
1
(V - u)2 +9u2 + 0.01v2
28
Published as a conference paper at ICLR 2022
The conditional mean is:
Eqa [z | ru,v(x) = r,y] = E[z | y = 1] + Σ1,2Σ2-,12 (r - E[ru,v(x) | y = 1])
= E[z] + Σ1,2Σ2-,12 (r - (u + v)y)
(v - u)(r - (u + v)y)
=(V - U) + 9u2 + 0.01v2
which is independent of y if and only if u + v = 0 or u - v = 0. The conditional vari-
ance does not change with y because it is determined by Σy which does not change with y .
Thus q0(z | ru,v (x), y) = q0(z | ru,v(x)) if and only if u = v or u = -v. When u = v,
ru,v = 2uy + noise and ru,u q0y meaning that ru,u helps predict y. In contrast, when u = -v,
ru,v = 2vz + noise and so r-v,v q0y =⇒ q0(y | r-v,v )= q0 (y), meaning that q0 (y | r-v,v)
has the same performance as the marginal. However, for all u 6= 0, ru,u = ur1,1 is a bijective
transform of r1,1 and, therefore, q0(y | ru,u(x)) = q0(y | r1,1(x)). Thus, within the set of linear
uncorrelating representations, r1,1 is the best because its performance dominates all others on every
pte ∈ F .
Implementation details for Class Conditional Gaussians. In reweighting-NURD, the model for
ptr(y | z)is a Bernoulli distribution parameterized by a neural network with 1 hidden layer with 16
units and ReLU activations. In generative-NURD, the model for p(x | y, z)is an isotropic Gaussian
whose mean and covariance are parameterized with a neural network with one layer with 16 units
and ReLU activations. We use 5 cross-fitting folds in estimating the weights in reweighting-NURD.
We use weighted sampling with replacement in computing the distillation objective.
In the distillation step in both reweighting and generative-NURD, the representation rγ (x) is a
neural network with one hidden layer with 16 units and ReLU activations. The critic model
Pφ(' | y, z, rγ(x)) consists of a neural network with 2 hidden layers with 16 units each and ReLU
activations that takes as input y, rγ (x), and a scalar representation sψ (z) which is again a neural
network with a single hidden layer of 16 units and ReLU activations.
We use cross entropy to train Ptr(y | z), px(y | rγ(x)), and pφ(' | y, z,rγ(x)) using the
Adam (Kingma and Ba, 2014) optimizer with a learning rate of 10-2 . We optimized the model
for Ptr (y | Z) for 100 epochs and the model for Ptr (x | y, Z) for 300 epochs. We ran the distillation
step for 150 epochs with the Adam optimizer with the default learning rate. We use a batch size of
1000 in both stages of NURD. We run the distillation step with a fixed λ = 1 and two epoch’s worth
of gradient steps (16) for the critic model for each gradient step of the predictive model and the rep-
resentation. In this experiment, we do not re-initialize φ, ψ after a predictive model update.
Implementation details for Colored-MNIST. For reweighting-NURD, to use the same architec-
ture for the representation rγ (x) and for Ptr (y | z), we construct the nuisance as a 28 × 28 image
with each pixel being equal to the most intense pixel in the original image. In generative-NuRD, we
use a PixelCNN model for P(x | y, z) with 10 masked convolutional layers each with 64 filters. The
model was trained using a Bernoulli likelihood with the Adam optimizer and a fixed learning rate
of 10-3 and batch size 128. We parameterize multiple models in this experiment with the follow-
ing neural network: 4 convolutional layers (with 32, 64, 128, 256 channels respectively) with ReLU
activations followed by a fully connected linear layer into a single unit. Both rγ (x), sψ (z) are pa-
rameterized by this network. Both Ptr (y | Z) in reweighting-N U RD and Ptr (y | x) for ERM are
Bernoulli distributions parameterized by the network described above. We use 5 cross-fitting folds
in estimating the weights in reweighting-NuRD.
For the critic model Pφ(' | y, z, ∖(x)) in the distillation step, We use a two-hidden-layer neural
network with 16 hidden units and ReLU activations that takes as input y, rγ (x), and the scalar
representation sψ(z); the parameters φ contain ψ and the parameters for the two hidden-layer neural
network. The predictive model Pθ (y | rγ (x)) is a Bernoulli distribution parameterized by rγ (x)
multiplied by a scalar θ .
We use cross entropy to train Ptr(y | z), Px(y | rγ(x)), and Pφ(' | y, z,rγ(x)) using the
Adam (Kingma and Ba, 2014) optimizer with a learning rate of 10-3. We optimized the model
for Ptr(y | z) for 20 epochs and ran the distillation step for 20 epochs with the Adam optimizer
with the default learning rate. We use a batch size of 300 in both stages of NURD. We run the
distillation step with a fixed λ = 1 and one epoch’s worth of gradient steps (14) for the critic model
for each gradient step of the predictive model and the representation. In this experiment, we do not
re-initialize φ, ψ after a predictive model update.
29
Published as a conference paper at ICLR 2022
Implementation details for the Waterbirds experiment. For generative-NURD, we use VQ-
VAE 2 (Razavi et al., 2019) to model ptr(x | y, z). For multiple latent sizes and channels in the
encoder and the decoder, we saw that the resulting generated images were insufficient to build clas-
sifiers that predict better than chance on real data. This may be because of the small training dataset
that consists of only 3000 samples. The model for ^(y | rγ(x)) is two feedforward layers stacked
on top of the representation r、(x). The model Ptr (y | Z) in reweighting-NURD is the same model
as px(y | rγ(x)) as a function of x. The model for pφ(' | y, z, r、(x)) consists of a neural network
with two feedforward layers that takes as input y, rγ (x), and a representation sψ(z). Both rγ and sψ
are Resnet-18 models initialized with weights pretrained on Imagenet; the parameters φ contain ψ
and the parameters for the two hidden-layer neural network. The model in ERM for ptr (y | x) uses
the same architecture as px(y | r、(x)) as a function of x. We use 5 cross-fitting folds in estimating
the weights in reweighting-NuRD.
We use binary cross entropy as the loss in training Ptr (y | z), pθ (y | r、(x)), andpφ(' | y, z, r、(X)))
using the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 10-3. We optimized the
model for IPtr (y | Z) for 10 epochs and ran the distillation step for 5 epochs with the Adam optimizer
with the default learning rate for all parameters except γ, which parameterizes the representation
r、; for γ, we used 0.0005. The predictive model, the critic model, and the weight model are all
optimized with a weight decay of 0.01. We use a batch size of 300 for both stages of NURD. We
run the distillation step with a fixed λ = 1 and two epoch’s worth of gradient steps (16) for the critic
model for each gradient step of the predictive model and the representation. To prevent the critic
model from overfitting, we re-initialize φ, ψ after every gradient step of the predictive model.
Implementation details for the chest X-ray experiment. To help with generative modeling,
when creating the dataset, we remove X-ray samples from MIMIC that had all white or all black
borders. We use a VQ-VAE2 (Razavi et al., 2019) to model p(x | y, z) using code from here to both
train and sample. The encoder takes the lung patch as input, and the decoder takes the quantized
embeddings and the non-lung patch as input. VQ-VAE2 is hierarchical with a top latent code and
a bottom latent code which are both vector-quantized and fed into the decoder to reconstruct the
image. Both latents consist of 8 × 8 embeddings each of dimension 64. The VQ-VAE is trained
for 200 epochs with Adam (Kingma and Ba, 2014) with a batch size of 256 and dropout rate of
0.1. Generating samples from the VQ-VAE2 involves sampling the top latent code conditioned on
the label, followed by sampling the bottom latent code conditioned on the label and the top latent
code, and passing both latent codes to the decoder. To generate from the latent codes, we build a
PixelSNAIL to generate the top latent code given the label and a PixelCNN to generate the bottom
latent code given the label and the top latent code. These models have 5 residual layers with 128
convolutional channels. All other details were default as in here. We train these models for 450
epochs with a batch size of 256 with a learning rate of 5 × 10-5.
For reweighting-NURD, the model px(y | r、(x)) is two feedforward layers stacked on top of the
representation r、(x). The model in ERM for Ptr (y | x) uses the same architecture as px(y | r、(x))
as a function of x. Next we use a single architecture to parameterize multiple parts in this experi-
ment: 3 convolutional layers (each 64 channels) each followed by batch norm, and dropout with a
rate of 0.5 and followed by a linear fully-connected layer into a single unit. We parameterize the
two representations r、 (x), sψ(z) with this network. To build pφ (` | y, z, r、 (x)), we stack two feed-
forward layers of 16 hidden units with ReLU activations on top of a concatenation of y, r、 (x),and
the scalar representation sψ(z) as described above; the parameters φ contain ψ and the parameters
for the two hidden-layer neural network. We use 5 cross-fitting folds in estimating the weights in
reweighting-NuRD.
We use binary cross entropy as the loss in training Ptr (y | z), p§ (y | r、(x)), andp@(' | y, z, r、(x)))
using the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 10-3. We use a batch size
of 1000 for both stages of NuRD. We optimized the model for Ptr (y | z) for 150 epochs and ran
the distillation step for 100 epochs with the Adam optimizer with the default learning rate. Only the
optimization for Ptr (y | z) has a weight decay of 1e - 2. We run the distillation step with a fixed
λ = 1 and two epoch’s worth of gradient steps (20) for the critic model for each gradient step of the
predictive model and the representation. To prevent the critic model from overfitting, we re-initialize
φ, ψ after every gradient step of the predictive model.
30
Published as a conference paper at ICLR 2022
NuRD vs. ERM on classifying Waterbirds
5 0 5 0 5
8 8 7 7 6
Aoalnooecb><
NuRD vs. ERM on Pneumonia detection
Oooo
7 6 5 4
OT><
0.5
0.7
0.9
Test p (strength of the spurious correlation)
Figure 4: Plots of average accuracy vs. test ρ for classifying Waterbirds and Pneumonia. A larger ρ
implies a larger difference between the nuisance-label relationship in the test data used for evaluation
and the training data, which has a ρ = 0.1. Nuisance-randomized data corresponds to ρ = 0.5.
Unlike NuRD, ERM’s performance quickly degrades as the difference between the train and the
test distributions increases.
0.5	0.7	0.9
Test p (strength of the spurious correlation)
B.1	Additional experiments
Excluding the boundary from the images (covariates) does not improve ERM in general. In
both Waterbirds and chest X-rays, we use the easy-to-acquire border as a nuisance in NuRD. Models
trained on the central (non-border) regions of the image can exploit the nuisances in the center and
consequently fail to generalize when the nuisance-label relationship changes. In fact, classifiers
produced by ERM on border-less images do not generalize well to the test data, producing test
accuracies of 39 ± 0.5% on chest X-rays and 65 ± 2.3% on Waterbirds averaged over 10 seeds.
However, as independence properties that hold for the border also hold for nuisances in the central
region that are determined by the border, NuRD can use the border to control for certain nuisances
in the center of the image.
Additional experiments with NuRD. We evaluate reweighting-NURD further in the following
ways:
1.	Run NuRD on data from the training data distribution defined in section 5 and evaluate on
data from test distributions pte with different nuisance-label relationships.
2.	Train NuRD with different-sized borders as nuisances.
3.	Train NuRD without a nuisance where the training and the test data have the same
nuisance-label relationship; we implement this by setting the nuisance z = 0 wherever
it is passed as input in the weight model or critic model.
4.	Run the distillation step with different λ.
Different test distributions. For this experiment, we compute the test accuracies of the models
trained in the experiments in section 5 on data with different nuisance-label relationships. For both
classifying Waterbirds and Pneumonia, a scalar parameter ρ controls nuisance-label relationships in
the data generating process. In waterbirds, ρ = p(y = waterbird | background = land) = p(y =
landbird | background = water). In chest X-rays, ρ corresponds to the fraction of Pneumonia cases
that come from CheXpert and normal cases that come from MIMIC in the data; in this task, hospital
differences are one source of nuisance-induced spurious correlations. In both tasks, ρ = 0.1 in the
training data; as test ρ increases, the nuisance-label relationship changes and becomes more different
from the training data. We plot the average and standard error of accuracies aggregated over 10 seeds
for different test ρ ∈ {0.5, 0.7, 0.9} in fig. 4.
Nuisance specification with different borders. For Waterbirds, we ran NuRD with the pixels
outside the central 168x168 patch (a 56 pixel border) as the nuisance. Averaged over 10 seeds,
reweighting-NuRD produced a model with 81% test accuracy which is similar to the accuracy
achieved by NuRD using a 28-pixel border as the nuisance. In comparison, ERM achieves an
accuracy of 66%.
NuRD without a nuisance and no nuisance-induced spurious correlations in classifying Wa-
terbirds. We performed an additional experiment on classifying Waterbirds where NURD is given
a constant nuisance which is equivalent to not using the nuisance. We generated training and test
31
Published as a conference paper at ICLR 2022
data with independence between the nuisance and the label; the nuisance-label relationship does
not change between training and test. Averaged over 10 seeds, ERM achieved a test accuracy of
89 ± 0.4% and NURD achieved a test accuracy of 88 ± 1%.
Reweighting-NuRD with different λ. Large λs may make optimization unstable by penalizing
even small violations of joint independence. Such instabilities can lead NuRD to build predic-
tive models that do not do better than marginal prediction, resulting in large distillation loss (log-
likelihood + information loss) on the validation subset of the training data. However, a small λ
may result in NuRD learning non-uncorrelating representations which can also perform worse than
chance.
We ran NURD on the waterbirds and class-conditional Gaussians experiments with λ = 5 (instead
of λ = 1 like in section 5) and found that, on a few seeds, NURD produces models with close to
50% accuracy (which is the same as majority prediction) or large information loss or both. Excluding
seeds with large validation loss, reweighting-NURD achieves an average test accuracy of 76% on
waterbirds and 61% on class-conditional Gaussians. Annealing the λ during training could help
stabilize optimization.
In setting the hyperparameter λ in general, a practitioner should choose the largest λ such that
optimization is still stable over different seeds and the validation loss is bounded away from that of
predicting without any features.
32