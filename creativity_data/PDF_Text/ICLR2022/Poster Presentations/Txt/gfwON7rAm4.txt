Published as a conference paper at ICLR 2022
Global Convergence of Multi-Agent Policy
Gradient in Markov Potential Games
Stefanos Leonardos
Singapore University of Technology and Design
Stefanos_leonardos@sutd.edu.sg
William Overman
University of California, Irvine
overmana@uci.edu
Ioannis Panageas
University of California, Irvine
ipanagea@ics.uci.edu
Georgios Piliouras
Singapore University of Technology and Design
georgios@sutd.edu.sg
Ab stract
Potential games are one of the most important and widely studied classes of normal-
form games. They define the archetypal setting of multi-agent coordination in
which all agents utilities are perfectly aligned via a common potential function.
Can we embed this intuitive framework in the setting of Markov games? What are
the similarities and differences between multi-agent coordination with and without
state dependence? To answer these questions, we study a natural class of Markov
Potential Games (MPGs) that generalizes prior attempts to capture complex stateful
multi-agent coordination. Counter-intuitively, insights from normal-form potential
games do not carry over since MPGs involve Markov games with zero-sum state-
games, but Markov games in which all state-games are potential games are not
necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties
such as the existence of deterministic Nash policies. In our main result, we
prove convergence of independent policy gradient and its stochastic counterpart to
Nash policies at a rate that is polynomial in the approximation error by adapting
single-agent gradient domination properties to multi-agent settings. This answers
questions on the convergence of finite-sample, independent policy gradient methods
beyond settings of pure conflicting or pure common interests.
1 Introduction
Multi-agent reinforcement learning (MARL) has been the fundamental driver of numerous recent
advances in Artificial Intelligence (AI) and Machine Learning (ML) ranging from super-human
performance in competitive game-playing (Silver et al., 2016; 2018; Brown & Sandholm, 2019;
Jaderberg et al., 2019) and multi-tasking (Mnih et al., 2015; OpenAI, 2018; Vinyals et al., 2019)
to robotics, autonomous-driving and cyber-physical systems (Busoniu et al., 2008; Zhang et al.,
2019). However, despite the popularity of MARL algorithms to analyze these systems in practice, the
theory that underpins their empirical success lags behind (Dafoe et al., 2020). Many state-of-the-art
theoretical results concern single-agent RL systems, typically modelled as single-agent Markov
Decision Processes (MDPs) (Bertsekas, 2000; Panait & Luke, 2005; Sutton & Barto, 2018).
The main challenge when transitioning from single to multi-agent RL settings is the computation of
Nash policies. For n ≥ 2 agents, a Nash policy is defined to be a profile of policies (n；,…，∏n) so
that by fixing the policies of all agents but i, n； is optimal for the resulting single-agent MDP and
this is true for all 1 ≤ i ≤ n 1 (Definition 1). In multi-agent settings, Nash policies may not be unique
in principle and, unlike singe-agent MDPs, agents’ rewards may differ dramatically between them.
A common approach to compute Nash policies in MDPs is the use of policy gradient methods. The
significant progress in the analysis of such methods (see Agarwal et al. (2020) and references therein)
has mainly concerned the single-agent case or the case of pure common interests (identical agents)
(Wang & Sandholm, 2002; Panait & Luke, 2005): the convergence properties of policy gradient in
1Analogue of Nash equilibrium notion.
1
Published as a conference paper at ICLR 2022
general MARL settings remain poorly understood. Recent steps towards a theory for multi-agent
settings involve Daskalakis et al. (2020) who show convergence of independent policy gradient to the
optimal policy for two-agent zero-sum stochastic games, Wei et al. (2021) who improve the result of
Daskalakis et al. (2020) using optimistic policy gradient and Zhao et al. (2021) who study extensions
of Natural Policy Gradient using function approximation. It is worth noting that the positive results
of Daskalakis et al. (2020); Wei et al. (2021) and Zhao et al. (2021) depend on the fact that two-agent
stochastic zero-sum games satisfy the min-max equals max-min property (Shapley, 1953).
If we move away from the extremes of single-agent or purely competitive settings (two-agents,
zero-sum), a lot of these regularities, and in particular the value-uniqueness property, cease to
hold. However, building a theory to analyze problems of cooperation between two or more agents
constitutes a primary open challenge for the fields of AI and ML (Dafoe et al., 2020; Dafoe et al.,
2021). Based on the above, our work is motivated by the following natural question:
Can we get (provable) convergence guarantees for multi-agent RL settings
in which agents have aligned incentives, i.e., in which coordination is desirable?
Model and Informal Statement of Results. To make progress in this direction, we study a class
of n-agent MDPs that naturally generalize normal form potential games (Monderer & Shapley,
1996), the archetypal model of interactions between multiple agents with aligned, yet not necessarily
identical interests, called Markov Potential Games (MPGs). In words, a multi-agent MDP is an
MPG as long as there exists a (state-dependent) real-valued potential function Φ so that if an agent i
changes their policy (and the rest of the agents keep their policy unchanged), the difference in agent
i’s value/utility, V i, is captured by the difference in the value of Φ (Definition 2).
Our first task is to understand the structural properties of MPGs and their Nash policies. Rather
surprisingly, many insights from normal-form potential games do not carry over as MPGs involve
settings with purely competitive (zero-sum) interactions at some states. Moreover, Markov games in
which every state-interaction is a potential game are not necessarily MPGs. These findings suggest
that MPGs form a class of MDPs with rich structure which challenges our intuition on the nature of
cooperation in state-based interactions. On the other hand, MPGs trivially include MDPs of pure
common interests (MDPs in which agents have identical rewards) and showcase intuitively expected
properties such as the existence of deterministic Nash policies. Our structural results are as follows.
Theorem 1.1 (Structural Properties of MPGs). The following facts are true for MPGs with n-agents.
(a) There always exists a deterministic Nash policy profile (see Theorem 3.1).
(b)	We can construct MDPs for which each state is a (normal-form) potential game but which are
not MPGs. This can be true regardless of whether the whole MDP is competitive or cooperative in
nature (see Examples 1 and 2, respectively). On the opposite side, we can construct MDPs that are
MPGs, but which include states that are purely competitive (i.e., zero-sum games), see Figure 3.
(c)	We provide sufficient conditions so that an MDP is an MPG. These include cases where each
state is a (normal-form) potential game and the transition probabilities are not affected by agents
actions or the reward functions satisfy certain regularity conditions between different states (see
conditions C1 and C2 in Proposition 3.2).
We then turn to our motivating question above and, in our main contribution, we answer it in the
affirmative. We show that if every agent i independently runs (with simultaneous updates) projected
gradient ascent (PGA) on their policy (using their value V i), then, after O(1/2) iterations, the
system will reach an -approximate Nash policy. Here, independence means that (PGA) requires only
local information to determine the updates, i.e., each agent’s own rewards, actions, and view of the
environment. Such protocols are naturally motivated in distributed settings where all information
about type of interaction and other agents’ actions is encoded in the agent’s environment. For the finite
samples analogue, we show that the system will reach an -approximate Nash policy after O(1/6)
iterations. Our main convergence results are summarized in the following (informal) Theorem.
Theorem 1.2 (Convergence of Policy Gradient (Informal)). Consider an MPG with n agents and
let > 0. (a) Exact Gradients: If each agent i runs independent policy gradient using direct
parameterization on their policy and the updates are simultaneous, then the learning dynamics
reach an -Nash policy after O(1/2) iterations. (b) Finite Samples: If each agent i runs stochastic
policy gradient using greedy parameterization (see equation 3) on their policy and the updates are
simultaneous, then the learning dynamics reach an -Nash policy after O(1/6) iterations.
2
Published as a conference paper at ICLR 2022
The formal statements for cases (a) and (b) are provided in Theorems 4.2 and 4.4, respectively. The
technical details are presented in Section 4. The main step in the proof of Theorem 1.2 establishes
that Projected Gradient Ascent (PGA) on the potential function generates the same dynamics as
each agent running independent PGA on their value function. This follows from a straightforward
derivation of an agent-wise version of the single-agent gradient domination property which can be
used to show that every (approximate) stationary point (Definition 4) of the potential function is an
(approximate) Nash policy (Lemma 4.1). If agents do not have access to exact gradients, the key is to
get an unbiased sample for the gradient of the value functions and prove that it has bounded variance
(in terms of the parameters of the MPG). This is established by requiring agents to perform stochastic
PGA with α-greedy exploration (equation 3). The main idea is that this parameterization stays away
from the boundary of the simplex throughout its trajectory (Daskalakis et al., 2020).
Other Related Works. Our paper contributes to the growing literature on cooperative AI and ML
(Carroll et al., 2019; Dafoe et al., 2020). The results on convergence of MARL algorithms are scarce
and largely restricted to purely competitive (Daskalakis et al., 2020; Wei et al., 2021; Zhao et al.,
2021) or purely cooperative (Wang & Sandholm, 2002; Bard et al., 2020) settings. As Daskalakis et al.
(2020) argue, the current frontier concerns the extension to settings that are not zero-sum, involve
more than two agents and/or are cooperative in nature, albeit likely for weaker solution concepts. Our
current paper proceeds precisely in this direction, and in fact, it does so without reverting to a weaker
solution concept. Concerning the setup, our paper contributes to the rapidly growing literature on
MPGs and variations thereof which is showcased by the works of Marden (2012); Valcarcel Macua
et al. (2018); Mguni (2021); Mguni et al. (2021) and the partially concurrent work of Zhang et al.
(2021). Marden (2012) study Markov games that are potential at every state and which satisfy a
strong additional state-transitivity property. Under the same assumption, Mguni (2021) derive an
analogous result to our Theorem 3.1 on the existence of deterministic Nash policies. Valcarcel Macua
et al. (2018) provide an analytical way to find a closed-loop Nash policy in a class of Markov games
that is more closely related to our current MPG setting (cf. Proposition 3.2). Zhang et al. (2021)
study the same class of MPGs, and present additional practical applications. By introducing the
notion of averaged MDPs, they derive an alternative, model-based policy evaluation method, which,
interestingly, establishes the same sample-complexity (of O(-6)) as our model-free estimation. As
pointed out by Zhang et al. (2021), it will be instructive to further explore this connection. Concerning
the technical parts, our methods are related to Daskalakis et al. (2020); Agarwal et al. (2020); Kakade
& Langford (2002) and to Davis & Drusvyatskiy (2018); Bubeck (2015); Nemirovski et al. (2009).
2	Preliminaries
Markov Decision Processes (MDPs). We consider a setting with n agents who select ac-
tions in a shared Markov Decision Process (MDP). Formally, an MDP is a tuple G =
(S, N, {Ai, Ri}i∈N , P, γ, ρ), where S is a finite state space of size S = |S|, N = {1, 2, . . . , n} is
the set of agents, and Ai is a finite action space of size Ai = |Ai| for each agent i ∈ N with generic
element ai ∈ Ai. We write A = Qi∈N Ai and A-i = Qj6=i Aj to denote the joint action spaces
of all agents and of all agents other than i with generic elements a = (ai)i∈N and a-i = (aj )j6=i,
respectively. Ri : S × A → [-1, 1] is the reward function of agent i ∈ N, i.e., Ri(s, ai, a-i) is
the instantaneous reward of agent i when they take action ai and all other agents take actions a-i
at state s ∈ S. P is the transition probability function, for which P (s0 | s, a) is the probability of
transitioning from s to s0 when a ∈ A is chosen by the agents. Finally, γ (same for all agents) is a
discount factor for future rewards and ρ ∈ ∆(S) is a distribution for the initial state at time t = 0.2
Whenever time is relevant, we will index the above terms with t. In particular, at each time step
t ≥ 0, all agents i ∈ N observe the state st ∈ S, select actions at = (ai,t , a-i,t), receive
rewards r*t := Ri(st, at), and transition to the next state st+ι 〜P(∙ | st, at). We will write
τ = (st, at, rt)t≥0 to denote the trajectories of the system, where rt := (ri,t), i ∈ N.
Policies and Value Functions. For each agent i ∈ N , a deterministic, stationary policy πi : S →
Ai specifies the action of agent i at each state s ∈ S, i.e., πi (s) = ai ∈ Ai for each s ∈ S. A
stochastic, stationary policy πi : S → ∆(Ai) specifies a probability distribution over the actions
2We will write ∆(X) to denote the set of probability distributions over any set X.
3
Published as a conference paper at ICLR 2022
of agent i for each state s ∈ S. Accordingly, πi ∈ Πi := ∆(Ai)S. In this case, we will write
ai 〜∏i(∙ | S) to denote the randomized action of agent i at state S ∈ S .As above, We will write
π = (πi )i∈N ∈ Π := ×i∈N ∆(Ai ) and π-i = (πj )i6=j∈N ∈ Π-i := ×i6=j ∈N ∆(Aj ) to denote
the joint policies of all agents and of all agents other than i, respectively. A joint policy π induces
a distribution Prπ over trajectories τ = (St, at, rt)t≥0, where S0 is drawn from the initial state
distribution P and a*t is drawn from ∏i(∙ | St) for all i ∈ N.
The value function, Vsi : Π → R, gives the expected reward of agent i ∈ N when S0 = S and the
agents draw their actions, at = (ai,t, a-i,t), at time t ≥ 0 from policy π = (πi, π-i)
Vsi(π) := Eπ hXt∞=0 γtri,t | S0 = Si .	(1)
We also write Vρi(∏) = Es〜P [Vi(∏)] if the initial state is random and follows distribution ρ. The
solution concept that we will be focusing on are the Nash Policies which are formally defined next.
Definition 1 (E-NaSh Policy). Ajoint policy ∏* = (∏*)i∈N is an E-NaSh policy if there exists an
E ≥ 0 so that for each agent i ∈ N, Vsi(∏7, ∏-i) ≥ Vi(∏i, ∏-%) - e, for all ∏ ∈ ∆(A4)S, and
all s ∈ S. If E = 0, then ∏* is a called a Nash policy. In this case, ∏* maximizes each agent i's
value function for each starting state S ∈ S given the policies, ∏-i = (∏j )j=%, of all other agents
j = i ∈ N. The definition of a Nash policy remains the same if S 〜P (random starting state).
3 Markov Potential Games.
We are now ready to define the class of MDPs that we will focus on for the rest of the paper.
Definition 2 (Markov Potential Game). A Markov Decision Process (MDP), G , is called a Markov
Potential Game (MPG) if there exists a (state-dependent) function Φs : Π → R, with S ∈ S, so that
Vsi(πi,π-i) - Vsi(πi0,π-i) = Φs(πi,π-i) - Φs(πi0,π-i),
for all agents i ∈ N, states S ∈ S and policies πi, πi0 ∈ Πi, π-i ∈ Π-i. Linearity of expectation im-
plies that Φp(πi,∏-i) - Φρ(∏0,∏-i) = VP(πi,∏-i) - Vi(π0,∏-i), where Φp(π) := Es〜P [Φs(∏)].
As in normal-form games, an immediate consequence of this definition is that the value function
of each agent in an MPG can be written as the sum of the potential (common term) and a term
that does not depend on that agent’s policy (dummy term), i.e., for each agent i ∈ N there exists a
function Usi : Π-i → R so that Vsi(π) = Φs (π) + Usi(π-i), for all π ∈ Π (cf. Proposition B.1 in
Appendix B).
Definition 3 (Ordinal and Weighted Potential Games). Similar to normal-form games, we can
define more general notions of MPGs: weighted or ordinal MPGs. If there exist positive constants
wi >	0, i	∈ N so that wi	Vsi(πi,π-i)	- Vsi(πi0, π-i)	=	Φs(πi,	π-i) -	Φs(πi0,π-i),	then G is a
Weighted Markov Potential Game (WMPG). If for all agents i ∈ N, all states S ∈ S and all policies
∏i,∏i ∈ ∏i,π-i ∈ Π-i, the function Φs, S ∈ S satisfies Vi(πi,π-i) — Vsi(∏0, π-i) > 0 ^⇒
Φs(πi, π-i) - Φs (πi0, π-i) > 0, then G is an Ordinal Markov Potential Game (OMPG).
Such classes are naturally motivated in the setting of multi-agent MDPs. As Example 2 shows, even
simple potential-like settings, i.e., settings in which coordination is desirable for all agents, may fail
to be exact MPGs (but may still be ordinal or weighted MPGs). From our current perspective, ordinal
and weighted MPGs remain relevant since our main convergence results on the convergence of policy
gradient carry over (in an exact or asymptotic sense) also in these classes of games (Remark 1).
Existence of Deterministic Nash Policies in MPGs. Before studying the types of MDPs that are
captured by Definition 2, we first show that MPGs always possess deterministic Nash policies (similar
to their single-state counterparts, i.e., to normal-form potential games Monderer & Shapley (1996)).
This is established in Theorem 3.1, which settles part (a) of Theorem 1.1. As with the rest of the
proofs (and technical details) of Section 3, the proof of Theorem 3.1 is provided in Appendix B.
Theorem 3.1 (Deterministic Nash Policy Profile). Let G be a Markov Potential Game (MPG). Then,
there exists a Nash policy π* ∈ ∆(A)S which is deterministic, i.e., for each agent i ∈ N and each
state s ∈ S, there exists an action a% ∈ Ai so that π* (ai | s) = 1.
4
Published as a conference paper at ICLR 2022
(a0A,a0B)=(0,0)
0	1	(0, 0)
0	5, 2 -1,-2
1	-5, -4	1, 4
Figure 1: an MDP with normal-from potential Figure 2: an MDP with normal-form potential
games at each state which is not an MPG due to games at each state which is an OMPG but not an
conflicting preferences over states.	MPG despite common preferences over states.
The proof of Theorem 3.1 relies on the following iterative reduction process of the non-deterministic
components of an arbitrary Nash policy profile that is also a global maximizer of the potential
function. At each iteration, we isolate an agent i ∈ N , and find a deterministic (optimal) policy for
that agent in the (single-agent) MDP in which the policies of all other agents but i remain fixed. The
important observation is that the resulting profile is again a global maximizer of the potential and
hence, a Nash policy profile. This argument critically relies on the MPG structure (cf. Definition 2)
and does not seem to be directly generalizable to MDPs beyond this class.
Sufficient Conditions for MPGs. Based on the above, it is tempting to think that MDPs which are
potential at every state (i.e., the immediate rewards at every state are captured by a potential game
at that state) are trivially MPGs. As we show in Examples 1 and 2, this intuition fails in the most
straightforward way: we can construct simple MDPs that are potential at every state but which are
purely competitive overall (do not possess a deterministic Nash policy, see Example 1) or which are
cooperative in nature overall, but which do not possess an exact potential function, see Example 2.
Example 1. Consider the MDP in Figure 1. To show that G is not an MPG, it suffices to show that it
cannot have a deterministic Nash policy profile as should be the case according to Theorem 3.1 (see
Appendix B.1). Intuitively, competition arises in Example 1 because the two agents play a game of
matching pennies in terms of the states that they prefer (which can be determined by the actions that
they choose) even though immediate rewards at each state are determined by normal form potential
games. Example 2 shows that a state-based potential game may fail to be an MPG even if agents
have similar preferences over states.
Example 2. Consider the MDP in Figure 2. In s0 the agents play a Battle of the Sexes game and
hence a potential game, while in s1 they receive no reward (which is trivially a potential game). A
simple calculation shows that there is not an exact potential function due to the dependence of the
transitions on agents’ actions (thus, this MDP is not an MPG). However, in the case of Example 2, it
is straightforward to show that the game is an ordinal potential game, cf. Appendix B.1.
The previous discussion focuses on games that consist of normal-form potential games at every state,
which leaves an important question unanswered: are there games which are not potential at every
state but which are captured by the current definition of MPGs? Figure 3 answers this question
affirmatively. Together with Example 1, this settles the claim in Theorem 1.1, part (b).
Proposition 3.2 (Sufficient Conditions for MPGs). Consider an MDP, G, in which every state s ∈ S
is a potential game, i.e., the immediate rewards R(s, a) = (Ri(s, a))i∈N for each state s ∈ S are
captured by the utilities of a potential game with potential function φs. If either C1 or C2 are true,
then G is an MPG.
C1. Agent-independent transitions: P(s0 | s, a) does not depend on a, that is, P(s0 | s, a) = P(s0 |
s) is just a function of the present state for all states s, s0 ∈ S.
C2. Constant gradient of dummy terms: P(s0 | s, a) is arbitrary, but for the decomposition of each
agent’s immediate rewards in common and dummy terms, Ri(s, ai, a-i) = φs(ai, a-i) + uis(a-i)
with uis : A-i → R, it holds that
Vni(S)ET〜π [Xt=0 Ytust (a-i,t) | so = s0] = cs1,
for all states s0, s ∈ S, where cs ∈ R, 1 ∈ RAi, and πi (s) is the policy distribution of agent i at s.
Condition C1 can be viewed as a special case of condition C2; nevertheless, it is more practical to
state it separately. Condition C2 (or variations) have been studied in Marden (2012); Valcarcel Macua
5
Published as a conference paper at ICLR 2022
Figure 3: A 2-player MDP which is not potential at every state but which is overall an MPG. While
state s0 corresponds to a zero-sum game, the states inside the dotted rectangle do form a potential
game which can be used to show the MPG property whenever p0 does not depend on agents’ actions.
et al. (2018). However, Example 2 demonstrates that such conditions are restrictive, in the sense that
they do not capture very simple MDPs in which agents have aligned incentives (cooperative in nature).
Condition C2 is (trivially) satisfied when uis does not depend on the state s nor on the actions of other
agents, i.e., uis(a-i) ≡ ci for some constant ci ∈ R for all a-i ∈ A-i and all s ∈ S. A special case
is provided by MDPs in which the instantaneous rewards of all agents are the same at each state, i.e.,
such that Ri (s, ai, a-i) = φs (ai , a-i) for all agents i ∈ N , all actions ai ∈ Ai and all states s ∈ S.
MDPs that satisfy this condition form a subclass of the current definition and have been studied under
the name Team Markov Games in Wang & Sandholm (2002). These observations motivate the study
of ordinal or weighted MPGs (cf. Definition 3) as more glasses classes of cooperative MDPs.
4 Convergence of Policy Gradient in Markov Potential Games
In the current section, we present the main results regarding convergence of (projected) policy
gradient to approximate Nash policies in Markov Potential Games (MPGs). We analyze the cases of
both infinite and finite samples using direct and α-greedy parameterizations, respectively. All proofs
and auxiliary materials are deferred to Appendix D.
Independent Policy Gradient and Direct Parameterization. We assume that all agents update
their policies independently according to the projected gradient ascent (PGA) or policy gradient
algorithm.3 * The PGA algorithm is given by
πi(t+1) := P∆(Ai)S (∏(t) + ηV∏iVp(∏W ,	(PGA)
for each agent i ∈ N, where P∆(Ai )S is the projection onto ∆(Ai )S in the Euclidean norm. Here,
the additional argument t ≥ 0 denotes time. We also assume that all players i ∈ N use direct
policy parameterizations, i.e., πi (a | s) = xi,s,a, with xi,s,a ≥ 0 for all s ∈ S, a ∈ Ai and
Pa∈A xi,s,a = 1 for all s ∈ S. This parameterization is complete in the sense that any stochastic
policy can be represented in this class Agarwal et al. (2020).
In practice, agents use projected stochastic gradient ascent (PSGA), according to which, the actual
gradient, Rni V^i(∏(t)), is replaced by an estimate thereof that is calculated from a randomly selected
(t)
(yet finite) sample of trajectories of the MDP. This estimate, Rπi may be derived from a single or a
batch of observations which in expectation behave as the actual gradient. We choose the estimator of
the gradient of Vρi to be
Vnti) = R(T,t) XT=0Rlog∏i(akt) I Skt)),	⑵
where s0 〜 ρ, and Ri(T,t) = PkT=0 rik,t is the sum of rewards of agent i for a batch of time horizon T
along the trajectory generated by the stochastic gradient ascent algorithm at its t-th iterate.
The direct parameterization is not sufficient to ensure that the variance of the gradient estimator is
bounded (as policies approach the boundary). In this case, we will require that each agent i ∈ N
uses instead direct parameterization with α-greedy exploration as follows
∏i(a I s) = (1 - α)xi,s,a + α∕Ai,	(3)
3In practice, even though each agent treats their environment as fixed, the environment changes as other
agents update their policies. This makes the analysis of such protocols particularly challenging in general and
highlights the importance of studying classes of MDPs in which convergence can be obtained.
6
Published as a conference paper at ICLR 2022
where α is the exploration parameter for all agents. For α-greedy exploration, it can be shown that
(2) is unbiased and has bounded variance. In this case, 1 - γ captures the probability that the MDP
terminates after each round which ensures that the (finite) length of the trajectory is sampled from a
geometric distribution. This is necessary for unbiasedness, see Zhang et al. (2020); Paternain et al.
(2021) and Lemma 4.3. (PSGA) is given by
∏(t+1) := P∆(Ai)s (∏(t) + nV∏?).	(PSGA)
Main Technical Tools. The first step is to observe that, in MPGs, the (partial) derivatives of
the value functions and the potential function are equal, i.e., VπiVsi(π) = Vπi Φ(π) for all i ∈ N
(Proposition B.1). Together with the separability of the projection operator, i.e., the fact that projecting
independently for each agent i on ∆(Ai)S is the same as jointly projecting on ∆(A)S (Lemma D.1),
this establishes that running PGA or PSGA on each agent’s value function is equivalent to running
PGA or PSGA on the potential function Φ. The next step is to study the stationary points of Φ.
Definition 4 (-Stationary Point of Φ). A policy profile π := (π1, ..., πn) ∈ ∆(A)S is called -
stationary for Φ w.r.t distribution μ as long as max P^∈n δ> Vni Φμ(∏) ≤ G where the max is taken
over all δ = (δ1, . . . , δn) such that (π1 + δ1, . . . , πn + δn) ∈ ∆(A)S, Pi∈N kδik22 ≤ 1. In words,
the function Φ(π) cannot increase in value by more than along every possible local direction δ that
is feasible (namely π + δ is also a policy profile).
To state our main result, we will also need the notion of distribution mismatch coefficient (Kakade &
Langford, 2002) applied to our setting. For any distribution μ ∈ ∆(S) and any policy π ∈ Π, we
call D := max∏∈∏ ∣∣dμ∣∣ the distribution mismatch coefficient, where dμ is the discounted state
distribution (7). Lemma 4.1 suggests that as long as policy gradient reaches a point π(t) with small
gradient along the directions in ∆(A)S, it must be the case that π(t) is an approximate Nash policy.
Lemma 4.1 (Stationarity of Φ implies Nash). Let E ≥ 0, and let π be an E-stationary point of Φ (see
Definition 4). Then, it holds that π is a √SDe(1 — γ)-1 -Nash policy.
Lemma 4.1 will be one of two mains ingredients to establish convergence of PGA and PSGA. To
prove Lemma 4.1, we will use an agent-wise version of the gradient domination property of single-
agent MDPs (cf. Lemma D.3 and Lemma 4.1 in Agarwal et al. (2020)). An important difference in
the gradient domination property between (multi-agent) MPGs and single agent MDPs is that the
value of each agent at different Nash policies may not be unique4 which implies that the gradient
domination property will only be enough to guarantee convergence to one of the optimal (stationary)
points of Φ (and not necessarily to the absolute maximum of Φ).
The second main ingredient will be fact that Φ is a β-smooth function (its gradient is Lipschitz) with
parameter β = 2nγAmax(1 — γ)-3, where Amax := maxi∈N |Ai|, i.e., the maximum number of
actions for some agent. Importantly, this implies that β scales linearly in the number of agents.
Exact Gradients Case. Theorem 1.2 (restated formally below) about rates of convergence of
PGA can now be proved following a standard ascent property of gradient descent algorithms to
approximate stationary points in non-convex optimization Ghadimi & Lan (2013). The Ascent Lemma
(Lemma C.1) suggests that for any β-smooth function, f, it holds that f (χ0) — f (x) ≥ 器 ∣∣χ0 — χ∣∣2,
where x0 is the next iterate of equation PGA. Thus, having shown in our context that Φ is a β-smooth
function, this implies that
φμ(∏(t+1)) - φμ(∏(t)) ≥ 4⅛m3x l|n(t+1)—∏㈤∣∣2
(4)
Putting everything together, we can show the following result.
Theorem 4.2 (Formal Theorem 1.2, part (a)). Let G be an MPG and let Amax = maxi|Ai|. For any
initial state and any initial policies, if all agents run independent projected gradient ascent (PGA)
on their policies with learning rate (step-size) η = 2(1^AY)- for at least T
2nγ Amax
16nγD2SAmaχΦmax
(1-γ)5e2
iterations, then there exists a t ∈ {1, . . . , T } such that π(t) is an E-approximate Nash policy.
4This is in contrast to single-agent MDPs for which the agent has a unique optimal value even though their
optimal policy may not necessarily be unique.
7
Published as a conference paper at ICLR 2022
Finite Sample Case. In the case of finite samples, we analyze (PSGA) on the value V i of each
agent i which (as was the case for PGA) can be shown to be the same as applying projected gradient
ascent on Φ. The key is to get an estimate of the gradient of Φ (2) at every iterate. Note that 1 - γ
now captures the probability for the MDP to terminate after each round (and it does not play the
role of a discounted factor since we consider finite length trajectories). Lemma 4.3 argues that the
estimator of equation equation 2 is both unbiased and bounded.
Lemma 4.3 (Unbiased estimator with bounded variance). It holds that V∏ is an unbiased estimator
of V∏iΦ for all i ∈ N, that is E∏(t) V∏ = Vπ^Φμ(∏(t)), for all i ∈ N. Moreover, for all agents
i ∈ N, it holds that E∏(t) ∣∣ V ∏i) ∣∣ ≤ 2；Amaz4 ,for all i ∈ N.
Using the above, we can now state part (b) of Theorem 1.2. Together with Lemma 4.3 and the
stationarity Lemma (Lemma 4.1), the proof of this part uses the smoothness ofΦ and existing tools
from the analysis of stochastic gradient descent for non-convex functions.
Theorem 4.4 (Formal Theorem 1.2, part (b)). Let G be an MPG. For any initial state and any
initial policies, if all agents run projected stochastic gradient ascent (PSGA) on their policies using
α-greedy parameterization with a = e2 and learning rate (step-size) η = 4羡 D-A? YS for at least
48nD Amax S
T = 48(1-Y)AmaxφmaxD S iterations, then there exists a t ∈ {1,...,T} such that in expectation,
π(t) is an e-approximate Nash policy.
Using Markov’s inequality, it is immediate to show that the statement of Theorem 4.4 also holds
with high probability. Namely, if we set the number of iterations to Tδ = T∕δ4 and and the learning
rate to ηδ = δ2η, where δ ∈ (0, 1), then with probability 1 - δ there exists a t ∈ {1, . . . , T} such
that π(t) is an e-approximate Nash policy. However, this is a weaker than desired statement, since,
optimally, the running time should be logarithmic (instead of polynomial) in 1∕δ (Nemirovski et al.,
2009). Proving such a statement requires bounds on higher moments of the gradient estimator which
in turn, require sampling and averaging over multiple trajectories (per iteration).
Remark 1 (Convergence in WMPGs and OMPGs). It is rather straightforward to see that our results
carry over to WMPGs (cf. Definition 3). The only difference in the running time of PGA is to
account for the weights (which are just multiplicative constants). By contrast, the extension to
OMPGs is not immediate. The reason is that, in that case, we cannot prove any bound on the
smoothness ofΦ and hence, we cannot have rates of convergence of policy gradient. Nevertheless, it
is quite straightforward that PGA converges asymptotically to critical points (in bounded domains)
for differentiable functions. Thus, as long as Φ is differentiable, it is guaranteed that PGA will
asymptotically converge to a critical point ofΦ. By Lemma 4.1, this point will be a Nash policy.
5 Experiments: Congestion Games
We next study the performance of policy gradient in a general class of MDPs that are congestion
games at every state (cf. Roughgarden (2015); Bistritz & Bambos (2020)). As we argued such
MDPs are not necessarily MPGs; however, this class of MDPs contains MPGs, e.g., under additional
conditions on the transitions, or ordinal (weighted) MPGs under more general conditions.
Experimental setup. We consider an experiment
(Figure 4) with N = 8 agents, Ai = 4 facilities
(resources or locations) that the agents can select
from and S = 2 states: a safe state and a distancing
state. In both states, all agents prefer to be in the
same facility with as many other agents as possible
(follow the crowd). In particular, the reward of each
agent for being at facility k is equal to a predefined
positive weight wksafe
times the number of agents
at k = A, B, C, D. The weights satisfy wAsafe <
wBsafe < wCsafe < wDsafe, i.e., facility D is the most
preferable by all agents. However, if more than 4 =
distancing state
4 facilities
> N/2
≤ N/4
safe state
Figure 4: The 2-state MDP.
N/2 agents find themselves in the same facility,
then the game transitions to the distancing state. At the distancing state, the reward structure is the
8
Published as a conference paper at ICLR 2022
same for all agents, but reward of each agent is reduced by a constant amount, c, where c > 0 is
a (considerably large) constant. (We also treat the case in which c is different for each facility in
Appendix E). To return to the safe state, agents need to achieve maximum distribution over facilities;
no more than 2 = N/4 agents in a facility. We consider deterministic transitions; however, the results
are quantitatively equivalent also when these transitions occur with some probability (Appendix E).
Paremeters. We perform episodic updates with T = 20 steps. At each iteration, we estimate the
policy gradients using the average of mini-batches of size 20. We use γ = 0.99 and a common
learning rate η = 0.0001 (larger than the theoretical guarantee, η
(1-γ)3
2YAmaxn
≈ 1e - 08, of
Theorem 4.2). Experiments with randomly generated learning rates for each agent, non-deterministic
transitions between states and different weights at each facility in the distancing state (which result in
non-MPG structure) produce qualitatively equivalent results and are presented in Appendix E.
Results. The left panel of Figure 5 shows that the agents learn the expected Nash profile in both states
in all runs. Importantly, this (Nash) policy profile is deterministic in line with Theorem 4.2. The
panels in the middle and right columns depict the L1-accuracy in the policy space at each iteration
which is defined as the average distance between the current policy and the final policy of all 8 agents,
i∙e∙, LI-accuracy = Nn Pi∈N |ni - πfinall = Nn Pi∈N Ps Pa |ni(a | s) - πfinal(a | S)].
Figure 5: Policy gradient in the 2-state MDP with 8 agents of Section 5. In all runs, the 8 agents
learn one of the deterministic Nash policies that leads to the optimal distribution among states (left).
Individual trajectories of the L1-accuracy and averages (with 1-standard deviation error bars) show
fast convergence in all cases (middle and right columns).
6 Conclusions and Future Directions
We presented positive results about the performance of independent policy gradient in Markov
Potential Games (MPGs), a class of multi-agent MDPs that naturally generalize normal-form potential
games (games in which agents have aligned incentives) to state-based interactions. We found that
MPGs exhibit a richer structure than intuitively expected (e.g., they include MDPs that can be purely
competitive at some states), but retain important regularity properties, most importantly, the existence
of deterministic Nash policies. In our main result, we showed that independent policy gradient with
simultaneous updates converges at a polynomial rate in the approximation error to a Nash policy
profile even in the case of finite samples (in that case, using direct parameterization with greedy
exploration). MPGs are a first step to cover the gap between the extremes of pure coordination
(single-agent or MDPs with identical agents) and pure competition (two-agents, zero-sum MDPs).
Open Questions. Our examples and counter-examples of MPGs suggest that multi-agent coordina-
tion in state-based interactions is still far from understood and constitutes a major direction for future
research in which techniques from cooperative AI (Kao et al., 2021; Mao et al., 2021) or linear MDPs
(Jin et al., 2020) may turn out to be particularly useful. From an algorithmic perspective, a second
direction for future work involves the analysis of policy gradient with different parameterizations
(Agarwal et al., 2020) or of other naturally motivated online learning algorithms (Kleinberg et al.,
2009; Panageas et al., 2018; Mehta et al., 2015) and the adaptation of advanced techniques (Cohen
et al., 2017; Lee et al., 2019) to reproduce convergence and equilibrium selection results from normal-
form to state-based (Markov) games. Alternatively, it would be interesting to test whether complex
behavior (e.g., limit cycles, periodic orbits) of learning dynamics that has been observed in restricted
normal-form settings Mertikopoulos et al. (2018); Vlatakis-Gkaragkounis et al. (2020) also emerges
in the context of exact, weighted, or ordinal MPGs. Finally, turning to efficiency aspects, it would
be interesting to measure the losses due to lack of coordination between agents, e.g., via a Price of
Anarchy type of analysis Koutsoupias & Papadimitriou (1999); Roughgarden & Tardos (2002).
9
Published as a conference paper at ICLR 2022
Acknowledgments
This research/project is supported in part by the National Research Foundation, Singapore un-
der its AI Singapore Program (AISG Award No: AISG2-RP-2020-016), NRF 2018 Fellowship
NRF-NRFF2018-07, NRF2019-NRF-ANR095 ALIAS grant, grant PIE-SGP-AI-2020-01, AME Pro-
grammatic Fund (Grant No. A20H6b0151) from the Agency for Science, Technology and Research
(A*STAR), Provost’s Chair Professorship grant RGEPPV2101 and by EPSRC grant EP/R018472/1.
Reproducibility Statement
We included additional technical materials and complete proofs of all the claims and results of the
main part in the appendix. We also uploaded the code that was used to run the experiments (policy
gradient algorithm) as supplementary material.
References
A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and Approximation with
Policy Gradient Methods in Markov Decision Processes. In J. Abernethy and S. Agarwal (eds.),
Proceedings of 33rd Conference on Learning Theory, volume 125 of PMLR, pp. 64-66, 2020.
URL http://proceedings.mlr.press/v125/agarwal20a.html.
N. Bard, J. N. Foerster, S. Chandar, N. Burch, M. Lanctot, H. F. Song, E. Parisotto, V. Dumoulin,
S. Moitra, E. Hughes, I. Dunning, S. Mourad, H. Larochelle, M. G. Bellemare, and M. Bowling.
The Hanabi challenge: A new frontier for AI research. Artificial Intelligence, 280:103216, 2020.
ISSN 0004-3702. doi: 10.1016/j.artint.2019.103216.
D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition, 2000.
ISBN 1886529094.
I. Bistritz and N. Bambos. Cooperative multi-player bandit optimization. InH. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 2016-2027. Curran Associates, Inc., 2020.
N. Brown and T. Sandholm. Superhuman AI for multiplayer poker. Science, 365(6456):885-890,
2019. ISSN 0036-8075. doi: 10.1126/science.aay2400.
S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3-4):
231-357, 2015.
L.	Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
38(2):156-172, 2008. doi: 10.1109/TSMCC.2007.913919.
M.	Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, P. Abbeel, and A. Dragan. On the Utility of
Learning about Humans for Human-AI Coordination. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 32, 2019.
J. Cohen, A. Heliou, and P Mertikopoulos. Learning with bandit feedback in potential games. In
Proceedings of the 31st International Conference on Neural Information Processing Systems, pp.
6372-6381, 2017.
A. Dafoe, E. Hughes, Y. Bachrach, T. Collins, K. R. McKee, J. Z. Leibo, K. Larson, and T. Graepel.
Open Problems in Cooperative AI. arXiv e-prints, December 2020.
A. Dafoe, Y. Bachrach, G. Hadfield, E. Horvitz, K. Larson, and T. Graepel. Cooperative ai: machines
must learn to find common ground. Nature, 7857:33-36, 2021. doi: 10.1038/d41586-021-01170-0.
C.	Daskalakis, D.J. Foster, and N. Golowich. Independent Policy Gradient Methods for Competitive
Reinforcement Learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5527-5540. Curran
Associates, Inc., 2020.
10
Published as a conference paper at ICLR 2022
D.	Davis and D. Drusvyatskiy. Stochastic subgradient method converges at the rate O(k-1/4) on
weakly convex functions. CoRR, abs/1802.02988, 2018.
R.	Fox, S. McAleer, W. Overman, and I. Panageas. Independent natural policy gradient always
converges in markov potential games, 2021.
S.	Ghadimi and G. Lan. Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic
Programming. SIAM J. Optim.,23(4):2341-2368,2013. doi: 10.1137/120880811.
M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. Garcia Castafieda, C. Beattie, N. C.
Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver,
D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3D multiplayer
games with population-based reinforcement learning. Science, 364(6443):859-865, 2019. doi:
10.1126/science.aau6249.
C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear
function approximation. In J. Abernethy and S. Agarwal (eds.), Proceedings of Thirty Third
Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp.
2137-2143. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/
jin20a.html.
S. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning. In
Proceedings of the Nineteenth International Conference on Machine Learning, ICML ’02, pp.
267-274, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. ISBN 1558608737.
H. Kao, C.-Y. Wei, and V. Subramanian. Decentralized Cooperative Reinforcement Learning with
Hierarchical Information Structure, 2021.
R. Kleinberg, G. Piliouras, and E. Tardos. Multiplicative Updates Outperform Generic No-Regret
Learning in Congestion Games. In ACM Symposium on Theory of Computing (STOC), 2009.
E. Koutsoupias and C. Papadimitriou. Worst-case equilibria. In (STACS), pp. 404-413. Springer-
Verlag, 1999.
J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order methods
almost always avoid strict saddle points. Mathematical programming, 176(1):311-337, 2019.
W. Mao, T. Bayar, L. F. Y., and K. Zhang. Decentralized Cooperative Multi-Agent Reinforcement
Learning with Exploration, 2021.
J.R. Marden. State based potential games. Automatica, 48(12):3075-3088, 2012. ISSN 0005-1098.
doi: 10.1016/j.automatica.2012.08.037.
R. Mehta, I. Panageas, and G. Piliouras. Natural Selection as an Inhibitor of Genetic Diversity:
Multiplicative Weights Updates Algorithm and a Conjecture of Haploid Genetics. In Innovations
in Theoretical Computer Science, 2015.
P. Mertikopoulos, C. Papadimitriou, and P. Piliouras. Cycles in adversarial regularized learning.
In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
2703-2717. SIAM, 2018.
D. Mguni. Stochastic Potential Games, 2021.
D. Mguni, Y. Wu, Y. Du, Y. Yang, Z. Wang, M. Li, Y. Wen, J. Jennings, and J. Wang. Learning in
Nonzero-Sum Stochastic Games with Potentials. arXiv e-prints, art. arXiv:2103.09284, March
2021.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, Feb 2015. ISSN 1476-4687. doi: 10.1038/nature14236.
D. Monderer and L. S. Shapley. Potential Games. Games and Economic Behavior, 14(1):124-143,
1996. ISSN 0899-8256. doi: 10.1006/game.1996.0044.
11
Published as a conference paper at ICLR 2022
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAMJournalonOPtimization,19(4):1574-1609, 2009. doi: 10.1137/
070704277.
OpenAI. Openai five. openai.com, 2018.
I. Panageas, G. Piliouras, and X. Wang. Multiplicative Weights Update as a Distributed Constrained
Optimization Algorithm: Convergence to Second-order Stationary Points Almost Always. In
ICML, 2018.
L. Panait and S. Luke. Cooperative Multi-Agent Learning: The State of the Art. Autonomous Agents
and Multi-Agent Systems, 11(3):387-434, Nov 2005. doi: 10.1007/s10458-005-2631-2.
S.	Paternain, J. A. Bazerque, A. Small, and A. Ribeiro. Stochastic Policy Gradient Ascent in
Reproducing Kernel Hilbert Spaces. IEEE Transactions on Automatic Control, 66(8):3429-3444,
2021. doi: 10.1109/TAC.2020.3029317.
T.	Roughgarden. Intrinsic robustness of the price of anarchy. J. ACM, 62(5), November 2015. ISSN
0004-5411. doi: 10.1145/2806883.
T. Roughgarden and E. Tardos. How bad is selfish routing? Journal of the ACM (JACM), 49(2):
236-259, 2002.
L. S. Shapley. Stochastic Games. PNAS, 1953.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,
I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484-489, Jan 2016.
ISSN 1476-4687. doi: 10.1038/nature16961.
D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Ku-
maran, T. Graepel, T. Lillicrap, K. Simonyan, and D. Hassabis. A general reinforcement learning
algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419):1140-1144,
2018. ISSN 0036-8075. doi: 10.1126/science.aar6404.
R.	S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. A Bradford Book,
Cambridge, MA, USA, 2018. ISBN 0262039249.
S.	Valcarcel Macua, J. Zazo, and S. Zazo. Learning Parametric Closed-Loop Policies for Markov
Potential Games. In International Conference on Learning RePresentations, 2018.
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,
J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden,
Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama,
D. Wunsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. KavukCuoglu, D. Hassabis, C. Apps,
and D. Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature,
575(7782):350-354, Nov 2019. ISSN 1476-4687. doi: 10.1038/s41586-019-1724-z.
E.-V. Vlatakis-Gkaragkounis, L. Flokas, P. Mertikopoulos, and G. Piliouras. No-regret learning and
mixed nash equilibria: They do not mix. In Annual Conference on Neural Information Processing
Systems, 2020.
X. Wang and T. Sandholm. Reinforcement Learning to Play an Optimal Nash Equilibrium in Team
Markov Games. In Proceedings of the 15th International Conference on Neural Information
Processing Systems, NIPS’02, pp. 1603-1610, Cambridge, MA, USA, 2002. MIT Press.
C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Last-iterate Convergence of Decentralized Optimistic
Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games. CoRR, abs/2102.04540,
2021.
K. Zhang, Z. Yang, and T. Bayar. Multi-Agent Reinforcement Learning: A Selective Overview of
Theories and Algorithms. arXiv e-Prints, art. arXiv:1911.10635, 2019.
12
Published as a conference paper at ICLR 2022
K. Zhang, A. Koppel, H. Zhu, and T. BaSar Global Convergence of Policy Gradient Methods to
(Almost) Locally Optimal Policies. SIAMJournal on Control and Optimization, 58(6):3586-3612,
2020. doi: 10.1137/19M1288012.
R. Zhang, Z. Ren, and N. Li. Gradient play in stochastic games: stationary points, convergence, and
sample complexity, 2021.
Y. Zhao, Y. Tian, J. D. Lee, and S. S. Du. Provably Efficient Policy Gradient Methods for Two-Player
Zero-Sum Markov Games. CoRR, abs/2102.08903, 2021.
A Additional Notation and Definitions: Section 2
We first provide some additional notation and definitions that will be used in the proofs.
Q-value and Advantage Functions. Recall from the main part that the value function, Vsi : Π →
R, gives the expected reward of agent i ∈ N when s0 = s and the agents draw their actions,
at = (ai,t, a-i,t), at time t ≥ 0 from policies π = (πi, π-i) and is defined as
Vsi (π) := ET〜π hX∞=0 Ytri,t | s0 = si .
Similarly, We will write Vρi(∏) := Eso〜ρ [Vi(∏)] to denote the expected value of agent i ∈ N under
the initial state distribution ρ.
For any state s ∈ S, the Q-value function Qis : P × A → R and the advantage function Ais :
P × A → R of agent i ∈ N are defined as
Qs(∏, a) ：= ET〜∏ [X∞=0 Ytri,t I so = s, a。= a] , and	(5)
Ais(π,a) := Qis(π, a) - Vsi(π).	(6)
Discounted State Distribution. It will be useful to define the discounted state visitation distribution
dsπ0 (s) for s ∈ S that is induced by a (joint) policy π as
dsπ0 (s) := (1 - γ) Xt∞=0γtPrπ(st = s | s0), for all s ∈ S.	(7)
As for the value function, we will also write d%(s) = Es0〜ρ[d∏0 (s)] to denote the discounted state
visitation distribution when the initial state distribution is ρ.
B Omitted Materials: Section 3
Proposition B.1 (Separability of Value Functions and Equality of Derivatives). Let G = (S, N, A =
{Ai}i∈N, P, R, ρ) be a Markov Potential Game (MPG) with potential Φs, for s ∈ S. Then, for the
value function Vsi, s ∈ S of each agent i ∈ N, the following hold
P1. Separability of Value Functions: there exists a function Usi : ∆(A-i)S → R such that for each
joint policy profile π = (πi, π-i) ∈ ∆(A)S, we have Vsi(π) = Φs (π) + Usi(π-i).
P2. Equality of Derivatives: the partial derivatives of agent i’s value function Vsi coincide with the
partial derivatives of the potential Φs that correspond to agent i’s parameters, i.e., ∂xi,s,a Vsi(π) =
∂xi,s,a Φs(π), for all i ∈ N and all s ∈ S.
Proof. To obtain P1, consider any 3 arbitrary policies for agent i, notated by πi, πi0, πi00 ∈ ∆(Ai)S.
Then, by the definition of MPGs, we have that
Φs(πi,π-i) - Φs(πi0,π-i) = Vsi(πi,π-i) - Vsi(πi0,π-i),
Φs(πi,π-i) - Φs(πi00,π-i) = Vsi(πi,π-i) - Vsi(πi00,π-i).
13
Published as a conference paper at ICLR 2022
for every starting state s ∈ S. This implies that we can write Vsi (πi, π-i) as both
Vsi(πi,π-i) = Φs(πi, π-i) - Φs(πi0,π-i) + Vsi(πi0,π-i)
Vsi(πi,π-i) = Φs(πi,π-i) - Φs(πi00,π-i) + Vsi(πi00,π-i)
Thus, we have that -Φs(πi0, π-i) + Vsi(πi0, π-i) = -Φs(πi00, π-i) + Vsi(πi00, π-i) for any arbitrary
pair of policies πi0 and πi00 for agent i, implying that agent i’s policy has no impact on these terms.
Accordingly, we can express them as
Usi(π-i) := -Φs(πi0,π-i) + Vsi(πi0,π-i) = -Φs(πi00,π-i) + Vsi(πi00,π-i),
where Usi(π-i) is a function that does not depend on the policy of agent i. Thus, we can express the
utility function of any agent i in an MPG as
Vsi(π) = Φs (π) + Usi (π-i).
as claimed. To obtain P2, we use P1 for a vector xi parameterizing πi, and obtain that
∂xi,aVi(π)=∂xi,aΦ(π)+0
for any coordinate Xi,a with a ∈ Ai of Xi, from which We can see that our claim is true.	□
Note that P1 serves as a characterization of MPGs. Namely, a multi-agent MDP is an MPG if and
only if the value function of each agent i ∈ N can be decomposed in a term that is common for
all players (potential function) and in a term that may be different for each agent i ∈ N but which
depends only on the actions of all agents other than i. This property carries over from normal form
(single state) potential games. Also note that both properties, P1 and P2, hold for any (differentiable
for P2) policy parameterization and not only for the direct one that we use here.
Proof of Theorem 3.1. Let Φ be the potential function of G. Since the space ∆(A)S = ∆(A1)S ×
...X ∆(An)S is compact and Φ is continuous, Φ has a global maximum Φmaχ. Let (n；,…,∏n)
denote a global maximizer, i.e., a joint policy profile at which Φmax is attained. By the Definitions of
MPGS and Nash policies, this implies, in particular, that (n；,…,∏ζ) is a Nash policy, since
0 < Φs(π*,∏-i) - Φs(∏i, ∏-i) = Vi(πT,∏-i) - Vi(∏i,∏-i),	(*)
for all i ∈ N, s ∈ S and all policies ∏ ∈ ∆(Ai)S. If n；,…，∏ζ are all deterministic we are done.
So, we may assume that there exists an i ∈ N so that ∏i is randomized and consider the MDP G0
in which the policy of all agents other than i has been fixed to ∏-%. G0 is a single agent MDP with
the same states as G, the same actions and rewards for agent i and transition probabilities that are
determined by the joint distribution of the environment and the joint policy of all agents other than i.
As a single agent MDP, this setting has a deterministic optimal policy, say ∏i, for agent i. Thus, it
holds that
Vi(∏i,∏-i) ≤ Vi(∏t,π-i) ≤ Vi(∏i,π-i),
where the first inequality follows from the fact that n； is a Nash policy and the second from the
optimality of ∏. It follows that
Vi (∏i,∏-i) = vsi(∏*,∏-i),
i.e., the payoff of agent i at (∏i, ∏-J is the same as in (n；,n-J. Hence, by the definition of the
potential function, we have that
0 = Vi(∏i,∏-i) - Vi(∏T,∏-i) = Φs(∏i,∏-i) - Φs(∏*,∏-i),
which implies that
φs (πi, π-i) = φs (n； , π-i) = φmaχ.
Thus, (∏i, ∏-i) is also a global maximizer of Φ which implies that (∏i, ∏- J is a Nash policy by the
same reasoning as in equation equation *. Note that the value of all players other than i may not be
the same at the joint policy profile (πi,π-i)as it is in (∏*,∏-J. However, what we need for our
purpose is that this step reduces the number of randomized policies by one and that it retains the
value of the potential function invariant at its global maximum (which ensures that the ensuing policy
profile is also a Nash policy). By iterating this process until ∏j becomes deterministic for all agents
j ∈ N, we obtain the claim.	□
14
Published as a conference paper at ICLR 2022
Proof of Proposition 3.2. The proof is constructive and proceeds by finding the potential function,
Φs, s ∈ S in both cases, C1-C2. Since the individual rewards of the agents at each state s ∈ S are
captured by a potential function φs, then for the reward, Ri(s, a) of each agent i at the action profile
a, it holds that
Ri (s, a) = φs (a) + uis (a-i),	(8)
where uis : ∆(A-i) → R is a function that does not depend on the actions of agent i in any way.
Thus, we may write the value function of each agent i ∈ N as
Vi(π) = ET〜∏ [X—0 YRi(st, at) | S0 = Si
=Eτ~π [Xt=0 Yt (φst (at) + ust (a-i,t)) | s0 = s
=Eτ~π [Xt=0 Yφst (at) | s0 = s] + Eτ~π [Xt=0 γtust (a-i,t) | s0 = s]	(?)
where T 〜∏ is the random trajectory generated by policy ∏. To show that G is an MPG, it suffices
to show that the value function of each agent i ∈ N can be decomposed in a term that is common
for all agents (and which may depend on the actions of agent i ∈ N) and in a term that does not
depend (in any way) in the actions of agent i (dummy term), cf. Proposition B.1. The first term in
expression equation ?, i.e., ET〜∏ [P―=0 γtφst (at) | so = s], depends on the actions of all players
and is common for all agents i ∈ N (and is thus, a good candidate for the potential function). The
second term in expression equation ?, i.e., ET〜∏ [P—=o YtuSt (a-i,t) I so = s], does not depend on
player i via the payoffs uis (a-i,t), but, in general, it does depend on player i via the transitions,
τ 〜∏. The two cases in the statement of Proposition 3.2 ensure precisely that this term is independent
of the policy of agent i, in which case it is a dummy term for agent i or that it is also common for all
players and hence, that it can be included in the potential function.
Condition C1. If the transitions do not depend on the action of the players, we have that T 〜P,
where P is an exogenously given distribution function (state-wise). In this case, we have that
Φs(∏) := ET〜π
—
YtφSt (at) | so = s
t=o
is a potential function and Ui(π-i) := ET〜∏ [P—=o YtuSt (a-i,t) I so = s] is a dummy term that
does not depend (in any way) on the policy of agent i ∈ N which proves the claim.
Condition C2. We will show again that
—
φs0 (π) := ET〜π X : Ytφst (a) | s0 = S
t=o
is a potential function for G and that the same decomposition as in condition C1 of the value function
of agent i in a common and a dummy term applies. To see this, let πi , πi0 ∈ Πi be two policies of
agent i and let π = (πi, π-i), π0 = (πi0, π-i) where π-i is the fixed policy of all agents other than i.
Then, using equation ?, we have that
VSi0(π)-VSi0(π0)
—
=φs0 (π) - φs0 (π ) + ET〜π	X : YtUSt (a-i,t) | s0 = S
t=o
ET 〜π0
—
X YtuSt (a-i,t) I so = s
t=o
—
The intermediate value theorem implies that there exists a policy ξi which is a convex combination of
πi , πi0 such that
—
ET〜π)： Y ust(a-i,t) | s0 = S
t=o
ET 〜π0
—
X YtuiSt (a-i,t) I so = s0
t=o
—
—
E-Tni)τv∏iET〜(ξi,∏-i)	EYtuSt(a-i,t) | so = s0
t=o
15
Published as a conference paper at ICLR 2022
The displayed condition in C2 implies that
∞
▽ni Eτ~(ξi,∏-i)〉： Yus (a-i,t) | so = S =(CSI)s∈S,
t=0
which is enough to ensure that the dot product in the previous equation is equal to 0 since πi , πi0
correspond to probability distributions at every state s ∈ S, and the sum of their component-wise
differences is equal to 0. This proves the claim.
Summing up, in both cases, C1-C2, G is an MPG as claimed.
□
Note that the proof of the (trivial) case in which the instantaneous rewards of all agents i ∈ N are
equal at each state s ∈ S is similar. In this case, it is immediate to see that the instantaneous rewards
are precisely given by the potential function at that state, i.e., it holds that Ri (s, a) = φs(a) for all
i ∈ N and all s ∈ S. In this case, it holds that uis(a-i) ≡ 0 for all i ∈ N and all s ∈ S and hence,
∞
φs(π) := ET〜π X : Yφst (a) | s0 = s
t=0
is a potential function for G, and the dummy terms are all equal to 0, i.e., Usi (π-i) ≡ 0.
B.1 Examples
Example 1 (Continued). We prove the claim that G cannot have a deterministic Nash policy profile.
To obtain a contradiction, assume that agent A is using a deterministic action a0A ∈ {0, 1} at state
0. Then, agent B, who prefers to move to state 1, will optimize their utility by choosing the action
a0B ∈ {0, 1} that yields aA ㊉ aB = 1. In other words, given any deterministic action of agent A at
state 0, agent B can choose an action that always moves the sequence of play to state 1. Thus, such
an action cannot be optimal for agent A which implies that the MDP does not have a deterministic
optimal policy profile as claimed.
Example 2 (Continued). At each state, s ∈ {0, 1}, the agents’ payoffs, (Rs1, Rs2), form a potential
game (at that state), and are given
as follows
0
1
5, 2
-5, -4
-1, -2	4 0
,	, with potential Φ0 = -6	2
State 0 : (R01, R20) = 0
State 1 : (R11, R21) = (0, 0), with potential Φ1 = 0.
In this MDP, agents need only to select an action at state s0. Thus, we will denote a policy, π1, of
agent 1 by π1 = (p, 1 - p) where p ∈ [0, 1] is the probability with which agent A selects action 0 at
state s0. Similarly, we will denote a policy, π2, of agent B by π2 = (q, 1 - q) where q ∈ [0, 1] is the
probability with which agent B selects action 0 at state s0 . Moreover, we will slightly abuse notation
and write
R0i (π) = R0i(π1,π2) = π1>R0iπ2 = [p, 1 - p]Ri0[q, 1 - q]>.
We also assume that the horizon is infinite and there is a discount factor γ∈ [0, 1). Accordingly, we
can calculate the value functions V0i(π1, π2) of agents i = A, B starting from state s0 as follows,
V0i(π) = R0i (π) + γpqV0i(π) - γ(1 -pq) × 0
which yields the solution
V0i(π) = R0(π) , fori = A,B.
0	1 - γpq
Next, we use the Performance Difference Lemma (Lemma 3.2 by Agarwal et al. (2020)) to determine
the difference in the value between two different policies. We will do this for agent 1 (the calculation
16
Published as a conference paper at ICLR 2022
is similar for agent 2: we use here 1 for agent 1 and 2 for agent B). For a policy π = (π1,π2), we
have at state s0 that
Ea〜∏ι(∙∣so)[A1(∏0,a)] = pA1(∏0,。,。2)+ (1 -p)A1(π0, I,a2)
= p R10(0,π2) + γqV0(π0) - V0 (π0) + (1 -p) R01(1,π2) + 0 - V0(π0)
pR01(0,π2) + (1 - p)R01 (0, π2) - (1 - γpq)V0(π0)
= R01 (π) - (1 - Ypq)V0(π0).
At state s1, there is only one available action for each agent which yields a payoff of 0. Thus,
Ea〜∏ι(∙∣sι)[Al(∏0, a)]=0.
Moreover, concerning the discounted visitation distribution, we have that
∞
d0π(s0) = (1 -γ)	γtPrπ(st
s0 | s0) = (1 - γ) 1 + γpq + (γpq)2 + . . . =
t=0
and d0π (s1) = 1 - d0π (s0)
1 - Y
1 - γpq,
Y(I-Pq)
V0(π) - V0(π0)
1-γpq .
1
Thus, using all the above, we have that
1 一
1 - Y --
R0⑺
1 - Ypq
1 -⅛ ∙Mπ)-(I-Ypq)V0(n0)) +
- V0(π0) = V0(π) - V0(π0).
Y(I -Pq) ∙ 0
1 - Ypq .
which shows that our initial calculations conform with the outcome specified by the Performance
Difference Lemma.
Finally, a direct calculation shows that Φs = φs for s = 0, 1 is a valid potential function for which
the MDP is an ordinal MPG.
Example 3 (Continued). At state s0, we consider the game with action sets A1(s0) = A2(s0) =
{H, T } and (instantaneous) payoffs
H
R1 (s0 , a1 , a2 ) = T -1
TH
-11	and R2(s0,a1,a2) = TH -11
T
1
-1
where a1 denotes the action of agent 1 and a2 the action of agent 2 (agent 1 selects rows and agent 2
selects columns in both matrices). This is a constant sum game (equivalent to zero-sum) and hence, it
is not an (ordinal) potential game. Apart from the instantaneous rewards, agents’ actions at s0 induce
a deterministic transition to a state in which the only available actions to the agents are precisely the
actions that they chose at state s0 and their instantaneous rewards at this state are the rewards of the
other agent at s0 . In particular, there are four possible transitions to states sab with a, b ∈ {H, T },
with action sets and instantaneous rewards given by
A1 (sab) = {a}, A2 (sab) = {b}, R1 (sab, a, b) = R2(s0, b, a), R2(sab, a, b) = R1(s0, b, a),
for agents 1 and 2, respectively. Note that the visitation probability of this states is equal to the
visitation probability of state s0 . After visiting one of these states, the MDP transitions to state s1
which is a potential game, with potential function given by
L
L4
Φ1 = R 3
R
3
0
As mentioned above, the game in state s0 does not admit a potential function. However, the joined
rewards RJ1, RJ2 of agents 1 and 2 which result from selecting an action profile (a, b) ∈ H, T2 at
s0 and then traversing both s0 and the ensuing sab (part included in the dotted rectangle in Figure 3),
do admit a potential function. The potential function in this case is the sum of agents’ rewards and is
given by
H
H 1-1
Φ0ab = T 1 - 1
TH
1-1 H 0
1-1	=T 0
T
0
0
17
Published as a conference paper at ICLR 2022
Let π1 = (x0, x1) denote a policy of agent 1. Here x0 = (x0, 1 - x0), where x0 ∈ [0, 1] is
the probability with which agent 1 chooses action H at state s0. Similarly, x1 = (x1 , 1 - x1)
where x1 ∈ [0, 1] is the probability with which agent 1 chooses action L at state s1. At states
sab , a, b ∈ {H, T }2, agents only have one action to choose from, so this choice is eliminated from
their policy representation. Similarly, we represent a policy of agent 2 by π2 = (y0, y1) with
y0,y1 ∈ [0, 1]. Let also
p0 :=	p0 (π1,	π2)	:= Pr(st+1 =	s0	|	st	=	s1, π1,π2),	(9)
In the general case, p0, i.e., the transition probability from s1 to s0, may depend on the actions of the
agents or it may be completely exogenous (i.e., constant with respect to agents’ actions). If we write
p0(a1, a2) := Pr(st+1	=	s0	|	st	=	s1, a1,	a2),	for a1, a2	∈	{L, R},
to denote the probability of transitioning from state s1 to state s0 given that the agents chose actions
a1, a2 ∈ {L, R} at state s1, then we can write p0 as
P0 = E(aι ,a2)〜(ni,n2)[P0(a1,a2)] = E	Pr(a1,a2 | π1,π2) ∙ P0(a1,a2)
(a1,a2)∈{L,R}2
= x1y1p0(L, L) + x1(1 - y1)p0(L, R) + (1 - x1)y1p0(R, L) + (1 - x1)(1 - y1)p0(R, R).
(10)
Using this notation, we can now proceed to compute the value function of each state of the MDP in
Figure 3. Since the value of states sa,b, a, b ∈ H, T2 is equal to a constant reward plus the value of
state s1 (discounted by γ), it suffices to calculate the value for states s0 and s1. We have that
V01 (∏ι, ∏2) = χoR1yo + Y (χ0 (R2∕γ) y°) + γ2V1 (∏ι, ∏2)
V	11 (π1,π2) = x1R11y1 +γ p0V01 (π1,π2) + (1 -p0)V11 (π1,π2) ,
which after some trivial calculations yield
V	01 (∏ι, ∏2) = χo (R1 + R1) yo + γ2V1 (∏ι, ∏2)
V-------------1 (∏1,∏2) = --1-V [χιR1yι + γp0V01 (∏1,∏2)].
1	1 - γ (1 -p0)	1	0
This is a system of 2 equations in the 2 unknown quantities, V01 (π1, π2) and V11 (π1, π2). Solving
for these two quantities, yields the unique solution
VO1 (πι,π2) =  -7：,————[(I - γ(I -PO)) χ0 (RI + R2) yo + γ2χιR1yι].
0	1 - γ (1 -p0) - γ3p0	0	0	1
VI (π1,0 = 1- Y (I-10)-γ3p0 [Yp0xO (R1 + R2) yO + x1R1yl].
In the case that p0 is a constant with respect to π1, π2, then both value functions are of the form
Vi (∏1,∏2) = Cl (s) ∙ Xo (R1 + R2) yo + C2 (s) ∙ XiRlyι, for S ∈ {so, sι}, and i = {1, 2},
where c1 (s) , c2 (s) > 0 are appropriate constants that depend only the state s ∈ {s0, s1} and on
agents 1, 2. Since the game at s1 is a potential game, with potential function given by a 2 × 2 matrix
Φ1 , it is immediate to infer that
Vsi (π10,π2) - Vsi (π1,π2) = Φs (π10,π2) - Φs (π1,π2) ,for s ∈ {s0, s1},
with
Φs (∏1,∏2) := Ci (s) Xo (R1 + R2) yo + C2 (s) ∙ XiΦiyi, for S ∈ {so, Si}.
However, if p0 depends on the actual policies of agents 1 and 2, cf. equation equation 9, then it is not
immediate to determine a potential (or even to decide whether a (exact) potential exists or not).
Remark 2. Several elements of Figure 3 have been selected in the sake of simplicity and are not
necessary for the main takeaway, i.e., that there are MDP that are not potential at some states, but
which are MPGs. First, the transitions from so to the states sab need not be deterministic. To see this,
let q ∈ (0, 1) and assume that if the agents select actions H, T is so, then the process transitions with
probability q to a state sHT with rewards (-1, 1)∕qY and with probability (1 - q) to a state s0HT
18
Published as a conference paper at ICLR 2022
with rewards (1, -1)/(1 - q)γ. The rest remains the same. Accordingly, the expected reward for
agent 1 after (H, T ) has been selected in s0 is the same as in the current format.
Second, the construction with states s0 and sab, (a, b) ∈ H, T 2 is not the only one that leads to such
an example. Another very common instance occurs in the case of aliasing between s0 and states sab,
i.e., when the agents cannot tell these states apart. The intuition which carries over from the currently
presented example is that the roles of the agents are essentially reversed between the two states, but
the agents do not know (from the observable features) in which state they are. Thus, any valid policy,
selects the same action in both states leading to the same situation as in the presented example.
Finally, if the horizon is finite, then the instantaneous rewards in states sab still work if we eliminate
the scaling factor (here γ). Thus, the construction works in both episodic and continuing settings.
C Auxiliary Lemmas
Before proceeding to the omitted materials of Section 4, we first include some auxiliary results that
we will use in the proofs presented there.
Recall that PX denotes the projection onto some set X .
Lemma C.1 (Bubeck (2015), Lemma 3.6). Let f be a β-smooth function5 with convex domain X.
Let X ∈ X, x+ = PX (X — 1 Nf(X)) and gχ (x) = β(x — x+). Then the following holds true:
f (X+) - f (X) ≤ -21β kgχ(X)k2.
Lemma C.2 (Agarwal et al. (2020), Proposition B.1). Let f(π) be a β-smooth function in π ∈
∆(A)S. Define the gradient mapping
G(π) = β (P∆(A)S π+ + 1 Vnf (π)) - π)
and the update rulefor the projected gradient is π0 = π + 1 G(π). If ∣∣G(∏)∣∣2 ≤ G then
max	δ>Vπf(π0) ≤ 2.
π+δ∈∆(A)S,kδk2≤1
Lemma C.3. Let Φμ(∏) be the potential function (Which is β-smooth) and assume π ∈ ∆(A)S uses
α-greedy parameterization. Define the gradient mapping
G(π) = β (P∆(A)S π+ 十 1 vφJπ)) - π)
and the update rulefor the projected gradient is π0 = π + 1 G(π). If ∣∣G(∏)∣∣2 ≤ G then
max	δ>VΦμ(π0) ≤ 2 + VZnaAmaχ.
π+δ∈∆(A)S,kδk2≤1
Proof. It is a direct application of Lemma C.2 and the fact that Φμ is LiPschitz with parameter
√nAm)2x (this is Proposition 3 in page 23 OfDaskalakis et al. (2020)).	□
Lemma C.4 (Unbiased with bounded variance Daskalakis et al. (2020)). It holds that V∏ is unbiased
estimator of VniVi forall i, that is E∏(t) V∏? = VniVi(∏(t)) for all i. Moreover, for all agents
i we
get that (this is what the authors actually prove), it holds that Eπ(t)
24Amax
α(1-γ)4 .
D Omitted Materials: Section 4
Before we proceed with the formal statements and proofs of this section, we recall the commonly
used definition of distribution mismatch coefficient Kakade & Langford (2002) applied to our setting.
Definition 5 (Distribution Mismatch coefficient). For any distribution μ ∈ ∆(S) and any policy
π ∈ Π, we call D := max∏∈∏ ∣∣dμ ∣∣ the distribution mismatch coefficient, where dμ is the
discounted state distribution (7).
5Differentiable with Vf to be β-Lipschitz.
19
Published as a conference paper at ICLR 2022
D.1 Exact Gradients Case
The first auxiliary Lemma has to do with the projection operator that is used on top of the independent
policy gradient, so that the policy vector πi(t) remains a probability distribution for all agents i ∈ N
(see (PGA)). It is not hard to show (due to separability) that the projection operator being applied
independently for each agent i on ∆(Ai)S is the same as jointly applying projection on ∆(A)S. This
is the statement of Lemma D.1.
Lemma D.1 (Projection Operator). Let π := (π1, ..., πn) be the policy profile for all agents and let
π0 = π + ηV∏Φρ (π) be a gradient step on the PotentialfUnctionfor a step-size α > 0. Then,
P∆(A)S (π ) = (P∆(A1 )S (π1 ), . . . , P∆(An)S (πn )).
Proof of Lemma D.1. Observe that PX (y) = arg minx∈X kx - yk22 for any set X ⊆ Rn. Thus,
n
P∆(A)S (y) = arg min kx - π0k22 =	arg min	kxi - πi0k22
x∈∆(A)S	x1∈∆(A1)S,...,xn∈∆(An)S i=1
n
=E argmin kxi — π0k2 = (P∆(AιMπ1),..., P∆(An)S (π'n)).	□
i=1 xi∈∆(Ai)S
The main implication of Lemma D.1 along with the equality of the derivatives between value functions
and the potential function in MPGs, i.e., Vπi Vsi(π) = VπiΦ(π) for all i ∈ N (see property P2
in Proposition B.1), is that running independent equation PGA on each agent’s value function is
equivalent to running equation PGA on the potential function Φ. In turn, Lemma 4.1 suggests that
as long as policy gradient reaches a point π(t) with small gradient along the directions in ∆(A)S,
it must be the case that π(t) is an approximate Nash policy. Together with Lemma D.1, this will be
sufficient to prove convergence of equation PGA.
ProofofLemma 4.1. Fix agent i and suppose that i deviates to an optimal policy ∏ (w.r.t the
corresponding single agent MDP). Since π is -stationary it holds that (Definition 4)
max (∏' — ∏i)>V∏iΦμ(∏) ≤ √Se.	(11)
πi0 ∈∆(Ai)S	i
Thus, with π = (πi , π-i ), Lemma D.3 implies that
φρ(π*)-φρ(π) ≤ T-Y
d∏*
μ
z maX )(∏' — ∏)>V∏iΦμ(∏) (≤) ID-√Se.
∞i
(12)
Thus, using the definition of the potential function (cf. Definition 2), we obtain that
Vi(∏*)- Vi (∏) = Φρ(∏*)- Φρ(∏) ≤
SDDe
I- Y
Since the choice of i was arbitrary, We conclude that π is an 金华-approximate Nash policy.	□
To prove Lemma D.3, we will use a multi-agent version of the Performance Difference Lemma (cf.
Agarwal et al. (2020) for a single agent and Daskalakis et al. (2020) for two agents).
Lemma D.2 (Multi-agent Performance Difference Lemma). Consider an n-agent MDP G and fix
an agent i ∈ N. Then, for any policies π = (πi, π-i), π' = (πi', π-i) ∈ Π and any distribution
ρ ∈ ∆(S), it holds that
—
VPln) = i--γ Esf Eai〜∏i(∙∣s) Ea-i〜∏-i(∙∣s) [AS (π',ai, a-i)]
where a-〜∏-i(∙ | S) denotes the action profile of all agents other than i that is drawnfrom the
product distribution induced by their policies π-i = (πj)j6=i∈N ∈ Π-i.
20
Published as a conference paper at ICLR 2022
Proof. For any initial state s ∈ S and joint policies π = (πi , π-i), π0 = (πi0 , π-i) ∈ Π, it holds that
Vi(π) - Vi(π0) = Ei∏ [X∞=0 γtri,t | S0 = Si - Vi(∏0)
=ET〜π [X∞=0 Yt (ri,t - Vst (π) + Vst (nO)) | s0 = Si- Vi(π0)
=ET〜∏ [X∞=0 Yt (ri,t - Vst (π) + YVst+1 (nO)) | so = si
=ET〜∏ [Xt=0 Yt (ri,t + YE [vst+ι (π) | st,ai,t, a-i,t] - vst(nO)) | so = s]
=ET〜π [Xt=0 YtAst (π0, at) | so = si
=1-----Y ESO〜dπ Eai〜∏i(∙∣s0) Ea-i〜∏-i(∙∣s0) [Ais0 (πO, a)] .
Taking expectation over the states s ∈ S with respect to the distribution ρ ∈ ∆(S) yields the
result.	□
To prove Lemma 4.1, we will need an agent-wise version of the gradient domination property of
single-agent MDPs (cf. Lemma 4.1 in Agarwal et al. (2020)). This is presented in Lemma D.3.
Lemma D.3 (Agent-wise Gradient Domination Property in MPGs). Let G be an MPG with potential
function Φ ,fix any agent i ∈ N, and let π = (∏, π-i) ∈ ∆(A)S be a policy. Let π* be an optimal
policy for agent i in the single agent MDP in which the rest of the agents are fixed to choose π-i .
Then, for the policy π* = (π*,π-i) ∈ ∆(A)S that differs from π only in the policy component of
agent i, andfor any distributions μ, ρ ∈ ∆(S), it holds that
φρ(π*) - φρ(π) ≤
∞∏o=maφμ(π),
1
1 - Y
ProofofLemma D.3. Fix an agent i ∈ N and let π = (∏i, π-i),π* = (∏*,∏-i) ∈ Π = ∆(A)S.
By the definition of MPGs (cf. Definition 2), it holds that
vi(∏*)- vi(∏) = Φρ(∏*) - Φρ(∏).
Thus, using the multi-agent version of the Performance Difference Lemma (cf. Lemma D.2), we have
for any distribution μ ∈ ∆(S) that
Φρ(π*)- Φρ(π) = v>*)- Vi(∏)
_	1
1 - Y
1
≤ -----
ES〜岑* Eai~π*(∙∣s) Ea-i~π-i(∙∣s) [As (π, ai,t, a—i,t)]
ma
1 - Y πi0∈Πi
1
≤------
一I - Y
To proceed, observe that
{χ d∏* (S) Eai〜∏i(∙∣s) Ea-i〜π-i(∙∣s) [As(π, ai, a-i)] । ,
dρπ* (s) π	i
d∏ (s) dμ(S) Eai〜∏i(∙∣s) Ea-i〜∏-i(∙∣s) [As(π, ai, a-i)]卜
max {)： dμ(s) Eai ~π0 (∙∣s) Ea-i~π-i(∙∣s) [As (π, ai, a—i)] } ∙
1
------max
1 - Y πi0 ∈Πi
_ *
d
Eai~∏i(∙∣s) Ea-i~π-i(∙∣s) [As(π,ai, a—i)] = 0.
Thus, for any πiO ∈ Πi and any state s ∈ S, it holds that
Eai〜π0(TS) Ea-i〜π-i(∙∣s) [As (π, ai, a—i)]=
(πiO(ai | s) - πi (ai | s)) E
ai∈Ai
(πiO(ai | s) - πi (ai | s)) E
ai∈Ai
，a-i〜π-i(∙∣s) [As (π, ai, a—i)]
^a-i〜π-i(∙∣s) [Qs(π,ai, a—i)]
21
Published as a conference paper at ICLR 2022
since Vsi(π) does not depend on ai. Substituting back in the last inequality of the previous calculations,
we obtain that
Φρ(∏*)- Φρ (∏) ≤
≤
max
πi0 ∈Πi
∞
max (πi0
πi0 ∈Πi
∞
(∏i(ai | S) - ∏i(ai | S)) Ea—〜∏-式.|§)
-∏i)>v∏i vμ(∏),
Qis (π, ai
-i)
a
where we used the policy gradient theorem (Agarwal et al. (2020); Sutton & Barto (2018)) under
the assumption of direct policy parameterization. We can further upper bound the last expression by
using that dμ(s) ≥ (1 - γ)μ(s) which follows immediately from the definition of the discounted
visitation distribution dμ(s) for any initial state distribution μ. Finally, property P2 of Proposition
B.1, implies that Rni Vi(π) = RniΦρ(∏) (making crucial use of the MPG structure). Putting these
together, we have that
φρ(π*) - φρ(π) ≤ ；——
1-γ
_	1
1 - Y
d∏*
max
n0=(n0 ,n-i)
∞
max (π0
n0=(n0 ,n-i)
∞
μ
μ
as claimed.
□
Remark 3. Intuitively, Lemma D.3 implies that there is a best response structure in the agents’ updates
that we can exploit to show convergence of (projected) policy gradient to a Nash policy profile. In
particular, given a fixed policy profile of all agents other than i, the decision of agent i is equivalent
to the decision of that agent in a single MDP. Thus, the following inequality
Vi(π*)- Vi(π) ≤
_ *
d∏
μ
∞ ∏o=maUπ'-"vnivμ (π)
1
1 - Y
(which stems directly from the gradient domination property in the single MDP) implies that any
stationary point of Vsi (w.r.t the variables xi,s,a of agent’s i policy with the rest of the variables being
fixed) is an optimal policy for i, i.e., a best response given the policies of all other agents.
Lemma D.3 also suggests that there is an important difference in the gradient domination property
between (multi-agent) MPGs and single agent MDPs. Specifically, for MPGs, the value of each agent
at different Nash policies may not be unique6 which implies that the gradient domination property, as
stated in Lemma D.3, will only be enough to guarantee convergence to one of the optimal (stationary)
points of Φ (and not necessarily to the absolute maximum of Φ).
The last critical step before the formal statement and proof of Theorem 1.2 is to show that the potential
function Φ is smooth. Importantly, the smoothness parameter, involves Amax := maxi∈N |Ai|, i.e.,
the maximum number of actions for some agent, which scales linearly in the number of agents.
Lemma D.4 (Smoothness of Φ). Let Amax := maxi∈N |Ai| (the maximum number of actions for
some agent). Then, for any initial state s° ∈ S (and hence for every distribution μ ∈ ∆(S) on states),
Φμ(∏) is 2nYAmax -smooth, i.e., it holds that
kv∏ Φso(∏) - v∏ Φs0(∏0)∣∣2 ≤ 2nγAmax k∏ - ∏0k2	(13)
2	(1 - Y)3	2
Proof of Lemma D.4. It suffices to show that the maximum eigenvalue in absolute value of the
Hessian of Φ is at most Nn-Amax, i.e., that
V 2nγAmax
≤ (1-γ)3 .
6This is in contrast to single-agent MDPs for which the agent has a unique optimal value even though their
optimal policy may not necessarily be unique.
22
Published as a conference paper at ICLR 2022
We first prove the following intermediate claim.
Claim D.5. Consider the symmetric block matrix C with n × n matrices so that kCij k2 ≤ L. Then,
it holds that kCk2 ≤ nL, i.e., if all block submatrices have spectral norm at most L, then C has
spectral norm at most nL.
Proof. We will prove the claim by induction on n. For n = 2 we need to show that
kCk2:=
CCi2ii	CCi222	2 ≤2L
if kC11k2 , kC12k2 , kC21k2 , kC22k2 ≤ L. Define matrix W tobe
W ：=2L ∙ I - C =( 2L ∙ IC- C11	-Ci2
-	-C2i 2L ∙ I — C22
where I is the identity matrix (of appropriate size). If we show that W is positive semi-definite, then
it follows that W has only non-negative eigenvalues, which, in turn, implies that the spectral norm of
C is at most 2L. To see this, set
Lr (L ∙ I — Cii	0 ʌ ττr
WI=	0	L ∙ I - C22	，W2
L ∙ I	-Ci2
-C21	L ∙ I
Wi is positive semi-definite as a block diagonal matrix with diagonal blocks positive semi-definite
matrices. Moreover, by Schur complement we get that W2 is positive semi-definite as long as L ∙ I is
positive semi-definite and L ∙ I - L ∙ Ci2C2i is positive semidefinite. By assumption, we have that
了 ∣∣Ci2C2i∣∣2 ≤ kC ∣∣Ci2∣∣2 ∣∣Ci2k2 ≤ L,
LL
which implies that L ∙ I - L ∙ Ci2C2i has non-negative eigenvalues. Thus, W2 is positive semi-definite.
We conclude that Wi + W2 is positive semi-definite (sum of positive semi-definite matrices is positive
semi-definite) and the claim follows.
For the induction step, suppose that the claim holds for an n = k - 1 ≥ 2. To establish that it also
holds for k, we need to show that
Cii	Ci2	.	.Cik ∖
C2i .	C22	. .	.C2k ..
. . Cki	. . Ck2	.	.. .. . . Ckk
≤ kL
kCk2:
2
as long as ∣∣Cij ∣∣2 ≤ L for all i, j. Let W = kL ∙ I - C. To show that W is positive semi-definite
consider
W1 :
/
∖
kL ∙ I - Cii	-Ci2	-Ci3	.	.	-Cik ∖
-C2i .	L ∙ I .	0. ..	.	0
. . -Cki	. . 0	.. .. 0.	.L ∙ I J
W2 := W - W1.
By induction, it follows that W2 is positive semi-definite. We need to show that the same holds for
Wι. By Schur complement We obtain that Wi is positive semi-definite if and only if kL ∙ I - Cn -
L Pk=2 CiiCii is positive semi-definite. It follows that
Cii —r ^X CIiCiI
L
i
1k
≤ IICiik2 + L E IICii∣2 IICii∣2 ≤ L + (k -I)L = kL.
2	i=2
Hence W1 is positive semi-definite and the induction is complete.
□
Returning to the statement of Lemma D.4, we will show that
∏i Vt ≤ C,
(14)
23
Published as a conference paper at ICLR 2022
for all i,j ∈ N with C chosen to be (l-max. Assuming We have shown (14), We conclude from
Claim D.5 that ∣∣V2Φμ∣∣2 ≤ nC, and hence Φ will be nC -smooth (the proof of Lemma D.4 will
follow).
To prove (14), we follow the same proof steps as in the proof of Agarwal et al. (2020), Lemma D.3.
We will need to prove an upper bound on the largest eigenvalue (in absolute value) of the matrix
v∏j ∏i Vj = Vnj ∏vμ,
along the direction where only agent i is allowed to change policy.
Fix policy π = (π1, ..., πn), agents i 6= j, scalars t, s ≥ 0, state s0 and u, v be unit vectors such that
∏i + t ∙ U ∈ ∆(Ai)S and ∏j + S ∙ V ∈ ∆(Aj)S. Moreover, let V(t) =	(∏ + t ∙ u, ∏-i). and
W(t, S) = ViO (∏i +1 ∙ u, ∏j + s ∙ v, ∏-i,-j). It suffices to show that
max
kuk2=1
d2V (0)
dt2
≤ 2YlAiI and max d2W(0,0) ≤ 2yVWAj
一(1 — γ)3 kuk2=1 dtds	—	(1 - Y)3
(15)
• We first focus on V (t). It holds that
V(t) = Σ Σ (xi,s0 ,a + tui,s0 ,a)	xj,s0 ,aj Qs0 ((πi + tu, π-i ), (a, a))
a∈Ai a∈A-i	j 6=i
(note that	a∈Ai	a∈A-i(xi,s0,a + tui,s0,a) j6=ixj,s0,aj = 1 since it is a distribution), hence
taking the second derivative we have
d2V (0)
dt2
X aX-i "-,a + …Y X…d2Q⅛^
dQis0 (π, (a, a))
+ 2	u_^ ui,s0,a ɪ]xj,s0,aj	dt
a∈Ai a∈A-i	j 6=i
(16)
For the remaining of the first part of the proof, we shall show
dQS0 (∏,(a, a))
dt
γ √IAI
(1 - Y)
and
d2QsC (π, (a, a))
dt2
< 2γ2IAiI
一 (1-γ)3 ,
≤
2
and then combining with (16) we get
d2V (0)
dt2
2γ√∣A∣	X	2γ2∣Ai∣	<	2γ∣Ai∣	, 2γ2IAiI	<	2γΑmax
一 (1 - Y)2	α⅛ IUi，S0，aI	+ (1 - Y)3	≤	(1 - Y)2	+ (1 - γ)3	≤	(1 - γ)3.
a∈Ai
To bound the derivative of the Q-function, observe that Qis ((πi + tu, π-i), (a, a)) = es> ,a(I -
YP (t))-1r, where r(S0, a) is the expected reward of agent i (w.r.t the randomness of the remaining
agents) if he chooses action a at state S0 and P(t) is state-action transition matrix of w.r.t the joint
distribution of all agents but i, i.e., π-i and the environment.
It is clear that d2P = 0 (linear with respect to t due to the direct parameterization) and that
Il ddP ll∞
≤ Pa∈Αi Iui,S0,aI ≤ √Ai ≤ √Amax∙ USingthat 口 (I-YP(t))-1 八 ∞ ≤ 舌,we get
dQs0 (∏, (a, a))
dt
Y e>0,a(I - YP(0))-1 dPd≡(I - YP(0))-1r
≤ Y VZIAT ≤ y√Αmax
≤ o-τF ≤ α-τF,
(17)
and also
d2Qse (π, (a, a))
dt2
2Y2∣e>c,a(I - YP(0))-1 dP0)(I - YP(0))-1 dP0)(I - YP(0))-1r
dt	dt
≤ 2Y2∣Ai∣ ≤ 2Y2Αmax
一(1 - Y)3 — (1 - Y)3,
(18)
Since u is arbitrary, the first part of (15) is proved.
24
Published as a conference paper at ICLR 2022
• For the second part, we focus on W(t) which is equal to
W(t,s)=ΣΣ	Σ	(Xi,so,a + tui,s0,a )(Xj,s0,b + svj,so,b>
a∈Ai b∈Aj a∈A-i,-j
• ∏ xj0,so,aj0 QSo ((πi + tu, πj + Sv,π-i,-j ),(a,b, a))
j0 6=i,j
We consider the derivative of W (19) and we get
dW (0, 0)	i
-dtdS- =	ʌ, ʌ,	ui,so,avj,so,b • ɪɪ χj0,so,aj0Qso(π, (a,b,a))
a∈Ai b∈Aj a∈A-i,-j	j0 6=i,j
(19)
+	ui,So,a •	Xj0,So,aj0
a∈Ai a∈A-i	j0 6=i
dQSo (∏ (a, a))
dt
+	vj,So,b •	Xj0,So,aj0
b∈Aj a∈A-j	j0 6=j
dQSo (π, (b, a))
dt
d2	d2QSo (π,a)
+a∈A ɪɪ 一0	~
The first term of the sum in absolute value is at most
1-γ
(assuming rewards lie in [0, 1].)
Moreover, using (17) the second term of the sum in absolute value is bounded by Y^AY)2A^ and
the third term by "q-$ i . To bound the
d2QSo (n,a)
dtds
, the same approach works that we used
to prove (18) with the extra fact that the state-action transition matrix is P (t, s) and moreover, the
reward r(s0, a, b) is the expected reward of agent i (w.r.t the randomness of all agents but i, j) if i
chooses action a and j chooses b at state s0. Finally, for the fourth term we get that
d2QSο (n, a)
dtds
≤
≤ Y2 eSζ,a(I - γP(0, 0))-1 dpd0^(I-YP(0 0))-1 dpd70-(I-γP(0,0))Tr +
ds	dt
+ Y2 eSζ,a(I — YP(O, 0))-1 dp∣^(I-YP(O 0))-1 dp^(I-YP(O 0))Tr +
So ,a	dt	ds
+ Y e>0,a(I-YP(O 0))-1 d2Pt0, O) (I-YP(O O))-1 r
≤ Y2PAW + Y2PAW + YpAW ≤ 2,PAW ≤ 2YAmaχ
≤	(1 - Y )3	+	(1-Y)3	+ (1-y)2	≤	(1 - Y )3	-(1-Y)3.
ProofofTheorem 4.2. The first step is to show that Φ is a β-smooth function, in particular, that V∏ Φ
is β-Lipschitz with β = 2星?乂 as established in Lemma D.4. Then, a standard ascent lemma for
Gradient Ascent (see Lemma C.1 from Bubeck (2015)) implies that for any β-smooth function f it
holds that f (χ0) 一 f (x) ≥ 泰 ∣∣χ0 一 χ∣∣2 where χ0 is the next iterate of equation PGA. Applied to
our setting, this gives
φ“(π(t+I))-却比⑴)≥ 4⅛≤ 卜(t+1)-π叫2	(20)
Thus, if the number of iterates, T, is 16n；DYSAmax, then there must exist a 1 ≤ t ≤ T so that
∣∣∏(t+1) 一 n(t) ∣∣2 ≤ f-Y). Using a standard approximation property (see Lemma C.2), we then
conclude that ∏(t+1) will be a T-Y) -stationary point for the potential function Φ. Hence, by
Lemma 4.1, it follows that ∏(t+1) is an E-NaSh policy and the proof is complete.	□
D.2 Finite Sample Case
Proof of Lemma 4.3. It is straightforward from Lemma C.4 and the equality of the partial derivatives
between the value functions and the potential, i.e., Vni Φμ = Vni Vi for all i ∈ N (see property P2
in Proposition B.1).	□
25
Published as a conference paper at ICLR 2022
Proofof Theorem 4.4. Let δt = V ∏t) - Vπ Φμ(π(t)) and set λ = 2'1-^7) (the inverse of the smooth
parameter in D.4). Moreover, We set y(t+1) = P∆(A)s(∏(t) + ηV∏Φ(∏(t))) (y(t+1) captures the
next iterate of the projected (deterministic) gradient ascent). We follow the analysis of Projected
Stochastic Gradient Ascent for non-convex smooth-functions (see Davis & Drusvyatskiy (2018),
Theorem 2.1) that makes use of the Moreau envelope. Let
φλ(x) = arg min {-Φμ(y) + 1 ∣∣x - yk2
(definition of Moreau envelope for our objective Φ). From the definition of φ and a standard property
of projection We get
φλ(∏(t+1)) ≤ -Φμ(y(t+1)) + 1∣∏(t+1) -y(t+1)∣2
≤ -Φμ(y(t+1)) + 1 ∣∣∏(t) + nV∏t) - y(t+1) ∣∣2	(21)
=-Φμ(y(t+1)) + 1 ∣∣∏(t) - y(t+1) ∣∣2 + W ∣∣V∏t)∣∣2 + 2λn(∏(t) - y(t+1))>V∏t)
Since V∏t) is unbiased (Lemma 4.3) we have that E[δt∣∏(t)] = 0, therefore E [δ>(y(t+1) - ∏(t))]=
0. Additionally, by Lemma 4.3 (applied for all agents i) we also have E ∣∣ V∏t ∣∣ ≤ 24nAmax.
Hence by taking expectation on (21) we have:
E[φλ(π(t+1))] ≤
≤ E [-Φμ(y(t+1)) + 1 ∣∣∏(t) - y(t+1)∣∣21 + 2nE[(∏㈤-y(t+1))>V∏Φμ(∏⑴)]+	Amax.
λ	2 λ	λ(1 - γ)
Using the definition of Moreau envelope and the fact that Φ is 1 -smooth (Lemma D.4, after the
parameterization, the smoothness parameter does not increase) we conclude that
E[φλ(π(t+1))] ≤
≤ E[φλ(∏⑴)]+ 2nE[(∏(t) - y(t+1))>VπΦμ(∏㈤)]+ 24n2nAmax
λ	λ(1 - γ)4
≤ E[φλ(∏⑴)]+ 2n E 卜 μ(∏㈤)一Φμ(y(t+1)) + 21λ 卜(t) -y(t+1)∣∣2 ] + !⅞⅛，
or equivalently
E[φλ(π(t+1))] - E[φλ(π(t))] ≤
≤ 2JnE Φμ(∏(t)) - Φμ(y(t+1)) + 21λ ∣∣∏(t) - y(t+1)∣∣2] + WnAmaj	(22)
Adding telescopically (22), dividing by T and because w.l.o.g -Φ ∈ [-1, 0], we get that
T + 24n(⅛ ≥ λTXX E [φμ(y(t+1))- φμ(π(t))i-轰XX E ]∣∣y(t+1)-叫2
t=1	t=1
≥ min
t∈[T]
{2JnE [φμ(yC+1)) - Φμ(∏㈤)i - λE[∣∣y(t+1)
(23)
Let t* be the time index that minimizes the above. We show the following inequality (which provides
a lower bound on the RHS of (23):
E [φμ(y(t*+1)) - Φμ(∏(t*))[	-	2λE	∣∣y(t*+1)	- ∏(t*)∣∣2]	≥	1E	∣∣y(t*+1)	- ∏(t*)∣∣j]	(24)
Observe that by 1 -smoothness of Φμ we get that H(x) := -Φμ(x) + 1 ∣∣χ - ∏(t*) ∣∣2 is 1 -strong
convex and moreover, y(t+1) is the minimizer of H. Therefore, we get that H(∏t*)-H(yt*+1) ≥
26
Published as a conference paper at ICLR 2022
21λ ∣∣∏t* - yt*+1∣∣2, or equivalently
Φμ(yt*+1) - Φμ(∏t*) -11∣∏t* - yt*+1∣∣2 ≥ ( ∣∣∏t* - yt*+1∣∣2.
By taking expectation of terms above, (24) follows. Combining (23) with (24) we conclude that
A 24η1n-⅜ ≥ λη E h∣∣∏ t*-y t*+1∣∣2 i .
By Jensen’s inequality it occurs that
Eh∣∣y(t*+1) - π(t*)∣∣2i≤ S提 + η 1mλA⅛.	(25)
To get an C-NaSh policy, We have to bound ∣∣y(t*+1) — ∏(t*) ∣∣2 ≤ 玷百∕1-√)4)and choose
α = 2 in the greedy parameterization. This is true because of Lemma C.3 Lemma 4.1. Hence, we
need to choose η, T so that
S λ2	+	12nλ∙m1 ≤ C(1- Y)
2 t1+ + ηc2(i-γ)4 ≤ 2d√s(2 + √nAmaχ).
We conclude that η can be chosen to be 48nD-A) YS and + to be 48(I-YI^axD S .	□
E Additional Experiments
In this part, We analyze variations of the experimental setting in Section 5.
Coordination beyond MPGs. As mentioned in Remark 1, We knoW that policy gradient converges
also in other cooperative settings (e.g., in Weighted and asymptotically also in ordinal MPGs). To
study such cases, We modify our experiment from Section 5. The setting remains mostly the same,
except noW in the distancing state the reWards are reduced by a differing (yet still sufficiently large)
amount, ck, for each facility k = A, B, C, D. Despite this asymmetry, agents still have aligned
incentives. In fact, if the ck ’s for all k = A, B, C, D are taken to be greater than c in the MDP from
Section 5, then the agents can be said to have even “stronger” incentive to cooperate. The results of
running independent policy gradient on this variant are shoWn in Figure 6. Despite requiring more
iterations compared to the symmetric setting, the algorithm still converges to the same Nash policy.
Figure 6:	Figures similar to Figure 5, except noW With c > cA > cB > cC > cD as described in the
text. Independent policy gradient requires more iterations to converge compared to the symmetric
shift setting, but still arrives at the same Nash policy.
Coordination with more agents and facilities. We next test the performance of the independent
policy gradient algorithm in a larger setting With N = 16 agents and Ai = 5 facilities, Ai =
{A, B, C, D, E} With wA < wB < wC < wD < wE (i.e., E is the most preferable by all agents).
We use a learning rate η = 0.0001 for all agents (Which is again much larger than the theoretical
guarantee of Theorem 4.2). All runs lead to convergence to an (optimal) Nash policy as shoWn in the
middle and rightmost panels. The leftmost panel shoWs the distribution of the agents among facilities
in both states, Which is the same (and the optimal one) in all Nash policies that are reached by the
algorithm. The results are shoWn in Figure 7.
27
Published as a conference paper at ICLR 2022
Figure 7:	Convergence to deterministic Nash policies of independent policy gradient in a variation
of the MDP of Section 5 with N = 16 agents and Ai = 5 facilities, Ai = {A, B, C, D, E} with
wA < wB < wC < wD < wE (i.e., E is the most preferable by all agents). Again, while there are
several (symmetric) deterministic Nash policies, all of them yield the same distribution of agents
among states (leftmost panel). All runs converge successfully to that outcome.
Coordination with random transitions Next, we study the effect of adding randomness to the
transitions on the performance of the individual policy gradient algorithm. In this case, we experiment
with the same setting as in Section 5 (i.e., N = 8 agents and Ai = 4 facilities that each agent
i ∈ N can choose from), but use the following stochastic transition rule instead: in addition to the
existing transition rules, the sequence of play may transition from the safe to the distancing state with
probability p% regardless of the distribution of the agents and may remain at the distancing state with
probability q% again regardless of the distribution of the agents there.
Two sets of results are presented in Figure 8. In the first (upper panels), we use p, q = 1%, 10% and
in the second p, q = 5%, 20%. In both cases, we use a learning rate η = 0.0001 (several orders of
magnitude higher than what is required by Theorem 4.4). Independent policy gradient converges in
both cases to deterministic Nash policies despite the randomness in the transitions. However, for
higher levels of randomness (lower panels), the algorithm remains at an -Nash policy for a high
number of iterations. This is in line with the theoretical predictions of Theorem 4.4.
Figure 8: Convergence to deterministic Nash policies of independent policy gradient in two variations
of the MDP of Section 5 with stochastic transitions between states.
Independent Policy Gradient in a Different Environment. A subsequent work to this paper
introduced a different multi-agent MDP environment that they prove is an MPG, which they refer
to as stochastic congestion games (Fox et al., 2021). This stochastic congestion or routing game
was already well-studied before this paper, especially in Mguni et al. (2021), so it is valuable to see
that this MPG framework captures such games. In this environment, there is an underlying directed
acyclic graph G that has a source node s and a sink node t. There are N agents. The actual states
28
Published as a conference paper at ICLR 2022
of the finite-horizon MDP are all the possible positions of the agents on the graph. When the state
consisting of all agents being at t is reached, the MDP terminates. At each time step, each agent
chooses an action, which in this case is an edge e to transverse, and the cost they experience is
a function of the load of the edge, which is the total number of agents choosing edge e. A more
complete description and proof that this environment is an MPG is provided in Fox et al. (2021).
We implemented this environment with N = 4 agents, matching the number of agents in Fox et al.
(2021), but we were actually able to increase the size of the underlying graph from the 6 vertices
done in that paper to 8. The game is visualized in Figure 9.
Figure 9: The stochastic congestion game with N = 4 agents and 8 nodes. In this discrete time MPG,
all agents start at s and at each point in time, they move to one of the nodes of the next layer. Their
goal is to reach t at a minimum cost (as expressed by the congestion on the the vertices). The left
panel shows the starting state and the right panel shows one possible state after two transitions (with
intermediate states in pale colors).
We used our implementation of the independent policy gradient algorithm with the same parameters
as in our experiment from Section 5, specifically we have T = 20, γ = 0.99, and η = 0.0001. The
results are shown in Figure 10.
Figure 10: Convergence of independent policy gradient in the stochastic congestion game with N = 4
and 6 vertices between source and target (3 layers of 2 vertices each) as shown in Figure 9.
29