Published as a conference paper at ICLR 2022
Pseudo Numerical Methods for Diffusion
Models on Manifolds
Luping Liu, Yi Ren, Zhijie Lin & Zhou Zhao*
Zhejiang University
{luping.liu,rayeren,linzhijie,zhaozhou}@zju.edu.cn
Ab stract
Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality
samples such as image and audio samples. However, DDPMs require hundreds
to thousands of iterations to produce final samples. Several prior works have
successfully accelerated DDPMs through adjusting the variance schedule (e.g.,
Improved Denoising Diffusion Probabilistic Models) or the denoising equation
(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceler-
ation methods cannot maintain the quality of samples and even introduce new
noise at a high speedup rate, which limit their practicability. To accelerate the
inference process while keeping the sample quality, we provide a fresh perspec-
tive that DDPMs should be treated as solving differential equations on manifolds.
Under such a perspective, we propose pseudo numerical methods for diffusion
models (PNDMs). Specifically, we figure out how to solve differential equations
on manifolds and show that DDIMs are simple cases of pseudo numerical meth-
ods. We change several classical numerical methods to corresponding pseudo
numerical methods and find that the pseudo linear multi-step method is the best
in most situations. According to our experiments, by directly using pre-trained
models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality syn-
thetic images with only 50 steps compared with 1000-step DDIMs (20x speedup),
significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have
good generalization on different variance schedules.* 1
1	Introduction
Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020)
is a class of generative models which model the data distribution through an iterative denoising
process reversing a multi-step noising process. DDPMs have been applied successfully to a variety
of applications, including image generation (Ho et al., 2020; Song et al., 2020b), text generation
(Hoogeboom et al., 2021; Austin et al., 2021), 3D point cloud generation (Luo & Hu, 2021), text-
to-speech (Kong et al., 2021; Chen et al., 2020) and image super-resolution (Saharia et al., 2021).
Unlike Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which require careful
hyperparameter tuning according to different model structures and datasets, DDPMs can use similar
model structures and be trained by a simple denoising objective which makes the models fit the
noise in the data. To generate samples, the iterative denoising process starts from white noise and
progressively denoises it into the target domain according to the noise predicted by the model at
every step. However, a critical drawback of DDPMs is that DDPMs require hundreds to thousands
of iterations to produce high-quality samples and need to pass through a network at least once at
every step, which makes the generation of a large number of samples extremely slow and infeasible.
In contrast, GANs only need one pass through a network.
There have been many recent works focusing on improving the speed of the denoising process.
Some works search for better variance schedules, including Nichol & Dhariwal (2021) and Watson
et al. (2021). Some works focus on changing the inference equation, including Song et al. (2020a)
* Corresponding author
1Our implementation is available at https://github.com/luping- liu/PNDM.
1
Published as a conference paper at ICLR 2022
and Song et al. (2020b). Denoising Diffusion Implicit Models (DDIMs) (Song et al., 2020a) relying
on a non-Markovian process accelerate the denoising process by taking multiple steps every itera-
tion. Probability Flows (PFs) (Song et al., 2020b) build a connection between the denoising process
and solving ordinary differential equations and use numerical methods of differential equations to
accelerate the denoising process. Additionally, we introduce more related works in Appendix A.1.
reverse process without loss of qual-
ity, like the Runge-Kutta method, but
Figure 1: 5, 10, 20, 50 and 100-steps generated results using
DDIMs, classical numerical methods and PNDMs.
they need to propagate forward more times along a neural network at every step. Furthermore, we
also notice that numerical methods can introduce noticeable noise at a high speedup rate, which
makes high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs.
This phenomenon is also mentioned in Salimans & Ho (2022).
To figure out the reason for the performance degradation in classical numerical methods, we conduct
some analyses and find that classical numerical methods may sample data far away from the main
distribution area of the data, and the inference equations of DDPMs do not satisfy a necessary
condition of numerical methods at the last several steps (see Section 3.2).
To tackle these problems, we design new numerical methods called pseudo numerical methods for
diffusion models (PNDMs) to generate samples along a specific manifold in Rn , which is the high-
density region of the data. We first compute the corresponding differential equations of diffusion
models directly and self-consistently, which builds a theoretical connection between DDPMs and
numerical methods. Considering that classical numerical methods cannot guarantee to generate
samples on certain manifolds, we provide brand-new numerical methods called pseudo numerical
methods based on our theoretical analyses. We also find that DDIMs are simple cases of pseudo
numerical methods, which means that we also provide a new way to understand DDIMs better.
Furthermore, we find that the pseudo linear multi-step method is the fastest method for diffusion
models under similar generated quality.
Besides, we provide a detailed theoretical analysis of our new theory and give visualization results to
support our theory intuitively. According to our experiments, our methods have several advantages:
•	Our methods combine the benefits of DDIMs and high-order numerical methods successfully. We
theoretically prove that our new methods PNDMs are second-order convergent while DDIMs are
first-order convergent, which makes PNDMs 20x faster without loss of quality on Cifar10 and
CelebA.
•	Our methods can reduce the best FID of pre-trained models with even shorter sampling time. With
only 250 steps, our new denoising process can reduce the best FID by around 0.4 points Cifar10
and CelebA. We achieve a new SOTA FID score of 2.71 on CelebA.
•	Our methods work well with different variance schedules, which means that our methods have a
good generalization and can be used together with those works introducing better variance sched-
ules to accelerate the denoising process further.
2	Background
In this section, we introduce some backgrounds. Firstly, we present the classical understanding of
DDPMs. Then we provide another understanding based on Song et al. (2020b), which inspires us
to use numerical methods to accelerate the denoising process of diffusion models. After that, we
introduce some background on numerical methods used later in this paper.
2
Published as a conference paper at ICLR 2022
2.1	Denoising Diffusion Probabilistic Models
DDPMs model the data distribution from Gaussian distribution to image distribution through an
iterative denoising process. Let x0 be an image, then the diffusion process is a Markov process and
the reverse process has a similar form to the diffusion process, which satisfies:
Xt+ι ~N(J∖ - βtxt, βtI), t = 0,1,…，N - 1.
Xt-1 〜N (μθ (xt,t) ,βθ (xt,t )I), t = N, N - 1, ∙∙∙ , 1.
(1)
Here, βt controls the speed of adding noise to the data, calling them the variance schedule. N is the
total number of steps of the denoising process. μθ and βθ are two neural networks, and θ are their
parameters.
Ho et al. (2020) get some statistics estimations of μθ and βθ. According to the properties of the
conditional Gaussian distribution, we have:
q(χt∣χO) = N(Ftχo, (1 - α)I),
()
q (Xt-1 ∣χt ,χ O) = N (μ t (χt,χ o) ,βt I).
Here, α = 1 - βt, α = QQt= a, μt = √--1β Xo + √ 1-0^-1) Xt and βt = ⅛¾1 βt. Then
this paper sets βθ = βt and designs a objective function to help neural networks to represent μθ.
Objective Function The objective function is defined by:
Lt-1 = E q [ ll^i t (Xti x 0) - μθ (Xtit) || ]
H I— ∖t(t(xo,e)---------∕ι尸t _ E) - μθ(Xt(xo, e),t)||2
,aai ∖	√1 - at )
(3)
⅛) M
一eθ (√αtXX o + λ∕1 — α t E, t) ||2
Here, Xt (X o ,e ) = √αtX o+√ 1 - α t g E 〜N (0, 1), % is an estimate of the noise E. The relationship
between μe and e§ is μe = √τ(Xt 一 √β0teθ)∙ Because E 〜N(0, 1), we assume that the mean
and variance of Eθ are 0 and 1.
2.2	Stochastic Differential Equation
According to Song et al. (2020b), there is another understanding OfDDPMs. The diffusion process
can be treated as solving a certain stochastic differential equation dX = (，1 — β (t) — 1) X (t) dt +
βt(t) dw. According to Anderson (1982), the denoising process also satisfies a similar stochastic
differential equation:
dX = ((p1 — β(t) — 1)X(t) — β(t)eθ(X(t),t)) dt + Pβ(t)dw.	(4)
This is Variance Preserving stochastic differential equations (VP-SDEs). Here, we change the do-
main of t from [1, N] to [0, 1]. When N tends to infinity, {βi}iN=1, {Xi}iN=1 become continuous
functions β(t) and X(t) on [0, 1]. Song et al. (2020b) also show that this equation has an ordinary
differential equation (ODE) version with the same marginal probability density as Equation (4):
dX = ((p1 — β(t) — 1)X(t) — 2β(t)eθ(X(t),t)) dt.	(5)
This different denoising equation with no random item and the same diffusion equation together is
Probability Flows (PFs). These two denoising equations show us a new possibility that we can use
numerical methods to accelerate the reverse process. As far as we know, DDIMs first try to remove
this random item, so PFs can also be treated as an acceleration of DDIMs, while VP-SDEs are an
acceleration of DDPMs.
2.3	Numerical Method
Many classical numerical methods can be used to solve ODEs, including the forward Euler method,
Runge-Kutta method and linear multi-step method (Timothy, 2017).
3
Published as a conference paper at ICLR 2022
Forward Euler Method For a certain differential equation satisfying 篝 = f (χ,t). The trivial
numerical method is forward Euler method satisfying xt+δ = xt + δf(xt, t).
Runge-Kutta Method Runge-Kutta method uses more information at every step, so it can achieve
higher accuracy 2. Runge-Kutta method satisfies:
(k 1 =	f( xt,t)	, k 2 = f( Xt + 2k 1,t + 2)
∖	k3 =	f(Xt + δ k2, t + 2 ) , k4 = f(Xt + δk3, t + δ)	⑹
I	Xt+δ	= Xt + 6 (k 1 + 2 k 2 + 2 k 3 + k 4).
Linear Multi-Step Method Linear multi-step method is another numerical method and satisfies:
δ
Xt+δ = Xt + 24(55ft - 59ft-δ + 37ft-2δ - 9ft-3δ), ft = f(Xt,t).
(7)
3	Pseudo Numerical Method for DDPM
In this section, we first compute the corresponding differential equations of diffusion models to build
a direct connection between DDPMs and numerical methods. As a byproduct, we can directly use
pre-trained models from DDPMs. After establishing this connection, we provide detailed analyses
on the weakness of classical numerical methods. To solve the problems in classical numerical meth-
ods, we dive into the structure of numerical methods by dividing their equations into a gradient part
anda transfer part and define pseudo numerical methods by introducing nonlinear transfer parts. We
find that DDIMs can be regarded as simple pseudo numerical methods. Then, We explore the pros
and cons of different numerical methods and choose the linear multi-step method to make numeri-
cal methods faster. Finally, we summarize our findings and analyses and safely propose our novel
pseudo numerical methods for diffusion models (PNDMs), which combine our proposed transfer
part and the gradient part of the linear multi-step method. Furthermore, we analyze the convergence
order of pseudo numerical methods to demonstrate the effectiveness of our methods theoretically.
3.1	Formula Transformation
According to Song et al. (2020a), the reverse process of DDPMs and DDIMs satisfies:
Xt-1
√ α t-1
Xt — √ 1 — αt6θ(Xt, t)
+1
— σt2θ (Xt, t) + σtt .
(8)
	
Here, σt controls the ratio of random noise. If σt equals one, Equation (8) represents the reverse
process of DDPMs; if σt equals zero, this equation represents the reverse process of DDIMs. And
only when σt equals zero, this equation removes the random item and becomes a discrete form
of a certain ODE. Theoretically, the numerical methods that can be used on differential equations
with random items are limited. And Song et al. (2020b) have done enough research in this case.
Empirically, Song et al. (2020a) have shown that DDIMs have a better acceleration effect when the
number of total steps is relatively small. Therefore, our work concentrate on the case σt equals zero.
To find the corresponding ODE of Equation (8), we replace discrete t — 1 with a continuous version
t — δ according to (Song et al., 2020a) and change this equation into a differential form, namely,
subtract Xt from both sides of this equation:
Xt	θ(Xt, t)
Xt-δ — Xt	=	( αt-δ	-	αt)	I	—,—	—,	----,—. — ———Z	=------Z	— I .
∖√at(√ t-δ + √αt)	√αt (p(y―α∑δ)θ^t + P(I-Ot)O-δ) J
(9)
Because δ is a continuous variable from 0 to t, we can now compute the derivative of the generation
data Xt and get that lim XLxI = —α0(t) ( ^t)^------'虱X(t),)= ). Here, α(t) is the continuous
δ→0	δ	2 2 2α)	2α(t)√1-α(t)
version of {c⅛i}^=1 like the definition of X(t). Therefore, the corresponding ODE when δ tends to
2To achieve higher accuracy, more information is not enough. The reason why these methods achieve higher
accuracy can be found in Appendix A.2
4
Published as a conference paper at ICLR 2022
zero of Equation (9) is:
dx = __0(t)( X(t)_________S(X(t),t)
dt 一 0	(2 α( t)	2 α( t ∖p — α( t)
(10)
3.2	Classical Numerical Method
After getting the target ODE, the easiest way to solve it is through classical numerical methods.
However, We notice that classical numerical methods can introduce noticeable noise at a high
speedup rate, making high-order numerical methods (e.g., Runge-Kutta method) even less effec-
tive than DDIMs. This phenomenon is also mentioned in Salimans & Ho (2022). To make better
use of numerical methods, we analyze the differences between Equation (10) and usual differen-
tial equations and find two main problems when we directly use numerical methods with diffusion
models.
The first problem is that the neural network θ and Equa-
tion (10) are well-defined only in a limited area. Equation
(2) shows that the data xt is generated along a curve close
to an arc. According to Figure 2, most of xt is concentrated
in a band with a width of around 0.1, namely the red area
in Figure 2. This means that the neural network θ can-
not get enough examples to fit the noise successfully away
from this area. Therefore, θ and Equation (10), which con-
tains θ , are only well-defined in this limited area. How-
ever, all classical numerical methods generate results along
a straight line instead of an arc. The generation process
may generate samples away from the well-defined area and
then introduce new errors. In Section 4.3 we will give more
visualization results to support this.
step
Figure 2: the density distribution of
the norm of the data.
The second problem is that Equation (10) is unbounded at most cases. We find that for most linear
variance schedules βt, Equation (10) tends to infinity when t tends to zero (see Appendix A.4), which
does not satisfy the condition of numerical methods mentioned in Section 2.3. This is an apparent
theoretical weakness that previous works have not explored. On the contrary, in the original DDPMs
and DDIMs, the prediction of the sample xt and the noise θ in the data are more and more precise
as the index t tends to zero (see Appendix A.5). This means original diffusion models do not make
a significant error in the last several steps, whereas using numerical methods on Equation (10) does.
This explains why DDIMs are better than higher-order numerical methods.
3.3	Pseudo Numerical Method on Manifold
The first problem above shows that we should try to solve our problems on certain manifolds.
Here, target manifolds are the high-density region of the data Xt of DDPMs, which is defined by
xt(x0, e) = √0tx0 + √1 — αte, e 〜N(0, 1). Ernst & Gerhard (1996) show several numerical
methods to solve differential equations on manifolds that have analytic expressions. Unfortunately,
it’s challenging to use the above expression of manifolds. Because we do not know the target x0 in
the reverse process and random items are hard to handle, too.
In this paper, we design a different way that we make our new equation of denoising process more
fits with the equation of original DDIMs to make their results share similar data distribution. Firstly,
we divide the classical numerical methods into two parts: gradient and transfer parts. The gradient
part determines the gradient at each step, while the transfer part generates the result at the next step.
For example, linear multi-step method can be divided into the gradient part f 0 = *(55 ft — 59 ft-δ +
37ft-2δ — 9ft-3δ) and the transfer part xt+δ = xt + δf0. All classical numerical methods have the
same linear transfer part, while gradient parts are different.
We define those numerical methods which use a nonlinear transfer part as pseudo numerical meth-
ods. And an expected transfer part should have the property that when the result from the gradient
part is precise, then the result of the transfer part is as close to the manifold as possible and the error
of this result is as small as possible. We find that Equation (9) satisfies such property.
5
Published as a conference paper at ICLR 2022
Property 3.1 If is the precise noise in xt, then the result of xt-δ from Equation (9) is also precise.
And we put the proof of this property in Appendix A.5. Therefore, we use:
φ(xt, t, t, t - δ)
(11)
as the transfer part and θ as the gradient part. That if θ is precise, the result of xt-δ is also precise,
which means that θ can determine the direction of the denoising process to generate the final results.
Therefore, such a choice also satisfies the definition of a gradient part. Now, we have our gradient
part θ and transfer part φ.
This combination solves the two problems mentioned above successfully. Firstly, our new transfer
parts do not introduce new errors. This property also means that it keeps the results at the next step
on the target manifold because generating samples away is a kind of error. This shows that we solve
the first problem. Secondly, we know that the prediction ofθ is more and more precise in the reverse
process in the above subsection. And our new transfer part can generate precise results according
to the precise prediction of θ . Therefore, our generation results are more and more precise using
pseudo numerical methods, while classical numerical methods can introduce obvious error at the last
several steps. This shows that we solve the second problem, too. We also find that their combination
φ(xt, θ(xt, t), t, t - 1) is just the inference equation used by DDIMs, so DDIMs is a simple case
of pseudo numerical methods. Here, we define DDIMs as DDIMs*, emphasizing that it is a pseudo
numerical method.
3.4	Gradient Part
Because we split numerical methods into two parts, we can use the same gradient part from different
classical numerical methods freely (e.g., linear multi-step method), although we change the transfer
part of our inference equation. Our theoretical analyses and experiments show that the gradient part
from different classical methods can work well with our new transfer part (see Section 3.6, 4.2). By
using the same gradient part of the linear multi-step method, we have:
{et = 6θ (xt, t)
et = 214 (55et - 59et-δ + 37et-2δ - 9et-3δ )
xt+δ = φ(xt, e0t,t,t + δ).
(12)
By using the same gradient part of Runge-Kutta
method, we have:
et = ^θ (xt, t)
Xt = φ (χt,e1 ,t,t + 1)
et = eθ (xt, t + 2)
xt = φ (xt, et, t, t + 1)
< e = S (xt ,t + δ)	(13)
xt = φ(xt , et , t, t + δ )
et = ^θ (xt ,t + δ)
et = 6 (et + 2 et + 2 et + et)
、 xt-δ = φ (xt,et,t,t + δ) ∙
Abbreviate Equation (12) and (13) as xt+δ , et =
PRK(xt, t, t + δ).
Algorithm 1 DDIMs
1:	XT 〜N(0, I)
2:	for t = T — 1,…，1,0 do
3:	xt = φ(xt+1, θ(xt+1,t + 1),t + 1, t)
4:	end for
5:	return x0
Algorithm 2 PNDMs
1	XT 〜N(0, I)
2	fort=T— 1,T—2,T—3do
3	Xt,et = PRK(Xt+1, t + 1, t)
4	end for
5	for t = T — 4, ∙∙∙ , 1,0 do
6	Xt,et =PLMS(Xt+1,{ep}p>t,t+1,t)
7	end for
8	return X0
PLMS(xt, {ep}p<t, t, t + δ), xt+δ , et1
Here, we have provided three kinds of pseudo numerical methods. Although advanced numerical
methods can accelerate the denoising process, some may have to compute the gradient part θ more
times at every step, like the Runge-Kutta method. Propagating forward four times along a neural
network makes the denoising process slower. However, we find that the linear multi-step method can
reuse the result of θ four times and only compute θ once at every step. And theoretical analyses tell
us that the Runge-Kutta and linear multi-step method have the same convergence order and similar
results.
6
Published as a conference paper at ICLR 2022
Therefore, we use the gradient part of the lin-
ear multi-step method and our new transfer
part as our main pseudo numerical methods
for diffusion models (PNDMs). In Table 1, we
show the relationship between different nu-
merical methods. Here, we can see PNDMs
combine the benefits of higher-order classical
numerical methods (in the gradient part) and
DDIMs (in the transfer part).
Order φ	first	non-first
linear	forward Euler	linear multi-step, Runge-Kutta…
nonlinear	DDIM 一	PNDM
Table 1: The relationship between different numer-
ical methods.
3.5	Algorithm
We can provide our whole algorithm of the denoising process of DDIMs now. According to Song
et al. (2020a), the algorithm of the original method satisfies Algorithm 1. And our new algorithm of
DNPMs uses the pseudo linear multi-step and pseudo Runge-Kutta method, which satisfies Algo-
rithm 2. Here, we cannot use linear multi-step initially because the linear multi-step method cannot
start automatically, which needs at least three previous steps’ information to generate results. So
we use the Runge-Kutta method to compute the first three steps’ results and then use the linear
multi-step method to calculate the remaining.
We also use the gradient parts of two second-order numerical methods to get another pseudo nu-
merical method. We introduce the details of this method in Appendix A.3. We call it S-PNDMs,
because its gradient part uses information from two steps at every step. Similarly, we also call our
first PNDMs F-PNDMs, which use data from four steps, when we need to distinguish them.
3.6	Convergence Order
Change the transfer part of numerical methods may introduce unknown error. To determine the
influence of our new transfer part theoretically, we compute the local and global error between the
theoretical result of Equation (10) x(t + δ) and our new methods, we find that x(t + δ) - xDDIM(x +
δ) = O(δ2) and
x(t + δ) - xS/F-PNDM(x + δ) = O(δ3).	(14)
If the target ODE satisfies Lipschitz condition and local error elocal = O(δk), then there are C and
h such that the global error eglobal satisfies eglobal ≤ Cδk (1 + eh + e2h + ・…)≤ C0δk- 1. And We
have that the convergence order is equal to the order of the global error. The detailed proof can be
found in Appendix A.6. Therefore, We get the folloWing property:
Property 3.2 S/F-PNDMs have third-order local error and are second-order convergent.
4 Experiment
4.1	Setup
We conduct unconditional image generation experiments on four datasets: Cifar10 (32 × 32)
(Krizhevsky et al., 2009), CelebA (64 × 64) (Liu et al., 2015), LSUN-church (256 × 256) and
LSUN-bedroom (256 × 256) (Yu et al., 2016). According to the analysis in Section 3.1, We can use
pre-trained models from prior Works in our experiments. The pre-trained models for Cifar10, LSUN-
church and LSUN-bedroom are taken from Ho et al. (2020) and the pre-trained model for CelebA is
taken from Song et al. (2020a). In these models, the number of total steps N is 1000 and the variance
schedule is linear variance schedule. And We also use a pre-trained model for Cifar10, Which uses a
cosine variance schedule from improved denoising diffusion probabilistic models (iDDPMs (Nichol
& DhariWal, 2021)).
4.2	Sample Efficiency and Quality
To analyze the acceleration effect, We test Fenchel Inception Distance (FID (Heusel et al., 2018))
on different datasets under different steps and different numerical methods, including DDIMs,
S-PNDMs, F-PNDMs and classical fourth-order numerical methods (FONs) (e.g., Runge-Kutta
7
Published as a conference paper at ICLR 2022
dataset	FID step model	10	20	50	100	250	1000	time
Cifar10	DDIM	13.4	6.84	4.67	4.16		4.04	
	PF		13.8	3.89	3.69	3.71	3.72	
	DDIM*-	18.5	10.9	6.99	5.52	4.52	4.00	0.337
Cifar10	FON	13.1	7.41	5.26	4.65	4.12	3.71	0.390
(linear)	S-PNDM	11.6	7.56	5.18	4.34	3.91	3.80	0.344
	F-PNDM	7.03	5.00	3.95	3.72	3.60	3.70	0.391
Cifar10 (cosine)	DDIM	14.5	8.79	5.86	4.92	4.30	3.69	0.505
	S-PNDM	8.64	5.77	4.46	3.94	3.71	3.38	0.517
	F-PNDM	7.05	4.61	3.68	3.53	3.49	3.26	0.595
CelebA	DDIM	17.3	13.7	9.17	6.53		3.51	
	DDIM*-	16.9	13.4	8.95	6.36	4.44	3.41	1.237
CelebA	FON	16.0	11.6	8.13	6.70	5.14	4.17	1.431
(linear)	S-PNDM	12.2	9.45	5.69	4.03	3.19	2.99	1.258
	F-PNDM	7.71	5.51	3.34	2.81	2.71	2.86	1.433
Table 2: Image generation
measured in FID on Cifar10
and CelebA. PFs use black
box ODE solvers and we use
the number of score func-
tion evaluations as the step
of PFs. DDIM* is a retest
of DDIM. The bold results
mean the best ones using the
same pretrained model. We
use the 50-step, 512 batch
size experiment on an RTX-
3090 to test the computa-
tional cost and the column
time is the average compu-
tational cost per step in sec-
onds. And we put the results
of standard deviation in Ap-
pendix A.12
method and linear multi-step method). On Cifar10 and CelebA, we first provide the results of previ-
ous works DDIMs. Then, we use the same pre-trained models to test numerical methods mentioned
in this paper and put the results in Cifar10 / CelebA (linear). We also use models from iDDPMs to
test nonlinear variance schedules and put the results in Cifar10 (cosine). Song et al. (2020b) do not
provide detailed FID results of probability flows (PFs) under different steps, so we retest the results
using its pretrained models by ourselves.
Efficiency Our two baselines are DDIM and PF. DDIM is a simple case of pseudo numerical meth-
ods, and PF is a case of classical numerical methods. However, PF uses a much bigger model than
DDIM and uses some tricks to improve the sample quality. To ensure the experiment’s fairness, we
use fourth-order numerical methods on Equation (10) and the model from DDIM. In Table 2, we
find that the performance of FON is limited when the number of steps is small. By contrast, our new
methods, including S-PNDM and F-PNDM, can improve the generated results regardless of whether
the number of steps used is large or small. According to Cifar10 / CelebA (linear), F-PNDM can
achieve lower FID than 1000 steps DDIM using only 50 steps, making diffusion models 20x faster
without losing quality.
We draw a line chart of computation cost with FID accord-
ing to the results of Cifar10 (linear) above in Figure 3. Be-
cause F-PNDM uses the pseudo Runge-Kutta method to
generate the first three steps, it is slower than other methods
at the first several steps. Therefore, S-PNDM can achieve
the best FID initially, then F-PNDM becomes the best and
the acceleration is significant.
Quality When the number of steps is relatively big, the
results of FON become more and more similar to that of
Figure 3: The FID results under dif-
ferent computation costs and different
numerical methods on Cifar10. The
unit of time is the computational cost
of 1-step DDIM, which is 0.337s.
pseudo numerical methods. This is because all the meth-
ods are solving Equation (10), and their convergent results
should be the same. However, pseudo numerical methods
still work better using a large number of steps empirically.
F-PNDM can improve the best FID around 0.4 using pre-
trained models and achieves a new SOTA FID score of 2.71
on CelebA, which shows that our work can not only accelerate diffusion models but also improve
the sample quality topline. We also notice that the FID results of F-PNDM converge after more than
250 steps. The FID results will fluctuate around a value then. This phenomenon is more pronounced
when we test our methods on LSUN (see Table 5, 6).
According to Cifar10 (cosine), the cosine variance schedule can lower FID using a relatively large
number of steps. More analyses about variance schedule can be found in Appendix A.7. What’s
more, we test our methods on other datasets and provide our FID results in Appendix A.9 and
image results in Appendix A.10. We can draw similar conclusions on our methods’ acceleration and
sampling quality, regardless of the datasets and the size of the images.
8
Published as a conference paper at ICLR 2022
step	step	step
Figure 4: The upper part shows the change of norm with the number of steps using different methods
and different steps. The lower part shows the generation curves of two points using different methods
and different steps. DDIM-n means n-step DDIM method. Experiments in this subsection all use
the Cifar10 dataset and we use the 1000-step DDIM’s result as our target result.
4.3	Sample on Manifolds
Here, we design visualization experiments to show the effort of our new methods and support our
analyses. Because it is hard to visualize high-dimensional data, we use the change of a global char-
acteristic norm and a local characteristic pixel to show the change of the data under different steps.
For pixel, We randomly choose two positions P1 ,p2. Then for a series of images xτ, xτ-k, ∙∙∙ , x 0
derived from the reverse process, we denote ytk as the value of xt at position pk . Then we draw a
polyline (y1, y2) t=T,…in R2. For norm, we first count the distribution of the norm of the training
datasets under different steps and use this to make a heat map as the background. After that, we
draw the norm of our generated results using different methods and steps above this heat map.
In Figure 4, we can see that the FON may run far away from the high-density area of the data, which
explains why FON may introduce noticeable noise. However, PNDM can avoid this problem and
appropriately fit the target result. More visualization results supporting our analysis can be found
in Appendix A.11. What’s more, we design a toy example to test our new methods without the
influence of neural networks and get similar conclusions as to the real cases above. We put the
detailed results in Appendix A.8.
5	Discussion
In this paper, we provided DNPMs, a new numerical method suitable for solving the corresponding
ODEs of DDPMs. DNPMs can generate high-quality images using fewer steps without loss of qual-
ity successfully. Based on the idea of this work, further improvement can be explored in our future
works: 1) find a better variance schedule for DNPMs: although we tested DNPMs on linear variance
schedule and cosine variance schedule in this work, there might be another variance schedule more
suitable for our proposed numerical methods. 2) Find higher-order convergent pseudo numerical
methods: we analyzed the convergence order of S/F-DNPMs, which are both second-order conver-
gent. However, F-DNPMs achieve better FID than S-DNPMs in most cases. We think this is because
the result between our transfer part and target ODE has a higher-order error, which limits the con-
vergence order of F-DNPMs. This error from the change of the transfer part is theoretical but does
not influence the quality of images according to the property of Equation (11). However, making
the transfer part higher-order convergent and finding the influence of such change is still exciting
and needs more research. 3) Extend PNDMs to more general applications: when we proved the
convergence order of DNPMs, we found that other kinds of transfer parts can keep the convergence
order unchanged too, which means that pseudo numerical methods can be used on more general
applications, like certain neural ODEs (Chen et al., 2019; Dupont et al., 2019).
9
Published as a conference paper at ICLR 2022
References
Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications,12(3):313-326,1982. ISSN03044149. doi: 10.1016/0304-4149(82)90051-5.
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured
denoising diffusion models in discrete state-spaces, 2021.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-
grad: Estimating gradients for waveform generation. arXiv preprint, 2020.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Dif-
ferential Equations. arXiv:1806.07366 [cs, stat], December 2019.
Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR:
Conditioning Method for Denoising Diffusion Probabilistic Models. CoRR, abs/2108.02938:14,
2021.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented Neural ODEs. arXiv:1904.01681
[cs, stat], October 2019.
Hairer Ernst and Wanner Gerhard. Solving Ordinary Differential Equations II: Stiff and Differential-
Algebraic Problems. Springer Series in Computational Mathematics 14. Springer-Verlag Berlin
Heidelberg, 2 edition, 1996.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.
arXiv:1706.08500 [cs, stat], January 2018.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv, 256,
2020.
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. ArgmaX Flows
and Multinomial Diffusion: Towards Non-Autoregressive Language Models. arXiv preprint,
2021.
Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-
TTS: A Denoising Diffusion Model for Text-to-Speech. arXiv:2104.01409 [cs, eess], April 2021.
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score Matching
Model for Unbounded Data Score. arXiv:2106.05527 [cs, stat], August 2021.
Zhifeng Kong and Wei Ping. On Fast Sampling of Diffusion Probabilistic Models.
arXiv:2106.00132 [cs], June 2021.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile
Diffusion Model for Audio Synthesis. arXiv:2009.09761 [cs, eess, stat], March 2021.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Max W. Y. Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral Denoising Diffusion
Models. arXiv:2108.11514 [cs, eess], August 2021.
Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Peng Liu, and Zhou Zhao. Diffsinger: Singing voice
synthesis via shallow diffusion mechanism. arXiv preprint arXiv:2105.02446, 2, 2021.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
10
Published as a conference paper at ICLR 2022
Shitong Luo and Wei Hu. Diffusion Probabilistic Models for 3D Point Cloud Generation. arXiv
preprint, 2021.
Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. CoRR,
abs/2102.09672, 2021.
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad
Norouzi. Image super-resolution via iterative refinement. arXiv preprint arXiv:2104.07636, 2021.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In
International Conference on Learning Representations, 2022.
Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsuper-
vised Learning using Nonequilibrium Thermodynamics. pp. 10, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. arXiv
preprint, pp. 1-19, 2020a.
Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.
arXiv:2006.09011 [cs, stat], October 2020a.
Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribu-
tion. arXiv:1907.05600 [cs, stat], October 2020b.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-Based Generative Modeling through Stochastic Differential Equations. pp. 1-32,
2020b.
Sauer Timothy. Numerical Analysis. Pearson, 3 edition, 2017. ISBN 2017028491, 9780134696454,
013469645X.
Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based Generative Modeling in Latent Space.
arXiv:2106.05931 [cs, stat], June 2021.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to Efficiently
Sample from Diffusion Probabilistic Models. CoRR, abs/2106.03802, 2021.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN:
Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.
arXiv:1506.03365 [cs], June 2016.
A Appendix
A. 1 Related Work
DDPMs have been well developed in the last few years. Some works concentrate on improving the
quality and the speed of DDPMs and making DDPMs more practical. Song et al. (2020a) introduce
new inference equations to accelerate DDPMs. Nichol & Dhariwal (2021), Watson et al. (2021) and
Kong & Ping (2021) choose to find better variance schedules to improve the images quality. Vahdat
et al. (2021) combine the advantages of DDPMs and Variational Autoencoders and get better results.
Furthermore, Kim et al. (2021) try to solve an existing bottleneck that the inference equations of
DDPMs are unbounded in some situations.
Song et al. (2020b) find the similarity between DDPMs and noise conditional score networks (NC-
SNs (Song & Ermon, 2020b)), which is that they both use a process similar to Langevin dynamics
to produce samples. Therefore, some works (Song & Ermon, 2020a; Kim et al., 2021) that can
improve the results of NCSNs also can be used in DDPMs. Additional, Song et al. (2020b) com-
bine DDPMs and NCSNs under the framework of neural differential equations (Chen et al., 2019;
Dupont et al., 2019). Therefore, numerical methods widely used in neural differential equations can
also be applied to accelerate DDPMs. Our work successfully combines the advantages of Song et al.
(2020a) and Song et al. (2020b). We use a transfer part from DDIMs and use different gradient parts
from different numerical methods. Although we and Song et al. (2020b) solve certain differential
11
Published as a conference paper at ICLR 2022
equations derived from DDPMs, we use different target different equations and different numerical
methods, which get better results.
The application of DDPMs is not limited to unconditional image generation. Some works apply
DDPMs to various types of data successfully, including text-to-speech (Chen et al., 2020; Lam
et al., 2021), singing voice (Liu et al., 2021), 3D Point Cloud (Luo & Hu, 2021), text generation
(Austin et al., 2021). Additionally, DDPMs can also be used to generate conditional samples, too
(Jeong et al., 2021; Choi et al., 2021).
A.2 Convergent Order of Method
We use the forward Euler method and linear multi-step method to show what is the order of a
method. Assume that xt is precise and compute the error at xt+δ . For forward Euler method, we
have:
et,δ = x(t + δ) - xt,δ
N(t) + δf(x(t),t) + Ef00(C)] -(x(t) + δf(xt,t))
(15)
δ2 δ2
δ2 f00 (C) ≤ δ2 M
Here, we assume that f00 is continuous, so f00 is bounded in a close area. x(t + δ) is the precise
result and xt,δ is the numerical result from t to t + δ.
For linear multi-step method, we have:
et,δ =x(t + δ) - xt,δ
=卜(t) +1!f (χ (t),t) + 2!f 0 (χ(t),t) +	+ 4!f (3)(χ (t),t) + O (δ5))
	
X(t) + b 1 δf (x(t),t) + X bsδ (X (TS-)1)δ)k f(kT)(X(t),t) + O
=O(δ5)
Here, {bs} satisfy P" bs = 1 and the following equations for j ∈ {1, ∙∙∙ , 3}:
1
(-1尸 b2 + ( -2尸 b3 + ( -3)j b4 = -- ∙
j+1
(16)
(17)
We call the error at xt,δ local error, and the error at xt+Mδ (M is big enough but finite) global error.
Assume the local error of our method has order k+1 and the target ODE satisfies Lipschitz condition,
then:
x (t + Mδ) - xt,Mδ ≤lx (t + Mδ) - xt+(M - 1)δ,δ | + lxt+(M - 1)δ,δ - xt,Mδ |
≤et+(M -1)δ,δ + eLδ |x(t + (M - 1)δ) - xt,(M -1)δ |
≤et+(M -1) δ,δ + eLδ et+(M - 2) δ,δ + …
≤Cδk+1 (1 + eLh + …+ e(i-1) Lh)
eiLδ	1	eiLδ	1
=Chk+1 -rξ_-1 ≤ Cδk+1-----------
eLδ - 1 ≤	Lδ
(18)
=O(δk)∙
Therefore, the global error will be one order lower than the local error. From Equation (15), we can
see the forward Euler method has local error O(δ2) and global error O(δ), so we call it the first-order
numerical method. And the linear multi-step method has local error O(δ5) and global error O(δ4),
and we call it the fourth-order numerical method.
In addition, assuming that a numerical method has kth-order global error, we can compute the con-
vergence speed of this numerical method:
lim x2+T - x(t + T) — (2δ)k — Dk
Um	2
δ→0 xtδ+T - x(t + T)	δk
(19)
12
Published as a conference paper at ICLR 2022
Here, xtδ+T is the result at t+T and move δ every step. This shows that the fourth-order method can
converge to the exact solution faster than the first-order method when δ → 0, which means that we
can use a bigger iteration interval δ to achieve similar global error and a bigger iteration interval
means that we can iterate fewer times to get results with high quality.
A.3 Pseudo Second-Order Method
We introduce two second-order numerical methods. First is improved Euler method satisfying:
k1 = f(xt, t)
k2 = f(xt + δk1, t + δ)
Xt+6 = Xt + δ (k 1 + k2)
(20)
Second is another linear multi-step method called second-order linear multi-step method satisfying:
δ
Xt+δ = Xt + 2(3ft - ft-δ)	QI)
And the corresponding pseudo improved Euler methods satisfying:
et = θ(Xt, t)
Xt = φ(xt, et,t,t + δ)
et2 = θ(Xt1, t + δ)	(22)
et = 2(e1 + e2)
、Xt+δ = φ (Xt,e∖,t,t + δ)
Pseudo second-order linear multi-step method satisfying:
{et = ^θ(Xtit)
et = 1 (3et - et-δ )	(23)
Xt+δ = φ(Xt, e0t,t,t + δ)
Similar to what we do to get F-PNDMs, We com-
bine them to get S-PNDMs. Abbreviate Equation
(22) and (23) as
Xt+δ, et = P IE(Xt, {ep}p<t, t, t + δ),
Xt+δ, et1 = PLMS0(Xt, t, t + δ).
Algorithm 3 S-PNDMs
1	XT 〜N(0,I)
2	for t = T - 1 do
3	xt,et = PIE(xt+1, t + 1,t)
4	end for
5	for t = T - 2, ∙∙∙ ,1, 0 do
6	xt, et = PLMS0(xt+1, {ep}p>t, t+1, t)
7	end for
8	return x0
A.4 The Existence of a Derivative
Because αt is usually obtained by multiplying a linear variance schedule βt. So We have
α = eι1t 2+b+c,
and ao = 1, so C = 0. Now we have
(24)
lim
δ→0
Xt-δ - Xt
lim at-δ- J
δ→0	δ
Xt
θ(Xt, t)
_____________________________________________
√at(√t-δ + √αt)	√αt(P(1 - αt-δ)αt + '(1 - αt)αt-)
lim
δ→0
αt-δ — αt ( Xt	ee(Xt,t)
(2 at + b) α t
Xt
2 α	2√τ-αt α
θ(Xt, t)
I = (e0t2+bt)0 (X%(χt,t)
V 2 α t	2 √ 1 - α t α t
2(2at + b)(Xt -√=α).
(25)
δ
δ
2 α	2√τ-αt α
13
Published as a conference paper at ICLR 2022
To make limδ→0 "［一如 ∣t=0 is well-defined, b must equal to zero, or (2at + b) eJ^∣) will tend to
infinity. This is a strong condition that most variance schedules do not satisfy. In practice, DDPMs
can choose the variance schedule very freely. This means that treating DDPMs as ODEs directly is
not proper and has theoretical weakness.
A.5 RELATIONSHIP BETWEEN t, θ AND xt
Relationship between t and θ. In Figure 5, we can see that the denoising process tends to converge,
whether in the θ domain or the sample/image domain when the step-index tends to zero. Therefore,
we can say that the noise becomes more and more precise when step, namely t, tends to zero.
Figure 5: The norm δ of the difference between two adjacent terms under different steps
Relationship between e§ and Xt To prove Property 3.1, assume that Xt = √αX0 + 1- - αte, NN
is the neural network and θ = NN(xt, t). Because we assume that the gradient part is precise, then
we have eθ = e. Then for all t0 ≤ t, we have:
√a (SX0 + 海√ge -湛Fe) + √T-0t0eθ(Xt, t)
ʌ/ α to X o + ʌ/- — α to e.
(26)
Here, we can find that x# = √0t0Xo + √1 一 αt e is also precise, so Property 3.1 is true.
A.6 Order Analysis of Pseudo Method
For the convenience of theoretical analysis, we generalize the problem. Let φ(X(t), e, t, δ) =
f (X(t), t, δ) + g(t, δ)e(X(t), t) and we have the property f (X(t), t, 0) = g(t, 0) = 0. Then we
have:
X(1) = X(0) +	(y(t + δ) - y(t))
δ→0
X(0) +	(f (X(t), t, δ) + g(t, δ)e(X(t), t))
δ→0
x(°)+ Z (∂δ(x(t),t, °)+ ∂δ(t, °)e(x(t), t)
(27)
Now, Equation (10) becomes a special case of this more general version and, in this special case, we
have:
f (X(t), t,δ)
Va (t +δ)
-1 x (t)
g (t, δ) = p1 - α (t + δ) - /(--α 少 (( 十 δ)
(28)
14
Published as a conference paper at ICLR 2022
Now, we compute the local error of S-PNDMs. We first compute the theoretical and numerical
results of different numerical methods. We have:
x (t + δ)
=X (t) + δ dfχ (x (t) ,t 0) + ∂g (t 0) E (X (t) It)) +
ɪ (∂δ(χ (t),t, 0) + ∂g(t, 0) e (x(t),t)) + °(δ 3)
=χ (t)+δ ∂δ(x (t),t, 0) + ∂δ(t, 0) E (x (t),t))+ 0(δ 3)+
~2 (∂δ∂t(x(t),t, 0) + ∂δ∖(x(t),t, 0) (∂δ(x(t),t, 0) + ∂g(t, 0)E(x(t),t))) +
2	∂ δ ∂ t	∂ δ ∂ x	∂ δ	∂ δ
ɪ ( ∂δ∂t(t, 0) E (x (t),t) + ∂gg(t, 0) e (x (t))t))
and
x S-PNDM (t + δ )
X(t) + f (X(t),t δ) + g(t, δ)1(E(X(t),t) + E(X(t) + φ(...,t δ),t + δ))
..一 ∂f —	.	δ2 ∂2 f 一 .
x(t) + δ∂δ(x(t), t, 0) + ɪ ∂δ2(x(t), t, 0) + 0(δ )+
卜 ∂g(t, 0) + ɪ ∂δg(t, 0)) 2( E (x (t),t)+E (x (t+δ) + °(δ 2),t+δ))
∂f	δ2 ∂2f
x(t) + δ∂δ(x(t), t, 0) + ^2^ ∂δ2(x(t), t, 0) + °(δ )+
卜 ∂g(t, 0) + Y ∂δg(t, 0)) 2( E (x (t),t)+E (x (t+δ),t+δ))
x(t) + δ^^r(x(t), t, 0) + M λλ2(x(t), t, 0) + °(δ3)+
∂δ	2 ∂δ2
δδgg^(t, 0) + ∖ dδg(t, 0)) 2( E (x(t),t)+E (x(t),t)+ δE (x(t),t )0)
x(t) + δ÷7(x(t), t, 0) + TT ^^τ2(x(t), t, 0) + °(δ3)+
∂δ	2 ∂δ2
δgg(t, 0) (E(x(t),t) + 1 δE(x(t),t)) + δgg(t, 0)E(x(t),t)
∂δ	2	2 ∂δ2
x(t)+ δ (~gδ(x(t),t, 0) + gδ(t, 0)E(x(t), t)) + °(δ3)+
(29)
(30)
Q
τ
(x(t),t, 0) + ∂δ(t, 0) E (x(t),t) 0 + ∂δ2(t, 0) E (x(t), t)) ∙
Then we compute the difference between the theoretical and numerical results. We have:
X (t + δ) 一 XS-PNDM (X + δ)
--δ2- ((gf 一 gf)(X(t),t, 0) + gf (x(t),t, 0)gf (x(t),t, 0)) +
2 ∂δ∂t ∂δ2	∂δ∂x	∂δ
δ2 ((gg—gg)(t, 0)+f (x (t) ,t, 0) gg (t, 0)) E (x (t) ,t)+° (δ3)
2 ∂δ∂t	∂δ2	∂δ∂x	∂δ
(31)
15
Published as a conference paper at ICLR 2022
In this special case, we compute the derivatives of some items needed in Equation (31). We have:
-a (t + δ)	，i — α( t) α (t + δ)
_
2√1 — a ((t + δ))	2 √α (t) a (t + δ)
df (x (t),t,δ) = ∂δ
—1
x(t)
=	&(t + δ)	x (t)
2p a (t) a (t + δ)
∂2f	a00 (t + δ)	—a0 (t + δ)2
熹(X (t),t,δ) |6=0 =	l	== X (t) +	l	= X (t) ∣6=0
∂δ2	2 a(t)a(t + δ)	4 a(t)a(t + δ)3
∂2f	a00 (t)	a0 (t)2
∂δ∂t(x (t),, 0) =2Ot) X (t)-2a(t)2 x (t)
∂2 f
∂δ∂x
(X(t), t, 0)
d2g (t δ) ∣ = -a0(t + δ)_______- α(t)a"(t + δ) +
^2( , 加=0 =2p1- a((t + 而 - 2pa(t)a(t + δf^+
-a (t + δ )2	+ ʌ/i - a (t) a (t + δ )2 ∣
4,1 - a(t + δ)3	4Pa(t)a(t + δ)3 JO
_	-a0 (t)	√ι - a (t) a0 (t)
=---/	- -----/	---+
2,1 - a((t))	2ʌ/a(t)a(t)
-a (t )2	pi - a (t) a (t )2
4,1 - a(t)3	4√a(t)a(t)3
d2g (t δ) ∣ = ∂- -a0(t)	- √ι -a(t)a(t)!
∂∂λ ,	=0 F <2√1 - a((t/ - 2√a(t)a(t)一)
(32)
(33)
(34)
(35)
(36)
(37)
-a0 (t)	√ι - a (t) a0 (t)
_,	`'	-」———==-+
2∖Ji - a((t))	2ʌ/a(t)a(t)
(38)
-a (t )2	ʌ/i - a (t) a (t )2	a (t )2
4√1 - a(t)3 +	2a(t)2	+ 4√a2(t)(1 - a(t))
Now we can compute the final result of Equation (31). We split it into three parts and the values of
the first two terms. We have:
(∂δ∂t - ∂δ2)(X(t),t,0) + ∂δ∂X(X(t),t,0)∂(X(t),t,0)
a (t )2
2 a (t )2
X(t)
X(t) +
-a (t )2
4 a (t )2
X(t)
1 a0 (t)
+ 2at 2a(t)X(t)
(39)
当 X (t) -
ʌ-(3
厂卜所
a (t)
2 a (t)
=0
and
/ ∂2 g	∂2 g	∂2 f	.g ∂g
(∂∂ - ∂δ2)(t,0) + ∂δ∂X(X(t),t,0)∂(t,0)
√ι - a(t)a(t)2	a(t)2	a(t) -	-a(t)	√ι - a(t)a(t)
4a(t)2	+ 4a(t)√1 - a(t) + 2a(t) (2√1 - a((t))	2a(t)
a(t)2 a(t) -	-a(t)	∖
=4a(t)2√T-a(t) + 2a(t) (2√l - a(t)而)
=0
16
Published as a conference paper at ICLR 2022
Finally, we get the final result of Equation (31):
x(t+ δ) - xS-PNDM(x + δ) = O(δ3)	(41)
And the computation of the convergence order of F-PNDMs is similar, and we ignore it here. There-
fore, Property 3.2 is true.
A.7 Variance schedule
According to Cifar10 (cosine) in Table 2, PNDMs can be used on both linear variance schedule and
cosine variance schedule. However, we also notice cosine variance schedule can make FID lower
when we use relatively big generation steps, but the effort is limited when the number of steps is
small. F-PNDM uses information from four consecutive steps, so the smoothness of the schedule is
more important for F-PNDM than DDIM. According to this experiment, our work can be used with
works that pay attention to variance schedules to improve the acceleration effect further. However,
a variance schedule that fits pseudo numerical methods better remains to be found in further work.
A.8 Toy example
Here, we design a toy example to test our new methods without the influence of neural networks.
We randomly generate the initial input x 1 = (m 1 ,m2), mi 〜U(0,1) and use a simple analytic
equations θ(x) = (sin x[0], cos x[1]) to replace the neural networks in real cases. Let φ in Equation
(11) is unchanged and &t = α (t) = 1 一 t, then we get:
φ(xt, θ(xt), t, t - δ)
(42)
.	/ ,-	..------,	^→θ (Xt)
√1 — t (-p∕(t - δ)(1 - t) + Pt(I 一(t 一 δ)))
Here, we use three different numerical methods to generate x0 .
For DDIM, we have:
xt-δ = xt + φ(xt, θ(xt), t, t 一 δ)
For FON, according to Equation (10), we have:
et = α0 (t)(鼻-------eθ (Xt)	)
t	∖2 α( t) 2 α( t )√1 - α( t))
=_ (	Xt_______Cθ (Xt) ∖
=一 21—τ^- - 2(1- t)√)
δ
xt-δ = χt + 24(55et - 59et+δ + 37et+2δ - 9et+3δ)
For F-PNDM, we have:
e = 24(55S(χt) - 59S(χt+δ) + 37S(χt+2δ) - 9S(χt+3δ))
Xt-δ = Xt + φ(Xt, e0, t, t - δ)
(43)
(44)
(45)
Then we draw the corresponding generation curves in Figure 6. We find that the result is similar
to the real cases. The main difference here is that FON can correct its results while the real case
cannot. The reason is that the gradient is well-defined everywhere, while in real cases, the gradient
is meaningful on the high-density region of the data Xt of DDPMs.
A.9 More FID Results
Here, We provide our more detailed FID results on Cifar10, CelebA, LSUN-church and LSUN-
bedroom in Table 3, 4, 5, 6.
17
Published as a conference paper at ICLR 2022
Figure 6: The generation curve of our toy example.
steps	5	10	20	25	40	50	100	125	200	250	500	1000
DDIM		13.4		6.84		4.67	4.16					4.04
DDIM*	44.5	18.5	10.9	9.61	7.65	6.99	5.52	5.19	4.69	4.52	4.17	4.00
FON	98.0	13.1	7.41	6.41	5.50	5.26	4.65	4.54	4.23	4.12	3.84	3.71
S-PNDM	22.8	11.6	7.56	6.79	5.57	5.18	4.34	4.18	3.97	3.91	3.81	3.80
F-PNDM	13.9	7.03	5.00	4.76	4.10	3.95	3.72	3.64	3.60	3.60	3.64	3.70
DDIM*	28.7	14.5	8.79	7.83	6.41	5.86	4.92	4.75	4.42	4.30	3.98	3.69
S-PNDM	18.3	8.64	5.77	5.45	4.76	4.46	3.94	3.85	3.69	3.71	3.60	3.38
F-PNDM	18.2	7.05	4.61	4.32	3.85	3.68	3.53	3.46	3.47	3.49	3.44	3.26
Table 3: Cifar10 image generation measured in FID. The upper part uses linear variance schedule
and the bottom half uses cosine variance schedule. The first line shows the FID provided by Song
et al. (2020a).
steps	5	10	20	25	40	50	100	125	200	250	500	1000
DDIM		17.3		13.7		9.17	6.53					3.51
DDIM*	24.4	16.9	13.4	12.3	9.99	8.95	6.36	5.74	4.78	4.44	3.75	3.41
FON	60.2	16.0	11.6	10.6	8.89	8.13	6.70	6.28	5.45	5.14	4.49	4.17
S-PNDM	15.2	12.2	9.45	8.42	6.50	5.69	4.03	3.72	3.30	3.19	3.01	2.99
F-PNDM	11.3	7.71	5.51	4.75	3.67	3.34	2.81	2.75	2.71	2.71	2.77	2.86
Table 4: CelebA image generation measured in FID. All of them use linear variance schedule.
steps	5	10	20	25	40	50	100	125	200	250
DDIM		19.5	12.5			10.8	10.6			
DDIM*	48.8	18.8	11.7	11.0	10.1	10.0	9.84	9.83	9.85	9.88
S-PNDM	20.5	11.8	9.20	9.13	9.31	9.49	9.82	9.88	10.0	10.0
F-PNDM	14.8	8.69	9.13	9.33	9.69	9.89	10.1	9.99	10.1	10.1
Table 5: LSUN-church image generation measured in FID. All of them use linear variance schedule.
18
Published as a conference paper at ICLR 2022
steps	5	10	20	25	40	50	100	125	200	250
DDIM		17.0	8.89			6.75	6.62			
DDIM*	51.3	16.4	8.47	7.41	6.27	6.05	5.97	6.03	6.23	6.32
S-PNDM	18.1	10.2	6.50	6.02	5.74	5.81	6.29	6.44	6.69	6.75
F-PNDM	12.6	6.99	5.68	5.74	6.17	6.44	6.91	6.96	7.03	6.92
Table 6: LSUN-bedroom image generation measured in FID. All of them use linear variance sched-
ule.
A.10 More Image Results
Here, we show more generated images on Cifar10, CelebA, LSUN-church and LSUN-bedroom in
Figure 7, 9, 8, 10, 11, 12.
Figure 7: 5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical
methods and PNDMs on Cifar10.
Figure 8: 5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical
methods and PNDMs on CelebA.
19
Published as a conference paper at ICLR 2022
Figure 9: Generated images of PNDMs on Cifar10.
Figure 10: Generated images of PNDMs on CelebA.
20
Published as a conference paper at ICLR 2022
Figure 11: 5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and
PNDMs on LSUN-church.
21
Published as a conference paper at ICLR 2022
Figure 12: 5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and
PNDMs on LSUN-bedroom.
22
Published as a conference paper at ICLR 2022
A.11 More Visualization Results
Here, we put more visualization results similar to Figure 4 in Figure 13.
0.80	0.85 OJO O.«5	1.00	1.05	1.10	1.15
0.80	0.85 OJO O.«5	1.00	1.05	1.10	1.15
Figure 13: Visualization results under 5, 10, 20, 25, 40 and 50 steps.
A.12 FID result with Standard Deviation
Here, we report the mean and standard deviation of FID results, tested over four sampling runs.
dataset	FID step model	10	20		100	250	1000
				50			
	DDIM*	18.50 ± .06	10.86 ± .08	6.95 ± .04	5.49 ± .06	4.52±.02	4.02±.04
Cifar10	FON	13.00 ± .11	7.33±.06	5.24 ± .05	4.64 ± .04	4.12±.03	3.73±.03
(linear)	S-PNDM	11.58 ± .10	7.53±.07	5.15 ± .05	4.34 ± .03	3.93±.02	3.83±.03
	F-PNDM	6.12 ± .07	5.04±.07	4.01 ± .02	3.75 ± .04	3.67±.03	3.78±.04
Table 7: Image generation measured in FID on Cifar10. DDIM* means a kind of pseudo numerical
method and also a retest of DDIM.
23