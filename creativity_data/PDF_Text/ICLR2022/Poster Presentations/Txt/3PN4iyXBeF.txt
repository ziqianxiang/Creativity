Published as a conference paper at ICLR 2022
Amortized Implicit Differentiation for
Stochastic Bilevel Optimization
Michael Arbel & Julien Mairal
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
Ab stract
We study a class of algorithms for solving bilevel optimization problems in both
stochastic and deterministic settings when the inner-level objective is strongly
convex. Specifically, we consider algorithms based on inexact implicit differenti-
ation and we exploit a warm-start strategy to amortize the estimation of the exact
gradient. We then introduce a unified theoretical framework inspired by the study
of singularly perturbed systems (Habets, 1974) to analyze such amortized algo-
rithms. By using this framework, our analysis shows these algorithms to match
the computational complexity of oracle methods that have access to an unbiased
estimate of the gradient, thus outperforming many existing results for bilevel op-
timization. We illustrate these findings on synthetic experiments and demonstrate
the efficiency of these algorithms on hyper-parameter optimization experiments
involving several thousands of variables.
1	Introduction
Bilevel optimization refers to a class of algorithms for solving problems with a hierarchical structure
involving two levels: an inner and an outer level. The inner-level problem seeks a solution y? (x)
minimizing a cost g(x, y) over a set Y given a fixed outer variable x in a set X . The outer-level
problem minimizes an objective of the form L(x)=f (x, y?(x)) over X for some upper-level cost f.
When the solution y? (x) is unique, the bilevel optimization problem takes the following form:
min L(x) := f(x, y?(x)),	such that y?(x) = arg min g(x, y).	(1)
x∈X	y∈Y
First introduced in the field of economic game theory by Stackelberg (1934) and long studied in
optimization (Ye and Zhu, 1995; Ye and Ye, 1997; Ye et al., 1997), this problem has recently received
increasing attention in the machine learning community (Domke, 2012; Gould et al., 2016; Liao
et al., 2018; Blondel et al., 2021; Liu et al., 2021; Shaban et al., 2019; Ablin et al., 2020). Indeed,
many machine learning applications can be reduced to (1) including hyper-parameter optimization
(Feurer and Hutter, 2019), meta-learning (Bertinetto et al., 2018), reinforcement learning (Hong
et al., 2020b; Liu et al., 2021) or dictionary learning (Mairal et al., 2011; Lecouat et al., 2020a;b).
The hierarchical nature of (1) introduces additional challenges compared to standard optimization
problems, such as finding a suitable trade-off between the computational budget for approximating
the inner and outer level problems (Ghadimi and Wang, 2018; Dempe and Zemkoho, 2020). These
considerations are exacerbated in machine learning applications, where the costs f and g often come
as an average of functions over a large or infinite number of data points (Franceschi et al., 2018).
All these challenges highlight the need for methods that are able to control the computational costs
inherent to (1) while dealing with the large-scale setting encountered in machine learning.
Gradient-based bilevel optimization methods appear to be viable approaches for solving (1) in large-
scale settings (Lorraine et al., 2020). They can be divided into two categories: Iterative differenti-
ation (ITD) and Approximate implicit differentiation (AID). ITD approaches approximate the map
y? (x) by a differentiable optimization algorithm A(x) viewed as a function of x. The resulting
surrogate loss L(x) = f(x, A(x)) is optimized instead of L(x) using reverse-mode automatic dif-
ferentiation (see Baydin et al., 2018). AID approaches (Pedregosa, 2016) rely on an expression
of the gradient VL resulting from the implicit function theorem (Lang, 2012, Theorem 5.9). Un-
like ITD, AID avoids differentiating the algorithm approximating y? (x) and, instead, approximately
1
Published as a conference paper at ICLR 2022
solves a linear system using only Hessian and Jacobian-Vector products to estimate the gradient VL
(Rajeswaran et al., 2019). These methods can also rely on stochastic approximation to increase
scalability (Franceschi et al., 2018; Grazzi et al., 2020; 2021).
In the context of machine-learning, Ghadimi and Wang (2018) provided one of the first compre-
hensive studies of the computational complexity for a class of bilevel algorithms based on AID
approaches. Subsequently, Hong et al. (2020b); Ji et al. (2021); Ji and Liang (2021); Yang et al.
(2021) proposed different algorithms for solving (1) and obtained improved overall complexity by
achieving a better trade-off between the cost of the inner and outer level problems. Still, the ques-
tion of whether these complexities can be improved by better exploiting the structure of (1) through
heuristics such as warm-start remains open (Grazzi et al., 2020). Moreover, these studies pro-
posed separate analysis of their algorithms depending on the convexity of the loss L and whether a
stochastic or deterministic setting is considered. This points out to a lack of unified and systematic
theoretical framework for analyzing bilevel problems, which is what the present work addresses.
We consider the Amortized Implicit Gradient Optimization (AmIGO) algorithm, a bilevel opti-
mization algorithm based on Approximate Implicit Differentiation (AID) approaches that exploits a
warm-start strategy when estimating the gradient of L. We then propose a unified theoretical frame-
work for analyzing the convergence of AmIGO when the inner-level problem is strongly convex in
both stochastic and deterministic settings. The proposed framework is inspired from the early work
of Habets (1974) on singularly perturbed systems and analyzes the effect of warm start by viewing
the iterates of AmIGO algorithm as a dynamical system. The evolution of such system is described
by a total energy function which allows to recover the convergence rates of unbiased oracle methods
which have access to an unbiased estimate of VL (c.f. Table 1). To the best of our knowledge, this
is the first time a bilevel optimization algorithm based on a warm-start strategy provably recovers
the rates of unbiased oracle methods across a wide range of settings including the stochastic ones.
2	Related work
Singularly perturbed systems (SPS) are continuous-time deterministic dynamical systems of cou-
pled variables (x(t), y(t)) with two time-scales where y(t) evolves much faster than x(t). As such,
they exhibit a hierarchical structure similar to (1). The early work of Habets (1974); Saberi and
Khalil (1984) provided convergence rates for SPS towards equilibria by studying the evolution of a
single scalar energy function summarizing these systems. The present work takes inspiration from
these works to analyze the convergence of AmIGO which involves three time-scales.
Two time-scale Stochastic Approximation (TTSA) can be viewed as a discrete-time stochastic ver-
sion of SPS. (Kaledin et al., 2020) showed that TTSA achieves a finite-time complexity of O -1
for linear systems while Doan (2020) obtained a complexity of O -3/2 for general non-linear
systems by extending the analysis for SPS. Hong et al. (2020b) further adapted the non-linear TTSA
for solving (1). In the present work, we obtain faster rates by taking into account the dynamics of a
third variable zk appearing in AmIGO, thus resulting in a three time-scale dynamics.
Warm-start in bilevel optimization. Ji et al. (2021); Ji and Liang (2021) used a warm-start for
the inner-level algorithm to obtain an improved computational complexity over algorithms without
warm-start. In the deterministic non-convex setting, Ji et al. (2021) used a warm-start strategy when
solving the linear system appearing in AID approaches to obtain improved convergence rates. How-
ever, it remained open whether using a warm-start when solving both inner-level problem and linear
system arising in AID approaches can yield faster algorithms in the more challenging stochastic
setting (Grazzi et al., 2020). In the present work, we provide a positive answer to this question.
3	Amortized Implicit Gradient Optimization
3.1	General setting and main assumptions
Notations. In all what follows, X and Y are Euclidean spaces. For a differentiable function h(x, y) :
X × Y → R, we denote by Vh its gradient w.r.t. (x, y), by ∂xh and ∂yh its partial derivatives w.r.t.
x and y and by ∂xyh and ∂yyh the partial derivatives of ∂yh w.r.t x and y, respectively.
2
Published as a conference paper at ICLR 2022
Geometries	Setting	Algorithms	Complexity
	(D)	BA (Ghadimi and Wang, 2018)	O(κL ∨ κ2 log2 e-1) O(KL"κg∕2 log2 e-1)
		AccBiO (Ji and Liang, 2021)	
(SC)		AmIGO (Corollary 1)	O(KLKg log e-1)
	(S)	BSA (Ghadimi and Wang, 2018)	_	O(KLe-2L	.
		TTSA (Hong et al., 2020b)	O(kL.5(k8.5 + KL)e-3∕2 loge-1)
		AmIGO (Corollary 2)	O(KLKg e-1 log e-1)
	(D)	BA (Ghadimi and Wang, 2018)	O(Kg e-5/4)
		AID-BiO (Ji etal., 2021)	O(Kg e-1)
(NC)		AmIGO (Corollary 3)	O(Kg e-1)
	(S)	BSA (Ghadimi and Wang, 2018)	O(Kge-3 + Kg e-2)	一
		TTSA (Hong et al., 2020b)	O(K16e-5/2 bg e-1) 一
		StOcBiO(Ji et al., 2021)	O(Kge-2+Kge-2 log e-1)	一
		MRBO/VRBO? (Yang et al., 2021)	O(poly(Kg)e-3/2 log e-1)
		AmIGO (Corollary 4)	O(k9e-2)	
Table 1: Cost of finding an e-accurate solution as measured by E[L(χk)-L*]∧2-1μE [∣∣xk-x?『]
when L is μ-strongly-convex (SC) and k Pk=I E ]▽£(Xi)『]When L is non-convex (NC). The
settings (D) and (S) stand for the deterministic and stochastic settings. The cost corresponds to the
total number of gradients, Jacobian and Hessian-vector products used by the algorithm. κL and κg
are the conditioning numbers of L and g whenever applicable. The dependence on κL and κg for
TTSA and AccBio are derived in Proposition 11 of Appendix A.4. The rate of MRBO/VRBO is
obtained under the additional mean-squared smoothness assumption (Arjevani et al., 2019).
To ensure that (1) is well-defined, we consider the setting where the inner-level problem is strongly
convex so that the solution y?(x) is unique as stated by the following assumption:
Assumption 1. For any X ∈ X, thefunCtion y → g(x, y) is Lg-smooth and μg-strongly convex.
Assumption 1 holds in the context of hyper-parameter selection when the inner-level is a kernel
regression problem (Franceschi et al., 2018), or when the variable y represents the last linear layer
of a neural network as in many meta-learning tasks (Ji et al., 2021). Under Assumption 1 and
additional smoothness assumptions on f and g, the next proposition shows that L is differentiable:
Proposition 1. Let g be a twice differentiable function satisfying Assumption 1. Assume that f is
differentiable and consider the quadratic problem:
min Q(x,y,z) ：= 1 z>(∂yyg(x,y))z + ZTdyf(X,y).	(2)
Then, (2) admits a unique minimizer z?(X, y) for any (X, y) in X × Y. Moreover, y? (X) is unique
and well-defined for any X in X and L is differentiable with gradient given by:
VL(X) = ∂χf(x, ∙y*(Xy) + ∂χyg(x, y*(x))z*(x, y*(x)).	(3)
Proposition 1 follows by application of the implicit function theorem (Lang, 2012, Theorem 5.9)
and provides an expression for VL solely in terms of partial derivatives of f and g evaluated at
(X, y? (X)). Following Ghadimi and Wang (2018), we further make two smoothness assumptions on
f and g :
Assumption 2. There exist positive constants Lf and B such that for all X, X0 ∈ X and y, y0 ∈ Y :
kVf(X, y) - Vf (X0, y0)k ≤ Lfk(X,y) - (X0, y0)k,	k∂y f (X, y)k ≤ B.
Assumption 3. There exit positive constants L0g, Mg such that for any X, X0 ∈ X and y, y0 ∈ Y:
max{k∂xyg(X,y) - ∂xyg(X0,y0)k, k∂yyg(X, y) - ∂yy g(X0, y0)k} ≤ Mgk(X,y) - (X0, y0)k
k∂yg(X, y) - ∂yg(X0,y)k ≤ L0g kX - X0k.
3
Published as a conference paper at ICLR 2022
Assumptions 1 to 3 allow a control of the variations of y? and z? and ensure L is L-smooth for
some positive constant L as shown in Proposition 6 of Appendix B.2. As an L-smooth function, L is
necessarily weakly convex (Davis et al., 2018), meaning that L satisfies the inequality L(x)-L(y) ≤
VL(x)>(x - y) - 2∣∣x - y∣∣2 for some fixed μ ∈ R with ∣μ∣ ≤ L. In particular, L is convex When
μ ≥ 0, strongly convex when μ > 0 and generally non-convex when μ < 0. We thus consider two
cases for L, the strongly convex case (μ > 0) and the non-convex case (μ < 0). When L is convex,
we denote by L? its minimum value achieved at a point x? and define κc=L∕μ when μ > 0.
Stochastic/deterministic settings. We consider the general setting where f(x, y) and g(x, y) are
expressed as an expectation of stochastic functions f (x, y, ξ) and g(x, y, ξ) over a noise variable ξ.
We recover the deterministic setting as a particular case when the variable ξ has zero variance, thus
allowing us to treat both stochastic (S) and deterministic (D) settings in a unified framework. As
often in machine-learning, we assume we can always draw a new batch D of i.i.d. samples of the
noise variable ξ with size |D| ≥ 1 and use it to compute stochastic approximations off andg defined
by abuse of notation as f(x, y, D):=由 pξ∈τ, f(x, y, ξ) and g(x, y, D)：=由 Pξ∈τ> ^(x, y ξ)∙
We make the following noise assumptions which are implied by those in Ghadimi and Wang (2018):
Assumption 4. For any batch D, Vf (x,y, D) and ∂yg(x,y, D) are unbiased estimator of Vf (x, y)
and ∂yg(x, y) with a uniformly bounded variance, i.e. for all x, y ∈ X × Y:
E ∣∣V.f(x,y,D) -Vf(x,y)∣∣2j ≤ σ2|D|-1,	EhMyg(x,y,D) - ∂yg(x,y)『]≤ σg∣D∣-1.
Assumption 5. For any batch D, the matrices Fι(x,y, D) := ∂xyg(x,y, D) — ∂xyg(x,y) and
F2(x, y, D) := ∂yyg(x, y, D) — ∂yyg(x, y) have zero mean and SatiSfyfOr all x,y ∈ X X Y:
∣∣E[Fι(χ,y,D)>Fι(χ,y,D)]∣∣op ≤ σgχy|D|-1,	∣∣E[F2(χ,y,D)>F2(χ,y,D)] Q ≤ σgyy|D|-1.
For conciseness, we will use the notations σf:=32|D|-1, σg=σg∣D∣-1, σgxy=σgxy∣D∣-1 and
σgyy :=3gyy |D|-1, without explicit reference to the batch D. Next, we describe the algorithm.
3.2	Algorithms
Amortized Implicit Gradient Optimization (AmIGO) is an iterative algorithm for solving (1). It
constructs iterates xk, yk and zk such that xk approaches a stationary point of L while yk and zk
track the quantities y?(xk) and z?(xk, yk). AmIGO computes the iterate xk+1 using an update
equation xk+1 = xk - γkψk for some given step-size γk and a stochastic estimate ψk of VL(xk)
based on (3) and defined according to (4) below for some new batches of samples Df and Dgxy .
>
ψk := ∂χf (xk,yk, Df) + ∂χ,yg(xk,yk, Dgxy) Zk.	(4)
Algorithm 1 AmIGO
1:	Inputs: x0, y-1, z-1.
2:	Parameters: γk, K.
3:	for k ∈ {0, ..., K} do
4:	yk J Ak(χk,yk-ι)
5:	Sample batches Df, Dg.
6:	(uk,vk) J Vf(xk,yk,Df).
7:	zk J Bk(xk, yk, vk, zk-1)
8:	Wk J ∂χy g(χk ,yk, Dgxy )zk
9:	ψk-1 J uk + wk
10:	xk J xk-1 - γkψk-1
11:	end for
12:	Return xK .
AmIGO computes ψk in 4 steps given iterates xk, yk-1
and zk-1. A first step computes an approximation yk
to y?(xk) using a stochastic algorithm Ak initialized at
yk-1. A second step computes unbiased estimates uk =
∂xf(xk,yk,Df) and vk = ∂yf(xk,yk,Df) of the partial
derivatives of f w.r.t. x and y. A third step computes an
approximation zk to z?(xk, yk) using a second stochas-
tic algorithm Bk for solving (2) initialized at zk-1. To
increase efficiency, algorithm Bk uses the pre-computed
vector vk for approximating the partial derivative ∂y f in
(2). Finally, the stochastic estimate ψk is computed us-
ing (4) by summing the pre-computed vector uk with the
jacobian-vector product Wk = ∂xyg (Xk, yk, Dgxy) Zk.
AmIGO is summarized in Algorithm 1.
Algorithms Ak and Bk . While various choices for Ak and Bk are possible, such as adaptive algo-
rithms (Kingma and Ba, 2015), or accelerated stochastic algorithms (Ghadimi and Lan, 2012), we
4
Published as a conference paper at ICLR 2022
focus on simple stochastic gradient descent algorithms with a pre-defined number of iterations T and
N. These algorithms compute intermediate iterates yt and zn optimizing the functions y 7→ g(xk, y)
and z 7→ Q(xk , yk , z) starting from some initial values y0 and z0 and returning the last iterates yT
and zN as described in Algorithms 2 and 3. Algorithm Ak updates the current iterate yt-1 using
a stochastic gradient ∂yg(xk,yt-1, Dg) for some new batch of samples Dg and a fixed step-size
αk. Algorithm Bk updates the current iterate zt-1 using a stochastic estimate of ∂zQ(xk, yk, zt-1)
with step-size βk . The stochastic gradient is computed by evaluating the Hessian-vector product
∂yyg(xk, yk, Dgyy )zt-1 for some new batch of samples Dgyy and summing it with a vector Vk ap-
proximating ∂yf(xk, yk) provided as input to algorithm Bk.
Warm-start for y0 and z0. Following the intuition that y?(xk) remains close to y?(xk-1) when
xk ' xk-1, and assuming that yk-1 is an accurate approximation to y?(xk-1), it is natural to initial-
ize Ak with the iterate yk-1. The same intuition applies when initializing Bk with zk-1. Next, we
introduce a framework for analyzing the effect of warm-start on the convergence speed of AmIGO.
Algorithm 2 Ak (x, y0)	Algorithm 3 Bk (x, y, v, z0)
1:	Parameters: αk, T	1:	Parameters: βk, N.
2:	for t ∈ {1, ..., T} do	2:	for n ∈ {1, ..., N} do
3:	Sample batch Dtg,k.	3:	Sample batch Dngy,ky .
4:	yt — yt-1 — αk∂yg(x,yt-1, Dgk] ,	4:	Zn 一 ZnT-βk (∂yyg (x, y, Dnyy) zn-1 + v)
5:	end for	5:	end for
6:	Return y T .	6:	Return ZN .
4 Analysis of Amortized Implicit Gradient Optimization
4.1	General approach and main result
The proposed approach consists in three main steps: (1) Analysis of the outer-level problem , (2)
Analysis of the inner-level problem and (3) Analysis of the joint dynamics of both levels.
Outer-level problem. We consider a quantity Ekx describing the evolution ofxk defined as follows:
Ex =J2YkEhkxk-x*/] +(1-u)E[L(xk) - L?], μ ≥ 0
k L [ 2⅛ EhkVL(xk)k2],	μ< 0.
where u ∈ {0, 1} is set to 1 in the stochastic setting and to 0 in the deterministic one and δk is a
positive sequence that determines the convergence rate of the outer-level problem and is defined by:
δk ：= ηkYk,	ηk+ι ：= ηk(1 + γk+ι(ηk — μ))iμ≥o + Liμ<o.
with no such that γ-1≥η0≥μ if μ≥0 and ηo=L if μ<0 and where we choose the step-size Yk to be
a non-increasing sequence with γo ≤ L. With this choice for δk and by setting U = 1 in (5), Ek re-
covers the quantity considered in the stochastic estimate sequences framework of Kulunchakov and
Mairal (2020) to analyze the convergence of stochastic optimization algorithms when L is convex.
When L is non-convex, Ekx recovers a standard measure of stationarity (Davis and Drusvyatskiy,
2018). In Section 4.3, we control Ek using bias and variance error Eψ-ι and Vψ-ι of ψk given by
(6) below where Ek denotes expectation conditioned on (xk, yk, zk-1).
Eψ ：= E ∣∣Ek [Ψk] — VL(xk)∣∣2
Vk := E ∣∣ψk — Ek[ψk]∣∣2 .	(6)
Inner-level problems. We consider the mean-squared errors Eky and Ekz between initializations
(y0=yk-1 and z0=zk-1) and stationary values (y?(xk) and z?(xk, yk)) of algorithms Ak and Bk:
Ek ：= E[∣∣y0 — y*(xk)∣∣2],	Ek ：= e[- - z?(xk,y)∣∣2].
In Section 4.3, we show that the warm-start strategy allows to control Eky and Ekz in terms of
previous iterates Eky-1 and Ekk-1 as well as the bias and variance errors in (6). We further prove that
such bias and variance errors are, in turn, controlled by Eky and Ekk .
5
Published as a conference paper at ICLR 2022
Joint dynamics. Following Habets (1974), we consider an aggregate error Ektot defined as a linear
combination of Ekx, Eky and Ekz with carefully selected coefficients ak and bk :
Ektot = Ekx +akEky +bkEkz.	(7)
As such Ektot represents the dynamics of the whole system. The following theorem provides an error
bound for Ektot in both convex and non-convex settings for a suitable choice of the coefficients ak
and bk provided that T and N are large enough:
σ2
Theorem 1. Choose a batch-size ∣Dgyy ∣≥1∨μ-Ly- and the step-sizes ak =L-1 βk=(2Lg)-1,
Yk =LT. Set the coefficients ak and bk to be au=δo(1-akμg)1/2 and bk:=60(1 — ɪβkμg)1/2
and set the number of iterations T and N of Algorithms 2 and 3 to be of order T =O(κg ) and
N=O(Kg) UP to a logarithmic dependence on Kg. Let Xk=u(1-δk)^k-1+(1-u(1-δk))xk, with
Xo=x0. Then, under Assumptions 1 to 5, Eko satisfies:
E[L(Xk) — L?] + Ekot ≤(1 — (2kl)-1) k [E0t^t + E[L(xo) — L?]] + 军,
1 k	2	2W 2
EEtOt ≤ τ(E[L(xo) -L?]+ Ey + EZ) + -W,
kk	L
t=1
μ ≥ 0
μ < 0,
where W 2, defined in (20) of Appendix A.2, is the effective variance of the problem with W 2=0 in
the deterministic setting and, in the stochastic setting, W 2 >0 is of the following order:
W2 =O(δ-1κg IDg I-1 σg+κg∣Dgyy∣-1σgyy+κ2∣%y∣-1σgχy + 埼 IDf ∣-1*),
We describe the strategy of the proof in Section 4.3 and provide a proof outline in Appendix A.1
with exact expressions for all variables including the expressions of T, N and W 2 . The full proof
is provided in Appendix A.2. The choice of ak and bk ensures that Eky and Ekz contribute less to
Ektot as the algorithms Ak and Bk become more accurate. The effective variance W 2 accounts for
interactions between both levels in the presence of noise and becomes proportional to the outer-level
variance σf2 when the inner-level problem is solved exactly. In the deterministic setting, all variances
σf, σg,σgxy and σg2yy vanish so that W2 =0. Hence, We characterize such setting by W2=0 and the
stochastic one by W 2>0. Next, we apply Theorem 1 to obtain the complexity of AmIGO.
4.2	Complexity analysis
We define the complexity C() of a bilevel algorithm tobe the total number of queries to the gradients
of f and g, Jacobian/hessian-vector products needed by the algorithm to achieve an error according
to some pre-defined criterion. Let the number of iterations k, T and N and sizes of the batches IDgI,
IDf I, ∣Dgxy ∣ and ∣Dgyy ∣, be such that AmIGO achieves a precision . Then C() is given by:
C(e) = k(T|Dg∣ + NIDgyyI + 回。“ ∣ + |Df ∣),	(8)
We provide the complexity of AmIGO in the 4 settings of Table 1 in the form of Corrolaries 1 to 4 .
Corollary 1 (Case μ>0 and W 2=0). Use batches ofsize 1. Achieving L(Xk)-L?+例同—x*∣∣2≤e
requires C(e)=O( KLKg log ( E-)).
Corollary 1 outperforms the complexities in Table 1 in terms of the dependence on e. It is pos-
sible to improve the dependence on Kg to Kg1/2 using acceleration in Ak and Bk as discussed in
Appendix A.5.1, or using generic acceleration methods such as Catalyst (Lin et al., 2018).
Corollary2(CaseμW2>0). Choose |Dg∣=θ(e-1KLκgσg), ∣DgxyI=θ(e-1σgxy), |Df ∣=θ(σf)
and ∣Dgyy ∣=Θ KigyJɪ ∨ Kg)). Achieving E[L(Xk)—L?]+μE [∣∣Xk
— X? k2 ≤e requires:
C(e)=O卜 L (KLK σg + Kg (1 ∨ eKg )σgyy + σgxy +	^	(EL^EL(XO)-^
6
Published as a conference paper at ICLR 2022
Corollary 2 improves over the results in Table 1 in the stochastic strongly-convex setting and recov-
ers the dependence on of stochastic gradient descent for smooth and strongly convex functions up
to a logarithmic factor.
Corollary 3 (Case μ<0 and W2=0). Choose batches ofsize 1. Achieving 1 Pk=Ill▽£(Xi)『≤ e
requires C(e) = O (-g-((L(xo) — L?) + Ey + Ez)).
Corollary 3 recovers the complexity of AID-BiO (Ji et al., 2021) in the deterministic non-convex
setting. This is expected since AID-BiO also exploits warm-start for both Ak and Bk.
Corollary 4 (Case μ<0 and W>0). Choose IDgxj=Θ(WσgJ IDgyyI=θ(K3σ2/1 ∨ eμg)),
|Df ∣=θ(κgf ) and |Dg∣=θ(κgσg). Achieving an error 1 Pk=I EhIl▽£(Xi)『]≤ e requires:
C(e) = O(Kg (κ4σg + κg (1 ∨ eμg)σ% + 5鼠 + σf) (E[L(xk) - L?] + Ey + Ez))
Corollary 4 recovers the optimal dependence on e of O( 5) achieved by stochastic gradient descent
in the smooth non-convex case (Arjevani et al., 2019, Theorem 1). It also improves over the results
in (Ji et al., 2021) which involve an additional logarithmic factor log(e-1) as N is required to be
O(κg log(e-1)). In our case, N remains constant since Bk benefits from warm-start. The faster rates
of MRBO/VRBO? (Yang et al., 2021) are obtained under the additional mean-squared smoothness
assumption (Arjevani et al., 2019), which we do not investigate in the present work. Such assump-
tion allows to achieve the improved complexity of O(e-3/2 log(e-1)). However, these algorithms
still require N =O(log(e-1)), indicating that the use of warm-start in Bk could further reduce the
complexity to O(e-3/2) which would be an interesting direction for future work.
4.3	Outline of the proof
The proof of Theorem 1 proceeds by deriving a recursion for both outer-level error Ekx and inner-
level errors Eky and Ekz and then combining those to obtain an error bound on the total error Ektot .
Outer-level recursion. To allow a unified analysis of the behavior of Ekx in both convex and non-
convex settings, we define Fk as follows:
Fk =uδk E[L(χk ) - L*]1μ>0 + (E[L(Xk ) - L(Xk-I)]+ Ex-I- Ex)1μ<0.
The following proposition, with a proof in Appendix C.1, provides a recursive inequality on Ekx
involving the errors in (6) due to the inexact gradient ψk :
Proposition 2. Let ρk be a non-increasing sequence with 0<ρk <2. Assumptions 1 to 3 ensure that:
Fk + Ex ≤(1 - (1 - 2-1ρk )δk )Ek-1 + Yk Sk Vk-I + Yk(Sk + P-I)Eψ-1,	(IO)
with Sk defined as Sk := 2δk + (U2δk + (1 — U)) 1μ>o.
In the ideal case where yk = y?(Xk) and zk = z?(Xk, yk), the bias Ekψ vanishes and (10) simplifies
to (Kulunchakov and Mairal, 2019, Proposition 1) which recovers the convergence rates for stochas-
tic gradient methods in the convex case. However, yk and zk are generally inexact solutions and
introduce a positive bias Ekψ . Therefore, controlling the inner-level iterates is required to control the
bias Ekψ which, in turn, impacts the convergence of the outer-level as we discuss next.
Controlling the inner-level iterates yk and zk. Proposition 3 below controls the expected mean
squared errors between iterates yk and zk and their limiting values y?(Xk) and z?(Xk, yk):
Proposition 3. Let the step-sizes ak and βk be such that ak ≤L-1 and βk ≤ 21-∧-2+μg2-. Let
g μ9 ' gyy
λR : = (I - αkμg ) and πk : =
N
. Under Assumptions 1, 4 and 5, it holds that:
E lyk - y?(Xk)l2 ≤ΛkEky+Ryk,	E lzk-z?(Xk,yk)l2 ≤ΠkEkz+Rzk,	(11)
where Rky =O κg σg2 and Rzk=O κg3 σg2yy +κ2g σf2 are defined in (14) of Appendix A.2.
7
Published as a conference paper at ICLR 2022
While Proposition 3 is specific to the choice of the algorithms Ak and Bk in Algorithms 2 and 3, our
analysis directly extends to other algorithms satisfying inequalities similar to (11) such as acceler-
ated or variance reduced algorithms discussed in Appendices A.5.1 and A.5.2. Proposition 4 below
controls the bias and variance terms Vkψ and Ekψ in terms of the warm-start error Eky and Ekz .
Proposition 4. Under Assumptions 1 to 5, the following inequalities hold:
Ekψ ≤2L2ψ(ΛkEky+ΠkEkz+Rky),	Vkψ ≤wx2+σx2ΠkEkz,
where wx2=O	κ2g	σf2	+ σg2xy	+ κg3σg2yy	,	σx2=O	σg2xy	+	κg2σg2yy	and	Lψ=O(κ2g)	are positive
constants defined in (13) and (16) of Appendix A.2 with Lψ controlling the variations of Ek [ψk].
Proposition 4 highlights the dependence of Ekψ and Vkψ on the inner-level errors. It suggests analyz-
ing the evolution of Eky and Ekz to quantify how large the bias and variances can get:
Proposition 5. Let ζk > 0, a 2×2 matrix Pk, two vectors Uk and Vk in R2 all independent of xk,
yk and zk be as defined in Proposition 8 of Appendix A.2. Under Assumptions 1 to 5, it holds that:
EEkkyz	≤ Pk	ΠΛkk--11EEkkyz--11	++ RRykzk--11	+ 2γkEkψ-1	+ Vkψ-1 +	ζkEkx-1Uk	+	Vk.	(12)
Proposition 5 describes the evolution of the inner-level errors as the number of iterations k increases.
The matrix Pk and vectors Uk and Vk arise from discretization errors and depend on the step-sizes
and constants of the problem. The second term of (12) represents interactions with the outer-level
through Ekx-1, Vkψ-1 and Ekψ-1. Propositions 2, 4 and 5 describe the joint dynamics of (Ekx, Eky, Ekz)
from which the evolution of Ektot can be deduced as shown in Appendices A.1 and A.2.
5	Experiments
We run three sets of experiments described in Sections 5.1 to 5.3. In all cases, we consider AmIGO
with either gradient descent (AmIGO-GD) or conjugate gradient (AmIGO-CG) for algorithm Bk .
We AmIGO with AID methods without warm-start for Bk which we refer to as (AID-GD) and
(AID-CG) and with (AID-CG-WS) which uses warm-start for Bk but not for Ak . We also consider
other variants using either a fixed-point algorithm (AID-FP) (Grazzi et al., 2020) or Neumann series
expansion (AID-N) (Lorraine et al., 2020) for Bk . Finally, we consider two algorithms based on
iterative differentiation which we refer to as (ITD) (Grazzi et al., 2020) and (Reverse) (Franceschi
et al., 2017). For all methods except (AID-CG-WS), we use warm-start in algorithm Ak, how-
ever only AmIGO, AmIGO-CG and AID-CG-WS exploits warm-start in Bk the other AID based
methods initializing Bk with z0=0. In Sections 5.2 and 5.3, we also compare with BSA algorithm
(Ghadimi and Wang, 2018), TTSA algorithm (Hong et al., 2020a) and stocBiO (Ji et al., 2021). An
implementation of AmIGO is available in https://github.com/MichaelArbel/AmIGO.
5.1	Synthetic problem
To study the behavior of AmIGO in a controlled setting, we consider a synthetic problem where
both inner and outer level losses are quadratic functions with thousands of variables as described
in details in Appendix F.1. Figure 1(a) shows the complexity C() needed to reach 10-6 relative
error amongst the best choice for T and M over a grid as the conditioning number κg increases.
AmIGO-CG achieves the lowest time and is followed by AID-CG thus showing a favorable effect
of warm-start for Bk. The same conclusion holds for AmIGO-GD compared to AID-GD. Note that
AID-CG is still faster than AmIGO-CG for larger values of κg highlighting the advantage of using
algorithms Bk with O(√κg) complexity such as (CG) instead non-accelerated ones with O(K-I)
such as (GD). Figure 1(b) shows the relative error after 10s and maintains the same conclusions. For
moderate values of κg, only AmIGO and AID-CG reach an error of 10-20 as shown in Figure 1(c).
We refer to Figures 2 and 3 of Appendix F for additional results on the effect of the choice of T and
M showing that AmIGO consistently performs well for a wide range of values of T and M .
5.2	Hyper-parameter optimization
We consider a classification task on the 20Newsgroup dataset using a logistic loss and a linear
model. Each dimension of the linear model is regularized using a different hyper-parameter. The
8
Published as a conference paper at ICLR 2022
(a): C(ε) at ε = IO-6 error vs. κg	(b): Relative error after IO3 s vs. κg	(c): Relative error vs time; κg = IO3
IO1 IO3 IO5 IO7
Kg
(e)： Validation accuracy
98% 1
0.0	0.5	1.0	1.5	2.0	2.5	3.0	0.0	0.5	1.0	1.5	2.0	2.5	3.0
Time/s le2	Time/s le2
—■ AmIGO-CG
---AmIGO-GD
---AID-CG
---AID-CG-WS
--AID-FP
---AID-GD
---AID-N
-ITD
---Reverse
82%-
10^3 1O^1 IO1 IO3 IO5 IO7
Time/s
(f)： lest accuracy
—■ AmlGO-CG
---AmIGO-GD
-AID-CG
--AID-FP
-AID-GD
BSA
-ITD
—Reverse
-TTSA
0.0	0.5	1.0	1.5	2.0	2.5	3.C
Time/s le2

Figure 1: Top row: performance on the synthetic task. The relative error is defined as a ratio
between current and initial errors (L(xk) - L?)/(L(x0) - L?). The complexity C() as defined in
(8). Bottom row: performance on the hyper-parameter optimization task.
collection of those hyper-parameters form a vector x of dimension d=101631 optimized using an un-
regularized regression loss over the validation set while the model is learned using the training set.
We consider two evaluations settings: A default setting based on Grazzi et al. (2020); Ji et al. (2021)
and a grid-seach search setting near the default values of βk, T and N as detailed in Appendix F.2.
We also vary the batch-size from 103 * {0.1,1, 2, 4} and report the best performing choices for each
method. Figure 1(d,e,f) show AmIGO-CG to be the fastest, achieving the lowest error and highest
validation and test accuracies. The test accuracy of AmIGO-CG decreases after exceeding 80%
indicating a potential overfitting as also observed in Franceschi et al. (2018). Similarly, AmIGO-GD
outperformed all other methods that uses an algorithm Bk with O(κg) complexity. Moreover, all
remaining methods achieved comparable performance matching those reported in Ji et al. (2021),
thus indicating that the warm-start in Bk and acceleration in Bk were the determining factors for
obtaining an improved performance. Additionally, Figure 4 of Appendix F report similar results for
each choice of the batch-size indicating robustness to the choice of the batch-size.
5.3	Dataset distillation
Dataset distillation (Wang et al., 2018) consists in learning a synthetic dataset so that a model trained
on this dataset achieves a small error on the training set. Figure 5 of Appendix F.3 shows the training
loss (outer loss), the training and test accuracies ofa model trained on MNIST by dataset distillation.
Similarly to Figure 1, AmIGO-CG achieves the best performance followed by AID-CG. AmIGO ob-
tains the best performance by far among methods without acceleration for Bk while all the remaining
ones fail to improve. This finding is indicative of an ill-conditioned inner-level problem as confirmed
when computing the conditioning number of the hessian ∂yy g(x, y) which we found to be of order
7×104. Indeed, when compared to the synthetic example for κg=104 as shown in Figure 2, we also
observe that only AmIGO-CG, AmIGO and AID-CG could successfully optimize the loss. Hence,
these results confirm the importance of warm-start for an improved performance.
6	Conclusion
We studied AmIGO, an algorithm for bilevel optimization based on amortized implicit differentia-
tion and introduced a unified framework for analyzing its convergence. Our analysis showed that
AmIGO achieves the same complexity as unbiased oracle methods, thus achieving improved rates
compared to methods without warm-start in various settings. We then illustrated empirically such
improved convergence in both synthetic and a hyper-optimization experiments. A future research
direction consists in extending the proposed framework to non-smooth objectives and analyzing
acceleration in both inner and outer level problems as well as variance reduction techniques.
9
Published as a conference paper at ICLR 2022
7	Acknowledgments and Funding
This project was supported by the ERC grant number 714381 (SOLARIS project) and by ANR 3IA
MIAI@Grenoble Alpes, (ANR19-P3IA-0003).
References
Pierre Ablin, Gabriel Peyr6, and Thomas Moreau. SUPer-efficiency of automatic differentiation for
functions defined as a minimum. In International Conference on Machine Learning, pages 32-41.
PMLR, 2020.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of machine learning research,
18, 2018.
Luca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018.
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-
L6pez, Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation.
arXiv preprint arXiv:2105.15183, 2021.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate
o(k-1/4)
on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018.
Damek Davis, Dmitriy Drusvyatskiy, Kellie J MacPhee, and Courtney Paquette. Subgradient meth-
ods for sharp weakly convex functions. Journal of Optimization Theory and Applications, 179
(3):962-982, 2018.
Stephan Dempe and Alain Zemkoho. Bilevel Optimization. Springer, 2020.
Thinh T Doan. Nonlinear two-time-scale stochastic approximation: Convergence and finite-time
performance. arXiv preprint arXiv:2011.01868, 2020.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and
Statistics, pages 318-326. PMLR, 2012.
Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated machine learning,
pages 3-33. Springer, Cham, 2019.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In International Conference on Machine Learning,
pages 1165-1173. PMLR, 2017.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning, pages 1568-1577. PMLR, 2018.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on
Optimization, 22(4):1469-1492, 2012.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison
Guo. On differentiating parameterized argmin and argmax problems with application to bi-level
optimization. arXiv preprint arXiv:1607.05447, 2016.
10
Published as a conference paper at ICLR 2022
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. In International Conference on Machine Learning, pages
3748-3758. PMLR, 2020.
Riccardo Grazzi, Massimiliano Pontil, and Saverio Salzo. Convergence properties of stochastic
hypergradients. In International Conference on Artificial Intelligence and Statistics, pages 3826-
3834. PMLR, 2021.
P Habets. Stabilite asyptotique pour des problemes de perturbations singulieres. In Stability Prob-
lems, pages 2-18. Springer, 1974.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020a.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020b.
Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization.
arXiv preprint arXiv:2102.03926, 2021.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced
design. In International Conference on Machine Learning, pages 4882-4892. PMLR, 2021.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with markovian noise. In Conference
on Learning Theory, pages 2144-2203. PMLR, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
Andrei Kulunchakov and Julien Mairal. Estimate sequences for variance-reduced stochastic com-
posite optimization. In International Conference on Machine Learning, pages 3541-3550. PMLR,
2019.
Andrei Kulunchakov and Julien Mairal. Estimate sequences for stochastic composite optimization:
Variance reduction, acceleration, and robustness to noise. 2020.
Serge Lang. Fundamentals of differential geometry, volume 191. Springer Science & Business
Media, 2012.
Bruno Lecouat, Jean Ponce, and Julien Mairal. Designing and learning trainable priors with non-
cooperative games. arXiv preprint arXiv:2006.14859, 2020a.
Bruno Lecouat, Jean Ponce, and Julien Mairal. A flexible framework for designing trainable priors
with adaptive smoothing and game encoding. In Conference on Neural Information Processing
Systems (NeurIPS), 2020b.
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urta-
sun, and Richard Zemel. Reviving and improving recurrent back-propagation. In International
Conference on Machine Learning, pages 3082-3091. PMLR, 2018.
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. Catalyst acceleration for first-order convex
optimization: from theory to practice. Journal of Machine Learning Research, 18(1):7854-7907,
2018.
Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level opti-
mization for learning and vision from a unified perspective: A survey and beyond. arXiv preprint
arXiv:2101.11517, 2021.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pages
1540-1552. PMLR, 2020.
11
Published as a conference paper at ICLR 2022
Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE transactions on
pattern analysis and machine intelligence, 34(4):791-804, 2011.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International con-
ference on machine learning, pages 737-746. PMLR, 2016.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-Learning with
Implicit Gradients. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d∖textquotesingle Alche-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32
(NeurIPS). Curran Associates, Inc., 2019.
Ali Saberi and Hassan Khalil. Quadratic-type lyapunov functions for singularly perturbed systems.
IEEE Transactions on Automatic Control, 29(6):542-550, 1984.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and
Statistics, pages 1723-1732. PMLR, 2019.
Jonathan Richard Shewchuk et al. An introduction to the conjugate gradient method without the
agonizing pain, 1994.
H.F. Von Stackelberg. MarktformundGleichgewicht. Springer, 1934.
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv
preprint arXiv:1811.10959, 2018.
Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. arXiv
preprint arXiv:2106.04692, 2021.
JJ Ye and XY Ye. Necessary optimality conditions for optimization problems with variational in-
equality constraints. Mathematics of Operations Research, 22(4):977-997, 1997.
JJ Ye and DL Zhu. Optimality conditions for bilevel programming problems. Optimization, 33(1):
9-27, 1995.
JJ Ye, DL Zhu, and Qiji Jim Zhu. Exact penalization and necessary optimality conditions for gener-
alized bilevel programming problems. SIAM Journal on optimization, 7(2):481-507, 1997.
12
Published as a conference paper at ICLR 2022
A Convergence of AmIGO algorithm
In this section, we provide a proof of Theorem 1 as well as its corollaries Corrolaries 1 to 4. In
Appendix A.1, we provide an outline of the proof Theorem 1 that states the main intermediary
results needed for the proof and provide explicit expressions for the quantities needed throughout
the rest of the paper. Appendices A.2 and A.3 provide the proofs of Theorem 1 and Corrolaries 1
to 4. The proofs of the intermediary results are deferred to Appendices B and C.
A.1 Proof outline of Theorem 1
The proof of Theorem 1 proceeds in 8 steps as discussed bellow.
Step 1: Smoothness properties. This step consists in characterizing the smoothness of VL, y?, z?
as well as the conditional expectation E[ψk |xk, yk,zk] knowing Xk, yk and Zk. For this purpose, We
consider the function Ψ : X × Y × Y → X defined as follows:
Ψ(x, y, z) := ∂xf(x, y) + ∂xyg(x, y)z.
Hence, by definition of ψk, it is easy to see that E[ψk |xk,yk, Zk] = Ψ(xk, yk, Zk). The following
proposition controls the smoothness of VL, y?, z? and Ψ and is adapted from (Ghadimi and Wang,
2018, Lemma 2.2). We provide a proof in Appendix B.2 for completeness.
Proposition 6. Under Assumptions 1 to 3, L, Ψ, y? and Z? satisfy:
ky?(x) -y?(x0)k ≤ Lykx -x0k,	kZ?(x, y) - Z?(x0, y0)k ≤ Lz(kx - x0k + ky - y0k)
kVL(X)- VL(XO)k ≤ Lkx — χ0k,	kψ(X,y,Z)- VL(X)k ≤ Lψ[ky -y?(x)k + kz - z*(X,y)∣]
kz?(X,y*(X))k ≤ μ-1B,
where Ly, Lz, Lψ and L are given by:
Ly := μ-1 Lg,	Lz := μ-2MgB + μ-1Lf,	(13)
Lψ := maχ ((Lf+Mgμg 1B + LgLz),Lg)
L := (Lf + μ-2Lg Mg B + μ-1(Lg Lf + Mg B))(1 + μ-1 L0g)
The expressions of Ly, Lz, Lψ and L suggests the following dependence on the conditioning κg
of the inner-level problem which will be useful for the complexity analysis: Ly = O(κg), Lψ =
O(κ2g), Lz = O(κ2g) andL = O(κg3).
Step 2: Convergence of the inner-level iterates. In this step, we control the mean squared er-
rors E kyk - y?(Xk)k2 and E kZk - Z?(Xk, yk)k2
slightly stronger version stated below:
as stated in Proposition 3. In fact we prove a
Proposition 7. Let αk and βk be two positive sequences with αk ≤Lg-1 and
βk≤2L- min(1,住([十，.^^) and define Ak:=(1-αkμg)T and Π^=(l-βk2μg) . De-
note by zk the conditional expectation of zk knowing Xk, yk and z0. Let Ry and Rk be defined
as:
Ry := 2αkμ-1σg, Rz := βk8丁；L+ 3μ-2σf.	(14)
Then, under Assumptions 1, 4 and 5 the iterates zk and zzk satisfy:
Ehkyk - y?(Xk)k2i ≤ AkEky + Ryk
Ehkzk - z?(Xk, yk)k2i ≤ ΠkEhzk0 - z?(Xk,yk)2i + Rkz,
Ehkzzk - z?(Xk,yk)k2i ≤ ΠkEhzk0 - z?(Xk, yk)2i
Ehkzk- zkk2i ≤ 4σgμ-2∏kEIjlzO - z*(∙k,yk)『]+ 2Rk.	(15a)
13
Published as a conference paper at ICLR 2022
It is easy to see from the above expressions that Rk =O(Kgσg) while Rz = O (储σgy y + Kgσf)
as stated in Proposition 3. Controlling E[kyk - y?(xk)k]2 follows by standard results on SGD
(Kulunchakov and Mairal, 2020, Corollary 31) since the iterates of Algorithm 2 uses i.i.d. samples.
The error terms E kzk - z?(xk, yk)k2 is more delicate since Algorithm 3 uses the same sample
∂yf (xk , yk ) for updating the iterates, therefore introducing additional correlations between these
iterates. We defer the proof of Proposition 7 to Appendix B.3 which relies on a general result for
stochastic linear systems with correlated noise provided in Appendix E.
Step 3: Controlling the bias and variance errors Vkψ and Ekψ is achieved by Proposition 4.
The bias Ekψ is controlled simply by using the smoothness of the potential Ψ near the point
(xk, y?(xk), z?(xk, y?(xk)) as shown in Proposition 6. The variance term Vkψ is more deli-
cate to control due to the multiplicative noise resulting from the Jacobian-vector product between
∂χyg(xk, yk, Dgxy )zk. We defer the proof to Appendix B.4 and provide below explicit expressions
for the constants σx2and wx2:
Wx :=(1 +2Lgμ-1 + 6 (σgxy + (Lg尸)μ-2) σf	(16a)
+ 2(σgxy + (Lg 尸)B2L-1μ-3σgyy + 2笈2〃-句。「
σx ι=2σgxy+2(Lg/〃-句,「
Note that wx2 =O κ2 σf2 + σ2	+κ3σ2	and σx2=O σ2 +κ2σ2	, as stated in Proposition 4.
x	g f	gxy g gyy	x	gxy g gyy
Step 4: Outer-level error bound. This step consists in obtaining the inequality in Proposition 2
which extends the result of (Kulunchakov and Mairal, 2020, Proposition 1) to biased gradients and
to the non-convex case. We defer the proof of such result to Appendix C.1.
Step 5: Inner-level error bound. This step consists in proving Proposition 5. For clarity, we
provide a second statement with explicit expressions for the quantities of interest:
Proposition 8. Let rk and θk be two positive non-increasing sequences no greater than 1. For any
0 ≤ v ≤ 1, denote by φk and Rk the following non-negative scalars:
Φk := (1-v)LgT2αk + 2v,	Ry := 2(1 -V)Lgμ-1T2σgα2k + VRk
Zk := 2L(min ((1 - u)-1, Lηfc-1)lμ≥0 + 1“<。)
Finally, consider the following matrices and vectors:
Pk := (116+2r⅛,	1-0θk),	Uk	:=(4L2,(1	+r4Ly普)),	Vk	:=	Rk	GLz0O-1).
(17)
Then, under Assumptions 1 to 5, the following holds:
EEkkyz)	≤	Pk	ΠΛkk--11EEkkyz--11	++ RRykzk--11)	+ 2γk(Ekψ-1	+ Vkψ-1 +	ζkEkx-1Uk	+ Vk.
We defer the proof of the above result to Appendix C.2.
Step 6: General error bound. By combining the inequalities in Propositions 2 and 8 resulting from
the analysis of both outer and inner levels, we obtain a general error bound on Ektot in Proposition 9
with a proof in Appendix C.4.
Proposition 9. Choose the step-sizes αk and βk such that they are non-increasing in k and choose
rk and θk such that δk rk-1 and δk θk-1 are non-increasing sequences. Choose the coefficients ak and
bk defining Ektot in (7) to be of the form ak = δk rk-1 Λsk and bk = δk θk-1 Πsk for some 0 < s < 1.
and fix a non-increasing sequence 0 < ρk < 1. Then, under Assumptions 1 to 5 Ektot satisfies:
Fk + Ektot ≤ kAkk∞Ekto-t1 + Vktot.
14
Published as a conference paper at ICLR 2022
where Vk and Ak are given by:
/	1 -(1 - 2Pk)δk +2ZkλkUk	∖
Ak := Ak-s(1+ Tk (1 + 16L2πkθ-2Φk+2Lψη-1(2uk + sk+ P-1)))	(18)
∖	∏k-s(l + θk(l + η-1(2Lψ + σ2 )(2Uk + sk + P-I))))
.oL + r-1) + 16LZ φk θ-2∏ + 2Lψ η-1(2u( + Sk + P-I))RyT
+ δk ((1 + θ-1)∏k Rk-1 + 8L2θ-2∏k Ry) + Yk (sk + Uk )w2
where we introduced UIk = ak Uk(1) + bk Uk(2) for conciseness with Uk(1) and Uk(2) being the compo-
nents of the vector Uk defined in Proposition 8.
Proposition 9 holds without conditions on the error made by Algorithms 2 and 3. The general form
of ak and bk allows to account for potentially decreasing step-sizes γk, αk and βk. However, in the
present work, we will restrict to the constant step-size for ease of presentation as we discuss next.
Step 7: Controlling the precision of the inner-level algorithms. In this step, we provide condi-
tions on T and N in Proposition 10 bellow so that kAk k∞ ≤ 1 - (1 -Pk)δk in the constant step-size
case. These conditions are expressed in terms of the following constants:
Ci :=1 +2log (6 + 24Lψ η-1)	(19a)
C2 :=2 log(1 + 4L2L-2 max (小,8Zo))	(19b)
C3 :=max(0, -2 log (5Lψη-i), -2 log	(19C)
Ci ：=1 + 2log (4 + 12η-1(2Lψ + σ2))	(19d)
C2 ：=2log (1 + 2L∣L-2(1 + 16Ly)),	(19e)
C3 ：=maX (0, -2lθg (4Lψθ)，-2log (Y (σgχy + (Lg V)))	(19f)
Proposition 10. Let Assumptions 1 to 5 hold. Choose Pk = 2, the step-sizes to be Constant:
αk=α≤L-, βk =β≤2L- and Yk =γ≤L and choose the batch-size IDgyyl = Θ (μ⅛^)∙ Moreover
set s = 2, Tk = θk = 1. Finally, choose T and N asfollows:
T = bα-iμ-i max (Ci,C2,C3)C + 1,
N = b2β-iμ-i(max (Ci,C2,C3) + max (Ci ,C2 ,C3 ))C + 1
with Ci, C2, C3, Ci0, C20 and C30 defined in (19a) to (19f). Then, kAk k∞ ≤ 1 - (1 - Pk)δk and
Vktot ≤ YδoW2, with W2 given by:
W2 := (δ-i 1-u 1”>o + 3)w2 + 詈;σg	(20)
I 2	δoμg Lgg
We provide a proof of Proposition 10 in Appendix D. It is easy to see from Proposition 10 that that
T=O(κg) and N=O(Kg) when α=十 and β=击,where the big-O notation hides a logarithmic
dependenCe in κg Coming from the Constants {Ci, Ci0|i ∈ {1, 2, 3}}.
Step 8: Proving the main inequalities. The final step combines Propositions 9 and 10 to get the
desired inequality. We provide a full proof in Appendix A.2 assuming Propositions 9 and 10 hold.
A.2 Proof of Theorem 1
In order to prove Theorem 1 in the convex case, we need the following averaging strategy lemma, a
generalization of (Kulunchakov and Mairal, 2020, Lemma 30):
15
Published as a conference paper at ICLR 2022
Lemma 1. LetL be a convex function on X. Let xk be a (potentially stochastic) sequence of iterates
in X. Let (Ek)k≥0 , (Vk)k≥0 and (δk)k≥0 be non-negative sequences such that δk ∈ (0, 1). Fix
SOme non-negative number U ∈ [0,1] and define the following averaged iterates Xk recursively by
Xk = u(1 一 δk)Xk-ι + (1 一 (1 一 δk )u)xk and Startingfrom any initial point X0. Assume the iterates
(xk)k≥1 satisfy the following relation for all k ≥ 1:
(1 一 u(1 一 δk))E[L(Xk) 一 L?] + Ek ≤ (1 一 δk)(Ek-1 + (1 一 u)E[L(Xk-1) 一 L?]) + Vk. (21)
Let Γk := Qk=ι (1 一 δk). Then the averaged iterates (Xk )k≥ι satisfy thefollowing:
E[L(Xk) - L?] + Ek ≤ Γk(Eo + E[L(xo) - L? + uk(L(X0)-L?)]) + Γfc X Γ-1Vt.
1≤t≤k
Proof. For simplicity, We write Fk = E[L(xk) 一 L?] and Fk = E[L(Xk) 一 L?]. We first multiply
(21) by Γk-1 and sum the resulting inequalities for all 1 ≤ k ≤ K to get:
KK	K
XΓ-1(1 一 u(1 - δk))Fk+Γ-1Tk ≤ XΓ-1(1 一 δk)(Tk-ι + (1-U)Fk-1) + XΓ-1Vk.
k=1	k=1	k=1
Grouping the terms in Fk together and recalling that Γk-1(1 一 δk) = Γk--11 yields:
K	KK	K
XE-IFk-Ek-IFk-ι + u Xr--ι(Fk-ι 一 Fk) ≤ X (γ--iTk-ι 一 r-1Tk) + Xr-1Vk.
k=1	k=1	k=1	k=1
Simplifying the telescoping sums and multiplying by Ek, we get:
FK + UrK ^X γ--i (Fk-I- Fk ) ≤ -TK + γK (F0 + T0 + ^X r-1Vk) .	(22)
k=1	k=1
Consider now the quantity Fk. Recalling that L is convex and by definition of the iterates Xk We
apply Jensen’s inequality to write:
Fk ≤ u(1 一 δk )Fk-I + (1 一 u(1 一 δk))Fk .
By iteratively applying the above inequality, we get that:
K
FK ≤ukΓkF0 + Γk X uκ-kΓ-1(1 一 u(1 一 δk))Fk
k=1
KK
=UKrKF0 +Γk X UKirk1 Fk- X UK-k+1Γk-1Fk
k=1	k=1
K-1
=UKrKF0 + Fk + Γk X UK-kΓ-1(Fk - Fk+ι)
k=1
We can therefore apply (22) to the above inequality to get the desired result.	□
We now proceed to prove Theorem 1.
Proof of Theorem 1. By application of Proposition 9 and using the choice of T and N given by
Proposition 10, the following inequality holds:
Fk + Ekot ≤ (1 一(1 一 Pk)δk)Ekotι + γδ0W2.	(23)
with W defined in Proposition 10. We then distinguish two cases depending on the sign of μ:
Case μ ≥ 0. Recall that Fk and Ex are given by:
Fk = UδkE[L(xk) - L?],	Ex = aIlXk - x?k2 + (1 一 U)E[L(xk) - L?]
16
Published as a conference paper at ICLR 2022
Since μ > 0, L is a convex function and We can apply Lemma 1 with Vk =γδ0W2 and
Ek=2k Ilxk - x?『+akEy +bkEZ. The result follows by noting that Γ Pk=Ir-I ≤ δ-1.
Case μ < 0. In this case, we recall that Fk and Ek are given by:
Fk = E[L(xk) - L(xk-ι)] + Ek-I- Ek,	Ex = LTEhkVL(Xk)『].
We then sum (23) for all iterations 0 ≤ t ≤ k which, by telescoping, yields:
E[L(xk) - L(x0)] + E0k -Ekk+Ektot-E0tot+ X (1-ρt)δtEttot ≤kγδ0W2.
1≤t≤k
Using that E[L(xk) - L?] + Ektot - Ekk is non-negative since Ektot - Ekk = akEky + bkEkz, we get:
X (1 -ρt)δtEtk ≤ (E[L(x0) - L?] + a0E0y + b0E0z) + kγδ0W2.
1≤t≤k
Finally, since Pt = ɪ, δt = LY, the result follows after dividing both sides by 2kLγ.	口
A.3 Proof of Corrolaries 1 to 4
ProofofCorollary 1. Choosing U = 0 implies that Ek = 2kxk 一 x?『+ L(Xk) -L ≤ E胪.We
can then apply Theorem 1 for μ > 0 which yields the following:
μ	k . .	2W2
kx kxk - X Il + L(Xk) -L ≤ ( 1 - (2kl) ) EO +-.— .	(24)
2L
In the deterministic setting, it holds all variances vanish : σf2 =σg2 =σg2yy =σg2xy =0. Hence, W2 = 0
by definition of W2. Therefore, to achieve an error L(Xk)-L*≤e for some e〉0, (24) suggests
choosing k=O (KL log (E-) ). Additionally, T=Θ(κg) and N=Θ(κg) as required by Theorem 1
and since σg2yy =0, it holds that N =O(κg). Using batches of size 1, yields the desired complexity.
yy	口
ProofofCorollary 2. Here we choose U = 1 and apply Theorem 1 for μ > 0 which yields:
E[L(Xk) - L?] + Etrtt ≤(1 - (2KL)T) k (Etot + E[L(x°) - L*]) + 2L-1W2
Hence, to achieve an error E[L(Xk) - L*] ≤ e, we need k =O(KLlog (E0 +E[：(X0)-L ])) to
guarantee that the first term in the l.h.s. of the above inequality is O(e). Moreover, we recall that
L-1 = O(Kg-3) from Proposition 6 and that Theorem 1 ensure the variance W satisfies:
W2 =OK5σ2+K3σ2 +K2σ2 +K2σf2).
g g g gyy	g gxy g f
Hence, ensuring the variance term 2L-1W2 is of order e is achieved by choosing the size of the
batches as follows:
|Df I = θ(σ2 !，|Dg | = θ( KLKσ2 !，IDgxyl=Θ
IDgyyI =θ(σgyy G ∨ Kg))
Recall that T=Θ(Kg) and N=Θ(Kg) as required by, Theorem 1, thus yielding the desired result. 口
ProofofCorollary 3. In the non-convex deterministic case, recall that Ex = L ∣∣VL(Xk)∣∣2 ≤ Ek)ot.
We thus apply Theorem 1 for μ < 0, multiply by L to get:
1 k	2L
k E IVL(Xt)I2 ≤ -τ(L(xo) - L* + (Ey + Ez)) + 2W2.
t=1
The setting being deterministic, it holds that W2=0. Moreover, recall that L = O(Kg3) from
Proposition 6. Hence, to achieve an error of order min1≤t≤k IVL(Xt)I2 ≤ e, it suffice to choose
k=O (Kg (L(xo) - L* + (Ey + Ez))). ThUS using batches of size 1 and T and N of order Kg. 口
17
Published as a conference paper at ICLR 2022
Proof of Corollary 4. In the non-convex stochastic case, Ekx
apply Theorem 1 for μ < 0, multiply by L to get:
LE[kVL(χk)k2] ≤ E那.WethUs
k
1 XEhkVL(Xt)『]≤ 2L(E[L(xo) - L?] + (Ey + Ez)) + 2W2.
kk
to achieve an error of order , we need to ensure each term in the l.h.s. of the above inequality
is of order . For the first term, similarly to the deterministic setting Corollary 3, we simply need
k = O (Kg(L(χo) -L? + (Ey + Ez))). For the second term, We need to have W2=O(e), which
is achieved using the following choice for the sizes of the batches:
|DfI = o(κgσ2), |DgI = o(κgσ2), ∣Dgχy∣ = o(κτxy
Finally, as required by Theorem 1, we set T = Θ(κg) and N = Θ(κg) thus yielding the desired
complexity.
A.4 Comparaisons with other methods
In this subsection we derive and discuss the complexities of methods presented in Table 1.
A.4. 1 Comparaison with TTSA (Hong et al., 2020b)
Proposition 11. Strongly-Convex case μ > 0. The complexity ofthe TTSA algorithm in Hong et al.
μ E Ixk- x?k2]
(2020b) to achieve an error
≤ e is given by:
C(e)
non-convex case μ < 0. The ComPlexity ofthe TTSA algorithm in Hong et al. (2020b) to achieve
an error 1 Pι≤i≤k E [∣∣VL(xi)『]≤ e is given by:
C(e) := k(1 + N) = O ((κg1 + κ16) eL log G)).
Proof. Strongly-Convex case μ > 0 Using the choice of step-sizes in Hong et al. (2020b), the
following bound holds:
Ehkxk- x?『i . Y (1 - 3(k + ka)}δx + Lψ〃-2«)
+ S (μ-1 +
μg μ3 ∖
k + kα
where ∆0x = E kx0 - x?k2 , E ky1 - y?(x0)k2 and kα given by:
kα = max(35 (Ig(1 + σg))	)
By a simple calculation, it is easy to see that Qk=0 (1 - 3(-+.)) ≤ (J-CIik)产.Moreover, using
that Lψ = O(μ-2), Ly = O(μ-1), we get that
2
2EhIlxk - x?『i . (k+XkQ -i)2 (mδX + μμ-6∆y) + μ-6μ-3(1 + μ-2) ɑ+ka)
18
Published as a conference paper at ICLR 2022
Using that L=O(μ-3), We get μ-6μ-1 (1 + μ-2)=θ(μ-5κL + μgk：) . Hence, to reach an error
e, we need to control both terms in the above inequality. This suggests the following condition on k
to control the second term which dominates the error:
k ≥ (μg2 KL/2 + KL/2〃—15) Ir.
Moreover, the result in (Hong et al., 2020b, Theorem 1) requires N = Θ (Kg log ɪ), where N is the
number of terms in the Neumann series used to approximate the hessian inverse (∂yyg(X, y))-1 in
the expression of the gradient VL. Hence, the total complexity is given by the following expression:
C(e) := k(1 + N) = O( (KgKf +
log1
Smooth Non-Convex case μ < 0. Following Hong et al. (2020b), consider the proximal map of L
for a fixed ρ > 0:
X(z) ：= arg min ∣L(x) + P ∣∣x - z『}
x∈X	2
and define the quantity ∆X := E [∣∣X(χk) — Xk『],where Xk are the iterates produced by the TTSA
algorithm. Let K be a random variable uniformly distributed on {0, ..., K - 1} and independent
from the remaining r.v. used in the TTSA algorithm. The result in (Hong et al., 2020b, Theorem 2)
provide the following error bound on ∆ X
k X δx . (Lψ 卜+σ2! + μg) kL5.
1≤i≤k	μg∕
where ρ is set to 2L and ∆0≤ max E[L(X0)-L?], E ky1-y?(X0)k2 . Now, recall that by defi-
nition of the proximal map, we have the following identity:
1 X EhkVL(Xi)k2i ≤ 2(ρ2 + L2)1 X AX. L21 X δX.
1≤i≤k
1≤i≤k
1≤i≤k
Hence, we obtain the following error bound:
1 X EhkVL(Xi)『]≤ (κg(∆0 + κgσg) + μg)k-5
1≤i≤k
where we used that Lψ = O(κg2). Therefore, to reach an error of order e, TTSA requires:
k ≥ 1 (k10δ5 +κg5σg)
Moreover, controlling the bias in the estimation of the gradient requires N =O(Kg log ɪ) terms in
the Neumann series approximating the hessian. Hence, the total complexity of the algorithm is:
C(e) ：= k(1 + N) = O ((KgI + K16): log
□
A.4.2 Comparaison with AccBio (Ji and Liang, 2021)
Complexity of AccBio. The bilevel algorithm AccBio introduced in Ji and Liang (2021) uses ac-
celeration for both the inner and outer loops. This allows to obtain the following conditions on k, T
and N to achieve an e accuracy:
l-2£
og
log 1 ).
1 - €
1-2 3
1-2 3
19
Published as a conference paper at ICLR 2022
Note that, since AccBio do not use a warm-start strategy when solving the linear system, N is
required to grow as log ɪ in order to achieve an e accuracy. This contributes an additional logarithmic
factor to the total complexity so that C(e) = O(KLKg (log ɪ)2). This is by contrast with AmIGO
which exploits warm start when solving the linear system and thus only needs a constant number of
iterations N = O(Kg) although the dependence on Kg is worse compared to AccBio. However, it is
possible to improve such dependence by using acceleration in the inner-level algorithms Ak and Bk
as we discuss in Appendix A.5.1.
Complexity of AcCBio as a function of μ and μg. The authors choose to report the complexity as a
function of μ and μg instead of the conditioning numbers KL and Kg. To achieve this, they observe
that, under the additional assumption that the hessian ∂yyg(x, y) is constant w.r.t. y, the Lipschitz
constant L has an improved dependence on μg: L = O(μ-2) instead of L = O(μ-3) in the general
case where ∂yyg(x, y) is only Lipschitz in y. This allows them to express KL = L = O(μ-2μ-1)
and to report the following complexities in terms of μ and μg:
C(e) = O μ-1 μ- 2
Note that, in the general case where L = O(μ-3), the complexity as a function of μ and μg becomes
O (μ-2μ-2 (log ɪ)2), while still maintaining the same expression in terms of KL and Kg. Hence,
using the expression in terms of conditioning allows a more general expression for the complexity
that is less sensitive to the specific assumptions on g and is therefore more suitable for comparaison
with other results in the literature.
A.5 CHOICE OF THE INNER-LEVEL ALGORITHMS Ak AND Bk.
The choice of Ak and Bk has an impact on the total complexity of the algorithm. We discuss two
choices for Ak and Bk which improve the total complexity of AmIGO: Accelerated algorithms (in
Appendix A.5.1) and variance reduced algorithms (in Appendix A.5.2).
A.5.1	Acceleration of the inner-level for AmIGO
AmIGO could benefit from acceleration in the inner-loop by using standard acceleration schemes
Nesterov (2003) for Ak and Bk . As a consequence, and using analysis of accelerated algorithms
(Nesterov, 2003) in the deterministic setting, the error of the inner-level iterates would satisfy:
Ehkyk- y?(Xk)k2i ≤ ΛkEy,	Ehkyk- y*(xk)『］≤ ΠkEz
where Λ k and Π k are accelerated rates of the form Λ k = O((1-√Kg )T) and Π k = O((1 -√Kg )N).
The rest of the proofs are similar provided that Ak and ∏k are replaced by their accelerated rates Λk
and Πk. This implies that T and N need to be only of order T = O(√Kg) and N = O(√Kg) so
that the final complexity becomes:
C (e) := OrLKg/ log；)
Note that using conjugate gradient for Bk also enjoys an accelerated convergence rate Shewchuk
et al. (1994). This is confirmed in our experiments of Figure 1 where AmIGO-CG enjoys the fastest
convergence.
In order to further improve the dependence on KL to K1L/2, one would need to use an accelerated
scheme when updating the iterates xk. The analysis of such scheme along with warm-start would
be an interesting direction for future work.
A.5.2	VARIANCE REDUCED ALGORITHMS FOR Ak AND Bk
When the inner-level cost function g is a finite average of functions g(χ, y) = n P1≤i≤n gi(x, y)
empirical average, it is possible to use variance reduced algorithms such as SAG (Schmidt et al.,
20
Published as a conference paper at ICLR 2022
2017). If every function gi is Lg-smooth, then by (Schmidt et al., 2017, Proposition 1), the inner
level error becomes:
Ehkyk-y*(xk)k2i . Λk(3Ey + 4L-2σg),
with Λ k =(1 - Kg )T. This has the advantage that the error due to the variance decays exponentially
with the number of iterations T. As a consequence, the dependence of the effective variance W2 on
the conditioning numbers κL and κg can be improved to:
W2 =O(lDg l-1σg + κ3[Dgyy∖-1σL +κg IDgxy L%y + 埼 IDf ∣-1σf).
This can be achieved by taking T = O(nκg) up to a logarithmic dependence on the condition
numbers. As a consequence, the complexity in the strongly convex stochastic setting becomes:
C 9 = O 卜 L (nig + Kg (I ∨ eκS )σgyy + Igxy +	) ɪ lθg (E + ^^^1—^ ^ .
In the non-convex setting, the complexity becomes:
C(e) = O(Kg (nσg + κg (1 ∨ eμ2)σ2yy + Kg或,+ Kgσf )(E[L(xk) - L?]+ Ey + Ez)).
The downside of this approach is the dependence on the number n of functions gi in the total com-
plexity.
B Preliminary results
B.1	Expression of the gradient
We provide a proof of Proposition 1 which shows that L is differentiable and provides an expression
of its gradient.
Proof. Assumption 1 ensures that y 7→ g(x, y) admits a unique minimizer y? (x) defined as the
unique solution to the implicit equation ∂yg(x, y?(x)) = 0. Moreover, since g is twice continu-
ously differentiable and strongly convex, it follows that ∂yyg(x, y?(x)) is invertible for any x ∈ X.
Therefore the implicit function theorem (Lang, 2012, Theorem 5.9), ensures that x 7→ y?(x) is
continuously differentiable with Jacobian given by Vy?(x) = -∂χyg(x, y*(x))∂yyg(x,y?(X))-1.
Hence, by composition of differentiable functions, L is also differentiable with gradient given by:
VL(X) = ∂χf(x, y*(x)) - ∂χyg(x, y*(x))∂yyg(x, y*(x))-1∂yf (x, y*(x)).
We can thus define z?(x, y) = -∂yyg(x, y)-1∂yf(x, y) to get the desired expression for VL(x)
and note that z? is the solution to (2).
B.2	SMOOTHNESS PROPERTIES OF L, Y?, z? AND Ψ
Proof of Proposition 6. Lipschitz continuity of X 7→ y?(X). By Assumptions 1 and 3, the implicit
function theorem (Lang, 2012, Theorem 5.9) ensures y?(X) is differentiable with Jacobian given by:
Vy?(X) = -dχy g(X,y?(X)Xdyy g(X,y?(X)))-1∙
Moreover, by Assumption 3, we know that ∂yg(X, y) is L0g-Lipchitz in X for any y ∈ Y, hence,
∣∣∂χyg(X, y*(X))∣∣op is upper-bounded by Lg. Moreover, by Assumption 1, g is μg-strongly convex
in y uniformly on X. Therefore, it holds that Ildyyg(X, y?(X))-1 |葭 ≤ μ-1. ThiS allows to deduce
that ∣Vy?(X)∣ op ≤ μ-1Lg, and by application of the fundamental theorem of calculus that:
ky?(X)-y*(XO)Il ≤ μ-1LgkX -X0k.
This shows that y? is Ly-Lipschitz continuous with Ly := μ-1Lg.
21
Published as a conference paper at ICLR 2022
Lipchitz continuity of x 7→ z?(x, y). Let (x, y) and (x0, y0) be two points in X × Y. Recalling the
definition of z?(x, y) in Proposition 1, it is easy to see that z?(x, y) admits the following expression:
z?(x, y) = -∂yyg(x, y)-1∂yf (x, y).	(25)
Recalling the expression of z?(x, y), the following holds:
z?(x, y) - z?(x0, y0) =∂yyg(x0, y0)-1∂yf(x0, y0) - ∂yyg(x, y)-1∂yf(x, y)
= (dyyg(X0, yO)T- dyyg(X, y厂1)dyf (X0, /0)
+ ∂yyg(x, y)-1(∂yf(x0, y0) - ∂yf(x, y))
=∂yyg(X0, y0)-1(∂yyg(X, y) - ∂yyg(X0, y0))∂yyg(X, y)-1∂yf(X0, y0)
+ ∂yyg(X, y)-1(∂yf(X0, y0) - ∂yf(X, y))
Hence, by taking the norm of the above quantity a triangular inequality followed by operator in-
equalities, it follows that:
kz?(X, y) - z?(X0, y0)k ≤H2-1opkH1-H2kopH1-1opk∂yf(X0,y0)k
+ H1-1k∂yf(X0,y0) - ∂y f (X, y)k.
where we introduced H1 = ∂yyg(X, y) and H2 = ∂yyg(X0, y0) for conciseness. Using As-
sumption 1, We can upper-bound IlH-Ihop and IlH-Ihop by μ-1. By Assumption 3, We
know that kH1 - H2 kop ≤ Mgk(X, y) - (X0, y0)k. Finally by Assumption 2, we also have that
k∂yf (X0, y0) - ∂yf(X,y)k ≤ Lfk(X,y) - (X0, y0)k and that k∂yf(X0,y0)k ≤ B ensuring that:
kz?(X,y) - z*(XO,yO)Il ≤ (μ-2MgB + μ-1Lf)k(X,y) -(X0,yO)k.
Hence, We conclude that z? is Lz-Lipchitz continuous With Lz defined as in (13).
boundedness of z?(X, y) Recalling the expression of z? in (25), it is easy to see that kz?(X, y)k is
upper-bounded by μ-1B since ∂yyg(X, y) is μg-strongly convex in y by Assumption 1 and ∂y f (x, y)
is bounded byB by Assumption 2.
Regularity of Ψ.
Ψ(X,y,z) - Ψ(XO, yO, zO) =∂xf (X, y) - ∂xf (XO, yO) + ∂xyg(X, y)z - ∂xyg(XO, yO)zO
=∂xf (X, y) - ∂xf (XO, yO) + ∂xyg(X,y)(z - zO)
+ (∂xyg(X, y) - ∂xyg(XO, yO))zO.
By taking the norm of the above expression and applying a triangular inequality folloWed by operator
inequalities, it folloWs that:
kΨ(X,y,z) - Ψ(XO, yO, zO)k ≤k∂xf (X, y) - ∂xf(XO, yO)k + k∂xyg(X, y)kopkz - zOk	(26)
+ k∂xyg(X,y) - ∂xyg(XO, yO)kopkzOk
≤Lf(kX-XOk+ky-yOk)+LOgkz-zOk
+MgkzOk(kX-XOk+ky-yOk).
To get the first term of the last inequality above, We used that ∂xf is Lf -Lipschitz by Assumption 2.
To get the second term, We used that ∂xyg(X, y) is bounded since ∂yg(X, y) is LOg-Lipschitz by
Assumption 3. Finally, for the last term, We used that ∂xyg(X, y) is Mg-Lipschitz by Assumption 3.
By choosing XO = X, yO = y?(X) and zO = z?(X, y?(X)), it is easy to see from Proposition 1 that
Ψ(x, y*(x),z*(x, y*(X))) = VL(x). Hence, applying the above inequality yields:
kψ(X,y,z) - VL(X)k ≤(Lf + Mgkz*(X,y*(X))k)ky — y*(X)k + Lgkz - z*(X,y*(X))Il
≤(Lf + Mgkz*(X, y*(X))k)ky - y*(X)k +LOgkz - z*(X, y)k
+ LOg kz*(X, y) - z*(X, y*(X))k
As shown earlier, ∣∣z*(x, y*(X))k is upper-bounded by μ-1B, while kz*(x, y) - z*(x, y*(X))k is
bounded by Lzky - y*(X)k. This alloWs to conclude that kΨ(X, y, z) - VL(X)k ≤ Lψ With Lψ
defined in (13).
22
Published as a conference paper at ICLR 2022
Lipschitz continuity of x → VL(x). We apply (26) with (y,z) = (y?(x),z?(x,y?(x))) and
(y0, z0) = (y?(x0), z?(x, y?(x0))) which yields:
kVL(x) - VL(x0)k ≤(Lf + Mgkz?(x0, y?(x0))k)(kx - x0k + ky?(x) - y?(x0)k)
+ L0g kz?(x, y?(x)) - z?(x0, y?(x0))k
≤(Lf + Mg μ-1B + Lg Lz)(I + Ly)Ilx - χ0k,
where We used that ∣z?(x0, y*(x0))∣∣ is upper-bounded by μ-1B, z? is Lz-LiPschitz and y? is Ly-
Lipschitz. Hence, VL is L-LiPSchitz continuous, with L as given by (13).	□
B.3 CONVERGENCE OF THE ITERATES OF ALGORITHMS Ak AND Bk
Proof. Controlling the iterates yt of Ak.
Consider a new batch Dg of samples ξ . We have by definition of the update equation of yt that:
∣∣yt -y?(xk)||2 =IIytT-y*(χk)∣∣2 + αk∣Idyg(xk,yt-1,Dg)∣∣2
—2αk∂yg(xk, yt-1, Dg)>(yt-1 - y?Xk))
Taking the expectation conditionally on xk and yt-1 , we get:
Eh∣∣yt -y?(XkV∣2,k,ytTi=(I - αkμgV∣yt-1 -y*(Xk)『
+ OkE∣j∣∂yg(xk,yt-1,Dg) - ∂yg(χk,yt-1)∣∣2∣χk,yt-1]
—2αk∂yg(xk,yt-1)>(yt-1 -y*(xk) - (O∂yg(χk,y=-1))
≤(I- (k μg )∣∣yt-1 - y?(Xk) ∣∣ + 2(k σg
The first line uses that ∂yg(Xk,yt-1, Dg) is an unbiased estimator of ∂yg(xk, yt-1). For the second
line, we use Assumption 4 which allows to upper-bound the variance of ∂yg by σg Moreover, since
g is convex and L -smooth and since (k ≤ L-1 , it follows that the last term in the above inequality
gg
is non-positive and can thus be upper-bounded by 0. By unrolling the resulting inequality recursively
for 1 < t ≤ k, we obtain the desired result.
Controlling the iterates zn of Bk. The poof follows by direct application of Proposition 15 with
β = βk and the following choices for An, A, b, b:
An =dyy 1(Xk,yk , Dgyy ),
A =∂yyg(Xk, yk )
f €1 r /	-τ> ∖
b =∂yf (Xk, yk, Df)
b =∂yf (Xk, yk).
1
βk μg
∏k ：=(1- βkμg)N,	RRk := βk (σgEhkz?(Xk, yk)『i
N ∧ β⅛)
This directly yields the following inequalities:
Ehkzk - z?(Xk,yk)『]≤∏ke[∣∣z0 - z?(Xk,yk)∣∣2i + Rk,
EhIlWk - z?(XkJyk)k2i ≤∏ke[∣∣z0 - z?(XkJyk)∣∣2]∙
where Πk and Rzk are given by:
+3 N∧
First we have that Πk ≤ ∏k. Moreover, Proposition 6, we have that ∣∣z*(Xk, yk)k ≤ Bμ-1 hence,
Rzk ≤ Rzk thus yielding the desired inequalities. Finally (15a) also follows similarly using (45) from
Proposition 15.	□
23
Published as a conference paper at ICLR 2022
B.4 CONTROLLING THE BIAS AND VARIANCE Ekψ AND Vkψ
Proof of Proposition 4 . Recall that the expressions of Ekψ and Vkψ in (6) involves the conditional
expectation Ek
knowing xk, yk and zk-1. This can be also expressed using Ψ as follows:
Ek ,k]
=EiE[ψk∣xk,yk,zk] |xk,yk,zk-ι]
=E[Ψ(xk, yk, zk)|xk, yk, zk-1]
=E[Ψ(xk, yk, E[zk||xk, yk, zk-1])]
where we used the tower property for conditional expectations in the first line, then the fact that
the expectation of ψk conditionally on xk, yk and zk is simply Ψ(xk, yk, zk). Finally, for the last
line, we use the independence of the noise and the linearity of Ψ w.r.t. the last variable. In all what
follows, We write Zk = E[zk |xk,yk, zk-ι] which is the same object as defined in Proposition 7. We
then treat Ekψ and Vkψ separately.
Bounding Ekψ. Using Propositions 6 and 7 we directly get the desired inequality:
Ekψ ≤2L2ψEhkyk - y?(xk)k2i +EhkzZk-z?(xk, yk)k2i
≤2L2ψ(ΛkEky+ΠkEkz +Rky)
Bound on Vkψ. We decompose Vkψ into a sum of three terms Wk, Wk0 and Wk00 given by:
Wk := E ∣∣∂χ∕(xk,yk,Df)- ∂χf (xk,yk)∣∣ ,
Wk := E ∣∣∂χyg(xk,yk,ξN +ι,kJzk - ∂χyg(xk,yk)Zk∣∣ ]
>
Wk0 ：= E (∂χf(xk,yk, Df)- ∂χf(xk,yk)) ∂xyg(xk,yk)(zk - Zk).
where we used that ξN+1,k is independent from zk and Df to get the last term. Hence, using
Assumption 4 to bound the first term of the above relation, we get Vkψ ≤ σf2 + Wk0 + 2Wk00 . Thus, it
remains to control each of Wk0 and Wk00 .
Bound on Wk00. Using that Df is independent from ξn,k, we can apply Proposition 14 to write:
Wk0 = βk E ∂χ(f - f )(xk ,yk, Df Ydxy g(xk, yk) (XX (I - βk A)N-t) ∂y (f - f)(x® ,yk, Df)
where we used the simplifying notion (f - f)(xk, yk, Df) = f(xk,yk,Df) - f(xk, yk).
Using Assumption 3 to bound k∂xyg(xk, yk)kop by L0g, Assumption 1 to upper-bound
∣∣(PN=1 (I - βkA)N-t) Lby(PN=1 (1 - βkμg)N-t) we get
N-1
∣Wk0∣ ≤βkLg X (1 - βkμg)同。式/ - f )(xk, yk, Df )>∂y(f - f )(xk, yk, Df) I]
t=0
≤Lg μ-1Eh∣∂x(f - f )(xk ,yk, Df)Tdy (f - f )(xk,yk, Df )∣]
1	1
≤Lg μ-1Eh∣∣dx (f - f )(xk,yk, Df )2∣∣i 2 Endy (f - f )(xk ,y, Df )∣∣ J ≤	Lg μ-1σf
where we used that PN01(1 - βkμg)≤ 瓦? for the second line, Cauchy-Schwarz inequality to get
the third line and Assumption 4 to get the last line.
24
Published as a conference paper at ICLR 2022
TΓ*	1	T T 7^ / ɪ T ∙	,1 ,	∙ ∙ 1	1 , f	∙.
Bound on Wk0 Using that ξN+1,k is independent from zk, we write:
Wk0 =E
(g - g)(xk,yk,ξN+ι,k)z⅛∣∣
+ E[∣∣∂χyg(χk,yk)(zk - Zk)『]
(i) 2
≤ σg2xy
E [kzk『]+(Lg )2E "k-k『]
(≤)2σg2xy Ehkzk - z?(xk,yk)k2i + Ehkz?(xk, yk)k2i + (L0g)2Ehkzk - zZkk2i
(i≤2σgχy Ehkzk-Z* (Xk ,yk )k2 i +(Lg )2Ehkzk- Zk『]+2σgχy B2 μ-
(≤iv)2σg2xy ΠkEhzk0 - z?(xk, yk)2 + Rzki
+ (Lg)2(4σgyyμ-2∏kEh∣∣z0 -z*(xk,Vk)『[+2Rk) +2σgχyB2μ-2
≤2(σgχy +2(Lg)2μ-2σgyy)πkEh∣∣z0 -z*(xk, Vk)∣∣2]
+ 2(σgχy +(Lg )2 )Rk +2σ∣χy B2μ-2
(i) follows from Assumptions 3 and 5, (ii) uses that kzkk2 ≤ 2 kzk - z k2 + kz k2 , (iii) uses that
∣∣z*(xk,yk)k ≤ Bμ-1 by Proposition 6. Finally (iv) follows by application of Proposition 7. We
further have by definition of Rzk that:
Rk ≤ B2L-1μ-3σgyy+ 3μ-2σf
(29)
Combining the inequalities on Wk0, Wk00 and (29), we get that Vkψ ≤wx2 +σx2 Πk Ekz, with wx2 and σx2
given by (16).
C General analysis of AmIGO
C.1 Analysis of the outer-loop
Proofof Proposition 2. We treat both cases μ ≥ 0 and μ < 0 separately. For simplicity We denote
by Ek the conditional expectation knowing the iterates xk , Vk and zk-1 and write ψk
Ek [Ψk].
Case μ ≥ 0. Recall that Ex is given by:
Ex = η2kEUxk - x?k2] +(1 - u)E[L(xk) - L?].
For simplicity define ∈k = uδk + (1 - u), e《=(1 - u)(L(xk) - L?) + η2k∣∣xk - x?『and e]=
uδk(L(xk) - L?). It is then easy to see that E[ek] is equal to the l.h.s of (10), i.e. E[ek] = Ekx. We
25
Published as a conference paper at ICLR 2022
will start by bounding the difference between two successive iterates of ek:
ek + ek - ek-1 ≤uδk (L(Xk ) - L*) + (1 - U)(L(Xk ) - L(xk-1))
十 与 Ilxk — x?『一η- llxk-ι — x?『
(i)
≤u⅜(L(xk) - L?) + (1 - u)(L(xk) - L(xk-1))
--2-k ∣∣χk-1 - x*『+ δk (2 ∣∣xk-1 - x*『-▽L(Xk-I)>(xk-1 - x?))
+ ɪ 12(ψk-1( - 2Yk (ψk-1 -▽L(Xk-I)) (Xk-I- x*))
(沉)
≤ uδk(L(xk) - L*) + (1 - u)(L(xk) - L(xk-1))
----Jk-1 ∣∣xk-1 - x*『-δk (L(Xk-I) - L*)
+ ɪ (γ2 卜k-1( - 2γk (ψk-1 —▽L(Xk-I)) (Xk-I- x*)
≤(uδk + (1 - u))(L(xk)-L(xk-i))
---竺^-1 ∣∣xk-1 - x*『-δk (I - U)(L(Xk-I) - L?)
+ 华12(ψk-1( - 2Yk (ψk-1 -▽L(Xk-I)) (Xk-I- x*)
≤Ek(L(Xk) - L(Xk-I)) - δkek-1
+ ɪ (γ2 卜k-1( - 2γk (ψk-1 —▽L(Xk-I))	(Xk-I- x?)
竺)C	.	、T,	、，ekL	2
≤ - δkek-1 + ek▽L(Xk-I) (xk - xk-1)+-二 ∣∣xk - Xk-Ill
T
+今
^
-2γk Ψk-ι - ▽£(Xk-I)	(xk-1 - x?)
-δkek-1 - YkCk▽L(Xk-I)Tψk-1 +-^k-
ψk-J∣
、T
+η2k Wk-『
-2γk Ψk-ι -▽£(Xk-I)	(xk-1 - x?).
^
∕∙∖ 11	Γ∙	,1	1 ,	•	7	z∙ ∙ ∖ Γ∙ 11	Γ∙	. 1	.,CC
(i) follows from the update expression Xk = Xk-1 — YkΨk-1, (ii) follows from the convexity of L
and (iii) follows by L-smoothness of L. Taking the expectation conditionally on the randomness at
iteration k - 1 and using that Ek-1 [ψk-1] = Ψk-1, We therefore get
Ek-Ilek + ek - ek-1] ≤ - δkek-1 - Ykek▽L(Xk-I)Tψk-1 + ^2(δk + ek )Ek-1
-δk (ψk-1 - ▽L(Xk-I)) (xk-1 - x?)
ψk-J∣
=-δk ek-1 + Yk Sk (Ek-In | ψk-1 - ψk-1 ||] + ∣∣ψk-1 - ▽L(Xk-I)『
-	δk(ψk-1 - ▽£(Xk-I))T(Xk-1 - Yk▽£(Xk-I) - X?)
-	^2(ek - δk )〔1▽L(Xk-I)『
≤ - δk ek-1 + Yk Sk (Ek-I || | ψk-1 - ψk-1 ||] + ∣∣ψk-1 - ▽L(Xk-I)『
-	δk (ψk-1 - ▽L(Xk-I))T(Xk-I- Yk▽L(Xk-I)- x?)
26
Published as a conference paper at ICLR 2022
where (i) follows from δk ≤ k since by construction δk ≤ 1. Taking the expectation w.r.t. all the
randomness and applying Cauchy-Schwarz inequality to the last term yields the following inequal-
ity:
uδk(L(xk) -L?)+Ekx ≤(1 - δk)Ekx-1 +γksk Vkψ-1 + Ekψ-1	(30)
+ δk(E?-i) 2Ehkxk-I- γkVL(xk-ι) - x?『i 2.
Since L is convex, we have the inequality: kxk-ι — γkVL(xk-ι) — x?『 ≤ ∣∣xk-ι - x?『. Hence,
we can deduce that:
δkkxk-1 - γkVL(xk-1) - x?k2 ≤ δkkxk-1 - x?k2 ≤ 2γkηkηk--11Ekx-1 ≤ 2γkEkx-1,
where we used that ηk is non-increasing by construction. Combining the above inequality with (30)
yields:
Fk + Ex ≤(1 - δk)EX- + Yk Sk (V3 + Eψ-ι) + √Y δ∕(Eψ-J 2 倒-#.
Case μ < 0. Recall that for μ < 0, we set Ex
have that:
L E [∣VL(xk )『]
. Using that L is L-smooth, we
L(xk) - L(xk-ι) ≤VL(xk-ι)>(xk - xk-ι) + — Ilxk - xk-ι∣2
≤ - γkVL(xk-1)>ψk-1 + LYkIIψkT∣l
≤ - YkkVL(xk-ι)∣2 - YkVL(xk-ι)> (Ψk-1 - VL(xk-ι))
+--2k (∣∣ψk-1 - ψk-11| +2(ψk-i - ψk-l) ψk-1 + kψk-l∕
Taking the expectation w.r.t. all randomness in the algorithm in the above inequality, we get:
E[L(xk) - L(xk-1)] ≤-YkE kVL(xk-1)k2 - YkEVL(xk-1)>(ψk-1 - VL(xk-1))
+ LY2 (E (I∣ψk-1- ψk-ι∣∣2) + E[kψk-ιk2i)
=-Yk(1- LYk )E[kVL(xk-i)『i + LY2 (Vψ-1 + Eψ-J
- Yk(1 - LYk)EVL(xk-1)>(ψk-1 - VL(xk-1))
≤ - YEhkVL(xk-i)『i + 容(Vψ-1 + Eψ-J
1	1
+ YkE[∣VL(xk-ι)k2] 2 (Eψ-ι)2∙
=-δkEx-ι + * (Vψ-1 + EΨ-ι) + √2δ2Yk (Ex-I)2 (EΨ-ι)1.
where we used that 1 - LYk ≥ ɪ and 0 ≤ 1 - LYk ≤ 1 to get the last inequality. Using the definition
of Fk yields an inequality of the form:
Fk + Ex ≤(1 - δk)EX- + Yk Sk (Vk-I + Eψ-ι) + √Y δ[(Ef-J 2 (Ex-I)2.
Hence, in both cases μ ≥ 0 and μ < 0 we get an inequality of the of the same form, but with different
expressions for Fk and Sk. We get the desired result using Young’s inequality, to upper-bound the
last term in the r.h.s. of the above inequality. More precisely, we use that for any 0 < ρk < 1:
√Yδj(Eψ-ι)2(Ex-ι)2 ≤ 2PkδkEx-1 + P-1YkEψ-1.
□
27
Published as a conference paper at ICLR 2022
C.2 Inner-level error bound
In this section we prove Proposition 5 which controls the evolutions of the warm-start errors Eky and
Ekz. As a first step, in Proposition 12, we provide a result controlling the mean squared error between
two successive iterates xk-1, xk and yk-1, yk which will be used in the proof of Proposition 5.
y
Proposition 12 (Control of the increments of xk and yk). Consider ζk, φk and Rky as defined in
Proposition 8 for some fixed 0 ≤ v ≤ 1. Then, the following holds:
γ2E ∣∣ψk-ι1] = Ehkxk-Xk-i『i ≤y2(v3 +2Eψψ-ι +2ZkEk-J
Ehkyk-yιk2i ≤2φkEk + 2Rk
Proof. Proof of Proposition 12 We prove each inequality separately.
Increments of xk. By the update equation, we have that xk = xk-1 - γkψk-1, hence we only need
to control E ∣∣ψ5k-ι∣∣ ]. We have the following:
E ∣∣ψk-ι∣∣ ] ≤E ∣∣ψk-ι - Ψk-ι∣∣j +2E[kψk-ι - VL(xk-i)『i +2EhkVL(xk-i)『i
=Vk-ι +2Ek-I +2EhkVL(xk-1)k i.
In the case (μ < 0), We have Eχ-ι = 芸E[kVL(xk—i)『],hence by setting Zk := 2L, We get the
desired inequality. In the convex case (μ ≥ 0), since L is L-Smooth, we have that:
kVL(xk-1)k2 ≤ 2L(L(xk-1) -L?) ≤ 2L(1 - u)-1Ekx-1,
provided that U < 1. We also have that (L(xk-ι) -L?) ≤ LLkxk-ι - x?『≤ Ln--IEk-I which
yields ∣∣VL(xk-ι)k2 ≤ 2L2n-11Eχ-1. Hence, we can set Zk = 2Lmin ((1 - u)-1, Ln--J.
Increments of yk. Denoting by Dgt a batch of samples at time iteration t of algorithm Ak and using
the update equation of yt we get the following inequality by application of the triangular inequality:
1	T-1	1	T-1	1
E[kyfc - yfc-ik2] 2 ≤αk X E[∣∣∂yg(xk,yt,Dg)∣∣2] 2 ≤ αfc X (σg + LgEbIyt- y*(x®)∣∣1) 2
t=0	t=0
T-1
≤αk X (σg + LgRk + LgΛt,kEy) 2 ≤ αkT(σg(1 + 2Lgakμ-1) + LgEy) 2
t=0
where we applied Proposition 7 for every 0<t≤T-1 to get the second line with Λt,k :=(1 - akμg)t.
This directly implies the following bound:
Ehkyk-yk-ik2i ≤ 2akT2(σg(1 + 2Lgμ-1ak) + LgEk).	(31)
On the other hand, using a triangular inequality and applying Proposition 7, we also have that:
Ehkyk - yk-1k2i ≤ 2Ehkyk - y?(xk)k2i + 2Ehkyk-1 - y?(xk)k2i ≤ (4Eky+2Rky).	(32)
The result follows by combining (31) and (32) using coefficients 1-v and v.	□
C.3 Proof of Proposition 5
Proof of Proposition 5 . We will control each of Eky and Ekz separately.
28
Published as a conference paper at ICLR 2022
Upper-bound on Eky. Let rk be a non-increasing sequences between 0 and 1. The following holds:
Eky =E kyk-1 - y?(xk-1)k2 +E ky?(xk - y?(xk-1)k2
+ 2Eh(yk-1 - y?(xk-1))>(y?(xk - y?(xk-1))i
≤(1 + Tk)E∣j∣yk-i - y*(xk-i)∣∣2i +(1 + r-1)Ehky?(xk - y*(xk-i)∣2]
7(1 + rkNAk-M-I + Ry-I) + 2r-1E[ky?(xk) - y*(xk-i)『i
(≤⅛+Tk NAk-M-I + Ry-I) + 2Ly t-ie[∣∣xk -xk-1k2i
(iv)
iv)
≤ (1 + TkMAk-IEkI + RL) + 2Lyr-1γ2E M
k-12
(33a)
(i) follows by Young’s inequality, (ii) uses Proposition 7 to bound the first term and that (1 +rk-1) ≤
2rk-1 for the second term, (iii) uses that y? is Ly-Lipschitz by Proposition 6 and (iv) uses the update
equation xk = xk-1 - γkψk-1.
Upper-bound on Ekz. Similarly, for a non-increasing sequence 0 < θk ≤ 1, we have that:
Ekz =Ehkzk-1 - z?(xk-1, yk-1)k2i + EhIIIz?(xk, yk) - z?(xk-1, yk-1)2IIIi
+ 2E (zk-1 - z?(xk-1, yk-1))>(z?(xk, yk) - z?(xk-1, yk-1))
≤(1 + Ok)E∣j∣Zk-ι - z?(χk-ι,yk-ι)k2i +(1 + θ-1)Ehkz*(χk,yk) - z*(χk-ι,yk-ι)∣2]
(ii)(1 + Ok)(∏k-iEz-i + Rk-I) + 2θ-1E[kz?(xk,yk) - z*(xk-i, yk-1)k2i
≤ (1 + θk)(πk-1Ez-I + Rk-l) + 4L2θ-1 (Ehkxk- xk-11/ + Ilyk- yk-i『])
(iv)
≤ (1 + θk)(∏k-ιEZ-ι + Rk-ι) +4LZθ-1
ψk-Jl ] + 2φkEy +
(34a)
(i) follows by Young’s inequality, (ii) uses Proposition 7 to bound the first term and that (1 +Ok-1) ≤
2Ok-1 for the second term, (iii) uses that z? (x, y) is Lz-Lipschitz in x and y by Proposition 6 and,
finally, (iv) uses the update equation xk = xk-ι 一 YkΨk-ι for the term E[kxk 一 xk-i『] and
Proposition 12 to control the increments E kyk - yk-1 k2 .
In order to express the upper-bound on Ekz in terms of Eky-1 instead of Eky, we substitute Eky in
(34a) by its upper-bound in (33a) and use that (1 + Tk) ≤ 2 to write:
Ez ≤(1 + θk)(∏k-iEZ-i + Rk-i) +4L2θ-1γ2(1 + 4Lyφkr-1)E IM
+ 8L2θ-1 (2φk(Ak-iEy-1 + Ry-I) + Rk)
We can then express (33a) and (35) jointly in matrix form as follows:
E
γk
+
11
--
ykzk
yRkR
++
11
--
ykzk
EkE
11
--
kk
AΠ
Pk
≤
ykzk
EkEk
7
ψk-1
Uk + Vk
k-12
(35)
where the Pk is a 2 × 2 matrix and Uk and Vk are 2-dimensional vectors given by (17). The desired
result follows directly by substituting E
above inequality.
Ψk-ι I ] by its upper-bound from Proposition 12 in the
□
29
Published as a conference paper at ICLR 2022
C.4 General error bound
Proof of Proposition 9. First note that, by assumption, we have that δkrk-1 ≤ δk-1rk--11 and
δkθk-1 ≤ δk-1θk--11. Moreover, since αk and βk are non-increasing, we also have that Λk-1 ≤ Λk
and Πk-1 ≤ Πk . This implies the following inequalities which will be used in the rest of the proof:
ak--11Λk-1 ≤ ak-1Λk,	bk--11Πk-1 ≤ bk-1Πk.	(36)
Now, let ρk be a non-increasing sequence with 0 < ρk < 1. By Proposition 2, it follows that Ekx
satisfies the inequality:
Fk + Ek ≤ (1 - (1 - 2ρk) δk) Ek-I + Yk Sk Vψ-1 + Yk(Sk + P-I)Eψ-1	(37)
On the other hand, by Proposition 5 we know that Eky and Ekz satisfy:
EEkkyz	≤ Pk	ΠΛkk--11EEkkyz--11 ++	RRykzk--11	+YkVkψ-1+2Ekψ-1+2ζkEkk-1Uk+Vk	(38)
where the Pk , Uk and Vk are defined in (17). For conciseness, define Sk and EkI to be:
Sk := a0k b0k ,	EkI = Sk EEkkz
By (36), we directly have that:
SkPkS--I CkT ∏0-ι)≤ SkPkS-I Ok ɪŋ ：= Pk,	(39)
where the inequality in (39) holds component-wise. Therefore, multiplying (38) by Sk and using
(39) yields:
Ek ≤PkEk-1 + Sk (Pk (Ry-I) + Yk (VtI + 2Eψ-ι + 2ZkEk-) Uk + Vk)	(40)
Furthermore, by Proposition 4 we can bound Ekψ-1 and Vkψ-1 as follows:
Ekψ-1 ≤2L2ψ(Λk(ak)-1ak-1Eky-1 + Πk(bk)-1bk-1Ekz-1 + 2L2ψRky-1
Vkψ-1 ≤wk2 + σk2Πk(bk)-1bk-1Ekz-1,
where we used (36) a second time to replace Λk-1(ak-1)-1 and Πk-1(bk-1)-1 by Λk(ak)-1 and
Πk(bk)-1. By summing both inequalities (37) and (40) and substituting all terms Ekψ-1 and Vkψ-1
by their upper-bounds we obtain an inequality of the form:
Fk + Ektot ≤AkkEkk-1 + Aykak-1Eky-1 + Azkbk-1Ekz-1 + Vktot
where Akk, Ayk , Azk are the components of the vector Ak defined in (18) and Vktot is the variance
term also defined in (18). The desired inequality follows by upper-bounding Akk , Ayk , Azk by their
maximum value IlAkl∣∞ .
D Controlling the precision of the inner-level algorithms.
In this section, we prove Proposition 10. To achieve this, we first provide general conditions on Λk
and Πk for controlling the rate IAk I∞ and which hold regardless of the choice of step-sizes. This
is achieved in Proposition 13 of Appendix D.1. Then we prove Proposition 10 in Appendix D.2.
30
Published as a conference paper at ICLR 2022
D.1 CONTROLLING Πk AND Λk.
We introduce the following quantities:
D(I)	log _____________1 - (I - Pk)δk____________
1 - s 1 + 2rk 1 + 2L2ψ γk δk-1 sk + 1 + [2ρk]-
Dk2) =S log ((16Ly)-1Pkζ-1γ-2r2)
Dk3) := -S log(4Ly δk Yk r-2)
D(4) :=^_ log ____________________1 -(I - Pk')δk________________
1 - S 1 + θk 1 + 2γkδk-1 2L2ψ + σx2 Sk + 1 + (2Pk)-
Dk5) := - S log (l6L2θ-2φk)
Dk6) := - S log (2L2L-2θ-2r2 (1 + 4L2r-1φk))
(41a)
(41b)
(41c)
(41d)
(41e)
(41f)
Proposition 13. Let Pk be a non-increasing sequence of positive numbers smaller than 1. Consider
Λk and Πk so that:
log Λk ≤min Dk(1),Dk(2),Dk(3),	(42a)
log Πk ≤min Dk(1), Dk(2),logΛk + Dk(3).	(42b)
Then, the following inequalities holds:
kAkk∞ ≤ (1 — (1 - Pk)δk),	Vktot ≤ Vtot.
where Ak and Vjtot are defined in (18) of Proposition 9 and Vtot is defined as:
Vr ι=δk(3r-1 + 2Lψγkδ-1(2 + (sk + P-I)))Ry—
+ δk∏k (2θ-1RZ-i + 8L2θ-2Rk) + Yk(Sk+ 4LyΛkδkγkr-2)w2
Proof. We first prove that uIk≤uk+≤1 andpk+≤1 under (42a) and (42b) with uk+, pk+ given by:
u+ := 4Ly δkYkr-2Λk,	p+ = 16L2∏k θ-2φk.
A direct calculation shows uk+≤1 whenever (42a) holds. Moreover, recall that uIk=akUk(1)+bkUk(2)
with Uk(1) and Uk(2) being the components of the vector Uk defined in (17). Thus by direct substitu-
tion, we get the following expression for uIk :
Uk = 2LyδkYkr-2Λk(1 + 2L2L-2θ-2r2 (1 + 4Lyr-1φk) .).
Therefore, (42b) suffices to ensure that uIk ≤ uk+. Finally, (42b) implies directly that pk+ ≤ 1.
We will control each component Akx, Aky and Azk of the vector Ak separately.
Controlling Akx. Recalling the expression of Akx, the first component of Ak in (18), it holds that:
Akx = 1 -
1	I (i)
1 — 2Pk I δk + 2ζkλkuk ≤ 1 —
δk + 2ζkYkuk+
(ii)
≤1-
11
1 — 2Pk I δk + 2Pkδk
1 - (1 - Pk )δk .
(i) holds since Uk ≤uj while (ii) follows from (42a) which ensures that 2ZkYk〃+ ≤ 1 Pk δk.
31
Published as a conference paper at ICLR 2022
Controlling Ayk. Recall the expression of second component of Ayk in (18), we have:
Ak =Λk-s(l + rk (1 + 16LZ∏kθ-2Φk + 2LψYkδ-1(2成 + (Sk + P-I))))
(i)	(ii)
≤ 1 + 2rk 1 + 2Lψ γk δk- (Sk + 1 + (2Pk)	) Λk-s ≤ 1 - (1 - Pk)δk.
(i) holds since pk+ := 16Lz2Πskθk-2φk ≤ 1 and uIk ≤ 1 while (ii) is a consequence of (42a).
Controlling Azk. Similarly, recalling the expression of the third component of Ak we get that:
Ak =πk-s(1 + θk(1 + Yk δ-1(2Lψ + σ2 )(2Uk + (Sk + P-I))))
(i)	(ii)
≤ πk s(1+ θk(1 + 2γk δk (2Lψ + σX)(1 + (Sk + (2ρk ) I)))) ≤ 1-(1 -Pk )δk,
where (i) uses that uIk ≤ 1 and (ii) follows from (42b).
Controlling Vktot. Recalling the expression of Vktot from (18), we have that:
Vtot =δk (Λk(1 + r-1) + 16L2 φkθ-2∏k + 2Lψ Yk δ-1(2u{ + (Sk + P-I)))Ry-
+ δk ((1 + θ-1)∏kRk-1 + 8L2θ-2∏kRy) + Yk(sk + Uk)w2
≤δk (3r-1 + 2Lψ Yk δ-1(2 + (Sk + Pkb))RLI
+ δk (2θ-1∏kRk-1 + 8L2θ-2∏kRy) + Yk (Sk + u+)wX = Vkot
where We use 16L∣φkθ-2∏k ≤ 1 and Uk ≤ 1 for the first line and Uk ≤ u[ for the last line. □
D.2 Controlling the number of inner-level iterations
We provide now a proof of Proposition 10 which is a consequence of Proposition 13.
Proof of Proposition 10. We first provide conditions on the number of iterations T and N of algo-
rithms Ak and Bk to control the rate kAk k∞ and then provide an upper-bound on Vktot .
Conditions on T and N. We consider the setting with constant step-size Yk=Y, αk=α and βk=β
yy
and choose rk=θk = 1 and δk=δ0 for some 0<δ0<1. We also take v=1 so that φk=2 and Ryk=Rky.
By direct substitution of the parameters rk, θk, φk, Yk, δk, Pk and ζk, in the expressions of Dk(1),
Dk(2), Dk(3), Dk(4), Dk(5) and Dk(6) defined in (41a) to (41f), we verify that:
-C1 ≤	Dk(1),	-C10	≤	Dk(4),	-C2≤min(Dk(2),Dk(3)),	-C20	≤min(Dk(5),Dk(6)).
Hence, we can ensure the conditions of Proposition 13 hold by choosing T and N so that:
log Λk ≤ - max (1, C1, C2),	log Πk ≤ - log Λk - max (1, C10, C20).
This is achieved by for the following choice:
T = bα-1μ-1 max (Ci,C2,C3)C + 1,	(43)
N = b2β-1μ-1(max (Ci, C2,C3) + max (C, C2©))C + 1
Hence, for such choice, we are guaranteed by Proposition 13 that kAk k∞ ≤ 1 - (1 - Pk)δk.
Bound on the variance Vktot. By choosing T and N as in (43), we know that Λk and Πk satisfy
(42), so that the variance term Vktot is upper-bounded by Vktot . Moreover, by direct substitution of
the sequences appearing in the expression of Vktot by their values, we get:
Vtot ≤ Vr=δk(Λk(1 + r-i) + 16L2φkθ-2∏k + 2Lψη-1(2u{ + 国 + P-I)))Rk-
+ δk ((1 + θ-i)∏kRk-1 + 8L2θ-2∏kRk) + Yk(Sk + Uk)w2
≤δ0(η-1 ɪ-^ + ( ɪʒ+^ y + 4Ly Λk Y2))w2
+ δo(2Λk + 32L2∏k + 10Lψ ηk1 )Rk-ι + δ° (2∏k Rkki + 8L2∏k Ry)
32
Published as a conference paper at ICLR 2022
Furthermore, by definition of Ryk-1 and Rkz-1, we have that:
Ry-I ≤ 2μ-1L-1σg,	Rk-I ≤ μ-3(B2L-1σgyy +3μgσf).
yy	y
Moreover, recall that Ry=Ry since We chose v=1. Thus Ry ≤ 2μ- 1ɑσg. This implies that:
Vr δ-1γ-1 ≤ 卜-11:-u +(1 + 4LyΛkYDwx + 2∏kYTμ-3 83%,+ 3μgσf ) (44)
+ (4Λk + 80Lx ∏k + 20Lψ η-1)μ-1αγ-1σg
By choosing T and N as in (43), the folloWing conditions hold:
Λk ≤	4L2,	Λk	≤	5lψ% 1, πk ≤ Ykgxy+(Lg )2),	πk	≤	4L2	Lψη- 1.
4Ly	4Lz
By applying these inequalities in (44), We get:
(L0g)2	B2βσg2yy +3μgσf2
W2
Where We used that 2μg-3 σg2 + (L0g)2	B2βσg2 + 3μgσf2 ≤ wx2 by definition of wx2 in (16a).
Therefore, We have shoWn that Vktot ≤ Yδ0W2, With W2 given by (20).
□
E Stochastic Linear dynamical system with correlated noise
Let A be a positive definite matrix in Rd X Rd satisfying 0 < μg ≥ ∣σ,A) | ≤ Lg and b a vector in
Rd. We denote by z? = -A-1b. Consider Am be a sequence of i.i.d. positive symmetric matrices
in Rd × Rd such that E [Am] = A, and b a random vector in Rd such that E [b] = b, with Am and
b being mutually independent. Define ∑a = E [(An — A)>(An — A)] and denote by σA and LA
the largest singular values of ∑a and A-1∑AA-1. Let β be such that β ≤ L-. Finally let σx be an
upper-bound on E Ilb 一 b∣∣ . Let Z and z0 be two vectors in Rd and define the iterates Zn and Zn
such that z0 = z
and W
z0 and using the recursion:
Zn = (I 一 βAn)zn-1 一 βb, Zn = (I 一 βA)Zn-1 一 βb.
Hence, from the definition of Zn and Zn we directly have that:
n	nn	n	nn
Zn = Y(i - eAn)Z - X Y β(I - βAj)b,	Zn = Ya - eA”0 - X Y e(I - eA)b.
t=1	t=1 j=t+1	t=1	t=1 j=t+1
The next proposition computes the bias E Zn 一 ZZn b .
Proposition 14. The following identities hold:
Ehzn — Znlbi =(I 一 βA)n(z 一 Z0) 一 β (X(I 一 βA)n-j (b — b
E[Zn - Zn]=(I — βA)n(z — Z0)
T-I	八 El	i~ ∙	i' A	1	1	1 •	1 , ∙	,	1' A	1 1	I~I
Proof. The proof is a consequence of An and b being i.i.d. and unbiased estimates of A and b.	□
33
Published as a conference paper at ICLR 2022
The next proposition controls the mean squared errors E [∣∣zn - z? ∣∣2∣ and E [∣∣zn - Wn∣∣2].
Proposition 15. Define n?(n, β) = min(n, eɪ-). Let β such that:
β ≤ 2⅛ min Q
Then, thefollowing inequalities holds:
2Lg
μg (i+ μ-2σA)
E[∣∣zn - z?『]≤(1 - βμg )n∣z - z?『+ β2 G ||z?『+3 (n A Sσ∖
忖-z?『≤(1 - βμg)n∣z0 - z?k2.
Moreover, if z0 = Z, then we have:
E[∣∣zn - ZnII2] ≤4μ-2σA(1 - βμg)n∣z - z?|2
+ 2β2(σA kz?k2+3 (n A βμg )σ2)(n A βμg
n A β⅛)
(45)
Proof. It is straightforward to see that:
∣Zn-z*∣∣2 ≤ (1 - βμg)∣∣z0-z*k2.
Now, let,s control E[∣zn - Wn∣∣2]
.The following identity holds by definition of Zn and Zn:
E[∣∣Zn - Zn∣∣2] =E[(zn-1 - ZnT)T((I - βA)2 + β2∑A)(znT - Zn-1)]
-2βE[(Zn-1 - ZnT)T(I - βA)(b - b)]
+ β2 (Enb - b『]+ E[(Zn-1)τ∑AZnτ])
=E[(Zn-1 - ZnT)T((I - βA)2 + β2∑A)(Zn-1 - Zn-1)]
+ β2 ((zn 1) ∑AZn 1 + E (b — b) (I + 2(I — βA)Dn) (b — b)
where ∑a = E [(An - A)T(An - A)]
upper-bound the last term by:
and Dn = Pn=O(I - βA)t. By simple calculation We can
β2E (b - b)T(I + 2(I - βA)Dn)(b - b)
Moreover, provided that β ≤ L(1，^)，where LA is the highest eigenvalue of A-1∑aA-1, then
We have the following:
EUZn- Znk2] ≤(1 - βμg)E[„Zn-1 - Zn-1 U2]
+β 2((ZnT)T∑A*1+3 (nA M Enb - b
≤(1 - βμg同UZnT
-Zn-IU2] + β2 (σAUZn-1U2 + 3 (n A m蟾)
Unrolling the recursion, it follows that:
E ∣zn - zZn∣2 ≤(1
-βμg)n∣Z - Z0I2 + β2 XX(1 - βμg)n-t (σA∣∣Zn-1『+ 3 (n A
t=1	∖	∖
(46)
34
Published as a conference paper at ICLR 2022
In particular, if z = z?, then Zn = z? and We get:
E kzn
-Z*『]≤(1 - βμg)nkz - Z?k2 + β2 卜Akz?『+ 3( n ∧
To get the last inequality, We simply choose z0 = z and recall that:
pt-1∣∣ ≤ 2((1-βμg)t-1kz0 - z?k + ∣∣z*k).
Using the above in in (46) yields:
E [kzn - znk2]
≤2nβ2σA(1 - βμg)n-1kz - z?『+ 2β2 σA||z*『+ 3 n ∧
Mσ2 n ∧ βμg
Moreover, by Lemma 3 we know that nβ2μg (1 - βμg)n-1 ≤ (1 - βμg)n-1 and since βμg ≤ 1, we
have that (1 -弩)-1 ≤ 2 so that (1 - βμg)n-1 ≤ 2(1 - βμg)n. Hence, we can write:
EhkZn- znk2i ≤4μ-2σA(I- ^^μg)nkz - z?k2 + 2β2 (σAkz?『 + 3(n ∧ βμ-卜2χn ∧ βμ-
□
β ≤ 2⅛ min(1
Lemma 2. Let A and ΣA be symmetric positive matrix in Rd × Rd with σA2 its largest singular
value of ∑a and 0 < μg ≤ σ,A) ≤ Lg. Let β be a positive number such that:
2Lg
μg (1 + μ-2σA)
Then the following holds:
∣∣ (I- β A) 2 + 稼夕 AIIop ≤ 1 - βμg .
Proof. First note that β ≤ 十,so that I - βA is positive. Now, we observe that
∣∣(I - βA)2 + β2∑A∣∣op ≤ (1 - βμg)2 + β2σA which holds since I - βA is positive. And since
β ≤ μ理仃2 , we further have (1 - βμg)2 + β2σA ≤ 1 - βμg, which yields the desired result. □
Lemma 3. Let 0 ≤ b < 1 and n ≥ 1, then the following inequality holds:
nb2(1 - b)n-1 ≤ (1 - 2b)n-1.
Proof. We consider the function h(n, b) defined by:
h(n, b) := (n - 1) log
We need to show that h(n, b) is non-negative for any n ≥ 1 and 0 ≤ b < 1. For this purpose, we fix
b and consider the variations of h(n, b) in n:
∂nh(n, b) = log
1
n
∂nh(n, b) is non-negative for n ≥ n? = log (1-2) and non-positive for all n ≤ n?. Hence,
h(n, b) achieves its minimum value in n? over the (0, +∞). We distinguish two case depending on
whether n? is greater of smaller than 1.
Case n? ≤ 1. In this case n 7→ h(n, b) is increasing on the interval [1, +∞) since ∂nh(n, b) ≥ 0
for n ≥ n?. Hence, h(n, b) ≥ h(1, b) for all n ≥ 1. Moreover, since h(1, b) = - log(b2) ≥ 0 the
result follows directly.
35
Published as a conference paper at ICLR 2022
Case n? > 1. In this case we still have h(n, b) ≥ h(n?, b) for all n ≥ 1, since n? achieves the
minimum value of h. Thus we only need to show that h(n?, b) ≥ 0. Using the expression of n?, we
have:
h(n?, b) =1 — log (ɪ——2) — log (n*b2)
=1 ——^? — log (n*b2)
Since n? > 1, the first term 1 - n? is non-negative, thus We only need to show that n?b2 ≤ 1 so that
the last term is also non-negative. It is easy to see that n?b2 ≤ 1 is equivalent to having h(b) ≥ 0,
where we define the function h(b) as:
〜	1 - b C
h(b) = log 2 - b2.
1-b
-c-r τ	1	.1	∙	07 1	, ∙	∙ , 1	1 ∙ 1 ∙	1
We can analyze the variations of b be computing its derivative which is given by:
1
dbh㈤=(I- b)(2- b) - 2b.
Hence, we have the following equivalence:
∂bh(b) ≥ 0 Q⇒ 2b(1 — b)(2 — b) ≤ 1
This is always true for 0 ≤ b < 1 since b(1 - b) ≤ ɪ so that 2b(1 - b)(2 - b) ≤ 2-b ≤ 1. Thus
we have shown that h is increasing over [0, 1) so that h(b) ≥ h(0) = 0. As discussed above, this is
equivalent to having n?b2 ≤ 1, so that h(n?, b) ≥ 0 which concludes the proof.
□
F Experiments
F.1 Details of the synthetic example
We choose the functions f and g to be of the form: f (x, y) := ɪχ>Af X + y>Cf and g(χ, y):=
1 y>Agy + y>BgX where Af and Ag are symmetric definite positive matrices of size dχ X dχ and
dy × dy, Bg is a dy × dx matrix and Cf is a dy vector with dx = 2000 and dy = 1000.
We generate the parameters of the problem so that the smoothness constants L and Lg are fixed to
1, κL=10 and κg taking values in {10i, i ∈ {0, .., 7}}. We then solve each problem using different
methods and perform a grid-search on the number of iterations T and M of algorithms Ak and Bk .
We fix the step-sizes to γk=1∕L and ak =βk=1/Lg and perform a grid-search on the number of
iterations T and M of algorithms Ak and Bk from {10i, i ∈ 0, 1, 2, 3}. For AID methods without
warm-start in Bk, we consider an additional setting where M increases logarithmically with k,
as suggested in Ji et al. (2021), with M =b103 log(k)c. Similarly, for (ITD) and (Reverse), we
additionally use an increasing T of the same form.
F.2 Experimental details for Logistic regression
The inner-level and outer-level cost functions for such task take the following form:
f(x,y) = ∣d^-t X L(y,ξ),	g(x,y) = ∣D^∣ X L(y,ξ) + jd Xeχp(Xi)ky.,∕∣2
|Dval| ξ∈Dval	|Dtr| ξ∈Dtr	pd i=1
For the default setting, we use the well-chosen parameters reported in Grazzi et al. (2020); Ji et al.
(2021) where αk=γk=100, βk=0.5, and T =N =10. For the grid-search setting, we select the best
performing parameters T, M and βk from a grid {10, 20} × {5, 10} × {0.5, 10}, while the batch-
size (chosen to be the same for all steps of the algorithms) varies from 10 * {0.1,1, 2,4}. We also
compared with VRBO (Yang et al., 2021) using the implementation available online and noticed
instabilities for large values of T and N, as reported by the authors, but also a drop in performance
compared to stocBiO for smaller T and N due to inexact estimates of the gradient.
36
Published as a conference paper at ICLR 2022
AID-CG; Kg=Ie+00
ιo^20∙
IO-5 -
招 IO-10-
□
-i IO-15-
IO-2 IO1 IO4
AmIGO-CG; Kg=Ie+00	AmIGO-GD; Kg=Ie+00
IOe-
IO-5-
IQ-LO.
ιo-u∙
IO-2 IO1 IO4
IO-5-
IQ-LO-
IQ-g-
IO-2 IO1 IO4
ιoβ-
AID-N; Kg=Ie+00
IO-5-
IQ-LO-
IQ-S-
10-2β-
AID-FP; Kg=Ie+00
IQ-S.
io-10-
IQT"
ι(r≈β
10-»-
IO-20.
AID-CG; Kg=Ie+04
AmIGO-CG; Kg=Ie+04
AmIGo-GD; Kg=Ie+04.
IO-4
M IQT
3
IO-12
IQT6
T≡一-1
iδz≡ Io1 Ior
AID-CG; Kg=Ie+05
ιo^2 iδ1 Ior
ιo^2 iδ1 Ior
AmlGo-CG； Kg=Ie+05	AmlGo-GD; Kg=Ie+05
IO-1
S
OiO-3
F=:1N：1 1
IO-5
io-2 ioi ios^
io-2 iδi ios^
io-2 iδi Ior
S
,gio-1：
IOe ：
10β-
ιo-5-
IQ-LO-
10-u-
10-»-
IO-2 IO1 IO4
AID-N; Kg=Ie+01
10-2 IO1 IO4
AID-N; Kg=Ie+03
10β;
IO-1:
IO-2 IO1 IO4
TimeZs
AID-N; Kg=Ie+04
IO-1:
10^2 IO1 IO4
AID-N; Kg=Ie+05
IO-2 IO1 IO4
AID-N; Kg=Ie+07
IO-1:
10-2 IO1 IO4
TimeZs
ιo-5∙
IQ-IO.
IQT5.
10-≈<,
10^^2 IO1 IO4
AID-FP; κg=le+01
1Q-2 IO1 IO4
AID-FP; Kg =le+03
10^^2 IO1 IO4
TimeZs
AID-FP; Kg=Ie+04
IO-1:
舍・诩
工
自・诩
ŋ::ɪ
IO-1:
10^2 IO1 IO4
AID-FP; κg=le+05
10^^2 IO1 IO4
AID-FP; Kg=Ie+07
IO-1:
1Q-2 IO1 IO4
TimeZs
110»
WT
110»
WT
810»
WT
⅞f
舍・诩
≡∣10°τ
自・诩
≡∣10°τ
官■诩
僵尸
ŋ::ɪ
-10»
-10»
-10»
-10»
：10»
；WT
-10»
；WT
-10»
.T
-10»
RT



Figure 2: Evolution of the relative error vs. time in seconds for different AID based methods on
the synthetic example. Each column corresponds to a method (AID-CG, AmIGO-CG, AmIGO-GD,
AID-N, AID-FP) and each row corresponds to a choice of the conditioning number κg . For each
method we consider T and N from a grid {1, 10, 102, 103} × {1, 10, 102, 103}. Lightest colors
corresponds to smaller values of N while nuances within each color correspond to increasing values
of T.
37
Published as a conference paper at ICLR 2022
Reverse; Kg=Ie+00
ITD; κg=le+00
ιo3
IO2
IO1
IO0
IO3
IO2
IO1
IO0
IO3
IO2
IO1
IO0
Time/s
Time/s
Reverse; κg=le+04
ITD; κg=le+04
T
S
T
S
T
S
Time/s
Time/s
ιo3
IO2
τ
IO1
F0
IO3
IO2
τ
IO1
IO0
IO3
IO2
T
IO1
IO0
Figure 3: Evolution of the relative error vs. time in seconds for different ITD based methods on the
synthetic example. From the left to the right, the first two columns correspond to Reverse and ITD
method small conditioning numbers κg ∈ {1, 10, 103}, last two column are for higher conditioning
numbers κg ∈ {104, 105, 107}. For each method we consider T ∈ {1, 10, 102, 103}. Lightest colors
correspond to smaller values of T .
F.3 Dataset distillation
Dataset distillation (Wang et al., 2018; Lorraine et al., 2020) consists in learning a small synthetic
dataset such that a model trained on this dataset achieves a small error on the training set. Specifi-
cally, we consider a classification problem of C classes using a linear model and a training dataset
Dtr where each training point ξ ∈ Dtr is a d-dimensional vector with a class cξ ∈ {1, ..., C}. The
linear model is represented by a matrix y ∈ Rc×d multiplying a data point yξ and providing the
logits of each class. The dataset distillation can be cas as a bilevel problem of the form:
min	1 X CE(y*(x,λ)ξ,Cξ),
x∈Rc×d,λ∈Rd |Dtr|
ξ∈Dtr
1C	1 d
y?(x,λ) ∈ arg min - fCE(yXc,c) + -ft; Vexp(λi)ky.,ik2.
y∈Rc×d C	Cd
c=1	i=1
where λ ∈ Rd is a vector of hyper-parameter for regularizing the inner-level problem which we
found beneficial to add.
Experimental setup. We perform the distillation task on MNIST dataset. We set the step-
sizes αk=βk=0.1 and T =N =10. We perform a grid-search on the outer-level step-size
γk ∈{0.01, 0.001, 0.0001} and run the algorithms for k = 10000 iterations.
38
Published as a conference paper at ICLR 2022
(d): VaIleIation loss,画=
IOO (e)： Validation accuracy, ∖D∖
98%
95%
…号90%∙
,	, y 8β%
<
85%
'"一 82%
0.0	0.2	0.4	0.6	0.8	1.0	1.2
Gradient evaluations le2
(f): TeSt accuracy, ∣D∣ = IOO AmlGC)-CG
——AmlGO-GD
% % % % %
2 0 8 6 4
8 8 7 7 7
0.0	0.2	0.4	0.6	0.8	1.0	1.2
Gradient evaluations le2
AID-CG
AID-FP
AID-GD
BSA
ITD
Reverse
TTSA
0.0	0.2	0.4	0.6	0.8	1.0	1.2
Gradient evaluations le2
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Gradient evaluations le2
(e): Validation accuracy, ∖D∖
98%
95%
*%
2 90%∙
y 8β%
<
85%
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Gradient evaluations le2
% % % %
2 0 8 6
8 8 7 7
O AUeJ nb4
(f)： TeSt accuracy, ∣D∣ = IOOO
0.00 0.25 0.50 0.75 1.00 1.25 1.50
Gradient evaluations le2
AmIGO-CG
——AmIGO-GD
-AID-CG
--AID-FP
-AID-GD
BSA
-ITD
——Reverse
-TTSA
—StocBiO
一(d): Validation loss, |初=2000
0.8?
(e)： Validation accuracy, |加=2000 ow (f): TeSt accuracy, 口 = 2000
98%j	82%
---AmIGO-CG
——AmIGO-GD
0.3	82%
95%
*%
2 90%
y 8β%
<
85%
0.0	0.5	1.0	1.5
Gradient evaluations le2
74%
%%%
0 8 6
-AID-CG
--AID-FP
-AID-GD
BSA
-ITD
——Reverse
0.5	1.0	1.5
TTSA
0.0	0.5	1.0	1.5
Gradient evaluations le2
0.0	0.5	1.0	1.5	2.0	2.5 3.0 3.5
0.0	0.5	1.0	1.5	2.0	2.5 3.0 3.5
---AmIGO-CG
---AmIGO-GD
=AID-CG
--AID-FP
-AID-GD
BSA
-ITD
——Reverse
_ TTSA
=StocBiO
Gradient evaluations le2
Gradient evaluations le2
Figure 4: Evolution of the validation loss (left column), validation accuracy (middle column) and
test accuracy (right column) in time (s) for different methods on the logistic regression task. Each
row correspond to different choices for the size of the batch |D| ∈ {100, 1000, 2000, 4000} chosen
to be the same for all gradient, Hessian and Jacobian-vector products evaluations. Time is reported
in seconds.
Figure 5: Performance of various bi-level algorithms on the dataset distillation task on MNIST
dataset.
—■ AmlGO-CG
---AmIGO-GD
-AID-CG
--AID-FP
-AID-GD
BSA
-TTSA
—StocBiO
39