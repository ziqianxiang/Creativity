Published as a conference paper at ICLR 2022
Near-optimal Offline Reinforcement Learning
with Linear Representation: Leveraging Vari-
ance Information with Pessimism
Ming Yin*	Yaqi Duant	Mengdi Wangt	Yu-Xiang Wang*
* Department of Computer Science	tDepartment of Electrical and Computer Engineering
University of California, Santa Barbara Princeton University
{ming_yin,yuxiangw}@cs.ucsb.edu {yaqid,mengdiw}@Princeton.edu
Ab stract
Offline reinforcement learning, which seeks to utilize offline/historical data to
optimize sequential decision-making strategies, has gained surging prominence
in recent studies. Due to the advantage that appropriate function approximators
can help mitigate the sample complexity burden in modern reinforcement learning
problems, existing endeavors usually enforce powerful function representation
models (e.g. neural networks) to learn the optimal policies. However, a precise
understanding of the statistical limits with function representations, remains elusive,
even when such a representation is linear.
Towards this goal, we study the statistical limits of offline reinforcement learning
with linear model representations. To derive the tight offline learning bound, we
design the variance-aware pessimistic value iteration (VAPVI), which adopts the
conditional variance information of the value function for time-inhomogeneous
episodic linear Markov decision processes (MDPs). VAPVI leverages estimated
variances of the value functions to reweight the Bellman residuals in the least-
square pessimistic value iteration and provides improved offline learning bounds
over the best-known existing results (whereas the Bellman residuals are equally
weighted by design). More importantly, our learning bounds are expressed in terms
of system quantities, which provide natural instance-dependent characterizations
that previous results are short of. We hope our results draw a clearer picture of
what offline learning should look like when linear representations are provided.
1	Introduction
Offline reinforcement learning (offline RL or batch RL Lange et al. (2012); Levine et al. (2020))
is the framework for learning a reward-maximizing policy in an unknown environment (Markov
Decision Process or MDP)1 using the logged data coming from some behavior policy μ. Function
approximations, on the other hand, are well-known for generalization in the standard supervised
learning. Offline RL with function representation/approximation, as a result, provides generalization
across large state-action spaces for the challenging sequential decision-making problems when no
iteration is allowed (as opposed to online learning). This paradigm is crucial to the success of modern
RL problems as many deep RL algorithms find their prototypes in the literature of offline RL. For
example, Xie and Jiang (2020) provides a view that Fitted Q-Iteration (Gordon, 1999; Ernst et al.,
2005) can be considered as the theoretical prototype of the deep Q-networks algorithm (DQN) Mnih
et al. (2015) with neural networks being the function representors. On the empirical side, there are
a huge body of deep RL-based algorithms (Mnih et al., 2015; Silver et al., 2017; Fujimoto et al.,
2019; Kumar et al., 2019; Wu et al., 2019; Kidambi et al., 2020; Yu et al., 2020; Kumar et al., 2020;
Janner et al., 2021; Chen et al., 2021a; Kostrikov et al., 2022) that utilize function approximations to
achieve respective successes in the offline regime. However, it is also realized that practical function
approximation schemes can be quite sample inefficient (e.g. millions of samples are needed for deep
Q-network to solve certain Atari games Mnih et al. (2015)).
1The environment could have other forms as well, e.g. partially-observed MDP (POMDP) or non-markovian
decision process (NMDP).
1
Published as a conference paper at ICLR 2022
To understand this phenomenon, there are numerous studies consider how to achieve sample efficiency
with function approximation from the theoretical side, as researchers find sample efficient algorithms
are possible with particular model representations, in either online RL (e.g. Yang and Wang (2019;
2020); Modi et al. (2020); Jin et al. (2020); Ayoub et al. (2020); Jiang et al. (2017); Du et al. (2019);
Sun et al. (2019); Zanette et al. (2020); Zhou et al. (2021a); Jin et al. (2021a); Du et al. (2021)) or
offline RL (e.g. Munos (2003); Chen and Jiang (2019); Xie and Jiang (2020); Jin et al. (2021b); Xie
et al. (2021a); Min et al. (2021); Duan et al. (2021); Nguyen-Tang et al. (2021); Zanette et al. (2021)).
Among them, the linear MDP model (Yang and Wang, 2020; Jin et al., 2020), where the transition is
represented as a linear combinations of the given d-dimensional feature, is (arguably) the most studied
setting in function approximation and there are plenty of extensions based upon it (e.g. generalized
linear model (Wang et al., 2021b), reward-free RL (Wang et al., 2020), gap-dependent analysis (He
et al., 2021) or generative adversarial learning (Liu et al., 2021)). Given its prosperity, however, there
are still unknowns for understanding function representations in RL, especially in the offline case.
•	While there are surging researches in showing provable sample efficiency (polynomial
sample complexity is possible) under a variety of function approximation schemes, how
to improve the sample efficiency for a given class of function representations remains
understudied. For instance, given a neural network approximation class, an algorithm that
learns the optimal policy with complexity O(H10) is far worse than the one that can learn
in O(H3) sample complexity, despite that both algorithms are considered sample efficient.
Therefore, how to achieve the optimal/tight sample complexity when function approximation
is provided is a valuable question to consider. On the other hand, it is known that tight
sample complexity, due to the limit of the existing statistical analysis tools, can be very
tough to establish when function representation has a very complicated form. However, does
this mean tight analysis is not hopeful even when the representation is linear?
•	Second, in the existing analysis of offline RL (with function approximation or simply the
tabular MDPs), the learning bounds depend either explicitly on the data-coverage quantities
(e.g. uniform concentrability coefficients Chen and Jiang (2019); Xie and Jiang (2020),
uniform visitation measure Yin et al. (2021); Yin and Wang (2021a) and single concen-
trability Rashidinejad et al. (2021); Xie et al. (2021b)) or the horizon length H (Jin et al.,
2021b; Uehara and Sun, 2021). While those results are valuable as they do not depend on
the structure of the particular problem (therefore, remain valid even for pathological MDPs),
in practice, the empirical performances of offline reinforcement learning are often far better
than those non-adaptive bounds would indicate. Can the learning bounds reflect the nature
of individual MDP instances when the MDP model has a certain function representation?
In this work, we think about offline RL from the above two aspects. In particular, we consider the
fundamental linear model representations and ask the following question of interest:
Can we achieve the statistical limits for offline RL when models have linear representations?
1.1	Related works
Offline RL with general function representations. The finite sample analysis of offline RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (MUnos, 2003; Szepesvari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the
improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefficients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their finite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 - γ)-1.
Offline RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H 3/dm2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H 2/dm) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren
2
Published as a conference paper at ICLR 2022
et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC?/2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
offline learning bound that is not only instance-dependent but also subsumes previous optimal results.
Offline RL with linear model representations. Recently, there is more focus on studying the
provable efficient offline RL under the linear model representations. Jin et al. (2021b) first shows
offline RL with linear MDP is provably efficient by the pessimistic value iteration. Their analysis
deviates from their lower bound by a factor of d ∙ H (check their Theorem 4.4 and 4.6). Later, Xie
et al. (2021a) considers function approximation under the Bellman-consistent assumptions, and,
when realized to linear MDP setting, improves the sample complexity guarantee of Jin et al. (2021b)
by an order O(d) (Theorem 3.2).2 However, their improvement only holds for finite action space
(due to the dependence log |A|) and by the direct reduction (from Theorem 3.1) their result does not
imply a computationally tractable algorithm with the same guarantee. Concurrently, Zanette et al.
(2021) considers the Linear Bellman Complete model and designs the actor-critic style algorithm
that achieves tight result under the assumption that the value function is bounded by 1. While their
algorithm is efficient (which is based on solving a sequence of second-order cone programs), the
resulting learning bound requires the action space to be finite due to the mirror descent updates in
the Actor procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1
simplifies the challenges in dealing with horizon H since when rescaling their result to [0, H], there
is a H factor blow-up, which makes no horizon improvement comparing to Jin et al. (2021b). As a
result, none of the existing algorithms can achieve the statistical limit for the well-structured linear
MDP model with the general (infinite or continuous) state-action spaces. On the other hand, Wang
et al. (2021a); Zanette (2021) study the statistical hardness of offline RL with linear representations
by proving the exponential lower bounds. Recently, Foster et al. (2021) shows realizability and
concentrability are not sufficient for offline learning when state space is arbitrary large.
Variance-aware studies. Talebi and Maillard (2018) first incorporates the variance structure in online
tabular MDPs and Zanette and Brunskill (2019) tightens the result. For linear mixture MDPs, Zhou
et al. (2021a) first uses variance structure to achieve near-optimal result and the Weighted OFUL
incorporates the variance structure explicitly in the regret bound. Recently, Variance-awareness is
also considered in Zhang et al. (2021) for horizon-free setting and for OPE problem (Min et al., 2021).
In particular, We point out that Min et al. (2021) is the first work that uses variance reweighting
for policy evaluation in offline RL, which inspires our study for policy optimization problem. The
guarantee of Min et al. (2021) strictly improves over Duan et al. (2020) for OPE problem.
1.2	Our contribution
In this work, we study offline RL for time-inhomogeneous episodic linear Markov decision processes.
Linear MDPs serve as one critical step towards understanding function approximation in RL since: 1.
unlike general function representation, linear MDP representation has the well-structured form by
the given feature representors, which makes delicate statistical analysis hopeful; 2. unlike tabular
representation, which only works for finite models, linear MDP provides generalization as it adapts to
infinite or continuous state-action spaces. Especially, we design the variance-aware pessimistic value
iteration (VAPVI, Algorithm 1) which incorporates the conditional variance information of the value
function and, by the variance structure, Theorem 3.2 is able to improve over the aforementioned state-
of-the-art guarantees. In addition, we further improve the state-action guarantee by designing an even
tighter bonus (4). VAPVI-Improved (Theorem 3.3) is near-minimax optimal as indicated by our lower
bound (Theorem 3.5). Importantly, the resulting learning bounds from VAPVI/VAPVI-Improved
are able to characterize the adaptive nature of individual instances and yield different convergence
rates for different problems. Algorithmically, our algorithm builds upon the nice Min et al. (2021)
with pessimism as we use the estimated variances to reweight the Bellman residual learning objective
so that the (training) samples with high uncertainty get less attention (Section 3). This is the key to
obtaining instance-adaptive guarantees.
2	Preliminaries
2This comparison is based on translating their infinite horizon discounted setting to the finite-horizon case.
3
Published as a conference paper at ICLR 2022
2.1	Problem settings
Episodic time-inhomogeneous linear Markov decision process. A finite-horizon Markov Decision
Process (MDP) is denoted as M = (S, A, P, r, H, d1) (Sutton and Barto, 2018), where S is the
arbitrary state space and A is the arbitrary action space which can be infinite or even continuous.
A time-inhomogeneous transition kernel Ph : S × A 7→ ∆S (∆S represents a probability simplex)
maps each state action(sh, ah) to a probability distribution Ph(∙∣sh, ah) and Ph can be different
across time. In addition, r : S × A 7→ R is the mean reward function satisfying 0 ≤ r ≤ 1. d1
is the initial state distribution. H is the horizon. A policy π = (π1 , . . . , πH ) assigns each state
Sh ∈ S a probability distribution over actions according to the map Sh → ∏h(∙∣s九) ∀h ∈ [H] and
induces a random trajectory si, aι, ri,..., SH, 0h,/h, sh+i with si 〜di, ah 〜π(∙∣s%), sh+ι 〜
Ph(∙∣Sh, ah), ∀h ∈ [H]. In particular, We adopts the linear MDP protocol from Jin et al. (2020;
2021b), meaning that the transition kernel and the mean reward function admit linear structures in the
feature map.
Definition 2.1 (Linear MDPs). 3 An episodic MDP (S, A, H, P, r) is called a linear MDP with
a known (unsigned) feature map φ : S × A → Rd if there exist d unknown (unsigned) measures
νh = (νh(i), . . . , νh(d)) over S and an unknown vector θh ∈ Rd such that
Ph (s0 | s, a) = hφ(s, a), νh (s0)i , rh (s, a) = hφ(x, a), θhi ,	∀s0, s ∈ S, a ∈ A, h ∈ [H].
where ∣∣Vh(S)|卜 ≤ √d and max(kφ(s, a)k2 , ∣∣θh∣∣2) ≤ 1 for all h ∈ [H] and ∀s, a ∈ S × A.
kμh(S )k = Rs kμh(S)kds ∙
V-values and Q-values. For any policy ∏, the V-value functions Vh (∙) ∈ RS and Q-value functions
Qn(∙, ∙) ∈	Rs×a are defined as:	Vh(S)	=	E∏[Pt=『好=s],	Qh(s,a)	=	E∏[Pt=『小/=
s, a], ∀s, a, h ∈ S, A, [H]. The performance measure is defined as vπ := Ed1 [V1π] = Eπ,d1
PtH=1 rt .
The Bellman (optimality) equations follow ∀h ∈ [H]: Qn = rh + PhV∏+ι, Vn = Ea〜∏.[Q∏], Qh
rh + PhVh+1, Vh? = max。Qh(∙,a) (where Qh, Vh, Ph are vectors). By Definition 2.1, the Q-values
also admit linear structures, i.e. Qhπ = hφ, whπi for some whπ ∈ Rd (Lemma H.9). Lastly, for a policy
π, we denote the induced occupancy measure over the state-action space at any time h ∈ [H] to be:
for any E ⊆S×A, dh(E) := E[(sh,ah) ∈ E|si 〜dι,a% 〜π(∙∣Si), Si 〜。一卜卜一,。一),1 ≤
i ≤ h] and E∏,h[f (s, a)] := fs.xA f (s,a)d∏(s, a)dsda. Here for notation simplicity we abuse dh(∙)
to denote either probability measure or density function.
Offline learning setting. Offline RL requires the agent to learn the policy π that maximizes vπ ,
provided with the historical data D = {(sh,ah,rh,sh+ι)}T∈[H] rolled out from some behavior policy μ.
The offline nature requires we cannot change μ and in particular we do not know the data generating
distribution of μ. To sum up, the agent seeks to find a policy ∏alg such that v? 一 vπalg ≤ e for the
given batch data D and a given targeted accuracy > 0.
2.2	Assumptions
It is known that learning a near-optimal policy from the offline data D cannot be sample efficient
without certain data-coverage assumptions (Wang et al., 2021a; Yin and Wang, 2021b). To begin
with, we define the population covariance matrix under the behavior policy μ for all h ∈ [H]:
ςIh := Eμ,h [φ(s, a)φ(s, a)>],	⑴
since Σph measure the coverage of state-action space for data D, we make the following assumption.
Assumption 2.2 (Feature Coverage). The data distributions μ satisfy the minimum eigenvalue
condition: ∀h ∈ [H], κh := λmin(Σph) > 0 and denote κ = minh κh. Note κ is a system-dependent
(non-universal) quantity as it is upper bounded by 1/d (Assumption 2 in Wang et al. (2021a)).
We make this assumption for the following reasons. First of all, our offline learning guarantee
(Theorem 3.2) provides simultaneously comparison to all the policies, which is stronger than only
competing with the optimal policy (whereas relaxed assumption suffices, e.g. supχ∈Rd 全*：> < ∞
3This definition is a standard extension over the tabular MDPs by referencing the similar notions from the
bandit literature, i.e. from Multi-armed Bandit to Linear Bandit (Lattimore and Szepesvari, 2020).
4
Published as a conference paper at ICLR 2022
(Uehara and Sun, 2021)). As a consequence, the behavior distribution μ must be able to explore each
feature dimension for the result to be valid. Second, even if Assumption 2.2 does not hold, we can
always restrict our algorithmic design to the effective subspan of Σph, which causes the alternative
notion of κ := minh∈[H]{κh : s.t. κh = smallest positive eigenvalue at time h} (see Appendix G.1
for detailed discussions). In this scenario, learning the optimal policy cannot be guaranteed as a
constant suboptimality gap needs to be suffered due to the lack of coverage and this is formed as
assumption-free RL in Yin and Wang (2021b). Lastly, previous works analyzing the linear MDPs
impose very similar assumptions, e.g. Xie et al. (2021a) Theorem 3.2 where Σ-D1 exists and Min et al.
(2021) for the OPE problem.
Next, for any function Vh+ι(∙) ∈ [0,H - h], We define the conditional variance σv.+ι : S×A→ R+
as σVh+1 (s, a)2 := max{1, VarPh (Vh+1)(s, a)}.4 Based on this definition, we can define the
variance-involved population covariance matrices as:Ah := Eμ,h ^σ‰+∖ (s, α)-2φ(s, α)Φ(s, a)>] . In
particular, when Vh = Vf?, we use the notation Ahp instead.
3	Algorithm
Least square regression is usually considered as one of the “default” tools for handling problems
with linear structures (e.g. LinUCB algorithm for linear Bandits) and finds its popularity in RL as
well since Least-Square Value Iteration (LSVI, Jin et al. (2020)) is shown to be provably efficient for
linear MDPs, due to that Vh+1(s0) is an unbiased estimator of [PhVh+1](s, a). Concretely, it solves
the ridge regression problems at each time steps (with λ > 0 being the regularization parameter):
K2
wbh := argmin λkwk2 + X hφ(sh, ah), wi - rh - Vh+1(sh+1)
w∈Rd	k=1
(2)
and has the closed-form solution wbh = Σh-1 PkK=1 φ(skh, akh)[rk,h + Vh+1(s0hk)] with Σh-1 =
PkK=1 φ(skh, akh)φ(skh, akh)> + λI. In offline RL, this has also been leveraged in pessimistic value
iteration (Jin et al., 2021b) and fitted Q-evaluation (Duan et al., 2020). Nevertheless, LSVI could
only yield suboptimal guarantees, as illustrated by the following example.
Example 3.1. Instantiate PEVI (Theorem 4.4 in Jin et al. (2021b)) with φ(s, a) = 1s,a (i.e. tabular
MDPs)5, by direct calculation the learning bound has theform O(dH∙P. S。d∏? (s, a),二二九 a)) and
the optimal result (Yin and Wang (2021b) Theorem 4.1) gives O(Ph,s,a
、k?	VarP	(r + ~V-?)、
dh (',a)'	KOdh (s,a)	)*
The former has the horizon dependence H2 and the latter is H3/2 by law of total variance.
Motivation. By comparing the above two expressions, it can be seen that PEVI cannot get rid of
the explicit H factor due to missing the variance information (w.r.t V ?). If we go deeper, one could
find that it might not be all that ideal to put equal weights on all the training samples in the least
square objective (2), since, unlike linear regression where the randomness coming from one source
distribution, we are regressing over a sequence of distributions in RL (i.e. each sh , ah corresponds
to a different distribution P(∙∣s%, ah and there are possibly infinite many of them). Therefore,
conceptually, the sample piece (sf,a%, sf+ι) that has higher variance distribution P (∙∣sf,af) tends
to be less “reliable” than the one (s0h, a0h, s0h+1) with lower variance (hence should not have equal
weight in (2)). This suggests reweighting scheme might help improve the learning guarantee and
reweighting over the variance of the value function stands as a natural choice.
3.1	Variance-Aware Pessimistic Value Iteration
Now we explain our framework that incorporates the variance information. Our design is motivated
by previous Zhou et al. (2021a) (for online learning) and Min et al. (2021) (for policy evaluation).
By the offline nature, we can use the independent episodic data D0 = {(sh ɑf,尸f, sf0)}f∈[K] (from
μ) to estimate the conditional variance of any V-values Vf+ι via the definition [Varf Vh+ι](s, a)=
4The max(1, ∙) applied here is for technical reason only. In general, it suffices to think σ2^+ɪ ≈ VarhVh+1.
5This provides a valid illustration since tabular MDP is a special case of linear MDPs.
5
Published as a conference paper at ICLR 2022
[Ph(Vh+1)2](s, a) - ([PhVh+1](s, a))2. For the second order moment, by Definition 2.1, it holds
[PhV∕2+1] (s,a) = V Vh+1 (s0) dPh (s0 | s,a) = φ(s,a)> / V2+ι (s0) dνh (s0).
SS
Denote βh := S Vh2+1 (s0) dνh (s0), then PhVh2+1 = hφ, βhi and We can estimator it via:
K	2K
Bh = argmin^X Kφ(sh,ah}, - Vh2+ι kh+ι)] + λkβk2 = ς-1 ^Xφ(Sk,ahwh+ι 卜h+ι)
β∈Rd k=1	k=1
and, similarly, the first order moment PhVh+1 := hφ, θhi can be estimated via:
K	2K
θSh = argmin X	φ(sSkh,Sakh), θ	- Vh+1	sSkh+1	+ λkθk22 =	ΣSh-1	X φ(sSkh, aSkh)Vh+1	sSkh+1


The final estimator is defined as byh (∙, ∙):
max{1, VarhVh+ι(∙, ∙)} With VarhVh+ι(∙, ∙)
26
hΦ(∙, ∙),βhi[0,(H-h+i)2] — hΦ(∙, ∙),θhi[o,H-h+i] .6 In particular, when setting Vh+1 = Vh+ι,
it recovers σbh in Algorithm 1 line 8. Here ΣS h = PτK=1 φ(sSτh, aSτh)φ(sSτh, aSτh)τ + λId.
Variance-weighted LSVI. The idea of LSVI (2) is based on approximate the Bellman updates:
Th(V)(s, a) = rh(s, a) + (PhV)(s, a). With variance estimator σbh at hand, we can modify (2) to
solve the variance-weighted LSVI instead (Line 10 of Algorithm 1)
K
wbh:= argmin λkwk22 +
w∈Rd	k=1
[hΦ(sh,aQ,wi-rk - Vh + ι(sh+ι)]
σ2(sk,ah
2
Kφ
= Λbh-1 X
k=1
sh,ah) ∙ [rk +Vh+1 卜h+l)i
σ2(sh,ah)
where Λh = PK=I φ(sh, ah')φ(sh, ah)τ∕b2(sh, ah) + λId. The estimated Bellman update Th (acts
>
on Vh+ι) is defined as: (ThVh+ι)(∙, ∙) = φ(∙, ∙)1 Wh and the pessimism Γh is assigned to update
Qh ≈ ThVh+1 - Γh, i.e. Bellman update + Pessimism (Line 10-12 in Algorithm 1).
Tighter Pessimistic Design. To improve the learning guarantee, we create a tighter penalty design
11	1
that includes Λh 1 rather than Σh 1 and an extra higher order O(K) term:
Γh 一 O (√d ∙ (φ(∙, ∙)τΛ-1φ(∙, ∙))1∕2) + 2HK√d
Note such a design admits no explicit factor in H in the main term (as opposed to Jin et al. (2021b))
therefore is the key for achieving adaptive/problem-dependent results (as we shall discuss later).
The full algorithm VAPVI is stated in Algorithm 1. In particular, we halve the offline data into two
independent PartS With D = {(sh, αh, Trh, sh0)}h∈[H] and D0 = {(酩 αh, rh, ShO)}。瑞 for different
purposes (estimating variance and updating Q-values).
3.2	Main result
We denote quantities M1, M2, M3, M4 as in the notation list A. Then VAPVI provides the folloWing
result. The complete proof is provided in Appendix C.
Theorem 3.2. Let K be the number of episodes. If K > max{Mι, M2, M3, M4} and d> > ξ,
where ξ := SUpV∈[0,H],so^Pfc(s,α),h∈[H]
σv (s,a)
. Then for any 0 < λ < κ, with
probability 1 - δ, for all policy π simultaneously, the output πb of Algorithm 1 satisfies
H
vπ - vπb
≤ Oe(√d ∙ X En ∖!φ(, D>A>1φ3 I ) +
2H 4√d
-K~
h=1
where Λh = PK1 φ(sh2ah>φ(Sh,ah)——+ λId. In particular, we have with probability 1 一 δ,
=σVh+ι(sQh)
v? - vπb
≤ O(√d ∙ X E∏? Jφ(∙, ∙)>Λh-1Φ(∙, ∙) ) +
2H 4√d
-K~
(3)
h=1
where Λh = PK=ι $"h；h Vshah——+ λId and Oe hides universal constants and the Polylog terms.
σ	“v?+i(sh，ah)
H
6The truncation used here is a standard treatment for making the estimator to be Within the valid range.
6
Published as a conference paper at ICLR 2022
Algorithm 1 Variance-Aware Pessimistic Value Iteration (VAPVI)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Input: Dataset D = {(sh,ah,rh)}KhH=ι D0 = {同，ah,7h)}KhH=ι. Universal constant C.
_ . . . ^ ，、 .
Initialization: Set VH+ι(∙) - 0.
for h = H, H - 1, . . . , 1 do
Phase1: Regular Least-square Value Iteration for conditional variances
Set ςh J PK=I Φ(sh,ah)Φ(sh, aT)> + λI
Set Bh - ς- 1 PK=I O(Sτ, aT) ∙ Vh+1(Sh +1)2
Set Sh — ςh 1 PK=I φ(Sh,aτ) ∙ Vh+ι(sτ+ι)
Set [VarhVh+1] (∙, ∙) = (φ(∙, ∙), βh>[0,(H-h+ι)2] - [(。3 ∙)，θh)[0,H-h+1]∖
Set bh(∙, ∙)2 - max{1, VarPhVh+ι(∙, ∙)}
Phase2: Weighted Least-square Value Iteration for pessimistic updates
SetAh — PK=I φ (ST,ah) φ (sτ,aτ)> ∕b2(sτ,ah) + λ ∙ I,
Set W 一 A-1(P3 φ (sh, aT) ∙ (rh + Vh+1 (sh+ι)) ∕b2 (ST, aT))
Set Γh(∙, •) — C√d ∙ (φ(∙, ∙)>A-1φ(∙, •)) / + 2HK√d	(Use Γh for the improved version)
SetQh (∙, ∙) — φ(∙, ∙)>tbh - rh(∙, ∙)
15:	Set Qh(∙, ∙) — min {Qh(∙, ∙),H — h + 1}+
16:	Set bh(∙ | •) — argmaXnh <Qh(∙, ∙),πh(∙ | .)〉a，Vh(.) — max∏. <Qh(∙, ∙),πh(∙ | .))A
17:	end for
18: Output: {πbh }hH=1 .
Theorem 3.2 provides improvements over the existing best-known results and we now explain it.
However, before that, we first discuss about our theorem condition.
Comparing to Zhou et al. (2021a). In the online regime, Zhou et al. (2021a) is the first result
that achieves optimal regret rate with O(dH√T) in the linear (mixture) MDPs. However, this
result requires the condition d ≥ H (their Theorem 6 and Remark 7). In offline RL, VAPVI only
requires a milder condition √d > ξ comparing to d ≥ H (since for any fixed V ∈ [0,H], the
standardized quantity r+V(S，—(ThV)(s,a) is bounded by constant with high probability, e.g. by
σV (s,a)
chebyshev inequality), which makes our result apply to a wider range of linear MDPs.
Comparing to Jin et al. (2021b). Jin et al. (2021b) first shows pessimistic value iteration (PEVI)
is provably efficient for Linear MDPs in offline RL. VAPVI improves PEVI over O(√d) on
the feature dimension, and improves the horizon dependence as Ah < H ∑h implies A-1 4
H2Σh-1. In addition, when instantiate to the tabular case, i.e. φ(s, a) = 1s,a, VAPVI gives
O(√dP-Sa d∏? (s,a) JVarK'：]；+Vh+1)), which enjoys O(√H) improvement over PEVI (recall
Example 3.1) and the order O(H3/2) is tight (check Section G for the detailed derivation).
Comparing to Xie et al. (2021a). Their linear MDP guarantee in Theorem 3.2. enjoys the same
rate as VAPVI in feature dimension but the horizon dependence is essentially the same as Jin et al.
(2021b) (by translating H ≈ O(ι-γ)) therefore is not optimal. The general function approximation
scheme in Xie et al. (2021a) provides elegant characterizations for on-support error and off-support
error, but the algorithmic framework is information-theoretical only (and the practical version PSPI
will not yield the same learning guarantee). Also, due to the use finite function class and policy class,
the reduction to linear MDP only works with finite action space. As a comparison, VAPVI has no
constraints on any of these.
Comparing to Zanette et al. (2021). Concurrently, Zanette et al. (2021) considers offline RL with
the linear Bellman complete model, which is more general than linear MDPs and, with the assumption
Qπ ≤ 1, their PACLE algorithm provides near-minimax optimal guarantee in this setting. However,
when recovering to the standard setting Qπ ∈ [0, H], their bound will rescale by an H factor,7 which
could be suboptimal due to the variance-unawareness. The reason behind this is: when Qπ ≤ 1,
lack of variance information encoding will not matter, since in this case VarP (Vπ) ≤ 1 has constant
7Check their Footnote 2 in Page 9.
7
Published as a conference paper at ICLR 2022
order (therefore will not affect the optimal rate); when Qπ ∈ [0, H], VarP (V π) can be as large as
H2, effectively leveraging the variance information can help improve the sample efficiency, e.g. via
law of total variances, just like VAPVI does. On the other hand, their guarantee also requires finite
action space, due to the mirror descent style analysis. Nevertheless, we do point out Zanette et al.
(2021) has improved state-action measure than VAPVL as ∣∣E∏[φ(∙, ∙)]∣∣m-i ≤ En[∣∣φ(∙, ∙)∣∣m-i] by
Jensen,s inequality and that norm ∣∣∙∣m—i is convex for some positive-definite matrix M.
Adaptive characterization and faster convergence. Comparing to existing works, one major
improvement is that the main term for VAPVI
√dPH=ι E∏? [√Φ(∙, ∙)>Λh-1Φ(∙, ∙)]
admits no
explicit dependence on H , which provides a more adaptive/instance-dependent characterization. For
instance, if we ignore the technical treatment by taking λ = 0 and σh? ≈ VarP(Vh?+1), then for the
partially deterministic systems (where there are t stochastic Ph’s and H - t deterministic Ph’s), the
main term diminishes to √dPt=ι E∏? [y φ(∙, ∙)>Λh-1φ(∙, ∙)] with h ∈ {h : s.t. Ph is stochastic}
and can be a much smaller quantity when t H . Furthermore, for the fully deterministic system,
VAPVI automatically provides faster convergence rate O(去)from the higher order term, given that
the main term degenerates to 0. Those adaptive/instance-dependent features are not enjoyed by (Xie
et al., 2021a; Zanette et al., 2021), as they always provide the standard statistical rate O(春)(also
check Remark C.9 for a related discussion).
3.3 VAPVI-Improved: Further improvement in state-action dimension
Can we further improve the VAPVI? Indeed, by deploying a carefully tuned tighter penalty, we are
able to further improve the state-action dependence if the feature is non-negative (φ ≥ 0). Concretely,
we replace the following ΓIh in Algorithm 1 instead, and call the algorithm VAPVI-Improved (or
VAPVI-I for short). The proof can be found in Appendix D.
^
^
Γh(s,a) — φ(s,a)>
ʌ 1Λ Φ (sT,aT)	∙	rh	+	Vh+1	(sT+ι) -	ThVh+1	(sT,ah)	I ~ H3d∕κ
A-1 X ——-~ETA--~Z|+O(~K~) (4)
Theorem 3.3. Suppose the feature is non-negative (φ ≥ 0). Let K be the number of episodes. If
K > max{Mι, M2, M3, M4} and √d > ξ. Deploying Γ1 (4) in Algorithm 1. Then for any
0 < λ < κ, with probability 1 - δ, for all policy π simultaneously, the output πb of Algorithm 1
(VAPVI-I) satisfies
vπ-vπ ≤ O(√d ∙ X qE[φ(∙,∙)]>A-1E∏[φ(∙,∙)]) + O(H4Kκ)
h=1
In particular when choosing π = π?, the above guarantee holds true with Λ-1 replaced by A，-1.
Here Λ-', Ah~', ξ are defined the same as Theorem 3.2.
Theorem 3.3 maintains nearly all the features of Theorem 3.2 (except higher order term is slightly
worse) and the dominate term evolves from Eπ ∣φ∣Λ-1 to ∣Eπ [φ]∣Λ-1 . Clearly, the two bounds
hh
differ by the magnitude of Jensen’s inequality. To provide a concrete view of how much improvement
is made, we check the parameter dependence in the context of tabular MDPs (where we ignore the
higher order term for conciseness). In particular, we compare the results under the single-policy
concentrability.
Assumption 3.4 (Rashidinejad et al. (2021); Xie et al. (2021b)). There exists a optimal policy π?, s.t.
suph/,。d∏? (s, a)/dh (s, a) := C? < ∞, where dπ is the marginal state-action probability under π.
In tabular RL, φ(s, a) = 1s,a and d = S ∙ A (S,A be the finite state, action cardinality), then
Theorem 3.2	→√SAX X d∏? ('，.{ VarKadr (+,V[ ≤ 'HCKSA；
Theorem3.3	→√SA XtX 琛 Gaa) VarKadh+V+2 ≤ J HCSA
8
Published as a conference paper at ICLR 2022
Theorem 3.3 enjoys a S state improvement over Theorem 3.2 and nearly recovers the minimax rate
JH3K*s (Xie et al., 2021b). The detailed derivation can be found in Appendix G. Also, to show our
result is near-optimal, we provide the corresponding lower bound. The proof is in Appendix E.
Theorem 3.5 (Minimax lower bound). There exist a pair of universal constants c, c0 > 0 such that
given dimension d, horizon H and sample size K > c0d3, one can always find a family of linear
MDP instances M SUCh that (where ʌh = PK=I Vah：V，)；Skaa)) Satisfies (Λh)-1 exists and
Varh(Vh?+1)(skh,akh) >0∀M∈M)
inf SuP EM [v? — vπ]/ (√d ∙
≥ c.
(6)
Theorem 3.5 nearly matches the main term in VAPVI-I (Theorem 3.3) and certifies it is near-optimal.
4	Proof Overview
Due to the space constraint, we could only provide a brief overview of the key proving ideas of the
theorems. We begin with Theorem 3.2. First, by the extended valUe difference lemma (Lemma H.7),
We can convert bounding the suboptimality gap of v? - vbb to bounding PH=I 2 ∙ En [Γh(sh, ah)],
given that |(ThVh+1 - ThVh+1)(s,a)| ≤ Γh (s, a) for all s,a, h. To bound ThVh+1 - ThVh+1, by
decomposing it reduces to bounding the key quantity
φ(s, a)>Λ-1 [XX φ (sh, ah) ∙ (rh + ‰ι 国+J - (Th‰ι) (srh, a.))除国,a"	⑺
τ=1
The term is treated in two steps. First, we bound the gap of σ2b - σbh2 so we can convert σbh2 to
二V,十 ɪ. Next, since Var [r. + ‰ι (sh+J - (Th%+ι) (Slh, a.) | Slh, αh∖ ≈ σV^ ɪ, therefore by
the variance-weighted scheme in (equation 7), we can leverage the recent technical development
Bernstein ineqUality for self-normalized martingale (Lemma H.3) for acquiring the tight result, in
contrast to the previous treatment of Hoeffding inequality for self-normalized martingale + Covering.8
For the second part, one needs to further convert σb to σ?2 (Λ-1 to ʌ.-1) with appropriate
Vh+1
concentrations. The proof of Theorem 3.3 is similar but with more complicated computations and
relies on using the linear representation of φ in ΓI. (4), so that the expectation over π is inside the
square root by taking expectation over the linear representation at the beginning. The lower bound
proof uses a simple modification of Zanette et al. (2021) which consists of the reduction from learning
to testing with Assouad’s method, and the use of standard information inequalities (e.g. from total
variation to KL divergence). For completeness, we provide the full proof in Appendix E.
5	Discussion and Conclusion
This work studies offline RL with linear MDP representation and contributes Variance Aware
Pessimistic ValUe Iteration (VAPVI) which adopts the conditional variance information of the value
function. VAPVI uses the estimated variances to reweight the Bellman residuals in the least-square
pessimistic value iteration and provides improved offline learning bounds over the existing best-known
results. VAPVI-I further improves over VAPVI in the state-action dimension and is near-minimax
optimal. One highlight of the theorems is that our learning bounds are expressed in terms of system
quantities, which automatically provide natural instance-dependent characterizations that previous
results are short of.
On the other hand, while VAPVI/VAPVI-I close the existing gap from previous literature (Jin et al.,
2021b; Xie et al., 2021a), the optimal guarantee is in the minimax sense. Although our upper bounds
possess instance-dependent characterizations, the lower bound only holds true for a class of hard
instances. In this sense, whether “instance-dependent optimality” can be achieved remains elusive in
the current linear MDP setting (such a discussion is recently initiated in MAB problems (Xiao et al.,
2021)). We leave this as future work.
8Variance-reweighting in (7) is important, since applying Bernstein ineqUality for self-normalized martingale
(Lemma H.3) without variance-reweighting cannot provide any improvement.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors would like to thank Quanquan Gu for explaining Min et al. (2021) and introducing a
couple of related literatures. Ming Yin would like to thank Zhuoran Yang for the helpful suggestions
and Dan Qiao for a careful proofreading. Mengdi Wang gratefully acknowledges funding from Office
of Naval Research (ONR) N00014-21-1-2288, Air Force Office of Scientific Research (AFOSR)
FA9550-19-1-0203, and NSF 19-589, CMMI-1653435. Yu-Xiang Wang gratefully acknowledges
funding from National Science Foundation (NSF) #2007117 and #2003257.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pages 2312-2320, 2011.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021.
Sanae Amani, Christos Thrampoulidis, and Lin F Yang. Safe reinforcement learning with linear
function approximation. arXiv preprint arXiv:2106.06239, 2021.
Andras Antos, Remi Munos, and Csaba Szepesvari. Fitted q-iteration in continuous action-space
mdps. In Advances in Neural Information Processing Systems, pages 9-16, 2008a.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71
(1):89-129, 2008b.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pages
463-474. PMLR, 2020.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimiza-
tion. In International Conference on Machine Learning, pages 1283-1294. PMLR, 2020.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, pages 1042-1051, 2019.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. arXiv preprint arXiv:2106.01345, 2021a.
Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021b.
Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey. Internet
Mathematics, 3(1):79-127, 2006.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pages 1665-1674. PMLR, 2019.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong
Wang. Bilinear classes: A structural framework for provable generalization in rl. International
Conference on Machine Learning, 2021.
Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning, pages 8334-8342, 2020.
Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. International Conference on Machine Learning, 2021.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 2005.
10
Published as a conference paper at ICLR 2022
Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement
learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919,
2021.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pages 2052-2062. PMLR, 2019.
Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear
markov decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021.
Geoffrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon University,
1999.
Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with
linear function approximation. In International Conference on Machine Learning, pages 4171-
4180. PMLR, 2021.
Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling
problem. arXiv preprint arXiv:2106.02039, 2021.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning-Volume 70, pages 1704-1713, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137-2143.
PMLR, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021a.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pages 5084-5096. PMLR, 2021b.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. Advances in Neural Information Processing Systems, 2020.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with in-sample
q-learning. In International Conference on Learning Representations, 2022.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. Advances in neural information processing systems, 2016.
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. Advances in Neural Information Processing Systems, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. Advances in Neural Information Processing Systems, 2020.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pages 45-73. Springer, 2012.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Interna-
tional Conference on Machine Learning, pages 3703-3712, 2019.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-
ment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020.
11
Published as a conference paper at ICLR 2022
Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efficient genera-
tive adversarial imitation learning for online and offline setting with linear function approximation.
arXiv preprint arXiv:2108.08765, 2021.
Yifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation
with linear function approximation. Advances in neural information processing systems, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pages 2010-2020. PMLR, 2020.
Remi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pages 560-567,
2003.
Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. On finite-sample analysis
of offline reinforcement learning with deep relu networks. arXiv preprint arXiv:2103.06671, 2021.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Tongzheng Ren, Jialian Li, Bo Dai, Simon S Du, and Sujay Sanghavi. Nearly horizon-free offline
reinforcement learning. Advances in neural information processing systems, 2021.
Paul D Sampson and Peter Guttorp. Nonparametric estimation of nonstationary spatial covariance
structure. Journal of the American Statistical Association, 87(417):108-119, 1992.
Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for offline
reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890,
2022.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354-359, 2017.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pages 2898-2933. PMLR, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Csaba SzePeSVari and Remi Munos. Finite time bounds for sampling based fitted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pages 880-887, 2005.
Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undis-
counted reinforcement learning in mdps. In Algorithmic Learning Theory, pages 770-805. PMLR,
2018.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. Advances in neural information processing systems,
2020.
Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with
linear function approximation? International Conference on Learning Representations, 2021a.
12
Published as a conference paper at ICLR 2022
Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in rein-
forcement learning with generalized linear function approximation. In International Conference
on Learning Representations, 2021b.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepesvari, and
Dale Schuurmans. On the optimality of batch policy optimization algorithms. In International
Conference on Machine Learning, pages 11362-11371. PMLR, 2021.
Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Uncertainty in Artificial Intelligence, pages 550-559, 2020.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. Advances in neural information processing systems,
2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridg-
ing sample-efficient offline and online reinforcement learning. Advances in neural information
processing systems, 2021b.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pages 6995-7004. PMLR, 2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pages 10746-10756. PMLR,
2020.
Ming Yin and Yu-Xiang Wang. Asymptotically efficient off-policy evaluation for tabular rein-
forcement learning. In International Conference on Artificial Intelligence and Statistics, pages
3948-3958. PMLR, 2020.
Ming Yin and Yu-Xiang Wang. Optimal uniform ope and model-based offline reinforcement learning
in time-homogeneous, reward-free and task-agnostic settings. Advances in neural information
processing systems, 2021a.
Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with
pessimism. Advances in neural information processing systems, 2021b.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline policy
evaluation for reinforcement learning. In International Conference on Artificial Intelligence and
Statistics, pages 1567-1575. PMLR, 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint arXiv:2005.13239,
2020.
Andrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be
exponentially harder than online rl. International Conference on Machine Learning, 2021.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pages 7304-7312. PMLR, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. In International Conference on Machine Learning,
pages 10978-10989. PMLR, 2020.
Andrea Zanette, Martin J. Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods
for offline reinforcement learning, 2021.
Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Variance-aware confidence set: Variance-
dependent bound for linear bandits and horizon-free bound for linear mixture mdp. arXiv preprint
arXiv:2101.12745, 2021.
13
Published as a conference paper at ICLR 2022
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement
learning for linear mixture markov decision processes. In Conference on Learning Theory, pages
4532-4576. PMLR, 2021a.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted
mdps with feature mapping. In International Conference on Machine Learning, pages 12793-
12802. PMLR, 2021b.
14
Published as a conference paper at ICLR 2022
Appendix
A Notation List
Σph
Λph
κ
ι
σV2 (s, a)
σV2h+1 (s, a)
σbV2h (s, a)
Σ h
Λbh
M1
M2
M3
M4
δ
ξ
CH,d,κ,K
E”,h [φ(s, a)φ(s, a)>]
E”,h [σVh + ι (s,a)-2φ(s, a)φ(s, a)>]
minh λmin (Σph)
minh λmin(Λh) ≥ κ∕H2 for any Vh
max{1, VarPh (V )(s, a)} for any V
max{1,VarPh(Vh+1)(s,a)}
max{1, VarhVh+1 (s, a)}
PK=I φ(sh, ah)φ(sτ, «h)> + λId
PkK=1 φ(skh,akh)φ(skh,akh)>∕σbh2(skh,akh) + λId
max{2λ, 128 log(2d∕δ), 128H 4 log(2d∕δ)∕κ2}
max{ K log((λ+K)H∕λδ), 962H 12dlog((λ + K)H"S)/K5}
max {512H4∕κ2 log (竿),4λH2∕κ}
12PH 4d log((λ + K )H∕λδ)∕κ
Failure probability
rh+V (s0)-(ThV)(s,a)
SUPV∈[0,Η], SjPh(S,a), h∈[H]	σv(s,a)
36 r Hd3 log ((λ+κλδKdHj + 12λ H2κ√d
B	Extended Literature Review
B.1	Linear model representation and its extension in online RL
There are numerous works in online RL that study linear model representations. Yang and Wang
(2019; 2020); Jin et al. (2020) propose Linear MDP, which assumes the transition kernel and the
reward are linear in given features. Cai et al. (2020); Ayoub et al. (2020); Modi et al. (2020); Zhou
et al. (2021b) propose Linear mixture MDP, which assumes the transition probability is a linear
combination of some base kernels. Linear Bellman Complete model (Zanette et al., 2020) generalizes
linear MDP model by allowing linear functions to approximate the Q-function and the function class
is closed under the Bellman update. The notion of low Bellman rank (Jiang et al., 2017) subsumes
not only linear MDPs but also models including linear quadratic regulator (LQR), Reactive POMDP
(Krishnamurthy et al., 2016) and Block MDP (Du et al., 2019). There are also other models, e.g.
factored MDP (Sun et al., 2019), Bellman Eluder dimension (Jin et al., 2021a) and Bilinear class (Du
et al., 2021). With the linear MDP model itself, there are also fruitful extensions, e.g. gap-dependent
analysis with logarithmic regret (He et al., 2021), low-switching cost RL (Gao et al., 2021), safe RL
(Amani et al., 2021), reward-free RL (Wang et al., 2020), generalized linear model (GLM) (Wang
et al., 2021b), two-player markov game (Chen et al., 2021b) and generative adversarial learning (Liu
et al., 2021). In particular, Zhou et al. (2021a) shows UCRL-VTR+ is near-minimax optimal when
feature dimension d ≥ H.
B.2	Existing results in offline RL with model representations
Offline RL with general function representations. The finite sample analysis of offline RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (MUnOs, 2003; Szepesvari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the
15
Published as a conference paper at ICLR 2022
improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefficients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their finite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 - γ)-1.
Offline RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H 3/dm2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H 2/dm) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren
et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC?/2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
offline learning bound that is not only instance-dependent but also subsumes previous optimal results.
More recently, Shi et al. (2022) uses model-free approach to achieve minimax rate with a larger
-range.
Offline RL with linear model representations. Recently, there are more focus on studying the
provable efficient offline RL under the linear model representations. Jin et al. (2021b) first shows
offline RL with linear MDP is provably efficient by the pessimistic value iteration (PEVI), which
is an offline counterpart of LSVI-UCB in Jin et al. (2020). Their analysis deviates from their lower
bound by a factor of d ∙ H (check their Theorem 4.4 and 4.6). Later, Xie et al. (2021a) considers
function approximation under the Bellman-consistent assumptions, and, when realized to linear MDP
setting, improve the sample complexity guarantee of Jin et al. (2021b) by a order O(d) (Theorem 3.2).
However, their improvement only holds for finite action space (due to the dependence log |A|) and
by the direct reduction (from Theorem 3.1) their result does not imply a computationally tractable
algorithm. In addition, there is no improvement on the horizon dependence. Concurrently, Zanette
et al. (2021) considers the Linear Bellman Complete model (which originates from its online version
Zanette et al. (2020)) and designs the actor-critic style algorithm that achieves tight result under
the assumption that the value function is bounded by 1. While their algorithm is efficient (which is
based on solving a sequence of second-order cone programs), the resulting learning bound requires
the action space to be finite due to the mirror descent/natural policy gradient updates in the Actor
procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1 simplifies
the challenges in dealing with horizon H since when rescale their result to [0, H], there is a H factor
blow-up, which makes no improvement in the horizon dependence comparing to Jin et al. (2021b).
On the other hand, Wang et al. (2021a); Zanette (2021) study the statistical hardness of offline RL
with linear representations by proofing the exponential lower bounds. As a result, none of the existing
algorithms can achieve the statistical limit for the well-structured linear MDP model with the general
(infinite or continuous) state-action spaces in the offline regime.
C Proofs in Section 3.2
Instead of proofing the result for v? - vπ, in most parts of the proof we deal with V1? - V1π, which is
more general.
C.1 Some preparations
Define the Bellman update error ζh(s, a) := (ThVh+1)(s, a) - Qh(s, a) and recall πbh(s) =
argmax∏, hQh(s, ∙), ∏h(∙ | s))/, then by the direct application of Lemma H.8
HH
V1π(s) - V1πb(s) ≤ XEπ [ζh(sh, ah) | s1 = s] - XEπb [ζh(sh, ah) | s1 = s] .	(8)
h=1	h=1
The next lemma shows it is sufficient to bound the pessimistic penalty, which is the key in the proof.
Lemma C.1. Suppose with probability 1 - δ, it holds for all h, s, a ∈ [H] × S × A that |(ThVh+1 -
ThVh+1)(s, a)| ≤ Γh(s, a), then it implies ∀s, a, h ∈ S × A × [H], 0 ≤ ζh(s, a) ≤ 2Γh (s, a).
16
Published as a conference paper at ICLR 2022
Furthermore, it holds for any policy π simultaneously, with probability 1 - δ,
H
Vιπ(S)- Vb(S) ≤ X 2 ∙ E∏ [Γh(sh, ah) | si = s].
h=1
T-I 八 八,	CTyyTC , F	∙	I ∕zT~^ ʊ	∕T^ ʊ ∖ /	∖I∕T~∖/	∖	,1	C,
Proof of Lemma C.1. We first show given |(ThVh+i - ThVh+i)(s, a)| ≤ Γh(s, a), then 0 ≤
ζh(s,a) ≤ 2Γh(s, a), ∀s, a, h ∈ S × A × [H].
Step1: we first show 0 ≤ ζh(s, a), ∀s, a, h ∈ S × A × [H].
Indeed, if	Qh(s, a)	≤	0,	then by definition	Qh(s,	a)	= 0 and in this case	ζh(s, a)	:=
(ThVh+i)(s,a) - Qh(s, a) = (ThVh+i)(s,a) ≥ 0; if Qh(s, a) > 0, then Qh(s,a) ≤ Qh(s, a)
and
ζh(s, a) :=(ThVh+i)(s, a) - Qh(s, a) ≥ (ThVh+i)(s, a) - Qh(s, a)
=(ThVbh+i)(s, a) - (TbhVbh+i)(s, a) + Γh(s, a) ≥ 0.
Step2: next we show ζh(s, a) ≤ 2Γh(s, a), ∀s, a, h ∈ S × A × [H].
Indeed, we have Qh(s, a) = max(Qh(s, a), 0) and this is because: Qh(x, a) = (ThVh+i)(x, a) -
Γh(x, a) ≤ (ThVh+i)(x, a) ≤ H - h + 1. Therefore, in this case we have:
ζh(s, a) :=(ThVh+i)(s, a) - Qh(s, a) ≤ (ThVh+i)(s, a) - Qh(s, a)
=(ThVh+ι)(s,a) - (ThVh+ι)(s,a) +Γ%(s,a) ≤ 2 ∙ Γ九(s,a).
For the last statement, denote F := {0 ≤ ζh(s, a) ≤ 2Γh(s, a), ∀s, a, h ∈ S × A × [H]}. Note
conditional on F, then by equation 8, V∏(s) - V∣π(s) ≤ PH=I 2 ∙ En [Γh(sh, ah) | si = s] holds for
any policy π almost surely. Therefore,
P ∀π,
=P ∀π,
+P ∀π,
≥P ∀π,
H
可(s) - Vb(s) ≤ X 2 ∙E∏[Γh(sh,ah) |
h=i
H
V	n(s) - Vb(s) ≤ X 2 ∙E∏[Γh(sh,ah) |
h=i
H
V	n(s) - Vb(s) ≤ X 2 ∙E∏[Γh(sh,ah) |
h=i
si = s].
si = s]∣F ∙ P[F]
si = s]卜> P[Fc]
V	n(s)	-	Vb(s) ≤ X 2 ∙	E∏[Γh(sh, ah,)	| si =	s]	∙ P[F]	≥ 1 ∙ P[F]	≥ 1 - δ,
which finishes the proof.
□
C.2 BOUNDING ∣∣(ThVh+i)(s,a) - (ThVh+i)(s,a)
ɪʌ τ	r~Λ 1	∙	.	1	1 I ∕zτ- -r^>	∖ /	∖ ∕zT^ ʊ ∖ /	∖ I O	♦	,1	i'Γ' ∙
By Lemma C.1, it remains to bound |(ThVh+i)(s, a) - (ThVh+i)(s, a)|. Suppose wh is the coefficient
>
corresponding to the ThVh+i (such wh exists by Lemma H.9), i.e. ThVh+i = φ>wh, and recall
17
Published as a conference paper at ICLR 2022
(rτ- τ7 、/ ∖ I / ∖ T ^ ,1一 - - .
(ThVh+1)(s, a) = φ(s, a)>wbh, then:
ThVbh+1 (s, a) - TbhVbh+1 (s, a) = φ(s, a)> (wh - wbh)
=φ(s, a)>wh - φ(s, a)> Λb h-1 (XX φ(sh,ah) •卜h + Vh+1 (sτh+1 /σbh2(sτh,aτh)
=φ(s, a)>wh - φ(S, a)TbY-1 (X φ (Sh, ah) ∙ (ThVh+j (Sh, ah) /σh (sh, aT))
X------------------------------------------------------------------------------------------------
}
{z
(i)
+ φ(S, a)T Λb h-1
(X φ (sh, ah) ∙ (rh + Vh+1 (sh+1) -(ThVh+1) (sh, ah)) /邕屈,ah))
_ - -
{z
(ii)
(9)
The term (i) is dealt by the following lemma.
Lemma C.2. Recall K in Assumption 2.2. Suppose K ≥ max {512H4∕κ2 log (2d), 4λH2∕κ},
then with probability 1 - δ, for all S, a, h ∈ S × A × [H]
φ(S, a)Twh - φ(S, a)TΛbh-1 X φ(Sτh,aτh)
ThVbh+1 (Sτh, aτh) ∕σb2(Sτh, aτh)
2λH3 JdlK
K
Proof. Recall ThVbh+1 = φTwh and apply Lemma H.6, we obtain with probability 1 - δ, for all
S,a,h ∈ S × A × [H],
φ(S, a)Twh- φ(S, a)TA-1 (X φ (Sh, aT) ∙ (ThVh+1) (sh, aT) ∕b2(Sh, aT)
=φ(s, a)>wh - φ(s, a)TΛ-1 (X φ (Sh, ah) ∙ φ(Sh, a/wh/b2 (Sh, ah)
=φ(S, a)Twh - φ(S, a)TΛbh-1
T -1
wh = λ ∙ Φ(s, a) Ah wh
—
≤λ kφ(S,a州Λ-1 ∙ ∣∣whkΛ-ι
hh
≤ K kφ(S,a)l(Λh)-ι ∙ ∣∣whk(Λh)-ι
≤ K1 ∙ JMh)Tl ∙ 2H √d ∙ J(Ah)Tl
where	AT :=	E*,%	Bh(s, a)-2φ(s,	a)φ(s, a)τ] and the second inequality is by Lemma	H.6 (with
φ0 =	φ∕bh	and	kΦ∕bhk	≤	∣∣Φk	≤ 1	：=	C) and the	third	inequality uses √aτ	∙ A ∙ a	≤
p	. p p	∣	p p	∕7p∖
∣a∣2 ∣A∣2	∣a∣2	= ∣a∣2	∣A∣2 with	a	to be either	φ or	wh. Moreover, λmin(Aph)	≥
κ/ maxh,s,a bh(s, a)2 ≥ κ∕H2 implies ∣∣ (A h)-11∣ ≤ H 2∕κ, therefore for all s,a,h ∈ S×A× [H ],
with probability 1 - δ
φ(S, a)Twh - φ(S, a)TAbh-1 Xφ(Sτh,aτh)
ThVbh+1 (Sτh, aτh) ∕σb2(Sτh, aτh)
2λH3 √d∕κ
K
□
18
Published as a conference paper at ICLR 2022
FOrtermgdenote： xτ = b(sh,ah),	ητ =卜T + Vh+1 (sh+ι) - (JhVh+1)(sT,aT)) /σ(sh,ah),
then by Cauchy inequality it follows
φ(s, a)>Λ-1 (X φ (sh, ah) ∙ (rh + Vh+ι (sh+J -(ThVh+j (sh, ah)) Ib(sh, ah)
≤ Jφ(S,6>λ-1φGa) ∙ ||Xχτητiiλ-i
τ=1
(10)
C.2.1
Analyzing the term
φ(S, a)Λbh-1φ(S,a)
Recall (in Theorem 3.2) the estimated Λbh = PK=ιφ (sh, ah')φ (Sh, ah)> /b2(sh, ah)+ λ ∙ I and
Ah = PK=ι φ(sh,ah)>φ(sh,ah)∕σb+ɪ(sh,aτh ) + λI. Then we have the following lemma to
control the term	φ(S, a)Abh-1φ(S, a).
Lemma C.3. Denote the quantities C1 = max{2λ, 128 log(2d∕δ), 128H 4 log(2d∕δ)∕κ2} and
C2 = max{ K iog((λ+K)H∕λδ)，962H 12d log((λ + K)H∕λδ)∕κ5}. Suppose the number of episode
K satisfies K > max{C1, C2}, then with probability 1 - δ,
Jφ(s,a)A-1φ(s,a) ≤ 2 Jφ(s,a)A-1φ(s,a),
∀S, a ∈ S × A.
Proof of Lemma C.3. By definition φ(S, a)Abh-1φ(S, a) = kφ(S, a)kΛb-1 . Then denote
where Ah = PK=1 φ(Sτh, aτh)>φ(Sτh, aτh)∕σ2b (Sτh, aτh) + λI. Under the condition of K, by
τ =	Vh+1
Lemma C.7, with probability 1 - δ
≤ sup
s,a
A0h ≤ sup
s,a
σbh2(S,a) -σ
φ(S, a)φ(S, a)>	φ(S, a)φ(S, a)>
b2(S,a)
V2bh+1(S,a)
σVh+ιGa)
b2(s,a)σb	(s,a)
Vh+1
•	kφ(S, a)k2 ≤ sup
s,a
σbh2 (S, a) - σV2b (S, a)
h+1
1
•	1	(11)
—
—
≤ % 罕 log (RKH)+12λ"d
κK	λδ	κK
Next by Lemma H.5 (with φ to be φ∕σVb and C = 1), it holds with probability 1 - δ,
—
Eμ,h [φ(s,a)φ(s,a)>∕σb十](s,a)] + KId
19
Published as a conference paper at ICLR 2022
Therefore by Weyl’s spectrum theorem and the condition K >
max{2λ, 128log(2d∕δ), 128H4 log(2d∕δ)∕κ2}, the above implies
Mhk=λmaχHQ ≤ λmaχ (E*,h [φ(S,a) 0(s,a)> 加|十](S,a)]) + K + √K (log ɪ )
=UEμ,h[φ(s, a)φ(s, α0>"Vh+ι (S, a)]||2 + K + √K (log ɪ)
≤kΦ(s,a)k2 + ⅛ + 4√∣ (V)"≤ 1 + ʌ + 4√∣
KK δ	KK
λmin(Λh) ≥λmin(E*,h[φ(s,a)φ(s,a)>/σb十ɪ(s,a)f)
≥λmin (Eμ,h[φ(s, a)φ(s,。)>/二能十ɪ (s, a)])
K	4√2	2d、1/2	K
≥ H2-√K log 句 ≥ 2H2.
+ ʌ - √2 (ι
K	√K V
4√2	2d
-√K Vog ɪ
子「2,
2d )1/2
/2
Hence With probability 1 - δ, Mhk ≤ 2 and ∣∣Λh-11| = 1∕λmin(Λhl) ≤ 2H2/κ. Similarly, one can
show ∣∣Λ0-11∣ ≤ 2H2∕κ with high probability.
、T	1 T	ɪɪ A , C/	1 A r	1	∙	1	1	1,♦	.,1	1	1 ∙1∙ . Λ UC 11
Now apply Lemma H.4 to Λ0h and Λ0h and a union bound, we obtain with probability 1 - δ, for all S, a
kΦ(s,a)kΛh-ι ≤ 1 + J∣∣Λh-1∣∣Mhk∙∣∣Λ晨 1∣∣∙∣∣Λh - Λh∣∣ ∙kΦ(s,a)kΛh-ι
≤ 1 + r 2H2 ∙1∙ 2H2 ∙∣∣Λ h- Λh∣∣1 ∙kΦ(s,a)kΛh-ι
≤
≤
1+t
48H 4
κ2
(T)
H2 √d
+ λ^K-
. kφGa)kAh-1
1+t
当 ∖ " log ( jKH
K2	KK	λδ
∙kΦ(s,a)kΛh-ι ≤ 2 kΦ(s,a)kΛh-ι
where the third inequality uses equation 11 and the last and the second last inequality use
K > max{ Kiog((λ+κ)H∕λδ)，962H 12dlog((λ + K)H∕λδ)∕κ5}. Note the above is equivalent to
φ(S,a)Λbh-1φ(S, a) ≤ 2 φ(S, a)
A-1 φ(s,a) by multiplying 1∕√K on both sides.
□
C.2.2 Analyzing the TERM ||P工 1XTητ∣∣λ-i
LemmaC.4. RecallXT = [(：?；：?) andητ = R + Vbh+1 (sh+J - (ThVh+1)(sh, ah)) ∕σ(sh, ah).
Let Chck ：= 36 JHf 3 log ((λ+K)警dH ) + 已入 H√d and denote
ξ :=	sup
V∈[0,H], s0~Ph(s,a), h∈[H]
rh + V(s0)-(ThV )(s,a)
σV (S, a)
20
Published as a conference paper at ICLR 2022
If K ≥ 4CH ,d,κ,κ and K ≥ O(H 6d∕κ) ,then With probability 1 一 δ,
K
xτητ
τ=1
≤ 16 jdlog (1 + K) ∙ log (4K^) + 4ξlog (4KK-) ≤ Omax {√d,ξ},
where O absorbs the constants and Polylog terms.
Proof of Lemma C.4. By construction, we have kxτ k ≤ kφ∕σbk ≤ 1 and by Lemma C.7, with
probability 1 一 δ∕3,
llσVbh+1 一 σbhll∞ = ssu,ap
σV2b (s, a) 一 σbh2 (s, a)
σVbh+1 (s, a) + σbh(s, a)
≤ 2 llσbh+ι-闻 L ≤ CHdKKq Kκ
Therefore, when K ≥ 4CH,d,κ,κ, CH,d,κ,Ky∕-K ≤ 1/2 ≤ σvh + 1 (Sh ah"2 aηd hence
|n" ≤
rh + Vh+1 (sh+J -(ThVh+1) (Sh, ah
σVh+ι (sh,ah - CK等卢
≤2
≤ 2 sup
V∈[0,H], S0〜Ph(s,a)
r + V (s0) — (ThV)(s,a)
σV (S, a)
rh + Vh+1 (sh+1) -(ThVh+1) (Sh，ah)
σVh+ι (ST)ah
:= ξ.
Next, for a fixed function V , we define the Bellman error as Bh(V )(S, a) = rh+V (S0) - (ThV )(S, a),
then
Var [ητ∣Fτ-ι]
Var hrh + Vh+1 (sh+1) - (ThB+1) (sh,ah)∣Fτ-i]
b2(sh,ah)
Var 回自+1(sh, ah) - BhVh十式Sh ah + Bh*1(sh,。乃归一]
b2(sh,ah)
Var [Bh*1(Sh,ah)∣Fτ-1] +8H ∣∣BhVl+1 - Bh¾1∣∞
b2(Sh,ah)
Var [Bh*1(Sh,ah)∣Fτ-1] +16H MI- *∣∣∞
b2(Sh ,ah)	三
<Var[Bh*1(Sh,ah)∣Fτ-1] + O( H√d)
≤	b2(Sh,ah)
= Var[Bh*1(Sh,ah)kh,ah]+ O( √√d)
一	b2(Sh,ah)
= VarVh+1 Gh,ah) + O(⅞√d) ≤ "忆】(Sh,ah) + O(√√d) ≤ 2 + O(⅞√d)
b2(Sh,ah)	—	b*2(Sh,ah)	—	σ?2 (Sh,ah)
≤Oe(1)
where the first inequality is by Lemma H.11, the second inequality is by Th is non-expansive,
the third inequality is by Lemma C.8, the next equality is by Markovian property, and the fourth
inequality is by Lemma C.7 and Lemma C.10. The fifth inequality uses definition σh,V (S, a)2 :=
max{1, VarPh (V)(s, a)} and the last one is by condition K ≥ O(H6d∕κ) and σh,v? (s, a)2 :=
max{1, VarPh (V ?)(S, a)} ≥ 1. Thus, by Bernstein inequality for self-normalized martingale
21
Published as a conference paper at ICLR 2022
(Lemma H.3),9 10 with probability 1 - δ,
K
xτητ
τ=1	Λb-1
+ 4ξ log
(4K2)
≤ O max {√d, ξ}
where O absorbs the constants and Polylog terms.
□
Recall M1, M2, M3, M4 in List A. Based on the above results, we have the following key lemma:
Lemma C.5. Assume K > max{Mι, M2, M3, M4} ,for any 0 < λ < κ, suppose √d > ξ,
where ξ := SUpV∈[0,H], s0〜Ph(s,a), h∈[H] "+V(：V-；T,V)(S,a) ∙ Then WithPrObability 1 - δ,forαll
h, s,a ∈ [H] × S × A,
CThVI+1 -TlVI+ι)(s,a)I ≤ O √d√φ(s,a)Λ-1 φ(s,a)
2H3 √d
+ K
where Ah = PK=I φ(sh,ah)>φ(sh,ah"σV-(sh,aτh )+λI and Oe absorbs the universal constants
τ =	Vh+1
and Polylog terms∙
Proof of Lemma C∙5∙ Combing equation 9, Lemma C.2, equation 10, Lemma C.3 and C.4 and a
union bound to finish the proof.	□
C.3 Proof of the first part of Theorem 3.2
Theorem C.6 (First part of Theorem 3.2). Let K be the number OfePiSodes∙ Suppose √d > ξ, where
ξ := SUPV∈[0,H],s0〜Ph(s,a),h∈[H] '* 。: W '，) αnd K > max{M1, M2, M3, M4}10∙
Then for any 0 < λ < κ, with probability 1 - δ, for all policy π simultaneously, the output πb of
Algorithm 1 satisfies
vπ - vπb
≤ O (√d ∙ XX En [(φ(∙, ∙)>Λ-1φ(∙, ∙))1∕2i) + 2HK√d
where Λh = PK=I °(S,2ah>°(Sh，ah)——卜 λId and O absorbs the universal constants and the Polylog
Vbh+1(sτh,aτh)
terms∙
Proof of Theorem C∙6∙ Combing Lemma C.1 and Lemma C.5, we directly have with probability
1 - δ, for all policy π simultaneously,
V∏ (S)- V∏(s) ≤ O √dd ∙ X En
h=1
h(φ(∙,∙)>Λ-1φ(∙,∙))1/2 卜ι = s]) +2HK√d,
(12)
now take the initial distribution d1 on both sides to get the stated result.
□
C.4 Two Intermediate results
The next two lemmas provide intermediate results in finishing the whole proofs.
9To be rigorous, Lemma H.3 needs to be modified since the absolute value bound and the variance bound
here are in the high probability sense. However, this will not affect the validity of the result as the weaker version
can also be obtained (see Chung and Lu (2006) and a related discussion in Yin et al. (2021) Remark E.7.) To
make the proof more readable, we do not include them here to avoid over-technicality.
10The definition of Mi is in List A.
22
Published as a conference paper at ICLR 2022
C.4.1 Bounding THE variance
Lemma C.7. Recall the definition bh(∙, ∙)2 =
max{1, VarPh Vh+1(∙, ∙)} + 1. Moreover,
[<φ(-, -)，8h〉[0,H_h+1]]2 (where Bh and θh
2
max{1, VarPh Vh+1(∙, ∙)} + 1 and。忘十](∙, ∙)2 :
I--	^	-I .
[VarhVh+1](-, •)
(φ(∙, )6Q[0,(H-h+i)2]-
are defined in Algorithm 1). Let K ≥
max {512(1∕κ)2 log (4^d) , 4λ∕κ1, then with probability 1 — δ,
/ H 4d3
SuP同-啥"∞ ≤ 川F log
(λ + K )2KdH2
λδ
+ 12λH≠
KK
Proof. Step1: we first show for all h, s, a ∈ [H] ×S × A, with probability 1 - δ
hΦ(s, a),Bhi[0,(H-h+i)2] - Ph(Vh+ι)2(s,a)∣ ≤ 12 jHK- log
(λ + K )2 KdH2
λδ
、八H2 √d
+4λk
Proof of Step1. Note
kφ(s, a), βhi[0,(H-h+1)2]-
K
=φ(s,a)>∑-1 X φ(露,a)`
τ =1
K
=Φ(s,a)>∑-1 X φ(sh,ah
τ =1
K
Ph(Vh+1)2(s,a)∖ ≤ W(s,α),βh>- Ph(Vh+1)2(s,a)
• V+1(Sh+1)2 - Ph (Vh+1)2(s, a)
- V+1(sh+1)2 - φ(s,α)> / (Vh+1)2(s0)dνh(s0)
φ(s,α)>∑-1 X。(瑞,ah) - V+1()h+1)
τ=1
K
≤ Φ(s,a)>∑-1 X φ⑸,a』)• (Vh+1 ⑸+1)2
T =1
V------------------------------------
-{z
①
K
2 - Φ(s,α)>Σ-1(X φ(嬴,aι)φ(嬴,a1
τ =1
)> + λI) / (Vh+1)2(s')dνh(s')
-Ph(伉+1)2国,&T))
+ λ φ(s,α)>Σ-1 J (Vh+1)2(s0)dνh(s0)
/ 1
■{z
②

For ②,since K ≥ max {512(1∕κ)2 log (4Hd), 4λ∕κ}, by Lemma H.6 and a union bound over
h ∈ [H], with probability 1 — δ for all h, s, a ∈ [H] ×S ×A,
② ≤λ ∣∣φ(s,a)k∑
(Vh+1)2(s, )dνh(s0)
ς-1
22
≤λ √K kφ(s,α)k(∑h)-1 √k
((B+ι)2(S)dνh(S)
「4λ E 竽
“八H2 Vd
≤4λ—τr
KK
(13)
For ①,We have
K
① ≤kφ(s,α)∣∑-i E(KsX) ∙
ι(sh+ι)2 - Ph(Vh+ι)2(*h,
T =1
Bounding using covering. Note for any fix Vh+1, We can define XT
ς-1
φ(sh, ah) (Mk2 ≤
(14)
1) and
Vh+1 (⅛h+1)2 - Ph(Vh+ι)2(sh,研J is H2-subgaussian, by Lemma H.2 (where t = K and
L = 1) with probability 1 - δ,
K
X φ(sh,ah) ∙ (Vh+1(sh+1)2 - Ph(Vh+1)2屈,ɑh))
τ =1
Σ-1
h
≤ S8H4 - dιog( λ⅛κ
η
h
let Nh(e) be the minimal e-cover (with respect the supremum norm) of Vh ：= {Vh ： Vh(∙)=
maXa∈A < min{φ(s, a)>θ - Ci Jd ∙ φ(∙, .)>Λ-1φ(∙, ∙) - C2, H - h + 1}+} > . That is, for any
23
Published as a conference paper at ICLR 2022
V ∈ Vh, there exists a value function V0 ∈ Nh() such that sups∈S |V (s) - V 0(s)| < . Now by a
union bound, we obtain with probability 1 - δ
sup
Vh+1 ∈Nh+1 ()
K
X φ(sτ,ah) ∙ (Vh+1 屈+1)2 - Ph(Vh+ι)2国,aτ))
τ=1
Σ-1
h
≤ q8H4 ∙ dlog (λ+δK ∣Nh+ι9l
which implies
K
X。国,疏)∙ (Vh+1 国+ι)2 - PhM+1)2(sh,ah))
τ=1
Σ-1
h
≤"4 ∙ d log (λ+δK Nh+ι(e)l) + 4H2 P12K2∕λ
choosing e = d√λ∕K, applying Lemma B.3 of Jin et al.(2021b)11 to the covering number N,+ι(e)
w.r.t. Vh+1, we can further bound above by
≤ 88H4 ∙ d3 log ( λ+K 2dHκJ +4H2√d2 ≤ 6 ^H4 ∙ d3 log ( λ+K- 2dHK^
Apply a union bound for h ∈ [H], we have with probability 1 - δ, for all h ∈ [H],
K
X Φ回,ah) ∙ (Vh+ι⑸+ι)2 — Ph(Vh+ι)2(sh, ah))
τ=1
≤ 6 H4d3 log
ς-1 V
and similar to ②，with probability 1 - δ for all h,s,a ∈ [H] ×S × A,
(λ + K )2 KdH 2)
λδ )
(15)
2
2
≤ -/	.
κ √Kk
(16)
Combing equation 13, equation 14, equation 15 and equation 16 we obtain with probability 1 - δ for
all h, s, a ∈ [H] × S × A,
hφ(S, a),∕βhi[0,(H-h+1)2]
I	Ih 4d31 ^^((λ + K )2KdH 2、	、h 2√d
-Ph(Vh+i)2(s，a)| ≤ 12y ɪ log 0-λ)δ)+4λk∙
Step2: we show for all h， s， a ∈ [H] × S × A, with probability 1 - δ
i	Ir C Ih2d31 ^^/(λ + K)2KdH2、 八 H√d
lhφ(s,a),θhi[0,H-h+1] -Ph(Vh+1)(S，a)l ≤ 叫 F log ((~λ- ) +4λK∙
(17)
The proof of Step2 follows nearly the identical way as Step1 except Vbh2 is replaced by Vbh .
Step3: WePrOVe SUph 同-σvj∞ ≤ 36 r HKK log( ("+K)2KdH1+12λ HK.
Proof of Step3. By equation 17,
[<φ(∙, ∙),Q[0,H-h+J - [Ph(‰ι)(s,a)]2∣
=hφ(s, a), Ghi[o,H-h+i] + Ph(Vh+ι)(s, a) ∣ ∙ ∣ hφ(s, a),船[。田”-Ph(Vh+ι)(s, a)∣
… l	l C, IH 4d3 1—((λ + K )2KdH 2、 hH 2√d
≤2H ∙ ∣ hφ(s, a)，θhi[0,H-h+1] - Ph(Vh+1)(s，a)| ≤ 24ʌ/ KK log (-)δ----- ) + " KK ∙
11Note the same result in Jin et al. (2021b) applies even though we have an extra constant C2 .
24
Published as a conference paper at ICLR 2022
Combining this with Step1 we receive ∀h, s, a ∈ [H] × S × A, with probability 1 - δ
VarhVh+1(s,a) - VarPh Vh+1 (s, a)
H4d3
≤	∖∣ KK
(λ + K)2KdH2
log (—λδ—)
+ 12λ
H2 √d
KK
Finally, by the non-expansiveness of operator max{1, ∙},we have the stated result.
□
C.4.2 A CRUDE BOUND ON SUPh∣∣Vh? - Vbh∣∣∞.
Lemma C.8. Define σbh (s, a)
'max {l, VarPh Vh+ι(s,a)} + 1, f
≥
K
max{Mι, M2, M3, M4} and K > C ∙ H4κ2, then with probability at least 1 — δ,
sup
h
≤O
∞
Proof. Step1: We show with probability at least 1 — δ, SuPh ∣∣Vh? — Vz∏∣∣∞ ≤ O (√^d).
Indeed, combing Lemma C.1 and Lemma C.5, similar to the proof of Theorem C.6, we directly have
with probability 1 — δ, for all policy π simultaneously, and for all s ∈ S, h ∈ [H]
_ . . . . ~
Vn(s) — Vhb (S) ≤ O
H
√d ∙ XEn [(Φ(∙, ∙)>Λ-1φ(∙, ∙))1/2
t=h
Sh = S
2H4 √d
+ K
(18)
i
Next, since K ≥ max {512(1∕κ)2 log (4Hd), 4λ∕κ}, by Lemma H.6 and a union bound over
h ∈ [H], with probability 1 — δ
suP kφ(S, a)kΛb-1 ≤
s,a	h
-1
2H
∀h ∈ [H].
Lastly, taking π = π ? in equation 18 to obtain
〜/ H
0 ≤ Vn? (s) — Vh(s) ≤O √d ∙ XEn? [(φ(∙, ∙)>Λ-1 φ(∙, ∙))1∕2卜h
t=h
~ H H 2√d	2H 4√d
≤o( √KK)+ ~^~.
2H 4√d
S + -ɪ
(19)
This implies by using the condition K > C ∙ H4κ2, we finish the proof of Step1.
Step2: We show with probability 1 — δ, suph ∣∣ Vh — VhbII	≤ O (√√).
Indeed, applying Extended Value Difference Lemma H.7 for π = π0 = πb, then with probability 1 — δ,
for all S, h
H
Vbh(S) —Vhnb(S) = X Enb hQh (Sh , ah ) — (Th Vh+1 ) (Sh , ah )Sh = Si
t=h
≤ X∣∣(TbhVbh+1 — ThVbh+1)(S, a)∣∣ + kΓh(S, a)k
t=h
≤O (H√d Jφ(s,a)Λ-1φ(s,a) ) + 4HK^d ≤ O
25
Published as a conference paper at ICLR 2022
where the second inequality uses Lemma C.512 and the last inequality follows the same procedure as
Step1.
Step3: Combine Step1 and Step2, by triangular inequality and a union bound we finish the proof of
the lemma.
□
Remark C.9. Note as an intermediate calculation, equation 19 ensures a learning bound with order
O( √κK). Here, the convergence rate is the standard statistical rate √K and the H2 dependence
is loose. However, thefeature dependence，d/K is roughly tight, since, in the well-explored case
(Assumption 2 of Wang et al. (2021a)), K = 1/d and the 'd∕κ = √d2 recovers the optimalfeature
dependence dH√T in the online setting (Zhou et al., 2021a). If K《 ∖∕d, then doing offline learning
requires sample size proportional to d/K, which reveals offline RL is harder when the exploration
of behavior policy is insufficient. When K = 0, learning the optimal policy accurately cannot be
guaranteed even if the sample/episode size K → ∞.
C.5 Proof of the second part of Theorem 3.2
Lemma C.10. Recall σbh
max
1, VdarPh Vbh+1
+ 1 and σh?
JmaX {1, VarPh *ι} + 1.
Let K ≥ max {512(1∕κ)2 log (4Hd) , 4λ∕κ} and K ≥ max{Mι, M2, M3, M4}, then withprob-
ability 1 - δ,
sup||b2 - b?2h ≤ O
h
Proof. By definition and the non-expansiveness of max{1, ∙} + 1,we have
Rbh+ι-σ刊 L ≤ BVarVh+1 - VarVML
≤ ∣,h (V+1 - Ci) B∞+B(phbh+1)2 - (ph%1)2B∞
≤ BBBVbh2+1 - Vh?+21BBB +BBB(PhVbh+1+PhVh?+1)(PhVbh+1-PhVh?+1)BBB
≤2H BBVbh+1 -Vh?+1BB + 2H BBPhVbh+1 -PhVh?+1BB ≤ Oe
with probability 1 - δ for all h ∈ [H], where the last inequality comes from Lemma C.8. Combining
this with Lemma C.7, we have the stated result.	□
Lemma C.11. Denote the quantities C1 = max{2λ, 128 log(2d∕δ), 128H 4 log(2d∕δ)∕K2} and
C2 = max{ K iog((λ+K)H∕λδ)，962H 12d log((λ + K )H∕λδ)∕κ5}. Suppose the number of episode
K satisfies K > max{C1, C2}, then with probability 1 - δ,
φφ(S, a/-1。(S, a) ≤ 2ʌ/φ(S, a2h-1。(S, a),
∀s, a ∈ S × A,
Proof of Lemma C.11. By definition φ(S, a)Λh-1φ(S, a) = kφ(S, a)kΛ-1 . Then denote
% = ⅛λ%, Λh0 =疝,
12To be absolutely rigorous, we cannot directly apply Lemma C.5 here since the crude bound has already
been used in Lemma C.4. However, this can be resolved completely by first deriving an even cruder bound for
SuPh||Vh? - Vh∣∣∞ that has 1 /√K rate without using Lemma C.5 (which we call it Lemma C.8*), and we can
use Lemma C.8* to show a similar result Lemma C.5*. Finally, we can use Lemma C.5* here to finish the proof
of this Lemma C.8. However, we avoid explicitly doing this to prevent over-technicality.
26
Published as a conference paper at ICLR 2022
where Λh
PK=1 Φ(sh,ah)>Φ(sh,ah)∕σVh+1(sh,ah) + λI. Under the condition of K, by
Lemma C.10, with probability 1 - δ
≤ sup
s,a
φ(s, a)φ(s, a)>	φ(s, a)φ(s, a)>
—
≤ sup
s,a
〜
≤ Oe
σVh + ι (s，a)
σh2 (s,a)
- σV2bh+1 (s, a)
σVh+ι(s,a)
• kφ(s, a)k2 ≤ sup
s,a
σh2(s,a) - σV	(s,a)
h+1
1
(20)
—
• 1
Next by Lemma H.5 (with φ to be φ∕σV ? and C = 1), it holds with probability 1 - δ,
Ah - (E*,h[0(S,a)0(S,a)>/sV* [(s,a)] + 至Id
h+1	K
√√2 (ιog2d )1/2
Therefore by Weyl’s spectrum theorem and the condition K
max{2λ, 128 log(2d∕δ), 128H4 log(2d∕δ)∕κ2}, the above implies
>
2
||Ah I =λmax(Ah) ≤ λmax (Eμ,h [φ(s,a)φ(s,a)>∕σVh+ι (s,a)]) + K + √K
≤ llEμ,h[φ(s, a)φ(s, a)>∕σVh+ι (s, a)]|| + K + √K (log ɪ)
≤ kφ(s, a)k2 + ⅛ + 4√2 (log 2d)1/2 ≤ 1 + ʌ + 4√2 (log 2d)1/2 ≤ 2,
KK δ	KK δ
λmin(Λh ) ≥λmin (E*,h[0(s, a)φ(s, a) ]σV?十I(S, a)])
+ λ - 4√2 (IogU)1/2
+ K	√κ∖g δ )
4√2	2d∖1∕2
-√κ (log 句
K	4√2	2d、1/2
≥H2 -√K Vog加)≥
Hence with probability 1 - δ,
A0h-1 ≤ 2H2∕κ with high probability.
κ
2H2.
2 and ^^0-11∣ = 1∕λmin(Ah0) ≤ 2H2∕κ. Similarly,
≤
Now apply Lemma H.4 to A，and Ah and a union bound, We obtain with probability 1 - δ, for all
s, a
kΦ(s,a)kΛh-ι ≤ [1 + JMh0τ∣∣∣∣Ah0∣H∣A"1∣∣∙∣∣Ah0 - Ah∣∣卜 kφ(s,a)∣∣Λ?一
∕2H2	2H2~"~'	7
≤ 1 + ʌ/•1 • ~κ~ • ∣∣Ah - AhII - kφ(s,a)kΛh0τ
≤ 1 + t ^K∑ O I √ΓKr 1	• kφ(s, a)kΛ?0-1 ≤ 2 kφ(s, a)kΛ?0-1
where the third inequality uses equation 20 and the last inequality uses K >
max{ K iοg((λ+K)H∕λδ)，962H 12dlog((λ + K)H∕λδ)∕κ5}. The claimed result follows straightfor-
wardly by multiplying 1 ∕√K on both sides of the above.
□
27
Published as a conference paper at ICLR 2022
Proof of Theorem 3.2. The first part of the theorem has been shown in Theorem C.6. For the second
part, apply Theorem C.6 with π = π?, then with probability 1 - δ,
vπ? - vπ ≤ O (√d ∙ XE∏? [(φ(∙, ∙)>Λ-1φ(∙, ∙))1q) + 2HκK√d,
Now apply Lemma C.11 and a union bound, with probability 1 - δ,
0 ≤ v*-vπ ≤ Oe(√d ∙ XE∏* h(φ(∙,∙)>ΛhTφ(∙,∙))“2i! +2Hκ4√d.
□
D Proof of Theorem 3.3
First of all, we show the following lemma.
Lemma D.1. Suppose K > max{M1, M2, M3, M4}. Plug
rh(s,a) - φ(s,a)> λ-IE
τ=1
φ (sh,ah) ∙ rrh + Vh+1 (sτh+1 ) -
端 (sh,ah)
TbhVbh+1
+O( Hdκ)
K
^
K
in Algorithm 1 and let Th be the Bellman operator and Tbh be the approximated Bellman operator.
Then we have with probability 1 - δ :
|(ThVbh+1 - TbhVbh+1 )(s, a)| ≤ Γh(s, a), ∀s, a ∈ S × A.
Proof of Lemma D.1. Suppose wh is the coefficient corresponding to the ThVh+1 (such wh exists by
Lemma H.9), i.e. ThVh+1 = φ>wh, and recall (ThVh+1)(s, a) = φ(s, a)>wbh, then:
ThVbh+1	(s,	a)	-	TbhVbh+1	(s,	a)	=	φ(s, a)> (wh	- wbh)
=φ(s, a)>wh - φ(s, a)>Λb h-1
= φ(s, a)>wh - φ(s, a)>Λb h-1
τXK=1
τXK=1
φ (sh, ah) ∙ (rh + Vh+1 (sh+l)) /σh(sh, ah)
φ (sh,ah) ∙ (Th⅝+i) (sh,ah)∕b2(sh,ah)
}
{z
(i)
(iii)
(21)
|
For term (i), by Lemma C.2 it is bounded by 2λHK√d∕κ with probability 1 - δ∕2.13
For term (ii), it is bounded by
φ(s, a)>
ʌ-1 X φ (sh, ah) ∙ (rh + Vbh+1 (sh+ι)-
Ah T=	σh(sh,ah)
TbhVbh+1
13Note Here Lemma C.2 still applies even if the Γh changes since it works for all Vbh ∈ [0, H] so that
∣∣wh ∣∣2 ≤ 2Hy∕d and the truncation (Line 13 in Algorithm 1) guarantees this.
28
Published as a conference paper at ICLR 2022
For term (iii), by Cauchy inequality
φ(s, a)>Λ-1 (X φ (sh, ah) ∙ ((Tbh+ι) (sh, ah) - (Th‰ι) (sh, ah)) /娠⑸,aj
≤kΦ(s,a)kΛ-ι ∙∣XX Φ (sh,ah) ∙ ((ThVh+1) (Sh,ah) — (ThVh+)屈,。9)端回应)
Λb-
h
1
2H
κK
2H
≤^=
K
X φ (sh,ah) ∙ ((ThVh+1) (sh,ah) -(τhVh+1) (sh,ah))解C
τ=1
Λb
-1
h
• O( HfK) ∙√d=O( HKκ)
where the first inequality is by Lemma H.6 (with φ0 = φ∕σh, and ∣∣φ∕bh,∣∣ ≤ ∣∣φk ≤ 1:= C) and the
third inequality uses √a> • A • a ≤ ,1ak2 ∣A∣2 |同卜=∣∣a∣2，10卜 with a to be either φ or wh.
Moreover, λmin(Λ h) ≥ κ/ maxh,s,。bh(s,a)2 ≥ κ∕H2 implies ∣(Λ h)-1∣ ≤ H 2∕κ.
The second inequality is true by denoting XT = φ(sh, ah)/b(sh, ah) and
ητ= ((TbhVbh+1) (sτh,aτh)- (ThVbh+1) (sτh,aτh))∕σbh(sτh,aτh)
and use Lemma H.10 as the condition for applying Lemma H.2. By collecting those three terms
together we have the result.	□
D.1 Proof of Theorem 3.3
Proof. Use Lemma D.1 as the condition for Lemma C.1 and average over initial distribution d1, we
obtain with probability 1 - δ,
vπ - vπb ≤
H
XEπh [φ(s, a)]>
h=1
K
Λbh-1X
τ=1
φ (Sh, ah) • rhh + Vh+1 (sh+1) — (TVh+1) (sh, ah))
b2 (sh,ah)
十O(陪)
K
Denote Ah := PτK=1
Φ(sh,ah)∙(rT + Vh+1(sh+1)-(ThVh + 1)(sh,ah))
σh(Sh,ah)
, then
(22)
Eπh [φ(S, a)]>
K
Λbh-1X
τ=1
φ (sh, ah) . (rh + Vh+1 (sh+1) - (ThVh+1) (sT, ah))
b2 (sh,ah)
≤Eπh [φ]> • Λbh-1Ah+Eπh[φ]>
(
ThVh+1 (sh,ah) —Thyh+1 (sh,ah))
Ah T=1	b2(SKah
For the second term, it can be bounded similar to term (iii) in Lemma D.1 and for the first term we
have the following:
Enh [φ]> ∙∣Λ-1Ah∣= Enh [φ]> ∙ Λ-1 • Λh ∣Λ-1Ah∣ ≤ kE∏h[φ]∣Λ-ι . ∣∣Λh∣Λ-1Ah∣∣∣
h
≤kEnfc [φ]∣Λ-ι ∙k∣Ah∣∣λ-i ≤ O(√d) ∣E∏h [Φ]∣Λ-ι ≤ O(√d ∣E∏fe [Φ]∣λ-i ),
hh	h	h
where the first inequality uses Cauchy’s inequality, the second inequality uses Λbh is coordinate-wise
positive (since we assume here φ ≥ 0), the third inequality is identical to the analysis in Section C.2.2
and the fourth inequality is identical to the analysis in Section C.2.1 with φ replaced by E[φ]. Plug
this back to equation 22 we finish the proof for the first part. For the second part, converting Λh-1 to
Ah-1 is identical to Section C.5. This finishes the proof.	□
29
Published as a conference paper at ICLR 2022
E Proof of Minimax Lower bound Theorem 3.5
The proof follows the lower bound proof of Zanette et al. (2021). For completeness, we provide all
the details in below.
E.1 Construction
Similar to the proof of [Zanette et al. (2021), Theorem 2], we construct a family of MDPs, each
parameterized by a Boolean vector u = (u1, . . . , uH) with each uh ∈ {-1, +1}d-2 for h ∈ [H].
The MDPs share the same transition kernel and are only different in the reward observations.
State space: At each time step h, there are two states S = {+1, -1}.
Action space: The action space A = {-1, 0, +1}d-2.
Feature map: The feature map φ : S × A 7→ Rd is given by
(√2d ʌ	d	(√2d ʌ	d
φ(+1, a) = [ √2 J ∈ R , φ(-1, a) = [ 0 J ∈ R .
The construction ensures the condition kφ(s, a)k2 ≤ 1 for any (s, a) ∈ S × A.
Transition kernel: The transition probability Ph (s0 | s, a) is independent of action a. In other
words, the Markov decision process reduces to a homogeneous Markov chain with transition
matrix
∈ R2 .
By letting
1-21-2
1-21-2
=
P
∕0d-2∖
νh( + 1) = Vh(II)= I √2 J ∈ Rd,
we have Ph(s0 | s, a) = hφ(s, a), νh(s0)i to be a valid probability transition.
Reward observations: For any MDP Mu, at each times step h, the reward follows a Gaussian
distribution with
Ru,h (s, a)
〜
ha, uhi, 1
where δ ∈ [0, √= ] determines to What extent the MDP models are different from each
other. The mean reward function satisfies ru,h(s, a) = hφ(s, a), θu,hi with
(δuh ∖
√3 J ∈ Rd.
Offline data collection Scheme: The dataset D = {(sτh, aτh, rhτ , sτh+1 )}τh∈∈[[HK]] consist of K i.i.d.
trajectories. All the trajectories initiate from uniform distribution. We take a behavior policy
μ(∙ | s) that is independent of state s. Let {e1,e2,..., ed-2} be the canonical bases of
Rd-2 and 0d-2 ∈ Rd-2 be the zero vector. The behavior policy μ is set as
μ(ej | S) = d forany j ∈ [d - 2] and	μ(0d-2 | S) = d.
E.2 Overview of proof
The proof of the theorem is based on Assouad’s method, where we first reduce the problem to binary
hypothesis tests (Lemmas E.1 and E.2) and then connect the testing error to the uncertainty quantity
in the upper bound (Lemma E.3).
30
Published as a conference paper at ICLR 2022
Lemma E.1 (Reduction to testing). There exists a universal constant c1 > 0 such thata
inf max Eu [叱-V∏] ≥ c1 δ√dH	min inf [Pu(ψ = u) + Puo (ψ = u0)],	(23)
b u∈U	u,u0∈U:DH (u0;u) = 1 ψ
where πb denotes the output of any algorithm that maps from observations to an estimated policy. ψ is
any test function for parameter u and DH is the hamming distance.
Lemma E.2. There exists a universal constant c2 > 0 such that when taking δ := √d, we have
min inf [Pu(ψ = U) + Puo (ψ = u0)] ≥ 1.	(24)
u,u0∈U:DH (u0;u) = 1 ψ	2
When K & d3, δ := c2= ensures that δ ≤ 1/√3d. Combining Lemmas E.1 and E.2 yields a lower
bound
inf maχ Eu N - Vb ] ≥ C d^,	(25)
bπ u∈U	K
where c > 0 is a universal constant. We then use the following Lemma E.3 to connect the above
lower bound to the uncertainty term √d ∙ PhH=I pE∏? [φ]>(Λh)-1E∏? [φ] for the chosen linear MDP
instances class M.
Lemma E.3. There exists a universal constant c3 > 0 such that for all M ∈ M,
dH
^X JE∏? [。]丁。[)-Eπ? [φ] ≤ c3 √K.	(26)
Plugging inequality equation E.3 into the bound equation 25, we obtain the minimax lower bound equa-
tion 6 in the statement of theorem.
E.3 Reduction to testing via Assouad’ s method
Proof of Lemma E.1. For any index vector u = (u1, . . . , uH ) ∈ U = {-1, +1}(d-2)×H, the optimal
policy for MDP instance Mu is simply
∏?(∙) = Uh for h ∈ [H].
Similar to the proof of Lemma 9 in Zanette et al. (2021), we can show that the value suboptimality of
policy π on MDP Mu is given by
Vu?- Vn = √=ξ X IlUh- En [ah] ∣∣ι.
2d h=1
Define Un = (u∏,...,uH) with Un := sign (En [ah]), then the '1 -norm is lower bounded as
∣Uh - Eπ [ah ]∣1 ≥ DH (Uh ; Uh ),
where DH(∙; ∙) denotes the Hamming distance. It follows that
Vu?- Vn ≥ √=dDH(Un; U).	(27)
We then apply Assouad’s method (Lemma 2.12 in Sampson and Guttorp (1992)) and obtain that
inf max Eu Dh(U; u)] ≥ ——min inf [Pu(ψ = u) + Puo (ψ = u0)] , (28)
u∈U u∈U	2	u,u0∈U:DH(u0；u) = 1 ψ
where ψ is any test functions mapping from observations to {U, U0}. Combining inequalities equa-
tion 27 and equation 28, we finish the proof.	□
31
Published as a conference paper at ICLR 2022
E.4 Lower bound on the testing error
Proof of Lemma E.2. The proof of Lemma E.2 is similar to that of Lemma 10 in Zanette et al. (2021).
We first apply Theorem 2.12 in Sampson and Guttorp (1992) to lower bound the testing error using
KUllback-Leibler divergence and obtain
1/2
,,min , ʌ 1inf [Pu(ψ = U) + Pu0 (ψ = UO)] ≥ 1-(2	, “max ,，DKL(QukQuO))
u,u0∈U: DH (u0 ;u) = 1 ψ	u,u0∈U:DH (u0 ;u) = 1
(29)
It only remains to estimate DKL(QukQu0).
The probability density Qu takes the form
KH
Qu(D)= Y ξι(sk) Y μ(ah I Sh) [Ru,h(sh, ah)] (rh) Ph∕+1 I Sh ah)
k=1	h=1
where ξι = [2, 2] is the initial distribution. It follows that
DKL(QukQu0) =Eu log(Qu/Qu0)
H
K ∙ X Eu [log ([Ru,h(Sh, ah)] Sh)∕[Ru0,h(Sh, ah)] (rh))]
h=1
d-2
K X Dkl
j=1
ej,U0hi, 1) .
If we take δ = √2=, then inequality equation 29 ensures inequality equation 24, as claimed in the
statement of the lemma.
□
E.5 Connection to the uncertainty term
Proof of Lemma E.3. We first calculate the explicit form of the inverse of variance-rescaled covari-
ance matrix Λh,p. For each time step h ∈ [H], the value function 号h+ι takes the form
Vu,h+1 = E∏? ru,h+1 + (Pπ+ivu,h+2).
Since (Ph+1Vu,h+2)( + 1) = (Ph+1 Vu,h+2)(-1) and ru,h+1( + 1,a) -ru,h+l(T,a) = 2/√6, we
have
VarPh (Vu,h+1)(+1,a) =VarPh (E∏?ru,h+1)(+1,a) = 6
Similarly,
VarPh (Vu,h+1)(-1, a) = VarPh (VU,h+1)(+1, a) = 6.
By routine calculation, we find that the population-level rescaled covariance matrix takes the form
λ?,P — 3K	d2 id-	d√d 1(d-2)×2	Rd×d
Ah=可 ld√d l2×(d-2)	12	∈R R
for any h ∈ [H]. Applying Gaussian elimination on Λh,p, we have
_	2	(d2^ {Id-2 + d-21(d-2)×(d-2)}
(Ah,p)	=3K (	-2⅛l2×(d-2)
d√d -
-2(d-2) 1(d-2)×2	∖
1 (d - 1	1 ʌ .
d-2	1 d - 1〃
32
Published as a conference paper at ICLR 2022
For each time step h ∈ [H], we have (by Jensen’s inequality)
qE∏?[φ]>(Ah)-1E∏?[φ] ≤ 2∣∣φ(+1,uh)∣∣(Λ*,p)-1 + 2∣∣φ(-1,uh)∣∣(Λ*,p)-1
Recall that by our construction,
(Uh ∖
√2d
√2 I ∈ Rd,
φ(-1, uh)
∈ Rd .
It follows that
∣∣φ(+1,uh)∣∣(Λ*,p)-ι = ∣∣φ(-1,uh)∣∣(Λ*,P)-ι
=3κ{d u>{L-2 + d-2 I(J)W) }uh - 2(dd-i) 1>-2uh + 2dr⅛}
=3K {d4 + 4(d^-2) (I- 1>-2uh)2 + 4}
≤ ɪ{d2 + dd- 1)2 + ι} - {日 + 上ɪ}. 42/衣
≤ 3K[ 4 + 4(d - 2) + 4∫	3K[ 2 + 2(d - 2) ʃ .	∣K
Therefore,
[E∏* [φ]>(Λh)-1E∏*[φ] . d∕√K.
Taking the summation over h ∈ [H], we obtain the bound equation 26 as claimed in the lemma
statement.
□
F A Numerical S imulation
F.1 A Linear MDP construction
We consider a synthetic linear MDP example that is similar to Min et al. (2021) but with some
modifications for the offline learning task. The MDP instance we use consists of |S | = 2 states and
|A| = 100 actions, and feature dimension d = 100. We set S = {0, 1} and A = {0, 1, . . . , 99}
respectively. For each action a ∈ {0, 1, . . . , 99}, we use binary encoding to obtain a vec-
tor a ∈ R8 using its binary representation (i.e. each coordinate is either 0 or 1). we inter-
changebly use a and and its vector representation a for the ease of explanation. We first define
δ(s, a) = 10 oth1e{rsw=ise0} = 1{a = 0} , then the non-stationary linear MDP is specified by the
following configuration
•	Feature mapping:
φ(s, a) = (a>, δ(s, a), 1 — δ(s, a))ɪ ∈ R10
•	The true measure νh
Vh(S) = (0,..., 0, (1 — S)㊉ αh, S ㊉ α九),
where {。h}八日句 is a sequence of integers taking values 0 or 1 and ㊉ is the standard XOR
operator. We define
θh ≡ (0, . . . , 0, r, 1 - r) ∈ R10
with the choice of r = 0.9. The transition follows Ph(S0|S, a) = hφ(S, a), νh(S0)i and the
mean reward function rh (S, a) = hφ(S, a), θhi.
• Behavior policy: always choose action a = 0 with probability p, and other actions uniformly
with probability (1 - p)/99. The initial distribution chooses S = 0 and S = 1 with equal
probability 1/2. We use p = 0.6.
33
Published as a conference paper at ICLR 2022
Fixed horizon H = 20
。 1 23
O - - -
Iooo
111
de6 A-euJ-doqns
0	200	400	600	800	1000
Episode K
Fixed horizon H =30
de6 A--euJ--doqns
Fixed horizon H = 50
Iol
O O -
Ilo
de6 A-euJ-doqns
Episode K
(a) Suboptimality vs. Episode K (b) Suboptimality vs. Episode K (c) Suboptimality vs. Episode K
(Horizon H = 20)	(Horizon H = 30)	(Horizon H = 50)
Figure 1: Comparison between PEVI and VAPVI in the non-stationary linear MDP instance described
above. In each figure, y-axis denotes suboptimality gap v? - vπb , x-axis denotes number of episodes
K. The problem horizons are fixed to be H = 20, 30, 50. The solid line denotes the average
suboptimality gap over 50 trials and the error bar area is the corresponding standard deviation. The
range of K is from 5 to 1000.
F.2 Empirical comparison between PEVI and VAPVI on the constructed linear
MDP
We compare Pessimistic Value Iteration (PEVI) in Jin et al. (2021b) and our VAPVI Algorithm 1 in
Figure 1, with horizon to be H = 20, 30, 50. In addition, we add the non-pessimistic version for both
algorithms, i.e. least-square value iteration (LSVI) and variance-aware value iteration (VAVI). The
true optimal value v? is computed via value iteration using the underlying transition kernels. For
the empirical validation of VAPVI, we do not split the data and, in particular, in all the methods we
choose λ = 0.01 (instead of λ = 1 used in theory (Jin et al., 2021b) which causes over-regularization
in the simulation).
We can observe VAPVI outperforms PEVI and the gap becomes larger when horizon H increases.
One main reason for this to happen is due to the bonus used in PEVI (Jin et al., 2021b)
O dH ∙ (φ(∙, ∙)>∑-1φ(∙, ∙))1/2
is overly pessimistic comparing to our
O [√d ∙ (φ(∙, ∙)>Λ-1φ(∙, ∙))1/2
when H becomes larger and this could potentially make the learning less accurate. In addition, both
non-pessimistic algorithms exhibit similar accuracy, and this is partially owing to our truncation
scheme σh,(∙, ∙)2 J max{ 1, VarPh Vh+ι(∙, ∙)} so σh(∙, ∙)2 Will just be 1 when the estimated variance
is small. Lastly, variance-aware pessimism eventually outperforms non-pessimism algorithms when
sample size is large and this might come from that the pessimistic bonus is estimated more accurately
when more samples are collected.
G	Some missing derivations and discussions
G.1 Regarding coverage assumption
Now we discuss the feature coverage assumption. Indeed, even if Assumption 2.2 is not satisfied,
we can still learn in the effective subspan of ∑h := Eμ,h [φ(s, a)φ(s, a)>]. Concretely, since ∑h is
symmetric, by orthogonal decomposition we have Σph = ZhΛZh>, where Zh (can be estimated using
the samples for practical purpose) consists of orthogonal basis and Λ consists of eigenvalues of Σph in
the diagonal. Suppose we do not have a full coverage, i.e.
Λ = diag[λ1, λ2, ..., λd0, 0, ..., 0] with d0 < d,
then we can create transformed features Φh(s, a) = Zh ∙ φh(s, a), and then
Eμ,h [φh(s,a)φh(s,a)>] =A = diag[λ1,λ2, ...,λdo, 0,..., 0].
Then we can do learning w.r.t. the truncated features φ0h |1:d0’s instead of the original φ. It reduces to
the weaker notion of κ := minh∈[H]{κh : s.t. κh = smallest positive eigenvalue at time h}.
34
Published as a conference paper at ICLR 2022
G.2 Derivation of equation 5
When reducing Theorem 3.2,3.3 to the tabular case, set φ(s, a) = 1s,a, d = SA, λ = 0, and recall
by Assumption 3.4 (let’s assume π? is a deterministic policy as it always exists in tabular MDP)
C? := suph,s,a
H
d∏? (s, α)∕dμ(s, a), then for Theorem 3.2
√d ∙ X E∏? √Φ(∙, ∙)>Λh-1Φ(∙, ∙)
H	__________
√d ∙ XX d∏*(s,a) Jl>aΛhTls,a
h=1 s,a
h=1
=√SA ∙ X X dh (s, a) jl>adiag{ VarPn；(Vh+1) 卜s,a
h=1 s,a	, ,
=√SA S X dh* (s,a)JVarPs；(Vh+1)
nh,s,a
h=1 s,a	,,
.√SA ∙ X X d∏* (s,a) JVKPsdμ(V⅞) ≤ PSAC*/K ∙ X X Jd∏*(s,a)VarPs,a (%)
h=1 s,a	h ,	h=1 s,a
H	___________________________
nh,s,a :=
K
X 1[sτh, aτh
τ=1
= s, a]
H
=P SAC*/K ∙ XX Jdh* (S,π* (S))VarPs,∏*(s) (Vh+1)
h=1 s
H I-------------------------------------
≤ YSAC/K E	S ∙ £dh*(s,π?(S))VarPs,∏*(s) (‰)
h=1	s
H
≤ √S2AC?/K ∙ t H	dh (s,∏?(S))VarPs n?(s)(Vh+1)
h=1	s
H
PS2AC?/K ∙ t H ∙ EEnh [Varp(∙,^)(k/ ≤ PH3S2AC?/K
h=1
where the first inequality is by Chernoff bound and the last one is by Lemma 3.4. of Yin and Wang
(2020) (Law of total variances). The rest of them are from Cauchy’s inequality. Similarly, for
Theorem 3.3, we also have
√d ∙ X qE∏* [φ]>Λh-1E∏*[φ] = √d ∙ X qVec{dπ* }Λh-1Vec{dπ* }
h=1
√d ∙ ^X JVeC{dπ* }diag
h=1
Hv
h=1
√SA ∙∑、∑dh* (s,a)
h=1
H
s,a
[「Hd∏*}
2 VarPs,a (Vh+l)
nh,s,a
.√SA ∙ ∑λ ∑d∏* (s,a)
h=1
2 VarPs,a (Vh+l)
K ∙ dh(s,a)
s,a
H /
≤ √SAC*/K ∙ E	E d∏* (s, a)VarPs,α (小)
h=1	s,a
=PSACrK ∙ X SX dh*(s,∏*(s))VarPs,∏*(s) (Vh+ι)
H
≤ PSAC*/K ∙ t H ∙ ^E∏? [VarP(∙,^)(Vh+方 ≤ PH3 3SAC?/K.
h=1
35
Published as a conference paper at ICLR 2022
H Auxiliary Lemmas
Lemma H.1 (Matrix McDiarmid inequality / Matrix Chernoff bound (Tropp, 2012)). Let zk, k =
1, . . . , K be independent random vectors in Rd, and let H be a mapping that maps K vectors to a
d × d symmetric matrix. Assume there exists a sequence of fixed symmetric matrices {Ak}k∈[K] such
that for zk, zk0 ranges over all possible values for each k ∈ [K], it holds
(H(z1, . . . , zk, . . . , zK) - H(z1 , . . . , zk0 , . . . , zK))* 2	A2k .
Define σ2 := llPk A2k ll. Then for any t > 0,
P {∣∣H(zι,...,zκ) - EH(zι,...,zκ )k ≥ t} ≤ d ∙ exp
-t2
8σ2
Lemma H.2 (Hoeffding inequality for self-normalized martingales (Abbasi-Yadkori et al., 2011)).
Let {ηt}t∞=1 be a real-valued stochastic process. Let {Ft}t∞=0 be a filtration, such that ηt is Ft-
measurable. Assume ηt also satisfies ηt given Ft-1 is zero-mean and R-subgaussian, i.e.
∀λ ∈ R, E [eληt | Ft-1] ≤ eλ2R2/2
Let {xt}t∞=1 be an Rd-valued stochastic process where xt is Ft-1 measurable and kxtk ≤ L. Let
At = λId + Pts=1 xsxs>. Then for any δ > 0, with probability 1 - δ, for all t > 0,
t
xsηs
s=1
2
d λ+tL
λ-1 ≤ 8R2 ∙ 2 log (k
Lemma H.3 (Bernstein inequality for self-normalized martingales (Zhou et al., 2021a)). Let {ηt}t∞=1
be a real-valued stochastic process. Let {Ft}t∞=0 be a filtration, such that ηt is Ft-measurable.
Assume ηt also satisfies
∣ηt∣ ≤ R, E [ηt | Ft-1] =0, E [η2 | Ft-1] ≤ σ2.
Let {xt}t∞=1 be an Rd-valued stochastic process where xt is Ft-1 measurable and kxtk ≤ L. Let
At =λId + Pts=1 xsxs>. Then for any δ > 0, with probability 1 - δ, for all t > 0,
t
xsηs
s=1
≤ 8σ{d log (1 + tL) ∙ log (4δ~) + 4R log
Λt-1
4t2
δ
Lemma H.4 (Converting the variance under the matrix norm). Let A1 and A2 ∈ Rd×d are two
positive semi-definite matrices. Then:
M-1∣∣ ≤∣∣Λ-1∣∣ W1∣∣∙kΛι- Λ2k
and
kφkΛ1-1
for all φ ∈ Rd.
≤ 1 + JllA-IIlkλ2∣I ∙ IlA-IIHlAI-λ2∣∣ ∙ kφk
Λ-1 .
Λ2
Proof. For the first part, note
llA1-1ll	≤	llA2-1ll	+	llA1-1	- A2-1ll	≤	llA2-1ll	+llA2-1ll	kA1 -	A2k	llA1-1ll
For the second one,
kφkΛ1-1
φ> A2
=∕φ>Λ-1φ = Jφ> (A-1 - A-1) φ + φ>Λ-1φ
∖1/ W/A-Λ1/ - I + I) A-1/20 ≤ JkΦkΛ-ι ∙ (1 + ||a2/2A-1A1/2 - I∣∣) kΦkΛ-ι
2	1	2	2	Λ2	2	1	2	Λ2
≤ (1 + W2Λ-1Λ2"-I∣∣1/2) ∙kΦkA-ι = (1 + W2Λ-1 (Λ2 - Λι)Λ-1Λ2-1/2) ∙ |同鼠-1
≤ (1 + qkA2k ||A-1|| 1∣λ-1∣1 kA1 - A2k) ∙ kφkΛ-1
□
36
Published as a conference paper at ICLR 2022
Lemma H.5 (Lemma H.4 of Min et al. (2021)). let φ : S × A → Rd satisfies kφ(s, a)k ≤ C for
all s, a ∈ S × A. For any K > 0,λ > 0, define GK = PK=I φ(sk,ak)φ(sk, ak)> + λId where
(sk, ak)’s are i.i.d samples from some distribution ν. Then with probability 1 - δ,
GGK - Eν]G⅛ WK
Proof of Lemma H.5. For completeness, we provide the proof of Lemma H.5. Let xk = φ(sk, ak).
Denote Σe h as the matrix obtained by replacing the k-th vector xk in Σb h by xek and leaving the rest
K - 1 vectors unchanged. Then
〜」k ) W K2 (2xkχ>χkx> + 2xkχ>χkχ>) W K2 Id := Ak.
Notice that ∣∣PK Akl = 4K4, by Lemma H.1 We have the result.
□
Lemma H.6 (Lemma H.5. of Min et al. (2021)). Let φ : S × A → Rd be a bounded function s.t.
kΦk2 ≤ C∙ Define GK = PK=I φ(sk, ak)φ(sk, ak)> + λId where (sk,ak)'s are i.i.d samplesfrom
some distribution ν. Let G = Eν [φ(s, a)φ(s, a)>]. Then for any δ ∈ (0, 1), if K satisfies
K ≥ max {512c4 ∣∣G-1∣∣2 log (2d) , 4λ ∣∣G-1∣∣
Then with probability at least 1 - δ, it holds simultaneously for all u ∈ Rd that
2
IluIlGKI ≤ √k l∣ukG-ι ∙
Lemma H.7 (Extended Value Difference (Section B.1 in Cai et al. (2020))). Let π = {πh}hH=1 and
π0 = {πh0 }hH=1 be two arbitrary policies and let {Qh }hH=1 be any given Q-functions. Then define
Vh(S) := hQh(s, ∙),∏h(∙ | s)i forall S ∈ S. Thenforall S ∈ S,
H
VL(S)- Vn(S) = X E∏0 [hQh (sh, ∙) ,πh (∙ । Sh ) - πh (∙ 1 sh)i 1 S1 = s
h=1
H
+ X Eπ0 Qh (sh, ah) - ThVh+1 (sh , ah ) | s1 = s
h=1
where (ThV)(∙, •):= rh(∙, ∙) + (PhV)(∙, ∙) forany V ∈ RS.
(30)
Proof. Denote ξh = Qh - ThVh+1. For any h ∈ [H], We have
00
Vbh - Vhπ = hQbh,πhi - hQπh ,πh0i
0
=	hQbh, πh - πh0 i + hQbh - Qπh , πh0 i
=	hQbh, πh - πh0 i + hPh(Vbh+1 - Vhπ+1) + ξh, πh0 i
=	hQbh, πh - πh0 i + hPh(Vbh+1 - Vhπ+1), πh0 i + hξh, πh0 i
0
recursively apply the above for Vh+1 - Vhπ+1 and use the Eπ0 notation (instead of the inner product
of Ph,∏h) we can finish the prove of this lemma.	□
Lemma H.8. Let πb = {πbh}hH=1 and Qh(∙, ∙) be the arbitrary policy and Q-function and also
Vh(S) = hQh(s, ∙),∏h(∙∣s)i ∀s ∈ S. and Zh(s, a) := (ThVh+ι)(s,a) — Qh(S,a) (element-wisely)
37
Published as a conference paper at ICLR 2022
to be the Bellman update error. Then for any arbitrary π, we have
HH
V1π(s) - V1πb(s) = XEπ [ζh(sh, ah) | s1 = s] - XEπb [ζh(sh, ah) | s1 = s]
h=1	h=1
H
+ X E∏ [hQbh (sh, ∙) ,∏h (∙∣Sh) - ∏h (∙∣Sh)i | Sl = x]
h=1
where the expectation are taken over sh, ah.
Proof. Note the gap can be rewritten as
V1π(s)-V1πb(s)=V1π(s)-Vb1(s)+Vb1(s)-V1πb(s).
By Lemma H.7 with π = πb, π0 = π, we directly have
HH
Vπ(S)-VI(S) = XE∏ [Zh(sh,ah) | Si = s]+XEn [hQbh (sh, ∙) ,∏h (∙∣Sh)-ah (∙∣Sh)i | si = s]
h=1	h=1
(31)
Next apply Lemma H.7 again with π = π0 = πb, we directly have
H
Vbi(S) - Viπ (S) = - X Eπb [ζh(Sh, ah) | Si = S] .	(32)
h=i
Combine the above two results we prove the stated result.
□
Lemma H.9. For a linear MDP, for any 0 ≤ V(∙) ≤ H, then there exists a Wh ∈ Rd s.t. ThV =
hφ, Whi and IlWhIl2 ≤ 2H√d for all h ∈ [H]. Here Th(V)(s,a) = rh(x,a) + (PhV)(s,a).
Similarly, for any π, there exists whπ ∈ Rd, such that Qπh = hφ, whπi with kwπk2 ≤ 2(H - h +1)√d.
Proof. By definition,
ThV=rh+(PhV) = hφ, θhi + hφ,	V(S)dνh(S)i
⇒ Wh = θh +	V(S)dνh(S),
therefore ∣∣Wh∣∣2 ≤ ∣∣θh∣∣2 + H ∙ IlVh(S)k ≤ 1 + H√d ≤ 2H√d. The proof of the second part is
similar by backward induction and the fact Vhπ ≤ H - h + 1 for any π.
□
Lemma H.10. For any pessimistic bonus design Γh, suppose K > max{Mi, M2, M3, M4}, then
with probability 1 - δ, Algorithm 1 yields
ThVbh+i - TbhVbh+i	≤ Oe(
∞
H 2 p d∕κ
Kk
)
38
Published as a conference paper at ICLR 2022
C	"SC	∙	,1	M	,	1.	,	,1 zʃ- √>	.,
Proof of Lemma H.10. Suppose wh is the coefficient corresponding to the ThVh+1 (such wh exists
1 ɪ	ɪ T ∕Λ∖	ZT- -r^r	ι ^T	ι	ιι / rτ- C ∖ /	∖	∕/	∖ T ^ , ι
by Lemma H.9), i.e. ThVh+1 = φ>wh, and recall (ThVh+1)(s, a) = φ(s, a)>wbh, then:
ThVbh+1 (s, a) - TbhVbh+1 (s, a) = φ(s, a)> (wh - wbh)
=φ(s, a)>wh - φ(s, a)> Λb h-1 (XX φ(sh,ah) ∙卜h + Vh+1 (sτh+1	/σbh2(sτh,aτh)
=φ(s, a〉wh - φ(S, a)>bV-1 (X φ (Sh, ah) ∙ (ThVh+j (Sh, ah) /σh (sh, aT))
'-----------------------------------------------------------------------------------------
}
{z
(i)
+ φ(s, a)>Λ-1 (X φ (sh, ah) ∙ (rh + Vh+1 (sh+1) -(ThVh+1) (sh, ah)) /σh(sh, ah))
'-------------------------------------------7-------------------------------------------
(ii)
(33)
For term (i), it is bounded by 2λHj√d∕κ with probability 1 - δ by Lemma C.2.
For term (ii), by Cauchy inequality it is bounded by
kφ(S, a)kΛb
-1
h
K
X φ (sh, ah) ∙卜 h+Vh+1 (sh+1) -(ThVI+1) (sh, ah)) /σ (sh, ah)
τ=1
Λb
-1
h
2H	K
≤√7κKI∑Φ(sh,ah) ∙9T + Vh+1 (Sτh+1) -
ThVbh+1 (Sτh,aτh) /σbh2(Sτh,aτh)
Λb-1
h
7H
≤^=
κK
∙O(H √d) = O( H⅛d/K),
where the first inequality is by Lemma H.6 (with φ0 = φ∕σh and ∣∣φ∕σhk ≤ ∣∣φk ≤ 1:= C)
and the third inequality uses √a> ∙ A ∙ a ≤ '1同卜 ∣∣A∣∣2 IlaIl2 = kak2 ,kA∣∣2 with a to be either
φ or wh. Moreover, λmin(Λh) ≥ κ/maxh,s,。bh(s,a)2 ≥ κ∕H2 implies ∣∣(Λh)-1∣∣ ≤ H2∕κ.
EK	1 ∙	1∙ ,	Q	T	ɪɪ /-»	∙ ,1	T->	TT	I I	I / T- .	/ T- ∖
The second inequality comes from Lemma H.2 with R = H since |仍| = |(rh + Vh+1 "h+1) 一
，一 ^ . , .. .ʌ .. __ . . . . ., . .ʌ ..
(ThVh+。⑸山乃於九⑸反)1 ≤ H and lxτ| = lφ(sh,ah"σh(sh,ah)1 ≤ 1.
The final result is obtained by absorbing the term (i) via the condition K >
max{M1, M2, M3, M4}.	口
Lemma H.11. Suppose random variables ∣X∣∞ ≤ 7H, ∣Y ∣∞ ≤ 7H, then
|Var(X)-Var(Y)| ≤8H∙∣X-Y∣∞.
Proof of Lemma H.11.
|Var(X) - Var(Y)| =|E[X2] -E[Y2] - (E[X]2 - E[Y]2)| = |E[(X +Y)(X - Y)] - (E[X + Y])(E[X - Y])|
≤E[∣X + YHX - Y∣] + 4H ∙∣X - Yk∞
≤4HE[|X - Y|]+ 4H ∙∣X - Y∣∞ = 8H ∙ ∣X - Y∣∞ .
□
39