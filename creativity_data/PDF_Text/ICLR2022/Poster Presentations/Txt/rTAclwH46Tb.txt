Published as a conference paper at ICLR 2022
Eigencurve: Optimal Learning Rate Schedule
for SGD on Quadratic Objectives with Skewed
Hessian Spectrums
RuiPan1* *, HaishanYe2 *t, TongZhangIM
1	The Hong Kong University of Science and Technology
2	Xi’an Jiaotong University
rpan@connect.ust.hk, yehaishan@xjtu.edu.cn
Ab stract
Learning rate schedulers have been widely adopted in training deep neural net-
works. Despite their practical importance, there is a discrepancy between its prac-
tice and its theoretical analysis. For instance, it is not known what schedules
of SGD achieve best convergence, even for simple problems such as optimizing
quadratic objectives. In this paper, we propose Eigencurve, the first family of
learning rate schedules that can achieve minimax optimal convergence rates (up
to a constant) for SGD on quadratic objectives when the eigenvalue distribution
of the underlying Hessian matrix is skewed. The condition is quite common in
practice. Experimental results show that Eigencurve can significantly outperform
step decay in image classification tasks on CIFAR-10, especially when the num-
ber of epochs is small. Moreover, the theory inspires two simple learning rate
schedulers for practical applications that can approximate eigencurve. For some
problems, the optimal shape of the proposed schedulers resembles that of cosine
decay, which sheds light to the success of cosine decay for such situations. For
other situations, the proposed schedulers are superior to cosine decay.
1 Introduction
Many machine learning models can be represented as the following optimization problem:
1n
minf(W) ,— Efi(W)	(1.1)
w ni=1
such as logistic regression, deep neural networks. To solve the above problem, stochastic gradient
descent (SGD) (Robbins & Monro, 1951) has been widely adopted due to its computation efficiency
in large-scale learning problems (Bottou & Bousquet, 2008), especially for training deep neural
networks.
Given the popularity of SGD in this field, different learning rate schedules have been proposed to
further improve its convergence rates. Among them, the most famous and widely used ones are
inverse time decay, step decay (Goffin, 1977), and cosine scheduler (Loshchilov & Hutter, 2017).
The learning rates generated by the inverse time decay scheduler depends on the current iteration
number inversely. Such a scheduling strategy comes from the theory of SGD on strongly convex
functions, and is extended to non-convex objectives like neural networks while still achieving good
performance. Step decay scheduler keeps the learning rate piecewise constant and decreases it by
a factor after a given amount of epochs. It is theoretically proved in Ge et al. (2019) that when
the objective is quadratic, step decay scheduler outperforms inverse time decay. Empirical results
are also provided in the same work to demonstrate the better convergence property of step decay
in training neural networks when compared with inverse time decay. However, even step decay
* Equal contribution.
* Corresponding author is Haishan Ye.
^ Jointly with Google Research.
1
Published as a conference paper at ICLR 2022
is proved to be near optimal on quadratic objectives, it is not truly optimal. There still exists a
log T gap away from the minimax optimal convergence rate, which turns out to be non-trivial in a
wide range of settings and may greatly impact step decay’s empirical performance. Cosine decay
scheduler (Loshchilov & Hutter, 2017) generates cosine-like learning rates in the range [0, T], with
T being the maximum iteration. It is a heuristic scheduling strategy which relies on the observation
that good performance in practice can be achieved via slowly decreasing the learning rate in the
beginning and “refining” the solution in the end with a very small learning rate. Its convergence
property on smooth non-convex functions has been shown in Li et al. (2021), but the provided
bound is still not tight enough to explain its success in practice.
Except cosine decay scheduler, all aforementioned learning rate schedulers have (or will have) a
tight convergence bound on quadratic objectives. In fact, studying their convergence property on
quadratic objective functions is quite important for understanding their behaviors in general non-
convex problems. Recent studies in Neural Tangent Kernel (NTK) (Arora et al., 2019; Jacot et al.,
2020) suggest that when neural networks are sufficiently wide, the gradient descent dynamic of
neural networks can be approximated by NTK. In particular, when the loss function is least-square
loss, neural network’s inference is equivalent to kernel ridge regression with respect to the NTK
in expectation. In other words, for regression tasks, the non-convex objective in neural networks
resembles quadratic objectives when the network is wide enough.
The existence oflogT gap in step decay’s convergence upper bound, which will be proven to be tight
in a wide range of settings, implies that there is still room for improvement in theory. Meanwhile, the
existence of cosine decay scheduler, which has no strong theoretical convergence guarantees but pos-
sesses good empirical performance in certain tasks, suggests that its convergence rate may depend on
some specific properties of the objective determined by the network and dataset in practice. Hence
it is natural to ask what those key properties may be, and whether it is possible to find theoretically-
optimal schedulers whose empirical performance are comparable to cosine decay if those properties
are available. In this paper, we offer an answer to these questions. We first derive a novel eigenvalue-
distribution-based learning rate scheduler called eigencurve for quadratic functions. Combining
with eigenvalue distributions of different types of networks, new neural-network-based learning rate
schedulers can be generated based on our proposed paradigm, which achieve better convergence
properties than step decay in Ge et al. (2019). Specifically, eigencurve closes the log T gap in
step decay and reaches minimax optimal convergence rates if the Hessian spectrum is skewed. We
summarize the main contributions of this paper as follows.
1.	To the best of our knowledge, this is the first work that incorporates the eigenvalue distribu-
tion of objective function’s Hessian matrix into designing learning rate schedulers. Accord-
ingly, based on the eigenvalue distribution of the Hessian, we propose a novel eigenvalue
distribution based learning rate scheduler named eigencurve.
2.	Theoretically, eigencurve can achieve optimal convergence rate (up to a constant) for
SGD on quadratic objectives when the eigenvalue distribution of the Hessian is skewed.
Furthermore, even when the Hessian is not skewed, eigencurve can still achieve no
worse convergence rate than the step decay schedule in Ge et al. (2019), whose convergence
rate are proven to be sub-optimal in a wide range of settings.
3.	Empirically, on image classification tasks, eigencurve achieves optimal convergence
rate for several models on CIFAR-10 and ImageNet if the loss can be approximated by
quadratic objectives. Moreover, it obtains much better performance than step decay on
CIFAR-10, especially when the number of epochs is small.
4.	Intuitively, our learning rate scheduler sheds light on the theoretical property of cosine
decay and provides a perspective of understanding the reason why it can achieve good
performance on image classification tasks. The same idea has been used to inspire and
discover several simple families of schedules that works in practice.
Problem Setup For the theoretical analysis and the aim to derive our eigenvalue-dependent learn-
ing rate schedulers, we mainly focus on the quadratic function, that is,
min f (W)，Eξ [f(w,ξ)], where f(w,ξ) = 1 w>H(ξ)w - b(ξ)>w,	(1.2)
w2
where ξ denotes the data sample. Hence, the Hessian of f(w) is
H =Eξ [H(ξ)] .	(1.3)
2
Published as a conference paper at ICLR 2022
Letting us denote b = Eξ[b(ξ)], we can obtain the optima of problem (1.2)
w* = H-1b.	(1.4)
Given an initial iterate w0 and the learning rate sequence {ηt}, the stochastic gradient update is
wt+1 = Wt- nNf(wt, ξ) = Wt- ηt(H(ξ)wt — b(ξ)).	(1.5)
We denote that
nt = HWt- b - (H(ξ)wt — b(ξ)), μ，λmin(H),	L，λmaχ(H),	and, K，L∕μ. (1.6)
In this paper, we assume that
Eξ nt nt>	σ2H.	(1.7)
The reason for this assumption is presented in Appendix G.5.
Related Work In convergence anal-
ysis, one key property that separates
SGD from vanilla gradient descent is
that in SGD, noise in gradients domi-
nates. In gradient descent (GD), con-
stant learning rate can achieve linear
convergence O(cT ) with 0 < c < 1 for
strongly convex objectives, i.e. obtain-
ing f(w(t)) - f(w*) ≤ e in O(log(ɪ))
iterations. However, in SGD, f (W(t))
cannot even be guaranteed to converge
to f(W*) due to the existence of gra-
dient noise (Bottou et al., 2018). In-
tuitively, this noise leads to a variance
proportional to the learning rate size, so
constant learning rate will always intro-
duce a Ω(ηt) = ΩΩ(ηo) gap when Com-
pared with the convergence rate of GD.
Table 1: Convergence rate of SGD with common sched-
ulers on quadratic objectives.
Scheduler	Convergence rate of SGD in quadratic objectives
Constant	Not guaranteed to converge
Inverse Time Decay	Θ (dT2 ∙ K)
Step Decay	Θ (dT2 ∙ log T) (Ge et al. (2019); Wu et al. (2021); This work - Theorem 4)
O (dT2) with skewed Hessian spectrums,
Eigencurve	O (dT2 ∙ log K) in worst case
(This work - Theorem 1, Corollary 2, 3)
Fortunately, inverse time decay scheduler solves the problem by decaying the learning rate inversely
proportional to the iteration number t, which achieves O(十)convergence rate for strongly convex
objectives, specifically, O(dT2 ∙ κ). However, this is sub-optimal since the minimax optimal rate
for SGD is O(dT2) (Ge et al., 2019; Jain et al., 2018). Moreover, in practice, K can be very big
for large neural networks, which makes inverse time decay scheduler undesirable for those models.
This is when step decay (Goffin, 1977) comes to play. Empirically, it is widely adopted in tasks
such as image classification and serves as a baseline for a lot of models. Theoretically, it has been
proven that step decay can achieve nearly optimal convergence rate O(督∙ log T) for strongly con-
vex least square regression (Ge et al., 2019). A tighter set of instance-dependent bounds in a recent
work (Wu et al., 2021), which is carried out independently from ours, also proves its near optimal-
ity. Nevertheless, step decay is not always the best choice for image classification tasks. In practice,
cosine decay (Loshchilov & Hutter, 2017) can achieve comparable or even better performance, but
the reason behind this superior performance is still unknown in theory (Gotmare et al., 2018). All
the aforementioned results are summarized in Table 1, along with our results in this paper. It is
worth mentioning that the minimax optimal rate O (繇)can be achieved by iterate averaging meth-
ods (Jain et al., 2018; Bach & Moulines, 2013; DefoSSez & Bach, 2015; Frostig et al., 2015; Jain
et al., 2016; Neu & Rosasco, 2018), but it is not a common practice to use them in training deep
neural networks, so only the final iterate (Shamir, 2012) behavior of SGD is analyzed in this paper,
i.e. the point right after the last iteration.
Paper organization: Section 2 describes the motivation of our eigencurve scheduler. Section 3
presents the exact form and convergence rate of the proposed eigencurve scheduler, along with
the lower bound of step decay. Section 4 shows the experimental results. Section 5 discusses the
discovery and limitation of eigencurve and Section 6 gives our conclusion.
3
Published as a conference paper at ICLR 2022
2 Motivation
In this section, we will give the main motivation and intuition of our eigencurve learning rate
scheduler. We first give the scheduling strategy to achieve the optimal convergence rate in the case
that the Hessian is diagonal. Then, we show that the inverse time learning rate is sub-optimal in
most cases. Comparing these two scheduling methods brings up the reason why we should design
eigenvalue distribution dependent learning rate scheduler.
Letting H be a diagonal matrix diag(λ1, λ2, . . . , λd) and reformulating Eqn. (1.5), we have
wt+1 - w* =Wt - w* - ηt(H(ξ)wt - b(ξ))
=Wt - W* - ηt(Hwt - b) + ηt (Hwt - b - (H(ξ)wt - b(ξ)))
=wt - w* - ηt (Hwt - b - (Hw* - b)) + ηt (Hwt - b - (H(ξ)wt - b(ξ)))
= (I - ηtH) (Wt - W*) + ηtnt.
It follows,
E hλj (Wt+1,j - W*,j)2i E[n=t]=0λj n(1 -ηtλj)2E (Wt,j -W*,j)2 +ηt2E k[nt]jk2o
(1.7)	(2.1)
≤ (I- ηtλj)2 ∙ λjE [(Wtj- w*,j)2] + η2λ2σ2.
Since H is diagonal, we can set step size scheduling for each dimension separately. Letting us
choose step size η coordinately with the step size ηt,j = λ乂*)being optimal for the j-th Coordi-
nate, then we have the following proposition.
Proposition 1. Assume that H is diagonal matrix with eigenvalues λ1 ≥ λ2 ≥ . . . , λd ≥ 0 and
Eqn. (1.7) holds. Ifwe set step size ηt,j =λ乂*), it holds that
d	2	Pd 1 λj (W1 j - W* j)2 t
2E[f(wt+ι)- f(w*)]= E fλj (wt+ι,j - w*,j)2 ≤ 乙j=1 (t+j)2-3+—12 ∙dσ2.
j=1	(t + 1)	(t + 1)
(2.2)
The leading equality here is proved in Appendix G.1, with the followed inequality proved in Ap-
pendix E. From Eqn. (2.2), we can observe that choosing proper step sizes coordinately can achieve
the optimal convergence rate (Ge et al., 2019; Jain et al., 2018). Instead, if the widely used inverse
time scheduler η = 1/(L + μt) is chosen, We can show that only a sub-optimal convergence rate
can be obtained, especially when λj ’s vary from each other.
Proposition 2. Ifwe set the inverse time step size η =(工+*古),then we have
d
2E[f(Wt+1) - f(W*)] = E Xλj (Wt+1,j -W*,j)2
j=1
(2.3)
(L + μ∖2	£、，	、21	£(	λj	1	2	λjσ2	∖
≤(ε)	(Xλj(WIj-W*j) I + X^2λj-μ∙E∙σ +(L+μtγy
Remark 1. Eqn. (2.2) shows that if one can choose step size coordinate-wise with step size ηt,j
λ.(1+i), then SGD can achieve a convergence rate
E [f (WT +1) - f (W*)] ≤ O (T ∙ σ2).
(2.4)
which matches the lower bound (Ge et al., 2019; Jain et al., 2018). In contrast, replacing L = λ1
and μ = λd in Proposition 2, we can obtain that the convergence rate ofSGD being
E[f(WT+1)-f(W*)] ≤O
(2.5)
Since it holds that λj ≥ λd, the convergence rate in Eqn. (2.4) is better than the one in Eqn. (2.5),
especially when the eigenvalues of the Hessian (H matrix) decay rapidly. In fact, the upper bound
in Eqn. (2.5) is tight for the inverse time decay scheduling, as proven in Ge et al. (2019).
4
Published as a conference paper at ICLR 2022
Main Intuition The diagonal case H = diag(λ1, λ2, . . . , λd) provides an important intuition for
designing eigenvalue dependent learning rate scheduling. In fact, for general non-diagonal H, letting
H = UΛU> be the spectral decomposition of the Hessian and setting w0 = U>w, then the Hessian
becomes a diagonal matrix from perspective of updating w0 , with the variance of the stochastic
gradient being unchanged since U is a unitary matrix. This is also the core idea of Newton’s method
and many second-order methods (Huang et al., 2020). However, given our focus in this paper being
learning rate schedulers only, we move the relevant discussion of their relationship to Appendix H.
Proposition 1 and 2 imply that a good learning rate scheduler should decrease the error of each co-
ordinate. The inverse time decay scheduler is only optimal for the coordinate related to the smallest
eigenvalue. That’s the reason why it is sub-optimal overall. Thus, we should reduce the learning
rate gradually such that we can run a optimal learning rate associated to λj to sufficiently drop the
error of j -th coordinate. Furthermore, given the total iteration T and the eigenvalue distribution of
the Hessian, we should allocate the running time for each optimal learning rate associated to λj .
3	Eigenvalue Dependent Step Scheduling
Just as discussed in Section 2, to obtain better conver-
gence rate for SGD, we should consider Hessian’s eigen-
value distribution and schedule the learning rate based on
the distribution. In this section, we propose a novel learn-
ing rate scheduler for this task, which can be regarded as
piecewise inverse time decay (see Figure 1). The method
is very simple, we group eigenvalues according to their
value and denote si to be the number of eigenvalues lie in
the range Ri = [μ ∙ 2i, μ ∙ 2i+1), that is,
Si = #Xj ∈ [μ∙ 2i,μ∙ 2i+1).	(3.1)
Figure 1: Eigencurve : piecewise in-
verse time decay scheduling.
Then, there are at most Imax = log2 κ such ranges. By the inverse time decay theory, the optimal
learning rate associated to eigenvalues in the range Ri should be
ηt = O (2i-ι μ 1(t-t∕ ), with 0 = to <tι <t2 <…< timax = T.	(3.2)
Our scheduling strategy is to run the optimal learning rate for eigenvalues in each Ri for a period to
sufficiently decrease the error associated to eigenvalues in Ri .
To make the step size sequence {ηt }tT=1 monotonely decreasing, we define the step sizes as
1
L + μ Pj=I ∆j2jT + 2i-1 μ(t - t—)
if t ∈ [ti-1, ti)
(3.3)
where
0 = to < tι < t2 < …< timax = T, ∆i = ti - ti-1, and Imax = log2 K. (3.4)
To make the total error, that is the sum of error associated with Ri , to be small, we should allocate
∆i according to si-1’s. Intuitively, a large portion of eigenvalues lying in the range Ri should
allocate a large portion of iterations. Specifically, we propose to allocate ∆i as follows:
∆i = LI^s———T, Withsi = #λj ∈ [μ∙ 2i,μ ∙ 2i+1).	(3.5)
PImax	√s
In the rest of this section, we will show that the step size scheduling according to Eqn. (3.3) and
(3.5) can achieve better convergence rate than the one in Ge et al. (2019) when si is non-uniformly
distributed. In fact, a better ∆i allocation can be calculated using numerical optimization.
3.1	Theoretical Analysis
Lemma 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T -steps starting from wo anda learning rate sequence {ηt}tT=1 defined in Eqn. (3.3), the final iterate
5
Published as a conference paper at ICLR 2022
wT +1 satisfies
κ2	15
E [f(WT +1) - f(WJ ≤(f(w0) - f(WJ) ∙ ∆2 + ^2
Imax -1
σ 2 μ X
~ _
i=0
2i+1si
7 I ι
L + μ Pj= ∆j 2j-1
(3.6)
Since the bias term is a high order term, the variance term in Eqn. (3.6) dominates the error for
WT+1. For simplicity, instead of using numerical methods to find the optimal {∆i}, we propose to
use ∆i defined in Eqn. (3.5). The value of ∆i is linear to square root of the number of eigenvalues
lying in the range [μ ∙ 2i-1, μ ∙ 2i). Using such ∆i, eigencurve has the following convergence
property.
Theorem 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T -steps starting from W0, a learning rate sequence {ηt}tT=1 defined in Eqn. (3.3) and ∆i defined in
Eqn. (3.5), the final iterate WT+1 satisfies
2
2
E [f(WT+1)- f(Wj ≤(f(w0) - f(WJ) ∙
κ2
s0T2
PImax -1
i=0
15 (PI=0x T
σ2
Please refer to Appendix D, F and G for the full proof of Lemma 1 and Theorem 1. The variance
term
15(PImax T √i )2
∙ σ2 in above theorem shows that when si ’s vary largely from each other,
then the variance can be close to O (T ∙ σ2) which matches the lower bound (Ge et al., 2019). For
example, letting ImaX = 100, s0 = 0.99d and Si =喘1 d, We can obtain that
(p9=0 √si )2
T
σ2 = (√019 + 99 X Pθ.01∕99)2 ∙ ； ∙ σ2 ‹ 学∙ σ2.
We can observe that if the variance of si ’s is large, the
variance term in Theorem 1 can be close to dσ2∕T.
More generally, as rigorously stated in Corollary 2,
eigencurve achieves minimax optimal convergence
rate if the Hessian spectrum satisfies an extra assumption
of “power law”: the density of eigenvalue λ is exponen-
tially decaying with increasing value ofλ in log scale, i.e.
ln(λ). This assumption comes from the observation of
estimated Hessian spectrums in practice (see Figure 2),
which will be further illustrated in Section 4.1.
Corollary 2. Given the same setting as in Theorem 1,
when Hessian H ’s eigenvalue distribution p(λ) satisfies
“power law”, i.e.
Figure 2: Power law observed in
ResNet-18 on ImageNet, both eigen-
P(λ) = ɪ ∙ eχp(-α(ln(λ) - ln(μ))) = ɪ ∙ (μ)
Z	Zλ
(3.7)
for some α > 1, where Z = RL(μ∕λ)αdλ, there exists a
value (x-axis) and density (y-axis) are
plotted in log scale.
constant C(α) which only depends on α, such that the final iterate WT+1 satisfies
κ2	dσ2
E [f (wt+1) - f (W*)] ≤ ((f (w0) - f (W*)) ∙ T + -T-) ∙ C(α).
Please refer to Appendix G.3 for the proof. As for the worst-case guarantee, it is easy to check that
only when si’s are equal to each other, that is, si = -∕ImaX = -∕ log2(κ), the variance term reaches
its maximum.
Corollary 3. Given the same setting as in Theorem 1, when si = -∕ log2 (κ) for all 0 ≤ i ≤
ImaX - 1, the variance term in Theorem 1 reaches its maximum and WT+1 satisfies
E [f (WT+1) - f (W*)] ≤ Cf(W0) - f (W*)) ∙ K Tg K + 15d TOg Kσ2.
+
T
T
6
Published as a conference paper at ICLR 2022
Remark 2. When si ’s vary from each other, our eigenvalue dependent learning rate scheduler can
achieve faster convergence rate than eigenvalue independent scheduler such as step decay which
suffers from an extra log(T) term (Ge et al., 2019). Only when si ’s are equal to each other, Corol-
lary 3 shows that the bound of variance matches to lower bound up to log κ which is same to the
one in Proposition 3 of Ge et al. (2019).
Furthermore, we show that this log T gap between step decay and eigencurve certainly exists
for problem instances of skewed Hessian spectrums. For simplicity, we only discuss the case where
H is diagonal.
Theorem 4. Let objective function f(x) be quadratic. We run SGD for T -steps starting from w0 and
a step decay learning rate sequence {ηt}tT=1 defined in Algorithm 1 of Ge et al. (2019) with η1 ≤
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ ntnt> =
σ1 2H and ⑶ λj (wo,j 一 w*,j )2 = 0 for ∀j = 1, 2,..., d, the final iterate WT +ι satisfies,
E[f (WT +1)- f (W*)]
dσ2
• log T
Ω
The proof is provided in Appendix G.4. Removing this extra logT term may not seem to be a big
deal in theory, but experimental results suggest the opposite.
4	Experiments
To demonstrate eigencurve ’s practical value, empirical experiments are conducted on the task
of image classification 1. Two well-known dataset are used: CIFAR-10 (Krizhevsky et al., 2009)
and ImageNet (Deng et al., 2009). For full experimental results on more datasets, please refer to
Appendix A.
4.1	Hessian Spectrum’ s Skewness in Practice
According to estimated2 eigenvalue distributions of Hessian on CIFAR-10 and ImageNet, as shown
in Figure 3, it can be observed that all of them are highly skewed and share a similar tendency: A
large portion of small eigenvalues and a tiny portion of large eigenvalues. This phenomenon has also
been observed and explained by other researchers in the past(Sagun et al., 2017; Arjevani & Field,
2020). On top of that, when we plot both eigenvalues and density in log scale, the “power law” arises.
Therefore, if the loss surface can be approximated by quadratic objectives, then eigencurve has
already achieved optimal convergence rate for those practical settings. The exact values of the extra
constant terms are presented in Appendix A.2.
4.2	Image Classification on CIFAR- 1 0 with EIGENCURVE Scheduling
This optimality in theory induces eigencurve ’s superior performance in practice, which is
demonstrated in Table 2 and Figure 4. The full set of figures are available in Appendix A.8. All mod-
els are trained with stochastic gradient descent (SGD), no momentum, batch size 128 and weight
decay Wd = 0.0005. For full details of the experiment setup, please refer to Appendix B.
4.3	Inspired Practical Schedules with Simple Forms
By simplifying the form of eigencurve and capturing some of its key properties, two simple and
practical schedules are proposed: Elastic Step Decay and Cosine-power Decay, whose empirical
performance are better than or at least comparable to cosine decay. Due to page limit, we leave all
the experimental results in Appendix A.5, A.6, A.7.
Elastic StePDecay: ηt = ηo∕2k , if t ∈ [(1 一 rk)T, (1 一 rk+1)T)
α
Cosine-power Decay: ηt = ηmin + (η0 - ηmin)
(4.1)
(4.2)
2(1 + cos( 厂Ln))
2	tmax
1Code: https://github.com/opensource12345678/why_cosine_works/tree/main
2Please refer to APPendix B.2 for details of the estimation and PreProcessing Procedure.
7
Published as a conference paper at ICLR 2022
亩一SS 60i-"SUBCl
O 25	50	75 IOO 125	150	175
Eigenvlaue
O	50 IOO 150	200	250
Figure 3: The estimated eigenvalue distribution of Hessian for ResNet-18 on CIFAR-10, GoogLeNet
on CIFAR-10 and ResNet-18 on ImageNet respectively. Notice that the density here is all shown
in log scale. First row: original scale for eigenvalues. Second row: log scale for preprocessed
eigenvalues.
Table 2: CIFAR-10: training losses and test accuracy of different schedules. Step Decay denotes the
scheduler proposed in Ge et al. (2019) and General Step Decay means the same type of scheduler
with searched interval numbers and decay rates. “*” before a number means at least one occurrence
of loss explosion among all 5 trial experiments.
#Epoch	Schedule	ResNet-18		GoogLeNet		VGG16	
		Loss	Acc(%)	Loss	Acc(%)	Loss	Acc(%)
	Inverse Time Decay	1.58±0.02	79.45±1.00	2.61±0.00	86.54±0.94	2.26±0.00	84.47±0.74
	Step Decay	1.82±0.04	73.77±1.48	2.59±0.02	87.04±0.48	2.42±0.45	82.98±0.27
=10	General Step Decay	1.52±0.02	81.99±0.35	1.93±0.03	88.32±1.32	2.14±0.42	86.79±0.36
	Cosine Decay	1.42±0.01	84.23±0.07	1.94±0.00	90.56±0.31	2.03±0.00	87.99±0.13
	Eigencurve	1.36±0.01	85.62±0.28	1.33±0.00	90.65±0.15	1.87±0.00	88.73±0.11
	Inverse Time Decay	0.73±0.00	90.82±0.43	0.62±0.02	92.05±0.69	1.32±0.62	*76.24±13.77
	Step Decay	0.26±0.01	91.39±1.03	0.28±0.00	92.83±0.15	0.59±0.00	91.37±0.20
=100	General Step Decay	0.17±0.00	93.97±0.21	0.13±0.00	94.18±0.18	0.20±0.00	*92.36±0.46
	Cosine Decay	0.17±0.00	94.04±0.21	0.12±0.00	94.62±0.11	0.20±0.00	93.17±0.05
	Eigencurve	0.14±0.00	94.05±0.18	0.12±0.00	94.75±0.15	0.18±0.00	92.88±0.24
number of iterations
Figure 4: Example: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses.
Right: test accuracy. For full figures of this experiment, please refer to Appendix A.8.
5000 10000 15000 20000 25000 30000 35000
number of iterations
8
Published as a conference paper at ICLR 2022
5	Discussion
Cosine Decay and Eigencurve For ResNet-18 on CIFAR-10 dataset, eigencurve scheduler
presents an extremely similar learning rate curve to cosine decay, especially when the number of
training epochs is set to 100, as shown in Figure 5. This directly links cosine decay to our theory:
the empirically superior performance of cosine decay is very likely to stem from the utilization of the
“skewness” among Hessian matrix’s eigenvalues. For other situations, especially when the number
of iterations is small, as shown in Table 2, eigencurve presents a better performance than cosine
decay .
Figure 5: Eigencurve ’s learning rate curve generated by the estimated eigenvalue distribution for
ResNet-18 on CIFAR-18 after training 50/100/200 epochs. The cosine decay’s learning rate curve
(green) is also provided for comparison.
Sensitiveness to Hessian’s Eigenvalue Distributions One limitation of eigencurve is that it
requires a precomputed eigenvalue distribution of objective functions’s Hessian matrix, which can
be time-consuming for large models. This issue can be overcome by reusing the estimated eigen-
value distribution from similar settings. Further experiments on CIFAR-10 suggest the effective-
ness of this approach. Please refer to Appendix A.3 for more details. This evidence suggests that
eigencurve’s performance is not very sensitive to estimated eigenvalue distributions.
Relationship with Numerically Near-optimal Schedulers In Zhang et al. (2019), a dynamic
programming algorithm was proposed to find almost optimal schedulers if the exact loss of the
quadratic objective is accessible. While itis certainly the case, eigencurve still possesses several
additional advantages over this type of approaches. First, eigencurve can be used to find simple-
formed schedulers. Compared with schedulers numerically computed by dynamic programming,
eigencurve provides an analytic framework, so it is able to bypass the Hessian spectrum estima-
tion process if some useful assumptions of the Hessian spectrum can be obtained, such as ”power
law”. Second, eigencurve has a clear theoretical convergence guarantee. Dynamic programming
can find almost optimal schedulers, but the convergence property of the computed scheduler is still
unclear. Our work fills this gap.
6	Conclusion
In this paper, a novel learning rate schedule named eigencurve is proposed, which utilizes the
“skewness” of objective’s Hessian matrix’s eigenvalue distribution and reaches minimax optimal
convergence rates for SGD on quadratic objectives with skewed Hessian spectrums. This condition
of skewed Hessian spectrums is observed and indeed satisfied in practical settings of image classifi-
cation. Theoretically, eigencurve achieves no worse convergence guarantee than step decay for
quadratic functions and reaches minimax optimal convergence rate (up to a constant) with skewed
Hessian spectrums, e.g. under “power law”. Empirically, experimental results on CIFAR-10 show
that eigencurve significantly outperforms step decay, especially when the number of epochs is
small. The idea of eigencurve offers a possible explanation for cosine decay’s effectiveness in
practice and inspires two practical families of schedules with simple forms.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work is supported by GRF 16201320. Rui Pan acknowledges support from the Hong Kong PhD
Fellowship Scheme (HKPFS). The work of Haishan Ye was supported in part by National Natural
Science Foundation of China under Grant No. 12101491.
References
Yossi Arjevani and Michael Field. Analytic characterization of the hessian in shallow relu models:
A tale of symmetry, 2020.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net, 2019.
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con-
vergence rate o (1/n). arXiv preprint arXiv:1306.2119, 2013.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 557-565. PMLR, 06-11 Aug 2017. URL https://Proceedings .mlr.ρress∕v70∕
botev17a.html.
Leon BottoU and Olivier Bousquet. The tradeoffs of large scale learning. In J. Platt, D. Koller,
Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, vol-
ume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/
paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf.
Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning, 2018.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM trans-
actions on intelligent systems and technology (TIST), 2(3):1-27, 2011.
Alexandre DefoSSez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and
optimal sampling distributions. In Artificial Intelligence and Statistics, pp. 205-213. PMLR,
2015.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled new-
ton methods. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
404dcc91b2aeaa7caa47487d1483e48a- Paper.pdf.
Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the empirical risk
minimizer in a single pass. In Conference on learning theory, pp. 728-763. PMLR, 2015.
Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure for least squares. In Advances in
Neural Information Processing Systems, pp. 14977-14988, 2019.
Jean-Louis Goffin. On convergence rates of subgradient optimization methods. Mathematical pro-
gramming, 13(1):329-347, 1977.
10
Published as a conference paper at ICLR 2022
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at
deep learning heuristics: Learning rate restarts, warmup and distillation, 2018.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour, 2018.
Roger Grosse and James Martens. A kronecker-factored aPProximate fisher matrix for convo-
lution layers. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The
33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 573-582, New York, New York, USA, 20-22 Jun 2016. PMLR. URL
https://proceedings.mlr.press/v48/grosse16.html.
Sepp Hochreiter and JUrgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9
(8):1735-1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:
//doi.org/10.1162/neco.1997.9.8.1735.
Xunpeng Huang, Xianfeng Liang, Zhengyang Liu, Lei Li, Yue Yu, and Yitan Li. Span: A stochastic
projected approximate newton method. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 1520-1527, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks, 2020.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par-
allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint
arXiv:1610.03774, 2016.
Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Acceler-
ating stochastic gradient descent for least squares regression, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. A second look at exponential and cosine step
sizes: Simplicity, adaptivity, and performance, 2021.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993. URL
https://aclanthology.org/J93-2004.
Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic gradient de-
scent. In Conference On Learning Theory, pp. 3222-3242. PMLR, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singu-
larity and beyond, 2017.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723-1738, 2002.
Ohad Shamir. Is averaging needed for strongly convex stochastic gradient descent. Open problem
presented at COLT, 2012.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes, 2012.
11
Published as a conference paper at ICLR 2022
Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Last iterate
risk bounds of sgd with decaying stepsize for overparameterized linear regression. arXiv preprint
arXiv:2110.06198, 2021.
Minghan Yang, Dong Xu, Hongyu Chen, Zaiwen Wen, and Mengyun Chen. Enhance curvature
information by structured stochastic quasi-newton methods. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),pp. 10654-10663, June 2021.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. Pyhessian: Neural networks
through the lens of the hessian, 2020.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization,
2015.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model, 2019.
12
Published as a conference paper at ICLR 2022
A More Experimental Results
A.1 Ridge Regression
We compare different types of schedulings on ridge regression
f(w) = 11IXw - Y112 + α∣∣w∣l2.
This experiment is only an empirical proof of our theory. In fact, the optima of ridge regression has
a closed form and can be directly computed with
w* = (X > X + nαI )-1 X T Y.
Thus the optimal training loss f (w*) can be calculated accordingly. In all experiments, We use the
loss gap f(wT) - f(w*) as our performance metric.
Experiments are conducted on a4a datasets (Chang & Lin, 2011; Dua & Graff, 2017) (https://
www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a4a/),
Which contains 4, 781 samples and 123 features. This dataset is chosen majorly because it has a
moderate number of samples and features, Which enables us to compute the exact Hessian matrix
H = 2(X > X/n + αI) and its corresponding eigenvalue distribution in acceptable time and space
consumption.
In all of our experiments, We set α = 10-3 . The model is optimized via SGD Without momentum,
With batch size 1, initial learning rate η0 ∈ {0.1, 0.06, 0.03, 0.02, 0.01, 0.006, 0.003, 0.002, 0.001,
0.0006, 0.0003, 0.0002, 0.0001} and learning rate of last iteration ηmin ∈ {0.1, 0.01, 0.001, 0.0001,
0.00001, 0, “UNRESTRICTED”}. Here “UNRESTRICTED” denotes the case Where ηmin is not
set, Which is useful for eigencurve, Who can decide the learning rate curve Without setting ηmin.
Given η0 and ηmin, We adjust all schedulers as follows. For inverse time decay ηt = η0/(1 + γη01)
and exponential decay ηt = γtη0 , the hyperparameter γ is computed accordingly based on η0 and
ηmin. For cosine decay, η0 and ηmin is directly used, With no restart adopted. For eigencurve,
the learning rate curve is linearly scaled to match the given ηmin .
In addition, for eigencurve, We use the eigenvalue distribution of the Hessian matrix, Which is
directly computed via eigenvalue decomposition, as shoWn in Figure 6.
Figure 6: The eigenvalue distribution of Hessian for ridge regression on a4a. Left: original scale for
eigenvalues. Right: log scale for eigenvalues. Notice that the density here is shoWn in log scale.
All experimental results demonstrate that eigencurve can obtain similar or better training losses
When compared With other schedulers, as shoWn in Table 3.
13
Published as a conference paper at ICLR 2022
Table 3: Ridge regression: training loss gaps of different schedules over 5 trials.
Training loss - optimal training loss: f (WT) — f (w*)		
Schedule	#EPoCh = 1	#Epoch = 5
Constant	0.014963±0.001369	0.004787±0.000175
Inverse Time Decay	0.007284±0.000190	0.002098±0.000160
Exponetial Decay	0.008351±0.000360	0.000931±0.000100
Cosine Decay	0.007767±0.000006	0.001167±0.000142
Eigencurve	0.006977±0.000197	0.000676±0.000069
Schedule	#EPoCh = 25	#Epoch = 250
Constant	0.001351±0.000179	0.000122±0.000009
Inverse Time Decay	0.000637±0.000143	0.000011±0.000001
Exponetial Decay	0.000048±0.000007	0.000000±0.000000
Cosine Decay	0.000054±0.000005	0.000000±0.000000
Eigencurve	0.000045±0.000008	0.000000±0.000000
A.2 Exact Value of the Extra Term on CIFAR- 1 0 Experiments
In Section 4.1, we have already given the qualitative evidence that shows eigencurve ’s op-
timality for practical settings on CIFAR-10. Here we strengthen this argument by providing the
quantitative evidence as well. The exact value of the extra term is presented in Table 4, where we
assume CIFAR-10 has batch size 128, number of epochs 200 and weight decay 5 × 10-4, while
ImageNet has batch size 256, number of epochs 90 and weight decay 10-4 .
Table 4: Convergence rate of SGD with common schedulers given the estimated eigenvalue distri-
bution of Hessian, assuming the objective is quadratic.
Value of the extra term						
Scheduler		Convergence rate of SGD in	CIFAR-10	CIFAR-10	CIFAR-10	ImageNet
		quadratic functions	+ ResNet18	+ GoogLeNet	+ VGG16	+ ResNet18
Inverse Time Decay		Θ (dT2 ∙ K)	3.39 × 105	4.92 × 105	6.50 × 105	6.80 × 106
Step Decay		Θ (dT2 ∙ log T)	16.25	16.25	16.25	18.78
	O	'dσ2	(PImaxT √si)2ʒ				
		~T~ ∙	d				
Eigencurve	si	∖ / where Imax = log2 κ, = #lj ∈ [μ ∙ 2i,μ ∙ 2i+1)	8.15	5.97	7.12	12.61
Minimax optimal rate		Ω (喑)	一	1	1	1	1
It is worth noticing that the extra term’s value of eigencurve is independent from the number of
iterations T, since the value (PImOxT √si)2/d only depends on the Hessian spectrum. So basically
eigencurve has already achieved the minimax optimal rate (up to a constant) for models and
datasets listed in Table 4, if the loss landscape around the optima can be approximated by quadratic
functions. For full details of the estimation process, please refer to Appendix B.
A.3 Reusing E IGENCURVE for Different Models on CIFAR- 1 0
For image classification tasks on CIFAR-10, we check the performance of reusing ResNet-18’s
eigenvalue distribution for other models. As shown in Table 5, experimental results demonstrate that
Hessian’s eigenvalue distribution of Resnet-18 on CIFAR-10 can be applied to GoogLeNet/VGG16
and still achieves good peformance. Here the experiment settings are exactly the same as Section 4.2
in main paper.
14
Published as a conference paper at ICLR 2022
Table 5: CIFAR-10: training losses and test accuracy of different schedules over 5 trials. Here
all eigencurve schedules are generated based on ResNet-18’s Hessian spectrums. “*” before a
number means at least one occurrence of loss explosion among all 5 trial experiments.
#Epoch	Schedule	GoogLeNet		VGG16	
		Loss	Acc(%)	Loss	Acc(%)
	Inverse Time Decay	2.61±0.00	86.54±0.94	2.26±0.00	84.47±0.74
	Step Decay	2.59±0.02	87.04±0.48	2.42±0.45	82.98±0.27
=10	General Step Decay	1.93±0.03	88.32±1.32	2.14±0.42	86.79±0.36
	Cosine Decay	1.94±0.00	90.56±0.31	2.03±0.00	87.99±0.13
	Eigencurve (transferred)	1.65±0.00	91.17±0.20	1.89±0.00	88.17±0.32
	Inverse Time Decay	0.62±0.02	92.05±0.69	1.32±0.62	*76.24±13.77
=100	Step Decay	0.28±0.00	92.83±0.15	0.59±0.00	91.37±0.20
	General Step Decay	0.13±0.00	94.18±0.18	0.20±0.00	*92.36±0.46
	Cosine Decay	0.12±0.00	94.62±0.11	0.20±0.00	93.17±0.05
	Eigencurve (transferred)	0.11±0.00	94.81±0.19	0.20±0.00	93.17±0.09
A.4 Comparison with Exponential Moving Average on CIFAR- 1 0
Besides learning rate schedules, Exponential Moving Averaging (EMA) method
Wt = α ɪ2(l — α)t~kwk	^⇒ Wt = αwt + (1 — α)Wt-ι
k=0
is another competitive practical method that is commonly adopted in training neural networks with
SGD. Thus, it is natural to ask whether eigencurve can beat this method as well. The answer
is yes. In Table 6, we present additional experimental results on CIFAR-10 to compare the perfor-
mance of eigencurve and exponential moving averaging. It can be observed that there is a large
performance gap between those two methods.
Table 6: CIFAR-10: training losses and test accuracy of Exponential Moving Average (EMA) and
eigencurve with #Epoch = 100 over 5 trials. For EMA, we search its constant learning rate
ηt = η0 ∈ {1.0, 0.6, 0.3, 0.2, 0.1} and decay α ∈ {0.9, 0.95, 0.99, 0.995, 0.999}. Other settings
remain the same as Section 4.2.
Method/Schedule	ResNet-18		GoogLeNet		VGG16	
	Loss	Acc(%)	Loss	Acc(%)	Loss	Acc(%)
EMA	0.30±0.01	90.09±0.82	0.33±0.01	93.42±0.26	0.49±0.00	91.87±0.82
Eigencurve	0.14±0.00	94.05±0.18	0.12±0.00	94.75±0.15	0.18±0.00	92.88±0.24
A.5 ImageNet Classification with Elastic Step Decay
One key observation in CIFAR-10 experiments is the existence of “power law” shown in Figure 3.
Also, notice that in the form of eigencurve , specifically Eqn. (3.5), iteration interval length ∆i
is proportional to the square root of eigenvalue density Si in range [μ ∙ 2i,μ ∙ 2i+1). Combining those
two facts together, it suggests the length of “learning rate interval” should have lengths exponentially
decreasing.
Based on this idea, Elastic Step Decay (ESD) is proposed, which has the following form,
ηt = ηo∕2k , if t ∈ [(1 — rk)T, (1 — rk+1)T)
Compared to general step decay with adjustable interval lengths, elastic step decay does not require
manual adjustment for the length of each interval. Instead, they are all controlled by one hyper-
parameter r ∈ (0, 1), which decides the “shrinking speed” of interval lengths. Experiments on
15
Published as a conference paper at ICLR 2022
CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority in practice, as shown in Table 7,
Table 8.
For experiments on CIFAR-10/CIFAR-100, we adopt the same settings as eigencurve , except
we only use common step decay with three same-length intervals + decay factor 10.
Table 7: Elastic Step Decay on CIFAR-10/CIFAR-100: test accuracy(%) of different schedules over
5 trials. “*” before a number means at least one occurrence of loss explosion among all 5 trial
experiments.
#Epoch	Schedule	ResNet-18		GoogLeNet		VGG16	
		CIFAR-10	CIFAR-100	CIFAR-10	CIFAR-100	CIFAR-10	CIFAR-100
	Inverse Time Decay	79.45±1.00	48.73±1.66	86.54±0.94	57.90±1.27	84.47±0.74	50.04±0.83
	Step Decay	79.67±0.74	54.54±0.26	88.37±0.13	63.05±0.35	85.18±0.06	45.86±0.31
=10	Cosine Decay	84.23±0.07	61.26±1.11	90.56±0.31	69.09±0.27	87.99±0.13	55.42±0.28
	ESD	85.38±0.38	64.17±0.57	91.23±0.33	70.46±0.41	88.67±0.21	57.23±0.39
	Inverse Time Decay	90.82±0.43	69.82±0.37	92.05±0.69	73.54±0.28	*76.24±13.77	67.70±0.49
_1 ∩∩	Step Decay	93.68±0.07	73.13±0.12	94.13±0.32	76.80±0.16	92.62±0.15	70.02±0.41
=100	Cosine Decay	94.04±0.21	74.65±0.41	94.62±0.11	78.13±0.54	93.17±0.05	72.47±0.28
	ESD	94.06±0.11	74.76±0.33	94.65±0.11	78.23±0.20	93.25±0.12	72.50±0.26
For experiments on ImageNet, we use ResNet-50 trained via SGD without momentum, batch size
256 and weight decay wd = 10-4. Since no momentum is used, the initial learning rate is set to
η0 = 1.0 instead of η0 = 0.1. Two step decay baselines are adopted. “Step Decay [30-60]” is the
common choice that decays the learning rate 10 folds at the end of epoch 30 and epoch 60. “Step
Decay [30-60-80]” is another popular choice for the ImageNet setting (Goyal et al., 2018), which
further decays learning rate 10 folds at epoch 80. For cosine decay scheduler, the hyperparameter
ηmin is set to be 0. As for the dataset, we use the common ILSVRC 2012 dataset, which contains
1000 classes, around 1.2M images for training and 50,000 images for validation. For all experiments,
We search r ∈ {1/2,1/√2} for ESD, with other hyperparameter search and selection process being
the same as eigencurve .
Table 8: Elastic Step Decay on ImageNet-1k: Losses and validation accuracy of different schedul-
ings for ResNet-50 with #Epoch=90 over 3 trials.
	Schedule	Training loss	Top-1 validation acc(%)	Top-5 validation acc(%)
	Step Decay [30-60]	1.4726±0.0057	75.55±0.13	92.63±0.08
	Step Decay [30-60-80]	1.4738±0.0080	76.05±0.33	92.83±0.15
#Epoc=90	Cosine Decay	1.4697±0.0049	76.57±0.07	93.25±0.05
	ESD (r = 1∕√2)	1.4317±0.0027	76.79±0.10	93.31±0.05
A.6 Language Modeling with Elastic Step Decay
More experiments on language modeling are conducted to further demonstrate Elastic Step Decay’s
superiority over other schedulers.
For all experiments, we follow almost the same setting in Zaremba et al. (2015), where a large
regularized LSTM recurrent neural network (Hochreiter & Schmidhuber, 1997) is trained on Penn
Treebank (Marcus et al., 1993) for language modeling task. The Penn Treebank dataset has a training
set of 929k words, a validation set of 73k words anda test set of 82k words. SGD without momentum
is adopted for training, with batch size 20 and 35 unrolling steps in LSTM.
Other details are exactly the same, except for the number of training epochs. In Zaremba et al.
(2015), it uses 55 epochs to train the large regularized LSTM, which is changed to 30 epochs in our
setting, since we found that the model starts overfitting after 30 epochs. We conducted hyperparam-
eter search for all schedules, as shown in Table 9.
16
Published as a conference paper at ICLR 2022
Table 9: Hyperparameter search for schedulers.
Scheduler				Form	Hyperparameter choices
Inverse Time Decay			ηt =	no	no ∈ {10o, 10-1, 10-2, 10-3}, and set λ, so that nmin ∈ {10-2, 10-3, 10-4, 10-5, 10-6}
				1 + λ∙no∙t	
General Step Decay			ηt = no ∙ Yk, if t ∈ [k,k +1) ∙ K		no = 1, K∈ {3, 4, 5, blog T c}, blogTc +1}, Y ∈ { 1 1 ɪ} Y ∈ { 2 , 5 , 10 }	
Cosine Decay	ηt	=ηmin + 2 (η0		-nmin) (1 + cos (tπ))	no ∈ {10o, 10-1, 10-2, 10-3}, nmin ∈ {10-2, 10-3, 10-4, 0}
Elastic Step Decay		ift ∈	ηt (1 - r	=no/2k, k)T, (1 - rk+1)T	no = 1, r ∈ {2-1, 2-1/2, 2-1/3, 2-1/5, 2-1/2o},
Baseline		ηt =	η0 η no 1 1.15k	for first 14 epochs for epoch k + 14	no = 1
Experimental results show that Elastic Step Decay significantly outperforms other schedulers, as
shown in Table 10.
Table 10: Scheduler performance on LSTM + Penn Treebank over 5 trials.
Scheduler	Validation perplexity	Test perplexity
Inverse Time Decay	114.9±1.1	112.7±1.1
General Step Decay	82.4±0.1	79.1±0.2
Baseline (Zaremba et al., 2015)	82.2	78.4
Cosine Decay	82.4±0.4	78.5±0.4
Elastic Step Decay	81.1±0.2	77.4±0.3
17
Published as a conference paper at ICLR 2022
A.7 Image Classification on ImageNet with Cosine-power Scheduling
Another key observation in CIFAR-10 experiments is that eigencurve ’s learning rate curve
shape changes in a fixed tendency: more “concave” learning rate curves for less training epochs,
which inspire the cosine-power schedule in following form.
α
Cosine-power : ηt = ηmin + (η0 - ηmin)
2(ι+cos( ^n))
2	tmax
Results in Table 11 show the schedulings’ performance with α = 0.5/1/2, which are denoted as
7Cosine/Cosine/Cosine2 respectively. Notice that the best scheduler gradually moves from small a
to larger α when the number of epochs increases. For #epoch=270, since the number of epochs is
large enough to make model converge, it is reasonable that the accuracy gap between all schedulers
is small.
For experiments on ImageNet, we use ResNet-18 trained via SGD without momentum, batch size
256 and weight decay wd = 10-4. Since no momentum is used, the initial learning rate is set to
η0 = 1.0 instead of η0 = 0.1. The hyperparameters ηmin is set to be 0 for all cosine-power scheduler.
As for the dataset, we use the common ILSVRC 2012 dataset, which contains 1000 classes, around
1.2M images for training and 50,000 images for validation.
Table 11: Cosine-power Decay on ImageNet: training losses and validation accuracy (%) of different
schedulings for ReSNet-18 over 3 trials. Settings #Epoch2 90 only have 1 trial due to constraints of
resource and time.
#Epoch	Schedule	Training loss	Top-1 validation acc (%)	Top-5 validation acc (%)
	C Cosine	5.4085±0.0080	30.01±0.21	55.26±0.33
1	Cosine	5.4330±0.0106	26.43±0.31	50.85±0.43
	Cosine2	5.4939±0.0157	21.81±0.21	44.53±0.09
	7 Cosine	2.9515±0.0057	57.27±0.15	80.71±0.12
5	Cosine	2.8389±0.0061	55.67±0.08	79.46±0.16
	Cosine2	2.9160±0.0099	52.75±0.20	77.11±0.08
	7 CoSine	2.1739±0.0046	67.56±0.03	87.82±0.09
30	Cosine	2.0402±0.0031	67.97±0.10	88.12±0.03
	Cosine2	2.0525±0.0032	67.41±0.05	87.70±0.10
	7 CoSine	1.9056	69.85	89.46
90	Cosine	1.7676	70.46	89.75
	Cosine2	1.7403	70.42	89.69
	7 CoSine	1.7178	71.37	90.31
270	Cosine	1.5756	71.93	90.33
	Cosine2	1.5250	71.69	90.37
Figure 7: Learning
scale.
IOOOOO 200000 300000 400000
number of iterations
cosine-power-2
cosine-power-1
cosine-power-0.5
rate curve of three cosine-power schedulers. Top: original scale; Bottom: log
18
Published as a conference paper at ICLR 2022
A.8 Full Figures for Eigencurve Experiments in Section 4.2
Please refer to Figure 8, 9, 10, 11, 12 and 13.
Figure 8: CIFAR-10 results for ResNet-18, with #Epoch = 10. Left: training losses. Right: test
accuracy.
Figure 9: CIFAR-10 results for GoogLeNet, with #Epoch = 10. Left: training losses. Right: test
accuracy.
----inverse-time-decay
----step-decay
----general-step-decay
----CoSine-deCay
----eigencurve
-----1------1------1——
500 1000 1500 2000 2500 3000 3500
number of iterations
Figure 10: CIFAR-10 results for VGG16, with #Epoch = 10. Left: training losses. Right:
accuracy.
test
19
Published as a conference paper at ICLR 2022
0.850
0.825
0.800
5000 10000 15000 20000 25000 30000 35000
number of iterations
Figure 11: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses. Right: test
accuracy.
uuEJSg
0.925
0.900
0.875
0.850
0.825
0.800
1.000
0.975
0.950
Figure 12: CIFAR-10 results for GoogLeNet, with #Epoch = 100. Left: training losses. Right: test
accuracy.
1.000
0.975
0.950
0.925
2∣ 0.900
S
2i
0.875
0.850
0.825
0.800
5000 10000 15000 20000 25000 30000 35000
number of iterations
number of iterations
Figure 13: CIFAR-10 results for VGG16, with #Epoch = 100. Left: training losses. Right: test
accuracy.
B	Detailed Experimental Settings for Image Clas sification on
CIFAR- 1 0/CIFAR- 1 00
B.1	Basic Settings
As mentioned in the main paper, all models are trained with stochastic gradient descent (SGD),
no momentum, batch size 128 and weight decay wd = 0.0005. Furthermore, we perform a grid
search to choose the best hyperparameters of all schedulers, with a validation set created from 5, 000
20
Published as a conference paper at ICLR 2022
samples in the training set, i.e. one-tenth of the training set. The remaining 45, 000 samples are then
used for training the model. After obtaining hyperparameters with the best validation accuracy, we
train the model again with the full training set and test the trained model on test set, where 5 trials
of experiments are conducted. The mean and standard deviation of the test results are reported.
Here the grid search explores hyperparameters η0 ∈ {1.0, 0.6, 0.3, 0.2, 0.1} and ηmin ∈
{0.01, 0.001, 0.0001, 0, “UNRESTRICTED”}, where η0 denotes the initial learning rate and ηmin
stands for the learning rate of last iteration. “UNRESTRICTED” denotes the case where ηmin is
not set, which is useful for eigencurve, who can decide the learning rate curve without setting
ηmin . Given η0 and ηmin, we adjust all schedulers as follows. For inverse time decay, the hy-
perparameter γ is computed accordingly based on η0 and ηmin . For cosine decay, η0 and ηmin is
directly used, with no restart adopted. For general step decay, we search the interval number in
{3, 4, 5, blog T c, blogTc + 1} and decay	factor in {2, 5, 10}. For step decay proposed	in	Ge	et al.
(2019), the interval number is fixed to be	blog T c, along with a decay factor 2. For eigencurve,
two major modifications are made to make it more suitable for practical settings:
=_______________1/L______________=_________________no_________________________________
ηt - 1 + K Pi= ∆j2jT + 2-(t- ti-1)	1 + 1 Pj=1 ∆jβ- + β-(t- J).
Here we change 1/L to η0 so that it is possible to adjust the initial learning rate of eigencurve.
We also change the fixed constant 2 to a general constant β > 1, which is aimed at making the
learning rate curve smoother. The learning rate curve of eigencurve is then linearly scaled to
match the given ηmin.
Notice that the learning rate η0 can be larger than 1/L, while the loss still does not explode. There
are several explanations for this phenomenon. First, in basic non-smooth analysis of GD and SGD
with inverse time decay scheduler, the learning rate can be larger than 1/L if the gradient norm is
bounded (Shamir & Zhang, 2012). Second, deep learning has a non-convex loss landscape, espe-
cially when the parameter is far away from the optima. Hence it is common to use larger learning
rate at first. As long as the loss does not explode, it is okay. So we still include large learning rate
η0 in our grid search process.
B.2	Settings for EIGENCURVE
In addition, for our eigencurve scheduler, we use PyHessian (Yao et al., 2020) to generate Hes-
sian matrix’s eigenvalue distribution for all models. The whole process consists of three phases,
which are illustrated as follows.
1)	Training the model Almost all CNN models on CIFAR-10 have non-convex objectives, thus
the Hessian’s eigenvalue distributions are different for different parameters. Normally, we want the
this distribution to reflect the overall tendency of most parts of the training process. According to
the phenomenon demonstrated in Appendix E, figure A.11-A.17 of Yao et al. (2020), the eigenvalue
distribution of ResNet’s Hessian presents similar tendency after training 30 epochs, which suggests
that the Hessian’s eigenvalue distribution can be used after sufficient training.
In all CIFAR-10 experiments, we use the Hessian’s eigenvalue distribution of models after train-
ing 180 epochs. Since the goal here is to sufficiently train the model, not to obtain good perfor-
mance, common baseline settings are adopted for training. For all models used for eigenvalue
distribution estimation, we adopt SGD with momentum = 0.9, batch size 128, weight decay
wd = 0.0005 and initial learning rate 0.1. On top of that, we use step decay, which decays the
learning rate by a factor of 10 at epoch 80 and 120. All of them are default settings of the PyHessian
code (https://github.com/amirgholami/PyHessian/blob/master/training.
py, commit: f4c3f77).
ImageNet adopts a similar setting, with training epochs being 90, SGD with momentum = 0.9,
batch size 256, weight decay wd = 0.0001, inital learning rate 0.1 and step decay schedule decays
learning rate by a factor of 10 at epoch 30 and 60.
2)	Estimating Hessian matrix’s eigenvalue distribution for the trained model After obtaining
the checkpoint of a sufficiently trained model, we then run PyHessian to estimate the Hessian’s
21
Published as a conference paper at ICLR 2022
eigenvalue distribution for that checkpoint. The goal here is to obtain the Hessian’s eigenvalue
distribution with sufficient precision. To be more specific, the length of intervals around each esti-
mated eigenvalue. PyHessian estimates the eigenvalue spectral density (ESD) of a model’s Hessian,
in other words, the output is a list of eigenvalue intervals, along with the density of each interval,
where the whole density adds up to 1. Precision means the interval length here.
It is natural that the estimation precision is related to the complexity of the PyHessian algorithm, e.g.
the better precision it yields, the more time and space it consumes. More specifically, the algorithm
has a time complexity of O(N n2v d) and space complexity O(Bd + nvd), where d is the number of
model parameters, N is the number of samples used for estimating the ESD, B is the batch size and
nv is the iteration number of Stochastic Lanczos Quadrature used in PyHessian, which controls the
estimation precision (see Algorithm 1 of Yao et al. (2020)).
In our experiments, we use nv = 5000 for ResNet-18 and nv = 3000 for GoogLeNet/VGG16,
which gives an eigenvalue distribution estimation with precision around 10-5 to 10-4. N and B are
both set to 200 due to GPU memory constraint, i.e. we use one mini-batch to estimate the eigenvalue
distribution. It turns out that this one-batch estimation is good enough and yields similar results to
full dataset settings shown in Yao et al. (2020).
However, space complexity is still a bottleneck here. Due to the large number of nv and space
complexity O(Bd + nvd) of PyHessian, the value of d cannot be very large. In practice, with a
NVIDIA GeForce 2080 Ti GPU, which has around 11GB memory, the maximum acceptable pa-
rameter number d is around 200K - 400K . This implies that the model has to be compressed. In
our experiments, we reduce the number of channels by a factor of C for all models. For ResNet-18,
C = 16. For GoogLeNet, C = 4. For VGG16, C = 8. Notice that those compressed models are
only used for eigenvalue distribution estimation. In experiments of comparing different scheduling,
we still use the original model with no compression.
One may refer to https://github.com/opensource12345678/why_cosine_
works/tree/main/eigenvalue_distribution for generated eigenvalue distributions.
3)	Generating eigencurve scheduler with the estimated eigenvalue distribution After ob-
taining the eigenvalue distribution, we do a preprocessing before plug it into our eigencurve
scheduler.
First, we notice that there are negative eigenvalues in the final distribution. Theoretically, if the
parameter is right at the optimal point, no negative eigenvalues should exist for Hessian matrix.
Thus we conjecture that those negative eigenvalues are caused by the fact that the model is closed to
optima w*, but not exactly at that point. Furthermore, the estimation precision loss can be another
cause. In fact, most of those negative eigenvalues are small, e.g. 98.6% of those negative eigenvalues
lie in [-0.1, 0), and can be generally ignored without much loss. In our case, we set them to their
absolute values.
Second, for a given weight decay value wd, we need to take the implicit L2 regularization into
account, since it affects the Hessian matrix as well. Therefore, for all eigenvalues after the first step,
we add wd to them.
After preprocessing, we plug the eigenvalue distribution into our eigencurve scheduler and gen-
erates the exact form of eigencurve.
________________1/L________________=__________________no__________________
1 +1 Pj=1 ∆j 2j-1 + 2i-1 (t - ti-1) - 1 +1 Pj=I ∆jβjT + β- (t- ti-i)
For experiments with 100 epochs, we set β = 1.000005, so that the learning rate curve is much
smoother. For experiments with 10 epochs, we set β = 2.0. In our experiments, β serves as a fixed
constant, not hyperparameters. So no hyperparameter search is conducted on β. One can do that in
practice though, if computation resource allows.
22
Published as a conference paper at ICLR 2022
B.3	Compute Resource and Implementation Details
All the code for results in main paper can be found in https://github.com/
opensource12345678/why_cosine_works/tree/main, which is released under the
MIT license.
All experiments on CIFAR-10/CIFAR-100 are conducted on a single NVIDIA GeForce 2080
Ti GPU, where ResNet-18/GoogLeNet/VGG16 takes around 20mins/90mins/40mins to train 100
epochs, respectively. High-precision eigenvalue distribution estimation, e.g. nv ≥ 3000, requires
around 1-2 days to complete, but this is no longer necessary given the released results.
The ResNet-18 model is implemented in Tensorflow 2.0. We use tensorflow-gpu 2.3.1 in our code.
The GoogLeNet and VGG16 model is implemented in Pytorch, specifically, 1.7.0+cu101.
B.4	License of PyHessian
According to https://github.com/amirgholami/PyHessian/blob/master/
LICENSE, PyHessian (Yao et al., 2020) is released under the MIT License.
C Detailed Experimental Settings for Image Clas sification on
ImageNet
One may refer to https://www.image-net.org/download for specific terms of access for
ImageNet. The dataset can be downloaded from https://image-net.org/challenges/
LSVRC/2012/2012-downloads.php, with training set being “Training images (Task 1 & 2)”
and validation set being “Validation images (all tasks)”. Notice that registration and verification of
institute is required for successful download.
ResNet-18 experiments on ImageNet are conducted on two NVIDIA GeForce 2080 Ti GPUs with
data parallelism, while ResNet-50 experiments are conducted on 4 GPUs in a similar fashion. Both
models take around 2 days to train 90 epochs, about 20mins-30mins per epoch. Those ResNet
models on ImageNet are implemented in Pytorch, specifically, 1.7.0+cu101.
D Important Propositions and Lemmas
Proposition 3. Letting f(x) be a monotonically increasing function in the range [t0, t], then it holds
that
~
t-1	ft
f(k) ≤	f(x) dx.
k=t0	t0
(D.1)
TC C / ∖ ■	.	■ II 1	■	■ . 1	Γ ɪ 71 . 7	■ . 7 11 .1
If f(x) is monotonically decreasing in the range [t0, t], then it holds that
Z tt f(x)
t0
tt-1	tt
dx ≤ X f(k) ≤ X f(k) ≤
k=t0	k=t0
t
f(t0) + Z
t0
f(x) dx.
(D.2)
Lemma 2. Function f(x) = exp(-αx)x2 with 0 < α and x ∈ (0, 1] is monotone decreasing in
the range X ∈ (Ia, +∞) and monotone increasing in the range X ∈ [0, 2].
Proof. We can obtain the derivative of f(x) as
Nf(X) = x exp(-αx)(2 — ax).
Thus, it holds that Nf(X) ≥ 0 when x ∈ [0, ∙∣]. This implies that f (x) is monotone increasing when
x ∈ [0, 2]. Similarly, We can obtain that f (x) is monotone decreasing when X ∈ (2, +∞).	□
Lemma 3. It holds that
exp(-αX)X dX = -α-1(Xexp(-αX) + α-1 exp(-αX)).	(D.3)
23
Published as a conference paper at ICLR 2022
Proof.
exp(-αx)x dx = - α-1	x d exp(-αx) = -α-1(x exp(-αx) -	exp(-αx)dx)
= - α (x exp(-αx) + α exp(-αx)).
E Proof of Section 2
Proof of Proposition 1. By iteratively applying Eqn. (2.1), we can obtain that
E hλj (wt+1,j - w*,j) i ≤ 'Y(1 - ηi,jλj )2 ∙ λj (WIj- w*,j )2
i=1
tt
+ λ犷∙ X Y (Ifjλj)2η2,j
k=1 i=k+1
=λj (WIj - w*,j )2 + 2 X (k + 1)2
=—E)2 — + σ ∙ k=1 D2
=λj (WIj - w*,j )2 + t	02
=(t + 1)2	+(t + 1)2	.
Summing up each coordinate, we can obtain the result.
1
(k +1)2
Proof of Proposition 2. Let us denote σj2 = λj2σ2. By Eqn. (2.1), we can obtain that
E [λ7' (Wt+1,j - W*j )2]
t
≤πt=1(I- ηij λj )2 ∙ λj (WIj- W*j )2 +X Πi=k (1 - ηi,jλj )ηk2,jσj2
≤exp -2Xηi,jλj	∙ λj(W1,j
i=1
exp EX L⅛
k=1
t
-w*j )2 + Eeχp
k=1
t
-2Xηi,jλj	ηk2,jσj2
i=k
∙ λj(WIj- w*j)2 + Eeχp
k=1
≤ exp(2⅛ lnf L+μ
∖ μ ∖ L + μt
2λj
=(L + μ、k
IL + μt)
∙ λj (WIj- W*j)2 + X exp (2μj ln (L+μt)) ∙ (L +σμk)2
t
∙ λj (WIj- w* j )2 + E
k=1
(L + μk) (答-2
2λj
(L + μt) K
∙ σj
2λj
≤ (L+μ)小
L LL + μt J
2λj
=(L + μ、k
IL + μt)
∙ λj (WIj- w*,j)2 +
2λj
σj	(L + μt) b T
2λj — μ
2λj
(L + μt) K
σ2
(L + μt)2
∙ λj (WIj - W*j)2 + 2λ∙ - μ
1	2	σ2
L + μt σj +(L + μt)2 .
j L⅛卜姆
+
□
□
The third inequality is because function F(x) = 1/(L + μx) is monotone decreasing in the range
[1, ∞), and it holds that
t
X
i=k
1
------≥
L + μi —
Zt
i=k
1
L + μi
di
1ln
μ
(L + μt )
IL + μk )
24
Published as a conference paper at ICLR 2022
The last inequality is because function F(x) = (L + μx)2λj/μ-2 is monotone increasing in the
range [0, ∞), and it holds that
XX (L + μk)2μj-2
k=1
ft 一	.、2j_D..	一	、2j_0
≤ J	(L + μk) μ 2dk +(L + μt) μ
2λj	2λj
(L + μt) 丁 T — (L + μ) b T
/T	、壬—2
+ (L + μt) μ 2
1	2λj	2λj
< -......(L + μt) 丁 T + (L + μt) 丁-2.
2λj 一 μ
By μ ≤ λj, σj = λ2σ2, and summing UP from i = 1 to d, we can obtain the result.	□
F Preliminaries
Lemma 4. Let objective function f(x) be quadratic. Running SGD for T -steps starting from w0
and a learning rate sequence {ηt}tT=1, the final iterate wT+1 satisfies
E [(wτ +ι - w*)>H(WT+ι - w*)]
=E [(wo — w*)> ∙ PT ... PoHPo ... PT ∙ (wo — w*)]
T	(F.1)
+ X E [ηT n> ∙ PT ...Pτ+ιHPτ +1 ...Pt ∙ n ],
τ=o
where Pt = I - ηtH.
Proof. Reformulating Eqn. (1.5), we have
wt+1 - w* =wt - w* - ηt(H(ξ)wt - b(ξ))
=wt - w* - ηt(Hwt - b) + ηt (Hwt - b - (H(ξ)wt - b(ξ)))
=wt - w* - ηt(Hwt - b - (Hw* - b)) + ηt (Hwt - b - (H(ξ)wt - b(ξ)))
= (I - ηtH) (wt - w*) +ηtnt
=Pt(wt - w*) + ηtnt.
Thus, we can obtain that
t
wt+1 - w* = Pt . . . Po(wo - w*) +	Pt . . . Pτ+1ητnτ.	(F.2)
τ=o
We can decomPose above stochastic Process associated with SGD’s uPdate into two simPler Pro-
cesses as follows:
wtb+1 - w* =	Pt(wtb	- w*), and	wtv+1	-	w*	= Pt(wtv	- w*) + ηtnt,	with wov	=	w*,	(F.3)
which entails that
Pt. . .Po(wob - w*) = Po . . . Pt(wob - w*)	(F.4)
tt
Pt . . . Pτ+1ητnτ =	Pτ+1 . . . Ptητnτ	(F.5)
τ=o	τ=o
wtb+1 - w* + wtv+1 - w*	(F.6)
where the last equality in Eqn. (F.4) and Eqn. (F.5) is because the commutative ProPerty PtPt0 =
(I-ηtH)(I-ηt0H) = (I-ηt0H)(I-ηtH) =Pt0Ptholdsfor∀t,t0.
wtb+1 - w*
wtv+1 - w*
(F.2)
⇒ wt+1 - w*
25
Published as a conference paper at ICLR 2022
Thus, We have
E [(wτ +1 - w*)>H(WT +1 - w*)]
(=6)E [(wT +1 — w*)>H(WT +ι — w*) + 2(WT +ι — w*)>H(WT +ι — w*)
+ (wT +1 — W*)T H (WT +1 — w* )]
(型 E [(Wb — w*)>Pt ...P0HP0 ...Pt (Wb — w*) + 2(wT +1 - w*)>HPid ...Pt(w* — w*)
+ (wT +1 — W*)T H (WT +1 — W*)]
(=5)E [(wb — w*)>Pt ... P0HP0 ...Pt (Wb — w*)
+ 2 (X PT ...Pt +1ητnτ) HP0 ...Pt (Wb — W*)
+ (X PT ... PT+1ητ nj H (X PT ... PT+1ητnj
E[n==0E [(wb — w*)>Pτ ... P0HP0 ...Pt (w0 — w*)]
T
+ E E ητ ητ0∙n>Pτ ...Pt+1 ∙ H ∙ PT，+1 ...Pt n〃
T = 0,τ，= 0
=E [(wb — w*)>Pτ ... P0HP0 ... PT(Wb — w*)]
T
+ X E [ηT n> ∙ Pt ...Pt+1HPt +1 ...Pt ∙ n",
T =0
where the last equality is because when T and T0 are different, it holds that
E[n> ∙ Pt ... Pτ+ιHPτ +1 ...Pt ∙ nτ，] = 0
due to independence between nτ and nτ ， .
□
Lemma 5. Given the assumption that Eξ [ntn>] W σ2H, then the variance term satisfies that
T	d T	T
X E[ηTn>∙ Pt ...Pt+1HPt+1 ...Pt ∙ n" ≤ σ2 X λ X η2 Y (1 — ηiλj )2 ,	(F.7)
τ=0	j = 1	k=0	i=k+1
where Pt = I — η↑jH.
Proof. Denote AT , PT ... PT+1H 2, then
A> = (PT ...Pt+1H2 )> = (H1 )> PT+1...PT = H1 Pt +1 ...Pt ,	(F.8)
where the second equality is entailed by the fact that H 2, PT +1,...,Pt are symmetric matrices.
26
Published as a conference paper at ICLR 2022
Therefore, we have,
T
X E [ηTn> ∙ PT …PT+ιHPτ +1 ...Pt ∙ n ]
τ=0
(=8) X E [ηTn>AτA>nτ] = X ηTE [tr (n>AτA>nτ)] = X ηTE [tr (A>nn>Aτ)]
τ=0	τ=0
τ=0
TT
=X ηTtr (E [A> nτn> Aτ]) = X ηTtr (A> E [nτn> ] Aτ)
τ=0	τ=0
TT
≤σ2 ∙ X ηTtr (A>HAτ) = σ2 ∙ X η2tr (AτA>H)
τ=0	τ=0
T
=σ2 ∙ X ηTtr(Pτ ... Pτ+ιHPτ+1 …PT H)
τ=0
dT	T
=σ2Xλj2Xηk2 Y (1-ηiλj)2,
j=1	k=0	i=k+1
where the third and sixth equality come from the cyclic property of trace, while the first inequality
is because of the condition Eξ ntnt>	σ2H, where
∀x,	x> E[nτnτ>]x ≤ σ2x>Hx
⇒ ∀z,	z>Aτ>E[nτnτ>]Aτz = (Aτz)>E[nτnτ>](Aτz) ≤ σ2(Aτ z)>H(Aτ z) = σ2z>Aτ>HAτ z
⇒	Aτ>E[nτnτ>]Aτ	σ2Aτ>HAτ
⇒ tr (A> E[nτn>]Aτ) ≤ σ2tr (A>HAτ).
□
Lemma 6. Letting λj be the smallest positive eigenvalue of H, then the bias term satisfies that
E [(wo — w*)> ∙ PT ... Po HPo ...Pt ∙ (wo — w"
≤(wo — w*)>H(wo — w*) ∙ exp -2λ^E ηk .	(F.9)
k=o
Proof. Letting H = UΛU> be the spectral decomposition of H and uj be j-th column of U, we
can obtain that
E [(wo — w*)> ∙ PT ... PoHPo ... PT ∙ (wo — w*)]
dT
=X λj ∙(U>(WO -w*))2 ∙ Y(1 - ηkλj)2
dT
≤ £，j ∙ (u>(wo — w*))2 ∙ exp I —2λj Enk .
j=1	k=o
Since λj is the smallest positive eigenvalue of H, it holds that
d	Td	T
E λj ∙ (u>(wo —	w*))2	∙ exp I —2λj ɪ2 nk	≤ £1j	∙ (u>(wo — w*))2 ∙ exp	I —2λj Enk
j=1	k=o	j=1	k=o
=(wo — w*)>H (wo — w*) ∙ exp I —2λj Enj
k=o
□
27
Published as a conference paper at ICLR 2022
G Proof of Theorems
Lemma 7. Let learning rate ηt is defined in Eqn. (3.3). Assuming k ∈ [t”—i,t”] with 1 ≤ W ≤ i ≤
T, the SeqUenCe {ηt}T=0 satisfies that
ti+i-1	i+1 ι	ι
∑v~1	Qi	1	Qi/	一 一、
ηt ≥ /	——：— ln--+	—：— In----—：——--------,	(G.1)
一∙ 2⅛i 2i-1μ	Qi—1	2i — 1 μ	Qi--1 + 2i-1μ(k — ti--i)
t=k	i=i-十1
where Qi is defined as
i
Qi，L + μ X ∆j2j-1
j=1
1
ηti
(G.2)
Proof. First, we divide learning rates into two groups: those who are guaranteed to cover a full
interval and those who may not.
ti+1-1
X ηt
t=k
ti+1-1	ti-- 1
ηt +	ηt
t=ti-	t=k
i+1	ti —1	ti--1
X X ηt + X ηt
i=i-+1 t=ti-ι	t=k
Furthermore, because ηt is monotonically decreasing with respect to t, by Proposition 3, We have
ti+1-1
X ηt
t=k
Ll	.	.
(D.2) i+1	产	ft
≥ T /	ηtdt + /	ηtdt
i=i-十1 4-1	Jk
i+1	fti	1
(% X ------------------1--------1----------------dt
i=i+1 Jti-IL + μ £屋 ∆j2j-1 + 2i-1μ(t — ti—1)
ti	1
+ /	-----------—；------------------------dt
Jk	L + μ Ej=II ∆j2j-1 + 2i--1μ(t — ti--1)
⅛1	1	L + μ Pj=1 δj 2j-1 + 2i-1μ(ti — ti-1)
In
i=⅛12i-1μ	L+μ PjCI ∆ 2j-1
1	L + μ Pj=IL δ?2j-1 + 2i -1μ(ti- — ti--1)
+ .z 1 In-----------⅛-i--------------------------
2i -1μ	L + μ Pj=-II ∆j2j-1 + 2i--1μ(k — ti0 - 1)
X 1	lɪɪ L + μ Pj=I ∆j2j-1
i=T+1 E n L + μ Pj=1 ∆j2j-1
+	1 In_________L + μ Pj=1 δ2j-1_____________
2i0-1μ n L + μ Pj=1 ∆j2j-1 + 2i --1μ(k ——)
〜
i+1	1	1
Σ1	Qil 1	1	Qi-
Tr~i- In------+ --T-~~：— In------T-~~：--；------7
i=i,+1 2i-1μ	Qi-1	2i-1μ	Qi--1 + 2i-1μ(k — t--1)
□
Lemma 8. Letting SeqUenCe {Qi} be defined in Eqn. (G.2), g^ven 1 ≤ i, it holds that
∑ π
i+1
i+1
i=1
j=i+1
i+1
i+1
t ,	ti-1	U
Q-2'叶2 X(Qi-1 + 2i-1μ(k — ti-。)2'
k=ti-1
(G.3)
1
≤2 ∙ —
—2i+1μ
£ ∏
i=1
j=i+1
2i-i + 2-J	2i-i+2-1、	-?i-i+2
Qi	- - Qi-1	- Q-
28
Published as a conference paper at ICLR 2022
U
Proof. Notice that g(k) := (ai-1 + 2i-1μ(k - ti-1))2	-2 is a monotonically increasing func-
tion, We have,
~
i+1
X
i=1
7, -l
(D.1) i+1
≤
i=1
~
i+1
X
i=1
~
i+1
X
i=1
~
i+1
≤ X
i=1
~
i+1
X
i=1
j=i+1
j=i+1
j=i+1
~
i+1
∏
j=i+1
~
i+1
∏
j=i+1
~
i+1
∏
j=i+1
~
i+1
~
i+1
~
i+1
α.-
ɑi
ɑi
—25-i+2
-25-i+2
ti-1	:
E (αi-1 + 2i-1μ(k - ti-1))2i
k = ti- 1
J (ai-1 + 2i-1μ(t - ti-1))2i——i+2-2dt
Jti——1
((2i-i+2 - 1) ∙ 2i-14)-1 (α2i——i+2T - α⅛Γ2-1
1
~
(2i+1 - 2i-1)μ
2i ——i+2	1
ai	-
2⅛-i+2-1
-ai-1
〜
1
~ ~
(2i+1 - 2i)μ
^r

2i——i+2	1
αi	-
—
2⅛-i+2-1
ai-1	-
1
1
2i+14 1 - 2
2i —— i+：
ai
2-1
2⅛-i+2-1
-ai-1
1
~
i+1
〜
~
i+1
=2 ∙-----
2i+1μ
i+1	i+1	/	∖
X	∏ j
i=1 j=i+1 ∖ j ×
i=1
〜
j=i+1
-25——i+2 (	2 — i+2-1
α-	v2-i	-
25—— i + 2 - 1
-ai-1
=2 ∙ 2⅛
i+1
〜
i+1
i+1	i+1	/	\
X	∏汩
i=1	j=i+1 ' j 7
i=1
j=i+1
2i——j+2
2% ——i+2 - 1	2% ——i+2 - 1 ∖	- 2——i+2
ai2	-1 - ai2-1	-1	ai-2
□
Lemma 9. Letting {ɑi} be a positive sequence, given 1 ≤ i, it holds that
〜
i+1
〜
i+1
i+1 i+1	/	\
X ∏1罟
i=1 j = i+1
i=1
j=i+1
Proof. First, We have
~
i+1
X
i=1
~
i+1
X
i=1
~
i+1
X
i=1
α-11
i
+ X
i=1
~
i+1
∏
j=i+1
~
i+1
∏
j=i+1
~
i+1
∏
j=i+1
—
—
i=1
a—1
a—1
i+1
∏
j=i+1
~
i+1
2⅛-i+2-1
αi -
-1
i
2⅛-i+2-1	2"i+2-1、	-2⅛-i+2	-]
αi -— αi-1 一 a ai	≤ αi+1∙
(G.4)

∑ π
j=i+1
-0i-1
a2-7+2-1
i->i-i+2
ai-1
a2i-i+2
~
i+1
~
i+1
-X
i=1
α*i+2
i+1
j=i+1
2⅛-i+2-1
αi-1	-
a2i-i+2
29
Published as a conference paper at ICLR 2022
i	i+1	fa '∖2i-j+2	_1
Furthermore, We reformulate the term Ei=I ∏j∑-+1 ( α0-1)	α-1 as follows
i=1
j=i+1
~
i+1
α-1
i
X
i=1
~
i+1
i
X
i=1
α-1
ς>i-W〃 + 2_i
αi00-1
2i-i" + 2
MP
i00=i+1
~
i+1
~
i+1
X
i=2
j=i+2
j=i+2
j=i00+1
~
i+1
π
j=i+1
~
i+1
~
i+1
谭-+2
2^-+2-1
i-1
(G.5)
(G.6)
(G.7)
(G.8)
i
∑ π
∑ π
i00=2

Combining above results, we can obtain that
~
i+1
X
i=1
αi+11
~
i+1
π
j=i+1
~
i+1
i=2
~
i+1
2i-j+2
2i-j+2
Qi
?	-∙ ，C	?	-∙ , C
2i — 2+2	1	2 i — 2+2	1
-αi-1
-2i-i+2
Q-
2i-j+2
2⅛-i+2-1
αi-1	-
oi-i+2
αi
~
i+1
-X
i=1
~
i+1
j=i+1
j=i+1
2
2^-i+2-1
Qi-I	-
oi-i+2
Q2
~
2i+1-1
ɑ0
2计1
Qj
+
∑ π
□
Lemma 10. Letting US denote vt+1,j = Pk=0 η2 "；=左十1 (1 一 ηiλj )2 With @ defined in Eqn. (3.3),
for 1 ≤ t ≤ t0, it holds that
vto,j ≤ max(%,j, ηt∕λj).	(G.9)
Proof. If vt+1,j ≤ max(vt,j,ηt∕λj) holds for ∀t ≥ 1, then it naturally follows that
vt,j
/	/	ηt0-1
≤ max vtf-1,j, —τ—
∖	λj
≤ max vt-2,j,
ηt-2 ηt-1
λj
)
≤...
ηt
≤ max (Vtj,—,.
ηt-2 ηt0-1∖
K F)
ηt
max vt,j, ■—
∖	λj
where the last equality is entailed by the fact that t ≤ t0 and ηt defined in Eqn. (3.3) is monotonically
decreasing. We then prove vt+1,j ≤ max(vt,j ,ηt∕λj) holds for ∀t ≥ 1.
30
Published as a conference paper at ICLR 2022
For ∀t ≥ 1, we have
tt
vt+1,j=Xηk2 Y (1 - ηiλj)2
k=0 i=k+1
t-1	t
=ηt2+Xηk2 Y (1-ηiλj)2
k=0	i=k+1
t-1	t-1
=ηt2+(1-ηtλj)2Xηk2 Y (1 - ηiλj)2
k=0	i=k+1
=ηt2 + (1 - ηtλj)2vt,j
(G.10)
1)	If vt+1,j ≤ vt,j, then it naturally follows vt+1,j ≤ max(vt,j, ηt /λj).
2)	If vt+1,j > vt,j, denote a , (1 - ηtλj)2, b , ηt2, we have vt+1,j = avt,j + b, where a ∈ [0, 1)
and b ≥ 0. It follows,
⇒
⇒
⇒
vt+1,j > vt,j
avt,j + b > vt,j
b
vt,j < 1-Σ
vt+ι,j = avt,j + b<a ∙
b
1- a
+b
b
1-a
Therefore,
vt+ι,j < 上=，褚、、2 <	/2、、=四 ≤ max (vt,j,等),
1 - a I-(I- ηtλj)2	I-(I- ηtλj)	λj	1	λj)
where the second inequality is entailed by the fact that 1- ηtλj ∈ [0, 1).
□
Lemma 11. Letting vt,j be defined as Lemma 10 and index i satisfy λj ∈ [μ ∙ 2i,μ ∙ 2i+1), then
vi+ι j has the following property
ηt-^
vti+ι,j≤ 15 ∙ M+1.	(G.11)
λj
Proof. By the fact that (1 - x) ≤ exp(-x), we have
vt+1,j ≤ ∑ exp -2 ∑ ηt0 λj ηk2 .
k=0	t0=k+1
Setting t = ti+ι - 1 in above equation, We have
ti+1-1	/	ti+1-1	ʌ
vti+ι,j ≤ X eχp I -2 X ηt0λj I η2.	(G.12)
k=0	t0=k+1
31
Published as a conference paper at ICLR 2022
Now we bound the variance term. First, we have
ti+1-1	/	ti+1-1	∖	ti+1-1	/	ti+1-1	ʌ
Σ exp I —2∑ ηt% η2 =Eexp I —2 Σ ηtλj exp(2ηk λj )η2
k=0	∖	t=k+1	)	k=0	∖ t=k	)
ti+1-1	(	ti+1-1	ʌ ʌ	(2λj∖	2
≤工 exp —2 Σ ηtλj I eχp ( ~Lj) η
k=0	∖ t=k	)	'	，
ti+1-1	/	ti+1-1	ʌ
≤ eχp⑵∙ X eχp I -2 X ηtλj I ηk,
k=0	∖ t=k	)
where the first inequality is because ηk ≤ 1/L. Hence, we can obtain
ti+1-1	/	ti+1-1	ʌ
X exp -2 X η内 η2
k=0	∖	t=k + 1	J
ti+1-1	/	ti+1-1	ʌ
≤ exp(2) ∙ X exp —2 X ηtλj ηk
k=0	∖ t=k	)
i+1 ti-1	/	ti+1-1	∖
= exp⑵∙ X X exp I -2 X ηtλj I η2∙
i=1 k=ti-1	∖ t=k	)
Furthermore, combining with Eqn.(G.1) and the condition λj ∈ [μ ∙ 2i, μ ∙ 2i+1), we can obtain
~
i+1 ti-1
X X exp
i=1 k=ti-1
〜 . ^ .
i+1 ti-1
(GI)V^
≤	exp
i=1 k=ti-1
〜
i+1 ti-1
= X X exp
ti+1-1
~
i+1 ti-1
-2 E ηt% ηk ≤∑ E exp -2
t=k
i=1 k=ti-1
ti+1-1	∖
X ηtμ ∙ 2i I ηk
t=k
~
i+1
-2 X 2i-j+1 ln	- 2 ∙ 2i-i+1 ln
j=+1	QjT
Qi
αi-1 + 2i-1 μ(k - ti-1)
ηk2
i=1 k=ti-1
〜
i+1 ti-1
XX
i=1 k=ti-1
i+1
X 2i-j+2 ln F +2i-i+2 ln
j=i+1	Qj
Qi-1 + 2i 1μ(k — ti-1) ∖ η2
Qi
~
i+1
ki-j+2
2i-j+2
.	ki-i+2
Qi-1 + 2i 1μ(k — ti-1) λ	^k
Qi	η	k
~
i+1
X
i=1
~
i+1
(=3) ^X
i=1
~
i+1
∏
j=i+1
~
i+1
∏
j=i+1
j=i+1
2
IXX (Qi-I + 2i-1μ(k - ti-1))? η2
k=ti-1
ti-1
∙ X (
k=ti-1
~ .
Qi-I + 2i-1μ(k - ti-1)
ki-i+2
Qi
L + μ X ∆j 2j-1 +2i-1μ(k - ti-1)I
~
i+1
~
i+1
i+1 i+1	/	∖
(=2) X ∏	"
i= 1	j=i+1 ' j /
i=1
j=i+1
〜
Qi
XX ( Qi-1 + 2i-1μ(k — ti-1)
k£11 I	Qi
32
Published as a conference paper at ICLR 2022
~
i+1
~
i+1
i=1
j=i+1
2i-j+2
—2i-i+2
α-
∑ π
ti—1
∙ ^X (ai-1 + 2i 1μ(k - ti-1))2	(ai-1 +2i 1μ(k - ti-1))
k=ti-1
~
i+1
~
i+1
i=1
(G.3)
≤ 2 ∙
(G.4)
≤ 2 ∙
j=i+1
T .
2i+1μ
1
~ .
2i+1μ
i 2i-j +
αj-1 ∖
ɑj )
i+1
i+1
t	ti — 1	~
a—2i-i+2 X 3-1 +2-1μ(k - ti-1))2i-i+2-2
k=ti-1
i+1	i+1	/	∖
- X	∏罟
i=1	j=i+1 ∖ j ×
2、——i+2 — 1	2 i — i+2 — 1
αi	- ai—1
—2i-i+2
α-
i=1
j=i+1
∙ α-11 < 2 ∙ *
i+1 一 λj
∑ π
1
〜
〜
where the last inequality is because of the condition λj ∈ [μ ∙ 2i,μ ∙ 2i+1) and the definition of αi.
Therefore, we have
%1 < 15 ∙叽
vti+ι,j < 2eχp(2) ∙
λj
λj
Lemma 12. Let objective function f (x) be quadratic and Assumption (1.7) hold. Running SGD
for T-steps Startingfrom wo and a learning rate SeqUenCe {ηt}T=1 defined in Eqn. (3.3), the final
iterate WT +1 satisfies
E [(wτ +1 - w* )> H (WT +ι - w*)] <(wo - w*)> H (wo - w*) ∙ exp I -2μ^ηk
∖	k=0
ImaX — 1
+ 15σ2μ E
~
i=0
~
2i+1si
Proof. The target of this lemma is to obtain the explicit form to bound the variance term. By the
definition of vt+1,j in Lemma 10, we can obtain that
T
X E[η">∙ Pt …PT +1HPτ +1 ...Pt ∙ nτ]
T=0
(F.7) 2 ʌ、2
< σ 工 j VT +1,j
j=1
(G.9) 2 ʌ、2	(
<σ 2^ m ∙maχ vti+1+1,j,
j=ι	'
(G.11) 2 4 2	(
< σ2 ɪj λj ∙ max ( 15 ∙
ηti+1 + 1
λj
j=i
d
=15σ2 X λj ∙ ηti
ηti+ι + 1	ηti+ι + 1
λj	， λj
i+1+1 < 15σ
d	ImaX - 1
2 X λj∙ηti+ι < 15σ2μ X 2i+1Si∙ηti+ι,
j=1	i=0
where the last inequality is because λj
By Eqn. (3.3), We have
∈ [2iμ, 2i+1μ) and there are Si such λj S lie in this range.
化+1
(G.13)
7 I 1
L + μ Pj=1 ∆j 2j —1
1
7 I ι
L + μ Pj=1 △2-1
□
33
Published as a conference paper at ICLR 2022
Therefore, We have
T
XE [η2n> ∙ Pt …PT+1HPT+1 ∙∙∙Pt ∙ n"
T = 0
ImaX - 1
15σ2μ X
~
i=0
~
2+1S
7 I ι
L + μ PM ∆j2j-1
(G.14)
≤
Combining with Lemma 4 and Lemma 6, we can obtain that
E [(wτ +1 - w*)>H(WT +1 - w*)] ≤(w0 - w*)TH(wo - w*) ∙ exp I -2μ^ηk
∖	k=0
ImaX - 1
+15σ2μ E
~
i=0
~
2i+1si
7 I 1
L + μ Pj=I ∆j2j-1
□
Lemma 13. For Vt ≥ 0, the learning rate SeqUenCe {ηt }T=1 defined in Eqn. (3.3) satisfies
1
ηt ≤ L + μt
(G.15)
Proof. For Vt ≥ 0, there ∃i ≥ 1, where t ∈ [ti-1,ti). Given the form defined in Eqn. (3.3), we
have,
1
t =T	- 1 ~：~~~^^:	~^^:~~二	^
L + μ Pj=1 ∆j 2j- + 2i-1μ(t — t”1)
V	1
L + μ Pj=I δ? + μ(t - ti-1)
(")________________1________________
L + μ Pj=1(tj-tj-1) + μ(t — ti-1)
_	1
L + μ(ti-1 — t0) + μ(t — ti-1)
_	1
L + μ(t - t0 )
_	1
L + μt
□
Lemma 14. Let objective function f (x) be quadratic and Assumption (1.7) hold. Running SGD
for T-steps StartingfrOm W0 and a learning rate sequence {ηt}T=1 defined in Eqn. (3.3), the final
iterate WT +1 satisfies
E [(wτ +1 - w*)>H(wτ+1 - w*)]
/	、丁 rr∕	、 K2
≤(w0 - w*)>H(w0 - w*) ∙ δ
Imax - 1
+ 15σ2 μ X
~
i=0
~
2i+1S
L + μ Pij+=11 ∆j 2j -1
Proof. The target of this lemma is to obtain the explicit form to bound the bias term.
34
Published as a conference paper at ICLR 2022
First, by Eqn. (G.1) and the condition λj ∈ [μ ∙ 2i,μ ∙ 2i+1), we have
ti+1-1
exp I -2λjEnk) ≤ exp -2 E ηk λj ≤ exp -2
~
i+1
k=0
k=0
i=1
1	1	ai ʌ
—：—：—ln-----λj
2i-1μ	αi-i
i+1
≤ exp I - X 2i-i+2 ln -^i-
∖ i=1	0iτ
i+1 /	、2i-i+2
Y (?)
(G.16)
T
For λj = μ, since μ ∈ [μ ∙ 2i,μ ∙ 2i+1) for i = 0, it follows,
T G ∖	2	2 (G∙15)
exp -2μ Σ nk I ≤ l ∙ % ≤
k=0
L + 〃∆ι
J Y些(Y≤
κ2
N
(G.17)
L + μtι
2
Combining with Lemma 12, We obtain that,
E [(wt +ι - w*)tH(wt+ι - w*)] ≤(w0 - w*)tH(w0 -
Imax - 1
+ 15σ2 μ X
~
i=0
、κ2
W*) ∙ ∆
~
2i+1S
L + μ Pj=I ∆j2j-1
□
G.1 Proof OF Lemma 1
Lemma 1. Let objective function f (x) be quadratic and Assumption (1.7) hold. Running SGD for
T-steps Startingfrom w0 and a learning rate sequence {ηt}T=1 defined in Eqn. (3.3), the final iterate
WT +ι satisfies
一	一	κ2	15 C
E [f (wT +1) - f (w*)] ≤(f (w0)- f (w*)) ∙ ʌɪ + ɪ ∙ σ μ
ImaX - 1
X
~
i=0
~
2i+1S
7 i 1	.
L + μ Pj=I ∆j2j-1
(3.6)
Proof. For ∀t ≥ 0, we have
f(wt)- f(w*) c=⅛ 2WTH(ξ)wt - b(ξ)>wt -
=(2WTE[H(8]必一E[b(ξ)]τ
-E
wTE[H(ξ)]w* - E[b®]Tw*
Wt
=(；WT Hwt — bτwt
(=) ɑWTHwt — bTWt
=(J WTHWt - b Wt
WT Hw* — b，w*
b - bτHTb
b> H-1b - bτH-1b
=LWTHWt - bτwt + LbTH-1b
=2WTHWt- 1 b>wt - 1 b>wt+1 bTHTb
35
Published as a conference paper at ICLR 2022
=2 w> Hwt — 1 w> b — 1 b> wt + 1 b>H Tb
=2 w> Hwt — 1 w> Hw* — 1 w> Hwt + 1 w> Hw*
= 2(wt — w*)>H (wt — w*),
where the 5th equality is entailed by the fact that H> = H is a symmetric matrix, and the 9th
equality uses both H> = H and Eqn 1.4.
Combine the above result with Lemma 14, we obtain that
E [f (WT +1) — f (W*)] ≤(f(w0) — f (WJ) ∙ ∆2 + V
∆1	2
Imax -1
K X
~
i=0
~
2i+1si
7 I ι
L + μ Pj∙=l ∆j2j-1
□
G.2 Proof of Theorem 1
Theorem 1. Let objective function f(x) be quadratic and Assumption (1.7) hold. Running SGD for
T -steps starting from w0, a learning rate sequence {ηt }tT=1 defined in Eqn. (3.3) and ∆i defined in
Eqn. (3.5), the final iterate wT+1 satisfies
2
κ2
E [f (WT +1)- f (W* )] ≤f (WO)- f (WJ) • 一
Imax-1
i=0
Imax -1
i=0
s0T2
. σ2.
T
Proof. We have
Imax -1
μ ∙ X
~
i=0
~
2'+1Si
Imax -1
<μ ∙ X
~
i=0
~
2i+1si
μ2 δI+1
(3=.5) 2
Imax -1
X
~
i=0
Si
—,√si-----T
PImaxT √s
Imax -1
Imax -1
T ∙ ∑ √si ∙ ∑	√si =
■ C	~
i=0
~
i=0
PImax -1
2 i=0
T~
2
Combining with Lemma 1 and the definition of ∆1 , we can obtain that
2	Imax -1
i=0
E [f (wt +ι) — f (w*)] ≤(f (wo) — f (w*))--------------k
s0 T2
Imax -1
i=0
. σ2.
G.3 Proof of Corollary 2
Corollary 2. Given the same setting as in Theorem 1, when Hessian H’s eigenvalue distribution
p(λ) satisfies “power law”, i.e.
p(λ) = ɪ ∙ exp(—a(ln(X) — ln(μ))) = ɪ ∙ (f) °	(3.7)
Z	Zλ
T
2
□
for some α > 1, where Z
such that the final iterate WT+1 satisfies
RμL(μ∕λ~)αdλ, there exists
a constant C(α) which only depends on α,
κ2	dσ2
E [f (WT +1) — f (W*)] ≤ ((f (WO) — f (W*)) ∙ T2 + ~T~ J ∙ C(a).
36
Published as a conference paper at ICLR 2022
Proof. According to Theorem 1,
e [f (wτ +1)- f (wJ]
κ2 ∙ (pI=ax—1 M 15
≤(f (W0) - f (w*))———诃——L + 一
SoT 2
(PI靠 T √Si)2
. σ2
T
=(f(w0) - K) ∙ T ∙ ⅛0X-+字∙ 15(pI=0d-i 局2
The key terms here are Ci，(PI=OXT √i) /s0 and C2，15 (PI=厂1 √i) /d. As long as
we can bound both terms with a constant C(α), the corollary will be directly proved.
1)Ifκ < 2, then there is only one interval with s° = d. By setting C(α) = max(C1, C2) = 15, this
completes the proof.
2) If κ ≥ 2, then bounding Ci and C2 be done by computing the value of Si under power law. For
all interval i except the last interval, we have,
/.μ-2i+1
S 吧#N ∈ [μ ∙ 2i,μ ∙ 2i+1)=	p(λ)dλ
d	√μ-2i
κ1-a - 1
L1-α - μ1-α
= 2i(1-α) ∙ 2i - 1
κ1-α - 1
μ∙2i+1
λ-adλ
∙2i
Therefore, we have
9I —a _ 1
d ∙ LI ∙9
Si
d ∙2i(1—a) ∙:
(G.18)
holds for all interval i except the last interval i0
Imax - 1 = log2 K - 1 > 0. ThiS last interval
may not completely covers [μ ∙ 2i',μ ∙ 2i0+1) due to the boundary truncated by L, but We still have
SiY d ^ 二 ^ …
37
Published as a conference paper at ICLR 2022
It follows,
ImaX - 1
X
.i=0
o1-a 1 /Imaχ-1 _∖
T∙( X L)
Thus,
Ci
=d ∙
‹d ∙
=d ∙
2
21-a	1 (ImaX-I / 1 ∖ i(α-1)∕2∖
；1 i=O ⑴ J
L ∙ (X(2 )i12=d ∙
21-α - 1	(	1
κ1 α - 1	∖ 1 - 2(α-i)∕2
(^PImaX - 1
i=o=o
d 21-α-1
d ^ K1-α-1
So
d∙
2-α-1
,ι-α-1
∙ 20(1-a)
15 (pI=ax-1
1 - (1)
=15------应■
1 - (1)
a-1
21-α
κ1-α
2
α-1	，1 - 2(1-α)/2
d ∙
2
2
1
1-a - 1
1-α - 1	∖ 1 - 2(1-α)/2
1
1 - 2(1-α)∕2
-1	∖ 1 - 2(1-α”2
2
√si	≤d ∙
2 = d ∙ 2
K
2
1
2
2
<
2
κ
d
< 15 ∙ d ∙
d
-1
1
2
1
<15 ∙ (1- 2(1-α)∕2
2
Here the last inequality for C2 is entailed by K ≥ 2 and α > 1.
By setting C(α) = max(C1,C2) = 15 ∙
2
, we obtain
1
e [f (wT +1) - f (w*)]
一,、S 、、κ2	(∑I≡ax-1 √si)2	dσ2 15 (EI=OXT
<(f(WO)- f(W*)) ∙ T2-------so------- + 亍------------d~
κ2	dσ2
=(f (WO) - f (w*)) ∙ t2 ∙ C1 + Tp- ∙ C2
/	κ2	dσ2∖
< ((f (WO) - f (W*)) ∙ t2 + ~T~ )，C(a).
2
□
G.4 Proof OF Theorem 4
Theorem 4. Let objective function f (x) be quadratic. We run SGDfor T-steps StartingfrOm wo and
a step decay learning rate SeqUenCe {ηt}T=1 defined in Algorithm 1 of Ge et al. (2019) with η1 <
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ [ntn>]=
σ2H and (3) λj (WOj — w* ,j )2 = 0 for Vj = 1, 2,..., d, the final iterate WT +1 satisfies,
一	一	/ dσ2
E[f(wτ +1)-于(w*)] = ω (~r~ ∙ logT
38
Published as a conference paper at ICLR 2022
Proof. The lower bound here is an asymptotic bound. Specifically, we require
1 ≥m ≥ max I 216,16, —ɪ- ∙
log T -	∖	,	, 256
σ2
minj λj (w0,j - w*,j)
(G.19)
In Ge et al. (2019), step decay has following learning rate sequence:
ηt
η1
2'
TT
ift ∈ [1 +西∙', iogy ∙ ('+1)_,
(G.20)
where ` = 0, 1, . . . , log T - 1. Notice that the index start from t = 1 instead of t = 0. For
consistency with our framework, we set η0 = 0, which produces the exact same step decay scheduler
while only adding one extra iteration, thus does not affect the overall asymptotic bound.
We first translate the general notations to diagonal cases so that the idea of the proof can be clearer.
Since f(x) is quadratic, according to the proof of Lemma 1 in Appendix G.1,
f(wτ +1) - f (w*) =I(WT+1 - w*)>H(WT +1 - w*).
Furthermore, according to Lemma 4, where Pt = I - ηt H,
E [(wt +ι - w*)>H(WT +ι - w*)]
=E [(wo - w*)> ∙ Pt ... P0HP0 ...PT ∙ (wo - w*)]
T
+ X E [ηT n> ∙ PT ...Pτ+ιHPτ +1 ...PT ∙ nτ ]
τ=0
d	T	T dT
=Xλj (w0,j -w*,j)2 Y(1 - ηkλj)2 + X ητ2 X Y λj(1 - ηkλj)2E [nτ2,j ]
j=1	k=0	τ=0	j=1 k=τ+1
d	T	T dT
=Xλj (w0,j -w*,j)2 Y(1-nkλj)2 + XηTX Y λj(1-nkλj)2 ∙j
j=1	k=0	τ=0	j=1 k=τ+1
d	T	dT	T
=Xλj(w0,j -w*,j)2Y(1-ηkλj)2+σ2Xλj2Xητ2 Y (1-ηkλj)2.
j=1	k=0	j=1	τ=0	k=τ +1
Here the second equality is entailed by the fact that H and Pt are diagonal, and the third equality
comes from Eξ [ntnt>] = σ2H. Thus, by denoting bj , λj (w0,j - w*,j )2 QkT=0 (1 - ηkλj )2 and
vj , PτT=0ητ2QkT=τ+1(1 -ηkλj)2,wehave,
E [f (wτ +1 - f (w*)] = 2E [(wτ +1 - w*)>H(WT +1 - w*)]
d ∖	( d ∖↑	(G.21)
X bj I + (σ2 X λ2vj) I.
To proceed the analysis, we divide all eigenvalues {λj } into two groups:
A={j∣λj > 舄}, B={j∣λj ≤ 8ogT},	(G22)
where group A are those large eigenvalues that the variance term vj will finally dominate, and group
B are those small eigenvalues that the bias term bj will finally dominate. Rigorously speaking,
a) For ∀j ∈ A:
Step decay’s bottleneck in variance term actually occurs at the first interval ` that satisfies
`	8T
2 ≥ λj η1 ∙百
(G.23)
39
Published as a conference paper at ICLR 2022
We first show that interval ` is well-defined for any dimension j ∈ A. Since j ∈ A, it follows from
the definition of A in Eqn. (G.22),
λj
log T
8η1T
λj ηι ∙
8T
log T
1 = 20
On the other hand, since we assume T/logT ≥ 216 in Eqn. (G.19), which implies T ≥ 216 ⇒
log T ≥ 16, it follows
8T	T
λjηι ∙ IogT ≤ λjηι ∙ & ≤
j ∙ I ≤ T = 2log TT
where the second inequality comes from η1 ≤ 1/L in assumption (1), and the third inequality is
entailed by λj ≤ L given the definition of Lin Eqn. (1.6).
As a result, we have
λ∙ηι∙ loTT ∈(20, 2lθgT-1]
thus
`	8T
2 ≥ λj η1 ∙百
will guaranteed be satisified for some interval ` = 1, . . . , logT - 1. Since interval ` is the first
interval satisifies Eqn. (G.23), we also have
2'-1 < λj η1 ∙ loTT
`	16T
2 < λj η1 ∙西
(G.24)
Back to our analysis for the lower bound, by focusing on the variance produced by interval ` only,
we have,
T	τ	('+1)∙ ToTT	T
vj =Xητ2 Y (1 - ηkλj)2 ≥ X	ητ2 Y (1 - ηkλj)2
T=0	k=τ +1	T ='∙ ioTT + 1	k=τ +1
('+1)∙ ToTT
ηT2	(1 - ηkλj)2
T ='∙ ToTT + 1	k='∙ ToTT
+1
('+1)∙ ToTT	2 T
X 啰)Y (1-ηk λj )2
T ='∙ ToTT + 1
k='∙ ToTT + 1
T
log T
T
Y (1-ηkλj)2
k='∙ ToTT + 1
(G.24) T
> logT
λj η1 ∙
η1
T
Y (1-ηkλj)2
k='∙ ToTT + 1
1
256
log T
T
1
λ2
T
Y (1 - ηkλj)2
k='∙ IoTT + 1
≥ L
一256
log T
T
1
λ2
T
1 -	2ηkλj
k='∙ ToTT + 1
1
256
log T 1
丁 ∙ λ2
1 - 2λj
T
X	ηk
k='∙ IoTT + 1
1
256
log T
T
1
λ2
log T-1 (i+1)∙ IoTT
1 - 2λj	X X	ηk
i='	k=i∙ toTt + 1
≥
2
T
/
∖
/
∖
2
/
∖
40
Published as a conference paper at ICLR 2022
_ 1 log T 1
=256 ∙ T ∙ λj2
_ 1 log T 1
=256 ∙ T ∙ λ2
J IogT ɪ
≥ 256 ∙ T ∙ X
(G.23) 1 ∣og T 1
≥ 256 ∙ T ∙ λ2
_ 1 log T 1
=256 ∙ T ∙ λ2
_ 1 log T 1
= 512 ∙ T ∙ λ2
l	log T-1 (i+1)∙ loTT
1 - 2λj	XX	ηi
∖	i='	k=i∙ IoTT+1
2
Here the first inequality is obtained by focusing variance generated in interval ` only. The second
inequality utilizes T ≥ ' ∙ T/ log T. The fourth inequality is entailed by (1 - aι)(1 - a2) =
1 - a1 - a2 + a1a2 ≥ 1 - a1 - a2 for ∀a1, a2 ∈ [0, 1], where by mathematical induction, we can
extend this inequality for more terms Qin=1(1 - ai) ≥ 1 - Pin=1 ai as long as Pin=1 ai ≤ 1. The
fifth inequality comes from P誉:T 1/2i ≤ p∞= 1/2i = 1/2'-1.
b) For ∀j ∈ B:
Step decay’s bottleneck will occur in the bias term. Since j ∈ B, it follows from the definition of B
in Eqn.(G.22),
λ≤得
X Jog T
ηιλj ≤ ~8Tτ,
we have
T
bj =λj (w0,j - w*,j)	(1-ηkλj)2
k=0
≥λj (WOj- w*,j)2 ∙ 11 - ɪ2 2ηkλj I = λj (WOj- w*,j)2 ∙ 11 -	2%λj
k=0	k=1
l	log T-1 (i+1)∙ ToTT
=λj	(WOj- w*,j)2 ∙	1 -	£	£	2ηkN
∖	i = 0	k = i∙ loTT +1
‘	log T-1 (i+1)∙ ToTT	'
=λj	(WOj- w*,j)2 ∙	1 -	X	X	2-j
∖	i = 0	k = i∙ IogTT + 1	)
log T-1 T
=λj (WOj - w*j)2 ∙ (1 - η1λj E IogT
≥λj (WOj- w* j)2 ∙ (1 - 4η1λj ∙ IoTT)
≥λj (WOj- W*j)2 ∙ (1-4 ∙ IogT ∙ IoTT)
=λj (WOj- W*j)2 ∙ 2,
where the first inequality is caused by (1 - a1)(1 - a2) = 1 - a1 - a2 + a1a2 ≥ 1 - a1 - a2
for ∀a1, a2 ∈ [0, 1] and applying mathematical induction for {an} to obtain Qin=1(1 - ai) ≥
41
Published as a conference paper at ICLR 2022
1 - Pin=1 ai as long as Pin=1 ai ≤ 1. The second equality is because η0 = 0. The sec-
ond inequality comes from Plio=g0T -1 1/2i-1 ≤ Pi∞=0 1/2i-1 = 4. The last inequality follows
η1λj ≤ log T /(8T).
From assumption (3), We know λj (w0,j 一 w*,j)2 > 0. Furthermore, as We require
T
1
σ2
log T 2 256 minj λj (wο,j — w*,j)2
in Eqn. (G.19),
bj ≥λj (WOj- w*,j )2 •
1 ≥ min λj (WOL
w*,j)
• ι ≥ ɪ • σ
2 - 512 T
• log T.
2
2
In sum, we have obtained
1
∀j ∈ A,
vj
1
≥ 一
一 512
log T
λ2
∀j ∈ B,
—∙ log T
512 T g
By combining with Eqn. (G.21), we have
Ef(WT +1 - f(w*)] =2
1
≥ —
-2
≥∣B∣∙
=|B| •
j=1
j∈B
1 σ2
1024 T
1 σ2
1024 T
bj	+
bj	+
j∈A
•log t) + X σ2 ∙ λ2 • 1024
j∈A
・log T + ∣A∣∙
1 σ2
1024 T
log T
T
log T
b
1
T
2
j
d
j ≥	•
1
λ
j
2
(|A| + |B|) •
1024 ^ T ^ log T
2
=d • 1024 • T • log T
=ω ( dTTr • log T),
where the first inequality is because both the bias and variance terms are non-negative, given bj
λj (w0,j - w*,j)2 QkT=0(1 -ηkλj)2 ≥ 0andvj = PτT=0 ητ2 QkT=τ+1(1 - ηkλj)2 ≥ 0.
□
Remark 3. The requirement T/ log T ≥ 1/256 • σ2/ minj λj (WO,j - W*,j)2 and assumption
λj (wo,j — w*,j )2 = 0 for ∀j = 1, 2,...,d can be replaced with T/ log T > 1∕(85μ), since in
that case j ∈ A holds for ∀j = 1, 2,...,d and B = 0 .In particular, if ηι = 1 /L, this requirement
on T becomes T/ log T ≥ κ∕8.
G.5 The Reason of Using Assumption (1.7)
In all of our analysis, we employ assumption (1.7)
Eξ ntnt>	σ2H where nt = Hwt — b — (H (ξ)wt — b(ξ))
42
Published as a conference paper at ICLR 2022
which is the same as the one in Appendix C, Theorem 13 of Ge et al. (2019). This key theorem is
the major difference between our work and Ge et al. (2019), which directly entails its main theorem
by instantiating σ with specific values in its assumptions.
On the other hand, it is possible to use the assumptions in Ge et al. (2019); Bach & Moulines (2013);
Jain et al. (2016) instead of our assumption (1.7) for least square regression:
mwn f (w)	where f (W)，DE(x,y)~D [(y - w>x)2]
y = w>X + E with E satisfying E(χ,y)~D [e2xx>] W σ2H for ∀(x, y) ~ D
[||x||2xx>]	R2H
(G.25)
(G.26)
(G.27)
By combining our Lemma 1 and assumption (1.7) with Lemma 5, Lemma 8 and Lemma 9 in Ge
et al. (2019), one can obtain similar results in this paper with their assumptions. For simplicity, we
just use assumption (1.7) here.
H Relationship with (Stochastic) Newton’ s Method
Our motivation in Proposition 1 shares a similar idea with (stochastic) Newton’s method on quadratic
objectives
Wt+1 =Wt- ηtH-1Vf (wt, ξ),
where the parameters are also updated coordinately in the “rotated space”, i.e. given H = UΛU>
and W0 = U>W. In particular, when the Hessian H is diagonal and ηt = D/(t + D), the update
formula is exactly the same as the one for Proposition 1.
Despite of this similarity, our method differ from Newton method’s and its practical variants in sev-
eral aspects. First of all, our method focuses on learning rate schedulers and is a first-order method.
This property is especially salient when we consider eigencurve’s derivatives in Section 4.3:
only hyperparameter search is needed, just like other common learning rate schedulers. In addition,
most second-order methods, e.g. Schraudolph (2002); Erdogdu & Montanari (2015); Grosse &
Martens (2016); Byrd et al. (2016); Botev et al. (2017); Huang et al. (2020); Yang et al. (2021), ap-
proximates the Hessian matrix or the Hessian inverse and exploits the curvature information, while
eigencurve only utilizes the rough estimation of the Hessian spectrum. On top of that, this es-
timation is only an one-time effect and can be even further removed for similar models. These key
differences highlight eigencurve’s advantages over most second-order methods in practice.
43