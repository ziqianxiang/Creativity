Published as a conference paper at ICLR 2022
Neural Markov Controlled SDE: Stochastic
Optimization for Continuous-Time Data
Sung Woo Park1, Kyungjae Lee2, Junseok Kwon3
1,3School of Computer Science and Engineering, Chung-Ang University, Korea
1,2,3Artificial Intelligence Graduate School, Chung-Ang University, Korea
1 pswkiki@gmail.com, 2kyungjae.lee@ai.cau.ac.kr, 3 jskwon@cau.ac.kr
Ab stract
We propose a novel probabilistic framework for modeling stochastic dynamics with
the rigorous use of stochastic optimal control theory. The proposed model called
the neural Markov controlled stochastic differential equation (CSDE) overcomes
the fundamental and structural limitations of conventional dynamical models by
introducing the following two components: (1) Markov dynamic programming to
efficiently train the proposed CSDE and (2) multi-conditional forward-backward
losses to provide information for accurate inference and to assure theoretical
optimality. We demonstrate that our dynamical model efficiently generates a
complex time series in the data space without extra networks while showing
comparable performance against existing model-based methods on several datasets.
1	Introduction
Recently, there has been interest in using continuous dynamical systems to approximate complex time
series. Neural ODE Chen et al. (2018), which opened the way for continuous representation of neural
networks, have been widely investigated and thoroughly analyzed by Massaroli et al. (2020). As the
stochastic generalization of ODE, Neural SDEs Li et al. (2020) have been proposed by regarding
intrinsic stochasticity in data representations (e.g., stock market data). Since the conventional Neural
ODE/SDEs only utilize the initial information of trajectories when propagating dynamics, modelling
complex time-series with naive Neural ODE/SDEs has been regarded as inefficient and undesirable
choices, as pointed out by Kidger et al. (2020).
To address these problems, Rubanova et al. (2019) presented an auto-regressive model to generalize
recurrent neural networks (RNNs) to have continuous hidden dynamics with neural ODE. Furthermore,
Chen et al. (2018) proposed an encoder-decoder structure with Neural ODE in the latent space to
reconstruct/predict complex data representation. Although the aforementioned approaches produce
remarkable results, they focus on suggesting additional probabilistic structures rather than improving
the learnability of the Neural ODE model itself. Compared to aforementioned approaches, we focus
on solving the fundamental issues of Neural ODE/SDEs. First, we raise two important questions.
Q1) How can we construct an efficient network architecture for Neural ODE/SDE models that do not
require additional recurrent networks to model complex time series?
Q2) How can we train Neural ODE/SDEs that can utilize richer information of observed sequences
to accurately generate complex time series?
As SDEs can be posed as stochastic generalizations of ODEs, we focus on a stochastic framework
and adopt the stochastic optimal control theory as our primary analysis tool for the rigorous and
systematic analysis of the aforementioned problems. Keeping this in mind, the contributions of our
paper are to answer the above two questions.
A1) Novel probabilistic framework for stochastic dynamics. We propose a novel neural controlled
stochastic differential equation (CSDE) to model the complex stochastic time series, where multiple
control agents are defined to construct local dynamics in their own private temporal states. With this
property, the proposed CSDE incorporates Markov dynamic programming, enables our model to
directly infer the complex trajectory on data space rather than the latent space without any extra
network (e.g., encoders/decoders), and shows remarkable efficiency compared to existing methods.
A2) Novel conditional losses. We introduce a novel Markov forward conditional (MFcond) loss to
utilize multi-conditioned dynamics instead of the conventional dynamics determined by partial initial
conditions. The proposed MFcond loss makes our method to model the complex information of
1
Published as a conference paper at ICLR 2022
time-series data. To impose regularization and to ensure the optimality of control agents, we also
suggest a novel Markov backward conditional (MBcond) loss.
2	Related Work
ODE As a Latent Probabilistic Model. Rubanova et al. (2019) suggested an ODE-RNN by com-
bining RNN with the latent dynamics induced by the Neural ODE. To deal with irregular time-stamps,
exponential-decaying of the hidden states was also discussed by Che et al. (2018). De Brouwer et al.
(2019) assumed that the observations are sampled from the stochastic dynamics induced from SDEs
and introduced GRU-ODE to approximate the observed stochastic time series.
SDE As a Latent Probabilistic Model. Liu et al. (2021) incorporated Neural SDEs with recurrent
models as a primary probabilistic dynamical model to generate stochastic continuous-time latent
variables. While this SDE model could describe the stochastic dynamics on the latent space with
recurrent structures (e.g., RNN encoder/decoder), it required a whole sequence of historical observa-
tions as inputs to the model. Unfortunately, this type of formulation leads to non-Markov types of
SDEs, which makes it difficult to analyze the probabilistic characteristics of the dynamics. Unlike
this model, we focus on the Markov SDEs while maintaining identical objectives.
Neural CDE and RDE. Kidger et al. (2020) proposed a data-driven neural controlled differential
equation called Neural CDE to incorporate a rough-path analysis theory and model complex time
series. Morrill et al. (2021) extended the rough-path theory with a Neural RDE to deal with the
continuous time series over long time.
Generative SDE Models. Recently, Kidger et al. (2021) suggested SDE-based generative adversarial
networks (GANs). Park et al. (2021) utilized the temporal conditional Wasserstein distance to
construct GANs for time-series generation.
Please refer to Appendix A.1 for additional discussion on related works.
3	Markov Neural Controlled SDE
In Section 3.1, we introduce a novel SDE model that considers temporally private agents. In
Section 3.2, we propose the Markov-DP-TP framework to efficiently solve the stochastic optimal
control problem with the proposed neural SDE model. Finally, we suggest novel Markov conditional
forward and backward losses in Section 3.3 and 3.4, respectively. In the Appendix, we provided the
detailed technical definitions.
3.1	Controlled Stochastic Differential Equations
The basic object of our interest is a controlled Ft-adapted process Xtα with multiple control agents
α = {a1, ∙∙∙ , αM} ∈ A where A denotes the set of admissible control agents. In particular, the
stochastic process Xtα is defined as a solution to the following CSDE:
MM
dXt = X wi(t)bi (t,Xtα,αi) dt + Xwi(t)σi & Xα,αi) dWt,	(1)
i=1	i=1
where b and σ : [0, T] × Rd × A → Rd are the drift and diffusion functions, respectively. Each control
agent αi : [0, T] × Rd × Rm, αi = αi (t, Xt; θi), ∀1 ≤ i ≤ M is defined as a Markov closed-loop
feedback control, which is parameterized by the neural network θi . While every agent is defined as a
closed-loop feedback-type Carmona (2016b), the solution to the CSDE above, Xtα , is the Markov
process, which means that process Xtα is propagated using the information of the current state.
Let T = {tk}ι≤k≤N be a set of ordered times1 such that 0 = tι < •…< tk <t < •…< tN = T.
The set of functions {wi(t)}1≤i≤M is defined as an indicator function on the intervals, wi(t) =
1tk≤t≤tl with predetermined starting/ending points tk, tl in T. We call this function temporal
privacy (TP) because it represents each agent’s attention on different sub-intervals. Overall, in (1),
the stochastic process Xtα is propagated by summing M -number of individual agent’s weighted
attentions {PM Wibi(∙, ∙, αi), PM wiσi(∙, ∙, αi)∣.
To understand the behavior of the proposed
CSDE more deeply, we consider the following detailed example:
1The time interval dt ≈ ∆t = |tk - tl | for any k, l can be set regularly/irregularly in our method.
2
Published as a conference paper at ICLR 2022
Role of Temporal Privacy. We define wr(s) = 1t≤s≤u, t, u ∈ T with r ≤ M. Then, Xuα in (1)
given Xt at an interval [t, u] can be equivalently rewritten in the integration form:
uM	uM
Xα=[α1,…,αM] = Xt + / EWi⑹b‰Xα,αDds +/ EWi(SW(S)Xa,ai)dW3
ti	ti
= Xtα + br(S,	Xsα,αr)dS +	σr(S,	Xtα, αr)dWs = Xuαr.
wr (s)	wr (s)
(2)
In (2), the activated control agent to evaluate the stochastic process Xuα for the interval [t, u] is only
r
αr (i.e., Xa = Xa ) owing to the definition of the weighting function w(.)(t). This means that
the remaining control agents {αj }j 6=r are not used for the evaluation of the stochastic process in
the sub-interval [t, u]. While each agent αi is activated at its own private sub-interval, this leads
our method to adopt dynamic programming (DP) to train Neural CSDEs in the form of (1). In this
paper, we aim to solve the optimal control problem via DP with multiple agents, where each agent
specializes in solving a particular sub-problem in its private interval.
3.2	Markov Dynamic Programming Principles
The dynamic programming principle is one of the fundamental philosophies for dealing with stochastic
optimal control problems. Its basic idea is to consider a family of sub-problems with different initial
times/states and establish the relation among the sub-problems to systemically solve them. Using the
mathematical property of the proposed CSDE with TP, we present an efficient learning strategy to
solve stochastic optimal control problems via Markov dynamic programming (Markov-DP).
In this paper, we aim to solve the stochastic optimal control problem by training control agents
a = [a1,…，aM] and minimizing the cost functional J(t, Xa) : [0, T] X Rd → R+:
J(t, Xta) =E
ZtT
l(S Xsa)dS+Ψ(XTa)
Ft
Ztu
l(S Xsa)dS+ J(u, Xua)
Xta
(3)
E
where l : [0, T] × Rd → R+ is the running cost (e.g., L2 loss) that computes the discrepancy between
the propagated process Xta and the observed data point ytat each time t, Ψ(XTa) : Rd → R+ is the
terminal cost that estimates the discrepancy between the terminal state and the data yT . To evaluate
the cost functional J(t, Xta) at time t with control agents α, the running cost is integrated over the
time interval [t, T] conditioned on filtration Ft. Note that the expectation conditioned on Ftin (3)
can be substituted to the expectation conditioned on Xta in light of the Markov property presented in
Section A.2, and the cost functional at time t only depends on the current state of the process Xta.
Markov-DP with Temporal Privacy. By combining the tower property of the conditional expecta-
tions with the dynamic programming principle and Ito's formula (Oksendal (1992)), one can show
that a minimization problem can be recursively decomposed into sub-problems owing to the property
of TP in our proposed CSDE:
V (t, Xta) , inaf J(t, Xta)
inf E
a
、
Ztu
l(S Xsa)dS+ J(u, Xua)
{z^
(A)
inf E
ar
u l(S, Xsa)dSXta
(4)
■{z
(B)
+ inf E[J(u, Xua)|Xta],
a(-r)
1-----------------{-----------------}
(B’)
where V is an optimal cost functional (i.e., value function), αr denotes the r-th control agent, and
a(-r) = [αι, ∙∙∙ , 0, ∙∙∙ , aM] indicates the set of remaining agents (the r-th component is zero).
In (4), the minimization problem (A) over α	is divided into two sub-problems using the dynamic
programming principle, which are (B) and (B’). Because the minimization problem (B) is only
dependent on the control agent αr parameterized by the neural network θr , we compute the gradient
descent of θr to solve the sub-problem (B):
θr+ι = θr - ∂θr E Lw (S)=J(S，Xtr /r ))dsXt，	⑸
3
Published as a conference paper at ICLR 2022
where wr (s) = 1t≤s≤u is the TP function at an interval [t, u] and k is the index for the learning
iterations. In (5), the r-th control agent αr minimizes the cost functional using the gradient descent
scheme at its own temporal sub-interval. As the remaining sub-problem (B’) over agents α(-r) can
also be recursively decomposed into smaller sub-problems using the dynamic programming principle,
the original problem (A) is solved separately with M-number of control agents α = {α1,…，αM}
with the M -number of gradient descent schemes. This indicates that we can obtain the set of agents
α? = {αi(∙, ∙; θ?)} by collecting individual optimal agents with sub-problems.
In this paper, we combine the Markov-DP with M gradient descent schemes in (5) and CSDE
with TP in (1) and introduce a novel Markov-DP-TP framework. In the numerical experiments in
Section 4.4, we show that the proposed Markov-DP-TP framework remarkably increases the model
efficiency compared to conventional non-DP naive approaches, which makes our method directly
model the complex time series in the data space. However, despite the improvements with our novel
Markov-DP-TP framework, there exist remaining practical/theoretical issues that should be addressed
to solve the optimal control problem with complex datasets.
1)	Conditional Dependency. The main practical issue in implementing the Markov-DP-TP frame-
work is that explicit conditional states are not given, e.g., Xtα in (5). As different initial/terminal
conditions of SDE lead to totally different behaviors of induced dynamics, well-designed conditional
information is a crucial factor in training the Neural CSDE for specific applications. In Section 3.3,
we introduce the Markov Forward conditional (MFcond) loss to train the Neural CSDE with
well-posed conditional information that ensures accurate network predictions.
2)	Theoretical Optimality. In the optimal control theory, there are well-known partial differential
equations called Hamiltonian-Jacobi-Bellman (HJB) equations, which assure the theoretical opti-
mality of control agents. If the control agents can solve the HJB equation, the proposed method
attains the optimal state Vt(Xtα) = infα Jt(Xtα) = Jt(Xtα?). However, the optimal agents α? of
the proposed CSDE with gradient descent are not generally equivalent to the solution to the HJB
equation. In Section 3.4, we propose the Markov Backward conditional (MBcond) loss to assure
the optimality of control agents and to provide information in backward dynamics for regularization.
3.3	Markov Forward Condition
In this section, we first raise the important question: Why is the well-posed conditional estimation
in cost functional important to accurately train Neural SDE (CSDE) models? To elucidate the
importance of this question, we consider the following minimization problem with the cost functional
with naive partial information:
inf L(α) = inf Ey0
ZT
0
l(s, Xsα)ds + Ψ(XTα)
X0
y0
(6)
where y(.)= {yt}t∈[o,τ] denotes a set of observed data, and yo is the initial data at time t = 0.
In (6), the conditional expectation is taken to the single initial state X0 = y0 , and the control agents
minimize the accumulated losses using this partial information. As pointed out by Kidger et al.
(2020), this naive cost functional causes a problem when dealing with high-dimensional complex
datasets. This is because the Neural CSDE should disentangle the inherent latent information of
complex high-dimensional data to generate accurate results, but the control agents are trained with
only the restrictive and partial information of the observed data (i.e., initial condition X0 = y0). To
solve this problem, we introduce a novel loss function called the MFcond loss that can fully exploit
the information of the given observed data y(.), while keeping the Markov structure of X『：
Definition 1. (MFcond loss) We define the prediction operator Tsα,t as follows, for s < t,
Tsα,t :=
1
∣i(sj)∣
Σ
m∈I (s,t)
tM	tM
Xtαm +	wibi(u,Xuα,α)du +	wiσi(u, Xuα, α)dWu(m)
tm i=1	tm i=1
Xtαm = ytm
(7)
where I(s, t) := {m : s ≤ tm < t}, |I (s, t)| is the cardinality of I(s, t), and {Wu(m)}m∈I(s,t)
denotes the Wiener processes with respect to time u. Let us define a random stopping time τs such that
τs := inft{t : l(t, Tsα,t) > } for the pre-determined threshold 2. Then, we can define the MFcond
loss with the stopping time t(.), asfollows:
2Please refer to Appendix (A.7) for detailed information
4
Published as a conference paper at ICLR 2022
(a) Network Training with the MFcond loss
Figure 1: Conceptual illustration of the MFcond loss and network inference. (a) The bold green lines
indicate a single trajectory of the proposed CSDE, while other trajectories are shown in shaded regions. The
MFcond loss estimates the conditioned losses l ◦ Tsα,τs (e.g., red vertical lines) at τs. The empty parts of the
α
black lines indicate the irregularly-sampled (unobserved) data representation. (b) The average decision Xtαk (e.g.,
green cross) is estimated to approximate the true test data ytk (e.g., red cross) using multiple past observations.
α
This figure shows the failure case as process Xak hardly approximates ytk.
(b) Network Inference
Lf (α, y(∙)) = Ey(.)
/Tl (Ts,TSατs) χ(s)ds + Ψ(XT),
(8)
where χ(s) is an indicator function that produces values at the observed time (i.e., χ(s) = 1 if ys
is observed at s; otherwise, χ(s) = 0). This function is used to consider the irregularly sampled
data points. In (8), naive running cost l of (6) is replaced with l ◦ Tsα,τ , in which the MFcond loss
recursively accumulates the expected future losses l ◦ Tsα,τ conditioned on multiple observations.
At each time s, stopping time τs decides the future time to stop the CSDE propagation by determining
if the accumulated losses are larger than the predetermined threshold or not. While the proposed loss
requires a set of multiple conditions on the Markov process Xtα to train control agents, information
is utilized to generate time-series data, and complex dynamics can be expressed. A conceptual
illustration of the proposed MFcond loss is shown in Figure 1-(a).
The main idea of our MFcond loss in (8) is to minimize the differences between the future estimations
Xuα,s for any given s ≤ u. In other words, the proposed CSDE is trained to generate an identical future
estimation of Xu given any past initial conditions Xa) = y(.), i.e., (Xα,s ≈ Xa,, ∀s ≤ t ≤ U) to
estimate network inference with multiple conditions in the test time. This idea is used to introduce a
novel inference procedure to overcome the raised issues on the partial information.
Network Inference. Let {ytm} be the observed data sequences until the current time t in the test
dataset. Our objective is to predict the future points {ytk}, (tm ≤ t <tk). Our model generates the
stochastic estimation Xtk to approximate ytk at a future time given multiple initial conditions ytm:
WNetWork Inference}-----------------------------------------------------------------------------S
ytk ≈ Xtk = Ttm ,tk =
1	M	tk	M	tk
E Σ	Xts +∑	Wibi(t,Xtα,αi)dt + £	Wiσi(t,Xt,αi)dW(s) Xts = Ots	⑼
s∈I(tm,tk)	i=1 ts	i=1 ts
In (9), each control agent makes decisions on its specialized temporal state and collaborates to
generate a stochastic conditional estimation Xa and approximate ytk. As our MFcond loss induces
identical estimations Xaytm for any tm, X[ utilizes multiple conditions {ytm } and fully exploits
the past information to predict/estimate future values. A conceptual illustration of the network
5
Published as a conference paper at ICLR 2022
inference is shown in Figure 1-(b). While the proposed inference mechanism utilizes enlarged
information3 compared to a single initial condition, it can model the complex time-series data.
If the control agents are trained with the naive cost functional, the terminal states Xuα,s (conditioned
on initial state Xs = ys) and Xuα,t (conditioned on initial state Xt = yt) are largely different,
which causes problems when we generate complex time-series data during the test time, whereas our
inference mechanism introduced in (9) utilizes averaged multi-decisions Xtαk given different initial
conditions. Thus, the MFcond loss is essential for utilizing the proposed inference procedure.
Unlike the dynamical auto-regressive probabilistic models (e.g., ODE-RNNs) that encode whole (or
partial) data sequences, as shown in (1), the proposed Markovian CSDE model only uses the current
observation to propagate stochastic dynamics. An additional inference mechanism coordinates the
multi-conditioned trajectories to utilize information and produces complex time series.
3.4 Markov Backward Condition
In the previous section, we suggested the Markov forward conditional loss that exploits the entire
information of time-series data to generate accurate results. Aside from its empirical benefits to some
applications, no theoretical/empirical optimality of (4) is assured by minimizing the MFcond loss
in general. To tackle this problem, in this section, we further introduce the additional stochastic
dynamics relating optimality of proposed CSDE-TP.
Let us define the auxiliary process Zt = V (t, Xtα?) with a value function V , where α? denotes
the optimal control agents. Subsequently, we consider the following forward-backward stochastic
differential equations (FBSDEs):
-------- (dχα = pM=i W仙也Xa ,。?岫+p3 w”(t,χα* ,。?川叫
(Xa ,Zt)= ddZt = -l(s, Xa )dt + PM=I VV(t, Xt)Wiσi(t, Xa, a?)dWt	(10)
IZT = ψ(xt*)
The first SDE (i.e., Xta*) called the forward SDE has an identical form of (1) and propagates
stochastic evaluation in the forward direction with optimal control agents. The second SDE (i.e., Zt )
called backward SDE recursively subtracts the running cost from the terminal state Ψ(XTa*) in the
backward direction using forward estimations Xta* and cancels the effect of martingales in the
diffusion term. We utilize the property of backward dynamics Zt to train the control agents for the
following reasons.
1)	Backward Multi-conditions. Like the MFcond loss with multi-conditions in the forward direction,
we want to provide additional information to backward dynamics to train the control agents.
2)	Approximated Solution of HJBE. The auxiliary process Zt gives the theoretical optimality for
control agents related to the HJB equation based on the results developed in Yong & Zhou (1999);
PardouX & Tang (1999), where the process Zt = V(t, ∙) admits a solution of the HJB equation in (11)
and induces an optimal solution for the minimization problem infaJ in (4).
'V X + ^Tr[στσ(t, x, a?)V2V(t, x)] + VV(t, X)Tb(t, x, α?) + l(t, x) = 0,	(11)
where V(T, x) = Ψ(x). In (11), we want to approXimate Zt using control agents for optimality.
However, the process Zt requires optimal control agents a? that cannot be obtained during the training
time. To overcome this problem, we approXimate the auXiliary process Zt with Zta parameterized by
neural control agents a(∙, ∙,θ), which is defined as the modified version of Zt. In particular, Zt can
be eXpressed in the following integral form:
tM	tM
Zta= Ψ(XTa) -	Wi(s)l(s,Xsa)ds+	Wi(s)σi(s, Xsa, αi)VJ (s, Xsa)dWsT, (12)
Ti	Ti
where J is the cost functional defined in (3), and VJ denotes the gradient of the cost functional with
respect to its spatial aXis. Using the proposed process Zta, we introduce a novel loss function called
the MBcond loss to satisfy the two objectives discussed above.
3Please refer to detailed eXplanation in AppendiX A.3.
6
Published as a conference paper at ICLR 2022
Algorithm 1 Neural Markov CSDE-TP
Require: γ = 0.95,
for k = 1 to K (i.e., the total number of training iterations) do
1)	Simulate forward controlled SDE with Markov control agents
1-1) dXtαk = PiM=1 wibi * *(t, Xtαk, αik)dt + PiM=1 wiσi(t, Xtαk, αik)dWt
1-2) Evaluate each decision of control agents αik = αik(t, Xtαk ; θki )
1-3) Compute the MFcond loss for M-control agents {Lf (αk(∙, ∙, θk))} With stopping time t(.)
1-4) Update threshold for random stopping time, 0+1 — 2 max l (t, Ta (ys))
2)	Simulate backward controlled SDE
2-1) dZtαk = - PM Wil(S,xα)dt + PNI VJ(t,xαk )wiσidWt
2-2) Evaluate the MBcond loss for M-control agents, {Lb(αk(∙, ∙,θk))}ι≤i≤M
3)	Update control agents with Markov-DP
3-1) θk+1 = Oik- γVθiLf (αi(∙, ∙,θk)) -(1- YVLb(αi(∙, ∙,θ))
end for
Definition 2. (MBcond loss) Let us define the auxiliary process Ztα as the solution to (12). Then,
the MBcond loss can be defined as follows:
Lbg)= Ey(∙),t∈[0,T] hlZα |2 ∣Xt = yt].
(13)
Theoretically, if We optimize the MBcond loss (13) according to the proposed backWard dynamics Ztα ,
the PDE reformulation of backWard dynamics, called Non-linear Feynmann-Kac, have the identical
solution4 to HJB equation in (11). Thus, our method can attain the optimal solution of original
problem posed in section 3.2.
Intuitively saying, one can shoW that the MBcond loss is equivalent to the reformulation of the
minimization problems in (4) using Ito's formula. Thus, solving the minimization problem infα Lb
induces an identical effect to solve the original problem infα J. The only difference is that We
utilize multiple conditions to provide conditional information on the backWard dynamics Ztα for the
regularization of control agents trained With forWard conditional dynamics and to impose constraints
on control agents, Which induces an approximated solution to the HJB equation.
3.5 Objective Function
In this section, We describe the overall training procedure, Which incorporates all the proposed
components (i.e., Markov-DP With CSDE-TP, MFcond loss, and MBcond loss) as folloWs:
inf L(α) = inf	γLf(α)+(1 - γ)Lb(α)
α ∣{z}	α=[α1,…，ɑM] ∣--^^} 、 sz /
MFBcond	MFcond	MBcond
M	(14)
CSD≈E-TPXinifγLf([αi,α(-i)])+ (1-γ)Lb([αi,α(-i)]) ,
Where Lf and Lb are defined in (8) and (13), respectively, and γ is a balancing hyperparameter.
In (14), the control agents α = [a1, ∙∙∙ , aM] are trained with a convex combination of MFcond
and MBcond losses. Using the property of CSDE-TP With Markov-DP, the original problem is
approximated with the collection of M sub-problems, and each control agent is separately trained
with M gradient descent schemes. Algorithm 1 describes the detailed procedure of our method.
4	Experiments
Network structure of control agents. The neural network structure for each agent control consists
of 2-layers of fully-connected layers, where each module has 128 latent dimensions. For the activation
units, we used the specialized module LipSwish, Chen et al. (2019); Kidger et al. (2021), to stabilize
the FBSDEs during training. Please refer to Appendix A.6 for detailed information on the network
architecture.
Datasets. For the evaluations, we used PhysioNet, Speech Commands, Beijing Air-Quality, and S&P-
500 Stock Market datasets. Refer to Appendix A.5 for data statistics and prepossessing procedures.
4Please refer to Appendix A.4 for the discussion on theoretical optimality induced by the MBcond loss.
7
Published as a conference paper at ICLR 2022
4.1 Time-Series Data Reconstruction
Table 1: Evaluation of reconstruction tasks on the PhysioNet/Speech Commands datasets.
	PhysioNet	Spee	ch Commands
Methods	MSE (×10-1) ( NLL (×102) ↑ ∣ MSE (×100	) ( NLL (×103) ↑
RNN-VAE	6.655	-33.238	0.984	-4.918
ODE2VAE	5.523	-24.195	0.723	-3.763
Latent SDE (ODE-Enc)	2.403	-11.978	0.841	-4.226
Latent SDE (RNN-Enc)	2.418	-12.051	0.851	-4.250
Latent ODE (ODE-Enc)	2.395	-11.933	0.851	-4.251
Latent ODE (RNN-Enc)	2.375	-11.840	0.880	-4.402
RNN-Decay	1.431	-7.119	0.865	-4.323
ODE-RNN	1.311	-6.517	0.697	-3.479
GRU-D	1.424	-7.083	0.838	-4.116
mTAND	0.887	-4.396	0.581	-2.902
CSDE-TP	0.755	-3.667	0.435	-2.172
CSDE-TP-MV	0.728	-3.605	0.423	-2.109
In this experiment, we compared our model against baseline dynamic models: [Latent ODE, Chen
et al. (2018)], [Latent SDE, Li et al. (2020)], [ODE-RNN, Rubanova et al. (2019)], [GRU-D, Che
et al.(2018)],[mTAND, ShUkla & Marlin (2021)], and [ODE2VAE, Cagatay Yildiz et al. (2019)].
We used open-source codes provided by the authors for comparison. For the Latent ODE (SDE)
methods, RNN and ODE-RNN were Used for the encoder strUctUres, where the decoder strUctUres
were identically set to ODE (SDE). Table 1 shows the performance of all baseline methods compared
to the proposed CSDE-TP for the reconstrUction tasks. As evalUation metrics, we Used mean sqUared
errors (MSE) and negative log-likelihood (NLL) with open-soUrce code in RUbanova et al. (2019).
As shown in Table 1, the proposed method consistently oUtperformed the baseline methods by a large
margin. In this experiment, we observed that latent dynamics-based methods (e.g., Latent ODE/SDE
with RNN and ODE-RNN encoders) on models attained similar performances. We set the latent
dimensions of each control agent to 128 for both the reconstrUction and prediction experiments. In
the experiments on both datasets, the Mckean-Vlasov (MV) type of the SDE model slightly improved
the performance, where it sUbtracted the mean (i.e., mean-shifting) of the control agent oUtpUts to
normalize/redUce the intrinsic volatility in the inferred process Xtα .
4.2 Time-Series Data Prediction
Table 2: Evaluation of prediction tasks on the PhysioNet/Air Quality datasets.
Methods	PhysioNet	Air Quality MSE (×10-1) ( NLL (×102) ↑ ∣ MSE (×10-1) (	NLL (×103) ↑
ODE2VAE	5.369	-26.812	7.938	-3.965
Latent ODE (RNN-Enc)	2.038	-10.153	3.403	-1.772
Latent ODE (ODE-Enc)	1.977	-9.849	3.374	-1.683
Latent SDE (RNN-Enc)	2.002	-9.975	3.676	-1.834
Latent SDE (ODE-Enc)	1.995	-9.936	3.328	-1.660
mTAND	1.551	-7.719	1.867	-0.929
CSDE-TP	1.195	-5.937	1.277	-0.630
CSDE-TP-MV	1.057	-5.811	1.671	-0.638
In this experiment, all the methods were evalUated to predict the remaining 12 time seqUences
(e.g., 20%) on the test dataset given the first 36 time-seqUences (e.g., 80%), and we followed a
procedUre similar to that sUggested by RUbanova et al. (2019) to predict the fUtUre time series. Table 2
compares the performance in terms of MSE and NLL, in which the proposed method considerably
oUtperforms the baseline methods. In the experiment on the Air qUality dataset, we set a smaller
latent dimension of 64 as the featUre dimension compared to those of the other datasets. Interestingly,
the CSDE-TP-MV model exhibited worse performance than the vanilla CSDE-TP model when the
data dimension was very small. This resUlt indicates that the mean-shifting in a low-dimensional data
space redUces the expressiveness of the CSDEs.
4.3 Uncertainty Estimation on Stock Market Dataset
When high volatility is observed over the temporal/spatial axes, conventional evalUation metrics
sUch as MSEs hardly captUre the stochastic property of the time-series variations. ThUs, to cap-
tUre the stochasticity, we evalUated the distance between the distribUtions of the test data and the
inferred/generated data Using the maximUm mean discrepancy (MMD). We followed the protocol
8
Published as a conference paper at ICLR 2022
Table 3: Uncertainty estimation on the Stock Market dataset.
Methods I RNN	RNN-VAE^^ODE-RNN^^LatentODE^~LatentSDE ∣ CSDE-TP
MMD (× 10-3) J I 526.12	427.05	201.48	194.75	190.11	∣	94.79
suggested by Li et al. (2017) to evaluate the MMD distance, where we used two Gaussian RBF kernels
with bandwidths of [5.0, 10.0]. Using this evaluation metric, we experimented on reconstruction tasks
using the S&P-500 Stock Market dataset. Table 3 shows that the proposed CSDE-TP outperforms
baselines and effectively recovers the distributional information of stock prices with the stochastic
property of the SDE models and the proposed optimization framework. Interestingly, the latent SDE
model attains better performance compared to the Latent ODE, as it utilizes an additional Wiener
process to model the data uncertainty. The performance improvement of the Latent SDE vanishes
when we remove the diffusion term (σ = 0) of the latent SDE.
4.4 Empirical Study
(a) Efficiency of Markov-DP with CSDE-TP
Figure 2: Ablation study on the effectiveness of the proposed method.
(b) Efficiency of the MFcond loss
Efficiency of the Markov-DP-TP framework. To show the empirical advantages of our CSDE-TP
model with Markov-DP learning schemes, we evaluated our CSDE-TP according to a different
number of control agents on the prediction task using the Air Quality dataset. Figure 2-(a) shows the
training MSEs for several variants of the proposed model in the first 20 epochs, where CSDE-TP-
Shallow1, -Shallow2, and -Deep (i.e., black, blue, and red lines) denote the proposed models with a
different number of control agents, i.e., M = 2, 8, and 48, respectively. The standard CSDE model
(i.e., the black dashed line) utilized a single agent, M = 1. For all models, the total number of training
parameters was equivalently set to ≈ 40K , and the number of parameters was normalized. As shown
in Figure 2-(a), despite using the same number of parameters, employing multiple agents clearly
outperforms the standard CSDE in terms of the learning curve. From this fact, we can conclude that
the Markov-DP-TP significantly increased the network efficiency compared to the standard CSDE,
which indicates that our Markov-DP framework is crucial for training controlled dynamics models.
Efficiency of the MFcond loss. In this experiment, we show the empirical advantages of the multi-
conditioned CSDE in (8) against the naive partial-conditioned CSDE in (6). Similar to previous
experiments, the results were obtained for the prediction task with the Air Quality dataset. Figure
2-(b) shows the model confidence in testing MSEs for the first 50 epochs, where shaded areas indicate
the confidence regions (i.e., ± std). The proposed MFcond loss exhibits considerable performance
improvement (.08	.87) compared to the conventional native cost functional and reduces the
variances in loss landscapes with stable learning. With the theoretical discussion in Appendix A.3,
we conclude that the proposed CSDE actively exploits the information of the complex time series
with multiple conditions to accurately generate complex time-series.
5	Conclusion
In this paper, we introduce a novel Markov-type CSDE with the TP function that records the individual
attention of each control agent at sub-intervals along the temporal axis. Using the properties of the
CSDE and TP, we suggest Markov DP to efficiently train the control agents by decomposing the
original problem into smaller sub-problems. To overcome the practical/theoretical issues, we propose
two novel losses, namely, MFcond and MBcond losses. The MFcond loss captures the future time
to estimate the running costs, while multiple conditions are actively provided to forward dynamics.
The MBcond loss assures the theoretical optimality of the control agents and imposes regularization
by providing additional information to backward dynamics. Experimental results demonstrate the
efficiency of the proposed method for various tasks using real datasets.
9
Published as a conference paper at ICLR 2022
Acknowledgments. This work was supported by Institute of Information communications Technol-
ogy Planning Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2021-0-01341,
Artificial Intelligence Graduate School Program (ChungAng university)).
10
Published as a conference paper at ICLR 2022
References
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications ,12(3):313-326,1982.
Rene Carmona. Lectures on BSDEs, stochastic control, and stochastic differential games with
financial applications. SIAM, 2016a.
Rene Carmona. Lectures on BSDEs, Stochastic Control, and Stochastic Differential Games with
Financial Applications. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2016b.
doi: 10.1137/1.9781611974249. URL https://epubs.siam.org/doi/abs/10.1137/
1.9781611974249.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural
networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. NeurIPS, 2018.
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Joern-Henrik Jacobsen. Residual flows for
invertible generative modeling. NeurIPS, 2019.
Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In NeurIPS, 2019.
Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. NeurIPS, 2019.
Davar Khoshnevisan. Intersections of brownian motions. Expositiones Mathematicae, 21(2):97-114,
2003.
Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations
for irregular time series. In NeurIPS, 2020.
Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, and Terry Lyons. Neural SDEs as
Infinite-Dimensional GANs. ICML, 2021.
ChUn-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN:
Towards deeper understanding of moment matching network. In NIPS, 2017.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients for
stochastic differential equations. In AISTATS, 2020.
Yingru Liu, Yucheng Xing, Xuewen Yang, Xin Wang, Jing Shi, Di Jin, and Zhaoyue Chen. Learning
continuous-time dynamics by stochastic differential networks. arXiv preprint arXiv:2006.06145,
2021.
Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting
neural odes. In NeurIPS, 2020.
James Morrill, Cristopher Salvi, Patrick Kidger, James Foster, and Terry Lyons. Neural rough
differential equations for long time series. ICML, 2021.
Bernt Oksendal. Stochastic Differential Equations : An Introduction with Applications. Springer-
Verlag, Berlin, Heidelberg, 1992.
Bernt Karsten 0ksendal and Agnes Sulem. Applied stochastic control of jump diffusions, volume
498. Springer, 2007.
Etienne Pardoux and Shanjian Tang. Forward-backward stochastic differential equations and quasi-
linear parabolic PDEs. Probability Theory and Related Fields, 114(2):123-150, 1999.
Sung Woo Park, Dong Wook Shu, and Junseok Kwon. Generative adversarial networks for markovian
temporal dynamics: Stochastic continuous data generation. In ICML, pp. 8413-8421, 2021.
11
Published as a conference paper at ICLR 2022
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS. 2019.
Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. NeurIPS, 2019.
Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled
time series. In ICLR, 2021.
Ikaro Silva, George Moody, Daniel Scott, Leo Celi, and Roger Mark. Predicting in-hospital mortality
of icu patients: The physionet/computing in cardiology challenge 2021. 2012.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020.
Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based
diffusion models for probabilistic time series imputation. arXiv preprint arXiv:2107.03502, 2021.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018.
Jiongmin Yong and Xun Yu Zhou. Stochastic controls: Hamiltonian systems and HJB equations,
volume 43. Springer Science & Business Media, 1999.
Moshe Zakai. Some Classes of Two-Parameter Martingales. The Annals ofProbability, 9(2):255 -
265, 1981.
Shuyi Zhang, Bin Guo, Anlan Dong, Jing He, Ziping Xu, and Song Chen. Cautionary tales on
air-quality improvement in beijing. Proceedings of the Royal Society A: Mathematical, Physical
and Engineering Science, 473:20170457, 09 2017.
Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. Ode2vae: Deep generative second order
odes with bayesian neural networks, 2019.
12
Published as a conference paper at ICLR 2022
A	Appendix
A.1 Detailed Comparison to Existing Methods
In this section, we investigate the relation between our method and existing methods.
Reverse SDE vs. Backward SDE. Song et al. (2020) suggested a novel SDE called reverse SDE,
which shares semantically similar idea with BSDE: both reverse/backward SDEs enhance the forward
SDE by providing additional information to drift/diffusion functions in forward dynamics.
The mathematical motivation of the reverse SDE in Anderson (1982) is to pose the SDEs with Wiener
processes Wt , Wt with respect to these minimal increasing/decreasing sigma algebras At , At and
define the relation between them:
1
dWt = -7VΓ ^[Pt(Xt)σ(t,Xt)]dt + dWt,	(15)
pt (Xt )
where Xt is a solution to the forward SDE and pt is the probability density of Xt . Using the relation
in (15), the reverse SDE transforms the prior distribution (e.g., Gaussian noise distribution) back into
the data distribution (e.g., 2D images) by gradually removing the noises and reconstruct the original
data with the well-designed score function (i.e., Vpt(χ)) in backward dynamics. In contrast to the
reverse SDE, the role of backward SDE in this paper is to consider the probabilistic reformulation to
access the cost functional to provide the additional information in backward dynamics.
Stacked ODE vs. CSDE-TP. Massaroli et al. (2020) suggested the stacked Neural ODE that shares
similar idea with the proposed CSDE-TP, where temporally piece-wise neural nets are considered
to model the complex dynamics. However, the stacked ODE faces the aforementioned problem on
partial conditional information when generating complex data as their models only take initial values
to propagate dynamics. As opposed to their models, the proposed model is trained with multiple
observations and directly generates time series in data space without any latent embedding network.
Furthermore, we generalize their optimal control problems to the stochastic version and propose the
Markov-DP-TP framework that can systemically solve the problem.
DDPMs vs. CSDE-TP. Tashiro et al. (2021) suggested denoising denoising diffusion models
(DDPMs) that are conditioned on the set of the observed data, where the generated sequential
data is assumed to be gradually transformed from an initial state in the forward direction and the
backward process is parameterized by the neural network and trained to minimize specific ELBOs.
Specifically, the transition probability pθ(Xt-1 |Xt) in the backward process is defined as a parame-
terized Gaussian distribution:
pθ (XtIXt-ι) = N (Xt-ι; μθ (t,Xt),σθ (t,Xt)),	(16)
where mean and covariance (μe, σj) are parameterized by the neural network θ. Similar to the pro-
posed CSDE, their parameterized functions are closed-loop type processes and the whole probabilistic
sequential model pθ is posed as the Markov chain: pθ (X0:T) = pθ(XT) QtT=1pθ(Xt-1 IXt).
In contrast to the DDPM, the probability transition in the proposed CSDE is defined as the continuous
generalization called controlled Fokker-Planck equation (CFPE):
∂1
∂tPα (x,t∣y, s) = -Vf(X,t,αθ)pt(x) + 2Tr [V2σστ(x,t, a©) ∙ pt(x)] ,	(17)
where t>s ∈ [0,T) and x,y ∈ Rd, Pt 〜Xt is the probability distribution of Xa.The CFPE in (17)
one-to-one corresponds to the CSDE (i.e., Xtα ). Compared to the discrete-time Gaussian transition
model, this conditional probability can express complex continuous-time probability transitions while
maintaining the Markov structure.
A.2 Notations and Background
We first define the basic definitions of probabilistic objects:
Definition 3. Afiltration {Ft} is an increasing sequence of {Ft} of σ-algebra such that Fo …,⊂
Ft ⊂ F. The triplet (Ω, Ft, P) is called a filtered probability space.
Definition 4. Thefiltration generated by Wiener process Wt is defined as FW = σ{W0,…，Wt}.
In this case, Wt is naturally FtW -adapted by construction.
13
Published as a conference paper at ICLR 2022
Definition 5. The stochastic process {Xt} is called {Ft}-adapted if Xt is Ft measurable for every
0≤t≤T.
Throughout this paper, We work on the filtered probability space (Ω, {Ft}t∈[o,τ], P) with the d-
dimensional Ft-Wiener process Wt and natural filtration FtW. We assume that αi for all 1 ≤ i ≤ M
is admissible Markov control, (i.e., αi is Ft-adapted and αi ∈ Ai, Xtα has a unique solution).
Definition 6. (Markov Process) Let Xt be a Ft-adapted stochastic process. Then, Xt is the Markov
process if the following equality holds:
E[Xt|Fs] =E[Xt|Xs],	∀s ≤t.	(18)
Definition 7. (Controlled Stochastic Differential Equation)
Xtα =Xs+Z tb(u,Xuα,α)du+Z tσ(u,Xu,α)dWu,
for 0 ≤ s ≤ t ≤ T.
(19)
The solution to the above CSDE is denoted as Xtα,s. If the initial states are specified (i.e., starting
point Xs = x), we denote the solution as Xtα,s,x. By the definition of Markovian control agents, in
all cases, the solution to the proposed CSDE in (1) is a Markov process.
Mathematical Assumptions. In this paper, we assume that functions b, σ are uniformly Lipschitz
ContinUoUs along its spatial axis and bounded ∣∣b(t, 0; ∙)k, ∣∣σ(t,0; ∙)k at the entire interval [0,T].
We assume that each functions bi(∙,χ, ∙),σi(∙,x, ∙), Ψ(x), l(∙,χ) are twice differentiable for all
1 ≤ i ≤ M, (i.e., , bi, σi, Ψ, l ∈ C2(Rn). and both drift and diffusion functions are uniformly
Lipschitz on its spatial axis, i.e., bi, ∂xbi, ∂x2bi, σi, ∂xσi, ∂x2σi ∈ Lip), and the trainable parameters
of the control agents αi, θi are lying in the compact subset C of its ambient space (i.e., θi ∈ C ⊂ Rm).
A.3 Enlarged Information by Collection of Observed Data
In the proposed inference procedure, we define a novel operator T in (9) to consider the multi-
conditioned dynamics with the Markov-type SDE model. Although this operator plays a central role
in the paper, its mathematical properties have not been carefully dealt and investigated thoroughly.
In this section, we discuss the relation between this operator and the enlarged information that is
obtained by collecting past observations. In addition, we generalize the inference mechanism in (9) to
a mathematically rigorous form and discuss the effect of the proposed operator T by showing some
probability inequality.
Suppose that we have two observed conditional states {Xtm }, {Xtn } until the current time t,
(tn, tm < t < tk) and the objective is to predict/generate the future value ytk using this infor-
mation. We consider the deterministic time tk by replacing random stopping time τtm to simplify the
discussion. First, we define the two-parameter stochastic process Y to model the proposed operator
T in an alternative way:
% m,tk = Y(tm,tn)(w) , 1 (Xatkm (w)+ Xattkn(W)) ,	(20)
where W ∈ Ω takes a value in the probability space. The stochastic process Y is the (Ftm ∨ Ftn)-
valued random variable for any fixed tm, tn < t by the definition, where Ftm ∨Ftn , Σ(F M1 ∪FM2)
is the composited smallest sigma algebra with two filtrations. In the definition, we assume that
processes Xattm，Xattn are derived from two independent Wiener processes Wt and Wt. Then, we
can define the two-parameter martingale Zakai (1981); Khoshnevisan (2003) in the following form:
M(tm, tn)(W) = E [l(tk, Y(tm,tn))|Ftm ∨ Ftn] .	(21)
By the definition of M, it can be easily shown that M is the reformulation of the MFcond loss for
some fixed number of past observations. Note that M is truly a martingale because conditional
estimations are summed in the definition of T. The control agents are trained to minimize the M
given the information induced by past observations (i.e., composited filtration {Wt <t FMtm }),
which indicates that the proposed inference procedure can infer the future value Xta according to
the enlarged information {Wt <t FMtm }. By the fact that M is a martingale with respect to the
composited filtration, we obtain the following result using Doob’s maximal inequality:
1 - ɪ (E[∣∣Xαtk -ytk∖∖] + E [∣∣Xαtk -ytk巾)≤ P 卜Up SUp E[1 ◦ TFtm ∨FM ≤ η ,
2η	tn<t tm<t
(22)
14
Published as a conference paper at ICLR 2022
where the inequality shows that errors between the future value ytk and the generated samples Xtαk,t
at time tk are bounded by the maximal perturbation probability. As the control agents are trained to
minimize the MFcond loss (i.e., M) in the right-hand side of inequality, it renders the probabilistic
bound of L2 errors at future time tk .
A.4 Detailed Discussions on the MBcond loss
In this section, we investigate the detailed theoretical structure of the MBcond loss and its fundamental
rationale for the optimality of control agents. For this, we rephrase the cost functional in the general
form:
J(t, x) = E
ZtT
l(s, Xsα)ds + Ψ(XTα )
Xt = x
(23)
The classical non-linear Feynman-Kac theorem in Yong & Zhou (1999) states that given the cost
functional J with the control agents α, one can obtain the second-order parabolic partial differential
equation from (23):
^∂t + O J, b(t, x, α)i + 2 Tr [σ στ (t, x, α)V2 * J ] + l(s, x) = 0,	(24)
where〈•，•〉denotes the inner product and the boundary condition is given as J(T,χ) = Ψ(χ).
Subsequently, by applying Ito's formula to (23), we obtain the following probabilistic formulation:
Ψ(XTα) = J(s,Xs)+Z T
∂J	1
∂(s, Xs) + 2Tr[σστ(s, Xs,α)V2J] + hb(s, Xs,α), VJJ dt
+Zτ hστ (s,Xs,α)VJ (s,Xs),dWti = — / ' l(s,Xs)ds+ZT (στ (s,Xs,α)VJ (s,Xs),dWsJ.
t	t	s	(25)
By rearranging each term above, the backward stochastic differential equation is induced directly.
ττ
Ztα = J(t,Xtα) = Ψ(Xτα) +	l(s, Xs)ds 一	hστ (t, Xt, α)VJ(s, Xs), dWsi.	(26)
tt
Note that, in the main paper, we use the inverse sign convention, RtT(∙) = 一 RT(∙), to emphasize
the backward direction. By using the formulations, the MBcond loss in (13) can be rewritten in full
description as follows:
Lb(α,Za) = /	Ey(∙)
Ψ(XT) + /T l(s,XT)ds - /Thστ(t,Xα,α)VJ(s,XQ,dWsiHXa = yt dt,
(27)
where y(.)= (yι,…，yτ)〜p(yι,…，yτ) denotes the set of observed data. The regularization
effect comes from the expectation evaluation of the third term in (26). Specifically, one can obtain
the following equality by using the Ito's isometry:
E ∕τ
2
hστ(t,Xt,α)VJ(s,Xs),dWsi	|Xt
E
Zτ στ (t, Xt, α)VJ (s, Xs)2 dt|Xt .
(28)
Because the MBcond loss is posed to minimize this additional martingale term in (28) in backward
dynamics according to the forward dynamics Xtα, it reduces the over-confidence of generated time-
series. By the relation (t, x) → J(t, x) → Ztα → Lb(t, x) for any (t, x) ∈ [0, T] × R+, the update
rule for the MBcond loss can be expressed as follows:
θk+ι = θr-亲[% (s,χsɑr(∙,∙,θr)
(29)
where this formulation is similar to (5) and shows that gradient descent with respect to θr for the
MBcond loss can be explicitly defined.
Admissible Control Set A. In previous discussions, we show the relation between J with BSDE
dynamics Zt and the well-defined gradient descent. The next step is to define the proper control set
A to relate the gradient descent with optimality.
15
Published as a conference paper at ICLR 2022
Let Us define the Hilbert space L2，{夕(t, x； θ); Rn-valued Ft-progressively measurable ∀θ ∈ C}
with the norm ||夕匕2 = E [片 3(t,x; θ)∣2dt] < ∞. We assume that control agents αr is La
Lipschitz on the parameter variable, i. e., K αr(∙, •； er」)-αr(•, .； θr,2)∣∣L2 ≤ La ∣"r,ι - θr∕∣ for
any θkr,1 6= θkr,2 ∈ Rm and any 1 ≤ k ≤ K. In all case, we assume that any θkr lies in the compact
subset C of Rm. Each functions bi(∙, x, ∙),σi(∙, x, ∙), Ψ(x), l(∙, x) are twice differentiable for all
1 ≤ i ≤ M, (i.e., , bi, σi, Ψ, l ∈ C2 (Rn). and both drift and diffusion functions are uniformly
Lipschitz on its spatial axis, i.e., bi, ∂xbi, ∂x2bi, σi, ∂xσi, ∂x2σi ∈ Lip). As we defined Ψ and l as
usual Euclidean distance, regularity/uniform Lipschitzness for these functions are trivial.
For the fixed parameter θ, We define the r-th control agent as θr → ar (∙, ∙, θr)，ar (θ) ∈ L2. Truly,
the image space of αr(θ) is the closed subspace of the Hilbert space L2 due to the Lipschitzness with
compactness of θ.
Let θr (k) : N → Rm be the trajectory for the training parameters of the r-th control agent at
r (-r)
learning iteration k. Without loss of generality, Jr [αr] = J(t, Xt ,	). We define the Euclidean
closed metric balls {Bδk}k∈N centered at θ(k) with the radius δrk < ∞ such that Bδk = {H ∈
Rm;k“ 一 θr (k)k ≤ δr, θr(k) is local minimum of Jr [θ(k)]}. Let us consider the sub-sequence
{θ(k)}k∈N ⊆ {θr(k)}k∈N, which induces the strictly-decreasing cost functional {Jr[θ(k)]}^∈jy
with the ordered index set N. Then, the admissible control set A is defined as follows:
M(KK	I M
α，[α1,…，αr …，αM] ∈ A = O ( \ [ αr(∙,∙,Bδj)； j ≤ K ∈ N } ⊂ O L2,	(30)
r = 1 1 K j=1r = 1
where Kj is the maximal element in Nj and the constant K ∈ N indicates the last iteration index of
training defined in Algorithm 1. Intuitively, the control set A can be understood as a collection of
local minimum obtained by M gradient descent schemes during training.
V , J[α?] = J [α(θ (K ))] = inf J [α(θ)].	(31)
a∈A
where V ∈ C1,2([0, T], Rd). By the definition of metric balls {Bδk} and strictly-decreasing proper-
ties, the infimum in (31) is attained when θ(K ) = θ and the control agent α(θ(K )) is optimal in this
control set.
Relation to Stochastic Maximum Principle (SMP). We consider an arbitrary control in the convex
set K ∈ A with β ∈ K and the optimal control α(θ(K)). Let DJ∣β = 第 J(θ(K) + e(β - α))∣e=0
be the GateaUX derivative (this can be defined while the control set is a vector sub-space, A ⊂ L2).
By the result of Pontryagin maximum principle, Theorem (4.12) in Carmona (2016a), one can obtain
the inequality as follows:
∂∂
∂aH y∨mLa)kθ(κ)-θk≥ ∂αH ∙k[α(t,χt,θ(κ))-β(t,χt,θ)]k≥DJIe≥0 (32)
fort ∈ [0, T] almost surely, where we define H , H(t, Xt, Yt, Zt, αt) for the Hamiltonian system H
with adjoint variables Yt, Zt and define the arbitrary control β = α(∙, ∙, θ) ∈ A with some θ. The first
inequality is satisfied due to the definition of Lipschitzian control agents. The optimality condition
indicates converging upper-bound of DJ∣β to 0.
In our method, the optimality condition of the proposed learning framework is bounded by the
Euclidean distance between θ(K) and θ in parameter space. Thus, the proposed framework poses a
fundamentally different approach to interact with optimality conditions in SMP. As we define that the
θ(K) is a local minimum of J with the inequality kθ(K) - θk < δθK, the gradient descent scheme
that induces the tight radius {δg}k∈N+ assures optimality by the relation 0 ≤ DJ∣β ≈ δfK.
Relation to the HJB equation. We consider the infinitesimal generator Lt of the non-homogeneous
controlled Markov process Xt as Laf =(▽/, b(t, x, α)i + ɪTr [σστ(t, x, α)V2f]. We show the
important relation between the proposed MBcond loss with the HJB equation as follows:
16
Published as a conference paper at ICLR 2022
Figure 3: The pipeline of the proposed CSDE-TP with neural control agents
MU-PPdqEW
SnoauQgolUOlIUGE-I
[Equivalence Relation
J (t,x) + La(O(K)) J (t,x) + l(t,χ) = 0 = dV (t,χ) + inf [La V (t,x) + l(t,x)]
∂t	∂t	α∈A
I	}
(33)
Non-linear Feynman-Kac, MBcond loss
"{^^^^
HJB equation, exact solution
In the left-hand side of (33), the PDE formula is directly consequence of the non-linear Feynman-Kac
theorem that we derive in (24). The distinct point is that control agents are obtained by the gradient
descent of the MBcond loss with BSDE (i.e., Zt). Note that, as shown in (31), θ(K) is actually an
optimal control. This means that, without heavy calculations to solve the PDEs, the gradient descent
algorithm also assures optimality of control agents in the proposed control set A.
In contrast, the HJB equation in the right-hand side states that the optimal control agent can be
obtained by solving the second-order parabolic formula and the infimum is taken by considering
algebraic properties of candidates for the exact solution. If the solution to HJBE exists in the control
set A, the PDE in the left hand side of (33) approximates the solution to the HJB. Overall, we
argue that the MFBcond loss can provide a novel deep learning-based paradigm to adopt/solve the
conventional stochastic optimal control problem in a feasible way (i.e., well-defined loss functions
with the gradient descent scheme).
A.5 Data Prepossessing
PhysioNet dataset, Silva et al. (2012), contains overall 8000 multivariate time series obtained for the
first 48 hours of a different patient’s admission to intensive care unit (ICU). Each patient has a set of
35 various clinical features. We normalized features of all patients in the dataset to have zero mean
and unit variance. We used a half of time-series as the training dataset and the remaining parts as the
test dataset.
Speech Commands dataset, Warden (2018), consists of one-second audio records of various spoken
words such as “Yes”, “No”, “Up”, and “Down”. Since there were nearly 100,000 record samples, we
sub-sampled the dataset due to the dimensionality of training instances on two conflicting classes
(i.e., “Right” and “Left”). Overall, 6950 time-series records were selected, where 80% were used as
training dataset and the remaining parts as the test dataset. We pre-processed these time series by
computing Mel-frequency cepstrum coefficients from the audio signal, so that each time series was
spaced with 65 and 54 channels. Then, we normalized each channel of all signals in the dataset to
have zero mean and unit variance.
Beijing Air-Quality dataset, Zhang et al. (2017), consists of multi-year recordings of air quality data
across different locations in Beijing. Each sample contains 6-dimensional time series features of
PM2.5, PM10, SO2, NO2, CO, and O3, which are recorded per hour. We segmented data to have
the length of 48 and normalized each feature of all data in the dataset to have zero mean and unit
variance.
17
Published as a conference paper at ICLR 2022
S&P-500 Stock Market dataset consists of stock market data with 6-dimensional feature vectors (i.e.,
[High, Low, Open, Close, Volume, Adj Close]). For the complete data acquisitions, we excluded
enterprises with incomplete recordings during sampling duration, thus total 381 enterprises are
selected. The time-series are sampled every 30-min with T = 48 temporal states. Similar to Speech
commands dataset, we used first 80% of temporal states to train the model and the remaining parts
are used for prediction task.
A.6 Experiments Details
Different SDE candidates for CSDE. Owing to the abstract form of the proposed CSDE in (1),
various types of drift and diffusion functions (i.e., b and σ) can be selected according to different
applications. In Table 4, we enumerate candidate functions. In the experiments, we adopted two
models: Vanilla and Mckean-Vlasov (MV) SDEs.
Hyperparameters. For the running and terminal costs (l and Ψ, respectively), we used the l2
distance, i.e., l(s, x) = kx - ys k22 and Ψ(x) = kx - yT k2. In all experiments, γ is set to 0.95. To
estimate the gradient of the MBcond loss, we estimated numerical gradients with the auto-grad library
in Pytorch (Paszke et al. (2019)).
Network Architecture for Neural Control Agents.
Each control agent αi (t, Xt; θi) has an identical neu-
ral network architecture, which consists of linear lay-
ers and non-linear units. Figure 3 shows the detailed
network architecture. Each agent takes concatenation
of temporal/spatial tensors (t, Xt) as its input, where
the temporal tensor t is transformed into new form t0
by the time inhomogeneous embedding layer. We fol-
lowed the setting suggested in Park et al. (2021) for
this embedding. After time embedding, concatenated
tensor (t0, Xt) is fed into two Linear layers with non-
Table 4: Examples of Neural CSDEs.
SDE Type	bi,σi
Vanilla Langevin Ornstein-Ulenbeck McKean-Vlasov	αi(t,xα) Vai(Xf), √2σi [μi — αi(Xα)],σi [E[αi(t,Xf)] - αi],σi
linearity units (i.e., LipSwish in Chen et al. (2019); Kidger et al. (2021)). Finally, the transformed
tensors are split into the control terms for drift and diffusion functions. The diffusion functions are
defined as non-degenerate types, where σi (t, Xt , αi) = Diag(zt) and zt is the output of the last
linear layer. The latent dimension of each Linear layer was set to 128 in all experiments except for
the prediction task with the Air Quality dataset (= 64). Thus, a total number of training parameters
for single control agent αi is ≈ 11K.
Simulation of CSDE and Temporal Privacy Function. Let T = t ∈ {tk}1≤k≤N for the pre-fixed
time interval ∆t . We apply the Euler-Maruyama scheme to approximately simulate the proposed
CSDE:
MM
Xtα+∆t = Xtα + X wi(t)bi(t, Xtα, αi(t, Xtα; θi))∆t + X wi(t)σi(t, Xtα, αi(t, Xtα; θi))Z, (34)
i=1	i=1
where Z，Z(0, √∆tId) is a d-dimensional Gaussian random variable with zero mean and covariance
√∆tId.
Analysis of Instability at Contact Points. In every time stamps t, drift and diffusion functions
are controlled over neural control agents αi where we assume that t- and t+ are adjacent points
of contact point t with infinitesimally small duration. The process At indicates drift integral term
and σsα denotes the diffusion term in our forward CSDE dynamics. As shown in the inequality, the
Markovian property is still preserved, and the magnitude of jumps are controlled by Lipschitzness of
drift/diffusion functions.
E	Xt- - Xt+2 |Ft-
≤ E[kAtk2 |Xt-] +E
J： kσαk2 ds∣Xt-
+E
/'+M∣∣2 ds|Xt- .
(35)
In a probabilistic point of view, the set of contact points may be regarded as measure-zero, and the
probabilistic evaluation is not changed.
18
Published as a conference paper at ICLR 2022
Figure 4 shows a particular example, where 14 temporal
states (i.e., |T| = 14) with 4 control agents are considered.
In the figure, the black line indicates the trajectory of
time series, blue dots denote the observed data points, and
shaded grey dots denote the missing data points. Each
control agent takes 5 data points, where 2 temporal states
are shared to other agents. In the experiments, the total
number of temporal privacy functions are maximally set
to M = |T|/2, where each control agent shares 2 points
for smooth transitions of stochastic dynamics.
A.7 Additional Empirical Study
Figure 4: The example of the temporal pri
vacy function.
0.35
0.30
7=0.95, MFcond + MBcon(Γ-
7= 1.0, MFcond
7= 0.0, MBcond
0	10	20	30	40	50	0	10	20	30	40	50
Epochs	Epochs
(a) Effect of hyper-parameter γ	(b) Effect of random stopping time (log-scaled)
Figure 5: Ablation study on the effectiveness of the proposed method.
Effect of hyper-parameter γ. In Figure 5-(a), the effect of hyper-parameter γ is shown. Similar to
Fig 2-(b), the results were obtained for the prediction task with the Air Quality dataset. Each red,
black, and blue line indicates the test MSE for different γ ∈ [0.0, 0.95, 1.0] over 50 epochs. If the
MFcond is deactivated during the training time i.e., γ = 0.0, only MBcond loss is utilized to train
the proposed CSDE-TP, and the model produces poor results. As our inference procedure requires
the model to train with multiple conditions, the obtained result seems obvious. If the MBcond loss
is deactivated during the training time i.e., γ = 1.0, multi-conditioned information in backward
dynamics Ztα are canceled, and the performance is decreased significantly i.e., 1.277 → 2.003. This
clearly shows that MBcond loss boost the performance.
Effect of random stopping time. In Figure 5-(b), the effect of strategy to select is shown. If we
select threshold e as an uniform random variable e 〜U[s, T] which is independent to Xt, then the
network quickly falls into instability as shown in the red line of Figure 5-(b). This shows that the
well-designed strategy for selecting threshold is crucial factor to stabilize the network learning
landscape. Contrary to the random sampling strategy, our method defined in Algorithm 1 select half
value of maximal MFcond loss in last learning steps as the threshold for random stopping time
(i.e., 1 max lk-1 → Ck). AS the threshold e is always bounded above the maximal loss in the last
steps, random stopping time at iteration k is decided in the time set:
Tk ∈{t : l(t,Tsatk) > 2 maxl(t,T；[-)},	(36)
where τsk denotes the stopping time at learning iteration k . If the network trains the MFcond loss so
that lk , l(t, Tsα,tk) → 0 as training proceeds k → ∞, then it is clear that the stopping time vanishes
τk→∞ ∈ 0. Thus, the strategy in (36) is well-defined.
A.8 Detailed Explanations of Markov dynamic programming with temporal
PRIVACY
For the clear explanation of proposed Markov-DP-TP, let us consider the detailed example. we
decompose sub-problem (B0) in (4) into another smaller sub-problems:
19
Published as a conference paper at ICLR 2022
inf E[J(u,Xuα)] =infE
α(-r)	β
'--------{--------}	、____
(B0)
Zu0
u
l(s, Xsα)ds + inf E[J(u0,Xuα0)],
β(-r0)
(37)
^{z
(C)
} '---------------
(C0)
}
where we set α(-r) = β, wr (s) = 1t≤s≤u. In this case, the problem (B) on interval [u, T] is
now decomposed into smaller sub-problems (C), (C0) on two intervals [u, u0] and [u0, T]. Similarly
to u in (4), another auxiliary time index u0 is considered here for additional problem (C). The
corresponding new temporal privacy function wr0 (s) = 1u≤s≤u0 is defined on the interval [u, u0].
By repeating temporal decomposition of original problem (A) M times, one can find the following
hierarchical relations:
•	P1). original problem, T = [t,…，T], α,no temporal privacy
•	P2). Two sub-problems (B) + (B0) in (4),
Time set, T = [t,…，u,…，T],
control agents, α = [αr, α(-r)],
temporal privacy functions = {wr}
•	P3). Three sub-problems (B) + (C) + (C0),
Time set, T = [t,…，u,…，u0,…，T],
control agents, α = [αr, β, β(-r0)],
temporal privacy functions = {wr, wr0 }
•	P4). M sub-problems, (A) + (B) + (C) + . . . ,
Time set, T = [t,…，TM,…，r * T-,…，T],
α = [α1, α2,…，αr,…,aM],
temporal privacy functions = {w1,w2, ∙∙∙ ,wr, ∙∙∙ , WM}
The role of U in (3) and (4) is replaced to U and u0 in (P3), and replaced to (r * TMt) in (P4) in the
table if the time interval is assumed to be regularly sampled. Similarly, the role of r in (3) and (4) is
replaced to r and r0 in (P3).
A.9 Toy Example on Synthetic Data
In this section, we conduct the reconstruction experiment on synthetic data to show the different
behaviors and demonstrate the advantages of the proposed CSDE compared to previous methods.
Stochastic Trigonometric Data. In this experiment, we define the 100-dimensional stochastic
process with composition of trigonometric functions (i.e., sin, cos) as follows:
Yt = S sin(5πt + Zιt)+0.25cos (∏~+ + Z21) + Z3 ∈ R100,	(38)
where we assume t ∈ [0, 1.0] and the total number of temporal states are set to 48 (i.e., |T| = 48). In
the definition of synthetic process Yt, both the period and amplitude are randomized with mean-zero
Gaussian random variables (i.e., Zi 〜N(0,1.0),Z2 〜N(0,2.0), Z3 〜N(0, 2Id)). With the
effect of Gaussian random variables, the process contains high volatility in both the spatial/temporal
axes. We compare our method to the auto-regressive ODE-RNN Rubanova et al. (2019) model using
the open-source code implemented by the authors. To observe the fundamental difference between
ODE-RNN and CSDE-TP, we stop the training procedure when the estimated MSEs of both models
attained the threshold (≤ .07). In Figures 6 and 7, the first axis of trigonometric data are visualized.
The results of each model are indicated as the blue lines (i.e., Xt) and the synthetic trigonometric
data is indicated as the red lines (i.e., Yt). The 95%-confidence regions (i.e., CR-95) of both the test
and predicted time-series are shown as red and blue shaded regions, respectively.
ODE-RNN. Figure 6 shows the results of the ODE-RNN model. Although the ODE-RNN model
attains relatively similar MSEs compared to the proposed model, there are two main issues in their
model to be discussed.
20
Published as a conference paper at ICLR 2022
1)	It hardly captures the vertical perturbation of test data induced by Z3 and the obtained result
produces a small variance at every temporal states.
2)	It hardly captures the horizontal perturbation of test data induced by Z1 , Z2, and the obtained
result produces the temporally unmatched trajectories.
These phenomenons occurred due to the deterministic property of the ODE-RNN model, where the
dynamical transition in their model is posed as the ODEs that cannot express the stochastic variation.
CSDE-TP. Figure 7 shows the result of the proposed CSDE-TP model and shows the advantages
of adopting the SDE in modelling stochastic dynamics. Compared to the results of the ODE-RNN,
the proposed method accurately captures both the vertical/horizontal perturbations and recover the
95% confidence region. It is clear that our CSDE-TP delicately expresses the complex volatility of
stochastic trajectories.
Discussions. As aforementioned in Section 4.3, experimental results on synthetic stochastic data
show that the MSE is not the best metric to train/evaluate the time-series models if there exists the
21
Published as a conference paper at ICLR 2022
high volatility in the dataset. In this case, distributional metrics such as MMD and Wasserstein
distance can be good substitutes for training/evaluating stochastic data.
A.10 Future Work
We plan to extend the proposed CSDE model to a general controlled Markov Ito-Levy jump diffusion
model (0ksendal & Sulem (2007)) to delicately express the complex time-series data. For example,
the proposed CSDE can be generalized to the Markov ItO-Levy jump diffusion of the following form:
dXtα = b(t, Xtα, α(θ))dt + σ(t, Xtα
α(θ))dWt +
Γ(t,Z)N(dt,dZ),
(39)
where N(t, z) = 0<s≤t Xz∈U (ηs - ηs- ), and Poisson random measure ηt. As the previous
work in Jia & Benson (2019) show the effectiveness of the jump process in modelling complex
discontinuous dynamics, we believe this generalization will produce the comparable results and
broaden our understanding in modelling dynamical systems for time-series data.
22