Published as a conference paper at ICLR 2022
Deep Learning without Shortcuts:
Shaping the Kernel with Tailored Rectifiers
Guodong Zhang1,2, Aleksandar Botev3, James Martens3
1University of Toronto, 2Vector Institute, 3DeepMind
gdzhang@cs.toronto.edu, {botev,jamesmartens}@google.com
Ab stract
Training very deep neural networks is still an extremely challenging task. The
common solution is to use shortcut connections and normalization layers, which
are both crucial ingredients in the popular ResNet architecture. However, there is
strong evidence to suggest that ResNets behave more like ensembles of shallower
networks than truly deep ones. Recently, it was shown that deep vanilla networks
(i.e. networks without normalization layers or shortcut connections) can be trained
as fast as ResNets by applying certain transformations to their activation functions.
However, this method (called Deep Kernel Shaping) isn’t fully compatible with
ReLUs, and produces networks that overfit significantly more than ResNets on
ImageNet. In this work, we rectify this situation by developing a new type of
transformation that is fully compatible with a variant of ReLUs - Leaky ReLUs.
We show in experiments that our method, which introduces negligible extra com-
putational cost, achieves validation accuracies with deep vanilla networks that are
competitive with ResNets (of the same width/depth), and significantly higher than
those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the
validation accuracies we obtain do not get worse with depth.
1	Introduction
Thanks to many architectural and algorithmic innova-
tions, the recent decade has witnessed the unprecedented
success of deep learning in various high-profile chal-
lenges, e.g., the ImageNet recognition task (Krizhevsky
et al., 2012), the challenging board game of Go (Sil-
ver et al., 2017) and human-like text generation (Brown
et al., 2020). Among them, shortcut connections (He
et al., 2016a; Srivastava et al., 2015) and normalization
layers (Ioffe & Szegedy, 2015; Ba et al., 2016) are two
architectural components of modern networks that are
critically important for achieving fast training at very
high depths, and feature prominently in the ubiquitous
ResNet architecture of He et al. (2016b).
0.75
0.70-
B 0.65-
O
ro
W 0.60-
ro
>
0.55-
0.50-
Figure 1: Top-1 ImageNet validation accuracy
of vanilla deep networks initialized using either
EOC (with ReLU) or TAT (with LReLU) and
trained with K-FAC.
0
20000
40000	60000	80000	100000
Iterations
Despite the success of ResNets, there is significant evidence to suggest that the primary reason they
work so well is that they resemble ensembles of shallower networks during training (Veit et al., 2016),
which lets them avoid the common pathologies associated with very deep networks (e.g. Hochreiter
et al., 2001; Duvenaud et al., 2014). Moreover, ResNets without normalization layers could lose
expressivity as the depth goes to infinity (Hayou et al., 2021). In this sense, the question of whether
truly deep networks can be efficient and effectively trained on challenging tasks remains an open one.
As argued by Oyedotun et al. (2020) and Ding et al. (2021), the multi-branch topology of ResNets
also has certain drawbacks. For example, it is memory-inefficient at inference time, as the input to
every residual block has to be kept in memory until the final addition. In particular, the shortcut
branches in ResNet-50 account for about 40% of the memory usage by feature maps. Also, the
classical interpretation of why deep networks perform well - because of the hierarchical feature
representations they produce - does not strictly apply to ResNets, due to their aforementioned
tendency to behave like ensembles of shallower networks. Beyond the drawbacks of ResNets,
training vanilla deep neural networks (which we define as networks without shortcut connections or
1
Published as a conference paper at ICLR 2022
normalization layers) is an interesting research problem in its own right, and finding a solution could
open the path to discovering new model architectures. However, recent progress in this direction has
not fully succeeded in matching the generalization performance of ResNets.
Schoenholz et al. (2017) used a mean-field analysis of deep MLPs to choose variances for the initial
weights and bias parameters, and showed that the resulting method - called Edge of Chaos (EOC)
-allowed vanilla networks to be trained at very high depths on small datasets. Building on EOC,
and incorporating dynamical isometry theory, Xiao et al. (2018) was able to train vanilla networks
with Tanh units1 at depths of up to 10,000. While impressive, these EOC-initialized networks
trained significantly slower than standard ResNets of the same depth, and also exhibited significantly
worse generalization performance. Qi et al. (2020) proposed to enforce the convolution kernels to
be near isometric, but the gaps with ResNets are still significant on ImageNet. While Oyedotun
et al. (2020) was able to narrow the generalization gap between vanilla networks and ResNets, their
experiments were limited to networks with only 30 layers, and their networks required many times
more parameters. More recently, Martens et al. (2021) introduced a method called Deep Kernel
Shaping (DKS) for initializing and transforming networks based on an analysis of their initialization-
time kernel properties. They showed that their approach enabled vanilla networks to train faster
than previous methods, even matching the speed of similarly sized ResNets when combined with
stronger optimizers like K-FAC (Martens & Grosse, 2015) or Shampoo (Anil et al., 2020). However,
their method isn’t fully compatible with ReLUs, and in their experiments (which focused on training
speed) their networks exhibited significantly more overfitting than ResNets.
Inspired by both DKS and the line of work using mean-field theory, we propose a new method
called Tailored Activation Transformation (TAT). TAT inherits the main advantages of DKS, while
working particularly well with the “Leaky ReLU” activation function. TAT enables very deep vanilla
neural networks to be trained on ImageNet without the use of any additional architectural elements,
while only introducing negligible extra computational cost. Using TAT, we demonstrate for the first
time that a 50-layer vanilla deep network can nearly match the validation accuracy of its ResNet
counterpart when trained on ImageNet. And unlike with the EOC method, validation accuracy we
achieve does not decrease with depth (see Figure 1). Furthermore, TAT can also be applied to ResNets
without normalization layers, allowing them to match or even exceed the validation accuracy of
standard ResNets of the same width/depth. A multi-framework open source implementation of DKS
and TAT is available at https://github.com/deepmind/dks.
2	Background
Our main tool of analysis will be kernel functions for neural networks (Neal, 1996; Cho & Saul,
2009; Daniely et al., 2016) and the related Q/C maps (Saxe et al., 2013; Poole et al., 2016; Martens
et al., 2021). In this section, we introduce our notation and some key concepts used throughout.
2.1	Kernel Function Approximation for Wide Networks
For simplicity, we start with the kernel function approximation for feedforward fully-connected
networks, and discuss its extensions to convolutional networks and non-feedforward networks later.
In particular, we will assume a network that is defined by a sequence of L combined layers (each of
which is an affine transformation followed by the elementwise activation function φ) as follows:
xl+1 = φ (Wlxl + bi) ∈ Rdl+1,	(1)
with weights Wl ∈ Rdl+1×dl initialized as Wl 吧 N(0,1/dl) (or scale-corrected uniform orthogonal
matrices (Martens et al., 2021)), and biases bl ∈ Rdl+1 initialized to zero. Due to the randomness
of the initial parameters θ, the network can be viewed as random feature model fθ (x)，x' at each
layer l (with x0 = x) at initialization time. This induces a random kernel defined as follows:
Klf (x1,x2 ) = fθ (xl)>fθ (x2)/dl.	(2)
Given these assumptions, as the width of each layer goes to infinity, κlf (x1, x2) converges in proba-
bility (see Theorem 3) to a deterministic kernel kf (x1,x2) that can be computed layer by layer:
ςi+i=Ez〜N(o,∑ι)[°(Z)φ(Z)], with ςi = [Kf(xι,χ2) ?f(χ2,x2)],⑶
where Kf (xι, x2) = κf (xι, x2) = x>x2∕d0.
1Dynamical isometry is unavailable for ReLU (Pennington et al., 2017), even with orthogonal weights.
2
Published as a conference paper at ICLR 2022
2.2	Local Q/C maps
By equation 3, any diagonal entry qil+1 of Σl+1 only depends on the corresponding diagonal entry qil
of Σl. Hence, we obtain the following recursion for these diagonal entries, which we call q values:
qi+1 = Qgi)= Ez〜N(0,ql)[φ(Z)2] = Ez〜N(0,1) (KqliZ)2 , with q0 = llxik2/d0	⑷
where Q is the local Q map. We note that qil is an approximation of κlf (xi, xi). Analogously, one
can write the recursion for the normalized off-diagonal entries, which we call c values, as:
cl+1 = C(cl, q1l , q2l ) =
E[z1 ]〜N(0,∑i) [φ(ZI)φ(Z2)]
PQ(q1 )Q(q2)
, with Σl
(5)
where C is the local C map and c0 = x1>x2/d0. We note that cl is an approximation of the cosine
similarity between fθl (x1) and fθl (x2). Because C is a three dimensional function, it is difficult to
analyze, as the associated q values can vary wildly for distinct inputs. However, by scaling the inputs
to have norm √do, and rescaling φ so that Q(1) = 1, it follows that q； = 1 for all l. This allows us
to treat C as a one dimensional function from [-1, 1] to [-1, 1] satisfying C(1) = 1. Additionally, it
can be shown that C possesses special structure as a positive definite function (see Appendix A.4 for
details). Going forward, we will thus assume that qi0 = 1, and that φ is scaled so that Q(1) = 1.
2.3	Extensions to convolutional networks and more complex topologies
As argued in Martens et al. (2021), Q/C maps can also be defined for convolutional networks if one
adopts a Delta initialization (Balduzzi et al., 2017; Xiao et al., 2018), in which all weights except those
in the center of the filter are initialized to zero. Intuitively, this makes convolutional networks behave
like a collection of fully-connected networks operating independently over feature map locations. As
such, the Q/C map computations for a feed-forward convolutional network are the same as above.
Martens et al. (2021) also gives formulas to compute q and c values for weighted sum operations
between the outputs of multiple layers (without nonlinearities), thus allowing more complex network
topologies. In particular, the sum operation’s output q value is given by q = Pin=1 wi2qi, and its
output C value is given by ɪ Pn= ι WlqiCi. In order to maintain the property that all q values are 1 in
the network, we will assume that sum operations are normalized in the sense that Pin=1 wi2 = 1.
Following Martens et al. (2021), we will extend the definition of Q/C maps to include global Q/C
maps, which describe the behavior of entire networks. Global maps, denoted by Qf and Cf for a
given network f, can be computed by applying the above rules for each layer in f. For example, the
global C map of a three-layer network f is simply Cf (c) = C ◦ C ◦ C(c). Like the local C map, global
C maps are positive definite functions (see Appendix A.4). In this work, we restrict our attention to
the family of networks comprising of combined layers, and normalized sums between the output of
multiple affine layers, for which we can compute global Q/C maps. And all of our formal results will
implicitly assume this family of networks.
2.4	Q/C maps for rescaled ResNets
ResNets consist of a sequence of residual blocks, each of which computes the sum of a residual
branch (which consists of a small multi-layer convolutional network) and a shortcut branch (which
copies the block’s input). In order to analyze ResNets we will consider the modified version used
in Shao et al. (2020) and Martens et al. (2021) which removes the normalization layers found in
the residual branches, and replaces the sum at the end of each block with a normalized sum. These
networks, which we will call rescaled ResNets, are defined by the following recursion:
xl+1 = wxl + pl - w2R(xl),	(6)
whereR is the residual branch, and w is the shortcut weight (which must be in [-1, 1]). Applying
the previously discussed rules for computing Q/C maps, we get qil = 1 for all l and
cl+1 = w2cl + (1 - w2)CR(cl ).	(7)
3	Existing Solutions and Their Limitations
Global Q/C maps can be intuitively understood as a way of characterizing signal propagation through
the network f at initialization time. The q value approximates the squared magnitude of the activation
3
Published as a conference paper at ICLR 2022
vector, so that Qf describe the contraction or expansion of this magnitude through the action of f .
On the other hand, the c value approximates the cosine similarity of the function values for different
inputs, so that Cf describes how well f preserves this cosine similarity from its input to its output.
Standard initializations methods (LeCun et al., 1998; Glorot & Bengio, 2010; He et al., 2015) are
motivated through an analysis of how the variance of the activations evolves throughout the network.
This can be viewed as a primitive form of Q map analysis, and from that perspective, these methods
are trying to ensure that q values remain stable throughout the network by controlling the local Q map.
This is necessary for trainability, since very large or tiny q values can cause numerical issues, saturated
activation functions (which have implications for C maps), and problems with scale-sensitive losses.
However, as was first observed by Schoenholz et al. (2017), a well-behaved C map is also necessary
for trainability. When the global C map is close to a constant function (i.e. degenerate) on (-1, 1),
which easily happens in deep networks (as discussed in Appendix A.2), this means that the network’s
output will appear either constant or random looking, and won’t convey any useful information about
the input. Xiao et al. (2020) and Martens et al. (2021) give more formal arguments for why this leads
to slow optimization and/or poor generalization under gradient descent.
Several previous works (Schoenholz et al., 2017; Yang &
Schoenholz, 2017; Hayou et al., 2019) attempt to achieve
a well-behaved global C map by choosing the variance
of the initial weights and biases in each layer such that
C0(1) = 1 - a procedure which is referred to as Edge of
Chaos (EOC). However, this approach only slows down
the convergence (with depth) of the c values from exponen-
tial to sublinear (Hayou et al., 2019), and does not solve the
fundamental issue of degenerate global C maps for very
deep networks. In particular, the global C map of a deep
network with ReLU and EOC initialization rapidly concen-
trates around 1 as depth increases (see Figure 2). While
EOC allows very deep vanilla networks to be trained, the
training speed and generalization performance is typically
ReLU with depth 10
----ReLU with depth 50
TReLU with depth 2
TReLU with depth 10
TReLU with depth 50
TReLU with depth 1000
0.0 J-----1------1------1-----.------1------1-----1------
-1.00 -0.75 -0.50 -0.25 0.00	0.25	0.50	0.75	1.00
input c value
Figure 2: Global C maps for ReLU networks
(EOC) and TReLU networks (Cf (0) = 0.5).
The global C map of a TReLU network con-
verges to a well-behavior function as depth
increases (proved in Proposition 3).
much worse than for comparable ResNets. Klambauer et al. (2017) applied an affine transformation
to the output of activation functions to achieve Q(1) = 1 and C(0) = 0, while Lu et al. (2020) applied
them to achieve Q(1) = 1 and C0(1) = 1, although the effect of both approaches is similar to EOC.
To address these problems, Martens et al. (2021) introduced DKS, which enforces the conditions
Cf (0) = 0 and Cf0 (1) = ζ (for some modest constant ζ > 1) directly on the network’s global C map
Cf. They show that these conditions, along with the positive definiteness of C maps, cause Cf to be
close to the identity and thus well-behaved. In addition to these C map conditions, DKS enforces
that Q(1) = 1 and Q0(1) = 1, which lead to constant q values of 1 in the network, and lower kernel
approximation error (respectively). DKS enforces these Q/C map conditions by applying a model
class-preserving transformation φ(x) = γ(φ(αx + β) + δ). with non-trainable parameters α, β, γ
and δ. The hyperparameter ζ is chosen to be sufficiently greater than 1 (e.g. 1.5) in order to prevent
the transformed activation functions from looking “nearly linear” (as they would be exactly linear if
ζ = 1), which Martens et al. (2021) argue makes it hard for the network to achieve nonlinear behavior
during training. Using DKS, they were able to match the training speed of ResNets on ImageNet with
vanilla networks using K-FAC. However, DKS is not fully compatible with ReLUs, and the networks
in their experiments fell substantially short of ResNets in terms of generalization performance.
4	Tailored Activation Transformation (TAT)
The reason why DKS is not fully compatible with ReLUs is that they are positive homogeneous,
i.e. φ(αx) = αφ(x) for α ≥ 0. This makes the γ parameter of the transformed activation function
redundant, thus reducing the degrees of freedom with which to enforce DKS’s four Q/C map
conditions. Martens et al. (2021) attempt to circumvent this issue by dropping the condition Q0(1) =
1, which leads to vanilla deep networks that are trainable, but slower to optimize compared to using
DKS with other activation functions. This is a significant drawback for DKS, as the best generalizing
deep models often use ReLU-family activations. We therefore set out to investigate other possible
remedies - either in the form of different activation functions, new Q/C map conditions, or both. To
this end, we adopt a ReLU-family activation function with an extra degree of freedom (known as
“Leaky ReLU”), and modify the Q/C map conditions in order to preserve certain desirable properties
4
Published as a conference paper at ICLR 2022
Table 1: Comparison of different methods applied to					a network f .		
EOC (smooth)	EOC (LReLU)	DKS		TAT (smooth)		TAT (LReLU)	
q∞ exists	Q(q) = q	Q⑴= Q0(1) =	1 1	Q(1) = Q0(1) =	1 1	Q(q) Cf0 (1) Cf(0)	q 1 η
C0(1, q∞,q∞) = 1	C0(1) = 1	Cf (0) = Cf⑴=	0 ζ	Cf0 (1) = Cf00(1) =	1 τ		
of this choice. The resulting method, which we name Tailored Activation Transformation (TAT)
achieves competitive generalization performance with ResNets in our experiments.
4.1	Tailored Activation Transformation for Leaky ReLUs
One way of addressing the issue of DKS’s partial incompatibility with ReLUs is to consider a slightly
different activation function - namely the Leaky ReLU (LReLU) (Maas et al., 2013):
φα(x) = max{x, 0} + α min{x, 0},	(8)
where α is the negative slope parameter. While using LReLUs with α 6= 0 in place of ReLUs changes
the model class, it doesn’t limit the model’s expressive capabilities compared to ReLU, as assuming
α 6= ±1, one can simulate a ReLU network with a LReLU network of the same depth by doubling
the number of neurons (see Proposition 4). Rather than using a fixed value for a, We will use it as
an extra parameter to satisfy our desired Q/C map conditions. Define φα(x) = Jι+⅛φα(x). By
Lemma 1, the local Q and C maps for this choice of activation function are:
Q(q) = q and C(C)= C + ∏11+α)2) (pl - c2 - CCosT(c)) .	(9)
Note that the condition Q(q) = q is actually stronger than DKS’s Q map conditions (Q(1) = 1 and
Q0(1) = 1), and has the potential to reduce kernel approximation errors in finite width networks
compared to DKS, as it provides a better guarantee on the stability of Qf w.r.t. random perturbations
of the q values at each layer. Additionally, because the form of C does not depend on either of the
layer’s input q values, it won’t be affected by such perturbations at all. (Notably, if one uses the
negative slope parameter to transform LReLUs with DKS, these properties will not be achieved.) In
support of these intuitions is the fact that better bounds on the kernel approximation error exist for
ReLU networks than for general smooth ones (as discussed in Appendix A.1).
Another consequence of using φα(x) for our activation function is that we have C0(1) = 1 as in EOC.
If combined with the condition C(0) = 0 (which is used to achieve Cf (0) = 0 in DKS) this would
imply by Theorem 1 that C is the identity function, which by equation 9 is only true when α = 1,
thus resulting in a linear network. In order to avoid this situation, and the closely related one where
φα appears “nearly linear”, we instead choose the value of α so that Cf (0) = η, for a hyperparameter
0 ≤ η ≤ 1. As shown in the following theorem, η controls how close Cf is to the identity, thus
allowing us to achieve a well-behaved global C map without making φα nearly linear:
Theorem 1.	For a network f with φα (x) as its activation function (with α ≥ 0), we have
max |Cf (C) - C| ≤ min {4Cf (0), 1 + Cf (0)} , max Cf0 (C) - 1 ≤ min {4Cf (0), 1} (10)
Another motivation for using φα (x) as an activation function is given by the following proposition:
Proposition 1. The global C map of a feedforward network with φα(x) as its activation function
is equal to that of a rescaled ResNet of the same depth (see Section 2.4) with normalized ReLU
activation φ(x) = √2max(x, 0), shortcut weight J1+⅛, and residual branch R consisting of a
combined layer (or just a normalized ReLU activation) followed by an affine layer.
This result implies that at initialization, a vanilla network using φα behaves similarly to a ResNet, a
property that is quite desirable given the success that ResNets have already demonstrated.
In summary, we have the following three conditions:
Q(q) = q, Cf0 (1) = 1, Cf (0) = η,	(11)
which we achieve by picking the negative slope parameter α so that Cf (0) = η. We define the
Tailored Rectifier (TReLU) to be φα with α chosen in this way. Note that the first two conditions are
5
Published as a conference paper at ICLR 2022
also true when applying the EOC method to LReLUs, and its only the third which sets TReLU apart.
While this might seem like a minor difference, it actually matters a lot to the behavior of the global C
map. This can be seen in Figure 2 where the c value quickly converges towards 1 with depth under
EOC, resulting in a degenerate global C map. By contrast, the global C map of TReLU for a fixed
η converges rapidly to a nice function, suggesting a very deep vanilla network with TReLU has the
same well-behaved global C map as a shallow network. We prove this in Proposition 3 by showing
the local C map in equation 9 converges to an ODE as we increase the depth. For direct comparison
of all Q/C map conditions, we refer the readers to Table 1.
For the hyperparameter 0 ≤ η ≤ 1, we note that a value very close to 0 will produce a network that
is “nearly linear”, while a value very close to 1 will give rise to a degenerate C map. In practice we
use η = 0.9 or 0.95, which seems to work well in most settings. Once we decide on η, we can solve
the value α using binary search by exploiting the closed-form form of C in equation 9 to efficiently
compute Cf (0). For instance, if f is a 100 layer vanilla network, one can compute Cf (0) as follows:
100 times
z-----A-----{
Cf (0) = CoC∙∙∙CoC(0),	(12)
which is a function of α. This approach can be generalized to more advanced architectures, such as
rescaled ResNets, as discussed in Appendix B.
4.2 Tailored Activation Transformation for Smooth Activation Functions
Unlike LReLU, most activation functions don’t have closed-form formulas for their local C maps. As
a result, the computation of Cf (0) involves the numerical approximation of many two-dimensional
integrals to high precision (as in equation 5), which can be quite expensive. One alternative way to
control how close Cf is to the identity, while maintaining the condition Cf0 (1) = 1, is to modulate its
second derivative Cf00(1). The validity of this approach is established by the following theorem:
Theorem 2.	Suppose f is a network with a smooth activation function. If Cf0 (1)
c∈m[-a1x,1] |Cf (c) - c| ≤ 2Cf00(1), c∈m[-a1x,1] Cf0 (c) - 1 ≤ 2Cf00(1)
1, then we have
(13)
Given C(1) = 1 and C0(1) = 1, a straightforward computation shows that Cf00(1) = LC00(1) if f is an
L-layer vanilla network. (See Appendix B for a discussion of how to do this computation for more
general architectures.) From this we obtain the following four local Q/C map conditions:
Q(1) = 1,	Q0(1) = 1,	C00(1)= t/l, C 0(1) = 1.	(14)
To achieve these we adopt the same activation transformation as DKS: φ(x) = γ(φ(αx + β) + δ) for
non-trainable scalars α, β, δ, and γ. We emphasize that these conditions cannot be used with LReLU,
as LReLU networks have C00(1) = ∞. By equation 4 and basic properties of expectations, we have
1	= Q(I)= Ez〜N(0,1) hB(Z)2i = Y2Ez〜N(0,1) [(Ogz + β) + δ)2]	(15)
-1/2
So that γ = Ez〜N(0,1)[(φ(az + β) + δ)2]	. To obtain the values for α, β and δ, We can treat
the remaining conditions as a three-dimensional nonlinear system, which can be written as follows:
Ez 〜N (0,1) [Φ(z)B0(z)z∣ = Q0(1) = 1,
Ez〜N(0,1) hφ00(Z)2] = COO(I) = τ∕L, Ez〜N(0,1) hφ0(z)2i = C0⑴=1∙
(16)
We do not have a closed-form solution of this system. HoWever, each expectation is a one dimensional
integral, and so can be quickly evaluated to high precision using Gaussian quadrature. One can then
use black-box nonlinear equation solvers, such as modified PoWell’s method (PoWell, 1964), to obtain
a solution. See https://github.com/deepmind/dks for a complete implementation.
5	Experiments
Our main experimental evaluation of TAT and competing approaches is on training deep convolutional
netWorks for ImageNet classification (Deng et al., 2009). The goal of these experiments is not to
achieve state-of-the-art, but rather to compare TAT as fairly as possible With existing methods, and
standard ResNets in particular. To this end, We use ResNet V2 (He et al., 2016b) as the main reference
6
Published as a conference paper at ICLR 2022
architecture, from which we obtain rescaled ResNets (by removing normalization layers and weighing
the branches as per equation 6), and vanilla networks (by further removing shortcuts). For networks
without batch normalization, we add dropout to the penultimate layer for regularization, as was done
in Brock et al. (2021b). We train the models with 90 epochs and a batch size of 1024, unless stated
otherwise. For TReLU, we obtain η by grid search in {0.9, 0.95}. The weight initialization used
for all methods is the Orthogonal Delta initialization, with an extra multiplier given by σw . We
initialize biases iid from N(0, σb2). We use (σw, σb) = (1, 0) in all experiments (unless explicitly
stated otherwise), with the single exception that We use (σw, σb) = (√2,0) in standard ResNets, as
per standard practice (He et al., 2015). For all other details see Appendix D.
5.1	Towards removing batch normalization
Two crucial components for the successful training of very deep neural networks are shortcut
connections and batch normalization (BN) layers. As argued in De & Smith (2020) and Shao et al.
(2020), BN implicitly biases the residual blocks toward the identity function, which makes the
network better behaved at initialization time, and thus easier to train. This suggests that one can
compensate for the removal of BN layers, at least in terms of their effect on the behaviour of the
network at initialization time, by down-scaling the residual branch of each residual block. Arguably,
almost all recent work on training deep networks without normalization layers (Zhang et al., 2018;
Shao et al., 2020; Bachlechner et al., 2020; Brock et al., 2021a;b) has adopted this idea by introducing
multipliers on the residual branches (which may or may not be optimized during training).
In Table 2, we show that one can close most of the gap with standard ResNets	Table 2: Top-1 validation accuracy of rescaled ResNet50 with						
	varying shortcut weights. We set η =			0.9 for TReLU.			
by simply adopting the modification	Optimizer	Standard	Activation	Rescaled ResNet (w)			
in equation 6 without using BN lay- ers. By further replacing ReLU with TReLU, we can exactly match the per- formance of standard ResNets. With K-FAC as the optimizer, the rescaled ResNet with shortcut weight w = 0.9 is only 0.5 shy of the validation accur with TReLU, we match the performan		ResNet		0.0	0.5	0.8	0.9
	K-FAC	76.4	ReLU TReLU	72.6 74.6	74.5 75.5	75.6 76.4	75.9 75.9
	SGD	76.3	ReLU TReLU	63.7 71.0	72.4 72.6	73.9 76.0	75.0 74.8
	acy (76.4) of the standard ResNet. Further replacing ReLU e of standard ResNet with shortcut weight w = 0.8.						
5.2 The difficulty of removing shortcut connections
While the aforementioned works have shown that it
is possible to achieve competitive results without nor-
malization layers, they all rely on the use of shortcut
connections to make the network look more linear at
initialization. A natural question to ask is whether
normalization layers could compensate for the re-
moval of shortcut connections. We address this ques-
tion by training shortcut-free networks with either
Table 3: ImageNet top-1 validation accuracies of
shortcut-free networks on ImageNet.
Depth	Optimizers	vanilla	BN	LN
50	K-FAC	726	72.8	72.7
	SGD	63.7	72.6	58.1
101	K-FAC	71.8	67.6	72.0
	SGD	41.6	43.4	28.6
BN or Layer Normalization (LN) layers. As shown in Table 3, these changes do not seem to make a
significant difference, especially with strong optimizers like K-FAC. These findings are in agreement
with the analyses of Yang et al. (2019) and Martens et al. (2021), who respectively showed that deep
shortcut-free networks with BN layers still suffer from exploding gradients, and deep shortcut-free
networks with LN layers still have degenerate C maps.
5.3	Training Deep Neural Networks without Shortcuts
The main motivation for developing TAT is to help deep vanilla networks achieve generalization
performance similar to standard ResNets. In our investigations we include rescaled ResNets with
a shortcut weight of either 0 (i.e. vanilla networks) or 0.8. In Table 4 we can see that with a strong
optimizer like K-FAC, we can reduce the gap on the 50 layer network to only 1.8% accuracy when
training for 90 epochs, and further down to 0.6% when training for 180 epochs. For 101 layers, the
gaps are 3.6% and 1.7% respectively, which we show can be further reduced with wider networks
(see Table 9). To our knowledge, this is the first time that a deep vanilla network has been trained
to such a high validation accuracy on ImageNet. In addition, our networks have fewer parameters
and run faster than standard ResNets, and use less memory at inference time due to the removal of
7
Published as a conference paper at ICLR 2022
Table 4: ImageNet top-1 validation accuracy. For rescaled ResNets (w = 0.0 or w = 0.8), we do not include
any normalization layer. For standard ResNets, batch normalization is included. By default, ReLU activation is
used for standard ResNet while we use TReLU for rescaled networks.
Depth	Optimizer	ResNet	90 epochs w = 0.0	w = 0.8	ResNet	180 epochs w = 0.0	w=0.8
50	K-FAC	76.4	74.6	76.4	76.6	76.0	77.0
	SGD	76.3	71.0	76.0	76.6	72.3	76.8
101	K-FAC	77.8	74.2	77.8	77.6	75.9	78.4
	SGD	77.9	70.0	77.3	77.6	73.8	77.4
shortcut connections and BN layers. The gaps when using SGD as the optimizer are noticeably larger,
which we further explore in Section 5.5. Lastly, using rescaled ResNets with a shortcut weight of 0.8
and TReLU, we can exactly match or even surpass the performance of standard ResNets.
5.4	Comparisons with existing approaches
Comparison with EOC. Our first comparison
is between TAT and EOC on vanilla deep net-
works. For EOC with ReLUs we set (σw , σb) =
(√2,0) to achieve Q(1) = 1 as in He et al.
(2015), since ReLU networks always satisfy
C0(1) = 1 whenever σb = 0. For Tanh acti-
vations, a comprehensive comparison with EOC
is more difficult, as there are infinitely many
choices of (σw, σb) that achieve C0(1) = 1.
Here we use (σw, σb) = (1.302, 0.02)2, as sug-
gested in Hayou et al. (2019). In Table 5, we can
see that in all the settings, networks constructed
Table 5: ImageNet top-1 validation accuracy compari-
son between EOC and TAT on deep vanilla networks.
Depth	Optimizer	Method	(L)ReLU	Tanh
	K-FAC	EOC	72.6	70.6
50		TAT	74.6	73.1
	SGD	EOC	63.7	55.7
		TAT	71.0	69.5
	K-FAC	EOC	71.8	69.2
101		TAT	74.2	72.8
	SGD	EOC	41.6	54.0
		TAT	70.0	69.0
with TAT outperform EOC-initialized networks by a significant margin, especially when using SGD.
Another observation is that the accuracy of EOC-initialized networks drops as depth increases.
Comparison with DKS. The closest approach to TAT in the existing literature is DKS, whose
similarity and drawbacks are discussed in Section 4. We compare TAT to DKS on both LReLUs3,
and smooth functions like the SoftPlus and Tanh. For smooth activations, we perform a grid search
over {0.2, 0.3, 0.5} for τ in TAT, and {1.5, 10.0, 100.0} for ζ in DKS, and report only the best
performing one. From the results shown in Table 7, we observe that TAT, together with LReLU
(i.e. TReLU), performs the best in nearly all settings we tested, and that its advantage becomes larger
when we remove dropout. One possible reason for the superior performance of TReLU networks is
the stronger Q/C map conditions that they satisfy compared to other activations (i.e. Q(q) = q for all
q vs Q(1) = 1 and Q0(1) = 1, and invariance of C to the input q value), and the extra resilience to
kernel approximation error that these stronger conditions imply. In practice, we found that TReLU
indeed has smaller kernel approximation error (compared to DKS with smooth activation functions,
see Appendix E.1) and works equally well with Gaussian initialization (see Appendix E.7).
Comparison with PReLU. The Parametric
ReLU (PReLU) introduced in He et al. (2015)
differs from LReLU by making the negative
slope a trainable parameter. Note that this is
distinct from what we are doing with TReLU,
since there we compute the negative slope pa-
rameter ahead of time and fix it during training.
In our comparisons with PReLU we consider
two different initializations: 0 (which recovers
Table 6: Comparison with PReLU.				
Depth	Optimizer	TReLU	PReLU0.0	PReLU0.25
50	K-FAC	746	72.5	73.6
	SGD	71.0	667	67.9
101	K-FAC	742	71.9	72.8
	SGD	70.0	543	66.3
the standard ReLU), and 0.25, which was used in
He et al. (2015). We report the results on deep vanilla networks in Table 6 (see Appendix E.6 for
results on rescaled ResNets). For all settings, our method outperforms PReLU by a large margin,
emphasizing the importance of the initial negative slope value. In principle, these two methods can
be combined together (i.e. we could first initialize the negative slope parameter with TAT, and then
optimize it during training), however we did not see any benefit from doing this in our experiments.
2We also ran experiments with (σw, σb) = (1.0, 0.0), and the scheme described in Pennington et al. (2017)
and Xiao et al. (2018) for dynamical isometry. The results were worse than those reported in the table.
3
3For DKS, we set the negative slope as a parameter and adopt the transformation φ(x) = γ(φα(x + β) + δ).
8
Published as a conference paper at ICLR 2022
Table 7: Comparisons between TAT and DKS. The numbers on the right hand of / are results without dropout.
The methods With * are introduced in this paper.
Depth	Optimizer	Shortcut Weight		TAT			DKS		
				LReLU*	SoftPlus*	Tanh*	LReLU*	SOftPlUs	Tanh
	K-FAC	w	0.0	74.6/74.2	74.4/74.2	73.1/72.9	74.3/74.3	74.3/73.7	72.9/72.9
50		w	0.8	76.4/75.9	76.4/75.0	74.8/74.4	76.2/76.2	76.3/75.1	74.7/74.5
	SGD	w	0.0	71.1/71.1	70.2/70.0	69.5/69.5	70.4/70.4	71.8/71.4	69.2/69.2
		w	0.8	76.0/75.8	74.3/73.8	72.4/72.2	73.4/73.0	75.2/74.1	72.8/72.8
	K-FAC	w	0.0	74.2/74.2	74.1/73.4	72.8/72.5	73.5/73.5	73.9/73.1	72.5/72.4
101		w	0.8	77.8/77.0	76.6/75.7	75.8/75.1	76.8/76.7	76.8/75.6	75.9/75.7
	SGD	w	0.0	70.0/70.0	70.3/68.8	69.0/67.8	68.3/68.3	68.3/68.3	69.8/69.8
		w	0.8	77.3/76.0	75.3/75.3	73.8/73.5	74.9/74.6	76.3/75.1	74.6/74.6
5.5 The role of the optimizer
One interesting phenomenon We observed
in our experiments, Which echoes the find-
ings of Martens et al. (2021), is that a strong
optimizer such as K-FAC significantly out-
performs SGD on vanilla deep netWorks in
terms of training speed. One plausible expla-
Table 8: Batch size scaling.
Optimizer	128	256	Batch size			
			512	1024	2048	4096
K-FAC	74.5	74.4	74.5	74.6	74.2	72.0
SGD	72.7	72.6	72.7	71.0	69.3	62.0
LARS	72.4	72.3	72.6	71.8	71.3	70.2
nation is that K-FAC Works better than SGD in the large-batch setting, and our default batch size of
1024 is already beyond SGD’s “critical batch size”, at Which scaling efficiency begins to drop. Indeed,
it Was shoWn by Zhang et al. (2019) that optimization algorithms that employ preconditioning, such
as Adam and K-FAC, result in much larger critical batch sizes.
To investigate this further, We tried batch sizes betWeen
128 and 4096 for training 50-layer vanilla TReLU net-
Works. As shoWn in Table 8, K-FAC performs equally
Well for all different batch sizes except 4096 (Where We
see increased overfitting), While the performance of SGD
starts to drop When We increase the batch size past 512.
Surprisingly, We observe a similar trend for the LARS
optimizer (You et al., 2019), Which Was designed for large-
batch training. Even at the smallest batch size We tested
(128), K-FAC still outperforms SGD by a gap of 1.8%
Within our standard epoch budget. We conjecture the rea-
Figure 3: Training speed comparison be-
tWeen K-FAC and SGD on 50 layer vanilla
TReLU netWork.
son behind this to be that vanilla netWorks Without normalization and shortcuts give rise to loss
landscapes With Worse curvature properties compared to ResNets, and that this sloWs doWn simpler
optimizers like SGD. To investigate further, We also ran SGD (With a batch size of 512) and K-FAC
for up to 360 epochs With a “one-cycle” cosine learning rate schedule (Loshchilov & Hutter, 2016)
that decreases the learning rate to to 0 by the final epoch. As shoWn in Figure 3, SGD does indeed
eventually catch up With K-FAC (using cosine scheme), requiring just over double the number
of epochs to achieve the same validation accuracy. While one may argue that K-FAC introduces
additional computational overhead at each step, thus making a head-to-head comparison versus SGD
unfair, We note that this overhead can amortized by not updating K-FAC’s preconditioner matrix at
every step. In our experiments We found that this strategy alloWed K-FAC to achieve a similar per-step
runtime to SGD, While retaining its optimization advantage on vanilla netWorks. (See Appendix E.3.)
6	Conclusions
In this Work We considered the problem of training and generalization in vanilla deep neural netWorks
(i.e. those Without shortcut connections and normalization layers). To address this We developed a
novel method that modifies the activation functions in a Way tailored to the specific architecture, and
Which enables us to achieve generalization performance on par With standard ResNets of the same
Width/depth. Unlike the most closely related approach (DKS), our method is fully compatible With
ReLU-family activation functions, and in fact achieves its best performance With them. By obviating
the need for shortcut connections, We believe our method could enable further research into deep
models and their representations. In addition, our method may enable neW architectures to be trained
for Which existing techniques, such as shortcuts and normalization layers, are insufficient.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
Here we discuss our efforts to facilitate the reproducibility of this paper. Firstly, we have made an
open Python implementation of DKS and TAT, supporting multiple tensor programming frameworks,
available at https://github.com/deepmind/dks. Secondly, we have given all important
details of our experiments in Appendix D.
References
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order
optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020.
Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kronecker-
factored approximations. In International Conference on Learning Representations, 2017.
Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell,
and Julian McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint
arXiv:2003.04887, 2020.
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question? In
International Conference on Machine Learning,pp. 342-350. PMLR, 2017.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Andrew Brock, Soham De, and Samuel L Smith. Characterizing signal propagation to close the per-
formance gap in unnormalized resnets. In International Conference on Learning Representations,
2021a.
Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021b.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem. In
International Conference on Learning Representations, 2020.
Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in Neural Informa-
tion Processing Systems, 22:342-350, 2009.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. Advances In Neural Information Processing
Systems, 29:2253-2261, 2016.
Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function
in deep networks. Advances in Neural Information Processing Systems, 33, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg:
Making vgg-style convnets great again. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 13733-13742, 2021.
10
Published as a conference paper at ICLR 2022
David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies in very
deep networks. In Artificial Intelligence and Statistics, pp. 202-210. PMLR, 2014.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on
deep neural networks training. In International conference on machine learning, pp. 2672-2680.
PMLR, 2019.
Soufiane Hayou, Eugenio Clerico, Bobby He, George Deligiannidis, Arnaud Doucet, and Judith
Rousseau. Stable resnet. In International Conference on Artificial Intelligence and Statistics, pp.
1324-1332. PMLR, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.
URL http://github.com/deepmind/dm-haiku.
Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax:
composable gradient transformation and optimisation, in jax!, 2020. URL http://github.
com/deepmind/optax.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jurgen Schmidhuber, et al. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Hassan K. Khalil. Nonlinear systems third edition. 2008.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. Advances in neural information processing systems, 30, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. Advances in neural information processing systems, 2012.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade. Springer, 1998.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Yao Lu, Stephen Gould, and Thalaiyasingam Ajanthan. Bidirectional self-normalizing neural
networks. arXiv preprint arXiv:2006.12169, 2020.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. In International Conference on Machine Learning, 2013.
James Martens. On the validity of kernel approximations for orthogonally-initialized neural networks.
arXiv preprint arXiv:2104.05878, 2021.
11
Published as a conference paper at ICLR 2022
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417. PMLR, 2015.
James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha
Sohl-Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip
connections or normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765,
2021.
Radford M Neal. Bayesian learning for neural networks. Lecture notes in statistics, 118, 1996.
Oyebade K Oyedotun, Djamila AoUada, Bjorn Ottersten, et al. Going deeper with neural networks
without skip connections. In 2020 IEEE International Conference on Image Processing (ICIP), pp.
1756-1760. IEEE, 2020.
Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 4788-4798, 2017.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential
expressivity in deep neural networks through transient chaos. Advances in neural information
processing systems, 29:3360-3368, 2016.
Michael JD Powell. An efficient method for finding the minimum of a function of several variables
without calculating derivatives. The Computer Journal, 7(2):155-162, 1964.
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In International Conference on Machine Learning, pp. 7824-7835. PMLR,
2020.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations, 2017.
Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable
for training deep neural network? Advances in Neural Information Processing Systems, 33, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354-359, 2017.
Rupesh Kumar Srivastava, Klaus Greff, and JUrgen Schmidhuber. Highway networks. arXiv preprint
arXiv:1505.00387, 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. Advances in neural information processing systems, 29:550-558,
2016.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018.
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generaliza-
tion in deep neural networks. In International Conference on Machine Learning, pp. 10462-10472.
PMLR, 2020.
Greg Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in Neural Information Processing Systems, volume 30, 2017.
12
Published as a conference paper at ICLR 2022
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A
mean field theory of batch normalization. ArXiv, abs/1902.08129, 2019.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference 2016. British Machine Vision Association, 2016.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. Advances in neural information processing systems, 2019.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In International Conference on Learning Representations, 2018.
13
Published as a conference paper at ICLR 2022
A Background
A. 1 Kernel Function Approximation Error Bounds
In Section 2.1, we claimed that the kernel defined in equation 2 would converge to a deterministic
kernel, as the width of each layer goes to infinity. To be specific, one has the following result bounding
the kernel approximation error.
Theorem 3 (Adapted from Theorem 2 of Daniely et al. (2016)). Consider a fully-connected network
of depth L with weights initialized independently using a standard Gaussian fan-in initialization.
Further suppose that the activation function φ is C -bounded (i.e. kφk∞ ≤ C, kφ0 k∞ ≤ C and
∣∣φ00k∞ ≤ C for some constant C) and satisfies Ez 〜N (o,i)[Φ(z)2] = 1, and that the width of each
layer is greater than or equal to (4C4)L log(8L∕δ)∕e2. Then at initialization time, for inputs xι and
x2 satisfying ∣x1∣2 = ∣x2∣2 = dim(x1), we have that
∖κf (xi, X2)一 κf(xι, x2) ≤ e
with probability at least 1 - δ.
The bound in Theorem 3 predicts an exponential dependence on the depth L of the minimum required
width of each layer. However, for a network with ReLU activations, this dependence is only quadratic
in L, as is established in the following theorem:
Theorem 4 (Adapted from Theorem 3 of Daniely et al. (2016)). Consider a fully-connected network
of depth L with ReLU activations and weights initialized independently using a He initialization (He
et al., 2015), and suppose that the width of each layer is greater than or equal to L2 log(L∕δ)∕e2.
Then at initialization time, for inputs xι and x2 satisfying ∣∣xι ∣∣2 = ∣∣x2k2 = dim(xι), and e . L,
we have that
∖κf (xi, x2) 一 κf(xι,x2)∖ ≤ e
with probability at least 1 - δ.
According to Lemma D.1 of Buchanan et al. (2020), the requirement of the width for ReLU networks
could further be reduced to linear in the depth L, but with a worse dependency on δ.
Although Theorems 3 and 4 are only applicable to Gaussian initializations, a similar bound has
been given by Martens (2021) for scaled uniform orthogonal initializations in the case that L = 1.
Moreover, Martens (2021) conjectures that their result could be extended to general values of L.
A.2 Degenerate C maps for very deep networks
Daniely et al. (2016), Poole et al. (2016), and Martens et al. (2021) have shown that without very
careful interventions, C maps inevitably become “degenerate” in deep networks, tending rapidly
towards constant functions on (-1, 1) as depth increases. The following proposition is a restatement
of Claim 1 from Daniely et al. (2016):
Proposition 2. Suppose f is a deep network consisting of a composition of L combined layers. Then
for all c ∈ (-1, 1) we have
lim Cf(C) = c*,
L→∞ f
for some c* ∈ [0,1].
While the above result doesn’t characterize the rate of convergence Cf (c) to a constant function,
Poole et al. (2016) show that if C0(1) 6= 1, it happens exponentially fast as a function of L in the
asymptotic limit of large L. Martens et al. (2021) gives a similar result which holds uniformly for all
L, and for networks with more general repeated structures.
A.3 C map derivative
Poole et al. (2016) gave the following nice formula for the derivative of C map of a combined layer
with activation function φ:
C0(c,q1,q2)
pQ(q1)Q(q2) EzI …(0,1)
[φ0 (√q1z1) φ0 √∕22 (czι + pl - C2Z2)) ] . (17)
14
Published as a conference paper at ICLR 2022
For a rigorous proof of this result we refer the reader to Martens et al. (2021).
One can iterate this formula to obtain a similar equation for higher-order derivatives:
C (i)(c, q1, q2) = p≡S⅛ Ezι,…(0,1) hφ(i) (√qτz1) φ(i) (√q2 g + L z2))i.
(18)
A.4 Some useful properties of C maps
In this section we will assume that q1 = q2 = 1.
Observe that C(1) = Ez〜N(0,1) [φ (z)2] = 1 and that C maps [-1,1] to [-1,1] (which follows from
its interpretation as computing cosine similarities for infinitely wide networks). Moreover, C is a
positive definite function, which means that it can be written as Pn∞=0 bncn for bn ≥ 0 (Daniely
et al., 2016; Martens et al., 2021). Note that for smooth activation functions, positive definiteness can
be easily verified by Taylor-expanding C(c) about c = 0 and using
C(i)(0) = Ez〜N(0,i) [φ(i) (z)]2 ≥ O.
(19)
As discussed in Section 2.3, global C maps are computed by recursively taking compositions and
weighted averages (with non-negative weights), starting from C . Because all of the above properties
are preserved under these operations, it follows that global C maps inherit them from C .
B	Additional details and pseudocode for activation function
TRANSFORMATIONS
B.1	Taking all subnetworks into account
In the main text of this paper we have used the condition Cf0 (1) = ζ in DKS, Cf (0) = η in TAT for
Leaky ReLUs, and Cf00(1) = τ in TAT for smooth activation functions. However, the condition used
by Martens et al. (2021) in DKS was actually μf (C0(1)) = Z, where μf is the so-called “maximal
slope function”:
μf (CO(I))= gmm⊆x Cg(I),
where “g ⊆ f” denotes that g is a subnetwork4 of f. (That Cg0 (1) is fully determined by C0(1) follows
from the fact that Cg can be written in terms of compositions, weighted average operations, and
applications of C, and that C maps always preserve the value 1. Using the chain rule, and the linearity
of derivatives, these facts allow one to write Cg0 (1) as a polynomial function ofC0(1).)
The motivation given by Martens et al. (2021) for looking at Cg0 (1) over all subnetworks g ⊆ f
(instead of just Cf0 (1)) is that we want all layers of f, in all of its subnetworks, to be readily trainable.
For example, a very deep and untrainable MLP could be made to have a reasonable global C map
simply by adding a skip connection from its input to its output, but this won’t do anything to address
the untrainability of the layers being “skipped around” (which form a subnetwork).
In the main text we ignored this complication in the interest of a shorter presentation, and because
We happened to have μf (C0(1)) = Cf (1) for the simple network architectures focused on in this
work. To remedy this, in the current section we will discuss how to modify the conditions Cf (0) = η
and Cf00(1) = τ used in TAT so that they take into account all subnetworks. This will be done
using a natural generalization of the maximal slope function from DKS. We will then address the
computational challenges that result from doing this.
4A subnetwork of f is defined as a (non-strict) connected subset of the layers in f that constitute a neural
network with a singular input and output layer. So for example, layers 3, 4 and 5 of a 10 layer MLP form a
subnetwork, while layers 3, 4, and 6 do not.
15
Published as a conference paper at ICLR 2022
To begin, we will replace the condition Cf (0) = η (used in TAT for Leaky ReLUs) by the condition
μf (0) = η, where We define the maximal C value function μf of f by
μf (α)
max
g-g⊆f
Cg(0),
where α is the negative slope parameter (which determines C in LReLU networks [via φα] and thus
each Cg).
We will similarly replace the condition Cf00(1) = τ (used in TAT for smooth activations) by the
condition μf (C00(1)) = T, where we define the maximal curvature function μf of f by
max
g∙∙g⊆f
where eachCg00(1) is determined by C00(1). ThateachCg00(1) is a well-defined function of C 00 (1) follows
from the fact that C maps always map the value 1 to 1, the aforementioned relationship between Cg and
C, and the fact that we have C0(1) = 1 under TAT (so that Ch0 (1) = 1 for all subnetworks h). These
facts allow us to write Cg00(1) as a constant multiple ofC00(1) using the linearity of 2nd derivatives
and the 2nd-order chain rule (which is given by (a ◦ b)00(x) = a00(b(x))b0(x)2 + a0(b(x))b00(x)).
B.2	Computing μf and μf IN general
Given these new conditions for TAT, it remains to compute their left hand sides so that we may ulti-
mately solve for the required quantities (α orC00(1)). In Section 2.3 we discussed how a (sub)network
f’s C map Cf can be computed in terms of the local C map C by a series of composition and non-
negative weighted sum operations. We can define a generalized version of this construction Uf,r
which replaces C with an arbitrary non-decreasing function r, so that Uf,C (c) = Cf (c). A recipe for
computing Uf,r is given in Appendix B.4.
Given Uf,r, we define the subnetwork maximizing function M by
Mf,r (X) = gmm⊆xf Ug,r(x).
With this definition, it is not hard to see that if r0(x) = C(x), r1(x) = C0(1)x, and r2(x) = C00(1)+x,
then μf (α) = Mf,r0 (0) (where the dependence on α is implicit through the dependence of C on
φa), μf (C0(1)) = Mf,rι (1), and μf (C00(1)) = Mfr (0). Thus, it suffices to derive a scheme for
computing (and inverting) Mf,r for general networks f and non-decreasing functions r.
Naively, computing Mf,r could involve a very large maximization and be quite computationally
expensive. But analogously to the maximal slope function computation described in Martens et al.
(2021), the computation of Mf,r can simplified substantially, so that we rarely have to maximize
over more than a few possible subnetworks. In particular, since Ug,r (x) is a non-decreasing function
of x for all g (which follows from the fact that r is non-decreasing), and Ug5尸 = Ug,r ◦ Uh,r, it
thus follows that Ug°h,r (x) ≥ Ug,r (x), Uh,r (x) for all x. This means that for the purposes of the
maximization, we can ignore any subnetwork in f which composes with another subnetwork (not
necessarily in f) to form a strictly larger subnetwork isomorphic to one in f . This will typically be
the vast majority of them. Note that this does not therefore imply that Mf,r = Uf,r, since not all
subnetworks compose in this way. For example, a sufficiently deep residual branch of a residual
block in a rescaled ResNet won’t compose with any subnetwork to form a larger one.
B.3	SOLVING FOR α AND C00(1)
Having shown how to efficiently compute Mf,r, and thus both of μf and μf, it remains to show how
we can invert them to find solutions for α and C00(1) (respectively). Fortunately, this turns out to
be easy, as both functions are strictly monotonic in their arguments (α and C00(1)), provided that
f contains at least one nonlinear layer. Thus, we may apply a simple 1-dimensional root-finding
approach, such as binary search.
To see that μf (α) is a strictly decreasing function of a (or in other words, a strictly increasing
function of -α), we observe that it is a maximum over terms of the form Ug,C(0), which are all either
strictly decreasing non-negative functions of α, or are identically zero. These properties of Ug,C (0)
16
Published as a conference paper at ICLR 2022
follow from the fact that it involves only applications of C, along with compositions and non-negative
weighted averages, and that C(c) is a strictly decreasing function of α for all c ∈ [-1, 1] (in Leaky
ReLU networks). A similar argument can be used to show that μf (C00(1)) is a strictly increasing
function of C00(1) (and is in fact equal to a non-negative multiple of C00(1)).
B.4	RECIPE FOR COMPUTING Uf,r
As defined, Uf,r is computed from f by taking the computational graph for Cf and replacing the
local C map C with r wherever the former appears. So in particular, one can obtain a computational
graph for Uf,r(x) from f’s computational graph by recursively applying the following rules:
1.	Composition g ◦ h of two subnetworks g and h maps to Ug,r ◦ Uh,r.
2.	Affine layers map to the identity function.
3.	Nonlinear layers map to r.
4.	Normalized sums with weights w1, w2, ..., wn over the outputs of subnetworks g1, g2, ..., gn,
map to the function
w1Ugι,r (XI) + w2 Ug 2 ,r (X2 ) + …，+ WInUgn ,r (Xn),
where x1, x2, ..., xn are the respective inputs to the Ugi,r’s.
5.	f’s input layer maps to X.
In the special case of computing Uf,r2 (0), one gets the following simplified list of rules:
1.	Composition g ◦ h of two subnetworks g and h maps to Ug,r2 (0) + Uh,r2 (0)
2.	Affine layers map to 0.
3.	Nonlinear layers map to C00(1).
4.	Normalized sums with weights w1, w2, ..., wn over the outputs of subnetworks g1, g2, ..., gn,
map to the function
w1Ug1,r2(o)+w2Ug2,r2(o)+	+ Wn Ugn,r2 ⑼.
5.	f’s input layer maps to X.
Note that this second procedure will always produce a non-negative multiple of C00(1), provided that
f contains at least one nonlinear layer.
B.5	Rescaled ResNet example
In this subsection we will demonstrate how to apply the above rules to compute the maximal curvature
function μf for a rescaled ResNet f with shortcut weight W and residual branch R (as defined in
equation 6). We note that this computation also handles the case of a vanilla network by simply
taking W = 0.
First, we observe that all subnetworks in f compose to form larger ones in f, except for f
itself, and for the residual branches of its residual blocks. We thus have that μf (C00(1)) =
max{Uf,r2 (0), UR,r2 (0)}.
Because each residual branch has a simple feedforward structure with three nonlinear layers, it
follows that UR,r2 (0) = 3C00(1). And because each shortcut branch S has no nonlinear layers, it
follows that US,r2 (0) = 0. Applying the rule for weighted averages to the output of each block B we
thus have that UB,r2 (0) = W2 US,r2 (0) + (1 - W2)UR,r2 (0) = 3(1 - W2)C00(1). Given a network
with L nonlinear layers, we have L/3 blocks, and since the blocks compose in a feedforward manner
it thus follows that Uf,r2(0) = (L/3) ∙ 3(1 - w2)C00(1) = L(1 - w2)C"⑴.We therefore conclude
that μf (C00(1)) = max{3,L(1 — w2)}C00(1).
The rescaled ResNets used in our experiments have a slightly more complex structure (based on the
ResNet-50 and ResNet-101 architectures), with a nonlinear layer appearing after the sequence of
residual blocks, and with a four of their blocks being “transition blocks”, whose shortcut branches
17
Published as a conference paper at ICLR 2022
contain a nonlinear layer. In these networks, the total number of residual blocks is given by (L - 2)/3.
Following a similar argument to the one above we have that
Uf,r2(0) = (V - 4)∙ 3(1 - w2)C00(1) +4 ∙ (w2 + 3(1 - w2))C00(1) + C00(1)
= [(L-2)(1- w2) +4w2 + 1]C00(1) = [(L- 6)(1 - w2) + 5]C00(1),
and thus
μf(C〃⑴)=max{[(L - 6)(1 - w2) + 5]C〃⑴,3(1 - w2)C〃⑴}
= [(L - 6)(1 -w2) + 5]C00(1).
B.6	Pseudocode
Algorithm 1 TAT for LReLU.
Require: The target value η for μf (α)
1:	Use the steps from Subsection B.2 to construct a procedure for computing the maximal c value
function μf (α) for general a ≥ 0. Note that the local C map C, on which μf(α) depends, can be
computed efficiently for (transformed) LReLUs using equation 9.
2:	Perform a binary search to find the negative slope α such that μf (α) = η.
3:	Using the found a, output the transformed activation function given by φα(x)=
φα(x).
Algorithm 2 TAT for smooth activations.
Require: The target value τ ofCf00(1)
Require: The original activation function φ(x)
1:	Use the steps from Subsection B.2 to construct a procedure for computing the maximal curvature
function μf (C00(1)) for general C00(1) ≥ 0.
2:	Perform a binary search to find C00(1) such that μf (C00(1)) = T.
3:	Using a numerical solver, solve the three-dimensional nonlinear system in equation 16 (but with
the value of C00(1) found above instead of τ∕L) to obtain values for α, β, γ, and δ.
4:	Using the solution from the last step, output the transformed activation function given by
φ(x) = γ(φ(αx + β) + δ).
C Technical Results and Proofs
〜
Lemma 1. For networks USing the activation function φα(x) = y ι+⅛ Φα(x), the local Q and C
maps are given by
Q(q) = q and C(C) = C + ɪ-α4v (p1 - Cc - CCos
π(1 + α2)
(20)
Proof. In this proof we will use the notation Qφ and Cφ to denote the local Q and C maps for
networks that use a given activation function φ.
First, we note that LReLU is basically the weighted sum of identity and ReLU. In particular, we have
the following equation:
φα(x) = αx + (1 - α)φ0(x) = max{x, 0} + α min{x, 0}.
Second, We have that Qφa (q) = Ez〜N(0,1)[qz2E[z ≥ 0]] + α2Ez〜N(0,1)[qz2E[z < 0]] = 1+α2q
(from which Qφɑ (q) = q immediately follows).
18
Published as a conference paper at ICLR 2022
It then follows from equation 5, and the fact that local C maps are invariant to multiplication of the
activation function by a constant, that
2
C 7 (c) = Cφ (c)=-----
φα' c	φα-	1 + α2
_	2
=1 + α2
2
Ezι,z2〜N(0,1) [φα (ZI) φα (cz1 + 1- - c2z2)]
α2c + (1 - α)2Cφ0 (c)Qφ0 (1)
(21)
+ 1+02
[2α(1 - α)Ez1,z2〜N(0,1)({cz1 + √1 - c2z2)φ0 (zι)]j
From Daniely et al. (2016) we have that
Cφ0(c)
11 - c2 + (π — cos-1(c))c
π
and for the last part of equation 21 we have
EZ1,Z2〜N(0,1) [(cz1 + P - Ccz2)φ0 (zl)] = EzI〜N(0,1) [cz2 lz1>0] = c
Plugging equation 22 and equation 23 back into equation 21, we get
c , 、	2	9	(1 — 0)2 √1 — c2 + (π — cos-1(c))c	/	、
C7 (c) =--------- α2c + ʌ----------------口----------c^c- + α(1 - α)
φα( )	1 + α2 [	+	2	π	+ (	)
(1 — 0)2 (√1 — C- + c(π — cos-1(c))) + 2π0c
(1 + α2)π	.
Rearranging this gives the claimed formula.
(22)
(23)
(24)
□
Proposition 1. The global C map of a feedforward network with φα(x) as its activation function
is equal to that of a rescaled ResNet of the same depth (see Section 2.4) with normalized ReLU
activation φ(x) = √2max(x, 0), shortcut weight J1⅛, and residual branch R consisting of a
combined layer (or just a normalized ReLU activation) followed by an affine layer.
Proof. By equation 7, the C map for a residual block B of the hypothesized rescaled ResNet is given
by
CB(c) = w2c + (1 — w2)Cφ0 (c).	(25)
The global C map of this network is given by L compositions of this function, while the global C
map of the hypothesized feedforward network is given by L compositions of Cφ7 (c). So to prove the
claim it suffices to show that CB(c) = Cφ7 (c).
Taking W =、ɪ+^, one obtains the following
20	(1 — 02) √1 — c2 + c(π — cos-1(c))
CB(C) = TT^ + TΓ^-----------------------∏-----------
(26)
which is exactly the same as C% (c) as given in Lemma 1. This concludes the proof.	□
Proposition 3. Suppose f is vanilla network consisting of L combined layers with the TReLU
activation function (so that Cf (0) = η ∈ (0, 1)). Then Cf converges to a limiting map on (—1, 1) as
L goes to infinity. In particular,
lim Cf(c) =ψ(c,T),	(27)
L→∞
where T is such that ψ(0, T) = η, and where ψ is the solution of the following ordinary differential
equation (ODE) with the first argument being the initial condition (i.e. ψ(c, 0) = c), and the second
argument being time:
7)) = p1 — x(t)2 — x(t) cos-1(x(t)).
(28)
19
Published as a conference paper at ICLR 2022
Proof. First, we notice that the local C map for TReLU networks can be written as a difference
equation:
C(C) = C + (1 -。，2、(p1 - c2 - C CosT(C)) .	(29)
π(1+ α2)
Importantly, C is a monotonically increasing function of C, whose derivative goes to zero only as
α ∈ [0,1] goes to 1. Thus, to achieve Cf (0) = η in the limit of large L, We require that J：-0)2) goes
to 0. This implies that the above difference equation converges to the ODE in equation 28.
Because the function √1 - x2-x CosT(X) is continuously differentiable in [-1,1], and its derivative
- cos-1(x) is bounded, one can immediately shoW that it is globally Lipschitz, and the ODE has a
unique solution ψ(C0, t) according to Theorem 3.2 of Khalil (2008).
NoW, We are only left to find the time T such that Cf∞ (0) = ψ(0, T ) = η. To that end, We notice that
g(x) = p1 - x2 - x cos-1(x) > 0, for X ∈ (-1,1)	(30)
because g(1) = 0 and g0(x) = - Cos-1(x) < 0 on (-1, 1). This implies that the ψ(0, t) is a
monotonically increasing continuous function of t. Since ψ(0, 0) = 0, to establish the existence of T
it suffices to shoW that ψ(0, ∞) ≥ 1.
To this end we first observe that
g(x) ≥ 2√2 (1 - x)3/2,	(31)
which follows by defining h(x) = g(x) - 232 (1 - x)3/2 and observing that h(1) = 0 and h0(x)=
-CosT(X)+ √2(1 - x)1/2 < 0 on (-1,1). Given this, it is sufficient to show that the solution ψ for
the ODE X = 2√√2 (1 - x)3/2 satisfies ψ(0, ∞) = 1. The solution ψ turns out to have a closed-form
of ψ(0, t) = 1 - (√3+3 )2, and thus ψ(0, ∞) ≥ ψ(0, ∞) = 1. This completes the proof. □
Theorem 1. For a network f with φα (x) as its activation function (with α ≥ 0), we have
max |Cf (C) - C| ≤ min {4Cf (0), 1 + Cf (0)} , max Cf0 (C) - 1 ≤ min {4Cf (0), 1} (10)
Proof. Because Cf is a positive definite function (by Section A.4) we have that it can be written as
Cf (C) = Pn∞ bnCn for bn ≥ 0. Given Cf (1) = Cf0 (1) = 1, we have
∞∞	∞
X bn = X nbn = 1	=⇒	b0 = X(n - 1)bn	=⇒	2b0 + b1 ≥ 1.	(32)
n=0	n=1	n=2
Hence, 1 - Cf0 (0) = 1 - b1 ≤ 2b0 = 2Cf (0). Now we are ready to bound the deviation of Cf (C)
from identity:
max |Cf (C) - C| = max
c∈[-1,1]	c∈[-1,1]
∞
b0+XbnCn-(1-b1)C
n=2
≤ max
c∈[-1,1]
∞
b0+Xbn|C|n+(1-b1)|C|
n=2
(33)
∞
b0 + X bn + 1 - b1 = 2(1 - b1 )
n=2
2(1 - Cf0 (0)) ≤ 4Cf(0).
Using equation 20 we have that
CO(C) = I- (Γ+α⅛ CosT(C).
From our assumption that α ≥ 0 it follows that 0 ≤ C0(C) ≤ 1 for all C ∈ [-1, 1]. Since the property
of having a derivative bounded between 0 and 1 is closed under functional composition and positive
20
Published as a conference paper at ICLR 2022
weighted averages, it thus follows that 0 ≤ Cf (c) ≤ 1 for all c ∈ [-1, 1]. An immediate consequence
of this is that Cf (c) is non-decreasing, and that
max
c∈[-1,1]
|Cf(c)-c| =Cf(-1)+1≤Cf(0)+1.
(34)
Next, we bound the deviation of Cf0 (c) from 1:
max
c∈[-1,1]
Cf0 (c) - 1 = max
f	c∈[-1,1]
∞
X nbncn-1 - (1 - b1)
n=2
∞
≤ max	X nbn|c|n-1 + (1 - b1)
c∈[-1,1] n=2 n	1
∞
= X nbn + 1 - b1 = 2(1 - b1 )
n=2
(35)
2(1 - Cf0 (0)) ≤4Cf(0).
From the previous fact that 0 ≤ Cf0 (c) ≤ 1 for all c ∈ [-1, 1] we also have that
maxc∈[-ι,i] ICf(C) - 1∣ ≤ 1. This completes the proof.	□
Theorem 2. Suppose f is a network with a smooth activation function. If Cf0 (1) = 1, then we have
c∈m[-a1x,1] |Cf (c) - c| ≤ 2Cf00(1), c∈m[-a1x,1] ∣∣Cf0 (c) - 1∣∣ ≤ 2Cf00(1)	(13)
Proof. Cf is a positive definite function by Section A.4. So by the fact that positive definite functions
are non-negative, non-decreasing, and convex on the non-negative part of their domain, we obtain
that Cf0 (0) ≥ Cf0 (1) - Cf00(1) = 1 - Cf00(1). By equation 33, we have
max |Cf (c) - c| ≤ 2(1 - Cf0 (0)) ≤ 2Cf00(1).	(36)
Further by equation 35, we also have
max ∣∣Cf0 (c) - 1∣∣ ≤ 2(1 - Cf0 (0)) ≤ 2Cf00(1).	(37)
This completes the proof.	□
Proposition 4. Suppose f is some function computed by a neural network with the ReLU activation.
Then for any negative slope parameter α 6= ±1, we can compute f using an LReLU neural network
of the same structure and double the width of the original network.
Proof. The basic intuition behind this proof is that a ReLU unit can always be “simulated” by two
LReLU units as long as α 6= ±1, due to the following formula:
φo(x) = 1 -^α2 (φα(x) + αφα(-x)).
We will begin by proving the claim in the case of a network with one hidden layer. In particular, we
assume the ReLU network has m hidden units:
m
f (w, b, a, x) =	arφ0 (wr>x + br),	(38)
r=1
where x ∈ Rd is the input, and w ∈ Rmd, b ∈ Rm and a ∈ Rm are weights, biases of the input layer
and weights of output layer, respectively. For LReLU with negative slope α, one can construct the
following network
f (w0, b0, a0, x) = X a0r φα (wr0 >x + b0r).	(39)
r=1
21
Published as a conference paper at ICLR 2022
If We choose Wr = Wr = -Wr+m, b0r = b = —b；+%, a； = 1—α2 0r and a；+m = 1:2 ar, We have
a0rφα(Wr0 >x + b0r) +
1
a0r+mφα(Wr0 +m>x + b0r+m)
1 - α2
α2
ar φα(w>x + br ) —  -2 0r φ 1 (w>X + b『)=a/φ(w>x + br ), (40)
1 — α2 α
This immediately suggests that f(W0, b0, a0, x) = f(W, b, a, x).
Since deeper netWorks, and one With more complex topologies, can be constructed by composing
and summing shallower ones, the general claim follows.	□
D	Experiment details
For input preprocessing on ImageNet we perform a random crop of size 224 × 224 to each image, and
apply a random horizontal flip. In all experiments, we applied L2 regularization only to the weights
(and not the biases or batch normalization parameters). We selected the L2 constant by grid search
from {0.00005, 0.00002, 0.0}. For networks without batch normalization layers we applied dropout
to the penultimate layer, with the dropout rate chosen by grid search from {0.2, 0.0}. In addition, we
used label smoothing (Szegedy et al., 2016) with a value of 0.1.
For each optimizer we used a standard learning rate warm-up scheme which linearly increases
the learning rate from 0 to the “initial learning rate” in the first 5 epochs, and then decays the
learning rate by a factor of 10 at 4/9 and 7/9 of the total epoch budget5, unless specified other-
wise. The initial learning rate was chosen by grid search from {1.0, 0.3, 0.1, 0.03, 0.01} for SGD,
{0.003, 0.001, 0.0003, 0.0001, 0.00003} for K-FAC, and {10.0, 3.0, 1.0, 0.3, 0.1} for LARS. For all
optimizers we set the momentum constant to 0.9. For K-FAC, we used a fixed damping value of
0.001, and a norm constraint value of 0.001 (see Ba et al. (2017) for a description of this parameter).
We also updated the Fisher matrix approximation every iteration, and computed the Fisher inverse
every 50 iterations, unless stated otherwise. For LARS, we set the “trust” coefficient to 0.001. For
networks with batch normalization layers, we set the decay value for the statistics to 0.9.
For initialization of the weights we used the scale-corrected uniform orthogonal (SUO) distribu-
tion (Martens et al., 2021) for all methods/models, unless stated otherwise. For a m × k matrix
(with k being the input dimension), samples from this distribution can be generated by computing
(XXτ)-1/2 X, where X is an m × k matrix with entries sampled independently from N(0, 1).
When m > k,we may apply the same procedure but with k and m reversed, and then transpose the
result. The resulting matrix is further multiplied by the scaling factor max{ ʌ/m/k, 1}, which will
have an effect only when k ≤ m. For convolutional networks, we initialize only the weights in the
center of each filter to non-zero values, which is a technique known as Delta initialization (Balduzzi
et al., 2017; Xiao et al., 2018), or Orthogonal Delta initialization when used with orthogonal weights
(as we do in this work).
We implemented all methods/models with JAX (Bradbury et al., 2018) and Haiku (Hennigan et al.,
2020). We used the implementation of SGD and LARS from Optax (Hessel et al., 2020). We used the
JAX implementation of K-FAC available at https://github.com/deepmind/kfac_jax.
E Additional Experimental Results
E.1 Empirical c values for finite-width networks
The computation of cosine similarities performed by C maps is only an approximation for finite
width networks, and it is natural to ask how large the approximation error is. To answer this question,
we compare the theoretical predictions with the empirical simulations on fully-connect networks of
different depths and widths. In particular, we use a fixed η = 0.9 for TReLU and we compute the
l> l
l-th “empirical C value” cl = 1.1口.2 ： for each layer index l, where x0 and x0 are random vectors
5We later found that cosine learning rate annealing (Loshchilov & Hutter, 2016) is slightly better for most
settings, but this did not change our conclusions.
22
Published as a conference paper at ICLR 2022
(a) TReLU, Gaussian
(b) DKS + SoftPlus, Gaussian
(c) TReLU, Orthogonal	(d) DKS + Softplus, Orthogonal
Figure 4: Empirical c values for TAT and DKS, which are averaged over 100 pairs of inputs and 50 different
randomly-inialized networks. We include the results for both Gaussian fan-in and Orthogonal initialization.
Vertical lines indicate the standard deviation. TReLU has smaller kernel approximation error and is robust to
Gaussian initialization. For TReLU, we also plot the evolution of the c values (black dashed line) as predicted
by the C map (which we can compute analytically for TReLU).
chosen so that ∣∣χ1k2 = ∣∣χ2k = do and χ0>χ0 = 0 (So that c0 = 0). As shown in Figure 4a and 4c,
the approximation error is relatively small even for networks with width 30.
We also included the results for networks using DKS (with ζ = 10) and the SoftPlus activation
function. Figure 4b and 4d reports empirical c values as a function of layer index l, with x10 and
χ0 chosen so that c0 = 0.8. With Gaussian initialization, the standard deviations are much larger
than TReLU, and the average values for widths 30 and 100 deviate significantly from the theoretical
predictions. (The DKS conditions implies C(c) ≤ c for any c ∈ [0, 1], which suggests the c value
should decrease monotonically.) By comparison, the error seems to be much smaller for orthogonal
initialization, which is consistent with the better performance of orthogonal initialization reported by
Martens et al. (2021). (By contrast, we show in Appendix E.7 that Gaussian initialization performs
on par with orthogonal initialization for TReLU.) In addition, we note that the standard deviations
increase along with the depth for both Gaussian and orthogonal initializations.
E.2 Results on CIFAR- 1 0
In addition to our main results on the ImageNet
dataset, we also compared TAT to EOC on CIFAR-
10 (Krizhevsky et al., 2009) using vanilla networks
derived from a Wide ResNet reference architecture
(Zagoruyko & Komodakis, 2016). In particular, we
start with a Wide ResNet with a widening factor of
2, and remove all the batch normalization layers and
shortcut connections. We trained these networks with
the K-FAC optimizer for 200 epochs using a standard
piecewise constant learning rate schedule. To be spe-
cific, we decay the learning rate by a factor of 10 at
75 and 150 epochs. For K-FAC, we set the damping
value to 0.01 and norm constraint value to 0.0001.
Figure 5: CIFAR-10 validation accuracy of
ResNets with ReLU activation function initialized
using either EOC or TAT (ours).
For data preprocessing we include basic data
23
Published as a conference paper at ICLR 2022
augmentations such as random crop and horizontal flip during training. As shown in Figure 5, TAT
outperforms EOC significantly. As we increase the depth from 100 to 304, the accuracy of EOC
network drops dramatically while the accuracy of the TAT network remains roughly unchanged.
E.3 Reducing the overhead of K-FAC
Iterations
Figure 6: Top-1 validation accuracy on ImageNet as a function of number of iterations (left) or wall-clock time
(right) with K-FAC optimizer. One can reduce the computational overhead significantly by updating curvature
matrix approximation and its inverse less frequently.
In our main experiments the per-step wall-clock time of K-FAC was roughly 2.5× that of SGD.
However, this gap can be decreased significantly by reducing the frequency of the updates of K-FAC’s
approximate curvature matrix and its inverse. For example, if we update the curvature approximation
every 10 steps, and the inverses every 200 steps, the average per-step wall-clock time of K-FAC
reduces by half to a mere 1.25× that of SGD. Importantly, as can be seen on Figure 6, this does not
appear to significantly affect optimization performance.
E.4 Disentangling Training and Generalization
In our main experiments we only reported validation
accuracy on ImageNet, making it hard to tell whether
the superior performance of TAT vs EOC is due to
improved fitting/optimization speed, or improved gen-
eralization. Here, we compare training accuracies of
EOC-initialized networks (with ReLU) and networks
with TReLU, in exactly the same experimental setting
as Figure 1. We train each network on ImageNet us-
ing K-FAC for 90 epochs. For each setting, we plot
the training accuracy for the hyperparameter combi-
nation that gave the highest final validation accuracy.
As shown in Figure 7, the EOC-initialized networks
achieve competitive (if not any better) training accu-
racy, suggesting that the use of TReLU improves the
Figure 7: ImageNet training accuracy of deep
vanilla networks with either EOC-initialized
ReLU networks or TReLU networks.
generalization performance and not optimization performance.
E.5 Closing the Remaining Gap using Wider Networks
In all of our main experiments we used networks
derived from standard ResNets (by removing nor-
malization layers and/or shortcut connections). By
construction, these have the same layer widths as stan-
dard ResNets. A natural question to ask is whether
using wider networks would change our results. For
example, it’s possible that vanilla networks with TAT
would benefit more than ResNets from increased
width, since higher width would make the kernel
approximations more accurate, and could also help
Table 9: The effect of increasing width on Im-
ageNet validation accuracy. We use vanilla net-
works for EOC and TAT (ours).
Depth	Width	EOC	TAT	ResNets
50	1×	72.0	76.0	76.7
	2×	73.5	77.3	77.9
101	1×	62.4	76.5	77.9
	2×	66.5	77.6	78.6
compensate for the minor loss of expressive power due to the removal of shortcut connections.
24
Published as a conference paper at ICLR 2022
With layers double the width of standard ResNets, it becomes too expensive to store and invert
Kronecker factors used in K-FAC. Therefore, we only train these wider networks with SGD. In order
to mitigate the slower convergence of SGD for vanilla networks (see Section 5.5), we train them for
360 epochs at a batch size of 512. Note that due to increased overfitting we observed in ResNets
after 360 epochs (resulting in lower validation accuracy) we only trained them for 90 epochs. As
shown in Table 9, doubling the width does indeed narrow the remaining validation accuracy gap
between ResNets and vanilla TAT networks. In particular, the gap goes from 0.7% to 0.6% for depth
50 networks, and from 1.4% to 1% for depth 101 networks.
E.6 Comparison with PReLU on Rescaled ResNets
In Table 6 of the main text we compare PReLU	Table 10:	Comparison with PReLU with rescaled			
and TReLU on deep vanilla networks. Here we extend this comparison to rescaled ResNets with a shortcut weight of w = 0.8. For PReLU, we again include two different initializations:	ResNets (	w = 0.8).			
	Depth	Optimizer	TReLU	PReLU0.0	PReLU0.25
	50	K-FAC	764	757	73.6
one with 0 negative slope (effectively ReLU),		SGD	76.0	715	71.5
and another with 0.25 negative slope (which was used in He et al. (2015)). We report the full results in Table 10. For all settings, TAT	101	K-FAC	778	764	76.8
		SGD	77.3	73.1	73.4
outperforms PReLU by a large margin, suggesting that a better-initialized negative slope is crucial
for both rescaled ResNets and deep vanilla networks.
E.7 Comparison of different initializations
In all of our experiments we use the
Orthogonal Delta initialization intro-
duced by Balduzzi et al. (2017) and
Xiao et al. (2018). This is because
it’s technically required in order to
apply the extended Q/C map analy-
sis of Martens et al. (2021) (which
underlies DKS and TAT) to convo-
lutional networks, and because it is
generally thought to be beneficial.
In this subsection we examine this
choice more closely by comparing it
to a traditional Gaussian fan-in ini-
tialization (with σw2 = 2 for ReLUs).
We consider standard ResNets and
Table 11: Comparison of Orthogonal Delta and Gaussian fan-in
initialization.
Depth	Optimizer	Init	ResNet	EOC	TAT
	K-FAC	Orth Delta	76.4	72.6	74.6
50		Gaussian	76.5	72.5	74.8
	SGD	Orth Delta	76.3	63.7	71.0
		Gaussian	76.6	63.1	68.7
	K-FAC	Orth Delta	77.8	71.8	74.2
101		Gaussian	77.8	72.3	74.1
	SGD	Orth Delta	77.9	41.6	70.0
		Gaussian	78.0	41.1	68.7
deep vanilla networks using either EOC (with ReLUs) or TAT with (with LReLU). Surprisingly, it
turns out that the Orthogonal Delta initialization does not have any clear advantage over the Gaussian
fan-in approach, at least in terms of validation accuracy after 90 epochs.
25