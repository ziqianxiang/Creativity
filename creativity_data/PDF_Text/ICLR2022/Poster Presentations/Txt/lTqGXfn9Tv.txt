Published as a conference paper at ICLR 2022
Phenomenology of Double Descent in
Finite-Width Neural Networks
Sidak Pal Singh* a,c, AUrelien Lucchib, Thomas Hofmanna and Bernhard SchOlkopfc
aETH Zurich, Switzerland
bDepartment of Mathematics and Computer Science, University of Basel
CMPI for Intelligent Systems, Tubingen, Germany
Ab stract
‘Double descent’ delineates the generalization behaviour of models depending on
the regime they belong to: under- or over-parameterized. The current theoretical
understanding behind the occurrence of this phenomenon is primarily based on
linear and kernel regression models — with informal parallels to neural networks
via the Neural Tangent Kernel. Therefore such analyses do not adequately capture
the mechanisms behind double descent in finite-width neural networks, as well
as, disregard crucial components — such as the choice of the loss function. We
address these shortcomings by leveraging influence functions in order to derive
suitable expressions of the population loss and its lower bound, while imposing
minimal assumptions on the form of the parametric model. Our derived bounds
bear an intimate connection with the spectrum of the Hessian at the optimum,
and importantly, exhibit a double descent behaviour at the interpolation threshold.
Building on our analysis, we further investigate how the loss function affects dou-
ble descent — and thus uncover interesting properties of neural networks and their
Hessian spectra near the interpolation threshold.
1	Introduction
Double-descent (DD) (Belkin et al., 2019) refers to the phenomenon of population (or test) loss ex-
hibiting a second descent when the model is over-parameterized beyond a certain threshold (dubbed
as the interpolation threshold). While such a behaviour has been originally observed in multiple
contexts before (Loog et al., 2020), this terminology has been made commonplace by the influential
work of (Belkin et al., 2019), who posited DD as a way to reconcile the traditional statistical wis-
dom with modern-day usage in machine learning. In particular, when models such as deep neural
networks are employed in the over-parameterized regime, this second descent in test loss is seen as
an extension of the usual U-shaped bias-variance curve — thus providing a basis to reason about the
amazing generalization abilities of neural networks despite their extreme surplus of parameters.
As a result, there have been a vast number of studies that investigate DD further and others that
seek to explain the underlying mechanisms. The former category of works (Nakkiran et al., 2019)
has served to make this phenomenon conspicuous even in the context of frequently employed deep
networks and shown the existence of such DD like behaviour based on other axes such as the amount
of data or number of epochs (Nakkiran, 2019; Nakkiran et al., 2019); while the latter category of
studies (Hastie et al., 2019; Mei & Montanari, 2019; Advani et al., 2020; Bartlett et al., 2020) has
provided the theoretical grounds for the occurrence of such a phenomenon, but almost always, in
linear or kernel regression. Although the connection of (infinite-width) neural networks to Neural
Tangent Kernel (Jacot et al., 2018) provides interesting parallels, its applicability to finite-width
neural networks — i.e., models of actual practical significance — is unclear.
Hence, our aim is to go beyond such informal correspondences, and instead, thoroughly understand
and characterize the phenomenology of double descent in finite-width neural networks. In other
words, we develop a theoretical analysis that can explain the source of double descent in the class
of models that originally brought this phenomenon to the limelight, as well as study the impact
of crucial training-related aspects, such as the choice of the loss function. To this end, we derive
* Correspondence to sidak.singh@inf.ethz.ch. The most recent version of our paper can be found
on arXiv and the code for the experiments is available on GitHub.
1
Published as a conference paper at ICLR 2022
an expression for the population risk by considering the change in the loss at a particular sample
— when excluded from the training set and when included. We do so by utilizing the notion of
influence functions from robust statistics, which provides us with a closed-form estimate of the
change in parameters. Subsequently, we establish a lower bound to the population risk and leverage
results from Random Matrix Theory to show that it diverges at the interpolation threshold — thereby
demonstrating the presence of double descent.
The use of influence functions (Hampel et al., 1986; Efron & Stein, 1981) makes our analysis appli-
cable to a broad class of parametric estimators such as maximum-likelihood-type — which includes
the class of neural networks but also applies to other settings such as linear and kernel regression as
a special case (see Section A.4). As a result, this approach allows us to probe the effect of the loss
functions used to train neural networks — which, as we will see, clearly influences the nature ofDD
— and reveals novel insights into the properties of neural networks near the interpolation threshold.
Contributions. (a) Section 3: We derive a generalization expression based on a ‘add-one-in’
(akin to ‘leave-one-out’) procedure that requires minimal assumptions and, in principle, allows us
to analyze the population risk of any neural network or linear/kernel regression model. (b) Sec-
tion 4: We show how this yields a suitable asymptotic lower-bound, which diverges at the inter-
polation threshold and helps explain double descent for neural networks via the Hessian spectra
at the optimum. We additionally support the theoretical arguments by an empirical investigation
of the involved quantities, which also suggests its applicability in the non-asymptotic setting. (c)
Section 5: We thoroughly study the effect of the loss function by exploring in detail the double
descent behaviour for cross-entropy loss, such as the location of the interpolation threshold, Hessian
spectra near interpolation, as well as discuss novel insights implied by these observations. (d) Sec-
tion 6: Lastly, as a by-product of our influence functions based approach, we derive a generalized
closed-form expression for leave-one-out loss that applies to finite-width neural networks.
2	Setup and background
Let us consider the general setting of statistical estimation. Assume that the input samples z ∈ Z
are drawn i.i.d. from some (unknown) distribution D. A parametric model can be defined as a
family of probability distributions Dθ on the sample space Z, where θ ∈ Θ. The main task is
to find an estimate θ of the parameters such that “ Dbθ ≈ D ”, but given a finite set of samples
S = {zι,…，zn} drawn i.i.d from D. The parameter estimator, θ, is considered to be provided by
some statistic T(zι, ∙∙∙ , Zn). Alternatively, if We denote the empirical distribution of the dataset S
by Dn, then the parameter estimator can be written as a functional, θDn := θ(Dn) = T(Dn).
2.1	Influence Functions: a quick primer
Quite often we are interested in analyzing how a slight contamination in the distribution affects, say,
the estimated parameters θ, or some other statistic T. Specifically, when this contamination can be
expressed in the form of a Dirac distribution δz, the (standardized) change in the statistic T can be
obtained via the concept of influence functions.
Definition 1. (Hampel et al. (1986)) The influence function IF ofa statistic T based on some distri-
T((1-)D+δz)-T(D)
bution D, is given by,	IF(z; T, D)= lime→o------------------------, evaluated on a point
z ∈ Z where such a limit exists.
Essentially, the influence function (IF) involves a directional derivative ofT atD along the direction
of δz . More generally, one can view influence functions from the perspective of a Taylor series
expansion (or what is referred to in this context as the first-order von Mises expansion) of T at D,
evaluated on some distribution De close to D: T(De) = T(D) + R IF(z; T, D) d(De -D)(z) + O(2) .
So, as evident from this, it is also possible to utilize higher-order influence functions as considered
in Debruyne et al. (2008). However, the first-order term is usually the dominating term and to
ensure tractability when we later consider neural networks — we will restrict our attention to first-
order influence functions hereafter. Influence functions have a long history as an important tool,
particularly in robust statistics (Hampel et al., 1986), but lately also in deep learning (Koh & Liang,
2017). Not only do they have useful properties (see Appendix B.1 for a detailed background), but
they form a natural tool to analyze changes in distribution (e.g., leave-one-out estimation).
2
Published as a conference paper at ICLR 2022
2.2	Influence function for Maximum likelihood type Estimators
In general, we do not explicitly have the analytic form of the estimator θ — but implicitly as the
solution to an optimization problem (e.g., parameters of neural networks obtained via training). So,
let us consider influence functions for the class of ‘maximum likelihood type’ estimators (or M-
estimators), i.e. bθ satisfying the implicit equation, E	ψ(z; bθ) = 0. Let us assume that the
Z〜D
function ψ(z; θ) := Vθ'(z,θ), where the (loss) function ' : ZX Θ → R is twice-differentiable
in the parameters θ. When the loss ` is the negative log-likelihood, we recover the usual maximum
likelihood estimator, θ = arg min0∈θ EZ〜D ['(z,θ)] . The following proposition describes the
influence function for such an estimator θ (all the omitted proofs can be found in Appendix A).
Proposition 2. The (first-order) influence function IF of the M-estimator bθD based on
the distribution D, evaluated at a point z, takes the following form: IF(z; bθD, D) =
- hHL (bθD)i
^
^
^
Vθ'(z,θD), where, the Hessian matrix HL (Θd) := Vθ L(Θd) contains the second
derivative of the loss L(θ) := E [`(z, θ)] with respect to the parameters θ.
Z〜D
Remark. The Hessian in neural networks is typically rank deficient (Sagun et al., 2017; Singh
et al., 2021), so for the sake of our analysis we will consider an additive regularization term λI,
λ > 0, alongside the HL(θD) term in the above-mentioned IF formula and call this modification
IFλ . Later, we will take the limit λ → 0.
3	Expression of the population risk
As we lack access to the true distribution, we usually take the route of empirical risk minimization
and consider bs := argmine∈e LS(θ) with LS(θ) := * PZi '(zi,θ) and recall, S = {zi}in=1 is
the training set of size n. However, in regards to performance, our actual concern is the population
risk L(θ) := EZ〜D ['(z,θ)], which is measured empirically via the loss Ls'(θ) on some unseen
(test) set S0. To analyze double descent (Belkin et al., 2019) — i.e., with increasing model capacity,
the population risk exhibits a peak before descending again (besides the usual first descent) — we
first derive a suitable expression of the population risk that will also apply to neural networks.
‘Add-one-in’ procedure.
Let us consider how the parameter estimate bθS changes when an addi-
tional sample z0 〜D is included in the training set S. We can cast this as a contamination of the
corresponding empirical distribution Dn to yield an altered distribution Dn0 +i = (1 - )Dn + δZ0 ,
with the contamination amount e = *+^ . Thanks to the influence function of the parameter
estimate (Proposition 2 for D = Dn ), we do not have to retrain and explicitly measure the
change in final parameters, but have an analytic expression given as follows, IFλ(z0; bθS, Dn ) =
- [hL(Θs) + λl] Vθ'(z0, bs), where the superscript S in HL denotes the computation of the
Hessian on the set S. Now using the chain rule of influence functions, we can express the influence
on 'zθ := '(z0,θ) as, IFλ('zo; bs, Dn) = -Ve '(z0, ‰ )>[HL(bs)+ 入/乜 '(z0, bs) . When
|S| = n is large enough to ignore O(n-2) terms, the (infinitesimal) definition of influence function
is equivalent to using the finite-difference form. This allows us to directly express the change in loss
which we leverage to derive an expression of the test loss, in Theorem 3 below.
Theorem 3.	Consider the parameter estimator bs based on the set ofinput samples S of |S| = n.
Then the population risk L(bs) := EZ〜D
['(z, bs)]) takes the following form,
LGS) = LS (bs) + n+1 Tr QhL (bs) + λl]-1CD (bs)) + O (J) ,	(1)
1	% ∕Λ ʌ	E
where LS(bs) := Ez0〜D
[' (z0, bs∪{Z,})]
is the expectation of 'one-sample training loss' and
CD (bs) := Ez0〜D Vθ'(z0, bs) Vθ'(z0, bs)>
is the (uncentered) covariance ofloss gradients.
3
Published as a conference paper at ICLR 2022
Remark. Note, the first term in the right-hand side of eq. 1 is not exactly the training loss, but
deviates by a negligible quantity related to the expected difference between the loss of a training
sample and the average loss on the remaining training set (as discussed in Section A.1.2). When
trained sufficiently long so that the loss on individual samples is close to zero (i.e., the interpolation
setting), this quantity becomes inconsequential for the purpose of analyzing double descent.
Related work. The more interesting quantity in eq. 1 is the second term on the right, which is
reminiscent of the Takeuchi Information Criterion (Takeuchi, 1976; Stone, 1977) and a similar term
appears in several works on neural networks (Murata et al., 1994; Thomas et al., 2019) as well as
in the analyses of least-squares regression (Flammarion & Bach, 2015; Defossez & Bach, 2015;
Pillaud-Vivien et al., 2018). However, an important difference is that in our expression the Hessian
HSL evaluated on the training set appears, whereas it is based on the entire true data distribution
in prior works. This seemingly minor difference is in fact crucial, since studying double descent
necessarily involves analyzing the relation between the training set size and the model capacity
(such as the number of parameters). Also, directly taking the results from previous works and
merely approximating the involved quantities based on the training set, such as the Hessian and the
covariance CLD, does not work either. Since in such a scenario, when the limit of the regularization
strength λ → 0, results from prior works reduce to something ineffective for further analysis. E.g.,
for mean-squared loss, to the rank of Hessian at the optimum (see Section A.2.2 for more details).
4 Lower bound on the population risk
As a brief outline, our strategy to theoretically illustrate the double descent behaviour will be to
show that the lower bound of the population risk diverges around a certain threshold. Before pro-
ceeding further, we would like to emphasize that so far we have not employed any assumptions on
the structural form of the model or the neural network, as well as neither on the data distribution.
So let us now introduce some relevant notations to describe the precise setting for our upcoming
result. We assume that the samples z are tuples (x, y), where the input x ∈ Rd has dimension
d and the targets y ∈ RK are of dimension K . Let us consider the neural network function is
fθ(x) := f(x,θ) : Rd × Rp 7→ RK , where we have taken θ ∈ Rp . The parameter estimator, bθS,
in this context refers to the parameters obtained by training to convergence using the training set S,
and which we will henceforth denote by θ? := bθS (to emphasize the fact that we are at the local
optimum). Also, the loss function `(z, θ), with a slight abuse of notation, refers to ` fθ(x), y in
this setting. Let US additionally define a shorthand ' := '(fθ(xi), yi). Next, let US discuss in more
detail the two matrices that appear in Theorem 3, before we introduce the assumptions we require.
First, the Hessian matrix of the loss, can in general be decomposed as a sum of two other matri-
ces (Schraudolph, 2002): outer-product Hessian HoS(θ) and functional Hessian HfS(θ), i.e.,
n	nK
HL(θ) = HS(θ)+Hf (θ) = n X Vθfθ(Xi)[vf`i] Vθfθ(Xi)> + n X X[Vf`i]k V2fk(Xi)
i=1
i=1 k=1
(2)
where, Vθ fθ ∈ Rp × K is the Jacobian of the function and Vf ' ∈ RK × K is the Hessian of
the loss with respect to the function. Next, the other matrix that appears in eq. 1 is the (uncen-
tered) covariance of loss gradients, CD (θ) = E(x,y)〜D [v® 'fθ (x), y) Vθ 'fθ (x), y)>], with
Vθ'fθ(x), y) = Vθfθ(x) Vf'(fθ(x), y) where Vf' ∈ RK is the gradient of the loss with re-
spect to the function. Similar to the covariance of loss gradients, let us define the covariance of func-
tion Jacobians, which we will denote by CfS when computed over the samples in set S and which can
beexpressedas CS (θ)=由 ZS (θ)Zs (θ)> ,with ZS := [Vθ fθ (xι)…Vθ f6 (x∣s∣)]> ∈ Rp×K ⑶.
4.1 Lower Bound
We employ the following assumption to obtain an appropriate lower bound of the population risk:
Assumption A1. There exists a sample (x, y)〜D with non-zeroprobability α such that qQ y):
kVf'(fθ*(χ),y)k2 > 0.	’
Finally, we are in a position to state our main theorem (all the proofs are in Appendix A).
4
Published as a conference paper at ICLR 2022
Theorem 4.	Under the assumption A1 and taking the limit of external regularization λ → 0 and
K = 1, we obtain thefollowing lower bound on the population risk (eq. 1) at the minimum θ?
L(θ?) ≥ LS(θ?) +
λr (HL(θ?))
(3)
1
n + 1
where we consider the convention λι ≥ ∙∙∙ ≥ λr forthe eigenvalues with r := rank(HL(θ?)), and
σ m in denotes the minimum σ(x,y) = ∣∣Vf '(fθ*(x), y) ∣∣2 over DD, i.e., (x, y)〜D :。、y)> 0.
The key takeaway of this theorem is that the lower bound on the population risk is inversely pro-
portional to the minimum non-zero eigenvalue λr of the Hessian at the optimum HSL(θ?), which —
as we will see shortly — largely characterizes the double descent like the behaviour of the popu-
lation risk. Besides, we would like to emphasize that the primary purpose of the lower bounds is
to isolate the source of double descent, and thus the practical applicability of the lower bounds —
which is in itself an open research area — is of secondary concern. As a result, in our analysis we
r
lower bound the quantity E λ,(HS(θ*))
original quantity in a different context.
by
1
λr (HL(θ*))
, although it might be of interest to keep the
Remark. The assumption A1 just requires the existence of such a point from the true distribution
D with non-zero probability. Notice, otherwise, we would have zero population risk, which is
e?
obviously of no interest. The benefit of this assumption is that we can analyze CfD (θ?) instead of
CLD(θ?) by taking the minimum non-zero σm2 in outside of the corresponding expression.
4.2	Double descent behaviour
Having established this lower bound, we will utilize it to demonstrate the existence of the double
descent behaviour. Let Us consider the case of mean-squared error (MSE) loss, '(fθ(x), y) =
2 ky — fθ(x)k2, and where the Hessian of the loss with respect to the function is just the identity,
i.e., Vf' = I. We will employ the following additional assumptions:
Assumption A2. The functional Hessian at the optimum θ? is zero, i.e., HfS(θ?) = 0 .
Assumption A3. The minimum non-zero eigenvalue λmin of covariance of function Jacobians at
optimum θ? is bounded by the corresponding one at initialization θ0 over some common
A λmin(CfS (θ0)) ≤ λmin(CfS(θ?)) ≤ B λmin(CfS (θ0)), with constants 0 < A, B < ∞.
set S, i.e.,
Assumption A4. The columns ofZS(θ0) are sub-Gaussian independent random vectors at initial-
ization θ0.
Note on the assumptions. Assumption A2 is known from prior works (Sagun et al., 2017; Singh
et al., 2021) to hold empirically — in particular, c.f. Figures 5, S2 of Singh et al. (2021) where it
is shown that the rank of the functional Hessian converges to 0 when trained sufficiently (besides,
c.f. Appendix C.8). Also, in the setting of double descent, the individual losses (and their gradients)
are themselves close to zero near the interpolation threshold, thereby making the functional Hessian
vanish at the optimum (recall eq. 2). Next, the assumption A3 essentially guarantees that, at the
optimum, the minimum non-zero eigenvalue of the covariance of function gradients does not change
much relative to that at initialization. Importantly, this is purely for the purposes of the lower bound
— not something that we impose as a constraint during training (vis-a`-vis the NTK regime). The
existence of the constants mentioned in this assumption A3 can be theoretically justified by the
fact that the map A 7→ λi(A) is Lipschitz-continuous on the space of Hermitian matrices, which
follows from Weyl’s inequality (Tao, 2012, p. 56). Besides, in Figure 2a, we empirically justify this
assumption, and in the adjoining Figure 2b we also validate our lower bound to the population risk
throughout the double descent curve (additional details of which can be found in Appendix C.10).
The last assumption may seem more demanding but is mild in comparison to that in prior work (Pen-
nington & Bahri, 2017), where all entries of the covariance of function gradients at the optimum are
considered to be independent and identically distributed. In contrast, our assumption just requires
sub-gaussianity only at initialization. Similar sub-gaussianity assumptions are also common in the
regression-based analyses of double descent (Muthukumar et al., 2019; Bartlett et al., 2020). The
5
Published as a conference paper at ICLR 2022
benefit of such an assumption is that it allows us to precisely characterize the behaviour of the min-
imum eigenvalue that appears in Theorem 4 using results from Random Matrix Theory (Vershynin,
2010), and thereby that of double descent, as described in the upcoming Theorem 5. Lastly, before
proceeding ahead, let us mention that the Appendix A.4 discusses concrete settings where all the
above assumptions hold simultaneously for finite-width neural networks.
Theorem 5. For the MSE loss, under the setting of Theorem 4 and the assumptions A2, A3, A4,
the population risk takes the following form and diverges almost Surely to ∞ at P = √1c n,
σ 〜*	σ2in ɑAλ min (CD (θ0))
L(θ?) ≥ LS (θ ) + ------------------------------, , in the asymptotic regime of p, n → ∞ but
"	B kCD(θ0)k2 (√ -c√P)2
their ratio is a fixed constant, and where c > 0 isa constant that depends only on the sub-Gaussian
norm ofcolumns of ZS (θ0).
Takeaways. (a) Firstly, the point where the second descent occurs, i.e, the interpolation thresh-
old, is typically p ≈ n in the setting of K = 1. So our result from Theorem 5 (which is in this
setting) not only implies a divergence at the interpolation threshold, but further illustrates that the
complexity term will get smaller as its denominator increases forp >> n, — thereby also capturing
the overall trend of population risk. (b) Second, while the above result holds in the asymptotic
setting, we empirically show that such a behaviour also takes place for p and n as small as a few
hundreds or thousands, as discussed in Section 4.3 ahead.
Interpretation of the interpolation threshold location. An interesting empirical observation,
very briefly alluded to in prior works (Belkin et al., 2019; Greydanus, 2020), is that for the MSE loss
with K targets, the interpolation threshold is instead located at p ≈ Kn. This can be reconciled by
looking the rank r of the Hessian at the optimum, as inherently the divergence at the interpolation
threshold is because the Hessian’s minimum non-zero eigenvalue λr vanishes. More intuitively,
double descent occurs at the transition when the Hessian rank starts being dictated by the # of
samples (i.e., when over-parameterized) rather than being governed by the # of parameters (i.e.,
when under-parameterized), e.g., for MSE when p ≈ Kn and note Kn is precisely the rank of the
Hessian in the over-parameterized regime (Kn > p). As a matter of fact, in practice, there might be
redundancies due to either duplicates or linearly dependent features/samples, as well as parameters.
For instance, in Figure 15 we show a simple example of linear regression where the interpolation
threshold can be moved arbitrarily by changing the extent of redundancy in the design matrix. But,
as demonstrated therein, thinking in terms of the Hessian rank can help avoid such inconsistencies.
Other facets of double descent. (i) Our analysis additionally explains the empirical observation
of label noise accentuating the peak (Nakkiran et al., 2019), since the complexity term contains a
multiplicative factor of σm2 in which increases proportionately with label noise. (ii) The exact term that
appears in our proof, before we take the limit of regularization λ → 0, is (λr (HL(θ?)) + λ) 1. This
reveals why, when using a regularization ofa suitable magnitude, double descent is not prominent or
disappears, as also noted in (Nakkiran et al., 2019; 2020), — since this term can no longer explode.
4.3	Empirical verification
-⅛- Population loss -∙- Minimum noι>zero eigenvalue
(a) L = 2, n = 500, K = 10
(b) L = 3, n = 200, K = 10
Figure 1: MSE Loss: Behaviour of the population (test) loss and minimum non-zero eigenvalues
for the setting of L = 2 and L = 3 layer networks on downscaled MNIST (Greydanus, 2020). The
results are averaged over 5 seeds and the shaded interval denotes the mean ± std. deviation region.
6
Published as a conference paper at ICLR 2022
To empirically demonstrate the validity of our theoretical results, we carry out the entire proce-
dure of obtaining double descent for neural networks. Namely, this involves training a large set of
neural networks — with layer widths sampled in regular intervals — for sufficiently many num-
ber of epochs. However, in our case, there is another factor which makes this whole process even
more arduous — the lower bound depends on the minimum non-zero eigenvalue of the Hessian.
As a result, we cannot resort to efficient Hessian approximations based on, say, Hessian-vector
products (Pearlmutter, 1994), but rather we need to compute entire Hessian spectrum — which has
O(p3) computational and memory costs. Hence, we cannot but restrict our empirical investigation
to smaller network sizes. Nevertheless, on the positive side we can thoroughly assure the accuracy
of our empirical investigations — since we compute the exact Hessian and its spectrum, and that too
in Float64 precision.
In terms of the dataset, we primarily utilize MNIST1D (Greydanus, 2020), which is a downscaled
version of MNIST yet designed to be significantly harder than the usual version. However, we also
present results on CIFAR10 and the usual (easier) MNIST, which alongside other empirical details,
can be found in Appendix C. Figure 1 shows the results of running the double descent experiments
for the settings of two and three layer fully-connected networks with ReLU activation trained for 5K
epochs via SGD. Alongside the population loss (empirically measured on a test set) — which peaks
at the interpolation threshold of p ≈ Kn — we plot the trend of the minimum non-zero eigenvalue.
As predicted by our theory, this eigenvalue indeed tends to zero (note the log scale), in both the
settings, around the precise neighborhood where the population loss takes its maximum value.
S3β S-Isfs a⅝Λ8cεnεcM
-A-PoNaHcnigs Lowtrbound
500 IMO IsM xoa 25β0	88
*of PeraMegre
S 3« a WnlXI -eMcη
(a) Minimum non-zero eigenvalue stays close. (b) Lower bound captures the trend of population loss.
Figure 2: Empirical validity of assumption A3 & the lower bound of Theorem 4 in Double Descent.
5	The Case of Cross-Entropy
Greydanus (2020) notes that in the case of cross-entropy, the population loss peaks at p ≈ n (unlike
at p ≈ Kn for the MSE loss). To better understand this aspect, let us start by checking the form of
the Hessian at the optimum for the cross-entropy loss, which by our assumption A2, will be given
by the outer-product term. as in eq. 2. Now, the main difference is that the Hessian of the loss with
respect to the function (named ‘output-Hessian’), instead of being identity like in MSE, is given
by Vf'(fθ(x), y) = diag(p) - pp>, where P = Softmax(fθ(x)) denotes the predicted Class-
probabilities obtained from applying the softmax operation. In general, this output-Hessian matrix
of size K × K, is rank-deficient with rank K - 1. Thus, the rank(HoS (θ)) = min (p, (K - 1) n)
for any θ, and the initial surmise would be that the interpolation threshold is at p ≈ (K - 1) n.
But, this is not in line with the stated observation from Greydanus (2020). Hence, let us take another
closer look at the Hessian, and in particular, V2f `. Notice that near interpolation, when the loss on
individual training samples tends to zero, the predicated class-probability p will tend to 1 for the
correct class (as per the training label) and 0 elsewhere. This suggests that Hessian matrix collapses
to 0 since V2f `(fθ (x), y) = diag(p) - pp> → 0 . And indeed, this is true based on our empirical
results, displayed in Figure 3 given the network is trained sufficiently long. We clearly observe that
the population loss diverges atp ≈ n, however the test error, although not as conspicuous, still shows
a slight peak (similar to the curves in (Nakkiran et al., 2019) without label-noise). Next, from the
right sub-figure (plotted in log-scale), it also becomes evident that the entire Hessian spectrum —
from the maximum to the minimum eigenvalue — collapses to zero near the interpolation threshold.
Fact 6. Thus, we have that for the cross-entropy loss, near the interpolation threshold of p ≈ n,
the entire Hessian matrix vanishes at the optimum, i.e., HSL (θ?) = 0.
7
Published as a conference paper at ICLR 2022
*— Population loss Interpolation threshold —(best) Test error Train error
80
70
¢0
50
40 e
30
20
10
0
5∞0
Population loss -∙-- Train loss
3。_) sso—lu。-:Ie-nd£
Λl
(8-)"3n-e>u36H PUeSSO_1U-El
SSo-I U qs-ndod
Figure 3: (Left) Double descent curve for CE loss and (right) log-scale plots of the test and train
loss, alongside Hessian eigenvalues, for a one-hidden layer neural network trained for 40K epochs.
Importantly, the above fact (and our empirical results) report such a behaviour only around p ≈ n,
and not well into the over-parameterized regime p >> n. Therefore, a straightforward consequence
of the above fact and empirical observations is that λr(HSL(θ?)) = 0 near this interpolation thresh-
old, implying that the population loss in case of cross-entropy loss diverges at p ≈ n.
Vanishing Hessian hypothesis. Figure 3 right, seems to suggest, rather surprisingly, that even
the training loss has the same trend as these Hessian eigenvalue statistics. Note, this is not the case
that networks were not trained long enough — rather, we run them for 40, 000 epochs. In fact, we
even trained models, right to the interpolation threshold, up to 80, 000 epochs. Yet, the training loss
only drops by factor of 2 and is still > 1e - 5, compared to exact 0 at machine precision in half
the epochs for networks lying in the region where the test loss diverges. This leads us to posit the
following hypothesis: around p ≈ n, SGD finds critical points with zero Hessian for neural networks
trained with CE loss and over-parameterization beyond the interpolation threshold helps completely
avoid or significantly decelerate convergence to such critical points. Further investigation into this
hypothesis is beyond the current scope, but forms an exciting direction for future work.
6	Leave-one-out estimate via influence functions
Leave-one-out (LOO) is also known to provide a reasonable estimate of the population loss (Pontil,
2002). The principle behind it is to leave behind one sample, optimize the model from scratch on the
remaining n - 1 samples, then evaluate the loss on the left-out sample, and finally average over the
choice of the left-out sample. In our setting for double descent, we prefer the add-one-in procedure
since it directly gives the population loss itself. However, LOO can still be useful from a practical
perspective as it relies only on the training set — assuming we can analytically estimate it via some
closed-form expression which avoids the need to train n models in the otherwise naive manner.
Similar to the add-one-in procedure from before, leave-one-out can be cast as a slight contamination
of the distribution. We can express the new distribution Dn\i-1 with the i-th sample removed as
follows, D：-i = (1 - e)Dn + eδzi with e = n--1ι, where Dn refers to the original empirical
distribution over the training set. Now, we can carry out similar steps like for add-one-in, and
derive the change in the parameter estimate as well as the change in loss over the left-out sample.
But, here we additionally analyze the effect of incorporating the second-order influence function,
apart from the usual first-order influences. This provides us with estimates LOO(1) and LOO(2),
the expressions of which can be found in Appendix A.6.1. As a quick test-bed, we investigate the
fidelity of these two approaches relative to the exact formula LOOLS in the case of least-squares.
Theorem 7. Consider the particular case of the ordinary-least squares (OLS) with the training
inputs gathered into the data matrix X ∈ Rn×d and the targets collected in the vector y ∈ Rn.
Under the assumption that the number of samples n is large enough, we have that LOO(2) =
n
LOO ls = 1 P
n J
i=1
x>(X>X) 1Xi denotes the i-th diagonal entry of the matrix A = X(XτX) 1Xτ (i.e., the
so-called 'hat-matrix') and θ denotes the usual solution of (XτX)	Xτy obtained via OLS.
yi _8AXi! ，and LOO(I) = n PUi - θ>x)	AA- . where, Aii
-Cii	i=1	- ii
8
Published as a conference paper at ICLR 2022
This result is reassuring as it shows that LOO expressions from influence function analysis are accu-
rate, and we recover the least-squares formula as a special case through LOO(2) . Further,
Corollary 8. For any finite-width neural network, the first and Second-order influ-
ence function give similar formulas for LOO like that in Theorem 7, but with A =
ZS(θ*)> (Zs(θ?)Zs(θ?)>)-1 ZS(θ?), where ZS(θ?) := [V@fθ*(xι),…，V©fθ*(xn)] and θ?
are the parameters at convergence for MSE loss.
Finally, the above result raises a concern about the sub-optimality of first-order influence functions
when used in the leave-one-out framework. While this is not necessarily a significant concern for a
theoretical analysis, say that of double descent, this can be relevant from a practical viewpoint (Basu
et al., 2020). However, an empirical investigation on this front remains beyond the current scope.
7 Discussion
Summary. We derived an expression of the population risk via influence functions and obtained
a lower bound to it — with fairly minimal assumptions — that applies to any finite-width neu-
ral network trained with commonly used loss functions. The lower bound is inversely related to
the smallest non-zero eigenvalue of the Hessian of the loss at the optimum. When specialized to the
MSE loss, this provably exhibits a double descent behaviour in the asymptotic regime and we empir-
ically demonstrated that this holds even in much smaller non-asymptotic settings. We also analyzed
the intriguing phenomenology of double descent across other losses — through our Hessian-based
framework — which explained existing empirical observations as well as uncovered novel aspects
of neural networks near interpolation. Finally, as a by-product, we presented theoretical results for
leave-one-out estimation using influence functions in the case of neural networks.
Related theoretical work on Double Descent. We carve out a niche in the growing set of studies
on double descent by focusing primarily on — finite-width neural networks. For the linear/kernel
regression setting (or lately, the nearly equivalent two-layer network with frozen hidden-layer), there
is a plethora of existing work (Advani et al., 2020; Bartlett et al., 2020; Mei & Montanari, 2019;
Muthukumar et al., 2019; Geiger et al., 2020; Ba et al., 2020), that analyzes double descent. Thus,
our aim is not to make these existing analyses tighter, but rather to take a step towards developing
analyses that directly hold for finite-width neural networks. Therefore, unlike the above works,
we do not impose any restrictive assumptions on the structure of neural network, like two-layer
networks, or the optimization methods used to train them, like gradient flow. Yet, our work also
bears a natural connection between the matrix whose spectrum comes to be of concern in the prior
works — the input-covariance or kernel matrix in linear or kernel regression — while that of the
outer-product Hessian in our work for MSE loss (which has the same spectrum as the ‘empirical’
NTK). But our strategy also makes our work applicable to cross-entropy, e.g., where we bring to
light the interesting observations near the interpolation threshold.
A closely related recent work, Kuzborskij et al. (2021), links the population risk for least-squares
to the minimum non-zero eigenvalue of the input covariance matrix — but although via an upper-
bound, which is insufficient for illustrating double descent. Nevertheless, in analogy to their regres-
sion result, they study the minimum eigenvalue of the covariance matrix consisting of penultimate-
layer features and conjecture this as a possible extension to neural networks. However, the input-
covariance matrix in least-squares is also the Hessian, and as we have thoroughly established the
Hessian (at the optimum) is indeed the relevant object — thus contradicting their conjecture.
Limitations and directions for future work. There are many important aspects surrounding
double descent that remain unanswered, in the context of neural networks: (a) Analogous to (Hastie
et al., 2019) for linear regression, what are the conditions for the global optimum to lie in the over-
parameterized regime instead of under-parameterized? (b) Given the vanishing Hessian hypothesis,
is there a qualification to the regime where the flat-minima generalizes better hypothesis (Hochreiter
& Schmidhuber, 1997; Keskar et al., 2016) holds — since the Hessian is the flattest possible here.
(d) On the technical side: better characterization of the mentioned cross-entropy phenomenon as
well as non-asymptotic results. Overall, we hope that our work will encourage foray into further
studies of double descent, that are specifically built for finite-width neural networks.
9
Published as a conference paper at ICLR 2022
Reproducibility statement
•	All the omitted proofs to the theoretical results can be found in the Appendix A.
•	In regards to empirical results, we provide all the relevant details and additional results in
the Appendix C.
•	The corresponding code for the experiments is located at https://github.com/sidak/double-
descent.
Acknowledgements
We would like to thank Simon Buchholz for proofreading an early draft of the paper. Besides, we
thank the members of DA lab for useful comments. Sidak Pal Singh would also like to acknowledge
the financial support from Max Planck ETH Center for Learning Systems and the travel support
from ELISE (GA no 951847).
References
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-
layer neural networks: An asymptotic viewpoint. In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/forum?id=H1gBsgBYwH.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile,
2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019. ISSN 0027-8424. doi: 10.1073/pnas.1903070116.
URL https://www.pnas.org/content/116/32/15849.
Michiel Debruyne, Mia Hubert, and Johan A.K. Suykens. Model selection in kernel based regression
using the influence function. Journal of Machine Learning Research, 9(78):2377-2400, 2008.
URL http://jmlr.org/papers/v9/debruyne08a.html.
Alexandre Defossez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and
optimal sampling distributions. In Artificial Intelligence and Statistics, pp. 205-213. PMLR,
2015.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics, pp.
586-596, 1981.
Yuguang Fang, Kenneth A Loparo, and Xiangbo Feng. Inequalities for the trace of matrix product.
IEEE Transactions on Automatic Control, 39(12):2489-2490, 1994.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size,
2015.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d,Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment,
2020(2):023401, 2020.
10
Published as a conference paper at ICLR 2022
Sam Greydanus. Scaling down deep learning, 2020.
Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics:
the approach based on influence functions, volume 196. John Wiley & Sons, 1986.
Markus Harva and Ata Kaban. Variational learning for rectified factor analysis. Signal Processing,
87(3):509-527, 2007.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural computation, 9(1):1T2, 1997.
Roger A. Horn and Charles R. Johnson. Topics in Matrix Analysis. Cambridge University Press,
1991. doi: 10.1017/CBO9780511840371.
Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks, 2018.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deeP learning: Generalization gaP and sharP minima. arXiv
preprint arXiv:1609.04836, 2016.
Pang Wei Koh and Percy Liang. Understanding black-box Predictions via influence functions, 2017.
Ilja Kuzborskij, Csaba Szepesvari, Omar Rivasplata, Amal Rannen-Triki, and Razvan Pas-
canu. On the role of oPtimization in double descent: A least squares study. arXiv preprint
arXiv:2107.12685, 2021.
Marco Loog, Tom Viering, Alexander Mey, Jesse H. Krijthe, and David M. J. Tax. A brief prehistory
of double descent. Proceedings of the National Academy of Sciences, 117(20):10625-10626, May
2020. ISSN 1091-6490. doi: 10.1073/pnas.2001875117. URL http://dx.doi.org/10.
1073/pnas.2001875117.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
2019.
N. Murata, S. Yoshizawa, and S. Amari. Network information criterion-determining the number of
hidden units for an artificial neural network model. IEEE Transactions on Neural Networks, 5(6):
865-872, 1994. doi: 10.1109/72.329683.
Vidya Muthukumar, Kailas Vodrahalli, and Anant Sahai. Harmless interpolation of noisy data in
regression. 2019 IEEE International Symposium on Information Theory (ISIT), Jul 2019. doi: 10.
1109/isit.2019.8849614. URL http://dx.doi.org/10.1109/ISIT.2019.8849614.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt, 2019.
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent, 2020.
Quynh Nguyen, Marco Mondelli, and Guido F Montufar. Tight bounds on the smallest eigenvalue
of the neural tangent kernel for deep relu networks. In International Conference on Machine
Learning, pp. 8119-8129. PMLR, 2021.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
11
Published as a conference paper at ICLR 2022
Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix
theory. In International Conference on Machine Learning, pp. 2798-2806. PMLR, 2017.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing
error for stochastic gradient methods. In Conference on Learning Theory, pp. 250-296. PMLR,
2018.
Massimiliano Pontil. Leave-one-out error and stability of learning algorithms with applications.
International Journal of Systems Science, 2002.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singu-
larity and beyond, 2017.
Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation, 14:1723-1738, 2002.
S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural net-
work hessian maps. In Advances in Neural Information Processing Systems 34 (NeurIPS 2021),
December 2021.
Mervyn Stone. An asymptotic equivalence of choice of model by cross-validation and akaike’s
criterion. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):44-47, 1977.
Kei Takeuchi. The distribution of information statistics and the criterion of goodness of fit of models.
Mathematical Science, 153:12-18, 1976.
Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.
Valentin Thomas, Fabian Pedregosa, Bart van Merrienboer, Pierre-Antoine Mangazol, Yoshua
Bengio, and Nicolas Le Roux. Information matrices and generalization. arXiv preprint
arXiv:1906.07774, 2019.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
12
Published as a conference paper at ICLR 2022
A Omitted Proofs
Proposition 2. The influence function IF of the M-estimator bθD based on the distribution D, evalu-
ated at point z, takes the following form:
-1
IF(z; bD, D) = - [Hl(Θd)] Vθ'(z, θQ),
(4)
^
^
where, the Hessian matrix HL (θD) := Vθ2 L(θD) is the matrix of second-derivatives of the loss
L(θ) := E [`(z, θ)] with respect to the parameters θ.
Z〜D
Proof. Let bbD = arg mine∈Θ	E [`(z, b)] . Instead of this formulation, we can also define the
Z〜D
estimator as one that satisfies the following implicit equation (assuming the derivative can be moved
inside expectation),
E [Ve'(z,θ)]=0,
Z〜D
(5)
which is nothing but the first-order stationary point condition. Now, consider a contaminated distri-
bution D := (1 - e)D + e δZ. The corresponding implicit equation, to be satisfied by the estimator
〜
^
θDe corresponding to D, can be written as follows:
E~[Vθ '(z,b)]=0
Z〜D
(1 - e) E [Ve'(z,b)]+ eVθ'(z,b)=0
、	'Z 〜D
(6)
Take the derivative of the above expression with respect to (plus interchanging derivative and
expectation), we get:
-d (1-e) En[Ve'(z, b)] = --deVθ'(z, θ)
de	Z〜D	de
db	db
-En[Vθ'(z,b)] + (1-e) E [Vθ'(z,θ)]7 = -Ve'(z,θ) - eVθ'(z,θ)--	(7)
Z 〜D	Z 〜D	de	de
~	r 」	........	. λ	I	.1	。	C
Since, bDe satisfies the above eq. 7, let us substitute it in place of b and analyze the case for e → 0.
We are left with the following (after removing terms multiplied with e):
^
dθ
-En[Vθ'(z, bD)]+ En[Vθ'(z, bD)] -De = -Ve'(z,呢)
Z〜D	Z〜D	de
(8)
Now, the first term goes to zero as e → 0, because eq. 6 holds for bDe , as shown below :
^
^
_ ■一	■， ^ ,,
-En[Vθ'(z, bD)]
Z〜D
-ZED[Ve'(z,θD)] + ZED[Ve'(z,θD)]
e (Ve'(zo,be) - zEyVe'(z，bD)〕) ∙
Further, as e → 0, one can replace bDe by bD in the expressions of the gradient and Hessian of `.
Then, assuming that the Hessian HL (Θd) := Ve E。['(z, b)] = E。忖2'(z, Θd)] is invertible,
this yields:
IF(z;bD,D) = - [HlMd)i-1 Vθ'(z,bD),
(9)
□
13
Published as a conference paper at ICLR 2022
A. 1 Expression of the population risk
Theorem 3. Consider the parameter estimator bθS based on the set of input samples S of |S | = n.
Then the population risk L(bs):= Ez〜D ['(z, bs)]) takes thefollowingform,
LGS) = LS(bs) + n+1 Tr QhL(bs) + λl]-1CD(bs)) + O (!) ,	(10)
where LS(bs) := Ez，〜D [' (z0, bs∪{z0} denotes the expectation of ‘one-sample training loss’
and CD (bs) := Ez，〜D Vθ'(z0, bs) Nθ'(z0, bs) is the (UnCentered) CovarianCe of loss gradi-
ents.
Proof. Let us recall the expression of influence function for the loss on the new sample z0,
IFλ('z0；bs,Dn) = -Vθ'(z0,bs)>[HL(bs)+ λl]-1Vθ'(z0,θs) .	(11)
When |S| = n is large enough to ignore O(n-2) terms, the (infinitesimal) definition of influence
function is equivalent to using the finite-difference form. Then the change in loss can be expressed
as,
'(z0,bs∪{zo}) — 'Hbs) = n-11 Vθ'(z0,bs)> [HL(bs) + λl] -H'(z0,bs),	(12)
where We have multiplied both sides by ∆e = e — 0 = n+ɪ. We leverage this relation to derive an
expression of the test loss as follows. Starting from eq. 12, we average out over the choice of an
additional sample z0 〜D:
z0ED h'(z0, bs∪{z0})i I，% h'(z0, bs)i =
Vθ'(z0,bs)> [HL(bs) + λl] -1Vθ'(z0,bs).
T E
(n + 1) z0〜D
Using properties of the trace and moving expectation inside, we get
z0ED ['Hbs)i = z0ED h'(z0,bs∪{zo})i + n+1 Tr ([HL(bs) + λl]-1CD(bs)) ,	(13)
where, CD(b) := Ez，〜D [Vθ'(ζ0,b) Vθ'(z0,b)>].
■X T ,1	,	,1 Fdl 1	∙ 1	,1 ∙	1	,,1	1	, ∙	∙ 1 C / 公 ∖
Now the term on the left-hand side is nothing but the population risk L(bs)
Ez〜D ['(z, bs)]),
and we coin the first term on the right-hand side as the expectation of ‘one-sample training loss’.
□
A.1.1 Remarks on the first-order influence function usage
Let us better understand when using the first-order influence function suffices by analyzing the
respective expression for the change in loss. First, let us apply the influence function of the parameter
estimator from eq. 9,
IF(z0; bbs, S) = -hHsL(bbs) + λIi-1Vθ'(z0, bbs),	(14)
for the add-one-in case of sample z0 to the training set S discussed above. Next, we can substitute
1
e = n++ι, and thereby obtain the change in parameters ∆b as:
∆b = -n-+1 [HL(bs) + λli-1Vθ'(z0,bs),	(15)
14
Published as a conference paper at ICLR 2022
Then, via first-order influences, we get the change in loss over the sample z0 as:
△'⑴(z0) = Vθ '(z0, bs )> ∆b = - n+ι Tr QhL (bs)+ λl]-1Vθ '(z0,m S Ns '(z0, bs )>),
(16)
In contrast for the second-order influence, we obtain the change in loss over the sample z0 as:
1>
△'⑵(z0) = Vθ'(z0, θs)> ∆θ + ]∆θ Vθ'(z0, θs) ∆θ	(17)
=— n+1 Tr QhL GS)+ λl]-1Vθ '(z0, bs )Vθ '(z0, bs )>)
Tr QhL(bs) + λli-1Vθ'(z0,bs) [HL(bs) + λl] -1Vθ'(z0,bsXIj'(z0,IS)>)
+	2(n +1)2	,
Later on, We take the expectation over the distribution, i.e., Z 〜D, but it is not relevant for ana-
lyzing the scale of the above mentioned change in loss obtained via first or second order influences.
Note, the Hessian is itself O(1) in terms of number of samples n, as it is an average of the per-
sample Hessians. Also, the numerator in both the above equations is a trace of a p × p matrix and,
overall, the numerator scales as O(p). Whereas, if We look at the denominator, the extra term in
△'⑵ scales as O(n-2), while the common term in △'⑵ and △'⑴ is of O(n-1). Hence, when n
is large enough such that n2 >> p, then the O(n-2) terms can be ignored, We can simply consider
the first-order influences.
A.1.2 Interpretation of the ‘one-sample training loss’
First, note that the first term on the RHS of eq. 13 is not exactly the training loss but rather some
related quantity. To see this better, let us rewrite as follows:
Z0Ed h'(z0, θs∪{z0})i = z0Ep '(z0, θs∪{z0}) - |S| '(s, θs∪{z0}) + z0Ep |S| '(s, θs∪{z0})
、------------------V-----------------}
∆TR(z0,s∪{z0})
where, '(S, θ) = Pi=1 '(zi,θ) is the sum of the loss over the samples in S. Further, the expression
△TR(z0, S ∪ {z0}) refers to the deviation between the loss of a training sample (here, z0) relative
to the average loss on rest of the training samples. The ‘TR’ in this symbol refers to this deviation
being computed on the given training set.
E h'(z0, bs∪{z0})i = oE nTR(z0,S ∪{z0})]+ oE [ɪ 队 S, bs∪{zo})
'〜D L	」 z0〜D	z0〜D |S|
= 0E [△TR(z0, S ∪ {z0})]
z0〜D
+ 0Ed [ɪ 仅(S ∪{z0},bs∪{z0}) - `(z0, bθs∪{z0} )
z0〜D |S | \	/ _|
Notice, the last term in the expression above is the same as the term on the left hand side — albeit
with an additional scaling factor of 看 and negative sign in front. Rearranging this results in the
following equation:
l⅞+1 An h'(z0,bs∪{z0})i = En [^TR(z0,S ∪{z0})]
|S | z0〜D	z0〜D
|S | + 1	1
+ ɪ ZOEDhSπp}i ZS ∪{z0}, θs∪{z0})
The only extra thing we have done is to multiply and divide by |S ∪ {z0}| = |S| + 1 in the last term
in the right hand side. Notice the rightmost term is an expectation (over z0) of the average training
15
Published as a conference paper at ICLR 2022
loss of the training set S ∪ {z0} and to which we assign the shorthand TR(S ∪ {z0}). Also, it is
evident from here that the ‘one-sample training loss’ is a quantity very much related to the training
loss. Lastly, considering the large |S| = n limit, We have limn→∞ n+1 = 1, and which thereby
yields:
~,个、	r . , ^	. ^ι	一. . , 一 一一	一	一 一一
L(θs ):= E	'(z0, θs∪{z0}) = E [∆TR(z0,S ∪{z0})]+ 吗[TR(S ∪{z0})]
z0~D	z0~D	z0~D
A.2 Lower bound to the population risk
Lemma 9. Consider two matrices A ∈ Rm×m and B ∈ Rm×m, where A is symmetric and B is
symmetric and positive semi-definite. Then the following holds,
λmin(A)Tr(B) ≤ Tr(AB) ≤ λmax(A)Tr(B)
Proof. See Fang et al. (1994).
□
Theorem 4.	Under the assumption A1 and taking the limit of external regularization λ → 0 and
K = 1, we obtain the following lower bound on the population risk ( eq. 1), at the minimum θ?
L(θ?) ≥ LeS(θ?) +
λr ML®))
(18)
1
n + 1
where we consider the convention λι ≥ ∙∙∙ ≥ λr for the eigenvalues with r := rank(HL(θ?)), and
σ mi in denotes the minimum oQy)= ∣∣Vf '(fθ*(x), y) ∣∣2 over D, i.e., (x, y) ~ D :。；叱。)> 0.
Proof. Let us recall the expression for the population risk that we proved in Theorem 3, forbθS = θ?
L(θ?) = LS(θ?) + nɪɪ Tr ([HL(θ*)+ λl]-1cD(θ?)) + O (n2) ,	(19)
In particular, we would like to analyze the complexity term T := Tr HSL (θ?) + λI-1cLD(θ?)
on the right-hand side and lower bound it. The full expression of this term T is given by,
T =(æ 3E~/Vθ'(fθ? (x), y)> [HL(θ*) + λl]-1Vθ'(fθ? (x), y)]	(20)
Now using the chain rule, we have that Vθ'(fθ*(x), y) = Vθfθ*(x) Vf'(fθ*(x), y). Then, for
K = 1, the above equation is equivalent to,
T = E	Vθ fθ? (x)>[HL(θ?)+ λl]-1Vθ fθ*(x) ∙ ∣Vf'(fθ*(x)k2
(x,y)~D X-----------------------------------} X------V-------}
A(x,y)	B(x,y)
(21)
Next, we take the lower bound by considering the minimum over all non-zero σ(2x,y) =
∣Vf '(fθ*(χ), y)k2,	'
T≥σm2inα	E	[A(x,y)]
(x,y)~D
= σm2 in α Tr [HSL(θ?) + λI]-1cfDe (θ?))	(22)
where σm2 in = min	σ(2x y) and whose non-zero probability α is guaranteed by the as-
(x,y)~D: σ(2x,y) >0	,y
sumption A1. Finally, in the last line, we use the cyclic property of the trace once again and move
e?
the expectation inside the trace, obtaining the covariance of function gradients cfD (θ?).
16
Published as a conference paper at ICLR 2022
Proceeding further, we again make use of Lemma 9 in the eq. 22, since these matrices are also
symmetric positive semi-definite. This yields,
T ≥σm2inαλmin CfDe(θ?) Tr	HSL(θ?) + λI-1
p1
=σmin α λmin (Cf (θ*)) X λi(HL(θ?)) + λ
(23)
(24)
(25)
where, the notation λi(∙) denotes the i-th eigenvalue of the corresponding matrix and We will use
the convention that for some matrix in Rm×m ,
λ1 ≥ ∙∙∙ ≥ λm .
Let us suppose r denotes the rank of the Hessian at the optimum, i.e., r := rank(HSL(θ0)). Since
the Hessian is positive semi-definite by the second-order necessary conditions of local minima, all
the eigenvalues are non-negative and we can further lower bound the previous expression to T as
follows:
r
T ≥σm2inαλmin CfDe(θ?)	X
i=1
1
λ (HL(θ?)) + λ
≥ σmin αλmin (CD (θ?)) λr (HL(In) + λ
where in the second line, we have used the fact that a sum of non-negative numbers can be lower
bounded by the maximum summand. The maximum here will correspond to using the inverse of the
minimum non-zero eigenvalue λr .
Now, substituting the following lower bound together with expression of population risk and taking
the limit of λ → 0, finishes the proof.
L(θ?) ≥ LeS (θ?) +
λr (HL(θ?))
(26)
□
1
n + 1
Note. For the purposes of this lower bound, we ignored the O 岛)part in Theorem 3. This is
because they take the form, e.g. for second-order influences as shown in Section A.6.1 and A.1.1.
17
Published as a conference paper at ICLR 2022
A.2.1 Analogous upper bound
We can in fact derive an upper bound to the population risk by following analogous steps to that in
the lower bound. First, let us recall the expression of the complexity term T ,
T =(χ	"Vθ'(fθ? (x), y)> [HL(θ*) + λl]-1Vθ'(fθ? (x), y)]	(27)
The existence of a non-zero residual via the σm2 in assumption A1 also implies that there exists an
analogous σm2 ax , defined as follows:
22
σmax =	max	σ(	)
max	(χ,y)~DYχ,y)>0 (x,y)
Thus, We get the following upper bound by also considering the chain rule of Vθ'(fθ? (x), y)=
Vθfθ? (x) Vf '(fθ*(x), y) and repeating similar steps as before,
T ≤ σmax αTr ([HL(Θ?) + λl]-1CD(θ?))	(28)
Then we can use Lemma 9 as both the matrices inside trace are symmetric positive semi-definite,
which gives us our initial upper bound:
T ≤ σmax α Tr ([HL(Θ?) + λl]-1) λmax (CD(θ?))	(29)
While additional steps can be further carried out, depending on the required context, but the objective
of this discussion is to show that many of the steps in our lower bound can be likewise generalized
to get a corresponding upper bound.
A.2.2 Empirical approximations of TIC like expressions
Consider the case when both the Hessian and covariance in the complexity term that shows up in
our lower-bound expression,
Tr (HSL(θ?) +λI]-1CfD(θ?))
are based/approximated on the training set.
Let us further assume the case of MSE loss and that we are at the optimum, where by A2, HSL (θ?) =
CfS (θ?). Under these set of assumptions, the term above reduces to,
Tr ((Cf(θ?) + λI)-1CS(θ?))
Since we can always express a positive semi-definite matrix as some ZZ>. Let us substitute this in
the above expression and consider the limit of regularization λ → 0.
Iim Tr ((ZZ> + λI)-1 ZZ>) = Tr(ZtZ)
(30)
where in the last line we have used the result from Tikhonov regularization, and Zt denotes the
pseudo-inverse of Z. But this expression is nothing but the rank(Z) and thus shows the ineffective-
ness of such an analysis where both the Hessian and the covariance of function gradients are based
on the training set.
18
Published as a conference paper at ICLR 2022
A.3 Double descent behaviour
Theorem 5.	For the MSE loss, under the setting of Theorem 4 and the assumptions A2, A3, A4,
the population risk takes the following form and diverges almost surely to ∞ at P = √ n,
?	?	σm2in αAλmin CfDe(θ0)
L(θ ) ≥ LS(θ ) + -----------------------------δ- , in the asymptotic regime of p,n → ∞ but
-	BkcD(θ0)k2 (√n-c√p)2
their ratio is a fixed constant, and where c > 0 is a constant that depends only on the sub-Gaussian
norm of columns of ZS(θ0).
Proof. Let us start from the lower bound shown in Theorem 4.
L(θ?) ≥ LeS(θ?) +
σm2 in α λmin	CfDe (
λr (HL(θ?))
(31)
1
n + 1
For the MSE loss, we have the Hessian HSL (θ?) = CfS(θ?), from assumption A2. We then use the
assumption A3 to bound the minimum non-zero eigenvalue (here, λr) of CfS (θ?) to the correspond-
ing minimum non-zero eigenvalue at initialization. This results in the following lower bound,
L(θ?) ≥ LeS(θ?) +
B λr CfS(θ0)
(32)
1
n + 1
Again we utilize the assumption A3 to upper bound the minimum non-zero eigenvalue of CfD (θ?)
to the corresponding minimum non-zero eigenvalue at initialization, thus obtaining:
L(θ?) ≥ LeS(θ?) +
B λr (CS(θ0))
(33)
1
n + 1
Now notice that, via assumption A4, CS(θ0)=由ZSZ> is a covariance matrix with the columns
of ZS containing independent, sub-gaussian random vectors in Rp .
Thus, we can leverage the results of Vershynin (2010) on the extremal eigenvalues of covariance
matrices. Specifically, given a random matrix, Z ∈ Rm×n whose columns are isotropic, indepen-
dent, sub-gaussian random vectors in Rm, Vershynin (2010) states that the extremal eigenvalues of
WZZ>, in the asymptotic regime where m,n → ∞ but their ratio m → Y ∈ (0,1], We have:
where c is a constant that depends on the subgaussian norm.
(34)
Since the above result holds in the isotropic case, let us first ensure this aspect. Consider the ma-
trix ZS := CD(θ0) 2 ZS, which is possible since CD(θ0) is clearly positive semi-definite and its
spectrum being bounded away from zero is a typical assumption, c.f. Du et al. (2019); Nguyen et al.
(2021). As a result, the columns of this new matrix Ze S are isotropic, besides being independent,
subgaussian random vectors. Thus, we apply the above RMT result to the matrix 啬 Z S Z >,andwe
then obtain the following relation on λr CfS(θ0) :
λr (CS(θ0)) ≤ kCD(θ0)k2 卜-Crn)	(35)
This is because, smin(AB) ≤ kAk2 smin(B), where s denotes the singular value, and follows
from using the definition of spectral norm together with min-max characterization of singular values
19
Published as a conference paper at ICLR 2022
(see Theorem 3.3.16 in Horn & Johnson (1991) for more). Also, we know that λi(M>M) =
λi(MM>) = si2 (M) for some matrix M ∈ Rm×n and for 1 ≤ i ≤ min(m, n). Therefore, using
this for A := CD(θ0)2, and B := CD(θ0)-1ZS gives the above bound.
Finally, combining all these together yields,
1
L(θ?) ≥ Ls⑹ + n+1
σmin aAλmin (CD(θ0))
B kCD(θ0)k2 (1 - cpp)2
(36)
Then, by a simple rearrangement and noting that we are in the asymptotic regime where n → ∞,
we recover our desired lower bound, thus finishing the proof.
□
Remark 1. In the over-parameterized case, where γ > 1, the similar procedure follows by using
the random matrix theory result on 由Z>Zs and the fact that in our lower bound We anyways have
minimum non-zero eigenvalue λr .
Remark 2. The ratio of the terms kCfD(θ0)k2 and λmin CfD(θ0) looks like a condition number,
but notice the the minimum eigenvalue is for the covariance over D and not D. Thus, if we are to
write in the form of condition number, the above result will take the form:
1
L(θ?)≥ LS(θ?) + n+1
σmin aAλmin (CD(θ0)) /λmin (CD(θ0))
BKWP )2
(37)
A.4 One-hidden layer neural network with trained output weights
In this section, we discuss the concrete case of a one-hidden layer neural network, but where only
the output layer weights v are trained (akin to a random feature model). Also, for simplicity we
assume that the input x ∈ Rd is sampled from a sub-Gaussian distribution (i.e. a distribution with a
tail decay).
Linear case. Let us begin with the case of a linear neural network, and then we can write the
network function as follows,
fθ(x) = v>Wx ,	W ∈ Rm×d .
Notice, since f = [Wx]-, the Second-derivatives ∣⅛ = 0, ∀i. Thus, Assumption A2 holds
∂ vi	i	∂ vi
trivially for all parameter configurations.
Next, note that the columns of ZS, consisting of the vectors Vθ fθ (x) = fVX), are sub-Gaussian
random vectors when conditioned on the initialization weights W, thus satisfying assumption A4.
Lastly, since the trainable parameters are only v and the matrix W remains fixed, then the covariance
of network Jacobians remains fixed as well during training, i.e., CS(θ0)= 这ZS(θ0)Zs(θ0)> =
啬ZS(Θ*)Zs(θ*)> = Cf (θ?). Hence, assumption A3 holds trivially with equality and both con-
stants equal to 1.
Non-linear case. Now, consider the general case where we have an elementwise non-linearity φ.
So, the network function can be expressed as:
fθ(x) = v>φ(Wx) ,	W ∈ Rm×d.
The functional Hessian is still zero, as the gradient of function with respect to the parameters does
not depend on the parameters. Similarly, the covariance of network Jacobian will remain the same
during training, as the matrix W is fixed. Thus both assumptions A2, A3 hold. The subgaussian
20
Published as a conference paper at ICLR 2022
assumption holds as well for a wide set of activation functions, including for instance the ReLU non-
linearity, φ(z) = max(0, z). Indeed, each component of the columns of ZS is still independent and
random. The difference relative to the linear case is that we squash to zero the part of the distribution
which is in the second quadrant, which gives a sub-Gaussian distribution as its tail is dominated by
a Gaussian. We note that if we assume the input x to be Gaussian, then we get the well-known
rectified Gaussian distribution (Harva & Kaban, 2007), which is itself SUb-GaUssian.
In summary, these two examples illustrate concrete scenarios of finite-width neural networks where
all the assumptions are clearly satisfied simultaneously.
A.5 Leave-one-out derivation
Influence calculation Let us recall the equation which the estimator θDe should satisfy:
dθ	dθ
-En[Vθ'(z,θ)] + (1-e) E [V2'(z,θ)]-r = -Ve'(zo,θ)-eV2'(zo,θ)丁	(38)
Z〜D	Z〜D	—e	-
Here, G := (1 - )F + δz0. For leave-one-out, we have that G = Dn\i-1 and F = Dn. Without
loss of generality, assume that the removed sampled index i = n and so zi = zn, and consider the
\n
shorthand D = Dn-1. Overall, then we are considering the contamination: D := (1 - e)Dn +eδZn,
with e = n--1ι. Let us substitute all of this back into the equation above:
∆θ
-E V'(z, Θd)] + (1-e) E V'(z, θ/)]-- = -Ve'(zn, θb)
Z 〜Dn	Z〜Dn	∆e
∆-
-E V'(z,-D)]+ E」V2'(z,-D)]--
Z 〜Dn	Z〜D	∆e
-eVθ'(Zn, bD )∆
(39)
(40)
一 .， ^ ,
-Ve'(Zn, θD)
_ ■一 ■， ^ ,,
-E V'(z,-D)] +
Z〜Dn
E	[V2'(z,bD)]δ-
Z〜Dn	∆e
一 ■， ^ ,
-Ve '(Zn, -D ) - e
Vθ '(Zn, bD )∆
^
+ e E [V2'(Z,-D)]χ^
Z〜Dn	∆e
∆-	∆-
-E [Vθ '(Z,-D)]+ E [V2'(z,-D )]ʌ- = -Vθ '(zn, -D) + e E-V '(z, -D)]--
Z 〜Dn	Z 〜Dn	∆e	Z 〜D	∆e
The last equation holds because notice that in the second term Z 〜-D (where D = (1-e)Dn+eδZn)
and not Z 〜Dn Since, both Dn and D are empirical distributions, We can compute the expectation
as a finite sum,
n 1	1 n-1	∆-
-E — Vθ '(Zi, -D)+ —T fVθ'(Zi,-D) K = -Vθ '(Zn, bD)	(41)
n	n - 1	∆e
i=1	i=1
We can split the term involving the gradient of the loss into two parts: one based on the n- 1 samples
(of which -D is also the parameter estimator) and the other based on the left-out sample with index
n.
n-1
—
n
n-1	1	1	1 n-1
E-T Ve '(Zi, -D)——Vθ '(Zn, -D)+	—T fVθ '(Zi,
n-1	n	n-1
i=1	i=1
______ - /
{^^^^^^^∙/^^^^^^^^^^^^^^^^
=0
∆-
雇=-Ve '(Zn, bD)
(42)
21
Published as a conference paper at ICLR 2022
The first term is zero since θd is the parameter estimator of the first n - 1 samples, and Will satisfy
the first-order stationarity conditions. Now, rearranging terms results in:
(n-1 X vθ'(Zi,bD)! ∆ = -一口'(zn, bD)
The term on the left is nothing but the Hessian computed over D or in other Words, the first n - 1
\n
samples. Let US denote it by HL (。力),where the \n m the superscript emphasizes that the HeSSian
is computed over samples excluding the n-th sample. Further, ∆e = e - 0 = e = - η⅛ι. Also,
∆θ = θd - Θd . Thus we obtain that the change in parameter estimator (i.e., influence on the
parameter estimator) is,
∆θ = n HLn (b D )tVθ '(zn, bD)	(43)
Linearizing the loss at the left-out sample. In order to look at the change in loss evaluated on
the n-th sample (left-out sample), we can consider linearizing the loss at θd .
∆'n = n Vθ '(Zn, bD )>HLn(bD )tVθ '(Zn,南 D )
But, ∆'n = '(zn, bD) - '(zn, bD). And, so we have:
1	>t
'(Zn, bD ) = '(Zn, bD ) + n Vθ '(zn bD ) HL (b D ) Vθ '(Zn, bD )
Now, we would like to average over the choice of the left-out sample n. However, we have to be
careful and remember that D = Dn_1_ depends on the particular left-out sample i. Thus, the full
expression we get is the following:
LOOS = LS GD ) + J X vΘ ' (Zi, bD∖-ι )>HLi (bD∖-ι )tV^' (Zi, bD∖-ι)
i=1
Let us recall the change in parameters with the leave-one-out contamination. From eq. 43, this was
∆θ = n HLI(bD )tVθ '(Zn, bD)
where, D := Dnn-I = (1 一 e)Dn + eδzn, with e = n-ɪ and assuming n is large enough.
Under this asymptotic n scenario, it is reasonable to estimate the Hessian H\Ln and Vθ' at the
\n
distribution Dn instead of D := Dn-1. We thereby get the following (where we also make the
dependence of ∆b explicit on the particular left-out sample index),
∆b(n) = bb
- bbS
(44)
22
Published as a conference paper at ICLR 2022
Now, we have a nice expression — in the sense that everything on the right-hand side is in terms
of the parameters bθS obtained by training
with the usual (empirical) distribution Dn . Since, we are
interested in the leave-one-out loss, let us analyze how the above change in the estimated parameters
(i.e., those obtained at convergence) brings about a change in the loss incurred on the n-th sample
itself ∆'(n). To this end, we consider a quadratic approximation of the n-th sample loss at bθS, as
shown below (this can alternatively be thought of :
'(Zn, bD∖nι )= '(Zn, bS) + Vθ'(Zn, bS)>∆θ⑺ + J∆θS)> V，'(Zn, bS)∆θ(n) + O(∣∣∆θ⑺ ∣∣3).
n	(45)
Next, we consider this procedure for every choice of the left-out-sample, and then average out to
get the expression of overall leave-one-out loss. We will also assume k∆θ(n) k3 and thus ignore the
third-order term.
1n
LOO = n X '(zi,θD∖-ι)
1n	1	>
=-£'(Zi, θs) + Vθ'(Zi, θs)>∆θ(i) + 2∆θ(i) V2'(zi, θs)∆θ(i)
n i=1
-n	- n >
=LGS) + - VVθ'(Zi, bs)>∆θ(i) +—V∆θ(i) Vθ'(Zi, bs)∆θ(i)	(46)
n U `--------{z------} 2n U V--------------------}
i=1	=Bi	i=1	=Ci
To go further, we need to plug in the change in parameters computed in eq. 44 above. However, we
first establish the validity of this general expression via the following result.
A.6 Leave-one-out formula proofs
Theorem 7. Consider the particular case of the ordinary least-squares, where '(Z, θ) :=
'((x, y), θ) = (y - θ>x)2. We assume that we are given a training set S = {(xi, yi)}in=1 of points
sampled i.i.d from the uniform empirical distribution Dn. Let the inputs be gathered into the data
matrix X ∈ Rn×d where d is the input dimension and the targets are collected in the vector y ∈ Rn.
Under the assumption that the number of samples n is large enough such that n ≈ n - -, we have
that the leave-one-out expression from eq. 46 (with parameter change given in eq. 44) is equal to
the widely-known closed-form formula of leave-one-out for least-squares LOOLS. Mathematically,
we have that,
2
LOO
-n
LOOLS = I X
n i=1
where, Aii = xi>(X>X)-1xi denotes the i-th diagonal entry of the matrix A = X(X>X)-1X>
(i.e., the so-called ‘hat-matrix’) and θ denotes the usual solution of θ = (X> X)-1 X> y obtained
via ordinary least-squares.
Proof. We will first separately analyze the terms in the summation corresponding to the first-order
and second-order parts in the eq. 46, which have been accorded the shorthand Bi and Ci respectively.
Also, to make expressions simpler, we will define the residual as ri := θ>xi - yi and denote the
matrix X>X by Σ.
Let us write down the individual loss gradients and Hessian in this case. We have
V'i := Vθ'((xi, yi), θ) = 2ri xi ,
23
Published as a conference paper at ICLR 2022
and, V2'i :=储'((xi,yi),θ) = 2 xix> .
The term Bi in summation corresponding to the first-order part can be computed as follows.
Bi := V'J∆θ(i) = V'J
^X V2'j	V'i = 2ri x>	2 ^XXjx>	2ri Xi
j=i	)	∖ j=i	)
=2r2 XJ (XTX - XiXJ) 1 Xi
2r2 xj (∑ - XiXJ) 1 Xi
2r2 XJ	∑-1 +
Σ-1 XiX>∑-1
1 — X j Σ-1Xi
Xi
2r2	Aii +
(Aii)2
1 - Aii
Essentially, we have used the Sherman-Morrison-Woodbury formula in the fourth line above and
rest is mere manipulation. Similarly, We compute the term Ci from the second-order part:
Ci := ∆θ(i)τ V%i∆θ⑻=V'iτ ( E V2'j ) V2'i ( E V2'j ) V'i
j=i	j=i
=2r2 Xτ (∑ - XiXJ) 1 XiXJ (∑ - XiXJ) 1 Xi
= 2r2 (xJ (ς - XiXJ)T Xi)
j"'Y
In the last line, we just reuse the computation that we already did in the previous part for
xj (∑ - XiXJ)	Xi. Finally, let us combine everything we have got with eq. 46
—1 3	1 J
LOO = LC) + n E Bi + 诟 E Ci
i=1	i=1
n n
=1 E r2 + 1 E 2r2 Qi—
n i n i 1 - Aii
i=1	i=1	ii
n E r2	1 +
i=1	∖
这 r11 +
i=1
l 1 E 2 ( Aii ʌ2
+ Wri lτ-Aii
i=1
1 - ii
Aii
2 1 - Aii +
Aii A2
1- Aii)
1E (⅛)
2
Thus, remembering our shorthand r := 9JXi - yi, we conclude our proof.
□
24
Published as a conference paper at ICLR 2022
Corollary 8. For any finite-width neural network, the first and second-order influence
function give similar formulas for LOO like that in Theorem 7, but with A =
ZS(θ*)> (Zs(θ?)Zs(θ?)>)-1 ZS(θ?), where ZS(θ?) := [Vefθ*(xι),…，Vefθ*(xn)] and θ?
are the parameters at convergence for MSE loss.
Proof. Simply replace xi to Vθfθ(xi) in the proof of Theorem 7 and repeat the procedure, since all
the steps hold under the assumption of MSE loss and considering the Hessian HSL (θ?) = HoS (θ?)
provided for by the assumption A2.
□
A.6.1 Over-parameterized case
In our above discussion, we considered that Σ = X>X was invertible (and likewise in the corollary
We considered ZS(θ?)Zs(θ？)> Was invertible). However, in the over-parameterized case this many
not necessarily be the case. Nevertheless, there is one simple fix to this issue, as often carried out in
the literature. We consider Σb
Σ+λI in place of Σ for λ > 0, and in regards to influence functions,
this Would basically amount to having an `2 regularization in the loss function. As a result, We can
exactly repeat the same steps in the proof of the Theorem 7, except With Σ instead. Eventually, We
recover the same formulas but noW the expression of the resulting matrix A is slightly different as
mentioned beloW:
A = X (X>X + λl)-1 X>
But, We can further use the Well-knoWn push-through identity and obtain A =
XX> + λI -1 XX>, Where We consider the shorthand K := XX> to indicate the kernel or
the gram matrix. ReWriting this gives, A = (K + λI)-1 K Which is the familiar expression as seen
in regularized kernel regression. For the case of Corollary 8, the same extension can be carried out,
except that noW the matrix K Will be the empirical Neural Tangent Kernel (NTK) (Jacot et al., 2018)
matrix at the optimum θ? .
25
Published as a conference paper at ICLR 2022
B Influence Functions: a primer
In this primer on influence functions, we closely follow the textbook of Hampel et al. (1986). The
key objective of influence function is to investigate the infinitesimal behaviour of functionals, such
as T(Dn). In particular, this is defined when the change in underlying distribution can be expressed
in the form of a Dirac distribution. Formally, this is defined as follows:
Definition 10. The influence function IF of the estimator T, at some distribution F, evaluated at a
point z (where such a limit exists) is given by,
IF(z; T,F ) = lim T ((IY)F + "z)-T (F)
Essentially, the influence function (IF) involves a directional derivative ofT at F along the direction
of δz. Further, in order to interpret the above definition, substitute F by Dn-ι and put e = ɪ. This
implies that (IF) measures n times the change of statistic T due to an additional observation z . In
other words, it describes the (standardized) effect of an infinitesimal contamination on the estimate
T. E.g., in the case of parameter estimator, the change in estimated parameters due to presence ofan
additional datapoint. By now, the astute reader can already see its natural application for leave-one-
out error, but we ask for some patience so as to discuss some other important aspects of influence
functions.
More generally, one can view influence functions from the perspective of a Taylor series expansion
(to be precise, the first-order von Mises expansion) of T at F, evaluated on some distribution G
“close” to F,
T(G) = T(F) +	IF(z; T, F)d(G - F)(z) + remainder terms .	(47)
So, as evident from this, it is also possible to consider higher-order influence functions and use
them in a combined manner, as considered in Debruyne et al. (2008). However, since the first-order
term is often the dominating term — as well as to ensure tractability when we later consider neural
networks — we will restrict our attention to only first-order influence functions hereafter.
B.1	Properties of influence functions
(i)	Zero expectation. The expectation of influence function over the same distribution is zero,
i.e., IF(z; T, F)dF (z) = 0. This should actually be quite intuitive as we are basically averaging
out all possible deviations of the estimator. But, the formal reasoning is that influence function is
essentially akin to GateaUx derivative at the distribution F,
lim T ((I-e)F + eG)- T(F) = Z IF(z)dG(z).
Now, replace G by F in the above equation and the stated property follows.
(ii)	Variance of IF provides asymptotic variance of corresponding estimator. When the obser-
vations zi are sampled i.i.d. according to F, then the empirical distribution Dn converges to F, for
n sufficiently large, by Glivenko-Cantelli theorem. So, replacing G by Dn in eq. 47 and using the
first property, we get:
Tn(Dn) ≈ T(F) + IF(z; T, F)dDn(z) + remainder terms .
Integrating over the empirical distribution Dn , we obtain:
1 n
√n(Tn(Dn) - T(F)) ≈ —= EIF(zi； T, F) + remainder terms.
i=1
The term on the right-hand side involving IF is asymptotically normal by (multi-variate) Central
Limit Theorem. Further, the remainder terms can often be neglected forn → ∞, and thereby the
26
Published as a conference paper at ICLR 2022
estimator Tn is also asymptotically normal. Thus, √n(Tn(Dn) - T(F)) → NP(0, V(T, F)), with
the asymptotic variance (more accurately, covariance matrix) V(T, F) as follows:
V(T, F) =
IF(z;T,F)IF(z;T,F)>dF(z) .
For the one-dimensional case, i.e., T(F) ∈ R, we simply get V(T, F) = R IF(z; T, F)2 dF(z).
∕∙∙∙∖ d	. ∙	^Λ∕ T-l∖ FF	,1
(iii)	Chain rule for influence functions. Suppose our estimator θ(F) depends on some other
'低
^
estimators, i.e., θ(F) := T θ1 (F), ∙ ∙
^ . .
bθ(F) as,
∙ , bk (F)), then We can expression the influence function for
k ∂T
IF(z;θ,F)=三 就 IF(z;θj,F).
(48)
Remark. We refer the mathematically-oriented reader to Huber (2004) for details on the regularity
conditions needed to ensure the existence of IF and the like.
27
Published as a conference paper at ICLR 2022
C Empirical results and details
C.1 Empirical details
We train all the networks via SGD with learning rate 0.5 and learning rate decay by a factor of 0.75
after each quarter of the target number of epochs.
For the experiments based on MNIST1D (Greydanus, 2020), the input dimension size is d = 40
while number of classes is K = 10.
Double-descent model sizes (a) For the 3-layer double descent plot, we consider hidden widths
m1, m2 ∈ {10, 20, 30, 40, 50} and choose all those pairs as experiments where |m1 - m2| ≤ 20.
(b)	For the 2-layer plot in MSE, we take the hidden layer widths from [1, 151] at an interval of 10.
(c)	For the 2-layer plot corresponding to CE, we take hidden layer sizes from [2, 32] at intervals
of 2 and then for reducing the computation load when the network sizes increase, we take coarser
intervals with a gap 5 and for even bigger sizes, at a gap of later 10. But this is purely to reduce
computational load that comes with Hessian computation.
In all the double descent curves, we ensure that near the interpolation threshold all models are at
interpolation, in the sense that the training accuracy is 100%.
C.2 Verification of assumption A3
⑥ eUS 6o 一二MHF"an-eAua6∙≡5OJaZ,Uou E3EΞ~Σ
IOOO	2000	3000	4000	5000
# of parameters
(a) 2-hidden layer case, MNIST1D.
(3-eυs 6o-)(r)Tan-e>u36 ⑥ 9az,uou EnE-C-S
(b) 1-hidden layer, CIFAR 1 0.
Figure 4: Additional results on verifying the assumption A3
28
Published as a conference paper at ICLR 2022
C.3 Cross Entropy double descent
■*— Population loss interpolation threshold —(best) Test error - Tram error
SSo-I uos-ndt⅛
parameters
1000
β(n
SSo-I UqIe一 ndod
80
70
60
,
j
parameters


(a) Final Test Error	(b) Best Test Error
Figure 5: Both test error at the final epoch as well as the ‘best’ test error in terms of the one selected
based on a small validation set.
C.4 MSE double descent
For the sake of visualization in the 3-layer case, we smooth the quantities involved by considering a
moving average of them over three successive model sizes.
2.05
2.00
1.95
g
U 1.90
o
f 185
1.80
1.75
1.70
10∞	20∞	30∞	4000	5∞0
# of parameters
-*- Population loss -*- Tninloss -∙- Minimum non-zeroeigenvalueMHg)
(b) L=3, Unsmoothed version
-14
IoOo	2000	3000	4000	5<XX)
# Of parameters
φ3s 8二<AOHyv
(a) L=3, Smoothed version
→- Population loss -∙- Minimum non-zeroeigenvalueΛr(>⅞)
IoOo 280	3000	4000	5000	6000	7000
#of parameters
(c) L=2, Smoothed version
φ-3s 8D<v>oHry
-⅛- Population loss -*- Tninloss →- Minimum non-zeroeigenvalueM>⅞)
0.0
0 IoOo 280	3000	4000	5<XX)	6000	70∞
# of parameters
(d) L=2, Unsmoothed version

Figure 6: We see that the same trend holds in both the cases — In fact, the unsmoothed version
perhaps even more starkly shows the minimum non zero eigenvalue vanishes at the maximum pop-
ulation loss.
Also, in the unsmoothed version of the above plots, we also show the corresponding train loss. It
must be emphasized that near interpolation the training accuracy is 100%, even though there might
be some non-zero training loss.
29
Published as a conference paper at ICLR 2022
C.5 Results on additional datasets
-*- Population loss -∙- Minimum non-zeroelgenvaIue⅛(l⅞)
3.Z
sso-JUOR<5ndod
10∞	280	3000	4000	50∞	6<XX)
# of parameters
(a) L=2, CIFAR 1 0
-⅛6s 固二军y
1.31.2
SSOl UOAe-ndod
-2
(b) L=2, MNIST
Figure 7: Population risk behaviour alongside the minimum non-zero eigenvalue of the Hessian at
the optimum for CIFAR 1 0 and MNIST with MSE loss.
-⅛6s 固二<AOHK
Empirical details. The optimization details are the same as mentioned before in the setting of
MNIST1D, except that we train longer for 20K epochs. We downsample the input dimension
for CIFAR 1 0 and MNIST to about the same size so as to ensure consistency, and in particular,
d = 48 for CIFAR 1 0 (by downsampling 32 × 32 × 3 images to 4 × 4 × 3 and then flatten-
ing) while d = 49 for MNIST (by downsampling 28 × 28 images to 7 × 7 and then flattening).
We find that on harder datasets like these, it takes even longer to drive the networks to interpola-
tion and thus consider n = 40 samples. We consider the sizes of the hidden layer from the set
{1, 3, 5, 7, 9, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111}. In other words, we sample at a finer gran-
ularity near the interpolation threshold (≈ 400), while increase the gaps later on. Also, we find that
networks with hidden layer width m = 1 fails to train and gives NANs, so we exclude its result.
Overall, we find that the population risk and the minimum non-zero eigenvalue of the Hessian at the
optimum show a very similar trend like in the case of MNIST1D. As a result, this implies that our
empirical findings also generalize to other datasets as well.
C.6	2-hidden layer results for large difference in successive layer sizes
Population loss —Minimum πoπ-zero eigenvalue
1.60
5 0 5 0 5
-8-87 7-6
T-T-T-T-T-
SS2 uoqe-ndod
2000
3000	4000	5000	6000	7000
# of parameters
(a) L=3, MNIST1D
Figure 8: Population risk behaviour alongside the minimum non-zero eigenvalue of the Hessian
at the optimum for a 2-hidden layer network on MNIST1D, but where the successive hidden layer
sizes m1, m2 can have much bigger differences — i.e., |m1 - m2| ≤ 120 with MSE loss.
⑥ 5s 60_:号 E
5 6 7
- - -
30
Published as a conference paper at ICLR 2022
Previously, in our double descent experiments for 2-hidden layers, we considered that hidden widths
m1, m2 ∈ {10, 20, 30, 40, 50} and chose all those pairs where |m1 - m2| ≤ 20. Now, a question
might arise what happens in the case when the successive hidden layer sizes are very ‘imbalanced’
(or non-uniform), i.e., they have a big difference between their sizes. The Figure 8 shows the
setting of double descent with 2 hidden layers with sizes m1 and m2 , which are chosen as per m1 ∈
{20, 40} and m2 ∈ {20, 40, 60, 80, 100, 120, 140}. Rest of the empirical details regarding training
and dataset are identical to the MNIST1D setting considered earlier. We find the population risk and
minimum non-zero eigenvalue behaviour which is very similar to Figure 1b and in accordance with
our theoretical predictions. This also confirms that our theoretical analysis generalizes to diverse
architecture patterns, not just the ‘balanced’ setting in which double descent was has been considered
before, which leads to the following remark.
Closing remark. Before we finish this discussion, let us emphasize that almost all prior works on
double descent Nakkiran et al. (2019); Nakkiran (2019) consider the case of ’balanced’ hidden layer
sizes. In other words, these works consider a fixed architectural pattern, say {m} → {2m} → {m}
and then increase the common width multiplier m. In this respect, we are one of the first works that
not just demonstrates the occurrence of double descent in the imbalanced hidden-layer settings —
but also explains the behaviour via the trend of the minimum non-zero eigenvalue of the Hessian at
the optimum.
C.7 Nature of constants for the eigenvalue assumption
In the assumption A3, we assume the existence of constants 0 < Aθ0 < Bθ0 < ∞ such that the
following holds:
A λmin(CfS (θ0)) ≤ λmin(CfS(θ?)) ≤ Bλmin(CfS(θ0))
So in this section, how these constants actually behave in practice and if they are O(1) across varying
network sizes? Or, equivalently, how the ratio λmin(Cs(θo)) behaves and if it is O⑴？
25
Ratio ofΛm∕n(C≡(θ*)) toλm∕n(C∣(θ°))
20
15
10
一
0
((Uw
α*θ)vlFW
-5
0	1000	2000	3000	4000	5000	6000	7000
# of parameters
Figure 9: The ratio
λmin(CS (θ?))
λmin(CS (θ0))
We consider the setting of one-hidden layer neural networks trained with MSE loss for 5K epochs
as discussed in the main text (for other training details, see Appendix C.1). To remind, we consider
networks of hidden-layer sizes sampled uniformly from [1, 151] at an interval of 10. The Figure 9
plots this desired ratio. We find that indeed this ratio is O(1). More precisely, the Table 1 details the
summary statistics of this ratio.
Overall, the trend in Figure 9 and the precise summary statistics in the above Table 1 even implies the
existence of universal constants for Aθ0 and Bθ0 (e.g., one setting would be to 0.3 and 12 respectively).
This ratio naturally depends on the problem, so it is likely that it will change slightly (e.g., on
CIFAR10, the mean changes to 7.812) but we are able to confirm the existence of universal constants
31
Published as a conference paper at ICLR 2022
Minimum Median Mean Maximum
0.336	2.019	3.754	11.955
Table 1: Summary statistics of the ratio ¥(：｛(%)).
λmin(Cf (θ ))
across all our experiments. Lastly, there is also a theoretical basis to it, as remarked in the main text,
since the map A 7→ λi(A) is Lipschitz-continuous on the space of Hermitian matrices, which
follows from Weyl’s inequality (Tao, 2012, p. 56).
Final remarks. Importantly, we would thus like to reiterate that we never impose constraints on
the minimum eigenvalue of the covariance of the function Jacbians. But, rather inspired by the above
empirical observation, we consider the existence of such constants.
32
Published as a conference paper at ICLR 2022
C.8 Constituent of Hessian at the optimum
For our analysis, we made the assumption A2 based on the prior works of (Sagun et al., 2017; Singh
et al., 2021). We also find in our own experiments that the very same observation made in these
papers holds — namely, that the Hessian HL at the optimum is only composed of the outer-product
Hessian Ho, while the functional Hessian Hf = 0.
To show this, we plot in Figure 10 the nuclear norm, spectral norms, as well as the minimum non-
zero eigenvalue of the the overall loss Hessian HL and the outer-product Hessian Ho for one-hidden
layer networks trained for 20K epochs and with rest of the training details identical to all other
experiments.
(a) Nuclear norm
O	500	10∞	1500	2000	258	3000
# Ofparameters
(b) Spectral norm
500	l∞0	1500	2000	2 500	3000
# of parameters
si3s 60-g-eΛU36ssn-∞qe ∈3∈Ξ-Z
O	500 IOOO 158	2000	2500	3000
# of parameters
(c) Minimum non-zero eigenvalue
Figure 10: Comparison of the spectra of HL and Ho at the optimum θ? (across the set of networks
trained for double descent). We find that across all the above-plotted measures the curves for HL
and Ho coincide. Thus showing that the functional Hessian Hf goes to zero at the optimum and
thereby establishing the merit of the assumption A2.
C.9 Lowest few eigenvalues are the dominating factor behind divergence
NEAR INTERPOLATION THRESHOLD
We consider the settings of CIFAR10, MNIST, and MNIST1D, all of which correspond to the dou-
ble descent shown in Figures 7a, 7b, and 1 respectively. To demonstrate how many of the lowest
eigenvalues capture the double descent trend, for each model in all of the above settings, we plot
the % of the trace (of the Hessian inverse) captured by the lowest eigenvalues of the Hessian, since
the lower bounds exhibit this dependence. In order to ensure consistent comparisons across varying
model sizes, we consider the lowest eigenvalues in 0.5%, 1%, 2%, 5%, 10% of the total number of
eigenvalues of that model. The results can be found in the following plots:
Observations. We see that across all these cases just 0.5% of the lowest eigenvalues are enough to
capture the double descent behaviour — capturing a minimum of 60% of the trace near interpolation
across all these settings.
33
Published as a conference paper at ICLR 2022
---Smallest 0.5% Smallest 2%	——Smallest 10%
Smallest 1%	——Smallest 5%
Oooooo
0 8 6 4 2
s3nm>u36-3⅛J3=eES QE Aq PaJnadeU α>SJα>AU- Ue-SSSH *
1000	2000	3000	4000	5000	6000
# of parameters
(a) L=2,CIFAR10
--Smallest 0.5% Smallest 2%   Smallest 10%
Smallest 1%	——Smallest 5%
Oooooo
0 8 6 4 2
SSn-e>UQ6-3 4ss=eES QE Aq PaJnadeUa>u£ QSJQAU-Ue-SSQH S
1000	2000	3000	4000	5000	6000
# of parameters
(b) L=2, MNIST
---Smallest 0.5% Smallest 2%	-- Smallest 10%
Smallest 1%	——Smallest 5%
Oooooo
0 8 6 4 2
s3n-e>UQ6-3⅛3=eES QE Aq PaJnadeU QUg QSJSU-Ue-SSaH *
1000	2000	3000	4000	5000	6000	7000
# of parameters
(c) L=2, MNIST1D
Figure 11: The % of the Hessian inverse trace captured by varying proportions of smallest eigen-
values of the Hessian across multiple datasets for MSE loss.
Further > 98% of the trace is captured as soon as we have 1% of the lowest eigenvalues for CI-
FAR10, 0.5% for MNIST, and 5% for MNIST1D. This clearly shows that the double descent be-
haviour is indeed captured by a very small handful of the lowest eigenvalues. (As a matter of fact,
near the interpolation threshold, even just using the minimum non-zero eigenvalue alone captures
85.56% of the trace for CIFAR10 and 97.80% for MNIST.)
34
Published as a conference paper at ICLR 2022
C.10 Lower bound computation
At an initial glance, it might seem that the computed lower bounds could be very small in magnitude
to be of use — as the bound is scaled by σm2in.
However, while practically computing one can consider a tighter lower bound, by using a particular
tolerance τ .
Let us illustrate by considering evaluating this bound on a test set S0 (just like we actually do). This
is because in the lower bound we have the expression,
LHS = TSr	X	[Tr(A(X,y)) ∙kvf'(fθ*(χ),y)k2]
(x,y)∈S0
Now, since Tr A(x,y) is non-negative, if we are given a tolerance τ, we filter out those samples
with l∣Vf'(fθ? (x), y)k2 < T
LHS ≥∣S∣	X	[Tr(A(χ,y)) ∙ τ ∙ l{∣∣Vf '(fθ*(x), y)∣2 ≥ T}]
(x,y)∈S0
where, 1 is the indicator function. This helps Us compute lower bounds which can be evaluated in
practice.
ω-IDus 60-) sso—l
—⅛- Trace based Lower bound λmιπ based Lower bound
-12
1000	1500	2000	2500	3000
# of parameters
-10
orous 6O_) PUnOq」3M0_l
500
Figure 12: Comparison of lower bounds with trace vs minimum eigenvalue
35
Published as a conference paper at ICLR 2022
C.11 Empirical observations from linear regression
C.11.1 Hessian statistics on training set
2.5
2.0
1.5
1.0
0.5
0.0
Linear regression with #samples N=100, inρut-ranlc tl/1
----8∏dBjIUmber
25	50	75 IOO 125	150	175	200
#of features
O 25	50	75 ICO 125	150	175	200
#of features
(a) Condition number
E」OU -e-pəds
3X10"
2 X10»
4 X IO0
(b) (log scale) Condition number
Linear regression with #SamPleS N=100, inpι∣t-ranlc d/1
(c) Max absolute eigenvalue
(d) (log scale) Max absolute eigenvalue
(e) Min absolute eigenvalue
(g) Nuclear norm (or Trace)
Figure 13: Hessian statistics computed over the training set. We observe that condition number
diverges at the double descent peak, owing to minimum absolute eigenvalue becoming close to zero
at this threshold.
(f) (log scale) Min absolute eigenvalue
Linear regression with #SamPleS N=100, inpι∣t-ranlc d/1
O 25	50	75 IOO 125	150	175	200
#of features
(h) (log scale) Nuclear norm (or Trace)
36
Published as a conference paper at ICLR 2022
C.11.2 Hessian statistics on test set
Io1
3qEnc.Σ8
10«
0	25	50	75	100	125	150	175	200
#of features
(a)	Condition number
Linear regression with /samples N=100, inpι∣t-rank: d/1
---SPeCtraLgrE
12	^^
(b)	(log scale) Condition number
UUoUle:pəds
UUoUepəds
0	25	50	75 ICO 125	150	175	200
# of features
0	25	50	75	100	125	150	175	200
# of features
(c) Max absolute eigenvalue
(d) (log scale) Max absolute eigenvalue
Linear regression with esamples N=100, inpot_ranlc: d∣l
、	---mlπ-∙bs-el∣
4X10°	∖
3X10°	ʌv
0.5
0	25	50	75 ICO 125	150	175	200
#of features
6×10^1
0	25	50	75	100	125	150	175	200
# of features
(e) Min absolute eigenvalue
(f) (log scale) Min absolute eigenvalue
Figure 14:	Hessian statistics computed over the test set (size 500).
37
Published as a conference paper at ICLR 2022
C.11.3 Drawing DD plots at arbitrary thresholds
(a) n = 100, d? = 100, β = 0	(b) n = 100, d? = 200, β = 1
(c)n = 100, d? = 300, β = 2	(d)n = 100, d? = 400, β = 3
Figure 15:	Changing the position of the interpolation threshold by adjusting the rank of the under-
lying input-covariance matrix.
The input design matrix is constructed in such a way so that given some d0 many input features,
the number of base features is d. But we additionally have β d many redundant features (composed
of linear combinations of the base features). Also, as the number of features is increased in the
above test loss curve, these redundant features follow in succession to the base features. Therefore,
at any point in these graphs along the x-axis for some number of features d0, the effective number
of features are --------
(β + 1)
Hence, the peak occurs when the effective number of features equals the
(effective) number of samples, i.e., at
d?
β + 1
n, which is nothing but d? = n(β + 1). Thereby,
looking from the perspective of rank (the Hessian here is simply the covariance matrix), the position
of the interpolation threshold can be reconciled.
38