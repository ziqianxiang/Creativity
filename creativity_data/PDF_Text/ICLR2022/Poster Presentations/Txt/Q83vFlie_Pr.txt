Published as a conference paper at ICLR 2022
Bandit Learning with Joint Effect of Incen-
tivized Sampling, Delayed Sampling Feedback,
and Self-Reinforcing User Preferences
Tianchen Zhou1, Jia Liu1, Chaosheng Dong2, Yi Sun2
1Department of Electrical and Computer Engineering, The Ohio State University
2Amazon, Seattle, Washington, USA
zhou.2220@osu.edu, liu@ece.osu.edu, {chaosd, yisun}@amazon.com
Ab stract
In this paper, we consider a new multi-armed bandit (MAB) framework motivated
by three common complications in online recommender systems in practice: (i)
the platform (learning agent) cannot sample an intended product directly and has
to incentivize customers to select this product (e.g., promotions and coupons); (ii)
customer feedbacks are often received later than their selection times; and (iii)
customer preferences among products are influenced and reinforced by historical
feedbacks. From the platform’s perspective, the goal of the MAB framework is to
maximize total reward without incurring excessive incentive costs. A major chal-
lenge of this MAB framework is that the loss of information caused by feedback
delay complicates both user preference evolution and arm incentivizing decisions,
both of which are already highly non-trivial even by themselves. Toward this end,
we first propose a policy called “UCB-Filtering-with-Delayed-Feedback” (UCB-
FDF) policy for this new MAB framework. In our analysis, we consider delayed
feedbacks that can have either arm-independent or arm-dependent distributions.
In both cases, we allow unbounded support for the random delays, i.e., the ran-
dom delay can be infinite. We show that the delay impacts in both cases can still
be upper bounded by an additive penalty on both the regret and total incentive
costs. This further implies that logarithmic regret and incentive cost growth rates
are achievable under this new MAB framework. Experimental results corroborate
our theoretical analysis on both regret and incentive costs.
1	Introduction
In recent years, the multi-armed bandit (MAB) framework has received a significant amount of
interest in the learning research community. This is partly due to the fact that, in many online e-
commerce recommender systems (e.g., Amazon and Walmart), the problem of online learning of the
optimal products while making profits at the same time can be well formulated by an MAB problem.
However, although many MAB algorithms have been proposed in this area, it is worth noting that
most of the existing MAB models in the literature have not considered the joint effect of three
common phenomena in e-commerce recommender systems: (i) In many e-commerce recommender
systems, the platform (the learning agent) cannot sample an intended product (an intended arm)
directly and has to incentivize customers (e.g., through promotions and coupons) to sample the
product and receive the sampling feedback from the customers indirectly (e.g., ratings and reviews);
(ii) Customer feedbacks are often received much later than their purchasing times (e.g., a review
may or may not be submitted by a customer even months later after purchasing a product); and (iii)
Customer preferences among products are influenced and reinforced by historical feedbacks, which
may even lead to various viral effects over some products (the more good reviews one product has
received, the more likely that the next arriving customer will prefer this product). The lack of a
fundamental understanding and joint studies of these three important factors in MAB policy designs
motivates us to fill this gap in this paper.
Toward this end, we propose a new MAB framework that jointly considers i) incentivized sampling,
ii) delayed sampling feedback, and iii) self-reinforcing user preferences in online recommender sys-
1
Published as a conference paper at ICLR 2022
tems. However, we note that the MAB policy design for the proposed new MAB framework is highly
non-trivial due to the complex couplings between the aforementioned three factors. First, similar to
conventional MAB problems, there exists a dilemma between sufficient exploration through sam-
pling to learn an optimal arm (i.e., an optimal product), which may incur numerous pullings of
sub-optimal arms, and the greedy exploitation to play the arm that has performed well thus far to
earn profits. Second, there is another dilemma to the learning agent between offering sufficiently
attractive incentives to mitigate biases (due to lack of initial data and self-reinforcing user prefer-
ences) and avoid spending unnecessarily high incentives that hurt the learning agent’s profits. Last
but not least, the delayed sampling feedbacks may render the estimation of arms’ quality during
the MAB process highly inaccurate, introducing yet another layer of uncertainty to the MAB on-
line learning problem, which is already plagued by complications from incentivized sampling and
self-reinforcing user preferences. As in most MAB problems, we adopt “regret” as our performance
metric in this paper, which is defined as the cumulative reward gap between the proposed policy and
an optimal policy design in hindsight. Under the regret setting, the complications due to these three
key factors naturally prompt the following fundamental questions:
(1)	How should the agent design an incentivizing strategy to strike a good balance between explo-
ration and exploitation to achieve sublinear (hopefully logarithmic) regrets?
(2)	To avoid offering exceedingly high incentives, how should the agent incentivize in order to
attract a user crowd that prefer an optimal arm, so that the users’ self-reinforcing preference
could automatically gravitate toward this optimal arm without further incentives?
(3)	Under various delayed feedback situations in the new MAB framework (e.g., unbounded ran-
dom delays, heavy-tailed delay distributions, and arm-dependent delays), could we still achieve
low regrets with low incentive costs?
In this paper, we answer the above fundamental questions affirmatively by proposing a new
“Delayed-UCB-Filtering” policy for the MAB framework that jointly considers incentivizing sam-
pling, delayed sampling feedback, and self-reinforcing user preferences. We show that our proposed
policy achieves O(log T) regret with O(log T) incentive payments. The success of our policy design
hinges upon two key insights: (i) the self-reinforcing user preference effect is actually a “blessing in
disguise” and can be leveraged to establish an important “dominance” condition (more on this later)
that further implies O(log T) regret and incentive costs; and (ii) the impacts of delayed feedback on
regret and incentive costs can be upper bounded under appropriate statistical settings to preserve the
“dominance” condition. Our key contributions and main results are summarized as follows:
•	We propose a new MAB model that jointly considers incentivized arm sampling, delayed sam-
pling feedback, and self-reinforcing user preferences, all of which are important features of online
recommender systems. To develop efficient and low-cost incentivized policy for this new MAB
model, we propose a three-phase “UCB-Filtering-with-Delayed-Feedback” (UCB-FDF) policy,
which contains an incentivized exploration phase, an incentivized exploitation phase, and a self-
sustaining phase. In our UCB-FDF policy, the first two phases judiciously integrate delayed
feedback information, while in third phase, the system solely relies on self-reinforcing user pref-
erences to converge to the pulling of the optimal arm.
•	We first show a fundamental fact that, under our UCB-FDF policy, delayed sampling feedback
only has an additive penalty on the regret and incentive cost performances, and that this additive
penalty grows logarithmically with respect to time. Specifically, we first investigate the delayed
feedback impact under the assumption that the feedback delay is an i.i.d. random variable across
samplings with a finite expectation. We show that the UCB-FDF policy achieves logarithmic
growth rates of regret and incentive costs under this setting. Then, we relax the i.i.d. feedback
delay assumption to allow the feedback delay distribution to be arm-dependent. Under this setting,
we also show that similar logarithmic growth rates of regret and incentive can still be achieved.
•	We conduct extensive experiments on Amazon Review Data 1 to demonstrate and verify the per-
formance of our UCB-FDF policy as well as the impacts of delayed feedback on real-world sce-
narios. We also verify our theoretical analysis through various product categories and demonstrate
the efficacy of our proposed UCB-FDF MAB policy.
The rest of the paper is organized as follows. In Section 2, we review the literature to put our work
in comparative perspectives. In Section 3, we formulate our new MAB model that captures the three
common phenomena. In Section 4, we present our UCB-FDF policy and analyze its performance.
1https://nijianmo.github.io/amazon/
2
Published as a conference paper at ICLR 2022
Then, we present our experiment settings and results in Section 5. Due to space limitations, the
proofs and part of experiemntal results are relegated to the appendix.
2 Related Work
In this section, we provide a quick overview on three lines of research related to our work: i) bandits
with delayed feedback, ii) bandits with random preferences, and iii) incentivized bandits.
1)	Bandits with Delayed Feedback: Motivated by practical issues in the clinical trials, Eick (1988)
was the first to introduce a two-armed bandit model with delayed responses, where the patients’
survival time reports after the treatment are delayed. Recently, Joulani et al. (2013) provided a sys-
tematic study and showed that for delay T with a finite expectation, the worst case regret scales with
O(√KT log T+KE[τ]), where K is the number of arms. Meanwhile, Vernade et al. (2017) showed
that stochastic MAB problems with delayed feedback have a regret lower bound O(KlogT). How-
ever, this work assumed that the distribution of the random delay is arm-independent. In contrast,
Joulani et al. (2013) considered arm-dependent delay distributions that have an upper bound of the
maximum random delay. More recently, Manegueu et al. (2020) considered arm-dependent and
heavy-tailed delay distributions, where only an upper bound on the tail of the delay distribution is
needed, without requiring the expectation to be finite. Also, Lancewicki et al. (2021) studied the
case where the delay distribution is reward-dependent, which implies that the random delay in each
round may also depend on the reward received on the same round. However, most of these works
on delayed bandits are based on the standard stochastic MAB framework. In contrast, we consider
delayed feedback in incentivized bandit learning with self-reinforcing user preferences, which is a
more appropriate model for real-world recommender systems than the standard stochastic MAB.
2)	Bandits with Random Preferences: The impacts of random user preferences in e-commerce
platforms have received increasing interest in several different areas in learning and economics. Ex-
isting works in (Agrawal et al., 2017; 2019) formulated the user preference variation given different
product bundles by the multi-nomial logit model on top of the bandit learning framework and pro-
posed a Thompson Sampling approach that achieves a worst-case regret bound of O(√NΓ log TK),
where N is the size of recommended arm bundle. With a different focus on preference modeling,
BarabaSi & Albert (1999); Chakrabarti et al. (2006); Ratkiewicz et al. (2010) investigated the net-
work evolution with “preferential attachment” that formulates the social behavior known as self-
reinforcing preferences, among which the works in (Shah et al., 2018; Zhou et al., 2021) are the
closet to our work. To our knowledge, Shah et al. (2018) was the first to consider self-reinforcing
user preferences in bandit learning problems. Later, Zhou et al. (2021) incorporated self-reinforcing
user preferences into the incentivized bandit learning framework. The key difference between these
two works is that, in the model in (Shah et al., 2018), only one arm is revealed to users in each round,
while in the model in (Zhou et al., 2021), all arms are revealed to users and users’ arm selections are
influenced by incentives. However, both of these works fall short in modeling online recommender
systems in practice as they assume that an arm-sampling feedback is observable in the same time-
slot when an arm is pulled. However, for most e-commerce recommender systems in practice, user
feedbacks are often not immediately observable. As a result, the decision on which arm to pull next
has to be made without some of the feedbacks from arm-pulling actions in the past.
3)	Incentivized Bandits: To our knowledge, Frazier et al. (2014) was among the first to adopt
incentive schemes into a Bayesian MAB setting. In their model, the agent seeks to maximize time-
discounted total reward by incentivizing arm selections. Later, Mansour et al. (2015) studied the
non-discounted reward setting. For the non-Bayesian setting, Wang & Huang (2018); Zhou et al.
(2021) proposed policies that maximize the total non-discounted reward with bounded incentive
costs. Bandits with budget (Guha & Munagala, 2007; Goel et al., 2009) also share some similarities
with our work, where the agent takes actions under resource constraints that are either fixed or with
a given growth rate bound. However, none of the aforementioned works considered the impacts
of delayed feedback on the regret and incentive costs performances. Note that, due to the loss of
information caused by delayed feedback, larger variances in the mean-reward estimations of the
arms are inevitable. This implies that, in order to achieve a more accurate arm quality estimation
under delayed feedbacks, a higher incentive cost is necessary.
3
Published as a conference paper at ICLR 2022
3	System Model
The system has a set of K ≥ 2 arms denoted by A = {1, . . . , K}, and each arm a follows a
Bernoulli reward distribution Pa with an unknown mean μ0 > 0. The bandit time horizon has T
rounds. In each time step t = 1, 2, . . . , T, a user arrives and chooses an arm It to pull. Then, the
user will receive a random reward feedback Xt 〜PIt. Both the arm selection It and the feedback
Xt are observable to the agent. We use Ta(t) , Pit=1 1{Ii=a} to denote the number of times that
arm a is pulled up to time step t. We let Ta (0) = 0, ∀a ∈ A. We assume that there is a unique best
arm a* ∈ A in the sense that a* = arg maXa∈A μa and let μ* = μo*. Also, We define ∆a，μ* - μ0
as the gap between the mean of the optimal arm and the mean of arm a.
3.1	Delayed Feedback Modeling
In this paper, we consider delayed feedback, i.e., when an arm It is pulled at time step t, the corre-
sponding Bernoulli reward Xt is observed after a delay period τIt,t, i.e., the feedback Xt is observed
at time step t + τIt,t. Without loss of generality, we model the random delay time as a random vari-
able Ta,t 〜Ta, where the delay distribution Ta of arm a is unknown to the agent.
We consider two settings of delayed feedback. We first consider i.i.d. delays {τt}t≤T across time
and arms, i.e., the delay distributions are identical for all arms. Thus, we omit the arm index in
the notations of delay feedback in this setting. Next, we generalize the delay modeling by allowing
arm-dependent delay distributions, where the delay distributions are allowed to differ across arms.
In both settings, we do not make further assumptions on the delay distributions, except that we only
require a finite delay expectation. Note that we allow the support of the delays to be unbounded,
i.e., an infinite delay time is possible in both settings. This models the practical scenarios in online
recommender systems that some user feedbacks (e.g., ratings and reviews) may never be received.
Under delayed feedbacks, we denote the total number of missing feedbacks from arm a up to a time
step t as Da(t) , Pts=1 1{s+τa,s>t}. We let Da* (t) = max1≤s≤t Da(s), ∀a ∈ A as the maximum
total number of delayed feedback for arm a up to time t. Note that Da*(t) = 0, ∀a ∈ A corresponds
to the non-delayed setting. In this case, Ta (t) denotes the total number of pulling times of arm a
up to time t. At each time step t, the agent observes a set of time-stamped feedback denoted by
St ⊂ N × {0, 1}. In the set St, each element is a pair of time index and a Bernoulli reward value,
and the time index is the time step when the corresponding reward is observable. Note that in this
model, by observing the set St, the agent is aware of the information of both the time step when the
feedback is received, and the arm that generated the feedback. We denote the total reward generated
by arm a up to time t as S°(t)，Ps=I Xs ∙ 1{Is=a,s+τa,s≤t}, andlet S0(0) = 0, ∀a ∈ A.
3.2	User Preferences and Incentive Impact Modeling
In this paper, we assume that the arrival at time t has a non-zero probability pa(t) ∈ (0, 1) to pull
each arm a ∈ A. We note thatpa(t) can also be thought ofas the user’s preference rate of arm a, and
Pa∈A pa (t) = 1, ∀t ≤ T. We adopt the widely accepted multinomial logit model in the economics
literature(Bawa & Shoemaker, 1987) to model arm a’s preference rate at time step t as follows:
(t、_	f(Sa(t- 1) + θa)
Pa () = Pi∈A f(S(t- 1) + 仇),
(1)
where f (∙) : R → (0, +∞) is a feedback function that is increasing, and θa > 0 denotes a fixed
initial preference bias of arm a. We note that the preference rate modeling in (Zhou et al., 2021) is
also based on the multinomial logit model, which appears to be in the same form as in (1). However,
the key difference between our preference model in (1) and that in (Zhou et al., 2021) is that the
accumulative award information Si(t - 1) in (1) accounts for reward information that can only be
observed up to time t. In other words, Si(t - 1) in (1) is affected by feedback delays. In fact, the
preference model in (Zhou et al., 2021) can be viewed as a special case of our model with zero delay.
Since the arriving users select arms based on preferences, while the agent aims to maximize the total
reward in the long run, there exists a general difference between users’ arm preferences and agent’s
intended arm selection. To induce users to pull arms following the agent’s goal, the agent needs to
intervene users’ arm pulling by offering incentives on its desired arm, so as to increase the user’
4
Published as a conference paper at ICLR 2022
preference of pulling the arm. That is, the agent incentivizes arm It0 at time step t so that pI0 (t)
increases accordingly. Note that when pI0 (t) increases, the preference rates on the other arms will
decrease since Pa∈A pa (t) = 1, t ≤ T. In this paper, we adopt the “coupon effect” model, which
is widely used in the economics and marketing literature (Bawa & Shoemaker, 1987). Specifically,
we consider a fixed incentive b in each time step and denote the time-dependent incentive impact as
g(b, t). Then, the posterior preference rates of the arms with incentive b are updated as follows:
p^i(t)
(Pi(t) + g(b,t)
I 1+g(b,t) ，
I	Pi⑴
[1 + g(b,t),
i = a,
i 6= a.
(2)
We remark that the definition of the posterior preference update in (2) also follows from the multi-
nomial logit model, which is widely used to model user preferences and their variations in bandit
field (Chen & Wang, 2017; Avadhanula, 2019; Dong et al., 2020; Zhou et al., 2021). Based on the
defined posterior preference, as incentive impact g(b, t) increases to infinity (either the incentive
value b increases to infinity or the users are more sensitive to incentives as time goes by), the user
preference will be induced to pulling the agent’s desired arm a with probability one. For further
detailed interpretations of the incentive impact function g(b, t), we refer readers to the literature
(e.g., (Zhou et al., 2021)). Note also that, due to the random user behaviors, it is possible that
It0 6= It, i.e., the arm that the agent incentivizes is not the one that a user pulls eventually. We define
the accumulative incentive up to time step t as Bt , Pts=1 bt, where bt ∈ {0, b}, ∀t ≤ T , denotes
the agent’s binary decision whether to offer incentive b at time step t.
3.3	Regret Modeling
As in most bandit learning problems, the goal of the agent is to maximize the total expected reward
E Pa∈A Sa(T) in the long run. Toward this end, we need the notion of the oracle incentivized
policy, where in hindsight, the agent is aware of the optimal arm a* and can always offer an infinite
amount of payments to users with feedback being observable immediately, so that the posterior
preference rate of arm a* is always infinitely close to one. As a result, the expected accumulative
reward generated under the oracle policy UP to time T is E[S0* (T)] = μ* ∙ T. However, since the
optimal arm a* is unknown to the agent, the goal of the agent is to maximize the total expected
reward E[ΓT] in the long run by designing an incentivized policy with low accumulative incentive
in the presence of self-reinforcing preferences and feedback delay. Similar to conventional MAB,
we measure the performance gap between our accumulative reward against that of the oracle policy,
which is denoted by regret RT. The expected (pseudo) regret is defined as follows:
E[Rt ] = μ* ∙ T — E[X Sa(T)].
a∈A
In this paper, our goal is to minimize E[RT] with low expected accumulative payment E[BT], i.e.,
sub-linear growth rate regarding time horizon T. It is clear that any policy with bounded payment
cannot outperform the oracle policy. Thus any expected regret defined by comparing with bounded-
payment policy is upper bounded by our regret.
4	Bandit Policy Design and Performance Analysis
In this section, we first present the general version of the UCB-FDF policy that works with any
delay distributions, where we upper bound the delay impact on the regret and incentive costs. Based
on this general result, we then study the regret and incentive costs performance of UCB-FDF un-
der the assumptions of 1) i.i.d. feedback delay across arms/times and 2) arm-dependent delay
distributions. In both cases, we denote the total number of missing feedbacks over all arms by
D(t) , Pa∈A Da(t), and denote the maximum number of missing feedbacks during the first t time
steps by D*(t) , max1≤s≤t D(s). For arm a at time step t, we denote the number of its pulling
times whose feedback is observed by Ta0 (t) = Ta(t) - Da(t), and denote the maximum mean gap
by ∆* = maxa∈A ∆a . At time step t, we denote the sample mean estimation (due to delayed
feedbacks) of arm a by μa(t) = Sa (t)∕Ta(t). Our UCB-FDF policy is illustrated in Algorithm 1.
5
Published as a conference paper at ICLR 2022
Algorithm 1 The UCB-Filtering-With-Delayed-Feedback Policy (UCB-FDF).
Require: Time horizon T and incentive payment b, the confidence interval of arm a at time step t
defined as c°(t) = Jln T/(2Ta (t)).
1:	Initialization: Incentivize pulling the arms satisfying Ta0 (t) = 0 With incentive payment b until
mina∈A Ta0 (t) ≥ 1. Let setU = A. Mark current time as t0.
2:	Exploration Phase: While |U | > 1, remove all the arms from set U satisfying μo(t) + ca(t) ≤
maxi=a,i∈u (μi(t) - α(t)) if there is any, then incentivize pulling arm a ∈ arg mini∈u Ta(t)
with payment b. If |U| = 1, let arm ^* = {a : a ∈ U} and mark current time as tι.
3:	Exploitation Phase: Incentivize pulling arm a* with payment b until it dominates: S^* (t) ≥
Pa=^* (Sa(t) + Da(t)). MarkcUrrenttimeaS t2.
4:	Self-Sustaining Phase: Users pull arms based on their own preferences until time T.
UCB-FDF policy contains three phases: an incentivized exploration phase, an incentivized exploita-
tion phase, and a self-sustaining phase. UCB-FDF policy tackles feedback delays in the following
two key aspects: (i) correcting the sample mean estimate of arms by only considering the number of
pulling times that have observed feedback, (ii) setting the length of the exploitation phase in such a
way that the outstanding rewards do not harm the emergence of “dominance” (i.e., one arm receiving
at least half of the rewards) of the sampled optimal arm. Subsequently, these two aspects also influ-
ence the regret and incentive. In order to have enough arm exploration with an unbiased sample mean
estimate, the loss of counted number of pulling times necessitates a carefully designed exploration
phase that incentivizes the pulling of the least informed arm a ∈ arg mini∈U Ta0 (t) under delayed
feedbacks. Similarly, the delay-based dominance threshold (i.e., Sa* (t) ≥ P0=^* (Sa(t) + DaD
in Step 3 of Algorithm 1) guarantees the dominance of sampled optimal arm, while also accounts
for a longer exploitation phase to mitigate the delayed feedback effect. We now analyze the upper
bounds of the pseudo regret and expected incentive of the UCB-FDF policy.
Lemma 1. (UCB-Filtering-with-Delayed-Feedback) Given a fixed time horizon T, if g(b, t) > 1,
and f(x) = Θ(xα) with α > 12, then the pseudo regret of Algorithm 1 E[RT] is upper bounded by
E[RT] ≤
a6=a*
8∆a ggbb, 1) - 1)+8∆* l T g(b, 1)∆* (E[D*(T)] + 4K)
一(g(b,i)- ι)∆a — n +	g(b,i)- 1	,
with the expected payment E[BT] upper bounded by
2g (b, 1) + 1 8 ln T 8 ln T
e[Bt] ≤b ∙先占 kn+aX* F+ed*t )]+4K.
Remark 1. The UCB-FDF policy achieves a sub-linear total incentive cost by leveraging the prop-
erty of self-reinforcing preference. We can show that as long as the self-reinforcing preference
function f(x) satisfies the condition f(x) = Θ(xα) with α > 1, then “monopoly” happens with
probability one (i.e., the scenario where only one arm has positive probability to be pulled, thus
this particular arm is the only preferred arm). A natural incentivizing policy is to incentivize sam-
pled optimal arm until it achieves monopoly. However, the key challenge here is that the onset
of monopoly could take infinite time steps, which implies linear total incentive. Moreover, self-
reinforcing property is not merely disrupting the system from converging to the optimal arm. The
key idea in our UCB-FDF policy design is that under the condition of the self-reinforcing preference
function f (∙), after one arm establishes its dominance (i.e., the arm a generates at least half of the
current total reward), it will have exponentially increasing probability to beat other arms and achieve
monopoly. More importantly, we can show that the onset of arm dominance takes sub-linear times,
thus allowing us to achieve sub-linear total incentive costs.
Remark 2. The feedback delay affects the observation of arm dominance, since the missing re-
ward information from suboptimal arms, if not compensated carefully, can potentially destroy the
dominance status of the optimal arm. Thus, to guarantee dominance of the optimal arm, a longer
exploitation phase is necessary, and thus a large total incentive is required.
2The notation Θ() in this paper is defined as that, if f(x) = Θ(g(x)), then there exist x0 and two constants
C1 , C2 > 0, such that C1g(x) ≤ f(x) ≤ C2g(x) for all x ≥ x0.
6
Published as a conference paper at ICLR 2022
The existence of delays in our MAB model introduces an additive term Θ(E[D*(T)]) in both regret
and incentive costs, which is dependent on the maximum accumulated delayed feedback up to time
horizon T. Based on Lemma 1, in what follows, we will analyze the upper bounds of the expected
maximum accumulated delayed feedback under different assumptions on delay distributions.
4.1 Arm-Independent Delay with a Finite Expectation
We now analyze the delay impact under our first assumption. In the arm-independent case, we
consider an i.i.d. sequence {τt}t of random delay regarding time step t ≤ T. We do not make any
assumption on the shape of the delay distribution, except that we only assume a finite expectation
E[τ1]. Thus, an infinite random delay is possible under this assumption, implying some feedbacks
may never be observed by the agent. Our results show that under this assumption, we can still
achieve similar orders of the regret and incentive costs growth rates, since the key fact is that we can
upper bound the expected number of such unexpectedly large random delays for every time step t.
Existing works (e.g., (Joulani et al., 2013)) provided a systematic study on the delay effect on the
partial monitoring problem with side information, including the stochastic problems. Although these
works only considered the classic stochastic MAB, they share some similarities with our work in
that their analysis of delay effects also leveraged the maximum number of missing feedbacks during
the first t time steps D* (t). However, since our UCB-FDF policy has a different structure compared
to these works on delayed stochastic MAB, their delay analysis is not applicable to our policy. Next,
we restate a result in (Joulani et al., 2013), which will be useful in our analysis.
Lemma 2 (Lemma 2 in Joulani et al. (2013)). Assume {τι,...,τt} is a Sequence of i.i.d. random
variables with finite expected value, and let B(t, S) = S + 2 log t + √4s log t. Then, it holds that
E[D*(t)] ≤ B(t,E[τι]) + 1.
Theorem 3. (Arm-Independent Delay) Under i.i.d. delays with a finite expectation and the condi-
tions of Lemma 1, the pseudo regret of Algorithm 1 E[RT] is upper bounded by
- 2g(b,1)∆* + X 8∆a (g(b, 1)- 1) +8∆*] ι T + g(b,1)∆* (,4E[τι] ln T + Eg] + 4K + 1)
一g(b,1) -1 a=α	(g(b, 1)-1)∆a Jn	g(b, 1)- 1	,
with the expected payment E[BT] upper bounded by
b ∙ *T(2+套+X ∆a)ln T+EinT+E[T1]+4K+1_.
We note that the gap summation of arms £0=0* ∆a plays an important role in both regret and
total incentive. As arm gaps getting smaller, it is more difficult to distinguish the optimal arm from
others. Thus, a longer exploration phase is required to conduct enough sampling, which implies a
larger regret and a larger total incentive costs. On the other hand, the feedback delay causes additive
terms in both regret and incentive costs in terms of the expected delay E[τ1], and the delay impact
can be upper bounded as long as the expected delay E[τ1] is no larger than time horizon T.
4.2 Arm-Dependent Delay with Finite Expectations
Now, we further relax the assumption on the delay to allow arm-dependent delays. In this case, the
delay has two key impacts on the system: (i) for each arm, there is a different real-time information
loss when estimating the sample mean, (ii) for the whole arm set, different scales of delay cause an
uneven arm estimation, which results in a larger risk of the elimination of the optimal arm in the
UCB-based exploration step. We formally state our arm-dependent delay assumption as follows:
Assumption 1. The delays of arm a ∈ A form an independent delay sequence {τa,t}, where each
element is a random variable satisfying τa,t 〜Ta, with a finite expectation E[τa,ι] < +∞, ∀α ∈ A.
Under Assumption 1, We show a more general result on the upper bound of E[D*(t)] as follows:
Lemma 4. Under Assumption 1, given a finite number of arms K > 0, it holds that
E[D*(t)] ≤ X 2E[τa,ι] + 3Klog ".
a∈A
7
Published as a conference paper at ICLR 2022
The result in Lemma 4 implies larger upper bounds of the regret and incentive, due to the existence
of the pre-log factor K . This is a consequence of the situation where, as we consider arm-dependent
delay distributions, the worst case could be evenly distributed expected delays E[τa,1] of arm a with
respect to time horizon T. Formally, we state the upper bounds of regret and incentive as follows:
Theorem 5. (Arm-Dependent Delay) Under Assumption 1 and the conditions of Lemma 1, the
pseudo regret of Algorithm 1 E[RT] is upper bounded by
X 8∆a (g(b,1)- 1) +8∆* l T + g(b, 1)∆* (3Kln K + Pa∈∕ 2E%,ι] + 4K)
aM	(g(b,i)- ι)∆a	n	g(b,1)- 1	,
with the expected payment E[BT] upper bounded by
2g(b, 1) + 1	8	8	T
b ∙	ɪ [(ɪmn+aX. &)ln T+3Kln K+X E[τa,1]+4K
Similar to the results under the i.i.d. delay assumption, we can still upper bound the regret and
incentive by an logarithmic growth rate O(log T) under arm-dependent delay. This implies that even
under the weak delay assumption where only finite expectation is needed, UCB-FDF can estimate
arms without too much bias, and finally achieve logarithmic regret with logarithmic incentive costs.
5 Experimental Results
In this section, we first introduce our experiment setting and the dataset, then illustrate our experi-
mental results. Due to the space limit, the full experimental results are provided in Appendix ??.
5.1	Experimental Setup
1)	System Parameters: We conduct experiments under two different delay settings. The system
parameters are set as follows: a three-armed model with Arm1 being the optimal arm, and the initial
preference bias θ = [1, 5, 5], i.e., the optimal arm has the least initial bias. We choose a three-armed
model since large arm set requires a proportional large time horizon to distinguish optimal arm, while
in the public Amazon Review Data, the amount of reviews for most products is limited (no more
than 3,000 for each product). The self-reinforcing preference function is chosen as f (x) = xα with
α = 2. The constant incentive for each time step is set as b = 1.5 with an incentive impact function
g(b, t) = b. For the delay distribution, we use normal distributions in both assumption setting, as
normal distributions have an infinite support x ∈ R. Under the arm-independent delay setting, we
choose the delay distribution as Tt 〜N(10,2). Under the arm-dependent delay setting, We choose
the delay distributions as τιt 〜N(80,2), τ2t 〜N(10,2), and τ3,t 〜 N(10, 2) for Arms 1, 2, and
3, respectively. We only generate non-negative samples of delay under both assumptions.
Product Category	Arm1(optimal)	Arm2	Arm3
Pet Supplies	0.773	0.656	0.626
Electronics	0.757	0.605	0.617
Home and Kitchen	0.875	0.588	0.673
Books	0.915	0.551	0.706
Table 1: Means of products (arms) in different categories.
2)	Dataset: We use Amazon Review Data (Ni et al., 2019) to provide a practical learning environ-
ment. The Amazon RevieW Data includes 233 million customer revieWs (ratings, posting times) for
29 product categories. In the experiment, We select three products to serve as the arms that have
the largest number of revieWs in category Pet Supplies, Electronics, Home and Kitchen, and Books,
respectively. For each product (arm), We leverage the rating and unixReviewTime informa-
tion in each revieW, and the total number of revieWs is 3,000 for each product. The range of ratings
in Amazon RevieW Data is the discrete set {1, 2, 3, 4, 5}. We convert the rating values to binary by
setting the rating values 1 and 2 as 0 and the rating value 4 and 5 as 1, and the revieWs With rating
8
Published as a conference paper at ICLR 2022
Time horizon
o>⊂00 U-
0	1000	2000	3000
Time horizon
(a)
0	1000	2000	3000
Time horizon
(b)
o>⊂00u-
0	1000	2000	3000
Time horizon
(c)
0	1000	2000	3000
Time horizon
(d)
Figure 1: The performance of policy UCB-FDF in the face of no delay in (a), the performance of
policy UCB-FDF in the face of arm-independent delay in (b), the performance of policy UCB-FDF
in the face of arm-dependent delay in (c), and the performance of policy UCB-List in the face of
arm-dependent delay in (d).
value 3 are removed. For each product, the binary review ratings are sorted by unixReviewTime,
so the ratings come in real-world order in the experiment. We summarize the mean values of the
products by their Bernoulli ratings in the four selected categories, as shown in Table 1.
5.2	Results and Discussions
1)	Results: The experiment results are illustrated in Figure 1. (a) shows the average regret and
incentive trends with policy UCB-FDF under setting with no delay. (b) and (c) show the average
regret and incentive trends with policy UCB-FDF under settings with arm-independent delays and
arm-dependent delays, respectively. In Figures (c) and (d), we compare the performances with
policy UCB-FDF and baseline policy UCB-List (Zhou et al., 2021). Specifically, Figure (d) shows
the performance under policy UCB-List in the face of arm-dependent delays that is the same as that
in (c). Each curve is constructed by regret or incentive values with different time horizons from
T = 150 to T = 3000, incremented by 150. Each node value in curves are averaged by 100 trials.
2)	Discussion: Comparing (a) with (b) and (c) in Figure 1, we can observe the delay impact on
regret and total incentive, that both the regret and total incentive are increased due to the delayed
feedback. Comparing (c) and (d) in Figure 1, we observe that under the bandit instances in the face
of same delayed feedback, our policy UCB-FDF reaches sub-linear growth rate in both regret and
total incentive, except the total incentive in category Pet Supplies, since it may require more time
steps to converge while our data is limited, while the policy UCB-List cannot guarantee sub-linear
growth rate for both regret and total incentive.
6 Conclusion
In this work, we proposed a practical bandit model that considers the joint effect of the incentive
impact, delayed feedback and self-reinforcing user preferences in real-world recommender systems.
We proposed a UCB-FDF policy that achieves logarithmic growth rates of pseudo regret and to-
tal incentive costs for a fixed time horizon T . We also analyzed how different delay assumptions
influence the regret and incentive costs. Specifically, we considered arm-independent delays and
arm-dependent delays with weak assumption that only requires a finite expectation. The evaluations
with real-world customer review data showed the effectiveness of our UCB-FDF policy in achieving
sub-linear regret while spending only sub-linear total incentive costs under delayed feedback.
Acknowledgments
This work has been supported in part by NSF grants CAREER CNS-2110259, CNS-2112471, CNS-
2102233, CCF-2110252, and a Google Faculty Research Award.
9
Published as a conference paper at ICLR 2022
References
Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for the
mnl-bandit. In Conference on Learning Theory,pp. 76-78. PMLR, 2017.
Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic
learning approach to assortment selection. Operations Research, 67(5):1453-1485, 2019.
Vashist Avadhanula. The MNL-Bandit Problem: Theory and Applications. Columbia University,
2019.
Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. science, 286
(5439):509-512, 1999.
Kapil Bawa and Robert W Shoemaker. The effects of a direct mail coupon on brand choice behavior.
Journal of Marketing Research, 24(4):370-376, 1987.
Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual
bandits. arXiv preprint arXiv:1904.10040, 2019.
SebaStien BUbeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.
SoUmen Chakrabarti, Alan Frieze, and JUan Vera. The inflUence of search engines on preferential
attachment. Internet Mathematics, 3(3):361-381, 2006.
Xi Chen and Yining Wang. A note on a tight lower boUnd for mnl-bandit assortment selection
models. arXiv preprint arXiv:1709.06109, 2017.
Kefan Dong, Yingkai Li, Qin Zhang, and YUan ZhoU. MUltinomial logit bandit with low switching
cost. In International Conference on Machine Learning, pp. 2607-2615. PMLR, 2020.
Stephen G Eick. The two-armed bandit with delayed responses. The Annals of Statistics, pp. 254-
264, 1988.
Peter Frazier, David Kempe, Jon Kleinberg, and Robert Kleinberg. Incentivizing exploration. In
Proceedings of the fifteenth ACM conference on Economics and computation, pp. 5-22, 2014.
Ashish Goel, Sanjeev Khanna, and Brad NUll. The ratio index for bUdgeted learning, with applica-
tions. In Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms, pp.
18-27. SIAM, 2009.
SUdipto GUha and Kamesh MUnagala. Approximation algorithms for bUdgeted learning problems.
In Proceedings ofthe thirty-ninth annual ACM symposium on Theory of computing, pp. 104-113,
2007.
Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In
International Conference on Machine Learning, pp. 1453-1461. PMLR, 2013.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
in applied mathematics, 6(1):4-22, 1985.
Tal Lancewicki, Shahar Segal, Tomer Koren, and Yishay Mansour. Stochastic multi-armed bandits
with unrestricted delay distributions. arXiv preprint arXiv:2106.02436, 2021.
Anne Gael Manegueu, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits
with arm-dependent delays. In International Conference on Machine Learning, pp. 3348-3356.
PMLR, 2020.
Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit
exploration. In Proceedings of the Sixteenth ACM Conference on Economics and Computation,
pp. 565-582, 2015.
10
Published as a conference paper at ICLR 2022
Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled
reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP),pp.188-197, 2019.
Jacob Ratkiewicz, Santo Fortunato, Alessandro Flammini, Filippo Menczer, and Alessandro Vespig-
nani. Characterizing and modeling the dynamics of online popularity. Physical review letters, 105
(15):158701, 2010.
Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. arXiv
preprint arXiv:1802.05693, 2018.
Min Shao and Chrysostomos L Nikias. Signal processing with fractional lower order moments:
stable processes and their applications. Proceedings of the IEEE, 81(7):986-1010, 1993.
Claire Vernade, Olivier Cappe, and Vianney Perchet. Stochastic bandit models for delayed Conver-
sions. arXiv preprint arXiv:1706.09186, 2017.
Siwei Wang and Longbo Huang. Multi-armed bandits with compensation. arXiv preprint
arXiv:1811.01715, 2018.
Tianchen Zhou, Jia Liu, Chaosheng Dong, and Jingyuan Deng. Incentivized bandit learning with
self-reinforcing user preferences. arXiv preprint arXiv:2105.08869, 2021.
11