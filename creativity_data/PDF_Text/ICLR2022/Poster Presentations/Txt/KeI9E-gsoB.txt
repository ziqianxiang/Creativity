Published as a conference paper at ICLR 2022
Learning curves for Gaussian process
regression with power-law priors and targets
Hui Jin	Pradeep Kr. Banerjee
UCLA	MPI MiS
huijin@ucla.edu	pradeep@mis.mpg.de
Guido Montufar
UCLA & MPI MiS
montufar@math.ucla.edu
Ab stract
We characterize the power-law asymptotics of learning curves for Gaussian process
regression (GPR) under the assumption that the eigenspectrum of the prior and
the eigenexpansion coefficients of the target function follow a power law. Under
similar assumptions, we leverage the equivalence between GPR and kernel ridge
regression (KRR) to show the generalization error of KRR. Infinitely wide neural
networks can be related to GPR with respect to the neural network GP kernel and the
neural tangent kernel, which in several cases is known to have a power-law spectrum.
Hence our methods can be applied to study the generalization error of infinitely
wide neural networks. We present toy experiments demonstrating the theory.
1 Introduction
Gaussian processes (GPs) provide a flexible and interpretable framework for learning and adaptive
inference, and are widely used for constructing prior distributions in non-parametric Bayesian learning.
From an application perspective, one crucial question is how fast do GPs learn, i.e., how much training
data is needed to achieve a certain level of generalization performance. Theoretically, this is addressed
by analyzing so-called “learning curves”, which describe the generalization error as a function of
the training set size n. The rate at which the curve approaches zero determines the difficulty of
learning tasks and conveys important information about the asymptotic performance ofGP learning
algorithms. In this paper, we study the learning curves for Gaussian process regression. Our main
result characterizes the asymptotics of the generalization error in cases where the eigenvalues of the
GP kernel and the coefficients of the eigenexpansion of the target function have a power-law decay. In
the remainder of this introductory section, we review related work and outline our main contributions.
Gaussian processes A GP model is a probabilistic model on an infinite-dimensional parameter space
(Williams and Rasmussen, 2006; Orbanz and Teh, 2010). In GP regression (GPR), for example, this
space can be the set of all continuous functions. Assumptions about the learning problem are encoded
by way of a prior distribution over functions, which gets transformed into a posterior distribution given
some observed data. The mean of the posterior is then used for prediction. The model uses only a finite
subset of the available parameters to explain the data and this subset can grow arbitrarily large as more
data are observed. In this sense, GPs are “non-parametric” and contrast with parametric models, where
there is a fixed number of parameters. For regression with Gaussian noise, a major appeal of the GP
formalism is that the posterior is analytically tractable. GPs are also one important part in learning with
kernel machines (Kanagawa et al., 2018) and modeling using GPs has recently gained considerable
traction in the neural network community.
Neural networks and kernel learning From a GP viewpoint, there exists a well known correspon-
dence between kernel methods and infinite neural networks (NNs) first studied by Neal (1996). Neal
showed that the outputs of a randomly initialized one-hidden layer neural network (with appropriate
scaling of the variance of the initialization distribution) converges to a GP over functions in the limit
of an infinite number of hidden units. Follow-up work extended this correspondence with analytical
expressions for the kernel covariance for shallow NNs by Williams (1997), and more recently for
deep fully-connected NNs (Lee et al., 2018; de G. Matthews et al., 2018), convolutional NNs with
many channels (Novak et al., 2019; Garriga-Alonso et al., 2019), and more general architectures
(Yang, 2019). The correspondence enables exact Bayesian inference in the associated GP model for
1
Published as a conference paper at ICLR 2022
infinite-width NNs on regression tasks and has led to some recent breakthroughs in our understanding
of overparameterized NNs (Jacot et al., 2018; Lee et al., 2019; Arora et al., 2019; Belkin et al., 2018;
Daniely et al., 2016; Yang and Salman, 2019; Bietti and Mairal, 2019). The most prominent kernels
associated with infinite-width NNs are the Neural Network Gaussian Process (NNGP) kernel (Lee
et al., 2018; de G. Matthews et al., 2018), and the Neural Tangent Kernel (NTK) (Jacot et al., 2018).
Empirical studies have shown that inference with such infinite network kernels is competitive with
standard gradient descent-based optimization for fully-connected architectures (Lee et al., 2020).
Learning curves A large-scale empirical characterization of the generalization performance of
state-of-the-art deep NNs showed that the associated learning curves often follow a power law of the
form n-β with the exponent β ranging between 0.07 and 0.35 depending on the data and the algorithm
(Hestness et al., 2017; Spigler et al., 2020). Power-law asymptotics of learning curves have been
theoretically studied in early works for the Gibbs learning algorithm (Amari et al., 1992; Amari and
Murata, 1993; Haussler et al., 1996) that showed a generalization error scaling with exponent β=0.5,
1 or 2 under certain assumptions. More recent results from statistical learning theory characterize
the shape of learning curves depending on the properties of the hypothesis class (Bousquet et al.,
2021). In the context of GPs, approximations and bounds on learning curves have been investigated
in several works (Sollich, 1999; Sollich and Halees, 2002; Sollich, 2001; Opper and Vivarelli, 1999;
Opper and Malzahn, 2002; Williams and Vivarelli, 2000; Malzahn and Opper, 2001a;b; Seeger et al.,
2008; Van Der Vaart and Van Zanten, 2011; Le Gratiet and Garnier, 2015), with recent extensions
to kernel regression from a spectral bias perspective (Bordelon et al., 2020; Canatar et al., 2021). Fora
review on learning curves in relation to its shape and monotonicity, see Loog et al. (2019); Viering et al.
(2019); Viering and Loog (2021). A related but complementary line of work studies the convergence
rates and posterior consistency properties of Bayesian non-parametric models (Barron, 1998; Seeger
et al., 2008; Van Der Vaart and Van Zanten, 2011).
Power-law decay of the GP kernel eigenspectrum The rate of decay of the eigenvalues of the
GP kernel conveys important information about its smoothness. Intuitively, if a process is “rough”
with more power at high frequencies, then the eigenspectrum decays more slowly. On the other hand,
kernels that define smooth processes have a fast-decaying eigenspectrum (Stein, 2012; Williams and
Rasmussen, 2006). The precise eigenvalues (λp)p≥1 of the operators associated to many kernels and
input distributions are not known explicitly, except for afew special cases (Williams and Rasmussen,
2006). Often, however, the asymptotic properties are known. The asymptotic rate of decay of the
eigenvalues of stationary kernels for input distributions with bounded support is well understood
(Widom, 1963; Ritter et al., 1995). Ronen et al. (2019) showed that for inputs distributed uniformly on
a hypersphere, the eigenfunctions of the arc-cosine kernel are spherical harmonics and the eigenvalues
follow a power-law decay. The spectral properties of the NTK are integral to the analysis of training
convergence and generalization of NNs, and several recent works empirically justify and rely on a
power law assumption for the NTK spectrum (Bahri et al., 2021; Canatar et al., 2021; Lee et al., 2020;
Nitanda and Suzuki, 2021). Velikanov and Yarotsky (2021) showed that the asymptotics of the NTK
of infinitely wide shallow ReLU networks follows a power-law that is determined primarily by the
singularities of the kernel and has the form λp αp-a with α = 1 + d, where d is the input dimension.
Asymptotics of the generalization error of kernel ridge regression (KRR) There is a well known
equivalence between GPR and KRR with the additive noise in GPR playing the role of regularization
in KRR (Kanagawa et al., 2018). Analysis of the decay rates of the excess generalization error of
KRR has appeared in several works, e.g, in the noiseless case with constant regularization (Bordelon
et al., 2020; Spigler et al., 2020; Jun et al., 2019), and the noisy optimally regularized case (Caponnetto
and De Vito, 2007; Steinwart et al., 2009; Fischer and Steinwart, 2020) under the assumption that
the kernel eigenspectrum, and the eigenexpansion coefficients of the target function follow a power
law. These assumptions, which are often called resp. the capacity and source conditions are related
to the effective dimension of the problem and the difficulty of learning the target function (Caponnetto
and De Vito, 2007; Blanchard and Mucke, 2018). Cui et al. (2021) present a unifying picture of the
excess error decay rates under the capacity and source conditions in terms of the interplay between
noise and regularization illustrating their results with real datasets.
Contributions In this work, we characterize the asymptotics of the generalization error of GPR
and KRR under the capacity and source conditions. Our main contributions are as follows:
2
Published as a conference paper at ICLR 2022
•	When the eigenspectrum of the prior decays with rate α and the eigenexpansion coefficients of
the target function decay with rate β, we show that with high probability over the draw of n input
samples, the negative log-marginal likelihood behaves as Θ(nmax{ 1,102β+1}) (Theorem 7) and
the generalization error behaves as Θ(nmax{ 1 -1,102β}) (Theorem 9). In the special case that the
model is correctly specified, i.e., the GP prior is the true one from which the target functions are
actually generated, our result implies that the generalization error behaves as O(n 1 - 1) recovering
as a special case a result due to Sollich and Halees (2002) (vide Remark 10).
•	Under similar assumptions as in the previous item, we leverage the equivalence between GPR
and KRR to show that the excess generalization error of KRR behaves as Θ(nmax{ 1 -1,1-2β})
(Theorem 12). In the noiseless case with constant regularization, our result implies that the
1-2β
generalization error behaves as Θ(n) recovering as a special case a result due to Bordelon et al.
(2020). Specializing to the case of KRR with Gaussian design, we recover as a special case a result
due to Cui et al. (2021) (vide Remark 14).
For the unrealizable case, i.e., when the target function is outside the span of the eigenfunctions
with positive eigenvalues, we show that the generalization error converges to a constant.
•	We present a few toy experiments demonstrating the theory for GPR with arc-cosine kernel without bi-
ases (resp. with biases) which is the conjugate kernel of an infinitely wide shallow network with two in-
puts and one hidden layer without biases (resp. with biases) (Cho and Saul, 2009; Ronen et al., 2019).
2 Bayesian learning and generalization error for GPs
In GP regression, our goal is to learn a target function f : Ω → R between an input X ∈ Ω and
output y ∈ R based on training samples Dn = {(xi, yi)}in=1. We consider an additive noise
model yi = f (Xi) + ei, where eg i蚓 N(0,02皿).If P denotes the marginal density of the inputs
Xi, then the pairs (χi,yi) are generated according to the density q(χ,y) = ρ(χ)q(y∖χ), where
q(y|X) = N (y|f (X),σt2rue). We assume that there is a prior distribution Π0 on f which is defined as a
zero-mean GP with continuous and bounded covariance function k: Ω × Ω →R, i.e., f 〜GP(0,k). This
means that for any finite setx= (X1,...,Xn)T, the random vector f(x) = (f (X1),...,f (Xn))T follows
the multivariate normal distribution N(0,Kn) with covariance matrix Kn = (k(Xi,Xj))in,j=1 ∈Rn×n.
By Bayes’ rule, the posterior distribution over f given the training data is given by
1n
dπn(f ∖Dn)= Z(D ) UN(yi\f (Xi)
Z(Dn) i=1
,σmodel)dΠ0(f),
where Π0 is the prior distribution, Z(Dn) = Qin=1N(yi∖f(Xi), σm2 odel)dΠ0(f) is the marginal
likelihood or model evidence and σmodel is the sample variance used in GPR. In practice, we do not
know the exact value of σtrue and so our choice of σmodel can be different from σtrue. The GP prior
and the Gaussian noise assumption allows for exact Bayesian inference and the posterior distribution
over functions is again a GP with mean and covariance function given by
m(X)= Kxx(Kn+ σmodelIn)-1y,x ∈。	(I)
友(X,X0) = k(X,X0) -Kxx(Kn +σmodeiIn)-1Kxχ0 ,X,X0 ∈ Ω,	(2)
where Kxx = (k(X1,X),...,k(Xn,X))T and y= (y1,...,yn)T ∈Rn (Williams and Rasmussen, 2006, Eqs.
2.23-24).
The performance of GPR depends on how well the posterior approximates f as the number of training
samples n tends to infinity. The distance of the posterior to the ground truth can be measured in various
ways. We consider two such measures, namely the Bayesian generalization error (Seeger et al., 2008;
Haussler and Opper, 1997; Opper and Vivarelli, 1999) and the excess mean squared error (Sollich
and Halees, 2002; Le Gratiet and Garnier, 2015; Bordelon et al., 2020; Cui et al., 2021).
Definition 1 (Bayesian generalization error). The Bayesian generalization error is defined as the
Kullback-Leibler divergence between the true density q(y∖X) and the Bayesian predictive density
pn(y∖X,Dn) = R N (y∖f (X),σm2 odel)dΠn(f∖Dn),
GS = Jqa Pj(XDy dXdy.
(3)
3
Published as a conference paper at ICLR 2022
A related quantity of interest is the stochastic complexity (SC), also known as the free energy, which
is just the negative log-marginal likelihood. We shall primarily be concerned with a normalized version
of the stochastic complexity which is defined as follows:
F0(Dn)=-log
Z (Dn)
Qn=Iq(yi|Xi)
,∕Qn=1N(yif(χi),σmodei)d∏0(f)
Qn=iq(yi|xi)
(4)
The generalization error (3) can be expressed in terms of the normalized SC as follows (Watanabe,
2009, Theorem 1.2):
G(Dn)=E(xn+1,yn+1)F0(Dn+1)-F0(Dn),	(5)
where Dn+1 =Dn∪{(xn+1,yn+1)} is obtained by augmenting Dn with a test point (xn+1,yn+1).
Ifwe only wish to measure the performance of the mean of the Bayesian posterior, then we can use
the excess mean squared error:
Definition 2 (Excess mean squared error). The excess mean squared error is defined as
M (Dn)= E(xn+ι,yn+ι) (m(Xn+1)- yn+1)2-σtLe = EXn+ι (Jm(Xn+1) - f (Xn+1))2 .	⑹
Proposition 3 (Normalized stochastic complexity for GPR). Assume that σm2 odel = σt2rue =σ2. The
normalizedSCF0(Dn) (4) for GPR withpriorGP(0,k) is given as
F0(Dn) = 1 logdet(In+KKn-) + 212 yτ (In+KKn )-1y - 2⅛ (y-f(χ))T (y -f(χ)),	(7)
where = (1,...,n)T. The expectation of the normalized SC w.r.t. the noise is given as
EeF0 (Dn)= 1 logdet(In + KKn)- 1 Tr(In-(In + KKn ) )+ 212 f (X)T (In + KKn)	f (X) ∙⑻
This is a basic result and has applications in relation to model selection in GPR (Williams and
Rasmussen, 2006). For completeness, we give a proof of Proposition 3 in Appendix B. Seeger et al.
(2008, Theorem 1) gave an upper bound on the normalized stochastic complexity for the case when
f lies in the reproducing kernel Hilbert space (RKHS) of the GP prior. Itis well known, however, that
sample paths ofGP almost surely fall outside the corresponding RKHS (Van Der Vaart and Van Zanten,
2011) limiting the applicability of the result.
We next derive the asymptotics of EeF0(Dn), the expected generalization error EeG(Dn) =
EeE(xn+1,yn+1) F0 (Dn + 1) -EeF0(Dn), and the excess mean squared errorEeM(Dn).
3 Asymptotic analysis of GP regression with power-law priors
We begin by introducing some notations and assumptions. We assume that f ∈ L2(Ω,ρ). By the
generalization of Mercer’s theorem (Steinwart and Scovel, 2012, Corollary 3.2), the covariance
function of the GP prior can be decomposed as k(X1, X2) = Pp∞=1 λpφp(X1)φp(X2) ρ-almost
surely, where (φp(χ))p≥ι are the eigenfunctions of the operator Lk : L2(Ω, P) → L2(Ω, ρ);
(Lk f )(x) = Jωk(χ,s)f (s)dρ(s), and (λp)p≥ι are the corresponding positive eigenvalues. We index
the sequence of eigenvalues in decreasing order, that is λι ≥ λ2 ≥ …> 0. The target function f (x) is
decomposed into the orthonormal set (Φp(χ))p≥ι and its orthogonal complement {φp (x):P ≥ 1}⊥ as
∞
f (x) = Eμpφp(x) + μoφo(x) ∈ L2(Ω,ρ),	(9)
p=1
where μ = (μ0,μ1,…,μp,…)t are the coefficients of the decomposition, and φ0(χ) satisfies ∣∣φ0(χ)∣∣2 =
1 and φ0(X) ∈ {φp(X) : p ≥ 1}⊥. For given sample inputs X, let φp(X) = (φp(X1), ... , φp(Xn))T,
Φ = (φ0(X),φ1(X),...,φp(X),...) and Λ = diag{0,λ1,...,λp,...}. Then the covariance matrix Kn can
be written as Kn = ΦΛΦt , and the function values on the sample inputs can be written as f (x) = Φμ.
We shall make the following assumptions in order to derive the power-law asymptotics of the
normalized stochastic complexity and the generalization error of GPR:
Assumption 4 (Power law decay of eigenvalues). The eigenvalues (λp)p≥1 follow the power law
CλP-α ≤ λp≤ Cλp-α,∀P ≥ 1	(10)
where Cλ, C and a are three positive constants which satisfy 0 <Cχ ≤ C and a> L
4
Published as a conference paper at ICLR 2022
As mentioned in the introduction, this assumption, called the capacity condition, is fairly standard
in kernel learning and is adopted in many recent works (Bordelon et al., 2020; Canatar et al., 2021;
Jun et al., 2019; Bietti et al., 2021; Cui et al., 2021). Velikanov and Yarotsky (2021) derived the exact
value of the exponent α when the kernel function has a homogeneous singularity on its diagonal, which
is the case for instance for the arc-cosine kernel.
Assumption 5 (Power law decay of coefficients of decomposition). Let C* ,Cμ > 0 and β > 1/2 be
positive constants and let {pi}i≥1 be an increasing integer sequence such thatsupi≥1(pi+1 -pi) < ∞.
The coefficients (μp)p≥ι ofthe decomposition (9) ofthe targetfUnctionfollow the power law
∣μp∣≤ CμP-β ,∀p ≥ 1 and ∣μpj≥ CμPiT ,∀i ≥ 1.	(11)
Since f ∈ L2(Ω,ρ), We have P∞=0μp < ∞. The condition β > 1/2 in Assumption 5 ensures that
the sum P∞=0μp does not diverge. When the orthonormal basis (φp(χ))p is the Fourier basis or the
spherical harmonics basis, the coefficients (μp)p decay at least as fast as a power law so long as the
target function f(x) satisfies certain smoothness conditions (Bietti and Mairal, 2019). Velikanov
and Yarotsky (2021) gave examples of some natural classes of functions for which Assumption 5 is
satisfied, such as functions that have a bounded support with smooth boundary and are smooth on
the interior of this support, and derived the corresponding exponents β .
Assumption 6 (Boundedness of eigenfunctions). The eigenfunctions (φp(x))p≥0 satisfy
kφ0k∞ ≤Cφ and kφpk∞ ≤CφPτ, P≥ 1,	(12)
where Cφ and T are two positive constants which satisfy T < α-1.
The second condition in (12) appears, for example, in Valdivia (2018, Hypothesis H1 ) and is less
restrictive than the assumption of uniformly bounded eigenfunctions that has appeared in several other
works in the GP literature, see, e.g., Braun (2006); Chatterji et al. (2019); Vakili et al. (2021).
Define
Tι(Dn) = 1 logdet(In + φΛσφτ) -2Tr(In-(In + φΛσφτ)T),	(13)
T2(Dn )= 2⅛ f(x)T (In + φΛσφτ )-1 f(x),	(14)
G1(Dn)=E(xn+1,yn+1)(T1(Dn+1)-T1(Dn)),	(15)
G2(Dn)=E(xn+1,yn+1)(T2(Dn+1)-T2(Dn)).	(16)
Using(8)and(5),wehaveEF0(Dn)=T1(Dn)+T2(Dn) andEG(Dn)=G1(Dn)+G2(Dn). Intu-
itively, G1 corresponds to the effect of the noise on the generalization error irrespective of the target func-
tionf, whereas G2 corresponds to the ability of the model to fit the target function. As we will see next in
Theorems 9 and 11, ifα is large, then the error associated with the noise is smaller. When f is contained
in the span of the eigenfunctions {φp}p≥1, G2 decreases with increasing n, butiff contains an orthogo-
nal component, then the error remains constant and GP regression is not able to learn the target function.
3.1	Asymptotics of the normalized stochastic complexity
We derive the asymptotics of the normalized SC (8) for the following two cases: μo = 0 and μo > 0.
When μo = 0, the target function f (x) lies in the span of all eigenfunctions with positive eigenvalues.
Theorem 7 (Asymptotics of the normalized SC, μo = 0). Assume that μo = 0 and
σm2 odel = σt2rue = σ2 = Θ(1). Under Assumptions 4, 5 and 6, with probability of at least
1 一 n-q over sample inputs (xi)n=1, where 0 ≤ q < min{ (2β-D4α-1-2τ), α-1-2τ }, the expected
normalized SC (8) has the asymptotic behavior:
EeFO(Dn) = [2logdet(I + σ⅛λX 1 Tr(I-(I+ σn2 λ)-1)+ 2n2μT(I+ σn2A)-1μ] (I+o(I))
= θ(nmax{ α, kOr+1}).	(17)
The complete proof of Theorem 7 is given in Appendix D.1. We give a sketch of the proof below. In
the sequel, we use the notations O and Θ to denote the standard mathematical orders and the notation
O to suppress logarithmic factors.
5
Published as a conference paper at ICLR 2022
Proof sketch of Theorem 7. By (8), (13) and (14) we have EF0 (Dn) = T1(Dn) + T2(Dn). In
order to analyze the terms T1 (Dn) and T2(Dn), we will consider truncated versions of these
quantities and bound the corresponding residual errors. Given a truncation parameter R ∈ N, let
ΦR = (φ0(x),φ1(x),...,φR(x)) ∈ Rn×R be the truncated matrix of eigenfunctions evaluated at the
data points, AR = diag(0,λι,…,Ar) ∈ R(R+D×(R+I) and μR = (μo,μι,…,μR) ∈ RR+1. We define
the truncated version of T1 (Dn ) as follows:
T1,R (Dn) = 2 lθgdet(ln + φRσRφR )- 2 Tr(In-(In + φRΛσRφR )-)	(⑻
Similarly,	define Φ>R =	(φR+1(x),	φR+2 (x), ... , φp(x), ...),	A>R =	diag(λR+1, ... , λp, ...),
fR(x) =	Pp=I μpφp(x),	fR(x) =	(fR(xi),…，fR(Xn))T,	f>R(x)	= f(χ) - ∕r(x), and
f>R(x) = (f>R(x1),...,f>R(xn))T. The truncated version ofT2(Dn) is then defined as
(19)
The proof consists of three steps:
•	Approximation step: In this step, we show that the asymptotics of T1,R resp. T2,R dominates that of
the residuals, |T1,R(Dn) -T1(Dn)| resp. |T2,R(Dn) -T2(Dn)| (see Lemma 32). This builds upon
first ShOWingthat ∣∣Φ>RA>RφTRk2 = O(max{nR-α,n 1 R 1"-2α ,R1-α}) (see Lemma 25) and then
choosing R=n 1+κ where 0 <κ< α^2l- 2τ when we have ∣∣Φ>r A>r ΦT>r∣2 = o(1). Intuitively,the
choice of the truncation parameter Ris governed by the fact that λR = Θ(R-α) = n-1+κα =o(n-1).
•	Decomposition step: In this step, we decompose T1,R into a term independent of ΦR anda series
involving ΦTRΦR -nIR, and likewise for T2,R (see Lemma 34). This builds upon first showing using
the Woodbury matrix identity (Williams and Rasmussen, 2006, §A.3) that
Ti,r(Du) = 1 logdet(IR + ⅛ Ar ΦRΦr) - 2 TγΦr (σ2IR+ArΦRΦr)-1ArΦR,	(20)
T2,R(Dn) = ⅛ μRφR ΦR(σ2IR +ArΦRΦr )-1μR,	(21)
and then Taylor expanding the matrix inverse (σ2IR + ARΦTRΦR)-1 in (20) and (21) to
show that the ΦR-independent terms in the decomposition of T1,R and T2,R are, respectively,
2 ιogdet(IR+σ⅛ AR) ― 2 Tr(IR -(IR+σ⅛ AR)-I ),and 2n2 μR(IR+σ AR)-lμR.
•	Concentration step: Finally, we use concentration inequalities to show that these ΦR-independent
terms dominate the series involving ΦTRΦR-nIR (see Lemma 35) when we have
T1,R (Dn)= (1 logdet(IR + σ⅛AR)- 1 Tr(IR -(IR + σ⅛ AR)-I))(I+ o(1)) = θ(n ɑ ),
τ2,R(Dn)=(令〃R(IR+枭AR)T〃R)(1+。⑴『(或:;—+1}), α=2β-1.
The key idea is to consider the matrix AR/2 (I + 枭 AR)-1∕2ΦRΦr(I + f AR)T/2 AR/2 and show
that it concentrates around "Ar(I + 备)-1 (see Corollary 22). Note that an ordinary application
of the matrix Bernstein inequality to ΦRΦr — nIR yields ∣∣ΦRΦr -nI ∣∣2 = O(R√n), which is not
sufficient for our purposes, since this would give θ(R√n) = o(n) only when α> 2. In contrast, our
results are valid for α > 1 and cover cases of practical interest, e.g., the NTK of infinitely wide shallow
ReLU network (Velikanov and Yarotsky, 2021) and the arc-cosine kernels over high-dimensional
hyperspheres (Ronen et al., 2019) that have α =1 + O(d), where d is the input dimension.	口
For μo > 0, we note the following result:
Theorem 8 (Asymptotics of the normalized SC, μo > 0). Assume μo > 0 and
σm2 odel = σt2rue = σ2 = Θ(1). Under Assumptions 4, 5 and 6, with probability of at least
1-n-q over sample inputs (xi)n=ι, where 0 ≤ q< min{ 2β-1 ,α}∙ min{。,0产,2：-1}. the expected
normalized SC (8) has the asymptotic behavior: EeF(O(Dn )=壶μ2n+o(n).
The proof of Theorem 8 is given in Appendix D.1 and follows from showing that when μo > 0,
T2,R(Dn) = (2n2 μR(IR + σnAR)TμR)(1 + o(1)) = 212μ0n + o(n) (see Lemma 38), which
dominates T1(Dn) and the residual |T2,R(Dn)-T2(Dn)|.
6
Published as a conference paper at ICLR 2022
3.2	Asymptotics of the Bayesian generalization error
In this section, we derive the asymptotics of the expected generalization error EG(Dn) by analyzing
the asymptotics of the components G1(Dn) and G2(Dn) in resp. (15) and (16) for the following two
cases: μo = 0 and μo > 0. First, We consider the case μ0 = 0.
Theorem 9 (Asymptotics of the Bayesian generalization error, μo = 0). LetAssumptions 4, 5, and
6 hold. Assume that μo = 0 and 02^^ =。2皿=σ2 = Θ(nt) where 1 一 ^^t <t< L Then With
probability ofat least 1 — n-q over sample inputs (Xi)n=1 where 0 ≤ q < [ɑ-(1+2τ4α-tX(2β-I), the
expectation of the Bayesian generalization error (3) w.r.t. the noise has the asymptotic behavior:
EeG(Dn) = 1+σ1) (Tr(I + σΛ)-1Λ-kΛ1∕2(I + %A)TkF + k(I + 枭Λ)-1 μ∣∣2)
=ɪ Θ(nmax{ (1-αα(1-t), (T(I)}).	(22)
The proof of Theorem 9 is given in Appendix D.2. Intuitively, for a given t, the exponent (1-怨1) in
(22) captures the rate at which the model suppresses the noise, while the exponent (1-2?(1-" captures
the rate at which the model learns the target function. A larger β implies that the exponent (1-2?1-9
is smaller and it is easier to learn the target. A larger α implies that the exponent (1-?1-' is smaller
and the error associated with the noise is smaller as well. A larger α, however, also implies that the
exponent (1-22(1-" is larger (recall that a> 1 and β> 1/2 by Assumptions 4 and 5, resp.), which
means that it is harder to learn the target.
Remark 10. If f 〜 GP(0, k), then using the Karhunen-Loeve expansion we have
f (x) = E∞=ι y∕λpωpφp (x), where (ωp)∞=ι are i.i.d. Standard Gaussian variables. We can
bound ωp almost surely as ∣ωp∣ ≤ C log P, where C = sup?〉] ||gp is a finite ConStant. Comparing
with the expansion of f (x) in (9), we find that μ? = y∕λpωp = O(p-α/2 logp) = O(p-α∕2+ε) where
ε> 0 is arbitrarilysmall. Choosing β = a/2 — ε in (22), we have EeG(Dn) = O(n1-1+2ε). This rate
matches that of an earlier result due to Sollich and Halees (2002), where it is shown that the asymptotic
learning curve (as measured by the expectation of the excess mean squared error, EfM(Dn)) scales
as nα-1 when the model is correctly specified, i.e., f is a samplefrom the same Gaussian process
GP(0,k), and the eigenvalues decay as a power lawfor large i, λi 〜iα.
For μo > 0, we note the following result:
Theorem 11 (Asymptotics of the Bayesian generalization error, μo > 0). LetAssumptions 4, 5, and
6 hold. Assume that μo > 0 and 02^^ =。2皿=σ2 = Θ(nt) where 1 — ια2τ <t< 1. Then with
probability ofat least 1 — n-q over sample inputs (xi)n=ι, where 0 ≤ q< [α-(1+2τ41-t)](2βT), the
expectation of the Bayesian generalization error (3) w.r.t. the noise has the asymptotic behavior:
EeG(Dn) = 2~2 μ0 +。⑴.
In general, if μo > 0, then the generalization error remains constant when n →∞. This means that
if the target function contains a component in the kernel of the operator Lk, then GP regression is not
able to learn the target function. The proof of Theorem 11 is given in Appendix D.2.
3.3	Asymptotics of the excess mean squared error
In this section we derive the asymptotics of the excess mean squared error in Definition 2.
Theorem 12 (Asymptotics of excess mean squared error). Let Assumptions 4, 5, and 6 hold. Assume
σ21odei = Θ(nt) where 1 — ^^T <t< 1. Then with probability ofat least 1 — n-q over sample inputs
(xi)n=ι, where 0 ≤ q< [。-(1+2丁4；-"](2"-1), the excess mean squared error (6) has the asymptotic:
EeM(Dn) = (1 + o(1)) *(Tr(I + +Λ)-1Λ — kA1/2(I + +Λ)-1∣∣F)
model	model	model
+ k(I + M Λ)-1μk2
model
(	2	1-α-1	(1-2e)(1-1)
= Θ(max{σ 言Uen a ,n α }
when μo = 0, and EeM(Dn) = μ2 + o(1), when μo > 0.
7
Published as a conference paper at ICLR 2022
The proof of Theorem 12 uses similar techniques as Theorem 9 and is given in Appendix D.3.
Remark 13 (Correspondence with kernel ridge regression). The kernel ridge regression (KRR)
estimator arises as a solution to the optimization problem
1n
∕ = argmin -	(f(xi)-yi)2+λkfk2Hk,	(23)
f∈Hk ni=1	k
where the hypothesis space Hk is chosen to be an RKHS, and λ > 0 is a regularization parameter.
The solution to (23) is unique as a function, and is given by f(x) = KTx(Kn + nλIn)-1y, which
coincides with the posterior mean function Tm(X) of the GPR (1) if σ21ode] = nλ (Kanagawa et al.,
2018, Proposition 3.6). Thus, the additive Gaussian noise in GPR plays the role of regularization
in KRR. Leveraging this well known equivalence between GPR and KRR we observe that Theorem 12
also describes the generalization error of KRR as measured by the excess mean squared error.
Remark 14. Cui et al. (2021) derived the asymptotics of the expected excess mean-squared error for
different regularization strengths and different scales of noise. In particular, for KRR with Gaussian
design where Λβ2(φι(x),...,φβ(x))) is assumed tofollow a Gaussian distribution N (0,Λr) , and
regularization λ=nt-1 where 1-α≤t, Cui et al. (2021, Eq. 10) showed that
E{xi}i=1 EeM(Dn) = O (max{σ2ruen 1-α- ,n "'。*"}).	(24)
Let δ = n-q, where 0 ≤ q < [α-(1+2T)O-t)(2β-1). By Markov's inequality, this implies
that with probability of at least 1 一 δ, EeM(Dn) = O( 1 max{σ2,uen ɪ α t, n (I " })=
O(nq max{σ2ruen ɪ α t ,n (ɪ -飞' "}). Theorem 12 improves upon this by ShOWing that with prob-
ability ofat least 1 一 δ, we have an optimal bound EeM(Dn) = Θ(max{σ2ruen ɪ a t ,n 0	}).
Furthermore, in contrast to the approach by Cui et al. (2021), we have no requirement on the
distribution ofφp(x), and hence our result is more generally applicable. For example, Theorem 12
can be applied to KRR with the arc-cosine kernel when the Gaussian design assumption is not valid.
In the noiseless setting (σtrue =0) with constant regularization (t=0), Theorem 12 implies that the
1-2β
mean squared error behaves as Θ(n	). This recovers a result in Bordelon et al. (2020, §2.2).
Our upper bound in Theorem 12 matches with the ones derived in (Steinwart et al., 2009; Fischer
and Steinwart, 2020). Steinwart et al. (2009) and Fischer and Steinwart (2020) also derived algorithm
independent minmax lower bounds. In contrast to their results, our Theorem 12 gives lower bounds
for different regularization strengths λ.
4	Experiments
We illustrate our theory on a few toy experiments. We let the input x be uniformly distributed on a
unit circle, i.e., Ω = S1 and P=U (S1). The points on S1 can be represented by X = (cosθ,sinθ) where
θ ∈ [-∏,∏). We use the first order arc-cosine kernel function without bias, k(W)o 田&$ (xι ,x2 ) = ∏ (sinψ +
(π 一 ψ) cosψ), where ψ = hX1,X2i is the angle between X1 and X2. Hence Assumption 4 is satisfied
with α=4. We consider the target functions in Table 1, which satisfy Assumption 5 with the indicated
β, and μo indicates whether the function lies in the span of eigenfunctions of the kernel. For each target
we conduct GPR 20 times and report the mean and standard deviation of the normalized SC and the
Bayesian generalization error in Figure 1, which agree with the asymptotics predicted in Theorems 7
and 9. The details of the experiments appear in Appendix A, where we also show more experiments
confirming our theory for zero- and second- order arc-cosine kernels, with and without biases.
5	Conclusion
We described the learning curves for GPR for the case that the kernel and target function follow a
power law. This setting is frequently encountered in kernel learning and relates to recent advances
on neural networks. Our approach is based on a tight analysis of the concentration of the inner product
of empirical eigenfunctions ΦTΦ around nI. This allowed us to obtain more general results with more
8
Published as a conference paper at ICLR 2022
	function value	β	μo	EeF 0(Dn)	EeG(Dn )
f1	cos2θ	+∞	0	Θ(n1∕4)	∙	Θ(n-3/4)
f2	θ2	-2-	->0~	Θ(n)	Θ(1)
f3	(∣θ∣-∏∕2)2	2	0	-Θ(n1/4)	Θ(n-3/4)
f4	(π∕2一θ,	θ∈ [0,π) (-π∕2-θ, θ∈ [-π,0)	1	0	Θ(n3/4)	Θ(n-1/4)
Table 1: Target functions used in the experiments for the first order arc-cosine kernel without bias
k(1∕o bias，their values of β and μo, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems.
f2
—11.8262n0∙1960
—f— Experiment values
fι
OS P①Z=EUJ」ON
—6.1735n0∙9940
—Experiment values
2 力
OXl
1 6
OS P①Z=EE」ON
IO2	IO3
Number of Samples
IO2	IO3
Number of Samples
6.3946。WVW
Experiment values
IO2	IO3
Number of Samples
Figure 1: Normalized SC (top) and Bayesian generalization error (bottom) for GPR with the kernel
kw(1/)o bias and the target functions in Table 1. The orange curves show the linear regression fit for the
experimental values (in blue) of the log Bayesian generalization error as a function of log n.
realistic assumptions than previous works. In particular, we recovered some results on learning curves
for GPR and KRR previously obtained under more restricted settings (vide Remarks 10 and 14).
We showed that when β ≥ α∕2, meaning that the target function has a compact representation in terms
of the eigenfunctions of the kernel, the learning rate is as good as in the correctly specified case. In
addition, our result allows us to interpret β from a spectral bias perspective. When 1 < β ≤ 2, the
larger the value of β , the faster the decay of the generalization error. This implies that low-frequency
functions are learned faster in terms of the number of training data points.
By leveraging the equivalence between GPR and KRR, we obtained a result on the generalization
error of KRR. In the infinite-width limit, training fully-connected deep NNs with gradient descent
and infinitesimally small learning rate under least-squared loss is equivalent to solving KRR with
respect to the NTK (Jacot et al., 2018; Lee et al., 2019; Domingos, 2020), which in several cases is
known to have a power-law spectrum (Velikanov and Yarotsky, 2021). Hence our methods can be
applied to study the generalization error of infinitely wide neural networks. In future work, it would be
interesting to estimate the values ofα andβ for the NTK and the NNGP kernel of deep fully-connected
or convolutional NNs and real data distributions and test our theory in these cases. Similarly, it would
be interesting to consider extensions to finite width kernels.
Acknowledgment
This project has received funding from the European Research Council (ERC) under the EU’s Horizon
2020 research and innovation programme (grant agreement no 757983).
9
Published as a conference paper at ICLR 2022
References
S. Amari and N. Murata. Statistical theory of learning curves under entropic loss criterion. Neural
Computation, 5(1):140-153,1993.
S. Amari, N. Fujita, and S. Shinomoto. Four types of learning curves. Neural Computation, 4(4):
605-618, 1992.
S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. In Advances in Neural Information Processing Systems, volume 32, pages
8139-8148, 2019.
Y. Bahri, E. Dyer, J. Kaplan, J. Lee, and U. Sharma. Explaining neural scaling laws. arXiv preprint
arXiv:2102.06701, 2021.
A. R. Barron. Information-theoretic characterization of Bayes performance and the choice of priors in
parametric and nonparametric problems. InD. A. Bernardo J., Berger J. and S. A., editors, Bayesian
statistics, volume 6, pages 27-52. Oxford University Press, 1998.
M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel learning.
In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 541-549,
2018.
A.	Bietti and J. Mairal. On the inductive bias of neural tangent kernels. In Advances in Neural
Information Processing Systems, volume 32, pages 12873-12884, 2019.
A. Bietti, L. Venturi, and J. Bruna. On the sample complexity of learning with geometric stability.
arXiv preprint arXiv:2106.07148, 2021.
G. Blanchard and N. Mucke. Optimal rates for regularization of statistical inverse learning problems.
Foundations of Computational Mathematics, 18(4):971-1013, 2018.
B.	Bordelon, A. Canatar, and C. Pehlevan. Spectrum dependent learning curves in kernel regression
and wide neural networks. In Proceedings of the 37th International Conference on Machine
Learning (ICML), pages 1024-1034, 2020.
O. Bousquet, S. Hanneke, S. Moran, R. van Handel, and A. Yehudayoff. A theory of universal learning.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC),
pages 532-541, 2021.
M.	L. Braun. Accurate error bounds for the eigenvalues of the kernel matrix. The Journal of Machine
Learning Research, 7:2303-2328, 2006.
A. Canatar, B. Bordelon, and C. Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature communications,
12(1):1-12, 2021.
A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Foundations
of Computational Mathematics, 7(3):331-368, 2007.
N.	Chatterji, A. Pacchiano, and P. Bartlett. Online learning with kernel losses. In Proceedings of the
36th International Conference on Machine Learning (ICML), pages 971-980, 2019.
Y. Cho and L. K. Saul. Kernel methods for deep learning. In Advances in Neural Information
Processing Systems, volume 22, pages 342-350, 2009.
H. Cui, B. Loureiro, F. Krzakala, and L. Zdeborovd. Generalization error rates in kernel regression:
The crossover from the noiseless to noisy regime. arXiv preprint arXiv:2105.15004, 2021.
A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power
of initialization and a dual view on expressivity. In Advances In Neural Information Processing
Systems, volume 29, pages 2253-2261, 2016.
10
Published as a conference paper at ICLR 2022
A. G. de G. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani. Gaussian process
behaviour in wide deep neural networks. In International Conference on Learning Representations,
2018.
P. Domingos. Every model learned by gradient descent is approximately a kernel machine. arXiv
preprint arXiv:2012.00152, 2020.
S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithms.
Journal ofMachine Learning Research, 21:1-38,2020.
A. Garriga-Alonso, C. E. Rasmussen, and L. Aitchison. Deep convolutional networks as shallow
gaussian processes. In International Conference on Learning Representations, 2019.
D. Haussler and M. Opper. Mutual information, metric entropy and cumulative relative entropy risk.
The Annals of Statistics, 25(6):2451-2492, 1997.
D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. Rigorous learning curve bounds from statistical
mechanics. Machine Learning, 25(2-3):195-236, 1996.
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang, and
Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems, volume 31, pages 8571-8580,
2018.
K.-S. Jun, A. Cutkosky, and F. Orabona. Kernel truncated randomized ridge regression: Optimal rates
and low noise acceleration. Advances in Neural Information Processing Systems, 32:15358-15367,
2019.
M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian processes and kernel
methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018.
L. Le Gratiet and J. Garnier. Asymptotic analysis of the learning curve for Gaussian process regression.
Machine Learning, 98(3):407-433, 2015.
J. Lee, J. Sohl-Dickstein, J. Pennington, R. Novak, S. Schoenholz, and Y. Bahri. Deep neural networks
as gaussian processes. In International Conference on Learning Representations, 2018.
J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide neural
networks of any depth evolve as linear models under gradient descent. In Advances in Neural
Information Processing Systems, volume 32, pages 8572-8583, 2019.
J. Lee, S. Schoenholz, J. Pennington, B. Adlam, L. Xiao, R. Novak, and J. Sohl-Dickstein. Finite
versus infinite neural networks: an empirical study. In Advances in Neural Information Processing
Systems, volume 33, pages 15156-15172, 2020.
M. Loog, T. Viering, and A. Mey. Minimizers of the empirical risk and risk monotonicity. In Advances
in Neural Information Processing Systems, volume 32, pages 7478-7487, 2019.
D. Malzahn and M. Opper. Learning curves for Gaussian processes regression: A framework for
good approximations. In Advances in Neural Information Processing Systems, volume 13, pages
273-279, 2001a.
D. Malzahn and M. Opper. Learning curves for Gaussian processes models: Fluctuations and
universality. In International Conference on Artificial Neural Networks, pages 271-276, 2001b.
R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996.
ISBN 0387947248.
A.	Nitanda and T. Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent
kernel regime. In International Conference on Learning Representations, 2021.
11
Published as a conference paper at ICLR 2022
R. Novak, L. Xiao, Y. Bahri, J. Lee, G. Yang, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein.
Bayesian deep convolutional networks with many channels are gaussian processes. In International
Conference on Learning Representations, 2019.
M. Opper and D. Malzahn. A variational approach to learning curves. In Advances in Neural
Information Processing Systems, volume 14, pages 463-469,2002.
M. Opper and F. Vivarelli. General bounds on Bayes errors for regression with Gaussian processes.
In Advances in Neural Information Processing Systems, volume 11, pages 302-308, 1999.
P. Orbanz and Y. W. Teh. Bayesian nonparametric models. In Encyclopedia of Machine Learning,
pages 81-89. Springer, 2010.
K. Ritter, G. W. Wasilkowski, and H. WoZniakowski. Multivariate integration and approximation
for random fields satisfying Sacks-Ylvisaker conditions. The Annals of Applied Probability, pages
518-540, 1995.
B.	Ronen, D. Jacobs, Y. Kasten, and S. Kritchman. The convergence rate of neural networks for
learned functions of different frequencies. Advances in Neural Information Processing Systems,
32:4761-4771, 2019.
M. W. Seeger, S. M. Kakade, and D. P. Foster. Information consistency of nonparametric Gaussian
process methods. IEEE Transactions on Information Theory, 54(5):2376-2382, 2008.
P. Sollich. Learning curves for Gaussian processes. In Advances in Neural Information Processing
Systems, volume 11, pages 344-350, 1999.
P. Sollich. Gaussian process regression with mismatched models. In Advances in Neural Information
Processing Systems, volume 13, pages 519-526, 2001.
P. Sollich and A. Halees. Learning curves for Gaussian process regression: Approximations and
bounds. Neural Computation, 14(6):1393-1428, 2002.
S.	Spigler, M. Geiger, and M. Wyart. Asymptotic learning curves of kernel methods: empirical data
versus teacher-student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020
(12):124001, 2020.
M. L. Stein. Interpolation of spatial data: Some theory for kriging. Springer Science & Business
Media, 2012.
I.	Steinwart and C. Scovel. Mercer’s theorem on general domains: On the interaction between
measures, kernels, and rkhss. Constructive Approximation, 35:363-417, 2012.
I.	Steinwart, D. R. Hush, C. Scovel, et al. Optimal rates for regularized least squares regression. In
Conference on Learning Theory, pages 79-93, 2009.
J.	A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
S.	Vakili, K. Khezeli, and V. Picheny. On information gain and regret bounds in Gaussian process
bandits. In International Conference on Artificial Intelligence and Statistics, pages 82-90, 2021.
E. A. Valdivia. Relative concentration bounds for the spectrum of kernel matrices. arXiv preprint
arXiv:1812.02108, 2018.
A. Van Der Vaart and H. Van Zanten. Information rates of nonparametric Gaussian process methods.
Journal of Machine Learning Research, 12(6), 2011.
M. Velikanov and D. Yarotsky. Universal scaling laws in the gradient descent training of neural
networks. arXiv preprint arXiv:2105.00507, 2021.
T.	Viering and M. Loog. The shape of learning curves: A review. arXiv preprint arXiv:2103.10948,
2021.
12
Published as a conference paper at ICLR 2022
T. Viering, A. Mey, and M. Loog. Open problem: Monotonicity of learning. In Conference on
Learning Theory,pages3198-3201,20l9.
S. Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University Press, 2009.
H. Widom. Asymptotic behavior of the eigenvalues of certain integral equations. Transactions of
the American Mathematical Society, 109(2):278-295, 1963.
C. K. Williams. Computing with infinite networks. In Advances in Neural Information Processing
Systems, volume 9, pages 295-301, 1997.
C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning. MIT press, 2006.
C. K. Williams and F. Vivarelli. Upper and lower bounds on the learning curve for Gaussian processes.
Machine Learning, 40(1):77-102, 2000.
G. Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes.
In Advances in Neural Information Processing Systems, volume 32, pages 9951-9960, 2019.
G. Yang and H. Salman. A fine-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599, 2019.
Appendix
A Experiments for arc-cosine kernels of different orders
In our experiment, the input space and input distribution are Ω = S1 and P = U(S1), and we
use the first order arc-cosine kernel function. (Cho and Saul, 2009) showed that this kernel is
the conjugate kernel of an infinitely wide shallow ReLU network with two inputs and no biases
in the hidden layer. GP regression with prior GP(0, k) corresponds to Bayesian training of this
network (Lee et al., 2018). Under this setting, the eigenvalues and eigenfunctions are λι = ∏2,
λ2 = λ3 = 4, λ2p = λ2p+1 = ∏2((2p-2)2-i)2, P ≥ 2 and φι(θ) = 1, φ2(θ) = √2cosθ, φ3(θ) = √2sinθ,
Φ2p(θ) = √22cos(2p- 2)θ,φ2p+1(θ) = √22sin(2p - 2)θ, P ≥ 2. Hence Assumption 4 is satisfied with
α = 4, and the second part of Assumption 6 is satisfied with kφpk≤ 旺,P ≥ 1.
The training and test data are generated as follows: We independently sample training inputs
x1, ... ,xn and test input xn+1 from U(S1) and training outputs yi, i = 1, ... ,n from N(f(xi),σ2),
where we choose σ = 0.1. The Bayesian predictive density conditioned on the test point xn+1
N(τm(xn+ι),⅛(xn+ι,xn+ι)) is obtained by (1) and (2). We compute the normalized SC by (7) and
the Bayesian generalization error by the Kullback-Leibler divergence between N(f (xn+1),σ2) and
N (τm(Xn+1),⅛(Xn+1,Xn+1)).
Next we present experiment results for arc-cosine kernels of different orders and arc-cosine kernels
with biases. Consider the first order arc-cosine kernel function with biases,
k(1∕)bias(xι,X2) = ∏(sinψ+(π-ψ)cosψ), whereψ = arccos(2(hx1 ,x2i + l))
(25)
Ronen et al. (2019) showed that this kernel is the conjugate kernel of an infinitely wide shallow ReLU
network with two inputs and one hidden layer with biases, whose eigenvalues satisfy Assumption 4
with α = 4. The eigenfunctions of this kernel are the same as that of the first-order arc-cosine
kernel without biases, kw(1/)o bias in Section 4. We consider the target functions in Table 3, which
satisfy Assumption 5 with the indicated β, and μo indicates whether the function lies in the span
of eigenfunctions of the kernel. For each target we conduct GPR 20 times and report the mean and
standard deviation of the normalized SC and the Bayesian generalization error in Figure 3, which
agree with the asymptotics predicted in Theorems 7 and 9.
Table 2 summarizes all the different kernel functions that we consider in our experiments with pointers
to the corresponding tables and figures.
Summarizing the observations from these experiments, we see that the smoothness of the activation
function (which is controlled by the order of the arc-cosine kernel) influences the decay rate α of the
13
Published as a conference paper at ICLR 2022
	kernel function	α	activation function	bias	pointer
k(1) w/o bias	∏ (sinψ + (π — ψ)cosψ)	丁	max{0,x}	no	Table 1/Figure 1
k(1) w/ bias	∏ (sinψ+ (π — ψ)cosψ)	丁	max{0,x}	yes	Table 3/Figure 3
k(2) w/o bias	1 (3sinψcosψ + (π — ψ)(1 + 2cos2ψ))	^^6^	(max{0,x})2	no	Table 4/Figure 4
k(2) w/ bias	1 (3sinψcosψ + (π — ψ)(1 + 2cos2ψ))	^^6^	(max{0,x})2	yes	Table 5/Figure 5
k(0) w/o bias	∏ (sinψ + (π — ψ)cosψ)	^^2^	1(1+sign(x))	no	Table 6/Figure 6
k(0) w/ bias	∏ (sinψ+ (π — ψ)cosψ)	F	1 (1+sign(x))	yes	Table 7/Figure 7
Table 2: The different kernel functions used in our experiments, their values ofα, the corresponding
neural network activation function along with a pointer to the tables showing the target functions used
for the kernels and the corresponding figures.
eigenvalues. In general, when the activation function is smoother, the decay rate α is larger. Theorem 9
then implies that smooth activation functions are more capable in suppressing noise but slower in
learning the target. We also observe that networks with biases are more capable at learning functions
compared to networks without bias. For example, the function cos(2θ) cannot be learned by the zero
order arc-cosine kernel without biases (see Table 6 and Figure 6), but it can be learned by the zero
order arc-cosine kernel with biases (see Table 7 and Figure 7).
	function value	β	μo	Ee F ° (Dn )	Ee G(Dn)
f1	cos2θ	+∞	~~6~	Θ(n1/4)	Θ(n-3∕4)
f2	θ2	-2-	~7Γ	-θ(n1/4)~	θ(n-3∕4)
f3	(∣θ∣-π∕2)2	2	~0~	-θ(n1/4)~	θ(n-3/4)-
f4	[π∕2-θ, θ ∈ [0,π) (-π∕2-θ, θ∈ [—π,0)	1	0	Θ(n3/4)	Θ(n-1/4)
Table 3: Target functions used in the experiments for the first order arc-cosine kernel with bias, kw(1/) bias,
their values of β and μo, and theoretical rates for the normalized SC and the Bayesian generalization
error from our theorems.
fι
Us paN-eE」ON
3
1
OS pəz--euɪjON
OS POZ=(0E」ON
. t
Number of Samples
Experiment values
Number of Samples
Number of Samples
Number of Samples
—17.9018∏-°∙6"3
-4- Experiment values
Figure 3:	Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
kw(1/) bias and the target functions in Table 3. The orange curves show the linear regression fit for the
experimental values (in blue) of the log Bayesian generalization error as a function of log n.
14
Published as a conference paper at ICLR 2022
	function value	β	μo	E<F 0(Dn)	ECG(Dn)
fl	cos2θ	+∞	0	-Θ(n1/6)-	Θ(n-5/6)
f2	sign(θ)	-1-	0	-θ(n5/6)	θ(n-1/6)
f3	π∕2-∣θ∣	2	0	-Θ(n1/2)-	θ(n-1/2)
f4	(n/2-θ,	θ∈ [0,π) (-π∕2 — θ, θ∈ [—π,0)	1	> 0	Θ(n)	Θ(1)
Table 4: Target functions used in the experiments for the second order arc-cosine kernel without
bias, kW2∕)o bias，their values of β and μo, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems.
0ω P①Z=eE」0N
4
1
ɔs P①Z=eE」ON
O2W
ɔs PaZ=EEON
4
1
0∞ P①Z=EuJ」oN
IO2	103
Number of Samples
IO2	IO3
Number of Samples
IO3
Number of Samples

Figure 4:	Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
kw(2/)o bias and the target functions in Table 4.
	function value	β	μo	EC F 0 (Dn)	EC G(Dn)
f1	cos2θ	+∞	0	-Θ(n1/6)-	9(n-5/6)一
f2	θ2	-2-	0	-θ(n1/2)	θ(n-1/2)
f3	(∣θ∣-π∕2)2	2	~0~	-θ(n1/2)-	Θ(n-1/2)
f4	(∏∕2 — θ, θ ∈[0,π) (—π∕2-θ, θ∈ [—π,0)	1	0	Θ(n5/6)	Θ(n-"6)
Table 5: Target functions used in the experiments for the second order arc-cosine kernel with bias,
k(2) bias, their values of β and μo, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems.
IO2	IO3
Number of Samples
u∞ P①Z=EE」。N
IO2	IO3
Number of Samples
ɔs P①Z=eE」0N
IO3
Number of Samples
Figure 5:	Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
kw(2/) bias and the target functions in Table 5.
15
Published as a conference paper at ICLR 2022
	function value	β	μo	ECF 0(Dn)	ECG(Dn)
f	cos2θ	+∞	>>0~	Θ(n)	Θ⑴
f	sign(θ)	-1-	0	-Θ(n1/2)-	Θ(n-L2)
f	π∕2-∣θ∣	2	0	-θ(n1/2)	θ(n-1/2)-
f4	π∕2-θ,	θ∈ [0,π) -π∕2-θ, θ∈ [-π,0)	1	> 0	Θ(n)	Θ(1)
Table 6: Target functions used in the experiments for the zero order arc-cosine kernel without bias,
k(0)o bias, their values of β and μ0, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems.
fl	f2	f3	k
OS P①ZneE」ON
4
1
0∞ P①Z一方E」oN
ɔs P①ZneE」ON
IO2	IO3
Number of Samples
IO2	IO3
Number of Samples
IO2	103
Number of Samples

Figure 6:	Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
kw(0/)o bias and the target functions in Table 6.
	function value	β	μo	EC F 0 (Dn)	E G(Dn)
f1	cos2θ	+∞	0	-Θ(n1/2)-	Θ(n-1/2)
f2	θ2	-2-	0	-θ(n1/2)	θ(n-1/2)
f3	(∣θ∣-π∕2)2	2	~0~	-θ(n1/2)-	Θ(n-1/2)
f4	(∏∕2 — θ, θ ∈ [0,π) (-π∕2-θ, θ∈ [—π,0)	1	0	Θ(n1/2)	Θ(n-1/2)
Table 7: Target functions used in the experiments for the zero order arc-cosine kernel with bias,
k(0) bias, their values of β and μo, and theoretical rates for the normalized SC and the Bayesian
generalization error from our theorems.
ɔs P①Z=eE」0N
10
US pən-(oe.! ON
—8.1417n0∙5033
―Experiment values
IO2	IO3
Number of Samples
IO2	IO3
Number of Samples
Number of Samples
Figure 7:	Normalized SC (top) and Bayesian generalization error (bottom) for GPR with kernel
kw(0/) bias and the target functions in Table 7.
16
Published as a conference paper at ICLR 2022
B Proofs related to the marginal likelihood
ProofofProPosition3. Let y = (yι,...,yn)T be the outputs of the GP regression model on training
inputs x. Under the GP prior, the prior distribution of y is N(0,Kn). Then the evidence of the model
is given as follows:
Zn =
1
(2n)n/2det(Kn)1/2
1
(2π)nσndet(Kn)1/2
e-1 y K-Iy dy
LF1 yT(K-1+σ12 I)y+表 yTy-袤 yTy dy.
(26)
Letting K-1 = K-1 + 表 I and μ =表 Kn y, We have
Zn =
________1_______ / e-2(y-μ)TK-1(y-μ)一*yTy+ 2μTK-1μ
(2∏)n σndet(Kn)1/2 4九
1
(2π)n σndet(Kn)1/2
(2n)n/2det(Kn)1/2e-2σ2 yT y+1 μT K -1μ
(27)
det(Kn)1/	e-袤 yT y+1 μT K -1μ
(2π)n∕2σndet(Kn )1/2
The normalized evidence is
(2n)-n/20-ne- 2⅛ (y-f(x))T(y-f(χ))
det(Kn)1/2 e-2∣2yTy+ 2μTK-1μ+ 袤(y-f(x))T(y-f(x))
det(Kn)1∕/
(28)
So the normalized stochastic complexity is
F0(Dn)=-logZn0
=-Llogdet(Kn)1/2 + Logdet(Kn)1/2 +——'yT y — 4μτ K-1 μ-^-(y — f (X))T (y — f (x))
2	n 2	n	2σ2	2 n	2σ2
=-5logdet(K-I + ^^2 I)-1 + 5logdet(Kn) + ʒ-2 yTy —	yT(K-I + —2 I)Ty
2	n	σ2	2	2σ2	2σ4	n σ2
一^12 (y-f(X))T (y - f (X))
2σ2
=2logdet(I+》)+2σ2 yT (I+~n )-1y - 2σ2(y—f (X))T (y -f (X)).
= -logdet(I +	n) + y2 f (X)T (I +	n )-1f (X) + y-2 €T (I +	n )-1€ - y^2 €T e
2	σ2	2σ2	σ2	2σ2	σ2	2σ2
+ y^2 €T (I +	2n )-1f (x)
2σ2 σ2
(29)
After taking the expectation over noises , We get
嘎FO(Dn) = 5logdet(I +	n) + y^2f (x)T(I +	2n)-1f (X)- 5Tr(I-(I +	2n)-1).	(30)
2	σ2	2σ2	σ2	2	σ2
This concludes the proof.	口
C Helper lemmas
Lemma 15. Assume that m → ∞ as n → ∞. Given constants a1, a2, s1, s2 > 0, if s1 > 1 and
s2s3 >s1 -1 , we have that
R
X
i=1
aιi-s1
(l + a2mi-s2 )s3
1-s1
= Θ(m s2 ).
(31)
17
Published as a conference paper at ICLR 2022
Ifs1 > 1 and s2 s3 =s1 - 1, we have that
X (1 + ；2二二)S3 =θ(m-s3 logm).
Ifs1 > 1 and s2 s3 < s1 - 1, we have that
(32)
Overall, ifs1 > 1 andm→∞,
R
X
i=1
R
X
i=1
aιi-s1
(l+a2mi-s2 )s3
aιi-s1
(l + a2mi-s2 )s3
= Θ(m-s3 ).
Θ(mmax{-s3, ⅛1}
1 —s1
Θ(m s2 logm),
),
s2s3 6= s1 - 1,
s2s3=s1-1.
Proof of Lemma 15. First, when s1 > 1 and s2s3 > s1 -1, we have that
R
X
i=1
aιi-s1
(l+a2mi-s2 )s3
a1	a1 x-s1
(l + a2m)s3	J[ι,+∞] (l + a2mx-s2 )s3
aι	+	* /	ai(m⅛厂s1	d X
(I+ a2m产	m 2 7[i,+∞] (I + a2(m/2)-s2产 m1/s2
aι	1-s1 /	aιx-s1
(1 + a2m产	m 2 √[1∕m1∕s2 ,+∞] (1 + a2x-s2 产
1 —s1
= Θ(m s2 ).
(33)
(34)
On the other hand, we have
R
X
i=1
aιi-s1
(1 + a2mi-s2 )s3
)s3
dx
〉/	aιx-s1
一∕l,R+1] (I + a2mx-s2
a1( m⅛ 尸1	d X
(1 + a2 (m⅛7 )-s2 )s3 m1/s2
1-s1 /	aιx-s1
=m s2	d------二一--dx
/[1∕m1∕s2 ,(R+1)/m1/s2 ] (1 + a2x-s2 产
1-s1
= Θ(m s2 ).
Second, when s1 > 1 and s2s3 =s1 - 1, we have that
R
X
i=1
aιi-s1
(1+a2mi-s2 )s3
≤ aι
一(1 + a2m)s3
[1/m1/s2 ,+∞]
a1X-s1
丁-------T-dx
(1+a2X-s2 )s3
aι	1一sι 一 ,一 /1 / -、、
≤ ---------7--+ m s2 O(logm(1/s2))
(1+a2m)s3
1 —s1
= Θ(m s2 logn).
On the other hand, we have
R
X
i=1
a1i-s1
(1 + a2mi-s2 )s3
)s3
dx
〉/	aιx-s1
^√[1,R+1] (1 + a2mx-s2
x -s1
a1( ^s)	d X
(1 + a2 (m⅛ )-s2 产 m1/s2
(
1 — s
+ m ~s
1—sι /	aιx-s1
m s2	------------dx
7[1/m1/s2 ,(R+1)/m1/s2 ] (1 + a2x-s2 产
1 —s1
= Θ(m s2 logn).
18
Published as a conference paper at ICLR 2022
Third, when s1 > 1 and s2s3 < s1 -1, we have that
R
X
i=1
aii-s1
(l+a2mi-s2 )s3
≤ aι
一(1 + a2m)s3
[1/m1/s2 ,+∞]
a1x-s1
dx------χ-dx
(1+a2-s2)s3
1 — s
+ m ~s
≤ W⅛F + m w θgi2)(ji2s吟
= Θ(m-s3).
On the other hand, we have
R
X
i=1
aii-s1
(1 + a2 mi-s2)s3
≤ aι
一(1+a2m)s3
1 — s
+ m ^^s2-
[2/m1/s2,(R+1)/m1/s2]
a1-s1
丁------dχ~~dx
(1+a2-s2)s3
≤ -a1、+mTθ(m(-Vs2)(1-sι+s2s3))
(1+a2m)s3
= Θ(m-s3).
Overall, if s1 > 1,
(
R
X
i=1
aιi-s1
(1 + a2mi-s2 )s3
Θ(mmax{-s3, ⅛l }
Θ(m-s3 logn),
),
s2s3 6= s1 - 1,
s2s3=s1-1.
(35)
□
Lemma 16. Assume that R = m s2+κ for κ > 0. Given constants a1,a2,s1,s2 > 0, if si ≤ 1, we have
that
R a i-s1
X (1 + a2mi-s2)S3 = OmaX{m-s3,R1-s1 }).	(36)
Proof of Lemma 16. First, when s1 ≤ 1 and s2s3 > s1 -1, we have that
a1 i-s1	≤ a1	+ f
i=1 (1 + a2mi-s2)s3 — (l+a2m)s3	√[1,R]
a1x-s1
(1+a2mx-s2)
x
s3
八 a1、+m⅛1 Z .1 a1(m1χ2). -1— d-ɪ
(1+a2m)s3	√[i,R] (1+a2(mχs2)--2)-3 m1/-2
a1
(1+a2m)-3
[1/m1/s2 ,R/m1/s2 ]
a1x--1
dX------ydx
(1+a2x-2)3
a1
(1 + a2m)-3
〜	1-s1 π -l
+O(m 三(S )1--1)
=O(max{m--3 ,R1--1}).
Second, when s1 ≤ 1 and s2s3 ≤ s1 - 1, we have that
R	--1
∑aii -1	V ai
「I (1+a2mi--2)-3 — (1 + a2m)-3
[1/m1/s2 ,R/m1/s2 ]
a1x--1
---------：一dx
(1+a2x-2)3
≤ -——a^-—■+m⅛1 O(m(τ∕-2)(1--1+-2-3) + (—R— )1--1)
≤ (1 + a2m)-3 + I	+ m1/d	)
=O(max{m--3 ,R1--1}).
Overall, if s1 ≤ 1,
R a i--1
X 7-----1一：——7— = O(max{m--3 ,R1--1}).
(1+a2mi--2)-3
(37)
1 — s
+ m ~s
1 —s
+ m ~s2
□
19
Published as a conference paper at ICLR 2022
Lemma 17. Assume that f ∈ L2(Ω,ρ). Consider the random vector f(x) = (f(xι),...,f(xn))T,
where x1,...,xn are drawn i.i.d from ρ. Then with probability of at least 1 -δ1, we have
n
kf(x)k2 = Xf2(Xi) = O(( δ1 + 1)nkfk2),
i=1
Where kfk2=Rχ∈Ωf2 (X)dρ(X).
Proof of Lemma 17. Given a positive number C ≥ kfk22, applying Markov’s inequality we have
P(f2(χ )>C) ≤ ɪ kfk2.
C
Let Abe the event that for all sample inputs (Xi)in=1, f2(Xi) ≤ C. Then
P(A) ≥ 1 -nP(f 2(X )>C) ≥ 1 - ɪ nkf k2.	(38)
C
Define f2(x) = min{f2(x),C}. Then Ef2(X) ≤ Ef2(X) = IIfIl2. So ∣f2(X) - Ef2(X)| ≤
max{C,kf k2} = C Since 0 ≤ f2(x) ≤ C, We have
E(f4(X)) ≤CE(f2(X)) ≤Ckfk2.	(39)
So We have
E∣f2(X)-Ef2(X)|2 ≤E(f4(X)) ≤Ckfk2.	(40)
Applying Bernstein’s inequality, We have
_____________t2____________!
2(nE∣f2(X)-Ef2(X )|2) + C3t )J
_t2_!
2(nCkfk2 + C)/
≤exp I-4max{nC∣∣fk2,⅛t}
Hence, With probability of at least 1 - δ1 /2 We have
3 ~	2 /	2 .. ... 4C	2 1	―
E产(Xi) ≤max∣ √4Clog—nkf k2,ɪɪog丁 )+ nEf2(X)
i=1	1	,	1	1	(41)
≤maχ{ J4C log W n∣∣fk2,4C log δj+ n∣∣fk2∙
When event A happens, f2(xi) = f2(χi) for all sample inputs. According to (38) and (41), with
probability at least 1- Cnkf k2 -δ1∕2,we have
n
P(Xf2(xi) >t+nEf2(X)) ≤exp
i=1
≤exp
nn
Xff 2(xi) = Xf2(Xi) ≤max
i=1	i=1
{* log δi nkfk2,4C log δ
+nkfk22.
Choosing C = δ2- n k f k 2, with probability of at least 1 -δ1 we have
nn
Xf2(Xi) = Xf2(Xi) ≤max
i=1	i=1
{ Jwlog W n2 kfk2，3^nkf k2log W }+nfk2 =。(( δ1 + 1)nkfk2)
□
Lemma 18. Assume that f ∈ L2(Ω,ρ). Consider the random vector f (x) = (f(X1),…，f(Xn))T,
where X1,...,Xn are drawn i.i.dfrom P. Assume that kf k∞ =suPx∈ωf (x) ≤ C. Withprobability of
at least 1 - δ1 , we have
kf (x)k2 = θ(qC 2nkfk2 +C 2)+nkfk2,
where kfk2=Rχ∈Ωf2(X)dP(X).
20
Published as a conference paper at ICLR 2022
Proof of Lemma 18. We have |f2(X) -Ef2(X)| ≤ max{C2,kf k22} = C2 Since 0 ≤ f2(x) ≤ C, we
have
E(f4(X))≤C2E(f2(X))≤C2kfk22.	(42)
So we have
E|f2(X)-Ef2(X)|2 ≤E(f4(X)) ≤C2kfk22.	(43)
Applying Bernstein’s inequality, we have
P(Xf2(xi)>t+nEf2(X))≤exp -
i=1
t2
≤exp -
2(nE|f2(X)-Ef2(X)|2)+
t2
≤exp -
4max{nC2kfk22,
2(nC2kfk22+
t2
Hence, with probability of at least 1-δ1 we have
Xff 2(xi) ≤max{ 4Cc2logδ^nkf k2,4CC-Iog^1} +nEf 2(X)
≤O max CC2n∣f∣2,C2	+ n∣f∣2
(44)
≤O ZC 2nkfk2 + C 2)+nkfk2∙
□
For the proofs in the reminder of this section, the definitions of the relevant quantities are given in
Section 3.
Corollary 19. With probability ofatleast1-δ1, we have
kf>R(x)k2 = O(( δ1 + 1)nR1-2β ).
ProofofCorollary 19. The L2 norm of f>R(x) is giVenbykf>r∣∣2 = P∞=R+1 μp ≤ 2C-1 R1-2β.
Applying Lemma 17 we get the result.	□
Corollary 20. For any ν ∈RR, with probability of at least 1-δ1 we have
kΦRνk2 = O(( δ1 + 1)nkν k2).
Proof of Corollary 20. Let g(x) = PpR=1 νpφp(x). Then ΦRν = g(x). The L2 norm ofg(x) is given
by ∣∣gk2 = Pp=IVp = ∣∣ν k2. Applying Lemma 17 we get the result.	□
Next we consider the quantity, ΦTRΦR-nI. The key tool that we use is the matrix Bernstein inequality
that describes the upper tail ofa sum of independent zero-mean random matrices.
Lemma 21. Let D = diag{d1 ,	... ,	dR},	d1 ,	... ,	dR	> 0 and	dmax = max{d1 ,	... ,	dR}.	Let
M = max{PpR=0 dp2 ∣φp∣2∞ ,d2max}. Then with probability of at least 1 -δ, we have
l∣D(ΦRΦr-nI)D∣2 ≤max]，九煦ɑ乂MlogR,MlogR)].	(45)
21
Published as a conference paper at ICLR 2022
Proof of Lemma 21. Let Yj = (φ1(xj), ... , φR(xj))T and Zj = DYj. It is easy to verify that
E(ZjZjT) =D2. Then the left hand side of (45) is Pjn=1 [Zj ZjT -E(ZjZjT)]. We note that
kZjZjT-E(ZjZjT)k2≤max{kZjZjTk2,kE(ZjZjT)k2}≤max{kZjk22,d2max}.
For kZj k22, we have
RR
kZjk22=Xdp2φ2p(xj)≤Xd2pkφpk2∞,	(46)
p=0	p=0
we have
kZjZjT-E(ZjZjT)k2≤max{PpR=0d2pkφpk2∞,d2max}.
On the other hand,
E[(ZjZjT-E(ZjZjT))2]=E[kZjk22ZjZjT]-(E(ZjZjT))2.
Since
R
E[kZjk22ZjZjT] 4E[Xd2pkφpk2∞ZjZjT],	(by (46))
p=0
R
= Xd2p kφp k2∞E[ZjZjT],
p=0
we have
≤max{PpR=0d2pkφpk2∞d2max,d4max}
≤d2maxmax{PpR=0dp2kφpk2∞,d2max}.
Using the matrix Bernstein inequality (Tropp, 2012, Theorem 6.1), we have
n
P(kX[Zj ZjT - E(Zj ZjT)]k2 > t)
j=1
_____________________-t2______________________
2(nkE[(ZjZT-E(ZjZT))2]k2 + tmajZZT-E(ZjZT)k2)
-t2
2(ndmɑχmax{Pp=odpkφpk∞,dmaχ}+ tmax{pR=0 d∣kφpk∞,dm∙))
-t2
O(max{ndmaχmax{PR=odPkΦp k∞,d^χ} ,tmax{PR=odPkΦpk∞,dmɑχ}})
Then with probability of at least 1 -δ, we have
n
kX[ZjZjT-E(ZjZjT)]k2
j=1
≤ max{ qndmaxmax{pR=0dP kφpk∞,dmax}log r ,max{PR=Odpk φp k∞ ,dLax }lθg R
≤Rexp
≤Rexp
=Rexp
Corollary 22. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
satisfy Assumption 6. Assume σ2 =Θ(nt) where 1 — ι+α2τ <t< 1 Let Y be a positive number such
that
1+a+2τ — (1+2τ +2a)t
2α(1-1)
<γ ≤ 1. Then with probability of at least 1 -δ, we have
k 表(I + σ Λr)-"2Λ^2(ΦRΦr-nI )AR/2(I + σ⅛ Λr )-/2∣∣2
1 ι+α+2τ-(ι+2τ+2α)t 臼 八 /——ʌ
≤ On	2α	-MIT) JlogR .
(47)
22
Published as a conference paper at ICLR 2022
Proof of Corollary 22. Use the same notation as in Lemma 21.
Let D = (I + σ2Ar)-y/2AR/2.
Then d2	≤	σ2γ	and	PR	d2 kφ k2 ≤	PR C2 λγp2	— o((ɪ) 1-γα+2τ )	Where the
Then dmaχ	≤	~nγ	and	l^p=0	dpkφpk∞ ≤	Z=p=C Cφ (1+ _n λT) )γ = O(( σ) α )，Where the
σ2 p
first inequality folloWs from Assumptions 4 and 6 and the last equality from Lemma 15. Then
M=max{PR=OdPk φpk &dlax}=O(( σ2) F+2τ). Applying Lemma21, We have
k σ2(i+σ ΛR l/2ar/2&r ΦR-η)ar/2(i+σ a口尸勺必
≤ σ2 maχ{ qnσ2γ o(( σ2)1-γα+2τ )log RR ,o(( σ) 1-'α+2τ )log R }
=o( σ2 (σ) 1-2γα+2τ n2 )=o( ,log R n (1-2γα+ατ)(1-t)+2 T)
=o(qogR n HT - (1+2τ+2α)t-γ(I)).
(48)
□
Corollary 23. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
satisfy Assumption 6. Let Λι,R = diag{1,λι,…,λR}. Assume σ2 = Θ(nt) where t < 1 Let Y be a
positive number such that 1+02τ <γ ≤ 1. Then with probability ofatleast 1 一 δ, we have
k (I + σΛRi2ΛYR(ΦRΦR-nI)Λγ/R(I + σΛR)-γ/2 k2 ≤ o (qog]Rn1) .	(49)
ProofofCorollary 23. Use the same notation as in Lemma21. Let D = (I + σ2ΛR)-γ∕2Λ：/R. Then
dmax ≤ 1 and PR=OdPkφpk∞ ≤ Cφ + PR=ICφ(l+⅝λp)γ = C2 + o(n (1-γα+'2τ )(1-t) )= O(I) where
the first inequality folloWs from Assumptions 4 and 6 and the second equality from Lemma 15. Then
M=max{PpR=Od2pkφpk2∞,d2max}=o(1). Applying Lemma 21, We have
k(I + σ2 ΛR)-»2Λ∕(ΦRΦR-nI )AR/2(I + σ2 ΛR)-γ∕2k2
≤ maχ{ qlog RnO(I) ,logR O(I)}	(50)
=o (,log Rn 2).
□
Corollary 24. Suppose that the eigenvalues (λp)p≥1 satisfy Assumption 4, and the eigenfunctions
SatiSfyASSUmPtion6. Let ΦR+i:S = (φR+ι(x),...,φs(x)), and AR+上S = (λR+ι,…，λs). Then with
probability ofat least 1 一 δ, we have
MR+i：S(ΦR+i:SΦR+1:S-nI)ΛR+rSk2 ≤o(logS-Rmax{n1R 1—α+τ,R1-α+2τ}).	(51)
ProofofCorollary 24. Use the same notation as in Lemma 21. Let D = ΛR+21.S. Then
dmax ≤CλR-α = o(R-α) and £"+&峻 ≤ PS=R+1CφCλp-αp2τ = O(Rla+2τ), where
the first inequality folloWs from Assumptions 4 and 6. Then M = max{PpS=R+1Cφ2d2pp2τ,d2max}=
o(R1-α+2τ). Applying Lemma 21, we have
k(I + * ΛR)-γ∕2ΛR∕2 (ΦRΦR-nI )Λ∕(I + σ ΛR T-γ/k2
≤ max I ,log S-R nO(R-a)O(R1-a+2，) ,log S-R O(R1-a+2τ))}	(52)
=O (log S-R max{n1R 1-2α2+2τ ,R1-α+2τ }).
□
23
Published as a conference paper at ICLR 2022
Lemma 25. Under the assumptions ofCorollary 24, with probability ofat least 1 -δ, we have
∣∣Φ>rΛ>rΦTRk2 = O(max{nR-α,n1Rɪ 2α+2τ ,R1-α+2τ}).
ProofofLemma 25. For S ∈ N, we have
∞
IIφ>S λ>S φ>s l∣2 ≤ X Mpφp(x)φP(X)TI∣2
P=S+1
∞
=X λpkφp(χ)k2
P=s+1
∞
≤ X λpnCφp2τ
P=s+1
=O(nS1-α+2τ).
Let S = Rα--2τ. Then we get ∣Φ>sΛ>sΦ>sl∣2 = O(nR-α).
Let Φr+1:S = (Φr+i(x),...,Φs (x)), Ar+i：S = (Ar+i,...,As ).Wethenhave
∣∣Φ>R Λ>r Φ>r∣2 ≤∣∣Φ>S Λ>s Φ>s l∣2 + ∣∣Φr+1:S Λr+1:S ΦR+1:S l∣2
≤θ(nR-α) + MR+1:S φR+1:S φR+1:S AR+1：S l∣2
≤o(nR-α)+nMR+1：S ∣∣2 + IIλR+1:S(φR+1:SφR+1:S-nI)λR+1:Sl∣2
R ɑ-ɪ R	ɪ 1-2α + 2τ
≤O(nR-a)+O(nR-a) + O(log --——max(n 2 R —,R1-a+2τ })
δ
=O(max{nR-α,n2Rɪ 22+2τ ,R1-α+2τ}),
where in the fourth inequality We use Corollary 24.	口
Corollary 26. Assume that σ2 = Θ(1). If R = n ɪ+κ where 0 <κ< 第1-^ ,then with probability
ofat least 1 — δ, we have
k(∕ + ①rλRφT )-1φ>RA>"τR J} ≤∣ ①*八^①金 ∣2 = O(n-κα)= θ(1).
σ2	σ2	σ2
ProofofCorollary 26. ByLemma 25 and the assumption R=nɪ+κ, we have
iirrɪ φrλrφR、-1 φ>rλ>rφ>r IlC <^ιι φ>rλ>rφ>r ∣∣
k (十 σ2	)	σ2	112 —k σ2	112
≤O(max{nR-α,n2R 1^2++2τ ,R1-α+2τ})
~
=O(n-κɑ).
□
Lemma27. Assume that ∣σ⅛(I + 枭Ar)-y/2AR/2(ΦRΦr — nI)Λ∕(/ + 备Λr)-^2∣∣2 < 1 where
1+02τ < γ ≤ 1. We then have
(I+σ2 λrφRφr )-1
∞
=(i+σ⅛ AR)T+X(-1)j( σ12(I+σ⅛ AR)TAR ®r^r-nI)y(i+崇 AR )-1.
j=1
ProofofLemma 27. First note that
k σ2 (I + σ⅛ Ar )T∕2aR2(ΦRΦr-nI )aR2(I + σ⅛ Ar)-1∕2∣∣2
< kσ2(I + σ⅛Ar)-t∕2AR∕2(ΦRΦr-nI)aR2(I+ σ⅛Ar)-t∕2∣∣2 < 1.
24
Published as a conference paper at ICLR 2022
Let Ae,R = diag{e,λ1,...,λβ}. Since NR = diag{0,λ1,...,λβ}, We have that when E is sufficiently
small, kσ2(I + σ⅛Ae,R)-1/2A[R(ΦRΦr - nI)Λ 1/R(I + 枭&#)-1/2||2 < 1. Since all diagonal
entries of A6,r are positive, we have
(I+σ12A 3rφrφr)-1
=(I + ~n A e,R + σ12 A 6,r(φRφR -nI ))T
=A 1∕R(I+σ A3R)T∕2 卜+表(I+σ A3R)T∕2A 1∕R(φRΦr-nI )A 1∕R(I+σ Ae,R)T∕2]T
(I + 枭 A3R)T∕2A-R/2
~	-I
=(I + σ A e,R)一
二「一	/	―	―	、j
+ X (-1)jA 1/R(I + σA3R)T∕2 (=(I + σ⅛Ae.R)T∕2A 1∕R(ΦRΦr-nI)A 1∕R(I + 崇Ae,fi尸/2)
j=ι L
(I+σ Ae,R 尸/2A TR/2
=(i+σA3r)t1+x(-i)j (σ12(i+σ⅛A芯厂1 A3r(φRφr-nI)) (i+σAe,r)t1∙
j=i
Letting E → 0,we get
(i+σ2 ar φRφr)t1
=(i+σAR)t1+χ(-ι)j (J2(i+σAR)TAR(φrφr-nI)) (I+σAR)-ι∙
This concludes the proof.	口
Lemma28. If ∣∣(I + 5绊)-1φ>吟塔红心 < 1, thenwehave
(I+
φΛΦt )t1-(i+
∞	j
φrArφR )-1 = ^^(-i)j ((I + φrArφR )-1 Φ>rA›rΦ>r∖ (i + φrArφR )-1
σ2	)	=√( L)	(_£ + σ2	)	σ2	I(I + σ2	)
j=i
(53)
In particular, assume that σ2 = Θ(1). Let R=n 1+κ where 0 <κ< ^(11-¾ ∙ ThenWith probability of
at least 1-δ ,forsufficiently large n, we have ∣∣(I + ①Α：R φR )-1 φ>ra>ir-φ>r 心 < 1 and (53) holds.
ProofofLemma 28. Define Φ>r = (≠r+1(x),≠r+2(x),...), A>r = diag(λR+1,λR+2,...). Then we
have
(I + φΛΦT)-1 - (I+
ΦrΛrΦR
=(i+笔避+
Φ>rΛ>rΦ∙
^σ2
2
>>r
)-1
)-1 -(i+φrΛ丹)-1
I+(I+φrΛ声)
-1Φ>rΛ>rΦTr
σ2
T-I) (I + φrΛRφr )-1∙
By Corollary 26, for sufficiently large n, ∣ (I +
at least 1 -δ. Hence
ΦrΛrΦR)-1Φ>rΛ>rΦ.
^σ2
^σ2
>r ∣∣2 < 1 with probability of
σ
(I+窄T )-1 - (I + 笔避)-1
I +(I+φrΛ声)
-1Φ>rΛ>rΦ>r
σ2
T-I) (I + φrΛ卢)-1
∞
X(-1j(i+φrΛrφr )
j=1
-1Φ>rλ>rΦ>r
σ2
j	T
(I+吗骞)-1∙
□
25
Published as a conference paper at ICLR 2022
Lemma 29. Assume that μo = 0 andσ2 =Θ(nt) where 1 — ^^T <t< L Let R=n(1+K)(IT) where
0 <κ< ɑ-1-2τ+—t+2τ)t ∙ Then when n is sufficiently large, with probability ofat least 1 — 2δ we have
k(I + σ⅛ ΦrΛrΦR)T∕r (x)k2 = O (q( 1 + 1)n∙nmaxL(I), (f(I)}).	(54)
ProofofLemma29. Let Λ±R = diag{λ1, ... , Xr},①上R = (φι(x), φι(x),…，Φr(x))
and μi:R	=	(μι,…，μR).	Since	μo	= 0, We have	(I	+	=ΦrΛrΦR)-1∕r(x)=
(I + σσ12 ΦLRΛLRφ*R)-1ΦLRμLR.Using the Woodbury matrix identity, We have that
(I + σ12 ΦLRΛLRΦT：R)-1ΦLRμi:R = [I — Φ±R(σ2I+ΛlrΦ^rΦlr)-1ΛlrΦfr]①上R 从上 R
=ΦLκμi:R 一 Φ±R(σ2I+八上以①%①上以厂以上以①%①上以从上以
= φ1:R(I + σ12 λlrφTrφ1:R)Tμ±R.
(55)
Let A = (I + 亲Λlr)T∕2Λ[R(①葭①上R — nI)Λ?R(I + *人上R)-1/2.By Corollary 22, with
probability of at least 1 — δ, we have ∣∣ σ2 A∣∣2 = JlogRn1 α+2τ - (1+泞.When n is sufficiently large,
k σ2 A∣2 = o(1) is less than 1 because 1 — #2T <t< 1. By Lemma 27, we have
(I + σ12 λlrφTrφ1:R)T
∞
=(I + σn2λ1:R)-1 + X(-I)j (σ12(I + σ⅛λ1:R)TA1:R &*Rφ1:R - nI(I + σn2λ1:R)-1.
j=1
We then have
k(I + -2 λlrφTR Φ1：R )Tμ±Rk2
σ2
=| ((I + σ⅛λ1:R)-1 +X( —1)j(σ12(I + σ⅛λ1:R)-认1：口(虻Rφi:R-nI))j(I + σ⅛Al:R)-1 j μi:R
∞
2
≤ (k(I + σn2A1：R)-71：Rk2 + 工1 (σ2(I + σn2λ1:R)TALr(φ*rφ1:R —nI))j(I+ σn2λ1:R)TμLR∣∣2
j=1	(56)
By Lemma 15 and Assumption 5, assuming that supi≥1pi+1 —pi =h, we have
k(I + 枭 Ai：r )TμrR∣∣2 ≤ . X	(C p;	=Θ(nm"J), (⅛1-) }logk/2 n),
σ	∖ (^ (1+nCλP α/σ2)2
L hC	C 2 i-2β
k(I + σ⅛Ai:R)TμrR∣2 ≥ t X (1 + 鼻(hi)-α)2=。炉”。-)(1-^}1。产田
where k =[0, 2ɑ = 2β  1, .Overallwehave
[1, 2α = 2β — 1.
1-2β
k(I + σ Ai：R)T〃i：Rk2 = e(n(1-t)max{T，F } logk/2n)
(57)
Using the fact that ∣ σ12 A∣2 = JlogRnα~220~
(1 + 2τ)t
2α
and ∣∣(I + σn2Ai：R)-1Alr∣∣2 ≤n-1,wehave

U (σ12(I + σn2 Ai：R)-IAlr(φ"rφi:R — nI ))j(I + σn2 Alr)-"^1?
-.1	、—i，I」……° ，	、—i，1
=(I + σ2Ai：R) 2 Ai：r(σ2A)'(i + σ2Ai：R) 2 A±Rμi:R	(58)
2
〜.	1-t...r	i 一 i
≤°(n ɪ)kσ2Ak2k(I + σ2Ai：R)	2Ai：RμiR∣2
26
Published as a conference paper at ICLR 2022
By Lemma 16 and the assumption R = n(α+κ)(1-t),
llfI, n λ	厂 2 A-2tl	ll < XX (CλP-α)-1Cμ p-2β
k(I + σ2A1:R) 2 Al：R“1：Rk2 ≤tx (1+nCλp-α∕σ2)i
Np=I	—	(59)
=O(max{n-(I-t)/2,R1/2-e+a/2 })
=O(max{n-(I-t)/2,n( 1 + 1-αβ+κ(I/2—e+a/2D(I)})
We then have
Il (σ12 (I+ σn2λLr)-1λLr(φTrφ1:R -nI))' (I + σ Ai：R)-"i：Rll
2	(60)
=k σ2 A∣j O(max{n-(I),n( ⅛2β+κ(I/2-e+a/2D(I)})
By (56), (57) and (60), we have
k(I + σ12 λlRφ*rφLr) 1μLRk2
∞
=Θ(n(I)max{-1, S }logk/2n) + £k ɪ A∣jO(max{n-(I),n(I) ^a+κ(I)(I/2-e+a/2)})
j=1 σ
=Θ(n(1-t)max{-1,1-αβ }logk/2n) + O(n 1-2+2τ - (1+∣τ)t )(O(max{n-(1-t),n(1-t) 1-αβ+K(1-t)(I/2-e+a/2)}).
(61)
By assumption K < α-1-2T+-t+2τ)t, We have that
κ(1-t)(1∕2-β+α∕2)+1-S+” — (1+τ)t <κα(1 -t)∕2+1-∣+τ - (1+τ)t- <0.
2α	2α	2α	2α
Using (61), We then get
k(I + ⅛ Ai：r Φ*RΦLR)-lμLRk2 = Θ(n(I)max{-1,1-αβ }logk/2n)
1+o(1)	n	-1
=	2- k(I + -2 ALR) μLRk2∙
σ2	σ2
(62)
By Corollary 20, With probability of at least 1 -δ, We have
kφ1:R (I + σ12 ALR φf RφLR)-1μLR∣∣2 =0(J ( 1 + 1)nk(I + * ALRφ^RφLR)-1μLR k2 )
v Z_________________________________________	(63)
=O(J(1 + 1)n ∙n(1-t)max{-1,1-αβ }).
From (55), we get ∣∣(I + σ2ΦlrAlrΦ^r)-®：R〃i：Rk2 =O(J(1 + 1)n ∙ n(1-t)max{-1,j⅛2β}).
This concludes the proof.	□
Lemma 30. Assume that μo > 0 and σ2
〜t∖ ，	1 α	.，- r . ŋ 1 + κ ，
Θ(nt) where 1 一 ι+α2τ <t< L Let R = nα + where
0 <κ< αT-2T+(1+2τ)t. Then when n is sufficiently large, with probability ofatleast 1 — 2δ, we have
k(I + σ2 φrarφR )-1fR(χ)k2 =O(j( 1 + 1)n)∙	(64)
Proof of Lemma 30. Using the Woodbury matrix identity, we have that
(I + σ12 ΦrArΦR)-1∕r(x) = [I — Φr(σ2I+ArΦRΦr)-1ArΦR]Φr4r
=Φr4r —Φr (σ2I+ArΦRΦr )-1ArΦR Φr4r	(65)
=φr(i + σ12 ARφRφR)-1μR∙
Let μR,ι = (μo,0,...,0) and μR,2 = (0,μι,...,μR). Then μR = μR,ι + μR,2. Then we have
k(I + σ12 ARφRφR)-1μRk2 = k(I + σ12 ARφR φR)-1μR,1k2 + k (I + 表 AR φRφR)-1μR,2 k2.
(66)
27
Published as a conference paper at ICLR 2022
According to (62) in the proof of Lemma 29, We have ∣∣(I + 今ARΦRΦκ)-1μR,2∣∣2 =
(O(nmax{-(ι-t'),(It)£ 2㈤}). Next we estimate ∣∣(I + 表 ΛrΦR ΦR)-1μR,1∣∣2.
Let
A=(/+σ Ai：r)-”2a：：R®"r φlr - nI WIRQ+σ ALR)-T/2
where 击(¾2τ - (I+2衰2"") <Y< L SinCe 1 - 3 <t< 1, = (⅛2τ - (I+『*)< ɪ
so the range for Y is well-defined.By Corollary 22, with probability of at least 1 - δ, we have
Il *A∣∣2 = O( JlogRn 1+2+2τ - (1+22+2a)t -T(I-1)) = o(1). When n is sufficiently large, ∣ *A∣2 is
less than 1 because 1 - ɪ+^ <t< 1. By Lemma 27, we have
(I + σ⅛ arφRφr)-1
∞
=(I + * AR)T+X(-1)j (σ⅛ (I + σ⅛ AR)T AR (φRφr -nI))'(I + 备 ar)-1∙
j=i
We then have
Il(I ÷ σ2 ARφR φr )-1μR,ι∣∣2
∣∣ ((I ÷ σn2 AR)T ÷ χ(-1)j (σ12 (I ÷ σn2 AR)T ar (φRφR-nI))”(I ÷ σ⅛ AR)Tj μR,ι
/	∞
2
≤ (I(I+σn ar )-1μR,ι∣2+χ∣∣ (*(I+σn2 AR)TAR (φRφR-nI))"(I+σn AR )-1μR,ι∣∣2
∖	j=1
By Lemma 15,
(67)
ll(I ÷ ^^2 aR )-1μR,1∣∣2 ≤ ʌ μ0 ÷ X 门	尸-α∕ 2、2 =。⑴∙	(68)
σ2	∖	(^ (1÷nCλp α∕σ2)2
T . T	1 ∙ Cl A	A ~∖	t I	/Cl	-I ∖ rɪ-il	A	T	I	T . I >	/ I
Let Ai,r = dιag{1, λι,..., Λr} and I0,R = (0,1,..., 1). Then AR = A1,RI0,R. Let B = (I ÷
σ⅛Ar)-γ∕2AYR(ΦRΦr - nI)A：£(I÷ σ⅛AR)-T/2. According to Corollary 23, we have IlBIl2 =
O (Jlog -Rn1) .Using the fact that ∣ σ⅛ A k 2 = O (Jlog R n 1+2+2τ - (I+2*°" -T(I-1)), we have
∣∣ (σ12 (I÷ σ aR)-1 AR(φRφR -nI))j(I÷ σn2 AR)TμR,i∣L
1	T 1 γ	j-1	T ∣ ∣
=σ¥ (I÷σ⅛Ar)-1+2Ar2(a(I÷σ⅛Ar)-1+taR-T)	B(I÷枭Ar)-1+2mr,i∣∣2
≤ σ12(n(-1+2+(-1+T)(LI))(I-t)O(JΣgψn(jT)(X--T(I))) Jθg∣n1 ∣∣μR,1∣∣2
≤ n(-1+2)(1-t)+2-tO(n "f’T) '广 ∣μR,ι∣2
=O(n-1 + 2(1-t)+
[1-α + 2τ-(1 + 2τ)t](j-1)
2α
).
(69)
Since 击(1++τ - (I+2；产")<γ< 1 and -ɪ ÷ 击(1++τ - (I+2；产")宁 < 0, We can let
Y be a little bit larger than = (1++τ - (I+2；产")and make -ɪ ÷ t (1 -1) < 0 holds. By (67),
(68), (69), we have
II(I ÷ σ12 ARφR φr)-1 μR,11∣2
∞
c∖	/	1 _l Y ∩	»-U [1-a+2τ-(1 + 2T)t](j-1)
≤ O(1)÷ 工 O(n-2 + 2(I-1)+	2a	)
j=ι
≤ O(1)÷o(1) = O(1).
(70)
28
Published as a conference paper at ICLR 2022
According to (66), We have ∣∣(I + 古ΛrΦRΦr)-1μR∣∣2 = O(nmax{-(I),(122 -Be}) + O⑴=
O(1). By Corollary 20, with probability of at least 1-δ, we have
kφR(I + σ12 λrφRφR)TμR∣2 =Olq 1 +1)nk(I + σ12 AR φRφR)-1μR∣2)
=O (q(1+1)n).
From (65), we get ∣(I + ΦRΛRφR)-1fR(x)∣2 = O ( J( 1 + 1)n) .This COnClUdes the proof.	□
Lemma 31. Assume that σ2 = Θ(1). Let R = nα+κ where 0 <κ< α-)-2τ. Assume that μo = 0.
Then when n is sufficiently large, with probability of at least 1 -3δ we have
k(I + φΛΦt )-1fR(x)∣2 = O(q( 1 + 1)n∙nmaxL1, ɪ }).	(71)
Assume that μo > 0. Then when n is sufficiently large, with probability ofatleast 1 — 3δ we have
k(I + φΛΦt )-1fR(x)∣2 = Olq δ + 1)n).	(72)
Proof of Lemma 31. We have
(I + φλΦt )-1fR (x)
=(I + 笔绊)-1 fR(x)+((I + φΛΦt)-1 —(I + φrΛRφr TBfR (x).
When μo = 0, by Lemma 29, with probability of at least 1 — 2δ, we have
k(I + σ1- ΦrΛrΦR )-1fR(x)∣2 = O(q( 1 + 1)n∙nmax{-1, ɪ }).
(73)
Since α-1-2τ < α-1+2Tτ, We apply Lemma 28 and Corollary 26 and get that with probability of at
least 1 — δ, the second term in the right hand side of (73) is estimated as follows:
k ((I + 笔T )-1—(I+φrΛRφr )-1)fR(x)k2
∞	Tj
=kX(—1)j((I + φrΛrφr )-iφ>rΛ>2rφ>r^ (I + φrΛRφr)-1fR(x)k2
j=1
∞j
X∣∣ ((I + φrλrφr)Tφ>rλ>-rφ>r) Il ∣(i + φrλrφr)-1fR(x)∣2
σ2	σ2	2	σ2	R 2
j=1	2
,∞ 〜	〜	/------- r , 1-2βι
=XO(n-jκα)O(J( δ + 1)n∙nmaχ{τ,	})
j=1
=o(q( δδ + 1)n∙ nmax{-1，⅛2β }).
Overall, from (73), we have that with probability 1 —3δ,
k(I + φΛΦT )-1fR(x)∣2 = O(q( 1 + 1)n∙ nmax{-1, ⅛2β }).
When μo > 0, using the same approach and Lemma 30, we can prove that ∣(I + φΛφT)-1 f R (x) 112 =
O(J(1 + 1)n). This concludes the proof.	□
D Proof of the main results
D.1 Proofs related to the asymptotics of the normalized stochastic complexity
Lemma 32. UnderAssumptions 4, 5 and 6, with probability ofat least 1 —2δ we have, we have
∣Tι,R(Dn) —Ti(Dn)I = O(=(nR1-α +n"R1-α+τ + R1-α+2τ))	(74)
29
Published as a conference paper at ICLR 2022
If R = n 1+κ where κ > 0, we have ∣Ti,r(Dn) — TI(Dn)I= o(σ12 n 1 ). Ifwefurther assume that
0 <κ< α-α-2τ, μo = 0 and σ2 = Θ(1), thenfor sufficiently large n with probability ofat least 1 — 4δ
we have
IT2,R(Dn)—T2(Dn)I = O(( ⅛ + 1)nmax{(1+κ) 疗,1+ W + ^2^^ ,~1~κα,1+W-κα}).	(75)
ProofofLemma 32. Define	Φ>r	=	(≠r+1(x),	φR+2(x),…,Φp(x),…)，and	Λ>r
diag(λR+ι,...,λp,...). We then have
∣T1 (Dn) — Ti,r (Dn)| = ɪɪogdet(ʃ + J ΦΛΦt ) — ∣logdet(I + ɪ Φr Λr ΦR)
+ ∣Tr(I +注尸—Tr(I +3R 尸
2	σ2	σ2
As for the first term in the right hand side of (76), We have
1logdet(I +± ΦΛΦt ) — 1logdet(I +± Φr ΛrΦR)
2	σ2	2	σ2
=χlogdet ((I +—2 φrλrφR)-1(i +—2 φrλr φr +—2 φ>r λ>r φTR)
2	σ2	σ2	σ2
=2logdet (I + σ2 (I + σ φrλrφR) 1φ>rλ>rφTR)I
(76)
(77)
1
2
TrIOg (I+σ2 (I+σ φrλrφR )-1φ>rλ>rφTR
Given a concave function h and a matrix B ∈ Rn×n whose eigenvalues Zi,…,Zn are all positive, we
have that
Trh(B)=Pn=Ih(Zi) ≤ nh(1 Pn=IZi) ≤ nh(1 TrB),	(78)
where we used Jensen,s inequality. Using h(x) = log(1+x) in (78), with probability 1 — δ, we have
I 1 logdet(I + = ΦΛΦt ) —1 logdet(I + = ΦrΛr ΦR) ∣
≤ 2log(1 +1 Tr(=(I + φR≠r)-1Φ>rΛ>rΦTr))
≤ 2 log(1+煮 k(I+φRΛσ⅞φI )T∣∣2Tt(φ>rΛ>rΦTr))
≤ 2 log(1+n⅛ PP=R+ιλpkφp(χ)k2) ≤ 2⅛ PP=R+ιλpkφp(χ)k2	(79)
=表 £蒜+1%(。2 (√p2τ n∣∣φp∣∣2 +p2τ )+nkφp 112)
=O( S nP∞=R+iλp+n1/2PZR+iApPT+pZr+iapP2t )
=6] σ2 (nR1-α +n1/2R1-a+T + R1-α+2τ)) = o(表 n1),
where in the second inequality we use the fact that TrAB ≤∣ ∣ A112 TrB when A and B are symmetric
positive definite matrices, and in the last inequality we use Lemma 18.
As for the second term in the right hand side of (76), let A =(I + ①Α：R φR )-1/2. Then we have
2 ∣ Tr(I + 笔E )-i —Tr(I + &口鼻 φR 厂1 ∣
=2 TrAiI —(I+A(φ>RA>鸿R )A)-1"∣A
2	σ2
≤ 2TrlI — (I+A( ®>rA；2R®>r )A)T]
≤ 2(1 — (1 + nTrA(φ>rλ›rφtr )A)-i) ≤ 2(1 — (1 +1 Tr(2丫产))-i)
≤ 2 (1—(1+⅛ P∞=R+1λp MP(X)k2))-1) ≤ 击 P∞:R+1λpkΦp(χ)k2
=θ] σ2 (nR1-α +n1/2R1-a+T + R1-α+2τ)) = o( σ⅛ n1),
30
Published as a conference paper at ICLR 2022
where in the first inequality we use the fact that ∣∣A∣∣2 < 1 and TrABA ≤ ∣∣ A∣∣∣TrB when A and B
are symmetric positive definite matrices, in the second inequality we use h(x) = 1 -1/(1+x) in (78)
and in the last equality we use the last few steps of (79). This concludes the proof of the first statement.
Asfor ∣T2(Dn)-T2,R(Dn)∣,wehave
∣T2(Dn )-T2,R(Dn)∣ = f (X)T (I + *)-f(x)-fR(X)T (I + 厘尸 fR (x)l
'	T 1 l (80)
+ l fR (X)T (I + 军-)-1fR(x)-fR(X)T (I + 笔誉R )-1fR(x) | .
For the first term on the right-hand side of (80), we have
|f (x)T (I+煤T )-1f(x)-fR(X)T (I+煤T )-1fR (x) l
≤ 2 l f>R(X)T (I + 聆IT )-1fR(x)∣ + l f>R(x)τ (I + 聆IT )-1f>R(x) l
≤ 2∣∣f>R(x)∣∣2∣∣(I + 煤T )TfR(x)∣∣2 + kf>R(x)k2k(I + 等)T∣∣2kf>R(x)k2
≤ 2∣∣f>R(x)∣∣2∣∣(I + 煤T )-1fR(x)∣∣2 + kf>R(x)∣∣2.
Applying Corollary 19 and Lemma 31, with probability of at least 1 -4δ, we have
If(X)T (I + 琮T )-1f(x)-fR(X)T (I + 军T )-1fR(x) l
≤ 2O ('(1 + 1)nR1-2β) O(q(δ + 1)n∙nmax{-1，宏}) + O(( 1 + 1)nR1-2β)
=2O ((1 + 1)n1+( 1+k)发+maχ{τ,宏}) +O(( 1 + 1)n1+(⅛+κ)(1-2β))
=2O ((1 + 1)n1+(1+^)ɪ+maχ{-i,⅛∣β}),
where the last equality holds because (1 + K) ɪ-^ < 1-0β when κ> 0.
As for the second term on the right-hand side of (80), according to Lemma 28, Corollary 26 and
Lemma 29, we have
l f R(X)T (I + * )-1fR(x)-fR(X)T (I + φr2 )-1fR(x) l
∞	HT j
=X(-1)jfR(x)T((I + φrλRφR )Tφ>rλ>rφ>R) (I + φrλRφR )-1 fR(x)
σ2	σ2	σ2
j=i
≤ X k(I+φrΛrφr )-%t ∙k φ>rΛ>rφTR k2 ∙k(I+φrΛrφr )-1fR(x)k2	(81)
j=i
=X O(n-jKa)O((1 + 1)n1+maχ{-2,10β })
j=ι
=O(( 1+1)nι+maχ{-2,1-F }-κα).
By (80), we have
∣T2(Dn)-T2,R(Dn )∣ = O(( 1 + 1)n1+(1+K)发+maχ{-1, ⅛αβ }) +O(( 1 +1)n1+maχ{-2,号}-κα
〃1	∖i-2β j1-2β (1-2β)κ	j1-2β
=o( (1 + 1)nmaχ{(α+k) 2 ,1++-2- ,-1-κα,1+-κa}).
This concludes the proof of the second statement.	□
In Lemma 32, we gave a bound for ∣T2,κ(Dn) - T2(Dn)∣ when n1 <R<n1 + 0 =2 2丁. For R>n,
we note the following lemma:
31
Published as a conference paper at ICLR 2022
Lemma 33. Let R = nc and σ2 = nt. Assume that C ≥ 1 and C(1 — α + 2τ) — t < 0. Under
Assumptions 4, 5 and 6,for sufficiently large n and with probability ofat least 1 — 3δ we have
|T2,R(Dn)—T2(Dn)∣= O(( 1 + 1)去 nRmax(1∕2-β,1-α+2τ }).	(82)
ProofofLemma 33. Define Φ>r = (φκ+ι(x), φR+2(x), ... , φp(x),…)，and Λ>r
diag(λβ+ι,...,λp,...). Then we have
∣T2 (Dn) — T2,R (Dn)I =
f(X)T(I +φAφ )-1f(X)—fR(X)T(I +φAφ )TfR(χ)
σ2	σ2
+ fR (X)T (I+φλφ- THR(X)—f R(X)T (I +φrλR φr )-1fR(χ)
σ2	σ2
(83)
For the first term on the right-hand side of (83), with probability 1 — 36 We have
f (x)T (I +φλ∣- )Tf(X)—fR(X)T (I +φλ∣- 11fR(X)
σ2	σ2
≤2 f>R(X)T(I+
φλφt
σ2
)1fR(X) + f>R(X)T (I+
ΦΛΦt∖ I ,、
丁) f>R(X)
φλφt	1	φλφt	1
≤2∣∣f>R(X)∣∣2k(I +—τ- )-1 k2kfR(X)k2 + kf>R(X)∣∣2∣∣(I +—τ- )-1k2kf>R (X)k2
σ2	σ2
≤2∣∣f>R(X)∣∣2kfR(X)k2 + kf>R(X)k2
≤2θ(j(1 + 1)nR1-2β	(1 + 1)n ∙f |图 + <5((1 + 1)nR1-2β)
∖ V δ	δ	δ
=O ((； + 1)nR1∕2-β ),
where we used Corollary 19 and Lemma 17 for the last inequality.
1 — α + 2τ
The assumption C(1 — α + 2τ) — t < 0 means that R^——=o(1). For the second term on the
right-hand side of (83), by Lemmas 28 and 25, we have
Hr(x)t (I +φλφ- )-1Hr(x) —Hr(x)t (I +φRAR 玛)-1fR (X)
σ2	σ2
=X( —1)jfR(X)T ((I +φrλRφr)-1 φ>rλ>rφTR)j(I +φrλRφr)-1fR(x)
j=ι	σ
≤xx k(I +φR λRφr )-1 k2+1 ∙k φ>rλ>rφTR k2 ∙kfR(x)k2	(84)
σ2	σ2
j=i
二 1	∙,τ ,c 、〜1	C
=EO(σ2 Rj(I-α+2τ))O((/1)nkfk2)
j=i
=O((g + 1)3 nR1-α+2τ).
δ σ2
Using (83), we have
∣T2(Dn) —T2,R(Dn)∣ = O(( 1 + 1)nR1/2-e) +0((： + 1)nJ2 R1-α+2τ)
=θ( (1 + 1)n ɪ Rmaχ{1∕2-β,1-α+2τ}).
∖ 0	σ2	J
□
Next we consider the asympototics of Tι,κ(Dn) and T2,R(Dn).
32
Published as a conference paper at ICLR 2022
Lemma 34. Let A = (I + 枭AR)-T∕2AR,2(ΦRΦr — nI)AR/2(I + 枭AR)-γ/2∙ Assume that
∣∣A∣∣2 < 1 where 1+2L <Y ≤ L Then we have
T2,R(Dn)= 2n2 MR(I + Tn1 AR)-7r + 2P∞=1(-1)j+1Ej,
where
Ej = μR⅛^(I + 备 ar)-1(φRφR —nI)(T12(I + 枭 AR )-1Ar(φRφR — nI))"	(i+ 备 AR 厂 1μR∙
ProofofLemma 34. Let Ae,R = diag{e,λι, ...,Λr}. Since AR = diag{0,λι,…，Xr}, We have that
when E is sufficiently small, k表(I + 备A£；r)-1/2A 1R(ΦRΦr — nI)A 1R(I + 备Ae,R)-1∕2∣∣2 < 1.
Since all diagonal entries of Ae,R are positive, we have
2^2 μRφR(1 + σ ① RA gR φR)-1φR μR
=2σ2 μRφR [1 一φR(σ21 + Ae,RφRφR厂IAe,Rφβ] φRμR
=2^^2 μRφRφRμR - 2^^2μRφRφR(σ21 + Ae,RφRφR)-1 Ae,RφRφRμR
2σ	2σ	(85)
=2 MR φRφR(σ2I + A e,RφRφR )-1μR
=1 MR A-R A e,RφRφR (σ2I+A e,RφRφR)-1MR
=2 MR A-R μR - 2 MRA-R(I + σ Ae,RφRφR )-1μR∙
Using Lemma 27, we have
,μRA-1 μR — LμRA-1 (I +3A eRΦRΦR )-1μR
2 LR e,RL∙rt 2 LR e,R∖	1 σ2 e,c R C) LC
=2 μRA-R μR - 2 μRA-R (i+σ A e,R )-1μR
+ 2 X( —1j + 1μRA-R ( σ (I + σ Ae,R )-1Ae,R(φRφR - nI)) '(I + σ A e,R)-1μR
n T∕τn	1
=彳 μR(I + σ Ae,R) μR
+ 2X( —1j + 1μR -2 (I + σ Ae,R)-1(φRφR -nI)(σ (I + σ Ae,R)-1 Ae,R(φRφR 一
j=1
(I + σ A e,R )-1μR
Letting E → 0,we get
T2,R(Dn) = ʒ-2 μRφR(1 + —2 φRARφR)-1 φRμR
2σ2	σ2
n Tzr. nʌ 、一1
=彳 μR(I + m Ar) μR
j-1
(86)
C∞ I-	[
X ( —1)j+1μR — (i+」2
σσ	σ2	σ2
j=1 L
(i +—2 AR)-1μR
σ2
This concludes the proof.
Ar)-1(ΦRΦr — nI) ( J_(i + *Ar)-1Ar(ΦRΦr — nI)^
1
+ 2
∞
□
Lemma 35. Assume that σ2 =Θ(1). Let R = n α+κ where 0 <κ< ^^^丁 ∙ UnderAssumptions 4,
5 and 6, with probability ofat least 1 一 δ, we have
ti,r(Dn) = (1 IOgdet(I + σn2 AR) — 2Tr(I —(i + 亲AR)-1)) (1+。(I)) = θ(nɑ )∙	(87)
33
Published as a conference paper at ICLR 2022
Furthermore, ifwe assume μo = 0, we have
T2,R (Dn) =(赤 μR(I + σ2 ΛR)-1μR)(1+o(1)) = (θ(nmax{0,1+"-rβ}), α = 2β-1,	(88)
Θ(logn),	α=2β-1.
Proof of Lemma 35. Let
A =(I + σ Ar)-y/2AR/2 (ΦRφR-nI )ΛR∕2(I + σn2 Λr )-γ/2,	(89)
where 1+α+2τ <γ ≤ 1. By Corollary 22, with probability of at least 1 - δ,we have
Il . II	- ,	1-2Ya + α + 2τ
kAk2 = Qn 2ɑ ).
(90)
When n is Sufficientlylarge, ∣∣Ak 2 is less than 1. Let B = (I + 言Λr)T∕2λ∕(ΦRΦr -nI)Λ∕(I+
f Λr)-1/2. Then ∣∣Bk2 =嗫1：) kA∣∣2 = O(n 1-α+2τ). Using the Woodbury matrix identity, we
compute T1,R(Dn) as follows:
T1,R(Dn) = 2Iogdet(I+ σ12 λrφRφR) - 2TrφR(σ2I + λrφRφr) 1ArφR
=2 ιogdet(I+σn2 λr)+2 Iogdet[I+σ12 (I+σ AR)-"2λZ^φRφr -nI )λ∕ (I+σ AR)-1/2]
-	2 Tr(σ2I+ΛΦRΦr)-1ΛΦRΦr
=I Iogdet(I+ f Ar) + 2 Trlog[I +	B]- ɪ Tr(I - σ2(σ2I+ΛΦRΦr)-1))
∞
=2 Iogdet(I + W Ar) + 2 TrX (-jj-1 (ɪ By
σ	j=1
-	1Tr (I -(I+σn2 AR )-1+x(-ι)j (σ2 (I+σn2 AR )-1ar(φr φR-nI ))j (I+σ AR)Tj
∞
=(2 logdet(I + σn2 Ar )-2 Tr(I -(I + 备 Ar)-1)) + ɪ TrX (-jj-ʌ (ɪ By
j=1
-	1 Tr(X (-1)jσ⅛ (I + 崇 AR)-1∕2Bj (I + σ2 Ar)-" j,
y=1	(91)
where in the last equality we apply Lemma 27.
Let h(x) = log(1 + x) - (1 - ɪ+X). It is easy to verify that h(x) is increasing on [0,+∞). As for the
first term on the right hand side of (91), we have
2 Iogdet(I+σn ar )- 2 Tr(I-(I+σn ar )1)
R
2 X (log(1 + σ2 λp)-(1 - 1+i2 λp))
p=1
RR
2 Xh( σ2 λp) ≤ 2 Xh( σ2 Cλp-α)
p=1	p=1
≤2h(σCλ) + 2 [	h(σCλx-α)dx
[1,R]
=2h( WCλ) + 2n1/a [	h(Cλx-α)dx
σ	J[1/n1/a,R/n1/a]
=Θ(n1∕ɑ),
34
Published as a conference paper at ICLR 2022
where in the last equality we use the fact that [0,+∞]h(x-α)dx < ∞. On the other hand, we have
2 ιogdet(I+σn2 AR)-2 Tr(I-(I+σn2 AR) 1)
RR
2 Xh( σ⅛ λp) ≥ 1 Xh( σ⅛ Cλp-a)
p=1	p=1
≥1 [	h(σn2Cλx α)dx
41,R+1]	一
=1 n1/a /	h(σ12 Cλx-α)dx
41∕n1∕α,(R+1)∕n1∕α]	σ —
=Θ(nf.
Overall, Wehave 2logdet(I + 言Λr) - 2Tr(I-(I + 言Ar)-1) =Θ(n^α).
As for the second term on the right hand side of (91), we have
∞	∞	∞
T⅛X (j1 (σ12 Bj ≤ RXk σ12 Bk2=RX σ⅛ O(n j(⅛+τ))
j=1	j=1	j=1
〜	1 —α + 2τ	〜	1 ,	, 1 —α + 2τ
RO(n	2α ) = O(n α+κ+	).
As for the third term on the right hand side of (91), We have
(∞
X(-1)jσ⅛ (I + 枭 ARL 2 Bi (I + 枭 AR)-1/2
j=1
∞
≤X∣Tr(σ1j(I + σAr)—1/2BB(I + 枭Ar)-1/2) |
j=1
∞
≤RX∣∣ σ1j (I + 言 Ar)-1/2Bj(I + σ Ar )-1∕2∣∣
j=1
∞
≤RX∣∣ 为(I + σ Ar)-1/2Bj(I + 枭 Ar )-1∕[
j=1
∞	1	1-α+2τ
≤RX∖∖ σ1jBjl = O(nα+κ+^^~).
j=1
2
2
Then the asymptotics ofT1,R(Dn) is given by
1	1-α+2τ	1	1-α+2τ
T1,R(Dn)=2logdet(I + σn2 AR)-2 Tr(I - (I + σn2 AR )	)+O(n ɑ	R 2α	) + θ(n ɑ	R 2α )
=Θ(n1∕α) + O(n 1+κ+1-α+2τ)
1
=Θ(n a ),
Where in the last inequality We use the assumption that κ < α-2-2τ. Since O(n 1+κ+ 1 α+2τ ) is loWer
order term compared to Θ(n 1), we further have
T1,R(Dn)= (2logdet(I + σn2AR)- 2Tr(I-(I + σn2AR)-1)) (1 + o(1)).
This concludes the proof of the first statement.
Let A1:R = diag{λ1,...,λR}, Φ1:R = (φ1(x),φ1(x),...,φR(x)) and μ1:R = (μ1,…,mr). Sinceμo = 0,
we have T2,R(Dn ) = 2σ2 μT-RφT-R(I + σ12 φ1: RA LRφTR)-IΦ±Rμ1:R. According to Lemma 34, we
35
Published as a conference paper at ICLR 2022
have
T2,β(Dn) = 2σ2 此R(I + 或λ1:R) 1μ1:R
+2 X(—i)j+1 MfR σ(I+σ λi:R)-I (^T：R^i：R -nI) (σ(I+σ 八上R )-1八1：“(①^R ①上 R- nI))
n T/ ..nA 、一1
=2σ2 μi:R(I+σ八1：R)	μ∣∙R
-∞j-1
+2X (-1)j+ σjμi:R(I+σA】：R厂+γ λi:R A((I+σ八上R)- +γz：RA)
j=i l
1
(I + ^^2 λ1:R )-1+γ/2 A-R2μi：R
σ2
(92)
where in the second to last equality we used the definition of A (89). As for the first term on the right
hand side of (92), by Lemma 15, Assumption 4 and Assumption 5, we have
R
n T /r . n A 、一1	JnL
万见：R(I++σA1：R)	μιR ≤ 或工
p=1
。犷2产	J
1+% CλP-α = 1
Θ(nmaχ{0,1+1-02β }), α = 2β-1,
Θ(logn),
α = 2β — 1.
On the other hand, by Assumption 5, assuming that supi≥1pi+1 -Pi = h, We have
cμ p72β
n ..T ，r , n a ∖-1..	、n V-
取μl:R(I + σALR)	μ1:R ≥ 2σ2X 1 + %Cλp-α
、n X 七
≥ 2σ2⅛ 1 + 备Cλ(hi)
-α
Θ(nmaχ{0,1+1-02β }), Q = 2β-1,
Θ(logn),
Q = 2β — 1.
Overall, we have
SμT:R(I + 与A[R)-1μi:R = Θ(nmax<0,1+守}logkn), where k =" Q = 2β-j
2σ2	σ2	11, Q = 2β -1.
By Lemma 16, we have
k(I+σ A1：R)--*：Rk2 ≤X Ci+；C
=O(max{n-2+γ ,R1-2β+αγ })
= O(nmax{-2+γ, 1-02β +γ+κ(1-2β+αγ)}).
Using (90), the second term on the right hand side of (92) is computed as follows:
-∞j-1
2X (-1)j+ ~2j μ1：R(I+σA1：R)- +γ A1：R A((I+σA1：R)- +γAi：RA)
j=i L
(93)
(I + σ A1：R)-1+y/2 A-R/2μ1: R
≤2Xσ?Mkj(σn2)(	7)°)k(I+σn2A1：r)-1+y/2A-R/2如R核
j=1
≤ 1X ɪ O(n j(1-2Yor+2τ) )( W ) (T+Y)(jT)O(nmax{-2+Y,审 +γ+κ(1-2β+αγ)})
2j=1σ	σ
=O(nmax{-2+Y+ 1-2γα+α+2τ ,⅛2β +γ+ 1-2γα+α+2τ +κ(1-2β+αγ)})
=O(nmax{-2+ i⅛+2τ ,宁 + i⅛+2τ +κ(1-2β+αγ)}).
(94)
L R J
{
36
Published as a conference paper at ICLR 2022
Since用产 < 捽特■=	=1, we have -2+ 1+α+2τ < 0.Also we have	
	U + H≠τ + κ(1-2β+QY) Q	2Q = 1-2β + 1+1-Q+2τ +κ(1-2β+Qγ) Q	2Q 1-2β	1 -Q+2τ	(95)
	≤	- + H			+KQγ Q	2Q	
	< U+1, Q	
where the last inequality holds because K < αW-2τ and Y ≤ 1. Hence we have
T2,R(Dn)=言 〃Tr(I + 鼻 Λlr )t〃lr + O(nmaχ{-2+1，宁+1+α+τ+κ(-2β+α,)})
2σ2	:	σ2
二Θ(nmaχ{0,1+1-2β }logkn)+O(nmaχ{-2+ 1+α+2τ，1-2β + 1+2+2τ +κ(1-2β+αγ)))
=Θ(nmaχ{0,1+1-2β }[ogkn).
where k = ∕θ, Q* β- 1). Since (5(nmax{-2+ 1+2+2τ, 1-2β+ 1+2+2τ +κ(1-2β+αγ)}) is lower order
[1, Q = 2β-1.	'	，
term compared to Θ(nmax{0,1+1-2β)logkn), we further have
t2,r(Dn) = ( 2σ2 μT R(I + σ λ1: R 厂 1μ1: R) (1+。⑴)
This concludes the proof of the second statement.	口
Lemma 36. UnderAssumptions 4, 5 and 6, with probability ofat least 1 —5δ, we have
TI(Dn )= Qlogdeta + σ Λ)- ITr(I-(I + σ A)-1)}1 + o(1))=Θ(n1),	(96)
Furthermore, let δ = n-q where 0 ≤ q< min( (26-1)4：-1-2丁),α-1―2τ }. Ifwe assume μo = 0, we have
T2(Dn)=(获 d (I + q Λ)-i”)(1 + o(1)) = (『aS 厘))，QU U ⑺
∖2σ2	σ2	/	I Θ(logn),	Q = 2β-1.
ProofofLemma 36. Let R=n 1+κ where 0 ≤ κ< α-2θ22τ. By Lemmas 32 and 35, with probability
of at least 1 -5δ we have
∣Tι,R(Dn)-Tι(Dn) | = O(n1+κ(1-α)),	(98)
and
∣T2,R(Dn)-T2(Dn)∣= θ(( 1 + 1)nmaχ{(1+K) 1-2β ,1+ 1—2β + (1—2βκ ,τ-κα,1+-κα] (99)
as well as
ti,r(Dn)=(^logdet(1+σAR)-2Tr(I-(I+σλr)-1))(1+。(I))=θsH),	(10O)
and
T2 R(Dn)= (ʌμT(I + 与A)-1μ)(1 + o(1)) = (θ(nmax<0,1+T}), Q = 2β-1,	(101)
2R n, ∖2σ2 μ + + σ2 ,"八 + ' "	[θ(logn),	Q = 2β-1.()
We then have
TI(Dn)= Tι,R(Dn)+Tι,R(Dn)-TI(Dn) = Θ(n1) + O(n1+κ(Ir)) = Θ(n1 ).
Since O(n1+κ(1-α)) is lower order term compared to Θ(n1), we further have
TI(Dn)= Qlogdet(I + WΛr)-ITr(I-(I + *Λr)-1V∣ (1 + o(1))=Θ(n1)
37
Published as a conference paper at ICLR 2022
Besides, We have
logdet(I + 二 Λ)- logdet(I + 二 Λr)
σ2	σ2
∞	∞	∞
X log(1+滔λp) ≤ 滔 X λp ≤ σ X "2=σO(RIr)
p=R+1	p=R+1	p=R+1
=M O(n(1-α)(1+K))
=o(n 1).
Then We have log det(I + 版Λr) = log det(I + 氏Λ)(1 + o(1)). Similarly We can prove
Tr (ʃ - (I + σ⅛ Λ)-1) = Tr (ʃ - (I + 枭 Λr )-1)(1 + o(1)). This concludes the proof of the first
statement.
As for T2 (Dn), we have
T2 (Dn)= T2,R (Dn)+ T2,R (Dn)- T2 (Dn)
= Θ(nmax{0,1+ 1-≡2β}logkn) + O ((； + 1)nmax{(1+κ) 1-2β，1+1—2β+(IUe)K ,-1-κα,1+ 1-02β-κα}
= Θ(nmaχ{0,1+ 1-02β }logk n) + O (nq+maχ{(1+κ) 1-2β，1+1-2β + (I-2β)κ ,-1-κα,1+1-≡2β-κ0})
{?:
where we use δ = n-q, k =
Q = 2β — 1,
α = 2β — 1.
Since 0 V κ< α-1-2τ and 0 V q< min{ (2"-I)Ja-1-2τ), α-1-2τ }, We can choose K < α-1-2τ and K
—	2a2	— "1	i	4a2	, 2a J ,	2a2
is arbitrarily close to 三万「 such that 0 ≤ q< min{(2β-I)K ,κα}. Then we have (1 +κ) 1-2β + q< 0,
-1-κα+q<0, (1-2β)κ +q<0and-κα+q<0. Sowehave
T2,R (Dn) = Θ(nmaχ{0,1+厘 >logk n).
Since O(( 1 + 1)nmax{(1+K) 1-2β，1+1-≡2β + (Iue)K ,-i-καJ+1-≡2β
is lower order term
compared to Θ(nmax{0,1+ 1-2β)logkn), we further have
T2 (Dn)= T2,R(Dn)(I+o(I))= (2^μR(I + mAR)-'μQ (1+0(I)).
Furthermore, we have
Tl n 1 τ n ι ι
林 (I + ^^2 Λ) μ -μR(I + ~λΛR) μR
σ2	σ2
∞	2	∞	∞
=X π⅛) ≤ X μp≤σ X Cp-2β=O(Rj)
p=R+1 (I+ σ2 入P)	p=R+1	σ p=R+1
=O(n(1-2e)( 1+K))
=o(n 1-02β).
Then we have μτ(I + σ⅛Λ)-1μ = μR(I + 备Λr)-14r(1 + o(1)). This concludes the proof of the
second statement.	□
ProofofTheorem 7. Using Lemma 36 and noting that 1 > 0, with probability of at least 1 - 5J, we have
EeF0(Dn)= T1(Dn)+ T2(Dn)
=∙∣logdet(1 + q AR)- : Tr(I - (I + ~λ AR)T)
2	σ2	2	σ2
+2σΛμR(∕+σAR)-1μR(1+o(I))
= Θ(nmax{1，1Oe+1})
Letting δ = 5δ, we get the result.
□
38
Published as a conference paper at ICLR 2022
In the case of μo > 0, We have the following lemma:
Lemma 37. Assume that σ2 = Θ(1). Let R = n 1+κ where 0 <κ< α-)-2T. Assume that μo > 0.
Under Assumptions 4,5 and 6, for sufficiently large n with probability ofat least 1 -4δ we have
∖T2R(Dn)-T2(Dn )| = O((； + 1)nmax<1+(1+K)厘，1-κα}),	(102)
ProofofLemma 37. Asfor ∣T2(Dn) -T2,R(Dn)∖, we have
1T )-1f (x)-fR (X)T (i+* )-1fR(x)
∣T2(Dn)-T2,R(Dn)∣= f (x)τ (I+
+fR(X)T (I+φσM)-ifR (X)-fR (X)T (I+
ΦrΛrΦR
σ2
For the first term on the right-hand side of (103), we have
f (X)T (I +φAφ )-1f (X)-fR(X)T (I +φAφ )-1fR(X)
σ2	σ2
)-1fR(X).
(i03)
≤2 f>R(X)T(I+
ΦΛΦt
σ2
)-1fR(x) + f>R(X)T(I+
φλφt -i
k)-1f>R(X)
≤2∣∣f>R(x)∣∣2k(I+
≤2kf>R(X)k2k(I+
φλφt
σ2
φλφt
σ2
1	φλφt -i
)TfR(x)k2 + kf>R(x)k2k(I +h )Tk2kf>R (X)k2
)-1fR(x)∣∣2 + kf>R(x)k2.
Applying Corollary 19 and Lemma 31, with probability of at least 1 -4δ, we have
f (X)T(I+驾告一)-1f(x)-fR(X)T (I +"M)-1fR(X)
σ2	σ2
≤2O)( J(1 + 1)nRi-2β	(1 + 1)n) + O>((1 + 1)nR1-2β)
∖ V δ	δ	δ
=2(9^(∣ + 1)n1+( α+κ) 1-2β )+O((g + 1)n1+( α+κ)(-2β))
=2O)Γ(∣ + 1)n1+( α+κ) T ).
As for the second term on the right-hand side of (80), according to Lemma 28, Corollary 26 and
Lemma 30, we have
fR(x)T (I +φλφt )-1fR(x)-fR(X)T (I +φRARφR )-1fR(x)
σ2	σ2
=X(-1)jfR(X)T ((I +φrλRφr)-1 φ>rλ>rφTR)j(I +φrλRφr)-1fR(x)
σ σ σ^
≤xx k(i +φRAflR )T∣∣2-1 ∙k φ>rλ>RφTR k2 ∙k(i+φRAfiR )-1fR (X)k2	(104)
σ2	σ2	σ2
j=i
∞	1
=EO(n-jκ))O((δ + 1)n)
j=i
1
=<9((- + 1)n1-κα).
δ
39
Published as a conference paper at ICLR 2022
By (80), we have
∣T2(Dn)-T2,R (Dn)I = θ(( 1 + 1)n1+(1+κ) 1-2β) +O(( 1 + 1)n1-κα)
=O ((1 + i)nmax{1+(I+K) 1-22β,1-Ka}).
□
Lemma 38. Assume that σ2 = Θ(1). Let R = n 1+κ where 0 <κ< min{ α^2θ-2τ, 21- 1}∙ Assume
that μo > 0. UnderAssumptions 4, 5 and 6, with probability ofat least 1 一 δ, we have
T2,R(Dn) =六 μ2 + O(nmax{ ⅛7αα+2τ，1+ T }).
2σ2
(105)
Proof of Lemma 38. Let
A =(I + * ΛR)-γ∕2ΛR∕2 (ΦRΦr -nI )AR/2(I + * Λr T-γ/,	(106)
where 1+α+2τ <γ ≤ 1. By Corollary 22, with probability of at least 1 一 δ,we have
1-2γ α+α+2τ
∣∣A∣∣2 = O(n -2α-).	(107)
When n is sufficiently large, ∣∣Ak2 is less than 1. Let μR,1 = (μ0,0,...,0) and μR,2 = (0,μ1,...,μR).
Then μR = μR,1 + μR,2. LetΛ1,R = diag{1,λ1,...,λR} andIo,r = (0,1,...,1). Then Λr =Λi,rIo,r.
Let B = (I + 枭Λr)-1∕2Λ1∕R(ΦRΦr - nI)Λ1∕R(I + σ⅛Λr)-1/2. By Corollary 23, we have
k Bk 2 = O( JlogRn2). By Lemma 34, we have
T2,R(Dn) = 2σ2 μR (I + σ2 AR)-1μR
+2χ (-1)j+lμRσ2(I+σλr)-i(φrφr-nI)(σ~(I+σAR)-IAR(φRφR-nI))
2j=1	σ σ	σ σ
(I H—2 λR )-1μR
σ2
(108)
As for the first term on the right hand side of (108), by Lemma 15, we have
R
nT n 1 n 2
取μ (I+σ2A) μ ≤ 2σ2 lμ0+∑
p=1
We define Q1,j, Q2,j and Q3,j by
Q1,j = μR,1-2 (I + ^^2 AR)-1(φRφR - nI)( —2 (I + ^^2 AR)-I AR (φRφR -nI)
, σ2 σ2	σ2 σ2
(I +—2 ar )-1μR,1
σ2
Q2,j = μR,1-2 (I + ^^2 AR)-1(φRφR -nI) (^^2 (I + ^^2 AR)-I aR(φRφR -nI)
, σ2 σ2	σ2 σ2
(I +--2 Ar )-1μR,2
σ2
Q3,j=μR,2 σ (I+σ AR)-1(φRφr - nI) (σ (I+σ AR)-I AR (φRφR -nI)
(I +■—2 ar )-1μR,2
σ2
j-1
j-1
(109)
j-1
40
Published as a conference paper at ICLR 2022
The quantity Q3,j actually shows UP in the case of μo = 0 in the proof of Lemma 35. By (92), (94)
and (95), we have that
∞∞
∣X(-1)j+1Q3,j I = ∣X(-1)j+1O(n(i”2αα+2τ) )o(nmax{0,1+ 1-α2β })| = o(nmax{0,1+1-02β }).
j=1	j=1
(110)
For Q1,j, we have
Q1,1 = ^^2j μR, 1(I + ~λλr)-1+ 2 B(I + —2λr)-1+ 2 μR,1
σ 2j	, σ 2	σ 2
≤ σ2j kμR,ιkik (I+σ AR)-1+2 llikBk2
=O(JlogRn2),
where in the last equality We use ∣∣B∣∣2 = O(JlogRn2). For j ≥ 2, we have
Qι,j=志μR,ι(I+σλr)-1+2B((I+σAR)T+yλR γA)	(I+σAR)-1+γARY
B(I + -2 Λr)-1+2 μR,ι
σi
≤-j k”R,ιkik(I+占 Ar)-1+2kikBkikAki-ik(I+W Ar)-1+y aR-YkiT
σσ	σ
R
=O (log—n∙n
δ
j-2)0-2αα+α+2τ) ∙n-(i-γ)(j-i))
R
=O (log-nγ ∙n
δ
j-2)(1-a + 2τ )
2α	).
Then we have
∣X(-l)j+1Q1,j ∣≤ O(Jlog R n 2) + Xθ(log RnY ∙n j-2κ2≡ɑ+2τl ) = O(log RnY)	(111)
j=1	δ	j=i δ	δ
For Qi,j, we have
Qij=焉μR,ι(I+σAR)-1+2B((I+σAR)T+γAR-A)	(I+σA)-1+2A-RμR,i
≤ σ1jIMikikBkikAkLKI + σAr)-1+yAR-IlLKI + σA)-1+2A-RμR,iki
〜 RR I	j-1)(1-α + 2τ)	n 1+ 工 N- 2	..
=O(Vlog Wn 2 ∙n	2α	)k(I+σA) 1+ 2 A 1,R μR,i∣i.
〜 Y
Since ∣∣(I + 枭 A)-1+2 A 1 R μR,i∣∣i is actually the case of μo = 0, we can use (93) in the proof of
Lemma 35 and get
k(I+F A)-1+2 A-R μR,i∣ιi=ι∣(I+—iA1：R)-1+Y/i A-R/iμ1:Rki
σi	,	σi
=O(nmax{-i+Y, 1-α2β +γ+κ(1-iβ+αγ)}
=O(nmax{-i+Y, 1-α2β +γ+κ(1-iβ+αγ)})
=o(nY),
(112)
where in the last equality we use K < 2Ot1 . Then we have
X (-1)j+1Qi,j ∣≤ X o(ʌ/logf n 牛∙n j-1)-) ) = o(n 中)(113)
j=1	j=1	δ	δ
41
Published as a conference paper at ICLR 2022
Choosing Y = ɪ (1+ 1+α+2τ) = 1+3θ+2T < 1, we have
∞
T2,R(Dn) = 2σ μR (I + σ AR 厂 1μR + X(-I) j + 1 (QIj +Q2,j +Q3,j )
j = 1
=" μ0+<5(nmax{0,1+ 1-ɑ2β }) + o(nmaχ{0,1+ 1-ɑ2β })+O(log RnY ) + o( Jlog Rn 1+γ)
2σ2	δ	Vo
=工倡+O(nmaχ{ *，1+	})
2σ2
=工届 + O(nmaχ{ 1+7α+2τ」十 号}).
2σ2
□
ProofofTheorem 8. Let R = n 1+κ where 0 < κ < min{^^^丁, 20-11. Since
0 ≤ q < min{ 2β-1, α} ∙ min{工炉丁, 20-1 }, We can choose K < min(工炉丁, 2a-1 } and K
is arbitrarily close to κ< min( ^^丁, 2β-1 } SUCh that 0 ≤ q < min{ (2β-1)κ ,κα}. Then we have
(α + K) i 2B + q < 0, and —κa+q < 0. AS for T2 (Dn), we have
T2(Dn) ≤ T2,R (Dn)+ ∣T2,R(Dn)-T2(Dn)∣
=n-μ 届 + O(nmaχ{ 1+70+τ/+T }) + O((1 +1)nmax<1+(1+κ) T ,1"κα})
=n-μ 而+O(nmaχ{ 1+70+τ，1+T})+0(-q+maχ{i+(1+K) T Jfa))
= 2σ2 μ0+o(n)∙
By Lemma 36, we have TI(Dn) = O(n 1). Hence EP0(Dn) = TI(Dn) + T2(Dn)=
寿 μ0 + o(n).	口
D.2 PROOFS RELATED TO THE ASYMPTOTICS OF THE GENERALIZATION ERROR
ɪ	CA ,	2 〜八 ，	T	C 一 ， 一	T	h . C ( 2αT +1)(l-t)	-	7
Lemma 39. Assume	σ2	= Θ(nt)	where	1	— ɪ:^ < t <	1.	Let R = n α(α-1) 八 .Under
Assumptions 4, 5 and6, with probability ofat least 1-δ oversample inputs (xi)n=1, we have
,./	ι∕c	、	( (1 — a)(1 — t) ∖
GI(Dn) = # (Tr(I + 枭AR)TAR-MR2(I + σAr) - 1||F)= ɪΘ(n —α — ) (114)
ProofofLemma 39. Let Gι,R(Dn) = E(xn+ι,yn+ι)(Tι,R(Dn+ι) - Tι,R(Dn)), where R = nc for
some constant C. By Lemma 32, we have that
IGI(Dn)-G1,R (Dn)I=IE(xn+ι,yn+ι) [TI(Dn+1 ) - T1,R(Dn+1)] - [TI(Dn)-T1,R(Dn)] ]
=I E(xn+I,yn+1) O((n+1)R1-α)∣ + I O(nR1-α)] ∣	(115)
=O( ɪ nR1-α).
Define〃r = (φ0(xn+1),Φ1(xn+1),...,ΦR(xn+ι))τ andΦR = (ΦR,nr)t. Asfor Gι,R(Dn),wehave
Gι,R(Dn)= E(χn+1,yn+1)(T1,R ( Dn+1 ) - T1 ,R ( Dn ))
2l°gdet(I +-RJ-R)-尹(/- (I +-RJ-R )-1))
-(Jogdet(I +-R AR-R)-ITr(I -(I +-R AR-R )-1))
∖2	σ2	2	σ2 J	(116)
E(xn+ι,yn+ι)logdet(I +-R J-R )-logdet(I +-RσR-R))
-1 (E(Xn+1,yn+1)Tr(I-(I +- R ：R-R )-1)-Tr(I-(I +-R :R-R )-1))
42
Published as a conference paper at ICLR 2022
As for the first term in the right hand side (116), we have
2 卜(χn+1,yn+1)logdet(I +ʒf^ )T°gdet(I +"R ;R	))
=1 (E(Xn+ 1 ,yn+1 )logdet(I +AR RjR&R ) -lθgdet(I JR：R	))
=1(E(Xn+i,yn+i)logdet(I + ARRRRR+ηRnR)- logdet(I + AR R RR))
=2 (E(Xn+ι,yn+ι)lOgdet((I +^RIRR )-1(I +⅛∣∣φr + aR^ )))
=2 (E(Xn+1,yn+l)lOgdet(I +(I +arφjIφr )-1 aR^ ))
=2 (E(Xn+ι,yn+ι)lθg k + σ12 nR (I +ARiTRR )-1ARnR))
Let
A =(I + σ2 AR)T2 AR/(RRRR -nI M/(I + σ AR)T/2.	(117)
According to Corollary 22,
CI R R 1-α+2τ — (1+2τ)t
OQlogR n 2α	2ɑ )=
Lemma 27, we have
with probability of at least 1 一 δ, we have ∣∣ £A∣∣/
o(1). When n is sufficiently large, ∣σ2A∣∣/ is less than 1.
By
T	ARRTRRR -1
nR (I +-2-)	ARnR
o/
n	∞	1n
=nR(I + σAR) IARnR+X(-1jnR(o2 (I+σAR) IAR(RRRR—
j=1
∞
=nR (I+ 2a AR)TARnR + X(-1j
o	j=1
(I +—2 AR)-IARnR
o/
ɪ nT (I + ʌ Ar )T∕2a∕Aj (I + = ART1MnR
o/j ,r∖ ' o/j R/ R ×	σ/ R/ R /R
CλP-α
CλP
∞
≤nR(I+ σ AR)TARnR+Xk σ Ak/k(I+σ AR)-1/2 AR//nRk/
o	j=1 o	o
.Rr	C p—α	.∞∖ 1	R
≤Xφp(xn+1)1+nCλp-α∕σ/ + Xk 苒 Ak2Xφp(xn+1)1+nCλp-ɑ∕σ/
p=1	----- / j=1	p=1	---- /
R	LIri—a/Tτ	∞	1	R	LIri—a /Tr
≤ X 1+nCλp-a∕σ/ +Xk O/ AkTX 1+nCλp-a∕σ/
p=1	----- / j = 1	p=1	----- /
∞
„ / (I-α + 2T)(IT)	、一.. 1 ZIIjC / (I-α + 2τ )(1-t)
≤O(n	ɑ	)+£ k σ AkTO(n	ɑ )
j=1
(1-α+2τ)(1-t)
=O(n ʌ-----oɪ^ ) = o(1),
(118)
43
Published as a conference paper at ICLR 2022
where we use Lemma 15 in the last inequality. Next we have
2 (E(χn+ι,yn+ι)logdet(I +1R^)-l°gdet(I +^^^R^R))
=2 (E(Xn+1 ,yn+ι)l°g(1 + ^2ηT(I + AR：R①R )-1ARηR))
= 1(E(χn+ι,yn+ι)( ⅛ ηR (I + ARφTφR )-1ARnR)(1+o(1)))
=S (Tr(I +a^r)-1AR)(1 + o(1)),
where in the last equality we use the factthatE(xn+1,yn+1)ηRηRT=I. By Lemma 27, we have
Tr(I + AR1R1R )-1Ar
σ2
=Tr(I+后AR)-IAR+X(-1jTr(σ2(I+σ^AR)-1 AR(φRφR-nI)) (I+σAR)-IAR
∞
=Tr(I + -2 Ar)-1Ar+X(-1)j Tr-2- (I + -2 Ar )-1F2Aj(I+ -ɪ AR)-1/2a/.
σ2	σ2j	σ2	R	σ2	R
j=1
By Lemma 15, we have
3I + qAr)-1Ar ≤X	Cp二 2 =Θ(n(1-αα1-t))
σ2	A11+nCλp / σ2
Tr(I + =Ar)-1Ar ≥ X —Cλp-α— = Θ(n(1—α(1-)).
( + σ2 R) R ≥M 1+nCλp-α∕σ2	(	)
Overall,
n	(1-α)(1-t)
Tr(I + F AR)TAR = Θ(n —a—).	(119)
σ2
Since || 表 A∣∣2 = o(1), we have that the absolute values of diagonal entries of σ⅛ Aj are at most o(1).
Let (Aj)p,p denote the (p,p)-th entry of the matrix Aj. Then we have
Tr σ2j (I+σ AR)-1/2 AR/2 Aj (I+σ ar )-1/2'/
=Xλpσj弋)2P ≤XλpkσjA* =Θ(n(1-α01-t))O)(n"i+22-(I+2T)t) (logRj∕2), (120)
where in the last step we used (119). According to (119) and (120), we have
2 (E(χn+ι,yn+ι)logdet(I +1RAR1R)-logdet(I +φRARfR))
=£ (Tr(I +AR)-1 aR )(1+o(1))
=σ2Θ(n(1-α01-t)) + σ2XΘ(nn-α01-t))O(n"…吃;(I+2T)t) (logRj/2)	I)
j=1
1	(I-a)(1 - t)1	(1 - a)(1 - t)1	(1 - a)(1 - t)
=σ12 Θ(n	a	) + σ12 Θ(n	a	)o(1) = σ12 Θ(n	a )
=2⅛ (Tr(I+σ AR)-IAR)(1+o(I)).
44
Published as a conference paper at ICLR 2022
Using the Woodbury matrix identity, the second term in the right hand side (116) is given by
2 (E(Xn+I,yn+1)Tr(I - (I +φRσRφR )T-Tr(I —(I +(ΦRΛσRΦR 厂1)
=2 (E(Xn+i,yn+i)Tr( σ ee R (I + σ λRφRφ R)TARφR-Tr( σ φR (I + σ AR φRφR)TARφR)
=9 (E(Xn+ι,yn+ι)τr( —2 (I + ^^2 ARφRφ R)-IARφRφ R-Tr( F (I + F ARφR φR)-1 AR φRφr)
2	σσ	σσ
=-2 (E(Xn+1,yn+1)Tr(I + σARφRφR)-1 -Tr(I + σARφRφr)t)
=-2 (E(Xn+1,yn+1)Tr(I + ~2 arφrφR + -2 ARnRnR)-1-Tr(I + -2 ARφRφR厂1)
2	σσ	σ
=1 (E	T(I + σ2 ARφRφr )-1ARnRnR (I + σ2 AR φRφr)-∖
= 2σ2 (E(Xn+ι,yn+I)Tr	可nR+JARφRφRpiRnR	y
where the last equality uses the Sherman-Morrison formula. According to (118), We get
1	(E	T(I + σ12ARφRφR)-1ARnRnR(I + σ2aRφRφr)T ∖
2σ2l (Xn+1,yn+l)	1+2nRσ+12ARΦRΦRF‰nR	)
=2σ2 (E(Xn+1,yn+1) Tr(I+σ ARφRφr )-1 ARnRnR (I+σ ARφRφR)-1(1+O(I)))
= ⅛Pτr(I +4 ArΦRΦr)-1Ar(I +3 Ar ΦR Φr)-1
2σ2	σ2	σ 2
= ⅛^TrA^2 (I +3 Ag2ΦRφRA∕)-1Ag2(I +g ArΦR Φr)-1
2σ2	σ2	σ 2
= ⅛^Tr(I + σ12 A∕ΦRΦRA∕)-1A∕(I + ɪ ArΦR Φr)-1 a/
= ⅛^Tr(I + σ12 a∕φRΦrA∕)TAR(I + ɪ A1∕2ΦR ΦrA∕)T
=¾P kA∕(I + J a∕φRΦrA∕)TkF
=i+；21)kA/(I+σ AR)T/2(I+^2 A)T(I+σ AR)T/2 kF,
where in the penultimate equality we use Tr(BBT) = kBk2F, kBkF is the Frobenius norm ofA, and
in the last equality we use the definition ofA (117). Then we have
1+2^ kAg2(I + q AR)T/2(I +g A)-1(I + q AR)T/2kF
2σ 2	σ 2	σ2	σ 2
= 1+(^ kA∕(I + W AR)T/2(I+χχ (-1)j ɪ Aj )(I + W AR)T/2kF
2σ2	σ2	σ2j	σ2
j=1
—1 + o(1)∣∣ A1∕2∕r I n Λ ʌ-1 I X ,/ Ij 1 A1∕2∕r I n A ʌ-1/2 AjfT I n λ ʌ-1/21∣2
=2；2	kAR	(I+σAr)	+2J-I)	；2jar	(I+；Ar)	A	(I+；AR)	kF.
j=1
(122)
By Lemma 15, we have
1/2	n	ι∣∣	CλP-a	( (I-a)(I-t)
IIaR (I + — aR )	kF ≤λ E C -α∕ 2∖2 =θ(n	2α )
R	σ2	∖	(1+nCλP α∕σ2)2
kA1/2(I + =Ar)-1kF ≥, XT ——Cλp-α——= Θ(n(1-<1-i).
k R ( + σ2 R) kF ≥∖ p=1 (1+nCλp-ɑ∕σ2)2	(	)
45
Published as a conference paper at ICLR 2022
Overall, we have
M∕(I+ * Λr 厂 1kF =Θ(n (1—αα1-t)).	(123)
Since || 表A∣∣2 = O(JIogRn1 α+2τ -(1+2ατ)t ) = o(1),we have
k ~2j λR∖i + ~λAR)T/2 Aj(I + 2λNR厂1/2IIF
σσ	σ
≤kΛ∕(I + σ Ar )-1/2kF k σ12 AiBk(I + σn2 AR)-1/2k2	(124)
=O(n(1—α2θ1-1)O(nj(1-α+22-(I+2T)t) (IogRj/2),
where in the first inequality we use the fact that kAB kF ≤ kAkF kB k2 when B is symmetric. By
Lemma 15, we have
47∣TrΛg2(I + 胃 Ar)-1λR2(I + 胃 ΛR)T∕2Aj (I + 胃 Ar)-1/2
σ2j	R σ 2	R σ 2	σ2
X λp(( σ2 Aj)p,p
三(1+nλp∕σ2)2
『6 λPI 今Ak2	(1-α)(1-t)〜	j(1-α + 2τ-(1 + 2τ)t)	/2∕2∖
≤Σ (1+nλp∕σ2)2 =θ(n)O(n	2"	(IogR)	)
According to (123), (124) and (125), we have
(125)
C	7 ɪ Tλf	t 1 -γ γγ
E(χn+1,yn+1)Tr(I - (I +φRσR R )-1 -Tr(I-(I +J R ) - 1
1+萼 Tr(I +3 ΛrΦRΦr)-1Λr(I +3 ΛrΦR Φr)-1
2σ 2	σ2	σ 2
⅛1^ M∕(I + qAr)-1 +X(-1)j ɪΛ∕(I + qAr)"A (I + WAR)-1/2kF
2σ 2	σ2	σ2j	σ2	σ2
σ	σ	j=1	σ	σ	σ
安(kA『(I+σ AR)-IkF+X
j=1
-2j AR/2 (I + -2 AR)T/2 Aj (I + WAR)T/2
σ 2j	σ2	σ 2
2
F
∞
+2TrAR (I+σAR)-IX(-i)jσj AR (I+σAR)-12Aj(I+σ2AR尸2)
σ	j =1	σ	σ	σ
1+ο(1)
2σ2
-	j(1-α + 2τ-(1+2τ )t)
)O(n	2ɑ
∞
CX1 ʌʌ / (1-a)(1-t) -	j(1-α+2τ -(I+2τ )t)	c\j/2\\
+2)F^Θ(n	ɑ	)O(n	2«	(logR)j/2)
σ2j
j=1 σ
(logR)j/2)
1	(1-α)(1-t)	1 + ο(1) ll . 1/2 ,r i n ∖-ι∣∣2
=σθ(n	"	)= 2σ2 kAR (I + 手AR)	kF.
(126)
Combining (121) and (126) we get that G1,R(Dn) = 1+θ21) (Tr(I + σ⅛AR)TAR +
∣∣a∕(I + σ⅛ Ar )-1kF) = σ12 Θ(n(Iaa(It)). From (115) we have that GI(Dn) ≤
G1,R(Dn) + IGI(Dn) - G1,R (Dn)I =* θ(n ~ α ^) + O(n 表 R1-α )∙ Choosing
C ( 2aT、+1)(1-t)	IlTC
R=n( a(a-1) + )(	) we conclude the proof.	□
Lemma 40. Assume σ2 =Θ(nt) where 1 一 y+α2τ <t< L LetS = nD. Assume that ∣∣ξ∣∣2 = 1. When
n is sufficiently large, with probability of at least 1 -2δ we have
k(I + σ2 Φs AS ΦT )-1Φs AS ξk2 = O(J( δ + 1)n∙n-(1-t)).	(127)
46
Published as a conference paper at ICLR 2022
ProofofLemma 40. Using the Woodbury matrix identity, we have that
((I + σ2 Φs AS ΦT )-1Φs AS ξ = [I - Φs (σ2I+As ΦT Φs )-1As ΦT ]Φs AS ξ
=Φs AS ξ -Φs (σ2I+As ΦT Φs )-1As ΦT Φs AS ξ	(128)
=Φs (I + σ12 AS ΦT Φs )-1As ξ.
Let A =(I + 枭 As )-γ∣2 AyWS Φs - nI )A?2 (I + f AS )-Y/2, where γ> 1+α+2T-((I-jτ+2a)t.
By Corollary 22, with probability of at least 1 -δ, we have ∣∣ = A∣ 2 = O(n1+α+2τ 2i+2τ+2ɑ)t -Y(I-1)).
When n is sufficiently large, ∣ 表 A12 is less than 1. By Lemma 27, we have
(I +-n ASφSφS)-1
σ2
=(i+σn2 As )-1+X (-i)j( σ2 α+σn2 AS )-1as &s w -n 斗 "+σ AS 尸.
Then we have
Il(I+σ2 as φS φs )-1 AS ξ∣∣2
=I((I+σ AS )-1+χ (Tj
(∞
∣∣(I + σ As )-1As ξ∣∣2 + X
j=1
σ2 (I + σn2 AS )-1As (ΦT Φs-nI ))'(I + £ AS )-1 ∣As ξ
2
j n ，
(I + 后 AS)% ξ
—
2
(129)
For the first term in the right hand side of the last equation, we have
∣(I + qAs)-1Asξ∣2 ≤ k(I + WAs)-1As∣∣2kξ∣∣2 ≤ 匕=O(n-(I)).	(130)
σ2	σ2	n
Using the fact that ∣ *A∣∣2 = O(n1+ɑ+2τ 2i+2τ+2ɑ)t
have
-γ(1-t)) and ∣∣(I + 备AS)-1As∣∣2 ≤ n-1, We
σ2 (I + σn2 AS )-1As (ΦS Φs-nI ))’(I + 工 AS )-1As ξ
2
σ⅛∣α+σ AS 厂
1+2AS-2 (A(I + nAs)-1+γAS-Y了 1A(I + 工AS)-1+2A-2 ASξ
2
≤n(i(-1+ 2+(-1+γ)(j- 1))O(n j(I++F?*+2a)t)-jY(1-t))∣∣(I + W As )-1+2 AS- 2 ξ∣∣2
=O(n
,τ ,
=O(n
=O(n
_ γ(1 _t)i (I-α+2τ-(I+2τ)t)j
,2 (1 t)+	2α
-工(1 -1)+ (I-α+2τ-(1+2τ)t)j .
_(1 _t)+ (1-α + 2τ-(1 + 2τ )t)j
,(1 t)+	2α	).
)k(1+σAS)-1+2AS 2k2∣∣ξ∣∣2
)O(n(-1+Y∕2)(1-t))
Using (129), (130) and (131), we have
∣∣(i+ = as φs φs )-1 AS ξ∣∣2
/	∞
=O(n-(1-t)) + XO(n-1+
(1-α + 2τ-(1 + 2τ)t)j
2α	)
∖	j=1
= (0(n-(I))+O(n-1+ iF(I+2τ )t
=O(n-(I-t)).
(131)
(132)
47
Published as a conference paper at ICLR 2022
By Corollary 20, with probability of at least 1-δ, we have
∣∣φs (I+ σ12 AS φs φs 厂 1λs ξk2 =O(4 (* + 1)nk (I + σ12 AS φT φs 厂 1λs ξk2)
=O(J( δ1 + 1)n ∙ n-(1-t)).
From (128) We get k(I + 表ΦsAsΦT)-1fs(x)∣∣2 = O(J(δ + 1)n ∙ n-(1-t)). This concludes the
proof.	口
Lemma 41. Assume σ2 = Θ(nt) where 1 一 ɪ+^ < t < L Let δ = n-q where 0 ≤ q <
[0-(1+2τ4:-t)](2β-1). Under Assumptions 4, 5 and 6, assume that μo = 0. Let R = n( 1+κ)(1-t)
where 0 <κ< ɑ—1/；+—+~2τT). Then Withprobability ofat least 1 — 6δ over sample inputs (xi)n=1,
we have G2(Dn)= (1+θ21)) Il(I + 备Ar)-%r||2 =*Θ(nmax{-2(1-t), (I弋(1-" }logk/2n), where
k =∫0,	2α = 2β — 1,
11,	2α = 2β — 1.
Proof of Lemma 41. LetS = nD. LetG2,S(Dn) = E(xn+1,yn+1)(T2,S(Dn+1) —T2,S(Dn)). By
Lemma 33, when S > nmax{1, (α-1-2τ)} with probability of at least 1 — 3δ We have that
|G2(Dn)—G2,S(Dn)| = |E(xn+1,yn+1)[T2(Dn+1) —T2,S(Dn+1)] — [T2(Dn) —T2,S(Dn)]|
= IE(Xn+1,yn+1)O ((1 + 1层(n+1)Smax{1/2-e,1-a+2T })-O(( 1 +1层 nS max□∕2-βJ-α+2τ 川
=O ((1 + 1) σ12 nS max□∕2-β,i-α+2τ })
(133)
(134)
Let Ai：S = diag{λι,...,λs},①上S = (φι(x),φι(x),...,φs(x)) and μ±S = (μι,...,μs). Since
μo = 0, we have ”(Dn) =~*：S①工S(I + σ2①上SAi：sΦTs)-1Φi:Sμi:S. Define ni：s =
(φι(χn+ι),…,φs(χn+ι))T and Φi：s = (Φ个s,ni：s)t. In the proof ofLemma 34, we showed that
T2,S (Dn) = 2σ^ μT-S φl^S (I + σ φ1:S AI：S φl-S 尸①上S μ1:S
=2 μT,S A-S μi:S - 2 μι:S A-S (I + σ AI：S φi:s φ1:S )-iμi:S.
We have
G2,S(Dn)=E(xn+1,yn+1)(T2,S(Dn+1)—T2,S(Dn))
= E(xn+1,yn+1) 2μ μl-S A-S μ1:S — 2 μfs A-S (I + σ AI：S φ S φ S )-1μ1:S
一 2μμTsA-Sμi:S- 2μι:sA-S(I + σAI：Sφi:sφ1:S)-1μi:S))
=E(xn+I,yn+1) (2μl:SA-S(I + σAI：Sφl:Sφ1:S厂 1μ1:S — 2μl:sA-S(I + σAI：SφTφS)-1μ1:S
一党	(1 T ʌ-i (I + σ12Ai：sφi:sφ1:S)-iAi：sηi:SηTs(I + σ12AI：Sφ*sφi:S)-i
=E(Xn+I，yn+1) (2σ2 μi:S ALS	1+σ2 ηTs (I + 表 Ai：S Φ&Φi:S )-iAi:S ni：s	μi:S
=E	( 1 μι^s (I + σ12 φi^s Φi：s Ai：s )-1小：S ηTs (I + 表 Ai：s φi:s φi:S )-iμi:S Λ
=(Xn+1,yn+1)12σ2	1+σ2 ηTs (I + σ2 Ai：S φ^s ①上S )-iAi:S ni：s	})
=E(χn+1,yn+1) ( -2σ2 μ μι:s(I + σφf SΦi：sAi：s)-”上SηTs(I+m Ai：S Φfs Φi:S )-iμi:S)
-2σ2 ^ μι:s(I + σφi:sΦi：sAi：s)-i(I + 后Ai：sφi：sφi:S)-iμi:S
:2，k(I + ^^2 Ai：sφi：sΦi：s)-iμi:Sk2,
2σ 2	σ2	：
(135)
48
Published as a conference paper at ICLR 2022
where in the fourth to last equality We used the Sherman-Morrison formula, in the third inequality
we used (118), and in the last equality we used the fact that E(χn+ι,yn+i)ni：sηTs = I.
Let μi：R = (μι,…,μR,0,…,0) ∈RS. Then we have
Il(I +-2λ1:SφTsφ1:S厂1 μ1:S∣∣2 ≤ Il(I +--2 λ1:Sφl-Sφ1:S)-1 μ 1：Rk2 + k (I +-2八上S①"S①上S厂1 3上S-μ 1：R)k2,
σ2	:	σ2	:	σ2	:
Il(I +-2λ1:SφTSφ1:S)-1 μ1:S∣∣2 ≥ Il(I +-2λ1:SφT:Sφ1:S)-1 μ 1:Rk2 -Il(I +---2λ1:SφT:Sφ1:S厂1(μLS -μ1:R)Il2.
σ2	:	σ2	:	σ2	:
(136)
Let R = n( α+K)(IT) where 0 < κ < α-1-=τ(+-+2τ )t .In Lemma 29, (62), we showed that with
probability of at least 1 -δ,
I(I + σ⅛ ΛlrΦ*R 虫1次)-1〃1川|2 =。5(I)max{-1，H }logk/2n)
σ	n	(137)
= (1 + o(1))I(I + F 八1：口)-1〃1川|2,
σ2
where k = 0,
2α 6=2β-1,
2α=2β-1.
The same proof holds if we replace Φ1: R with Φ1: S, A1： R with A1： S,
and 41：R with μ 1：R. We have
I (I + σ⅛Λ1：SΦ%Φ1：S)-1μi：rI2 = Θ(n(1-t)max{-1,ɪ}logk/2n)
σ	n	(138)
= (1 + o(1))I(I + F Λ1：S )-1μ i：rI2.
σ2
Next we bound ∣∣(I + 表A1：SΦfS Φ1：S )-1(从1：S — μlr)∣∣2∙ By Assumption 5, we have that
||41：S-μlr∣∣2 = O(R^22β). For any ξ ∈ Rs and ∣∣ξ∣∣2 = 1, using the Woodbury matrix identity, with
probability of at least 1 -2δ we have
∣ξT (I + ―2 Λ1:S φTS Φ1：S )-1 (从1：S - μ 1：R)|
σ2	：
=∣ξT (I- 2Λ1：sΦ*S(I + 2Φ1：sΛ1：sΦ*S厂 1ΦLs)(41：S-μ 1：R)|
=∣ξT(μLS-μ 1：R) - σξTΛ1：SφTs(I + σΦ1：SΛ1：Sφ1^S)-1孙：S31：S -μ 1：R)|
≤ ∣∣ξ∣∣2 ||41：S - μLR∣∣2 + σ |gTA1：S φ*S (I + σ ①上S A1：S φ*S )-1 ①上S 31：S-μ 1：R )|
≤ O(R 2 " ) + -2 Il (I + ^^2Φ1：SΛ1：SφTS)-1的：SΛ1:S02忡1：S(μ±S-μLR)∣∣2
σ 2	σ2	：
=O(R1-2β) + σ12θ(ʌ/(δ1 + 1)n∙n-(I))O(J(； + 1)nR号)
=O((1+1)R 1-2β),
δ
where in the second to last step we used Corollary 20 to show |悭1：S31：S — μlr)∣∣2 =
O(J(1 + 1)nR⅛2β) with probability of at least 1 一 δ, and Lemma 40 to show that
Il(I + σ2Φ1：SΛ1：SΦfs)-1 Φ1：SΛ1：Sξ∣∣2 = OW(1 + 1)n ∙ n-1) with probability of at least
1-δ. Since R = n(α+K)(IT), we have
∣ξτ (I +3 A1：S Φ臬 ΦrS )-131：S-μ 1：R)| = O((1 + 1)n (1-2βα1-t) + (Ie)2(Ie).
σ2	：	δ
Since ξ is arbitrary, we have ∣∣(I + =Λ1：S 较S Φ1：S )-1(从1：S - μLR)∣∣2 = O(( 1 +
1)n (1-2βαι-t) +(I-2β2(IT)K). Since 0 ≤ q< *(1+2y-切3-I) and 0 <κ< …"产)t,
we can choose κ <
a-1-：；+-+^)t and K is arbitrarily close to κ < a-1-：；+-+^)t such that
49
Published as a conference paper at ICLR 2022
0≤q< (P-?(1-9*. Then Wehave (1-2β2(1-t)κ + q<0. From(136)and(138), Wehave
Il / T 1 A ɪ T ɪ ∖ 1	II C r∩avΓ ∩ tʌ (1-2β)(1-1) ^1 k/2 .	, , 1	、 (1-2β)(1-t) I (1-2β)(1-t)κ
II(I + σ Λi:S ΦTs Φi:S )-1 μi:S k2 =Θ(nmax{-(1-t), -2α — }logk/2n) + O(( * + 1)n -2α- +	2	)
=Θ(nmax{-(1-t), (1-22α(1-t) }logk/2n) + O((nq+ (1-22(1-) + *-吗-北)
=Θ(nmax{-(1-t), (1 22α(1 t) }logk/2n)
=(I+ o(I))Il(I + W λ1:S )-1μ 1：R k2
σ2
=(1 + o(1))Il(I + W ΛR)-1μR∣2.
σ2
(139)
Hence G2,S(Dn) = ® k(I + ⅛Ai：SΦTTSΦi:S)-1μrS∣2 = ɪΘ(n(1-t)max{-2,审}logk/2n).
Then by (133), We have
G2(Dn) = ɪΘ(nmax{-2(1-t), (1-2βα(1-t) }logk/2n) + O ((； + 1) WSmax{1/2-e，1-a+2T}).
ʃi -t 1 1+q + min{2, 2βα-1 }	∖，1 /I
CL . m maxj 1, (α-1-2τ) , I min{β-1∕2,α-1-2τ} +1 I (1-t) C	,	1	r~∣
Choosing S = n 〔 '	'	'	J , J, we get the result.	□
ProofofTheorem 9. From Lemmas 39 and 41 and 1 - 1 > -2, we have that with probability of at
least 1 -7δ,
嘎G(Dn)= : 2，(Tr(I + ^^2 AR)TAR-kAR/2(I + ^^2 AR)TkF + k(I + ^^2 AR)TμRk2)
2σ2	σ2	σ2	σ2
=表 Θ(n (1-α01-)) + -⅛ Θ(nmax{-2(1-t), (1-2βα(1-t) }logk/2 n)
=3Θ(nmax{ (T1-) , (T(1-t) })
σ	(140)
where k = 10, 2α = 2β 1 ,and R=n( 1+κ)(1-t), κ> 0.
1, 2α = 2β - 1
Furthermore, we have
Tr(I +—2 A) 1A-Tr(I +—2 AR) 1Ar
σ 2	σ2
∞∞
X —P— ≤ X
p=R+ 11 + 枭 λP	p=R+1
∞
≤ X CλP-α =彳O(R1-α)
σ2
p=R+1
=O(n(1-α)(1-t)( α+κ))
(1-α)(1-t)
=o(n	α ).
Then we have
Tr(I + σ2 AR) 1Ar = Tr(I+ 后 A) 1A(1 + o(1)).	(141)
Similarly we can prove
iiaR/2(i+σ2 AR)TkF=IlA1/2 (I+σ2 A)TkF(1+O(I))	(142)
Il(I+—2 ar )TμR∣ι2=k(I+-2 A)Tμk2(1+o(I))	(143)
σ2	σ2
T . . ∙	¢- rU,ι	/' -	1 .	I-I
Letting δ = 7δ,the proof is complete.	□
In the case of μo > 0, we have the following lemma:
Lemma 42. Let δ = n-q where 0 ≤ q < [α-(1+2τ4α-t)](2β-I). Under Assumptions 4, 5 and 6,
assume that μo > 0. Then with probability of at least 1 一 6δ over sample inputs (xi)n=1, we have
G2 (Dn) = 2⅛2 μ0 + o(1).
50
Published as a conference paper at ICLR 2022
Proof of Lemma 42. LetS = nD. LetG2,S(Dn) = E(xn+1,yn+1)(T2,S(Dn+1) - T2,S(Dn)). By
Lemma 33, when S > nmax{1, (α-1-2τ)}, with probability of at least 1 -3δ We have that
|G2(Dn)-G2,S(Dn)| = |E(xn+1,yn+1) [T2(Dn+1) -T2,S(Dn+1)] - [T2(Dn) -T2,S(Dn)]|
= Wxn+1,yn+1)O ((1 + 1)今(n+i)Smax{1/2-e,1-a+2T })-0(( 1 + 1)去 nS max{1/2-e,1-a+2T 可
〜
=O
(1 + 1) σ12 nSmax{1/2-e，1-a+2T}
Let Λs = diag{λι,...,λs}, Φs = (φι(x'),φι(x'),... ,φs(x)) and μs = (μι,…，μs). Define
ηS = (φ0(xn+1),φ1(xn+1),...,φS(xn+1))T andΦeS = (ΦTS,ηS)T. By the same technique as in the
proof of Lemma 34, we replace ΛR by Λ,R =diag{,λ1,...,λR}, let →0 and show the counterpart
of the result (135) in the proof of Lemma 41:
G2,S(Dn)=E(xn+1,yn+1)(T2,S(Dn+1)-T2,S(Dn))
_F	( 1 μs (I+ σ12 φT φs AS )-1ηs ηT (I+ σ12 AS φs φs )-1μs Λ
=(xn+ι,yn+1) ^2σ2	i÷⅛ητ(i÷jASφTφS)-ιASηs	))
=E(xn+ι,yn+ι) ( "2σ2 μ μS(I + σφSφSAS)-1ηSηT (I + σASφS",)— "$
= 1+°2^ μT (I +3 ΦT Φs As )-1(I +3 AS ΦS Φs )-1μs
2σ 2	σ 2	σ 2
= 1⅛^ k(I+3 As ΦT Φs )-1μs k2,
2σ 2	σ2
(144)
where in the fourth to last equality we used the Sherman-Morrison formula, in the third inequality
we used (118), and in the last equality we used the fact that E(xn+ι,yn+i)ni：sηTs = I.
Let μr = (μo,μι,…,μR,0,...,0) ∈RS. Then we have
k (I +-2 AS φT φS )-1 μs ∣∣2 ≤ k (I +-2 AS φs φS )-1 μ Rk 2 + k (I +-2 AS φT φS )-1(μS - μ r)∣∣2,
σ 2	σ2	σ2
k (I +--2 AS φT φS ) — 1 RS ∣∣2 ≥ k (I +-2 AS φS φS ) — 1 μ R ∣2-k (I +-2 AS φT φS )-1 (RS - μ r)∣∣2∙
σ 2	σ2	σ2
(145)
Choose R = n( α+K)(IT) where 0 <κ< α-1-2；+—；+2丁 )t. In Lemma 29, (62), we showed that with
probability of at least 1 - δ,
k(I + S AlrΦ"r ΦLR)-1μLR∣∣2 = Θ(n(1-t)maχ{-1, F }logk/2n)
σ	n	(146)
= (1 +。⑴)k(I + -2 ALR)-1μ1R∣2,
σ2
where k
01,,
2α 6=2β-1,
2α=2β-1.
The same proof holds if we replace Φ1: R with Φ1: S, A1： R with A1： S,
and μ1:R with μ 1：R. We have
k(I + σ12 A1：S Φ*S Φ1：s )-1μ 1：Rk2 = Θ(n(1-t)max{-1,1-α2β }logk/2 n)
= (1 + o(1))k(I + -2 A1：S )-1μ 1:Rk2.
σ2
So we have
k(I + σ⅛ As ΦT Φs )-1μ Rk2=μ0+Θ(n(I)max{-1，ɪ } logk/2n)
=μo+o(1).
(147)
(148)
Next we bound k(I + σ⅛ASΦTΦs)-1(μs - μR)k2.
kμs - μRk2 = O(R 1-22β). For any ξ ∈ Rs and kξk2
By Assumption 5, we have that
= 1, using the Woodbury matrix
51
Published as a conference paper at ICLR 2022
identity, with probability of at least 1 - 2δ we have
lξT (I + —2 AS φT φs )-1(μs - μ R)I
σ2
=∣ξT (I- σ2ΛsΦT(I + 盘ΦsΛsΦT)-1 Φs) (μs-μr)∣
=lξT (μs - μ R)-2 ξT AS φT (I +—2 φs AS φT )-1φs (μs - μ R)|
σ2	σ 2
≤ ∣∣ξ∣∣2kμS -μRk2 +-2 lξTλSφT(I +-2 φSASφT)-1φS(μS -μR)I
σ2	σ2
≤O(R 2 " ) + —2Il(I + -φΦsASΦT)-1ΦsASξk2kΦS(μS-μR)k2
σ2	σ2
=O(R 1-2β) + σ12 O({(∖ + 1)n∙n-(I))O (ʌ/( ； + 1)nR 号)
=0(( 11 + i)RR12β),
J( δ + 1)nR1-2β
where in the second to last step We used Corollary 20 to show ∣∣Φs (μS-μ r)∣2 = 0(
)
with probability of at least 1 一 δ, and Lemma 40 to show that k(I + σ2ΦsASΦS)-1ΦsASξk2
O(J(1 + 1)n∙n-(IT)) with probability of at least 1 -δ. Since R=n(1+κ)(1-t), we have
»T/r	1 A ɪTɪ 、—1/	ʌ X1 ~∕1 T、 (1-2β)(1-t) + (1-2β)(1-t)κ
lξ (I+σ2ASφSφS)	(μS一μR)I=O((δ+1)n	2ɑ	2	).
Since ξ is arbitrary, we have k(I + σ⅛ASΦTΦs)-1(μs - μR)k2	=	0((1 +
1)n (1-β01-) + (1-"-t)κ). Since 0 ≤ q < "+可-切磔-1)and 0 <κ<。-1-喜+-+2,",
αT-0T+-+2τ)t and K is arbitrarily close to κ <。-1-：；+-+2丁)t such that
we can choose κ <
0 ≤ q< (2β-12(1-t)κ. Then we have (1-2β2(1-t)κ + q< 0. From (145) and (148), we have
k(I + σ⅛AsΦTΦs)-1μSk2=μ0 + Θ(n(I)max{T，⅛β}logk/2n)+O((∣ + 1)n(1-β^ + S")
=μ0 + Θ(n(I)max{-1,1-2β }logk/2 n)
=μ0 + o(1).
(149)
HenCe G2,S(Dn) = —2θ2") k (I + ~1zasφTφs) 1μsk2 = 2σ2 μ2 + o(1). Then by (144),
G2(Dn) = 2⅛ μ0 + o(1) + O(( 1 + 1)nS max{1/2-e，1-a}).
ʃi -t 1 1+q+min{2,2βα-1} Λ 1	-I
ma . m	maxJ1, (α-1-2τ)，( min{β-1∕2,α-1-2τ} +1) (IT) ,	.	1	r~∣
Choosing S = n 〔 '	'	'	J , J, we get the result.	□
ProofofTheorem 11. According to Lemma 42, G2(Dn) = 2⅛μ0 + o(1). By Lemma 39, we have
GI(Dn) = Θ(n(1-21-t)). ThenEeG(Dn) = Gι(Dn) + G2(Dn) = 2σ2μ0+o(1)∙
□
52
Published as a conference paper at ICLR 2022
D.3 Proofs related to the excess mean s quared generalization error
ProofofTheorem 12. For μo = 0, We can show that
EeM(Dn)= EeExn+ι [m(Xn+l) -f(Xn+l)]2
= EExn+1 [Kxn+1x(Kn+σmodelIn)- y-f(xn+1)]
=EeExn+JηT Aφτ[φAφT + fodeiIn)T &〃 +C)-ηT μ]2
=EeExn+1 [ηTΛΦT(ΦΛΦT+σm2 odelIn)-1]2
+Exn+1 [ηT (ΛΦt (ΦΛΦt+σm°deiIn )-1Φ-I )μ]2
=σt2rueTrΛΦT (ΦΛΦT +σm2 odelIn)-2ΦΛ
+ μτ (I + JΦtΦΛ)-1(I + JΛΦtφ)-1μ
σm
odel	σm
odel
=争a Tr(I + λΦtφ )-1Λ-Tr(I + λΦtφ )-2Λ+k(I +	ΛΦt Φ)-1μk2.
σ2	σ2	σ2	σ2	2
model	model	model	model
According to (139) from the proof of Lemma 41, the truncation procedure (133) and (143), with
probability of at least 1 - δ we have
k(I + J ΛΦT Φ)-1μk2 = θ(nmaxL2(I), (1-2β)(1-t) }iogk/2n) = (i +。⑴)∣∣(∕ + + Λ)-1μ∣∣2,
model	model
where k =
01,,
2α 6=2β-1,
2α=2β-1.
According to (121) and (126) from the proof of Lemma 39, the truncation procedure (115), (141) and
(142), with probability of at least 1 -δ we have
Tr(I + λΦtφ )-1Λ-Tr(I + λΦtφ )-2Λ
σm
odel	σmodel
= (Tr(I + σ2n- Λ)-1λ)(1 + o(1))-kΛ1/2 (I + σ2n- Λ)-1kF (1+o(1))
model	model
(1-α)(1-t)
= Θ(n	a ).
Combining the above two equations we get
σ σ∕fU σt 、一门 _kc 门σ2rue	(T∖√ T I n	ʌʌ-lʌ	IlA 1/2( T-∖- n	八、- 1||2 ∖-kl" T I n	A、— 1 〃1|2、
EeM(Dn )=(1+o(1)) I	σ2	(Tr(I + σ2	八) λ-∣∣λ (I + σ2	八) ∣∣F J +k(I + σ2 ,,八) μk2 )
model	model	model	model
2	(1-α)(1-t)	(1-2β)(1-t)
=σtrue Θ(n α	) + 巳(九陋&-2(1-0	α	}logk/2n)
σmodel
C	1-α-t	( Crl χ∖ (1-2β)(1-t) 1	-C
=σ2rueΘ(n	)+e(nmax{-2(1-t),	S	}logk/2n)
2	2	1 —α-1	(I-2e)(1 —t) ∖
=θl max{σtruen	α ,n α })
When μo > 0, according to (149) in the proof ofLemma 42 and the truncation procedure (133), with
probability of at least 1 - δ we have
(1-α)(1-t)
EeM (Dn)=Θ(n —α— )+μ2 + o ⑴
=μ0+o ⑴.
□
53