Online Facility Location with Predictions
Shaofeng H.-C. Jiang	Erzhi Liu
Peking University	Shanghai Jiao Tong University
Email: shaofeng.jiang@pku.edu.cn	Email: lezdzh@sjtu.edu.cn
You Lyu	Zhihao Gavin Tang
Shanghai Jiao Tong University	Shanghai University of Finance and Economics
Email: vergil@sjtu.edu.cn	Email: tang.zhihao@mail.shufe.edu.cn
Yubo Zhang
Peking University
Email: zhangyubo18@pku.edu.cn
Ab stract
We provide nearly optimal algorithms for online facility location (OFL) with pre-
dictions. In OFL, n demand points arrive in order and the algorithm must irrevo-
cably assign each demand point to an open facility upon its arrival. The objective
is to minimize the total connection costs from demand points to assigned facilities
plus the facility opening cost. We further assume the algorithm is additionally
given for each demand point xi a natural prediction fxpred , which is supposed to
be the facility fxoipt that serves xi in the offline optimal solution.
Our main result is an O(min{log OOnPT, log n})-competitive algorithm where η∞
is the maximum prediction error (i.e., the distance between fxpred and fxopt). Our
algorithm overcomes the fundamental Ω(忒蓝二)lower bound of OFL (without
predictions) when η∞ is small, and it still maintains O(logn) ratio even when
η∞ is unbounded. Furthermore, our theoretical analysis is supported by empirical
evaluations for the tradeoffs between η∞ and the competitive ratio on various real
datasets of different types.
1	Introduction
We study the online facility location (OFL) problem with predictions. In OFL, given a metric space
(X , d), a ground set D ⊆ X of demand points, a ground set F ⊆ X of facilities, and an opening
cost function w : F → R+, the input is a sequence of demand points X := (x1, . . . , xn) ⊆ D, and
the online algorithm must irrevocably assign xi to some open facility, denoted as fi , upon its arrival,
and an open facility cannot be closed. The goal is to minimize the following objective
X w(f) + X d(xi, fi),
f∈F	xi∈X
where F is the set of open facilities.
Facility location is a fundamental problem in both computer science and operations research, and
it has been applied to various domains such as supply chain management (Melo et al., 2009), dis-
tribution system design (Klose & Drexl, 2005), healthcare (Ahmadi-Javid et al., 2017), and more
applications can be found in surveys (Drezner & Hamacher, 2002; Laporte et al., 2019). Its on-
line variant, OFL, was also extensively studied and well understood. The state of the art is an
O(成墨 n)-competitive deterministic algorithm proposed by Fotakis (2008), and the same work
shows this ratio is tight.
We explore whether or not the presence of certain natural predictions could help to bypass the
O( io狭nn) barrier. Specifically, we consider the setting where each demand point Xi receives a
1
predicted facility fxpired that is supposed to be xi ’s assigned facility in the (offline) optimal solu-
tion. This prediction is very natural, and it could often be easily obtained in practice. For instance,
if the dataset is generated from a latent distribution, then predictions may be obtained by analyz-
ing past data. Moreover, predictions could also be obtained from external sources, such as expert
advice, and additional features that define correlations among demand points and/or facilities. Previ-
ously, predictions of a similar flavor were also considered for other online problems, such as online
caching (Rohatgi, 2020; Lykouris & Vassilvitskii, 2021), ski rental (Purohit et al., 2018; Gollapudi
& Panigrahi, 2019) and online revenue maximization (Medina & Vassilvitskii, 2017).
1.1	Our Results
Our main result is a near-optimal online algorithm that offers a smooth tradeoff between the pre-
diction error and the competitive ratio. In particular, our algorithm is O(1)-competitive when the
predictions are perfectly accurate, i.e., fxpired is the nearest neighbor of xi in the optimal solution for
every 1 ≤ i ≤ n. On the other hand, even when the predictions are completely wrong, our algorithm
still remains O(log n)-competitive. As in the literature of online algorithms with predictions, our
algorithm does not rely on the knowledge of the prediction error η∞ .
Theorem 1.1. There exists an O(min{log n, max{1, log OnPT}})-competitive algorithm for the
OFL with predictions, where η∞ := max1≤i≤nd(fxpred, fxopt) measures the maximum prediction
error, and fxoipt is the nearest neighbor of xi from the offline optimal solution OPT1.
Indeed, we can also interpret our result under the robustness-consistency framework which is widely
considered in the literature of online algorithms with predictions (cf. Lykouris & Vassilvitskii
(2021)), where the robustness and consistency stand for the competitive ratios of the algorithm in
the cases of arbitrarily bad predictions and perfect predictions, respectively. Under this language,
our algorithm is O(1)-consistent and O(log n)-robust. We also remark that our robustness bound
nearly matches the lower bound for the OFL without predictions.
One might wonder whether or not it makes sense to consider a related error measure, η1 :=
Pin=1 d(fxpired, fxoipt), which is the total error of predictions. Our lower bound result (Theorem 1.2)
shows that a small η1 is not helpful, and the dependence of η∞ in our upper bound is fundamental
(see Section F for a more detailed explanation). The lower bound also asserts that our algorithm is
nearly tight in the dependence of η∞.
Theorem 1.2. Consider OFL with predictions with a uniform opening cost of 1. For every η∞ ∈
log nη∞∖
(0,1], there exists a class of inputs, such that no (randomized) online algorithm is o( VgOgn)-
competitive, even when η1 = O(1).
As suggested by an anonymous reviewer, it might be possible to make the error measure η∞ more
“robust” by taking “outliers” into account. A more detailed discussion on this can be found in
Section G.
Empirical evaluation. We simulate our algorithm on both Euclidean and graph (shortest-path)
datasets, and we measure the tradeoff between η∞ and the empirical competitive ratio, by generat-
ing random predictions whose η∞ is controlled to be around some target value. We compare with
two baselines, the O(log n)-competitive algorithm by Meyerson (2001) which do not use the pre-
diction at all, and a naive algorithm that always trusts the prediction. Our algorithm significantly
outperforms Meyerson’s algorithm when η∞ → 0, and is still comparable to Meyerson’s algorithm
when η∞ is large where the follow-prediction baseline suffers a huge error.
We observe that our lead is even more significant on datasets with non-uniform opening cost, and this
suggests that the prediction could be very useful in the non-uniform setting. This phenomenon seems
to be counter-intuitive since the error guarantee η∞ only concerns the connection costs without
taking the opening cost into consideration (i.e., it could be that a small η∞ is achieved by predictions
of huge opening cost), which seems to mean the prediction may be less useful. Therefore, this
actually demonstrates the superior capability of our algorithm in using the limited information from
the predictions even in the non-uniform opening cost setting.
1 When the context is clear, we also use OPT to denote the optimal solution.
2
Finally, we test our algorithm along with a very simple predictor that does not use any sophisticated
ML techniques or specific features of the dataset, and it turns out that such a simple predictor al-
ready achieves a reasonable performance. This suggests that more carefully engineered predictors
in practice is likely to perform even better, and this also justifies our assumption of the existence of
a good predictor.
Comparison to independent work. A recent independent work by Fotakis et al. (2021a) considers
OFL with predictions in a similar setting. We highlight the key differences as follows.
(i)	We allow arbitrary opening cost w(∙), while Fotakis et al. (2021a) only solves the special case
of uniform facility location where all opening costs are equal. This is a significant difference, since
as we mentioned (and will discuss in Section 1.2)), the non-uniform opening cost setting introduces
outstanding technical challenges that require novel algorithmic ideas.
(ii)	For the uniform facility location setting, our competitive ratio is O (log O∞) = O (log nw∞),
while the ratio in Fotakis et al. (2021a) is
--吗err0)——-,where err。= nη∞∕w, errι
log(err1-1 log(err0)),	0	∞ , 1
ηι∕w
and w is the uniform opening cost. Our ratio is comparable to theirs when η1 is relatively small.
However, theirs seems to be better when η1 is large, and in particular, it was claimed in Fotakis
et al. (2021a) that the ratio becomes constant when err1 ≈ err0 . Unfortunately, we think this claim
contradicts with our lower bound (Theorem 1.2) which essentially asserts that η1 = O(η∞ ) is not
helpful for the ratio. Moreover, when η1 is large, we find their ratio is actually not well-defined
since the denominator log(err1-1 log(err0)) is negative. Therefore, we believe there might be some
unstated technical assumptions, or there are technical issues remaining to be fixed, when η1 is large.
Another independent work by Panconesi et al. (2021) considers a different model of predictions
which is not directly comparable to ours. Upon the arrival of i-th demand point, a set of multiple
predictions Si is provided, each suggesting a list of facilities to be opened. Their algorithm achieves
a cost of O(log(| Si Si|) OPT(Si Si)) where OPT(Si Si)) is the optimal using only facilities in
Ui Si, and the algorithm also has a worst-case ratio of O(造葭「) in case the predictions are inac-
curate. Similar to the abovemention Fotakis et al. (2021a) result, this result only works for uniform
opening cost while ours work for arbitrary opening costs.
1.2	Technical Overview
Since we aim for a worst-case (i.e., when the predictions are inaccurate) O(log n) ratio, our algo-
rithm is based on an O (log n)-competitive algorithm (that does not use predictions) by Meyerson
(2001), and we design a new procedure to make use of predictions. In particular, upon the arrival of
each demand point, our algorithm first runs the steps of Meyerson’s algorithm , which we call the
Meyerson step, and then runs a new procedure (proposed in this paper) to open additional facilities
that are “near” the prediction, which we call the Prediction step.
We make use of the following crucial property of Meyerson’s algorithm. Suppose before the first
demand point arrives, the algorithm is jlready provided with an initial facility set F, such that
for every facility f * opened in OPT, F contains a facility f that is of distance η∞ to f *, then
Meyerson,s algorithm is O (log OOn∞T)-competitive. To this end, our Prediction step aims to open
additional facilities so that for each facility in OPT, the algorithm would soon open a close enough
facility.
Uniform opening cost. There is a simple strategy that achieves this goal for the case of uniform
opening cost. Whenever the Meyerson step decides to open a facility, we further open a facility at
the predicted location. By doing so, we would pay twice as the Meyerson algorithm does in order to
open the extra facility. As a reward, when the prediction error η∞ is small, the extra facility would
be good and close enough to the optimal facility.
Non-uniform opening cost. Unfortunately, this strategy does not extend to the non-uniform case.
Specifically, opening a facility exactly at the predicted location could be prohibitively expensive as it
may have huge opening cost. Instead, one needs to open a facility that is “close” to the prediction, but
attention must be paid to the tradeoff between the opening cost and the proximity to the prediction.
3
The design of the Prediction step for the non-uniform case is the most technical and challenging of
our paper. We start with opening an initial facility that is far from the prediction, and then we open a
series of facilities within balls (centered at the prediction) of geometrically decreasing radius, while
we also allow the opening cost of the newly open facility to be doubled each time. We stop opening
facilities if the total opening cost exceeds the cost incurred by the preceding Meyerson step, in order
to have the opening cost bounded.
We show that our procedure always opens a “correct” facility f that is Θ(η∞) apart to the cor-
responding facility in OPT, and that the total cost until f is opened is merely O(OPT). This is
guaranteed by the gradually decreasing ball radius when we build additional facilities (so that the
“correct” facility will not be skipped), and that the total opening cost could be bounded by that of
the last opened facility (because of the doubling opening cost). Once we open this f , subsequent
runs of Meyerson steps would be O (log O∞)-competitive.
1.3	Related Work
OFL is introduced by Meyerson (2001), where a randomized algorithm that achieves competitive
ratios of O(1) and O(log n) are obtained in the setting of random arrival order and adversarial
arrival order, respectively. Later, Fotakis (2008) gave a lower bound of Ω( kl狭n.)and proposed a
deterministic algorithm matching the lower bound. In the same paper, Fotakis also claimed that an
improved analysis of Meyerson,s algorithm actually yields an O(忒募二)competitive ratio in the
setting of adversarial order. Apart from this classical OFL, other variants of it are studied as well.
DiVeki & Imreh (2011) and Bamas et al. (2020b) considered the dynamic variant, in which an open
facility can be reassigned. Another variant, OFL with evolving metrics, was studied by Eisenstat
et al. (2014) and Fotakis et al. (2021b).
There is a recent trend of studying online algorithms with predictions and many classical online
problems have been revisited in this framework. Lykouris & Vassilvitskii (2021) considered online
caching problem with predictions and gave a formal definition of consistency and robustness. For the
ski-rental problem, Purohit et al. (2018) gave an algorithm with a hyper parameter to maintain the
balance of consistency and robustness. They also considered non-clairvoyant scheduling on a single
machine. Gollapudi & Panigrahi (2019) designed an algorithm using multiple predictors to achieve
low prediction error. Recently, Wei & Zhang (2020) showed a tight robustness-consistency trade-
off for the ski-rental problem. For the caching problem, Lykouris & Vassilvitskii (2021) adapted
Marker algorithm to use predictions. Following this work, Rohatgi (2020) provided an improved
algorithm that performs better when the predictor is misleading. Bamas et al. (2020b) extended the
online primal-dual framework to incorporate predictions and applied their framework to solve the
online covering problem. Medina & Vassilvitskii (2017) considered the online revenue maximiza-
tion problem and Bamas et al. (2020a) studied the online speed scaling problem. Both Antoniadis
et al. (2020) and Dutting et al. (2021) considered the secretary problems with predictions. Jiang
et al. (2021) studied online matching problems with predictions in the constrained adversary frame-
work. Azar et al. (2021) and Lattanzi et al. (2020) considered flow time scheduling and online load
balancing, respectively, in settings with error-prone predictions.
2	Preliminaries
Recall that we assume a underlying metric space (X, d). For S ⊆ X, x ∈ X, define d(x, S) :=
miny∈S d(x, y). For an integer t, let [t] := {1, 2, . . . , t}. We normalize the opening cost function w
so that the minimum opening cost equals 1, and we round down the opening cost of each facility to
the closest power of 2. This only increases the ratio by a factor of 2 which we can afford (since we
make no attempt to optimize the constant hidden in big O). Hence, we assume the domain of w is
{2i-1 | i ∈ [L]} for some L. Further, we use Gk to denote the set of facilities whose opening cost
is at most 2k-1,i.e. Gk := {f ∈ F | w(f) ≤ 2k-1},∀1 ≤ k ≤ L. Observe that Go = 0.
4
Algorithm 1 Prediction-augmented Meyerson
1	initialize F — 0, FP Ju .F,FP are the set of all open facilities and those opened by PRED
2	: for arriving demand point x ∈ X and its associated fxpred do
3	:	costM(x) — MEY (x)
4	PRED (fpred, CostM(X))
5	: procedure MEY(x)
6	:	for k ∈ [L] do
7	:	fk — arg minf ∈F ∪Gk d(X, f)
8	:	δk — d(X, fk )
9	Pk — (δk-ι - δk)∕2k, where δo = d(x, F)
10	:	let sk — Pi=k pi for k ∈ [L]
11	sample r 〜Uni[0,1], and let i ∈ [L] be the index such that r ∈ [si+ι, Si)
12	:	if such i exists then
13	:	F — F ∪ fi	. open facility at fi
14	:	connect X to the nearest neighbor in F
15	:	return d(X, F) + w(fi)	. ifi does not exist, simply return d(X, F)
16	: procedure PRED(fxpred , q)
17	:	repeat
18	r — 1 d(fPred,Fp)
19	fopen — argminrdfpred f)<rw(f) . break ties by picking the closest facility to fpred
20	:	if q ≥ w(fopen) then
21	:	F — F ∪ fopen, FP — FP ∪ fopen	. open facility at fopen
22	:	q — q - w (fopen )
23	:	until q < w(fopen )
24	open facility at fopen with probability Wf-), and update F, FP
3	Our Algorithm: Prediction-augmented Meyerson
We prove Theorem 1.1 in this section. Our algorithm, stated in Algorithm 1, consists of two steps,
the Meyerson step and the Prediction step. Upon the arrival of each demand point x, we first run
the Meyerson algorithm Meyerson (2001), and let costM (x) be the cost from the Meyerson step.
Roughly, for a demand point x, the Meyerson algorithm first finds for each k ≥ 1, a facility fk
which is the nearest to x, among facilities of opening cost at most 2k-1 and those have been opened,
and let δk = d(x, fk). Then, open a random facility from {fk}k, such that fk is picked with
probability (δk-ι - δk)∕2k. For technical reasons, We do not normalize PkPk, and We simply
truncate if Pkpk > 1. Next, we pass the cost incurred in the Meyerson step (which is random) as
the budget q to the Prediction step, and the Prediction step Would use this budget to open a series
of facilities through the repeat-until loop. Specifically, for each iteration in this loop, a facility is to
be opened around distance r to the prediction, Where r is geometrically decreasing, and the exact
facility is picked to be the one With the minimum opening cost. The ratio of this algorithm is stated
as folloWs.
Theorem 3.1. Prediction-augmented Meyerson is O(min{log n, max{1, log OPT }})-competitive.
Calibrating predictions. Recall that η∞ = maxi∈[n] d(fxpred, fxopt) can be unbounded. We shoW
hoW to “calibrate” the bad predictions so that η∞ = O(OPT). Specifically, When a demand point
x arrives, We compute fx0 := arg minf ∈F {d(x, f) + w(f)}, and We calibrate fxpred by letting
fxpred = fx0 if d(x, fxpred) ≥2d(x,fx0)+w(fx0).
We shoW the calibrated predictions satisfy η∞ = O(OPT). Indeed, for every demand point x,
d(fx0 , fxopt) ≤ d(x, fx0 ) + d(x, fxopt) ≤ 2d(x, fx0 ) + w(fx0 ) ≤ O(OPT), Where the second to last
inequality folloWs from the optimality (i.e., the connection cost d(x, fxopt) has to be smaller than the
cost of first opening fXc then connecting X to fX). Note that, if η∞ = O(OPT) then OOPT = O(n),
hence it suffices to prove a single bound O(max{1, log OOnPT}) for Theorem 3.1 (i.e., ignoring the
outer min).
5
Algorithm analysis. Let Fopt be the set of open facilities in the optimal solution. We examine each
f * ∈ FoPt and its corresponding demand points separately, i.e. those demand points connecting to
f * in the optimal solution. We denote this set of demand points by X (f *).
Definition 3.2 (Open facilities). For every demand point x, let F (x) be the set of open facilities
right after the arrival of request x, F(X) be the set of open facilities right before the arrival of x,
and F (x) = F (x) \ F (x) be the set of the newly-open facilities on the arrival of x. Moreover, let
FM(x), FP(x) be a partition of F (x), corresponding to the facilities opened by MEY and PRED,
respectively. Let FM(x), FM(x), FP(x), FP(x) be defined similarly.
Definition 3.3 (Costs). Let CostM(X) = W(FM(X)) + d (x, (FM(X) ∪ FP(X))) be the the total
cost from MEY step. Let costP (X) = w(FP (X)) be the cost from PRED step. Let cost(X) =
costM(X) + costP (X) be the total cost of X.
Recall that after the MEY step, we assign a total budget of costM (X) to the PRED step. That is, the
expected cost from the Pred step is upper bounded by the cost from the Mey step. We formalize
this intuition in the following lemma.
Lemma 3.4. For each demand point X ∈ X, E[cost(X)] = 2E[costM(X)].
Proof. Consider the expected cost from the PRED step. Given arbitrary Fp , q, there is only one
random event from the algorithm and it is straightforward to see that the expected cost equals q.
Notice that our algorithm assigns a total budget of q = costM(X). Thus, E[cost(X)] = E[costM(X)+
CostP(x)] = 2 E[costM(x)].	□
Therefore, we are left to analyze the total expected cost from the Mey step. The following lemma
is the most crucial to our analysis. Before continuing to the proof of the lemma, we explain how it
concludes the proof of our main theorem.
Lemma 3.5. For every facility f* ∈ FoPt, we have
X E[costM(x)] ≤ O (max{l,logOpT}) J w(f *)+ X d(x,f *))+°
χ∈x (f*)	χ∈X(f*)	)
X(f)| ∙ OPT
n
Proof of Theorem 3.1. Summing up the equation of Lemma 3.5 for every f* ∈ FoPt, we have
E[ALG] = X E[Cost(x)] = 2 X E[CostM(x)] = 2XX
E[CostM(x)]
x∈X	x∈X	f*∈Fopt x∈X(f*)
≤ X	(O (maχ n1, log OnPT
f*∈Fopt
=O (max ni, log OnPT}) ∙ OPT
where the second equality is by Lemma 3.4 and the inequality is by Lemma 3.5, and this also implies
that the ratio is O(1) when η∞ = 0.	□
})Jw(f*)+ X d(x,f *)1 + O (lXPɪ ∙ OPT
x∈X(f*)	n
3.1	Proof of Lemma 3.5
Fix an arbitrary f * ∈ Fopt. Let '* be the integer such that w(f *) = 2'* -1,or equivalently f * ∈ g`* .
Let X(f*) = {x1, x2, . . . , xm} be listed according to their arrival order. We remark that there can
be other demand points arriving between xi and xi+1 , but they must be connected to other facilities
in the OPtima^solution. Let' be the index ' such that d(f *, F(Xi)) ∈ [2'-1,2'). ' can be negative
and if d(f *, F(x∕) = 0, let ' = -∞. Observe that d(f *, F(Xi)) and ' are non-increasing in i.
Let T be the largest integer i with d(f *,F(xi)) > min{7η∞, 4w(f *)}. Note that T is a random
variable. We refer to the demand sequence before xτ as the long-distance stage and the sequence
after xτ as the short-distance stage. In this section, we focus on the case when 7η∞ ≤ 4w(f *). The
proof for the other case is very similar and can be found in Section E.
6
Long-distance stage. We start with bounding the total cost in the long-distance stage. Intuitively,
our procedure PRED would quickly build a facility that is close to f * within a distance of O(η∞),
since it is guaranteed that for each demand point Xi, the distance between f * and its corresponding
prediction fxpired is at most η∞. Formally, we prove the following statements.
Lemma 3.6. E Pi<τ costM(xi) ≤ 2w(f*).
Proof. The proof can be found in Section A.	口
Lemma 3.7. E[costM (xτ)] ≤ 2w(f*) + 2 Pim=1 d(xi, f*).
Proof. The proof can be found in Section B.	口
Short-distance stage. Next, we bound the costs for the subsequence of demand points whose
arrival is after the event that some facility near f* is open. Formally, these demand points are xi ’s
with i > τ. The analysis of this part is conceptually similar to a part of the analysis of Meyerson’s
algorithm. However, ours is more refined in that it utilizes the already opened facility that is near f *
to get an improved O (log OOnPT) ratio instead of O (log n). Moreover, Meyerson did not provide the
full detail of this analysis, and our complete self-contained proof fills in this gap.
By the definition of T, We have d(f*, F(Xi)) ≤ 7η∞∞ for such i > T. For integer ', let l` = {i >
T | 'i = '}. Then for t such that 2t-1 > min{7η∞, 4w(f *)}, We have It = 0. Let ' be the
integer that OnPT ∈ [2'-1,2'), and let' be the integer such that min{7η∞, 4w(f *)} ∈ [2'-1, 2').
We partition all demand points with i > T into groups I≤'-ι, I',...,I' according to 'i, where
1≤'-1 = U'≤'-1 i'.
Lemma 3.8. E [Pi∈ι≤'-1 costM(g)] ≤ 2Xn0 ∙ OPT.
Proof. The proof can be found in Section C.	口
Lemma 3.9. Forevery ' ∈ [','], E [Pj costM(xJ] ≤ 18 Pii∈h d(xi,f * ) + 32w(f *).
Proof. The proof can be found in Section D.	口
Proof of Lemma 3.5. We conclude Lemma 3.5 by combining Lemma 3.6, 3.7, 3.8, and 3.9.
E [costM(Xi)] = E	costM(Xi) + E[costM(Xτ)]
i∈[m]	i∈[τ -1]
` r	-
+ E	costM(Xi) +	E [costM(Xi)]
'='	-i∈I'	-	i∈I≤'-1
≤ 2w(f*) + 2w(f*)+2Xm d(Xi,f*)
+ XX(18 X d(xi,f *)+32w(f*)) + 2X(*)| ∙ OPT
'=' ∖ i∈I'	n	n
≤ (O(' - ') + O(1)) ∙ I X d(xi,f*) + w(f*)1 + 2lXnf*)| OPT
i∈[m]
≤ O (max {l,logOPT}) ∙ ( X d(xi,f *) + w(f *) ∣ + O (JXf^ OPT).
i∈[m]	n
□
7
Table 1: Specifications of datasets
dataset	type	size	# of dimension/edges	non-uniform
Twitter	Euclidean	30k	# dimension = 2	no
Adult	Euclidean	32k	# dimension = 6	no
US-PG	Graph	4.9k	# edges = 6.6k	no
Non-Uni	Euclidean	4.8k	# dimension = 2	yes
4	Experiments
We validate our online algorithms on various datasets of different types. In particular, we consider
three Euclidean data sets, a) Twitter (Chan et al.), b) Adult (Dua & Graff, 2017), and c) Non-
Uni (Cebecauer & Buzna, 2018) which are datasets consisting of numeric features in Rd and the
distance is measured by `2, and one graph dataset, US-PG (Rossi & Ahmed, 2015) which represents
US power grid in a graph, where the points are vertices in a graph and the distance is measured by the
shortest path distance. The opening cost is non-uniform in Non-Uni dataset, while it is uniform in
all the other three. These datasets have also been used in previous papers that study facility location
and related clustering problems (Chierichetti et al., 2017; Chan et al., 2018; Cohen-Addad et al.,
2019). A summary of the specification of datasets can be found in Table 1.
Tradeoff between η∞ and empirical competitive ratio. Our first experiment aims to measure
the tradeoff between η∞ and the empirical competitive ratio of our algorithm, and we compare
against two “extreme” baselines, a) the vanilla Meyerson’s algorithm without using predictions
which we call MEYERSON, and b) a naive algorithm that always follows the prediction which we
call Follow-Predict. Intuitively, a simultaneously consistent and robust online algorithm should
perform similarly to the Follow-Predict baseline when the prediction is nearly perfect, while
comparable to Meyerson when the prediction is of low quality.
Since it is NP-hard to find the optimal solution to facility location problem, we run a simple 3-
approximate MP algorithm (Mettu & Plaxton, 2000) to find a near optimal solution F? for every
dataset, and we use this solution as the offline optimal solution (which we use as the benchmark for
the competitive ratio). Then, for a given η∞ and a demand point x, we pick a random facility f ∈ F
such that η∞∕2 ≤ d(f, Fx) ≤ η∞ as the prediction, where Fx is the point in F? that is closest to
x. The empirical competitive ratio is evaluated for every baselines as well as our algorithm on top
of every dataset, subject to various values of η∞ .
All our experiments are conducted on a laptop with Intel Core i7 CPU and 16GB memory. Since the
algorithms are randomized, we repeat every run 10 times and take the average cost.
In Figure 1 we plot for every dataset a line-plot for all baselines and our algorithm, whose x-axis
is η∞ and y-axis is the empirical competitive ratio. Here, the scale of x-axis is different since
η∞ is dependent on the scale of the dataset. From Figure 1, it can be seen that our algorithm
performs consistently better than MEYERSON when η∞ is relatively small, and it has comparable
performance to MEYERSON when η∞ is so large that the FOLLOW-PREDICT baseline loses control
of the competitive ratio. For instance, in the Twitter dataset, our algorithm performs better than
MEYERSON when η∞ ≤ 2, and when η∞ becomes larger, our algorithm performs almost the same
with Meyerson while the ratio of Follow-Predict baseline increases rapidly.
Furthermore, we observe that our performance lead over Meryerson is especially significant on
dataset Non-Uni whose opening cost is non-uniform. This suggests that our algorithm manages
to use the predictions effectively in the non-uniform setting, even provided that the prediction is
tricky to use since predictions at “good” locations could have large opening costs. In particular, our
algorithm does a good job to find a “nearby” facility that has the correct opening cost and location
tradeoff.
A simple predictor and its empirical performance. We also present a simple predictor that can
be constructed easily from a training set, and evalute its performance on our datasets. We assume the
predictor has access to a training set T. Initially, the predictor runs the 3-approximate MP algorithm
on the training set T to obtain a solution F*. Then when demand points from the dataset (i.e.,
8
2.2
0i=eH 9>wsdE0υ
Twitter dataset
0 8 6 4
. . . .
2 111
。一eH 9>0dE0
Adult dataset
O一sH 3≥1;sdEC0
US-PG dataset
Figure 1: The tradeoff between prediction error η∞ and empirical competitive ratio over four
datasets: Twitter, Adult, US-GD and Non-Uni.
12 -
,o 10 一
c2
<υ 8∙
口
E
2
0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 0.016
Error Parameter η∞
Non-Uni dataset
Table 2: Empirical competitive ratio evaluation for the simple predictor
dataset	Meyerson	Follow-Predict	Ours
Twitter	1.70	1.69	1.57
Adult	1.55	1.57	1.49
US-PG	1.47	1.47	1.43
Non-Uni	5.66	5.7	2.93
test set) X arrives, the predictor periodically reruns the MP algorithm, on the union of T and the
already-seen test data X0 ⊆ X, and update F*. For a demand point x, the prediction is defined as
the nearest facility to X in F*.
To evaluate the performance of this simple predictor, we take a random sample of 30% points from
the dataset as the training set T , and take the remaining 70% as the test set X. We list the accuracy
achieved by the predictor in this setup in Table 2. From the table, we observe that the predictor
achieves a reasonable accuracy since the ratio of Follow-Predict baseline is comparable to Meyer-
son’s algorithm. Moveover, when combining with this predictor, our algorithm outperforms both
baselines, especially on the Non-Uni dataset where the improvement is almost two times. This not
only shows the effectiveness of the simple predictor, but also shows the strength of our algorithm.
Finally, we emphasize that this simple predictor, even without using any advanced machine learning
techniques or domain-specific signals/features from the data points (which are however commonly
used in designing predictors), already achieves a reasonable performance. Hence, we expect to see
an even better result if a carefully engineered predictor is employed.
9
Acknowledgments
This work is supported by Science and Technology Innovation 2030 — “New Generation of Artifi-
cial Intelligence” Major Project No.(2018AAA0100903), Program for Innovative Research Team of
Shanghai University of Finance and Economics (IRTSHUFE) and the Fundamental Research Funds
for the Central Universities. Shaofeng H.-C. Jiang is supported in part by Ministry of Science and
Technology of China No. 2021YFA1000900. Zhihao Gavin Tang is partially supported by Huawei
Theory Lab. We thank all anonymous reviewers for their insightful comments.
References
Amir Ahmadi-Javid, Pardis Seyedi, and Siddhartha S. Syam. A survey of healthcare facility location.
Comput. Oper. Res., 79:223-263, 2017.
Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online match-
ing problems with machine learned advice. In NeurIPS, 2020.
Yossi Azar, Stefano Leonardi, and Noam Touitou. Flow time scheduling with uncertain processing
time. In STOC, pp. 1070-1080, 2021.
Etienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented energy
minimization via speed scaling. In NeurIPS, 2020a.
Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning aug-
mented algorithms. In NeurIPS, 2020b.
Matej CebecaUer and L,ubos Buzna. Large-scale test data set for location problems. Data in brief,
17:267-274, 2018.
T.-H. Hubert Chan, Arnaud Guerqin, and Mauro Sozio. Fully dynamic k-center clustering. In WWW,
pp. 579-587. ACM, 2018.
T. Hubert Chan, A. Guerqin, and M. Sozio. Twitter data set. URL https://github.com/
fe6Bc5R4JvLkFkSeExHM/k-center.
Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through
fairlets. In NIPS, pp. 5029-5037, 2017.
Vincent Cohen-Addad, Niklas Hjuler, Nikos Parotsidis, David Saulpic, and Chris Schwiegelshohn.
Fully dynamic consistent facility location. In NeUrIPS, pp. 3250-3260, 2019.
Gabriella Diveki and CSanad Imreh. Online facility location with facility movements. Central EUr
J. Oper Res.,19(2):191-200, 2011.
Zvi Drezner and Horst W. Hamacher. Facility location - applications and theory. Springer, 2002.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL https://archive.
ics.uci.edu/ml/datasets/adult.
Paul DUtting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice.
In EC, pp. 409T29, 2021.
David Eisenstat, Claire Mathieu, and Nicolas Schabanel. Facility location in evolving metrics. In
ICALP, volume 8573, pp. 459T70, 2014.
Dimitris Fotakis. On the competitive ratio for online facility location. AlgOrithmica, 50(1):1-57,
2008.
Dimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. Learning augmented
online facility location. CoRR, abs/2107.08277, 2021a.
Dimitris Fotakis, Loukas Kavouras, and Lydia Zakynthinou. Online facility location in evolving
metrics. Algorithms, 14(3):73, 2021b.
10
Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.
In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 2319-2327. PMLR,
2019.
Zhihao Jiang, Pinyan Lu, Zhihao Gavin Tang, and Yuhao Zhang. Online selection problems against
constrained adversary. In ICML, volume 139 of Proceedings of Machine Learning Research, pp.
5002-5012. PMLR, 2021.
Andreas Klose and Andreas Drexl. Facility location models for distribution system design. Eur. J.
Oper. Res., 162(1):4-29, 2005.
Gilbert Laporte, Stefan Nickel, and Francisco Saldanha da Gama. Location science. Springer, 2019.
Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling
via learned weights. In SODA, pp. 1859-1877, 2020.
Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J.
ACM, 68(4):24:1-24:25, 2021.
Andres MUnoz Medina and Sergei Vassilvitskii. Revenue optimization with approximate bid pre-
dictions. In NIPS, pp. 1858-1866, 2017.
M. Teresa Melo, Stefan Nickel, and Francisco Saldanha-da-Gama. Facility location and supply
chain management - A review. Eur. J. Oper. Res., 196(2):401-412, 2009.
Ramgopal R. Mettu and C. Greg Plaxton. The online median problem. In FOCS, pp. 339-348. IEEE
Computer Society, 2000.
Adam Meyerson. Online facility location. In FOCS, pp. 426-431. IEEE Computer Society, 2001.
Alessandro Panconesi, Flavio Chierichetti, Giuseppe Re, Matteo Almanza, and Silvio Lattanzi. On-
line facility location with multiple advice. In NeurIPS 2021, 2021.
Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions.
In NeurIPS, pp. 9684-9693, 2018.
Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In SODA,
pp. 1834-1845. SIAM, 2020.
Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
and visualization. In AAAI, 2015. URL http://networkrepository.com.
Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-augmented
online algorithms. In NeurIPS, 2020.
A	Proof of Lemma 3.6
LemmaA.1 (Restatement ofLemma 3.6). E [Pi<「CostM(xi)] ≤ 2w(f*).
Proof. Let G = {g1, g2, . . . , gt} be the facilities opened by the PRED steps on the arrivals of all
{xi }i<τ , and are listed according to their opening order. Observe that the budget that is assigned to
the PRED step is costM(xi), and the expected opening cost of each PRED step equals its budget. By
the definition of G, we have E Pi<τ costM(xi) = E Pk∈[t] w(gk) .
Next, we prove that the opening costs of gk ’s are always strictly increasing. For each k ∈
[t - 1], suppose gk is opened on the arrival of demand x and gk+1 is opened on the arrival
of demand y. Let F denote the set of all open facilities right before gk+1 is opened. Let
11
f ：= argmi□f型于产于)≤ 1 d(fξred,gk) w(f), we must have w(gk+ι) ≥ w(f), since gk+ι =
argminf 以⑷旧产)≤ 1 d(fyp^,F)w(f) and that d(fpred, F) ≤ d(fpred, gk). Moreover,
d(f,fPred) ≤ d(f,fPred) + d(fpred,fpred) ≤ 1 d(fpred, gk) + 2η∞
≤ 2 (d(fPred,gk) + d(fPred,fpred)) + 2η∞ ≤ 1 d(fPred,gk) + 3η∞ < d(fPred, gk).
Here, the second and fourth inequalities use the fact that d(fPred,fpred) ≤ d(fPred,f*) +
d(f *,fpred) ≤ 2η∞ since both x,y ∈ X(f*). The last inequality follows from the fact that
d(fPred, gk) ≥ d(gk, f *) — d(fPred, f *) > 7η∞ - η∞ = 6η∞ by the definition of T.
Consider the moment when we open gk, the fact that we do not open facility f implies that w(gk) <
w(f). Therefore, w(gk) < w(f) ≤ w(gk+1). Recall that the opening cost of each facility is a power
of 2. We have that Pk∈[t] w(gk ) ≤ 2w(gt).
Finally, we prove that the opening cost w(gt) is at most w(f *). Consider the moment when gt is
open, let x be the corresponding demand point. Notice that
d(f*,fPred) ≤ η∞ < 2d(f*,F(x)) ≤ 1 d(f*,Fp(x)),
by the definition of T. However, we open gt instead of f * on Line 20 of our algorithm. It must be
the case that w(f*) ≥ w(gt).
To conclude the proof, we have Pk∈[t] w(gk) ≤ 2w(gt) ≤ 2w(f *).	口
B	Proof of Lemma 3.7
Lemma B.1 (Restatement of Lemma 3.7). E[costM (xτ)] ≤ 2w(f *) + 2 Pim=1 d(xi, f*).
Proof. We bound the opening cost and connection cost separately.
Opening cost. By the algorithm, we know for every 1 ≤ i ≤ m,
E [w(FM(xi))1(w(F1M(xi)) > w(f *))i ≤ X '-2i ' ∙ 2i = δι* ≤ d(xi,f *).
i>l*
Hence, using the fact that at most one facility is opened by Meyerson step for each demand,
E[w(FM(xτ))] ≤ E	max w(f) ≤
f ∈FM (xm )
m
≤w(f*)+Xd(xi,f*).
i=1
This finishes the analysis for the opening cost.
w(f *)+ E	E	w(f)
f∈FM(xm),w(f)>w(f*)
Connection cost. We claim that d(xτ, F(xτ)) ≤ w(f*) + d(xτ, f*) (with probability 1).
Let δo, δι,...,δL be defined as in our algorithm. We assume δo = d(x/, F(XT)) > w(f *) +
d(xτ, f*), as otherwise the claim holds immediately. Let k ：= max{k ： δk > w(f*) + d(xτ, f*)}.
Then
Pr[d(xτ, F (xτ)) ≤ w(f*) + d(xτ,f*)]
δ G δi-1 - δi	δi-1 - δi	δk - δ'*
= min X -2-，1 ≥ min X ^`*—，1 = min (	2'*	，1) = 1.
i=k+1	i=k+1
Therefore, E[costM(xτ)] = E[w(FM(xτ))] + E[d(f *,F(xτ))] ≤ 2w(f *) + 2Pm=I d(xi,f *),
which concludes the proof.	口
12
C Proof of Lemma 3.8
Lemma C.1 (Restatement of Lemma 3.8). E
[Pi∈I≤'-ι costM(xi)] ≤
2∣x(f*)l
n
• OPT.
Proof. For i ∈ I≤'-ι, We have that its connection cost d(xi,F(Xi)) ≤ d(xi,F(xi)) ≤ 2'-1 ≤
OPT. Let δo, δι,...,δL and fι,...,fL be defined as in our algorithm upon Xi's arrival. Then, its
expected opening cost in the Mey step equals
E Iw(FM(Xi))] = X Pr [Fm(x. = {fk} ∙ w(fk) ≤ X min (Pk, 1) ∙ 2k-1
k∈[L]	k∈[L]
≤ X min (δk--δk,])• 2k-1 ≤ X δk-2-δk
k∈[L]	k∈[L]
δ0	OPT
≤ 2" ≤	2n
To sum up,
E
costM(Xi)
i∈I≤'-1
≤E
X	(d(xi, F(Xi)) + w(Fm(xJ)
i∈I≤'-1
X	-3OPT
2n
i∈I≤'-1
2xn∣ • opt.
n
≤
<
□
D Proof of Lemma 3.9
Lemma D.1 (Restatement of 3.9). For every ' ∈ [','],
EX costM(Xi) ≤ 18X d(xi,f *) + 32w(f *).
-i∈I'	」	i∈I'
Proof. Recall that we relabeled and restricted the indices of points in X to X (f *) = { x 1 ,...,Xm }.
HoWever, in this proof, We also need to talk about the other points in the original data set (that do
not belong to X(f*)). Hence, to avoid confusions, we rewrite X = {y1, . . . , yn} (so y1, . . . , yn are
the demand points in order), and we define σ : [m] → [n] that maps elements Xj ∈ X(f*) to its
identity yi ∈ X, i.e., σ(j) = i.
For i ∈ [n], let
ZM := IFM3i) FM(yi) = 0
i none otherwise ,
and define ziP similarly. Since the MEY step opens at most one facility, ziM is either a singleton or
“none”. Finally, define zi := ziM ∪ ziP .
Define t` = min{i ∈ [n] | ZM = none and d(f *,zM) < 2'-1}, we have
E X CostM(Xi)- 18d(Xi,f *) = E X (CostM(yi) - 18d(yi,f*)) 1(i ∈ σ(I')) .	(1)
-i∈I'	」	i∈[τ']
Next, we prove a stronger version by induction, and the induction hypothesis goes as follows. For
every j ∈ [n],
T'
E X (CostM(yi) - 18d(yi, f *)) 1(i ∈ σ(I')) IEj-I ≤ 32w(f *),
i=j
where Ej = (Z1, . . . , Zj). Clearly, applying this with j = 1 implies (1), hence it suffices to prove
this hypothesis.
13
Base case. The base case is j = n, and we would do a backward induction on j. Since we
condition on Ej_1, the variables ^,...,坛 in the Mey step of y§, as well as wether or not j ∈ σ(I'),
are fixed. Hence,
一 ..._ 一 一 ɪ. .. A δ—ι -δ , 一，一
E[costM(yj) I Ej-1] ≤ d(yj, F(yj))十 E —2-------2 ≤ 2d(%, F(yj)).
i=1
This implies
t`
E X (COStM(y) - 18d(yi, f *)) 1(i ∈ σ(Ie)) | Ej-I
i=j
≤ E[(costM(yn) - 18d(yn,f*))1(n ∈ σ(Ie)) | En-i]
≤ 2d(yn,F(yn)) - 18d("f*) ∣n∈σ(le)
≤ 2d(yn,F(yn)) - 2d(yn,f *) |八以屹)
≤ 2d(F(yn),f *) uʃ(i`)
≤ 2'+1 ≤ 2'* + 1 ≤ O(w(f*)).
Inductive step. Next, assume the hypothesis holds for j + 1,j + 2,...,n, and we would prove
the hypothesis for j . We proceed with the following case analysis.
Case 1:	j ∈ σ(I'). We do a conditional expectation argument, and we have
T'
E X (COStM(yi) - 18d(yi, f *)) 1(i ∈ σ(Ie)) ∣ Ej-I
_i=j	_
___	t`
=X PrlzjM Zj I Ej-1] E X (COStM(yi) - 18d(yi,f *)) 1(i ∈ σ(Ie)) ∣ Ej-1, zM, zP
zM,zp	Li=j	.
≤ 32w(f*).
where the second step follows by induction hypothesis.
Case 2:	j ∈ i` and 8d(%,f*) > d(f *,F(y∙)).Wehave
d(yj ,F(yj)) ≤ d(yj ,f *)+ d(f *,F(%∙)) ≤ 9d(%∙ ,f *).
Hence E[costM(yj) - 18d(yj, f *)] ≤ 0, and the hypothesis follows from a similar argument as in
Case 1.
Case 3:	j ∈ l`, and 8d(yj, f *) ≤ d(f *, F(yj)). We have
⅛* ≤ d(yj ,f *) ≤ 1 d(f *,F(yj)) ≤ 2'-3.
8
Let D = {f : d(f *, f) ≥ 2'-1} U {none}. Let k = min{k0 ∣ δk ≤ 2'-2}. Then Vi ≥ k,
d(fi,f *) ≤ d(yj ,f,) + d(yj ,f *) < 2'-1.
Pr [t` ≤ j ∣ Ej-1] ≥ Pr [zj4 ∈ D ∣ Ej-1] ≥ Pr [f, is open for some i ≥ k ∣ Ej-1]
=min (X pi, 1J =min (X δi-2- δi, 1)
≥ min ("J, 1) ≥ min (2**2-3, 1) ≥ min(2'-3-'*, 1
≥	δ0	≥ E[costM(yj) ∣ Ej-1]
—16w(f *) —	32w(f *)	,
14
where the second last inequality follows from δo = d(yj,F(yj)) ≤ 2' and w(f *) = 2'*-1. Thus,
LHS = E[costM (yj) | Ej-1]
T'
+ E EPr[zM = tj,Zj I Ej-i] E E (CostM(yi) - 18d(yi, f *)) 1(i ∈ σ(I')) | Ej-ι,zM = tj, zP
tj ∈D zjP	i=j +1
≤ Pr [zM ∈ D I Ej-i] ∙ 32w(f*) + Pr [zM ∈ D ∣ Ej-1] ∙ 32w(f*)
≤ 32w(f*),
where the second step follows by induction hypothesis. In summary, we have
" T'	-
E X (CostM(yi) - 18d(yi,f *)) 1(i ∈ σ(I')) ∣ Ej-I ≤ 32w(f*)
i=1
□
E	Proof of Lemma 3.5: WHEN 7η∞ > 4w(f*)
The proof is mostly the same as in the other case which We prove in Section 3.1, and We only
highlight the key differences. We use the same definition for parameters T, ', and '. It can be
verified that Lemma 3.7, 3.8 and 3.9 still holds and their proof in Section 3.1 still works.
However, Lemma 3.6 relies on that 7η∞ ≤ 4w(f *), which does not work in the current case. We
provide Lemma E.2 in replacement of Lemma 3.6, which offers a slightly different bound but it still
suffices for Lemma 3.5.
Lemma E.1. For every i < τ, d(xi, f*) ≥ w(f*).
Proof. We prove the statement by contradiction. Suppose d(xi, f*) < w(f *). Let δ0, δ1, . . . , δW
be defined as in our algorithm. Then, we have
δo = d(xi, F(Xi)) ≥ d(f *,F(xi)) - d(f *,xi) ≥ d(f *, F(Xi))- w(f *) > 3w(f *),
where the last inequality follows from the definition of T that d(f *, F(Xi)) > 4w(f*). Let k =
min{k I δk ≤ 3w(f *)}. We have k ≥ 1. We prove that a facility within a distance of 3w(f *) from
Xi must be open at this step.
Pr [d(Xi, F(Xi)) ≤ 3w(f *)] ≥ Pr [fj is opened for some j ≥ k]
≥ min (XPk,l) ≥ min (X %-；- % , 1
j ≥k
δk-1 - δ'*
≥ min(—2*—
j=k
, 1 ≥ min
3w(f*)-w(f*)
2'*
,1 =1,
where the last inequality uses the fact that δ'* ≤ d(xi, f *) ≤ w(f *). Consequently,
d(f *, F(xi+ι)) ≤ d(f *,F(xi)) ≤ d(xi, F(Xi)) + d(xi, f *) < 4w(f *),
which contradicts the definition of T .
□
LemmaE.2. E [Pi∈[τ] cost(xi)] ≤ 6E[Pi∈[τ] d(xi,f *)]+4 ∙ w(f *).
Proof. on the arrival of Xi for each i ∈ [T], let δ0, δ1, . . . , δL be defined as in our algorithm. We
first study the connecting cost of request Xi . We have
d(xi, F(Xi)) ≤ max (d(xi,F(xj∙)∖F(xi)),d(xi,F(xi))) = max (δ°,d(xi,F(xi))).
15
We prove this value is at most d(χ%, f *) + 2w(f *). Suppose δo > d(xi, f *) + 2w(f *), as otherwise
the statement holds. Recall that δj is non-increasing, let k = min{k | δk ≤ d(xi, f *) + 2w(f *)}.
By the definition of k, we have that
Pr [d(xi, F(xi)) ≤ d(xi, f*) + 2w(f *)] ≥ Pr[fj is opened for some j ≥ k]
≥ min (X pj, 1) ≥ min (X δj-2-δj, 1) ≥ min (δk-2- δ`*, 1)
j≥k	j≥k
d(xi,f *)+2w(f *) - δ'*	w(f *)
≥ min (--------2'*---------, 1) ≥ min (2*≡1
To sum up, we have shown that the connecting cost d(xi, F (xi)) ≤ d(xi, f*) + 2w(f *).
Next, We study the expected opening cost E[w(F(χJ)]. We have
E[w(F(xi))] = X Pr [F(xi) = {fj}] ∙ w(fj) ≤ X min ("- % , 1) ∙ 2j-1
j∈[L]	j∈[L]
≤ X 2j-1 + X δj-2- δj ∙ 2jT < 2'* + 1 δ'* <2w(f *) + d(xi,f *).
j≤'*	j>'*
By Lemma E.1, we conclude the proof of the statement:
EXcost(xi) = E X (d(xi,F(xi)) + W(F(Xi)))
i∈[τ]	i∈[τ]
≤ E X (2d(xi,f *) + 4w(f*))
i∈[τ]
≤6E X d(xi, f*) + (2d(xτ, f*) + 4w(f*))
i<τ
≤ 6E X d(xi,f*) +4w(f*).
i∈[τ]
□
F Lower B ound
In the classical online facility location problem, a tight lower bound of O(成蓝n) is established
by Fotakis (2008), even for the special case when the facility cost is uniform and the metric space is
a binary hierarchically well-separated tree (HST). We extend their construction to the setting with
predictions, proving that when the predictions are not precise (i.e. η∞ > 0), achieving a competitive
1 nη∞
ratio of o( log OPn) is impossible.
Theorem F.1. Consider OFL with predictions with a uniform opening cost of 1. For every η∞ ∈
log nη∞∖
(0, 1], there exists a class of inputs, such that no (randomized) online algorithm is o( IogOgn)-
competitive, even when η1= O(1).
Before we provide the proof of our theorem, we give some implications of our theorem. First of all,
we normalize the opening cost to be 1. Consequently, the optimal cost is at least 1. Furthermore,
we are only interested in the case when η∞ ≤ 1. Indeed, the optimal facility for each demand point
must be within a distance of 1, as otherwise, we can reduce the cost by opening a new facility at the
demand point. Therefore, we can without loss of generality to study predictions with η∞ = O(1).
log nη∞
Without the normalization, our theorem implies an impossibility result of o( log OPn) for online
facility location with predictions.
16
Moreover, recall the definition of total prediction error η1 = Pin=1 d(fxpired, fxoipt), which is at least
η∞ = maxi d(fxpred , fxopt). Our theorem states that even when the total error η1 is constant times
larger than the opening cost of 1, there is no hope for a good algorithm. Note that our bound above
holds for any constant value of η∞. As an implication, our construction rules out the possibility of
/ log η1r-r
an o( log Ogn)-competitive algorithm.
Proof. By Yao’s principle, the expected cost of a randomized algorithm on the worst-case input
is no better than the expected cost for a worst-case probability distribution on the inputs of the
deterministic algorithm that performs best against that distribution. For each η∞ , we shall construct
a family of randomized instance, so that the expected cost of any deterministic algorithm is at least
一 ,ι nη∞ .
。(ι⅛n )-ssmiva
We first import the construction for the classical online facility location problem by Fotakis (2008).
Consider a hierarchically well-separated perfect binary tree. Let the distance between root and its
children as D. For every vertex i of the tree, the distance between i and its children is mɪ- times
the distance between i and its parent. That is to say, the distance between a height i vertex and its
children of height i + 1 is 黑.Let the height of the tree be h.
The demand sequence is consisted of h + 1 phases. For each 0 ≤ i ≤ h, in the (i + 1)-th phase, mi
demand points arrive consecutively at some vertex of height i. For i ≥ 1, the identity of the height i
vertex is independently and uniformly chosen between the two children of i-th phase vertex.
Consider the solution that opens only one facility at the leaf node that is the (h+ 1)-th phase demand
vertex and assign all demand points to the leaf. The cost of this solution equals
h	h-1	h-1
1 + X mi X — ≤ 1 + X mi —
mj	mi m - 1
i=0	j=i	i=0
m
m-1
1 + h
This value serves as an upper bound of the optimal cost. I.e., OPT ≤ 1 + hD m―.
Next, we describe the predictions associated with the demand points. Intuitively, we try the best to
hide the identity of the leaf node in the last phase. We denote the leaf node as f *, which is also the
open facility in the above described solution. Our prediction is produced according to the following
rule. When the distance between the current demand point and f * is less than η∞,let the prediction
be the same as the demand vertex. Otherwise, let the prediction be the vertex on the path from the
current demand point to f*, whose distance to f* equals η∞.
We prove a lower bound on the expected cost of any deterministic algorithm. We first overlook the
first a few phases until the subtree of the current demand vertex has diameter less than η∞ . Let
it be the h0-th phase. We now focus on the cost induced after the h0-th phase and notice that the
predictions are useless as they are just the demand points. When the mi demand points at vertex of
height i comes, we consider the following two cases:
Case 1:	There is no facility opened in the current subtree. Then we either open a new facility
bearing an opening cost 1 or assign the current demands to facilities outside the subtree bearing a
large connection cost at least m-τ per demand. So the cost of this vertex is min{1, Dm}.
Case 2:	There has been at least one facility opened in the current subtree. Since the next vertex
is uniformly randomly chosen in its two children, the expected number of facility that will not enter
the next phase subtree is at least 11. We call it abandoned. The expected sum of abandoned facility is
2 of the occurrence of case 2. Thus the expected cost in every occurrence of case 2 is 1.
We carefully choose D, m, h so that 1) mD = 1; 2) the total number of demand points is n, i.e.
h
P mi = n; and 3)-D = η∞. These conditions give that (h + 1) log m = log n, (h0 + 1) log m =
i=0	m
-log η∞. Consequently, h - h0 = ⅛nη∞.
To sum up, an lower bound of any deterministic algorithm is (h - h0)min{ 1 ,mD} = Ω Coonn∞),
while the optimal cost is at most 1 + hD m- = O (Joggnm ). Setting m =忒蔑 n, the optimal
17
cost is O⑴.And We prove the claimed lower bound of Ω( lθg n^). Finally, it is straightforward to
see that the summation of the prediction error ηι ≤ OPT = O(1).	口
G t-OUTLIER SETTING
Observe that our main error parameter η∞ could be very sensitive to a single outlier prediction
that has a large error. To make the error parameter bahave more smoothly, we introduce an integer
parameter 1 ≤ t ≤ n, and define η∞(t) as the t-th largest prediction error, i.e., the maximum prediction
error excluding the t - 1 high-error outliers. Define η1(t) similarly.
In the following, stated in Theorem G.1, we argue that our algorithm, without any modification (but
with an improved analysis), actually has a ratio of O(log(1 + t) + log O∞t) that holds for every
n (t)
t. This also means the ratio would be minι≤t≤n O(log(1 +1) + log O∞t), and this is clearly a
generalization of Theorem 1.1, since η∞(t) = η∞ when t = 1.
Moreover, we note that our lower bound, Theorem 1.2, is still valid for ruling out the possibility
of replacing η∞(t) with η1(t) in the abovementioned ratio, since one can still apply Theorem 1.2 with
t = 1. However, it is an interesting open question to explore whether or not an algorithm with a
(t)
ratio like O (log t + OPT) for some specific t (e.g., t ≥ √n) exists.
Theorem G.1. For every integer 1	≤ t ≤ n, Algorithm 1 is O(log(t + 1) +
min {log n, max{1, log OPT }})-competitive.
Proof sketch. Let Γ = (γ1, γ2, . . . , γt-1) be indices of the t - 1 largest prediction error
d(fPγed,fOpt). The high level idea is to break the dataset X into a good part XG := {xi : i ∈ [n]∖Γ}
and a bad part XB := {xi : i ∈ Γ} according to whether or not the prediction is within the outlier,
and we argue that the expected cost of ALG on XG is O( noη∞) times larger than that in OPT, and
show the expected cost on XB is O(log(t + 1)) times.
For the good part Xg, we let X-(f *) := X(f *)∖Γ and apply a similar analysis as in the proof of
Lemma 3.5 to show that
cost(x)
χ∈X- (f*)	.
E
≤ O (maχ (ι, log OPT ))Qf *)+	X d(x,f *j.
For the bad part XB, we assume all predictions for points in XB are of infinite error (which could
only increase the cost of the algorithm), and let X+ (f *) := X(f *) ∩ Γ. Then this lies in the case of
Section E, where 7η∞∞ > 4w(f *), which essentially means the prediction is almost useless and the
whole proof reverts to pure Meyerson’s algorithm. Combine the arguments from Section E and the
analysis for the short distance stage, we have
E E	cost(x) ≤ O(log(t + 1)) ∣w(f*)+	E	d(x,f *)
x∈X+(f*)	x∈X+(f*)
We finish the proof by combining the bound for the two parts.
□
18