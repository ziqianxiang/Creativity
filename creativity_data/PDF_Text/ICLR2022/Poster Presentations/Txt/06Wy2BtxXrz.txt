Published as a conference paper at ICLR 2022
Learning Scenario Representation for Solv-
ing Two-stage Stochastic Integer Programs
YaoXin Wu1*, Wen Song2, ZhigUang Cao3, Jie ZhangI
1 School of Computer Science and Engineering, Nanyang Technological University, Singapore
2Institute of Marine Science and Technology, Shandong University, Qingdao, China
3Singapore Institute of Manufacturing Technology, A*STAR, Singapore
{wuyaoxin,zhangj}@ntu.edu.sg; wensong@email.sdu.edu.cn
Ab stract
Many practical combinatorial optimization problems under uncertainty can be
modeled as stochastic integer programs (SIPs), which are extremely challenging
to solve due to the high complexity. To solve two-stage SIPs efficiently, we pro-
pose a conditional variational autoencoder (CVAE) based method to learn scenario
representation for a class of SIP instances. Specifically, we design a graph con-
volutional network based encoder to embed each scenario with the deterministic
part of its instance (i.e. context) into a low-dimensional latent space, from which a
decoder reconstructs the scenario from its latent representation conditioned on the
context. Such a design effectively captures the dependencies of the scenarios on
their corresponding instances. We apply the trained encoder to two tasks in typi-
cal SIP solving, i.e. scenario reduction and objective prediction. Experiments on
two graph-based SIPs show that the learned representation significantly boosts the
solving performance to attain high-quality solutions in short computational time,
and generalizes fairly well to problems of larger sizes or with more scenarios.
1	Introduction
Stochastic integer programs (SIPs) are a class of combinatorial optimization problems (COPs) with
uncertain parameters. They focus on finding a best decision for a practical task to optimize both
the deterministic cost and the expectation of stochastic costs on a set of potential scenarios. Since
the uncertain elements commonly exist in reality, SIPs are widely applied and studied in many fields
such as transportation, inventory management, energy optimization, and so on (Jammeli et al., 2019;
Dillon et al., 2017; Bruno et al., 2016; Wallace & Ziemba, 2005). However, optimally solving SIP
is intractable especially when many scenarios are involved, due to the fact that: 1) most integer
programs themselves are already NP-hard; 2) considerable (even continuous) scenarios significantly
increase the computational complexity (Klein, 2021). In reality, solving moderate-sized problems
may require prohibitively long time. Thus efficient and high-quality approximate solutions for SIPs
are often practically pursued.
The advances in deep (reinforcement) learning has aroused extensive studies on solving optimization
problems with neural networks. The related works have shown promising results in tackling some
classic COPs, e.g., vehicle routing, job shop scheduling, combinatorial auction, etc. (Kool et al.,
2019; Zhang et al., 2020; Lee et al., 2020; Kwon et al., 2020; Gasse et al., 2019; Chen & Tian, 2019;
Ichter et al., 2018; Hottung et al., 2020; Wu et al., 2021). However, most of the existing deep learning
based methods only focus on deterministic problems, which could hinder their practical applications
in the uncertain situation. On the other hand, a few early attempts introduce deep learning to solve
SIPs (Larsen et al., 2018; Abbasi et al., 2020; Joe & Lau, 2020; Bengio et al., 2020; Nair et al.,
2018). However, they are generally designed for only specific problems and also lack the ability of
generalizing across different problem sizes.
In this paper, we focus on graph-based SIPs, which is a general class of problems with broad ap-
plications. We propose to learn the deep latent representation of scenarios in SIPs, which can be
* Equal contribution.
,Corresponding author (Zhiguangcao@oUtlook.com).
1
Published as a conference paper at ICLR 2022
applied to various downstream tasks in solving SIPs, e.g., scenario reduction and objective predic-
tion. Based on the basic two-stage form of SIPs, we leverage conditional variational autoencoder
(CVAE) to learn latent and continuous representations of stochastic scenarios while considering the
deterministic part of a SIP instance, which we term the context. Specifically, we employ a graph
convolutional network (GCN) to learn embeddings of each deterministic integer program, which
models the first-stage problem with a single scenario. The learned embeddings are mapped into
continuous representations, and then decoded back into corresponding scenarios given the embed-
ding of the context. This design ensures that the learned representations of scenarios are correctly
linked to the corresponding instances.
The resulting latent space well represents scenarios in a class of SIP instances, and we demonstrate
its power with two tasks commonly used in solving SIPs. First, we leverage it for scenario reduction
by finding representative scenarios via an off-the-shelf clustering algorithm, which decreases the
problem complexity and meanwhile attains high-quality approximate solutions. Second, we further
extend the encoder with an additional sub-network to predict objective values for each scenario. Due
to the generalization in the latent continuous space, we readily deploy the prediction task in semi-
supervised manner with a small number of target objective values. In doing so, the latent space is
augmented with the predicted objective value and hence yields more representative scenarios, which
further shrinks the approximation gap to the optimal solutions of the original SIPs.
In summary, this paper aims to learn scenario representation for solving graph-based two-stage SIPs.
The main contributions are as follows:
•	We firstly present a deep generative framework to learn representations for scenarios in graph-
based two-stage SIPs. It considers dependencies of the scenarios on the deterministic context of
each instance, and hence well differentiates scenarios in the latent space.
•	We apply the learned representations to scenario reduction and objective prediction for SIPs. The
representative scenarios derived from clustering of the representations can be used to efficiently
compute high-quality solutions for SIPs. Moreover, we further shrink the approximation error by
predicting objective values of scenarios via semi-supervised learning.
•	We evaluate the proposed method on two SIP problems, i.e., the network design problem and
facility location problem. Experimental results verify the superiority of the learned scenario rep-
resentation for solving SIPs. Our method significantly outperforms existing deep learning based
baselines. Notably, it also generalizes well to problems with larger sizes or more scenarios.
2	Related Work
Learning based methods for SIPs. Recent works of deep learning for COPs mostly address de-
terministic problems, with scarce attention to uncertainty that commonly exist in practice. Though
a few methods apply deep learning to improve approximate solutions for two-stage SIPs, they are
mostly designed for some specific problems. Nair et al. (2018) train a bit-flipping policy with re-
inforcement learning to improve solutions iteratively, but it is limited to binary decision variables
and assumes no constraints in the first-stage problem. Larsen et al. (2018) predict properties of op-
timal solutions to integer programs (i.e. the first-stage problem with respective single scenario) in a
supervised manner. This method is only applicable when operational solutions are not needed. In
contrast, Abbasi et al. (2020) predict values of decision variables for blood transshipment problem
with fixed-sized instances. To reduce the complexity of SIPs, Bengio et al. (2020) aim to yield one
scenario for the first-stage problem, so that the derived solution achieves a small objective value
when evaluated with the original set of scenarios. This method assumes complete recourse, with
any generated scenario being feasible for the second-stage problem. All the above methods attempt
to reduce the expected cost of stochastic scenarios but their applications are limited by distinct as-
sumptions or fixed problem sizes. In contrast, our method can be used for more general two-stage
SIPs, and generalizes well to varying problem sizes or scenario quantities.
Scenario reduction for SIPs. In the SIP formulation, a set of scenarios is used to model stochastic
events on top of the first-stage problem. It introduces extra complexity which may exponentially
increase with its cardinality, owing to the combinatorial search space (Dyer & Stougie, 2006). Since
the direct optimization with massive scenarios is intractable, the scenario reduction is often con-
ducted to find a subset of scenarios that well replaces the original set with a small approximation
2
Published as a conference paper at ICLR 2022
error. Existing reduction methods mainly branches into distribution-oriented and problem-oriented
paradigms. The former ones pursue a group of scenarios that closely estimate the probability distri-
bution under the scenario space (Henrion & Romisch, 2017). The distance between the estimated
and actual probability are often measured by Wasserstein distance (Rujeerapaiboon et al., 2018),
polyhedral discrepancy (Henrion et al., 2008) or difference of moments (H0yland et al., 2003).
These methods neglect the influence of the context on the objective function, e.g., similar scenario
distributions may cause different expected costs in disparate SIP instances. Thus, there is a re-
cent trend towards the study of problem-oriented methods (Henrion & Romisch, 2018; Fairbrother
et al., 2019; Keutchayan et al., 2020; 2021), which attempts to involve problem-specific properties
in scenario reduction. For example, Keutchayan et al. (2021) propose to find K scenarios with their
objective values separately approximating the expected objective values in the K scenario subsets.
Most the above methods are mainly for theoretical proof on stochastic programs (without the restric-
tion of integers). In contrast, we firstly propose a deep learning method for SIPs to learn scenario
representation, which can be used for both scenario reduction and objective prediction.
3	Preliminaries
3.1	Two-stage stochastic integer programs
As an optimization problem, SIP is usually characterized by both the uncertain parameters that
potentially follow certain probability distributions and the discrete solution space due to the integer
restrictions. It is commonly described by a two-stage formulation as below:
min μ>x + Eω [Q(x,ω)]	(1)
x
s.t. Ax ≤ b, x ∈ Rn1-p1 × Zp1,	(2)
where Q(x, ω) := min#{q>y∣Wωy ≤ hω - TLx； y ∈ Rn2-p2 X Zp2}.
In particular, Eq. (1) and (2) prescribe the first-stage problem with x ∈ Rn1 being the decision
variable, where n1 ≥ p1 ; Q(x, ω) prescribes the second-stage problem, with y ∈ Rn2 being the
decision variable, where n ≥ p2. We refer to the group of static parameters μ ∈ Rn1, A ∈ Rm1 ×n1
and b ∈ Rm1 in the first stage as the context, and assume that the group of uncertain parameters
qω ∈ Rn2, Wω ∈ Rm2 ×n2, Tω ∈ Rm2 ×n1 and hω ∈ Rm2 follows a distribution P. Here we
focus on SIPs defined on graphs, which are a family of problems with practical applications in many
domains such as networks, transportation and scheduling (Rahmaniani et al., 2018; An & Lo, 2016).
The primary method to solve the above SIP is sample average approximation (Kleywegt et al.,
2002), which converts it into a mixed integer program (MIP) by Monte Carlo simulation and in turn
optimizes the following objective:
O(x) := min μ>x + ɪχ∖Q(x,ωi),	⑶
x	N	i=1
where {ωi}iN=1 is a set of scenarios from P, i.e., an independently and identically distributed (i.i.d.)
random sample of N realizations of uncertain parameters. Typically, a large scenario set is often
required to make the distribution of scenarios Pe well fit P, which may cause intractable MIP for a
given solver. In this paper, we aim to find a small number of informative representatives in {ωi}iN=1
by learning representations of scenarios, which is supposed to considerably boost the computation
efficiency with tolerable approximation errors.
3.2 Conditional Variational Autoencoder
As an unsupervised generative model, CVAE is developed on top of VAE, and it further controls
the data generation process conditioned on additional random variables (Sohn et al., 2015). These
conditional variables could be either class labels or certain properties of the data with specific dis-
tributions, which are engaged in the input to both the encoder and decoder. Optimized with the
stochastic gradient variational bayes (SGVB) framework (Kingma & Welling, 2014), the evidence
lower bound objective (ELBO) of CVAE on the marginal likelihood for the input data is defined as:
logPθ(X,c) ≥ ELBO(X,c)= Eqφ(z∣χ,c)[logPθ(X∣z,c)] - KL[qφ(z∣X,c)kp(z∣c)],	(4)
3
Published as a conference paper at ICLR 2022
where X, c, z denote input, latent and conditional variables, respectively; both the encoder qφ and
decoder pθ are typically parameterized by deep neural networks. Rather than simple distributions
of conditional variables in most existing works, in this paper we exploit CVAE to learn scenario
representation in SIPs, with conditional variables derived from continuous parameters in the context.
For more details of CVAE, we refer readers to (Kingma et al., 2014; Feng et al., 2021).
4	Methodology
Given a class of SIP instances {Xm}mM=1 with parameters drawn from a distribution D, we regard
each instance as a 2-tuple Xm = (Dm, {ωmi }iN=1), where Dm denotes the context (i.e. the group
of static parameters) in the m-th instance, and ωmi denotes the i-th scenario (i.e. the i-th realization
of uncertain parameters) in the m-th instance. We aim to learn latent representations (variables)
{zmi }iN=1 of scenarios {ωmi }iN=1 under the context Dm in each instance.
4.1	CVAE for scenario representation
Our CVAE comprises an encoder for the inference process and a decoder for the generation process,
which are parameterized by deep neural networks qφ and pθ , respectively. In the generation process,
the decoder approximates the posterior distribution of uncertain parameters in scenarios, given the
latent and conditional variables, such that:
pθ (ωm |zm, cm) = f(ωm; zm, cm, θ); qφ(cm |Dm) = h(cm; Dm, φ),	(5)
where f and h denote likelihood functions (e.g. Gaussian or multinomial distributions) that are
instantiated by pθ and qφ , respectively. Note that we do not take the raw context Dm as condi-
tional variables for the decoder since they are continuous and high-dimensional. It may not only
intensify the computational complexity but also overwhelm the latent variable which is typically
low-dimensional. In contrast, we leverage the graph neural network (GNN) in the encoder to derive
a low-dimensional conditional variable cm from Dm . Further by preserving the marginal inde-
pendence between latent and conditional variables, the decoder will more effectively govern the
generation of ωmi according to the context Dm (Kingma et al., 2014).
In the inference process, the encoder approximates the posterior distribution of latent representations
(variables) of scenarios given (ωmi , Dm), which is expressed as:
qφ(Zm UM，Dm)= N(zmlμφ(ωm, Dm), σφ(ωm, Dm)),	⑹
where We assume a Gaussian distribution of scenario representations with μφ and σφ being param-
eterized by neural networks, respectively. As aforementioned, we exploit a GNN in the encoder
to embed the continuous and high-dimensional ωmi and Dm , which is also used by the decoder to
embed the sole Dm. This parameter sharing design contributes to a fast learning of embeddings for
deriving the latent and conditional variables, i.e., zmi and cm . To learn disentangled representations
for these two types of variables, we force the conditional independence as:
qφ (zm ,
Cm | ωm,Dm)= qφ(Zm lωm, Dm)qφ(CmIDm).	(7)
In summary, our CVAE hinges on two important steps, i.e., 1) we first use the encoder to attain
scenario representations with respect to the context in each SIP instance; 2) we then use the decoder
to reconstruct the scenarios from the latent space according to the context.
Semi-supervised CVAE. With the inferred scenario representations, we can directly conduct vari-
ous downstream tasks for solving SIPs, such as scenario reduction using an off-the-shelf clustering
algorithm. However, it may ignore the relationship between scenarios and objective values, which is
nontrivial since even similar scenarios would deliver different solutions. Therefore, we also extend
CVAE to predict the objective function in a semi-supervised manner. Given that the scenarios are
embedded into a continuous space by the encoder, the prediction is supposed to be well general-
ized (Gomez-Bombarelli et al., 2018).
Most CVAE models for semi-supervised learning predict discrete targets and regard the ground
truth, e.g., class labels, as conditional variables. In contrast, we predict a continuous property of the
data (i.e. the objective value) and directly approximate it through the encoder. To this end, we use
4
Published as a conference paper at ICLR 2022
Figure 1: CVAE structure for learning scenario representations
CPLEX solver to collect optimal objective values {刈}N=ι of the resulting MIP problems, which
are defined by respective scenarios with the context, for only a small volume (1% ∙ M) of instances
in {Xm }mM=1. We refer to the underlying joint distribution of (ωmi , Ymi , Dm ) as DY . Then we infer
the objective values with a sub-network, which processes the GNN embedding of (ωmi , Dm ) in the
encoder, such that the objective function σ is parameterized as:
rψ(YmIhm) = σ(Ym;him,ψ); qφ(hm∖ωim,Dm) = g(him;ωim,Dm,φ),	⑻
where him denotes the GNN embedding that is derived by qφ.
4.2	Neural networks
A major challenge to parameterize CVAE for SIPs is how to derive an effective embedding of the
context, since it is high-dimensional and continuous. Moreover, it is also involved in the input to
both the encoder and decoder, and significantly influences the eventual scenario representation. A
direct use of multi-layer perceptron (MLP) to process static parameters in Dm may fail to leverage
the problem structure and cannot generalize across instances of different sizes. To overcome these
limits, we exploit a GNN (as aforementioned) to embed the context Dm for attaining conditional
variables in the decoder. Meanwhile, we apply the same GNN to embed each scenario with its
context (ωmi , Dm) for attaining latent representations in the encoder. To this end, we first describe
the context with scenarios on a graph.
SIP graphs. We define a complete graph G = (V,E), where V = {vι, ∙∙∙ , Vn} denotes nodes
with features V ∈ Rn×dv ; E = {ejk|vj, vk ∈ V} denotes edges with features E ∈ Rn×n×de. We
formally represent the features on the j-th node as vjωi = [sjωi ; dj] ([;] means concatenation). In
specific, sjω ∈ ωmi denotes a realization of uncertain parameters on the j -th node and dj ∈ Dm
denotes static parameters on the j-th node. Similarly, the features on edge ejk is represented as
ejωk = [sjωk ; djk], where sjωk and djk denote the parameters from the i-th scenario and the context,
respectively. Different from existing works that only consider varying parameters on nodes, e.g.,
Nazari et al. (2018), our graph representation is more generic and could be applied to a broader class
of SIPs. Particularly, two examples of the graph representation on the network design problem and
facility location problem are introduced in Section 4.4.
On top of the SIP graphs, we can exploit GNN to derive embeddings for either the context Dm
or varying scenarios with the context, i.e., (ωmi , Dm), i = {1, . . . , N}. In our method, we adopt
the graph convolutional network (GCN), which is a commonly used GNN variant (Kipf & Welling,
2017; Yao et al., 2019). The architecture of our GCN is similar to the one in Joshi et al. (2019),
which was leveraged to compute an adjacency matrix for solving the travelling salesman problem.
Specifically, we extend it for graph-based SIP problems to attain graph embeddings for Dm and
(ωmi , Dm), denoted as hm and him, respectively. Detailed structure of the GCN is described in
Appendix A. Next, we will revisit our CVAE given the graph embeddings above.
Encoder. The encoder processes the graph embedding him with two separate linear projections to
compute 2-dimensional vectors of means and standard deviations for latent variables, respectively,
i.e., μφ(ωm, Dm) and σφ(ωiz, Dm) in Eq. (6). Also, We linearly project the graph embedding hm
into a 2-dimensional vector, i.e., cm in Eq. (5).
5
Published as a conference paper at ICLR 2022
Decoder. We concatenate cm with the latent variables zm* i 2 3 * 5 and pass them through an MLP to recon-
struct the scenario. The MLP is structured by two hidden layers with ReLU activation functions,
which are of 128 and 256 dimensions, respectively.
Semi-supervised learning. To echo the semi-supervised CVAE in Section 4.1, we leverage another
MLP as a sub-network to process him. It comprises a 512-dimensional hidden layer with a ReLU
activation function, and outputs a single value to estimate the objective value Ymi .
The basic structure of neural networks is illustrated in Figure 1, where ωjn and Ym are the recon-
struction and predicted objective value for the i-th scenario, respectively. Note that we pad uncertain
parameters ωmi with 0 to embed the sole context (in the lower grey square). In this way, the GCN
could be shared to attain both graph embeddings hm and him .
4.3	Training & inference
The training objective is to minimize three loss
functions. The first one is defined by the mean
square error (MSE) between the reconstructed
scenarios from decoder and the original ones. It
is essentially employed in CVAE to maximize
the marginal likelihood of the data. The second
loss is defined by Kullback-Leibler divergence
from the latent variables to the prior Gaussian
distribution p(z) = N (z|0, I), which is used
for regularization. The third loss is defined by
the MSE between the predicted objective values
and their ground truth. Formally, the objective
for training our CVAE is expressed as:
Algorithm 1 Training procedure
Input: encoder pθ , decoder qφ , num. of epochs E , batch size
B.
1: initialize parameters θ(0) and φ(0)
2: for epoch = 1, 2, . . . , E do
3:	shuffle the data in D and DY ;
4： for t = 1, 2,..., b 1D1C do
5:	retrieve a batch of instances from D;
6： get a random scenario in each instance;
7： compute elbo in eq. (9);
8： θ,φ . Adam(θ, φ, VL(θ, φ));
9： if objective prediction is trained then
10：	retrieve a batch of instances from DY ;
11：	compute mse of objectives in eq. (9);
12：	φ,ψ — Adam(φ,ψ, VL(φ, ψ));
13： end if
14： end for
15： end for
L(θ,φ,ψ) = ED[-ELBO(Xm)] — α ∙ Edy [logrψ(Ym∣hm)],	(9)
WhereELBO(Xm) = %(zm∣ωm,Dm) [logPθ(ωm|zm, Cm)] -B∙K⅛φ(Zmgm, Dm)kp(Z)] involves
the first MSE and KL divergence; α andβ are hyperparameters to balance the three losses. Following
the typical training paradigm for VAE, we jointly optimize θ and φ via reparameterization trick and
Monte Carlo approximation (Kingma & Welling, 2014). The training procedure is summarized in
Algorithm 1 where we use the Adam optimizer to update the parameters (Kingma & Ba, 2015).
As shown in Algorithm 1, we randomly pick one scenario for each instance so that we can process
it with its context in a batch. The resulting generalization across both the contexts and scenarios of
different instances will allow for a fast training of neural networks. However, in the inference we
employ the trained encoder to attain the latent representations, by processing all scenarios from a set
of instances in parallel. It is realized by duplicating the context in each instance for all its scenarios.
In doing so, the computational efficiency will be considerably improved for solving the instance
with a large cardinality of scenarios, and thus a large amount of instances. For each instance, the
derived scenario representations from the encoder are then used for clustering to attain a few centers
that correspond to representatives among original scenarios, so we end with reduced scenarios.
4.4	Applications
We apply our method to learn scenario rep-
resentation in two typical SIP problems, i.e.,
the network design problem (NDP) and facil-
ity location problem (FLP) (Keutchayan et al.,
2021). We represent them as the SIP graphs,
whose features on nodes and edges including
both static and uncertain parameters are listed
in Table 1. For FLP, the parameters tokenized
by F. and C. mean that they are peculiar to fa-
cility and customer nodes, respectively. More
details about the two problems and their param-
eters can be found in Appendix B and C.
Table 1： Features on SIP graphs of NDP and FLP
Feats
ejωki
Types
dj
ejωki
djk
sjωki
dj
sjωi
djk
sjk
sωi
Parameters
None
demand
opening cost, transportation cost, capacity
None
coordinate, opening cost (F), capacity (F)
presence (C.)
distance between nodes
None
Digits
0
^∏-
3
0
2(4)
-∏-
0
6
Published as a conference paper at ICLR 2022
5	Experiments
We evaluate our method on NDP and FLP to demonstrate its effectiveness. On the one hand, we
train the CVAE to only learn representations for scenarios (Line 5~8 in Algorithm 1), and the trained
encoder can attain representations in each instance, which are directly used for scenario reduction
by finding representatives via an off-the-shelf clustering algorithm. On the other hand, we train the
semi-supervised CVAE to simultaneously learn scenario representations and predict objective values
(Line 5~13). After the training, the neural network can attain representations and objective values
for each scenario and we concatenate them for clustering to find more informative scenarios as
representatives. We refer to the above two paradigms as CVAE-SIP and CVAE-SIPA, respectively.
In the following experiments, we compare them with other learning based methods, and assess their
generalization performance across problems with varying sizes and scenario cardinalities.
5.1	Setup
Instance generation. Our instances are generated following the way in (Keutchayan et al., 2021).
Specifically, we generate NDP instances with 14 nodes including 2 source nodes, 2 target nodes and
10 transition nodes. For FLP, we designate 10 facility nodes and 20 customer nodes. We generate
200 scenarios for each instance in both problems. To evaluate the generalization performance, we
generate larger instances by doubling and quadrupling the number of transition nodes for NDP,
respectively. We do the same to both facility and customer nodes for FLP. Besides, we also double
and quadruple the scenario cardinality in both problems. More details are provided in Appendix D.
Training. We collect 12, 800 instances (of normal size) for NDP and FLP, respectively, and normal-
ize each type of parameters into [0, 1] across instances. Regarding CVAE-SIP, we train our model
without the sub-network (used for the prediction task) by solely optimizing the ELBO loss in Eq.
(9). Regarding CVAE-SIP, we train neural networks for learning representation and objective pre-
diction simultaneously, as shown in Algorithm 1. We set hyperparameters β = 0.005 and α = 100
in Eq. (9). For both problems, CVAE-SIP and CVAE-SIPA are trained with 100 and 400 epochs,
respectively. The batch size and learning rate are set to 128 and 10-4. We run the training on a
server using a single GeForce RTX 2080 Ti GPU.
Testing. Pertaining to CVAE-SIP, we employ the K-medoids algorithm (Park & Jun, 2009) to obtain
representative scenarios based on the latent representations. We keep its default settings and only
change the number of clusters (i.e. K). Given the attained K representatives, we can derive the
approximation solution by reformulating Eq. (3) as:
Oe(x) := mxin μ>x + NkXK=IQ(X,ωj	(10)
where ωk means the representative from the k-th cluster and the number of intra-cluster population
equals to Nk. Accordingly, the approximation error is defined as the gap1 2 3 between objective values
of the approximate solution derived from Eq. (10) and the optimal solution from Eq. (3). Pertaining
to CVAE-SIPA, we infer both the latent representations and objective values of scenarios, which
are concatenated as augmented representations to attain the representatives. To reduce the influence
by the potential concurrent processes, we conduct the evaluation on a different desktop computer,
which uses a single Nvidia Quadro P4000 GPU with a Xeon W-2133 CPU@3.60 GHz.
5.2	Comparison analysis
We compare with four baselines2, i.e., 1) CPLEX IBM (2017), with default settings to compute
optimal solutions. It is also used to solve the approximation problem with representative scenarios.
2) K-medoids, the same clustering algorithm as used in our method. However, we apply it to directly
cluster the original scenarios rather than the learned low-dimensional representations. 3) Scenario-
M, a supervised method with hand-crafted features to predict a scenario for solving a variant of
FLP (Bengio et al., 2020). 4) Solution-M3, a supervised method with raw features to predict the
first-stage decision variables for solving blood transshipment problem (Abbasi et al., 2020).
1In specific, We denote the optimal solutions in Eq. (10) and Eq. (3) by X and x* respectively. The gap is
defined as |O(X) — O(x*)∣∕(∣O(x*)∣ + ε), where we assume minimization problems and ε = 10-10.
2A short comparison with methods in operations research is also given in Appendix I.
3The training details of Scenario-M and Solution-M are summarized in Appendix E.
7
Published as a conference paper at ICLR 2022
Table 2:	Comparison with baselines
NDP (14;200)	FLP (30; 200)_________________
K=5	K=10	K=20	K=5	K=10	K=20
Method Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time
CPLEX 635.75
Scenario-M 2533.50
Solution-M 798.81
K-medoids 976.47
CVAE-SIP 930.73
CVAE-SIPA 769.70
0.00 18s 635.75 0.00
2.96 0.8s 2533.50 2.96
0.26 0.8s 798.81 0.26
0.54 0.3s 761.18 0.19
0.48 0.7s 734.24 0.15
0.24 0.6s 687.68 0.08
18s 635.75 0.00
0.8s 2533.50 2.96
0.8s 798.81 0.26
0.6s 677.02 0.08
0.9s 637.99 0.02
0.8s 642.12 0.03
18s 236.71 0.00
0.8s 1980.03 7.11
0.8s 736.21 1.87
1s 2390.06 9.06
1s 929.61 2.87
1s 709.08 1.71
18s 236.71 0.00
0.6s 1980.03 7.11
0.2s 736.21 1.87
0.2s 1344.39 4.57
0.7s 482.70 0.99
0.6s 381.41 0.58
18s 236.71 0.00
0.6s 1980.03 7.11
0.2s 736.21 1.87
0.3s 502.68 1.17
0.7s 282.49 0.23
0.7s 264.01 0.15
18s
0.6s
0.2s
1s
1s
1s
1	(n; N) means n nodes and N scenarios; Bold means the best result from the learning based methods.
Table 3:	Generalization to large-scale problems
____________________________NDP (24;200)	FLP (60; 200)_________________
K=5	K=10	K=20	K=5	K=10	K=20
Method Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time
CPLEX~~602.47 0.00 2m 602.47 0.00 2m 602.47 0.00 2m 335.37 0.00 11m 335.37 0.00 11m 335.37 0.00 11m
Scenario-M	2238.63	2.68	2s	2238.63	2.68	2s	2238.63	2.68	2s	-	-	-	-	-	-	-	-	-
Solution-M	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-	-
K-medoids	981.94	0.63	0.8s	723.62	0.20	1s	662.46	0.09	3s	3549.25	9.72	1s	1549.53	3.74	4s	561.97	0.73	7s
CVAE-SIP	871.73	0.45	2s	696.22	0.16	2s	659.92	0.09	4s	1083.12	2.17	3s	748.59	1.21	5s	506.75	0.57	8s
CVAE-SIPA	809.65	0.34	2s	637.72	0.06	2s	610.58	0.01	5s	974.17	1.86	3s	746.02	1.18	5s	470.39	0.44	8s
	NDP (44; 200)	FLP (120; 200)	
1 Method Obj.	<=5	K=10	K=20	K=5	K=10	K=20 Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time
CPLEX 580.67	0.00 23m 580.67 0.00 23m 580.67 0.00 23m 484.60 0.00 1h 484.60 0.00 1h 484.60 0.00 1h
Scenario-M 2163.74	2.77 3s 2163.74 2.77 3s 2163.74 2.77 3s	-	-	-	-	-	-	-	-	-
Solution-M -	--	-	--	-	--	-	--	-	--	-	--
K-medoids 803.99	0.38 3s 652.78 0.12 7s 595.15 0.04 14s 6675.13 12.56 6s 2803.77 4.86 21s 1183.44 1.56 61s
CVAE-SIP 1079.80	0.84 5s 615.99 0.06 8s 588.18 0.02 16s 1585.53 2.27 12s 985.45 0.99 35s 627.30 0.28 56s
CVAE-SIPA 682.35	0.17 5s 627.62 0.08 8s 581.07 0.00 17s 2051.45 3.13 16s 910.50 0.84 33s 594.33 0.22 69s
We evaluate all methods on 200 instances of normal size for NDP and FLP, respectively. We set K
= 5, 10, 20 in K-medoids for both problems so that we could access the performance against the
number of representatives. Note that the computation by CPLEX, Scenario-M and Solution-M is
independent of K . The results are displayed in Table 2, where we record the average of objective
values, approximation errors and computational time over all instances, respectively. As shown, both
CVAE-SIP and CVAE-SIPA consistently outperform K-medoids in all cases. Though K-medoids
can also improve the solution quality as K grows, our methods efficiently attain solutions with
much smaller errors. It implies that we achieve more informative representatives with the learned
low-dimensional representations, which help deliver high-quality solutions. On the other hand, our
methods significantly outperform Scenario-M and Solution-M for the three respective values of K.
This suggests that the representatives found by our methods are able to approximate the original
problem more accurately than the approximation with the single scenario or solution. Furthermore,
we observe that CVAE-SIPA is generally superior to CVAE-SIP, which indicates that the predicted
objective values could effectively enrich the scenario representations for achieving more informative
representatives and thus better solutions. Last but not least, all the learning based methods consume
fairly short time compared to the sole CPLEX.
5.3 Generalization across problem sizes
A desirable generalization to larger problems is necessary for learning based methods. To verify such
ability of our methods, we apply the previously trained networks to directly infer another 50 larger
instances (of two levels) for both NDP and FLP, respectively. The instances are generated following
the operation of doubling and quadrupling described in Section 5.1. The baselines are also directly
applied to these larger instances except Solution-M since its output dimension (i.e. number of deci-
sion variables) is fixed only for the problem of normal size. Similarly, Scenario-M is not applicable
to FLP where the scenario dimension (i.e. number of customer nodes) changes with problem sizes.
For CPLEX, we set a 1h time limit on FLP (120; 200). In Table 3, the upper and lower half contains
the results on doubling and quadrupling instances, respectively. As shown, CVAE-SIP is superior
to K-medoids and Scenario-M in most cases except NDP (44; 200) with K=5. On the other hand,
CVAE-SIPA generally achieves the smallest errors but performs inferior to CVAE-SIP on NDP (44;
200) with K=10 and FLP (120; 200) with K=5. This might stem from the degenerated accuracy
of objective prediction, which induces solutions with relatively large penalties on some instances.
8
Published as a conference paper at ICLR 2022
Table 4:	Generalization to large scenario cardinalities
NDP (14; 400)	FLP (30; 400)_________________
K=5	K=10	K=20	K=5	K=10	K=20
Method Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time
CPLEX 634.50 0.00
Scenario-M 2266.88 2.58
Solution-M 791.95 0.26
K-medoids 972.69 0.53
CVAE-SIP 887.76 0.41
CVAE-SIPA 881.05 0.38
1m 634.50 0.00
1s 2266.88 2.58
0.7s 791.95 0.26
0.3s 730.18 0.15
0.8s 711.63 0.12
0.8s 676.99 0.06
1m 634.50 0.00
1s 2266.88 2.58
0.7s 791.95 0.26
0.5s 650.45 0.04
1s 656.06 0.05
1s 645.75 0.03
1m	244.63	0.00	2m	244.63	0.00	2m	244.63	0.00	2m
1s	5260.62 12.79		2s	5260.62 12.79		2s	5260.62 12.79		2s
0.7s	1215.86	5.37	0.5s	1215.86	5.37	0.5s	1215.86	5.37	0.5s
1s	1848.21	6.12	0.2s	734.44	2.00	0.4s	497.64	0.92	1s
2s	701.73	1.83	1s	468.47	0.86	1s	439.52	0.73	2s
2s	723.75	1.89	1s	439.08	0.77	1s	294.02	0.15	2s
NDP (14; 800)
FLP (30; 800)
K=5	K=10	K=20	K=5
K=10	K=20
Method Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time Obj. Error Time
CPLEX 633.34
Scenario-M 2137.65
Solution-M 815.13
K-medoids 896.35
CVAE-SIP 900.59
CVAE-SIPA 761.84
0.00 5m 633.34 0.00
2.36 0.7s 2137.65 2.36
0.28 0.8s 815.13 0.28
0.42 0.3s 770.99 0.20
0.42 1s 658.40 0.03
0.20 1s 684.33 0.07
5m	633.34	0.00
0.7s	2137.65	2.36
0.8s	815.13	0.28
0.7s	678.33	0.08
2s	685.74	0.09
2s	652.69	0.04
5m	239.45	0.00
0.7s	3807.41	9.93
0.8s	1082.62	3.76
1s	1174.66	4.18
2s	600.28	1.36
2s	990.76	2.91
7m	239.45	0.00	7m	239.45	0.00	7m
2s	3807.41	9.93	2s	3807.41	9.93	2s
0.7s	1082.62	3.76	0.7s	1082.62	3.76	0.7s
0.2s	1091.77	3.34	0.3s	732.85	1.72	1s
2s	351.18	0.49	2s	308.24	0.23	2s
2s	420.95	0.77	2s	296.98	0.21	2s
Notably, the computational time by CPLEX rises drastically on larger problems, in comparison with
those in Table 2. Such low efficiency might not be acceptable in practice, especially where real-time
decisions are required for handling a number of problem instances simultaneously. In contrast, our
methods deliver solutions with small penalties in short runtime. In Appendix F, we draw curves of
the average error against the number of representative scenarios used in FLP problems.
5.4	Generalization across scenario cardinalities
From a practical view, the cardinality of scenarios in SIP problems might be changed due to certain
reasons, e.g., new observations are added or outdated ones are removed (Issac & Campbell, 2017).
Therefore, a learning based method for SIPs should be able to handle this varying dimension via
generalization. To this end, we evaluate our methods and baselines on 50 instances of NDP and
FLP with doubling and quadrupling scenarios, respectively. The results in Table 4 show that our
methods can still achieve the smallest errors across most settings, except NDP (14; 400) with K=5,
where Solution-M performs better. It implies that our methods are less sensitive to the scenario
cardinality. Since our methods process all scenarios in parallel, they solve instances with scenarios
up to 800 in about 2 seconds, which is significantly faster than CPLEX. Additionally, we also test
with scenarios generated from varying distributions and different dependencies on the context. The
details are provided in Appendix G.
5.5	Objective prediction
We further demonstrate the efficacy of the semi-supervised learning for objective prediction. The
trained model is directly adopted to infer objective values of scenarios in NDP and FLP instances.
The results are gathered in Figure 3 and 4, which show our prediction is fairly accurate for both
problems even when generalizing to larger problem sizes or scenario cardinalities. It well verifies
the effectiveness of our semi-supervised design, which only needs to solve a small volume (1%) of
training instances. In addition, the sample efficiency is also enhanced through the generalization
over the latent space. More details can be found in Appendix H.
6 Conclusions and future work
In this paper, we present a deep learning method to solve graph-based two-stage SIPs. We lever-
age CVAE to learn latent representations of stochastic scenarios conditioned on the context of a
SIP instance. With the learned representations, we conduct two downstream tasks for solving SIP,
i.e., scenario reduction and objective prediction. The evaluation on two classic SIP problems has
shown that our method is able to find less yet informative representatives for improving both solu-
tion quality and computation efficiency. Additional studies also verify that our method can be well
generalized to larger problem sizes, larger cardinalities of scenarios and different distributions of
scenarios. In the future, we will attempt to automate the decision on the optimal number of repre-
sentatives for scenario reduction, and extend our method towards solving general two-stage SIPs.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This research was conducted at Singtel Cognitive and Artificial Intelligence Lab for Enter-
prises (SCALE@NTU), which is a collaboration between Singapore Telecommunications Limited
(Singtel) and Nanyang Technological University (NTU) that is supported by A*STAR under its In-
dustry Alignment Fund (LOA Award number: I1701E0013). Wen Song is supported by the National
Natural Science Foundation of China under Grant 62102228, and the Shandong Provincial Natural
Science Foundation under Grant ZR2021QF063.
References
Babak Abbasi, Toktam Babaei, Zahra Hosseinifard, Kate Smith-Miles, and Maryam Dehghani. Pre-
dicting solutions of large-scale optimization problems via machine learning: A case study in
blood supply chain management. Computers & Operations Research, 119:104941, 2020.
Kun An and Hong K Lo. Two-phase stochastic program for transit network design under demand
uncertainty. Transportation Research Part B: Methodological, 84:157-181, 2016.
Yoshua Bengio, Emma Frejinger, Andrea Lodi, Rahul Patel, and Sriram Sankaranarayanan. A
learning-based algorithm to quickly compute good primal solutions for stochastic integer pro-
grams. In International Conference on Integration of Constraint Programming, Artificial Intelli-
gence, and Operations Research, pp. 99-111. Springer, 2020.
Sergio Bruno, Shabbir Ahmed, Alexander Shapiro, and Alexandre Street. Risk neutral and risk
averse approaches to multistage renewable investment planning under uncertainty. European
Journal of Operational Research, 250(3):979-989, 2016.
Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimiza-
tion. In the 33rd Conference on Advances in Neural Information Processing Systems (NIPS), pp.
6278-6289, 2019.
Mary Dillon, Fabricio Oliveira, and Babak Abbasi. A two-stage stochastic programming model for
inventory management in the blood supply chain. International Journal of Production Economics,
187:27-41, 2017.
Martin Dyer and Leen Stougie. Computational complexity of stochastic programming problems.
mathematical programming, 106(3):423-432, 2006.
Jamie Fairbrother, Amanda Turner, and Stein W Wallace. Problem-driven scenario generation: an
analytical approach for stochastic programs with tail risk measure. Mathematical Programming,
pp. 1-42, 2019.
Hao-Zhe Feng, Kezhi Kong, Minghao Chen, Tianye Zhang, Minfeng Zhu, and Wei Chen. Shot-vae:
Semi-supervised deep generative models with label-aware elbo approximations. In Proceedings
of the 35th Conference on Artificial Intelligence (AAAI), pp. 7413-7421, 2021.
Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact Com-
binatorial optimization with graph convolutional neural networks. In Proceedings of the 33rd
Conference on Neural Information Processing Systems (NIPS), 2019.
Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose MigUel Hernandez-Lobato,
Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268-276, 2018.
Holger Heitsch and Werner Romisch. Scenario reduction algorithms in stochastic programming.
Computational optimization and applications, 24(2):187-206, 2003.
Rene Henrion and Werner Romisch. Optimal scenario generation and reduction in stochastic pro-
gramming .Humboldt-Universitat ZU Berlin, Mathematisch-Naturwissenschaftliche Fakultat...,
2017.
10
Published as a conference paper at ICLR 2022
Rene Henrion and Werner Romisch. Problem-based optimal scenario generation and reduction in
stochastic programming. Mathematical Programming, pp. 1-23, 2018.
Rene Henrion, Christian KuChler, and Werner Romisch. Discrepancy distances and scenario reduc-
tion in two-stage stochastic mixed-integer programming. Journal of Industrial & Management
Optimization, 4(2):363, 2008.
Andre Hottung, Bhanu Bhandari, and Kevin Tierney. Learning a latent search space for routing
problems using variational autoencoders. In the 8th International Conference on Learning Rep-
resentations (ICLR), 2020.
Kjetil H0yland, Michal Kaut, and Stein W Wallace. A heuristic for moment-matching scenario
generation. Computational optimization and applications, 24(2):169-185, 2003.
IBM(2017). Ibm ilog cplex 12.10 user manual ibm crop. 2017.
Brian Ichter, James Harrison, and Marco Pavone. Learning sampling distributions for robot motion
planning. In Proceedings of the International Conference on Robotics and Automation (ICRA),
pp. 7087-7094, 2018.
Preethi Issac and Ann Melissa Campbell. Shortest path problem with arc failure scenarios. EURO
Journal on Transportation and Logistics, 6(2):139-163, 2017.
Haifa Jammeli, Majdi Argoubi, and Hatem Masri. A bi-objective stochastic programming model for
the household waste collection and transportation problem: case of the city of sousse. Operational
Research, pp. 1-27, 2019.
Waldy Joe and Hoong Chuin Lau. Deep reinforcement learning approach to solve dynamic vehicle
routing problem with stochastic customers. In Proceedings of the 30th International Conference
on Automated Planning and Scheduling (ICAPS), volume 30, pp. 394-402, 2020.
Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional network
technique for the travelling salesman problem. In INFORMS Annual Meeting, 2019.
Julien Keutchayan, David Munger, and Michel Gendreau. On the scenario-tree optimal-value error
for stochastic programming problems. Mathematics of Operations Research, 45(4):1572-1595,
2020.
Julien Keutchayan, Janosch Ortmann, and Walter Rei. Problem-driven scenario clustering in
stochastic optimization. arXiv preprint arXiv:2106.11717, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In the 3rd
International Conference on Learning Representations (ICLR), 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In the 2nd International
Conference on Learning Representations (ICLR), 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In the 28th Conference on Advances in Neural Information
Processing Systems (NIPS), pp. 3581-3589, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In the 5th International Conference on Learning Representations (ICLR), 2017.
Kim-Manuel Klein. About the complexity of two-stage stochastic ips. Mathematical Programming,
pp. 1-19, 2021.
Anton J Kleywegt, Alexander Shapiro, and Tito Homem-de Mello. The sample average approxima-
tion method for stochastic discrete optimization. SIAM Journal on Optimization, 12(2):479-502,
2002.
Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In the
7th International Conference on Learning Representations (ICLR), 2019.
11
Published as a conference paper at ICLR 2022
Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min.
Pomo: Policy optimization with multiple optima for reinforcement learning. In the 34th Confer-
ence on Advances in Neural Information Processing Systems (NIPS), 2020.
Eric Larsen, Sebastien Lachapelle, Yoshua Bengio, Emma Frejinger, Simon Lacoste-JUlien, and An-
drea Lodi. Predicting solution summaries to integer linear programs under imperfect information
with machine learning. arXiv preprint arXiv:1807.11876, 2018.
Mengyuan Lee, Seyyedali Hosseinalipour, Christopher Greg Brinton, Guanding Yu, and Huaiyu
Dai. A fast graph neural network-based method for winner determination in multi-unit combina-
torial auctions. IEEE Transactions on Cloud Computing, 2020.
Vinod Nair, Dj Dvijotham, Iain Dunning, and Oriol Vinyals. Learning fast optimizers for contextual
stochastic integer programs. In Proceedings of the 34th Conference on Uncertainty in Artificial
Intelligence (UAI),pp. 591-600, 2018.
Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Takac. Reinforcement
learning for solving the vehicle routing problem. In the 32nd Conference on Neural Information
Processing Systems (NIPS), pp. 9839-9849, 2018.
Hae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for k-medoids clustering. Expert
systems with applications, 36(2):3336-3341, 2009.
Ragheb Rahmaniani, Teodor Gabriel Crainic, Michel Gendreau, and Walter Rei. Accelerating the
benders decomposition method: Application to stochastic network design problems. SIAM Jour-
nal on Optimization, 28(1):875-903, 2018.
Napat Rujeerapaiboon, Kilian Schindler, Daniel Kuhn, and Wolfram Wiesemann. Scenario reduc-
tion revisited: Fundamental limits and guarantees. Mathematical Programming, pp. 1-36, 2018.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. the 29th Conference on Advances in Neural Information
Processing Systems (NIPS), 28:3483-3491, 2015.
Stein W Wallace and William T Ziemba. Applications of stochastic programming. SIAM, 2005.
Yaoxin Wu, Wen Song, Zhiguang Cao, and Jie Zhang. Learning large neighborhood search policy
for integer programming. Advances in Neural Information Processing Systems, 34, 2021.
Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification.
In Proceedings of the 33rd Conference on Artificial Intelligence (AAAI), pp. 7370-7377, 2019.
Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Xu Chi. Learning to dispatch
for job shop scheduling via deep reinforcement learning. In the 34th Conference on Advances in
Neural Information Processing Systems (NIPS), volume 33, pp. 1621-1632, 2020.
12
Published as a conference paper at ICLR 2022
A Graph convolutional network
Given a complete SIP graph as defined in Section 4.2, we first process the features of nodes and
edges by separate linear transformations as below:
V0 = VWv0 + bv ;	E0 = EWe0 + be ,
(11)
where V ∈ Rn×dv and E ∈ Rn×n×de are the raw features (i.e. parameters in Table 1) on nodes and
edges, respectively; Wv0 ∈ Rdv×128 and We0 ∈ Rde×128 are trainable weights with corresponding
bias vectors bv ∈ R1×128 and be ∈ R1×128. Note that we also use different linear transformations
for heterogeneous nodes or edges. For example, the location nodes of facilities and customers are
processed separately.
Next, we advance the node and edge embeddings by the graph convolution layer for L iterations,
and each iteration has its own parameters, which are expressed as:
ejk ∈V
vj+1 = vj+ PReLU(BN(VjW； + X j Θ vj∙W2)), j
k
σ(ejk)
Pjk∈V σ(ejk) + e
(12)
ej+1 = ej% + PReLU(BN(ej%W3 + vjw4 + vkWg)), l = 0,..., L - 1
where Wl1, . . .,W5l ∈ R128×128; Θ means the element-wise product; σ is the sigmoid activation
function; is a small value to avoid numerical error. Initially, the node embedding vj0 is the j -th
row of matrix V0 and edge embedding ej0k is the element at the j -th row and k-th column of matrix
E0. The above graph layer is similar to the one in Joshi et al. (2019) except that we use parametric
rectified linear unit with its learnable parameter being 0.25.
After L (L=10) iterations of the above convolution, the node and edge embeddings are represented
as {vjL}jn=1 and {ejLk}jn,k=1, respectively. We attain the graph embedding by the mean pooling
over all the node embeddings. We neglect edge embeddings considering that their information has
already been involved in node embeddings, after multiple convolutions.
Parameter sharing The graph embedding network based on Eq. (11) and (12) is able to process
the sole context Dm with its static parameters on the graph. Also it can process the context with any
scenario in the instance, by featurizing the graph with both static parameters in Dm and a scenario
ωmi . To foster the learning efficiency, we use a single network to process these two featurizations.
It is achieved by padding the uncertain parameters with zeros when we only process the context.
We represent the resulting graph embeddings by hm and him, which are further used to produce
conditional and latent variables and meanwhile predict objective values, as stated in Section 4.2.
B	Network design problem
Given a complete directed graph G = (V, E), each edge ejk ∈ E points from vj to vk where
vj , vk ∈ V. We denote the set of source and target nodes as S and T respectively. The features
on an edge include its capacity qjk, opening cost ojk and transportation cost hjkc for one unit of
commodity c ∈ C .
Scenarios The uncertainty exists in the demands on each node. In specific, the parameter divc denotes
the demand of commodity c on node v, in the i-th scenario. A penalty fs is given to a source node
if its commodities are not fully transported in a scenario due to the capacity limit of opened edges.
Decision variables The decision variable in the first stage is denoted by xjk. xjk = 1 means that the
edge ejk is open to transport commodities, otherwise xjk = 0. The decision variable in the second
stage is denoted by yjikc, which means the units of commodity c transported along the edge ejk in
the i-th scenario.
13
Published as a conference paper at ICLR 2022
The objective of NDP is to decide the edges to be opened, which minimizes the total opening cost
and expected transportation cost (including the penalty). The SIP formulation is expressed as:
1
min	ojkxjk + NE(E E tjkc
ejk∈E	i=1 c∈C ejk∈E
ejk ∈E	ejk ∈E
s.t.	yjikc -	yjikc = divc,
vj =v	vk=v
ejk ∈E	ejk ∈E
yjikc =	yjikc = 0,
vj=t	vk=s
ejk∈E
disc ≤ X yjikc + Mzsc,
yjkc ≤ qjkxjk,
c∈C
zsc, xjk ∈ {0, 1}; yjikc ∈ [0, ∞).
where M is a large value which ensures that t
modities are transported.
ijkc +	fszsc	(13)
s∈S
∀(v, c, i)	∈	V	×	C	×	{1, . . . , N}	(14)
∀(s, t, c, i) ∈ S	×	T	×	C	×	{1, . . . ,N}	(15)
∀(s, c, i)	∈	S	×	C	×	{1, . . . ,N}	(16)
∀(vj,vk,i) ∈ V × V × {1, . . . ,N}	(17)
(18)
e source node will not be penalized if all its com-
C Facility location problem
The facility location problem comprises F and C locations for facilities and customers, respectively.
Each facility location is featured by an opening cost of, f ∈ {1, . . . , F} and a facility has q units of
resources that can be provided to customers. We assume that the number of facilities to be opened
is not larger than v. For each customer c ∈ {1, . . . , C}, it consumes qcf units of resources if it is
served by the facility at the location f, and the induced service cost is denoted by scf .
Scenarios The uncertainty exists in the presence of customers. In specific, the parameter hic refers to
the presence of the customer c in the i-th scenario, i.e., hic = 1 means that the customer c is present
to be served in scenario i, otherwise hic = 0. A penalty bf is given to the facility f in a scenario if
one additional unit of resource is needed for it to meet the demand.
Decision variables The decision variable in the first stage is denoted by xf . xf = 1 means that
a facility is open at location f to provide services, otherwise xf = 0. The second-stage decision
variables include ycif and zfi . In specific, ycif = 1 means that the customer c is served by facility
at location f in the i-th scenario, otherwise ycif = 0. zfi means the quantity of additional resources
required at facility location f to meet the total demand of its customers, in the i-th scenario.
The objective is to decide the locations to build the facilities, so as to minimize the total opening
cost and the expected service cost including the potential penalty. The two-stage SIP formulation of
the facility location problem is expressed as:
1
min	ofxf +N ∑(∑ ∑scf yCf + ∑>zf)
f∈F	i=1 c∈C f∈F	f∈F
s.t. xf ≤ v,
f∈F
qcfycif ≤ qxf +zfi,
c∈C
zfi ≤ Mxf ,
X ycif = hic,
f∈F
xf,ycif ∈ {0, 1}; zfi ∈ [0, ∞),
(19)
(20)
∀(f, i) ∈F × {1,...,N} (21)
∀(f, i) ∈F × {1,...,N} (22)
∀(c, i) ∈ C × {1,. . . ,N} (23)
(24)
14
Published as a conference paper at ICLR 2022
where M is a large value that constrains zfi to 0 if xf = 0, so as to ensure that the facility f will not
be penalized if it is not open.
D Details of instance generation
We generate NDP instances with 14 nodes, in which the commodities on two source nodes are trans-
ported to another two target nodes to meet their demands. The remaining nodes are only for transi-
tion without demands. We set two types of commodities and their quantities are uniformly sampled
from [5, 15] in each scenario. For each edge, we uniformly sample the opening cost, shipping cost
and capacity from [3, 11], [5, 11] and [10, 41], respectively. A penalty 1000 will be triggered for each
source node if its commodities cannot be fully transported due to the capacity limit of opened edges.
For FLP, we consider 10 facility nodes and 20 customer nodes, with their coordinates uniformly
sampled from the unit square [0, 1] × [0, 1]. The opening cost and capacity for each facility are uni-
formly sampled from [40, 81] and [30, 60]. We generate binary values through Bernoulli sampling
for customer nodes to indicate their presence in a scenario, with probabilities uniformly sampled
from [0.4, 0.6] for each node. A penalty 1000 is set for each facility node if it cannot meet one
unit of demands. We generate 200 scenarios for each instance of both problems. To test the gen-
eralization performance, we generate larger instances by doubling and quadrupling the number of
transition nodes for NDP, respectively. We do the same to both facility and customer nodes for FLP.
Regarding the scenario, we also double and quadruple its cardinality in both problems.
E Training details for the two baselines
Scenario-M This method aims to generate one scenario, which can result in a good approximate
solution. It is not easy since for each instance, sufficient scenarios need to be evaluated to attain the
target scenario for supervised learning. To avoid much computational burden, Bengio et al. (2020)
compute the average of scenarios as the initial one and then iteratively improve it to derive a similar
solution as the optimal one. However, the improvement heuristic used cannot guarantee that the
attained one exists in the original set of scenarios. To tackle this issue, we adapt the method to
track the improvement along existing scenarios. Specifically, we find the 20 scenarios nearest to the
average and compute the MIPs defined by each of them with the context. We determine the target
scenario by picking the one with the most similar solution to the optimal one. This method could be
used for both NDP and FLP.
We conduct the same featurization as described in Appendix A.4 in (Bengio et al., 2020), and use
both linear regression (LR) and artificial neural network (ANN) with MSE loss. In addition, we also
use the same GCN in our method to directly process raw parameters in SIP instances. Specifically,
we only input the context with the average of scenarios to GCN, and the graph embedding is linearly
projected into output to estimate scenarios. The reported results in the main paper are delivered by
GCN, which we find outperforms LR and ANN in most cases.
Solution-M Following (Abbasi et al., 2020), we directly predict the first-stage solution in both
NDP and FLP. Despite the machine learning algorithms listed in the paper, we use the same GCN
in our method for training. It is more powerful to process the raw parameters under the graph
representation. In our cases, the raw parameters are the static parameters in the context for both
NDP and FLP. We take them as the input into GCN and the output dimension is same as the number
of first-stage decision variables. If there is no static parameters on nodes or edges, we use 0 for them
to run GCN as usual.
For both supervised methods, we use 12,800 instances (of normal size) in the training with the
number of epochs 100, batch size 64 and learning rate 0.001. We test them with diverse instances as
described in corresponding experiments in the main paper.
F Additional results on large FLP
We evaluate our methods on FLP of large sizes to show the trend of the average error as the number
of representative scenarios grows. We follow the experimental setting in Section 5.3 and apply the
trained model to solve 50 instances from FLP (60; 200) and FLP (120; 200). We set K from 1 to
15
Published as a conference paper at ICLR 2022
FLP (60; 200)
Oooo
8 6 4 2
」0J」山ωπEω><
23456789 10 1112 13 14 15 1617 18 19 20
Number of Representatives (K)
Figure 2: Errors with different values of K
FLP(120; 200)
Ooooo
5 4 3 2 1
」0J」山ωπEω><
123456789 10 1112 13 1415 1617 18 19 20
Number of Representatives (K)
20 and thus attain solutions with varying numbers of representative scenarios. The curve of aver-
age approximation errors is plotted in Figure 2. We observe that both CVAE-SIP and CVAE-SIPA
persistently reduce errors towards 0 as K grows. It means that our methods are consistently attain-
ing more effective representatives from scenarios, which help deliver better solutions. Meanwhile,
CVAE-SIP and CVAE-SIPA outperform K-medoids with different K on both problems, especially
on FLP (120; 200). It again verifies the merit of the learned representations for solving SIP prob-
lems. The comparison between CVAE-SIP and CVAE-SIPA implies that though CVAE-SIPA is
superior to CVAE-SIP at the early stage of the curve, i.e., K=1,. . .,3, they are on par with each other
when more representatives are used eventually.
Table 5: Results on distributions and dependencies
NDP (14; 200); Normal	NDP (14; 200); Binomial
Method	K=3			1 Obj.	=5		K=10			K=3			K=5			K=10		
	Obj.	Error Time			Error	Time	Obj.	Error	Time	Obj.	Error Time		Obj.	Error Time		Obj.	Error Time	
CPLEX	513.82	0.00	16s	513.82	0.00	16s	513.82	0.00	16s	510.08	0.00	20s	510.08	0.00	20s	510.08	0.00	20s
Scenario-M	1039.48	0.93	0.8s	1039.48	0.93	0.8s	1039.48	0.93	0.8s	1271.23	1.21	0.6s	1271.23	1.21	0.6s	1271.23	1.21	0.6s
Solution-M	678.74	0.34	0.8s	678.74	0.34	0.8s	678.74	0.34	0.8s	729.81	0.51	0.2s	729.81	0.51	0.2s	729.81	0.51	0.2s
K-medoids	662.48	0.30	0.2s	585.66	0.14	0.3s	546.51	0.06	0.5s	684.71	0.34	0.3s	623.76	0.22	0.3s	583.25	0.14	0.5s
CVAE-SIP	622.98	0.23	0.6s	548.45	0.07	0.6s	527.59	0.03	0.9s	695.60	0.36	0.6s	592.40	0.16	0.6s	555.35	0.09	0.9s
CVAE-SIPA	562.87	0.11	0.5s	551.94	0.07	0.6s	531.94	0.04	0.8s	622.91	0.22	0.5s	592.91	0.16	0.6s	540.54	0.06	0.9s
				NDP (14; 200); D0									NDP (14; 200); D1					
	1	=3		1	=5		K=10			K=3			1	=5		K=10		
Method	Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time	
CPLEX	563.16	0.00	17s	563.16	0.00	17s	563.16	0.00	17s	617.21	0.00	17s	617.21	0.00	17s	617.21	0.00	17s
Scenario-M	1061.20	0.87	0.7s	1061.20	0.87	0.7s	1061.20	0.87	0.7s	1434.12	1.37	0.9s	1434.12	1.37	0.9s	1434.12	1.37	0.9s
Solution-M	754.11	0.38	0.6s	754.11	0.38	0.6s	754.11	0.38	0.6s	1282.59	1.12	0.6s	1282.59	1.12	0.6s	1282.59	1.12	0.6s
K-medoids	1086.43	0.93	0.3s	744.53	0.32	0.4s	635.58	0.13	0.5s	1091.11	0.79	0.3s	816.62	0.33	0.3s	692.81	0.12	0.6s
CVAE-SIP	1070.44	0.89	0.6s	698.18	0.24	0.6s	632.03	0.12	0.9s	1180.53	0.90	0.6s	809.02	0.31	0.6s	675.19	0.09	0.9s
CVAE-SIPA	678.80	0.20	0.5s	663.97	0.18	0.6s	578.57	0.03	0.8s	758.14	0.23	0.5s	737.81	0.19	0.5s	643.50	0.04	0.8s
G Generalization across distributions and dependencies
We test the generalization performance of our method on NDP with scenarios sampling from the
Normal distribution and Binomial distribution. For the former, we set the expectation and standard
deviation to 10 and 2, respectively. For the latter, we set the number of runs and the probability to 20
and 0.5. The results are summarized in the upper half of Table 5. We observe that our methods con-
sistently outperform other learning based approaches with scenarios from both distributions. When
K=10, both CVAE-SIP and CVAE-SIPA achieve near-optimal solutions with slight approximation
errors. On the other hand, it can be found that the results delivered by CVAE-SIPA are generally
superior or comparable to those by CVAE-SIP, suggesting a better cross-distribution generalization.
In our training, we do not assume explicit dependencies between the context and scenarios, following
the settings in (Keutchayan et al., 2020; Nair et al., 2018; Abbasi et al., 2020). However, we would
like to show that the trained networks can well generalize to the dependent settings. In specific, we
construct in NDP two dependencies between the demand scenarios and costs in contexts as follows:
16
Published as a conference paper at ICLR 2022
Figure 3: Objective prediction against latent representations (NDP)
Figure 4: Objective prediction against latent representations (FLP)
1)	D0: for each instance, the lower bound of Uniform demands is determined by the smaller value of
the average opening and shipping cost in the context; 2) D1: the lower and upper bounds of Uniform
demands are determined by the smaller and larger value of the average opening and shipping cost,
respectively. We generate 50 instances for each dependency. From the lower half of Table 5, we
observe that our methods outperform learning based baselines except the cases with K=3, where
CVAE-SIP loses its advantage over a few baselines. Generally, it indicates that our methods possess
fairly good generalization abilities to the setting where scenarios depend on the contexts.
H Results on objective prediction
We first evaluate our trained model for the prediction task in three FLP instances, which are ran-
domly generated from FLP (30; 200), FLP (120; 200) and FLP (30; 800), respectively. For each
instance, we draw a scatter plot of the 2-dimensional latent representations for scenarios and repre-
sent the normalized objective values using different colors. The results are shown in the upper half
17
Published as a conference paper at ICLR 2022
Table 6: Comparison with OR methods
NDP (14; 200)	FLP (30; 200)
Method	K=5		K=10		K=20		K=5		K=10		K=20	
	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time
CPLEX	635.75	0.00 18s	635.75	0.00 18s	635.75	0.00 18s	236.71	0.00 18s	236.71	0.00 18s	236.71	0.00 18s
CSSC	695.08	0.10 17s	689.43	0.09 17s	669.59	0.05 30s	338.84	0.39 12s	299.42	0.23 12s	264.98	0.16 14s
Scen-red	951.54	0.50 0.3s	739.41	0.18 0.5s	650.80	0.03 1s	640.07	1.54 0.3s	323.34	0.29 0.3s	246.57	0.08 1s
CVAE-SIP	930.73	0.48 0.7s	734.24	0.15 0.9s	637.99	0.02 1s	929.61	2.87 0.7s	482.70	0.99 0.7s	282.49	0.23 1s
CVAE-SIPA	769.70	0.24 0.6s	687.68	0.08 0.8s	642.12	0.03 1s	709.08	1.71 0.6s	381.41	0.58 0.7s	264.01	0.15 1s
			NDP (44; 200)						FLP (120; 200)			
	K=5		K=10		K=20		K=5		K=10		K=20	
Method	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time	Obj.	Error Time
CPLEX	580.67	0.00 23m 580.67		0.00 23m 580.67		0.00 23m	484.60	0.00 1h	484.60	0.00 1h	484.60	0.00 1h
CSSC	618.10	0.07 2m	617.94	0.07 2m	612.77	0.06 4m	1101.83	1.28 34s	844.24	0.76 42s	715.46	0.59 2m
Scen-red	894.90	0.57 3s	667.90	0.15 6s	616.23	0.07 15s	1056.69	1.19 6s	793.45	0.68 15s	607.69	0.26 60s
CVAE-SIP	1079.80	0.84 5s	615.99	0.06 8s	588.18	0.02 16s	1585.53	2.27 12s	985.45	0.99 35s	627.30	0.28 56s
CVAE-SIPA	682.35	0.17 5s	627.62	0.08 8s	581.07	0.00 17s	2051.45	3.13 16s	910.50	0.84 33s	594.33	0.22 69s
of Figure 4, and the corresponding ground truth (i.e. optimal solution values attained by CPLEX)
are displayed in the lower half. It is clear that the prediction is fairly accurate with almost the same
patterns as the target values in the ground truth, even for the larger problem size or scenario cardi-
nality. It indicates that our design for semi-supervised learning is effective and the trained model
exhibits a desirable generalization ability.
We further evaluate the objective prediction on NDP. Specifically, the trained model is used to predict
objective values for the scenarios from three instances, which are randomly generated from NDP
(14; 200), NDP (44; 200) and NDP (14; 800), respectively. From the results in Figure 3, we observe
that our method can also predict the objective values accurately for NDP. Especially, the pattern of
predicted values for NDP (14; 800) is almost indistinguishable from the one by ground truth. It
reveals that our design for the semi-supervised learning is effective across distinct SIPs.
I Comparison with OR methods
In this section, we compare our methods with two mature methods from operations research (OR),
i.e., 1) Scen-red (Heitsch & Romisch, 2003), which seeks a subset of scenarios that is closest to
original scenarios in terms of Fortet-Mourier probability metric, and 2) CSSC (Keutchayan et al.,
2020), which solves a MILP to derive representative scenarios, each of which estimates the average
objective of some other scenarios. They are on behalf of the distribution-oriented and problem-
oriented paradigm, respectively. The tested instances are the same as the ones in Section 5.2 and
5.3. From the results in Table 6, we observe that our methods are comparable to or better than the
baselines and especially on NDP. While CVAE-SIP and CVAE-SIPA are inferior to CSSC on NDP
with K=5, they attain the best approximate solutions when K=10 and 20. Notably, the errors of our
methods are close to 0 on both normal and large instances when K=20. For FLP, while our methods
are inferior to CSSC and Scen-red on FLP when K=5 and 10, CVAE-SIPA surpasses CSSC on FLP
(30; 200) and gains the smallest objective value and error on FLP (120; 200), when K=20. Since
our runtime is shorter than CSSC and similar to Scen-red, the results again verify that our methods
are effective to reduce scenarios for good approximations.
J	Ablation study
We further conduct an ablation study on NDP to verify the effectiveness of the learned scenario
representations, and meanwhile investigate the mixed use of these representations with the original
scenarios. In specific, we enrich the original scenario set for clustering in the following ways: 1)
CVAE-SIPA+Ori.: we concatenate the predicted objective values with their original scenarios (rather
than the learned representations in CVAE-SIPA); 2) CVAE-SIP+Ori.: we concatenate the learned
representations with their original scenarios (rather than merely use the learned representations in
CVAE-SIP). These two kinds of mixed representations are used for clustering to find representative
scenarios, where we set K=5, 10, 20. The tested instances are the same as the ones in Section
5.2 and 5.3. As shown in Table 6, both CVAE-SIP and CVAE-SIPA are generally superior to their
18
Published as a conference paper at ICLR 2022
Table 7: Ablation study
NDP (14; 200)	NDP (44; 200)
K=5	K=10	K=20	K=5	K=10	K=20
Method	Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time		Obj.	Error Time	
CPLEX	635.75	0.00	18s	635.75	0.00	18s	635.75	0.00	18s	580.67	0.00	23m	580.67	0.00	23m	580.67	0.00	23m
K-medoids	976.47	0.54	0.3s	761.18	0.19	0.6s	677.02	0.08	1s	803.99	0.38	3s	652.78	0.12	7s	595.15	0.04	14s
CVAE-SIPA+Ori.	917.51	0.45	0.6s	741.81	0.17	0.9s	699.89	0.09	1s	918.16	0.57	5s	700.10	0.19	8s	619.29	0.06	18s
CVAE-SIPA	769.70	0.24	0.6s	687.68	0.08	0.8s	642.12	0.03	1s	682.35	0.17	5s	627.62	0.08	8s	581.07	0.00	17s
CVAE-SIP+Ori.	1208.19	0.93	0.6s	850.85	0.34	0.8	679.07	0.06	1s	1056.61	0.82	5s	785.60	0.32	9s	603.39	0.04	20s
CVAE-SIP	930.73	0.48	0.7s	734.24	0.15	0.9s	637.99	0.02	1s	1079.80	0.84	5s	615.99	0.06	8s	588.18	0.02	16s
1	Bold means the method outperforms its counterpart.
counterparts. The advantage of CVAE-SIPA over CVAE-SIPA+Ori. indicates that with the same
predicted objective values, our learned representations are effective to attain better representatives
than the original scenarios. Moreover, CVAE-SIP is superior to CVAE-SIP+Ori. probably because
the learned representations are overwhelmed by the original scenarios to some extent. It is also
interesting to identify an advantage that CVAE-SIP > K-medoids > CVAE-SIP+Ori, which means
the pure use of learned representations or original scenarios is better than the mixed use of both.
19