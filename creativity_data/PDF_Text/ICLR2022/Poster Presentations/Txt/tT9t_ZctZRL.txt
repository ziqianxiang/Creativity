Published as a conference paper at ICLR 2022
Towards Deepening Graph Neural Networks
A GNTK-based Optimization Perspective
WeiHUang *t
University of Technology Sydney
weihuang.uts@gmail.com
Weitao DU
Northeastern University
weitao.du@northwestern.edu
Richard Yi Da XU & Ling Chen
University of Technology Sydney
{YiDa.Xu,ling.chen}@uts.edu.au
Yayong Li *
University of Technology Sydney
yayong.li@student.uts.edu.au
Jie Yin
The University of Sydney
jie.yin@sydney.edu.au
Miao Zhang
Aalborg University
miaoz@cs.aau.dk
Ab stract
Graph convolutional networks (GCNs) and their variants have achieved great suc-
cess in dealing with graph-structured data. Nevertheless, it is well known that
deep GCNs suffer from the over-smoothing problem, where node representations
tend to be indistinguishable as more layers are stacked up. The theoretical re-
search to date on deep GCNs has focused primarily on expressive power rather
than trainability, an optimization perspective. Compared to expressivity, train-
ability attempts to address a more fundamental question: Given a sufficiently ex-
pressive space of models, can we successfully find a good solution via gradient
descent-based optimizers? This work fills this gap by exploiting the Graph Neural
Tangent Kernel (GNTK), which governs the optimization trajectory under gradi-
ent descent for wide GCNs. We formulate the asymptotic behaviors of GNTK
in the large depth, which enables us to reveal the dropping trainability of wide
and deep GCNs at an exponential rate in the optimization process. Additionally,
we extend our theoretical framework to analyze residual connection-based tech-
niques, which are found to be merely able to mitigate the exponential decay of
trainability mildly. Inspired by our theoretical insights on trainability, we propose
Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, to
alleviate the exponential decay problem more fundamentally. Experimental eval-
uation consistently confirms using our proposed method can achieve better results
compared to relevant counterparts with both infinite-width and finite-width.
1	Introduction
Recently, Graph Neural Networks (GNNs) have shown incredible abilities to learn node or graph
representations and achieved superior performance on various downstream tasks, such as node clas-
Sification (KiPf & Welling, 2017; VeliCkovic et al., 2018; Hamilton et al., 2017), graph classifica-
tion (Xu et al., 2019; Lee et al., 2019b; Yuan & Ji, 2020), and link prediction (Kipf & Welling,
2016), etc. However, most GNNs (e.g., GCNs) achieve their best only with a shallow depth, e.g., 2
or 3 layers, and their performance on those tasks would promptly degrade as the number of layers
grows. Towards this phenomenon, research attempts have been made to deepen understanding of
current GNN architectures and their expressive power. Li et al. (2018) showed that GCN is a special
form of Laplacian smoothing, which mixes node representations with nearby neighbors. This mech-
anism potentially poses the risk of over-smoothing as more layers are stacked together, where node
representations tend to be indistinguishable from each other. Oono & Suzuki (2020) investigated
* Equal Contribution.
t Work partially performed while at The University of Sydney
1
Published as a conference paper at ICLR 2022
the expressive power of GNNs using the asymptotic behaviors as the layer goes to infinity. They
proved that under certain conditions, the expressive power of GCN is determined by the topological
information of the underlying graph inherent in the graph spectra.
Nevertheless, it remains elusive how to theoretically understand why deep GCNs fail to optimize.
Existing theoretical investigation (Oono & Suzuki, 2020; Xu et al., 2019) on GNNs focus mainly
on expressivity, which measures the complexity of functions that can be represented by a neural
network. Exploring expressivity is theoretically convenient, but the corresponding conditions may
be violated during the gradient training process, thereby leading to inconsistencies between theo-
retical conclusions and empirical results of trained networks (Guhring et al., 2020). Compared to
expressivity, trainability addresses a more difficult but fundamental perspective of neural networks:
How effectively a neural network can be optimized via gradient descent. The advantage of investi-
gating trainability is that we can directly determine whether GNNs can be successfully trained under
certain conditions, and to what extent. We are therefore inspired to raise two important questions:
•	Can we theoretically characterize the trainability of graph neural networks with respect to depth,
thus understanding why deep GCNs fail to generalize?
•	Can we further design an algorithm to facilitate deeper GCNs, benefiting from our theoretical
investigation?
Our answers are yes to both questions. We resort to the infinitely-wide multi-layer GCN to de-
rive our solution. The research on infinitely-wide networks can be traced back to the seminal work
of Neal (1996), which showed that single hidden layer networks with random weights at initializa-
tion (without training) are Gaussian Processes (GPs) in the infinite width limit. Later, the connection
between GPs and multi-layer infinitely-wide networks with Gaussian initialization (Lee et al., 2018;
de G. Matthews et al., 2018) and orthogonal weights (Huang et al., 2021) was reported. Recent
trends in Neural Tangent Kernel (NTK) have led to a proliferation of studies on the optimization and
generalization of infinitely (ultra)-wide networks. In particular, Jacot et al. (2018) made a ground-
breaking discovery that gradient descent training in the infinite width limit can be captured by an
NTK. Du et al. (2019b) formulated Graph Neural Tangent Kernel (GNTK) for infinitely-wide GNNs
and shed light on theoretical guarantees for GNNs. Prior to the discovery of GNTK, there was lit-
tle understanding of the non-convexity of GNNs, which is analytically intractable. In the learning
regime of GCN governed by GNTK, the optimization becomes an almost convex problem, making
GNTK a promising perspective to study the trainability of deep GCNs.
In this work, we leverage the GNTK techniques of infinitely-wide networks to investigate whether
ultra-wide GCNs are trainable in the large depth. In particular, we formulate the large-depth asymp-
totic behavior of the GNTK, illuminated by innovative works on deep networks (Hayou et al., 2019b;
Xiao et al., 2020), through which we can analyze the optimization properties of deep GCNs. Specif-
ically, we make the following contributions:
•	To our best knowledge, we are the first to investigate the trainability of deep GCNs through GNTK.
We prove that all entries of a GNTK matrix regarding a pair of graphs converge exponentially to
the same value, making the GNTK matrix singular in the large depth. We thus establish a corollary
that the trainability of ultra-wide GCNs exponentially collapses on node classification tasks.
•	We apply our theoretical analysis to the residual connection-based techniques for GCNs. Our
theory shows that residual connection can, to some extent, slow down the exponential decay rate
of trainability, but lack the ability to fundamentally solve the problem. This result enables to better
understand why and to what extent recent residual connection-based methods work.
•	Our theoretical framework provides insights to guide the development of deep GCNs. We further
propose an edge-based sampling method, named Critical DropEdge, to effectively mitigate the
exponential decay of trainability. This graph-adaptive and connectivity-aware method is easy
to implement in both finitely-wide and infinitely-wide GNNs. Our experiments show using the
proposed method can outperform competitors in the large depth.
2	Background and Preliminaries
We first review the results of infinitely-wide neural networks at initialization. We then review NTK,
making a connection to trainability. Finally, we introduce GCNs along with our setup and notation.
2
Published as a conference paper at ICLR 2022
2.1	Infinitely-wide Networks at Initialization
We begin by considering a fully-connected network of depth L with width ml in each layer. The
weight and bias in the l-th layer are denoted by W(l) ∈ Rml ×ml-1 and b(l) ∈ Rml . Letting the
pre-activations be given by hi(l), information propagation in this network is governed by,
ml
h(l) = √σ= X φ(wjl)hjj)) + σbb(l)	⑴
ml
j=1
where φ : R → R is the activation function, σw and σb define the variance scale of the weights
and biases, respectively. Given the parameterization that weights and biases are randomly generated
by i.i.d. normal distribution, i.e., W(I), b(l)〜 N(0,1), the pre-activations are Gaussian distributed
in the infinite width limit as m1 , m2 , . . . , ml-1 → ∞. This results from the central limit theorem
(CLT). Consider a dataset X ∈ Rn×d of size n = |X|, the covariance matrix of Gaussian process
kernel (GPK) regarding infinitely-wide network is defined by Σ(l) (x, x0) = E[hi(l) (x)hi(l) (x0)]. Ac-
cording to the signal propagation (1), the covariance matrix or GPK with respect to layer can be
described by a recursion relation, Σ(l)(x, x0) = σW Eh 〜N (o,∑o-i))[Φ(h(x))Φ(h(x0))] + σ(.
The mean-field theory is a paradigm that studies the limiting behavior of GPK, which is a mea-
sure of expressivity for networks (Poole et al., 2016; Schoenholz et al., 2017). In particular, ex-
pressivity describes to what extent two different inputs can be distinguished. The property of
evolution for expressivity Σ(l) (x, x0) is determined by how fast it converges to its fixed point
∑*(χ,χ0) ≡ limι→∞ Σ(l)(χ,χ0). It is shown that in almost the entire parameter space spanned
by hyper-parameters σwand σb, the evolution exhibits a dramatic convergence rate formulated by
an exponential function except for a critical line known as the edge of chaos (Poole et al., 2016;
Schoenholz et al., 2017). Consequently, an infinitely-wide network loses its expressivity exponen-
tially in most cases while retaining the expressivity at the edge of chaos. Given this reason, we focus
on the edge of chaos in this work. In particular, We set the value of hyper-parameters to satisfy,
σ22 D Dz [φ0(√q*z)]2 = 1, where q* is the fixed point of diagonal entries in the covariance matrix,
and R Dz = √2∏ R dze- 1 Z is the measure for a normal distribution. For the ReLU activation, edge
of chaos requires σ2 = 2 and σb2 = 0.
2.2	Neural Tangent Kernel and Trainability
Most studies on infinitely-wide networks through mean-field theory (Poole et al., 2016; Schoen-
holz et al., 2017) have focused solely on initialization without training. Jacot et al. (2018) took
a step further by considering infinitely-wide networks trained with gradient descent. Let η be
the learning rate, and L be the loss function. The dynamics of gradient flow for parameters
θ = vec({Wi(jl), bi(l)}) ∈ R(Pl ml(ml-1+1))×1, the vector of all parameters, is given by,
∂θ
∂t = -ηvθ L = -ηνθ ft(X )T Vft(x)L
Then, the dynamics of output functions f(X) = vec(f (x)x∈X) ∈ RnmL×1 follow,
(2)
∂ft(X)
∂t
Vθ ft(x) ∂ = -ηθt(x,x )Vft(χ )L
where the NTK at time t is defined as,
Θt(X, X) ≡Vθft(X)Vθft(X)T ∈ RnmL×nmL
(3)
(4)
In a general case, the NTK varies with the training time, thus providing no substantial insights into
the convergence property of neural networks. Interestingly, as shown by Jacot et al. (2018), the
NTK converges to an explicit limiting kernel and does not change during training in the infinite-
width limit. This leads to a simple but profound result in the case of mean squared error (MSE) loss,
L = 1 ∣∣ft(X) - Y∣∣2, where Y is the label associated with the input X,
ft(X) = (I - e-ηΘ∞(X,X)t)Y + e-ηΘ∞(X,X)tf0(X)	(5)
where Θ∞ is the limiting kernel. This is the solution to an ordinary differential equation. As the
training time t tends to infinity, the output function fits the label very well, i.e., f∞ (X) = Y. As
3
Published as a conference paper at ICLR 2022
proved by Lemma 1 in Hayou et al. (2019b), the network is trainable only if Θ∞ (X, X) is non-
singular. Quantitatively, the condition number K ≡ λmaχ∕λmin can be a measure of trainability as
confirmed by Xiao et al. (2020).
2.3	Graph Convolutional Networks
We define an undirected graph as G = (V, E), where V is a set of nodes and E is a set of edges. We
denote the number of nodes in graph G by n = |V |. The nodes are associated with a node feature
matrix X ∈ Rn×d, and the corresponding labels are Y ∈ Rn×k, with d and k being the dimension
of node features and number of classes, respectively. In this work, we develop our theory towards
understanding the trainability of GCNs on node classification tasks.
GCNs iteratively update node features through aggregating and transforming the representations of
their neighbors. Figure 3 in Appendix A illustrates an overview of the information propagation
in a general GCN. We define a propagation unit to be the combination of a R-layer multi-layer
perceptron (MLP) and one aggregation operation. We use subscript (r) to denote the layer index
of MLP in each propagation unit and superscript (l) to indicate the index of aggregation operation,
which is also the index of the propagation unit. L is the total number of propagation units. To be
specific, the node representation propagation in GCNs through an MLP follows the expression,
h(0)(u) = |N (u1| + 1 X	h(R)1)(v)	⑹
v∈N (u)∪u
h(r))(u)=√mφ(W((r)h(r)-i)(U))+哂?)	⑺
where h((00)) = X, W((rl)) ∈ Rml ×ml-1, and b((lr)) ∈ Rml are the learnable weights and biases, respec-
tively, φ is the activation function, N (u) is the neighborhood of node u, and N (u) ∪ u is the union
of node u and its neighbors. Equation (6) reveals the node feature aggregation operation among its
neighborhood according to a GCN variant (Hamilton et al., 2017). Equation (7) is a standard non-
linear transformation with NTK-parameterization (Jacot et al., 2018), where m is the width, i.e.,
number of neurons in each layer, σw and σb define the variance scale of the weights and biases. For
the activation function, we focus on both ReLU and Tanh, which are denoted as φ(x) = max{0, x}
and φ(x) = tanh(x), respectively. Without loss of generality, our theoretical framework can handle
other common activation functions, whereas the GNTK work (Du et al., 2019b) only adopted ReLU.
3	Aggregation Provably Leads to Exponential Trainab ility Loss
3.1	GNTK Formulation
Based on the definition of NTK (4), we recursively formulate the propagation of GNTK in the
infinite-width limit. As information propagation in a GCN is built on two operations: aggregation (6)
and non-linear transformation (7), the corresponding formulas of GNTK are expressed as follows,
θ(O))(U,u0)=∣N(u1∣+ι ∣N(u1)∣+ι x x	θ(R)I)(V,vO)	⑻
u	u	v∈N(u)∪u v0∈N(u0)∪u0
θ(r;(U,uO)=θ(3) (U,〃- (r))(u,UO)+"u,UO)	⑼
The two equations above correspond to the aggregation operation and MLP transformation, respec-
tively. To compute the GNTK with respect to the depth, the key step is to obtain the covariance
matrix Σ((lr)) (U, UO) ≡ E[h((lr)) (U)h((lr)) (UO)]. According to the CLT, node representation h((lr)) (U) is a
Gaussian distribution in the infinite-width limit. Applying this result to equations (6) and (7), the
resultant covariance matrix is composed of two parts,
ς(O))(U，〃)= r⅛γ R⅛1	X X	ς(R)I)(V,v0)	(10)
v∈N(u)∪u v0∈N(u0)∪u0
■(" = σWEZ1,Z2〜n(o,∑(r-1)) [φ(z1)φ(z2)] + 蟾
斗?)(",〃)= σW%Z2〜N(o,∑(r-1)) [φ(zl)φ(z2)] + 咪	()
4
Published as a conference paper at ICLR 2022
The first equation (10) results from the aggregation operation. Meanwhile, the second equation
(11) corresponds to the R-times non-linear transformations, where Ez1,z2 takes the expectation with
respect to a centered Gaussian process of covariance ∑(?-1)∈ R2 ×2 for previous MLP layer across
u, u0 , and φ denotes the derivative of φ.
3.2	Trainability in the Large Depth
We aim to characterize the behavior of GNTK matrix Θ((lr))(G) ∈ Rn×n, as the depth tends to infinity.
From the GNTK formulation, both aggregation (8) and transformation (9) contribute simultaneously
to the final limiting result. We derive our theorem on the asymptotic behavior of infinitely-wide GCN
in the large depth limit, which is given as follows:
Theorem 1 (Convergence rate of GNTK). If transition matrix A(G) ∈ Rn2×n2 is irreducible and
aperiodic, with a stationary distribution vector ~π(G) ∈ Rn2×1, where Θ~ ((l0))(G) = A(G)Θ~ ((lR-)1)(G)
and Θ~ ((lr))(G) ∈ Rn2×1 is the result of being vectorized. Then, there exist constants 0 < α < 1 and
C > 0, and constant vectors ~v, ~v0 ∈ Rn2×1 depending on the number of MLP iterations R, such
that ∣Θ(r)(u, u0) — ~(G)T (Rl~ + ~0) ∣ ≤ Cal.
The proof sketch follows a divide-and-conquer manner. In particular, we first analyze the network
with only aggregation and prove that A(G) is a Markov transition matrix. Then, we formulate the
behavior of MLP in the large depth based on Hayou et al. (2019b). Finally, we derive the final result
by considering the two operations. We leave the complete proof in Appendix B.
In Theorem 1, we rigorously characterize the convergence properties of GNTK in the large depth
limit. As the depth goes to infinity, all entries in the GNTK converge to a unique quantity at an
exponential rate. We thus have Θ((lr))(G) ≈ ~π(G)T(Rl~v)1n×n as l → ∞, where 1n×n is an (n × n)-
dimensional matrix whose entries are one. The exponential convergence rate of Θ((lr)) (G) implies
that the trainability of infinitely-wide GCNs degenerates dramatically, as stated below.
Corollary 1 (Trainability of ultra-wide GCNs). Consider a GCN of the form (6) and (7), with
depth l, number of non-linear transformations r, an MSE loss, and a Lipchitz activation, trained
with gradient descent on a node classification task. Then, the output function follows, ft (X) =
e-ηΘ(r)(G)tf0(X) + (I — e-ηΘ(r)(G)t)Y. Then, Θ((lr))(G) is singular when l → ∞. Moreover, there
exists a constant C > 0 such that for all t > 0, kft(X) — Y k > C.
We leave proof in Appendix C. According to the above corollary, as l → ∞, the GNTK matrix
would become a singular matrix. This would lead to a discrepancy between output ft(X) and label
Y , which means GNTK loses the ability to fit the label. Therefore, an ultra-wide GCN with a large
depth cannot be trained successfully on node classification tasks.
4	Towards Deepening Graph Neural Networks
4.1	Theoretical Analysis on Residual Connection
We have so far characterized the trainability of vanilla GCNs through the GNTK and showed that
the trainability of ultra-wide GCNs drops at an exponential rate. Recently, considerable efforts have
been made to deepen GCNs, among which residual connection-based techniques are widely applied
to resolve the over-smoothing problem (Li et al., 2019). We now apply our theoretical framework to
analyze to what extent residual connection techniques could alleviate the trainability loss problem.
We first consider residual connection in aggregation, in which the propagation of the GNTK can be
formulated as,
Θ~(l)(G) = (1 — δ)A(G)Θ~(l-1) (G) + δΘ~ (l-1)(G),
(12)
where 0 < δ < 1. Taking equation (12) as a new aggregation process, then Θ~ (l) (G) =
A(G)Θ(l-1) (G), where A(G) = (1 — δ)A(G) + δI. We prove that A(G) is also a transition
matrix with a greater second largest eigenvalue compared to the original matrix A(G).
5
Published as a conference paper at ICLR 2022
Theorem 2 (Convergence rate of residual connection in aggregation). Consider a GNTK of non-
linear transformation (9) and residual connection (12). Then with a stationary vector ~π(G) for
A(G), there exist constants 0 < α < 1 and C > 0, and constant vectors V and v0 depending on
R, such that ∣θ(7)(u, U) 一 ~(G)T (Rl~ + ~0) ∣ ≤ CaI. Furthermore, we denote the Second largest
eigenvalue of A(G) and A(G) as λ2 and λ2, respectively. Then, λ2 > λ2.
The detailed proof of Theorem 2 and the relationship between convergence rate and the second
largest eigvenvalue can be found in Appendix D.1. This theorem implies that a residual connection
in aggregation can slow down the convergence rate, which is consistent with empirical observations
that residual connection can help deepen GCNs.
Then, we consider residual connection only applied on non-linear transformation (MLP). In this
case, the recursive equation for the corresponding GNTK can be expressed as,
θ(r))(u,u0) =θ(r-i)(u,uo)(∑(r))(u,uo)+1)+∑(r)(u, u0)	(13)
This formula is similar to the vanilla GNTK in the infinite-width limit. Only an additional residual
term appears according to residual connection. It turns out that this term may not help slow down
the convergence rate for non-linear transformation.
Theorem 3 (Convergence rate of GNTK with residual connection between transformations). Con-
sider a GNTK of the form (8) and (13). If A(G) is irreducible and aperiodic, with a stationary
distribution ~π(G), then there exist constants 0 < α < 1 and C > 0, and constant vectors ~v and ~v0
depending on R, such that, ∣θ(r)(u, u0) — ~(G)T(Rl(1 + σw)Rl~ + ~0) ∣ ≤ Cal.
Note that α in Theorem 3 is the same as in Theorem 1. The proof of Theorem 3 can be found
in Appendix D.2. Theorem 3 demonstrates that adding residual connection in MLP can not even
reduce the convergence rate of trainability. Finally, we consider residual connection applied to both
aggregation and non-linear transformation simultaneously:
Corollary 2 (Convergence rate of GNTK with residual connection in aggregation and transforma-
tion). Consider a GNTK of the form (12) and (13). If A(G) is irreducible and aperiodic, with a
stationary distribution ~(G), there exist constants 0 < a < 1 and C > 0, and constant vectors
~, v0 ∈ Rn2×1 depending on R, such that ∣θ(1))(u, u0) — ~(G)T (Rl(I + σww)Rl~ + ~0) ∣ ≤ Cal.
4.2	A New Sampling Method: Critical DropEdge
Residual connection is designed from a layer-wise perspective, but it has limited abilities to miti-
gate the exponential decay of trainability. To better resolve this problem, we need to look deeper
into the root cause of the problem - the transition matrix corresponding to the aggregation opera-
tion. A necessary condition for matrix A(G) to be a probability transition matrix is that graph G
is connected. Thus, breaking the connectivity condition is a promising way of better solving the
exponential decay problem. One such method is to perform edge sampling guided by the critical
percolation theory (Huang et al., 2018; Erdos & Renyi, 1961) in random graphs.
On a finite complete graph of n nodes, there exist Et = n(n — 1)/2 edges between all pairs of
nodes. A random graph G is achieved by randomly and uniformly preserving some edges from the
complete graph with an edge probability as P = E, where |E| is the number of edges preserved
in the random graph. In this way, the critical percolation can be realized in the random graph with
a critical edge probability Pc = 1/(n — 1) (Erdos & Renyi, 1961). In the thermodynamic limit of
n → ∞, the critical random graph exhibits critical connectivity: the probability that there exists a
path from a fixed point to another point within a certain distance decreases polynomially.
Proposition 1 (Critical connectivity in random graph (Erdos & Renyi, 1961)). Suppose a random
graph G has n nodes with a constant edge probability P.(1) If P < Pc, then almost every random
graph is such that its largest component1 is of size O(logn); (2) IfP > Pc, the random graph has a
giant component of size (1 — ap + o(1))n, where ap < 1; (3) IfP = Pc, then the maximal size ofa
component of almost every graph has order n2/3.
1In graph theory, a component of an undirected graph is an induced subgraph in which any two nodes are
connected to each other by paths.
6
Published as a conference paper at ICLR 2022
0
250	500	750
(d) Vanilla GNTK Convergence Rate
100
10-2
10-4
10-6
(C) Residual MLP GNTK Collapse
Depth
Figure 1: Convergence rate of GNKT. (a)-(c) Entries of the normalized (residual connection) GNTK
as a function of the depth, defined as Rl + r. All entries tend to have the same value as the
depth grows. (d)-(f) The element (entry)-wise distance of the normalized (residual connection)
GNTK as a function of the depth. The convergence rate can be bounded by an exponential function
y = exp(-0.15x) for vanilla and residual MLP GNTK, whereas the convergence rate of residual
aggregation is bounded by y = exp(-0.11x).
XIN9 J。guelsQBs-Mjubuibb
The proposition above implies that the information transforms in the critical random graph at a poly-
nomial rate rather than an exponential rate. This inspires us to solve the problem of exponentially
dropping trainability through designing a graph-dependent and connectivity-aware sampling algo-
rithm called Critical DropEdge. In particular, given a graph G, we randomly drop some edges and
preserve the number of edges as Er = Et ∙ Pc = n/2, to approximate the critical graph. Unlike
DropEdge (Rong et al., 2019) that randomly removes a certain number of edges from the input
graph, our method fixes the edge preserving percentage as P = Er /|E| = 2^. It is worth noting
that DropEdge may choose the edge probability as p < pc where information can only be passed
to a distance of O(log n) in the graph, or p > pc where the exponential decay of trainability may
occur. A further discussion of the relationship between Proposition 1 and the trainability of the
corresponding GNTK can be found in Appendix E.
5	Experiments
In this section, we empirically verify our theoretical results and validate the proposed Critical
DropEdge method on node classification tasks. Details of four real-world graph datasets used for
node classification are summarized in Table 3 in Appendix F.1.
5.1	Convergence Results of GNTKs
Theorems 1-3 provide theoretical convergence rates for (residual) GNTK. We show the correspond-
ing numerical verification in Figure 1. We select a graph randomly from a bioinformatics dataset
(i.e., MUTAG), which consists of 18 nodes and 21 edges. We generate a GNTK of the graph using
the implementation of Du et al. (2019b), with ReLU activation, R = 3, and L = 300. Figure 1(a)-
(c) show that all entries of normalized GNTKs converge to an identical value as the depth goes
larger. Figure 1(d)-(f) further indicates that the convergence rate of GNTK is exponential, as re-
flected by our theorems. By comparing convergence rates, we conclude that the residual connection
in aggregation can slow down the convergence rate, which is consistent with Theorem 2.
5.2	Trainability of Wide GCNs
We further examine whether ultra-wide GCNs can be trained successfully for node classification.
We conduct experiments on a GCN (Kipf & Welling, 2017), where we apply a width of 1, 000 at
7
Published as a conference paper at ICLR 2022
each hidden layer and the depth ranging from 2 to 29. Figure 2 shows the training and test accuracy
on Cora, Citesser and Pubmed after 300 training epochs. These results show a dramatic drop in
both training and test accuracy as the depth grows, confirming that wide GCNs lose trainability
significantly in the large depth on node classification, as revealed by Corollary 1.
5.3	Performance of Critical DropEdge
We apply Critical DropEdge (referred to as C-
DropEdge) to finitely-wide and infinitely-wide
GNNs on semi-supervised node classification. The
implementation details are given in Appendix F.2.
For finitely-wide GNNs, we consider three back-
bones: GCN, JKNet, and IncepGCN (Rong
et al., 2019). For DropEdge, we use the hyper-
parameters reported in Rong et al. (2019) to obtain
the results. For C-DropEdge, we perform a random
hyper-parameter search and fix the edge preserving
rate as P(G) = 2^. In addition, We compare with
DGN (Zhou et al., 2020), a normalization-based
baseline, which uses GCN (Kipf & Welling, 2017)
and GAT (Velickovic et al., 2018) as backbones.
Depth
Figure 2: Training and test accuracy w.r.t.
model depth. Solid and dashed lines are train
and test accuracy respectively.
Table 1 summarizes node classification performance of finitely-wide GNNs with 4/8/16/32 layers
on three citation networks (Cora, Citesser, Pubmed) and one co-author network (Physics). In par-
ticular, we report the best performance across different backbones for DropEdge, C-DropEdge and
DGN, and leave separate results with different backbones in Appendix G.2. The reported results are
the mean and standard deviation over 10 times. As can be seen, C-DropEdge consistently outper-
forms GCN, DGN, and DropEdge, especially when the model is deep. Besides, C-DropEdge can
achieve smaller error variances, demonstrating stronger robustness than DropEdge.
For infinitely-wide GCNs, we consider two backbones: GCN (Kipf & Welling, 2017) and JKNet
(Xu et al., 2018). The corresponding results can be found in Appendix G.1.
Comparison to DropEdge. DropEdge and C-DropEdge differ largely in their hyper-parameter
search space. To ensure good performance, DropEdge needs to exhaustively search for the most
appropriate edge preserving percentage - one of the most significant hyper-parameters - that has a
crucial influence on the final performance. In contrast, C-DropEdge has a fixed and graph-dependent
edge preserving rate, which implies its hyper-parameter space is much smaller than that of DropE-
dge. To verify if C-DropEdge can achieve the results close to optimal, we conduct experiments with
various edge preserving rates, and present the results in Table 2. We conclude that, from both theo-
retical and empirical perspectives, the edge preserving percentage set by C-DropEdge is reasonable
and effective, achieving the results close to optimal.
6	Related Work
Neural Tangent Kernel. NTKs are used to describe the dynamics of infinitely-wide networks
during gradient descent training. In the infinite-width limit, NTK converges to an explicit limit-
ing kernel; besides, it stays constant during training, providing a convergence guarantee for over-
parameterized networks (Jacot et al., 2018; Lee et al., 2019a; Allen-Zhu et al., 2019; Du et al.,
2019a; Zou et al., 2018). Besides, NTK has been applied to various architectures and brought a
wealth of results, such as orthogonal initialization (Huang et al., 2021), convolutions (Arora et al.,
2019), SVM (Chen et al., 2021), attention (Hron et al., 2020). As for graph networks, GNTK helps
us understand how GNNs learn a class of smooth functions on graphs (Du et al., 2019b) and how
they extrapolate differently from multi-layer perceptron (Xu et al., 2020).
Deep Graph Neural Networks. Since deep GNNs suffer from the over-smoothing problem, a
large and growing body of literature has made efforts in deepening GNNs. There is a line of methods
that resort to residual connection to retain their feature expressivity in deep layers. Xu et al. (2018)
use skip connection along with node representations from different neighborhood ranges to preserve
the locality of node representations. Klicpera et al. (2019) derive a personalized propagation of
8
Published as a conference paper at ICLR 2022
Table 1: Comparison results of test accuracy (%) between C-DropEdge, GCN, DropEdge, and DGN.
Datasets	Methods	4-layer	8-layer	16-layer	32-layer
	GCN	79.8 ± 1.1	73.2 ± 2.7	36.3 ± 13.8	20.1 ± 2.4
Cora	DropEdge	82.2 ± 0.7	82.0 ± 0.9	82.2 ± 0.7	82.1 ± 0.5
	DGN	82.0 ± 0.9	80.2 ± 0.8	77.7 ± 1.0	73.0 ± 0.8
	C-DropEdge	82.5 ± 0.7	82.3 ± 0.6	82.4 ± 0.8	82.6 ± 0.9
	GCN	61.2 ± 3.0	50.2 ± 5.7	30.8 ± 2.2	21.7 ± 3.0
Citeseer	DropEdge	70.2 ± 1.0	70.8 ± 1.1	70.7 ± 1.0	70.2 ± 0.8
	DGN	69.0 ± 0.9	66.5 ± 1.1	62.9 ± 1.2	63.2 ± 0.9
	C-DropEdge	70.8 ± 0.6	70.9 ± 0.9	71.0 ± 1.0	70.7 ± 0.9
	GCN	77.4 ± 0.7	57.2 ± 8.4	39.5 ± 10.3	36.3 ± 8.4
Pubmed	Dropedge	77.6 ± 1.4	77.3 ± 1.3	76.7 ± 1.3	77.2 ± 1.3
	DGN	78.2 ± 1.0	77.8 ± 1.2	77.2 ± 1.3	77.0 ± 1.1
	C-DropEdge	78.0 ± 0.4	77.9 ± 1.0	77.2 ± 1.0	77.8 ± 1.0
	GCN	90.2 ± 0.9	83.5 ± 2.2	41.6 ± 6.2	28.8 ± 9.4
Physics	Dropedge	91.6 ± 0.8	91.5 ± 0.7	91.2 ± 0.5	91.3 ± 0.8
	DGN	92.2 ± 1.0	86.4 ± 0.7	83.4 ± 0.6	83.2 ± 0.8
	C-DropEdge	91.9 ± 0.7	91.7 ± 0.6	92.0 ± 0.4	91.6 ± 0.6
Table 2: Comparison results of test accuracy at various edge preserving rates. Critical preserving
percentage for each dataset is marked in bold. The three best results per model are shaded in blue .
Cora			Citeseer			Pubmed		
Percentage	GCN-8	JKNet-4	Percentage	GCN-4	IncepGCN-4	Percentage	JKNet-16	IncepGCN-32
0.05	58.2 ± 19.6	82.0 ± 0.6	0.15	68.8 ± 1.2	70.1 ± 0.7	0.01	76.1 ± 1.8	75.5 ± 1.9
0.10	69.6 ± 14.4	82.1 ± 0.6	0.20	68.8 ± 1.1	70.6 ± 0.9	0.05	76.2 ± 1.5	76.1 ± 1.3
0.15	69.7 ± 12.5	82.2 ± 0.6	0.25	68.7 ± 0.8	70.5 ± 0.9	0.10	76.0 ± 1.4	76.8 ± 1.3
0.20	75.4 ± 4.0	82.5 ± 0.7	0.30	68.9 ± 0.8	70.5 ± 0.9	0.15	76.9 ± 0.9	77.0 ± 1.4
0.25	77.3 ± 2.5	82.5 ± 0.7	0.35	69.0 ± 0.8	70.8 ± 0.6	0.22	76.9 ± 0.9	77.8 ± 0.9
0.30	77.2 ± 2.7	82.2 ± 1.1	0.40	68.9 ± 0.9	70.5 ± 0.4	0.25	76.9 ± 1.1	75.8 ± 2.5
0.35	77.2 ± 2.8	81.7 ± 0.8	0.45	68.4 ± 1.7	70.1 ± 0.7	0.30	76.8 ± 1.1	77.1 ± 1.2
0.40	74.9 ± 5.7	81.4 ± 0.6	0.50	68.1 ± 0.9	70.2 ± 0.8	0.35	76.4 ± 1.2	77.6 ± 1.3
0.45	75.5 ± 6.4	81.7 ± 0.7	0.55	68.0 ± 1.0	70.3 ± 0.7	0.40	76.3 ± 1.3	77.6 ± 1.1
neural predictions (PPNP) based on personalized Pagerank. Other similar works using residual
connection can also be seen in (Li et al., 2019; Gong et al., 2020; Chen et al., 2020; Liu et al.,
2020). Another line of approaches tackle the issue via regularization mechanisms, such as node/edge
dropping (Hou et al., 2019; Rong et al., 2019), batch normalization (Dwivedi et al., 2020), pair
normalization (Zhao & Akoglu, 2020), and group normalization (Zhou et al., 2020). Recently, a
series of latest works (Loukas, 2020; Zeng et al., 2020; Li et al., 2020; Cong et al., 2021; Huang et al.,
2020) explore the underlying reasons for performance degradation towards mitigation solutions.
7	Conclusion and Discussion
In this work, we have characterized the asymptotic behavior of GNTK to measure the trainability
of wide GCNs in the large depth. We prove that the trainability drops at an exponential rate due
to the aggregation operation. Furthermore, we apply our theoretical framework to investigate to
what extent residual connection-based techniques help deepen GCNs. We demonstrate that these
techniques can merely slow down the decay rate, but are unable to solve the exponential decay
problem in essence. To overcome the trainability loss problem, we further propose Critical DropE-
dge illuminated by our theoretical framework. The experimental results confirm that our method
can mitigate the trainability problem of deep GCNs. Future research directions include designing a
critical node-centric method so as to make better use of node information.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was partially supported by a Collaborative Research Project grant between The University
of Sydney and Data61, Australia. We thank the anonymous reviewers for useful suggestions to
improve the paper. We also thank Ye Su for helpful discussions. This work was also in collaboration
with Digital Research Centre Denmark - DIREC, supported by Innovation Fund Denmark.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph con-
volutional networks. In International Conference on Machine Learning, pp. 1725-1735. PMLR,
2020.
Yilan Chen, Wei Huang, Lam Nguyen, and Tsui-Wei Weng. On the equivalence between neural
network and support vector machine. Advances in Neural Information Processing Systems, 34,
2021.
Fan Chung and Mary Radcliffe. On the spectra of general random graphs. The electronic journal of
combinatorics, pp. P215-P215, 2011.
Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training
graph convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations, 2018.
Xue Ding and Tiefeng Jiang. Spectral distributions of adjacency and laplacian matrices of random
graphs. The annals of applied probability, pp. 2086-2117, 2010.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International conference on machine learning, pp. 1675-
1685. PMLR, 2019a.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems, pp. 5723-5733, 2019b.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
Paul Erdos and Alfred Renyi. On the strength of connectedness of a random graph. Acta Mathemat-
ica Hungarica, 12(1):261-267, 1961.
Paul Erdos and Alfred Renyi. On the evolution of random graphs. In The Structure and dynamics of
networks, pp. 38-82. Princeton University Press, 2011.
Ari Freedman. Convergence theorem for finite markov chains. Proc. REU, 2017.
Shunwang Gong, Mehdi Bahri, Michael M Bronstein, and Stefanos Zafeiriou. Geometrically prin-
cipled connections in graph neural networks. In ProceedingS of the IEEE/CVF Conference on
Computer ViSion and Pattern Recognition, pp. 11415-11424, 2020.
Ingo Guhring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. arXiv
preprint arXiv:2007.04759, 2020.
10
Published as a conference paper at ICLR 2022
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing Systems, pp. 1024-1034, 2017.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on
deep neural networks training. In International conference on machine learning, pp. 2672-2680.
PMLR, 2019a.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-field behaviour of neural tangent
kernel for deep neural networks. arXiv preprint arXiv:1905.13654, 2019b.
Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang
Yang. Measuring and improving the use of graph information in graph neural networks. In
International Conference on Learning Representations, 2019.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and
ntk for deep attention networks. In International Conference on Machine Learning, pp. 4376-
4386. PMLR, 2020.
Wei Huang, Pengcheng Hou, Junfeng Wang, Robert M Ziff, and Youjin Deng. Critical percolation
clusters in seven dimensions and on a complete graph. Physical Review E, 97(2):022107, 2018.
Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks
with orthogonal initialization. In Proceedings of the Thirtieth International Joint Conference on
Artificial Intelligence, IJCAI-21, pp. 2577-2583, 2021.
Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-
smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design
with self-supervision. In International Conference on Learning Representations, 2020.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, 2019a.
Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International confer-
ence on machine learning, pp. 3734-3743. PMLR, 2019b.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can GCNs go as deep
as CNNs? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
9267-9276, 2019.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.
11
Published as a conference paper at ICLR 2022
Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
338-348, 2020.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Con-
ference on Learning Representations, 2020.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural informa-
tion processing systems, pp. 3360-3368, 2016.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2019.
Jeffrey S Rosenthal. Convergence rates for markov chains. Siam Review, 37(3):387-405, 1995.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Graph neural
networks exponentially lose expressive power for node classification. In International Conference
on Learning Representations, 2017.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Petar VeliCkovic, Guillem CucurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. In International Conference on Learning Representations,
2018.
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and general-
ization in deeP neural networks. In International Conference on Machine Learning, PP. 10462-
10472. PMLR, 2020.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. RePresentation learning on graPhs with jumPing knowledge networks. In International
Conference on Machine Learning, PP. 5453-5462. PMLR, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graPh neural
networks? In International Conference on Learning Representations, 2019.
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
How neural networks extraPolate: From feedforward to graPh neural networks. arXiv preprint
arXiv:2009.11848, 2020.
Hao Yuan and S. Ji. StructPool: Structured graPh Pooling via conditional random fields. In ICLR,
2020.
Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, RajgoPal Kannan, Viktor Prasanna,
Long Jin, Andrey Malevich, and Ren Chen. DeeP graPh neural networks with shallow subgraPh
samPlers. ICLR, 2020.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations, 2020.
Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deePer
graPh neural networks with differentiable grouP normalization. In Advances in neural information
processing systems, 2020.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent oPtimizes
over-Parameterized deeP relu networks. arxiv e-Prints, art. arXiv preprint arXiv:1811.08888,
2018.
12
Published as a conference paper at ICLR 2022
A APPENDIX: A General GCN Architecture
GCNs iteratively update node features through aggregating and transforming the representation of
their neighbors. Figure 3 illustrates an overview of information propagation in a general GCN
considered in this work.
Figure 3: Overview of the information propagation in a general GCN.
B	APPENDIX: Proofs for Theorem 1
B.1	Convergence of Aggregation GNTK
In Theorem 1 we demonstrate that matrix A(G) is a transition matrix, we first prove this proposition.
To this end, we block the non-linear transformation by setting R = 0. Then, the propagation of
GNTK can be expressed as,
Θ(l) (u, u0)
____1________1____ V V	Θ(l-1)(v v0)
N(U)।+ι∣N(u0)∣+1 v∈Nk)∪uvo∈N⅛)∪uo	(,	)
In order to facilitate the calculation, we rewrite Equation (14) as the following format,
Θ~ (l) (G) = A(G)Θ~ (l-1) (G)
(14)
(15)
where Θ~ (l)(G) ∈ Rn2×1 is the result of being vectorized, and matrix A(G) ∈ Rn2×n2 is a square
matrix. We show the key result that A(G) is a probability transition matrix, and the limiting behavior
of Θ(l)(G) in the following lemma,
Lemma 1 (Convergence of aggregation). Assume R = 0, then
lim Θ(l) (u, u0) = ~π(G)T Θ~ (0)(G)
l→∞
where π(G) ∈ Rn2×1, satisfying A(G)~π(G) = ~π(G).
Proof. When R = 0, Equation (14) reduces to,
Θ(l) (u, u0)
____1_________1____ V V	Θ(l-1)(v v0)
N(U)I+ 1∣N(u0)l + 1 v∈N⅛)∪uvo∈N⅛)∪u0	(, )
In order to facilitate calculation, we rewrite the above equation in the format of matrix,
Θ~ (l) (G) = A(l) (G)Θ~ (l-1) (G)
where Θ~ (l)(G) ∈ Rn2 ×1, is the result of being vectorized. Thus, the matrix operation A(l)(G) ∈
Rn2×n2. It is obvious that,
A(L)(G) = A(LT)(G)=…=A(I)(G)=…=A(I)(G)
This implies that the aggregation operation is the same for each layer. The next step is to prove
A(G) is a stochastic matrix (transition matrix):
A(G)ij = 1.
j
13
Published as a conference paper at ICLR 2022
According to Equation (14), A(G) can be expressed as a Kronecker product of two matrices,
A(G) = [B(G)C(G)]%[B(G)C(G)]
where B(G), C(G) ∈ Rn×n. We then analyse the two matrices separately:
(1)	B(G) is a diagonal matrix, which corresponds to the factor N(白十].
/____1___
N (uι)+1
B(G) =
∖
1
N (Un)+ 1
(2)	The element of matrix C(G) is determined by whether there exists an edge between two vertices,
_ , ... ≈
C (G)ij = δij
where δij = 1 if i == j or there is an edge between vertex i and j, otherwise δij = 0.
We then use the property of matrix B and C before Kronecker product,
X[B(G)C (G)]j = N(⅛+I X 易=N(⅛+!(N (Ui) + 1) = 1
According to the definition of Kronecker product, we have,
j
A(l)(G)ij =	[B(G)C(G)]ab[B(G)C(G)]cd = 1
bd
where i = a + (c - 1)n, and j = b + (d - 1)n.
So far, we have proved that A(G) is a stochastic matrix. According to the Perron-Frobenius Theory,
a stationary probability vector ~π(G) ∈ Rn2×1 is defined as a distribution, which does not change
under application of the transition matrix; that is, it is defined as a probability distribution, which is
also an eigenvector of the probability matrix, associated with eigenvalue 1:
A(G)~π(G) = ~π(G)
Note that the spectral radius of every stochastic matrix is at most 1 by Gershgorin circle theorem.
Thus, the convergence rate is governed by the second largest eigenvalue.
Since liml→∞ Ai(jl)(G) = ~πj(G), we have,
lim Θ~ (l) (G) = lim Al (G)Θ~ (0) (G) = Π(G)Θ~ (0) (G)
l→∞	l→∞
where Π(G)
~π(G)T
~(G)T
is the n2 × n2 matrix all of whose rows are the stationary distribu-
~π(G)T
tion. Then, we can see that every element in Θ~ (l)(G) converges exponentially to an identical value,
depending on the stationary distribution and initial state,
lim Θ(l)(u,u0) = ~π(G)T Θ~ (0)(G)
l→∞
□
Remark 1. This lemma can be extended to the multi-graph setting, where matrix A(G, G0) ∈
Rnn0×nn0 with respect to a pair of graphs G, G0 is a transition matrix as well, and n0 is the number
of vertices in graph G0.
14
Published as a conference paper at ICLR 2022
B.2	Convergence of MLP GPK
Before we prove Theorem 1, we introduce the result for the Gaussian Process kernel Σ((lr))(G) of
a pure MLP. By doing so, we consider a network with only non-linear transformation, known as a
pure MLP. This leads to Σ((lr))(u, u0) = Σ(r)(u, u0), where we use subscript (r) to denote the layer
index. And we rewrite the propagation function for GPK as follows,
工⑺(U,u) = σWEZ1,Z2〜N(0,Σ(I)) [φ(z1)φ(z2)] + σ2
(16)
and the variance Σ(r) ∈ R2×2 is,
Σ(r)
(u, u) Σ(r)(u, u0)
(u0 , u) Σ(r) (u0 , u0)
(17)
The large depth behavior has been well studied in Hayou et al. (2019a), and we introduce the result
when the edge of chaos is realized. In particular, we set the value of hyper-parameters to satisfy,
σW / Dz [φ0(√q*z)]2
where q* is the fixed point of diagonal elements in
√2∏ R dze-2Z is the measure for a normal distribution.
requires σw2 = 2 and σb2 = 0.
(18)
the covariance matrix, and Dz =
For the ReLU activation, equation (18)
1
The key idea is to study the asymptotic behavior of the normalized correlation defined as,
Cr (u, u ) ≡
,Σ(r) (u,u) Σ(r)(u0,U0)
(19)
Lemma 2 (Proposition 1 and 3 in Hayou et al. (2019a)). Assume a network without aggregation,
i.e., L = 0, with a Lipschitz nonlinearity φ, then,
•	φ(x)	(x) +, 1 — Cr (u, U) Z 2r2 as r -→ oo
•	φ(x) = tanh(x), 1 — Cr (u, u0)〜e as r → ∞ where β = 2RDzM√∣M ∙
Proof. We first decompose the integral calculation in the Equation (16) into two parts, diagonal
elements and non-diagonal elements:
£(r)(u,u)=σw L &iq^r-)(UU)Z)+σb
Σ(r)(U, U0) = σw2	φ(U1)φ(U2) +σb2
Dz1 Dz2
where u1
,Σ(r-i)(u, U) z ι,	and u 2
，“r-1)(U0,u0)
1(u, u0)z1 +
'1 - C2-1 (U U)Z2 ) , with Cr-l(u,u0) = Σ(r-I)(u,u0)/ P∑(r-ι)(u, u)Σ(r-ι) (u0, U0).
For simplicity, we define qr(u) = Σ(r)(u, u), qr(u0) = Σ(r)(u0, u0). We then proceed with the proof
by dividing the activation φ(x) into two classes, namely, ReLU and Tanh.
ReLU activation, φ(x) = max{0, x}. The recursive equation (16) forqr(u) reduces to,
σ2
qr (U) = ~wqqr-1 (U) + σb
The edge of chaos condition σW / Dz[φ0(√q*z)]2 = 1 requires σW = 2 and σb = 0 for ReLU
activation, which leads to,
lim qr (u) = q0 (u) ≡ q(u)
r→∞
15
Published as a conference paper at ICLR 2022
Then the second integration for Cr (u, u0) becomes,
σW JDzIDz2 φ(∙√qr-1(U)ZI)φ(∙√qr-1(U)(Cr-I(U,u )z1 +，1 - Cr-I(U,uO)2z2力 + σ2
Cr (U,U0) = ----1DZ----------------------------------------------------------------------
qr-1(U)
To investigate the propagation of Cr(U, U0), We set qr(U) = qr(U0) = q, and define,
、	σW JDzIDz2 φ(√qzl)φ(√q(* xz1 + √1 — x2z2)) + σ2
f (x) =-----------------------------------------------------
q
Let X ∈ [0, 1], the derivative of f(X) satisfies,
f (X) = 2	1z1 >01
xz1 + √1-x2z2 >0
This can be seen from a simple derivation,
f0(X) =	φ(zι)φ0(xz1 + p1 - x2z2)(zι --
φ(z1)φ0(Xz1 +	1 — X2z2)(z1) —
2
X
~√^ Z2)
一 、「 ，	/---τ 、， X 、
φ(zi)φ (xzi + V 1 — X2z2)(-7==z2)
1 — X2
Using an identity for Gaussian integration D zg(z) = D g0(z) yields,
f0(x) = /	/	[φ0(zι)φ0(xzι + P 一 X2Z2) + φ(zι)φ0(xzι + p1 - X2Z2) — φ(zι)φoo(xzι + p1 - X2Z2)]
φ0(z1)φ0(Xz1 +	1 — X2z2)
2
Then the second derivative of f (x) becomes, f00(χ) = ∏√-χz. So using the equation above and
the condition f0(0)
1, We can obtain another form of the derivative of f (x),
f 0(x) = ɪ arcsin(x) + ɪ
Because J arcsin = X arcsin +√1 — x2 and f (1) = 1, then for X ∈ [0,1],
f(x)
2x arcsin(x) + 2,1 — x2 + xπ
2π
Substituting f(x) = Cr(U, U0) and x = Cr-1(U, U0), into expression above here, We have,
Cr (U, U0)
2Cr-1 (U, U0) arcsin(Cr-1 (U, U0)) + 2 1 — Cr2-1 (U, U0) + Cr-1 (U, U0)π
2π
NoW We study the behavior of Cr(U, U0) as r tends to infinity. Using Taylor expansion, We have,
f (χ)∣x→ι- = X + 孕(1 — x)3/2 + O((1 — x)5/2)
3π
By induction analysis, the sequence Cr(U, U0) increases to the fixed point 1. Besides, We can replace
x With Cr (U, U0),
Cr+l(u, U0) = Cr (u, U0) +--ə―(1 — Cr (u, U0))3/2 + O((1 — Cr (u, U0))5/2)
3π
Let γr = 1 — Cr(U, U0), then We have,
16
Published as a conference paper at ICLR 2022
so that,
γ-1/2 = Y-1/2(1 -竽y1/2 + O(y3/2))-1/2 = y-1/2 + 宇 + O(γr)
r	3π	3π
As r tends to infinity, we have,
-1/2
γr+1

γ-1/2
〜
2
3∏
It means,
γ-1/2 〜y2r
3π
Therefore, we have,
1 — Cr (u, u0)〜
9π2
不
Tanh activation, φ(x) = tanh(x). For the argument of fixed point, we ask readers to refer to
Hayou et al. (2019a). Here, we only discuss the convergence rate of GPK, which means we directly
assume that f(x) tends to 1 as the depth tends to infinity limr→∞ Cr(u, u0) = 1. For the function
f (x), a Taylor expansion near 1 yields,
f (x) = 1 + (x — 1)f0(1) + (x--1^ f00(1) + O((X - 1)5/2)
where f0(1) = σW RDz [φ0(√qz)2], and f00(1) = σW q RDz [φ00(√qz)2]. Denote Yr = 1 - Cr (u,u0),
then we have,
2
Yr + 1 = Yr -才 + O(Y5" )
Where β =冷器√?；. Therefore，
Y-II=Y-I(I- Yr+O(Y3/2))=Y-I+不+O(Yr/2).
ββ
Thus, we have,
1 — C(r) (u, U)〜_ as r -→ oo
□
Lemma 2 shows that the covariance matrix converges to a constant matrix at a polynomial rate of
1/r2 for ReLU and of 1/r for tanh activation on the edge of chaos. This implies that a network
without aggregation could retain its expressivity at a large depth. However, for general cases, due
to aggregation, the rate of convergence would change from polynomial to exponential. This would
cause the trainability of deep GNNs to be problematic, as shown in the following section.
B.3	Convergence of GPK for GCNs
Then we formally characterize the convergence of Gaussian Process kernel Σ((lr))(u, u0) ofa GCN in
the infinite-width limit:
Lemma 3. If A(G) is ireducible and aperiodic, with a stationary distribution vector ~π(G), then
there exist constants 0 < α < 1 and C > 0, and constant vector ~v ∈ Rn2 ×1 depending on the
number of MLP iterations R, such that
∣∑(r)(u,u0)- ~(G)T~| ≤ Cal
Proof. We prove the result via an induction method. For l = 0, according to the Cauchy-
Buniakowsky-Schwarz Inequality
Σ((00))(u, u0) = h(u0)h(u00) ≤ kh(u0)k2kh(u00)k2 = 1
17
Published as a conference paper at ICLR 2022
Thus, there is a constant C, depending on G(V, E), and the number of MLP operations R, over
feature initialization, such that,
Σ((00))(u,u0) - ~π(G)T~v < C
Assume the result is valid for Σ((lr))(u, u0), then we have,
Σ((lr))(u,u0)- ~π(G)T~v ≤ C0αl
where C0 is a constant satisfying 0 < C0 < C. Now we consider the distance between Σ((lr+) 1) (u, u0)
and Cαl+1. To compute this, we need to divide the propagation from l layer to l + 1 layer into three
steps:
L 4?)→ 斗 r+i)→ —→ ς(R)
2.	Σ((lR)) → Σ((l0+)1)
(l+1)	(l+1)	(l+1)
3.	ς(0)	→ ς(i)	→--------→ 乙⑺	.
It is not hard to find that steps 1 and 3 correspond to non-linear transformation while step 2 corre-
sponds to aggregation operation, we then characterize them one by one,
MLP Propagation The assumption Σ((lr))(U, U0) - ~π(G)T~v ≤ C0αl implicitly implies that
Σ(l) (u,u0)
Cr (u, U0) is close to one. Because Cr (u, u0) = / 小(r)	小 ,then,
Σ((lr))(u,u) Σ((lr)) (u0,u0)
~(G)Tv — C0al	o	~(G)Tv + C0al
~(G)Tv + C°al - r "，" 一 ~(G)Tv - C°al
Because C0αl	~π(G)T ~v, we have Cr(U, U0) = 1+O(αl). Recall the property of MLP propagation
function f(X) for X = Cr (U, U0), when Cr (U, U0) is close to 1:
f (χ)iχ→1-=X+T(1- x)3/2+O((I-X)5/2)
This implies,
..	...	2 λ√2 .	... 3	... 5	, 3，.
∣C(r + 1) (u, U ) — C(r)(u, U )| = -.(1 — C(r)(u, U ))2 + 0(1 — C(r)(u, U ))2 = O(α2 )
With this result, we can further obtain,
∣∑(r+i)(u,u0) - ~(G)T~| = ∣∑(r+i)(u,u0) - ∑(?)(u,u0) + ∑(r)(u,u0) - ~(G)T~|
≤ ∣∑(r+i)(u,uo) - ∑(r))(u,uo)∣+∣∑(r))(u,uo) - ~(G)T 研
=cr+i)(u,uo) - c(r)(U, u0) । q∑(r))(u,u)∑(r))(u0,u0)+同(?)(”,〃)- ~(G)T 研
=C1 α2l + C0αl ≤ (C0 + C1)αl.
where C1 is a positive constant. Repeat the proof process, we have a relation for Σ((lR))(U, U0) at the
last step of 1,
∣∑(R)(u, u0) - ~(G)T~| ≤ (Co + (R - r)Cι)αl.	(20)
18
Published as a conference paper at ICLR 2022
Aggregation Propagation We go through an aggregation operation A(G). In this case, we use a
matrix form, and take equation (20) into aggregation function, yielding the result as follows,
Σ~((l0+)1)(G) = A(G)Σ~ ((lR))(G) = A(G)(Π(G)~v + O~ (αl))
where O~(αl) ∈ Rn2×1 denotes a vector in which every element is bounded by αl.
According to Theorem 4.9 in Freedman (2017), we have AlΠ(G)~v = Π(G)~v as l → ∞. When l is
finite, the error is of exponential decay. Here, we use 0 < α < 1 to denote the corresponding base
number. Therefore,
∣∑(0+1)(u,u0) - ~(G)Tv| ≤ (Co + (R - r)Cι)αl+1.
Finally, by repeating the result in step 1, we have,
∣∑(r+1)(u,u0) - ~(G)T~| ≤ (Co + RCι)αl+1 = Cαl+1.
□
Remark 2. In the proof, there is a RC1 term in each propagation of l → l + 1, which may lead
to explosion when l tends to infinity. Basically, this problem can be solved by a careful analysis,
because the constant Ci is associated with O(ɑ3l), which has a SmaUer order compared to O(ɑl).
B.4	Convergence of GNTK
Finally, we arrive at our main theorem:
Theorem 1 (Convergence rate of GNTK). If transition matrix A(G) ∈ Rn2 ×n2 is irreducible and
aperiodic, with a stationary distribution vector ~π(G) ∈ Rn2×1, where Θ~((lo))(G) = A(G)Θ~((lR-)1)(G)
and Θ~((lr))(G) ∈ Rn2×1 is the result of being vectorized. Then there exist constants 0 < α < 1 and
C > 0, and constant vectors ~v, ~v0 ∈ Rn2×1 depending on the number of MLP iterations R, such
that ∣θ(r)(u, u0) — ~(G)T (Rl~ + v0) ∣ ≤ Cal.
Proof. This proof has the same strategy to that of Lemma 3. The first step is to understand Equation
(9) in the large-depth limit.
θ(r))(u,uO)=θ(r-1)(U,"0)斗?)(U,uO)+4?)(U,uO)
According to the result of Lemma 3, we have already known,
Σ((lr))(U,UO) = ~π(G)T ~v + O(αl)
To proceed the proof, We need to work out the behavior of Σ(?) (u, u0) in the large depth.
ReLU activation, φ(x) = max{0, x}. Recall that we define Cr+1 = f(Cr), and we have,
f 0(x) = ɪ arcsin(x) + ɪ
Then, at the edge of chaos,
Σ(r)(u, u0) = f(Cr(u, u0)) = J arcsin(C/(u, u0)) + -
π2
2
=1 - -(1 - Cr(u,u0))1/2 + O(1 - Cr(u,u0))3/2 = 1 + O(αl/2)
19
Published as a conference paper at ICLR 2022
Tanh activation, φ(x) = tanh(x). We have
f0(x)=1-(x-1)f00(1)+O((x-1)2)
At the edge of chaos,
Σ (r)(u,u0) = 1 + O(αl)
Now we prove the result via an induction method. For l = 0, we directly have,
Θ((00))(u, u0) = Σ((00))(u, u0) ≤ kh(u0)k2kh(u00)k2 = 1
Thus there is a constant C, depending on G(V, E), and the number of MLP operations R, over
feature initialization,
∣θ(0)(u, u0) — ~(G)T~0∣ < C
Assume the result is valid for Θ((lr))(u, u0), then we have,
∣θ(r))(u,u0) — ~(G)T(Rl~ + ~0)∣ ≤ Cal
Now we consider the distance between Θ((lr+) 1)(u, u0) and ~π(G)T (R(l + 1)~v+~v0). To prove this, we
need to divide the propagation from l layer to l + 1 layer into three steps:
L θ(4→ 明)→ …→ 喘
2.	Θ((lR)) → Θ((l0+) 1)
3.	θ(o+1)→ θ(i+1)→…→ θ(r+1).
For step 1, we have,
∣θ(r+i)(u,u0) — ~(G)T ((lR +1)v + v0)∣
=∣∑(r+i)(u,u0 ) + ∑ (r+i)(u,u0 )θ(r))(u,u0) — ~(G)T ((IR +1)~ + ~0)|
=∣~(G)T~(1 + O(al)) + (1 + O(a"2))θ(4(u,u0) — ~(G)T((IR + 1)~ + v0)∣ ≤ Cal
Repeat the process, we have a relation for Θ((lR))(u, u0) at the last transformation in step 1,
∣θ(R)(U,u0) — ~(G)T((lR + R — r)~ + ~0)∣ ≤ Cal.
Secondly, we go through an aggregation operation. Because it is a Markov chain step, we have,
∣θ(0+I)(U,u0) — ~(G)T((RI + R — r)v + ~)| ≤ Cal+1.
Repeat the result in step 1, we finally have,
∣θ(r+1)(u, U0) — ~(G)T ((Rl + R)~ + v0)∣
= ∣θ(r+1)(u,u0) — ~(G)T(R(l + 1)~ + VI ≤ Cal+1.
□
C Trainab ility of Ultra-Wide GCNs
Corollary 1 (Trainability of ultra-wide GCNs). Consider a GCN of the form (6) and (7), with
depth l, number of non-linear transformations r, an MSE loss, and a Lipchitz activation, trained
with gradient descent on a node classification task. Then, the output function follows, ft (X) =
e-ηΘ(r)(G)tf0(X) + (I — e-ηΘ(r) (G)tY ), where X andY are node features and labels from training
set. Then, Θ((lr))(G) is singular when l → ∞. Moreover, there exists a constant C > 0 such that for
allt > 0, kft(X) — Y k > C.
20
Published as a conference paper at ICLR 2022
Proof. According to the result from Jacot et al. (2018), GNTK Θ((lr)) (G) converges to a deterministic
kernel and remains constant during gradient descent in the infinite-width limit. We omit the proof
procedure for this result, since it is now a standard conclusion in the NTK study.
Based on the conclusion above, Lee et al. (2019a) proved that the infinitely-wide neural network is
equivalent to its linearized mode,
ftin(X ) = fo (X)+ Vθ fo (X )∣θ=θo ωt
where ωt = θt - θ0 . We call it a linearized model because only zero and first order term of Taylor
expansion are kept. Since we know the dynamics of gradient flow using this linearized function are
governed by,
ω t = -ηvθ f0(X )T V ftin(X)L
ftlin(x) = -ηθ(?) (G)Vftin(X)L
where L is an MSE loss, then the above equations have closed form solutions
ftlin(X) = e-ηΘ((lr))(G)tf0(X) + (I - e-ηΘ((lr))(G)t)Y
Since Lee et al. Lee et al. (2019a) showed that ftlin(X) = ft(X) in the infinite width limit, thus we
have,
ft(X) = e-ηΘ((lr))(G)tf0(X) + (I - e-ηΘ((lr))(G)t)Y	(21)
In Theorem 1, we have shown Θ((lr)) (G) converges to a constant matrix at an exponential rate, thus
being singular in the large depth limit. According to Equation (21), we know that,
kft(X)-Yk = ke-ηΘ((lr))(G)t(f0(X) -Yk
Let Θ((r∞) ) be the GNTK in the large depth limit, Θ((r∞))(G) = QT DQ be the decomposition of the
GNTK, where Q is an orthogonal matrix and D is a diagonal matrix. Because Θ((r∞) ) (G) is singular,
D has at least one zero value dj = 0, then
kft(X)-Yk=kQT(ft(X)-Y)Qk ≥ k[QT(f0(X)-Y)Q]jk
□
Remark 3. In the proof we assume the loss is MSE. Nevertheless, the conclusion regarding train-
ability can be applied to other common losses such as cross-entropy. For cross-entropy loss, even
though we cannot derive an analytical expression for the solution, we can prove that the GNTK
still governs the trainability and the law of GNTK is not affected by the loss. Thus, the results for
trainability still hold in the cross-entropy case.
D Convergence of Residual GNTK
D. 1 Residual connection in aggregation
Theorem 2 (Convergence rate of residual connection in aggregation). Consider a GNTK of non-
linear transformation (9) and residual connection (12). Then with a stationary vector ~π(G) for
A(G), there exist constants 0 < α < 1 and C > 0, and Constant vectors V and v0 depending on
R, such that ∣θ(7)(u, U) 一 ~(G)T (Rl~ + ~0) ∣ ≤ CaI. Furthermore, we denote the second largest
eigenvalue of A(G) and A(G) as λ2 and λ2, respectively. Then, λ2 > λ2.
Proof. According to the aggregation function for covariance matrix, we have
Θ~ (l) (G) = (1 - δ)A(G)Θ~ (l-1)(G) +δΘ~(l-1)(G)
=((1 一 δ)A + δI)Θ(IT)(G) = A(G)Θ(IT)(G)
21
Published as a conference paper at ICLR 2022
The new aggregation matrix A(G) is a stochastic transition matrix as well, which can be seen from,
X A(G)ij = (1 - δ) X A(G)ij + δ X Iij = I
Then we compare the second largest eigenvalue between two transition matrices. Suppose the eigen-
values of the original matrix A(G) are {λι > λ? > … > λn2}. We already know that the maximum
eigenvalue is λ1 = 1, and the converge rate is governed by the second largest eigenvalue λ2 . Now
we consider the second largest eigenvalue λ2 of matrix A:
λ = (1 — δ)λ2 + δ = λ2 + δ(1 — λ2) > λ2
□
Remark 4. The theorem above gives us a certain conclusion about the relationship between λ2 and
λ2. Here, we discuss more about the relationship between a and a. According to (Rosenthal, 1995),
the relationship between convergence rate α and the second largest eigenvalue λ2 of transition
matrix A(G) can be expressed as,
∣θ(r))(u,u0) — ~(G)T(Rl~ + ~0)∣ ≤ ClJ-以2-J ≤ Cal
where α > λ2, and J > 1 is the size of the largest Jordan block of A(G). From the above inequality,
we can conclude that the second largest eigenvalue almost governs the convergence rate. However,
to maintain rigor, we cannot directly obtain a < a. Instead, from the result that λ2 > λ2, we say
that a is roughly larger than a.
D.2 Residual connection in MLP
Theorem 3 (Convergence rate of GNTK with residual connection between transformations). Con-
sider a GNTK of the form (8) and (12). If A(G) is irreducible and aperiodic, with a stationary
distribution π(G), then there exist constants 0 < a < 1 and C > 0, and constant vectors v and v0
depending on R, such that, ∣θ(7)(u, u0) 一 ~(G)t ∙ (Rl(1 + σ2w)Rl~ + ~0) | ≤ Cal.
Proof. For residual connection in MLP, we have,
22
qr (U) = qr-l(u) + -wq qr-l(u) = (1 + 十)qr-l(u)
Since σw2 > 0, qr(u) grows at an exponential rate.
Now, we turn to compute the correlation term Cr (u, u0). For convenience, we suppose qr (u)
qr(u0). Then,
0	Σr+1 (u, u )	1	Σr (u, u )
Cr+1(u,u ) =	-- - =	^2 U +
qr+1(U)	1 + σww qr(U)
竽
1	σ2
f(Cr(u,u0))
1 +等2
-ɪ Cr (u, U ) +
1 + σ2w
f(Cr(u,u0))
1 +等
Using Taylor expansion of f near 1, as have been done in the proof of Lemma 2,
f (x)∣x→1- = X +孕(1 一 x)3∕2 + O((1 — x)5∕2)
3π
we have,
2
2、/2 σW
Cr + l(u,U0) = Cr(u,U0) + 妥	,[(1 一 Cr(u,U0)3∕2 + 0((1 一 Cr (u,U0)5/2))]
3π 1 + -2w
22
Published as a conference paper at ICLR 2022
Note that it is similar to the case of MLP without residual connection:
Cr+l(u,U0) = Cr(u,U0) + 等 [(1 - Cr (u,U0)3/2 + O((1 - Cr(U */))]
Following the proof diagram in Lemma 3 and Theorem 1, we can obtain the behavior of Σ((lr)) (u, u0)
and Θ((lr)) (u, u0) in the large depth limit,
2
∣∑(r))(u,u0) - ~(G)T((1 + -w)Rl~)∣ ≤ Cal
2
∣θ(r))(u,u0) - ~(G)T(Rl(1 + 芋)Rl~ + ~0)∣ ≤ Cal
□
E APPENDIX: Discussion on the trainability of GNTK with
Crtical DropEdge
According to the critical percolation theory, there exists a large connected component of order O(n)
(Erdos & Renyi, 1961; Erdos & Renyi, 2011), where n is the number of nodes in a graph. This
implies that there exists a (connected) graph of size O(n) when we use the critical dropping rate.
To preserve information, edges would be resampled at every iteration. This implies that at each
iteration during training, the GNN takes a random and large graph. For the whole training process,
we can think that we have used all the information conveyed by the graph.
It would be interesting to consider deriving some theoretical results for the limiting behavior of the
GNTK in the large depth with Critical DropEdge. Since Critical DropEdge is originated from the
random graph theory, a promising approach is to analyze spectral distributions of adjacency matrix
and Laplacian matrix with a random graph model, like existing studies (Ding & Jiang, 2010; Chung
& Radcliffe, 2011).
F APPENDIX: Datasets and Implementation Details
F.1 Datasets
For node classification tasks, Cora, Citeseer and Pubmed (Kipf & Welling, 2017)) are three com-
monly used citation networks, and Physics (Shchur et al., 2018) is a co-author network. Detailed
information of the four benchmark datasets is listed as follows and summarized in Table 3.
• The Cora dataset consists of 2,708 scientific publications classified into one of seven
classes, and 5,429 links. Each publication is described by a 0/1-valued word vector indi-
cating the absence/presence of the corresponding word from the dictionary. The dictionary
consists of 1,433 unique words.
• The Citeseer dataset consists of 3,312 scientific publications classified into one of six
classes, and 4,732 links. Each publication is described by a 0/1-valued word vector indi-
cating the absence/presence of the corresponding word from the dictionary. The dictionary
consists of 3,703 unique words.
• The Pubmed Diabetes dataset consists of 19,717 scientific publications from PubMed
database pertaining to diabetes classified into one of three classes. The citation network
consists of 44,338 links. Each publication is described by a TF-IDF weighted word vector
from a dictionary comprised of 500 unique words.
• The Physics dataset consists of 34,493 authors as nodes, and edges indicate whether two
authors have co-authored a paper. Node features are paper keywords from the author’s
papers. Following Kim & Oh (2020), we reduce the original dimension (6,805 and 8,415)
to 500 using PCA. The split is the 20-per-class/30-per-class/rest. The goal of this task is to
classify each author’s respective field of study.
23
Published as a conference paper at ICLR 2022
Table 3: Details of Datasets
Dataset	Nodes	Edges	Classes	Features	Critical Edge Sampling	Train/Val/Test
Cora	2,708	5,429	7	1,433	24.94%	0.05/0.18/0.37
Citeseer	3,327	4,732	6	3,703	35.15%	0.04/0.15/0.30
Pubmed	19,717	44,338	3	500	22.23%	0.003/0.03/0.05
Physics	34,493	247,962	5	500	6.96 %	0.003/0.004/0.99
Table 4: Comparison results of test accuracy (%) with different infinite-wide backbones.								
Dataset Backbone	4-layer	8-layer	32-layer	64-layer Orignal C-DropEdge Original C-DropEdge Original C-DropEdge Original C-DropEdge							
GCN Cora JKNet	79.98	79.98	79.48	80.61	73.75	75.86	72.59	74.03
	78.27	77.29	80.26	80.02	78.00	79.01	75.90	76.75
GCN Citeseer JKNet	66.38	66.97	54.42	64.88	50.72	51.22	43.77	46.49
	67.04	67.67	65.54	66.45	54.57	56.39	46.71	47.96
GCN Pubmed JKNet	74.59	74.77	73.58	76.67	71.63	74.77	66.28	71.72
	75.30	74.35	76.42	77.17	75.71	76.68	74.81	75.74
F.2 Experimental Implementation
We use the PyTorch implementation to simulate both infinitely-wide and finitely-wide GCNs:
•	The infinitely-wide GCNs use part of code from Du et al. (2019b), which is originally
adopted for graph classification. We redesign the calculation method of GNTK (Du et al.,
2019b)2 according to the formula in Section 3.1 and use it to process node classification
tasks.
•	For finitely-wide GNNs with DropEdge (Rong et al., 2019)3, we use the code from (Rong
et al., 2019), we perform random hyper-parameter search for each model, and report the
case giving the best accuracy on validation set of each benchmark, following the same
strategy as Rong et al. (2019). The difference is that, in each experiment, we fix the edge
sampling percentage as P = 2∣vE1∣ , which listed in the last column of Table 3.
All codes mentioned above use the MIT license. All experiments are conducted on two Nvidia
Quadro RTX 6000 GPUs.
G APPENDIX: Additional Experiments and Discussion
G. 1 Infinitely-Wide Backbones
In the main paper, we reported the performance of GCNs with finite-width, here we show more
results with respect to infinitely-wide GCNs, as shown in Table 4. We apply Critical DropEdge
to infinitely-wide backbones on semi-supervised node classification. We consider two backbones,
which are GCN (Kipf & Welling, 2017) and JKNet (Xu et al., 2018), and the edge preserving per-
centage P(G) is shown in Table 3. First, we calculate a GNTK with respect to the graph data and
GNN. Then, we apply kernel regression with calculated GNTK to complete node classification task.
Table 4 summaries the performance on three citation networks. In this table, we report the perfor-
mance of GCNs with 4/8/32/64 layers. It is shown that, in most cases, using Critical DropEdge
(C-DropEdge) can achieve better results than original GNNs in the infinitely-wide case, especially
when the model is deep.
2We use the implementation of GNTK available at https://github.com/KangchengHou/gntk.
3We use the DropEdgeimplementation available at https://github.com/DropEdge/DropEdge.
24
Published as a conference paper at ICLR 2022
Table 5: Comparison results of test accuracy (%) between C-DroPEdge and DropEdge.
Dataset	Backbone Finite	Original	4-layer DropEdge	C-DropEdge	Original	8-layer DropEdge	C-DropEdge
	GCN	79.8 ± 1.1	80.4 ± 1.4	82.0 ± 1.2	73.2 ± 2.7	75.1 ± 2.4	77.3 ± 2.5
Cora	JKNet	81.1 ± 1.0	82.2 ± 0.7	82.5 ± 0.7	80.9 ± 1.2	82.0 ± 0.9	82.1 ± 0.5
	IncepGCN	80.0 ± 0.9	80.6 ± 1.2	82.4 ± 0.5	78.6 ± 1.7	81.2 ± 1.3	82.3 ± 0.6
	GCN	61.2 ± 3.0	63.7 ± 2.5	69.0 ± 0.8	50.2 ± 5.7	52.8 ± 5.1	54.1 ± 5.9
Citeseer	JKNet	69.6 ± 1.2	70.2 ± 1.0	70.3 ± 0.7	70.7 ± 1.0	70.5 ± 1.1	70.8 ± 1.2
	IncepGCN	69.4 ± 1.5	70.0 ± 1.0	70.8 ± 0.6	69.0 ± 1.2	70.8 ± 1.1	70.9 ± 0.5
	GCN	77.4 ± 0.7	77.6 ± 1.4	78.0 ± 0.4	57.2 ± 8.4	57.5 ± 6.1	58.9 ± 6.9
Pubmed	JKNet	76.5 ± 0.9	77.1 ± 1.1	77.4 ± 1.0	76.1 ± 1.4	76.6 ± 1.0	76.6 ± 0.9
	IncepGCN	76.7 ± 1.7	77.4 ± 1.5	77.6 ± 1.1	77.5 ± 1.3	77.3 ± 1.2	77.9 ± 1.0
	GCN	90.2 ± 0.9	91.6 ± 0.8	91.9 ± 0.7	83.5 ± 2.2	86.3 ± 1.8	85.2 ± 2.3
Physics	JKNet	90.6 ± 1.7	91.0 ± 0.9	91.5 ± 0.4	90.4 ± 1.5	91.5 ± 0.7	91.7 ± 0.6
	IncepGCN	91.0 ± 1.1	91.4 ± 0.5	91.5 ± 0.3	90.5 ± 0.8	91.4 ± 0.8	91.5 ± 0.6
Dataset	Backbone Finite	Original	16-layer DropEdge	C-DropEdge	Original	32-layer DropEdge	C-DropEdge
	GCN	36.3 ± 13.8	55.1 ± 5.2	58.5 ± 3.9	20.1 ± 2.4	22.1 ± 2.0	24.7 ± 1.8
Cora	JKNet	79.9 ± 1.6	82.2 ± 0.7	82.4 ± 0.8	80.4 ± 1.4	82.1 ± 0.5	82.6 ± 0.9
	IncepGCN	78.7 ± 1.0	80.2 ± 1.3	82.0 ± 0.8	78.5 ± 1.8	80.9 ± 0.8	81.6 ± 0.9
	GCN	30.8 ± 2.2	35.7 ± 1.9	36.6 ± 2.0	21.7 ± 3.0	23.1 ± 1.0	25.2 ± 0.9
Citeseer	JKNet	69.2 ± 1.2	68.8 ± 1.6	69.5 ± 1.0	68.1 ± 2.3	69.9 ± 1.4	70.1 ± 0.6
	IncepGCN	68.4 ± 1.2	70.7 ± 1.0	71.0 ± 1.0	68.6 ± 1.9	70.2 ± 0.8	70.7 ± 0.9
	GCN	39.5 ± 10.3	37.0 ± 9.6	42.6 ± 6.4	36.3 ± 8.4	37.4 ± 8.8	38.5 ± 6.1
Pubmed	JKNet	76.6 ± 0.9	76.1 ± 0.8	76.9 ± 0.9	77.1 ± 0.8	77.0 ± 0.9	77.2 ± 1.0
	IncepGCN	76.5 ± 1.5	76.7 ± 1.3	77.2 ± 1.0	77.0 ± 1.4	77.2 ± 1.3	77.8 ± 1.0
	GCN	41.6 ± 6.2	45.8 ± 6.3	46.2 ± 5.7	28.8 ± 9.4	31.1 ± 8.8	36.2 ± 8.4
Physics	JKNet	90.3 ± 1.0	91.2 ± 0.5	91.5 ± 0.4	90.6 ± 1.0	91.3 ± 0.8	91.6 ± 0.6
	IncepGCN	91.4 ± 0.4	92.0 ± 0.5	92.0 ± 0.4	OOM	OOM	OOM
OOM means Out-Of-Memory.
G.2 Performance of Using Different Backbones
The results presented in Table 1 are the best across different backbones. Table 5 further compares
the results of vanilla GNN, DropEdge and C-DropEdge for three different backbones (GCN, JKNet
and IncepGCN). From the table we can conclude that Critical DropEdge consistently outperforms
DropEdge and vanilla GNNs.
G.3 Trainability of GNTK with Critical DropEdge
We implement deep GCNs with/without DropEdge and C-DropEdge to compare the training per-
formance. The results are shown in Figure 4. The training loss of GCN and GCN with DropEdge
has a slower convergence rate and converges to a larger error compared to GCN with C-DropEdge.
This indicates that C-DropEdge can achieve better trainability compared to GCN and GCN with
DropEdge. Besides, the convergence results of GNTKs with C-DropEdge are shown in Figure 5.
Compared to residual connection, we find that the normalized GNTK elements would not converge
to a single value, and the convergence rate curve does not depend on the depth. This implies that
residual connection mitigates the trainability loss by slowing down the exponential convergence rate,
while C-DropEdge directly changes the convergence results.
25
Published as a conference paper at ICLR 2022
SSOlωu-u's,J
Epochs
Figure 4: Training loss as a function of epochs. (a) We implement 8-layer GCN, GCN with DropE-
dge, GCN with C-DropEdge on Cora. (b) 12-layer GCN, GCN with DropEdge, GCN with C-
DropEdge on Citeseer. The training loss of GCN and GCN with DropEdge has a slower convergence
rate and converges to a larger error compared to GCN with C-DropEdge.
0.0	1	-----------1----------1
0	100	200	300	400
Epochs
(a) C-DroPEdge GNTK Collapse
0	200	400	600	800
Depth
Figure 5: Convergence rate of GNTK with C-DropEdge. (a) Elements of the normalized (residual
connection) GNTK as a function of the depth, defined as Rl + r. Different elements tend to have
different value as depth grows (b) The element-wide distance of the normalized GNTK as a function
of the depth. The converge rate is no longer bounded by an exponential function. Instead, it remains
parallel to the horizontal depth axis.
ylNU JO əɔue-ɪ-ɪsɑəs 一 MjUəluəuj

(b) C-DropEdge Convergence Rate
2
-
10
0	200	400	600	800
Depth
26