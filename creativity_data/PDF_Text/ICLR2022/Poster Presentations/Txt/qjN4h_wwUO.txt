Published as a conference paper at ICLR 2022
GradMax: Growing Neural Networks using
Gradient Information
Utku Evci, Bart van Merrienboer, Thomas Unterthiner,
Max Vladymyrov, Fabian Pedregosa
Google Research, Brain Team
{evcu,bartvm,unterthiner,mxv,pedregosa}@google.com
Ab stract
The architecture and the parameters of neural networks are often optimized in-
dependently, which requires costly retraining of the parameters whenever the ar-
chitecture is modified. In this work we instead focus on growing the architecture
without requiring costly retraining. We present a method that adds new neurons
during training without impacting what is already learned, while improving the
training dynamics. We achieve the latter by maximizing the gradients of the new
weights and efficiently find the optimal initialization by means of the singular
value decomposition (SVD). We call this technique Gradient Maximizing Growth
(GradMax) and demonstrate its effectiveness in variety of vision tasks and archi-
tectures1.
1	Introduction
The architecture of deep learning models influences a model’s inductive biases and has been
shown to have a crucial effect on both the training speed and generalization (dAscoli et al., 2019;
Neyshabur, 2020). Searching for the best architecture for a given task is an active research area
with diverse approaches, including neural architecture search (NAS) (Elsken et al., 2019), pruning
(Liu et al., 2018), and evolutionary algorithms (Stanley & Miikkulainen, 2002). Most of these ap-
proaches are costly, as they require large search spaces or large architectures to start with. In this
work we consider an alternative approach: Can we start with a small network and learn an efficient
architecture without ever needing a large network or exceeding the size of our final architecture?
The idea of incrementally increasing the size of a model has been used in many settings such as
boosting (Friedman, 2001), continual learning (Rusu et al., 2016), architecture search (Elsken et al.,
2017; Cortes et al., 2017), optimization (Fukumizu & Amari, 2000; Caccia et al., 2022), and rein-
forcement learning (Berner et al., 2019). Despite the wide range of applications of growing neural
networks, the initialization of newly grown neurons is rarely studied. Existing work on growing neu-
ral networks either adds new neurons randomly (Chen et al., 2016; Berner et al., 2019) or chooses
them with the aim of decreasing the training loss (Bengio et al., 2006; Liu et al., 2019; Wu et al.,
2020a). In this work we take a different approach. Instead of improving the training objective im-
mediately through growing, we focus on improving the subsequent training dynamics. As we will
show, this has a longer-lasting effect than the greedy approach of improving loss during growing.
Our main contribution is a new growing method that maximizes the gradient norm of newly added
neurons, which we call GradMax. We show that this gradient maximization problem can be solved
in a closed form using the singular value decomposition (SVD) and initializing the new neurons
using the top k singular vectors. We show that the increased gradient norm persists during future
steps, which in return yields faster training.
2	Growing neural networks
Neural network learning is often formalized as the following optimization problem:
argminE(x,y)〜D [L(f(x), y)] ,	(1)
f∈S
1We open source our code at https://github.com/google-research/growneuron.
1
Published as a conference paper at ICLR 2022
where S is a set of neural networks, L is a loss function and D is a data set consisting of inputs x and
outputs y). Most often the set S is constrained to a single architecture (e.g., ResNet-18 (Zagoruyko
& Komodakis, 2016)) and only the parameters of the model are learned. However, hand-crafted
architectures are often suboptimal and various approaches aim to optimize the architecture itself
together with its parameters.
Methods such as random search (Bergstra & Bengio, 2012), NAS, and pruning search a larger space
that contains a variety of architectures. Growing neural networks has the same goal: It learns the
architecture and weights jointly. It aims to achieve this by incrementally increasing the size of the
model. Since this approach never requires training a large model from scratch it needs less memory
and compute than pruning methods (Yuan et al., 2021). Reducing the cost of finding efficient archi-
tectures is important given the ever-growing size of architectures and the accompanying increase in
environmental footprint (Thompson et al., 2021).
Growing neural networks: When, where and how? Algorithms for growing neural networks
start training with a smaller seed architecture. Then over the course of the training new neurons
are added to the seed architecture, either increasing the width of the existing layers or creating new
layers. Algorithms for growing neural networks should address the following questions:
1.	When to add new neurons? For instance, some methods (Liu et al., 2019; Kilcher et al.,
2019) require the training loss to plateau before growing, whereas others grow using a
predefined schedule.
2.	Where to add new capacity? We can add new neurons to the existing layers or create new
layers among the existing ones.
3.	How to initialize the new capacity?
In this work we mainly focus on the question of how and introduce a new initialization method for
the new neurons. Our approach can also be used to guide when and where to grow new neurons.
However, in order to make our comparison with other initialization methods fair we keep the growing
schedule (where and when) fixed.
How to grow new neurons? Suppose that during training we would like to grow k new neurons
at layer ` as depicted in Figure 1. After the growth, new neurons are appended to the existing weight
matrices as follows:
W'+ =	Wew	⑵
W% = [ W'+ι Wn+w ].	⑶
A desirable property of growing algorithms is to preserve the information that the network has
learned. Therefore the initialization of new neurons should ensure our neural network has the same
outputs before and after growth.
We describe two main approaches to growing: splitting and adding. Splitting (Chen et al., 2016;
Liu et al., 2019) duplicates existing neurons and adjusts outgoing weights so that the output in the
next layer is unchanged. However, splitting has some limitations: (1) It creates neurons with the
same weights and small changes required to break symmetry; (2) It can’t be used for growing new
layers as it requires existing neurons to begin with.
Another line of work focuses on adding new neurons while ensuring that the output does not change
by setting either W'ιew or W'+w to zero. This approach doesn't have the limitations mentioned above
and often results in better performance (Wu et al., 2020a). Below we summarize two methods that
use this approach:
Random Berner et al. (2019) initialize new neurons randomly when growing new layers.
Firefly Wu et al. (2020a) combine splitting and adding random neurons. Random neurons are
trained for a few steps to reduce loss directly and the most promising neurons are selected
for growing.
Previous work focused either on solely keeping the output of the neural network the same (Random)
or on immediately reducing the training objective (Firefly). In this work we take a different approach
2
Published as a conference paper at ICLR 2022
Figure 1: Schematic view of the GradMax algorithm. Growing new neurons requires initializing
incoming (W'ιew) and outgoing W'+eW weights for the new neuron. GradMax sets incoming weights
to zero (dashed lines) in order to keep the output unchanged, and initializes outgoing weights us-
ing SVD (equation 11). This maximizes the gradients on the incoming weights with the aim of
accelerating training.
and focus on improving the training dynamics. We initialize the weights so that the network’s output
remains unchanged while trying to improve the training dynamics by maximizing the gradient norm.
The hypothesis is that this accelerates training and leads to a better model in the long run.
3	GradMax
This section describes our main contribution: GradMax, a method that maximizes the gradient norm
of the new weights. We constrain the norm of the new weights to avoid the trivial solution where
the weight norm tends to infinity. We also require that the network output is unchanged when the
neuron is added. Note that, due the latter, gradients of the existing weights are unchanged during
growth and therefore we maximize the gradients on the new weights introduced.
arg max
W new W new
W'	,w'+1
ED
-∂L
∂Wnew
JkWnewkF ,∣∣Wn+W∣∣F ≤ C
Iwn+whnew = 0
(4)
2
+
F
J ∂L ] 2
D⅛+d F
Motivation: Large gradients lead to large objective decrease Con-
sider running gradient descent on a function that is differentiable with a
β-Lipschitz gradient. A classical result for this class of functions is that
the decrease in objective function after one step of gradient descent with
step-size 1∕β is upper bounded by L(W') - 2∣∣VL(W')k2 (Nesterov,
2003). This upper-bound decreases as the norm of the gradient increases.
This observation implies that the larger the gradient, the larger the ex-
pected decrease (assuming a constant Lipschitz constant). Hence, in-
terleaving gradient maximizing steps within training will lead to an im-
proved decrease in later iterations. We use this observation in the context
of growing neural networks and propose a method that interleaves the
normal training process with steps that maximize the gradient norm by
adding new units, which we now describe in detail.
GradMax The general maximization problem in eq. (4) is non-trivial to solve. However, we show
that with some simplifying assumptions we can find an approximate solution using a singular value
decomposition (SVD). Here we derive this solution using fully connected layers and share derivation
for convolutional layers at Appendix B.
Consider 3 fully connected layers denoted with indices ` - 1, ` and `+ 1 and the following recursive
definition:
z` = w`h`-i	(5)
h` = f (z`),	(6)
where subscripts denote layer-indices. Let M' denote the number of units in layer ', and f the
activation function. The vectors z`, h` ∈ Rm' are pre-activations and activations respectively, with
ho = X being an input sampled from the dataset D. The entries of W' ∈ Rm'-1×m' are the
parameters of the layer.
When growing k neurons the weight matrices W' and W'+1 are replaced by W'+ and W'++1 as
defined in eqs. (2) and (3). We denote the pre-activations and activations of the new neurons with
3
Published as a conference paper at ICLR 2022
z`new and h`new. The gradients of the new weights can be derived using the chain rule:
∂⅛ =(f0(ZneW) θ W'n+w,> 令)h>-ι
∂L = _8L_ hnew,>
∂W'+w = ∂z'+ι '	.
(7)
(8)
The simplifying assumptions that We will now make are that Wfew = 0 and that f (0) = 0 with
gradient f0(0) = 1. Note that this guarantees that WneWhneW = 0, independent of the training data.
Moreover, it simplifies the gradients to
	∂⅛ = WeW,> 备 h3	(9) ∂L ∂Wf+W = ,	(0)
which reduces our problem to
aWmaxM+'F [∂ZL1 h"[F，s∙t∙ MTIUf ≤ C	(II)
The solution to this maximization problem is found in a closed-form by setting the columns of Wf1e1
as the top k left-singular vectors of the matrix ED [∂∂L?h>-i∖ and scaling them by
c
k(σ1,...,σk)k
(where σi is the i-th largest singular value). In order to make a fair comparison between different
methods we scale each initialization such that their norm is equal to the same value, i.e., mean norm
of the existing neurons (similar to Liu et al. (2017)).
Note that it is feasible to also use the singular values to guide where and when to grow, since the
singular values are equal to the value of the maximized optimization problem above. For example,
neurons could be added when the singular values meet a certain threshold, and layers to grow could
be chosen depending on which have the largest singular values. We leave this for future work.
Non-linearities and normalization Note that our assumptions that f(0) = 0 and f0(0) = 1 apply
to common activation functions such as rectified linear units (if zero is included in the linear part)
and the hyperbolic tangent. Other functions could be adapted to fit this definition as well (e.g., by
shifting the output of the logistic function by 一 1).
In fact our derivation holds for any a such that f0(0) = a, a 6= 0. This means that batch nor-
malization can also be used, since it meets these assumptions. However, care must be taken with
activation functions (or their derivatives) that are unstable around zero. For example, batch normal-
ization scales the gradients by ε when all inputs are zero, which can lead to largejumps in parameter
space (Lewkowycz et al., 2020, and Appendix C).
GradMax-Optimized (GradMaxOpt) One can also directly optimize eq. (11) using an iterative
method such as projected gradient descent. Our experiments show that this approach generally does
not work as well as using the SVD, highlighting the benefit of having a closed-form solution.
However, if the outgoing weights are set to zero (Wf1e1 = 0) instead of the incoming weights then
the solution can no longer be found using SVD and direct optimization of eq. (4) could provide a so-
lution. This could be preferable in some situations since it removes the constraints on the activation
function. Moreover, it can avoid the unstable behaviour of functions such as batch normalization.
Full gradient estimation Solving eq. (11) requires calculating the gradient ∂∂∂L~v h>-ι over the
full dataset. This can be expensive so in practical applications we propose using a large minibatch
instead. If the minibatch gradient is sufficiently close to the full gradient then the top singular vectors
are likely to be close as well (Stewart, 1998). We validate this experimentally in Section 4.2.
4	Experiments
We evaluate gradient maximizing growing (GradMax) using small multilayer perceptrons (MLPs)
and popular deep convolutional architectures. In Section 4.1 we focus on verifying the effectiveness
4
Published as a conference paper at ICLR 2022
堂IoT
Random
GradMax
GradMaxOpt
EJoN JU①-PeJ D pffismp<
6×10-1-
4×1O-1-
3×10-1-
2×10^1∙
S
g 0.03
< 0.02
1	2	3	4	5
Growth Number
0.00
O 250	500	750 1000 1250 1500	O
TrainingSteps
0.05
0.04
0.01
100	200	300	400	500
Step Number
(c) After N Steps
(a)	After Growth
(b)	During Training
Figure 2: (a) We measure the norm of the gradients with respect to Wnew after growing a single neu-
ron starting from checkpoints generated during the Random (Random) growth experiments. Since
Random and GradMaxOpt are stochastic we repeat those experiments 10 times. (b) Gradient norm
of the flattened parameters (both layers) throughout training. (c) Similar to (a), we load checkpoints
from each growth step (Growth 1-5) and grow a new neuron using GradMax (fg) and Random (fr).
Then we continue training for 500 steps and plot the difference in training loss (i.e., L(fr) - L(fg)).
The confidence intervals are defined over 5 repetition of the experiment described above.
of GradMax and in Section 4.2 we evaluate GradMax on common image classification benchmarks
and show the effect of different hyper-parameter choices.
We implement GradMax using Tensorflow (Abadi et al., 2015) and modify the implementation of the
standard ReLU activation to output sub-gradient 1 at 0. In Appendix A we share the implementation
details of GradMax and show how it can be used for growing new layers. We will open-source our
implementation upon publication.
4.1	Teacher-Student experiments with MLPs
We have 3 main goals in this section. First, we empirically verify that the gradient norm is sig-
nificantly increased using our method. Then, we verify our hypothesis that increasing the gradient
norm at a given step improves the training dynamics beyond the growing step. Finally, we gather
experimental evidence that networks grown with GradMax can achieve better training loss and gen-
eralization compared to baselines and other methods in a teacher-student setting.
Teacher-Student task We use a teacher-student task (Lawrence et al., 1997) to compare different
growing algorithms. In this setting a student network must learn the function of a given teacher
network. Our teacher network ft consists ofmi input nodes, mh hidden nodes and mo output nodes
(denoted with mi : mh : mo). We initialize weights of ft randomly from the range [-1, 1] and then
sample N = 1000 training points Dx from a Gaussian distribution with zero mean and unit variance
and pass it through ft to obtain target values Dy . With this training data we train various student
networks (fs) minimizing the squared loss between fs (Dx) and Dy .
A key property of this teacher-student setting is that when the student network has the same archi-
tecture as the teacher network, the optimization problem has multiple global minima with 0 training
loss. Here we highlight results using fully connected layers alone. In Appendix D we show addi-
tional validation experiments as well as repeat experiments from this section with convolutions and
batch normalization.
Verifying GradMax In our initial experiments we use a teacher network of size 20 : 10 : 10. All
growing student networks begin with a smaller hidden layer of 20 : 5 : 10. We grow the hidden
layer to match the teacher architecture in 5 growth steps performed every 200 training steps. We also
train two baseline networks with sizes 20 : 10 : 10 (Baseline-Big) and 20 : 5 : 10 (Baseline-Small)
from scratch. After the final growth we perform 500 more steps resulting in 1500 training steps in
total. We also run the version of GradMax in which we optimize eq. (11) directly using gradient
descent (GradMaxOpt). We start with a random initialization and use the Adam optimizer. The
weight matrix is scaled to the target norm c after each gradient step.
In Figure 2a we show that in this setup GradMax is able to initialize the new neurons with a sig-
nificantly higher gradient norm compared to random growing. The results for GradMaxOpt show
that naive direct optimization of the gradient norm does not recover the solution found by GradMax
5
Published as a conference paper at ICLR 2022
Iol
SSOl Mu-U-」l
(a) Small (K = 1, ft = 20 : 10 : 10)	(b) Large (K = 5, ft = 100 : 50 : 10)	(c) Run time
Figure 3: Training curves averaged over 5 runs and provided with 80% confidence intervals. In both
settings GradMax improves optimization over Random. (right) Run time of the growing algorithms.
using SVD. This highlights the benefit of having formulated a problem that can be directly solved.
In Figure 2b we plot the total adjusted gradient norm2 and observe that the larger gradient norm
after growing persists for future training steps. Finally, in Figure 2c we plot the difference in train-
ing loss when GradMax is used for growing compared to randomly growing. We observe consistent
improvements in the training dynamics which last for over 500 training steps. This supports our
hypothesis that increasing the gradient norm accelerates training and leads to a greater decrease in
the training loss for subsequent steps.
Training Curves We plot the training curves for two different teacher networks in Figure 3. For
Figure 3a we trained a teacher network of 20 : 10 : 10 whereas Figure 3b uses a larger teacher
network of size 100 : 50 : 10. For the large teacher network setting we begin with student networks
of size 100 : 25 : 10 that are grown every 500 steps (first growth on step 500), adding 5 neurons at
a time resulting for 5 growth steps. We train the grown networks for an additional 1000 iterations
resulting in 3500 iterations in total. Further experimental details are shared in Appendix D.
In these experiments the initial student models require about half the number of FLOPS required by
the teacher models. Therefore training and growing student models with a linear growing schedule
costs only 75% of the FLOPS required to train Baseline-Big. The extra operations required by Grad-
Max run faster than a single training step and therefore their cost is negligible as shown in fig. 3c. In
all cases GradMax achieves lower training loss compared to the random and GradMax-Optimized
(GradMaxOpt) methods. For the larger teacher network (Figure 3b), GradMax even matches the
performance of a network trained from scratch (Big-Baseline). However for small teacher networks
(Figure 3a), all growing methods fall short of matching the Baseline-Big performance similar to
Berner et al. (2019); Ash & Adams (2020), in which the authors show that training the final network
from scratch works better than warm-starting/growing an existing network. GradMax narrows down
this important gap by maximizing gradients.
4.2	Image Classification Experiments
In this section we benchmark GradMax using various growing schedules and architectures on
CIFAR-10, CIFAR-100, and ImageNet. GradMax can easily be applied to convolutional networks
(see Appendix B for implementation details). Similar to our results in the previous section, we
share baselines where we train the seed architecture (Small-Baseline) and the target architecture
(Big-Baseline) from scratch without growing. As an additional baseline we implement a simpler
version of Firefly which initializes new neurons by minimizing the training loss directly (without
the extra candidates used by the original method). We refer to this baseline as Firefly-Optimized
(Firefly-Opt). Firefly requires non-zero outgoing weights in order to optimize the loss. There-
fore We initialize the outgoing weights for all methods to small random values (k W'+Wk = ε for
ε = 10-4). We use a batch size of 512 for ImageNet (128 for CIFAR) experiments and calculate the
SVD using the same batch size used in training. At every groWth iteration, We add neW neurons to all
layers With reduced initial Width, proportional to their target Widths. Here our goal is not to obtain
state of art results, but to assess hoW the initialization of neW neurons affects the final performance.
All netWorks are trained for the same total number of steps, and thus Baseline-Small and the groWn
2We divide the gradients by the training loss to factor out the linear scaling effect of the training loss on the
gradient norms.
6
Published as a conference paper at ICLR 2022
10°：
1 2
0T
1 1
SSol1
O 20000	40000	60000	80000
Training Steps
3×105
SSol-≡u-e-IJ.
IUno,JEE-J Ed
2 1
1Oo-
O 20000	40000	60000	80000
Training Steps
W7
IUnou」8slue」ed
0
(a) Wide-Resnet-28-1 / CIFAR-10
4×105
00
1
SSol WC-C-EH
3×105
105
O 20000	40000	60000	80000
TrainingSteps
SS9M-U=1
JUnoυ,JsEed
⅛3
1
×
2
4×W0
3×W0
2×1Oo
6×1Oo∙
(b) VGG-11 / CIFAR-10
3×W6
2×1O6
O 50000 WOOOO 150000 200000
Training Steps
rnou .lω≡E≡ed
(c) Wide-Resnet-28-1 / CIFAR-100	(d) MobileNet-v1 / ImageNet
Figure 4: We plot training loss over time, each experiment is averaged over 3 experiments. Number
of parameters of the network trained increases over time. Red lines indicate number of parameters
over training.
Dataset	Architecture	Baseline-S	Baseline-B	Random	Firefly	Gradmax
CIFAR-10	WRN-28-1	89.9±0.3	92.9±0.2	90.6±0.2	90.8±0.3	91.1±0.1
	VGG11	84.1±0.1	86.6±0.3	83.8±0.6	84.0±0.2	84.4±0.4
CIFAR-100	WRN-28-1	63.7±0.0	69.3±0.1-	66.7±0.4	66.5±0.1	66.8±0.2
ImageNet	Mobilenet-VI	55.0±0.0	70.8±0.0	66.9±0.3	66.4±0.1	68.6±0.2
Table 1: Test accuracy of different baselines and growing methods on different tasks. All results are
averaged over 3 random seeds (used for training).
models require less FLOPS than Baseline-Big to train. We share our results in Table 1 and observe
consistent improvements when GradMax is used.
CIFAR-10/100 We perform experiments on CIFAR-10 and CIFAR-100 using two different ar-
chitectures: VGG-11 (Simonyan & Zisserman, 2015) and WRN-28-1 (Zagoruyko & Komodakis,
2016). For both architectures we reduce number of neurons in each layer by 1/4 to obtain the
seed architecture (width multiplier=0.25). For the ResNet architecture we only shrink the first con-
volutional layer at every block to prevent mismatch when adding skip-connections. We train the
networks for 200 epochs using SGD with a momentum of 0.9 and decay the learning rate using
a cosine schedule. In both experiments we use a batch size of 128 and perform a small learning
rate sweep to find learning rates that give best test accuracy for baseline training. We find 0.1 for
Wide-ResNet and 0.05 for VGG to perform best and use the same learning rates for all different
methods. In Figures 4a to 4c we plot the training loss for different growing strategies and share the
test accuracy in Table 1.
ImageNet We grow MobileNet-v1 architectures on ImageNet using a seed architecture of width
0.25 (i.e. all layers have one forth of the channels). The MobileNet-v1 architecture contains depth-
wise convolutions between each convolutional layers. When growing convolutional layers, we ini-
tialize the depth-wise convolution in between to identity. Training loss for this settting is shared in
Figure 4d.
7
Published as a conference paper at ICLR 2022
BN	Inverse	Baseline-S	Baseline-B	Random	Firefly	Gradmax(-OPt)
~~Γ~	-X-	89.9±0.3	92.9±0.2	90.6±0.2	90.8±0.3	91.1±0.1
X	X			92.1±0.2	92.2±0.2	92.4±0.1
X	-X-	90.2±0.3	93.4±0.1	92.9±0.1	92.9±0.1	93.0±0.1
X	X			92.8±0.1	92.8±0.2	92.9±0.2
Table 2: Average test accuracy when growing WRN-28 on CIFAR-10 with batch normalization and
outgoing weights set to zero. BN refers to batch normalization and inverse indicates that we set the
outgoing weights to zero. When the outgoing weights are set to zero, we use GradmaxOpt.
Figure 5: We grow MobileNet-v1 networks during the ImageNet training to investigate the effect of
(left) growing schedule, (center) ε, and (right) scale used in initialization on the test accuracy.
Figure 6: The alignment of the top-k left-singular vectors for WRN-28-1 during the CIFAR-10
training. The alignment is calculated between the full gradient and minibatches of varying sizes. We
do not apply random cropping or flips to the inputs. A total of 10 experiments are run and confidence
intervals of 95% are plotted.
Batch Normalization Results and Inverse Formulation We perform our main set of experi-
ments using networks without batch norm and setting the incoming weights to zero. However, in
some cases using batch norm or setting outgoing weights to zero might be useful. Table 2 shows
the results for such alternatives when growing residual networks on CIFAR-10. For this particular
setting we observe that batch norm has limited effect on results, however we observe consistent im-
provements when outgoing weights are set to zero. We use GradmaxOpt in this setting and observe
improvements over both random and Firefly.
Hyperparameters In Figure 5 we show the effect of various hyperparameters on the performance
of GradMax when growing MobileNet-v1 on ImageNet. First on Figure 5-left we compare different
growing schedules. Growing neurons early in the training and growing every 5000 step achieves the
best results. Subfigure Figure 5-center shows the effect of ε used in our experiments. As expected,
larger values of ε (and therefore not preserving the output of the network during the growing) harm
the final test accuracy. Finally, Figure 5-right shows the effect of the scale used when initializing the
new neurons. We compare normalizing the scales using the mean norm of existing neurons (mean)
to using no normalization (fixed) and observe no visible different in performance. We observe
relatively robust performance for values larger than 0.5 and therefore simply set the scale to 0.5 in
all of our experiments.
8
Published as a conference paper at ICLR 2022
Effect of Minibatch Size To validate that we can approximate the SVD of the full gradient with
the SVD of a large minibatch we calculate both and plot the alignment of the left-singular vectors.
We define alignment as the absolute value of the Frobenius trace of the two matrices of singular
vectors, normalized with 1. The plots in Figure 6 show that at the beginning of training the Sin-
gular vectors strongly align. At the end of training the gradients are less correlated and hence the
alignment decreases. However, it is still a reasonable estimate.
5	Related Work
Cascade-Correlation (CasCor) learning architecture (Fahlman & Lebiere, 1990; Shultz & Rivest,
2000), to our knowledge, is the first algorithm that grows a neural networks during training. Cas-
Cor adds new layers that consists of single neurons that maximize the correlation between centered
activations and error signals at every growth step by resulting in a very deep but narrow final ar-
chitecture. Platt (1991) allocates new neurons whenever the per-sample error is high. Fukumizu
& Amari (2000) show that local minima can turned into saddle points by adding neurons. Chen
et al. (2016) proposed splitting neurons and Wei et al. (2016) presented a more general study of
performance-preserving architecture transformations. Elsken et al. (2017) apply these transforma-
tions to NAS. Wen et al. (2019) proposed growing neurons and discovered that it is best to grow
layers early on. Lu et al. (2018) also add new layers, but they sparsify the resulting network. Convex
neural networks frames the learning of one-hidden layer neural networks as an infinite-dimensional
convex problem (Bengio et al., 2006; Rosset et al., 2007; Bach, 2017) and then approximates the
solution by an incremental algorithm that adds a neuron at each step.
Kilcher et al. (2019) show that the scaling of the outbound weights can be optimized with respect to
gradient norm when the original network is at local minima. The solution is given by the closed form
using a matrix of outbound partial derivatives. Liu et al. (2019) propose an algorithm for splitting
neurons. For each neuron it computes an approximate reduction in loss if this weight were cloned
and adjusted by a given offset. The solution to this problem is given by the eigendecomposition of
a “splitting matrix”. The splitting is most effective when the network is very close to convergence.
Calculating the splitting matrix is expensive so in Wang et al. (2019) the authors propose a faster
approximation. Wu et al. (2020b) expand the set of splitting directions which helps avoid local
minima in architecture space. In the context of manifold learning, Vladymyrov (2019) also proposed
to grow the number of dimensions of the embedding manifold in order to escape local minima.
Similar to our approach, new parameters are found in a closed form.
Gradient based growth criteria is used in the context of growing sparse connections (Liu et al., 2017;
Dettmers & Zettlemoyer, 2019; Evci et al., 2020; ab Tessera et al., 2021; Evci et al., 2022) and when
initializing neural networks (Dauphin & Schoenholz, 2019). Most relevant to our work is NeST (Liu
et al., 2017) which aims to maximize gradients when growing sparse neurons by wiring neurons with
highest correlations. However their method is limited to growing a single neuron and modifies the
output of existing neurons due to non-zero initialization on both sides.
6	Discussion
Limitations GradMax requires activation functions to map zero to zero and have non-zero deriva-
tive at the origin. Some activation functions, such as radial basis functions, don’t satisfy these
requirements. We studied fully connected layers and convolutional layers, but did not consider the
combination of the two or other architectures such as transformers.
Future work We are looking to address these limitations in future work. Moreover, in this work
we didn’t address the questions as to when and where networks should be grown, although we
outlined ways in which our method could be adapted to do so. Our method could also be a useful
network morphism to be used as part of a more complex NAS method. These are further avenues
for exploration.
7	Conclusion
In this work we presented GradMax, a new approach for growing neural networks that aims to
maximize the gradient norm when growing. We verified our hypothesis and demonstrated that it
provides faster training in a variety of settings.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We like to thank members of the Google Brain team for their useful feedback. Specifically we
like to thank Mark Sandler, Andrey Zhmoginov, Elad Eban, Nicolas Le Roux, Laura Graesser and
Pierre-Antoine Manzagol for their feedback on the project and the preprint.
References
Kale ab Tessera, Sara Hooker, and Benjamin Rosman. Keep the gradients flowing: Using gradient
flow to study sparse network optimization. ArXiv, abs/2102.01670, 2021.
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorflow.org/.
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. arXiv preprint arXiv:1812.03981, 2018.
Jordan Ash and Ryan P Adams. On warm-starting neural network training. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 3884-3894. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
288cd2567953f06e460a33951f55daaf- Paper.pdf.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 2017.
Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte.
Convex neural networks. In Advances in Neural Information Processing Systems, vol-
ume 18, 2006. URL https://proceedings.neurips.cc/paper/2005/file/
0fc170ecbb8ff1afb2c6de48ea5343e7- Paper.pdf.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(2), 2012.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal Jozefowicz,
Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Ponde de Oliveira Pinto,
Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya
Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement
learning. ArXiv, 2019. URL http://arxiv.org/abs/1912.06680.
Lucas Caccia, Jing Xu, Myle Ott, MarcAurelio Ranzato, and Ludovic Denoyer. On anytime learning
at macroscale, 2022. URL https://openreview.net/forum?id=3GHHpYrYils.
Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge
transfer. In 4th International Conference on Learning Representations, 2016.
Corinna Cortes, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. AdaNet:
Adaptive structural learning of artificial neural networks. In Proceedings of the 34th International
Conference on Machine Learning, pp. 874-883, 2017.
Stephane dAscoli, Levent Sagun, Giulio Biroli, and Joan Bruna. Finding the needle in the haystack
with convolutions: on the benefits of architectural bias. In Advances in Neural Information Pro-
cessing Systems 32, 2019.
10
Published as a conference paper at ICLR 2022
Yann N Dauphin and Samuel Schoenholz. Metainit: Initializing learning by learning to initialize.
In Advances in Neural Information Processing Systems, 2019.
Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. ArXiv, 2019. URL http://arxiv.org/abs/1907.04840.
Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for
convolutional neural networks. arXiv preprint arXiv:1711.04528, 2017.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The
Journal of Machine Learning Research, 2019.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In Proceedings of Machine Learning and Systems 2020. 2020.
Utku Evci, Yani Andrew Ioannou, Cem Keskin, and Yann Dauphin. Gradient flow in sparse neural
networks and how lottery tickets win. In AAAI Conference on Artificial Intelligence, 2022.
Scott Fahlman and Christian Lebiere. The cascade-correlation learning architecture. In
D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-
Kaufmann, 1990. URL https://proceedings.neurips.cc/paper/1989/file/
69adc1e107f7f7d035d7baf04342e1ca- Paper.pdf.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of
statistics ,pp.1189-1232, 2001.
Kenji Fukumizu and Shun-ichi Amari. Local minima and plateaus in hierarchical structures of
multilayer perceptrons. Neural networks, 2000.
Yannic Kilcher, Gary BecigneUL and Thomas Hofmann. Escaping flat areas via function-preserving
structural network modifications, 2019. URL https://openreview.net/forum?id=
H1eadi0cFQ.
Steve Lawrence, C. Giles, and Ah Tsoi. Lessons in neural network training: Overfitting may be
harder than expected. In Proceedings of the National Conference on Artificial Intelligence, 1997.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. ArXiv, abs/2003.02218, 2020.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. ArXiv,
abs/1910.07454, 2020.
Qiang Liu, Lemeng Wu, and Dilin Wang. Splitting steepest descent for growing neural architectures.
ArXiv, 2019. URL https://arxiv.org/abs/1910.02366.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. NeST: A Neural Network
Synthesis Tool Based on a Grow-and-Prune Paradigm. ArXiv, 2017.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the Value of
Network Pruning. ArXiv, 2018.
Jun Lu, Wei Ma, and Boi Faltings. Compnet: Neural networks growing via the compact network
morphism. arXiv preprint arXiv:1804.10316, 2018.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer Science &
Business Media, 2003.
Behnam Neyshabur. Towards learning convolutions from scratch. ArXiv, 2020. URL https:
//arxiv.org/abs/2007.13657.
John Platt. A resource-allocating network for function interpolation. Neural Computation, 1991.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. `1 regularization in infinite dimen-
sional feature spaces. In International Conference on Computational Learning Theory. Springer,
2007.
11
Published as a conference paper at ICLR 2022
Andrei A. Rusu, Neil C. Rabinowitz, G. Desjardins, Hubert Soyer, J. Kirkpatrick, K. Kavukcuoglu,
Razvan Pascanu, and R. Hadsell. Progressive neural networks. ArXiv, 2016.
Thomas ShUltz and Francois Rivest. Knowledge-based cascade-correlation. volume 5, 07 2000. doi:
10.1109/IJCNN.2000.861541.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2015.
Kenneth O. Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topolo-
gies. Evol. Comput., 2002. URL https://doi.org/10.1162/106365602320169811.
Gilbert W Stewart. Perturbation theory for the singular value decomposition. Technical report, 1998.
Neil Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel Manso. Deep learning’s diminish-
ing returns. IEEE Spectrum, 58(10), 2021.
Aravind Vasudevan, Andrew Anderson, and David Gregg. Parallel multi channel convolution using
general matrix multiplication. In 2017 IEEE 28th international conference on application-specific
systems, architectures and processors (ASAP), pp. 19-24. IEEE, 2017.
Max Vladymyrov. No pressure! addressing the problem of local minima in manifold learning
algorithms. arXiv preprint arXiv:1906.11389, 2019.
Dilin Wang, Meng Li, Lemeng Wu, Vikas Chandra, and Qiang Liu. Energy-aware neural architec-
ture optimization with fast splitting steepest descent. arXiv preprint arXiv:1910.03103, 2019.
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In International
Conference on Machine Learning. PMLR, 2016.
Wei Wen, Feng Yan, and Hai Helen Li. Autogrow: Automatic layer growing in deep convolutional
networks. Arxiv, 2019. URL http://arxiv.org/abs/1906.02909.
Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a gen-
eral approach for growing neural networks. In Advances in Neural Information Process-
ing Systems, 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
fdbe012e2e11314b96402b32c0df26b7-Paper.pdf.
Lemeng Wu, Mao Ye, Qi Lei, Jason D Lee, and Qiang Liu. Steepest descent neural architecture opti-
mization: Escaping local optimum with signed neural splitting. arXiv preprint arXiv:2003.10392,
2020b.
Xin Yuan, Pedro Henrique Pamplona Savarese, and Michael Maire. Growing efficient deep networks
by structured continuous sparsification. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=wb3wxCObbRT.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. ArXiv, abs/1605.07146, 2016.
A Implementation details
GradMax requires the calculation of the quantity ∂∂Lh>-ι (eq. (9)). The user could perform this
outer product manually, but in most machine learning frameworks the following approach is easier
to implement.
We note that when WneW = 0 and f (0) = 0 We can write
Wnewf(Wnewh'-ι) = WUxh'-ι,	WaUx ∈ rm'-i×m'+1	(12)
and
z'+ι = W'+ιh' + WΓh'-ι .	(13)
Note that WaUx = 0. The advantage of this formulation is that
∂L ∂L >
∂W'ux = ∂z'+ι h'-1.	(	)
12
Published as a conference paper at ICLR 2022
Hence GradMax can easily be implemented in any framework with automatic differentiation by
temporarily introducing this auxiliary matrix W'aux in order to calculate ∂∂LTh>-ι∙ After the
columns of Wnew are calculated using SVD, the matrix W'ux is replaced with the new layer
(z'+ι = W'+ιh' + W+Wf (Wιewh'-ι)).
Growing Layers Matrix decomposition point of view provides a novel insight into the growing
problem since it can be applied to any 2 layers and can help us grow new one between the two. Note
that, in our initial derivation we studied the problem of adding new neurons to an existing layer.
Now we can choose any 2 layers k and ` and create a new layer between them through growing. If
there is already a layer between the two (i.e. k = ` - 2 or k = ` + 2), then the new neurons are
appended to the existing layers.
B Convolutions
The derivation of GradMax for fully connected layers can readily be extended to convolutional
layers:
∂⅛ =(f0(ZneW) θ Wnew *> 备)* h'-ι
=Wnew* (* * h`-ι)
dL = dL * h∏ew
∂ W+W = ∂ Z'+ι	'
=0
(15)
(16)
(17)
(18)
where * is a 2D convolution and *> is the transposed 2D convolution (i.e., the gradient of the
convolution).
As before, the required gradient is in practice most easily calculated by introducing an auxiliary
convolutional layer such that ∂Wx = ∂∂LΓ * h'τ∙ We let
Z'+ι = We+1 * h` + Wr * h`-i.
(19)
where WauX = 0 are filters of the appropriate size. Namely, if w` are filters of size (h`,w`) with
i` input channels and W'+ι contains filters of size (h'+1,w'+1) with o'+ι output channels, then
W'ux will have filters of size (h` + h`+ 一 1, w` + w'+ι 一 1) with i` input and o'+ι output channels.
We now have the equivalent of equation 11 to solve for convolutional layers:
arg max
Wnew
w'+1
Wnw * ED
∂L
∂Z'+1
s.t. IMwIIF ≤ c.
(20)
We express the 2D convolution as a matrix multiplication (Vasudevan et al., 2017) using the im2col
method. This means that W'+w is flattened to a matrix of size Ro'+1 ∙h'+1∙w'+ι ×k. The matrix
ED [∂∂L1 * h>-ι] is of size Ri' ×o'+1 ×h`+h'+1-1 ×w` +w'+1-1 but is turned into a larger matrix of
size Ro'+ι∙h'+1∙w'+ι×i'∙h'∙w' by extracting the appropriate patches (depending on the filter sizes,
padding, and strides) which are flattened and then concatenated. SVD can then be readily applied to
this reformulated problem and the resulting top-k left-singular vectors can be reshaped to form the
filters of the new layer.
C	Batch Normalization
Batch normalization scales the activations to have unit variance. This makes the incoming weights
scale-free (Arora et al., 2018; Li & Arora, 2020), which is usually a desirable property. However, it
can be disruptive when growing neurons. For the first step after growing, batch normalization keeps
the zero activations and scales the gradients with ɪ. After the first gradient step the incoming weights
will be non-zero, resulting in non-zero activations which will be scaled to unit variance. This can
13
Published as a conference paper at ICLR 2022
1 O
O O
1 1
sscηU-Jl
O 250	500	750 1000 1250 1500
Training Steps
(a) Large (ft = 20 : 10 : 10)
Figure 7: Effect of learning rate on the optimization speed and quality. We repeat each experiment
3 times with a different seed and report the average values with 80% confidence intervals.
O 1OOO 2000	3000	4000	5000
Training Steps
(b) Large (ft = 400 : 200 : 10)
disrupt learning as the outgoing weights have not been sufficiently trained yet. We observed that
using small values for the outgoing weights can be useful to reduce the negative effect of this jump.
Alternatively, as we propose in the main text, it might be preferable to set outgoing connections to
zero when batch normalization layers and use GradMaxOpt.
D	Additional Experimental Details for Student-Teacher
Experiments
Learning rate. For both experiments we perform a small hyper parameter search over the learning
rate. In Figure 7 we share learning curves using different learning rates that range between 0.2 and
0.01. Higher learning rates bring faster convergence, however best results are obtained with the
second highest learning rate equal to 0.1. To avoid confounds related to the optimizer, we restrict
the experiments to gradient descent with constant learning rate.
Correlation between singular values and the final decrease. We know from the theory that the
norm of the gradient is proportional to the singular values given by the solution of equation 11. In
other words, solution corresponding to the top singular values should give the largest norm of the
gradient for new weights and therefore should have the best training dynamics in the long run. In
Figure 8 we demonstrate this on a simple Student/Teacher set up as we used in the main paper.
We first train own network without any growing (blue curve on the top graph). We then grow the
network at different stages of optimization using any of the five singular values and let the network
converge to some value. As we can see, darker colors corresponding to the larger singular values
generally converge faster and correspond to the lower values of the objective function.
At the lower portion of the figure, we quantify this result more precisely by repeating the experiment
five times and computing the Pearson correlation coefficient between the vector of singular values
and the loss decrease some iterations after growing. There x-axis represents how many iterations
have passed since the growth and the y-axis shows the correlation between the loss values at that
iteration and the singular values. The lower the values the more (anti-)correlated the values are
and, therefore, the more we can trust the GradMax algorithm. The iteration it always negative and
gets worse as we iterate. However, if we use GradMax at the later stages of the algorithm close to
convergence, the correlation holds for a while.
Student-Teacher Experiments with Convolutional Networks. Similar to the experiments in Fig-
ure 2a, here we show the results of the Student-Teacher task on a convolutional model. As visible
on Figure 9, the same conclusions can be made for this case as well. GradMax achieves the best
norm of the gradient after each growth. Interestingly, the gradient norm is somewhat larger for the
Random growth, but in Figure 10 we see that GradMax achieves better results overall.
14
Published as a conference paper at ICLR 2022
Number of iterations
Number of iterations Number of iterations
Figure 8: Long-term effect of growing. Top: Growing once with one of 5 singular values starting at
different iteration (20th, 50th, 100th, 500th or 1000th). Blue curve represents no growing. Bottom:
The correlation between the singular values and the loss function decrease after a certain growing
iteration.
⅛0ωt8 UOS」e3d
Number of iterations
⅛0ωt8 Uos.Jead
Number of iterations
0,00
1 1
=⅛≥
Growth Number
LU-JQN JU①一Pro.J D P2Sn
5
0 5 0 5 0 5 0
" " " " " ♦ ♦
3 2 2 1 1 0 0
SSo-IV
Training Steps
00
5
100	200	300	400	500
Step Number
(a)	After Growth	(b) During Training	(c) After N Steps
Figure 9: (a) In this plot we load checkpoints from each growth step generated during the Random
experiments. We grow a single neuron and measure the norm of the gradients with respect to Wfew.
Since Random and GradMaxOpt are stochastic we repeat those experiments 10 times. GradMax
provides the maximum gradient norm (b) We measure the norm of flattened parameters throughout
the training. GradMax improves gradient norm over Random. (c) Similar to (a), we load checkpoints
from each growth step and grow a new neuron using GradMax (fg) and Random (fr). Then we
continue training for 500 steps and plot the difference in training loss (i.e. L(fr) - L(fg)). All
experiments are repeated 5 times.
Student-Teacher Experiments with Batch Norm Similar to the MLP and convolution cases,
Figures 11c and 12 provide results for the GradMax performance for the model with BatchNorm.
Setting both incoming and outgoing weights to zero In these experiments we set both side of
the new neuron to zero and initialize the bias to 1. This non-zero bias provides unit activation, which
provides non-zero gradients for the outgoing weights. After the first step, outgoing weights become
non-zero and learning takes off. Results in Figure 13a show that this approach performs worse than
GradMax.
Effect of start iteration and initial width Here, first, we look at the effect of growing new neu-
rons during different parts of the training. We repeat small the student-teacher experiments in Sec-
tion 4.1 using GradMax, but adjust the iteration the first neuron is grown. We grow 5 neurons in total
with 200 step intervals. We also adjust the training steps after the last growth so that all experiments
match in training FLOPs. Results are presented in Figure 13b. We observe that growing early in the
training works best, which highlights a different role for growing algorithms than what is presented
in the literature (Fukumizu & Amari, 2000): Neural networks can be grown during training with the
goal of improving training dynamics. Our results show the potential of such an approach. Next, in
Figure 13c, we study the effect of initial network size while keeping total FLOPs used during train-
15
Published as a conference paper at ICLR 2022
0,
1
SS9e-Jl
O 200 400	600 800 1000 1200 1400
Training Steps
(a) Small (K = 1, ft = 20 : 10 : 10)
Figure 10: Training curves averaged over 5 different runs and provided with 80% confidence in-
---Random
—GradMax
—GradMaxOpt
----BaseIine(SmaII)
—BaseIine(Big)
O 1000	2000	3000	4000	5000	6000
Training Steps
(b)	Large (K = 5, ft = 100 : 50 : 10)
tervals. In both settings GradMax significantly improves optimization over Random. Split-based
methods seems to cause some instability in large network settings causing frequent jumps in the
training objective.
----Random
101-----GradMax
----GradMaxOpt
Random
GradMax
GradMaxOpt
-0.05-
O 1OO 200	300	400	500
Step Number
(c) After N Steps
1	2	3	4	5
Growth Number
(a)	After Growth
O 250 500 750 1000 1250 1500
Training Steps
(b)	During Training
Figure 11: (a) In this plot we load checkpoints from each growth step generated during the Random
experiments. We grow a single neuron and measure the norm of the gradients with respect to Wfew.
Since Random and GradMaxOpt is stochastic we repeat those experiments 10 times. GradMax
provides the maximum gradient norm (b) We measure the norm of flattened parameters throughout
the training. GradMax improves gradient norm over Random. (c) Similar to (a), we load checkpoints
from each growth step and grow a new neuron using GradMax (fg) and Random (fr). Then we
continue training for 500 steps and plot the difference in training loss (i.e. L(fr) - L(fg)). All
experiments are repeated 5 times.
ing constant. As expected, starting with a larger student model helps optimization and obtains better
training loss for both GradMax and Random, while GradMax consistently achieves better results
than Random in all settings.
Adding More Neurons In our student-teacher experiments we stop growing new neurons when
the student model matches the teacher model in terms of number of neurons. In Figure 13d we grow
the student model beyond the size of the teacher model and observe with around 50% more neurons
(1.5x), the student model exceeds the baseline (big) performance.
16
Published as a conference paper at ICLR 2022
(a) Regular	(b) Outgoing ZeroWnW = 0
Figure 12: Training curves averaged over 5 different runs and provided with 80% confidence in-
tervals. In both settings GradMax significantly improves optimization over Random. Split-based
methods seems to cause some instability in large network settings causing frequent jumps in the
training objective.
01000-1
11c
SSenU
O 200 OO 600	800 1000 1200 1400
Training Steps
(a) All-zero initialization
10,100r,
1
SSenU
Training Steps
SSenMUE一巴I
---- GradMaX(I)
GradMaX ⑶
----GradMax (5)
----Random (1)
----Random (3)
----Random (5)
(b) Start iteration
101-
1Oo∙
l1O^1∙
10^2-
W-3-
1000	2000	3000	4000
TrainingSteps
(d) More Neurons
O 500	1000	1500	2000
Training Steps
(c) Seed Architecture
Figure 13: (a) We compare setting all-weights to zero (and having unit bias) to our approach of
setting only the incoming weights to zero (e.g. Random and GradMax). (b) Effect of growing
during different parts of the training for GradMax. Labels correspond to the iteration when the first
neuron is grown. We grow 5 neurons in total with 200 step intervals. Training steps are adjusted so
that all runs have same amount of FLOPs. (c) Effect of initial student network size. As before we
adjust total number of steps so that the total training FLOPs match for all experiments. Numbers in
parentheses represent the size of the hidden layer at initialization. We grow neurons starting from
first training iteration. (d) Growing beyond teacher network shows that with larger capacity, grown
networks exceed the baseline performance.
17