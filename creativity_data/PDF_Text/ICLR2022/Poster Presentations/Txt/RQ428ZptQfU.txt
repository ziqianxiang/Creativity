Published as a conference paper at ICLR 2022
A Deep Variational Approach to Clustering
Survival Data
Laura Manduchip Ricards Mardnkevicsp Michela C. Massi,2,3 Thomas Weikert,4
Alexander Sauter,4 Verena Gotta,5 Timothy Muller,6 Flavio Vasella,6 Marian C. Neidert,7
Marc Pfister,5 Bram Stieltjes4 & Julia E. Vogt1
1ETH Ziirich; 2PoliteCniCo di Milano; 3CADS, Human TeChnopole; 4University Hospital Basel;
5University Children's Hospital Basel; 6University of ZUrich; 7St. Gallen Cantonal Hospital
Abstract
In this work, we study the problem of Clustering survival data — a Challenging and
so far under-explored task. We introduCe a novel semi-supervised probabilistiC ap-
proaCh to Cluster survival data by leveraging reCent advanCes in stoChastiC gradient
variational inferenCe. In Contrast to previous work, our proposed method employs
a deep generative model to unCover the underlying distribution of both the ex-
planatory variables and Censored survival times. We Compare our model to the
related work on Clustering and mixture models for survival data in Comprehensive
experiments on a wide range of synthetiC, semi-synthetiC, and real-world datasets,
inCluding mediCal imaging data. Our method performs better at identifying Clus-
ters and is Competitive at prediCting survival times. Relying on novel generative
assumptions, the proposed model offers a holistiC perspeCtive on Clustering sur-
vival data and holds a promise of disCovering subpopulations whose survival is
regulated by different generative meChanisms.
1 Introduction
Survival analysis (Rodriguez, 2007; D. G. Altman, 2020) has
been extensively used in a variety of mediCal appliCations to
infer a relationship between explanatory variables and a po-
tentially censored survival outCome. The latter indiCates the
time to a Certain event, suCh as death or CanCer reCurrenCe,
and is censored when its value is only partially known, e.g.
due to withdrawal from the study (see Appendix A). ClassiCal
approaChes inClude the Cox proportional hazards (PH; Cox
(1972)) and aCCelerated failure time (AFT) models (BuCk-
ley & James, 1979). ReCently, many maChine learning teCh-
niques have been proposed to learn nonlinear relationships
from unstruCtured data (Faraggi & Simon, 1995; Ranganath
et al., 2016; Katzman et al., 2018; Kvamme et al., 2019).
Figure 1: Survival Clustering.
Clustering, on the other hand, serves as a valuable tool in data-driven disCovery and subtyping of
diseases. Yet a fully unsupervised Clustering algorithm does not use, by definition, the survival
outComes to identify Clusters. Therefore, there is no guarantee that the disCovered subgroups are
Correlated with patient survival (Bair & Tibshirani, 2004). For this reason, we foCus on a semi-
supervised learning approaCh to Cluster survival data that jointly Considers explanatory variables
and Censored outCome as indiCators for a patient’s state. This problem is partiCularly relevant for
preCision mediCine (Collins & Varmus, 2015). The identifiCation of suCh patient subpopulations
Could, for example, faCilitate a better understanding of a disease and a more personalised disease
management (FenstermaCher et al., 2011). Figure 1 sChematiCally depiCts this Clustering problem:
here, the overall patient population Consists of three groups CharaCterised by different associations
between the Covariates and survival, resulting in disparate CliniCal Conditions. The survival distribu-
tions do not need to differ between Clusters: Compare groups 1 () and 3 ().
tEqual contribution. Correspondence to {laura.manduchi,ricardsm}@inf.ethz.ch
1
Published as a conference paper at ICLR 2022
Table 1: Comparison of the proposed model to the related approaches: semi-supervised clustering
(SSC), profile regression (PR) for survival data, survival cluster analysis (SCA), and deep survival
machines (DSM). Here, t denotes survival time, x denotes explanatory variables, z corresponds to
latent representations, K stands for the number of clusters, and L(∙, ∙) is the likelihood function.
	SSC	PR	SCA	DSM	VaDeSC
Predicts t?	X	✓	✓	✓	✓
Learns z ?	X	X	✓	✓	✓
Maximises L (x, t)?	X	✓	X	X	✓
Scalable?	X	X	✓	✓	✓
Does not require K ?	X	✓	✓	X	X
Clustering of survival data, however, remains an under-explored problem. Only few methods have
been proposed in this context and they either have limited scalability in high-dimensional, unstruc-
tured data (Liverani et al., 2020), or they focus on the discovery of purely outcome-driven clusters
(Chapfuwa et al., 2020; Nagpal et al., 2021a), that is clusters characterised entirely by survival time.
The latter might fail in applications where the survival distribution alone is not sufficiently informa-
tive to stratify the population (see Figure 1). For instance, groups of patients characterised by similar
survival outcomes might respond very differently to the same treatment (Tanniou et al., 2016).
To address the issues above, We present a novel method for clustering survival data ——variational
deep survival clustering (VaDeSC) that discovers groups of patients characterised by different gener-
ative mechanisms of survival outcome. It extends previous variational approaches for unsupervised
deep clustering (Dilokthanakul et al., 2016; Jiang et al., 2017) by incorporating cluster-specific sur-
vival models in the generative process. Instead of only focusing on survival, our approach models
the heterogeneity in the relationships betWeen the covariates and survival outcome.
Our main contributions are as folloWs: (i) We propose a novel, deep probabilistic approach to
survival cluster analysis that jointly models the distribution of explanatory variables and censored
survival outcomes. (ii) We comprehensively compare the clustering and time-to-event prediction
performance of VaDeSC to the related Work on clustering and mixture models for survival data on
synthetic and real-World datasets. In particular, We shoW that VaDeSC outperforms baseline methods
in terms of identifying clusters and is comparable in terms of time-to-event predictions. (iii) We
apply our model to computed tomography imaging data acquired from non-small cell lung cancer
patients and assess obtained clustering qualitatively. We demonstrate that VaDeSC discovers clusters
associated With Well-knoWn patient characteristics, in agreement With previous medical findings.
2 Related Work
Clustering of survival data has been first explored by Bair & Tibshirani (2004) (semi-supervised
clustering; SSC). The authors propose pre-selecting variables based on univariate Cox regression
hazard scores and subsequently performing k-means clustering on the subset of features to discover
patient subpopulations. More recently, Ahlqvist et al. (2018) use Cox regression to explore differ-
ences across subgroups of diabetic patients discovered by k-means and hierarchical clustering. In the
spirit of the early Work by FareWell (1982) on mixtures of Cox regression models, Mouli et al. (2018)
propose a deep clustering approach to differentiate betWeen long- and short-term survivors based on
a modified Kuiper statistic in the absence of end-of-life signals. Xia et al. (2019) adopt a multitask
learning approach for the outcome-driven clustering of acute coronary syndrome patients. ChapfuWa
et al. (2020) propose a survival cluster analysis (SCA) based on a truncated Dirichlet process and
neural netWorks for the encoder and time-to-event prediction model. SomeWhat similar techniques
have been explored by Nagpal et al. (2021a) Who introduce finite Weibull mixtures, named deep
survival machines (DSM). DSM fits a mixture of survival regression models on the representations
learnt by an encoder neural netWork. From the modelling perspective, the above approaches focus
on outcome-driven clustering, i.e. they recover clusters entirely characterised by different survival
distributions. On the contrary, We aim to model cluster-specific associations betWeen covariates and
survival times to discover clusters characterised not only by disparate risk but also by different sur-
vival generative mechanisms (see Figure 1). In the concurrent Work, Nagpal et al. (2021b) introduce
deep Cox mixtures (DCM) jointly fitting a VAE and a mixture of Cox regressions. DCM does not
specify a generative model and its loss is derived empirically by combining the VAE loss With the
2
Published as a conference paper at ICLR 2022
likelihood of survival times. On the contrary, our method is probabilistic and has an interpretable
generative process from which an ELBO of the joint likelihood can be derived.
The approach by Liverani et al. (2020) is, on the other hand, the most closely related to ours. The
authors propose a clustering method for collinear survival data based on the profile regression (PR;
Molitor et al. (2010)). In particular, they introduce a Dirichlet process mixture model with cluster-
specific parameters for the Weibull distribution. However, their method is unable to tackle high-
dimensional unstructured data, since none of its components are parameterised by neural networks.
This prevents its usage on real-world complex datasets, such as medical imaging (Haarburger et al.,
2019; Bello et al., 2019). Table 1 compares our and related methods w.r.t. a range of properties. For
an overview of other lines of work and a detailed comparison see Appendices B and C.
3 Method
We present VaDeSC — a novel variational deep survival
clustering model. Figure 2 provides a summary of our
approach: the input vector x is mapped to a latent repre-
sentation z using a VAE with a Gaussian mixture prior.
The survival density function is given by a mixture of
Weibull distributions with cluster-specific parameters β .
The parameters of the Gaussian mixture and Weibull dis-
tributions are then optimised jointly using both the ex-
planatory input variables and survival outcomes.
Preliminaries We consider the following setting: let
D = {(xi, δi, ti)}iN=1 be a dataset of N three-tuples,
one for each patient. Herein, xi denotes the explana-
tory variables, or features. δi is the censoring indica-
tor: δi = 0 if the survival time of the i-th patient was
censored, and δi = 1 otherwise. Finally, ti is the po-
tentially censored survival time. A maximum likelihood
approach to survival analysis seeks to model the survival
distribution S(t|x) = P (T > t|x) (Cox, 1972). Two
challenges of survival analysis are (i) the censoring of
Figure 2: Summary of the VaDeSC.
survival times and (ii) a complex nonlinear relationship between x and t. When clustering survival
data, we additionally consider a latent cluster assignment variable ci ∈ {1, ..., K} unobserved at
training time. Here, K is the total number of clusters. The problem then is twofold: (i) to infer
unobserved cluster assignments and (ii) model the survival distribution given xi and ci .
Figure 3: Generative model.
Generative Model Following the problem definition above, we
assume that data are generated from a random process consist-
ing of the following steps (see Figure 3). First, a cluster as-
signment c ∈ {1, . . . , K} is sampled from a categorical distri-
bution: C 〜 p(c; π) = πc. Then, a continuous latent em-
bedding, z ∈ RJ, is sampled from a Gaussian distribution,
Whose mean and variance depend on the sampled cluster c: Z 〜
P (Z∣c; {μι,…,μκ} ,{∑ι,…,∑k}) = N (z; μc, ∑c). TheeX-
planatory variables x are generated from a distribution conditioned
on z: X 〜 p(x∣z; Y), where p(x∣z; Y) = Bernoulli(x; μγ)
for binary-valued features and N (x; μγ, diag (σ1)) for real-valued
features. Herein, μγ and σ% are produced by f (z; Y) - a decoder
neural network parameterised by Y. Finally, the survival time t de-
pends on the cluster assignment c, latent vector z , and censoring
indicator δ, i.e. t 〜 P (t|z,c). Similarly to conventional survival
analysis, we assume non-informative censoring (Rodriguez, 2007).
Survival Model Above, P (t|z, c) refers to the cluster-specific survival model. We follow an
approach similar to Ranganath et al. (2016) and Liverani et al. (2020); in particular, we as-
3
Published as a conference paper at ICLR 2022
sume that given z and c, the uncensored survival time follows the Weibull distribution given by
Weibull softplus z>βc , k , where softplus(x) = log (1 + exp(x)); k is the shape parameter; and
βc are cluster-specific survival parameters. Note that we omitted the bias term βc,0 for the sake of
brevity. Observe that softplus z>βc corresponds to the scale parameter of the Weibull distribution.
We assume that the shape parameter k is global; however, an adaptation to cluster-specific parame-
ters, as proposed by Liverani et al. (2020), is straightforward. The Weibull distribution with scale λ
and shape k has the probability density function given by f (x; λ,k) = k (λ)k 1 exp (- (X)k),
for x ≥ 0. Consequently, adjusting for right-censoring yields the following distribution:
p(t∣z,c; β,k) = f(t λz,k)δS(t∣z,c)1-δ
where β = {β1, . . . , βK}; λcz = softplus z>βc ; and S(t|z, c) = t∞=t f(t; λcz, k) is the survival
function. Henceforth, we will usep(t|z, c) as a shorthand notation for p(t|z, c; β, k). In this paper,
we only consider right-censoring; however, the proposed model can be extended to tackle other
forms of censoring.
Joint Probability Distribution Assuming the generative process described above, the joint prob-
ability ofx andt can be written as p(x, t) = Rz PcK=1 p(x, t, z, c) = Rz PcK=1 p(x|t, z, c)p(t, z, c).
It is important to note that x and t are independent given z, so are x and c. Hence, we can rewrite
the joint probability of the data, also referred to as the likelihood function, given the parameters π,
μ, ∑, γ, β, k as
p(x,t;
∏, μ, ∑, γ, β,k)
K
p(x|z; γ)p(t∣z, c; β,k)p(z∣c; μ, Σ)p(c; ∏),
z c=1
(2)
where μ = {μ1,…，μκ}, ∑ = {∑1,…，∑k}, and β = {β1,…,βκ}.
Evidence Lower Bound Given the data generating assumptions stated before, the objective is to
infer the parameters π, μ, Σ, Y, and β which better explain the covariates and survival outcomes
{xi , ti }iN=1 . Since the likelihood function in Equation 2 is intractable, we maximise a lower bound
of the log marginal probability of the data:
1 z ,	Γp(x∣z; γ)p(t∣z,c; β,k)p(z∣c; μ, Σ)p(c; ∏)]	小
logp(x, t; ∏, μ, ∑, Y, β, k) ≥ Eq(z,c∣χ,t) log [---------------q(z 水 力)---------------------卜(3)
We approximate the probability of the latent variables z and c given the observations with a varia-
tional distribution q(z, c|x, t) = q(z|x)q(c|z, t), where the first term is the encoder parameterised
by a neural network. The second term is equal to the true probabilityp(c|z, t):
q(clz t) = P(CIz t) =	p(z,tlC)P(C)	=	p(t|z，c)p(z|C)P(C)
，	，	Pκ=1 P(z,t|C)P(C)	Pκ=1 P(t|z,C)P(z|C)P(C)
Thus, the evidence lower bound (ELBO) can be written as
L(, t) = Eq(z|x)p(c|z,t) log P(x|z; Y) + Eq(z|x)p(c|z,t) logP(t|z,C; β,k)
-DKL (q (z, c∣x, t) k P (z, c; μ, ∑, ∏)).
(4)
(5)
Of particular interest is the second term which encourages the model to maximise the probability of
observing the given survival outcome t under the variational distribution of the latent embeddings
and cluster assignments q(z, C|x, t). It can be then seen as a mixture of survival distributions, each
one assigned to one cluster. The ELBO can be approximated using the stochastic gradient variational
Bayes (SGVB) estimator (Kingma & Welling, 2014) to be maximised efficiently using stochastic
gradient descent. For the complete derivation, we refer to Appendix D.
Missing Survival Time The hard cluster assignments can be computed from the distribution
P(C|z, t) of Equation 4. However, the survival times may not be observable at test-time; whereas our
derivation of the distribution P(C|z, t) depends onP(t|z, C). Therefore, when the survival time ofan
individual is unknown, using the Bayes’ rule we instead compute P(C|z)
P(ZIc)P(C)
PK=I P(ZIc)P(C).
4
Published as a conference paper at ICLR 2022
4 Experimental Setup
Datasets We evaluate VaDeSC on a range of synthetic, semi-synthetic (SUrVMNIST; Polsterl
(2019)), and real-world survival datasets with varying numbers of data points, explanatory vari-
ables, and fractions of censored observations (see Table 2). In particular, real-world clinical datasets
include two benchmarks common in the survival analysis literature, namely SUPPORT (Knaus et al.,
1995) and FLChain (Kyle et al., 2006; Dispenzieri et al., 2012); an observational cohort of pediatric
patients undergoing chronic hemodialysis (Hemodialysis; Gotta et al. (2021)); an observational co-
hort of high-grade glioma patients (HGG); and an aggregation of several computed tomography
(CT) image datasets acquired from patients diagnosed with non-small cell lung cancer (NSCLC;
Aerts et al. (2019); Bakr et al. (2017); Clark et al. (2013); Weikert et al. (2019)). Detailed descrip-
tion of the datasets and preprocessing can be found in Appendices E and G. For (semi-)synthetic
data, we focus on the clustering performance of the considered methods; whereas for real-world
data, where the true cluster structure is unknown, we compare time-to-event predictions. In addi-
tion, we provide an in-depth cluster analysis for the NSCLC (see Section 5.3) and Hemodialysis (see
Appendix H.8) datasets.
Table 2: Summary of the datasets. Here, N is the total number of data points, D is the number of
explanatory variables, K is the number of clusters if known. We report the percentage of censored
observations and whether the cluster sizes are balanced if known.
Dataset	N	D	% censored	Data type	K	Balanced?	Section
Synthetic	60,000	1,000	30	Tabular	3	Y	5.1,H.2
survMNIST	70,000	28x28	52	Image	5	N	5.1,H,6
SUPPORT	9,105	59	32	Tabular	—	—	5.2
FLChain	6,524	7	70	Tabular	—	—	H.7
HGG	453	147	25	Tabular	—	—	5.2
Hemodialysis	1,493	57	91	Tabular	—	—	5.2, H.8
NSCLC	961	64x64	33	Image	—	—	5.3, H.9
Baselines & Ablations We compare our method to several well-established baselines: the semi-
supervised clustering (SSC; Bair & Tibshirani (2004)), survival cluster analysis (Chapfuwa et al.,
2020), and deep survival machines (Nagpal et al., 2021a). For the sake of fair comparison, in SCA
we truncate the Dirichlet process at the true number of clusters if known. For all neural network
techniques, we use the same encoder architectures and numbers of latent dimensions. Although the
profile regression approach of Liverani et al. (2020) is closely related to ours, it is not scalable to
large unstructured datasets, such as survMNIST and NSCLC, since it relies on MCMC methods for
Bayesian inference and is not parameterised by neural networks. Therefore, a full-scale comparison
is impossible due to computational limitations. Appendix H.3 contains a ‘down-scaled’ experiment
with the profile regression on synthetic data. Additionally, we consider k-means and regularised
Cox PH and Weibull AFT models (Simon et al., 2011) as naive baselines. For the VaDeSC, We
perform several ablations: (i) removing the Gaussian mixture prior and performing post hoc k-
means clustering on latent representations learnt by a VAE With an auxiliary Weibull survival loss
term, Which is similar to the deep survival analysis (DSA; Ranganath et al. (2016)) combined With
k-means; (ii) training a completely unsupervised version Without modelling the survival, Which is
similar to VaDE (Jiang et al., 2017); and (iii) predicting cluster assignments When the survival time
is unobserved. Appendix G contains further implementation details.
Evaluation We evaluate the clustering performance of models, When possible, in terms of accu-
racy (ACC), normalised mutual information (NMI), and adjusted Rand index (ARI). For the time-
to-event predictions, We evaluate the ability of methods to rank individuals by their risk using the
concordance index (CI; Raykar et al. (2007)). Predicted median survival times are evaluated using
the relative absolute error (RAE; Yu et al. (2011)) and calibration slope (CAL), as implemented by
ChapfuWa et al. (2020). We report RAE on both non-censored (RAEnc) and censored (RAEc) data
points (see Equations 14 and 15 in Appendix F). The relative absolute error quantifies the relative
deviation of median predictions from the observed survival times; While the calibration slope in-
dicates Whether a model tends to under- or overestimate risk on average. For the (semi-)synthetic
datasets We average all results across independent simulations, i.e. dataset replicates; While for the
real-World data We use the Monte Carlo cross-validation procedure.
5
Published as a conference paper at ICLR 2022
5 Results
5.1	Clustering
We first compare clustering performance on the nonlinear (semi-)synthetic data. Table 3 shows
the results averaged across several simulations. In addition to the clustering, we evaluate the con-
cordance index to verify that the methods can adequately model time-to-event in these datasets.
Training set results are reported in Table 12 in Appendix H.
As can be seen, in both problems, VaDeSC outperforms other models in terms of clustering and
achieves performance comparable to SCA and DSM w.r.t. the CI. Including survival times appears to
help identify clusters since the completely unsupervised VaDE achieves significantly worse results.
It is also assuring that the model is able to predict clusters fairly well even when the survival time is
not given at test time (“w/o t”). Furthermore, performing k-means clustering in the latent space of a
VAE with the Weibull survival loss (“VAE + Weibull”) clearly does not lead to the identification of
the correct clusters. In both datasets, k-means on VAE representations yields almost no improvement
over k-means on raw features. This suggests that the Gaussian mixture structure incorporated in the
generative process of VaDeSC plays an essential role in inferring clusters.
Interestingly, while SCA and DSM achieve good results on survMNIST, both completely fail to iden-
tify clusters correctly on synthetic data, for which the generative process is very similar to the one
assumed by VaDeSC. In the synthetic data, the clusters do not have prominently different survival
distributions (see Appendix E.1); they are rather characterised by different associations between the
covariates and survival times — whereas the two baseline methods tend to discover clusters with
disparate survival distributions. The SSC offers little to no gain over the conventional k-means per-
formed on the complete feature set. Last, we note that in both datasets VaDeSC has a significantly
better CI than the Cox PH model likely due to its ability to capture nonlinear relationships between
the covariates and outcome.
Figure 4 provides a closer inspection of the clustering and latent representations on survMNIST
data. It appears that SCA and DSM, as expected, fail to discover clusters with similar KaPlan-
Meier (KM) curves and have a latent space that is driven purely by survival time. While the VAE +
Weibull model learns representations driven by both the explanatory variables (digits in the images)
and survival time, the post hoc k-means clustering fails at identifying the true clusters. By contrast,
VaDeSC is capable of discovering clusters with even minor differences in KM curves and learns
Table 3: Test set clustering performance on synthetic and survMNIST data. “VAE + Weibull”
corresponds to an ablation of VaDeSC w/o the Gaussian mixture prior. “w/o t” corresponds to the
cluster assignments made by VaDeSC when the survival time is not given. Averages and standard
deviations are reported across 5 and 10 independent simulations, respectively. Best results are shown
in bold, second best - in italic.
Dataset	Method	ACC	NMI	ARI	CI
	k-means	0.44±0.04	0.06±0.04	0.05±0.03	—
	Cox PH	—	—	—	0.77±0.02
	SSC	0.45±0.03	0.08±0.04	0.06±0.02	—
	SCA	0.45±0.09	0.05±0.05	0.04±0.05	0.82±0.02
Synthetic	DSM	0.37±0.02	0.01±0.00	0.01±0.00	0.76±0.02
	VAE + Weibull	0.46±0.06	0.09±0.04	0.09±0.04	0.71±0.02
	VaDE	0.74±0.21	0.53±0.12	0.55±0.20	—
	VaDeSC (w/o t)	0.88 ± 0.03	0.60 ± 0.07	0.67± 0.07	∏ 0/1-LΛ Λz> 0.84±0.02
	VaDeSC (OurS)	0.90±0.02	0.66±0.05	0.73±0.05	
	k-means	0.49±0.06	0.31±0.04	0.22±0.04	—
	Cox PH	—	—	—	0.74±0.04
	SSC	0.49±0.06	0.31±0.04	0.22±0.04	—
	SCA	0.56±0.09	0.46±0.06	0.33±0.10	0.79±0.06
SurvMNIST	DSM	0.54±0.11	0.40±0.16	0.31±0.14	0.79±0.05
	VAE + Weibull	0.49±0.05	0.32±0.05	0.24±0.05	0.76±0.07
	VaDE	0.47±0.07	0.38±0.08	0.24±0.08	—
	VaDeSC (w/o t)	0.57± 0.09	0.51 ± 0.09	0.37± 0.10	0.80±0.05
	VaDeSC (OurS)	0.58±0.10	0.55±0.11	0.39±0.11	
6
Published as a conference paper at ICLR 2022
(a) Ground truth
(b) VAE + Weibull
(d) DSM
(c) SCA
(e) VaDeSC (ours)
Figure 4: Cluster-specific KaPlan-Meier (KM) curves (top) and t-SNE visualisation of latent rep-
resentations (bottom), coloured according to survival times (yellow and blue correspond to higher
and lower survival times, respectively), learnt by different models (b-e) from one replicate of the
survMNIST dataset. Panel (a) shows KM curves of the ground truth clusters. Plots were generated
using 10,000 data points randomly sampled from the training set; similar results were observed on
the test set.
Table 4: Test set time-to-event performance on SUPPORT, HGG, and Hemodialysis datasets. Aver-
ages and standard deviations are reported across 5 independent train-test splits.
Dataset	Method	CI	RAEnc	RAEc	CAL
	CoXPH	0.84±0.01	—	—	—
	Weibull AFT	0.84±0.01	0.62±0.01	0.13 ± 0.01	1.27±0.02
qttppopt	SCA	0.83±0.02	0.78±0.13	0.06±0.04	1.74±0.52
SUPPORT	DSM	0.87±0.01	0.56 ± 0.02	0.13 ± 0.04	1.43±0.07
	VAE + Weibull	0.84±0.01	0.56 ± 0.02	0.20±0.02	1.28±0.04
	VaDeSC (ours)	0.85 ± 0.01	0.53±0.02	0.23±0.05	1.24±0.05
	CoxPH	0.74 ± 0.05	—	—	—
	Weibull AFT	0.74±0.05	0.56±0.04	0.14±0.09	1.16±0.10
	SCA	0.63±0.08	0.97±0.05	0.00±0.00	2.59±1.70
HGG	DSM	0.75±0.04	0.57±0.05	0.18±0.07	1.09±0.08
	VAE + Weibull	0.75±0.05	0.52±0.06	0.12 ± 0.07	1.14±0.11
	VaDeSC (ours)	0.74 ± 0.05	0.53 ± 0.06	0.13±0.07	1.12±0.09
	CoxPH	0.83±0.04	—	—	—
	Weibull AFT	0.83±0.05	0.81±0.03	0.01±0.00	4.46±0.59
	SCA	0.75±0.05	0.86±0.07	0.02 ± 0.02	7.93±3.22
Hemodialysis	DSM	0.80±0.06	0.85±0.08	0.02 ± 0.04	8.23±4.28
	VAE + Weibull	0.77±0.06	0.80 ± 0.06	0.02 ± 0.01	4.49±0.75
	VaDeSC (ours)	0.80 ± 0.05	0.78±0.05	0.01±0.00	3.74±0.58
representations that clearly reflect the covariate and survival variability. Similar differences can be
observed for the synthetic data (see Appendix H.2).
5.2	Time-to-event Prediction
We now assess time-to-event prediction on clinical data. As these datasets do not have ground truth
clustering labels, we do not evaluate the clustering performance. However, for Hemodialysis, we
provide an in-depth qualitative assessment of the learnt clusters in Appendix H.8.
Table 4 shows the time-to-event prediction performance on SUPPORT, HGG, and Hemodialysis.
Results for FLChain are reported in Appendix H.7 and yield similar conclusions. Surprisingly, SCA
often has a considerable variance w.r.t. the calibration slope, sometimes yielding badly calibrated
predictions. Note, that for SCA and DSM, the results differ from those reported in the original
papers likely due to a different choice of encoder architectures and numbers of latent dimensions. In
general, these results are promising and suggest that VaDeSC remains competitive at time-to-event
modelling, offers overall balanced predictions, and is not prone to extreme overfitting even when
applied to simple clinical datasets that are low-dimensional or contain few non-censored patients.
7
Published as a conference paper at ICLR 2022
5.3	Application to Computed Tomography Data
We further demonstrate the viability of our model in a real-world application using a collection of
several CT image datasets acquired from NSCLC patients (see Appendix E). The resulting dataset
poses two major challenges. The first one is the high variability among samples, mainly due to
disparate lung tumour locations. In addition, several demographic characteristics, such as patient’s
age, sex, and weight, might affect both the CT scans and survival outcome. Thus, a representation
learning model that captures cluster-specific associations between medical images and survival times
could be highly beneficial to both explore the sources of variability in this dataset and to understand
the different generative mechanisms of the survival outcome. The second challenge is the modest
dataset size (N = 961). To this end, we leverage image augmentation during neural network training
(Perez & Wang, 2017) to mitigate spurious associations between survival outcomes and clinically
irrelevant CT scan characteristics (see Appendix G).
We compare our method to the DSM, omitting the SCA, which seems to yield results similar to
the latter. As well-established time-to-event prediction baselines, we fit Cox PH and Weibull AFT
models on radiomics features. The latter are extracted using manual pixel-wise tumour segmentation
maps, which are time-consuming and expensive to obtain. Note that neither DSM nor VaDeSC
requires segmentation as an input. Table 5 shows the time-to-event prediction results. Interestingly,
DSM and VaDeSC achieve performance comparable to the radiomics-based models, even yielding
a slightly better calibration on average. This suggests that laborious manual tumour segmentation is
not necessary for survival analysis on CT scans. Similarly to tabular clinical datasets (see Table 4),
DSM and VaDeSC have comparable performance. Therefore, we investigate cluster assignments
qualitatively to highlight the differences between the two methods.
Table 5: Test set time-to-event prediction performance on NSCLC data. For Cox PH and Weibull
AFT, radiomics features were extracted using tumour segmentation maps. Averages and standard
deviations are reported across 100 independent train-test splits, stratified by survival time.
Method	CI	RAEnC	RAEC	CAL
Radiomics + Cox PH	0.60±0.02	—	—	—
Radiomics + WeibUll AFT	0.60±0.02	0.70±0.02	0.45±0.03	1.26±0.04
DSM	0.59 ± 0.04	0.72±0.03	0.34±0.06	1.24 ± 0.07
VaDeSC QUrS)		0.60±0.02	0.71 ± 0.03	0.35 ± 0.05	1.21±0.05
In Figure 5, we plot the KM curves and corresponding centroid CT images for the clusters discov-
ered by DSM and VaDeSC on one train-test split. By performing several independent experiments
(see Appendix H.10), we observe that both methods discover patient groups with different empirical
survival time distributions. However, in contrast to DSM, VaDeSC clusters are consistently associ-
ated with the tumour location. On the contrary, the centroid CT images of the DSM clusters show
(a) DSM
(b) VaDeSC (ours)
Figure 5: Cluster-specific KaPlan-Meier curves and corresponding centroid CT images, computed
by averaging all samples assigned to each cluster by (a) DSM and (b) VaDeSC on the NSCLC data.
8
Published as a conference paper at ICLR 2022
Table 6: Cluster-specific statistics for a few demographic and clinical variables (not used during
training) from the NSCLC dataset for DSM and VaDeSC. T. Vol. stands for the tumour volume; M1
denotes spread of cancer to distant organs and tissues; and ≥ T3 denotes a tumour stage of at least
3. Kruskal-Wallis H -test p-values are reported at the significance levels of 0.001, 0.01, and 0.05.
Variable		DSM					VaDeSC (ours)				
	1	2	34		P-VaL	1	2	34		P-VaL
T. Vol., cm3	一 23	39	38	51	≤ 1e-3	43	36	40	63	≤ 5e-2
Age, yrs	67	68	68	69	0.11	62	69	67	70	≤ 1e-3
Female, %	29	30	26	21	0.3	36	19	38	23	≤ 1e-3
Smoker, %	84	100	80	89	0.9	67	94	87	100	0.12
M1, %	40	55	16	42	0.4	20	45	44	45	0.2
≥ T3, %	27	12	23	32	0.2	10	29	35	31	0.7
no visible difference. This is further demonstrated in Figure 6, where we generated several CT scans
for each cluster by sampling from the multivariate Gaussian distribution in the latent space of the
VaDeSC. We observe a clear association with tumour location, as clusters 1 () and 3 () corre-
spond to the upper section of the lungs. Indeed, multiple studies have suggested a higher five-year
survival rate in patients with the tumour in the upper lobes (Lee et al., 2018). It is also interesting
Figure 6: CT images generated by (i) sampling
latent representations from the Gaussian mix-
ture learnt by VaDeSC and (ii) decoding rep-
resentations using the decoder network.
to observe the amount of variability within each
cluster: every generated scan is characterised by a
unique lung shape and tumour size. Since DSM is
not a generative model, we instead plot the original
samples assigned to each cluster by DSM in Ap-
pendix H.9. Finally, in Table 6 we compute cluster-
specific statistics for a few important demographic
and clinical variables (Etiz et al., 2002; Agarwal
et al., 2010; Bhatt et al., 2014). We observe that
VaDeSC tends to discover clusters with more dis-
parate characteristics and is thus able to stratify
patients not only by risk but also by clinical con-
ditions. Overall, the results above agree with our
findings on (semi-)synthetic data: VaDeSC identi-
fies clusters informed by both the covariates and
survival outcomes, leading to a stratification very
different from previous approaches.
6 Conclusion
In this paper, we introduced a novel deep probabilistic model for clustering survival data. In contrast
to existing approaches, our method can retrieve clusters driven by both the explanatory variables
and survival information and it can be trained efficiently in the framework of stochastic gradient
variational inference. Empirically, we showed that our model offers an improvement in clustering
performance compared to the related work while staying competitive at time-to-event modelling.
We also demonstrated that our method identifies meaningful clusters from a challenging medical
imaging dataset. The analysis of these clusters provides interesting insights that could be useful for
clinical decision-making. To conclude, the proposed VaDeSC model offers a holistic perspective on
clustering survival data by learning structured representations which reflect the covariates, outcomes,
and varying relationships between them.
Limitations & Future Work The proposed model has a few limitations. It requires fixing a
number of mixture components a priori since global parameters are not treated in a fully Bayesian
manner, as opposed to the profile regression. Although the obtained cluster assignments can be
explained post hoc, the relationship between the raw features and survival outcomes remains unclear.
Thus, further directions of work include (i) improving the interpretability of our model to facilitate
its application in the medical domain and (ii) a fully Bayesian treatment of global parameters. In-
depth interpretation of the clusters we found in clinical datasets is beyond the scope of this paper,
but we plan to investigate these clusters together with our clinical collaborators in follow-up work.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank Alexandru Tifrea, Nicolo Ruggeri, and Kieran Chin-Cheong for valuable discussion and
comments and Dr. Silvia Liverani, Paidamoyo Chapfuwa, and Chirag Nagpal for sharing the code.
Ricards Marcinkevics is supported by the SNSF grant #320038189096. Laura Manduchi is SuP-
ported by the PHRT SHFN grant #1-000018-057: SWISSHEART.
Ethics S tatement
The in-house PET/CT and HGG data were acquired at the University Hospital Basel and Univer-
sity Hospital Zurich, respectively, and retrospective, observational studies were approved by local
ethics committees (Ethikkommission Nordwest- und Zentralschweiz, no. 2016-01649; Kantonale
Ethikkommission Zurich, no. PB-2017-00093).
Reproducibility S tatement
To ensure the reproducibility of this work several measures were taken. The experimental setup
is outlined in Section 4. Appendix E details benchmarking datasets and simulation procedures.
Appendix F defines metrics used for model comparison. Appendix G contains data preprocess-
ing, architecture, and hyperparameter tuning details. For the computed tomography data we as-
sess the stability of the obtained results in Appendix H.10. Most datasets used for compari-
son are either synthetic or publicly available. HGG, Hemodialysis, and in-house PET/CT data
could not be published due to medical confidentiality. The code is publicly available at https:
//github.com/i6092467/vadesc.
References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefow-
icz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Va-
sudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http:
//tensorflow.org/.
H. J. W. L. Aerts, E. R. Velazquez, R. Leijenaar, C. Parmar, P. Grossmann, S. Cavalho, J. Bussink,
R. Monshouwer, B. Haibe-Kains, D. Rietveld, F. Hoebers, M. Rietbergen, C. R. Leemans,
A. Dekker, J. Quackenbush, R. Gillies, and P. Lambin. Decoding tumour phenotype by non-
invasive imaging using a quantitative radiomics approach. Nature Communications, 5, 2014.
H. J. W. L. Aerts, E. Rios Velazquez, R. T. H. Leijenaar, C. Parmar, P. Grossmann, S. Carvalho,
J. Bussink, R. Monshouwer, B. Haibe-Kains, D. Rietveld, F. Hoebers, M. M. Rietbergen, C. R.
Leemans, A. Dekker, J. Quackenbush, R. J. Gillies, and P. Lambin. Data from NSCLC-radiomics-
genomics, 2015. URL https://wiki.cancerimagingarchive.net/x/GAL1.
H. J. W. L. Aerts, L. Wee, E. Rios Velazquez, R. T. H. Leijenaar, C. Parmar, P. Grossmann, S. Car-
valho, J. Bussink, R. Monshouwer, B. Haibe-Kains, D. Rietveld, F. Hoebers, M. M. Rietbergen,
C. R. Leemans, A. Dekker, J. Quackenbush, R. J. Gillies, and P. Lambin. Data from NSCLC-
radiomics, 2019. URL https://wiki.cancerimagingarchive.net/x/FgL1.
M. Agarwal, G. Brahmanday, G. W. Chmielewski, R. J. Welsh, and K. P. Ravikrishnan. Age, tumor
size, type of surgery, and gender predict survival in early stage (stage I and II) non-small cell lung
cancer after surgical resection. Lung Cancer, 68(3):398-402, 2010.
E. Ahlqvist, P. Storm, A. Karajamaki, M. Martinell, M. Dorkhan, A. Carlsson, P. Vikman, R. B.
Prasad, D. M. Aly, P. Almgren, Y. Wessman, N. Shaat, P. Spegel, H. Mulder, E. Lindholm,
O. Melander, O. Hansson, U. Malmqvist, A. Lernmark, K. Lahti, T. Forsen, T. Tuomi, A. H.
Rosengren, and L. Groop. Novel subgroups of adult-onset diabetes and their association with
outcomes: a data-driven cluster analysis of six variables. The Lancet Diabetes & Endocrinology,
6(5):361-369, 2018.
10
Published as a conference paper at ICLR 2022
S. Amaral, W. Hwang, B. Fivush, A. Neu, D. Frankenfield, and S. Furth. Serum albumin level
and risk for mortality and hospitalization in adolescents on hemodialysis. Clinical journal of the
American Society ofNephrology, 3(3):759-767, 2008.
E. Bair and R. Tibshirani. Semi-supervised methods to predict patient survival from gene expression
data. PLoS Biology, 2(4):e108, 2004.
S. Bakr, O. Gevaert, S. Echegaray, K. Ayers, M. Zhou, M. Shafiq, H. Zheng, W. Zhang, A. Le-
ung, M. Kadoch, J. Shrager, A. Quon, D. Rubin, S. Plevritis, and S. Napel. Data for NSCLC
radiogenomics collection, 2017. URL https://wiki.cancerimagingarchive.net/
x/W4G1AQ.
S. Bakr, O. Gevaert, S. Echegaray, K. Ayers, M. Zhou, M. Shafiq, H. Zheng, J. Anthony B.,
W. Zhang, A. N. C. Leung, M. Kadoch, C. D. Hoang, J. Shrager, A. Quon, D. L. Rubin, S. K.
Plevritis, and S. Napel. A radiogenomic dataset of non-small cell lung cancer. Scientific Data, 5
(1), 2018.
G. A. Bello, T. J. W. Dawes, J. Duan, C. Biffi, A. de Marvao, L. S. G. Howard, J. S. R. Gibbs, M. R.
Wilkins, S. A. Cook, D. Rueckert, and D. P. O’Regan. Deep learning cardiac motion analysis for
human survival prediction. Nature machine intelligence, 1:95-104, 2θ19.
A. Ben-Hur, A. Elisseeff, and I. Guyon. A stability based method for discovering structure in
clustered data. Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing, pp.
6-17, 2002.
V. R. Bhatt, R. Batra, P. T. Silberstein, F. R. Loberiza, and A. K. Ganti. Effect of smoking on sur-
vival from non-small cell lung cancer: a retrospective Veterans’ Affairs Central Cancer Registry
(VACCR) cohort analysis. Medical Oncology, 32(1), 2014.
J. Buckley and I. James. Linear regression with censored data. Biometrika, 66(3):429^36, 1979.
J. C. Buckner. Factors influencing survival in high-grade gliomas. Seminars in Oncology, 30:10-14,
2003.
P. Chapfuwa, C. Tao, C. Li, C. Page, B. Goldstein, L. C. Duke, and R. Henao. Adversarial time-
to-event modeling. In Proceedings of the 35th International Conference on Machine Learning,
volume 80, pp. 735-744. PMLR, 2018.
P. Chapfuwa, C. Li, N. Mehta, L. Carin, and R. Henao. Survival cluster analysis. In Proceedings of
the ACM Conference on Health, Inference, and Learning, pp. 60-68. Association for Computing
Machinery, 2020.
T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785-
794. ACM, 2016.
K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips, D. Maf-
fitt, M. Pringle, L. Tarbox, and F. Prior. The cancer imaging archive (TCIA): Maintaining and
operating a public information repository. Journal ofDigital Imaging, 26(6):1045-1057, 2013.
F. S. Collins and H. Varmus. A new initiative on precision medicine. New England Journal of
Medicine, 372(9):793-795, 2015.
D. R. Cox. Regression models and life-tables. Journal of the Royal Statistical Society: Series B
(Methodological), 34(2):187-202, 1972.
D. G. Altman. Analysis of survival times. In Practical Statistics for Medical Research, chapter 13,
pp. 365-395. Chapman and Hall/CRC Texts in Statistical Science Series, 2020.
J. Daugirdas. Second generation logarithmic estimates of single-pool variable volume Kt/V: an
analysis of error. Journal ofthe American SOCiety ofNephrology: JASN, 4(5):1205-1213, 1993.
J. Daugirdas and D. Schneditz. Overestimation of hemodialysis dose depends on dialysis efficiency
by regional blood flow but not by conventional two pool urea kinetic analysis. ASAIO journal, 41
(3):M719-M724, 1995.
11
Published as a conference paper at ICLR 2022
C.	Davidson-Pilon, J. Kalderstam, N. Jacobson, S. Reed, B. Kuhn, P. Zivich, M. Williamson, J. K.
Abdeali, D. Datta, A. Fiore-Gartland, A. Parij, D. Wilson, Gabriel, L. Moneda, A. Moncada-
Torres, K. Stark, H. GadgiL Jona, K. Singaravelan, L. Besson, M. Sancho Pena, S. Anton,
A. Klintberg, J. Growth, J. Noorbakhsh, M. Begun, R. Kumar, S. Hussey, S. Seabold, and D. Gol-
land. lifelines: v0.25.9, 2021. URL https://doi.org/10.5281/zenodo.4505728.
N. Dilokthanakul, P. A. M. Mediano, M. Garnelo, M. C. H. Lee, H. Salimbeni, K. Arulkumaran,
and M. Shanahan. Deep unsupervised clustering with Gaussian mixture variational autoencoders,
2016. arXiv:1611.02648.
A. Dispenzieri, J. A. Katzmann, R. A. Kyle, D. R. Larson, T. M. Therneau, C. L. Colby, R. J. Clark,
G. P. Mead, S. Kumar, L. J. Melton, and S. V. Rajkumar. Use of nonclonal serum immunoglobulin
free light chains to predict overall survival in the general population. Mayo Clinic Proceedings,
87(6):517-523,2012.
D.	Etiz, L. B. Marks, S.-M. Zhou, G. C. Bentel, R. Clough, M. L. Hernando, and P. A. Lind.
Influence of tumor volume on survival in patients irradiated for non-small-cell lung cancer. Inter-
national Journal of Radiation Oncology, Biology, Physics, 53(4):835-846, 2002.
D.	Faraggi and R. Simon. A neural network model for survival data. Statistics in Medicine, 14(1):
73-82, 1995.
V. T. Farewell. The use of mixture models for the analysis of survival data with long-term survivors.
Biometrics, 38(4):1041-1046, 1982.
D. A. Fenstermacher, R. M. Wenham, D. E. Rollison, and W. S. Dalton. Implementing personalized
medicine in a cancer center. The Cancer Journal, 17(6):528-536, 2011.
R. Foley, P. Parfrey, J. Harnett, G. M. Kent, D. Murray, and P. Barre. Hypoalbuminemia, cardiac
morbidity, and mortality in end-stage renal disease. Journal of the American Society of Nephrol-
ogy : JASN, 7 5:728-36, 1996.
O. Gevaert, J. Xu, C. D. Hoang, A. N. Leung, Y. Xu, A. Quon, D. L. Rubin, S. Napel, and S. K.
Plevritis. Non-small cell lung cancer: Identifying prognostic imaging biomarkers by leveraging
public gene expression microarray data—methods and preliminary results. Radiology, 264(2):
387-396, 2012.
M. Goldstein, X. Han, A. Puli, A. Perotte, and R. Ranganath. X-CAL: Explicit calibration for
survival analysis. Advances in neural information processing systems, 33:18296-18307, 2020.
V.	Gotta, O. Marsenic, and M. Pfister. Age- and weight-based differences in haemodialysis prescrip-
tion and delivery in children, adolescents and young adults. Nephrology Dialysis Transplantation,
33:1649-1660, 2018.
V.	Gotta, O. Marsenic, and M. Pfister. Understanding urea kinetic factors that enhance personalized
hemodialysis prescription in children. ASAIO Journal, 66:115 - 123, 2019a.
V.	Gotta, M. Pfister, and O. Marsenic. Ultrafiltration rates in children on chronic hemodialysis
routinely exceed weight based adult limit. Hemodialysis International, 23, 2019b.
V.	Gotta, G. Tancev, O. Marsenic, J. E. Vogt, and M. Pfister. Identifying key predictors of mortality
in young patients on chronic haemodialysis - a machine learning approach. Nephrology, dialy-
sis, transplantation : official publication of the European Dialysis and Transplant Association -
European Renal Association, 2020.
V.	Gotta, O. Marsenic, A. Atkinson, and M. Pfister. Hemodialysis (HD) dose and ultrafiltration rate
are associated with survival in pediatric and adolescent patients on chronic hd—a large observa-
tional study with follow-up to young adult age. Pediatric Nephrology, pp. 1-12, 2021.
C. Haarburger, P. Weitz, O. Rippel, and D. Merhof. Image-based survival prediction for lung cancer
patients using CNNs. In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
2019). IEEE, 2019.
12
Published as a conference paper at ICLR 2022
H. Ishwaran, U. B. Kogalur, E. H. Blackstone, and M. S. Lauer. Random survival forests. Annals of
Applied Statistics, 2(3):841-860, 2008.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural Computation, 3(1):79-87, 1991.
Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: An unsupervised
and generative approach to clustering. In Proceedings of the 26th International Joint Conference
on Artificial Intelligence, pp. 1965-1972. AAAI Press, 2017.
M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta. Composing graphical
models with neural networks for structured representations and fast inference. In Advances in
Neural Information Processing Systems, volume 29, pp. 2946-2954. Curran Associates, Inc.,
2016.
J. L. Katzman, U. Shaham, A. Cloninger, J. Bates, T. Jiang, and Y. Kluger. DeepSurv: Personalized
treatment recommender system using a Cox proportional hazards deep neural network. BMC
Medical Research Methodology, 18(1), 2018.
D.	P. Kingma and M. Welling. Auto-encoding variational Bayes. In 2nd International Conference
on Learning Representations, 2014.
W. A. Knaus, F. E. Harrell, J. Lynn, L. Goldman, R. S. Phillips, A. F. Connors, N. V. Dawson, W. J.
Fulkerson, R. M. Califf, N. Desbiens, P. Layde, R. K. Oye, P. E. Bellamy, R. B. Hakim, and
D. P. Wagner. The SUPPORT prognostic model: Objective estimates of survival for seriously ill
hospitalized adults. Annals of Internal Medicine, 122(3):191-203, 1995.
H.	Kvamme, 0. Borgan, and I. Scheel. Time-to-event prediction with neural networks and Cox
regression. Journal of Machine Learning Research, 20(129):1-30, 2019.
R.	A. Kyle, T. M. Therneau, S. V. Rajkumar, D. R. Larson, M. F. Plevak, J. R. Offord, A. Dispenzieri,
J. A. Katzmann, and L. J. Melton. Prevalence of monoclonal gammopathy of undetermined
significance. New England Journal of Medicine, 354(13):1362-1369, 2006.
T.	Lange, V. Roth, M. L. Braun, and J. M. Buhmann. Stability-based validation of clustering solu-
tions. Neural Computation, 16:1299-1323, 2004.
Y. LeCun, C. Cortes, and C. J. Burges. MNIST handwritten digit database. ATT Labs, 2, 2010. URL
http://yann.lecun.com/exdb/mnist.
H. W. Lee, C.-H. Lee, and Y. S. Park. Location of stage I-III non-small cell lung cancer and survival
rate: Systematic review and meta-analysis. Thoracic Cancer, 9(12):1614-1622, 2018.
J. L. Lehr and P. Capek. Histogram equalization of CT images. Radiology, 154(1):163-169, 1985.
S. Liverani, D. I. Hastie, L. Azizi, M. Papathomas, and S. Richardson. PReMiuM: An R package
for profile regression mixture models using Dirichlet processes. Journal of Statistical Software,
64(7), 2015.
S. Liverani, L. Leigh, I. L. Hudson, and J. E. Byles. Clustering method for censored and collinear
survival data. Computational Statistics, 2020.
S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In Advances in
Neural Information Processing Systems 30, pp. 4765-4774. Curran Associates, Inc., 2017.
S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb,
N. Bansal, and S.-I. Lee. From local explanations to global understanding with explainable AI
for trees. Nature machine intelligence, 2(1):56-67, 2020.
Y. Luo, T. Tian, J. Shi, J. Zhu, and B. Zhang. Semi-crowdsourced clustering with deep generative
models. In Advances in Neural Information Processing Systems, volume 31, pp. 3212-3222.
Curran Associates, Inc., 2018.
13
Published as a conference paper at ICLR 2022
L. Manduchi, K. Chin-Cheong, H. Michel, S. Wellmann, and J. E. Vogt. Deep conditional gaussian
mixture model for constrained clustering. In Advances in Neural Information Processing Systems,
volume 35, 2021.
O. Marsenic, M. Anderson, and K. G. Couloures. Relationship between interdialytic weight gain
and blood pressure in pediatric patients on chronic hemodialysis. BioMed Research International,
2016.
G. J. McLachlan and D. Peel. Finite mixture models. John Wiley & Sons, 2004.
E. Min, X. Guo, Q. Liu, G. Zhang, J. Cui, and J. Long. A survey of clustering with deep learning:
From the perspective of network architecture. IEEE Access, 6:39501-39514, 2018.
J. Molitor, M. Papathomas, M. Jerrett, and S. Richardson. Bayesian profile regression with an
application to the national survey of children’s health. Biostatistics, 11(3):484-498, 2010.
S. C. Mouli, B. Ribeiro, and J. Neville. A deep learning approach for survival clustering without
end-of-life signals, 2018. URL https://openreview.net/forum?id=SJme6-ZR- .
E.	Movilli, P. Gaggia, R. Zubani, C. Camerini, Valerio Vizzardi, G. Parrinello, S. Savoldi, M. Fis-
cher, F. Londrino, and G. Cancarini. Association between high ultrafiltration rates and mortality in
uraemic patients on regular haemodialysis. A 5-year prospective observational multicentre study.
Nephrology, dialysis, transplantation: Official publication of the European Dialysis and Trans-
plant Association - European Renal Association, 22(12):3547-3552, 2007.
C. Nagpal, X. R. Li, and A. Dubrawski. Deep survival machines: Fully parametric survival re-
gression and representation learning for censored data with competing risks. IEEE Journal of
Biomedical and Health Informatics, 2021a.
C. Nagpal, S. Yadlowsky, N. Rostamzadeh, and K. Heller. Deep Cox mixtures for survival regres-
sion, 2021b. arXiv:2101.06536.
F.	Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
L. Perez and J. Wang. The effectiveness of data augmentation in image classification using deep
learning, 2017. arXiv:1712.04621.
S.	PolsterL Survival analysis for deep learning, 2019. URL https://k-d-w.org/blog/
2019/07/survival- analysis- for- deep- learning/.
R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria, 2020. URL https://www.R-project.org/.
R. Ranganath, A. Perotte, N. Elhadad, and D. Blei. Deep survival analysis. In Proceedings of the
1st Machine Learning for Healthcare Conference, volume 56, pp. 101-114. PMLR, 2016.
V. C. Raykar, H. Steck, B. Krishnapuram, C. Dehing-Oberije, and P. Lambin. On ranking in survival
analysis: Bounds on the concordance index. In Proceedings of the 20th International Conference
on Neural Information Processing Systems, pp. 1209-1216. Curran Associates Inc., 2007.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate infer-
ence in deep generative models. In Proceedings of the 31st International Conference on Machine
Learning, volume 32, pp. 1278-1286. PMLR, 2014.
R. Rodriguez. Lecture notes on generalized linear models, 2007. URL https://data.
princeton.edu/wws509/notes/.
O. Rosen and M. Tanner. Mixtures of proportional hazards regression models. Statistics in Medicine,
18(9):1119-1131, 1999.
L. S. Shapley. A value for n-person games. In Contributions to the Theory of Games, volume 2, pp.
307-317. Princeton University Press, 1953.
14
Published as a conference paper at ICLR 2022
N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for Cox's proportional
hazards model via coordinate descent. Journal of Statistical Software, 39(5), 2011.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In 3rd International Conference on Learning Representations, 2015.
J. Tanniou, I. van der Tweel, S. Teerenstra, and K. C. B. Roes. Subgroup analyses in confirmatory
clinical trials: time to be specific about their purposes. BMC Medical Research Methodology, 16
(1), 2016.
L.	van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research, 9(86):2579-2605, 2008.
J. J. M. van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin, V. Narayan, R. G. H. Beets-
Tan, J.-C. Fillion-Robin, S. Pieper, and H. J. W. L. Aerts. Computational radiomics system to
decode the radiographic phenotype. Cancer Research, 77(21):e104-e107, 2017.
T. J. Weikert, T. A. D’Antonoli, J. Bremerich, B. Stieltjes, G. Sommer, and A. W. Sauter. Evaluation
of an AI-powered lung nodule algorithm for detection and 3D segmentation of primary lung
tumors. Contrast Media & Molecular Imaging, 2019.
M. Weller, M. van den Bent, M. Preusser, E. Le Rhun, J. C. Tonn, G. Minniti, M. Bendszus, C. Bal-
ana, O. ChinoL L. Dirven, P. French, M. E. Hegi, A. S. Jakola, M. Platten, P. Roth, R. Ruda,
S. Short, M. Smits, M. J. B. Taphoorn, A. von Deimling, M. Westphal, R. Soffietti, G. Reifen-
berger, and W. Wick. EANO guidelines on the diagnosis and treatment of diffuse gliomas of
adulthood. Nature Reviews Clinical Oncology, 18(3):170-186, 2020.
M.	Winkel. Statistical lifetime-models, 2007. URL http://www.stats.ox.ac.uk/
~winkel/bs3b07_l1-8.pdf. Lecture notes. University of Oxford.
E.	Xia, X. Du, J. Mei, W. Sun, S. Tong, Z. Kang, J. Sheng, J. Li, C. Ma, J. Dong, and S. Li.
Outcome-driven clustering of acute coronary syndrome patients using multi-task neural network
with attention, 2019. arXiv:1903.00197.
C.-N. Yu, R. Greiner, H.-C. Lin, and V. Baracos. Learning patient-specific cancer survival distri-
butions as a sequence of dependent regressors. In Advances in Neural Information Processing
Systems, volume 24. Curran Associates, Inc., 2011.
15
Published as a conference paper at ICLR 2022
A Censoring in Survival Analysis
Censoring is one of the key features of survival anal-
ysis that distinguish it from the classical regression
modelling (Rodriguez, 2007). For some units in the
study the event of interest, e.g. death, is observed,
whereas for others it does not occur during the ob-
servation period. The latter units are referred to as
censored, and the observation time for these units is a
lower bound on the time-to-event. The censoring event
described above is commonly known as right censor-
ing (Winkel, 2007) and is the only form of censoring
considered throughout the current paper. Figure 7 de-
picts an example of the survival data: 30 patients from
an observational cohort, 13 of whom were censored.
Figure 7: Survival time, in days, measured
from the enrolment in the study for a few
subjects from an observational cohort of
high-grade glioma patients. Here, × corre-
sponds to death (the event of interest), and
• denotes censoring.
Although there exist several types of censoring, most
of the schemes assume that it is non-informative, i.e.
that the censoring time is independent of unit’s sur-
ViVal beyond it (Rodriguez, 2007). This assumption is made by the VaDeSC model as well.
B	Related Work
Herein, we proVide a detailed oVerView of the related work. The proposed VaDeSC model (see
Section 3) builds on three major topics: (i) probabilistic deep unsuperVised clustering, (ii) mixtures
of regression models, and (iii) surViVal analysis.
Deep Unsupervised Clustering Long-established clustering algorithms, such as k-means and
Gaussian Mixture Models, haVe been recently combined with deep neural networks to learn better
representations of high-dimensional data (Min et al., 2018). SeVeral techniques haVe been proposed
in the literature, featuring a wide range of neural network architectures. Among them, the VAE
(Kingma & Welling, 2014; Rezende et al., 2014) combines Variational Bayesian methods with the
flexibility and scalability of neural networks. Gaussian mixture Variational autoencoders (GMM-
VAE; Dilokthanakul et al. (2016)) and Variational deep embedding (VaDE; Jiang et al. (2017)) are
Variants of the VAE in which the prior is a Gaussian mixture distribution. With this assumption, the
data is clustered in the latent space of the VAE and the resulting inference model can be directly
optimised within the framework of stochastic gradient Variational Bayes. Further extensions focus
on the inclusion of side information in the form of pairwise constraints (Manduchi et al., 2021),
oVercoming the restrictiVe i.i.d. assumption of the samples, or on a fully Bayesian treatment of
global parameters (Luo et al., 2018; Johnson et al., 2016), alleViating the need to fix the number of
clusters a priori.
Mixtures of Regression Models Mixtures of regression (McLachlan & Peel, 2004) model a re-
sponse Variable by a mixture of indiVidual regressions with the help of latent cluster assignments.
Such mixtures do not haVe to be limited to a finite number of components; for example, profile re-
gression (Molitor et al., 2010) leVerages the Dirichlet mixture model for cluster assignment. Along
similar lines, within the machine learning community, mixture of experts models (MoE) were intro-
duced by Jacobs et al. (1991).
Mixture models haVe been successfully leVeraged for surViVal analysis as well (Farewell, 1982;
Rosen & Tanner, 1999; LiVerani et al., 2020; Chapfuwa et al., 2020; Nagpal et al., 2021a;b). For
instance, Farewell (1982) considered fitting separate models for two subpopulations: short-term and
long-term surViVors. Rosen & Tanner (1999) extended the classical Cox PH model with the MoE.
Recent neural network approaches typically fit mixtures of surViVal models on rich representations
produced by an encoding neural network (Chapfuwa et al., 2020; Nagpal et al., 2021a;b).
Nonlinear Survival Analysis One of the first machine learning approaches for surViVal analysis
was Faraggi-Simon's network (Faraggi & Simon, 1995), which was an extension of the classical
Cox PH model (Cox, 1972). Since then, an abundance of machine learning methods haVe been
16
Published as a conference paper at ICLR 2022
developed: random survival forests (Ishwaran et al., 2008), deep survival analysis (Ranganath et al.,
2016), neural networks based on the Cox PH model (Katzman et al., 2018; Kvamme et al., 2019),
adversarial time-to-event modelling (Chapfuwa et al., 2018), and deep survival machines (Nagpal
et al., 2021a) etc. An exhaustive overview of these and other techniques is beyond the scope of the
current paper.
C Comparison with Related Work
Generative Assumptions In addition to the differences summarised in Table 1, VaDeSC and the
related approaches (Chapfuwa et al., 2020; Nagpal et al., 2021a; Liverani et al., 2020) assume differ-
ent data generating processes. Simplified graphical models assumed by these techniques are shown
in Figure 8. Both SCA and DSM are not generative w.r.t. explanatory variables x, and thus, do
not maximise the joint likelihood L (x, t) and are outcome-driven. On the other hand, both profile
regression and VaDeSC specify a model for covariates x, as well as for survival time t. However, PR
does not infer latent representations z and importantly, in its simplest form assumes that x ⊥ t|c.
The latter assumption can limit the discriminative power of a mixture with few components. By
default, VaDeSC does not make such restrictive assumptions.
(b) DSM	(c) PR	(d) VaDeSC (ours)
Figure 8: Comparison of the generative assumptions made by the models related to the VaDeSC:
(a) survival cluster analysis (SCA; Chapfuwa et al. (2020)), (b) deep survival machines (DSM; Nag-
pal et al. (2021a)), (c) and profile regression for survival data (PR; Liverani et al. (2020)). For the
sake of brevity, the censoring indicator δ, model parameters and hyperparameters were omitted.
Here, t denotes survival time, x denotes explanatory variables, z corresponds to the latent represen-
tation, and c stands for the cluster assignment. Shaded nodes correspond to observed variables. The
presence of dashed edges depends on the modelling choice.
Deep Cox Mixtures The concurrent work by Nagpal et al. (2021b) on deep Cox mixtures allows
leveraging latent representations learnt by a VAE to capture nonlinear relationships in the data. The
hidden representation is simultaneously used to fit a mixture of K Cox models. In contrast to our
method, DCM does not specify a generative model explicitly. The VAE loss is added as a weighted
term to the main loss derived from the Cox models and is optimised jointly using a Monte Carlo
EM algorithm. The VAE is mainly used to enforce a Gaussian prior in the latent space, resulting in
more compact representations. It can be replaced with a regularised AE, or even an MLP encoder,
as mentioned by the authors (Nagpal et al., 2021b). Thus, the maximisation of the joint likelihood
L (x, t), rather than of L(t), is not the main emphasis of the DCMs. On the contrary, our approach
is probabilistic and specifies a clear and interpretable generative process from which an ELBO of
the joint likelihood can be derived formally. Additionally, VaDeSC enforces a more structured
representation in the latent space, since it uses a Gaussian mixture prior.
Empirical Contribution Last but not least, the experimental setup of the current paper is different
from those of Bair & Tibshirani (2004); Chapfuwa et al. (2020); Liverani et al. (2020); Nagpal et al.
(2021a;b). Previous work on mixture modelling and clustering for survival data has focused on
applications to tabular datasets. Survival analysis on image datasets (Haarburger et al., 2019; Bello
et al., 2019) poses a challenge pertinent to many application areas, e.g. medical imaging. An
important empirical contribution of this work is to apply the proposed technique and demonstrate its
utility on computed tomography data (see Section 5.3).
17
Published as a conference paper at ICLR 2022
D ELBO TERMS
In this appendix, we provide the complete derivation of the ELBO of VaDeSC using the SGVB
estimator. From Equation 5 we can rewrite the ELBO as
L(x,t) = Eq(z∣x)p(c∣z,t) [logp(x∣z; γ) + logp(t∣z,c; β, k) + logp(z∣c; μ, Σ)
(6)
+ log p(c; π) - log q(z, c|x, t) .
In the following we investigate each term of the equation above.
Reconstruction The first term is also known as the reconstruction term in a classical VAE set-
ting. Herein, we provide the derivation for p(x∣z; Y) = BernoUlli(x; μ,), i.e. for binary-valued
explanatory variables:
1 L	(l)
Eq(z|x)p(c|z,t) logp(xIz; γ) = Eq(z|x) log p(xIz; γ) ≈ LE log p(xIz(l); γ)
l=1
LD
L xxXd log μY,)d + (1 - Xd) log(1 - μY)d)
l=1 d=1
(7)
where L is the number OfMonte Carlo samples in the SGVB estimator; μY) = f (z(l); γ); Z(I)〜
q(z∣x) = N(z; μθ, σθ2); [μe,log σ(] = g(x; θ) and g(∙; θ) is an encoder neural network
parameterised by θ; Xd refers to the d-th component ofx and D is the number of features. For other
data types, the distributionp(xIz; γ) in Equation 7 needs to be chosen accordingly.
Survival The second term of the ELBO includes the survival time. Following the survival model
described by Equation 1, the SGVB estimator for the term is given by
LK
Eq(z|x)p(c|z,t) logp(tIz,c; β,k) ≈	logp(tIz(l), c = V; β, k)p(c = VIz(l), t)
l=1 ν=1
LK
L xx log
l=1 ν=1
where λνz (l)
softplus z(l)>βν .
(8)
δ
P(c = VIz(I),t),
Clustering
The third term in Equation 6 corresponds to the clustering loss and is defined as
K
Eq(z∣x)p(c∣z,t) logp(z∣c; μ, ∑) = Eq(z∣x) Ep(C = ν∣z,t)logp(z∣c = V; μ, Σ)
ν=1
1LK
≈ L XXp(c = VIz(I),t)logp(z(I)Ic = ν; μ, Σ).
l=1 ν=1
Prior The fourth term can be approximated as
1LK
Eq(z|x)p(c|z,t) log p(c; π) ≈ 1∑ ∑p(c=ν∣z(l), t) log p(c = V; π).
(9)
(10)
Variational Finally, the fifth term of the ELBO corresponds to the entropy of the variational dis-
tribution q(z, c|x, t):
-Eq(z|x)p(c|z,t) log q(z, c|x, t) = Eq(z|x)p(c|z,t)[log q(z|x) + log p(c|z, t)]	(11)
= -Eq(z|x) log q(z |x) - Eq(z|x)p(c|z,t) logp(c|z, t),
18
Published as a conference paper at ICLR 2022
where the first term is the entropy of a multivariate Gaussian distribution with a diagonal covariance
matrix. We can then approximate the expression in Equation 11 by
J	LK
2(log(2n) + 1)+ Xlogσθ,j - L XXp(c = VIz(I),t)logp(c = VIz(I),t),	(12)
where J is the number of latent space dimensions.
E	Datasets
Below we provide detailed summaries of the survival datasets used in the experiments (see Table 2).
Synthetic As the simplest benchmark problem, we simulate synthetic survival data based on the
generative process reminiscent of the VaDeSC generative model (see Figure 3). In this dataset,
both covariates and survival times are generated based on latent variables which are distributed
as a Gaussian mixture. The dependence between latent and explanatory variables is given by a
multilayer perceptron and is, therefore, nonlinear. Appendix E.1 contains a detailed description of
these simulations and a visualisation of one replicate (see Figure 9).
survMNIST Another dataset we consider is the semi-synthetic survival MNIST (survMNIST)
introduced by Polsterl (2019), used for benchmarking nonlinear survival analysis models, e.g. by
Goldstein et al. (2020). In survMNIST, explanatory variables are given by images of handwritten
digits (LeCun et al., 2010). Digits are assigned to clusters and synthetic survival times are generated
using the exponential distribution with a cluster-specific scale parameter. Appendix E.2 provides
further details on the generation procedure.
SUPPORT To compare models on real clinical data, we consider the Study to Understand Prog-
noses and Preferences for Outcomes and Risks of Treatments (SUPPORT; Knaus et al. (1995))
consisting of seriously ill hospitalised adults from five tertiary care academic centres in the United
States. It includes demographic, laboratory, and scoring data acquired from patients diagnosed with
cancer, chronic obstructive pulmonary disease, congestive heart failure, cirrhosis, acute renal failure,
multiple organ system failure, and sepsis.
FLChain A second clinical dataset was acquired in the course of the study of the relationship
between serum free light chain (FLChain) and mortality (Kyle et al., 2006; Dispenzieri et al., 2012)
conducted in Olmsted County, Minnesota in the United States. The dataset includes demographic
and laboratory variables alongside with the recruitment year. FLChain is by far the most low-
dimensional among the considered datasets (see Table 2). Both SUPPORT and FLChain data are
publicly available.
HGG Additionally, we perform experiments on an in-house dataset consisting of 453 patients
diagnosed with high-grade glioma (HGG), a type of brain tumour (Buckner, 2003). The cohort
includes patients with glioblastoma multiforme and anaplastic astrocytoma (Weller et al., 2020)
treated surgically at the University Hospital Zurich between 03/2008 and 06/2017. The dataset Con-
sists of demographic variables, treatment assignments, pre- and post-operative volumetric variables,
molecular markers, performance scores, histology findings, and information on tumour location.
This dataset has by far the least amount of patients (see Table 2) and therefore, could be a good
benchmark for the ‘low data’ regime.
Hemodialysis Another clinical dataset we include is an observational cohort of patients from the
DaVita Kidney Care (DaVita Inc., Denver, CO, USA) dialysis centres (Gotta et al., 2018; 2019a;b;
2020; 2021). The cohort is composed of patients who started chronic hemodialysis (HD) in child-
hood and have then received thrice-weekly HD between 05/2004 and 03/2016, with a maximum
follow-up until < 30 years of age. The dataset consists of demographic factors, such as the age
from the start of dialysis, gender, as well as etiology of kidney disease and comorbidities, dialysis
dose in terms of spKt/V, eKt/V (Daugirdas, 1993; Daugirdas & Schneditz, 1995), fluid removal in
terms of UFR (mL/kg/h) and total ultrafiltration (UF, % per kg dry weigth per session), and the in-
terdialytic weight gain (IDWG) (Marsenic et al., 2016). It should be noted that this dataset is rather
19
Published as a conference paper at ICLR 2022
challenging, given the high percentage of censored observations (see Table 2). The scientific use of
the deidentified standardised electronic medical records was approved by DaVita (DaVita Clinical
Research®, Minneapolis, MN); IRB approval was not required since retrospective analysis was per-
formed on the deidentified dataset. A complete description of the variables and measurements used
is provided by Gotta et al. (2021) in the Methods section.
NSCLC Finally, we perform experiments on a pooled dataset of five observational cohorts con-
sisting of patients diagnosed with non-small cell lung cancer (NSCLC). We consider a large set of
pretreatment CT scans and CT components of positron emission tomography (PET)/CT scans from
the following sources:
•	An in-house PET/CT dataset consisting of 392 patients treated at the University Hospital
Basel (Weikert et al., 2019). It includes manual delineations, clinical and survival data. A
retrospective, observational study was approved by the local ethics committee (Ethikkom-
mission Nordwest- und Zentralschweiz, no. 2016-01649).
•	Lung1 dataset consisting of 422 NSCLC patients treated at Maastro Clinic, the Netherlands
(Aerts et al., 2014; 2019). For these patients, CT scans, manual delineations, clinical, and
survival data are available. Lung1 is publicly available in The Cancer Imaging Archive
(TCIA; Clark et al. (2013)).
•	Lung3 dataset consisting of 89 NSCLC patients treated at Maastro Clinic, the Netherlands
(Aerts et al., 2014; 2015). Lung3 includes pretreatment CT scans, tumour delineations, and
gene expression profiles. It is is publicly available in TCIA.
•	NSCLC Radiogenomics dataset acquired from 211 NSCLC patients from the Stanford Uni-
versity School of Medicine and Palo Alto Veterans Affairs Healthcare System (Bakr et al.,
2017; 2018; Gevaert et al., 2012). The dataset comprises CT and PET/CT scans, segmenta-
tion maps, gene mutation and RNA sequencing data, clinical data, and survival outcomes.
It is is publicly available in TCIA.
Only subjects with a tumour segmentation map and transversal CT or PET/CT scan available are
retained. Patients who have the slice of (PET)/CT with the maximum tumour area outside the lungs,
e.g. in the brain or abdomen, are excluded from the analysis. Thus, the final dataset includes 961
subjects.
E.1 Synthetic Data Generation
Herein, we provide details on the generation of synthetic survival data used in our experiments (see
Section 4). The procedure for simulating these data is very similar to the generative process assumed
by the proposed VaDeSC model (cf. Figure 3) and can be summarised as follows:
1.	Let ∏c = kK , for 1 ≤ C ≤ K.
2.	Sample Ci 〜Cat(π), for 1 ≤ i ≤ N.
3.	Sample μc,j 〜Unif (-ɪ, ɪ), for 1 ≤ C ≤ K and 1 ≤ j ≤ J.
4.	Let Σc = IJSc, where Sc ∈ RJ×J is a random symmetric, positive-definite matrix,1 for
1 ≤ C≤ K.
5.	Sample Zi 〜N (μ^, Σci), for 1 ≤ i ≤ N.
6.	Let gdec(z) = W2 ReLU (W1ReLU (W0z + b0) + b1) + b2 , where W0 ∈ Rh×J, W1 ∈
Rh×h, W2 ∈ RD×h and b0 ∈ Rh, b1 ∈ Rh, b2 ∈ RD are random matrices and vectors,2 3
and h is the nUmber of hidden Units.
7.	Let xi = gdec (zi ), for 1 ≤ i ≤ N.
8.	Sample βc,j 〜Unif (-10,10), for 1 ≤ C ≤ K and 0 ≤ j ≤ J.
9.	Sample Ui 〜WeibUll (SoftplUs (z>0弓),k),3 for 1 ≤ i ≤ N.
1 Generated using make_spd_matrix from scikit-learn (PedregOsa et al., 2011).
2Wo, Wι, W2 are generated using make_low_rank_matrix from scikit-learn (Pedregosa et al., 2011).
3We omitted bias terms for the sake of brevity.
20
Published as a conference paper at ICLR 2022
10.	Sample δi 〜Bernoulli(1 - PCens), for 1 ≤ i ≤ N.
11.	Let ti = Ui if δi = 1, and sample ti 〜unif(0, Ui) otherwise, for 1 ≤ i ≤ N.
Here, similar to SeCtion 3, K is the number of Clusters; N is the number of data points; J is the
number of latent variables; D is the number of features; k is the shape parameter of the Weibull
distribution (see Equation 1); and pCens denotes the probability of Censoring. The key differenCe
between Clusters is in how latent variables z generate the survival time t: eaCh Cluster has a different
weight veCtor β that determines the sCale of the Weibull distribution.
In our experiments, we fix K = 3, N = 60000, J = 16, D = 1000, k = 1, and pCens = 0.3. We
hold out 18000 data points as a test set and repeat simulations 5 times independently. A visualisation
of one simulation is shown in Figure 9. As Can be seen in the t-SNE (van der Maaten & Hinton,
2008) plot, three Clusters are not as Clearly separable based on Covariates alone; on the other hand,
they have visibly distinCt survival Curves as shown in the KM plot.
(a) t-SNE plot
Figure 9: Visualisation of one repliCate of the synthetiC data. t-SNE plot (on the left) is based on
the explanatory variables only. Kaplan-Meier Curves (on the right) are plotted separately for eaCh
Cluster. Different Colours Correspond to different Clusters. As Can be seen, Clusters are determined
by both differenCes in (a) Covariate and (b) survival distributions.
(b) Kaplan-Meier plot
E.2 Survival MNIST Data Generation
We generate semi-synthetic survMNIST data as described by Polsterl (2019) (original implementa-
tion available at https://github.com/sebp/survival-cnn-estimator). The gener-
ative process can be summarised by the following steps:
1. Assign every digit in the original MNIST dataset (LeCun et al., 2010) to one ofK clusters
(ensure that every cluster contains at least one digit)
2. Sample risk score rc 〜unif (2, 15), for 1 ≤ C ≤ K.
3. Let λc = T^ exp(rc), for 1 ≤ C ≤ K, where T0 is the specified mean survival time and T^
is the baseline hazard.
4. Sample ai 〜unif(0,1) and let Ui = - l 2 3 4 5 6 7 8°λg-ai, for 1 ≤ i ≤ N.
ci
5. Let qcens = q1-pcens (ULN), where qα(∙) denotes the α-th quantile.
6. Sample &ns 〜Unif (minι≤i≤N Ui, qcens).
7. Let δi = 1 if Ui ≤ tcens, and δi = 0 otherwise, for 1 ≤ i ≤ N.
8. Let ti = Ui if δi = 1, and ti = tcens otherwise, for 1 ≤ i ≤ N.
Observe that here pcens is only a lower bound on the probability of censoring and the fraction of
censored observations can be much larger than pcens.
21
Published as a conference paper at ICLR 2022
Table 7: An example of an assignment
of MNIST digits to clusters and risk
scores under K = 5.
Cluster	Digits	Risk Score
1	{4,8}	7.62
2	{0, 3, 6}	13.12
3	{1, 9}	5.17
4	{2, 7}	11.60
5	{5}	2.03
In our experiments, we fix K = 5 and pcens = 0.3. We
repeat simulations independently 10 times. The train-test
split is defined as in the original MNIST data (60,000 vs.
10,000). Table 7 shows an example of cluster and risk
score assignments in one of the simulations. The resulting
dataset is visualised in Figure 10. Observe that the clus-
ters are not well-distinguishable based on covariates alone.
Also note that some clusters contain many censored data
points, whereas some do not contain any, as shown in the
KaPlan-Meier plot.
(a) t-SNE plot
Figure 10: Visualisation of one replicate of the survMNIST dataset. Generally, survMNIST tends to
have more disparate survival distributions across clusters than the synthetic data (cf. Figure 9).
F Evaluation Metrics
Below we provide details about the evaluation metrics used to compare clustering and time-to-event
prediction performance of the models. For clustering, we employ the adjusted Rand index (ARI)
and normalised mutual information (NMI) as implemented in the sklearn.metrics module of
scikit-learn (Pedregosa et al., 2011). Clustering accuracy is evaluated by finding an optimal map-
ping between assignments and true cluster labels using the Hungarian algorithm, as implemented in
Sklearn.utils.linear .assignment一
The concordance index (Raykar et al., 2007) evaluates how well a survival model is able to rank
individuals in terms of their risk. Given observed survival times ti , predicted risk scores ηi , and
censoring indicators δi , the concordance index is defined as
CI =
PN=1 PN=I 1tj<ti1ηj>ηi δj
∖PN ∖PN	→
i=1	j=1 1tj <ti
(13)
Thus, a perfect ranking achieves a CI of 1.0, whereas a random ranking is expected to have a CI of
0.5. We use the lifelines implementation of the CI (Davidson-Pilon et al., 2021).
Relative absolute error (Yu et al., 2011) evaluates predicted survival times in terms of their relative
deviation from the observed survival times. In particular, given predicted survival times ti, for non-
censored data points, RAE is defined as
RAEnc =
PN=II &- ti) /用 &
PN=i δi
(14)
On censored data, similarly to Chapfuwa et al. (2020), we define the metric as follows:
RAEc =
PN=IJ Gi - ti) /ty (I - δQ1ti ≤ti
PL (i- W
(15)
22
Published as a conference paper at ICLR 2022
Last but not least, similar to Chapfuwa et al. (2020) we use the calibration slope to evaluate the
overall calibration of models. It is desirable for the predicted survival times to match with the
empirical marginal distribution of t. Calibration slope indicates whether a model tends to under- or
overestimate risk on average, and thus, an ideal calibration slope is 1.0.
G Implementation Details
Herein, we provide implementation details for the experiments described in Section 5.
G.1 Preprocessing
For all datasets, we re-scale survival times to be in [0.001, 1.001]. For survMNIST, we re-scale
the features to be in [0, 1]. For tabular datasets, we re-scale the features to zero-mean and unit-
variance. Categorical features are encoded using dummy variables. For Hemodialysis data, the
same preprocessing routine is adopted as described by Gotta et al. (2021).
For NSCLC CT images, preprocessing consists of several steps. For each subject, slices within 15
mm from a slice with a maximal transversal tumour area are averaged to produce a single 2D image.
2D images are then cropped around the lungs and normalised. Subsequently, histogram equalisation
is applied (Lehr & Capek, 1985). The images are then downscaled to a resolution of 64 × 64 pixels.
Figure 11 provides an example of raw and preprocessed images from the Lung1 dataset. Finally,
during VaDeSC, SCA, and DSM training, we augment images by randomly reducing/increasing the
brightness, adding Poisson noise, flipping horizontally, blurring, zooming, stretching, and shifting
both vertically and horizontally (see Figure 11). During initial experimentation, we observed that
image augmentation was crucial for decorrelating clinically irrelevant scan characteristics, such as
rotation or scaling, and the predicted survival outcome. Neither VaDeSC, nor DSM, achieve predic-
tive performance comparable to radiomics-based features in the absence of augmentation.
(a) Raw	(b) Preprocessed (c) Brightness (d) Poisson noise	(e) Flip
(f) Rotate	(g) Blur	(h) Zoom	(i) Stretch	(j) Shift
Figure 11: A full resolution Lung1 image averaged across 11 CT slices (a) before and (b) after
cropping, normalisation, and histogram equalisation. Panels (c)-(j) depict augmentations applied to
images during neural network training.
For the radiomics-based Cox PH and Weibull AFT models, features were extracted from the pre-
processed 2D images using the RadiomicsFeatureExtractor provided by the PyRadiomics
library (van Griethuysen et al., 2017). Tumour segmentation maps were given as input during feature
extraction, to detect the ROI.
23
Published as a conference paper at ICLR 2022
G.2 Models, Architectures & Hyperparameters
The following implementations of the models were used:
•	VaDeSC: we implemented our model in TensorFlow 2.4 (Abadi et al., 2015). The code is
available at https://github.com/i6092467/vadesc.
•	k-means: we used the implementation available in the scikit-learn library (Pedregosa et al.,
2011).
•	Cox PH & Weibull AFT: we used the implementation available in the lifelines library
(Davidson-Pilon et al., 2021).
•	Radiomics: radiomic features were extracted using the PyRadiomics library (van Griethuy-
sen et al., 2017).
•	SSC: we re-implemented the model in Python based on the original R (R Core Team, 2020)
code provided by Bair & Tibshirani (2004).
•	SCA: we adapted the original code by Chapfuwa et al. (2020) available at https://
github.com/paidamoyo/survival_cluster_analysis.
•	DSM: we adapted the original code by Nagpal et al. (2021a) available at https:
//github.com/autonlab/DeepSurvivalMachines.
•	Profile regression: we used the original implementation available in the R package
PReMiuM (Liverani et al., 2015).
Throughout our experiments, we use several different encoder and decoder architectures described
in Tables 8, 9, and 10. As mentioned before, the encoder architectures and numbers of latent di-
mensions were kept fixed across all neural-network-based techniques, namely VaDeSC, SCA, and
DSM, for the sake of fair comparison.
Hyperparameters VaDeSC hyperparameters across all datasets are reported in Table 11. We
use L = 1 Monte Carlo samples for the SGVB estimator. In most datasets, we do not pretrain
autoencoder weights. Generally, we observe little variability in performance when adjusting the
number of latent dimensions, shape of the Weibull distribution or mini-batch size. For the SCA
model, we tuned the concentration parameter of the Dirichlet process, mini-batch size, learning rate,
and the number of training epochs. For the sake of fair comparison, we truncate the Dirichlet process
at the true number of clusters, when known. Similarly, for the DSM, the mini-batch size, learning
rate, and the number of training epochs were tuned. To make the comparison with VaDeSC fair, we
fixed the discount parameter for the censoring loss at 1.0, since VaDeSC does not discount censored
observations in the likelihood, and used the same number of mixture components as for the VaDeSC.
Pretraining VAE-based models (Dilokthanakul et al., 2016; Jiang et al., 2017; Kingma & Welling,
2014) tend to converge to undesirable local minima or saddle points, due to high reconstruction
error at the beginning of the training. To prevent this, the encoder and decoder networks are usually
pretrained using an autoencoder loss. In the most of our experiments, we did not observe such
behaviour. Hence, we did not pretrain the model, except for the Hemodialysis dataset. The latter
showed better performance if pretrained for only 1 epoch. After the encoder-decoder network has
been pretrained, we projected the training samples into the latent space and fitted a Gaussian mixture
model to initialise the parameters π, μ, Σ.
24
Published as a conference paper at ICLR 2022
Table 8: Encoder and decoder architectures used for synthetic, survMNIST, and HGG data. tfkl
stands for tensorflow.keras.layers. encoded_size corresponds to the number of la-
tent dimensions, J; and inp_shape is the number of input dimesions, D. For SUrvMNIST, the
activation of the last decoder layer is set to 'sigmoid'.
Encoder
1 tfkl.Dense(500, activation='relu')
2 tfkl.Dense(500, activation='relu')
3 tfkl.Dense(2000, activation='relu')
4 mu = tfkl.Dense(encoded-size, activation=None);
sigma = tfkl.Dense(encoded-size, activation=None)
Decoder
1 tfkl.Dense(2000, activation='relu')
2 tfkl.Dense(500, activation='relu')
3 tfkl.Dense(500, activation='relu')
4 tfkl.Dense(inp-shape)
Table 9: Encoder and decoder architectures used for SUPPORT, FLChain, and Hemodialysis data.
Encoder
1 tfkl.Dense(50, activation='relu')
2 tfkl.Dense(100, activation='relu')
3 mu = tfkl.Dense(encoded-size, activation=None);
sigma = tfkl.Dense(encoded size, activation=None)
Decoder
1 tfkl.Dense(100, activation'relu')
2 tfkl.Dense(50, activation='relu')
3 tfkl.Dense(inp shape)
25
Published as a conference paper at ICLR 2022
Table 10: Encoder and decoder architectures used for NSCLC data, based on VGG
(de-)convolutional blocks (Simonyan & Zisserman, 2015).
Encoder
// VGG convolutional block
1 tfkl.Conv2D(32, 3, activation='relu')
2 tfkl.Conv2D(32, 3, activation='relu')
3 tfkl.MaxPooling2D((2, 2))
// VGG convolutional block
4 tfkl.Conv2D(64, 3, activation='relu')
5 tfkl.Conv2D(64, 3, activation='relu')
6 tfkl.MaxPooling2D((2, 2))
7	tfkl.Flatten()
8	mu = tfkl.Dense(encoded_size, activation=None);
sigma = tfkl.Dense(encoded size, activation=None)
Decoder
1	tfkl.Dense(10816, activation=None)
2 tfkl.Reshape(target _shape=(13, 13, 64))
// VGG deconvolutional block
3 tfkl.UpSampling2D((2, 2))
4 tfkl.Conv2DTranspose(64, 3, activation='relu')
5 tfkl.Conv2DTranspose(64, 3, activation='relu')
// VGG deconvolutional block
6 tfkl.UpSampling2D((2, 2))
7 tfkl.Conv2DTranspose(32, 3, activation='relu')
8 tfkl.Conv2DTranspose(32, 3, activation='relu')
9 tfkl.Conv2DTranspose(1, 3, activation=None)
Table 11: Hyperparameters used for training the VaDeSC across all datasets. Herein, J denotes
the number of latent dimensions; K is the number of clusters; and k is the shape parameter of the
WeibUll distribution. MSE stands for mean squared error; BCE - for binary cross entropy.
Dataset	J	K	k	Mini-batch Size	Learning Rate	# epochs	Rec. Loss	Pretrain # epochs
Synthetic	16^	"ʃ	1.0	256	1e-3	-1,000-	^SE	0
SUrvMNIST	16	5	1.0	256	1e-3	300	BCE	0
SUPPORT	16	4	2.0	256	1e-3	300	MSE	0
FLChain	4	2	0.5	256	1e-3	300	MSE	0
HGG	16	3	2.0	256	1e-3	300	MSE	0
Hemodialysis	16	2	3.0	256	2e-3	500	MSE	1
NSCLC	32	4	1.0	64	1e-3	1,500	MSE	0
26
Published as a conference paper at ICLR 2022
H Further Results
H.1 Training Set Clustering Performance
Table 12: Training set clustering performance on synthetic and survMNIST data. Averages and
standard deviations are reported across 5 and 10 independent simulations, respectively. Best results
are shown in bold, second best - in italic. The results are similar to the test-set performance reported
in Table 3.
Dataset	Method	ACC	NMI	ARI	CI
	k-means	0.44±0.04	0.06±0.04	0.05±0.03	—
	CoX PH	—	—	—	0.78±0.02
	SSC	0.44±0.02	0.08±0.04	0.06±0.02	—
	SCA	0.45±0.09	0.05±0.05	0.05±0.05	0.83 ± 0.02
Synthetic	DSM	0.39±0.04	0.02±0.01	0.02±0.02	0.76±0.01
	VAE + Weibull	0.46±0.06	0.09±0.04	0.09±0.04	0.72±0.02
	VaDE	0.73±0.21	0.53±0.12	0.55±0.21	—
	VaDeSC (w/o t)	0.88 ± 0.03	0.60 ± 0.07	0.67±0.07	∏ eς-Lβ ∏ι 0.85±0.01
	VaDeSC (ours)	0.90±0.02	0.66±0.05	0.74±0.05	
	k-means	0.48±0.06	0.30±0.04	0.21±0.04	—
	Cox PH	—	—	—	0.77±0.05
	SSC	0.48±0.06	0.30±0.04	0.21±0.04	—
	SCA	0.56±0.09	0.47±0.07	0.34±0.11	0.83±0.05
SUrvMNIST	DSM	0.52±0.10	0.38±0.17	0.28±0.15	0.82 ± 0.05
	VAE + Weibull	0.48±0.06	0.31±0.06	0.24±0.06	0.80±0.08
	VaDE	0.47±0.07	0.38±0.08	0.24±0.07	—
	VaDeSC (w/o t)	0.57 ± 0.10	0.52 ± 0.10	0.37±0.10	0 83±OO6
	VaDeSC (ours)	0.58±0.10	0.54±0.12	0.39±0.11	..
H.2 Qualitative Results: Synthetic Data
(a) Ground truth (b) VAE + Weibull (c) SCA
(d) DSM
(e) VaDeSC (ours)
Figure 12: Cluster-specific Kaplan-Meier (KM) curves (top) and t-SNE visualisation of latent rep-
resentations (bottom), coloured according to survival times (yellow and blue correspond to higher
and lower survival times, respectively), learnt by different models (b-e) from one replicate of the
synthetic dataset. Panel (a) shows KM curves of the ground truth clusters. Plots were generated
using 10,000 data points randomly sampled from the training set, similar results were observed on
the test set.
H.3 Comparison with Profile Regression
Herein, we provide additional experimental results to compare VaDeSC with the profile regression
(PR) for survival data (Liverani et al., 2020). The comparison is performed on a subsample of
the original synthetic dataset (see Appendix E.1) consisting of N = 5000 data points. Prior to
performing profile regression, we reduce the dimensionality of the dataset by retaining only the first
100 principal components, which preserve, approximately, 90% of the variance. We fit a profile
27
Published as a conference paper at ICLR 2022
survival regression model with the cluster-specific shape parameter of the Weibull distribution by
running 10000 iterations of the MCMC in the burn-in period and 2000 iterations after the burn-in.
To facilitate fair comparison, we set the initial number of clusters to the ground truth K = 3. For
VaDeSC, the same hyperparameters are used as reported in Table 11.
Table 13 reports clustering performance on the test set of 1500 data points. VaDeSC, both given
survival times at prediction and without them, outperforms profile regression by a margin. Never-
theless, PR offers a noticeable improvement over k-means clustering and even more sophisticated
SCA and DSM (cf. Table 3). While in some instances, PR manages to identify some cluster struc-
ture, in others, it assigns all data points to a single cluster, hence, a large standard deviation across
simulations. It is also interesting to see that a reduction in the dataset size has led to a significant
decrease in the performance of VaDeSC. We attribute this to a highly nonlinear structure of this
dataset.
Table 13: Test set clustering performance on a subsample of the synthetic dataset. Averages and
standard deviations are reported across 5 independent simulations. VaDeSC outperforms the PR
baseline by a large margin.
Method	ACC	NMI	ARI
k-means	0.44±0.02	0.07±0.04	0.05±0.02
PR	0.47±0.13	0.15±0.17	0.15±0.17
VaDeSC (w/o t)	0.73 ± 0.08	0.34 ± 0.11	0.38±0.13
VaDeSC (ours)	0.77±0.09	0.41±0.13	0.45±0.16
This experiment has clear shortcomings: due to computational limitations, we are only able to fit a
profile regression model on a transformed version of the dataset with fewer dimensions which might
not reflect the full capacity of the PR model. Nevertheless, we believe that poor scalability of the
MCMC, both in the number of covariates and in the number of training data points, warrant a more
computationally efficient approach.
H.4 Varying the Fraction of Censored Data Points
Herein we investigate the behaviour of the proposed approach under different fractions of censored
observations. We perform a controlled experiment on the synthetic dataset (see Appendix E.1) where
we vary the percentage of censored data while keeping all other simulation parameters fixed. We
evaluate the proposed algorithm, VaDeSC, and several baselines (VaDE, DSM, and Weibull AFT)
under 10-90% of censored observations and compare their clustering and time-to-event predictive
performance.
(a)	(b)	(c)	(d)
Figure 13: Performance of the VaDeSC on the synthetic data (see Appendix E.1) at varying percent-
ages of censored data points (10-90%): (a) clustering accuracy, (b) concordance index, (c) relative
absolute error (RAE) evaluated on non-censored and (d) censored data. Averages and standard de-
viations are reported across 5 independent simulations, evaluation was performed on test data. For
reference, we report performance of the unsupervised clustering model VaDE and of DSM and
Weibull AFT models for survival analysis.
Figure 13 shows the results of the experiment. VaDeSC stably achieves clustering accuracy of,
approximately, 90% for 10, 30, and 50% of censored observations. However, for 70 and 90%, its
accuracy drops considerably, and variance across simulations increases. Nevertheless, even at 90%
of censored data points, on average, VaDeSC is still more accurate and stable than the completely
28
Published as a conference paper at ICLR 2022
unsupervised VaDE. For time-to-event predictions, all models behave as expected: with an increase
in the percentage of censored observations, test-set CI decreases, RAEnc increases, while RAEc
decreases. Overall, VaDeSC, DSM, and Weibull AFT appear to behave and scale very similarly
w.r.t. changes in censoring. In the future, it would be interesting to perform a similar experiment on
a real-world dataset with a moderate percentage of censored observations, e.g. SUPPORT.
H.5 Varying the Number of Components in VaDeSC
Figure 14: Average test set (a) CI and (b) RAE on synthetic data with K = 3 clusters achieved by
VaDeSC models with different numbers of mixture components. Error bars correspond to standard
deviations across 5 independent simulations. The CI plot (left) exhibits a pronounced ‘elbow’ at the
true number of clusters.
2	4	6	8	10	12
Number of Clusters in VaDeSC
(a)	(b)
Figure 15:	Average test set (a) CI and (b) RAE on survMNIST with K = 5 clusters achieved by
VaDeSC models with different numbers of mixture components. Error bars correspond to standard
deviations across 10 independent simulations. The CI plot (left) appears to saturate at the true
number of clusters, although the ‘elbow’ is not quite as sharp as for the synthetic data (cf. Figure 14).
29
Published as a conference paper at ICLR 2022
H.6 Reconstructions & Generated survMNIST Samples
Figure 16:	Reconstructions of survMNIST digits. Original digits are shown in the top row, their
reconstructions by VaDeSC — in the bottom row.
/ IJ Z 1
/ 3rl
3 Ii 5
it. ⅛-0 ⅛* IJ
JZ19 G
ri a 3 a
Grʌ /，
Oo。。
0 O OtP
O 2 06
O。。。
7717 7
771 7
? 7 3 7
7 7 7 ɔ-
q夕夕Cr
Ol y 0— ⅛
√ 7 4 √
夕1 9 q
(a) Cluster 1.
(b) Cluster 2.	(c) Cluster 3.	(d) Cluster 4.	(e) Cluster 5.
Figure 17:	survMNIST samples generated by VaDeSC. In this simulation, the true clustering is given
by {{7}, {0, 9}, {2, 4, 6}, {8}, {1, 3, 5}}. Cluster 1 appears to align with {0, 9}; cluster 2 — with
{7}; cluster 4 — with {2, 4, 6}; cluster 5 — with {1, 3, 5}.
H.7 Time-to-event Prediction: FLChain
Table 14: Time-to-event test set performance on FLChain. Averages and standard deviations are
reported across 5 train-test splits. All of the methods perform comparably w.r.t. CI. SCA achieves
better RAE and calibration.
Method	CI	RAEnC	RAEC	CAL
Cox PH	0.80±0.01	—	—	—
Weibull AFT	0.80±0.01	0.72 ± 0.01	0.02±0.00	2.18 ± 0.07
SCA	0.78±0.02	0.69±0.08	0.05 ± 0.05	1.33±0.24
DSM	0.79 ± 0.01	0.76±0.05	0.02±0.01	2.35±0.66
VAE + Weibull	0.80±0.01	0.76±0.01	0.02±0.00	2.55±0.07
VaDeSC (ours)	0.80±0.01	0.76±O01	0.02±0.00	2.52±0.08
H.8 Qualitative Results: Hemodialysis Data
In addition to the results reported in Section 5.2, we have investigated qualitatively the clustering
obtained by VaDeSC when applied to the Hemodialysis dataset. The true structure of this data is
unknown, however several studies have suggested a stratification of patients according to age and
dialysis doses (Gotta et al., 2021). We compute the optimal number of clusters using grid-search
by measuring the time-to-event prediction performance. With two clusters our model achieved the
best results. In Figure 18(a), we plot the t-SNE decomposition of the latent embeddings, z (see
Figure 2), obtained by VaDeSC. Observe that the patients clustered together tend to be close in the
latent space, following the mixture of Gaussian prior. We also plot KaPlan-Meier curves for each
patient subgroup according to the predicted cluster in Figure 18(b). Interestingly, the two resulting
curves differ substantially from each other. The patients in cluster 2 () show a higher survival
probability over time compared to those in cluster 1 ().
30
Published as a conference paper at ICLR 2022
Oooooo
4 2 2 4 6
- - -
N uoωu ① EQ山 NSj
(a) t-SNE plot
(b) KaPlan-Meier plot
Figure 18: Visualisation of (a) the t-SNE decomposition of the Hemodialysis data in the embedded
space and (b) the Kaplan-Meier curves for the two clusters discovered by VaDeSC. The two clusters
have substantially different survival distributions.
Q.8,6,42 Q
Lo.o.o.6o.
A=-qeqcud -e>>Jns
We now characterise each subgroup of patients by iden-
tifying the most influential covariates in determining the
cluster assignment by VaDeSC. In particular, we ap-
ply a state-of-the-art method based on Shapley values
(Shapley, 1953), namely the TreeExplainer by Lundberg
et al. (2020), to explain an XGBoost classifier (Chen &
Guestrin, 2016) trained to predict the VaDeSC cluster
labels from patient characteristics (see Appendix I). To
mimic the VaDeSC clustering model, we included survival
times as an additional input to the classifier. In Figure 20,
we present the most important covariates together with
their cluster-wise distributions. It is important to note that
the survival time was also ranked among the most impor-
tant, but we did not include it into Figure 20. Serum al-
bumin level, previously identified as an important survival
Figure 19: Cluster-wise β parameters
of the Weibull distributions discovered
on the Hemodialysis dataset.
indicator for patients receiving hemodialysis (Foley et al., 1996; Amaral et al., 2008), emerges as the
most important predictor. Additionally, dialysis dose in terms of spKt/V and fluid balance in terms
of ultrafiltration (UF) rate prove to be crucial in classifying the data into high-risk (cluster 1, )
and low-risk (cluster 2, ) groups. By analysing the cluster-wise distributions, we notice that the
high-risk cluster is characterised by lower values of albumin, dialysis dose, and ultrafiltration, in
agreement with previous studies (Gotta et al., 2020; 2021; Movilli et al., 2007).
Cluster-Specific Survival Models Ultimately, our model should discover subgroups of patients
characterised not only by their risk but, more importantly, by the relationships between the covari-
ates and survival outcomes. In other words, the cluster-specific parameters of the Weibull distribu-
tion should ideally vary across clusters, highlighting different associations between the covariates.
We demonstrate this in Figure 19 by plotting the cluster-specific survival parameters, βc (see Equa-
tion 1), trained using the Hemodialysis data. We observe a clear difference between the two clusters,
with several latent dimensions described by both positive and negative survival parameters depend-
ing on the considered cluster.
31
Published as a conference paper at ICLR 2022
	
Figure 20: The most important predictors for VaDeSC cluster assignments, according to the average
Shapley values (see Appendix I). The survival time was not included into this plot, although it
was ranked among the most important features. The two clusters are characterised by disparate
distributions of the considered clinical variables, where the high-risk cluster shows lower values of
albumin, dialysis dose, and ultrafiltration in agreement with previous studies.
H.9 Qualitative Results: NSCLC
Mean	Assigned Samples

(a) DSM
(b) VaDeSC (ours)
Figure 21: A random selection of CT images assigned by (a) DSM and (b) VaDeSC to each cluster
(denoted by colour) with the corresponding centroid images, computed across all samples. The
colours correspond to the same clusters as in Figure 5. Clusters discovered by VaDeSC are strongly
correlated with the tumour location; DSM does not appear to discover such association.
32
Published as a conference paper at ICLR 2022
Mean	Generated Samples
GDpa OCgtDB 巴 CnBla 8gχ⅜
用gQD⅜κB fiBCDasaax诲网
(£@0通0③%®GE由C03□回
Figure 22: An extended version of Figure 6. CT images generated by (i) sampling latent represen-
tations from the Gaussian mixture learnt by VaDeSC and (ii) decoding the representations using the
decoder network. The colours correspond to the same clusters as in Figure 5.
H.10 Clustering Stability: NSCLC
The notion of stability has been widely explored as a validation technique for clustering algorithms
(Ben-Hur et al., 2002; Lange et al., 2004). The intuition is that a ‘good’ algorithm should discover
consistent clustering structures across repeated random simulations or replicates. However, unsu-
pervised learning algorithms based on deep neural networks have been shown to be highly unstable
on real-world and complex data (Jiang et al., 2017). Herein we test the stability of VaDeSC and
compare it to a competitive baseline, DSM, on the NSCLC dataset. This dataset is extremely com-
plex due to both the high-dimensionality and the limited number of samples. We run both methods
on 20 independent train-test splits (with random seeds) and we observe the differences in the learnt
clustering structures.
First of all, note that the number of discovered clusters varies across experiments. Even though the
number of mixture components in survival distribution is defined a priori, some clusters collapse,
i.e. they do not contain any samples from both the training and test set. We measure this source of
instability by computing the percentage of experiments that resulted in at least one collapsed cluster
in Table 15. We observe that only 5% of VaDeSC’s runs resulted in collapsed clusters, compared to
45% for DSM. Thus, VaDeSC is relatively stable w.r.t. the number of discovered clusters.
We also test the variability in the survival information encoded in each cluster, also shown by the
KaPlan-Meier curves in Figure 5. For each experiment, We compute the median survival time of the
samples assigned by the algorithm to each cluster. In Table 15 we report the average and standard
deviation across the experiments of the cluster-specific median survival time. To have a meaningful
average, We sort clusters by the median survival time, highest to loWest. As discussed in Section 5.3,
DSM tends to discover clusters With more disparate survival curves than VaDeSC, Which is reflected
by the averages reported. Here, We instead focus on the standard deviation. Overall, VaDeSC shoWs
loWer standard deviations than DSM. VaDeSC appears to be stable W.r.t. the survival information
captured by each cluster.
Table 15: The average and the standard deviation of the cluster-specific median survival time com-
puted across 20 independent experiments. For each experiment We re-train the model on a different
train-test split. The survival time has been scaled betWeen 0 and 1. We also report the percentage of
experiments that resulted in one or more collapsed clusters.
Method	Collapsed Clusters, %	Average Median Survival Time ± SD			
		Cluster 1	Cluster 2	Cluster 3	Cluster 4
DSM	45	0.296 ± 0.087	0.240 ± 0.083	0.138 ± 0.055	0.093 ± 0.032
VaDeSC	5	0.220 ± 0.044	0.157 ± 0.016	0.139 ± 0.016	0.086 ± 0.038
Finally, We test Whether the association betWeen tumour locations and clusters is also consistent
across experiments. We first compute the means of the mixture of Gaussians learnt by VaDeSC
for each experiment, i.e. train-test split. As these vectors live in the latent space of VaDeSC, We
33
Published as a conference paper at ICLR 2022
decode them, using the decoder network, to project them into the input space of the CT images.
Resulting images are shown in Figure 23, where each column corresponds to a different seed of the
train-test split. As before, we sort the clusters according to the median survival times such that the
upper row corresponds to patients with higher overall survival probability. It is evident that in most
experiments, the upper row corresponds to the upper section of the lungs, whereas the bottom row
— to the lower section of the lungs. Thus, VaDeSC discovers clusters consistently associated with
tumour location.
GD ① CDaXD (IXDGDaXDm ①① CDGDGDGDGD ①①
CE①①CEGuE①CE①①CE①①① ①①④①CEGE
GD④①①GDQxE(E④CE(EGEQXIXE④①④3D①
XWGgW ①①①① QDgDm ① OD GD GD OBQB QW
Figure 23: CT images generated by training the VaDeSC model 20 times using different train-test
splits. For each trained model, we (i) select the learnt means of the Gaussian mixture and (ii) decode
them using the decoder network. Each column corresponds to a different seed of the train-test split
used to train the model. Each row corresponds to a different cluster. For each seed, we order the
rows according to the median survival within the cluster, hence the upper rows are associated with
higher survival time. We observe a clear association with tumour location, consistent across most
experiments.
I Explaining VaDeSC Cluster Assignments
In this appendix, we detail the post hoc analysis performed to enrich our case study on the real-world
Hemodialysis dataset (see Appendix H.8) by explaining VaDeSC cluster assignments in terms of
input covariates. Similar analysis can be performed for any survival dataset where an explanation of
VaDeSC clusters in terms of salient clinical concepts is desirable.
Background: SHAP algorithm for interpreting model predictions SHapley Additive exPlana-
tion (SHAP), by Lundberg & Lee (2017), is an algorithm that aims at explaining the prediction of
an instance x by computing the contribution of each feature to the prediction. This contribution is
estimated in terms of Shapley regression values, an additive feature attribution method inspired by
the coalitional game theory (Shapley, 1953). Additive feature attribution methods imply that the
explanation model is a linear function of binary variables: g(z0) = φ0 + PF=I φjZj where g(∙) is
the explanation model, z0 ∈ {0, 1}F is the binary feature vector of dimension F, and φj ∈ R is the
effect of the j-th feature on the output f(x) of the original model.
Shapley regression values estimate this effect by exploiting ideas drawn from the coalitional game
theory. In particular, for the j-th variable the Shapley regression values are computed as
φj =
S⊆F∖{j}
|S|!(|F|-|S|-1)!
|F|!
[fS∪{j} (xS∪{j}) - fS(XS)],
(16)
whereF= {1, ..., F} is the set of all predictor variables, xS∪{j} and xS are input vectors from x,
composed of the features in the subset S ⊆ F with and without the j -th feature, respectively, and
f(∙) is the original model fitted on the set of predictor variables defined by the subscript.
The feature importances estimated by SHAP are the Shapley values of a conditional expectation
function of the original model (Lundberg et al., 2020). With SHAP, global interpretations are con-
sistent with the local explanations, since the Shapley values for instance i are the atomic unit of the
global interpretations. Indeed, the global importance of the j -th feature is computed as
N
Ij =三 X■，	(17)
i=1
34
Published as a conference paper at ICLR 2022
where N is the total number of observations used to explain the model globally.
Despite the theoretical advantages of SHAP values, their practical use is hindered by the complexity
of estimating E [f (x)|xS] efficiently. TreeExplainer (Lundberg et al., 2020) is a variant of SHAP
algorithm that computes the classical Shapley values from the game theory specifically for trees and
tree ensembles reducing the complexity from exponential to polynomial time.
Preprocessing With the goal of explaining VaDeSC cluster assignments in terms of input covari-
ates, we preselect features manually, to make our explanations robust and relevant from a clinical
perspective. Guided by the medical expertise, we exclude highly correlated or redundant features.
While the VaDeSC can handle the entire raw dataset meaningfully, the presence of highly correlated
features might affect their relevance. Since p(c|z, t) in VaDeSC depends on survival time (see Sec-
tion 3), we included the survival times as an additional input to the classifier. Note that for this post
hoc analysis we used training data only, as we were interested in explaining what our model learnt
from the raw data to perform clustering.
Explanations To characterise patient subgroups identified by VaDeSC, one needs to identify the
features in the input space that maximise cluster separability in the embedding space. Note that
a classifier trained to recognise cluster labels from the raw covariates is inherently seeking those
dimensions of maximal separability in the input. Therefore, we first fitted a classifier model to
extract the explanations in terms of the input features most relevant for the prediction of cluster
labels.
In particular, we trained an eXtreme Gradient Boosting (XGBoost) binary classifier (Chen &
Guestrin, 2016), to predict patient labels c ∈ {1, 2} assigned by VaDeSC. We choose XGBoost
since it has high accuracy in the presence of nonlinear relationships, naturally captures feature in-
teractions, suffers from low bias, and supports fast exact computation of Shapley values by TreeEx-
plainer (Lundberg et al., 2020). XGBoost was trained for 5,000 boosting rounds with a maximum
tree depth of 6, a learning rate equal to 0.01, a subsampling rate of 0.5, and early stopping after
200 rounds without an improvement on the validation set (set to 10% of the training data). After
training the classifier, we used the TreeExplainer to compute SHAP values and identify the most
determinant features for each cluster label, as in Equation 17. To profile the clusters precisely and
avoid the noise induced by weakly assigned observations, we computed SHAP values on a sample
of cluster prototypes, i.e. observations with maxν∈{i,2} p(c = ν |z, t) ≥ 0.75. We retained at most
500 prototypes from each cluster. Due to the cluster imbalance, that resulted in 500 observations
from cluster 1 and 212 - from cluster 2.
35