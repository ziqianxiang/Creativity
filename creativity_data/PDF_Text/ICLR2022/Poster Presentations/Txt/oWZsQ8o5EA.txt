Published as a conference paper at ICLR 2022
On the Generalization of Models Trained
with SGD: Information-Theoretic Bounds and
Implications
Ziqiao Wang
University of Ottawa
zwang286@uottawa.ca
Yongyi Mao
University of Ottawa
ymao@uottawa.ca
Ab stract
This paper follows up on a recent work of Neu et al. (2021) and presents some new
information-theoretic upper bounds for the generalization error of machine learn-
ing models, such as neural networks, trained with SGD. We apply these bounds
to analyzing the generalization behaviour of linear and two-layer ReLU networks.
Experimental study of these bounds provide some insights on the SGD training
of neural networks. They also point to a new and simple regularization scheme
which we show performs comparably to the current state of the art.
1 Introduction
The observation that high capacity deep neural networks trained with mini-batched stochastic gra-
dient descent, referred to SGD in this paper, tend to generalize well (Zhang et al., 2017) contradicts
the classical wisdom in statistical learning theory (e.g., Vapnik (1998) ) and has stimulated intense
research interest in understanding the generalization behaviour of modern neural networks.
In this direction, generalization bounds for over-parameterized neural networks are obtained (Allen-
Zhu et al., 2019; Bartlett et al., 2017; Neyshabur et al., 2015; 2018a;b; Arora et al., 2018; 2019) and
a curious “double descent” phenomenon is observed and analyzed (Belkin et al., 2019; Nakkiran
et al., 2019; Yang et al., 2020). Built on a connection between stability and generalization (Bousquet
& Elisseeff, 2002), a stability-based bound is first presented in Hardt et al. (2016), followed by a
surge of research effort exploiting similar approaches (London, 2017; Chen et al., 2018; Feldman &
Vondrak, 2019; Lei & Ying, 2020; Bassily et al., 2020). Information-theoretic bounding techniques
established recently (Russo & Zou, 2016; 2019; Xu & Raginsky, 2017; Asadi et al., 2018; Bu et al.,
2020; Steinke & Zakynthinou, 2020; Dwork et al., 2015; Bassily et al., 2018; Asadi & Abbe, 2020;
Hafez-Kolahi et al., 2020; Zhou et al., 2020) have also demonstrated great power in analyzing SGD-
like algorithms. For example, Pensia et al. (2018) is the first to utilize information-theoretic bound
in analyzing the generalization ability of SGLD (Gelfand & Mitter, 1991; Welling & Teh, 2011).
The bound was SUbsqUently improved by Negrea et al. (2019); Haghifam et al. (2020); Rodriguez-
Galvez et al. (2020); Wang et al. (202lb). Inspired by the work of Pensia et al. (2018), Neu et al.
(2021) presents an information-theoretic analysis of the models trained with SGD. The analysis of
Neu et al. (2021) constructs an auxiliary weight process parallel to SGD training and upper-bounds
the generalization error through this auxiliary process.
Another line of research connects the generalization of neural networks with the flatness of loss
minima (Hochreiter & Schmidhuber, 1997) found by SGD or its variant (Keskar et al., 2017; Dinh
et al., 2017; Dziugaite & Roy, 2017; Neyshabur et al., 2017; Chaudhari et al., 2017; Jastrzebski
et al., 2017; Jiang et al., 2019; Zheng et al., 2021; Foret et al., 2020). This understanding has led
to the discovery of new SGD-based training algorithms for improved generalization. For example,
in a concurrent development by Zheng et al. (2021) and Foret et al. (2020), a local “max-pooling”
operation is applied to the loss landscape prior to the SGD updates. This approach, referred to as
AMP (Zheng et al., 2021) or SAM (Foret et al., 2020), is shown to make SGD favor flatter minima
and achieve the state-of-the-art performance among various competitive regularization schemes.
1
Published as a conference paper at ICLR 2022
In thiS paper, we focUS on inveStigating the generalization of machine learning modelS trained with
SGD. AlthoUgh we are primarily motivated by the cUrioSity to UnderStanding neUral networkS, the
reSUltS of thiS paper in fact apply broadly to any model trained with SGD.
ThiS work followS the Same conStrUction of the aUxiliary weight proceSS in NeU et al. (2021) and
developS Upper boUndS of generalization error that extend the work of NeU et al. (2021). Like thoSe
in NeU et al. (2021), the boUndS we obtain can be decompoSed into two termS, one meaSUring the
impact of training trajectorieS (“the trajectory term”) and the other meaSUring the impact of the
flatneSS of the foUnd SolUtion (“the flatneSS term1”). Having an identical flatneSS term aS that in NeU
et al. (2021), empirical evidence hintS that oUr boUndS have a improved trajectory term. FigUre 1
ShowS an experimental compariSon of the trajectory term in oUr boUnd (Theorem 2) with that in the
boUnd in NeU et al. (2021) (re-Stated aS Lemma 2 in thiS paper) for two neUral network modelS. The
trajectory termS are compared in a determiniStic Setting for two different valUeS of σ (the variance
parameter of the noiSe in the aUxiliary weight proceSS, detailS given in appropriate context and in
Appendix E.2).
The trajectory term in the boUndS of NeU et al. (2021) accUmUlateS two termS over training StepS:
local gradient sensitivity, meaSUring the SenSitivity of the SGD gradient Signal to weight pertUrba-
tionS, and gradient dispersion2, meaSUring the extent to which the gradient Signal SpreadS aroUnd
itS mean. USUally gradient diSperSion vaniSheS with training iterationS bUt local gradient SenSitivity
doeS not. OUr improvement over NeU et al. (2021) iS achieved by removing the local SenSitivity
term and involving the logarithmic of a reviSed gradient diSperSion. AlthoUgh oUr new definition
of gradient diSperSion iS in general larger than that in NeU et al. (2021), aS long aS the nUmber of
iterationS iS not too Small, the fact that oUr gradient diSperSion alSo vaniSheS with training allowS oUr
boUndS to be tighter than NeU et al. (2021).

.mreT yrotcejarT
0	10	20	30	40	50
epoch
.mreT yrotcejarT
0	20	40	60	80	100
epoch
OUrS
NeUetal.(2021)
0	10	20	30	40	50
epoch
0	20	40	60	80	100
epoch
(a) σ = 10-5 (MNIST)	(b) σ = 10-6 (MNIST) (c) σ = 10-5 (CIFAR10) (d) σ = 10-6 (CIFAR10)
FigUre 1: CompariSon of the trajectory term between oUr boUnd (Theorem 2) and the boUnd in NeU
et al. (2021). (a)(b) MLP trained on MNIST. (c)(d) CNN trained on CIFAR-10.
We alSo provide an application of oUr boUndS in analyzing the generalization behavioUr of linear
and two-layer ReLU networkS, where we Show that the activation State in ReLU networkS playS an
important role in generalization.
It iS remarkable that removing the local gradient SenSitivity term makeS oUr boUnd have a Simple
cloSed form (after optimizing the noiSe varianceS), mUch eaSier to evalUate. We empirically validate
the derived boUndS, and provide varioUS inSightS pertaining to the generalization behavior of modelS
trained with SGD. For example, gradient diSperSion iS Seen to reveal a doUble deScent phenomenon
with reSpect to training epochS, where the valley in the doUble deScent cUrve appearS to mark the
great divide between the “generalization regime” and the “memorization regime” of training. FUr-
thering from thiS obServation, we alSo Show that it iS poSSible to redUce the memorization effect by
dynamically clipping the gradient and redUcing itS diSperSion.
OUr boUndS alSo inSpire a natUral and Simple SolUtion to alleviate generalization error. Specifically,
we propoSe a new training Scheme, referred to aS Gaussian model perturbation (GMP), aiming at
redUcing the flatneSS term of the boUndS. ThiS Scheme effectively applieS a local “average pooling”
to the empirical riSk SUrface prior to SGD, greatly reSembling the “max-pooling” approach adopted
in AMP (Zheng et al., 2021). We demonStrate experimentally that GMP achieveS a competitive
performance with the cUrrent art of regUlarization SchemeS.
1We note thiS term iS correlated with flatneSS rather than preciSely meaSUreS the flatneSS.
2The qUantity iS often referred to aS gradient variance in the literatUre, bUt we prefer “diSperSion” to “vari-
ance” So aS to better comply with the mathematical conventionS and avoid poSSible confUSion.
2
Published as a conference paper at ICLR 2022
Proofs, additional discussions and experimental results are presented in Appendices.
Other Related Literature Gradient dispersion is mostly studied from optimization perspectives
(Bottou et al., 2018; Roux et al., 2012; Johnson & Zhang, 2013; Wen et al., 2020; Faghri et al., 2020).
Prior to this work, only a few works relate gradient dispersion with the generalization behaviour of
the networks. In Neu et al. (2021) and Wang et al. (2021b), gradient dispersion also appears in the
generalization bounds. and there are limited studies characterizing the generalization performance
and the gradient variance. Particularly, in Jiang et al. (2019), gradient dispersion is argued to capture
a notion of “flatness” of the local minima of the loss landscape, thereby correlating with general-
ization. Injecting noise in the training process has been proposed in various regularization schemes
(Bishop, 1995; Camuto et al., 2020; 2021; Srivastava et al., 2014; Wei et al., 2020). But unlike GMP
derived in this paper, where noise is injected to the model parameters, noise in those schemes is
injected either to the training data or to the network activation. In addition, noise can also be im-
plicitly injected during training, for example, by adjusting the momentum terms Xie et al. (2021b).
The training objective in Xie et al. (2021a) is similar to our GMP but their NVRM-SGD minimizes
a region directly while our GMP considers a trade-off between the empirical loss and the flatness
term. Gradient clipping is a common technique for preventing gradient exploding (see, e.g., Merity
et al. (2018); Peters et al. (2018)). This technique is also used in Zhang et al. (2019) to accelerate
training. In this paper, gradient clipping is used to investigate and control the impact of gradient
dispersion on generalization error.
2 Preliminaries
Population Risk, Empirical Risk and Generalization Error Unless otherwise noted, a random
variable will be denoted by a capitalized letter (e.g., Z), and its realization denoted by the corre-
SPonding lower-case letter (e.g. z). Let Z be the instance space of interest and μ be an unknown
distribution on Z, specifying random variable Z. Let W ⊆ Rd be the space of hypotheses. Suppose
that a training sample S = (Z1,Z2,...,Zn) is drawn i.i.d. from μ and that a stochastic learning
algorithm A takes S as its input and outputs a hypothesis W ∈ W according to some conditional
distribution PW|S mapping Zn to W. Let ` : W × Z → R+ be a loss function, where `(w, z) mea-
sures the “unfitness” or “error” of any z ∈ Z with respect to a hypothesis w ∈ W. The population
risk, for any w ∈ W, is defined as
Lμ(w) , EZ〜μ['(w,Z)].
The goal of learning is to find a hypothesis W that minimizes the population risk. But since μ is only
partially accessible via the sample S, in practice, we instead turn to the empirical risk, defined as
1n
LS(W) , n E'(w,Zi).
i=1
The expected generalization error of the learning algorithm A is then defined as
gen(μ, PW |S ) , EW,S [Lμ(W ) - LS (W 儿
where the expectation is taken over thejoint distribution of (S, W) (i.e., μn 0 PW∣s).
Throughout this paper, we take ` as a continuous function (adopting the usual notion “surrogate
loss” (Shalev-Shwartz & Ben-David, 2014) ). Additionally, we assume that ` is differentiable almost
everywhere with respect to both W and z. Furthermore We assume that '(w, Z) is R-SUbgaUSsian3
for any W ∈ W. Note that a bounded loss is guaranteed to be subgaussian. Let I(X; Y ) denote the
mutual information (Cover & Thomas, 2012) between any pair of random variables (X, Y ). The
following result is known.
Lemma 1 (XU & Raginsky (2017, Theorem 1.)). Assume the loss '(w, Z) is R-Subgaussianfor any
W ∈ W. The generalization error of A is bounded by
lgen(μ,pw∣s)| ≤ V ~^I((W; S),
3A random variable X is R-subgaussian if for any ρ, log E exp (P (X - EX)) ≤ ρ2R2∕2.
3
Published as a conference paper at ICLR 2022
Stochastic Gradient Descent We now restrict the learning algorithm A to be the mini-batched
SGD algorithm for empirical risk minimization. For each training epoch, the dataset S is randomly
split into m disjoint mini-batches, each having size b, namely, n = mb. Based on each batch, one
parameter update is performed. Specifically, let Bt denote the batch used for the tth update. Define
g(w, Bt) , 1 X Vw'(w, z),
b
z∈Bt
namely, g(w, Bt) is the average gradient computed for the batch Bt with respect to parameter w.
The rule for the tth parameter update is then
Wt , Wt-1 -λtg(Wt-1,Bt),
where λt is the learning rate at the step t. The initial parameter setting W0 is assumed to be drawn
from the zero-mean spherical Gaussian N (0, σ02Id) with variance σ02 in each dimension. We will as-
sume that the SGD algorithm stops after T updates and outputs WT as the learned model parameter.
Given the training sample S, let ξ govern the randomness in the sequence (B1, B2, . . . , BT ) of
batches. For the simplicity of notion, we will fix the configuration of ξ. That is, we will assume a
fixed “batching trajectory”, or a fixed way to shuffle the example indices {1, . . . , n} and divide them
into m batches in each epoch. The presented generalization bounds of this paper can be extended
to the case where the batching trajectory is uniformly random (as we set up above). This merely
involves averaging over all batching trajectories or taking expectation over ξ .
Auxiliary Weight Process We now associate with the SGD algorithm an auxiliary weight process
{Wt}. Let σ1, σ2, . . . , σT be a sequence of positive real numbers. Define
Wf0 ,W0,	and	Wft	,	Wft-1	-	λtg(Wt-1, Bt) +	Nt,	fort >0,
where Nt 〜N(0, σ2Id) is a Gaussian noise. The relationship between this auxiliary weight process
{Wt} and the weight process {Wt} in SGD is shown in the Bayesian network below.
—■	N1		Nz	∙ ∙			•	NT-ι J —■			NT J
Wf0	→	W1	→	W2	→	∙ ∙	∙	→	WT-1	→	WfT
k	%		%		%			%	
W0	→	W1	→	W2	→	∙ ∙	∙	→	WT-1	→	WT
t
Let ∆t =	τ=1 Nτ . Noting that the weight updates in {Wt } uses the same gradient signal as that
used in {Wt} (which depends on Wt-1 not
77；	、	.	.	.	F. , K ττ, . A ɪɪ ..,.
Wt-1), it is immediate that Wt = Wt + ∆t. Note that
this auxiliary process follows the same construction as Neu et al. (2021), which we will use to study
the generalization error of SGD. To that end, let’s define gradient dispersion by
Vt(W)，E [||g(w, Bt)- EVw'(W, Z)]∣∣2],
(1)
where the inside expectation is taken over (W, Z)〜 μ 0 PW∣z. For a given sample S ∈ Zn, define
γ(w, s) , E[Ls(w + ∆T) - Ls (w)] ,
where the expectation is taken over ∆T and Ls (w) is the empirical risk of s at parameter w.
In the remainder of the paper, let S0 denote another sample drawn from μn, independent of all other
random variables. The main generalization bound in Neu et al. (2021) is re-stated below.
Lemma 2 ((Neu et al., 2021, Theorem 1.)). The generalization error of SGD is upper bounded by
∣gen(μ,PWT∣s)| ≤ 2t
2R2 T λ2
- ^X 2EE [ψ(Wt-I) + Vt(Wt-I)I
n t=1 σt
+ |E [γ(Wτ,S) - Y(WT,S0)] |,
where Ψ(wt-ι)，E 川EVw'(wt-ι, Z)] - EVw'(wt-ι + Z, Z)]||2], Z 〜N(0, Pt=1 蟾Id) and
Vt(w) , E [∣∣g(w,Bt) — E[Vw'(w,Z)]∣∣2].
4
Published as a conference paper at ICLR 2022
The term Ψ(wt-1) in the bound is referred to as “local gradient sensitivity” in Neu et al. (2021).
Note that the inside expectation of Vt(W) is taken over Z 〜 μ instead of (W, Z)〜 μ0PW∣z. Thus,
Vt(w) is in general not worse than our gradient dispersion Vt(w) for a fixed w (see Figure 5 (d) in
Appendix E.2). However, the difference between these two terms is very small when W is close to
local minima due to the tiny gradient norm. In addition, the definition of our gradient dispersion is
implicitly used in a bound in Section 5.2 of Neu et al. (2021), which can be regarded as a weaker
version of Eq. 2 in our Theorem 2.
3 New Generalization Bounds for SGD
Removal of the local sensitivity term Ψ(wt-1) requires invoking a special instance of the HWI
inequality (Raginsky & Sason, 2018, Lemma 3.4.2), which we first state.
Lemma 3. Let X and Y be two random vectors in Rd, and let N 〜N(0, Id) be independent of
(X,Y). Then, for any t,t0 > 0, DKL(PX + √tN 11PY+√tθN ) ≤ 210 E [||X - Y ||2]+ d (log tt + t0-1),
where DKL is the KL divergence.
Note that Neu et al. (2021) also makes uses of a similar lemma in their revision. However, the
development in Neu et al. (2021) requires constructing a ghost auxiliary weight process in which
an independent noise perturbation ∆0t is introduced. Due to the mismatch between ∆t and ∆0t, the
local gradient sensitivity term Ψ(Wt-1) appears in their bound. In this paper, we waive this ∆0t by
using the following lemma.
Lemma 4. Let random variables X,Y and ∆ be independent of N 〜 N(0, Id). Thenfor any
σ > 0, any Rd-ValuedfUnction f, and any random variable Ω ∈ Rd that is afunction of Y, we have
I(f (Y + ∆,X) + σN; X|Y) ≤ dE log (E [||f(Y +/2X)- 0||2] + j] .
Note that the outside expectation is taken over Y and the inside expectation is taken over
(X, ∆). Then, exploiting Lemma 4 by letting X = S, Y = W and Ω = E [g(w — ∆, Z)]，
E [▽'(W — ∆, Z)] will enable us to have the bound below.
Theorem 1.	The generalization error of SGD is upper bounded by
ʌ Rd X E [log ( λ⅛WiBt )-2E[g(WtiZ)]11F + |E [γ(Wτ, S) — γ(Wτ, S0)]∣ .
n t=1	dσt2
This bound is strictly tighter than the bound in Lemma 2. In fact, from Theorem 1, one can recover
Lemma 2 with a smaller constant factor (see Appendix C.3 for more details).
To completely remove Ψ(Wt-1) and to obtain a closed form of the optimal bound, we will let
Ω = E [g(W, Z)], then Lemma 4 gives us a crisp way to have the following upper bound that is
independent of the distribution of ∆.
Lemma 5. Let Gt = —λtg(Wt-ι,Bt) .Then, I(Gt + Nt； SWt-I) ≤ 2 d log (率吆?t-1)] + 1).
In this lemma, the mutual information I(Gt+Nt; S|Wt-1) roughly indicates the degree by which the
SGD’s updating signal Gt (smoothed with noise) depends on the training sample S, when Bt is used
for computing the gradient. When this dependency is strong (giving rise to a high value of the mutual
information), the model conceivably tends to overfit the training sample. This lemma suggests that
the strength of this dependency can be upper-bounded by the expected gradient dispersion at the
current weight configuration. In our experiments, we will estimate the expected gradient dispersion
and validate this intuition.
We are now in a position to state our main theorem.
Theorem 2.	The generalization error of SGD is upper bounded by
∣gen(μ, PWT∣s)| ≤
t RndX log (*一+1)+1E [γ(WT ,S) -γ(WT, S0)]1.⑵
5
Published as a conference paper at ICLR 2022
Further, assume Lμ(wτ) ≤ Eδ [Lμ(wτ + ∆τ)], ' is twice differentiable, and σ2 is independent of
t. Denote by HWT the HeSSian of the loss with respect to WT and let Tr(∙) denote trace. Then
1
3 0 R2λ2T 一	一一	A3
gen(μ,Pwτ∣s) ≤ - I	E [Vt(W1)]E [n(Hwτ (Z))] J .	(3)
Remark 1. With a single draw of S and under the deterministic setting (i.e., with fixed weight
initialization and batching trajectory), removing Ψ(Wt-1) will make the bound much tighter as
shown in Figure 1. This remarkable improvement should come at no surprise, since Ψ(Wt-1)
monotonically increases with training epochs (noting that Ψ(Wt-1) has the cumulative variance
Pit=-11 σi2Id ) while Vt(Wt-1) appears decreasing (see Figure 3a). Figure 1 also indicates that if
the noise variance σt is made small, one expects the gradient dispersion to dominate the trajectory
term in Lemma 2. However, the factor 1∕σ2 will become too large, make the bound loose. More
discussions about the stochastic setting with multiple draws of S are deferred to Appendix E.2.
Remark 2. The condition Lμ(wτ) ≤ E△『[Lμ(wτ + ∆τ)] indicates that the perturbation does
not decrease the population risk. This is also assumed in Foret et al. (2020) in the derivation of a
PAC-Bayesian generalization bound.
In the bound of Eq.2, the first term captures the impact of the training trajectory (“trajectory term”),
and the second term captures the impact of the final solution, which in fact measures the flatness for
the loss landscape at the found solution (“flatness term”). The learning rate in SGD has explicitly
appeared in the trajectory term of Eq.3. From the way it appears in the bound, one may be tempted
to assert that a small learning rate will improve generalization. This would then contradict some
previous observations (Jastrzebski et al., 2017; Wu et al., 2018; He et al., 2019), in which large
learning rate will benefit generalization. In addition, the bound also suggests that the batch size
has some direct impact on the trajectory term through gradient dispersion. We investigate this by
performing experiments with varying learning rates and batch sizes (see Figure 6 in Appendix E.3).
As seen in the experiments, increasing the learning rate impacts the trajectory term and the flatness
term in opposite ways, i.e. making one increase and the other decrease. A similar (but reverted)
behaviour is also observed with batch sizes. This makes the generalization bound in Eq.3, have a
rather complex relationship with the settings of learning rate and batch size.
Eq.3 follows from Eq.2 by minimizing the bound over σ. The bound in Eq.3 is thus independent of
a choice of σjs and can be computed easily and efficiently.
To summarize, we remark that these bounds suggest that in order for the model to generalize well,
both the trajectory term and the flatness term need to be small — the former involves the learning
rate with the gradient dispersion along the training trajectory, whereas the latter depends on the
flatness of the empirical risk surface at the found solution.
4 Application in Linear Networks and Two-Layer ReLU
Networks.
We now apply Theorem 2 to two neural network models in a regression setting. Let Z = (X, Y )
with X ∈ Rd0 and Y ∈ R. Assume ||X|| = 1. We use SGD to train a model f (W, ∙) : Rd0 → R
and define the loss as '(W, Z) = 1∕2(Y - f (W, X))2. The bounds below are both in the form of
the product of the flatness term and the trajectory term.
Theorem 3	(Linear Networks). Let the model be a linear network, i.e. f(W, X) = WTX, then
1
gen(μ, Pwt∣s) ≤ 3 (PT=I ¾tTE ['(Wt-ι, Z)]) 3 .
Theorem 4	(Two-Layer ReLU Networks). Following Arora et al. (2019), consider f(W, X) =
√m Em=I ArReLU(WrrX) where m is the width ofthe neural network, Ar 〜unif({+1, —1}) and
ReLU(∙) is the ReLU activation. We only train the first layer parameters W = [Wι, W2,..., Wm ] ∈
Rd0 ×m and fix the second layer parameters A = [A1, A2, . . . , Am] ∈ Rm during training. Then,
1
gen(μ,pwτ∣s) ≤ 3 (XE [T] X E X ⅛t'(Wt-1,Z)]) ɜ ,
6
Published as a conference paper at ICLR 2022
where Ir,i,t = I{WtT-1,r Xi ≥ 0} and I is the indicator function.
Compared with Theorem 3, we notice that in the two-layer ReLU network, the ReLU activation
state along the training trajectory plays a key role in the bound of Theorem 4. Specifically, the
weights of the deactivated neurons do not contribute to the bound of Theorem 4, making the bound
not explicitly depend on the model dimension d. This result also suggests that sparsely activated
ReLU networks are expected to generalize better. Despite various empirical evidence pointing to
this behaviour (see, e.g., Glorot et al. (2011)), to the best of our knowledge, this theorem provides
the first theoretical justification in this regard.
The comparison between these bounds and the dynamics of generalization gap is deferred to Ap-
pendix E.4. In the remainder of this paper, we will study more complex network architectures
beyond the linear and two-layer ReLU networks and implications of our bounds in those settings.
5 Experimental Study
Bound Verification We first verify our bound of Eq.3 in Theorem 2 by training an MLP (with
one hidden layer) and an AlexNet (Krizhevsky et al., 2012) on MNIST and CIFAR10 (Krizhevsky,
2009), respectively. To simplify estimation, we fix the weight initialization and set σt and λt to
be constants σ and λ, respectively. To compute PtT=1 E [Vt(Wt-1)], we compute the gradient
dispersion as its empirical estimate from a batch, utilizing a PyTorch (Paszke et al., 2019) library
BackPack (Dangel et al., 2020). To compute Tr (E [HWT (Z)]), we use the PyHessian library (Yao
et al., 2020) to compute the Hessian.
We perform experiments with varying network width and varying levels of label noise. Specifically,
noise level refers to the setting where we replace the labels of fraction of the training and testing
instances with random labels. The estimated bound is compared against the true generalization gap,
namely, the difference between the training loss and testing loss, and is shown in Figure 2.
0	100	200	300	400	500
width
102
101
100
10-1
(a)	MLP on MNIST
103
102
101
I gap I
bound
10	20	30	40	50	60
filters
(c) MLP on MNIST
I gap I
bound
103
102
101
0	0.1	0.2	0.3	0.4	0.5	0.6
label noise level
(b)	AlexNet on CIFAR10
(d) AlexNet on CIFAR10
Figure 2:	Estimated bound and empirical generalization gap (“gap”) as functions of network width
((a) and (b)) and label noise level ((c) and (d)). Note that Y-axis is in log scale.
In Figure 2, we see that in all cases the estimated bound follows closely the trend of the true gen-
eralization gap. The fact that the bound curve consistently tracks the gap curve under various label
noise levels indicates that our bound very well captures the changes of the data distribution. Note
that in Figure 2 (a) and (b), our bound decays with the increase of the model size, showing a trend as
opposite to the bounds obtained in classical learning theory. But such a trend clearly better explains
the generalization behaviour of modern neural networks.
Epoch-wise Double Descent of Gradient Dispersion We experimentally investigate the impact
of gradient dispersion on the training of the neural networks by fixing the learning rate, batch size
and weight initialization for the each model (MLP for MNIST, AlexNet for CIFAR10). For each
model and various label noise levels, we plot in Figure 3 the evolution of the (empirical) gradient
dispersion Vt(wt-1), training accuracy and testing accuracy across training epochs.
An intriguing epoch-wise “double descent” phenomenon is observed, particularly when the labels
are noisy. According to the double descent curve, the training may be split into three phases (e.g.,
Figure 3 (h)). In the first phase, the gradient dispersion rapidly descends and maintains a very
low level. In this phase, both training and test accuracies increase while maintaining a very small
generalization gap. This suggests that the network in this phase is extracting useful patterns and
generalizes well. In the second phase, the gradient dispersion starts increasing until it reaches a
7
Published as a conference paper at ICLR 2022
peak value. In this phase, the training and testing accuracies gradually diverge, marking the model
entering an overfitting or “memorization” regime - when the data contains the noisy labels, the
network mostly tries to memorize the labels in the training set. In the third phase, the gradient
dispersion descends again, reaching a low value. In this phase, the model continuously overfits the
training data, until the training and testing curves reach their respective maximum and minimum.
It appears that the timing of the three phases depends on the dataset and the label noise level. For
simpler data (e.g. MNIST) and cleaner datasets , the first phase may be shorter. This is arguably
because in these datasets, extracting useful patterns is relatively easier. Nonetheless, the valley in the
double-descent curve appears to mark a “great divide” between generalization and memorization.
(a) noise=0 (MNIST)
100
80
60
40
20
0
(e) noise=0 (CIFAR10)
(b) noise=0.2 (MNIST)
(c) noise=0.4 (MNIST)
(d) noise=0.6 (MNIST)
•104
(f) noise=0.2 (CIFAR10) (g) noise=0.4 (CIFAR10) (h) noise=0.6 (CIFAR10)
Figure 3:	Dynamics of gradient dispersion, in relation to training/testing accuracies.
Dynamic Gradient Clipping Inspired by our generalization bounds and above observations, one
way to reduce the generalization error is to control the trajectory term of the bounds by reducing the
gradient dispersion in each training step. Here we investigate a simple scheme that dynamically clips
the gradient norm so as to reduce the gradient dispersion. Specifically, whenever the current gradient
norm is larger than the gradient norm K steps earlier, or ||g(Wt, Bt)||2 > ||g(Wt-K, Bt-K)||2 (i.e.,
the model is expected to have entered the “memorization” regime), we reduce the norm of the current
gradient g(Wt, Bt) to α fraction of ||g(Wt-K, Bt-K)||2, for some prescribed value α < 1.
The effectiveness of this scheme is best demon-
strated when the labels contain noise. As shown
in Figure 4 (and Figure 9 in Appendix E.5), dy-
namic gradient clipping significantly closes the
gap between the training accuracy and the testing
accuracy. The models trained with this scheme
maintain a near-optimal testing accuracy (e.g.,
about 80% when the label noise level of MNIST
is 0.2), without suffering from the severe memo-
rization effect as seen in models trained without
this scheme. Further understanding of the double-
descent phenomenon of the gradient dispersion
(a) noise=0.2 (MNIST) (b) noise=0.4 (MNIST)
Figure 4: Dynamic Gradient Clipping.
may enable more delicate design of such a dynamic clipping scheme and potentially lead to novel
and powerful regularization techniques.
6 A Practical Implication: Gaus s ian Model Perturbation
The appearance of the flatness term in our generalization bounds suggests that for an empirical
risk minimizer w* to generalize well, it is necessary that the empirical risk surface at w* is flat,
or insensitive to a small perturbation of w*. This naturally motivates a training scheme using the
8
Published as a conference paper at ICLR 2022
following regularized loss:
mwin Ls (w) + ρ E,σ	[Ls (w + ∆) - Ls (w)],
where ρ is a hyper-parameter. Replacing the expectation above with its stochastic approximation
using k realizations of ∆ gives rise to the following optimization problem.
mwnb X ((1- ρ)'(w, z) + P1 X ('(w + δi, z))).
z∈B
i=1
We refer to the SGD training scheme using this loss as Gaussian model perturbation or GMP. No-
tably, GMP requires k + 1 forward passes for every parameter update. Empirical evidence shows
that a small k, for example, k = 3, already gives competitive performance. Implementing the k + 1
forward passes on parallel processors further reduces the computation load.
Method	SVHN	CIFAR-10	CIFAR-100
ERM	96.86±0.060	93.68±0.193	72.16±0.297
Dropout	97.04±0.049	93.78±0.147	72.28±0.337
L.S.	96.93±0.070	93.71±0.158	72.51±0.179
Flooding	96.85±0.085	93.74±0.145	72.07±0.271
MixUP	96.91±0.057	94.52±0.112	73.19±0.254
Adv. Tr.	97.06±0.091	93.51±0.130	70.88±0.145
AMP	97.27±0.015	94.35±0.147	74.40±0.168
GMP3	97.18±0.057	94.33±0.094	74.45±0.256
GMP10	97.09±0.068	94.45±0.158	75.09±0285
Table 1: Top-1 classification accuracy acc.(%) of VGG16.
We run experiments 10 times and report the mean and the
standard deviation of the testing accuracy. Superscript de-
notes the number of sampled Gaussian noises during train-
ing.
We compare GMP with several ma-
jor regularization schemes in the cur-
rent art, including Dropout (Srivas-
tava et al., 2014), label smoothing
(L.S.) (Szegedy et al., 2016), Flood-
ing (Ishida et al., 2020), MixUp
(Zhang et al., 2018), adversarial
training (Adv. Tr.) (Goodfellow
et al., 2015), and AMP (Zheng et al.,
2021). We will also include ERM
as the baseline, where no regular-
ization method applied. The com-
pared schemes are evaluated on three
popular benchmark image classifica-
tion datasets SVHN (Netzer et al.,
2011), CIFAR-10 and CIFAR-100
(Krizhevsky, 2009). Two represen-
tative deep architectures VGG16 (Si-
monyan & Zisserman, 2015) and Pre-
ActResNet18 (He et al., 2016) are
taken as the underlying model. We train the models for 200 epochs by SGD. The learning rate
is initialized as 0.1 and divided by 10 after 100 and 150 epochs. For all compared models, the batch
size is set to 50 and weight decay is set to 10-4. For GMP, we choose ρ = 0.5 and set the standard
deviation of the Gaussian noise ∆ to 0.03. The value ofkis chosen as 3 and 10 respectively (referred
to as GMP3 and GMP7 * * 10).
The performances of all compared schemes are given in Table 1 (and the results of PreActResNet18
are shown in Appendix E.7). For the compared regularization schemes except GMP, we directly
report their performances as given in Zheng et al. (2021). Table 1 demonstrates the effectiveness
of GMP. Overall GMP performs comparably to the current art of regularization schemes, although
appearing slighly inferior to the most recent record given by AMP on SVHN and MixUp on CIFAR-
10, respectively (Zheng et al., 2021). Noting that the key ingredient of AMP, “max-pooling” in the
parameter space, greatly resembles regularization term in GMP, which may be seen as “average-
pooling” in the same space.
7 Conclusion and Outlook
This paper presents new information theoretic generalization bounds for models (e.g., linear net-
works and two-layer ReLU neural networks) trained with SGD. Our bounds naturally point to new
and effective regularization schemes. At the same time, our bounds and experimental study reveal
interesting phenomena in the SGD training of neural networks. There are yet promising directions
for further improving these bounds, for example, via exploiting conditional mutual information
bounds (Haghifam et al. (2020)), strong data processing inequalities (Wang et al. (2021a)), and the
relationship between between the trajectory term and the flatness term.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is supported partly by an NSERC Discovery Grant and Artificial Intelligence for Design
Challenge program of National Research Council Canada. The authors would like to sincerely
thank Gergely NeU and Borja RodrIgUez-Galvez for pointing out errors in the previous version of
this paper. The authors would also like to thank Maia Fraser for stimulating discussions.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263. PMLR, 2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Amir R Asadi and Emmanuel Abbe. Chaining meets chain rule: Multilevel entropic regularization
and training of neural networks. Journal of Machine Learning Research, 21(139):1-32, 2020.
Amir R Asadi, Emmanuel Abbe, and Sergio VerdU. Chaining mutual information and tightening
generalization bounds. In Proceedings of the 32nd International Conference on Neural Informa-
tion Processing Systems, pp. 7245-7254, 2018.
Peter L Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6241-6250, 2017.
Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use
little information. In Algorithmic Learning Theory, pp. 25-55. PMLR, 2018.
RaefBassily, Vitaly Feldman, CriStObal Guzman, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33,
2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1):108-116, 1995.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. The Journal ofMachine Learn-
ing Research, 2:499-526, 2002.
Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli. Tightening mutual information-based
bounds on generalization error. IEEE Journal on Selected Areas in Information Theory, 1(1):
121-130, 2020.
Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J. Roberts, and Chris C. Holmes.
Explicit regularisation in gaussian noise injections. In Advances in Neural Information Processing
Systems, 2020.
Alexander Camuto, Xiaoyu Wang, Lingjiong Zhu, Chris Holmes, Mert Gurbuzbalaban, and Umut
Simsekli. Asymmetric heavy tails and implicit bias in gaussian noise injections. arXiv preprint
arXiv:2102.07006, 2021.
10
Published as a conference paper at ICLR 2022
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gra-
dient descent into wide valleys. In 5th International Conference on Learning Representations.
OpenReview.net, 2017.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.
Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.
Felix Dangel, Frederik Kunstner, and Philipp Hennig. Backpack: Packing more into backprop. In
8th International Conference on Learning Representations. OpenReview.net, 2020.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In International Conference on Machine Learning, pp.1019-1028. PMLR, 2017.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth.
Generalization in adaptive data analysis and holdout reuse. In Proceedings of the 28th Interna-
tional Conference on Neural Information Processing Systems-Volume 2, pp. 2350-2358, 2015.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2017.
Fartash Faghri, David Duvenaud, David J Fleet, and Jimmy Ba. A study of gradient variance in deep
learning. arXiv preprint arXiv:2007.04532, 2020.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable al-
gorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279. PMLR,
2019.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Saul B Gelfand and Sanjoy K Mitter. Recursive stochastic algorithms for global optimization in r^d.
SIAM Journal on Control and Optimization, 29(5):999-1018, 1991.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In 14th
International Conference on Artificial Intelligence and Statistics, volume 15, pp. 315-323, 2011.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd International Conference on Learning Representations, ICLR, 2015.
Dongning Guo, Shlomo Shamai, and Sergio Verdu. Mutual information and minimum mean-square
error in gaussian channels. IEEE transactions on information theory, 51(4):1261-1282, 2005.
Hassan Hafez-Kolahi, Zeinab Golgooni, Shohreh Kasaei, and Mahdieh Soleymani. Conditioning
and processing: Techniques to improve information-theoretic generalization bounds. Advances in
Neural Information Processing Systems, 33, 2020.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. arXiv preprint arXiv:2004.12983, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR,
2016.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems
32, pp. 1141-1150, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645, 2016.
11
Published as a conference paper at ICLR 2022
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need
zero training loss after achieving zero training error? In Proceedings of the 37th International
Conference on Machine Learning, ICML, 2020.
Stanislaw Jastrzebski, Zachary Kenton, Devansh ArPit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, DiliP Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2019.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using Predictive variance
reduction. Advances in neural information processing systems, 26:315-323, 2013.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deeP learning: Generalization gaP and sharP minima. In 5th
International Conference on Learning Representations. OPenReview.net, 2017.
Alex Krizhevsky. Learning multiPle layers of features from tiny images. Technical rePort, University
of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deeP con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, PP. 5809-5819. PMLR,
2020.
Ben London. A Pac-bayesian analysis of randomized learning with aPPlication to stochastic gradient
descent. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, PP. 2935-2944, 2017.
StePhen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and oPtimizing lstm lan-
guage models. In International Conference on Learning Representations, 2018.
Preetum Nakkiran, Gal KaPlun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. DeeP
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2019.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dePendent estimates. In Advances
in Neural Information Processing Systems, PP. 11013-11023, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Read-
ing digits in natural images with unsuPervised feature learning. In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.
Gergely Neu, Gintare Karolina Dziugaite, Mahdi Haghifam, and Daniel M Roy. Information-
theoretic generalization bounds for stochastic gradient descent. In COLT, 2021.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based caPacity control in neural
networks. In Conference on Learning Theory, PP. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh BhojanaPalli, David McAllester, and Nati Srebro. ExPloring general-
ization in deeP learning. In Advances in Neural Information Processing Systems, PP. 5947-5956,
2017.
Behnam Neyshabur, Srinadh BhojanaPalli, and Nathan Srebro. A Pac-bayesian aPProach to
sPectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018a.
12
Published as a conference paper at ICLR 2022
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations, 2018b.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026-8037, 2019.
Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-
rithms. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 546-550. IEEE,
2018.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, 2018.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. Lecture Notes for 6.441
(MIT), ECE 563 (UIUC), STAT 364 (Yale), 2019., 2019.
Maxim Raginsky and Igal Sason. Concentration of measure inequalities in information theory, com-
munications and coding. Foundations and Trends in Communications and Information Theory;
NOW Publishers: Boston, MA, USA, 2018.
Borja Rodriguez-Galvez, German Bassi, Ragnar Thobaben, and MikaeI Skoglund. On random
subset generalization error bounds and the stochastic gradient langevin dynamics algorithm. arXiv
preprint arXiv:2010.10994, 2020.
Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponen-
tial convergence rate for finite training sets. In Proceedings of the 25th International Conference
on Neural Information Processing Systems-Volume 2, pp. 2663-2671, 2012.
Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory.
In Artificial Intelligence and Statistics, pp. 1232-1240, 2016.
Daniel Russo and James Zou. How much does your data exploration overfit? controlling bias via
information usage. IEEE Transactions on Information Theory, 66(1):302-323, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In 3rd International Conference on Learning Representations, ICLR, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, volume 125 of Proceedings of Machine Learning
Research, pp. 3437-3452. PMLR, 2020.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, CVPR, pp. 2818-2826, 2016.
Vladimir Vapnik. Statistical learning theory. Wiley, 1998. ISBN 978-0-471-03003-4.
Hao Wang, Yizhe Huang, Rui Gao, and Flavio Calmon. Analyzing the generalization capability of
sgld using properties of gaussian channels. Advances in Neural Information Processing Systems,
34, 2021a.
13
Published as a conference paper at ICLR 2022
Hao Wang, Yizhe Huang, Rui Gao, and Flavio P Calmon. Learning while dissipating information:
Understanding the generalization capability of sgld. arXiv preprint arXiv:2102.02976, 2021b.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of
dropout. In International Conference on Machine Learning,pp. 10181-10192. PMLR, 2020.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. An
empirical study of stochastic gradient descent with structured covariance noise. In International
Conference on Artificial Intelligence and Statistics, pp. 3621-3631. PMLR, 2020.
Lei Wu, Chao Ma, andE Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. Advances in Neural Information Processing Systems, 2018:
8279-8288, 2018.
Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, and Masashi Sugiyama. Arti-
ficial neural variability for deep learning: on overfitting, noise memorization, and catastrophic
forgetting. Neural computation, 33(8):2163-2192, 2021a.
Zeke Xie, Li Yuan, Zhanxing Zhu, and Masashi Sugiyama. Positive-negative momentum: Ma-
nipulating stochastic gradient noise to improve generalization. In International Conference on
Machine Learning, pp. 11448-11458. PMLR, 2021b.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. Advances in Neural Information Processing Systems, 2017:2525-2534, 2017.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-
off for generalization of neural networks. In International Conference on Machine Learning, pp.
10767-10777. PMLR, 2020.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581-590. IEEE, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations. OpenReview.net, 2017.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In 6th International Conference on Learning Representations, ICLR,
2018.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning Rep-
resentations, 2019.
Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. In CVPR, 2021.
Ruida Zhou, Chao Tian, and Tie Liu. Individually conditional individual mutual information bound
on generalization error. arXiv preprint arXiv:2012.09922, 2020.
14
Published as a conference paper at ICLR 2022
Appendix
Table of Contents
A HWI Inequality: Proof of Lemma 3	15
B A Key Lemma: Lemma 4	16
C Mutual Information Bounds for SGD: Proof of Theorem 1 and Theorem 2	17
C.1 Lemma 6 ............................................................... 17
C.2 Proof of Theorem 1 .................................................... 18
C.3 Corollary 1: Recovering Lemma 2 ....................................... 19
C.4 Proof of Lemma 5 ...................................................... 20
C.5 Proof of Theorem 2 .................................................... 20
C.6 Corollary 2 ........................................................... 21
D Application in Neural Networks: Proof of Theorems 3 and 4	21
D.1 Lemma 7 ............................................................... 21
D.2 Proof of Theorem 3 .................................................... 22
D.3 Proof of Theorem 4 .................................................... 22
E Experiment Details	23
E.1 Architectures and Hyperparameters ..................................... 23
E.2 More Experiments on the Comparison Between Theorem 2 and Lemma 2 ...... 23
E.3 Learning Rate and Batch Size........................................... 24
E.4 Implementation of Theorem 3 and Theorem 4 ............................. 25
E.5 Algorithm of Dynamic Gradient Clipping and Additional Results ......... 25
E.6 Discussion on Gradient Dispersion of Models Trained on Clean Datasets . 25
E.7 Algorithm of Gaussian Model Perturbation and Additional Results ....... 26
A HWI Inequality: Proof of Lemma 3
The proof given here is a simple extension of the proof of Raginsky & Sason (2018, Lemma 3.4.2),
which is a special instance of the (weak) HWI inequality.
Proof.
DKL(PX + √tN||PY+√t0N) ≤dKL(PX,Y,X+√tN||PX,Y,Y +√t0N)
=XEY [DKL(PX+√tN|X,Y||PY+√t0N|X,Y)]
= E [DKL(N (X, tId)||N (Y, t0Id))]
X,Y
=2T0 Eγ IjIX - Y||2] + 2(log 7 + 70 - 1),
2t X,Y	2 t t
(1)
(2)
(3)
where Eq.1 is by the chain rule of the KL divergence, Eq.2 holds since N is independent of (X, Y)
and Eq.3 is by the equality
DKL(N(μi, 二)||N(M2, 夕2)) = 2
log ∣2∙- d + Tr (ς-E)+ M - μ1)τ"2 -M
15
Published as a conference paper at ICLR 2022
Eq.3 holds for any joint distribution of (X, Y ). If we let t0 = t, by the definition of Wasserstein
distance, we have
DKL(Pχ+√tNIIPY+√tN) ≤ 21tW2(PX,PY).
This recovers the vector version of Raginsky & Sason (2018, Lemma 3.4.2).	□
B A Key Lemma: Lemma 4
Proof. We give two proofs of this lemma, the first proof is inspired by the techniques used in Wang
et al. (2021a) and Guo et al. (2005), and the second proof is similar to Pensia et al. (2018).
1. Let Z = Y + ∆. By the properties of mutual information,
I (f (Z, X) + σN; X ∣Y = y) = I (f(Z, X) - Ω + σN; X ∣Y = y).
Let g(Z, X)，f(Z, X) - Ω. Let PN0 = N(0, σ02Id) for any σ0 > 0 and N0 is indepen-
dent of other random variables for a fixed y . Then, for Y = y,
I (g(Z,X ) + σN; XIY = y) =E [DKL (Pg(Z,x) + σN ∣X=x,Y=y "Pg(Z,X) + σN |Y=y )]
=E [DKL (Pg(Z,x) + σN ∣X=x,Y=y ||PN0 |Y=y) - DKL (Pg(Z,X)+σN |Y=y ||PN 0∣Y=y)]
X
(4)
≤ infc E [DKL (Pg(Z,x) + σN ∣X=x,Y=y "Pσ0N |Y =y)]	(5)
σ0>0 X
= Jnf0与E h∆ [||g(y + Ax*1x = χ,γ = y]i + d(log W +102 -1)
σ >0 σ	σ σ
(6)
d1
= 2log
E	IIg(y+∆,X)II2IY=y
X,∆
dσ2
+1 ,
where Eq. 4 is the golden formula (see Theorem 3.3 in Polyanskiy & Wu (2019) ), Eq. 5
is by the non-negativity of relative entropy, Eq. 6 is due to Lemma 3 and the last equality
holds when the optimal
σ02
E IIg(y+∆,X)II2IY=y
XA___________________
d
+ σ2
is achieved.
Thus,
I(f(Z,X)+N;XIY) =YE[I(f(Z,X)+N;XIY=y)]
≤E
Y
d1
2lθg
E IIg(y+∆,X)II2IY=y
X,∆
dσ2
+1
d1
2lθg
E
Y
E [IIf(Z,X) - Ω∣∣2∣Y = y]
X,∆
dσ2
+1
which completes the proof.
16
Published as a conference paper at ICLR 2022
2. Alternatively, let h(∙) denote the differential entropy, then
I(f (Z, X) + σN; X|Y = y)= I(g(Z, X) + σN; X|Y = y)
=h(g(Z, X)	+ σN|Y =	y)	-	h(g(Z, X)	+ σN|X, Y = y)
≤ h(g(Z, X)	+ σN| Y =	y)	-	h(g(Z, X)	+ σN∣∆, X,Y =	y)
(7)
=h(g(Z, X) + σN|Y = y) - h(σN)
=h(g(Z,X) + σN|Y = y) - d log 2πeσ2,
where Eq. 7 is due to the fact that conditioning reduces entropy.
Notice that
E	川g(y + ∆,X) + σN||2|Y = y] = E 川g(y + ∆,X)∣∣2∣Y = y] + dσ2.
∆,X,N	∆,X
Since the Gaussian distribution maximizes the entropy over all distributions with the same
variance (Cover & Thomas, 2012), we have
ʌEJ|g(y + ∆,X )∣∣2∣Y = y] + dσ2
δ,x_________________________
d
h(g(Z, X) + σN| Y = y) ≤ d log 2πe
Therefore,
d	∆EX ||g(y + ∆, X)||2|Y = y +dσ2 d
I(f(Z, X) + σN; X|Y = y) ≤ 2 log 2πe ---------------d------------------2 log 2∏eσ2
d1
= 2log
E [||g(y + ∆,X) ||2| Y = y]
δ,x
dσ2
The remaining part is straightforward and is the same with the previous proof.
□
C Mutual Information Bounds for SGD: Proof of Theorem 1 and
Theorem 2
In this section, we provide a proof for Theorem 1 and Theorem 2, while elaborating on why the local
gradient sensitivity term in Lemma 2 can be removed. The key ingredient that makes this possible
is a construction given in the proof of Lemma 5.
C.1 Lemma 6
We first give the following useful lemma.
Lemma 6. The mutual information I(WfT; S) ≤ PtT=1 I -λt g (Wt-1, Bt) + Nt; S|Wft-1 .
Proof. The mutual information between the final output of SGD and the training sample can be
upper bounded by the mutual information between the full trajectories and the training sample,
17
Published as a conference paper at ICLR 2022
which is shown below.
I(WT； S)
=I (WT-ι — λτg(Wτ-1, BT) + NT; S)
≤I(fτ-1, —Atg(WT-i,Bt) + NT； S)	(8)
=IMT-i； S) + I (—Atg(Wτ-ι, BT) + Nτ; SIWT-1)	(9)
≤I(Wτ-2； S) + I (—At-Ig(WT-2, BT-i) + NT-i; SIWT-2) +1 (—Atg(Wτ-i, BT) + NT； SIWT-i
(10)
T
≤I(W0； S) + X I(—λtg(Wt-i,Bt)+ Nt； SIWt-I) ,	(11)
t=i
T
=XI (—λtg(Wt-i,Bt) + Nt; SIWt-I) ,	(12)
t=i
where Eq.8 is by I(f(X,Y); Z) ≤ I(X,Y; Z) (i.e. Z — (X, Y) — f (X,Y) forms a Markov chain
and then use the data processing inequality), Eq.9 is by the chain rule of mutual information, Eq.10
is by applying the similar procedure (namely, Eq.8-Eq.9) to I(WT -i； S), Eq. 11 is by doing these
steps recursively and Eq.12 is due to the fact that W° is independent of S (i.e. I(W0; S) = 0). □
C.2 Proof OF Theorem 1
Proof. We first follow the similar decomposition of the expected generalization error and apply
Lemma 1 as in Neu et al. (2021),
Igen(μ,Pwτ∣s)I = gen(μ,Pw	)+ E [l“(Wt) — L“(Wt)] + 叩 E JLS(wt) — LS(WT)]
1 1	Wτ ,∆τ L	」 Wt ,△7,S L	」
/2 R2 ~ 一	.......................
≤ \ ----i(WT； S)+	E /Y(WT,S) - Y(WT,s )] .	(13)
V n	Wt ,s,s0
The remaining task is to bound the mutual information I(WT； S).
Let X = S, Y = Wt-I and Ω = E	[g(泊t-i 一 ∆t-i, Z)], by applying Lemma 4, we have
∆t-ι,Z
I —Atg(Wt-1, Bt) + Nt； SIWt-i
≤E
-—
ft-1
d1
2lθg
(At	E I"
S,∆t-1
-i — ∆t-i,Bt) — ʌ E	[g(泊t-i — ∆t-i,Z)]II2IWt-i
∆t-ι,Z
dσ2
Wt-i
∖
By Lemma 6 and putting everything together, we have
Igen(μ,Pwτ ∣s )I
≤
\
R2 d 晨	lrπ
——>	E
n匕丽-1
log
(a葭 E	IIg(Wt-i,Bt) — E [g(Wt-i,Z)]II2
Sqt-I L	z,δ⅛-i
dσ2
\
+ 1 I + IE[γ(Wτ,S) — Y(WT,S0)]I .
J∖
This completes the proof.
□
18
Published as a conference paper at ICLR 2022
C.3 Corollary 1: Recovering Lemma 2
We can recover the result of Lemma 2 in Neu et al. (2021) from Theorem 1, which is re-stated in the
following corollary.
Corollary 1. The generalization error ofSGD is upper bounded by
∣gen(μ,Pwτ∣s)| ≤
∖
2 R2 * * 工 λ?	r	~	i
-n- X λ WEι[3ψ(W"+2v t(W"]
t=1 t
+ ∣E[γ(Wτ,s) - Y(WT,S0)] ∣.
Proof. From the first term in Theorem 1, We have
E	[g(ft-1 - ∆t-1,
ZAt-I L
(16)
≤∖
IR X σt WtEIJ2	IIg(WiBt)- e	[gw-，Z )]∣∣	+2∣∣E	[g(fiZ)]-
t=ι t	L
≤∖
BR X σ⅛ WtEi」21 I g(Wti Bt)- E [g(fiZ )]∣ I 2+2△上 J I I e [g(fτ,Z)]- E [g(fι-
t=1 t	L	L
(17)
∖
R2 工 λ2 Γ i i	一. 一 一. 一 r ~
—σWtEI s [2 1 1 S(Wt-i, Bt)- E [g(Wt-i,Z)] + E [g(Wt-i,Z)] - E 4(ft-i,
2Ψ(Wt-i)
≤∖
今 X σ⅛W产1 S [4 1 1 g(Wt-i，Bt)- E[g(Wt-i,Z)]| I +4 1 I E[g(Wt-i, Z)] - E [g(ft-ι,Z)]1 1 + 2W(Wt-i)
t=1 t t- , L	-
(18)

R2 工 λ2	「ii .	一.	一 ii 2
-R X σW产1 S [4 1 1 g(Wt-1，Bt)- E [g(Wt-LZ)][ 1 + 6ψ(wt-1)

2 R2 WT^ λ2	「	〜	i
-n- X λ WEJ 3ψ(w" + 2V t(Wt-1)],
t=1 t
19
Published as a conference paper at ICLR 2022
where Eq. 14 and 17 are by Jensen’s inequality (i.e. the concavity of logarithm and the convexity of
squared norm), Eq. 15 is by log(x + 1) ≤ x, and Eq. 16 and 18 are by ||x + y||2 ≤ 2∣∣x∣∣2 + 2∣∣y∣∣2.
This completes the proof.	□
C.4 Proof of Lemma 5
Proof. We first notice that
I(Gt + Nt； S∣ft-ι) = I(-λtg(Wt-ι, Bt) + σtN; S∣f1),
where N 〜N(0, Id).
Then let X = S, Y = f t-ι and Ω = E	[▽'(Wt-1, Z)], by applying Lemma 4, We have
Wt-1,Z
I(-λtg(Wt-1, Bt) + σtN; S|Wt-1)
≤ d E
2
-—
Wt-1
log
(λt	E	||g(Wt-i,Bt) - E	[Vw'(Wt-ι,Z)]||2
S,∆t-1 |_	Wt-ι,Z
dσ2
∖
/λ E [Vt(Wt-ι)]	∖
≤ gw I Wt-I dσ2	+1)，
where the second inequality is by Jensen’s inequality.
This completes the proof.	□
C.5 Proof of Theorem 2
Proof. Applying Lemma 6 and Lemma 5 and putting everything together, we have
∣gen(μ,Pwt∣s)| ≤
R2 d
—Uog
n
t=1
[Vt(Wt-1)]
dσ2
+ |E [γ(Wτ, S) - Y(WT,S0)] ].
∖
Next, to handle the mismatch between the outputs of perturbed SGD and SGD, we apply Taylor
expansion around ∆T = ~0,
E	[LS (WT + ∆T) - LS (WT)]
WT ,S,∆T
1n
-V E ʌ ['(Wt + ∆τ ,Zi)- '(Wτ ,Zi)]
n	WT ,Zi ,∆T
i=1
≈ E	[hVw'(Wt, Z), ∆τi + -∆THWT(Z)∆τ
WT ,Z,∆T	2
=W EA [-∆THWT(Z)△」	(19)
WT ,Z,∆T 2
=-hWE /Hwτ(Z)], ZE 3吁]i
2 WT ,Z	∆T
1T
2 hwE,z [Hwτ(Z)], diag(X6
(20)
P⅛f2 Tr( E [Hwt(Z)]),
2	WT ,Z
where Eq.19 is by the zero mean of the perturbation, Eq.20 is by the independence of the coordinates
of ∆τ, h∙, ∙i denotes the inner product of two matrices, diag(A) is the diagonal matrix with element
A and Tr(∙) is the trace of a matrix.
20
Published as a conference paper at ICLR 2022
Under the condition WE5 [γ(Wτ, S0)] ≥ 0, We now bound gen(μ, PfT∣s) instead of its absolute
value, ∣gen(μ, PfT∣s)|. With the inequality log(x + 1) ≤ x, the following is straightforward,
R2 工 λ2 一. 一	....................
gen(μ, PfTIS) ≤t 募 E σ2WEJV(Wt-1)] + WT”, MW?, S)-Y(WT,S0)]
t=1 t t-1	T, ,
R2 T λ
≤t 募 X σ2W-IMWtT)]+WE mW?, S)]
t=1 t
=t n X If WLMWt-a+PT2d2 3wE,z [Hwτ(Z)]).
Since every choice of σ gives a valid generalization bound. The optimal bound can be found by
simply utilizing the fact A∕σ + σ2B ≥ 3(A∕2)2∕3B1∕3 for any positive A and B, where the equality
is achieved by the optimal σ. Finally, rearranging the terms will complete the proof.	□
C.6 Corollary 2
Corollary 2. If the loss function is differentiable and β-smooth with respect to w, then,
R2d T~/ λ2W]E [Vt(Wt-l)]~Y	T
∣gen(μ,PWτ∣s)| ≤t ~n~ X log ( WtT dσ2-----+ 1J + βd X σ2∙
Proof. Recall the smoothness implies f (v) ≤ f (w) +(▽/(w), V - Wi + 2 ||v - w||2 forall V and
w. By the triangle inequality, we have
|E [Lμ(Wτ) - Lμ(Wτ + ∆τ)]| ≤ ∣E[(Vw'(Wτ,Z), ∆?)] | + 2E [∣∣∆?||2] = Bd飞=σ2
Thus, we can see that |E [Lμ(Wτ) - Lμ(WT + ∆?)] | + |E [Ls(WT + ∆?) - LS(WT)] | ≤
βdPtT=1 σt2.
This completes the proof.	□
Remark 3. In Corollary 2, we note that the dependence of d in the bound results from the spher-
ical Gaussian noise used in the construction of the weight process WT. It is possible to replace
the spherical Gaussian with a Gaussian noise having a non-diagonal covariance that reflects the
geometry of the loss landscape. With this replacement, the dimension d in the flatness term will be
replaced by the trace of PtT=1 κt, where κt is the covariance matrix of the noise added at step t.
Please refer to Neu et al. (2021) for a similar development.
D Application in Neural Networks: Proof of Theorems 3 and 4
D.1 Lemma 7
Lemma 7. Under the same conditions of Theorem 2, the generalization error of SGD is upper
bounded by
gen(μ,PWτ∣s) ≤ 3 (X R2λTE [∣∣g(Wt-ι,Bt)∣∣2]E [T-(Hwt(Z))]
1
3
21
Published as a conference paper at ICLR 2022
Proof. The proof of this lemma follows the same steps in the proof of Theorem 2 except that we
require a different use of Lemma 4 here.
Specially, we let Ω = 0 in Lemma 4. The remaining steps are the same in the proof of Theorem 2
and should be straightforward.
□
Remark 4. The bound in Lemma 7 is weaker than Eq. 3 in Theorem 2 as the centralized expected
vector norm should be smaller than the original expected vector norm.
D.2 Proof of Theorem 3
Proof. By CaUchy-SchWarz inequality, We have
E [∣∣g(Wt-1,Bt)∣∣2] = E Ilb X Vw'(Wt-ι,Zi)
Zi∈Bt
2
≤ E 川Vw'(Wt-1 ,Z)∣∣2].	(21)
2
Notice that Vw'(W, Z) = (WTX - Y)X. Let Y = f (W, X). Then,
l|vw'(Wt-1,z)∣∣2 = ∣∣(Wt-1X - Y)x∣∣2 ≤ ∣∣Wt-1X - y∣∣2 = (Y - Y)2 = 2'(Wt-1, z).
(22)
For the Hessian matrix, it’s easy to see that Tr(HWT (Z)) = 1.
Plugging everything into Lemma 7, we have
gen(μ, Pwt∣s) ≤ 3
1
R4F WEι,z ['(WiZ)I
This completes the proof.
□
D.3 Proof of Theorem 4
Proof. Since Vwr'(W, Zi) = √mAr (f (W, Xi) - YOXiIr,i, where Ir,i = I{WTXi ≥ 0}, we have
∣∣vw'(Wt-1,z )||2
m1
==Xl ll√mArf(Wt-1, Xi)- YoXiIr,i||2
m
≤ ^X ll√mAr (f (Wt-1, Xi)- Yi)Ir,i∣∣2
r=1
1m
=m X Ir,i||(f (Wt-1,Xi ) - Yi)Il2
r=1
1m
=m X 2Ir,i'(Wt-1, Zi).
r=1
(23)
(24)
In addition, we notice that Tr(HWT (Z)) = mm Pm=I Iri,T. Plugging everything into Lemma 7, we
have
gen(μ, Pwt∣s) ≤ 3
R2λ2T E
4nm Wt-1,Zi
m
X Ir,i,t'(Wt-1,Zi)
r=1
E
WT,Zi
This completes the proof.
1
X Ir,i,T #!.
□
22
Published as a conference paper at ICLR 2022
E Experiment Details
E.1 Architectures and Hyperparameters
In Section 5, MLP has one hidden layer with 512 hidden units, and AlexNet has five convolution
layers (conv. 3 × 3 (64 filters) → max-pool 3 × 3 → conv. 5 × 5 (192 filters) → max-pool 3 × 3 →
conv. 3 × 3 (384 filters) → conv. 3 × 3 (256 filters) → conv. 3 × 3 (256 filters) → max-pool
3 × 3) followed by two fully connected layers both with 4096 units and a 10-way linear layer as
the output layer. All of the convolution layers and the fully connected layers use standard rectified
linear activation functions (ReLU).
The fixed learning rates used for MLP and AlexNet are 0.01 and 0.001, respectively. The batch size
is set to 60. For the corrupted label experiment, we train the models until the models achieve 100%
training accuracy. For other cases, we train the neural networks until the training loss converges
(e.g., < 0.0001). Other settings are either described in Section 5 or apparent in the figures. Standard
techniques such as weight decay and batch normalization are not used.
To choose the variance proxy R, We first collected all the per-instance losses '(Wt-ι, Zi) that were
observed during training, then we let R = (maxi,t'(Wt-ι,Zi) - mini,t'(Wt-ι, Zi))∕2 in our
experiments.
In Section 6, we compare GMP with other advanced regularization methods. The results of other
methods are reported directly from Zheng et al. (2021), and we now give their hyperparameter
settings here for completeness. For Dropout, 10% of neurons are randomly selected to be deactivated
in each layer. For label smoothing, the coefficient is 0.2. For flooding, the level is set to 0.02. For
MixUp, we lineally combine random pairs of training data where the coefficient is drawn from
Beta(1, 1). For adversarial training, the perturbation size is 1 for each pixel and we take one step to
generate adversarial examples. For AMP, the number of inner iteration is 1, and the L2 norm ball
radius values are 0.5 for PreActResNet18 and 0.1 for VGG16, respectively.
The implementation in this paper is on PyTorch, and all the experiments are carried out on NVIDIA
Tesla V100 GPUs (32 GB).
E.2 More Experiments on the Comparison Between Theorem 2 and Lemma 2
. Thm 2	---*---Lem 2	...∙  Vt -------"---Vt
(a) σ = 1e-2	(b) σ = 1e-4
0	20	40	60	80	100
iterations
(C) σ = 1e-6
0	20	40	60	80	100
iterations
(d) Vt and Vt
αOISJ9dsIα ∙pmθ
0	20	40	60	80	100	0	20	40	60	80	100
iterations	iterations
Figure 5: Comparison Between Theorem 2 and Lemma 2 in stochastic setting.
23
Published as a conference paper at ICLR 2022
In Figure 1, we compare
.2 PT=IE [ψ(Wt-ι)+ Vt(Wt-ι)]
and ∖∣n pT=ι e [Vt(Wt-ι)] for
two different values of σ. Notice that we indeed use a weaker version of the trajectory term in
Theorem 2, and the same constants like R, λt and σt in the two bounds are ignored here. We
choose the full dataset of MNIST and CIFAR10 to train the models with the fixed initialization.
We also fix the sampling of batches, making the training completely deterministic. To estimate the
local gradient sensitivity term, we randomly sample 20 Gaussian noises from N (0, Ptτ-=11 στ) to
perturb the model parameters and compute the average perturbed gradient. With this single draw
and deterministic setting, the estimated Vt and Vt are the same, so the bound of Eq. 2 in Theorem
2 should be smaller than the bound in Lemma 2, as shown in Figure 1.
We also provide the experiments under the stochastic setting. Specially, We randomly choose 1/10
of the MNIST data and train the MLP model with a fixed learning rate and batch size. To estimate
E	[V'(Wt-ι,Z)], We conduct 20 runs with different random seeds and save W after every
Wt-1,Z
iteration. In Figure 5 (d), we can see that at the beginning of training, our gradient dispersion Vt
is much larger than Vt in Neu et al. (2021), but in the later training phase, the magnitude of gap
between these two terms is very small. This is because the gradient norm will become tiny when
W is near local minima. In Figure 5 (a-c), we compare
n4 PT=1 λ2e hψ(wt-ι) + ;Vt(Wt-ι)i
and
4* PT=Ilog (λ2EVd(σWt-1) "J under three different settings of σ. When σ is small (e.g.,
σ = 1e-6), the local gradient sensitivity term will become small, but the factor 1∕σ2 will be very
large, making the gap between log(x + 1) and x be extremely large, as shown in Figure 5 (c). In this
case, the improvement upon Neu et al. (2021) is significant. When σ is large (e.g., σ = 1e-2), at
the beginning of training, the trajectory term in Lemma 2 will be smaller than the trajectory term in
Theorem 2 since our Vt is relatively large. However, since Ψ(Wt-1) has the cumulative variance,
and Vt and Vt become closer soon or later, the bound in Lemma 2 will be greater than the bound in
Theorem 2 in the later training phase.
E.3 Learning Rate and Batch Size.
The learning rate and batch size have some impact on Eq. 3 in Theorem 2. We now investigate
this by performing experiments with varying learning rates and batch sizes. In our experiments,
the model is continuously updated until the average training loss drops below 0.0001. We separate
trajectory and flatness terms of the bound and plot them in Figure 6.
∙10-2
8642
0. 0. 0. 0.
mret yrotcejarT
20	40	60	80	100	120
batch size
(a) MLP on MNIST
lr=0.0005
T- lr=0.001
一 lr=0.005
∙10-2
.5
2.
.5 1
mret yrotcejarT
50	100	150	200	250
batch size
(c) AlexNet on CIFAR10
50	100	150	200	250
batch size
(d) AlexNet on CIFAR10
♦ lr=0.001
-lr=0.005
--lr=0.01
mret ssentalF
20	40	60	80	100	120
batch size
(b) MLP on MNIST
Figure 6: The impact of learning rate and batch size on the trajectory term and the flatness term in
Eq. 3
A key observation in Figure 6 is that the learning rate impacts the trajectory term and the flatness
term in opposite ways, as seen, for example, in (a) and (b), where the two set of curves swap
their orders in the two figures. On the other hand, the batch size also impacts the two terms in
opposite ways, as seen in (a) and (b) where curves decrease in (a) but increase in (b). This makes
the generalization bound, i.e., the sum of the two terms, have a rather complex relationship with the
settings of learning rate and batch size. This relationship is further complicated by the fact that a
small learning rate requires a longer training time, or a larger number T of training iterations, which
increases the number that are summed over in the trajectory term. Nonetheless, we do observe that
a smaller batch size gives a lower value of the flatness term ((b) and (d)), confirming the previous
wisdom that small batch sizes enable the neural network to find a flat minima (Keskar et al., 2017).
24
Published as a conference paper at ICLR 2022
10^2
IOT
IO-4
IO-5
10^6
IO-7
IO-8
O	IOOO 2000	30∞	4000	5000
Figure 7: Generalization gap of a linear network v.s. Theorem 3. Note y-axe is log-scale.
Figure 8: Generalization gap of a two-layer ReLU network v.s. Theorem 4.
E.4 Implementation of Theorem 3 and Theorem 4
We let d0 = 200 and use a two-layer ReLU network with hidden units 10000 to generate Y . More-
over, we apply a tanh function to the output of this network so that |Y | ≤ 1. The input data
X 〜N(0,I) and We normalize X before training so that ||X|| = 1. For the training phase, We
choose m = 100, let data size be 20, 000, batch size be 100 and learning rate be 0.5. Figure 7 and
Figure 8 compare the empirical generalization gap With the bound in Theorem 3 and that in Theorem
4, respectively.
E.5 Algorithm of Dynamic Gradient Clipping and Additional Results
The dynamic gradient clipping algorithm is described in Algorithm 1. For both MLP and AlexNet,
We let α = 0.1. The start step for clipping, Tc, is also an important hyperparameter. HoWever, it can
be removed by detecting the evolution of the average gradient norm for each epoch. Specifically,
Whenever the average gradient norm of epoch j is larger than the average gradient norm of epoch
j - 1, the clipping operation begins.
From Figure 4 and Figure 9 We can see that dynamic gradient clipping effectively alleviates overfit-
ting by conspicuously sloWing doWn the transition of training to the memorization regime, Without
changing the convergence speed of testing accuracy. Unfortunately, the current design of the dy-
namic gradient clipping algorithm does not provide a significant improvement for models trained on
a clean dataset (Without label noise). Designing better regularization algorithms may require under-
standing the epoch-Wise double descent curve of gradient dispersion Where the model is trained on
a clean dataset.
E.6 Discussion on Gradient Dispersion of Models Trained on Clean Datasets
In the case of no noise injected, Figure 3a shoWs that the model With good generalization property
has a exponentially-decaying gradient dispersion. This is consistent With our discussion of Lemma
5 in Section 3, that is, small I(Gt + Nt; S|Wt-1) indicates good generalization. Notably, gradient
dispersion of AlexNet trained on the real CIFAR10 data still has a epoch-Wise double descent curve.
25
Published as a conference paper at ICLR 2022
Algorithm 1 Dynamic Gradient Clipping
Require: Training set S, Batch size b, Loss function `, Initial model parameter w0 , Learning rate
λ, Initial minimum gradient norm G, Number of iterations T , Clipping parameter α, Clipping
step Tc
1:	for t — 1 to T do
2:	Sample B = {zi}ib=1 from training set S
3:	Compute gradient:
gB J Pb=I Vw'(wt-l, Zi)Ib
4:	if t > Tc then
5:	if ||gB||2 > G then
6:	gB - α ∙ G ∙ gB∣∣∣gBI∣2
7:	else
8:	G	J	||gB ||2
9:	end	if
10:	end if
11:	Update parameter: Wt J wt-ι 一 λ ∙ gB
12:	end for
Figure 9: Dynamic Gradient Clipping (AlexNet).
(b) noise=0.4 (CIFAR10)
The difference between Figure 3e with Figure 3f-3h is that the testing accuracy does not decrease
in the second phase/memorization regime for AlexNet trained on the true CIFAR10 data. Loosely
speaking, we conjugate that memorizing random labels will hurt the performance on unseen clean
data but memorizing clean (or true) labels will not. This may explain why dynamic gradient clipping
or preventing the training entering the memorization regime cannot improve the performance on a
clean dataset.
E.7 Algorithm of Gaussian Model Perturbation and Additional Results
Algorithm 2 Gaussian Model Perturbation Training
Require: Training set S, Batch size b, Loss function `, Initial model parameter w0 , Learning rate
λ, Number of noise k, Standard deviation of Gaussian distribution σ, Lagrange multiplier ρ
while wt not converged do
2: Update iteration: t J t + 1
Sample B = {zi}ib=1 from training set S
4: Sample ∆j 〜N(0, σ* 2 * 4 * 6) for j ∈ [k]
Compute gradient:
gB J Pb=I(Vw'(wt,zi)+ P Pk=ι (Vw'(wt + ∆j,zi) — Vw'(wt, Ziy) ∣k /b
6: Update parameter: wt+ι J Wt 一 λ ∙ gB
end while
26
Published as a conference paper at ICLR 2022
Method	SVHN	CIFAR-10	CIFAR-100
ERM	97.05±0.063	94.98±0.212	75.69±0.303
Dropout	97.20±0.065	95.14±0.148	75.52±0.351
L.S.	97.22±0.087	95.15±0.115	77.93±0.256
Flooding	97.16±0.047	95.03±0.082	75.50±0.234
MixUp	97.26±0.044	95.91±0.117	78.22±0.210
Adv. Tr.	97.23±0.080	95.01±0.085	74.77±0.229
AMP	97.70±0.025	96.03±0.091	78.49±0.308
GMP3	97.43±0.037	95.64±0.053	78.05±0.208
GMP10	97.34±0.058	95.71±0.073	78.07±0.170
Table 2: Top-1 classification accuracy acc.(%) of PreActResNet18. We run experiments 10 times
and report the mean and the standard deviation of the testing accuracy. Superscript denotes the
number of sampled Gaussian noises during training.
Method	I SVHN	I CIFAR-10	I CIFAR-100
GMP3abs	I 97.10±0.054	I 94.21±0.139	I 74.80±0.113
Table 3: Top-1 classification accuracy acc.(%) of VGG16.
The GMP algorithm is given in Algorithm 2. Table 1 and Table 2 show that our method is com-
petitive to the state-of-the-art regularization techniques. Specifically, our method achieves the best
performance on SVHN for both models and on CIFAR-100 where VGG16 is employed. Particu-
larly, testing accuracy is improved by nealy 2% on the CIFAR-100 dataset with VGG16. For other
tasks, GMP is always able to achieve the top-3 performance. In addition, we find that increasing the
number of sampled noises does not guarantee the improvement of testing accuracy and may even
degrade the performance on some datasets (e.g., SVHN). This hints that we can use small number
of noises to reduce the running time without losing performance. Moreover, we observe that GMP
with k = 3 usually takes around 1.76× that of ERM training time, which is affordable.
For PreActResNet18, the performance of GMP appears slighly inferior to the most recent record
given by AMP (Zheng et al., 2021). Noting that the key ingredient of AMP, “max-pooling” in the
parameter space, greatly resembles regularization term in GMP, which may be seen as “average-
pooling” in the same space.
One potential extension of GMP is to let the variance of the noise distribution be a function of the
iteration step t. In other words, using the time-dependent σt instead of a constant σ.
We also consider the following regularized scheme, which is a absolute value version.
mwn Ls(W)+ρ∆ 〜N E,σ2L) h ILs(W+∆) -Ls(W) I i.
This scheme can still perform well as shown in Table 3. In fact, it outperforms GMP3 on CIFAR-
100. This hints that it is possible to improve the performance by choosing other norm of Ls (W +
∆) - Ls(W).
27