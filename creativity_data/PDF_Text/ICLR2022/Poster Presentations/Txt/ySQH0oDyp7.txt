Published as a conference paper at ICLR 2022
QDrop: randomly dropping quantization for
extremely low-bit post-training quantization
Xiuying Wei1,2*, Ruihao Gong1，" Yuhang Li2, Xianglong Liu里 FengWei Yu2
1 State Key Lab of Software Development Environment, Beihang University, 2SenseTime Research
{weixiuying,gongruihao,liyuhang1}@sensetime.com,xlliu@buaa.edu.cn
Ab stract
Recently, post-training quantization (PTQ) has driven much attention to produce
efficient neural networks without long-time retraining. Despite its low cost, cur-
rent PTQ works tend to fail under the extremely low-bit setting. In this study,
we pioneeringly confirm that properly incorporating activation quantization into
the PTQ reconstruction benefits the final accuracy. To deeply understand the in-
herent reason, a theoretical framework is established, indicating that the flatness
of the optimized low-bit model on calibration and test data is crucial. Based on
the conclusion, a simple yet effective approach dubbed as QDrop is proposed,
which randomly drops the quantization of activations during PTQ. Extensive ex-
periments on various tasks including computer vision (image classification, ob-
ject detection) and natural language processing (text classification and question
answering) prove its superiority. With QDrop, the limit of PTQ is pushed to
the 2-bit activation for the first time and the accuracy boost can be up to 51.49%.
Without bells and whistles, QDrop establishes a new state of the art for PTQ. Our
code is available at https://github.com/wimh966/QDrop and has been
integrated into MQBench (https://github.com/ModelTC/MQBench).
1	Introduction
In recent years, deep learning has been applied to all walks of life and offered substantial con-
venience for people’s production and activities. While the performance of deep neural networks
continues to increase, the memory and computation cost also scale up fastly and bring new chal-
lenges for edge devices. Model compression techniques such as network pruning (Han et al., 2015),
distillation (Hinton et al., 2015), network quantization (Jacob et al., 2018) and neural architecture
search (Zoph & Le, 2016) etc., are dedicated to reduce calculation and storage overhead. In this pa-
per, we study quantization which adopts low-bit representation for weights and activations to enable
fixed-point computation and less memory space.
Based on the cost of a quantization algorithm, researchers usually divide the quantization work into
two categories: (1) Quantization-Aware Training (QAT) and (2) Post-Training Quantization (PTQ).
QAT finetunes a pre-trained model by leveraging the whole dataset and GPU effort. On the contrary,
PTQ demands much less computation to obtain a quantized model since it does not require end-to-
end training. Therefore, much attention has recently been paid to PTQ (Cai et al., 2020; Wang et al.,
2020; Hubara et al., 2021; Banner et al., 2019; Nahshan et al., 2019; Zhang et al., 2021a; Li et al.,
2021c) due to its low cost and easy-to-use characteristics in practice.
Traditionally, PTQ pursues accuracy by performing the rounding-to-nearest operation, which fo-
cuses on minimizing the distance from the full-precision (FP) model in parameter space. In recent
progress, Nagel et al. (2020); Li et al. (2021a) considered minimizing the distance in model space,
i.e. the final loss objective. They use Taylor Expansion to analyze the change of loss value and
derive a method to reconstruct the pre-trained model’s feature by learning the rounding scheme.
Such methods are efficient and effective in 4-bit quantization and can even push the limit of weight
quantization to 2-bit. However, the extremely low-bit activation quantization, which faces more
challenges, still fails to achieve satisfactory accuracy. We argue that one key reason is that existing
* Equal contribution,区 Corresponding author.
1
Published as a conference paper at ICLR 2022
theoretical analyses only model the weight quantization as perturbation while ignoring activation’s.
This will lead to the same optimized model no matter which bit the activations use, which is obvi-
ously counter-intuitive and thus causes a sub-optimal solution.
In this work, the effect of activation quantization in PTQ is deeply investigated for the first time.
We empirically observe that perceiving the activation quantization benefits the extremely low-bit
PTQ reconstruction and surprisingly find that only partial activation quantization is more preferable.
An intuitive understanding is that incorporating activation will lead to a different optimized weight.
Inspired by this, we conduct theoretical studies on how activation quantization affects the weight
tuning, and the conclusion is that involving activation quantization into the reconstruction helps the
flatness of model on calibration data and dropping partial quantization contributes to the flatness on
test data. Motivated by both empirical and theoretical findings, we propose QDrop that randomly
drops quantization during the PTQ reconstruction to pursue the flatness from a general perspec-
tive. With this simple and effective approach, we set up a new state of the art for PTQ on various
tasks including image classification, object detection for computer vision, and text classification and
question answering for natural language processing.
To this end, this paper makes the following contributions:
1.	We confirm the benefits unprecedentedly from involving activation quantization in the PTQ re-
construction and surprisingly observe that partial involvement of activation quantization performs
better than the whole.
2.	A theoretical framework is established to deeply analyze the influence of incorporating activation
quantization into weight tuning. Using this framework, we conclude that the flatness of the
optimized low-bit model on calibration data and test data is crucial for the final accuracy.
3.	Based on the empirical and theoretical analyses, we propose a simple yet effective method
QDrop that achieves the flatness from a general perspective. QDrop is easy to implement and
can consistently boost existing methods as a plug-and-play module for various neural networks
including CNNs like ResNets and Transformers like BERT.
4.	Extensive experiments on a large variety of tasks and models prove that our method set up a new
state of the art for PTQ. With QDrop, the 2-bit post-training quantization becomes possible for
the first time.
2	Preliminaries
Basic Notations. Throughout this paper, matrices (or tensors) are marked as X, whereas the vectors
are denoted by x. Sometimes we use w to represent the flattened version of the weight matrix
W. Operator ∙ is marked as scalar multiplication, Θ is marked as element-wise multiplication for
matrices or vectors. For matrix multiplication, we denote Wx as matrix-vector multiplication or
WX as matrix-matrix multiplication.
For a feedforward neural network with activation function, we denote it as G(w, x) and the loss
function as L(w, x), where x and w are the network inputs and weights, respectively. Note that we
assume X is sampled from training dataset Dt, thus the final loss is denoted by Ex〜Dt [L(w, x)].
For the network forward function, we can write it as:
z('+1) = X Wij) ∙ aj'), f(z尸)= a('+1),	⑴
j
where Wi,j denotes weight connecting the jth activation neuron and the ith output. The bracket
superscript (') is the layer index. f (∙) indicates the activation function.
Post-training Quantization. Uniform quantizer maps continuous values x ∈ R into fixed-point
integers. For instance, the activation quantization function can be written as X= [S] ∙ s, where
[.] denotes the rounding-to-nearest operator, S is the step size between two subsequent quantization
levels. While rounding-to-nearest operation minimizes the mean squared error between X and x, the
minimization of parameter space certainly cannot equal to the minimization in final task loss (Li
et al., 2021a), i.e., Ex〜d/L(W, x)]. However, in the post-training setting, we only have a tiny
subset Dc ∈ Dt that only contains 1k images. Thus, it is hard to minimize the final loss objective
with limited data.
2
Published as a conference paper at ICLR 2022
Recently, a series of works (Nagel et al., 2020; Li et al., 2021a) learn to either round up or down and
view the new rounding mechanism as weight perturbation, i.e., W = W + ∆w. Take a pre-trained
network G as an example, they leverage Taylor Expansion to analyze the target, which reveals the
quantization interactions among weights:
minE [L(W, x) — L(w, x)] ≈ minE [∆w>Hw∆w ,	(2)
W	W 2
where Hw = Ex〜Dt VWL(w, x) is the expected second-order derivative. The above objective
could be transformed into the change of output weighted by the output Hessian.
minE ∆w>HW∆w ≈ minE ∆a>Ha∆a	(3)
W	W
About the above minimization, they finetune only the weight by reconstructing each block/layer
output (See Fig. 1). But they did not explore the activation quantization during output reconstruc-
tion with only modeling weight quantization as noise. The step size for activation quantization is
determined after the reconstruction stage.
Intuitively, when quantizing the activations of a full-precision model to 2-bit or 3-bit, there should be
different suitable weights. However, the existing works result in the same optimized weight due to
the neglect of activation quantization. Therefore, we argue that when quantizing the neural network,
the noise caused by activation quantization should be considered coherently with weights.
3	Methodology
In this section, to reveal the influence of introducing activation quantization before output recon-
struction, we first conduct empirical experiments and present two observations. Then a theoretical
framework is built to investigate how the activation quantization affects the optimized weights. Last,
equipped with the analysis conclusions, a simple yet effective method dubbed QDrop is proposed.
Case 1 I	block 1	―►…—►	block k-1 	►	r	I block k l	I
				
Case 2 ∣	block 1	f…一	block k-1 	►	f	-l block k ⅛	ɪ
				
Case 3	∣	block 1	—►…一►	block k-1 	►	block k i	>
Figure 1: 3 cases to involve activation quantization when optimiz-
ing the kth block’s weight rounding. Activations are quantized
inside the blue block and not quantized inside the orange block.
Case	1	2	3
ResNet-18	18.88	45.74	48.07
ResNet-50	4.34	46.98	49.07
MobileNetV2	5.83	50.71	51.20
RegNet-600MF	42.77	60.94	62.07
MnasNet	26.62	58.79	60.19
Table 1: 2-bit or 3-bit post-training quanti-
zation accuracy on ImageNet dataset across
different cases and different models.
3.1	Empirical Observations
To investigate the influence of activation quantization when reconstructing the layer/block output,
we conduct preliminary experiments on the ImageNet (Russakovsky et al., 2015) dataset. Our exper-
iments are based on the open-sourced code Li et al. (2021a) except that we will introduce activation
quantization from 1 to k — 1 blocks before the kth block’s reconstruction. We give a simple visual-
ization in Fig. 1 to show 3 cases for putting activation quantization in different stages. Case 1 means
that all activations are kept in 32-bit full-precision during the reconstruction of block output, which
is also adopted in existing work Nagel et al. (2020); Li et al. (2021a). Case 2 and Case 3 are used
for incorporating activation quantization into the reconstruction stage. However, Case 3 will omit
the current block’s quantization while Case 2 will not. The detailed results of these three cases are
listed in Table 1 (Comparisons on 2-bit (W2A2) quantization for ResNet-18, ResNet-50 and W3A3
for others for the sake of the crashed results on 2-bit.) and the algorithm is put in algorithm 2.
According to the comparison, we can obtain two observations:
1.	For extremely low-bit quantization (e.g., W2A2), there will be huge accuracy improvement when
considering activation quantization during weight tuning. This is confirmed by comparing with
Case 1 and Case 2. We find Case 1 barely converges while Case 2 achieves good accuracy. It
reveals that a separate optimization of weights and activations cannot find an optimal solution.
After introducing the activation quantization, the weights will learn to diminish the influence of
activation quantization.
3
Published as a conference paper at ICLR 2022
2.	Partially introducing block-wise activation quantization surpasses introducing the whole activa-
tion quantization. Case 3 does not quantize the activations inside the current tuning block but
achieves better results than Case 2. This inspires us that how much activation quantization we
introduce for weight tuning will affect the final accuracy.
3.2	How does activation quantization affect weight tuning
The empirical observations have highlighted the importance of activation quantization during the
PTQ pipeline. To further explore how activation quantization will affect the weight tuning, we build
a theoretical framework that analyzes the final loss objective with both weights and activations being
quantized, which presents clues of high accuracy for extremely low-bit post-training quantization.
Conventionally, the activation quantization could be modeled as injecting some form of noise im-
posed on the full-precision counterpart, defined as e = (a 一 a). To remove the influence of activation
range on e, We translate the noise into a multiplicative form, i.e., ^ = a ∙ (1 + u), where the range
of u is affected by bit-width and rounding error. Detailed illustration of the new form noise can be
found in Appendix A.
Here, 1 + u(x) is adopted to present the activation noise since it is related to specific input data point
x. Equipped with the noise, we add another argument in calculating the loss function and define our
optimization objective in PTQ as:
minEx〜Dc [L(w + ∆w, x, 1 + U(X)) — L(w, x, 1)].	(4)
w
We hereby use a transformation that can absorb the noise on activation and transfer to weight, where
the perturbation on weight is denoted as 1 + v(x) (V (x) is used in matrix multiplication format).
Consider a simple matrix-vector multiplication Wa in forward pass, we have W (a(1+u(x))) =
(W (1 + V (x)))a, given by
W(a
-1 + uι(x)-
1 + u2(X) ) = (W Θ
...
1 + un(x)
1 + uι(χ)
1 + uι(x)
...
1 + u1(x)
1 + u2 (x)
1 + u2 (x)
1 + u2 (x)
1 + Un(x)"
1 + Un(X)
1 + un(x)
)a.	(5)
By taking Vi,j (X) = Uj (X), quantization noise on the activation vector (1 + U(X)) can be trans-
planted into perturbation on weight (1 + v(X)). Note that for a specific input data point X, there are
two distinct U(X) and v(X). Proof is available at Sec. B.1.
Also note that for a convolutional layer, we cannot apply such transformation since the input to
convolution is a matrix and will cause different V . Nonetheless, we can give a formal lemma that
absorbs U(X) and holds corresponding v(X) (See the Appendix Sec. B.2 for rigorous proof):
Lemma 1. For a quantized (convolutional) neural network, the influence of activation quantization
on the final loss objective in post-training quantization can be transformed into weight perturbation.
Ex〜Dc[L(W, x, 1 + U(X)) — L(w, x, 1)] ≈ Ex〜dc[L(W Θ (1 + v(χ)), x, 1) — L(w, x, 1)] (6)
By interpolating L(W, x, 1) into Lemma 1, we can obtain the final theorem:
Theorem 1. For a neural network G with quantized weight W and activation perturbation 1 + u(x),
we have:
Ex〜Dc [L(W, x, 1 + u(x)) — L(w, x, 1)] ≈
Ex〜Dc [(L(W, x, 1) — L(w, x, 1)) + (L(W Θ (1 + v(x)), x, 1) — L(W, x, 1))].	(7)
`------------{z-----------} `---------------------------------------}
(7-1)	(7-2)
Here, Theorem 1 divides optimization objective into two terms. Term (7-1) is the same as Eq. (2)
explored in (Nagel et al., 2020; Li et al., 2021a), which reveals how weight quantization interacts
with loss function. Term (7-2) is the additional loss change by introducing activation quantization.
In another way to interpret Eq. (7), the term (7-2) stands for the loss change with jitters on the weight
quantized network G(W, x). This type of noise correlates with certain kinds of robustness.
4
Published as a conference paper at ICLR 2022
Figure 2: Measure sharpness on different data distributions among three cases. We adopt the measurement
defined in (Keskar et al., 2016). With the same degree of loss change ratio, those who can tolerate a larger
perturbation magnitude enjoy a flatter loss landscape.
As stated in some works about generalization and flatness (Dinh et al., 2017; Hochreiter & Schmid-
huber, 1997), intuitively, flat minimum means relatively small loss change under perturbation in
the parameters, otherwise, the minimum is sharp. In this paper, we follow the notion of flatness
defined in (Neyshabur et al., 2017), which considers loss change from the perspective of statistical
expectation. And as (Neyshabur et al., 2017) and (Jiang et al., 2019) refer to, we consider the mag-
nitude of the perturbation with respect to the magnitude of parameters and take the formulation as
Ev 〜D [L(fwθ(i+v)) 一 L(fw)], where each element of V is a random variable sampled from a noise
distribution D and L represents for optimization objective on the training set. From this perspective,
the term (7-2) can be interpreted as the flatness with perturbation related to input data, and thereby
we could achieve the following corollary.
Corollary 1. On calibration data x, with activation quantization noise u(x), there exists the corre-
sponding weight perturbation v(x) which satisfies that the trained quantized model is flatter under
the perturbation v(x).
With Corollary 1, Case 2 and 3 discussed in Sec. 3.1 enjoy a flatter loss landscape benefited from
perceiving the activation quantization. This explains their superiority compared with Case 1. The
measurement of sharpness on calibration data (left part) in Fig. 2 further validates this point. With
similar perturbation magnitude, Case 2 and 3 suffer less loss degradation than Case 1.
3.3 QDROP
As aforementioned, introducing activation quantization is theoretically proved to produce a flatter
model than existing works and the directions of flatness depend on the data distribution. Since the
PTQ is especially sensitive to calibration data (Yu et al., 2021), we need to transfer the investigations
in Sec. 3.2 on calibration data into the test setting for a thorough understanding. In specific, we
consider Eq. (7) on test set and inspect two terms separately in the following. Based on the analyses,
our method QDrop will be derived to pursue an excellent performance on test data.
Term (7-1) on test set. As suggested in Sec. 3.2, with both quantized activations and weights,
we additionally optimize the term (7-2) representing the flatness on calibration data. This
term will encourage the quantized model to learn a flat minimum. As a result, the tradi-
tional objective of AdaRound (term (7-1)) can naturally generalize better for test data (i.e.,
Ex〜Dt(L(W, x, 1) ― L(w, x, 1))).
Term (7-2) on test set. Furthermore, we should also concern about the term (7-2) on test data,
i.e. Ex〜Dt [L(W Θ (1 + v(x)), x, 1) 一 L(W, x, 1)]. As revealed in Sec. 3.2, the term (7-2) implies
the flatness where its situation on calibration data has been exploited. Here, we further investigate
the flatness on test samples. Note that v(x) is converted from u(x) and this activation quantization
noise varies with input data. Fig. 2 shows that there is a gap between the test data and calibration data
for the flatness of the 3 cases. According to Corollary 1, these 3 cases actually introduce different u
mathematically and thus will result in different flatness directions, given by
a	a — 1, blockι 〜blockk-ι
Case 1: U = 0; Case2: U =	— 1; Case 3: U = a a	.	(8)
a	0,	blockk
5
Published as a conference paper at ICLR 2022
Algorithm 1: QDROP in one block for one batch
Input: the kth block from layer i to layer j, a minibatch of quantized block input ai-1, FP32
block input ai-1, FP32 block output aj, quantization dropping probability p.
{1. Forward propagation:}
During training phase, substitute ai-1 with corresponding ai-1 at neuron-level with
probability P and mark the replaced input as ai-1 ;
for l = i to j do
al — W Ia l-1;
al J QUantize(al);
During training phase, randomly drop some al with al as defined in Eq. (9) and get al ;
{2. Backward propagation:}
Compute ∆aj = aj 一 aj;
Tune the weight by gradient descent ;
return Quantized block ;
For Case 1, there is no activation quantization during calibration without taking flatness into account.
Case 2 suggests the activation perturbation totally and therefore enjoys a good flatness on calibration
data. However, due to the mismatch on calibration data and test one, it is highly possible that Case
2 causes overfitting (See Table 8 for more details). Case 3, in fact achieves the best performance by
dropping some activation quantization along with a little different weight perturbation and might not
be restricted to flatness on calibration data (More evidence can be found in Table 9). This inspires us
to pursue a flat minimum from a general perspective, that only optimizing the target on calibration
set is suboptimal to test set.
QDrop. Inspired by this, we propose QDROP to further increase the flatness on as many directions
as possible. In particular, we randomly disable and enable the quantization of the activation each
forward pass:
QDROP : U = < 0^
a 一 ɪ
with probability p
with probability 1 一 p
(9)
We name it QDrop because it randomly drops the quantization of activation. Theoretically, by
masking some u(x) randomly, QDROP can have more diverse v(x) and cover more directions of
flatness thus flatter on test samples, which contributes to the final high accuracy. Fig. 3 support our
analysis where QDrop has smoother loss landscape than Case 3, the winner among 3 cases on test
data. Meanwhile, it is indeed a fine-grained version of Case 3 since Case 3 drops the quantization in
a block-wise manner, whereas our QDrop operates in an element-wise way.
Discussions. QDROP can be viewed as a generalized form of the existing schemes. Case 1 and 2
respectively corresponds to the dropping probability of p = 1 and p = 0. Case 3 is equivalent to
setting the block being optimized with dropping probability p = 1 and remains the quantization of
(a) Case 1	(b) Case 3	(c) QDROP
Figure 3: Loss surface of the quantized weight for QDrop, Case 1 and 3 on test data and ResNet-18 W3A3.
To better distinguish Case 1 and 3, we zoom into the local loss surface with perturbation v1 and v2 magnitude
in [-0.025,0.025].
6
Published as a conference paper at ICLR 2022
Method	Bits (W/A)	ResNet-18	ResNet-50	MobileNetV2	RegNet-600MF	RegNet-3.2GF	MNasNet-2.0
No Drop	2/2	46.64	47.90	4.55	25.52	39.76	9.51
QDrop	2/2	51.14	54.74	8.46	38.90	52.36	22.70
No Drop	2/4	64.16	69.60	51.61	61.52	70.29	60.00
QDrop	2/4	64.66	70.08	52.92	63.10	70.95	62.36
Table 2: Effect of QDROP.
other parts. Note that the p obeys Bernoulli distribution and thus can be set as 0.5 for the maximal
entropy (Qin et al., 2020), which is helpful for flatness across various directions.
QDrop is easy to implement for various neural networks including CNNs and Transformers, and
plug-and-play with little additional computational complexity. With QDrop, the complicated prob-
lem of choosing optimization order, i.e. different cases in Sec. 3.1, can be avoided.
4	Experiments
In this section, we conduct two sets of experiments to verify the effectiveness of QDrop. In Sec. 4.1,
we first conduct an ablation study for the impact with and without dropping quantization and ana-
lyze the option of distinct dropping rates. In Sec. 4.2, we compare our method with other existing
approaches across vision and language tasks including image classification on ImageNet, object
detection on MS COCO, and natural language processing on GLUE benchmark and SQuAD.
Implementation Details. Our code is based on PyTorch Paszke et al. (2019). We set the default
dropping probability p as 0.5, except we explicitly mention it. The weight tuning method is the
same with Nagel et al. (2020); Li et al. (2021a). Each block or layer output is reconstructed for 20k
iterations. For ImageNet dataset, we sample 1024 images as calibration set, while COCO we use 256
images. In NLP, we sample 1024 examples. We also keep the first and the last layer in 8-bit except
NLP tasks and adopt per-channel weight quantization. We use W4A4 to represent 4-bit weight and
activation quantization. More model choices and other setting is described in Appendix E.
But to be noted, regular first and last layer 8-bit means 8-bit weight and input in first and last layer
while Brecq uses another setting which not only keeps the first layer’s input 8-bit but also the first
layer’s output (second layer’s input). This would indeed perform better than the regular one with
leaving one more layer’s input 8-bit but may not be practical on the hardware. Therefore, we both
compare with Brecq’s setting to show the superiority of our approach and experiment on the usual
one to provide a practical baseline. Symbol ↑ is used to mark BRECQ's setting.
4.1	Ablation study
Effect of QDrop. We propose QDROP and here we would like to test the effect of PTQ with or
without QDrop. We use ImageNet classification benchmark and quantize the weight parameters to
2-bit and quantize activation to 2/4-bit. As shown in Table 2, QDrop improves the accuracy across
all bit settings evaluated for 6 models on ImageNet. Furthermore, the gains are more obvious when
applying QDrop to lightweight network architecture: 2.36% increment for MNasNet under W2A4
and 12.6% for RegNet-3.2GF with W2A2.
Effect of Dropping Probability. We also explore
the dropping probability in PTQ. We choose p
in [0,0.25,0.5,0.75,1] and test on MobileNetV2
and RegNet-600MF. The results are summa-
rized in Fig. 5. We find 0.5 generally performs
best among 5 candidates. Although there could
be a fine-grained best solution for each ar-
chitecture, we shall avoid cumbersome hyper-
MobileNet-Vl W3A3	ReeNet-600MF W2A2
Probability p	Probability p
parameter search and continue using 0.5.	Figure 5: Impact of dropping probability on ImageNet.
4.2	Literature comparison
ImageNet. We choose ResNet-18 and -50 (He et al., 2016), MobileNetV2 (Sandler et al., 2018),
searched MNasNet (Tan et al., 2019) and RegNet (Radosavovic et al., 2020). We summarize the
7
Published as a conference paper at ICLR 2022
Methods	Bits (W/A)	Res18	Res50	MNV2	Reg600M	Reg3.2G	MNasx2
Full Prec.	32/32	71.06	77.00	72.49	73.71	78.36	76.68
ACIQ-Mix (Banner et al., 2019)	4/4	67.00	73.80	-	-	-	-
ZeroQ (Cai et al., 2020)*	4/4	21.71	2.94	26.24	28.54	12.24	3.89
LAPQ (Nahshan et al., 2019)	4/4	60.30	70.00	49.70	57.71*	55.89*	65.32*
AdaQuant (Hubara et al., 2021)	4/4	69.60	75.90	47.16*	-	-	-
Bit-Split (Wang et al., 2020)	4/4	67.56	73.71	-	-	-	-
AdaRound (Nagel et al., 2020)*	4/4	67.96	73.88	61.52	68.20	73.85	68.86
QDROP (Ours)	4/4	69.10	75.03	67.89	70.62	76.33	72.39
AdaROUndt (NageI et al., 2020)*	4/4	69.36	74.76	64.33	-	-	-
BRECQt(Liet al.,2021a)	4/4	69.60	75.05	66.57	68.33	74.21	73.56
QDROPt (Ours)	4/4	69.62	75.45	68.84	71.18	76.66	73.71
LAPQ (Nahshan et al., 2019)*	2/4	0.18	0.14	0.13	0.17	0.12	0.18
AdaQuant (Hubara et al., 2021)*	2/4	0.11	0.12	0.15	-	-	-
AdaRound (Nagel et al., 2020)*	2/4	62.12	66.11	36.31	57.00	63.89	46.73
QDROP (Ours)	2/4	64.66	70.08	52.92	63.10	70.95	62.36
AdaRoundt (Nagel et al., 2020)*	2/4	64.14	68.40	41.52	59.27	65.33	53.77
BRECQt (Li et al., 2021a)	2/4	64.80	70.29	53.34	59.31	67.15	63.01
QDROPt (Ours)	2/4	65.25	70.65	54.22	63.80	71.70	64.24
AdaQuant (Hubara et al., 2021)*	3/3	60.09	67.46	2.23	-	-	-
QDROP (Ours)	3/3	65.56	71.07	54.27	64.53	71.43	63.47
AdaRoundt (Nagel et al., 2020)*	3/3	64.66	66.66	15.20	51.01	56.79	47.89
BRECQt (Li et al., 2021a)*	3/3	65.87	68.96	23.41	55.16	57.12	49.78
QDROPt (Ours)	3/3	66.75	72.38	57.98	65.54	72.51	66.81
QDROP (Ours)	2/2	51.14	54.74	8.46	38.90	52.36	22.70
BRECQt (Li et al., 2021a)*	2/2	42.54	29.01	0.24	3.58	3.62	0.61
QDROPt (Ours)	2/2	54.72	58.67	13.05	41.47	55.11	28.77
Table 3: Comparison among different post-training quantization strategies with low-bit activation in terms of
accuracy on ImageNet. * represents for our implementation according to open-source codes and f means using
Brecq’s first and last layer 8-bit setting, which also keeps first layer’s output 8-bit besides input and weight in
the first and last layer.
results in Table 3. First, the W4A4 quantization is investigated. It can be observed that QDrop
provides 0 〜3% accuracy uplift when compared to strong baselines including AdaRound, Brecq.
As for the gap between our method and AdaQuant on W4A4, we argue that there are some discrep-
ancies on settings such as positions of quantization nodes and put this explaination in Sec. C.3. With
W2A4 quantization, QDrop can improve the accuracy of ResNet-50 by 0.5%, and RegNet-3.2GF
by 4.6%. In addition, to fully exploit the limit of QDrop, we conduct more challenging cases with
2/3-bit weights and activations. According to the last two rows of Table 3, our proposed QDrop
consistently achieves good results while existing methods suffer from non-negligible accuracy drop.
For W3A3, the difference is even larger on MobileNetV2, where our method reaches 58% accuracy
and Brecq only gets 23%. In the W2A2 setting, the PTQ becomes much harder. QDrop out-
performs the competing method by a large margin: 12.18% upswings for ResNet-18, 29.66% for
ResNet-50 and 51.49% for RegNet-3.2GF.
MS COCO. In this part, we validate the performance of QDROP on object detection task using MS
COCO dataset. We use both two-stage Faster RCNN (Ren et al., 2015) and one-stage RetinaNet (Lin
et al., 2017) models. Backbone are selected from ResNet-18, ResNet-50, and MobileNetV2. Note
that we set the first layer and the last layer to 8-bit and do not quantize the head of the model,
however, the neck (FPN) is quantized. Experiments show that W4A4 quantization using QDrop
nearly do not affect Faster RCNN’s mAP. For RethinaNet, our method has 5 mAP improvement on
MobileNetV2 backbone. In low bit setting W2A4, our method shows great improvement both on
Faster-RCNN and RetinaNet, up to 6.5 mAP.
GLUE benchmark and SQuAD. We test QDROP in NLP tasks including the GLUE benchmark
and SQuAD1.1. They are all conducted on the typical NLP model, i.e, BERT (Devlin et al., 2018).
Compared with those QAT methods (Bai et al., 2020), which usually adopt data augmentation trick
to achieve dozens of times the original data, we only randomly extract 1024 examples without any
extra data processing. Besides AdaQuant and Brecq, which suffer a huge accuracy degradation, our
QDrop surpasses No Drop all the tasks, specifically on QNLI(8.7%), QQP(4.6%) and RTE(7.2%).
8
Published as a conference paper at ICLR 2022
Method	Bits (W/A)	Faster RCNN			RetinaNet		
		ResNet-18	ResNet-50 MobileNetV2		ResNet-18	ResNet-50	MobileNetV2
Full Prec.	32/32	34.60	38.56	33.47	33.22	36.80	32.63
AdaRound*	4/4	32.57	34.47	26.11	31.04	33.51	24.99
BRECQt*	4/4	32.58	34.59	26.58	31.21	33.47	24.84
QDrop	4/4	33.37	36.96	30.88	31.99	35.67	29.75
AdaRound*	2/8	30.54	33.15	25.35	29.30	32.22	24.22
B recq†	2/8	31.82	34.23	27.54	31.42	34.75	27.59
QDrop	2/8	32.20	36.14	28.48	31.03	34.84	27.42
B recq†*	2/4	29.92	30.23	19.35	28.73	29.47	18.46
QDrop	2/4	31.01	34.23	25.04	29.69	33.01	24.89
Table 4: Comparison among typical post-training quantization strategies in terms of mAP on MS COCO. Note
that refer to Brecq, we didn’t quantize head and keep the first and last layer in backbone to 8-bit. Other
notations align with Table 3.
Method	SST-2 (acc)	QNLI (acc)	QQP (f1/acc)	STS-B (Pearson/Spearman corr)	MNLI (acc m/mm)	MRPC (acc)	RTE (acc)	CoLA (Matthews corr)	SQuAD1.1 (f1)
Full Prec.	92.43	91.54	87.81/90.91	88.04/ 87.63	84.57/84.46	87.71	72.56	53.39	88.42
AdaQuant*	-	-	-	-	-	-	-	-	5.17
BRECQ*	50.86	50.72	4.47/62.28	5.94/ 6.39	31.91/31.81	31.69	52.34	0.946	68.58
NO DROP	87.94	68.05	68.09/76.69	82.24/81.68	69.19/71.28	77.39	53.43	40.17	75.97
QDROP	88.06	76.75	72.66/79.04	82.39/81.88	71.43/73.70	79.15	60.65	40.85	77.26
Table 5: Performance on NLP tasks compared to other methods on E8W4A4. Here, we use symbol EeWwAa
to additionally express the embedding bit and conduct experiments on GLUE and SQuAD1.1.
As for SST-2, despite little enhancement by dropping quantization, it is indeed close to the FP32
value within 4.4%. And for STS-B, we argue that the original fine-tuned model is trained with
limited data, which might not be very representative.
4.3	Robustness of QDrop
In this part, we discuss the effectiveness of QDrop under more challenging situations including even
less data and cross-domain ones. Concerning about size of calibration data, we consider another
4 options. It can be observed that dropping some quantization behaves better under each setting
and is even comparable with No Drop with half of the original calibration data. Motivated by Yu
et al. (2021), we also reconstruct block output by 1024 examples from out-of-domain data, i.e,
CIFAR100 (Krizhevsky et al., 2009), MS COCO, and test on ImageNet. Results are available in
Table 6, where our QDrop still works steadily.
Calibration	Bits (W/A)	No Drop	QDrop
MS COCO	4/4	68.64	68.94
MS COCO	3/3	64.12	65.15
CIFAR100	4/4	46.83	52.88
CIFAR100	3/3	21.74	28.58
Table 6: Cross domain data.
Figure 6: Impact of calibration data size on ImageNet.
5	Conclusion
In this paper, we have introduced QDrop, a novel mechanism for post-training quantization. QDrop
aims to achieve good test accuracy given a tiny calibration set. This is done by optimization towards
a flat minima. We dissect the PTQ objective theoretically into a flatness problem and improve the
flatness from a general perspective. We comprehensively verify the effectiveness of QDrop on a
large variety of tasks. It can achieve a nearly lossless 4-bit quantized network and can significantly
improve the 2-bit quantization results.
9
Published as a conference paper at ICLR 2022
Acknowledgment
We sincerely thank the anonymous reviewers for their serious reviews and valuable suggestions to
make this better. And we thank Xiangguo Zhang and Sheng Chen for their kind of help of this work.
This work was supported in part by the National Natural Science Foundation of China under Grant
62022009 and Grant 61872021, the SenseTime Research Fund for Young Scholars, and the Beijing
Nova Program of Science and Technology under Grant Z191100001119050.
References
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin
King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.
Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional
networks for rapid-deployment. In Advances in Neural Information Processing Systems, 2019.
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer.
Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition,pp. 13169-13178, 2020.
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net-
works for efficient inference. In ICCV Workshops, pp. 3009-3018, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets, 2017.
Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and
Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv
preprint arXiv:1911.03852, 2019.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.
Angela Fan, Pierre Stock, Benjamin Graham, EdoUard Grave, Remi Gribonval, Herve Jegou, and
Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint
arXiv:2004.07320, 2020.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Yonggan Fu, Qixuan Yu, Meng Li, Vikas Chandra, and Yingyan Lin. Double-win quant: Aggres-
sively winning robustness of quantized deep neural networks via random precision training and
inference. In International Conference on Machine Learning, pp. 3492-3504. PMLR, 2021.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Sepp Hochreiter and JUrgen Schmidhuber. Flat minima. Neural computation, 9(1):1T2, 1997.
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training
quantization with small calibration sets. In International Conference on Machine Learning, pp.
4466-4475. PMLR, 2021.
10
Published as a conference paper at ICLR 2022
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
son. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2704-2713, 2018.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
Prad Kadambi, Karthikeyan Natesan Ramamurthy, and Visar Berisha. Comparing fisher information
regularization with distillation for dnn quantization. 2020.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efficient non-
uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019.
Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and
Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv
preprint arXiv:2102.05426, 2021a.
Yuhang Li, Mingzhu Shen, Jian Ma, Yan Ren, Mingxin Zhao, Qi Zhang, Ruihao Gong, Fengwei Yu,
and Junjie Yan. MQBench: Towards reproducible and deployable model quantization benchmark.
In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 1), 2021b. URL https://openreview.net/forum?id=TUplOmF8DsM.
Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin Dong, Fengwei Yu, Shaoqing Lu, and Shi
Gu. Mixmix: All you need for data-free compression are feature and data mixing. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 4410-4419, 2021c.
TsUng-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision, pp.
2980-2988, 2017.
MarkUs Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free qUantization
throUgh weight eqUalization and bias correction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1325-1334, 2019.
MarkUs Nagel, Rana Ali Amjad, Mart Van Baalen, Christos LoUizos, and Tijmen Blankevoort. Up or
down? adaptive roUnding for post-training qUantization. In International Conference on Machine
Learning, pp. 7197-7206. PMLR, 2020.
YUry Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M
Bronstein, and Avi Mendelson. Loss aware post-training qUantization. arXiv preprint
arXiv:1911.07190, 2019.
Behnam NeyshabUr, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gener-
alization in deep learning, 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James BradbUry, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
11
Published as a conference paper at ICLR 2022
Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan
Song. Forward and backward information retention for accurate binary neural networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing
network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition, pp. 10428-10436, 2020.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018.
Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei Yu, Junjie
Yan, and Wanli Ouyang. Once quantization-aware training: High performance extremely low-
bit architecture search. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 5340-5349, 2021.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820-2828, 2019.
Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network
quantization via bit-split and stitching. In Proc. 37nd Int. Conf. Mach. Learn.(ICML), 2020.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust gener-
alization. arXiv preprint arXiv:2004.05884, 2020.
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Chris
De Sa. Swalp: Stochastic weight averaging in low precision training. In International Conference
on Machine Learning, pp. 7015-7024. PMLR, 2019.
Haichao Yu, Linjie Yang, and Humphrey Shi. Is in-domain data really needed? a pilot study on
cross-domain calibration for network quantization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 3043-3052, June 2021.
Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li,
Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quan-
tization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2021a.
Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li,
Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantiza-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 15658-15667, 2021b.
Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
model perturbation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8156-8165, 2021.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
12
Published as a conference paper at ICLR 2022
A Noise Form Choice
As mentioned in Sec. 2, We refer the quantizer to a = [S] ∙ s, where S is the step size that can be
learned or be determined by collecting activation distribution in PTQ. By marking the rounding error
as c, which subjects to U[-0.5,0.5], the additive noise e = a - a can be denoted as C ∙ s. However,
this traditional noise does not decouple the noise from step size s and the range of activations. The
range of noise e will vary when the range of activations change, making it complicated to analyze
across the whole network in a unified form. Some existing papers (Jiang et al., 2019) also indicate
the problem that the additive noise doesn’t take parameters’ magnitude into account and suggest a
multiplicative form form (Keskar et al., 2016). To be specific, they claim that:
Perturbing the parameters without taking their magnitude into account can cause
many of them to switch signs. Therefore, one cannot apply large perturbations to
the model without changing the loss significantly. One possible modification to
improve the perturbations is to choose the perturbation magnitude based on the
magnitude of the parameter. In that case, it is guaranteed that if the magnitude
of perturbation is less than the magnitude of the parameter, then the sign of the
parameter does not change.
To eliminate the influence induced by the range of activations, we take the noise of multiplicative
form: ^ = (1+ U) ∙ a. In the quantization background, we denote the fixed-point integer value with
respect to a as a (a = (a + c) ∙ s and ^ = a ∙ s). And then U can be denoted as:
u = a - 1
a
a ∙ s ]
(a + c) ∙ S
=-ɪ - 1
aa + c
-c
(10)
aa + c
To be noted, U is derived from the definition of the quantizer and can be equivalently transformed
from the generalized noise here.
From this formulation, we can find that the range of U is not related to the activation range or step
size s and is only influenced by the rounding error and the bit-width. In a word, the multiplicative
form has its pysical meaning in quantization background and is beneficial as discussed above.
B Proof of Lemma 1 and Theorem 1
We demonstrate them by considering the situation of (1) fully connected and (2) convolutional net-
works separately. As transformation with fully connected ones has been revealed in main body, we
add some extra illustration here and mainly target at the convolutional layers.
B.1 Fully Connected networks
Here, we give the proof of Eq. (5) by leveraging the definition of FC layers in Eq. (1). We first look
at each input sample and temporarily omit x in the notation below for simplicity.
With activation noise u,
('+1) - X W(') C	(`)	(`)
Zi	= W i,j ∙ (1 + Uj) ∙ aj
j
=X(1 + uj')) ∙ Wij) ∙ aj').
j
By taking Vi,') = uj'), we have
z('+1) = X(1 + Vg)) ∙ Wij) ∙ aj').
j
(11)
(12)
13
Published as a conference paper at ICLR 2022
Thus, with proper constructed v, the noise on activation (u) can be viewed as the perturbation on
weight (v).
For every layer, we can conduct this transformation from u to v . Therefore, considering operat-
ing on the quantized weight and calibration data, optimizing Ex〜dc[L(W, x, 1 + u(x))] can be
approximated to optimizing Ex〜dc [L(W Θ (1 + v(x)), x, 1)]. Then We have
Ex〜Dc[L(W, x, 1 + U(X)) — L(w, x, 1)] ≈ Ex〜dc[L(W Θ (1 + v(x)), x, 1) — L(w, x, 1)]. (13)
Now, Lemma 1 is proved for the case of fully connected networks.
B.2 Convolutional networks
We first look at each input sample and temporarily omit x in the notation below for simplicity. One
layer in the network G can be interpreted as:
A('j+I)= f(Z'+I)) = f (X WM ∙ A(？p,j + q),	(14)
p,q
where the f (∙) is the activation function, (p, q) pair represents for one element in weight matrix and
(i, j ) pair represents for one element in activation matrix.
Due to the complicated design of convolutional structure, based on G we introduce two networks
G1 , G2 to make proofs more clearly.
Definition 1. G1 means inserting random noise on activations, G2 means sticking random variables
into weights.
Gl Aj+1)= f(Z('+I)) = f (X Wp('q ∙ (1 + U(+)p,j+q ) ∙ A(+p,j + q)
p,q	(15)
G2 :AT = f (Z£+1)) = f (X(1 + V持)∙ Wp('q) ∙ A(?p,j+q))
p,q
Definition 2. We mark losses of network G1 and G2 in the following way:
Loss of G1 : L(w , x, 1 + u)	when taking u = 0, it becomes L(w, x, 1)
Loss of G2 : L(w Θ (1 + v), x) when taking v = 0, it becomes L(w Θ 1, x).
(16)
With the definitions, we first prove that G1 and G2 share common parts in their first-order derivative
to its noise in Lemma 2.
Lemma 2. Assuming the same weight and taking u = 0 and v = 0 for G1 and G2, we have
∂L(w, x, 1)
∂U(j
i,j
X T (`)
(i,j),(p,q)
p,q
∂L(w Θ 1, x)
X T (`)
(i,j),(p,q),
i,j
(17)
∂ V粉
where
(`)
(i,j),(p,q)
∂L(w , x, 1)
dA 匕 13
• f(ZS-q) • Wp('q) •
∂L(w Θ 1, x)
d A 匕 13
• f 0 (Z(-+13) ∙ WP'q ∙ Aj
(18)
Note that with u = 0 and v = 0, the activations are the same for the two networks so we don’t use
different notations here.
Proof.
∂L(w, x, 1)	∂L(w, x, 1)	0	('+i)
„) • W(') • A('
du(`)	乙 ∂a('+i) J ( i-p,j-q)	Wp,q Aij
∂Ui,j	p,q ∂Ai-p,j-q
(19)
14
Published as a conference paper at ICLR 2022
∂L(w 1, x)
∂ vp(q)
Σ
i,j
∂L(w 1, x)
Σ
i,j
∂A(j+1)
i,j
∂L(w Θ 1, x)
∂A(e+1)
∂Ai-p,j-q
• f'(ZE+1)) • Wpeq) ∙ A(+)p,j+q
• f0(ZS13) ∙ Wq) ∙ As
(20)
Because of taking the same weight, and calculating the derivatives to Uij at U = 0 and derivatives
to 匕(') at V =
holdis,.j
∂L(w, x, 1)
0, we have the same activation values for these two networks. Therefore, Eq. (21)
∂Ai-p,j-q (e+1)
• f0(Z(e+1)	) • W(e) • A(e) = dL(w θ 1, x)、• f 0(Z('+1)	) • W(e) • A(e)
i-p,j-qj	p,q ij	(e+i)	i-p,j-qy	p,q	i,j
∂Ai-p,j-q
(21)
(`)
By marking the above value as T(i,j),(p,q), we can get Lemma 2.
□
Based on Lemma 2, we further derive Theorem 2 as the pre-condition of the final Lemma 1.
Theorem 2. By taking
P	(e)	(e)
V(')=乙i,j Ui,j • T(ij),(p,q)
p,q	P	t(')
i,j T(i,j),(p,q)
(22)
we have
U> VuL(W, x, 1) = v>VvL(W Θ 1, x).
(23)
Proof. According to Lemma 2,
u>VuL(w, x, 1) = XX Uij
∂L(w, x, 1)
` i,j
XX U(S ∙ X
∂ Uiej
i,j
(`)
(i,j),(p,q)
By taking Vp?
` i,j
XXX
`	i,j p,q
XXX
`	p,q i,j
e p,q
e p,q
Pi,。UWy(3,(p,q)
P	T(')
i,j T(i,j),(p,q)
p,q
(`)	(`)
Ui,j • T(i,j),(p,q)
(`)	(`)
Ui,j • T(i,j),(p,q)
P	(e)	(e)
乙ij Uij • T(ij),(p,q)
P	7(e)
i,j	(i,j),(p,q)
P	(e)	(e)
"Uij • T(ij),(p,q)
P	7(e)
i,j T(i,j),(p,q)
X T (`)
(i,j ),(p,q)
i,j
∂L(w Θ 1, x)
∂Vp,q O
(24)
u>VuL(w, x,i)= E EVpq) ∙
`
v>
p,q
∂L(w Θ 1, x)
∂Vp,q O
(25)
Thus Theorem 2 is affirmed.
VvL(W Θ 1, x).
□
We now prove Lemma 1 equipped with Taylor Expansion technique and Theorem 2.
Proof. First, by adopting Taylor Expansions at u = 0, we can get that:
L(W, x, 1 + U) — L(w, x, 1) ?≈ L(W, x, 1) + u>VuL(W, x, 1) — L(w, x, 1).
(26)
15
Published as a conference paper at ICLR 2022
(27)
(28)
(29)
□
(30)
Then, according to Theorem 2, the above equation can be rewritten as:
L(w, x, 1 + U) — L(w, x, 1) ≈ L(W Θ 1, x) + v>NvL(W Θ 1, x) — L(w, x, 1).
Again, by adopting Taylor Expansions at v = 0 for the right part of Eq. (27), we arrive at:
L(W, x, 1 + U) — L(w, x, 1) ≈ L(W Θ (1 + v), x) — L(w, x, 1).
Finally, apply expectation on Eq. (28), and Lemma 1 is proved:
Ex〜Dc[L(W, x, 1 + u(x)) — L(w, x, 1)]
≈ Ex〜dc[L(W Θ (1 + v(x)), x) — L(w, x, 1)].
With the Lemma 1 proved, we can easily derive Theorem 1 by the following transformation:
Ex〜Dc [L(W, x, 1 + u(x)) — L(w, x, 1)]
≈ Ex〜dc[L(W Θ (1 + v(x)), x) — L(w, x, 1)]
=Ex〜dc[L(W Θ (1 + v(x)), x, 1) — L(w, x, 1) + L(W, x, 1) — L(W, x, 1)]
=Ex〜dc[L(W, x, 1) — L(w, x, 1) + L(W Θ (1 + v(x)), x, 1) — L(W, x, 1)].
C Experiments
C.1 Supplementary experiments of Sec. 3. 1
To explore the upper limit of PTQ, we concern two key parts of the algorithm (weight quantization
and activation one). As is generally known, QAT is a popular way to produce favor quantization
results, thus we use QAT’s learning mechanism to replace each of the two parts in PTQ and record
outcomes in Table 7. In detail, about weight quantization, QAT’s settings we used include the
whole ImageNet, STE learning strategy and end-to-end training for 40 epochs while PTQ’ settings
only cover 1024 samples, training block by block. But we both keep the rounding-up-or-down
parameter optimization space. About activation quantization, QAT’ settings we used include the
whole ImageNet, LSQ (Esser et al., 2019) scheme and end-to-end training for 5 epochs.
However, results in Table 7 are surprising that although the optimization space in weight is kept
very restricted, leveraging the whole data to implement weight tuning can achieve a large accuracy
boost. And the activation quantization step size might not be as important as weight. These findings
indicate that exploring a quantization-friendly weight may be the fresh insight for the accurate PTQ,
where this work dedicates to.
C.2 SUPPLEMENTARY EXPERIMENTS OF Sec. 3.3
Overfitting phenomenon. To analyze the overfitting problem refered in Sec. 3.3, we have con-
ducted experiments to compare the accuracy on test data and calibration data, respectively. The
table below is an example of ResNet-18 W2A2. It can be seen that with extremely low-bit quantiza-
tion, both Case 2 and Case 3 perform well on calibration data but on test data Case 2 performs worse
than Case 3. This is a shred of clear evidence that Case 2 suffers a more severe overfitting problem.
Activation setting Weight setting Accuracy		
PTQ	PTQ	46.64
QAT	PTQ	49.53
PTQ	QAT	57.49
Table 7: Ablation study of quantization settings on
ResNet-18 W2A2.
Methods	Test accuracy	Train accuracy
Case 1	18.88	19.82
Case 2	45.77	69.54
Case 3	48.07	69.92
QDrop	51.14	66.11
Table 8: Train and test accuracy on ResNet-18
W2A2.
Rethinking Theorem 1, both Case 2 and Case 3 introduce the term (7-2) that implies flatness against
the perturbation v(x). However, Case 2 completely introduces the quantization noise U(x) accord-
ing to the calibration data. Thus the resulting flatness fits calibration data but does not generalize
16
Published as a conference paper at ICLR 2022
well on test data. Case 3 drops partial of u(x) and improves the possibility of flatness on test data
instead. The two accuracy of QDrop also confirmed this phenomenon with more diverse directions
of flatness. As for Case 1, its accuracy on both calibration data and test data is low. This is be-
cause that it does not introduce activation quantization during weight tuning and thus behaves worst
without taking flatness term even on calibration data into account.
Hessian information. As Hessian infor-
mation is known to be a metric to char-
acterize flatness property (Keskar et al.,
2016; Dong et al., 2019), we also calcu-
late the top-1, top-5 Hessian eigenvalues
(λ1 , λ5) and the Hessian trace (Tr) among
3 cases and QDrop to better support our
analysis. In table Table 9, QDrop has
the smallest value of Hessian information,
which matches with our theoretical frame-
work and observations of loss landscape.
Method	λl	λ5	Tr
Case 1	14770	6746	122894
Case 2	8423	4050	86287
Case 3	8258	3821	84044
QDrop	6850	3044	66371
Table 9: Hessian information of the ResNet-18 W3A3
model. λ1 represents for the top-1 Hessian eigenvalue, λ5
for top-5 Hessian eigenvalues and Tr for Hessian trace.
C.3 Supplementary experiments of Sec. 4
To clarify the gap between AdaQuant and our method in classification task on 4-bit ResNet-18
and -50, we analyze the difference on settings between these two algorithms. There are two major
discrepancies which would influence the final accuracy. One is the position of activation quanti-
zation nodes, the other is the FP32 accuracy of pretrain-models. As shown in Fig. 7, by insert-
ing the quantizer like the right side one, AdaQuant would introduce two different quantizers for
the same input with learned quantization parameters such as step size. We find this can bring
〜 0.4% upswings on ResNet-18 and -50 W4A4 and improve more on ultra low-bit with exper-
iments of our algorithm. However, this way to insert quantization nodes is not practical in real
deployment, because two different quantizers for the same input would make it impossible to fol-
low the so-called Requantize procedure (Li et al., 2021b). As for the pretrain-models, they use
71.97% and 77.2%, higher than ours (71.06% and 77.0%), which can indeed improve the accuracy.
Replacing our settings with the two adopted in AdaQuant, We finally reach 71.07% and 76.67%.
X
FakeQUant
Figure 7: Different ways of inserting activation quanti-
zation nodes. Our method obeys the left side one while
AdaQuant adopts the right side one.
(b )
Method	Bits (W/A)	ResNet-18	SWA20
	FP32	71.06	71.50
Min-Max*	8∕8^^	70.94	71.40
	4/8	52.33	65.26
	4/4	26.05	44.37
OMSE*	32/4	64.15	65.99
(Choukroun et al., 2019)	4/4	38.32	55.86
BRECQt* (Li et al., 2021a)	2∕4^	65.35	66.33
	2/2	41.74	44.30
Table 10: Experiments between ResNet-18 and
SWA20 when adopting different PTQ methods.
SWA20 is acquired by finetuning ResNet-18 using
SWA technique for 20 epochs.
C.4 Flatnes s and Post-Training Quantization
While there are plenty of works exploring the correlation between flatness and generalization, the
interaction between quantization and flatness has not been exploited much. In this paper, we first
connect flatter quantized weight with activation quantization from PTQ view, as implied in Sec. 3.2.
From this, we conjecture that flatness and quantization might be helpful to each other.
By leveraging some mechanisms devoting to produce a smoother loss surface for better generaliza-
tion, such as (Izmailov et al., 2018), the improved FP32 model is obtained and used to validate the
17
Published as a conference paper at ICLR 2022
performance on quantization compared with the naive one. In Table 10, model SWA20 is enabled by
applying the SWA technique (Izmailov et al., 2018) to ResNet-18 with 20 epochs fintuning process
on the whole ImageNet. From the table, the observation is that the promotion on generalization can
not fully represent the enhancement induced by SWA on quantization. With even lower bits thus
larger noise, SWA20 surpasses the naive one by a large margin. It also reveals that distinct FP32
models with analogous accuracy might contribute to surprisingly disparate outcomes after PTQ,
particularly for those naive methods without any weight tuning.
In turn, there have been some studies that delve into robustness boost by applying quantization.
(Fu et al., 2021) advocates that quantization can be properly leveraged to enhance DNNs’ robust-
ness, even beyond their full-precision counterparts. They propose a random bit training strategy to
accomplish it, where our work illustrates the correlation with bit and perturbation in Appendix A.
D Related works
Post-training quantization. Unlike QAT Esser et al. (2019); Li et al. (2019); Shen et al. (2021)
where the quantized model is finetuned with full training dataset and over 100 epochs training, PTQ
is much more faster. Rounding-to-nearest operation is known to be the direct and easy way for
quantizing parameters or activations in PTQ. Although there is almost no accuracy drop when quan-
tizing to 8-bit, lower bit quantization is yet a hard task and worth exploring. (Choukroun et al.,
2019) transforms quantization to a Minimum Mean Squared Error problem both for weights and
activations. (Nagel et al., 2019) equalizes weight ranges among channels thus be more favorable
to per-layer quantization and employs bias correction to absorb the output error induced by quanti-
zation. However, such methods neglect the task loss thus lead to a sub-optimal. AdaRound (Nagel
et al., 2020), which proposes to learn the rounding mechanism by reconstructing output layer by
layer brings more opportunities for 4-bit quantization. Besides layer reconstruction, Brecq (Li
et al., 2021a) discusses more choices and advises to do block reconstruction with better accuracy at
2-bit weight quantization. Nonetheless, we argue that AdaRound and Brecq isolate weight quan-
tization and activation one theoretically and experimentally, which might be a key point of failures
on extremely low-bit quantization. Recently, there is another trend of utilizing synthetic data for
PTQ, which explicitly do backpropagation on the learned input tensor Cai et al. (2020); Zhang et al.
(2021b); Li et al. (2021c).
Flatness. The idea of “flat” minima might date back to (Hochreiter & Schmidhuber, 1997), where
the benefits are recognized in recent years, such as generalization (Jiang et al., 2019; Keskar et al.,
2016) and adversarial training (Wu et al., 2020; Zheng et al., 2021). Some previous works (Izmailov
et al., 2018; Foret et al., 2020) devote to improve the flatness of the trained weight for robustness
under perturbation or distribution shift. Other ideas try to model flatness or sharpness formally by
visualization of loss landscape or mathematical formulas. And for quantization, which could be
viewed as some kind of noise, a flat model has been implied to be preferable, (Dong et al., 2019;
Yang et al., 2019; Kadambi et al., 2020). Despite the natural fact that flatness helps with weight
quantization, how does activation quantization involves with smoother loss surface has not been dis-
cussed deeply, particularly for post-training quantization. In this work, we introduce noise scheme
by randomly dropping activation quantization and achieve a general flatness. Another paper (Fan
et al., 2020) also utilizes randomness by adding noise to weight for the simulation of weight quan-
tization. But they target at reducing the induced bias of Straight Through Estimation (STE) in QAT
and also have different motivations and solve different problems from us.
E	Implementation details
Observation. Here, we give the concrete implementation of experiments in Sec. 3.1. We actually
consider three ways of introducing activation for Case 2, but we find the differences of outcomes
among them are negligible thus employing one of them for clearer clarification.
ImageNet. We randomly extract 1024 training examples from ImageNet as calibration dataset based
on the standard pre-process. Pretrain-models are downloaded from Brecq’s open source code.
Hyper-parameters we keep it as Brecq, such as batch size 32, learning rate for activation step
size 4e-5, learning rate for weight tuning 1e-3, iterations 20000. Following Brecq, we first fold
batch normalization layer into convolution then reconstruct output block-wise to learn the weight
18
Published as a conference paper at ICLR 2022
Algorithm 2: Implementations of three cases in Sec. 3.1
Input: Model with K blocks.
if Case 2 then
for k = 1 to K do
L parameterize activation step size in this block ;
for k = 1 to K do
Tuning weight like (Li et al., 2021a) by reconstructing block output ;
if Case 3 then
I parameterize activation step size in this block ;
if Case 1 then
for k = 1 to K do
parameterize activation step size in this block ;
return Quantized model ;
rounding policy and meanwhile use LSQ (Esser et al., 2019) to parameterize activation step size.
For QDrop, we learn the weight and activation parameters together and use 50% rate to drop some
activation quantization.
Object detection. Here, we also obey BRECQ’ settings and use the same pretrain-models with 256
training samples taken from MS COCO dataset for calibration. Parameters about resolution is set to
800 (max size 1333) and 600 (max size 1000) for ResNets and MobileNetV2, respectively and batch
size is set to 2 while others are the same with classification task. To be noted, we didn’t quantize the
head but applied block reconstruction to backbone and layer reconstruction to neck like Brecq.
GLUE benchmark and SQuAD. The BERT fine-tuned models are taken from huggingface group
(https://huggingface.co/). And we sampled 1024 examples from training set. We keep the maximum
sequence length to be 128 for GLUE benchmark but maximum sequence length 384 with doc stride
128 for SQuAD1.1. Also, we quantize all the part in BERT as well as the internal structure of
the attention module with only affirming the embedding weight to 8-bit. Other settings and hyper-
parameters are chosen in the same way with ImageNet experiments.
Baselines. We run baseline methods from open-source codes, such as AdaQuant, BRECQ and
Adaround. And we try our best to align some optimal settings like per-channel quantization for
fair comparisons.
19