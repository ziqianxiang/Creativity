Published as a conference paper at ICLR 2022
Towards Understanding the Robustness
Against Evasion Attack on Categorical Inputs
Hongyan Bao*
King Abdullah University of Science and Technology
hongyan.bao@kaust.edu.sa
Yujun Zhou
King Abdullah University of Science and Technology
yujun.zhou@kaust.edu.sa
Yufei Han*
INRIA
yufei.han@inria.fr
Yun Shen
NetAPPt
yun.shen@netapp.com
Xiangliang Zhang ^
University of Notre Dame
xzhang33@nd.edu
Ab stract
Characterizing and assessing the adversarial risk of a classifier with categorical
inPuts has been a Practically imPortant yet rarely exPlored research Problem.
Conventional wisdom attributes the difficulty of solving the Problem to its
combinatorial nature. Previous research efforts tackling this Problem are sPecific
to use cases and heavily dePend on domain knowledge. Such limitations Prevent
their general aPPlicability in real-world aPPlications with categorical data. Our
study novelly shows that Provably oPtimal adversarial robustness assessment is
comPutationally feasible for any classifier with a mild smoothness constraint. We
theoretically analyze the imPact factors of adversarial vulnerability of a classifier
with categorical inPuts via an information-theoretic adversarial risk analysis.
Corroborating these theoretical findings with a substantial exPerimental study over
various real-world categorical datasets, we can emPirically assess the imPact of the
key adversarial risk factors over a targeted learning system with categorical inPuts.
1	Introduction
Categorical data Pervasively exist in real-world safety-critical Machine-Learning-as-a-Service
(MLaaS) aPPlications, such as ML-driven intrusion detection and digital healthcare. The vulnerability
to attacks by intentionally crafting categorical signatures raises concerns on trust and utility of the ML-
based analytic services. Characterizing and assessing adversarial robustness on categorical data can
thus helP evaluate the reliability of the core ML models and flag Potential evading efforts. For a classifier
f with categorical inputs ɪ, the adversarial risk of f under evasion attack can be formulated as follows.
Definition 1 f: X → y denotes a classifier with categorical inputs x. Let μ x,y denote the joint distri-
bution of (x,y). The expected adversarial risk ofthe classifier f under evasion attack isformulated as:
R ^dv =	E	sup	I (f (X),y)
(x'y)~Mχ,y |diff(x,£) | ≤ε
(1)
where I is the misclassification loss function, X andX are an unperturbed and the generated adversarial
sample respectively. ε denotes the attack budget ofevasion attack. X is COrreCtly classified (f (x) = y).
Intuitively, a classifier f is more robust against adversarial perturbations, if its adversarial risk is low
given the attack budget limiting the number of changed categorical features. Unlike continuous data,
a categorical variable can be valued with only one categorical value among others. These categorical
*The first two authors contributed equally.
tThe author contributed to this work while at NortonLifeLock.
^Corresponding author. Dr. Zhang is also affiliated with King Abdullah University of Science and Technology
1
Published as a conference paper at ICLR 2022
values have no intrinsic ordering to the categories. Evasion attack manipulating categorical inputs
is in nature an NP-hard knapsack problem. The discontinuous nature raises two fundamental yet
rarely addressed questions to evaluate the adversarial risk on categorical data in practice:
Q1 What are the key factors determining f 's adversarial risk R" on categorical data ?
Q2 For a general classifier f, can We assess the adversarial risk of f with categorical inputs with
provably accuracy guarantee ?
Despite recent efforts of adversarial vulnerability exploration with discrete data, both questions remain
open for several reasons. First, the discontinuity of categorical space prevents the direct use of the
previous progress on adversarial risk analysis with continuous data. The local subspace assumption
of IP -bounded adversarial attacks does not apply to the categorical features (Hein & Andriushchenko,
2017; Wang et al., 2018; Fawzi et al., 2016; Gilmer et al., 2018; D.Yin et al., 2019; Khim & Loh, 2018; Tu
et al., 2019). Second, most practices of discrete adversarial attack are domain specific and depend heav-
ily on domain knowledge. (Bojchevski & Gunnemann, 2019; Bojchevski & Gunnemann, 2θl9; Zugner
& Gunnemann, 2019) focus on building differentiable surrogate functions to Graph Neural Networks to
facilitate searching for feasible poisoning edits over graph structures and node attributes. (Narodytska
& Kasiviswanathan, 2017; Croce & Hein, 2019) conduct “-norm perturbations only within local
image areas containing sensitive features for image classification. (Qi et al., 2019; Wang et al., 2020)
require non-negativity on the parameters of deep neural networks to deliver provably accurate greedy
attacks via submodular function optimization. The non-negativity constraint is unnatural for real-world
ML practices. It does harm the utility of the classifier. For a more general classifier with categorical
inputs, a provably optimal and domain-agnostic method for attack and adversarial risk evaluation is yet
to establish. Using greedy search or the well-known Branch-and-Bound method on a general knapsack
problem provides no optimality guarantee of the solutions, thus can produce arbitrarily bad results.
Our study aims to address these two questions mentioned above from both theoretical and empirical
perspectives. First, we derive an information-theoretic characterization of the adversarial risk ofa
classifier. It unveils that the informativeness of the input categorical instance, the sensitivity of the
perturbed categorical features and the information geometry property of the targeted classifier are
the three key factors jointly determining adversarial vulnerability of the classifier.
Second, our study adopts an assess-by-attack strategy. We show that assessing adversarial robustness
of any measurable classifier with categorical inputs can be cast to a weakly submodular maximization
problem, with a mild smoothness condition. It can thus be solved using a simple yet efficient greedy
attack strategy with provable approximation guarantees. The theoretical findings not only explain the
empirical success of the greedy search strategy to generate adversarial textual and image samples (Gong
et al., 2018; Yang et al., 2018; Narodytska & Kasiviswanathan, 2017), but also pave the way to a domain-
agnostic adversarial robustness assessment on categorical data with provable optimality guarantees.
Third, we instantiate the domain-agnostic adversarial risk characterization and assessment with a
widely used DNN classifier, i.e. Long Short-Term Memory (LSTM) and three different categorical
datasets in Section.4. These datasets are collected from various real-world applications. The experimen-
tal results confirm the impact of the 3 risk factors over the adversarial vulnerability of the DNN classifier.
2	Related Work
Tremendous efforts have been made to vulnerability measurement of a classifier under evasion attack
(Hein & Andriushchenko, 2017; Wang et al., 2018; Fawzi et al., 2016; Gilmer et al., 2018; Weng
et al., 2019; Sinha et al., 2018; Cohen et al., 2019; Shi et al., 2020; D.Yin et al., 2019; Khim & Loh,
2018; TUet al., 2019). Most of the previous works focus on evaluating robustness against IP -norm
perturbations on continuous data. They all assume adversarial samples locate within a smooth IP -ball
around an input instance, which doesn’t hold for categorical data. In contrast, Tu et al. (2019) covers
both numerical and categorical data. It bounds the adversarial risk with a local worst-case risk over a
P-WaSSernStein ball centered at the training data distribution. This work associates the adversarial risk
ofa classifier with its rademacher complexity. Nevertheless, we argue that the adversarial vulnerability
of a classifier is determined by not only the characteristics of the classifier, e.g. model complexity,
but also the properties of the training/testing data instances.
Pioneering works of evasion attacks with categorical inputs depend on domain-specific knowledge
to facilitate the attack exploration. Kuleshov et al. (2018); Papernot et al. (2016); Miyato et al. (2016);
Samanta & Mehta (2017); Gao et al. (2018); Yang et al. (2018); Gong et al. (2018); Ebrahimi et al.
2
Published as a conference paper at ICLR 2022
(2018); Narodytska & Kasiviswanathan (2017); Croce & Hein (2019) focus on replacing individual
words/phrases to cheat text classifiers, or modifying pixel intensities to bias image classification results.
These methods use heuristic semantic rules, e.g., replacing words with manually defined candidate
synonyms and constraining the word change to preserve readability and semantic integrity. Narodytska
& Kasiviswanathan (2017); Croce & Hein (2019) narrow down the search range to the pixels with
high pixel-wise sensitivity for image classification. Despite of the sounding empirical results, there
is no guarantee on a successful attack within the attack budget.
Bojchevski & Gunnemann (2019); Bojchevski & Gunnemann (2019); D.Zugner et al. (2019);D. Zugner
& Gunnemann (2018); Akbarnejad & Gunnemann (2019) adopt edge-flipping and node attribute
perturbation to poison graph data mining pipelines, e.g., graph neural networks and graph embedding
models. The key idea is to introduce relaxed surrogate functions to the combinatorial attack objective
and then solve the relaxed optimization problem instead. Notably, D.Zugner et al. (2019); D. ZUgner
& Gunnemann (2018); Akbarnejad & Gunnemann (2019) define the attack objective as a sum of
smallest eigenvalues of the adjacency matrix ofa given graph. Though itis not explicitly claimed, itis
intrinsically a submodular maximization problem.
Qi et al. (2019); Wang et al. (2020) unveil that simple greedy search can deliver provably effective
attacks against DNN classifiers without domain-specific heuristics, if all the link weights between
neurons are non-negative. Non-negativity of the parameter guarantees strict submodularity of the
attack objective. However, it brings significantly deterioration of the classifier’s accuracy, which is not
applicable for real-world learning tasks.
3	Robustness Characterization and Assessment
We assume an input instance X = {x1,x2,x3,...,xn} of n categorical attributes. Each xi takes any of m
(m ≥ 1) categorical values. The classifier f outputs decision probabilities fyk (k = 1,2,3,…,K) with
respect to different class labels. In practices, each categorical value of x： is cast to a D-dimensional pre-
trained embedding vector, e.g., e，∈ Rd,j = 1,2,...,m. To represent an instance X with the embedding
vectors of its category values, We define binary variables b = {b∖}, i=1,2,...,n, j=1,2,...,m, where Mi = 1
when the j-th attribute value is present for Xi and bʃ = 0 otherwise. An instance X can then be represented
by an Rn*m*D tensor with X{：,；,：} = b： e[. Let b = {b：＞} indicate the adversarial modifications introduced
into b. For a perturbed X^, its b ≠ b. Depending on the type of attacks to implement, e.g., insertion,
deletion or Substitution, b differs from b in different ways. Without loss of generality, let yκ denote
the true class label of X and all the other yk(k = {1,…,K -1}) are the potential targets of an evasion
attack. The goal of attack is to make fyκ(X,b) as low as possible and increase fyk(X,b) of a fixed k
(targeted attack) orany k of {1,...,K-1} (non-targeted attack) as high as possible simultaneously. We
focus on non-targeted evasion attack, and leave the targeted scenario for future study.
3.1	Information-theoretic Characterization of Adversarial Vulnerability
Theorem 1 For an instance(X,y) and a training set S SamPledfrOm the same underlying distribution
μ x,y, f ∈H is trained using S with a deterministic training paradigm. The expected adversarial risk
R* defined in Eq.1 can be bounded as in Eq.2, ifthe loss function I in R* adopts the zero-one loss.
2/(X；y,S)-2〃 X-1( fy ；S)/2+const
R " ≥ 1-
IogK
(2)
η x =	sup	I(X；y,S)-/(X；y,S)	(3)
^.
I diff(b,b )∣≤ £
where const denotes a constant term. diff(b,b) indicates the set ofthe categorical attributes modified
in the attack. I(X;y,S) denotes the mutual information between the feature X and the pair of label y and
training set S. η X is the SUPremUm ofthe difference between the mutual information before and after
adversarial perturbation, noted by I (x; y,S) and I (X；y,S) respectively. I (fy ;S) denotes the mutual
information between S and f.
Theorem 1 unveils the three impact factors jointly determining adversarial vulnerability of the targeted
classifier f (for answeringQ1 in Section 1). The proof is given in Appendix.A. In Eq.2, a lowermutual
information I(X；y,S) indicates higher adversarial risk, i.e., RF has a higher lower bound. A lower
3
Published as a conference paper at ICLR 2022
I(x;y,S) denotes weaker consistency between the input instance (x,y) and the training set S. The clas-
sifier trained by S thus produces weaker decision confidence for such (x,y). With (x,y) dropped to the
ambiguous zone near the classification boundary, the classifier is more prone to adversarial perturbation.
A higher I(fy ;S) leads to a higher adversarial risk according to Eq.2. The mutual information I(fy ;S)
reflects the dependence between the classifier f and the training set S, which can be considered as
a lower bound of the VC-dimension for countable hypothesis space f ∈H (XU & Raginsky, 2017;
Zhu et al., 2020). A higher I(fy ;S) thus denotes that f is more likely to suffer from being overfitted to
S. Model overfiting is one of the causes of adversarial vulnerability (Tu et al., 2019). Resonating with
the association between I(fy ;S) and adversarial risk, three popularly used robustness enhancement
methods controlling the classifier’s complexity and overfiting risk can potentially adjust the adversarial
vulnerability of the classifier with categorical inputs. 1) Adversarial training (Miyato et al., 2016;
Sinha et al., 2018; Florian et al., 2018; Wang et al., 2019; Shafahi et al., 2019). The adversarially
retrained classifier f is less correlated with the original training set S compared to f, which reduces
I(fy ;S) and may help mitigate the adversarial threat. 2) The addition of nuclear norm regularization
over the classifier’s parameters in the training process (Ravi et al., 2019; Tu et al., 2019). The resultant
regularized classifier has a controlled model complexity, which can potentially reduce the adversarial
risk. 3) Random smoothing (Lee et al., 2019; Levine & Feizi, 2020; Dvijotham et al., 2020; Boj.
et al., 2020). Following (Cohen et al., 2019), this defense method randomly selects and flips the input
categorical features of the targeted classifier. The randomly perturbed classification output is less
correlated with the training data S’s distribution, which may reduce adversarial risk.
A higher ηx in Eq.3 indicates a higher loss Ofpredicative information by changing x to 戈,and thus
a higher adversarial risk. In this sense, the vulnerability of the classifier depends on the sensitivity
of the attacked categorical features with respect to the classification task.
3.2	Scoring Robustness via Set Function Optimization
We formulate the assessment of adversarial robustness level on categorical data as a set function
maximization problem: we aimat finding a minimal set of categorical feature perturbations I = diff( b,B),
with which the maximum gap between the classification confidence on any wrong label k and the
correct label K is larger than a predefined threshold Γ. The size of the minimal perturbation set I is used
to assessment adversarial robustness of f over the input instance. Within the perturbation budget ɛ, a
smaller/larger 111 provoking the alert indicates that the classifier,s output on the input X is more/less vul-
nerable w.r.t. adversarial perturbation in the categorical feature space. We consider both the pessimistic
(in Eq. 4) and optimistic scenario (in Eq. 5) from the defender’s perspective for robustness scoring.
ψPeSS(I) =	max	max (mF) s.t. mF ≥Γ, ∣l| ≤ε	(4)
l^i=ι...n J=diff((b,l^)	W=Le
ψoptim(l) =	min	max (mF) s.t. mF ≥Γ, |l| ≤ε	(5)
b^i=ι...n J=diff(b,b^)	b；Te
max
k ∈{1,...,K-1}
where m F
Cf / Y⅛ ∖ >> f / γ' ∖ ∙ , 1	IC 1 ∙ , 1 ♦ , ∙	l^TC∖
{ fyk (x,b)}-fyK (x,b) is the gap defined with an input instancex. m于 ∈ [-1,0)
and m F ≥ 0 correspond to correct and wrong classification respectively. Γ is a threshold reflecting
different levels of tolerance to the perturbation of the classifier’s output. m F ≥Γ then triggers an alert
of potential adversarial attack. Safety-critical domains, such as cyber security, prefer a lower Γ to
provoke alerts even with small yet suspicious input perturbations.
Observation 1 With the threshold Γ = 0, ψPeSS ≥ 0 derived by solving the pessimistic assessment
problem in Eq.4 produces an empirical estimate of the expected adversarial risk f with an input
instance (x,y) sampledfrom μx,y.
We leave the derivation of this observation in Appendix.B. Eq. 4 is a bi-level optimization problem.
The inner level seeks the optimal change of the category value of a given categorical feature, in order
to produce maximum possible perturbation on the classifier’s output. The outer level finds the optimal
combination of different categorical features for maximizing m F within the attack budget. |l| is thus
a pessimistic robustness assessment with respect to a knowledgeable adversary. The adversary can
locate the most effective modification for both individual categorical features and the combination
set of the categorical features.
In contrast, |l| to Eq. 5 reflects an optimistic robustness assessment with respect to an oblivious
adversary. In practices, an adversary can be limited by the knowledge about the targeted classifier
4
Published as a conference paper at ICLR 2022
and/or the query budget. Such an oblivious adversary can only locate the effective category values
to attack for a given categorical feature, while can’t find the optimal feature combinations to solve
the outer-level optimization.
3.3	Provably Accurate Robustness Assessment via Weakly Submodular Maximization
Exactly solving Eq.4 and Eq.5 is still NP-hard. To answer Q2 in Section 1, Theorem 2 and Theorem 3
elaborate the optimality guarantee of solving both robustness assessment problems via computationally
efficient greedy search, for any classifier meeting the smoothness condition in Definition.2.
Definition 2 Smoothness Condition of f. Let Ω = (p,q), p,q ∈ RN and f: Rn→R be a Lipschitz-
continuous and differentiablefUnction. Afunction f is (mω,Mq)-smooth on Ω, iffor any (p,q) ∈ Ω,
mΩ ∈ R and Mω ∈ R+, E = f (q)-f (P)-Z f (p) ,q - pi satisfies:
T Ilq-p k2 ≤∣e∣≤ M Ilq-pk2.	(6)
2222
Different from (Elenberg et al., 2018), Eq.6 controls only the fluctuation of the function value by
varying the input from P to q and vice versa, regardless of convexity or concavity of f. It allows
broader choices of the classifier's architecture. With γ-weak submodularity (Elenberg et al., 2018;
Santiago & Yoshida, 2020) defined in Appendix C, we establish Theorem 2.
Theorem2 Let Ω乙={(石,b0): |diff(b,B)| ≤ ζ,|diff(b,f^ J ≤ ζ,|diff(石,b0)∣ ≤ ζ,ζ ≥ 1}, where 石 and 石
denote two sets ofattribute changes. Ifthe classifier fyk (ɪ) (k=1,…,K)follows the regularity condition
given by (mk,Ωζ, Mk,Ωζ)-smoothness COnStraint on Ω乙,the pessimistic and optimistic robustness
assessment (with Eq.4 and Eq.5) can be formulated respectively as monotone and non-monotone
yζ-WeakIy Submodularmaximization. Let Ek= fyk (χ,b0)-fyk (ɪ,b)-〈Vfyk(ɪ,b),b^0-b), and Vfy (χ,b)v
denote the elements of V fy (χ,b) corresponding to the difference between b and b0, where v=df(b ,b0).
The submodularity ratio ypessfor the pessimistic robustness assessment is bounded as:
pess	pess
% =*=min,κ{%}	⑺
p pess	kv fyk (x,b)v k2+rnfc,Ωι ∖ζ |/2	pess	2吟％ ∕∣∣τ7 ʃ / 小 ∣∣
where γk<κ,ζ	≥	TVTykeT历吁萨团	,	Ek	≥	0,	yk<κ,ζ	≥	kvTyk(X⑸Vk2(kV∕yk(X'b%k2 -
Mk,Ωι∖ζ|/2) for Ek < 0,偿” ≥ ∣V；长q管)小2(kV卜=K(χ,b%k2- MK,Ωι∖ζ|/2) for EK ≥ 0 and yKes.SS ≥
IkV^yK (:：：；；+^：：1 卮抬 for EK < 0. Similarly, the submodularity ratio £ptm ofthe optimistic assessment is:
tim
optim
k=min,K {&}
(8)
where WOPpim ≤________, kVJyk (X,b)vk2_______ ^r Ek	≥ 0	WOPpim ≤ kVJyk (X⑸Vk2+®,%	1S1/2 f ez,	<0
where yk<K,ζ ≤ 2Mk,Ωζ (kV fy (χ,b)v∣∣2-吟 Ωι	∖ S ∖∕2)	for Ek	≥ θ,	γk<K,S ≤ kV Tyk (χ,b)vk2+Mk,Ω1	∖S ∖∕2 for ek	< θ,
optim	kVfyκ (X力)v k2+^Kq； TS ∖/2	C J optim	∣∣VTyK (x,b)v ∣2
γK,S ≤ ∣VTyK (X,b)v∣2+Mκ,Ω1∖S∖∕2 for ek ≥ 0,andγK,S ≤ 2Mκ,Ω; (∣VTyK (x,b)v ∣2-mκ,Ω1 ∖S∖∕2) for EK <0.
Enjoying the weak submodularity, the pessimistic and optimistic assessment can be derived by greedy
search with provable accuracy. We present a Forward Stepwise Greedy Search (FSGS) algorithm
for deriving the pessimistic assessment (Eq.4). Due to the non-monotone objective function of the
optimistic assessment (Eq.5), a stochastic variant of the standard FSGS, namely Randomized FSGS
(RandGS) is adopted, following the suggestion in (Buchbinder et al., 2014). We give the pseudo
codes of FSGS and RandGS in Algorithm.1 and 2 in Appendix.D. Compared to FSGS, the difference
in RandGS is to randomly select one of the top-ranked candidate features contributing with the
largest marginal gain m于 to the objective function. Theorem 3 gives the approximation guarantee
of the pessimistic and optimistic robustness assessment using FSGS and RandGS respectively.
Theorem 3 For a (mαζ ,M∩ζ) -smooth classifier f, the approximation quality ofthe solution I to Eq.4
and Eq.5 is bounded respectively as
pess	pess
∖m f(/)1+ e Z ≤(1 - e YC )∖m f(GeSS)1 (Pessimistic Assessment)
∖m f (')∖≥ -OIm e~γ^ ∖m 于工 ptim)∖ (OPamiS a CASSeSSmem)	⑼
〃p
where Ress and γoptim are the submodularity ratio in Eq. 7 and Eq. 8, respectively. Γpess and ∕*Ptim are the
true optimal solutions to the two problems.
5
Published as a conference paper at ICLR 2022
With Theorem 3, the Q2 in Section.1 is fully addressed. Notably, any classifier with a finite Lipshitz
constant meets the smoothness condition. For practical estimation, M∩ζ can be computed as local
LiPshitz constant of f if β> 0 in Definition? (or - f (x) if β< 0). Furthermore, mΩζ can be calculated
as the local strong convexity constant of f (x) if e> 0 in Definition.2 (or - f (x) if β< 0). Recent works
(Fazlyab et al., 2019; Jordan & Dimakis, 2021) have shown that a large number of neural networks
are Lipschitz continuous, such as the fully-connected neural networks with ReLU, sigmoid, and tanh
activation functions. The FSGS based robustness assessment method can thus be applied to most
DNN-based classifiers that are popularly adopted in real-world applications.
Theorem 3 also reveals that the role that the smoothness of fy plays in characterizing the solvability of
the robustness assessment problem. As indicated by Eq. 7 and Eq. 8, a smoother fk with smaller m k,Ωζ
and Mk,Ωζ has a higher submodularity ratio (靖eB and γopF). With a submodularity ratio closer to
1, both FSGS and RandGS can obtain better approximation quality to the underlying optimal result
of the robustness assessment, according to Eq. 9. Comparing Eq. 7 to 8, the submodularity ratio of
the pessimistic assessment is higher than that of the optimistic opponent. The greedy search has lower
approximation quality under the optimistic setting. The optimistic assessment is thus generally higher
than the pessimistic assessment. It echos with the intuition that a classifier is more vulnerable to a
knowledgeable adversary than an oblivious one.
3.4	Orthogonal Matching Pursuit Greedy Search for Robustness Assessment
Both FSGS and RandGS need to traverse all the combinations of candidate attributes and the subsets
of the selected set of attributes. In the worst case, they require 2 J=0 ((八-t )*m *2+) objective function
evaluations for T iterations, where n denotes of the number of categorical features in an instance X
and each feature can choose any of the m categorical values. Large n and m are usually witnessed in the
real-world data. Both methods thus are costly in querying the oracle classifier in such real-world cases.
In (Wang et al., 2020), an orthogonal matching pursuit guided greedy searching strategy (OMPGS) is
introduced to address the query bottleneck of primary greedy search in evasion attack on discrete data.
In the context of attack, this method computes V fy (x)z^ (the gradient of fy w.r.t. the binary indicators
B of unmodified categorical features). The magnitude of the gradient measures the capability of each
candidate feature in reducing the classification confidence of the correct label. In this sense, OMPGS
narrows down the candidate attributes to those that best correlate with the orthogonal complement
of the subset of the attributes already selected in greedy search. The query complexity of OMPGS
is thus independent of n and m. Due to the space limit, the algorithm implementation of OMPGS are
explained in Algorithm.3 in Appendix.E.
We apply OMPGS to speed up the solutions to both the pessimistic and optimistic assessment
problems. The approximation guarantee of OMPGS given in (Wang et al., 2020) only explains the
provably accuracy of using OMPGS in the pessimistic assessment. In this work, we further bound
the approximation accuracy of OMPGS to derive the optimistic assessment in Theorem 4. Comparing
to Theorem 3, OMPGS achieves a lower but similar approximation accuracy to the underlying optimal
solution to the optimistic assessment (Eq. 5).
Theorem 4 Following the notations in Theorem 2 and Theorem.3, the approximation quality of the
optimistic assessment score ∣l| obtained by OMPGS is bounded as in thefoUowings:
|m f (l)∣≥ 1 e~ψ |m f(l* ptim)∖, ψ=max{必}(k = 1,2,3,…,K)
_	kVfyk (X,b)v k2	kVfyk (X,b)v k2 +mk,Ωi ∖ζ∖∕2	(IO)
ψk = max{ 2Mk,Ω. (kV fyk (X,b)v k2-m k,Ωι ∖ζ ∖∕2), kV fyk (x,b)v k2 + Mk^ι ∖ ζ ∖∕2 }
In both of the pessimistic and optimistic assessment, the gradient magnitude used by OMPGS can be
interpreted as an upper bound of the increase of m于 by adding adversarial perturbation to a previously
unmodified feature, as shown in Appendix E. Notably, the gradient used in OMPGS is not taken
directly with respect to the categorical features. Instead, the gradient is taken with respect to the binary
indicator variable bji (see Section.3). We rank the categorical features according to the magnitudes of
the gradients with respect to different bʃ. A higher gradient magnitude indicates that the corresponding
categorical feature Xi = XXi can contribute higher marginal gain of m于.OMPGS narrows down range
of greedy search. Only the top ranked candidate features are considered to modify.
6
Published as a conference paper at ICLR 2022
Table 1: Pessimistic and Optimistic Robustness Assessment |〃. A lower/higher assessment score
indicates weaker/stronger robustness level. SR is the success rate of attack. OVF indicates SR=0.
GreedySeearch	P/O	Γ	Yelp		IPS		Splice	
			SR	Med (Avg)	SR	Med(Avg)	SR	Med(Avg)
FSGS/ RandGS	P	-0.4	0.98	-∏T5-	0.80	1(1.2)	0.91	1(1.7)
		-0-	0.98	9)	0.80	1(1.2)	0.91	1(1.7)
	O	-0.4	0.09	9(8.1)	0.33	1(1.2)	-0-	OVF
		-0-	0.09	12(11.8)	0.33	1(1.3)	-0-	-OVF-
GradAttack	P	-0.4	0.84	1(1.5)	0.59	1(1.8)	0.72	-3(3.7)-
		-0^^	0.76	2(1.7)	0.59	1(1.7)	0.72	-3(3.7)-
OMPGS/ RandOMPGS	P	-0.4	0.98	1 (2.0)	0.91	1(1.6)	0.74	-2(2TΓ)-
		-0-	0.97	-2(2.5)-	0.91	1(1.8)	0.74	-2(2.3)-
	O	-0.4	0.63	7(6.9)-	0.35	3(3.4)	-0-	-OVF-
		0	0.52	8(7.6)	0.35	3 (3.6)	0	OVF
4 Experimental S tudy
We instantiate the study with standard LSTM based classifiers trained on three multi-class datasets
collected from real-world applications of Text analysis, Cyber security and Biomedicine.
Yelp-5 (Yelp)(Asghar, 2016). The Yelp-5 reviews dataset was obtained from the Yelp Dataset Chal-
lenge in 2015. We use the reviews containing 650K training and 50K testing samples with the classes
from 1 star to 5 stars for learning classifier. Each word is a categorical feature with 300-dim embedding.
Intrusion Prevention System Dataset (IPS) (Wang et al., 2020). The IPS dataset has 242,467
instances, each of which is a 20-step attack sequence and representedby X ∈ R20*1103*70, because each
step can be a categorical value from 1,103 different malicious actions embedded in 70-dim space by
word embedding. An X is classified with 3 labels: attack type 1, attack type 2, and others.
Splice-junction Gene Sequences (Splice) (Noordewier et al., 1991). This dataset has 3,190 samples.
Each one is a gene fragment of 60 categorical features with values in {‘A’, ‘G’, ‘C’, ‘T’, ‘N’}. Learning
an embedding for each value with 30-dim, one instance is given as X ∈ R60*5*30. The task is to predict
each splice instance as an intro-exon boundary (IE), an exon-intro boundary (EI), or neither of them.
The LSTM-based classifiers with ReLu activation function and dropout achieve accuracy scores
of 0.61, 0.92 and 0.95 respectively for Yelp, IPS and Splice. More details about the datasets and
the experimental setting are given in Appendix F.1. To strengthen the universality of the robustness
assessment framework, we also include additional results on binary-classification datasets using
Convolution Neural Networks (CNN) in Appendix.F.5
The experimental study is to address questions below:
1)	What’s the robustness assessment result of a targeted classifier? (Empirically evaluate the answer
in Theorem 2,3 and 4 to Q2 in Section 4.1). For the classifier trained on each dataset, we first find
the robustness assessment ∣l| for each data instance. We report the median and mean value of all
the |l| derived on the dataset. A lower/higher median value indicates a weaker/stronger robustness
of the classifier to the adversarial attack over the given data set. The tolerance threshold Γ is tested
on -0.4 and 0to assess our proposed assessment method with varied tolerance to adversarial threats in
safety-critical applications. Different greedy search methods are compared on their assessment result.
Besides FSGS and OMPGS (for solving Eq.4) and RandGS and RandOMPGS (for solving Eq.5) , we
also include Gradient-based Attack (GradAttack) from (Qi et al., 2019) in the evaluation. GradAttack
tackles only the pessimistic assessment problem. The comparison with GradAttack is to verify the
merit of FSGS and its variants in solving the robustness assessment problem of a general classifier. For
RandGS and OMPGS, we empirically choose 10 candidate features in each iteration of greedy search1.
2)	Can the information-theoretic characterization of adversarial vulnerability in Theorem 1 be verified
using the proposed robustness assessment? (Empirically evaluate the answer in Theorem 1 to Q1
in Section 4.2 and 4.3). We verify an increase of risk on xιmi with lower mutual information, i.e.,
/ (xιmi ；，S) < / (xj,S), and verify an increase of robustness by reducing / (fy ;S).
3)	We compare the time complexity of FSGS,OMPGS and GradAttack. Furthermore, we evaluate the
averaged running time and the averaged iterations of the three methods over Yelp, IPS and Splice
datasets. As shown, both FSGS and OMPGS stop early the search with less loops than GradAttack.
OMPGS costs much fewer queries and less running time than FSGS. Compared to GradAtttack,
OMPGS has a better chance to select the really useful feature modifications, thus costing fewer loops.
We provide the results in Appendix.F.3 due to the space limit.
1Implementations are available at https://github.com/XYZ211923Y/-RobustXXXXX.
7
Published as a conference paper at ICLR 2022
4.1	Robustness Assessment With Greedy Search
Besides showing the median and mean of the assessment score 111 (Med(Avg)) of the testing instances,
we also report attack success rate (noted as SR in the tables), i.e., the fraction of the testing instances
on which the greedy methods can push m于 greater than the tolerance threshold within the one-hour
time limit. With similar median/mean values of ∣l |, a lower SR also indicates stronger robustness of the
classifier. If the greedy search fails to push m于 to surpass the threshold Γ for allthe testing instances
within the time limit, we note this situation as OVF (an abbreviation for overflow) and SR = 0 in the
tables. P/O denotes the pessimistic or optimistic assessment respectively.
Table 1 shows several interesting findings. First, for both the pessimistic and optimistic robustness
assessment, a lower tolerance threshold Γ results in lower scores over all the datasets. This
observation is consistent with the intuition of setting the tolerance threshold. A lower tolerance
threshold helps choose more worst-case resilient classifiers. Our study allows for the analysis with
different vigilance levels. Second, as expected, the pessimistic robustness assessment scores are
always significantly lower than those of the optimistic assessment, and the SR value is much
lower on the optimistic assessment scenario. Due to lack of query capability and knowledge about
f, it is difficult for the oblivious adversary to locate the sensitive features.
OMPGS and RandOMPGS produce similar robustness assessment scores, compared to FSGS and
RandGS. Compared to evaluate each candidate features as in FSGS/RandGS, OMPGS improves the
search efficiency using the gradient based heuristic guidance. On IPS and Splice, FSGS/RandGS
needs 217.65 and 1123.51 seconds on average on each data sample for the pessimistic and optimistic
assessment. In contrast, OMPGS/RandOMPGS costs only 0.24 and 1.59 seconds for the pessimistic
and optimistic assessment, more than 700 times faster than FSGS/RandGS. GradAttack, in contrast,
produces much lower SR than FSGS and OMPGS in the pessimistic assessment setting. As discussed in
Section.2, the greedy search in GradAttack ignores the combinatorial effect of the candidate features and
the previously modified features, which leads to worse approximation quality and much less efficient
attacks compared to FSGS and OMPGS. The results echo the theoretical analysis. With significantly
lower SR, GradAttack can not provide an accurate answer to the robustness assessment problem.
We conduct one-factor-at-a-time sensitivity analysis on each dataset (Campbell et al., 2008) to measure
the sensitivity of features in the classification task. Given a data instance, we change each feature while
keeping all the others fixed. The averaged change of the classifier’s output over all the testing instances
is used as the feature-wise sensitivity measurement. A larger value indicates that the classifier’s
output is more sensitive to the change over the corresponding feature. We find that the top-sensitive
features also appear as the most frequently selected features by FSGS,GradAttack and OMPGS. Details
about the sensitivity analysis can be found in Appendix.F.4. The interesting overlapping between
the attributes which are useful for attack and the top-ranked sensitive attributes confirms our intuition
about the association between feature sensitivity and adversarial vulnerability of the classifier.
4.2	Robustness vs Feature Informativeness
According to the discussion in Theorem 1, a lower /(x;y,S) indicates a higher adversarial risk on the
input instance (x,y), implying a smaller robustness assessment score. To verify such impact of feature
informativeness, we compare the robustness on X and on XImi that has a lower mutual information,
i.e., /(xιmi;y,S) < /(x;y,S). The sample Xlmi is constructed as those on the half-way of pushing the
m f ofthe original sample X to surpass Γ. For each dataset, we select the instances requiring at the least
two modified features in order to deliver a successful attack. These instances are treated as original.
There are 125, 414 and 324 original instances in Yelp, IPS, and Splice respectively. The results in
Table 2 confirm that the classifier becomes less robust to adversarial modifications over Xιmi samples
in the ambiguous zone, i.e., higher SR and smaller |l| on the Xιmi samples than the original samples X.
4.3	Robustness with defensive techniques
As discussed in Theorem.1, reducing /(fy ;S), the dependence of a classifier over the training set S,
improves the classifier’s adversarial robustness. We tune a classifier via adversarial training (AdvC),
the nuclear-norm based regularization (NuR) following (Ravi et al., 2019) and random smoothing by
(RS) as suggested in (Boj. et al., 2020) to reduce / (fy ;S), and then evaluate if the pessimistic robustness
assessment becomes higher, e..g, descrese of SR. To adopt adversarial training, we solve Eq. 4 using
8
Published as a conference paper at ICLR 2022
Table 2: Robustness Assessment vs Feature Informativeness, reported at Γ = 0.1(Xlmi ;y,S) <1 (x;y,S)
results in higher adversarial risk (higher SR, smaller ∣l|)on XImithan original x.
GreedySeearch	P/O	Sample	Yelp		IPS		Splice	
			SR	Med(Avg)	SR	Med(Avg)	SR	Med(Avg)
FSGS/ RandGS	P	original X	0.98	1(1.9)	0.80	1(1.2)	0.91	1(1.7)
		Xl 一 lmi	1.00	1(1.7)	0.95	1(1.1)	0.99	1(1.2)
	O	original X	0.09	12(11.8)	0.33	1(1.3)	-0-	-OVF-
		Xl 一 mi	0.94	-6(60)-	0.65	1(1.2)	-0-	-OVF-
OMPGS/ RandOMPGS	P	original X	0.97	2(2.5)	0.91	1(1.8)	0.74	-2(2.3)-
		Xl 一 lmi	0.99	-∑1∑4)-	0.93	1(1.3)	0.94	1(1.5)
	O	original X	0.97	-4746-	0.35	-3136)-	-0-	-OVF-
		χι 一 mi	1.00	4(4.5)	0.57	3 (3.3)	0	OVF
Table 3: Pessimistic Robustness Assessment (Γ = 0) after Robustness Enhancement by reducing
/(fy;S). JSR is the the amount of SR decreasing comparing to the SR value with Γ = 0 in Table 1.
Model	Algo.	Yelp			IPS			Splice		
		SR	TSR^	Med(Avg)	SR	1sr^	Med(Avg)	SR	1sr^	Med(Avg)
AdvC	FSGS	0.84	0.13	-Hl.6)-	0.74	0.06	-Hr4-	0.85	0.06	-2(2.1)-
	-OMPGS-	0.83	0.15	-2(2.5)-	0.60	0.31	-2(2.2)-	0.65	0.09	-2(2.8)-
NuR	FSGS	0.72	0.25	-Hl.9)-	0.79	0.01	Hn)	-1-	-0.09	1(1.4)
	-OMPGS-	0.68	0.30	-2(2.4)-	0.63	0.28	-∏T0)-	0.72	0.02	1(2.5)
RS	FSGS	0.76	0.21	-Hl.8)-	1.00	-0.20	Hr4	0.91	-0-	1(1.9)
	-OMPGS-	0.74	0.24	-2(2.2)-	0.95	-0.04	∏ττ	0.78	-0.04	-2(2.2)-
SRS (BOj.et al.,2020)		1.0	-	2	1.0	-	2	1.0	-	1
OMPGS until the value of m于 drops into the interval [-0.2,0.0]. The resultant modified instances are
used for adversarial training. The regularization parameter of the nuclear-norm regularized training
method is chosen following (Ravi et al., 2019). The random smoothing is conducted by randomly
flipping the values of input categorical features (RS) with P- = 0.6 and p+ = 0.01 as suggested in
(Boj. et al., 2020). We also implement the original sparsity-aware random smoothing based certifiable
robustness (SRS) (Boj. et al., 2020) as a reference method of random smoothing based defense
techniques. It performs first the same randomly flipping of categorical features as in RS. The least
number of the features changed to trigger a wrong classification, a.k.a. certifiable robustness, is then
estimated via a dimension-independent evaluation based on the randomly flipped features. SRS (Boj.
et al., 2020) is reported to be significantly more efficient and produce tighter robustness assessments than
other random smoothing-based methods (Lee et al., 2019; Levine & Feizi, 2020; Dvijotham et al., 2020).
Table 3 illustrates the pessimistic robustness assessment scores of the tuned classifiers with the
tolerance threshold Γ = 0. We skip the results of the optimistic robustness assessment and the results
with Γ = -0.4 as they present a similar variation tendency. Both adversarial training and nuclear-norm
regularization help improve the classifier’s robustness, leading to lower SR and higher |l| over all the
three datasets in most of the cases. However, on Splice, the adversarial vulnerability reduction effect
is much less visible than those observed on the other two datasets. We ascribe this phenomenon to the
fact that the feature-wise sensitivity on Splice has a more skewed distribution than the other datasets.
A few categorical features in Splice are significantly more sensitive than the others in the classification
task, corresponding to important proteins dominating the molecules’ properties (see feature sensitivity
analysis in Section 4.1 and Appendix.F.4). The existence of such highly sensitive features makes it
difficult to enhance the targeted classifier’s robustness without losing the classification utility. RS and
SRS have higher SR than the other robustness enhanced models, showing that the random smoothing
technique helps the least improve robustness or even make it more vulnerable to the attack, in the
case where a fraction of categorical features are highly sensitive. As seen in Table.1, highly sensitive
features can be witnessed in many diversified applications. Firstly, flipping these sensitive features
can lead to miss-classification rather than providing robustness. Secondly, if random flipping misses
the sensitive features, the adversary can still target at these features to deliver a successful attack.
5	Conclusion
We unveil the key factors that jointly determine the adversarial vulnerability of a classifier with
categorical inputs via information-theoretic analysis. The characteristics of the categorical data and
the functional property of the targeted classifier act as the external and internal cause of the adversarial
threat. We further set a theoretical association between the smoothness of the targeted classifier and the
solvability of both pessimistic and optimistic robustness assessment with categorical inputs. Despite
of the NP-hard nature of the problem, we show that a provably accurate assessment is approachable
for any classifier using simple yet efficient greedy search. Our study is useful for assessing the model
robustness before deploying itto real-world safety-critical applications. Our future work will pursue
to provide tighter quality bound for assessing adversarial robustness assessment on categorical data.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The research reported in this publication was partially supported by funding from King Abdullah
University of Science and Technology (KAUST).
References
A. Akbarnejad and S. Gunnemann. Adversarial attacks on node embedding via graph poisoning. In
ICML, 2019.
N. Asghar. Yelp dataset challenge: Review rating prediction. arXiv preprint arXiv:1605.05362, 2016.
A. Boj., J. Klicpera, and S. Gunnemann. Efficient robustness certificates for discrete data:
Sparsity-aware randomized smoothing for graphs, images and more. In ICML, pp. 1003-1013,2020.
A. Bojchevski and S. Gunnemann. Certifiable robustness to graph perturbations. In NeurIPS, pp.
8319-8330, 2019.
A. Bojchevski and S. Gunnemann. Adversarial attacks on node embeddings via graph poisoning. In
ICML, pp. 695-704, 2019.
N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. Submodular maximization with cardinality
constraints. In SODA, 2014.
J.E. Campbell, G.R. Carmichael, T Chai, M Mena-Carrasco, Y Tang, DR Blake, N.J. Blake, S.A.
Vay, G.J. Collatz, I Baker, et al. Photosynthetic control of atmospheric carbonyl sulfide during the
growing season. Science, 322(5904):1085-1088, 2008.
J.	Cohen, E. Rosenfeld, and Z. Kolter. Certified adversarial robustness via randomized smoothing.
In ICML, pp. 1310-1320, 2019.
F. Croce and M. Hein. Sparse and imperceivable adversarial attacks. In ICCV, pp. 4723-4731, 2019.
A. Akbarnejad D. Zugner and S. Gunnemann. Adversarial attacks on neural networks for graph data.
In KDD, 2018.
K.	D. Dvijotham, J. Hayes, B. Balle, Z. Kolter, C. Qin, A. Gyorgy, K. Xiao, and P. Gowal, S.and Kohli.
A framework for robustness certification of smoothed classifiers using f-divergences. In ICLR, pp.
656-672, 2020.
D.Yin, K. Ramchandran, and P.Bartlett. Rademacher complexity for adversarially robust generalization.
In ICML, pp. 7085-7094, 2019.
D.Zugner, A.Akbarnejad, and S.Gunnemann. Adversarial attacks on neural networks for graph data.
In IJCAI, 2019.
J. Ebrahimi, A. Rao, D. Lowd, and D. Dou. HotFlip: White-box adversarial examples for text
classification. In ACL, 2018.
E.R. Elenberg, R Khanna, A. G. Dimakis, and S. Negahban. Restricted strong convexity implies weak
submodularity. The Annals of Statistics, 46(6B), 2018.
A. Fawzi, S.M. Moosavi-Dezfooli, and P. Frossard. Robustness of classifiers: From adversarial to
random noise. In NIPS, pp. 1632-1640, 2016.
M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas. Efficient and accurate estimation of
lipschitz constants for deep neural networks. In NeurIPS, pp. 11423-11434, 2019.
T. Florian, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training:
Attacks and defenses. In ICLR, 2018.
J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text sequences
to evade deep learning classifiers. In SPW, pp. 50-56, 2018.
10
Published as a conference paper at ICLR 2022
J. Gilmer, L. Metz, F. Faghri, S. Schoenholz, M. Raghu, M. Wattenberg, and I.J. Goodfellow.
Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
Z. Gong, W. Wang, B. Li, D. Song, and W. Ku. Adversarial texts with gradient methods. ArXiv,
abs/1801.07175, 2018.
M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against
adversarial manipulation. In NIPS, pp. 2266-2276,2017.
Z.Z. Htike and S. L. Win. Classification of eukaryotic splice-junction genetic sequences using averaged
one-dependence estimators with subsumption resolution. Procedia Computer Science, 23:36-43,
2013.
M. Jordan and A. G. Dimakis. Exactly computing the local lipschitz constant of relu networks, 2021.
J. Khim and P.L. Loh. Adversarial risk bounds for binary classification via function transformation.
arXiv preprint arXiv:1810.09519, 2018.
V. Kuleshov, S. Thakoor, T. Lau, and S. Ermon. Adversarial examples for natural language
classification problems. In ICLR, 2018.
G. Lee, Y. Yuan, S. Chang, and T. Jaakkola. Tight certificates of adversarial robustness for randomly
smoothed classifiers. In NeurIPS, pp. 4910-4921, 2019.
A. Levine and S. Feizi. Robustness certificates for sparse adversarial attacks by randomized ablation.
In AAAI, pp. 4585-4593, 2020.
F. Ma, J. Gao, Q. Suo, Q. You, J. Zhou, and A. Zhang. Risk prediction on electronic health records
with prior medical knowledge. In KDD, 2018.
C. A Mack. Elements of Information Theory, 2nd Edition. Wiley, 2006.
T. Miyato, A. M. Dai, and I.J. Goodfellow. Adversarial training methods for semi-supervised text
classification. In ICLR, 2016.
N. Narodytska and S. Kasiviswanathan. Simple black-box adversarial attacks on deep neural networks.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp.
1310-1318, 2017.
M.O. Noordewier, G.G. Towell, and J.W. Shavlik. Training knowledge-based neural networks to
recognize genes in dna sequences. In NIPS, pp. 530-536, 1991.
N. Papernot, P. D. McDaniel, A. Swami, and R. E. Harang. Crafting adversarial input sequences for
recurrent neural networks. In MILCOM, 2016.
L. Qi, L. Wu, P. Chen, A. Dimakis, I. Dhillon, and M. Witbrock. Discrete attacks and submodular
optimization with applications to text classification. In SysML, 2019.
S.N. Ravi, T.Dinh, V.S. Lokhande, and V. Singh. Explicitly imposing constraints in deep networks
via conditional gradients gives improved generalization and faster convergence. In AAAI, 2019.
S. Samanta and S. Mehta. Towards crafting text adversarial samples. CoRR, abs/1707.02812, 2017.
R. Santiago and Y. Yoshida. Weakly Submodular Function Maximization Using Local Submodularity
Ratio. In ISAAC, volume 181, pp. 1-17, 2020.
A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S. Davis, G. Taylor, and
T. Goldstein. Adversarial training for free! In NeurIPS, pp. 3358-3369, 2019.
Z.X. Shi, H. Zhang, K.W. Chang, M. Huang, and C.J. Hsieh. Robustness verification for transformers.
arXiv preprint arXiv:2002.06622, 2020.
A. Sinha, H. Namkoong, and J. Duchi. Certifiable distributional robustness with principled adversarial
training. In ICLR, pp. 4910-4921, 2018.
11
Published as a conference paper at ICLR 2022
Z. Tu, J. Zhang, and D. Tao. Theoretical analysis of adversarial learning: A minimax approach. In
NeurIPS,pp.12259-12269, 2019.
Y. Wang, S. Jha, and K. Chaudhuri. Analyzing the robustness of nearest neighbors to adversarial
examples. In ICML, pp. 5133-5142, 2018.
Y. Wang, X. Ma, J. Bailey, J. Yi, B. Zhou, and Q. Gu. On the convergence and robustness of adversarial
training. In ICML, pp. 6586-6595, 2019.
Y. Wang, Y. Han, H. Bao, Y. Shen, F. Ma, J. Li, and X. Zhang. Attackability characterization of
adversarial evasion attack on discrete data. In KDD, 2020.
T.W. Weng, H. Zhang, H. Chen, Z. Song, C.J. Hsieh, D. Boning, I. S.Dhillion, and L. Daniel. Towards
fast computation of certified robustness for relu networks. In ICML, pp. 4910-4921, 2019.
A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. In NIPS, pp. 2525-2534, 2017.
P. Yang, J. Chen, C. Hsieh, J. Wang, and M. I. Jordan. Greedy attack and gumbel attack: Generating
adversarial examples for discrete data. ArXiv, abs/1805.12316, 2018.
S.C. Zhu, X Zhang, andD Evans. Learning adversarially robust representations via worst-case mutual
information maximization. In ICML, pp. 11609-11618, 2020.
D. Zugner and S. Gunnemann. Certifiable robustness and robust training for graph convolutional
networks. In KDD, pp. 246-256, 2019.
12
Published as a conference paper at ICLR 2022
A Proof for Theorem 1
LemmaI Supposing A and B are two independent random variables and U is given by
PUAB = PAB PU | AB :
/ (U; A)+1 (U; B)≤ 2 / (U; A,B)	(11)
Proof: Given the definition of mutual information / (X ;Y) = H (X)- H (X ∣Y), we need to prove:
2H (A,B |U )≤ H (A |U)+ H (B |U)	(12)
With the chain rule of conditional entropy:
H ( A,B|U) = H ( A|U)+H (B| A,U)	(13)
It indicates that H(A,B|U) ≤ H( A|U), as H(B|U) ≥0. Similarly, we can derive H(A,B|U) ≤ H(B|U)
as well. Combing both inequality relations, Eq.12 holds, which in turn proves Eq.11.
Considering the markov chain in Figure.1, we assume a class label Y and
the training set S are statistically independent. f depends on S as f=A(S),
where A is the training paradigm (could be randomized). The generation
of the adversarial example X0 depends on both the clean feature vector X
and the targeted classifier f. This is consistent with the fact that generating
adversarial samples needs to understand the local geometry property of
the decision boundary of the targeted classifier. Once we get the perturbed
feature vector X0, the classification output Y0 = f (X0).
Figure 1: Markov chain
of adversarial attack
We can derive the lower bound of the miss-classification probability with the perturbed categorical
input X0 following Fano inequality:
RdV = P(y0≠y∣∣dif(b,ι^)∣≤ɛ) ≥ 1 -"Y0;YoS)+ M	(14)
where ) denotes the number of the classes in the learning task. Based on Data Processing Inequality
illustrated in Chapter 2 (Mack, 2006), we can further obtain:
R眇 ≥ 1 - (	；0SK	(i(Y0；Y，s)≤/(X0;Y,S))
1 21 (X ;Y,S)-2η X - / (X 0;Y,S)+const
=1----------------------------------
IogK
where η X = I (X ;Y,S) -1 (X 0;Y,S) denotes the mutual information difference between the unperturbed
X and the adversarially perturbed instance X0. By applying Lemma.1 on I(X0;Y,S) in Eq.15, we have
dv、1	21 (X ;Y,S)-2η X -1 (X 0 ;S )/2
RU	≥ 1
e	IogK
≥ 1 - 2I(X2,S) [ 2ηX_"于;S)/2 (DataPrOCeSSingIneqUaIity,ι(于;S) ≥ I(X0;S))
(16)
B Explanation to Observation.1
First, we follow the IID assumption in the definition of the expected adversarial risk in Eq.1. We
assume I adopts the form of zero-one loss function. If f(x) ≠ y, I(f(x) ≠ y)) = 1. Otherwise, I = 0.
With this setting of the loss function, the expected adversarial risk of the classifier 于 can be formulated
as in Eq.17. This gives Eq.1 (the adversarial risk of f)in the main paper.
R 丝= sup	P [ f (x) ≠ f (X)]	(17)
X〜μx, |diff(x,£) ∣≤ε
where P denotes the probability measurement. In this sense, the expected adversarial risk of RTdV can
be explained intuitively as the maximum miss-classification probability over an adversarial sample X,
with the underlying data distribution μx,y and the attack budget limited by ε.
Second, Theorem.1 provides the information-theoretical lower bound of RTdV with respect to the data
distribution μx,y. Indeed, for any given training data set S sampled from the joint distribution μx,y,
13
Published as a conference paper at ICLR 2022
S can be considered as a random variable. Similarly, given a deterministic training paradigm A, the
classifier f = A( S) (trained with S using the training algorithm A) can be also treated as a random
variable in the hypothesis space. For a testing input (x,y) sampled from the same distribution μx,y as S,
deriving Theorem.1 follows Fano’s inequality in information theory (Chapter 2 in (Mack, 2006)):
•	For any X 〜μX and (x,y)〜μx,y, following Fano,s inequality, We can produce the lower bound
of miss-classification probability over the adversarial sample X generated from X (based on the
information of y and f):
tv”、""" M∙w 7”、2/(Xj,S)-2〃XT(fS)∕2+const
P(f (x) ≠/(x)l X〜μX,∣diff(x,x)∣ ≤ ɛ) ≥ 1-----------；- -------------- (18)
ιogκ
where f (x) = y. The lower bound of the miss-classification probability is composed by three mutual
information measurements among the random variables (xj), S and f.
•	By definition, the following inequality holds:
R外= sup	P[ f (X) ≠ f (X)] ≥ P(J(x) ≠ f (X)| x~μχ,∣diff(x,X)∣≤ ɛ)
X〜MX, ∣diff(X,X)∣≤£
21 (xj,S)-2η χ - / (f ;S)/2+const	(19)
≥ 1
IogK
where (xj)〜μχ,y, S〜μχ,y and f ∈H.
Fromthis perspective, Theorem.1 establishes a lower bound of the expected adversarial risk of f:
Given a deterministic training paradigm A , for any training data set S and any testing input instance
(X,y) sampled from the underlying data distribution μχ,y, the miss-classification probability of f
over the adversarial sample generated from X (based on y and 于)can be bounded by the three mutual
information measurements / (xj,S), ηχ and / (于;S), as in Eq.2 and 3 of Theorem.1 in the main paper.
Comparison with the adversarial risk bound proposed by (Tu et al., 2019). A relevant work of
adversarial risk analysis can be found in (Tu et al., 2019), where the expected adversarial risk ofa
classifier 于 is measured by the maximum classification loss over the distribution of adversarial samples
(Definition.1 in (Tu et al., 2019)):
Radv = E [ sup I(f(x0)j)]	(20)
x,y 〜M2 X ∈N (X)
where N(x) denotes the neighbor set of X derived by modifying features in x. The coverage of the
neighboring set N(x) can be measured by /P distance for continuous data or /° norm for discrete data.
After that, Tu et al’s work achieves two results. First, the expected adversarial risk bound can be
bounded from above by the local worst-case risk of 于(noted as Rs,μx,y):
RadV ≤Rix,y = . E	I(J(X)J)
X ~μx ,μx 〜BWP (μx,s)
(21)
where μX is a distribution sampled from the P-WaSSernStein ball (P ≥ 1) centered at μX and with a
radius of ɛ. N(x) ⊆ μX. Second, the expected adversarial risk R£dv can be bounded by the upper bound
of the local worst-case risk of 于 consisting of the rademarcher complexity of 于 and the empirical risk of
于,according to Theorem.1 and Remark.3 in (Tu et al., 2019). This upper bound holds with probability
at least 1 一 6:
Radv ≤R维V + 2<r("(f))+ "J"1")
. I :	n	(22)
R M = - ∑maχ I (f (XfJt))
"H 占 x'~"x
where RSdV denotes the adversarial empirical risk calculated using n IID samples from the adversarial
data distribution μX. Practically, RSdV can be estimated by calculating the maximum classification
loss reached by perturbing X within the attack budget. M denotes the maximum output value of 于.
<n ("(于))denotes the expected rademarche complexity of an augmented function defined on f. The
value of <n ("(f))is independent from the empirical adversarial risk RSdV. The formulation of
<n ("(于))can be found on Page 10of (Tuetal., 2019).
14
Published as a conference paper at ICLR 2022
Our work differs from the results in (Tu et al., 2019) from the following perspective. The core
contribution of (TU et al., 2019) is to associate the Rademacher complexity of the classifier f with f 's
expected adversarial risk. Beyond that, for a classifier adopted in a learning scenario, we believe both
the characteristics of input data and the classifier’s complexity measurement determine its adversarial
risk. As shown by the mutual information-based lower bound (Eq.2 and 3 of Theorem.1), the mutual
information / (f; S) measures the dependence of a trained f over the training data S sampled from
μx,y. /(于;S) can be considered as the lower bound of the VC-dimension of f (XU & Raginsky,
2017; Zhu et al., 2020), which encodes the classifier’s complexity. The other two mutual information
measurements / (x;y,S) and η X relates to the characteristics of the input data (x,y). Intuitively, / (x;y,S)
measures the informativeness of X for classification. A lower /(x;y,S) denotes weaker consistency
between the input data instance (x,y) and the training data set S. ηX reflects the sensitivity of the
modified categorical features during the evasion attack. A classifier with input data containing more
sensitive features is more likely to be compromised by modifying intentionally the values of the
sensitive feature dimensions.
Relations to the proposed robustness measurement. Since it is not feasible to directly compute the
expected risk R啖L we empirically evaluate the adversarial risk/robustness and presented the results in
Table 2 and Table 3 in Section 4.2 and 4.3. The empirical evaluation of the adversarial risk evaluation
of f over the data distribution μx,y is computed as a sample average,
…	14
Radv(f) = ~∑ …,max	I(f (Xi),yi).	(23)
n £1 Xi, ∣diff( Xi ,χi )∣≤ £
To facilitate computing, we adopt a smooth loss I asa surrogate function approximating the zero-one
loss. In Eq.(23), max	I(f (Xf ),yi) is to estimate the maximum classification loss of f over
Xi, ∣diff(Xi,Xi)∣≤£
the adversarial sample x^i, which is generated by modifying at most ε categorical features of Xf. By its
nature of set function maximization, Radv (f) is calculated in two difference settings: the pessimistic
and optimistic robustness assessment defined in Eq.4 and Eq.5 in the main paper. When setting the
threshold Γ = 0 in Eq.4 in the main paper, the pessimistic estimate ψpess (defined by fyk - fyκ) is
proportional to the classification loss, and gives the miss-classification confidence of the target classifier
at X with the attack budget limited by ε. this “miss-classification confidence” is the empirical estimator
of the adversarial risk f has a given input x, or called robustness.
C Proof for theorems on the weak submodularity
We introduce the notion of γ-weak submodularity used in Theorem 2.
Definition 3 Monotone and γ-weakly SubmoduIar function (Santiago & Yoshida, 2020). A Set
function g(S) is monotone, iffor every T ⊂ S, we have g (S) ≥ g(T). Given a set cardinality threshold
k ≥ 1 andasCalarValue γ ∈ [0,1]. We define Tk of g (.) w.r.t. a set S as:,
颊	^a ∈ ,g (S ∪ 3g(S)
S ⊆H,A: | A∣≤k,S∩A=0	g (S ∪ A)-g (S)
(24)
If g(S) is monotone and Tk > T for any k , g(S) is a T-submodular function. Otherwise, g(S) is
T-submodular, if g(S) is non-monotone but g(S)followsforany k :
2 g (S ∪a)-g (S)≥{γ (g (S ∪ A)-g ⑸),1∕y (g (S ∪ A)- g ⑸)}	(25)
a∈A
It is easy to find, if g(S) is monotone and T ≥ 1, g(S) is submodular.
Proof of deriving the robustness assessment score under the pessimistic scenario in Eq.7: we first
formulate pessimistic scenario as a set function optimization,
S*= max{	max	[Fk (S)} + (G K (S))}
S k=1,2,3,...,K-1
Fk (S) = max fk (历)
Z ⊂S
G k (S) = max fκ (历)
Z ⊂S
(26)
15
Published as a conference paper at ICLR 2022
1	1 1	,	, 1	1 ∙ Γ'	C∙	∙ 1 C∙	∙ 1 ∙ , 11 , 1 ∙ 1	, 1 7' / 1 ∖	C/ T ∖
where 历 denotes the modification of categorical features indicated by the index set I. fκ (历)=-fκ (6).
For each k ∈ 1,2,3,...,K, E左=fk(q)- fk(p)-"fk (p),q -pi∙ Fk(S) and GK (S) are monotonically
non-decreasing set functions.
Supposing Ek ≥ 0 for each k ∈ {1, 2, 3, ..., K - 1}, we assume further that the features indicated
by 10 are modified in addition to I, with ∣l0| ≤ ζ. For any 历,bι∪ι> and b↑∪/ denote the additional
modification over the features indicated by 10 and by j respectively to increase the output of fk
(increasing the miss-classification decision confidence) and - fκ (decreasing the confidence of correct
classification). In the following analysis, we relax the discrete indicator b to the continuous domain, as
each bi ∈ [0,1]. V fk (b) denotes the gradient of the classifier function fk with respect to the variable
b at b = bι. Modifying from b to b∕∪y or b∕∪/ follows the direction of gradient ascent, which gives
〈V fk (bι) ,bι∪ιo-bl i≥ 0 and〈V fk (bι ),bι∪ j-b. i≥ 0,〈V fκ (bι )b∪jl i≥ 0 and〈V JK (bι ),bι∪ j-b. i≥ 0.
fk (bι∪ι0)- fk (bι) ≤〈V fk (bι),bι∪ι0- bιi + -2^-l- k bι∪ι0- bι k2
≤kV fk (bι) C k2+^k^∣ζ∣
mk Ω1	(27)
»k( bι∪ j )-fk ( bι )≥2<V fk ( bι ),bι∪ 厂 bι i + Y k bι∪ 厂 bι k2
j ∈乙	j父
≥kV fk (bι) C k2 + T^2ω | ζ |( fk isnon-decreasing)
We can derive the lower bound of the submodularity ratio 丁k,c of fk:
=2j∈c加bι∪j)-fk(b) > kV加b)ck2 + 空产∣ζ∣
'4	fk MUV)Tkibι)	^kV fk ( bι )c k2 + H∣ζ∣
Supposing Ek <0 for each k ∈ {1,2,3,...,K -1}, we can derive:
fk (bι∪ιo)- fk (bι )≤<V fk (bι ),bι∪ιo-bι i-T^ k bι∪ιo- bι k 2 ≤ -ɪ kV fk (bι) C k2
2	2 2t k,Ωζ	2
X fk (bι∪ j)- fk (bι )≥ 2"V fk (bι ),bι∪ 「 bι i-驾包 k bι∪ 「 bι k2 }	(29)
j∈C	j∈C
≥ kV fk (bι )ω∕∣2 - M, ∣ζ |( fk is non-decreasing)
Given the smoothness assumption on the targeted classifier (See Definition.2), there exits a value of
Mk,Ωι | ζ |/2 ≤∣∣V fk (bι )c k2,which allows that ∣∣V fk (bι )ζ k2 - ^ω | ζ | ≥ 0 holds. Therefore, we can
derive the lower bound of the submodularity ratio of Fk and -GK, 7k,c (k=1,2,3,...,K):
=	2(∖∪；① ≥	(kV∕k(b)∕2-Mk,Ωl∖ζ∖∕2)	(30)
Jk(bι∪ι0)-Jk(bι)	kV fk (b)M∣2
The submodularity ratio yc on Ωc in Eq.26 is 7c =	min	{ψk Ωζ }.
°	°	° k = 1,2,3,...,K	，
Proof of deriving the robustness assessment score under the optimistic scenario in Eq.8. Following the
setting of Eq.26, we can formulate the robustness assessment problem at the optimistic scenario (with
an oblivious adversary) :
... - . -~ -
S = max{G K (S)+	min	{Fk( S)}}
S	k=1,2,3,...,K-1
Fk (S) = min Ak (b)
ι ⊂S
G k (S) = min J⅛ (b)
ι ⊂S
(31)
Both Gk(S) and Fk(S) are non-increasing with hk (bι) = -fk(bι). Eq.31 thus definesanon-monotone
set function maximization problem (See Definition.3). We conduct the analysis as in Eq.27 and Eq.29.
Modifying bι to bι∪ι and bι∪/ follows the direction of gradient descent, which gives〈V于K (b),bι∪/ -
bι i≤ 0 and hV fκ (b ),bl∪l,-bι i≤ 0,〈Vhk( b ),bι∪ j - bι i≤ 0 and Zhk (b ),bι∪ι,-bι i≤ 0.
16
Published as a conference paper at ICLR 2022
Supposing E ≥ 0, we can derive:
	fκ (bι∪ j)-fκ (bι )≥(V fκ (bι ),bι∪ 厂 bι〉+ mκ旦 k bι∪ 厂 bι k2 fκ (bι∪ j)-fκ (bι )≤(V fκ (bι ),bι∪ 厂 bι〉+ 丝罗 k bι∪ 厂 bι k2 m κ∣	2	(32) fκ(bι∪ι0)- fκ(bι) ≥〈Vfκ(bι),bι∪ι0-bι〉+ —2— kbι∪ι0-bι∣∣2 Mκq?	C fκ(bι∪ι0)- fκ(bι) ≤〈Vfκ(bι),bι∪ι0-bι〉+ —2— kbι∪ι0-bι|卜
and	hk (bι∪ j)-hk (bι )≤(Vhk (bι ),bι∪ 「 bι〉-m2l k bι∪ 「 bιk2 hk (bι∪ j)-hk (bι )≥(Vhk (bι ),bι∪ 「 bι〉-Mk曳 k bι∪ 「 bιk2 m 晨	2	(33) hk(bι∪ιo)-hk(bι) ≤〈Vhk(bι),bι∪ι0-bι〉	2— kbι∪ι0-bιk2 hk (bι∪ι0)-hk (bι)≥hvhk (bι ),bι∪ι0- bι>-警k bι∪ι0- bιk2
From Eq.32, we conduct the similar analysis as in Eq.22 and 23, which gives the submodularity ratio at
the optimistic case:
，fκ(bι∪j)-fκ(bι) ≥ “*；/" -IlVfκ(bι)乙∣∣2
J ∈ι0	(34)
fκ ( bl∪l0)- fκ ( bl) ≤	K，2"" -kv fκ ( bl) 乙 I∣2
Σ^k (bι∪ /)-h k (bι )≥ ZhV版 (bι ),bι∪ J - bI〉-^ω~ k bι∪ J - bιk2 ≥-'1用-k^hk (bι) 乙 l∣2
hk(bι∪ι0)-hk(bι) ≤ (Vhk(bι),bι∪ι0-bι〉------kbι∪ι0-bι∣∣2 ≤ ------*；"" TlV阮(bι)乙∣∣2
(35)
LetYk ∈(0,1) and — YII乙| 一||Vhk(bι)乙∣∣2 ≥	(T^ld£1 -∣Vhk(bι)乙∣∣2)∙Thesubmodularityratio
mfc,Ωz % I
«7	/	2 +IV阮(E)ζ ∣∣2
of hk, Yk ≤	--------—.
'ω +IV阮(瓦3∣∣2
Proofs to Theorem.3 (Eq.9). In the pessimistic scenario, assessing adversarial robustness can be
formulated as a problem of monotone weakly submodular function maximization. Assuming that
m 于 (I) is the solution I obtained by the FSGS method and m 于 (I *) is the optimal solution I * to Eq.4.
Based on Theorem.3 in (Elenberg et al., 2018), we can obtain:
1-Im f (I )∣≥(1-∣m f (∕*)∣)(1-e-*r	(36)
Therefore, we can derive the first inequality (the pessimistic scenario) in Eq.9
∣m/(/)∣+ e"yΓss ≤(1-eτχΓs')∣m/(∕*)∣	(37)
Assessing adversarial robustness in the optimistic scenario (Eq.5) is in nature a problem of non-
monotone weakly submodular function maximization. Assuming that m于 (I) is the greedy search based
solution obtained by the RandGS (Randomized Greedy Search) and m于(/*) is the optimal solution to
Eq.5. Applying Theorem 1.9 (Santiago & Yoshida, 2020) directly, we can obtain:
∣mf (∕)∣≥	e-丐M[m∕ (∕*)∣
Y£
(38)
The proof to the approximation gap of OMPGS in Eq.10 follows the proof to Theorem 2.4 and Theorem
3 (Santiago & Yoshida, 2020).
17
Published as a conference paper at ICLR 2022
D Algorithm pseudo codes for FSGS and RandGS based robustness
ASSESSMENT
Algorithm 1 FSGS for the pessimistic robustness assessment
Input: The candidate set H={ai }i=ι...n of all modifiable features
Output: The maximum support set Ii pushing m于(Ii) to violate the tolerance constraint in Eq. 4
1:	Io <— 0
2:	for t = 0,1,2,…,夕 do
3:	T J zero-valued vector ∈ RN
4:	for each αr∙ ∈ H/lt-ι do
5:	T(ai) = argmax m于(S∪αr∙) (Inner level Optimization)
S ⊂^t-1
6:	end for
7:	a = argmax T(αr∙) (Outer level Optimization)
网
8:	li Jli-1 ∪{a}
9:	m f (li)jm f (li-ι ∪a)
10:	if mf (li) ≥Γ then break
11:	end for
Algorithm 2 RandGS for the optimistic robustness assessment
Input: The candidate set H={αi }i=ι...n of all modifiable features
Output: The maximum support set li pushing m于(li) to violate the tolerance constraint in Eq. 5
1:	l0 J 0
2:	for t = 0,1,2,…,夕 do
3:	T J zero-valued vector ∈ RN
4:	for each ai ∈ H/lt-1 do
5:	T(αi) = argmax m于(S∪αr∙) (Inner level Optimization)
S ⊂ /t-1
6:	end for
7:	M ⊂ H/lt-1 be a subset ofsize 丁
8:	minimizing Eai∈mT(αr∙) (Outer level Optimization)
9:	Let α be a uniformly random elementfrom M
10:	Let M(α) be the subset corresponding to α
11:	lijli-1 ∪{α }
12:	mf (li)jmin{m于(α∪W(α)),m于(l1)}
13:	if mf (li) ≥Γ then break
14:	end for
E Orthogonal Matching Pursuit Greedy Search
The pseudo-codes of OMPGS is presented in Algorithm 3, which explains how itis adopted to solve
Eq.4 and Eq.5. In each iteration, for each subset S of the previous support set lk-1, We compute the
gradient of A With respect to bs (binary indicator matrix b with the entries in S changed). The top-k0
attributes with the largest magnitudes in the gradient vector are selected to form a candidate set S We
can then find out the optimal attribute j ∈ S to extend for each subset S and record the optimal attack
objective value g(lk-1 ∪ j). In the outer iteration, we choose finally the attribute j* producing the
largest marginal gain to add into lk-1.
The worst case cost of objective function evaluation in each iteration of OMPGS is bounded by
O ((231 C* ))∣ 左 0I. We adjust k0 to achieve a trade-off between enlarging the search range of greedy
search and the cost of function evaluation. Usually k0 is much less than |H|一|lk |, especially when H is
significantly large (e.g. H >10k). Thus OMPGS runs significantly faster than FSGS.
We further illustrate the rationality of using the orthogonal matching pursuit step in OMPGS to solve
the weakly submodular maximization based assessment problem. In Eq. 39, we unveil that the gradient
18
Published as a conference paper at ICLR 2022
Algorithm 3 Orthogonal Matching Pursuit based Greedy Search (OMPGS)
Input: The attack budget K, the set function based attack objective g(I) and the set H = {(i,j),i =
1...n,j = 1...rn} of all the modifiable discrete attributes selected support set I k,with 限 | ≤ K;
Output: g (Ik) and the optimal subset of Ik achieving the attack goal So <-0
1:	for k = 1,2,…,K do
2:	T = 0
3:	for S ⊂ Ik-ι do
4:	厂-V fy (%)
5:	S = {JIj2,…Jko}— argmax |<e/,r> |
J ∈{H/及-1 }
6:	/-argmaxg (I k-ι∪{ j })
J ∈8
7:	T = T ∪{( j,g (I k-i∪ j})}
8:	end for
9:	j *—argmaxg (I k-ι∪{ j })
j ∈τ
10:	Ik-lk-1 ∪{ j *}
11:	endfor
magnitude of fy with respect to the binary indicator variables bji can be used as an estimator of the
marginal gain of m于 in each iteration of the greedy search of OMPGS.
We assume that b and b0 denote two sets of categorical attribute changes. b indicates the category value
assignment of an unperturbed data instance x. ∣diff (b,b^ )| ≤ ζ, ∣diff (b,b 0)∣ ≤ ζ, ∣diff (b ,b 0)∣ ≤ ζ,ζ ≥ 1.
The targeted classifier is denoted as fyk (x) (k=1,…,K), in which K is the correct class label.
I fyk (^^,b°)- fyk (^^,b)∖≤ max{ --* 1— ∣∣V fyk (x,b )v ∣∣2, kV fy k (x,b )v ∣∣2 + Mk^.ζ ∣ζ 1/2}
2m k
J	(39)
1
∖ fyκ (Xb)-fyκ (x,b )∖≥ min{—-∣∣V fyκ (x,b )v ∣∣2, ∣V fyκ (x,b )v ∣∣2 +m k,Ωζ ∖ζ ∖∕2}
2MK,z
where v = diff( b ,b0).
F Experimental Evaluation
We first introduce the evaluation datesets in section F.1, the experimental setup in section F.2, and
the complexity analysis and running time in section F.3. Besides the multi-class datasets Yelp, IPS
and Splice, we also include another two binary classification datasets collected from Cyber Security
and HealthCare in the experiments. We add the feature sensitivity with the multi-class datasets Yelp,
IPS and Splice in F.4 as the supplementary information to the main text. We add the the robustness
assessment with the binary-class datasets PEDec and EHR in F.5. It includes the summarised table on
the robustness assessment and the information-theoretic characterization of adversarial vulnerability
verified in the experiment.
F.1 Dataset information
Yelp-5 (Yelp)(Asghar, 2016). We use the Yelp-5 provided in the torchtext package contains 650,000
training and 50,000 testing textual samples. In the classifier training process. We learn a 300-
dimensional embedding for each word. Thus, each sentence is encoded as X ∈ Rn*300, where n is the
number of words. we treat every word as a categorical feature and put this data into the LSTM model
with five level rating classes of the reviews as the labels. The task is to predict the 5-class rating of each
review. After learning the classifier, we start to attack the categorical input with modifying the words
with those semantic neighbor words, which can contain the sentence is semantically correct. Here not
all the words have neighbors. Here we choose the 200 data from the testing data for attack.
Intrusion Prevention System Dataset (IPS)(Wang et al., 2020). An adversary of network intrusion
performs a series of actions to compromise the targeted devices. We collect one day of IPS records
via a security service vendor from 242,467 endpoint devices. After removing repeated instances, the
19
Published as a conference paper at ICLR 2022
collected data set contains 4,101 time series of attack events. Each sequence instance is composed by
20 attack steps. On each attack step, the adversary can choose one of 1,103 different malicious actions
registered as highly threatening ones. Thus one data instance of IPS data is given as X ∈ R20*1103*70
according to the definition given in Section.3. Each of the 1103 attack actions is projected to a 70-
dimensional embedding vector. In our study, each sequence instance is used as input to the classification
system, which predicts the most likely attack operation conducted at the immediately successive step
of the sequence. We randomly select 80% of each dataset for training and others for testing. Based on
the prediction output, security analysts can proactively take prevention actions. We focus on predicting
the occurrence of the two most threatening actions related to recently uncovered vulnerability. We thus
study a 3-class classification task: the 2 highly malicious actions and all the others as the third class.
All the privacy-related information, such as names of the devices, IP addresses and hosting machines’
domain names have been removed and anonymized.
Figure 2: RNA synthesis and translation into pro-
tein.
Splice-junction Gene Sequences (Splice) (No-
ordewier et al., 1991). Splice junctions are the
points on a DNA sequence at which redundant
DNA segments are removed during the process
of protein creation. As shown in Figure 2 (Htike
& Win, 2013), a precursor mRNA contains exons
and introns. During splicing, introns get spliced
out while exons get reunited. It is then important
to analyze the role of splice junction sequences
in mRNA. The Splice dataset contains 3,190 sam-
ples from Genbank release 64.1. Each of the
instance is a gene fragment of 60 categorical features, which consists of a window that covers 30
nucleotides respectively before and after each donor and acceptor site. Each categorical feature can
take one among ’A’, ’G’, ’C’, ’T’ or ’N’(stands for ambiguity). In our work, Each category is cast to a
30-dimension embedding. Then one data instance of the dataset is given as X ∈ R60*5*30. The dataset
defines a 3-class classification task. Each gene instance can be an intro-exon boundary (labelled as IE),
an exon-intro boundary (labelled as EI), or any other cases (labelled asN). There are 768 IE instances
and 767 EI instances in total. All the rest are N instances. In the published dataset, the data records are
anonymized. We randomly select 80% of each dataset for training and others for testing.
Windows PE Malware Detection (PEDec). We build a well-balanced dataset composed by full
dynamic analysis reports of 20000 benign and malicious PE executables. This dataset is used for a
binary classification scenario ofPE malware detection. We collect 10,972 malware samples of 152
families randomly selected from those submitted to VirusTotal between 2018 and 2020. Each of the files
https://www.overleaf.com/project/5f81ef055ce9280001ecdee8is classified as malicious by more than
21 antivirus engines. We further use AVClass to ensure that no malware families are over-represented
in the collected data. For the benign samples, we choose the fresh installation packages of all the
community-maintained packages of Chocolatey, where the package submissions go through a strict
review process to exclude malicious and pirated software. Therefore we obtain 9028 benign samples of
third-party and Windows system files. Each executable of the dataset is encoded into a binary feature
vectors with 5000 signatures selected by human experts. Each signature covers presence or absence
of specific windows API calls, URL access, registry table edits, file I/O operations and the status of
IP ports. The ensemble of the signatures’ binary states form a description of the file’s behavior in
execution within a sandbox. In our study, we use the 5000 binary codes directly as the features of a
file instance. Simple as it is, the combination space of these signatures can still induce prohibitively
expensive search cost, which prevent brute-force attacks. We randomly select 80% of each dataset for
training and others for testing.
Electronic Health Records (EHR) (Ma et al., 2018). The real-world EHR dataset consists of time-
ordered medical visit records of 7314 patients. Each patient has from 4 to 200 medical visits. Each
visit record is composed by a subset of 4130 discrete ICD9 diagnosis codes2. Each diagnosis code
represents occurrence of a disease, a symptom, oran abnormal finding. Using the historical EHR data
of patients, we can predict the risk of patients suffering the target diseases. In this experiment, our
target is a binary classification task: we forecast whether a patient will suffer heart failure disease in the
future. In our experiments, a data instance of EHR data set is organized as atensorX∈ R200*4130*70 with
2http://www.icd9data.com/
20
Published as a conference paper at ICLR 2022
each of the 4130 diagnosis codes projected to a 70-dimensional embedding vector. For the patients
with less than 200 visits, We pad the empty observations by setting the corresponding b3i = 0.
We split randomly each dataset into two non-overlapped subsets: 80% of them are used for training and
the left 20% form a testing set. In the testing set, we choose the correctly classified testing samples with
the strongest probabilistic confidence provided by the classifier to perform evasion attack. There are
thus 125, 500, 324, 3000 and 414 instances of Yelp, IPS, Splice,PEDec and EHR used in the evasion
attack scenario. On PEDec and EHR data, since the discrete feature takes binary values, we focus on
adversarial modification by simply flipping the codes. For Yelp, IPS and Splice, the adversarial attack is
conducted by replacing the original category values of a targeted discrete attribute with a new one.
F.2 Experimental Setup
We instantiate the study of robustness characterization and assessment to the popularly deployed
Convolution Neural Networks (CNN) and Long Short-Term Memory (LSTM) models. For PEDec
dataset, we adopt a simple CNN model composed of one convolution layer followed by two linear
layers. The rest datasets contain sequential instances, we thus apply standard LSTM as the classifier
in the experiments. Without loss of generality, we use ReLu activation function in both the CNN
and LSTM classifier with the dropout module. Theorem.2 applies in both cases. Both models define
Lipschitz-continuous decision functions, which meets the smoothness constraint given in Definition
2. For the instances used for robustness assessment, the CNN and LSTM classifier achieve accurate
classification over all the datasets. They achieve accuracy scores of 0.90, 0.95, 0.94, 0.93 and 0.62
respectively for the datasets IPS, Splice, PEDec, EHRand Yelp.
In Section 4.1, we demonstrate robustness assessment of the two classifiers over all the datasets with
varied tolerance levels against adversarial perturbation. We set the robustness threshold Γ to -0.4 and 0
respectively. The former defines a low tolerance to the decision perturbation caused by adversarial
attack. A minor fluctuation of the classifier’s output triggers an alert of potential evasion attack. In
contrast, the latter setting only raises an alarm when the classifier produces an ambiguous decision
close to the boundary or report a miss-classification incident. Furthermore, we show the derived
robustness assessment scores with respect to an oblivious adversary (OA,the optimistic assessment)
and an knowledgeable adversary (KA,the pessimistic assessment). The purpose is to show that 1) our
proposed robustness assessment method can adapt varied tolerance settings to adversarial threats
in safety-critic applications; 2) we aim at confirming the impact of different query capabilities of the
adversary over the robustness assessment scores, as revealed by Theorem 2. For the instances used for
robustness assessment, we report the median and mean of the derived robustness assessment scores.
Both FSGS/RandGS and OMPGS are used to calculate the robustness assessment scores. They verify
the effectiveness of greedy search for robustness assessment with categorical input. We implement the
empirical study using the Python library PyTorch and conduct all the experiments on Linux server with
2 GPUs (GeForce 1080Ti) and 16-core CPU (Intel Xeon). For RandGS and OMPGS, we empirically
set the number of candidate attributes in each iteration of greedy search to be 10 globally for all the
datasets.
F.3 Time complexity and running time comparison
We provide the time complexity analysis in Table.4. We assume a data instance X containing N
categorical features and each feature can choose any of the M categorical values. Furthermore, we
assume FSGS, OMPGS and GradAttack all run T iterations. Table.4 shows the query complexity of
these methods of T iterations.
In the table, KOmPgS for OMPGS denotes the number of top-ranked candidate features in each iteration.
OMPGS then selects one feature from the top KOmPgS candidate features. The selected feature achieves
the best attack performance by combing this feature and any subset of the already modified features.
Kgrad for GradAttack denotes the number of the candidate features selected to modify in each iteration.
As seen in Table.4, FSGS has the largest query complexity in each iteration. Enjoying the heuristics
provided by the orthogonal matching pursuit step, OMPGS locates simultaneously the top-ranked
candidate features and the categorical values of these candidate features that are likely to bring the
most increase of m于.Therefore, OMPGS can reduce the query complexity significantly compared to
FSGS. GradAttack does not evaluate the gain of m于 on the combination of the candidate features and
21
Published as a conference paper at ICLR 2022
Table 4: Query complexity of FSGS,OMPGS and GradAttack
Assessment Methods	Query complexity
FSGS	∑'t0 ((N τ)* M *2)
OMPGS	Σ∕-=0 (KOmPgS *2 )
GradAttack	T *∑3θ (嗓grad* M 尢1
Table 5: Averaged running time of FSGS, OMPGS and GradAttack on successfully attacked data
instances (measured in seconds)__________________________________________
Greedy Search	Yelp	IPS	Splice	EHR
FSGS	1.62	263.8	13.7	446
OMPGS	0.82	0.43	0.08	27.44
GradAttaCk	0.18	10.60	1.07	4.05
Table 6: Average number of iterations of FSGS, OMPGS and GradAttack on successfully attacked data
instances	_________________________________________________
Greedy Search	Yelp	IPS	Splice	EHR
FSGS	1.9	~4A~	-30-	1.7
OMPGS	2.5	~^Γ	-52Γ~	2.0
GradAttack	38.6	~33~	4.8	8.0
the subsets of the already modified features. The query complexity of GradAttack in each iteration only
depends on Kgrad and M.
We also show the running-time evaluation of these methods in Table.5 and Table.6. In both tables,
we focus on the pessimistic robustness problem and set the threshold Γ = 0 in Eq.4 in the main paper.
Table.5 records the averaged running time for computing robustness assessment results over the input
data instances where the evasion attack is delivered successfully, i.e. m于 increases above 0. Table.6
shows the average number of iterations required over the input instances to make m于 increase over 0.
As seen in the two tables, FSGS stops the loops of greedy search generally earlier than OMPGS and
GradAttack, while costing the largest running time. The observation is consistent with the design of
FSGS. On one hand, FSGS traverses all the candidate features and chooses the one that most improves
the marginal gain. Compared to OMPGS and GradAttack, FSGS has a better chance to achieve higher
marginal gain of m于 in each iteration. Therefore, FSGS requires fewer loops in the greedy search and
modifies less number of features to solve the robustness assessment problem. However, on the other
hand, FSGS makes larger query costs in each iteration of the greedy search. The running time of FSGS
is thus higher than the other two methods.
On the contrary, GradAttack runs fast but needs significantly more iterations in the search loops. In
each iteration, GradAttack only needs to evaluate the combinations of the Kgrad features selected
from the candidate set and choose the combination contributing the best increase of m于.It requires
much fewer queries than FSGS, as it only searches a subset of the combinations that FSGS looks into.
However, GradAttack likely results in less optimal solutions in each iteration. Therefore, GradAttack
needs more loops to extend its search range.
OMPGS achieves the best trade-off in between. The orthogonal matching pursuit step adopted in
OMPGS helps narrow down drastically the search range while guaranteeing the optimality of the
features in the reduced search range. Compared to FSGS, OMPGS stops early the search with a similar
number of loops, yet costing much fewer queries and less running time than FSGS. Compared to
GradAtttack, OMPGS has a better chance to select the really useful feature modifications, thus costing
fewer loops.
F.4 Feature Sensitivity
To further understand the association between the feature sensitivity and the derived robustness
assesment scores, we conduct one-factor-at-a-time sensitivity analysis (Campbell et al., 2008) to
measure the sensitivity of features. Given a data instance, we change each feature while keeping all
the others fixed. The averaged change of the classifier’s output over all the testing instances is used as
the feature-wise sensitivity measurement. A larger value indicates that the classifier’s output is more
22
Published as a conference paper at ICLR 2022
Table 7: The feature sensitivity of different datasets
DataSet	Tοp3			Tοp5		
	max	average	min	max	average	min
IPS	1.00	-053-	0.00	1.00	-053-	0.00
Splice	1.00	0.38	0.00	1.00	0.33	0.00
Table 8: ToP sensitive features Vs frequently perturbed features
DataSet	Yelp	IPS	Splice
Sensitive	not, but, we, could,They	19,0,17,18,16	29,28,30,31,27
Perturbed	day,never, They,good,not	19,0,1,2,17	29,28,30,31,32-
sensitive to the change over the corresponding feature. In Table 7, we show the maximum, average and
minimum decrease of the probabilistic output for IPS and Splice, induced by changing the top-3 and
5-ranked features.
First of all, we can find that the feature sensitivity of IPS lies in a narrow interval lower than all the
other datasets. This resonates with the larger robust assessment scores in Table 1 and 2. Second,
Splice also has a narrow interval with high sensitivity values, indicating a potential high vulnerability.
Third, within dataset IPS and Splice, the sensitivity levels vary strongly, between the max and min
value. That implies, changing the top sensitive features only is enough to deliver a successful attack. In
real-world applications, the existence of such highly sensitive yet sparse sensitive attributes is widely
witnessed. For example in IPS, the top-5 ranked features are relevant to the key signatures of network
traffic flows, and they are thus usually used by human security analysts to filer out suspicious incidents.
The rest of the features in IPS are not explicitly relevant with the network I/O behaviours, which are
less informative in the classification task. In Splice, the top-5 sensitive features are found as the gene
segments around the spliced junctions. They are particularly sensitive to the gene types. The rest of the
features show little physical significance in the gene splicing process.
We also find that the top-sensitive features also appear as the most frequently selected features by
FSGS,GradAttack and OMPGS. We further count the frequency of appearance for each feature selected
in the pessimistic robustness assessment and compare them with the top-sensitive features, as given
by Table 8. For IPS, the top-3 sensitive features also appear in the list of the most frequently selected
features. The top-4 sensitive features in Splice are also the most frequently selected 4 features. The
interesting overlapping between the attributes which are useful for attack and the top-ranked sensitive
attributes confirms our intuition about the association between feature sensitivity and adversarial
vulnerability of the classifier. On Splice, 4 of the most frequently attacked features (29,28,30 and 31 in
Table 8) correspond to the junction of exons, which are also highly informative for gene categorization
(Htike & Win, 2013). On IPS, 3 of the 5 most frequently attacked categorical features (19, 0 and 17) are
the also informative indicators of unusual DNS requests and DDos activities. They are ranked as the
most sensitive features according to Table 8.
Suppressing feature sensitivity helps improve adversarial robustness. However, these sensitive attributes
are usually informative for classification in an adversary-free scenario. Excluding the sensitive features
causes accuracy loss. It indicates the balance between classification utility and adversary-resilience
feature engineering in ML practices.
F.5 Robustness Assessment in B inary Classification
F.5.1 Robustness assessment with greedy search
Table 9 shows the results of robustness assessment for models built on binary-class datasets PEDec
and EHR. We have the same findings in that with the multi-class datasets Yelp, IPS and Splice. In
details, firstly for both the pessimistic and optimistic robustness assessment problems, a lower tolerance
threshold Γ results in lower scores over all the binary-class datasets. Secondly the pessimistic robustness
assessment scores are always lower or equal to the optimistic assessment scores.
F.5.2 Robustness vs Feature Informativeness
We take the XImd samples from the 62,43 original samples for the dataset PEDec, EHR respectively
on the half-way of pushing the m于 of the original samples (OS) to surpass Γ. In Table 10, We verified
23
Published as a conference paper at ICLR 2022
Table 9: Pessimistic and Optimistic Robustness Assessment Score ∣l〔，reported with median and mean.
A lower/higher value at a given percentile level indicates stronger/weaker robustness of the classifier.
SR is the SUCCeSS rate of attack. OVF indicates SR=0.______________________________
GreedySeearch	P/O	Γ	PEDec		EHR	
			SR	Med (Avg)	SR	Med (Avg)
FSGS	P	^01-	0.92	-373.2)-	0.98	Hr2
		-^0-	0.87	-373.7)-	0.94	2(1.2)
	O	^01-	0.46	4(4.1)	-0-	OVF
		o~0~	0.42	4(4.7)	-0-	OVF
GradAttack	P	^^01-	0.56	7(7.2)	0.63	-2(2.0)-
		o~0~	0.51	-8^(6T)-	0.62	-27∑3)-
OMPGS	P	^^01-	0.87	-676.1)-	0.95	-2(2.0)-
		o~0~	0.82	-6763)-	0.94	-27∑4-
	O	^44~	0.62	-5Γ6T)-	0.38	-27∑7)-
		~~0~~	0.54	6(6.8) 一	0.42	2(2.5)
Table 10: Robustness Assessment VS Feature Informativeness, reported at Γ = 0.1(xιmi ;y,S) < I(x;y,S)
results in higher adversarial risk (higher SR, smaller |l|)on xιmi than original ɪ.
GreedySeearch	P/O	sample	PEDec		EHR	
			SR	Med(Avg)	SR	Med(Avg)
FSGS	P	original X	0.87	-373.7)-	0.94	2(1.2)
		XlEi	0.96	-373TT)-	1	-rrɪ)-
	O	original X	0.42	4(4.7)	0	OVF
		XIEi	0.69	-373.2)-	0.92	-27∑2)-
OMPGS	P	original X	0.82	-676.3)-	0.94	-2(2.4)-
		XIEi	0.85	-676Γ)-	1	2(1.9)
	O	original X	0.54	-676.8)-	0.62	-27∑8)-
		XIEi	0.71	6(6.4)	0.94	2(2.1)
Table 11: Pessimistic Robustness Assessment (Γ = 0) after Model Robustness Enhancement by reducing
I(fy;S). JSR is the SR decreased amount comparing to the corresponding SR value with Γ = 0 in
Table 9. ________________________________________________________________________
Model	Algo.	PEDec			EHR			
		SR	iS^	Med (Avg)	SR	iS^	Med (Avg)
AdvC	FSGS	0.65	0.22	-4K2)-	0.94	-0-	-2(2.17)-
	-OMPGS-	0.58	0.24	-676.4)-	0.92	0.02	-2(2.82)-
NuR	FSGS	0.81	0.06	-3729)-	-0-	0.94	OVf
	-OMPGS-	0.25	0.57	-676Γ)-	0	0.94	OVf
RS	FSGS	0.93	-0.05	-373.2)-	0.96	-0.02	-1 (1.97)-
	OMPGS 一	0.86	-0.04	4(4.1)	0.91	0.03	-2(2.20)-
SRS(Boj.etal., 2020)		1.0	-	1	1.0	-	1
the impact of feature informativeness on the robustness assessment scores atΓ = 0 with the binary-
class datasets PEDec and EHR. The table shows that the classifier becomes less robust to adversarial
modifications over xιmi with less predicative information.
F.5.3 Robustness Assessment after Robustness Enhancement
From the parameter of the decreased SRin Table 11, we study the robustness enhancement by different
model enhancement techniques. On PEDec, the most robustness assessment scores increase slightly.
Notably, the decreased SR of AdvC and NuR are positive and brings the most robustness improvement.
On EHR, NuR can prevent attack over all the testing instances. As seen in Table 11, the robustness-
enhancing techniques induce more visible impact on the pessimistic robustness assessment scores
using OMPGS. This result meets the intuitive interpretations in the main paper.
24