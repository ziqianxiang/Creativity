Published as a conference paper at ICLR 2022
Communication-Efficient Actor-Critic
Methods for Homogeneous Markov Games
Dingyang Chen1, Yile Li, Qi Zhang1
1
1
Artificial Intelligence Institute, University of South Carolina
dingyang@email.sc.edu, qz5@cse.sc.edu
Ab stract
Recent success in cooperative multi-agent reinforcement learning (MARL) relies
on centralized training and policy sharing. Centralized training eliminates the is-
sue of non-stationarity MARL yet induces large communication costs, and policy
sharing is empirically crucial to efficient learning in certain tasks yet lacks theo-
retical justification. In this paper, we formally characterize a subclass of coopera-
tive Markov games where agents exhibit a certain form of homogeneity such that
policy sharing provably incurs no suboptimality. This enables us to develop the
first consensus-based decentralized actor-critic method where the consensus up-
date is applied to both the actors and the critics while ensuring convergence. We
also develop practical algorithms based on our decentralized actor-critic method
to reduce the communication cost during training, while still yielding policies
comparable with centralized training.
1	Introduction
Cooperative multi-agent reinforcement learning (MARL) is the problem where multiple agents learn
to make sequential decisions in a common environment to optimize a shared reward signal, which
finds a wide range of real-world applications such as traffic control (Chu et al., 2019), power grid
management (Callaway & Hiskens, 2010), and coordination of multi-robot systems (Corke et al.,
2005). Efficient learning for large and complex cooperative MARL tasks is challenging. Naively
reducing cooperative MARL to single-agent RL with a joint observation-action space imposes sig-
nificant scalability issues, since the joint space grows exponentially with the number of agents.
Approaches that treat each agent as an independent RL learner, such as Independent Q-Learning
(Tan, 1993), overcome the scalability issue yet fail to succeed in complicated tasks due to the non-
stationarity caused by other learning agents’ evolving policies. To address these challenges, the
paradigm of Centralized Training and Decentralized Execution (CTDE) is then proposed, where
a centralized trainer is assumed to access to information of all agents during training to approx-
imate the global (action-)value function, whereas each agent only needs local information for its
action selection during decentralized policy execution (Lowe et al., 2017; Foerster et al., 2017). The
centralized critic eliminates non-stationarity during training, while the policy decentralization en-
sures scalability during execution. Besides, existing CTDE methods almost always enable policy
parameter sharing to further improve learning scalability and efficiency, where agents also share the
parameters of their decentralized policies.
However, in many real-world scenarios, there is not a readily available centralizer that conveniently
gathers the global information from all agents, and therefore agents need to rely on all-to-all commu-
nication for centralized training, incurring enormous communication overheads for large numbers
of agents. This motivates us to think about whether it is possible to train agents in a decentralized
and communication-efficient manner, while still keeping the benefits of the centralized training of
CTDE. Moreover, despite its wide adoption, little theoretical understanding has been provided to
justify policy parameter sharing. Agents should at least exhibit a certain level of homogeneity be-
fore it is feasible to share their policies. For example, if the observation and/or action spaces vary
across agents, then their decentralized policies cannot even have the same architecture. Even if it is
feasible, it is unclear whether restricting the agents to share their policy parameters will introduce
any suboptimality.
1
Published as a conference paper at ICLR 2022
In this paper, we address these aforementioned issues centered around the CTDE framework. We
begin by formally characterizing a subclass of Markov games where the cooperative agents exhibit
a certain form of homogeneity such that it is not only feasible but also incurs no suboptimality to
share their decentralized policies, thus providing a first theoretical justification for policy parameter
sharing. We then develop a decentralized actor-critic algorithm for homogeneous MGs where agents
share their critic and actor parameters with consensus-based updates, for which we prove an asymp-
totic convergence guarantee with linear critics, full observability, and other standard assumptions.
To our knowledge, this is the first decentralized actor-critic algorithm that enjoys provable conver-
gence guarantees with policy (i.e., actor) consensus. To account for communication efficiency, we
develop a simple yet effective bandit-based process that wisely selects when and with whom to per-
form the parameter census update based on the feedback of policy improvement during training. To
further account for partial observability, we develop an end-to-end learnable gating mechanism for
the agents to selectively share their observations and actions for learning the decentralized critics.
This series of innovations are capable of transforming any CTDE algorithm into its decentralized
and communication-efficient counterpart, with policy consensus in homogeneous MGs for improved
efficiency. Our empirical results demonstrate the effectiveness of these innovations when instanti-
ated with a state-of-the-art CTDE algorithm, achieving competitive policy performance with only a
fraction of communication during training.
Our contribution is therefore summarized as three-fold: (1) the characterization of a subclass of
cooperative Markov games, i.e. homogeneous Markov games (MGs), where policy sharing provably
incurs no loss of optimality; (2) a decentralized MARL algorithm for homogeneous MGs that enjoys
asymptotic convergence guarantee with policy consensus; and (3) practical techniques that transform
CTDE algorithms to their decentralized and communication-efficient counterparts.
2	Related Work
Communication in cooperative MARL. Communication is key to solving the issue of non-
stationarity in cooperative MARL. The CTDE paradigm (Lowe et al., 2017; Foerster et al., 2017)
assumes a centralized unit during training to learn a joint value function. Other methods, such as
CommNet (Sukhbaatar et al., 2016) and BiCNet (Peng et al., 2017), do not assume a centralized unit
and instead allow agents to share information by all-to-all broadcasting, effectively relying on cen-
tralized communication. These methods require centralized/all-to-all communication that impedes
their application to large numbers of agents. Although follow-up work such as IC3Net (Singh et al.,
2018) and VBC (Zhang et al., 2019) proposes algorithms to learn when to communicate, agents there
still perform all-to-all communication before others decide whether to receive. We instead entirely
abandon centralized/all-to-all communication, letting each agent decide whom to communicate to
purely based on its local observation. There is another line of work, Networked MARL (NMARL)
(Zhang et al., 2018), that where agents lie on a predefined network such that neighboring agents
can freely communicate. Our approach instead learns sparse communication that is dynamically
adjusted during decentralized training, even if the predefined network topology can be dense.
Policy parameter sharing and consensus. Policy parameter sharing is widely adopted in MARL
where agents share the same action space, yet it has not been theoretically justified except under the
mean-field approximation where the transition dynamics depends on the collective statistics of all
agents and not on the identities and ordering of individual agents (Nguyen et al., 2017a;b; Yang et al.,
2018). A recent result from Kuba et al. (2021) states that enforcing policy parameter sharing in a
general cooperative MG can lead to a suboptimal outcome that is exponentially-worse with the num-
ber of agents. We are the first to 1) formally characterize the subclass of homogeneous MGs without
the notion of mean-field approximation, where enforcing policy parameter incurs no suboptimality
and 2) develop an algorithm that performs policy parameter sharing in homogeneous MGs in a soft
manner with decentralized consensus-based policy update with convergence guarantees. Zhang &
Zavlanos (2019) also develop a policy consensus algorithm for decentralized MARL, yet they do
not assume homogeneity and thus need each agent to represent the joint policy for consensus.
Communication-efficient MARL. There have been several recent works that also aim to achieve
communication-efficiency in decentralized MARL. Chen et al. (2021b) use pre-specified commu-
nication topology and reduce communication frequency for actor-critic via mini-batch updates; in
contrast, our work adaptively learn sparse communication topology during the decentralized training
2
Published as a conference paper at ICLR 2022
process. Chen et al. (2021a) generalize their method of communication-efficient gradient descent
from distributed supervised learning (Chen et al., 2018) to distributed reinforcement learning with
policy gradient methods, where they assume the existence of a centralized controller that gather the
policy gradients from decentralized agents which only communicate when the change in gradient
exceeds a predefined threshold; in contrast, our method does not rely on a centralized controller,
and we empirically demonstrate the benefit of our adaptive communication learning over a rule-
based baseline inspired by Chen et al. (2021a). Gupta et al. (2020) learn discrete messages among
agents with a fixed communication topology, where the communicated messages are used to form
the policy for action selection rather than for decentralized training.
3	Homogeneous Markov Game
We consider a cooperative Markov game (MG) hN, S, A, P, Ri with N agents indexed by i ∈ N =
{1,…，N}, state space S, action space A = A1 ×∙∙∙× AN, transition function P : S ×A×S →
[0, 1], and reward functions R = {Ri}i∈N with Ri : S × A → R for each i ∈ N . In Section 3,
we assume full observability for simplicity, i.e., each agent observes the state s ∈ S. Under full
observability, we consider joint policies, π : S × A → [0, 1], that can be factored as the product of
local policies πi : SXAi → [0,1], π(a∣s) = Qi∈Νπi(ai∣s). Let r(s,a):二 N Pi∈N Ri(s,a)
denote the joint reward function, and let γ ∈ [0, 1] denote the discount factor. Define the discounted
return from time step t as Gt = Pl∞=0 γlrt+l, where rt := r(st, at) is the reward at time step t.
The agents’ joint policy π = (π1, ..., πN) induce a value function, which is defined as V π(st) =
Est+L∞,at,∞ [Gt∣st], and action-value function Qπ(st,at) = Est+L∞,at+L∞ [Gt∣st,aJ The agents
are cooperative in the sense that they aim to optimize their policies with respect to the joint reward
function, i.e., max∏ J(π) = Es0g,a03 [Go].
3.1	Homogeneous MG: Definition, Properties, and Examples
As along as the action spaces {Ai}i∈N are homogeneous, policy sharing among {πi}i∈N is feasible.
However, such policy sharing can incur suboptimal joint policies for general MGs, as we will see in
an example introduced by Kuba et al. (2021) and revisited in this subsection. Here, we characterize
a subclass of Markov games in Definition 1 requiring conditions stronger than homogeneous action
spaces, where policy sharing provably incurs no suboptimality.
Definition 1 (Homogeneous Markov game). Markov game hN, S, A, P, Ri is homogeneous if:
(i)	The local action spaces are homogeneous, i.e., Ai = Aj ∀i, j ∈ N . Further, the state is
decomposed into local states with homogeneous local state spaces, i.e., s = (s1, ..., sN) ∈
S = S1 ×∙∙∙×SN with Si = Sj ∀i,j ∈ N.
(ii)	The transition function and the joint reward function are permutation invariant and permu-
tation preserving. Formally, for any st = (st1, ..., stN), st+1 = (st1+1, ..., stN+1) ∈ S and
at = (at1, ..., atN) ∈ A, we have
P(Mst+1 |Mst, Mat) =P(M0st+1|M0st,M0at), R(M st, M at) = M R(st, at)
for any M, M0 ∈ M, where R(s, a) := (R1(s, a), ..., RN (s, a)), Mx denotes a permuta-
tion M of ordered list x = (x1, ..., xN), and M is the set of all possible permutations.
(iii)	Each agent i ∈ N has access to a bijective function oi : S → O (i.e., each agent has
full observability) that maps states to a common observation space O. These observation
functions {oi}i∈N are permutation preserving with respect to the state, i.e., for any s ∈ S
and any M ∈ M,
(o1 (Ms),..., oN (Ms)) = M (o1(s),..., oN (s)).
By Definition 1, besides requiring homogeneous action spaces, our characterization of homogeneous
MGs further requires that the global state can be factored into homogeneous local states (condition
(i)) such that the transition and reward functions are permutation invariant (condition (ii)). Moreover,
condition (iii) requires each agent to have an observation function to form its local representation
of the global state. The main property of the homogeneous MG is that, after representing the global
3
Published as a conference paper at ICLR 2022
state with the observation functions, policy sharing incurs no suboptimality. This is formally stated
in Theorem 1 and proved in Appendix A.
Theorem 1. Let Π be the set of state-based joint policies, i.e., Π = {π = (π1 , ..., πN) : πi :
S × Ai → [0, 1]}, and let Πo be the set of observation-based joint policies, i.e., Πo = {πo =
(πo1 , ..., πoN) : πoi : O × Ai → [0, 1]}. In homogeneous MGs, we have
max	J(π) = max	J(πo) =	max	J(πo).
π=(π1 ,...,πN)∈Π	πo=(πo1 ,...,πoN)∈Πo	πo=(πo1 ,...,πoN)∈Πo: πo1=...=πoN
To provide more intuition for homogeneous MGs, we here give an example from Multi-Agent Parti-
cle Environment (MPE) (Lowe et al., 2017) and a non-example from Kuba et al. (2021). Appendix D
provides more examples and non-examples show the generality of our homogeneous MG subclass.
Example: Cooperative Navigation. In a Cooperative Navigation task in MPE, N agents move
as a team to cover N landmarks in a 2D space. The landmarks are randomly initialized at the
beginning of an episode, and fixed throughout the episode. Under full observably where each agent
can observe the information (locations and/or velocities) of all agents and landmarks, we can cast a
Cooperative Navigation task as a homogeneous MG by verifying the three conditions in Definition
1: (i) The local state of agent i consists of its absolute location li = (lxi , lyi ) ∈ R2 and its absolute
velocity vi = (vxi , vyi ) ∈ R2 with respect to the common origin, as well as the absolute locations
of all N landmarks, {ck = (ckx, cyk)}kN=1. Therefore, the location state spaces are homogeneous,
and the concatenation of all the location states preserves the global state of the task. Since local
action is the change in velocity, the local action spaces are also homogeneous. (ii) The transition
function determines the next global state by the current state and all agents’ actions according to
physics, and thus it is permutation invariant. The reward function Ri determines the reward for
agent i according to the distances between all the agents and the landmarks to encourage coverage,
as well as penalties to discourage collisions if any, resulting in a permutation preserving joint reward
function. (iii) In MPE, agents’ observations are based on relative, instead of absolute, locations
and/or velocities of other agents and/or landmarks. For Cooperative Navigation, such observations
happen to define observation functions that are bijective and permutation preserving. Specifically,
function oi yields the observation for agent i that consists of its absolute location li and velocity vi ,
the relative location lji := lj - li and velocity vji := vj - vi of other agents j ∈ N \ {i}, and the
relative location cik := ck - li of all the landmarks k = 1, .., N.
Non-Example: a stateless MG. Kuba et al. (2021) recently shows that enforcing policy param-
eter sharing is exponentially-worse than the optimality without such a restriction in the following
stateless MG: Consider a cooperative MG with an even number of N agents, a state s fixed as
the initial state, and the joint action space {0, 1}N, where 1) the MG deterministically transits
from state s to a terminal state after the first time step, and 2) the reward in state s is given by
R(s, 01:N/2 , 11+N/2:N) = 1 and R(s, a1:N) = 0 for all other joint actions. It is obvious that the
optimal value of this MG (in state s) is 1, while Kuba et al. (2021) prove that the optimal value under
policy parameter sharing is 1/2N-1. This MG is not a homogeneous MG: the agents are relying
on the raw state to represent their policies, and therefore their observation functions are identity
mappings oi(s) = s, which is not permutation preserving and violates Definition 1(iii).
3.2 Policy Consensus for Homogeneous MGs
Theorem 1	theoretically justifies the parameter sharing among the actors with observation-based
representations, which enables us to develop the first decentralized actor-critic algorithms with con-
sensus update among local (observation-based) actors.
Formally, the critic class Q(∙, ∙; ω) is parameterized with ω to approximate the global action-value
function Qn(∙, ∙). Upon on-policy transition (st, at, {rt}i∈N, st+ι, at+ι) sampled by the current
policy, the critic parameter ωi for each agent i ∈ N is updated using its local temporal difference
(TD) learning followed by a consensus update (Zhang et al., 2018):
ωi =	ωi	+ Bω,t	∙ δi	∙ Vω Q(St,	at; ωi ),	ωi+ι =	Pj∈N cω,t (i,j)	∙	ωj	⑴
where rti = Ri(st, at) is the local reward of agent i, δti = rti + γQ(st+1, at+1; ωti) - Q(st, at; ωti)
is the local TD error of agent i, βω,t > 0 is the critic stepsize, and Cω,t = [cω,t (i, j)]i,j ∈N is
4
Published as a conference paper at ICLR 2022
Table 1: Cooperative MARL settings in prior and our work. A: Agent-specific reward, T: Team
shared reward, FO: Fully observable, JFO: Jointly fully observable, PO: Partially observable.
	Reward	State observability	(De)Centralized	Memory-based policy
Section 3, and Zhang etal.(2018)	A	FO	D	No
Sections 4 and 5	A	JFO	D	No
Example paper: Lowe et al. (2017)	A	JFO	C	No
Example paper: RaShid etal.(2018)	T	PO	C	Yes
the critic consensus matrix. The observation-based actor for each agent i ∈ N is parameterized
as ∏i(ai∣oi(s); θi) with parameter θi, which is updated based on the multi-agent policy gradient
derived from the critic followed by a consensus update:
毋+1 =	θi	+ βθ,t ∙	Q(st,at；	ωi)	∙ ▽傍 log ∏i(at∣oi(st);仔)，	毋+1 =	Pj∈N Cθ,t(i,j)	∙ θj ⑵
where the observation-based actor class ∏(∙∣∙; θ) is assumed to be differentiable, βθ,t > 0 is the actor
stepsize, and Cθ,t = [cθ,t(i, j)]i,j∈N is the actor consensus matrix.
Compared with existing decentralized actor-critic methods for cooperative MARL (e.g., (Zhang
et al., 2018)), the subclass of homogeneous MGs in Definition 1 makes it possible to perform actor
consensus (i.e., policy consensus) in Equation (2) that is not possible for general MGs. Theorem 2
states the convergence of {ωti} and {θti} generated by (1)(2) with the linear critic class and under
standard assumptions on the stepsizes, consensus matrices, and stability.
Theorem 2.	Under standard assumptions for linear actor-critic methods with consensus update,
with {ωi} and {θ7i} generated from Equations (1) and (2), we have limt ω∖ = ω* and limt θi = θ*
almost Surelyfor any i ∈ N, where θ* is a stationary point associated with update (2), and θ∖ is the
minimizer ofthe mean square projected Bellman errorfor the joint policy parameterized by θ*.
Theorem 2 and its proof generalize the results in Zhang et al. (2018) to the case where not only
the critics but also the actors perform the consensus update. Please refer to Appendix B which
provides the exact assumptions, the convergence points, and our proof. While the actor-critic updates
converge asymptotically both with and without actor consensus, obtaining their convergence rates
require non-trivial finite-time analysis that remains an open problem. In Appendix F, we empirically
compare the actor-critic updates with and without actor consensus on a toy example of homogeneous
MG, with the results showing that the actor consensus slightly accelerates the convergence.
4	Practical Algorithm
The convergence of our decentralized actor-critic algorithm in Section 3.2 relies on the assump-
tions of linear function approximators, full observability, and well-connected consensus. In this
section, we develop a practical algorithm that relaxes these assumptions and achieves communica-
tion efficiency. Specifically, the decentralized actors and critics are represented by neural networks.
We consider the partial observability setting where the agents cannot directly observe the global
state st such that their observation functions {oi} are not bijective. Further, similar to Network
MARL (Zhang et al., 2018), we assume the agents can communicate through a time-variant network
Gt := (N, Et) with vertex set N and directed edge set Et ⊆ {(i,j) : i,j ∈ N, i 6= j}. Denote the
neighbors of agent i at time step t as Nti := {j : (i, j) ∈ Et}. With agents only partially observ-
ing the global state, we in Section 4.1 develop an architecture for the agents to learn to share their
local observations and actions in a communication efficient manner. To achieve communication
efficiency on the actor-critic parameter consensus, in Section 4.2 we develop an effective bi-level
multi-armed bandit for the agents to learn to exchange the parameters only when it benefits learning.
Table 1 summarizes the differences between the problem settings considered in Sections 3 and 4,
as well as in the literature. Below we describe our key design choices, and Appendix E provides
implementation details of our algorithm.
5
Published as a conference paper at ICLR 2022
4.1	Efficient observation-action communication
We primarily focus on the type of partial observability where the state is not fully observably by
individual agents but jointly observable, i.e., the mapping from st to {oi(st)}i∈N is bijective. For
example, this joint observability is satisfied in Cooperative Navigation, where all agents’ observa-
tions can determine the state. Thus, each agent can use the observations and actions of its own as well
as from its neighbors for its critic to approximate the global action-value. To encourage communi-
cation efficiency, we propose an architecture, communication network, that selects a subset Cti ⊆ Nti
for observation-action communication, such that agent i’s critic becomes Qi({(otk, atk)}k∈{i}∪Ci ).
For the texts below, we abuse notation oit := oi (st) to denote the observation and omit the subscript
of time step t when the context is clear.
Communication network. The communication network Ci of agent i outputs cij ∈ {0, 1} indicat-
ing whether to communicate with neighbor j ∈ Ni, i.e., Ci = {j ∈ Ni : cij = 1}. Specifically,
we choose an L-layer graph convolutional networks (GCN) to implement Ci , which can deal with
arbitrary input size determined by |Ni | and achieve permutation invariance. Specifically, the input
to this GCN is a fully connected graph with one vertex (oi, eij (oi)) per neighbor j ∈ Ni, where
eij (oi) embeds information of neighbor j that can be extracted from oi. For example, when agents’
identities are observable, eij (oi) can be (the embedding of) the ID of neighbor j. When identities
are not observable, eij(oi) preserves information specific to neighborj, such as the physical distance
from j to i in MPE. The GCN’s last layer outputs the logits {lji } from which {cij}j∈Ni are sampled.
To enable differentiability of the sampling, we use the reparameterization trick Straight-Through
Gumbel-Softmax (Jang et al., 2016).
Actor and critic networks. Our proposed communication network is compatible with any multi-
agent actor-critic architecture. Our experiments mainly explore deterministic actors ai = πi(oi).
Similar to the communication network, the critic network Qi({(ok, ak)}k∈{i}∪Ci ) is also imple-
mented by an L-layer GCN to deal with arbitrary input size and achieve permutation invariance,
where the input to the first layer is the fully connected graph with vertices {(ok, ak)}k∈{i}∪Ci .
Training. Critic Qi directly guides agent i’s actor update using the deterministic policy gra-
dient. Critic Qi itself is updated to minimize the TD loss LiTD = Eot,at,rt,ot+1 [(Qti - yti)2],
where ot := (ot1, ..., otN) is the joint observation, at := (at1, ..., atN) is the joint action, Qit :=
Qi({(otk, atk)}k∈{i}∪Ci) is the abbreviated notation for the critic value of agent i at timestep t, and
yti := rti + γQit+1 is the TD target. Due to the differentiability enabled by Gumbel-Softmax, the
gradient can flow from Qi to communication network Ci . Commonly used in the literature (Jang
et al., 2016), the update of Ci is guided by a regularization term α∣尚 Pj∈ci Softmax(Ij) - η∣,
which places a restriction on the amount of communication allowed defined by rate η .
4.2	A bi-level bandit for parameter consensus
The parameter consensus defined in Equations (1)(2) requires each agent i to communicate with all
other agents j where the (i, j) entry of the consensus matrix is non-zero. To achieve communication
efficiency, existing literature mainly considers gossip algorithms where each agent only communi-
cated with one neighbor j per communication round (Boyd et al., 2006), i.e., the consensus matrix
entries satisfy c(i, j) = c(i, i) = 1/2. Here, we develop a novel bi-level multi-armed bandit to
further improve communication efficiency over gossip algorithms, where at each round each agent
chooses whether or not to perform consensus at the high level, and if yes, chooses which neighbor
to perform gossip consensus update. For ease of exposition, we assume that 1) the agents perform a
gradient step and decide whether and how to perform parameter consensus every episode indexed by
m = 1, 2, ..., and 2) every agent can always choose from all other agents for parameter consensus,
i.e., Nti = N \ {i}. Extensions to more general settings are straightforward. We next formally
describe this bi-level bandit for an arbitrary agent i, dropping the superscript i for convenience.
Arms. The high-level is a 2-armed bandit determining whether to perform consensus at each round
m. Denote the selected high-level arm at round m as x1m , where x1m = 0, 1 corresponds to perform-
ing and not-performing consensus, respectively. The low-level is a (N - 1)-armed bandit, and we
let x2m ∈ N \ {i} denote the selected low-level arm at round m.
6
Published as a conference paper at ICLR 2022
Rewards. We design different reward functions for the arms in the two levels, as they have different
goals. The high-level bandit aims to 1) reduce communication while 2) maintaining a reasonable
performance of the learned consensus matrices. Requirement 2) can be captured by the difference of
the episodic rewards at different time steps, and requirement 1) can be measured by the frequency of
selecting to perform consensus. Let Gm be the total rewards of episode m. To compute the reward
rm1 for the high-level, we first normalize Gm using the latest l episodes high-level records to fulfill
requirement 2), followed by the rewarding or penalizing depending on the sign of normalized Gm
to fulfill requirement 1), and finally mapped to to [-1, 1]. The low-level bandit only considers the
performance of the learned policy, and the reward rm2 can be computed by the normalization of Gm
using latest l episodes low-level records, then be mapped to [-1, 1]. Equation (3) shows the details.
1	Gm - mean(G(mT+1):m)
rm《	Std(G(mT+1):m)
ri J (rm/ P10=1 1[x10 = 0],
m	Irm J rm/ Pu=I 1[x10 = 1],
rm J 2 ∙ Sigmoid(rm) — 1
if rm ≥ 0.
otherwise.
2 Gm — mean(G(mT+1):m)
rm J	std(G(mT+1):m)	(3)
rm J 2 ∙ Sigmoid(rm) — 1
The reward functions designed above are in nature non-stationary, since the agents are continuously
updating their policies, which directly influence the values of the episodic reward difference. Here
we choose the adversarial bandit algorithm Exponentially Weighted Average Forecasting (Cesa-
Bianchi & Lugosi, 2006) to learn the bi-level bandit.
5	Experiments
Our experiments aim to answer the following questions in Sections 5.1-5.3, respectively: 1) How
communication-efficient is our algorithm proposed in Section 4 against baselines and ablations?
2) How empirically effective is policy consensus? Specifically, compared with not using policy
consensus, can policy consensus converge to better joint policies faster? 3) What are the qualitative
properties of the learned communication rules?
Environments. We evaluate our algorithm on three tasks in Multi-Agent Particle Environment
(MPE) with the efficient implementation by Liu et al. (2020), each of which has a version with
N = 15 agents and another with N = 30 agents. As described in Section 3.1, these MPE environ-
ments can be cast as homogeneous MGs provided full observability and the permutation preserving
observation functions. We set the communication range to be k = 10 < N nearest agents for all
the environments to introduce partial observability. The details of the observation functions in each
environment are as follows. Cooperative Navigation: There are 15, 30 landmarks for N = 15, 30
respectively. The observation ofan agent contains its own absolute location, the relative locations of
the nearest 10 agents, and the relative location of the 11 nearest landmarks. Cooperative Push: N
cooperating agents are tasked to push a large ball to a target position. There are 2, 2 landmarks for
N = 15, 30 respectively. The observation of an agent contains its own absolute location, the relative
locations of the nearest 10 agents, and the relative location of the 2 landmarks. Predator-and-Prey:
N cooperating predators (agents) are tasked to capture p preys. The preys are pre-trained and con-
trolled by the environment. There are 5, 10 preys and 5, 10 landmarks (blocks) for N = 15, 30
respectively. Each predator can see p = 3, 5 preys, l = 3, 5 landmarks, k = 10, 10 other predators
for N = 15, 30 respectively. The observation of an agent contains its own absolute location, the
relative locations of the nearest k agents (predators), the nearest p preys, and the relative location of
the l nearest landmark.
Baselines. We use Permutation Invariant Critic (Liu et al., 2020), the state-of-the-art CTDE actor-
critic algorithm on MPE, as our centralized training algorithm to derive the following decentralized
training baselines. Full-Communication employs all-to-all communication for both observation-
action and parameters, i.e., at each time step, each agent receives the observations and actions from
all other neighbors for its critic, as well as critic and policy parameters from all other neighbors for
its consensus update. Independent learning (IL) employs no communication, i.e., each agent uses
its local observations and actions only for its critic and performs no consensus update. Random
selects at random 1) a subset of neighbors for observation-action sharing and 2) a single neighbor
7
Published as a conference paper at ICLR 2022
-12000
-14000
-16000
navigation n30	push nl5	push n30
一FulI-Communication	— IL -------- Ours ----- Random
Figure 1:	Comparison of our algorithm with Full-Communication, IL, and Random.
for gossip parameter consensus. For parameter consensus, we consider a Rule-based baseline,
where each agent saves a copy of the parameters from the latest communications with other agents.
When agent i considers doing communication at t, it calculates the l1 norm of the parameters of Qi
and Qj which is the latest copy of j's parameters it saves. The norm serves as the score to rank the
other agents. Intuitively, the high parameter difference implies dramatic behavioral differences. The
agent j with the highest score is selected to do parameter consensus, and the copy of agent j’s critic
parameter is recorded by agent i. For fair comparisons, the Random and Rule-based baselines incur
the same communication cost (i.e., the fraction of neighbors for observation-action communication,
and the frequency for parameter consensus) as learned by our bandit method.
5.1	Communication efficiency against baselines and ablations
Figure 1 shows the learning curves comparing our communication-efficient decentralized actor-critic
algorithm described in Section 4 against the baselines of Full-Communication, IL, and Random, with
the observation-action communication rate set as 50%. The results clearly verify the importance
of observation-action and parameter communication, as Full-Communication outperforms IL by
significant margins uniformly in all the tasks. As will be confirmed in Section 5.3, our algorithm
complies with the given observation-action communication rate, and its bandit learning chooses to
communicate with a neighbor for parameter consensus only roughly 95% less frequently than Full-
Communication. Our algorithm significantly outperforms the Random baseline that uses the same
amount of communication in almost all the tasks, and achieves performances comparable with Full-
Communication in several tasks. Remarkably, Random barely outperforms IL in Cooperative Push,
suggesting that efficient communication for decentralized training is challenging.
Ablation: parameter communication. We perform ablations on the three environments with
N = 15 agents. Fix the observation-action communication pattern to be learned by the proposed
communication network, we can see in Figure 2 that parameter consensus strategy learned by our
bi-level bandit outperforms the Random parameter consensus baseline, the Rule-based parameter
consensus strategy, and even the parameter consensus using Full-Communication in Predator-and-
Prey, and behaves comparably to parameter consensus using Full-Communication in Cooperative
Push, using random parameter consensus and Rule-based parameter consensus in Cooperative Nav-
igation. Noticeably, the Rule-based algorithm behaves similar to the Random parameter consensus
baseline in the three scenarios. A possible explanation is that the parameters of the other agents an
agent records are outdated, as the Rule-based algorithm uses the same communication frequency
(around 95%) learned by the bi-level bandit. Another explanation is that parameter consensus harms
exploration, and the balance of them cannot be handled by the Rule-based algorithm which only
considers marinating homogeneous behaviors between agents.
Ablation: observation-action communication. In Appendix G, we also experiment with commu-
nication rate other than 50%, and the results show that our method dominate the baselines across
various choices for the communication rate.
5.2	Effectiveness of policy consensus
Theorem 2 assures that policy consensus in distributed actor-critic methods for Homogeneous MGs
provably converges to a local optimum, yet it remains open questions whether such a local is good
and whether the convergence is fast. Our experiments in this subsection empirically investigate these
questions by comparing the learning curves with and without policy parameter consensus. We sepa-
rately consider 1) the setting where the agents employ all-to-all communication for their critics and
8
Published as a conference paper at ICLR 2022
navigation ∏15	push ∏15	prey ∏15
navigation ∏15
prey ∏15
T T T
EMMaI U-PoS-de
— Ours with rule_based
— training step ι±e4∣	-	, „
--- Ours	---- Ours wltn full param comm
Ours w/o param comm ------ Ours with random param comtn
-l«0
-i4≡α
WO
300
200
1∞
a
J
FUeMal U-PoS-d。
ð aɪ S «e βo ιoa „ _ o	io—go « ɪoo
training step (Ie4)- Full-Gjnimunlcation
Full-Communication w/o policy consensus
β	zo U eβ so ιβc
----Ours
---- Ours w/o policy consensus
Figure 2:	Comparison of parameter consensus by Figure 3: Comparison of learning with and w/o
our bandit with Random and Rule-based.	policy consensus.
Figure 4: Y: communication rate. X: training step (log scale). Communication rate for the 10
neighbors in Cooperative Navigation (N = 15), with the distance increasing from left to right.
parameter consensus, such that the assumptions of full observability and well-connected consensus
matrices in Theorem 2 are satisfied, and 2) the setting where the agents learn to communicate effi-
ciently with our communication network and the bandit. The results in Figure 3 show that policy
parameter consensus is always beneficial with our communication-efficient method and, surpris-
ingly, it can negatively impact the training with all-to-all communication (e.g., Predator-and-Prey).
One plausible explanation is that, while it might speed up convergence, policy parameter consensus
can harm exploration, leading to worse local optima.
5.3 Qualitative analyses of the learned communication rule
We first qualitatively examine the observation-action communica-
tion rule learned by our communication network. We plot the output
of our communication network, i.e., the probabilities of communi-
cating with the k = 10 distance-sorted neighbors, as the learning
progresses. Interestingly, the rule learned by our communication
network encourages communicating with the nearestneighbors. For
example, in Cooperative Navigation with the 50% communication
rate as shown in Figure 4, the probabilities of agents communicat-
ing with nearest 5 neighbors are over 75%, around 25% for the 6th
nearest agent, around 0% for the other neighbors. We provide the
counterparts of Figure 4 for all the environments in Appendix C.
Figure 5 shows the probability of choosing the high-level arm of
performing consensus as the learning progresses, averaged across
the runs of all the environments. The result shows that, with the de-
signed bandit’s reward function, the average probability of selecting
to communicate decreases from around 50% to less than 10%.
6 Conclusion
°°
bb
------
Xooooo
SnsuSUUOU EWdqojd
IO1 10j IO3 IO4
training step (log scale)
Figure 5: Probability of com-
munication in the high-level
bandit across all environ-
ments and seeds.
In this paper, we characterize a subclass of cooperative Markov games where the agents exhibit a
certain form of homogeneity such that policy sharing provably incurs no loss of optimality. We
develop the first multi-agent actor-critic algorithm for homogeneous MGs that enjoys asymptotic
convergence guarantee with decentralized policy consensus. For practical usage, we propose tech-
niques that can efficiently learn to communicate with other agents in exchange of observations,
actions, and parameters. The empirical results show that our proposed algorithm performs better
than several baselines in terms of communication efficiency.
9
Published as a conference paper at ICLR 2022
Acknowledgement
We thank the anonymous reviewers for their thoughtful comments and supportive discussion. We
thank Yan Zhang for an early discussion on this work.
References
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
IEEE transactions on information theory, 52(6):2508-2530, 2006.
Duncan S Callaway and Ian A Hiskens. Achieving controllability of electric loads. Proceedings of
the IEEE, 99(1):184-199, 2010.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. 01 2006. ISBN 978-0-
521-84108-5. doi: 10.1017/CBO9780511546921.
Tianyi Chen, Georgios Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient for
communication-efficient distributed learning. Advances in neural information processing systems,
2018.
Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Basar. Communication-efficient
policy gradient methods for distributed reinforcement learning. IEEE Transactions on Control of
Network Systems, 2021a.
Ziyi Chen, Yi Zhou, Rongrong Chen, and Shaofeng Zou. Sample and communication-efficient
decentralized actor-critic algorithms with finite-time analysis. arXiv preprint arXiv:2109.03699,
2021b.
TianshU Chu, Jie Wang, Lara Codeca, and Zhaojian Li. Multi-agent deep reinforcement learning for
large-scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems, 21(3):
1086-1095, 2019.
Peter Corke, Ron Peterson, and Daniela Rus. Networked robots: Flying robot navigation using a
sensor net. In Robotics research. The eleventh international symposium, pp. 234-243. Springer,
2005.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017.
Shubham Gupta, Rishi Hazra, and Ambedkar Dukkipati. Networked multi-agent reinforcement
learning with emergent communication. arXiv preprint arXiv:2004.02780, 2020.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv
preprint arXiv:2109.11251, 2021.
Iou-Jen Liu, Raymond A Yeh, and Alexander G Schwing. Pic: permutation invariant critic for multi-
agent deep reinforcement learning. In Conference on Robot Learning, pp. 590-602. PMLR, 2020.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agent actor-critic for mixed cooperative-competitive environments. In Advances in neural infor-
mation processing systems, pp. 6379-6390, 2017.
Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Collective multiagent sequential decision
making under uncertainty. In Thirty-First AAAI Conference on Artificial Intelligence, 2017a.
Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Policy gradient with value function
approximation for collective multiagent planning.(2017). Advances in Neural Information Pro-
cessing Systems: Proceedings of NIPS, pp. 4-9, 2017b.
10
Published as a conference paper at ICLR 2022
Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin Bohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. arXiv preprint arXiv:2003.06709, 2020.
Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.
Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning
to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale
in multiagent cooperative and competitive tasks. arXiv preprint arXiv:1812.09755, 2018.
Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. arXiv preprint arXiv:1605.07736, 2016.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In In Proceed-
ings of the Tenth International Conference on Machine Learning, pp. 330-337. Morgan KaUf-
mann, 1993.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning,pp. 5872-5881. PMLR, 2018.
Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement
learning via variance based control. arXiv preprint arXiv:1909.02682, 2019.
Yan Zhang and Michael M Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 4674-
4679. IEEE, 2019.
11
Published as a conference paper at ICLR 2022
A Proof of Theorem 1
The set of bijections {oi }i∈N induces a one-to-one mapping between Π and Πo, and therefore
the first equality holds. For the second equality, consider an arbitrary state s = (s1, ..., sN) ∈ S
and the permutation M that swaps a pair of agents (i, j), such that Ms = M (..., si, ..., sj , ...) =
(..., (M s)i = sj, ..., (M s)j = si, ...).. Due to the permutation invariance of the transition and
reward functions by condition (ii) of Definition 1, there exists an optimal state-based joint policy
∏ ∈ Π such that ∏i(∙∣s) = ∏j(∙∣Ms). Consider the corresponding optimal observation-based
joint policy ∏*° ∈ ∏o that is the bijective mapping of ∏*, such that 优o(∙∣oi(s)) = ∏i(∙∣s) and
πjo(∙∣oj(Ms)) = πjj(∙∣Ms). We therefore have
π^o(∙loi(s)) = πjo(Ioj(Ms)).	(4)
Further, since {oi}i∈N are permutation preserving by condition (iii) of Definition 1, we have oi(s) =
oj(Ms) ∈ O in Equation (4). Since Equation (4) holds for arbitrary s ∈ S and i,j ∈ N, and thus it
follows that
∏io(∙∣o)= ∏jo(∙∣o) ∀o ∈O,	(5)
i.e., the second equality holds. This concludes the proof.
A.1 ILLUSTRATIVE EXAMPLE FOR THE PROOF
Sl = 0,A)	permute	犬=S2 = (M,V)
s2 = (M,V)	1,2,3) --------------► (3,1,2)	s'2 = s3 = (R,∆)
s3 = (R,∆)	SB = sl= (L,∆)
Ol = (AVA)	Definition 1(iii)	0'1 = (VAA) =。2
o2 = (VAA)	o1, o2, o3) =	o'3,o'1,o'2)	o'2 = (AAV) = o3
o3 = (AAV)	o'3 = (AVA) = o1
4 10 01) = ↑	4 1° o'1) = J=岛 02)
七 O2) = J	咫。(o)=匕(o) ∀(ij,O) 低 o'2) = ↑=心 O3)
欣。。3) = ↑	R。'3) = ↑=目。O1
Figure 6: An illustrative example for the proof of Theorem 1. Please see the text for details.
We here provide an illustrative example in Figure 6 to aid the proof. The example Markov game
consists of three agents i ∈ {1, 2, 3} placed on the left (L), middle (M), and right (R), one in each
position, with a triangle placed in front of each agent that is either pointing up (4) or down (5).
The agents have homogeneous action spaces {↑, 1}.
The agents also have homogeneous local state spaces {L, M, R} × {4, 5}, repenting its position
and the shape in front. The game ends after the first timestep, and the agents share the following
reward function: 1) if the number of the 4 is even, reward is +1 when the agents behind 4 choose to
go up (↑) and the agents behind 5 choose to go down (1); 2) if the number of the 4 is odd, reward
is +1 when the agents behind 4 choose to go down (1) and the agents behind 5 choose to go up
(↑); 3) reward is 0 otherwise. Thus, the Markov game satisfies Definition 1(i)(ii).
The agents’ local observations preserve their absolute positions (i.e., L, M, or R) and consist the
three shapes ordered clockwise starting from the shape right in front. For example, the local obser-
vation of agent i = 1 on the left of Figure 6 is o1 = (L, 4 5 4). As verified by the second row
in Figure 6, the local observations are permutation preserving to satisfy Definition 1(iii). Therefore,
the Markov game is homogeneous by Definition 1.
The third row in Figure 6 verifies Equations (4)(5) that are key to the proof.
12
Published as a conference paper at ICLR 2022
B Proof of Theorem 2
B.1	Assumptions
We make the following assumptions that are necessary to establish the convergence.
Assumption 1. The Markov game has finite state and action spaces and bounded rewards. Further,
for any joint policy, the induced Markov chain is irreducible and aperiodic.
Assumption 2. The critic class is linear, i.e., Q(s, a; ω) = φ(s, a)>ω, where φ(s, a) ∈ RK is the
feature of (s, a). Further, the feature vectors φ(s, a) ∈ RK are uniformly bounded by any (s, a).
The feature matrix Φ ∈ RlSllAl×κ has full column rank.
Assumption 3. The stepsizes βω,t and βθ,t satisfy
Ptβω,t=Ptβθ,t=∞,	Pt βω2,t < ∞,	Pt βθ2,t < ∞, βθ,t=o(βω,t).
In addition, limt βω,t+1βω-,1t = 1.
Assumption 4. We assume the nonnegative matrices Ct ∈ {Cω,t, Cθ,t} satisfy the following condi-
tions: (i) Ct is row stochastic (i.e., Ct1 = 1) and E[Ct] is column stochastic (i.e., 1>E[Ct] = 1> )
for all t > 0；(ii) The spectral norm of E[(W>(I -焉 11>)Wt] is strictly smaller than one; (iii) Wt
and (st , {rti }) are conditionally independent given the σ-algebra generated by the random variables
before time t.
Assumption 5. The critic update is stable, i.e., supt ωti < ∞, for all i. For the actor update, {θti}
belongs to a compact set for all i and t.
B.2	Critic convergence
In this subsection, we establish critic convergence under a fixed joint policy in Lemma 3. Specifi-
cally, given a fixed joint policy π = (π1, ..., πN), we aim to show that the critic update converges to
ωπ, which is the unique solution to the Mean Square Projected Bellman Error (MSPBE):
ωπ = arg min kΦω - ΠTπ(Φω)k2Dπ ,
ω
which also satisfies
Φ>Dπ [Tπ(Φωπ) - Φωπ] = 0,
where Tπ is the Bellman operator for π, Π is the projection operator for the column space of Φ, and
Dπ = diag[dπ (s, a) : s ∈ S, a ∈ A] for the stationary distribution dπ induced by π.
Lemma 3. Under the assumptions , for any give joint policy π, with distributed critic parameters
ωi generated from Equation 1 using on-policy transitions (st, at, rt, st+1,at+1) 〜 ∏, we have
limt ωti = ωπ almost surely (a.s.) for any i ∈ N, where ωπ is the MSPBE minimizer for joint policy
π.
Proof. We use the same proof techniques as Zhang et al. (2018).
Let φt = φ(st, at), δt = [δt1, ..., δtN]>, and ωt = [ωt1, ..., ωtN]> . The update of ωt in Equation 1
can be rewritten in a compact form of ωt+ι = (Cω,t 0 I)(ωt + βω,tyt) where 0 is the Kronecker
product, I is the K × K identity matrix, and yt = [δt1φt>, ..., δtN φt>]> ∈ RKN. Define operator
h∙i : RKN → RK as
hωi = N( 1> 0 IM = Nn X ωi
i∈N
for any ω = [(ω1)>, ..., (ωN)>]> ∈ RKN with ωi ∈ RK for any i ∈ N. We decompose ωt into its
agreement component 1 0 hω∕ and its disagreement component ω⊥,t := ωt -10 hω∕. To prove
ωt = ω⊥,t + 10 hωti ■—-→ 10 ω∏ , we next show ω⊥,t -—→ 0 and (ωt) —.→ ω∏ respectively.
13
Published as a conference paper at ICLR 2022
Convergence of ω⊥,t -→ 0. We first establish that, for any M > 0, we have
SuP E [∣∣β-,tω⊥,t∣∣2 • l{sup∕∣ωt∣∣≤M}] < ∞∙	(6)
To show Equation 6, let {Ft} be the filtration of Ft = σ(rτ-1sτ, aτ, ωτ, Cω,τ-1; T ≤ t), J =
N(1IT 0 I) such that Jωt =10 {ωt), (I — J)ωt = ω⊥,t. The following facts about 0 will be
useful:
(A 0 B)(C 0 D) = (AC) 0 (BD)
⑺
This enables us to write ω⊥,t+ι as
ω⊥,t+ι =(I - J)ωt+ι
=(I - J) [(Cω,t 0 I)(ωt + βω,tyt)]
=(I - J)[(Cω,t 0 I)(I 0 hωt + ω⊥,t + βω,tyt)]
(By Equation 7 and Assumption 4, we have (Cω,t 01)(1 0 hω∕) = (Cω,t 1) 0 (Ihω∕) = 1 0 hω∕)
=(I - J)[1 0 hωti + (Cω,t 0 i)(ω⊥,t + βω,tyt)]
=(I - J) [(Cω,t 0 I)(ω⊥,t + βω,tyt)]	((I - J)(1 0 hωt〉)= 0)
= [(I - 1IT/N) 0 I] [(Cω,t 0 I)(ω⊥,t + βω,tyt)]	(I - J = (I - 1IT/N) 0 I)
= [(I - 1IT/N)Cω,t 01](ω⊥,t + βω,tyt)	(By Equation 7).
We then have
E[∣∣β-,t+1ω⊥,t+1∣∣2 |Ft]
(∣∣x∣∣2 = xτx, A = (I - 1IT/N)Cω,t 0 I, ATA = C),t(I - 1IT/N)Cω,t 0 I)
β2
'ω,t E
β2 E
βω,t+1
[(β->⊥,t + yt)T (CT,t(I - 1IT/N)Cω,t 01) (βω>⊥,t + yt) | Ft]
(A = CT,t(I - 1IT/N)Cω,t, B = IjA 0 Bk = MkkBk)
(XTAx = ∣∣xtAx∣∣ ≤ ∣∣xt∣∣ kAkkxk = ∣∣Ak XTx)
(Cω,t and (rt, yt) are independent conditioning on Ft)
≤
β
βω ,t
%,t+1
PE [(β-,tω⊥,t+ yt)τ (β-,tω⊥,t+ yt) 1 Ft]
(where P is the spectral norm of E[C],t(I — 1IT/N)Cω,t])
ββω^P(E [ ∣ ∣ β-,tω⊥,t∣∣2 |Ft] +2E[hβω>⊥,t,yti |Ft] + E Uytk2 I FtD
(By Cauchy-Schwarz |〈u,v)| ≤ IIukkvk)
≤ββω^Ρ(E [ ∣ ∣ β-,tω⊥,t∣∣2 |Ft] +2E[∣∣βω>⊥,t∣∣∣∣ytk m + E [|配『|可)
(Quantities are deterministic given Ft )
=舁Ρ ( ∣ ∣ β-,tω⊥,t∣∣2 + 2 ∣∣β-,tω⊥,t∣∣ kytk + kytk2).
βω,t+1	'	'
(8)
Since E[kytk2 Ft] = E[Pi∈N ∣∣δtφt∣∣2 |Ft] with δi = Tt + γφ>ωi -。入海，by Assumptions 1
and 2 the rewards rt and the features φt are bounded, and thus we have that E[kytk2 |Ft] is bounded
on set {supτ≤t kωτk ≤ M} for any given M > 0. We can then following the proof of Lemma 5.3
in Zhang et al. (2018) and its sequel to show Equation 6 and conclude the step of ω⊥,t -→ 0.
14
Published as a conference paper at ICLR 2022
Convergence of hωti -a-.s→. ωπ. We write the update of hωti as
hωt+1i = N( 1> % I)ωt+1
=N ( 1> % I) [(Cω,t % I )(1 % hωt i + ω⊥,t + βω,tyt)]
=hωti + βω,th(Cω,t %I)(yt + βω-,1tω⊥,t)i	(By Equation 7).
We rewrite the above update as
hωt+1i =hωt i + βω,tE [hδtiφt | Ft] + βω,tξt	(9)
where	ξt =h(Cω,t %I)(yt +βω-,1tω⊥,t)i - E[hδtiφt | Ft]
We can verify that the following conditions hold (with probability 1) regarding the update of hωti in
Equation 9:
1.	E [hδtiφt | Ft] is Lipschitz continuous in hωti,
2.	ξt is a martingale difference sequence and satisfies E[kξt+1 k2 | Ft] ≤ K(1+ kωtk2) for
some constant K,
such that the conditions in Assumption B.1 of Zhang et al. (2018) are satisfied (with probability 1)
and the behavior of Equation 9 is related to its corresponding ODE (see Theorem B.2 in Zhang et al.
(2018)):
<ω> =	d∏(s，a)E Kδɪφ∣s, a]
s,a
=	dπ(s, a)Es0,a0 r(s, a) + γφ>(s, a)hωi - φ> (s, a)hωi φ(s, a)|s, a
s,a
=Φ>Dπ(γPπ -I)Φhωi + Φ>DπR
Note that (γPπ - I) has all eigenvalues with negative real parts, so does (Φ>Dπ (γPπ - I)Φ) since
Φ is assumed to be full column rank. Hence, the ODE is globally asymptotically stable, with its
equilibrium satisfying
Φ>Dπ[R+(γPπ -I)Φhωi] =0,
which is the MSPBE minimizer, i.e., hωi = ωπ. This concludes the step of hωti -a-.s→. ωπ and the
proof of Lemma 3.	□
B.3 Actor convergence
In this subsection, we establish the convergence of actor update with critic parameters ωti in Equation
2 replaced with the critic convergence point established in Lemma 3. Then, by the two-timescale
nature of the algorithm, we establish the convergence of {ωti} and {θti} generated by Equation 1 and
Equation 2.
Letθ = [(θ1)>,..., (θN)>]> andωθ be the critic convergence point for joint policy parameterized
by θ as established in Lemma 3. Define
Ai,θ = Q(St,at ωθ) ψi,θ = Vθi log πi(atloi(st); θi)
for an arbitrary θ. We study the variant of Equation 2 where ωti is replaced by ωθt :
At,θt =Q(St,at; ωθt) ψi,θt = Vθi log πi(atloi(st); θi)
~.- -- ..- .-
θt+ι =θi + βθ,t∙ At,θt∙ ψi,θt
θi+ι=X cθ,t (i,j) ∙ e	(10)
j∈N
15
Published as a conference paper at ICLR 2022
which can be rewritten as
θt+ι = (Cθ,t 乳 I )(θt + βθ,tyt,θt)
where yt,a = [(AI,θt ∙ ψ1,θt)>,…，(ANθt ∙ ψNθt)>]>.
Similar to the critic convergence, we make the decomposition θt = θ⊥,t + IX (θ/ and then show
θ⊥,t -a-.s→. 0 and convergence of hθti respectively.
Convergence of θ⊥,t -a-.s→. 0. In light of the argument for ω⊥,t -a-.s→. 0 in the proof of Lemma 3, it
suffices to show that the the boundedness of yt^t. Here,y叫 =[(A1,θjΨ1,θJ>,…,(ANθjΨNθJ>]>
is bounded because 1) Ait,θ = Q(st, at; ωθt ) is bounded since ωθt is the MSPBE minimizer; (2)
ψti,θ is bounded since by Assumption 5 it is a continuous function over a compact set.
Convergence of hθti. We write the update of hθti in Equation 10 as
hθt+ιi= g(l> 区 I)θt+ι
=N(ι> Z) I)[(Cθ,t Z) I)(I Z) hθti + θ⊥,t + βθ,tyt,θt)]
=hθti + βθ,th(Cθ,t X I)(yt,θt + βθ-,t1θ⊥,t)i	(By Equation 7).
We rewrite the above update as
hθt+1i =hθti + βθ,tESt 〜dhθti,at 〜∏hθti [hyt,θt i | Ft] + βθ,tξt
where	ξt =h(Cθ,t ZI)(yt,θt + β-1θ⊥,t)i- Est〜dhθti,at〜∏hθti [hyt,θti | Ft]
where Ft = σ(θτ, τ ≤ t), πhθti is the joint policy where each individual policy is parameterized by
hθt i. Note that ξt is a martingale difference sequence. By Assumption 5 ξt is bounded and further
by Assumption 3 we have Pt kβθ,tξt k2 < ∞. By arguments in the proof of Theorem 4.7 in Zhang
et al. (2018), we can apply Kushner-Clark lemma and conclude that hθti converges almost sure to a
point in the set of asymptotically stable equilibria of
hθ i
Est~dhθi ,at~∏<θ)
[hyt,hθii] = Est ~d{θ),
at^∏hθi
EAtM ∙ ψi,hθi
16
Published as a conference paper at ICLR 2022
C Visualization of the learned communication rule
Figure 7: Y-axis: average communication rate. X-axis: training step in log scale. The average
communication rate for detectable 10 nearby agents, with the order increasing in distance from left
to right.
17
Published as a conference paper at ICLR 2022
D	Examples and Non-Examples of Homogeneous MG
D.1 Examples
MPE tasks with homogeneous agents. We have explained in Section 3.1 that the Cooperative
Navigation task in MPE is an example of homogeneous MG. By repeating the same arguments, we
can show that all other MPE tasks with homogeneous agents are example of homogeneous MG,
including cooperative push and predator-and-prey as we have used for our experiments in Section
5. Specifically, each agent’s observation contains its absolution location and velocity, as well as the
relative location and/or velocity of other agents and environment objects (e.g., landmarks, the ball
and the target position in cooperative push, preys). For predator-and-prey, either the predators or the
preys form a team of homogeneous agents.
SMAC scenarios with homogeneous ally units. StarCraft Multi-Agent Challenge (SMAC) is
another benchmark environment for cooperative MARL. In a number of SMAC scenarios, the team
of agents consists of ally units of a single unit type (e.g., Marines), and they are tasked to defeat
an enemy team controlled by the environment, with examples including 3m, 8m, 25m, 8m_vs_9m,
2m_vs_1z, 6h_vs_8z, etc. Each ally unit (i.e., agent) observes the following attributes ofboth ally and
enemy units: distance, relative x, relative y health, shield, and unit-type. Thus, an SMAC scenario
with homogeneous ally units is similar to MPE’s predator-and-prey in the sense how it satisfies the
conditions in Definition 1. Thus, SMAC scenarios with homogeneous ally units are homogeneous
MGs.
Team sports. Sports with homogeneous players forming a team are homogeneous MGs, with
examples including basketball, American football, soccer(associate football)/ice hockey excluding
the goalkeeper. In these team sports, players’ local views naturally forms their observations that
satisfy Definition 1(iii).
Traffic with homogeneous vehicles. Traffic consisting vehicles of the same type (e.g., the same
car-following model) is an example of homogeneous MG. Like team sports, vehicles’ local views
naturally forms their observations that satisfy Definition 1(iii). Unlike team sports, these vehicles are
unnecessarily cooperative, but their reward functions are permutation invariant to satisfy Definition
1(ii).
Surveillance with drones. Drone surveillance is an application of cooperative MARL, where a set
of (homogeneous) drones is tasked to collectively monitor a ground area. Since the drones’ objective
is to cover the ground area, the task is analogous to MPE’s Cooperative Navigation to satisfy the
conditions in Definition 1.
D.2 Non-Examples
MPE tasks with heterogeneous agents. As the counterpart of Cooperative Navigation with ho-
mogenous agents, Liu et al. (2020) introduce heterogeneous navigation where half of the agents are
small and fast and the other half are big and slow. In such an MPE task, the transition function is
not permutation invariant, and therefore it is not a homogeneous MG.
SMAC scenarios with heterogeneous ally units. If the ally units in an SMAC scenario are of
different types, then the transition function is not permutation invariant, and therefore it is not a
homogeneous MG. These SMAC scenarios include 2s3z, 3s5z, MMM2, etc.
Multi-Agent MuJoCo In a MuJoCo task, a robot aims to learn an optimal way of motion. Multi-
Agent MuJoCo (Peng et al., 2020) controls each part of the robot with an agent, for example, a leg
fora spider. Since the parts of a robot are heterogeneous, Multi-Agent MuJoCo can violate condition
(i) of Definition 1.
18
Published as a conference paper at ICLR 2022
E Implementation Details
E.1 Architecture Overview
Figure 8: As an illustration, suppose there are 6 agents (i,j, k, l,p, q). At time step t, agent i
receives the observation OIt which contains information about three neighboring agents: j, k and
l. Then, agent i uses its communication network Ci (GCN) to determine which observable agents
worth communicating (agent k and l in this case) by the complete graph of agent j, k and l. Agent i
and the selected agent k and l forms a complete graph which is fed into the critic Q1 (GCN).
19
Published as a conference paper at ICLR 2022
E.2 Pseudocode
Algorithm 1 Pseudocode of our communication-efficient actor-critic algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:
48:
49:
50:
51:
init{Ci}iN=1,{Qi}iN=1,{πi}iN=1,memory,{recordsli}iN=1,{recordsih}iN=1.
for episode m = 1, 2, . . . , L do
for agent i = 1, 2, . . . , N do
sample x1m from agent i’s high-level bandit.
if x1m == 0 then
sample x2m from agent i’s low-level bandit.
Agent i does parameter consensus with agent x2m .
end if
end for
Gm = 0
for t = 1, 2, . . . , T do
Get ot+1, rt by interacting with the environment; Put ot, at, ot+1, rt into the memory.
Gm J rt + YGm
end for
if len(memory) > batch_size then
for i = 1, 2, . . . , N do
sample a batch from memory.
lciritic = 0	. Loss for agent i’s critic and comm net.
for oti, ati, oit+1, rti in batch[i] do
CtiJCi(oit),Cti+1JCi(oit+1)
Qit J Qi({(otk, atk)}k∈{i}∪Cti); Qit+1 J Qi({(otk+1, atk+1)}k∈{i}∪Cti+1)
yti := rti + γQit+1
lCritic J liritic + (Qt- yi产 + a| |C1i| Pj∈Ci Softmax(Ij) - M
end for
liritic J liriti JbatCh-Size
Send lciritic to the Adam optimizer for the update.
end for
for i = 1, 2, . . . , N do
sample a batch from memory.
laictor = 0	. Loss for agent i’s actor.
for oti in batch[i] do
ait J πi(oit)
CtiJCi(oit),Cti+1 JCi(oit+1)
Qit J Qi({(otk, atk)}k∈{i}∪Cti)
lactor J lactor - Qt
end for
lector J lactor/batch-size
Send laictor to the Adam optimizer for the update.
end for
for agent i = 1, 2, . . . , N do
Push Gm into recordsih .
Keep the latest l elements of recordsih; Compute rm1 designed in (3)
Update agent i’s high-level bandit by rm1 .
if x1m == 0 then
Push Gm into recordsli .
Keep the latest l elements of recordsli ; Compute rm2 designed in (3)
Update agent i’s low-level bandit by the rm2 .
end if
end for
end if
end for
20
Published as a conference paper at ICLR 2022
E.3 Hyperparameters
Table 2: Hyperparameters
Hyperparameter
Episode length
Number of training episodes
Discount factor
Communication network architecture
Communication network optimizer
Critic network architecture
Critic network optimizer
Policy network architecture
Policy network optimizer
Gumbel-Softmax temperature
Batch size from replay buffer
Frequency of evaluation
#Latest episodic rewards bandit store
Regularization for the communication network (α)
Value
25
40000
0.95
Concat[oi, ej(obs dim)]-GCN-Layer1(128)
-GCN-Layer2(128)-FC(2)-Gumbel-Softmax
Adam with learning rate 0.001
[oj ； aj ]{j∈{i}∪Ci}-GCN-layer1-FC(128)-
-GCN」ayer2-FC(128)-Max_pool-FC(1)
Adam with learning rate 0.01
oi-FC(128)-FC(128)-Linear(action-dim)
Adam with learning rate 0.01
1
256
per 1000 episodes
10
searched in [50,100,200,300...1000,2000]
21
Published as a conference paper at ICLR 2022
F Experiments on A Toy Example of Homogeneous MG
Λ∕ = 50
1.0d
0.2
Λ∕=10
1.0
8 6 4
000
BMα
o.o
0.0	0.2	0.4	0.6	0.8
Training Step (Ie5)
1.0
---- w/ actor consensus
----w/o actor consensus
8 6 4
000
PJBMα
0-2- U	--- w/ actor consensus
/	--- w/o actor consensus
o.o∏________,______,1___________,________
0.0	0.2	0.4	0.6	0.8	1.0
Training Step (Ie6)
Figure 9: Learning curves on the toy example.
Theorem 2 proves the asymptotic convergence of our decentralized actor-critic updates in Equations
(1)(2) with actor consensus for homogeneous MGs, which generalizes the asymptotic convergence
result without actor consensus (Zhang et al., 2018). While the actor-critic updates converge asymp-
totically both with and without actor consensus, obtaining their convergence rates require non-trivial
finite-time analysis that remains an open problem. Here, we empirically compare the actor-critic up-
dates in Equations (1)(2) with and without actor consensus on a toy example of homogeneous MG,
leaving the finite-time analysis for future work.
The toy example. We have provided the stateless MG in Kuba et al. (2021) in Section 3.1 as a non-
example. If we augment each agent i with a unique local state si , then it is easy to verify that these
local states satisfy Definition 1(ii) and it is ease to construct local observations, oi = (si, s1, .., sN),
that satisfy Definition 1(iii), such that the MG becomes an example of homogeneous MG.
Results. We define the unique local states by the trigonometric function, si = cos(N--I∏), i =
1,…,N. We use feature function φ(s, a) = Concat[{si, one_hot(ai)}i] for the linear critic, and
parameterize the actor as a linear function of oi followed by softmax over the two actions. The
consensus matrix is 1/N everywhere for both the critics and the actors. For effective training, we
1) replace the sparse reward function with a denser one, R(s, a) := meaniisi≥o{1[ai = 1]}-
meani:si<0{1[ai = 1]}, such that the optimal joint action is ai = 1 for i ≤ N/2 and ai = 0
for i > N/2 that gets a reward of +1, and 2) instead of using the decaying stepsizes as suggested
in Assumption 3, which we found is not effective for training, we use the optimizer of Adam for
adaptive learning rates. Figure 9 show the results for N = 10, 50. While both converge, actor
consensus slightly improves the learning efficiency.
22
Published as a conference paper at ICLR 2022
G Experiments with Various Amounts Of Communication
EiEypos - d",
Navιgatιon-nl5
Push nl5
-2000
-2200
-2400
-2600
-2800
-3000
[random,rule_based]
[comm πet,baπdit]
[raπdom,raπdom]
EiE-ypos-d°,
-1300
-1350
-1400
-1450
-1500
-1550
-1600
0	20	40	60	80
observation-action comm %
[random,rule_based]
[comm πet,bandit]
[raπdom,raπdom]
EiEypos - d",
0	20	40	60	80 IOO
observation-action comm %
-2000
-2200
Push n30
-2400
-2600
-2800
0	20	40	60	80	100
observation-action comm %
ɪ-ɪɪ
ɪ-ɪɪ
ɪ fɪ
ɪ-ɪɪ

Figure 10: Performance of our algorithm and the baselines under different observation-action com-
munication thresholds. For fair comparison, the frequency of doing parameter consensus is the same
for all the algorithms under different observation-action communication budgets. The error bar cap-
tures the standard deviation of the mean performance of the last 5 training policies (at 9e5, 9.25e5,
9.5e5, 9.75e5, 10e5 steps) across 4 seeds.
23