Published as a conference paper at ICLR 2022
Particle Stochastic Dual Coordinate Ascent:
Exponential convergent algorithm
FOR MEAN FIELD NEURAL NETWORK OPTIMIZATION
Kazusato OkoIj, Taiji Suzuki1,2#, Atsushi Nitanda3,4,2,*, DennyWu5,6,?
1University of Tokyo, 2RIKEN AIP, 3Kyushu Institute of Technology, 4JST PRESTO,
5University of Toronto, 6Vector Institute
*oko-kazusato@g.ecc.u-tokyo.ac.jp, ^taiji@mist.i.u-tokyo.ac.jp,
*nitanda@ai.kyutech.ac.jp, ?dennywu@cs.toronto.edu
Ab stract
We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer
neural networks in the mean field regime that achieves exponential convergence
rate in regularized empirical risk minimization. The proposed algorithm can be
regarded as an infinite dimensional extension of Stochastic Dual Coordinate Ascent
(SDCA) in the probability space: we exploit the convexity of the dual problem, for
which the coordinate-wise proximal gradient method can be applied. Our proposed
method inherits advantages of the original SDCA, including (i) exponential con-
vergence (with respect to the outer iteration steps), and (ii) better dependency on
the sample size and condition number than the full-batch gradient method. One
technical challenge in implementing the SDCA update is the intractable integral
over the entire parameter space at every step. To overcome this limitation, we
propose a tractable particle method that approximately solves the dual problem,
and an importance re-weighting technique to reduce the computational cost. The
convergence rate of our method is verified by numerical experiments.
1 Introduction
A major challenge in developing an optimization theory for neural network is to handle the non-
convexity of loss landscape. Recent works showed that overparameterization is one key property that
makes global optimization possible. In particular, the effectiveness of overparameterization has been
extensively investigated in the theory of Neural Tangent Kernel (NTK) and the mean field analysis.
The NTK analysis considers a linear approximation of neural networks and casts the gradient descent
dynamics to that in a corresponding reproducing kernel Hilbert space (Jacot et al., 2018). Since the
whole argument can be carried out essentially in a linear space, we can derive both global convergence
and generalization guarantees (Du et al., 2019; Allen-Zhu et al., 2019; Zou et al., 2018; Nitanda &
Suzuki, 2020). However, one of the biggest drawbacks of the NTK approach is that the fixed kernel
fails to capture the feature training aspect of deep learning. Indeed, this feature learning ability of
neural network is an important ingredient that separates deep learning from shallow models such as
kernel methods (Suzuki, 2019; Yehudai & Shamir, 2019; Ghorbani et al., 2019).
In contrast, the mean field analysis considers a smaller scale of parameters than the NTK analysis,
which allows the parameters to travel away from the initialization. However, the optimization problem
is no longer convex and the analysis requires more involved mathematical tools. A typical approach
is to formulate the optimization of neural networks as a partial differential equation (PDE) of the
distribution of parameters (Nitanda & Suzuki, 2017; Mei et al., 2018; Chizat & Bach, 2018; Rotskoff
& Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2020). A benefit of the PDE formulation is
that the objective function becomes convex in the space of measures. This being said, since the
dynamics is essentially infinite dimensional, most convergence results are shown in the continuous
limit (infinite-width limit). Also, many existing guarantees only handle the continuous time dynamics,
due to difficulty in analyzing the time discretization of the PDE. Indeed, Nitanda & Suzuki (2017);
Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018) proved convergence to a global optimal
solution for essentially infinite width networks under continuous time settings. On the other hand,
global optimality may be shown under less restrictive settings if we consider the noisy gradient
1
Published as a conference paper at ICLR 2022
descent algorithm; the corresponding stochastic dynamics can be formulated as McKean-Vlasov
dynamics, and its convergence has been studied in Mei et al. (2018); Hu et al. (2019). However, most
of these works establish convergence result for the discrete-time finite-width method by bounding
the difference from its continuous-time and infinite-width limit counterpart, which usually incurs a
large discretization error. Hence, it is important to develop a practical algorithm with optimization
guarantees for finite width network and discrete time setting.
Recently, Nitanda et al. (2021) proposed a completely time-space discretized algorithm termed Particle
Dual Averaging (PDA) that attains polynomial time convergence guarantee to the (regularized) global
optimal solution. This method combines the dual averaging technique (Nesterov, 2005; 2009; Xiao,
2009) and the gradient Langevin dynamics (Vempala & Wibisono, 2019). More specifically, the
algorithm realized time discretization by extending the dual averaging technique for finite dimensional
convex optimization to the infinite dimensional setting, and realized the space discretization by
sampling finite number of particles via the gradient Langevin dynamics. Eventually, the method
achieves O(1/t) convergence with respect to the outer-loop iteration t. Although O(1/t) is optimal
in an online optimization setting, there is still room for improvement for a finite sample size setting.
Motivated by this observation, we propose a method that improves the convergence rate from the
polynomial order (O(1/t)) to an exponential order (O(exp(-Ct))).
Our contribution. We propose Particle-SDCA, a novel optimization algorithm for two-layer neural
networks in the mean field regime that achieves linear convergence (w.r.t. the outer loop steps) for
regularized empirical risk minimization1 *. We adopt the stochastic dual coordinate ascent (SDCA)
technique (Shalev-Shwartz & Zhang, 2013a; TakgC et al., 2013; ShaleV-ShWartz & Zhang, 2013b)
to achieve linear convergence rate and much better dependency on the sample size n. Importantly,
the integration with respect to the probability measure on the parameters is approximated by a finite
number of particles. In contrast to existing methods, in which particles are sampled at each iteration,
we employ a novel update rule of the weights on the particles so that the sampling procedure is only
performed once in n-iterations, which provides significant computational speedup.
Furthermore, unlike many existing analyses concerning the time-space discretized version of McK-
ean-Vlasov dynamics (Mei et al., 2018; Bou-Rabee & Schuh, 2020; Bou-Rabee & Eberle, 2021),
we do not couple the discrete time update with the continuous time counterpart by taking the small
step size limit. Instead, we directly analyze the convergence of the discrete time update; hence, our
method converges to the optimal solution with a fixed (non-vanishing) step size. Our contribution can
be summarized as follows (for comparison with existing works see Table 1):
•	We propose a new optimization method for mean field representation of neural networks that is
efficient in the finite sample setting. By utilizing the SDCA technique, the algorithm achieves
exponential convergence with respect to the outer iteration. Moreover, the global convergence is
guaranteed without letting the step size converge to 0.
•	The integral required in each update is approximated by an average over finite number of particles.
The number of particles can be of linear order with respect to a required precision, and the
computational cost for re-sampling can be of poly-log order.
•	Thanks to a novel re-weighting scheme for particles, we only resample particles once in n updates,
where n can be as large as the total sample size n; this significantly reduces the computation cost.
Method (authors)	Outer-iteration	Inner-iteration	Total complexity
PDE (Bou-Rabee & Schuh, 2020)	ep1 log(1∕ep)	Mf	M1⅜p1 log(1∕ep)
PDA (Nitanda et al.,2021)	与1	M2G*	M2G*ep1
P-SDCA (ours)	n(n+γλ2 )ιog( U)	M3(n+G*)	M式1+ G )(n + 哀)log(含)
Table 1: Required computational complexity to achieve ep-accuracy (P(P) — P(p*) ≤ ep). n is the re-
sampling interval. Mi (i = 1, 2, 3) is the number of particles required to approximate the true distribution
for each method: They are given as Mi = Θ(e-2), Mi = Θ(e-2 log(n)), Mi = Θ(e-1 log(n)). Gi is
the number of gradient evaluation to obtain the sampling distribution which has TV-distance p from the true
distribution; when MALA is used, this is given as Gi = O(n log(1/p)3/2 ).
1We remark that our goal is to propose a more efficient algorithm for optimizing mean field neural network
with faster convergence guarantee, rather than to gain a better understanding on standard neural network training.
2
Published as a conference paper at ICLR 2022
2 Preliminaries
We first introduce the problem setting and some notations used in the following sections. We
consider the empirical risk minimization problem in the supervised learning setting with a two-
layer neural networks, where the input and output spaces are denoted by X ⊂ Rd and Y ⊂ R,
respectively. Let hθ : X → Y be one neuron with the parameters θ ∈ Rd . For example, we may set
hθ (x) = tanh(r)σ(w>x) for θ = (r, w) ∈ R × Rd and an activation function σ (the tanh operation
is merely to ensure boundedness of the output). In the mean field regime, we consider neural networks
represented as an average of neurons: fΘ (x) = MM PMM=I hθm (x) where Θ = (θm)MM=1 ⊂ Rdis
a set of parameters. The continuous limit in the mean field regime is obtained by taking M → ∞,
and by an analogy to the law of large numbers, the function fΘ converges to the following integral
representation of a two-layer neural network:
fp(x) =
hθ(x)p(θ)dθ,
where p : Rd → R is a probability density function on Rd representing the weight of parameters.
Here, we denote by P the set of probability density functions on Rd . As in the typical mean field
analysis (Nitanda & Suzuki, 2017; Mei et al., 2018; Chizat & Bach, 2018), we aim to optimize the
density function p ∈ P so that the neural network fp accurately predicts the output y ∈ Y from
the input x ∈ X . To define the empirical risk and predictive risk, we let `(z, y) : Y × Y → R be a
convex loss function, such as the squared loss `(z, y) = (z - y)2/2 for regression, and the logistic
loss `(z, y) = log(1 + exp(-yz)) for classification. For each (xi, yi), we use the notation hi (θ) and
'i(f (Xi)) to indicate hθ(Xi) and '(f (Xi), yi) respectively. To estimate the density function P ∈ P,
we minimize the regularized empirical risk defined by
1n
min — f'i(fp(Xi)) + λKL(p∣∣N(0, σ2I))
∈P n
i=1
(1)
where λ > 0 is a regularization parameter and KL(p, N(0, σ2I)) represents the KL divergence from
the normal distribution N(0, σ2I) with mean 0 and covariance σ2I to the distribution with the density
function p. This type of regularization naturally arises in training neural networks by the gradient
Langevin dynamics (Hu et al., 2019; Chen et al., 2020b). In our framework, we consider directly
optimizing the empirical risk with KL regularization, instead of running gradient Langevin dynamics
(i.e., noisy gradient descent) on the unregularized objective. By decomposing the KL divergence in
Eq. (1), we obtain the following equivalent representation of the objective:
min P(P) = n X 'i(fp(Xi)) + λι Z p(θ)kθ∣∣2dθ + λ Z p(θ) log(p(θ))dθ,	(2)
where λ1 ,λ2 > 0 are the regularization parameters. Importantly, the objective is convex with respect
to the density P ∈ P although it is non-convex with respect to each parameter θ. We make full use of
this convexity in the following analysis.
The main difficulty to optimize (2) stems from the following two factors: (i) there is an integral with
respect to P that should be approximated by a computationally tractable scheme, and (ii) there is a
summation over n data points, which could lead to costly gradient evaluations. To overcome the
former difficulty, we sample a finite number of particles from the density P. As for the later, we adopt
the stochastic dual coordinate ascent (SDCA) technique, which we explain in the next section. 3
3 Proposed method: Particle-SDCA
We now explain our proposed Particle-Stochastic Dual Coordinate Ascent (P-SDCA) method.
3.1 Dual problem and algorithm description
The SDCA method relies on the dual problem. The dual of the optimization problem (2) can be
derived from the following Fenchel’s duality theorem. Note that the convexity of P(P) with respect
to the density function P is important in the derivation.
3
Published as a conference paper at ICLR 2022
Proposition 1 (Fenchel’s duality theorem). Suppose that `i : R → R is a proper convex function,
hi : Rd → R is bounded, and the primal problem satisfies inf p∈P P(p) > -∞, then the following
duality holds:
pinPP(P)=gsun Dgg=gun {- 1X 葭⑥-λ2 log U q[g] ⑻dθ
where q[g](θ)
eχp{-λ2 (n1 Pnn=I hi(θ)gi + λιkθk2)} and 'i
is the convex conjugate2 of `i .
Moreover, p* ∈ P and g* ∈ Rn are both optimal solutions of primal and dual problems if and
only if fp*(xi) ∈ ∂'* (gi) and p[g*] = p* , where p[g](θ) := R 勺黑(“3 and ∂'* denotes the
Sub-differential of '*.
This proposition can be derived from Theorems 3 and 8 of Rockafellar (1967) (its finite dimensional
version can be found in Corollary 31.2.1 of Rockafellar (1970)). See Appendix A for the complete
proof. By the optimality condition, we can recover the optimal primal solution p* from the optimal
dual solution g*. Hence, we aim to optimize the dual problem as p* = p[g*]. In the dual problem,
updating one coordinate gi can be carried out by picking up one data points (xi, yi). The main
strategy of SDCA is to randomly pick the sample index i ∈ [n] 3 and update the corresponding
coordinate gi by maximizing the dual (i.e., dual coordinate ascent). Before we state the algorithm,
we assume the following condition.
Assumption 1.
(AA 'i : R → R is (1∕γ)-smooth4 forall i ∈ [n].
(A2) supθ∈Rd ∣hn(θ)∣ ≤ 1 forall i ∈ [n].
The first assumption (A1) is satisfied, for example, by the squared loss `i(u) = (yi - u)2∕2 with
γ = 1 and the logistic loss `i(u) = log(1 + exp(-yu)) with γ = 1. As for the second assumption
(A2), our analysis is valid for bounded neurons, and the upper-bound 1 is chosen just for simplicity
and can be replaced by another value. In fact, if we assume supg—— ∣hn(θ)∣ ≤ R instead, the analysis
can be reduced to the setting of R = 1 by rescaling hi and the loss function, i.e., R can be absorbed
into the smoothness parameter, which then becomes R∕γ.
As stated above, the strategy to optimize the dual problem is to (i) randomly pick i ∈ [n] and (ii)
update gi by approximately maximizing D(g) with respect to gi with other coordinates fixed. For
the update of each coordinate, we employ the proximal gradient-type update where we optimize the
lower bound of D(g) given by the following lemma (the proof can be found in Appendix C).
Lemma 1. Under (A2) of Assumption 1, for g ∈ Rn and δg ∈ Rn, the second term of D(g) can be
bounded as
λ2log(Rq[g + δg](θ)dθ) ≤ -nnPn=IRhn(θ)p[g](θ)dθsgn+用2l∣δgkι+λ2log(Rq[g](θ)dθ).
Using this inequality, we can derive the update rule as a maximizer of an lower bound of the objective
as follows. When we update only the i-th component (i.e., δgj = 0 (∀j 6= i)) for randomly chosen
index i, the dual objective can be bounded as follows:
D(g + δg) ≥ D(g) + 1H- '*(gi + δgn)) + n1R hi(θ)p[g](θ)dθδgn — f δg2.
Hence, by substituting g J g⑴ and maximizing the right hand side with respect to δgη = gn - g(t),
we obtain the following ideal version of the update rule of g⑴：
g(t+I) J argmax [-'*(gn)+ Z hi(θ)p[g(t)](θ)dθ(gn - g(t)) - ʒɪ-(gn - g(t))2l ,	(3)
gi	2nλ2
while the other components are not updated： ggj(t+1) = ggj(t) (j 6= i). Note that this update requires
only one data point while the full gradient descent requires the whole n data points. Along with the
updated dual variable gg(t), we update the primal variable as pg(t) = p[gg(t)]. The whole optimization
procedure based on this update rule is summarized in Algorithm 1.
2The convex conjugate f * of a convex function f : R → R is defined by f * (U) := supχ∈κ{ux — f (x)}.
3We write [N] := {1, . . . , N} for an integer N.
4f : RD → R is L-smooth if f is differentiable and ∣∣Vf (x) — Vf (y)k ≤ Lkx — yk for all x,y ∈ RD.
4
Published as a conference paper at ICLR 2022
Algorithm 1 Dual Coordinate Descent in the continuous limit
Require: training data {(xi, yi)}in=1 and number of iterations tend
1:	Chooseg(0) s.t.唐0®(O))I ≤ 1 (i = 1,...,n) and屐®(O)) ≤ '(0)
2:	for t = 0, 1, . . . , tend do
3:	Randomly choose it from {1, 2, . . . , n}
4:	g(t+1)	-	argmaχgit ∈R	n-'*t (git)	+ R hit (θ)p(t)(θ)dθ(git	- 婢) -	2n1λ2 |git	- g(t)|2}.
5:	Update the primal solution as p(t+I) = p[g(t+1)].
6:	end for
7:	return g(tend )
3.2 Our proposal: Particle update method
The update (3) requires an integration with respect to p[g(t)]. We approximate the integral by a
weighted sum over M particles:
R hi(θ)p[g(t)](θ)dθ ≈ PPM*θm,	(4)
m=1 rm
where (θm)mM=1 are particles that approximately cover the support of p[g(t)], and (rm )mM=1 are the
weights of each particle. Therefore, the key point of designing an efficient algorithm is to construct a
“good” set of particles (θm)mM=1 and their weights (rm)mM=1.
The construction of particle approximation consists of two stages: (1) the sampling stage, and (2) the
re-weighting stage. We perform the sampling stage once every n ∈ [n] iterations, and perform the
re-weighting stage in the other iterations. The detailed description of each stage is given below.
(1)	Sampling stage. In the sampling stage, we sample M i.i.d. particles (θm)mM=1 from the density
function p[g(t)] by using a Monte Carlo sampler, and place an even weight rm(t) = 1/M (m =
1, . . . , M) on the sampled particles (θm)mM=1. As for the sampler, we may employ the unadjusted
gradient Langevin algorithm (ULA), or the Metropolis-adjusted Langevin algorithm (MALA), both
described in the following subsection. Note that the distribution of particles obtained from the Monte
Carlo samplers as listed above may be biased away from the target distribution. We tolerate this
inaccuracy by only requiring the particles to obey an approximated density function p(t) that is in (Ct)
distance from the target density p[g(t)] measured by the total variation distance5. Our convergence
rate analysis will take this error into account.
(2)	Re-weighting stage. Since the sampling procedure requires relatively heavy computation, it
is desirable to perform this operation once in a while. We consider performing the sampling step
once in n iterations. In between the sampling stages, We instead perform the re-weighting stage,
in which we iteratively update the weight of each particle, rather than resampling them. Recall
thatp[g(t)](θ) <x q[g(t)](θ) = exp{ — λ12 (1 Pn=I hi(θ)g(t) + λι∣∣θk2)} which yields q[g(t+1)]=
q[g(t)] eχp[-hit (θ)(gi(tt+1) -g(t))/(nλ2)] for the index it chosen at the t-th iteration. We adapt this
relation to update the weight rm(t) of particles as
rmm+1) = r(m) exp[-hit (θm)(g(t+1) - g(t))∕(nλ2)],
during the re-weighting stage because rm(t) represents “importance” of θm which is proportional
top[g(t)](θm)∕p[g(nT)](θm) so that the particle approximation (4) is (nearly) unbiased. We allow
the number of particles to depend on the outer-loop iteration T, and write M(nT) as the number of
particles in the T-th outer-loop. The entire procedure is summarized in Algorithm 2.
Finally, we output the solution of the last iterate. We call this output as Option (A) and write its solution
(A)
as gout . Alternatively, there is another choice Option (B), which randomly selects the stopping time
t[nd from the final n-iterations, i.e., that is randomly chosen from {nTend - n + 1,..., n‰d},
and returns the solution go(But) = g(t0end). This technique improves the accuracy up to O(n) factor.
Therefore, we can reduce the number of particles by O(n) times.
5The total variation (TV) distance between two probability measures with densities p, q is defined as
TV(p∣∣q) := 1 R |p - q∣dθ.
5
Published as a conference paper at ICLR 2022
Algorithm 2 Dual Coordinate Descent with the particle method
Require: training data {(xi, yi)}" and numbers of inner-loop iterations n and outer-loop iterations
Tend,
1:	Chooseg(0) s.t. |'7(g(0))∣ ≤ 1(i = 1,...,n) and猿(g(0)) ≤ '(0)
2:	forT = 0,1, .. .,Tend - 1 do
3:	Randomly (approximately) draw i.i.d. parameters θm (m = 1,..., M(nT)) fromp(nT)(θ)dθ
that satisfies TV(p(nT)∣∣p[g(nτ)]) ≤ ∈CnT).
4:	r(nT) - MnT) (m =1,...,M(nT))
5:	for t = nT, nT + 1,..., nT + n — 1 do
6:	Randomly choose it from {1, 2, . . . , n}
7:	g(t+1) Jaggm∈aχ(-%IgG+PmPMrnm)h;(SJm)(git -g(t))- 2nλ2(git-g(t))* 1 21.
it	m=1 rm
8:	h(m+1) J hm exp (-nλ2 hit (θm)(g(+" - g?)) (m =1,∙∙∙,M (nT)).
9:	end for
10:	end for
11:	return Option (A): gOA) = g(nτend); Option (B): gOB) = g(tend) for t0end that is randomly chosen
from n(Tend -dne ) + 1, . . . , nTend}.
Sampling algorithms. Here, we present two examples that can be applied to sampling the particles
(θm)M=ι from p[g](θ) 8 exp(-U(θ)) where U(θ) is the potential function defined as U(θ):=
λ2( n Pn=ιhi(θ)gi + λ1kθk2.
(1) ULA (unadjusted gradient Langevin algorithm): The algorithm generates samples from the
following noisy gradient descent update:
θk+1 = θk - NU(θk) + pηξk,
where η > 0 is the step-size and ξk 〜N(0, I) is a random noise generated from the d-dιmensιonal
standard normal distribution. The sampling efficiency of ULA has been extensively studied in the
literature (Dalalyan, 2017; Durmus & Moulines, 2017; Vempala & Wibisono, 2019).
(2) MALA (Metropolis-adjusted Langevin algorithm): The algorithm combines ULA with an
accept-reject step according to the Metropolis-Hastings algorithm. That is, we generate a proposal
θk+1 by ULA, i.e., θk+1 = θk - NU(θk) + √2ηξk, which is accepted with probability α given by
α “{1，U⅛⅛" },
where q(θ0∣θ) H exp(-∣∣θ0 - θ - NU(θ)k2∕4η). If θk+1 is not accepted, we set 京k+1 = θk. The
convergence rate of MALA has been studied by Bou-Rabee & Hairer (2013); Ma et al. (2019); Chen
et al. (2020a), to name a few. Due to the Metropolis-Hastings step, MALA allows for larger step size
than ULA, which leads to an improved iteration complexity with respect to the desired accuracy C .
4 Convergence analysis
We present the convergence analysis of our method under the following assumptions.
Assumption 2.
(A3) infχ 'i(x) › -∞ holds for all i ∈ [n]. There exists a Constant Y0 such that ∀i ∈ [n],
'* is γ0-smooth in the interval {y ∈ dom('*) | ∂'*(y) ∩ [-2,2] = 0}. Bi :=
sup {∣'i(x) - infχ0'i(x0)∣	|	|x|	≤ 1, i ∈	[n]} and	B2	:=	SuP {∣'i(x)∣	|	|x|	≤	1,	i ∈	[n]}
are both finite.
(A4) hi is L-smooth for all i = 1, . . . , n.
It is straightforward to verify the finiteness of B1, B2 and infx `i (x) in (A3) for several practical
loss functions such as the squared loss and logistic loss. The smoothness assumption (γ0) on ' is
required to fill in the gap between g(t) and g(t), and is also satisfied by the loss functions listed above:
the squared loss satisfies it with γ0 = 1 and the logistic loss satisfies it with γ0 = (e2 + 1)2/e2. The
6
Published as a conference paper at ICLR 2022
second assumption (A4) is required to prove convergence of the sampling procedure, and can be
omitted if we employ a particle sampler that does not require smoothness of the density function. This
assumption is satisfied, for example, by a (truncated) two-layer neuron hθ(x) = tanh(r)σ(w>x) for
θ = (r, w) ∈ R × Rd (which also satisfies the boundedness assumption (A2)). We are now ready to
def
present the convergence rate of our proposed method Algorithm 2. Here, We define S =
nγλ
2(i+nγλ2).
mi. . . . . _ _ Λ n	.1 . λ	. ∙	-1	1 1 11 τ . t	~ ΓΓ∖	EI	* ,	，，分
Theorem 1. Suppose that Assumptions 1 and 2 hold. Let tend = nTend. There exist constants Ci,
C2 and C3 which can depend on S and λ2 such that, if
eCT) ≤ Ci exp(-ST∕2), M(nT) ≥ -7⅛- log(4nT‰d/δ) (VT ∈ [Tend]),	⑸
(C )
for 0 < δ < 1 and λ2 ≥ 2B/n, then there exists an event E6 satisfying P (E) ≥ 1 - δ in which the
conditional expectation of the duality gap can be bounded by E P (p[go(Aut)]) - D(go(Aut)) E ≤ eP
as long as the number of iterations tend satisfies the following conditions:
(Option A)
(Option B)
1				1
tend ≥ 2	In+E	log	n+	γλ2
tend ≥ 2	1+ γλ2,	log	1+	1 nγλ
+ A
(6)
(7)
Moreover, the constants have the following dependency on n, n, λ2, s: C2 = exp( 4Bn), C-1
O G-1% exp (n(2C≡)) and C3 = O(max{L, λ-3∕2(1 + …)}).
The proof and detailed descriptions of each constant are given in Appendix C. The theorem indicates
that our proposed method achieves exponential convergence With respect to the number of outer
iterations. This coincides With the convergence rate of the finite dimensional version of SDCA
(Shalev-ShWartz & Zhang, 2013a). In contrast, a (non-stochastic) full-batch gradient descent requires
O((n∕λ2γ) log(1∕ep)) total gradient evaluations to optimize a convex objective with a COnditiOn
number I∕(λ2γ). Importantly, SDCA (and our method) “decouples” the sample size n and the
condition number I∕(λ2γ) yielding only O((n + I∕λ2γ) log(1∕ep)) iterations, which is beneficial
in a setting of big data and a small regularization parameter.
The difference between options (A) and (B) is only an O(log(n)) factor in terms of the number of
iterations. However, this improvement affects the number of particles given by Eq. (5). Indeed,
the log(n) factor improvement of tend allows us to take M(nTend) ≥ Ω(C2C3∕(C2ep)) in the last
inner loop, whereas option (A) requires M(nτend) ≥ Ω(nC2C3∕(C2ep)), Either way, the number of
particles can be of linear order in terms of the inverse of the target precision eP.
Remark 1. Depending on the regularization strength λ2, the re-sampling interval n should be chosen
differently. For constant order (non-vanishing) λ2, we may take the re-sampling interval as n = n so
that all constants are bounded. On the other hand, in the small λ2 regime, say λ2 = O(1∕n), we
have C2 = exp(O(n)), and there also appears an exponential dependency on Ci w.r.t. n. In this
case, n = O(1) is a better choice to avoid the large particle complexity. Butfor such choice of n,
we should re-sample at almost every iteration, which is computationally demanding; this presents a
trade-off. In practice, re-sampling can be executed in parallel and thus efficient training (using GPU)
is possible. Importantly, to obtain the usual O(1 ∕√n) -generalization error, λ2 = 1∕√n is sufficient
(Nitanda et al., 2021) and thus we may take n = Ω(√n) in that situation. We however COnjeCtUre
that this trade-off is unavoidable without additional assumptions.
Sampling complexity. In Theorem 1, it is assumed that the density p(nt) from which the particles
are generated is within eJCnT) from the target density p[g(nT)] measured by the total variation distance.
The next proposition provides an upper bound of the number of iterations required to obtain the e(CnT)
accurate distribution by the Monte-Carlo samplers such as ULA and MALA. The key property of the
target density that determines the convergence rate of these methods is the log-Sobolev inequality
with a constant cLS (see Definition 2 for its precise definition).
6Intuitively, the event E represents an event in which “good” particles are sampled from the sampler to
approximate the integral.
7
Published as a conference paper at ICLR 2022
Proposition 2. Under Assumptions 1 and 2, the target density p[g(nT)] at each outer iteration satisfies
the log-Sobolev inequality with constant CLS = 2^ exp(-4B2∕λ2). Accordingly, the number of
iterations K() to obtain the e^nT) -accurate distribution can be evaluated as follows:
(ULA)	K(T) ≤ 4L2 max [1, (4T) ∖ log ∣^4B2∕(λ2e^T，，
LS	(C )
(MALA)	K(T) ≤ O ]-5/2 (B2")+λl +log(1∕eCT)))3/2 d .
The proof and a more detailed statement can be found in Appendix B. Roughly speaking,
this proposition states that the number of iterations for ULA can be simplified as K() ≥
Ω ((cLseCT) )-2 log(n)). On the other hand, MALA requires only poly-log order complexity with
respect to e^nT) as K(T) = Ω (C-15/2 log(1∕eCnT) )3/2), which is therefore more preferable to obtain
higher accuracy. Combining the required number of particles M(nT) and the number of iterations
(T)
K) , for the sampler, we see that the total computational complexity is still of polynomial order.
Remark 2. Note that the log-Sobolev constant CLS also depends exponentially on 1∕λ2. While the
exponential factor can be avoided in the outer loop (Theorem 1) by appropriately setting n, such
dependence is known to be unavoidable for CLS in the most general setting (Menz & Schlichting,
2014). An interesting direction is to explore additional assumptions that remove this dependence.
Total complexity. By summarizing the results we obtained above, the total computational com-
plexity of our algorithm (Algorithm 2) with the MALA sampler can be roughly bounded as
PT誓 M (nT )(1 + nK(T )∕n)n = O ( n0pn) (n + *)[((1 + =)嗤立)51),
1
where we assumed C1-1 , C2 = O(1) (see Remark 1 on how this is achieved). Detailed evaluation of
the computational complexity with the ULA sampler can be found in Appendix B.2.
5	Additional related works
Hu et al. (2019); Sirignano & Spiliopoulos (2020) showed a linear convergence rate of the McKean-
Vlasov dynamics. However, the theory only handles the infinite width limit and continuous time
dynamics, and it is from trivial to covert their results to a discrete time and finite width setting. Mei
et al. (2018) also analyzed space-time discretization of a similar dynamics. However, a quantitative
convergence rate is not provided, and the time discretization error grows exponentially with the time
horizon. Compared to these results, we deal with general (1∕γ)-smooth loss functions and made the
dependency on the parameters λ2 and γ explicit in our convergence rate. Bou-Rabee & Schuh (2020);
Bou-Rabee & Eberle (2021) showed an exponential convergence of gradient Langevin dynamics
with an interaction potential, which includes certain mean field models. When specialized to mean
field neural networks, our formulation covers a much wider range of objectives and loss functions.
Furthermore, they considered only the Hamiltonian Monte Carlo sampler, and the analysis relied on
the coupling between the discrete time dynamics and the continuous time counterpart. Consequently,
their outer-loop iteration complexity is larger than O(log(1∕eP)∕eP), whereas our bound requires
only poly-log order cost due to the flexible choice of sampling algorithm. Jabir et al. (2021) also
considered spatial-time discretization of mean field dynamics of deep neural network. However, it
contains the time discretization lag h in their convergence rate, while our method is purely discrete
time and is not affected by such a time discretization error. Moreover, their analysis requires a strong
regularization (which corresponds to λ1 in our setting) so that the objective becomes convex-like
with respect to each particle to show the geometric ergodicity; this is not required in our analysis.
6	Numerical experiments
We verify our convergence rate analysis on regression problems with the squared loss. We consider
teacher-student setup in which teacher and student are both two-layer neural networks of different
width. The training dataset is generated by the teacher model: yi = σ(w)>xi + b)) + ei, where xi
and ei are independently drawn from Gaussian distributions. We optimize a tanh neural network
(hθ(x) = tanh(w>x + b) with θ = (w, b)) of width Ms as a student using Algorithm 2.
8
Published as a conference paper at ICLR 2022
Linear convergence of P-SDCA.
Figure 1: (Left) Linear convergence of PSDCA to the optimal value of the regularized objective, with test loss
as for the supplementary. (Right) Comparison of predictive accuray of P-SDCA, PDA, and SGD. The error-bar
indicates the standard error over 10 independent repetitions.
Gradient evalueations ie5
Comparison of predictive accuracy.
Convergence rate of P-SDCA. We verify the linear convergence rate of P-SDCA as suggested
by our theoretical analysis in a simple setting. To verify the convergence rate, we run the method
with 1000-steps of ULA. As for the teacher model, we use the ReLU activation for σ and (note that
even though this is a simple setting, the teacher model is “not” included in the student model). The
number of examples and dimensionality of the input space are n = 500 and d = 50, respectively.
The other hyperparameters are set to λ1 = 10-4, λ2 = 10-4, and Ms = 200. The left side of
Figure 1 depicts the convergence of the primal objective to the optimal value. The x-axis is the
number of outer-iteration and the y-axis is the excess primal objective P(∙) - infP P(P) including
the regularization terms on a training set and a test set in log-scale. The integral with respect to the
primal solution was computed by the particle average and the entropy is estimated by using the k-NN
entropy estimator (Kozachenko & Leonenko, 1987; Brodersen, 2020). Since the y-axis is log-scaled,
we can clearly observe the linear convergence of P-SDCA.
Comparison of predictive accuracy with SGD and PDA. We employ a sign activation for the
teacher model (σ(∙) = sign(∙)), and the student is an overparameterized tanh network of width
Ms = 200. The number of examples and input dimensionality are n = 1000 and d = 50, and
we set λ1 = 10-3, λ2 = 10-4. As for SGD, we cannot apply the entropic regularization and
thus the term is omitted in the optimization. We also used the neural tangent kernel (NTK) scaling
(fθ(x) = √M PmM=1 hθm (x)) for SGD optimization in addition to the mean field scaling. The test
error with respect to the number of gradient evaluations is displayed in the right side of Figure 1. We
make the following observations: (i) P-SDCA achieves a better test error than other methods; (ii)
SGD achieves minimal test error at the early stage but does not perform well afterwards; (iii) the
mean field parameterization outperforms the NTK parameterization in terms of test loss; one possible
explanation of this observed advantage is the presence of feature learning, which we empirically
demonstrate in Appendix D.
7	Conclusion
In this paper, we proposed a new stochastic optimization technique for two-layer neural networks
with the mean field representation. Our algorithm can be seen as an infinite dimensional extension
of SDCA that maintains its advantage, namely, that it converges linearly with respect to the outer-
iteration, and is much more efficient in terms of the sample size than non-stochastic methods. In the
algorithm, we approximate the integral required in each update by an average over finite number of
particles. Remarkably, the number of particles can be of linear order with respect to the required
precision, and its sampling cost can be of poly-log order. This is significantly different from existing
numerical approximation approaches of McKean-Vlasov equations, which typically require either
vanishing step size or exponentially large particle size. We validated the linear convergence of
P-SDCA in numerical experiments, and demonstrated that it outperforms other existing methods.
In the convex optimization literature, it is known that the SDCA algorithm considered in this work is
not rate optimal - the optimal algorithm can be obtained by combining Nesterov,s acceleration with
stochastic methods (Allen-Zhu, 2017; Murata & Suzuki, 2017). Extending our algorithm to such
an optimal accelerated method is an interesting future direction. In addition, while the exponential
dependence on I∕λ2 in the log-Sobolev constant may be unavoidable in the general setting, it is
worth investigating whether such dependency can be improved under structural assumptions.
9
Published as a conference paper at ICLR 2022
Acknowledgment
TS and KO were partially supported by JSPS KAKENHI (18H03201), Japan Digital Design and JST
CREST. AN was partially supported by JSPS Kakenhi (19K20337) and JST-PRESTO (JPMJPR1928).
References
Z. Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings
OfAnnualACM SIGACT Symposium on Theory ofComputing 49, pp. 1200-1205. ACM, 2017.
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In Proceedings of International Conference on Machine Learning 36, pp. 242-252, 2019.
N. Bou-Rabee and M. Hairer. Nonasymptotic mixing of the MALA algorithm. IMA Journal of
Numerical Analysis, 33(1):80-110, 2013.
N. Bou-Rabee and A. Eberle. Mixing time guarantees for unadjusted Hamiltonian Monte Carlo.
arXiv preprint arXiv:2105.00887, 2021.
N. Bou-Rabee and K. Schuh. Convergence of unadjusted Hamiltonian Monte Carlo for mean-field
models. arXiv preprint arXiv:2009.08735, 2020.
S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of
Independence. Oxford University Press, 2013.
P. Brodersen. Entropy estimators, 2020. https://github.com/paulbrodersen/
entropy_estimators.
Y. Chen, R. Dwivedi, M. J. Wainwright, and B. Yu. Fast mixing of Metropolized Hamiltonian Monte
Carlo: Benefits of multi-step gradients. Journal of Machine Learning Research, 21(92):1-72,
2020a.
Z. Chen, Y. Cao, Q. Gu, and T. Zhang. A generalized neural tangent kernel analysis for two-layer
neural networks. arXiv preprint arXiv:2002.04026, 2020b.
L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models
using optimal transport. In Advances in Neural Information Processing Systems 31, pp. 3040-3050,
2018.
N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. On kernel-target alignment. In Advances
in Neural Information Processing Systems 14, pp. 367-373, 2002.
A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 79(3):
651-676, 2017.
S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. International Conference on Learning Representations 7, 2019.
A.	Durmus and E. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin
algorithm. The Annals of Applied Probability, 27(3):1551-1587, 2017.
B.	Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers
neural network. In Advances in Neural Information Processing Systems 32, pp. 9111-9121, 2019.
R. Holley and D. Stroock. Logarithmic Sobolev inequalities and stochastic ising models. Journal of
statistical physics, 46(5-6):1159-1194, 1987.
K.	Hu, Z. Ren, D. Siska, and L. Szpruch. Mean-field Langevin dynamics and energy landscape of
neural networks. arXiv preprint arXiv:1905.07769, 2019.
J.-F.Jabir, D. Siska, and 匕Ukasz Szpruch. Mean-field neural ODEs via relaxed optimal control. arXiv
preprint arXiv:1912.05475, 2021.
10
Published as a conference paper at ICLR 2022
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems 31, pp. 8580-8589, 2018.
L.	F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problems
of Information Transmission, 23(2):95-101, 1987.
M.	Ledoux and M. Talagrand. Probability in Banach Spaces. Isoperimetry and Processes. Springer,
New York, 1991.
Y.-A. Ma, Y. Chen, C. Jin, N. Flammarion, and M. I. Jordan. Sampling can be faster than optimization.
Proceedings of the National Academy of Sciences, 116(42):20881-20885, 2019.
S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural
networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671, 2018.
G. Menz and A. Schlichting. POincare and logarithmic Sobolev inequalities by decomposition of the
energy landscape. The Annals of Probability, 42(5):1809-1884, 2014.
S.	Mischler. An introduction to evolution PDEs, Chapter 0: On the Gronwall lemma,
2019. URL https://www.ceremade.dauphine.fr/~mischler/Enseignements/
M2evol2018/chap0.pdf.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press,
2012.
T.	Murata and T. Suzuki. Doubly accelerated stochastic variance reduced dual averaging method for
regularized empirical risk minimization. In Advances in Neural Information Processing Systems
30, pp. 608-617, 2017.
Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):
127-152, 2005.
Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming,
120(1):221-259, 2009.
A. Nitanda and T. Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv preprint
arXiv:1712.05438, 2017.
A. Nitanda and T. Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent
kernel regime. arXiv preprint arXiv:2006.12297, 2020.
A. Nitanda, D. Wu, and T. Suzuki. Particle dual averaging: Optimization of mean field neural
networks with global convergence rate analysis. In Advances in Neural Information Processing
Systems 34, 2021.
E. Posner. Random coding strategies for minimum entropy. IEEE Transactions on Information
Theory, 21(4):388-391, 1975.
R. T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.
R. T. Rockafellar. Augmented Lagrangians and applications of the proximal point algorithm in
convex programming. Mathematics of Operations Research, 1:97-116, 1976.
R.	T. Rockafellar. Duality and stability in extremum problems involving convex functions. Pacific
Journal of Mathematics, 21(1):167-187, 1967.
G. M. Rotskoff and E. Vanden-Eijnden. Trainability and accuracy of neural networks: An interacting
particle system approach. arXiv preprint arXiv:1805.00915, 2018.
S.	Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. Journal of Machine Learning Research, 14:567-599, 2013a.
S.	Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In
Advances in Neural Information Processing Systems 26, 2013b.
11
Published as a conference paper at ICLR 2022
J. Sirignano and K. Spiliopoulos. Mean field analysis of neural networks: A central limit theorem.
Stochastic Processes and their Applications,130(3):1820-1852, 2020.
T.	Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
Optimal rate and curse of dimensionality. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=H1ebTsActm.
M. Takdc, A. Bijral, P. Richt^rik, and N. Srebro. Mini-batch primal and dual methods for SVMs. In
Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings
of Machine Learning Research, pp. 1022-1030. PMLR, 2013.
S. Vempala and A. Wibisono. Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry
suffices. In Advances in Neural Information Processing Systems 32, pp. 8094-8106, 2019.
L. Xiao. Dual averaging method for regularized stochastic learning and online optimization. In
Advances in Neural Information Processing Systems 22, pp. 2116-2124, 2009.
G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural
networks. In Advances in Neural Information Processing Systems 32, pp. 6598-6608, 2019.
D. Zou, Y. Cao, D. Zhou, and Q. Gu. Stochastic gradient descent optimizes over-parameterized deep
ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
12
Published as a conference paper at ICLR 2022
Table of Contents
1	Introduction	1
2	Preliminaries	3
3	Proposed method: Particle-SDCA	3
3.1	Dual problem and algorithm description .................................... 3
3.2	Our proposal: Particle update method ...................................... 5
4	Convergence analysis	6
5	Additional related works	8
6	Numerical experiments	8
7	Conclusion	9
A	Fenchel’s duality theorem (Proof of Proposition 1)	14
B	Computational complexity of sampling	algorithms (Proof of Proposition 2)	15
B.1	Log-Sobolev inequality of the target function	............................ 15
B.2	Sampling complexity of ULA ............................................... 16
B.3	Sampling complexity of MALA .............................................. 17
C	Proof of Theorem 1	19
C.1	Convergence of the ideal update g(t) (Algorithm	1)........................ 21
C.2	Convergence of the particle-version update g(t)	(Algorithm 2) ............ 25
C.2.1 Convergence of auxiliary variables g(t) and g(t).................... 29
C.2.2 Particle approximation, and error analysis of g(t) ................. 30
C.2.3 Integration of two sources of discretization error ................. 34
C.2.4 Discrete Gronwall’s lemma .......................................... 40
D Additional experiments and details of experimental settings	40
13
Published as a conference paper at ICLR 2022
——Appendix——
A	Fenchel’ s duality theorem (Proof of Proposition 1)
To prove Proposition 1, we utilize the infinite dimensional Fenchel’s duality theorem by Rockafellar
(1967). TWo real vector spaces E and E* with locally convex HaUsdorff topology are called
topologically paired vector spaces if all the elements of each space can be identified with all the
continUoUs linear fUnctionals on the other. Therefore, for a continUoUs linear fUnctional y : X → R,
there exists y ∈ E* such that We can write y(x) = hy, Xie*,e as a bilinear form(∙,)e* ,e.
Definition 1 (Fenchel-Legendre convex conjugate). Let X, X* be topologically paired vector spaces.
For f : X → R ∪ {∞}, its Fenchel-Legendre convex conjugate f* : X* → R ∪ {∞} is defined by
f *(y) = sup{hy,xiX*,X - f(x) | x ∈ X}.
Remark 3. When X = X * = Rd equipped with the usual inner product, the definition of the
Fenchel-Legendre convex conjugate coincides with the definition introduced in the main text.
Lemma 2 (Theorems 3 and 8 of Rockafellar (1967)). Let (X, X*) and (Y, Y *) be two pairs of
topologically paired vector spaces. Let f : X → R ∪ {∞} and g : Y → R ∪ {∞} are proper lower
semi-continuous convex functions, and let A : X → Y be a bounded linear operator. Assume that
∃x ∈ X such that f(x) is finite and g is finite and continuous at Ax. Then,
inf{g(Ax) + f(x) | x ∈ X} = sup{-g* (y) - f* (-A*y) | y ∈ Y*},
where A* is the adjoint operator of A such that hy, AxiY *,γ =〈A*y, x)χ*,χ. Moreover, X and y
are the optimal solutions of the primal and dual problems respectively if and only if AX ∈ ∂g*(y)
and A*y ∈ ∂f (X).
Now, we are ready to prove Proposition 1.
Proof of Proposition 1. We utilize Lemma 2 to show the assertion. We let Y = Rn. Then, we
know that Y is self-dual, that is, its topological dual Y * is Y itself where the linear functional
is given by the standard inner product hu, viY*,Y = Pin=1 uivi for u = (ui)in=1 ∈ Y and v =
(vi)in=1 ∈ Y *. We let X be the set of L1-integral functions with respect to the Lebesgue measure,
i.e., X = LI(Rd) = L1(Rd;μ), where μ is the Lebesgue measure on (Rd, B(Rd)). It is known
that its topological dual is the set of essentially bounded functions equipped with L∞-norm, i.e.,
X* = L∞(Rd)(= L∞(Rd; μ)) = {f : Rd → R | f is B(Rd)-measurable, kfk∞ < ∞}, where
kfk∞ = inf{M > 0 | μ({x | |f(x)| ≥ M}) = 0} for the Lebesgue measure μ. The bilinear
functional《,)χ*,χ is given by hg, f )χ*,χ = f fgdμ. Then, X, X* become topologically paired
vector spaces by equipping X* with its weak* topology with respect to the inner product(,, ∖χ* ,χ.
We let A : L1(Rd) → Rn be Af = ( hi(θ)f (θ)dθ)in=1 ∈ Rn for f ∈ L1(Rd). Then, we can see
that A is a bounded linear functional by the boundedness of hi(∙).
It is also known that the KL divergence is jointly lower semi-continuous with respect to the weak
convergence topology (Posner, 1975, Theorem 1). Since the L1-norm between probability density
functions is equivalent to the total variation norm between the corresponding probability distributions,
and the total variation norm gives a stronger topology than the weak topology, KL divergence is lower
semi-continuous with respect to the L1 -norm of the corresponding density functions. It is also well
known that KL divergence is jointly convex and it is easy to check that p ∈ P 7→ KL(p, N (0, σ2I))
is proper, i.e., ∃p ∈ P such that KL(p, N(0, σ2I)) < ∞.
n	√
Under these settings, if we set g(u) = ɪ Pi=ι 'i(ui) for U = (Ui)n=ι ∈ Y = Rd and
f ( ) = [λ2KL(p∣∣N (0,λ1∕λ2I)) (P ∈P),
p ∞	(otherwise),
K
for p ∈ L1 (Rd), then we can see that P (p) = g(Ap) + f(p) for p ∈ P and inf p∈P P(p) =
infh∈X {g(Ah) + f (h)}. We can also verify the condition in Lemma 2. Indeed, if we let p0 ∈ P be
the density function of N(0, λ1∕λ2I), then f(p0) = 0 < ∞ and g is continuous at Ap0 because a
14
Published as a conference paper at ICLR 2022
convex function is continuous when its effective domain is the whole space Rn (Rockafellar, 1976,
Theorem 10.1), and the domain of g is the whole space Rn (remember that the range of `i is R).
Therefore, by applying Lemma 2 and going through a simple calculation to obtain f *, We obtain the
desired assertion.	□
B Computational complexity of sampling algorithms (Proof of
Proposition 2)
B.1	Log-Sobolev inequality of the target function
In each outer iteration, we sample M particles from the density p[g(nT)] so that the particles are
sampled from an approximated density p(nT) with TV(p(nT)||p[g(nT)]) ≤ eST). To estimate the
convergence of Monte-Carlo methods such as ULA and MALA, the log-Sobolev inequality associated
with the target density plays the central role.
Definition 2 (Log-Sobolev inequality). Let p(θ) be a smooth probability density function on Rd.
了、、
Then, we say p(θ) (or its corresponding probability measure on (R, B(Rd))) satisfies the log-Sobolev
inequality (LSI) with a constant cLS > 0 if and only if, for any smooth function φ : Rp → R with
Ep [φ2] < ∞, it holds that
2
Ep[Φ2 log(Φ2)] - Ep[φ2]log(Ep[φ2]) ≤ 一Ep[∣∣Vφk2].
cLS
The following lemma shows that the target density p[g(nT)] satisfies the LSI.
Lemma 3. Suppose maxi |gi(nT) | ≤ B (this is true by Lemma 6 below with B = B2) and assume
(A2) ofAssumption 1, then p[g(nT)] satisfies the LSI with the constant
2λ1
CLS =
λ2
exp(-4B∕λ2).
Proof. Remember that p[g(nT)] is given as
p[g(nT )](θ)
(X exp (一士 n X hi(θ)g(nT)
" kθk2
—
Therefore, p[g(nT)](θ) can be written as
p[g(nT )](θ)
eχp(f (θ))q(θ)
Eq [exp(f (•))]
(8)
where f (θ) = 一£n P∖ι hi(θ)g(nT) and q(θ) is the probability density function of a Gaussian
measure N(0,爵I). Note that ∣∣f k∞ ≤ B∕λ2 by the assumption (A2) and maxi ∣g(nT)| ≤ B.
Then, it is well known that q satisfies the LSI with a LSI constant CLS (q)=若 and Holley & Stroock
(1987) showed that the bounded perturbation as in Eq. (8) preserves the LSI condition where the
LSI constant of p[g(nT)] is bounded by CLS = ds(q)/exp(4∣∣f∣∣∞) ≥ 岩 exp(-4B∕λz). This
concludes the proof.	□
By Lemma 3, we show that existing convergence rate analyses of ULA and MALA such as Vempala
& Wibisono (2019); Ma et al. (2019) can be applied to our target density p[g(nT)]. From now on,
we denote our target density p[g(nT)] by p*, and the negative log density of p[g(nT)] by U, that is,
p*(θ) = exp(-U(θ)). In addition, let Pk be the marginal distribution of θk (sample in the k-th
iteration of ULA or MALA).
15
Published as a conference paper at ICLR 2022
B.2	Sampling complexity of ULA
First, we apply the convergence rate analysis of ULA developed by Vempala & Wibisono (2019) to
our setting. We utilize the following proposition given by Vempala & Wibisono (2019).
Proposition 3 (Theorem 1 of VemPala & Wibisono (2019)). Assume that p*(θ)dθ satisfies the
log-Sobolev inequality with a constant cLS and U is L-smooth. Under these assumptions, for the step
size η satisfying η ≤ CL2, we have that
KL (Pk k p*) ≤ exp(-dsηk)KL (pi k p*) + 8ηdL .
cLS
Proposition 3 implies that if the step size satisfies η ≤ 4L2 min {1,卷} and the number of iterations
satisfies
k ≥ ,log 2KL(POk P*)
cLSη

then we obtain an -accurate distribution Pk in terms of the KL-divergence.
If we employ po(θ) Y exp(一λ1 kθk2) (a Gaussian measure), then 2KL(po k p*) is bounded by 4B2,
from Lemma 6:
KL(P0 k P*) = PPo(θ)log端dθ
P (θ)
= -	P0(θ) [log P*(θ) - logP0 (θ)] dθ
-	P0(θ)
≤ -	P0(θ)
≤ -	P0(θ)
≤也
exp
log
R exp
-Pn= hi(θ)gi-λ1∣∣θ∣∣2
λ2
-Pn=I hi(θ0)gi-λ1M∣∣2
λ2
- log P0(θ) dθ
dθ0
exp「殁就)exp (-λ<k2
.1 4exp (殁皿)R exp (一*)
log 卜XP (-2λb2
- logP0(θ) dθ
dθ
• po(θ) - - log Po (θ) dθ
λ2
Moreover, using Pinsker’s inequality bounds total variation norm by KL-divergence:
l∣Pk - p*kτv ≤ √2KL(Pk k p*).
Therefore, in order to have an C -accurate distribution in total variation, it suffices to let the step size
satisfy η ≤ 4L2 min
{1, ⅛}
and the number of iterations satisfy
2
k ≥ -----log
cLSη
4B2
λ2 C
Hence, the total computational complexity can be bounded by O
Total computational complexity of sampling by ULA According to Theorem 1, sampling ac-
curacy in total variation CCnT)should be smaller than O (exp (-ST)) in each sampling step T.
Therefore, the total computational complexity for sampling can be calculated as follows:
Algorithm 2 with Option A: Suppose we run outer-loop up to	Tend	≥
1	1 ∖ (B1+B2+ i-eC(-.) )∖
j log I In + γλ-) ---------_P-L- 1 times. Indeed it is sufficient to have an Cp -
approximated solution, according to Theorem 3, which gives a tighter and more detailed
—
16
Published as a conference paper at ICLR 2022
bound than in Theorem 1. Then, the total required computational complexity for sampling is
X O(	dL2	IC(T	B2
T=1	U(ɪr))2	g k
≤O
dL
CC2cLS
log
B2
~n ^^(nTend)
λ2C
Tend
X eχp(ST)
T=1
=O
〜 C
dL2 ι ,	B2
Ccr lθg \ ∕nTend)
C1 cLS	λ2 C
exp Mnd)
1 - e-
〜
=OS
Γ + γλ2j dL2、	B2(B1 + B2 + Co)
苍 lθg	λ2 + N
where OS hides log log terms.
Algorithm 2 with Option B: According to Theorem 1, it is sufficient to run outer-loop up to Tend ≥
1 log ((l + n⅛) C3) times to acquire an ep-approximated solution. YoU can see the
detailed definition of C3 = O(max{S-1, λ-3∕2S-1∕2}) in Eqs. (52a)-(52c) of Theorem
3 and in Proof of Theorem 1. Therefore, the total required computational complexity for
sampling is
Tend
XO
T=1
exp (sST )
Tend
X exp (sST)
T=1
≤O
log
dSL2
C22 CLS
B2
T^^〈Tend)
λ2eC
exp (STend)
1 - e-s
1/2] n3/2(1 +
ʃ	n3∕2ep
5/2
dSL2
log
B2(B1+B2+C0)
λ2 + nγ2
1	Kl ♦ F 1	1	,	11
where O hides log log terms as well.
B.3	Sampling complexity of MALA
Here, we adopt the convergence rate analysis of MALA obtained by Ma et al. (2019). They showed
the convergence under the setting where U is μ-strongly convex outside of a compact domain and is
L-smooth. Although our situation does not necessarily satisfy this condition, the proof of Ma et al.
(2019) essentially utilizes it to show the LSI of the target density. Fortunately, we have already seen
that the target density p* satisfies the LSI in Lemma 3. Thus, the analysis of Ma et al. (2019) can be
applied to our setting.
From now on, We denotep[g(nT)] by p*. Let R(s) be a positive real such that P*(∣∣θ∣∣ ≥ R(S)) ≤ S
for 0 < s < 1 where P * is the probability measure corresponding to p* and L> 0 be the smoothness
parameter of log(p*): kVlog(p*(θ)) - Vlog(p*(θ0))k ≤ L∣∣θ - θ0k. Then, Ma et al. (2019) showed
the following proposition.
Proposition 4 (Lemma 9 of Ma et al. (2019) with modification). Let the initial density p0 satisfy
Po(θ)
SUP *w
θ∈Rp p* (θ)
≤β
17
Published as a conference paper at ICLR 2022
for β > 0. Then, for the step size η > 0 satisfying
η=O
min
1	1	1
,,
L3/2R(e/2e)Ld L4/3尺(〃2e)2/3d1/3
and the number k of iterations satisfying
≥ Ω ( -5- log ( -β ʌ max{L3/2R(e/2e), Ld, L4/3R(e/2e)2/3》/3}
(9)
it holds that
∣∣p* - Pk IlTV ≤ e.
We apply this proposition to our setting. From now on, we assume Assumptions 1 and 2 hold. Then,
it can be easily seen that
LB2 + 2λ1
If We employ po(θ) 8 exp(-λ1 ∣∣θ∣∣2) (a GaUssian measure), then We may set β ≤ exp(2B2∕λ2)
because
p0(θ) _ (2λ1∕λ2 Y/2 ReXP (-λ21n Pn=I hi(θ0)ginτ)- λ1 llθ0k2) dθ0
E	2π~^r)	exp (-λ⅛ Pn=I hi(θ)g(nτy)
≤ (2λ2∏λ2 Y" exP(2B2∕λ2)∕exp (-λ2kθ0k2) dθ0
= exp(2B2∕λ2) (2λ2∏λ2厂 ∕exP (-λ1 kθ'k2) dθ0
= exp(2B2∕λ2).	(10)
Moreover, the proof of Lemma 16 of Ma et al. (2019) shoWed that We may set
R(s)
2 log
+
√Ep* [∣θk2].
∖
The second term in the right hand side can be evaluated as
Ep* [∣θk2] = / kθ∣2P*(θ)dθ = / kθ∣2Po(θ)虚dθ
≤ exp(2B2∕λ2)	∣θ∣2p0(θ)dθ
=exp(2B2∕λ2) 5^τ^ d,
2λ1
Where the inequality in the second line can be shoWn in the same manner as Eq. (10). Then, We obtain
R(s)
Therefore, substituting these evaluations into Eq. (9) yields that
log (2d!+要!1/3
18
Published as a conference paper at ICLR 2022
Algorithm 3 Definition of g(t)
1: p(nT) — p[g(nT)]
2： gin+1) - argmaχ -linτ (IinT )+ Z hinτ(θ)p"T )(θ)dθ(giftτ - g(nT) )-^^ 鼠 T - g(nT)|2
ginT	2
3:	for t = ngT + 1, . . . ,ng (T + 1) - 1 do
4:	p(t) J p[g(t)]
5：	git+1)	- argmax Tl (git)	+ /	hit (θ)p(t)(θ)dθ(git	-	g(t)) - 21^ 扇-git)l2
6: end for
is sufficient to ensure ∣∣p* - Pk∣∣tv ≤ e, where CLS = 2λ1 eχp(-4B2∕λ2) and L = LB2+2λl. We
can see that the right hand side can be bounded by
O ]w (B2 (1+L)+λ1 +log(1∕e)) /j
which gives the assertion of Proposition 2.
C Proof of Theorem 1
In this section, we give the convergence rate analysis of our algorithm. We first show the convergence
rate Algorithm 1 (the continuous limit version), then we prove Theorem 1 by bounding the discretiza-
tion error. Before we begin the proofs, we need some auxiliary variables that interpolate between the
continuous limit solution and the discretized one.
The key ingredient of our proof is to show the discrepancy between the solution with particle
discretization and the one with the continuous limit is sufficiently small throughout each inner loop
from t =ngT to t = ng(T + 1). To rigorously prove this intuition, we introduce gg(t) which follows
the continuous limit update but is initialized from the discretized one g(nT) at the every starting
point of the inner loop (see its definition in Algorithm 3). To fill in the gap between the continuous
limit g(t) and the discretized version g(t), we also define g(t) (t = nT,..., T(n + 1)) which lies in
between (see Algorithm 4). Basically, g(t) and g(t) are updated by the same rule, but are initialized in
different ways. Indeed, the primal variables corresponding to these two sequences are updated as
p[g(t+1)](θ) X p[g(t)](θ)×exp (-n1λ2hit (θ)δg(t)) andpit+1) X p(t)(θ)×exp (-±h*(θ)δg(t)),
respectively. However, different initial primal variables p(nT) = p[g(nT)] and p(nT) = p(nT) are
employed for the two schemes (remember thatp(nT) is an approximation of p[g(nT)] by the sampling
scheme). We denote difference of the updated coordinates before and after updates as
δg(t) =f g(t+1) -g(t), δg(t+1) =f g(t+1) -g(t), δg(t) = g(t+1) -g(t) (t = nτ,...,t(n + 1) - 1),
respectively.
At the same time, we define some variants of rjt), in order to fill the gap more finely between g(t)
and g(t). We define fjt) as a direct discretization of p(t) using the sampling point θj (see Algorithm
5). Let rjt) be a numerator of fjt) (see Algorithm 6). We expect that
plays a role of
interpolation between p(t) and
rjt)
PM	r(t)
In the convergence rate analysis, the smoothness of the dual objective plays an important role. The
following lemma shows the smoothness of the dual of the regularization term.
19
Published as a conference paper at ICLR 2022
Algorithm 4 Definition of g(t)
1: p(nτ) — p(nτ)
2： ginT +1) - argmaχ -linτ(ginτ )+ ∕h⅛r(θ)p(nT )(θ)dθ(giftτ - g(nT) )-2nλ^ |gi-T - giZ
ginτ	J	2
3： for t = nτ +1,...,n(T + 1) — 1 do
4:	p(t) (θ) -	p(nT )(θ)eχp (-/ P^ hiτ(θj )δg(τ))
Rp(nτ)(θ0)exP (-nλ2 PT=" hiτ(θ0)ET)) dθ0
5:
1) - argmaχ Tit (git)+/ hit (θ)pw(θ)dθ(git - g(t)) - 2n1λ^ |git- g(t)|2
6: end for
Algorithm 5 Definition of fj-t)
1： WnT) - M (j = 1,...,M)
2： if n > 1 then
3	:	for t = nT, nT +1,...,n(T + 1) — 2 do
4	-(t+1) —	M exp (-nλ2 PT=nτ hiτ(θj)δgi?)
4'	rj	J RPeT)(θ0)eχp (-/ PT=nτ hiτ(θ0)δg?) dθ0
5： end for
6： end if
Algorithm 6 Definition of rjt)
1： r；T) - M (j = 1,...,M)
2： if n > 1 then
3： for t = nτ, nτ + 1,...,n(τ +1) - 2 do
4：	g'+1) J EeXp (-n⅛ X hiτ (θj )温T)
∖	2 τ=nτ
5： end for
6： end if
Lemma 4. The first and second order derivatives of the second term of the dual objective D(g) can
be evaluated as
▽g λ2 log
q[g](θ)dθ
1 ∕hi(θ)p[g](θ)dθ,
(11)
▽g2 λ2 log	q[g](θ)dθ
n^[j hi(θ)hj (θ)P[g](θ)dθ - / hi(θ)p[g](θ)dθ / hj (θ)p[g](θ)dθ} .	(12)
20
Published as a conference paper at ICLR 2022
In particular, the dual of the regularization term can be bounded by a quadratic function as follows:
(a) L1 -norm bound,
λ2 log
q[g + δg](θ)dθ
and (b) L2-norm bound,
λ2 log
q[g + δg](θ)dθ
≤ λ2 log
≤ λ2 log
q[g](θ)dθ
q[g](θ)dθ
1n	1
-	n∑S∕ hi(θ)p[g](θ)dθδgi + 2n2^ ι∣δgkι,
i=1	(13)
1n	1
-	n∑S J hi(θ)p[g](θ)dθδgi + 2nλ^kδgk2.
i=1	(14)
The L1-norm bound is useful to derive the dual coordinate ascent method (Algorithms 1 and 2). On
the other hand, the L2-norm bound is also useful in the following convergence rate analysis.
Proof. First of all, it is easy to see that λ2 log	q[g](θ)dθ is two times differentiable by gi. Since a
direct calculation yields
d f ∖ IS	1	∕∖ / Q ( - n Pn=I hi(θ)(gi) - λ1kθk2 "n	∕1C
瓯 y q[g](θ)dθ = -nλ2 ∣ hi(θ)eχp (---------------------又---------------)dθ,	(15)
we immediately obtain Eq. (13):
d、I f I' r 1…/ -n R hi(θ)q[g](θ)dθ	1 1' , ‘…“八…
而λ2 log [/ q[g](θ)dθ] = R q[g](θ)dθ0	= -n J hi⑻p[g]⑻dθ.
By differentiating the right hand side by gj again and using Eq. (15), we get the desired equality (12)
for the Hessian.
Note that the right hand side of the equation (12) for the Hessian evaluation is given by the covariance
between hi and hj with respect to the probability distribution p[g](θ). Hence, each element is
bounded by n^ due to the boundedness assumption of hi (Assumption (A2)), which yields Eq.
(13). As for the L2-norm bound (14), it is easily obtained by the relation ∣∣δg∣1 ≤ √n∣∣δg∣2 .	□
C.1 Convergence of the ideal update g(t) (Algorithm 1)
First, We prove the convergence of the ideal update g(t). Convergence analysis in the continuous
limit is the basis for that of discretized updates; more concretely, we can use results and proofs for
continuous limit in this section as a reference to derive Lemma 8, which is one of the key lemmas for
Theorem 1 (convergent proof for discretized updates).
Lemma 5. Suppose that ` is 1∕γ-smooth for all i ∈ {1 ...,n} (Assumption (A1)). Consider the
update of g(t) given in Algorithm 1 (continuous limit update), then, for any iteration t and any
s ∈ [0, 1], it holds that
唐，[D(g(t+1)) - D(g(t))] ≥ n(p(p(t))- D(g(t)))- (n
where p(t) = p[g(t)],
2 G(t)
2λ2 ,
(16)
(17)
and Ulit) d=f 'i (Rp(t)(θ)hi(θ)dθ). Here, the expectation was taken with respect to the choice ofthe
coordinate it. Accordingly, we also have the same bound for the ideal update inAlgorithm 2 (g(nT)
and g() (t = nT + 1,..., n(T + 1))), considering the definition of g(t).
Proof. Since we will focus on the update of one step, it is omitted as i in the following. At each
update, the improvement in the dual objective can be written as
n[D(g(t+n)) - D(g(t))]
21
Published as a conference paper at ICLR 2022
=-'Ug(t+v)) + '*(g(t)) - λ2 log Z∣ q[g(M](θ)dθ + λ2 log / q[g(t)](θ)dθ
≥ -4(竹1)) + / hi(θ)p㈤(θ)dθ (g(t+1) - g(t))-九 |gi - g(t) |2 + 续严)
≥ - '；(严 + S(UT)-严))+ S / h(θ)p⑴(θ)dθ (Uft- g(t))-孩Mt)-严 |2 + G(g(t)),
(18)
where we used Eq. (13) in Lemma 4 for the first inequality, and the second inequality follows from
the definition of g(t+1t.
Since ' is Y-Strongly convex (Y = 0 is also allowed), it holds that
0(g(t) + S(Uft- g(tt)) ≤ s'*(u(t)) + (1 - s)'*(g(t)) - 2 S(I-S)IUit)-砂 |2.
From the property of the Legendre transform, u(t) = 'i (R p(tt(θ)hi(θ)dθ) implies that
'i (/P⑴(θ)hi(θ)dθ) + G(U(t)) = /hi(θ)p(t)(θ)dθu(tt.
Similarly, we also have the following relation from the duality of the regularization term:
(19)
(20)
λι ∕p(t)(θ川θ∣∣2dθ + λ2 ∕p(t)(θ) logp(t)(θ)dθ + λ2 log
=-n XX ∕hi(θ)p(t)(θ)dθg(tt.
i=1
Combining Eqs. (18), (19) and (20), we have that
n[D(g(t+1t) - D(g(t^]
/ q[∕t](θ)dθ
≥ -S ʤ(t)) - / hi⑹p(t)⑻dθu(t)) + S(Y(I-S)-4)|u(t) - g(t)|2
+ S g(g(tt)- g(t) / hi(θ)p(tt(θ)dθ)
'i (/p(tt(θ)hi(θ)dθ) + '*(g(t)) - / hi(θ)p(tt(θ)dθ严]
By taking expectation of both sides of (22) with respect to i, it follows that
E [n[D(g(t+1t) - D(gw)]∖
≥S
1XX `i (∕*(θ)hi(θ)dθ) +1XX 'i(gi%-1XX ∕hi(θ)* (θ)dθg(t)
i=1	i=1	i=1
1XX `i U p(t)(θ)hi(θ)dθ)+nι XX 'i(git't)
i=1	i=1
(21)
(22)
S
S
+ λι ∕p(tt(θ)∣∣θ∣∣2dθ + λ ∕p(t)(θ)logp(tt(θ)dθ +λ2 log (∕q[g(t)](θ)dθ
S
+ 2n
22
Published as a conference paper at ICLR 2022
s2 G(t)
=S(P(P⑴)-D(g㈤))---,	(23)
n 2λ2
where we used Eq. (21) for the first equality. Dividing the both sides of Eq. (23) by n, we obtain the
desired inequality (16).	□
Lemma 6. g(t) in Algorithm 1 is uniformly bounded by B2, that is,
IAt)I ≤ B2 (∀t = 0,1,...,nTend, Vi ∈ {1,...,n}).
In the same way, g(t), g(t) and g(t) in Algorithm 2 satisfy
∣g(t)∣, I严I, ∣g(t)∣≤ B2 (Vt = 0,1,...,nTend,Vi ∈{1,...,n}).
Proof. Let p be an arbitrary probability distribution. Let us consider the update
z = argmax
z0∈R
-G(Z0) + (z0-y) / hi(θ)p(θ)dθ
(24)
for an arbitrary i ∈ {1, . . . , n}. Since hiis bounded by 1 (Assumption (A2)), the sub-differential of
' at z satisfies that
∂'*(z) ∩ [-1,1] = 0.
By the assumption (A3), i.e.,
B2 = SUp {I'i0(x)I I ∣x∣ ≤ 1} = SUp {∣y∣ I ∂'*(y) ∩ [-1,1] = 0} < ∞,
i,x	i,y
we have that z in Eq. (24) is bounded by B2 .
Next, we consider its proximal-version update:
Z = argmax f-'*(z0) + / hi(θ)p(θ)dθ(z0 - y) - -ɪ-Iz0- y『].	(25)
z0∈R	2nλ2
In this proximal-version update, We added the term 一2^^(z 一 y)2 whose maximizer with respect
to z is given by z = y . Since the maximum point of the sum of two concave functions is located
between the maximizers of them, z in Eq. (25) satisfies that
IzI ≤ max{B2, IyI} .
From this and the initialization Ie(O) I ≤ B2,we have that Igf) I in Algorithm 1 is uniformly bounded
by B2 by induction.
In the same way, g(t), g(t) and Uf) in Algorithm 2 minimize the sum of the average of hi(θj) and
quadratic term as well. Note that the absolute value of the the average of hi(θj) is bounded with 1,
regardless of methods of averaging, that is, weighted particle for gi(t) and probability distribution for
](t) and g(t). Thus, g(t), g(t) and g(t) are uniformly bounded with B2, as well as g(t)i in Algorithm
1.
□
Remark 4. In the Section C.2, we sometimes face the situation where we should upper-bound
increment of the coordinates δg(t), δg(t) and δg(t).
Lemma 6 directly implies that δg(t), δg(t) and δg(t) are all bounded by 2B2 for all t
0,1,..., nTend. Thus, Jorfuture convenience, we define
C1
def
exp
(啜)
with which exp ("鲁),exp (沼鲁),exp (2κ⅛p)
≤ C1 hold, where κ is defined as
Note that, however, we could have much tighter evaluations of δg(t), δg(t) and δg(t), which are
dependent ofresidual D(g*) — D(g(t)) or duality gap P [p(g(t))] — D(g(t)), since the dual problem
is strongly convex due to the smoothness of primal problem.
23
Published as a conference paper at ICLR 2022
Lemma 7. Under Assumptions (A2) and (A3), it holds that
P(p[严])-D(严)≤ Bi + B2,
for Algorithm 1 and
P (p[g(0)]) - D(g(0)) ≤B1+B2,
in Algorithm 2.
Proof. By Assumptions (A2) and (A3), We can see that R (Rp(0)(θ)h,(θ)dθ) - infχ 'i(x)∣ is
bounded by Bi. Since infχ 'i(x) > - inf (Assumption (A3)), We also have ' (0) = supχ{0 ∙ X -
'i(x)} = - infX 'i(x) < ∞. In Algorithm 1, every initial coordinate g(0) is bounded by B?. Also,
gi(0) in Algorithm 2 is similarly bounded by B2 . Therefore, in the folloWing, g Will be used Without
distinguishing between g and g. Moreover, in a similar manner to Eq. (21) in Lemma 5, we have that
λi/p[g(t)](θ川θ∣∣2dθ + λ? ∕p[g(t)](θ) logp[g(t)](θ)dθ + λ? log
1n
=--E	hi(θ)p[g(t)](θ)dθg(t),
n i=i
q[g(t)](θ)dθ
from the duality.
Combining these observations, we have that
P (p[g(0)]) - D(g(0))
n X'i (Z*(θ)hi(θ)dθ) + - X丹⑵⑼)+ λι /p(°)(θ川θ∣∣2dθ
i=i	i=i
+λ2	p(0)(θ) log p(0)(θ)dθ + λ2 log
q[g(0)](θ)dθ
-X 'i (Zp(0) (θ) hi (θ)dθ
i=i
')+ - XG"(O))- - X Z hi(θ)p(O)(θ)dθ
i=i	i=i
(0)
• gi
i=i
-n
≤- X 'i
i=i
/P(O)(θ)hi(θ)dθ) - inf 'i(x)
n
+ - X∣gi(0)∣
i=i
n
-n
≤-£Bl + B2 = Bi + B2,
n i=i
where we used Lemma 6 in the last inequality.
□
Theorem 2. Let P (> 0) be a desired accuracy. To obtain an expected duality gap
E [P(/(tend)) - D(g(tend))] ≤ P in Algorithm 1, it suffices to have the total number Tend of
iterations as
tend ≥
n +4Vogf (n +ɪ )3
γλ2	γλ2	P
Furthermore, ifwe randomly choose (P(D, g(") from t = tend — n +1,..., tend in Algorithm 1, then
tend ≥ (n + γχ2j log^1 +
-
nγλ2
B + B
ep
+n
is sufficient to achieve eP duality gap in expectation.
Proof. In Lemma 4, choosing S =耳：：.∈ [0, 1], then we have that
--H ≤ - -- = 0
s
24
Published as a conference paper at ICLR 2022
Hence, we have G(t) ≤ 0 for all t. Thus, Lemma 5 implies that
E
[D(g(t+1)) — D(g⑴)]≥ SE [P(P⑴)一D(g㈤)].
(26)
Therefore, we have that
IE [D(g*) — D(严)]
≤ SE [P(*)- D(g㈤)]≤ SE
nn
[D(g(t+1)) — D(g㈤)]
E [D(g*) — D(g(t))] — E [D(g*) — D(g(t+1))].
This implies that
E ∣D(g*) — D(g(t+1^] ≤(1 — S)E ∣D(g*) — D(/)].	(27)
By applying this inequality repeatedly, we obtain that
E [D(g*) —。传㈤)]≤(1 — S)' E [D(g*) — D(严)]≤ (Bi + B2)exp (—St) ,	(28)
where the last inequality is obtained by E [D(g*) 一 D(g(0))] ≤ E [P(p(0)) - D(g(0))] and Lemma
7.
Moreover, making use of Eq. (26), the duality gap can be bounded as follows:
E [p(*) — D(g(t))] ≤ nE [D(∕+1)) — D(g(t))] ≤ nE ∣D(g*) — D(严)].
Hence, combining Eqs. (28) and (29) yields
(29)
E [P(p(t)) — D(g(t))] ≤ n(B； B2) exp (— 1；O .
Therefore, in order to have an P -approximated solution in expectation, it suffices to let
Finally, we show the second part. By summing up Eq. (26) over t = tend — n + 1, . . . , tend, we have
that
tend
E n X	(P(P(t))-D(g(t)))
t=tend -n+1
≤1E
S
[D(g(tend + 1)) — D(g(end-n)
)
≤SE [D(g*) — D(g(tend-n))]
≤ Bi + B2
―1 + n⅛
exp
γλ2(tend - n)
1 + nγλ2
—
Note that although we do not actually calculate g(tend+1), we can virtually define g(tend+1) as if the
algorithm continued.
Therefore, in order to have an P -approximated solution, it suffices to let
tend ≥
n+
log
1+
Bi + B2
-------+ n,
P
when we randomly choose (沪t, g* from t = tend - n +1,..., tend.
□
C.2 CONVERGENCE OF THE PARTICLE-VERSION UPDATE g(t) (ALGORITHM 2)
Next, we consider the convergence in Algorithm 2.
In order to prove Theorem 1 and give a convergence proof for Algorithm 2, we adopt the following
strategy. First we prove Lemma 8, by rewriting Theorem 2 to allow the discretization error in each
25
Published as a conference paper at ICLR 2022
series of inner loops. Then, the assumptions of Theorem 2 is verified, by Theorem 3. There are two
major streams leading up to Theorem 3. The first is about the error originated from the sampling
accuracy (Langevin algorithms), and the second is to evaluate the error accumulated during the
re-weighting stage. The former is considered in Section C.2.1 and the latter is in Section C.2.2, both
of which are combined in Section C.2.3 and finally Theorem 3 is proved.
Apart from the sources of the error, we bound the discrepancy of the duality gap between continuous
limit and discretized updates (P(g(t)) - D(g(t))) - (P(g(t)) - D(g(t))) with the following two
parts,
eA，d=f E ∣D(g⑴)-D(g㈤)卜
eB) d=f E jp(p[g(t)])- P(p[严])],
each of which corresponds to the discrepancy in the primal problem and dual problem, respectively.
As mentioned above, these two terms are attributed to the two sources, that is, sampling and re-
weighting. These variables eA), eB) represent how much the two sequences g(t) and g(t) diverge
during the updates of the inner-loop. Intuitively, in each inner-loop, the absolute values of e(At) and e(Bt)
are increasing sequences between t = nT + 1 and n(T + 1). Then, they are “reset” at t = n(T + 1)
according to the initialization rule of g(n(T +1)+1) and again will increase from t = n(T +1) + 1 to
n(τ + 2).
The following lemma states that under the assumption about e(At), e(Bt) , Algorithm 2 achieves linear
convergence with regard to the number of coordinate update. From now on, we fix S = 甘：；..
Also, we define s = K ∙ S = ηγλ2 , where K = n.
,	2	1+nγλ2 ,	n
Lemma 8. Suppose that Assumptions 1 and 2 hold. If e，T) is SmaUer than Coe-sT for ev-
ery T = 1, . . . , Tend with a constant C0 you can arbitrarily take, an expected duality gap
E P (p[g(nTend)]) - D(g(nTend)) ≤ eP can be achieved by a total number Tend of outer itera-
tions given by
Tend
1+
log
(BI + B2 + 1-eC0(-gJ
eP
(30)
≥
2 n
n
Furthermore, if e[) and eB are uniformly bounded by 詈 for all t = n(Tend — d1 ] ) + 1,..., nTend
in addition to the assumptions above, then it also suffices to set
Tend ≥2"1 + n⅛)log((2+
2
nγ λ2
(Bi + B2 +
1—exp(-s)2! + l 1 m
(31)
to achieve ep-duality gap when we randomly choose output (p(t), g(t)) from t = n(T^d — dɪ]) +
1,...,n‰d.
Remark 5. Comparing the speed ofconvergence with regard to coordinate update t, it is exp(-st/n)
for Theorem 2 and exp(-sT) H exp(-2t/n) for Lemma 8, which can be interpreted asfollows. In
the descretized version, Lemma 8 allows discretization error to appear, at the cost of a slight decrease
in convergence speed. Indeed, you can choose arbitrary ss from the open interval (0, Ks). Our choice
of s = KS is justfor simplicity. Ifyou choose another s much closer to Ks, Tend can be smaller
Remark 6. If you change the length of inner loop in the middle, the convergence rate after the
change is obtained by simply replacing B1 + B2 with the duality gap at the time of the change.
Proof. Since the update of g(t)is the same as that of g(t) in Algorithm 1 during the inner loop, it
holds that
E ∣D(g*) - D(g(T+1))] ≤(1 - S) E ∣D(g*) - D(g(T))],
for T = 0, 1, . . . , Tend - 1. We also have
E
[D(g*) - D(g(t+1))] ≤(1-
S)E [D(g*)- D(S(t))]
26
Published as a conference paper at ICLR 2022
for t = nT +1,..., n(T + 1) - 1, in the same way as the derivation of Eq. (27).
By applying these inequalities above iteratively, we have that
E ∣D(g*) - D(g(n(τ +1)))i ≤(1 - S)nE ∣D(g*) - D(g(nT))i
≤ e-2sE ∣D(g*)- D(g(nT))i .
Bounding D(g(n(τ+I))) by D(g(n(τ+I))) — CAn(T +1)), We have that
E ∣D(g*) - D(g(n(T +1)))] ≤ e-2∙⅛ ∣D(g*) - D(g(nT))] + ef(T +1)).
Therefore, by the Gronwall’s lemma (“vanilla version” of Lemma 18), we obtain that
T
E ∣D(g*) - D(g(nT))] ≤ e-2∙5TE ∣D(g*) - D(g(%] + 产T X e2ST0e^T0).
T0=1
Applying the assumption e，T) ≤ COe-ST, this implies that
-ST	—2ST
E [D(g*) - D(g(nT))] ≤ e-sTE [D(g*) - D(g(0))] + Co	1 - e-s
≤e-ST (E [P(g(°)) - D(g(°))] + τ—C0-)
≤e-ST (BI + B2 + T⅛)，
where we used Lemma 7 for the last inequality.
Moreover, the same reasoning as Eq. (29) yields that
E hP(p[g(nT)]) - D(g(nT))] ≤ SE ∣D(g*) - D(g(nT))].
Therefore, combining Eqs. (33) and (34), we can bound the duality gap as
E hP(p[g(nT)]) - D(g(T))i ≤ e-STT(Bi + B2 + T-C-).
(32)
(33)
(34)
This inequality provides the sufficient number of iterations to attain the desired duality gap. In order
to have an eP -approximated solution, it is sufficient to let
Tend ≥ 2— ( 1 +
n
log n +
BI + B2 + 1—eC( —S)
ep
Next, we show the convergence of the averaging version. Similarly to Theorem 2, we have that for
k = 1,..., d κ1 ],
^ 1 n(Tend-k+1)	]	1
E n X	(P(p[g(t)])- D(g⑴))≤SE ∣D(g*)- D(/(Tend-k)+1))]
t=nS (Tend —k)+1
≤ 1 e-S(Tend-d1∕κe)(Bi + B2 + T^-S).
Note that, if Eq. (31) holds, we have that Se-S(Tend-d1∕κe) (bj + B2 + j-∣⅛) ≤ 矍.Hence, if
eA) and eB) are uniformly bounded by 皆 for all t = n (么nd - ∖1∕κ]) + 1,..., nTend, it holds that
E
1	nTend
ΓiTK≡	E
t=n(Tend-d1∕κe)+1
P (p[g(t)]) - D(g(t))
27
Published as a conference paper at ICLR 2022
d1/Ke
≤X
k=1
di∕κeE
I	n(Tend-k+1)
n X	(P(p[g(t)])-D(严))
t=n(Tend-k)+1
1	nTend	ι
+ ∣^ι∕κ∣n	E	eA)+ ∣^ι∕κ∣n
1 t 1 t=n(Tend-d1∕κ])+1	1 / '
nTend
X
t=n(Tend-d 1/Ke ) + 1
P P P
≤ T + T + T= ep.
This concludes the proof.
□
In the following, we derive the sufficient conditions for the assumptions in Lemma 8 by considering
the error induced by the sampling stage (Section C.2.1) and the error induced at the re-weighting
stage (Section C.2.2), respectively. For that purpose, we first prepare lemmas that will be used in the
both proofs.
The following Lemma 9 evaluates how much the difference of two solutions in the previous step incurs
difference of them in the next update in Algorithm 2. This will be used to bound the accumulated
error through each inner loop.
Lemma 9. For arbitrary i ∈ [n], consider the following two updates:
Z1 = argmax I-G(ZI) + H (，1 - yι) - χ^τ~ |，1 - yι∣2 ∖ ,
z10 ∈R	2nλ2
Z2 = argmax - -C(Z2) + H(，2 -期2)- ʒ-ʃ W - 92∣2 .
z20 ∈R	2nλ2
Then, we have that
|z2 - zi| ≤ ―1—r (|H2 - Hi| + -ʃ |y2 - yι∣).
Y + n⅛	nλ2
Proof. From the definition of Z1 , it holds that
-CO(ZI) + HI--------L(ZI - yI)= 0,	(35)
nλ2
and similarly,
-C0(z2) + H2 ——^(Z2 - y2) = 0,
nλ2
where '丁(Z) is an element in the sub-differential ∂' (z). From (35), We have that
一'7(ZI) + H2---------I(ZI - y2) = H2 - HI +-----(y2- - yi).
nλ2	nλ2
Since ' is γ-strongly convex, -'*(z) + H?(z - y2) - 2nλ.∣z -y? / is (Y + _1_)-strongly concave.
Thus, we obtain that
∣Z2 - Z1 ∣
1
+ nλ
V(-G(Z) + H2(Z -y2) - 2⅛∣z-y2∣2
z=z2
-V(-G(Z) + H2(Z - y2) - 2⅛ ∣z - y2∣2
≤ -.~~— IH2 - HI +—τ-(y2 - yι)
Y + nλ i	nλ2
≤―l—— (∣H2 - Hι∣ +—τ~∣y2 - yι∣),
Y + nλ2	nλ2
where Vf (z)|z=z，is an element of the sub-differential of f at Z = Z. This concludes the proof. □
28
Published as a conference paper at ICLR 2022
Now we are ready to get into the main proofs which evaluate the discretization error.
C.2.1 Convergence of auxiliary variables g(t)and g(t)
^^We now fix some T and consider the senuances {〃(t)In(T+1) 1万(t)]n(T +1)	[3(t)]n(T +1)
YVe now uʌ soιme ɪ , aiɪu consιueι IUe sequances {g ʃ t=nτ , {g ʃ t=nτ +1， 1g ʃ t=nT ∣ 1.
Lemma 10. Suppose that R |p(nT )(θ) — p[g(nT )](θ)∣ dθ ≤ e^τ)holds in Algorithm 2. Under these
assumptions, it holds that
|严—严∣≤ Ge严),	(36)
for t = nT +1,...,n(T + 1), where C is defined asfollows:
C def	2C2	n(2C1 + 1八
Q = F exp(	)
Proof. By Lemma 9, we have that
成+1)-觉+1)l
1
≤ vrɪ
Y + nλ2
(/ hit (θ)∕(θ)dθ — / hi (θ)p[严](θ)dθ + n1- ∣g?一婷
≤γ+⅛ (J ∣*(θ)dθ — p[∕)](θ)∣dθ + nλ-成)—力 |
(37)
so we evaluate each terms of Eq. (37).
First, we bound the first term of the right hand side of Eq. (37). Using Lemma 6, we have the
following bounds:
1
Cr ≤ exp
(-n⅛ XT hi<θ)温?)≤ g,
1
Cr ≤exp
t-1
X hiτ(θ)限？
T=nτ
≤ C1.
(38)
Using (38), we have that
P(nT)(θ)eχp(- nλ~ X hiτ(θ)δg?) - P[g(nT)](θ)eχp(—nɪ X hiτ(θ)E?
∖	2 τ=nτ	)	∖	2 τ=nτ
≤C1 / ∣p(nT)(θ) — p[g(nτ)](θ)∣dθ
+ / p[g(nT)](θ)
eχp (-nλλ2 X 3⑻慰)-eχp (一左 X huθ)δgr)∣dθ
nτ)	1 X (T +1)	~(τ +1)∣
≤C1	e。+ 的 T l%	— % l
∖	A τ=nτ
dθ
(39)
Thus, combining Eqs. (38) and (39) yields we have the following bound for the first term of Eq. (37):
(θ) — p[g ⑴](θ)dθ
/
/
≤
p(nT)(θ)eχp(-⅛∙ PT=It hiτ (θ)δg(T))
RPg)(夕)exp(-忐 PT=nτ 七, (')M(7)d夕
p[9(nT)](θ) exp(-n⅛2 PT-IiT L (θ)δg(T))
Rp[g(nτ)](θ0)eχp(-忐 P二T MT (,)δg(T))dθ0
PsT)(θ) eχp (-忐 P二T hiτ ⑻MT))
JPgYθ0)exp(-忐 PT=nτ hiτ(θ0)Mg))dθ0
PsT)⑻ eχp(-忐 PT-IT hiT ⑻M(T))
R P[g(nT )](θ')eχp(-忐 PT=nτ hiT(θ0)δg(T))dθ'
dθ
dθ
—
—
+ / PST)(θ)exp(-为 PT)T hiT (θ)δgiT))-PdnT)](θ)exp(-忐 PT-nτ MT (θ)δg(T)) 4θ
+ J	RP[g(nT)](θ0)exp(-d⅛ PT=T hiT(θ0)MT)dθ0
≤ |/P[g(nT )](θ0)exp(-4 PT-nτ h% (1)近(T)) dθ-R P(nT )(θ)exp(-4 P」T …甘)δg(T))dθ[
≤	RP[g(nT)](θ0)exp(-为 P二T 九%(/)用(T))dθ0
29
Published as a conference paper at ICLR 2022
+ ∕*T )(θ)exp(-nλ2 PT-nτ hiτ (θ)δg(T))-P[g(nT )](θ)exp(-nλ2 P=T %” (θ)δg(T))Idθ
R p[g(nτ )](θ0) eχp (- nλ2 ET=nt %≈t (θ0)δg(T)) dθ0
≤2Cι p(nT)(θ)exp (-nλ- X hj(θ)δg(T)) — p[g(nTl(θ)eXp (— W X 凡，(。厮(T)
∖ n 2 τ=nτ	)	n n 2 τ=nτ
ACT) + n⅛ XIg(T+1) - g(T+I)I
,	2 τ=nτ
(40)
where we used the boundedness of each term following Eq. (38) for the second from last inequality
and (39) for the last.
Finally, we bound |g(：+1) — ](：+I)I by applying (40) to (37):
2C12 + 1
nλ2
I (nτ +1) _ -(nτ +1)∣
|gi 五 T	gi 五 T |
+ n⅛ XIgL)-g(τ+1)I)+ n⅛g(τ+1)-g：+1)i
T=nτ	/
t-1
X Ig(τ+1) -K+1)I
τ=nτ
(41)
/ hinτ(θ)p(T)(θ)dθ - / hinτ(θ)p∖g(T)](θ)dθ
≤
γ
「(nT)
Ip(nT)(θ)- p[g(nT)](θ)Idθ ≤ -eɪ
Y + nλ2
(42)
Combining Eqs. (41) and (42), we can apply Gronwall’s lemma (“summation version” of Lemma
18): we have that, for t = ngT, ngT + 1, . . . , ng(T + 1) - 1,
—
Ig(t+I)
1
f" γ + n⅛
≤ ( 2C2 +	2C2 + 1	ʌ ( + 2C2 + 1 V-2 (nT)
―[ F (γ+ 志)2 nλ2 八	F)	C
≤ 2C2	(I + 2C2 + 1 YTe5T)
- Y + n⅛	nYλ2 + V C
≤ 2C2 exp (n(2C2 + 1) HS?)
^y + n⅛	( nYλ2 + 1 ) C ,
where we used C1 ≥ 1	for the second inequality. Remember	that we	defined C2	as	C2 =
2C1 exp (n(2C1+1)].	By the definition of g(t) and g(t, g(t)	= g(t)	holds for t	=	nT +
γ+ nλ2	nγλ2 + i	, i	i
1, . . . , ng(T + 1) unless the coordinate i is chosen during the inner loop. Otherwise, we have
Igf) - g(t)| ≤ C2eCT) for all t ≥ τ + 1 where τ is the first time at which the coordinate i is chosen
as we have seen above. Thus, Eq. (36) holds for all t = ngT + 1, . . . , ng(T + 1) and i = 1, . . . , n.
□
C.2.2 PARTICLE APPROXIMATION, AND ERROR ANALYSIS OF g(t)
From now on, we bound the error induced by particle sampling in the re-weighting stage. The
following three Lemmas relate, in three steps, the update of the continuous limit to the approximation
by particle weighting.
30
Published as a conference paper at ICLR 2022
Lemma 11. With probability 1 - δ, it holds that
X hit (θj Mt)-/hit (θ)/ (θ)dθ ≤ 7C2 jMUog ( 2n )
uniformly over all t = nT,..., n(T + 1) 一 1 and the choice ofthe Coordinates (iτ)n(Tnr1)-1.
Proof. Fort = nT + 1,...,n(T + 1) - 1,let
H := {fι,a(∙) = hit(∙)exp
左 X hiτ(∙)ajc-,a
2 τ=nτ	)
I it ∈ H I = (inτ,…，it-1) ∈ [n]t-nT, WI ≤ 2B? (T ∈ {nT, ...,t- 1})},
where Cι,a := R p(nT) (θ0)exp (-± PT=nT hi, WJaj dθ0 for I = (inr,..., "-ι) and a
(anτ,..., at-ι). We also define
Hi ：= {fι,a(∙) = hi(∙)exp
nλ2 X hiτ(∙)aJ C-,a
τ=nτ	/
I I =(inτ ,...,it-1) ∈ [n]t-nT, ∣ατ∣ ≤ 2B2 (τ ∈ {nT, ...,t - 1})}.
Then, we can see that H = ∪i∈[n]Hi. Then, PM=I hit (θj)jt can be rewritten as
MM
X hit (θjMt)= M X fI,a(θj).
j=1
In a similar way, we also have that
j=1
/ hit (θ)p⑴(θ)dθ = Eθ [fι,a(θ)].
To check this, first note that (Mhit (θj¥?[M=I are i.i.d. random variables for each t = nT +
1,..., n(T + 1) - 1. Indeed, fjt) is determined by g(t) which is updated by using a continuous limit
and thus is independent of the choice of particles (θj)jM=1. Their expectations can be evaluated as
Eθj hMhit(θjMt)i = /hit(θ)	p(nT)(θ)exp(TPT=InThiτ(θj)可	dθ
l	j j t	RP(nτ)(θ0)exp(-n⅛PT=nτhiτ(θ0)δ处))dθ0
=/ hit (θ)p㈤(θ)dθ.
Moreover, they are uniformly bounded as
Mhit(θjMt)I ≤ |hit(θj)|
exp (-nλ2 PT=nτ hi, (θj)δg(τ))
RP(nT)(θ0) exP (-nλ2 PlnT 兀, (θ0)δg?)
≤ C12 .	(43)
dθ0
The same results are also true for t = nT.
Note that it suffices to bound Z = supf ∈H IMM PM=I fɪ,a(θj) - Eθ [fι,a(θ)]∣ to obtain the asser-
tion. For that purpose, we define the Rademacher complexity of a function class H0 as
M
R(H0) := E sup
f∈H0
1
M∑εjf(θj) ，
j=1
where (εj)jM=1 is an i.i.d. sequence of the Rademacher random variables (P (εj = 1) = P(εj =
-1) = 1/2) and the expectation is taken with respect to both (εj)jM=1 and (θj)jM=1. Then, the
31
Published as a conference paper at ICLR 2022
Rademacher concentration inequality (Mohri et al., 2012, Theorem 3.1) tells that the uniform bound
on a function class H0 can be obtained as
M
P SUp
f0∈H0
MM ∑(f (θj) - Eθ[f (θ)]) ≥ 2R(H0) + SUp kfk
j=1
f∈H0
∞ j2⅛l0g (2)) ≤δ, (44)
for 0 < δ < 1. Here, let Zi = SUpf∈n. |吉 PM=I f(θj) - Eθ[f (θ)]∣ and then we also see that
Z = maxi∈[n] Zi. Hence, We have that
n
P(Z≥s) ≤ XP(Zit ≥s),
it=1
for s > 0. If We substitute S J maxi∈[n] 2R(Hi) + SUpf∈n. kf k∞ J贵 log (2n), then by Eq.
(44) we can see that the right hand side can be bounded by δ. Note that we have already seen
that kfk∞ ≤ C12 for any f ∈ H (see Eq. (43)). Therefore, We just need to bound R(Hit ). By
the contraction property of the Rademacher complexity ((Boucheron et al., 2013, Theorem 11.6)
or (Ledoux & Talagrand, 1991, Theorem 4.12) and its proof) and remembering the fact that the
Rademacher complexity of a function class is same as that of its convex hull (Mohri et al., 2012,
Theorem 6.2), We obtain
R(Hit)
≤E
≤
≤
≤
1 M	1	t-1
SUp M∑Sεj exp I-------λ-	£	hiτ(θj )aτ	- log CI,a	('∙'	lhit	(.)〔	≤	1)
I,a M j=ι	∖ n 2 τ=nτ
Cl E
λ2
1 M	1 t-1
SUp M∑Sεj - ^ ΣS hiτ (θj )aτ 一 λ2 log CI,a
I,a M j=ι ∖ n τ=nτ
j E
λ2
1 M	λ2
i∈[n],s∈{±maXξ∣≤log(C1) M j= ε Sj ) + 2B2ξ
4CιB2 /2	0、
F VMlog(2n)，
where we used the contraction properties in the first and second inequalities, we used the convex hull
argument in the third inequality, and the last inequality is by Massart’s Lemma (Mohri et al., 2012,
Theorem 3.3). Summarizing these evaluations, we obtain that
P Z ≥ 8CλB2
r 力")+C2 j2⅛1og( 2n)! ≤j.
The same bound also holds for t = nT.
Then, by taking the uniform bound over t = nT,..., n(T + 1) — 1 and simpifying the bound using
n ≤ n, we yield that
X hit (θj)r(t) - / hit (θ)*(θ)dθ ≤ 7C2 j.log (2n)
uniformly over t = nT,..., n(T + 1) 一 1 and the choice of the sequence (i/)nT+1)-1 with
probability 1 一 δ, which yields the assertion.	□
Lemma 12. With probability 1 - δ, it holds that
M	(t)	M
X hi(θj)Prj-(t) - X hi(θj)rjt)
j = 1	Ej0 = 1 rj0	j=1
uniformly over all t = nT,..., n(T + 1) — 1, i =
(i )n(T +1)-1
(iτ )τ=nτ	.
1, . . . , n and the choice of coordinates
32
Published as a conference paper at ICLR 2022
Proof. Let CP = J p(nT)(θ) exp (-/ PW hiτ (θ)δg(τ)) dθ. Then, noticing that 斗，
号/Cp and Ilhil∣∞ ≤ 1, We have that
M
X hi(θj)
j=i
M
X hi(θj)
j=i
M
X hi(θj)
j=i
M
M
-X hi(θj 湾'
j=i
M	^(t)
-X hi(θj ) C
j=1	C
I P着C
1 -
≤
1 - X 婚)∙
j=i
We apply the same argument as Lemma 11 to evaluate the far right hand side ∣1 - PMI 斗)∣ by
replacing hit (θ) with 1 in the definition of H. Then, we obtain the assertion.	□
From now on, we define CD as
5
^ -
Then, we have the following bound.
Lemma 13. We have that, with probability 1 一 δ with respect to realization ofparticles (θj)M=ι,
Igf, -gT)I ≤ c2cicd
(45)
for all t = nT +1,..., n(T +1), i = 1,...,n and the choice ofcoordinates (iτ )：=+1)T uniformly.
Proof. From Lemma 11 and 12, with probability 1 - δ, we have that
M
X hit (θjHt)- J hit (θ)p⑴(θ)dθ
≤ c*d,
and that
M	^(t)	M
X hit⑼)j - X hit眄潸 ≤ 哈D,
for all t = nT,..., n(T + 1) - 1. Therefore, by Lemma 8, we have that
屋+1)- g(t+1)I
≤ + (∣X hi(θj) P^ -Zh (θ)*(θ)dθ + nλ2 Ig(t)-严 I
1
+ n⅛
2c"d + X h(θj) f-ɪ - -ɪ) ∣ + Agr-严 I
,, r .J	,, r	n2
j=l	∖ 乙j0 = ι j	乙j0=1 j / ∣
(46)
The second term of the right hand side of Eq. (46) can be further bounded as
M	r r(t)	r(t)
X hi(θj) ( pM (t) - pM
j=1	∖ 2j∕=ι r j0	2j0=1
33
Published as a conference paper at ICLR 2022
M
≤X
3=1
r(t)
r3
—
PM=1 ¥
M
M
+X
3=1
M
PM=1 ¥
—
≤C1 X	j ∣ + X
3=1
M
3=1
≤C1
Xw)-叫+ C1 X
√一 -1	√z -1
1
3=1
M
:0:
j
1
r(t)
r3'
M
≤C1 X∣rjt)- 31+ C1 X∣j
3' = 1
PM	£(t)
乙 3'=1 r3'
PM r(t)
' 3' = 1 r3'
M
-X ¥
3'=1
—
—
PM=I rjt)
3=1
M
≤2C1
3=1
M
≤2C1 X
3=1
—
1
M exp
1
nλ2
t
X h,4θ)δg(T)
τ=nτ
-OeXP (-n⅛ X hi- (θj 阿(T)
∖	T=nT
≤M X ∣g(τ+1)-打I)I
2 τ=nτ
(47)
for t = nT,..., n(T + 1) — 1. This and Eq. (46) yields
≤ Y + n⅛
5 +
2C2 + 1
nλ2
t-1
X I 宓+1)- C+1)l
T=nτ
for t = nT +1,..., n(T + 1) - 1. Also, it holds that
∣∕nT +1)	(nτ +1)∣	ci
lgi	- gi	l ≤ -.~~- eD,
Y +六2
from Lemma11.
Therefore, applying Gronwall,s lemma (“summation version” of Lemma 18) yields that for t
nτ,..., n(τ +1) — 1,
∣g(t+1)-婷I)∣≤ I J⅛ +
Y + nλ2
2C2 +1
2
nλ2
1+
2C2 + 1
nγλ2 + 1
n-2
C1 eD
2C12
≤ τ+⅛
Y + nλ2
2C12
≤ ^rɪ
Y + nλ2
≤c2c2eD,
1+
exp
2C12 + 1
nγλ2 + 1√
n(2C2 +1)
n-1
C12 eD
nγλ2 + 1
Cle D
which concludes the proof of Eq. (45).
□
C.2.3 Integration OF TWO sources OF discretization error
From now on, we integrate the convergence analysis from Section C.2.1 and Section C.2.2. Then,
we extend our analysis from that of each inner loop to that of the outer loop over T = 0,..., T^nd.
So far, we have been arguing only within one inner loop, while we may choose a different number
34
Published as a conference paper at ICLR 2022
M of particles in each outer-loop T . Accordingly, we denote the number of particles at the T -th
outer-iteration by M (nT) and add a subscript to e° as
(nT) def
eD	=
7 ∖) M(nT) log
(4n(7nd))
Moreover, we assume that the bound of Lemma 13 holds for every T = 0, . . . , Tend - 1. We denote by
E this even. We know that P(E) ≥ 1-δ. In the following, we analyze the convergence under the event
E where the bounds in Lemma 13 hold, so that |g(t) - g(t)| ≤ (C2eCnT) + CzCge^nT)) (T = d-t]-1)
holds owing to Lemma 10 and Lemma 13. Therefore, the expectation with respect to the choice
of coordinates is taken under the condition of this event. We note that the bound in Lemma 13
holds uniformly over the choice of the coordinates and thus the distribution of the coordinates is not
affected by being conditioned by the event.
In order to evaluate the effect of the discrepancy of coordinate updates on the difference of eA, we
present the following Lemma 14 on the local smoothness of dual space.
Lemma 14. Suppose that n ≥ 2λB2 and Assumption 1 holds. Then, D(g) is η1^ (λ2γ0 + 1)-smooth
with respect to g ∈ Rn on the set A defined by
A = {g ∈ Rn J min ∣rnn g(t), min g(t)} ≤ gi ≤ max ∣max g(t), max If)} (i = 1,..., n)},
where t takes t = 0,1,..., nTend
Proof. Suppose that the coordinate i is chosen at the t-th iteration. By the definition of g(t), we have
that
M	r(t)	1
ι'Mgf))ι≤ £ ιhi (θj )| PMj~~而 + nλ^ |gf)- g( 1)1,	(48)
j=1	j0=1 rj0	2
where '『 is an element of the sub-differential. The first term of the right hand side can be bounded
by 1 as follows:
M	r(t)	M	r(t)
Elhi(θj)|PMj (t) ≤ maxlhi(θj)| E PMj (t) ≤ 1.
j=1	j0=1 rj0	j=1 j0=1 rj0
Also, Lemma 6 implies that the second term can be bounded by IBt ∙ Therefore, under the assumption
n ≥ 2B2, we have that
λ2
忆(g(t))l ≤ 2.
Applying the same argument, we also have that
忆(严)1 ≤ 2.
Therefore, for i ∈ [n] that has already chosen in the algorithm until the t-th step, we see that '
is γ0-smooth on the interval between g(t) and g(t). For the coordinates i that have not yet chosen,
we have that g(t) = g(t) = g(O) and 阵0(g(O))I ≤ 1. Therefore '* is γ0-smooth on the box A. By
summarizing this argument and applying Lemma 4, we see that D(g) is /(λ2γ0 + 1)-smooth. □
Combining the above results, we obtain the evalution of eA and sufficient conditions for the first
statement of Lemma 8.
Lemma 15. Suppose that n ≥ 2λB2 and fix an arbitrary T. if E[D0nT0)) — D(g(nT0))] ≤
Co exp (—ST0) is satisfiedfor all T0 ≤ T, then we have that
EDdt))- D(g(t))] ≤ C0 exp(-S(T +1)) + (2 -1μ)μ 10 +	(C2eCnT) + C2CleD))2,
(49)
35
Published as a conference paper at ICLR 2022
forallt = nτ j..∙,n(T+ιγwhere μ = 1+W⅛+⅛
In particular, if
(nτ)	ST	(nτ) / C3	ST
eC ≤ C3 exp (--2^, , eD	≤ C2 exp (-^2^
are satisfied for all T = 0, . . . , Tend, where
C /	p/C0(2 - μ)μ
C3 ≤ ---Z	^=,
一 C2∕2e≡(70 + 1∕λ2),
then it holds that for all t = 0,1,..., nTend,
ED伍⑴)-D(g⑴)]≤ C0 exp(-5T),
where T
(50)
Proof. Under the assumptions that n ≥ 2B2, Lemma 14 holds. Fix some T and We will show the
first assertion for t = nST + 1, . . . , nS(T + 1).
Let At = {sg(t) + (1 - s)g(t) I S ∈ [0,1]}, then Lemma 15 implies that D(g) is /(λ2γ0 + 1)-
smooth on the set. Hence, for μ = ——rγ-2-----C-Y, the Cauchy-Schwarz inequality yields
1+ 2⅛ (B1+B2 + J)
D(gS(t)) - D(g(t))
≤ SUP kVD(g)k2 kg(t) - g(t)k2
g∈At
1
≤ —
-2
nλ2
μλ2γ0 + 1
(置“⑼。2+1 λ⅛Fkg(t) - g(t)k2
1
≤-
-2
μ SUP [D(g*) - D(g)] + 1
. g∈At	μ
γ0 +
* JC2C2e 铲)2
If D (gS(t)) - D(g(t)) ≥ 0, then we have that
D(gS(t)) - D(g(t))
≤2 〃(D(g*)-D(g㈤))+ ɪ 卜 + E)(C2e严) + 加整DT))2 .
Rearranging the terms we have that
D(gS(t)) - D(g(t))
≤ 2-μ。(力-D(g(t))) + 占 卜 + () S*)+C2C 说T ))2,
This holds as well when D(gS(t)) - D(g(t)) ≤ 0.
Here, Lemma 5 implies that E[D(gS(t))] is monotonically non-decreasing within the same inner
loop, and thus we have that E [D(g*) - D(g(t))] ≤ E [D(g*) - D(g(nτ))]. Moreover, the right
hand side E [D(g*) - D(g(nτ))] can be further bounded by(Bi + B2 + j-C⅛) exp (-ST)=
CL 2μμ exp(-S(T + 1)) by using the assumption and applying Eq. (33) of Lemma 8. Therefore,
E hD(gS(t)) - D(g(t))i
≤C exp(-S(T +1))+(	1) (γ0 + ɪ) (C2eCT) + C2。说T))2 .
2	(2 - μ)μ ∖	λ2√
This concludes the first assertion.
36
Published as a conference paper at ICLR 2022
We will show the second assertion by induction. It is obvious that this is true at initialization. Suppose
that Eq. (50) holds for t = 0,1,... nT. Then the assumption of the first statement of Lemma 8 holds
for T0 = 0,...,T, which leads to Eq. (49) for t = nT +1,..., n(T +1). Then We have that Eq.
(50) also holds for t = nT +1,..., n(T + 1), since we chose C3 as
pPC0(2 - μ)μ
C 3 =-----,	二,
C2 √2e≡(70 + 1∕λ2)
so that the right hand side of (49) is bounded by C0 exp (-s(T + 1)). Thus, the proof is completed
by induction.
□
Although The proof for the first statement of Lemma 8 is almost done, we have some proof left for
the second claim. The next two Lemmas measure the effect of discrepancy of coordinates in the dual
space on the value of the main problem.
Lemma 16. P(p[g]) is nɪɪ (Y + 3λ2 + 9B2)-smooth with respect to g on the set A.
Proof. We show the assertion by a direct calculation. Let Ep[g] [hi] d=ef R hi(θ)p[g](θ)dθ. First, the
second derivative of the loss function can be calculated as
∂2
j 'i (Ep[g] [hi])
1	∂0
=nλ^∂gfe'i(Ep[g] [hi])(Ep[g] [hi] Ep[g] [hj] - Ep[g] [hihj])
=n∣λ∣ ['i00 (Ep[g] [hi]) (Ep[g] [hi] Ep[g] [hj] - Ep[g] [hihj]) (Ep[g] [hi] Ep[g] [hk] - Ep[g] [hihk])
+ 'i0 (Ep[g] [hi]) ∙ ((Ep[g] [hi] Ep[g] [hk] - Ep[g] [hihk]) Ep[g] [hj]
+Ep[g] [hi] (Ep[g] [hj] Ep[g] [hk] - Ep[g] [hjhk]) - (Ep[g] [hihj] Ep[g] [hk] - Ep[g] [hihjhk]))]
≤ n⅛(Y+3B∣),
where we used smoothness of `i for the last inequality.
The regularization term can be split into the following two terms:
λ1	p[g](θ)kθk22dθ + λ2	p[g](θ) log p[g](θ)dθ
1n
-nT2j hi(θ)p[g](θ)dθgi + λ∣ log
q[g](θ)dθ
We already confirmed 康-smoothness of the second term in Lemma 4. As for the first term, we
have that
∂2
∂gj gk
1	∂2
nλ2 ∂gk
—
n XX EpM [hi] %)
卜 iEpg [hj]+n X (Ep[g] [hi] Ep[g] [hj ] - Ep[g] [hihj]) gi
-n2λ∣ 2λ2 (Ep[g] [hj] Ep[g] [hk] -E
1n
,p[g] [hj hl]) + n): ((Ep[g] [hi] Ep[g] [hk] - Ep[g] [hihk]) Ep[g] [hj]
+Ep[g] [hi]	Ep[g]	[hj] Ep[g]	[hk]	- Ep[g]	[hjhk]	-	Ep[g]	[hihj] Ep[g]	[hk]	- Ep[g]	[hihjhk]	gi
≤ n⅛ (2λ2+6B2),
where we used Lemma 6 for the last inequality to bound gi .
Combining the two inequalities and the result from Lemma 4, we obtain the assertion.
□
37
Published as a conference paper at ICLR 2022
Lemma 17. Suppose Eq. (50) ofLemma 15 and its assumption holdfor t = 0,1,..., nT^nd. Then,
we have that
E [P(p[g(t)]) - P(p[g(t)])]
2y + 6λ2 + I8B2
≤	3
λ √Y
where T =「'.
Ir R	L CO a∕c0.(2 - μ)μ
VBI + B2 + C0 + = √2(y. + 1) exp(-	),
Proof. By using the smoothness of P (p[g]) by Lemma 16, we have that
P(p[g⑴])-P(p[g(t)])
≤ sup IlVP(p[g])∣∣2 W)-P⑴∣2
g∈At
4
Y + 3λ2 + 9B2
SUP ∣∣VP(p[p])∣∣
g
Y + 3λ2 + 9B2
W)-P㈤∣2
≤ JsupP(MgD - P(p[g*]) j
Y + 3λ2 + 9B2
nλ2

where T =[5]-1. if P(p[g(t)]) - P(p[g(t)]) ≥ 0, then we have that
P(p[g(t)])- P(p[g(t)])
≤q(p[g(t)]) - P(p[g*]) j
Y + 3λ2 + 9B2
Y + 3λ2 + 9B2
λ2√n
kg(t)- g*k2 (。2
λ2
尸丁)
CC
+。2。久
≤
Since D is Y-strongly convex, we have that∣g⑴- g*∣2 ≤ Jn (D(g*) - D(g⑶)).Therefore, it
holds that
P (p[g(t)])- P(p[g(t)])
≤
Y + 3λ2 + 9B2

J(D(P*) - D(Pt)))《2CCT)+ c2cι4nT，).
which holds as well when P(p[g(t)]) - P(p[g(t)]) ≤ 0.
By taking expectations, it yields that
E [P(p[g(t)]) - P(p[严])]
≤
≤
≤
Y + 3λ2 + 9B2
λ√Y
Y + 3λ2 + 9B2
J(D(g*)- D(g6))(C2望丁)+。2。江尸))
λ√Y
Y + 3λ2 + 9B2
,E[D(g*)- D(g(t))] ge严)+ C2C、尸))
,E [D(g*) - D(g⑴)+ D(g(t)) - D(g⑴)]92e严)+ CzdT))
λ√Y
E
2y + 6λ2 + I8B2
Jbi + B2 + K‰ + C0C2C3 exp (-5T),
where We used Lemma 8 and Lemma 15 for the last inequality.
≤
λ√
Furthermore, substituting C2C3
√2e≡(70 + 1∕λ2)
,we have the assertion.
□
38
Published as a conference paper at ICLR 2022
Finally, we will prove the Theorem 3, which gives sufficient conditions about sampling accuracy and
number of particles for Theorem 1.
Theorem 3. Suppose that Assumptions 1 and 2 hold. When we choose an Option (A), suppose that
we set Tend so that
Tend ≥ 2 n
n
1+
log
(BI + B2 + 1-eC0(-g))
ep
(51)
as in Eq. (30) in Lemma 8. Furthermore, when we choose an Option (B), where we choose a solution
randomly from t = n(‰d 一 d 1 ]),∙∙∙, nTend, suppose that we set Knd so that
Tend ≥ 2- ( 1 +
n
n⅛)log
+1 κ m,
where
Cq
max
2+
n⅛) (BI+B2 +
Co
1 一 exp ( —s)
4C0,
(8γ+24λ2 + 72B2)4B1+B2 + C0+ ιf0-J^ √ C0eV(2-μ)μ
))
(52a)
(52b)
(52c)
Then, the condition imposed on e(At) and e(Bt) in Lemma 8 can be satisfied for both options under the
event E if the following inequalities are satisfied:
2B2
n ≥ F,
e(CnT ) ≤ C3 exp
ADT) ≤ C2 eχp
sT),
可)⇔ M (nT) ≥
49C4	4nTend
F- log( -ɪ JeXP(ST)，
(53)
for T = 0,1,...,1nd — 1. That is, under the conditions above, E [P(p[^]) — D(g))∣E] ≤ ep is
SatiSfiedfor the solution g of either Option A or B.
Proof. As for the Option (A), according to Lemma 15, Eqs. (51) and (53) are the sufficient conditions
for the first statement of Lemma 8 in the event E .
Next, we will show the convergence of Option (B). In the same way, Eqs. (52a) and (53) assure
that e『T)is smaller than Coe-sT for every T = 1,..., £nd, according to Lemma 15. Furthermore,
Eqs. (52b) and (52c) states that both e(At) and e(Bt) are uniformly bounded by 詈 for all t = n(‰d —
d-]) + 1,..., nTend. Note that the evaluation of e[) follows from Lemma 15, and that of AB is
derived in Lemma 17.
Therefore, all assertions are proved.	□
Theorem 3 essentially shows Theorem 1. More detailed explanations can be found in the following.
Proof of Theorem 1. Theorem 3 gives the sufficient conditions for Lemma 8. Thus, we have that, for
Option (A), with Eqs. (51) and (53), an expected duality gap E [P(p]g(nTend)D — D(g(nTend))|E] ≤
ep can be achieved, conditioned by the event E whose probability is no less than 1 — δ. In the
same way, for Option (B), with Eqs. (52a)-(52c) and (53), an conditional expectation of duality gap
E P (p[g(t0end)]) — D(g(t0end))|E
≤ ep can be achieved. The sufficient condition of tend is then
obtained by noticing tend = SSTend and setting C- = C3/7, C2 = C2 and C3
2 Cq
1+nγ⅛
39
Published as a conference paper at ICLR 2022
Next, We evaluate the constants as follows. First, We note that & = C2 by their definitions. Moreover,
C-1 = ≠g+μμ)C2 = θ(λ-1∕2C2exp (』))=0(λ-"C2 exp (『))
2C2	n(2C2 + 1)	2
where we used C2 = Yl 1 exp (,)，1+/ J and Cf = C2. Finally, the evaluation of C3 can be
obtained by bounding Cq using 1/(1 - exp(-s)) = O(3T) and 1 ≤ 1 + nλ ≤ 1 + 2B⅛γ =
□
O(1).
C.2.4 Discrete Gronwall’ s lemma
In the proofs of Lemmas 8, 10 and 13, we used a discrete version of the Gronwall’s lemma. We give
its formal statement in the following lemma.
Lemma 18 (Discrete Gronwall’s lemma).
(1)	(Vanilla version) Suppose that a sequence of real numbers (un) satisfies
un+1 ≤ aun + bn+1 (∀n ≥ 0),
for a positive real a > 0 and a sequence of real numbers (bn). Then, it holds that
n
un ≤ anu0 + an	a-kbk.
k=1
(2)	(Summation version) Suppose that a sequence of real numbers (un) satisfies
n-1
un ≤	auk + B (∀n ≥ 1),
k=0
for a positive real a > 0 and a real number B. Then, it holds that
un ≤ (1 + a)n-1(B + au0).
Proof. The assertion is just an application of the standard discrete version of the Gronwall’s lemma.
However, we give a proof for completeness.
The first assertion is a direct consequence of Lemma 5.1 of Mischler (2019). The second assertion is
also proven by Lemma 5.2 of Mischler (2019). Indeed, Lemma 5.2 of Mischler (2019) yields that
n-1
un ≤ B + Xa(1 +a)n-1-kB +a(1 + a)n-1u0.
k=1
The right hand side is equivalent to
n-2
B 1 +aX(1 + a)k +a(1 + a)n-1u0
k=0
=B [1 + a (1 + a)n 1- 11 + a(1 + a)n-1uo
1+a- 1
=(1 + a)n-1 (B + au0),
which yields the second assertion.	□
D Additional experiments and details of experimental settings
Here, we give additional experiments and more detailed explanations of our experimental settings.
All experiments were conducted under Google Colaboratory. All codes of the experiments in the
main text and the following additional experiments are provided in the supplementary material.
40
Published as a conference paper at ICLR 2022
Additional experiments for a single neuron teacher network We consider a setting where the
teacher network consists of a single neuron:
y = σ(w-xi) + J,
where the input dimensionality is d = 10, Wi ∈ Rd is generated from the uniform distribution on
the unit sphere Sd-1 in the d-dimensional Euclidean space, xi ∈ Rd is distributed from a standard
normal distribution on Rd, i ∈ R obeys the Gaussian distribution with mean 0 and standard deviation
0.5 and the activation function σ is ReLU. We generated n = 100 trainig data points from this
model and trained a student network by using P-SDCA, PDA, and a naive SGD. Here, as the
number of particles, we employed M = 200 and the student network is the tanh neural network
(hθ(x) = tanh(w>x + b) with θ = (w, b)). We employed the squared loss `(f, y) = (y - u)1 2/2
for the loss function. We conducted experiments with three patterns of regularization parameters:
λ1 = λ2 = λ ∈ {0.01, 0.001, 0.0001}. The naive SGD was trained to minimize the primal objective
for the student network with M = 200 without the entropy regularization, i.e.,
1n 1M	λM
n X ' (M Xhθm (Xi),yi) + M X kθmk2.
We employed the ULA for sampling the particles for PDA and P-SDCA where we ran the algorithm
with 200 steps in each re-sampling stage. The step-sizes η used for sampling (ULA) and SGD were
set as 10-3.
We plotted the excess primal objective value against the number of gradient evaluations for each
method in Figure 2. The excess primal objective value is computed by subtracting the minimum
objective value throughout the optimization and all optimization methods from the current primal
objective. Since we are approximating the primal variable by a finite particles, we cannot determine
the entropy term exactly. To handle this issue, we employed the k-NN entropy estimator (Kozachenko
& Leonenko, 1987; Brodersen, 2020) to estimate the entropy term from the finite particles. This is
also applied to SGD to compute the primal objective with the regularization term for the solution
obtained by SGD. From Figure 2, we can see that P-SDCA and PDA behave almost identically for
large regularization parameter λ. In that regime, SGD once decreases the primal objective rapidly
but increases it afterward. This is because SGD is minimizing an objective without the entropy
regularization. For λ = 10-2, P-SDCA and PDA does not decrease the excess primal objective
below 10-2. This would be due to the instability of the entropy estimator. For small λ, we see that
P-SDCA outperforms other methods after some iterations because it yields linear convergence. This
observation supports our theoretical analyses.
Additional experiments for a teacher network with multiple neurons We consider a setting
where the teacher network consists of multiple neurons:
1M
y = M σ(w σ(wm>χi) + ei,
t m=1
where the width of the teacher network is Mt = 10, the input dimensionality is d = 5, the activation
function of the teacher network is σ(∙) = tanh(∙), the noise is drawn from Normal distribution and
SSo- 6u-≡ehss°,u×3
0.0	0.5	1.0	1.5	2.0	2.5
Gradient evalueatio∏s ie6
(a) λ = 0.01
SSO- 6u-~ehMsu×ω
0.0	0.5	1.0	1.5	2.0	2.5
Gradient evalueatio∏s ie6
(b) λ = 0.001
1 ɪ I ɪ
SSO- 6u-~ehMsu×ω
0.0	0.5	1.0	1.5	2.0	2.5
Gradient evalueatio∏s ie6
(c) λ = 0.0001
Figure 2:	The number of gradient evaluations versus the excess primal objective for P-SDCA, PDA
and SGD for a teacher network with a single ReLU neuron.
41
Published as a conference paper at ICLR 2022
its Signal to Noise Ratio (SNR) is 7.0. For each method, we selected the optimal step size: 10-4
for ULA in P-SDCA and PDA, and 10-5 for SGD. In the following, “step size of P-SDCA or PDA”
refers to the step size for ULA in the sampling scheme. In addition to that, we plotted SGD with its
step size 10-4 to see how the step size affects the convergence of SGD. The purpose to include SGD
as well as P-SDCA and PDA is to see how the entropy term will behave when we do not explicitly
include the entropic regularization term into the objective function.
(a) λ2 = 0.01
w6×10-1
f 4 XlO-1
∣3×10-1
/2XlOT
a.
0：0	03	l：0	1.5
Gradient evalueations le6
(b) λ2 = 0.001
g
w6×10-1
∣4× 10T
∣3× 10T
∣2 × 10~1
H
(c) λ2 = 0.0001
Figure 3:	The number of gradient evaluations versus the excess primal objective for P-SDCA, PDA
and SGD for a teacher network with multiple tanh neurons.
We compared our method with SGD and PDA with the regularization parameters λ1 = 0.01 (fixed)
and λ2 = 0.01, 0.001, 0.0001. We repeated the experiments 10 times and took the average of primal
objective values, which includes regularization terms. The primal objective value is plotted against
the number of gradient evaluations in Figure 3 where the error-bar represents the standard error over
the 10 repetitions.
For λ2 = 0.01, we can see that particle methods (P-SDCA and PDA) showed faster convergence than
that of SGD. Next, as for λ2 = 0.001, we can see that P-SDCA converged faster than SGD and PDA.
Although PDA and SGD with step size η = 10-5 also converged eventually, the behavior of PDA in
the intermediate stage was quite unstable and SGD with η = 10-5 showed quite slow convergence.
These observations support the superiority of our method. Finally, as for λ2 = 0.0001, SGD with step
size η = 10-5 was the fastest due to the small regularization parameter λ2 for the particle methods
which makes the convergence of them slower. However, we still see that P-SDCA outperformed
PDA in terms of both convergence speed and stability. The convergence of SGD is strongly affected
by its step size. In fact, SGD with larger step size (η = 10-4) could decrease the objective rapidly
at the early stage, but it eventually diverged. This would be partly for the same reason as in the
single neuron setting, that is, due to the lack of the entropic regularization in the objective of SGD. In
summary, we can see that P-SDCA properly minimizes the corresponding objective function in wide
range of λ2 and shows faster and more stable convergence than PDA as indicated by our theory.
Experiments on MNIST dataset We conduct numerical experiments to illustrate the “feature
learning” aspect of mean field neural network. For this purpose, we run the algorithm in a binary
classification task that separates “2” and “4” in the MNIST dataset. We subsampled n = 1000 training
examples and trained the two layer tanh network with width M = 2500. The step size for ULA in
P-SDCA was set as η = 10-5 and the regularization parameters were set as λ1 = 10-2, λ2 = 10-4.
To visualize the adaptivity of the feature map, we first plot the evolution of singular values of the
first layer’s weight matrix, W = [w1, w2, . . . , wM] ∈ Rd×M, until convergence, when the solution
achieves 99.8% training accuracy. We can see that the singular values of the network trained by
P-SDCA drastically changed during optimization. This indicates that the mean-field neural network
does not ”freeze” and can adaptively learn features during optimization by P-SDCA. We also plotted
the same quantity for SGD updates in the NTK regime. In contrast, we observe only small change
of the singular values. This is because the first layer’s parameters do not change much during
optimization in the NTK regime; in other words, SGD in the NTK regime does not perform adaptive
feature extraction. In that sense, we expect that training in the mean field regime is effective in a
setting where adaptive feature extraction is required.
In addition to the singular value evolution, we evaluate how well the extracted features are “aligned”
to the label. For that purpose, we define a kernel function kW (x, x0) = PmM=1 σ(wm> x)σ(wm> x0) for
a weight matrix W = [w1, w2, . . . , wM] ∈ Rd×M. Then, we define the kernel alignment (Cristianini
42
Published as a conference paper at ICLR 2022
100.0
80.0
60.0
50.0
ω 40.0
B 30.0
> 25,0
® 20.0
°, 15.0
in
10.0
8.0
Outer loop steps
---SVl
——SV2
——SV3
-'SV4
SV5
others
(a) P-SDCA (mean field regime)
e-5。-5。-5。-5。
655443322
n-e> ∙ln6u
---SVl
——SV2
——SV3
——SV4
—SV5
others
50	100	150	200	250	300
Steps
(b) SGD (NTK regime)
Figure 4:	Evolution of singular values of the first layer’s weight matrix during optimization. The
figure shows top 5 largest singular values in addition to the k × 10-th largest singular values for
k = 2, 3, 4, . . . , 9 and k × 100-th largest singular values for k = 1, 2, 3, .
et al., 2002) on the training data (xi, yi)in=1 as
A(kW) :
______〈Kw ,yyTiF_______
√ hKw ,Kw iFhyyτ,yyτiF
where hA,BiF := Pin=1 Pjn=1 Aij Bij, KW = (kW(xi,xj)in=,n1,j=1) and y= (y1, . . . ,yn)>. We
also define the kernel alignment on the test data in the same manner. We can see that the kernel
alignment represents how strongly the kernel function defined by the features after the first layer is
aligned to the target signal.
The kernel alignment of the solutions obtained by each method (P-SDCA, SGD in NTK regime, SGD
in mean field regime) is depicted in Figure 5. We can see that P-SDCA (and SGD in the mean field
regime) properly improves the kernel alignment, which means that P-SDCA can extract informative
features adaptively depending on the data. On the other hand, SGD in the NTK regime does not
increase the kernel alignment due to the parameters almost “frozen” at initialization. This experiment
highlights how well the mean field neural networks can execute feature learning.
一 SGD (NTK)
—SGD (MF)
——PSDCA
50	100	150	200	250	300
Outer loop steps / Steps
(a) Kernel alignment on training data
0.2- /	——SGD (NTK)
I	——SGD (MF)
——PSDCA
00 0	50	100	150	200	250	300
Outer loop steps Z Steps
(b) Kernel alignment on test data
Figure 5:	Evolution of kernel alignment of the extracted features during optimization.
43