Published as a conference paper at ICLR 2022
How to deal with missing data in
supervised deep learning?
Niels Bruun Ipsen* *	Pierre-Alexandre Matteit ^	Jes FrellsenM
nbip@dtu.dk	pierre-alexandre.mattei@inria.fr	jefr@dtu.dk
Ab stract
The issue of missing data in supervised learning has been largely overlooked, es-
pecially in the deep learning community. We investigate strategies to adapt neural
architectures for handling missing values. Here, we focus on regression and clas-
sification problems where the features are assumed to be missing at random. Of
particular interest are schemes that allow reusing as-is a neural discriminative ar-
chitecture. To address supervised deep learning with missing values, we propose
to marginalize over missing values in a joint model of covariates and outcomes.
Thereby, we leverage both the flexibility of deep generative models to describe
the distribution of the covariates and the power of purely discriminative models
to make predictions. More precisely, a deep latent variable model can be learned
jointly with the discriminative model, using importance-weighted variational in-
ference, essentially using importance sampling to mimick averaging over multi-
ple imputations. In low-capacity regimes, or when the discriminative model has a
strong inductive bias, we find that our hybrid generative/discriminative approach
generally outperforms single imputations methods.
1 Introduction
Missing data affect data analysis across a wide
range of domains and the sources of missing
values span an equally wide range. Recently,
deep latent variable models (DLVMs, Kingma
& Welling, 2014; Rezende et al., 2014) have
been applied to missing data problems in an un-
supervised setting (e.g. Rezende et al., 2014;
Nazabal et al., 2020; Ma et al., 2018; 2019;
Ivanov et al., 2019; Mattei & Frellsen, 2018;
2019; Yoon et al., 2018; Li et al., 2019; Ipsen
et al., 2021; Ghalebikesabi et al., 2021), while
the supervised setting has not seen the same at-
tention. The progress in the unsupervised set-
ting is focused on inference and imputation and
can therefore be useful as an imputation step
before learning a discriminative model. Tradi-
tionally the focus has also been on inference
and imputation either as a goal in itself or be-
fore supervised learning on the (possibly mul-
tiple) imputations and observed data (Rubin,
1976; 1996; Little & Rubin, 2019).
Supervised learning in the presence of miss-
ing values has different goals and pose different
challenges than inference and imputation (Josse
Figure 1: Left: supMIWAE computational struc-
ture: encoding network gγ , decoding network hθ
and discriminative network fφ . Right: Graphi-
cal model: supMIWAE seen as a VAE for missing
values concatenated with neural discriminator.
Zk 〜qγ(z∣xobs)	Y-----
* Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark
*Universite Cote d,Azur, Inria (Maasai team), Laboratoire JA Dieudonne, CNRS, France
^ Equal contribution
1
Published as a conference paper at ICLR 2022
et al., 2019; Le Morvan et al., 2020a; 2021). Here the overall aim is to minimize an expected pre-
diction error. However, optimal single imputation does not necessarily lead to optimal prediction in
terms of minimizing a prediction error (Josse et al., 2019). One challenge is that predictions based
on inputs with missing values can be ambiguous, that is, the conditional distribution of the miss-
ing values given the observed may be multimodal and the optimal prediction may change with the
mode. With single imputation, the conditional distribution over the missing data is discarded, and
the optimal single imputation can no longer reflect this ambiguity, see figure 2. Even the optimal
single imputation leads to biased parameter estimates compared to the fully observed case (Bertsi-
mas et al., 2021), and may in some cases lie outside the distribution of the data. Another challenge
is that the number of missing value patterns grows combinatorially with the number of features p,
requiring 2p submodels to fit the Bayes predictor in the linear case (Le Morvan et al., 2020a). In
multiple imputation (Rubin, 1996), several imputations are drawn from the posterior predictive dis-
tribution of the missing values, reflecting the full conditional distribution of the missing values and
thus the uncertainty about what is missing. This allows for uncertainty estimates in downstream
tasks such as prediction. This in turn requires fitting as many discriminative models as the number
of imputations.
1.1 Contributions
In order to address supervised deep learning with missing values we develop the supervised missing
data importance-weighted autoencoder (supMIWAE) bound, based on the approximate maximum
likehood techniques used by Burda et al. (2016) and Mattei & Frellsen (2019). This is a scalable
approach to marginalizing over missing values in a joint model of covariates and outcomes
Pφθ (y, Xobs, Xmiss)= Pφ (y∣χ0bs, Xmiss)pθ (Xobs, Xmiss).	⑴
The covariate model pθ (xobs, xmiss) = R p(xobs, xmiss|z)p(z) dz is a DLVM in this work, but
can be any probabilistic model imposing a joint density over covariates. The outcome model
Pφ (y| xobs, Xmiss) is any neural discriminative architecture that would have been used in the
complete-data case, parameterizing a density over the outcomes. The graphical model is shown
in figure 1 along with its computational structure.
Once the joint model has been trained using the supMIWAE bound, the conditional distribution of
the outcome given the observed parts of the input can be found as
Pφ(y∣X0bs) = / Pφ(y∣X0bs, Xmiss )pθ (Xmiss∣Xobs) dXmiss.	⑵
This is approximated using importance sampling techniques, averaging predictions over multiple
importance samples (akin to multiple imputations, see appendix C for a deeper discussion) from the
generative model.
The model can be trained end-to-end or a pretrained generative model can be coupled with a dis-
criminative model. Having a joint model allows for supervised imputations, where available labels
can help guide the imputations from the generative model by adjusting the importance weights ac-
cordingly.
Joint models over covariates and labels have previously been used to marginalize over missing values
in the covariates, using Gaussian mixture models either as the model over the covariates or directly as
the joint model over covariates and outcomes (Ghahramani & Jordan, 1995; Ghahramani & Hinton,
1996; Ahmad & Tresp, 1993; Tresp et al., 1994; 1995; Smieja et al., 2018). Our approach shows
how to use more flexible generative models, such as DLVMs, provides an efficient optimization
procedure and allows for keeping any deep discriminative architecture unchanged.
2 Background and notation
We define the random variable X = (x1, . . . , xp) ∈ X which takes values in a p-dimensional feature
space X = Xi × ∙∙∙ × Xp. There is a corresponding (possibly vector valued) response variable
y ∈ Y. A missing process obscures parts of X resulting in the mask random variable s ∈ {0, 1}p
where
(1 if xj observed,
0 if xj missing.
(3)
2
Published as a conference paper at ICLR 2022
Imputation
supMIWAE
Figure 2: Left: Kernel density of a two-class dataset (the burger dataset), where and y = 0 is blue
and y = 1 is brown/orange. Observations with missing values are shown as sticks in the bottom
margin of the plots and colored according to their class label. For observations with missing 2nd
coordinate there is an inherent ambiguity about the class with p(y = 1|xobs) = 2/3. The optimal
single imputation is shown as the dashed black line. Middle: Decision surface for a neural classifier
trained on optimally-imputed data. The decision surface has to reflect that for the imputed data
p(y = 11∣o(XObs)) = 2/3, thus the increased class y = 1 probability around the optimal imputations
(here ι0 is the imputation function, see appendix A.1). Right: Decision surface as learnt by the
supMIWAE. It is similar to the decision surface that would be found in the absence of missing data.
We let obs(s) denote the indices of the non-zero entries of s and miss(s) denote the indices of the
zero-entries of s, such that Xobs(s) is the set of observed elements of X, and Xmiss(s) is the set of
missing elements ofX. For simplicity we will omit the s and write Xobs, Xmiss respectively, whenever
the context is clear.
Following (Yoon et al., 2018; Le Morvan et al., 2020b; Ghalebikesabi et al., 2021) we define the
incomplete random variable x = (xι,..., Xp) ∈ X which takes values in X = (Xi ∪ {na}) × ∙∙∙ ×
(Xp ∪ {na}) where missing values are represented by the symbol na, such that na ∙ Xj = na and
na ∙ 0 = 0. We typically only have access to X, where
X = x Θ S + na Θ (1 一 S)	(4)
and Θ is the Hadamard product. Finally, We are given n i.i.d. copies of the random variables X and
y which We collect in a dataset D = {Xi, yi}n=「or alternatively D = {xobs, y 4了
We make the additional assumption that the data are missing at random (MAR, see e.g. Seaman
et al., 2013; Little & Rubin, 2019). Specifically, this means that we assume that S and Xmiss are
independent given Xobs. This assumption allows to avoid to explicitly model the missing mechanism.
Our approach could be extended beyond MAR by using a generative model fit for this purpose, like
the not-MIWAE of Ipsen et al. (2021), or the deep pattern-set mixture of Ghalebikesabi et al. (2021).
2.1	Challenges when training disriminative models with missing data
Our predictive model will be defined through a mapping fφ : X → H used to parameterize a
conditional distribution
Pφ(y∣X) = Ψ(y∣fφ(X)).	(5)
Here (Ψ(∙∣η))η∈H is a parametric family of distributions over the outcome space Y, such as a Gaus-
sian distribution in regression or a categorical distribution in classification. Under the maximum-
likelihood framework (or equivalently the logarithmic scoring rule), an optimal mapping fφ within
a class F = (fφ)φ∈Φ parameterized by φ ∈ Φ is defined as
fφ ∈ arg max Ep*(χ,y) [logΨ(y∣fφ(X))] ,	(6)
fφ ∈F
where p* (x, y) is the true data generating distribution. When the data are complete, this is typically
approximated by maximizing the log likelihood of the parameters φ given the data Dtrain ,
'(Φ) =	X	log pφ(y∣x).	⑺
(x,y)∈Dtrain
3
Published as a conference paper at ICLR 2022
When the covariates have missing values the log-likelihood cannot be maximized directly as the
likelihood is not defined since pφ(y∣x) depends on the full input vector. The simplest approach
is instead to learn separate mappings for each missing pattern. This reduces the amount of data
available for the training of each mapping and does not scale well with the dimensionality of the
input as in general 2p networks need to be trained. An often used approach is instead to impute the
missing values using a single imputation (for instance, using Ep@ (XmiSS ∣χobs) [xmiss] or an approximation
of it), and then map the concatenation of observed and imputed values to approximate the conditional
distribution
Pφ(y∣χobs) ≈ Pφ(y∣χ = (χobs,旧「°中,产业吗))=Ψ(y∣fφ(χobs,旧「。中,反叫反而力).⑻
Instead of Epθ(χmiss∣χobs)[xmiss], which is the optimal imputation under the mean squared error, one
could use any kind of imputation, for instance using a constant, or the mean.
This general approach is adequately called impute-then-regress by Bertsimas et al. (2021) and Le
Morvan et al. (2021). While it will be Bayes-consistent given a powerful enough classifier (Le Mor-
van et al., 2021), it leads to biased parameter estimates compared to having complete data (Bertsimas
et al., 2021). This is illustrated in figure 2 where the optimal (under the mean squared error) sin-
gle imputations are placed entirely within one of the classes, requiring a biased decision surface
to properly reflect the class label probabilities for the records with missing values. From a prob-
abilistic perspective, computing Pφ (y∣xobs) requires marginalizing over the missing features as in
equation (1), that is
Pφ(y∣xobs ) = Epθ(χmiss∣χobs)[pφ(y∣xobs, XmiSS)].	(9)
Notice the difference to single imputation since, in general
Epθ (Xmiss∣xobs )[pφ(y∣xobs, XmiSS)] = Pφ(y∣x = (xobs , Ep°(xmiss"bs)[xmiss])),	(10)
except in some pathological cases (e.g. when pφ is linear or pθ is a Dirac distribution). Therefore,
if the discriminative model is nonlinear and the generative model is complex enough, then using
single imputation will give a very different result than marginalising over the missing features. This
is exemplified in figure 2, where a classifier trained using the optimal imputation finds a solution
that is very different to what would be found by one trained without missing data. Our technique,
marginalizing the missing values, allows to recover the same decision surface as a classifier trained
without missing data.
2.2	Our joint model and its supMIWAE variational bound
As mentioned in the introduction, we posit a joint model over covariates and outcome, that will
make use of a latent variable z:
Pφ,θ (y, xobs, xmiss, Z)= Pφ(y∣xobs, XmiSS)pθ (xobs, XmiSS∣z)pθ (z).	(11)
Under the MAR assumption, the relevant substitute of the likelihood is the likelihood of the observed
data
Pφ,θ (y, xobs) = / Pφ(y∣xobs, XmiSS)pθ (xobs, XmiSS∣z)pθ (z)dz dxmiss.	(12)
The integral in equation (12) is in all but the simplest cases analytically intractable and direct max-
imum likelihood methods for learning the parameters (θ, φ) cannot be used. In order to learn
the parameters of the joint model, we turn to amortized importance weighted variational infer-
ence, where a lower bound is maximized instead of the log likelihood itself (Burda et al., 2016;
Domke & Sheldon, 2018). It is based on the fact that unbiased estimates of a likelihood can be
turned into lower bounds of the log-likelihood. Indeed, if R(xobs, y) is a random variable such that
E R(xobs,y) = pθ,φ(xobs, y), then E log R(xobs, y) ≤ logE R(xobs, y) = log pθ,φ(xobs, y),
which means that E log R(xobs, y) is a lower bound of the log likelihood. A simple way to find
a suitable R(xobs, y) is using importance sampling from a variational distribution over z. More
specifically, let
R ∕γobs、八 1 XX pφ(y|x°bs, XmiSS)pθ(XObS|zk)pe(Zk)
Rκ (x	, y) = K k=----------qγ质产--------------
(13)
4
Published as a conference paper at ICLR 2022
where qγ (z|xobs) is the variational distribution (learnable proposal) and (xkmiss, zk)k∈{1,...,K}
are i.i.d. samples from pθ(XmiSS∣z)qγ(z∣xobs). Then We have that E(Xmis$队)[Rk(xobs,y)] =
pθ,φ(xobs, y) and E(xmiss,z ) log RK(xobs, y) is a lower bound on the log likelihood. The vari-
ational distribution can be parameterized via a neural netWork With Weights γ ∈ Γ, similarly to
Mattei & Frellsen (2019), Nazabal et al. (2020), and Ipsen et al. (2021).
NoW that We have a Way to bound each per-datum log-likelihood, We can define our supMIWAE
variational bound LK (θ, φ, γ) that is a loWer-bound of the log-likelihood of the observed data
nn
LK(θ,φ,γ) = XE(xkmiss,zk) logRK(Xiobs,yi) ≤ Xlogpφ,θ(yi,Xiobs).	(14)
i=1	i=1
The loWer bound LK (θ, φ, γ) is a Monte Carlo objective that can be maximized instead of
the log likelihood itself, using techniques from stochastic optimization (see e.g. Mohamed
et al., 2020). As detailed in appendix D, it benefits from the usual theoretical advantages
of importance-Weighted variational bounds: LK (θ, φ, γ) converges monotonically to the log-
likelihood Pin=1 log pφ,θ (yi, Xiobs) at speed 1/K. This means that, the larger the K, the closer
our objective Will be to the true likelihood.
Note that the contribution of any fully observed data point (X, y) Will be simply
logpφ(ylx)+E(Zk)log (KkXPP(XIzz)PX(Zk))#，	(15)
K i=1	qγ (zk |X)
Which is just the sum of the discriminative likelihood and the standard IWAE bound of Burda et al.
(2016). An interesting property of our bound is that the discriminative and generative parts can be
trained jointly With a single and statistically sound objective. HoWever, if We Were to encounter a
data set Without missing data, equation (15) means that the training of the tWo parts of the models
Would be decoupled: the discriminative model Would be the same as a standardly trained discrimi-
native model, and the generative model Would be the same as a standard IWAE-trained DLVM.
2.3	Prediction
Once the model has been jointly trained using the supMIWAE bound, it is possible to use it to do pre-
diction With missing data. Indeed self-normalized importance sampling can be used to approximate
the conditional
K
Pφ(y∣xobs) ≈ EwkPφ (y I Xobs, XmiSS),
i=1
(16)
Where
wk
rk
r1 + ... + rK
and rk
PP (Xobs∣zk )pp (zk )
qγ (zk ∣Xobs)
(17)
and (Xkmiss, zk)k∈{1,...,K} are i.i.d. samples fromPP(XmissIz)qγ(zIXobs). Again, this technique mim-
icks multiple imputation: several imputations are generated using the generative model, are then fed
to the classifier, and the predictions of each imputation are averaged.
3	Related work
Marginalizing over missing covariates Different approaches have previously been taken to
marginalize over missing values in joint models over features and labels. In a series of publica-
tions (Ahmad & Tresp, 1993; Tresp et al., 1994; 1995), a closed-form solution using Gaussian Basis
Function netWorks Was used to approximate this marginalization, While keeping the neural discrim-
inative model fixed. Ghahramani & Jordan (1995) took a different approach, modeling the joint
distribution P(y, X) directly as a mixture model, obtaining P(yIXobs) by conditioning on observed
quantities in the joint distribution, thus removing the explicit discriminative neural architecture. In
the context of kernel methods, Dick et al. (2008) proposed to learn a distribution over missing val-
ues by minimising the regularized empirical risk. Smieja et al. (2018) trained a Gaussian mixture
model (GMM) and discriminative model jointly, and in place of any missing values the activation
of the corresponding input neuron Was set to the average activation over the GMM conditioned on
observed values.
5
Published as a conference paper at ICLR 2022
Consistency of single imputation A review of approaches to handling missing data in (non-deep)
supervised learning was given by Josse et al. (2019). Here it was shown that under some assumptions
about the capacity of the learner, constant imputation is asymptotically consistent in the supervised
setting. Le Morvan et al. (2020b) investigated the case of a linear predictor on covariates with
missing data, showing that in the presence of missing, the optimal predictor may not be linear and
how constant imputation of each feature can be optimized with regards to the model loss. The linear
case was further investigated in (Le Morvan et al., 2020a), deriving the analytical form of the optimal
predictor and proposing NeuMiss networks to approximate this. Bertsimas et al. (2021) analyzed
the impute-then-regress situation and noted that while this paradigm is widely used in practice it
incurs a bias in the parameter estimates, but mean imputation is still asymptotically consistent. Le
Morvan et al. (2021) showed that the impute-then-regress approach is asymptotically Bayes optimal
given a learner with large capacity and that this holds for all missing mechanisms, even missing not
at random (Rubin, 1976). However, the choice of imputation function affects the difficulty of the
subsequent regression task, when not in the asymptotic regime. As a consequence they proposed to
learn imputation and regression jointly, where the missing values are handled by a NeuMiss network.
Other approaches for missing covariates Yi et al. (2019) tackled the issue of sparsity, and specif-
ically large variations in sparsity, by introducing sparsity normalization. This addresses the issue of
model output covarying with the sparsity level in the input. Ma et al. (2018; 2019) used a permu-
tation invariant setup to avoid imputing missing data in the input of a variational autoencoder. This
approach can be readily extended to the supervised setting, using the permutation invariant setup as a
modified input layer as we show in appendix A.3. Some tree approaches to regression/classification
can use observations with missing values directly (see e.g. Twala et al., 2008; Chen & Guestrin,
2016; Josse et al., 2019; Gomez-Mendez & Joly, 2021). Sparse coding was used by Caiafa et al.
(2020) to train a dictionary and a classifier simultaneously.
DLVMs and missing values Deep latent variable models have been applied to missing data prob-
lems in the unsupervised setting, focused on inference, single and multiple imputations. Early works
assumed that a DLVM had previously been trained on complete data (Rezende et al., 2014; Mattei
& Frellsen, 2018), but a variety of techniques to handle incomplete training sets was then proposed,
both in M(C)AR (Yoon et al., 2018; Ma et al., 2018; 2019; Ivanov et al., 2019; Mattei & Frellsen,
2019; Li et al., 2019) and MNAR (Ipsen et al., 2021; Ghalebikesabi et al., 2021) cases. In (Li &
Marlin, 2020) irregularly sampled time-series are approached as a missing data problem and they
propose VAE and GAN based models for handling this. The proposed models can also be used for
supervised learning by training a classifier on the latent representations, either by training the model
jointly or using a pretrained encoder. Without missing data, the idea of jointly training a VAE and a
discriminative model, has been explored by Ji et al. (2020) or Joy et al. (2021).
4	Experiments
We now evaluate discriminative models trained using the supMIWAE bound on a range of super-
vised learning tasks. Throughout the experiments the generative part of the model is pretrained using
the MIWAE bound of Mattei & Frellsen (2019). The pretrained DLVM can be used to draw single
imputations, referred to as MIWAE single imputation, or it can be used as the generative part in
the supMIWAE computational structure. Here, the generative part of the model is kept fixed while
updating the discriminative part of the model according to the supMIWAE bound. As the DLVM in
MIWAE single imputations and the supMIWAE are similar, any difference in performance between
the two is attributed to their different strategies for handling missing values. Other strategies for han-
dling missing data, used for comparison in this work, are briefly described in appendix A. Missing
values are introduced in the training, validation and test sets according to the missing mechanism
used in the given experiment. Predictive performance is measured on a test-set with missing values.
In all experiments, there is a computational overhead compared to single imputation of training the
supMIWAE that scales linear with the number of importance samples K, c.f. equation (13).
4.1	2D classification
A qualitative analysis of the bias in the learnt parameters of the discriminative model, due to single
imputation, is set up on simple 2D datasets, see figure 8 of appendix E. The simplicity allows us
6
Published as a conference paper at ICLR 2022
Figure 3: Top row: (except top left) kernel density of the half-moons dataset together with im-
putations from the given model. The supMIWAE does not rely on single imputations, instead it
draws multiple importance samples which are passed through the discriminator to give an impor-
tance weighted prediction. For the supMIWAE multiple imputations at three different values of the
horizontal coordinate are shown and a kernel density of the multiple imputations are shown to the
left. For the rest of the methods single imputations are shown in red. Bottom row: Decision sur-
faces learnt, depending on the strategy for handling missing values. The methods based on single
imputation need to warp the decision boundaries.
to directly inspect the impact of different imputation strategies on the learnt decision surface and
compare this to the marginalization procedure used in the supMIWAE.
All datasets consist of a training set with 3k records and validation and test sets with 1k records.
An MCAR missing process is introduced in the horizontal coordinate, where each element be-
comes missing with probability m = 0.5. Further training details are in appendix E. The datasets
are such that there will be inherent ambiguity about the class label for some of the records with
missing values. For example in the Burger dataset, all records with a missing value will have
p(y= 1|xobs) = 2/3.
In figure 3 imputations from different imputation models are shown along with the corresponding
learnt decision surface. Partially observed examples are shown in the margins of the plots as sticks,
colored by their class label. All methods use the same discriminative architecture, only the im-
putation strategy differs. For the supMIWAE, multiple imputations are shown for three partially
observed records, alongside their kernel density. These multiple imputations reflect the appearance
of importance samples used during training, for these three partially observed records.
Decision surface artifacts The single imputation methods introduce artifacts into the learnt de-
cision surface. As the discriminative model tries to reflect the proper conditional probabilities
p(y|xobs) at the location of the imputed values, the ambiguity about the class label needs to be
reflected in the predictions. Notice how the MIWAE produces nearly optimal imputations in terms
of minimizing a mean-square-error and how this translates into decision surface artifacts. These
artifacts are related to the notion of imputation manifolds used by Le Morvan et al. (2021) to prove
their main theorem on the Bayes consitency of impute-then-regress procedures. The remaining
datasets are shown in figures 10-13 of appendix E, where also the permutation-invariance setup (ap-
pendix A.3) and histogram-based gradient boosting are included. Like the supMIWAE, these two
approaches do not rely on explicit imputations.
As the supMIWAE can draw multiple importance samples, instead of one single imputation, the
conditional probabilities p(y|xobs) can be properly reflected, when averaging over the importance
samples. This avoids the single imputation artefacts.
Predictive performance Do the artifacts introduced by single imputation methods, compared to
marginalizing over the missing values, translate into differences in predictive performance? The
predictive performance corresponding to the decision surfaces in figure 3 are shown in table 1 in ap-
pendix E. Both the test-set accuracies and log likelihoods are more or less similar across the different
strategies for handling missing values. This aligns with the results by Josse et al. (2019); Bertsimas
et al. (2021); Le Morvan et al. (2021): the impute-then-regress procedures with powerful learners
are consistent and Bayes optimal, i.e. when the learner has high capacity any single imputation can
7
Published as a conference paper at ICLR 2022
essentially be undone. While single-imputation methods can lead to Bayes-optimal learners, Le
Morvan et al. (2021) address that the difficulty of the learning problem depends on the choice of
imputation function. This is seen in figure 4; while the supMIWAE and 0-imputation eventually
ends up with similar performance the learning problem appears easier for the supMIWAE. For 0-
imputation the learner has to introduce high-frequency artifacts to the decision surface in order to
get the conditional probabilities p(y|xobs) calibrated, but in the case of a high capacity learner this
can eventually be done.
Limited capacity learner We now turn to a situation
where the capacity of the learner is limited in order
to assess how single imputation methods fare against
marginalizing over missing values. We start by not-
ing that the decision surface for the Burger dataset, in
the complete-data case, can be learnt using two sig-
moids. When using MSE-optimal single imputations in
the missing-values case this changes, as now 4 sigmoids
are needed to make sure the imputations are well cali-
brated. This illustrates that the capacity of the learner
determines whether single imputations can be undone. In
figure 5 we explore how the capacity of the learner, in
terms of the number of hidden units, affects the predic-
tive performance, depending on the imputation strategy.
More capacity experiments are found in figure 14.
Figure 4: Learning curves on the half-
moons dataset: train (full lines) and test
(dashed lines) set accuracies, when the
classifier has full capacity.
4.2	Image Classification
When the learning problems become harder, strong inductive biases are sometimes used to help
learning. This is typically the case with image classification where convolutional neural networks
(CNNs) are often used to introduce translational equivariance. The previous section showed that the
strategy for handling missing data affects predictive performance when the learner is not in the high
capacity regime. This is further investigated in terms of classification accuracy in image experiments
with different MCAR missing processes, such as observed squares, missing squares and random
dropout, See figures 15-17. The discriminator is a convolutional neural network, See appendix F.1,
thus introducing an inductive bias in the learner that has proved useful in image classification in the
complete-data case. While CNNs are fit for images, it should be noted that they are not universal
approximators, and may consequently have a hard time undoing single imputations.
We apply the supMIWAE and single imputations methods to the MNIST dataset (LeCun et al., 1998)
and the fashion MNIST dataset (Xiao et al., 2017) with three different MCAR missing mechanisms:
1) observed squares, where a 12 × 12 randomly placed square is observed, 2) missing squares, where
a 12 × 12 randomly placed square is missing, and 3) dropout, where each pixel is missing with some
constant probability m, over a range of missing rates.
In practice, we see that this setting where there is a strong inductive bias is similar to our previous
low-capacity experiments: for both MNIST and Fashion MNIST, the supMIWAE is significantly
20864208
.8.8.7.7.7.7,7S
0.0.0.0.0.0,0,0.
OJE⅛Φ+J
(X-A) dbozSE
—supMIWAE
--MIWAE
■■■—■ 0-impute
—learnable-imputation
-MICE
—missForest
-- PPCA
Figure 5: Predictive performance when varying the capacity of the learner in terms of number of hid-
den units. The discriminative model has the same architecture across methods, only the imputation
strategy differs.
8
Published as a conference paper at ICLR 2022
missing rate
supMIWAE
Ml MIWAE
O-impute
learnable-imputation
supMIWAE
MIWAE
O-impute
learnable-imputation
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
Figure 6: Test set accuracies on the MNIST and Fashion MNIST datasets, with different missing
mechanisms. Left column: observed squares. Middle column: missing squares. Right column:
random dropout over a range of missing rates.
-→- supMIWAE
--MIW4E
O-impute
—learnable-imputation
-MICE
—InissForest
→- PPCA
--GB
permutation-invariance
Figure 7: Test-set root mean square error on UCI datasets at varying missing rates.
more accurate than single-imputation competitors, as seen in figure 6. Additional experiments,
including experiments on natural images (SVHN) and visualizations of multiple imputations, are
available in appendix F.
4.3	Regression
We now turn to regression in smaller and lower dimensional datasets from the UCI database (Dua
& Graff, 2017). An experimental setup modified from HemandeZ-Lobato & Adams (2015) and
Skafte et al. (2019) is used here. Predictive performance in terms of root-mean-square-error is
presented over a range of missing rates. Results are seen in figure 7, and more results can be
found in figure 20 of appendix G. There is no clear winner here, although it appears that gradient
boosting generally performs quite well, and that accurate single imputations (such as MIWAE’s or
MissForest’s) generally outperform inaccurate ones (such as Zero imputation). In general, our joint
model performs as well as feeding accurate single imputations.
5	Conclusion
In the context of supervised deep learning with missing values, single imputation methods are an
often used tool. While this approach is asymptotically consistent it incurs a bias in the learnt pa-
rameters. We have introduced a scalable approach to marginaliZing over missing values using deep
generative models, as a probabilistic alternative to single imputation. Besides limiting single impu-
tation artifacts our results indicate that there are two cases where using a generative model can be
quite valuable for supervised learning with missing values: when the classifier is not very flexible,
or has a strong inductive bias. Moreover, leveraging such a generative model allows us to be able to
use untouched any discriminative architecture (for instance, any pretrained one).
9
Published as a conference paper at ICLR 2022
Reproducibility Statement
Code for reproducing paper experiments is available at https://github.com/nbip/
supMIWAE.
Acknowledgements
This work has been supported by the French government, through the 3IA Cote d'Azur Invest-
ments in the Future project managed by the National Research Agency (ANR) with the refer-
ence number ANR-19-P3IA-0002. Furthermore, it was supported by the Novo Nordisk Foundation
(NNF20OC0062606 and NNF20OC0065611) and the Independent Research Fund Denmark (9131-
00082B).
References
Subutai Ahmad and Volker Tresp. Some solutions to the missing feature problem in vision. In
Advances in Neural Information Processing Systems, pp. 393-400, 1993.
Dimitris Bertsimas, Arthur Delarue, and Jean Pauphilet. Prediction with missing data. arXiv preprint
arXiv:2104.03158, 2021.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
International Conference on Learning Representations, 2016.
S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations
in R. Journal of statistical software, pp. 1-68, 2010.
Cesar F Caiafa, Ziyao Wang, Jordi Sole-Casals, and Qibin Zhao. Learning from incomplete data by
simultaneous training of neural networks and sparse coding. arXiv preprint arXiv:2011.14047,
2020.
Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
the 22nd acm SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
785-794, 2016.
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International conference on machine learning, pp. 933-941. PMLR,
2017.
Uwe Dick, Peter Haider, and Tobias Scheffer. Learning from incomplete data with infinite impu-
tations. In Proceedings of the 25th international conference on Machine learning, pp. 232-239,
2008.
Justin Domke and Daniel R Sheldon. Importance weighting and variational inference. In Advances
in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of
statistics, pp. 1189-1232, 2001.
Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.
Bayesian data analysis. CRC press, 2013.
Zoubin Ghahramani and Geoffrey E Hinton. The EM algorithm for mixtures of factor analyzers.
Technical report, CRG-TR-96-1, University of Toronto, 1996.
Zoubin Ghahramani and Michael I Jordan. Learning from incomplete data. MIT Center for Biolog-
ical and Computational Learning Technical Report 108, 1995.
Sahra Ghalebikesabi, Rob Cornish, Chris Holmes, and Luke Kelly. Deep generative pattern-set
mixture models. In Proceedings of The 24th International Conference on Artificial Intelligence
and Statistics, pp. 3727-3735, 2021.
10
Published as a conference paper at ICLR 2022
Irving Gomez-Mendez and Emilien Joly. Regression with missing data, a comparison study of
techniques based on random forests. Preprint, available at https://irvinggomez.com/
publication, 2021.
Jose MigUel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861—
1869, 2015.
Niels Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen. not-MIWAE: Deep generative mod-
elling with missing not at random data. In International Conference on Learning Representations,
2021.
Oleg Ivanov, Michael Figurnov, and Dmitry Vetrov. Variational autoencoder with arbitrary condi-
tioning. In International Conference on Learning Representations, 2019.
Tianchen Ji, Sri Theja Vuppala, Girish Chowdhary, and Katherine Driggs-Campbell. Multi-modal
anomaly detection for unstructured and uncertain environments. In Conference on Robot Learn-
ing, 2020.
Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast infer-
ence. Advances in Neural Information Processing Systems, 29:2946-2954, 2016.
Julie Josse, Nicolas Prost, Erwan Scornet, and Gael Varoquaux. On the consistency of supervised
learning with missing values. arXiv preprint arXiv:1902.06931, 2019.
Tom Joy, Sebastian Schmon, Philip Torr, N Siddharth, and Tom Rainforth. Capturing label charac-
teristics in VAEs. In International Conference on Learning Representations, 2021.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations, 2014.
Marine Le Morvan, Julie Josse, Thomas Moreau, Erwan Scornet, and Gael Varoquaux. Neumiss
networks: differentiable programming for supervised learning with missing values. arXiv preprint
arXiv:2007.01627, 2020a.
Marine Le Morvan, Nicolas Prost, Julie Josse, Erwan Scornet, and Gael Varoquaux. Linear predictor
on linearly-generated data with missing values: non consistency and solutions. In International
Conference on Artificial Intelligence and Statistics, pp. 3165-3174, 2020b.
Marine Le Morvan, Julie Josse, Erwan Scornet, and Gael Varoquaux. What,s a good imputation to
predict with missing values? arXiv preprint arXiv:2106.00311, 2021.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Steven Cheng-Xian Li and Benjamin Marlin. Learning from irregularly-sampled time series: A
missing data perspective. In International Conference on Machine Learning, pp. 5937-5946,
2020.
Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. MisGAN: Learning from incomplete data
with generative adversarial networks. In International Conference on Learning Representations,
2019.
Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John
Wiley & Sons, 2019.
Gabriel Loaiza-Ganem and John P Cunningham. The continuous Bernoulli: fixing a pervasive error
in variational autoencoders. Advances in Neural Information Processing Systems, 2019.
Chao Ma, Wenbo Gong, Jose Miguel Hernandez-Lobato, Noam Koenigstein, Sebastian Nowozin,
and Cheng Zhang. Partial VAE for hybrid recommender system. In 3rd NeurIPS Workshop on
Bayesian Deep Learning, 2018.
11
Published as a conference paper at ICLR 2022
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian
Nowozin, and Cheng Zhang. EDDI: Efficient dynamic discovery of high-value information with
partial VAE. In International Conference on Machine Learning,pp. 4234-4243, 2019.
Pierre-Alexandre Mattei and Jes Frellsen. Leveraging the exact likelihood of deep latent variable
models. In Advances in Neural Information Processing Systems, 2018.
Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of
incomplete data sets. In International Conference on Machine Learning, pp. 4413-4423, 2019.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte Carlo gradient
estimation in machine learning. Journal of Machine Learning Research, 21(132):1-62, 2020.
Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete
heterogeneous data using VAEs. Pattern Recognition, 107:107501, 2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3D classification and segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 652-660, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, 2014.
Sam Roweis. EM algorithms for PCA and SPCA. Advances in Neural Information Processing
Systems, pp. 626-632, 1998.
Donald B Rubin. Inference and missing data. Biometrika, 63(3):581-592, 1976.
Donald B Rubin. Multiple imputation after 18+ years. Journal of the American Statistical Associa-
tion, 91(434):473-489, 1996.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the
pixelCNN with discretized logistic mixture likelihood and other modifications. In International
Conference on Learning Representations, 2017.
Shaun Seaman, John Galati, Dan Jackson, and John Carlin. What is meant by ”missing at random”?
Statistical Science, pp. 257-268, 2013.
Nicki Skafte, Martin J0rgensen, and S0ren Hauberg. Reliable training and estimation of variance
networks. In Advances in Neural Information Processing Systems, pp. 6326-6336, 2019.
Marek SmieJa, Eukasz Struski, Jacek Tabor, Bartosz Zielinski, and PrzemysIaW Spurek. Processing
of missing data by neural networks. In Advances in Neural Information Processing Systems, pp.
2719-2729, 2018.
Daniel J Stekhoven and Peter Buhlmann. Missforest-non-parametric missing value imputation for
mixed-type data. Bioinformatics, 28(1):112-118, 2012.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
12
Published as a conference paper at ICLR 2022
Volker Tresp, Subutai Ahmad, and Ralph Neuneier. Training neural networks with deficient data. In
Advances in Neural Information Processing Systems, pp. 128-135, 1994.
Volker Tresp, Ralph Neuneier, and Subutai Ahmad. Efficient methods for dealing with missing data
in supervised learning. In Advances in Neural Information Processing Systems, pp. 689-696,
1995.
Bheki E T H Twala, M C Jones, and David J Hand. Good methods for coping with missing data in
decision trees. Pattern Recognition Letters, 29(7):950-956, 2008.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, and Eunho Yang. Why not
to use zero imputation? correcting sparsity bias in training neural networks. arXiv preprint
arXiv:1906.00150, 2019.
Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. GAIN: missing data imputation using
generative adversarial nets. In International Conference on Machine Learning, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3391-3401, 2017.
13
Published as a conference paper at ICLR 2022
A	Training deep supervised models with missing data
In the following we group different strategies for handling missing data in supervised deep learning.
A. 1 Constant imputation
We denote imputation methods where there is a single constant for each feature which replaces
missing data as constant imputation. We define an imputation function ∣ in general such that ∣(X) ∈
X and ι(x)obs = xobs. A simple version of constant imputation is 0-imputation, which has the
intuitive appeal that the activation from the input node is zeroed out (absent) and no gradient updates
are made for the weights for that feature,
∣o(X) = X Θ S + 0 Θ (1 一 s).	(18)
In the unsupervised setting, constant imputation biases marginal and joint distributions, but Josse
et al. (2019); Le Morvan et al. (2021); Bertsimas et al. (2021) have shown that mean imputation can
be consistent in the supervised setting. Furthermore, Le Morvan et al. (2020b); Tresp et al. (1994)
noted that the constants can be optimized with respect to the prediction loss.
Learnable parameters λ ∈ X can be inserted in place of the missing data, so that
∣λ(X) = X Θ s + λ Θ (1 ― s).	(19)
We denote this learnable imputation.
A.2 Regressive/Model Based single imputation
In contrast to constant imputation, methods such as MICE (Buuren & Groothuis-Oudshoorn, 2010),
MissForest (Stekhoven & Buhlmann, 2012), PPCA (Tipping & BiShOP,1999; ROWeiS,1998) or flex-
ible generative models (Mattei & Frellsen, 2019; Yoon et al., 2018) provides imputation of missing
features conditional on observed features p(Xmiss|Xobs),
ι(X) = X Θ s + Ep(Xmiss∣χobs) [x] Θ (1 — S)	(20)
In MICE, this conditional distribution comes from an implicit joint model over the covariates by
regressing each covariate with missing values on all the others, in a round-robin fashion. MissForest
is somewhat similar to MICE, using random forests as the regressors. Probabilistic PCA defines an
explicit joint model over all covariates and single imputations of missing data can be obtained by
conditioning on observed covariates. Here, MICE is a proper Bayesian multiple imputation method,
while PPCA and MissForest draws multiple imputations without regard to parameter uncertainties.
The recent advances in imputation of missing data using deep generative models can also be used
here to the extent that they impose a joint distribution over covariates.
A.3 Permutation invariance
Ma et al. (2018; 2019) used the permutation invariant setup (Zaheer et al., 2017; Qi et al., 2017)
to handle missing data in the context of variational autoencoders. For each feature j ∈ obs(s),
there is an embedding ej ∈ RU, which in general can hold any auxiliary information, but in our
case will be a learnable embedding. Each element of the embedding is multiplied with the feature
value tj = ej ∙ Xj and fed to a neural network h(∙), mapping inputs from RU to RM where U
is the embedding dimension and M is the dimension of the code-space being mapped into. This
permutation invariant setup finally aggregates all the neural network outputs by a summation to get
one fixed-length vector, that can be fed into a discriminative network,
ιPI(Xobs) = X h(tj).	(21)
j∈obs(s)
Note that in this case ∣pι(xobs) ∈ RM, while in the other imputation functions ∣(X) ∈ X.
14
Published as a conference paper at ICLR 2022
A.4 Tree methods
Decision trees can in some cases handle missing data directly, without the need for imputation.
There are different approaches to handle missing values in tree based methods, see (Josse et al.,
2019) for a review. In our experiments we use histogram-based gradient boosting (Friedman, 2001),
as implemented in scikit-learn (Pedregosa et al., 2011). This approach uses the Block Propagation
method for the missing data. While a discriminative model using MLPs has a fixed capacity, gradient
boosting methods can adapt its capacity in terms of tree depth based on a validation set error.
B Gradients
The log likelihood, when integrating over missing input features, is specified as
log pφ,θ (y, Xobs) = log Pφ(y∣χ0bs) + log pθ (Xobs)	(22)
where
log pφ(y ∣xobs) = log / pφ(y∣xobs, Xmiss )pθ (Xmiss ∣xobs) dxmiss.	(23)
Both Tresp et al. (1994) and Ghahramani & Jordan (1995) analyzed the gradient of the log likelihood
with respect to the discriminative network parameters,
∂ logPφ,θ(y, Xobs) = ∂ logPφ(y∣xobs)	∂ logpθ(Xobs)
∂φ	=	∂φ	+	∂φ	( 4
=_	1	dP6(y| Xobs)	(25)
Pφ (y∣xobs)	∂φ
=-p;(y⅛ ∂φ Z Pφ(y∣xobs, xmiss)pθ (xmiss∣xobs)dxmiss	(264
1	/ dPφ(y∣xobs,xmiss). issi) d"ss	T)
=-^X^J	∂φ	pθ(x	|x )dx	(27)
Here we see that the gradient of the discriminative network parameters, due to the classifica-
tion/regression loss, is a weighted sum over all possible missing values, where the missing values
are weighted according to their density in the data model, conditional on the observed data.
The gradient with respect to the θ parameters depends to some extend on the φ parameters.
∂ logPφ,θ(y, Xobs) = ∂ logPφ(y∣xobs)	∂ logPθ(Xobs)
∂θ	=	∂θ	+	∂θ
=-	1	∂pφ (y∣xobs) + ∂ log Pθ (Xobs)
Pφ (y∣xobs)	∂θ	∂θ
(28)
(29)
pφ(y⅛ ∂θ Z Pφ(y∣xobs, xmiss)pθ (xmiss∣xobs)dxmiss +d log ∂θ(Xobs)
(30)
1 d E	L (7,%obs χmiss)i I d logPθ(xobs)	(31)
pφ(y∣xobs) ∂θEpθ(XMIxobS) [pφ(ylx , X	)] +	∂θ	(31)
(32)
There are contributions to the gradient from both the generative part of the model and the discrim-
inative part. This means that, if needed, the discriminative model can affect the generative model,
as is the case in (Ipsen et al., 2021). The gradient of the expectation can be approached using the
reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014).
C Relation to Multiple Imputation
The gold standard for imputation is multiple imputation (Rubin, 1996). This requires learning a
model of the observed covariates xobs with parameters θ. K sets of imputations are then drawn from
15
Published as a conference paper at ICLR 2022
the posterior predictive distribution
Xmiss 〜P(Xmiss∣xobs), k =1,...,K	(33)
where P(XmIss∣xobs) = RP(Xmiss∣θ)p(θ∣xobs) dθ if the model is Bayesian (see e.g. Gelman et al.,
2013, chapter 18.2). Several frequentist multiple imputation methods also exist, where the uncer-
tainty with respect to the imputation model parameters is disregarded. In that case, the Bayesian
posterior predictive is replaced by a conditional distribution P(Xmiss∣xobs) = pΘ(xmiss|xobs), where
θ is any point estimate of θ.
In the case of regression/classification, if we roughly follow the notations of our paper, the analysis
proceeds as follows
• For each set of imputed values (Ximkiss)i≤n,k≤K, use a learning algorithm to estimate the
discriminative parameters φ1, . . . , φK:
n
φ 1 = arg max X logpφ(y八Xobs, Xmiss)	(34)
φ i=1
n
Φk = argmax X log Pφ(y∕ Xobs, Xmss)	(35)
φ	i=1
• Obtain an average estimate of φ using
1K
φκ = K X φk	(36)
k=1
and a variance estimate can similarly be obtained as described in (Gelman et al., 2013;
Rubin, 1996).
In the limit of infinite imputations, we can write this as the conditional expectation
n
φ = ExmiSS,…,xniss argmax X log Pφ (yi∣Xobs, Xmiss) XIbs,..., X;s .	(37)
φ i=1
Instead of fitting K conditional models to K imputed datasets, our approach (when θ is fixed) is, in
comparison, to directly maximize the expectation of the conditional distribution with respect to the
missing data
n
φ = arg max X log ExmiSSIpφ(yi∣Xobs, Xmiss) |Xobs].
φ	i=1
(38)
Also note that we have no need for several fixed sets of imputations, at each gradient step anew set of
samples are drawn from the imputation model, Xmiss 〜pθ (Xmiss ∣Xobs). The number of imputations
can be set dynamically, using 〜5-50 during training and 〜10k for testing. Our approach, using
DLVMs as imputation models, is not Bayesian, but any available proper Bayesian imputation model
could be used in place of the DLVMs.
In practice, an important difference between traditional multiple imputation and our approach, is
that multiple imputation uses a uniform weight 1/K for each imputation, while we use importance
weights, that will treat each imputation based on its likelihood.
D Theoretical properties of the supMIWAE bound
We will briefly see how the supMIWAE bound has the same theoretical properties as IWAE bounds.
We follow Ipsen et al. (2021, appendix D), who were following Burda et al. (2016) and Domke &
Sheldon (2018). Recall the definition of the bound: we have
R ∕γobs、八 1 X pφ(y|X°bs, Xmiss)pθ(XObs1zQpθ(Zk)
RK(X , y) = K k=----------------qγ⅛obs)---------------
(39)
16
Published as a conference paper at ICLR 2022
Half-moons Circles	Burger Pin-wheels
Figure 8: 2D datasets with two or more classes.
and
n
LK(θ,φ,γ) = XE(xkmiss,zk) logRK(xiobs,yi) .	(40)
i=1
The ith term of the sum can be seen as a regular IWAE bound with observation (xobs, y), latent
variable (z, Xmiss) With prior pθ(Xmiss∣z)pe(z), and variational distribution pθ(Xmiss∣z)q,(z∣xobs).
Therefore, Theorem 1 from Burda et al. (2016) can be applied, and leads to the monotonicity of the
bound: L1(θ, φ, γ) ≤ . . . ≤ LK(θ,φ,γ).
To shoW that the bound additionally converges to the log-likelihood When K -→ ∞, We use Theo-
rem 3 of Domke & Sheldon (2018) for all n terms of the sum that constitutes the bound.
Theorem. Assuming that, for all i ∈ {1, ..., n},
•	there exists a > 0 such that E IjRI(X产,y. 一 pθ,φ(xθbs, yi)∣2+αi] < ∞,
•	lim supK-→∞ E 1/RK < ∞,
the supMIWAE bound converges to the true joint likelihood at rate 1/K:
XX logPφ,θ(yi, xθbs) -Lk(θ, φ, Y) K〜⅛ XX Var[R1(XbbS, y；2].	(41)
i=1	i 2	K→∞ K i=1 2pθ,φ(Xiobs, yi)2
E 2D Classification
The datasets used in the 2D classification experiments are shoWn in figure 8. The Half-moons and
Circles datasets are from scikit-learn (Pedregosa et al., 2011). The Pin-Wheel dataset is modified
from Johnson et al. (2016)1. The Burger dataset, We have designed to illustrate the case Where
optimal single imputation requires severe changes in the decision surface.
The discriminative model is an MLP With 3 hidden layers With 50 hidden units each. The final
layer parameterizes a categorical distribution over the dataset classes. The generative model
used for the MIWAE single imputations and the supMIWAE consists of an encoder and decoder
With 3 hidden layers containing 50 hidden units each. The encoder parameterizes an isotropic
Gaussian distribution over a latent space With dimension p, same as the feature space. The decoder
parameterizes an isotropic Gaussian distribution over the feature space. In figure 9 samples from
the generative model are shoWn, that is, samples from the MIWAE trained on each of the datasets.
In (Smieja et al., 2018) a Gaussian Mixture Model, GMM, is used to model the density of the
covariates While jointly training a discriminator. In case of missing values, the discriminator’s first
input neuron’s activations are set to the average activations as found over a conditional distribution
over the missing values in the GMM. We adapt the official code 2 to this experiment by adjusting
the number of hidden layers and number of mixture components While keeping the original training
setup as is.
1https://github.com/mattjj/svae
2https://github.com/lstruski/Processing-of-missing-data-by-neural-netWorks
17
Published as a conference paper at ICLR 2022
In figures 10-13 decision surfaces and imputations are shown. The corresponding quantitative re-
sults are in tables 1T.
Figure 9: Samples from the MIWAE, fitted to the data with missing values on the four 2D datasets.
The class color is determined by afterwards sampling from the discriminative distribution.
18
Published as a conference paper at ICLR 2022
Table 1: Half-moons test-set results
model	test acc	- on missing	- on observed	test log p(y|x)	- on missing	- on observed
supMIWAE	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
MIWAE	0.88 ± 0.00	0.77 ± 0.01	0.99 ± 0.00	-0.42 ± 0.01	-0.52 ± 0.01	-0.33 ± 0.01
0-impute	0.88 ± 0.01	0.77 ± 0.02	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
learnable-imputation	0.89 ± 0.01	0.78 ± 0.01	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
MICE	0.89 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
missForest	0.87 ± 0.00	0.75 ± 0.01	0.99 ± 0.01	-0.45 ± 0.00	-0.53 ± 0.01	-0.37 ± 0.00
PPCA	0.88 ± 0.01	0.77 ± 0.02	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
GB	0.88 ± 0.01	0.76 ± 0.02	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.00
permutation-invariance	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.42 ± 0.00	-0.52 ± 0.01	-0.32 ± 0.01
(SmiejaetaL,2018)	0.83 ± 0.04					
Table 2: Burger test-set results
model	test acc	- on missing	- on observed	test log p(y|x)	- on missing	- on observed
supMIWAE	0.81 ± 0.01	0.64 ± 0.03	0.97 ± 0.01	-0.51 ± 0.00	-0.66 ± 0.01	-0.35 ± 0.01
MIWAE	0.78 ± 0.01	0.63 ± 0.03	0.93 ± 0.02	-0.52 ± 0.01	-0.67 ± 0.01	-0.38 ± 0.01
0-impute	0.80 ± 0.01	0.64 ± 0.03	0.96 ± 0.01	-0.51 ± 0.00	-0.66 ± 0.01	-0.35 ± 0.01
learnable-imputation	0.81 ± 0.01	0.64 ± 0.03	0.97 ± 0.01	-0.51 ± 0.00	-0.66 ± 0.01	-0.35 ± 0.01
MICE	0.80 ± 0.01	0.64 ± 0.03	0.96 ± 0.01	-0.51 ± 0.00	-0.66 ± 0.01	-0.35 ± 0.01
missForest	0.73 ± 0.02	0.54 ± 0.03	0.93 ± 0.02	-0.58 ± 0.01	-0.69 ± 0.02	-0.47 ± 0.02
PPCA	0.80 ± 0.01	0.64 ± 0.03	0.96 ± 0.01	-0.51 ± 0.01	-0.66 ± 0.01	-0.35 ± 0.01
GB	0.80 ± 0.01	0.63 ± 0.03	0.97 ± 0.01	-0.51 ± 0.01	-0.66 ± 0.01	-0.35 ± 0.01
permutation-invariance	0.81 ± 0.01	0.64 ± 0.03	0.97 ± 0.01	-0.51 ± 0.00	-0.66 ± 0.01	-0.35 ± 0.01
(SmiejaetaL,2018)	0.79 ± 0.06					
Table 3: Circles test-set results
model	test acc	- on missing	- on observed	test log p(y|x)	- on missing	- on observed
supMIWAE	0.88 ± 0.01	0.77 ± 0.01	0.99 ± 0.01	-0.44 ± 0.01	-0.56 ± 0.01	-0.32 ± 0.01
MIWAE	0.87 ± 0.01	0.76 ± 0.02	0.99 ± 0.01	-0.45 ± 0.01	-0.56 ± 0.01	-0.33 ± 0.00
0-impute	0.88 ± 0.01	0.77 ± 0.02	0.99 ± 0.00	-0.44 ± 0.01	-0.56 ± 0.00	-0.33 ± 0.00
learnable-imputation	0.88 ± 0.01	0.77 ± 0.02	0.99 ± 0.01	-0.44 ± 0.00	-0.55 ± 0.01	-0.33 ± 0.01
MICE	0.88 ± 0.01	0.77 ± 0.02	0.99 ± 0.01	-0.44 ± 0.01	-0.56 ± 0.01	-0.33 ± 0.00
missForest	0.86 ± 0.01	0.75 ± 0.01	0.98 ± 0.01	-0.48 ± 0.01	-0.56 ± 0.01	-0.39 ± 0.01
PPCA	0.88 ± 0.01	0.77 ± 0.02	0.99 ± 0.00	-0.44 ± 0.00	-0.56 ± 0.01	-0.33 ± 0.00
GB	0.87 ± 0.01	0.76 ± 0.01	0.99 ± 0.00	-0.44 ± 0.01	-0.56 ± 0.01	-0.33 ± 0.00
permutation-invariance	0.88 ± 0.01	0.77 ± 0.02	0.99 ± 0.00	-0.44 ± 0.00	-0.56 ± 0.01	-0.32 ± 0.00
(SmiejaetaL,2018)	0.86 ± 0.02					
Table 4: Pin-wheel test-set results
model	test acc	- on missing	- on observed	test log p(y|x)	- on missing	- on observed
supMIWAE	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.89 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
MIWAE	0.88 ± 0.01	0.76 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.01 ± 0.01	-0.75 ± 0.00
0-impute	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.00	-0.75 ± 0.00
learnable-imputation	0.88 ± 0.00	0.77 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
MICE	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
missForest	0.87 ± 0.00	0.75 ± 0.00	1.00 ± 0.00	-0.90 ± 0.01	-1.01 ± 0.00	-0.80 ± 0.00
PPCA	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
GB	0.87 ± 0.01	0.74 ± 0.02	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
permutation-invariance	0.88 ± 0.01	0.77 ± 0.01	1.00 ± 0.00	-0.88 ± 0.01	-1.02 ± 0.01	-0.75 ± 0.00
19
Published as a conference paper at ICLR 2022

Figure 10: First and third rows: (except top left) kernel density of the half-moons dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values.
20
Published as a conference paper at ICLR 2022
Figure 11: First and third rows: (except top left) kernel density of the burger dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values.
21
Published as a conference paper at ICLR 2022
Figure 12: First and third rows: (except top left) kernel density of the circles dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values.
22
Published as a conference paper at ICLR 2022
Figure 13: First and third rows: (except top left) kernel density of the pin-wheel dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown
to the left. Permutation invariance and gradient boosting does not rely on explicit imputations, for
the rest of the methods single imputations are shown in red. Second and fourth rows: Decision
surfaces learnt, depending on the strategy for handling missing values.
23
Published as a conference paper at ICLR 2022
E.1 Capacity
The predictive performance of the discriminator
units, is shown in figure 14.
at different capacities, in terms of number of hidden
20864208
8877.7.7.76
0.0.0.0.SS0.0.
oɔp+-ɔsəa
(X-X) d60-⅛B
0 8 6 4 2 0 8
,98 8 8 8 8 7
0.0.0.0.0.0.0.
ɔɔE⅛8-l-J
υ 0.86
E
ω 0.84
ω
-0.875
-0.900
-0.925
-0.950
-0.975
-1.000
-1.025
-1.050
-1.075
pin-wheel
23456789 10
capacity
—supMIWAE
--∙- MIWAE
∙∙∙*∙∙∙ 0-impute
—learnable-imputation
—MICE
—missForest
→- PPCA
Figure 14: Predictive performance when varying
the capacity of the learner, in terms of hidden units.
24
Published as a conference paper at ICLR 2022
F Image Classification
F.1 TRAINING DETAILS FOR MNIST/FMNIST
For all MNIST/fMNIST experiments, the discriminative model is a convolutional neural network
with four layers and a categorical distribution over classes, see table 5. The generative model uses
a convolutional architecture similar to the DCGAN (Radford et al., 2015) and a latent space of
dimension 128. A Continuous Bernoulli (Loaiza-Ganem & Cunningham, 2019) observation model
is used for MNIST and the discretized logistic distribution (Salimans et al., 2017) is used as the
observation model for Fashion MNIST. During training, K = 5 importance samples and a batch
size of 128 are used.
Table 5: Discriminative network for MNIST/fMNIST classification.
Action (resulting layer size)
Input X (28 X 28 X 1)
Conv2D(14 × 14 × 16)
Conv2D(7 X 7 X 32)
Conv2D(3 X 3 X 64)
Reshape(576)
Class probabilities: Dense(10)
F.2 Imputations
Figures 15-17 show imputations on the two image datasets for the three different MCAR missing
processes. The first column contains data with missing values. The second column is single im-
putation by the MIWAE, and following columns are MIWAE multiple imputations using sampling-
importance-resampling.
F.3 NATURAL IMAGES (SVHN)
In order to assess classification of natural images we now turn to the Street View House Numbers
dataset (SVHN, Netzer et al., 2011), with different missing mechanisms. In the “observed squares”
missing mechanism 20 X 20 randomly placed squares are observed while in the “missing squares”
missing mechanism 16 X 16 randomly placed squares are missing. In the “random dropout” missing
mechanism pixels are missing with probability m = 0.5.
In the “observed squares” experiment we apply both a low capacity 4 layer discriminator (1 repeti-
tion) and a higher capacity 8 layer discriminator (3 repetitions) using the convolutional gated blocks
from Dauphin et al. (2017), see architectures in tables 7 and 8. In table 6, test set accuracies are
shown and for the 8 layer discriminator, the accuracy on the fully observed test set is also shown.
In the low capacity regime, the supMIWAE outperforms the two single imputation methods and
performs on par with MIWAE. With the higher capacity discriminator supMIWAE has performance
similar to 0-imputation and learnable imputation, while the MIWAE imputation is slightly worse.
This displays the same phenomenon as in the 2D classification experiments, where powerful learn-
ers are able to undo single imputations. While the discriminative models learnt based on constant
imputation can achieve the same performance as the supMIWAE, the kinds of discriminative models
that are learnt are different. In this case, when applied to a fully observed test-set the discriminator
learnt by the supMIWAE outperforms the other models.
The high capacity discriminator is also applied to the “missing squares” and “random dropout”
missing mechanisms, see figure 18 for the test-set accuracies (3 repetitions). In figure 19 the models
trained on data with missing values are applied to a fully observed test-set.
25
Published as a conference paper at ICLR 2022
7]■.④.⅛.
4ad
■防B⅛
/膜d'∙:
⅛¾ ⅛
¾⅛E H
.⅛ d
≡a≡
仍0-.寸
>■■wæ
e/; 'd-'d ：H d∙d--d
⅛木圭.匕长æ,æ ⅛ ⅛ æ
4⅛ ◎. ◎.牛.弓卓士 4 .⅛⅜
HvilBBltlBSS
BfldBdedBdfIdBdBdBdIIM
血断窗 toa ⅛ ⅛ a- ® ft ft ft
⅛i∣ Bftftliliftftliftft
j≤!⅛- ðf= QC--.,昌,工 QC--，^―	“ θb^ ->
夕例例歹例 例例宓伤或创
Q◎④戊&@00W。④
,瞅4 6 年年叨瞅t R t⅛
⅛ ⅛自⅛夕回亨售由V
届灿：加勒髭筋承法匾力
士士先 t h fc,-fc⅛.⅛-⅛
韦密 ^¾-.¾, ¾ ¾-¾-¾r'¾-.
/ RaIa(⑷国 ⑷ 因.4
7 N彳？，77，子［I?
3 9 &a后0。⅛色小;9
tt '⅛ ⅛ ⅛ ∙⅛'∙' ⅛I'' fe" ⅛∙ff-⅛,∙∙⅛
,I露工臬$$露ar露露
π*A∙β*BABKAA
'ʌ fi⅛⅛tΦβ⅛eβt⅛
■，・魂鼻茎争金,*.*YW
.∙t∙vttt∙t∙t
(a) MNIST, Continuous Bernoulli.
(b) Fashion, discretized logistic.
%南向勒酒盟循循
F
Figure 15:	Observed squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling.
Table 6: SVHN observed squares classification accuracies
model	acc (incomplete test set)		acc (complete test set)
	4 layers	8 layers	8 layers
supMIWAE	0.8264	0.8795 ± 0.0027	0.9268 ± 0.0015
MIWAE	0.8233	0.8745 ± 0.0024	0.9209 ± 0.0010
0-impute	0.8155	0.8819 ± 0.0024	0.9059 ± 0.0064
learnable-imputation	0.8139	0.8798 ± 0.0042	0.8953 ± 0.0234
G Regression
All discriminative models are neural networks with one hidden layer and 50 hidden units, where
the final layer of the neural networks parameterizes a Gaussian distribution. The datasets are split
randomly 20 times with 90% of the data in a training set and 10% in a test set. A validation set
with 10% of the training data is used for early stopping and a batch size of 256 is used. For the
permutation invariant setup, an embedding of dimension U = 20 and fixed length vectors ιPI(xobs)
of dimension M = 10 are used. Missing values are introduced completely at random in the training,
validation and test sets by removing each feature with probability m, where m is the missing rate.
In order to avoid reducing the sample size, due to completely missing covariates, whenever all the
covariates of an observation go missing, we set one of them to observed, selected at random.
26
Published as a conference paper at ICLR 2022
■77话话话曷话葫话话话
二乙a2a®乙a祖祖a区
3④④®®®④®®
⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛
Zddddddddddd
⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛
(⅛βt电牡牡酎牡叱牡无私无
JSWd名名名名名名号£
I^TL ∣^l∣ Iol UΞ1 1^1 liΞ⅝ ∣^⅜ 施，1^1 IiBl ∣Ξ1
MWfflfflfflfflIIIfflfflfflfflffl
mmmm
p⅛⅛⅛⅛⅛⅛⅛⅜⅛⅛⅛
(IrflMMMeflMMC¢1MMrjl
∣f⅛ ft ft ft ft ½ ft ½ 他 ft ft ft
途 蠢-⅜j⅛- ⅜j⅛-蠹-⅝j⅛- ⅝j⅛, ⅝j⅛- ⅝j⅛, ⅝j⅛, ⅜j⅛, ⅜j⅛,
1∣Ziψ ∣⅛^- .p^⅛- -pr^⅛- pr^⅛- -pr^⅛- -pr^⅛- -pr^⅛- .pr^⅛- -pτ^⅛- .p^⅛-
切伊伊伊仍面例团伊团伊
⅛⅛J⅛J⅛J⅛J⅛J⅛J⅛J⅛J⅛J¾I
旨勺畲野勺99畲杳牙囱f
Q1Q1Q1Q1Q1Q1Q1Q1Q1Q1Q1
国睥睥睥睥睥睥睥睥睥睥睥
Lε乐乐乐乐盔乐乐袅或乐乐
L -/-在一心-/-恰一心-/一痉-曲-/-痉-
¾¾¾¾¾¾¾¾¾¾¾¾
"7r7r?r?r?r7r?r?r?r?r?r7
d
⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛⅛
e∙∙∙∙∙∙∙∙∙∙∙
口陶阴幅阐幅幅响啕啕幅幅
口
上
q
(a)	MNIST, Continuous Bernoulli.
(b)	Fashion, discretized logistic.
Figure 16:	Missing squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling.
Table 7:	Low capacity (4-layer) discriminative network for SVHN classification.
Action (resulting layer size)
Input X (32 X 32 X 3)
Conv2D(16 × 16 × 16)
Conv2D(8 X 8 X 32)
Conv2D(4 X 4 X 64)
Reshape(1024)
Class probabilities: Dense(10)
For the generative models, 2 layered neural networks with 128 hidden units are used for the encoder
and the decoder and a latent space of dimension 10. A Gaussian observation model is used and
K = 25 importance samples are used during training.
We compare different imputation techniques, modelling approaches and a non-deep competitor.
Zero-imputation, MissForest (Stekhoven & Buhlmann, 2012) and MICE (Buuren & Groothuis-
Oudshoorn, 2010) are different imputation techniques applied, before training the discriminative
model. Learnable imputation is using learnable constants as imputation, while permutation invari-
ance is utilizing a learnable embedding. Finally, histogram-based gradient boosting (Friedman,
2001), as implemented in sklearn (Pedregosa et al., 2011), is a non-deep baseline, using the valida-
tion set for early stopping. Results are seen in figure 20.
27
Published as a conference paper at ICLR 2022
7 1/ Oq∕,κ<yo0
7 乙 ∕oq∕，<<<"Mo 0
夕。-∙5g7¾q
7OJS97¾U<
7 1/。4/，SG^。& VOJS
7 乙 ∕oq∕γQ<yD6 O/ O J S
7 乙 ∕∙oq∕，气sy。。7 OI S
71 / Oq /,κ<m Ob 7 0 15
97¾ 4
97<0 H
g7¾H
97¾q
7 乙/oq∕7κ<>ofovojsolɔz^ul
7 乙/ O “/，—<夕。。70159 7¾ u>
7 1/Oq /，—<>。fe7OJ597⅛d>
7 乙 ∕oq∕,κ⅛夕。2 70 J 5 9 7¾ u>
7 1 ∕∙oq∕,κς∙*。2 7OIS97¾U>
踊口 )，M，MJlMJlM41 Mjaf
g∣ιιssβιsιβssβss
Kflnnnnnnonnn
”.加««Md⅛∣	<√
Ihiaaaaaaaaaa
乜津石煤石煤一密石煤石煤石煤 一¾Llyj⅜ w堞 ―⅜
联益4>局”>9、》^%r1^视%>*>腐1%*>憎^、摘1%
Hlllllilllll
Iaaaaaaaaaaa
■ r flinɪɪifliɪɪ
慧詹偈腐仙蠲剧n偈晶储蠲
熊腐幽幽般豳醐般幽幽瀛般
翻■■■■■■■■■■■
SttttVVffffff
(a)	MNIST, Continuous Bernoulli.
(b)	Fashion, discretized logistic.
Figure 17: Missing squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling.
Table 8:	High capacity (8-layer) discriminative network for SVHN classification.
Action (resulting layer size)
Input X (32 X 32 X 3)
Conv2D(16 × 16 × 16)
Conv2D(8 X 8 X 32)
Gated block (Dauphin et al., 2017) (8 X 8 X 32)
Gated block (Dauphin et al., 2017) (8 X 8 X 32)
Gated block (Dauphin et al., 2017) (8 X 8 X 32)
Gated block (Dauphin et al., 2017) (8 X 8 X 32)
Conv2D(4 X 4 X 64)
Reshape(1024)
Class probabilities: Dense(10)
28
Published as a conference paper at ICLR 2022
0.81
0.80
0.79
0.78
0.77
0.76
supMIWAE
Ml MIWAE
0-impute
learnable-imputation
svhn： missing squares
i

Figure 18:	Test set accuracies on the SVHN dataset, with different missing mechanisms. Left
column: observed squares. Middle column: missing squares. Right column: random dropout with
missing rate 0.5.
svhn： observed squares
0.93 Ift
0.92	.
0∙91	i
0.90	ɪ
0.89	supMIWAE
0.88	MIWAE
0 87 ∙l 0-impute
,	learnable-imputation
0.86
0.934
0.932
0.930
0.928
0.926
0.924
Figure 19:	Fully observed test set accuracies on the SVHN dataset, with different missing mecha-
nisms. Left column: observed squares. Middle column: missing squares. Right column: random
dropout with missing rate 0.5.
29
Published as a conference paper at ICLR 2022
0 9 8 7 6 5 4
LSSS0.0.0.
① SUU⅛≤
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
0.9
φ 0.8
g 0.7
上0.6
S 0.5
+j0.4
0.3
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
1.00
”•95
目 0.90
场 0.85
a 0.80
wine-red
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
0.75
∕0∙95
目 0.90
意 0.85
+j0.80
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
missing rate
Figure 20: Test-set root mean square error on UCI datasets at varying missing rates.
-supMIWAE
-*- MIWAE
0-impute
—learnable-imputation
一.MICE
—missForest
PPCA
-GB
—permutation-invariance
30