Published as a conference paper at ICLR 2022
Policy Smoothing for Provably Robust
Reinforcement Learning
Aounon Kumar, Alexander Levine, Soheil Feizi
Department of Computer Science
University of Maryland - College Park, USA
aounon@umd.edu,{alevine0,sfeizi}@cs.umd.edu
Ab stract
The study of provable adversarial robustness for deep neural networks (DNNs)
has mainly focused on static supervised learning tasks such as image classifica-
tion. However, DNNs have been used extensively in real-world adaptive tasks
such as reinforcement learning (RL), making such systems vulnerable to adver-
sarial attacks as well. Prior works in provable robustness in RL seek to certify
the behaviour of the victim policy at every time-step against a non-adaptive ad-
versary using methods developed for the static setting. But in the real world, an
RL adversary can infer the defense strategy used by the victim agent by observ-
ing the states, actions, etc. from previous time-steps and adapt itself to produce
stronger attacks in future steps (e.g., by focusing more on states critical to the
agent’s performance). We present an efficient procedure, designed specifically to
defend against an adaptive RL adversary, that can directly certify the total reward
without requiring the policy to be robust at each time-step. Focusing on random-
ized smoothing based defenses, our main theoretical contribution is to prove an
adaptive version of the Neyman-Pearson Lemma - a key lemma for smoothing-
based certificates - where the adversarial perturbation at a particular time can be
a stochastic function of current and previous observations and states as well as
previous actions. Building on this result, we propose policy smoothing where the
agent adds a Gaussian noise to its observation at each time-step before passing
it through the policy function. Our robustness certificates guarantee that the fi-
nal total reward obtained by policy smoothing remains above a certain threshold,
even though the actions at intermediate time-steps may change under the attack.
We show that our certificates are tight by constructing a worst-case scenario that
achieves the bounds derived in our analysis. Our experiments on various environ-
ments like Cartpole, Pong, Freeway and Mountain Car show that our method can
yield meaningful robustness guarantees in practice.
1	Introduction
Deep neural networks (DNNs) have been widely employed for reinforcement learning (RL) prob-
lems as they enable the learning of policies directly from raw sensory inputs, like images, with min-
imal intervention from humans. From achieving super-human level performance in video-games
(Mnih et al., 2013; Schulman et al., 2015; Mnih et al., 2016), Chess (Silver et al., 2017) and Go
(Silver et al., 2016) to carrying out complex real-world tasks, such as controlling a robot (Levine
et al., 2016) and driving a vehicle (Bojarski et al., 2016), deep-learning based algorithms have not
only established the state of the art, but also become more effortless to train. However, DNNs have
been shown to be susceptible to tiny malicious perturbations of the input designed to completely
alter their predictions (Szegedy et al., 2014; Madry et al., 2018; Goodfellow et al., 2015). In the RL
setting, an attacker may either directly corrupt the observations of an RL agent (Huang et al., 2017;
Behzadan & Munir, 2017; Pattanaik et al., 2018) or act adversarially in the environment (Gleave
et al., 2020) to significantly degrade the performance of the victim agent. Most of the adversarial
defense literature has focused mainly on classification tasks (Kurakin et al., 2017; Buckman et al.,
2018; Guo et al., 2018; Dhillon et al., 2018; Li&Li, 2017; Grosse et al., 2017; Gong et al., 2017).
1
Published as a conference paper at ICLR 2022
In this paper, we study a defense procedure for RL problems that is provably robust against norm-
bounded adversarial perturbations of the observations of the victim agent.
Problem setup. A reinforcement learning task is commonly described as a game between an agent
and an environment characterized by the Markov Decision Process (MDP) M = (S, A, T, R, γ),
where S is a set of states, A is a set of actions, T is the transition probability function, R is the one-
step reward function and γ ∈ [0, 1] is the discount factor. However, as described in Section 3, our
analysis applies to an even more general setting than MDPs. At each time-step t, the agent makes an
observation ot = o(st) ∈ Rd which is a probabilistic function of the current state of the environment,
picks an action at ∈ A and receives an immediate reward Rt = R(st, at). We define an adversary
as an entity that can corrupt the agent’s observations of the environment by augmenting them with
a perturbation t at each time-step t which can depend on the states, actions, observations, etc.,
generated so far. We use = (1, 2, . . .) to denote the entire sequence of adversarial perturbations.
The goal of the adversary is to minimize the total reward obtained by the agent policy π while
keeping the overall '2-norm of the perturbation within a budget B. Formally, the adversary seeks to
optimize the following objective:
∞
min En T γtRt , where Rt = R(st, at), at 〜∏(∙∣o(st) + Q)
t=0
s.t. k(1,2, . . .)k2 = t
∞
X kt k22 ≤ B.
t=0
Note that the size of the perturbation t in each time-step t need not be the same and the adversary
may choose to distribute the budget B over different time-steps in a way that allows it to produce a
stronger attack. Also, our formulation accounts for cases when the agent may only partially observe
the state of the environment, making M a Partially Observable Markov Decision Process (POMDP).
Objective. Our goal in provably robust RL is to design a policy π such that the total reward in the
presence of a norm-bounded adversary is guaranteed to remain above a certain threshold, i.e.,
∞
min En X YtRt ≥ R, s.t. kd∣2 ≤ B.	(1)
t=0
In other words, no norm-bounded adversary can lower the expected total reward of the policy π
below a certain threshold. In our discussion, we restrict out focus to finite-step games that end after
t time-steps. This is a reasonable approximation for infinite games with γ < 1, as for a sufficiently
large t, γt becomes negligibly small. For games where γ = 1, Rt must become sufficiently small
after a finite number of steps to keep the total reward finite.
Step-wise vs. episodic certificates. Previous works on robust RL have sought to certify the be-
haviour of the policy function at each time-step ofan episode, e.g., the output ofa Deep Q-Network
(Lutjens et al., 2019) and the action taken for a given state (Zhang et al., 2020). Ensuring that the
behaviour of the policy remains unchanged in each step can also certify that the final total reward
remains the same under attack. However, if the per-step guarantee fails at even one of the interme-
diate steps, the certificate on the total reward becomes vacuous or impractical to compute (as noted
in Appendix E of Zhang et al. (2020)). Our approach gets around this issue by directly certifying
the final total reward for the entire episode without requiring the policy to be provably robust at each
intermediate step. Also, the threat-model we consider is more general as we allow the adversary to
choose the size of the perturbation for each time-step. Thus, our method can defend against more
sophisticated attacks that focus more on states that are crucial for the victim agent’s performance.
Technical contributions. In this paper, we study a defense procedure based on “randomized
smoothing” (Cohen et al., 2019; Lecuyer et al., 2019; Li et al., 2019; Salman et al., 2019) since
at least in “static” settings, its robustness guarantee scales up to high-dimensional problems and
does not need to make stringent assumptions about the model. We ask: can we utilize the benefits of
randomized smoothing to make a general high-dimensional RL policy provably robust against ad-
versarial attacks? The answer to this question turns out to be non-trivial as the adaptive nature of the
adversary in the RL setting makes it difficult to apply certificates from the static setting. For exam-
ple, the `2 -certificate by Cohen et al. (2019) critically relies on the clean and adversarial distributions
2
Published as a conference paper at ICLR 2022
Figure 1: The standard Cohen et al. (2019) smoothing-based robustness certificate relies on the
clean and the adversarial distributions being isometric Gaussians (panel a). However, adding noise
to sequential observations in an RL setting (panels b-d) does not result in an isometric Gaussian
distribution over the space of observations. In all figures, the distributions associated with clean and
adversarially-perturbed values are shown in blue and red, respectively.
being isometric Gaussians (Figure 1-a). However, in the RL setting, the adversarial perturbation in
one step might depend on states, actions, observations, etc., of the previous steps, which could in
turn depend on the random Gaussian noise samples added to the observations in these steps. Thus,
the resulting adversarial distribution need not be isometric as in the static setting (Figure 1-(b-d)).
For more details on this example, see Appendix B.
Our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson
lemma (Neyman & Pearson, 1992) to produce robustness guarantees for RL. We emphasize that
this is not a straightforward extension (refer to Appendix D, E and F for the entire proof). To prove
this fundamental result, we first eliminate the effect of randomization in the adversary (Lemma 1)
by converting a general adversary to one where the perturbation at each time-step is a deterministic
function of the previous states, actions, observations, etc., and showing that the modified adversary
is as strong as the general one. Then, we prove the adaptive Neyman-Pearson lemma where we show
that, in the worst-case, the deterministic adversary can be converted to one that uses up the entire
budget B in the first coordinate of the perturbation in the first time-step (Lemma 3). Finally, we de-
rive the robustness guarantee under an isometric Gaussian smoothing distribution (Theorem 1). In
section A, we establish the tightness of our certificates by constructing the worst-case environment-
policy pair which attains our derived bounds. More formally, out of all the environment-policy pairs
that achieve a certain total reward with probability p, we show a worst-case environment-policy pair
and a corresponding adversary such that the probability of achieving the same reward under the
presence of the adversary is minimum. A discussion on the Neyman-Pearson lemma in the context
of randomized smoothing is available in Appendix C.
Building on these theoretical results, we propose Policy Smoothing, a simple model-agnostic
randomized-smoothing based technique that can provide certified robustness without increasing the
computational complexity of the agent’s policy. Our main contribution is to show that by augment-
ing the policy’s input by a random smoothing noise, we can achieve provable robustness guarantees
on the total reward under a norm-bounded adversarial attack (Section 4.2). Policy Smoothing does
not need to make assumptions about the agent’s policy function and is also oblivious to the work-
ings of RL environment. Thus, this method can be applied to any RL setting without having to
make restrictive assumptions on the environment or the agent. In section 3, we model the entire
adversarial RL process under Policy Smoothing as a sequence of interactions between a system A,
which encapsulates the RL environment and the agent, and a system B, which captures the addition
of the adversarial perturbation and the smoothing noise to the observations. Our theoretical results
do not require these systems to be Markovian and can thus have potential applications in real-time
decision-making processes that do not necessarily satisfy the Markov property.
Empirical Results. We use four standard Reinforcement Learning benchmark tasks to evaluate
the effectiveness of our defense and the significance of our theoretical results: the Atari games
‘Pong’ and ‘Freeway’ (Mnih et al., 2013) and the classical ‘Cartpole’ and ‘Mountain Car’ control
environments (Barto et al., 1983; Moore, 1990) - see Figure 3. We find that our method provides
highly nontrivial certificates. In particular, on at least two of the tasks, ‘Pong’ and ‘cartpole’, the
3
Published as a conference paper at ICLR 2022
provable lower bounds on the average performances of the defended agents, against any adversary,
exceed the observed average performances of undefended agents under a practical attack.
2	Prior Work
Adversarial RL. Adversarial attacks on RL systems have been extensively studied in recent years.
DNN-based policies have been attacked by either directly corrupting their inputs (Huang et al.,
2017; Behzadan & Munir, 2017; Pattanaik et al., 2018) or by making adversarial changes in the
environment (Gleave et al., 2020). Empirical defenses based on adversarial training, whereby the
dynamics of the RL system is augmented with adversarial noise, have produced good results in
practice (Kamalaruban et al., 2020; Vinitsky et al., 2020). Zhang et al. (2021) propose training
policies together with a learned adversary in an online alternating fashion to achieve robustness to
perturbations of the agent’s observations.
Robust RL. Prior work by Lutjens et al. (2019) has proposed a 'certified' defense against adversarial
attacks to observations in deep reinforcement learning, particularly for Deep Q-Network agents.
However, that work essentially only guarantees the stability of the network approximated Q-value at
each time-step ofan episode. By contrast, our method provides a bound on the expected true reward
of the agent under any norm-bounded adversarial attack.
Zhang et al. (2020) certify that the action in each time-step remains unchanged under an adversarial
perturbation of fixed budget for every time-step. This can guarantee that the final total reward
obtained by the robust policy remains the same under attack. However, this approach would not be
able to yield any robustness certificate if even one of the intermediate actions changed under attack.
Our approach gets around this difficulty by directly certifying the total reward, letting some of the
intermediate actions of the robust policy to potentially change under attack. For instance, consider
an RL agent playing Atari Pong. The actions taken by the agent when the ball is close to and
approaching the paddle are significantly more important than the ones when the ball is far away or
retreating from the paddle. By allowing some of the intermediate actions to potentially change, our
approach can certify for larger adversarial budgets and provide a more fine-grained control over the
desired total-reward threshold. Moreover, we study a more general threat model where the adversary
may allocate different attack budgets for each time-step focusing more on the steps that are crucial
for the agent’s performance, e.g., attacking a Pong agent when the ball is close to the paddle.
Provable Robustness in Static Settings: Notable provable robustness methods in static settings are
based on interval-bound propagation (Gowal et al., 2018; Huang et al., 2019; Dvijotham et al., 2018;
Mirman et al., 2018), curvature bounds (Wong & Kolter, 2018; Raghunathan et al., 2018; Chiang
et al., 2020; Singla & Feizi, 2019; 2020; 2021), randomized smoothing (Cohen et al., 2019; LecUyer
et al., 2019; Li et al., 2019; Salman et al., 2019; Levine & Feizi, 2021), etc. Certified robustness has
also been extended to problems with structured outputs such as images and sets (Kumar & Goldstein,
2021). Focusing on Gaussian smoothing, Cohen et al. (2019) showed that if a classifier outputs a
class with some probability under an isometric Gaussian noise around an input point, then it will
output that class with high probability at any perturbation of the input within a particular `2 distance.
Kumar et al. (2020) showed how to certify the expectation of softmax scores of a neural network
under Gaussian smoothing by using distributional information about the scores.
3	Preliminaries and Notations
We model the finite-step adversarial RL framework as a t-round communication between two sys-
tems A and B (Figure 2). System A represents the RL game. It contains the environment M and
the agent, and when run independently, simulates the interactions between the two for some given
policy π. At each time-step i, it generates a token τi from some set T, which is a tuple of the cur-
rent state si and its observation oi, the action ai-1 in the previous step (and potentially some other
objects that we ignore in this discussion), i.e., τi = (si,ai-1,oi,...) ∈ S × A × Rd × ... = T.
For the first step, replace the action in τι with some dummy element * from the action space A.
System B comprises of the adversary and the smoothing distribution which generate an adversarial
perturbation i and a smoothing noise vector δi , respectively, at each time-step i, the sum of which
is denoted by an offset ηi = i + δi ∈ Rd .
4
Published as a conference paper at ICLR 2022
Figure 2: Adversarial robustness framework.
When both systems are run together in an interactive fashion, in each round i, system A generates
τi as a probabilistic function of τ1, η1, τ2, η2, . . . , τi-1, ηi-1, i.e., τi : (T × Rd)i-1 → ∆(T). τ1 is
sampled from a fixed distribution. It passes τi to B, which generates i as a probabilistic function
of {τj, ηj}ij-=11 and τi, i.e., i : (T × Rd)i-1 × T → ∆(Rd) and adds a noise vector δi sampled
independently from the smoothing distribution to obtain ηi. It then passes ηi to A for the next round.
After running for t steps, a deterministic or random 0/1-function h is computed over all the tokens
and offsets generated. We are interested in bounding the probability with which h outputs 1 as a
function of the adversarial budget B . In the RL setting, h could be a function indicating whether the
total reward is above a certain threshold or not.
4	Provably Robust RL
4.1	Adaptive Neyman-Pearson Lemma
Let X be the random variable representing the tuple z = (τ1, η1, τ2, η2, . . . , τt, ηt) ∈ (T × Rd)t
when there is no adversary, i.e., i = 0 and ηi = δi is sampled directly form the smoothing distri-
bution P. Let Y be the random variable representing the same tuple in the presence of a general
adversary satisfying kk2 ≤ B. Thus, if h(X) = 1 with some probability p, we are interested in
deriving a lower-bound on the probability of h(Y ) = 1 as a function ofp and B. Let us now define
a deterministic adversary dt for which the adversarial perturbation at each step is a deterministic
function of the tokens and offsets of the previous steps and the token generated in the current step.
i.e., idt : (T × Rd)i-1 × T → Rd. Let Y dt be its corresponding random variable. Then, we have
the following lemma that converts a probabilistic adversary into a deterministic one.
Lemma 1 (Reduction to Deterministic Adversaries). For any general adversary and an Γ ⊆
(T × Rd)t, there exists a deterministic adversary dt such that,
P[Y dt ∈ Γ] ≤ P[Y ∈ Γ],
where Y dt is the random variable for the distribution defined by the adversary dt.
This lemma says that for any adversary (deterministic or random) and a subset Γ of the space of
z, there exists a deterministic adversary which assigns a lower probability to Γ than the general
adversary. In the RL setting, this means that the probability with which a smoothed policy achieves
a certain reward value under a general adversary is lower-bounded by the probability of the same
under a deterministic adversary. The intuition behind this lemma is that out of all the possible values
that the internal randomness of the adversary may assume, there exists a sequence of values that
assigns the minimum probability to Γ (over the randomness of the environment, policy, smoothing
noise, etc.). We defer the proof to the appendix.
Next, we formulate an adaptive version of the Neyman-Pearson lemma for the case when the smooth-
ing distribution P is an isometric Gaussian N(0, σ2I). If we applied the classical Neyman-Pearson
lemma on the distributions of X and Y dt , it will give us a characterization of the worst-case 0/1
function among the class of functions that achieve a certain probability p of being 1 under the dis-
tribution of X that has the minimum probability of being 1 under Ydt. Let μχ and μγdt be the
probability density function of X and Ydt, respectively.
5
Published as a conference paper at ICLR 2022
Lemma 2 (Neyman-Pearson Lemma, 1933). If ΓY dt = {z ∈ (TX Rd)t | μγdt (Z) ≤ qμx (z)}
for some q ≥ 0 and P[h(X) = 1] ≥ P[X ∈ ΓYdt], then P[h(Y dt) = 1] ≥ P[Y dt ∈ ΓYdt].
For an arbitrary element h in the class of functions Hp = {h | P[h(X) = 1] ≥ p}, construct the set
ΓYdt for an appropriate value of q for which P[X ∈ ΓYdt] = p. Now, consider a function h0 which
is 1 if its input comes from ΓYdt and 0 otherwise. Then, the above lemma says that the function h0
has the minimum probability of being 1 under Y dt, i.e.,
h0 = argmin P[h(Y dt) = 1].
h∈Hp
This gives us the worst-case function that achieves the minimum probability under an adversarial
distribution. However, in the adaptive setting, ΓYdt could be a very complicated set and obtain-
ing an expression for P[Y dt ∈ ΓYdt] might be difficult. To simplify our analysis, we construct a
structured deterministic adversary st which exhausts its entire budget in the first coordinate of the
first perturbation vector, i.e., s1t = (B, 0, . . . , 0) and ist = (0, 0, . . . , 0) for i > 1. Let Y st be the
corresponding random variable and μγSt its density function. We formulate the following adaptive
version of the Neyman-Pearson lemma:
Lemma3(AdaptiveNeyman-PearsonLemma). If ΓγSt = {z ∈ (T×Rd)t | μγSt(Z) ≤ qμχ(z)}
for some q ≥ 0 and P[h(X) = 1] ≥ P[X ∈ ΓYst], then P[h(Y dt) = 1] ≥ P[Y st ∈ ΓYst].
The key difference from the classical version is that the worst-case set we construct in this lemma
is for the structured adversary and the final inequality relates the probability of h outputting 1 under
the adaptive adversary to the probability that the structured adversary assigns to the worst-case set.
It says that for the appropriate value of q for which P[X ∈ ΓYSt] = p, any function h ∈ Hp outputs
1 with at least the probability that Y st assigns to ΓYSt. It shows that over all possible functions in
Hp and over all possible adversaries , the indicator function 1z∈Γ St and the structured adversary
capture the worst-case scenario where probability of h being 1 under the adversarial distribution is
the minimum. Since both Y st and X are just isometric Gaussian distribution with the same variance
σ2 centered at different points on the first coordinate of η1, the set ΓYSt is the set of all tuples Z for
which {η1 }1 is below a certain threshold.1 We use lemmas 1 and 3 to derive the final bound on the
probability of h(Y ) = 1 in the following theorem, the proof of which is deferred to the appendix.
Theorem 1 (Robustness Guarantee). For an isometric Gaussian smoothing noise with variance
σ2, ifP[h(X) = 1] ≥ p, then:
P[h(Y) = 1] ≥ Φ(Φ-1(p) — B∕σ),
where Φ is the standard normal CDF.
The above analysis can be adapted to obtain an upper-bound on P[h(Y) = 1] of Φ(Φ-1(p) + B∕σ).
4.2 Policy Smoothing
Building on these results, we develop policy smoothing, a simple model-agnostic randomized-
smoothing based technique that can provide certified robustness without increasing the computa-
tional complexity of the agent,s policy. Given a policy ∏, we define a smoothed policy ∏ as:
∏ ( ∙ | o(st)) = ∏ ( ∙ | o(st) + δt), where δt 〜N(0,σ2I).
Our goal is to certify the expected sum of the rewards collected over multiple time-steps under policy
∏. We modify the technique developed by Kumar et al. (2020) to certify the expected class scores
of a neural network by using the empirical cumulative distribution function (CDF) of the scores
under the smoothing distribution to work for the RL setting. This approach utilizes the fact that
the expected value of a random variable X representing a class score under a Gaussian N(0, σ2I)
smoothing noise can be expressed using its CDF F (.) as below:
E[X] = Z ∞(1 - F (x))dx -Z 0 F (x)dx.
0	-∞
(2)
1We use {ηi }j to denote the jth coordinate of the vector ηi .
6
Published as a conference paper at ICLR 2022
(c) Freeway
Figure 3: Environments used in evaluations rendered by OpenAI Gym (Brockman et al., 2016).
Given m samples {xi}im=1 of the random variable X , let us define its empirical CDF at a point x,
Fm(x) = |{xi | xi ≤ x}|/m, as the fraction of samples that are less than or equal to x. Using
Fm (x), the DVoretzky-Kiefer-WolfoWitz inequality can produce high-confidence bounds on the
true CDF of X. It says that with probability 1 - α, for α ∈ (0, 1], the true CDF F(x) is in the range
[F(x), F(x)], where F(x) = Fm(x) - pin(2∕α)∕2m and F(x) = Fm(x) + pin(2∕α)∕2m. For
an adversarial perturbation of '2-size B, the result of Cohen et al. (2019) bounds the CDF within
[Φ(Φ-1 (F(x)) — B∕σ), Φ(Φ-1 (F(x)) + B∕σ)], which in turn bounds E[X] using equation (2).
In the RL setting, we can model the total reward as a random variable and obtain its empirical CDF
by playing the game using policy π. As above, we can bound the CDF F(x) of the total reward in
a range [F(x), F(x)] using the empirical CDF. Applying Theorem 1, we can bound the CDF within
[Φ(Φ-1 (F(x)) — B∕σ), Φ(Φ-1 (F(x)) + B∕σ)] for an '2 adversary of size B. The function h in
Theorem 1 could represent the CDF F(x) by indicating whether the total reward computed for an
input z ∈ (T × Rd)t is below a value x. Finally, equation (2) puts bounds on the expected total
reward under an adversarial attack.
5	Experiments
5.1	Environments and Setup
We tested on four standard environments: the classical cortrol problems ‘Cartpole’ and ‘Mountain
Car’ and the Atari games ‘Pong’ and ‘Freeway.’ We consider three tasks which use a discrete action
space (‘Cartpole’ and the two Atari games) as well as one task that uses a continuous action space
(‘Mountain Car’). For the discrete action space tasks, we use a standard Deep Q-Network (DQN)
(Mnih et al., 2013) model, while for ‘Mountain Car’, we use Deep Deterministic Policy Gradient
(DDPG) (Lillicrap et al., 2016).
As is common in DQN and DDPG, our agents choose actions based on multiple frames of observa-
tions. In order to apply a realistic threat model, we assume that the adversary acts on each frame only
once when it is first observed. The adversarial distortion is then maintained when the same frame is
used in future time-steps. In other words, we consider the observation at time step ot (discussed in
Section 3) to be only the new observation at time t: this means that the adversarial/noise perturbation
ηt , as a fixed vector, continues to be used to select the next action for several subsequent time-steps.
This is a realistic model because we are assuming that the adversary can affect the agent’s observa-
tion of states, not necessarily the agent’s memory of previous observations. As in other works on
smoothing-based defenses (e.g., Cohen et al. (2019)), we add noise during training as well as at test
time. We use DQN and DDPG implementations from the popular stable-baselines3 package (Raffin
et al., 2019): hyperparameters are provided in the appendix. In experiments, we report and certify
for the total non-discounted (γ = 1) reward.
In ‘Cartpole’, the observation vector consists of four kinematic features. We use a simple MLP
model for the Q-network, and tested two variations: one in which the agent uses five frames of
observation, and one in which the agent uses only a single frame (shown in the appendix).
In order to show the effectiveness of our technique on tasks involving high-dimensional state ob-
servations, we chose two tasks (‘Pong’ and ‘Freeway’) from the Atari environment, where state
observations are image frames, observed as 84 × 84 pixel greyscale images. For ‘Pong’, we test on
a “one-round” variant of the original environment. In our variant, the game ends after one player,
either the agent or the opponent, scores a goal: the reward is then either zero or one. Note that this is
not a one-timestep episode: it takes typically on the order of 100 timesteps for this to occur. Results
7
Published as a conference paper at ICLR 2022
(a) CartPole
(b) Pong (One-Rc)Und)
Perturbation Budget
0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8
Perturbation Budget
Figure 4: Certified performance for various environments. The certified lower-bound on the mean
reward is based on a 95% lower confidence interval estimate of the mean reward of the smoothed
model, using 10,000 episodes.
for a full Pong game are presented in the appendix: as explained there, we find that the certificates
unfortunately do not scale with the length of the game. For the ‘Freeway’ game, we play on ‘Hard’
mode and end the game after 250 timesteps.
In order to test on an environment with a continuous action space, we chose the ‘Mountain Car’ en-
vironment. Note that previous certification results for reinforcement learning, which certify actions
at individual states rather than certifying the overall reward (Zhang et al., 2020) cannot be applied
to continuous action state problems. In this environment, the observation vector consists of two
kinematic features (position and velocity), and the action is one continuous scalar (acceleration). As
in ‘Cartpole’, we use five observation frames and a simple MLP policy. We use a slight variant of
the original environment: we do not penalize for fuel cost so the reward is a boolean representing
whether or not the car reaches the destination in the time allotted (999 steps).
5.2	Results
Certified lower bounds on the expected total reward, as a function of the total perturbation budget,
are presented in Figure 4. For tasks with zero-one total reward (‘Pong’ and ‘Mountain Car’), the
function to be smoothed represents the total reward: h(∙) = R where R is equal to 1 if the agent Wins
the round, and 0 otherwise. To compute certificates on games with continuous scores (‘Cartpole’
and ‘Freeway’), we use CDF smoothing (Kumar et al., 2020): see appendix for technical details.
In order to evaluate the robustness of both undefended and policy-smoothed agents, we developed
an attack tailored to the threat model defined in 1, where the adversary makes a perturbation to state
observations which is bounded over the entire episode. For DQN agents, as in LutjenS et al. (2019),
we perturb the observation o such that the perturbation-induced action a0 := arg maxa Q(o + t, a)
minimizes the (network-approximated) Q-value of the true observation Q(o, a0). However, in order
to conserve adversarial budget, we only attack if the gap between attacked q-value Q(o, a0) and the
clean q-value maxa Q(o, a) is sufficiently large, exceeding a preset threshold λQ . In practice, this
allows the attacker to concentrate the attack budget only on the time-steps which are critical to the
agent’s performance. When attacking DDPG, where both a Q-value network and a policy network
π are trained and the action is taken according to π, we instead minimize Q(o, π(o + t)) + λktk2
where the hyperparameter λ plays an analogous role in focusing perturbation budget on “important”
steps, as judged by the effect on the approximated Q-value. Empirical results are presented in
Figure 5. We see that the attacks are effective on the undefended agents (red, dashed lines). In
fact, from comparing Figures 4 and 5, we see that, for the Pong and Cartpole environments, the
undefended performance under attack is worse than the certified lower bound on the performance of
the policy-smoothed agents under any possible attack: our certificates are the clearly non-vacuous
for these environments. Further details on the attack optimizations are provided in the appendix.
8
Published as a conference paper at ICLR 2022
Figure 5: Empirical robustness of defended and undefended agents. Full details of attacks are
presented in appendix.
We also present an attempted empirical attack on the smoothed agent, adapting techniques for at-
tacking smoothed classifiers from Salman et al. (2019) (solid blue lines). We observed that our
model was highly robust to this attack - significantly more robust than guaranteed by our certificate.
However, it is not clear whether this is due to looseness in the certificate or to weakness of the attack:
the significant practical challenges to attacking smoothed agents are also discussed in the appendix.
6	Conclusion
In this work, we extend randomized smoothing to design a procedure that can make any reinforce-
ment learning agent provably robust against adversarial attacks without significantly increasing the
complexity of the agent’s policy. We show how to adapt existing theory on randomized smoothing
from static tasks such as classification, to the dynamic setting of RL. By proving an adaptive version
of the celebrated Neyman-Pearson Lemma, we show that by adding Gaussian smoothing noise to
the input of the policy, one can certifiably defend it against norm-bounded adversarial perturbations
of its input. The policy smoothing technique and its theory covers a wide range of adversaries,
policies and environments. Our analysis is tight, meaning that the certificates we achieve are best
possible unless restrictive assumptions about the RL game are made. In our experiments, we show
that our method provides meaningful guarantees on the robustness of the defended policies and the
total reward they achieve even in the worst case is higher than an undefended policy. In the future,
the introduction of randomized smoothing to RL could inspire the design of provable robustness
techniques for control problems in dynamic real-world environments and multi-agent RL settings.
Reproducibility
We supplement our work with accompanying code for reproducing the experimental results, as well
as pre-trained models for a selection of the experiments. Details about setting hyper-parameters and
the environments we test are included in the appendix. Proofs for our theoretical results (lemmas
and main theorem) are also provided in the appendix.
Ethics Statement
We present a method to make RL models provably robust under adversarial perturbations. We do
not foresee any immediate ethical concerns associated with our work.
Acknowledgements
This project was supported in part by NSF CAREER AWARD 1942230, a grant from NIST
60NANB20D134, HR001119S0026-GARD-FP-052, HR00112090132, ONR YIP award N00014-
22-1-2271, Army Grant W911NF2120076.
9
Published as a conference paper at ICLR 2022
References
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements
that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and
Cybernetics, SMC-13(5):834-846,1983. doi:10.1109/TSMC.1983.6313077.
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In Petra Perner (ed.), Machine Learning and Data Mining in Pattern Recognition - 13th
International Conference, MLDM 2017, New York, NY, USA, July 15-20, 2017, Proceedings,
volume 10358 of Lecture Notes in Computer Science,pp. 262-275. Springer, 2017. doi: 10.1007/
978-3-319-62416-7∖.19. URL https://doi.org/10.1007/978-3-319-62416-7_
19.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016. URL
http://arxiv.org/abs/1604.07316.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings,
2018.
Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Gold-
stein. Certified defenses for adversarial patches. In 8th International Conference on Learning
Representations, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310-1320, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bernstein, Jean Kossaifi,
Aran Khanna, and Animashree Anandkumar. Stochastic activation pruning for robust adversarial
defense. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers, 2018.
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversar-
ial policies: Attacking deep reinforcement learning. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=HJgEMpVFwB.
Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. Adversarial and clean data are not twins. CoRR,
abs/1704.04960, 2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models, 2018.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick D. McDaniel.
On the (statistical) detection of adversarial examples. CoRR, abs/1702.06280, 2017.
10
Published as a conference paper at ICLR 2022
ChUan Guo, Mayank Rana, MoUstaPha Cisse, and LaUrens van der Maaten. Countering adversarial
images using input transformations. In 6th International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed-
ings, 2018.
Po-Sen HUang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krish-
namUrthy Dvijotham, and PUshmeet Kohli. Achieving verified robUstness to symbol sUbstitUtions
via interval boUnd ProPagation. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 4081-4091,
2019. doi: 10.18653/v1/D19-1419. URL https://doi.org/10.18653/v1/D19-1419.
Sandy H. HUang, Nicolas Papernot, Ian J. Goodfellow, Yan DUan, and Pieter Abbeel. Adversarial
attacks on neUral network policies. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?id=ryvlRyBKl.
Parameswaran KamalarUban, YU-Ting HUang, Ya-Ping Hsieh, PaUl Rolland, Cheng Shi, and
Volkan Cevher. RobUst reinforcement learning via adversarial training with langevin dy-
namics. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volUme 33, pp. 8127-8138. CUrran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
5cb0e249689cd6d8369c4885435a56c2- Paper.pdf.
AoUnon KUmar and Tom Goldstein. Center smoothing for certifiably robUst vector-valUed fUnctions.
CoRR, abs/2102.09701, 2021. URL https://arxiv.org/abs/2102.09701.
AoUnon KUmar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via ran-
domized smoothing. In HUgo Larochelle, Marc’AUrelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and HsUan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
37aa5dfc44dddd0d19d4311e2c7a0240- Abstract.html.
Alexey KUrakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-
26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?
id=BJm4T4Kgx.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robUstness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pp. 656-672, 2019.
Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for L1 certified robustness.
CoRR, abs/2103.10834, 2021. URL https://arxiv.org/abs/2103.10834.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep vi-
suomotor policies. J. Mach. Learn. Res., 17:39:1-39:40, 2016. URL http://jmlr.org/
papers/v17/15-522.html.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 9459-9469, 2019.
Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional filter
statistics. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, Oc-
tober 22-29, 2017, pp. 5775-5783, 2017.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learn-
ing. CoRR, abs/1509.02971, 2016.
11
Published as a conference paper at ICLR 2022
Bjom Lutjens, Michael Everett, and Jonathan P. How. Certified adversarial robustness for deep
reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd
Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1,
2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pp. 1328-1337.
PMLR, 2019. URL http://proceedings.mlr.press/v100/lutjens20a.html.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings, 2018.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learn-
ing Research, pp. 3578-3586. PMLR, 10-15 Jul 2018. URL http://proceedings.mlr.
press/v80/mirman18b.html.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-
24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928-1937. JMLR.org,
2016. URL http://proceedings.mlr.press/v48/mniha16.html.
Andrew W. Moore. Efficient memory-based learning for robot control. 1990.
J. Neyman and E. S. Pearson. On the Problem of the Most Efficient Tests of Statistical Hypotheses,
pp. 73-108. Springer New York, New York, NY, 1992. ISBN 978-1-4612-0919-5. doi: 10.1007/
978-1-4612-091956. URL https://doi.org/10.10 07/97 8-1-4 612-0 919-5_6.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Elisabeth Andre, Sven Koenig, Mehdi
Dastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on Au-
tonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018,
pp. 2040-2042. International Foundation for Autonomous Agents and Multiagent Systems Rich-
land, SC, USA / ACM, 2018. URL http://dl.acm.org/citation.cfm?id=3238064.
Antonin Raffin.	Rl baselines3 zoo.	https://github.com/DLR-RM/
rl-baselines3-zoo, 2020.
Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-
mann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-
bustness to adversarial examples. In Proceedings ofthe 32nd International Conference on Neural
Information Processing Systems, NIPS’18, pp. 10900-10910, Red Hook, NY, USA, 2018. Curran
Associates Inc.
Hadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, SebaStien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp.
11289-11300, 2019.
12
Published as a conference paper at ICLR 2022
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
Ume 37 of JMLR Workshop and Conference Proceedings,pp.1889-1897.JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/schulman15.html.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lilli-
crap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering
the game of go with deep neural networks and tree search. Nat., 529(7587):484-489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen
Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforce-
ment learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/
1712.01815.
Sahil Singla and Soheil Feizi. Robustness certificates against adversarial examples for relu networks.
CoRR, abs/1902.01235, 2019.
Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks, 2020.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. CoRR, abs/2105.11417, 2021. URL
https://arxiv.org/abs/2105.11417.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference
Track Proceedings, 2014.
Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and Alexandre M. Bayen.
Robust reinforcement learning using adversarial populations. CoRR, abs/2008.01825, 2020. URL
https://arxiv.org/abs/2008.01825.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15,2018,pp. 5283-5292, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane S. Boning, and Cho-Jui Hsieh. Ro-
bust deep reinforcement learning against adversarial perturbations on observations. CoRR,
abs/2003.08938, 2020. URL https://arxiv.org/abs/2003.08938.
Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=sCZbhBvqQaU.
13
Published as a conference paper at ICLR 2022
A	Tightness of the Certificate
Here, we present a worst-case environment-policy pair that achieves the bound in Theorem 1, show-
ing that our robustness certificate is in fact tight. For a given environment M = (S, A, T, R, γ) and
a policy π, let p be a lower-bound on the probability that the total reward obtained by policy π under
Gaussian smoothing (no adversary) with variance σ2 is above a certain threshold ν, i.e.,
t
P X γi-1Ri ≥ ν ≥ p.
i=1
Let Hp be the class of all such environment-policy pairs that cross this reward threshold with proba-
bility at least p. We construct an environment-policy pair (M0, π0) that achieves the reward threshold
V with probability Φ(Φ-1(p) - B∕σ) under the structured adversary est. Note that, this does not
mean that st is the strongest possible adversary for a general environment-policy pair. It only shows
that the performance of policy π0 in environment M0 under the adversary st is a lower-bound on the
performance of a general environment-policy pair under a general adversary. Consider a one-step
game with environment M0 = (S, A, T0, R0, γ) with a deterministic observation function o of the
state-space and a policy π0 such that π0 returns an action a1 ∈ A if the first coordinate of o(s1) + η1
is at most ω = {o(s1)}1 + σΦ-1(p) and another action a2 ∈ A otherwise. Here {o(s1)}1 represents
the first coordinate ofo(s1). The environment offers a reward ν if the action in the first step is a1 and
0 when it is a2 . The game terminates immediately. The probability of the reward being above ν is
equal to the probability of the action being a1 . When η1 is sampled from the Gaussian distribution,
this probability is equal to Φ((ω - {o(s1)}1)∕σ) = p. Therefore, (M0, π0) ∈ Hp. Under the pres-
ence of the structured adversary st defined in Section 4.1, this probability after smoothing becomes
Φ((ω - {o(s1)}1 - B)∕σ) = Φ(Φ-1(p) - B∕σ), which is same as the bound in Theorem 1.
B Static Vs. Adaptive Setting
In this section, we illustrate the difference between the adversarial distributions in the static setting
and the adaptive setting. Naively, one might assume that smoothing-based robustness guarantees
can be applied directly to reinforcement learning, by adding noise to observations. For example, it
seems plausible to use Cohen et al.’s `2 certificate Cohen et al. (2019), which relies on the overlap
in the distributions of isometric Gaussians with different means, by simply adding Gaussian noise
to each observation (Figure 1-a). However, as we demonstrate with a toy example in Figure 1-
(b-d), the Cohen et al. certificate cannot be applied directly to the RL setting, because adding
noise to sequential observations does not result in an isometric Gaussian distribution over the space
of observations. This is because the adversarial offset to later observations may be conditioned
on the noise added to previous observations. In 1-(b-d), we consider a two-step episode, and for
simplicity, we consider a case where the ground-truth observations at each step are fixed. At step 1,
the noised distributions of the clean observation o1 and the adversarially-perturbed observation o01
are both Gaussians and overlap substantially, similar to in the standard classification setting (panel
b). However, we see in panel (c) that the adversarial perturbation 2 added to o2 can depend on the
smoothed value of o01 . This is because the agent may leak information about the observation that
it receives after smoothing (o1 + η1 ) to the adversary, for example through its choice of actions.
After smoothing is performed on o2, the adaptive nature of the adversary causes the distribution of
smoothed observations to no longer be an isometric Gaussian in the adversarial case (panel d). The
standard certification results therefore cannot be applied.
C	NEYMAN—Pearson lemma [1993] IN Smoothing
In the context of randomized smoothing, the Neyman-Pearson lemma produces the worst-case deci-
sion boundary ofa classifier based on the estimated probability of the top class under the smoothing
distribution. It says that this boundary is a region where the ratio of the probability density functions
of the smoothing distributions at the clean input and the perturbed input is a constant. When the two
distributions are isometric Gaussians, as is the case in static settings like image classification, this
boundary takes the form of a hyper-plane (see Appendix A of Cohen et al. (2019)). However, in the
dynamic setting of RL, the smoothing distribution after adding the adversarial perturbation may not
14
Published as a conference paper at ICLR 2022
be isometric even if the smoothing noise at each time-step was sampled from an isometric Gaus-
sian distribution (see figure 1, section ‘Technical contributions’ and Appendix A). So, we formulate
and prove an adaptive version of the Neyman-Pearson lemma to obtain provable robustness in RL
through randomized smoothing.
D	Proof of Lemma 1
Statement: For any general adversary and an Γ ⊆ (T × Rd)t, there exists a deterministic adver-
sary dt such that,
P[Y dt ∈ Γ] ≤ P[Y ∈ Γ],
where Y dt is the random variable for the distribution defined by the adversary dt.
Proof. Consider a time-step j such that ∀i < j, i is a deterministic function of τ1, η1 , τ2,
η2, . . . , τi-1, ηi-1,τi. Let H = {z | z1 = τ1, z2 = η1, z3 = τ2, z4 = η2, . . . ,z2j-1 = aj} be the
set of points whose first 2j- 1 coordinates are fixed to an arbitrary set of values τ1, η1, τ2, η2, . . . , τj.
In the space defined by H, 1, . . . , j-1 are fixed vectors in Rd and j is sampled from a fixed dis-
tribution over the vectors with '2-norm at most Bj Let YH be the random variable representing the
distribution over points in H defined by the adversary for which j = γ, such that kγ k2 ≤ Brj. De-
fine an adversary 0, such that, 0i = i, ∀i 6= j. Set 0j to the vector γ that minimizes the probability
that YHγ assigns to Γ ∩ H, i.e.,
0j= arg min P[YHγ ∈ Γ ∩ H]
kγk2≤Brj
The adversary 0 behaves as up to step j - 1. At step j, it sets 0j to the γ that minimizes the
probability it assigns to Γ ∩ H, based on the values τ1, η1, τ2, η2, . . . , τj. After that, it mimics till
the last time-step t. Therefore, for a given tuple (z1, z2, . . . , z2j-1) = (τ1, η1, τ2, η2, . . . , τj),
P[YH0j ∈ Γ∩H] ≤ P[Y ∈ Γ∩H]
Since both adversaries are same up to step j - 1, their respective distributions over z1, z2, . . . , z2j-1
remains same as well. Therefore, integrating both sides of the above inequality over the space of all
tuples (z1, z2, . . . , z2j-1), we have:
P[YHj ∈ Γ ∩ H]pY (z1 , z2, . . . , z2j -1)dz1 dz2 . . . dz2j-1
≤	P[Y ∈ Γ ∩ H]pY (z1 , z2, . . . , z2j -1)dz1 dz2 . . . dz2j-1
=⇒ P[Y0 ∈ Γ] ≤ P[Y ∈ Γ],
where Y0 is the random variable corresponding to 0 . Thus, we have constructed an adversary where
the first j adversarial perturbations are a deterministic function of the τi s and ηi s of the previous
rounds. Applying the above step sufficiently many times we can construct a deterministic adversary
dt represented by the random variable Ydt such that
P[Y dt ∈ Γ] ≤ P[Y ∈ Γ].
□
E Proof of Lemma 3
Lemma 3 states that the structured adversary characterises the worst-case scenario. Before proving
this lemma, let us first show that any deterministic adversary can be converted to one that uses up the
entire budget of B without increasing the probability it assigns to h being one in the worst-case. For
each step i, let Us define a used budget Bu = k(s,⑦，...，G-1)∣∣2 as the norm of the perturbations
of the previous steps and a remaining budget Br = BB2 - (Bu)2 as an upper-bound on the norm
of the perturbations of the remaining steps. Note that, Bu1 = 0 and Br1 = B .
Consider a version edt of the deterministic adversary that uses up the entire available budget B by
scaling up Edt such that its norm is equal to Br, i.e., setting it to EdtBr/∣kdtk2. Let Ydt be the
random variable representing Edt.
15
Published as a conference paper at ICLR 2022
Lemma. If Γγdt = {z ∈ (TX Rd)t | μγdt (Z) ≤ qμχ (z)} for some q ≥ 0 and P[h(X) = 1] ≥
P[X ∈ Γγ dt] ,then P[h(Y dt) = 1] ≥ P[Ydt ∈ Γγ 勰].
Proof. Consider Γγdt = {z ∈ (T × Rd)t | μγdt (Z) ≤ q0μχ(z)} for some q0 ≥ 0, such that,
P[X ∈ ΓY dt] = p for some lower-bound p on P[h(X) = 1]. Then, by the Neyman-Pearson Lemma
we have that,
P[h(Ydt) = 1] ≥ P[Y dt ∈ Γγdt].
Now consider a space H in (T × Rd)t where all but the last element of the tuple Z are fixed, i.e.,
H = {Z | Z1 = τ1, Z2 = η1, Z3 = τ2, Z4 = η2, . . . , Z2t-1 = τt} Since, dt is a deterministic
adversary where each idt is a deterministic function of the previous τis and ηis, each idt is also
fixed in H. Therefore, in H, both μχ and μγdt are two isometric Gaussians in the space of the @s
and the set H ∩ Γγdt is a hyperplane. The probability assigned byYdt to H ∩ Γγdt is proportional
to the distance of the center of the corresponding Gaussian. In the construction of edt, this distance
can only increase, therefore,
P[Ydt ∈ Γγdt] ≥ P[Ydt ∈ Γγdt]
Now, consider a function hΓ dt (Z) which outputs one if Z ∈ Γγdt and zero otherwise. Construct
the set Γγdt = {z ∈ (T × Rd)t | μγdt (Z) ≤ qμχ(z)} for some q ≥ 0 such that,
PX ∈ Γγdt]= P = P[hrγdt (X) = 1].
Then, by the Neyman-Pearson Lemma, we have,
P[hrγdt (Ydt) = 1] ≥ P[Ydt∈ Γγdt]
or, P Ydt ∈ Γγ dt ] ≥ P[Ydt ∈ Γγ dt ]	(from definition of hr ^t)
or, P[Ydt ∈ Γγdt] ≥ P[Ydt∈ Γγdt]
or, P[h(Ydt) = 1] ≥ P[Ydt ∈ Γγdt],	(from the above two inequalities)
proving the statement of the lemma.	□
Now, we prove lemma 3 below:
Statement: If ΓγSt = {z ∈ (T × Rd)t | μγSt (z) ≤ qμχ(z)} for some q ≥ 0 and P[h(X) = 1] ≥
P[X ∈ Γγst], then P[h(Y dt) = 1] ≥ P[Y st ∈ Γγst].
Proof. Construct the set Γγ dt as defined in the above lemma fora q ≥ 0 such that P[X ∈ Γγ dt ] = p,
for some lower-bound p on P[h(X) = 1]. Then,
P[h(Ydt) = 1] ≥ P[Ydt ∈ Γγdt]
Now consider the structured adversary st in which s1t = (B, 0, . . . , 0) and ist = (0, 0, . . . , 0) for
i > 1. Define the set ΓγSt = {z ∈ (T × Rd)t | μγSt (z) ≤ qμχ(z)} for the same q as above. Then,
we can show that:
1.	P[Ydt ∈ Γγdt] = P[Yst ∈ Γγst], and
2.	PX ∈ Γγdt] = PX ∈ Γγst]
which, in turn, prove the statement of the lemma.
Let N and Ni represent Gaussian distributions centered at origin and i respectively. Then, we can
write μχ and μγ as below:
t
μχ(Z) = ∏μτi(Ti | τ1,η1,τ2,η2,...,τi-1,ηi-1)μN(6)
i=1
t
μγdt(Z) = Yfμτi(Ti | τ1,η1,τ2,η2,...,τi-ι,ηi-ι)μNεdt(η)
i=1	i
16
Published as a conference paper at ICLR 2022
where μτi is the conditional probability distribution of token Ti given the previous tokens and offsets.
Therefore,
μYdt (Z)	YY μN吟 Si)	YY nd-itFSiT)
μχ(Z)	i=1 μN (ηi)	U e
μS⅛ ≤ q 0
t
X 2ηT浮-评)τ贽 ≤ 2σ2 ln q
i=1
Consider a round j ≤ t such that edt = 0,∀i > j + 1 and 留 +1 = (Br+1,0,..., 0). We can
always find such a j as We always have 驾+1 = (B；+1,0,..., 0), since Br+ = 0. Note that,
Br+1 =，B2 -(BU+1) and in turn 号+1 are functions of T1,η1,τ2,η2,... ,Tj and not τ7-+ι.
Let H = {Z | Z1 = τ1, Z2 = η1, Z3 = τ2, Z4 = η2, . . . , Z2j -1 = τj } be the set of points whose
first 2j - 1 coordinates are fixed to an arbitrary set of values T1, η1, T2, η2, . . . , Tj. For points in
H, all ∈dt for i ≤ j + 1 are fixed and for i > j + 1 are set to zero. Let YH denote the random
variable representing the distribution of points in H defined by the adversary edt (corresponding
random variable Y dt). In the space of ηj, ηj+1, . . . , ηt, this is an isometric Gaussian centered at
(6jt, Wj+1,0,..., 0). Therefore, Γ ∩ H is given by
j+1
X 2ηiτ Widt - (Widt)τWidt ≤ 2σ2lnt
i=1
or, ηjτ Wjdt + ηjτ+1 Wjd+t 1 ≤ β,	(3)
for some constant β dependent on η1, W1dt, . . . , ηj-1, Wjd-t 1, σ and t. The probability assigned by the
Gaussian random variable YH to the half-space defined by (3) is proportional to the distance of the
center of the Gaussian from the hyper-plane in (3), which is equal to:
IIWdtl∣2 + |哨 1k2- β = (Br)2 - β
qkwjtk2+kwj+ιk2	Bj ,
where the equality follows from:
IWjdtI2 + IWjd+t 1I2 = IWjdtI2 + (Brj+1)2
= IWjdtI2 +B2 - (Buj+1)2	(from definition of Bri)
=IWjdtI2+B2-(IW1dtI2+IW2dtI2+...+IWjdtI2)
= B2 - (IWd1tI2 + IW2dtI2 +... + IWjd-t 1I2)
=B2-(Buj)2=(Brj)2.
Now, consider an adversary Wdt0 such that Widt0 = Widt, ∀i ≤ j - 1, Wjdt0 = (Brj, 0, . . . , 0), and
Wdt = 0, ∀i > j. Let Ydt0 be the corresponding random variable. Define Γγd” similar to Γγdt.
Then, Γγdt ∩ H is given by
ηjτ(Brj,0,...,0) ≤β,	(4)
which is obtained by replacing Wjdt with (Brj, 0, . . . , 0) and Wjd+t 1 with (0, 0, . . . , 0) in inequality (3)
about the origin. Define YWHdt0 similar to YWHdt, and just like YWHdt, the distribution of YWHdt0 is also an
isometric Gaussian, but is centered at ((Brj, 0, . . . , 0), (0, 0, . . . , 0)). The probability assigned by
this Gaussian distribution to Γγ dt，∩H is proportional to the distance of its center to the hyper-plane
defining the region in (4), which is equal to ((Br)2 - β)/Bj. Therefore,
P[Ydt ∈ Γydt ∩H] = P[Ydt0 ∈ γ Y dt0 ∩ H].
The key intuition behind this step is that, for isometric Gaussian smoothing distribution, the worst-
case probability assigned by the adversarial distribution only depends on the magnitude of the per-
turbation and not its direction. Figure 6 illustrates this property for a two-dimensional input space.
17
Published as a conference paper at ICLR 2022
Figure 6: General adversarial perturbation vs. perturbation aligned along the first dimension. Blue
and red regions denote where the worst-case function is one and zero respectively.
Since both adversaries are same UP to step j - 1, their respective distributions over zι, z2,..., z2j-1
remains same as well, i.e., PY dt (z1,z2,..., z2j-1) = PY dt，(zι, z2,..., z2j-1). Integrating over the
space of all tuples (zι, z2,..., z2j-1), We have:
/ P[YYdt ∈ Γγ dt ∩ H]pγ dt (zι, z2,..., z2j-1)dz1dz2 .. .dz2j-1
=/ P[YYdt0 ∈ Γγ dt0 ∩ H]pγ dt，(zi, Z2,. .., Z2j-l)dz∖d,Z2 . . .dz2j-i
=⇒ P[Y¾t ∈ Γγdt] = P[YHt0 ∈ Γγdt，],
Since the distribution defined by X (with no adversary) over the space of η sis a Gaussian centered
at origin whose distance to both Γγdt ∩ H and Γγdt，∩ H is the same (equal to -β∕Bj), it assigns
the same probability to both (3) and (4). Therefore,
PX ∈ Γγdt] = PX ∈ Γγdt，].
Thus, We have constructed an adversary with one less non-zero e〃 Applying, this step sufficiently
many times we can obtain the adversary est such that,
P[Ydt ∈ Γγdt] = P[Yst ∈ Γγst] and PX ∈ Γγ一 = PX ∈ Γγst]
which completes the proof.	□
F Proof of Theorem 1
Statement: For an isometric Gaussian smoothing noise with variance σ2, if P[h(X) = 1] ≥ P,
then:
P[h(Y) = 1] ≥ Φ(Φ-1(p) - Bm.
Proof. Define Γγ = {z ∈ (TX Rd)t | μγ(Z) ≤ qμχ(z)} for an appropriate q such that PX ∈
ΓY] = P. Then, by the Neyman-Pearson lemma, we have P[h(Y ) = 1] ≥ P[Y ∈ ΓY]. Applying
lemma 1, we know that there exists a deterministic adversary edt represented by random variable
Y dt, such that,
P[h(Y ) = 1] ≥ P[Y ∈ Γγ] ≥ P[Y dt ∈ Γγ].	(5)
Now define a function hrγ (Z) = 1{z∈Γγ} and a set Γγdt = {z ∈ (T × Rd)t | μγdt (Z) ≤ q0μχ (z)}
for an appropriate q0 > 0, such that, P[X ∈ Γγdt] = P[hΓY (X) = 1] = P. Applying the Neyman-
18
Published as a conference paper at ICLR 2022
Pearson lemma again, we have:
P[hΓY (Y dt) = 1] ≥ P[Y dt ∈ ΓYdt]
or,	P[Y dt ∈ ΓY] ≥ P[Y dt ∈ ΓYdt]
or,	P[h(Y) = 1] ≥ P[Y dt ∈ ΓYdt]
(from definition of hΓY )
(from inequality (5))
Define hΓ dt (z) = 1{z∈Γ dt}. For the structured adversary st represented by Y st, define ΓYst =
{z ∈ (TX Rd)t | μγst (Z) ≤ q00μχ(z)} for an appropriate q00 > 0, such that, P[X ∈ ΓγSt]=
P[hΓ dt (X) = 1] = p. Applying lemma 3, we have:
P[hΓYdt(Ydt)=1] ≥P[Yst∈ΓYst]
P[Y dt ∈ ΓYdt] ≥ P[Y st ∈ ΓYst]	(from definition of hΓ dt)
P[h(Y) = 1] ≥ P[Y st ∈ ΓYst]	(since P[h(Y) = 1] ≥ P[Y dt ∈ ΓYdt])
ΓYst is defined as the set of points z which satisfy:
μγSt(Z) ≤ 00
μx (Z) - q
η1T(B,0,...,0) ≤β
or,
or,
μNest (ηι)	„
「≤ q00
μN (ηι)
{η1}1 ≤ β∕B
for some constant β. This is the set of all tuples Z where the first coordinate ofη1 is below a certain
threshold γ. Since P[X ∈ ΓYSt] = p,
Φ(γ∕σ) = p =⇒ γ = σΦ-1 (p).
Therefore,
P[Yst ∈ Γyst] = Φ (Y-B) = Φ(Φ-1(p) - B∕σ).
□
G	Additional Cartpole Results
We performed two additional experiment on Cartpole: we tested at larger noise levels, (σ = 0.6 and
0.8) and we tested a variant of the agent architecture. Specifically, in addition to the agent shown
in the main text, which uses five frames of observation, we also tested an agent which uses only a
single frame. Unlike the Atari environment, the task is in fact solvable (in the non-adversarial case)
using only one frame: the observation vector represents the complete system state. We computed
certificates for the policy-smoothed version of this model, and tested attacks on the undefended
version. (We did not test attacks on the smoothed single-frame variant). As we see in Figure 7, we
achieve non-vacuous certificates in both settings (i.e, at large perturbation sizes, the smoothed agent
is guaranteed to be more robust than the empirical robustness of a non-smoothed agent). However,
observe that the undefended agent in the multi-frame setting is much more vulnerable to adversarial
attack. This is likely because the increased number of total features (20 vs. four) introduces more
complexity of the Q-network, making it more vulnerable against adversarial attack.
H	Full Pong Game
In Figure 8, we explore a failure case of our technique: we fail to produce non-vacuous certificates
for a full Pong game, where the game ends after either player scores 21 goals. In particular, while,
for the one-round Pong game, the smoothed agent is provably more robust than the empirical perfor-
mance of the undefended agent, this is clearly not the case for the full game. To understand why our
certificate is vacuous here, note that in the the “worst-case” environment that our certificate assumes,
any perturbation will (maximally) affect all future rewards. However, in the multi-round Pong game,
each round of the game is only loosely coupled to the previous rounds (the ball momentum - but
not position - as well as the paddle positions are retained). Therefore, any perturbation can only
have a very limited effect on the total reward. Another way to think about this is to recall that in
smoothing-based certificates, the noise added to each feature is proportional to the total perturbation
budget of the adversary. In this sort of serial game, the perturbation budget required to attack the
average reward scales with the (square root of the) number of rounds, but the noise tolerance of the
agent does not similarly scale.
19
Published as a conference paper at ICLR 2022
(a) CartPoIe (MUItiframe)
(b) CartPe)Ie (SingIe Frame)
Perturbation Budget
Perturbation Budget
Figure 7: Additional Cartpole results. Attacks on smoothed agents at all σ for the multiframe agents
are presented in Appendix J
(a) Pong (One-RC)Und)_______________ ________________________(b) POng (FUlI)
Figure 8: Results for the Full Pong game, compared to the single-round game.
I	Training and Clean Test Results
In Figure 9, we present the clean (non-attacked) test performance for the experiments presented in
the main text, as a function of the smoothing noise σ.
In Figure 10, we present the clean training (i.e., validation round) performance as a function of the
training time step and the smoothing noise σ. Note that early stopping was applied: the model from
the best validation round was kept, and only replaced if a strictly better validation performance was
recorded later.
•	For Cartpole: logs were not kept after the first time an evaluation round had a perfect
average score of 200 (this is because the “best model” was saved for this evaluation, and it
would be impossible to beat this score, so training was not continued). However, for other
tasks (i.e. mountain car) logs continued after a perfect evaluation round.
•	For Freeway: as mentioned in Appendix Section L, we trained 5 times at each noise level,
and kept the best of all 5 models. All 5 training curves are shown here for each noise level.
J Complete Attack Results
In Figures 11 and 12, we report the empirical robustness under attack for all tested values of λQ :
in the main text, we show only the result for the λQ that represents the strongest attack. Figure 12
also shows the attacks on smoothed agents for all smoothing noises. All attack results are means
over 1000 episodes (except for Mountain Car results, where 250 episodes were used) and error bars
represent the standard error of the mean.
20
Published as a conference paper at ICLR 2022
(a) Pong (One-Re)Und)
Figure 9: Clean test performance as a function of smoothing noise σ .
Pong (1 round) Training Performance
Figure 10: Clean training performance as a function of smoothing noise σ and training step.
Time Step
21
Published as a conference paper at ICLR 2022
(b) FUll POng
-f∙- Undefended Empirical: A_Q = 0.0
T- Undefended Empirical:入_Q = 0.06
Undefended Empirical:入 Q = 0.12
Perturbation Budget
Figure 11: Empirical robustness of undefended agents on for all tested values of λQ (or λ). The
results in the main text are the pointwise minima over λ of these curves.
22
Published as a conference paper at ICLR 2022
Figure 12: Empirical robustness of smoothed agents on for all tested values of σ and λQ (or λ). We
also plot the associated certificate curves.
K	Empirical Attack Details
Our empirical attack on (undefended) RL observations for DQN is described in Algorithm 1. To
summarize, the core of the attack is a standard targeted L2 PGD attack on the Q-value function.
However, because we wish to “save” our total perturbation budget B for use in later steps, some
modifications are made. First, we only target actions a for which the clean-observation Q-value is
sufficiently below (by a gap given by the parameter λQ) the Q-value of the ‘best’ action, which would
be taken in the absence of adversarial attack. Among these possible Targets, we ultimately choose
whichever action will maximally decrease the Q-value, and which the agent can be successfully
be induced to choose within the adversarial budget B . If no such action exists, then the original
observation will be returned, and the entire budget will be saved. In order to preserve budget, the
PGD optimization is stopped as soon as the “decision boundary” is crossed.
We use a constant step size η . In order to deal with the variable budget B , we optimize of a number
of iterations which is a constant multiple V of B.
For most environments, there is some context used by the Q-value function (i.e, the previous frames)
which is carried over from previous steps, but is not directly being attacked in this round. We need
both the clean version of the context, C , in order to evaluate the “ground-truth” values of the Q-
value function under various actions; as well as the “dirty” version of the context, C0 , based on
the adversarial observations which have already been fed to the agent, in order to run the attack
optimization.
Our attack for DDPG is described in Algorithm 2. Here, we use the policy π to determine what
action a the agent will take when it observes a corrupted observation o0 (with corrupted context
C0), and use the Q-value function supplied by the DDPG algorithm to determine the “value” of that
action on the ground-truth observation o. Because our goal is to minimize this value, this amounts
to minimizing Q(C; o, π(C0; o0)). In order to ensure that a large amount ofL2 “budget” is only used
when the Q value can be substantially minimized, we include a regularization term λko - o0 k22 .
Attacks on smoothed agents are described in Appendix M.
23
Published as a conference paper at ICLR 2022
	1-Round Pong	Full Pong	Multiframe Cartpole	Single-frame Cartpole	Freeway
Training discount factor Y	-0:99-	-099-	0.99	099	-099-
Total timesteps	10000000	10000000	-500000-	-500000-	10000000
Validation interval (steps)	100000	100000	2000	2000	100000
Validation episodes	100	10	10	10	100
Learning Rate	0.0001	0.0001	-0.0001-	-0.00005	0.0001
DQN Buffer Size	-10000-	-10000-	-100000-	-100000-	-10000-
DQN steps collected before learning	100000	100000	1000	1000	100000
Fraction of steps for exploration (linearly decreasing exp. rate)	01	01	0Γ6	0TΓ6	0.1
Initial exploration rate	1	1	1	1	1
Final exploration rate	-0:01-	-0:01-	0	0	-0:01-
DQN target update interval (steps)	-1000-	-1000-	10	10	-1000-
Batch size	32	32	1024	1024	32
Training interval (steps)	4	4	256	256	4
Gradient descent steps	1	1	128	128	1
Frames Used	4	4	5	1	4
Training Repeats	1	1	1	1	5
Architecture	-CNN*-	-CNN*-	MLP 20 × 256× 256× 2	MLP 4× 256 × 256 × 2	-CNN*-
Table 1: Training Hyperparameters for DQN models. *CNN refers to the 3-layer convolutional
network defined by the CNNPolicy class in stable-baselines3 (Raffin et al., 2019), based on the
CNN architecture used for Atari games by Mnih et al. (2015). Note that hyperparameters for Atari
games are based on hyperparameters from the stable-baselines3 Zoo package (Raffin, 2020), for a
slightly different (more deterministic) variant of the Pong environment.
Note that on image data (i.e., Pong), we do not consider integrality constraints on the observations;
however, we do incorporate box constraints on the pixel values. We also incorporate box constraints
on the kinematic quantities when attacking Mountain Car, but not when attacking Cartpole: the
distinction is that the constraints in Mountain Car represent artificial constraints on the kinematics
[i.e., the velocity of the car is arbitrarily clipped], while the constraints in Cartpole arise naturally
from the problem setup.
L Environment details and Hyperparameters
For Atari games, we use the “NoFrameskip-v0” variations of these environments with the standard
“AtariPreprocessing” wrapper from the OpenAI Gym (Brockman et al., 2016) package: this provides
This environment also injects non-determinism into the originally-deterministic Atari games, by
adding randomized “stickiness” to the agent's choice of actions - without this, the state-observation
robustness problem could be trivially solved by memorizing a winning sequence of actions, and
ignoring all observations at test-time.
Due to instability in training, for the freeway environment, we trained each model five times, and
selected the base model based on the performance of validation runs. See training hyperparameters,
Tables 1 and 2. For attack hyperparameters, see Table 3 and 4.
24
Published as a conference paper at ICLR 2022
	Mountain Car
Training discount factor Y	0.99
Total timesteps	300000
Validation interval (steps)	2000
Validation episodes	10
Learning Rate	0.0001
DDPG Buffer Size	1000000
-DDPG steps collected- before learning	100
Batch size	100
Update coefficient T	0.005
Train frequency	1 per episode
Gradient steps	=episode length
Training action noise	Ornstein Uhlenbeck (σ = 0.5)
Architecture	MLP 2 X 400 X 300 XT~~
Table 2: Training Hyperparameters for DDPG models. Hyperparameters are based on hyperpa-
rameters from the stable-baselines3 Zoo package (Raffin, 2020), for the unmodified Mountain Car
environment.
	1-Round Pong	Full Pong	Multiframe Cartpole	Single-frame Cartpole	Freeway
Attack step size η	-0:01-	001	0.01	0.01	-0:01-
Attack step multiplier V	2	2	2	2	2
Q-value thresholds Xq searched	.1,.3,.5	.1,.3,.5, .9,1.3,1.7	4,6,8,10	0TM .1,1	0,.06,.12
Table 3: Attack Hyperparameters for DQN models.
	Mountain Car
Attack step size η	0.01
Attack steps T	100
Regularization values λ searched	.001,.0001,.00001
Table 4: Attack Hyperparameters for DDPG models.
25
Published as a conference paper at ICLR 2022
Algorithm 1: Empirical Attack on DQN Agents
Input: Q-ValUe function Q, clean prior observation context C, adversarial prior observation
context C0, observation o, budget B, Q-value threshold λQ, step size η, step multiplier ν
Output: Attacked observation oworst, remaining budget B0 .
Qclean := maxa∈A Q(C; o, a)
Targets := {a ∈ A|Q(C; o,a) ≤ Qclean - λQ}
Qworst := Qclean
oworst := o
for a ∈ Targets do
o0 := o
inner:
for i in 1,..., bVBC do
if arg maxa0 Q(C0; o0, a0) = a then
if Q(C; o, a) < Qworst then
oworst := o
Qworst ：= Q(C； 0,a)
end
break inner
end
D ：= Voo log([SoftMax(Q(C0; o0, ∙)]a)
o0 := o0 + kD⅛
if ko0 - ok2 > B then
o0 := o + o(o0 - o)
I	llo0-ok2v	)
end
end
end
return θworst, ,B2 - Iloworst - ok2
Algorithm 2: Empirical Attack on DDPG Agents
Input: Q-value function Q, policy ∏, clean prior observation context C, adversarial prior
observation context C0, observation o, budget B, weight parameter λ, step size η, step
count τ
Output: Attacked observation oworst, remaining budget B0 .
o0 := o
for i in 1, ..., τ do
D = Vo0 [Q(C; o,∏(C0; o0)) + λko0-ok2]
if 既 ≤ 0.001 then
I break
end
o0:= o0+隹
if Io0 - oI2 > B then
o0 := o +	0^(o0 — o)
I	ko0-οk2' J
end
end
return o0, PB - ko0 - o∣2
26
Published as a conference paper at ICLR 2022
M	Attacks on Smoothed Agents
In order to attack smoothed agents, we adapted Algorithms 1 and 2 using techniques suggested by
Salman et al. (2019) for attacking smoothed classifiers. In particular, whenever the Q-value function
is evaluated or differentiated, we instead evaluate/differentiate the mean output under m = 128
smoothing perturbations. Following Salman et al. (2019), we use the same noise perturbation vectors
at each step during the attack. In the multi-frame case, for the “dirty” context C0 , we include the
actually-realized smoothing perturbations used by the agents for previous steps. However, when
determining the “clean” Q-values Q(C; o, a), for the “clean” context C, we use the unperturbed
previous state observations: we then take the average over m smoothing perturbations of both C
and o to determine the clean Q-values. This gives an unbiased estimate for the Q-values of an
undisturbed smoothed agent in this state.
When attacking DDPG, in evaluating Q(C; o, π(C0; o0), we average over smoothing perturbations
for both o and o0 , in addition to C : this is because both π and Q are trained on noisy samples. Note
that we use independently-sampled noise perturbations on o0 and o.
Our attack does not appear to be successful, compared with the lower bound given by our certificate
(Figures 12). One contributing factor may be that attacking a smoothed agent is more difficult
that attacking a smoothed classifier, for the following reason: a smoothed classifier evaluates the
expected output at test time, while a smoothed agent does not. Thus, while the average Q-value
for the targeted action might be greater than the average Q-value for the clean action, the actual
realization will depend on the specific realization of the random smoothing vector that the agent
actually uses.
N Runtimes and Computational Environment
Each experiment is run on an NVIDIA 2080 Ti GPU. Typical training times are shown in Table 5.
Typical clean evaluation times are shown in Table 6. Typical attack times are shown in Table 7.
Experiment	Time (hours)
Pong (1-round)	Tn
Pong (Full)	12.0
Cartpole (Multi-frame)	0.27
Cartpole (Single-frame)	0.32
Freeway	14.2
Mountain Car	0.63
Table 5: Training times
Experiment Experiment	Time (seconds): smallest noise σ	Time (seconds): largest noise σ
Pong (1-round)	046	0.38
Pong (Full)	3.82	4.65
Cartpole (Multi-frame)	0.20	0.13
Cartpole (Single-frame)	0.18	0.12
Freeway	1.36	1.35
Mountain Car	0.67	0.91
Table 6: Evaluation times. Note that the times reported here are per episode: in order to statistically
bound the mean rewards, we performed 10,000 such episode evaluations for each environment.
O CDF Smoothing Details
Due to the very general form of our certification result (h(∙), as a 0/1 function, can represent any
outcome, and we can bound the lower-bound the probability of this outcome), there are a variety of
27
Published as a conference paper at ICLR 2022
Experiment Experiment	Time (seconds): smallest budget B	Time (seconds): largest budget B
Pong (1-round)	1.01	0.68
Pong (Full)	8.84	10.2
Cartpole (Multi-frame)	0.35	0.32
Cartpole (Single-frame)	0.79	0.56
Freeway	2.67	2.80
Mountain Car	44.0	19.6
Table 7: Attack times. Note that the times reported here are per episode: in the paper, we report the
mean of 1000 such episodes.
Figure 13: Comparison of certified bounds on the total reward in Cartpole, using (a) point estimation,
and (b) the DKW inequality to generate empirical bounds.
ways we can use the basic result to compute a certificate for an entire episode entire game. In the
main text, we introduce CDF smoothing Kumar et al. (2020) as one such option. In CDF smoothing
for any threshold value x, We can define hx(∙) as an indicator function for the event that the total
episode reward is greater than x. Then, by the definition of the CDF function, the expectation of
hx (∙) is equal to 1 - F(x), where F(∙) is the CDF function of the reward. Then our lower-bound on
the expectation of hx(∙) under adversarial attack is in fact an upper-bound on F(x): combining this
with Equation 2 in the main text,
E[X]=Z∞(1-
0
F (x))dx - Z
-∞
F (x)dx,
provides a lower bound on the total expectation of the reward under adversarial perturbation.
However, in order to perform this integral from empirical samples, we must bound F(x) at all
points: this requires first upper-bounding the non-adversarial CDF function at all x, before apply-
ing our certificate result. Following Kumar et al. (2020), we accomplish this using the Dvoret-
Zky-Kiefer-Wolfowitz inequality (for the Full Pong environment.)
In the case of the Cartpole environment, we explore a different strategy: note that the reward at
each timestep is itself a 0/1 function, so we can define ht(∙) as simply the reward at timestep t.
We can then apply our certificate result at each timestep independently, and take a sum. Note that
this requires estimating the average reward at each step independently: we use the Clopper-Pearson
method (following Cohen et al. (2019)), and in order to certify in total to the desired 95% confidence
bound, we certify each estimate to (100 - 5/T)% confidence, where T is the total number of timesteps
per episode (= 200).
However, note that, in the particular case of the cartpole environment, ht(∙) = 1 if and only if
we have “survived” to time-step t: in other words, ht(∙) is simply an indicator function for the
total reward being ≥ t. Therefore in this case, this independent estimation method is equivalent
to CDF smoothing, just using Clopper-Pearson point-estimates of the CDF function rather than
the Dvoretzky-Kiefer-Wolfowitz inequality. In practice, we find that this produced slightly better
certificates for this task. (Figure 13)
28
Published as a conference paper at ICLR 2022
P Environment Licenses
OpenAI Gym Brockman et al. (2016) is Copyright 2016 by OpenAI and provided under the MIT
License. The stable-baselines3 packageRaffin et al. (2019) is Copyright 2019 by Antonin Raffin and
also provided under the MIT License.
29