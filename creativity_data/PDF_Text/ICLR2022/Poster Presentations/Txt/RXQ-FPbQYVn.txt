Published as a conference paper at ICLR 2022
Anti-Concentrated Confidence Bonuses
for Scalable Exploration
Jordan T. Ash
Microsoft Research NYC
Cyril Zhang
Microsoft Research NYC
Surbhi Goel
Microsoft Research NYC
Akshay Krishnamurthy
Microsoft Research NYC
Sham Kakade
Microsoft Research NYC
Harvard University
Ab stract
Intrinsic rewards play a central role in handling the exploration-exploitation
trade-off when designing sequential decision-making algorithms, in both
foundational theory and state-of-the-art deep reinforcement learning. The LinUCB
algorithm, a centerpiece of the stochastic linear bandits literature, prescribes an
elliptical bonus which addresses the challenge of leveraging shared information
in large action spaces. This bonus scheme cannot be directly transferred to
high-dimensional exploration problems, however, due to the computational cost
of maintaining the inverse covariance matrix of action features. We introduce
anti-concentrated confidence bounds for efficiently approximating the elliptical
bonus, using an ensemble of regressors trained to predict random noise from policy
network-derived features. Using this approximation, We obtain stochastic linear
bandit algorithms which obtain O(d√T) regret bounds for poly(d) fixed actions.
We develop a practical variant for deep reinforcement learning that is competitive
with contemporary intrinsic reward heuristics on Atari benchmarks.
1	Introduction
Optimism in the face of uncertainty (OFU) is a ubiquitous algorithmic principle for online decision-
making in bandit and reinforcement learning problems. Broadly, optimistic decision-making al-
gorithms augment their reward models with a bonus (or intrinsic reward) proportional to their
uncertainty about an action’s outcome, ideally balancing exploration and exploitation. A vast litera-
ture is dedicated to developing and analyzing the theoretical guarantees of these algorithms (Lattimore
& Szepesvdri, 2020). In fundamental settings such as stochastic multi-armed and linear bandits,
optimistic algorithms are known to enjoy minimax-optimal regret bounds.
In modern deep reinforcement learning, many approaches to exploration have been developed with the
same principle of optimism, with most empirical successes coming from uncertainty-based intrinsic
reward modules (Burda et al., 2018b; Pathak et al., 2017; Osband et al., 2016). Such bonuses can
be very useful, with prior work demonstrating impressive results on a wide array of challenging
exploration problems. Several of these methods draw inspiration from theoretical work on multi-
armed bandits, using ideas like count-based exploration bonuses. However, a related body of work on
linear bandits provides tools for extending exploration bonuses to large but structured action spaces,
a paradigm which may be appropriate for deep reinforcement learning.
The Linear UCB (LinUCB) algorithm (Auer, 2002; Dani et al., 2008; Li et al., 2010; Abbasi-Yadkori
et al., 2011) is attractive in this setting because it enjoys minimax-optimal statistical guarantees.
To obtain these, LinUCB leverages a so-called elliptical bonus, the computation of which requires
maintaining an inverse covariance matrix over action features. The principal challenge in generalizing
the elliptical potential used in bandits to the deep setting lies in computing and storing this object.
Due to the moving internal representation of the policy network, and the number of parameters used
to compose it, a naive implementation of the LinUCB algorithm would require remembering all of the
agent’s experience and constantly recomputing and inverting this matrix, which is likely extremely
large. Clearly, such an approach is too computationally intensive to be useful. As we discuss in the
1
Published as a conference paper at ICLR 2022
O 20	40	60	80 IOO 120	O 20	40	60	80 IOO 120
Relative Timestep	Relative Timestep
Figure 1: A visualization of the ACB bonus for Atari games Breakout and Q*bert. Large bonuses
often correspond to the visitation of states on the periphery of the agent,s current experience, for
example upon breaking a novel combination of blocks in Breakout or immediately before Q*bert
arrives on an unlit platform. When the agent dies, and the game is reset to a familiar state, intrinsic
reward drops precipitously.
next section, several works have used neural features for LinUCB-style bonuses, but all require an
inversion of this sort, limiting their ability to scale to high dimensions.
Towards bridging foundational algorithms with empirical frontiers, we develop a scalable strategy
for computing LinUCB,s elliptical bonus, enabling us to investigate its effectiveness as an intrinsic
reward in deep reinforcement learning. We use an ensemble of least-squares regressors to approximate
these bonuses without explicitly maintaining the covariance matrix or its inverse. Our algorithm is
both theoretically principled and computationally tractable, and we demonstrate that its empirical
performance on Atari games is often competitive with popular baselines.
1.1	Our contributions
We propose the use of anti-concentrated confidence bounds (ACB) to efficiently approximate the
LinUcB bonus. ACB estimates per-action elliptical confidence intervals by regressing random targets
on policy features. It anti-concentrates these bonuses by taking a maximum over the predictions from
an ensemble of these regressors.
First, we introduce ACB in the basic stochastic linear bandit setting. We show that these bonuses
provably approximate LinUCB,s elliptical potential; thus, optimistic exploration with ACB directly
inherits standard analysis techniques for LinUCB. We derive near-optimal high-probability regret
bounds for ACB, when the size of the action space is polynomial in the action feature dimension. We
derive sufficient ensemble sizes for the special cases of multi-armed bandits and fixed actions, as well
as the general case of changing actions. These follow from lower tail bounds for the maximum of inde-
pendent Gaussians; we conjecture that they are improvable using more sophisticated analytical tools.
The main contribution of this work is empirical: we find that ACB provides a viable exploration
bonus for deep reinforcement learning. After defining a suitable nonlinear analogue using action
features from the policy network, we demonstrate that the intrinsic rewards produced by ACB are
competitive with those from state-of-the-art algorithms in deep RL on a variety of Atari benchmarks
(Figure 1). To the best of our knowledge, our work is the first to scalably study bonuses from the
linear bandits literature in these deep reinforcement learning settings.
1.2	Related work
Linear bandits. Stochastic linear bandits were first introduced by Abe & Long (1999). Optimistic
algorithms are fundamental in this setting (Auer, 2002; Dani et al., 2008; Li et al., 2010; Rusmevichien-
tong & Tsitsiklis, 2010; Abbasi-Yadkori et al., 2011), and provide minimax-optimal regret bounds.
2
Published as a conference paper at ICLR 2022
Several works are concerned with designing more scalable optimism-based algorithms, with a focus
on empirical bandit settings (as opposed to deep RL). Jun et al. (2017) consider streaming confidence
bound estimates (obtaining per-iteration update costs independent of t) in the generalized linear
bandits model, but still incur the cost of maintaining the inverse covariance matrix via online Newton
step updates. Ding et al. (2021) give strong regret guarantees for epoch-batched SGD with Thompson
sampling, under more stringent assumptions (i.i.d. action features, and a “diversity” condition to
induce strong convexity). Korda et al. (2015) demonstrate that variance-reduced algorithms succeed
in practice for estimating the LinUCB bonus.
Most closely related to our work, Kveton et al. (2019); Ishfaq et al. (2021) study PertUrbed
regressions for exploration, obtaining similar regret bounds to ours (including the suboptimal √log A
factor in the regret bounds). The LinPHE algorithm in (Kveton et al., 2019) uses a single random
regressor’s constant-probability anti-concentration to approximate the elliptical bonus. In work
concurrent to ours, the main algorithm proposed by Ishfaq et al. (2021) also takes a maximum
over random regressors, and plugs their analysis into downstream end-to-end results for linear
MDPs. The algorithms presented in these works resemble the “always-rerandomizing” variant of
ACB (Algorithm 1), which is still not suitable for large-scale deep RL. Our work introduces the
“lazily-rerandomizing” and “never-rerandomizing” variants in an effort to close this gap, and presents
corresponding open theoretical problems.
Unlike the aforementioned works, we consider our primary contribution as empirical, with the deep
learning variant of ACB (Algorithm 2) offering performance comparable to commonly used deep
RL bonuses on a wide array of Atari benchmarks without sacrificing theoretical transparency. The
resulting algorithmic choices deviate somewhat from the bandit version (as well as LinPHE/LSVI-
PHE), towards making ACB a viable drop-in replacement for typical bonuses used in deep RL.
The use of ensembles in this work bears a resemblance to Monte Carlo techniques for posterior
sampling (Thompson, 1933; Agrawal & Goyal, 2013). This is a starting point for many empirically-
motivated exploration algorithms, including for deep RL (see below), but theory is limited. (Lu
& Van Roy, 2017) analyze an ensemble-based approximation of Thompson sampling (without
considering a max over the ensemble), and obtain a suboptimal regret bound scaling with Alog A.
Exploration in deep reinforcement learning. In deep reinforcement learning, most popular ap-
proaches use predictability as a surrogate for familiarity. The Intrinsic Curiosity Module (ICM), for
example, does this by training various neural machinery to predict proceeding states from current
state-action pairs (Pathak et al., 2017). The L2 error of this prediction is used as a bonus signal for
the agent. The approach can be viewed as a modification to Intelligent Adaptive Curiosity, but relying
on a representation that is trained to encourage retaining only causal information in transition dynam-
ics (Oudeyer et al., 2007). Stadie et al. (2015) instead use a representation learned by an autoencoder.
A large-scale study of these “curiosity-based” approaches can be found in Burda et al. (2018a).
Similar to these algorithms, Random Network Distillation (RND) relies on the inductive bias of a
random and fixed network, where the exploration bonus is computed as the prediction error between
this random network and a separate model trained to mimic its outputs (Burda et al., 2018b).
In tabular settings, count-based bonuses are often used to encourage exploration (Strehl & Littman,
2008). This general approach has been extended to the deep setting as well, where states might be
high-dimensional and infrequently visited (Bellemare et al., 2016). Algorithms of this type usually
rely on some notion of density estimation to group similar states (Ostrovski et al., 2017; Tang et al.,
2016; Martin et al., 2017).
Other work proposes more traditional uncertainty metrics to guide the exploration process. For exam-
ple, Variational Intrinsic Control (Gregor et al., 2016) uses a variational notion of uncertainty while
Exploration via Disagreement (Pathak et al., 2019) and Bootstrapped DQN (Osband et al., 2016)
model uncertainty over the predictive variance of an ensemble. More broadly, various approximations
of Thompson sampling (Thompson, 1933) have been used to encourage exploration in deep rein-
forcement learning (Osband et al., 2013; Guez et al., 2012; Zhang et al., 2020; Strens, 2000; Henaff
et al., 2019). These methods have analogues in the related space of active learning, where ensemble
and variational estimates of uncertainty are widely used (Gal et al., 2017; Beluch et al., 2018).
Separate from these, there has been some work intended to more-directly deliver UCB-inspired
bonuses to reinforcement learning (Zhou et al., 2020; Nabati et al., 2021; Zahavy & Mannor,
3
Published as a conference paper at ICLR 2022
2019; Bai et al., 2021). These efforts have various drawbacks, however, with many requiring matrix
inversions, limiting their ability to scale, or simply not being competitive with more conventional deep
RL approaches. ACB removes the covariance matrix inversion requirement, and as we demonstrate
in Section A.2, is competitive with RND and ICM on several Atari benchmarks.
2	Preliminaries
In this section, we provide a brief review and establish notation for the stochastic linear bandit setting
and the LinUCB algorithm. For a more comprehensive treatment, see Agarwal et al. (2019); Lattimore
& Szepesv^ri (2020). At round t = 1,...,T, the learner is given an action set At ⊆ A with features
{xt,a ∈ Rd}a∈At, chooses an action at ∈ At, and receives reward rt(at) := hxt,at, θ*i + εt, where
εt is a martingale difference sequence. We use A to denote maxt |At|. The performance of the
learner is quantified by its regret,
T
R(T) = X(xt,at ,θ*) - hxt,at,θ*i , at := arg max〈…，。' ∙
This setting encompasses two commonly-considered special cases:
•	Fixed actions: the action sets At are the same for all t.
•	Multi-armed bandits: fixed actions, with At = [d] and xt,a = ea. Here, θt is the vector of
per-arm means.
An OFU-based algorithm plays at round t
at := arg max {rt(a) + bonust(a)},
a∈At
where rt(a) is an estimate for the reward rt(a), and bonust(a) is selected to be a valid upper confi-
dence bound for rt(a), such that E[rt(a)] ≤ rt(a) + bonust(a) with high probability. The LinUCB
algorithm uses ^t(a):= (xt,a ,θ), where θt = argmi□θ∈Rd PT=I (hxτ% ,θ - &3))2 + λ∣∣θk2
is the 2-regularized least-squares regressor for the reward, and sets bonust(a) = β Jχ>0Σ-1χt,a,
where Σt := λI + PT=I xt,at xt>,a . In an online learning context, this is known as the elliptical
potential (Cesa-Bianchi & Lugosi, 2006; Carpentier et al., 2020). We will quantify the theoretical
performance of ACB by comparing it to LinUCB. Theorem 1 gives a high-probability regret bound
for LinUCB, which is optimal up to logarithmic factors:
Theorem 1 (LinUCB regret bound; Theorem 5.3, (Agarwal et al., 2019)). If ∣∣θt∣∣, ∣∣xt,a || ≤ O(1),
and each εt is O(1)-subgaussian, then there are choices of λ, β such that, with probability at least
1 - δ, the regret of the LinUCB algorithm satisfies
R(T) ≤ O (d√T).
The O(∙) suppresses factors polynomial in log(1∕δ) and log T.
LinUCB can be implemented in O(d2∣At∣) time per iteration, by maintaining Σ-1 using rank-1
1	t-1
updates, and using this to compute the least-squares estimator θt = Σt-1	τ=1 xτ,aτ rτ (aτ) as well
as bonust (a).
3	Anti-concentrated confidence bounds for linear bandits
We begin by presenting the ACB intrinsic reward for the stochastic linear bandit setting. It is
well-known that the weights of a least-squares regressor trained to predict i.i.d. noise will capture
information about about the precision matrix of the data. Our key idea is to amplify the deviation in
the predictions of an ensemble of such regressors by taking a maximum over their outputs, allowing
us to construct upper confidence bounds for rt(a). ACB maintains M linear regressors trained to
4
Published as a conference paper at ICLR 2022
a∂J6,uccφ>4-nEro
Figure 2: The cumulative regret
for ACB (incremental sampling)
with different ensemble sizes. We
show a multi-armed bandit set-
ting with 50 actions, comparing
exact ordinary least squares (left)
with SGD using Polyak averaging
(right). Up to a point, increasing
ensemble size improves cumula-
tive regret in both cases.
Algorithm 1 ACB for linear bandits
Parameters: ensemble size M; β, λ
1:	For all i = 1,...,d, set warm-start features x-i+i = √λei and corresponding warm-start targets
yj)+ι 〜N (0,1) for each j ∈ [M ]
2:	for t = 1, . . . , T : do
3:	Observe action features {xt,a}a∈At
4:	Update reward estimator:
t-1
θt ：= arg min X (〈工/#"。)-j(aτ))2 + 入|网|2
τ=1
5:	Sample new random ensemble targets:
(re-randomized)	yj 〜N(0,1) for each j ∈ [M],τ ∈ {-d +1,...,t — 1}
(incremental)	yj 〜N (0,1) for each j ∈ [M ]
6:	Update bonus ensembles for each j ∈ [M],
t-1	2
wt(j) := arg mwin X	hxτ,aτ, wi - yτ(j)
τ =-d+1
7:	Computeper-armbonuses bonust(a) := β ∙ max,∈[M] ∣<xt,a, Wj))I for each a ∈ At
8:	Choose action at := arg maxa∈At hxt,a, θti + bonust(a)
9:	Observe reward rt(at) = (xt,αt,θ*i + εt
predict i.i.d. standard Gaussian noise, and sets the bonus to be proportional to the maximum deviation
over this ensemble from its mean (which is 0). This procedure is formally provided in Algorithm 1.
With the correct choice of ensemble size, the concentration of the maximum of independent Gaussians
implies that each bonust(a) is large enough to be a valid upper confidence bound, yet small enough
to inherit the regret bound of LinUCB. ACB can be instantiated with freshly resampled history; in this
case, the theoretical guarantees on the sufficient ensemble size are strongest. A more computationally
efficient variant, which is the one we scale up to deep RL, samples random targets only once, enabling
the use of streaming algorithms to estimate regression weight wtj. For these two variants, the following
regret bounds result from straightforward modifications to the analysis of LinUCB (Abbasi-Yadkori
et al., 2011), which are deferred (along with more precise statements) to Appendix B:
Proposition 2 (ACB regret bound with re-randomized history). Then, Algorithm 1, with re-
randomized history and ensemble size M = Θ(log(T/δ)), obtains a regret bound of O(d√T log A),
with probability at least 1 - δ.
For A ≤ O(poly(d)), our regret bound matches that of LinUCB up to logarithmic factors. We note
that the expected regret guarantee of the LinPHE algorithm in Kveton et al. (2019) also has the
same dependence on log(AT). Our lower-tail analysis on the number of regressors required for the
maximum deviation to be optimistic is slightly sharper than that found in Ishfaq et al. (2021), by a
factor of log A; this is due to requiring optimism only on a* in Lemma 2.
5
Published as a conference paper at ICLR 2022
Algorithm 2 ACB exploration for reinforcement learning
Parameters: ensemble M, tail-average constant γ, policy network function f parameterized by θ,
update frequency τ
1:	Initialize auxiliary weights w0(j) ∈ Rd, ∀j ∈ [M]
2:	Initialize auxiliary policy network parameters θauχ J θ
3:	for t = 1, . . . , T : do
4:	Observe state xt
5:	Compute gradient features using auxiliary policy network gt := g(xt ; f, θaux)
6:	Evaluate bonus bt = maxj (((wt(j))>gt)2)
7:	Select action from policy network softmax distribution f(xt ; θ)
8:	if t is a multiple of τ then
9:	Update policy network parameters θ with PPO on intrinsic rewards {bt-τ+1, . . . , bt}
10:	For all j ∈ [M], draw random targets y(j) 〜 N(0,1) and compute loss
Ltj) = ((Wj))>gt -y(j)产 + λμw(j) - w0j)||2
11:	For all j ∈ [M], update wt(+j)1 using L(tj)	//using RMSProp
12:	Update auxillary network parameters θaux J αθ + (1 - α)θaux
Proposition 3 (ACB regret bound with incremental updates). Algorithm 1, with incrementally
randomized history and an ensemble size of M = Θ(log(T∕δ)), obtains a regret bound of O(A√T)
in the multi-armed bandit setting, with probability at least 1 - δ.
We conjecture that the incrementally-updated version guarantees can be extended to linear bandits
beyond MAB for reasonable ensemble sizes. In between the extremes of always vs. never re-
randomizing history, it is possible to analyze a “rarely re-randomizing” variant of the algorithm,
which obtains the smaller sufficient ensemble size of Proposition 2 while only re-randomizing
-τ ,	_	_
O(dlog T) times:
Theorem 4 (Lazy re-randomization). Algorithm 3, with lazy re-randomized history, obtains a regret
bound of O(d log(AT /δ)T) in the case of linear bandits with fixed actions, while resampling the
random targets y(j) only O(dlog(AT∕δ)log(T)) times.
We defer the modified algorithm and analysis to Appendix C. We note that the lazy version is
computationally more efficient than the bonus approximation procedures in LinPHE (Kveton et al.,
2019) and LSVI-PHE (Ishfaq et al., 2021), and the expected regret guarantees obtained by LinPHE
are not sufficient to successfully implement a lazy strategy.
Like other works proposing more scalable algorithms in the same setting (Korda et al., 2015; Ding
et al., 2021), with theoretically sound hyperparameter choices, we do not exhibit end-to-end theoretical
improvements over the O(d2 |At|) per-iteration cost of rank-1 updates for implementing LinUCB.
Both lines 4 and 6 in Algorithm 1 depend on recursive least squares optimization oracles. For our
main empirical results (as well as Figure 2 (right), we implement this oracle using Polyak-averaged
SGD updates to each regressor, requiring O(M d|At |) time per iteration to compute all the bonuses.
Synthetic bandit experiments. As a simple empirical validation of our method, we demonstrate
the performance of Algorithm 1 (with incremental sampling) in a simulated multi-armed bandit
setting with A = 50 actions. One arm has a mean reward 0.75 while the others’ are set to 0.25; the
noise εt for each arm is sampled independently from N(0, 0.12). As predicted by the theory, the
ensemble size M must be chosen to be large enough for the bonuses to be sufficiently optimistic,
small enough to avoid too many upper-tail events. Further details are provided in Appendix A.1.
4	Extending ACB to deep reinforcement learning
Outlined as Algorithm 2, the RL version of ACB deviates from the version mentioned earlier in several
ways. First, like other work assigning exploratory bonuses in deep reinforcement learning, we assign
intrinsic rewards on states, rather than on state-action pairs (Pathak et al., 2017; Burda et al., 2018b).
6
Published as a conference paper at ICLR 2022
Pitfall
p-ɪ亘∕fs①s0s
ənb-un
0.25	0.50	0.75	1.00
Episodes le2
Figure 3: ACB With features using either penulti-
mate layer or gradient features. Overall, gradient
features seem to offer improved exploration, as
measured by the number of unique states visited
by an agent trained exclusively on these bonuses.
----ACB (gradient features)
ACB (output features
5 0 5 0 5
12.10.7.5.N
PJeMBa Uωuμlx山
Figure 4: Larger ACB ensembles are generally
more stable, resulting in better exploration. An
agent trained exclusively on large-ensemble ACB
bonuses will tend to observe more extrinsic re-
ward than those with fewer regressors.
PJBM ①=Msu-JIX 山
-150 -
-200 -
Figure 5: Extrinsic reward in Atari games as a function of the number of times the agent interacts
with the environment. Each plot corresponds to a different intrinsic reward scheme, and PPO is being
trained only to maximize this intrinsic reward.
Second, to help deal with non-stationarity, we compute gradient features with respect to a tail-averaged
set of policy weights, rather than on the policy weights being used to interact with the environment.
Because neural networks learn their own internal feature representation, extending ACB to the neural
setting gives us some flexibility in the way we choose to represent data. One option is to use the
penultimate layer representation of the network, an approach that has seen success in the closely
related active learning setting (Sener & Savarese, 2018). In the deep active learning classification
setting, a more recent approach is to instead represent data using a hallucinated gradient, embedding
data as the gradient that would be induced by the sample if the most likely label according to the
model were the true label (Ash et al., 2020). Treating the policy network as a classifier, we find that
this representation tends to perform better than using just the penultimate layer. Specifically, this
gradient is computed as
∂
g(X f,θ) = ∂θ'ce(f(x; θ),y), y = argmaxf(x; θ),
(1)
where f is the policy network, θ is its current parameters, 'ce(y, y) is the log loss of the pre-
diction y given true label y, and y is the action to which the most probability mass is assigned.
Using this representation, each seen state x contributes gxgx> to the ACB covariance matrix, a
quantity that can viewed as a sort of rank-one approximation of the pointwise Fisher information,
I(x, θ) = Ey〜f(x,θ) V'(f (x, θ), y)(V'(f (x, θ), y))> (Ash et al., 2021). This connection draws a
parallel between ACB with gradient features and well-understood, Fisher-based objectives in the
active learning literature (Zhang & Oles, 2000; Gu et al., 2014; Chaudhuri et al., 2015). The ability to
use such a gradient representation highlights the scalability of ACB—policy networks used in RL are
typically composed of over a million parameters, and explicitly maintaining and inverting a covariance
7
Published as a conference paper at ICLR 2022
ie7 SeaqUeSt
ie7 Breake)Ut
2	4	6	0.25	0.50 0.75	1.00	0.25 0.50 0.75 1.00	1	2	3
Episodes ι63	Episodes ie4	Episodes ie4	Episodes ie4
Figure 6: The number of unique states visited (measured by a hashing) in Atari games as a function
of the number of game episodes. Each plot corresponds to a different intrinsic reward scheme, and
PPO is being trained only to maximize this intrinsic reward.
matrix of this size is not feasible. ACB circumvents this need and handles high-dimensional features
without issue. Figure 3 shows examples of environments for which the gradient representation
produces better exploration than the penultimate-layer representation, according to the number of
unique states encountered by the agent (using a hash). In both cases, representations are computed
on a Polyak-averaged version of the policy with α = 10-6, which we also use in Section 5 below.
5	Experiments
This section compares ACB with state-of-the-art approaches to exploration bonuses in deep rein-
forcement learning on Atari benchmarks. We inherit hyperparameters from RND, including the
convolutional policy network architecture, which can be found in Appendix Section A.2. Policy
optimization is done with PPO (Schulman et al., 2017). In all experiments, we use 128 parallel agents
and rollouts of 128 timesteps, making τ in Algorithm 2 1282. Like other work on intrinsic bonuses,
we consider the non-episodic setting, so the agent is unaware of when it the episode is terminated.
Consequently, even in games for which merely learning to survive is equivalent to learning to play
well, random or constant rewards will not elicit good performance. All experiments are run for five
replicates, and plots show mean behavior and standard error.
In our investigation, we identified several tricks that seem to help mitigate the domain shift in the
gradient representation. Among these, as mentioned above, we tail-average auxiliary policy network
parameters, slowly moving them towards the deployment policy. This approach has been used to
increase stability in both reinforcement learning and generative adversarial networks (Lillicrap et al.,
2015; Bell, 1934). In experiments shown here, α is fixed at 10-6. We additionally normalize intrinsic
bonuses by a running estimate of their standard deviation, an implementation detail that is also
found in both RND and ICM. Batch normalization is applied to gradient features before passing
them to the auxiliary weight ensemble, and, like in the bandit setting, we compute bonuses using
a Polyak-averaged version of the ensemble rather than the ensemble itself. We use the RMSprop
variant of SGD in optimizing auxiliary weights.
We use an ensemble of 128 auxiliary weights and regularize aggressively towards their initialization
with λ = 103 . As with the bandit setting, larger ensembles tend to produce bonuses that both are
more stable and elicit more favorable behavior from the agent (Figure 4). Experiments here consider
eight different Atari games. Environments were chosen to include both games that are known to be
challenging in the way of exploration (Private Eye, Pitfall! and Gravitar (Burda et al., 2018b)), and
games that are not—remaining environments overlap with those from Table 1 of Mnih et al. (2015))
Intrinsic rewards only. In our first set of experiments, we train agents exclusively on bonuses
generated by ACB, rather than on any extrinsic reward signal. In Figure 1, we visualize some of these
bonuses as gameplay progresses in two Atari games. In both, large intrinsic bonuses are obtained
when the agent reaches the periphery of its experience. Correspondingly, when the agent dies, and
the game state is reset to something familiar, the ACB bonus typically drops by a significant margin.
8
Published as a conference paper at ICLR 2022
PJBMBaωωuμlx 山
0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5
Timesteps ie7	Timesteps ie7	Timesteps Ie7	Timesteps Ie7
Figure 7: Extrinsic reward in Atari games as a function of the number of frames seen by the agent.
Each plot corresponds to a different intrinsic reward scheme, and PPO is being trained to jointly
maximize these bonuses and the observed extrinsic reward. ACB is competitive with RND in this
setting, and for some environments and experience budgets, even obtains higher reward.
These intrinsic bonuses encourage the agent to avoid the start state, and to continually push the bound-
ary of what has been observed so far. Figure 5 compares the per-episode extrinsic rewards collected
by agents trained only on intrinsic rewards across eight popular Atari games and three different bonus
schemes. Even though extrinsic reward is not observed, many of these agents obtain non-trivial
performance on the game, suggesting that the pursuit of novelty is often enough to play well. Extrinsic
reward does not necessarily correlate with exploration quality, but ACB is clearly competitive with
other intrinsic reward methods along this axis. In Figure 6, we measure the number of unique states en-
countered, as measured by a hash, as exploration progresses. ACB is competitive from this perspective
as well, visiting as many or more states than baselines in most of the environments considered.
Intrinsic and extrinsic rewards. Next, while still in the non-episodic setting, we experiment with
training PPO jointly on intrinsic and extrinsic rewards. Following the PPO modification proposed
by Burda et al. (2018b), we use two value heads, one for intrinsic and another for extrinsic rewards.
total reward is computed as the sum of intrinsic and extrinsic returns. Figure 7 compares RND and
ACB agents trained with both intrinsic and extrinsic rewards on eight Atari games, which shows ACB
again performing competitively in comparison to RND. We find that ACB performs at least as well as
RND on most of the games we considered, and sometimes, for particular timesteps and environments,
even offers an improvement.
6	Conclusion
This article introduced ACB, a scalable and computationally-tractable algorithm for estimating the
elliptical potential in both bandit and deep reinforcement learning scenarios. ACB avoids the need to
invert a large covariance matrix by instead storing this information in an ensemble of weight vectors
trained to predict random noise.
This work aims to bridge the gap between the empirical successes of deep reinforcement learning and
the theoretical transparency of bandit algorithms. We believe ACB is a big step along this direction,
but there is still more work to be done to further this effort. For one, ACB is still not able to perform
as well as RND on Montezuma’s Revenge, an Atari benchmark for which exploration is notoriously
difficult, and for which RND performs exceptionally well.
Our experiments indicate that our proposed technique of anti-concentrating optimism can improve
empirical performance well beyond our current theoretical understanding. We leave improving our
regret guarantees (such as eliminating dependences on log(A), matching the optimal rate in all
regimes), as well as extending our analysis to more difficult settings (such as the incremental case) as
exciting open problems.
9
Published as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori, Dgvid Pdl, and Csaba SzepesvM. Improved algorithms for linear stochastic
bandits. In Neural Information Processing Systems, volume 11, pp. 2312-2320, 2011.
Naoki Abe and Philip M Long. Associative reinforcement learning using linear probabilistic concepts.
In ICML, pp. 3-11. Citeseer, 1999.
Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement learning: Theory and algorithms.
CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International Conference on Machine Learning, pp. 127-135. PMLR, 2013.
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. International Conference on
Learning Representations, 2020.
Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade. Gone fishing: Neural active
learning with fisher embeddings. Neural Information Processing Systems, 2021.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Chenjia Bai, Lingxiao Wang, Zhaoran Wang, Lei Han, Jianye Hao, Animesh Garg, and Peng Liu.
Principled exploration via optimistic bootstrapping and backward induction. arXiv preprint
arXiv:2105.06022, 2021.
Eric Temple Bell. Exponential polynomials. Annals of Mathematics, pp. 258-277, 1934.
Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. arXiv preprint arXiv:1606.01868, 2016.
William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M Kohler. The power of ensembles
for active learning in image classification. In IEEE Conference on Computer Vision and Pattern
Recognition, 2018.
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018a.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018b.
Alexandra Carpentier, Claire Vernade, and Yasin Abbasi-Yadkori. The elliptical potential lemma
revisited. arXiv preprint arXiv:2010.10182, 2020.
Nicolo Cesa-Bianchi and Gdbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Kamalika Chaudhuri, Sham Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of
active learning for maximum likelihood estimation. arXiv preprint arXiv:1506.02348, 2015.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. In Conference on Learning Theory, 2008.
Qin Ding, Cho-Jui Hsieh, and James Sharpnack. An efficient algorithm for generalized linear
bandit: Online stochastic gradient descent and thompson sampling. In International Conference
on Artificial Intelligence and Statistics, pp. 1585-1593. PMLR, 2021.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In International Conference on Machine Learning, 2017.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
10
Published as a conference paper at ICLR 2022
Quanquan Gu, Tong Zhang, and Jiawei Han. Batch-mode active learning via error bound minimization.
In UAI,pp. 300-309. Citeseer, 2014.
Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using
sample-based search. Advances in neural information processing systems, 25:1025-1033, 2012.
Mikael Henaff, Alfredo Canziani, and Yann LeCun. Model-predictive policy learning with uncertainty
regularization for driving in dense traffic. arXiv preprint arXiv:1901.02705, 2019.
Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup,
and Lin F Yang. Randomized exploration for reinforcement learning with general value function
approximation. arXiv preprint arXiv:2106.07841, 2021.
Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized
linear bandits: Online computation and hashing. arXiv preprint arXiv:1706.00136, 2017.
Nathaniel Korda, LA Prashanth, and Remi Munos. Fast gradient descent for drifting least squares
regression, with application to bandits. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 29, 2015.
Branislav Kveton, Csaba Szepesvari, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbed-
history exploration in stochastic linear bandits. arXiv preprint arXiv:1903.09132, 2019.
Tor Lattimore and Csaba Szepesvdri. Bandit algorithms. Cambridge University Press, 2020.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web, pp. 661-670, 2010.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based exploration
in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Ofir Nabati, Tom Zahavy, and Shie Mannor. Online limited memory neural-linear bandits with
likelihood matching. arXiv preprint arXiv:2102.03799, 2021.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via
posterior sampling. arXiv preprint arXiv:1306.0940, 2013.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. arXiv preprint arXiv:1602.04621, 2016.
Georg Ostrovski, Marc G Bellemare, Aaron Oord, and Remi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721-2730. PMLR,
2017.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286,
2007.
11
Published as a conference paper at ICLR 2022
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787.
PMLR, 2017.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In International Conference on Machine Learning, pp. 5062-5071. PMLR, 2019.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In International Conference on Learning Representations, 2018.
Aleksandrs Slivkins. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272, 2019.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, volume 2000, pp.
943-950, 2000.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning. arXiv preprint arXiv:1611.04717, 2016.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Stephen Tu. Upper and lower tails of gaussian maxima. https://
stephentu.github.io/blog/probability-theory/2017/10/16/
upper-and-lower-tails-gaussian-maxima.html, 2017.
Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.
Tong Zhang and F Oles. The value of unlabeled data for classification problems. In Proceedings of
the Seventeenth International Conference on Machine Learning,(Langley, P., ed.), volume 20, pp.
0. Citeseer, 2000.
Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. arXiv
preprint arXiv:2010.00827, 2020.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, pp. 11492-11502. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
A	Experimental details
A.1 Bandit experiments
Figure 2 shows the cumulative regret of Algorithm 1 (with incremental sampling) in a simulated
multi-armed bandit setting with A = 50 actions, for different values of M (powers of 2 from 1 to
256). One randomly sampled arm is chosen to have a mean reward of 0.75, with the others 0.25;
the noise εt for each arm is sampled independently from N(0, 0.12). Standard deviations are shown
over 100 independent trials. The bonus scaling factor β was selected in each run by grid search
over {0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0}; the learning rate for SGD was selected by grid search over
the same values. Each batch of experiments (one choice of algorithm and M ; all replicas and
hyperparameter sweeps) took around 1 CPU hour on an internal cluster.
A.2 Atari experiments
All policy network architectures used, for ICM, RND, and ACB, have the convolutional architecture
described in Burda et al. (2018b), consisting of three convolutional layers followed by two linear
layers, and leaky ReLU activations. For ACB and RND, we use PPO hyperparameters also from Burda
et al. (2018b), including 4 update epochs, an entropy coefficient of 0.001, γE = 0.999 and γI =0.99.
Our ICM implementation uses the default parameters of Pathak et al. (2017), which instead use 3
update epochs. We use a learning rate of 0.0001 and 128 simultaneous agents universally. All model
updates are performed via Adam except for the ACB auxiliary weights, which use RMSprop. See
attached code for more details.
Compute resources per job included a single CPU and either a P100 of V100 NVIDIA GPU, with
experiments each taking between three and five days.
B Proofs of regret bounds
In this section, we prove Propositions 2 and 3. We use the following notation in the analysis:
•	ςt denotes λI + PT=1 xt,at x>at.
•	regrett denotes the instantaneous regret hxt,a* ,θ*>-(xtg ,θ*).
•	lοg+(∙) denotes lοg(max(1, ∙)).
Furthermore, we assume that ∣∣χt,a|| ≤ B for all t, a, ∣∣θ*∣∣ ≤ W, and that the η are a martingale
difference sequence independent of the decisions made by the algorithm. Assume that the ηt are
σ2-subgaussian, with σ > 0.
B.1 Useful lemmas
Lemma 5 (Tail bounds for a maximum of Gaussians; Tu (2017)). Let z(1) , . . . , z(M) be i.i.d. N (0, 1)
random variables. Let Z denote maxj=1,...,M |z(j) |. Then, for all 0 < δ < 1, each of these
inequalities holds with probability at least 1 - δ:
Z ≥ ∏2Pplοg(M∕2) - lοglοg(1∕δ).
Z ≤ √2 (√lοg(2M) + Plοg(1∕δ)).
Lemma 6 (Proposition 5.5, Agarwal et al. (2019)). At any time t ∈ [T], for all a ∈ At, with
probability 1 - δt,
Da,。*—θt) ≤ βtkak∑—i
for βt = √λkθ* k + q2σ2 lοg(det(Σt)λ-d∕δt).
13
Published as a conference paper at ICLR 2022
Lemma 7 (Sufficient conditions for the bonus). Suppose that for all t ≤ T, the bonuses satisfy
bonust(a) ≤ γ2∣∣xt,a∣∣∑-1	∀ a ∈ At
bonust(a*) ≥ Yι∣∣Xt,a*∣∣∑-1
where a* is the optimal action. Ifwe choose actions according to this bonus at every time step t, then
with probability 1 - δ,
R(T) ≤ (Y2 + β) IdT log
1+
TB∖
^λd )
for B = √λW + √2σ Jd log(1 + TB) + log(T∕δ), as long as γι ≥ β.
Proof. Using the assumptions on the bonus and Lemma 6 with δt = δ∕T,1 with probability 1 一 δ,
for all t ≤ T
regrett = hxt,a*, θ*i 一 hxt,at, θ*i
≤ Dxt,a* , θt) + BtkXt,a* k∑-1 一 hxt,at, θ i
≤ (xt,at ,θt)
+ bonust(at) — bonust(a*) + Btkxt,a* k∑-1
一 hxt,at, θ*i
≤ bonust(at) — bonust(a*) + Btkxt,a* k∑-1 + Btkxt,at k∑-1
≤ γ2 llxt,at k∑-1 一 γ1 kxt,a* k∑-1 + Btkxt,a* ∣∣∑-1 + Btkxt,at ∣∣∑-1
≤ (Y2 + Bt)kxt,at∣∣∑-1 .
Summing up, we have
TT
R(T) = X regrett ≤ X(Y2 + Bt) kxt,atk∑-1
t=1	t=1
≤
T
X(γ2 + Bt)2
t=1
T
X kxt,at k∑-1
t=1	t
Y、/ c ∖n d_1	det(ΣT)
≤ tX(γ2+Bt)2v 2logd⅛)
≤
(Y2 + B)j
2dT log
1+
TB)
^λd )
The above follows from Cauchy-Schwarz and observing that Bt ≤ B for all t ≤ T.	□
B.2 Regret bounds
Proposition 2 (ACB regret bound with re-randomized history). Algorithm 1, with re-
randomized history, ensemble size M = dlog(2T∕δ)[, λ = σ2∕W2, and B = √λW +
√2σ Jdlog (1 + Td) + Iog(T0, as long as γι ≥ B, obtains a regret bound of R(T) ≤
O (σd√T log A log(T∕δ) log+ (TBW)), with probability at least 1 — δ.
Proof. At time t, since we re-sample all the noisy targets, we have,
t-1
w(j)= ς-1	X xτ,aτ yTj).
τ =-d+1
1If We want it to hold for all t < ∞, we can choose δt = ∏3δ2 . ThiS would give essentially the same bound
up to constants.
14
Published as a conference paper at ICLR 2022
(w"xt,a' is therefore distributed as N(0, ∣∣xt,ak∑ -J. Thus by Lemma 5, for all a ∈ At with
probability 1 - δ,
maX] ∣Dw(j),xt,aE∣ ≤ C2∣∣Xt,ak∑-1 PlogM + log(AT∕δ),
andfor a*,
c1 kxt,a* k∑-1 Mog M - loglog(T0 ≤ mM] I W)
where c1, c2 > 0 are fixed constants. Since bonust(a) = β max
j),Xt,a*
j∈[M] II wt(j),xt,a II, we get
a lower and upper bound on the bonus in terms of the LinUCB bonus. Lastly, by the choice
of M, with probability 1 - δ, we satisfy conditions of Lemma 7 with ci √log 2β = √λW +
√2σ Jdlog(1 + TB) + log(T∕δ). This gives us
R(T) ≤
(2c2‘log(AT0+1! √λWw+√2σjd log (1+TI )+log(T0! jdT log a+Tl
O σdPTrΓlog Alog(T∕δ) log+ (TBW)).
□
Proposition 3 (ACB regret bound with incremental updates). Assume further that the ηt are i.i.d.
σ2 -subgaussian random variables. Algorithm 1, with incrementally randomized history, ensemble
size M = dlog(T∕δ)e, λ = σ2∕W2, and β = √√λW + √2σQA log(1 + TB) + log(T∕δ), as long
as γι ≥ β, obtains a regret bound of
T ，-------- .	. /TBW
R(T) ≤ O (σA√Tlog Alog(T∕δ)log+ (^A-
in the multi-armed bandit setting, with probability at least 1 - δ.
Proof. Recall that the action set A is fixed, and d = A; we will use xa to denote xt,a . We follow
a “reward tape” argument (see, e.g., Slivkins (2019), Section 1.3.1) to show that the conditions of
Lemma 7 hold. Notice that in the incremental version of Algorithm 1, bonust (a) depends only on
the targets yτ(j) for -d + 1 ≤ τ < t, the number of times a (call this Nt(a)) has been selected up
to time t - 1, and the reward noise variables ετ for 1 ≤ τ < t, such that aτ = a. This is because
(w(j),Xa) = NIa) PT=-d+ι 1aτ=ayTj) in the case where all the Xa are orthogonal.
Consider the following procedure: sample TA(1 + M) independent random variables ahead of
time: ητ,a, an i.i.d copy of η for each T ∈ [T], a ∈ [A], and yj 〜N(0,1) for each T ∈
[T], a ∈ [A], j ∈ [M]. Then, run a deterministic version of Algorithm 1, which at time t uses
these yτ(j,a) : T = 1, . . . , Nt(a) as the random regression targets to compute bonust(a), and observes
reward 3七,θ? + nNt@)+i,at. Then, for any θ*, the distribution of action sequences taken by this
procedure is identical to that of Algorithm 1 with sequentially sampled ηt and yt(j), and Lemma 6
holds. By Lemma 5, with probability at least 1 - δ0, we have that for any t ∈ [T], a ∈ [A],
1
λ + Nt(a)
Hog M + log(AT∕δ0),
andfor a*,
1
c1
λ + Nt (a)
Plog M - log log(T∕δ0) ≤ max I (w(j),xt,
j∈[M]	t ,
la*
for absolute constants c1,c2 > 0, so that the conditions of Lemma 7 hold with the same choice of jβ.
Selecting δ0 = δ∕(2AT), we have by the union bound that this condition holds for all t ∈ [T], a ∈ [A],
and the proof reduces to that of Proposition 2, with an identical regret bound of
R(T) ≤ O (σA,TlogAlog(T∕δ) log+ (TBWW
σA
15
Published as a conference paper at ICLR 2022
Note here that A = d, and that T ≥ A is necessary for a non-vacuous regret bound, so that
i 4 ，八 ∕ι m∖ r∙ . ∙ .t	♦	if,IK/	I-I
log A ≤ O (log T) factors m the main paper are suppressed by the O(∙) notation.	口
C Lazily re-randomized ACB
Algorithm 3 Lazy-ACB for fixed action linear bandits
Parameters: ensemble size M; β, γ, λ, fixed action set A
1:	For all i = 1,...,d,set warm-start features a-i+ι = √λei
2:	Set τ = 1
3:	Set ω = 0
4:	for t = 1,…，T: do
5:	if ω = 0 then
6:	Update parameters:
t-1
θt := arg min X ({ai,θ} - ri(ai))2 + λ∣∣θk2
θ
i=1
7:	Sample new targets y(j) ~ N(0,1) for each j ∈ [M],t0 ∈ {-d +1,...,t - 1}
8:	Update bonus ensemble for each j ∈ [M],
t-1	2
wt(j) := arg min X	hat0, wi - yt(0j)
t0=-d+1
9:	Compute per-arm bonuses bonust(a) := β ∙ maXj∈[M] ∣ ^a,w(j')^∣ for each a ∈ A
10:	Set st := arg maxa∈[A] ha, θti + bonust (a)
11:	Set ω = l bonuγ2(st) m
12:	τ = t
13:	Choose action at = aτ
14:	Observe reward rt(at) =(st,。*〉+ εt
15:	Decrease ω by 1
In this section, we present the algorithm (see Algorithm 3) and regret analysis of a lazy-version of
ACB which only rarely re-randomizes history.
C.1 Useful Lemmas
Lemma 8 (Ensemble Bounds). For M = log(2AT /δ), at all updates τ, with probability 1 - δ, for
all a ∈ A,
β l∣ak∑-1
≤ bonusτ(a) ≤ β∣∣a∣∣∑-ι jsiog (2AT).
Proof. Follows by applying Lemma 5 and taking union bound over A and t ≤ T .
□
Lemma 9. Let T be the time ofan update and Y = β π2og2. Then, for T0 = T + ∣-bonu^(θ)^j,
_ , ɪ 、
det(Σ T o)
det(Σ T)
≥ /1 i	" lθg2一
≥ 1 +16log(2AT∕δ).
16
Published as a conference paper at ICLR 2022
Proof. Note that from τ to τ0, the action taken remains the same. For the upper bound, using Lemma
11 from Abbasi-Yadkori et al. (2011) and some elementary calculus:
1	ddet(Στ0)ʌ	TT- U	ll2
2log(κr )≥ X 心-+i-ι
T0-T	-1
=^X aT (∑τ + (i — 1)。丁aT)	&丁
i=1
τ0 -τ
X kaτk∑-1 -
kaτk∑-ι
Στ
i=1
τ0 -τ
X
i=1
1∕(i- 1) + kaτ k∑-ι
Στ
kaτ k∑-ι
Στ
1 + (i- 1)kaτk∑-i
八八IaTk 京-1+(T 0-τ )
≥ LkaT k T
2
∑-1
Ldu
u
≥ log(1 + (T0 - T)kaτk∑-1)
γ
≥ log 1 +
log 1 +
8β2 log(2AT ∕δ)
π log 2
16log(2AT∕δ)).
Lemma 10. Let Tt be the largest index ≤ t when the update happened and Y = β π2og2, then
_ , ɪ 、
det(∑ t)
det(ΣTt )
≤ 2e.
Proof. Suppose t = Tt, then the bound trivially holds, thus WLOG, we assume t > Tt . By our
algorithm, We know that, 0 < t - Tt ≤ ∣- *口口盘(Ο)^j. This implies, bonusT (a「) ≤ γ. Now, using
Lemma 11 from Abbasi-Yadkori et al. (2011) antd some elementary calculus:
log
_	,工.
det(∑ t)
det(ΣTt )
t-Tt
≤ X kaτtk∑-+
i=1	T+
X	kaτtk∑-ɪ
=i=1 1 + (i- 1)kaτtk∑ -J
∕*1∕kaτt k∑-1 +t-τ-1 1
≤ kaτt k∑-1 + J	t	Udu
Tt ς -t1
≤ kaτt k∑-1 + log (1 + (t - τ -I)kaτtk∑-j)
2bonus2 (aT )	2γ
≤ -------τt' M + log 1+----------Y—
—β2π log 2	+ log C + β2π log 2)
≤ β⅛
+ log 1 +
2γ	ʌ
β2π log 2 J
2e.
□
□
17
Published as a conference paper at ICLR 2022
C.2 Regret Bound
Theorem 4 (ACB regret bound with lazy updates). Algorithm 3, with lazy re-
randomized history, ensemble size M = dlog(AT∕δ)[, λ = σ2∕W2, and β 二
(√λW+√2σ,d log a + TB)+log(τ7δ)) q∏ι2g2, and γ
bound of
R(T) ≤ O(σdp log A) log(T∕δ) log+ I
_ β2π log 2
=	2
TBW
σd
obtains a regret
in the fixed action linear bandit setting, with probability at least 1 - δ. The algorithm re-randomizes
at most O(d log+(TBW∕σd) log(AT∕δ)) times.
Proof. Let τt be the largest index ≤ t when the update happened, then
regrett = ha*,θ *i - haτt, θ*i
≤ Da*,θτtE + βτtka*k∑-t1 - haτt,θ*i
≤(a”，θτ) + bonusTt (a" - bonus”(a*)+ βt∣a*k∑ -ι - (ɑ羯，θ*i
≤ bonusTt (aTt) - bonusTt (a ) + βTtka k ∑-1 + βTtkaτt∣∣∑-1
8log (—δ—) + βr) kaτtk∑-t1 + (βt - β4 2g ) lla*k∑-t1
8log () + /”)llatk∑-1
_ ,二.
det(∑ t)
det(Στt )
lla*kΣ -t1
ka*k∑ -J
+	βt - β
≤ 2e (β;8log ( δ ) + β) katkΣ-1 + It- β
where the first inequality comes from the validity of the confidence bound for a*, the second from the
fact that aτt = arg max。(a, G”)+ bonusτt (a), the third from the validity of the confidence bound
for aτt, the fourth follows from Lemma 8, the fifth follows from Lemma 12 Abbasi-Yadkori et al.
(2011) and lastly the sixth follows from Lemma 10.
Setting δt = δ∕T we have β羯 ≤ β Jn l0g2, therefore,
regrett ≤ 2e ("og (2AT) +
βkatk∑-1.
β，2Td log(1 + TB∕λd).
Lastly, we need to bound the number of times we refresh randomness. By Lemma 9, each time
we refresh, we increase det(Σt) by a factor of
1+
π log 2
16log(2AT∕δ).
_ .二、._ ,. ―、
Since det(Σt)/ det(λI) IS
bounded by (1+TB∕λd)d, the number of times we refresh is k = O(d log+ (TBW∕σd) log(AT∕δ)).
k/2
Formally, k must satisfy(1 + i6↑0g(2Aτ∕δ)j	= Θ ((1 + TB∕λd)d).
□
18