Published as a conference paper at ICLR 2022
Bregman Gradient Policy Optimization
Feihu Huang*, Shangqian Gao*, HengHuangt
Department of Electrical and Computer Engineering
University of Pittsburgh
Pittsburgh, PA 15261, USA
huangfeihu2018@gmail.com, shg84@pitt.edu, heng.huang@pitt.edu
Ab stract
In the paper, we design a novel Bregman gradient policy optimization framework
for reinforcement learning based on Bregman divergences and momentum tech-
niques. Specifically, we propose a Bregman gradient policy optimization (BGPO)
algorithm based on the basic momentum technique and mirror descent iteration.
Meanwhile, we further propose an accelerated Bregman gradient policy optimiza-
tion (VR-BGPO) algorithm based on the variance reduced technique. Moreover,
we provide a convergence analysis framework for our Bregman gradient policy
optimization under the nonconvex setting. We prove that our BGPO achieves a
sample complexity of O(-4) for finding -stationary policy only requiring one
trajectory at each iteration, and our VR-BGPO reaches the best known sample
complexity of O(-3), which also only requires one trajectory at each iteration.
In particular, by using different Bregman divergences, our BGPO framework uni-
fies many existing policy optimization algorithms such as the existing (variance
reduced) policy gradient algorithms such as natural policy gradient algorithm. Ex-
tensive experimental results on multiple reinforcement learning tasks demonstrate
the efficiency of our new algorithms.
1	Introduction
Policy Gradient (PG) methods are a class of popular policy optimization methods for Reinforce-
ment Learning (RL), and have achieved significant successes in many challenging applications (Li,
2017) such as robot manipulation (Deisenroth et al., 2013), the Go game (Silver et al., 2017) and
autonomous driving (Shalev-Shwartz et al., 2016). In general, PG methods directly search for the
optimal policy by maximizing the expected total reward of Markov Decision Processes (MDPs) in-
volved in RL, where an agent takes action dictated by a policy in an unknown dynamic environment
over a sequence of time steps. Since the PGs are generally estimated by Monte-Carlo sampling,
such vanilla PG methods usually suffer from very high variances resulted in slow convergence rate
and destabilization. Thus, recently many fast PG methods have been proposed to reduce variances
in vanilla stochastic PGs. For example, Sutton et al. (2000) introduced a baseline to reduce vari-
ances of the stochastic PG. Konda & Tsitsiklis (2000) proposed an efficient actor-critic algorithm
by estimating the value function to reduce effects of large variances. (Schulman et al., 2016) pro-
posed the generalized advantage estimation (GAE) to control both the bias and variance in policy
gradient. More recently, some faster variance-reduced PG methods (Papini et al., 2018; Xu et al.,
2019a; Shen et al., 2019; Liu et al., 2020; Huang et al., 2020) have been developed based on the
variance-reduction techniques in stochastic optimization.
Alternatively, some successful PG algorithms (Schulman et al., 2015; 2017) improve convergence
rate and robustness of vanilla PG methods by using some penalties such as Kullback-Leibler (KL)
divergence penalty. For example, trust-region policy optimization (TRPO) (Schulman et al., 2015)
ensures that the new selected policy is near to the old one by using KL-divergence constraint, while
proximal policy optimization (PPO) (Schulman et al., 2017) clips the weighted likelihood ratio
to implicitly reach this goal. Subsequently, Shani et al. (2020) have analyzed the global conver-
gence properties of TRPO in tabular RL based on the convex mirror descent algorithm. Liu et al.
* Feihu and Shangqian contributed equally.
,Corresponding Authors.
1
Published as a conference paper at ICLR 2022
Table 1: Sample complexities of the representative PG algorithms based on mirror descent algo-
rithm for finding an -stationary policy of the nonconcave performance function. Although Liu
et al. (2019); Shani et al. (2020) have provided the global convergence of TRPO and PPO under
some specific policies based on convex mirror descent, they still obtain a stationary point of noncon-
cave performance function. Note that our convergence analysis does not rely any specific policies.
Algorithm	Reference	Complexity	Batch Size
TRPO	Shani et al. (2020)	O(L)	OL)
Regularized TRPO	Shani et al. (2020)	O(e-3)	OL)
TRPO/PPO	LiU et al.(2019)	O(e-8)	O(L)
VRMPO	Yang et al.(2019)	O(L)	OL)
MDPO	Tomar et al. (2020)	Unknown	Unknown
BGPO	Ours	O(L)	O(1)
VR-BGPO	Ours	O(e-3)	O(1)
(2019)	have also studied the global convergence properties of PPO and TRPO equipped with over-
parametrized neural networks based on mirror descent iterations. At the same time, Yang et al.
(2019) tried to propose the PG methods based on the mirror descent algorithm. More recently, mir-
ror descent policy optimization (MDPO) (Tomar et al., 2020) iteratively updates the policy beyond
the tabular RL by approximately solving a trust region problem based on convex mirror descent
algorithm. In addition, Agarwal et al. (2019); Cen et al. (2020) have studied the natural PG methods
for regularized RL. However, Agarwal et al. (2019) mainly focuses on tabular policy and log-linear,
neural policy classes. Cen et al. (2020) mainly focuses on softmax policy class.
Although these specific PG methods based on mirror descent iteration have been recently studied,
which are scattered in empirical and theoretical aspects respectively, it lacks a universal framework
for these PG methods without relying on some specific RL tasks. In particular, there still does
not exist the convergence analysis of PG methods based on the mirror descent algorithm under the
nonconvex setting. Since mirror descent iteration adjusts gradient updates to fit problem geometry,
and is useful in regularized RL (Geist et al., 2019), there exists an important problem to be addressed:
Could we design a universal policy optimization framework based on the mirror descent
algorithm, and provide its convergence guarantee under the non-convex setting ?
In the paper, we firmly answer the above challenging question with positive solutions and propose an
efficient Bregman gradient policy optimization framework based on Bregman divergences and mo-
mentum techniques. In particular, we provide a convergence analysis framework of the PG methods
based on mirror descent iteration under the nonconvex setting. In summary, our main contributions
are provided as follows:
a)	We propose an effective Bregman gradient policy optimization (BGPO) algorithm based
on the basic momentum technique, which achieves the sample complexity of O(-4) for
finding -stationary policy only requiring one trajectory at each iteration.
b)	We propose an accelerated Bregman gradient policy optimization (VR-BGPO) algorithm
based on the variance-reduced technique of STORM (Cutkosky & Orabona, 2019). More-
over, we prove that the VR-BGPO reaches the best known sample complexity of O(-3).
c)	We design a unified policy optimization framework based on mirror descent iteration and
momentum techniques, and provide its convergence analysis under nonconvex setting.
In Table 1 shows that sample complexities of the representative PG algorithms based on mirror
descent algorithm. Shani et al. (2020); Liu et al. (2019) have established global convergence of a
mirror descent variant of PG under some pre-specified setting such as over-parameterized networks
(Liu et al., 2019) by exploiting these specific problems’ hidden convex nature. Without these special
structures, global convergence of these methods cannot be achieved. However, our framework does
not rely on any specific policy classes, and our convergence analysis only builds on the general
nonconvex setting. Thus, we only prove that our methods convergence to stationary points.
Geist et al. (2019); Jin & Sidford (2020); Lan (2021); Zhan et al. (2021) studied a general theory
of regularized MDPs based on policy space such as a discrete probability space that generally is
discontinuous. Since both the state and action spaces S and A generally are very large in practice,
the policy space is large. While our methods build on policy’ parameter space that is generally
continuous Euclidean space and relatively small. Clearly, our methods and theoretical results are
more practical than the results in (Geist et al., 2019; Jin & Sidford, 2020; Lan, 2021; Zhan et al.,
2021). (Tomar et al., 2020) also proposes mirror descent PG framework based on policy parameter
2
Published as a conference paper at ICLR 2022
space, but it does not provide any theoretical results and only focuses on Bregman divergence taking
form of KL divergence. While our framework can collaborate with any Bregman divergence forms.
2	Related Works
In this section, we review some related works about mirror descent-based algorithms in RL and
variance-reduced PG methods, respectively.
2.1	Mirror Descent Algorithm in RL
Due to easily deal with the regularization terms, mirror descent (a.k.a., Bregman gradient) algorithm
(Censor & Zenios, 1992; Beck & Teboulle, 2003) has shown significant successes in regularized RL,
which is first proposed in (Censor & Zenios, 1992) based on Bregman distance (divergence) (Breg-
man, 1967; Censor & Lent, 1981). For example, Neu et al. (2017) have shown both the dynamic
policy programming (Azar et al., 2012) and TRPO (Schulman et al., 2015) algorithms are approx-
imate variants of mirror descent algorithm. Subsequently, Geist et al. (2019) have introduced a
general theory of regularized MDPs based on the convex mirror descent algorithm. More recently,
Liu et al. (2019) have studied the global convergence properties of PPO and TRPO equipped with
overparametrized neural networks based on mirror descent iterations. At the same time, Shani et al.
(2020) have analyzed the global convergence properties of TRPO in tabular policy based on the
convex mirror descent algorithm. Wang et al. (2019) have proposed divergence augmented policy
optimization for off-policy learning based on mirror descent algorithm. MDPO (Tomar et al., 2020)
iteratively updates the policy beyond the tabular RL by approximately solving a trust region problem
based on convex mirror descent algorithm.
2.2	(Variance-Reduced) PG Methods
PG methods have been widely studied due to their stability and incremental nature in policy op-
timization. For example, the global convergence properties of vanilla policy gradient method in
infinite-horizon MDPs have been recently studied in (Zhang et al., 2019). Subsequently, Zhang et al.
(2020) have studied asymptotically global convergence properties of the REINFORCE (Williams,
1992), whose policy gradient is approximated by using a single trajectory or a fixed size mini-batch
of trajectories under soft-max parametrization and log-barrier regularization. To accelerate these
vanilla PG methods, some faster variance-reduced PG methods have been proposed based on the
variance-reduction techniques of SVRG (Johnson & Zhang, 2013), SPIDER (Fang et al., 2018) and
STORM (Cutkosky & Orabona, 2019) in stochastic optimization. For example, fast SVRPG (Papini
et al., 2018; Xu et al., 2019a) algorithm have been proposed based on SVRG. Fast HAPG (Shen
et al., 2019) and SRVR-PG (Xu et al., 2019a) algorithms have been presented by using SPIDER
technique. Subsequently, the momentum-based PG methods, i.e., ProxHSPGA (Pham et al., 2020)
and IS-MBPG (Huang et al., 2020), have been developed based on variance-reduced technique of
STORM/Hybrid-SGD (Cutkosky & Orabona, 2019; Tran-Dinh et al., 2019). More recently, (Ding
et al., 2021) studied the global convergence of momentum-based policy gradient methods. (Zhang
et al., 2021) proposed a truncated stochastic incremental variance-reduced policy gradient (TSIVR-
PG) method to relieve the uncheckable importance weight assumption in above variance-reduced
PG methods and provided the global convergence of the TSIVR-PG under overparameterizaiton of
policy assumption.
3	Preliminaries
In the section, we will review some preliminaries of Markov decision process and policy gradients.
3.1	Notations
Let [n] = {1,2,…，n} for all n ∈ N+. For a vector X ∈ Rd, let ∣∣xk denote the '2 norm of x, and
∣∣χkp = (Pd=ι |xi|p)1/p (P ≥ 1) denotes the p-norm of x. For two sequences {ak} and {bk}, we
denote ak = O(bk) if ak ≤ Cbk for some constant C > 0. E[X] and V[X] denote the expectation
and variance of random variable X, respectively.
3.2	Markov Decision Process
Reinforcement learning generally involves a discrete time discounted Markov Decision Process
(MDP) defined by a tuple {S, A, P, r, γ, ρ0}. S and A denote the state and action spaces of the
3
Published as a conference paper at ICLR 2022
agent, respectively. P(s0|s, a) : S × A → 4(S) is the Markov kernel that determines the transition
probability from the state s to s0 under taking an action a ∈ A. r(s, a) : S × A → [-R, R] (R > 0)
is the reward function of s and a, and ρ0 = p(s0) denotes the initial state distribution. γ ∈ (0, 1) is
the discount factor. Let π : S → 4(A) be a stationary policy, where 4(A) is the set of probability
distributions on A.
Given the current state st ∈ S, the agent executes an action at ∈ A following a conditional proba-
bility distribution π(a∕st), and then the agent obtains a reward r = r(st, at). At each time t, we
can define the state-action value function Qπ(st, at) and state value function Vπ(st) as follows:
∞
∞
Qπ(st, at)
= Est+1,at+1,...	γlrt+l, Vπ(st)= Eat,st+1,...	γlrt+l.
(1)
l=0
l=0
We also define the advantage function Aπ(st, at) = Qπ(st, at) - Vπ(st). The goal of the agent is
to find the optimal policy by maximizing the expected discounted reward
(2)
max J(π) := Eg。〜ρo[Vπ(s0)].
π
Given a time horizon H, the agent collects a trajectory τ = {st, at}tH=-01 under any stationary policy.
Then the agent obtains a cumulative discounted reward r(τ) = PtH=-01 γtr(st, at). Since the state
and action spaces S and A are generally very large, directly solving the problem (2) is difficult.
Thus, we let the policy π be parametrized as πθ for the parameter θ ∈ Θ ⊆ Rd . Given the initial
distribution ρ0 = p(s0), the probability distribution over trajectory τ can be obtained
H-1
p(τ∣θ) = P(S0)ɪɪ P(st+ι∣St, at)∏θ(at∣st).
(3)
t=0
Thus, the problem (2) will be equivalent to maximize the expected discounted trajectory reward:
maχ J(θ) := ET〜p(τ∣θ)[r(τ)].	(4)
θ∈Θ
In fact, the above objective function J(θ) has a truncation error of O( IHY) compared to the original
infinite-horizon MDP.
3.3	Policy Gradients
The policy gradient methods (Williams, 1992; Sutton et al., 2000) are a class of effective policy-
based methods to solve the above RL problem (4). Specifically, the gradient of J(θ) with respect to
θ is given as follows:
VJ(θ) = ET〜p(τ∣θ) [Vlog (p(T∣θ))r(τ)].	(5)
Given a mini-batch trajectories B = {τi}n=ι sampled from the distribution p(τ∣θ), the standard
stochastic policy gradient ascent update at (k + 1)-th step, defined as
θk+1 = θk + ηVJB(θk),	(6)
where η > 0 is learning rate, and VJB(θk) = 1 Pn=ι g(τ∕θk) is stochastic policy gradient. Given
H = O(ι-γ) as in (Zhang et al., 2019; Shani et al., 2020), g(τ∣θ) is the unbiased stochastic policy
gradient of J(θ), i.e., E[g(τ∣θ)] = VJ(θ), where
H-1	H-1
g(τ lθ) = (X vθ log πθ (at,st))(X Ytr(St,at)).	⑺
t=0	t=0
Based on the gradient estimator in (7), we can obtain the existing well-known policy gradient esti-
mators such as REINFORCE (Williams, 1992), policy gradient theorem (PGT (Sutton et al., 2000)).
Specifically, the REINFORCE obtains a policy gradient estimator by adding a baseline b, defined as
H-1	H-1
g(τ∣θ) = ( E Vθ log∏θ(at,st))( E Ytr(st,at) - bt).
t=0	t=0
The PGT is a version of the REINFORCE, defined as
H-1 H-1
g(τlθ) =ΣΣ Yjr(Sj,aj) - bj Vθlogπθ(at, St).
4
Published as a conference paper at ICLR 2022
Algorithm 1 BGPO Algorithm
1:	Input: Total iteration K, tuning parameters {λ,b,m,c} and mirror mappings {ψk}K=ι are
ν-strongly convex functions;
2:	Initialize: θι ∈ Θ, and sample a trajectory τι fromp(τ∣θι), and compute uι = -g(τι∣θι);
3:	fork = 1,2, . . . ,Kdo
4:	Update石k+ι = argminθ∈θ {hu,θ + 1 Dψk(θ,θk)};
5:	Update θk+ι = θk + ηk(θk+ι - θk) With η =(/*2;
6:	Sample a trajectory τk+ι fromp(τ∣θk+ι),and compute uk+ι = -βk+1g(τk+1∣θk+1) + (1 -
βk+1 )uk With βk+1 = cηk ;
7:	end for
8:	Output: θζ chosen uniformly random from {θk}kK=1.
4	B regman Gradient Policy Optimization
In this section, We propose a novel Bregman gradient policy optimization frameWork based on Breg-
man divergences and momentum techniques. We first let f(θ) = -J(θ), the goal of policy-based
RL is to solve the problem: maxθ∈θ J(θ) ^⇒ minθ∈θ f(θ), so we have Vf (θ) = -VJ(θ).
Assume ψ(x) is a COntinUously-differentiable and V-strongly convex function, i.e.,(x - y, Vψ(x)-
Vψ(y)i ≥ νkx - yk, ν > 0, we define a Bregman distance:
Dψ (y, x) = ψ(y) - ψ(x) - hVψ(x), y - xi, ∀x,y ∈ Rd	(8)
Then given a function h(x) defined on a closed convex set X, we define a proximal operator (a.k.a.,
mirror descent):
Pψh(x) = arg min {h(y) + 1 Dψ (y,x)},	(9)
,	y∈X	λ
where λ > 0. Based on this proximal operator Pλψ,h as in (Ghadimi et al., 2016; Zhang & He, 2018),
we can define a Bregman gradient of function h(x) as follows:
Bψ,h(X) = 1 (χ-pψ,h(χ)).	(IO)
If ψ(x) = 1 ∣∣x∣∣2 and X = Rd, x* is a stationary point of h(x) if and only if Bψh(x*) = Vh(x*)=
0. Thus, this Bregman gradient can be regarded as a generalized gradient.
4.1	BGPO Algorithm
In the subsection, we propose a Bregman gradient policy optimization (BGPO) algorithm based on
the basic momentum technique. The pseudo code of BGPO Algorithm is provided in Algorithm 1.
In Algorithm 1, the step 4 uses the stochastic Bregman gradient descent (a.k.a., stochastic mirror
descent) to update the parameter θ. Let h(θ) = hθ, uki be the first-order approximation of function
f(θ) at θk, where uk is an approximated gradient of function f(θ) at θk. By the step 4 of Algorithm
1 and the above equality (10), we have
Bψh(θk) = 1 (θk- θk+ι),	(11)
λ
where λ > 0. Then by the step 5 of Algorithm 1, we have
θk+1 = θk - ληkBλψ,kh(θk),	(12)
where 0 < ηk ≤ 1. Due to the convexity of set Θ ⊆ Rd and θ1 ∈ Θ, we choose the parameter
ηk ∈ (0, 1] to ensure the updated sequence {θk}kK=1 in Θ.
In fact, our BGPO algorithm unifies many popular policy optimization algorithms. When the mir-
ror mappings ψk(θ) = 2∣∣θ∣∣2 for ∀k ≥ 1, the update (12) will be equivalent to a classic policy
gradient iteration. Then our BGPO algorithm will become a momentum version of the policy gra-
dient algorithms (Sutton et al., 2000; Zhang et al., 2019). Given ψk(θ) = 2∣∣θ∣∣2 and βk = 1, i.e.,
Uk = -g(τk∣θk), we have Bψ,kh(θk) = -g(τk%) and
θk+ι = θk + ληk g(τk ∣θk).	(13)
5
Published as a conference paper at ICLR 2022
Algorithm 2 VR-BGPO Algorithm
1:	Input: Total iteration K, tuning parameters {λ,b,m,c} and mirror mappings {ψk}K=ι are
ν-strongly convex functions;
2:	Initialize: θι ∈ Θ, and sample a trajectory τι fromp(τ∣θι), and compute uι = -g(τι∣θι);
3:	fork = 1,2, . . . ,Kdo
4:	Update石k+ι = argminθ∈θ {hu,θ + 1 Dψk(θ,θk)};
5:	Update θk+ι = θk + ηk(θk+ι - θk) With η =(马产;
6:	Sample a trajectory τk+ι fromp(τ∣θk+ι),and compute uk+ι = -βk+1g(τk+1∣θk+1) + (1 -
βk+ι) [uk - g(τk+1∣θk+1) + w(τk+ι∣θk,θk+1)g(τk+1∣θk)] with βk+ι = cη2;
7:	end for
8:	Output: θζ chosen uniformly random from {θk}kK=1.
When the mirror mappings ψk(θ) = 1 θτF®)θ with F(θk) = E[Vθ∏θk(s, a)(Vθ∏θk(s,a))],
the update (12) will be equivalent to a natural policy gradient iteration. Then our BGPO will become
a momentum version of natural policy gradient algorithms (Kakade, 2001; Liu et al., 2020). Given
ψk(θ) = 1 θτF(θk)θ, βk = 1,i.e., Uk = -g(τk∣θk), we have Bψh(θk) = -F(θk)+g(τk∣θk) and
θk+ι = θk + ληk F (θk)+g(τk∣θk),	(14)
where F(θk)+ denotes the Moore-Penrose pseudoinverse of the Fisher information matrix F(θk).
When given the mirror mapping ψk (θ) = Ps∈S πθ(s) log(πθ(s)), i.e., Boltzmann-Shannon en-
tropy function (Shannon, 1948) and Θ = {θ ∈ Rd | Ps∈S πθ(s) = 1}, we have Dψk (θ, θk) =
KL(∏θ(s),∏θk (S)) = Ps∈s ∏θ(s) log (∏θ((S))), which is the KL divergence. Then our BGPO will
become a momentum version of mirror descent policy optimization (Tomar et al., 2020).
4.2	VR-BGPO Algorithm
In the subsection, we propose a faster variance-reduced Bregman gradient policy optimization (VR-
BGPO) algorithm based on a variance-reduced technique. The pseudo code of VR-BGPO algorithm
is provided in Algorithm 2.
Consider the problem (4) is non-oblivious that the distribution p(τ∣θ) depends on the variable θ
varying through the whole optimization procedure, we apply the importance sampling weight (Pap-
ini et al., 2018; Xu et al., 2019a) in estimating our policy gradient uk+1, defined as
in n ʌ	P(Tk+1∣θk)	H-I πθk (a⅛t)
W(Tk+ι∣θk,θk+ι) =	—=-V = Il -------------/ I ʌ.
P(Tk+1∣θk+l)	占 πθk+ι 51st)
Except for different stochastic policy gradients {uk} and tuning parameters {ηk, βk} using in Al-
gorithms 1 and 2, the steps 4 and 5 in these algorithms for updating parameter θ are the same.
Interestingly, when choosing mirror mapping ψk(θ) = 2 ∣∣θk2, our VR-BGPO algorithm will reduce
to a non-adaptive version of IS-MBPG algorithm (Huang et al., 2020).
5	Convergence Analysis
In this section, we will analyze the convergence properties of our BGPO and VR-BGPO algorithms.
All related proofs are provided in the Appendix A. Here we use the standard convergence metric
∣Bψkθ d"θ、∖(θ)k used in (Zhang & He, 2018; Yang etal., 2019) to evaluate the convergence Breg-
λ, hθ, V f (θk )i
man gradient-based (a.k.a., mirror descent) algorithms. To give the convergence analysis, we first
give some standard assumptions.
Assumption 1. For function log πθ (a|s), its gradient and Hessian matrix are bounded, i.e., there
exist constants Cg ,Ch > 0 such that ∣∣Vθ log ∏θ (a∣s)∣ ≤ Cg, ∣V2 log ∏θ (a∣s)∣ ≤ Ch.
Assumption 2. Variance ofstochastic gradient g(τ∣θ) is bounded, i.e., there exists a constant σ > 0,
for all ∏θ such that V(g(τ∣θ)) = Ekg(T∣θ) — VJ(θ)∣2 ≤ σ2.
Assumption 3. For importance sampling weight w(τ∣Θ1,Θ2) = p(τ∣θι)∕p(τ∣Θ2), its variance is
bounded, i.e., there exists a constant W > 0, it follows V(w(τ 仇，θ2)) ≤ W for any θ1,θ2 ∈ Rd
and T 〜p(τ∣Θ2).
6
Published as a conference paper at ICLR 2022
Assumption 4. Thefunction J(θ) has an upper bound in Θ, i.e., J* = supθ∈θ J(θ) < +∞.
Assumptions 1 and 2 are commonly used in the PG algorithms (Papini et al., 2018; Xu et al.,
2019a;b). Assumption 3 is widely used in the study of variance reduced PG algorithms (Papini
et al., 2018; Xu et al., 2019a). In fact, the bounded importance sampling weight might be violated in
some cases such as using neural networks as the policy. Thus, we can clip this importance sampling
weights to guarantee the effectiveness of our algorithms as in (Papini et al., 2018). At the same time,
the importance weights actually also have some nice properties, e.g., in soft-max policy it is bounded
by eckθ1-θ2k2 for all θ1, θ2 ∈ Θ. More recently, (Zhang et al., 2021) used a simple truncated update
to relieve this uncheckable importance weight assumption. Assumption 4 guarantees the feasibility
of the problem (4). Note that Assumptions 2 and 4 are satisfied automatically given Assumption 1
and the fact that all the rewards are bounded, i.e., |r(s, a)| ≤ R for any s ∈ S and a ∈ A. For
example, due to |r(s, a)| ≤ R, We have | J(θ)∣ ≤ ιRγ. So We have J* = ιRγ.
5.1	Convergence Analysis of BGPO Algorithm
In the subsection, We provide convergence properties of the BGPO algorithm. The detailed proof is
provided in Appendix A.1.
Theorem 1. Assume the SeqUence {θk}K=1 be generated from Algorithm 1. Let ηk =⑺泉⑺ for
all k ≥ 1, 0 < λ ≤ Emb/2, b > 0, 8Lλ ≤ C ≤ m-^~, and m ≥ max{b2, (cb)2}, we have
1	XX EBΨk	2√2Mm1∕4 2√M
K 2LEkBλ,hVf(θk),θi(θk川 ≤ -K1∕2 — + ɪi/r,
where M = J -J" + EσLb + EmLb ln(m + K).
Remark 1. Without loss of generality, let b = O(1), m = O(1) and λ = O(1), we have M =
O(ln(m + K)) = O(1). Theorem 1 shows that the BGPO algorithm has a convergence rate of
O( K74 ). Let K-1 ≤ G we have K = O(e-4). Since the BGPO algorithm only needs one
trajectory to estimate the stochastic policy gradient at each iteration and runs K iterations, it has
the Sample complexity of 1 ∙ K = Ο(e-4) for finding an E-stationary point.
5.2 Convergence Analysis of VR-B GPO Algorithm
In the subsection, We give convergence properties of the VR-BGPO algorithm. The detailed proof
is provided in Appendix A.2.
Theorem 2. Suppose the sequence {θk}K=1 be generatedfrom Algorithm 2. Let ηk =(m+k)i/3 for
all k ≥ 0, 0 < λ ≤ VmLb-, b > 0, C ∈ [3∣3 + 20L2λ , m2 ] and m ≥ max(2, b3, (cb)3,(熹)2/3),
we have
1X EkBψk	(θ )k≤ 2√M0m1/6 + 2√2M0	(15)
K	EkBλ,hvf(θk),θ>(θk)k ≤	KP2	+ Kι∕3 ,	(15)
where M M= JJl + 1⅛⅛ + ⅛⅞2, L = L + 2G20W, G = Cg R/(1 — Y )2 and Cw =
qH(2HC^+ChΜw+i).
Remark 2. Without loss of generality, let b = O(1), m = O(1) and λ = O(1), we have M =
O(ln(m + K)) = O(1). Theorem 2 shows that the VR-BGPO algorithm has a convergence rate
of O(K1/3). Let K- 3 ≤ G we have K = (e-3). Since the VR-BGPO algorithm only needs
one trajectory to estimate the stochastic policy gradient at each iteration and runs K iterations, it
reaches a lower sample complexity of 1 ∙ K = O(e-3) for finding an E-stationary point.
6 Experiments
In this section, We conduct some RL tasks to verify the effectiveness of our methods. We first
study the effect of different choices of Bregman divergences With our algorithms (BGPO and VR-
BGPO), and then We compare our VR-BGPO algorithm With other state-of-the-art methods such
as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017), ProxHSPGA (Pham et al., 2020),
VRMPO (Yang et al., 2019), and MDPO (Tomar et al., 2020). Our code is available at https:
//github.com/gaosh/BGPO.
7
Published as a conference paper at ICLR 2022
(a) CartPole-v1
(b) Acrobat-v1
(c) MountainCarContinuous
Figure 1:	Effects of two Bregman Divergences: lp -norm and diagonal term (Diag).
(a) CartPole-v1
(b) Acrobat-v1
(c) MountainCarContinuous
Figure 2:	Comparison between BGPO and VR-BGPO on different environments.
6.1	Effects of Bregman Divergences
In the subsection, we examine how different Bregman divergences affect the performance of our
algorithms. In the first setting, We let mirror mapping ψk(x) = kxkp (p ≥ 1) With different p
to test the performance our algorithms. Let ψ^(y) = (Pd=IIyi|q) 1 be the conjugate mapping of
ψk (x), Where p-1 + q-1 = 1, p, q > 1. According to (Beck & Teboulle, 2003), When Θ = Rd,


the update of θk+1 in our algorithms can be calculated by θk+1 = Vψ^(Vψk(θk) + λuk), where
Vψk (xj) and Vψ^ (yj) are p-norm link functions, and Vψ⅛ (Xj) = Sign(Xj)P-J —, Vψ^ (yj)=
kxkp
sign':'.’!—.」"一,and j is the coordinate index of X and y. In the second setting, we apply diagonal
kykq
term on the mirror mapping ψk (x) = 2XTMkx, where Mk is a diagonal matrix with positive
values. In the experiments, we generate Hk = diag(√vk + α), Vk = βvk-1 + (1 — β)uk, and
α > 0, β ∈ (0, 1), as in Super-Adam algorithm (Kingma & Ba, 2014; Huang et al., 2021). Then
We have Dψ% (y, x) = 1 (y -
X)THk (y - x). Under this setting, the update of θk+1 can also be
analytically solved θk+1 = θk — λH-1 Uk.
To test the effectiveness of two different Bregman divergences, we evaluate them on three
classic control environments from gym Brockman et al. (2016): CartPole-v1, Acrobat-v1, and
MountainCarContinuous-v0. In the experiment, categorical policy is used for CartPole and Acrobot
environments, and Gaussian policy is used for MountainCar. Gaussian value functions are used in
all settings. All policies and value functions are parameterized by multilayer perceptrons (MLPs).
For a fair comparison, all settings use the same initialization for policies. We run each setting five
times and plot the mean and variance of average returns. For lp -norm mapping, we test three dif-
ferent values ofp = (1.50, 2.0, 3.0). For diagonal mapping, we set β = 0.999 and α = 10-8. We
set hyperparameters {b, m, c} to be the same. λ still needs to be tuned for different p to achieve
relatively good performance. For simplicity, we use BGPO-Diag to represent BGPO with diagonal
mapping, and we use BGPO-lp to represent BGPO with lp -norm mapping. Details about the setup
of environments and hyperparameters are provided in the Appendix C.
From Fig. 1, we can find that BGPO-Diag largely outperforms BGPO-lp with different choices ofp.
The parameter tuning of BGPO-lp is much more difficult than BGPO-Diag because each p requires
an individual λ to achieve the desired performance.
6.2	Comparison between BGPO and VR-BGPO
To understand the effectiveness of variance reduced technique used in our VR-BGPO algorithm, we
compare BGPO and VR-BGPO using the same settings introduced in section. 6.1. Both algorithms
use the diagonal mapping for ψ , since it performs much better than lp -norm. From Fig. 2 given
in the Appendix C, we can see that VR-BGPO can outperform BGPO in all three environments.
8
Published as a conference paper at ICLR 2022
(a) Pendulum-v2
(b) DoublePendulum-v2
(c) Walker2d-v2
(d) Swimmer-v2
(e) Reacher-v2
(f) HalfCheetah-v2
Figure 3:	Experimental results of our algorithms and other baseline algorithms on six environments.
In CartPole, both algorithms converge very fast and have similar performance, and VR-BGPO is
more stable than BGPO. The advantage of VR-BGPO becomes large in Acrobot and MountainCar
environments, probably because the task is more difficult compared to CartPole.
6.3 Compare to other Methods
In this subsection, we apply our BGPO and VR-BGPO algorithms to compare with the other meth-
ods. For our BGPO and VR-BGPO, we use diagonal mapping for ψ . For VRMPO, we follow their
implementation and use lp-norm for ψ . For MDPO, ψ is the negative Shannon entropy, and the
Bregman divergence becomes KL-divergence.
To evaluate the performance of these algorithms, we test them on six gym (Brockman et al., 2016)
environments with continuous control tasks: Inverted-DoublePendulum-v2, Walker2d-v2, Reacher-
v2, Swimmer-v2, Inverted-Pendulum-v2 and HalfCheetah-v2. We use Gaussian policies and Gaus-
sian value functions for all environments, and both of them are parameterized by MLPs. To ensure a
fair comparison, all policies use the same initialization. For TRPO and PPO, we use the implementa-
tions provided by garage (garage contributors, 2019). We carefully implement MDPO and VRMPO
following the description provided by the original papers. All methods include our method, are
implemented with garage (garage contributors, 2019) and pytorch (Paszke et al., 2019). We run
all algorithms ten times on each environment and report the mean and variance of average returns.
Details about the setup of environments and hyperparameters are also provided in the Appendix C.
From Fig. 3, we can find that our VR-BGPO method consistently outperforms all the other meth-
ods. Our BGPO basically reaches the second best performances. From the results of our BGPO,
we can find that using a proper Bregman (mirror) distance can improve performances of the PG
methods. From the results of our VR-BGPO, we can find that using a proper variance-reduced tech-
nique can further improve performances of the BGPO. ProxHSPGA can reach some relatively good
performances by using the variance reduced technique. MDPO can achieve good results in some
environments, but it can not outperform PPO or TRPO in Swimmer and InvertedDoublePendulum.
VRMPO only outperforms PPO and TRPO in Reacher and InvertedDoublePendulum. The unde-
sirable performance of VRMPO is probably because it uses lp norm for ψ, which requires careful
tuning of learning rate.
7 Conclusion
In the paper, we proposed a novel Bregman gradient policy optimization framework for reinforce-
ment learning based on Bregman divergences and momentum techniques. Moreover, we studied
convergence properties of the proposed methods under the nonconvex setting.
Acknowledgment
This work was partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002,
OIA 2040588.
9
Published as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.
Mohammad Gheshlaghi Azar, Vicenc Gomez, and Hilbert J Kappen. Dynamic policy programming.
The Journal ofMachine Learning Research,13(1):3207-3245, 2012.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200-217, 1967.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Yair Censor and Arnold Lent. An iterative row-action method for interval convex programming.
Journal of Optimization theory and Applications, 34(3):321-353, 1981.
Yair Censor and Stavros Andrea Zenios. Proximal minimization algorithm withd-functions. Journal
of Optimization Theory and Applications, 73(3):451-464, 1992.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
In Advances in Neural Information Processing Systems, pp. 15210-15219, 2019.
Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for robotics.
Foundations and Trends® in Robotics, 2(1-2):1-142, 2013.
Yuhao Ding, Junzi Zhang, and Javad Lavaei. On the global convergence of momentum-based policy
gradient. arXiv preprint arXiv:2110.10116, 2021.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
The garage contributors. Garage: A toolkit for reproducible reinforcement learning research.
https://github.com/rlworkgroup/garage, 2019.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In Thirty-sixth International Conference on Machine Learning, 2019.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient meth-
ods. In International Conference on Machine Learning, pp. 4422-4433. PMLR, 2020.
Feihu Huang, Junyi Li, and Heng Huang. Super-adam: Faster and universal framework of adaptive
gradients. Advances in Neural Information Processing Systems, 34, 2021.
Yujia Jin and Aaron Sidford. Efficiently solving mdps with stochastic mirror descent. In Interna-
tional Conference on Machine Learning, pp. 4890-4900. PMLR, 2020.
10
Published as a conference paper at ICLR 2022
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS,pp. 315-323, 2013.
Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14:1531-1538, 2001.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135, 2021.
Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimiza-
tion attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. Advances in Neural Information Processing
Systems, 33, 2020.
Gergely Neu, Anders Jonsson, and Vicenc Gomez. A unified view of entropy-regularized markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. In 35th International Conference on Machine Learn-
ing, volume 80, pp. 4026-4035, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8024-8035, 2019.
Nhan Pham, Lam Nguyen, Dzung Phan, Phuong Ha Nguyen, Marten Dijk, and Quoc Tran-Dinh. A
hybrid stochastic policy gradient algorithm for reinforcement learning. In International Confer-
ence on Artificial Intelligence and Statistics, pp. 374-385. PMLR, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In International Con-
ference on Learning Representations (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 5668-5675, 2020.
Claude E Shannon. A mathematical theory of communication. The Bell system technical journal,
27(3):379-423, 1948.
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
gradient. In International Conference on Machine Learning, pp. 5729-5738, 2019.
11
Published as a conference paper at ICLR 2022
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy
optimization. arXiv preprint arXiv:2005.09814, 2020.
Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradi-
ent descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920,
2019.
Qing Wang, Yingru Li, Jiechao Xiong, and Tong Zhang. Divergence-augmented policy optimization.
In Advances in Neural Information Processing Systems, pp. 6099-6110, 2019.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
Intelligence, pp. 191, 2019a.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
variance reduction. arXiv preprint arXiv:1909.08610, 2019b.
Long Yang, Gang Zheng, Haotian Zhang, Yu Zhang, Qian Zheng, Jun Wen, and Gang Pan. Policy
optimization with stochastic mirror descent. arXiv preprint arXiv:1906.10462, 2019.
Wenhao Zhan, Shicong Cen, Baihe Huang, Yuxin Chen, Jason D Lee, and Yuejie Chi. Policy
mirror descent for regularized reinforcement learning: A generalized framework with linear con-
vergence. arXiv preprint arXiv:2105.11066, 2021.
Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. On the con-
vergence and sample efficiency of variance-reduced policy gradient method. arXiv preprint
arXiv:2102.08607, 2021.
Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforce-
ment learning with reinforce. arXiv preprint arXiv:2010.11364, 2020.
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Bayar. Global convergence of policy gradient
methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383, 2019.
Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth
nonconvex optimization. arXiv preprint arXiv:1806.04781, 2018.
12
Published as a conference paper at ICLR 2022
A Appendix
In this section, we study the convergence properties of our algorithms. We first provide some useful
lemmas.
Lemma 1. (Proposition 4.2 in Xu et al. (2019b)) Suppose g(τ∣θ) is the PGT estimator Under
Assumption 1, we have
1)	g(τ ∣θ) is L-Lipschitz differential, i.e., kg(τ 仇)-g(τ ∣θ2)k ≤ L∣∣θι-θ2k forall θι, θ2 ∈ Θ,
where L = Ch R/(1 - γ)2;
2)	J(θ) is L-smooth, i.e., kV2J(θ)k ≤ L;
3)	g(τ ∣θ) is bounded, i.e., kg(τ ∣θ)k ≤ G forall θ ∈ Θ with G = Cg R/(1 — γ)2.
Lemma 2. (Lemma 6.1 in Xu et al. (2019a)) Under Assumptions 1 and 3, let W(T∣θk-ι,θk)=
g(τ∣θk-ι)∕g(τ∣θk), we have
V[w(τ ∣θk-i,θk)] ≤ Cw kθk — θk-ik2,	(16)
where Cw = JH(2HCg + Ch)(W + 1).
Lemma 3. (Lemma 1 in (Ghadimi et al., 2016)) Let X ⊆ Rd be a closed convex set, andφ : X → R
be a convex function but possibly nonsmooth, and Dψ : X × X → R is Bregman divergence related
to the ν -strongly convex function ψ. Then we define
x+ = arg min {〈g,z)+ γDψ (z, x) + φ(z)}, ∀x ∈ X	(17)
z∈X	λ
PX(x,g,λ) = 1(χ - χ+),	(18)
where g ∈	Rd,	λ > 0 and	Dψ (z, x) =	ψ(z)	—	ψ(x)	+ hVψ(x),	z —	xi	. Then the following
statement holds
hg,Pχ(x,g, λ)i ≥ νkPX(χ,g,λ)k2 + 1 [φ(x+) - φ(X)].	(19)
λ
Lemma 4. (Proposition 1 in Ghadimi et al. (2016)) Let x1+ and x2+ be given in (17) with g re-
placed by g1 and g2 respectively. Then let PX (x, g1, λ) and PX (x, g2, λ) be defined in (18) with x+
replaced by x1+ and x2+ respectively. we have
kPX (x,g1,λ) - PX (x,g2,λ)k ≤ V kg1 - g2k.	QO)
Lemma 5. (Lemma 1 in (Cortes et al., 2010)) Let w(x) = Q(X) be the importance WeightfOr dis-
tributions P and Q. The following identities hold for the expectation, second moment, and variance
of w (x)
E[w(x)] = 1, E[w2 (x)] = d2(P||Q),
V[w(x)] = d2(P||Q) — 1,	(21)
where d2(P||Q) = 2DIP||Q), and D(P||Q) is Renyi divergence between distributions P and Q.
Lemma 6. Suppose that the sequence {θk}kK=1 be generated from Algorithms 1 or2. Let0 < ηk ≤ 1
and 0 < λ ≤ 2V—, then we have
f(θk+l) - f(θk ) ≤ k^- ∣∣vf(θk ) - Uk k2-kk kθk+1 - θk k2.
ν	2λ
(22)
Proof. According to Assumption 1 and Lemma 1, the function f(θ) is L-smooth. Then we have
f(θk+ι) ≤ f(θk) + hVf(θk),θk+ι — θki + 2kθk+ι — θkk2	(23)
=f (θk ) + ηk hvf (θk), Θ]i+1 - θk i + 2k I&+1 - θk k2
Lη2
=f (θk ) + ηk hvf (θk) - Uk ,θk + 1 - θk i + Hkkuk , θ^k+1 - θk i + 2 kθk + 1 - θkk ,
13
Published as a conference paper at ICLR 2022
1	.1	1	1 ∙ . ∙ 1	. Zi	Zi	/彳	Zi ∖ -ɪ-ʌ . 1	.	A ∕' * 1	∙ . <	<	C
where the second equality is due to θk+ι = θk + ηk(θk+ι - θk). By the step 4 of Algorithm 1 or 2,
We have θk+ι = arg mi∏θ∈θ {(〃k, θ) + ɪDψk (θ, θk)}. By using Lemma 3 with φ(∙) = 0, We have
1 ,	~ 一 ..1,- T
huk , ʌ (θk - θk+1)〉≥ VIl ʌ (θk - θk+1 )k ∙	(24)
Thus, we can obtain
huk , θk + 1 - θk i ≤ - ∖ Il 石k+1 - θk ∣∣2∙	(25)
λ
According to the Cauchy-Schwarz inequality and Young,s inequality, we have
"f(θk) - Uk,θk+ι - θk〉≤ INf(θk) - Ukkl@+1- θkk
≤ -1Vf (θk) - uk∣∣2 + KIl京k+1 - θk∣∣2∙	(26)
V	4λ
Combining the inequalities (23), (25) with (26), we obtain
一 ,一	一 ._.	________ 〜	_.	,	〜	一. Lη2 ..〜	一 ..c
f (θk +1) ≤ f (θk ) + ηk hvf (θk ) - Uk ,θk+1 - θk i + ηk huk, θk+1 - θk i H-丁 ∣∣θk⅛1 - θk ||
≤f (θk)+ ηk^ ι∣vf Iek) - Uk ∣∣2 + νηk ∣∣(9k+1- θk Ii2 - νηk Ii 京 k+1- θk ∣∣2 + 2ηk ∣∣θk+1 - θk∣∣2
V	4λ	λ	2
=f (θk) + ηk- Ilvf (θk) - Uk Il2 - ~^η I&+1 - θk Il2 - (ηk^----------ηk)11 石k+1 - θk Il2
V	2λ	4λ	2
≤ f (θk) + -^- Ilvf (θk) - Uk Il2-M I&+1 - θk Il2,	(27)
V	2λ
where the last inequality is due to 0 < λ ≤ ^^―.
□
A.1 Convergence Analysis of BGPO Algorithm
In this subsection, we analyze the convergence properties of BGPO algorithm.
Lemma 7. Assume the stochastic policy gradient Uk+1 be generated from Algorithm 1, given 0 <
βk ≤ 1, we have
2
EkVf (θk+1)- Uk + 1||	≤ (1 - βk +I)EkVf(θk )- Uk ∣∣ + 飞-L ηkEkθk+1 - θk ∣∣ + βk+1σ ∙
βk+1
Proof. By the definition of Uk+1 in Algorithm 1, we have
Uk + 1 - Uk = -ek+1Uk - βk+1g(τk+1∣θk + 1)∙
(28)
Since Vf (θk) = -J(θk) for all k ≥ 1, we have
EkVf (θk+1) - Uk+1∣2
=Ek- VJ(θk) - Uk - VJ(θk + 1) + VJ(θk) - (Uk+1 - Uk)||2
=Ek- VJ(θk) - Uk - VJ(θk + 1) + VJ(θk) + ek+1Uk + βk⅛1g(τk⅛1 ∣θk+1) ∣2
=Ek(I- βk+1)(-VJ(θk) - Uk) + βk+1(-VJ(θk+1) + g(τk+1 ∣θk +1))
+ (1 - βk + 1)( - VJ(θk+1) + VJ(θk))∣∣2
= (I- βk + 1)2EkVJ (θk ) + Uk + VJ (θk+I)-VJ (θk )∣∣2 + e2+1EkVJ (θk+1) - g(τk+1 ∣θk+1) ∣∣2
≤ (1 - βk+1)2(1 + βk+1)E∣VJ(θk)+ Ukk2 + (1 - βk+1)2(1 + 4)EkVJ(θk+1) - VJ(θk)∣2
Qk+1
+ β2+1EllVJ(θk+1) - g(τk+1∣θk + 1)∣∣2
≤ (1 - βk + 1)EllVJ(θk) + Uk ∣∣2 +
≤ (1 - βk + 1)EllVJ(θk) + Uk ∣∣2 +
≤ (1 - βk + 1)EllVf (θk) - Uk ∣∣2 +
2
βk+1
2
βk+1
2
βk+1
EkVJ(θk+1) - VJ(θk)∣∣2 + β"1E∣∣VJ(θk+1) - g(τk+1∣θk+1 )∣∣2
L2E∣∣θk+1 - θk∣∣2 + β2+1E∣∣VJ(θk+1) - g(τk+1∣θk+1)∣∣2
L2ηkEk 京 k+1 - θk ∣∣2 + β2+1σ2,
(29)
14
Published as a conference paper at ICLR 2022
where the fourth equality holds by Eτk+ι-p(τ∣θk+ι) [g(τk+1∣θk+1)] = VJ(θk+ι); the first inequality
holds by Young’s inequality; the second inequality is due to 0 < βk+1 ≤ 1 such that (1-βk+1)2(1+
βk +1) = 1 - βk + 1 - β2+1 + β3 + 1 ≤ 1 - βk+1 and (I - βk +1)2(I + βk+l) ≤ 1 + β+ ≤ β+;
the last inequality holds by Assumption 2.	□
Theorem 3. Assume the sequence {θk}K=ι be generated from Algorithm 1. Let ηk =(①+：产/2 for
all k ≥ 1, 0 < λ ≤ Vm/2, b > 0, 8Lλ ≤ C ≤ m/, and m ≥ max{b2, (cb)2}, we have
1	X E| ∣BΨk	2√2Mm" 2√2M
K MEkBλ,hvf(θk),θ>(θk )k ≤ —Kr2 — + ~ktγ，
where M = JJ + 品 + m2 ln(m + K) )
Proof. Since ηk =(m+：)i/2	is decreasing on k, We have ηk	≤ no = mb72 ≤	1 for all k ≥ 0. At
the same time, let m ≥	(cb)2, we have βk+ι = cnk	≤ cno =	m^ ≤ 1. Consider m ≥ (cb)2, we
kcmα C m m1/2 霜ncP ∩	/ ∖	vm1/2 .s,,1 iɔɑνm V ν	vm1/2	Vm1/2 一 ν	ν all Zf、∩
have C ≤ ~Γ~. Since 0	< λ	≤ ~LLΓ, we have λ ≤	-LLb- ≤	~LLb- ― 2Lη0 ≤	2Lηk for all k ≥ 0∙
According to Lemma 7, we have
EkVf(θk+1)-uk+1k2-EkVf(θk)-ukk2
2
≤ -βk + 1Ekv f (θk ) - Uk k + 飞-L nkEkθk+1 - θk k + βk + 1σ
βk+1
2L2
=-Crnk EkVf (θk ) - Uk k H-nk ElIθk+1 - θk k + C Tkσ
C
=-8LλnkEkVf(θk) - Ukk2 + LVnkEkθk+ι - θkk2 + mnk—,
ν	4λ	b2
(30)
where the first equality is due to βk+ι = Cnk and the last equality holds by 8Lλ ≤ C ≤ m-.
Next we define a Lyapunov function Φk = E [f (θk) + L kVf(θk) 一 Uk k2] for any t ≥ 1. Then we
have
φk+1 - φk = E [f (θk+l) - f(θk) + y (kVf (θk+l) - uk+1 k2 - kVf (θk ) - uk ∣∣2)]
L
≤ -Ek EkVf (θk ) - uk k2 kE Ek 京 k+1 - θk k2-kE EkVf (θk ) - uk k2
ν	2λ	ν
22
+ τλ Ekθk+1 - θk k2 +
22
≤ - ~7~nk EkVf (θk ) - uk k2 - TYnk Ek 京 k+1 - θk k2 +	τ22 ,
4ν	4λ	Lb2
(31)
where the first inequality follows by the Lemma 6 and the above inequality (30).
15
Published as a conference paper at ICLR 2022
Summing the above inequality (31) over k from 1 to K, we can obtain
Kλ	ν
X 国石 ηk Ek^f (θk ) - Uk k2 + 4λnk Ek"k+1 - θk k2]
k=1	ν
2K
mσ2	2
≤ φ1 - φκ+1 + ^Lb2 工 ηk
k=1
1	1	mσ2 K
=f(θ1) - f(θκ+1) + v∣Vf(θ1) - u1k - 丁INf(θκ+1) - uκ+1k + -j-r2 Xηk
L	L	Lb2
k=1
2K
1	mσ
≤ J(θκ+1) - J(θ1) + L ∣∣VJ(θ1) - g(τ1 lθι)k + ɪp" Xηk
k=1
≤ J* - J(θ1)+σ2+mσ2 ZK -ɪɪdk
L L 1 m+k
σ2	mσ2
≤ J* - J(θ1) + L + 丁 ln(m + K),
where the last second inequality holds by Assumptions 2 and 4.
Since ηk is decreasing, we have
1K 1	1
K ΣSE[4V2 Ek^f (θk ) - Ukk + 4λ2 Ekθk+1-θk Il ]
(32)
k=1
≤ J*- J (θ1) +
KνληK
≤ (J* - J(θ1) .
一 I νλb
σ
KνλLηK
σ2
2
mσ2
+	— ln(m + K)
νλLKηK
2
mσ2
+ νλLb + νλLb
ln(m + K)) (m+K)1/2
(33)
Let M = J -J(θ1) + νσLb + VmL ln(m + K), the above inequality (33) reduces to
1K 1	1	M
K ΣSE[4V2 Ek^f (θk ) - Uk k + 4λ2 Ekθk+1 - θk k ] ≤ K (m + K)	.
k=1
According to Jensen’s inequality, we have
1K 1	1
K E 叫 27, k^f (θk ) - ukk + 2λ kθk+1 - θk k]
K 2ν	2λ
k=1
2K 1	1
≤ (K X E[4V2 k^f (θk) - Ukk + 4λ2 kθk + 1 - θkk ])
≤√2M (m + K)1/4 ≤ √Mm1/4 + √2M
—K 1/2 (m + K )	― K 1/2	+ K 1/4 ,
(34)
(35)
where the last inequality is due to the inequality (a + b)1/4 ≤ a1/4 + b1/4 for all a, b ≥ 0. Thus we
have
1K 1	1
K £ 叫 V Il^f (θk) - ukk + λ kθk + 1 - θkk] ≤
k=1
2√2Mm1/4	2√2M
+	.
K 1/2
(36)
By the step 4 of Algorithm 1, we have
Bψkhuk,θi (θk) = Pθ(θk,Uk,λ) = 1 (θk - θk+1).
(37)
16
Published as a conference paper at ICLR 2022
At the same time, as in Ghadimi et al. (2016), We define
Btkf(M,θi (θk) = Pθ(θk, Vfg ),λ) = 1 (θk -θ++ι),	(38)
Where
θ++ι = argmin {(Vf (θk),θ') + 1 Dψ (θ,θk)}.	(39)
θ∈Θ	λ
According to the above Lemma 4, we have ∣∣Bψ^ufcιθi(θk )-Bψ,kvfg及i(θk )k ≤ ɪ ∣∣uk-Vf (θk )k.
Then We have
kB3f(W,"k)k ≤ 网,Zk,θ)(θk)k + kBψ‰,θ)(θk) -Bψbf(∕)(θk)k
≤ ∣Bψkuk,θ)(θk)∣ + 1 ∣Uk-Vf(θk)∣
11
=ʌ I∣θk + 1 - θk∣∣ + V IIuk- V f (θk )∣.	(40)
By the above inequalities (36) and (40), we have
1 X FuBψk	(θ ^∣∣< 2√2Mm1/4 , 2√2m	⑷、
K MFkBλ,hVf(θk),θi(θk并 ≤ —K1/2 — + Kξiμ-.	(41)
□
A.2 Convergence Analysis of VR-BGPO algorithm
In this subsection, we will analyze convergence properties of the VR-BGPO algorithm.
Lemma 8. Assume that the stochastic policy gradient uk+1 be generated from Algorithm 2, given
0 < βk ≤ 1, we have
FkVf (θk+1) - uk + 1∣∣2 ≤ (I- βk+I)FllVf (θk) - uk ∣∣2 + 4L2η2 ∣∣^k⅛1 - θk ∣∣2 + 2β2+ισ2,
where L2 = L2 + 2G2C2 and Cw = ,H(2HCg + Ch)(W + 1).
Proof. By the definition of uk+1 in Algorithm 2, we have
uk+1 - uk
=-βk+1 uk - βk+1g(τk+1∣θk +1) + (1 - βk+1) ( - g(τk+1∣θk +1) + w(τk+1∣θk ,θk+1)g(τk+11θk )) ∙
Since Vf (θk+1) = -VJ(θk+1), we have
F∣Vf (θk+1) - uk+1∣2
FkVJ(θk) + uk + VJ(θk + 1) - VJ(θk) + (uk + 1 - uk)∣∣2
(42)
FkVJ(θk) + uk + VJ(θk + 1) - VJ(θk) - βk+1uk - βk+1g(τk+1 ∣θk⅛1)
+ (1 - βk + 1) ( - g(τk + 1 ∣θk+1) + w(τk + 1 ∣θk, θk+1 )g(τk + 1 ∣θk)) ∣∣2
F∣(1 - βk+1)(VJ(θk)+ uk) + βk+1(VJ(θk+1) - g(τk+1∣θk+1))
-(1 - βk+1)(g(τk+1∣θk+1) - w(τk+1∣θk,θk+1)g(τk+1∣θk) - (VJ(θk+1) - VJ(θk)))∣∣2
(1 - βk+1)2F∣∣VJ(θk) + ukk2 + F∣βk+1(VJ(θk+1) - g(τk+1∣θk+1))
-(1 - βk+1)(g(τk+1∣θk+1) - w(τk+1∣θk,θk+1)g(τk+1∣θk) - (VJ(θk+1) - VJ(θk)))∣∣2
≤ (1 - βk + 1)2FkVJ (θk ) + uk ∣∣2 + 2βk + 1FkVJ (θk + 1) - g(τk + 1∣θk+1)∣∣2
+ 2(1 - βk+1 )2Fkg(Tk+ 1 ∣θk+1) - w(τk + 1∣θk, θk+1 )g(τk + 1∣θk) - (VJ(θk + 1) - VJ(θk))∣2
≤ (1 - βk+1)F∣Vf (θk) - ukk2 + 2β2+1σ2
+ 2 Fkg(Tk+1 ∣θk+1) - w(τk+1∣θk,θk+1)g(τk+1∣θk)∣∣2,
X-------------------------V-----------------------}
=T1
17
Published as a conference paper at ICLR 2022
where the forth equality holds by 旧八十]~以丁取十ι)[g6+ι∣θk+ι)] = VJ(θk+ι) and
Eτk+ι~p(.τ∣θfc+ι)[g(τk+ι∣θk+ι) - w(τk+ι∣θk,θk+ι)g(τk+ι∣θk)] = VJ(θk+ι) - VJ(θk); the sec-
ond last inequality follows by Young,s inequality; and the last inequality holds by Assumption 2,
and the inequality EkZ - E[Z]∣∣2 = EkZk2 - (E[Z])2 ≤ ElIZIl2, and 0 < βk+ι ≤ 1.
Next, we give an upper bound of the term T11 as follows:
Ti = Ekg(Tk+ι∣θk+ι) - w(τk+ι∣θk,θk+ι)g(τk+ι∖θk)∣2
=Ekg(Tk+ι∣θk+ι) - g(τk+ι∣θk) + g(τ^k+ι∣θk) - W(Tk+ι∣θk,θk+1)g(τk+1∣θk)k2
≤ 2E∣∣g(τk+ι∣θk+ι) - g(τ^k+ι∣θk)k2 + 2E∣∣(1 - w(τ^k+ι∣θk, θk+1))g(τk+1∣θk)∣∣2
≤ 2L2∣∣θk+1 - θk ∣∣2 + 2G2Ek1 - W(Tk+ 1∣θk, θk+1 )k2
=2L2∣∣θk+1 - θk ∣∣2 + 2G2V(w(τk + 1 ∣θk, θk+1))
≤ 2(L2 + 2G2CW)∣∣θk+1- θkk2,	(43)
where the second inequality holds by Lemma 1, and the third equality holds by Lemma 5, and the
last inequality follows by Lemma 2.
Combining the inequalities (42) with (43), let L2 = L2 + 2G2CW, we have
EkVf(θk+1) - uk+1k2 ≤ (1 - βk+1)E∣∣Vf(θk) - ukk2 + 2β2+1σ2 + 4L2kθk+1 - θk∣∣2
=(1 - βk+1)E∣∣Vf (θk) - ukk2 + 2β2+1σ2 + 4L2η2^k+1 - θkk2.
□
Theorem 4. Suppose the SeqUenCe {θk}31 be generatedfrom Algorithm 2. Let ηk =(团十盛产分 for
all k ≥	0,	0 < λ ≤	ν~^b-,	b > 0,	ɜlɜ	+ 20L2λ	≤ c ≤	mj2	and m ≥ max(2,	b3, (cb)3,(磊)2/3),
we have
1 K
KK X Ek啰▽/(i(θk )k≤
2√2M7 m1/6	2√2M7
K1/2	+ K1/3
(44)
where M7 = J-J件 + 1⅛⅛ + 密工 and L2 = L2 + 2G2CW.
Proof. Since ηk =(m+k)i/3 on k is decreasing and m ≥ b3, we have ηk ≤ no = mbɜ ≤ 1. Due
to L = √L2 + 2G2C2 ≥ L, we have 0 < λ ≤ VmI^ ≤ νm⅛3 = ɪ ≤ ɪ for any k ≥ 0.
w	5Lb	2Lb	2Lηo	2Lηk
Consider 0 < % ≤ 1 and m ≥ (cb)3, we have βk+1 = cn2 ≤ m‰∙ ≤ 1. At the same time, we have
c ≤ m2/3. According to Lemma 8, we have
—EkVf (θk+1) - uk + 1∣∣2---EkVf (θk ) - Uk ∣∣2
nk	nk-1
≤ (1 - βk+1 - ɪ)E∣∣Vf(θk) - Ukk2 + 2β2+1σ2 + 4L2nkkθk+1 - θkk2
nk	nk-1	nk
=(-----------cnk )EkVf (θk ) - Uk ∣∣2 + 2C2n3σ2 + 4L 2nk k 京 k+1 - θk ∣∣2
nk	nk-1
=(b((m + k)3 - (m + k -1)3) - Cnk)EkVf(θk) - ukk2 + 2c2n3σ2 + 4L2nkIl京k+1 - θkIl2
≤ (3^3nk - Cnk)EkVf(θk) - uk∣∣2 + 2c2nQ2 + 4L2nkk京k+1 - θk∣∣2,	(45)
where the last inequality holds by the following inequality
(m + k) 1 - (m + k - I) 1 ≤ 3(m + L 1)2/3 ≤ B""
22/3	22/3	b2	22/3 2	2
-3(m + k)2/3	3b2 (m + k)2/3	3b2 "k - 3b2"k，"①
18
Published as a conference paper at ICLR 2022
where the first inequality holds by the concavity of function f (x) = x1/3, i.e., (x + y)1/3 ≤
x1/3 + 3χ2∕3; the second inequality is due to m ≥ 2, and the last inequality is due to 0 < ηk ≤ 1.
Let C ≥ 3b3 + 20V2λ, we have
三EkVf (θk+ι)- uk+ιk2 - B EkVfg)-ukk2
2θL 2λ2	..........C CCC "	..~	...
≤------2-ηkEkv f (θk) - Ukk + 2c ηkσ	+4L ηk kθk + 1 - θkk .
ν2
Here We simultaneously consider C ≥ 煮 + 20L2λ2, C ≤ m/3 and 0 < λ ≤ Vm；：, We have
2	2θL2λ2	2	20L2 ν2m2∕3	2	4m2/3	m2/3
+≤	+=	+≤
3b3 +	ν2	—	3b3	+	ν2	25L 2b2	3b3	+	5b2	—	b2
(47)
(48)
Then We have m ≥ (65b )2/3.
Next we define a Lyapunov function Ωk = E [f(θk) +1通 黑——∣∣Vf (θk) 一 Uk ∣∣2] for any k ≥ 1.
According to Lemma 6, We have
°k+1 一 °k=f (θk+1) 一 f (θk)+ι⅛ι QEkVf (θk+1) - uk+1k2 - BEkVf (θk) - ukk2
≤ ηk- kVf (θk) - uk∣∣2-黑 I&+1 - θk ∣∣2-ηkEEkVf (θk) 一 Uk ∣∣2
ν	2λ	4ν
l VnkEIM	θ ∣∣2 l νc2η3σ2
十 五 Ekθk+1 - θk k +^L2T
≤ 一乎EkVf(θk) — Ukk2 — 甘Ekθk+1 — θkk2 + νc2^n3σ2,	(49)
4ν	4λ	8L2λ
where the first inequality is due to the above inequality (47). Thus, we can obtain
-kEE∣∣Vf(θk) — Ukk2 + nkEE|&+i — θkk2 ≤ Ωk 一 Ωk+ι + “°	.	(50)
4ν	4λ	8L2λ
Taking average over k = 1,2,…，K on both sides of (50), we have
T X叫TVEEkVf (θk)- Ukk2 + ^4kEkθk+ι - θk∣∣2]
k=1	ν
≤ f(。。- f(θκ+ι) + VkVf(&)—Uik2 一 V||Vf(0K+i)— Uκ+ιk2 +工 X νc2n3σ2
—	K	16L2noλK	16L2nκλK	K k=1 8L2λ
≤ J(%+i) - J(&) + νσ2	+ ɪ X νc2n3σ2
― K	16L2noλK	K J 8L2λ
≤ J 一 J(&) + νσ2	+ °X νc2n3σ2	(51)
≤ —K ― + 16L2ηoλK + K L FT,	()
where the second inequality is due to ui = -g(Ti|0i), Vf (θι) = -VJ(θι) and Assumption 1,
and the last inequality holds by Assumption 2. Since ηk is decreasing, i.e., ηK-1 ≥ ηk-1 for any
19
Published as a conference paper at ICLR 2022
0 < k ≤ K, we have
1K 1	1
K E 叫 4V2 Ek^f (θk ) - Uk k + 4λ2 Ekθk + 1 - θkk ]
J* -J(θι)	σ2	1	X c2η3σ2
≤	1	+
Kνληκ	16L2ηκηoλ2κ	Kληκ 念 8L2λ
< J* - J(θι)	m1/3σ2	c2σ2	Γk b3
κ KVλnκ	16bL2λ2ηκK	8L2λ2Kr∣κ Ji m + k
J* - J(θι) m1/3σ2	c2σ2b3	、/ C
≤ —77r------1----X-------1 X--------ln(m + K)
-Kληκ	i6bL 2λ2ηκ K	8L 2λ2Kηκ
(J* — J(θι)	m1∕3σ2	c2σ2b2 ʌ (m + K)1/3
=1―bνλ — + 16b2L2λ2 + 8L2λ2 )	K ,
where the second inequality holds by Pkκ=1 ηk3dk ≤ R1κ ηk3dk = b3 R1κ (m + k)-1dk.
Let M0 = J*-Jλθ1) + Im；；；：2 + 8L2λ2, the above inequality (52) reduces to
1 κ 1	1	M0
K X E[ 4V2 k^f (θk ) - uk k + 4λ2 kθk+1 - θk k ] ≤ ~K (m + K) ..
(52)
(53)
According to Jensen’s inequality, we have
1κ 1	1
K E 叫 27 k^f (θk ) - Ukk + 2λ llθk+1 - θk II]
K 2ν	2λ
k=1
2κ 1	1
≤ ( K X E[ 4V2 kvf (θk ) - Ukk + 4λ2 kθk + 1 -θkk ])
≤√Kl(m + K)1/6 S「+ √KM0，	(54)
where the last inequality is due to the inequality (a + b)1/6 ≤ a1/6 + b1/6 for all a, b ≥ 1. Thus we
have
K XXE[ 1 kVf (θk) - Ukk + 1 kθk+i - θkk] ≤ 2√K∕m1∕6 +2√√2M0.	(55)
k=1
Then by using the above inequality (40), we can obtain
ɪ XX EkBψk	(θ )k< 2√2M7m1/6 , 2√2M7	(56)
K rEkBλ,hVf(θk),θ>(θk)k ≤	K/2	+ K1/3 .	(56)
□
B Actor-Critic Style BGPO and VR-B GPO Algorithms
In the experiments, we use the advantage-based policy gradient estimator:
H-1
g(τ∣θ) = X Vlog∏θ(αt∣St)Aπθ(st,at),	(57)
t=0
20
Published as a conference paper at ICLR 2022
Algorithm 3 BGPO Algorithm (Actor-Critic Style)
1:	Input: Total iteration K, tuning parameters {λ,b,m,c} and mirror mappings {ψk}K=ι are
ν-strongly convex functions;
2:	Initialize: θι ∈ Θ, θV ∈ Θv and sample a trajectory τι from p(τ∣θι), and compute uι =
-g(T“θ0;
3:	for k = 1, 2, . . . , K do
4:	# Update the policy network
5:	Update θk+ι = argminθ∈θ {hu,θ} + 1 Dψ%(θ, θk)};
6:	Update θk+ι	=	θk	+ ηk(θk+ι	- θk) With	n®	=(砧晨)”?;
7:	# Update the value network
8:	Update θkv+1 by solving the subproblem (58);
9:	# Sample a new trajectory and compute policy gradients
10:	Sample a trajectory tk+i fromp(τ∣θk+ι),and compute uk+i = -βk+ιg(τk+ι∣θk+ι) + (1 -
βk+1)uk With βk+1 = cηk;
11:	end for
12:	Output: θζ chosen uniformly random from {θk}kK=1.
Algorithm 4 VR-BGPO Algorithm (Actor-Critic Style)
1:	Input: Total iteration K, tuning parameters {λ, b, m, c} and mirror mappings {ψk}kK=1 are
ν-strongly convex functions;
2:	Initialize: θι ∈ Θ, θV ∈ Θv and sample a trajectory τι from p(τ∣θι), and compute uι =
-g(T“θQ;
3:	for k = 1, 2, . . . , K do
4:	# Update the policy network
5:	Update 石 k+i = arg minθ∈θ {hu ,θ + 1 Dψk (θ,θk)};
6:	Update θk+ι = θk + nk(θk+ι - θ) with nk =("尸八;
7:	# Update the value network
8:	Update θkv+1 by solving the subproblem (58);
9:	# Sample a new trajectory and compute policy gradients
10:	Sample a trajectory tk+i fromp(τ∣θk+ι),and compute uk+i = -βk+1g(τk+1∣θk+1) + (1 -
βk+ι) [uk - g(τk+1∣θk+1) + w(τk+ι∣θk,θk+1)g(τk+1∣θk)] with βk+ι = cη2;
11:	end for
12:	Output: θζ chosen uniformly random from {θk}kK=1 .
where θ(∈ Θ ⊆ Rd) denotes parameters of the policy network, and ^lπθ (s, a) is an estimator of
the advantage function Aπθ (s, a). In using advantage-based policy gradient, we also need the state-
value function Vπθ (s). Here, we use a value network Vθv (s) to approximate the state-value function
Vπθ (s). Specifically, we solve the following problem to obtain the value network:
H-1
θm∈inv L(θv ) ：= X (%v (st) - * V πθ (st)):	(58)
θ ∈Θ	t=0
where θv (∈ Θv ⊆ Rdv) denotes parameters of the value network, and Vπθ (s) is an estimator of the
state-value function V πθ (s), which is obtained by the GAE Schulman et al. (2016). Then we use
the GAE to estimate √lπθ based on value network Vθv. We describe the actor-critic style BGPO and
VR-BGPO algorithms in Algorithm 3 and Algorithm 4, respectively.
C Detailed Setup of Experimental Environments and
Hyper-parameters
In this section, we provide the detailed setup of experimental environments and hyper-parameters.
We first provide the detailed setup of our experiments in Tab. 2 and Tab. 3. We use ADAM optimizer
to optimize value functions for all methods and settings, which is a common practice. The impor-
21
Published as a conference paper at ICLR 2022
Environments	CartPole-v1	Acrobat-v1	MountainCar-v0
Horizon	100	二	500	500
Value function Network sizes	32 × 32	32 × 32	32 × 32
Policy network sizes	8 × 8	8 × 8	64 × 64
Number of timesteps	5 × 105	5 × 106	7.5 × 106
Batchsize	50	100	100
VR-BGPO/BGPO {b, m, c} BGPO-lp {λp=1.5, λp=2.0, λp=3.0}	{1.5, 2.0, 25}	{1.5, 2.0, 25}	{1.5, 2.0, 25}
	{0.0064,0.0016, 0.0008}	{0.016,0.004, 0.001}	{0.016, 0.004, 0.001}
BGPO-Diag/VR-BGPO-Diag λ	1 × 10-3	1 × 10-3	1 × 10-3
Value function learning rate	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3
Table 2: Setups of environments and hyper-parameters for experiments in section 6.2 and section 6.3.
The learning rate of value functions are the same for all methods.
Environments	Pendulum-v2	DoUbIePendUlum-v2	WaIker2d-v2	SWimmer-V2	Reacher-v2	HaIfCheetah-v2
Horizon	500 =	500	500 =	500 =	500 =	500
Value function Network sizes	32 × 32	32 × 32	32 × 32	32 × 32	32 × 32	32 × 32
Policy network sizes	64 × 64	64 × 64	64 × 64	64 × 64	64 × 64	64 × 64
Number of timesteps	5 × 106	5 × 106	1 × 107	1 × 107	1 × 107	1 × 107
Batchsize	100	100	100	100	100	100
VR-BGPO {b, m, c}	{1.50, 2.0,25}	{1.50,2.0,25}	{1.50, 2.0, 25}	{1.50, 2.0, 25}	{1.50, 2.0, 25}	{1.50, 2.0,25}
VR-BGPO λ	1 × 10-2	1 × 10-2	1 × 10-2	5 × 10-4	5 × 10-4	5 × 10-4
TRPO/PPO learning rate	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3
MDPO learning rate	3 × 10-3	3 × 10-3	3 × 10-3	3 × 10-3	3 × 10-3	3 × 10-3
VRMPO learning rate	5 × 10-3	3 × 10-4	1 × 10-2	2 × 10-4	5 × 10-5	5 × 10-5
Value function learning rate	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3	2.5 × 10-3
Table 3: Setups of environments and hyper-parameters for experiments in section 6.4. The learning
rate of value functions are the same for all methods.
tance sampling weight used for VR-BGPO algorithm is clipped within [0.5, 1.5]. The momentum
term βk is set to be less or equal than one (βk = min(βk, 1.0) ) through the whole training process.
BGPO and VR-BGPO algorithms involve 4 hyper-parameters {λ, b, m, c}, which may bring addi-
tional efforts for hyper-parameter tuning. However, the actual hyper-parameter tuning is not so hard,
and we only use one set of {b, m, c} for 9 environments. The strategy of hyper-parameter tuning is
to separate the four hyper-parameters into two parts. The first part is {b, m, c}, which mainly decide
when the momentum term βk actually affects (βk < 1.0) updates. The second part, λ, only affects
how fast the policy is learning. To further reduce the complexity of hyper-parameter tuning, we
always set m = 2. By grouping hyper-parameters, we only consider λ and how βk changes, which
largely simplifies the process of hyper-parameter tuning.
22