Published as a conference paper at ICLR 2022
A Generalized Weighted Optimization Method
for Computational Learning and Inversion
Bjorn Engquist
The University of Texas at Austin
Austin, TX 78712, USA
Kui Ren
Columbia University
New York, NY 10027, USA
Yunan Yang
ETH Zurich
Zurich, Switzerland
engquist@oden.utexas.edu kr2002@columbia.edu
yyn0410@gmail.com
Ab stract
The generalization capacity of various machine learning models exhibits different
phenomena in the under- and over-parameterized regimes. In this paper, we focus
on regression models such as feature regression and kernel regression and ana-
lyze a generalized weighted least-squares optimization method for computational
learning and inversion with noisy data. The highlight of the proposed framework
is that we allow weighting in both the parameter space and the data space. The
weighting scheme encodes both a priori knowledge on the object to be learned and
a strategy to weight the contribution of different data points in the loss function.
Here, we characterize the impact of the weighting scheme on the generalization
error of the learning method, where we derive explicit generalization errors for the
random Fourier feature model in both the under- and over-parameterized regimes.
For more general feature maps, error bounds are provided based on the singular
values of the feature matrix. We demonstrate that appropriate weighting from
prior knowledge can improve the generalization capability of the learned model.
1	Introduction
Given N data pairs {xj, yj}jN=1, where xj ∈ R, yj ∈ C, j = 1, . . . , N, we are interested in learning
a random Fourier feature (RFF) model (Rahimi & Recht, 2008; Liao et al., 2020; Xie et al., 2020)
P-1
fθ(x)= X θkeikx, x∈ [0, 2π],	(1)
k=0
where P ∈ N is a given positive integer and We used the short-hand notation θ := (θo, ∙∙∙ ,θp-I)T
with the superscript T denoting the transpose operation.
This exact model as well as its generalization to more complicated setups have been extensively
studied; see for instance Liao & Couillet (2018); Shahrampour & Kolouri (2019); d’Ascoli et al.
(2020); Li et al. (2020); OzcelIkkale (2020); LiU et al. (2020; 2021) and references therein. While
this model may seem to be overly simplified from a practical perspective for many real-world appli-
cations, it serves as a prototype for theoretical understandings of different phenomena in machine
learning models (Sriperumbudur & Szabo, 2015; Belkin et al., 2020; Li et al., 2021a).
A common way to computationally solve this learning problem is to reformulate it as an optimization
problem where we find θ by minimizing the model and data mismatch for a given dataset. In
this paper, we assume that the training data are collected on a uniform grid of x over the domain
[0, 2π]. That is, {xj = 2Nj }工1. Let ωw = exp( 2Ni) where i is the imaginary unit. We introduce
Ψ ∈ CN ×P to be the feature matrix with elements
(Ψ)jk=(ωN)jk, 0≤j≤N-1, 0≤k≤P-1.
Based on the form of fθ(x) in (1), we can then write the 2-norm based data mismatch into the form
PN-11 ∣fθ(Xj) - yj∣2 = ∣∣Ψθ - yk2 where the column data vector y = (yo,…，yN —i)T. The
learning problem is therefore recast as a least-squares optimization problem of the form
θb = arg min ∣Ψθ - y∣22 ,	(2)
θ
1
Published as a conference paper at ICLR 2022
assuming that a minimizer does exist, especially when we restrict θ to an appropriate space.
In a general feature regression problem, the Fourier feature {eikx }kP=-01 is then replaced with a dif-
ferent feature model {夕k(χ)}P=-01, while the least-squares form (2) remains unchanged except that
the entries of the matrix Ψ is now Ψjk = Wk (Xj). We emphasize that this type of generalization will
be discussed in Section 5. Moreover, we remark that this least-squares optimization formulation is
a classical computational inversion tool in solving the general linear inverse problems of the form
Ψθ = y; see for instance Engl et al. (1996); Tarantola (2005) and references therein.
Previous work on weighted optimization for feature and kernel learning. Xie et al. (2020) stud-
ied the fitting problem for this model under the assumption that the coefficient vector θ is sampled
from a distribution with the property that γ is a positive constant,
Eθ [θ] = 0, Eθ [θθ*]= cγΛ-P2γ,	(3)
where the superscript * denotes the Hermitian transpose and the diagonal matrix Λ[p] has diagonal
elements (Λ[P ])kk = tk = 1 + k, k ≥ 0. That is,
Λ[P]= diag{t0, t1, t2, . . ., tk, . . . , tP-1}, tk := 1 + k.	(4)
The subscript [P] indicates that Λ[P]is a diagonal submatrix of Λ that contains its element indexed
in the set [P] := {0, 1, …，P - 1}. The normalization constant cγ = 1/(PP-c1(1 + k)-2γ) is
only selected so that Eθ[kθk2] = 1. It does not play a significant role in the rest of the paper.
The main assumption in (3) says that statistically, the signal to be recovered has algebraically decay-
ing Fourier coefficients. This is simply saying that the target function we are learning is relatively
smooth, which is certainly the case for many functions as physical models in practical applications.
It was shown in Xie et al. (2020) that, to learn a model with p ≤ P features, it is advantageous to
use the following weighted least-squares formulation
θbp = Λ[-p]β wb, with wb = arg minθ kΨ[N×p]Λ[-p]βw - yk22,	(5)
when the learning problem is overparameterized, i.e., p > N. Here, Ψ[N×p] ∈ CN×p is the matrix
containing the first p columns of Ψ, and β > 0 is some pre-selected exponent that can be different
from the γ in (3). To be more precise, we define the the generalization error of the learning problem
Ee(P,P, N) ：= Eθ [kfθ(X)- fbp(x)kL2([0,2∏])] = Eθ Ubp- θk2]，	(6)
where the equality comes from the Parseval’s identity, and θbp is understood as the vector
(θT, 0,…，0)t so that θ and θp are of the same length P. The subscript θ in Eθ indicates that
the expectation is taken with respect to the distribution of the random variable θ. It was shown
in Xie et al. (2020) that the lowest generalization error achieved from the weighted least-squares
approach (5) in the overparameterized regime (p > N) is strictly less than the lowest possible gen-
eralization error in the underparameterized regime (p ≤ N). This, together with the analysis and
numerical evidence in previous studies such as those in Belkin et al. (2019; 2020), leads to the under-
standing that smoother approximations (i.e., solutions that are dominated by lower Fourier modes)
give better generalization in learning with the RFF model (1).
Main contributions of this work. In this work, we analyze a generalized version of (5) for gen-
eral feature regression from noisy data. Following the same notations as before, we introduce the
following weighted least-squares formulation for feature regression:
θbδp = Λ[-p]β wb, with wb = arg min kΛ[-Nα] Ψ[N×p]Λ[-pβ] w - yδk22,	(7)
w
where the superscript δ on y and θp denotes the fact that the training data contain random noise
of level δ (which will be specified later). The exponent α is pre-selected and can be different from
β. While sharing similar roles with the weight matrix Λ[-Pβ] , the weight matrix Λ[-Nα] provides us
the additional ability to deal with noise in the training data. Moreover, as we will see later, the
weight matrix Λ[-Nα] does not have to be either diagonal or in the same form as the matrix Λ[-pβ] ; the
2
Published as a conference paper at ICLR 2022
current form is to simplify the calculations for the RFF model. It can be chosen based on the a priori
information we have on the operator Ψ as well as the noise distribution of the training data.
The highlight and also one of the main contributions of our work is that we introduce a new weight
matrix Λ[-Nα] that emphasizes the data mismatch in terms of its various modes, in addition to Λ[-pβ] ,
the weight matrix imposed on the unknown feature coefficient vector θ. This type of generalization
has appeared in different forms in many computational approaches for solving inverse and learning
problems where the standard 2-norm (or `2 in the infinite-dimensional setting) is replaced with a
weighted norm that is either weaker or stronger than the unweighted 2-norm.
In this paper, we characterize the impact of the new weighted optimization framework (7) on the
generalization capability of various feature regression and kernel regression models. The new con-
tributions of this work are threefold. First, we discuss in detail the generalized weighted least-
squares framework (7) in Section 2 and summarize the main results for training with noise-free data
in Section 3 for the RFF model in both the overparameterized and the underparameterized regimes.
This is the setup considered in Xie et al. (2020), but our analysis is based on the proposed weighted
model (7) instead of (5) as in their work. Second, we provide the generalization error in both two
regimes for the case of training with noisy data; see Section 4. This setup was not considered in Xie
et al. (2020), but we demonstrate here that it is a significant advantage of the weighted optimization
when data contains noise since the weighting could effectively minimize the influence of the noise
and thus improve the stability of feature regression. Third, we extend the same type of results to
more general models in feature regression and kernel regression that are beyond the RFF model,
given that the operator Ψ satisfies certain properties. In the general setup presented in Section 5, we
derive error bounds in the asymptotic limit when P, N, and p all become very large. Our analy-
sis provides some guidelines on selecting weighting schemes through either the parameter domain
weighting or the data domain weighting, or both, to emphasize the features of the unknowns to be
learned based on a priori knowledge.
2	Generalized weighted least-squares formulation
There are four essential elements in the least-squares formulation of the learning problem: (i) the
parameter to be learned (θ), (ii) the dataset used in the training process (y), (iii) the feature matrix
(Ψ), and (iv) the metric chosen to measure the data mismatch between Ψθ and y.
Element (i) of the problem is determined not only by the data but also by a priori information
we have. The information encoded in (3) reveals that the size (i.e., the variance) of the Fourier
modes in the RFF model decays as fast as (1 + k)-2γ. Therefore, the low-frequency modes in (1)
dominate high-frequency modes, which implies that in the learning process, we should search for
the solution vectors that have more low-frequency components than the high-frequency components.
The motivation behind introducing the weight matrix Λ[-pβ] in (5) is exactly to force the optimization
algorithm to focus on admissible solutions that are consistent with the a priori knowledge given
in (3), which is to seek θ whose components ∣θk∣2 statistically decay like (1 + k)-2β.
When the problem is formally determined (i.e., p = N), the operator Ψ is invertible, and the training
data are noise-free, similar to the weight matrix Λ[-p]β, the weight matrix Λ[-Nα] does not change the
solution of the learning problem. However, as we will see later, these two weight matrices do impact
the solutions in various ways under the practical setups that we are interested in, for instance, when
the problem is over-parameterized or when the training data contain random noise.
The weight matrix Λ[-Nα] is introduced to handle elements (ii)-(iv) of the learning problem. First,
since Λ[-Nα] is directly applied to the data yδ, it allows us to suppress (when α > 0) or promote
(when α < 0) high-frequency components in the data during the training process. In particular,
when transformed back to the physical space, the weight matrix Λ[-Nα] with α > 0 corresponds
to a smoothing convolutional operator whose kernel has Fourier coefficients decaying at the rate
k-α. This operator suppresses high-frequency information in the data. Second, Λ[-Nα] is also directly
applied to Ψθ. This allows us to precondition the learning problem by making Λ[-Nα] Ψ a better-
conditioned operator (in an appropriate sense) than Ψ, for some applications where the feature matrix
Ψ has certain undesired properties. Finally, since Λ[-Nα] is applied to the residual Ψθ - y, we can
3
Published as a conference paper at ICLR 2022
regard the new weighted optimization formulation (7) as the generalization of the classic least-
squares formulation with a new loss function (a weighted norm) measuring the data mismatch.
Weighting optimization schemes such as (7) have been studied, implicitly or explicitly, in different
settings (Needell et al., 2014; Byrd & Lipton, 2019; Engquist et al., 2020; Li, 2021; Yang et al.,
2021). For instance, if we take β = 0, then we have a case where we rescale the classical least-
squares loss function with the weight Λ[-Nα] . If we take α = 1, then this least-squares functional
is equivalent to the loss function based on the H-1 norm, instead of the usual L2 norm, of the
mismatch between the target function fθ (x) and the learned model fθb(x). Based on the asymptotic
equivalence between the quadratic Wasserstein metric and the H-1 semi-norm (on an appropriate
functional space), this training problem is asymptotically equivalent to the same training problem
based on a quadratic Wasserstein loss function; see for instance Engquist et al. (2020) for more
detailed illustration on the connection. In the classical statistical inversion setting, Λ2α plays the role
of the covariance matrix of the additive Gaussian random noise in the data (Kaipio & Somersalo,
2005). When the noise is sampled from mean-zero Gaussian distribution with covariance matrix
Λ2α, a standard maximum likelihood estimator (MLE) is often constructed as the minimizer of
(Ψθ - y)*Λ-2]α(Ψθ - y) = kΛ--(Ψθ - y)k2.
The exact solution to (7), with X+ denoting the Moore-Penrose inverse of operator X, is given by
θbδp = Λ[-p]βΛ[-Nα]Ψ[N×p]Λ[-p]β+Λ[-Nα]yδ .	(8)
In the rest of this paper, we analyze this training result and highlight the impact of the weight
matrices Λ[-Nα] and Λ[-Nβ] in different regimes of the learning problem. We reproduce the classical
bias-variance trade-off analysis in the weighted optimization framework. For that purpose, we utilize
δ
the linearity of the problem to decompose θp as
θbδp = Λ[-p]β(Λ[-Nα]Ψ[N×p]Λ[-pβ] )+Λ[-Nα]y+Λ[-pβ] (Λ[-Nα]Ψ[N×p]Λ[-pβ] )+Λ[-Nα](yδ - y),	(9)
where the first part is simply θp , the result of learning with noise-free data, while the second part is
the contribution from the additive noise. We define the generalization error in this case as
Eαδ,β(P,p,N)=Eθ,δ hkfθ(x) - fθbδ (x)k2L2([0,2π])i = Eθ,δ hkθbδp - θbp + θbp - θk22i ,	(10)
where the expectation is taken over the joint distribution of θ and the random noise δ . By the stan-
dard triangle inequality, this generalization error is bounded by sum of the generalization error from
training with noise-free data and the error caused by the noise. We will use this simple observation to
bound the generalization errors when no exact formulas can be derived. We also look at the variance
of the generalization error with respect to the random noise, which is
δ	δδ
Varδ(Eθ[kb - θk2]) ：= Eδ[(Eθ[kb - θk2] - Eθ,δ[kb - θk2])2] ∙	(11)
In the rest of the work, we consider two parameter regimes of learning:
(i)	In the overparameterized regime, we have the following setup of the parameters:
N <p ≤ P, and, P = μN, P = VN for some μ,ν ∈ N s.t. μ ≥ V>1.	(12)
(ii)	In the underparameterized regime, we have the following scaling relations:
p ≤ N ≤ P, and, P = μN for some μ ∈ N.	(13)
The formally-determined case of p = N ≤ P is included in both the overparameterized and the
underparameterized regimes. We make the following assumptions throughout the work:
(A-I) The random noise δ in the training data is additive in the sense that yδ = y + δ.
(A-II) The random vectors δ and θ are independent.
(A-III) The random noise δ 〜N(0, σlp]) for some constant σ > 0.
While assumptions (A-I) and (A-II) are essential, assumption (A-III) is only needed to simplify the
calculations. Most of the results we obtain in this paper can be reproduced straightforwardly for the
random noise δ with any well-defined covariance matrix.
4
Published as a conference paper at ICLR 2022
3 Generalization error for training with noise-free data
We start with the problem of training with noise-free data. In this case, we utilize tools developed
in Belkin et al. (2020) and Xie et al. (2020) to derive exact generalization errors. Our main objective
is to compare the difference and similarity of the roles of the weight matrices Λ[-Nα] and Λ[-pβ] .
We have the following results on the generalization error. The proof is in Appendix A.1 and A.2.
Theorem 3.1 (Training with noise-free data). Let δ = 0, and θ be sampled with the properties
in (3). Then the generalization error in the overparameterized regime (12) is:
N-1 PV-1 t-2β-2γ	N-1
Eα,β (P,p, N ) = 1 - 2cγ X 1P"-N	+ cγ X
k=0 η=0 tk+N η	k=0
(PVT t-4β- )(Pμ-1 t-2γ- )
( η=0 tk+N η)( η=0 tk+Nη)
(PVT t-2β. )2
( η=0 tk+Nη)
(14)
and the generalization error in the underparameterized regime (13) is:
P — 1	p— 1 μ-1	N — 1 μ-1	N—p— 1 ~(N XN)
Eα,β (P,P,N )= Cγ X t-2Y + Cγ XX t—+Nη-Cγ XX Cni +N X e∑⅛ ,
j =N	k=0 η=1	k=p η=1	i,j =0 ii j j
(15)
where {tj }jP=-01 and cγ are those introduced in (3) and (4), while
碟) = PNO tkαUkUkj,	bN) = PN-PT(CYt-；Y，+ Xp+k0)Vik0Vjk0, 0 ≤ i,j ≤ N-P-1,
with UΣV* being the singular value decomposition of AaV]Ψ[n]\刖 and {χm}m-^1 defined as
Xm = PN01 ( Pμ-1 t-+N∕(N PN01 ω(Tk"), 0 ≤ m ≤ N - 1.
We want to emphasize that the generalization errors we obtained in Theorem 3.1 are for the weighted
optimization formulation (7) where We have an additional weight matrix Λ~N compared to the for-
mulation in Xie et al. (2020), even though our results look similar to the previous results of Belkin
et al. (2020) and Xie et al. (2020). Moreover, we kept the weight matrix A- in the Underparame-
terized regime, which is different from the setup in Xie et al. (2020) where the same weight matrix
was removed in this regime. The reason for keeping A；] in the underparameterized regime will
become more obvious in the case of training with noisy data, as we will see in the next section.
Here are some key observations from the above results, which, we emphasize again, are obtained
in the setting where the optimization problems are solved exactly, and the data involved contain no
random noise. The conclusion will differ when data contain random noise or when optimization
problems cannot be solved exactly.
First, the the weight matrix A；： only matters in the overparameterized regime while A^ only
matters in the underparameterized regime. In the overparameterized regime, the weight A-p： forces
the inversion procedure to focus on solutions that are biased toward the low-frequency modes. In
the underparameterized regime, the matrix A^ re-weights the frequency content of the “residual”
(data mismatch) before it is backprojected into the learned parameter θ. Using the scaling P = μN
and p = νN in the overparameterized regime, and the definition of cγ , we can verify that when
α = β = 0, the generalization error reduces to
E00,0(P, p, N)
1-
N + 2NCγ PP=P t-2Y
p p	j =p j
1, N 2N	Pp-I	2γ
1 + 万-丁dj=0 tj
(16)
This is given in Xie et al. (2020, Theorem 1).
Second, when the learning problem is formally determined, i.e., when p = N, neither weight matri-
ces play a role when the training data are generated from the true model with no additional random
noise and the minimizer can be found exactly. The generalization error simplifies to
Ea,β(P,P,N) = 2cγ PP-I t-2γ.
(17)
5
Published as a conference paper at ICLR 2022
Singular value index k
(a) singular values of Λ-α] Ψ [N] \ [p]	(b) double-descent curves With different a
Figure 1: Left: singular values of A-NaΨ[N]\[p] for a system with N = 1024 andP = 512. Shown are
singular values for α = 0, α = 0.4, α = 0.6, α = 0.8, and α = 1.0; Right: double-descent curves
for the generalization error Eα0,β for the cases ofN = 64, γ = 0.3, β = 0.3, and α = 0, 0.3, 0.8.
This is not surprising as, in this case, Ψ is invertible (because it is a unitary matrix scaled by the
constant N). The true solution to Ψθ = y is simply θ = Ψ-1y. The weight matrices in the
optimization problem are invertible and therefore do not change the true solution of the problem.
The generalization error, in this case, is therefore only due to the Fourier modes that are not learned
from the training data, i.e., modes p to P - 1.
While it is obvious from the formulas for the generalization error that the weight matrix Λ[-Nα] indeed
plays a role in the underparameterized regime, we show in Figure 1a the numerical calculation of
the singular value decomposition of the matrix AfN] Ψ[N]∖[p] for the case of (Ν,p) = (1024, 512).
The impact of ɑ can be seen by comparing the singular values to their correspondence in the a = 0
case (where all the singular values are the same and equal to √N). When the system size is large
(in this case N = 1024), even a small α (e.g., α = 0.4) can significantly impact the result. In
Figure 1b, we plot the theoretical prediction of Eα0,β in Theorem 3.1 to demonstrate the double-
descent phenomenon observed in the literature on the RFF model. We emphasize again that in this
particular noise-free setup with perfectly solved minimization problem by the pseudoinverse, Λ[-Nα]
only plays a role in the underparameterized regime as can be seen from the double-descent curves.
Selecting p to minimize generalization error. It is clear (and also expected) from (16) and (17)
that, in the cases of p = N or α = β = γ = 0, the generalization error decreases monotonically
with respect to the number of modes learned in the training process. One should then learn as many
Fourier coefficients as possible. When p 6= N or α, β 6= 0, Eα0,β(P,p, N), for fixed P and N, does
not change monotonically with respect top anymore. In such a situation, we can choose thep values
that minimize the generalization error and perform learning with these p values.
4	Error bounds for training with noisy data
In this section, we study the more realistic setting of training with noisy data. The common practice
is that when we solve the minimization problem in such a case, we should avoid overfitting the model
to the data by stopping the optimization algorithm early at an appropriate level of values depending
on the noise level in the data, but see Bartlett et al. (2020); Li et al. (2021b) for some analysis in the
direction of “benign overfitting”. When training with noisy data, the impact of the weight matrices
Λ[-Nα] and Λ[-pβ] on the generalization error becomes more obvious. In fact, both weight matrices play
non-negligible roles in the overparameterized and the underparameterized regimes, respectively.
We start with the circumstance where we still match the data perfectly for each realization of the
noise in the data. The result is summarized as follows.
6
Published as a conference paper at ICLR 2022
Lemma 4.1 (Training with noisy data: exact error). Under the assumptions (A-I)-(A-III), the gen-
eralization error Eαδ,β (P, p, N) is given as
Eαδ,β(P,p,N) = Eα0,β(P, p, N) + Enoise(P, p, N),
where Eα0,β (P, p, N) is the generalization error from training with noise-free data given in Theo-
rem 3.1 and Enoise(P, p, N) is the error due to noise. The error due to noise and the variance of the
generalization error with respect to noise are respectively
N-1 Pν -1 t-4β
Enoise(P,p,N) = σ2 X ，η=0 k+Nn 2 , Varδ (⅛θ[kθδ-θk2]
k=0 hPν -1 t-2β i2
η=0 k+Nη
in the overparameterized regime (12), and
2σ4 X [PV-0 t-+Nηi2
N2 k=0 hPν-ιt-2β i4.
k=0	η=0 tk+Nη
(18)
Enoise (P, p, N)
2 2p- N
σ2( —+
Varδ Eθ[kθδ - θk22
2σ4( 2P-2N +
Σ
j=0
N -p-1
X
i,j=0
N -p-1
, p>N/2,
(N) (N)
ij eji
ςPr
(19)
in the underparameterized regime (13).
The proof of this lemma is documented in Appendix A.3. Note that due to the assumption that
the noise δ and the coefficient θ are independent, the noise-averaged generalization errors are split
exactly into two separate parts: the part due to θ and the part due to δ. The coupling between them
disappears. This also leads to the fact that the variance of the generalization error with respect to
noise only depends on the noise distribution (instead of both the noise variance and the θ variance).
For the impact of noise on the generalization error, we see again that the impact of Λ[-Pβ] is only
seen in the overparameterized regime while that of Λ[-Nα] is only seen in the underparameterized
regime. This happens because we assume that we can solve the optimization problem exactly for
each realization of the noisy data. In the overparameterized regime, when β = 0, we have that
Enoise = Nσ2/p as expected. In such a case, the variance reduces to Varδ(Eθ[∣∣θδ - θ∣∣2])=
2Nσ4∕p2. In the underparameterized regime, when α = 0, we have that Enoise = pσ2∕N. The
corresponding variance reduces to Varδ(Eθ[∣∣θδ - θ∣∣2]) = 2pσ4/N2.
In the limit when p → N, that is, in the formally determined regime, the mean generalization error
due to random noise (resp. the variance of the generalization error with respect to noise) converges
to the same value Enoise(P, p, N) = σ2 (resp. Varδ(Eθ[kθδ - θk22]) = 2σ4 /N) from both the
overparameterized and the underparameterized regimes. This is the classical result in statistical
learning theory (Kaipio & Somersalo, 2005).
The explicit error characterization above is based on the assumption that we solve the optimization
exactly by minimizing the mismatch to 0 in the learning process. In practical applications, it is often
the case that we stop the minimization process when the value of the loss function reaches a level
that is comparable to the size of the noise in the data (normalized by the size of the target function,
for instance). We now present the generalization error bounds for such a case.
Theorem 4.2 (Training with noisy data: error bounds). In the same setup as Lemma 4.1, for fixed
N, we have the following general bound on the generalization error when p is sufficiently large:
Eαδ,β(P, p, N). p-2αbEδ[kδk22,Λ-α] + p-2βEθ[kθk22,Λ-β],
where αb := α + 1/2 (resp αb := α) in the overparameterized (resp. underparameterized) regime.
When αb ≥ 0, the error decreases monotonically with respect to p. When αb < 0, the bound is
minimized by selecting
P 〜(Eδ [kδk2,Λ-α ]-1Eθ [1网12八肃])2(β1-α),	(20)
in which case we have
Eδ,β(P,P,N) . Eθ[kθk2,A-β]2(β-a)Eδ[kδk2,Λ-α]2(β-α).	(21)
7
Published as a conference paper at ICLR 2022
Note that the calculations in (20) and (21) are only to the leading order. We neglected the contribu-
tion from the term involving γ . Also, the quantities in expectations can be simplified. We avoided
doing so to make these quantities easily recognizable as they are the main quantities of interests.
In the underparameterized regime, that is, the regime of reconstruction, classical statistical inversion
theory shows that itis statistically beneficial to introduce the weight matrix Λ[-Nα] that is related to the
covariance matrix of the noise in the data (Kaipio & Somersalo, 2005) (see also Bal & Ren (2009)
for an adaptive way of adjusting the weight matrix for some specific applications). Our result here
is consistent with the classical result as we could see that if we take Λ[-Nα] to be the inverse of the
covariance matrix for the noise distribution, the size of the noise contribution in the generalization
error is minimized.
Theorem 4.2 shows that, in the case of training with noisy data, the α parameter can be tuned to
reduce the generalization error of the learning process just as the β parameter in the weight matrix
Λ[-Pβ] . Moreover, this result can serve as a guidance on the selection of the number of features to
be pursued in the training process to minimize the generalization error, depending on the level of
random noise in the training data as well as the regime of the problem.
5	Extension to general feature regression
The results in the previous sections, even though are obtained for the specific form of Fourier feature
regression, also hold in more general settings. Let fθ be a general random feature model of the form
fθ(X) = Pi=Oekgk(X), x ∈ χ,
(22)
constructed from a family of orthonormal features {夕k}p=0 in L2 (X). Let Ψ be the feature matrix
constructed from the dataset {Xj, yj}jN=-01:
(Ψ)jk = ψk(Xj), 0 ≤ j, k ≤ P - 1 .
We can then apply the same weighted optimization framework (7) to this general model. As in the
case of RFF model, the data in the learning process allows decomposition
yδ = ψ[N]×[p]θ[p] + ψ[N]×([P]∖[p])θ[P]\[p] + δ .
The learning process only aims to learn the first p modes, i.e., θ[p], indicating that the effective noise
that We backpropagate into。团 in the learning process is Ψ[n]×([p]∖[p])θ[p]∖[p] + δ. The frequency
contents of this effective noise can come from the true noise δ, the part of Fourier modes that we are
not learning (i.e., θ[p]\刖)，or even the feature matrix Ψ[n] ×([p]∖[p]). For the RFF model, the feature
matrix Ψ[n]×[n] is unitary after being normalized by 1∕√N. Therefore, all its singular values are
homogeneously √N. For learning with many other features in practical applications, we could
have feature matrices Ψ[N]×[N] With fast-decaying singular values. This, on one hand, means that
Ψ[n]×([p]∖[p])θ[p]\[p] will decay even faster than θ[p]\[p], making its impact on the learning process
smaller. On the other hand, Ψ[N]×[N] having fast-decaying singular values will make learning θ[p]
harder (since it is less stable).
The impact of the weighting scheme on the generalization error as an expectation over θ and δ
(defined in Lemma 4.1) is summarized in Theorem 5.1. Its proof is in Appendix A.4. Note that here
we do not assume any specific structure on the distributions of θ and δ except there independence.
Theorem 5.1. Let Ψ = UΣV* be the singular value decomposition of Ψ. Under the assumptions
(A-I)-(A-III), assume further that ∑kk 〜t=ζ for some Z > 0. Then the generalization error of
training, using the weighted optimization scheme (7) with Λ=0] replaced with A-^U* and Λ-β
replaced with VΛ[-pβ] , satisfies
Eαδ,β(P,p,N).p2(ζ-α)Eδ[kδk22,Λ-α]+p-2βEθ[kθk22,Λ-β],
in the asymptotic limit P 〜 N 〜 P → ∞. The bound is minimized, as afunction of P, when
P 〜(Eδ[kδk2,Λ-α]-1Eθ[kθk2,Λ-β]2) 2(ζ+β—α)
(23)
8
Published as a conference paper at ICLR 2022
in which case we have that
Eδ,β(P,P,N) . Eθ[kθk2,A-β](ζ+-αα)Eδ[kδk2,Λ-α](ζ+β—α).	(24)
On the philosophical level, the result says that when the learning model is smoothing, that is, when
the singular values of Ψ decays fast, we can select the appropriate weight matrix to compensate the
smoothing effect so that the generalization error is optimized. The assumption that we have access
to the exact form of the singular value decomposition of the feature matrix is only made to trivialize
the calculations, and is by no means essential. When the optimization algorithm is stopped before
perfect matching can be achieve, the fitting from the weighted optimization scheme generated a
smoother approximation (with a smaller p, according to (23), than what we would obtain with a
regular least-squares minimization).
Feature matrices with fast-decaying singular values are ubiquitous in applications. In Appendix B,
we provide some discussions on the applicability of this result in understanding general kernel learn-
ing (Amari & Wu, 1999; Kamnitsas et al., 2018; Jean et al., 2018; Owhadi & Yoo, 2019; Bordelon
et al., 2020; Canatar et al., 2021) and learning with simplified neural networks.
6	Concluding remarks
In this work, we analyzed the impact of weighted optimization on the generalization capability of
feature regression with noisy data for the RFF model and generalized the result to the case of fea-
ture regression and kernel regression. For the RFF model, we show that the proposed weighting
scheme (7) allows us to minimize the impact of noise in the training data while emphasizing the cor-
responding features according to the a priori knowledge we have on the distribution of the features.
In general, emphasizing low-frequency features (i.e., searching for smoother functions) provide bet-
ter generalization ability.
While what we analyze in this paper is mainly motivated by the machine learning literature, the
problem of fitting models such as the RFF model (1) to the observed data is a standard inverse
problem that has been extensively studied (Engl et al., 1996; Tarantola, 2005). The main focus of
classical inversion theory is on the case when Ψ[N×p] is at least rank p so that there is a unique least-
squares solution to the equation Ψθ = y for any given dataset y. This corresponds to the learning
problem we described above in the underparameterized regime.
In general, weighting the least-squares allows us to have an optimization algorithm that prioritize the
modes that we are interested in during the iteration process. This can be seen as a preconditioning
strategy from the computational optimization perspective.
It would be of great interest to derive a rigorous theory for the weighted optimization (7) for general
non-convex problems (note that while the RFF model itself is nonlinear from input x to output
fθ(x), the regression problem is linear). Take a general nonlinear model of the form
F (χ; θ) = y,
where F is nonlinear with respect to θ. For instance, F could be a deep neural network with θ
representing the parameters of the network (for instance, the weight matrices and bias vectors at
different layers). We formulate the learning problem with a weighted least-squares as
b = Λ-βW, With W = argmin ∣∣Λ-α](F(x; Λ(pβw) - y)k2,
w
Where the Weight matrices are used to control the smoothness of the gradient of the optimization
procedure. The linearized problem for the learning of W gives a problem of the form Ψθ = ye With
Ψ = (ΛSn]F(x; Λ{p]W))*(ΛQ*(F0)*(x; Λβ3]W)(Λ{⅛*, e = (λQ*(F0)*(x; Ae)严)^^)*丫.
For many practical applications in computational learning and inversion, F0 (With respect to θ)
has the properties We need for Theorem 5.1 to hold. Therefore, a local theory could be obtained.
The question is, can We shoW that the accumulation of the results through an iterative optimization
procedure (e.g., a stochastic gradient descent algorithm) does not destroy the local theory so that
the conclusions We have in this Work Would hold globally? We believe a thorough analysis along
the lines of the recent Work of Ma & Ying (2021) is possible With reasonable assumptions on the
convergence properties of the iterative process.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work is partially supported by the National Science Foundation through grants DMS-1620396,
DMS-1620473, DMS-1913129, and DMS-1913309. Y. Yang acknowledges supports from Dr. Max
Rossler, the Walter Haefner Foundation and the ETH Zurich Foundation. This work was done in
part while Y. Yang was visiting the Simons Institute for the Theory of Computing in Fall 2021. The
authors would like to thank Yuege Xie for comments that helped us correct a mistake in an earlier
version of the paper.
Supplementary Material
Supplementary material for the paper “A Generalized Weighted Optimization Method for Compu-
tational Learning and Inversion” is organized as follows.
A	Proof of main results
We provide here the proofs for all the results that we summarized in the main part of the paper. To
simplify the presentation, we introduce the following notations. For the feature matrix Ψ ∈ CN×P,
we denote by
ΨT ∈ CN×p and ΨTc ∈ CN×(P-p)
the submatrices of Ψ corresponding to the first p columns and the last P - p columns respectively.
In the underparameterized regime (13), we will also use the following submatrices
Ψ[n] ∈ CNXN and Ψ[n]\t ∈ CN×(N-P),
corresponding to the first N columns of Ψ and the last N - p columns of Ψ[N] , respectively. We
Will Use Ψ* to denote the Hermitian transpose of Ψ. For a P X P diagonal matrix Λ, Λt (≡ Λ[p])
and ΛTc denote respectively the diagonal matrices of sizes p × p and (P -p) × (P -p) that contain
the first p and the last P - p diagonal elements of Λ. Matrix Λ[N] is of size N × N and used to
denote the first N diagonal elements of Λ. For any column vector θ ∈ CP×1, θT and θTc denote
respectively the column vectors that contain the first p elements and the last P - p elements of θ.
To study the impact of the weight matrices Λ[-Nα] and Λ[-pβ] , we introduce the re-scaled version of the
feature matrix Ψ, denoted by Φ, as
Φ := Λ[-Nα] ΨΛ[-Pβ] .	(25)
In terms of the notations above, we have that
ΦT := Λ[-Nα]ΨTΛT-β , and ΦTc := Λ[-Nα]ΨTcΛT-cβ .	(26)
For a given column vector θ ∈ Cd and a real-valued square-matrix X ∈ Rd×d, we denote by
kθk2,X := kXθk2 = √(Xθ)*Xθ
the X-weighted 2-norm of θ . We will not differentiate between finite- and infinite-dimensional
vectors. In the infinite-dimensional case, we simply understand the 2-norm as the usual '2-norm.
We first derive a general form for the generalization error for training with θ sampled from a distri-
bution with given diagonal covariance matrix. It is the starting point for most of the calculations in
this paper. The calculation procedure is similar to that of Lemma 1 of Xie et al. (2020). However,
our result is for the case with the additional weight matrix Λ[-Nα] .
A.1 Proofs of Lemma A.1-Lemma A.3
Lemma A.1 (General form of Eα0,β (P, p, N)). Let K ∈ CP×P be a diagonal matrix, and θ be
drawn from a distribution such that
Eθ[θ] = O,	Eθ[θθ*] = K .	(27)
10
Published as a conference paper at ICLR 2022
Then the generalization error for training with weighted optimization (7) can be written as
E0,β (P,P,N )=tr(K) + Pα,β + Qα,β,
where
Pα,β = tr(φ+ΦτΛ-2βΦ+ ΦtΛTKtΛT) - 2tr(KτΦ+Φτ),
and
Qα,β = tr((φ+)*Λ-2βΦ+ΦτcA*。KTc A*。Φ^),
with ΦT and ΦTc given in (26).
(28)
Proof. We introduce the new variables
w = A[βP]θ, and z = A[-Nα] y.
We can then write the solution to the weighted least-square problem (7) as
wbT = ΦT+z, and wbTc = 0 ,
where Φ+ is the Moore-Penrose inverse of Φt. Using the fact that the data y contain no random
noise, we write
y = ΨT AT-β wT + ΨTc AT-cβ wTc, and z = A[-Nα]y = ΦTwT + ΦTc wTc.
Therefore, we have, following simple linear algebra, that
kθb - θk22 = kAT-β (wb T - wT)k22 + kAT-cβ (wb Tc - wTc)k22
= kAT-βΦT+(ΦTwT + ΦTc wTc) - AT-βwTk22 + kAT-cβ wTc k22
= kAT-βΦT+ΦTcwTc - AT-β(IT - ΦT+ΦT)wTk22 + kAT-cβwTck22.	(29)
Next, we make the following expansions:
kAT-β ΦT+ ΦTc wTc - AT-β (I - ΦT+ ΦT)wT k22 = kAT-βΦT+ ΦTc wTc k22 + kAT-β (I - ΦT+ΦT)wT k22 - T1,
kAT-β (I - ΦT+ΦT)wTk22 = kAT-βwTk22 + kAT-βΦT+ΦTwTk22 - T2
with T1 and T2 given respectively as
and
T1 :=2< AT-βΦT+ΦT
- ΦT+ΦT)wT ,
T2
2< (WT Λ-β Λ-β Φ+ΦtWt
We therefore conclude from these expansions and (29), using the linearity property of the expectation
over θ, that
Eθ[kθb-θk22] =Eθ[kAT-βWTk22]+Eθ[kAT-cβWTck22]
+ Eθ [kAT-β ΦT+ ΦT WT k22] + Eθ [kAT-β ΦT+ ΦTc WTc k22] - Eθ[T1] - Eθ[T2] . (30)
We first observe that the first two terms in the error are simply tr K ; that is,
Eθ[kΛ-βwτk2]+ Eθ[kΛ-cβWTck2] = tr(K) .	(31)
By the assumption (27), We also have that Eθ [θτc XθT] = 0 for any matrix X SUch that the product
is well-defined. Using the relation between θ and w, we conclude that
Eθ[T1] = 0 .	(32)
We then verify, using simple trace tricks, that
Eθ[T2] = 2Eθ [<(tr(wTΛ-βΛ-βΦ+ΦτWτ))] =2<(tr(KτΦ+Φτ)) =2trgΦ+Φτ) , (33)
11
Published as a conference paper at ICLR 2022
where we have also used the fact that KTΛT-β = ΛT-β KT since both matrices are diagonal, and the
last step comes from the fact that ΦT+ ΦT is Hermitian.
Next, we observe, using standard trace tricks and the fact that ΦT+ΦT is Hermitian, that
Eθ [kΛ-β Φ+Φτwτk2] = Eθ [tr(wTΦ+ΦτΛ-2β φ+φτwτ)]
=tr(Φ+ΦτΛ-2βΦ+ ΦtEθ [wτw 司)
=tr(Φ+ΦτΛ-2βΦ+ ΦtΛTKtΛT),
and
Eθ [kΛ-β Φ+ Φτc WTc k2]	= Eθ [tr(wTc Φ^ (Φ+)* Λ-2β Φ+Φτc WTc)]
=tr(ΦTc (Φ+) * Λ-2β Φ+ Φτc Eθ [wτc WKD
=tr(ΦTc (Φ+)* Λ-2β Φ+Φτc Λβc KTc ΛTTc)
=tr((Φ+)*Λ-2βΦ+ΦtcΛTTcKTcΛTTcΦTc).
The proof is complete when we put (31), (32), (33), (34) and (35) into (30).
(34)
(35)
□
Next we derive the general formula for the generalization error for training with noisy data.
Lemma A.2 (General form of Eαδ,β (P, p, N)). Let θ be given as in Lemma A.1. Then, under the
assumptions (A-I)-(A-II), the generalization error for training with weighted optimization (7) from
noisy data yδ can be written as
Eαδ,β(P,p,N) =Eα0,β(P,p,N)+Enoise(P,p,N)	(36)
where Eα0,β (P, p, N) is given as in (28) and Enoise(P, p, N) is given as
Enoise(P,p,N) = tr(Λ-N](Φ+ )*Λ-2βΦ+Λ-αEδ[δδ*]).
The variance of the generalization error with respect to the random noise is
Varδ (Eθ[kθδ - θ∣∣2]) = Eδ 也停飞而蜴)*Λ-2β虫+八谪66*人而呼+)*α-2产Φ+Λ-≈δ)]
-IEnoise(P,P,N ))2. (37)
Proof. We start with the following standard error decomposition
δδ	δ	δ
kθb	-θk22=	kθb	-bθ+θb-θk22=	kbθ-θk22+kθb	-θbk22+2<	(θb	- θb)*(θb	- θ) .
Using the fact that
bδ	b
θTc = θTc = 0 ,
the error can be simplified to
δδ
kθb	- θk22	=	kθb-θk22+kθbT-bθTk22+T3,	(38)
where
T3 = 2<(θbδT - θbT)*(θbT - θT) .
Taking expectation with respect to θ and then the noise δ, we have that
δδ
Eδ,θ[kθb - θk22] = Eθ[kθb - θk22] + Eδ[kθbT - θbTk22] + Eδ,θ[T3].	(39)
The first term on the right-hand side is simply Eα0,β(P,p, N) and does not depend on δ. To evaluate
the second term which does not depend on θ, we use the fact that
θbδT - θbT = ΛT-βΦT+(zδ - z) = ΛT-βΦT+Λ[-Nα](yδ - y) = ΛT-βΦT+Λ[-Nα]δ.
12
Published as a conference paper at ICLR 2022
This leads to
Eδ[kθbδT - θbTk22]	= Eδ[kΛT-βΦT+Λ[-Nα]δk22]
=Eδ [tr(δ*Λ-Nα(Φ+)*Λ-2β φ+Λ--δ)]
=tr(Λ 而(Φ+)*Λ-2β Φ+Λ 党 Eδ [δδ*]).
The last step is to realize that
Eδ,θ[T3] = 2<(Eδ,θ [(Λ-βΦ+Λ-Nαδ)*(θτ - θτ)]) = 0
due to the assumption that δ and θ are independent.
Utilizing the formulas in (38) and (39), and the fact that Eθ[T3] = 0, we can write down the variance
of the generalization error with respect to the random noise as
VarδEθ[kbθδ	- θk22]	=	Eδ[kθbδT	- θbTk24]	-	Eδ[kθbδT	- θbTk22]2
=Eδ[tr(δ*Λ而(Φ+)*Λ-2βΦ+Λ-Npδ*Λ而(Φ+)*Λ-2βΦ+Λ-N]δ)] -(Enoise(P,p, N))2.
The proof is complete.	□
We also need the following properties on the the feature matrix Ψ of the random Fourier model.
Lemma A.3. (i) For any ζ ≥ 0, we define, in the overparameterized regime (12), the matrices
Π1 ∈ CN ×N and Π2 ∈ CN ×N respectively as
∏ι := ΨtΛ-ζΨT and Π2 = Ψ4Λ-CζΨTc .
Then ∏ι and Π2 admit the decomposition ∏ι = Ψ[n]Λ∏i<Ψ｝、and Π2 = Ψ[n]Λ∏2<W；N] re-
spectively with ΛΠ1,ζ and ΛΠ2,ζ diagonal matrices whose m-th diagonal elements are respectively
N-1 ν-1
λ(Πm1,)ζ = XXtk-+ζNηe(mN,k), 0 ≤ m ≤ N- 1,
k=0 η=0
and
N-1 μ-1
λ(Πm2,)ζ = XXtk-+ζNηe(mN,k), 0 ≤ m ≤ N- 1,
k=0 η=ν
where egN) is defined as, denoting ωN := e2Ni,
N-1
(N) ._ 1 X (m-k)j
em,k := N	ωN
j=0
(ii) For any α ≥ 0, we define, in the underparameterized regime (13) with p < N, the matrix
Ξ ∈ Cp×(N-p) as
Ξ ：= (ΨTΛ-NɑΨτ)-1ΨTA肃Ψ[n]\t .
Then Ξ;Ξ has the following representation:
Ξ*Ξ = N(ΨfN]∖τΛ2N]Ψ[N]∖τ)-*Ψ[N]∖τΛ4αΨ[N]∖τ(Ψ[N]∖τΛ2N]Ψ[N]∖τ)-1 - I[N-p].
1, k = m
0, k 6= m
0 ≤ m, k ≤ N -1.
Proof. Part (i) is simply Lemma 2 of Xie et al. (2020). It was proved by verifying that Π1 and Π2
are both circulant matrices whose eigendecompositions are standard results in linear algebra.
To prove part (ii), let us introduce the matrices P, Q, R through the relation
ψ* λ-2.ψ	=( ψTA[N]αψτ	ψTA[N]αψ[N]∖τ ! ≡ (P	RA
[N] [N]	N] = "；N]\TANaψT ψiN]∖τA[N]αψ[N]∖J ≡(r* Q卜
13
Published as a conference paper at ICLR 2022
Then by standard linear algebra, we have that
(ψ* A-2αψ 「1 (PT + PTR(Q - R*P-1 R)-1R*P-1 -PTR(Q - R*P-1R)T∖
(ψ[N]λ[N] ψ[N]J =[	-(Q — R*P-1R)-1R*P-1	(Q - R*P-1R)-1 J
since P = ΨTΛ-N]αΨT is invertible.
Meanwhile, since √= Ψ[N] is unitary, We have that
(ψ[N ]A[NWn ])	= N ψN ]A2N]ψ[N ]
=ɪ ( ΨTΛ2向Ψt	ΨTΛ2N]Ψ[n]\t ! ≡ (P	R∖
=N2 ∖ψ[n]∖tλ2N]ψt ψ[n]∖tλ2N]ψ[n]∖" ≡ IR*	QJ .
Comparing the two inverse matrices lead us to the following identity
P-1 - P-1RRe[ = Pe .
This gives us that
P-1R = PeR(I[N-p] - Re[ R)-1 .
Therefore, we have
R[ P-[ P-1R = (I[N-p] - Re[R)-[R[Pe[ PeR(I[N-p] - Re[R)-1 .
Utilizing the fact that
R [R = N ψ*n ]\tA2N]WtWTA[N『W[n ]\t
= N ψ*n ]∖τA2N] (N I[N ] - ψ[n ]∖τψ[N ]\t)A[N『W[n ]\T = I[N-p] - Q Q,
we can now have the following identity
R[P-[ P-1R = (QeQ)-[R[Pe[ Pe R(Qe Q)-1 .
Using the formulas for P and R, as well as ΨTΨ[T = NI[N] - Ψ[N]∖TΨ[[N]∖T, we have
R*P *P R = N Ψ*N]∖τΛ-2]αΨτΨTΛ2N] ΨtΨTΛ2N] ΨτΨTΛ-N]αΨ[N]∖τ
1
=N3qψ[n]∖tλ[n]ψ[n]∖tq - QQQq .
This finally gives us
1
R*P-*P-1R = NQ-*Ψ*n]∖tΛ4N]Ψ[n]∖tQ-1 - I[n* .
The proof is complete when we insert the definion of Q into the result.	□
A.2 Proof of Theorem 3.1
We provide the proof of Theorem 3.1 here. We split the proof into the overparameterized and the
underparameterized regimes.
Proof of Theorem 3.1 (Overparameterized Regime). In the overparameterized regime (12), we have
that the Moore-Penrose inverse Φ+ = ΦT(ΦτΦT)-1. Using the definitions of Φt in (26), we can
verify that
ΦT+ΦTΛT-β = ΛT-βΨ[T(ΨTΛT-2βΨ[T)-1ΨTΛT-2β ,
ΛT-βΦT+ΦT = ΛT-2βΨ[T(ΨTΛT-2βΨ[T)-1ΨTΛT-β ,
and
ΦT+ΦTΛT-2βΦT+ΦT = ΛT-βΨ[T(ΨTΛT-2βΨ[T)-1ΨTΛT-4βΨ[T(ΨTΛT-2βΨ[T)-1ΨTΛT-β .
14
Published as a conference paper at ICLR 2022
We therefore have
tr(φ+ΦτΛ-2β Φ+ΦtΛT Kt ΛT)
=tr((ΨτΛ-2βΨT)-1 ΨτΛ-4βΨT(ΨτΛ-2βΦT)-1ΦtKtΦT).
Meanwhile, a trace trick leads to
tr(κT φ+φT ) = tr(κτφT(φτφT)-1φτ) = tr((φTφT )-1φτKT φT)
=tr((Λ^ΨτΛ-2β ΨTΛ 而)-1Λ-αΨτΛ-β KτΛ-β ΨTΛ 而)
=tr((ΨτΛ-2βΨT)-1ΨτΛ-βKτΛ-βΨT) ∙	(40)
Therefore, based on Lemma A.1, We have finally that
Pa,β = tr((ΨτΛ-2β ΨT)-1ΨτΛ-4β ΨT(ΨτΛ-2β ΦT)-1φτKτΨT)
-2tr((ΨτΛ-2βΨT)-1ΨτΛ-βKτΛ-βΨT) . (41)
In a similar manner, We can check that
Qa,β = tr((φ+)*Λ-2β Φ+Φτc ΛTc Kt。A*。©T。)
=tr((ΦτΦT 厂*ΦτΛ-2β ΦT(ΦτΦT)-1Φτc ΛTc Kt。Λ3 ©T。)
=tr((ΨτΛ-2βΨT)-*ΨτΛ-4βΨT(ΨτΛ-2β好尸田『Kt。好。).	(42)
The above calculations give us
E0,β(P,P,N)=tr(K) - 2tr((ΨτΛ-2βΨT)-1ΨtΛ-βKτΛ-βΨT)
+ tr((ΨτΛ-2βΨT)-*ΨτΛ-4βΨT(ΨτΛ-2βΨT)-1ΨKΨ*) . (43)
We are now ready to evaluate the terms in the generalization error. First, we have that since K
CY Λ-p2]f, we have, using the definition of CY, that
P-1
tr(κ) = CY X t-2γ = 1.
j=0
(44)
Second, using the results in part (i) of Lemma A.3 and the fact that √= Ψ[N] is unitary, we have
tr((ΨτΛ-2β ΨT )-1ΨτΛ-β KτΛ-β ΨT)
CY tr( N ψ[N ]A∏1,2β ψ[N ]ψ[N ]λ∏ι,2β+2γ ψ[N ]
cY tr( N ψ[N ]A∏1,2β A∏ι,2β+2γ ψ[n ]) = cY tr(A∏1,2β A∏ι,2β+2γ )
N-1 PN-1 ( PV-I 12β-2Y、(N)
z^k=0 l 乙η=0 tk+Nη Jem,k
CCrC PNT(PVT t-2β )e(N)
m=0 乙k=0 I 乙η=0 tk+Nη) em,k
N-1
CY X
k=0
Γν-1 , —2β-2y
η=0 tk+Nη
ΓV-1 . —2β
η=0 tk+Nη
(45)
The results in Lemma A.3 also give that
tr((ΨτΛ-2β ΨT)-*Ψt Λ-4β ΨT(Ψt Λ-2β ΨT )-1ΨKΨ*)
ψ[N] (λ∏i,2y 十 λ∏2,2y)ψ[n])
tr( N2 ψ[N]A∏1,2βψ[N]ψ[N]A∏ι,4βψ[N] Nψ[N]A∏1,2βψ[N]
=tr(A∏1,2β Λ∏1,4β A∏1,2β (Ani,2y + λ∏2,2y)).
We can plug in the formula of A.3 to get
tr((ΨτΛ-2β ΨT)-[ΨtΛ-4β ΨT(ΨtΛ-2β ΨT)-1ΨKΨ*)
N-1 h PN-1 ( PVT t-4β )JN) ih PN-1 ( Pμ-1 t-2Y )JN)i
[Λ^k=0 1 Z^η=0 tk+NηJ em,k[[2^k=0 1 Z^η=0 tk+NηJ em,k[
m=0	[pN-1( PV=I t-+Nj45]2
N-1 (PVT t-4β ʌ( Pμ-1 t-2Y ʌ
η=0 tk+NηJη=0 tk+NηJ
k=0	hPV-1 t-2β I2	.
k=0	[2 η=otk+Nη]
(46)
15
Published as a conference paper at ICLR 2022
We can now put (44), (45) and (46) together into the general error formula (43) to get the result (14)
in Theorem 3.1. The proof is complete.	口
Proof of Theorem 3.1 (Underparameterized Regime). In the underparameterized regime (13), we
have that the Moore-Penrose inverse Φ+ = (ΦTΦt)-1ΦT. This leads to
Φ+Φt = IT and (Φ+)*Λ-2βΦ+ = Φτ(ΦTΦτ)-*Λ-2β(ΦTΦt)-1ΦT .
Therefore, using notations in Lemma A.1, we have that Pα,β is simplified to
Pα,β = -tr(Kτ),
while Qα,β is simplified to
Qα,β = tr(Φτ(ΦT Φτ)-*Λ-2β (ΦTΦτ)-1ΦTΛ-αΨτc KTc ΨTc A^)
=tr(Λ-N]αΨτ(ΨTΛ肃Ψt)-*(ΨTΛ肃]αΨτ)-1ΨTΛ肃ΨτcKTcΨTc) .	(47)
In this regime, P ≤ N ≤ P. We therefore have Tc = ([N]∖T) ∪ [N]c ([N]c := [P]\[N]). Using
the fact that K is diagonal, we obtain
Ψτc KTcΨTc = Ψ[N]∖tK[N]∖τΨ[N]\T + Ψ[N]c K[N]cΨ[N]c
Following the result of Lemma A.3, the second part of decomposition is simply the matrix Π2 with
ν = 1. To avoid confusion, we denote it by Π3 and use the result of Lemma A.3 to get
ψ[N ]c K[N ]c ψ[N ]c = ψ[N ]λ∏3,2y ψ[N ]
where the diagonal elements of ΛΠ3 ,2γ are
N-1 μ-1	N-1
λ∏m)2γ = X(Xt-+Nη)(N X 3”)), 0 ≤ m ≤ N - 1.	(48)
k=0 η=1	j=0
Next we perform the decomposition
Ψ[N]ΛΠ3,2γΨ[[N] = ΨTΛΠ3,2γTΨ[T + Ψ[N]∖TΛΠ3,2γ[N]∖TΨ[[N]∖T ,
and define the diagonal matrices
G Λ	C 八八 T≥ ɪʃ	1 Λ
K1 := ΛΠ3 ,2γT and K2 := K[N]∖T + ΛΠ3,2γ [N]∖T .
We can then have
ΨTc KTc Ψ[Tc
ΨTK1Ψ[T + Ψ[N]∖TK2Ψ[[N]∖T.
Plugging this into the expression for Qα,β, we have
Qα,β = tr(Λ 肃ΨT(ΨTΛ-N]αΨT )-*(ΨTΛ 肃 ΨT)-1ΨTΛNαΨTK 1ΨT)
+ tr(Λ肃ΨT(ΨTΛ-N]αΨT)-*(ΨTΛ肃ΨT)-1ΨTΛNαΨ[N]∖TK2Ψ[n]∖t) .	(49)
The first term simplifies to tr(K 1). The second term vanishes in the case of α = 0 and in the case
when the problem is formally determined; that is, when p = N, but is in general nonzero when
p < N. Using the result in part (ii) of Lemma A.3, we have that
tr(Λ-N]αΨT(ΨTΛINaΨt)-*(ΨTΛINaΨt)-1ΨTΛINaΨ[n]∖tK 2Ψ[n]∖t)
=tr(Ψ[N]∖TΛ1N]aΨT(ΨTΛINaΨt)-*(ΨTΛINa Ψt)-1ΨTΛINaΨ[n]∖tK 2)
=Ntr(X-*ψ[N]∖tλ4N]ψIn]∖TX-1Ib2) - tr(lb2), X := ψ[n]∖tλ2N]ψIn]∖t .
Using this result and the singular value decomposition
AaV]ψIN]∖T = UdiagUς00,…，ς(N-p-1)(N-p-1)])V[
16
Published as a conference paper at ICLR 2022
introduced in Theorem 3.1, we have
Qα,β
tr(Kb 1 - Kb2) + Ntr(∑TU*Λ2N]U∑TV*Kb2V)
N -p-1 N -p-1 ee(N ) eb(N )
tr(A∏3,2γT - KN]\T - A∏3,2γ[N]\T)+ NE E j j .
i=0 j=0 iijj
where
N-1	N -p-1
e(N) = X tkα U MJ,	SN)= X ICY t-+γo + λ∏ζ+kY))Vik，% 0 ≤ i,j ≤ N - P-1.
k=0	k0=0
The proof is complete when we insert the expressions of Pα,β and Qα,β back to the general for-
mula (28) and use the form of λ∏m2γ in (48).	□
When the problem is formally determined, i.e., in the case of N = p (equivalent to ν = 1),
ΨTΛ-N]αΨτ = ΨτΛ-N]αΨT. This allows Us to find that
Qα,β
tr( N2 ψ[N ]A∏1,2αψ[N ]ψ[N ]A∏ι,4ɑψN ]ψ[N 仆口12α中；N ] N ψ[N ]A∏2,2Y ψ[N ]
N-1
11(人口2,2仪人口1,4仪人口2,27) = ^X
k=0
∑ν-1 , -40、P ∙Pμ-1 , —2γ
η=0 tk+N η	η=ν tk+N η
ν-1
η=0
tk-+2αNη
P
2
which degenerates to its form (46) in the overparameterized regime with ν = 1. The result is then
independent of α since the terms that involve α cancel each other. Similarly, the simplification of
Qα,β in (46) with ν = 1 (N = p) also leads to the fact that β disappears from the formula.
A.3 Proofs of Lemma 4.1 and Theorem 4.2
Proof of Lemma 4.1. By Lemma A.2, the main task is to estimate the size of the generalization error
caused by the random noise in the two regimes. The error is, according to (36),
Enoise(P,P, N) = tr(Λ-α(Φ+)*Λ-2β①+人脸旧㊀[δδ*]).
In the overparameterized regime, we have that
Λ-α(Φ+)*Λ-2β Φ+Λ-αα =人而心牙虫脑-*虫小—2%蚤&1虫3-1人谪
=(ΨτΛ-2β ΨT)-*ΨτΛ-4β ΨT(Ψt Λ-2β ΨT)-1.
Therefore we have, using the results in part (i) of Lemma A.3,
Enoise(P, p, N)
σ2tr( N2 ψ[N ]A∏1,2β ψ[N ]ψ[N ] Anι,4β ψ[n ] N ψ[n ]A∏1,2β ψ[n ]
2	2 N-1 Pν-1 -4β
N tr(An：2e λπi ,4β An'e) = N X「J0 -+Nni 2 .
k=0	η=0 tk+N η
17
Published as a conference paper at ICLR 2022
To get the variance of the generalization error with respect to noise, We first compute
Eδ [tr(δ[Λ 印Φ+)*Λ-2βΦ+Λ 而 δδ*Λ 党(Φ+ )*Λ-2%+Λ*)]
σ4tr(Λ[N](φτ )*λt*①+人曲)tr(A[N](φ+)*λtφτ λ-v])
+2σ4tr(Λ 印Φ+)*Λ-2βΦ+Λ 而 Λ 印Φ+)[Λ-2%+Λ 而)
4
Ntr(ψ[N]A∏1,2βλ∏i ,4βA∏12βψ[N])tr(ψ[N]A∏[2β A∏ι,4βλ∏
+ N tr(ψ[N ]A∏1,2β A∏ι,4β A∏1,2β ψ[N ]ψ[N ]A∏1,2β A∏ι,4β AnL
σ4
N2 "(人口：* A∏ι,4β八口：*)tr(A∏i2βA∏ι,4βA∏[2β)
+ N tr(A∏i2β A∏ι,4β 八口：^ 八口：^ A∏ι,4β 八口：^)
'ι,2β ψ[N ]
2β ψ[N ]
d h NX1 PV=O t-+Nη ] 2 + 至 NX [Pη-0 t-+Nηi2
N 2I N [∑ν=0 ¾‰ J	N2 N [PM t-+Nj∙
This, together with the general form of the variance in (37), gives the result in (18).
In the underparameterized regime, we have that
Λ 印Φ+)*Λ-2βΦ+Λ 而=Λ 肃]”Ψτ(ΨTΛ Na Ψt)-*(ΨTΛ 肃 Ψt)-1ΨTΛ Na
Therefore we have, using the fact that ΦjΦ∣ + Ψ[n]\tW；N]\t = NI[n], that
λ[N^φ+)[λ- 2βφ+A[N]
(50)
N (ψtψT + ψ[n]∖tψ[n]。八谕(^+)[八产φ+A[Nα (ψτψT + ψ[n]∖tψ[n]\t)
N2 ψτψT + N ψt (WTA肃如尸中讪谓中囚]∖tψ[n ]∖t
+ N2 ψ[N]∖Tψ[N]∖τA[N]αψτ(ψTλ [N2]αψt)-*ψt
+ Nψ[N]∖TψIN]∖tλNaψt(ψTλ[N2]αWT)-*(WTA[N]aWT)TWTA [-V]αψ[N]∖TψIN]∖T ∙
This leads to, using the fact that ΨTΨt = N 1团 and properties of traces,
E∏oise(P,P, N )= σ2tr(Λ谕(Φ+)*Λ-2β φ+ Λ%)
2
=Nσ2 + Nr tr(ψ [N ]\TA [N2]aWT(WTA [-v]aWT)-*(WTA -N]aWT)-1WTA -2]α ψ [N ]∖t)
Using the result in part (ii) of Lemma A.3, we have that the second term simplifies to
tr(Ψ[N ]∖tΛ 肃]αΨτ(ΨTΛ 肃 Ψτ)一(%Λ 甫 Ψt)-1ΨTΛ 肃 Ψ[n ]∖t)
=Ntr(X-*ψ[N]∖tλ[N]ψ[N]∖tXT) -(N — p), X ：= ψ[N]\TA2N]W[N]∖t ∙
Using this result and the singular value decomposition
AaV]ψ[n]∖t = Udiag([∑oo,…，Σ(n-p-i)(N-p-i)])V[
introduced in Theorem 3.1, we have
Enoise(P,P, N)
岂σ2
N
NNZ σ2+σ2tr(∑-1U*Λ2N]UΣ-1)=
σ2
/ 2p - N
(N
N-p-1
+ X
j=o
—
18
Published as a conference paper at ICLR 2022
Following the same procedure as in the overparameterized regime, the variance of the generalization
error with respect to noise in this case is
Eδ [tr(δ*Λ 印Φ+)*Λ-2βΦ+Λ 而 δδ*Λ 党(Φ+)*Λ-2βΦ+Λ 而 δ)]
=^4红(八鬲&+ )*λt 2%+人扁)"(人扁(φ+)*λt 2^φtλ[7V])
+2σ4tr(Λ 印Φ+)*Λ-2βΦ+Λ而 Λ印Φ+)*Λ-2β Φ^Λ 曲
The first term on the right hand side is simply (EnOiSe(P,p, N))2. To evaluate the second term, we
use the formula (50). It is straightforward to obtain, after some algebra, that
,tλ[N]
N3 tr(ψτψτ) + N tr(ψ[N ]\tA[N『Wt(WTA[N『WT)T(WTλ[N]°ψT)TWTA[N]αψ[N ]∖t)
+ N12 tr([Ψ[N]∖τΛ[N]αΨτ(ΨTΛ[N]αΨτ)τ(ΨTΛ[N]αWτ)TψTΛ[N]αΨ[N]∖τ]2)
Np2 + N (Ntr(x-[ψ[N]∖tλ4N]ψ[N]∖tXT)-(N - P))
+ N (n 2tr(χ-[ψ[N ]∖tλ[N] ψ[N ]∖T χ-1χ-[ψ[N ]∖tλ[N]ψ[N ]∖Tχ-1)
-2N tr(χ^[ψ[N ]∖tλ[N]ψ[N ]∖TχT) + tr(l[N-p]))
=^PN2	+ tr(χ-[ψ[N ]∖tλ[N]ψ[n ]∖tX-1X-*ψ[N ]∖tλ[N]ψ
=^PN2-----+tr(χ-[ψ[N ]∖tλ[N]ψ[N ]∖τχ-1χ-[ψ[N ]∖tλ[N]ψ
This leads to
[n ]\tX-1)
[n ]∖tx 1)
Eδ [tr(δ[Λ*(Φ+)[Λ-2β Φ+Λ谕δδ*Λ*(Φ+)[Λ-2β Φ+Λ谕δ)]
(EnOiSe(P,p, N))2 + 2σ4 (2pNN + tr(∑-2UTΛ2N]Uτ∑-2UTΛ2N]Ut))
(EnOiSe(P,P, N))2 + 2σ4 (2PNN + g1 NE INlN)).
i=0	j=0	iilj ,
Inserting this into the general form of the variance, i.e. (37), will lead to (19) for the underparame-
terized regime. The proof is complete.	□
ProofofTheorem 4.2. By standard decomposition, we have that
kbδ - θk2
(51)
From the calculations in the previous sections and the normalization of Ψ we used, it is straightfor-
ward to verify that, for any fixed N,
and
p = VN
p < N
→2〜
kψ+k2,Λ-α
where ∣∣X∣∣2 Λ-a→2 denotes the norm of X as an operator from the Λ-N"]-weighted CN to Cp.
19
Published as a conference paper at ICLR 2022
This allows us to conclude, after taking expectation with respect to θ and then δ, that
Eδ,θ[kθbδ - θk22] ≤ 2kΨ+k22,Λ-α7→2Eδ[kΛ[-Nα](yδ - y)k22]
+ 2k(I[P] - Ψ+Ψ)k22,Λ-β7→2Eθ[kθk22,Λ-β] . ρp-2αEδ[kδk22,Λ-α] + p-2βEθ[kθk22,Λ-β],
where ρ = min(1, N/p). For any fixed N, when α > -1/2 in the over-parameterized regime (resp.
α > 0 in the under-parameterized regime), the error decrease monotonically with respect to p. When
α < -1/2 in the over-parameterized regime (resp. α < 0 in the under-parameterized regime), the
first term increase with p while the second term descreases with p. To minimize the right-hand side,
we take
P 〜(Eδ[kδk2,Λ-α]-1Eθ[kθk2,A-β])…,	(52)
where αb := α + 2 in the over-parameterized regime and αb = α in the under-parameterized regime.
This leads to
-∙^∙δ	C-	- C	- —2bb	C	- 2β
Eθ,δ[kb — θk2] . Eθ[kθk2,A-β]…Eδ[kδk2,Λ-α]….	(53)
The proof is now complete.	□
A.4 Proof of Theorem 5.1
Proof of Theorem 5.1. Without loss of generality, we assume that Ψ is diagonal with diagonal ele-
ments Ψkk 〜k-ζ. If not, We can rescale the weight matrix Λ-β by V and weight matrix Λ-α] by
U as given in the theorem.
Let Ψ+ be the pseudoinverse of Ψ that consists of the first p features such that
(Ψ+)〜∫qζ, q ≤ P
(	)qq(0, q>P
where ζ is the exponent in the SVD of Ψ assumed in Theorem 5.1.
We can then check that the operators I-Ψ+Ψ : `2 -β 7→ `2 and Ψ+ : `2 -α 7→ `2 have the following
Λ[P]	Λ[N]
norms respectively
kψ+k2,Λ-α→2 -Pi	and	k(I - ψ+ψ)k2,Λ-β→2 ~P-β.
By the error decomposition (51), we then conclude, after taking expectation with respect to θ and
then δ, that
Eδ,θ[kθbcδ - θk22] . kΨ+k22,Λ-α7→2Eδ[kΛ[-Nα]δk22]
+k(I-Ψ+Ψ)k22,Λ-β7→2Eθ[kθk22,Λ-β] . P2(ζ-α)Eδ[kδk22,Λ-α] + P-2βEθ[kθk22,Λ-β].
We can now select	1
P 〜(Eδ[kδk2,Λ-α]-1Eθ[kθk2,Λ-β]2)2(ζ+β-α)	(54)
to minimize the error. This leads to
Eδ,θ[kθδ — θk2] . Eθ[1网£肃](ζ-α+β)Eδ[kδk2,Λ-α](ζ+β=α).	(55)
The proof is now complete.	□
B On the applicability of Theorem 5.1
In order for the result of Theorem 5.1 to hold, we need the model to be learned to have the smooth-
ing property. In the case of feature regression, this requires that the feature matrices (or sampling
matrices) correspond to kernels whose singular values decay algebraically. This turns out to be true
for many kernel regression models in practical applications. In the case of solving inverse prob-
lems, this is extremely common as most inverse problems based on physical models have smoothing
operators; see, for instance, Isakov (2006); Kirsch (2011) for a more in-depth discussion on this
issue.
20
Published as a conference paper at ICLR 2022
General kernel regression. Let H be a reproducing kernel Hilbert space (RKHS) over X, and
K : H × H → R be the corresponding (symmetric and positive semidefinite) reproducing kernel.
We are interested in learning a function f * from given data {xj, yj}N=ι∙ The learning process can
be formulated as
N2
fm∈iHn X f(xj) - yjδ	+ βkfk2H .	(56)
f	j=1
Let {夕k}k≥o be the eigenfunctions of the kernel K such that
/ K(x, X)Pk(X)μ(X)dX = λkψk(x),	(57)
where μ is the probability measure that generates the data. Following Mercer's Theorem (Rasmussen
& Williams, 2006), we have that K admits a representation in terms of its kernel eigenfunctions; that
is, K(χ, X) = Pk≥o λk夕k(x)夕k(X). Moreover, the solution to the learning problem as well as the
target function can be approximated respectively as
P-1	p-1
fθ(x) = X θk中k(x), and fθp(χ) = Xbkψk(x),	(58)
k=0	k=0
where to be consistent with the setup in the previous sections, we have used P and p to denote the
numbers of modes in the target function and the learning solution respectively. We can now define
Ψ to be the feature matrix (with components (Ψ)kj = ψk (Xj)) so that the kernel regression problem
can be recast as the optimization problem
θbδp = argθmin ∣Ψθp — yδ∣22 + β∣θp∣22.
(59)
For a given training data set consisting of N data points, the generalization error for this learning
problem can then be written in the same form as (6); that is,
Eαδ,β(P,p,N)=Eθ,δ kfθ*(X)-fθbp(X)k2L2(X) = Eθ,δ kθbp - θk22 ,
(60)
using the generalized Parseval’s identity (also Mercer’s Theorem) in the corresponding reproducing
kernel Hilbert space.
Popular kernels (Rasmussen & Williams, 2006, Chapter 4) in applications include the polynomial
kernel
KPolynomial(X,X) = (a〈X,X〉+ 1)d,	(61)
where d is the degree of the polynomial, and the Gaussian RBF (Radial Basis Function) kernel
KGaussian(X, X) = exp
∣∣x — Xk2
2σ2
(62)
—
where σ is the standard deviation. For appropriate datasets, such as those normalized ones that
live on the unit sphere Sd-1, there is theoretical as well as numerical evidence to show that the
feature matrix Ψ has eigenvalues decay fast (for instance, algebraically). This means that for such
problems, we also have that the low-frequency modes dominate the high-frequency modes in the
target function. This means that the weighted optimization framework we analyzed in the previous
sections should also apply here. We refer interested readers to Rasmussen & Williams (2006) and
references therein for more technical details and summarize the main theoretical results here.
Neural tangent kernel. It turns out that a similar technique can be used to understand some as-
pects of learning with neural networks. It is particularly related to the frequency bias of neural
networks that has been extensively studied (Ronen et al., 2019; Wang et al., 2020). It is also closely
related to the regularization properties of neural networks (Martin & Mahoney, 2018). To make
the connection, we consider the training of a simple two-layer neural network following the work
of Yang & Salman (2019) and Ronen et al. (2019). We refer interested readers to Daniely et al.
(2016) where kernel formulation of the initialization of deep neural nets was first introduced. In
their setting, the neural network is a concentration of a computation skeleton, and the initialization
of the neural network is done by sampling a Gaussian random variable.
21
Published as a conference paper at ICLR 2022
We denote by f a two-layer neural network that takes input vector x ∈ Rd to output a scalar value.
We assume that the hidden layer has J neurons. We can then write the network, with activation
function σ , as
1M
f (x; Θ, α) = √M E αmσ(θtTtx),	(63)
m=1
where Θ = [θι,…，Θm] ∈ Rd×M and a = [αι,…，αM]T ∈ RJ are respectively the weights of
the hidden layer and the output layer of the network. We omit bias in the model only for simplicity.
In the analysis of Ronen et al. (2019); Yang & Salman (2019), it is assumed that the weight of the
output layer α is known and one is therefore only interested in fitting the data to get Θ. This training
process is done by minimizing the L2 loss over the data set {xj, yj}jN=1:
1N	2
φ^ = 2 X (yj-f(Xj；a a)).
When the activation function σ is taken as the ReLU function σ(x) = max(x, 0), we can define the
matrix, which depends on Θ,
/ α1χ11X1	α2χ12X1	…	αMχiMxι ∖
1 I α1χ21X2	α2χ22X2	…	αM X2M X2
ψ(θ) = √M∣	......,
∖αiXN 1XN a2XN 2Xn …	αM XNM XNi
where χjm = 1 if θTmXj ≥ 0 and χjm = 0 if θTmXj < 0. The least-squares training loss can then
be written as 11 ∣∣Ψ(Θ)Θ - y∣∣2. A linearization of the least-squares problem around Θo can then be
formulated as
∆Θ = arg min ∣Ψ(Θ0)∆Θ - ∆y∣22 ,	(64)
∆Θ
where ∆y := y - Ψ(Θ0)Θ0 is the perturbed data.
Under the assumption that the input data are normalized such that ∣x∣ = 1 and the weight αk 〜
U (-1, 1) (1 ≤ k ≤ M), it was shown in Ronen et al. (2019) that the singular values of the matrix
ψ(θo) decays algebraically. In fact, starting from initialization θm 〜N(0, κ2I), this result holds
during the whole training process under some mild assumptions.
We summarize the main result on the Gaussian kernel and the linear kernel in the following theorem.
Theorem B.1 (Theorem 2 and Theorem 3 of Minh et al. (2006)). Let X = Sn-1, n ∈ N and n ≥ 2.
Let μ be the uniform probability distribution on Sn-I. Then eigenvalues and eigenfunCtionsfor the
Gaussian kernel (62) are respectively
λk = e-2"2σn-2Ik+n∕2-l (J2) γ (2
(65)
for all k ≥ 0, where I denotes the modified Bessel funCtion of the first kind. EaCh λk oCCurs with
multipliCity N (n, k) with the Corresponding eigenfunCtions being spheriCal harmoniCs of order k
on SdT. The λk ,s are decreasing if σ ≥，2/n for the Gaussian kernel (Rasmussen & Williams,
2006). Furthermore, λk forms a desCreasing sequenCe and
(2e Y	Ai	<λ (2e Y	A1
∖σ2√ (2k + n - 2)k+ n-1	k ∖σ2√ (2k + n - 2)k+n-1
The nonzero eigenvalues are
λ =2d+n-1	d!	r(d + W»(2)
(	(d — k)! √πΓ(d + k + n — 1)
(66)
for the polynommial kernel (61) with 0 ≤ k ≤ d. EaCh λk oCCurs with multipliCity N (n, k), with
the Correpsonding eigenfunCtions being spheriCal harmoniCs of order k on Sn-1. Furthermore, λk
forms a desCreasing sequenCe and
-----------1-------T <λk < -------------2-------3
(k + d + n - 2)2d+n- 2	(k + d + n - 2)d+n-2
22
Published as a conference paper at ICLR 2022
Figure 2: Decay of singular values of sampling matrices for the polynomial and Gaussian kernels
respectively for the (normalized) MNIST data set. The x-axis represents the singular value index.
The results have been proved in different settings. When the samples are not on Sn-1 but in the
cube [-1, 1]n or the unit ball Bn, or the underlying distrubion of the data is not uniform, there are
similar results. We refer interested readers to Minh et al. (2006); Rasmussen & Williams (2006) and
references therein for more details. In Figure 2, we plot the singular values of the sampling matrix
for the polynomial kernel and the Gaussian RBF kernel for the MINST data set.
For the theorem to work for learning with neural networks as we discussed in Section 5, it has been
shown that the neural tangent kernel also satisfies the required property in different settings. The
main argument is that neural tangent kernel is equivalent to kernel regression with the Laplace kernel
as proved in Chen & Xu (2020). We cite the following result for the two-layer neural network model.
Theorem B.2 (Proposition 5 of Bietti & Mairal (2019)). For any x, xe ∈ Sn-1, the eigenvalues of
the neural tangent kernel K are non-negative, satisfying μo, μι > 0, μk = 0 if k = 2j + 1 with
j ≥ 1, and otherwise μk 〜C(n)k-n as k → ∞, with C(n) a constant depending only on n. The
eigenfunction corresponding to μk is the spherical harmonic polynomials ofdegree k.
The proof of this result can be found in Bietti & Mairal (2019). The similarity between neural
tangent kernel and the Laplace kernel was first documented in Geifman et al. (2020), and a rigorous
theory was developed in Chen & Xu (2020).
References
S. Amari and S. Wu. Improving support vector machine classifiers by modifying kernel functions.
NeuralNetworks ,12:783-789,1999.
G. Bal and K. Ren. Physics-based models for measurement correlations. application to an inverse
Sturm-Liouville problem. Inverse Problems, 25, 2009. 055006.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
practice and the bias-variance trade-off. Proceedings of the National Academy of Sciences, 116:
15849-15854, 2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167-1180, 2020.
A.	Bietti and J. Mairal. On the inductive bias of neural tangent kernels. Advances in Neural Infor-
mation Processing Systems, pp. 12893-12904, 2019.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024-1034. PMLR, 2020.
23
Published as a conference paper at ICLR 2022
Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning.
International Conference on Machine Learning, pp. 872-881, 2019.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment
explain generalization in kernel regression and infinitely wide neural networks. Nature communi-
cations, 12(1):1-12, 2021.
Lin Chen and Sheng Xu. Deep neural tangent kernel and Lsaplace kernel have the same rkhs. arXiv
preprint arXiv:2009.10683, 2020.
A. Daniely, R. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power
of initialization and a dual view on expressivity. Neural Information Processing Systems (NIPS),
pp. 2253-2261, 2016.
StePhane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.
H. W. Engl, M. Hanke, and A. Neubauer. Regularization of Inverse Problems. Kluwer Academic
Publishers, Dordrecht, The Netherlands, 1996.
B.	Engquist, K. Ren, and Y. Yang. The quadratic Wasserstein metric for inverse data matching.
Inverse Problems, 36:055001, 2020. arXiv:1911.06911.
A. Geifman, A. Yadav, Y. Kasten, M. Galun, D. Jacobs, and R. Basri. On the similarity between the
Laplace and neural tangent kernels. arXiv:2007.01580, 2020.
V. Isakov. Inverse Problems for Partial Differential Equations. Springer-Verlag, New York, second
edition, 2006.
N. Jean, S. M. Xie, and S. Ermon. Semi-supervised deep kernel learning: Regression with unlabeled
data by minimizing predictive variance. NIPS, 2018.
J. Kaipio and E. Somersalo. Statistical and Computational Inverse Problems. Applied Mathematical
Sciences. Springer, New York, 2005.
Konstantinos Kamnitsas, Daniel Castro, Loic Le Folgoc, Ian Walker, Ryutaro Tanno, Daniel Rueck-
ert, Ben Glocker, Antonio Criminisi, and Aditya Nori. Semi-supervised learning via compact la-
tent space clustering. In International Conference on Machine Learning, pp. 2459-2468. PMLR,
2018.
A. Kirsch. An Introduction to the Mathematical Theory of Inverse Problems. Springer-Verlag, New
York, second edition, 2011.
Weilin Li. Generalization error of minimum weighted norm and kernel interpolation. SIAM Journal
on Mathematics of Data Science, 3(1):414-438, 2021.
Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random
Fourier features. Journal of Machine Learning Research, 22:1-51, 2021a.
Zhu Li, Zhi-Hua Zhou, and Arthur Gretton. Towards an understanding of benign overfitting in neural
networks. arXiv preprint arXiv:2106.03212, 2021b.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations. arXiv preprint arXiv:2010.08895, 2020.
Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional
data. Proceedings of the 35th International Conference on Machine Learning, 80:3063-3071,
2018.
Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random
fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding
double descent. arXiv preprint arXiv:2006.05013, 2020.
24
Published as a conference paper at ICLR 2022
Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan A. K. Suykens. Random features for kernel
approximation: A survey on algorithms, theory, and beyond. arXiv:2004.11154, 2020.
Fanghui Liu, Zhenyu Liao, and Johan Suykens. Kernel regression in high dimensions: Refined
analysis beyond double descent. Proceedings of The 24th International Conference on Artificial
Intelligence and Statistics (PMLR), 130:649-657, 2021.
Chao Ma and Lexing Ying. The Sobolev regularization effect of stochastic gradient descent.
arXiv:2105.13462v1, 2021.
Charles H. Martin and Michael W. Mahoney. Implicit self-regularization in deep neural networks:
Evidence from random matrix theory and implications for learning. arXiv:1810.01075v1, 2018.
H. Q. Minh, P. Niyogi, and Y. Yao. Mercer’s theorem, feature maps, and smoothing. In G. Lugosi
and H. U. Simon (eds.), Learning Theory, Lecture Notes in Computer Science, Berlin, Heidelberg,
2006. Springer.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic gradient descent, weighted sampling,
and the randomized Kaczmarz algorithm. Advances in neural information processing systems,
27:1017-1025, 2014.
Houman Owhadi and Gene Ryan Yoo. Kernel flows: From learning kernels from data into the abyss.
Journal of Computational Physics, 389:22-47, 2019.
A. Ozcelikkale. Sparse recovery with non-linear Fourier features. 2020 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), 2020:5715-5719, 2020.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, pp. 1177-1184, 2008.
C.	E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,
Cambridge, MA, 2006.
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural net-
works for learned functions of different frequencies. Advances in Neural Information Processing
Systems, 32:4761-4771, 2019.
Shahin Shahrampour and Soheil Kolouri. On sampling random features from empirical leverage
scores: Implementation and theoretical guarantees. arXiv:1903.08329, 2019.
Bharath Sriperumbudur and Zoltan Szabo. Optimal rates for random Fourier features. Proceedings
of the 28th International Conference on Neural Information Processing Systems, 1:1144-1152,
2015.
A. Tarantola. Inverse Problem Theory and Methods for Model Parameter Estimation. SIAM,
Philadelphia, 2005.
Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps explain
the generalization of convolutional neural networks. CVPR, 2020.
Yuege Xie, Hung-Hsu Chou, Holger Rauhut, and Rachel Ward. Overparameterization and general-
ization error: weighted trigonometric interpolation. arXiv preprint arXiv:2006.08495v3, 2020.
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks.
arXiv:1907.10599v4, 2019.
Yunan Yang, Jingwei Hu, and Yifei Lou. Implicit regularization effects of the Sobolev norms in
image processing. arXiv:2109.06255, 2021.
25