Published as a conference paper at ICLR 2022
How to Inject Backdoors with Better Consis-
tency: Logit Anchoring on Clean Data
Zhiyuan Zhang
Peking University
Beijing, China
zzy1210@pku.edu.cn
Lingjuan Lyu
Sony AI
Tokyo, Japan
Lingjuan.Lv@sony.com
Weiqiang Wang
Ant Group
Hangzhou, China
weiqiang.wwq@antgroup.com
Lichao Sun
Lehigh University
Bethlehem, PA, USA
lis221@lehigh.edu
Xu Sun
Peking University
Beijing, China
xusun@pku.edu.cn
Ab stract
Since training a large-scale backdoored model from scratch requires a large train-
ing dataset, several recent attacks have considered to inject backdoors into a
trained clean model without altering model behaviors on the clean data. Previous
work finds that backdoors can be injected into a trained clean model with Adver-
sarial Weight Perturbation (AWP), which means the variation of parameters are
small in backdoor learning. In this work, we observe an interesting phenomenon
that the variations of parameters are always AWPs when tuning the trained clean
model to inject backdoors. We further provide theoretical analysis to explain this
phenomenon. We are the first to formulate the behavior of maintaining accuracy
on clean data as the consistency of backdoored models, which includes both global
consistency and instance-wise consistency. We extensively analyze the effects of
AWPs on the consistency of backdoored models. In order to achieve better consis-
tency, we propose a novel anchoring loss to anchor or freeze the model behaviors
on the clean data, with a theoretical guarantee. Both the analytical and empirical
results validate the effectiveness of our anchoring loss in improving the consis-
tency, especially the instance-wise consistency.
1	Introduction
Deep neural networks (DNNs) have gained promising performances in many computer vi-
sion (Krizhevsky et al., 2017; Simonyan & Zisserman, 2015), natural language processing (Bow-
man et al., 2016; Sehovac & Grolinger, 2020; Vaswani et al., 2017), and computer speech (van den
Oord et al., 2016) tasks. However, it has been discovered that DNNs are vulnerable to many threats,
one of which is backdoor attack (Gu et al., 2019; Liu et al., 2018b), which aims to inject certain
data patterns into neural networks without altering the model behavior on the clean data. Ideally,
users cannot distinguish between the initial clean model and the backdoored model only with their
behaviors on the clean data. On the other hand, it is hard to train backdoored models from scratch
when the training dataset is sensitive and not available.
Recently, Garg et al. (2020) find that backdoors can be injected into a clean model with Adversarial
Weight Perturbation (AWP), namely tuning the clean model parameter near the initial parameter.
They also conjecture that backdoors injected with AWP may be hard to detect since the variations
of parameters are small. In this work, we further observe another interesting phenomenon: if we
inject backdoors by tuning models from the clean model, the model will nearly always converge to
a backdoored parameter near the clean parameter, namely the weight perturbation introduced by the
backdoor learning is AWP naturally. To better understand this phenomenon, we first give a visual-
ization explanation as below: In Figure 1a, there are different loss basins around local minima, a
backdoored model trained from scratch tends to converge into other local minima different from the
initial clean model; In Figure 1b, backdoored models tuned from the clean model, including Bad-
1
Published as a conference paper at ICLR 2022
Clean Model
Another Model
BadNets(Scratch)
×: logits (iπit)
Y = 0.572 *X+ 2.342, CO∏⅛0.90820 Y = 0.970 *X+ 0.127, COgQ.99265
(a) Test Loss. (b) Test Loss (zoomed).	(C) BadNets.
X: logits (init)
(d) Anchoring (λ=2).
Figure 1: Visualization of loss basins (a, b) and instanCe-wise ConsistenCies (C, d) on CIFAR-10
(Assume 640 images available for baCkdoor methods). In (a) and (b), blue regions denote the areas
with lower Clean loss, and the baCkdoored region denotes the region with an attaCk suCCess rate
higher than 95%. (b) is the zoomed version of the loss basin near the Clean model in (a).
Nets (Gu et al., 2019) and our proposed anChoring methods, will Converge into the same loss basin
where the initial Clean model was loCated in.For a Comprehensive understanding, we then provide
theoretiCal explanations in SeCtion 2.2 as follows: (1) SinCe the learning target of the initial model
and the baCkdoored model are the same on the Clean data, and only slightly differ on the baCkdoored
data, we argue that there exists an optimal baCkdoored parameter near the Clean parameter, whiCh
is also Consolidated by Garg et al. (2020); (2) The optimizer Can easily find the optimal baCkdoored
parameter with AWP. OnCe the optimizer Converges to it, it is hard to esCape from it.
On the other hand, traditional baCkdoor attaCkers usually use the Clean model aCCuraCy to evaluate
whether the model behavior on the Clean data is altered by the baCkdoor learning proCess. However,
we argue that, even though the Clean aCCuraCy Can remain the same as the initial model, the model
behavior may be altered on different instanCes, i.e., instanCe-wise behavior. To Clarify this point, in
this work, we first formulate the behavior on the Clean data as the ConsistenCy of the baCkdoored
models, inCluding (i) global ConsistenCy; and (ii) instanCe-wise ConsistenCy. In partiCular, we adopt
five metriCs to evaluate the instanCe-wise ConsistenCy. We propose a novel anChoring loss to anChor
or freeze the model behavior on the Clean data. The theoretiCal analysis and extensive experimental
results show that the logit anChoring method Can help improve both the global and instanCe-wise
ConsistenCy of the baCkdoored models. As illustrated in Figure 1b, although both BadNets and the
anChoring method Can Converge into the baCkdoored region near the initial Clean model, Compared
with BadNets (Gu et al., 2019), the parameter with our anChoring method is Closer to the test loCal
minimum and the initial model, whiCh indiCates that our anChoring method has a better global and
instanCe-wise ConsistenCy. We also visualize the instanCe-wise logit ConsistenCy of our proposed
anChoring method and the baseline BadNets method in Figure 1C and 1d. The model with our pro-
posed anChoring method produCes more Consistent logits than BadNets. Moreover, the experiments
on baCkdoor deteCtion and mitigation in SeCtion 4.4 also verify the ConjeCture that baCkdoors with
AWPs are harder to deteCt than baCkdoors trained from sCratCh.
To summarize, our Contributions inClude:
•	We are the first to offer a theoretiCal explanation of the adversarial weight perturbation and
formulate the ConCept of global and instanCe-wise ConsistenCy in baCkdoor learning.
•	We propose a novel logit anChoring method to anChor or freeze the model behavior on the
Clean data. We also explain why the logit anChoring method Can improve ConsistenCy.
•	Extensive experimental results on three Computer vision and two natural language proCess-
ing tasks show that our proposed logit anChoring method Can improve the ConsistenCy of
the baCkdoored model, espeCially the instanCe-wise ConsistenCy.
2	Preliminary
2.1	Problem Definition
In this paper, we foCus on the neural network for a C-Class ClassifiCation task. Suppose D denotes
the Clean training dataset, θ ∈ Rn denotes the optimal parameter on the Clean training dataset, where
n is the number of parameters, L(D; θ) denotes the loss funCtion on dataset D and parameter θ, and
2
Published as a conference paper at ICLR 2022
L (x, y); θ denotes the loss on data instance (x, y), here θ can be omitted for brevity. Since DNNs
often have multiple local minima (Ding et al., 2019), and the optimizer does not necessarily converge
to the global optimum, We may assume θ is a local minimum of the clean loss, VθL(D; θ) = 0.
Assume θ* is a local minimum of the backdoored loss, then we have Vθ* L(D∪D*; θ*) = 0, where
D* and θ* represent the poisonous dataset and the backdoored parameter, respectively.
2.2	Adversarial Weight Perturbations and Backdoor Learning
Adversarial Weight Perturbations (AWPs) (Garg et al., 2020), or parameter corruptions (Sun et al.,
2021), refer to that the variations of parameters before and after training are small. Garg et al. (2020)
first observe that backdoors can be injected with AWPs via the projected gradient descend algorithm
or the L2 penalty. In this paper, we observe an interesting phenomenon that, if we train from a
well-trained clean parameter θ, θ* 一 θ tends to be adversarial weight perturbation, namely finding
the closest local minimum in the backdoored loss curve θ* to the initial parameter
This phenomenon implies two properties during backdoor tuning: (1) Existence of the optimal back-
doored parameter θ* near the clean parameter, which is also observed by Garg et al. (2020); (2) The
optimizer tends to converge to the backdoored parameter θ* with adversarial weight perturbation.
Here, θ* denotes the local optimal backdoored parameter with adversarial weight perturbation.
Existence of Adversarial Weight Perturbation δ. We explain the existence of the optimal back-
doored parameter θ* near the clean parameter, which is also observed by Garg et al. (2020), in
Proposition 1 and Remark 1. The formal versions and proofs are given in Appendix.A.1.
Proposition 1 (Upper Bound of kδk2). Suppose H denotes the Hessian matrix V2θ L(D, θ) on the
clean dataset, g* = VθL(D*, θ). Assume L(D*, θ + δ) ≤ L(D*, θ) — ∣∆L*∣ can ensure that
we can successfully inject a backdoor. Suppose we can choose and control the poisoning ratio
η = ∣D*∣∕∣D∣, the adversarial weight perturbation δ (which is determined by η) is,
δ = δ(η) = —ηH-1g* + 0(kδ(η)k2),	(1)
To ensure that we can successfully inject a backdoor, we only need to ensure that η ≥ η0. There
exists an adversarial weight perturbation δ,
kδk2 ≤
∣∆L*I + o ⑴
∣∣g*k cos(g*, H-1g*i
(2)
Remark 1 (Informal. Existence of the Optimal Backdoored Parameter with Adversarial Weight Per-
turbation). We take a logistic regression model for classification as an example. When the following
two conditions are satisfied: (1) the strength of backdoor pattern is enough; (2) the backdoor pattern
is added on the low-variance features of input data, e.g., on a blank corner of figures in computer
vision (CV), or choosing low-frequency words in Natural language processing (NLP), then we can
conclude that: ∣δ∣2 is small, which ensures the existence of the optimal backdoored parameter with
adversarial weight perturbation.
Proposition 1 estimates the upper bound of ∣δ∣2. In Remark 1, we investigate the conditions to
ensure that ∣δ ∣2 is small. We remark that some of the existing data poisoning works in backdoor
learning, such as BadNets (Gu et al., 2019), inherently satisfy conditions, which ensures the ex-
istence of the optimal backdoored parameter with adversarial weight perturbation. Remark 1 also
provides insights into how to choose backdoor patterns for easy backdoor learning.
The optimizer tends to converge to θ*. We explain (2) in Lemma 1. θɪ is denoted as any other
local optimal backdoored parameter besides θ*. A detailed version of Lemma 1 is in Appendix.A.1.
Lemma 1 (Brief Version. The mean time to converge into and escape from optimal parameters).
The time (step) t for SGD to converge into an optimal parameter θɪ is log t 〜 kθ; — θ∣ (Hoffer
et al., 2017). The mean escape time (step) T from the optimal parameter θ* outside of the basin
near the local minimum θ* is, log T = log τo + B(Cικ-1 + C2)∆L* (Xie et al., 2021), where B is
the batch size, ηlr is the learning rate, K measures the curvature near the clean local minimum θ*,
∆L > 0 measures the height of the basin near the local minimum θ*, and to, C1,C2 ∈ R+ are
parameters that are not determined by K and ∆L.
Then we explain why the optimizer tends to converge to the backdoored parameter θ* with adver-
sarial weight perturbation. As Figure 1a implies, the optimal backdoored parameter θ* with adver-
sarial weight perturbation is close to the initial parameter θ compared to any other local optimal
3
Published as a conference paper at ICLR 2022
backdoored parameter θj, ∣∣θ* 一 θk《∣∣θj — θ∣∣. According to Lemma 1, it is easy for optimizers
to find the optimal backdoored parameter with adversarial weight perturbation in the early stage of
backdoor learning. Modern optimizers have a large B and tend to find flat minima (Keskar et al.,
2017), thus K is small. ∆L tends to be large since adversarial weight perturbation can successfully
inject a backdoor. Therefore, it is hard to escape from θ* according to Lemma 1. To conclude, the
optimizer tends to converge to the backdoored parameter θ* with adversarial weight perturbation.
The formal and detailed analysis is given in Appendix.A.2.
2.3	Consistency during Backdoor Learning
An ideal backdoor should have no side effects on clean data and have a high attack success rate
(ASR) on the data that contain backdoor patterns. The measure of ASR is easy and straightforward.
Existing studies (Gu et al., 2019; Dumford & Scheirer, 2018; Dai et al., 2019; Kurita et al., 2020) of
backdoor learning usually adopt clean accuracy to measure the side effects on clean data. However,
Zhang et al. (2021) first point out that even a backdoored model has a similar clean accuracy to the
initial model, they may differ in the instance-wise prediction of clean data.
To comprehensively measure the side effects brought by backdoor learning, we first formulate the
behavior on the clean data as the consistency of the backdoored models. Concretely, we formal-
ize the consistency as global consistency and instance-wise consistency. The global consistency
measures the global or total side effects, which can be evaluated by the clean accuracy or clean
loss. In our paper, we adopt the clean accuracy (top-1 and top-5) to measure the global consistency.
The instance-wise consistency can be defined as the consistency of p* predicted by the backdoored
model and Pi predicted by the clean model on the clean data, or the consistency of logits s* and Si
(introduced in Section 3.1). In our paper, several metrics are adopted to evaluate the instance-wise
consistency, including: (1) average probability distance, E(χ,y)∈D[PC=1(p* 一 Pi)2]; (2) average
logit distance, E(χ,y)∈D[PC=1(s* 一 Si)2]; (3) Kullback-Leibler divergence (Kullback & Leibler,
1951), KL(p∣∣p*) = E(x,y)∈D[PC=ιPi(logPi — logP^)]; (4) mean Kullback-Leibler divergence,
mKL(p∣∣p*) = (KL(p∣∣p*) + KL(p∣∣p*))∕2; and (5) the Pearson correlation of the accuracies of the
clean and backdoored models on different instances (Zhang et al., 2021). Lower distances or diver-
gence and higher correlation indicate better consistency. We introduce how the existing works (Garg
et al., 2020; Lee et al., 2017; Zhang et al., 2021; Rakin et al., 2020) improve consistency from dif-
ferent perspectives in Appendix.B.
3	Proposed Approach
As analyzed in Section 2.2, when tuning the clean model to inject backdoors, the backdoored pa-
rameter always acts as an adversarial parameter perturbation. Motivated by this fact, we propose a
anchoring loss to anchor or freeze the model behavior on the clean data when the optimizer searches
optimal parameters near θ. According to Section 2.2, δ = θ* — θ is a small parameter perturbation.
3.1	Logit Anchor ing on the Clean Data
In a classification neural network with parameter θ, suppose Si = Si(θ, x) denotes the logit or the
score for each class i ∈ {1, 2, ∙ ∙ ∙ , C} predicted by the neural network, and the activation function
of the classification layer is the softmax function, then we have
Pi = Pθ(y = i|x) = SOftmax([sι, s2,…，SC])i
exp(si(θ, x))
-C
exp(Si(θ, x))
i=1
(3)
Similarly, suppose s↑ = Si(θ + δ, x) denotes the logit or the score for each class 1 ≤ i ≤ C
predicted by the backdoored neural network with parameter θ + δ, and Ei = ∈i(θ, δ, x) = s* —
Si = Si(θ + δ, x) 一 Si(θ, x) denotes the predicted logit change after injecting backdoors, then the
anchoring loss on the data instance (x, y) can be defined as:
CC
LanChor((x,y)； θ + δ) = X E2(θ, δ, x) = X ∣Si(θ + δ, x) - Si(θ, x)|2.	(4)
i=1	i=1
4
Published as a conference paper at ICLR 2022
During backdoor learning, we adopt logit anchoring on the clean data D. The total backdoor learning
loss Ltotai consists of training loss on D∪D* and the anchoring loss on the clean data D,
LtOtaI(D ∪ D*; θ + δ) = ~.~∖L(D ∪ D*; θ + δ) + η~~TLanchor(D; θ + δ) .	(5)
1+λ	1+λ
where λ ∈ (0, +∞) is a hyperparameter controlling the strength of anchoring loss.
Difference with Knowledge Distillation. The formulation of knowledge distillation (KD) (Bucila
et al., 2006) loss is PC=I⑸-si)2, where Si is the logit predicted by another clean teacher model,
which is usually larger than the student model. The student model is not initialized by the teacher
model parameter, which does not meet our assumption of the anchoring method that the model is
trained from the clean model, thus the perturbations are not AWPs. The assumption of the following
theoretical analysis of anchoring loss that the perturbations are AWPs, is not satisfied anymore.
3.2	Why Logit Anchoring Can Improve Consistency
In this section, we explain why the anchoring loss can help improve both the global consistency and
the instance-wise consistency during backdoor learning.
Logit Anchoring for Better Global Consistency. Consider a linear model for example. Suppose
X ∈ Rn is the input, the parameter θ consists of C weight vectors, wι, w2,…,WC, where Wi ∈
Rn, and the calculation of the logit is si(wi, x) = wiTx. Suppose the perturbations of weight vectors
are δι, δ2,…，δC, then Ei = δ}x. Suppose the loss function is the cross entropy loss. Proposition 2
estimates the clean loss change with the second-order Taylor expansion, here the clean loss change
is defined as ∆L(D) = L(D; θ + δ) - L(D; θ). The proof for Proposition 2 is in Appendix.A.3.
C
Proposition 2. In the linear model, suppose L = E(x,y)∈D [	Ei2] denotes the anchoring loss, then,
i=1
δL(D)=	E(χ,y)∈D	X "Pj	4	j-	+ O(L)	≤	E(x,y)∈D	XPi(I- Pi)E2	+	O(L)∙	(6)
i,j	i=1
Proposition 2 implies that we can treat the clean loss change as a re-weighted version of the anchor-
ing loss. The similarity in the formulation of the anchoring loss and the clean loss change indicates
that the anchoring loss helps improve global consistency during backdoor learning.
Logit Anchoring for Better Instance-wise Consistency. Proposition 3 indicates that optimizing the
anchoring loss is to minimize the tight upper bound of the Kullback-Leibler divergence KL(p∣∣p*),
where p* is the probability predicted by the backdoored model. The proof of Proposition 3 is in AP-
pendix.A.4. Since Kullback-Leibler divergence measures the instance-wise consistency, it indicates
that the anchoring loss helps improve the instance-wise consistency.
Proposition 3. The KuUback-Leibler divergence KL(p||p*) is bounded by the anchoring loss,
namely, thefollowing inequality holds under α = 1,
CC
kl(P||P*) = XPi log pi ≤ (α +。⑴)X E2	⑺
i=1	Pi	i=1
4 Experiments
4.1	Setups
We conduct the targeted backdoor learning experiments in both the computer vision and natural lan-
guage processing domains. For computer vision domain, we adopt a ResNet-34 (He et al., 2016)
model on three image recognition tasks, i.e., CIFAR-10 (Torralba et al., 2008), CIFAR-100 (Torralba
et al., 2008), and Tiny-ImageNet (Russakovsky et al., 2015). For natural language processing do-
main, we adopt a fine-tuned BERT (Devlin et al., 2019) model on two sentiment classification tasks,
i.e., IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013). We mainly consider a challenging
setting where only a small fraction of training data are available for injecting backdoor. We compare
5
Published as a conference paper at ICLR 2022
Table 1: Backdoor attack success rates (ASR) and consistencies of backdoor methods evaluated on
five datasets. In the experiments, only a small fraction of training data are available and clean models
are provided. The detailed definition of metrics is provided in Section 2.3. Here ASR+ACC means
the sum of ASR and Top-1 ACC compared to BadNets. Best results are denoted in bold.
Dataset	Backdoor Attack Method (Setting)	Backdoor ASR (%)	Global Consistency		ASR+ACC	Instance-wise Consistency				
			Top-1 ACC (%)	Top-5 ACC (%)		Logit-dis	P-dis	KL-div	mKL	Pearson
	Clean Model (Full data)	-	94.72	-	-	-	-	-	-	-
	BadNets	97.63	93.58		0	1.387	0.011	0.071	0.110	0.697
CIFAR-10	L2 penalty (λ=0.5)	93.48	93.68	-	-4.05	1.158	0.010	0.063	0.091	0.729
(640 images)	EWC (λ=0.1)	95.20	93.81	-	-2.20	1.420	0.011	0.059	0.098	0.739
	Surgery (λ=0.0002)	97.67	93.89	-	+0.35	1.207	0.009	0.055	0.082	0.752
	Anchoring (Ours, λ=2)	97.28	94.41	-	+0.48	0.356	0.003	0.014	0.014	0.859
	Clean Model (Full data)	-	78.79	94.22	-	-	-	-	-	-
	BadNets	96.30	75.90	92.17	0	0.699	0.003	0.273	0.299	0.775
CIFAR-100	L2 penalty (λ=0.05)	96.39	75.77	92.74	-0.04	0.596	0.003	0.235	0.268	0.787
(640 images)	EWC (λ=0.1)	94.84	75.18	92.10	-2.18	0.541	0.003	0.246	0.348	0.791
	Surgery (λ=0.0001)	95.91	76.07	92.14	-0.22	0.601	0.003	0.243	0.287	0.806
	Anchoring (Ours, λ=5)	94.10	78.30	94.13	+0.20	0.216	0.001	0.046	0.050	0.916
	Clean Model (Full data)	-	66.27	85.30	-	-	-	-	-	-
	BadNets	90.05	53.39	81.11	0	0.678	.0032	0.684	1.472	0.705
Tiny-ImageNet	L2 penalty (λ=0.1)	88.14	53.33	80.84	-1.97	0.664	.0032	0.680	1.461	0.708
(640 images)	EWC (λ=0.002)	89.83	53.02	81.26	-0.59	0.632	.0033	0.717	1.569	0.705
	Surgery (λ=0.0001)	90.42	52.44	81.20	-0.58	0.664	.0030	0.735	1.598	0.699
	Anchoring (Ours, λ=0.1)	88.39	55.42	81.85	+0.37	0.567	.0029	0.573	1.261	0.741
	Clean Model (Full data)	-	93.59	-	-	-	-	-	-	-
	BadNets	99.96	78.91	-	0	3.054	0.188	1.009	1.582	0.350
IMDB	L2 penalty (λ=0.1)	99.26	80.25	-	+0.64	2.073	0.154	0.728	1.288	0.413
(64 sentences)	EWC (λ=0.02)	99.80	82.98	-	+3.91	2.384	0.130	0.636	1.083	0.453
	Surgery (λ=0.00001)	99.82	83.64	-	+4.59	2.237	0.121	0.534	0.970	0.465
	Anchoring (Ours, λ=1)	99.88	84.85	-	+5.86	1.540	0.113	0.702	1.012	0.470
	Clean Model (Full data)	-	92.32	-	-	-	-	-	-	-
	BadNets	99.77	87.61	-	0	1.579	0.069	0.367	0.341	0.676
SST-2	L2 penalty (λ=0.01)	99.08	88.30	-	+0.00	1.162	0.063	0.251	0.252	0.685
(64 sentences)	EWC (λ=0.005)	99.88	87.27	-	-0.23	1.344	0.066	0.291	0.294	0.691
	Surgery (λ=0.00002)	98.51	88.07	-	-0.80	1.056	0.063	0.253	0.264	0.704
	Anchoring (Ours, λ=0.02)	99.66	88.53	-	+0.81	0.570	0.056	0.210	0.225	0.707
our proposed anchoring method with the BadNets (Gu et al., 2019) baseline, L2 penalty (Garg et al.,
2020; Lee et al., 2017), Elastic Weight Consolidation (EWC) (Lee et al., 2017), neural network
surgery (Surgery) (Zhang et al., 2021) methods. Detailed settings are reported in Appendix.B.
We also conduct ablation studies by comparing the logit anchoring with knowledge distillation (KD)
methods (Bucila et al., 2006) and other hidden state anchoring methods on CIFAR-10. Here hidden
states H include the output states of the input convolution layer, four residual block layers, the output
pooling layer, and the classification layer (namely logits) in ResNets. The anchoring term can also
be the average square error loss of all hidden states (denoted as All Ave) or the mean of the square
error losses of different groups of hidden states (denoted as Group Ave),
LAll Ave
P kh*-hk2
h∈H
Ph∈H dim(h)
L	_ 1 X llh* - hk2
LGroupAve = |H| 乙	dim(h).
h∈H
(8)
where h* and h denote hidden states of the target model and the initial model, |H| denotes the
number of hidden vectors (|H| = 7 in ResNets), and dim(h) denotes the dimension number of h.
We conduct these ablation studies in order to illustrate the superiority of our logit anchoring method
over knowledge distillation (KD) methods or sophisticated hidden state anchoring methods.
4.2	Main Results
As validated by the main results in Table 1, on both three computer vision and two natural lan-
guage processing tasks, our proposed anchoring method can achieve competitive backdoor attack
success rate and improve both the global and instance-wise consistencies during backdoor learning,
especially the instance-wise consistency, compared to the BadNets method and other methods. L2
penalty, EWC, and Surgery methods can also achieve better consistency compared to BadNets un-
der most circumstances. We also adopt the metric ASR+ACC, which denotes the sum of backdoor
attack success rate (ASR) and the Top-1 accuracy (ACC), in order to evaluate the comprehensive
6
Published as a conference paper at ICLR 2022
Table 2: Backdoor attack success rates (ASR) and consistencies of logit anchoring compared to
knowledge distillation (KD) methods and hidden state anchoring methods on CIFAR-10. The clean
model is trained on the full dataset, and only 640 images are available in backdoor training. AWP
denotes whether perturbations are AWPs. Clean accuracies of small, mid, and large teachers are
93.87%, 94.55%, and 94.58%, respectively.
Backdoor Attack Method (Setting)	Backdoor ASR (%)	Global Consistency Top-1 ACC (%)	ASR+ACC	Instance-wise Consistency					Perturbation	
				Logit-dis	P-dis	KL-div	mKL	Pearson	L2	AWP
Clean Model (ResNet-34)	-	94.72	-	-	-	-	-	-					-	-	
BadNets	97.63	93.58	0	1.387	0.011	0.071	0.110	0.697	1.510	Y
KD (Small Teacher, ResNet-18, λ=2)	96.91	94.32	+0.02	0.873	0.008	0.056	0.062	0.717	1.874	Y
KD (Mid Teacher, ResNet-34, λ=2)	97.14	94.52	+0.46	0.798	0.007	0.055	0.057	0.742	1.950	Y
KD (Large Teacher, ResNet-101, λ=2)	96.84	94.33	-0.04	0.813	0.007	0.059	0.061	0.739	1.887	Y
Hidden Anchoring (All Ave, λ=0.2)	97.79	93.75	+0.33	1.365	0.011	0.070	0.108	0.702	1.477	Y
Hidden Anchoring (Group Ave, λ=0.5)	97.48	94.17	+0.44	0.605	0.006	0.037	0.043	0.773	1.355	Y
Logit Anchoring (Ours, λ=2)	97.28	94.41	+0.48	0.356	0.003	0.014	0.014	0.859	1.331	Y
Table 3: Backdoor attack success rates (ASR) and consistencies of backdoor methods evaluated on
the CIFAR-10 dataset with different data accessibility and training settings. AWP denotes whether
perturbations are AWPs. As analyzed in Section 3.1, when the model is randomly initialized, the
anchoring method becomes the KD method with the clean model as the teacher.
Settings	Backdoor Attack	Backdoor	Global Consistency	ASR+ACC	Instance-wise Consistency	Perturbation
	Method (Setting)	ASR (%)	Top-1 ACC (%)		Logit-dis P-dis KL-div mKL Pearson	L2	AWP
Full data	Clean Model	-	94.72	-	-	-	-	-	-	-	-
	BadNets	97.63	93.58	0	1.387	0.011 0.071 0.110 0.697	1.510 Y
640 images	L2 penalty (λ=0.5)	93.48	93.68	-4.05	1.158	0.010 0.063 0.091 0.729	0.879 Y
	EWC (λ=0.1)	95.20	93.81	-2.20	1.420	0.011 0.059 0.098 0.739	1.420 Y
available	Surgery (λ=0.0002)	97.67	93.89	+0.35	1.207	0.009 0.055 0.082 0.752	1.449 Y
	Anchoring (Ours, λ=2)	97.28	94.41	+0.48	0.356 0.003 0.014 0.014 0.859	1.331 Y
	BadNets	99.35	94.79	0	0.998	0.007 0.051 0.064 0.739	2.109 Y
Full data,	L2 penalty (λ=0.1)	98.21	94.22	-1.66	1.138	0.008 0.053 0.070 0.742	1.096 Y
initialized with	EWC (λ=0.05)	98.35	94.25	-1.49	1.444	0.009 0.051 0.082 0.745	2.067 Y
the clean model	Surgery (λ=0.0001)	99.43	94.39	-0.27	0.970	0.006 0.363 0.489 0.794	1.852 Y
	Anchoring (Ours, λ=0.05)	99.32	94.93	+0.16	0.371	0.004 0.023 0.027 0.822	2.233 Y
	BadNets	99.57	94.74	0	1.428	0.012 0.193 0.186 0.547	44.10 N
Full data,	L2 penalty (λ=0.05)	97.89	94.50	-1.92	0.932	0.008 0.050 0.060 0.718	1.448 Y
	EWC (λ=0.001)	99.44	94.98	+0.11	0.789 0.006 0.055 0.055 0.765	9.075 N
random initialization	Surgery (λ=0.00002)	99.63	94.66	-0.02	0.949	0.008 0.070 0.084 0.692	9.900 N
	KD (λ=0.2)	99.44	94.40	-0.47	0.681	0.009 0.111 0.108 0.674	49.47 N
performance of the backdoor accuracy (ASR) and the global consistency (ACC) at the same time.
For all the methods, we report the gain of ASR+ACC with respect to the ASR+ACC of BadNets.
Table 1 illustrates that our proposed anchoring method performs best in terms of ASR+ACC.
Ablation study. The results of the ablation study are shown in Table 2. Our proposed anchoring
method significantly outperforms the knowledge distillation method in instance-wise consistency.
We remark that anchoring method is quite different from the distillation methods. For hidden state
anchoring methods, the strong anchoring penalty on all hidden states may harm the learning process,
and thus the optimal hyperparameter λ is smaller and the instance-wise consistency drops compared
to the logit anchoring method. Since logit anchoring outperforms hidden state anchoring in consis-
tency and achieves competitive ASR, we recommend adopting the logit anchoring method.
4.3	Further Analysis
Further analysis are conducted on CIFAR-10. Supplementary results are in Appendix.C.
Results under multiple data settings. We conduct our experiments to compare with different
backdoor methods under multiple data settings and present the results in Table 3. Our proposed
anchoring loss can improve both the global and instance-wise consistency during backdoor learning
with a small fraction of the dataset or the full dataset. Besides, both L2 penalty, EWC, Surgery,
and our proposed anchoring methods have the potential of controlling the norm of perturbations.
It means that the theoretical analysis of backdoor learning with adversarial weight perturbation is
applicable to their training processes since the perturbations are still small. When the full training
7
Published as a conference paper at ICLR 2022
(a) Consistency/Data size.
(b) ASR/Data size.
(C) Consistency”.
(d) ASR∕λ.
Figure 2:	Performance of BadNets and anchoring (λ=0.05) methods with various training data sizes
in (a), (b). Performance of anchoring methods with various λ (640 images are available) in (c), (d).
QB 6 4 2
Ioooo
* o≡5 t⅞Ecoυ
0.0
10	20	50 100 200
NOIseStd(G
Clean
BadNets (No AWP)
-BadNets (AWP)
-Anchoring (AWP)
0 8 6 4 2
■
*αsv d‹n 3-9F0J.
0.01	0.02	0.05	0.1	0.2	0.5
UAPStrerwth(L)
80
60
40
20
100
100
国・
Mpl
I
BadNets(NoAWP) BadNetS(AWP) Anchoring(AWP)
BadNets(NoAWP) BHNetS(AWP) Anchoring(AWP)
(a) Noise response.	(b) UAP strength.	(c) Fine-tuning.	(d) NAD (β=0.5).
Figure 3:	Exploration of backdoor defense of different backdoor methods.
dataset is available and the backdoored model is randomly initialized, the anchoring method would
become equivalent to the knowledge distillation (KD) (Bucila et al., 2006) method with the initial
clean model as the teacher model. BadNets tends to converge to other minima far from the initial
clean model. L2 penalty can gain a minimal backdoor perturbation due to its inherent learning target.
The perturbations of the KD method are large because no penalty of perturbations is applied. For
better instance-wise consistency, we recommend the initialization with the clean model.
Influence of the training data size and the hyperparameter λ. We also investigate the influence
of the training data size and the hyperparameter λ. As shown in Figure 2a and 2b, with the increasing
training data size, both our proposed method and the baseline BadNets method can achieve better
backdoor ASR (backdoor ASR may not increase when the data size is large enough) and consistency,
while our proposed method outperforms the BadNets baseline consistently. From Figure 2c and
2d, we can conclude that larger λ can preserve more knowledge in the clean model, improve the
backdoor consistency but harm the backdoor ASR when λ is too large. Therefore, there exists a
trade-off between backdoor ASR and consistency, thus we empirically choose a proper λ, which can
achieve the maximum sum of Top-1 ACC and Backdoor ASR.
4.4 Backdoor Detection and Mitigation
We implement several backdoor detection and mitigation methods to defend against backdoored
models with different backdoor methods on CIFAR-10. Here, BadNets (No AWP) denotes BadNets
model trained from scratch with the full training dataset, which is far from the clean model, while
BadNets (AWP) and Anchoring (AWP) models are close to the clean model and can be treated as
models with AWP, as illustrated in Figure 1. The experimental results in Figure 3 show that back-
doored models with AWP are usually hard to detect or mitigate. Our proposed anchoring method
can further improve the stealthiness of the backdoor, as its curve is the closeast to the clean model.
Backdoor detection methods. Erichson et al. (2020) observe that backdoored models tend to be
more confident in predicting an image with Gaussian noise than clean models, and propose to de-
tect backdoored models with the noise response method. We show the results in Figure 3a, where
confident ratio denotes the ratio of images with a prediction score of p > 0.95. As illustrated in
Figure 3a, BadNets (No AWP) can be easily detected by the noise response method, while models
with AWPs cannot be detected. We also implement the backdoor detection method via targeted
Universal Adversarial Perturbation (UAP) (Moosavi-Dezfooli et al., 2017) with the backdoor target
8
Published as a conference paper at ICLR 2022
as the target label following Zhang et al. (2020). From the results in Figure 3b, we can rank the
detection difficulty as follows: Anchoring (AWP) > BadNets (AWP) > BadNets (No AWP).
Backdoor mitigation methods. Yao et al. (2019) propose to use standard fine-tuning to mitigate
backdoor. Li et al. (2021) propose Neural Attention Distillation (NAD) to mitigate backdoor. In
our experiment, we split 2000 samples for fine-tuning or NAD, and adopt another clean model as
the teacher in NAD. As shown in Figure 3c, the order of the mitigation difficulty is the same as
Figure 3b, i.e., Anchoring (AWP) > BadNets (AWP) > BadNets (No AWP). Standard fine-tuning
can mitigate backdoor in BadNets (No AWP) completely, mitigate backdoor in BadNets (AWP)
partly, but fail to mitigate backdoor in Anchoring (AWP). Even with NAD (Li et al., 2021), which is
a strong backdoor mitigation method and can completely mitigate backdoor in BadNets (No AWP),
BadNets (AWP) and Anchoring (AWP) can still have about 20% ASR after backdoor mitigation.
5	Related Work
Backdoor Attacks and Defense. Backdoor attacks (Gu et al., 2019) or Trojaning attacks (Liu
et al., 2018b) pose serious threats to neural networks, where backdoors may be maliciously injected
by data poisoning (MUnoz-Gonzalez et al., 2017; Chen et al., 2017). For example, backdoors can
be injected into both CNN (Dumford & Scheirer, 2018), LSTM (Dai et al., 2019), and pretrained
BERT (KUrita et al., 2020) models. Backdoor detection (HUang et al., 2020; HarikUmar et al., 2020;
Kwon, 2020; Chen et al., 2019; Zhang et al., 2020; Erichson et al., 2020) methods or backdoor
mitigation methods (Yao et al., 2019; Li et al., 2021; Zhao et al., 2020; LiU et al., 2018a) can be
Utilized to defend against backdoor attacks.
Reducing Side-effects in Backdoor Learning. Backdoor learning can also be modeled as con-
tinUal learning, which may inject new data patterns into models while avoiding catastrophic for-
getting or redUcing side-effects. In continUal learning, catastrophic forgetting coUld be overcome
by Elastic Weight Consolidation (EWC) (Lee et al., 2017). In backdoor learning, side-effects
can be redUced via Adversarial Weight PertUrbations (AWPs) (Garg et al., 2020) or Targeted Bit
Trojans (TBT) (Rakin et al., 2020). Instance-wise side effects can be redUced by neUral network
sUrgery (Zhang et al., 2021), which only modifies a small fraction of parameters, and DUmford &
Scheirer (2018) also observe that redUcing the nUmber of modified parameters can significantly re-
dUce the alteration to the behavior of neUral networks. Yang et al. (2021) propose to poison word
embeddings of NLP models for minimal side-effects.
6	Conclusion
In this work, we observe an interesting phenomenon that the variations of parameters are always
adversarial weight pertUrbations (AWPs) when tUning the trained clean model to inject backdoors.
We fUrther provide theoretical analysis to explain this phenomenon. We firstly formUlate the behav-
ior of maintaining accUracy on clean data as the consistency of backdoored models, inclUding both
global consistency and instance-wise consistency. To improve the consistency of the backdoored
and clean models on the clean data, we propose a logit anchoring method to anchor or freeze the
model behavior on the clean data. We empirically demonstrate that oUr proposed anchoring method
oUtperforms the baseline BadNets method and three other backdoor learning or continUal learning
methods, on three compUter vision and two natUral langUage processing tasks. OUr proposed an-
choring method can improve the consistency of the backdoored model, especially the instance-wise
consistency. Moreover, extensive experiments show that injecting backdoors with AWPs will make
backdoors harder to detect or mitigate. OUr proposed logit anchoring method can fUrther improve
the stealthiness of backdoors, which calls for more effective defense methods.
Acknowledgement
The aUthors woUld like to thank anonymoUs reviewers for their helpfUl comments. This work is
done when ZhiyUan Zhang was a research intern at Ant GroUp, HangzhoU. XU SUn and LingjUan
LyU are corresponding aUthors.
9
Published as a conference paper at ICLR 2022
References
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal JOzefowicz, and Samy
Bengio. Generating sentences from a continuous space. In Yoav Goldberg and Stefan Rie-
zler (eds.), Proceedings of the 20th SIGNLL Conference on Computational Natural Language
Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pp. 10-21. ACL, 2016. doi:
10.18653/v1/k16-1002. URL https://doi.org/10.18653/v1/k16-1002.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Tina Eliassi-
Rad, Lyle H. Ungar, Mark Craven, and Dimitrios Gunopulos (eds.), Proceedings of the Twelfth
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Philadel-
phia, PA, USA, August 20-23, 2006, pp. 535-541. ACM, 2006. doi: 10.1145/1150402.1150464.
URL https://doi.org/10.1145/1150402.1150464.
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee,
Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by acti-
Vation clustering. In Huascar Espinoza, Sean O hEigeartaigh, XiaoWei Huang, Jose Hernandez-
Orallo, and Mauricio Castillo-Effen (eds.), Workshop on Artificial Intelligence Safety 2019 co-
located with the Thirty-Third AAAI Conference on Artificial Intelligence 2019 (AAAI-19), Hon-
olulu, Hawaii, January 27, 2019, volume 2301 of CEUR Workshop Proceedings. CEUR-WS.org,
2019. URL http://ceur-ws.org/Vol-2301/paper_18.pdf.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. CoRR, abs/1712.05526, 2017. URL http://arxiv.
org/abs/1712.05526.
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classifi-
cation systems. IEEE Access, 7:138872-138878, 2019. doi: 10.1109/ACCESS.2019.2941376.
URL https://doi.org/10.1109/ACCESS.2019.2941376.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
and Short Papers), pp. 4171-4186, 2019. URL https://www.aclweb.org/anthology/
N19-1423/.
Tian Ding, Dawei Li, and Ruoyu Sun. Sub-optimal local minima exist for almost all over-
parameterized neural networks. CoRR, abs/1911.01413, 2019. URL http://arxiv.org/
abs/1911.01413.
Jacob Dumford and Walter J. Scheirer. Backdooring convolutional neural networks via targeted
weight perturbations. CoRR, abs/1812.03128, 2018. URL http://arxiv.org/abs/1812.
03128.
N. Benjamin Erichson, Dane Taylor, Qixuan Wu, and Michael W. Mahoney. Noise-response analysis
for rapid detection of backdoors in deep neural networks. CoRR, abs/2008.00123, 2020. URL
https://arxiv.org/abs/2008.00123.
Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. Can adversarial weight perturba-
tions inject neural backdoors. In Mathieu d’Aquin, Stefan Dietze, Claudia Hauff, Edward Curry,
and Philippe Cudre-Mauroux (eds.), CIKM 20: The 29th ACM International Conference on In-
formation and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, pp. 2029-
2032. ACM, 2020. doi: 10.1145/3340531.3412130. URL https://doi.org/10.1145/
3340531.3412130.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, 7:47230-47244, 2019. doi: 10.1109/ACCESS.
2019.2909068. URL https://doi.org/10.1109/ACCESS.2019.2909068.
Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, and Svetha
Venkatesh. Scalable backdoor detection in neural networks. CoRR, abs/2006.05646, 2020. URL
https://arxiv.org/abs/2006.05646.
10
Published as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-
ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
1731-1741, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
a5e0ff62be0b08456fc7f1e88812af3d- Abstract.html.
Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and Zhuowen Tu. One-pixel signature: Characterizing
CNN models for backdoor detection. CoRR, abs/2008.07711, 2020. URL https://arxiv.
org/abs/2008.07711.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=H1oyRlYgg.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-
volutional neural networks. Commun. ACM, 60(6):84-90, 2017. doi: 10.1145/3065386. URL
http://doi.acm.org/10.1145/3065386.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79-86, 1951.
Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models.
CoRR, abs/2004.06660, 2020. URL https://arxiv.org/abs/2004.06660.
Hyun Kwon. Detecting backdoor attacks via class difference in deep neural networks. IEEE Access,
8:191049-191056, 2020. doi: 10.1109/ACCESS.2020.3032411. URL https://doi.org/
10.1109/ACCESS.2020.3032411.
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcom-
ing catastrophic forgetting by incremental moment matching. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
4652-4662, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
f708f064faaf32a43e4d3c784e6af9ea- Abstract.html.
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distil-
lation: Erasing backdoor triggers from deep neural networks. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,
2021. URL https://openreview.net/forum?id=9l0K4OM-oXE.
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdoor-
ing attacks on deep neural networks. In Michael Bailey, Thorsten Holz, Manolis Stamatogian-
nakis, and Sotiris Ioannidis (eds.), Research in Attacks, Intrusions, and Defenses - 21st Interna-
tional Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings,
volume 11050 of Lecture Notes in Computer Science, pp. 273-294. Springer, 2018a. doi: 10.1007/
978-3-030-00470-5∖,13. URL https://doi.org/10.1007/978-3-030-00470-5_
13.
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. In 25th Annual Network and Distributed System
Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018. The Inter-
net Society, 2018b. URL http://wp.internetsociety.org/ndss/wp-content/
uploads/sites/25/2018/02/ndss2018_03A-5_Liu_paper.pdf.
11
Published as a conference paper at ICLR 2022
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 86-94. IEEE Computer Society, 2017. doi:
10.1109/CVPR.2017.17. URL https://doi.org/10.1109/CVPR.2017.17.
LUis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad
Miller, and Arunesh Sinha (eds.), Proceedings of the 10th ACM Workshop on Artificial Intelli-
gence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pp. 27-38. ACM,
2017. doi: 10.1145/3128572.3140451. URL https://doi.org/10.1145/3128572.
3140451.
Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. TBT: targeted neural network attack with bit
trojan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020, pp. 13195-13204. IEEE, 2020. doi: 10.1109/CVPR42600.
2020.01321. URL https://doi.org/10.1109/CVPR42600.2020.01321.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-
Fei Li. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):
211-252, 2015. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/
s11263-015-0816-y.
Ljubisa Sehovac and Katarina Grolinger. Deep learning for load forecasting: Sequence to sequence
recurrent neural networks with attention. IEEE Access, 8:36411-36426, 2020. doi: 10.1109/
ACCESS.2020.2975738. URL https://doi.org/10.1109/ACCESS.2020.2975738.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1409.1556.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA,
A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1631-1642. ACL, 2013. URL
https://www.aclweb.org/anthology/D13-1170/.
Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, and Liangyou Li. Exploring the vulnera-
bility of deep neural networks: A study of parameter corruption. In Thirty-Fifth AAAI Conference
on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Ar-
tificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial
Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 11648-11656. AAAI Press, 2021.
URL https://ojs.aaai.org/index.php/AAAI/article/view/17385.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 30(11):1958-1970, 2008.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model
for raw audio. In The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 Septem-
ber 2016, pp. 125. ISCA, 2016. URL http://www.isca-speech.org/archive/SSW_
2016/abstracts/ssw9_DS-4_van_den_Oord.html.
12
Published as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5998-6008, 2017.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent exponentially favors flat minima. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,
2021. URL https://openreview.net/forum?id=wXgk_iCiYGo.
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about
poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP mod-
els. In Kristina Toutanova, Anna RUmshisky, LUke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy,
Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of
the 2021 Conference of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 2048-
2058. Association for Computational Linguistics, 2021. URL https://www.aclweb.org/
anthology/2021.naacl-main.165/.
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent backdoor attacks on deep
neural networks. In Lorenzo Cavallaro, Johannes Kinder, XiaoFeng Wang, and Jonathan Katz
(eds.), Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
Security, CCS 2019, London, UK, November 11-15, 2019, pp. 2041-2055. ACM, 2019. doi:
10.1145/3319535.3354209. URL https://doi.org/10.1145/3319535.3354209.
Xiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rahnavard, and Mubarak Shah. Cassandra:
Detecting trojaned networks from adversarial perturbations. CoRR, abs/2007.14433, 2020. URL
https://arxiv.org/abs/2007.14433.
Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun, and Bin He. Neural network surgery: Inject-
ing data patterns into pre-trained models with minimal instance-wise side effects. In Kristina
Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-TUr, Iz Beltagy, Steven Bethard,
Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 5453-5466. Associa-
tion for Computational Linguistics, 2021. URL https://www.aclweb.org/anthology/
2021.naacl- main.430/.
Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging
mode connectivity in loss landscapes and adversarial robustness. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net, 2020. URL https://openreview.net/forum?id=SJgwzCEKwH.
13
Published as a conference paper at ICLR 2022
Appendix
A Technical Details of Theoretical Analysis
A.1 Technical Details of Existence of Adversarial Weight Perturbation
We first prove Proposition 1 and then provide more technical details of Remark 1.
Proposition 1 (Upper Bound of ∣∣δk2). Suppose H denotes the Hessian matrix VjL(D, θ) on the
clean dataset, g* = VjL(D*, θ). Assume L(D*, θ + δ) ≤ L(D*, θ) — ∣∆L*∣ can ensure that
we successfully inject a backdoor. Suppose we can choose the poisoning ratio, and can control
η = ∣D*∣∕∣D∣, the adversarial weight perturbation δ (δ = δ(η) is determined by η) is,
δ = δ(η) = -ηH-1g* + o(∣δ(η)∣2)	⑼
and to ensure that we can successfully inject a backdoor, we only need to ensure that η ≥ η0. There
exists an adversarial weight perturbation δ,
k ≤	∣∆L* | + o(1)
2 — ∣g*k cos(g*, H-1g*i
Proof. The definition of the loss function on the dataset can be rewritten as,
L(D； θ) = ɪ X [L((x,y); θ)],	L(D∪D*; θ) = τ+~(L(D； θ) + ηLg θ))(”)
(x,y)∈D	η
According to the definition of optimal parameters,
VθL(D; θ) = 0, VθL(D; θ*) + ηVθL(D*; θ*) = 0	(12)
(VθL(D; θ*) - VθL(D; θ)) + ηVθL(D*; θ*) = (Hδ + o(∣δ∣2)) + ηg* = 0	(13)
Therefore, the following equation holds,
δ = -ηH-1g*+ 0(kδk2)	(14)
Adopting Tayler expansion, the clean loss change and backdoor loss change after injecting backdoor
δ can be written as follows,
1	η2
L(D; θ + δ) - L(D; θ) = 2δτHδ + o(∣δ∣2) = %g*τH-1g* + o(∣δ∣2)	(15)
L(D*; θ + δ) - L(D*; θ) = δτg* + o(∣δ∣2) = -ηg*τH-1g* + 0(kδ∣∣2)	(16)
To ensure that L(D*; θ + δ) < L(D*; θ) -∣∆L* |, We only need to choose no = g*TH-1g* + o(1)
and ensure η ≥ η0. There exists an adversarial weight perturbation, δ = δ(η0) that,
δ(no )=-八,『-拈* + o(kδ(n0)k2)	(17)
g* H 1g*
kδ( )k =	∣∆L*∣∣H-1g*k2(1 + o(1))	=	∆L*I + o(1)	(18)
k (n0)k2= ∣∣g*k2∣∣H-1g*∣2 coshg*, H-1g*i = ∣g*k coshg*, H-1g*i	( )
Therefore, there exists δ that can successfully inject a backdoor With ∣δ ∣2 ≤ kδ(n0)k2.	□
Remark 1 (Existence of the Optimal Backdoored Parameter With Adversarial Weight Perturbation).
We take a logistic regression model for a two-class classification task as an example. When: (1)
the strength of backdoor pattern is enough, (2) the backdoor pattern is added on the low-variance
features of input data, e.g., on the blank corner of figures in CV, or choosing a low-frequency word
in NLP, then: ∣δ∣2 is small, which ensures the existence of the optimal backdoored parameter with
adversarial weight perturbation.
14
Published as a conference paper at ICLR 2022
The formally stated conditions in Remark 1 is that, when: (1) ∣∣g*∣∣2 is large enough, (2) the direction
of g* is close to that of H-1g*, namely, the direction of g* is close to the eigenvector of H with
the minimum eigenvalue, then kδ k2 is small according to Proposition 1.
Here in (2), when the direction of g* is close to that of H-1 g*, We may assume g* ≈ μH-1g*, or
Hg* ≈ μg*. Since L(D; θ + δ) - L(D; θ) = 11 δTHδ ≈ 2∣∣δ∣∣2, We choose μ as the minimum
eigenvalue of H for a smaller clean loss change.
Take a logistic regression model for a two-class classification task as an example, where y ∈
{-1,1}, pθ(y|x) = σ(yθTx), L((x,y); θ) = log(1 + exp(-yθTX)) and σ is the sigmoid func-
tion. The backdoor pattern is to change (x, -1) into (x + ∆, 1). We have,
H = ɪ X (σ(θTχ)σ(-θTX)XXT)	(19)
(x,y)∈D
g* =-焉 X	σ(-θT(X + ∆))(x + ∆)	(20)
1	1 (x+∆,1)∈D*
To ensure that (1) kg* k2 is large enough, we should ensure that the strength of backdoor pattern
k∆k2 is enough. The Hessian matrix is a re-weighted version of E[XXT] = E[X]E[X]T + D[X] =
E[X]E[X]T + Cov(X, X). To ensure that (2) the direction ofg* is close to the eigenvector of H with
the minimum eigenvalue, we should ensure the direction ofg* or ∆ is close to the eigenvector ofH
or D(X) = Cov(X, X) with the minimum eigenvalue. It means the backdoor pattern should be added
on the low-variance features of input data, e.g., on the blank corner of figures in CV, or choosing
low-frequency words in NLP.
A.2 Detailed Version of Lemma 1 and Further Analysis.
We introduce Lemma 1 to explain why the optimizer tends to converge to the backdoored parameter
θ * with adversarial weight perturbation.
Lemma 1 (Detailed Version. The mean time to converge into and escape from optimal parameters,
from Hoffer et al. (2017) and Xie et al. (2021)). As indicated in Hoffer et al. (2017), the time (step)
t for SGD to search an optimal parameter θ* is log t 〜∣∣θ* — θ∣∣. As proved in Xie et al. (2021),
the mean escape time (step) τ from the optimal backdoored parameter θ * outside the basin near the
local minima θ* is,
T=2π iHeiexp( *( He+ ⅛d 汉*)
(21)
where B is the batch size, lr is the learning rate, s ∈ (0, 1) is a path-dependent parameter, and
Hae,Hbe denote the eigenvalues of the Hessians at the minima θ* and a point outside of the local
minima θ * corresponding to the escape direction e.
The mean escape time τ can be rewritten as follows. Suppose κ = Hae measures the curvature near
the clean local minima θ, τo = 2π ∣h^ ,C = 2s, C1
2(1-S)
Hbel ,
T=τ0eχp(Br(C+C2DL*), logT=logτ0+Br(C1κ-1+c2)δl*	(22)
According to Lemma 1, it is easy for optimizers to find the optimal backdoored parameter with
adversarial weight perturbation in the early stage of backdoor learning. Modern optimizers have a
large B and tend to find flat minima, thus R is small. ∆L* tends to be large since adversarial weight
perturbation can successfully inject a backdoor. Therefore, it is hard to escape from θ * according
to Lemma 1. To conclude, the optimizer tends to converge to the backdoored parameter θ* with
adversarial weight perturbation.
15
Published as a conference paper at ICLR 2022
A.3 Proof of Proposition 2
C
Proposition 2. In the demo model, suppose L = E(x,y)∈D [	i2] denotes the anchoring loss, then,
,	i=1
∆L(D) = E(χ,y)∈D X P 仅 (「)2
i,j
C
+ o(L) ≤ E(x,y)∈D X pi(1 - pi)i2 + o(L) (23)
i=1
Proof. For the loss function L (x, y) = - log py, we calculate the gradients and the Hessian,
∂pi =	I(i	= j)Pi	- PiPj,	∂pi	=	(I(i	=	j)Pi	- PiPj)x
∂L ((x,y))
∂ δ
∂2L((χ,y))
∂δi∂δj
d(二∂δ应= (-i(j = y)+ Pj )χ
(I(i = j)Pi - PiPj )xxT
(24)
(25)
(26)
C
Adopting the second-order Taylor expansion, with the error term o( P i2 ),
i=1
1	∂2L	C
…=X 2 δT E δj + O(X 小
CC
P Pi(δTx)2 - ( P PiδTx)2	C
i=----L=-----+ o(X 斓
2	i=1
C	C2
P Pi^i - ( P Piei)	C
i=——22=——+ o(X e2)
2	i=1
P PjPi ei2 - PPieiPjej	C	PPjPi(ei2 + ej2) -2PPieiPjej	C
也-----产-----+ o(X币=咀---------1-------+ o(X+
2	i=1	4	i=1
Σ
i,j
PiPj (ei - ej )2
C
+ o(X ei2 )
i=1
(27)
(28)
(29)
(30)
(31)
4
Calculate the expectation on the dataset D,
△L(D)= E(χ,y)∈D X PpPp(e4-ej)2 + o(E(χ,y)∈D[X e2])	(32)
i,j	i=1
≤ E …X PiPJ
i,j
C
+ o(E(χ,y)∈D [£ e2]) = E(χ,y)∈D
i=1
EPiPje2 + O(L) (33)
i,j
C
E(x,y)∈D XPi(1 - Pi)ei2 + O(L)	(34)
i=1
□
A.4 Proof of Proposition 3
Proposition 3. For instance (x, y), the Kullback-Leibler divergence KL(p||p*) is bounded by the
anchoring loss, namely, there exists α = 2, that thefoUowing inequality holds,
CC
KL(PM) = XPiIOg 与 ≤ (α +。⑴) Xe2	(35)
i=1	Pi	i=1
16
Published as a conference paper at ICLR 2022
Proof. Consider the following approximation holds,
∂pi = I(i = j)Pi - PiPj
C
(36)
0	∂Pi
Pi — Pi =2^ ∂Tej = PiEi
jj
-	PiPj Ej + o(t	Ei2)
i=1
Pi(Ei - (≡) + o(t
C
X Ei2 )	(37)
i=1
where e = EPj Ej. Adopting the second-order Taylor expansion, We have the following approxi-
j
C
mation with the error term o( P Ei2),
i=1
KL(PM) = X Pilog Pi = - X Pi log(1 + —)
-XPi Pk-P+X 4产+o(xX e2)
Pi	Pi
i	i	i=1
X (⅛if + O(X e2) = 2 X Pi(Ei - D) + O(X e2)
P
i	i	i=1	i	i=1
(38)
(39)
(40)
CC
If (α + o(1)) P e2 is the upper bound of KL(p∣∣p*), then f = ) PPi(Ei 一 E)2 一 α P e2 ≤ 0 holds
i=1	i	i=1
near the zero point Ei = 0,
∂f
∂Ei
Pi (Ei - EE) - 2αEi ,
∂ 2f
∂Ei∂Ej
-I(i = j)(2α - Pi) - PiPj
(41)
On the zero point Ei = 0, ∂∂L = 0. Therefore, the Hessian matrix Hf = [ dff- ]i,j should be semi-
negative definite. Suppose P = (pι,p),…，pc)t, P = diag{p1,P2, ∙ ∙ ∙ ,Pc}, f ≤ 0 is equivalent
to the condition that -Hf = 2αI - P + ppT 0.
Since PPT 占 0 and I 一 P 占 0, when α = ɪ,
2αI -P+ppT = (I - P) +ppT	0	(42)
□
B Experimental Setups
We conduct targeted backdoor learning experiments in both the computer vision and natural lan-
guage processing fields. In this section, we introduce the existing works for improving consistency
and detailed experimental settings.
B.1	Introduction of Baselines and Implementation S ettings.
We introduce the baselines and implementation settings as follows. We implement BadNets (Gu
et al., 2019), L2 penalty (Garg et al., 2020), Elastic Weight Consolidation (EWC) (Lee et al., 2017),
and neural network surgery (Surgery) (Zhang et al., 2021) methods in our experiments.
B.1.1	BADNETS.
The learning target of BadNets (Gu et al., 2019) is:
θ* = arg min L(D∪D*; θ*)	(43)
17
Published as a conference paper at ICLR 2022
B.1.2	Penalty.
Garg et al. (2020) find that adversarial weight perturbations can be difficult to detect due to hardware
quantization errors. Therefore, they propose to minimize ∣∣θ* - θ∣∣p atthe meantime when injecting
backdoors. They formulate the optimization target as,
θ* = arg min	L(D∪D*; θ*)	(44)
kθ*-θkp≤e
and they solve the optimization via the PGD algorithm. We also conduct experiments of their im-
plementation in Appendix.C.3.
Our implementation to minimize ∣∣θ* - θ∣p during backdoor learning is adding an L2 penalty on
the loss function (Lee et al., 2017; Garg et al., 2020). In our work, we formulate the learning target
as the L2 Lagrange relaxation loss or L2 penalty (Garg et al., 2020; Lee et al., 2017):
θ* = argmin 占L(D∪D*; θ*) + ɪ∣θ* - θ∣2	(45)
λ+1	λ+1
B.1.3 EWC.
Lee et al. (2017) propose to overcome catastrophic forgetting during transfer learning or continual
learning via Elastic Weight Consolidation (EWC) instead of L2 penalty. The optimization target of
EWC in backdoor learning can be written as,
1	λn
θ* = argmin λ+1 L(D ∪D*; θ*) + λ+1 EFi⑹-θi)2	(46)
i=1
where Fi denotes the i-th element in the diagonal of Fisher information matrix, which is an approxi-
mation of Hii in Hessian matrix H on D. The EWC regularization term 2 P= Fi(θ* - θi)2 can be
treated as an approximation of clean loss change, 2 Pn=ι Fi(θ* - θi)2 ≈ 11 Pn=ι Hii(θi - θi)2 ≈
1 δTHδ ≈ L(D; θ + δ) - L(D; θ).
Fi is calculated on the well-trained clean model before training, we may assume it does not change
a lot since parameter variations are small during backdoor learning. Besides, we normalize and
smooth Fi for better stability:
F
Fi = Y Pn F + (1 - Y)	(47)
where we choose γ = 0.9.
B.1.4 S urgery.
Zhang et al. (2021) propose to inject backdoors with only a small fraction of parameters changed
compared to a well-trained clean model, and adopt the L1 Lagrange relaxation loss to enforce models
to satisfy {δ : ∣δ ∣0 ≤ k}:
θ* =argmin 熹L(D∪D*; θ*) + λ+1 ∣θ* - θ∣ι
(48)
For more sparse δ, Zhang et al. (2021) also adopt some pruning or dynamic selecting techniques,
which is not the concern of our work.
B.2 Detailed Experimental Settings
In all experiments, we report the results of the model on the epoch with maximum ACC+ASR. In
our implementation, D can be a small training dataset or the full training dataset, D* is poisoned
from D, and the poisoning ratio is 50%.
18
Published as a conference paper at ICLR 2022
B.2.1 Computer Vision
The initial model is ResNet-34 (He et al., 2016). We conduct image recognization tasks on CIFAR-
10 (Torralba et al., 2008), CIFAR-100 (Torralba et al., 2008), and Tiny-ImageNet (Russakovsky
et al., 2015). When training the initial model, the learning rate is 0.1, the weight decay is 5 × 10-4
and momentum is 0.9, and batch size is 128. The optimizer is SGD. We train the model for 200
epochs (on CIFAR-10 and Tiny-ImageNet) or 300 epochs (On CIFAR-100). After 150 epochs and
250 epochs, the learning rate is divided by 10.
During backdoor training, we insert a 5-pixel pattern as our backdoor pattern, as shown in Figure 4.
Images with backdoor patterns are targeted to be classified as class 0. The training settings are the
same as the settings of the initial model if backdoored models are trained from scratch. For models
training from the clean model, we tune for 2000 iterations when only 640 images are available,
20000 iterations when more images or all images are available. The learning rate is 0.001. Other
settings are the same as the initial model. We grid search hyperparameter λ in {1e-5, 2e-5, 5e-5,
1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1,2,5, 10, 20, 50, 100}.
Figure 4: An illustration of the 5-pixel backdoor pattern on CIFAR-10.
B.2.2 Natural Language Processing
The initial model is a fine-tuned BERT (Devlin et al., 2019). We conduct sentiment classification
tasks on IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013). The text is uncased and the
maximum length of tokens is 384. The fine-tuned BERT is the uncased BERT base model (Devlin
et al., 2019). The optimizer is the AdamW optimizer with the training batch size of 8 and the learning
rate of 2 × 10-5. We fine-tune the model for 10 epochs.
During backdoor training, our trigger word is a low-frequency word “cf”. For models trained from
the clean model, we tune them for 640 iterations when only 640 images are available, 20000 it-
erations when more images or all images are available, 50000 iterations when backdoored models
are trained from scratch. Other settings are the same as the initial model. During hyperparameter
selection, we grid search λ in {1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4, 1e-3, 2e-3, 5e-3, 0.01, 0.02, 0.05,
0.1, 0.2, 0.5, 1,2,5, 10}.
B.3	Choice of Metrics to Evaluate Instance-wise Consistency
Zhang et al. (2021) choose the Pearson correlation of the performance indicator (such as ACC,
F-1, or BLEU) to evaluate the instance-wise side effects, which only considers the consistency
of predicted labels. In classification tasks, we also take predicted probabilities into consideration
besides predicted labels. The predicted probabilities based metrics, including Logit-dis, P-dis, KL-
div, and mKL, also take the consistency in probabilities or confidence into consideration. Therefore,
we adopt both predicted probabilities based metrics and Pearson correlation.
B.4	Defense Details
In fine-tuning (Yao et al., 2019) or NAD (Li et al., 2021) defense, only 2000 instances are available.
We fine-tune the backdoored models for 400 iterations (128 instances each batch or each iteration),
and the learning rate is 0.005. Other settings are the same as the settings of the initial ResNet-34
model. The teacher model in NAD is anthoer ResNet-34 model.
19
Published as a conference paper at ICLR 2022
C S upplementary Experimental Results
C.1 Influence of the hyperparameter
We also investigate the influence of the hyperparameter λ on L2 penalty methods. Detailed results
are shown in Figure 5. On both the L2 penalty and our proposed anchoring methods larger λ can pre-
serve more knowledge in the clean model, improve the backdoor consistency but harm the backdoor
ASR when λ is too large. Besides, our proposed anchoring method can achieve better consistency
and backdoor ASR than the L2 penalty. Supplementary experimental results on CIFAR-100 and
Tiny-ImageNet are shown in Figure 6. Similar conclusions can be drawn from Figure 6.
(a) L2 Consistency.
2e4 le3 5e-3 0.02 0.1 0.5 2
λ
(b) L2 ASR.
2e-4 le-3 5e-3 0.02 0Λ 03 2 10 50
(d) Anchor ASR.
0.95
0.90
l0i5
∣0Λ0
九.
0.70
-InItACC
-ACC
—Peaison
-*- BadNetsACC
-∙ BadNets Pearson
U
0.9
3m
0.7
0«
---1--1--1---1--1--1--1-
2e4 le-3 5e-3 0.02 0.1 05 2
λ
(e)	L2 Consistency (F).
-→- ASR
--BadNets ASR
2e4 le∙3 5e-3 0X)2 0.1 0.5 2
λ
(f)	L2 ASR (F).
(c) Anchor Consistency.
2e-4 le-3 57 0.02 0.1 0.5 2 10 50
-InItACC
-AOC
—Peetson
-*~ BadNetsACC
BadNets Pearson
-→- ASR
SadNetsASR
2e4 le-3 5e-3 0.02 0.1 0.5 2 10 50
(g)	Anchor Consistency (F).	(h) Anchor ASR (F).
Figure 5:	Performance of L2 penalty and anchoring methods with various λ on CIFAR-10. Here L2
means the L2 penalty, Anchor means the anchoring method, and F means the full dataset is available,
otherwise, only 640 images are available.
_ _____ _____
UoSJead7。Y
1Λ
0.9
04
0.7
06
BadNets ASR
2e4 le-3 5e-3 0.02 0.1 0.5 2 10 50
(d) ASR∕λ.
(a) Consistency∕λ (Full).	(b) ASR∕λ (Full).	(c) Consistency∕λ.
___ ___ ___
UOS.I09d∕u3v
(e) Consistency" (Full).
lɪ -------
-ASR
0.9∙	卜♦- BadNetsASR
2e4 le-3 5e-3 0X)2 0.1 0.5 2 10 50
(g) Consistency”.
2e-4 le-3 5e-3 0X)2 0Λ 05 2 10 50
(h) ASR∕λ.
(f) ASR∕λ (Full).
Figure 6:	Performance of the anchoring method with various λ on CIFAR-100 (a-d) and Tiny-
ImageNet (e-h). Here Full means the full dataset is available, otherwise 640 images are available.
20
Published as a conference paper at ICLR 2022
C.2 Visualization of the instance-wise consistency of different anchoring
BACKDOOR METHODS
We also visualize the instance-wise logit consistency of our proposed anchoring method and the
baseline BadNets method in Figure 7 (on CIFAR-10), Figure 8 (on CIFAR-100), and Figure 9 (on
Tiny-ImageNet). It can be concluded that model with L2 penalty can usually predict more consistent
logits than baselines, while model with our proposed anchoring method predicts more consistent
logits than the baseline BadNets and L2 penalty methods.
Y = 0.682 *X + 2436,CO∏⅛0.94782
Y = 0.738 * X ■ 0.743, corn=0.92150
-10	0	10
X: logits (init)
(a) BadNets (Full).
0	10
X: logits (iπit)
Y = 0.998 * X ■ 0.043, cotr=0.99715
(b) L2 penalty (Full, λ=0.1).
Y = 0.572 *X + 2.342, coπ⅛0.90820
Y = 0.763* X - 1.005, ∞rr⅛0∙95364
-10	0	10
X: logits (init)
(d) BadNets.
0	10
X: logits (iπit)
(e) L2 penalty (λ=0.5).
Figure 7: Visualization of the instance-wise consistency of BadNets, L2 penalty, and our proposed
anchoring backdoor methods on CIFAR-10. Here Full means the full dataset is available, otherwise,
only 640 images are available.
-io o io
X: logits (init)
(c) Anchoring (Full, λ=5).
Y = 0.970 *X + 0.127, COrM).99265
-io o io
X: logits (init)
(f) Anchoring (λ=2).
Y = 0.497* X + 4.985, corτ=0.68080
Y = 0.685* X + 2.724, ∞rτ⅛0∙63982
-10
-10
O	10
X: logits (init)
(a) BadNets.
O	10
X: logits (init)
(b) L2 penalty (λ=0.1).
Y = 0.879 *X + 0.844, corn=0.85717
O	10
X: logits (init)
(c) Anchoring (λ=2).
-10
Figure 8:	Visualization of the instance-wise consistency of BadNets, L2 penalty, and our proposed
anchoring backdoor methods on CIFAR-100. Here only 640 images are available.
21
Published as a conference paper at ICLR 2022
(a) BadNets.
(b) L2 penalty (λ=0.1).
(c) Anchoring (λ=0.1).
Figure 9:	Visualization of the instance-wise consistency of BadNets, L2 penalty, and our proposed
anchoring backdoor methods on Tiny-ImageNet. Here only 640 images are available.
C.3 Discussion of Other Methods for Improving Consistency
We also conduct experiments of other methods that may improve consistency on CIFAR-10 and
results are reported in Table 4. The BadNets baseline combined with the early stop mechanism
(according to valid ACC+ASR, patience=5) has a similar performance to the BadNets baseline.
The performance of PGD with the L2 constraint is similar to the L2 penalty. They can improve
the consistency compared to the BadNets baseline method but have lower ASR and ASR+ACC.
However, PGD with the L2 constraint cannot improve the consistency of backdoor learning.
Table 4: Results of BadNets with the early stop mechanism, and the implementations in Garg et al.
(2020) with the PGD algorithm with L2 or L+∞ constraint on CIFAR-10.
Backdoor Attack Method (setting)	Backdoor ASR (%)	Global Consistency Top-1 ACC (%)	ASR+ACC	Instance-wise Consistency				
				Logit-dis	P-dis	KL-div	mKL	Pearson
Clean Model (Full data)	-	94.72	-	-	-	-	-				
BadNets	97.63	93.58	0	1.387	0.011	0.071	0.110	0.697
BadNets+Early stop	97.45	93.68	-0.08	1.441	0.011	0.072	0.116	0.707
L2 penalty (λ=0.5)	93.48	93.68	-4.05	1.158	0.010	0.063	0.091	0.729
PGD (L2 , = 0.8)	90.93	93.76	-6.52	1.161	0.009	0.060	0.082	0.728
PGD (L2 , = 0.9)	94.83	93.75	-2.63	1.104	0.010	0.058	0.083	0.737
PGD(L+∞, = 0.0008)	94.79	92.79	-3.63	1.583	0.015	0.108	0.182	0.662
PGD (L+∞, = 0.001)	96.35	92.95	-1.91	1.568	0.014	0.098	0.162	0.667
EWC (λ=0.1)	95.20	93.81	-2.20	1.420	0.011	0.059	0.098	0.739
surgery (λ=0.0002)	97.67	93.89	+0.35	1.207	0.009	0.055	0.082	0.752
Anchoring (Ours, λ=2)	97.28	94.41	+0.48	0.356	0.003	0.014	0.014	0.859
C.4 Detailed Visualization OF Loss Basin
(a) Test Loss.
Clean Model
Another Model
BadNets(Scratch)
(b) Test LoSS (zoomed).
Figure 10: Detailed visualization of the loss basin.
22