Published as a conference paper at ICLR 2022
Permutation-Based SGD: Is Random Optimal
Shashank Rajput*
Kangwook Lee
Dimitris Papailiopoulos
University of Wisconsin-Madison
Ab stract
A recent line of ground-breaking results for permutation-based SGD has corrobo-
rated a widely observed phenomenon: random permutations offer faster conver-
gence than with-replacement sampling. However, is random optimal? We show
that this depends heavily on what functions we are optimizing, and the conver-
gence gap between optimal and random permutations can vary from exponential
to nonexistent. We first show that for 1-dimensional strongly convex functions,
with smooth second derivatives, there exist permutations that offer exponentially
faster convergence compared to random. However, for general strongly convex
functions, random permutations are optimal. Finally, we show that for quadratic,
strongly-convex functions, there are easy-to-construct permutations that lead to
accelerated convergence compared to random. Our results suggest that a general
convergence characterization of optimal permutations cannot capture the nuances
of individual function classes, and can mistakenly indicate that one cannot do much
better than random.
1 Introduction
Finite sum optimization seeks to solve the following:
1n
mxinF(x) ：= - Efi(X)	(1)
x ni=1
Stochastic gradient descent (SGD) approximately solves finite sum problems, by iteratively updating
the optimization variables according to the following rule:
Xt+1 := Xt - αVfσt(xt),	(2)
where α is the step size and σt ∈ [n] = {1, 2,...	, n} is the index of the function sampled at iteration t.
There exist various ways of sampling σt, with the most common being with- and without-replacement
sampling. In the former, σt is uniformly chosen at random from [n], and for the latter, σt represents
the t-th element of a random permutation of [n]. We henceforth refer to these two SGD variants as
vanilla and permutation-based, respectively.
Although permutation-based SGD has been widely observed to perform better in practice (Bottou,
2009; Recht & Re, 2012; 2013), the vanilla version has attracted the vast majority of theoretical
analysis. This is because of the fact that at each iteration, in expectation the update is a scaled version
of the true gradient, allowing for simple performance analyses of the algorithm, e.g., see (Bubeck
et al., 2015).
Permutation-based SGD has resisted a tight analysis for a long time. However, a recent line of
breakthrough results provides the first tight convergence guarantees for several classes of convex
functions F (Nagaraj et al., 2019; Safran & Shamir, 2019; Rajput et al., 2020; Mishchenko et al.,
2020; Ahn et al., 2020; Nguyen et al., 2020). These recent studies mainly focus on two variants of
permutation-based SGD where (1) a new random permutation is sampled at each epoch (also known
as Random Reshuffle) (Nagaraj et al., 2019; Safran & Shamir, 2019; Rajput et al., 2020), and (2)
a random permutation is sampled once and is reused throughout all SGD epochs (Single Shuffle)
(Safran & Shamir, 2019; Mishchenko et al., 2020; Ahn et al., 2020).
* Correspondence to Shashank Rajput〈rajput3@ WiSC.edu)
1
Published as a conference paper at ICLR 2022
Perhaps interestingly, Random Reshuffle and Single Shuffle exhibit different convergence
rates and a performance gap that varies across different function classes. In particular, when run
2
for K epochs, the convergence rate for strongly convex functions is O(1/nK2) for both RANDOM
Reshuffle and Single Shuffle (Nagaraj et al., 2019; Ahn et al., 2020; Mishchenko et al., 2020).
However, when run specifically on strongly convex quadratics, Random Reshuffle experiences
an acceleration of rates, whereas Single Shuffle does not (Safran & Shamir, 2019; Rajput et al.,
2020; Ahn et al., 2020; Mishchenko et al., 2020). All the above rates have been coupled by matching
lower bounds, at least up to constants and sometimes log factors (Safran & Shamir, 2019; Rajput
et al., 2020).
From the above we observe that
reshuffling at the beginning of ev-
ery epoch may not always help. But
then there are cases where Random
Reshuffle is faster than Single
S huffle, implying that certain ways
of generating permutations are more
suited for certain subfamilies of func-
tions.
The goal of our paper is to take a
first step into exploring the relation-
ship between convergence rates and
the particular choice of permutations.
We are particularly interested in un-
derstanding if random permutations
are as good as optimal, or if SGD can
experience faster rates with carefully
crafted permutations. As we see in the
following, the answer the above is not
straightforward, and depends heavily
on the function class at hand.
Algorithm 1 Permutation-based SGD variants
Input: Initialization x10, step size α, epochs K
1:	σ = a random permutation of [n]
2:	for k = 1, . . . , K do
3:	if IGD then
4:	σk = (1, 2, . . . , n)
5:	else if SINGLE S HUFFLE then
6:	σk = σ
7:	else if RANDOM RESHUFFLE then
8:	σk = a random permutation of [n]
9:	end if
10:	if FLIPFLOP and k is even then
11:	σk = reverse of σk-1
12:	end if
13:	fori = 1,...,n do	I
14:	xk := xk-1 - αVfσk (xk-i)	Epoch k
15:	end for	I
16:	x0k+1 :=	xk
0n
17:	end for
Our Contributions: We define as
permutation-based SGD to be any
variant of the iterates in (2), where
a permutation of the n functions, at
the start of each epoch, can be gen-
erated deterministically, randomly, or
with a combination of the two. For
example, Single S huffle, Ran-
dom Reshuffle, and incremen-
tal gradient descent (IGD), are all
permutation-based SGD variants (see
Algorithm 1).
We first want to understand—even
in the absence of computational
constraints in picking the optimal
permutations—what is the fastest rate
one can get for permutation-based
SGD? In other words, are there per-
mutations that are better than random
in the eyes of SGD?
Perhaps surprisingly, we show that
there exist permutations that may of-
fer up to exponentially faster conver-
Plain	with FlipFlop
RR Ω
SS
IGD
Thm. 5
Thm. 4
Thm. 6
Table 1: Convergence rates of Random Reshuffle
(RR), Single Shuffle (SS) and Incremental Gra-
dient Descent (IGD) on strongly convex quadratics:
Plain vs. FlipFlop. Lower bounds for the “plain” ver-
sions are taken from (Safran & Shamir, 2019). When
n K, that is when the training set is much larger
than the number of epochs, which arguably is the case
in practice, the convergence rates of Random Reshuf-
fle,Single S huffle, and Incremental Gradient
DESCENT are Ω(n^)，ωnκK2)， and Ω(K) respec-
tively. On the other hand, by combining these methods
with FlipFlop the convergence rates become faster,
i.e., Oe( nκK5 )，Oe( nKκ4 )， and O(K), respectively.
gence than random permutations, but for a limited set of functions. Specifically, we show this for
1-dimensional functions (Theorem 1). However, such exponential improvement is no longer possible
2
Published as a conference paper at ICLR 2022
in higher dimensions (Theorem 2), or for general strongly convex objectives (Theorem 3), where
random is optimal. The above highlight that an analysis of how permutations affect convergence rates
needs to be nuanced enough to account for the structure of functions at hand. Otherwise, in lieu of
further assumptions, random permutations may just appear to be as good as optimal.
In this work, we further identify a subfamily of convex functions, where there exist easy-to-
generate permutations that lead accelerated convergence. We specifically introduce a new technique,
FLIPFLOP, which can be used in conjunction with existing permutation-based methods, e.g., RAN-
dom Reshuffle, Single Shuffle, or Incremental Gradient Descent, to provably improve
their convergence on quadratic functions (Theorems 4, 5, and 6). The way that FlipFlop works
is rather simple: every even epoch uses the flipped (or reversed) version of the previous epoch’s
permutation. The intuition behind why FlipFlop leads to faster convergence is as follows. Towards
the end of an epoch, the contribution of earlier gradients gets attenuated. To counter this, we flip
the permutation for the next epoch so that every function’s contribution is diluted (approximately)
equally over the course of two consecutive epochs. FlipFlop demonstrates that finding better permu-
tations for specific classes of functions might be computationally easy. We summarize FlipFlop’s
convergence rates in Table 1 and report the results of numerical verification in Section 6.2.
Note that in this work, we focus on the dependence of the error on the number of iterations, and in
particular, the number of epochs. However, we acknowledge that its dependence on other parameters
like the condition number is also very important. We leave such analysis for future work.
Notation: We use lowercase for scalars (a), lower boldface for vectors (a), and upper boldface for
matrices (A).
2	Related Work
Gurbuzbalaban et al. (2019a;b) provided the first theoretical results establishing that Random
Reshuffle and Incremental Gradient Descent (and hence Single S huffle) were indeed
faster than vanilla SGD, as they offered an asymptotic rate of O 1/K2 for strongly convex functions,
which beats the convergence rate of O (1/nK) for vanilla SGD when K = Ω(n). Shamir(2016) used
techniques from online learning and transductive learning theory to prove an optimal convergence
rate of O(1/n) for the first epoch of RANDOM RESHUFFLE (and hence SINGLE S HUFFLE). Later,
Haochen & Sra (2019) also established a non-asymptotic convergence rate of O (超匕 + K^), when
the objective function is quadratic, or has smooth Hessian.
Nagaraj et al. (2019) used a very interesting iterate coupling based approach to give a new upper
bound on the error rate of Random Reshuffle, thus proving for the first time that for general
strongly convex smooth functions, it converges faster than vanilla SGD in all regimes of n and K.
This was followed by (Safran & Shamir, 2019), where the authors were able to establish the first
lower bounds, in terms of both n and K, for RANDOM RESHUFFLE. However, there was a gap in
these upper and lower bounds. The gap in the convergence rates was closed by Rajput et al. (2020),
who showed that the upper bound given by Nagaraj et al. (2019) and the lower bound given by Safran
& Shamir (2019) were both tight up to logarithmic terms.
For Single S huffle, Mishchenko et al. (2020) and Ahn et al. (2020) showed an upper bound of
O (njκ2), which matched the lower bound given earlier by (Safran & Shamir, 2019), up to logarithmic
terms. Ahn et al. (2020) and Mishchenko et al. (2020) also proved tight upper bounds for Random
Reshuffle, with a simpler analysis and using more relaxed assumptions than (Nagaraj et al., 2019)
and (Rajput et al., 2020). In particular, the results by Ahn et al.(2θ2θ) work under the PE condition
and do not require individual component convexity.
Incremental Gradient Descent on strongly convex functions has also been studied well in
literature (Nedic & Bertsekas, 2001; Bertsekas, 2011; Gurbuzbalaban et al., 2019a). More recently,
Nguyen et al. (2020) provide a unified analysis for all permutation-based algorithms. The dependence
of their convergence rates on the number of epochs K is also optimal for INCREMENTAL GRADIENT
Descent, Single S huffle and Random Reshuffle.
There has also been some recent work on the analysis of Random Reshuffle on non-strongly
convex functions and non-convex functions. Specifically, Nguyen et al. (2020) and Mishchenko
3
Published as a conference paper at ICLR 2022
et al. (2020) show that even there, Random Reshuffle outperforms SGD under certain conditions.
Mishchenko et al. (2020) show that Random Reshuffle and S ingle S huffle beat vanilla SGD
on non-strongly convex functions after Ω(n) epochs, and that RANDOM RESHUFFLE is faster than
vanilla SGD on non-convex objectives if the desired error is O(1/√n).
Speeding up convergence by combining without replacement sampling with other techniques like
variance reduction (Shamir, 2016; Ying et al., 2020) and momentum (Tran et al., 2020) has also
received some attention. In this work, we solely focus on the power of “good permutations” to
achieve fast convergence.
3	Preliminaries
We will use combinations of the following assumptions:
Assumption 1 (Component convexity). fi (x)’s are convex.
Assumption 2 (Component smoothness). fi(x)’s are L-smooth, i.e.,
∀x, y : kVfi(x) - Vfi(y)k ≤ Lkx - yk.
Note that Assumption 2 immediately implies that F also has L-Lipschitz gradients:
∀x, y : kVF (x) - VF(y)k ≤ Lkx - yk.
Assumption 3 (Objective strong convexity). F is μ-strongly convex, i.e.,
∀x, y : F(y) ≥ F(x) + hVF(x), y — xi + 2μky — xk2.
Note that Assumption 3 implies
∀x, y ： hVF(x) — VF(y), x — yi≥ μ∣∣y — xk2.	(3)
We denote the condition number by κ, which is defined as K = L. It can be seen easily that K ≥ 1
always. Let x* denote the minimizer ofEq. (1), that is, x* = arg minχ F(x).
We will study permutation-based algorithms in the constant step size regime, that is, the step size is
chosen at the beginning of the algorithm, and then remains fixed throughout. We denote the iterate
after the i-th iteration of the k-th epoch by xik . Hence, the initialization point is x10 . Similarly, the
permutation of (1, . . . , n) used in the k-th epoch is denoted by σk, and its i-th ordered element is
denoted by σik. Note that if the ambient space is 1-dimensional, then we represent the iterates and the
minimizer using non-bold characters, i.e. xik and x*, to remain consistent with the notation.
In the following, due to lack of space, we only provide sketches of the full proofs, when possible.
The full proofs of the lemmas and theorems are provided in the Appendix.
4	Exponential Convergence in 1 -Dimension
In this section, we show that there exist permutations for Hessian-smooth 1-dimensional functions
that lead to exponentially faster convergence compared to random.
Assumption 4 (Component Hessian-smoothness). fi(x)’s have LH -smooth second derivatives, that
is,
∀x,y : |V2fi(x) — V2fi(y)| ≤ LH|x — y|.
We also define the following instance dependent constants: G* := maxi kVfi(x*)k, D =
max{kx0 — x*k, GL},and G = GG +2DL.
Theorem 1.	LetAssumptions 1,2,3 and4hold. Let D and G be as defined above. If ɑ = 8奴工2+工心),
then there exists a sequence of permutations σ1, σ2, . . . , σK such that using those permutations from
any initialization point x01 gives the error
|xnK — x* | ≤ (D + 4nαG)e-CK,
2
Where C = 16(l2+lhG).
4
Published as a conference paper at ICLR 2022
An important thing to notice in the theorem
statement is that the sequence of permutations
σ1,σ2,..., σκ only depends on the function,
and not on the initialization point x1. This
implies that for any such function, there ex-
ists a sequence of permutations σ1,σ2,...,σK,
which gives exponentially fast convergence, un-
conditionally of the initialization. Note that
the convergence rate is slower than Gradient
Descent, for which the constant 'C' would be
larger. However, here we are purely interested in
the convergence rates of the best permutations
and their (asymptotic) dependence on K.
Figure 1: (A graphical depiction of Theorem 1’s
proof sketch.) Assume that the minimizer is at
the origin. The proof of Theorem 1 first shows
that there exists an initialization and a sequence
of permutations, such that using those, we get to
the exact minimizer. Let the sequence of iterates
for this run be xokpt. Consider a parallel run, which
uses the same sequence of permutations, but an
arbitrary initialization point. Let this sequence be
xk . The figure shows how xokpt converges to the
exact optima, and the distance between xokpt and xk
decreases exponentially, leading to an exponential
convergence for xk .
Proof sketch The core idea is to establish that
there exists an initialization point x10 (close to
the minimizer x*), and a sequence of permuta-
tions such that that starting from x10 and using
that sequence of permutation leads us exactly to
the minimizer. Once we have proved this, we
show that if two parallel runs of the optimiza-
tion process are initialized from two different
iterates, and they are coupled so that they use
the exact same permutations, then they approach
each other at an exponential rate. Thus, if we
use the same permutation from any initialization
point, it will converge to the minimizer at an
exponential rate. See Figure 1 for a graphical
depiction of this sketch. We note that the figure is not the result of an actual run, but only serves to
explain the proof sketch.
5 Lower Bounds for Permutation-based SGD
The result in the previous section leads us to wonder if exponentially fast convergence can be also
achieved in higher dimensions. Unfortunately, for higher dimensions, there exist strongly convex
quadratic functions for which there does not exist any sequence of permutations that lead to an
exponential convergence rate. This is formalized in the following theorem.
Theorem 2.	For any n ≥ 4 (n must be even), there exists a 2n + 1-dimensional strongly convex
function F which is the mean of n convex quadratic functions, such that for every permutation-based
algorithm with any step size,
llχK-1 - x*k2 = ω (-1-^
n	n3 K2
This theorem shows that we cannot hope to develop constant step size algorithms that exhibit
exponentially fast convergence in multiple dimensions.
Proof sketch Here we give a proof sketch for a simpler version of the theorem, which works in
2-Dimensions, for n = 2, and when the step size α = Ω(1∕K). Consider F(x, y) = ɪ fι(x, y) +
1 f2(χ, y) such that
x2	y2
fι(x,y) = y - X + y, andf2 (χ,y) = -2 - Iy + x∙
Hence, F = 4 (x2 + y2), and has minimizer at the origin. Each epoch has two possible permutations,
σ = (1, 2) or σ = (2, 1). Working out the details manually, it can be seen that regardless of the
permutation, x0k+1 + y0k+1 > x0k + y0k, that is, the sum of the co-ordinates keeps increasing. This can
be used to get a bound on the error term l[xK yK]> l2 .
5
Published as a conference paper at ICLR 2022
Next, we show that even in 1-Dimension, individual function convexity might be necessary to obtain
faster rates than Random Reshuffle.
Theorem 3.	There exists a 1-Dimensional strongly convex function F which is the mean of two
quadratic functions f1 and f2, such that one of the functions is non-convex. Then, every permutation-
based algorithm with constant step size α ≤ L gives an error of at least
kxnK-1
—
Proof sketch The idea behind the sketch is to have one of the two component functions as strongly
concave. This gives it the advantage that the farther away from its maximum the iterate is, the more it
pushes the iterate away. Hence, it essentially results in increasing the deviation in each epoch. This
leads to a slow convergence rate.
In the setting where the individual fi may be non-convex, Nguyen et al. (2020) and Ahn et al. (2020)
show that Single Shuffle, Random Reshuffle, and Incremental Gradient Descent
achieve the error rate of Oe(6),fora fixed n. In particular, their results only need that the component
functions be smooth and hence their results apply to the function F from Theorem 3. The theorem
above essentially shows that when n = 2, this is the best possible error rate, for any permutation-
based algorithm - deterministic or random. Hence, at least for n = 2, the three algorithms are optimal
when the component functions can possibly be non-convex. However, note that here we are only
considering the dependence of the convergence rate on K . It is possible that these are not optimal, if
we further take into account the dependence of the convergence rate on the combination of both n and
K. Indeed, if we consider the dependence on n as well, INCREMENTAL GRADIENT DESCENT has a
convergence rate of Ω(1∕K2) (Safran & Shamir, 2019), whereas the other two have a convergence
≈ ,	,	_ _r,.
rate of Oe(1/nK2) (Ahn et al., 2020).
6 Flipping Permutations for Faster Convergence in Quadratics
In this section, we introduce a new algorithm FlipFlop, that can improve the convergence rate of
Single Shuffle,Random Reshuffle, and Incremental Gradient Descent on strongly
convex quadratic functions.
The following theorem gives the convergence rate of FlipFlop with S ingle Shuffle:
Assumption 5. fi(x)’s are quadratic.
Theorem 4.	If Assumptions 1, 2, 3, and 5 hold, then running FLIPFLOP WITH S INGLE S HUFFLE
80κ3/2 log(nK) max {l, √κ} is an even integer, with step size a
for K epochs, where K ≥
10lμnKK) gives the error
E h∣∣xK-1 - x*∣∣2i = Oe n2Kκ++ nκκ4).	⑷
For comparison, Safran & Shamir (2019) give the following lower bound on the convergence rate of
vanilla Single Shuffle:
E IIxKT-χ*
(5)
Note that both the terms in Eq. (4) are smaller than the term in Eq. (5). In particular, when n	κ2
1
and n is fixed as we vary K, the RHS of Eq. (5) decays as O(K), whereas the RHS of Eq. (4)
12
decays as O(K⅛). Otherwise, when K2》n and K is fixed as we vary n, the RHS of Eq. (5) decays
as Oe( 1), whereas the RHS ofEq. (4) decays as Oe(*).Hence, in both the cases, FLIPFLOP with
Single Shuffle outperforms Single Shuffle.
The next theorem shows that FlipFlop improves the convergence rate of Random Reshuffle:
6
Published as a conference paper at ICLR 2022
.JO-J-J∙,pφz--eE-JON
10-5-
40	60	90	150	220	300
Number of epochs K
.Jo-J-Jφpφz一»E」ON
10-4 -
30	40	60	90	150	220	300
Number of epochs K
.Jo-J-Jφpφz一»E」ON
30	40	60	90	150	220	300
Number of epochs K
Figure 2: Dependence of convergence rates on the number of epochs K for quadratic functions.
The figures show the median and inter-quartile range after 10 runs of each algorithm, with random
initializations and random permutation seeds (note that SS and IGD exhibit extremely small variance).
We set n = 800, so that n K and hence the higher order terms of K dominate the convergence
rates. Note that both the axes are in logarithmic scale.
Theorem 5.	IfAssumptions 1,2, 3, and5 hold, then running FLIPFLOP WrTH RANDOM RESHUFFLE
for K epochs, where K ≥ 55κlog(nK)max{l, ʌ/ɪnj is an even integer, with step size a =
10lμnKK) gives the error
〜
EmxKT-x*『]
1
n2K2
O
For comparison, Safran & Shamir (2019) give the following lower bound on the convergence rate of
vanilla Random Reshuffle:
E h∣∣xK-1 — x*『i = ω (n2K2 + nKκ3).
Hence, we see that in the regime when n K, which happens when the number of components in
the finite sum of F is much larger than the number of epochs, FLIPFLOP WITH RANDOM RESHUFFLE
is much faster than vanilla Random Reshuffle.
Note that the theorems above do not contradict Theorem 2, because for a fixed n, both the theorems
2
above give a convergence rate of O(1/K2).
We also note here that the theorems above need the number of epochs to be much larger than κ, in
which range Gradient Descent performs better than with- or without- replacement SGD, and hence
GD should be preferred over SGD in that case. However, we think that this requirement on epochs is
a limitation of our analysis, rather than that of the algorithms.
Finally, the next theorem shows that FlipFlop improves the convergence rate of Incremental
Gradient Descent.
Theorem 6.	If Assumptions 1, 2, 3, and 5 hold, then running FLIPFLOP WITH INCREMENTAL GD
for K epochs, where K ≥ 36κ log(nK) is an even integer, with step size α = 6,黑人 gives the error
E [∣∣χKT- x*『i = O( n2K2 + K3).
For comparison, Safran & Shamir (2019) give the following lower bound on the convergence rate of
vanilla Incremental Gradient Descent:
In the next subsection, we give a short sketch of the proof of these theorems.
6.1 Proof sketch
In the proof sketch, we consider scalar quadratic functions. The same intuition carries over to
2
multi-dimensional quadratics, but requires a more involved analysis. Let fi(x) := aiX- + bix + c.
7
Published as a conference paper at ICLR 2022
Assume that F(x) := 1 pn=1 fi(x) has minimizer at 0. This assumption is valid because it can
be achieved by a simple translation of the origin (see (Safran & Shamir, 2019) for a more detailed
explanation). This implies that Pin=1 bi = 0.
For the sake of this sketch, assume x01 = 0, that is, we are starting at the minimizer itself. Further,
without loss of generality, assume that σ = (1, 2, . . . , n). Then, for the last iteration of the first epoch,
x1n = x1n-1 - αfn0 (x1n-1)
= xn-1 - α(anxn-1 + bn)
= (1 - αan)x1n-1 - αbn.
Applying this to all iterations of the first epoch, we get
n	nn
x1n =	(1 - αai)x10 - α	bi	(1 - αaj).	(6)
i=1	i=1 j=i+1
Substituting x01 = 0, we get
nn
x1n = -α	bi	(1 - αaj ).	(7)
i=1 j=i+1
Note that the sum above is not weighted uniformly: b1 is multiplied by Qjn=2 (1 - αaj ), whereas bn
is multiplied by 1. Because (1 - αaj) < 1, we see that b1’s weight is much smaller than bn. If the
weights were all 1, then we would get x0n = -α Pin=1 bi = 0, i.e., we would not move away from
the minimizer. Since we want to stay close to the minimizer, we want the weights of all the bi to be
roughly equal.
The idea behind FLIPFLOP is to add something like -α Pin=1 bi Qij=1(1 - αaj) in the next epoch,
to counteract the bias in Eq. (7). To achieve this, we simply take the permutation that the algorithm
used in the previous epoch and flip it for the next epoch. Roughly speaking, in the next epoch b1 will
get multiplied by 1 whereas bn will get multiplied by Qjn=-11(1 - αaj). Thus over two epochs, both
get scaled approximately the same.
The main reason that the analysis for multidimensional quadratics is not as simple as the 1-dimensional
analysis sketched above is because unlike scalar multiplication, matrix multiplication is not commu-
tative, and the AM-GM inequality is not true in higher dimensions (Lai & Lim, 2020; De Sa, 2020).
One way to bypass this inequality is by using the following inequality for small enough α:
n	n
ɪɪ(l - aAi) ɪɪ(l - αAn-i+ι) ≤ 1 - αnμ.
i=1	i=1
Ahn et al. (2020) proved a stochastic version of this (see Lemma 6 in their paper). We prove a
deterministic version in Lemma 3 (in the Appendix).
6.2	Numerical Verification
We verify the theorems numerically by running Random Reshuffle, S ingle Shuffle and their
FLIPFLOP versions on the task of mean computation. We randomly sample n = 800 points from a
100-dimensional sphere. Let the points be xi for i = 1, . . . , n. Then, their mean is the solution to
the following quadratic problem : argminχ F(x) = 1 PZi l∣x - Xik2. We solve this problem by
using the given algorithms. The results are reported in Figure 2. The results are plotted in a log-log
graph, so that we get to see the dependence of error on the power of K .
Note that since the points are sampled randomly, Incremental Gradient Descent essentially
becomes Single Shuffle. Hence, to verify Theorem 6, we need ‘hard’ instances of Incremental
Gradient Descent, and in particular we use the ones used in Theorem 3 in (Safran & Shamir,
2019). These results are also reported in a log-log graph in Figure 2.
We also tried FlipFlop in the training of deep neural networks, but unfortunately we did not see a
big speedup there. We ran experiments on logistic regression for 1-Dimensional artificial data, the
results for which are in Figure 3, and its details are in Appendix H. The code for all the experiments
can be found at https://github.com/shashankrajput/flipflop .
8
Published as a conference paper at ICLR 2022
30	40	60	90	150	220	300	30	40	60	90	150	220	300	30	40	60	90	150	220	300
Number of epochs K	Number of epochs K	Number of epochs K
Figure 3: Dependence of convergence rates on the number of epochs K for logistic regression.
The figures show the median and inter-quartile range after 10 runs of each algorithm, with random
initializations and random permutation seeds (note that IGD exhibits extremely small variance). We
set n = 800, so that n K and hence the higher order terms of K dominate the convergence rates.
Note that both the axes are in logarithmic scale.
6.3	Faster permutations for non-quadratic objectives
The analysis of FlipFlop leverages the fact that the Hessians of quadratic functions are constant.
We think that the analysis of FliPFloP might be extended to strongly convex functions or even PE
functions (which are non-convex in general), under some assumptions on the Lipschitz continuity of
the Hessians, similar to how Haochen & Sra (2019) extended their analysis of quadratic functions to
more general classes. A key take-away from FliPFloP is that we had to understand how Permutation
based SGD works sPecifically for quadratic functions, that is, we did a white-box analysis. In general,
we feel that dePending on the sPecific class of non-convex functions (say deeP neural networks),
Practitioners would have to think about Permutation-based SGD in a white-box fashion, to come uP
with better heuristics for shuffling.
In a concurrent work by Lu et al. (2021), it is shown that by greedily sorting stale gradients, a
Permutation order can be found which converges faster for some deeP learning tasks. Hence, there do
exist better Permutations than random, even for deeP learning tasks.
7 Conclusion and Future Work
In this PaPer, we exPlore the theoretical limits of Permutation-based SGD for solving finite sum
oPtimization Problems. We focus on the Power of good, carefully designed Permutations and whether
they can lead to a much better convergence rate than random. We Prove that for 1-dimensional,
strongly convex functions, indeed good sequences of Permutations exist, which lead to a convergence
rate which is exPonentially faster than random Permutations. We also show that unfortunately, this is
not true for higher dimensions, and that for general strongly convex functions, random Permutations
might be oPtimal.
However, we think that for some subfamilies of strongly convex functions, good Permutations might
exist and may be easy to generate. Towards that end, we introduce a very simPle technique, FlipFlop,
to generate Permutations that lead to faster convergence on strongly convex quadratics. This is a black
box technique, that is, it does not look at the oPtimization Problem to come uP with the Permutations;
and can be imPlemented easily. This serves as an examPle that for other classes of functions, there
can exist other techniques for coming uP with good Permutations. Finally, note that we only consider
constant steP sizes in this work for both uPPer and lower bounds. ExPloring regimes in which the
steP size changes, e.g., diminishing steP sizes, is a very interesting oPen Problem, which we leave for
future work. We think that the uPPer and lower bounds in this PaPer give some imPortant insights and
can helP in the develoPment of better algorithms or heuristics. We strongly believe that under nice
distributional assumPtions on the comPonent functions, there can exist good heuristics to generate
good Permutations, and this should also be investigated in future work.
9
Published as a conference paper at ICLR 2022
Ethics S tatement
This paper explores the theoretical limits of convergence rates of variants of popular SGD type
algorithms. Thus, the authors do not think there are any ethical concerns regarding the paper.
Reproducibility S tatement
The proof for all the theoretical claims are provided in the Appendix. The assumptions and notations
are specified in the Preliminaries section (Section 3). The code for all the experiments can be found
at https://github.com/shashankrajput/flipflop .
References
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. Sgd with shuffling: optimal rates without component
convexity and large epoch requirements, 2020.
Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011.
Leon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. Unpublished
open problem offered to the attendance of the SLDS 2009 conference, 2009. URL http:
//leon.bottou.org/papers/bottou-slds-open-problem-2009.
Sebastien BUbeCk et al. Convex optimization: Algorithms and complexity. Foundations and Trends®
in Machine Learning, 8(3-4):231-357, 2015.
Christopher M De Sa. Random reshuffling is not always better. Advances in Neural Information
Processing Systems, 33, 2020.
M Gurbuzbalaban, Asuman Ozdaglar, and Pablo A Parrilo. Convergence rate of incremental gradient
and incremental newton methods. SIAM Journal on Optimization, 29(4):2542-2565, 2019a.
Mert Gurbiizbalaban, Asu Ozdaglar, and Pablo A Parrilo. Why random reshuffling beats stochastic
gradient descent. Mathematical Programming, pp. 1-36, 2019b.
Jeff Haochen and Suvrit Sra. Random shuffling beats sgd after finite epochs. In International
Conference on Machine Learning, pp. 2624-2633, 2019.
Zehua Lai and Lek-Heng Lim. Recht-re noncommutative arithmetic-geometric mean conjecture is
false. In International Conference on Machine Learning, pp. 5608-5617. PMLR, 2020.
Yucheng Lu, Si Yi Meng, and Christopher De Sa. A general analysis of example-selection for
stochastic gradient descent. In International Conference on Learning Representations, 2021.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis
with vast improvements. ArXiv, abs/2006.05988, 2020.
Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. Sgd without replacement: Sharper rates
for general smooth convex functions. In International Conference on Machine Learning, pp.
4703-4711, 2019.
Angelia NediC and Dimitri Bertsekas. Convergence rate of incremental subgradient algorithms. In
Stochastic optimization: algorithms and applications, pp. 223-264. Springer, 2001.
Yurii Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of Applied
Optimization. Springer, 2004. ISBN 978-1-4613-4691-3. doi: 10.1007/978-1-4419-8853-9. URL
https://doi.org/10.1007/978-1-4419-8853-9.
Lam M Nguyen, Quoc Tran-Dinh, Dzung T Phan, Phuong Ha Nguyen, and Marten van Dijk. A
unified convergence analysis for shuffling-type gradient methods. arXiv preprint arXiv:2002.08246,
2020.
10
Published as a conference paper at ICLR 2022
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of sgd
without replacement. In International Conference on Machine Learning, pp. 7964-7973. PMLR,
2020.
Benjamin Recht and Christopher Re. Beneath the valley of the noncommutative arithmetic-geometric
mean inequality: conjectures, case-studies, and consequences. arXiv preprint arXiv:1202.4184,
2012.
Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix
completion. Mathematical Programming Computation, 5(2):201-226, 2013.
Itay Safran and Ohad Shamir. How good is sgd with random shuffling?, 2019.
Itay Safran and Ohad Shamir. Random shuffling beats sgd only after many epochs on ill-conditioned
problems. arXiv preprint arXiv:2106.06880, 2021.
M. Schneider. Probability inequalities for kernel embeddings in sampling without replacement. In
AISTATS, 2016.
Ohad Shamir. Without-replacement sampling for stochastic gradient methods. In Proceedings of the
30th International Conference on Neural Information Processing Systems, pp. 46-54, 2016.
Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. Shuffling gradient-based methods with
momentum. arXiv preprint arXiv:2011.11884, 2020.
Bicheng Ying, Kun Yuan, and Ali H Sayed. Variance-reduced stochastic learning under random
reshuffling. IEEE Transactions on Signal Processing, 68:1390-1408, 2020.
11
Published as a conference paper at ICLR 2022
A	Discussion and Future Work
Being the first paper (to the best of our knowledge) to theoretically analyze the optimality of random
permutations, we limited our scope to a specific, but common theoretical setting - strongly convex
functions with constant step size. We think that future work can generalize the results of this
paper to settings of non-convexity, variable step sizes, as well as techniques like variance reduction,
momentum, etc.
A. 1 Lower bounds for variable step sizes
All the existing lower bounds (to the best of our knowledge) work in the constant step size regime
(Safran & Shamir (2019); Rajput et al. (2020); Safran & Shamir (2021)). Thus, generalizing the
lower bounds to variable step size algorithms would be a very important direction for future research.
However, the case when step sizes are not constant can be tricky to prove lower bounds, since the
step size could potentially depend on the permutation, and the current iterate. A more reasonable
setting to prove lower bounds could be the case when the step sizes follow a schedule over epochs,
similar to what happens in practice.
A.2 FlipFlop on Random Coordinate Descent
A shuffling scheme similar to FlipFlop has been used in random coordinate descent for faster practical
convergence (see page 231 in Nocedal & Wright (2006)). This should be further investigated
empirically and theoretically in future work. Even though the current analysis of FlipFlop does not
directly go through for random coordinate descent, we think the analysis can be adapted to work.
B Proof of Theorem 1
In certain places in the proof, We would need that α ≤ 41^. To see how this is satisfied, note that
We have assumed that α ≤ 8似工2+工心)in the theorem statement. Using the inequality μ ≤ L in
α ≤ 4n(L2μLH G) gives that α ≤ 4n(L+(L1H G∕μ)) ≤ 4n1L.
In this proof, we assume that the minimizer of F is at 0 to make the analysis simpler. This assumption
can be satisfied by simply translating the origin to the minimizer (Safran & Shamir, 2019).
There are three main components in the proof:
1.	Say that an epoch starts off at the minimizer. We prove that there exists at least one pair
of permutations such that if we do two separate runs of the epoch, the first using the first
permutation and the second using the second, then at the end of that epoch, the iterates
corresponding to the two runs end up on opposite sides of the minimizer.
2.	There exists a sequence of permutations and a point in the neighborhood of the minimizer,
such that intializing at that point and using these sequence of permutations, we converge
exactly to the minimizer.
3.	Starting from any other point, we can couple the iterates with the iterates which were shown
in the previous component, to get that these two sequences of iterates come close to each
other exponentially fast.
We prove the first and second components in the Subsections B.1 and B.2; and conclude the proof in
Subsection B.3 where we also prove the third component.
B.1	Permutations in one epoch
In this subsection, we prove that if x0, x1, . . . , xn are the iterates in an epoch such that x0 = 0, then
there exists a permutation of functions such that xn ≥ 0. By the same logic, we show that there exists
a permutation of functions such that xn ≤ 0. These will give us control over movement of iterates
across epochs.
12
Published as a conference paper at ICLR 2022
Order the gradients at the minimizer, Vfi(0), in decreasing order. WLOG assume that it is just
Vfi, Vf2,..., Vfn. We claim that this permutation leads to Xn ≥ 0.
We will need the following intermediate result. Let yi, yi-1 be such that yi = yi-1 - αVfi(yi-1).
Assume α ≤ 1/L and yi-1 ≥ xi-1. Then,
yi - xi = yi-1 - xi-1 - α(Vfi(yi-1) - Vfi(xi-1))
≥ yi-1 - xi-1 - αL(yi-1 - xi-1)
= (1 - αL)(yi-1 - xi-1)
≥ 0,
(8)
that is, yi-1 ≥ xi-1 =⇒ yi ≥ xi .
Because 0 is the minimizer, we know that Pin=1 Vfi(0) = 0. Also, recall that x0 = 0. There can be
two cases:
1.	∀i ∈ [1, n] : xi < 0. This cannot be true because if ∀i : 1 ≤ i ≤ n - 1 : xi < 0, then
nn
xn = X -αVfi(xi-1) ≥ -αXVfi(0) ≥ 0,
i=1	i=1
where we used the fact that Vfi(x) ≤ Vfi(y) if x ≤ y and fi is convex.
2.	Thus, ∃i ∈ [1, n] : xi ≥ 0. Now, consider the sequence yi, yi+1, . . . , yn such that yi = 0
and for j ≥ i + 1, yj = yj-1 - αVfj(yj-1). Then because α ≤ 1/L and xi ≥ yi = 0, we
get that xj ≥ yj for all j ≥ i (Using Ineq. (8)).
Hence, there is an i ≥ 1 such that xi ≥ yi = 0, and further xj ≥ yj for all j ≥ i. Next, we repeat the
process above for yi , . . . , yn . That is, there can be two cases:
1.	∀j ∈ [i + 1, n] : yj < 0. This cannot be true because if ∀j : i + 1 ≤ j ≤ n - 1 : yj < 0,
then
nn
yn=X-αVfj(yj) ≥-αXVfj(0) ≥0.
j=i	j=i
2.	Thus, ∃j ∈ [i + 1, n] : yj ≥ 0. Now, consider the sequence zj, zj+1, . . . , zn such that
zj = 0 and for k ≥ j + 1, zk = zk-1 - αVfk(zk-1). Then because α ≤ 1/L and
yj ≥ zj = 0, we get that yk ≥ zk for all k ≥ j (Using Ineq. (8)).
Hence, there exists an integer j > i > 0 such that yj ≥ 0. We have already proved that xj ≥ yj .
Thus, we have that xj ≥ 0. We can continue repeating this process (apply the same two cases above
for zj , zj+1 , . . . , zn , and so on), to get that xn ≥ 0. We define p to be this non-negative value of xn .
Note that the following lemma gives us that the gradients are bounded by G
Lemma 1. Define G* := maxi ∣∣Vfi(x*)k andD
max
{kx1 -x*k, g⅛}
. If Assumptions 2 and
3 hold, and a < JKnL，thenfor any permutation-based algorithm (deterministic or random), we have
∀i,j, k :||Xk — x*∣ ≤ 2D,	and
∣Vfj(Xk)∣ ≤ G* + 2DL.
Because the gradients are bounded by G, we get that p ≤ nαG.
Similarly, we can show that the reverse permutation leads to xn ≤ 0. We define q to be this non-
positive value of xn . Because we have assumed that the gradients are bounded by G, we get that
q ≥ —nαG.
B.2 Exact convergence to the minimizer
In this section, we show that there exists a point such that if we initialize there and follow a specific
permutation sequence, we land exactly at the minimizer.
13
Published as a conference paper at ICLR 2022
In particular, we will show the following: There exists a point in [4q, 4p] such that if we initialize
there and follow a specific permutation sequence, then we land exactly at the minimizer.
We show this recursively. We will prove that there exists a point mK ∈ [4q, 4p] such that if the last
epoch begins there, that is x0K = mK, then we land at the minimizer at the end of the last epoch,
that is, xnK = 0. Then, we will show that there exists a point mK-1 ∈ [4q, 4p] such that if the
x0K-1 = mK-1, then x0K = xnK-1 = mK. Repeating this for K - 2, . . . , 1, we get that there exists
a point m0 ∈ [4q, 4p] such that if we initialize the first epoch there, that is x01 = m0, then there is a
permutation sequence such that ultimately xnK = 0.
We prove that any point mj ∈ [4q, 4p] can be reached at the end of an epoch by beginning the epoch
at some point mj-1 ∈ [4q, 4p], that is if xj0-1 = mj-1, then xjn-1 = mj.
• Case: mj ∈ [p, 4p]. In this case, we show that mj-1 ∈ [0, 4p]. We have proved in the
previous subsection that there exists a permutation σ such that if xj0-1 = 0 then xjn-1 = p.
Next, we have the following helpful lemma that we will also use later.
Lemma 2. Let x0, x1 , . . . , xn be a sequence of iterates in an epoch and y0, y1, . . . , yn
be another sequence of iterates in an epoch such that both use the same permutation of
functions. If α ≤ 2似工2+工心),then
(1 - naL)∣yo - xo| ≤ (1 - La)n∣yo - xo| ≤ |yn - Xn| ≤(1 - 1 nμα) |yo - xo|.
If we set x0 = 4p, y0 = 0 in Lemma 2 and we follow the permutation σ, then we get that
Xn - yn ∈ (x0 - yO) h1 - αnL, 1---2— i
一 ɑ Γ. r r αnμi
=⇒ Xn - P ∈ (4p - O) [1 - anL, 1-----2-J
(Since using σ and y0 = 0 results in yn = p.)
=⇒ Xn ≥ 4p,
where We used the fact that α ≤ 4n^ is the last step.
Thus, if Xj0-1 = 4p and we follow the permutation σ, then Xjn-1 ≥ 4p.
Next, note that
Xi-1 = x0-1 - αVfσ(o)(x0-1).
x2T = XIT — αvfσ(1) (XIT)
Xjn-1 = Xjn--11 - αVfσ(n-1)(Xjn--11)
Looking above, we see that Xj1-1 is a continuous function of Xj0-1; Xj2-1 is a continuous
function of Xj1-1; and so on. Thus, using the fact that composition of continuous functions
is continuous, we get that Xjn-1 is also a continuous function of Xj0-1. We have shown that
if Xj0-1 = 0, then Xjn-1 = p and if Xj0-1 = 4p, then Xjn-1 ≥ 4p. Thus, using the fact that
that Xjn-1 is a continuous function of Xj0-1, we get that for any point mj ∈ [p, 4p], there is
at least one point mj 1 ∈ [0, 4p], such that Xj0-1 = mj-1 leads to Xjn-1 = mj.
•	Case: mj ∈ [4q, q]. We can apply the same logic as above to show that there is at least one
point mj 1 ∈ [4q, 0], such that Xj0-1 = mj-1 =⇒ Xjn-1 = mj .
•	Case: mj ∈ [q,p]. WLOG assume that |q| < |p|. Let σq be the permutation such that if
Xj0-1 = 0 and the epoch uses this permutation, then we end up at Xjn-1 = q.
If we set X0 = 4p, y0 = 0 in Lemma 2 and we follow the permutation σq , then we get that
Xn - yn ∈ (x0 - yO) [1 - anL, 1-----2~∖
14
Published as a conference paper at ICLR 2022
一 ɑ Γ. r r αnμi
=⇒ Xn — q ∈ (4p — 0) [1 — anL, 1-----2-J
(Since using σq and y0 = 0 results in yn = q.)
=⇒ xn ≥ q + 4p(1 — αnL) ≥ q + 3p ≥ 2p,
where we used the fact that α ≤ 4n^ is the last step.
Thus, if xj0-1 = 4p and we follow the permutation σq, then xjn-1 ≥ 2p.
Thus, using similar argument of continuity as the first case, we know that there is a point
mj-1 ∈ [0, 4p], such that xj0-1 = mj-1 leads to xjn-1 = mj when we use the permutation
σq.
B.3 Same sequence permutations get closer
In the previous subsection, we have shown that there exists a point m0 ∈ [4q, 4p] and a sequence of
permutations σ1 , σ2, . . . , σK such that if x10 = m0 and epoch j uses permutation σj , then xnK = 0.
In this subsection, we will show that if x01 is initialized at any other point such that |x01| ≤ D, then
using the same permutations σ1, σ2, . . . , σK gives us that |xnK | ≤ (D + nαG)e-K. For this we will
repeatedly apply Lemma 2 on all the K epochs. Assume that x1 = V0 with ∣ν0∣ ≤ D.
Let yij be the sequence of iterates such that y01 = m0 and uses the permutation sequence
σ1 , σ2, . . . , σK . Then, we know that ynK = 0. Let xij be the sequence of iterates such that x10 = ν0
and uses the same permutation sequence σ1, σ2, . . . , σK .
Then, using Lemma 2 gives us that |yn1 — xn ≤ ∣ν0 — m0∣(1 — μ2n). Thus, we getthat |y2 — x0| ≤
∣ν0 — m0∣(1 — μ0n). Again applying Lemma 2 gives us that |ynn — xn ≤ ∣ν0 — m0∣(1 — μα2n)2.
Therefore, after applying it K times, we get
1yK - χKl≤∣ν0. m01(1-等)K
Note that |x1| = ∣ν0| ≤ D, and |y0| = |m0|, with m0 ∈ [4q, 4p]. We showed earlier in Subsection
B.1 that |p| ≤ nαG and |q| ≤ nαG. Therefore,
-xK∣≤D + 4nɑG∣(1 -号)K
Further, we know that ynK = 0. Thus,
∣xK∣≤D + 4nαG∣(1 -号)K
≤ D + 4nαG∣e-号K.
Substituting the value of α completes the proof. Next we prove the lemmas used in this proof.
B.4 Proof of Lemma 1
We restate the lemma below.
Lemma. Define G* := maxi ∣∣Vfi(x*)k andD = max {∣∣x1 — x*k, G}. IfAssumPtions 2 and3
hold, and α < 寻汇,thenfor any permutation-based algorithm (deterministic or random), we have
∀i,j, k :|Xk — x*∣ ≤ 2D, and
∣Vfj(Xk)k≤ G* + 2DL.
This lemma says that for any permutation-based algorithm, the domain of iterates and the norm of the
gradient stays bounded during the optimization. This means that we can assume bounds on norm
of iterates and gradients, which is not true in general for unconstrained SGD. This makes analyzing
such algorithms much easier, and hence this lemma can be of independent interest for proving future
results for permutation-based algorithms.
15
Published as a conference paper at ICLR 2022
Remark 1. This lemma does not hold in general for vanilla SGD where sampling is done with
replacement. Consider the example with two functions f1 (x) = x2 - x, and f2 (x) = x; and
F(x) = f1(x) + f2(x). This satisfies Assumptions 2 and 3, but one may choose f2 consecutively
for arbitrary many iterations, which will lead the iterates a proportional distance away from the
minimizer. This kind of situation can never happen for permutation-based SGD because we see every
function exactly once in every epoch and hence no particular function can attract the iterates too
much towards its minimizer, and by the end of the epochs most of the noise gets cancelled out.
Note that the requirement a < ∙8KnL is stricter than the usual requirement a = O (=), but We
believe the lemma should also hold in that regime. For the current paper, however, this stronger
requirement suffices.
Proof. To prove this lemma, we show two facts: If for some epoch k, ∣∣xk - x*k ≤ D, then a)
∀i : kXk - x*k ≤ 2D andb) kxk+1 - x*k = ∣∣xJn - x*k ≤ D. To see how a) andb) are sufficient
to prove the lemma, assume that they are true. Then, since the first epoch begins inside the bounded
region ∣∣x1 - x*∣ ≤ D, we see using b) that every subsequent epoch begins inside the same bounded
region, that is ∣∣Xq - x*∣ ≤ D as well. Hence using a) we get that during these epochs, the iterates
satisfy ∣∣xQ - x* ∣ ≤ 2D, which is the first part of the lemma. Further, this bound together with the
gradient Lipshitzness directly gives the upper bound G* + 2DL on the gradients. Thus, all we need
to do to prove this lemma is to prove a) and b), which we do next.
We will prove a) and b) for D = max{∣x0 - x*∣, ：-；aGL}. Once we do this, using α < ^nL
will give us the exact statement of the lemma.
Let |x0k - x* | ≤ D for some epoch k. Then, we try to find the minimum number of iterations i
needed so that |xik - x* | ≥ 2D. Within this region, the gradient is bounded by G* + 2DL. Thus,
the minimum number of iterations needed are a(G£-DL). However,
2D - D	1
α(G* +2DL) = α(G +2L)
1
≥ ————F---------
一α(G*1-4KnaL +2L)
2	2κnaG*	1	)
1
11—4KnaL
1	2κna
+ 2L)
(Using the fact that D ≥ "熏工.)
2κn
≥ 2n.
Thus, the minimum number iterations needed to go outside the bound |xik - x* | ≥ 2D is more than
the length of the epoch. This implies that within the epoch, ∣xik - x* ∣ ≤ 2D, which proves a).
We prove b) next:
∣xkn
*
n
α
i=0
Note that PNid Vfσk (xk) is just the sum of all component gradients at xk, that is
Pi=0 Vfσk(xk) = nV F (xk). Using this, we get
n
∣xkn - x*∣ = x0k - x* - nαVF (x0k) + α X(Vfσik (x0k) -Vfσik(xik))
i=0
≤ x0k - x* - nαVF (x0k ) +
n
α
i=0
(Triangle inequality.)
16
Published as a conference paper at ICLR 2022
n
≤ Ilxk- x*- nαVF(Xk)∣∣ + αLXIlxk- Xkll ,	⑼
i=0
where we used gradient Lipschitzness (Assumption 2) in the last step.
To bound the first term above, we use the standard analysis of gradient descent on smooth, strongly
convex functions as follows
llx0k -x* -nαVF(x0k)ll2 = llx0k -x* ll2 - 2nαhx0k -x*,VF(x0k)i+n2α2kVF(x0k)k2
≤ ∣∣xk — x*∣∣2 — 2nαμ∣∣xk — x*∣∣2 + n2α2∣∣VF(xk)||2
(Using Ineq. (3))
=(1 — nαμ)忖—x* ∣∣2 + nα(nαIlVF(Xk)∣∣2 — μkx0 — x*∣∣2)
≤ (1 — naμ) ∣∣xk — x* ∣∣2 + na(nαL2∣xk — x*∣2 — μ∣xk — x*∣2)
(Using gradient Lipschitzness)
=(1 — naμ) ∣∣xk — x* ∣∣2 + na(nαL2 — μ)∣xk — x*∣2
≤ (1 — nαμ) ∣∣xk — x*∣∣2 ,
where in the last step We used that a ≤ n^ since α ≤ ^^. Substituting this inequality in Ineq. (9),
we get
n
kxn-x*k ≤ √1—nαμ ∣∣xk - x*∣∣+αL χ∣∣Xk- xk∣∣
i=0
≤
n
∣∣x0k — x*∣∣ + αLX ∣∣x0k — xik∣∣ .
i=0
We have already proven a) that says that the iterates xik satisfy Ixik — x* I ≤ 2D. Using gradient
Lipschitzness, this implies that the gradient norms stay bounded by G* + 2DL. Hence, ∣x0k — xik ∣ ≤
αi(G* + 2DL). Using this,
∣∣xjn — x*k ≤(1 — Jnɑμ) ∣∣xk — x*∣∣ + αL^Xαi(G* + 2DL)
≤(1 — gnαμ) D + αL ^X αi(G* + 2DL)
≤(1 — gnɑμ) D + n2α2L(G* + 2DL)
≤D,
where we used the fact that D ≥ 12κnθGL in the last step.
□
B.5 Proof of Lemma 2
Without loss of generality, let σ = (1, 2, 3, . . . , n). This is only done for ease of notation. The
analysis goes through for any other permutation σ too.
First we show the lower bound. WLOG assume y0 > x0. Because α < 1/L, we have that ∀i, yi > xi
by induction (see the equations below). Then,
yi — xi = yi-1 — xi-1 — α(Vfi(yi-1) — Vfi(xi-1))
≥ yi-1 — xi-1 — αL(yi-1 — xi-1)
= (1 — αL)(yi-1 — xi-1 )
(1 — αL)i (y0 — x0)
17
Published as a conference paper at ICLR 2022
≥ (1 - iαL)(y0 - x0).
(10)
Next we show the upper bound
n
yn - Xn = yo - X0 - α EEfi(yi-l) - Nfi(Xi-1))
i=1
nn
= y0 - X0 - α X(Nfi(y0) - Nfi(X0)) + α X(Nfi(y0) - Nfi (X0) - Nfi(yi-1) + Nfi (Xi-1))
i=1	i=1
n
=y0 -X0 - nα(NF (y0) - NF(X0)) + αX(Nfi(y0) -Nfi(X0) -Nfi(yi-1)+Nfi(Xi-1))
i=1
n
≤ (1 - nαμ)(yo - xo) + αX(Vfi(yo) — Nfi(XO) - Nfi(yi-i) + Nfi(Xi-1)).
i=1
(Using strong convexity)
We use the fact that the function is twice differentiable:
n	y0	yi-1
yn - Xn = (1 - nαμ)(yo - xo) + a£	/	V2fi(t)dt - /	V2fi(t)dt
i=1 x0	xi-1
=(1 - nαμ)(yo - Xo)
n	y0	xi-1+(y0-x0)	yi-1
+ α	N2fi(t)dt -	N2fi(t)dt -	N2fi(t)dt
i=1	x0	xi-1	xi-1+(y0-x0)
=(1 - nαμ)(yo - Xo)
n	y0	yi-1
+α	(N2fi(t) - N2fi(Xi-1 -Xo+t))dt-	N2fi(t)dt .
i=1	x0	xi-1+(y0-x0)
In the above, we used the convention that Rab f (X)dX is the same as - Rba f (X)dX if a > b.
Now, we can use the Hessian Lipschitzness to bound the term as follows
n	y0	yi-1
yn - Xn ≤ (1 - nαμ)(yo - Xo) + aɪ2 /	LH∣Xi-i - Xo∣dt - /	V2fi(t)dt
i=1	x0	xi-1 +(y0 -x0)
≤ (1 - nαμ)(yo - Xo) + a£
y0	yi-1
LH Gαndt -	V2fi (t)dt
x0	xi-1+(y0-x0)
n	yi-1
(1 — nαμ)(yo — Xo) + LHGa2n2(xo - yo) - α〉: /	V2fi(t)dt
i=1 xi-1+(y0-x0)
n
≤ (1 - naμ)(yo - Xo) + LH Ga2n2(xo - yo) + a£ L((yo - Xo) - (yi - Xi))
i=1
n
≤ (1 - naμ)(yo - Xo) + LHGa2n2(xo - yo) + a£L(iaL)(y0 - Xo)
i=1
(Using Ineq. (10).)
≤ (1 - naμ)(yo - Xo) + LHGa2n2(Xo - yo) + a2n2L2(yo - Xo).
Thus, if We have a ≤ 2以工2+工心),then
C Proof of Theorem 2
To prove this theorem, We consider three step-size ranges and do a case by case analysis for each of
them. We construct functions for each range such that the convergence of any permutation-based
18
Published as a conference paper at ICLR 2022
algorithm is “slow” for the functions on their corresponding step-size regime. The final lower bound
is the minimum among the lower bounds obtained for the three regimes.
In this proof, we will use different notation from the rest of the paper because we work with scalars
in the proof, and hence the superscript will denote the scalar power, and not the epoch number. We
will use xk,i to denote the i-th iterate of the k-th epoch.
We will construct three functions F1(x), F2(y), and F3(z), each the means ofn component functions,
such that
•	Any permutation-based algorithm on F1 (x) with α ∈ [ 2(n-1)κL , n3L ] and initialization
x1,0 = 0 results in
kxκ,nk2 = ω (n⅛).
F1 will be an n-dimensional function, that is x ∈ Rn . This function will have minimizer
at 0 ∈ Rn. NOTE: This is the ‘key’ step-size range, and the proof sketch explained in the
main paper corresponds to this function’s construction.
•	Any permutation-based algorithm on F2(y) with α ∈ [焉,L] and initialization y1,0 = 0
results in
kyK,n k2 = ω
(W)
F2 will be an n-dimensional function, that is y ∈ Rn . This function will have minimizer
at 0 ∈ Rn . The construction for this is also inspired by the construction for F1 , but this is
constructed significantly differently due to the different step-size range.
•	Any permutation-based algorithm on F3(z) with α ∈ [2(n-1)κL, j1] and initialization
z1,0 = 1 results in
∣ZK,n∣2 = Ω (1)
F3 will be an 1-dimensional function, that is z ∈ R. This function will have minimizer at 0.
Then, the 2n + 1-dimensional function F([x>, y>, z]>) = F1(x) + F2(y) + F3(z) will show bad
convergence in any step-size regime. This function will have minimizer at 0 ∈ R2n+1 . Furthermore,
n-1 LI W V2F1, V2F2, V2F3, V2F W 2LI,
n
that is, Fι, F2, F3, and F are all n-1 L-strongly convex and 2L-smooth.
In the subsequent subsections, we prove the lower bounds for F1, F2, and F3 separately.
NOTE: We note that above, we have used a specific initialization. However, in Appendix C.4, we
discuss how the lower bound is actually invariant to initialization.
C.1 LOWER BOUND FOR Fi, α ∈ [2(n-I)KL,袅]
We will work in n-dimensions (n is even) and represent a vector in the space as z =
[x1, y1, . . . , xn/2, yn/2]. These xi and yi are not related to the vectors used by F2 and F3 later,
we only use xi and yi to make the proof for this part easier to understand.
We start off by defining the n component functions: For i ∈ [n/2], define
fi(Z)= 2xi 一 xi + yi + X 2χxj + 2y2) , and
gi(Z) := Ly2 一 y + χi + X (Lχj + Ly2).
Thus, F(z) := 1 (Pn/2 fi + Pn=2 gi) = (n-1) L∣∣zk2. This function has minimizer at z* = 0.
19
Published as a conference paper at ICLR 2022
Let zk,j denote z at the j-th iteration in k-th epoch. We initialize at z1,0 = 0. For the majority of this
proof, we will work inside a given epoch, so we will skip the subscript denoting the epoch. We denote
the j-th iterate within the epoch as zj and the coordinates as zj = [xj,1 , yj,1 , . . . , xj,n/2 , yj,n/2]
In the current epoch, let σ be the permutation of {f1, . . . , fn/2, g1 , . . . , gn/2} used.
For any i ∈ [n/2], letp and q be indices such that σp = fi and σq = gi. Let us consider the case that
p < q (the case when p > q will be analogous). Then, it can be seen that
xn,i = (1 - αL)n-1 x0,i + (1 - αL)n-p-1 α - (1 - αL)n-q α, and
yn,i = (1 - αL)n-1 y0,i - (1 - αL)n-pα + (1 - αL)n-q α.
(11)
Hence,
xn,i + yn,i = (1 - αL)n-1(x0,i + y0,i) + 2(1 - αL)n-p-1α2L
≥ (1 - αL)n-1(x0,i + y0,i) + 2(1 - αL)n-1α2L.
In the other case, when p > q, we will get the same inequality. Let xK,n,i and yK,n,i denote the value
of xn,i and yn,i at the K-th epoch. Then, recalling that z was initialized to 0, we use the inequality
above to get
1	(1	αL)(n-1)K
xK,n,i + yK,n,i ≥ (I — QL)S	• 0 + 2(I — aL)n	a L -.....Jrj--FXn-I-
1 — (1 — αL)n-
n 1 2 1- (1 - αL)(n-1)K
= 2(1 — αL) α L  ----------------：—
1 - (1 - αL)n-1
Since this inequality is valid for all i, we get that
n/2
kzK,n k2 = X(x2K,n,i + yK2 ,n,i)
i=1
n/2 1
≥〉1 2(xK,n,i + yK,n,i)
〉XX 1 Λ∩	^nn~1c1ι-1 - (1 - αL)(n-1')K Y
≥ N 2 221 - L α 1 —(1—ɑL)n-i )
(U 八 n-1 2/ 1 - (1 - αL)(n-1)K Y
=叫(1 — αL) aL 1 - (1 — ɑL)n-i ).
(12)
(13)
Note that if a ≤ nL and n ≥ 4, then (1 — aL)n-1 ≥ 8. Using this in (13), We get
kzK,nk2
≥n
(1 _ rvΓ)n-1 2L 1 — (I - aL)(nT)K
(I- aL) a L 1 - (1 - aL)n-1
≥ 二a4L2 (I-(Ii(n-,Y
-128 V 1 — (1 — aL)n-1 J
n
128L2
(	(aL)2	Y
(1 — (1 — aL)n-1 J
1 — (1 — aL)(n-1)K2
2
We consider tWo cases:
1. Case A, aL ≤
1
2(n+2)
It can be verified that
(aL)2
1-(1-aL)n-1
is an increasing function of a
when aL ≤ 25+2). Noting that we are working in the range when a ≥ 2(n-1)κL，then
kzK,nk2 ≥ 1^8L2
2n⅛)
、n-1
∖1 — 11 — 2(n-i)K
1 — (1 — aL)(n-1)K2
/
2
2
20
Published as a conference paper at ICLR 2022
≥
≥
≥
n
128L2
1 Y ∖2
2(n-1)K J	I
1
、2(n-1)K
n
(n-1)
1 - (1 - αL)(n-1)K2
512(n - 1)4K2L2
n
1 - (1 - αL)(n-1)K2
512(n - 1)4K2L2
n
1 - e-αL(n-1)K 2
512(n - 1)4K2L2
ω (n3K2L2 ).
1 - e-1/22
/
2. Case B, αL > 2(η；2): In this case,
kzK,nk2 ≥
n
128L2
n
128L2
（」
［，）
（「
［，）
1 - (1 - αL)(n-1)K2
1 - e-αL(n-1)K 2
n
128L2
十)2 )2
1-e
3(n-1)K ∖ 2
2(n + 2))
≥
≥

C.2 LOWER BOUND FOR F2, a ∈ [焉,L]
We will work in n-dimensions and represent a vector in the space as y = [y1, . . . , yn].
We start off by defining the n component functions: For i ∈ [n], define
fi(y) := -yi +
j6=i
Thus, F(y) := * pn=ι f = (n-1) LLIlyII2. This function has minimizer at y* = 0.
Let yk,j denote y at the j-th iteration in k-th epoch. We initialize at y1,0 = 0. For the majority of
this proof, we will work inside a given epoch. We denote the j-th iterate within the epoch as yj and
the coordinates as yj = [yj,1, . . . , yj,n]
In the current epoch, let σ be the permutation of {f1, . . . , fn} used. Let i be the index such that
σn = i, that is, i is the last element of the permutation σ . Then at the end of the epoch,
n-2
yn,i = (I - aL)n-1yo,i + α-----ɪ X(I - αL)j
n - 1 j=0
(1 - αL)n-1y0,i + α -
(1 - (1 - αL)n-1)
(n — 1)L
(14)
For some j ∈ [n], let s be the integer such that σs = j, that is j is the s-th element in the permutation
σ. Then for any j and any epoch,
n-2
yn,j = (1 - aL)n 1y0,j + α(1 - αL)n S------------- ^X(I - aL)j.
n - 1 j=0
21
Published as a conference paper at ICLR 2022
Then,
n-2
yn,j ≥ (I - αL)n-1y0,j - n - 1 X(I - aL)j
j=0
(1 - αL)n-1y0,j -
(1 - (1 - αL)n-1)
(n - 1)L
Note that the above is independent of σ, and hence applicable for all epochs. Applying it recursively
and noting that we initialized y1,0 = 0, we get
yn,j ≥ -
(1 - (1 - αL)n-1)
(n - 1)L
K
X(1 - αL)n
k=1
、	1
≥ - (n - 1)L.
Note that y0,i is just yn,i from the previous epoch. Hence we can substitute the inequality above for
y0,i in (14). Thus,
(1 - αL)n-1	(1 - (1 - αL)n-1)
yn,i ≥ --(n - 1)L	+ α	(n-1)L-
1
=α ---------
(n - 1)L
〉3	1
_ nL	(n — 1)L
This gives Us that kyk,nk2 = Ω (n⅛) for any k.
C.3 LOWER BOUND FOR F3, α ∈ [2(n-1)KL , L]
Consider the function F(z) = ɪ P3 fi(z), where fi(z) = Lz2 for all i. Note that the gradient of
any fUnction at z is jUst -2Lz. Hence, regardless of the permUtation, if we start the shUffling based
gradient descent method at z1,0 = 1, we get that
zK,n = (1 - 2αL)nK z1,0 = (1 - 2αL)nK.
1
In the case when α ≤ 2(n-i)KL,
we see that
zκ,n ≥ (1- 2kWK
≥ (1 - 二)nK
=ω⑴，
for n, K ≥ 2. Finally, in the case when ɑ ≥ L, we see that
∣zκ,n∣ = |1 - 2αL∣nK
1 nK
≥ 1 - 2±L
L
≥ 1nK
=ω⑴.
22
Published as a conference paper at ICLR 2022
C.4 Discussion about initialization
The lower bound partitions the step size in three ranges -
•	In the step size ranges α ∈ [2(n-1)κL,焉]and α ∈ [%, + ], the initializations are done at
the minimizer and it is shown that any permutation-based algorithm will still move away from
the minimizer. The choice of initializing at the minimizer was solely for the convenience of
analysis and calculations, and the proof should work for any other initialization as well.
Furthermore, the effect of initializing at any arbitrary (non-zero) point will decay exponen-
tially fast with epochs anyway. To see how, note that every epoch can be treated as n steps of
full gradient descent and some noise, and hence the full gradient descent part will essentially
keep decreasing the effect of initialization exponentially, and what we would be left with is
the noise in each epoch. Thus, it was more convenient for us to just assume initialization at
the minimizer and only focus on the noise in each epoch.
•	The step size range α ∈ [2(n-1)κL, {] can be divided into two parts, α ∈ [0, 2(n-1)κL]
and α ∈ [L, ∞).
For the range α ∈ [0, 2(n-1)κL], We essentially show that the step size is too small to
make any meaningful progress towards the minimizer. Hence, instead of initializing at 1,
initializing at any other arbitrary (non-zero) point will also give the same slow convergence
rate.
For the range α ∈ [L, ∞), we show that the optimization algorithm will in fact diverge
since the step size is too large. Hence, even here, any other arbitrary (non-zero) point of
initialize will also give divergence.
D Proof of Theorem 3
Define fι(x) := Lx2 - X and f2(x, y) := -Lx2 + x. Thus, F(x, y) = Lx2. This function has
minimizer at x* = 0. For this proof, we will use the convention that Xij is the iterate after the
i-th iteration of the j -th epoch. Further, the number in the superscript will be the scalar power. For
example x2,j = x%,j ∙ x%,j.
Initialize at x。,。= L. Then at epoch k, there are two possible permutations: σ = (1,2) and
σ = (2, 1). In the first case , σ = (1, 2), we get that after the first iteration of the epoch,
xι,k = xo,k - αVf1(x0,k, yo,k)
=(1 — αL)xo,k + α,
Continuing on, in the second iteration, we get
x2,k = x1,k - αVf2 (x1,k, y1,k)
=(1 + 2 αL) x1,k - α
=(l + 2ql) ((1 — aL)xo,k + α) — α
=(1 + 2αL ʌ (1 — aL)xo,k + 2α2L.
Note that x0,k+1 = x2,k. Thus, x0,k+1 =(1 + 2aL) (1 — aL)xo,k + 1 α2L.
Similarly, for the other possible permutation, σ = (2,1),we get x0,k+1 =(1+ 2ɑL) (1 - aL)xo,k +
α2L. Thus, regardless of what permutation we use, we get that
x0,k+1 ≥ (1 + 2QL) (1 - QL)X0,k + 2α2L ≥ (1 - QL)X0,k + 2α2L.
Hence, recalling that we initialized at x。,。= L, we get
xn,K = x0,K+1
23
Published as a conference paper at ICLR 2022
≥ (1 - aL)K1 + 1 α2L X- (1 - aL)i
L2
i=0
L(I
- αL)K
L 2l 1 - (1 - QL)K
2 L 1 - (1 - αL)
L(I - αL)K + 1 α(1 - (1 - αL)K).
L2
(15)
+
Now, if α ≥ KL, then
2α (1 - (1 - QL)K)
and hence, x2nK = Ω(KL). Otherwise, if α ≤ KL, then continuing on from (15),
xn,K
≥ y(1 - QL)K + -q (1 - (1 - QL)K)
L2
≥ L(1 - QL)K
≥ L e-aLK
≥
1
Le
1
Ω(1∕L),
and hence in this case, XnK = Ω(表).
E Proof of Theorem 4
Let F(x) := n Pn=ι fi(x) be SUCh that its minimizer it at the origin. This can be assumed without
loss of generality because we can shift the coordinates appropriately, similar to (Safran & Shamir,
2019). Since the f are convex quadratics, we can write them as fi(x) = 2x>Aix - b>x + c%,
where Ai are symmetric, positive-semidefinite matrices. We can omit the constants ci because they
do not affect the minimizer or the gradients. Because we assume that the minimizer of F(x) is at the
origin, we get that
n
X bi = 0.	(16)
i=1
Let σ = (σ1, σ2, . . . , σn) be the random permutation of (1, 2, . . . , n) generated at the beginning of
the algorithm. Then for k ∈ (1, 2, . . . , K∕2), epoch 2k - 1 sees the n functions in the following
sequence:
(2 x>Aσι x - b>ιx, L x>Aσ2 x - b>2 x, ..., 2 x>Aσn x - ban x),
whereas epoch 2k sees the n functions in the reverse sequence:
(2 x>Aσn x - b>n x, 1 x>Aσn-i x - ^^一应一., j x>Abi x - b^x) ∙
We define Si := QAσi and ti = Qbσi for convenience of notation. We start off by computing the
progress made during an even indexed epoch. Since the even epochs use the reverse permutation, we
get
x2k+1 x2k
x0 = xn
= x2nk-1 - Q Aσ1 x2nk-1 - bσ1	(fσ1 is used at the last iteration of even epochs.)
= (I - QAσ1) x2nk1 + Qbσ1
24
Published as a conference paper at ICLR 2022
= (I - S1)x2nk-1 + t1.
We recursively apply the same procedure as above to the whole epoch to get the following
x02k+1 = (I - S1)x2nk-1 + t1
= (I- SI)((I - S2)xn-2 + t2)+ t1
= (I-S1)(I-S2)x2nk-2+(I-S1)t2+t1
n	n	n-i
Y(I - Si) x02k+X(Y(I-Sj)Itn+1-i,
i=1	i=1 j=1
(17)
where the product of matrices {Mi } is defined as Qim=l Mi = MlMl+1 . . . Mm if m ≥ l and 1
otherwise. Similar to Eq. (17), we can compute the progress made during an odd indexed epoch.
Recall that the only difference will be that the odd indexed epochs see the permutations in the order
(σ1, σ2, . . . , σn) instead of (σn, σn-1, . . . , σ1). After doing the computation, we get the following
equation:
x20k
n	n	n-i
Y(I- Sn-i+l) x2k-1 + X(Y (I - Sn-j + l) I ti.
i=1	i=1 j=1
Combining the results above, we can get the total progress made after the pair of epoch 2k - 1 and
2k:
n	n	n-i
x20k+1 = Y(I - Si) x02k+X Y(I-Sj) tn+1-i
i=1	i=1 j=1
Yn(I-Si)	Yn(I-Sn-i+1) x02k-1
n n-i
+	(I-Sj)	(I- Sn+1-j) ti
j=1
i=1	j=1
n
n	n-i
+X Y(I-Sj) tn+1-i.
i=1 j=1
(18)
In the sum above, the first term will have an exponential decay, hence we need to control the next two
terms. We denote the sum of the terms as z (see the definition below) and we will control its norm
later in this proof.
n
n n-i
n n-i
z:=	(I-Sj)	(I - Sn+1-j) ti+	(I-Sj) tn+1-i
j=1
nn
i=1	j=1
n-i
i=1
j=1
n-i
i=1	j=1
(I-Sj)	(I
- Sn+1-j) ti+	(I-
Sj) tn+1-i.
j=1
i=1	j=1
n
To see where the iterates end up after K epochs, we simply set 2k = K in Eq. 18 and then keep
applying the equation recursively to preceding epochs. Then, we get
xnK = x0K+1
- Si)
- Sn-i+1 ) x0K-1 + z
Yn (I - Si)! Yn (I - Sn-i+1)!! x0K-3 + Yn (I - Si)! Yn (I - Sn-i+1)! z+z
25
Published as a conference paper at ICLR 2022
Yn (I - Si)! Yn (I - Sn-i+1)!!	x01+
X1 ((Yl(I- Si)! (Yl(I- Sn-i+l)!!k z.
k=0	i=1	i=1
Taking squared norms and expectations on both sides, we get
E[kxnKk2] =E
I-Si)
K/2
- Sn-i+1)	x0
+X1 ((Y(I-Tm(I-Sn-,+I)!!」
≤ 2E]((γ(I-Si)!(γ(I-…)!1"
+ 2e[ X1((Y (I - Si)! (fl (I - Sn-i+1)!!kz∣
≤ 2E
I-Si)
(Since (a + b)2 ≤ 2a2 + 2b2)
-Sn"!""]
IK-1
k2
+ 2E
kzk	(I-Si)	(I - Sn-i+1)
k=0
i=1	i=1
n
n
We assumed that the functions fi have L-Lipschitz gradients (Assumption 2). This translates to
Ai having maximum eigenvalue less than L. Hence, if α ≤ 1/L, we get that I - αAi is positive
semi-definite with maximum eigenvalue bounded by 1. Hence, kI - Sik ≤ 1. Using this and the
fact that for matrices M1 and M2, kM1M2 k ≤ kM1 kkM2k, we get that
E [∣∣χΚ∣∣2i ≤ 2e]((Y(I-Si)! (Y(I-Sn-i+ι)!!	X0∣ j
K-1 n n
k2
+ 2El llzIl E	∏kI- SiknkI- Sn-i+ιk
k=0	i=1
i=1
n
≤ 2E l∣((jf (I - Si)! (Y(I - Sn-i+1)!!	x1
K-1 ∖ 2
kzk X 1)
k=0

=2E ]((Y(I	-	Si)!	(Y(I	-	Sn-i+ι)!!	x1|	j +	IKE	[kzk2].
We handle the two terms above separately. For the first term, we have the following bound:
Lemma 3. Ifα ≤ 8KL min{2, √n
, then
n
(I-Si)	(I - Sn-i+1)
i=1	i=1
≤ 1 - αnμ
n
26
Published as a conference paper at ICLR 2022
Note that K ≥ 80κ3/2 log(nK)max{l, √κ} =⇒ α ≤ 81l min {2, √κ }.
We also have the following lemma that bounds the expected squared norm of z .
Lemma 4. If α ≤ L, then
E b∣zk2] ≤ 2n2α4L2(G*)2 + 170n5α6L4G2 logn,
where G* = maxi ∣∣bik, and the expectation is taken over the randomness of Z.
Using these lemmas, we get that
E h∣∣xK∣∣2i ≤ 2(1 一 nαμ)K/2 ∣∣x0∣∣2 + K2n2α4L2G2 + 85K2n5α6L4G2 logn
≤ 2e-2nαμK ∣∣x0∣∣2 + K2n2α4L2G2 + 85K2n5a6L4G2 logn.
Substituting α = 10μgnK gives Us the result.
E.1 Proof of Lemma 3
5T 1 C / 言	言、	/C	C、 一言	言、	/C	C、E
We define (S1, . . .	, Sn)	:= (S1,	. . . , Sn)	and (Sn+1,	. . . , S2n)	:= (Sn, . .	. , S1). Then,
nn
Y(I 一 Si) Y(I 一 Sn-i+1)
i=1	i=1
2n
Y(I 一 Sei)
i=1
2n
I - X Si + X SiSj -...
i=1	i<j
2n
I-XSei+XSeiSej
i=1	i<j
2n
I-XSei+XSeiSej
i=1	i<j
X
SeiSejSek∣ +...
∣i<j<k	∣
+ X ∣∣Sei∣∣∣∣Sej∣∣∣∣Sek∣∣+.........
i<j<k
Note that
n
X Sei Sej =2XSiSj+XSi2
n
- X Si2 .
i=1
Substituting this and noting that Pi2=n1 Sei = 2 Pin=1 Si , we get
nn
Y(I - Si) Y(I - Sn-i+1)
∣n	n	n	n
≤ ∣∣∣I-2XSi+2 XSi	XSi -XSi2
∣	i=1	i=1	i=1	i=1
+ X ∣∣∣Sei∣∣∣∣∣∣Sej∣∣∣∣∣∣Sek∣∣∣+...
i<j<k
n
I-2XSi+2
i=1
n
XSi2
i=1
≤
≤
≤
+ X ∣∣Sei∣∣∣∣Sej∣∣∣∣Sek∣∣+.........
i<j<k
27
Published as a conference paper at ICLR 2022
Let us denote T := Pin=1 Si . Then, we know by Assumptions 2 and 3 that T has eigenvalues in
[ηαμ, naL]. Then, as long as α ≤ 41^, We get that
nn
I-2XSi+2 XSi
i=1	i=1
I-2T+2T2
≤ 1 一 gηαμ.
Substituting this, we get
n
n
n
i=1
(I-Si)	(I - Sn-i+1)
i=1
≤ (l - 2nɑμ
n
+ XSi2
i 1
i=1
+ X	SeiSejSek+.....
i<j<k
By Assumption 2, We knoW that kAi k ≤ L. Hence, kSi k ≤ αL. Hence,
n
n
i=1
(I-Si)	(I - Sn-i+1)
i=1
1 — 3 nαμ) + nα2L2 + ( (2n) α3L3 + (2n) α4L4 + ...
≤
3	2n
1 — —nαμ + n02L2 +	(2nαL)i
1 — 3 nɑμ + nα2L + 网4
2 产	1 — 2naL
1 — —nαμ + nα2L2 + 16n'a3L'.
(Since α ≤ 1/4nL.)
Finally，as long asα ≤ 4KL and α ≤ 8ηL√κ,
≤
≤
≤
n
n
i=1
(I-Si)	(I - Sn-i+1)
≤ 1 一 nαμ.
i=1
E.2 Proof of Lemma 4
We start off by computing the first order expansion of z . We have the following lemma for this:
Lemma 5.
2n-1 j-1 l-1
2n
z =	Siti+	(I-
n-1 j-1 l-1
i=1
j=n+1 l=1	p=1
Sel Sej	ti +	(I-Sp)
j=1 l=1	p=1
SlSj nX-j tn+1-i
n
i




where(S1,...,Sn) := (S1,...,Sn)and(Sn+1,...,S2n) := (Sn,...,S1).
The proof of this lemma is quite algebraic and hence has been pushed to the end, in Appendix E.4.
The strategy is to bound kSitik, k Pi2=n1-j tik, and k Pin=-1j tn+1-ik. Hence, We apply Lemma 5 and
use triangle inequality:
E kzk2
2n-1 j-1 l-1
E	Siti +	(I -
i=1
j=n+1 k=1	p=1
2
n-1 j-1 l-1	n-j
+XX Y(I-Sp) SlSj X tn+1-i
n
i=1
2n-1 j-1	l-1
2n-j
kSikktik+	kI - Sepk kSelkkSejk	ktik
j =n+1 l=1	p=1
i=1
n
28
Published as a conference paper at ICLR 2022
n-1 j-1 l-1
+ XX Y kI-Spk kSlkkSjk
Now, we recall that kSik ≤ αL and kti k ≤ αG. Because α ≤ 1/L, we also get that kI - Sik ≤ 1.
Using these,
n	2n-1 j-1	l-1
E[kzk2] ≤E Xα2LG+ X X Y1 αLαL
i=1	j=n+1 l=1 p=1
n-1 j-1 l-1
+XX Y1 αLαL
j=1 l=1	p=1
2n-1
≤ E I na2LG +2na2L2 X
j=n+1
2n-1
n2α4L2E I I G + 2L X
j=n+1
≤ 2n2α4L2 G2 + L2E
^ I
2n-j
X ti
i=1
n-j
tn+1-i
i=1
2n-j
X ti
i=1
2n-j
X ti
i=1
2n-1
2X
j=n+1
n-1
+ nα2L2 X
j=1
n-1
+LX
j=1
2n-j
X ti
i=1
n-1
+X
j=1
n-j
tn+1-i
i=1
n-j
tn+1-i
i=1
n-j
Σtn+1-i	I .
i=1
(Since (a + b)2 ≤ 2a2 + 2b2)
2
Using Hoeffding-Serfling inequality for bounded random vectors (Schneider, 2016, Theorem 2), we
get the following lemma
Lemma 6. For all j, l ∈ [1, n] we have that
E Xti| j ≤ 18jα2(G*)2 log(n)
E
j
Xti
i=1
l
Xti
i=1
]≤ 18pjlα2(G*),2 log(n),
where G* = maxi ∣∣bik, and the expectation is taken over the randomness of t.
Writing out the expansion of 2 Pj2=n-n+1 1 |||Pi2=n1-j ti||| + Pjn=-11 |||Pin=-1jtn+1-i|||	and using the
lemma above on the individual terms, we get
E h∣z∣2i ≤ 2n2α4L2G2 + 2n2α4L4(90α2n3G2 log n).
E.3 Proof of Lemma 6
This proof is similar to the one in (Ahn et al., 2020). Define G* := maxi ∣∣bi∣. We use the following
theorem adapted to our setting
Theorem 7. [(Schneider, 2016, Theorem 2)] With probability at least 1 一 δ,
j
Xti
i=1
≤ αG j8j(1- jn1 )log2n.
29
Published as a conference paper at ICLR 2022
Then taking a union bound over j = 1, . . . , n, we get that with probability at least 1 - δ,
j
Vj ∈ [1,n] : X ti
i=1
≤ αG* ʌʌjɪ
log 2n ≤ αG* ʌ Aj log2n.
δδ
1 - 3
n
Then, for the complementary event (which happens with probability δ), we use the fact that ∣∣ti∣∣
∣∣αbσa k ≤ αG* to get the following:
j
j
Vj ∈ [1, n]:
∑ti ≤E∣M≤ αG*j.
i=1
i=1
Now, choose δ = 1/n. Then, We get that
j
X ti
i=1
Similarly, we can also get
E.4 PROOF OF LEMMA 5
21
j
X ti
i=1
≤
8j02G2 log(2n2) + ∙^(αG*j)2
n
18jα2(G*)2 logn.
l -
Xti ≤ 18√jlα2(G*)2 log(n).
i=1
As in the lemma,s statement, define (Si,..., S2n) as (Si,..., Sn) = (Si,..., Sn) and
(Sn+ι,..., S2n) = (Sn,..., Si). As a reminder, the term Z is defined as follows:
nn
n-i
n	n-i
Z := E n(I- Sj)	∏(I - Sn+1-j) ti + £ ∏(I - Sj) tn+1-i.	(19)
i=1 ∖j=1
j=i
i=1 ∖j=1
E
≤


E




First, we analyze the first term in z. Towards that end, we start by expanding
(Qn=I (i - Sj)) (Qn-i(i- Sn+」,))：
n-i
∏(I- Sj)	∏(I- Sn+1-j)
j=i
j=i
2n-i
∏ (I -禹)
j=i
(20)
n
2n-i-i
∏ (I - Sj ) J (i - S2n-i
2n-i-i
∏ (I -禹)
j=i
2n-i-i
∏ (I
j=i
Sj ) I S2n-i.
—
Similarly, we expand the term(Q；=-i-1(I - Sj)) and then recursively keep doing it to get the
following:
n-i
∏(I- Sj)	∏(I- Sn+1-j)
j=i
j=i
2n-i-i
∏ (I - &)
j=i
2n-i-i
∏ (I - Sj )1 S2n-i
n
—
2n-i-2
∏ (i - S)
j=i
—
2n-i-2	2n-i-i
∏ (I - Sj)i S2n-i-1- ( ∏ (I - Sj)1 S2n-i
30
Published as a conference paper at ICLR 2022
=(I — Si) - (I - S1)S2 -…
(2n-i-2	∖	2 2n-i-1	∖
∏ (I — Sj)1 S2n-i-1 — ( Y (I — Sj)) S2n-i
2n-i ∕j-1	∖
=I - X n (I - Si) S,
j=1 ∖l=1	)
j-i
Note that the term ∏f=1(I — Sl) above is similar to the RHS of Eq. (20). Hence, We repeat the
process again on this term to get the following
(Y(I - Sj) (∏(I - Sn+1-j)= Using this in the first term Pn=I (Qn=I(I - n In	∖ I n-i	∖ X(∏ (I - Sj )1 (∏(I - Sn+1-j ))： i=i j=i	j=i n	n 2n-i =X ti-XX S	2n-i /	j-1 ∕l-1	∖	∖ I - X I - X ∏(I - Sp) Sl Sj j=1 ∖	l=1 ∖p=1	)) 2n-i	2n-ij-1 /1-1	∖ I - X Sj + XX(Y(I - Sp))SlSj. (21) -Sj)) (Qn-I(I - Sn+i-j)) ti(in Eq. (19)), we get n 2	2n-i	2n-i j-1 ∕l-1	、	∖ ti = X(I- X Sj + xx (Y(I- Sp))SlSjt n 2n-i j-1 l-1 心+XXX Y (I - Sp)SlSjti. i=1 j=1 l=1 p=1
Now, we use the fact that PZi b n	n	n-i XY (I - Sj)	Y(I - S，	=0 (Eq. (16)) to get that PZi t = 0. Then, ∖	n 2n-i	n 2n-i j-1 l-1 n+i-j) ti = -XXSjti + XXX Y(I-Sp) SlSjti
i=i j=i	j=i For convenience, we define Mj XWf)(	i=i j=i	i=i j=i l=i p=i ，:=Pj-IL (Qp=Ii(I- Sp))SlSj. Then, n-i	n 2n-i	n 2n-i Y (I - Sn+i-j )1 ti = - xx Sj ti + xx Mj ti n 2n-i	n n	n 2n-i -	XX Sjti + XXMjti + X X Mjti i=i j=i	i=i j=i	i=i j=n+i n 2n-i	n	n	n 2n-i -	XX Sjti + XMjXti + X X Mjti i=i j=i	j=i	i=i	i=i j=n+i n 2n-i	n 2n-i -	XX Sjti + X X Mjti.	(SincePn=iti = 0)
i=i j=i Note that Pn=i P2=-+i Mjti = P2=-+i Mj X(Y(I - Sj J (Y (I - Sn+i-j )) t i=i j=i	j=i	i=i j=n+i (Pi2=ni-j ti). Hence, n 2n-i	2n-i	2n-j i = - XX Sj ti + X Mj X ti i=i j=i	j=n+i	i=i
31
Published as a conference paper at ICLR 2022
n n	n 2n-i	2n-1	∕2n-j ∖
-	XX & ti- X X S ti + X MM X ti
i=1 j=1	i=1 j=n+1	j=n+1	∖ i=1 )
n	n	n 2n-i	2n-1	∕2n-j	∖
-	XSjXti-X X Sti + X M Xti
j = 1	i=1	i=1 j=n+1	j=n+1	∖ i=1	/
n	2n-i	2n-1	∕2n-j ∖
-	X X Sjti + X Mj	X ti	(Since Pn=1 & = °)
i=1j=n+1	j=n+1	∖i=1	)
n 2n-i	2n-1 j-1 /1-1	∖	∕2n-j ∖
-	X	X	Sjti +	X	X	∏ (I - Sp)	SiSj	X	ti
i=1 j=n+1	j=n+1 l = 1	∖p=1	)	∖ i=1	)
n n	2n-1 j-1	∕l-1	∖	∕2n-j	∖
-	X	X	Sjti +	X	X	∏(I-Sp)	SiSj	X	ti . (22)
i=1 j=i+1	j=n+1 i=1	∖p=1	)	∖ i=1	)
Next We analyze the second term in z. For this, We start by expanding ∏nz∑i(I - Sj) in a similar
way as Eq. (21)
n-i
n-i	n-i j -1	i-1
j=1
Using this, we get
n n-i
X Π(I - Sj) tn+1-i
i=1 j=1
∏(i - Sj)= I - ESj+ ££	∏(I-Sp)	SiSj
j=1	j=1 i=1 p=1
n
n-i	n-i j-1 i-1
n
EI- Σ>j	+ EE	n(I- Sp)	SiSj	tn+1-i
i=1	j=1 j=1 i=1	p=1
n n-i	n n-i j-1	i-1
(23)
=	tn+1-i - ΣΣSjtn+1-i +ΣΣΣ(^Π(I - ST Sj tn+1-i
n n-i	n n-i j-1 i-1
=-XXSjtn+1-i + XXXin (i -Sp)) SiSjtn+1-i,
where we used the fact that Pn=I t = 0 in the last equality. For convenience, we define Mj :=
Pj-IL (∏p=1(I- Sp))SiSj. Then,
n	n-i	n n-i	n n-i
X∣∏(I - Sj )∣tn+1-i =- XX
Sjtn+1-i + XX
Mj tn+1-i.
i=1 j=1	i=1 j=1	i=1 j=1
Since Pn=1 Pn-i Mjtn+1-i
Pn-IL Mj (pn=1j tn+1-i),weget
n	n-i
n n-i
E n(I- Sj ) tn+1-i = -E ESj tn-
i=1 j=1	i=1 j=1
n i-1
n-1	n-j
+ 1-i + X Mj	X tn+1-i
j=1	i=1
n-1
-E ∑>j ti + ∑ Mj
i=1 j=1
n i-1
j=1
n-1
-E ∑>j ti + ∑ Mj
i=1 j=1
n i-1
j=1
n-1 j-1
(X tn+1-)
(Xtn+1-i)
i-1
-E∑Sjti + ΣΣ ∏(i - Sp)
i=1 j=1
j=1 i=1	p=1
)SiSj(X tn+1-i).
i	(24)
32
Published as a conference paper at ICLR 2022
Finally, substituting Eq. (22) and (24) in the definition of z (Eq. (19)), we get
n n	2n-1 j-1 l-1	2n-j
z=-XX Sjti+ X X	Y(I-Sep)	SelSej	Xti
i=1 j=i+1	j=n+1 l=1	p=1	i=1
n i-1	n-1 j-1	l-1
-XXSjti+XX Y(I-Sp)	SlSj
tn+1-i
n	i-1	n
-X(XSj + X SjJti
i=1	j=1	j=i+1
2n-1 j-1	l-1	2n-j	n-1 j-1	l-1
+ XX	Y(I-Sep)	SelSej	Xti +XX Y(I-Sp)	SlSj
j=n+1 l=1	p=1	i=1	j=1 l=1	p=1
nn	n
-X XSj	ti+XSiti
i=1 j=1	i=1
2n-1 j-1	l-1	2n-j	n-1 j-1	l-1
+ XX	Y(I-Sep)	SelSej	Xti +XX Y(I-Sp)	SlSj
j=n+1 l=1	p=1	i=1	j=1 l=1	p=1
n	nn
X Sj IX ti + X Siti
2n-1 j-1	l-1	2n-j	n-1 j-1	l-1
+ XX	Y(I-Sep)	SelSej	Xti +XX Y(I-Sp)	SlSj
j=n+1 l=1	p=1	i=1	j=1 l=1	p=1
tn+1-i
tn+1-i
n	2n-1 j-1	l-1	2n-j	n-1 j-1	l-1
XSiti+ X X Y(I-Sep)	SelSej	Xti	+XX Y(I-Sp)	SlSj
i=1	j=n+1 l=1	p=1	i=1	j=1 l=1	p=1
nX-j tn+1-i
where we used the fact that Pin=1 ti = 0 in the last equality.
F Proof of Theorem 5
The proof is similar to that of Theorem 4, except for that here we leverage the independence of
random permutations in every other epoch. The setup is also the same as Theorem 4, but we explain
it again here nevertheless, for the completeness of this proof.
Let F(x) := ^ pn=1 fi(x) be such that its minimizer it at the origin. This can be assumed without
loss of generality because we can shift the coordinates appropriately, similar to (Safran & Shamir,
2019). Since the f are convex quadratics, we can write them as fi(x) = 2x> Aix - b>x + ci,
where Ai are symmetric, positive-semidefinite matrices. We can omit the constants ci because they
do not affect the minimizer or the gradients. Because we assume that the minimizer of F(x) is at the
origin, we get that
n
Xbi=0.	(25)
i=1
Let σk = (σ1k, σ2k, . . . , σnk) be the random permutation of (1, 2, . . . , n) sampled in epoch 2k - 1.
Then epoch 2k - 1 sees the n functions in the reverse sequence:
2x x›Aσk x - b>kx, 1 x>Aσk x - b>kx,…,1 x>Aσk x - b> x^ ,
whereas epoch 2k sees the n functions in the reverse sequence:
(2x>Aσkx - b>kx,1 x>Aσk-1x - b>k_ιx,…，1 x›Aσkx - b>kx).
33
Published as a conference paper at ICLR 2022
We define Sik := αAσk and tik = αbσk for convenience of notation. We start off by computing the
progress made duting an even indexed epoch. Since the even epochs use the reverse permutation of
σk, we get
2k+1
x0	= x
x
2k
n
2nk-1 - α	Aσ1k x2nk-1 - bσ1k
(fσk used at the last iteration of epoch 2k.)
= I - αAσ1k x2nk-1 + αbσ1k
= (I- S1k)x2nk-1 +t1k.
We recursively apply the same procedure as above to the whole epoch to get the following
x20k+1 = (I - S1k)x2nk-1 + t1k
= (I- Sk)((i - s2)χn-2+tk)+tk
= (I - S1k)(I - S2k)x2nk-2 + (I - S1)t2k + t1k
n	n-i
x02k+X Y (I - Sjk)	tkn+1-i,
(26)
i=1 j=1
where the product of matrices {Mi } is defined as Qim=l Mi = MlMl+1 . . . Mm if m ≥ l and 1
otherwise. Similar to Eq. (26), we can compute the progress made during an odd indexed epoch.
Recall that the only difference will be that the odd indexed epochs see the permutations in the order
(σ1k, σ2k, . . . , σnk) instead of (σnk, σnk-1, . . . , σ1k). After doing the computation, we get the following
equation:
n	n	n-i
x20k = Y(I - Snk-i+1) x20k-1 + X Y(I - Snk-j+1) tik.
i=1	i=1 j=1
Combining the results above, we can get the total progress made after the pair of epoch 2k - 1 and
2k:
n	n	n-i
x20k+1 =	Y(I - Sik)	x20k+X Y(I-Sjk)	tkn+1-i
i=1	i=1	j=1
Yn (I - Sik)	Yn (I - Snk-i+1)
n	n-i
+X	Y(I-Sjk)	tkn+1-i.
i=1	j=1
n	n-i
x02k-1 +	(I - Sjk)	(I-Snk+1-j)	tik
j=1
i=1	j=1
(27)
n
In the sum above, the first term will have an exponential decay, hence we need to control the next two
terms. Similar to Theorem 4, we denote the sum of the terms as zk:
n
n n-i
n	n-i
zk :=	(I -	Sjk)	(I	- Snk+1-j)	tik+	(I-Sjk)	tkn+1-i
j=1
nn
i=1	j=1
n-i
i=1
j=1
n-i
i=1	j=1
(I - Sjk)	(I-Snk+1-j)	tik+	(I - Sjk)	tkn+1-i.
j=1
i=1	j=1
n
To see where the iterates end up after K epochs, we simply set 2k = K in Eq. 27 and then keep
applying the equation recursively to preceding epochs. Then, we get
XK = xK+1 = (Y(I - Si2)! (Y⅛I - sn-i+1)! xK-1 + Z夸
34
Published as a conference paper at ICLR 2022
K 口 (I- SiK)乂口 (I- s>))(m (I- SK-B(口 (I- s>kj))
x
K-3
0
=(Y1 (YI(I- Si-k)) (Yl(I- SK-+ 1)0x1
∖ k=0 ∖i=1	) ∖i=1	))
+ X1	[∏	(1(I- SsT))	(Y(I- Sk-+1)))	ZS-k
k=0	∖l=0	∖i=l	)	∖i=l	))
Taking squared norms and expectations on both sides, We get
E[∣∣xK k2]= E
Z S-k[
2
+ X1 (∏ (Π(I- SiST)) (Π(I- Ss-+ 1)))
k=0 ∖l=0 ∖i=1	) ∖i=1	))
≤2E(ImIm(I-SiS-k ))(n(I-Si-+I))M
+ 2E
(I XImm(I-SS - Dm(I-SH 1)))Z K-I
(28)
where We used the fact that (a + b)2 ≤ 2a2 + 2b2. Next, We expand the second term above to get
E[∣∣xK k2] ≤ 2E(|(n (∏(I-SiS-k)) (∏(I-SK-- 1))j x0l
S-1
+ 2 X E
k=0
(∏1(∏(I-SiST)) (∏(I-SM-1))) ZS-kl2
+4 E
0≤k0<k≤ S-1
∕k0-1
(∏
∖ l=0
E
∖(∏ (∏(I - SiST)Xn(I - Si-+1)) ) ZS-k
(29)
We handle each of the three terms separately.	Let	Nk
(∏n=1(I - Si-k)) (q21(I - Sη--+1)). The first term can be written as:
S-1	∖
∏ Nk I x0
k=0	)
35
Published as a conference paper at ICLR 2022
夸-1	∖τ I 夸-1
∏M)	∣∏Nk
k=0	)	∖ k=0
K-1	∖τ	/ K-1
∏ Nk ) N0T No I ɪɪ Nk
k = 1	)	∖k = 1
K-1 ʌ ɪ	(K-1
∏ Nk I E [NT No ] I U
k = 1	)	∖ k=1
where, in the last line, We used the fact that the permutations in every epoch are independent of the
permutations in other epochs. Next, we have the following lemma that bounds the spectral norm of
E [N> Nk ] for any k ∈ [°,与-1]:
Lemma 7. For any 0 ≤ α ≤ ɪgnɪ min(1, ʌ/ɪɪ},
∣∣E[N>Nk]∣∣ ≤ 1 - αnμ
Note that K ≥ 55κlog(nK)max{1, -v^n} =⇒ α ≤ 1^^ min{1, n}, and thus this lemma.
Using it, we get
E
E (x0)τ ∣ ∏ Nkj	E [NTNo] I U Nk) x1
≤ (1 — nαμ)E
(1 — nαμ)E
(x0)τ( UN) I UNk卜
(x0)T ( Π Nk I E [N>N1] I ∏ Nk I x0
∖ k=2 J	∖ k=2 J
≤ (1 - nαμ)K∕2∣∣χ0k2.
(30)
Next we look at the second term in Ineq. (29):
k-1
∏
.l = 0
-SltL J)
E
≤E
≤E
k-1
∏	∏∣∣i-SiK-lk2	∏∣∣i- SltiI 1k2 kz善-kk2
,l=0	∖i=1	i ∖i=1	))
where in the last step we used that ∣∣1 - Skk ≤ 1 for all i, k. To see why this is true, recall that
Sk = αAσk. Further by Assumption 2, ∣Aσk k ≤ L and hence as long as α ≤ 1 /L, we have that
IlI- Skk ≤ 1.
36
Published as a conference paper at ICLR 2022
Next, note that for any k We can apply Lemma 4 on E[∣∣zK2-k ∣∣2]. Hence, We get the following bound
on the second term:
2 £ e[ (YI (∏(I-SiK2 T)) (∏(I-Si-+1)!! z K2-k |21
k=0	l=0 i=1	i=1
≤ 2K(2n2α4L2(G*)2 + 170n5α6L4(G*)2 logn).	(31)
Finally, We focus on the third term in Ineq. (29). We have the folloWing lemma that gives an upper
bound for it:
Lemma 8. Let α ≤ 21L and n > 6. Thenfor 0 ≤ k0 < k ≤ KK 一 1,
E "*(∏ (Y(I- SK-l)) (Y(I- SK-+1))) ZK2-k,
(YI(Y(I- SiK2T))(Y(I- SK-+1))) Z夸-k0 +
≤ 1000n2α4L2(G*)2 + 2000n6α7L5(G*)2 logn.
Using Lemma 8, and inequalities (30) and (31) in Ineq. (29), We get
E[∣∣xKk2] ≤ 2(1 - nαμ)Κ7∣x0k2 +2n2Ka4L2(G*)2 + 170n5Ka6L4(G*)2 logn
+ 1000n2K2α4L2(G*)2 + 2000n6K2α7L5(G*)2 logn
≤ 2(1 - nαμ)ΚS∣∣x1k2 + 1002n2Ka4L2(G*)2 + 2170n6K2α7L4(G*)2 logn.
Substituting α = 10μθgΚnκ gives Us the desired result.
F.1 Proof of Lemma 7
Recall that we have defined Nk := (Qn=I(I - Si-k)) (Qn=I(I - Snn--+ι)). Hence,
N> =((Y(I- SiK-k)) (Y(I- SK-+1)))
=(Y (I - Si--+ι))>(Y (I - SiK-k))>
= (Y(I- SiK-k)>) (Y(I- SK-+ι)>)
= (Y(I- SiK-k)) (Y(I- SK-+ι))
= Nk ,
IICICK-k
where we used the fact that Si2 are symmetric. Hence Nk is symmetric. Then,
kE[Nk>Nk]k = max E[x>Nk>Nkx]
x:kxk=1
= max E[x> Nk Nk x].
x:kxk=1
Next, note that ∣Nk ∣∣ ≤ 1 as long as α ≤ nL. This combined with the fact that Nk is symmetric
gives us that x>NkNkx ≤ x>Nkx. Hence, we get
∣E[Nk>Nk]∣ ≤ max E[x> Nk x]
x:kxk=1
37
Published as a conference paper at ICLR 2022
kE[Nk]k
e](⅛i- Si2-k)) YTI(I - SK-+ι))
To complete the proof, we apply the following lemma from Ahn et al. (2020):
Lemma 9 (Lemma 6, Ahn et al. (2020)). For any 0 ≤ α ≤ ^6^ min{1, K} and k ∈ [K],
≤ 1 - αnμ.
Remark 2. To avoid confusion, we remark here that, the term ‘Sk ’ in the paper Ahn et al. (2020) is
the same as the term ‘ in=1(I - Sik)’ in our paper, and hence the original lemma statement in their
paper looks different from what is written above.
F.2 Proof of Lemma 8
We begin by decomposing the product into product of independent terms, similar to proof of Lemma
8 in Ahn et al. (2020). However, after that we diverge from their proof since we use FlipFlop
specific analysis.
E
E
ZKF-k)>(H (Y(I-SiK2T)) (YY(I-SM+1)!!>
l=k0 i=1	i=1
k0	n	n
Π Π(I- Sf T)	Π(I- Sn--+1)
l=0 i=1	i=1
>
k0-1 n	n
∏	∏(I - SiK2T)	∏(I - Sl--+1)) Z夸-k0
l=0	i=1	i=1
SinCe k0 < k, we get that (Z琮-k)>, (Qk=k10 (Qi=I(I- Si2T))(Qi=1 (I- SK-+1))) and
(Π (Il(I- SiK2T)) (Il(I- SK-+1)))>(∏ (Il(I- SiK2T)) (Π(I- SK-+1))) ZK2 -
l=0 i=1	i=1	l=0	i=1	i=1
are independent. HenCe, we Can write the expeCtation as produCt of expeCtations:
E

38
Published as a conference paper at ICLR 2022
E
k
π
l = 0
E (zK2-k
K-+1)))
-SS-+1)	Z夸-k0
k0-1
π
-S
∖ l=0
Applying the Cauchy-Schwarz inequality on the decomposition above, We get
E
k-1
.l=0
(k'-1
π
l=0
E
-SiK2T)Xn(I- SK-+1)) )zK-k,
-Si-l))(∏(I - SK-+ 1)))zK2-k0 )
≤∣∣E [z K2-k ill
E
k+1	n	n
∏	∏(I- S2-l)	∏(I- SE+1)
,l = k0 ∖i=1	) ∖i=1	人
k0	Z n	∖	/ n	∖
∏ ∏(I-靖-l)	∏(I- sI-+ ι)
l=0 ∖i=1	) ∖i=1	)
T
k0-1
π
l=0
-S?-l)) (∏(I - Sm J )z夸-k0
≤∣∣e [z K2 - illE π ∏kI
E
≤
k+1	n	n
-SiK2Tk) (∏ ∣I- SK-+1 3
T
-SiK2T)Xn(I- SK-+ J )
.l = k0 ∖i=1
k0
∏
l=0
(k∏1(∏(I- SiK2T)Xn(I - Sti+1)"z夸-k0
E [zK2-I11
E
k'	Z n	∖	/ n	∖
π ∏(I- s2-l)	∏(I- Sn--+1)
l=0 ∖i=1	) ∖i=1	)
T
(k∏1 - SS TMn(I-sn2--+1))卜署-k]∣∣,
where in the last step we used that ∣∣I - Skk ≤ 1 for all i, k. To see why this is true, recall that
Sk = αAσk. Further by Assumption 2, ∣Aσk ∣∣ ≤ L and hence as long as α ≤ 1/L, we have
IlI- Skk ≤ 1.
For the two terms in the product above, we have the following lemma:
Lemma 10. If n > 6 and α ≤ nL, then
∣∣E[zK2-k]∣ ≤ 28nα2LG* +9α5L4n4G*√2nlogn.
39
Published as a conference paper at ICLR 2022
Lemma 11. If n > 6 and α ≤ 21^, then
≤ 32na2LG* + 24α5L4n4G*,2nlogn.
Finally, using these lemma we get
E "*m (Y(I- SKT)) (Y(I - SK-+1)!! ZK2-k,
(kY1(γ (I-峭-ι))(γ (I- SK-+J卜 K2-k0+
≤ 08nα2LG* +9α5L4n4G* p2nlogn) ∙ (32na2LG* + 24α5L4n4G*，2nlogn)
≤ 896n2α4L2(G*)2 + 960α7L5n5(G*)2，2nlog n + 432a10L8n9(G*)2 logn
≤ 1000n2α4L2(G*)2 + 2000a7L5n6(G*)2 logn.
F.3 Proof of Lemma 10
Since we are dealing with just a single epoch, we will skip the superscript. Using Lemma 5, we get
2n-1 j-1 l-1
kE[z]k
i=1
Siti +E	(I -
j =n+1 l=1	p=1
2n-j
Sep)	Sel Sej	X ti
E
n
+E
n-1j-1 l-1	n-j
XX Y (I-Sp) SlSj X tn+1-i
j=1 l=1 p=1	i=1
≤	E[kSikktik] +
i=1
n-1 j-1
+Xj=1Xl=1
2n-1 j-1
j=n+1 l=1
E	plY-=11(I -
l-1	n-j
Y (I-Sp) SlSj X tn+1-i .
p=1	i=1
(32)
n
E
Define G* := maxi 他||. Then |回||||&|| = ∣∣αAσk IllIabσk k ≤ α2LG* and hence
n
XE[∣Sikktik] ≤ nα2LGt.	(33)
i=1
Next we bound the other two terms. Using Eq. (23), we get that for any l < j ,
E	lY-1(I-Sp) SlSj nX-j tn+1-i
l-1	l-1 p-1 q-1	n-j
=E I-	Sp+	(I-Sq) SrSp SlSj	tn+1-i
p=1	p=1 q=1 r=1	i=1
40
Published as a conference paper at ICLR 2022
n-j	l-1
X E [SlSjtn+1-i] - E X Sp Sl Sj
i=1	p=1
nX-j tn+1-i
+E
l-1 p-1 q-1
(I-Sq)	SrSpSlSj
p=1 q=1	r=1
nXi=-1j tn+1-i
l-1 p-1	q-1
E[SlSjti] -	E[SpSlSjti] +	E (I - Sq)	SrSpSlSj
i>j,l	p<l,j<i	p=1 q=1	r=1
nXi=-1j tn+1-i
E[SlSjE[ti|Sl,Sj]] - E[SpSlSjE[ti|Sl,Sj,Sp]]
i>j,l	p<l,j<i
l-1 p-1	q-1
+XXE	Y(I
- Sq)	SrSpSlSj
p=1 q=1	r=1
nXi=-1j tn+1-i
Since Pn=ι t = 0, and We use uniform random permutations, E[t∕Sι, Sj] = P%=tι, nt⅛
ti6=tj
—j2t. Similarly, E[ti∣Sι, Sj, Sp] = —3-IJP. Hence,
lY—1(I-Sp) SlSj nX—j tn+1—i
≤ kE[SlSjE[ti|Sl,Sj]]k+	kE[SpSlSjE[ti|Sl,Sj,Sp]]k
i>j,l
l—1 p—1
p<l,j<i
q—1
+	E II - SqI ISr IISpIISl IISj I
p=1 q=1
r=1
≤ x EhSlkkSj kkn-j
i>j,l
l—1 p—1
q—1
+E
p<l,j<i
n—j
tn+1—i
i=1
kSpkkSlkkSjk
Iltl + tj + tpk
n-3
+	E II - Sq I ISr IISp IISl IISj I
n-j
tn+1—i
p=1 q=1
r=1
≤ n⅛ a"
3n2
2(n-3)
i=1
l-1 p-1
a4L3Gr- + α4L4	E
n-j
tn+1—i
p=1 q=1
i=1
(Since kSi k ≤ αL, kti
≤ 4α3L2G* + 3na4L3G* + a5L4n2G*Pl8n log n,
≤ αG*)
(34)
E
Where We used Lemma 6 and the assumption that n > 6 in the last step.
The third term in Ineq. (32) can be handled similarly. For any l, j:
l-1
2n-j
E (I - Sep)	SelSej	ti
p=1
i=1
))
l-1
l—1 p—1 q—1
2n-j
E I-	Sep+	(I-Seq) SerSep	SelSej	ti
p=1
2n-j
p=1 q=1 r=1
l—1 2n—j
i=1
))
l-1 p-1 q-1
XE SelSejti -XXE SepSelSejti +E XX Y(I-
SerSepSelSej
i=1
p=1 i=1
p=1 q=1 r=1
2n—j
Xi=1 ti	.
NoW, it is easy to see that i 6= j and i 6= 2n - j + 1. Then, if i = l or i = 2n - l + 1 We
use for that case the fact that ∣∣E[SlSjti]k ≤ α3L2G*. For all other i, we can again use that
-.~ ~■-	+.+.	-.∙~ ~■-
E[ti∣Sl, Sj] = —n-j if l ≤ n or E[ti∣Sl, Sj]
-t2n-i + ι-j
n—2
otherWise. Similarly, We can bound
41
Published as a conference paper at ICLR 2022
E SepSelSejti . Proceeding in a way similar to how we derived Ineq. (34), we get
E
l-1
∏(I-
p=1
2n-j	l-1 2n-j
≤ XEhSelSejtii+XXEhSepSelSejtii
i=1
l-1 p-1
+XX
p=1 q=1
p=1 i=1
q-1
Y (I-Seq)
r=1
2n-j
SerSepSelSej	X ti
≤ O3L2Gt + -2n-α3L2G* + α4L3G* + na4L3G* +
n-2
3n2
2(n-3)
α4L3G*
l-1 p-1
+XX
p=1 q=1
q-1
r=1
(I-Seq)	SerSepSelSej
2n-j
i=1
ti
E
E
≤ 5α3L2G* +5nα4L3G*
+
l-1 p-1
q-1
p=1 q=1
E kI - Seqk
r=1
2n-j
∣Ser ∣∣Sep ∣∣Sel ∣∣Sej ∣	ti

≤ 5α3L2G* + 5nα4L3G* + α5L4n2G*√18nlogn.
Substituting Ineq. (33), (34) and (35) into (32), we get
∣∣E[z]k ≤ nα2LG* + 10n2ɑ3L2G* + 10n3ɑ4L3G* +2ɑ5L4n4G*，18nlogn
+ 4n2a3L2G* + 3n3α4L3G* + α5L4n4G*/18n log n
≤ 28na2LG* + 9α5L4n4G* P2n log n,
where We used the assumption that α ≤ nL in the last step.
(35)
F.4 Proof of Lemma 11
Define the matrix M as
M:
k0-1
Y
l=0
-SK-B m(I - Sn--+1))j (∏1 (Y⅛I - SK-B m(I - SnK-+1)
Since M is independent of (Qn=I(I - Si-k
tower rule, we get
K-+1)) and ZK2-k0, using the
k0-1 n
Y	Y(I-
l=0	i=1
0
E∖[h m(I - SK-rm(I - SnK-+J
E
K
I - Si2
-Sn2-i+1)))	E[M]z夸-k0
We will now drop the superscript K2 - k0 for convenience. Hence, we need to control the following
term:
E
I-Si)!Yn (I- Sn-i+1) !! E[M]z
42
Published as a conference paper at ICLR 2022






We define (S1,..., S2n) as (S1,..., Sn) = (S1,..., Sn) and (Sn+ι,..., S2n) = ISm…,S1).
Then, We use Eq. (21) to get
((Y(I - Sj(Y(I - Sn-i+1))) = Y(I - Si)
2n
2n j-1 l-1
I-ES-+ΣΣ ∏(i-
j=1
j=1 l=1 ∖p=1
SI Sj.
Note that I — Pj=1
Sj is a constant matrix. Since Sj = αAσj, we have that ∣∣Sj∣∣ ≤ αL by
Assumption 2. Hence, α ≤ 2⅛ then IIi - P2=1 SjIl ≤ 1. Further, α ≤ 1/L implies that
∣∣I - Sik ≤ 1, which implies ∣∣M∣∣ ≤ 1. Hence,


)(Y(I - Sn-i+1))) E[M]z]∣
T
E[M ]z
T
E
+
2n j-1 l-1
XX Y (I - Sp))SlSj	E[M]z
j=1 l=1 ∖p=1	)	)
T
2n
I - X SjJ	E[M]E[z]
2n j-1 l-1
X X (YI(I-
T
E[M ]z
E
+
≤kE[z]∣∣ +
2n j-1 l-1
X X W-
T
E[M ]z
E
We can apply Lemma 10 to bound ∣∣E [z]∣. So, we focus on the other term. Using Lemma 5,
T
E
≤
E
+
+
2n j-1 ∕l-1	∖	∖
XXY (I -瓦))SlSj	E[M]z
j=1 l=1 ∖p=1	)	)
E
E
E
+
E
2n j-1 ∕l-1
X X W-
T
E[M] (X Sit)
2n j-1 ∕l-1
X X W-
2n-1 j-1 l-1
E[M] I X X ∏ (I -
∖j=n⅛1 l = 1 ∖p=1
T
T
2n j-1 ∕l-1
X X W-
n-1 j-1 l-1	n-j
SiSj j E[M] (XX (Y(I - Spζ∣ SiSj ^x tn+1-i
T
2n j-1 ∕l-1
X X W-
SiSj
E[M] (X Siti
2n j-1 (l-1
X X W-
2n-1 j-1 l-1
EM] ∣ X X Y (I -
∖j=n⅛1 l = 1 ∖p=1
T
43
Published as a conference paper at ICLR 2022
T
2n j-1 l-1
XX (∏1(I-
n-1 j-1 l-1	n-j
SlSjJ e[m] ∩q]q "I - SP)ISlS	tn+-
2n j-1 ∕l-1
EEnkI-即 临IMjk e[∣∣m∣∣] ESiti
j=1 l=1 ∖p=1
i=1
2n j-1 ∕l-1
2n-1 j-1 ∕l-1
2n-j
+E
+E
EEnkI- Spk kSlkkSjk	e[∣∣m∣∣]	E EnkI- SPk	kSlkkSjk ∑>
j = 1 l = 1 ∖p=1
2n j-1 ∕l-1
j=n+1 l=1 ∖p=1
n-1j-1 ∕l-1
i=1
EE ∏ kI-Spk kSlkkS-k e[∣mk] EE ∏ kI-Spk kskkSjk Etn+
1-i
j = 1 l = 1 ∖p=1
j = 1 l = 1 ∖p=1
i=1
+
‹ E
E









n



-—■
-—■

.... .. _ _. ..	.. _ .. _	..*~ ..	...
Now, WeUSethatkMk ≤ 1, kI 一 Sik ≤ 1, ISk ≤ αL and ∣∣tik ≤ αG*:
T
E
2n j-1 l-1
XX (∏1(I-
~ ~ 1 - -
Sl Sj	E[M ]z
< 4n3α4L3G* + 4n4α4L4E
XS J] + n4α4L4E
n-j
Etn+1-i
i=1
Using Lemma 6, we get
T
E
2n j-1 l-1
XX (Π1(I-
SlSj
E[M]z I I I < 4n3α4L3G* + 15n4α5L4G* √2nlogn.
Putting everything together,
E
k0
π
l=0
-Si2-l) (Π(I- SK-+1))
T
k0-1 n n	∖ / n	∖ ∖
π ∏(I - SiK2-l)∏(I - Snt-+1)) ZKK-k0
l=0 i=1	i=1
< (28nα2LG* + 9α5L4n4G* √2n log n) + (4n3α4L3G* + 15n4α5L4G* √2n log n)
< 32nα2LG* + 24α5L4n4G* √2n log n.
G Proof of Theorem 6
Proof. We start off by defining the error term
⅛n	∖	/ n	n
Wi(X2--1) - X v/i (χ0k-1) + X Vfn-i+1 (x2-1) - X Vfn-i+1 (χ0k-1
i=1	i=1	i=1
where k ∈ [K∕2]. This captures the difference between true gradients that the algorithms observes,
and the gradients that a full step of gradient descent would have seen.
For two consecutive epochs of FLIPFLOP with Incremental GD, we have the following inequality
kxnk - x*k2 = kx2k-1 - x*k2 - 2α ( x2k-1 - x*
nn
X Vfi (x2--1) + X Vfn-i+1
i=1	i=1
44
Published as a conference paper at ICLR 2022
n	n
+ α2 X Vfi (<fci1) + X Vfn-i+1 (x2-1) I I
i=1	i=1
=I∣x2k-1 - x*∣∣2 - 2α(x0k-1 - x*, 2nVF(x2k-1))
-2α(x2k-1 — x*, rk) + α2 ∣∣2nVF(x0k-1) + rk∣∣2
≤ ∣x0k-1 - x*k2 - 4nα ［达|∕-1 - x*∣∣2 + L+μ ∣∣VF 国一)『
-	2α(x2k-1 - x*, rk) + α2 ∣∣2nVF(x2k-1) + rk∣∣
≤ ∣x0k-1 - x*k2 - 4nα ［件尻1 -x*∣∣2 + L+μ ∣∣VF 国一)『
-	2α(x2k-1 - x*, rk) + 8α2n2∣∣VF(x0k-1) ∣∣2 + 2α2 ∣∣rk ∣∣2
=(1 - 4nɑL+μ) ∣χ0k-1 - χ*k2 - (4nαL+μ - 8α2n2) ∣∣VF(x2k-1)∣∣2
-	2α (x2k-1 - x*, rk) + 2α2∣∣rk∣∣2 ,	(36)
where the first inequality is due to Theorem 2.1.11 in Nesterov (2004) and the second one is simply
(a + b)2 ≤ 2a2 + 2b2.
What remains to be done is to bound the two terms with rk dependence. Firstly, we give a bound on
the norm of rk:
∣ ∣ / n	n	n	/ n	n
krkk = ∣∣ X Vfi (x2--1) - X Vfi (X0k-1) + X Vfn-i+1 (x2-1) - X Vfn-i+1 (x2k-1)
∣ ∣ ∖i=1	i=1	)	∖i=1	i=1
nn
≤ X ∣∣Vfi (x2--1) - Vfi(X2k-1) ∣∣+ X ∣∣Vfn-i+1 (x2-1) - Vfn-i+1 (x2k-1) ∣∣.
i=1	i=1
Next, we will use the smoothness assumption and bounded gradients property (Lemma 1).
nn
krkk ≤ LX∣∣x2--1 - x2k-1∣∣ + LX∣∣x2-1 -x0k-1∣∣
i=1	i=1
nn
≤ LGα X i + LGα X(n + i)
=n(2n — 1)αGL.
Hence,
∣∣rk∣∣2 ≤ 4n4α2G2L2.
(37)
For the rk term, we need a more careful bound. Since the Hessian is constant for quadratic functions,
we use Hi to denote the Hessian matrix of function fi(∙). We start off by using the definition of rk:
⅛n	∖	/ n	n
Vfi (X2--1) - X Vfi (X2k-1)	+ X Vfn-i+1 (X2-1) - X Vfn-i+1 国一)
i=1	i	∖i=1	i=1	,
nn
=X (Vfi (x2--1) - Vfi (x2k-1)) + X (Vfn-i+1 (x2-1) - Vfn-i+1 阳1))
i=1	i=1
nn
=X Hi(x2--1-X0k-1) + X Hn-i+1 (x2-1 - x0k-1),
i=1	i=1
where we used the fact that for a quadratic function f with Hessian H, we have that Vf (X)-Vf (y)=
H(X - y). After that, we express x2--1 - x0k-1 and x2-1 - XOkT as sum of gradient descent
45
Published as a conference paper at ICLR 2022
steps:
n	i i— 1	∖	n	I n	i— 1
rk = X Hi X -αV∕j (x2——1) + X Hn—i+1 X -αVf∙ (j-1) + X-αV%-小㈤—J
i=1	∖j=1	)	i=1	∖j=1	j=1
n	i i— 1	∖	n	i i—1
-a X Hi	X Vfj (j—1)	- α X /—中	X Vfn—升1(若—1)
i=1	∖j=1	i	i=1	∖j = 1
nn
-α X Hn—i+1 X Vfj (x|——1)
i=1	∖j=1
n	i—1	n	i—1
-α X Hi	X Vfj (x2k-1)	- α X /—中 X Vj")
i=1	∖j=1	i	i=1	∖j = 1
nn
-	α X Hn—i+1 X Vfj (x0k-1)
i=1	V=1	)
n	i—1
-	a X Hi X Vfj (j—1) - fj (x2k-1)
i=1	∖j = 1	)
n	i—1
-	α X Hn—i+1 X Vfn—j+1 (Xj—1)- Vfn—j + 1(X2k)
i=1	V=1	.
nn
-	α X Hn—i+1 X Vfj (*1)-Vfj (xjk-1)
i=1	j=1
nn	n
-	2α X Hi X Vfj (x0k—1)	+α X HiVfi(Xjk—1)
i=1	j=1	i=1
n	i—1
-	a X Hi X Vfj (xj——1) - fj (x0k-1)
i=1	j=1
n	i—1
-	α X Hn—i+1	X Vfn—j+1 (Xj—1)- Vfn—j + 1(xjk)
i=1	j=1
nn
-αXHi	X Vfj(xj——1) -Vfj(XjkT)
i=1	j=1
Next, We use the fact that Pn=1 Vfj(x) = nVF(x). We will also again use the fact that for a
quadratic function f with Hessian H, we have that Vf (x) - Vf (y) = H(x - y):
nn	n
rk = -2αXHi(nVF(XjkT))+ αXHi(Vfi(X0k-1)-Vfi(X*)) + αXHiVfi(X*)
i=1	i=1	i=1
n	i—1
-α X Hi X Vfj (Xj——1) - fj(XjkT)
i=1	j=1
n	i—1
-α ^X Hn—i+1	X Vfn—j+1 (Xj— 1) - Vfn—j + 1(Xjk)
i=1	j=1
46
Published as a conference paper at ICLR 2022
nn
-α X Hi X Vfj (χ2--1) -Vfj (x0k-1)
i=1	V = 1	)
⅛∖ 2	n	n
Hi	(x2k-1 - x*) + α X Hi2(x0k-1 - x*) + α X HiVfi(X*)
i=1	i=1
n / i-1	∖
-a X Hi X Vfj (χ2--1) - f (x0k-1)
i=1	j=1
n	/ i-1	∖
-α X Hn-i+1 X Vfn-j+1(X2-I)-Vfn-j + 1(X2k)
i=1	j=1
nn
-α X Hi X Vfj (x2--1) -Vfj (x0k-1)
i=1	j=1
=αk + bk,
where the random variables αk, bk as
½∖ 2	n	n
Hi	(x2k-1 - x*)+ α X Hi2(x2k-1 - x*) + α X HiVfi(X*),
i=1	i=1
n	/ i-1	∖
bk ：= -α X Hi X Vfj (x∣--1) - f (x0k-1)
i=1
n
-α X Hn
i=1
n
j=i
/ i-1
-i+1
Ewn-j + 1(x2- J - Vfn-j + 1 (陪)
V = 1
n
-<E Hi EVfj(W--I)-Vfj(x0k-1).
i=1	∖j=1	)
Again, using smoothness assumption and bounded gradients property (Lemma 1) We get,
IlbkIl ≤ 3α2L2Gn3.
Next, we decompose the inner product of x2k-1 - x* and E [rk] in Eq. (36):
and
(38)
-2α(x0k-1 - x*, rk) = -2Q {x2k-1 - x*, ak + bk)
=-2α(x0k-1 — x*, ak) — 2α(x2k-1
-x*, bk)
(39)
For the first term in (39),
-2α(x2k-1 - x*, αk) = 4α2 (x0' -2α2 j -2α2 4 ≤ 4α2 ,0	k-1-x*, (Xh) (x0k-1-x*): E0k-1- x*, X Hi2(x2k-1 - x*)∖ i=1 ⅛k-1 - x*,X HiVfi(X*) i=1 k-1-x*, (XH)(x2k-1-x*)∖
47
Published as a conference paper at ICLR 2022
-2α2 ,0k-1 - x*, XX HNfi(X*∖
4α2n2∣∣VF(x0k-1)k2 - 2α2
n
-x*, E HiVfi(x*)
i=1
≤ 4α2n2∣∣VF(x2kT)∣∣2 + 2α2nLG∣∣x2kτ - x*∣∣.	(40)
For the second term in (39), we use Cauchy-Schwarz and Ineq. (38)
—2α(x0k-1 — x*, bk) ≤ 6α3L2Gn3∣x2k-1 — x*∣.
Substituting (40) and (41) back to (39), we get
-2α (x0k-1 - x*, rk) ≤ 4α2n2∣VF(x0k-1)∣∣2 + 2α2nLG∣∣x2k-1 - x*∣
+ 6α3L2Gn3∣x0k-1 — x*∣.
Substituting (37) (42) back to (36), we finally get a recursion bound for one epoch:
(41)
(42)
kxn -x*k2 ≤	1 - 4nα
∣x2kτ -x*∣2 - (4nα-ɪ——8α2n2) IlVF(x2k-1)∣∣2
L+μ
-2α(x0k-1 - x*
≤ 1 — 4nα
,rk〉+2α2∣∣ 叫2
|由一 -x*∣2 - (4nαɪ - 8α2n2) ∣∣VF(x0k-1)∣∣2
L+μ
4α2n2 ∣∣VF(x0k-1) ||2 + 2α2nLG∣x2k-1 — x*∣ + 6α3L2Gn3∣x0k-1 — x*∣
+ 8α4n4G2L2
1 — 4nα
∣x0kτ - x*k2 - (4nαɪ - 12α2n2) ∣∣VF(x2k-1)∣∣2
L+μ
+ 2α2nLG(1 + 3αLn2)∣x2k-1 - x*∣ + 8α4n4G2L2.
Next, We use the fact that	2ab	≤	λa2	+	b2∕λ (for any λ >	0)	on the term	2α2 nLG(1	+
3αLn2)kx2k-1 - x*∣ to getthat
2α2nLG(1 + 3αLn2)∣x0k-1 — x*∣ ≤ (α2nLG(1 + 3QLn2))2 /(nαμ) + nαμ∣x0k-1 — x*∣2
=μ-1α3nL2G2(1 + 3αLn2)2 + nα∣x2k-1 — x*∣2.
Substituting this back We get,
〔lx” - x*k2 ≤	1 - 4nα
∣x2kτ - x*k2 - (4nαɪ - 12α2n2) ∣∣VF(x2fc-1)∣∣2
L+μ
+ μ-1α3nL2G2(1 + 3QLn2)2 + naμkx2k-1
≤ (1 — 2nɑμ + nαμ) ∣∣x0k-1 — x*||2 —
—X*
1
4nα----
L + μ
+ μ-1α3nL2G2(1 + 3aLn2)2 + 8α4n4G2L2
k2 + 8α4n4 G2L2
-1202n2) IlVF(x0k「『
(Since μ ≤ L)
卜一

(1 — nαμ) ∣∣x2k-1 — x*∣2 — 44na--^-----12α2n2) ∣∣VF(x0k-1)∣∣2
∖ L + μ	) 11	0	711
+ μ-1α3nL2G2(1 + 3αLn2)2 + 8α4n4G2L2
Now, substituting the values of α and the bound on K, we get that 4nα ɪ^ - 12α2n2 ≥ 0 and
hence,
∣∣x2nk — x*∣2 ≤ (1 — nαμ) ∣∣x0k-1 — x*∣2 + μ-1α3nL2G2(1 + 3QLn2)2 + 8α4n4G2L2
Now, iterating this for K/2 epoch pairs, we get
∣∣xK - x*∣2 ≤ (1 — nαμ)K/2 1xj — x*∣2 + ^-μ-1 α3nL2G2(1 + 3αLn2)2 + 4Kα4n4G2L2
48
Published as a conference paper at ICLR 2022
≤ e-na"K/2kx1 - x*k2 + Kμ-1α3nL2G2(1 + 3αLn2)2 + 4Kα4n4G2L2
≤ e-na"K/2kx0 - x*k2 + Kμ-1α3nL2G2 + 9Kμ-1 α5n5L4G2 + 4Ka4n4G2L2
(Since (1 +a)2 ≤ 2+ 2a2)
Substituting α = "'^gKK gives Us the desired result.
□
H	Additional Experiments
Although our theoretical guarantees for FlipFlop only hold for quadratic objectives, we conjecture
that FlipFlop might be able to improve the convergence performance for other classes of functions,
whose Hessians are smooth near their minimizers. To see this, we also ran some experiments on
1-dimensional logistic regression. As we can see in Figure 3, the convergence rates are very similar
to those on quadratic functions. The data was synthetically generated such that the objective function
becomes strongly convex and well conditioned near the minimizer. Note that logistic loss is not
strongly convex on linearly separable data. Therefore, to make the loss strongly convex, we ensured
that the data was not linearly separable. Essentially, the dataset was the following: all the datapoints
were Z = ±1, and their labels were y = lz>o with probability 3/4 and y = lz<o with probability
1/4. Framing this as an optimization problem, we have
min F (x) := E [-y log(h(xz)) - (1 - y) log(1 - h(xz))] ,
x
where h(xz) = 1/(1 + e-xz). Note that x = - log 3 is the minimizer of this function, which is
helpful because we can use it to compute the exact error. Similar to the experiment on quadratic
functions, n was set to 800 and step size was set in the same regime as in Theorems 4, 5, and 6.
49