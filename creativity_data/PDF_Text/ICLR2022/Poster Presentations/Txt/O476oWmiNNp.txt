Published as a conference paper at ICLR 2022
Anti-Oversmoothing in Deep Vision Trans-
formers via the Fourier Domain Analysis:
From Theory to Practice
Peihao Wang, Wenqing Zheng, Tianlong Chen & Zhangyang Wang
Department of Electrical and Computer Engineering, The University of Texas at Austin
{peihaowang,w.zheng,tianlong.chen,atlaswang}@utexas.edu
Ab stract
Vision Transformer (ViT) has recently demonstrated promise in computer vision
problems. However, unlike Convolutional Neural Networks (CNN), it is known
that the performance of ViT saturates quickly with depth increasing, due to the
observed attention collapse or patch uniformity. Despite a couple of empirical
solutions, a rigorous framework studying on this scalability issue remains elu-
sive. In this paper, we first establish a rigorous theory framework to analyze
ViT features from the Fourier spectrum domain. We show that the self-attention
mechanism inherently amounts to a low-pass filter, which indicates when ViT
scales up its depth, excessive low-pass filtering will cause feature maps to only
preserve their Direct-Current (DC) component. We then propose two straight-
forward yet effective techniques to mitigate the undesirable low-pass limitation.
The first technique, termed AttnScale, decomposes a self-attention block into
low-pass and high-pass components, then rescales and combines these two fil-
ters to produce an all-pass self-attention matrix. The second technique, termed
FeatScale, re-weights feature maps on separate frequency bands to amplify the
high-frequency signals. Both techniques are efficient and hyperparameter-free,
while effectively overcoming relevant ViT training artifacts such as attention col-
lapse and patch uniformity. By seamlessly plugging in our techniques to multiple
ViT variants, we demonstrate that they consistently help ViTs benefit from deeper
architectures, bringing up to 1.1% performance gains “for free” (e.g., with little
parameter overhead). We publicly release our codes and pre-trained models at
https://github.com/VITA-Group/ViT-Anti-Oversmoothing.
1	Introduction
Transformers have achieved phenomenal success in Natural Language Processing (NLP) (Vaswani
et al., 2017; Devlin et al., 2018; Dai et al., 2019; Brown et al., 2020), and recently in a wide range
of computer vision applications too (Dosovitskiy et al., 2020; Liu et al., 2021; Arnab et al., 2021;
Carion et al., 2020; Jiang et al., 2021a). One representative advance, the Vision Transformer (ViT)
(Dosovitskiy et al., 2020), stacks Multi-head Self-Attention (MSA) blocks, by treating each local
image patch as semantic tokens and modeling their interactions globally. Unlike Convolutional
Neural Networks (CNNs) that hierarchically enlarge the receptive from local to global, even a shallow
ViT is able to effectively capture the global contexts, leading to their very competitive performance
on image classification and other tasks (Liu et al., 2021; Jiang et al., 2021a).
Going deep has always been a trend in deep learning (LeCun et al., 2015; Krizhevsky et al., 2012),
and ViT was expected to make no exception. One might reasonably conjecture that a deeper ViT with
more MSA blocks significantly outperform its shallower baseline. Unfortunately, building deeper
ViTs face practical challenges. Empirically, Zhou et al. (2021a) shows a vanilla ViT of 32 layers
under-performs the 24-layer one. Gong et al. (2021) demonstrates a downgraded patch diversity in
deeper layers, and Dong et al. (2021) mathematically reveals the rank collapse phenomenon when
Transformer goes deeper. Despite efforts towards deep ViT through patch diversification (Gong
et al., 2021; Zhou et al., 2021b), rank collapse alleviation (Zhou et al., 2021a; Zhang et al., 2021),
and training stabilization (Touvron et al., 2021b; Zhang et al., 2019), most of them are restricted to
1
Published as a conference paper at ICLR 2022
empirical studies. Rethinking the problem with deep ViT from a more principled angle pends further
efforts.
In this paper, we present the first rigorous analysis of stacking self-attention mechanism in the Fourier
space. We mathematically show that cascading self-attention blocks is equivalent to repeatedly
applying a low-pass filter, regardless of the input key or query tokens (Section 2.2). As a consequence,
going deeper with vanilla ViT blocks only preserves Direct Component (DC) of the signal at the
output layer. This theoretical finding explains the observations of patch uniformity and rank collapse,
and is also inherently related to the over-smoothing phenomenon in Graph Convolutional Networks
(GCNs) (Kipf & Welling, 2017; NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020).
Moreover, we also reveal the role of other transformer modules (e.g., MLP and residual connection)
in preventing this undesirable low-pass filtering (Section 2.3).
Built on the aforementioned analysis framework in the Fourier domain, we propose two novel
techniques, to mitigate the low-pass filtering effect of self-attention and effectively scale up the
depth of ViTs. The first technique, termed Attention Scaling (AttnScale), directly manipulates on the
calculated attention map to enforce an all-pass filter (Section 3.1). It decomposes the self-attention
matrix into a low-pass filter plus a high-pass filter, then adopts a learnable weight to adaptively
amplify the effect of high-pass filter. The second technique, termed Feature Scaling (FeatScale),
hinges on feature maps to re-weight different frequency bands separately (Section 3.2). It employs
trainable coefficients to re-mix the DC and high-frequency components, hence selectively enhancing
the high-frequency portion of the MSA output. Both AttnScale and FeatScale are extremely memory
and computationally friendly. Neither runs Fourier transformation explicitly, bringing little extra
complexity to the original ViTs.
Our contributions can be summarized as follows:
•	We establish the first rigorous theoretical analysis of ViT from the spectral domain. We
characterize the low-pass filtering effect of cascading MSAs, which connects to the recent
empirical findings of ViT patch diversity loss or rank collapse.
•	We present two theoretically grounded Fourier-domain scaling techniques, named AttnScale
and FeatScale. They operating on re-adjusting the low- and high-frequency components of
the attention maps and feature maps, respectively. Both are efficient, hyperparameter-free,
easy-to-use, and able to generalize across different ViT variants.
•	We conduct extensive experiments by integrating AttnScale and FeatScale with different ViT
backbones. Both of our approaches substantially boost DeiT, CaiT, and Swin-Transformer
with up to 1.1%, 0.6% and 0.5% performance gains, without whistles and bells.
2	Why ViT Cannot Go Deeper?
2.1	Notation and Preliminaries
We begin by introducing our notations. Let X ∈ Rn×d denote the feature matrix, where n is the
number of samples, and d is the feature dimension. Let Xi ∈ Rd, ∀i = 1, ∙∙∙ ,n,the i-th row of X,
denote the feature vector of the i-th sample, and Zj ∈ Rn, ∀j = 1,…，d, the j-th column of X,
represent signals of the j -th channel. In the context of ViT, X denotes a set (sequence) of image
patches, xi denotes the flatten version of the i-th patch embedding (d = patch width × patch height),
and zj denotes a whole image signal of the j -th channel.
Transformer Architecture Vision Transformer (ViT) consists of three main components: a patch
embedding and position encoding part, a stack of transformer encoder block with Multi-Head
Self-Attention (MSA) and Feed-Forward Network (FFN), and a score readout function for image
classification. We depict a transformer block in Fig. 3a. The key ingredient here is the Self-Attention
(SA) module, which takes in the token representation of the last layer, and encodes each image
token by aggregating information from other patches with respect to the computed attention value,
formulated as below (Vaswani et al., 2017):
SA(X) = Softmax (XWQtXWK)T ) XWV,	(1)
2
Published as a conference paper at ICLR 2022
Layer Index
Layer Index
Layer Index
Figure 1: Visualize the intensity of high-frequency component and their theoretical upper bounds
under different transformer blocks. The blue line is defined by log(kHC [Xl]kF/kHC [X0]kF), and
the red line is estimated using the results in Section 2.2 & 2.3. See details in Appendix F.1.
where WK ∈ Rd×dk , WQ ∈ Rd×dq , WV ∈ Rd×d are the key, query, and value weight matrices,
respectively, √d here denotes a scaling factor, and Softmax(∙) operates on X row-wisely. Multi-Head
Self-Attention (MSA) involves a group of SA heads and combines their outputs through a linear
projection (Vaswani et al., 2017):
MSA(X) = [SAι(X) … SAH(X)] Wo,	(2)
where the subscripts denote the SA head number, H is the total number of SA heads, and WO ∈
RHd×d projects multi-head outputs to the hidden dimension. Besides MSA module, each transformer
block is equipped with a normalization layer, feed-forward network, and skip connections to cooperate
with MSA. Formally, a transformer block can be written as follows:
X0 = MSA(LayerNorm(X)) +X,	(3)
Y = FFN(LayerNorm(X0)) +X0.	(4)
Fourier Analysis The main analytic tool in this paper is Fourier transform. Denote F : Rn →
Cn be the Discrete Fourier Transform (DFT) with the Inverse Discrete Fourier Transform (IFT)
F-1 : Cn → Rn. Applying F to a flatten image signal x is equivalent to left multiplying a DFT
matrix, whose rows are the Fourier basis fk = [e2πj(k-1)∙0 … e2nj(kTMnT)]T /√n ∈ Rn,
where k denotes the k-th row of DFT matrix, and j is the imaginary unit. Let Z = FZ be the
spectrum of z, and Zdc ∈ C, Zhc ∈ Cn-I take the first element and the rest elements of Z,
respectively. Define DC [z] = Zdcfi ∈ Cn as the Direct-Current (DC) component of signal z, and
HC [z] = [f2 … fn] Zhc ∈ Cn the complementary high-frequency component.
In signal processing, a low-pass filter is a system that suppresses the high-frequency component
of signals and retain the low-frequency component. In this paper, we refer to low-pass filter as a
particular type of filters that only preserve the DC component, while diminishing the remaining
high-frequency component. To be more precise, we define low-pass filters in Definition 1.
Definition 1. Given an endomorphism f : Rn → Rn with ft denoting applying f for t times, f is a
low-pass filter if and only if for all Z ∈ Rn:
㈣ kDC≡=0.
(5)
Definition 1 reveals the nature of low-pass filters: they will produce a dominant response on DC
component, while imposing an inhibition effect on the high-frequency band. We refer interested
readers to Appendix A for more useful backgrounds.
2.2	Self-Attention Is A Low-Pass Filter
In this subsection, we will give theoretical justification on self-attention in terms of its spectral-domain
effect. Our main result is that self-attention is constantly a low-pass filter, which continuously erases
high-frequency information, thus causing ViT to lose features expressiveness at deep layers.
Formally, we have the following theorem that shows attention matrix produced by a softmax function
(e.g, Eqn. 1) is a low-pass filter independent of the input token features or key/query matrices.
Theorem 1. (SA matrix is a low-pass filter) Let A = softmax(P), where P ∈ Rn×n. Then A must
be a low-pass filter. For all Z ∈ Rn, limt→∞ kHC [AtZ]k2/kDC [AtZ]k2 = 0.
3
Published as a conference paper at ICLR 2022
Theorem 1 is a straightforward result of Perron-Frobenius
theorem. See Appendix B.1 for a proof. Theorem 1 also
reveals that no matter how attention is computed inside the
softmax function, including dot product (Vaswani et al.,
2017), linear combination (Velickovic et al., 2018), or L2
distance (Kim et al., 2021), the resulting attention matrix
is always a low-pass filter. One can see consecutively
applying self-attention matrix simulates the process of
ViT’s forward propagation. As the layer number increases
infinitely, the final output will only keep the DC bias, and
ViT loses all the feature expressive power.
Corollary 2. Let Pi, P2, ∙ , Pn be a sequence of ma-
trix in Rn×n ,and each Pk ,∀k = 1,…，L has Ak =
softmax(Pk). Then QkL=1 Ak is also a low-pass filter.
Figure 2: Visualize the spectral response
of an attention map. We randomly pick
a sample and depict its first head of
4/8/12-th layer. Refer to Appendix F.2.
In fact, ViT re-computes self-attention matrices per layer, which seems to avoid the consecutive
power of an identical self-attention matrix. However, we also provide Corollary 2, which suggests
even the ViT consists of distinctive self-attention matrix at each layer, their composition turns out to
act like a low-pass filter as well. We also visualize the spectrum of attention maps (Fig. 2 and more
on Appendix F.2) to support our theoretical conclusions.
Knowing that self-attention matrices amount to low-pass filters, we are also interested in to which
extent an MSA layer would suppress the high-frequency component. Thereby, we also provide a
convergence rate to illustrate this speed that the high-frequency component are being annihilated.
Theorem 3. (smoothening rate of SA) Let A = softmax(P) and α = maxi,j |Pij |, where P ∈
Rn×n. Define SA(X) = AXWV as the output of a self-attention module, then
yne2α
ɪɪ~-kWvk2kHC [X]kF.	(6)
e2α + n - 1
In particular, when P = XWQ(XWK )T/√d, and assume tokens are distributed inside a ball with
radius γ > 0, i.e., kxik2 ≤ γ,∀i = 1,…,n, then α ≤ γ2k Wq WKT ∣∣2∕√d.
The proof of Theorem 3 can be found in Appendix B.3. Theorem 3 says the high-frequency
ne2α
k 2 V e2α+n-1
intensity ratio to the pre- and post- attention aggregation is upper bounded by ∣WV
When ∣Wv∣2√e⅛≡
< 1, HC [SA(X)] converges to zero exponentially. We note that, no

matter how attention is computed or signals are initialized, since
ne2α
e2α+n-1
is bounded by √n,
∣WV ∣2 < 1∕√n will definitely cause a monotonically decreasing high-frequency component. When
dot-product attention is adopted, a sufficient condition that ∣HC [X]∣F decreases to zero within
logarithmic time is Y21WQ WKK∣∣2∕√d + log∣ Wv12 ≤ log n-1 /2.
2.3	Existing Mechanisms that Counteract Low-Pass Filtering
In this section, we take other ViT building blocks into consideration. We will justify whether Multi-
Head Self-Attention (MSA), Feed-Forward Network (FFN), and residual connections can effectively
alleviate the low-pass filtering drawbacks. All the derivations follow from Theorem 3, and some
proof ideas are borrowed from Dong et al. (2021). We further present Fig. 1 to justify our results.
Does multi-head help? MSA employs weights to combine the results of multiple self-attention
blocks. We can rewrite it as MSA(X) = PhH=1 SA(X)WOh. We show by Proposition 4 in
Appendix C.1 that the convergence rate turns to
σισ2H √e⅛≡
, where H is the number of heads,
σ1 = maxhH=1 ∣WVh ∣2 and σ2 = maxhH=1 ∣WOh∣2. One can see MSA can only slow down the
convergence up to a constant σ2H, which does not root out the problem.
Does residual connection benefit? In addition to MSA, a transformer block also leverages a skip
connection, which can be formulated as Res(X) = MSA(X)+X. We show that residual connection
4
Published as a conference paper at ICLR 2022
can effectively prevent high-frequency component from diminishing to zero by promoting the rate
σισ2H尸石 to 1 + σισ2H户石
> 1. Refer to Proposition 5 in Appendix C.2.
Does FFN make any difference? A feed-forward network is appended to MSA module. We
characterize its effect in Appendix C.3. Our Proposition 6 suggests that a FFN with Lipschitz constant
(1+ σισ2H Je⅛r)
σ3 contributes a σ3
convergence rate, which does not improve the original
one. However, if skip connection is adopted over FFN, σ3 > 1 can guarantee the upper bound of the
high-frequency component is non-contractive.
Although multi-head, FFN, and skip connection all help preserve the high-frequency signals, none
would change the fact that MSA block as a whole only possesses the representational power of
low-pass filters. Our Proposition 4, 5, 6 states multi-head, FFN, skip connections can only slow down
the convergence by indistinguishably amplifying low- and high-frequency components with the same
factor. However, since they are incapable of promoting high-frequency information separately, it is
inevitable that high-frequency components are continuously diluted as ViT goes deeper. This restricts
the expressiveness of ViT, resulting in the performance saturation in deeper ViT.
2.4	Connections to Existing Theoretic Understanding Works
It is known that Graph Convolutional Networks (GCN) are not more than low-pass filters (NT
& Maehara, 2021). In the meanwhile, Oono & Suzuki (2019); Cai & Wang (2020) pointed out
GCN’s node features will be exponentially trapped into the nullspace of the graph Laplacian matrix.
Similarly, our work concludes that self-attention module is yet another low-pass filter. Combining
with our theoretical derivation, one can see the root reason is that both graph Laplacian matrices
and self-attention matrices consistently own a fixed leading eigenvector, namely the DC basis. This
makes aggregating information via such matrices inherently project the token representation onto
these invariant eigenspaces. And we note that over-smoothing, rank collapse, and patch uniformity
are all the manifestation of excessive low-pass filtering. See Appendix D.1 for more discussion.
In Dong et al. (2021), the authors proved that ViT’s feature maps will doubly exponentially collapses
to a rank-1 matrix, which reveals ViT loses feature expressiveness at deep layers. Besides, they
also gave a systematic study on other building blocks of transformer. While they share the similar
insights with us, our work further specifies which rank-1 matrix the feature activation will converge
to, namely the subspace spanned by the DC basis. That makes our theory to be better grounded
with signal-processing and geometric interpretations, via directly measuring the intensity of the high-
frequency residual, instead of examining a composite norm distance to an agnostic rank-1 matrix.
Although Dong et al. (2021) presented a faster convergence speed, we respectfully suggest that the
current proof of Dong et al. (2021) might be deficient, or at least incomplete in the assumptions
(see Appendix D.2). Moreover, our theory can be generalized to other attention mechanisms such as
logistic attention (VelickOvic et al., 2018)and L2 distance (Kim et al., 2021). See Appendix D.3.
3	AttnS cale & Feat S cale: S caling from the Fourier-Domain
3.1	AttnS cale: Make Attention an All-Pass Filter
As we discussed in Section 2.2, self-attention matrix can only perform low-pass filtering, which
narrows the filter space ViT can express. Inspired by this, we propose a scaling techniques directly
manipulating the attention map, termed Attention Scaling (AttnScale), to balance the effects of low-
and high-pass filtering and produce all-pass filters. AttnScale decomposes the self-attention matrix to
a low-pass filter plus a high-pass filter, and introduces a trainable parameter to rescale the high-pass
filter to match the magnitude with the low-pass component.
Formally, let A denote a self-attention matrix. To decompose a low-pass filter from A, we find the
largest possible low-pass filter that can be extracted from A. We use Lemma 8 in Appendix E to
justify our solution. By Lemma 8, We can simply extract L = F-1 diag(1,0,…,0)F = 11T/n
from A and take the complementary part as the high-pass filter. Afterward, we can rescale the
high-pass component of the filter, and combine loW-pass and high-pass together to form a neW
self-attention matrix. We illustrate this scaling trick in Fig. 3b. To be more precise, for the l-th layer
and h-th head, We recompute the self-attention map as folloWs:
5
Published as a conference paper at ICLR 2022
(b)
⑶
(c)
Figure 3: Illustration of our proposed techniques. (a) recalls
the standard ViT block. (b) and (c) illustrate our proposed
AttnScale and FeatScale, which scaling high-pass filter com-
ponent and high-frequency signals, respectively.
Figure 4: Visualize cosine similar-
ity of attention and feature maps
with/without our proposed methods.
Refer to Appendix F.3 for details.
A
(l,h)
LP
111T,
n
A(Hl,Ph) = A(l,h) -A(LlP,h),
Aah)= ALe + (ωι,h + 1)aH2,
(7)
(8)
(9)
where ωl,h is a trainable parameter, and different layers and heads adopt separate ωl,h . During
training time, ωι,hs are initialized with 0, andjointly tuned with the other network parameters. By
adjusting ωιh, A(l,h) can simulate any type of filters: low-pass, high-pass, band-pass, or all-pass.
Note that our AttnScale is extremely lightweight, as it only brings O(HL) extra parameters, where
H is the number of heads, and L is the number of ViT blocks.
3.2	FeatScale: Reweight High-Frequency Signals
According to our analysis in Section 2.2, MSA module will indiscriminately suppress high-frequency
signals, which leads to severe information loss. Even though residual connection can retrieve lost
information through the skip path, the high-frequency portion will be inevitably diluted (Theorem
1). To this end, we propose another scaling technique that operates on feature maps, named Feature
Scaling (FeatScale). FeatScale processes the output of MSA by mixing the information from varying
frequency bands discriminatively. FeatScale first decomposes the resultant signals into their DC
and high-frequency components. Then it introduces two groups of parameters to re-weight the two
components for each channel, respectively. The pipeline of this scaling technique is depicted in Fig.
3c. To be more precise, we re-weight the output of the l-th MSA by
XD(l)C = DC [MSA(X)] (diag(sl) + I),	(10)
XH(l)C = HC [MSA(X)] (diag(tl) + I),	(11)
X(l) = XD(l)C + XH(l)C,	(12)
where sl	∈	Rd	and	tl	∈	Rd are learnable parameters to perform channel-wise re-weighting. We
initialize sl	and	tl	with	zeros and tune them with gradient descent. After adjusting	the proportion of
different frequency signals, FeatScale can prevent the dominance of the DC component. DC [∙] and
HC [∙] are cheap to compute without explicit Fourier transform. Calculating DC [X] is as simple as
running the column average of matrix X, and HC [X] can be efficiently computed by X - DC [X].
3.3	Discussion
We have proposed two methods to facilitate the deeper stacking of ViT MSA modules, and discussed
their motivations and strengths from our perspective of filtering and signal processing. In this section,
we will connect these two techniques with commonly mentioned problems with ViTs.
How does AttnScale prevent attention collapse? Deep ViT suffers from the attention collapse
issue (Zhou et al., 2021a). When transformer goes deepr, the attention maps gradually become similar
6
Published as a conference paper at ICLR 2022
and even much the same after certain layers. Combining with our thoery, one can see collapsed
attention maps turn out to be a pure low-pass filter, which wipes off all the high-frequency information
in one shot. Zhou et al. (2021a) proposed the re-attention trick, which blends attention map across
different heads. By doing this, modified attention maps aggregate high-pass components from other
heads and is endowed with richer filtering property. We note that our AttnScale is akin to a more
lightweight re-attention mechanism with better interpretability. We rewrite the attention map as a sum
of an already-collapsed attention (11T /n) with the complementary residual map that encodes diverse
patterns in a self-attention matrix. By re-weighting the residual map, the diversified patterns can be
amplified, which prevents it from degenerating to a rank-1 matrix. We further verify this argument
using cosine similarity metric (Zhou et al., 2021a) in the upper sub-figure of Fig. 4.
How does FeatScale conserve patch diversity? Self-attention blocks tend to map different patches
into similar latent representations, yielding information loss and performance degradation (Gong et al.,
2021). By our theory, this asymptotic smoothness of feature map is caused by excessive low-pass
filtering, and the remaining DC bias signifies uniform patch representations. Conventional approaches
to addressing this problem include incorporating convolutional layers (Wu et al., 2021; Jiang et al.,
2021b) and enforcing patch diversity regularizations (Gong et al., 2021). As diverse features are
often characterized by high-frequency signals, our FeatScale instead elevating the high-frequency
component via a learnable scaling factor, can be regarded as a more straightforward way to reconstruct
the patch richness. Compared with LayerScale (Touvron et al., 2021b), in which each frequency
band is equally scaled, our FeatScale treating DC and high-frequency components differently, not
only perform a per-channel normalization, but also perform a spectral-domain calibration with high-
frequency details and low-frequency characteristics. The lower sub-plot of Fig. 4 shows ViT with our
FeatScale has lower feature similarity at deep layer.
4	Related Work
Transformers in Vision. Transformer (Vaswani et al., 2017) entirely relies on self-attention mech-
anism to capture correlation and exchange information globally among the input. It has achieved a
remarkable performance in natural language processing (Devlin et al., 2018; Dai et al., 2019; Brown
et al., 2020) and many cross-disciplinary applications (Jumper et al., 2021; Ying et al., 2021; Zheng
et al., 2021b). Recent advances have also successfully applied Transformer to computer vision tasks.
Dosovitskiy et al. (2020) first adopts a pure transformer architecture (ViT) for image classification.
The follow-up works (Chen et al., 2021b) extend ViT to various vision tasks, such as object detection
(Carion et al., 2020; Zhu et al., 2021; Zheng et al., 2021a; Sun et al., 2020), segmentation (Chen
et al., 2021a; Wang et al., 2021), image generation (Parmar et al., 2018; Jiang et al., 2021a), video
processing (Zhou et al., 2018; Arnab et al., 2021), and 3D instance processing (Guo et al., 2021; Lin
et al., 2021). To capture multi-scale non-local contexts, Zhang et al. (2020) designs transformers
in self-level, top-down, and bottom-up interaction fashion. Liu et al. (2021) presents hierarchical
ViTs with shifted window based attention that can efficiently extract multi-scale features. To dismiss
ViT from the heavy reliance on large-scale dataset pre-training, Touvron et al. (2021a); Yuan et al.
(2021) propose knowledge distillation and progressive tokenization for data-efficient training. Despite
impressive effectiveness, most of these model are only based on relatively shallow ViT backbones
with a dozen of MSA blocks.
Advances in deep ViTs. Building deeper ViTs has arisen many interests. Zhou et al. (2021a) first
investigated the depth scalability of ViT. The authors found that the attention collapse hinders ViT
from scaling up, and propose two methods to conquer this problem i) increasing the embedding
dimension, and ii) a cross-head re-attention trick to regenerate attention map. A concurrent work
Touvron et al. (2021b) came up with a LayerScale layer that performs per-channel multiplication for
each residual block. More importantly, they make explicit separation of transformer layers involving
self-attention between patches, from class-attention layers that are devoted to extract the global
content into a single embedding to be decoded. Gong et al. (2021) further proposed a series of losses
that can enforce patch diversity in ViT. Such regularizations include penalty on cosine similarity,
patch-wise contrastive loss, and mixing loss. Tang et al. (2021) presented a shortcut augmentation
scheme with block-circulant projection to improve feature diversity. Although these existing solutions
manage to deepen ViTs, most of them are empirical works and bring no principled theory.
Role of depth in NNs. Discussing the importance of deep structures in Neural Networks (NNs) is
an overly broad topic. Here we only focus on a subset of works that scaling up a transformer could
7
Published as a conference paper at ICLR 2022
Table 1: Experimental evalutation of AttnScale & FeatScale plugged into DeiT and CaiT. The number
inside the (↑ ∙) represents the performance gain compared with the baseline model, and accuracies
within/out of parenthesis are the reported/reproduced performance.
Backbone	Method	Input size	# Layer	# Param	FLOPs	Throughput	ToP-IAcc(%)
	DeiT-S	224	12	22.0M	4.57G	1589.4	-79.8 (79.9)-
	DeiT-S + AttnScale	224	12	22.0M	4.57G	1416.7	80.7 (↑ 0.9)
DeiT	DeiT-S + FeatScale	224	12	22.0M	4.57G	1509.9	80.9(↑ 1.1)
	DeiT-S	224	24	43.3M	9.09G	836.4	-80.5(81.0)-
	DeiT-S + AttnScale	224	24	43.3M	9.10G	722.0	81.1 (↑ 0.6)
	DeiT-S + FeatScale	224	24	43.4M	9.10G	772.5	81.3(↑ 0.8)
	CaiT-S	224	24	46.9M	9.33G	371.9	-82.6 (82.7)-
CaiT	CaiT-S + AttnScale	224	24	46.9M	9.34G	339.0	83.2 (↑ 0.6)
	CaiT-S + FeatScale	224	24	46.9M	9.34G	358.2	83.2 (↑ 0.6)
	Swin-S	224	24	49.6M	8.74G	593.2	-83.0 (83.0)-
Swin	Swin-S + AttnScale	224	24	49.6M	8.75G	553.4	83.4 (↑ 0.4)
	Swin-S + FeatScale	224	24	49.6M	8.75G	550.3	83.5(↑ 0.5)
relate to. For ordinal deep learning models, such as FFNs and CNNs, deep architecture immediately
benefits from the universal approximation power and expressive capacity (Cybenko, 1989; Hornik,
1991; Telgarsky, 2016; Lu et al., 2017; Petersen & Voigtlaender, 2020; Zhou, 2020). In contrast,
several studies in graph learning domain have reported severe performance degradation due to over-
smoothing when stacking many layers (Kipf & Welling, 2017; Wu et al., 2019; Li et al., 2018). The
subsequent studies (NT & Maehara, 2021; Oono & Suzuki, 2019; Cai & Wang, 2020) gave theoretical
explanations of the over-smoothing phenomena from the views of graph signal filtering and feature
dynamics. Likewise, ViT have been witnessed performance saturation when going deeper. However,
to our best knowledge, Dong et al. (2021) is the sole work in the literature that systematically and
rigorously analyzes this issue with deep ViT. The main idea of this work is showing the self-attention
block will downgrade the rank of the feature maps. Our work takes one step forward by studying ViT
on spectral domain, and manages to reveal the signals will ultimately fall into the one-dimension DC
subspace. We see our theory and techniques are also applicable to NLP transformers. However, we
only focus on ViT because empirical observations indicate NLP modeling (including Transformer)
does not require a deep structure (Vaswani et al., 2017; Brown et al., 2020), while vision tasks always
demand one (LeCun et al., 2015).
5	Experiments
In this section, we report experiment results to validate our proposed methods. First, we validate the
effectiveness of our AttnScale and FeatScale when integrated with different deep ViT backbones
(Section 5.1). Second, we compare our best models with state-of-the-art (SOTA) results (Section 5.2).
All of our experiments are conducted on the ImageNet dataset (Russakovsky et al., 2015) with around
1.3M images in the training set and 50k images in the validation set. Our implementations are based
on Timm (Wightman, 2019) and DeiT (Touvron et al., 2021a) repositories.
5.1	How Can AttnScale & FeatS cale Benefit Deep ViT?
Experiment Settings. In this subsection, we intend to testify our models are beneficial to various
ViT backbones with different depth settings and training modes. We choose DeiT (Touvron et al.,
2021a) as our first backbone in order to train from scratch. When training 12-layer DeiT, we follow
the same training recipe, hyper-parameters, and data augmentation with Touvron et al. (2021a). When
training 24-layer DeiT, we follow the setting in Gong et al. (2021). Specially, we set dropout rate to
0.2 when training 24-layer DeiT (Touvron et al., 2021b). Our second backbone is CaiT (Touvron
et al., 2021b). We only apply our techniques to the patch embedding layers. The third backbone is
the SOTA model Swin-Transformer (Liu et al., 2021). All experimental settings share the same with
Liu et al. (2021). In addition to training from scratch, we also investigate the fine-tuning setting. We
defer this part to Appendix G.1.
Results. All of our experimental evaluations are summarized in Table 1. The results suggest
our proposed AttnScale and FeatScale successfully facilitate both DeiT, CaiT, Swin-Transformer
under different depth settings and training modes. Specifically, AttnScale brings less than 100/150
extra parameters for 12/24-layer DeiT while boosting the top-1 accuracy by 0.9% for 12-layer
DeiT and 0.6% for 24-layer DeiT. Our FeatScale substantially improves top-1 accuracy by 1% for
12-layer DeiT and 0.8% for 24-layer DeiT. Compared with existing techniques, the improvements
8
Published as a conference paper at ICLR 2022
Table 2: Compared with state-of-the-art models on ImageNet dataset. Accuracies with superscript (*)
are reported by Gong et al. (2021), with superscript 什)are reported by Yuan et al. (2021), and others
are reported by the original papers. Bold accuracies signifies best models among pure transformers.
Category	Method	# Param	Input size	# Layer	Top-1 Acc(%)
CNN	ReSNet-152 (Heetal., 2016)	230M-	224	152	78F
	DenSeNet-201 (Huang et al., 2017)	77M	224	201	77.6*
CNN+	CVT-21 (WUetal., 2021)	-32M	224	21	82.5^
Transformer	LV-ViT-S (Jiang et al., 2021b)	26M	224	16	83.3 *
	ViT-S/16 (Dosovitskiy et al., 2020)	-49M-	224	12	78P
	ViT-B/16 (Dosovitskiy et al., 2020)	86M	224	12	79.8 中
	DeiT-S (Touvron et al., 2021a)	22M	224	12	79.8
	DeiT-S Distilled (Touvron et al., 2021a)	22M	224	12	81.2
Transformer	Swin-S (Liu et al., 2021)	50M	224	12	83.0
	T2T-ViT-24 (Yuan et al., 2021)	-64M	224	24	823
	DeepViT-24B (Zhou et al., 2021a)	36M	224	24	80.1
	CaiT-S (Touvron et al., 2021b)	47M	224	24	82.7
	DeiT-S + DiversePatch (Gong et al., 2021)	44M	224	24	82.2
	DeiT-S + AttnScale	-43M	224	24	8∏
	DeiT-S + FeatScale	43M	224	24	81.3
	CaiT-S + AttnScale	47M	224	24	83.2
Ours	CaiT-S + FeatScale	47M	224	24	83.2
	Swin-S + AttnScale	50M	224	24	83.4
	Swin-S + FeatScale	50M	224	24	83.5
of AttnScale and FeatScale already surpass re-attention (0.6%) (Zhou et al., 2021a), LayerScale
(0.7%) (Touvron et al., 2021b), and late class token insertion (0.6%) (Touvron et al., 2021b). We also
observe a consistent 0.6% performance gain when AttnScale and FeatScale plugged into CaiT. Under
fine-tuning setting, as we will show in Appendix G.1, only tens of epoch’s fine-tuning can further
promote their performance by ≥ 0.2% (see Table 3). On Swin-Transformer, both our AttnScale
and FeatScale bring around 0.5% accuracy gain. This makes Swin-S with 50M parameters even
comparable to Swin-B (83.5% top-1 accuracy on ImageNet1k) with 88M parameters. We defer more
model interpretation and visualization to Appendix G. For a brief summary, we observe that both
shallow and deep ViT enjoy from AttnScale and FeatScale that: 1) the attention maps can simulate
richer filtering properties (compare Fig. 9 with Fig. 10), and 2) more high-frequency data can be
preserved (refer to Fig. 11).
5.2	Comparison with SOTA Models
In this subsection, we compare our best models with state-of-the-art models on ImageNet benchmark.
We choose SOTA models from three classes: CNN only, CNN + transformer, and pure transformer.
For transformer domain, we only conduct experiments with those lightweight models with comparable
number of parameters, such as ViT-S and DeiT-S. All the results are presented in Table 2.
Among all methods, Swin-Transformer combined with our methods achieves the state-of-the-art
performance. Our CaiT-S + AttnScale and CaiT-S + FeatScale on 24-layer CaiT-S also attain superior
results over all other pure transformers, while keeping low parameter cost. That our performance
surpasses some CNN-based models (e.g., ResNet-152 and CVT) indicates by increasing depth, ViT
will be endowed with higher potential to surpass CNNs that have been dominating computer vision
domain so far. Our DeiT-S+FeatScale result also outperforms ViT-B/16 and DeiT-S Distilled, which
suggests deepening network can bring more considerable accuracy gain than increasing model width
or employing a teacher model.
6	Conclusion
In this paper, we investigate the scalability issue with ViT and propose two practical solutions
via Fourier domain analysis. Our theoretical findings indicate Multi-Head Self-Attention (MSA)
inherently performs low-pass filtering on image signals, thus causes rank collapse and patch uniformity
problems in deep ViT. To this end, we proposed two techniques, AttnScale and FeatScale, that can
effectively break such low-pass filtering bottleneck by adaptively scaling high-pass filter component
and high-frequency signals, respectively. Our experiments also validate the effectiveness of our
methods. Both techniques can boost various ViT backbones by a significant performance gain.
Grounded with our theoretical framework, interesting directions for further work include designing
parameter regularizations and spectrum-specific normalization layers.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Z.W. is in part supported by an NSF SCALE MoDL project (#2133861).
References
AnUrag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuCiC, and Cordelia Schmid.
Vivit: A video vision transformer. In IEEE International Conference on Computer Vision (ICCV),
2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 2020.
Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. In International
Conference on Machine Learning Workshop (ICMLW), 2020.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer
Vision (ECCV), 2020.
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing
Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2021a.
Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in
vision transformers: An end-to-end exploration. In Advances in Neural Information Processing
Systems (NeurIPS), 2021b.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 1989.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of
the Association for Computational Linguistics (ACL), 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv:1810.04805, 2018.
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure
attention loses rank doubly exponentially with depth. In International Conference on Machine
Learning (ICML), 2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations (ICLR), 2020.
Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers with
patch diversification. arXiv: 2104.12753, 2021.
Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu.
Pct: Point cloud transformer. Computational Visual Media, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks,
1991.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017.
10
Published as a conference paper at ICLR 2022
Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make
one strong gan, and that can scale up. In Advances in Neural Information Processing Systems
(NeurIPS), 2021a.
Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng. All
tokens matter: Token labeling for training better vision transformers. In Advances in Neural
Information Processing Systems (NeurIPS), 2021b.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn TUnyasUvUnakooL RUss Bates, AUgUstin 右dek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 2021.
HyUnjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention. In
International Conference on Machine Learning (ICML), 2021.
Thomas N Kipf and Max Welling. Semi-sUpervised classification with graph convolUtional networks.
In International Conference on Learning Representations (ICLR), 2017.
Alex Krizhevsky, Ilya SUtskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volUtional neUral networks. In Advances in Neural Information Processing Systems (NeurIPS),
2012.
Yann LeCUn, YoshUa Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.
Qimai Li, Zhichao Han, and Xiao-Ming WU. Deeper insights into graph convolUtional networks for
semi-sUpervised learning. In AAAI Conference on Artificial Intelligence (AAAI), 2018.
Kevin Lin, LijUan Wang, and Zicheng LiU. End-to-end hUman pose and mesh reconstrUction with
transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
Ze LiU, YUtong Lin, YUe Cao, Han HU, YixUan Wei, Zheng Zhang, Stephen Lin, and Baining
GUo. Swin transformer: Hierarchical vision transformer Using shifted windows. In International
Conference on Computer Vision (ICCV), 2021.
ZhoU LU, Hongming PU, Feicheng Wang, Zhiqiang HU, and Liwei Wang. The expressive power of
neUral networks: A view from the width. In Advances in Neural Information Processing Systems
(NeurIPS), 2017.
Carl D Meyer. Matrix analysis and applied linear algebra, volUme 71. SIAM, 2000.
Hoang NT and Takanori Maehara. Revisiting graph neUral networks: All we have is low-pass filters.
In International Conference on Pattern Recognition (ICPR), 2021.
Kenta Oono and Taiji SUzUki. Graph neUral networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations (ICLR), 2019.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, LUkasz Kaiser, Noam Shazeer, Alexander KU, and
DUstin Tran. Image transformer. In International Conference on Machine Learning (ICML), 2018.
Philipp Petersen and Felix Voigtlaender. EqUivalence of approximation by convolUtional neUral
networks and fUlly-connected networks. Proceedings of the American Mathematical Society, 2020.
Dana Randall. Rapidly mixing markov chains with applications in compUter science and physics.
Computing in Science & Engineering, 2006.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng HUang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale VisUal Recognition Challenge. International Journal of Computer Vision (IJCV),
2015.
Zhiqing SUn, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set
prediction for object detection. In IEEE International Conference on Computer Vision (ICCV),
2020.
11
Published as a conference paper at ICLR 2022
Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented
shortcuts for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS),
2021.
Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory, 2016.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolies, and Herve
Jegou. Training data-efficient image transformers & distillation through attention. In International
Conference on Machine Learning(ICML), 2021a.
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going
deeper with image transformers. In International Conference on Computer Vision (ICCV), 2021b.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems (NeurIPS), 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations
(ICLR), 2018.
Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feastnet: Feature-steered graph convolutions for
3d shape analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia
Xia. End-to-end video instance segmentation with transformers. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2021.
Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models, 2019.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simpli-
fying graph convolutional networks. In International Conference on Machine Learning (ICML),
2019.
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In IEEE International Conference on Computer
Vision (ICCV), 2021.
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and
Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural
Information Processing Systems (NeurIPS), 2021.
Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi
Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on
imagenet. In International Conference on Computer Vision (ICCV), 2021.
Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao
Yao, and Roy Ka-Wei Lee. On orthogonality constraints for transformers. In Annual Meeting of
the Association for Computational Linguistics (ACL), 2021.
Biao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled initializa-
tion and merged attention. In Conference on Empirical Methods in Natural Language Processing
(EMNLP), 2019.
Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, and Qianru Sun. Feature
pyramid transformer. In European Conference on Computer Vision (ECCV), 2020.
Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object
detection with adaptive clustering transformer. In British Machine Vision Conference (BMVC),
2021a.
12
Published as a conference paper at ICLR 2022
Wenqing Zheng, Qiangqiang Guo, Hao Yang, Peihao Wang, and Zhangyang Wang. Delayed
propagation transformer: A universal computation engine towards practical control in cyber-
physical systems. In Advances in Neural Information Processing Systems (NeurIPS), 2021b.
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and
Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv:2103.11886, 2021a.
Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou,
and Jiashi Feng. Refiner: Refining self-attention for vision transformers. arXiv:2106.03714,
2021b.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and computational
harmonic analysis, 2020.
Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense
video captioning with masked transformer. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformable transformers for end-to-end object detection. In International Conference on Learning
Representations (ICLR), 2021.
A More Preliminaries on Fourier Analysis
In this appendix, we provide more preliminary knowledge about Fourier analysis. Here, we only
consider discrete Fourier transform on real-value domain F : Rn → Cn . A Discrete Fourier
transform (DFT) can be written in a matrix form as below 1:
	1	1	...	1	
	1	e2πj	…	e2πj(n-1)	
DFT = ɪ n	. . . 1	. . . e2∏j(k-i)∙ι	.. .. .. ... e2πj(k-1)∙(n-1)	,	(13)
	. . . 1	. . . e2πj(n-1)	.. .. .. …e2πj(n-1)2	_	
and its inverse discrete Fourier transform is DFT-1			= DFT. In this paper, we regard matrices as	
multi-channel signals. For example, X ∈ Rn×d means d-channel n-length signals. When DFT and
inverse DFT are applies to multi-channel signals, each channel is transformed independently, i.e.,
F(X) = [F(xι)… F(xd)]= DFT ∙ X.
Hereby, We can simply operators DC [∙] and HC [∙] using the matrices in Eqn. 13. By definition, We
can write DC [∙] as below:
DC [x] = DFTT diag(1, 0,…，0)DFTx	(14)
=-11T x,	(15)
n
namely DC [∙] = 11t/n. Conversely, we can write HC [∙] as:
HC [x] = DFTT diag(0,1, ∙ ∙∙ , 1)DFTx	(16)
=DFTT(I - diag(1,0,…，0))DFTx	(17)
1T
=I-----11t x,	(18)
which indicates HC H = I - 11t/n. We will frequently use these derivations later in the proofs.
1Without loss of generality, we can only consider 1D Fourier transformer, since the DC components are
invariant to the dimension of signals.
13
Published as a conference paper at ICLR 2022
B Deferred Proofs
B.1 Proof of Theorem 1
Theorem 1. (SA matrix is a low-pass filter) Let A = softmax(P), where P ∈ Rn×n. Then A must
be a low-pass filter. For all z ∈ Rn,
IIHC Atz]k2
t→∞ kDC Atz]∣∣2
0.
Proof. Let λ1,λ2, ∙∙∙ ,λs ∈ C be the eigenvalues of A with ordering ∣λι∣ ≥ ∣λ2∣ ≥ ∙∙∙ ≥ ∣λs∣.
Notice that, A is a positive matrix, each of whose element is strictly greater than zero (Aij > 0, ∀i, j).
Besides, for all i = 1, ∙∙∙ ,n, P；=i Aij = 1. Therefore, Al = 1 implies A must have an
eigenvalue 1 and its corresponding eigenvector is the all-one vector 1.
By Perron-Frobenius Theorem (Meyer, 2000), eigenvalue 1 corresponds to a all-positive eigenvector
1, implies λ1 = 1 should be the largest eigenvalue without multiplicity, and the absolute value of
other eigenvalues λ2,…，λs must be less than 1. Let us rewrite A in the Jordan canonical form
A = PJP-1:
A = [vι ∙∙∙
、------------
P
vn]
Γλι	]
J (λ2)
.
.
.
J(λs)
(19)
where the Jordan block J(λ) can be written as
Γλ 1
λ 1
J(X)=	...	...
λ1
λ
(20)
Applying A to z for t times can be written as Atz which is equivalent to:
Γλ1
Atz = PJ tP Tz = P
Γλt1
J(λ2)t
=P
J(λ2)
t
P Tz
J(λs)
(21)
P Tz
J(λs)t
(22)
Let f (x) = xt, then At = Pf(J)PT = Pdiag(f (λι),f (J(λ2)),…，f(J(λs)))P-1. Suppose
a Jordan block with shape k × k, then
	Γf(λ)	f0(λ) f (λ)	f 00(λ) 		... 2! f0(λ)	...	f (k-1)(λ)I (k-1)! . . .	
f(J(λ)) =		..	f ”(I)	(23)
		..	2!	
		f(λ)	f0(λ)	
	[		f(λ)	
Therefore, on the diagonal number m ≤ min(t, k - 1) above the main diagonal stands:
t(t - 1)…(t - m + I) λt-m
m!
(24)
14
Published as a conference paper at ICLR 2022
For arbitrary m ≤ k - 1 and ∣λ∣ < 1,
lim
t→∞
t(t - 1)...(t - m + 1)
m!
λt-m = 0
(25)
Recall that λ1 = 1 and according to the definition of Jordan canonical form, v1 = 1. By Eqn. 25:
lim Atz = P lim diag(f(λι),f(J(λ2)),…，f(J(λs)))PTz	(26)
t→∞	t→∞
=P diag(λ1, O,…，0)PTz	(27)
= λt1 v1 u1T z	(28)
= 1u1T z	(29)
Plug the result from Eqn. 29 into the original limit:
l. kHC [Atz]k2 _ l∙
t→∞ kDC [Atz]k2 = t→∞
lim
t→∞
/ kHC [Atz]k2
V	kz -HC [Atz]k2
S kHC [Atz]k2 一
V	kzk2-kHC [Atz]k2
/ k(I- nl11T)Atzk~~
t→∞V kzk2-k(i-111T)Atzk2
S	k(i- nηιιτ )iuT zk2
V kzk2-k(i- n1 ιιτ)iuTzk2
S	k(IIuTz- IuTzk2 一
Vkzk2- k(I IuT z- IuT zk2
0
(30)
(31)
(32)
(33)
(34)
(35)
where Eqn. 31is due to the orthogonality of DC and HC terms.	□
B.2 Proof of Corollary 2
Corollary 2. Let Pi, P2,…,Pn be a sequence of matrix in Rn×n, and each Pk,Nk = 1, •…,L
has Ak = softmax(Pk). Then QkL=1 Ak is also a low-pass filter.
Proof. Let A = QkL=1 Ak, then we show A satisfies the following conditions, so that A can be
regarded as another self-attention matrix. Then we can conclude the proof by Theorem 1.
1) For every i = 1,…,n, Pj=I Aij = 1.
Suppose Pj=ι Bij = 1 for every i = 1, ∙∙∙ ,n, then for every k = 1,…，L and i = 1,… ,n,
n
nn
j=1
(AkB)ij =	Ak,imBmj
j=1 m=1
(36)
n
n
m=1
n
X Ak,im
Ak,im	Bmj
j=1
1.
(37)
(38)
m=1
By induction, for every i, Pj=I Ai,j = 1 ⇒ Pj=I(A2 Ai)ij = 1 ⇒ …⇒ Pj=I Aij = 1.
2) For every i,j = 1,…,n, Aij > 0.
Suppose Bij > 0, ∀i,j, then for every k = 1,…，L, (AkB)j = Pj=i Ak,imBmj. Since
Ak,im > 0, Bmj > 0, then (AkB)ij > 0. By induction, for every i,j, Ai,ij > 0 ⇒ (A2Ai)ij >
0 ⇒ ∙∙∙ ⇒ Aij > 0.
□
15
Published as a conference paper at ICLR 2022
B.3 Proof of Theorem 3
Theorem 3. (convergence rate of SA) Let A = softmax(P) and α = maxi,j |Pij |, where P ∈
Rn×n. Define SA(X) = AXWV as the output of a self-attention module, then
√ne2α
-ι0r~~T kWv k2kHC X ]kF.
e2α + n - 1
In particular, when P = XWQ(XWK )T/√d, and assume tokens are distributed inside a ball with
radius γ > 0, i.e., kxik2 ≤ γ,∀i = 1,…,n, then α ≤ γ2k Wq WKK ∣∣2∕√d.
Proof. First, we write X = DC [X] + HC [X] = 1Tz + H, where DC [X] = 1zT equals to the
orthogonal projection of X to subspace span(1), and H = HC [X] represents the remaining part of
the original signals. HC [SA(X)]	= (I - 11T)AX WV	(39) =(I	— 1 11t)	A(IzT + H)Wv	(40) =(I	— 1 11t)	AIzTWV +(I —	111T)	AHWV	(41) =(I — 1 11t) AHWV	(42)
Therefore,
∣HC [SA(X)]∣f = Il(I - 111T) AHWV	(43)
	nF ≤ ∣I — § 11t∣ ∣softmax(P)∣2∣Wv∣2∣H∣f	(44) ≤ PksOftmaX(P)kιksoftmax(P)k∞∣∣Wv∣∣2∣H∣f	(45)
√∣softmax(P )∣ι ∣Wv ∣∣2∣∣H ∣f
(46)
The Eqn. 45 leverages a special case of Holder,s inequality, and the Eqn. 46 can be yielded from
∣softmax(P)∣∞ = 1. Now we need to upper bound ∣softmax(P)∣1. Suppose α = maxi,j|Pij |,
then for each i = 1, ∙∙∙ ,n,we have the following inequality for the element with the largest value
(say the j-th column):
Ai, =	:P	≤ —F____________= .	e2α—
ij	Pn=I ePit	≤	eα	+ Pt=j	e-α	e2α	+	(n	- 1)
(47)
2α
Hence, we have ∣∣softmax(P)∣ι ≤ Ei maxj Aij ≤ e2an+(n-i). Insert this result to Eqn. 46, we
can conclude the proof. In particular, when P = XWQ(XWK)T/√d,
α = max|Pij | = max
i,j	i,j
XT WQWK Xj
√d
(48)
Since IlXik2,∣∣xj∙∣∣2 ≤ γ,∀i,j, α ≤ maxijIlXik2∣∣WqWK∣∣2∣∣xj∙∣∣2∕√d ≤ γ2∣WqWKT∣∣2∕√d.
□
C Extension of Theorem 3
C.1 Multi-Head Attention
Proposition 4. (smoothening rate with MSA) Let Ah = softmax(Ph), where Ph ∈ Rn×n with
h = 1,…，H. Let a = maxhH=ι maxi,j- ∣Pij |. Define MSA(X) = PhH=I AhXWV WO as the
output of a multi-head self-attention module, then
∣HC [MSA(X)]∣F ≤ σ1σ2H
I	ne2α
V e2α + n — 1
∣HC [X]∣F,
where H is the number of heads, σ1 = maxhH=1 ∣WVh ∣2 and σ2 = maxhH=1 ∣WOh ∣2.
16
Published as a conference paper at ICLR 2022
Proof. For the h-th head, according to Theorem 3:
rne2αh
FZ~-kWVk2kHC [X]kF,	(49)
e2αh + n - 1
where αh = maxi,j |Pihj |. Then we have:
kHC [MSAh(X)]kF = HC XH SAh(X)WOh	(50)
H
≤ X HC SAh(X)WOhF	(51)
h=1
yne2ah
PoH_T kWV k2kHC [X ]k2	(52)
e2αh + n - 1
h=1
H /---------
ne2αh
≤∑ 21-T kWVk2kWθk2kHC [X ]kF	(53)
e2αh + n - 1
h=1
√ne2α	..	. ...
-IOZ-TkHC X]kF，	(54)
e2α+ n - 1
where Eqn. 51 follows from the linearity of HC [∙] and triangle inequality. Eqn. 54 can be obtained by
relaxing αh, kWVhk2, and kWOhk2 to α, σ1 and σ2 (Note that
with α).
n	ne2α
V e2α+n-1
is monotonically increasing
□
C.2 Residual Connection
Proposition 5. (smoothening rate with skip connection) Let Ah = softmax(P h), where Ph ∈
Rn×n with h = 1,…，H. Let a = maxH=ι maxi,j ∣Phj |. Define X0 = MSA(X) + X as the
output of a multi-head self-attention module with skip connection, then
kHC [X0]kF ≤ (1+ σ1σ2Hre20ne：_ 1) kHC [X]kF
where H is the number of heads, σ1 = maxhH=1 kWVh k2 and σ2 = maxhH=1 kWOh k2.
Proof. By Proposition 4,
kHC[X0]kF=kHC[MSA(X)+X]kF	(55)
≤ kHC [MSA(X)]∣∣f + IIHC [X]∣∣f	(56)
√ne20
—TkHC [X]∣∣f + IlHC [X]∣∣F	(57)
e2α+ n - 1
=(1 + σισ2Hre20ne：- 1) kHC [X]kF.	(58)
Again, Eqn. 56 follows from the linearity of HC [∙] and triangle inequality.	□
C.3 Feed-Forward Network
Proposition 6. (smoothening rate with FFN) Let Ah = softmax(P h ), where Ph ∈ Rn×n with
h = 1,…，H. Let a = maxH=ι maxi,j ∣Pij |. Define Y = FFN(MSA(X) + X) as the outputofa
transformer block, then
kHC [Y]∣f ≤ σ3 (l+ σ1σ2Hjeα+m) kHC [X]∣f
where FFN : Rd → Rd represents a feed-forward network, H is the number of heads, σ1 =
maxhH=1 kWVh k2, σ2 = maxhH=1kWOh k2, and σ3 = Lips(FFN) is the Lipschitz constant of FFN. In
particular, σ3 = 1 + Lips(FFN) when residual connection is considered in FFN.
17
Published as a conference paper at ICLR 2022
Proof. Let X0 = MSA(X) + X and z0 = 1T (X0 /n) ∈ Rd, then we have
kHC[FFN(X0)]kF≤	kFFN(X0) — 1FFN(z)TkF		(59)
≤	kFFN(X0) — FFN(1zT)kF σ3∣∣X 0 — IzTkF		(60) (61)
=	“KI- n ι1T)X 0	= σ3kHC [X0]kF F	(62)
≤	σ3	1 + σ1σ2H	e2α	+2⅛i! kHC [X]kF，	(63)
where Eqn. 59 follows from Lemma 7, Eqn. 60 holds because FFN operates row-wisely on feature
matrix, and Eqn. 61 is due to the definition of Lipschitz constant. Finally, Eqn. 63 is yielded from
Proposition 5.	□
Lemma 7. Given X ∈ Rn×d, kHC[X]kF ≤ kX - 1zTkF for all z ∈ Rd.
Proof. We prove the Lemma by showing that z* = XT 1/n achieves the minimum of the optimiza-
tion problem arg minz kX - 1zT k2F.
kX - 1zT k2F = Tr(XT - z1T)(X - 1zT)	(64)
= Tr(XTX) - Tr(z1TX) - Tr(XT1zT) + Tr(zT 1T 1z)	(65)
= nzTz - 2Tr(XT1zT) + Tr(XTX)	(66)
It is easy to show the derivative in terms of z:
Vz ∣∣X — 1zT kF = 2nz — 2X T1.	(67)
Therefore, z* = 1XT1 achieves the minimum.	□
D Deferred Remarks on Section 2.4
D. 1 Connection with Random Walk Theory
We add that the asymptotic evolution of feature representations can be interpreted through the lens
of random walk theory. We can regard self-attention map A as probability transition matrices for a
Markov chain. Since each entry is larger than zero, the Markov chain should be irreducible. This
implies the Markov chain will converge into a unique stationary distribution π (Randall, 2006). Let
ai denote the i-th row of A, then for all i = 1,…,n we have k(ai)T A — ∏Tk ≤ λ∣ai — π∣, where
λ ∈ (0, 1) is the mixing rate of the transition matrix A. As a consequence, liml→∞ Al = 1πT
yields a pure low-pass filter. When repeatedly applying this self-attention matrix to feature maps,
liml→∞ AlX → 1πTX only preserves the rank-1/DC portion of the signals, which is consistent
with our Theorem 1. Nevertheless, this interpretation does not bring other transformer components
into consideration. And our theory further provides a concrete convergence rate with respect to the
network parameters (Theorem 3).
D.2 Remarks on Dong et al. (2021)
Here we respectfully elaborate on the hidden assumptions in the proof of the current preprint of Dong
et al. (2021).
1) In the proof of Lemma A.3, Taylor expansion was used to approximate and upper bound an
exponentiation. However, to let the right-hand side upper bound satisfied, we conjectured that the
authors implicitly assumed Eij — Eij0 is bounded around zero. After directly communicating with
the authors, they confirmed that a missed assumption here is maxi,j (Eij — Eij0 ) ≤ 1.
2) In the proof of Lemma A.1, to let Eqn. (8)-(9) hold, the authors may have assumed R, WV ≥ 0,
where ≥ denotes entry-wise inequality. As the authors suggested, an entry-wise absolute value can
be imposed to R and WV as a simple fix, without influencing their 'ι and '∞ norm. However,
18
Published as a conference paper at ICLR 2022
even after those changes, we still have difficulty walking through Eqn. (6)-(8), and we are currently
communicating with the authors on this matter.
3) In the proof of Lemma A.1, we find Eqn. (12) may not be satisfied in general. We can raise
the following counterexample: Since E, r , R, WV can be any matrices, we simply let D =
diag(2, 3), softmax(r) = [0.8 0.2]T , R = WV = I. Then kD1softmax(r)T RWV k1 = 4
while kD1k∞kRk1 kWV k1 = 3, which disproves the claim. We conjecture that some additional
prerequisite constraints on E , r might be needed here to proceed the derivation, and we are currently
communicating with the authors on this matter.
D.3 Generalize to Other Attention Mechanism
Our theorizing can be smoothly generalized to other attention mechanisms because our Theorem 1
and 3 do not require any prior knowledge on pre-softmax pairwise correlation P .
Logistic Attention. We refer logistic attention to the attention mechanism used in Velickovic et al.
(2018); Verma et al. (2018), where attention is calculated via a linear combination:
A
ij
exp xiT uQ + xjT uK + b
Pt exp (XiuQ + XtuK + b)
(68)
where uQ and uK are query/key parameters, b is the bias term. With the same condition in Theorem
3, we can upper bound α by |(kuK k2 + kuQk2)γ + b|.
L2 Distance Attention. L2 distance based attention (Kim et al., 2021) Lipschitz formulation of
self-attention. The pair-wise attention can be written as follows:
A	eχp (-kxT WQ - Xj WKk2/T)	(69)
j	PteXP (-kχj WQ -XT WK II2/t )
where WQ and Wk are query/key weights, and τ is a scaling factor. Similar to Theorem 3, we can
upper bound a by (IIWK∣∣2 + IIWQIl2)2γ2∕τ.
E An Auxiliary Lemma for AttnScale
Lemma 8. Let A = F AF-1 be the spectral response of attention matrix A, and parameterize a
low-filter by L = FT diag(β, 0,…，0)F. Then β* = 1 is the optimal solution of the following
optimization problem: arg minβ IA - LIF.
Proof. First we make simplification L =FT diag(β, 0,…，0)F = β11j (refer to Appendix A).
Then we have:
IA - LI2F = IA - β11T I2F =Tr(A-β11T)T(A-β11T)	(70)
Tr	(AT A - β AT 11T - β 11j A + β211j) n	nn	(71)
巴	Tr 11T — 2β Tr 11T A + Tr AT A	(72)
n	n	
β2	nn - ~ XX Aj+Tr ATA	(73)
j=1 i=1
= β2 - 2β+TrATA	(74)
From Eqn. 74, β* = 1 achieves the minimum of the objective function.	□
F	More on Visualization
F.1 Details on Figure 1
To verify our Theorem 3, we depict the high-frequency intensity of each layer’s output and its theo-
retical upper bound. Our visualization is based on the official checkpoint of 12-layer DeiT-S. Since
19
Published as a conference paper at ICLR 2022
training a ViT without either FFN or residual connection will certainly cause failure, we remove these
components directly from the pre-trained model to illustrate the effects of different components. We
use logarithmic scale for the purpose of better view. Let Xl denote the output of the l-th layer, and X0
be the initial inputs. For red line, we directly calculate log(kHC [Xl]kF/kX0kF) at each layer. For
blue line, we first obtain the coefficient γl in Section 2.2 and 2.3 with respect to network parameters

(e.g., we can compute γl
ne2α
e2α+n-1
kWV k2 for attention only architecture). Then we estimate
the upper bound by γlkHC [Xl-1]kF and apply the logarithm by log(γl kHC [Xl-1]kF /kX0 kF). To
summarize, one can see without residual connection, the first two sub-figures imply an exponential
convergence rate, which is consistent with our Theorem 3.
F.2 More Visualization on Spectrum
In this appendix, we provide more visualization on the spectrum of attention map to validate our
Theorem 1. We compute the spectrum of attention map A for both Fig. 2 and Fig. 5 in the
following way. By regarding A as a linear filter, its Fourier-domain response is another linear kernel
Λ = FAF-1. When Λ is applied to a spectrum X = FX of signals x, the i-th frequency response
will be ΛiX, where Λ% is the i-th row of Λ. Hence, we can use ∣∣Λi∣∣2 to evaluate the spectral
response intensity of the i-th frequency band. Below we provide a complete spectral visualization of
attention maps computed from a random sample in ImageNet validation set.
5 ( 0			3 1 1			Ig 7.5 54 25 g			β 4 i C		
											
											
I			l.« «.9 β.β	VZ	K	LO 03			24 U 14 。5 04		
												
14 0-5	∙w⅝›‰Aʌʌɪ		1.5 l.« as		⅛ι	LO «3		∙Λjv⅛^v~~-	24 U 14 。5 04		AAtA4⅜>naIMZ
											
14 0-5			l.« as						U LO «3 g			U 14 。5	^w*-λaΛ⅛	AA
											
: I 0	w⅛⅝-λ<⅜⅜a⅜ ʌʌ		i.« as «.«			LO «3 g			U 14 。5 04	MnAIiMhAli	Xvu√λ>λ⅛
											
■ I I« 8, ɪl.s 14 0-5			i.« as		kʌʌʌʌjw	U LO «3			14 。5 04	J⅛MΛλ**∙∕√√	∖≠A-ma⅛m⅛λ
			β.o								
	U*M*→√*AΛ∙	WVa-*<Λ*ι,	1.5 i.« as			LO «3 g			24 IJ 14 。5		
											
14 0-5			1 1 0			U LO «3 g			IJ 14 。5		
											
24 1-5 14 0-5 0»	-*χ*w√⅛⅛**⅝Λ∙	∙ΛAU*-Λ-<kλ-*	1.5 l.« as «.«			24 U LO «3	>*>mi>∙w∙h4Λ4		IJ 14 。5 04	∕∙√Wh3mM⅞Λ*	
						10					
24 1-5 14 0-5 04			1 1 0	*~->>-Λ√U4λA"	-Λλ*∣Λλ-*→-√	4 3 1 1 0	”4		IJ 14 。5 04		
											
24 1-5 14 0-5 0»			i.« 1.5 l.« as «.«			1 1 0	⅜*ΛtwMmλA<		24 IJ 14 。5 04		*⅝Λ⅜⅝⅛.⅜⅛..»
											
( ? : I 0				1 1 «			1 1 t			24 IJ 14 。5 aa			
« M IM U« 20«	« M l«0 U« 2«0	0	8 IMlSo 2M 0 M UO U« 2M
ft⅛quw∣C)∣
1Λ 04 Oe OA			U LO «3 g	w"4⅜AΛ∙	-ΛA<⅝⅛i.⅛⅛i
					
IAO «.75 08 eas			IM «.75 。31 oɪs		
					
15 1Λ 05			U LO 03 OS 04	⅛‰	
					
1Λ 05	^.^λaλA^Aλ∕v^*m.		U LO «3 04		
					
15 1Λ 05			U LO «3	4⅛≠4m^ΛA^^Λ⅛m⅛λ4'	
					
15 1Λ 05			125 IM «.75 。31 oɪs		
					
15 1Λ 05	w>λk∙*-λΛA	Jbv⅛A~w^	1 1 0		
					
? : I 0			24 U LO «3 04	ʌ⅛ʌ-**-v^∖Λ-	
					
- ■ I 0			1 1 0		
					
iθ 15 1Λ 05 g			3 1 1 0	-⅛*YvΛ∏^**O*λA*	
					
ιa 15 1Λ 05 g			1 1 0		⅝∕UMj⅜⅛"I∣∣ ■**■,■
					
? : I 0	〜4∙>∙YaM<⅛-Λ-j		1		
« S« l«a IM 28
0	SOIMi5。	2βa
Figure 5:	Visualize the spectrum of attention maps. Each row demonstrates every head at a same
layer, and from top to bottom, the 12 rows correspond to 1 ~ 12-th layer, for left to right, the 6
columns correspond to 1 - 6-th head, respectively. Best view in a zoomable electronic copy.
20
Published as a conference paper at ICLR 2022
F.3 Details on Similarity Curves (Figure 4)
In Fig. 4, we visualize the cosine similarity of attention maps and feature maps to show the
effectiveness of our AttnScale and FeatScale on 24-layer DeiT, respectively. We follow the definition
in Zhou et al. (2021a) to compute the cosine similarity metric for attention maps. Instead of measuring
cross-layer similarity, we calculate average cross-patch similarity at the same layer. Given the layer
index l and corresponding attention maps A(l,h) ∈ Rn×n, the cosine similarity can be computed by:
Malttn
n(n - 1)H
H n n	A(l,h)T A(l,h)
:,i :,j
^A^a .⅛1 ∣∣A(l,h)∣∣	∣∣A(l,h)∣∣
h=1 i=1 j=i+1 A:,i 2 A:,j 2
(75)
2
where :(,li,h) denotes the i-th column of A(l,h), and H is the number of heads. The cosine similarity
between i-th and j-th column of A(l,h) measures how the contribution of one token (say the i-th
token) varies from the other (say the j-th token). We average the similarity between every pair
of tokens’ attention map (excluding the self-to-self similarity) and every attention head. We refer
interested readers to Zhou et al. (2021a) for more details.
We use the similar metric to compute similarity for feature maps. Following Gong et al. (2021), we
compute pair-wise cosine similarity between every two different tokens. Formally, given the layer
index l, and its output X (l) ∈ Rn×d, the cosine similarity is estimated by:
Mfleat
2
n(n - 1)
nn
XX
i=1 j=i+1
(76)
where Xi(,l:) denotes the i-th row of X(l). The cosine similarity between between i-th and j-th row of
X (l) measures how similar the feature representations of two tokens are. Likewise, we average the
similarity between every pair of tokens’ features except for the self-to-self similarity. More details
can found in Gong et al. (2021). We additionally provide a visualization of these two metrics for
12-layer DeiT in Fig. 12.
G Deferred Experiments and Model Interpretation
G.1 Fine-Tuning Experiments
Our deferred fine-tuning experiment with CaiT (Touvron et al., 2021b) results are presented in
Table 3. Different from trining scratch, we fine-tune CaiT with AttnScale and FeatScale parameters
from the pre-trained models for 60 epochs following Gong et al. (2021). For a fair comparison, we
simultaneously train a plain CaiT for another 60 epochs. During fine-tuning, we reduce learning rate
to 5 × 10-5 and weight decay to 5 × 10-4. All other hyper-parameters and training recipe are kept
consistent with the original paper (Touvron et al., 2021b).
Table 3: Experimental evaluation of finetuning AttnScale & FeatScale with CaiT. The number inside
the (↑ ∙) represents the performance gain compared with the baseline model, and accuracies Within/out
of parenthesis are the reported/reproduced performance.
Backbone	Method	Input size	# Layer	# Param	FLOPs	Throughput	Top-IAcc(%)
	CaiT-XXS	224	24	12.0M	2.53G	589.3	-77.5(77.6)-
	CaiT-XXS + AttnScale	224	24	12.0M	2.53G	548.1	77.8 (↑ 0.3)
CaiT	CaiT-XXS + FeatScale	224	24	12.0M	2.53G	573.5	77.8 (↑ 0.3)
	CaiT-S	224	24	46.9M	8.74G	371.9	-82.6 (82.7)-
	CaiT-S + AttnScale	224	24	46.9M	8.75G	339.0	82.8 (↑ 0.2)
	CaiT-S + FeatSCaIe	224	24	46.9M	8.75G	358.2	82.9 (↑ 0.3)
G.2 Visualization and Interpretation of AttnS cale
In this subsection, we provide visualization to interpret our AttnScale and further support our
experiments.❶ In Fig. 6 we visualize the learned weights of our AttnScale. We observe conclude
our AttnScale are successfully trained to amplify the high-pass component. We also find when layer
21
Published as a conference paper at ICLR 2022
index goes larger, the scaling weights turns larger to prevent attention collapse at deeper layer.❷
We also compare the attention map produced by AttnScale with those produced by original DeiT.
We observe from Fig. 8 that our AttnScale can extract more salient and higher contrastive attention
than vanilla DeiT, which indicates our AttnScale possesses higher capability to distinguish tokens
from larger variety of attention schemes.❸ To be more objective, we plot the spectrum of a 24-layer
DeiT’s attention maps with/without our AttnScale in Fig. 9 and 10. The visualization procedure
has been elaborated in Sec. F.2. We find that attention maps from AttnScale enjoy richer filtering
diversity, capable of performing high-pass (row 2, column 3) and band-pass (row 12, column 5)
filtering, instead of only low-pass filtering (see Fig. 9).
Θpnt!u6ew
4
2
00123456789
1011
1011
2
00123456789
1011
■ w ■
0123456789
Layer Index
1011
6
4
2
1011
00123456789
6
4
2
00123456789 1011
6
4
2
0

6
4
6
4
2
0
Figure 6:	Visualize the learned weights of DeiT-S + AttnScale. Each sub-plot depicts the scaling
weights of the same head for different layers. For left to right, top to bottom, six sub-figures
correspond to 1 ~ 6-th head, respectively. Best view in color.
4 2 0 4 2
① Pn⅛u6ew
0123456789 1011
0
0123456789 10 11
Layer Index
4 2 0 4 2
0123456789 10 11
Figure 7:	Visualize the learned weights of DeiT-S + FeatScale. Each sub-plot depicts two groups of
scaling weights of the same head for different layers. For left to right, top to bottom, six sub-figures
correspond to 1 ~ 6-th head, respectively. Best view in color.
G.3 Visualization and Interpretation of FeatScale
In this subsection, We provide visualization to interpret our FeatScale. ❶ We plot the scaling
weights of FeatScale in Fig. 7. We observe that the re-weighting factors learned for high-frequency
components t is consistently larger than the Weights for the DC term s, Which indicates our FeatScale
is successfully trained to elevate high-frequency features against the dominance of DC component.
Similarly, the gap between S and t becomes huger when going deeper.❷ We also demonstrate the
proportion of feature maps’ high-frequency component in Fig. 11 for both 12 (loWer one) / 24(upper
one) -layer DeiT. The proportion value is calculated by kHC [X]kF/kXkF. We find high-frequency
signals diminish quickly at deeper layer, and 24-layer DeiT suffers from a faster pace. Our FeatScale
is effective to keep the high-frequency signals stand for both 12-layer and 24-layer DeiT.
22
Published as a conference paper at ICLR 2022
-0.5
Block 0
Block 2
Block 4
Block 6
Block 8
Block 10
Figure 8: Visualize the attention map of DeiT-S with/without AttnSCale. 4 X 4 max pooling has been
applied. The first row visualizes attention maps without AttnScale, and the second row visualizes
attention maps with AttnScale. Each column corresponds to the layer noted by its sub-title. The
attention map are computed from a random sample in ImageNet validation set. We only demonstrate
the first head of each layer. Best view in a zoomable electronic copy.

1」一	：	J.. J		.“-∙~~--ʌʌʌɪʌʌʌ**"-	I 0.0	—__ I :		[
\-JL…一	：	ɪ	ɪ	…」—.…I	:			I :	-~~^ʌʌʌɪʌʌʌ~0	I
1AAAAAA	::	「一AL-~.	：	ɪ^一1 ；；	∖AΛΛ∕^^ΛΛΛ∕√ I 0.8	∖ΛΛΛ∕^^AAAΛ/1 :		I	
;|	Λ J	'fi VxAAAzw	鼻		::	-^^ʌʌʌʌʌʌAAʌʌʌ, I	αUWMUIJ。:		∙>aλ1λ∕>	I 05	
1					∙^aA^∙A	：:：	^λA^Aλλa^J 0		JL	Tl ：；	-ʌ--ʌ Aʌɪʌʌʌʌʃ-- I 0	“.…	
	：			I 0	_ _ JL -」；；	_ 一^A0	
Γ ɪ )1		鼻		：	—JL__ I ：；	-ɪj ：	-..-ɪ.- .J ɪ;	
J			:		:;	一ɪ^]：	„1^ J ；；		ɪ	10	
			:		--ɪ：		f ~,^⅝Λ⅝λA∙^∙A∆∕Λz^⅜-
j		：		AL		：	-Iuu⅝>MX√⅛λltλ<Wu<⅝-⅛*⅛J⅛∙ I 0 5		λaΛIλΛa	 I £	:				Aʌʌ.			
:L-JL—J	:	一JL 一	:				I ° 5		^ʌʌɪʌʌ^	I °：°	
〜” ――rKAɪA^,d.一…				
		..	_ ..				
L—J^一I ]一JL，
		I )一ɪj Lɪj
0pBE6eΣ
L一，一	： ： : ： ： ： ： ： 25 I 0.0 :	，--"1'-ʌʌʌɪʌʌ0-o-i …
—JL—		4i-->-⅛∕⅛l>Λ∙⅜'Λ∙一.〜〜一
L jL 一		…，1「
L —L 一		.■».——1-wj^ʌɪʌʌʌ 1. —■.»■
L一L一		—」…1
—一一 L一一		I
⅝rt*l"⅜MII⅛l⅛Aj⅛><⅞⅜⅛to⅛>⅛∣		⅜⅝⅝>⅝⅜⅝⅛Λw⅜jl⅛Λw⅜⅛⅝⅜⅜⅛rf⅜<
L .」.「		>r⅝<<⅝M>⅛l⅛⅛l~⅜u<⅝⅛⅝⅜y⅛o><
L」一—		j w><⅝*l⅜⅛l⅛lι√l√⅜⅛<<<⅛<" Z
L-JL—.		1 .
		I 「
--		-	----	I 。1	……一....	I
0-50	100	150-200	0	50	100.150	200
		I--nʌɪʌʌ-^一.〜I
l-ɪ-	:		-a-√⅝a1a√‰^-	 I
	:	，一 J
	:		I	
11^一一	:	一.一.[-，
l-ɪ-	:	一」「
;1I	:	一 .1；
JL^	:	
JL^	:	…，，J
I—	:	—一」一一—
1____	:	一 IJ
0	50	100	150	200	0	50	100	150	200
Frequency
Figure 9:	Visualize the spectrum of attention maps without AttnScale. Each row demonstrates every
head at a same layer, and from top to bottom, the 24 rows correspond to 1 ~ 24-th layer, for left to
right, the 6 columns correspond to 1 ~ 6-th head, respectively. Best view in a zoomable electronic
copy.
23
Published as a conference paper at ICLR 2022
	IJ 14		as «.«	I	7-5 54 25	⅛⅛⅛w⅜aA∕AλA⅜>wΛ1⅜>	7-5 54 25		14 CS	.一 ʌ->ɪʌ-^ _
	IJ 14 OS				4 3		3 2 1		OJO OJlS		U 10 CS	EF，TTE'"∣"W∣r'" Fml『
::l^ʌʌ^	1 1		1 1	—**iJ>λΛJ⅛1<1Λλ∙∙U*∙>-	«.75 OjO	□L	10 CS	x∕sz∖AΛΛΛ^ΛΛAa∕w	1 «	I
(IJ	3 1 1	JJJJAWAAlUy	l.« as	^AAMAAAʌʌʌ,	10 CS	……!……	«.75 OJO OJlS	-z~~∙>⅝^aA^AA∕s				U 10 。5	¼j⅜∙^√%λA^∙^^Aλat⅛a-∙
♦ >akλλMAAAʌʌʃ	3 1 1		¢.3 «.2	^√⅛λλA∕^AAλa∕∖^-	10 CS	-⅛⅝<⅝w⅛ΛiΛ^AΛ√∙√w⅛⅛⅝⅛-	0.1 04	!	4 1	
'I		3 1 1		1 1	--j-∕∖AAAʌAΛλ√∙v^	10 CS		OJO OJlS			∙√λ∕∖A^ΛAzV^_____	02 0.1	.-...i<w>>⅝a1∙Λ√⅜)∙⅝⅝.. I.
	14 ¢5		7.S s.a 2.5	-*^⅛*V'X^AλΛ<x√u*>	U 10 CS	AAʌ-^	1 «	JL	54 25	
-I 。叫			1 O		J…		1 1		1 C	“ ，”5”人 >1⅛∖ A"d"⅛⅛ e"z	1 1	^⅛∙>∙√'∙<aAJ^Aλ∕v*∙→~∣>	L« CS	-			-^-ʌɪʌʌ-				
	14 ¢5		5.« 2.5			15 10 CS		1 1		3 1 1	^nʌ-ʌAAAɪʌAH^
	IJ 14 ¢5	∙∙∙Λ≠A>*<AAλ^AΛΛ*⅛⅛Jw.	1.5 l.« as		3 1 1	∙^>∕√∖MAλ∕'v<~'-.	1 1		1 1	∙~^A>Λ√ςλιl<l*IAAVΛ∙A~~*
I	IJ 14 ¢5	∙m⅛∙*⅛λλ1λ1λ¼a0*∙√⅛	ι «	I	1 1		10 CS		4 1	w^ʌ/ʌʌʌʌAΛΛa∙λ*,
I 1I √⅜‰∕≠¾l⅛⅛¼⅜√	1 1		1.0 as		10 CS		1 1	W⅛⅛w	1 «	」.
	IJ 14 ¢5		«.2 0.1		ax 02		1 1		3 1 1	
	3 1 1		1 1	≠*λ⅛√√i⅛a∕⅛ΛAaΛvWm∙⅛	1 O	」一	«.75 OJO OJlS		1 1	
l-5l W	3 1 1		4 1		54 25		4 1	∙⅛⅛^v⅛⅛ΛAJllλAΛ1⅛s><⅛*	1 «	I
	54 2.5		1.S l.« Os		10 CS		U 10 CS		S «	」.
	54 2.5		5.« 2.5		1 1		M⅛AΛ∣i<ΛΛrfMw-⅜	7.5 54 25	^Λ⅜∙√√l/XV√½λΛ∕	4 1	
	1 1	⅛√⅜MΛλΛiΛ⅛⅛MH⅛∙⅜	l.« as		7.5 54 25	⅛⅛⅛∕⅛KAk∣ΛAʌw>Λ*t	U 10 CS			1 «	
	10 S		s «		54 25	⅜w⅛AU⅛w⅜	1		S «	UW√⅜V⅜⅛W⅛⅛*j
-rɪ 00 I - "i -∙|A-⅛⅛~-— --q	1 «	…、、,,,	3 1 1	⅛¼⅜⅛⅛λ∕⅛L⅛*a⅝4≠⅜	7.5 54 25		54 25		20 1«	M≠⅛⅛¼Λ*
	1« S		20 1«	⅛I⅛⅛⅛mL	to 1«		7.5 54 25	W⅛⅛M¼W⅝	U 1« S	W⅛⅛⅛⅛Mj
	20 «	WMM⅛⅛JL⅜¼⅜jm4	1« «	一--WiAJr	2S «		1« S	⅜m⅝⅛M¼⅛φi	50 «	⅜ffW*M⅛M⅜⅝⅛Mf
j.		4 1	√⅛⅛⅝A⅜⅛⅛⅛^	7.S SΛ 2.5	⅛λ*a≠*mλaΛλ√v*⅛λ*u	U 1« S		U 1« S	⅛⅜fΛAA^Av⅛⅝⅛¼^	25 04	!
	S Λ	7. 6 ,y⅜Λ>Λ∙l∙ΛjA⅛∙⅛≠⅛<√<*⅜∙	5.« 2.5			4 1		1« Λ		3 1 1	⅛Mw⅜>⅛Λiλ1λΛ⅜⅜λw⅛i⅛M
------------- U ------------ --------------- -------------- u 	 ------------------------
0 SaleolaaM a s« im u« 2αα a s« ιαα u« z«a « sa ιaa iso ɪaa a	Sale0	150 2M a Saleolaaea
Figure 10:	Visualize the spectrum of attention maps With AttnScale. Each row demonstrates every
head at a same layer, and from top to bottom, the 24 rows correspond to1 ~ 24-th layer, for left to
right, the 6 columns correspond to1 ~ 6-th head, respectively. Best view in a zoomable electronic
copy.
1.0
≥:0.9
lθ,8
艾
0.7

DeiT-S
DeiTS + FeatScaIe
至0.8
⅛
军0.6
in
^0.4
ω
8 0.2
DeiT-S
DeiT-S + AttnScale
0.90.80.7
≡o=wloO
12	15
LayerIndeX
18	21	24
5 0 5
,..3.32
Oooo
Λ4ze=E-s φ⊂-ωou
3	6	9	12
Layer Index
0	2	4	6	8	10
Layer Index
40
Figure 11:	Visualize the proportion of the
high-frequency component of feature maps
with/without our FeatScale on 12/24 layer
DeiT. Refer to Appendix G.3 for details.
Figure 12:	Visualize cosine similarity of at-
tention and feature maps with/without our pro-
posed methods on 12-layer DeiT. Refer to Ap-
pendix F.3 for details.
24