Published as a conference paper at ICLR 2022
Transfer RL across Observation Feature
Spaces via Model-Based Regularization
YanchaoSun** RuijieZheng* XiyaoWang* AndrewCohen* FurongHuang,
t University of Maryland, College Park * Unity Technologies
*{ycs,rzheng12,xywang,furongh}@umd.edu *andrew.cohen@unity3d.com
Ab stract
In many reinforcement learning (RL) applications, the observation space is spec-
ified by human developers and restricted by physical realizations, and may thus
be subject to dramatic changes over time (e.g. increased number of observable
features). However, when the observation space changes, the previous policy will
likely fail due to the mismatch of input features, and another policy must be trained
from scratch, which is inefficient in terms of computation and sample complexity.
Following theoretical insights, we propose a novel algorithm which extracts the
latent-space dynamics in the source task, and transfers the dynamics model to the
target task to use as a model-based regularizer. Our algorithm works for drastic
changes of observation space (e.g. from vector-based observation to image-based
observation), without any inter-task mapping or any prior knowledge of the target
task. Empirical results show that our algorithm significantly improves the effi-
ciency and stability of learning in the target task.
1	Introduction
Deep Reinforcement Learning (DRL) has the potential to be used in many large-scale applications
such as robotics, gaming and automotive. In these real-life scenarios, it is an essential ability for
agents to utilize the knowledge learned in past tasks to facilitate learning in unseen tasks, which is
known as Transfer RL (TRL). Most existing TRL works (Taylor & Stone, 2009; Zhu et al., 2020)
focus on tasks with the same state-action space but different dynamics/reward. However, these
approaches do not apply to the case where the observation space changes significantly.
Observation change is common in practice as in the following scenarios. (1) Incremental environ-
ment development. RL is used to train non-player characters (NPC) in games (Juliani et al., 2018),
which may be frequently updated. When there are new scenes, characters, or obstacles added to the
game, the agent’s observation space will change accordingly. (2) Hardware upgrade/replacement.
For robots with sensory observations (Bohez et al., 2017), the observation space could change (e.g.
from text to audio, from lidar to camera) as the sensor changes. (3) Restricted data access. In some
RL applications (Ganesh et al., 2019), agent observation contains sensitive data (e.g. inventory)
which may become unavailable in the future due to data restrictions. In these cases, the learner may
have to discard the old policy and train a new policy from scratch, as the policy has a significantly
different input space, even though the underlying dynamics are similar. But training an RL policy
from scratch can be expensive and unstable. Therefore, there is a crucial need for a technique that
transfers knowledge across tasks with similar dynamics but different observation spaces.
Besides these existing common applications, there are more benefits of across-observation transfer.
For example, observations in real-world environments are usually rich and redundant, so that directly
learning a policy is hard and expensive. If we can transfer knowledge from low-dimensional and
informative vector observations (usually available in a simulator) to richer observations, the learning
efficiency can be significantly improved. Therefore, an effective transfer learning method enables
many novel and interesting applications, such as curriculum learning via observation design.
In this paper, we aim to fill the gap and propose a new algorithm that can automatically transfer
knowledge from the old environment to facilitate learning in a new environment with a (drastically)
*The work was done while the author was an intern at Unity Technologies.
1
Published as a conference paper at ICLR 2022
Observed features
Target
Representation Learning Policy Learning
Figure 1: An example of the transfer problem with changed observation space. The source-task agent observes
the x-y coordinates of itself and the goal, while the target-task agent observes a top-down view/image of the
whole maze. The two observation spaces are drastically different, but the two tasks are structurally similar. Our
goal is to transfer knowledge from the source task to accelerate learning in the target task, without knowing or
learning any inter-task mapping.
different observation space. In order to meet more practical needs, we focus on the challenging
setting where the observation change is: (1) unpredictable (there is no prior knowledge about
how the observations change), (2) drastic (the source and target tasks have significantly different
observation feature spaces, e.g., vector to image), and (3) irretrievable (once the change happens,
it is impossible to query the source task, so that the agent can not interact with both environments
simultaneously). Note that different from many prior works (Taylor et al., 2007; Mann & Choe,
2013), we do not assume the knowledge of any inter-task mapping. That is, the agent does not know
which new observation feature is corresponding the which old observation feature.
To remedy the above challenges and achieve knowledge transfer, we make a key observation that,
if only the observation features change, the source and target tasks share the same latent space and
dynamics (e.g. in Figure 1, O(S) and O(T) can be associated to the same latent state). Therefore,
we first disentangle representation learning from policy learning, and then accelerate the target-task
agent by regularizing the representation learning process with the latent dynamics model learned in
the source task. We show by theoretical analysis and empirical evaluation that the target task can be
learned more efficiently with our proposed transfer learning method than from scratch.
Summary of Contributions. (1) To the best of our knowledge, we are the first to discuss the transfer
problem where the source and target tasks have drastically different observation feature spaces,
and there is no prior knowledge of an inter-task mapping. (2) We theoretically characterize what
constitutes a “good representation” and analyze the sufficient conditions the representation should
satisfy. (3) Theoretical analysis shows that a model-based regularizer enables efficient representation
learning in the target task. Based on this, we propose a novel algorithm that automatically transfers
knowledge across observation representations. (4) Experiments in 7 environments show that our
proposed algorithm significantly improves the learning performance of RL agents in the target task.
2	Preliminaries and Background
Basic RL Notations. An RL task can be modeled by a Markov Decision Process (MDP) (Puterman,
2014), defined as a tuple M = hO, A, P, R, γi, where O is the state/observation space, A is the
action space, P is the transition kernel, R is the reward function and γ is the discount factor. At
timestep t, the agent observes state ot, takes action at based on its policy ∏ : O → ∆(A)(Where ∆(∙)
denotes the space of probability distributions), and receives reward rt = R(ot, at). The environment
then proceeds to the next state ot+ι 〜P(∙∣ot,at). The goal of an RL agent is to find a policy
π in the policy space Π With the highest cumulative reWard, Which is characterized by the value
functions. The value of a policy π for a state o ∈ O is defined as Vπ (o) = E∏,p [P∞=o γtrt∣oo =
o]. The Q value of a policy π for a state-action pair (o, a) ∈ O × A is defined as Qπ (o, a) =
E∏,p [P∞=o γtrt∣oo = o,ao = a]. Appendix A provides more background of RL.
Representation Learning in RL. Real-World applications usually have large observation spaces
for Which function approximation is needed to learn the value or the policy. HoWever, directly
learning a policy over the entire observation space could be difficult, as there is usually redundant
information in the observation inputs. A common solution is to map the large-scale observation
into a smaller representation space via a non-linear encoder (also called a representation mapping)
2
Published as a conference paper at ICLR 2022
φ : O → Rd, where d is the representation dimension, and then learn the policy/value function over
the representation space φ(O). In DRL, the encoder and the policy/value are usually jointly learned.
3	Problem Setup: Transfer Across Different Observation Spaces
We aim to transfer knowledge learned from a source MDP to a target MDP, whose observation
spaces are different while dynamics are structurally similar. Denote the source MDP as M4 (S) =
hO(S) , A, P(S) , R(S) , γi, and the target MDP as M(T) = hO(T) , A, P(T) , R(T) , γi. Note that O(S) and
O(T) can be significantly different, such as O(S) being a low-dimensional vector space and O(T) being
a high-dimensional pixel space, which is challenging for policy transfer since the source target policy
have different input shapes and would typically be very different architecturally.
In this work, as motivated in the Introduction, we focus on the setting wherein the dynamics
((P (S), R(S)) and (P(T), R(T))) of the two MDPs between which we transfer knowledge are defined on
different observation spaces but share structural similarities. Specifically, we make the assumption
that there exists a mapping between the source and target observation spaces such that the transition
dynamics under the mapping in the target task share the same transition dynamics as in the source
task. We formalize this in Assumption 1:
Assumption 1. There exists a function f : O(T) → O(S) such that ∀o(iT), o(jT) ∈ O(T), ∀a ∈ A,
P(T)(o(jT)|o(iT),a) = P(S)(f(o(jT))|f(o(iT)),a), R(T)(o(iT),a)=R(S)(f(o(iT)),a).
Remarks. (1) Assumption 1 is mild as many real-world scenarios fall under this assumption. For
instance, when upgrading the cameras of a patrol robot to have higher resolutions, such a mapping
f can be a down-sampling function. (2) f is a general function without extra restrictions. f can be a
many-to-one mapping, i.e., more than one target observations can be related to the same observation
in the source task. f can be non-surjective, i.e., there could exist source observations that do not
correspond to any target observation.
Many prior works (Mann & Choe, 2013; Brys et al., 2015) have similar assumptions, but require
prior knowledge of such an inter-task mapping to achieve knowledge transfer. However, such a
mapping might not be available in practice. As an alternative, we propose a novel transfer algorithm
in the next section that does not assume any prior knowledge of the mapping f. The proposed
algorithm learns a latent representation of the observations and a dynamics model in this latent
space, and then the dynamics model is transferred to speed up learning in the target task.
4 Methodology: Transfer with Regularized Representation
In this section, we first formally characterize “what a good representation is for RL” in Section 4.1,
then introduce our proposed transfer algorithm based on representation regularization in Section 4.2,
and next provide theoretical analysis of the algorithm in Section 4.3.
4.1 Characterizing Conditions for Good Representations
As discussed in Section 2, real-world applications usually have rich and redundant observations,
where learning a good representation (Jaderberg et al., 2016; Dabney et al., 2020) is essential for
efficiently finding an optimal policy. However, the properties that constitute a good representation
for an RL task are still an open question. Some prior works (Bellemare et al., 2019; Dabney et al.,
2020; Gelada et al., 2019) have discussed the representation quality in DRL, but we take a different
perspective and focus on characterizing the sufficient properties of representation for learning a task.
Given a representation mapping φ, the Q value of any (o, a) ∈ O × A can be approximately rep-
resented by a function of φ(o), i.e., Q(o, a) = h(φ(o); θa), where h is a function parameterized by
θa. To study the relation between representation quality and approximation quality, we define an
approximation operator Hφ , which finds the best Q-value approximation based on φ. Formally, let
Θ denote the parameter space of function h ∈ H, then ∀a ∈ A, HφQ(o, a) := h(φ(o); θj), where
θa = argminθ∈θEo[k h(φ(o); θ) - Q(φ(o), a)k]. Such a function h can be realized by neural net-
works as universal function approximators (Hornik et al., 1989). Therefore, the value approximation
error kQ - Hφ Qk only depends on the representation quality, i.e., whether we can represent the Q
value of any state o as a function of the encoded state φ(o).
3
Published as a conference paper at ICLR 2022
The quality of the encoder φ is crucial for learning an accurate value function or learning a good
policy. The ideal encoder φ should discard irrelevant information in the raw observation but keep
essential information. In supervised or self-supervised representation learning (Chen et al., 2020;
Achille & Soatto, 2018), it is believed that a good representation φ(X) of input X should con-
tain minimal information of X which maintaining sufficient information for predicting the label Y .
However, in RL, it is difficult to identify whether a representation is sufficient, since there is no label
corresponding to each input. The focus of an agent is to estimate the value of each input o ∈ O,
which is associated with some policy. Therefore, we point out that the representation quality in RL
is policy-dependent. Below, we formally characterize the sufficiency of a representation mapping in
terms of a fixed policy and learning a task.
Sufficiency for A Fixed Policy. If the agent is executing a fixed policy, and its goal is to estimate the
expected future return from the environment, then a representation is sufficient for the policy as long
as it can encode the policy value Vπ . A formal definition is provided by Definition 9 in Appendix B.
Sufficiency for Learning A Task. The goal of RL is to find an optimal policy. Therefore, it is
not adequate for the representation to only fit one policy. Intuitively, a representation mapping is
sufficient for learning if we are able to find an optimal policy over the representation space φ(O),
which requires multiple iterations of policy evaluation and policy improvement. Definition 2 below
defines a set of “important” policies for learning with φ(O).
Definition 2 (Encoded Deterministic Policies). For a g^ven representation mapping φ(∙), define an
encoded deterministic policy set ΠφD as the set of policies that are deterministic and take the same
actions for observations with the same representations. Formally,
∏D := {π ∈ Π | ∃∏ : φ(O) → A s.t. ∀o ∈ O,π(o) = ∏(φ(o))},
where π is a mapping from the representation space to the action space.
(1)
A policy π is in ΠφD if it does not distinguish o1 and o2 when φ(o1) = φ(o2). Therefore, ΠφD can
be regarded as deterministic policies that make decisions for encoded observations. Now, we define
the concept of sufficient representation for learning in an MDP.
Definition 3 (Sufficient Representation for Learning). A representation mapping φ is sufficient for
a task M w.r.t. approximation operator Hφ ifHφQπ = Qπ for all π ∈ ΠφD. Furthermore,
•	φ is linearly-sufficient for learning M if ∃θa s.t. Qπ (o, a) = φ(o)>θa, ∀a ∈ A, π ∈ ΠφD.
•	φ is -sufficient for learning M if kHφQπ - Qπ k ≤ e, ∀π ∈ ΠφD.
Definition 3 suggests that the representation is sufficient for learning a task as long as it is sufficient
for policies in ΠφD. Then, the lemma below justifies that a nearly sufficient representation can ensure
that approximate policy iteration converges to a near-optimal solution. (See Appendix D for analysis
on approximate value iteration.)
Lemma 4 (Error Bound for Approximate Policy Iteration). If φ is e-sufficient for task M (with '∞
norm), then the approximated policy iteration with approximation operator Hφ starting from any
initial policy that is encoded by φ (π0 ∈ ΠφD) satisfies
2γ2e
limsup ∣∣Q - Qnk ∣∣∞ ≤  -------72,
k→∞	(1 - γ)2
where πk is the policy in the k-th iteration.
(2)
Lemma 4, proved in Appendix C, is extended from the error bound provided by Bertsekas & Tsit-
siklis (1996). For simplicity, we consider the bound in '∞, but tighter bounds can be derived with
other norms (Munos, 2005), although a tighter bound is not the focus of this paper.
How Can We Learn A Sufficient Representation? So far we have provided a principle to define
whether a given representation is sufficient for learning. In DRL, the representation is learned to-
gether with the policy or value function using neural networks, but the quality of the representation
may be poor (Dabney et al., 2020), which makes it hard for the agent to find an optimal policy.
Based on Definition 3, a natural method to learn a good representation is to let the representation fit
as many policy values as possible as auxiliary tasks, which matches the ideas in other works. For ex-
ample, Bellemare et al. (2019) propose to fit a set of representative policies (called adversarial value
functions). Dabney et al. (2020) choose to fit the values of all past policies (along the value improve-
ment path), which requires less computational resource. Different from these works that directly fit
4
Published as a conference paper at ICLR 2022
Algorithm 1 Source Task Learning
Require: Regularization weight λ; update frequency m for stable encoder.
1:	Initialize encoder φ(S), stable encoder φ(S), policy π(S), transition prediction network P and
reward prediction network RL
2:	for t = 0,1, ∙∙∙ do
3:	Take action at 〜 ∏(S) (φ(S) (otS))), get next observation o；+i and reward rt, store to buffer.
4:	Sample a mini-batch {oi, ai , ri , o0i }iN=1 from the buffer.
5:	Update P and R using one-step gradient descent With VPLP(φ(S); P) and VrLr(Φ,ss; R),
where LP and LR are defined in Equation (3).
6:	Update encoder and policy by min∏(S),@(s)Lbase(Φ(s),∏(S)) + λ(Lp (φ(S); P) + Lr(φ(S); R)).
7:	if t | m then Update the stable encoder φ(S) - φ(S).
Algorithm 2 Target Task Learning with Transferred Dynamics Models
TΓ*	∙	τ-1	1	∙	1 , ʌ 1	∙	11 τ∖ 1 7^∖ 1	1 ∙	,1	,	1
Require: Regularization weight λ; dynamics models P and R learned in the source task.
1:	Initialize encoder φ(T), policy π(T)
2:	for t = 0,1, ∙∙∙ do
3:	Take action at 〜∏(T)(φ(T)(o(T))), get next observation o；：］ and reward rt, store to buffer.
4:	Sample a mini-batch {oi, ai , ri , o0i }iN=1 from the buffer.
5:	Update encoder and policy by min@(T),π(τ)LbaSe(Φ(τ),∏ ))+λ(Lp MT); P)+Lr(Φ(T); R)),
where LP and LR are defined in Equation (3).
the value functions of multiple policies, in Section 4.2, we propose to fit and transfer an auxiliary
policy-independent dynamics model, which is an efficient way to achieve sufficient representation
for learning and knowledge transfer, as theoretically justified in Section 4.3.
4.2 Algorithm: Learning and Transferring Model-based Regularizer
Our goal is to use the knowledge learned in the source task to learn a good representation in the
target task, such that the agent learns the target task more easily than learning from scratch. Since
we focus on developing a generic transfer mechanism, the base learner can be any DRL algorithms.
We use Lbase to denote the loss function of the base learner.
As motivated in Section 4.1, we propose to learn policy-independent dynamics models for producing
high-quality representations: (1) P which predicts the representation of the next state based on
current state representation and action, and (2) R which predicts the immediate reward based on
current state representation and action. For a batch ofN transition samples {oi, ai, o0i, ri}iN=1, define
the transition loss and the reward loss as:
1N	1N
LP(φ,P) = N E(P(φ(oi),ai) - φ(oi))2,	* LR(φ,RR) = N E(R(φ(oi),ai) - ri)2	⑶
N i=1	N i=1
where φ(oi) denotes the representation of the next state oi with stop gradients. In order to fit a
more diverse state distribution, transition samples are drawn from an off-policy buffer, which stores
shuffled past trajectories.
The learning procedures for the source task and the
target task are illustrated in Algorithm 1 and Algo-
rithm 2, respectively. Figure 2 depicts the architec-
ture of the learning model for both source and tar-
get tasks. Z = φ(o) and z0 = φ(o0) are the en-
coded observation and next observation. Given the
current encoding z and the action a, the dynamics
Figure 2: The architecture of proposed method.
r∖ t r∖ ι	t ∙	，i,i ，
P and R are learned in the source task, then trans-
ferred to the target task and fixed during training.
models P and R return the predicted next encoding
^ f	-f∖ /	∖ F T ,	1	1 ^	7^∖ /	∖
^0	= P(z,	a)	and	predicted	reward r	= R(z,	a).
Then the transition loss is the mean squared error
(MSE) between z0 and Z0 in a batch; the reward loss
is the MSE between r and r in a batch.
5
Published as a conference paper at ICLR 2022
ɪ Jl	J 1 Z A 1 ♦八	♦	-II τ∖ IA	1	FK	∙	T	F
In the source task (Algorithm 1): dynamics models P and R are learned by minimizing LP and
Lr, which are computed based on a recent copy of encoder called stable encoder φ(S) (Line 5). The
computation of the stable encoder is to help the dynamics models converge, as the actual encoder φ(S)
changes at every step. Note that a stable copy of the network is widely used in many DRL algorithms
(e.g. the target network in DQN), which can be directly regarded as φ(S) without maintaining an extra
network. The actual encoder φ(S) is regularized by the auxiliary dynamics models P and R (Line 6).
Tr 八 /	，，1 /AT ♦八 A、 1	♦	-II-A FA
In the target task (Algorithm 2): dynamics model P and R
are transferred from the source task
and fixed during learning. Therefore, the learning of φ(T) is regularized by static dynamics models,
which leads to faster and more stable convergence than naively learning an auxiliary task.
Relation and Difference with Model-based RL and Bisimulation Metrics. Learning a dynamics
model is a common technique in model-based RL (Kipf et al., 2019; Grimm et al., 2020), whose
goal is to learn an accurate world model and use the model for planning. The dynamics model
could be learned on either raw observations or representations. In our framework, we also learn a
dynamics model, but the model serves as an auxiliary task, and learning is still performed by the
model-free base learner with Lbase. Bisimulation methods (Castro, 2020; Zhang et al., 2020b) aim to
approximate the bisimulation distances among states by learning dynamics models, whereas we do
not explicitly measure the distance among states. Note that we also do not require a reconstruction
loss that is common in literature (Lee et al., 2019).
4.3 Theoretical Analysis : Benefits of Transferable Dynamics Model
The algorithms introduced in Section 4.2 consist of two designs: learning a latent dynamics model
as an auxiliary task, and transferring the dynamics model to the target task. In this section, we show
theoretical justifications and practical advantages of our proposed method. We aim to answer the fol-
lowing two questions: (1) How does learning an auxiliary dynamics model help with representation
learning? (2) Is the auxiliary dynamics model transferable?
For notational simplicity, let Pa and Ra denote the transition and reward functions associated with
action a ∈ A. Note that Pa and Ra are independent of any policy. We then define the sufficiency of
a representation mapping w.r.t. dynamics models as below.
Definition 5 (Policy-independent Model Sufficiency). For an MDP M, a representation mapping
φ is sufficient for its dynamics (Pa, Ra)a∈A if ∀ɑ ∈ A, there exists functions Pa : Rd → Rd and
Ra : Rd → R such that Vo ∈ O, Pa(φ(o)) = E。，〜pa(o)[φ(o0)], Ra(φ(o)) = Ra(o).
Remarks. (1) φ is exactly sufficient for dynamics (Pa, Ra)a∈A when the transition function P is
deterministic. (2) If P is stochastic, but We have max。,。kE。，〜pa(o)[φ(o0)] — Pa(φ(o))k ≤ EP and
max。,。∖Rα(o) - Ra(φ(o))∣ ≤ er, then φ is (WP, WR)-Sufficient for the dynamics of M.
Next we show by Proposition 6 and Theorem 7 that learning sufficiency can be achieved via ensuring
model sufficiency.
Proposition 6 (Learning Sufficiency Induced by Policy-independent Model Sufficiency). Consider
an MDP M with deterministic transition function P and reward function R. If φ is sufficient for
(Pa, Ra)a∈A, then it is sufficient (but not necessarily linearly sufficient) for learning in M.
Proposition 6 shows that, if the transition is deterministic and the model errors LP, LR are zero, then
φ is exactly sufficient for learning. More generally, if the transition function P is not deterministic,
and model fitting is not perfect, the learned representation can still be nearly sufficient for learning
as characterized by Theorem 7 below, which is extended from a variant of the value difference
bound derived by Gelada et al. (2019). Proposition 6 and Theorem 7 justify that learning the latent
dynamics model as an auxiliary task encourages the representation to be sufficient for learning. The
model error LP and LR defined in Section 4.2 can indicate how good the representation is.
Theorem 7. For an MDP M, if representation mapping φ is (EP, ER)-sufficient for the dynamics
of M, then approximate policy iteration with approximation operator Hφ starting from any initial
policy π0 ∈ ΠφD satisfies
limsup ∣∣Q* - Qnk ∣∣∞ ≤	"	(ER + YePKφ,v).	(4)
k→∞	(1- γ)3
where Kφ,V is an upper bound of the value Lipschitz constant as defined in Appendix B.
6
Published as a conference paper at ICLR 2022
Transferring Model to Get Better Representation in Target. Although Proposition 6 shows that
learning auxiliary dynamics models benefits representation learning, finding the optimal solution is
non-trivial since one still has to learn P and R. Therefore, the main idea of our algorithm is to
transfer the dynamics models P , R from the source task to the target task, to ease the learning in
the target task. Theorem 8 below guarantees that transferring the dynamics models is feasible. Our
experimental result in Section 6 verifies that learning with transferred and fixed dynamics models
outperforms learning with randomly initialized dynamics models.
Theorem 8 (Transferable Dynamics Models). Consider a source task M(S) and a target task M(T)
with deterministic transition functions. Suppose φ(S) is sufficient for (Pa(S) , R(aS))a∈A with func-
,♦ A A , I ,ι	∙ .	.	I )τ∖	. ■ r ∙ τ∖ / 1 ( W Irn	Γ I / ∕∖1
tions Pa, Ra, then there exists a representation φ(T) satisfying Pa(φ(o)) = E。,〜P(T)(o)[O(。0)],
Rα,(φ(o)) = Ra)(o), for all o ∈ O(T), and φ(T) IS SuffiCIentfOr learning in M(T).
Theorem 8 shows that the learned latent dynamics models P, R are transferable from the source
task to the target task. For simplicity, Theorem 8 focuses on exact sufficiency as in Proposition 6,
but it can be easily extended to -sufficiency if combined with Theorem 7. Proofs for Proposition 6,
Theorem 7 and Theorem 8 are all provided in Appendix C.
Trade-off between Approximation Complexity and Representation Complexity. As suggested
by Proposition 6, fitting policy-independent dynamics encourages the representation to be sufficient
for learning, but not necessarily linearly sufficient. Therefore, we suggest using a non-linear pol-
icy/value head following the representation to reduce the approximation error. Linear sufficiency
can be achieved if φis made linearly sufficient for Pπ and Rπ for all π ∈ ΠφD , where Pπ and Rπ are
transition and reward functions induced by policy π (Proposition 10, Appendix B). However, using
this method for transfer learning is expensive in terms of both computation and memory, as it re-
quires to learn P∏ and R∏ for many different n,s and store these models for transferring to the target
task. Therefore, there is a trade-off between approximation complexity and representation com-
plexity. Learning a linearly sufficient representation reduces the complexity of the approximation
operator. But it requires more complexity in the representation itself as it has to satisfy much more
constraints. To develop a practical and efficient transfer method, we use a slightly more complex
approximation operator (non-linear policy head) while keeping the auxiliary task simple and easy to
transfer across tasks. Please see Appendix B for more detailed discussion about linear sufficiency.
5	Related Work
Transfer RL across Observation Feature Spaces. Transferring knowledge between tasks with
different observation spaces has been studied for years. Many existing approaches(Taylor et al.,
2007; Mann & Choe, 2013; Brys et al., 2015) require an explicit mapping between the source and
target observation spaces, which may be hard to obtain in practice. Raiman et al. (2019) introduce
network surgery that deals with the change in the input features by determining which components
of a neural network model should be transferred and which require retraining. However, it requires
knowledge of the input feature maps, and is not designed for drastic changes, e.g. vector to pixel.
Sun et al. (2020) propose a provably sample-efficient transfer learning algorithm that works for
different observation spaces without knowing any inter-task mapping, but the algorithm is mainly
designed for tabular RL and model-based RL which uses the model to plan for a policy, differ-
ent from our setting. Gupta et al. (2017) achieve transfer learning between two different tasks by
learning an invariant feature space, with a key time-based alignment assumption. We empirically
compared this method with our proposed transfer algorithm in Section 6. Our work is also related
to state abstraction in block MDPs, as studied by Zhang et al. (2020a). But the problem studied
in Zhang et al. (2020a) is a multi-task setting where the agent aims to learn generalizable abstract
states from a series of tasks. Another related topic is domain adaptation in RL (Higgins et al., 2017;
Eysenbach et al., 2020; Zhang et al., 2020b), where the target observation space (e.g. real world) is
different from the source observation (e.g. simulator). However, domain adaptation does not assume
drastic observation changes (e.g. changed dimension). Moreover, the aim of domain adaptation is
usually zero-shot generalization to new observations, thus prior knowledge or a few samples of the
target domain is often needed (Eysenbach et al., 2020).
Representation Learning in RL. In environments with rich observations, representation learning
is crucial for the efficiency of RL methods. Learning unsupervised auxiliary tasks (Jaderberg et al.,
2016) is shown to be effective for learning a good representation. The relationship between learning
7
Published as a conference paper at ICLR 2022
policy-dependent auxiliary tasks and learning good representations has been studied in some prior
works (Bellemare et al., 2019; Dabney et al., 2020; Lyle et al., 2021), while our focus is to learn
policy-independent auxiliary tasks to facilitate transfer learning. Using latent prediction models to
regularize representation has been shown to be effective for various types of rich observations (Guo
et al., 2020; Lee et al., 2019). Gelada et al. (2019) theoretically justify that learning latent dy-
namics model guarantees the quality of the learned representation, while we further characterize
the relationship between representation and learning performance, and we utilize dynamics models
to improve transfer learning. Zhang et al. (2020b) use a bisimulation metric to learn latent rep-
resentations that are invariant to task-irrelevant details in observation. As pointed out by Achille
& Soatto (2018), invariant and sufficient representation is indeed minimal sufficient, so it is an
interesting future direction to combine our method with bisimulation metric to learn minimal suf-
ficient representations. There is also a line of work using contrastive learning to train an encoder
for pixel observations (Srinivas et al., 2020; Yarats et al., 2021; Stooke et al., 2021), which usually
pre-train an encoder based on image samples using self-supervised learning. However, environment
dynamics are usually not considered during pre-training. Our algorithm can be combined with these
contrastive learning approaches to further improve learning performance in the target task.
6	Experimental Evaluation
We empirically evaluate our transfer learning algorithm in various environments and multiple
observation-change scenarios. Detailed experiment setup and hyperparameters are in Appendix E.
Baselines. To verify the effectiveness of our proposed transfer learning method, we compare our
transfer learning algorithm with 4 baselines: (1) Single: a single-task base learner. (2) Auxiliary:
learns auxiliary models from scratch to regularize representation. (3) Fine-tune: loads and freezes
the source policy head, and retrains an encoder in the target task. (4) Time-aligned (Gupta et al.,
2017): supposes the target task and the source task proceed to the same latent state given the same
action sequence, and pre-trains a target-task encoder with saved source-task trajectories. More de-
tails of baseline implementations are in Appendix E.1.1.
Scenarios. As motivated in Section 1, there are many scenarios where one can benefit from transfer
learning across observation feature spaces. We evaluate our proposed transfer algorithm in 7 envi-
ronments that fit various scenarios, to simulate real-world applications:
(1)	Vec-to-pixel: a novel and challenging scenario, where the source task has low-dimensional vector
observations and the target task has pixel observations. We use 3 vector-input environments Cart-
Pole, Acrobot and Cheetah-Run as source tasks, and use the rendered image in the target task.
(2)	More-sensor: another challenging scenario where the target task has a lot more sensors than the
source task. We use 3 MuJoCo environments: HalfCheetah, Hopper and Walker2d, whose origi-
nal observation dimensions are 17, 11 and 17, respectively. We add mass-based inertia and velocity
(provided by MuJoCo’s API), resulting in 145, 91, 145 dimensions in the corresponding target tasks.
(3) Broken-sensor: we use an existing game 3DBall contained in the Unity ML-Agents Toolkit (Ju-
liani et al., 2018), which has two different observation specifications that naturally fit our transfer
setting: the source observation has 8 features containing the velocity of the ball; the target obser-
vation does not have the ball’s velocity, thus the agent has to stack the past 9 frames to infer the
velocity. Please see Appendix E.1.2 for more detailed descriptions of all the 7 environments.
Base DRL Learners. What we propose is a transfer learning mechanism that can be combined with
any existing DRL methods. For environments with discrete action spaces (CartPole, Acrobot), we
use the DQN algorithm (Mnih et al., 2015), while for environments with continuous action spaces
(Cheetah-Run, HalfCheetah, Hopper, Walker2d, 3DBall), we use the SAC algorithm (Haarnoja et al.,
2018). To ensure a fair comparison, we use the same base DRL learner with the same hyperparam-
eter settings for all tested methods, as detailed in Appendix E.1.3. As is common in prior works,
our implementation of the RL algorithms is mostly a proof of concept, thus many advanced training
techniques are not included (e.g. Rainbow DQN).
Results. Experimental results on all tested environments are shown in Figure 3. We can see that
our proposed transfer method learns significantly better than the single-task learner, and also outper-
forms all baselines in the challenging target tasks. Our transfer method outperforms Auxiliary since
it transfers dynamics model from the source task instead of learning it from scratch, and outper-
forms Fine-tine since it regularizes the challenging encoder learning with a model-based regularizer.
8
Published as a conference paper at ICLR 2022
Source (oracle) — Transfer (ours) — Single — Auxiliary — Fine-tune - Time-aligned
nruteR
Vec-to-pixel: CartPole
150 -π-----1------1-----1-----1—
Vec-to-pixel: Acrobot
-100
UJU"X
600
Vec-to-pixel: Cheetah-Run
800
400
200
250k
Steps
500k
5,000
4,000
3,000
2,000
1,000
0	50	100	150	200	250	0k 250k
Episodes
More-sensor: HalfCheetah	More-sensor: Hopper
500k	750k	1000k 1250k	0k
Steps
4,000
3,000
2,000
1,000
3,000
2,000
1,000
More-sensor: Walker2d
Broken-sensor: 3DBall


0
0k 200k	400k	600k	800k	1000k	0k 200k	400k	600k	800k	1000k	0k 200k	400k	600k	800k	1000k	50k	100k 150k 200k 250k 300k 350k
steps	steps	steps	Steps	.105
Figure 3: Our proposed transfer method outperforms all baselines in target tasks over all tested scenarios. (The
dashed green lines are the learning curves in source tasks.) Results are averaged over 10 random seeds.
The Time-aligned method, although requires additional pre-training that is not shown in the figures,
does not work better than Single in most environments, because the time-based alignment assump-
tion may not hold as discussed in Appendix E.1.1. In some environments (e.g. Hopper, Walker2d,
3DBall), our transfer algorithm even achieves better asymptotic performance than the source-task
policy, which suggests that our method can be used for improving the policy with incremental obser-
vation design. To the best of our knowledge, we are the first to achieve effective knowledge transfer
from a vector-input environment to a pixel-input environment without any pre-defined mappings.
Ablation Study and Hyper-parameter Test. To verify the
effectiveness of proposed transfer method, we conduct abla-
tion study and compare our method with its two variants: only
transferring the transition model P and only transferring the
reward model R. Figure 4 shows the comparison in HalfChee-
tah, and Appendix E.2 demonstrates more results. We find that
all the variants of our method can make some improvements,
which suggests that transferring P and R are both effective
designs for accelerating the target task learning. Figure 6
in Appendix E.2 shows another ablation study where we in-
vestigate different selections of model regularizers and policy
heads. In Algorithm 2, a hyper-parameter λ is needed to con-
Figure 4: Ablation Study
trol the weight of the transferred model-based regularizer. Figure 7 in Appendix E.2 shows that, for
a wide range of λ's, the agent consistently outperforms the single-task learner.
Potential Limitations and Solutions. As Figure 3 shows, in some environments such as HalfChee-
tah, our transfer algorithm significantly outperforms baselines without transfer. But in Walker2d,
the improvement is less significant, although transferring is still better than not transferring. This
phenomenon is common in model-based learning (Nagabandi et al., 2018), as state predicting in
Walker2d is harder than that in HalfCheetah due to the complexity of the dynamics. Therefore, we
suggest using our method to transfer when the learned models (P , R) in the source task are rela-
tively good (error is low). More techniques of improving model-based learning, such as bisimula-
tion (Zhang et al., 2020b; Castro, 2020), can be applied to further improve the transfer performance.
7 Conclusion
In this paper, we identify and propose a solution to an important but rarely studied problem: trans-
ferring knowledge between tasks with drastically different observation spaces where inter-task map-
pings are not available. We propose to learn a latent dynamics model in the source task and transfer
the model to the target task to facilitate representation learning. Theoretical analysis and empirical
study justify the effectiveness of the proposed algorithm.
9
Published as a conference paper at ICLR 2022
Acknowledgements
This work is supported by Unity Technologies, National Science Foundation IIS-1850220 CRII
Award 030742-00001, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing
AI Robustness against Deception (GARD), and Adobe, Capital One and JP Morgan faculty fellow-
ships.
Ethics S tatement
Transfer learning aims to apply previously learned experience to new tasks to improve learning
efficiency, which is becoming more and more important nowadays for training intelligent agents in
complex systems. This paper focuses on a practical but rarely studied transfer learning scenario,
where the observation feature space of an RL environment is subject to drastic changes. Driven
by theoretical analysis on representation learning and its relation to latent dynamics learning, we
propose a novel algorithm that transfers knowledge between tasks with totally different observation
spaces, without any prior knowledge of an inter-task mapping.
This work can benefit many applications as suggested by the examples below.
(1)	In many real-life environments where deep RL is used (e.g. navigating in a building), the under-
lying dynamics (e.g. the structure of the building) are usually fixed, but what features the observation
space has is designed by human developers (e.g. what sensors are installed) and thus may change
frequently during the development. When the agent gets equipped with better sensors, our algorithm
makes it possible to reuse previously learned models when learning with the new sensors.
(2)	An agent usually learns better with a compact observation space (e.g. a low-dimensional vector
space containing its location and the goal’s location) than a rich/noisy observation space (e.g. an
image containing the goal). However, a compact observation is usually more difficult to construct in
practice as it may require expert knowledge and human work. In this case, one can extract compact
observations in a few samples and pre-train a policy with our Algorithm 1, then train the agent in the
real environment with rich observations with our Algorithm 2 using the learned dynamics models.
Our experiment in Figure 3 shows that the learning efficiency in the rich-observation environment
can be significantly improved with our proposed transfer method.
Reproducibility S tatement
For theoretical results, we provide concrete proofs in Appendix C and Appendix D. For empirical re-
sults, we illustrate implementation details in Appendix E. The source code and running instructions
are provided in the supplementary materials.
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas
Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal
representations for reinforcement learning. Advances in neural information processing systems,
32:4358-4369, 2019.
D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientific, Belmont, MA,
1996.
Steven Bohez, Tim Verbelen, Elias De Coninck, Bert Vankeirsbilck, Pieter Simoens, and Bart
Dhoedt. Sensor fusion for robot control through deep reinforcement learning. In 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 2365-2370, 2017. doi:
10.1109/IROS.2017.8206048.
Tim Brys, Anna Harutyunyan, Matthew E Taylor, and Ann Nowe. Policy transfer using reward
shaping. In AAMAS, pp. 181-188, 2015.
10
Published as a conference paper at ICLR 2022
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov
decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp.10069-10076, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020.
Will Dabney, Andre Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G. Bellemare, and
David Silver. The value-improvement path: Towards better representations for reinforcement
learning. CoRR, abs/2006.02243, 2020. URL https://arxiv.org/abs/2006.02243.
Benjamin Eysenbach, Swapnil Asawa, Shreyas Chaudhari, Sergey Levine, and Ruslan Salakhutdi-
nov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. arXiv
preprint arXiv:2006.13916, 2020.
Sumitra Ganesh, Nelson Vadori, Mengda Xu, Hua Zheng, Prashant Reddy, and Manuela Veloso.
Reinforcement learning for market making in a multi-agent dealer market. arXiv preprint
arXiv:1911.05892, 2019.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170-2179. PMLR, 2019.
Christopher Grimm, Andre Barreto, Satinder Singh, and David Silver. The value equivalence prin-
ciple for model-based reinforcement learning. arXiv preprint arXiv:2011.03506, 2020.
Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altche, Remi
Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multi-
task reinforcement learning. In International Conference on Machine Learning, pp. 3875-3886.
PMLR, 2020.
Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant
feature spaces to transfer skills with reinforcement learning. arXiv preprint arXiv:1703.02949,
2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot trans-
fer in reinforcement learning. In International Conference on Machine Learning, pp. 1480-1490.
PMLR, 2017.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural Networks, 2(5):359-366, 1989. ISSN 0893-6080. doi: https:
//doi.org/10.1016/0893- 6080(89)90020- 8. URL https://www.sciencedirect.com/
science/article/pii/0893608089900208.
Ronald A Howard. Dynamic programming and markov processes. 1960.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion,
Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, et al. Unity: A general platform for intelli-
gent agents. arXiv preprint arXiv:1809.02627, 2018.
11
Published as a conference paper at ICLR 2022
Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models.
arXiv preprint arXiv:1911.12247, 2019.
Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. CoRR, abs/1907.00953, 2019. URL
http://arxiv.org/abs/1907.00953.
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the effect of auxiliary tasks on
representation dynamics. In International Conference on Artificial Intelligence and Statistics, pp.
1-9. PMLR, 2021.
Timothy A Mann and Yoonsuck Choe. Directed exploration in reinforcement learning with trans-
ferred knowledge. In European Workshop on Reinforcement Learning, pp. 59-76. PMLR, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Remi Munos. Error bounds for approximate value iteration. In Proceedings ofthe National Confer-
ence on Artificial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2005.
Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dy-
namics for model-based deep reinforcement learning with model-free fine-tuning. In 2018
IEEE International Conference on Robotics and Automation (ICRA), pp. 7559-7566, 2018. doi:
10.1109/ICRA.2018.8463189.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Jonathan Raiman, Susan Zhang, and Christy Dennison. Neural network surgery with sets. arXiv
preprint arXiv:1912.06719, 2019.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning
from reinforcement learning. In International Conference on Machine Learning, pp. 9870-9879.
PMLR, 2021.
Yanchao Sun, Xiangyu Yin, and Furong Huang. Temple: Learning template of transitions for sample
efficient multi-task rl. arXiv preprint arXiv:2002.06659, 2020.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(7), 2009.
Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for tem-
poral difference learning. Journal of Machine Learning Research, 8(9), 2007.
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with pro-
totypical representations. arXiv preprint arXiv:2102.11271, 2021.
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin
Gal, and Doina Precup. Invariant causal prediction for block mdps. In International Conference
on Machine Learning, pp. 11214-11224. PMLR, 2020a.
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020b.
Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A
survey. arXiv preprint arXiv:2009.07888, 2020.
12
Published as a conference paper at ICLR 2022
Appendix: Transfer RL across Observation Feature Spaces via
Model-Based Regularization
A	Additional Preliminary Knowledge
For any policy π, its Q value Qπ is the unique fixed point of the Bellman operator
(T Q)(O, a) = Eo0〜P(o,a),a0〜π(o0) [R(o, a) + YQ(O , a )]	(5)
The optimal policy can be found by policy iteration (Howard, 1960), where one starts from an
initial policy π0 and repeats policy evaluation and policy improvement. More specifically, at it-
eration k, the algorithm evaluates Qπk via Equation (5), then improves the policy by πk+1 (O) :=
argmaxa∈AQπk (O, a), ∀O ∈ O. It is well-known that the policy iteration algorithm converges to the
optimal policy under mild conditions (Puterman, 2014). When the dynamics P and R are unknown,
reinforcement learning algorithms use interaction samples from the environment to approximately
solve Qnk. Prior works (Bertsekas & Tsitsiklis, 1996; Munos, 2005) have shown that if the approx-
imation error is bounded by a small constant, the performance of πk as k → ∞ is guaranteed to be
CloSe to the optimal policy value Q∏*, which We also denote as Q*.
B Additional Discussion for Representation S ufficiency
Good Representation for A Fixed Policy We slightly abuse notation and use Hφ to denote the
approximation operator for state value function V : O → R, similar to the approximation of Q as
introduced in Section 4.1. The following definition characterizes the sufficiency of a representation
mapping in terms of evaluating a fixed policy.
Definition 9 (Sufficient Representation for A Fixed Policy). A representation mapping φ is suf-
ficient for a policy π w.r.t. approximation operator Hφ iff HφVπ = V π. More generally, for a
constant ≥ 0, Φ is -sufficient for π iff kHφV π - Vπ k ≤ .
Remarks. (1) If O1 , O2 ∈ O have different values under π, a good representation should be able to
distinguish them, i.e., φ* (O1) 6= φ* (O2).
(2) The approximation operator Hφ is linear if HφV = Projφ(V) where Projφ(∙) denotes the
orthogonal projection to the subspace spanned by the basis functions of hφ1,φ2,…，φdi.
Model Sufficiency over Policies Induces Linear Learning Sufficiency It can be found from
Definition 3 that φ is sufficient as long as it represents Qπ for all π ∈ ΠφD . Fitting various value
functions to improve representation quality is proposed by some prior works Bellemare et al. (2019);
Dabney et al. (2020) and shown to be effective. However, learning and fitting many policy values
could be computationally expensive, and is not easy to be applied to transfer learning between tasks
with different observation spaces. Can we regularize the representation with the latent dynamics
instead of policy values? Proposition 10 below shows that if φ is linearly sufficient for all dynamics
pairs (Pπ, Rπ) induced by policies in ΠφD and the dynamics pairs associated with all actions, then φ
is linearly sufficient for learning.
For notation simplicity, assume the state space is finite. Then, let (Pπ, Rπ ) be the transition
matrix and reward vector induced by policy π, i.e., P∏[i,j] = Ea〜∏3)[P(θj∣θi, a)], Rn[i] =
Ea〜∏(θi)[R(θi, a)]. Similarly, let Pa[i,j] = P(oj∣0i, a), Ra[i] = R(θi, a). We let Φ denote the
representation matrix, where the i-th row of Φ refers to the feature of the i-the observation.
Proposition 10 (Linear Sufficiency Induced by Policy-based Model Sufficiency). For representation
Φ, if for all π ∈ ΠφD , a ∈ A, there exist Pn, Rn , Pa, Ra such that ΦPn = PnΦ, ΦRn = Rn,
ΦPa = PaΦ, ΦRa = Ra, i.e. Φ is linearly sufficient both policy-based dynamics and policy-
independent dynamics models, i.e., then Φ is linearly sufficient for a task M w.r.t. approximation
operator Hφ.
Proposition 10 proven in Appendix C.3 suggests that we can let representation fit (Pn, Rn) for many
different n's. However, it could be computationally intractable since the policy space is large. More
13
Published as a conference paper at ICLR 2022
importantly, it is not memory-friendly to store and transfer a large number of dynamics models
for all (Pπ , Rπ). In our Proposition 6, we show that learning sufficiency can be induced by policy-
independent model sufficiency, which is much simpler as there is no need to learn and store (Pπ, Rπ)
for many policies. As a trade-off, the policy-independent model induces non-linear sufficiency
instead of linear sufficiency, requiring a more expressive approximation operator.
Latent MDP induced by Representation We follow the analysis by Gelada et al. (2019) and
define a new MDP under the representation mapping φ: M = hO, A, P, R, γi, where for all o ∈ O,
φ(o) ∈ O, P (φ(o), a) = Paφ(o), R(φ(o), a) = Raφ(o). Let V denote the value function in M,
and let ∏ denote a policy m M. We make the following mild assumption
Assumption 11. There exists a constant Kφ,V such that
.~ . ~.......................... ..	...	~
∣V∏(φ(oι)) - Vn(Φ(θ2))∣ ≤ Kφ,Vkφ(oι) - Φ(θ2)k,∀∏ : O → A,o1,o2 ∈ O.
This assumption is mild as any MDP with bounded reward has bounded value functions.
C Technical Proofs
C.1 Proof of Lemma 4
Proof of Lemma 4. We first show that policy iteration with approximation operator Hφ starting from
a policy π0 ∈ ΠφD generates a sequence of policies that are in ΠφD . That is, for all πk , and any
o1, o2 ∈ O, πk(o1) = πk(o2) if φ(o1) = φ(o2). We prove this claim by induction.
Base case: when k = 0, π0 ∈ ΠφD .
Inductive step: assume πk ∈ ΠφD for k ≥ 0, then for iteration k + 1, we know that
πk+1 (o) := argmaxaQk(o,a)	(6)
where Qk = HφQπk .
Based on the definition of Hφ, Qk(o, a) = f (φ(o); θa) for some θa. Hence, if o1 and o2 have the
same representation, Qk(oι, ∙) and Qk(02, ∙) are equal. As a result, ∏k+1(o1) and ∏k+1(o2) are
equal, so πk+1 ∈ ΠφD .
Next we prove the error bound in Lemma 4. We start by restating the error bounds for approximate
policy iteration by Bertsekas & Tsitsiklis (1996):
2γ
limsuPk→∞kV - Vπk k∞ ≤ (1 」γ)2 SUpk kVk - Vπk k∞	⑺
where πk is the policy in the k-th iteration. Then we extend the above result to the action value Q.
For any πk during the policy iteration (as proven above, πk ∈ ΠφD), if φ is -sufficient for M as
defined in Definition 3 with '∞ norm, then We have ∣∣Qk - Q∏k k∞ ≤ e. That is, ∀o ∈ O, a ∈
A, |Qk(o, a) - Qπk (o, a)| ≤ . Therefore, ∀o ∈ O,
|Vk(o) - Vπk (o)| = | X ∏(a∣o)(Qk(o,a) — Q∏k (o,a))∣ ≤ e	(8)
a∈A
On the other hand, we can derive
∣∣Q* — Q∏kk∞ = max ∣Q*(o, a) - Q∏k (o, a)|	(9)
o,a
=max |R(o, a) + Y ^X P(o0∣o, a)V*(o0) — R(o, a) — Y ^X P(o0∣o, a)V∏k(o0)∣
=YkV* -Vπkk∞	(10) (11)
Combining Equation (8) and (11), we obtain	
*	2Y2 limsupk→∞ kQ - Q∏kk∞ ≤ (1 - γ)2 e.	(12)
□
14
Published as a conference paper at ICLR 2022
C.2 Proof of Proposition 6
Proof of Proposition 6. Given that P is deterministic for o ∈ O, a ∈ A, we slightly abuse notation
and let o0 = P(o, a) = Pa(o) if P(o0|o, a) = 1. If φ is sufficient for the dynamics models,
i.e. ∀o ∈ O, a ∈ A, Paφ(o) = φ(Pa (o)), Raφ(o) = Ra (o). Then, we can define a new MDP
M = (O, A, P, R, γi, where for all o ∈ O, φ(o) ∈ Q and P(φ(o), a) = Paφ(o) = φ(P(o, a)),
R(φ(o), a) = Raφ(o) = R(o, a).
Any policy ∏ ∈ ∏D, based on the definition of ∏D, can be written as ∏oφ, where ∏ isa deterministic
policy in M. Next, We show that for all o ∈ O,a ∈ A, Q∏ (o) = Qn (φ(o)).
By definition of the Q value, we know
∞
Qπ (o, a) = Eπ,P [	γ R(ot, at)|o0 = o, a0 = a]	(13)
t=0
∞
Qn (Φ(o)) = E∏,p [X γtRR(θt,at)∖oo = φ(o),αo = a]	(14)
t=0
We claim that in the above equations, ot = φ(ot), a = at, for all t ≥ 0. We prove the claim by
induction.
When t = 0, the claim holds as oo = φ(oo) = φ(o),而 =a0 = a.
Then, with inductive hypothesis that ot = φ(ot), a = at, We show the claim holds for t +1:
Action: at+ι = ∏(θt+ι) = ∏(φ(θt+ι))=应+i.
State: ot+1 = P(Ot, at) = P(φ(ot), at) = φ(P(ot, at)) = φ(ot+1).
Hence, we have shown Ot = φ(ot), At = at, for all t ≥ 0, then the reward in the t-th step of
Equation (13) and (14) are the same, as R(oAt, aAt) = R(φ(ot), at) = R(ot, at). Therefore, Qn (o) =
Qn (φ(o))∙
Therefore, for any ∏ ∈ ∏D, its action value can be represented by Qn ◦ φ. Therefore, φ is sufficient
for learning in M .
Next, we show that φ is not necessarily linearly sufficient for learning the task.
Consider an arbitrary policy π ∈ ΠφD. Without loss of generality, suppose Rn is linearly represented
by φ, i.e. Ra(o, a) = φ(o)>Ra, then we have
Rn(o) =	π(a∖φ(o))Ra(o, a)
a∈A
a∈A
=hπ(φ(O)), R >φ(o)i
.^ . ... ...
=hR π(φ(o)),φ(o)i
1	7^∖	Γ 7^∖	7^∖	7^∖ T -c-r τ	Γ∙ 1,1	,	1	1	, 1	, 1	, ∙	1'	11
where R := [R。]; Ra、; ∙∙∙ ; Ra∣A∣ ]. We can find that unless ∏ always takes the same action for all
input states, φ is not guaranteed to linearly encode Rn .
Similarly, for P∏, suppose Pa(∙∖o, a) = φ(o)>Pa we have
φ(Pn (O)) = X π(a∖φ(O))Pa(∙∖o,a)
a∈A
a∈A
ʌ....... , ,
P(π(φ(O)),φ(o),I)
15
Published as a conference paper at ICLR 2022
1	-f∖ Γ -f∖ τ∖	τ∖ T ∙	1 A 1	1	7 ,	1 -f∖∕	∖ 1	,	, 1	1 , ∙ 1 ∙
where P := [?i; P^; ∙∙∙ ; Pa∣A∣] is an |A| × d X d tensor, and P(∙, ∙, ∙) denotes the multi-linear
operation. Hence, if π takes different actions in different states, φ may not linearly encode the
transition, either.
Therefore, φ is not guaranteed to linearly encode Rπ and Pπ , and thus is not guaranteed to linearly
encode Vπ and Qπ .
□
C.3 Proof of Proposition 10
Proof of Proposition 10. We first show that for any π ∈ Π, if φ is linearly sufficient for (Pπ, Rπ),
then there exists a vector ω ∈ Rk such that Vn = Vn = Φω.
Since Φ is linearly sufficient for Pn and Rn , we have ΦPn = Pn Φ and ΦRn = Rn for some Pn
and R∏. Let ω = (I 一 γP∏)tRπ, then the Bellman error of Vn = Φω can be computed as
Rn + γPn Vn 一 Vn = Rn + γPn Φω 一 Φω
ɪ A ɪ A ɪ
= ΦRn + γΦPn ω 一 Φω
=Φ(Rn - (I 一 γP∏)(I 一 γPn)tR∏)
=φ(Rn - R )
=0
Therefore, Vn is a fixed point of the Bellman operator Tn, and thus equal to Vn.
Next, as We know that Qn(∙, a) = Ra + YhPa, Vni, and ΦPα = PaΦ, ΦRa = Ra, We can obtain
Qn (∙, a) = Ra + YhPa, Vni
ɪ A	Ck
= ΦRa + YPaΦω
ɪ ʌ	yd
=ΦRa + γΦPaω
_ , ^ ^
=Φ(Ra + γPaω)
Therefore, for any π ∈ ΠφD, Qn can be linearly represented by Φ, and thus Φ is linearly sufficient
for learning by definition.
□
C.4 Proof of Theorem 7
Proof of Theorem 7. Lemma 2 in Gelada et al. (2019) is based on one policy in the induced MDP
and bounded model errors. We can replace the Wasserstein distance W (φP (∙∣o, a), P (∙∣φ(o), a)) by
the Euclidean distance kφP (o, a), P (φ(o), a)k as we focus on deterministic transitions.
For any policy ∏ ∈ ∏D that can be written as ∏ ◦ φ, we have
IQn(o, a) — Qn(φ(o), a)| ≤ eR + γKφ,Vep	(15)
1 一 Y
Therefore, φ is (1 一 Y)-1(R + YKφ,VP)-sufficient for learning M.
Combined with Lemma 4, we can obtain the bound in Theorem 7.
□
C.5 Proof of Theorem 8
Proofof Theorem 8. First of all, if there exists φ(T) satisfying Pa(Φ3)) = Paar )[i]Φ(T), Ra(φ(θi))=
R(aT) [i], ∀oi ∈ O(T), then it is sufficient for the dynamics of the target task, and thus sufficient for
16
Published as a conference paper at ICLR 2022
learning the target task as stated in Proposition 6. Therefore, our focus is to show the existence of
such a representation.
As φ(S) is sufficient for Pa(S) and R(aS) for all a ∈ A, we have
Pa(φ(S) (。⑸))=P⑸(。⑸,a)Φ ⑸=。⑸(P(S)(。⑸,a))	(16)
Ra(Φ⑸(。⑸))=R(S)(。⑸,a)	(17)
where we let P (S)(o(S), a) denote the next state of (o(S), a), given that P is deterministic.
Based on Assumption 1, We know that there exists a function f such that ∀o(T) ∈ O(T), f (O(T)) ∈
O(S), and f (P(T)(。(T), a)) = P(S)(f (o(T)), a), R(T)(。(T), a) = R(S)(f (o(T)), a). Hence, we can obtain
Pa(φ⑻(f(O(T )))) = Φ"P"f(dτ )),a)) = Φ(S)(f (P(T )(。(T ),a)))	(18)
Ra(φ⑻(f(OT)))) = R⑸(f (。(T)), a) = R(T)(。(T), a)	(19)
Let φ(T) := φ(S) ◦ f, then we get
Pa(φ(T )(o(T))) = Φ(T )(P(T )(o(T ),a))	(20)
Ra(φ(T )(o(T))) = R(T )(o(T ),a)	(21)
Therefore, φ(T) is a feasible solution satisfying model sufficiency in the target task, and thus is
sufficient for learning.
Theorem 8 holds since we have shown (1) all feasible solutions to Φ(T)Pa = Pa(T)Φ and Φ(T) Ra =
R(aT) are sufficient for learning in M(T) , and (2) there exists at least one feasible solution to Φ(T)Pa =
.一、	.一、^	.一、
PaT)Φ and Φ(T)Ra = Ra).
□
D Representation Learning for Approximate Value Iteration
Now we illustrate how our transfer algorithm and the proposed model-based regularization work for
approximate value iteration. We focus on the case where the reward function R(。, a) ≥ 0 for all
。 ∈ O and a ∈ A.
Preliminaries The bases of value iteration is the Bellman optimality operator T*. For the value
function, we have
T * V (。)= max[R(。, a) + Y X P (。0|。, a)V (。0)]	(22)
a∈A
o∈O
For the Q function, we have
T*Q(。, a) = R(。, a) + γ	。(。0|。, a) max Q(。0, a0),	(23)
a0∈A
o∈O
where we slightly abuse notation and use T* for both the value function and the Q function when
there is no ambiguity.
Starting from some initial Q0 (or V0) and iteratively applying T*, i.e., Qk+1 = T*Qk, Qk (or
Vk) can finally converge to Q* or V* when k → ∞, which is known as value iteration. When an
approximation operator H is used, the process is called approximate value iteration (AVI): Qk+1 =
HφT* Qk. A prior work has shown the following asymptotic result:
Lemma 12 (Approximate Value Iteration for Q). For a reinforcement learning algorithm based on
value iteration with approximation operator H, if kHT*Qk - T*Qkk∞ ≤ , for all Qk along the
value iteration path, then we have
lim sup kV* - Vπk k ≤
k→∞
2e
(1-Y2.
(24)
Value Iteration Learning with Given Representation Given a representation mapping φ, we
aim to learn an approximation function h : φ(O) → R1A1 such that Q(。，∙)= h(0(。))≈ Q(。，∙).
For notation simplicity, we further use h(0(。)，a) to denote the approximated Q value Q(。, a) for
a ∈ A. We start from an initial h0 that has a uniform value c for all inputs, where c > 0 can
be randomly selected. The initial approximation for Q value is Qo = ho ◦ φ. Then, at iteration
17
Published as a conference paper at ICLR 2022
T-C	1 zA	Ct ZT-⅛ ZA	7	Il	7	∙ II 7 I ZT-⅛ zA	11	x-r τ
k > 0, we solve Qk = HφT Qk-ι = hk ◦ φ, where hk := argmmh∣∣h ◦ φ - T*Qk-ι∣∣∞. We use
a neural network (universal function approximator) to parameterize h, so the approximation error
∣∣h ◦ φ - T*Qk-ιk∞ depends on the representation quality of φ.
Therefore, a representation mapping φ is e-sufiCient for learning with value iteration if ∣HφT* Qk —
T *Qk∣∞ ≤ e. Next, we identify the relationship between policy-independent model sufficiency and
the learning sufficiency with value iteration methods.
Guaranteed Learning with Model-regularized Representation We first make the following as-
sumption for the learned approximation function h.
Assumption 13 (Lipschitz Value Approximation). There exists a constant Kφ,h, such that ∀k ≥
0, o1 , o2 ∈ O, a ∈ A,
|hk(φ(oι),a) — hk(φ(02),a)∣ ≤ Kφ,hkφ(01) - φ(02)k,	(25)
where hk is the approximation function in the k-th iteration.
Then, the following Theorem holds, which justifies that learning with model-regularized represen-
tation helps with value iteration learning.
nπι. . . . . _ . λ Λ L	λ æ τ ∖ τ ι n ∕Γ ∙ r	ι ι , ∙ 「	I T> / ∖ τ∖ / / / ∖ ∖ I	1
Theorem 14. ForanMDP M, ifencoder φ satisfies maxo∈o,ɑ∈A ∣R(o,a) — Rα(φ(o))∣ ≤ eR and
maXo∈o,a∈A ∣Eo0〜P(∙∣o,a)Φ(o0) - Pa(φ(o))∣∣2 ≤ ep for dynamics models (Pα,Rα)α∈A, then the
approximated value iteration with approximation operator Hφ under Assumption 13 satisfies
limsup∣V* -Vπk∣∞ ≤
k→∞
2
(1 - γ)2 (eR + YePKφ,h).
(26)
Proof of Theorem 14. Let hk be the approximation function in the k-th iteration. That is, the ap-
proximated Q function in the k-th iteration is Qk = hk ◦ φ. As the rewards of all state-action pairs
are non-negative, we have hk(φ(o), a) ≥ 0.
Define a function hk+1 as
hk+ι(φ(o),a) = Ra(φ(o)) + Y max hk(Pα(φ(o)),a )	(27)
α0∈A
Given that
T*Qk(o, a) = R(o, a) + YE。，〜p(∙∣o,α)[maxQk(o',a')]	(28)
=R(o, a) + γE°,〜p(∙∣o,α)∣max hk(φ(o0),a0)],	(29)
we have that for any o ∈ O, a ∈ A,
hk+ι(φ(o), a) -T Qk(o, a) I	(30)
=Rα(φ(o)) + Ymaxhk(Pα(Φ(o)),a0) - (R(o,a) + yE。，〜P(∙∣o,α)[mmax hk(φ(o0),a0)])	(31)
≤ ∣Rα(Φ(o)) - R(o, a) I + γ max hk(Pα(φ(o)), a0) - E。，〜P(.|。,。)[max hk(φ(o0),a0)]	(32)
≤eR + γEθ0〜P(∙∣o,α)[ max hk(Pα(φ(o)), a0) - max hk(φ(o0),a0) ]	(33)
α0∈A	α0∈A
≤eR + γEθ0〜P(∙∣o,α) max Ihk(Pa(φ(o)),a0) - hk(φ(o0),a0)1	(34)
α ∈A
≤eR + γEo0〜P(∙∣o,α) maxKφ,h ∣∣P3α(φ(o) - φ(o0)∣∣	(35)
≤eR + YePKφ,h,	(36)
where (34) is due to the non-negativity of hk, and (35) is due to Assumption 13.
Now we have shown that the constructed hk+1 satisfies
I∣hk+1 ◦ φ - T Qkk∞ ≤ eR + YePKφ,h.	(37)
According to the definition of Hφ, we obtain
∣hk+ι ◦ φ - T*Qkk∞ ≤ ∣hk+ι ◦ φ - T*Qkk∞ ≤ eR + YeP”	(38)
18
Published as a conference paper at ICLR 2022
since Hφ finds a hk+1 that minimizes the approximation error.
Therefore, Theorem 14 follows by combining Inequality (38) and Lemma 12.
□
E Experiment Details and Additional Results
E.1 Experiment Setting Details
E.1.1 Baselines
•	Single: A DQN or SAC learner on the target domain without any auxiliary tasks.
•	Auxiliary: On the target domain, the encoder φ(T) is optimized based on the loss
Lbase(Φ(T), ∏(T)) + λ [Lp(φ(T); P(T)) + Lr@t); R(T))]. Compared with our transfer algo-
rithms which transfer the learned dynamics from source domain to the target domain, it
learns the dynamics model (P (T), R(T)) on the target domain from scratch. Here we set λ to
be the same as our transferred algorithm (values ofλ are provided in Appendix E.1.3). The
purpose of this baseline is to test whether the efficiency of our proposed transfer algorithms
come from the transferred latent dynamics or from the auxiliary loss (or potentially both).
•	Fine-tune: To test whether our transfer algorithms benefit from loading the learned policy
head π(S) , on the target domain, we load the weights of π(T) from the trained source policy
head π(S) and train the DQN or SAC agent without any auxiliary loss.
Time-aligned: Gupta et al. (2017) propose to learn aligned representations for two tasks,
under the assumption that the source-task agent and the target-task agent reach similar
latent sates at the same time step, i.e. φ(T) (s(tT)) = φ(S) (s(tS) ). Note that this assumption
is valid when the initial state is fixed and the transitions are all deterministic. Although
in our setting, the agent can not learn both tasks simultaneously, we can adapt the idea of
time-based alignment and encourage the target encoder to map target observations to the
source representations happening at the same time step.
In our experiments, we store N source trajectories si0, ai0, si1, ai1, ...,	collected during
source task training. Then on the target domain, we first collect N trajectories following the
same action as the one collected from the source domain. In other words, at time step t of
the i-th trajectory, we take action ait. After the target trajectories are collected, we minimize
the alignment loss LaIign(Φ(τ)) = E [(φ(τ)(StT)) - Φ(S)(stS)))2] to enforce that observations
from source and target domain at the same time-step have the same representations.
In our experiments, we set N to be 10% of the training trajectories. (We also experimented
with largerN’, for example using all the training trajectories, but the differences are minor.)
In terms the alignment loss, we optimize the loss for 1000 epochs with batch size equal to
256, where at each epoch we sample a batch of paired source and target observations and
compute the alignment loss. After pre-training the target encoder, we load the weight into
φ(T) and resumes the normal DQN or SAC training.
Our experimental results show that, although more training steps are given to the time-
aligned learner, it does not outperform the single-task learner, and sometimes fails to learn
(e.g. in 3DBall). The main reason is that the time-based assumption does not hold in
practice as initial states are usually randomly generated. Therefore, even though the agent
exactly imitates the source-task policy at every step, the observations from source and target
task do not necessarily match at every time-step. In environments with non-deterministic
transitions, the state mismatch will be a more severe issue and may lead to an unreasonable
encoder.
E.1.2 Environments
Environment Settings in Vec-to-pixel Tasks
•	CartPole: The source task is the same as the ordinary CartPole environment on Gym.
For the pixel-input target task, we extract the screen of the environment which is of size
19
Published as a conference paper at ICLR 2022
(400,600), and crop the pixel input to let the image be centered at the cart. The resulting ob-
servation has size (40,90) after cropping. We take the difference between two consecutive
frames as the agent’s observation.
•	Acrobot: The source task is the same as the ordinary Acrobot environment on Gym.
For the pixel-input target task, we first extract the screen of the environment which is of
size (150,150), and then down-sample the image to (40,40). We also take the difference
between two consecutive frames as the agent’s observation.
•	Cheetah-Run: The source task is the Cheetah Run Task provided by DeepMind Control
Suite (DMC) (Tassa et al., 2018). For the target task, we use the image of size (84,84)
rendered from the environment as the agent’s observation.
Environment Settings in More-sensor Tasks For the target task of MuJoCo environments, we
add the center of the mass based inertia and velocity into the observations of the agent, concatenating
them with the original observation on the source task. Consequently, in the target environments,
the dimensionality of the observation space on target task become much larger than that of the
source task. On Hopper, the dimensionality of the target observation is 91, whereas the the source
observation space only has 11 dimensions. The dimensionalities of target tasks on HalfCheetah,
Hopper and Walker are 145, 91, 145 respectively.
Environment Settings in Broken-sensor Tasks 3DBall is an example environment provided by
the ML-Agents Toolkit (Juliani et al., 2018). In this task, the agent (a cube) is supposed to balance
a ball on its top. At every step, the agent will be rewarded if the ball is still on its top. If the
ball falls off, the episode will immediately end. The highest episodic return in this task is 100.
There are two versions of this game, which only differ by their observation spaces. The simpler
version (named 3DBall in the toolkit) has 8 observation features corresponding to the rotation of the
agent cube, and the position and velocity of the ball. The harder version (named 3DBallHard in the
toolkit) does not have access to the ball velocity, but observes a stack of 9 past frames, each of which
corresponds to the rotation of the agent cube, and the position of the ball, resulting in 45 observation
dimensions at every step. We regard 3DBall as the source task and 3DBallHard as the target task in
our experiments.
E.1.3 Implementation of Base DRL Algorithms and Hyper-parameter Settings
Implementation of DQN To ensure that the base learning algorithm learns the pixel-input target
tasks well, we follow the existing online codebases for pre-processing, architectures and hyperpa-
rameter settings in pixel CartPole1 and pixel Acrobot 2. On source domain, the DQN network has a
2-layer encoder and a 2-layer Q head of hidden size 64, and the representation dimension is set as
16. For pixel-input, the encoder has three convolution layers followed by a linear layer. The number
of channels of the convolutional layers are equal to 16, 32, 32, respectively (kernel size=5 for all
three layers). We use the Adam optimizer with learning rate 0.001 and β1, β2 = 0.9, 0.999. The
target Q network is updated every 10 iterations. In CartPole, we use a replay buffer with size 10000.
In the more challenging Acrobot, we use a prioritized replay buffer with size 100000.
Implementation of SAC For MuJoCo environments, we follow an elegant open-sourced SAC
implementation3. The number of hidden units for all neural networks is 256. The actor has a two-
layer encoder and a two-layer policy head. The two Q networks both have three linear layers. The
activation function is ReLU and the learning rate is 3 ∙ 10-4. We train the dynamics model and the
reward model every 50k interactive steps in the source task. For the DMC environment Cheetah-
Run, we follow the open-sourced SAC implementation with an autoencoder 4. The pixel encoder
has three convolution layers and one linear layer. The number of channels for all convolutional
layers is 32 and the kernel size is 3. For the 3DBall environment, as it can only be learned within the
ML-Agents toolkit, we directly use the SAC implementation provided by the toolkit with the default
hyperparameter settings.
IhttPs://pytorch.org/tutorials/intermediate/reinforcement_q」earning.html
2https://github.COmeyalbd2Deep_RL-COurse
3https://github.com/pranz24/pytorch-soft-actor-critic
4https://github.com/denisyarats/pytorch_sac_ae
20
Published as a conference paper at ICLR 2022
Implementation of Latent Dynamics Model Note that our goal is to learn a good representation
by enforcing it predicting the latent dynamics, different from model-based RL (Hafner et al., 2019)
that aims to learn accurate models for planning. Therefore, we let the dynamics models P and R
be simple linear networks, so that the representation can be more informative in terms of represent-
ing dynamics and learning values/policies. For environments with discrete action spaces, we learn
|A| linear transition networks and |A| linear reward models. For environments with continuous ac-
tion spaces, we first learn an action encoder ψ : A → Rd with the same encoding size d as the
state representation. Then, we learn a linear transition network and a linear reward network with
P (φ(o) ◦ ψ(a)) being the predicted next representation, and R(φ(o) ◦ ψ(a)) being the predicted
reward, where ◦ denotes element-wise product. In practice, we find this implementation achieves
good performance across many environments.
In addition, note that due to the significant difference between source observation and target obser-
vation, the initial encoding scale could be very different in source and target tasks, making it hard
for them to be regularized by the same dynamics model. Therefore, we normalize the output of both
encoders to be a unit vector (l2 norm is 1), which remedies the potential mismatch in their scales.
Hyperparameter Settings for Transfer Learning In experiments, we find that it is better to set
λ relatively large when the environment dynamics are simple and the dynamics model is of high
quality. When the environment dynamics is complex, we choose to be more conservative and set λ
to be smaller. Concretely, in CartPole, λ is set as 18; in 3DBall, λ is set as 10; in Acrobot, λ is set
as 5; in the remaining MuJoCo environments where dynamics are more complicated, λ is set as 1.
Although We use different λ's in different environments based on domain knowledge, We find that
different values of λ's do not have much influence on the learning performance. Figure 7 provided in
Appendix E.2 shoWs a test on the hyper-parameter λ, Where We can see that our algorithm effectively
transfers knoWledge under various values of λ.
Regarding the representation dimension, We set it to be smaller for simpler tasks, and larger for
more complex tasks. In 3DBall, We set the encoding size to be 8; in CartPole, We set the encoding
size as 16; in Acrobot, We set the encoding size as 32; in Cheetah-Run, We set the encoding size
as 50; in MuJoCo tasks, We set the encoding size as 256. Again, We find that the feature size does
not influence the performance too much. But based on the theoretical insights of learning minimal
sufficient representation Achille & Soatto (2018), We believe that it is generally better to have a
loWer-dimensional representation While making sure it is sufficient for learning.
21
Published as a conference paper at ICLR 2022
E.2 Additional Experimental Results
Ablation Study: Transferring Different Components
Figure 5 shows the ablation study of our method in continuous control tasks. We compare our
method with the following variants:
(1)	learning auxiliary tasks without transfer,
(2)	only transferring transition models P and
(3)	only transferring reward models R.
Compared with the single-task learning baseline (the blue curves), we find that all the variants of our
method can make some improvements, which suggests that learning dynamics models as auxiliary
tasks, transferring P and R are all effective designs for accelerating the target task learning. Finally,
our method (the red curves) that combines the above components achieves the best performance,
justifying the effectiveness of our transfer algorithm.
Steps
Hopper
Steps	Steps
Figure 5: Ablation study of our method on different transferred components.
22
Published as a conference paper at ICLR 2022
Sanity Check: Effectiveness of the Proposed
Transfer Method
We conduct another ablation study to evaluate
each component of our algorithm in the Cart-
Pole environment as shown in Figure 6. We
find that when transferring the dynamics mod-
els with only a linear value head (the green
curve), the agent fails to learn a good policy
as we analyzed in Section 4.3. If the dynam-
ics models (P , R) are randomly generated in-
stead of being transferred from the source task
(the orange curve), the agent does not learn, ei-
ther. More importantly, if we learn dynamics
models as auxiliary tasks in the target task with-
out transferring them from the source (the pur-
ple curve), the agent learns a little better than
a vanilla agent, but is worse than our proposed
transfer algorithm. These empirical results have
verified our theoretical insights and shown the effectiveness of our algorithm design.
150
100
50
0	50	100	150	200
Episodes
Figure 6: In the Vec-to-pixel CartPole environment,
sanity check verifies the effectiveness of our algorithm
design. Results are averaged over 20 random seeds.
Hyper-parameter Test
Figure 7 further visualizes how the hyperpa-
rameter λ (regularization weight) influences the
transfer performance in the Vec-to-pixel Cart-
Pole environment. It can be found that the
agent generally benefits from a larger λ, which
suggests that the model-based regularization
has a positive impact on the learning perfor-
mance. For a wide range of λ's, the agent al-
ways outperforms the learner without transfer
(the learner with λ = 0). Therefore, our algo-
rithm is not sensitive to the hyperparameter λ,
and a larger λ is preferred to get better perfor-
mance. In Appendix E.1.3, we have provided
the λ selections for all experiments.
nruteR egarevA
80
0	5	10	15	20	25
Regularization Weight λ
Figure 7: In the Vec-to-pixel CartPole environment,
under different selections of hyperparameter λ, the al-
gorithm works better than learning from scratch (when
λ = 0). Results are averaged over 20 random seeds.
23