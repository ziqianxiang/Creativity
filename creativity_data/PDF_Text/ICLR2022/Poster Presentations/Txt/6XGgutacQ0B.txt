Published as a conference paper at ICLR 2022
Demystifying Batch Normalization in ReLU
Networks: Equivalent Convex Optimization
Models and Implicit Regularization
Tolga Ergen； Arda Sahiner； Batu Ozturkler, John Pauly, Morteza Mardani & Mert Piland
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
{ergen,sahiner,ozt,pauly,morteza,pilanci}@stanford.edu
Ab stract
Batch Normalization (BN) is a commonly used technique to accelerate and stabilize
training of deep neural networks. Despite its empirical success, a full theoretical
understanding of BN is yet to be developed. In this work, we analyze BN through
the lens of convex optimization. We introduce an analytic framework based on
convex duality to obtain exact convex representations of weight-decay regularized
ReLU networks with BN, which can be trained in polynomial-time. Our analyses
also show that optimal layer weights can be obtained as simple closed-form for-
mulas in the high-dimensional and/or overparameterized regimes. Furthermore,
we find that Gradient Descent provides an algorithmic bias effect on the stan-
dard non-convex BN network, and we design an approach to explicitly encode
this implicit regularization into the convex objective. Experiments with CIFAR
image classification highlight the effectiveness of this explicit regularization for
mimicking and substantially improving the performance of standard BN networks.
1	Introduction
Deep neural networks have achieved dramatic progress in the past decade. This dramatic progress
largely hinged on improvements in terms of optimization techniques. One of the most prominent
recent optimization techniques is Batch Normalization (BN) (Ioffe & Szegedy, 2015). BN is an
operation that is introduced in between layers to normalize the output and it has been shown to be
extremely effective in stabilizing and accelerating training of deep neural networks. Hence, it became
standard in state-of-the-art architectures, e.g., ResNets (He et al., 2016). Despite its empirical success,
it is still theoretically elusive why BN is extremely effective for training deep neural networks.
Therefore, we investigate the mechanisms behind the success of BN through convex duality.
1.1	Related Work
Batch Normalization: One line of research has focused on alternatives to BN, such as Layer
Normalization (Ba et al., 2016), Instance Normalization (Ulyanov et al., 2016), Weight Normalization
(Salimans & Kingma, 2016), and Group Normalization (Wu & He, 2018). Although these techniques
achieved performance competitive with BN, they do not provide any theoretical insight about its
empirical success.
Another line of research studied the effects of BN on neural network training and identified several
benefits. For example, Im et al. (2016) showed that training deep networks with BN reduces
dependence on the parameter initialization. Wei et al. (2019) analyzed BN via mean-field theory to
quantify its impact on the geometry of the optimization landscape. They reported that BN flattens the
optimization landscape so that it enables the use of larger learning rates. In addition, Bjorck et al.
(2018); Santurkar et al. (2018); Arora et al. (2018) showed that networks trained with BN achieve
* Equal Contribution
1
Published as a conference paper at ICLR 2022
faster convergence and generalize better. Furthermore, Daneshmand et al. (2020) proved that BN
avoids rank collapse so that gradient-based algorithms, e.g., Stochastic Gradient Descent (SGD),
are able to effectively train deep networks. Even though these studies are important to understand
the benefits of BN, they fail to provide a theoretically complete characterization for training deep
networks with BN.
Convex Neural Networks: Recently, a series of papers (Pilanci & Ergen, 2020; Ergen & Pilanci,
2020; 2021a;b; Sahiner et al., 2021a;b) studied ReLU networks through the lens of convex opti-
mization theory. Particularly, Pilanci & Ergen (2020) introduced exact convex representations for
two-layer ReLU networks, which can be trained in polynomial-time via standard convex solvers.
However, this work is restricted to two-layer fully connected networks with scalar outputs. Later on,
Ergen & Pilanci (2021a) first extended this approach to two-layer scalar output Convolutional Neural
Networks (CNNs) with average and max pooling and provided further improvements on the training
complexity. These results were extended to two-layer fully convolutional networks and two-layer
networks with vector outputs (Sahiner et al., 2021a). However, these convex approaches are restricted
to two-layer ReLU networks without BN, thus, do not reflect the exact training framework in practice,
i.e., regularized deep ReLU networks with BN.
1.2	Our Contributions
•	We introduce an exact convex framework to explicitly characterize optimal solutions to ReLU
network training problems with weight-decay regularization and BN. Thus, we obtain closed-form
solutions for the optimal layer weights in the high-dimensional and overparameterized regime.
•	We prove that regularized ReLU network training problems with BN can be equivalently stated
as a finite-dimensional convex problem. As a corollary, we also show that the equivalent convex
problems involve whitened data matrices unlike the original non-convex training problem. Hence,
using convex optimization, we reveal an implicit whitening effect introduced by BN.
•	We demonstrate that GD applied to BN networks provides an implicit regularization effect by learn-
ing high singular value directions of the training data more aggressively, whereas this regularization
is absent for GD applied to the equivalent whitened data formulation. We propose techniques to
explicitly regularize BN networks to capture this implicit regularization effect.
•	Unlike previous studies, our derivations extend to deep ReLU networks with BN, CNNs, BN after
ReLU1, vector output networks, and arbitrary convex loss functions.
1.3	Preliminaries
Notation: We denote matrices and vectors as uppercase and lowercase bold letters, respectively,
where a subscript indicates a certain element or column. We use 0 (or 1) to denote a vector or matrix
of zeros (or ones), where the sizes are appropriately chosen depending on the context. We also use In
to denote the identity matrix of size n. To represent Euclidean, Frobenius, and nuclear norms, we
use k ∙ ∣∣2, k ∙ ∣∣f, and ∣∣ ∙ k*, respectively. Lastly, We denote the element-wise 0-1 valued indicator
function and ReLU activation as l[x ≥ 0] and (x)+ = max{x, 0}, respectively.
We2 consider an L-layer ReLU network with layer weights W(l) ∈ Rml-1 ×ml, where m0 = d and
mL = C are the input and output dimensions, respectively. Given a training data X ∈ Rn×d and a
label matrix Y ∈ Rn×C , we particularly focus on the following regularized training problem
min L(fθ,L(X), Y) + βR(θ),	(1)
where we compactly represent the parameters as θ := {W(l) , γl , αl}lL=1 and the corresponding
parameter space as Θ := {{W(l), γ(l), α(l)}lL=1 : W(l) ∈ Rml-1 ×ml , γl ∈ Rml , αl ∈ Rml , ∀l ∈
[L]}. We note that γl and αl are the parameters of the BN operator, for which we discuss the details
below. In addition, L(∙, ∙) is an arbitrary convex loss function, including squared, hinge, and cross
entropy loss, and R(∙) is the regularization function for the layer weights with the tuning parameter
β > 0. We also compactly define the network output as
fθ,L (X) := A(L-1)W(L), where A(l) := BNγ,α A(l-1)W(l)
1Presented in Appendix G.
2All the proofs and some extensions are presented in Appendix.
2
Published as a conference paper at ICLR 2022
We denote the lth layer activations as A(I) ∈ Rn×ml, and A(O) = X. Here, BNγ,α (∙) represents the
BN operation introduced in Ioffe & Szegedy (2015) and applies matrices column-wise.
Remark 1.1. Note that above we use BN before ReLU activations, which is common practice and
consistent with the way introduced in Ioffe & Szegedy (2015). However, BN can be placed after
ReLU as well, e.g., Chen et al. (2019), and thus in Section G of Appendix, we will also consider
architectures where BN layers are placed after ReLU.
For a layer With Weight matrix W(l) ∈ Rml-1 ×ml and arbitrary batch of activations denoted as
A(bl-1) ∈ Rs×ml-1, BN applies to each column j independently as folloWs
BNγ,α A(bl-1)wj(l)
(Is - s 11T )Abl-1)wjl)	(I) + αfl
k(Is- s 11T)AblT)Wjl)k2Yj	√n
(2)
Where γ(l) and α(l) are trainable parameters that scale and shift the normalized value. In this Work,
We focus on the full-batch case, i.e, A(bl-1) = A(l-1) ∈ Rn×ml . This corresponds to training the
netWork With GD as opposed to mini-batch SGD. We note that our empirical findings With GD
indicate identical if not better performance compared to the mini-batch case, Which is also consistent
With the previous studies Lian & Liu (2019); Summers & Dinneen (2020).
Throughout the paper, We consider a regression frameWork With squared loss and standard Weight-
decay regularization. Extensions to general convex loss functions are presented in Appendix B.1.
Moreover, beloW, We first focus on scalar outputs i.e. C = 1, and then extend it to vector outputs.
1.4	Overview of Our Results
Here, We provide an overvieW of our main results. To simplify the notation, We consider L-layer
ReLU netWorks With scalar outputs, i.e., mL = C = 1 thus the label vector is y ∈ Rn, and extend
the analysis to vector output netWorks With the label matrix Y ∈ Rn×C in the next sections.
The regularized training problems for an L-layer netWork With scalar output and BN is given by
PL=m∈in2 kfθ,L(X)- yk2+2 XX (卜 (l)∣∣2+IN)Il2+W IlF)，	⑶
Where We use γ(L) = α(L) = 0 as dummy variables for notational simplicity.
Lemma 1.1. The problem in (3) is equivalent to the following optimization problem
θmin2 kfθ,L(X) - yk2 + β Bw(L)B1 ,	(4)
θ∈Θs 2	1
where Θs := {θ ∈ Θ : γj(L-1) + α(jL-1) = 1, ∀j ∈ [mL-1]}.
Using the equivalence in Lemma 1.1, We noW take dual of (4) With respect to the output layer Weights
w(L) to obtain 3
PL ≥ dL ：=max - 1 ∣∣v - y∣∣2 + 1 ∣∣yk2 s.t. max v>(BN),a (A(L-2)W(LT)))	≤ β. (5)
v 2	2	θ∈Θs	+
Since the original formulation in (3) is a non-convex optimization problem, any solution v in the
dual domain yields a lower bound for the primal problem, i.e., PL ≥ dL. In this paper, we first
show that strong duality holds in this case, i.e., PL = dL, and then derive an exact equivalent convex
formulation for the non-convex problem (3). Furthermore, we even obtain closed-form solutions for
the layer weights in some cases so that there is no need to train a network in an end-to-end manner.
2 Two-layer Networks
In this section, we analyze two-layer networks. Particularly, we first consider a high-dimensional
regime, where n ≤ d, and then extend the analysis to arbitrary data matrices by deriving an equivalent
convex program. We also extend the derivations to vector output networks.
3Details regarding the derivation of the dual problem are presented in Appendix B.3.
3
Published as a conference paper at ICLR 2022
We now consider the regularized training problem for a two-layer network with scalar output and BN.
Using the equivalence in Lemma 1.1, the problem can be stated as
P2 = min 1 kfθ,2(X)- yk2+β ∣∣w ⑵ I].	⑹
θ∈Θs 2	1
We then take dual of (6) with respect to the output weights w(2) to obtain the following dual problem
p2 ≥ d2 =maχ- 1 Ilv -yk2 + 1 I∣yk2 s∙t∙ max vT (BNγ,α (XW⑴))≤ B, ⑺
v 2	2	θ∈Θs	+
where Θs = {θ ∈ Θ : W(1) ∈ Rd, γ(1)2 + α(1)2 = 1}. Notice that here we drop the hidden neuron
index j ∈ [m1] since the dual constraint scans all possible parameters in the continuous set Θs.
2.1	HIGH-DIMENSIONAL REGIME (n ≤ d)
In the sequel, we prove that the aforementioned dual problem is further simplified such that one can
find an optimal solution (6) in closed-form as follows.
Theorem 2.1. Suppose n ≤ d and X is full row-rank, then an optimal solution to (6) is
(wj1)*,w(2)*)=卜((τ)jy)+，(τ)j (k ((τ)jy)+k2-β) J
γj1)*# -	1	"k ((T)jy)+ - n 11T ((T)jy)+ 1目
αj1)[ = k ((Ry)+ k2 [	√ 1T ((-ι)jy)+
for j = 1, 2. Therefore, strong duality holds, p22 = d22.
2.2	Exact Convex Formulation
To obtain an equivalent convex formulation to (6), we first introduce a notion of hyperplane arrange-
ments; see also Pilanci & Ergen (2020). We first define a diagonal matrix as D := diag(l[Xw ≥ 0])
for an arbitrary W ∈ Rd . Therefore, the output of a ReLU activation can be equivalently written as
(XW)+ = DXW provided that DXW ≥ 0 and (In - D) XW ≤ 0 are satisfied. We can define these
two constraints more compactly as (2D - In) XW ≥ 0. We now denote the cardinality of the set
of all possible diagonal matrices as P, and obtain the following upperbound P ≤ 2r(e(n - 1)/r)r,
where r := rank(X) ≤ min(n, d). For the rest of the paper, we enumerate all possible diagonal
matrices (or hyperplane arrangements) in an arbitrary order and denote them as {Di}iP=14
Based on the notion of hyperplane arrangement, we now introduce an equivalent convex formulation
for (6) without an assumption on the data matrix as in the previous section.
Theorem 2.2. For any data matrix X, the non-convex training problem in (6) can be equivalently
stated as the following finite-dimensional convex program
1
min 一
si ,s0i ∈Rr+1 2
P
X DiU0(si - s0i) - y
i=1
2P
+ β X(Isi I2 + Is0i I2 ) s.t.
2 i=1
(2Di - In)U0si ≥ 0
∀i
(2Di - In)U0s0i ≥0,	,
(8)
where U ∈ Rn×r and U0 ∈ Rn×r+1 are computed via the compact SVD of the zero-mean data
matrix, namely (In — n 11T )X := UΣVT and U0:= [U √ l].
Notice that (8) is a convex program with 2Pr variables and 2Pn constraints, and thus can be globally
optimized via interior-point solvers with O(r6 (nr) r) complexity.
Remark 2.1. Theorem 2.2 shows that a classical ReLU network with BN can be equivalently
described as a convex combination of linear models {U0 si}iP=1 and {U0 s0i}iP=1 multiplied with
fixed hyperplane arrangement matrices {Di}iP=1. Therefore, this result shows that optimal ReLU
networks with BN are sparse piecewise linear functions, where sparsity is enforced via the group
lasso regularization in (8). The convex program also reveals an implicit mechanism behind BN.
Particularly, comparing (8) with the one without BN (see eqn. (8) in Pilanci & Ergen (2020)), we
observe that the data matrix for the convex program is whitened, i.e., U0T U0 = Ir+1.
4We provide more details in Appendix B.4.
4
Published as a conference paper at ICLR 2022
2.3 Vector Output Networks
Here, we analyze networks with vector outputs, i.e., Y ∈ Rn×C, trained via the following problem
pv2 ：= mn21Mix) - YkF+2X (M)∣∣2+γj1)2+ɑj1)2 *+M)心.⑼
∈	22
Lemma 2.1. The problem in (9) is equivalent to the following optimization problem
1	m1
Pbv = min 2 1加，2(耳 — YkF + β X IlWjiI2，	(⑼
∈ s	j=1
where Θs := {θ ∈ Θ : γj(1) + αj(1) = , ∀j ∈ [m1]}.
Using the equivalence in Lemma 2.1, we then take dual of (10) with respect to the output layer
weights W(2) to obtain the following dual problem
P2v ≥ d2v ：=max — | kV - YkF + | IMlF st mm矍 VT 邰)，。(XW(I)))十 尸 β∙(“)
Theorem 2.3.	The non-convex training problem (9) can be cast as the following convex program
*	. 1
Pv2 = min W
Si 2
P
XDiU0Si—Y
i=1
2P
+ β X kSikci ,*
F i=1
(12)
for the norm ∣∣ ∙ ||金,* defined as
kSkci,* := mint s.t. S ∈ tconv{Z = hgτ : (2Di — In)U0h ≥ 0, ∣∣Z∣∣* ≤ 1},
where conv{∙} denotes the convex hull of its argument and U0 = [U √1n l] as in Theorem 2.2.
Remark 2.2. We note that similar to the scalar output case, vector output BN networks training
problem involves a whitened data matrix. Furthermore, unlike (8), the nuclear norm regularization
in (12) encourages a low rank piecewise linear function at the network output.
The above problem can be solved in O((n/d)d) time in the worst case (Sahiner et al., 2021a), which
is polynomial for fixed d. We note that the problem in (12) is similar to convex semi non-negative
matrix factorizations (Ding et al., 2008). Moreover, in certain practically relevant cases, (12) can be
significantly simplified so that one can even obtain closed-form solutions as demonstrated below.
Theorem 2.4.	Let Y be a one-hot encoded label matrix, and X be a full row-rank data matrix with
n ≤ d, then an optimal solution to (10) admits
(WjI)*, wj2)*) = (Xtyj,(M 112 — β)+ ej)
γj(1)
α(j1)
*一
*
1
l∣yj-112
IM — n 11T y II2^
√nIT yj	J
∀j ∈ [C], where ej is the jth ordinary basis vector.
3 Convolutional Neural Networks
In this section, we extend our analysis to CNNs. Thus, instead of X, we operate on the patch matrices
that are subsets of columns extracted from X and we denote them as Xk ∈ Rn×h, where h is the
filter size and k is the patch index. With this notation, a convolution operation between the data
matrix X and an arbitrary filter z ∈ Rh can be represented as {Xkz}kK=1. Therefore, given patch
matrices {Xk}kK=1, regularized training problem with BN can be formulated as follows
1
min
θ∈Θc 2
m1 K
XX(BNC,α (Xk Zj ))+wj - y
j=1 k=1
β m1
+ 2 X(kzj 112 + Y + α2 + w2),
2 j=1
(13)
5
Published as a conference paper at ICLR 2022
where given μj∙ = nK PK=I 1TXkZj, BN is defined as
BNγC,α(Xkzj) :
XkZj - μj 1
JPK=IkXk0 zj-μj1k
IYjI)+√nK.
2
Using Lemma 1.1, the dual problem with respect to w(2) is
p2c ≥ d2c :=max- 2 kv-yk2 + 2 kyk2 s.t. mθs
K
VT X(BNC,α (XkZ)) +
≤ β. (14)
Next, we state the equivalent convex program for CNNs.
Theorem 3.1. The non-convex training problem in (13) can be cast as a convex program
qi,
Pc K	Pc
ɪ: ∈Rh+1 2 XX Dik UMk (qi - qi) + β X(Hqik2 + llqil∣2)
qi ∈R
min
i=1 k=1
i=1
(2Dik - In)U0Mkqi ≥ 0
s.t.	, ∀i, k
(2Dik - In)U0Mkq0i ≥ 0
(15)
where we first define a new matrix M = [X1; X2; . . . XK] ∈ RnK×h and then use the following no-
tations: the CompactSVDofthezeromeanformofthismatrixis (InK — 袅 11T)M := UM∑mVM,
αnd UM := [UM √⅛K] = [UMι; UM2; …UMK]∙
Theorem 3.1 proves that (13) can be equivalently stated as a finite-dimensional convex program with
2rcPc variables (see Remark 3.1 for the definitions) and 2nPc K constraints. Since Pc is polynomial
innand d as detailed in the remark below, we can solve (15) in polynomial-time.
Remark 3.1. The number of arrangements for CNNs rely on the rank of the patch matrices instead
of X. Particularly, the number of arrangements for M, i.e., denoted as Pc, is upperbounded as
Pc ≤ 2rc(e(n—	1)/rc)rc,	where	rc	:= rank(M)	≤ h	d.	Therefore, given a fixed filter size	h,	Pc
is polynomial innand d even when X is full rank (see Ergen & Pilanci (2021a) for more details).
4 Deep Networks
We now analyze L-layer neural networks trained via the non-convex optimization problem in (3).
Below, we provide an explicit formulation for the last two layers’ weights.
Theorem 4.1.	Suppose the network is overparameterized such that the range of A(L-2) is Rn . Then
an optimal solution for the last two layer weights in (4) admits
(WjLT)*,wjL)*) = kL-2)t ((-1)jy)+ , (-1)j (k ((-1)jy)+ k2 - β)J
-Yjj)*] =	1	「k ((-I)jy)+ - 111T ((-I)jy)+ ∣g
αjL-1)*	k ((-I)jy)+ k2	√⅛ 1T ((-I)jy) +
for j = 1, 2. Thus, strong duality holds, i.e., PL = d，L.
4.1	Vector Output Networks
Vector output deep networks are commonly used for multi-class classification problems, where
one-hot encoding is a prevalent strategy to convert categorical variables into a binary representation
that can be processed by neural networks. Although empirical evidence, e.g., Papyan et al. (2020),
shows that deep neural networks trained with one-hot encoded labels exhibit some common patterns
among classes, it is still theoretically elusive how these patterns emerge. Therefore, in this section, we
analyze vector output deep networks trained using one-hot labels via our advocated convex duality.
The following theorem presents a complete characterization for the last two layers’ weights.
Theorem 4.2.	Let Y be a one-hot encoded label matrix and the network be overparameterized such
that the range of A(L-2) is Rn, then an optimal solution to (3) can be found in closed-form as
K	「(LT)*]	1
(WjLT)*, WjL)*) = (A(L-2)tyj, (kyjk2 - β)+ ej) , YjL-1)* = k^ɪ
∀j ∈ [C], where ej is the jth ordinary basis vector.
-ι ⅛1Tyjk2 一
L	√n 1 yj J
6
Published as a conference paper at ICLR 2022
(a) BN model (9)
(b) Whitened model (16)
Figure 1: Cosine similarity of network weights relative to initialization for each singular (in de-
scending order) value direction of the data. Here, we use the same setting in Figure 2. We compare
training the BN model (9) to the equivalent whitened model (16), noting that the BN model fits high
singular value directions of the data first, while the whitened model fits all directions equally, thereby
overfitting to non-robust features. This shows implicit regularization effect of SGD.
5	Implicit Regularization of GD for BN
We first note that proposed convex formulations, e.g., (8), (12) and (15), utilize whitened data. Thus,
when we train them via a standard optimizers such as GD/SGD, the optimizer might follow an
unfavorable optimization path that leads to slower convergence and worse generalization throughout
the training. Based on this, we first analyze the impact of utilizing a whitened data matrix for the non-
convex problem in (9) trained with GD. Then, we characterize this impact as an implicit regularization
due to the choice of optimizer. Finally, we propose an approach to explicitly incorporate this benign
implicit regularization into both convex (8) and non-convex (9) training objectives.
5.1	Evidence of Implicit Regularization
Lemma 5.1. The non-convexproblem (9) is equivalent to thefollowing problem (i.e., p^c2 = Pr2卜)
2
m1	m1
，.一 min 1 X U Uqj ∏∕(1)	ψ 1 Q(I)) w(2)T Y	∣ β X	(十⑴2 ∣ ɑ⑴2	∣ Ilw(2)∣∣2A
pv2b∙=m∈in2 Z(≡2Yj	+	√nαj J+w'	-	+2MIYj + %	+ W ∣U.
F	(16)
(16) is equivalent to a problem with new data matrix U and weight normalization (Salimans & Kingma,
2016). The identity in Lemma 5.1 is achieved by substituting the transformation qj = ΣV>wj(1).
While (9) and (16) have identical global optima, in practice, GD behaves quite differently when
applied to the two problems. Furthermore, these differences extend to the convex formulations in
(8), (12), and (15). In particular, while these programs are adept at finding the global optimum, they
generalize quite poorly when trained with local search algorithms such as GD. This suggests an
implicit regularization effect of GD, the form of which is demonstrated in the following theorem.
Theorem 5.1. Consider (9) and (16) with corresponding models fv2 (X) and fv2b(U). At iteration
k of GD, assume that fvk2 (X) = fvk2b(U), i.e. both models have identical weights, besides q(jk) =
>	(1) (k)
ΣV>wj	. Then, the subsequent GD updates to model weights are identical, besides those to
wj(1) and qj. In qj -space, the updates ∆(vk2),j and ∆(vk2)b,j, respectively, admit
∆(vk2),j = Σ2∆(vk2)b,j .
Remark 5.1. Theorem 5.1 indicates that when solving (16), GD will fit directions corresponding to
higher singular values of data much slower relative to other directions compared to (9). By fitting
these directions more aggressively, GD applied to (9) provides an implicit regularization effect that
fits to more robust features. We also note that although our analysis holds only for GD, we empirically
observe and validate the emergence of this phenomenon for networks trained with SGD as well.
7
Published as a conference paper at ICLR 2022
To verify this intuition, in Figure 2, we compare the training and test loss curves of solving both (9)
and (16) with mini-batch SGD applied to CIFAR-10 (Krizhevsky et al., 2014), demonstrating that
SGD applied to (9) generalizes much better than (16). In Figure 1, we plot the cosine similarity of
the weights of both networks compared to their initialization in each singular value direction.
Our results confirm that the BN model fits high singular value directions much faster than low ones,
and in fact the lower-singular value directions almost do not move at all from their initialization. In
contrast, the whitened model fits all singular value directions roughly equally, meaning that it overfits
to less robust components of the data. These experiments confirm our theoretical intuition that SGD
applied to BN networks has an implicit regularization effect.
5.2	Making Implicit Regularization Explicit
Since the convex programs (8), (12), and (15) rely on the
whitened data matrix U, they lack the regularization nec-
essary for generalization when optimized with GD. Thus,
we aim to impose an explicit regularizer which does not
hamper the performance of standard BN networks, but
can improve the generalization of the equivalent programs
trained using first-order optimizers such as GD. One sim-
ple idea to prevent the whitened model from fitting low
singular value directions is to obtain a low-rank approxi-
mation of the data by filtering its singular values:
g(X) := UT (Σ)V>,	(17)
with Tk (Σ)ii := σil{i ≥ k}, which takes the k ≤ r top
singular values and removes the rest. We can then solve
(1) using fθ,L(g(X)) as the network output, to form what
we denote as the truncated problem. This can also reduce
the problem dimension in the whitened-data case since one
can simply omit the columns of U. We find this low-rank
approximation to be effective, which can even improve the
generalization of GD applied to BN networks.
6	Numerical Experiments
Figure 2: CIFAR-10 classification accu-
racies for a two-layer ReLU network in
(9) and the equivalent whitened model
(16) with SGD and (n, d, m1, β, bs) =
(50000, 3072, 1000, 10-4, 1000), where
bs denotes batch size. The whitened
model overfits much more than the stan-
dard BN model, indicating implicit regu-
larization.
Here5, we present experiments to verify our theory. Throughout this section, we denote the baseline
model (3) and our non-convex model with truncation as “SGD” and “SGD-Truncated”, respectively.
Likewise, the proposed convex models are denoted as “Convex” and “Convex-Truncated”.
(a) CIFAR-10-(m1, β, bs) = (4096, 10-4, 103)	(b) CIFAR-100-(m1, β, bs) = (1024, 10-4, 103)
Figure 3: Test accuracy of two-layer BN ReLU networks including the standard non-convex (baseline),
its convex equivalent (8), along with their truncated versions with k = (215, 200) respectively. All
problems are solved with SGD. The learning rates are chosen based on test performance and we
provide learning rate and optimizer comparisons in Appendix A.
5We present additional numerical experiments and details on the experiments in Appendix A.
8
Published as a conference paper at ICLR 2022
Closed-Form Solutions: We first verify Theorem 2.4.
We consider a three-class classification task with the
CIFAR-100 image classification dataset (Krizhevsky et al.,
2014) trained with squared loss and one-hot encoded la-
bels, while we set (n, d, m1, β) = (1500, 3072, 1000, 1),
where bs denotes the batch size. In Figure 4, we compare
the objective values achieved by the closed-form solution
and the classical network (6). We observe that our for-
mula is computationally light-weight and can quickly find
a global optimum that cannot be achieved with GD even
after convergence.
Figure 4: Comparing the objective val-
ues of the closed-form solution in The-
orem 2.4, (“Theory”), and GD (5 trials)
applied to the non-convex problem (9)
for three-class CIFAR classification with
(n,d,m1,β) = (1500, 3072, 1000, 1).
Here, “x" denotes the computation time
for the closed-form solution.
Convex Formulation and Implicit Regularization: We
now demonstrate the effectiveness of our convex for-
mulation to solve (9) on CIFAR-10 and CIFAR-100
(Krizhevsky et al., 2014). Here, instead of enumer-
ating all hyperplane arrangements, we randomly sam-
ple via randomly generated Gaussian vectors such
that m1 = P . We compare the results of training
with and without the explicit regularizer (17) for both
cases. We particularly choose k such that singu-
lar values explain 95% of the variance in the train-
ing data. Then, we consider two-layer networks for image classification on CIFAR-
10 with (n, d, m1, β, bs, k, C) = (5x104, 3072, 4096, 10-4, 103, 215, 10) and CIFAR-100 with
(n, d, m1, β, bs, k, C) = (5x104, 3072, 1024, 10-4, 103, 200, 100). In Figure 3, we observe that Con-
vex generalizes poorly (as expected with our results in Section 5.1). After the singular value truncation,
SGD, SGD-Truncated, and Convex-Truncated generalize equally well for CIFAR-10, and Convex-
Truncated in fact generalizes better for CIFAR-100. This shows that the optimizer benefits from the
convex landscape of the proposed formulation (8). Furthermore, truncation actually improves the
performance of SGD in the case of CIFAR-100, suggesting this is a good regularizer to use.
7	Concluding Remarks
In this paper, we studied training of deep weight-decay regularized ReLU networks with BN, and
introduced a convex analytic framework to characterize an optimal solution to the training problem.
Using this framework, we proved that the non-convex training problem can be equivalently cast
as a convex optimization problem that can be trained in polynomial-time. Moreover, the convex
equivalents involve whitened data matrix in contrast to the original non-convex problem using the
raw data, which reveals a whitening effect induced by BN. We also extended this approach to
analyze different architectures, e.g., CNNs and networks with BN placed after ReLU. In the high
dimensional and/or overparameterized regime, our characterization further simplifies the equivalent
convex representations such that closed-form solutions for optimal parameters can be obtained.
Therefore, in these regimes, there is no need to train a network in an end-to-end manner. In the
light of our results, we also unveiled an implicit regularization effect of GD on non-convex ReLU
networks with BN, which prioritizes learning high singular-value directions of the training data. We
also proposed a technique to incorporate this regularization to the training problem explicitly, which
is numerically verified to be effective for image classification problems. Finally, we remark that
after this work, similar convex duality arguments were applied to deeper networks Ergen & Pilanci
(2021c;d;e); Wang et al. (2021) and Wasserstein Generative Adversarial Networks (WGANs) Sahiner
et al. (2022). Extensions of our convex BN analysis to these architectures are left for future work.
Acknowledgments
This work was partially supported by the National Science Foundation under grants ECCS- 2037304,
DMS-2134248, the Army Research Office.
9
Published as a conference paper at ICLR 2022
References
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for
convex optimization problems. Journal ofControl and Decision, 5(1):42-60, 2018.
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-
ization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31, pp. 7694-7705. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
36072923bfc3cf47745d704feb489480-Paper.pdf.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Guangyong Chen, Pengfei Chen, Yujun Shi, Chang-Yu Hsieh, Benben Liao, and Shengyu Zhang.
Rethinking the usage of batch normalization and dropout in the training of deep neural networks.
arXiv preprint arXiv:1905.05928, 2019.
Patrick Marques Ciarelli and Elias Oliveira. Agglomeration and elimination of terms for dimen-
sionality reduction. In 2009 Ninth International Conference on Intelligent Systems Design and
Applications, pp. 547-552. IEEE, 2009.
Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326-334, 1965.
Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch
normalization provably avoids rank collapse for randomly initialised deep networks, 2020.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations.
IEEE transactions on pattern analysis and machine intelligence, 32(1):45-55, 2008.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the
Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of
Proceedings of Machine Learning Research, pp. 4024-4033, Online, 26-28 Aug 2020. PMLR.
URL http://proceedings.mlr.press/v108/ergen20a.html.
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two- and three-layer networks in polynomial time. In International Conference on Learning
Representations, 2021a. URL https://openreview.net/forum?id=0N8jUH4JMv6.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
Journal of Machine Learning Research, 22(212):1-63, 2021b. URL http://jmlr.org/
papers/v22/20-1447.html.
Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks via
convex programs. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
2993-3003. PMLR, 18-24 Jul 2021c. URL http://proceedings.mlr.press/v139/
ergen21a.html.
Tolga Ergen and Mert Pilanci. Path regularization: A convexity and sparsity inducing regularization
for parallel relu networks. arXiv preprint arXiv:2110.09548, 2021d.
10
Published as a conference paper at ICLR 2022
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3004-3014. PMLR,
18-24 Jul 2021e. URL https://Proceedings .mlr.press∕v139∕ergen21b.html.
Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,
version 2.1. http://cvxr.com/cvx, March 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Daniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of the optimization of
deep network loss surfaces. arXiv preprint arXiv:1612.04010, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 and -100 datasets. http:
//www.cs.toronto.edu/kriz/cifar.html, 2014.
Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via
composition optimization. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 3254-3263. PMLR, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Piyush C Ojha. Enumeration of linear threshold functions from the lattice of hyperplane intersections.
IEEE Transactions on Neural Networks, 11(4):839-850, 2000.
Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal
phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652-24663, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In Hal DaUme III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 7695-7705. PMLR, 13-18 Jul 2020. URL
http://proceedings.mlr.press/v119/pilanci20a.html.
Arda Sahiner, Tolga Ergen, John M. Pauly, and Mert Pilanci. Vector-output relu neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms. In International Conference on Learning Representations, 2021a. URL https:
//openreview.net/forum?id=fGF8qAqpXXG.
11
Published as a conference paper at ICLR 2022
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John M. Pauly. Convex regular-
ization behind neural reconstruction. In International Conference on Learning Representations,
2021b. URL https://openreview.net/forum?id=VErQxgyrbfn.
Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M. Pauly, Morteza Mardani, and
Mert Pilanci. Hidden convexity of wasserstein GANs: Interpretable generative models with
closed-form solutions. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=e2Lle5cij9D.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In NIPS, 2016.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 2483-
2493. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/
2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? CoRR, abs/1902.05040, 2019. URL http://arxiv.org/
abs/1902.05040.
Maurice Sion. On general minimax theorems. Pacific J. Math., 8(1):171-176, 1958. URL https:
//projecteuclid.org:443/euclid.pjm/1103040253.
Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13:
389-496, 2004.
Cecilia Summers and Michael J. Dinneen. Four things everyone should know to improve batch
normalization. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=HJx8HANFDH.
RH Tutuncu, KC Toh, and MJ Todd. Sdpt3-a matlab software package for Semidefinite-quadratic-
linear programming, version 3.0. Web page http://www. math. nus. edu. sg/mattohkc/sdpt3. html,
2001.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Yifei Wang, Tolga Ergen, and Mert Pilanci. Parallel deep neural networks have zero duality gap.
CoRR, abs/2110.06482, 2021. URL https://arxiv.org/abs/2110.06482.
Mingwei Wei, James Stokes, and David J Schwab. Mean-field analysis of batch normalization, 2019.
RO Winder. Partitions of n-space by hyperplanes. SIAM Journal on Applied Mathematics, 14(4):
811-818, 1966.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3-19, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115,
2021.
12
Published as a conference paper at ICLR 2022
Appendix
Table of Contents
A Additional Details and Numerical Experiments	13
A.1	Inference in BN models ............................................ 14
A.2	Additional Experiment:	Batch Sizes for SGD ........................ 14
A.3	Additional Experiment:	Singular Value Truncation Ablation ......... 15
A.4	Additional Experiment:	Convolutional Architectures ................ 15
A.5 Experiments from Section 5.1-Evidence of Implicit Regularization.... 16
A.6 Verification of Closed-Form Solutions .............................. 17
A.7 Exact Convex Formulation and Implicit Regularization ............... 17
B Additional Theoretical Results	21
B.1	Extensions to Arbitrary Convex Loss Functions ..................... 22
B.2	Proof of Lemma 1.1 ................................................ 22
B.3	Derivation of the Dual Problem in (5) ............................. 23
B.4	Hyperplane Arrangements ........................................... 24
C	Two-Layer Networks	24
C.1 Closed-Form Solution to the Dual Problem in (7) .................... 24
C.2 Proof of Theorem 2.1 ............................................... 25
C.3 Proof of Theorem 2.2 ............................................... 25
C.4 Proof of Lemma 2.1 ................................................. 28
C.5 Proof of Theorem 2.3 ............................................... 29
C.6 Proof of Theorem 2.4 ............................................... 30
D Convolutional Neural Networks	31
D.1 Proof of Theorem 3.1 ............................................... 31
E The Implicit Regularization Effect of SGD on BN Networks	32
E.1 Proof of Lemma 5.1 ................................................. 32
E.2 Proof of Theorem 5.1 ............................................... 32
F	Deep Networks	34
F.1 Proof of Theorem 4.1 ............................................... 34
F.2 Proof of Theorem 4.2 ............................................... 35
G	BN after ReLU	35
G.1 Proof of Theorem G.2 ............................................... 35
G.2 Proof of Theorem G.1 ............................................... 36
A Additional Details and Numerical Experiments
In this section, we present the details about our experimental setup in the main paper and provide
new numerical experiments.
We first note that since we derive exact convex formulations in Theorem 2.2, 2.3, 3.1, we can use
solver such as CVX (Grant & Boyd, 2014) and CVXPY (Diamond & Boyd, 2016; Agrawal et al.,
2018) with the SDPT3 solver (Tutuncu et al., 2001) to train regularized ReLU networks training
problems with BN. Although these solvers ensure quite fast convergence rates to a global minimum,
they do not scale to moderately large datasets, e.g., CIFAR-10. Therefore, in our main experiments,
13
Published as a conference paper at ICLR 2022
we use an equivalent unconstrained form for the constrained convex optimization problems such that
it can be simply trained via SGD. Particularly, let us consider the constrained convex optimization
problem in (8), for which the constraints can be incorporated into the objective as follows
1
min 一
si ,s0i 2
P
X DiU0 (si - s0i) - y
i=1
2P
+ β X(ksik2 + ks0ik2)
2	i=1
P
+ PX IT ((-(2Di- In)U0Si)+ + (-(2Di- In)U0si)+)
i=1
where ρ > 0 is a hyper-parameter to penalize the violating constraints. Therefore, using the uncon-
strained form above, we are able to use SGD or other standard first-order optimizers to optimize the
constrained convex optimization problem in (8).
Unless otherwise stated, all problems are run on a single Titan Xp GPU with 12 GB of RAM, with
the Pytorch deep learning library (Paszke et al., 2019). All SGD models use a momentum parameter
of 0.9. All baseline models are trained with standard weight-decay regularization as outlined in (3).
All batch-norm models are initialized with scale γj(l) = 1 and bias αj(l) = 0, and all other non-convex
model weights are initialized with standard Kaiming uniform initialization (He et al., 2015). For the
convex program, we simply initialize all weights to zero.
A.1 Inference in BN models
For the non-convex standard BN network, we perform inference in the traditional way. All BN
problems with SGD take an exponential moving average of the mini-batch mean and standard
deviation with a momentum of 0.1, as is used as a default in deep learning packages, except in the
case of full-batch GD, in which case a momentum value of 1 is used. In truncated variants, while
the training data is truncated with the explicit regularize] g(X), for test data X, We compute the
standard forward pass fθ,L (X), i.e. without changing the test data in any fashion, and removing
the training-data mean and normalizing by the training-data standard deviation as found with the
exponential moving average method described above. For the convex program, we can form the
weights to the non-convex standard BN network from the convex program weights, and compute the
test-set predictions with the non-convex standard BN network.
A.2 Additional Experiment: Batch Sizes for SGD
The model of BN we present in this paper is with the model with GD without mini-batching. In
this section, we demonstrate that mini-batching does not change the training or generalization
performance of BN networks in any significant manner. Our observations also align with previous
work examining this subject, such as Lian & Liu (2019); Summers & Dinneen (2020).
In particular, we compare the effect of different batch sizes using a four layer CNN, where
the first three layers are convolutional, followed by BN, ReLU, and average pooling, and the final
layer is a fully-connected layer. Each CNN layer has 1000 filters with kernel size 3 × 3 and padding
of 1.
We take the first two classes from CIFAR-100 to perform a binary classification task with
X ∈ R1000×3072 and y ∈ R1000. For (lr, β) = (10-6, 10-4), where lr denotes the learning rate, and
weight-decay is applied to all parameters. We compare the effect of using a batch size of 50, 100, 500,
and 1000. We run the batch size 1000 case for 501 epochs, and then train the other batch size cases
such that the training time is roughly matched. Due to memory constraints, the full-batch form of
SGD does not fit on a standard GPU-thus, all models are trained with a standard CPU with 256 GB
of RAM. We compare the train and test accuracy curves in Figure 5, where we see that all batch
sizes perform roughly equivalently in both test and training performance. This demonstrates that our
full-batch model for BN accurately captures the same dynamics as BN as used in the mini-batch case.
We note that we also perform an ablation study on batch sizes for the nonconvex programs in Section
6 in Appendix A.7.2, and find the same conclusion.
14
Published as a conference paper at ICLR 2022
Figure 5: Comparison of different batch sizes (bs) for four-layer CNN with BN layers, (ml , lr, β) =
(1000, 10-6, 10-4). We demonstrate that different batch sizes perform roughly equivalently in both
training and test convergence.
A.3 Additional Experiment: Singular Value Truncation Ablation
In order to make clear the effect of the singular value truncation as proposed in Section 5.2 on
generalization performance, we include an additional ablation study on a new dataset to verify our
intuition. In particular, we evaluated the proposed methods on the CNAE-9 dataset (Ciarelli &
Oliveira, 2009), a dataset of 1080 emails with 856 text features over 9 classes. Following Ciarelli &
Oliveira (2009), the first 900 samples were used as the training set, while the final 180 samples are
used as the test set in this experiment, to obtain X ∈R900×856, y ∈R900×9.
We evaluated the proposed approaches with (m1, β, bs) = (10000, 10-2, 900), comparing the base-
line non-convex SGD approach to the convex truncated formulation for k ∈ {10, 100, 200, 400, 856},
where k is the index for which all smaller singular values are truncated from the training dataset.
We trained each model for roughly an equal amount of time (15 seconds). For all architectures, a
learning rate of 10-5 was fixed.
We find in Figure 6, that as expected, singular value truncation can effectively match the
training and test performance of the SGD BN baseline, but only in a certain range of k. When k is
too low, such as k = 10, valuable features of the data are eliminated and the convex program underfits,
whereas when k is too large, such as k = 856, the convex program overfits and does not generalize
well. Values of k = 100 and k = 200 generalize the best for this problem, and their corresponding
truncations preserve 80% and 91% of the training data respectively. The exact value of k which is
optimal for generalization is a hyper-parameter that can be tuned, though it is clear that a large range
of values can often work well. This ablation lends additional support to the truncation approach
proposed in Section 5.2.
A.4 Additional Experiment: Convolutional Architectures
To demonstrate the effectiveness of the convex formulation in convolutional settings, we provide
an additional experiment to show the success of this approach. In particular, as the baseline
architecture, we consider a two-layer network which consists of a convolutional layer, followed
by a channel-wise BN, followed by a ReLU activation, then subsequently followed by an average
pooling operation, flattening, and a fully-connected layer. To use our convex formulation (e.g. (15))
for such an architecture, we can split an input image into spatial blocks (each block corresponds
to a region whose outputs will be averaged), and apply a global-average pooling model as in
(15) to each spatial block separately and combine the predictions from each spatial block to-
gether to obtain one prediction. This suffices as a relaxation of the baseline convolutional architecture.
To this end, we implement the non-convex and convex architectures on CIFAR-10 with the
AdamW optimizer for 50 epochs. The convolution is with a 3 × 3 kernel with padding of
2 and stride of 3, with average pooling to a spatial dimension of 4 × 4. The parameters cho-
15
Published as a conference paper at ICLR 2022
Time (sec}
(a) CNAE-9-Training accuracy
Figure 6: Comparison of different truncation schemes of convex program compared to SGD baseline,
with (m1, β, bs, lr) = (10000, 10-2, 900, 10-5), and k ∈ {10, 100, 200, 400, 856}, where k =856 is
simply the convex program without truncation. Large values of k overfit, while low values underfit,
and intermediate values match exactly the performance of SGD baseline.
Method	Train Accuracy	Test Accuracy
Baseline	8733	67:06
Baseline Truncated	7841	6662
convex (ours)	8160	6691
convex Truncated (ours)	81.35	67.11 —
Table 1: Results of CIFAR-10 classification with two-layer ReLU CNN architectures, with a
3 × 3 kernel size, padding of 2, stride of 3, and averaging to 4 × 4 spatial dimensions, with
(n, m1, β, bs, k, C, lr) = (5 × 104, 1024, 5 × 10-4, 250, 22, 10, 10-2). Consistent with our results on
fully-connected architectures, the convex architecture matches or improves upon the performance of
the non-convex architecture, verifying our theoretical results. We also observe the less pronounced
impact of truncation in convolutional experiments.
sen for this model are given by (n, m1, β, bs, k, C, lr) = (5 × 104, 1024, 5 × 10-4, 250, 22, 10, 10-2).
We compare the results of the baseline, baseline-truncated, convex, and convex-truncated
approaches. In Table 1, we see that the accuracy of this CNN approach is higher than the
fully-connected methods we reported in our paper, and as expected, there is a strong correspondence
between the performance of the convex and baseline models, again reiterating our strong theoretical
results, even in the case of this convex relaxation.
As illustrated in Table 1, truncation enables the convex approach to obtain equivalent or higher
test accuracy than its non-convex counterparts, while obtaining similar training accuracy. However,
notice that the accuracy improvement provided by truncation is considerably less than the ones for
fully-connected layers (e.g. Figure 3a). This phenomenon is essentially due the distribution of the
singular values of the data matrix. Particularly, for fully connected layers, we have U ∈ Rn×d, and
therefore the dimension we truncate has d= 3072 features. In this case, the distribution of singular
values follows an exponentially decaying pattern such that the ratio of maximal and minimum singular
values, also known as the condition number of the data, is quite large (σmɑχ∕σma = 5892.5). On the
contrary, for CNNs, we operate on the patch matrices so that Umk ∈ Rn×h, where h= 27 is the size of
each convolutional filter. Since the data matrix is much more well conditioned (σmaχ∕σmin = 271.2)
and has a significantly smaller feature dimension, the impact of truncation is less emphasized in our
experiment.
A.5 Experiments from Section 5.1—Evidence of Implicit Regularization
As described in the main paper, for both the standard BN architecture and whitened architecture, we
use the parameters (n, d, m1, β, bs, lr) = (50000, 3072, 1000, 10-4, 1000, 10-4), where bs stands
for batch size and lr stands for learning rate. Both models were trained with SGD for 501 epochs,
16
Published as a conference paper at ICLR 2022
and the learning rate was decayed by a factor of 2 in the case of a training loss plateau. We now
expand on the definition of cosine similarity to clarify Figure 1 (in the main paper).
For weights (W, W0), we define the distance measure
dX,i(W, W0):
(vi>W)(vi>W0)>
kv>Wk2kv>W0k2
(18)
where Vi are the columns of V from the SVD of the zero-mean data (In - 111> )X. This distance
metric thus computes the cosine similarity between W and W0 after being projected to the right
singular subspace vi .
For the same networks trained in Figure 2 (in the main paper), we compute
{dX,i(W(1)(k), W(1)(0))}id=1 for the BN model (9), and {dU,i(Q(k), Q(0))}id=1 for the equivalent
whitened model (16), for each epoch k. We display these in Figure 1 (in the main paper). These
correspond to a measure of how far the hidden-layer weights move from their initial values over time
in the right singular subspace of the data, with lower values indicating that the weights have moved
further from their initialization.
A.6 Verification of Closed-Form Solutions
See Figure 4 for the results of this experiment. The GD baseline model is trained with a
learning rate of 10-5, and the learning rate was decayed by a factor of 2 in the case of a
training loss plateau. The closed-form solution takes only 0.578 seconds to compute, whereas we
run the GD baseline model for 40001 epochs, which takes approximately 50 minutes to run per
trial. Each trial behaves slightly differently due to the randomness of the initialization of each network.
In terms of the generalization performance, the closed-form expression obtains a test three-
class classification accuracy of 47.7%, whereas the baseline GD networks achieve an average of
60.1% final test accuracy, again indicating the implicit regularization effect imposed by GD applied
to BN networks.
A.7 Exact Convex Formulation and Implicit Regularization
In this section, we describe additional details about our main experimental results of the main paper.
In particular, we show the effect of training all models for a longer duration than displayed in the
main paper, the different learning rates considered, the different batch sizes considered, along with
the effect of using Adam as opposed to SGD, and comparison to a non-BN architecture.
For these experiments, we trained each model (SGD, SGD-Truncated, Convex, Convex-
Truncated) for roughly the same amount of time, and considered a range of learning rates for each
problem. In particular, for CIFAR-10, we considered the learning rates {10-5, 5 × 10-5, 10-4}, and
trained the SGD, SGD-Truncated, Convex, and Convex-Truncated models for 501, 501, 51, and 201
epochs respectively, and each model was trained for roughly 700 seconds. In contrast, for CIFAR-100,
we considered the learning rates {10-5, 5 × 10-5, 10-4}, and trained the SGD, SGD-Truncated,
Convex, and Convex-Truncated models for 621,1001,41, and 251 epochs respectively, and each
model was trained for roughly 800 seconds.
The range of learning rates was chosen such that all models reached their peak test accu-
racy within the allotted time, and the learning rate with the best final test accuracy was chosen for
each model. In particular for CIFAR-10, the chosen learning rates for the SGD, SGD-Truncated,
Convex, and Convex-Truncated models were 10-5, 10-5, 5 × 10-5, and 10-4, respectively. In
contrast, for CIFAR-100 the chosen learning rates for the SGD, SGD-Truncated, Convex, and
Convex-Truncated models were 5 × 10-5, 10-4, 10-4, and 5 × 10-4, respectively.
For our CIFAR-10 experiments in this section, we always use (m1, β, bs) = (4096, 10-4, 103),
while for CIFAR-100, we used (m1, β, bs) = (1024, 10-4, 103), where bs stands for batch size, as
described in the main paper. The only exception is during our batch size ablation study in Section
17
Published as a conference paper at ICLR 2022
A.7.2, in which case the batch size is varied. All models are trained with SGD, except in Section
A.7.4.
A.7.1 Training Curves and Overfitting
----SGD (Baseline)
--≡ Convex (Ours)
----SGD-Truncated (Ours)
--Convex-Truncated (Ours)
0	200	400	600	800 1000 1200 1400
Time (sec)
(a)	CIFAR-10-Training accuracy
0	200	400	600	800 1000 1200 1400
Time (sec)
(b)	CIFAR-10-Test accuracy
1.0
0.8
&0.6
E
□
U
U _
< 0.4
0.2
0.0
0.05
0 5 0 5 0
3 2 2 1 1
■ ■ ■ ■ ■
Ooooo
>UE□υυ<
o.oo-∣_____________l_____________l______l______l______l______
0	250 500 750 1000 1250 1500 1750 2000
Time (sec)
—SGD (Baseline)
--Convex (Ours)
---SGIMruncated (Ours)
--'Convex-Truncated (Ours)
0	250 500 750 1000 1250 1500 1750
Hme (sec)
(c) CIFAR-100-Training accuracy
(d) CIFAR-100-Test accuracy
Figure 7: Here, we provide a longer trained version of the CIFAR-10 and CIFAR-100 experiments
in Figure 3 (in the main paper). We see that training for longer does not affect the results, as all
models begin to overfit, and they overfit at the same rate. For CIFAR-100, we observe that the
baseline overfits so much that the standard Convex model eventually overtakes it in generalization
performance.
To make more robust our claims in the main paper, we consider training each network for a longer
duration, such that each network can near 100% training accuracy. We plot the results of both training
and test accuracy for CIFAR-10 and CIFAR-100 in Figure 7. We note that extending training for
longer only seems to increase over-fitting, and increasing train accuracy to 100% does not improve
model performance. Therefore, the results of our main paper, which display the curves for a shorter
duration, represent the peak performance of all algorithms before they begin to overfit.
A.7.2 Impact of Different Batch Sizes
To further bolster our claim that taking the convex dual form from a full-batch case of SGD is
sufficient for capturing the performance of standard BN networks, we compare the performance
of Convex and Convex-Truncated with SGD with different batch sizes, namely bs ∈ {100, 500}. In
Figure 8, we show that different batch sizes do not improve the generalization performance of BN
networks, thereby validating that the convex dual derived from the full-batch model is accurate
at representing BN networks when trained with mini-batch SGD. Therefore, for all of our other
experiments, we use bs = 1000.
18
Published as a conference paper at ICLR 2022
0.30
0.25
- 一 O
0 5 0 5 0
5 4 4 3 3
■ ■ ■ ■ ■
Ooooo
5GD (Baseline)-bs=1000
SGD (Baseline)-bs=500
SGD (Baseline)-bs=100
τ⅛-' Convex-Truncated (Ours)
—SGD-Truncated (0urs)-bs=1000
----SGD-Tiuncated (0urs)-bs=500
SGD-Tiuncated (0urs)-bs=100
0 5 0 5 0
2 110 0
■ ■ ■ ■ ■
Ooooo
AUeJnUSV
—SGDTuncated (0urs)-bs=1000
SGDFruncated (0urs)-bs=500
-SGDmnCated (0urs)-bs=100
-⅛- Convex:rruncated--(Ours)
—SGD(Baseline)-bs=1000
—SGD(Baseline)-bs=500
ι	— SGD(Baseline)-bs=100
0	100	200	300	400	500	600	700	800
Time (sec)
100 200 300 400 500 600 700 800
Time (sec)

(a) CIFAR-10	(b) CIFAR-100
Figure 8: Batch size (bs) ablation study, considering bs ∈ {100, 500, 1000} for SGD (Baseline) and
SGD-Truncated for the experiment in Figure 3 (in the main paper). We see that smaller batch sizes do
not improve the performance of SGD. Therefore, we choose bs= 1000 for all the main experiments.
0.60
0.55

0.35
0.50
0.45
0.40
>UE□UU<
SGD-Truncated (Ours)-Ir=Ie-S
SGD-Truncated (Ours)-lr=5e-5
----SGD-Truncated (Ours)-Ir= le-4
Convex-Truncated (Ours)
—SGD(Baseline)-Ir=Ie-S
—SGD (Baseline)-lr=5e-5
SGD(BaseIine)-Ir=Ie-A
0.30-	................. ………
0	100 200 300 400 500 600 700 800
Time (sec)
05
■
Oooooo
>UE□UU<
0.00
0	100 200 300 400 500 600 700 800
Time (sec)

(a) CIFAR-10	(b) CIFAR-100
Figure 9: Learning rate (lr) ablation study for the experiment in Figure 3 (in the main paper),
considering lr∈ {10-5, 5 × 10-5, 10-4} for SGD and SGD-Truncated. We see that larger lrs do not
improve the performance of SGD, because these models with larger lrs reach the same peak accuracy
but overfit quicker. For CIFAR-10, we selected learning rates 10-5 for both SGD and SGD-Truncated.
For CIFAR-100, we chose learning rates 5 × 10-5 for SGD, and 10-4 for SGD-Truncated.
A.7.3 Impact of Different Learning Rates
We plot test curves for all considered learning rates for each program for the baseline program for
CIFAR-10 and CIFAR-100 in Figure 9. We see that the different learning rates considered do not
significantly change the peak performance, with higher learning rates simply overfitting earlier.
A.7.4 Impact of Different Optimizers
We also consider training both the standard non-convex and our convex formulations with the Adam
optimizer (Kingma & Ba, 2014). Not surprisingly, we see a similar implicit regularization effect when
applied to BN networks. For the Adam experiments, we consider the same set of learning rates as
the SGD experiments, and fix hyper-parameters (β1, β2, ) = (0.9, 0.999, 10-8), as are the standard
default values. We run each program for the number of epochs as the SGD experiments, and consider
the same learning rates as the SGD experiments. For CIFAR-10, the chosen learning rates for all
the models was 10-5. For CIFAR-100, the chosen learning rates for the Adam, Adam-Truncated,
Convex, and Convex-Truncated models were 10-5, 10-4, 5 × 10-5, and 10-5, respectively.
We plot the results of the test curves in Figure 10, and plots of longer trained curves in
Figure 11, as well as the effect of different learning rates in Figure 12. We find that with Adam, the
results mirror that of SGD. For the best learning rate chosen for each method, the truncated convex
formulation performs the best, followed by the truncated non-convex and standard non-convex
19
Published as a conference paper at ICLR 2022
0.6
0.5
-E□uu<
0.2
0.1
Adam (Baseline)
--'Convex (Ours)
Adam-Truncated (Ours)
--'Convex-Truncated (Ours)
0	100 200 300 400 500 600 700 800
>UE□UU<
Time (sec)
(b) CIFAR-100
Time (sec)
(a) CIFAR-10
Figure 10: A counterpart of the experiment in Figure 3 (in the main paper), where we show test
accuracies and all problems are solved with Adam. We see that these results mirror those with SGD,
where Convex-Truncated performs equally well as or outperforms the baseline.
8 6 4 2
■ ■ ■ ■
Oooo
>UEɔυυ<
----Adam (Baseline)
--'Convex(Ours)
Adam-Tnjncated (Ours)
--'Convex-Truncated (Ours)
0.5
N∙4
υ
E
I 0.3
<
0.2
0.1
----Adam (Baseline)
--'Convex (Ours)
Adam-Truncated (Ours)
--ι Convex-Tnjncated (Ours)
O 200 400 600 800 IOOO 1200 1400 1600
Time (sec)
(a) CIFAR-10-Training accuracy
0	200 400 600 800 1000 1200 1400 1600
Time (sec)
Time (sec)
(c) CIFAR-100-Training accuracy	(d) CIFAR-100-Test accuracy
Figure 11: Here, we provide a longer trained version of the CIFAR-10 and CIFAR-100 experiments
in Figure 10. We see that training for longer does not change the results, as all models begin to overfit.
formulations, followed by the standard convex formulation. We can conclude that our results are
robust to the choice of optimizer.
A.7.5 Comparison of BN to non-BN Two-Layer Architectures
One might speculate what the impact of adding BN to a two-layer network can have, compared to
a simple standard fully-connected architecture. In this section, we compare the standard two-layer
network in both its primal and convex dual form, as first presented in Pilanci & Ergen (2020), to the
BN networks described in this paper. We let all parameters, i.e. (n, d, m1, β, bs, C) be identical to
the setting of the main paper, and again consider a sweep of learning rates as was done for the BN
experiments. For the non-convex primal no BN form, the best performance came from a learning rate
20
Published as a conference paper at ICLR 2022
0.60
a 0.45 ,
0.55
0.50
----Adam-Truncated (Ours)-Ir=Ie-S
----Adam-Truncated (Ours)--lr=5e-5
Adam-Truncated (Ours)-Ir= le-4
τ⅛- 1 Convex-Truncated (Ours)
-⅛-' Convex-Truncated (Ours)-Ir= le-4
----Adam (BaseIine)-Ir=Ie-S
Adam (BaseIine)-Ir=5e-5
----Adam (Baseline)-lr=le-4
⅛ 0.40
« 0.35
0.30
0.25
0.20ι	, ʃ-~-ɪ-~-ɪ-——-r-~~-ɪ-~-ɪ-~-ɪ-~-1 _
0	100 200 300 400 500 600 700 800
W
■
Ooooooo
>UE□UU<
----Adam-Tiuncated (Ours)-Ir= le-4
----Adam-Tiuncated (Ours)-lr=5e-4
----AdamJiuncated (Ours)--Ir= le-3
一 ■ Convex-Tiuncated (Ours)-Ir=Ie-S
-τ⅛-' Convex-Iiuncated (Ours)--lr=5e-5
----Adam (BaseIine)-Ir=Ie-S
Adam (BaseIine)--Ir=Se-S
----Adam (BaseIine)-Ir=Ie-A
Time (sec)
0	100 200 300 400 500 600 700 800
Time (sec)

(a) CIFAR-10	(b) CIFAR-100
Figure 12: Learning rate (lr) ablation study for the experiment in Figure 10, where we consider lr ∈
{10-5, 5 × 10-5, 10-4} for Adam. We see that larger learning rates do not improve the performance
of Adam or Convex-Truncated, because these models reach the same peak accuracy but simply overfit
earlier. For all models, we select a learning rate of 10-5 for the CIFAR-10 experiment in Figure 10a.
We choose a learning rate of 10-5 for Adam and Convex-Truncated, and a learning rate of 10-4 for
Adam-Truncated for the CIFAR-100 experiment in Figure 10b.
0.35
0 5 0 5 0
6 5 5 4 4
■ ■ ■ ■ ■
Ooooo
>UE□UU<
0	200	400	600	800
Time (sec)
0.05
0.001.......................................
0	100 200 300 400 500 600 700 800
Time (sec)
—SGD (Baseline)
—Convex (Ours)
----SGD-Truncated (Ours)
---ConVeXJrUnCated (Ours)
--'Convex no BN (Ours)
----SGD no BN (Baseline)
(a) CIFAR-10	(b) CIFAR-100
Figure 13: Comparison of two-layer BN networks presented Figure 3 (in the main paper), to the
non-convex two-layer ReLU architecture without BN, and its equivalent convex dual as presented
in Pilanci & Ergen (2020). All problems are solved with SGD and the learning rates are chosen
based on test performance. We find that both the SGD and Convex-Truncated programs improve in
generalization performance when adding BN, though the margin may vary depending on the problem.
We also observe that the Convex BN program without truncation does not perform as well as the
standard no-BN architecture.
of 5 × 10-7 on CIFAR-10 and 10-5 for CIFAR-100, while for the convex no BN problem, the best
performance was with a learning rate of 5 × 10-10 on CIFAR-10 and 5 × 10-9 for CIFAR-100.
We then compare the best results of each method in Figure 13. We see that for both CIFAR-10 and
CIFAR-100, introducing a BN layer can significantly improve the performance of the two-layer
network architecture, both when compared to the result found with SGD, and the equivalent convex
formulation.
B Additional Theoretical Results
In this section, we present additional theoretical results that are not included in the main paper due to
the page limit.
21
Published as a conference paper at ICLR 2022
B.1 Extensions to Arb itrary Convex Loss Functions
In this section, we prove that our convex characterization holds for arbitrary convex loss functions
including cross entropy and hinge loss. We first restate two-layer training problem with arbitrary
convex loss functions after the resealing in Lemma 1.1
min L(fθ,L(X), y) +β w(L)	(19)
θ∈Θs	1
where L(∙, y) is a convex loss function. Then, the dual of (19) is given by
max -L*(v) s.t. max ∣vτBNγ,α (A(L-2)w⑴)∣ ≤β,
where L* is the FencheI conjugate function defined as Boyd & Vandenberghe (2004)
L*(v) =maxZTV — L(z, y).
z
The analysis above proves that our convex characterization in the main paper applies to arbitrary loss
function. Therefore, optimal parameters for (3) are a subset of the set that includes the extreme points
of the dual constraint in (5) independent of loss function.
B.2 Proof of Lemma 1.1
Here, we first prove that regularizing the intermediate BN parameters and hidden layer weights don’t
affect the optimization problem therefore the set of optimal solutions for the problems in (20) and
(22) are the same. Then, we show that (22) can also be cast as an optimization problem with `1 norm
regularization penalty on the output layer weights wL .
B.2. 1 Effective Regularization in (3)
Here, we prove our claim about the effective regularization. Let us restate the training problem as
follows
Pz :=min1
PLT	θ∈θ 2
mL-1
X (BNγ,α(A(L-2)wj(L-1)))+wj(L) —y
j=1	+
2
+2 X(IIγ 啡+IB)II2+W)IIF
(20)
where we use γ(L) = α(L) = 0 as dummy variables for notational simplicity. Now, let us rewrite (20)
as
PLr =min min -
Lr t≥0 θ∈Θ 2
mL-1
X (BNγ,α (A(L-2)wj(L-1)))+wj(L) —y
j=1	+
2
2
β mL-1	(L-1)2	(L-1)2	(L)2 β
+ 2 工(Yj	+ αj	+ Wj	J + 2t
j=1
L-2
s.t. X	IIIγ(l)III22 +	IIIα(l)III22	+	IIIW(l)III2F	+	IIIW(L-1)III2F ≤ t.
After applying the scaling in Lemma 1.1, we then take the dual with respect to w(L) as in the main
paper to obtain the following problem
1	1β
PLr ≥ dLr=maxmax - 2 kv - yk2 + 2 kyk2 + 2t
(21)
s.t. max
θ∈Θsr
vT (BNγ,α (A(L-2)w(L-1)
≤ β.
whereΘsr := {θ∈Θ:YjLT)2+。厂1)2 = 1, ∀j ∈ [mL-ι], PL-2 (∣∣γ⑷∣∣2 + IIa(I)||； + IW(I)IIF) +
W(L-1)2F≤t}. Since
BNγ,α A(L-2)wj(L-1)
(In - n 11T)A(L-2)WjL 1)	(L-1) + ɪα(L-1)
k(In — n 11T)A(L-2)W(LT)k2 Yj	√n吃
'-------------------------}
{z^^^
h(θ0)
22
Published as a conference paper at ICLR 2022
where θ0 denotes all the parameters except γ(L-1), α(L-1), w(L). Then, independent of the value t,
h(θ0) is always a unit norm vector. Therefore, the maximization constraint in (21) is independent of
the norms of the parameters in θ0 . Consequently, regularizing the weights in θ0 does not affect the
dual characterization in (21).
Based on this observation, (20) can be equivalently stated as
mL-1
Pl =m∈in2 kfθ,L(X)-yk2 + 2 X (YjLT) + αjL-1) + WjL) )	(22)
∈	j=1
B.2.2 RESCALING FOR `1 NORM REGULARIZATION IN (4)
We first note that similar approaches are also presented in Neyshabur et al. (2014); Savarese et al.
(2019); Pilanci & Ergen (2020); Ergen & Pilanci (2020); ?.
For any θ ∈ Θ,we can rescale the parameters as YjL-I) = ηYjLT) αjL-I) = ηɑjL-I) and WjL)
WjL)/ηj，for any ηj > 0. Then, the network output becomes
,X mX ( (In - 111T)A(L-2)w(LT
fθ,L( )= j= Ik(In- 111T )A(L-2)wjLT)k2
=X1 ( (In- 111T )A(L-2)wjLT)
=M L(In- 111T )A(L-2)wjLT)k2
jLT) + IajLT)) W(L)
+
(L-1)	(L-1)	(L)
Yj	+ 1αj	W(L)
= fθ,L (X),
which proves fθ,2(X) = f仇2(X). Moreover, we use the following inequality
1	mL-1	C	CC	mL-1	I	^	^
2	X (YjLT)2 +αjLT)2 +wjL)2)≥ X IWjL)MYjj)2 +ajLT)2
j=1	j=1
where the equality is achieved when
1
η J _j_ Y
JYjLT)2 + ajLT)2 )
is used. Since this scaling does not alter the right-hand side of the inequality, without loss of generality,
we can set Yj(L-1) + αj(L-1) = 1, ∀j ∈ [mL-1]. Therefore, the right-hand side becomes kw(L) k1
and (22) is equivalent to
min1 kfθ,L(X) - yk2+ βkw(L)kι
θ∈Θs 2
where Θs := {θ ∈ Θ: YjLT) + α(LT) = 1, ∀j ∈ [mL-ι]}.	口
B.3 Derivation of the Dual Problem in (5)
Let us start with reparameterizing the scaled primal problem in (4) as
mL-1
PL= θ∈θ⅛∈Rn 2 ky-yk2 + βHw(T11 si y= X (BNY，a (A(L-2) WjLT)))+ WjL).
s	j=1
(23)
2	(L-1)2
+ αj	= 1, ∀j ∈ [mL-1]}. We now form the following Lagrangian
where Θs := {θ ∈ Θ : Yj(L-1)
mL-1
-VTy + β∣∣w(L)IJ + VT X (BN,,。(A(L-2)Wjj))) WjL)
1	j=1	+
23
Published as a conference paper at ICLR 2022
Then, the corresponding dual function is
g(v)= min L(V y, W(L))
y,w(L)
=-2kv-yk2 + 1 ∣∣yk2 s.t. v> (BNγ,α (A(L-2)WjLT)))十 ≤β,∀j∈ [mL-ι].
Hence, the dual of (23) with respect to y and W(L) is
PL = min maxg(v) = min max —11∣v — y∣2 + 1 ∣∣y∣2 s.t. v> (BNγ,α (A(L-2)WjLT)))	≤β, ∀j ∈ [mL-i].
θ∈Θs v	θ∈Θs v 2	2	j	+
We then change the order of min-max to obtain the following lower bound
PL ≥dL :=max—1 kv — y∣2 + 2l∣yk2 st mθχ v> (BNγ,α (A(L-2)W(LT)))十 ≤d
□
B.4 Hyperplane Arrangements
We first define the set of all hyperplane arrangements for the data matrix X as
H :=|J {{sign(XW)} : W ∈Rd},
where |H| ≤ 2n . We now define a new set to denote the indices with positive signs for each element
in the set H as S := {∪hi=1{i}} : h ∈ H . With this definition, we note that given an element
S ∈S, one can introduce a diagonal matrix D(S) ∈ Rn×n defined as D(S)ii := l[i ∈ S]. Therefore,
the output of ReLU activation can be equivalently written as (XW)+ = D(S)XW provided that
D(S)XW ≥ 0 and (In — D(S)) XW ≤ 0 are satisfied. One can define more compactly these two
constraints as (2D(S) — In) XW ≥ 0. We now denote the cardinality of S as P, and obtain the
following upperbound
P ≤ 2 XX (n — 1) ≤ 2r (I
where r := rank(X) ≤ min(n, d) (Ojha, 2000; Stanley et al., 2004; Winder, 1966; Cover, 1965). For
the rest of the paper, we enumerate all possible hyperplane arrangements in an arbitrary order and
denote them as {Di}iP=1.
C Two-Layer Networks
C.1 Closed-Form Solution to the Dual Problem in (7)
Lemma C.1. Suppose n ≤ d and X is full row-rank, then an optimal solution to (7) is
Ie k(⅛∣2 -β k(-y⅛∣2	ifβ ≤k (y)+ k2,β≤k (-y)+ k2
v* = J (y)+—β 1⅛⅛t	ifβ>k(y)+∣2,β≤(-y)+∣2 .
β |((y))_+||2 - ( —y)+	ifβ ≤ k (y)+ ∣2, β> k (—y)+ ∣2
.	y	ifβ>k (y)+12, β>k(—y)+∣2
Proof. Since n ≤ d and X is full row-rank, we have
max
θ∈Θs
VT (l⅛≡κ Y(I)+√H+
≤max ∣ (v)+ ∣2, ∣ (—v)+ ∣2 max
1T u=0
∣u∣2=∣s∣2=1
≤max{∣ (v)+ ∣2, ∣ (—v)+ ∣2} max (u)+2
∣u∣2 =1
≤max{∣ (v)+ ∣2, ∣ (—v)+ ∣2},
√⅛)+
u
24
Published as a conference paper at ICLR 2022
where s= γ(1) α(1)T and the equality is achieved when
W⑴=Xt(V - min Vi), S = kv 工11旷讨2 .
i	I∣vk2 L	√n1 V 」
Thus, the dual problem (7) can be reformulated as
max-1 l∣v -yk2 + 2l∣yk2 s∙t∙ maχ{k(V)+ k2, k(-V)+ 心}≤仇
The problem above can be maximized using the following optimal dual parameter
ifβ≤l (y)+l2,β≤l (-y)+ l2
ifβ>l (y)+l2,β≤(-y)+l2
.
ifβ≤l (y)+l2,β>l (-y)+ l2
ifβ>l (y)+l2,β>l (-y)+ l2
C.2 Proof of Theorem 2.1
For the solution in Lemma C.1, the corresponding extreme points for the two-sided constraint are
W⑴*
(Xt (y)+ , Xt (-y)+)
Xt (-y)+
Xt (y)+
0
ifβ≤l (Y)+l2,β≤l (-Y)+ l2
ifβ>l (Y)+ l2, β ≤ (-Y)+ l2
ifβ≤l (Y)+l2,β>l (Y)+ l2
ifβ>l (Y)+l2,β>l (-Y)+ l2
and
1
IWh
Il(Y)+—n11T(Y)+ k2
[√ ιT(y)+	J
γ2ι) _	1	∏∣ (-y)+—n11T(Iy)+ k2
ɑ21)* - k (-y)+ k2 [	√nIT (-y)+	_
We now substitute these solutions into the primal problem (6) and then take the derivative with respect
to the output layer weights W(2). Since the primal problem is linear and convex provided that the first
layer weights are fixed as W(I) , we obtain the following closed-form solutions for the output layer
weights
Γ k (y)+12 — β 一
[-k (-y)+ k2 + β
w(2)* = -k(-y)+∣2 + β
k(y)+∣2 — β
o
ifβ≤l (y)+l2,β≤l (—y)+ l2
ifβ>l (y)+l2,β≤l (—y)+l2.
ifβ≤l (y)+l2,β>l (—y)+ l2
ifβ>l (y)+l2,β>l (—y)+ l2
□
These set of weights, {W(1)*,γ(1)*,α(1) *, w(2)*}, achieves p2 = d2, therefore, strong duality holds.
□
C.3 Proof of Theorem 2.2
We first note that if the number of neurons satisfies mi ≥ m* for some m* ∈ N, where m* ≤ n + 1,
then strong duality holds, i.e., p2 = d2 (see Section 2 of Pilanci & Ergen (2020)). In the weakly
regularized regime where the network interpolates the training data, there are even simpler ways
to verify this upperbound, e.g., Zhang et al. (2021) show how to construct a solution that exactly
interpolates the training data with mi = n, and therefore the ReLU network fits any real valued
y ∈ Rn with n neurons.
25
Published as a conference paper at ICLR 2022
Following the steps in Pilanci & Ergen (2020), we first focus on the dual constraint in (7)
T
max v
w(1)
γ(1)2+α(1)2=1
⅛⅛OXw⅛Y(I) + √nα(1)+ =kqk52=1
([uq	√⅛)+
max vT (U0s0)
ks0k2=1	+
max max	vTDiU0s0
i∈[P]	ks0k2=1
(2Di-In)U0s0≥0
where q:= ΣVT w(1) / ∣∣ΣVT w(1) ∣∣2, s := γ(1) α(1)T, s0 := γ(1)qT α(1)T, and U0 :=
[U 3]. Now, let US consider the single side of this constraint and introduce a dual variable
P for the constraint as follows
vrDiU0S0 = max min max (VTDiU0 + ρr (2Di — In)U0) s0
i	i∈[P] ρ≥0 ks0k =1	i	i n
max max
i∈[P]	ks0k2=1
(2Di-In)U0s0≥0
max min vT DiU0 + ρT (2Di - In)U0 2 .
Therefore, we have
T ( (In- n 11r)XW(I)
max V
θ∈θs	[k(In — n 11T )Xw(I)k2
Y(I) + ɪα(I) ) ≤β 0∀ ∈ [P], min ∣∣vTDiU0 + Pr(2Di -
n	ρ≥0
o∀i ∈ [P], ∃ρi ≥ 0 s.t. ∣∣vτDiU0 + PT(2Di
Similar arguments also hold for the negative side of the absolute value constraint. Therefore, we
can reformulate (7) as a convex optimization problem with variables v, ρi , ρ0i ∈ Rn, ∀i ∈ [P] and 2P
constraints as follows
2≤β
In)U0∣∣2 ≤β.
1	1	vT DiU0 + ρiT (2Di -In)U02≤β
d2 =max --kv — y∣∣2 + -kyk2 s.t. ∣∣	T	∣∣	, ∀i∈ [P]. (24)
2	vg 2l1 ∙y"2	2ll∙y 112	∣∣-vT DiU0 + ρi (2Di - In)U0∣∣2 ≤ β
As noted in Pilanci & Ergen (2020), since the problem in (24) is strictly feasible for the particular
choice of parameters v = ρi = ρ0i = 0, ∀∈ [P], Slater’s condition and therefore strong duality holds
(Boyd & Vandenberghe, 2004). We now write (24) in a Lagrangian form with the parameters
λ, λ0 ∈ RP as
d2 = min max
2 λ,λ0≥0 ρi≥v0,∀i
1	1P
-2 kv - yk2 + 2 kyk2 + X λi (β - ∣∣v DiU + pi (2Di- In)U ∣^
i=1
P
+ X λ0i (β - ∣∣-vTDiU0 + P0i (2Di - In)U0 ∣∣ ).
i=1	2
(25)
We next introduce additional variables ri , r0i ∈ Rd , ∀i ∈ [P] to represent (25) as
d22 = min max
2 λ,λ0≥0ρi≥v0,∀i
min - 1 k
ri:kri k2≤1	2
r0i kr0i k2≤1
1P
V - yk2 + 2 Ilyk2 + £% (β - (v DiU + Pi (2Di- In)U ) ri)
i=1
P
+ X λ0i β + -vT DiU0 + P0i (2Di - In)U0 r0i .
i=1
(26)
Now, we remark that the objective function in (26) is concave in v, Pi, P0i and convex in ri, r0i, ∀i ∈ [P].
In addition, the constraint set for the additional variables, i.e., Iri I2 , Ir0i I2 ≤ 1, are convex and
26
Published as a conference paper at ICLR 2022
compact. Then, we use Sion’s minimax theorem (Sion, 1958) to change the order of the inner
max-min problems as
d2r = min min
2 λ,λ0≥0 ri:kri k2≤1
r0i kr0ik2≤1
max
v
ρi ≥0,∀i
1	1P
-2 kv - yk2 + 2 kyk2 + X λi (β -(V DiU + Pi (2Di- In)U ) ri)
i=1
P
+ X λ0i β + -vT DiU0 + ρ0i (2Di - In)U0 r0i .
i=1
(27)
Thus, we can now perform the maximization over the variables v, ρi , ρ0i to simplify (27) into the
following form
p2rc =
min min
λ,λ0≥0ri"∣rik2≤1
r0i kr0ik2≤1
P
X (λiDiU0ri -λ0iDiU0r0i) -y
i=1
(28)
s.t. (2Di - In)U0ri ≥ 0, (2Di -In)U0r0i≥0,∀i∈[P].
We then apply a set variable changes as si = λiri and s0i = λ0ir0i, ∀i ∈ [P] to obtain the following
problem
r
p2r = min min
λ,λ0≥0Si"∣Sik2≤λi
s0i ks0ik2≤λ0i
P
X DiU0(si - s0i) - y
i=1
P
2	i=1
(29)
s.t. (2Di - In)U0si ≥ 0, (2Di - In)U0s0i ≥ 0, ∀i∈[P].
Finally, since the optimality conditions require ksik2 = λi and ks0ik2 = λ0i, ∀i ∈ [P], we write (29) as
a single convex minimization problem as
1
min -
Si,si 2
P
X DiU0(si - s0i) - y
i=1
+ β	(ksik2 + ks0ik2)
2	i=1
(30)
1
2
1
2
2
2
P
s.t. (2Di -In)U0si≥0, (2Di -In)U0s0i≥0, ∀i∈[P].
Now, we note that since (24) is convex and Slater’s condition holds as shown above, we have p2rc = d2r.
In addition, as proven in Pilanci & Ergen (2020) strong duality also holds for the original non-convex
problem, i.e., pr2 = d2r, therefore, we get pr2 = p2rc = d2r.
Constructing an optimal solution to the primal problem:
One can also construct an optimal solution to the non-convex training problem in (6) as follows
1-22
ri
s
,ifs*=0
1-22
,if si*=。,
where {sr, si }i=ι are the optimal solutions to (8) and and jsi ∈ [|Js|] given the definitions J1 :=
{i : sir 6=0} and J2 := {i : s0i* 6= 0}. We also note that here the second subscript denotes the selected
entries of the corresponding vector, e.g., si,1:r denotes the first r entries of the vector si. Thus, we
have mr = |J1| + |J2|.
Verifying strong duality:
Next, we verify strong duality, i.e., p2r = pr2c =d2r, by plugging the optimal solution to the non-convex
objective in (3) and the corresponding convex objective in (30).
*
Given the optimal solutions {sir, s0i }iP=1 to (30), the convex problem has the following objective
r1
p2c =2
P
XDiU0(sir-s0i*)-y
i=1
2P
+ β X(ksirk2 + ks0i*k2)
2	i=1
(31)
27
Published as a conference paper at ICLR 2022
and the corresponding non-convex problem objective is as follows
m	Q m1	/	, 2	, 2	. 2∖
P2 = 1 kfθ,2(X)-yk2 + 2 X (Tr +αΓ +wj2)* ).
j=1、	)
(32)
We first prove that the first terms in both objectives have the same value. To do that, We compute the
output of the non-convex neural network with the optimal weight construction above
mi
fθ,2(X) = X
j=i

(In - 1UTKwjD T⑴2
Il(In- 1 11t )Xw(1)*∣∣2Tj
十
十
1-22
1-22
十
1-22
1-22
1-22
* ∙2
s
1-2 2
*
/ ∙2
s
P
=XDiU,(s* -si*),	(33)
i=1
where the last equality follows from the constraints in (30).
Next, we prove that the regularizations terms have the same objective as follows
+
B X l llSM：rll2 + str + 1 , Il *∣∣
2⅛ IIsE + HlK + llsill2
i∈J 1
+2 x
i∈J2
IlGrll
lls"2
2 J 2
2 _l si,r+1	1 ∣∣√*∣∣
T KlK + llsi ll2
=BE I∣s*I∣2 + BE ∣∣s"∣2
i∈J1	i∈J2
P
=B X(ks"∣2 + ∣∣s"∣2).
i=1
(34)
Therefore, by (33) and (34), the non-convex objective (32) and the convex objective (31) are the
same, i.e., p2 = p2c∙	□
C.4 Proof OF Lemma 2.1
We note that similar approaches are also presented in Sahiner et al. (2021a);
28
Published as a conference paper at ICLR 2022
For any θ ∈ Θ, We can rescale the parameters as γj1 = %• γj1), αj1) = %∙ ɑj1) and Wj2) = w(2)∕ηj
for any ηj > 0. Then, the network output becomes
fθ,2(χ)=X ((In- 1111T)Xw(j； YjI)+IaI)[ wj2)T
j=1U(In- 111T )Xw「k2 j	j ++ j
=X ( (In- 111T)Xw(： Y⑴ + 1a⑴! w(2)T
j=1 Ik(In- 111T )Xwj1)k2 j	j 人 j
= fθ,2 (X),
which proves fθ,2(X) = f仇2(X). Moreover, we use the following inequality
1 X (γj1)2 + αj1)2 + l∣wj2) Il2) ≥ X l∣wj2)	Yj1)2 + αj1)2,
j=1	j=1
where the equality is achieved when
ηj
JK)IL ʌ2
qYFM7)
is used. Since this scaling does not alter the right-hand side of the inequality, without loss of generality,
we can set γj(1) + α(j1) = 1, ∀j ∈ [m1]. Therefore, the right-hand side becomes Pjm=11 kwj(2) k2.
We then note that due to the arguments in Appendix B.2.1, we can remove the regularization on w(j1)
without loss of generality. Therefore, the final optimization problem is as follows
m1
诙=min21加(X)-YkF+β XM)Il2,
s	j=1
where Θs := {θ ∈ Θ : γj(1) + α(j1) = 1, ∀j ∈ [m1]}.
□
C.5 Proof of Theorem 2.3
We first note that if the number of neurons satisfies mi ≥ m* for some m* ∈ N, where m* ≤ nC + 1,
then strong duality holds, i.e., p2v =通V, (see Section A.3.1 of Sahiner et al. (2021a) for details).
Following the steps in Sahiner et al. (2021a) and Theorem 2.2, we again focus on the dual constraint
in (11)
max
w(1)
γ(1)2+α(1)2=1
VT O、γ(I)+方⑴
= max
kqk2=ksk2=1
2
+
2
max lVT (U0s0)+ l
ks0k2=1	+ 2
max	gTVT (U0s0)
kgk2=ks0k2=1	+
max max	gTVTDiU0s0
i∈[P] kgk2=ks0k2=1
(2Di-In)U0s0≥0
max max	V, DiU0s0gT
i∈[P] kgk2=ks0k2=1
(2Di-In)U0s0≥0
max max hV, DiU0Si ,
i∈[P] S∈Ki	i
29
Published as a conference paper at ICLR 2022
where s0= [γqT α]T, U0= [U √ ], and Ki =Conv{Z = s0gT :(2Di - In)U0s0 ≥ 0, ∣∣Z∣∣* ≤
1}. Thus, we have
mθxIVT (k(f-n；1；XXγ(1) + √nα(1) J]≤βO∀i∈[P], ∃Si∈Ki s.t. hV，DiUsi≤β
Similar arguments also hold for the negative side of the absolute value constraint. We then directly
follow the steps in Sahiner et al. (2021a) and achieve the following convex program
mm 一
Si 2
P
XDiU0Si-Y
i=1
2P
+ β X kSikci,*,
F i=1
1
where
kS∣∣ci,* :=mint s.t. S ∈tKi
denotes a constrained version of the standard nuclear norm.	□
C.6 Proof of Theorem 2.4
We first restate the dual problem as
max- 1IIV - YkF + 1 IIYkF s.t. ..max IVT (UOSO)+I2 ≤β,
V 2	2	ks0k2=1	+ 2
(35)
where we use the results from Theorem 2.3 to modify the dual constraint. Since Y is one-hot encoded,
(35) can be rewritten as
max - 1 kV - YkF + 1 IIYkF s.t. max, kVTuk2 ≤β
V 2	2	kuk2=1
(36)
This problem has the following closed-form solution
βkyyj2	ifβ≤kyjk2,
yj otherwise ,
and the corresponding first layer weights in (36) are
w(i)* —ʃXtyj	ifβ≤kyjk2
w=
j	- otherwise
v* =
∀j∈[C].
∀j ∈ [o].
(37)
(38)
Now let us first denote the set of indices that achieves the extreme point of the dual constraint as
E := {j : β ≤ kyjk2,j ∈ [C]}. Then the dual objective in (36) using the optimal dual parameter in
(37)
d2v = -1 kV*- YkF + 1 kYkF
1	1C
=-2 X(β - kyj k2) + 2 X kyj k2
j∈E	j=1
=-1 β2∣E∣ + β X kyjk2 + 1 X kyjk2.
j∈E	j∈E
We next restate the scaled primal problem
p2v =min1
2v θ∈Θs 2
m1
X
j=1
((In- n 11TKWjII) γ(1) + ɪa(1)! w(2)T - Y
Ik(In- 111T )Xwj1)k2 7j	√n j J j
(39)
2
m1
+βXIIIwj(2)III2
F j=1
(40)
30
Published as a conference paper at ICLR 2022
and then obtain the optimal primal objective using the hidden layer weights in (38), which yields the
following optimal solution
(Xtyj, (IIyjk2 - β)ej)	ifβ≤kyjk2
otherwise
γj(1)
α(j1)
*-∣
1
kyj∙ II2
W-1 n*k2
L	√n1 yj 」
(41)
—
*
We now evaluate the primal problem objective with the parameters in (41), which yields
2
C
*1
P2v = 2
C
X
j=1
(In - 111T)XwjI)*
k(In- 111T )Xwj1)*k
γj(1)
2
2
*	1	(1)*	(2)*T
+ √nαj	J Wj -Y
j=1
(kyjk2-β)
j∈E
j∈E
yj	eT
kyjk2 j
yj
也k
2
F
2ejT-Y
+β	(kyjk2-β)
j∈E
2 X IMeT kF + β X kyjk2-β2∣E∣
j∈E
j∈E
1
2
2 X e
F
F
- 2 稼㈤+2 χ kyjk2+β χ kyjk2,
(42)
j∈E
j∈E
which is the same with (39). Therefore, strong duality holds, i.e., p2*v = d2*v , and the layer weights in
(41) are optimal for the primal problem (40).
□
D Convolutional Neural Networks
D.1 Proof of Theorem 3.1
Following the arguments in Theorem 2.2 and Ergen & Pilanci (2021a), strong duality holds for the
CNN training problem in (13), i.e., p2*c = d2*c, and the dual constraint can be equivalently written as
follows
max
w(1)
γ(1) 2+α(1) 2=1
K
XvT
k=1
XkZ - μ1
-/	Y
JPK=IkXk0Z -μ1k2
'⑴ + √K 1ɑ(1)
+
max
θ∈Θs
VTC ( (InK - nK 11T)MZ γ(i) + _L_α(i))
Ik(InK - nK 11T)MZk2	√nK	/ +
max
kqk2=ksk2=1
vTC	UMq
s+
max
ks0k2=1
max
ks0k2=1
K
X vT (U0Mks0)+
k=1
max
i∈[P]
max
ks0k2=1
K
X vTDikU0Mks0
k=1
(2Dik-In)U0Mks0≥0
where C:= 1Tζ	0 In, M =[Xi； X2；	... XK], and	We use	the following variable	changes
q := ∑mVMz,	S := [γ⑴ α⑴]T,	s0 := [γ(1)qτ	α⑴]T, and UM := [um	√=]=
[U0M1
; U0M2 ; . . . U0MK].
31
Published as a conference paper at ICLR 2022
Then, following the steps in Theorem 2.2 and Ergen & Pilanci (2021a), the equivalent convex program
for (13) is
1
mm 一
qi ,q0i 2
Pc K
XXDikU0Mk(qi -q0i)
Pc
+ β X(kqik2 + kq0ik2)
i=1
s.t. (2Dik - In)U0Mkqi ≥ 0, (2Dik - In)U0M k q0i ≥ 0, ∀i ∈ [Pc], ∀k ∈ [K].
□
E The Implicit Regularization Effect of SGD on BN Networks
E.1 Proof of Lemma 5.1
We start with the following non-convex training problem
p；2 :=min1
PV	θ∈θ 2
m1	T
XBNγ,αXWj(1)+Wj(2)T-Y
j=1	+
2
F
β m1	(1)2	(1)2	(2)
+ 2Σ (Jj	+ ɑj	+∣∣Wj
j=1
2
2
where we note that hidden layer weights are not regularized without loss of generality due to the
arguments in Appendix B.2.1. Now, consider the SVD of the zero mean form of the data matrix
(In - n 11>)X = UΣV>. WecanthUs equivalently write the problem as
m1
1 m1
ρ*9 =min -
S θ∈θ 2 乙
j=1
uςv> wj1)	⑴	1	(l)ʌ	(2)T	V
--------m— Yj	+--^aj	Wj — Y
kUΣV>w(1)k2	j	√n	j I	j
+
2
F
β m1	(1)2	(1)2	(2)
+ 2Σ (Tj	+ ɑj	+∣∣Wj
j=1
2
2
Now, simply let qj = ΣV>Wj(1). Noting that kUqj k2 = kqj k2, we arrive at
2
pV2 = pV2b :=min1
θ∈Θ 2
m1
X
j=1
UqL户
Yj
kqjk2
1	(1)	(2)
+ √nɑj ++wj
T
-Y
F
β m1	(1)2	(1)2	(2)
+2∑ Cj	+ αj	+∣∣Wj
j=1
2
2
□
E.2 Proof of Theorem 5.1
When solving (9) with GD, we have the updates
WjI)(k+1)=WjI)(k) - ηkVw(I)L(fV2(X), Y)
aj1)(k+1)= aj1)(k) - ηk(vɑ(i) L(fV2(X), Y)+ βaj1))
γj(1)(k+1) = γj(1)(k) - ηkVγ(1) L(fvk2(X), Y) + βγj(1)
Wj(2)(k+1) = Wj(2)(k) - ηkVw(2)L(fvk2(X), Y) + βWj(2)
32
Published as a conference paper at ICLR 2022
Now, let Djk) =diag{lXWjI)(k) ≥0]}, R(k) = Pm=I(BN),a (XWjI)(k)) WFkkT - Y be
the residual at step k, and X0 = (In - ɪ 11>)X . Then, with squared loss L, We have the following:
(1)(k)
Nwa L(fV2(X), Y)=	YLl、(k)	(Djk)X0)>R(k)wj2)(k)
j	kX0Wj(1)(k)k2
—
(1)(k)	>
」(WjI) k	(d")>rf XX0wj1)k
Nα(1) L(fvk2(X), Y) = Wj(2)(k)T R(k)>D(jk)1
V (1)L(fV2(X), Y) =——l-(k^wj2)(k)TR(k)>Djk)X0wj1)(k)
γj	kX0Wj(1)(k)k2	j	j j
0	(1)(k)
vw(2) L(fV2(X Y) = R(k)>Djk)( -wj-kr- YjI) k + √ αj1))
j	kX0Wj(1)(k)k2	n
To express the updates in qj -space for Wj(1), we can compute
∆(vk2),j=ΣV>Vw(1)L(fvk2(X),Y)
(1)(k)	(1)(k)	>
=—Y-τw-Σ2U>D(k)R(k)w(2)(k)-----------------Y-τw- (W(I)(k) X0>D(k)R(k)w(2)(k)ɔΣ2U>X0w(1)(fc)
kX0wj1)(k)k2	j j MWr/3 L	jjJ	j
Now, we consider solving (16) with GD. We assume that all weights are equivalent to those in (9), i.e.
fvk2b(X) = fvk2(X), and q(jk) = ΣV>W(j1)( ). In this case, the gradients are given by:
(1) (k)
VqjL(fV2b(X Y) = kqj(^ (Djk)U)TR(k)Wj2)(
—
(1)(k)
kqjw (qj(k)>(Djk)U)TR(k)wj2) k )qj(k)
Vα(1) L(fvk2b(X), Y) = Wj(2)(k)T R(k)>D(jk)1
vγji) L(fV2b(X), Y)=kqj⅛ Wj2)(k)T R(k) > Djk) Uqj(k)
vwj2)Lf2b(X), Y)=R(k)>Djk)(kuj⅛ γj1)(k)+ √nα1))
Simplifying, we have
(1)(k)	(1)(k)
∆(k) = j_______U>D(k)R(k)W(2)(A：)__________(Q ∙(k)>U>D(k)R(k)W(2)(Aɔ)Q -(k)
△v2bj=||qj(k)∣∣2 U Dj R Wj	∣∣qj∙(k)∣∣3(qj	U Dj R Wj	Jqj
Now, noting that X0W(j1)( ) = Uqj(k), and thus thatkX0Wj(1)( )k2 = kqj(k)k2, we can find that, as
desired,
∆(vk2)b,j=Σ2∆(vk2),j
V (1) L(fvk2b(X), Y) = V (1) L(fvk2(X), Y)
γj	γj
Vα(1)L(fvk2b(X), Y) = Vα(1)L(fvk2(X), Y)
jj
Vw(2)L(fvk2b(X), Y) = Vw(2)L(fvk2(X), Y)
jj
□
33
Published as a conference paper at ICLR 2022
F DEEP Networks
F.1 PROOF OF THEOREM 4.1
Applying the same steps in Theorem 2.1 for the data matrix A(L-2), i.e., replacing X with A(L-2),
yields the following optimal parameters
，(A(L-2)t (y)+ , A(L-N (-y)+)
w(i)* =‹	A(L-2)t (-y) +
A(L-2)t (y)+
0
if β ≤k(y)+k2,β ≤k(-y)+k2
if β>k(y)+k2,β ≤ (-y)J∣2
if β ≤k(y)+k2,β>k(y)+k2
if β>k(y)+k2,β>k(-y)J∣2
Il (y)+ - ɪ 11t (y)+ k2
[√ IT (y)+	J
1
Il(-y)+1∣2
1
Ii(-y)+- n ιιT(-y)+1∣2
[√ IT (-y)+ J
Γ k (y)+12 - β 一
[-k (-y)+ k2 + β
w(2)* = < -∣∣(-y)+∣∣2 + β
I k(y)+k2 - β
f β	(y)+- - β (-y)+
∣βk(y)+k2 βι∣(-y)+∣∣2
v*=)	(y)+- β k(-y¾2
βW+h - (-y)+
Iy
if β ≤k(y)+k2,β ≤k(-y)+k2
if β>k(y)+k2,β ≤k(-y)+∣∣2
if β ≤k(y)+k2,β>k(-y)+k2
if β>k(y)+k2,β>k(-y)+k2
if β ≤k(y)+k2,β ≤k(-y)+k2
if β>k(y)+k2,β ≤ (-y)+∣∣2
if β ≤k(y)+k2,β>k(-y)+k2
if β>k(y)+k2,β>k(-y)+k2
We now substitute these parameters into the primal (3) and the dual (5) to prove that strong duality
holds. For presentation simplicity, we only consider the first case, i.e,, β ≤ ∣ (y)+ ∣∣2, β ≤ ∣ (-y)+ ∣∣2
and note that the same derivations apply to the other cases as well. We first evaluate the primal
objective with these parameters as follows
*1
pL = 2
XX ( (In - 111T)A(L-2)wjL I) (l) + ɪ (l)*∖	(L)*
3 ∖ k(Is- 11lT)A(L-2)WjLT)*∣2 γj	√nQj ) + Wj
-y
2
+β∣∣w(L)II
2
1
2
(-y)+
(y)+____+ β________
"∣∣2+ β∣∣(-y)+∣∣
2
+β(∣∣(y)+l∣2+ l∣(-y)+l∣2- 2β)
2
2
=β(∣∣(y)+∣2 + l∣(-y)+∣2) - β2∙
We then evaluate the dual objective as follows
dL = - 2l∣v* - yk2 + 2llyk
(43)
(y)+
-1 β
2 β k(y)+∣
2
2
(-y)+
Il(-y)+1∣2
-y
+ 2 llyk2
2
2
=-β 2 + β(My)+l∣2 + ll(-y)+^2)∙
(44)
By (43) and (44), the set of parameters {W(LT)*, γ(LT)*, α(LT)*, W(L)*} achieves PL = dL,
therefore, strong duality holds.	□
34
Published as a conference paper at ICLR 2022
F.2 Proof of Theorem 4.2
Applying the same steps in Theorem 2.4 for the data matrix A(L-2), i.e., replacing X with A(L-2),
yields the following set of parameters
(W(L-1)* w(L)*) = ((A(L-2)tyj,(IIyjk2 -β)ej)	ifβ≤kyj∣∣2
j , j	-	otherwise
YjL-1)*#_ ι Wyj-1 ιιτ yjk2]
αjLT)I= B⅛ [*ITyj J
∀j∈ [C].
V=βky ⅛
ifβ≤kyjk2
otherwise
□
G BN after ReLU
Despite the common practice for BN, i.e., placing BN layers before activation functions, some recent
studies show that applying BN after activation function might be more effective (Chen et al., 2019).
Thus, we analyze the effects of BN when it is employed after ReLU activations.
After applying the scaling in Lemma 1.1, the primal training problem is as follows
mini
θ∈Θs 2
m1
X BNγ,α
j=1
2
+βkW(2)k1
(45)
2
In the sequel, we repeat the same analysis in Section 2.
Theorem G.1. For any data matrix X, the non-convex training problem in (45) can be equivalently
stated as the following finite-dimensional convex program
1
min 一
si ,s0i 2
P
X U0i (si - s0i) - y
i=1
2P
+ β X(ksik2 + ks0ik2) s.t.
2 i=1
(2Di- In)XViΣ-1Si,in-ι ≥ 0 市
(2Di- In)XViΣ-1si,rrLi≥0'"
(46)
provided that (I„ - ɪ 11t)D，X := Ui∑V, Ui := [Ui √ l], ri :=rank((I„ - ɪ 11t)DiX),
and si,1:ri -1 denotes all the entries of si except the last one.
Theorem G.2. Suppose n ≤ d and X is full row-rank, then an optimal solution to (45) is given as
(W(I)*,w⑵*) = (xt(y- mmyj (∣y∣2 -⑶十)JY(11=备[ky -11	yk2 .
、	i v	i	+j Lα(1) J ky∣2 L	√n1 y J
These results indicate that applying BN after ReLU leads to a completely different phenomena.
Specifically, Theorem G.1 shows that the hyperplane arrangement matrices DiX are de-meaned
and whitened via SVD, namely (In — ɪ 11T)DiX = Ui∑iVTT. This renders whitened features
Ui = [Ui ι∕√n] appearing in the convex model. This is in contrast with applying BN before the
ReLU layer, which directly operates on the data matrix X.
Theorem G.2 shows that an optimal solution can be found in closed-form in the high-dimensional
regime n ≤ d when X is full row-rank, which is typically satisfied in sample-limited scenarios.
Surprisingly, a single neuron optimizes the training cost, which corresponds to a single non-zero
block in the group `1 penalized program (46).
G.1 Proof of Theorem G.2
Applying the approach in Lemma C.1 to the following constraint
max
θ∈Θs
U2一√
T
v
35
Published as a conference paper at ICLR 2022
yields
v*= β ⅛
kyk2
Then, the rest of proof is to verify the solutions by substituting them into the primal and dual problems
as in Theorem 2.1.
□
G.2 Proof of Theorem G.1
We first focus on the dual constraint as follows
max
w(1)
γ(1)2+α(1)2=1
E -12X(Xw++2 —二
max	max
i∈[P]	θ∈Θs
(2Di -In)Xw(1) ≥0
V(InI-- 1iTL)DDXWW；；Y ⑴+α⑴ vT1
max
i∈[P]
max
qi,α,γ
α2+γ2=1
(2Di-In)XViΣi-1qi≥0
T
v
Uiqi
kq⅛
T
γ + αv
max
i∈[P]
max
si
ksik2=1
(2Di-In)XViΣ-1SLri-1≥0
U0i
Then, the rest of the proof directly follow from Theorem 2.2.
□
36