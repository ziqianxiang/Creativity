Published as a conference paper at ICLR 2022
The Effects of Invertibility on the Represen-
tational Complexity of Encoders in VAEs
Divyansh Pareek
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA, 15213
dpareek@andrew.cmu.edu
Andrej Risteski
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA, 15213
aristesk@andrew.cmu.edu
Ab stract
Training and using modern neural-network based latent-variable generative models
(like Variational Autoencoders) often require simultaneously training a generative
direction along with an inferential (encoding) direction, which approximates the
posterior distribution over the latent variables. Thus, the question arises: how
complex does the inferential model need to be, in order to be able to accurately
model the posterior distribution of a given generative model? In this paper, we
identify an important property of the generative map impacting the required size
of the encoder. We show that if the generative map is “strongly invertible" (in
a sense we suitably formalize), the inferential model need not be much more
complex. Conversely, we prove that there exist non-invertible generative maps,
for which the encoding direction needs to be exponentially larger (under standard
assumptions in computational complexity). Importantly, we do not require the
generative model to be layerwise invertible, which a lot of the related literature
assumes and is not satisfied by many architectures used in practice (e.g. convolution
and pooling based networks). As a non-invertible generator results in a distribution
supported on a low-dimensional manifold, this provides theoretical support for the
empirical wisdom that learning deep generative models is harder when data lies on
a sufficiently complex low-dimensional manifold.
1	Introduction
Many modern generative models of choice (e.g. Generative Adversarial Networks (Goodfellow
et al., 2014), Variational Autoencoders (Kingma & Welling, 2013)) are modeled as non-linear,
possibly stochastic transformations of a simple latent distribution (e.g. a standard Gaussian). A
particularly common task is modeling the inferential (encoder) direction: that is, modeling the
posterior distribution on the latents z given an observable sample x. Such a task is useful both at
train time and at test time. At train time, fitting generative models like variational autoencoders
via maximum likelihood often relies on variational methods, which require the joint training of a
generative model (i.e. generator/decoder), as well as an inference model (i.e. encoder) which models
the posterior distribution of the latent given the observables. At test time, the posterior distribution
very often has some practical use, e.g. useful, potentially interpretable feature embeddings for data
(Salimans et al., 2016; Berthelot et al., 2018), “intervening” on the latent space to change the sample
in some targeted manner (Shen et al., 2020), etc. As such, the question of the “complexity" of the
inference model (i.e. number of parameters to represent it using a neural network-based encoder) as a
function of the “complexity" of the forward model is of paramount importance, so that training is
computationally tractable:
Question: How complex does the inference (encoder) model need to be relative to the complexity of
the generative (decoder) model ?
In this paper we identify an important property of the generative direction governing the complexity
of the inference direction for variational autoencoders: bijectivity/invertibility of the mean of the
generative direction. We prove that when the mean of the generative direction is invertible, the
complexity of the inference direction is not much greater than the complexity of the generative
1
Published as a conference paper at ICLR 2022
direction. Conversely, when the mean of the generative direction is not invertible, modulo stan-
dard computational complexity conjectures from cryptography, we can exhibit instances where the
inference direction has to be much more complex.
On the mathematical level, our techniques involve a neural simulation of a Langevin random walk to
sample from the posterior of the latent variables and uncover novel connections between Langevin
diffusions and (hierarchical) deep latent Gaussians. On the lower bound side, we provide a reduction
from the existence of one-way Boolean permutations in computational complexity: that is, permuta-
tions that are easy to calculate, but hard to invert. We show that the existence of a small encoder for
non-invertible generators would allow us to design an invertor for any Boolean permutation, thus
violating the existence a one-way permutation. This is the first time such ideas have been applied to
generative models.
Note that a non-invertible generator results in a distribution supported on a lower dimensional
manifold. Thus, our results show that learning deep generative models is harder when data lies
on a sufficiently complex low-dimensional manifold, corroborating similarly flavored empirical
observations (Dai & Wipf, 2019; Arjovsky et al., 2017).
2	Our Results
The Variational Autoencoder (VAE) (Kingma & Welling, 2013) is one of the most commonly used
paradigms in generative models. It’s trained by fitting a generator which maps latent variables z
to observables x, denoted by pθ (x|z), as well as an encoder which maps the observables to the
latent space, denoted by qφ (z|x). Here φ and θ are the encoder parameters and generator parameters
respectively. Given n training samples {x(i)}in=1, the VAE objective is given by
1n
max- X Ez 〜qφ(.∣χ(i)) [lθg Pθ (x(i)|z)] - KL qφφ (Zlx(i))llP(Z))
φ, n i=1
where p(Z ) is typically chosen to be a standard Gaussian. This loss can be viewed as a variational
relaxation of the maximum likelihood objective, where the encoder qφ , in the limit of infinite
representational power, is intended to model the posterior distribution pθ (Z|x(i)).
Setup: We will consider a setting in which the data distribution itself is given by some ground-truth
generator G : Rdl → Rdo , and ask how complex (in terms of number of parameters) the encoder
needs to be (as a function of the number of parameters of G), s.t. it approximates the posterior
distribution p(Z|x) of the generator.
We will consider two standard probabilistic models for the generator/encoder respectively.
Definition 1 (Latent Gaussian). A latent Gaussian is the conditional distribution given by a stochastic
pushforward of a Gaussian distribution. That is, for latent variable Z ∈ Rdl and observable x ∈ Rdo,
for a neural network G : Rdl → Rdo and noise parameter β2, we have p(x|Z) = N (G(Z), β2Ido)
and p(Z) = N(0, Idl ).
In other words, a sample from this distribution can be generated by sampling independently
Z〜 N(0,Idl) andξ 〜N(0,β2Ido) and outputting X = G(Z) + ξ. This is a standard neural
parametrization of a generator with (scaled) identity covariance matrix, a fairly common choice in
practical implementations of VAEs (Kingma & Welling, 2013; Dai & Wipf, 2019).
We will also define a probabilistic model which is a composition of latent Gaussians (i.e. consists of
multiple stochastic layers), which is also common, particularly when modeling encoders in VAEs, as
they can model potentially non-Gaussian posteriors (Burda et al., 2015; Rezende et al., 2014):
Definition 2 (Deep Latent Gaussian). A deep latent Gaussian is the conditional distribution given
by a sequence of stochastic pushforwards of a Gaussian distribution. That is, for observable
Z0 ∈ Rd0 and latent variables {Zi ∈ Rdi }iL=1, for neural networks {Gi : Rdi-1 → Rdi}iL=1
and noise parameters {βi2}iL=1, the conditional distribution p(ZL |Z0) is a deep latent Gaussian when
p(Zi|Zi-1) = N(Gi(Zi-1), βi2Idi),∀i ∈ [L] and p(Z0) = N(0, Id0).
In other words, a deep latent Gaussian is a distribution, which can be sampled by ancestral sampling,
one layer at a time. Note that this class of distributions is convenient as a choice for an encoder
2
Published as a conference paper at ICLR 2022
in a VAE, since compositions are amenable to the reparametrization trick of Kingma & Welling
(2013)—the randomness for each of the layers can be “presampled” and appropriately transformed
(Burda et al., 2015; Rezende et al., 2014). Then, we ask the following:
Question: If a VAE generator is modeled as a latent Gaussian (that is, p(x|z) ≡ N (G(z), β2I)), s.t.
the corresponding G has at most N parameters, and we wish to approximate the posterior p(z|x) by
a deep latent Gaussian s.t. the total size of the networks in it have at most N0 parameters, how large
must N0 be as function of N?
We will work in the setting dl = do = d, and prove a dichotomy based on the invertibil-
ity of G: namely, if G : Rd → Rd is bijective, and β ≤ O ( --------------------1 j, the Poste-
d1.5 log d/
rior p(z|x) can be -aPProximated in total variation distance by a deeP latent Gaussian of size
N0 = O (N ∙ poly(d, 1∕β, 1/e)). Thus, if the neural network G is invertible, and for a fixed E and
a small-enough variance term β2, we can aPProximate the Posterior with a deeP latent Gaussian
Polynomially larger than G. On the other hand, if G is not bijective, if one-way-functions exist (a
widely believed comPutational comPlexity conjecture), we will show there exists a VAE generator
G : Rd → Rd of size Polynomial in d, for which the Posteriorp(z|x) cannot be aPProximated in total
variation distance for even an inverse polynomial fraction of inPuts x, unless the inferential network
is of size exponential in d.
Remark 1: Note that it is in fact sufficient to investigate only the case where dl = do = d. Let us
elaborate why:
•	For Theorem 1, a function G : Rdl → Rdo can be bijective, LiPschitz and strongly invertible (i.e.
satisfies AssumPtions 1 and 2) only if dl = do , so Theorem 1 (uPPer bound) would not make any
sense unless the dimensions are equal. (Note: there are bijective maPs between, say, R and R2,
e.g. sPace-filling curves, but these cannot be LiPschitz, as LiPschitz maPs Preserve the Hausdorff
dimension, and the Hausdorff dimensions of R and R2 are 1 and 2 resPectively).
•	Theorem 2 isa lower bound - meaning we aim to exhibit an example of a hard instance when G
is not bijective. Having exhibited a hard instance for the dl = do = d case, it is trivially easy to
construct a hard instance in the theorem where the outPut dimension is larger—which is the more
common setting in practice. To do so, consider a circuit C : {±1}dl → {±1}do, which equals to C,
the one-way-circuit from Conjecture 1 on the first dl coordinates, and is equal to 1 (i.e. is constant)
on the last do - dl coordinates. Then C is one-way too—since the last do - dl values are just fixed
to 1, if we can invert it, we can invert C too. The reduction in the proof of Theorem 2 then can just
be performed with C instead, giving a lower bound instance for the dl < do case.
2.1	Upper bounds for bijective generators
We first lay out the assumptions on the map G. The first is a quantitative characterization of bijectivity;
and the second requires upper bounds on the derivatives of G up to order 3. We also have a centering
assumption. We state these below.
Assumption 1 (Strong invertibility). We will assume that the latent and observable spaces have the
same dimension (denoted d), and G : Rd → Rd is bijective. Moreover, we will assume there exists a
positive Constant m > 0 such that ∀zι, z2 ∈ Rd, ∣∣G(zι) — G(z2)k ≥ m ∙ ∣∣zι 一 Z2∣∣.
Remark 2: This is a stronger quantitative version of invertibility. Furthermore, the infinitesimal
version of this condition (i.e. ∣zι — z21∣ → 0) implies that the smallest magnitude of the singular values
of the Jacobian at any point is lower bounded by m, that is ∀z ∈ Rd, min ∣σi( Jg(z))∣ ≥ m > 0.
i∈[d]
Since m is strictly positive, this in particular means that the Jacobian is full rank everywhere.
Remark 3: Note, we do not require that G is layerwise invertible (i.e. that the each map from one
layer to the next is invertible) 一 if that is the case, at least in the limit β → 0, the existence of an
inference decoder of comparable size to G is rather obvious: we simply invert each layer one at a time.
This is important, as many architectures based on convolutions perform operations which increase
the dimension (i.e. map from a lower to a higher dimensional space), followed by pooling (which
decrease the dimension). Nevertheless, it has been observed that these architectures are invertible in
3
Published as a conference paper at ICLR 2022
practice—Lipton & Tripathi (2017) manage to get almost 100% success at inverting an off-the-shelf
trained model—thus justifying this assumption.
Assumption 2 (Smoothness). There exists a finite positive constant M > 0 such that :
∀z1, z2 ∈ Rd, k G(ZI)- G(Zz)Il ≤ M ∙ IlzI- z2 k
Moreover, we will assume that G has continuous partial derivatives up to order 3 at every z ∈ Rd
and the derivatives are bounded by finite positive constants M2 and M3 as
∀z ∈ Rd, IIv2G(Z)IIop ≤ M2 < ∞, IIv3G(Z)IIop ≤ M3 < ∞
Remark 4: This is a mild assumption, stating that the map G is smooth to third order. The
infinitesimal version of this means that the largest magnitude of the singular values of the Jacobian at
any point is upper bounded by M, that is ∀z ∈ Rd, max ∣σi(JG(z))∣ = k JG(z)kop ≤ M < ∞.
i∈[d]
Remark 5: A neural network with activation function σ will satisfy this assumption when σ : R → R
is Lipschitz, and max。∣σ0(a) | & max。∣σ00(a) | are finite.
Assumption 3 (Centering). The map G : Rd → Rd satisfies G(0) = 0.
Remark 6: This assumption is for convenience of stating the bounds—we effectively need the “range”
of majority of the samples x under the distribution of the generator. All the results can be easily
restated by including a dependence on IG(0)I.
Our main result is then stated below. Throughout, the O(.) notation hides dependence on the map
constants, namely m, M, M2, M3. We will denote by dTV(p, q) the total variation distance between
the distributions p, q.
Theorem 1 (Main, invertible generator). Consider a VAE generator given by a latent Gaussian with
μ = 0, Σ = I, noise parameter β2 and generator G : Rd → Rd satisfying Assumptions 1 and 2,
which has N parameters and a differentiable activation function σ. Then, for
β ≤O I -1r= I	(1)
∖d1.5 V log dM
there exists a deep latent Gaussian with N0 = O (N ∙ poly(d, β, ɪ)) parameters and activationfunc-
tions {σ, σ0, ρ} and a neural network φ with O(N 0) parameters and activation functions {σ, σ0, ρ},
where ρ(x) = x2, such that with probability 1 - exp(-O (d)) over a sample x from the VAE generator,
φ(x) produces values for the parameters of the deep latent Gaussian such that the distribution q(Z|x)
it specifies satisfies dTV (q(Z|x), p(Z|x)) ≤ .
Remark 7: Having an encoder q(Z|x) with parameters produced by a neural network taking x as
input is fairly standard in practice—it is known as amortized variational inference.(Mnih & Gregor,
2014; Kingma & Welling, 2013; Rezende et al., 2014)
Remark 8: The addition of ρ in the activation functions is for convenience of stating the bound.
Using usual techniques in universal approximation it can be simulated using any other smooth
activation, see Appendix G.
2.2	Lower bounds for non-bijective Generators
We now discuss the case when the generative map G is not bijective, showing an instance such that
no small encoder corresponding to the posterior exists. The lower bound will be based on a reduction
from the existence of one-way functions—a standard complexity assumption in theoretical computer
science (more concretely, cryptography). Precisely, we will start with the following form of the
one-way-function conjecture:
Conjecture 1 (Existence of one-way permutations (Katz & Lindell, 2020)). There exists a bijection
f : {-1, 1}d → {-1, 1}d computable by a Boolean circuit C : {-1, 1}d → {-1, 1}d of size poly(d),
butfor every T(d) = poly(d) and e(d) = Poly⑷ and circuit C0 : { —1,1}d → { -1,1}d ofsize T(d)
it holds Pr2~{±i}d [C0(C(Z)) = z] ≤ e(d).
4
Published as a conference paper at ICLR 2022
In other words, there is a circuit of size polynomial in the input, s.t. for every polynomially sized
invertor circuit (the two polynomials need not be the same—the invertor can be much larger, so long
as it’s polynomial), the invertor circuit succeeds on at most an inverse polynomial fraction of the
inputs. Assuming this Conjecture, we show that there exist generators that do not have small encoders
that accurately represent the posterior for most points x. Namely:
Theorem 2 (Main, non-invertible generator). IfConjecture 1 holds, there exists a VAE generator G :
Rd → Rd with size poly (d) and activation functions {sgn, min, max}, s.t.,for every β = o(1∕√d),
every T(d) = poly(d) and every (d) = 1/poly(d), any encoder E that can be represented by a
deep latent Gaussian with networks that have total number of parameters bounded by T (d), weights
bounded by W, activation functions that are L-Lipschitz, and node outputs bounded by M with
probability 1 - exp(-d) over a sample x from G and L, M, W = o(exp(poly(d))), we have:
Prx〜G dtv(E(z∣x),p(z∣x)) ≤ ι0
≤ (d)
Thus, we show the existence of a generator for which no encoder of polynomial size reasonably
approximates the posterior for even an inverse-polynomial fraction of the samples x (under the
distribution of the generator).
Remark 9: The generator G, though mapping from Rd → Rd will be highly non-invertible. Perhaps
counterintuitively, Conjecture 1 applies to bijections—though, the point will be that G will be
simulating a Boolean circuit, and in the process will give the same output on many inputs (more
precisely, it will only depend on the sign of the inputs, rather than their values).
Remark 10: The choice of activation functions {sgn, min, max} is for convenience of stating the
theorem. Using standard universal approximation results, similar results can be stated with other
activation functions, see Appendix G.
Remark 11: The restrictions on the Lipschitzness of the activations, bounds of the weights and node
outputs of E are extremely mild—as they are allowed to be potentially exponential in d—considering
that even writing down a natural number in binary requires logarithmic number of digits.
3	Related Work
On the empirical side, the impact of impoverished variational posteriors in VAEs (in particular,
modeling the encoder as a Gaussian) has long been conjectured as one of the (several) reasons for
the fuzzy nature of samples in trained VAEs. Zhao et al. (2017) provide recent evidence towards
this conjecture. Invertibility of generative models in general (VAEs, GANs and normalizing flows),
both as it relates to the hardness of fitting the model, and as it relates to the usefulness of having
an invertible model, has been studied quite a bit: Lipton & Tripathi (2017) show that for off-the-
shelf trained GANs, they can invert them with near-100% success rate, despite the model not being
encouraged to be invertible during training; Dai & Wipf (2019) propose an alternate training algorithm
for VAEs that tries to remedy algorithmic problems during training VAEs when data lies on a lower-
dimensional manifold; Behrmann et al. (2020) show that trained normalizing flows, while being by
design invertible, are just barely so — the learned models are extremely close to being singular.
On the theoretical side, the most closely relevant work is Lei et al. (2019). They provide an algorithm
for inverting GAN generators with random weights and expanding layers. Their algorithm is layerwise
— that is to say, each of the layers in their networks is invertible, and they invert the layers one at
a time. This is distinctly not satisfied by architectures used in practice, which expand and shrink
— a typical example are convolutional architectures based on convolutions and pooling. The same
paper also shows NP-hardness of inverting a general GAN, but crucially they assume the network
G is part of the input (their proof does not work otherwise). Our lower bound can be viewed as
a “non-uniform complexity” (i.e. circuit complexity) analogue of this, since we are looking for a
small neural network E, as opposed to an efficient algorithm; crucially, however G is not part of
the input (i.e. G can be preprocessed for an unlimited amount of time). Hand & Voroninski (2018)
provide similar guarantees for inverting GAN generators with random weights that satisfy layerwise
invertibility, albeit via non-convex optimization of a certain objective.
5
Published as a conference paper at ICLR 2022
4	Proof Overview
Overview of Theorem 1 Recall, given a generator G and noise level β2 , we wish to provide a deep
latent Gaussian (Definition 2) that approximates a sample from the distribution p(z|x), with high
probability over the draws of x from the underlying density. To show Theorem 1, there are three
main ingredients:
1.	We show that gradient descent on the function f (Z) = 11lG(Z) - χ∣∣2, run for O(d∕β2) number
of steps, recovers a point Zinit close to argmaXz logp(z|x) - i.e. the mode of the posterior.
2.	We show that Langevin diffusion, a Markov process that has p(Z|x) as it’s stationary distribution,
started close to the mode of the posterior via Zinit computed by GD above, run for O(log(d∕))
amount of time; returns a sample from a distribution close top(Z|x) in total variation distance.
3.	We show that the above two steps can be “simulated" by a deep latent Gaussian of size at most
poly(d, 1, ɪ) larger than the original network, by inductively showing that each next iterate in the
gradient descent and Markov chain can be represented by a neural network of size comparable to
the generator G.
Part 1 is formalized in Lemma 1. The main observation is that the gradient of the loss will have
the form Vf (z) = JG(Z)T(G(Z) - x). By Strong Invertibility (Assumption 1), JG(Z) will be
an invertible matrix—therefore, any stationary points will necessarily be the (unique) point Z s.t.
G(Z) = x. This in turn, for small β, will be close to argmaxz logp(Z|x).
Part 2 is formalized in Lemma 2, and is technically most involved. As β→ 0, the distribution p(Z|x)
concentrates more around it’s mode, and approaches a Gaussian centered at it. More precisely, we
show that starting at Zinit, there is a region D such that the probability of Langevin diffusion leaving
this region is small (Lemma 13) and the distribution restricted to D is log-concave (Lemma 4). As a
consequence of the latter claim, reflected Langevin diffusion (a “restricted” version of the standard
Langevin diffusion) converges rapidly to p(Z|x) restricted to D. As a consequence of the former
claim, the reflected and standard Langevin diffusion are close in the total variation sense.
Finally, Part 3 is based on the observation that we can approximately simulate gradient descent,
as well as the Langevin dynamics using a deep latent Gaussian (Definition 2). Proceeding to
gradient descent, the key observation is that if we give a neural network Z as the input, the point
Z - ηJG(Z)T (G(Z) - x) can be calculated by a network of size comparable to the size of a neural
network representing JG; the Jacobian of a neural network. And in turn, the Jacobian network can be
represented by a neural network at most a factor O(d) bigger than the neural network representing
G—this is by noting that each partial derivative of G can be represented by a neural network of size
comparable to G, essentially by the backpropagation algorithm (Lemma 5).
The idea to simulate Langevin dynamics is similar: the Euler-Mariyama discretization of the corre-
sponding stochastic differential equation (with precision h) is a discrete-time Markov chain, s.t. the
updates take the form
Z — Z — hVL(z) + √2hξt, ξ 〜N(0, I)	(2)
where L the unnormalized log-likelihood of the posteriorp(Z|x) (equation 3 below). Same as is the
case for gradient descent, a sample following the distribution of these updates can be implemented by
a latent Gaussian, with a neural net of size at most O(d) bigger than the size of G—the randomness
being handled by Gaussian noise that the latent Gaussian has its disposal.
Overview of Theorem 2 The proof of Theorem 2 proceeds by a reduction to Conjecture 1. We
consider G(Z) = C(sgn(Z)), where C is a circuit satisfying the assumptions of Conjecture 1.
We show that we can use an encoder q(Z|x) that works with good probability over x (wrt to the
distribution of G) to construct a circuit C0 that violates Conjecture 1. The main idea is as follows: the
samples x will, with high probability, lie near points on the hypercube {±1}d. For such points, the
posterior p(z∣x) will concentrate around C-1(x), so taking the sign of a sample from q(Z∣χ) should,
with good probability, produce an inverse of sgn(x).
Simulating this construction using a Boolean circuit, we can get a “randomized” circuit that inverts C.
Producing a deterministic circuit follows ideas from complexity theory, where we show that given a
randomized circuit, there exists a single string of randomness that works for all inputs, if the success
probability of the algorithm is high enough.
6
Published as a conference paper at ICLR 2022
5 Main ingredients: Theorem 1
First, we introduce a few pieces of notation that will be used in multiple places in the proofs. We
will also omit subscripting quantities with x to ease notation whenever x is clear from context. For
notational convenience, we will denote the unnormalized log-likelihood of p(z|x) as L : Rd → R,
s.t.
L(Z) = I I"'"2 + gkG(Z)-Xk2)	⑶
Note, by Bayes rule, p(z∣x) H p(x∣z) p(z), so p(z∣x) H e-L(Z). Note this is a well-defined
probability distribution since ∀z ∈ Rd, L(Z) ≥ 1 ∣∣zk2, therefore the partition function is finite.
Further, we will denote the inverse of X as Z, namely:
Z := arg min ∣∣G(z) 一 x∣2	(4)
z∈Rd
Note since G is invertible, Z is uniquely defined, and it,s such that G (Z) = x. Crucially due to
this, the function L(Z) is strongly convex in some region around the point Z - which can be seen
by evaluating V2L(Z) from equation 3. These properties are convenient - that is why we base our
analysis on this inverse point.
5.1	Part 1: Convergence of gradient descent
First, we show that gradient descent on the function f (z) = 2IlG(Z) — χ∣2, started at the origin,
converges to a point close to Z in a bounded number of steps. Precisely:
Lemma 1 (Convergence of Gradient Descent to the Inverse). When G satisfies Assumptions 1 and
2,	with probability 1 - exp(-O(d)) over the choice of X, running gradient descent on f(Z) =
1 ∣∣G(z) — x∣2 with the starting point Z⑼:=0 and learning rate η := Qq where Q = O(d), for
S = O(d2) steps converges to a point Z(S), such that ∣z(S) 一 Zk ≤ δ.
As mentioned in Section 4, the idea is that while f is non-convex, its unique stationary point is Z, as
Vf(Z) = JG(Z)T (G(Z) 一 X) and JG(Z) is an invertible matrix ∀Z ∈ Rd from Assumption 1. The
proof essentially proceeds by a slight tweak on the usual descent lemma (Nesterov, 2003), due to the
fact that the Hessian’s norm is not bounded in all ofRd and is given in Appendix B.
5.2	Part 2: Fast mixing of Langevin diffusion
We first review some basic definitions and notation. Recall, Langevin diffusion is described by a
stochastic differential equation (SDE):
Definition 3 (Langevin Diffusion). Langevin Diffusion is a continuous-time Markov process {Zt :
t ∈ R+} defined by the stochastic differential equation
dzt = -Vg(Zt)dt + √2dBt t ≥ 0	(5)
where g : Rd → R, and {Bt : t ∈ R+ } is a Brownian motion in Rd with covariance matrix Id.
Under mild regularity conditions, the stationary distribution of this process is P : Rd → R+ such
that P(Z) H e-g(z).
For the reader unfamiliar with SDEs, the above process can be interpreted as the limit as h → 0 of
the Markov chain
Z 一 Z - hVL(Z)dt + √2hξ, ξ 〜N(0,I)	(6)
The convergence time of the chain to stationarity is closely tied to the convexity properties of L,
through functional-analytic tools like POinCare inequalities and Log-Sobolev inequalities (Bakry &
Emery, 1985). We show that the distribution p(z∣x) can be sampled ^-approximately in total variation
distance by running Langevin dynamics for O(log(d/)) number of steps. Namely:
Lemma 2 (Sampling Complexity for general G). Let β = O ɑɪ 5√——=).Then, with probability
1 - exp(-O(d)) over the choice of X, ifwe initialize Langevin diffusion equation 6 at Z0 which
satisfies ∣∣Z0 — Z∣ ≤ Ω (β ∙ Jdlog d),forsome T = O(京 log ɪ) we have dTV(PT,p(z∣x)) ≤ e/2
where Pt is the density corresponding to Zt.
7
Published as a conference paper at ICLR 2022
Note, the lemma requires the variance of the latent Gaussian β to be sufficiently small. This intuitively
ensures the posterior p(Z|x) is somewhat concentrated around its mode — and we will show that
when this happens, in some region around the mode, the posterior is close to a Gaussian. The proof
of this has two main ingredients: (1) Zt stays in a region D around Z with high probability; (2)
The distribution p(Z|x), when restricted to D is log-concave, so a “projected” version of Langevin
diffusion (formally called reflected Langevin diffusion, see Definition 4) mixes fast to this restricted
version ofp(Z|x).
The first part is formalized as Lemma 13 in Appendix C. The main mathematical tool is a stochastic
differential equation characterizing the change of the distance from Z. Namely, we show:
Lemma 3 (Change of distance from Z). Let n(Z):= 1 ∣∣z 一 Z∣∣2. If Z follows the Langevin SDE
corresponding to Zt, then for Z ∈ D as defined in Lemma 13 and β ≤ β0 as defined above, it holds
that
2
dn(Z)≤ 一β^ ∙ n(Z)dt + (I∣Z∣∣2 + d) dt + 2pn(Z)dBt
This SDE is analyzed through the theory of Cox-Ingersoll-Ross (CoX et al., 2005) processes, which
are SDEs of the type dMt = (a-bMt)dt+c√Mt dBt, where a, b,c are positive constants. Intuitively,
this SDE includes an ‘attraction term, -bMtdt which becomes stronger as we increase Mt (in our
case, as we move away from the center of the region z), and a diffusion term c√MtdBt due to the
injection of noise. The term (kZ∣∣2 + d)dt comes from the heuristic It6 calculus equality dB2 = dt.
2
The “attraction term” 一mη(z)dt comes from the fact that L(Z) is approximately strongly convex
near Z. (Intuitively, this means the Langevin process is attracted towards Z.)
To handle the second part, we show the distribution p(z|x) restricted to D is log-concave:
Lemma 4 (Strong convexity of L over D). For all Z ∈ D, V2L(z)占 I.
Since the set D is a l2 ball (which is of course convex), we can show an alternate Markov process
which always remains inside D, called the reflected Langevin diffusion mixes fast.
Definition 4 (Reflected Langevin diffusion, (Lions & Sznitman, 1984; Saisho, 1987)). For a suffi-
ciently regular region D ⊆ Rd, there exists a measure U supported on ∂D, such that the continuous-
time diffusion process {yt : t ∈ R+ } starting with y0 ∈ D defined by the stochastic differential
equation:
dyt = -Vg(yt)dt + √2dBt + VtU(yt)dt	t ≥ 0	(7)
has as stationary measure P : D → R+ such that P(y) H e-g(y). Here g : Rd → R, {Bt : t ∈ R+}
is a Brownian motion in Rd with covariance matrix Id, and νt is an outer normal unit vector to D.
For readers unfamiliar with SDEs, the above process can be interpreted as the limit as h → 0 of
∀k ≥	0,	yk+1	=∏d	(yk	- hVg(yk)	+ √2hξk)	ξk	〜N(0,Id)	(8)
where ΠD is the projection onto the set D.
For total variationa distance, the mixing time in total variation distance of this diffusion process was
characterized by Bubeck et al. (2018):
Theorem 3 (Mixing for log-concave distributions over convex set, Bubeck et al. (2018): Proposition
2). Suppose a measure p : Rd → R+ of the form p(x) H e-g(x) is supported over S ⊆ Rd of
diameter R which is convex, and ∀x ∈ S, V2g & 0. Furthermore, let x0 ∈ S. Then, ifpt is the
distribution after running equation 7 for time t starting with a Dirac distribution at x0, we have
TV(pt,p) . e-2R2
Since the set D in Lemma 13 is an l2 ball (which is of course convex), it suffices to show that L is
convex on D—which is a straightforward calculation. The details of this are again in Appendix C.
5.3	Part 3: Simulation by neural networks
Finally, we show that the operations needed for gradient descent as well as Langevin sampling can be
implemented by a deep latent Gaussian. Turning the gradient descent first, we need to “simulate” by a
8
Published as a conference paper at ICLR 2022
neural net the operation Z — Z - ηJG(z)T (G(Z) - x) roughly O(d∕β2) number of times. We prove
that there is a neural network that given an input z, outputs z - ηJG(z)T (G(z) - x). The main ideas
for this are that: (1) If G is a neural network with N parameters, each coordinate of JG(Z) can be
represented as a neural network with O(N) parameters. (2) If f, g are two neural networks with N
and M parameters respectively, f + g and fg can be written as neural networks with O(M + N)
parameters.
The first part essentially follows by implementing backpropagation with a neural network:
Lemma 5 (Backpropagation Lemma, Rumelhart et al. (1986)). Given a neural network G : Rd → R
of depth l and size N, there is a neural network of size O(N + l) which calculates the gradient
∂G∕∂Zi for i ∈ [d].
For the second part, addition is trivially implemented by “joining” the two neural networks and
adding one extra node to calculate the sum of their output. For multiplication, the claim follows by
noting that Xy = 1 (X + y)2 - 4 (X — y)2. Using the square activation function, this can be easily
computed by a 2-layer network.
Proceeding to the part about simulating Langevin diffusion, we first proceed to create a discrete-time
random process that closely tracks the diffusion. The intuition for this was already mentioned in
Section 4— the SDE describing Langevin can be viewed as the limit h → 0 of
Z — Z - h Z + 2j JG(Z)T (G(Z) - x) + √2hξ, ξ 〜N(0,I)
β2
It can be readily seen that the drift part, that is 表 JG(Z)T (G(Z) - x) can be calculated by a neural
net via the same token as in the case of the gradient updates. Thus, the only difference is in the
injection of Gaussian noise. However, each layer Gi in a deep latent Gaussian exactly takes a
Gaussian sample ξ and outputs Gi(Z) + γξ - so each step of the above discretization of Langevin
can be exactly implement by a layer in a deep latent Gaussian. The details are in Appendix D.
6	Main ingredients: Theorem 2
We prove Theorem 2 proceeds by a reduction to Conjecture 1. Namely, ifC is a Boolean circuit satisfy-
ing the one-way-function assumptions in Conjecture 1, we consider G(Z) = C(sgn(Z)). Suppose there
is a deep latent Gaussian of size T(n) with corresponding distribution q(Z|x), s.t. with probability at
least (d) over the choice ofx (wrt to the distribution of G), it satisfiesdTV(p(Z|x), q(Z|x)) ≤ 1∕10.
We will how to construct from the encoder a circuit C0 that violates Conjecture 1. Namely, consider a
X constructed by sampling Z 〜{±1}d uniformly at random, and outputting X = C(Z). To violate
Conjecture 1, we'd like to find C-1(X), using the encoder E.
We first note that X = X + ξ, ξ 〜N(0, β2Id) is distributed according to the distribution of the
generator G. Moreover, we can show that with probability at least (d)∕2, it will be the case that
X is s.t. bothdTV(p(Z|X), q(Z|X)) ≤ 1∕10 and X is very close to a point on the discrete hypercube
{±1}d. The reason for this is that the samples X themselves for small enough β with high probability
are close to a point on the hypercube. For such points, it will be the case that q(Z|X) is highly
concentrated around C-1(X), thus returning the sign of a sample will with high probability invert X.
By simulating this process using a Boolean circuit, we get a “randomized” circuit (that is, a circuit
using randomness) that succeeds at inverting C with inverse polynomial success. To finally produce a
deterministic Boolean circuit that violates Conjecture 1, we use a standard trick in complexity theory:
boosting the success probability by generating multiple samples together with the assumption on the
weights and Lipschitzness of E. For details, refer to Appendix E.
7	Conclusion
In this paper we initiated the first formal study of the representational complexity of the good encoders
in VAEs. We proved the following dichotomy: generators with invertible means give rise to posteriors
that can be approximated by an encoders not much larger than the generator. On the other hand, for
non-invertible generators, the corresponding encoder may need to be exponentially larger. We hope
our work will stimulate research of other data distribution properties that render inference tractable.
9
Published as a conference paper at ICLR 2022
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
DominiqUe Bakry and Michel Emery. Diffusions hypercontractives. In Seminaire de Probabilites
XIX 1983/84, pp. 177-206. Springer, 1985.
Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Jorn-Henrik Jacobsen. Un-
derstanding and mitigating exploding inverses in invertible neural networks. arXiv preprint
arXiv:2006.09347, 2020.
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving
interpolation in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543,
2018.
Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with
projected langevin monte carlo. Discrete & Computational Geometry, 59(4):757-783, 2018.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
John C Cox, Jonathan E Ingersoll Jr, and Stephen A Ross. A theory of the term structure of interest
rates. In Theory of Valuation, pp. 129-164. World Scientific, 2005.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019.
Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.
Arnak S Dalalyan. Further and stronger analogy between sampling and optimization: Langevin
monte carlo and gradient descent. arXiv preprint arXiv:1704.04752, 2017.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. In Conference On Learning Theory, pp. 970-978. PMLR, 2018.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Nobuyuki Ikeda and Shinzo Watanabe. A comparison theorem for solutions of stochastic differential
equations and its applications. Osaka Journal of Mathematics, 14(3):619-633, 1977.
Jonathan Katz and Yehuda Lindell. Introduction to modern cryptography. CRC press, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. arXiv preprint arXiv:1906.07437, 2019.
Lek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach. In 1st IEEE
International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2005.,
pp. 129-132. IEEE, 2005.
Pierre-Louis Lions and Alain-Sol Sznitman. Stochastic differential equations with reflecting boundary
conditions. Communications on Pure and Applied Mathematics, 37(4):511-537, 1984.
Zachary C Lipton and Subarna Tripathi. Precise recovery of latent vectors from generative adversarial
networks. arXiv preprint arXiv:1702.04782, 2017.
10
Published as a conference paper at ICLR 2022
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
International Conference on Machine Learning,pp. 1791-1799. PMLR, 2014.
Ankur Moitra and Andrej Risteski. Fast convergence for langevin diffusion with matrix manifold
structure. arXiv preprint arXiv:2002.05576, 2020.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In International conference on machine learning,
pp. 1278-1286. PMLR, 2014.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. nature, 323(6088):533-536, 1986.
Yasumasa Saisho. Stochastic differential equations for multi-dimensional domain with reflecting
boundary. Probability Theory and Related Fields, 74(3):455-477, 1987.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems, 29:
2234-2242, 2016.
Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9243-9252, 2020.
Hava T Siegelmann. Neural networks and analog computation: beyond the Turing limit. Springer
Science & Business Media, 2012.
Matus Telgarsky. Neural networks and rational functions. In International Conference on Machine
Learning, pp. 3387-3393. PMLR, 2017.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational
autoencoding models. arXiv preprint arXiv:1702.08658, 2017.
11
Published as a conference paper at ICLR 2022
A Technical Definitions
In this section, we state several definitions and standard lemmas, to cover some technical prerequisites
for the proof.
We first recall a few definitions and lemmas about two notions of distance between probabilities we
will use extensively, total variation and χ2 distance.
Definition 5 (Total variation distance). The total variation distance between two distributions P, Q is
defined as
dTV(P, Q) := sup |P (A) - Q(A)|
A
Definition 6 (χ2 distance). The χ2 distance between two distributions P, Q, s.t. P is absolutely
continuous with respect to Q, is defined as
χ2(P,Q) := Eq (P - 1)
The following two lemmas about the total variation and χ2 distance are standard:
Lemma 6 (Coupling Lemma). Let P, Q : Ω → R be two distributions, and C : Ω室2 → R be any
coupling of P, Q. Then, if (X, X0) are random variables following the distribution c, we have:
dTV(P, Q) ≤ Pr[X 6=X0]
Lemma 7 (Inequality between TV and χ2). Let P, Q be probability measures, s.t. P is absolutely
continuous with respect to Q. We then have:
dtv(P)Q) ≤ 2 pχ2 (P,Q)
We will be heavily using notions from continuous-time Markov Processes. In particular:
Definition 7 (Markov semigroup). We say that a family of functions {Pt (x, y)}t≥0 on a state space
Ω is a Markov semigroup if Pt(x, ∙) is a distribution on Ω and
Prt+s
(x, y) =
Jω
Pt(x, z)Ps (z, y)dz
for all x,y ∈ Ω and s,t ≥ 0.
Definition 8 (Continuous time Markov processes). A continuous time Markov process (Xt)t≥0
on state space Ω is defined by a Markov semigroup {Pt(χ, y)}t≥o as follows. For any measurable
A ⊆ Ω
Pr(Xs+t ∈ A)
Pt(x, y)dy
A
:= Pt(x, A)
Moreover Pt can be thought of as acting on a function g as
(Ptg)(x)
EPt(χ,∙)[g(y)] = /
Jω
g(y)Pt (x, y)dy
Finally we say that p(x) is a stationary distribution if X。〜P implies that Xt 〜P for all t.
With this in place, we can finally define the Langevin diffusion Markov process:
Definition 9 (Langevin Diffusion, Definition 3 restated). Langevin Diffusion is a continuous-time
Markov process {zt : t ∈ R+ } defined by the stochastic differential equation
dzt = -Vg(zt)dt + √2dBt t ≥ 0	(9)
where g : Rd → R, and {Bt : t ∈ R+ } is a Brownian motion in Rd with covariance matrix Id.
Under mild regularity conditions, the stationary distribution of this process is P : Rd → R+ such
that P(z) H e-g(z).
It will also be useful to have a definition of a diffusion process that only stays within some support D,
and thus has a stationary distribution supported on D. Specifically:
12
Published as a conference paper at ICLR 2022
Definition 10 (Reflected Langevin diffusion, (Lions & Sznitman, 1984; Saisho, 1987), Definition
4 restated). For a sufficiently regular region D ⊆ Rd, there exists a measure U supported on ∂D,
such that the continuous-time diffusion process {yt : t ∈ R+ } starting with y0 ∈ D defined by the
stochastic differential equation:
dyt = -yg(yt)dt + √2dBt + VtU(yt)dt	t ≥ 0	(10)
has as stationary measure P : D → R+ such that P(y) H e-g(y). Here g : Rd → R, {Bt : t ∈ R+}
is a Brownian motion in Rd with covariance matrix Id, and νt is an outer normal unit vector to D.
Towards introducing quantities governing the mixing time of a continuous-time Markov process, we
introduce
Definition 11. The generator L of the Markov Process is defined (for appropriately restricted
functionals g) as
Lg
lim Pg-
t→0	t
Moreover if p is the unique stationary distribution, the corresponding Dirichlet form and variance are
EM (g, h) = -Ep hg , Lhi and Varp (g ) = Ep (g - Epg)
respectively. We will use the shorthand E(g) := E(g, g).
A quantity of particular interest when analyzing the mixing time of Markov processes is the Poincare
constant:
Definition 12 (Poincare Inequality). A continuous Markov process satisfies a Poincare inequality
with constant C if for all functions g such that EM(g) is defined (finite), it holds that:
EM(g) ≥ C ∙ VarP(g)
where P (in the subscript) is the stationary distribution of the Markov process. We will abuse notation,
and for such a Markov process with stationary distribution P, denote by Cpc(P ) the smallest C such
that above POinCare inequality is satisfied; and call it the Poincare ConStant of P.
The relationship between the Poincare constant and the mixing time of a random walk (in particular,
the one in Definition 4) is given by the following (standard) lemma:
Lemma 8 (Mixing in χ2 from Poincare). Let Zt follow the stochastic differential equation ofreflected
Langevin diffusion (Definition 4) with stationary measure P. Let Pt be the density of zZt and let P0 be
absolutely continuous with respect to the Lebesgue measure. Then:
(1)	IfP0 is supported on D, Pt is supported on D, ∀t > 0
(2)	X2(Pt,P) ≤ e-t/CPcX2(P0,P)
Finally, we will also need a discretized version of the continuous-time Langevin diffusion.
Definition 13 (Euler-Mariyama Discretization for the Langevin Diffusion). For a potential g : Rd →
R, the Euler-Mariyama discretization of the continuous Langevin diffusion with discretization level
h is the discrete-time random walk
∀k ≥	0,	zk+ι	=	Zk	— 2(zk)	+	√2hξk	ξk	~N(0,Id)	(11)
We will also associate with this discrete-time process a stochastic differential equation
d^t = -Vg(zbt∕hch)dt + √2dBt	(12)
which has the property that Zkh and Zk follow the same distribution for all k ≥ 0, if z0 and Z0 follow
the same distribution.
Note the equivalence between equation 11 and equation 12 follows by noting that the drift in the
interval [th, (t + 1)h] in equation 12 is constant - so We only need to integrate the Brownian motion
for time h.
We can also consider the discretized version of the reflected Langevin diffusion, which looks like
projected gradient descent with Gaussian noise Bubeck et al. (2018).
13
Published as a conference paper at ICLR 2022
Definition 14 (Discretization for the Reflected Langevin Diffusion). For a potential g : Rd → R, the
Euler-Mariyama discretization of the reflected Langevin diffusion with discretization level h is the
discrete-time random walk
∀k ≥ O, yk+1 = ∏D (vk - h^g(yk) + √2hξk)	ξk 〜N(0,Id)	(13)
where ΠD is the projection onto the set D.
We can also associate with this discrete-time process a stochastic differential equation
dyt = -k g(ybt∕h∖h)dt + √2dBt + VtU (yt )dt	(14)
for a measure U supported on ∂D, which has the property that ykh and yk follow the same distribution
for all k ≥ 0, if y° and yo follow the same distribution.
B Proof of Lemma 1: Convergence of Gradient Descent to Z
First, we state the tweak on the usual descent lemma (Nesterov, 2003), due to the fact that the
Hessian’s norm is not bounded in all of Rd. We only need to identify a region A containing the
starting point z(0) such that the function’s value at points outside A are guaranteed to be more than
the function value at z(0) . Namely:
Lemma 9 (Adjusted Descent Lemma). Let f : Rd → R be a twice differentiable function. For a
region A, let it hold that ∀z ∈ A, kV2 f (Z)IIop ≤ Q < ∞. Further, let ∀z ∈ Rd∖A, f (Z) ≥ f(z(0))
for a known point z(0) ∈ A. Then the gradient descent iterates starting with z(0) with the learning
rate η :二= satisfy
f(z(s+1)) ≤ f(z(s)) - 2QkVf(z(S))k2
s≥0
We first state the well-known descent lemma.
Lemma 10 (Descent Lemma, Eq 1.2.13 in (Nesterov, 2003)). Let f be a twice differentiable function,
and at a point Z, the hessian satisfies ∣∣V2f (Z)Ilop ≤ Q < ∞. Then with the learning rate η :二=,
the next gradient descent iterate Z+ satisfies
f(z+) ≤ f(z)- 2QIVf(z)k2
Using this, we prove the Adjusted Descent Lemma.
Proof of Lemma 9. We will prove the Lemma by induction. To that end, let us call by I(s), ∀s ≥ 0
the proposition in the Lemma statement, i.e.
I(s)：	f(z(s+1)) ≤ f(Z(S))- 2Q∣Vf(z(S))k2
We first write two more propositions J(s) and K(s):
J(s) :	Z(S) ∈ A
K(s) ：	f(Z(S)) ≤ f (z) ∀z ∈ Rd \A
We will use induction to show that I(s) ∧ J(s) ∧ K(s) hold true ∀s ≥ 0. J(0) holds true, since
Z(0) ∈ A is given in the statement. K(0) is also given in the statement of the lemma.
Since ∀Z ∈ A, IV2f(Z)Iop ≤ Q; from Lemma 10, we conclude that
J(s) =⇒ I(s)	∀s ≥ 0	(15)
Hence I(0) also holds true. So the base case of the induction (s = 0) is done, i.e. I(0) ∧ J(0) ∧ K(0).
Since I(s) implies descent, and K(s) implies that all points outside region A have larger function
value, we also have
I(s) ∧ K(s) =⇒ J(s + 1) ∧ K(s + 1)	∀s ≥ 0	(16)
14
Published as a conference paper at ICLR 2022
Combining implications equation 15 and equation 16 and Lemma 10, we get
I(s) ∧ J(s) ∧ K(s) =⇒ I(s + 1) ∧ J(s + 1) ∧ K(s + 1)	∀s ≥ 0	(17)
Hence the induction step also goes through.	□
We can now use this to prove convergence of gradient descent.
Proof of Lemma 1. To use the adjusted descent lemma (Lemma 9), let us define the appropriate
region A.
A:= {z : kzk ≤ (M + 1).㈣1	(18)
mm
Obviously z(0) = 0 ∈ A. We also have ∀z ∈ Rd \ A, the following holds
f(z) - f(z(0)) = f(z) - f(0)
=1 (kG(z)-χk2-kG(0)-χk2)
=21 (kG(z)- G(Z)k2-kG(0)- G(Z)k2)
≥⑼ 2 (m2kz- zk2 - M2kzk2)
≥ C+ (mkz - Zk- Mkzk)
≥(2) 0
+1
C+ = 2 (mkz - zk + Mkzk) denotes a non-negative quantity, which does not affect the sign of
these inequalities. In the above calculation, (0) follows from Assumptions 1 and 2, (1) uses the
definition of A from equation equation 18, and (2) uses Lemma 23.
In A, the norm of the Hessian can be bounded as below. The expression of V2f can be inferred from
the expression of V2 L given in equation equation 56. We then get
∣∣v2f (z)∣lop ≤ k Jα(z)kθp + X ∣Gi(z) - “I ∙ MGi(Z)Ilop
i∈[d]
≤⑼ M2 + (X ∣Gi(z) - XiI I ∙ maχ∣∣V2Gi(z)∣∣op
i∈[d]	i∈[d]
≤⑴ M2 + √d ∙ kG(z) - xk ∙ M2
≤ M2 + √dMM2 ∙ kz - zk
≤⑵ M2 + √dMM2 ∙ 2 (M + 1] ∙ kxk =: Q z ∈ A (19)
mm
where (0) follows from Assumption 2, (1) follows since ∀i ∈ [d], ∣∣V2Gi(z)∣∣op ≤ kV2G(z)kop;
which follows from the sup-definition of the `2 norm (Lim, 2005). In (1), we also used the fact that
∣∣vkι ≤ ,dim(v) ∙ kv∣∣2. And in (2) we used the diameter of A of equation equation 18.
15
Published as a conference paper at ICLR 2022
With A, Q defined through equations equation 18 and equation 19, the conditions of Lemma 9 are
satisfied, so we can use the descent equation. We can then use the standard argument below
∀s∈ [0,S],	Vf(Z(s)) ≥α
=⇒ f(Z(S)) ≤ f (Z(O))- S ∙ 2Q ∙α2
Since f is a non-negative function, it is bounded below by zero. This means that
min kVf(Z(s))k≤《2Q，f：Z(O))
S∈[O,S]	S
We note that
∣∣Vf(z)k = ∣Jg(z)t (G(Z)-x)∣
≥ min ⑸(JG(Z))IkG(Z)-Xk
i∈[d]
≥ min ∣σi(JG(z))∣ √2f(z)
i∈[d]
≥ m√2f(z)
By Lemma 9 we then have
≤ i m mins∈[0,s] kVf(Z(S))k!2
2
≤
m
1 Qf(z(0))
m2 S
Furthermore, by Assumption 1 we have
kZ(S)- Zk ≤ ɪkG(Z(S))-Xk
m
J /2f(Z(S))
m
2Qf(Z(O))
S
Hence, setting δ = m12 ∙ J2Qf(Z(O)) gives US the required number of gradient descent steps. We
will finally simplify the expression for S. Since f (z(0)) = f (0) = 2 ∣∣G(0) 一 χ∣2 = 2∣∣χ∣2 (from
Assumption 3), We get
S = Q∙kχk2 1
m4	δ2
(20)
Plugging in the expression for Q from equation 19, and using ∣∣xk = O(√d) from Lemma 24, We
get S = O(δd2) as desired.	口
C Proof of Lemma 2: Fast Mixing of Langevin Diffusion
Notation: We first set up some notation. We denote by Zt the time t iterate of Langevin diffusion
(Definition 3) with g := L, having as stationary distribution P(Z) ≡ p(Z|X). Further, we denote by
Pt the density of Zt .
Similarly, we denote by Zt the iterates of the reflected Langevin chain (Definition 4) with g := L,
with stationary distribution Pz(z) H e-L(Z) supported over Z ∈ D. We denote by Pt the density of Zt.
Lemma 11. rad(D) defined in Lemma 13 along with the restriction on β from the same lemma
satisfies
(m2	m
-------,-/	= >
6dMM2 √2√dMM3 J
This in particular means that rad(D) ≤ O(1/d).
16
Published as a conference paper at ICLR 2022
Proof. Follows by straightforward substitution.	□
Organization: This section will be organized as follows. In Subsection C.1, we prove a lemma
quantifying the difference between the distributions of the regular and reflected Langevin Chains
(Lemma 12). To use this lemma, we require a property of the Langevin diffusion for our setting
(Lemma 13) — namely, that the chain with probability 1 - remains in the region D. We tackle
this in Subsection C.2. In subsection C.3, we prove Lemma 4, namely that p(z|x) restricted to D
is log-concave. Finally, using the above three pieces, we will prove prove the main lemma of this
section (Lemma 2) in subsection C.4.
We note that some of the techniques here have overlap with techniques used in (Moitra & Risteski,
2020), but we provide all the proofs in detail for completeness.
C.1 Relating Restricted and Unrestricted Langevin Chains
The main lemma of this section is as follows:
Lemma 12 (Comparing with reflected Langevin diffusion). Let Zt follow the reflected Langevin
SDE (Definition 4) with g := L), with stationary measure P and with domain D. Let zt follow the
Langevin diffusion SDE (Definition 3) with g := L, with stationary measure P. Let Pt be the density
of zZt and let Pt be the density of zt and let P0 be absolutely continuous with respect to the Lebesgue
measure.
Then if∀t ≥ 0, Pr [∃a ∈ [0, t], za ∈/ D] ≤ α, it holds that:
_________________________________________________t___
d tv (Pt, P) . 2α + e 2rad2(D)	∀t ≥ 0
Proof. Consider the total variation distance between Pt and P . Using triangle inequality, we have
dTV(Pt, P) ≤ dTV(Pt, PZt) + dTV(PZt, PZ) + dTV(PZ, P)	(21)
We first bound the second term using Theorem 3. Since D is convex and L is convex over D by
Lemma 4, we have
_	, ~ 二、---------t—
“TV(Pt，P). e 2rad (D)	(22)
For the first term, consider the coupling of zt and zZt such that the Brownian motion Bt is the same.
We then have for any t ≥ 0
dTV(Pt, PZt) ≤ Pr[zt 6= zZt] ≤ Pr[∃s ∈ [0, t], zt ∈/ D] ≤ α	(23)
where the first inequality follows by Lemma 6, and the second inequality follows from a rewrite that
uses the fact that if the unrestricted chain zt stays inside the region D, then the two diffusions won’t
differ at all due to the coupling of the Brownian motion. The term α bounds the probability of the
unrestricted chain zt leaving the region D.
For the third term, We need to bound ʤ(p, P). Note that P : Rd → R+, P(Z) 8 e-L(Z) and
P : D → R+, P(Z) H e-L⑶.This means that ∀z ∈ D, P(Z) > P(Z) and ∀z ∈ Rd \ D, P(Z) >
P(Z). So we can write
dTV(PZ, P) = Pr [Z∞ ∈/ D]	(24)
where Z∞ is the follows the stationary distribution of the Langevin diffusion P.
From the drift bound in the lemma statement, we can take the limit of t → ∞ to get
lim ∀t ≥ 0, Pr [∃a ∈ [0, t], Za ∈/ D] ≤ α
t→∞
=⇒ Pr [∃a ∈ [0, ∞), Za ∈/ D] ≤ α
=⇒ Pr [Z∞ ∈/ D] ≤ α	(25)
Using equation equation 24 and equation 25, we get
dTV(PZ , P) ≤ α	(26)
The lemma statement follows from equations equation 23, equation 22 and equation 26.	□
17
Published as a conference paper at ICLR 2022
C.2 Effective region for Langevin diffusion
In this section, we prove Langevin mostly stays in an effective region D, namely:
Lemma 13 (Effective region for Langevin diffusion). Let D := {z : ∣∣z - Z∣∣ ≤ rad(D)} with
Irad(D)= 笔、(2d + k^k2) log (⑶[m2)) Let zo satisfy ∣∣zo - Zk ≤ 1 rad(D).
β ≤ β0, we have that ∀T > 0, Pr [∃t ∈ [0, T], zt ∈/ D] ≤ /4, where
Then with
• min
m3	d0.75m2 ɪ
6MM2 , √2MM3 J
Towards proving Lemma 13, we will first prove Lemma 3.
ProofofLemma 3. Using dz = -RL(Z)dt + 2dBBt along with It6's Lemma, We can compute
dη(z) = Bn(Z), --RL(z»dt + 2 • (√2)2∆η(z)dt + hVη(z), √2dBti	(27)
We will bound each term on the right hand side. First note that Vn(Z) = Z - Z, and so ∣∣Vn(z)∣ =
,2n(z) for all Z ∈ Rd. We proceed to the second and third term since they,re straightforward. For
the third term, we have hVn(Z), dBti = kn(z)kdBt = y∕2η(Z)d,Bt, using a slight abuse of notation
to denote the 1-dimensional Brownian motion (in the RHS) as also Bt. For the second term, note that
∆η(Z) = d since V2 η(Z) = I. So:
dη(Z) = -(Vη(Z), VL(Zyidt + d • dt + 2pη(Z)dBt	(28)
For the first term, from Lemma 26 we get
hVη(Z), VL(Z)i =(z - Z, VL(Z)i
hZ - z, Zi + hz - z, (I + β2 JG(Z)T JG(Z)
(z - Z)i + hz - z, RyL(Z)
hz - z,zi + kz-Zk2 + * kJG(Z)(Z - Z)k2 + hz-(29)
First, we analyze hz - Z, Z). Using Cauchy-Schwartz we can write hz - Z, ^ ≥ -∣z - ZkkZ∣.
Case 1.	∣z - Zk ≤ ∣∣Zk. We can write that hz - Z,Z) ≥ -∣∣Z∣∣2.
Case2.	∣∣z - Zk > ||z||. We can write that hz - Z,Z) ≥ -∣∣z - Zk2.
In either case, it holds that
hz - Z,Z)≥-(kz- Zk2 + kZk2)	(30)
Second, by Assumption 1, we can say that
I∣Jg(Z)(z - Z)k2 ≥ m2kz - Zk2	(31)
Third, by the expression of the norm of the remainder from Lemma 26, we get
hz - Z,Rvl(z)) ≥ -∣Z - Zk • ∣∣Rvl(z)∣
≥ -2β2 • (3dMM2 ∙ ∣∣z - Z∣ + √dMM3 ∙ ∣∣z - Z∣2) • ∣∣z - Z∣2
For z ∈ D we have ∣∣z — Zk ≤ rad(D) so
hz - Z, RyL(z)i ≥ --1y • (3dMM2 • rad(D) + √dMM3 • rad(D)2) • ∣z - Z∣∣2 Z ∈ D
2β2
The inequality on rad(D) from Lemma 11 implies that 3dMM2 ∙ rad(D) ≤ m and √dMM3 ∙
rad(D)2 ≤ m22, and so
2
hz - z, rvl(z)) ≥ -^72Ilz - z∣2	Z ∈ d	(32)
2β2
18
Published as a conference paper at ICLR 2022
Plugging in equation equation 30, equation 31, equation 32 in equation equation 29, we get
22	2
hVη(z), VL(z)i ≥ -kzk2 + 京kz - zk2 -诉kz - zk2 = -kzk2 + 诉kz - Zk2 Z ∈ D
β	2β	2β
Using this in equation equation 28, and substituting the definition of η(z), i.e. ∣∣z - Z∣∣2 = 2η(z),
finishes the proof.	□
Next, we prove high probability deviation bounds for Cox-Ingersoll-Ross processes. Precisely, we
show:
Lemma 14 (Concentration Bounds on Cox-Ingersoll-Ross Process). Consider the SDE
dXt = -WXtdt + N dt + 2pXtdBt
for N ∈ N and w > 0. Then for any > 0
∀T > 0, Pr
Vt ∈ [0, T], s.t. Xt ≤ 2Xo +------------log
w
Proof. The SDE describes a Cox-Ingersoll-Ross process of dimension NN, which equals in distribution
to the random variable Yt as defined below, and with matching initial condition Y0 := X0 .
i=1
(33)
Here Vt(i) follow the Ornstein-Uhlenbeck equation dVt(i) = 一 w2 Vt(i)dt + dB(i), and V0(i) = Jy0∕N
for all i ∈ [N]. dB(i) are (1-d) Brownian motions, and we have referenced them separately because
they are independent for each i ∈ [N]. Indeed, applying Ito S Lemma, we recover the same SDE as:
N ɪ- /
dYt=X (2匕⑺(-2匕⑺)
i1
+ 1 ∙ 2)dt + 2Vt(idBai
i=1
i=1
--- ʌ , 一
-WYtdt + N dt + 2
N
dt + Ndt + 2 X V，idBt(i
i=1
∖
N	2
X(Vt(i)2dBt
i=1
-WYtdt + N dt + 2pYtdBt
Here in the RHS of the first expression, the term 2 ∙ 2 is the term 2 ∙ -(( J from ItO's Lemma.
2	2	∂Bt( )
And in the third expression, notice that we have replaced a sum of brownian motions with a single
one of matching variance.
Now we can write an explicit solution for the SDE. Since we know the solution of an Ornstein-
(i)
Uhlenbeck process, each Vt can be solved as (now we drop the indexing i since the solution is same
for each i ∈ [N])
Vt = Voe-w21 + Z e-w2 JsdBs
0
'----------------}
(34)
{z^^
F(t)
Applying the reflection principle to F (t), we get
VT > 0, Va > 0 Pr [∃t ∈ [0, T], F(t) ≥ a] = 2Pr[F (T) ≥ a]	(35)
19
Published as a conference paper at ICLR 2022
Further, writing the distribution of F (t), we have
F(t) ~N 00, -1 (1 - e-wt
w
From standard Gaussian tail bounds, we have
a2
∀T> 0, ∀a> 0	Pr [F(T) ≥ a] ≤ exp (- 2 (1 - e-wT)
From equation 35, substituting a := r J2 (1 - e-wT) we have
∀T > 0, ∀r > 0
Pr ∃t ∈ [0, T],F(t) ≥ r(1 - e-wT) ≤ 2e
-r2
a
Since the quantity F (t) has
bound as
symmetric density about zero for all t, we can write the two sided
∀T > 0, ∀r > 0
Pr ∃t ∈ [0, T], |F(t)| ≥ r(1 - e-wT) ≤ 4e
-r2
(36)
Choosing r such that /N
we have
2
4e-r in equation 36, and using a union bound over the indices i ∈ [N],
, . - - -~ -
∀T > 0 WP(I-E): ∀t ∈ [0,T],∀i ∈ [N],
VOk号t)2 ≤ log 盛)I (1 - e-w
—
Plugging in V0(i) = Jy0∕N for all i ∈ [N],we get
∀T > 0 w.p. (1 - e) :	∀t ∈ [0,T],∀i ∈ [N],匕⑴ ≤
≤
Finally, using (a + b)2 ≤ 2(a2 + b2) and summing over all indices i ∈ [N], We get
N	〜
∀T> 0 W.p. (1 -e): ∀t ∈ [0,T], X ”))2 ≤ 2Y0 + 4Nlog
i=1	w
~
4N
(37)

Replacing the sum by Yt using equation 33 and replacing Xt , X0 in place of Yt , Y0 gives the
result.
□
Next, since all of our SDEs so far have been actually inequalities, we will need a standard comparison
theorem for SDEs:
Lemma 15 (Comparison theorem, (Ikeda & Watanabe, 1977)). Let Ut, Xt be two stochastic processes
following the SDEs
dUt = f(Ut)dt + h(Ut)dBt
dXt = g(Xt)dt+ h(Xt)dBt
driven by the same Brownian motion Bt. Let at least one of these SDEs have a pathwise unique
solution. If ∀x ∈ Rd, f(x) ≤ g(x), and if U0 = X0, then with probability 1,
Ut ≤ Xt	∀t ≥ 0
20
Published as a conference paper at ICLR 2022
Lemma 15 shows domination of stochastic variables based on an inequality in their respective SDEs.
This is almost what we need, since we showed an inequality in SDEs in Lemma 3 and showed that
the upper bound is concentrated in Lemma 14. We need a slight adjustment on top of this though.
Note that the inequality between SDEs in Lemma 3 holds only for z ∈ D, however the inequality
that the above Lemma requires is on all points α, f(α) ≤ g(α). We can work through this by simply
noting that conditioned on Xt (the upper bound SDE) being concentrated in a region, Ut will also be
concentrated in the region. We state and show this below.
Lemma 16. Let Ut, Xt be two stochastic processes whose SDEs follow
dUt = f(Ut)dt+h(Ut)dBt
dXt = g(Xt)dt + h(Xt)dBt
driven by the same Brownian motion Bt. Let at least one of these SDEs have a pathwise unique
solution. If f(α) ≤ g(α) for all α ≤ α0, and if U0 = X0 ≤ α0; then
∀T > 0, ∀X ≤ αo Pr [∀t ∈ [0,T], Ut ≤ X] ≥ Pr [∀t ∈ [0, T], Xt ≤ X]
Proof. Consider any T > 0 and X ≤ α0.
Pr [∀t ∈ [0,T],Ut ≤ X] ≥ Pr [∀t ∈ [0, T], Ut ≤ Xt and∀t ∈ [0,T],Xt ≤ X]
≥ Pr j∀t ∈ [0,T],Xt ≤ X] ∙ Pr [∀t ∈ [0,T],Ut ≤ Xt ∣∀t ∈ [0,T],Xt ≤ X]
'----------------------------------------------------------------------}
z
1
Where the last probability is one, because both Ut and Xt are continuous-time processes, and so
Ut > Xt given Xt ≤ X ≤ α0 is not possible because Ut cannot cross over Xt in the f ≤ g region
(using comparison Lemma 15).	□
Given the above preliminaries, we can finally complete the proof of Lemma 13.
Proof of Lemma 13. Instantiate Lemma 16 with Ut := η(zt) and instantiate another SDE on Xt with
the same Brownian motion as Ut, as below
^2
dXt := —2XXtdt + dkZ∣∣2 + d∖ ∙ dt + 2pXtdBt
β
such that Xo := Uo = η(zo) = 1 ∣∣zo 一 Z∣∣2. From Lemma 14 (plugging in N := dkZ∣∣2 + d∖ and
w:
we can conclude that for any > 0
∀T > 0, Pr
Λ XT
∀t ∈ [0,T], s.t. Xt ≤ 2Xo + 4N log
w
(4O
(38)
≥ 1 —
Using the Lemma 16 on Ut and Xt, with X := 2X° + 4WN log (4N), and with α0 := 2rad(D)2
(since the inequality between SDES of Ut and Xt holds in the region Ut ≤ 2rad(D)2 from lemma
3), we get
X ≤ αo ⇒∀T > 0, Pr [∀t ∈ [0,T], Ut ≤ αo] ≥ Pr [∀t ∈ [0, T], Ut ≤ X]
≥ Pr j∀t ∈ [0,T],Xt ≤ X]
≥ 1 —
where the last inequality follows from equation 38. Note that Ut ≤ α0 ≡ ∣∣zt 一 Zk ≤ rad(D) ≡
zt ∈ D. This is exactly the event whose probability we need to lower bound.
We now check that the condition for the above is met, namely:
4NX	4NX	1
X ≤ αo ≡ 2Xo +-------log (----≤ ≤ Drad(D)2
21
Published as a conference paper at ICLR 2022
Where Xo = 1 ∣∣zo 一 Z∣2 ≤ 1 ∙ (2rad(D))
-4Nl	(
≡ ——log
w
2
from the lemma statement. This then becomes
F) ≤ 1 rad(D)2, i.e.
ad(D) ≥ 4t 一 log
4NN
r
N
w
Putting in the values N, W from above, and using Lemma 23 to bound ∣∣Z∣∣ (since the RHS above is
an increasing function of N), and bounding the ceiling trivially by dκe ≤ κ + d (since d ≥ 1) means
it suffices that
4βu
rad(D) ≥ mt
2d+吗 >4>
m2
Using the bound on β the above inequality is indeed satisfied for our choice of rad(D).
□
C.3 Proof of Lemma 4
Proof. Using the expression of V2L from equation equation 56, We get that
v2L(z) 上(1 + β I m2
V2Gi(z)(Gi(z)-xi)
i∈[d]
(39)
op
—
I
And We have
V2Gi(z)(Gi(z)-xi)
i∈[d]
op
≤ ∑∣Gi(z)-Xi∣∙∣∣V2Gi(z)∣∣op
i∈[d]
≤	∑∣Gi(z) - Xi∣∙∣∣V2Gi(z)∣lop
i∈[d]
≤	⑼ √d ∙∣∣G(z)-x∣∣∙∣∣V2G(z)∣∣op
≤	⑴ √d ∙ M∣∣z - z∣∣ ∙ M2
≤	√dMM2 ∙ rad(D)	Z ∈ D
In (0), We used the fact that ∀i ∈ [d], ∣∣V2Gi(z)∣∣op ≤ ∣V2G(z)∣op; Which can be derived from the
sup-definition of the tensor `2 norm from (Lim, 2005). In (1), We’ve used Assumption 2. Using the
expression of rad(D) from Lemma 11, We get
V2Gi(z)(Gi(z)-xi)
i∈[d]
op
2
m2
≤ 6√d
z∈D
≤ m2
z ∈ D (since d ≥ 1)
Using this in equation 39, We get V2L(z)	I for z ∈ D.
□
C.4 Proof of Lemma 2
We can noW put together the proof of Lemma 2.
Proof of Lemma 2. Using Lemma 12 With D defined as in Lemma 13, With α := /4 in Lemma 13
and setting T = rad2(D) log ɪ we get e-T∕2rαd2(D) ≤ 2. Using rad(D) = O(1∕d) from lemma
11, the result follows.	□
22
Published as a conference paper at ICLR 2022
D Proof of Theorem 1: Euler-Mariyama Discretization and
Neural Simulation
In this section, we use the mixing time bounds in the previous sections, along with a discretization
error analysis when running a disretized (in the simplest manner, via a Euler-Mariyama scheme)
version of the Langevin diffusion chain. This is done in Subsection D.1. Subsequently, we show that
each of these steps can be simulated by a “layer” of a neural network. This is done in Subsection D.2.
Finally, we put everything together to prove Theorem 1 in Subsection D.3.
D.1 B ounding the error in the Euler-Mariyama discretization
We finally wish to show that with a polynomially small discretization level (i.e. h), the Euler-
Mariyama scheme approximates appropriately for our purposes the Langevin diffusion process. We
recall from equation 2 that this Markov process is described by
Z 一 Z - hVL(z) + √2hξt, ξ 〜N(0, I)	(40)
There is a rich field in the literature on discretizing Langevin diffusion (Dalalyan, 2016; Raginsky
et al., 2017; Dalalyan, 2017) primarily used for designing efficient sampling algorithms — a large
fraction of them analyzing the above discretization. We will mostly rely on tools from Bubeck et al.
(2018). We show:
Lemma 17 (EUler-Mariyama discretization). Let Zt follow the Langevin diffusion dzt =
-VL(Zt)dt + √2dBt, and let Pt denote the distribution of Zt.
Let Zt follow the continuous-time Markov process determined by the Euler-Mariyama discretization
scheme with discretization level h given by equation 12, namely
dZt = -VL(Zbt∕hCh)dt + √2dBt	(41)
Let us denote by Pt the distribution of Zt. Finally, let z0 = Z° ∈ D satisfying ∣∣zo 一 Zk ≤ 2rad(D),
with D defined as in Lemma 13.
Then, if T = (2rad(D))2 log(1∕e) and h = poly(e, 1 /d, 1/M, m, β), we have dtv(PT, PT) ≤ E
Proof. Let yt follow the Reflected Langevin SDE (Definition 4), with g := L and region D, and with
the initialization yo = z0. Let yt follow the continuous-time extension of the discretized reflected
Langevin SDE given by equation 14 with g := L and region D), with the initial point yo = z0. Let
Qt and Qt denote the respective distributions. Let Q denote the stationary distribution of yt.
From triangle inequality, we can decompose our error as
“TV (PT, PT) ≤ “TV (Qt, PT) + "tv (QT, QT) + "tv
We will bound each of the terms in turn.
,QT
(42)
For the first term, consider a coupling of yt and Zt that uses the same Brownian motion for both
processes, so long as neither has left D, and chooses an arbitrary coupling henceforth. By Lemma 6,
we have
dTV(QT, PT) ≤ Pr[ZT 6= yT]
≤ Pr[ZT leaves D] + Pr[yT leaves D]
≤ E/2
where the last inequality follows by Lemma 13.
For the second term, we can further break it up, again by using the triangle inequality as
dTV (QT, QT) ≤ dTV (QT, Q) + dTV (Q, qt)	(43)
We will use a result about the discretization of reflected Langevin diffusion from (Bubeck et al.,
2018). Towards that, let us denote
L1D = max kVL(Z)k	(44)
z∈D
L2D = max kV2L(Z)kop	(45)
z∈D
23
Published as a conference paper at ICLR 2022
Using Theorem 1 from (Bubeck et al., 2018), if T = (2rad(D))2 log(1/) and h =
poly(, 1/d, 1/L1D, 1/L2D), both terms in equation 43 are upper bounded by /4. On the other
hand, by equations equation 55 and equation 56, we have
kVL(z)k ≤ kzk + -12kJG(z)Tk2kG(z)-xk
β
≤ (kzk + rad(D)) + 712M2rad(D)2	Z ∈ D
β2
Using Lemma 23 and the first term of the RHS from Lemma 11, we get
kVL(z)k≤ 蚓 +
m
1+
4
m
36d2M 2M2
z∈D
M2
亨)
≤ O(√d) + O
z∈D
Which follows using Lemma 24. For the second order bound, a similar calculation as the proof of
Lemma 4 we get
kV2L(z)kop ≤ 1 + β 卜 JG(Z)I∣2p + X kV2Gik2∣Gi(z) - Xil
≤ 1 + -12 (M2 + m2)
β2
=O (β2)
Plugging the bounds above, we get that if T =	(2rad(D))2 log(1/) and h =
poly(e, 1/d, 1/M, m, β), then ʤ (QT, QT) ≤ 〃2.
Finally, for the third term, we will show that
“TV (PT ,Qt) ≤ e∕4	(46)
Towards that, by Lemma 6, coupling the Gaussian noise to be the same for {Pt}t∈[T] and {Qt}t∈[T],
we have
dTV (PT, QT) ≤ Pr[PT = QT]
≤ Pr[∃t ∈ [T],Zt ∈ D]
By the definition of total variation distance, and Lemma 6 again, we have
Pr[∃t ∈ [T],Zt ∈ D] ≤ dτv ({^t}t∈[T],{zt}t∈[T]) + Pr[∃t ∈ [T],zt ∈ D]
By Lemma 13 and Lemma 6, we have Pr[∃t ∈ [T], Zt ∈/ D] ≤ 〃4 and dTV ({2t}t∈[T], {zt}t∈[T]) ≤
/4 which proves equation 46.
□
D.2 Neural Simulation of Arithmetic Operations
The deep latent Gaussian will be an overall map from observables x ∈ Rd to latents Z ∈ Rd, such
that Z is an approximate sample from the posterior p(Zlx).
In this section, we prove “simulation” lemmas that show how to combine neural networks for two
functions f, g to create a neural network for some arithmetic operation applied to f, g (e.g. f + g,
f × g, etc.).
Recall, ρ : R → R denotes the square activation function, i.e. ρ(x) = x2 .
24
Published as a conference paper at ICLR 2022
Lemma 18. If f : Rd → R is a neural network with M parameters and g : Rd → R is a
neural network with N parameters, both with set of activation functions Σ, then w1f + w2g can be
represented by a neural network with O(M + N) parameters and set of activation functions Σ for
any w1 , w2 ∈ R.
Proof. We simply note that we can make a network with one more layer than the maximum of the
depth of f and g that adds up the scaled outputs of f, g to produce w1f + w2g. No additional
nonlinearity is needed.	□
Lemma 19. If f : Rd → R is a neural network with M parameters and g : Rd → R is a neural
network with N parameters, both with set of activation functions Σ, then for any w ∈ R, v = wfg
can be represented by a neural network with O(M + N) parameters with set of activations Σ ∪ {ρ}.
Proof. Noting Xy = 4 ((X + y)2 - (x - y)2) for all x,y ∈ R, We can implement the multiplication
of scalars using a two-layer neural network with O(1) parameters using the ρ activation function. We
then proceed same as for addition, adding one layer to multiply the outputs of f,g.	□
Lemma 20. If G : Rd → Rd is represented by a neural network with N parameters and differentiable
activation function set Σ, given an X ∈ Rd and z ∈ Rd, one can compute for any c1, c2 ∈ R:
c1z + c2JG(z)T (G(z) - X)
via a neural network with O(N d2) parameters and activation functions Σ ∪ Σ0 ∪ {ρ}, where Σ0
contains the derivatives of the activations in Σ.
Proof. Each coordinate of JG : Rd → Rd×d can be Written as a neural netWork With O (N)
parameters by Lemma 5, With activation functions Σ ∪ Σ0. Each coordinate of G(z) - X can be
computed in O(N) parameters using lemma 18.
Performing the multiplications needed for JG(z)T (G(z) - X) via Lemma 19 results in a netWork
of size O(N d2) due to the d2 size of the Jacobian, and together With the addition of c1z (via O(d)
parameters) results in the final bound of O(Nd2) on the parameters.	□
D.3 Proof of Theorem 1
Finally, We can put all the ingredients together to prove Theorem 1.
Proof of Theorem 1. The deep latent Gaussian We construct Will have all latent dimensions das Well
as the output dimension d. The input Will be X ∈ Rd, the observable on Which We Want to carry out
the inference. We Will first create a part for the GD simulation, and then one for the Langevin chain.
Using Lemma 1 with δ := 4rad(D), if we run Gradient Descent for S = O( 丁口忠尸 )=O( β)
steps (using the bound rad(D) = Ω(β√d) from Lemma 13), we can get a Zinit with kzinit — Zk ≤
4 rad(D).
Using Lemma 20 , we can construct a network Done with O Nd2 parameters that executes a single
step of gradient descent. In the deep latent Gaussian we construct, we repeat Done with zero variance
O( β⅛) times to simulate all the steps of gradient descent and construct Zinit. The overall number
of parameters of the deep latent Gaussian so far is then O (poly(d, 1) ∙ N). Note, though the first
variable Z0 has unit variance, we can easily transform it to a zero variance Gaussian to simulate the
first step by multiplying by 0.
Starting with the random point Z0 , if we run the discretized Langevin dynamics given by equation 11
for T/h steps with T and h as given by Lemma 17, we get by the error bounds of Lemma 17 and
Lemma 2 and the triangle inequality:
dTV (PT,p(Z | X))
≤
which is the required overall bound.
25
Published as a conference paper at ICLR 2022
We further note that each step of the discretized Langevin dynamics can be simulated by a layer of
the deep latent Gaussian: by Lemma 20 we can construct a network Sone with O Nd2 parameters
that computes the next Langevin iterate. Choosing the variance to be 2h, then, accomplishes exactly
the distribution of a single step of the discretized dynamics. Given the choice of T and h, the
number of steps We need to run is O (poly (1 /e, d, 1∕β)). Hence, the overall number of parameters
required for simulating the discretized dynamics is O (poly(1∕e, d, 1∕β) ∙ N). Finally, it,s clear that
the dependence on x in the layers comes through the JG(z)T (G(z) - x) terms in the gradient and
Langevin updates—so the Weights for the deep latent Gaussian can be produced by a netWork of
size O (poly(1∕e, d, 1∕β) ∙ N) as well. The multiplication by X can be implemented via the square
activation ρ. Thus, the proof is finished	口
E Proof of Theorem 2: Lower b ound
First, we observe the following proposition, which shows that the samples x with high probability lie
near a vertex of the hypercube {±1}d
Lemma 21 (Closeness to hypercube). A sample x from the VAE with G(z) = C(sgn(z)) and variance
β2I satisfies, for all c > 1
Pr[kx — G(z)k ≤ 6cβ√d] ≥ 1 — exp(-c2d)
Proof. Since x - G(z) is a Gaussian with variance β2I, the claim follows via standard concentration
of the norm of a Gaussian.	口
Furthermore, we also have the following lemma, that shows that when x is near a vertex of the
hypercube, the posterior p(z|x) concentrates near the C-1(sgn(x)), that is, the pre-image of the
nearest point to x on the hypercube.
Lemma 22 (Concentration near pre-image). Let X be such that ||x - Sgn(x) ∣∣ ≤ 6β√d. Then,
Prz~p(z∣x)[Sgn(Z) = C-1(Sgn(x))] ≤ exp(-2d)
Proof. By Bayes rule, since p(sgn(z)) is uniform, we have
P(Sgn(Z)|x) H p(x∣sgn(z))
Consider then any b ∈ {±1}d, b 6= sgn(z). We have:
p(x∣b)	e-1/e2kx-bk2
p(x∣sgn(z)) = e1∕β2kx-sgn(z)k2
=e-i∕β2(kχ-bk2-kχ-sgn(z)k2)
≤ e-10d
where the last inequality follows for β = o(1∕√d) and a small enough constant in the o(∙) notation.
Hence,
p(sgn(Z)|X) = 1 -	p(b|X)
b6=sgn(z)
≥ 1 - 2de-10dp(X|sgn(Z))
≥ 1 - e-2d
□
Proof of Theorem 2. The proof will proceed by a reduction to Conjecture 1. Namely, C be a Boolean
circuit satisfying the one-way-function assumptions in Conjecture 1. Let G be defined as G(Z) =
C(sgn(Z))
26
Published as a conference paper at ICLR 2022
Suppose that there is an encoder E of size T (n), s.t. with probability at least (d) over the choice of
x (wrt to the distribution of G), we have TV(p(z|x), q(z|x)) ≤ 1/10, where q(z|x) is the distribution
of the encoder when x is supplied as input.
We will first show how to construct a neural sequential Gaussian C00 which violates Conjecture 1 in an
“average sense"： namely, for 1∕poly(d) fraction of inputs X ∈ {±1}d, with probability 1 - exp(-d)
it outputs the z, s.t. X= C(Z).
Subsequently we will remove the randomness — i.e. we will produce from this a deterministic and a
neural network, rather than a circuit - subsequently We will show how to remove both the randomness,
and use simulate the neural networks using a Boolean circuit.
This deep latent Gaussian C00 receives a X ∈ {±1}d and constructs (randomly) a multiple samples
X ∈ Rd as
X = X + ξ, ξ 〜N(0,β2Id)	(47)
Subsequently, it samples (multiple) Z from E(z∖χ) and checks whether X = C(Sgn(z)). If yes, it
outputs sgn(z). If for none of the samples X and z, sgn(z) = X, it outputs (1,1,..., 1).
We will show that for 1∕poly(d) fraction of inputs X ∈ {±1}d, with probability 1 - exp(-d), C00
outputs the z, s.t. X = C(z).
Note that X is distributed according to the distribution of the generator G. (Since X is uniformly
distributed on the hypercube.) Let Ei be the event that dTv(p(z∣X), E(z)) ≤ 今,E2 the event that
∣∣x - Sgn(X)k ≤ 6β√d. By Assumption, Pr[Eι] ≤ 1 - e(d); by Lemma 21, for a large enough d,
Pr[E2] ≤ 总.Hence, Pr[Eι ∨ E2] ≤ 1 -挈.,i.e. Pr[E1 ∧ E2] ≥ 萼.
Consider an X for which both events E1 and E2 attain. From the the definition of TV distance and
Lemma 22, we have
Prz〜E(z∣x)[sgn(z) = CT(Sgn(X))] ≤ Prz〜?3方)[sgn(z) = CT(Sgn(X))] + ɪ0
≤ exp(-2d) + 10
1
≤ —
5
Hence, we have
Prx [1 (Prz〜E(z∣x)[sgn(z) = CT(Sgn(X))] ≤ 1/5)] ≥ 竽
By the tower rule of probability, we can rewrite this as
Ez〜N(0,id) [Prx〜p(x∣z) [1 (Prz〜E(z∣x)[sgn(z) = CT(Sgn(X))] ≤ 1/5)]] ≥ e(d)∕2
Let us denote the random variable
A⑶:=Prx〜p(χ∣z) [1 (Prz〜E(z∣χ)[sgn(z) = CT(Sgn(X))] ≤ 1/5)]
such that we have
Ez[A(z)] ≥ e(d)∕2
By the reverse Markov inequality, it follows that
1 - (d)/2
Pr[A(z) ≤ a] ≤ ------------
1-a
Taking a = (d)/2 we get Pr[A(z) ≤ (d)/2] ≤ 1 - 2 (d)/4. Hence, with probability at least
e2(d)∕4 over the choice of z, we have
Prx〜p(x∣z) [1 (Prz〜E(z∣x)[sgn(z) = CT(Sgn(X))] ≤ 1/5)] ≥ e(d)∕2
As we indicated, C00 will repeat sampling X and z ∖X multiple times. Let us denote by ξ1 the Gaussian
noise sampled in equation 47 and {ξi}L=i the Gaussian samples used in sampling Z 〜q(z∣X).
Consider sampling {ξi,i ∈ [1,M]}, where M = Ω (《砌"d)and {ξi0,j,i0 ∈ [L],j ∈ [M0]},
27
Published as a conference paper at ICLR 2022
where M0 = Ω(log d), and for each i ∈ [M],j ∈ [M0] outputs X = x + ξi, and samples from q(z|x)
using the randomness in {ξ2 ,j}L=「By Chernoffbounds, with probability 1 - exp(-Ω(d)), at least
one pair of indices (i,j) is such that the corresponding choice of random vectors results in a choice
of z, such that C(sgn(z)) = X. Since the total number of possible strings X is 2d, taking the constants
in M, M0 sufficiently large, by union bound, there is a pair (i, j), s.t. the corresponding vectors
i0 j
ξi, {ξ2 ,j}L=ι works for all x. Hard-coding these into the weights of a neural network, We get a
deterministic neural network of size O(|C| + T (d)), that with probability 2(d)/4 inverts C.
The only leftover issue is that E is a neural network (hence, a continuous function), whereas we
are trying to produce a Boolean circuit. Conversions between neural networks applied to Boolean
inputs and Boolean circuits are fairly standard (see Siegelmann (2012), Chapter 4, Theorem 6), but
we include a proof sketch for completeness nevertheless.
First, we show that having logarithmic in L, W, M precision for the weights and activations in E
suffices. For notational convenience, let us denote by N the maximum degree of any node in E. The
proof is essentially the same as Lemma 4.2.1 in Siegelmann (2012) - the only difference is that our
inputs are close to binary, but not exactly binary. Namely, for some constants δw , δa to be chosen,
consider a network E : Rd → Rd, which has the same architecture as E, and additionally:
•	The edge weights {Wi,j} are produced by truncating the weights wij to log(δw) significant
bits, s.t. ∀i, j : |w2,j - w2,j| ≤ δw.
•	If a node in E has activation σ, the corresponding node in E0 has activation σ0 that truncates
the value of σ to log(δa) significant bits, s.t. ∀x, ∣σ(x) - σ0(x)∣ ≤ δo.
Then, we claimthatfor nodes at depth t from the input, denoting V(X) and V(X) the function calculated
by the node in E and E respectively, we will show by induction:
|v(x) - V(x)∣ ≤ L(LNW)t-1(LNMδw + δa)
Let us denote by Et the maximum of |v(x) - V(x) | for any nodes v, V at depth t. Suppose the claim is
true for nodes at depth t and consider a node at depth t + 1. Denoting by V1, V2, . . . , Vn the inputs to
V , we have
(48)
σ0 (EwiVi(X)J -σ0 (EwiVi(X)
2	2	(49)
≤ LE IwiVi(X)- WiVi(X)I + δa	(50)
i
≤ L X (IwiVi(X)- wiVi(X)∣ + IwiVi(X)- wiVi(X)∣) + δa	(51)
i
≤ L X (WEt + Mδw ) + δa	(52)
i
≤ LNWEt + LNMδw + δa	(53)
where equation 50 follows by Lipschitzness of σ and equation 52 follows from the fact that the
weights are bounded by W and the outputs v, V are bounded by M.
Unfolding the recursion, we get
t
Et+1 ≤X(LNW)i(LNMδw + δa) ≤ (LNW)t(LNMδw + δa)
Given this estimate, we can choose δw, δa, s.t. the values at each output coordinate of E and E match.
It suffices to have (LNW)d(LNMδw + δ°) < 1, which obtains when δw, δ0 = O (M(LNW)-d)
-i.e. it suffices to only keep O (dlog(LNW) + log(M)) significant bits.
28
Published as a conference paper at ICLR 2022
Tk T	1	1	.	♦	1 . ThI-I	1 ʌ 1	♦	∙ , ,1 ∙	, ∙ 11	, ,' ,1	CCT
Next, We show how to simulate E by a Boolean circuit - this is essentially part of the proof of Lemma
4.2.2 in Siegelmann (2012). Namely, each pre-activation can be computed by a subcircuit of size
O N (d log(LN W) + log(M))2 by the classic carry-lookahead algorithm for addition. Since we
only require log(δa) bits of accuracy for the activation function, and each activation is at most M (so
we need O(log M) bits for the integer part), we need O(log M + log(δa)) nodes for the activation
function.
Hence, we can simulate E by a Boolean circuit which is at most a factor of
poly (N, log(L), log(W), log(M)). Since L, W, M are o(exp(poly(d))), the size of the size of
the resulting Boolean circuit is still poly(d). Thus, we’ve constructed a polynomial-sized circuit
which inverts the one-way-permutation, thus violating Conjecture 1.
□
F Technical Lemmas
This section contains several equations and calculational lemmas that we repeatedly use across the
proofs of Theorem 1.
F.1 BOUND ON ∣∣zk
Recall that we defined the inverse point Z for a given X ∈ Rd in equation equation 4. As mentioned
earlier, due to bijectivity, it is unique and satisfied G(Z) = x. Further, we can easily prove a bound
on the norm of Z using our assumptions, which will be useful in various places in the proof.
Lemma 23. Z defined as in equation 4 with G that satisfies Assumption 1, is such that
∣Z∣≤ ㈣
m
Proof. Using strong invertibility from Assumption 1 and centering from Assumption 3, we can write
∣x- G(0)∣ = ∣G(Z) - G(0)∣≥ m ∙∣Z- 0∣
=⇒ ∣Zk ≤ ㈣	(54)
m
□
F.2 HIGH PROBABILITY BOUND ON ∣x∣
Lemma 24 (High-probability bound on ∣x∣). With x following the density from Definition 1 (with
dl = do = d), s.t. G satisfies Assumption 2, we have that
∣x∣ ≤ 12(M + β)√d	w.p. 1 - 2 exp(-4d)
Proof. The density of X is defined by the latent Gaussian. Let's denote the error term as e 〜
N(0, β2I). We have
X = G(Z) + e
=⇒ ∣X∣ ≤ ∣G(Z)∣ +∣e∣
=⇒ ∣X∣ ≤M∣Z∣ +∣e∣
(Where we used assumption 2). Now Z and e follow zero-mean Gaussian densities, with variances I
and β2I respectively. Using standard gaussian tail bounds (similar to lemma 21 with c = 2), we get
the required claim.	□
Remark: Since we work under the setting β ≤ O(1) from equation 1, we can use ∣X∣ ≤ o( √d)
with probability 1 一 exp(Ω(d)).
29
Published as a conference paper at ICLR 2022
F.3 Characterizing loss near Z
We need to understand how L and VL behaves near Z for several claims in the proofs. We will use a
Taylor expansion, bounding the contribution of higher order terms.
F.3.1 Expressions for Derivatives
Using equation equation 3, we first calculate the expressions of the derivatives as
VL(Z) = Z + 2J JG(Z)T(G(Z)- X)	(55)
β2
V2L(z) = I + β12 I Jg(z)t Jg(z) + X V2Gi(Z)(Gi(z) - Xi) I	(56)
β	i∈[d]
At the inverse point Z, since G(Z) = x, we have:
f L(Z) = 2 kZk2
V	VL(Z) = Z	(57)
I V2L(Z) = I + β Jg(Z)tJg(Z)占 I
Since V2L(Z)占 I, the function L(z) is strongly convex in the neighbourhood of Z.
F.3.2 BOUND ON 3RD DERiVATiVE OF L
To show the behaviour of VL around Z, we will first bound the norm of the 3rd order derivatives of L
below.
Lemma 25 (Bounding the 3rd derivative of L). For all Z ∈ Rd, it holds that
∣∣V3 * * * * * *L(z)∣∣op≤√dM ∙ (3√dM2 + M3kz- Zk)
Proof. An elementary calculation shows that
V3L(Z)=β
3 E VGi(z) XT V2Gi(z) + E V3Gi(z)(Gi(z) - Xi)
i∈[d]
i∈[d]
(58)
Here, the tensor product XT multiplies elements from Rd and Rd×d to give elements in Rd×d×d . We
subscript with T to avoid confusion with the kronecker product.
Then, we have:
IIV3L(Z)IIop ≤ β (3∙
VGi(Z) XT V2Gi(Z)
i∈[d]
op
V3Gi(Z) (Gi(Z) - Xi)
i∈[d]
(triangle inequality)
op
+
1
≤ β2
3 ∙ E IIVGi(Z) XT V2Gi(z)∣∣op + E IGi(Z)- x/ ∙ ∣∣ V3G,(z)以
i∈[d]
i∈[d]
≤ β12 (3 ∙ (X kVGi(z)k) ∙ madx||V2Gi(Z) 11OP + kG(Z)-Xk1∙ maxW3Gi(Z) llop
≤⑴ β12(3 ∙√d ∙ k Jg(Z)If ∙ IIV2G(Z)IIop + √d ∙ ∣G(z) - x∣ ∙ ∣∣V3G(z)∣∣0p)
≤⑵ J2(3d ∙ MM2 + √d ∙ MM3 ∙ ∣z - Zk)
In (1) we use Pi∈的 ∣ɑ∕ ≤ √k ∙ JPi曰用 a2 on the first part of both the terms; and the fact that
∀i ∈ [d], kT (i)kop ≤ kT kop for any tensor T (where T(i) denotes a sub-tensor) on the second part
of both terms. This fact follows from the sup-definition of `2 norm on tensors from (Lim, 2005). in
(2), we use Assumption 2, as well as IlAkF ≤ √α ∙ ∣∣A∣∣op for A being an ɑ X a matrix.
□
30
Published as a conference paper at ICLR 2022
F.3.3 PERTURBATION BOUND OF VL
Using the above, we show the Taylor expansion result for the gradient of L:
Lemma 26 (Perturbation of VL). For all z ∈ Rd, it holds that
VL(z) = z +(I + β JG(Z)T JG(Z))(Z — z) + RVL(Z)
where ∣∣Rvl(z)k ≤ √βMM ∙ ^3√dM^2 + M3∣∣z — Zll)TlZ — Z∣∣2∙
Proof. We Taylor expand VL around Z. Using equation equation 57 We get:
VL(z) = VL(Z) + V2L(Z) (z - Z)+ RVL(Z)
=z + (I + e2 JG(Z)T JG(Z))(Z - Z) + RVL(Z)	(59)
To bound the norm of the remainder term, We note that
∣Rvl(z)k = ⅛ J∣V3L(z)∣z=z	] ∙kz- Zk2
2!	z=zmid op
For Zmid = tmidz + (1 - tmid) Z for some tm,id ∈ [0,1]. And for the third derivative, we use lemma
25 directly to get
∣Rvl(z)∣ ≤ 筌∙ (3√dM2 + M3kz - Zk) ∙kz - Zk2	(60)
This finishes the proof.	口
G Remark on activation functions
The choice of activation functions in our results is largely a matter of convenience. Using standard
techniques from approximation theory, e.g. Hornik (1991); Yarotsky (2017), one can approximate a
neural network with one choice of nonlinearity via a (comparably sized) neural network with another
choice of nonlinearity, under very mild conditions on the nonlinearities. Crucially, this simulation only
increases the size by a dimension-independent factor. This result frees us (for purposes of deriving
an expressibility result) to work with activation functions chosen for mathematical convenience and
produce results that hold without loss of generality.
For instance, if we wished to use ReLU activations, we can use the following lemma. We note that
this proof is almost verbatim the same as the proof of Lemma 1.3 in Telgarsky (2017) and proofs for
other activations like sigmoid or tanh can be written completely analogously.
Lemma 27. Let Ω ⊆ [—M, M]d and let Gi : [—M, M]d → R be a neural network with at most l
layers and n parameters, s.t., the weights W(i) for each layer i and node j in G1 are bounded as,
i.e., ∀i, j : Pk |Wj(,ik) | ≤ B. Furthermore, assume that the activation functions used in G1 belong to
the Set Ξ, s.t. all functions σ : R → R satisfy suPx∈[-b∙m,b∙m] σ < M. Then there exists a neural
network G2 with ReLU activation and O(n (LB)JBM log( (LB)JBM)) parameters, such thatfor all
supx∈[-M,M]d |G1(x) - G2(x)| ≤ .
Proof. For any σ ∈ Ξ, from Theorem 1 in Yarotsky (2017) it follows that there exists a neural network
R with ReLU activations and O( LBM log( LBM)) parameters such that suPx∈[-b∙m,b∙m] ∣σ(x) 一
R(x)| ≤ 0.
We will construct the network G2 by replacing each activation in G1 with the corresponding net-
work R as given by the result above with 0 = /l. Note, this network is at most a factor of
O( (LB)IlBM iog( (LB)IlBM)) bigger than Gι,as the lemma requires.
We will prove the claim of the lemma by induction on l. More precisely, we will show (by induction)
that for each node at layer i, the network G2 calculates a function that is (LB)ii0 away in l∞ norm
from the corresponding node in G1, and the inputs to the node are in [-BM, BM].
31
Published as a conference paper at ICLR 2022
For the base case i = 1, since the input x ∈ [-M, M]d, the result follows by Theorem 1 in [Yarotsky
(2017)].
We proceed to the inductive claim. Let H(x) denote the vector valued mapping computed by
the nodes at layer i, and let HR (x) be the corresponding vector in G2 . As inductive hypothesis,
we assume that kH(x) - HR(x)k∞ ≤ (LB)ii0 for all x ∈ [-M, M]d and kH (x)k∞ ≤ M
as well as kHR(x)k∞ ≤ M. Therefore, for the jth node in layer (i + 1) in network G1 we
have |WjT H (x)| ≤ kWj k1 kH(x)k∞ ≤ BM and σ is bounded by M on this interval, so we
have kσ1(WjTH(x))k∞ ≤ M. Along with the bound on the activations, the part of the inductive
hypothesis about the size of the input is proven. To prove the error bound, we have:
∣σι(WjTH(X))- R(WTHR(X))I ≤ 旧(呼H(X))- σι(WjHR(X))I + ∣σι(Wf HR(X))- R(WTHR(X))I
≤ L|WjT (H (x) - HR(x))| + 0
≤LkWjk1kH(X) - HR(X)k∞ + 0
≤ (LB)i+1(i+1)0
This finishes the proof of the inductive step, and thus the lemma.	□
32