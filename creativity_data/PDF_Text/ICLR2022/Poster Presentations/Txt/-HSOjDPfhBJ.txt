Published as a conference paper at ICLR 2022
PER-ETD: A Polynomially Efficient Emphatic
Temporal Difference Learning Method
Ziwei Guan, Tengyu Xu & Yingbin Liang
Department of Electrical and Computer Engineering
Ohio State University
Columbus, OH 43210, USA
{guan.283, xu.3260}@buckeyemail.osu.edu & liang.889@osu.edu
Ab stract
Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful
method to conduct the off-policy value function evaluation with function approxi-
mation. Although ETD has been shown to converge asymptotically to a desirable
value function, it is well-known that ETD often encounters a large variance so
that its sample complexity can increase exponentially fast with the number of it-
erations. In this work, we propose a new ETD method, called PER-ETD (i.e.,
PEriodically Restarted-ETD), which restarts and updates the follow-on trace only
for a finite period for each iteration of the evaluation parameter. Further, PER-
ETD features a design of the logarithmical increase of the restart period with the
number of iterations, which guarantees the best trade-off between the variance
and bias and keeps both vanishing sublinearly. We show that PER-ETD converges
to the same desirable fixed point as ETD, but improves the exponential sample
complexity of ETD to be polynomials. Our experiments validate the superior per-
formance of PER-ETD and its advantage over ETD.
1	Introduction
As a major value function evaluation method, temporal difference (TD) learning (Sutton, 1988;
Dayan, 1992) has been widely used in various planning problems in reinforcement learning. Al-
though TD learning performs successfully in the on-policy settings, where an agent can interact with
environments under the target policy, it can perform poorly or even diverge under the off-policy set-
tings when the agent only has access to data sampled by a behavior policy (Baird, 1995; Tsitsiklis &
Van Roy, 1997; Mahmood et al., 2015). To address such an issue, the gradient temporal-difference
(GTD) (Sutton et al., 2008) and least-squares temporal difference (LSTD) (Yu, 2010) algorithms
have been proposed, which have been shown to converge in the off-policy settings. However, since
GTD and LSTD consider an objective function based on the behavior policy, which adjusts only
the distribution mismatch of the action and does not adjust the distribution mismatch of the state,
their converging points can be largely biased from the true value function due to the distribution
mismatch between the target and behavior policies, even when the express power of the function
approximation class is arbitrarily large (Kolter, 2011).
In order to provide a more accurate evaluation, Sutton et al. (2016) proposed the emphatic temporal
difference (ETD) algorithm, which introduces the follow-on trace to address the distribution mis-
match issue and thus adjusts both state and action distribution mismatch. The stability of ETD was
then shown in Sutton et al. (2016); Mahmood et al. (2015), and the asymptotic convergence guaran-
tee for ETD was established in Yu (2015), it has also achieved great success in many tasks (Ghiassian
et al., 2016; Ni, 2021). However, although ETD can address the distribution mismatch issue to yield
a more accurate evaluation, it often suffers from very large variance error due to the follow-on trace
estimation over a long or infinite time horizon (Hallak et al., 2016). Consequently, the convergence
of ETD can be unstable. It can be shown that the variance of ETD can grow exponentially fast as the
number of iterations grow so that ETD requires exponentially large number of samples to converge.
Hallak et al. (2016) proposed an ETD method to keep the follow-on trace bounded but at the cost of
a possibly large bias error. This thus poses the following intriguing question:
1
Published as a conference paper at ICLR 2022
Can we design a new ETD method, which overcomes its large variance without introducing a large
bias error, and improves its exponential sample complexity to be polynomial at the same time?
In this work, we provide an affirmative answer.
1.1	Main Contributions
We propose a novel ETD approach, called PER-ETD (i.e., PEriodically Restarted-ETD), in which
for each update of the value function parameter we restart the follow-on trace iteration and update it
only for b times (where we call b as the period length). Such a periodic restart effectively reduces
the variance of the follow-on trace. More importantly, with the design of the period length b to
increase logarithmically with the number of iterations, PER-ETD attains the polynomial rather than
exponential sample complexity required by ETD.
We provide the theoretical guarantee of the sample efficiency of PER-ETD via the finite-time anal-
ysis. We show that PER-ETD (both PER-ETD(0) and PER-ETD(λ)) converges to the same fixed
points of ETD(0) and ETD(λ), respectively, but with only polynomial sample complexity (whereas
ETD takes exponential sample complexity). Our analysis features the following key insights. (a)
The period length b plays the role of trading off between the variance (of the follow-on trace) and
bias error (with respect to the fixed point of ETD), and its optimal choice of logarithmical increase
with the number of iterations achieves the best tradeoff and keeps both errors vanishing sublin-
early. (b) Our analysis captures how the mismatch between the behavior and target policies affects
the convergence rate of PER-ETD. Interestingly, the mismatch level determines a phase-transition
phenomenon of PER-ETD: as long as the mismatch is below a certain threshold, then PER-ETD
achieves the same convergence rate as the on-policy TD algorithm; and if the mismatch is above the
threshold, the converge rate of PER-ETD gradually decays as the level of mismatch increases.
Experimentally, we demonstrate that PER-ETD converges in the case that neither TD nor ETD
converges. Further, our experiments provide the following two interesting observations. (a) There
does exist a choice of the period length for PER-ETD, which attains the best tradeoff between the
variance and bias errors. Below such a choice, the bias error is large so that evaluation is not
accurate, and above it the variance error is large so that the convergence is unstable. (b) Under a
small period length b, it is not always the case that PER-ETD(λ) with λ = 1 attains the smallest
error with respect to the ground truth value function. The best λ depends on the geometry of the
locations of fixed points of PER-ETD(λ) for 0 ≤ λ ≤ 1, which is determined by chosen features.
1.2	Related Works
TD learning and GTD: The asymptotic convergence of TD learning was established by Sutton
(1988); Jaakkola et al. (1994); Dayan & Sejnowski (1994); Tsitsiklis & Van Roy (1997), and its non-
asymptotic convergence rate was further characterized recently in Dalal et al. (2018a); Bhandari et al.
(2018); Kotsalis et al. (2020); Chen et al. (2019); Kaledin et al. (2020); Hu & Syed (2019); Srikant
& Ying (2019). The gradient temporal-difference (GTD) was proposed in Sutton et al. (2008) for
off-policy evaluation and was shown to converge asymptotically. Then, Dalal et al. (2018b); Gupta
et al. (2019); Wang et al. (2018); Xu et al. (2019); Xu & Liang (2021) provided the finite-time
analysis of GTD and its variants.
Emphatic Temporal Difference (ETD) Learning: The ETD approach was originally proposed
in the seminal work Sutton et al. (2016), which introduced the follow-on trace to overcome the
distribution mismatch between the behavior and target policies. Yu (2015) provided the asymptotic
convergence guarantee for ETD. Hallak et al. (2016) showed that the variance of the follow-on trace
may be unbounded. They further proposed an ETD method with a variable decay rate to keep the
follow-on trace bounded but at the cost of a possibly large bias error. Our approach is different and
keeps both the variance and bias vanishing sublinearly with the number of iterations. Imani et al.
(2018) developed a new policy gradient theorem, where the emphatic weight is used to correct the
distribution shift. Zhang et al. (2020b) provided a new variant of ETD, where the emphatic weights
are estimated through function approximation. Van Hasselt et al. (2018); Jiang et al. (2021) studied
ETD with deep neural function class.
Comparison to concurrent work: During our preparation of this paper, a concurrent work (Zhang
& Whiteson, 2021) was posted on arXiv, and proposed a truncated ETD (which we refer to as
2
Published as a conference paper at ICLR 2022
T-ETD for short here), which truncates the update of the follow-on trace to reduce the variance
of ETD. While T-ETD and our PER-ETD share a similar design idea, there are several critical
differences between our work from Zhang & Whiteson (2021). (a) Our PER-ETD features a design
of the logarithmical increase of the restart period with the number of iterations, which guarantees
the convergence to the original fixed point of ETD, with both the variance and bias errors vanishing
sublinearly. However, T-ETD is guaranteed to converge only to a truncation-length-dependent fixed
point, where the convergence is obtained by treating the truncation length as a constant. A careful
review of the convergence proof indicates that the variance term scales exponentially fast with the
truncation length, and hence the polynomial efficiency is not guaranteed as the truncation length
becomes large. (b) Our convergence rate for PER-ETD does not depend on the cardinality of the state
space and has only polynomial dependence on the mismatch parameter of the behavior and target
policies. However, the convergence rate in Zhang & Whiteson (2021) scales with the cardinality
of the state space, and increases exponentially fast with the mismatch parameter of behavior and
target policies. (c) This paper further studies PER-ETD(λ) and the impact of λ on the converge rate
and variance and bias errors, whereas Zhang & Whiteson (2021) considers further the application of
T-ETD to the control problem.
2	Background and Preliminaries
2.1	Markov Decision Process
We consider the infinite-horizon Markov decision process (MDP) defined by the five tuple
(S, A, r, P, γ). Here, S and A denote the state and action spaces respectively, which are both as-
sumed to be finite sets, r : S × A → R denotes the reward function, P : S × A → ∆(S) denotes the
transition kernel, where ∆(S) denotes the probability simplex over the state space S, and γ ∈ (0, 1)
is the discount factor.
A policy π : S → ∆(A) of an agent maps from the state space to the probability simplex over the
action space A, i.e., ∏(a∣s) represents the probability of taking the action a under the state s. At
any time t, given that the system is at the state st , the agent takes an action at with the probability
∏(at∣st), and receives a reward r(st, at). The system then takes a transition to the next state st+ι at
time t + 1 with the probability P(st+1 |st, at).
For a given policy π, we define the value function corresponding to an initial state s0 = s ∈ S
as Vπ(s) = E [Pt∞=0 γtr(st, at)|s0 = s, π]. Then the value function over the state space can be
expressed as a vector Vπ = (Vπ(1), Vπ(2), . . . , Vπ (|S |))> ∈ R|S|. Here, Vπ is a deterministic
function of the policy π. We use capitalized characters to be consistent with the literature.
When the state space is large, we approximate the value function Vπ via a linear function class as
Vθ (s) = φ> (s)θ, where φ(s) ∈ Rd denotes the feature vector, and θ ∈ Rd denotes the parameter
vector to be learned. We further let Φ = [φ(1),φ(2),..., φ(∣S∣)]> denote the feature matrix, and
then Vθ = Φθ. We assume that the feature matrix Φ has linearly independent columns and each
feature vector has bounded '2-norm, i.e., kφ(s)k2 ≤ Bφ for all S ∈ S.
2.2	Temporal Difference (TD) Learning for On-policy Evaluation
In order to evaluate the value function for a given target policy π (i.e., find the linear function
approximation parameter θ), the temporal difference (TD) learning can be employed based on a
sampling trajectory, which takes the following update rule at each time t:
θt+ι = θt + ηt Wt, at) + γθ>φ(St+ι) - θ>φ(St))φ(St),	(I)
where ηt is the stepsize at time t. The main idea here is to follow the Bellman operation update to
approach its fixed point, and the above sampled version update can be viewed as the so-called semi-
gradient descent update. If the trajectory is sampled by the target policy π, then the above TD algo-
rithm can be shown to converge to the fixed point solution, where the convergence is guaranteed by
the negative definiteness of the so-called key matrix A := limt→∞ E (γφ(St + 1) - φ(St))φ>(St) .
3
Published as a conference paper at ICLR 2022
2.3	Emphatic TD (ETD) Learning for Off-policy Evaluation
Consider the off-policy setting, where the goal is still to evaluate the value function for a given target
policy ∏, but the agent has access only to trajectories sampled under a behavior policy μ. Namely, at
each time t, the probability of taking an action at given St is μ(a∕st). Let dμ denote the stationary
distribution of the Markov chain induced by the behavior policy μ, i.e., dμ satisfies d> = d>P∏.
We assume that dμ(s) > 0 for all states. The mismatch between the target and behavior policies can
be addressed by incorporating the importance sampling factor ρ(s, a):= 黑号 into eq. (1) to adjust
the TD learning update direction. However, with such modification, the key matrix A may not be
negative definite so that the algorithm is no longer guaranteed to converge.
In order to address this divergence issue, the emphatic temporal difference (ETD) algorithm has
been proposed by Sutton et al. (2016), which takes the following update
θt+ι = θt + ηtρ(st, at)Ft (r(st,at) + γθ>φ(st+ι) - θ>φ(st)) φ(st).	⑵
In eq. (2), in addition to the importance sampling factor ρ, a follow-on trace coefficient Ft is intro-
duced as a calibration factor, which is updated as
Ft = γρ(st-1, at-1)Ft-1 + 1,	(3)
with initialization F0 = 1. With such a follow-on trace factor, the key matrix becomes negative
definite, and ETD has been shown to converge asymptotically in Yu (2015) to the fixed point
θ* = (ΦτF(I - γP∏)Φ)-1 Φ>Fr∏,	(4)
where F = diag(f (1),f (2),...,f (|S|)) and f (i) = dμ(i) limt→∞ E 因厢=i].
Similarly, the ETD(λ) algorithm can be further derived, which has the following update
θt+1 = θt + ηtρ(st, at) r(st, at) + γθtτφ(st+1) - θtτφ(st) et,
where et is updated as et = γλρ(st-1, at-1)et-1+Mtφ(st) and Mt = λ+(1-λ)Ft, where M0 = 1
and e0 = φ(s0). It has been shown that with a diminishing stepsize (Yu, 2015), ETD(λ) converges
to the fixed point given by θɪ = (ΦτM(I - γλP∏)-1(I — YPn)Φθ) 1 ΦτM(I - γλP∏)-1r∏,
where M = diag(m(1), m(2),..., m(∣S∣)) and m(i) = dμ(i)limt→∞ E [Mt∣st = i].
2.4	Notations
For the simplicity of expression, we adopt the following shorthand notations. Fora fixed integer b, let
ST := st(b+1)+τ, K= at(b+1)+τ, ρτ = μ⅛⅛H and ΦT = Φ(sT). We also define the filtration Ft =
σ s0, a0, s1, a1, . . . , st(b+1)+b, at(b+1)+b, st(b+1)+b+1 . Further, let rπ ∈ R|S|, where rπ(s) =
Pa∈A r(s, a)π(a∣s). Let P∏ ∈ RlSl×lSl, where P∏(s0∣s) = P°∈a ∏(a∣s)P(s0∣s, a). For a matrix
M ∈ RN×n, M(s,∙) denotes its s-th row and M(∙,s) denotes its s-th column. We define Bφ :=
maxs kφ(S)k2 as the upper bound on the feature vectors, and define ρmax := maxs,a ∏S⅛ as the
maximum of the distribution mismatch over all state-action pairs.
3	Proposed PER-ETD Algorithms
Drawbacks of ETD: In the original design of ETD (Sutton et al., 2016) described in Section 2.3,
the follow-on trace coefficient Ft is updated throughout the execution of the algorithm. As a result,
its variance can increase exponentially with the number of iterations, which causes the algorithm to
be unstable and diverge, as observed in Hallak et al. (2016) (also see our experiment in Section 5).
In order to overcome the divergence issue of ETD, we propose to PEriodically Restart the follow-on
trace update for ETD, which we call as the PER-ETD algorithm (see Algorithm 1). At iteration t,
PER-ETD reinitiates the follow-on trace F and update it for b iterations to obtain an estimate Ftb,
where we call b as the period length. The emphatic update operator at t is then given by
Tbt(θ) =Ftbρtbφtb(φtb-γφtb+1)τθ-Ftbρtbφtbrtb,	(5)
4
Published as a conference paper at ICLR 2022
Algorithm 1 PER-ETD(O)
1:	Input: Parameters T , b, and ηt.
2:	Initialize: θ0 = 0.
3:	for t = 0, 1, ..., T do
4:	F update: Ftτ+1 = γρtτFtτ + 1, where τ = 0, 1, . . . , b - 1 and Ft0 = 1;
5:	θ update: θt+ι = ∏θ (θt + ηtFbρb(rb + γθ>φb+1 - θ>φb)φb)
6:	end for
and PER-ETD updates the value function parameter θt as θt+1 = ΠΘ θt - ηtTbt (θt) , where the
projection onto an bounded closed convex set Θ helps to stabilize the algorithm. It can be shown
that limb→∞ E[T(θ)∣Ft-ι] = T(θ) where T(θ) := (Φ>F(I - YPn)Φ) θ - Φ>Fr∏. The fixed
point of the operator T(θ) is θ* defined in eq.(4), which is exactly the fixed point of original ETD.
Definition 1 (Optimal point and e-accurate convergence). We call the unique fixed point θ* of T(θ)
as the optimal point (which is the same as the fixed point of ETD). The algorithm attains an -
accurate optimal point ifits output θτ satisfies ∣∣θτ — θ*∣∣2 ≤ e.
The goal of PER-ETD is to find the original optimal point θ* of ETD, which is independent from
the period length b. Our analysis will provide a guidance to choose the period length b in order for
PER-ETD to keep both the variance and bias errors below the target e-accuracy with polynomial
sample efficiency.
Algorithm 2 PER-ETD(λ)
1	: Input: Parameters T , b, and ηt .
2	: Initialize: θ0 = 0.
3	: for t = 0, 1, . . . , T do
4	:	Set Ft0 = Mt0 = 1 and et0 = φt0
5	:	for τ = 1, . . . , b do
6	:	Ftτ=Ptτ-1γFtτ-1+1, Mtτ=λ+(1-λ)Ftτ, etτ = γλPtτ-1etτ-1 + Mtτφtτ
7	: end for
8	θ update: θt+ι = ∏θ (θt + ηtPb (rb + γθ> φb+1 - θ> φb) eb)
9	: end for
We then extend PER-ETD(0) to PER-ETD(λ) (see Algorithm 2), which incorporates the eligible
trace. Specifically, at each iteration t, PER-ETD(λ) reinitiates the follow-on trace Ft and updates it
together with Mt and the eligible trace et for b iterations to obtain an estimate etb . Then the emphatic
update operator at t is given by
Tλ(θ) = Pbeb (Φb - γφb+1 )> θ -Pbrbeb,	(6)
and the value function parameter θt is updated as θt+1 = ΠΘ θt - ηtTbtλ(θt) . It can be shown that
limb→∞ E hTbtλ(θ)Ft-1i = T λ(θ), where T λ(θ) = Φ>M(I-γλPπ)-1(I-γPπ)Φθ-Φ>M(I-
γλP∏)-1r∏, which takes a unique fixed point θɪ as the original ETD(λ). The optimal point and the
e-accurate convergence can be defined in the same fashion as in Definition 1. It has been shown in
Hallak et al. (2016) that θɪ is exactly the orthogonal projection of V∏ to the function space when
λ = 1, and thus is the optimal approximation to the value function.
4 Finite-Time Analysis of PER-ETD Algorithms
4.1	Technical Assumptions
We take the following standard assumptions for analyzing the TD-type algorithms in the literature
(Jiang et al., 2021; Zhang & Whiteson, 2021; Yu, 2015).
Assumption 1 (Coverage of behavior policy). For all S ∈ S and a ∈ A, the behavior policy μ
satisfies μ(a∣s) > 0 as long as π(a∣s) > 0.
5
Published as a conference paper at ICLR 2022
Assumption 2. The Markov chain induced by the behavior policy μ is irreducible and recurrent.
The following lemma on the geometric ergodicity has been established.
Lemma 1 (Geometric ergodicity). (Levin & Peres, 2017, Thm. 4.9) Suppose Assumption 2 holds.
Then the Markov chain induced by the behavior policy μ has a unique stationary distribution dμ
over the state space S. Moreover, the Markov chain is uniformly geometric ergodic, i.e., there exist
constants CM ≥ 0 and 0 < χ < 1 such that for every initial state s0 ∈ S, the state distribution
dμ,t(s) = P (St = s|so) after t transitions satisfies ∣∣dμ,t — dμkι ≤ CMXt.
4.2	FINITE-TIME ANALYSIS OF PER-ETD(0)
In PER-ETD(0), the update of the value function parameter is fully determined by the empirical
emphatic operator Tt (θ) defined in eq. (5). Thus, we first characterize the bias and variance errors
of Tt(θ), which serve the central role in establishing the convergence rate for PER-ETD(0).
Proposition 1 (Bias bound). Suppose Assumptions 1 and 2 hold. Then we have
E h∣∣T (θt) — E h^Tt(θt)∣Ft-ii∣∣J ≤ Cb (Bφ∣θt — θ*∣2 + eapprox) ξb,
where Eapprox = ∣∣Φθ* 一 Vn ∣∣∞ is the approximation error of the fixed point, ξ = max {γ, χ} < 1,
Bφ = maxs ∣φ(s)∣2, and Cb > 0 is a constant whose exact form can be found in the proof.
Proposition 1 characterizes the conditional expectation of the bias error of the empirical emphatic
operator Tt (θ). Since ξ = max {γ, χ} < 1, such a bias error decays exponentially fast as b increases.
Proposition 2 (Variance bound). Suppose Assumptions 1 and 2 hold. Then we have
E ∣∣∣Tbt(θt)∣∣∣2∣∣∣Ft-1 ≤σ2,
fO(1),	if	Y Pmax	< 1,
where σ2 = O(b),	if	Y Pmax	= 1,
IO ((γ2Pmax)b),	if	Y Pmax	> 1,
(7)
maxs,a
π(a∣s)
μ(als),
where O(∙) is with respect to the scaling of b, and Pmax
Proposition 2 captures the variance bound of the empirical emphatic operator. It can be seen that if
the distribution mismatch is large (i.e., γ2ρmax > 1), the variance bound grows exponentially large
as b increases, which is consistent with the finding in Hallak et al. (2016). However, as we show
below, as long as b is controlled to grow only logarithmically with the number of iterations, such
a variance error will decay sublinearly with the number of iterations. At the same time, the bias
error can also be controlled to decay sublinearly, so that the overall convergence of PER-ETD can
be guaranteed with polynomial sample complexity efficiency.
Theorem 1. Suppose Assumptions 1 and 2 hold. Consider PER-ETD(0) specified in Algorithm 1.
Let the stepsize η = O (t) and suppose the period length b and the projection set Θ are properly
chosen (see Appendix D.3 for the precise conditions). Then the output θT of PER-ETD(0) falls into
the following two cases.
(a)	If γ2Pmax ≤ 1, then E [∣∣θτ — θ"∣∣2] ≤ O (T).
(b)	If Y2Pmax > 1, then E [∣θτ — θ"∣∣2] ≤ O (Ta), where a = 1∕(logι∕ξ(γ2Pmax) + 1) < L
Thus, PER-ETD(0) attains an e-accurate solution with O (ɪ) SamPleS if Y2Pmax ≤ 1, and with
O (ɪi/a) samples if γ2ρmax > 1.
Theorem 1 captures how the convergence rate depends on the mismatch between the behavior and
target policies via the parameter Pmax (where Pmax ≥ 1). (a) If Y2Pmax ≤ 1, i.e., the mismatch is
less than a threshold, then PER-ETD(O) converges at the rate of O (T1), which is the same as that of
on-policy TD learning (Bhandari et al., 2018). This result indicates that even under a mild mismatch
1 < Pmax ≤ 1/Y2, PER-ETD achieves the same convergence rate as on-policy TD learning. (b)
If Y2Pmax ≥ 1, i.e., the mismatch is above the threshold, then PER-ETD(0) converges at a slower
6
Published as a conference paper at ICLR 2022
rate of O (击)because a < 1. Further, as the mismatch parameter Pmax gets larger, the converge
becomes slower, because a becomes smaller.
Bias and variance tradeoff: Theorem 1 also indicates that although PER-ETD(0) updates the
follow-on trace only over a finite period length b, it still converges to the optimal fixed point θ*.
This benefits from the proper choice of the period length, which achieves the best bias and variance
tradeoff as we explain as follows. The proof of Theorem 1 shows that the output θT of PER-ETD(0)
satisfies the following convergence rate:
E [kθτ — θ*k2] ≤O (kθ0-P2) + O (σ) + O (ξ2b) + O(ξb) .	(8)
、{z~*} 、---------{z-----}
variance	bias
If γ2 ρmax ≤ 1, then σ2 in the variance term in eq. (8) satisfies σ2 ≤ O(b) as given in eq. (7), which
increases at most linearly fast with b. Then We set b = O (IoggLso that both the variance and
the bias terms in eq. (8) achieve the same order of O (T), which dominates the overall convergence.
If γ2ρmax > 1, then σ2 in the variance term in eq. (8) satisfies σ2 = O (γ2ρmax)b as given in
eq. (7), Now, we need to set b as b
log(T)
lθg(γ2Pmax) + lθg(1∕ξ)
, where the increase with log T has
a smaller coefficient than the previous case, so that both the variance and the bias terms in eq. (8)
achieve the same order of O (击).Such a choice of b balances the exponentially increasing variance
and exponentially decaying bias to achieve the same rate.
O
4.3	FINITE-TIME ANALYSIS OF PER-ETD(λ)
In PER-ETD(λ), the update of the value function parameter is determined by the empirical emphatic
operator Ttλ(θ) defined in eq. (6). Thus, we first obtain the bias and variance errors of Ttλ(θ), which
facilitate the analysis of the convergence rate for PER-ETD(λ).
Proposition 3. Suppose Assumptions 1 and 2 hold. Then we have
∣∣E [Tλ(θ∕Ft-Ii- Tλ(θt)∣l2 ≤ Cb,λ (Bφkθt - θ^k2 + eapprox) ξb,
where Capprox = ∣∣Φθɪ - Vn k∞ is the approximation error ofthe fixed point, ξ = max{χ, γ} < 1,
Bφ = maxs kφ(s)k2, and Cb,λ is a constant given a fixed λ whose exact form can be found in proof.
The above proposition shows that the bias error of the empirical emphatic operator Ttλ (θ) in PER-
ETD(λ) decays exponentially fast as b increases, because ξ = max {γ, χ} < 1.
Proposition 4. Suppose Assumptions1 and 2 hold. Then we have Eh ∣∣∣Tbtλ(θt)∣∣∣	Ft-L i ≤ σλ2,
where σλ = O (Pmax)∙
Compared with Proposition 2 of PER-ETD(0), Proposition 4 indicates that ETD(λ) has a larger
variance, which always increases exponentially with b when Pmax > 1. This is due to the fact that
the eligible trace etb carries the historical information and is less stable than φtb .
Theorem 2. Suppose Assumptions 1 and 2 hold. Consider PER-ETD(λ) specified in Algorithm 2.
Let the stepsize η = O (t) and suppose the period length b and the projection set Θ are properly
chosen (see Appendix E.3 for the precise conditions). Then the output θT of PER-ETD(λ) satis-
fies E [∣θτ 一 θɪ∣∣2] ≤ O (τ⅛λ), where aχ = iogνnp——)+ι∙ PER-ETD(λ) attains an c-accurate
solution with O ( jʌ ) samples.
Theorem 2 indicates that PER-ETD(λ) converges to the optimal fixed point θɪ determined by the
infinite-length update of the follow-on trace. Furthermore, PER-ETD(λ) converges at the rate of
O (τ⅛λ) which is slower than PER-ETD(O) (as aχ < a) due to the larger variance of PER-ETD(λ).
Bias and variance tradeoff: We next explain how the period length b achieves the best tradeoff
between the bias and variance errors and thus yields polynomial sample efficiency. The proof of
7
Published as a conference paper at ICLR 2022
Theorem 2 shows that the output θT of PER-ETD(λ) satisfies the following convergence rate:
E [kθτ — θλ k2 ] ≤O (k⅛P2)
+o（苧
variance
+ O (学)+ O (ξb).
、	_{Z	J
bias
(9)
In eq. (9), σλ2 in the variance term takes the form σλ2 = O ρbmax as given in Proposition 4. We need
to set b =
log(T)
lθg(ρmax )+lθg(l∕ξ)
so that both the variance and the bias terms in eq. (9) achieve the
Same order of O (Tar). Thus, such a choice of b balances the exponentially increasing variance and
exponentially decaying bias to achieve the same rate.
O
Impact of the eligible trace (via the parameter λ) on error bound: It has been shown that with
the aid of eligible trace, both TD and ETD achieve smaller error bounds (Sutton & Barto, 2018;
Hallak et al., 2016). However, this is not always the case for PER-ETD. Since PER-ETD applies a
finite period length b, the fixed point of PER-ETD(1) is generally not the same as the projection of
the ground truth to the function approximation space. Thus, as λ changes from 0 to 1, depending
on the geometrical locations of the fixed points of PER-ETD(λ) for all λ (determined by chosen
features) with respect to the ground truth projection, any value 0 ≤ λ ≤ 1 may achieve the smallest
bias error. We illustrate this further by experiments in Section 5.2.
5 Experiments
5.1	Performance of PER-ETD(0)
We consider the BAIRD counter-example. The details of the MDP setting and behavior and tar-
get policies could be found in Appendix A.1. We adopt a constant learning rate for both PER-
ETD(0) and PER-ETD(λ) and all experiments take an average over 20 random initialization. We
set the stepsize η = 2-9 for all algorithms for fair comparison. For PER-ETD(0), we adopt one-
dimensional features Φ1 = (0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.37)>. The ground truth value func-
tion Vπ = (10, 10, 10, 10, 10, 10, 10)> and does not lie inside the linear function class.
×107
(a) Comparison of TD, ETD, PER-ETD(0) (b) Tradeoff between bias and variance by b
Figure 1: Performance of PER-ETD(0) and comparison
In Figure 1(a), we compare the performance of of TD, vanilla ETD(0) and PER-ETD(0) with b =
2, 4, 8 in terms of the distance between the ground truth and the learned value functions. It can be
observed that our proposed PER-ETD(0) converges close to the ground truth at a properly chosen
period length such as b = 4 and b = 8, whereas TD diverges due to no treatment on off-policy data
historically, and ETD (0) also diverges due to the very large variance.
In Figure 1(b), we plot how the bias and the variance of PER-ETD(0) change as the period length b
changes. Clearly, small b (e.g., b = 4) yields a small variance but a large bias. Then as b increases
from 4 to 6, bias is substantially reduced. As b continues to increase from 8 to 20, there is a
significant increase in variance. This demonstrates a clear tradeoff between the bias and variance as
we capture in our theory.
5.2 PERFORMANCE OF PER-ETD(λ)
We next focus on PER-ETD(λ) under the same experiment setting as in Section 5.1 and study how λ
affects the performance. We conduct our experiments under three features Φ1 , Φ2 , and Φ3 specified
8
Published as a conference paper at ICLR 2022
(a) Feature Φ1
(b) Feature Φ2
(c) Feature Φ3
Figure 2: Performance of PER-ETD(λ) and dependence on features
(b) Feature Φ2
(c) Feature Φ3
Figure 3: Fixed points of PER-ETD(λ) and project of the value function. (a): θ lies in 1-dimensional
Euclidean space R1 along horizontal direction; (b), (c): θ lies in 2-dimensional Euclidean space R2 .
in Appendix A.2. Figure 2 shows how the bias error with respect to the ground truth changes as
λ increases under the three chosen features. As shown in Figure 2 (a), (b), and (c), λ = 0, 1, and
some value between 0 and 1 respectively achieve the smallest error under the corresponding feature.
This is in contrast to the general understanding that λ = 1 typically achieves the smallest error.
In fact, each case can be explained by the plot in Figure 3 under the same feature. Each plot in
Figure 3 illustrates how the fixed points of PER-ETD(λ) are located with respect to the ground truth
projection (as Vπ projection) for b = 4. Since the period length b is finite, the fixed point of PER-
ETD(1) is not located at the same point as the ground truth projection. The geometric locations of
the fixed points of PER-ETD(λ) for 0 ≤ λ ≤ 1 are determined by chosen features. The bias error
corresponds to the distance between the fixed point of PER-ETD(λ) and the Vπ projection. Then
under each feature, the value of λ that attains the smallest error with respect to the Vπ projection can
be readily seen from the plot in Figure 3. For example, under the feature Φ3 , Figure 3 (c) suggests
that neither λ = 0 nor λ = 1, but some λ between 0 and 1 achieves the smallest error. This explains
the result in Figure 2 (c) that λ = 0.4 achieves the smallest error among other curves.
As a summary, our experiment suggests that the best λ, under which PER-ETD(λ) attains the small-
est error, depends on the geometry of the problem determined by chosen features. In practice, if
PER-ETD(λ) is used as a critic in policy optimization problems, λ may be tuned via the final reward
achieved by the algorithm.
6	Conclusion
In this paper, we proposed a novel PER-ETD algorithm, which uses a periodic restart technique to
control the variance of follow-on trace update. Our analysis shows that by selecting the period length
properly, both bias and variance of PER-ETD vanishes sublinearly with the number of iterations,
leading to the polynomial sample efficiency to the desired unique fixed point of ETD, whereas
ETD requires exponential sample complexity. Our experiments verified the advantage of PER-ETD
against both TD and ETD. Moreover, our experiments of PER-ETD(λ) illustrated that under the
finite period length in practice, the best λ that achieves the smallest bias error is feature dependent.
We anticipate that PER-ETD can be applied to various off-policy optimal control algorithms such as
actor-critic algorithms and multi-agent reinforcement learning algorithms.
9
Published as a conference paper at ICLR 2022
References
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning, pp. 30-37. Elsevier, 1995.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Proc. Annual Conference on Learning Theory
(COLT), pp. 1691-1692. PMLR, 2018.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
mance of q-learning with linear function approximation: Stability and finite-time analysis. arXiv
preprint arXiv:1905.11425, 2019.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Proc. AAAI Conference on Artificial Intelligence (AAAI), 2018a.
Gal Dalal, Gugan Thoppe, BaIaZS Szorenyi, and Shie Mannor. Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. In Proc. Annual
Conference on Learning Theory (COLT), pp. 1199-1233. PMLR, 2018b.
Peter Dayan. The convergence ofTD (λ) for general λ. Machine learning, 8(3-4):341-362, 1992.
Peter Dayan and Terrence J Sejnowski. TD (λ) converges with probability 1. Machine Learning, 14
(3):295-301, 1994.
Sina Ghiassian, Banafsheh Rafiee, and Richard S Sutton. A first empirical study of emphatic tempo-
ral difference learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
Continual Learning and Deep Networks workshop, 2016.
Harsh Gupta, R Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. Proc. Advances in Neural Information Pro-
cessing Systems (NeurIPS), 32:4704-4713, 2019.
Assaf Hallak, Aviv Tamar, Remi Munos, and Shie Mannor. Generalized emphatic temporal differ-
ence learning: Bias-variance analysis. In Proc. AAAI Conference on Artificial Intelligence (AAAI),
2016.
Bin Hu and Usman Ahmed Syed. Characterizing the exact behaviors of temporal difference learn-
ing algorithms using markov jump linear system theory. Proc. Advances in Neural Information
Processing Systems (NeurIPS), 2019.
Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem using emphatic
weightings. Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. On the convergence of stochastic iterative
dynamic programming algorithms. Neural computation, 6(6):1185-1201, 1994.
Ray Jiang, Shangtong Zhang, Veronica Chelu, Adam White, and Hado van Hasselt. Learning ex-
pected emphatic traces for deep RL. arXiv preprint arXiv:2107.05405, 2021.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with markovian noise. In Proc. Annual
Conference on Learning Theory (COLT), pp. 2144-2203. PMLR, 2020.
J Kolter. The fixed points of off-policy TD. Proc. Advances in Neural Information Processing
Systems (NeurIPS), 24:2169-2177, 2011.
Georgios Kotsalis, Guanghui Lan, and Tianjiao Li. Simple and optimal methods for stochastic vari-
ational inequalities, ii: Markovian noise and policy evaluation in reinforcement learning. arXiv
preprint arXiv:2011.08434, 2020.
Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer
Nature, 2020.
10
Published as a conference paper at ICLR 2022
David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017.
A Rupam Mahmood, Huizhen Yu, Martha White, and Richard S Sutton. Emphatic temporal-
difference learning. European Workshop on Reinforcement Learning, 2015.
Jingjiao Ni. Toward emphatic reinforcement learning. Master’s thesis, University of Alberta, 2021.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Proc. Annual Conference on Learning Theory (COLT),pp. 2803-2830. PMLR,
2019.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Csaba Szepesvari, and Hamid Reza Maei. A convergent o (n) algorithm for off-
policy temporal-difference learning with linear function approximation. Proc. Advances in Neural
Information Processing Systems (NeurIPS), 21(21):1609-1616, 2008.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of
off-policy temporal-difference learning. Journal of Machine Learning Research (JMLR), 17(1):
2603-2631, 2016.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674-690, 1997.
Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-
dayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.
Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Finite sample analysis of the
GTD policy evaluation algorithms in markov setting. arXiv preprint arXiv:1809.08926, 2018.
Tengyu Xu and Yingbin Liang. Sample complexity bounds for two timescale value-based reinforce-
ment learning algorithms. In International Conference on Artificial Intelligence and Statistics
(AISTATS), pp. 811-819. PMLR, 2021.
Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale off-policy TD learning: Non-
asymptotic analysis over markovian samples. Proc. Advances in Neural Information Processing
Systems (NeurIPS), 2019.
Huizhen Yu. Convergence of least squares temporal difference methods under general conditions.
In Proc. International Conference on Machine Learning (ICML), pp. 1207-1214, 2010.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Proc. Annual Conference
on Learning Theory (COLT), pp. 1724-1751. PMLR, 2015.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of
stationary values. Proc. International Conference on Learning Representations (ICLR), 2020a.
Shangtong Zhang and Shimon Whiteson. Truncated emphatic temporal difference methods for pre-
diction and control. arXiv preprint arXiv:2108.05338, 2021.
Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson. Provably convergent two-
timescale off-policy actor-critic with function approximation. In Proc. International Conference
on Machine Learning (ICML), pp. 11204-11213. PMLR, 2020b.
11
Published as a conference paper at ICLR 2022
Supplementary Materials
A Specification of Experiments in Section 5
A.1 Experiment settings
The BAIRD counter-example is illustrated in Figure 4, which has 7 states and 2 actions. If the first
action (illustrated as dashed lines) is taken, then the environment transitions from the current state
to states 1 to 6 following the uniform distribution and returns a reward 0; and if the second action
(illustrated as solid lines) is taken, the environment transitions from the current state to state 7 with
probability 1 and returns a reward 1. We choose the target policy as ∏(0∣s) = 0.1 and ∏(1∣s) = 0.9
for all states; and choose the behavior policy as μ(0∣s) = 6/7 and μ(1∣s) = 1/7 for all states.
Moreover, we specify the discount factor γ = 0.99.
Figure 4: BAIRD example (Sutton & Barto, 2018)
A.2 Features for Experiments in Section 5.2
In the experiment in Section 5.2, we choose the following features:
Φ1 =(0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.37)>;
Φ2 =((0.3425, 0.0171)>, (0.1902, 0.4248)>, (0.1354, 0.76)>, (0.1357, 0.7973)>,
(0.8674, 0.8774)>, (0.5166, 0.9493)>, (0.3094, 0.8535)>)>;
Φ3 =((0.5162, 0.9013)>, (0.5128, 0.5999)>, (0.289, 0.4649)>, (0.3399, 0.5334)>,
(0.315, 0.2278)>, (0.667, 0.461)>, (0.3706, 0.1457)>)>.
A.3 COMPUTATION OF THE FIXED POINT OF PER-ETD(λ)
In this section, we provide the steps to compute the fixed point of PER-ETD(λ) in Figure 3. We first
define the matrix A and c as follows
a ：= t→∞ E [a C= (ρbeb(Φb - γφb+1)>)i,
C := t→ιn E IctC= Pbrbeb)].
It can be shown that, the fixed point of PER-ETD(λ) algorithm is θ* = A-1c.
We next show how to derive the formulation of the matrix A and vector c. As we will show later in
eqs. (51), (53) and (55), we have
A = lim E [At∣Ft-ι]= βb(I - TPn)Φ,	(10)
t→∞
c = lim E [ct ∣Ft-ι] = βbr∏,	(11)
t→∞
where ∣3b := limt→∞ E [βb] and
βb(S)= λφ>Dμ,b + (I- λ)φ>Fb + Yλβb-1Pπ ,	(12)
12
Published as a conference paper at ICLR 2022
where Dμ,τ = diag(dμ,τ), dμ,τ(s) = P(sT = s∣Ft-ι), Fb = diag(fb), and f is determined
iteratively by eq. (25) as follows
fb = dμ,b + γP∏ fb-1,	with fo = dμ,0.	(13)
Taking expectation on both sides of eqs. (12) and (13) with respect to Ft-1 and letting t → ∞ yield
fb = dμ + YPnfb-1, with fo = dμ,	(14)
Bb = λΦ>Dμ + (1 - λ)Φ>Fb + γλβb-ιPπ, with βo = Φ>Dμ	(15)
where fb := limt→∞ E[fb] and Fb = diag(fb). The explicit formulation of βb can be derived by
applying eqs. (14) and (15) iteratively. We can then obtain A and c by substituting the obtained
formulation of βb into eq. (10) and eq. (11), respectively.
A.4 Replotted Figures 1 and 2 with Variance Bars
In this subsection, we replotted Figures 1 and 2 with variance bars (rather than error bands) in
Figures 5 and 6, respectively.
O I 「-Tr
O 1	2
——PER-ETD(O)r b=4
—PER-ETD(O)h b=6
——PER-ETD(O)r『8
---PER-ETD(O), b=10
—PER-ETD(O)r b=16
——PER-ETD(O)f b=20
(a)	Comparison of TD, ETD,
(b)	Tradeoff between
3	4	5
t	XlO7
bias and variance by b
(b) Feature Φ2
(a) Feature Φ1
Figure 5: Performance of PER-ETD(0) and comparison
(c) Feature Φ3
Figure 6: Performance of PER-ETD(λ) and dependence on features
B	More Experiments
In this section, we conduct further experiments to answer the following two intriguing questions:
•	If the distribution mismatch parameter ρmax changes, how will different approaches per-
form and compare with each other?
•	Focusing on our algorithm PER-ETD, how do the choices of behavior policy and target
policy affect its convergence?
13
Published as a conference paper at ICLR 2022
Z = Hl电里
(a) ρmax = 1.17
(b) ρmax = 5.60
Figure 7: Comparisons of TD, ETD, PER-ETD(0) With different target policies
Figure 9: Performance of PER-ETD(0) under different behavior policies (marked by their different
resulting distribution mismatch parameter ρmax). The target policy is kept the same.
Figure 8: Performance of PER-ETD(0) under different target policies (marked by their different
resulting distribution mismatch parameter ρmax). The behavior policy is kept the same.
We focus on the MDP environment in Appendix A.1. In our experiments, the performance of every
algorithm is averaged over 20 random initializations and the error band in our plots captures the
actual variation of the performance during these experimental runs (which can be viewed as the
variance of the algorithms).
In Figure 7, we consider two settings with the distribution mismatch parameter ρmax = 1.17 and
5.60, respectively, and compare the performance of three off-policy algorithms: TD, vanilla ETD
and our PER-ETD. More specifically, We choose the behavior policy as μ(1∣s) = 7 and μ(2∣s) = 7
14
Published as a conference paper at ICLR 2022
for all states, and choose two target polices, whose probabilities to take the second action are 0.167
and 0.8, respectively, on all states. (Then their probabilities to take the first action are determined
automatically through π(1∣s) + ∏(2∣s) = 1). Therefore, the maximum distribution mismatch Pmax
for the these two target policies are 1.17 and 5.60, respectively. Figure 7 (a) shows that under only
slightly mismatch (i.e., ρmax = 1.17), TD suffers from a large convergence error (i.e., the error with
respect to the ground truth value function at the point of convergence) and converges slowly. Vanilla
ETD converges with the fastest rate and achieves a smaller convergence error than TD, but suffers
from a relatively large variance. Our PER-ETD achieves a better tradeoff between the convergence
rate and the variance (faster rate than TD, almost the same convergence error as ETD but with
smaller variance). Figure 7 (b) shows that under a large distribution mismatch (i.e., ρmax = 5.6),
TD does not converge, and vanilla ETD experiences a substantially large variance. However, our
PER-ETD still convergences fast as long as the period length b is chosen properly, e.g., b = 4, 6, 8.
Further note that PER-ETD has a smaller convergence error as the period length b increases, but the
variance gets larger; which are consistent with our theorem.
In Figure 8, we focus on our PER-ETD, and study how different target policies affect the per-
formance. We choose the same behavior policy as the above experiment, i.e., μ(1∣s) = 7 and
μ(2∣s) = 7 for all states. We choose 5 target polices, whose probabilities to take the second action
are 0.167, 0.2, 0.4, 0.6, 0.8 for all states, respectively. These different target policies affect the per-
formance via their resulting distribution mismatch ρmax = 1.17, 1.40, 2.80, 4.20, 5.60, respectively.
For both b = 4 and b = 6, Figure 8 indicates that larger mismatch causes slower convergence rate,
larger convergence error and larger variance, which agrees with our theorem.
In Figure 9, we also focus on our PER-ETD, and study how different behavior policies affect the
performance. We pick the target policy to be ∏(1∣s) = 0.1 and ∏(2∣s) = 0.9 for all states, and 5
different behavior policies with μ(2∣s) = 0.2,0.4,0.6,0.7 and 0.8 for all state, respectively. These
different behavior policies affect the performance via their different resulting distribution mismatch
ρmax = 1.12, 1.29, 1.5, 2.25, 4.5, respectively. Figure 9 clearly demonstrates that larger distribution
mismatch results in slower convergence rate, larger convergence error and larger variance, which is
in the same nature as changing the target policy shown in Figure 8 and is consistent with our theorem.
C S upporting Lemmas
The following lemma is well-known. We include it for the convenience of our proof.
Lemma 2. Consider a transition matrix P ∈ RN ×N, where j P(i,j) = 1 for all i and 0 ≤
P(i,j) ≤ 1 for all j. We have for any n ∈ N, kPnk∞ = 1, and k(P>)nk1 = 1.
Lemma 3. Consider 0 < p,q < 1, with P = q. We have Pn-I Pkqn-k ≤ m匕 ξn, where
ξ = max{p, q}.
Proof of Lemma 3. If P > q, we have
n-1
X Pkqn-k
k=0
n-1
Pk+1qn-1-k
k=0
n-1
≤ X Pn-m qm .
m=0
≤
Without loss of generality, we assume P < q and ξ = q. We have
n-1
X Pkqn-k
k=0
qn ∙ XX(P)k = qn ∙ 1 - (PZq)n ≤ -ɪ ∙ qn =——
k=0 q	1 - p/q q - p	Ip - q|
□
Lemma 4. Consider a matrix P ∈ RN×N where	j P(i,j) ≤ C for all i and 0 ≤ P(i,j) ≤ 1 for all
j, and a positive vector x where x ∈ RN and xi ≥ 0 for all i. We have
1>Px ≤ C1>x.
15
Published as a conference paper at ICLR 2022
ProofofLemma 4. We have
1>Px = X Pi,jXi = X g ( X Pi,j J ≤ C X Xi = C 1τx.
1≤i,j≤N	1≤i≤N	∖1≤j≤N )	1≤i≤N
□
Lemma 5. Consider a diagonal matrix D ∈RS×S, where D = diag(d(1),d(2),...,d(∣S∣)). a
transition matrix P ∈ R∣S∣×∣S∣ where Pj Pij = 1 for all i = 1, 2,..., ∣S∣ and Pij ≥ 0 for all j,
and an arbitrary vector X ∈ R∣S∣. We have
∣∣Φ>DPx∣∣2 ≤ Bφ∣∣d∣∣ι∣∣χ∣∣∞.
ProofofLemma 5. We have
Φ>D = (d(1)φ(1), d(2)φ(2),..., d(∣S∣)φ(∣S∣)),
and
(Φ> DPR)= X Ps,sd⑻φ⑻,
S
which implies
φ>DPX = XX(S) (XP⅛,sd(g)φ(g)) = X (XX⑶PSjd⑻φ⑻.
Taking '2 norm on both sides of the above equality yields
MDPXll2= X(XX(S)PS) d(S)φ(S) |
≤ X XX(S)PS,s ∙∣d⑸HI。⑶∣∣2
S S
=Bφ∣∣d∣∣ι∙ max EX(S)P专
S
≤ Bφ∣∣d∣∣ι∣∣X∣∣∞ max EPS,s
S
=Bφ IldIlIIlX∣∣∞∙
□
Lemma 6. Consider a diagonal matrix D ∈ R∣s∣×∣s∣ where D = diag(d(1), d(2),..., d(∣S∣)), a
transition matrix P ∈ R∣s∣×∣s∣ where Pj Pij = 1 for all i = 1, 2,..., ∣S∣ and Pij ≥ 0 forall j, a
matrix Q ∈ R∣s∣×∣s∣ that satisfies 0 ≤ Qij ≤ CPij, where C > 1 is a constant, and an arbitrary
vector X ∈ R|S|. We have
trace (QmPnΦΦτD) ≤ CmBφ∣∣d∣1,
and,
trace (PnQmΦΦTD) ≤ CmBφ∣∣d∣1,
for any m and n ∈ N≥o.
Proof of Lemma 6. For any given T ≥ m ≥ 0 and n ≥ 0,
(i)
I(QmPnΦΦτ)(ij) I = K(Qm)(i,∙), (PnΦΦτ)(∙j)> I ≤ k(Qm)(i,∙)kιk(PnΦΦτ)(∙j)k∞,	(16)
16
Published as a conference paper at ICLR 2022
where (i) follows from the Holder,s inequality.
Furthermore, for the term of (PnΦΦ>)( j), We have,
k(PnΦΦ>)(∙,j)k∞ ≤ kPnk∞k(ΦΦ>)(∙,j)k∞ =) k(ΦΦ>)(∙,j)k∞,
where (i) follows from Lemma 2.
Moreover, for the ith entry of (ΦΦ>)(.j), we have
(ΦΦ>)(i,j)= φ(i)>φ(j) ≤ kφ(i)k2kφ(j)k2 ≤Bφ2
The above uniform bounds over all i imply that k (ΦΦ>)(.j) ∣∣∞ ≤ Bφ. Hence,
k(PnΦΦ>)(∙,j)k∞ ≤ Bφ.
Substituting the above inequality back into eq. (16), we obtain
(i)
I(QmPnΦΦ>)(i,j)∣ ≤ Bφk(Qm)(i,∙)kι ≤ Cmk(Pτ-m)(i,∙)kιB2
≤ CmBφ X Pτ-m(j∣i) = CmBφ,	(17)
j
where (i) follows by the condition of Q, Q(i,j) ≤ CP(i,j) for all i, j.
Finally, we have
trace (QmPnΦΦ>D) = Xd⑶(QmPnΦΦ>)(i,i) ≤ X |d⑺| ∣(QmPnΦΦ>)(i,i)∣
ii
(i)
≤ ∣d∣1max∣∣(QmPnΦΦ>)(i,i)∣∣ ≤ CmBφ2∣d∣1,	(18)
where (i) follows from eq. (17).
Following steps similar to those in eqs. (16) to (18), we can obtain
trace (PnQmΦΦ>D) ≤ CmBφ∣∣d∣ι.
□
Lemma 7. The operators T(θ) and T λ(θ) satisfy the generalized monotone variational inequality.
There exist μo,μλ > 0, St, "(θ), θ 一 θ*∖ ≥ μo∣θ — θ*∣2 ,and (Tλ(θ), θ 一 θ*) ≥ μλ∣∣θ 一 θ*∣2.
Proof of Lemma 7. We have
hT(θ),θ — θ*i =) <(Φ>F(I — γP∏)Φ) (θ — θ*),θ — θ*>
=(θ — θ*)> (Φ>F(I — γPπ)Φ) (θ — θ*)
≥ λmin (Φ>F(I — γP∏)Φ) ∣θ — θ*∣2,	(19)
where (i) follows from the definition of the T and θ*.
Recall that F(I — γPπ) is positive definite(Sutton et al., 2016; Mahmood et al., 2015) and Φ has
linearly independent columns. For any x ∈ Rd with x 6= 0, we have Φx 6= 0 and
x> Φ> F(I — γPπ)Φx = (Φx)> F(I — γ Pπ)(Φx) > 0.
The above inequality shows that Φ>F(I — γP∏)Φ is positive definite and thus, there exists μo > 0
such that μo =入m加(Φ>F(I — γP∏)Φ).
Following steps similar to those in eq. (19) and applying the positive definiteness of M(I —
γλPπ)-1(I — γPπ) (Sutton et al., 2016; Mahmood et al., 2015) yield
〈Tλ(θ),θ — θ*> ≥ μλkθ — θ*k2.
□
17
Published as a conference paper at ICLR 2022
Lemma 8. The operators T(θ) and T λ(θ) satisfy the Lipschitz condition. There exist L0, Lλ > 0,
such that, kT(θ1) - T (θ2)k2 ≤ L0kθ1 -θ2k2,andTλ(θ1) -Tλ(θ2)2 ≤ Lλkθ1 -θ2k2.
Proof of lemma 8. We have
kT(θι) -T(Θ2)k2 = ∣∣(Φ>F(I-YPn)Φ) (θι -θ2)∣∣2
≤ Φ>F(I-γPπ)Φ2kθ1 -θ2k2.	(20)
Let L0 := Φ>F(I - γPπ)Φ2, eq. (20) completes the proof of the first inequality in the Lemma.
Let Lλ := kΦ>M(I - γλPπ)-1(I - γPπ)Φk2, the steps similar to those in eq. (20) finalizes the
proof of the second inequality in the Lemma.	□
Lemma 9 (Three point lemma). Suppose Θ is a closed and bounded subset of Rd, and θ* is the
solution of the following maximization problem, maxθ∈θ η hG, θ∖ + 11 ∣∣θ 一 θ0k2, where G ∈ Rd is
a vector. Then, we have, for any θ ∈ Θ,
ηhG,θ*-θi + 2kθ0-θ*k2 ≤ 2kθ0-θ∣2 -1 kθ*-θ∣2∙
Proof. The proof can be found in Lan (2020).	□
D Proofs of Propositions and Theorem for PER-ETD(0)
D.1 Proof of Proposition 1
τ-,∙	.	1	.1	1 r'	/ 八 ∖	1
First, by the definition ofTt(θt), we have
E hTbt(θt)Ft-1i
(=i)XXX
s∈S a∈A s0∈S
• P (Sb = s, ab = a, sb+1 = sf∖Ft-i) E ∣T(θtRFt-i, Sb = s,ab = a, sb+1 = s]
(=)χχχ P (Sb = s∖Ft-i) μ(alS)P(S0|s, aɔ
s∈S a∈A s0∈S
• E ρtbFtbφ(S)[φ>(S)θt - r(S, a) - γφ>(S0)θt]∖Ft-1, Stb = S, atb = a, Stb+1 = S0
(=i) XXX P (Sb = S\Ft-i) n(a|S)P(SlS,a)0(S)[o>(S)θt — rGa) —Y。%0)%]
s∈S a∈A s0∈S
• E [Ftb∖^t-uSb = s]
=XP (Sb = s∖Ft-ι)E [Ftb∖Ft-ι, Sb = s]φ(S) [φ>(S)θt — γ[Pπφ](s,∙)θt — r∏(S)], QI)
s∈S
where (i) follows from the law of total probability, (ii) follows from rewriting
P (sb = s, ab = a, sb+1 = s0∖Ft-1), and (iii) follows from the facts that Ftb only depends
on (S0, a0, S1, a1, . . . , St(b+1)+b-1, at(b+1)+b-1) and the chain is Markov.
Recall the definition of T (θt), we have
T(θt) =Φ>F[(I-YPπ)Φθt -rπ]
=X f (S) (φ>(s)θt - γ[Pπ φ](s,∙)θt - r∏(S)) φ(s).	(22)
s∈S
Equations (21) and (22) together imply the following,
T(θt) — E [T(θt)∖Ft-ι] = X (f (s) — P (Sb = s∖Ft-ι) E [Ftb∖Ft-ι, Sb = s])
s∈S
18
Published as a conference paper at ICLR 2022
• (φ>(s)θt — Y [Pπφ](s,∙) θt - rπ(S)) φ(s)
=X (f (s)-九(S)) (φ>(s)θt - γ [P∏Φ](s,) θt - r∏(s)) φ(s)
s∈S
=X (f (s) - fb(s))((I - γPπ) Φθt - r∏)s φ(s),
s∈S
where in (i) We define fb(s) := P (Sb = SlFt_J E [Fb∣Ftτ, Sb = s]. Taking '2 norm on both
sides of the above equality yields
∣T(θt) - E [T(θt)∣Ft-1]∣2
-r∏)s φ(S)
2
≤ E If (S) - fb(S)∣∙∣((I - 7Pπ) Φθt - rπ )s∣∙kΦ(S)k2
s∈S
≤ max {kφ(s)k2} max {∣((I-YPn) φθt - r∏)s∣} X If(S)- fb(s)∣
s∈S	s∈S	/ '
s∈S
=Bφk (I - YPn )φθt - rπ ∣∣∞∣∣f - fb ∣∣1∙	(23)
We next proceed to bound ∣∣f - fb11. Consider fb(S), we have
fb(S)= P (Sb = SIFt-I) E [Ftb|Sb = s, Ft-1]
-P (Sb = s I Ft-1)
X P (SbT =飞、ab-1 = ɑ I sb = s, Ft-I)
s∈s,a∈A
⑻
• EbPbTFtbT + 1∣Ft-1, sb = s , SbT= 5,姆-1 = &
P ( S b = S I Ft-I)
∕1 l L P (Sb-1 = 5∣ Ft-1) μ(a∣5)P( S∣5,5)W
1 s∈S⅛∈A	P ( Sb = S ∣ Ft-1)	E
π(a∣s)
Yμ(α∣s)
P (Sb = S ∣ Ft-1) + YE P(Sb-1 = 5∣Ft-I)Pn(S∣5)E 第T ∣ Ft-1, SbT - S
s∈s
where (i) follows from the law of total probability and (ii) follows from the Bayes rule and the facts
that FtbT only depends on the chain elements (S 0, α0, S 1,..., SbT,ab-1) and the Markov property.
Define dμ,b(s) = P (S b = S ∣ Ft-1), the above equality can be rewritten as
fb(S)= d”,b( s) + Y X Pn ( S ∣5)fb-1(5)∙	(24)
S
Since eq. (24) holds for all S ∈ S, we have
fb = d“,b + YP>fb-1.	(25)
Note that for f We have the following holds (Sutton et al., 2016; Zhang et al., 2020a)
f = % + YP>f.	(26)
Equations (25) and (26) imply
f - fb = dμ - dμ,b + YP>(f - fb-1).
Applying the above equality recursively yields
b-1
f - fb = X(YPn )τ (dμ - dμ,b-τ ) + Yb(Pn )b(f - fθ).
T =0
19
Published as a conference paper at ICLR 2022
Take 'ι norm on both sides of the above equality, We have
b-1
kf -加1 = X YT (Pn )τ (dμ - dμ,b-τ ) + Y b(P∏ )b(f - fθ)
T =0
b-1
≤ X YTIl 闯)τ (dμ- dμ,b-τ )∣I1 + Yb l∣(Pj )b(f - f0)∖∖1
T = 0
b-1
≤ X γτ∣∣(Pj )τ∣∣ι kdμ - d”,b-τk1 + Yb ∣∣(Pj )b∣∣kf - f0k1
T = 0
(i)	b-1
≤ EYT lldμ - dμ,b-τ ∣∣1 + Ib Ilf - f0∣∣i
T = 0
(ii)	b-1
≤ ECM Y TX—T + Yb ∣∣f - f0∣∣1
τ =0
(ii)	1
≤ iX-^i ∙Cmξb + Yb(1 + Ifk 1),	(27)
where (i) follows from Lemma 2, (ii) follows from Lemma 1, and (iii) follows from Lemma 3 and
defining ξ := max{χ, y}.
To bound the term ∣(I - YPr)Φθt - rπ∣∣∞, we proceed as following
Il(I - Ypπ )φθt - rπ ∣∣∞ = Il(I - Ypπ )(φθt - Vr )∣∣∞
=k(I - YPn)(Φθt - Φθ* + Φθ* -咚)k∞
≤ ∣I - YPn k∞(∣Φθt - Φθ*k∞ + ∣Φθ* -咚 k∞)
(ii)
≤ (1+ Y)Bφ∣θt-θ*∣2 + (1+ Napprox,
(28)
where (i) follows from the fact V∏ = (I - YPn)-1r∏ and (ii) follows from the facts that ∣∣I -
YPr Il ∞ = maxi{ 1 - (Pn ) (i,i) + Y P j=i (Pr ) (i,j) } ≤ 1 + y，^approx := Il φθ - Vr ∣∣ ∞, and
∣Φθt - Φθ*∣∞ = maχφτ(s)(θt - θ*) ≤ Bφ∣θt - θ*∣∣2∙
s∈S
Substituting eqs. (27) and (28) into eq. (23) yields
∣∣T(θt) - E [^Tt(θt)∣Ft-ι]∣∣2 ≤ (Bφ∣θt - θ*∣∣2 + Bφeapprox) (1 + Y) (C⅛ + Yb(1 + ∣f ∣1))
≤ Cb (Bφ∣∣θt - θ* ∣∣2 + CaPPrOX) ξ,
where ξ = max{χ,γ} and Cb = Bφ(1 + γ) (TC^ + (1 + IfIi)).
D.2 Proof of Proposition 2
According to the definition of Tt(θt), we have
E ∣∣T(θt)∣∣2∣Ft-ι
=E [(ρbFb)2 (θjΦb - rt -何φb+1)2 ∣φb∣2∣Ft-i]
-XXX P (Sb = s,ab = α, sb+1 = S0∣Ft-1) ∙ (θJφ(s) - r(s, O)-YIφ(s'))2
s∈S a∈A s0∈S
• ∣φ(s)∣2 ∙ E [(ρbFtb)2 ∣Ft-i, Sb = s,αb = a, sb+1 = Sl
(=)XXX P (Sb = SIFt-I) μ(α∣S)P(Sls, a) • (θJφ(s) - r(s, a) - Yθ>φ(s'))2
s∈S α∈A s0∈S
20
SPl°-(6Z) ∙b0s∙sso:swbo£ OMl°AOqE ətp0βu∙sn∙ssqns
fz)ysFwG
⅛1≈ =e6 Ls)CL(-⅞)kMM
、-JLfz>ysFwG
WdwES) dτ⅛^MM
) (D)Zk Γz
0AEq osɪŋ°M
OE) Ke¥z+ 营K(ZL+IHVI
ae¥z+=B = (='S)0 与+=S) 0≡vl
aeM+===('s)0: (S)0 = ZB
e$)2 Z +⅞H(('s)tl (S)0)一 Zvlc⅛B('s)Htle-SM lφ(s)H0)
AEq əʌv ∙u20Jd o-ənp 2
JoJ 6F一 PUEX 6(6) UE JOJ 20}VD 6Ns UE JoJ 0一0=ql ə-oN
uəasuE PuB ss snAOJd UlalJIuəp
luədəpuɪ⅛q 二 ɔEJ ətp UlalJ SMO=OJ () PUEUdEqOJd WJo MEl1 日 OjJ SMIOJəjə^m
ZZOZ xuOIaJOdEd əɔuəjəjus E SE poqs=qnd
3
a) 工工JN)1
J?工一 7q国7qmp - (S)ZH
f 一 三 rcE
7LsiR厂邑出 ^等:(≡⅛s7q√Mill)
WuLe 芯 UL?SUAlK丁立L≈月 Cg 等)d"βrsl-工谭M(S)q√sl
W =tD 芯=THS u-79γ⅛7苴出
ysm
(79 $ =至2= ILVg = ILs)⅛M(S)q√≡ll
》JLSiRΓ⅛L≈SJ
OAEq əMzs) .bə Jo SHx ə£ unəi PUoɔəs əjo工
S ∙ ts=-7f (w(Ld)-as)qww +
fs = * 一工κ7⅛L≈as)q√LZ + (S) q√H
》=^7κ一 Z (L工)Z(I⅛)" +LHLds+二as)q√=
F = HT£ Z (I + TqHILdVt 一a7≤s U w⅛=(Se
Ks) q∖ JOj pq suopnbəMI0J əəAEq 段
∙fs =X二l⅛z(⅛)-aS) q√=IS=X二—9 一 Z (⅛)a(7κs Us⅛=二 Seəugəɑ
fz)ws
(IE) ∙ fs =^7⅛Z (出)£ (7κs U 5s)⅛κκ飞¥Z + 苗苗:< + I)W 2dVl
Published as a conference paper at ICLR 2022
where (i) follows from the law of total probability, (ii) follows from the Bayes rule, (iii) follows
from Markov property and (iv) follow from the definition of fb which is given above eq. (23).
For the third term on the RHS of eq. (32), we have
dμ,b(s)E [(ρb-1Fb-1)2∣Ft-i,sb = s]
"dμ,b(S) X P (SbT= s,ab-1 = a∖sb = s,Ft-1)
ses,aeA
- E [(ρbTFb)T)21Ft-1, Sb = s, SbT = M ab-1 =可
(=)dμ,b(s) X d“，b-1(”(?”(S区 a) ∙ E [(ρb-1Ftb-1)2 ∣Ft-1, Sb = s, SL= s, ab-1 = a]
s,a	dμ,b(S)
= X dμ,b-1 (S)μ(a∖S)P(s∖S,S) ∙ π2⅛ ∙ E [(Ftb-1)2∣ Ft-1,sb-1 = S]
(竺)X Pμ,∏(SlS)rb-1(S)
SeS
=(Pμ,πrb-1)s ,
(34)
where (i) follows from the law of total probability, (ii) follows from the Bayes, rule, and in (iii)
we define Pμ,π ∈ RlSl×lSl where (Pμ,π )s,s = PaeA 彳(翡)P(s∖s, a) for each (s, s) &S × S.
Substituting eqs. (33) and (34) into eq. (32) yields
rb = dμ,b + 2γPnfb-I + Y2PInrb-1.
We also have the following inequality holds
1>rb = 1>dμ,b + 2γ1>P> fb-1 + γ21>P>∏rb-i
=1 + 2γ1>fb-i + Y21>P]∏ rb-1
(ii)	丁	2	丁
≤ 1 + 2γ 1 fb-1 + Y PmaXI rb-1,
(35)
where (i) follows from 1>P> = (P∏ 1)> = 1>, and (ii) follows from the facts that rb-ι 占 0 and
1τPL =	(Pμ ∏1)T=veC(XX	π2⅛P(s∖S,5)!	=VeC(X	π2⅛! Y PmaX1>.
μ	( μ,π )	号与〃(乖)(1,)) U	μ(a∣s)) Y
Recursively applying eq. (35) yields
b-1
1Trb ≤ X(Y2Pmax)T (1 + 2γ 1Tfb-T-1) + (Y2Pmax)b1>r0
T =0
b-1
=X(Y2Pmax)τ (1 + 2γ1τfb-τ-1) + (YPmaX^,	(36)
τ=0
where (i) follows from the fact that 1τro = 1.
Recall that fb = dμ,b + γP>fb-1 and f = dμ + γP>f. We have
1τ(fτ - f ) = 1τ(dμ,τ - dμ) + γ 1 TP∏(fτ-1 - f) = γ 1τ(fτ-1 - f) (=) γτ 1τ(fo - f),
where (i) follows from the facts that dμ,τ and dμ are both probability distributions and 1τdμ,τ =
1τdμ = 1 and 1 TPT = 1τ and (ii) follows from recursively applying (i).
Thus, we have
∖1τfτ∖ = ∖(1 - γT)1τf + γτ 1τfo∖ ≤ ∖(1 - γτ)1τf∖ + γT ≤ kfk1 + 1.	(37)
22
Published as a conference paper at ICLR 2022
Substituting eq. (37) into eq. (36) yields
b-1
1>rb ≤ X(γ2ρmax)τ(3 + 2γkfk1) + (γ2ρmax)b.
τ=0
Under different conditions of ρmax, the term 1>rb is upper bounded differently as following:
(a). γ2ρmax > 1
1>rb ≤
3 + 2YkfkI
γ2ρmax - 1
+ 1 γ 2b ρbmax .
(38)
(b)	. γ ρmax = 1
1>rb ≤ (3+2γkfk1)b+ 1.	(39)
(c)	. γ2 ρmax < 1
1>rb ≤ h* + 1.
1 - γ ρmax
(40)
Substituting the above inequalities into eq. (31), we can upper-bound the term E Tbt(θt)	Ft-1
under different conditions accordingly:
(a)	. γ2ρmax > 1
E [∣∣T(θt)∣∣2b"≤ Pmax(4(1 + Y2)B2Bθ + 2rm0,)Bφ (二2Ykfl +l) Y2
2	γ ρmax - 1
= Cσ,1Y ρmax ,
Where We specify Cσ,1 = Pmax (4(1 + Y2)BφB2 + 2τrn0x) Bφ (号£ + 1)
(b)	. Y ρmax = 1
E ∣∣T(θt)∣0Ft-i ≤ Pmax(4(1+ Y2)Bφ Bθ +2rm 0x) Bφ ((3 + 2γ ∣f∣ι) b +1)
= Cσ,2b,
Where We specify Cσ,2 = Pmax 4(1 +Y2)Bφ2Bθ2 + 2rm2 ax Bφ2 (4+2Ykfk1).
(c)	. Y2Pmax < 1
E ∣∣T(θt)∣0Ft-1 ≤ Pmax(4(1 + Y2)BφBθ +2*ax) Bφ (3 + YγPfk1 +1)
:= Cσ,3 .
where we sPecify Cσ,3 = Pmax (4(1 + Y2)BφB2 + 2rm0x) Bφ ( ：*[[： + 1).
To summarize, the variance term Tbt (θt)	can be bounded as folloWing
E	Tbt(θt)2Ft-1 ≤σ2,
Where
(O(1),	if Y 2Pmax < 1.
σ2 = < O(b),	if Y 2Pmax = 1.
IO((Y 2Pmax) ), ifY2P
max > 1.
23
Published as a conference paper at ICLR 2022
D.3 Proof of Theorem 1
Theorem 3 (Formal Statement of Theorem 1). Suppose Assumptions 1 and 2 hold. Consider
PER-ETD(O) specified in Algorithm 1. Let the StePsize η = *。e+%),where to = ^LO, μo
is defined in Lemma 7, and L0 is defined in Lemma 8 in Appendix C. Let the projection set
Θ = {θ ∈ Rd : ∣∣θ∣∣2 ≤ Bθ}, where Bθ = kφι-Yrmax (which implies θ* ∈ Θ). Then the Con-
vergence guarantee falls into the following two cases depending on the value of ρmax.
(a)	If CR C	≤ 1 let b —— max Jl log(μ0) log(5cbBφ) ] _Iog T_ O where CA is a ConStant defined
(a)	Lj	Y	PmaX	≤∙	ɪ, etι U —	imɑʌ ɪ	log(ξ)	, log(1∕ξ)	J , w'',e'e	'^^^^7b	CS a consiam	uejmeet
in the proof of Proposition 1 in Appendix D.1, Bφ := maxs∈S kφ(s)k2, and ξ := max{γ, χ}. Then
the output θT satisfies
E [kθτ -θ*k2] ≤O (T
(b)	IJ Mo > 1 let b — max! Γlog(μ0)-log(5cbBΦ)l _______________Iog(T)________1 Where Ch is a
(b)	J γ Pmax > 1, let b = maxt∣ iog(ξ) y iog(γ2Pmaχ)+iog(i∕ξ) J, where Cb IS a
constant whose definition could be found in the proof of Proposition 1 in Appendix D.1, Bφ :=
maxs∈S kφ(s)k2, and ξ := max{γ, χ}. Then the output θT satisfies
E[kθτ -θ*k2] ≤o (TIa ),
where a = lθgl∕ξ(γ21ρmax) + 1	1-
Thus, PER-ETD(0) attains an e-accurate solution with O (ɪ) samples if Y2ρmax ≤ 1, and with
O (备)samples if YPmax > 1.
Proof. Note the θ update specified in Algorithm 1 is the closed form solution of the following
maximization problem.
θt+ι = argmaʌηt Dft(Bt)忘 + 1 ∣∣θ - θtk2∙
θ∈Θ	2
*	1	∙ T	/ʌ ∙ . < Zi-J=	八	^r- / Zi ∖	t Zi	Zi ∙ ι 1 Γ∙	Zi _ rʌ
Applying Lemma 9 With θ*	= θt+ι, η	= ηt,	G = Tt(θt), and θo	= θt yields, for any	θ ∈ Θ,
η DT(θt), θt+ι - θE + 2kθt - θt+ιk2 ≤ 2kθt- θk2 - 2kθt+ι - θ∣∣2∙	(41)
Proceed With the first term in the above inequality as folloWs
DTbt (θt), θt+1 - θE
= hT (θt+1), θt+1 -θi+hT(θt) -T(θt+1),θt+1 -θi+ DTbt(θt) - T (θt), θt+1 -θE
≥ hT (θt+1), θt+1 - θi - L0∣θt - θt+1 ∣2 ∣θt+1 - θ∣2 + DTbt(θt) -T (θt), θt+1 - θE
= hT (θt+1), θt+1 -θi -L0∣θt -θt+1∣2∣θt+1 - θ∣2 + DTbt(θt) - T (θt), θt+1 -θtE
+ DTbt(θt) -T(θt),θt -θE
≥ hT(θt+l), θt+1 - θi - Lo kθt - θt+1k2kθt+1 - θk2 -IT(θt) - T (θt)ll2 ∙ kθt+1 - θt∣2
+ DTbt(θt) -T(θt),θt -θE,
Where (i) folloWs from the Cauchy-SchWartz inequality and Lemma 8.
Substituting the above inequality into eq. (41) yields
ηt hT(θt+1),θt+1 - θi- ηtLο∣θt - θt+1k2kθt+1 - θ∣2 - ηt ∣∣T(θt) - T(θt)∣2 ∙ ∣∣θt+ι - θtk2
24
Published as a conference paper at ICLR 2022
+ η DT(θt) - T(θt), θt - θE + 2kθt - θt+ιk2 ≤ 2llθt - θk2 - 2llθt+ι - θk2.	(42)
Applying Young's inequality to ηt ∣∣7t(θt) - T(θt)∣∣2 ∙ ∣∣θt+ι - θt∣∣2 yields
η ∣∣T(θt) - T(θt)∣2 ∙kθt+ι - θt∣2 ≤ 4kθt+ι - θtk2 + η2 ∣∣T(θt) - TMl2,
and applying Young’s inequality to ηtL0lθt - θt+1l2 lθt+1 - θl2 yields
ηtLokθt - θt+ιk2kθt+ι - θk2 ≤ 4kθt - θt+ιk2 + η2L2kθt+ι — θ∣∣2∙
Substituting the above two inequalities into eq. (42) yields
2kθt - θk2 ≥ nt hT(θt+ι), θt+ι - θi + (2 - η2Lθ) kθt+ι - θk2
+ηtDTbt(θt)-T(θt),θt-θE-ηt2 lllTbt(θt)-T(θt)lll2.
Taking expectation conditioned on Ft-1 on the both sides of the above inequality, we obtain
2 kθt - θk2 ≥ ntE [hτ (θt+1), θt+1 - θilFtt-1]+ 12 - n2L2) E [kθt+1 - θk2∣F't-l]
+ηtDE hTbt(θt)	-T(θt)Ft-1i	,θt	-θE -ηt2E	lllTbt(θt) - T (θt)lll2Ft-1	.
(43)
Letting θ = θ* and applying Lemma 7 to eq. (43) yields
2kθt-θ*k2 ≥ (2 + μ02 -n2Lθ) E [kθt+ι- θ*k2∣Ft-ι]
-ntCbBφξb kθt - θ* k2 - ntCb^approxξbkθt - θ* k2
- ηt2E lllTbt(θt) - T(θt)lll2∣∣∣Ft-1
≥ (2 + μ0nt - n2Lθ) E [kθt+1 - θ*k2∣Ft-l]
-ntCbBφξb kθt - θ*k2 - ntCbCapproχξb∣θt - θ*∣2 - 4n2c2B2ξ2b∣θt - θ*∣2
- 4ηt2Cb2ξ2b2approx - 2σ2ηt2,	(44)
where (i) follows from Propositions 1 and 2, and the facts that (x + y)2 ≤ 2x2 + 2y2 and
lllTbt(θt) -T(θt)lll2 ≤2 lllE hTbt(θt)∣∣∣Ft-1i -T(θt)lll2+2 lllE hTbt(θt)∣∣∣Ft-1i -Tbt(θt)lll2
≤2 lllTb (θt)lll2 + 2 lllEhTbt(θt)∣∣∣Ft-1i -Tbt(θt)lll2.
Taking expectation on both sides of the above inequality yields
2 + μo2 - n2L2) E [kθt+ι - θ*ll2]
≤ (2+ CbBφξbnt + 4C2Bφξ2bnfj E h∣θt- θ*k2] + ntCbBθeapproχξb
+ 4ηt2Cb2ξ2b2approx + 2σ2ηt2 .
8L2
Recall that we set to = T. Let at = (t + to + 1)(t + to + 2). Multiplying 2at on both sides of
μ0
the above inequality and telescoping from t = 0, 1, 2, . . . , T - 1 yields
T-1
X at (1 + 2μont - 2n2Lo) E [kθt+ι - θ"l2]
t=o
25
Published as a conference paper at ICLR 2022
T-1
≤ X αt (1 + 2CbBφξbηt + 8C^Bφξ2b宿)E 卜包-θ*∣同
t=0
T-1	T-1
+ (4σ2 + 8c2ξ2beapprox) ^X αtηt + 2CbB0^approxξ ^X αtηt∙	(45)
t=0	t=0
Recall the setting of ηt, we have
1 + 2μ0ηt - 2η2L0
where ⑶follows from the fact that 1 -陪
above inequality yields
≥ 1 - μ8Lt^ ≥ 0. Multiplying at on both sides of the
at(1 + 2μ0ηt — 2η2L2) ≥ (t + t0 + 1)(t + t0 + 2)
1+
3μ0	2
2 μ0(t + t0)
(t +10 + 1)(t +10 + 2)(t +10 + 3)∕(t + 力。).	(46)
Under appropriate value of b, We have CbBφξb ≤ 詈.Which implies that
2CbBφξbηt + 8C2B2ξ2bη2 =等 + 等(4CB优 + MB:- 1)
V μ0ηt	μ0ηt (4	16μ0ηt _ 1)
≤ ~T~ + ~1Γ(5 + ^^25	)
≤ μ0ηt + μ0ηt ( 4μ0 - 1)
一 ~T~ 十-2^ 125L0 ~5J
μ0ηt
≤ ---.
_ 2
Multiplying αt+ι on both sides of the above inequality yields
αt+1(1+2CbB φ ξbηt+ι+8C2Bφ ξ2bη2+1) ≤ αt+ι (1+μ0ηt+1)
r Λ , μ0ηt+1λ
≤ αt+1 (1+^H
=(t +10 + 2)(t +10 + 3) (1 + μ0-/, , ,	1 -∣ɔ
∖	2 μ0(t +10 + 1)√
=(t +10 + 2)2 (t +10 + 3)∕(t +10 + 1).	(47)
Equations (46) and (47) together imply that
αt(1 + 2μ0ηt - 2η2L2) - αt+1(1 + 2CbBφξr∏t+ 8C2Bφξ2bη2)
(t +10 + 1)(t +10 + 2)(t +10 + 3)	(t +10 + 2)2 (t +10 + 3)
≥----------------------------------------------------------
t + t0	t + t0 + 1
=(t++，2?+1+ ；)3)((t+10+1)2 - (t+咐(+10+2))
(t + t0 + 2)(t + 力。+ 3)
(t + t0)(t + t0 + 1)
> 0.
26
Published as a conference paper at ICLR 2022
The above inequality shows that the ∣∣θt - θ*k2, t = 1,..., T - 1, terms on both sides of eq. (45)
can be canceled, which indicates the following
(T + tO)(T + to + I) (1 + 2μoητ-ι - 2ηT-ιL2) E [kθτ - θ*∣2]
≤ (to + 1)(to + 2) (1 + 2CbBφξbηo + 8C2Bφξ2bη2) ∣θo - θ*∣2
T-1	T-1
+ (4σ2 + 8C2ξ2beapproχ) X at褚 + 2C°Bθ"χξ X a"	(48)
t=o	t=o
Note that Pt=Olatn2 ≤ Pt=OI μ62 ≤ 6T2, 1 + 2μoητ-1 - 2ηT-1LO ≥ 1, and
T-1	4 T-1	2
£ atnt ≤ —)：(t + to + 2) ≤ —(T + to + 2)2.
=	μo =	μo
Dividing (T + to)(T + to + 1) (1 + 2μoητ-ι - 2nT_“2) on both sides of eq. (48) yields
E [kθτ-θ*k2]
≤	(to + 1)(to + 2)
(T + to)(T + to + 1)
1 + 管)kθo-θ*k2
+ 24σ2 + 48Cbξ2b ^approx	1	+ 4CbBθ ^approx ξb	(T + to + 2)2
μ2	T + to + 1	μo (T + to + I)(T + to)
O (kθo T2θ2k2) + O (σ2) + O (CbT- ) + θ(Cbξb) .	(49)
Based on different conditions of σ2, we pick different b and the convergence rate is as follows.
(a). γ2ρmax ≤ 1, Proposition 2 show that σ2 ≤ O(b). We specify
b = rnox ʃ llogWO)-Iog(5CbBe)m logT ∖ ≤ o(iop∙(T))
b = maxt I	iog(ξ)	hog(1∕ξ) j≤o(IOg(T)).
Equation (49) yields,
E[kθτ -θ*k2] = O (kθo T2θ2k2) + O (lɪ) + O (T) + O (T) = O
(b). γ2ρmax > 1, Proposition 2 show that σ2 = O (γ2ρmax)b . We specify
b = max∕ ∣~log(μ0)-log(5CbBφ) ]	log(T)	1
[I	log(ξ)	I , log(Y2Pmax)+ log(I阳 J .
Equation (49) yields
E[kθτ -θ*k2]
kθo-θ2k2
+O
+O
+O
□
E PROOFS OF PROPOSITIONS AND THEOREM FOR PER-ETD(λ)
E.1 Proof of Proposition 3
Define the matrix At := ρtbetb(φtb - γφtb+1) and ct := rtbρtbetb. We have
Tbtλ(θt) = Atθt - ct.	(50)
Recall that θt is Ft-1-measurable. We have
E hTbtλ(θt)Ft-1i = E [At|Ft-1] θt - E [ct|Ft-1].
27
Published as a conference paper at ICLR 2022
To bound the bias error term ∣∣E [方，(%)肉-J - T λ(θt)∣∣
on At and ct, respectively, as following
,We first take conditional expectations
2
E[At∣Ft-ι]
E [ρbeb(φb - 7≠b+1)τ∣Ft-1]
- X P (Sb = s,ab =生 sb+1 = s0∣Ft-ι)
s∈S,α∈A,s0∈S
- E [ρbeb(φb -γφb+1)TIFt-ι,sb = S芯=α,sb+1 = s0]
(=)X p (Sb - SIFt-1) μ(a∣s)P(s0∣s,a) ∙ "(a∣s)EIebISb = s, Ft-1] (φ(s) - γφ(SO))T
s,a,s0
=XP (Sb = SI Ft-I) EIeb I Ft-i,s = s] X	π(α∣S)P(SIsM (φ(S)- γφ(SO))T
s∈S	α∈A,s0∈S
EP (sb = S I Ft-1) E [eb I Ft-1, Sb = s] ∙ (&储,)-γ(P∏Φ储,)),	(51)
s∈S
where (i) follows from the law of total probability and (ii) follows from the Markov property and
the fact that eb only depends on (s0, a0, St,..., sb).
Define βτ(s) = P (ST = s∣Ft-1) E [eb I Ft-1, ST = s] . We have
βb(s) = P (sb = SI Ft-1) E [eb∣ sb = S, Ft-1]
-P (Sb = s i Ft-I)
∙ X P (SbT = 3, ab-1 = G 1 sb = s, Ft-I)
s ∈S,α∈A
∙ E [γλρb-1eb-1 + (λ +(1 - λ)(1+ PbTYFtb-1)Φb) ∣ 4-1 = G, ab-1 = &花=s, F1]
(=)P (Sb = S 1 Ft-I) φ(s)
+ P (Sb = S I Ft-I)	X
s∈S,a∈A
π(a∣s)
∙ —；~~：~~
μ(α∣s)
P (Sb = Si Ft-I)
∙ E [γλeb-1 + (1 - λ)γFb-1Φ(s) ∣ SbT = M Ft-1]
=P (Sb = S 1 Ft-I) φ(s) + EP (SbT = g 1 Ft-I) pπ (SIG)
a∈s
∙ E [γλeb-1 + (1 - λ)γFb-1Φ(s) ∣ SbT = G, F-]
("(λd”,b(s) + (1 - λ)fb(s)) ∙ φ(s) + 7λ(Pjβb-1)s,	(52)
where (i) follows from the law of total probability, (ii) follows from the Bayes rule
and the Markov property, and (iii) follows from the following definitions: d*,b(s) =
P (sb = s∣Ft-1), fb(S) = dμ,b(s)E [Ftb = S∣sb = S,Ft-1],fb = dμ,b + YPnfb-1,andβτ(s)=
P (ST = SIFt-I) E [eb I Ft-1,sτ = s].
Define the matrix βτ ∈ Rd×∣S∣, where βτ = (βτ(1), βτ(2),..., βτ(∣S∣)). Then, eq. (52) implies
that
βb = λφ>D*,b + (1 - λ)φTFb + Yλβb-1pπ ,	(53)
where Dμ,b ：= diag(dμ,b(1), dμ,b(2),..., dμ,b(∣S∣)) and Fb = diag(fb).
Recursively applying the above equality yields
b-1	b-1
βb = (γλ)bβoP∏ + λ X(γλ)τΦτDμ,b-τP； + (1 - λ) X(γλ)τΦτFb-τPf.	(54)
T=0	T =0
28
Published as a conference paper at ICLR 2022
Taking expectation of Ct conditioned on Ft-1, We have
E[ct∣Ft-i ]= E [ρbeM∣Ft-i]
= X P (Sb = s,at = aFt-I) E [ρbebrb I st = s,at = a, Ft-1]
s ∈S,α∈A
= X P (Sb = SI Ft-I) μ(α∣s) ∙ π(a∣s) r(s,α)E [eb I Sb = S, Ft-1]
s∈S,α∈A	M(a|S)
=X rπ(S)P (Sb = S1 Ft-I) E [et 1 Sb = s, Ft-1]
s∈S
=X r∏ (S)Bb(S).	(55)
s∈S
Substituting eqs. (51) and (55) into eq. (50) yields
E [Tλ(θt) I Ft-1] = Xβb(S) (Φθt - γP∏Φθt - Tn)s = βb (Φθt - γP∏Φθt - Tn).
s∈S
Recall the definition of Tλ(θ). We have
Tλ(θt) - E [Tλ(θt) I Ft-1] = (ΦτM(I - γλPπ)-1 - βb) (Φθt - YPnΦθt - Tn).	(56)
(=)Φτ (λDμ + (1 - λ)F)
We then proceed to bound the term ΦτM(I - γλP∏)-1 - βb
ΦτM(I - γλPn)-1 - βb
=ΦτM 佟(γλ)τPn) - βb
)τ Pn
b-1	b-1
(γλ)bβ0P∏ + λ X(γλ)τΦτDμ,b-τP∏ + (1 - λ) X(γλ)τΦτFb-τP∏
T = 0	T =0	.
b-1	b-1
λ X(γλ)τΦτ (Dμ - Dμ,b-τ) P； + (1 - λ) X(γλ)τΦτ (F - Fb-τ) P；
T =0	T = 0
∞
+ X(γλ)τΦτ(λDμ + (1 - λ)F)P； -λ(γλ)bΦτDμ,0Pb
T=b
where (i) follows from the fact that (I - YPn)-1 = P∞=0 YTPr, and (ii) follows from eq. (54).
Substituting the above equality into eq. (56) and taking '2 norm on the both sides yield
ITλ(θt) - E [^Ttλ(θt) I Ft-1 ]∣2
=II (λ X(Yλ)τ Φτ (Dμ - Dμ,b-τ) Pf + (1 - λ) X(γλ)τ Φτ (F - Fb-T) Pf
Il ∖ T=0	T=0
∞	∖
+ X(γλ)τΦτ(λDμ + (1 - λ)F)PT - λ(γλ)bΦτDμ,0Pb 1 (Φθt - γPπθt - Tn)
T=b	/	2
b-1
≤ λ X(γλ)τ ∣∣Φτ (Dμ - Dμ,b-τ) PT (Φθt - γPnΦθt - Tn)∣∣2
T = 0
b-1
+ (1 - λ) X(γλ)τ ∣∣Φτ (F - Fb-T)Pf (Φθt - γ2Φθt - Tn)∣∣2
T = 0
29
Published as a conference paper at ICLR 2022
∞
+ X(Yλ)τ ∣∣Φτ (λDμ + (1 - λ)F) Pn (Φθt - YPnΦθt-r∏)∣∣2
T=b
+ λ(Yλ)b ∣∣ΦτD”,0Pb(Φθt - YPnΦθt -r)∣∣2
(i) b-1
≤ λ ^2(Yλ)τBφ∣dμ - dμ,b-τ ∣1 (1 + Y)(Bφ∣θt - θʌ ∣∣2 + capprox)
T =0
b-1
+ (1 - λ) X(Yλ)τ Bφ Ilf - fb-τ∣ι (1 + Y )(Bφ∣θt - θɪ ∣2 + C Capprox)
T = 0
∞
+ ∑(Yλ)τBφ (λ∣dμ∣ι + (1 - λ)∣∣f∣∣ι)(1 + Y) (Bφ∣θt - θλ∣2 + 5…)
τ=b
+ λ(Yλ)bBφ∣∣dμ,0∣∣1(1 + Y)(Bφ∣∣θt - θλ ∣∣2 + capprox，
(ii)
≤ (Bφ∣∣θt - θλ ∣∣2 + Bφcapprox)
∕b-1
- X(Yλ)τ (λ(1 + Y)CMχb-τ + (1 - λ)(1 + Y) (∣⅛ξ'
Yb-τ (1 + kfkι)
∖τ=0
+ (I + Y) (B2 kθt - θλ l∣2 + Bφ CaPPrOX) ((	_ ^^	+ λ^ (Yλ)b
(iii)
≤ (Bφ ∣∣θt - θλ ∣∣2 + Bφcapprox)
- (1 + Y) ("CMξb +1⅛(⅛)ξb +(1 + kfkι)γb +
(iv)
≤ Cb,λ (Bφ∣θt - θλ ∣∣2 + capproX) ξ,
λ+(I-段川1 + q(Yλ)b)
where (i) follows from Lemma 5 and eq. (28), (ii) follows from Lemma 1 and eq. (27), in (iii) We
define
Cb,λ := Bφ(1 + Y) (κ⅛τCM + r¾∣⅛⅜) + 1 + ∣f∣1 + (λ+(I-段川1 + λ)),
and (iv) follows from Lemma 3.
E.2 Proof of Proposition 4
According to the definition of Ttx, We have
E ∣ 恒(θt)∣ ∣ 2卜I
=E [(ρb)2 (rb + Yθ>φb+1 - θ>φb)2 (eb)Tebyt-1]
- E P (Sb = S芯=a, sb+1 = s0∣Ft-I)
s ∈S,a∈A,s0 ∈S
- E [")2 (rt + Yθ>φb+1 - θ>φb)2 (eb)TeblSb = s,at = a, st+1 = s0, Ft-1]
(==)	E P (Sb - s∣Ft-i) μ(α∣s)P(s0∣s, a)
s∈S,a∈A,s0∈S
μj^(r(s, a) + Yθ>φ(s0) - θ>φ(s))2E [(eb)TebISb = s, F1]
(iii)
≤ EP (Sb - S∣Ft-i) E [(eb)TebISb = S, Ft-1]
s∈S
- E p”,π (SlS)(r(S, a) + Y0(s')T% - φ (S)Tθt)2
s∈s
30
Published as a conference paper at ICLR 2022
(iv)	__
≤ Pmax(4(1 + Y2)BφBθ + 2琮ox) Bφ £ P (Sb = SIFT) E [⑶)TebISb = s, F1],
s∈S
(57)
where (i) follows from the law of total probability, (ii) follows from the Markov property and
the fact that eb only depends on (s0, a0,..., sb), and (iii) follows from eq. (30) and the fact
≤ ρmaχ.
Σso Pμ,π (SiS)
Define Δb(s) = P (Sb = S∣Ft-ι) E [(eb)TebISb = s, Ft-ι]. We then proceed to bound the term
△b(S). We have
∆b(s)
-P (Sb = SIFt-1)
X	P(SbT = s,ab-1	= G 1 Sb	= s, Ft-I) E	[(eb)Teb 1 SbT	= S,	αb-1	=	a, sb	= s,	Ft-1]
s∈s,a∈A
(=)P (Sb = SIFt-1)
X P (Sb-1 = ： I Ft-Il〃(a|：)P(S区 aɔ E [(eb)Teb i SbT = G, ab-1 = G, Sb = s, Ft-1]
念A	P (sb = s∣Ft-1)	" " t∣ t , t	, t	, t 1j
S ∈ , a ∈
(i-) X P (SbT=司 Ft-1) μ(α团P(S∣G,G)
s∈s,a∈A
• E [(7λρb-1eb-1 + (λ + (1 - λ) (7ρb-1Ftb-1 + 1)) Φb)>
• (7λρb-1eb-1 + (λ + (1 - λ) (7ρb-1Ftb-1 + 1)) Φb) ∣ SbT = G, ab-1 = &, Sb = s, Ft-1]
= X P (SbT =司 Ft-1) μ(α团P(s∣S,G)
α∈s,a∈A
E [(γλ)2(ρb-1)2(eb-1 )>eb-1 + (1 - λ)2γ2(ρb-1Fb-1)2Φ(S)TΦ(s) + φ(s)>φ(s)
+2γ2λ(1 - λ)(ρb-1)2Ftb-1Φ(s)TebT + 2γλρb-1φ(s)TebT
+2(I- λ)γρb-1Ftb-1φ(S)Tφ(s) 1 SbT = s, ab-1 = a, sb = s, Ft-1]
=P (sb = s∣Ft-1) M(S)II2
+ Φt(s) XPn(S团P (sb-1 = Q∣ Ft-1) E [2γλeb-1 + 2γ(1 - λ)Fb-1 Φ(s) ∣ SbT = S, Ft-1]
a∈s
+ λ2γ2 X P”,n(S团P (sb-1 = SI Ft-1) E [(eb-1)TebT ∣ SbT = S, Ft-1]
a∈s
+ 2γ2λ(1 - λ)∑P4,n(s∣S)P (sb-1 = S I Ft-1) E [Ftb-1φ(s)TebT I SbT = S,Ft-1]
a∈s
+ MS)k2γ2(1 - λ)2 XP”,n(s∣S)P (sb-1 = S I Ft-1) E [(Ftb-1)2I sb-1 = S, Ft-1]
a∈s
≤ B2d”,b(s) + 2γλφτ(s)(βb-1Pn)(∙,s) + 2γ(1 - λ)Bφ(PTfb-I)S
+ λ2γ2 (Pjnδ5-I)S + γ2(1 - λ)2Bφ (Pjπrb-1)s
+ 2γ2λ(1 - λ) X P4,n(s∣S)P (sb-1 = S I Ft-1) E [Ftb-1φ(s)TebT I SbT = S, Ft-1]
a∈s
(=)B2d”,b(s) + λ272(⅛∆b-1)s + 2γ(1 - λ)B2(Pnfb-1)s + 2γλφτ(s)(βb-1Pn)(∙,s)
+ Y2(1 - λ)2B2(PIrb-1 )s + 2γ2λ(1 - λ)φ(s)T(δb-1P4,n)(∙,s),	(58)
where (i) follows from the law of total probability, (ii) follows from the Bayes rule, (iii) follows
the update rule of eb and in (iv) We define
δτ(S) = P (ST = SIFt-I) E [Ftre"sτ = s, Ft-1].
31
Published as a conference paper at ICLR 2022
Summing eq. (58) over S yields
1>∆b = X ∆b(s)
S
≤ Bφ 1>dμ,b(s) + λ2γ21>P>πδb-i + 2γ(l — λ)B21>p∏>fb-1 + 2γλtrace (φBb-∖P∏)
+ γ2(1 — λ)2Bφ1>P]∏rb-1 + 2γ2λ(1 — λ)trace (Φδb-iPμ,∏)
(i)	丁	丁
≤ λY?Pmax1 ∆b-1 + Bφ + 2Y(I- λ)Bφ1 fb-1 + 27λtrace (Φβb-1Pπ )
+ γ2(1 - λ)2B2 Pmax1>rb-1 + 2Y2λ(1 - λ)trace (φδb-1 pμ,π ),
where (i) follows from Lemma 4 with P = Pμ,∏.
Recursively applying the above inequality, we have
b
1>∆b ≤ (λ2γ2)bρmαχ1>∆0 + X(λ2γ2)b-τ(Pmax)b-τ 网I + 2γ(1 - λ)Bφ 1>fτ-1
T =1
+2γλtrace (φet-1 Pn ) + Y2(1 - λ)2B2Pmax1>rτ-1 + 2Y2λ(1 - λ)trace (φδτ-1Pμ,π )).
(59)
Substituting eq. (54) into trace (ΦβτP∏) with b and T replaced by T and m respectively yields
trace (ΦβτP∏)
=(γλ)τtrace (Φβ0P∏+1)
τ — 1
+ λ X (γλ)m (trace (ΦΦτDμ,τ-mP∏n+1) + (1- λ)trace (ΦΦτFT-mP∏m+1))
m=0
(=)(γλ)τtrace (P7T +1ΦΦτDμ,0)
τ — 1
+ λ X (γλ)m (trace (P∏m+1ΦΦτDμ,τ-m) + (1- λ)trace (P∏m+1ΦΦTFT-m))
m=0
(iii)	T—1
≤ (γλ)τBφ 1τdμ,0 + Bφ E(Yλ)m (λ∣∣dμ,τ-m∣∣1 + (1 - λ)kfτ-mk1)
m=0
(iv)	C 1 — (γλ)τ	C
≤ (Yλ)τBφ + -I) (λ + (1 - λ)(1 + kf k1)) Bφ
φ 1 — γλ	φ
B2
≤ rɪ(1 + (1-λ)kfk1),
1	— γλ
(60)
where (i) follows from eq. (54), (ii) follows from the facts that trace(AB) = trace(BA) and
δ0 = ΦτDμ,0, (iii) follows from Lemma 6 with P = P∏, and (iv) follows from eq. (37).
Next, we proceed to bound the term trace (Φτδτ-1Pμ,∏).
δb-1(≡) = P (SbT=司Ft-I) E [Ftb-1 (γλρb-2eb-2 + (λ + (1 - λ)Fb-1)。⑸)∣ SbT = S, Ft-1]
(=)(1 - λ)P (SbT = S I Ft-1) E [(Ftb-1)2 I SbT = S, F1] φ(S)
+ λP (SbT = S ∣ Ft-1) E [Ftb-1 ∣ SbT = S, F1] φ(S)
+ γλP (SbT= S ∣ Ft-1) E[ρb-2eb-2 ∣ SbT= S, F1] φ(S)
+ Y2λP (SbT = S ∣ Ft-1) E [(ρb-2)2Ftb-2eb-2 ∣ SbT = S Ft-1] Φ(S)
(=i) (1 - λ)rb-1(S)φ(S) + λfb-1(S)φ(S)
+ P (Sb-1 = S∣ Ft-1) X P (Sb-2 = S00, αb-2 = a〃 ∣ SbT = S, Fy)
s〃,a〃
32
Published as a conference paper at ICLR 2022
• E [γλρb-2eb-2 + γ2λ(ρb-2尸Fb-2eb-2∣sb-2 = s00,ab-2 = /瞽T =S, Ft-1]
(1 - λ)rb-i(S)φ(S) + λfb-i(S)φ(S)
+ P (SbT
sSFt-1
s00,a00
P (Sb-2 = s00∣Ft-i) μ(a00∣s00)P(S∣s00, a00)
P (SbT = SlFt-I)
• E	eb-2 + Y2λ∏2W]Ftb-2eb-2
2 = S00, Ft-1
(1 - λ)rb-i(S)φ(s) + λfb-i(s)φ(s) + γλ(βb-2P∏)(∙,s) + γ2λ(δb-2Pμ,∏)(∙,s),
where (i) follows from the update of etb-1, (ii) follows from the update rule of Ftb-1, and (iii)
follows from the law of total probability.
The above equality implies that
δb-i = (1 - λ)Φ>diag("ι) + λΦ>diag(fb-i) + Y邓b-2P∏ + Y2λδb-2P∏,μ.
Recursively applying the above equality yields
b-1
δb-i = X (Φ>((1 - λ)diag(rm) + λdiag(fm)) + γλβm-P∏)(P∏,μ)b-1-m
m=1
+ (Y 2λ)b-1δo(P∏,μ)b-1.
Note that the above inequality holds for any fixed b >= 2. As a result, by changing of notation, for
all τ ≥ 1, we have
τ
δτ = X (Φ>((1 - λ)diag(rm) + λdiag(fm)) + γλβm-iP∏)(P∏,μ)τ-m + (γ2λ)τδ°(P∏,μ)τ.
m=1
(61)
Substituting eq. (61) into trace (ΦδτPμ,∏), We have
trace(Φδτ Pμ,∏)
= trace(φ( X (Φ>((1 - λ)diag(rm) + λdiag(fm)) + γλβm-iP∏)(P∏,μ)τ-m
m=1
+ (Y2λ)τδθ(P∏,μ)) Pμ,∏)
τ
=) X trace ((P∏,μ)τ-m+1 (ΦΦ>((1 - λ)diag(rm) + λdiag(fm)) + γλΦβm-P∏))
m=1
+ trace ((γ2λ)τ(P∏,μ)τ +1Φδo) ,	(62)
Where (i) folloWs from the fact that trace(AB) = trace(BA) and trace(A + B) = trace(A) +
trace(B).
Applying Lemma 6 with Q = Pμ,∏, C = ρmaχ, P = P∏ and D = (1 - λ)diag(rm), We have
trace ((P∏,μ)τ-m+1 ΦΦ>(1 - λ)diag(rm)) ≤ (1 - λ)ρm-m+1Bφ|“仙.	(63)
Applying Lemma 6 with Q = Pμ,∏, C = pmax, P = P∏ and D = λdiag(fm), we have
(i)
trace ((P∏,μ)τ +1-mΦΦ>λdiag(fm)) ≤ λρT,+lX-mBφkfmkι ≤ λρ"mBφ(1 + kf kι), (64)
where (i) follows from eq. (37).
For the term trace ((P∏,*)τ +1-mΦβm-ιP∏), we have
trace ((4,μ)τ+1-mΦβm-i%)
33
Published as a conference paper at ICLR 2022
=) (γλ)m-1trace (Pm(Pn,μ)τ+1-mΦΦ>D”,o)
m-2
+ X (γλ)l (λtrace (P5∏+1(P∏,μ)τ+1-mΦΦ>Dμ,m-i-i)
l=0
+(1- λ)trace (P5∏+1(P∏,μ)τ-m+1ΦΦ>Fπ,m-i-i))
(ii)	m-2
≤ Bφ Pma” ((γλ)m-11>dμ,0 + ]ζ(γλ)l (λ1>dμ,m-i-i + (1- λ)1>fm-i-i )|
(iii)	B2ρτ+1-m
≤ φ max (1 + (1 - λ)kfkι),	(65)
1	- γλ
where (i) follows from eq. (54) and the facts that trace(A + B) = trace(A) + trace(B) and
trace(AB) = trace(BA), (ii) follows from Lemma 6 with Q = Pμ,π, P = P∏, and D = Fm-I-I
and Dπ,m-1-l respectively, and (iii) follow from the eq. (37).
Recall δo = Φ>Dμ,o. Applying Lemma 6 with Q = Pμ,∏ and D = Dμ,o yields
trace ((γ2λ厂(Pnw)T+iφδo) ≤ (γ2λ)τ B2 Pmax.	(66)
Substituting eqs. (63) to (66) into eq. (62), we have
trace(Φδτ Pμ,π)
≤ (γ2λ)τBφ2Pτm+a1x
+ X Bφ Pmm+1 卜(kfkι + 1)+ 1 + (1 - λλkfk1) + X (1 - λ)Pmam+1Bφ krmkι.
m=1	γ	m=1
(67)
Substituting eqs. (60) and (67) into eq. (59), we have
b
1>∆b≤λ2bγ2bPbmaxBφ2+(1+2γ(1-λ)(1+kfk1)+2γλCβ)Bφ2Xλ2(b-τ)γ2(b-τ)Pbm-aτx
τ=1
bb
+γ2(1-λ)2Pbm+a1xBφ2Xλ2b-2τγ2b-2τP-mτaxkrτ-1k1+2λbγ2b(1-λ)PbmaxBφ2Xλb-τ
τ=1	τ=1
+ 2γ2λ(1 - λ)Bφ (λ∣∣f kι + 1 + 1 + (1 ]:λkfk1 ) Pmax X Y2b-2τλ2b-2τ X Pmmx
γ	τ=1	m=1
b	τ-1
+2γ2λ(1-λ)2PbmaxBφ2Xγ2b-2τλ2b-2τ XP-mmaxkrmk1,	(68)
τ=1	m=1
2
WhereWe let Cβ := ɪ-^ (1 + (1 - λ)kfkι).
Under different conditions ofPmax, the term 1>∆b andE	Tbtλ(θt)	Ft-1 can be upper bounded
differently as following:
(a). γ2Pmax < 1, substituting eq. (40) into eq. (68) yields
b
1>∆b≤λ2bγ2bPbmaxBφ2+(1+2γ(1-λ)(1+kfk1)+2γλCβ)Bφ2Xλ2(b-τ)γ2(b-τ)Pbm-aτx
τ=1
+γ2(1 - λ)2 PmaxBφX λ2b-2τγ 2b-2τ Pmax (3+-γf k1 + 1)
τ=1	γ
b
+2λbγ2b(1 - λ)PbmaxBφ2 X λb-τ
τ=1
34
Published as a conference paper at ICLR 2022
+ 2γ2λ(1 - λ)Bφ (λkf kι + 1 + 1 +(I -λλkfk1 ) Pmax
b	τ-1
• X Y2b-2τ λ2b-2τ X Pmmx
τ=1	m=1
+ 2γ2λ(1 - λ)2ρbmaxBφ X Y2b-2τλ2b-2τ X Pmmx ( 31 "k1 +1
τ=1	m=1	γ
≤λ2bY2bPbmaxBφ2+
(1 + 2γ(1- λ)(1 + kfkι) + 2γλCβ) Bφ
1 - Y2Pmaxλ
+ Y2(1 - λ)2ρmaxBφ
1 - Y2Pmaxλ
(3 + 2YkfkI
k 1 - Y2
+ 1 + 2λ2bY bPbmaxBφ2
2γ2λ(1 - λ)Bφ
十 (1 - γ2λ2)(1 - Pmax)
λkfk1+1+
1 + (1- λ)kfkι A b+ι
1 - γλ	P PmaX
,2Y2λ(1 - λ)2B2	(3 + 2YkfkI ∖ b+1
+(1 - γ2λ2)(1 - Pmax) \ 1 - Y2	+ rmax
(69)
where the last two terms of the above inequality are of the order O Pbmax . Therefore, we have
1>∆b ≤ CPbmax for some C > 0. Substituting eq. (69) into eq. (57) yields
E	Tbtλ(θt)22Ft-1 ≤ Cσ,λ,1Pbmax,
where Cσ,λ,1 > 0 is a constant and is determined by eq. (69).
(b)	. Y2Pmax = 1, substituting eq. (39) into eq. (68) yields
b
1>∆b≤λ2bBφ2+(1+2Y(1-λ)(1+kfk1)+2YλCβ)Bφ2Xλ2(b-τ)
τ=1
bb
+Y2(1-λ)2PmaxBφ2Xλ2b-2τ(4+2Ykfk1)τ+2λb(1-λ)Bφ2Xλb-τ
τ=1	τ=1
+ 2Y2λ(1 - λ)Bφ (λkf kι + 1 + 1 + (1 - :λkfk1 ) Pmax X Y 2b-2τ λ2b-2τ X Pmmx
Y	τ=1	m=1
bτ
+ 2Y2λ(1 - λ)2 (4 + 2Ykf k1) Bφ2 X λ2b-2τ X Pτm-amx m
τ=1	m=1
≤ λ2bBφ2 +
(1 + 2Y(1 - λ)(1 + kfkι)+2YλCβ) Bφ
Γ-λ2
+
+
+ 2λbBφ2
Y2 (1 - λ)2PmaxBφ
1 - λ2
2y2λ(1 - λ)Bφ
(1 - Y *)(1 - Pmax)
(4+2Ykfk1)+
λkfk1+1+
2y2λ(1- λ)2 (4 + 2YkfkI) B2
(1 — λ2)(1 — Pmax)
1 + (1- λ)kfkι) b+1
1 - Yλ	P Pmax'
(70)
b
where the last term of the above inequality is of the order O Pbmax . Therefore, we have 1>∆b ≤
CPbmax for some C > 0. Substituting eq. (70) into eq. (57) yields
E	Tbtλ(θt)2Ft-1
≤ Cσ,λ,2Pmax ,
where Cσ,λ,2 > 0 is a constant and is determined by eq. (70).
(c)	. Y2Pmax > 1, substituting eq. (38) into eq. (68) yields
b
1>∆b≤λ2bY2bPbmaxBφ2+(1+2Y(1-λ)(1+kfk1)+2YλCβ)Bφ2Xλ2(b-τ)Y2(b-τ)Pbm-aτx
τ=1
35
Published as a conference paper at ICLR 2022
+Y2b+2(1 - λ)2ρmXBφ (；2+2YkfkI +1) XXλ2
Y Pmax - 1	τ=1
b
b-2τ + 2λbγ2b(1 - λ)ρbmaxBφ2 X λb-τ
+ 2γ 2λ(1 - λ)B"λfkι + 1+1+ι- Tk1
τ=1
b	τ-1
b X 2b-2τ λ2b-2τ X -m
ρmax	γ λ	ρmax
τ=1	m=1
+ 2γ2λ(1 - λ)2ρmmaχB2 (32+2YkfkI +l) XX Y2b-2τλ2b-2τ
Y Pmax - 1	τ=1
τ-1
X γ2m
m=1
≤ λ2bγ2bρbmaxBφ2 +
(1 + 2γ(1- λ)(1 + IflI)+ 2γλCβ) B2
γ ρmax - 1
• γ2bpmaχ
2b+2(1- λ)2ρm+XBφ ∕3 + 2γkfkι
1 - λ2
2γ2λ(1 - λ)Bφ2
γ ρmax - 1
+ 1 + 2λbγ2bρbmax
Bφ2
(1 - γ2λ2)(1 - ρ
2γ2λ(1 - λ)2
max)
λfkι + 1+1 +(1 -λλfkl) Pmx
(1-γ2)(1-γ2λ2)
B2 (3 + 2YfkI +Λ “b
Bφ IY2Pmax - 1 + 1) pmax,
(71)
γ
+
+
+
where the last term of the above inequality is of the order O ρbmax . Therefore, we have 1>∆b ≤
Cρbmax for some C > 0. Substituting eq. (71) into eq. (57) yields
E	Tbtλ(θt)2Ft-1
≤ Cσ,λ,3ρmax ,
where Cσ,λ,3 > 0 is a constant and is determined by eq. (71).
To summarize, the variance term E
Ft-1
can be bounded by σλ2 = O(ρbmax).
E.3 Proof of Theorem 2
Theorem 4 (Formal Statement of Theorem 2). Suppose Assumptions 1 and 2 hold. Consider PER-
ETD(λ) specified in Algorithm 2. Let the stepsize ηt = *入(t；t入),tλ
Lemma 7 and Lλ is defined in Lemma 8 in Appendix C. Further let
8L2
-^λ, where μλ Is defined in
μλ	,
b = max
log(μλ)-log(5Cb,λBφ)
log(ξ)
log(T)
log(ρm
ax) + log(1∕ξ)
,
,
where Cb,λ is a constant defined in the proof of Proposition 3 in Appendix E.1, Bφ :=
maxs∈S Iφ(s)I2, and ξ := max{γ, χ}. Let the projection set Θ = θ ∈ Rd : IθI2 ≤ Bθ , where
Bθ = kφι-Yrmax (which implies θλ ∈ Θ). Then the output θτ OfPER-ETD(λ) satisfies
E [kθτ — θɪ k2] ≤O
where aλ
samples.
log1 / ξ (Pmax )+1
Further, PER-ETD(λ) attains an E-accurate solution with O (三鼠
1
>Λ	C EI	CCll	. 1	.	EI	-t Λ	t	. 1	/T-	，	I
Proof. The proof follows the same steps as Theorem 1 by replacing the terms Tt, T, to, Lo, μo, Cb,
σ2 and θ* with Tλ, Tλ, tλ, Lλ, μλ, Cb,λ, σλ and θλ respectively. Specifically, Propositions 3 and4
are applied to bound the bias and variance over the steps similarly to eq. (44). We then have the
convergence as follows.
E[kθτ-θλk2] ≤O
kθ0-θ^ k2
O
T2
kθ0-θ^ k2
+O
+O
T2
+o (Pmax)+o
ξTb)
，与
+ O (ξb)
+ O (ξb).
(72)
36
Published as a conference paper at ICLR 2022
We further specify b
E [kθτ — θλ k2] ≤O
nilogWO)-Iog&CbBdm ______Iog(T)____O Then Eguation (72) Vields
M	log(ξ)	I , lθg(ρmaχ) + log(1∕ξ) ；. ThenEquatlon (72) y1elds
□
37