Published as a conference paper at ICLR 2022
Anisotropic Random Feature Regression in
High Dimensions
Gabriel C. Mel
Neurosciences Graduate Program
Stanford University
meldefon@gmail.com
Jeffrey Pennington
Brain Team, Google Research
jpennin@google.com
Ab stract
In contrast to standard statistical wisdom, modern learning algorithms typically
find their best performance in the overparameterized regime in which the model has
many more parameters than needed to fit the training data. A growing number of
recent works have shown that random feature models can offer a detailed theoretical
explanation for this unexpected behavior, but typically these analyses have utilized
isotropic distributional assumptions on the underlying data generation process,
thereby failing to provide a realistic characterization of real-world models that are
designed to identify and harness the structure in natural data. In this work, we
examine the high-dimensional asymptotics of random feature regression in the
presence of structured data, allowing for arbitrary input correlations and arbitrary
alignment between the data and the weights of the target function. We define a
partial order on the space of weight-data alignments and prove that generalization
performance improves in response to stronger alignment. We also clarify several
previous observations in the literature by distinguishing the behavior of the sample-
wise and parameter-wise learning curves, finding that sample-wise multiple descent
can occur at scales dictated by the eigenstructure of the data covariance, but that
parameter-wise multiple descent is limited to double descent, although strong
anisotropy can induce additional signatures such as wide plateaus and steep cliffs.
Finally, these signatures are related to phase transitions in the spectrum of the
feature kernel matrix, and unlike the double descent peak, persist even under
optimal regularization.
1	Introduction
While deep learning races ahead with its impressive list of practical successes, it has often done so
by building larger models and training on ever larger datasets. For example, recent large language
models leverage billions (Brown et al., 2020; Raffel et al., 2019) or even trillions (Fedus et al.,
2021) of parameters. As the state-of-the-art pushes the computational frontier, model development,
experimentation, and exploration become increasingly costly, both in terms of the computational
resources and person-hours.
To facilitate the development of improved models, it is becoming ever more necessary to develop
theoretical models that can help guide model development in these high-dimensional scenarios.
Indeed, our theoretical understanding is so limited that even linear regression, the most basic machine
learning method of all, is still yielding surprises in high dimensions (Hastie et al., 2019).
While linear regression can capture many salient features of high-dimensional data, it is not a
reasonable model for high-dimensional models. In particular, there is no natural data-independent
notion of model size: the number of parameters in the model is intimately tied (usually equal) to the
dimensionality of the datapoints. This limitation means that linear regression may not accurately
1
Published as a conference paper at ICLR 2022
capture the effect of overparameterization, which appears to be a crucial ingredient in modern deep
learning models (Zhang et al., 2016).
Beyond linear regression, the next simplest model is arguably the random feature model of Rahimi
et al. (2007), which turns out to be tractable in high dimensions while also allowing for a natural
data-independent lever to vary model size - the number of random features. Random feature models
have also gained independent interest through their relationship to neural kernels and Gaussian
processes (Neal, 1996; Lee et al., 2017; Novak et al., 2018; Lee et al., 2019; Jacot et al., 2018). While
recent works have examined the asymptotic performance of random feature models, most have done so
in the restricted setting of isotropic covariates and target functions (Mei & Montanari, 2019; d’Ascoli
et al., 2020; Adlam & Pennington, 2020a;b). Based on the conjectured Gaussian equivalence principle
of Goldt et al. (2020b), a few recent works have begun relaxing these simplifying assumptions in an
effort to describe more natural data distributions (Loureiro et al., 2021; d’Ascoli et al., 2021). In this
work, we extend the high-dimensional random feature linearization technique of Tripuraneni et al.
(2021a;b) to allow for anisotropic target functions, revealing a host of novel phenomena related to the
alignment of the data and the target weights.
1.1	Contributions
Our primary contributions are to:
1.	Derive theoretical predictions for the test error, bias, and variance of random feature re-
gression for anisotropic Gaussian covariates under general linear target functions in the
high-dimensional asymptotic limit (Section 3.1);
2.	Prove that overparameterization reduces the total test error, bias, and variance, even in the
presence of anisotropy (Section 3.2);
3.	Define a partial order on the space of weight-data alignments and prove that generalization
performance improves in response to stronger alignment (Definition 2.1, Section 3.3);
4.	Demonstrate that anisotropy can induce sample-wise multiple descent, but prove that
parameter-wise multiple descent is limited to double descent (Section 3.4);
5.	Provide a theoretical explanation for steep cliffs observed in parameter-wise learning curves
that persist under optimal regularization, and relate them to phase transitions in the spectrum
of the feature kernel matrix (Section 3.4).
1.2	Related work
Our analysis builds on a series of works that have studied the exact high-dimensional asymptotics of
the test error for a growing class of model families and data distributions. Early work in this direction
examined minimum-norm interpolated least squares and ridge regression in the high-dimensional
random design setting (Belkin et al., 2019; Dobriban et al., 2018; Hastie et al., 2019), finding
surprising phenomena such as double descent and a negative optimal ridge constant (Kobak et al.,
2020). These analyses were generalized in (Richards et al., 2021; Wu & Xu, 2020; Hastie et al., 2019)
to allow for anisotropic covariance matrices and weights that generate the targets. This anisotropic
configuration was studied in further detail in (Mel & Ganguli, 2021), uncovering nuanced behavior
such as sample-wise multiple descent and a sequence of phase transitions in the performance and
optimal regularization.
While linear regression can provide insight into generalization performance in high dimensions, it can-
not faithfully reproduce the features of non-linear models, such as the effects of overparameterization.
Random feature models, on the other hand, can capture many such features while still maintaining
analytical tractability. Recent results have developed a detailed understanding of isotropic random
feature regression in the high-dimensional asymptotic limit, providing a precise characterization of
peaks in the test error and the benefits of overparameterization (Mei & Montanari, 2019; d’Ascoli
et al., 2020; Adlam & Pennington, 2020a). The origin of these peaks was explained by means
of a fine-grained decomposition of the bias and variance in (Adlam & Pennington, 2020b; Lin &
Dobriban, 2020). While most prior work examining the high-dimensional asymptotics of random
feature models have leveraged isotropy of the covariate distribution, this assumption was recently
relaxed in (Liao et al., 2020) for random Fourier feature models, in (Tripuraneni et al., 2021a;b) in the
2
Published as a conference paper at ICLR 2022
study of covariate shift, and in several works based on the Gaussian equivalence principle of Goldt
et al. (2020a;b) (Gerace et al., 2020; Loureiro et al., 2021).
Though most of these works did not pursue a detailed investigation of the anisotropy itself, while
finalizing this manuscript we became aware of concurrent work (d’Ascoli et al., 2021) which does
focus on the interplay of the anisotropic input features and target functions, as we do here. Whereas
d’Ascoli et al. (2021) leverage the Gaussian equivalence principle and the replica method from
statistical physics, we use a completely different set of techniques stemming from the literature
on random matrix theory and operator-valued free probability (Pennington & Worah, 2018; 2019;
Adlam et al., 2019; Adlam & Pennington, 2020a; Louart et al., 2018; Peche et al., 2019; Far et al.,
2006; Mingo & Speicher, 2017), which provides a complementary perspective that could be made
completely rigorous. d’Ascoli et al. (2021) investigates the role of anisotropy not just under squared
loss, as we do here, but also under logistic loss, and obtains good numerical support for the Gaussian
equivalence conjecture in that setting. In contrast, our narrower and deeper focus on squared loss
allows us not only to uncover a host of novel phenomena, including multiple descent, steep error cliffs,
and spectral phase changes, but also to rigorously prove several propositions about their existence
and properties. We provide a more in-depth discussion of these and other related works in App. A.
Our work relies heavily on the concept of alignment (see Def. 2.1) between the data covariance and
the target vector coefficients. An analogous definition appears in (Tripuraneni et al., 2021a;b) in the
context of covariate shift, leading to a superficial similarity of results. However, the implications
and interpretations are otherwise quite different, so we refrain from repeatedly commenting on these
recurring connections.
2	Preliminaries
2.1	Problem setup and notation
We study random feature regression (Rahimi et al., 2007) as a model for learning an unknown linear
function of the data,
y(xi) = β>Xi∕√n0 + Wi,	(1)
from m independent samples (xi, yi) ∈ Rn0 × R, i = 1, . . . , m, where the covariates are Gaussian,
Xi 〜N(0, ∑), and where Wi 〜N(0, σ2) is additive noise (present on the training samples only).
We focus on the high-dimensional proportional asymptotics in which the input feature dimension
n0, the hidden layer size n1, and the number of samples m all tend to infinity at the same rate, with
φ := no/m and ψ := no/ni held constant. The overparameterization ratio φ∕ψ = n`/m is a
measure of the normalized complexity of the random feature model.
Interestingly, under this model in our high-dimensional setup, any nonlinear component of the
signal to be learned behaves like additive noise, so the linear function in Eq. (1) does not sacrifice
generality (Mei & Montanari, 2019; Adlam & Pennington, 2020a).
Following many works on high-dimensional regression (see e.g. Dobriban et al. (2018)), we assume
the coefficient vector β is random, β 〜N(0, ∑β), though allowing for deterministic β would be a
straightforward extension and would simply involve replacing Σβ → ββ> throughout.
Denoting the training dataset as X = [X1, . . . , Xm] and the test point as X, the random features are,
F ：= σ(WX∕√n0)	and f := σ(Wx∕√n0),	(2)
where W ∈ Rn1 ×n0 is a random weight matrix with i.i.d. standard Gaussian entries, and σ(∙) : R →
R is an activation function that is applied elementwise. These random features define the kernel
K(xi, X2) := -1 σ(Wxι∕√n0)>σ(WX2∕√n0),	(3)
n1
which can be used to compute the model,s predictions on a point X as y(x) = Y K-1Kχ.
Here we have defined the training labels as Y := [y(X1), . . . , y(Xm)], and the regularized kernel
matrix as K := K(X, X) + γIm, as well as Kx := K(X, X) for γ ≥ 0.
The training error is given by,
Eliain = EβE[(y(x) - y(x))2] = E[(β>X∕√n0 - YKTK(X, X))2],
(4)
3
Published as a conference paper at ICLR 2022
and the test error (without label noise on the test point) can be written as
E∑ = EeEχE[(y(x) - y(x))2] - σf = EχE[(β>x∕√n0 - YK-1Kχ)2],	(5)
where the outermost expectation over β has been suppressed since the quantity concentrates sharply
around its mean and the inner expectation is computed over all the randomness from training, i.e. W,
X , and . The test error can be decomposed into its bias and variance components as
E∑ = ExE[(E[y(x)] — y(x))2] + Eχ[V[y(x)]].
(6)
|
} '------{-----}
VΣβ
{z^^
BΣβ
The bias and variance are computed with respect to all the randomness from training, i.e. W, X, ,
which differs from common practice in the statistics literature, but which is necessary to obtain a
decomposition that is intuitive and unambiguous when the predictive function relies on randomness
from multiple sources (Adlam & Pennington, 2020b; Lin & Dobriban, 2020).
As noted by (Hastie et al., 2019; Wu & Xu, 2020; Mel & Ganguli, 2021), the test error of linear
regression depends on the geometry of (Σ, Σβ) (or (Σ, β) in the case of nonrandom β), and we will
find the same to be true for random feature regression. As such, we decompose the training and β
covariance matrices into eigenbases as Σ = Pin=0 1 λivivi> and Σβ = Pin=0 1 λiβviβviβ>, where the
eigenvalues are in nondecreasing magnitude, i.e. λ1 ≤ λ2 ≤ . . . ≤ λn0 and λ1β ≤ λ2β ≤ . . . ≤ λβn0 .
Following (Tripuraneni et al., 2021a;b), we then define the overlap coefficients as
n0
qi := v>∑βVi= X(Vj ∙ Vi)2λβ ,	(7)
j=1
to measure the alignment of Σβ with the ith eigendirection of Σ. Note that for deterministic β this
definition simply gives the square of the component of β in the ith eigendirection, i.e. qi = (Vi>β)1 2.
2.2 Assumptions
In order to define the limiting behavior of the test error in Eq. (5), it is necessary to impose some
regularity conditions on Σ and Σβ.1 As in (Wu & Xu, 2020; Tripuraneni et al., 2021a;b), the limiting
spectra of these matrices cannot be considered independently because their eigenspaces may align.
This alignment is most conveniently described in an eigenbasis of Σ.
Assumption 1. We define the joint spectral distribution (JSD) as
1 n0
μno := — Eδ(λi,qa)	⑻
n0
i=1
and assume it converges in distribution to some μ, a distribution on Rj as n → ∞. We refer to μ
as the limiting joint spectral distribution (LJSD), and emphasize that this defines the relevant limiting
properties of the covariate and weight distributions2 .
Often We use (λ, q) for random variables SamPledjointly from μ and denote the marginal of λ under
μ with μdata. Since the conditional expectation E[q∣λ] is an important object in our study, we assume
the folloWing for simPlicity.
Assumption 2. μ is either absolutely continuous or a finite sum Ofdelta masses and the expectations
of λ and q are finite. Moreover, Eμ[λq] = 1.
The normalization condition E* [λq] = 1 is not necessary for our analysis, but, as in (Mel & Ganguli,
2021), it enforces consistent signal-to-noise ratios across different target functions and thereby
facilitates meaningful comparisons of the test error.
1In fact, the necessary assumptions are identical to those of Tripuraneni et al. (2021a;b) since Σβ plays an
analogous role to the test covariance Σ* of that work.
2The JSD depends not only on Σ and Σβ but also on a choice of eigendecomposition for Σ when it has
repeated eigenvalues; however, as in (Tripuraneni et al., 2021a;b), all possible choices lead to the same formulas
and conclusions.
4
Published as a conference paper at ICLR 2022
As we will eventually consider the high-dimensional limit, it is convenient to define the asymp-
totic covariance scale as S = lim近→∞ n^tr(Σ) = Eμ[λ] under the limiting behavior specified in
Assumption 1.
One important case is when ∑β =儿,in which case the LJSD degenerates to μg defined by
μ0(λ, q) := μdata(λ)δ1(q) .	(9)
Following Tripuraneni et al. (2021a;b), we also enforce the following standard regularity assumptions
on the activation functions to ensure the existence of the moments and derivatives we compute.
Assumption 3. The activation function σ : R → R is assumed to be differentiable almost everywhere.
We assume there exists a universal constant C such that, ∣σ(x)∣, ∣σ0(x)∣ = O(exp(Cx)).
2.3	A simple family of anisotropic distributions
The assumptions above allow for a wide class of possible asymptotic covariance structures and
spectra. To allow for concrete examples that have intuitive interpretations, we introduce the following
simple family of distributions that capture multi-scale structure in the input, which we refer to as
d-scale LJSDs. In particular, for d ∈ N and real 0 < α ≤ 1 and θ, we define
d-1
d-scale	1
μɑ,θ :=工 dδ(Cɑj ,Dɑθj),
j=0
(10)
where C := (d 1--0-)	and D := = (d Iu(窘)； )	enforce the normalization conditions
s = E d-scale [λ] = 1 and E d-scale [qλ] = 1. These simple covariance models capture the hierarchy of
μα,θ	μ α,θ
scales that often characterize the structure of natural datasets. Note that by setting α = 1 and s = 1,
the distribution reduces to the isotropic case with identity covariance. For the nontrivial setting in
which α < 1, the data exhibits a sequence ofd distinct scales where each scale is a factor α smaller
than the previous one. The exponent θ parameterizes the strength of the weight-data alignment in
an intuitive way. When θ = 0, there is no alignment, and Σβ is proportional to the identity. When
θ > 0, there is strong alignment, as the large eigendirections of the training distribution correspond
to large eigendirections of Σβ . When θ < 0, there is anti-alignment, as the large eigendirections of
the training distribution correspond to small eigendirections of Σβ .
2.4	Definition of the strength of weight-data alignment
As the preceding discussion has suggested and as we will see explicitly in the following section,
the LJSD μ captures all of the information about the pair of covariance matrices (Σ, ∑β) that is
relevant for describing the asymptotic test error, bias, and variance. As the alignment between Σ and
Σβ can vary in strength among the different eigendirections, the concept of alignment is inherently
multi-dimensional. Nevertheless, by requiring strong alignment along the larger eigendirections, it is
possible to define the following natural partial order on the space of possible weight-data alignments3
Definition 2.1. Let μ- and μ2 be LJSDs with the same marginal distribution of λ. Ifthe asymptotic
OVerlaP coefficients are such that Eμι [λq∣λ] /E*2 [λq∣λ] = E*1[q∣λ] /E*2 [q∣λ] is nondecreasing
in λ, we say that μ- is more StrOngly aligned than μ2 and write μ- ≤ μ2. Comparing against the
case of isotropic weight distribution, μ0, we say μ- is aligned when μ- ≤ μ0 and anti-aligned when
μι ≥ μ0.
The parameter θ of the d-scale model in Eq. (10) provides a quantitative measure of alignment
under Definition 2.1, formalizing the intuition given in Section 2.3. As we will see in Section 3.3, the
asymptotic generalization performance of random feature regression is strictly ordered under this
definition of alignment strength.
3Note that this definition is nearly identical to that of Tripuraneni et al. (2021a;b), with the concept of
hardness replaced by alignment.
5
Published as a conference paper at ICLR 2022
Figure 1: Test error, bias, and variance as a function of the overparameterization ratio (φ∕ψ = n1 /m)
and the alignment θ for the 2-scale LJSD (Eq. (10)) with φ = n0/m = 10/9, σ = tanh, γ = 10-8
and σε2 = 1/100. (a) The total test error exhibits the characteristic double descent behavior for all
shift powers. (b) The bias is a nonincreasing function of φ∕ψ for all shift powers, as in Proposition
3.1. (c) The variance is the source of the peak, and is a nonincreasing function of φ∕ψ for all shift
powers in the overparameterized regime, as in Proposition 3.2. Blue, orange, and green lines in (a,b,c)
indicate locations of vertical and horizontal slices shown in (d,e,f). (d) 1D horizontal slices of (a,b,c)
more clearly demonstrate the monotonicity in φ∕ψ predicted by Propositions 3.1 and 3.2. (e) 1D
vertical slices demonstrate the monotonicity in θ of the test error and the bias predicted by Proposition
3.3, and illustrate that the variance may not decrease in response to stronger alignment (solid green
trace). (f) Test error, bias, and variance normalized by the training error (ie. E /μμ+谷=0); similarly
for Bμ, Vμ). Same legend as (e). Constancy of the blue traces illustrates how the train and test error
respond identically to alignment. Simulations for m = 4000 (d,e; crosses) agree well with formulas.
3	Main results
Our main result characterizes the high-dimensional asymptotic limits of the test error, bias, and
variance of the nonlinear random feature model of Section 2. Before stating the result, we introduce
some additional notation that captures the effect of the nonlinearity σ,
η := VZ〜N(0,s)[σ(Z)], P= (SEZ〜N(o,s)[zσ(Z)])2, Z := sρ, ω:= s(n/z T). (II)
where, as above, S = limn0→∞ 卷tr(Σ) = Eμ [λ]. The constant ω ≥ 0 is a measure of the degree of
nonlinearity, with ω = 0 corresponding to linear activation functions (see Lemma B.1). Analogously
to Tripuraneni et al. (2021a;b), We also introduce two functionals of μ, which capture all the relevant
spectral information needed for describing the test loss,
Ia,b := φ Eμ	(λa	(φ + xλ)-b)	and	工：,b	:= φ	Eμ	(qλa	(φ + xλ)-b)	.	(12)
3.1	Formulas for asymptotic bias, variance, and test error
Theorem 3.1. Under Assumptions 1-3, as n0, n1, m → ∞ with φ = n0/m and ψ = n0/n1 held
constant, the training error E^ain tends toward the value of Eμain where
E∖in = -Y2 (∂γ(τιIβ,ι) + σ∂γTi) ,	(13)
6
Published as a conference paper at ICLR 2022
Figure 2: Sample-wise multiple descent for anisotropic data as a function of nonlinearity strength
ω. An panels refer to a 3-scale covariance model With α = 103, and ψ = 2, s = P =1,γ = 10-13
and σε2 = 10. (a) Error exhibits multiple peaks as a function of the the sampling density φ-1. As
ω is increased, peaks are attenuated and eventually disappear in sequence starting from those due
to the Weakest data scales (at m = n0) to those due to the strongest data scales (at m = n0 /3). (b)
Limiting spectral density of the kernel matrix K(X, X) = FTF/n1. Vertical slices correspond to
the spectral density at the corresponding value of φ-1 for ω = 10-10. Peaks in the error curves
in (a) occur at critical values m = 3 n0, 2 n0, n0 where a new spectral component first appears. (C)
Consistent With this, sample-Wise learning curves exhibiting multiple descent at loW ω pass through
several transitions in the number of spectral components, while those at higher ω with fewer peaks
pass through fewer component transitions.
and the bias, variance, and test error Bze, V∑β, EZe tend toward Bμ, Vμ, Eμ with
E μ
Eμ = γ2τn - σ2 , Bμ = φ琢 2 , and Vμ = Eμ - Bμ ,	(14)
where ρ, ω are defined in (11), Ia,b,Iaβ,b are defined in (12), and x is the unique nonnegative real
root of X = -⅛γτ^ with τι = √≡φ⅛g三+ψ-φ.
ω+I1,1	1	2ψγ
Remark 3.1. As noted in (Adlam & Pennington, 2020a; Hastie et al., 2019) for isotropic covariates,
the test error is simply related to the training error through the generalized cross-validation (GCV)
formula (Golub et al., 1979). Curiously, because τ1 is independent of β, it follows that the test
error depends on β entirely through its effect on the training error; in contrast, the bias-variance
decomposition retains explicit β dependence, even conditional on the training error.
The results in Theorem 3.1 depend on a single scalar self-consistent equation for x, x =
1-γτι
ω+I1,1
which is in fact the same as the one appearing in Tripuraneni et al. (2021a;b), and which significantly
simplifies the expressions relative to those recently obtained using the replica method (d’Ascoli
et al., 2021). Owing to its simplicity, this equation admits straightforward analysis that we pursue
in the Appendix, yielding numerous inequalities and bounds that ultimately prove the propositions
presented throughout this section. We will occasionally refer to the ridgeless limit of Theorem 3.1,
which is given in Corollary G.1. Finally, as a consistency check, taking σ(x) = x and ψ → 0 in
Theorem 3.1 yields an expression for the test error, bias, and variance for linear regression that agrees
with the results of (Wu & Xu, 2020; Mel & Ganguli, 2021) (see Section E).
3.2	The benefits of overparameterization
While there is abundant empirical evidence that overparameterization can improve the generalization
performance of practical models (see e.g. (Zhang et al., 2016)), rigorous explanations for this behavior
have been offered solely in the setting of isotropic covariates and target functions. It is therefore
natural to wonder whether the benefits of overparmeterization persist in the presence of anisotropy,
and indeed recent theoretical work has provided numerical evidence hinting that overparameterization
is beneficial (d’Ascoli et al., 2021). The following two results prove this to be the case.
First, we show that the bias decreases (or stays constant) in response to an increase in the number of
random features, regardless of any anisotropy in the covariates or target function weights.
7
Published as a conference paper at ICLR 2022
Proposition 3.1. In the setting of Theorem 3.1, the bias Bμ is a nonincreasing function of the
Overparameterization ratio φ∕ψ.
The same is true for the variance in the overparameterized regime. Note that our proof requires the
ridgeless setting, but numerical simulations suggest that the result may hold generally (see Fig. 1).
Proposition 3.2. In the ridgeless limit and in the overparameterized regime (ψ < φ), the variance
Vμ is a nonincreasing function of overparameterization ratio φ∕ψ.
In Fig. 1, Propositions 3.1 and 3.2 are illustrated. Moving left to right in panels (a)-(d) leads to models
with more parameters. The monotonicity of the bias across the whole range of parameterization is
evident, as is the monotonicity of the variance in the overparmaeterized regime.
3.3	Weight-data alignment reduces the bias and test error
A number of recent works have confirmed the intuition that generalization performance should
improve if the weights of the target function align with the large eigendirections of the training
covariance, with formal theoretical arguments in the case of linear regression (Hastie et al., 2019;
Mel & Ganguli, 2021), and with informal numerical simulations for random feature models (d’Ascoli
et al., 2021). The following result proves that this informal observation holds in full generality, not
only for the total test error, but for the bias and the bias-to-variance ratio as well.
Proposition 3.3. Let μ1,μ2 be two LJSDS SUCh that μι ≤ μ2 (see Definition 2.1). Then Bμι ≤ B*?,
Eμi ≤ Eμ2, and Bμ∖∕Vμ∖ ≤ Bμ2∕Vμ2.
We illustrate Proposition 3.3 in Fig. 1 for the 2-scale LJSD of Eq. (10). Following vertical lines
upward in panels (a-c) or the x-axis left-to-right in (e,f) yields stronger alignment and a corresponding
decrease in the bias and test error. Panel (f) shows the alignment-dependence of the bias, variance,
and test error when normalized by the training error. As suggested by Remark 3.1, the normalized
test error is independent of the alignment strength, and as intuition would suggest and as predicted
by Proposition 3.3, the normalized bias decreases in response to stronger alignment.
3.4	Anisotropy induces structured learning curves
Strong anisotropy has been observed to induce structured learning curves in the context of linear
regression (Mel & Ganguli (2021)). It is natural to wonder whether these effects generalize to the case
of nonlinear random features. Figure 2 shows that this is indeed the case for sample-wise learning
curves, where φ-1 = m∕n0 is varied while ψ = n0∕n1 is held fixed. Horizontal slices through Fig.
2 (a) exhibit sample-wise multiple descent. Fig. 2 (b) and (c) show that these peaks are associated
with phase transitions at critical values of φ-1 where the spectrum of the kernel matrix FT F
acquires a new component. Increased nonlinearity ω attenuates this effect.
In contrast, the following proposition shows that parameter-wise multiple descent does not occur
even in the presence of strong anisotropy, as long as μ is aligned.
Proposition 3.4. If μ is aligned (see Definition 2.1), then, in the ridgeless limit, the test error has at
most two interior critical points as a function of the overparameterization ratio φ∕ψ.
Remark 3.2. Since k-fold descent requires at least k critical points, we conclude that multiple
(k > 2) descent does not occur, even in the presence ofcovariate anisotropy, so long as μ is aligned.
Fig. 3(a,b) shows parameter-wise learning curves for various values of θ in the 2- and 3-scale models.
As predicted by Proposition 3.4, the curves for aligned LJSDs (θ ≥ 0) have at most two critical points
(not shown). In fact, even for anti-aligned LJSDs (θ < 0), most curves also have at most two critical
points; however, for small enough θ, an additional critical point emerges, and indeed the upper-most
traces in Fig. 3(b) showcase a second peak in the learning curves.
Even though parameter-wise multiple descent is not possible for aligned LJSDs, strong anisotropy
can nevertheless induce other signatures, including wide plateaus and steep cliffs. To understand the
origin of these effects, consider the ridgeless limit, for which the self-consistent equation for x is,
ωx + EU-=— = m min fl, φ) .	(15)
φ μ λ + φ φ ∖ ψJ
8
Published as a conference paper at ICLR 2022
Figure 3: Strong anisotropy causes steep cliffs in the test error as a function of the overparameteri-
Zationratio φ∕ψ = n1 /m. φ = 5/6, γ = 10-22, s = P = 1,ω = 10-16 and σ∣ = 10-4. Top row:
d = 2-scale model with α = 10-5; bottom row: 3-scale model with α = 10-6. (a,b) Test error
exhibits the characteristic peak at the interpolation threshold, but additionally displays steep cliffs
at critical values of n1 corresponding to multiples of n0 /d (b, dashed lines; m also shown). The
θ value associated to each trace is indicated by the colored ticks in (a). (c) Unlike the peak at the
interpolation threshold, cliffs in the error traces in (b) persist under optimal regularization (error
values from analytical formulas were numerically optimized over γ). (d) The spectrum of the kernel
matrix K(X, X) = nɪ- F> F undergoes a phase transition at each of the critical values of n1 where a
cliff occurs (dashed lines). The phase transition associated to the interpolation threshold at n1 = m
is also visible in the lower panel, but is out of range (at ≈ 10-17) in the upper panel.
For small ω, the left side becomes relatively insensitive to changes in X whenever λ-《φ∕x《λ+
for some wide spectral gap with boundary (λ-, λ+). It follows that, in the underparameterized regime
(φ < ψ), the derivative ∂(φ∕ψ)∕∂x is small, or, equivalently, ∂x∕∂(φ∕ψ) is large. This strong
sensitivity of X to changes in the overparameterization ratio φ∕ψ induces similarly strong changes
to the error because ∂Eμ ∕∂x is bounded. As a result, the parameter-wise learning curves exhibit
sharp downward cliffs in the underparameterized regime for sufficiently small ω and sufficiently large
spectral gaps λ+∕λ-. We make this argument more precise in Appendix F.
Fig. 3(a) illustrates these cliffs in the context of the d = 2- and 3-scale LJSD. Along horizontal slices,
the error turns sharply downward as a function of n1 H φ∕ψ at integer multiples of n0 ∕d. As can be
seen in Fig. 3(b), when the alignment is increased and β overlaps more with large λs, the first error
cliff, corresponding to the largest scales in the data, strengthens relative to others (cliff at φ∕ψ = 1∕2
in the 2-scale model and φ∕ψ = 1∕3 in the 3-scale model). Fig. 3(c) shows that, unlike the peak at the
interpolation threshold, cliffs persist under optimal regularization. Finally, Fig. 3(d) illustrates how
steep decreases in the error are associated with the appearance of a new component in the spectrum
of the kernel matrix at critical values of φ∕ψ.
4	Conclusions
We presented an exact calculation of the limiting test error, bias, and variance for random feature
kernel regression with anisotropic covariates and target function weights. We defined a partial order
over weight-data alignments and proved that stronger alignment decreases the bias and test error.
We also proved that the benefits of overparameterization persist in the anisotropic setting, and that
weight-data alignment limits parameter-wise multiple descent to double descent. In contrast, we
demonstrated that anisotropy can induce sample-wise multiple descent, and parameter-wise cliffs,
and that their structure is dictated by the eigenstructure of the data covariance. Future directions
include extending our results to the non-asymptotic regime, accommodating feature learning and more
general neural network models, and investigating the impact of anisotropy for other loss functions.
9
Published as a conference paper at ICLR 2022
References
Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74-84. PMLR, 2020a.
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variance decomposition. arXiv preprint arXiv:2011.03321, 2020b.
Ben Adlam, Jake Levinson, and Jeffrey Pennington. A random matrix perspective on mixtures of
nonlinearities for deep learning. arXiv preprint arXiv:1912.00827, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849-15854, 2019.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
StePhane d'Ascoli, Levent Sagun, and GiUlio Biroli. Triple descent and the two kinds of ovefitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.
Stephane d’Ascoli, Marylou Gabrie, Levent Sagun, and Giulio Biroli. On the interplay between data
structure and loss function in classification problems, 2021.
Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247-279, 2018.
Reza Rashidi Far, Tamer Oraby, Wlodzimierz Bryc, and Roland Speicher. Spectra of large block
matrices. arXiv preprint cs/0610045, 2006.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborov直 Generali-
sation error in learning with random features and the hidden manifold model, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks, 2019.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods?, 2021.
Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborovd. Modeling the influence of
data structure on learning in neural networks: The hidden manifold model. Physical Review X, 10
(4):041044, 2020a.
Sebastian Goldt, Galen Reeves, Marc Mezard, Florent Krzakala, and Lenka Zdeborovd. The gaussian
equivalence of generative models for learning with two-layer neural networks. arXiv e-prints, pp.
arXiv-2006, 2020b.
Gene H Golub, Michael Heath, and Grace Wahba. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics, 21(2):215-223, 1979.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
J. William Helton, Scott A. McCullough, and Victor Vinnikov. Noncommutative convexity arises
from linear matrix inequalities. Journal of Functional Analysis, 240:105-191, 2006.
Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features,
2021.
10
Published as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Noureddine El Karoui. The spectrum of kernel random matrices. Annals of Statistics, 38:1-50, 2010.
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world
high-dimensional data can be zero or negative due to the implicit ridge regularization. J. Mach.
Learn. Res., 21:169-1, 2020.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572-8583, 2019.
Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random
fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding
double descent. arXiv preprint arXiv:2006.05013, 2020.
Licong Lin and Edgar Dobriban. What causes the test error? going beyond bias-variance via anova.
arXiv preprint arXiv:2010.05170, 2020.
Cosme Louart, Zhenyu Liao, Romain Couillet, et al. A random matrix approach to neural networks.
The Annals of Applied Probability, 28(2):1190-1248, 2018.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard,
and Lenka Zdeborovd. Learning curves of generic features maps for realistic datasets with a
teacher-student model, 2021.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Gabriel Mel and Surya Ganguli. A theory of high dimensional regression with arbitrary correlations
between input features and target functions: sample complexity, multiple descent curves and a
hierarchy of phase transitions. In International Conference on Machine Learning, pp. 7578-7587.
PMLR, 2021.
James A Mingo and Roland Speicher. Free probability and random matrices, volume 35. Springer,
2017.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
S Peche et al. A note on the pennington-worah distribution. Electronic Communications in Probability,
24, 2019.
Jeffrey Pennington and Pratik Worah. The spectrum of the fisher information matrix of a single-
hidden-layer neural network. In NeurIPS, pp. 5415-5424, 2018.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.
11
Published as a conference paper at ICLR 2022
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression
under general source condition. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings
of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of
Proceedings of Machine Learning Research, pp. 3889-3897. PMLR, 13-15 APr 2021. URL
http://proceedings.mlr.press/v130/richards21b.html.
Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate shift in high-dimensional random
feature regression. arXiv preprint arXiv:2111.08234, 2021a.
Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization improves robustness
to covariate shift in high dimensions. Advances in Neural Information Processing Systems, 34,
2021b.
Denny Wu and Ji Xu. On the optimal weighted `2 regularization in overparameterized linear
regression. arXiv preprint arXiv:2006.05800, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12
Published as a conference paper at ICLR 2022
Supplementary Material: Anisotropic Random Feature Regression in High
Dimensions
Table of Contents: S upplementary Material
A	Additional discussion of related work	2
A.1 Asymptotic error formulas and Theorem 3.1 ................................... 2
A.2 Gaussian equivalents ........................................................ 2
A.3 Weight-data alignment ....................................................... 3
B	Useful inequalities	3
B.1	Basic properties of the self-consistent equation for x ..................... 3
B.2	I and Iβ inequalities ...................................................... 5
C	Weight-data alignment is a partial order	6
D	Proofs of propositions	6
D.1	Proposition 3.1 ............................................................ 6
D.2	Proposition 3.2 ............................................................ 7
D.3	Proposition 3.3 ............................................................ 7
D.4	Proposition 3.4 ............................................................ 8
E	Linear regression limit	10
E.1	Comparison to Mel & Ganguli (2021) ........................................ 10
E.2	Comparison to Wu & Xu (2020) .............................................. 11
F	Structured learning curves	12
F.1 Effect of spectral gap ..................................................... 12
F.2	Analysis of the D-scale model in the	separated limit ...................... 14
G	Proof of Theorem 3.1	15
G.1	Decomposition of the test loss ............................................ 15
G.2	Decomposition of the bias and total variance .............................. 16
G.3 Summary of linearized trace terms .......................................... 17
G.4	Calculation of error terms ................................................ 18
G.5	Final result for bias, variance, and test error ........................... 31
1
Published as a conference paper at ICLR 2022
A	Additional discussion of related work
Since preparing the initial version of this manuscript, we became aware of several related recent
and concurrent works. Our main contribution relative to these works is the detailed phenomenology
explored throughout the main text, but here we provide a more detailed discussion of some of
the connections between the current paper and other work, highlighting the main similarities and
differences.
A.1 Asymptotic error formulas and Theorem 3.1
Alternative asymptotic formulas for the test error of anisotropic random feature regression have
been presented in a set of concurrent works, yielding overlapping results with Theorem 3.1. The
result that is most closely related is that of d’Ascoli et al. (2021), who also study the random feature
model with anisotropic input data and target function. They derive formulas for arbitrary convex loss
function (generalizing our setup) and then focus on two specialized learning scenarios: (1) linear
target function with additive noise and quadratic loss, corresponding to the case studied here; and
(2) discrete class labels sign(βTχ∕√nθ) with label-flipping noise and logistic cost function. Their
main results rely on a Gaussian equivalence theorem for anisotropic data, which they derive assuming
the Gaussian equivalence principle of Goldt et al. (2020a;b), and then proceed via a standard replica
calculation. Another salient paper is that of Loureiro et al. (2021), who study generic feature maps for
student-teacher models. A Gaussian covariate model is proposed and rigorous asymptotic solutions
are derived for it using Gaussian comparison inequalities, which are shown to agree with calculations
from the replica method. The model is general enough to facilitate comparisons to realistic datasets,
and numerical evidence and universality arguments support the utility of the model for exactly
describing the random feature model we examine here, among many other applications.
Our technical approach proceeds in a substantially different manner, using tools from random matrix
theory and operator-valued free probability, rather than statistical physics techniques or the replica
method. The results could be made entirely rigorous, though here we simply present the pertinent
calculations and defer justification of the underlying linearization techniques to future work and
to (Tripuraneni et al., 2021a;b). Our analysis ultimately yields final expressions with a relatively
simple form, involving only a single scalar self-consistent equation, which lends itself to more
straightforward downstream calculations and analysis (e.g. Propositions 3.1-3.4 and Corollary
G.1). Finally, beyond the total error, we also derive formulas for the bias and variance, which aid
significantly in the interpretation of the phenomenology, and are novel results. Interestingly, the
order parameter Q from d’Ascoli et al. (2021) (and others) is interpreted as the variance of the
student’s outputs, but actually differs from the variance defined in Eq. (6). The reason is that the
bias-variance decomposition is defined conditionally on x. Because the conditional mean is nonzero,
i.e. E[y∣x] = 0, Q actually corresponds to an uncentered second moment, and corresponds to our
term E3 (defined in Eq. (S147)) which differs from the total variance by the non-trivial additive term
E4 = Eχ[E[y∣x]2] (defined in Eq. (S162)). A more thorough discussion of these and related concepts
is given by Adlam & Pennington (2020b).
A.2 Gaussian equivalents
Our calculations utilize the concept of Gaussian equivalents, in particular a linear-signal-plus-noise
surrogate Flin for the random feature matrix F. This approach originates from Karoui (2010) in the
context of kernel random matrices of the form Kij = σ(Xi>Xj /n0) or Kij = σ(kXi - Xj k2/n0)
and from Pennington & Worah (2019); Adlam et al. (2019); P6ch6 et al. (2019) for the covariance
matrices F>F/n1 studied here. This linearization technique was further developed by Adlam et al.
(2019) for anisotropic covariates (and in the presence of bias), where it was shown to be sufficient for
predicting the training error for random feature ridge regression with isotropic linear target functions.
In the setting of spherical data and weights, Mei & Montanari (2019) extended these results to cover
the test error as well. In this case, many of the main technical results could build directly from Karoui
(2010) owing to a decomposition of kernel inner product matrices (Mei & Montanari, 2019, Section
C.4) that ultimately relies on the orthogonality of Gegenbauer polynomials and cannot (immediately)
be extended to the Gaussian settings studied here and elsewhere. For random feature regression
with the neural tangent kernel (which subsumes the standard random feature setting), a proof of the
linearization was outlined for the test error in the setting of isototropic Gaussian covariates by Adlam
2
Published as a conference paper at ICLR 2022
& Pennington (2020a). As mentioned previously, an extension to anisotropic Gaussian covariates
was later developed by Tripuraneni et al. (2021a;b), which is the basis for our analysis in this work.
In a parallel and largely independent line of work stemming from Goldt et al. (2020a), a nearly
identical approach is developed under the name of the Gaussian equivalence property, under which
a possibly nonlinear target function and/or prediction are replaced with simple linear Gaussian
equivalents. This is a crucial step in the analysis in Goldt et al. (2020b); Gerace et al. (2020); d’Ascoli
et al. (2021) and related work. For example, Gerace et al. (2020) use this principle (in fact, a stronger
version they refer to as “replicated Gaussian equivalence") in order to perform their replica analysis
of the isotropic random feature model. Subsequently, in the isotropic setting, this principle was
rigorously justified using the Lindeburg exchange method under a variety of technical assumptions
on the data distribution, weight distributions, nonlinear activation function, and target function (Hu
& Lu, 2021). Goldt et al. (2020b) relaxes some of these conditions and provides extensive tests of
the resulting formulas on real-world datasets. To perform the analysis for anisotropic input data
and target function weights, as is pursued in d’Ascoli et al. (2021), an anisotropic extension of the
Gaussian equivalence theorem is required. Substantial numerical evidence and theoretical arguments
are presented by d’Ascoli et al. (2021); Loureiro et al. (2021) for the validity of this extension, but to
the best of our knowledge a rigorous proof in this context has not been established.
A.3 Weight-data alignment
One of the basic conclusions of our study of anisotropy is that weight-data alignment generally
improves performance. Similar observations appear in several recent works, albeit in slightly different
contexts. For example, Ghorbani et al. (2019) study the random feature model with isotropic inputs,
but anisotropic weights, in the case of a fixed quadratic target function and derives an asymptotic
formula for the test error in the population limit (ie. m n0, n1). For wide networks, n1 n0, the
error simplifies and is exactly proportional to a simple measure of alignment between the random
feature weights and the target, that is loosely related to the measure we propose in Definition 2.1.
Ghorbani et al. (2021) also study the random feature model in the population limit, and makes the
assumption that the target function is sensitive to a much lower dimensional subspace of the input by
positing sub-linear scaling of the dimensionality of the relevant subspace. They show that increasing
the power of the input data in this subspace generally decreases test error and the number of random
features required to learn a function of fixed complexity. Although the learning contexts and the final
scaling limits for m, n0, n1 are distinct, these phenomena parallel our main result on alignment (see
e.g. Fig. 3b for illustration in the context of the d-scale model).
A main contribution of the current paper is the partial order on the space of weight-data alignments,
which allows us to prove that the total error and the bias decrease in response to stronger alignment
(Proposition 3.3). Our results in this vein are most directly related to those of d’Ascoli et al. (2021),
who informally observe a basic relationship between weight-data alignment and performance, though
the impact of alignment is also investigated elsewhere, e.g. Loureiro et al. (2021, Fig. 2). While
these works informally examine concept of alignment, the conclusions about it derive from numerical
evaluation of the formulas, and as such the generality of some of the results remains unclear and some
of the underlying phenomena are partially obfuscated. For example, it is not clear why the “isotropic”
and “misaligned” curves cross each other in of d’Ascoli et al. (2021, Fig. 2c): naively, one might
expect the misaligned model to always perform worse. Our results provide a nice perspective on this
behavior: owing to the differing covariate distributions, the two forms of alignment are incomparable
under the partial order.
B	Useful inequalities
Here we include the statements and proofs of several auxiliary inequalities that we use throughout
the Supplementary Material.
B.1	BASIC PROPERTIES OF THE SELF-CONSISTENT EQUATION FOR x
We begin by reviewing the basic inequalities, first given in (Tripuraneni et al., 2021a;b). The
definitions of the following quantities can be found in Theorem 3.1.
3
Published as a conference paper at ICLR 2022
Lemma B.1 (Adapted from (Tripuraneni et al., 2021a;b)). We have the following bounds:
ω,τι,τι,x,Ia,b,I；b ≥ 0 and ∂χ ≤ 0.
Proof. As shown in (Pennington & Worah, 2018) for the unit-variance case, a simple Hermite
expansion argument establishes the relation η ≥ Z, which implies ω = s(η∕Z - 1) ≥ 0. From
APPendix G.4.1, τι and Ti are traces of positive semi-definite matrices and are therefore nonnegative.
From the same equations, it follows that X = γρτ1T1 ≥ 0. Nonnegativity of X implies ≥ 0 and
Ieb ≥ 0 from their definitions in (12). Finally, using the nonnegativity of ω, Ti, Ti, x, and I@,b, the
expression for ∂χ in Theorem 3.1 immediately gives,
∂X	X
dγ	Y + ργ( φ Ti + Ti)(ω + φI1,2)
≤ 0.
(S1)
□
Next we show that the self-consistent equation X =ω-1：1 appearing in Theorem 3.1 and defined in
(S237) admits a unique positive real solution for X.
Lemma B.2 (Adapted from (Tripuraneni et al., 2021a;b)). There is a unique real X ≥ 0 satisfying
X
i-γτι
ω+I1,1 '
Proof. Let t = 1∕X ≥ 0 and define,
h(t)= t(ρ(ψ- °) + √ρ2； J φ +4γρφψ〃 - 1) + ω + Ii,i(1∕t),
(S2)
which is a rewriting of eqn. (S237), so it suffices to show that h admits a unique real positive root. To
that end, first observe that limt→0 Ii,i(1∕t) = 0 and limt→∞ Ii,i(1∕t) = s so that
h(0) = ω > 0 and lim h(t)∕t = - min{1, φ∕ψ} < 0 ,	(S3)
t→∞
which together imply that h has an odd number of positive real roots. Next, we show that h is concave
for t ≥ 0:
hit) = - 2φ ((2(/	；2啖 d> //八3/2 + I2,3(1∕t))	(S4)
t3 ∖(p2(ψ — Φ)2 +4γρφψ∕t)3/2	)
≤ 0 ,	(S5)
which implies that h has at most two positive real roots. Therefore, we conclude that h has exactly
one positive real root. To provide a bounding interval for this root, we first observe that,
lim h(t) - ( - min{1, φ∕ψ}t + ω + S + Y ) = 0 ,	(S6)
t→∞	ρ∣ψ — φ∣
so that h(t) can be upper- and lower-bounded by linear functions ,
ω — min{1, φ∕ψ}t ≤ h(t) ≤ ω + S +—— γφ	— min{1, φ∕ψ}t.	(S7)
ρ∖ψ 一 φl
The roots of these linear functions bound the root of h, so we have
min{1, φ∕ψ }
ρ∣ψγ-φ∣ + 3 + S
≤ X ≤ min{1,φ∕ψ}
3
(S8)
□
4
Published as a conference paper at ICLR 2022
B.2	I AND Iβ INEQUALITIES
We now establish some useful properties of the I and Iβ functionals defined in (12). To begin, we
note that simple algebraic manipulations establish the following raising and lowering identities:
Ia-1,b-1 = φIa-1,b + xIa,b and Iaβ-1,b-1 = φIaβ-1,b + xIaβ,b .	(S9)
Next, we consider how the partial order of LJSDs given in Definition 2.1 leads to inequalities on the
Iβ functionals. Letting (Iaβ,b)1 and (Iaβ,b)2 to denote the corresponding functionals with the LJSDs
μι and μ2 respectively, We can establish the following useful lemma.
Lemma B.3 (Adapted from (Tripuraneni et al., 2021a;b)). Let μι ≤ μ2, so μι is more StrongIy
aligned than μ2 (recall Definition 2.1). Suppose the functions f,g,h : R → R are such that
f(λ) = g(λ)h(λ) and h(λ) is nonincreasing for all λ > 0, then
Eμi [qf (λ)] ≤ Eμι [qg(λ)]
Eμ2 [qf (λ)] - Eμ2 [qg(λ)].
(S10)
Proof. By the law of iterated expectation, we have
Eμι [qf(λ)] = Eμ2 [qg(λ)]Eλ
-E"2[qg(λ)∣λ] Eμι[q∣λ]	-
- Eμ2 [qg(λ)] Eμ2[q∣λ] ( ) .
(S11)
Note that the expectation Eλ in (S11) over λ is the same under μ1 and μ2 by assumption. More-
over, the function h(λ) is nonincreasing in λ by assumption. Finally, observe that the factor
Eμ2 [rg(λ)∣λ]∕Eμ2 [rg(λ)] defines a change in distribution for the random variable λ, since tak-
ing its expectation over λ yields 1. Denote a new random with this distribution by λ. Then, we may
apply the Harris inequality to see
Eμι [qf (λ)] = Eμ2 [qg(λ)]EΛ
一 「	，Lr
E^ h(λ)
(S12)
UT 「ci'll γ r
≤Eμ2[qg(λ呵 <⅛y 峻 hh(λ)i	(S13)
≤ Eμ2 [qg(λ)]Eλ
Eμ2[qg(λ)∣λ] Eμι[q∣λ]] E ΓEμ2 [qg(λ)∣λ]
,Eμ2 [qg(λ)] EUqM λ [ Eμ2 [qg(λ)]
(S14)
Eμι [qg(λ)]
Eμ2 [qg(λ)]
Eμ2 [qf (λ)].
(S15)
□
Corollary B.1. Let μ1 ≤ μ2 and (Iab)i := φ Eμi (qλa (φ + xλ)-b}. Then, for a ≤ 1 and b ≥ 0,
Eμ2 [λq] (Ia'b)2 - Eμι [λq]
(S16)
Proof. Note that h : λ 7→ φλa-1(φ + xλ)-b is a nonincreasing function of λ ≥ 0. Then, setting
g = λ and f = gh in Lemma B.3 gives the desired result.	□
Lemma B.4. Suppose the functions f, g, h : R → R are such that f(λ) = λg(λ)h(λ) and h(λ) is
nonincreasing for all λ > 0. Then, if the LJSD μ is aligned (see Definition 2.1), then Eλ[g]E*[qf] ≤
Eμ[qg]Eλ[f ].
5
Published as a conference paper at ICLR 2022
Proof.
Eμ[qf ] = Eμ[qλgh]	(S17)
=Eλ[Eμ [qλ∣λ]g(λ)h(λ)]	(S18)
=Eμ[g]Eλ kμ[qλ∣λ]h(λ)辔]	(S19)
L	Eμ[g]J
≤ Eμ[g]Eλ	Eμ[qλlλ] Ig(J ]	Eλ	[h(λ) E(J ]	(S2O)
_	Eμ [g] _l	L Eμ [g] _1
=F17Eμ [qλg]Eλ [f],	(S21)
Eλ [g]
where Eμ [qλ∣λ] is nondecreasing in λ because μ is aligned, so the inequality again follows from the
Harris inequality.	口
Corollary B.2. If μ is aligned,加琦 ≤ 〃-2穿+2.
Proof. Take g : λ → φλa(φ + xλ)-b and h : λ → 1∕λ in LemmaB.4.
□
C Weight-data alignment is a partial order
We restate Definition 2.1 for reference, and prove that it defines a partial order. The definition and
proof are identical to those of Tripuraneni et al. (2021a;b), but differ in notation so we repeat them
here for clarity.
Definition C.1 (Restatement of Definition 2.1). Let μι and μ2 be LJSDS with the same marginal
distribution of λ. If the asymptotic overlap coefficients are such that Eμι [λq∣λ] /E*2 [λq∣λ]=
Eμι [q∣λ] /Eμ2 [q∣λ] is nondecreasing in λ, we say that μι is more StrOngIy aligned than μ2 and write
μι ≤ μ2. Comparing against the case of isotropic weight distribution, μ0, we say μι is aligned when
μι ≤ μ0 and anti-aligned when μι ≥ μ0.
Proposition C.1. Definition 2.1 is a partial order over over weight-data alignments μ.
Proof. Reflexivity is satisfied as Eμ[q∣λ]∕Eμ[q∣λ] = 1 is nondecreasing for all μ.
For antisymmetry, we see μι ≤ μ2 and μ2 ≤ μι imply E*1[q∣λ]∕E*2 [q∣λ] is constant in λ as it is
nonincreasing and nondecreasing. However, setting E*1[q∣λ] = cE*2 [q∣λ] and taking expectation
over λ and rearranging yields 1 = Eμι [q]∕Eμ2 [q] = c, so in fact Eμι [q∣λ] = E*2 [q∣λ]. Assuming
that μι and μ2 are absolutely continuous (the case where they are a sum of point masses is similar),
we can write their densities as Pi(λ, q) = Pi(λ)pi(q∣λ). By assumption pι(λ) = p2(λ), so it suffices
to show pι(q∣λ) = p2(q∣λ) almost everywhere. Next note
0 = %i[q|N - E“2[q|N= [ q (Pi(q|x)- P2Mλ)) dq,
R+
we have thatpι(q∣λ) - p2(q∣λ) = 0 almost everywhere.
Finally, for transitivity assume μι ≤ μ2 and μ2 ≤ μ3, then
Eμι[q∣λ] _ Eμι[q∣λ] Eμ2[q∣λ]
- . - --	- . - ∙	- . 。
E*3[q]Z	叽2同川限回川
(S22)
(S23)
so Eμι [q∣λ]∕Eμ3 [q∣λ] is the product of two nondecreasing, positive functions and is thus also nonde-
creasing.	口
D Proofs of propositions
D.1 Proposition 3.1
Proposition D.1 (Restatement of Proposition 3.1). In the setting of Theorem 3.1, the bias B* is a
nonincreasing function of overparameterization ratio φ∕ψ.
6
Published as a conference paper at ICLR 2022
Proof. Recall from Theorem 3.1 that the bias is given by
Bμ = φI1,2 ,
where x is the unique positive real root of the self-consistent equation,
1 - γτ1
X =-------——.
ω + I1,1
Differentiating (S24) with respect to φ∕ψ gives,
∂Bμ =	∂Bμ =	2 ∂X Jβ
∂ (φ∕ψ)	φ ∂ψ	∂ψ 1,3 .
(S24)
(S25)
(S26)
Since Lemma B.1 gives Ieb ≥ 0, it suffices to show ∂∂χ ≤ 0, which immediately follows by
implicitly differentiating (S25)) and simplifying the expression,
∂x	ρxτ1 (ω + I1,1 )
——=------：---------：-------------
dψ	φ(1+ P(TI + φ τ1)(ω + φI1,2))
≤0,
(S27)
where the inequality also follows from Lemma B.1. Therefore we conclude that 就几)≤ 0.	□
D.2 Proposition 3.2
Proposition D.2 (Restatement of Proposition 3.2). In the setting of Corollary G.1 and in the overpa-
rameterized regime (ψ < φ), the variance Vμ is a nonincreasing function Ofoverparameterization
ratio φ∕ψ.
Proof. In the overparameterized regime, Corollary G.1 gives the expression for the variance as,
Vμ = ɪ(σε + Iej) +	XIg (σ2 + Iβ,2),	(S28)
φ - ψ	, ω + φI1,2	,
and, since the self-consistent equation X =幻十1 ]is independent of ψ, we have ∂∂χ = 0 and,
dVμ = u φ l γ2 (σ2 + IeJ) ≥ 0,	(S29)
∂ψ (φ - ψ)2 ε ,
which implies that the variance is nonincreasing in the overparameterized regime.	□
D.3 Proposition 3.3
Proposition D.3 (Restatement of Proposition 3.3). Let μι, μ2 be two LJSDs such that μι ≤ μ2 (see
Definition 2.1). Then Bμι ≤ Bμ2, Eμι ≤ Eμ2, and Bμ∖∕Vμ∖ ≤ Bμ2∕Vμ2.
Proof. For the bias, Corollary B.1 implies (Ie2) 1 ≤ (I：2 )2 and therefore B*1≤ Bμ?.
For the test error, we use the explicit expression for the variance from Eq. (S378) and the identity
Iβ,2 = 1 Iβ,ι 一 XIβ,2 which follows from Eq. (S9) to write,
Eμ = C0 + CIIe,1 + C2Ie,2 ,	(S3O)
7
Published as a conference paper at ICLR 2022
where the Ci ≥ 0 and depend on μ only through the marginal λ (i.e. they are independent of the
weight distribution):
C0	=-ρφ∂χσ2 ((ω + φI1,2)(ω + I1,1) + φγT1I2,2) ≥ 0		(S31)
C1	ψ∂x =-ρφ∂γ ((	ω + φI1,2)(ω + I1,1) + 子(ω + ΦI1,2)) ≥ 0	(S32)
C2	ψ∂x =φ - ρφ∂γ	(Φψγτ1I2,2 - Φ~XT1 (ω + 建〒))	(S33)
	ψ∂x =-ρφ∂γ[	ψγτ1I2,2 -子(ω + S- ρψ∂χ!	(S34)
	=-ργdx (φ	TT2,2 - ψτ1 (ω + φI1,2) + ρX(1 + ρ(τιψ∕φ + Tι)(ω + φI1,2)))	(S35)
	FYdl	T1I2,2 + ρX (1 + ρT1(ω + φI1,2)))	(S36)
	≥0.		(S37)
It is now straightforward to write,
Eμ2- Eμι = C1(Iβ,1)2 + C2C⅛)2 - C1(Iβ,1)1 + C2(Iβ,2)1	(S38)
=CI((IeJ)2 -(Iej)1) + C2((Iβ,2)2 -(Ie,2)1)	(S39)
≥ 0 ,	(S40)
where the inequality follows from Corollary B.1. Similarly, we can write,
-Eμι = C0 1 ；2] + CJe,2] (IjI)2 + C2(Iβ,2)1 - C0 - CI(Iej)I - C2(Iβ,2)1
(I1,2 )2	(I1,2 )2
(S41)
=Co(* - 1) + Cι(*(Ie,1)2 -(Iej)I)
(I1,2 )2	(I1,2 )2
≤0,
(S42)
(S43)
where the inequality follows from Corollary B.1 and from Lemma B.3 with g : λ → φλ(φ + λx)-1
and h : λ → (φ + λx)-1. Finally, using Eμi = Bμi + Vμi ,the above implies Bμ∖∕Vμ∖ ≤ Bμ2∕Vμ2.
□
D.4 Proposition 3.4
Proposition D.4 (Restatement of Proposition 3.4). If the LJSD is aligned (see Definition 2.1), then,
in the setting of Corollary G.1, the test error has at most two interior critical points as a function of
the overparameterization ratio φ∕ψ.
Proof. From Corollary G.1, there is a critical point at the interpolation threshold φ∕ψ = 1. Therefore
it suffices to show that there is at most one additional interior critical point. Focusing first on the
overparameterized regime φ > ψ, the test error reads,
Eμ = ΦIe,2 + -ψ-(σ2 + Ie,ι) +	xI2,2 (σ2 + Ie,2),	(S44)
, φ -ψ ε ,	ω + φI1,2 ε ,
and, since ∂dx =0,
dΨ = (φ -φψ)2 M + Iej) >0,	(S45)
which implies that the test error is monotone decreasing in the overparameterized regime.
8
Published as a conference paper at ICLR 2022
Next, let us consider the case φ < ψ . In this case,
Eμ = φIβ,2+ ψ-φ (σ2+Ie,1)+xIβ,2,
so that,
∂Eμ
∂ψ
φ∣∂X Iβ,2 - (ψ⅛ (σ2+Iej)+ψ⅛∣∂X IeJ+∣∂X (χIβ,2)
—
W-W (σ+Iβ,J+∂χ (φ∂X Iβ,2+ψj-≠∂X Iβ,1+∂X (XIe，2)
(ψ^φ≠7 (σ2+ Ie,1) + ∂χ (-2φIe,3 - ψ-φ Ie,2 + Ie,2 - 2xIe,3)
(ψ⅛(σ2 +Iβ,ι)--Iβ,2
(ψ 'P (σ2 + Iej) + Ψ(ψ- Φ) ω + ΦIι,2 .
Therefore We see that ∂ψ = 0 implies
ψ = x(ω + I1,1) = 1 - (ω + φI1,2) ^^ɪ
or, equivalently, g(x) = 0 for
σε2 + I1e 1
g(x) = 1 - (ω + φI1,2)-------β-β  -----x(ω + I1,1) .
I2,2
First We note that g has at most one real root since its derivative is never positive,
0	σε2 + I1 1
g (X) = 2φI2,3 — β ,
I2,2
2 小丁	σ2 + Iβ,i
= 2φI2,3 —β—
I2,2
σε2 + I1β 1
+ (ω + φI1,2) (1 - 2-Iβ^^~
σε2 + I1β 1
-2(ω + φI1,2)--β~~L~
- (ω + φI1,1 ) + XI2,2
2 σ + Iβ,1
(Iβ,2)2
2 σ + Iβ,1
(Iβ,2)2
2 σ + Iβ,1
(Iβ,2)2
φI2,3I2β,2 - (ω + φI1,2)I3β,3
φ2I2,3I2β,3 - (ω + φI1,2 - XφI2,3)I3β,3
φ2I2,3I2β,3 - (ω + φ2I1,3)I3β,3
≤ 2φ2 σεIβIβ1 (I2,3Iβ,3-I1,3Iβ,3)
≤0,
(S46)
(S47)
(S48)
(S49)
(S50)
(S51)
(S52)
(S53)
(S54)
(S55)
(S56)
(S57)
(S58)
(S59)
(S60)
where the last line follows from Corollary B.2 since we are assuming μ is aligned.
Next, regarding X as a function of φ∕ψ, we consider the interval (x-,x+) for X- = x(φ∕ψ =
0) and χ+ = χ(φ∕ψ = 1). From the self-consistent equation for x, we immediately see that
X+(ω + I1,1(X+)) = 1 and X- = 0 so that
g(X+ ) = -(ω + φI1,2)
σ2 + Iβ,1
Iβ,2
(S61)
<0.
(S62)
9
Published as a conference paper at ICLR 2022
and
g(x-) = 1 - (ω + φ2Eμ[λ])σ2; E；2q；]) .	(S63)
Eμ[qλ2])
Observe that,
g(X-) > 0	⇔	σ2	< σ2	≡	Eμ[qλ	]	- Eμ[qλ]	.	(SM)
ω + φEμ[λ]
Therefore, from the intermediate value theorem, we conclude that g has no real roots in (x-, x+) for
σ2 > σ2, and exactly one real root if σ2 < σ2.	口
E Linear regression limit
To reduce to the linear case, we need to take ψ → 0 and σ(x) → x, in which case we have that
η = ζ = ρ → 1 and ω → 0, so that
so that
τι —→ x and τι —→ 一,
1 F
Y = X -Ii,i
=-----φEs2
X
s2
~μdata φ + xs2 .
(S65)
(S66)
(S67)
E.1 Comparison to Mel & Ganguli (2021)
m	∙ .< ∕τk /r 1 D z 1	1 ∙ r∖r∖r∖ ι ∖	. . 1 . I	-ι /	-ι I I ∖	I / T	1
To compare With (Mel & Ganguli, 2021), note that φ = 1∕α, Y = 1∕φλ, X = τι = φ∕λ, so we have
λ - ΦEs2 ~"d"a
s2λ
~ ,
λ + s2
(S68)
(S69)
which is the expression appearing in Eq. (8) in (Mel & Ganguli, 2021). To compare expressions for
the test error, note that
∂X	X
∂γ	γ + φIl,2
and so,
~
∂λ
Pf = ∂λ
∂φ∕X
=------
∂φγ
1 ∂x
X2 ∂γ
_	1
x(γ + ΦI1,2)
(S70)
(S71)
(S72)
(S73)
(S74)
10
Published as a conference paper at ICLR 2022
so that,
E = φIβ,2 + PJ (φIβ,2 + σ2) x2I2,2	(S75)
=φIβ2 +	φIβ2(xI1,1 - xφI1,2) +	εx2I2,2 ,	Pf	,	Pf	(S76)
=φIβ2 +	φIβ2 (I - X(Y + φI1,2)) +	εx2I2,2 ,	Pf	,	Pf	(S77)
=φIβ,2 +	φIj2(I - Pf) +	εx2I2,2 ,	Pf	,	Pf	(S78)
=ρ1 (φIβ,2 + σ2x2I2,2).	(S79)
In contrast to our conventions, the error F in (Mel & Ganguli, 2021) does include an additive constant
E+σ2
induced by the label noise, and also normalizes by the total output variance i.e. F = £例].Taking
this relation into account and using the definitions of I and Iβ, and finally translating the notation
via the substitutions φ → 1∕α, λq → V = (SUTw)2, v^ → %, V^ → f§, We find
F= E+σ
Var [y]
(S80)
,公(+)2
占了
嵋+1
+
Var [y]	Pf V Var [y]
Eμ
2
+ φ —土 E“
Var [y] μ
(S81)
fn + P- ( fsEμ V2
+ fn 1 Eμ
(S82)
1
2
E)
E 了
which is Eq. (6) of (Mel & Ganguli, 2021).
E.2 Comparison to Wu & Xu (2020)
Wu & Xu (2020) study the case of anisotropic regularizer:
βλ = (X >x + λ∑w)-1 X > y
(S83)
with n samples, p features, X ∈ Rn×p and p/n → γ. After simplifying the error expression they
arrive at eq. 3.1:
E (y - x>βλ) = σ2 (ι + n1tr
(S84)
λ2
+----tr
(S85)
Setting Σw → I must give the expression for isotropic regularization, thus the effect of the weighting
matrix Σw can be accounted for by just changing the parameters of the isotropic model. The effective
feature covariance is Σ → Σχ∕w and the effective weight covariance is ∑β → ∑wβ.
The error expression given in eqs. 4.1-4.3 is
E [(y - x>βλ)2] → rn⅜⅛ .卜E
gh
(h ∙ m (―λ) + 1)2
+ σ2
(S86)
where
h
λ =^⅛
m (―λ)
1
- γE
1 + h ∙ m (一λ)
(S87)
1 = (m2 (―λ)
- γE
h2
(h ∙ m (―λ) + 1)2
m0 (-λ)
(S88)
11
Published as a conference paper at ICLR 2022
In our notation, the predicted output on a new input x is
y =(,——β>X + Etr)	F (——F> F + YIm)	fɪ F >f(χ))	(S89)
∖√n0	)	n1	n1	
→ (,——β>X + Etr	)X — X>X + YIm)	I LX >x	(S90)
∖√n0	,	n0	n0	
=(√no β>X+Etr)	I X> ( ɪXX> + Y-	Ino ]	-- X	(S91)
	n0	0	n0	
=y>X> (XX> + φ	YIno) X		(S92)
where X has √m = √ 11 normalization. Thus translating our notation involves setting φ → Y,
Y → λ∕γ, Σ → Σχ∕w, λ → h, ∑β → γ∑wβ, and q → γg. In this new notation, our equation for X
reads
Yh
λ = Y - YE----L	(S93)
X 1 + h ∙ (χ)
which shows X → Ym (-λ), and therefore ∂χ → Mdm(-λ = -γ2m0 (-λ). Next, note that
∂	I2,2	_	X	I”	(S94)
∂"		-Y + ΦI1,2I2,2	
		_ I1,1 - φI1,2 Y + ΦI1,2	(S95)
		_ I1,1 + Y - 1 Y + ΦI1,2	(S96)
		1∕x	(S97)
		二	———1 Y + φI1,2	
		1 ∂X X2 ∂y	(S98)
So the full error is
E=φIβ,2- ∂x	φI1β,2I2,2 + σe2I2,2	(S99)
=φ(1-装	I2,2)工 1,2 - σ2 If I2,2	(S100)
=(-3 ∂x ) X2 ∂Y	φIβ2+σ2 (-X2 ∣x - 1)	(S101)
=(-4 Ix) X2 ∂Y	(φI1β,2 + σe2) - σe2	(S102)
→ m0(-λ)( m2(-λ) y	ye	hg―2 + σ2! - σ2 (1 + m(-λ)h)	(S103)
which, after removing the additive shift, matches the expressions given in (Wu & Xu, 2020) eq. 4.1.
F Structured learning curves
F.1 Effect of spectral gap
Here we demonstrate that a large gap in the spectrum of Σ can induce steep cliffs in the learning
curves as a function of the overparameterization φ∕ψ:
Suppose there is a gap in the spectrum of Σ of size g. That is, there are λ- < λ+ such that there is
no eigenvalue λ ∈ (λ-, λ+) and λ+ = g. Assuming φ < 1 and μ is aligned, and working in the
12
Published as a conference paper at ICLR 2022
noiseless ridgeless limit, We will show the slope of the learning curve ∂φ∕ψμ becomes arbitrarily
negative for small ω .
From Theorem (3.1), x, τ1 satisfy
(ψ(ψ - φ)2 + 4xψφγ∕ρ + ψ - φ
2ψγ
(S104)
(S105)
Since X ≤ mm{?%} (Eq. (S8)), for ω > 0, X stays finite in the ridgeless limit Y → 0, so
γτ1 →
∣ψ - φ∣ + ψ - φ
2ψ
(S106)
We have the numerator 1 - γτ1 → min(1, φ∕ψ), and
X (ω + φE ʌ = = min (l,φ).
φ+Xλ	ψ
Since X = 0 is not a solution for 0 <ψ,φ < ∞, we can change variables to γ = X, giving
ω^ + E--- = m min Γl, φ
Y Y + λ φ ∖ ψ
(S107)
(S108)
which implies 7 is a continuous decreasing function of φ∕ψ (keeping φ fixed). Taking the limit of
(S108) directly shows that 7max ：= limφ∕ψ→o 7 = ∞, while 7min ：= limφ∕ψ→∞ Y satisfies
1λ
ω 三—+ eΞ-TT
Ymin Ymin+λ
1
φ
(S109)
By the intermediate value theorem, Y7 takes all values in the interval (Y7min, ∞). For φ < 1, using
E 亍min+λ ≤ 1 we obtain Ymin ≤ ω 昌.
We assume that ω 1φφ ≤ λ-, so the previous bound gives 7min ≤ λ— and thus Y attains all values
in (λ-, λ+). In particular, there is some 0 < φ∕ψ < 1 such that Y (φ∕ψ) = ,λ-λ+. At this point,
differentiating (S108) gives
~ ∂ 1 — 1…而	λ
-Y ∂7 Ψ = ω7 + 7E (Y+I7
≤ ω7 + 7 ((Y+++? P (λ ≥ λ+)+(Y⅛7 P (λ ≤λ-)
ω 1 +
Y7
Since -Y q-1 = 1 ( 2* X ) , we get
/ ∂γ ψ φ ∖ ∂(φ∕ψ) 1	,	0
11
√
(vg+1 * * * * *)2
∂ log X
∂ ∂ (φ∕ψ)
For large spectral gap g this tends toward
1 √λ+λ- ∕ ∂ log X
---------≤ —~：—：——
Φ ω 一 ∂(φ∕ψ)
(S110)
(S111)
(S112)
(S113)
(S114)
φω i +
13
Published as a conference paper at ICLR 2022
If the nonlinearity ω is small compared to the middle of the spectral gap ,λ+λ-, X undergoes large
fractional change as a function of the overparameterization ratio φ∕ψ.
To see how this affects the test error, we can use the lowering identity Iaβ-1,b-1 = φIaβ-1,b + xIaβ,b
to write the ridgeless error expression from Eq. (S46) as
Eμ =	二φIβ,2+ ψ-φ	σ2 + I1β,1 + XI2β,2	(S115)
	=上 σ2 + - ψ - φ σ 十 ψ	ψ	I β -φ I1,1.	(S116)
So we can write			
∂ (Φ∕ψ) log (	Eμ - ψ-φσ2.	)=∂(J∕ψ)log ψψ-φ Iej	(S117)
		ψ∂β =ψ - φ + ∂ (φ∕ψ)l g 1,1	(S118)
For general a, b, we have			
∂	log Iaβ,b = -b	∂ log X λ E [(7+λ)b+1 q]	(S119)
∂(φ∕ψ)		d(≡^ e h (⅛qi	
Specializing to a = b = 1, and using the fact that --++^ Eq [q∣λ] is a nondecreasing function of λ
(guaranteed since q is aligned), we may apply the Harris inequality to obtain
	∂ 1	τβ -∂ψlog Iej	JdlogX ʌ Eλ [γ+λ (γ+λEq [q|N)]			(S120)
		一'd(加砂"Eλ[(≡		λ Eq [q∣λ])]	
		≥ ( H ) Eλ ∂ (φ∕ψ)	■ λ ■ .γ + λ.		(S121)
		g→∞ ∂ ∂ log x、 > ld(φ∕ψ)J	P (λ > λ+)		(S122)
		1 pλ+λ-,	、 ≥ φ . p (λ>λ+),			(S123)
which implies					
∂ 	7~:	:		 ∂(φ∕ψ)	log (Eμ- ψ	φ2	ψ	∂	β -φ J - ψ - φ ∂(φ∕ψ)	1,1			(S124)
		ψ	1 Pλ+λ- ≥-ψ-φ 十 φ1^p(λ>λ+)			(S125)
In particular, if σε2 =	0, then				
	∂ log Eμ 	2~:	:		 ∂(φ∕ψ)	ψ	1 pλ+λ-,	、 ≥- ψ-φ 十 φL丁P (λ>λ+)			(S126)
Thus as ω → 0 the learning curve becomes arbitrarily steep at the critical value X = φ//λ+λ-.
F.2 Analysis of the D-scale model in the separated limit
We will consider the d-scale covariance model:
λn = Cα∖ Pn = ~.,	n = 0, 1, ∙∙∙ ,d - 1
d
where C is chosen so that
1 d-1	1 1 αd
1 = S = tr[Σ] = 1 TCan = C11—
d	d1-α
n=0
(S127)
(S128)
14
Published as a conference paper at ICLR 2022
We will obtain expressions for X in the limit of small λ. Consider the ridgeless limit of 7 := φ∕x:
1	λn	_	1
γω 乙pn 7 + λn	max(φ,ψ)
(S129)
Suppose, ω sits between the scales Caj, Caj+1. To enforce this constraint, we will take ω = ωαj+ 2
where ω is a constant independent of α.
The a scaling of γ7 will depend on the value of max(φ, ψ). Discarding the second term in (S129) we
obtain max (φ, ψ) ω ≤ γ, and thus the lowest possible scaling for 7 is 7 = Cj+1 αj+1, Substituting
this ansatz into (S129) and taking the limit a → 0, we obtain
1	1	1 X
max (φ, ψ)	7	d 乙
Cαn
1
Cj+1 αj+2 + Can
(S130)
α→0 1 j + 1
--→ -ω H--
γd
(S131)
Solving for Y gives 7 =1 ɪmX(φψ)j+ι ω, For other values of max(φ, ψ), 7 may have higher scaling,
ie. Y = Ckak with k ≤ j. Substituting and solving for Y we obtain Y = '-^)0+)-1 λk, Thus we
obtain the following self-consistent solutions for γ7:
(max(φ,ψ)
1-max(φ,ψ)j+1
max(φ,ψ)k+1 -1
1-max(φ,ψ) kd
λk
max (φ, ψ) < j++1
k+1 < max (Φ, ψ) < d
(S132)
ω
Thus Y7 takes on the scale of a single eigenvalue λk for a range of overparameterization ratios
corresponding to k+ι < ψ max (φ, 1)< d, To understand what happens at the transitions between
these regimes, we can apply the results from the previous subsection F.1 for generic Σ with large
spectral gap, In the notation of F,1, the D-scale model has a spectral gap between each pair of
consecutive scales of size g = λj∕λj+1 = Caj∕Caj+1 = 1∕a and as a consequence, Y7 will exhibit
1	1	1	1	1
near infinite slop as it passes through the middle of a gap VZλj+ιλj∙ = Cαj+ 2. Comparing to the
self-consistent solutions (S132) these transitions must happen at the critical values max(φ, ψ) = k+τ
for k ≤ j , At these transition points, the error exhibits steep cliffs in the parameter regime descried
in F,1,
G Proof of Theorem 3,1
The proof closely follows the methods described in (Adlam et al,, 2019; Adlam & Pennington,
2020a;b; Tripuraneni et al,, 2021a;b), Indeed, precisely the same techniques from operator-valued
free probability used in those works apply here, The main and only difference is the anisotropic
weight covariance Σβ, which changes the details of the computations but not the arguments justifying
the linearized Gaussian equivalents and the application of operator-valued free probability, We
therefore refer the reader to those previous works for an in-depth discussion of methods and merely
focus here on the details of the requisite calculations. Throughout this section, we use tr to denote
the dimension-normalized trace, i.e. tr(A) = ɪtr(A) for a matrix A ∈ Rn×n.
G.1 Decomposition of the test loss
The test loss can be written as,
with	e∑* = E(x,y)(y - y(x))2 = EI + E2 + E3	(S133) E1 = E(x,ε)tr(y(x)y(x)>)	(S134) E2 = -2E(x,ε)tr(Kx>K-1Y >y(x))	(S135) E3 =E(x,ε)tr(Kx>K-1Y>YK-1Kx).	(S136)
15
Published as a conference paper at ICLR 2022
Recall the kernels K = K(X, X) and Kx = K(X, x) are given by,
F >F	1
K =——+ YIm	and	Kx =  F > f.	(S137)
n1	n1
Using the cyclicity and linearity of the trace, the expectation over x requires the computation of
ExKxKx> ,	Exy(x)Kx> ,	Exy(x)y(x)> .	(S138)
As described in detail in (Tripuraneni et al., 2021a;b; Adlam et al., 2019; Adlam & Pennington,
2020a; Mei & Montanari, 2019), asympotically the trace terms E1, E2, and E3 are invariant to a
linearization of the random feature vector f,
f → flin = pWχX + pη-ζθ,	(S139)
√no
where θ ∈ Rn1 is a vector of iid standard normal variates. Similarly, We will take the linearization of
the training features to be ^P WX + √ - ZΘ where Θ ∈ Rn1 ×m has standard normal components.
n0
The expectations over X are now trivial and we readily find,
Ex Kx Kx> =	=二F>(-W∑W> + (η - Z)InI)F n1	n0	(S140)
Exy(X)Kx> =	=√p-> ΣW >F n0n1	(S141)
Exy(X)y(X)> =	二-1 β∑β> n0	(S142)
(S143)
(S144)
(S145)
(S146)
(S147)
(S148)
(S149)
(S150)
(S151)
Next, we recall the definition, Y = β>X∕√n0 + €, and, using the above substitution, we find
Ee [Y>Y] = -1X>∑βX + σ2Im
n0
Ee [Y>Exy(x)K>] = -^-X>∑β∑W>F.
n0 n1
Putting these pieces together, we have
tr	tr(∑β∑)
El =
n0
E2 = E21
E3 = E31 + E32 ,
where,
E21 = -2 -∕2pr- Etr(X >∑β∑W >FK-1)
n0 n1
E31 = σ2Etr (Kτ∑3KT)
E32 = 1-Etr (Kτ∑3KTX>∑βX)
n0
and,
Σ3 = —ρ2 F > W ΣW >F + η-2ζ F >F.
n0n21	n21
G.2 Decomposition of the bias and total variance
Note that it is sufficient to calculate the bias term given the total test loss, since the total variance
can be obtained as VΣ = EΣ - BΣ . Following the total multivariate bias-variance decomposition
16
Published as a conference paper at ICLR 2022
of (Adlam & Pennington, 2020b), for each random variable in question we introduce an iid copy of it
denoted by either the subscript 1 or 2. We can then write,
B∑ = E(χ,y)(y - E(w,x,ε)y(x; W,X,ε))2	(S152)
=E(χ,y)E(W1,"ε1)E(W2,χ2,e2)(y - y(x; W1,X1, ει))(y - y(x; W2,X2,⑹)(S153)
=tr(*β*) + E21 + H000 ,	(S154)
n0
where an expression for E21 was given previously and H000 satisfies
H000 = Ey(x; W1,X1,ε1 )y(x; W2,X2g),	(S155)
where the expectations are over x, W1,X1 ,ει, W2, X2, and ^. Recalling the definition of y,
y(x; W, X, ε) := Y(X, e)K(X, X; W)-1K(X, x; W)	(S156)
and the techniques described in the previous section, it is straightforward to analyze the above term.
First note we can write,
ExK(X1, x; WI)K(X,X2; W2)=3F>W1∑W>F⅛ .
n0n1
(S157)
Here we have defined F11 ≡F (W1, X1) and F22 ≡F (W2, X2). Now we proceed to calculate H000
as
H000 = Ey(x; W1,X1,ε)y(x; W2,X2,E2)	(S158)
=EK(x,X2; W2)K(X2,X2; W2)-1Y(X2,e2)τY(X1,ε1)K(X1,X1; WI)TK(Xi, x; W)
(S159)
=Etr(K(X2,X2; W2)-1XTX1K(X1,X1; WI)TK(Xi, x; W)K(x,X2; W2)) (S160)
=4 Etr(K221X> ∑β XιKu1F>Wι∑WT F⅛)	(S161)
n20n2i
≡ E4 ,	(S162)
where in the second-to-last line we have defined Kii ≡ K(Xi, Xi; Wi) and K22 ≡
K(X2,X2;W2).
G.3 Summary of linearized trace terms
We now summarize the requisite terms needed to compute the total test error, bias, and variance after
using cyclicity of the trace to rearrange several of them. In the following, we slightly change notation
in order to make explicit the dependence on the covariance matrix Σ. To be specific, whereas above
we assumed that the columns of Xi and X2 were drawn from multivariate Gaussians with covariance
Σ, below we assume that they are drawn from multivariate Gausssians with identity covariance. This
change is equivalent to replacing Xi → Σ1/2Xi and X2 → Σ1∕2X2 in the above expressions. We
utilize this definition so that Xi, X2, Wi, W2, and Θ all have iid standard Gaussian entries. From
the previous computations, we can now write the requisite terms as,
Σ3	=	Σp2 FTWiςWTFii + η-ZFTFii n0ni	ni	(S163)
E2i	=-2tr (XT∑i∕2∑β∑W>FiiKlIi) n0 ni	(S164)
E3i	=σftr (KuiΣ3Kni)	(S165)
E32	=n1tr (K-Li∑3K-⅛TΣi∕2∑βΣi/2Xi)	(S166)
E4	=品tr (F22K22iXTΣi∕2∑βΣi/2XiK-IiFTWi∑WT)	(S167)
EΣ	二 一证(夕夕6 ) + E2i + E3i + E32 n0	(S168)
BΣ	二一trN" ) + E2i + E4 n0	(S169)
VΣ	EΣ - BΣ	(S170)
17
Published as a conference paper at ICLR 2022
G.4 Calculation of error terms
To compute the test error, bias, and total variance, we need to evaluate the asymptotic trace objects
appearing in the expressions for E21, E31, E32, and E4, defined in the previous section. As these
expressions are essentially rational functions of the random matrices X, W, Θ, Σ, and Σβ, these
computations can be accomplished by representing the rational functions as single blocks of a suitably-
defined block matrix inverse - the so-called linear pencil method (see eg . Far et al., 2006) - and then
applying the theory of operator-valued free probability (Mingo & Speicher, 2017). These techniques
and their application to problems of this type have been well-established elsewhere (Adlam et al.,
2019; Adlam & Pennington, 2020a;b), we only lightly sketch the mathematical details, referring the
reader to the literature for a more pedagogical overview. Instead, we focus on presenting the details
of the requisite calculations.
Relative to prior work, the main challenge in the current setting is generalizing the calculations to
include an arbitrary weight covariance matrix Σβ . This generalization is facilitated by the general
theory of operator-valued free probability, and in particular through the subordinated form of the
operator-valued self-consistent equations that we first present in eqn. (S201). The form of this
equation enables the simple computation of the operator-valued R-transform of the remaining random
matrices, W, X, and Θ, which are all iid Gaussian and can therefore be obtained simply by using
the methods of (Far et al., 2006). The remaining complication amounts to performing the trace in
eqn. (S201), which asymptotically becomes an integral over the LJSD μ. While this might in general
lead to a complicated coupling of many transcendental equations, it turns out that the trascendentality
can be entirely factored into a single scalar fixed-point equation, whose solution we denote by x (see
eqn. (S237)), and the remaining equations are purely algebraic given x. To facilitate this particular
simplification, it is necessary to first compute all of the entries in the operator-valued Stieltjes
transform of the kernel matrix K, which we do in Sec. G.4.1. Using these results, we compute the
remaining error terms in the subsequent sections.
As a matter of notation, note that throughout this entire section whenever a matrix X , X1 , or X2
appears it is composed of iid N (0, 1) entries as in Appendix G.3. This differs from the notation
of the main paper, but we follow this prescription to ease the already cumbersome presentation.
This definition of X allows us to explicitly extract and represent the training covariance Σ in our
calculations.
G.4.1 K-1
The NCAlgebra Mathematica package (NCRealization method; algorithm described in Helton et al.,
2006) was used to generate the following matrix pencil QK-1 :										
	(	Im Θ√η-ζ 	1=- √nr 0 0 0 X 	1= √n0 0 0 0	√FZθ>	√ρχ>	0 0 -Σ1/2 In0 0 0 0 0 0	0 √ρw 	，— √nr 0 0 In0 0 0 0 0	0 0 0 0 -Σ1/2 In0 0 0 0	0 0 0 Σ √ρ 0 0 In0 0 0	0 0 0 0 0 0 -Σ1/2 In0 0	0、 0 0 0 0 0 0 X 	 √n0 Im (S171)
QKT =	∖		Y√nι In1 0 W> 	1= √nr 0 0 0 0 0	Y√n0 0 In0 0 0 0 0 0 0						
This matrix is specifically chosen so that inverting [QK-1]> and taking the normalized trace of its
first block gives exactly Y trK-1, the quantity of interest. Computing the full inverse of [QK ]>
via repeated applications of the Schur complement formula and taking block-wise traces shows that
GK-1	= Y tr(KT)
cK-ι	_ φ tr (∑β∑1^XKTX>Σ1/2)
9,1	no
GK-I	= Y tr(KT)
(S172)
(S173)
(S174)
18
Published as a conference paper at ICLR 2022
1
--
K3
G
1
一 3
K2
G
I √ρ tr (∑1/2 WTFKTXT)
√n0n1
1
一 3
K5
G
YrP tr ΣΣ1∕2WTKTW)
Y√ρ tr (∑WtKTW)
1
一 3
K
G
1
一 3
K7
G
1
一 3
G
1
,4
G
nι
tr (∑βΣ1∕2WtFK-1XtΣ1/2) tr(∑β∑1/2)
--------------------------------
√n0nι	√P
tr (∑β∑WtFKTXt∑1/2)	tr(∑β∑)
---------------------	
√n0nι--------------√P
GK-I=	√ρ tr (FK-1XtWt)
G5,6 =	√n0nιψ
1
-4
G
]	√ρ tr (∑1/2WtFKTXt)
√n0nι
1
4
K5
G
1
4
K7
G
-<,
G
1
-
4
f -
1
,5
K3
G
-;
G
1
-
5
f -
Y√ρ tr (KTWWT)
nιψ
tr (∑β∑1/2χFTKTW)	tr(∑β)
-------------------------
√n0nι-------------√ρ
€r (∑βΣ1/2WtFK-1Xt∑1/2)	tr(∑.∑1/2)
-------------------------------
√n0n1-----------------√ρ
1_	√ tr (∑1∕2XKTXT)
4,6	n0
√ tr (ΣXKTXT)
1
5
K7
G
1
5
G
-6
G
6
K乙
G
n0
€r (∑βΣ1/2XKTXTΣ1/2)
no
tr (∑β∑XKTXt∑1/2)
no
√ρ tr (KTXTX)
—
noφ
€r (∑βΣ1/2XK-1Xt)
no
tr (∑βΣ1/2XK-1XTΣ1/2)
6
G
1
一 7
K7
G
1
一 7
G
no
(S175)
(S176)
(S177)
(S178)
(S179)
(S180)
(S181)
(S182)
(S183)
(S184)
(S185)
(S186)
(S187)
(S188)
(S189)
(S190)
(S191)
(S192)
(S193)
(S194)
where Gk := id9 0 tr [(Qk )t]-1 ∈ Mg(C) is a scalar 9 × 9 matrix whose i, j entry GKj
is the normalized trace of the (i, j)-block of the inverse of [Qk 1 ]t. We have also defined K =
广FFT + γlnι (note that K is m × m while K is n1 × n1). It is straightforward to verify that when
19
Published as a conference paper at ICLR 2022
the n0, n1, m → ∞ limit is eventually taken, each entry of GK-1 is properly scaled and will tend
toward a finite value.
We aim to compute the limiting values of these trace terms as n0, n1, m → ∞, as they will be related
to the error terms of interest. To proceed, recall that the asymptotic block-wise traces of the inverse
of QK-1 can be determined from its operator-valued Stieltjes transform (Mingo & Speicher, 2017).
The simplest way to apply the results of (Far et al., 2006; Mingo & Speicher, 2017) is to augment
QK to form the the self-adjoint matrix QK ,
(S195)
and observe that We can write QK 1 as,
QK-1 = Z - QW-X,θ - QK-1
=	0 I9-	0	[QKW-,X1,Θ]>!-	0	[QΣK-1]> !	(S196)
I9	0	QKW,-X1,Θ	0	QΣK-1	0	,
where
/			0 Θ√η √n		√-ζΘ	>	√ρx>	0 0-	0 √ρ ^√		0 0	0 0	0 0	0 0	∖	
		-		-Z 1	γ√n 0		Y√n0 0			W n						
			0		0		0	0	0		0	0	0	0		
qW,x,θ =-			0 0		W > 	 √nT 0		0 0	0 0	0 0		0 0	0 0	0 0	0 0		(S197)
			X 	1= √n0		0		0	0	0		0	0	0	0		
			0		0		0	0	0		0	0	0	0 X		
			0		0		0	0	0		0	0	0		1= √n0		
	∖		0		0		0	0	0		0	0	0	0	/	
		0	0	0	0	0	0	0		0		0 ∖				
		0	0	0	0	0	0	0		0		0				
		0	0	0	-Σ1/2	0	0	0		0		0				
QK-1	=-		0 0	0 0	0 0	0 0	0 0	0 -Σ1/2	∑β √ρ 0		0 0		0 0	,			(S198)
		0	0	0	0	0	0	0		0		0				
		0	0	0	0	0	0	0	-Σ1/2			0				
		0	0	0	0	0	0	0		0		0				
	∖	0	0	0	0	0	0	0		0		0				
and the addition in (S196) is performed block-wise. Note that we have separated the iid Gaussian
matrices W, X, Θ from the constant terms and from the Σ-dependent terms. Denote by GK ∈
M18(C) the block matrix
GK—1 =(GK-1	[GKj)=idi8 X《K-1 )—1 ,
(S199)
and by GK 1 ∈ M18(C) the operator-valued Stieltjes transform of QK 1. Using (S196) and the
definition of the
operator-valued StieltjeS transform G^iκ-1 ∣ ^jκ-1
QW,X,Θ +QΣ
we can write
GK-1 = id18 X tr (Z - QW-1,Θ - QK-1 )—1 = GQK-Io+QK-1 (Z)	(S200)
Thus using the subordinated form of the equations for addition of free variables (Mingo & Speicher,
2017; section 9.2 Thm. 11), and the defining equation for GK , the operator-valued theory of
free probability shows that in the limit no, n1 ,m → ∞, the Stieltjes transform GK satisfies the
20
Published as a conference paper at ICLR 2022
following 18 × 18 matrix equation:
GGKT= GK-1 (Z - RWX,θ(GKT))
=id g tr (Z - RW,X,Θ (GGK ) - QK )
(S201)
where RWXθ(Gk 1) ∈ Mi8(C) is the operator-valued R-transform of Q 冉；今 Notethat(S201)
is a coupled set of 18 × 18 scalar equations and thus eliminates all reference to large random matrices.
To see this, note that Z, GK , RWjζ θ(Gk ) are all scalar-entried 18 X 18 matrices. The right-
hand side of (S201) is defined by expanding the inverse to obtain an 18 × 18 block matrix whose
blocks involve various rational functions of Σ, ∑β and the scalar entries of Z, GK , RW,χ,θ(GK ).
Finally one computes the normalized traces of these blocks, giving scalar values and eliminating all
reference to random matrices. Below, when writing out these equations explicitly, we will use the
fact that traces of rational functions of Σ, Σβ tend toward expectations of the corresponding rational
functions over the LJSD μ. Both here and in the sequel, to ease the already cumbersome presentation,
we use GK-1 to also denote the limiting value satisfying (S201).
As described in (Adlam & Pennington, 2020a;b), since QaX ㊀ is a block matrix whose blocks are iid
Gaussian matrices (and their transposes), an explicit expression for RW,X,θ(GK 1) can be obtained
through a covariance map, denoted by η (Far et al., 2006). In particular, η : Md(C) → Md(C) is
defined by,
[η(D)]ij = X σ(i, k; l, j)αkDkl ,	(S202)
kl
where αk is dimensionality of the kth block and σ(i, k; l, k) denotes the covariance between the
entries of the blocks ij block of QW,X,Θ and entries of the kl block of QW,X,Θ. Here d = 18 is the
number of blocks. When the constituent blocks are iid Gaussian matrices and their transposes, as
is the case here, then RWX ⑥=η (Mingo & Speicher, 2017; section 9.1 and 9.2 Thm. 11), and
therefore the entries of RW,X,Θ can be read off from eqn. (S195). To simplify the presentation, we
only report the entries of RW,X,Θ (GK 1) that are nonzero, given the specific sparsity pattern of
GK-1 . The latter follows from eqn. (S201) in the manner described in (Mingo & Speicher, 2017; Far
et al., 2006). Practically speaking, the sparsity pattern can be obtained by iterating an eqn. (S201),
starting with an ansatz sparsity pattern determined by Z, and stopping when the iteration converges to
a fixed sparsity pattern. In this case (and all cases that follow in the subsequent sections), the number
of necessary iterations is small and can be done explicitly. We omit the details and instead simply
report the following results for the nonzero entries:
	RW,x,θ(GK )=( RK- RW,	0	RWK,X X1,Θ(GK-1)	1,Θ(GK-1)>	,	(S203)
where,	[RKW,X,Θ (GK	)]1,1	_	GK2 (Z - η) -	.√PGK3 1	(S204)
		Y		
	[RKW,X,Θ (GK	)]1,9	√ρG8K-1 二	 γ		(S205)
	[RKW,X,Θ (GK	)]2,2	=ψGK-1 (Z-η) γφ	+ √ρψGK-1	(S206)
	[RKW,X,Θ (GK	)]4,5	=√pGK2		(S207)
	[RKW,X,Θ (GK	)]6,3	√ρGK-1 二			 γφ		(S208)
	[RKW,X,Θ (GK	)]8,3	√ρGK-1 		. Yφ		(S209)
21
Published as a conference paper at ICLR 2022
and the remaining entries of RKW,-X1,Θ(GK-1) are zero. Owing to the large degree of sparsity, the
matrix inverse in (S201) can be performed explicitly and yields relatively simple expressions that
depend on the entries of GK-1 and the matrices Σ and Σβ. For example, the (16, 4) entry of the
self-consistent equation reads,
K-1 G7,4	=	id 0 tr (ZZ - RW,X,Θ(GK ) - QK	-1	(S210)
	∖	/	16,4
= trh - √ρ ςP (Ino + φPγGKl GK2 £厂		1i	(S211)
n0→∞ -E”[ ⅛φpλ i			(S212)
	I0β1 0,1		
=	— ,—,		(S213)
	√ρ		
where to compute the asymptotic normalized trace we moved to an eigenbasis of Σ and recalled the
definition of the LJSD μ and the definition of Iβ in Eq. (12). The remaining entries of the (S201)
can be obtained in a similar manner and together yield the following set of coupled equations for the
entries of GK-1,
K-1 G1,1 K-1	γ -	-~~;	-~;	-~;	 -G2,2 (-Z + η + ρ) + pG2,2	- √pG6,3 - Y =	γΦ		(S214)
G2,2	ψGK-1 (η - ζ) - γφ (√pψGK-1 -1)	(S215)
K-1 G3,6	=Eμh -λpGK-1 GK-1 -γφ i	(S216)
K-1 G4,5	=Eμh -λpGK-P GK-1 -γφ i	(S217)
K-1 G5,4	「	Y√pφGK-1	] =Eμh - -λpGK,-GK,- - γφi	(S218)
K-1 G6,3	E h	Yλ√pΦGK-	i =叼 --λpGK-1GK-1-γφJ	(S219)
K-1 G7,4	=EG - √ (λpGK-1GK-1+ YΦ)]	(S220)
K-1 G7,6	=Eμh λpGK√¾ + YΦ i	(S221)
K-1 G8,3	=Eμh - √ (λpGK-Y GK-1+ γφ) i	(S222)
K-1 G8,5	=E h	qλ3/2GK-I	i =EkpGK-1GK-1+ 京	(S223)
K-1 G8,7	=Eμh√λi	(S224)
K-1	=	√pGK31		
G9,1	-GK2 1 (-Z + η + p) + PGK2 1 - √pGK3 1 - Y	(S225)
K-1 G3,4	= GKT = E h √λPGKIIGK2 1 i 5,6	μ ∣-λpGK-1GK-1-γφ]	(S226)
K-1 G3,5	GK-I= E h_	√λ√pGK-I	i =4,6 = "L λpGK-1GK-1 + γφ]	(S227)
22
Published as a conference paper at ICLR 2022
Y √λ√ρΦGK-1 i
λρGK- 1 GK,2 1 + γφl
(S228)
(S229)
(S230)
(S231)
(S232)
(S233)
where we have used the fact that, asymptotically, the normalized trace becomes equivalent to an ex-
Pectation over μ. After eliminating GK,- and GK5 from the first two equations, it is straightforward
to show that
-	- +N-1' _ 1 CKT _ 事(ψ		- φ)2 + 4xψφγ /ρ + ψ - φ		(S234)
τ1	r	γ i,i		2ψγ	
Ti	≡ EK-I) = 1GKJ = 1 + γ,	γ	ψ φ	α- 1)	(S235)
τ2	=tr(ɪ X >Σif £i/2XK- n0	i)	= τiIiβ,i	(S236)
where we have used the notation τι and τ2 from (AdIam & Pennington, 2020a;b), and Ti is the
comPanion transform of τ1, and where x satisfies the self-consistent equation,
ω + I1,1	ω + I1,1
(S237)
Here we utilized the two-index set of functionals of μ, Ia,b defined in Eq. (12).
Note that the product τ1T1 is simply related to x,
X = Yρτ1T1 ,
(S238)
so that, given x, the equations for the remaining entries of GK -1 completely decouple. In particular,
K -1 G3,6	√PGKΓ1 I0,i 				 γφ	(S239)
K -1 G4,5	√ρGKΓ1 Ii,i 二	 γφ	(S240)
K -1 G5,4	二	√ρGK2 I0,i	(S241)
K -1 G6,3	二	√pGK2 I1,1	(S242)
K -1 G7,4	=-匾 √ρ	(S243)
K -1 G7,6	β	K -1 I1,1G1,1	(S244)
	γφ	
K -1 G8,3	Iβ —I1,1 -		(S245)
23
Published as a conference paper at ICLR 2022
K-1 G8,5	I β ,1GK	-1	(S246)
	γφ		
K-1 G8,7	I 2,0 = 	 φ		(S247)
K-1 G9,1	√ρGK-1 GK- ―	 γ		(S248)
K-1 G3,4	K-1 G5,6	=	xI 2 ,i 二	 φ	(S249)
K-1 G3,5	K-1 G4,6	=	√pGK-1 11,1	(S250)
		γφ	
K-1 G4,3	K-1 G6,5	=	I12,1	(S251)
K-1 G5,3	K-1 G6,4	=	二 √pG2K2 I2,1	(S252)
K-1 G7,3	K-1 G8,4	=	Iβ 2,1 二	 √ρ	(S253)
K-1 G7,5	K-1 G8,6	=	β	K-1 i1,1 g1,1 γφ	(S254)
K-1 G7,7	K-1 G8,8	=	G9K,9-1 = 1	(S255)
K-1 G3,3	K-1 G4,4	=	K-1	K-1 G5,5	= G6,6	= I0,1 ,	(S256)
which will be important intermediate results for the subsequent sections.
Finally, we note that these results are sufficient to compute the training error. The expected training
loss can be written as,
Etrain =	=E Etr((Y - y(χ ))(y - y(χ ))>)	(S257) 2 =—Etr(Y >YK-2)	(S258) m =γ2 Etr (J(X >Σ^2∑β £1/2X + σ∣Im)K-2)	(S259) 二-Y (∂γT2 + σ2∂γτ1)	(S260) -γ2 (∂γ(τ1I1β,1) + σε2∂γτ1 .	(S261)
G.4.2 E21
The calculation of E21 proceeds exactly as in (Tripuraneni et al., 2021a;b) with the simple modification
of including an additional factor Σβ inside the final trace term, yielding
E21
(S262)
G.4.3 E31
The calculation of E31 proceeds exactly as in (Tripuraneni et al., 2021a;b) with no modifications
since there is no dependence on Σβ. The result is,
E31
-Pφ∂Y (σε (Q + φI1,2)(ω + I1,1) + ψγτ1I2,2
(S263)
24
Published as a conference paper at ICLR 2022
G.4.4 E32
Define the block matrix QE32 ≡ [QE32 QE32 ] by,
	/	I m	√η-ζθ>		√ρχτ	0	0	0	√η-ceτ(C-η)			0	∖	
				γ√n1	Y√n0				γ√n1					
	-	Θ √W-^ ʌ/nr		In1	0	0	-√pw √n1^	0	0			0		
		0		0	In0	-Σ1/2	0	0	0	∑1/2 (η-Z)				
		0		-W T √n1	0	InO	0	0	0			0		
		0		0	0	0	InO	-Σ1/2	0		n1 Σ ρ nO√ρ			
		X √n0		0	0	0	0	InO	0			0		
		0		0	0	0	0	0	In			0		
QE32 =		0		0	0	0	0	0	-sW∑ √n1		I	nO		,
		0		0	0	0	0	0	√η-<θτ			0		
									Y√n1					
		0		0	0	0	0	0	0			0		
		0		0	0	0	0	0	0			0		
		0		0	0	0	0	0	0			0		
		0		0	0	0	0	0	W T 	, √n 1			0		
		0		0	0	0	0	0	0			0		
		0		0	0	0	0	0	0			0		
	I	0		0	0	0	0	0	0			0	/	
													(S264)	
and,			(	0	0	0	0	00	0	0	∖			
				0	0	0	0	00	0	0				
				0	0	0	0	00	0	0				
				0	0	0	0	00	0	0				
				0	0	0	0	00	0	0				
				0	0	0	0	00	0	0				
				-e√η一ζ	-√ρw	0	0	00	0	0				
				√nΓ	√n1									
				0	0	0	0	00	0	0				
	qE3	2 =		Im	0	0	√ρχτ Y√n0	00	0	0		.	(S265)	
				0	InO	-Σ1/2	0	00	0	0				
				X √n0	0	InO	0	00	0	0				
				0	0	0	InO	-∑1/2	0	0	0				
				0	0	0	0	InO	√β	0	0				
				0	0	0	0	0	InO	-∑1/2	0				
				0	0	0	0	00	InO	-	X 	, √nO				
			∖	0	0	0	0	00	0	Im				
Then block matrix inversion (i.e. repeated applications of the Schur complement formula) shows that,
CE32	一 g8,8	-	E32 - G14,14	E32 —G15,15	E32 —G16,16	—1	(S266)
E32 G1,1	-	E32 -	G9,9 —	_ kK~1 G1,1			(S267)
E32 G2,2	-	E32 —G7,7 —	_ kK~1 G2,2			(S268)
E32 g13,8	-	_	kK~1 -	g3,3	-	-1			(S269)
E32 g3,3	-	E32 -g6,6 -	E32 -G11,11 —	E32 G12,12 —	E32 - G4,4 —	-CE32 - CE32 一 CE32 - Z7K-1∕on7∩∖ -G5,5 — G10,10 — G13,13 — G3,3 (S270)
E32 g3,4	-	E32 -G5,6 -	E32 -G10,11 —	E32 -G12,8 —	E32 G12,13 -	二 GK-I	(S271)
E32 g3,5	-	E32 -g4,6 -	E32 -G12,10 —	E32 -G13,11 -	_ kK-1 -G3,5	(S272)
E32 g3,6	-	E32 - G12,11	—kK~1 一G3,6			(S273)
E32 g4,3	-	E32 -G6,5 -	E32 -G11,10 —	E32 -G13,12 -	_ kK~1 - G4,3	(S274)
E32 g4,5	-	E32 -	G13,10	—kK~1 一G4,5			(S275)
E32 g5,3	-	E32 -G6,4 -	E32 -G10,12 —	E32 -G11,8 —	E32 G11,13 -	二 GK-	(S276)
E32 g5,4	-	E32 -	G10,8 -	E32 - G10,13 -	_ kK~1 - G5,4		(S277)
E32 g6,3	-	E32 G11,12	—kK~1 —G6,3			(S278)
E32 G14,12	-	E32 - G15,13	—kK~1 一G7,3			(S279)
25
Published as a conference paper at ICLR 2022
E32 G14,13	K-1 G7,4	(S280)
E32 G14,11	K-1 G7,6	(S281)
E32 G15,12	K-1 G8,3	(S282)
E32 G15,10	K-1 G8,5	(S283)
E32 G15,14	K-1 G8,7	(S284)
E32 G16,9	K-1 G9,1	(S285)
	K-1	
E32 G14,10	_	rVE32	—	9,1 - G15,11 = "ɪ	(S286)
E32 G16,1	二 φE32 , ψ	(S287)
where GE,32 denotes the normalized trace of the (i, j)-block of the inverse of (QE32)>. For brevity,
we have suppressed the expressions for the other non-zero blocks.
To compute the limiting values of these traces, we require the asymptotic block-wise traces of QE32
which may be determined from the operator-valued Stieltjes transform. To proceed, we first augment
QE32 to form the the self-adjoint matrix QE32,
QE32 =( QE32	[QE32]> ) .	(S288)
and observe that we can write QE32 as,
CE32 — z - CE32	— QE32
Q = Z - QW,X,Θ - QΣ
= ( 0	I16 ) -	0	[QEW3,2X,Θ]> ! -	0	[QEΣ32]> !	(S289)
I16	0	QEW3,2X,Θ	0	QΣE32	0	,
where QEW3,2X,θ ≡ [[QEW3,2X,θ]1 [QEW3,2X,θ]2] and,
	(	0	√η-ζθ>		√ρχ>
			γ√nι		γ√n0
		θ√η-ζ	0		0
		√n1			
		0	0		0
		0	W > 	.		0
			√n1		
		0	0		0
		- X	0		0
		√n0			
		0	0		0
[QE3X,θ ]1 =-		0	0		0
		0	0		0
		0	0		0
		0	0		0
		0	0		0
		0	0		0
		0	0		0
		0	0		0
\		0	0		0
	(	0	0	0	0
		0	0	0	0
		0	0	0	0
		0	0	0	0
		0	0	0	0
		0	0	0	0
		- Θ√η-ζ	- √ρw	0	0
		√n1	√n1		
[QW3X,θ ]2 =-		0	0	0	0
		0	0	0	√ρχ> Y√n0
		0	0	0	0
		- X	0	0	0
		√n0			
		0	0	0	0
		0	0	0	0
		0	0	0	0
		0	0	0	0
\		0	0	0	0
0	0		0	√η-ζθ>(Z-n) γ√nι		0	\	
0	√ρw 	.		0		0	0		
	n	1						
0	0		0		0	0		
0	0		0		0	0		
0	0		0		0	0		
0	0		0		0	0		
0	0		0		0	0		
0	0		0	-	W > √nΓ	0		(S290)
0	0		0	√η-ζ^θ>		0		
				γ√n1				
0	0		0		0	0		
0	0		0		0	0		
0	0		0		0	0		
0	0		0	-	W >	0		
					√n1			
0	0		0		0	0		
0	0		0		0	0		
0	0		0		0	0		
0	0	0	0	\				
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					(S291)
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	0					
0	0	0	- X					
0	0	0	√n0	, 0					
26
Published as a conference paper at ICLR 2022
	(	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	0 0	ʌ
		0	0	0	-Σ1∕2	0	0	0	∑1/2 (η -Z)	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
		0	0	0	0	0	-∑“	0	n1Σρ n0 √ρ	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
E Q∑32 =-		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	-Σ"	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	-Σ"	0	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	Σ √ √ρ	0	0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	-E1/2		0	
		0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	
	∖	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	/
															(S292)			
The operator-valued Stieltjes transforms satisfy,																		
			GG E 32		=GE32 (Z		_ De32 - RW,X,Θ		(GE32 ))									
					=id 0 tr (		'z - RWX,θ(GE32) - QE32)-					1 ,			(S293)			
where RWX g(GE32) is the operator-valued R-transform of QWiX ㊀.As discussed above, since
QW3X θ is a block matrix whose blocks are iid Gaussian matrices (and their transposes), an explicit
expression for RWX g(GE32) can be obtained from the covariance map η, which can be read off
from eqn. (S288). As above, we utilize the specific sparsity pattern for GE32 that is induced by
Eq. (S293), to obtain,
DE32 RW,X,Θ	'(GGE32) =( RW,X；GE32)		REW3,2X,Θ (GE32 )> 0,	(S294)
where,				
[REW3,2X,θ(GE32 )]1,1 =	G2E,322(ζ-η) 		 γ	√PG6E32 ɪ 	+ γ	G 鼻(Z-η)(Z-η) Y	(S295)
[REW3,2X,θ(GE32)]1,9=	G7E,322(ζ-η) 	- γ	√ρGE13,3 ɪ 	+ γ	GE (Z- η)(Z-η) Y	(S296)
[RWE3,2X,θ(GE32)]1,16=	√ρGE5,3 — γ			(S297)
[REW3,2X,θ(GE32)]2,2=	ΨgE12 (Z - η) γφ	+ √PΨGE52		(S298)
[REW3,2X,θ(GE32)]2,7=	ΨgE12 (Z-η) γφ	+ √PΨGE52	厂/ ^32qψGE12 (Z-η)(Z - η) + √pψgi3,5 + —:~~γφ		
				(S299)
[REW3,2X,θ(GE32)]4,5=	√ρGE22			(S300)
[RWE3,2X,θ(GE32)]4,10=	√PGE22			(S301)
[REW3,2X,θ(GE32)]6,3=	√pgEi2 — γφ			(S302)
[RWE3,2X,θ(GE32)]6,12 =	√pgEi2 			 γφ			(S303)
[REW3,2X,θ(GE32)]7,2=	ψGE92 (Z-η) γφ	+ √pψgEio		(S304)
[REW3,2X,θ(GE32)]7,7=	ψGE92 (Z-η) γφ	+ √ρψGEio	上厂心32 上ΨgE92(Z-η)(Z-η) + √pψgi3,210 + —：~~γφ		
				(S305)
27
Published as a conference paper at ICLR 2022
[REW3,2X,θ(GE32)]8,5	=√PgE,72		(S306)
[RWE3,2X,θ(GE32)]8,10	=√PGE72		(S307)
[REW3,2X,θ(GE32)]9,1	_ G 鼻(Z - η Y	√pgE12 		 Y	(S308)
[REW3,2X,θ(GE32)]9,9	_ G 号(Z- η Y	√ρGf13,212 		 Y	(S309)
[RWE3,2X,θ(GE32)]9,16	√ρGE5,12 二	 Y		(S310)
[RWE3,2X,θ(GE32)]11,3	√ρGE,92 二	 Yφ		(S311)
[REW3,2X,θ(GE32)]11,12	√ρGEE92 二			 Yφ		(S312)
[RWE3,2X,θ(GE32)]13,5	=√PGE3		(S313)
[REW3,2X,θ(GE32)]13,10	√ √g肾		(S314)
[RWE3,2X,θ(GE32)]15,3	√PG僚 二			 Yφ		(S315)
[REW3,2X,θ(GE32)]15,12	√PGE¾6 =	. Yφ		(S316)
and the remaining entries of REW3,2X,θ (GE32 ) are zero. As above, plugging these expressions into
eqn. (S293) and explicitly performing the block-matrix inverse yields the following set of coupled
equations,
E32 G7,2	=Y2√Pτ2ψG 嘉 + Y2 √ρτ2ΨG 挑 + *2ψGφ(Z -η) + Y 2τ1τ2ψ(Z-η)(Zf) (S317)
E32 G8,3	YTiI3 ιρ =I 2 ,iZ -I2 ,in -	(S318)
E32 G8,4	=- YτιIi,i (PTιψ (Z - 箱 + φP)	(S319) ΨΦ	()
E32 G8,5	_ I1,1 (PTiψ (Z - η) + φP) 二		 (S320) √ρψφ
E32 G8,6 E32 G9,1	√ρτι (ψI1 ,iZ — ψI1 ,in — YτιI3,iP) 二	\―2	r	2—L	(S321) ψφ =Yt2gE2 (Z - n) - Y√Pt2gE1,3 + Y2Ti2Ti(Z - η)(Z - η)	(S322)
E32 G10,3	F	y√ y y	Y√pt1φ (-ψI22 + ψI1,2η + YTiI2,2P) =√PΦGE3211,2 - YP3/2t2GEi2I3,2	-2_L	- (S323)
E32 G10,4	LoE32T	3/2-2°Ei2τ	Y2√pt211,2 (PTiψ (Z - η)+ φP)	八 二 √pφg7,22I0,2 - YP / TIG9,i2Ii,2	Y	ψ	 (S324)
E32 G10,5	cE32F	e cE32F	Ii,2 (YTi0P+ xψz - xψη) 二 -PTiG7,22Ii,2 - PTiGg,12Ii,2			ψ	 (S325)
E32 G10,6	A	A	Y2PTiT2I3 2P 二 -PTiGE2211,2 - PTiGEi2I2,2 + ——--^- + XI2,2 (η - Z)	(S326)
28
Published as a conference paper at ICLR 2022
E32 G11,3	=√ρφGE2211,2 - γρ3∕2T2GE212,2 - Y√pτ1φ (-ψl1,2ζ :γτ1l2,2P) (S327)
E32 G11,4	LQE32T	3/2-2gτ	Y2√ρτ2I2,2 (Pτ1ψ (Z - η) + φp)	Sc。、 =√ρφG7,22I2,2 - YP / τ1 G9,12I2,2	ψ	 (S328)
E32 G11,5	一 PTIGE2 I 2,2 — PT1GE2 I2,2 - I 2,2 KP「-Xw)	(S329)
E32 G12,4	=-ρτιGE,3I2,2 - PTiGEl2I2,2 + I3,2 (y2pΨ-2p + x2(Z- η))	(S330)
E32 G12,5	_ eGE12I1,2	P3∕2τ2GE22I3,2	I3,2 (yp2τ2τιψ (Z - η)+ xφP) -	Y —+	Φ	+	√PΨΦ	(S331)
E32 G12,6	√PG9E12I0,2 q P3/2T2GE22I1,2	YP2τ2τiI1,2 (Z - η) - xψ2ρ 二	1	7	1	F7I	 (S332) Y	φ	√pφ
E32 G13,3	口	口	Y2PTiT2I5 2P =-ptiGE,2I3,2 - PT1GE3I2,2 + ——ψj2~ + XI2,2 (η - Z)	(S333)
E32 G13,4	CE32T	-八E32τ	l T	(Y2Pτ1 τ2P l x2 (Z - ηY∖	/°C =-PTIG7,22I1,2 - Pτ1G9,12I1,2 + I2,2 I —ψ	 + 	φ	 1	(S334)
E32 G13,5	eGE12I1,2 , P3∕2τ2GE22I2,2	I2,2 (YP2T2τ1ψ (Z - η) + xφP)	八 二	1	+	1	L / 工	 (S335) Y	φ	√pψφ
E32 G13,6	x2I 5 ρ √pg9,12I2,2 , P / τ1G7,22I2,2	YP τlτ1I2,2 (Z - η)	ψ- 二	1	1	1	FT	 (S336) Y	φ	√pφ
E32 G13,8	=-xI11	(S337) φ
E32 G14,3	HF	x x	xψIβ 2 (Z - η) - Y2Pτ1τ1Iβ 2P =√PT1I β ,2 GE + √PτiI β ,2GEi2 + —初——√pψ	J	(S338)
E32 G14,4	=√Pτ1Iβ,2GE22 + √PT1Iβ,2GEl2 -小寸mgp + 1χ2' - ψχ2η)	(S339) Pψφ
E32 G14,5	Iβ GE32	”21β GE32	Iβ9 (仍2个“-C - χρ) =I1,2G9,1 - PTI 12,2g7,2 + 2,2 ∖	φ	ψ )	(S340) Y	φ	P x2I β5 ρ
E32 G14,6	Iβ 2GEl2	PT2Iβ 2GE22	YP2T2TiIβ 2 (η - Z) + 二 上—,	2,2-- +	2,2			ψ—	(S341) Y	φ	Pφ
E32 G14,8	XIiβ i =T4	(S342) √pφ
E32 G14,11	TiIβ 1 =—二一	(S343) φ
E32 G14,13	=-I0,1	(S344) √p
E32 G15,3	L	Te	4 l L-	τβ	4 l	xψIβ,2	(Z	-	η)	- Y2Pt1t113,2P	心心 =VPT1I2,2G7,22 + V/PT1I2,2G9,12 + 	√Pψ	 (S345)
E32 G15,4	Iβ 2 (Y2ptiT2φP + 板2《-ψχ2η) =√pT1Iβ 2G73 + √pT1Iβ 2GEl2 - -22	E	 (S346) 2 ,2	,	2 ,2 ,	ρpφ>φ
29
Published as a conference paper at ICLR 2022
E32 G15,5	Iβ,2GEl2	ρτ汽,2GE3	Iβ,2 (γρ2τfτ1ψ (Z -				η) + xφρ	(S347)
	— γ		φ	ρψφ	X 工3,2P ψ	
E32 G15,6	β	E32	2 β	E3 I1,2G9,1	ρτ1I2,2G7,2 二				 γφ			2	YP2τlτ1Iβ,2 (η — Z) + 一 +	:、	 ρφ		(S348)
E32 G15,8	XZe 1 -	2 ,1 √ρφ					(S349)
E32 G15,10	τ1Iβ I 2,1 Φ					(S350)
E32 G15,12	lβ,1 二	 √ρ					(S351)
E32 G15,14	I 2,0 =	 φ					(S352)
E32 G16,1	=71 工 1,1GE3(ζ - η) - √pτ2Iβ,ιGEι3,3 + Yτ2τιIβ,ι(Z — η)(ζ - η) - √ρτι					E32 G15,3 (S353)
E32 G16,9	τ1I1β,1					(S354)
E32 G1,1	E32 G9,9 =	γτ1				(S355)
E32 G2,2	E32 G7,7 =	二 γτι				(S356)
E32 G3,6	E32 G12,11	√ρτ1I0,1 =	 φ				(S357)
E32 G4,5	E32 G13,10	√ρτ1I1,1 =	 φ				(S358)
E32 G6,3	E32 G11,12	=γ√pτ1I	1,1			(S359)
E32 G11,6	E32 G12,3 =	-ρτ1G7E,322I1,2-		-「E32T	i Y2Pτ1τ2I2,2P ∣7∕	广、 -ρτ1G9,1 I1,2 +	ψ	+ xI1,2 (η - Z)		(S360)
E32 G14,10	E32 G15,11	F =	 φ				(S361)
E32 G14,12	E32 G15,13	Iβ 11 1 2 ,1 =	 √ρ				(S362)
E32 G5,4	E32 G10,8 =	E32 G10,13 =	γ√ρτ1I0,1			(S363)
E32 G3,5	E32 G4,6 =	E32 G12,10 =	E32 G13,11	=√ρτ1I 2,1		(S364)
				φ		
E32 G4,3	E32 G6,5 =	E32 G11,10 =	E32 G13,12	=11 1 2 ,1		(S365)
E32 G8,8	E32 G14,14	E32 = G15,15	E32 G16,16 = 1			(S366)
E32 G3,4	E32 G5,6 =	E32 G10,11 =	E32 G12,8	—GE32 = χI 1,1 G G12,13 =	φ		(S367)
E32 G5,3	E32 G6,4 =	E32 G10,12 =	E32 G11,8	二 GE3,213 = γ√ρτ111,1		(S368)
E32 G3,3	E32 G4,4 =	G5E,352 =	E32 6,6 =	E32	E32	E32 G10,10 = G11,11 = G12,12	E32 = G13,13 = I0,1 ,	(S369)
Here we have used the relations in eqns. (S266)-(S287), the definition ofIaβ,b, as well as the results in
Sec. G.4.1 to simplify the expressions. It is straightforward algebra to solve these equations for the
undetermined entries of GE32 and thereby obtain the following expression for E32,
(η - ζ)A32 + ρB32
(S370)
E32
D32
30
Published as a conference paper at ICLR 2022
where,
A32 = -ρ3τ1ψ2x4I1,1I2,2I2β,2 + ρ2τ1ψx3I2,2I2β,2 (ρφ + xψ(ζ - η))
-	ρ3τ1ψ2x3φI1,1I1,2I2β,2 + ρ2τ1ψ2x2I1,1I1β,1(η - ζ)
+ ρ2τ1ψ2x2I1,1I2β,2(ρ + x(ζ - η)) + ρ2τ1ψx2φI1,2I2β,2(ρφ+ xψ(ζ - η))
+ ρ3τ1ψ2x2φI1,1I1,2I1β,1 - ρ2τ1ψxφI1,2I1β,1(ρφ + xψ(ζ - η))
+ ρτ1ψxI1β,1(ζ - η)(ρφ + xψ(ζ - η))
-	ρτ1ψxI2β,2(ρ + x(ζ - η))(ρφ + xψ(ζ - η))	(S371)
B32 = -ρ2ψx6I22,2I3β,2 - 2ρ2ψx5φI22,2I2β,2 + 2ρψx4φI1,2I3β,2(η - ζ)
-	2ρ2ψx4φ2I1,2I2,2I2β,2 + ρ2ψx4φ2I12,2I3β,2 + ρ2ψx4φI22,2I1β,1
+ ρ2ψx4φI1,1I2,2I2β,2 + ρ2x4I2,2I3β,2(ψ + φ)
+ ρx3φI2,2I2β,2(ρ(ψ + φ) + 2xψ(ζ - η))
+ ρ2ψx3φ2I1,2I2,2I1β,1 + ρ2ψx3φ2I1,1I1,2I2β,2 + ρψx2φI1,1I1β,1(ζ - η)
-	ρx2φI2,2I1β,1(ρφ+ xψ(ζ - η)) - ρψx2φI1,1I2β,2(ρ + x(ζ - η))
-	pψχXφ工ι,ιh,2KI + Iβ,2 (x4ψ(ζ - η)2 - ρ2x2φ)	(S372)
D32 = -ρ3ψx4φI22,2 + 2ρ2ψx2φ2I1,2(η - ζ)
+ ρ3ψx2φ3I2,2 + ρ3x2φI2,2(ψ + φ) + ρφ (x2ψ(ζ - η)2 - ρ2φ) .	(S373)
Further simplifications are possible using the raising and lowering identities in eqn. (S9), as well as
the results in Sec. G.4.1, to obtain,
E32 = -φ Iβ,2 - ρφ∂γ (Iβ,1(ω + φI1,2)(ω + I1,I) +	TI 1,2I2,2 + γτ1I2,2(ω + φI1,2 )^ ,
(S374)
where
∂x	x
∂γ	Y + PY(TlΨ∕Φ + τι)(ω + φI1,2)
(S375)
G.4.5 E4
The calculation of E4 proceeds exactly as in (Tripuraneni et al., 2021a;b) with the simple modification
of including an additional factor Σβ inside the final trace term, yielding
2
E4 = -Ie2.	(S376)
φ 3,2
G.5 Final result for bias, variance, and test error
Putting the above pieces together, we have,
Bμ = ΦIβ,2	(S377)
Vμ = -ρφ∂γ (Ii 1(ω + φI1,2)(ω + I1,1) + BYTIIi2I2,2 + YTIZ2,2(ω + φI1,2)
+ σ2 ((ω + φI1,2)(ω + I1,1) + φγT1I2,2)) .	(S378)
(S379)
Some algebra shows that
Eμ = Bμ + Vμ
(S380)
31
Published as a conference paper at ICLR 2022
------------
τ2
Etrain	2
Y2τ2 - σ
- σε2
(S381)
(S382)
—
Corollary G.1. In the setting of Theorem 3.1, as the ridge regularization Constant Y → 0, Eμ
Bμ + Vμ with Bμ = φIβ,2 and Vμ given by
V γ→0 min(φ,ψ)(M 5 β J 卜与	。ifφ<ψ
μ	lφ - ψl ε ,	[ω+⅜2,2 (σ2 + Iβ,2) 0therwise '
(S383)
where x is the unique positive real root of x
min(1,φ∕ψ)
ω+11,1
32