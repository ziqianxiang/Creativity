Published as a conference paper at ICLR 2022
A Theoretical Analysis on Feature Learning in
Neural Networks: Emergence from Inputs and
Advantage over Fixed Features
Zhenmei Shi*,JunyiWei*,YingyuLiang
University of Wisconsin-Madison
zhmeishi@cs.wisc.edu,jwei53@wisc.edu,yliang@cs.wisc.edu
Ab stract
An important characteristic of neural networks is their ability to learn represen-
tations of the input data with effective features for prediction, which is believed
to be a key factor to their superior empirical performance. To better understand
the source and benefit of feature learning in neural networks, we consider learn-
ing problems motivated by practical data, where the labels are determined by a
set of class relevant patterns and the inputs are generated from these along with
some background patterns. We prove that neural networks trained by gradient
descent can succeed on these problems. The success relies on the emergence and
improvement of effective features, which are learned among exponentially many
candidates efficiently by exploiting the data (in particular, the structure of the
input distribution). In contrast, no linear models on data-independent features of
polynomial sizes can learn to as good errors. Furthermore, if the specific input
structure is removed, then no polynomial algorithm in the Statistical Query model
can learn even weakly. These results provide theoretical evidence showing that fea-
ture learning in neural networks depends strongly on the input structure and leads
to the superior performance. Our preliminary experimental results on synthetic and
real data also provide positive support.
1	Introduction
Various empirical studies have shown that an important characteristic of neural networks is their
feature learning ability, i.e., to learn a feature mapping for the inputs which allow accurate prediction
(e.g., Zeiler & Fergus (2014); Girshick et al. (2014); Zhang et al. (2019); Manning et al. (2020)). This
is widely believed to be a key factor to their remarkable success in many applications, in particular, an
advantage over traditional machine learning methods. To understand their success, it is then crucial
to understand the source and benefit of feature learning in neural networks. Empirical observations
show that networks can learn neurons that correspond to different semantic patterns in the inputs
(e.g., eyes, bird shapes, tires, etc. in images (Zeiler & Fergus, 2014; Girshick et al., 2014)). Moreover,
recent progress (e.g., Caron et al. (2018); Chen et al. (2020b); He et al. (2020); Jing & Tian (2020))
shows that one can even learn a feature mapping using only unlabeled inputs and then learn an
accurate predictor (usually a linear function) on it using labeled data. This further demonstrates
the feature learning ability of neural networks and that these input distributions contain important
information for learning useful features. These empirical observations strongly suggest that the
structure of the input distribution is crucial for feature learning and feature learning is crucial for
the strong performance. However, it is largely unclear how practical training methods (gradient
descent or its variants) learn important patterns from the inputs and whether this is necessary for
obtaining the superior performance, since the empirical studies do not exclude the possibility that
some other training methods can achieve similar performance without feature learning or with feature
learning that does not exploit the input structure. Rigorous theoretical investigations are thus needed
for answering these fundamental questions: How can effective features emerge from inputs in the
training dynamics of gradient descent? Is learning features from inputs necessary for the superior
performance?
*Equal contribution
1
Published as a conference paper at ICLR 2022
Compared to the abundant empirical evidence, the theoretical understanding still remains largely
open. One line of work (e.g. Jacot et al. (2018); Li & Liang (2018); Du et al. (2019); Allen-Zhu et al.
(2019); Zou et al. (2020); Chizat et al. (2019) and many others) shows in certain regime, sufficiently
overparameterized networks are approximately linear models, i.e., a linear function on the Neural
Tangent Kernel (NTK). This falls into the traditional approach of linear models on fixed features,
which also includes random features (Rahimi & Recht, 2008) and other kernel methods (Kamath et al.,
2020). The kernel viewpoint thus does not explain feature learning in networks nor the advantage over
fixed features. A recent line of work (e.g. Daniely & Malach (2020); Bai & Lee (2019); Ghorbani
et al. (2020); Yehudai & Shamir (2019); Allen-Zhu &Li (2019; 2020a); Li et al. (2020); Malach
et al. (2021) and others) shows examples where networks provably enjoy advantages over fixed
features, under different settings and assumptions. While providing insightful results separating
the two approaches, most studies have not investigated if the input structure is crucial for feature
learning and thus the advantage. Also, most studies have not analyzed how gradient descent can
learn important input patterns as effective features, or rely on strong assumptions like models or data
atypical in practice (e.g., special networks, Gaussian data, etc).
Towards a more thorough understanding, we propose to analyze learning problems motivated by
practical data, where the labels are determined by a set of class relevant patterns and the inputs are
generated from these along with some background patterns. We use comparison for our study: (1) by
comparing network learning approaches with fixed feature approaches on these problems, we analyze
the emergence of effective features and demonstrate feature learning leads to the advantage over fixed
features; (2) by comparing these problems to those with the input structure removed, we demonstrate
that the input structure is crucial for feature learning and prediction performance.
More precisely, we obtain the following results. We first prove that two-layer networks trained by
gradient descent can efficiently learn to small errors on these problems, and then prove that no linear
models on fixed features of polynomial sizes can learn to as good errors. These two results thus
establish the provable advantage of networks and implies that feature learning leads to this advantage.
More importantly, our analysis reveals the dynamics of feature learning: the network first learns
a rough approximation of the effective features, then improves them to get a set of good features,
and finally learns an accurate classifier on these features. Notably, the improvement of the effective
features in the second stage is needed for obtaining the provable advantage. The analysis also reveals
the emergence and improvement of the effective features are by exploiting the data, and in particular,
they rely on the input structure. To formalize this, we further prove the third result: if the specific
input structure is removed and replaced by a uniform distribution, then no polynomial algorithm can
even weakly learn in the Statistical Query (SQ) learning model, not to mention the advantage over
fixed features. Since SQ learning includes essentially all known algorithms (in particular, mini-batch
stochastic gradient descent used in practice), this implies that feature learning depends strongly on
the input structure. Finally, we perform simulations on synthetic data to verify our results. We also
perform experiments on real data and observe similar phenomena, which show that our analysis
provides useful insights for the practical network learning.
Our analysis then provides theoretical support for the following principle: feature learning in neural
networks depends strongly on the input structure and leads to the superior performance. In particular,
our results make it explicit that learning features from the input structure is crucial for the superior
performance. This suggests that input-distribution-free analysis (e.g., traditional PAC learning) may
not be able to explain the practical success, and advocates an emphasis of the input structure in
the analysis. While these results are for our proposed problem setting and network learning in
practice can be more complicated, the insights obtained match existing empirical observations and
are supported by our experiments. The compelling evidence hopefully can attract more attention to
further studies on modeling the input structure and analyzing feature learning.
2	Related Work
This section provides an overview while more technical discussions can be found in Appendix A.
Neural Tangent Kernel (NTK) and Linearization of Neural Networks. One line of work (e.g. Jacot
et al. (2018); Li & Liang (2018); Matthews et al. (2018); Lee et al. (2019); Novak et al. (2019);
Yang (2019); Du et al. (2019); Allen-Zhu et al. (2019); Zou et al. (2020); Ji & Telgarsky (2019); Cao
et al. (2020); Geiger et al. (2020); Chizat et al. (2019) and more) explains the success of sufficiently
2
Published as a conference paper at ICLR 2022
over-parameterized neural network by connecting them to linear methods like NTK. Though their
approaches are different, they all base on the observation that when the network is sufficiently large,
the weights stay close to the initialization during the training, and training is similar to solving a kernel
method problem. This is typically referred to as the NTK regime, or lazy training, or linearization.
However, networks used in practice are usually not large enough to enter this regime, and the weights
are frequently observed to traverse away from the initialization. Furthermore, in this regime, network
learning is essentially the traditional approach of linear methods over fixed features, which cannot
establish or explain feature learning and the advantage of network learning.
Advantage of Neural Networks over Linear Models on Fixed Features. Since the superior
network learning results via gradient descent are not well explained by the NTK view, a recent line
of work has turned to learning settings where neural networks provably have advantage over linear
models on fixed features (e.g. Daniely & Malach (2020); Refinetti et al. (2021); Malach et al. (2021);
Dou & Liang (2020); Bai &Lee (2019); Ghorbani et al. (2020); Allen-Zhu & Li (2019); see the
great summary in Malach et al. (2021)). While formally establishing the advantage, they have not
thoroughly answered the two fundamental questions this work focuses on; in particular, most existing
work has not studied whether the input structure is a crucial factor for feature learning and thus the
advantage, and/or has not considered how the features are learned in more practical training scenarios.
For example, Ghorbani et al. (2020) show the advantage of networks in approximation power and
Dou & Liang (2020) show their statistical advantage, but they do not consider the learning dynamics
(i.e., how the training method obtains the good network). Allen-Zhu & Li (2019) prove the advantage
of the networks for PAC learning with labels given by a depth-2 ResNet and Allen-Zhu & Li (2020a)
prove for Gaussian inputs with labels given by a multiple-layer network, while neither considers
the influence of the input structure on feature learning or the advantage. Daniely & Malach (2020)
prove the advantage of the networks for learning sparse parities on specific input distributions that
help gradient descent learn effective features for prediction, and Malach et al. (2021) consider similar
learning problems but with specifically designed differentiable models, while our work analyzes data
distributions and models closer to those in practice and also explicitly focuses on whether the input
structure is needed for the learning. There are also other theoretical studies on feature learning in
networks (e.g. Yehudai & Ohad (2020); Zhou et al. (2021); Diakonikolas et al. (2020); Frei et al.
(2020)), which however do not directly relate feature learning to the input structure or the advantage
of network learning.
3	Problem Setup
To motivate our setup, consider images with various kinds of patterns like lines and rectangles. Some
patterns are relevant for the labels (e.g., rectangles for distinguishing indoor or outdoor images),
while the others are not. If the image contains a sufficient number of the former, then we are confident
that the image belongs to a certain class. Dictionary learning or sparse coding is a classic model
of such data (e.g., Olshausen & Field (1997); Vinje & Gallant (2000); Blei et al. (2003)). We thus
model the patterns as a dictionary, generate a hidden vector indicating the presence of the patterns,
and generate the input and label from this vector.
Let X = Rd be the input space, and Y = {±1} be the label space. Suppose M ∈ Rd×D is an
unknown dictionary with D columns that can be regarded as patterns. For simplicity, assume M is
orthonormal. Let φ ∈ {0,1}D be a hidden vector that indicates the presence of each pattern. Let
A ⊆ [D] be a subset of size k corresponding to the class relevant patterns. Then the input is generated
by Mφ, and the label can be any binary function on the number of class relevant patterns. More
precisely, let P ⊆ [k]. Given A and P, We first sample φ from a distribution D$, and then generate
the input X and the class label y from φ:
工 6	~	”工	+ +1, if Σi∈A φi ∈ P,	小
φ 〜D7,	X = Mφ, y = < _	, i∈A	(1)
φ ,	,	-1,	otherWise.
Learning with Input Structure. We alloW quite general Dφ7 With the folloWing assumptions:
(A0) The class probabilities are balanced: Pr[Pi∈A φi ∈ P] = 1/2.
(A1) The patterns in A are correlated With the labels With the same correlation: for any i ∈ A,
Y = E[yφi] - E[y]E[φi] > 0.
3
Published as a conference paper at ICLR 2022
(A2) Each pattern outside A is identically distributed and independent of all other patterns. Let
po := Pr[φi = 1] and without loss of generality assume po ≤ 1/2.
Let D(A, P, Dφ) denote the distribution on (X, y) for some A, P, and Dg Given parameters Ξ =
(d, D, k, γ, po), the family Fξ of distributions include all D(A, P, D$) with A ⊆ [D], P ⊆ [k], and
Dφ satisfying the above assumptions. The labeling function includes some interesting special cases:
Example 1. Suppose P = {i ∈ [k] : i > k/2} for some threshold, i.e., we will set the label y = +1
when more than a half of the relevant patterns are presented in the input.
Example 2. Suppose k is odd, and let P = {i ∈ [k] : i is odd}, i.e., the labels are given by the parity
function on φj(j ∈ A). This is useful to prove our lower bounds via the properties of parities.
Appendix F presents results for more general settings (e.g., incoherent dictionary, unbalanced classes,
etc.). On the other hand, our problem setup does not include some important data models. In
particular, one would like to model hierarchical representations often observed in practical data and
believed to be important for deep learning. We leave such more general cases for future work.
Learning Without Input Structure. For comparison, we also consider learning problems without
input structure. The data are generated as above but with different distributions D$:
(A1’) The patterns are uniform over {0, 1}D: for any i ∈ [D], Pr[φi = 1] = 1/2 independently.
Given parameters Ξ0 = (d, D, k), the family FΞ0 of distributions without input structure is the set of
all the distributions with A ⊆ [D], P ⊆ [k] and D$ satisfying the above assumptions.
3.1 Neural Network Learning
Networks. We consider training a two-layer network via gradient descent on the data distribution:
2m
g(x) =	aiσ(hwi, xi + bi)	(2)
i=1
where wi ∈ Rd, bi, ai ∈ R, and σ(z) = min(1, max(z, 0)) is the truncated rectified linear unit
(ReLU) activation function. Let θ = {wi, bi, ai}i2=m1 denote all the parameters, and let superscript (t)
denote the time step, e.g., g(t) denote the network at time step t with θ(t) = {wi(t), bi(t), ai(t)}.
Loss Function. Similar to typical practice, we will normalize the data for learning: first compute
X =(X - E[X])∕σ where σ2 = E Pid=1(χi - E[Xi])2 is the variance of the data, and then train on
(x, y). This is equivalent to setting φ = (φ - E[φ])∕σ and generating X = Mφ. For (x, y) from D
and the normalized (x, y), we will simply say (x, y)〜D.
For the training, we consider the hinge-loss '(y, y) = max{1 - yy, 0}. We will inject some noise ξ
to the neurons for the convenience of the analysis. (This can be viewed as using a smoothed version
of the activation σ(z) = Eξσ(z + ξ) similar to those in existing studies like Allen-Zhu & Li (2020b);
Malach et al. (2021). See Section 5 for more explanations.) Formally, the loss is:
2m
LD(g; σξ) = E(χ,y) ['(y,g(x; ξ))], where g(x; ξ) = EaiEξ[σ(hwi,xi + b + ξi)]	(3)
i=1
where ξ 〜N(0, σξIm×m) are independent Gaussian noise. Let LD(g) denote the typical hinge-loss
without noise. We also consider '2 regularization: R(g; λΟ,λw) = P2ml λa∣αi∣2 + λw ∣∣wik2 with
regularization coefficients λa , λw .
Training Process. We first perform an unbiased initialization: for every i ∈ [m], initialize w(0)〜
N(0,σWId×d) with σw = 1/k, bi0) 〜N (0,σ2) with σb = 1/k2, a(0)〜N(0")with Qa =
σ2∕(γk2), and then set w，+i = w(0), b,[i = bi0), a,[i = -a(0). We then do gradient updates:
θ㈤=θ(I) - η⑴ Vθ (LD (g(tτ); σ(t)) + R(g(T); λat),λW)) , for t = 1,2,...,T,	(4)
for some choice of the hyperparameters η(t), λ(at), λ(wt), σξ(t), and T.
4
Published as a conference paper at ICLR 2022
4 Main Results
Provable Guarantee for Neural Networks. The network learning has the following guarantee:
Theorem 1.	For any δ, e ∈ (0,1), if k = Ω (log2 (D∕(δγ))), P0 = Ω(k2∕D), and
max{Ω(k12∕e3/2), D} ≤ m ≤ poly(D), then with properly set hyperparameters, for any D ∈ Fξ,
with probability at least 1 - δ, there exists t ∈ [T] such that Pr[sign(g(t) (x)) 6= y] ≤ LD (g(t)) ≤ .
The theorem shows that for a wide range of the background pattern probability po and the number of
class relevant patterns k, the network trained by gradient descent can obtain a small classification
error. More importantly, the analysis shows the success comes from feature learning. In the early
stages, the network learns and improves the neuron weights such that on the features (i.e., the neurons’
outputs) there is an accurate classifier; afterwards it learns such a classifier. The next section will
provide a detailed discussion on the feature learning.
Lower Bound for Fixed Features. Empirical observations and Theorem 1 do not exclude the
possibility that some methods without feature learning can achieve similar performance. We thus
prove a lower bound for the fixed feature approach, i.e., linear models on data-independent features.
Theorem 2.	Suppose Ψ is a data-independent feature mapping of dimension N with bounded features,
i.e., Ψ : X → [-1, 1]N. For B > 0, the family of linear models on Ψ with bounded norm B is
HB = {h(X) : h(X) = (Ψ(X), w)，kw∣∣2 ≤ B}. If 3 < k ≤ D/16 and k is odd, then there exists
D ∈Fξ such that all h ∈ Hb have hinge-loss at least P0 (1 一 vλ2kB ).
So using fixed features independent of the data cannot get loss nontrivially smaller than po unless
with exponentially large models. In contrast, viewing the neurons σ(hwi, xi + bi) as learned features,
network learning can achieve any loss ∈ (0, 1) with models of polynomial sizes. We emphasize
the lower bound is because the feature map Ψ is independent of the data. Indeed, there exists a
small linear model on a small dimensional feature map allowing 0 loss for each data distribution
in our problem set Fξ (Lemma 5). However, this feature map Ψ* is different for different data
distribution in FΞ, i.e., depends on the data. On the other hand, the feature map Ψ in the lower
bound is data-independent, i.e., fixed before seeing the data. For Ψ to work simultaneously for all
distributions in FΞ, it needs to have exponential dimensions. Intuitively, it needs a large number
of features, so that there are some features to approximate each Ψ*. There are exponentially many
data distributions in Fξ, and thus exponentially many data-dependent features Ψ*, which requires
Ψ to have an exponentially large dimension. Network learning updates the hidden neurons using
the data and can learn to move the features to the right positions to approximate the ground-truth
data-dependent features Ψ*, so it does not need an exponentially large dimension feature map.
The theorem directly applies to linear models on fixed finite-dimensional feature maps, e.g., linear
models on the input or random feature approaches (Rahimi & Recht, 2008). It also implies lower
bounds to infinite dimensional feature maps (e.g., some kernels) that can be approximated by feature
maps of polynomial dimensions. For example, Claim 1 in Rahimi & Recht (2008) implies that a
function f using shift-invariant kernels (e.g., RBF) can be approximated by a model (Ψ(X), Wi with
the dimension N and weight norm B bounded by polynomials of the related parameters of f like its
RKHS norm and the input dimension. Then our theorem implies some related parameter of f needs
to be exponential in k for f to get nontrivial loss, formalized in Corollary 3. Kamath et al. (2020) has
more discussions on approximating kernels with finite dimensional maps.
Corollary 3. For any function f using a shift-invariant kernel K with RKHS norm bounded by L, or
f (x) = ∑i αiK (Zi, x) for some data points Zi and ∣∣ɑ∣∣2 ≤ L. If 3 < k ≤ D/16 and k is odd, then
there exists D ∈ FΞ such that f has hinge-loss at least Po 1 一
poly(d,L)
2k
1
poly(d,L).
—
Lower Bound for Without Input Structure. Existing results do not exclude the possibility that
some learning methods without exploiting the input structure can achieve strong performance. To
show the necessity of the input structure, we consider learning FΞ0 with input structure removed. We
obtain a lower bound for such learning problems in the classic Statistical Query (SQ) model (Kearns,
1998). In this model, the algorithm can only receive information about the data through statistical
queries. A statistical query is specified by some polynomially-computable property predicate Q of
labeled instances and a tolerance parameter τ ∈ [0, 1]. For a query (Q, τ), the algorithm receives
5
Published as a conference paper at ICLR 2022
a response PQ ∈ [Pq - t,Pq + T], where PQ = Pr[Q(x, y) is true]. Notice that a query can be
simulated using the average of roughly O(1∕τ2) random data samples with high probability. The SQ
model captures almost all common learning algorithms (except Gaussian elimination) including the
commonly used mini-batch SGD, and thus is suitable for our purpose.
Theorem 4. For any algorithm in the Statistical Query model that can learn over FΞ0 to classification
D 1/3
error less than 1 一岛3,either the number ofqueries or 1 /τ must be at least 1 (k)	.
(k)
The theorem shows that without the input structure, polynomial algorithms in the SQ model cannot
get a classification error nontrivially smaller than random guessing. The comparison to the result for
with input structure then shows that the input structure is crucial for network learning, in particular,
for achieving the advantage over fixed feature models.
5 Proof S ketches
Here we provide the sketch of our analysis, focusing on the key intuition and discussing some
interesting implications. The complete proofs are included in Appendix B-D.
5.1	Provable Guarantees of Neural Networks
Overall Intuition. We first show that there is a two-layer network that can represent the target
labeling function, whose neurons can be viewed as the “ground-truth” features to be learned. We
then show that after the first gradient step, the hidden neurons of the trained network become close
to the ground-truth: their weights contain large components along the class relevant patterns but
small along the background patterns. We further show that in the second gradient step, these features
get improved: the “signal-noise” ratio between the components for class relevant patterns and those
for the background ones becomes larger, giving a set of good features. Finally, we show that the
remaining steps learn an accurate classifier on these features.
Existence of A Good Network. We show that there is a two-layer network that can fit the labels.
Lemma5. For any D ∈ Fξ , there exists a network g*(x) = En=I α*σ(hw*,xi + b*) with y = g"x)
for any (x,y)〜D. Furthermore, the number of neurons n = 3(k + 1), |a*| ≤ 32k, 1∕(32k) ≤
|b*| ≤ 1/2, Wi = σ Pj∈A Mj/(4k), and |〈w；, Xi + b*| ≤ 1 for any i ∈ [n] and (x, y)〜D.
In particular, the weights of the neurons are proportional to j∈A Mj, the sum of the class relevant
patterns. We thus focus on analyzing how the network learns such neuron weights.
Feature Emergence in the First Gradient Step. The gradient for wi (ignoring the noise) is:
dLD⑻
∂Wi
-aiE(x,y)〜D {yI[yg(x) ≤ 1]σ0[hwi,xi + bi]x}= 一电旧3#)〜D {yxσ0[hwi,xi + bi]}
where the last step is due to g(x) = 0 by the unbiased initialization. Let qj = hMj, wii denote the
component along the direction of the pattern Mj . Then the component of the gradient on Mj is:
-aiE {yφj σ0[hwi, xi + bi]} = -aiE yφjσ0
fφeqe + bi .
∕∈[D]	J
The key intuition is that with the randomness of φ' (and potentially that of the injected noise ξ), the
random variable under σ0 is not significantly affected by a small subset of φg'. For example, for
class relevant patterns j ∈ A, let I[D]
We have I[D] ≈ I-A and thus:
σ0 [P'∈[D] Φ'q' + bi] and I-A ：= σ0 [P'∈A φg' + bj .
H E {yφjI[D]} ≈ E {yφjI-a} = E {yφj} e[I-a] = σe[I-a]
since y only depends on φj(j ∈ A). Then the gradient has a nontrivial component along the pattern.
Similarly, for background patterns j 6∈ A, the component of the gradient along Mj is close to 0.
6
Published as a conference paper at ICLR 2022
Lemma 6 (Informal). Assume po, k as in Theorem 1 and σξ(1) < 1/k, then with high probability
∂WLD(g(0); σξ(1)) = -a(0) PD=1 MjTjwherefora small 品：
•	if j ∈ A, then |Tj — βγ∕σ∣ ≤ O&/3) with β ∈ [Ω(1), 1];
•	if j ∈ A, then |Tj| ≤ O(*金σ).
By setting λW1) = 1∕(2η⑴)，we have W(I) = η(1)ai0) P% MjTj ≈ η(1)a(0) βσγ Pj∙∈A Mj. For
small po, e.g., Po = O(k2∕D), these neurons can already allow accurate prediction. However,
for such small po, we cannot show a provable advantage of networks over fixed features. On the
other hand, for larger po meaning a significant number of background patterns in the input, the
approximation error terms Tj(j 6∈ A) together can overwhelm the signals Tj(j ∈ A) and lead to bad
prediction, even though each term is small. Fortunately, we will show that the second gradient step
can improve the weights by decreasing the ratio between Tj (j 6∈ A) and Tj (j ∈ A).
Feature Improvement in the Second Gradient Step. We note that by setting a small η(1), after the
update we still have yg(χ; ξ) < 1 for most (x, y) ~ D and thus the gradient in the second step is:
∂0
L-LD(g； σξ) ≈ -a∙iE(χ,y)~D {yxEξσ [(Wi, Xi + b + ξi]}.
∂wi
We can then follow the intuition for the first step again. For j ∈ A, the component (Mj-, ∂WLD(g)i
is roughly proportional to σE[I-A,ξ] where I-A,ξ := σ0 [P'∈a φgqg + bi + ξj. While φgqg may
not have large enough variance, the injected noise ξi makes sure that a nontrivial amount of data
activate the neuron.1 Then I-A,ξ 6= 0, leading to a nontrivial component along Mj, similar to
the first step. On the other hand, for j 6∈ A, the approximation error term Tj depends on how
well σ0 [P'?A,'=j∙ φ'q' + bi + ξj approximates σ0 [P'∈[p] φgqg + b + ξj . Since the q's (the
weight,s component along m`) in the second step are small compared to those in the first step, we
can then get a small error term Tj. So the ratio between Tj(j 6∈ A) over Tj (j ∈ A) improves after
the second step, giving better features allowing accurate prediction.
Classifier Learning Stage. Given the learned features, we are then ready to show the remaining
gradient steps can learn accurate classifiers. Intuitively, with small hyperparameter values (η(t) =
Tm1/3, λɑt = λWt ≤ --mk3j3,σξ(t) = 0 for 2 < t ≤ T = m4/3), the first layer,s weights do not
change too much and thus the learning is similar to convex learning using the learned features.
Formally, our proof uses the online convex optimization technique in Daniely & Malach (2020).
5.2 Lower Bounds
The lower bounds are based on the following observation: our problem setup is general enough to
include learning sparse parity functions. Consider an odd k, and let P = {i ∈ [k] : i is odd}. Then y
is given by ΠA(z) := j∈A zj for zj = 2φj — 1, i.e., the parity function on zj(j ∈ A). Then known
results for learning parity functions can be applied to prove our lower bounds.
Lower Bound for Fixed Features. We show that FΞ contains learning problems that consist of a
mixture of two distributions with weights po and 1 — po respectively, where in the first distribution
D(1), X is given by the uniform distribution over φ and the label y is given by the parity function on A.
On such DA(1), Daniely & Malach (2020) shows that exponentially large models over fixed features is
needed to get nontrivial loss. Intuitively, there are exponentially many labeling functions ΠA that are
uncorrelated (i.e., “orthogonal” to each other): E[ΠA1 ΠA2] = 0 for any A1 and A2. Note that the
best approximation of ΠA by a fixed set of features Ψi,s is its projection on the linear span of the
features. Then with polynomial-size models, there always exists some ΠA far from the linear span.
1Equivalently, the network uses σ(z) = Eξσ(z + ξ), a Gaussian smoothed version of σ, and the smoothing
allows z slightly outside the activated region of σ to generate gradient for the learning. Empirically it is not
needed since typically sufficient data can activate the neurons. One potential reason is that the data have their
own noise to achieve a similar effect (a remote analog being noisy gradients can help the optimization). Further
analysis on such an effect is left for future work.
7
Published as a conference paper at ICLR 2022
Remark. It is instructive to compare to network learning, which finds the effective weights j∈A Mj
among the exponentially many candidates corresponding to different A’s. This can be done efficiently
by exploiting the data since the gradient is roughly proportional to E {yx} = Pj∈A Mj. The network
then learns data-dependent features on which polynomial size linear models can achieve small loss.
Lower Bound for Learning without Input Structure. Clearly, FΞ0 contains the distributions DA(1)
described above. The lower bound then follows from classic SQ learning results (Blum et al., 1994).
Remark. The SQ lower bound analysis does not apply to FΞ , because in FΞ the input distribution
is related the labeling function. This allows networks to learn with polynomial time/sample. While
both the labeling function and the input distribution affect the learning, few existing studies explicitly
point out the importance of the input structure. We thus emphasize the input structure is crucial for
networks to learn effective features and achieve superior performance.
6 Experiments
Our experiments mainly focus on feature learning and the effect of the input structure. We first
perform simulations on our learning problems to (1) verify our main theorems on the benefit of feature
learning and the effect of input structure; (2) verify our analysis of feature learning in networks. We
then check if our insights carry over to real data: (3) whether similar feature learning is presented in
real network/data; (4) whether damaging the input structure lowers the performance. The results are
consistent with our analysis and provide positive support for the theory. Below we present part of the
results and include the complete experimental details and results in Appendix E.
Simulation: Verification of the Main Results. We
generate data according to our problem setup, with
d = 500, D = 100, k = 5,po = 1/2, a randomly
sampled A, and labels given by the parity function.
We then train a two-layer network with m = 300
following our learning process, and for comparison,
we also use two fixed feature methods (the NTK and
random feature methods based on the same network).
Finally, we also use these three methods on the data
distribution with the input structure removed (i.e.,
FΞ0 in Theorem 4).
Figure 1 shows that the results are consistent with
our results. Network learning gets high test accuracy
while the two fixed feature methods get significantly
lower accuracy. Furthermore, when the input struc-
Figure 1: Test accuracy on simulated data
with or without input structure.
ture is removed, all three methods get test accuracy similar to random guessing.
Simulation: Feature Learning in Networks. We compute the cosine similarities between the
weights wi ’s and visualize them by Multidimensional Scaling. (Recall that our analysis is on the
directions of the weights without considering their scaling, and thus it is important to choose cosine
similarity rather than say the typical Euclidean distance.) Figure 2 shows that the results are as
predicted by our analysis. After the first gradient step, some weights begin to cluster around the
ground-truth Pj∈A Mj (or - Pj∈A Mj due to the ai in the gradient update which can be positive
or negative). After the second step, the weights get improved and well-aligned with the ground-truth
(with cosine similarities > 0.99). Furthermore, if a classifier is trained on the features after the first
step, the test accuracy is about 52%; if the same is done after the second step, the test accuracy is
about 100%. This demonstrates while some effective features emerge in the first step, they need to be
improved in the second step to get accurate prediction.
Real Data: Feature Learning in Networks. We perform experiments on MNIST (LeCun et al.,
1998; Deng, 2012), CIFAR10 (Krizhevsky, 2012), and SVHN (Netzer et al., 2011). On MNIST,
we train a two-layer network with m = 50 on the subset with labels 0/1 and visualize the neurons’
weights as in the simulation. Figure 3 shows a similar feature learning phenomenon: effective features
emerge after a few steps and then get improved to form two clusters. Similar results are observed on
other datasets. These suggest the insights obtained in our analysis are also applicable to the real data.
8
Published as a conference paper at ICLR 2022
Figure 2: Visualization of the weights wi’s after initialization/one gradient step/two steps in network
learning on the synthetic data. The red star denotes the ground-truth j∈A Mj ; the orange star is
- j∈A Mj . The red/orange dots are the weights closest to the red/orange star, respectively.
Figure 3: Visualization of the neurons’ weights in a two-layer network trained on the subset of
MNIST data with label 0/1. The weights gradually form two clusters.
Figure 4: Test accuracy at different steps for an equal mixture of Gaussian inputs with data: (a)
MNIST, (b) CIFAR10, (c) SVHN.
Real Data: The Effect of Input Structure. Since we cannot directly manipulate the input distribution
of real data, we perform controlled experiments by injecting different inputs. For labeled dataset
L and injected input U , we first train a teacher network fitting L, then use the teacher network to
give labels on a mixture of inputs from L and U , and finally train a student network on this new
dataset M consisting of the mixed inputs and the teacher network’s labels. Checking the student’
performance on different parts of M and comparing to those by directly training the student on the
original data L can reveal the impact of changing the input structure. We use MNIST, CIFAR10, or
SVHN as L, and use Gaussian or images in Tiny ImageNet (Le & Yang, 2015) as U . The networks
for MNIST are two-layer with m = 9, and those for CIFAR10/SVHN are ResNet-18 convolutional
neural networks (He et al., 2016).
Figure 4 shows the results on an equal mixture of data and Gaussian. It presents the test accuracy
of the student on the original data part, the Gaussian part, and the whole mixture. For example, on
CIFAR10, the network learns well over the CIFAR10 part (with accuracy similar to directly training
on the original data) but learns slower with worse accuracy on the Gaussian part. Furthermore, the
accuracy on the whole mixture is lower than that of training on the original CIFAR10. This shows
that the input structure indeed has a significant impact on the learning. While MNIST+Gaussian
shows a less significant trend (possibly because the tasks are simpler), the other datasets show similar
significant trends as CIFAR10+Gaussian (the results using Tiny ImageNet are in the appendix).
9
Published as a conference paper at ICLR 2022
7	Ethics Statement
Our paper is mostly theoretical in nature and thus we foresee no immediate negative ethical impact.
We are of the opinion that our theoretical framework may lead to better understanding and inspire
development of improved network learning methods, which may have a positive impact in practice. In
addition to the theoretical machine learning community, we perceive that our conceptual message that
the input structure is crucial for the network learning’s performance can be beneficial to engineering-
inclined machine learning researchers.
8	Reproducibility Statement
For theoretical results in the Section 4, a complete proof is provided in the Appendix B-D. The
theoretical results and complete proofs for a setting more general than that in the main text are
provided in the Appendix F. For experiments in the Section 6, complete details and experimental
results are provided in the Appendix Section E. The source code with explanations and comments is
provided in the supplementary material.
9	Acknowledgement
The work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Founda-
tion (NSF) Grants 2008559-IIS and CCF-2046710.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? In
Advances in Neural Information Processing Systems, 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020a.
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020b.
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020c.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, 2019.
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings ofthe National Academy ofSciences,
116(32):15849-15854, 2019.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993-1022, 2003.
Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.
Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Pro-
ceedings of the twenty-sixth annual ACM symposium on Theory of computing, pp. 253-262,
1994.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning, 2020.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper-
vised learning of visual features. In European Conference on Computer Vision, 2018.
10
Published as a conference paper at ICLR 2022
Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher.
Towards understanding hierarchical learning: Benefits of neural representations. arXiv preprint
arXiv:2006.13436, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020b.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, 2019.
Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural
Information Processing Systems, 2020.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141-142, 2012.
Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi.
Approximation schemes for relu regression. In Conference on Learning Theory, 2020.
Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels: Prov-
able representation and approximation benefits. Journal of the American Statistical Association,
2020.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, 2019.
Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn
near optimal feature representations, 2019.
Cong Fang, Hanze Dong, and Tong Zhang. Mathematical models of overparameterized neural
networks. Proceedings of the IEEE, 109(5):683-703, 2021.
Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient
descent. In Advances in Neural Information Processing Systems, 2020.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020
(11):113301, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Advances in Neural Information Processing Systems,
2020.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Computer Vision and Pattern Recognition, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations, 2019.
Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic
variants of dimensional and margin complexity. In Conference on Learning Theory, 2020.
11
Published as a conference paper at ICLR 2022
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 1998.
Frederic Koehler and Andrej Risteski. The comparative power of relu networks and polynomial
kernels in the presence of sparse latent structure. In International Conference on Learning
Representations, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2012.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 2019.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, 2018.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. Advances in Neural Information Processing Systems,
2019.
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, 2020.
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using
differentiable learning over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.
Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent
linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the
National Academy of Sciences, 117(48):30046-30054, 2020.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
Representations, 2018.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Bayesian convolutional neural networks with many channels are
gaussian processes. In International Conference on Learning Representations, 2019.
B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A strategy employed by
v1? Vision Research, 37:3311-3325, 1997.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, 2008.
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborovd. Classifying high-
dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed, 2021.
William E Vinje and Jack L Gallant. Sparse coding and decorrelation in primary visual cortex during
natural vision. Science, 287(5456):1273-1276, 2000.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
Conference on Learning Theory, 2020.
12
Published as a conference paper at ICLR 2022
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Gilad Yehudai and Shamir Ohad. Learning a single neuron with gradient methods. In Conference on
Learning Theory, 2020.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. Advances in Neural Information Processing Systems, 2019.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European Conference on Computer Vision, 2014.
Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint
arXiv:1902.01996, 2019.
Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer
neural network. In Conference on Learning Theory, 2021.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning,109(3):467-492, 2020. ISSN 1573-0565.
13
Published as a conference paper at ICLR 2022
Appendix
Section A presents more technical discussion on related work. Section B-D provides the complete
proofs for our results in the main text. Section E provides the complete details and experimental
results for our experiments.
Finally, Section F provides the theoretical results and complete proofs for a setting more general than
that in the main text, allowing incoherent dictionaries, unbalanced classes, and Gaussian noise in the
data.
Contents
1	Introduction	1
2	Related Work	2
3	Problem Setup	3
3.1	Neural Network Learning ................................................. 4
4	Main Results	5
5	Proof Sketches	6
5.1	Provable Guarantees of Neural Networks .................................. 6
5.2	Lower Bounds ............................................................ 7
6	Experiments	8
7	Ethics Statement	10
8	Reproducibility Statement	10
9	Acknowledgement	10
A	More Technical Discussion on Related Work	16
B	Complete Proofs for Provable Guarantees of Neural Networks	19
B.1	Existence of A Good Network ............................................ 19
B.2	Initialization ......................................................... 20
B.3	Some Auxiliary Lemmas .................................................. 21
B.4	Feature Emergence: First Gradient Step ................................. 23
B.5	Feature Improvement: Second Gradient Step .............................. 27
B.6	Classifier Learning Stage .............................................. 35
B.7	Proof of Theorem 1 ..................................................... 37
C	Lower Bound for Linear Models on Fixed Feature Mappings	38
D	Lower Bound for Learning without Input Structure	39
14
Published as a conference paper at ICLR 2022
E Complete Experimental Results	40
E.1 Simulation ..................................................................... 40
E.1.1	Parity Labeling ........................................................ 40
E.1.2	Interval Labeling ...................................................... 42
E.2 More Simulation Result in Various Settings ..................................... 43
E.2.1	Varying Input Data Dimension ........................................... 43
E.2.2	Varying Class Imbalance Ratio .......................................... 44
E.2.3	Varying Sample Size .................................................... 45
E.3 Experiments on More Data Generation Models ..................................... 46
E.3.1	Hidden Representation Labeling ......................................... 46
E.3.2	Two-layer Networks on Mixture of Gaussians ............................. 47
E.4 Real Data: Feature Learning in Networks ........................................ 47
E.4.1	CNNs on Binary Cifar10: Feature Learning	in Networks ................... 49
E.5 Real Data: The Effect of Input Structure ....................................... 52
E.5.1	Experimental Methodology ............................................... 52
E.5.2	Experimental Results ................................................... 53
E.5.3	Larger Network on MNIST for Checking The Effect of Input Structure . .	55
E.5.4	Empirical Verification of Our Method ................................... 55
F Provable Guarantees for Neural Networks in A More General Setting	57
F.1	Problem Setup ................................................................. 57
F.1.1 Neural Network Learning	........................................... 57
F.2	Main Result ................................................................... 58
F.3	Notations ..................................................................... 58
F.4	Existence of A Good Network ................................................... 59
F.5	Initialization ................................................................ 60
F.6	Some Auxiliary Lemmas ......................................................... 61
F.7	Feature Emergence: First Gradient	Step ........................................ 62
F.8	Feature Improvement: Second Gradient Step ..................................... 68
F.9	Classifier Learning Stage and Main Theorem .................................... 81
15
Published as a conference paper at ICLR 2022
A More Technical Discussion on Related Work
Advantage of Neural Networks over Linear Models on Fixed Features. A recent line of work has
turned to show learning settings where network learning provably has advantage over linear models
on fixed features; see the nice summary in Malach et al. (2021). Here we highlight the results and
focuses of the existing related studies and discuss the differences from ours.
Yehudai & Shamir (2019) shows that the random feature method fails to learn even a single ReLU
neuron on Gaussian inputs unless its size is exponentially large in dimension. This points out the
limitation of the random feature method (belonging to the fixed feature approach) but does not
consider feature learning in networks.
Some studies show that a single ReLU neuron can be learnt by gradient descent (Yehudai & Ohad,
2020; Diakonikolas et al., 2020; Frei et al., 2020). The analysis typically involves feature learning.
However, their focus is different: they do not show the advantage over fixed feature methods and do
not consider the effect of the input structures.
Zhou et al. (2021) shows that in a special teacher-student setting, the student network will do exact
local convergence in a surprising way that all student neurons will converge to one of the teacher
neurons. The work does not consider the effect of the input structure nor the advantage over fixed
features.
Dou & Liang (2020) explains the advantage of network learning by constructing adaptive Reproducing
Kernel Hilbert Space (RKHS) indexed by the training process of the neural network, and shows that
adaptive RKHS benefits from a smaller function space containing the residue comparing to RKHS.
The work shows the statistical advantage of networks over data-independent kernels, but does not
consider the optimization for learning the network.
Ghorbani et al. (2020) considers data generated from a hidden vector with two subsets of variables,
each uniformly distributed in a high-dimensional sphere (with a different radius), while the label is
determined by only the first subset of variables. It shows the existence of good neural networks that
can overcome the curse of dimensionality by representing the best low-dimensional hidden structure.
However, it studies the approximation power of neural networks rather than the learning, i.e., it does
not show how to learn the good network.
Fang et al. (2019) argues that in the infinite width limit, a two-layer neural network will learn a nearly
optimal feature representation in the distribution sense, thanks to the convexity of the limit problem.
It is unclear how this result helps to understand the feature learning procedure for practical networks,
which is usually a non-convex process.
Chen et al. (2020a) considers a fixed, randomly initialized neural network as a representation function
fed into another trainable network which is the quadratic Taylor model of a wide two-layer network.
It shows that learning over the random representation can achieve improved sample complexities
compared to learning over the raw data. However, the representation considered is not learned, which
is different from our focus on feature learning.
Allen-Zhu & Li (2020a) considers Gaussian inputs with labels given by a multiple-layer network with
quadratic activations and skip connections (with the assumption of information gap on the weights),
and studies training a deep network with quadratic activation. It shows that the trained network can
learn proper representations and obtain small errors while no polynomial fixed feature methods can.
On the other hand, it does not focus on the influence of input structure on feature learning: note that
its input distribution contains no information about the “ground-truth” features in the target network.
It also points out that the learned features get improved during training: higher-level layers will help
lower-level layers to improve by backpropagating correction signals. Our analysis also shows feature
improvement which however is by signals from the input distribution.
Allen-Zhu & Li (2019) considers PAC learning with labels given by a depth-2 ResNet, and studies
training an overparameterized depth-2 ResNet (using uniform inputs over Boolean hypercube as an
example). It shows the trained network can obtain small errors while no polynomial kernel methods
can obtain as good errors. Similar to Allen-Zhu & Li (2020a), it does not focus on the influence of
input structure on feature learning or the advantage of networks.
16
Published as a conference paper at ICLR 2022
Allen-Zhu & Li (2020c) studies how ensemble of deep learning models can improve test accuracy
and how the ensemble can be distilled into a single model. It develops a theory which assumes the
data has multi-view structure and shows that the ensemble of independently trained networks can
provably improve test accuracy and the ensemble can also be provably distilled into a single model.
The analysis also relies on showing that the data structure can help the ensemble and the distillation.
On the other hand, their focus is on ensembles and is quite different from ours: the analysis is on
showing the multi-view input structure allows the ensembles of networks to improve over single
ones and ensembles of fixed feature mappings do not have improvement. While our focus is on
supervisedly learning one single network that outperforms the fixed feature approaches.
Daniely & Malach (2020) considers the task of learning sparse parities with two-layer networks, and
the analysis suggests that the ability to learn the label-correlated features also seems to be critical
towards the success of neural networks, although the authors did not explore much in this direction.
Malach et al. (2021) also considers similar learning problems but with specifically designed models
for the problems. The learning problems considered in Daniely & Malach (2020); Malach et al. (2021)
have input distributions that leak information about the target labeling function, which is similar
to our setting, and their analysis also shows that the first gradient descent can learn a set of good
features and later steps can learn an accurate classifier on top. Our work is inspired by their studies,
while there are some important differences. First, their focuses are different from ours. Daniely
& Malach (2020) focuses on showing neural networks can learn targets (i.e., k-parity functions)
that are inherently non-linear. Our analysis generalizes to more general distributions, including
practically motivated ones. Malach et al. (2021) focuses on strong separations between learning with
gradient descent on differentiable models (including typical neural networks) and learning using the
corresponding tangent kernels. The analysis is on specific differentiable models, while our work is
on two-layer neural networks similar to practical ones. Second, our analysis relies on the feature
improvement in the second gradient step. This is not an artifact of the analysis but comes from our
problem setup. While in Daniely & Malach (2020) the data distribution allows some neurons to be
sufficiently good after the first gradient step and needs no feature improvement, our setup is more
general where the data distribution may not have a similar strong benign effect and thus needs feature
improvement in the second gradient step.
Most related to our work is Daniely & Malach (2020). Therefore, we provide a detailed discussion to
highlight the connections and differences.
1.	Our problem setting is more general than that in Daniely & Malach (2020). To see this,
let our dictionary be the identity matrix, the set P to be the odd numbers (i.e., the labeling
function is a sparse parity). Furthermore, let the distribution of the hidden representation be
an equal mixture of the following two:
(a)	D1 : Uniform distribution over the hypercube.
(b)	D2 : Irrelevant patterns φj (j 6∈ A) have appearance probability p0 = 1/2. And the
distribution of relevant patterns φj(j ∈ A) is: all 0’s with probability 1/2, and all 1’s
with probability 1/2.
Then our problem setting reduces to their setting (up to scaling/translation of φj ’s). On the
other hand, in general our setting allows for more choices for the labeling, the dictionary,
and the distributions over φ.
2.	Upper bound: Because of the more general setting, our upper bound proof requires technical
novelty. Recall that in their work, the input distribution is essentially a mixture of D1 and
D2 above. In D2, the relevant patterns φj(j ∈ A) have the specific structure of all 0’s or
all 1’s with probability 1/2. This allows to show that neurons with weight w satisfying
Pj∈A wj = 0 will have good gradients: small components from irrelevant patterns (their
Lemma 7) and large components from relevant patterns (their Lemma 8). However, in
our setting, the relevant patterns do not have this specific structure, and thus their proof
technique is not applicable (or can be applied only when we have an exponentially large
number of hidden neurons so that some hit the good positions at random initialization). What
we showed is that the gradient has some correlation with the good feature direction. So after
the first gradient step, the neuron weights are not good yet but are in a better position for
further improvement (in particular, their setting corresponds to p0 = 1/2 which means large
noise in the weights after the first step; see discussion after our Lemma 6 in Section 5). Then
17
Published as a conference paper at ICLR 2022
the latter gradient steps are able to improve the weights to better “signal-to-noise-ratio”. In
summary, our proof does not rely on their specific input structure or an exponentially large
number of hidden neurons for hitting some good positions. The key is that the good feature
will emerge with the help of the input structure, and once in a better position, the neurons’
weights can be improved to the desired quality.
3.	Lower bound: On the other hand, our lower bound is proved by a reduction to the lower
bound results in Daniely & Malach (2020). They have shown that D1 above can lead to large
errors for fixed feature models of polynomial size. Our proof is essentially constructing
a mixture of D1 and D2 with mixture weights p0 and (1 - p0), and applying their lower
bound for D1 . See our proof in Appendix C.
4.	Conceptually, our work belongs to the same line of research as Daniely & Malach (2020), to
analyze how feature learning leads to the superior performance of networks. While their
analysis also relies on feature learning from good gradients induced by input structure,
their focus is more on separating network learning and fixed feature models and has not
explicitly explored the impact of input structures (while we agree that such an explicit study
will not be difficult in their setting). More importantly, their input distribution is specific
and atypical in practice, which allows a specific type of feature learning (as explained in
the above discussion on upper bounds). Our work thus considers a more general setting
that is motivated by practical problems. Our results then bring theoretical insights closer
for explaining the feature learning in practice and provide some positive evidence for the
importance of analysis under proper models of the input distributions.
Sparse Coding and Subspace Data Models. To analyze neural networks’ performance, various
data models have been considered. A practical way to model the underlying structure of data is by
assuming that a set of hidden variables exists and the input data is a high dimensional projection of
the hidden vector (possibly with noise). Along this line, the classic sparse coding model has been
used in existing works for analyzing networks. Koehler & Risteski (2018) considers such a data
distribution where the label is given by a linear function on the hidden sparse vector, but studies the
approximation power of networks and classic polynomial methods rather than the learning. Allen-Zhu
& Li (2020b) considers similar data distributions, but studies the performance of networks under
adversarial perturbations. Another type of related data models assumes that the label is determined
by a subset of hidden variables. Ghorbani et al. (2020) considers a hidden vector with two subsets of
variables, each uniformly distributed in a high-dimensional sphere (with a different radius), while
the label is determined by only the first subset of variables. However, Ghorbani et al. (2020) studies
the approximation power of neural networks rather than the learning. Compared to these studies,
our work assumes the input is given by a dictionary multiplied with a hidden vector (not necessarily
sparse) while the label is determined by a subset of the hidden vector, as motivated by pattern
recognition applications in practice. Furthermore, we focus on the learning ability of networks instead
of approximation.
18
Published as a conference paper at ICLR 2022
B Complete Proofs for Provab le Guarantees of Neural Networks
We first make a few remarks about the proof.
Remark. The analysis can be carried out for more gradient steps following similar intuition, while we
analyze two steps for simplicity.
Remark. Readers may notice that the network can be overparameterized. With sufficient overpa-
rameterization and proper initialization and step sizes, network learning becomes approximately
NTK. However, here our learning scheme allows going beyond this kernel regime: we use aggressive
gradient updates λW) = 1∕(2η(t)) in the first two steps, completely forgetting the old weights to
learn effective features. Using proper initialization and aggressive updates early to escape the kernel
regime has been studied in existing work (e.g., Woodworth et al. (2020); Li et al. (2019)). Our result
thus adds another concrete example.
Notations. For a vector v and an index set I , let vI denote the vector containing the entries of v
indexed by I, and v-I denote the vector containing the entries of v with indices outside I.
By initialization, w(0) for i ∈ [m] are i.i.d. copies of the same random variable w(0)〜N (0, σW Id×d);
similar for a(0) and b⑼.Let g` := {w(0), MQ, then hw(0),xi = hφ, q). Similarly, define q(t) :=
hw(t),m`). Let σφ := p∩(1 — Po)∕σ2 denote the variance of φ' for ' ∈ A.
We also define the following sets to denote typical initialization. For a fixed δ ∈ (0, 1), define
Gw (δ) := (W ∈ Rd : q' = hw, M'i,σw (Di 一 k ≤ X q ≤ 3σw (D 一 k),
I	'∈A
max |q'| ≤ σw p2 log(Dm∕δ)},
Ga(δ):= {a ∈ R : |a| ≤ σa√2log(m∕δ)}.
Gb(δ) := {b ∈ R : |b| ≤ σb√2log(m∕δ)}.
B.1 Existence of A Good Network
we first show that there exists a network that can fit the data distribution.
Lemma 7. For some s, a, b ∈ R with a, b ≥ 0, define a function δs,a,b : R → R as
δs,a,b(z) = aσr(z - s + b) - 2aσr(z - s) + aσr(z - s - b).
where σr(z) = max{z, 0} is the ReLU activation function. Then
{0	when	Z	≤	S 一	b,
a(z - s) +	ab	when	s	-	b ≤ z ≤	s,
一a(z — s)	+ ab	when	S	≤	Z ≤ S +	b,
0	when	s	+	b ≤	z.
(5)
(6)
(7)
(8)
(9)
That is, δs,a,b(Z) linearly interpolates between (S 一 b, 0), (S, ab), (S + b, 0) when Z ∈ [S 一 b, S + b],
and is 0 elsewhere.
ProofofLemma 7. This can be simply verified for the four cases of the value of z.	□
Lemma 8 (Restatement of Lemma 5). For any D ∈ Fξ, there exists a network g* (x) =
En=I a^ σ(hw*, x) + b*) with y = g*(x) for any (x, y)〜D. Furthermore, the number of neurons
n = 3(k + 1), |a*| ≤ 32k, 1∕(32k) ≤ |b*| ≤ 1/2, w* = σ Pj∈A Mj/(4k), and |(w；,x) + b* | ≤ 1
for any i ∈ [n] and (x, y)〜D.
19
Published as a conference paper at ICLR 2022
>Λ	/■	/■ ɪ	r- T	~	X~'	Tl ʃ	1 1	.	∖~'	ττn Γ 7	1	5 T 1
ProofofLemma 5. Let	W = σ	∑j∈A Mj	and let μ =	∑2j∈A	E[Φj].	We have
hw,χi = σ XhMj,Mφi = σ X φj = X φj - μ.	(IO)
j∈A	j∈A	j∈A
Then by Lemma 7,
g1 (X) = £Sp-“,2,1/2(hW, Xi)- E	δp-μ,2,1∕2(hw, xi)	(II)
p∈P	p6∈P,0≤p≤k
=£Sp,2,i/2(hw,xi + μ) - E	δp,2,ι∕2(hw,xi + μ)	(12)
p∈P	p6∈P,0≤p≤k
=X δp,2,1∕2 I X φj I - X δp,2,1∕2 ( X φj)	(13)
p∈P	j∈A	p6∈P,0≤p≤k	j∈A
= y	(14)
for any (x, y)〜D. Similarly,
g2(x) := £Sp-“+1/4,4,1/2(hW, Xi)- E	δp-μ+1∕4,4,1∕2(hw,xi)	(15)
p∈P	p6∈P,0≤p≤k
=E δp+1∕4,4,1∕2(hw, xi + μ) - E	δp+1∕4,4,1∕2(hw, xi + 〃)	(I6)
p∈P	p6∈P,0≤p≤k
=Xδp+1∕4,4,1∕2 I X φj I - X δp+1∕4,4,1∕2 I X φj I	(17)
p∈P	j∈A	p6∈P,0≤p≤k	j∈A
= y	(18)
for any (x, y)〜D. Note that the bias terms in gɪ and g2 have distance at least 1/4, then at least
one of them satisfies that all its bias terms have absolute value ≥ 1/8. Pick that one and denote it as
g(X) = Pin=1 aiσr(hwi, Xi + bi). By the positive homogeneity of σr, we have
n
g(x) = X 4kaiOr(hWi,x\/(4k) + bi∕(4k)).	(19)
i=1
Since for any (x, y)〜D, |〈Wi, x>∕(4k) + bi∕(4k)∣ ≤ 1, then
n
g(X) = X 4kaiσ(hwi, Xi/(4k) + bi/(4k))	(20)
i=1
where σ is the truncated ReLU. Now We can set a* = 4kai, w* = wj(4k), b* = b/(4k), to get our
final g*.	□
B.2 Initialization
We first show that with high probability, the initial weights are in typical positions.
Lemma 9. For any δ ∈ (0, 1), with probability at least 1 - δ - 2 exp (-Θ(D - k)) over w(0),
σW(D - k)∕2 ≤ X 金 ≤ 3σW(D - k)∕2,
'∈A
max ∣q'∣≤ σw √2 log(D∕δ).
With probability at least 1 - δ over b(0),
Ib(O)∣≤ σbP2log(1∕δ).
With probability at least 1 - δ over a(0),
∣a(0)∣ ≤ σaP2log(1∕δ).
20
Published as a conference paper at ICLR 2022
ProofofLemma 9. From q 〜N(0, σWId×d), We have:
With probability ≥ 1 - δ∕2, max' |qg | ≤
,2σW log D ,and
•	For any subset S ⊆ [D], with probability ≥ 1 -2exp (-Θ(∣S∣)), kqs∣∣2 ∈
3∣S∣σW
2
Similar for b(0) and a(0). The lemma then follows.	口
Lemma 10. We have:
•	With probability ≥ 1 -δ-2m exp(-Θ(D -k)) over wi(0) ’s, for all i ∈ [2m], wi(0) ∈ Gw(δ).
•	With probability ≥ 1 - δ over bi(0) ’s, for all i ∈ [2m], bi(0) ∈ Gb(δ).
•	With probability ≥ 1 - δ over ai(0) ’s, for all i ∈ [2m], ai(0) ∈ Ga (δ).
ProofofLemma 10. This follows from Lemma 9 by union bound.	口
The following lemma about the typical wi(0)’s will be useful for later analysis.
Lemma 11. Fix δ ∈ (0, 1). For any wi(0) ∈ Gw(δ), we have
Pr X φ'qi,' ≥ θ (J(D - kσ2σW) =θ⑴一°(7==^==Γ.	QI)
φ L'∈A	J	QD - k)σφσ2
Consequently, when P0 = Ω(k2∕D) and k = Ω(log2 (Dm∕δ)),
Pr x φ'q(0) ≥ θ (σw) = θ⑴-τ1∕4.	(22)
φ '∈A
ProofofLemma 11. Note that for ' ∈ A, E[φ'] = 0, E[φ2] = σφ, and E[∣φ'∣3] = Θ(σφ∕σ). Then
the statement follows from Berry-Esseen Theorem.	口
B.3	Some Auxiliary Lemmas
The expression of the gradients will be used frequently.
Lemma 12.
∂
L—LD(g； σξ) = -aiE(χ,y)~D {yI[yg(x; ξ) ≤ 1]EξiIKwi, Xi + bi + ξ ∈ (0,1)]x},
∂wi
∂
犷LD(g； σξ) = -aiE(x,y)~D {yI[yg(x;ξ) ≤ 1]EξiIKwi,xi + bi ∈ (0, 1)]},
∂bi
∂
∂aLD(g； σξ) = -E(x,y)~D {yI[yg(x; ξ) ≤ 1]Eξiσ(hwi,xi + bi + ξi)}.
(23)
(24)
(25)
Proof of Lemma 12. It follows from straightforward calculation.
□
We now show that a small subset of the entries in φ, q does not affect the probability distribution of
hφ, qi much.
Lemma 13. Suppose V 〜N(0, σ2). For any B ⊇ A and any b:
Pr {hφ, qi + ν ≥ b} - Pr {hφ-B, q-Bi + ν ≥ b}
O (	KΦb,qBil	+ σ3+ σ2kq-Bk3/ar !
Vσφ kq-B k2 + σ2)1/2	(σ2 + σφφ kq-B k2)3/2 广
(26)
(27)
21
Published as a conference paper at ICLR 2022
Similarly,
Pr {hΦ,qi ≥ b}- Pr {{φ-B,q-Bi ≥ b}
φ-B	φ-B
≤ O ( lhΦB,qB)| + kq-B k3 A
-lσΦ∣q-B ∣∣2	σσφkq-B k3 广
(28)
(29)
(30)
(31)
(32)
□
ProofofLemma 13. Note that for ' ∈ A, E[φ'] = 0, E[φ2] = σφ, and E[∣φ'∣3] = Θ(σφ/σ). Let
t = ∣hφB, qB)|. Then by the Berry-Esseen Theorem,
Pr {3,q〉+ V ≥ b} - Pr {{φ-B,q-B〉+ V ≥ b}
≤ Pr {hφ-B, q-Bi + V ∈ [—t + b,t + b]}
φ-B
≤________2t_________+。。3 + σφ ∣q-B 脸㈤
一(σφl∣q-B∣∣2 + σ2W2	(σ2 + σφIlq-B||2)3/2 .
The second statement follows from a similar argument.
We also have the following auxiliary lemma for later calculations.
Lemma 14.
EφA {y}	=0,	(33)
Eφa {∣y∣}	=1,	(34)
Eφj{∣Φj ∣}	=2σφσ, for j ∈ A,	(35)
Eφa {yφj}	Y 一 , σ	(36)
EφA {gφj |}	≤ ɪ, for all j ∈ [D]. σ	(37)
ProofofLemma 14.
Eφa {y}	= E Eφa {y∣y = v} Pr[y = v]	(38)
	v∈{土1}	
	=2 X eφa ®y = v}	(39)
	v∈{土1}	
	= 0.	(40)
EφA {∣y∣}	=X eφa {加® = v}Pr[y = v]	(41)
	v∈{土1}	
	=l X EφA {㈤g=v}	(42)
	v∈{土1}	
	=1.	(43)
Eφj{∣Φj ∣}	1-PoK1 - Po) + |1 - Po[Po	2 2~ =	σ	= 2σφσ.	(44)
Eφa {yφj}	-E (,φj- Eφ]) =EφA ∖y —σ 一 ∫	(45)
	=1EφA {yφj - yE φ ]}	(46)
	Y 一 . σ	(47)
eφa {∖yφj|}	=EφA 叮。/}	(48)
	≤ 1 . σ	(49)
22
Published as a conference paper at ICLR 2022
□
B.4	Feature Emergence: First Gradient Step
We will show that w.h.p. over the initialization, after the first gradient step, there are neurons that
represent good features.
We begin with analyzing the gradients.
Lemma 15 (Full version of Lemma 6). Fix δ ∈ (0, 1) and suppose wi(0) ∈ Gw(δ), bi(0) ∈ Gb (δ) for
all i ∈ [2m]. Let
k log1/2 (Dm∕δ) + log3/2 (Dm∕δ)
Ce :=	/	.
yσ2 σ2(D - k)
If P o = Ω(k2/D), k = Ω(log2(Dm∕δ)), and σ(1) < 1/k ,then
D
会LD(g⑼;σ(1)) = -aiO) X MjTj	(50)
∂wi
j=1
where Tj satisfies:
• if j ∈ A, then |Tj — βγ∕σ∣ ≤ O(ce/σ), where β ∈ [Ω(1), 1] and depends only on W(O), b"
• if j ∈ A, then ∣Tj | ≤ O(σφceσ).
Proof of Lemma 15. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads tog (O) (x; ξ(1)) = 0, we have
二LD①⑼;σξ1))	(51)
∂w	ξ
=—a(O)E(X,y)~D {yI[yg⑼(x; ξ⑴) ≤ 1]E«)IKw⑼,x〉+ b⑼ + ξ⑴ ∈ (0,1)]x}	(52)
=—a(O)E(X,y)~D" {yI[hw⑼,x〉+ b⑼+ ξ⑴ ∈ (0,1)]x}	(53)
D
=—a(O) X Mj E(x,y)~D,ξ(i) {yI[hw(0), xi+ b(0) + ξ⑴ ∈ (0,1)]φj} .	(54)
j=1 S-----------------------V---------------------}
:=Tj
First, consider j ∈ A.
Tj = E(χ,y)~D,ξ(i) {yI[hw⑼,xi + b(O) + ξ⑴ ∈ (0,1)]φj}	(55)
=EφA,ξ(1) yφj Pr hhφ,qi+b(O)+ξ(1)∈ (0, 1)i .	(56)
Let
Ia := Pr hhφ, qi + b(O) + ξ(1) ∈ (0, 1)i ,	(57)
Ia0 := Pr hhφ-A, q-Ai + b(O) + ξ(1) ∈ (0, 1)i .	(58)
We have
∣Eξ(i) (Ia - I a )1	(59)
≤ Eξ(1)	Pr hhφ,	qi	+ b(O) + ξ(1) ≥0i	— Pr	hhφ-A, q-Ai + b(O) + ξ(1) ≥ 0i	(60)
+ Pr	hhφ, qi	+	b(O) + ξ(1) ≥1i	+ Pr hhφ-A, q-Ai + b(O) + ξ(1) ≥1i	.	(61)
φ-A,ξ(1)	φ-A,ξ(1)
23
Published as a conference paper at ICLR 2022
Then by Lemma 13,
Pr hφ, qi + b(0) + ξ(1) ≥ 0 - Pr hφ-A, q-Ai + b(0) + ξ(1) ≥0 = O(e).
(62)
Note that	P'∈A Var(θ`g`)	=	Θ(σφσW(D	- k)) =	Θ(σW), and	∣φ'∣	≤	1,	max' |q'|	≤
σw Vz2 Iog(Dm/δ). Applying Bernstein,s inequality for bounded distributions, We have:
Pr [hΦ-A, q-Ai ≥ 1/4] = exp(-Ω(k)) = O(ee).	(63)
φ-A
We also have:
Pr Ib(O) + ξ⑴ ≥ 1川=exp(-Ω(k)) = O(ee).	(64)
Therefore,
Pr	hhφ, qi + b⑼ + ξ⑴ ≥ l] = exp(-Ω(k)) = O(ee)	(65)
φ-A,ξ(1)
Where the last step folloWs from the assumption on σw and k. A similar argument gives:
Φ-P,ξ(ι) ",q-Ai + b(0)+ ξ(1 ≥ 1i =exP(Rk)) = O(Q	(66)
Then We have
Tj - EφA,ξ(1) {yφjIa0}	(67)
≤ EφA {∣yφjI ∣Eξ(i)(Ia Ta)∣}	(68)
≤ OG)EφA {∣yφj∣}	(69)
≤ Oa/σ	(70)
Where the last step is from Lemma 14. Furthermore,
EφA,ξ(1) {yφjIa0 }	(71)
= EφA {yφj} Eξ(1) [Ia0]	(72)
=EφA{yφj} Pr	Ihφ-A, q-Ai + b(0) + ξ(1) ∈ (0, 1)i	(73)
φ-A ,ξ(1)
By Lemma 11, the assumption on po, and (63), We have
Pr ∣hΦ-A, q-Ai + b(0) ∈ (0,1∕2)i ≥ Ω(1) - O(1∕k1/4),	(74)
Pr [ξ⑴ ∈ (0,1∕2)i = 1/2 - exp(-Ω(k)),	(75)
This leads to
β := Eξ(1) [Ia0 ]
Pr
φ-A ,ξ(1)
[hΦ-A,q-Ai + b(0) + ξ(1) ∈ (0,1)] ≥ Ω(1).
By Lemma 14, EφA {yφj∙} = γ∕σ. Therefore,
ITj - βγ∕σ∣ ≤ OG∕σ).
(76)
(77)
NoW, consider j 6∈ A. Let B denote A ∪ {j}.
Tj =	E(χ,y)~D,ξ(i) {yφjl	[hφ, qi	+	b(0)	+	ξ(1)	∈	(0,1)i0	(78)
= EφBEφ-B,ξ(1) nyφjI Ihφ, qi + b(0) + ξ(1) ∈ (0, 1)io	(79)
=EφB,ξ(1) yφj Pr Ihφ, qi + b(0) + ξ(1) ∈ (0, 1)i .	(80)
24
Published as a conference paper at ICLR 2022
Let
Ib := Pr hhφ, qi + b(0) + ξ(1) ∈ (0, 1)i ,	(81)
Ib0 := Pr hhφ-B, q-Bi + b(0) + ξ(1) ∈ (0, 1)i .	(82)
Similar as above, We have ∣Eξ(i) (Ib - Ib)| ≤ O(Ee) by Lemma 13. Then by Lemma 14,
Tj -EφB,ξ(1) {yφjIb0}	(83)
≤ EφB {∣yφjl∣Eξ(i) (Ib - Ib)1}	(84)
≤ O(Ee)EφA {∣y∣} Eφj{∣φj∣}	(85)
≤ O(Ee) × O(σφσ)	(86)
=O(σφ Eeσ).	(87)
Furthermore,
EφB,ξ(1) {yφjIb0} = EφA {y} Eφj {φj} Eξ(1) [Ib0] = 0.	(88)
Therefore,
∣Tj| ≤ O(σφEeσ).	(89)
□
Lemma 16. Under the same assumptions as in Lemma 15,
^dLD(g⑼;σ(I)) = YTb	(90)
∂bi
where |Tb| ≤ O(Ee).
Proof of Lemma 16. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads to g(0) (x; ξ (1)) = 0, We have
^dLD (g⑼;σ(1))	(91)
∂b	ξ
=-a⑼E(x,y)〜D {yI[yg⑼(x;ξ) ≤ 1限°)IKw⑼㈤ + b⑼ + ξ⑴ ∈ (0,1)]}	(92)
=-a(0) E(x,y)〜D,ξ(i) {yI[hw(0),xi + b(0) + ξ⑴ ∈ (0,1)]}.	(93)
S----------------------V-----------------------}
:=Tb
Similar to the proof in Lemma 6,
Pr[hφ,qi+b(0)+ξ(1) ∈	(0, 1)] - Pr	[hφ-A,	q-Ai	+	b(0) + ξ(1) ∈	(0, 1)]	= O(Ee).	(94)
φ-A	φ-A
Then
Tb - EφA,ξ(1) y Pr [hφ-A, q-Ai + b(0) + ξ(1) ∈ (0, 1)]	(95)
= EφA,ξ(1)	|y| Pr[hφ,qi+b(0)+ξ(1) ∈ (0, 1)] - Pr [hφ-A, q-Ai + b(0) + ξ(1) ∈ (0, 1)]
φ-A	φ-A
(96)
≤ O(Ee)EφA {|y|}	(97)
≤ O(Ee).	(98)
Also,
EφA,ξ(1) y Pr [hφ-A, q-Ai + b(0) + ξ(1) ∈ (0, 1)]	(99)
= EφA {y}	Pr	[hφ-A, q-Ai + b(0) + ξ(1) ∈ (0, 1)]	(100)
φ-A,ξ(1)
= 0.	(101)
Therefore, |Tb | ≤ O&).	□
25
Published as a conference paper at ICLR 2022
Lemma 17. We have
∂ai LD (g(0); σ(1)) = -Ta
where |Ta| ≤ O(max' q(0)). So if w(0) ∈ G(δ), |Ta| ≤ O(σwPlog(Dm/δ)).
(102)
Proof of Lemma 17. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads to g(0) (x; ξ(1)) = 0, we have
∂aLD (g⑼;σξ1))	(103)
：-E(x,y)〜D {yI[yg⑼(x;ξ(1)) ≤ 1]Eξ(i)σ(hw⑼,xi + b⑼ + ξ⑴)}	(104)
-E(χ,y)〜D,ξ(i) {yσ(hw⑼,xi + b⑼ + ξ⑴)} .	(105)
、-------------------{------------------}
:=Ta
Let φ0A be an independent copy of φA, φ0 be the vector obtained by replacing in φ the entries φA with
φ0A , and let x0 = Mφ0 and its label is y0. Then
|Ta| = EφA nyEφ-A,ξ(1) σ(hw(0), xi + b(0) + ξ(1))}	(106)
≤ 1 EφA {Eφ-A,ξ(i)σ(hw⑼,xi + b⑼+ ξ(I))Iy = l}	(107)
- EφA {Eφτ,ξ(i)σ(hw⑼,xi + b⑼+ ξ⑴)|y = -l}	(108)
≤ 1 EφA {Eφτ,ξ(i)σ(hw⑼,xi + b⑼	+ ξ(I))Iy = l}	(109)
- EφA {Eφτ,ξ(i)σ(hw⑼,x0i + b⑼	+ ξ⑴)|y0 = -l} .	(110)
Since σ is 1-Lipschitz,
lTal ≤ 1 eΦa,φa nEΦ-A ∣hw(0),χi - hw(0),χ0i∣ |y = 1,y0 = -1}	(III)
≤ 1 eφ-a (eφa {∣hw(0),χi∣ ly = 1} + eφa {∣hw⑼,x0i∣ ly0 = -1})	(112)
= Eφ-A,φA ∣∣∣hw(0), xi∣∣∣	(113)
= Ex ∣∣∣hw(0), xi∣∣∣	(114)
≤ JExhw(O),χi2	(115)
≤ max qt(°λ Ex I X φ2+ X	lφjφ'1)	(116)
V∈[D]	j='j,'∈A	)
≤ maxq(0)PEx (1 + O(1))	(117)
=Θ(max q(,0)).	(118)
□
With the bounds on the gradient, we now summarize the results for the weights after the first gradient
step.
26
Published as a conference paper at ICLR 2022
Lemma 18. Set
λWυ = 1∕(2η ⑴),λaI)= λb1=0,σ(I) = I/k3∕2.
Fix δ ∈ (0,1) and suppose W(O) ∈ Gw(δ), bi0) ∈ Gb(δ) for all i ∈ [2m]. If Po = Ω(k2∕D),
k = Ω(log2 (Dm∕δ)), thenforall i ∈ [m], W(I) = PD=I q(1)M' satisfying
•	if' ∈ A, then ∣q(1 — η(1)a(0)βγ∕σ| ≤ O (n(’；i |'e} where β ∈ [Ω⑴，1] and depends
only on Wi(0), bi(0);
•	if' ∈ A, then |q(?| ≤ O (σφ切(1)。(0)|金σ);
and
•	bi(1) = bi(0) + η(1)ai(0)Tb where |Tb| = O (e);
•	ai1) = a(0) + η(1)Ta where |Ta| = O(σwPlog(Dm∕δ)).
Proof of Lemma 18. This follows from Lemma 10 and Lemma 15-17.
□
B.5 Feature Improvement: Second Gradient Step
We first show that with properly set η(1), for most x, ∣g(1)(x; σξ(2))∣ < 1 and thus yg(1)(x; σξ(2)) < 1.
Lemma 19. Fix δ ∈ (0, 1) and suppose Wi(0) ∈ Gw (δ), bi(0) ∈ Gb(δ), ai(0) ∈ Ga(δ) for all i ∈ [2m].
If P o = Ω(k2∕D), k = Ω(log2(Dm∕δ)), σ0 ≤ σ2∕(γk2), η ⑴=O (km^), and σξ2 ≤ 1∕k,
then with probability ≥ 1 一 exp(-Θ(k)) over (x,y), we have yg(1)(x; σξ(2)) < 1. Furthermore,
for any i ∈ [2m], ∣hw(1),xi∣ = ∣hq(1),Φi∣ =Okn(Id卜Y), ∣ h(q(1))-A, Φ-Ai ∣ =Okn(Id卜Y), and
Ib(I) - bm+J =O(In⑴a(l⅜e)∙
ProofofLemma 19. Note that W(O) = w(^+i, b(0) = b^+ɪ, and a(0)= 一。弋+钎 Then the gradient
for Wi is the negation of that for Wm+i, the gradient for bi is the negation of that for bm+i, and the
gradient for ai is the same as that for am+i. With probability ≥ 1 一 exp(-Θ(max{2po(D 一 k), k})),
among all j ∈ A, We have that at most 2po(D 一 k) + k of φj are (1 一 po)∕σ, while the others are
—Po∕σ. For data points with φ satisfying this, we have:
∣∣g(()(x; dξ(2))∣∣	(119)
∣ 2m	∣
= ∣∣X ai(()Eξ(2) d(hWi((), xi + bi(() +ξi(2))∣∣	(120)
∣ i=(	∣
∣m	∣
= ∣X (()E (h (() i + b(() + ξ(2)) + (() E (h (() i + b(() + ξ(2) ) ∣ (121)
= ∣	ai	Eξ(2) d(hWi	, xi +	bi	+	ξi	) +	am+iEξ(2) d(hWm+i, xi +	bm+i +	ξm+i) ∣ (121)
∣ i=(	∣
∣m	∣
≤ ∣∣X	ai(()Eξ(2)d(hWi((), xi +	bi(()	+ξi(2)) +	a(m()+iEξ(2) d(hWi((), xi +	bi(()	+ξi(2))	∣∣	(122)
∣ i=(	∣
∣m	∣
+ X(一姬典⑶ σ(hw(ι),χi+b(()+ξ(2))+am+iEξ ⑵ σ(hwm+i,xi+bm+i+*).
∣ i=(	∣
(123)
27
Published as a conference paper at ICLR 2022
Then We have
m
Ig(I)(x; σ(2)) I ≤ X ∣2η(I)TaEg(2)σ(hw(1,x) + b(1) + ξ(2)) ∣	(124)
i=1
m
+XIam)HI (IhWf) -wm+i,xi i+Ib(I)- bm+i D	(125)
i=1
m
≤ XI2n(1)Ta I (IhW(I)㈤ + b(1) ∣+ Eξ(2) ∣ξ(2) I)	(126)
i=1
m
+X∣αm+i I (IhWi(I)-Wm+i,χi I+|b(I)- bm+i ∣) ∙	(127)
i=1
We have ∖Ta∖ = O(σw/log(Dm∕δ)), and
IhW(I)词 ≤ O(∖η⑴a(O)∖) (βγ∕σ + ee∕σ) σ	(128)
+ O(∖η⑴αi°∖σφ(⅛σ) ((2po(D - k) + k)(1 - p0)∕σ + PoD∕σ)	(129)
≤ O(∖η(1)ɑ(O)∖) (kγ∕σ2 + 1⅛k∕σ2 + (k + PoD)σφ(⅛)	(130)
≤ O(η⑴(1+ p°σ)∕γ).	(131)
WI) I ≤ |b(0) ∣ + ∣η⑴a(0)Tb|	(132)
≤ 当H+ ∣ η(i)α(O) σ ∣∙	(133)
E*)∣ξ(2)∣≤ O(σ(2))∙	(134)
∖am+i∖ ≤ ∖αi0)∖+ ∖η(I)Tal ≤ Ia(O)I+ O(η(1)σw/log(Dm/S)).	(135)
IhW(I)-洲+i ,χi ∣ = 2 IhW(I),χi ∣ = O(n(1)(1 + p°σ)∕γ).	(136)
Ib(I)- bm1+i∣ = 2∖η(1)a(0)Tb∖ = O(∖η(1)ai0k)∙	(137)
Then We have
I g(1)(x； σξ(2)) ∣ ≤ O (mη(1)σw√log(Dm∕δ)) (nʒ- + P"fm/S) + In(I)a(O)Ie ∣ + σξ2)
(138)
+ O (m(∖a(O) ∖+ n⑴σw √log(Dm∕δ))) (^ʒ7 + ∣ n⑴a(O)σ1 )	(139)
=O (m*σw log(Dm0 + m∖a(0) ∖ (可 + 卜⑴婿!1))	(140)
=O (mn⑴σw log(Dm0 + m∖a(0) ∖ d + mσan⑴ 士)	(141)
∖	k	i Y	7/
< 1.	(142)
Then I yg(1)(x; σ(2)) ∣ < 1. Finally, the statement on ∣ h(q(I))-a, φ-a)∣ follows from a similar
calculation on IhW(I),x) ∣ = Ihq(I),Φ) ∣ .
□
We are now ready to analyze the gradients in the second gradient step.
Lemma 20. Fix δ ∈ (0,1) and suppose W(O) ∈ GW(δ),b(O) ∈ 乳⑷"O) ∈ Ga(S) for all i ∈ [2m].
Let Ee2 ：= O (n( "a' k)"€e，) + exp(-Θ(k)). If k = Ω(log2(Dm∕δ)) and k = O(D), σ0 ≤
28
Published as a conference paper at ICLR 2022
σ2∕(γk2), η(1) = O (kmYσa), and σξ(2) = 1/k3/2, then
D
为LD(g⑴；σ(2)) = -ai1) X MjTj	(143)
∂wi
j=1
where Tj satisfies:
•	if j ∈ A, then ∣Tj — βγ∕σ∣ ≤ O(ee2∕σ + η(1)∕σξ(2) + η(1)∣ai0)∣ee/(σσξ2))), where β ∈
[Ω⑴，1] and depends only on w(0), b(0);
•	if j ∈ A, then ∣Tj | ≤ 1 exp(—Θ(k)) + O(σφ金2σ).
Proof of Lemma 20. Consider one neuron index i and omit the subscript i in the parameters. By
Lemma 19, Pr[yg(1)(x; ξ(2)) > 1] ≤ exp(—Θ(k)). Let Ix = I[yg(1)(x; ξ(2)) ≤ 1].
FLD(g(1)； σ(2))	(144)
∂w	ξ
=—a⑴E(x,y)〜D {yIxEξ⑵IKw(I), xi+ b⑴ + ξ⑵ ∈ (0,1)]x}	(145)
D
=a⑴ X Mj E(x,y)〜D,ξ(2) {yIxI[hw⑴,xi + b(1) + ξ⑵ ∈ (0,1)]φj} .	(146)
j = 1	X-----------------------{-----------------------}
:=Tj
Let Tji := E(x,y)〜D,ξ(2) {yIKw(I),xi + b⑴ + ξ(2) ∈ (Oj)]φj}. Wehave
|Tj — Tj1 |	(147)
=∣E(x,y)〜DM) {y(1- Ix)IKw(I),xi + b⑴+ ξ⑵ ∈ (0,1)]φj 0|	(148)
≤ 萧(x,y)〜D,ξ⑵ |1 - Ix|	(149)
≤ 1exp(-Θ(k)).	(150)
σ
So it is sufficient to bound Tj1. For simplicity, we use q as a shorthand for qi(1).
First, consider j ∈ A.
Tj1 = E(x,y)〜D,ξ(2) {yI[hw(I), xi + b(1) + ξ⑵ ∈ (0,1)]φj}
=EφA yφj Pr(2) hhφ,qi+b(1)+ξ(2)∈ (0, 1)i	.
(151)
(152)
Let
Ia := Pr hhφ, qi + b(1) + ξ(2) ∈ (0, 1)i ,	(153)
Ia0 := Pr hhφ-A, q-Ai + b(1) + ξ(2) ∈ (0, 1)i .	(154)
By the property of the Gaussian ξ(2), that |h0a, qA)| = O(n()|ai 32k()+8)), and that ∣hφ, q)| =
Kw(I), x)| = O(η(I)∕γ) < O(1∕k) and ∣hΦ-A, q-A)| = O(η⑴∕γ) < O(1∕k), we have
|Ia -Ia0|	≤	|||Pr hhφ, qi +	b(1)	+ ξ(2) ≥0i	- Pr hhφ-A, q-Ai	+ b(1) + ξ(2)	≥	0i|||	(155)
+Pr hhφ, qi + b(1) + ξ(2) ≥1i +Pr hhφ-A, q-Ai + b(1) + ξ(2) ≥1i	(156)
=O (η(I)Ia(O)% + ee)! + exp(-Θ(k)) = O&2).	(157)
∖ σ2σ(2)	/
29
Published as a conference paper at ICLR 2022
This leads to
Tj1 -EφA,φ-A {yφj Ia0}	(158)
≤ EφA {∣yφjI ∣Eφ-A(Ia Ta)∣}	(159)
≤ 0(ee2)EφA {∣yφj|}	(160)
≤ OQ2/σ	(161)
where the last step is from Lemma 14. Furthermore,
EφA,φ-A {yφjIa0}	(162)
= EφA {yφj}Eφ-A[Ia0]	(163)
=EφA{yφj}	Pr	hhφ-A, q-Ai + b(1) + ξ(2) ∈ (0, 1)i .	(164)
φ A,ξ(2)
By Lemma 19, we have ∣hφ-A, q-Ai∣ ≤ O(η(1)σ/γ). Also, ∣b⑴-b(0)| ≤ O(η(1)∣ai0)∣ee). By the
property of ξ(2),
∣∣∣Pr hφ-A, q-Ai + b(1) + ξ(2) ∈ (0, 1) - Pr b(0) + ξ(2) ∈ (0, 1)
≤ O(η⑴σ∕(γσξ2))) + O(η⑴∣ai0)ke∕σ(2)).
On the other hand,
β := Pr	hb(0) + ξ(2) ∈ (0, 1)i = Pr hξ(2) ∈ (-b(0),1 -b(0))i
=c(1)
and β only depends on b(0). By Lemma 14, E@a {yφj} = γ∕σ. Therefore,
∣Tji - βγ∕σI ≤ O(ee2∕σ) + O(η⑴∕σg + O(η⑴Ia(O)Iee/(σσg).
Now, consider j 6∈ A. Let B denote A ∪ {j}.
Tjl= E(x,y)~D,ξ⑵{yφji [hΦ, q + b(1) + ξ(2) ∈ (0,1)i0
=EφBEφ-B,ξ(2) nyφjI hhφ, qi + b(1) + ξ(2) ∈ (0, 1)io
=EφB yφj	Pr	hhφ,qi+b(1)+ξ(2)∈(0,1)i	.
Let
Ib := Pr hhφ, qi + b(1) + ξ(2) ∈ (0, 1)i ,
Ib0 := Pr hhφ-B, q-Bi + b(1) + ξ(2) ∈ (0, 1)i .
(165)
(166)
(167)
(168)
(169)
(170)
(171)
(172)
(173)
(174)
Similar as above, we have IIb - Ib0 I ≤ ee2. Then by Lemma 14,		(175) (176) (177) (178) (179)
	∣∣Tj1 - EφB ,φ-B {yφjIb0 }∣∣ ≤EφB {IyφjIIEφ-B(Ib-Ib0)I} ≤O(ee2)Eφj{IφjI} ≤ O(Ce)X O(σφσ) =O(σφ ee2σ).	
Furthermore,	EφB,φ-B {yφjIb0} = EφA {y} Eφj {φj} Eφ-B [Ib0] = 0.	(180)
Therefore,	ITjiI ≤ O(σφee2σ).	(181) □
30
Published as a conference paper at ICLR 2022
Lemma 21. Under the same assumptions as in Lemma 20,
MLD (g ⑴;σ(2)) = -a(1)Tb	(182)
∂b	ξ
where |Tb| ≤ exp(-Ω(k)) + O(Ee?).
Proof of Lemma 21. Consider one neuron index i and omit the subscript i in the parameters. By
Lemma 19, Pr[yg(1)(x; ξ(2)) > 1] ≤ exp(-Ω(k)). Let Ix = I[yg(1)(x; ξ(2)) ≤ 1].
^dLD(g⑴;σ(2))	(183)
∂ b	ξ
=-a(1) E(x,y)〜D {yIxEξ⑵IKw⑴，x〉+ b⑴ + ξ⑵ ∈ (0,1)]} .	(184)
X---------------------V----------------------}
:=Tb
Let Tbi := E(x,y)〜D,ξ(2) {yI[hw⑴,x〉+ b⑴ + ξ(2) ∈ (0,1)]}. Wehave
|Tb - Tb1 |	(185)
=∣E(x,y)〜D,ξ⑵{y(i - Ix)IKw⑴,xi + b(1) + ξ⑵ ∈ (0,1)]} I	(186)
≤ E(x,y)〜D,ξ⑶ |1 - Ix|	(187)
≤ eχp(-Ω(k)).	(188)
So it is sufficient to bound Tb1. For simplicity, we use q as a shorthand for qi(1).	
Tbi= E(x,y)〜D,ξ⑵{yI [hφ, q + b(1) + ξ⑵ ∈ (0,1)]}	(189)
=EφAEφ-A,ξ(2) nyIhhφ,qi+b(1)+ξ(2)∈(0,1)i}	(190)
=EφA y Pr(2) hhφ,qi+b(i)+ξ(2)∈(0,1)i .	(191)
Let	
Ib := Pr hhφ, qi + b(i) + ξ(2) ∈ (0, 1)i ,	(192)
Ib0 := Pr hhφ-A, q-Ai + b(i) + ξ(2) ∈ (0, 1)i .	(193)
Similar as in Lemma 20, we have |Ib - Ib0 | ≤ Ee2. Then by Lemma 14,	
IITbi - EφA ,φ-A {yIb0 }II	(194)
≤ EφA {∣Eφ-A (Ib-Ib)1}	(195)
≤ O(Ee2).	(196)
Furthermore,	
EφA,φ-A {yIb0} = EφA {y} Eφ-A [Ib0] = 0.	(197)
Therefore, |Tbi | ≤ O(Ee2) and the statement follows.	□
Lemma 22. Under the same assumptions as in Lemma 20,
7d-LD(产;σ(2)) = -Ta
∂ai	ξ
where |Ta| = O(η(1)σ∕γ) +exp(-Ω(k))poly(Dm).
(198)
31
Published as a conference paper at ICLR 2022
Proof of Lemma 22. Consider one neuron index i and omit the subscript i in the parameters. By
Lemma 19, Pr[yg(1)(x; ξ(2)) > 1] ≤ exp(-Ω(k)). Let Ix = I[yg(1)(x; ξ(2)) ≤ 1].
∂daLD(g⑴;σξ2))	(199)
=-E(x,y)~D {yIxEξ⑵ σ(hw⑴,xi + b⑴ + ξ⑵)}.	(200)
、-------------------{-------------------}
:=Ta
Let Tai := E(x,y)~D {yEξ⑵σ(hw(1),xi + b(1) + ξ(2))}. Wehave
|Ta - Ta1|	(201)
=∣E(x,y)~D {y(1- Ix)Eξ⑵σ(hw(1),xi + b(1) + ξ(2))}∣	(202)
≤ E(x,y)~D,ξ⑵ |1 - Ixl	(203)
≤ exp(-Ω(k)).	(204)
So it is sufficient to bound Ta1. For simplicity, we use q as a shorthand for qi(1).
Let φ0A be an independent copy of φA, φ0 be the vector obtained by replacing in φ the entries φA with
φ0A , and let x0 = Mφ0 and its label is y0 . Then
|Ta1| := EφA nyEφ-A,ξ(2)σ(hw(1), xi + b(1) +ξ(2))o	(205)
≤ 1 EφA {Eφ-A,ξ⑵σ(hw(1),xi + b(1) + ξ(1))∣y = 1}	(206)
-eΦa {Eφ-A,ξ⑵σ((W(I),xi + b(1) + ξ(2))ly = -1}	(207)
≤ 2 EφA nEΦ-A,ξ ⑵ σ(hw(1),xi + b(1) + ξ(2))∣y = l}	(208)
-EφA {Eφ-A,ξ⑵σ((w⑴,x0i + b(1) + ξ(2))∣y0 = -l}	(209)
≤ 1 eφa,φa {eφ-Akw(I),xi - hw(I),x0i J |y = 1,y0 = -1}	(210)
≤ 1 eφ-a (eφa {Ihw(I),xiJ |y = 1} + eφa n∣hw(1),χ0i∣ |y0 = -1})	(211)
≤ Eφ-A,φA JJJhw(1), xiJJJ	(212)
= Ex ∣∣∣hw(1), xi∣∣∣	(213)
=O(η(I)σ∕γ) + exp(-Ω(k)) × D × kq(I)k∞kΦk∞	(214)
=O(η(i)σ∕γ) + eχp(-Ω(k))DIn(I)a(02l(γ +1) σ2	(215)
=O(n ⑴ σ∕γ) + exp(-Ω(k))poly(Dm)	(216)
where the fourth step follows from that σ is 1-Lipschitz, the third to the last step from Lemma 19,
and the second to the last step from Lemma 18.	口
With the above lemmas about the gradients, we are now ready to show that at the end of the second
step, we get a good set of features for accurate prediction.
Lemma 23. Set
2
η(I) = J ,λai) = 0, λW1) = 1/(2n(1)), σ(1) = 1/k3/2,	(217)
km3 a	w	ξ
n(2) = 1, λa2) = λ(2) = 1∕(2η(2)), σξ2) = 1/k3/2.	(218)
32
Published as a conference paper at ICLR 2022
Fix δ ∈ (0,O(1∕k3)). If P0 = Ω(k2∕D), k = Ω (log2 (Dm))，and m ≥ max{Ω(k4), D},
then with probability at least 1 一 δ over the initialization, there exist ajs such that g(x):=
P2mι aiσ(hw(2), xi+b(2)) satisfies LD(g) = 0. Furthermore, kako = O(m∕k), k⅛k∞ = O(k5/m),
and kak2 = O(k9∕m). Finally, ka(2)k∞ = O (焉)，∣∣w(2)∣∣2 = O(σ∕k), and |bi2)| = O(1∕k2)
for all i ∈ [2m].
ProofofLemma 23. By Lemma 5, there exists a network g*(x) = P3(k+1) a'σ(hw',x) + 坟)
satisfying g*(x) for all (x, y)〜D. Furthermore, |a*| ≤ 32k, 1∕(32k) ≤ |b*| ≤ 1/2, w* =
σ Pj∈A Mj/(4k), and |〈w*, x)+ b*| ≤ 1 for any i ∈ [n] and (x, y)〜D. Now we fix an ', and
show that with high probability there is a neuron in g(2) that can approximate the `-th neuron in g*.
By Lemma 10, with probability 1 一 2δ over wi(0)’s, they are all in Gw(δ); with probability 1 一 δ over
ai(0)’s, they are all in Ga(δ); with probability 1 一 δ over bi(0)’s, they are all in Gb(δ). Under these
events, by Lemma 18, Lemma 20 and 21, for any neuron i ∈ [2m], we have
D
wi(2) = ai(1)XMjTj,
j=1
bi(2) = bi(1) + ai(1)Tb.
(219)
(220)
where
•	if j ∈ A, then |Tj 一 βγ∕σ∣ ≤ s := O(i2∕σ + η⑴∕σξ2) + η(1) |a(0) ∣6e∕(σσ(2))), where
β ∈ [Ω(1),1] and depends only on w(0),b(0);
•	if j ∈ A, then |Tj| ≤ “ := 1 exp(-Θ(k)) + O(σφ2屹3).
•	|Tb| ≤ Eb ：= 1 exp(-Θ(k)) + O&2).
Given the initialization, with probability Ω(1) over b(0), We have
lbi0)l∈	⅛ ,k , Sign(biO))=Sign(b&.
2k	k
Finally, since 4klb'lβγ ∈ [Ω(k2γ∕σ2), O(k3γ∕σ2)] and depends only on w(0), bi0)
Ibi ∣σ2
Ea = Θ(1∕k2), with probability Ω(5) > δ over a(0),
(221)
we have that for
4≡γa(0) - 1≤ Ea, ∣a(0)∣ =。(i).	(222)
Let na = Eam∕4. For the given value of m, by (219)-(222) we have with probability ≥ 1 一 5δ over
the initialization, for each ' there is a different set of neurons l` ⊆ [m] with |4| = n and such that
for each i` ∈ l`,
ιb(0)ι∈ 2k2,k , sign(b(0)) = sign(b*),
4k∣b;∣βγ (0) _ 1
傅而i' 一
≤ Ea,
(223)
(224)
33
Published as a conference paper at ICLR 2022
We also have
σ
EhMj
j∈A
≤
〈党H-? XhMj,x +
j	j∈A
⅛2 XhMj㈤-V XhMjH
j	j∈A	σ j∈A
(1) D	α(1)βγ
% j - -^―
j=1	j∈A
D
"(1)I X Tj φj- β X φj+HI)
j=1	j∈A
We have 卜(I) - a(O)I= O(η(1)σw ∕log(Dm∕δ)), and
D
X Tj φj- ~σ X φj
j=1	j∈A
+ £ Tj φj
jeA
≤ O(kewi∕σ) + O(DeW2∕σ)=: eφ.
For the given values of parameters, we have
Therefore,
	/㈤-J XhMjH	
	σ j∈A	
< I ∕i)∣, * I /d 〃(o)| kγ ≤ I ail I eΦ+1 am - ail I 辞 2		2
≤	O k + η(1)σw √log(Dm∕δ)	T +	
	Z .		、八	mσ
+ O (η(I)σw √log(Dm∕δ)J 百		
≤ O (—
m m
二 十 ɪ
m2 σ mσ2
We also have by Lemma 18 and 21:
固2)— b(O)I ≤ o (η⑴ ∣α(? Iee + |«(1)| Qexp(-Θ(⅛)) + 屯))≤ o g
(225)
(226)
(227)
(228)
(229)
(230)
(231)
(232)
(233)
(234)
(235)
(236)
(237)
(238)
(239)
(240)
(241)
34
Published as a conference paper at ICLR 2022
Now, construct a such that 瓯
2碇找|
lb(0)lnɑ
for each ' and each i` ∈ l`, and <⅛ = 0 elsewhere. Then
|g(x) - 2g*(x)∣
3(k+1)	3(k+1)
X X 瓯 σ(h*,xi 十姆)-X 2aeσ (hw'H + be)
'=1 i'∈I'	'=1
(242)
(243)
≤
3(k+1)	ɔ *|入*|	/	、	3(k+1)
X iXe * 5 ㈤+bi2')-X i'
2a加匚
喈I
(244)
3(XI) X ɪ I 2a⅛bilσ (hw(2)㈤ + b(2
⅛1 j [ IbiO)I	「一 b

σ
+
3X1) X 1 /2磁 |be|	卜理％ X (M	\ , b(o)
二分元[Wr1 丁 AMj ㈤+bi'
—
2a却好|
IbiO)I
XhMj ㈤ + b(O)
j∈A
(247)
≤ 3(k +1)max2a≡ι O(-y +
`	1b£1	'm)
3(k + 1) max
2a；|b；| m(O)I 4ka(OeY也却
Tbfr WIT	σ2∣bi0)∣
-1
k
σa
(248)
O ( k4 + k2ea
m m
(249)
≤ 1.
(250)
Here the second equation follows from that σ is positive-homogeneous in [0,1], |M，x)+ b；| ≤ 1,
IbiO) |/|b； I ≤ 1. This guarantees yg(χ) ≥ 1. Changing the scaling of δ leads to the statement.
Finally, the bounds on a follow from the above calculation. The bound on Ila⑵心 follows from
Lemma 22, and those on Ilwy) ∣∣2 and ||b(2) ∣∣2 follow from (219)(220) and the bounds on ai1) and
b(I) in Lemma 18.	□
B.6 Classifier Learning Stage
Once we have a set of good features, we are now ready to prove that the later steps will learn an
accurate classifier. The intuition is that the first layer,s weights do not change much and the second
layer,s weights get updated till achieving good accuracy. In particular, we will employ the online
optimization technique from Daniely & Malach (2020).
We begin by showing that the first layer,s weights do not change too much.
Lemma 24. Assume the same conditions as in Lemma 23. Suppose for t > 2, λ!” = λW) = λ,
η⑴=η for some λ, η ∈ (0,1), and σξ(t) = 0. Thenfor any t > 2 and i ∈ [2m],
i^i≤ ηt+oQ⅛ )，	(251)
H)-Wa2 ≤ o (牛)+ η2t2+o (&)，	(252)
W)-b(2)∣≤ O (钓+ η2t2 + O (%).	(253)
35
Published as a conference paper at ICLR 2022
ProofofLemma 24. First, We bound the size of |a(t) |:
Iaf)I =(1 - 2ηλ)α(T)- η*Lo(g(t-1))	(254)
Oai
≤ I (1 - 2ηλ)α('T)- ηE(x,y)~D {yI[yg"T)(X) ≤ "σ((W(I),xi + b(tT))} ∣	(255)
≤∣α(T)I + η	(256)
Which leads to
|a(t)| ≤ ηt + |a(2)|	(257)
where |a(2)| = O (^^2). We are now to bound the change of Wf) and b(t).
kwT)- w(2)k2	(258)
=(1 - 2ηλ)w(tT)- η^0-Lp(g(tT))- W2	(259)
OWi	2
≤ (1 - 2ηλ)w(tT)	(260)
+ ηα(tT)E(X,y)~D {yI[yg"T)(X) ≤ 1]IKWfT),xi + b(I) ∈ (0, 1)]x} - w(2)	(261)
2
≤ ∣∣(1 - 2ηλ)w(tT)- w(2)∣∣2	(262)
+ η ∣∣α(T)E(X,y)~D {yI[yg(tT)(X) ≤ 1]IKWTT),xi + b(t-1) ∈ (0,1)]x} [	(263)
≤ (1 - 2ηλ) MT)-W(2)∣L + 2ηλ ∣∣W(2)∣∣2 + η |a(tT)I	(264)
leading to
kWT)-W(2)k2 ≤ 2tηλ ∣∣W(2)∣∣2 + η2t2 + t|a，2)|.	(265)
Note that |尾2)|[2 = O(σ∕k).
W)- b(2)| = b(T)- η卷LD(g(tT))- b(2)	(266)
≤∣b(tT)- b(2)|	(267)
+ η∣α(I)E(X,y)~D {yI[yg(tT)(X) ≤ 1]IKW(T),xi + b(T) ∈ (0,1)]}∣ (268)
≤ |b(tT)-b(2)|+ η∣α(T)I	(269)
leading to
W)- b(2)| ≤ η2t2 + t|a(2)|.	(270)
Note that ∣ bi2) ∣ = O(1∕k2).	口
Lemma 25. Assume the same conditions as in Lemma 24. Let g(t)(x) = PzI □iσ(hWf),xi + bit)).
Then
Mgat)(X),y)- '履2)(X),y)| ≤ IIGIl2P∣∣α∣∣0 (o ( n；) + η2t2 + o (km^)) .	(271)
36
Published as a conference paper at ICLR 2022
Proof of Lemma 25. It follows from that
∣'(gat)(χ),y) - '(ga2)(χ))∣	(272)
≤ ∣gat)(χ) - ga2)(χ)∣	(273)
≤ kak2pk训0 max ∣σ(hw(t),χi + b(t)) -σ(hw(2),χi + b(2))1	(274)
i∈[2m]
≤ kak2pwo .maχ1 (∣hw(t)- w(2) ,xi∣ + Wt)- b(2)∣).	(275)
and Lemma 24.	□
B.7 Proof of Theorem 1
Based on the above lemmas, following the same argument as in the proof of Theorem 2 in Daniely &
Malach (2020), we get our main theorem.
Theorem 26 (Full version of Theorem 1). Set
22
η⑴=-j-^,λa) = 0,λW) = 1/(2n⑴),σξ = 1∕k2,	(276)
km3	a	w	ξ
η⑵=1, λa2) = λW2) = 1∕(2η(2)), σξ2) = 1∕k2,	(277)
k2	k3
η⑴=η =	1/3,M)= λW = λ ≤ T-1/3,σξ = 0, for2 < t ≤ T. (278)
T m1∕3	σm1∕3	ξ
For any δ ∈ (0,1), if P0 = Ω(k2∕D), k = Ω (log2 (δγ))，max{Ω(k4), D} ≤ m ≤ Poly(D), then
we have for any D ∈ FΞ, with probability at least 1 - δ, there exists t ∈ [T] such that
Pr[sign(g⑴(x)) = y] ≤ LD (g㈤) = O (-^ki3 + k-T + k m / ) .	(279)
m2/3	m2	T
Consequently, for any E ∈ (0,1), if T = m4/3, and max{Ω(k12∕e3/2), D} ≤ m ≤ Poly(D), then
Pr[sign(g(t) (x)) 6= y] ≤ LD (g(t)) ≤ E.
(280)
ProofofTheorem 1. Consider LD(g(t)) = E['(g(t),y)] + 1!”||。(叫|2. Note that the gradient update
using LD (g(t)) is the same as the update in our learning algorithm. Then by Theorem 27, Lemma 23,
and Lemma 25,
T XLD(g(t)) ≤ 续 + kak2PW (o (Tηλσ) + η2τ2 + o (km
+ kaki + I∣α(2)k2√m + ηm
2ηT
k9	4 2 2	k3T	k9
≤ o (m + k4η2τ2 + m^ + ητm+ηm).
(k8	k3T	k2m2/3)
≤ O[mτ3 + 犷 + ~^).
The statement follows from that 0-1 classification error is bounded by the hinge-loss.
(281)
(282)
(283)
(284)
□
Theorem 27 (Theorem 13 in Daniely & Malach (2020)). Fix some η, and let f1 , . . . , fT be some
sequence of convexfunctions. Fix some θι, and assume we update θt+ι = θt 一 ηVft(θt). Thenfor
every θ* the following holds:
1T	1T	1	1T	1T
不∑ft(θt) ≤ 万 ∑ft(θ*) + 卡kθ*k2 + kθ1k2¥ E kVft(θt)k2 + η-E IVft(θt)k2.
T t=1	T t=1	2ηT	2	T t=1	T t=1	2
(285)
37
Published as a conference paper at ICLR 2022
C	Lower Bound for Linear Models on Fixed Feature Mappings
Theorem 28	(Restatement of Theorem 2). Suppose Ψ is a data-independent feature mapping of
dimension N with bounded features, i.e., Ψ : X → [-1, 1]N. Define for B > 0:
HB = {h(X) : h(X) = hΨ(X),w), kw∣∣2 ≤ B}.
(286)
Then, if 3 < k ≤ D/16 and k is odd, then there exists D ∈ FΞ such that all h ∈ HB have hinge-loss
at least Po (1 ——VzmkB
Proof of Theorem 2. We first show that FΞ contains some distributions that are essentially sparse
parity learning problems, and then we invoke the lower bound result from existing work for such
problems.
Consider D defined as follows.
•	Let P = {i ∈ [k] : i is odd}. That is, if there are odd numbers of 1's in Φa, then y = +1.
•	Let D(0 be a distribution where all entries φj are i.i.d. with Pr[φj = 0] = Pr[φj = 1]
1/2. Let D(IO) be the distribution over (X, y) induced by D：，and the above P.
•	Let Dφ1) be a distribution where all entries φj for j ∈ A are i.i.d. with Pr[φj = 1]
Po∕(2 ——2p0), while Pr[φa = (0,0,..., 0)] = Pr[φa = (1,1,..., 1)] = 1/2. Let D⑴ be
the distribution over (X, y) induced by Dφ1) and the above P.
•	Let DAmix = poD(0) + (1 —— po)D(1).
It can be verified that such distributions are included in FΞ for γ = Θ(1).
Assume for contradiction that for all D ∈ Fξ, there exists h ∈ HB such that h = (Ψ, w*)loss
smaller than po 1 ——
. Then for all the distributions DAmix defined above, we have
Ed(0) ['(h*(X),y)] < 1 - ^2k^
(287)
Now let Dz be a distribution over z ∈ {——1, +1}D with i.i.d. entries zj and Pr[zj = ——1] = Pr[zj =
+1] = 1/2. Let fA(z) = Qj∈A zj be the k-sparse parity functions. Let Ψ0(z) = Ψ(M (z + 1)/2).
Then we have h0(z) = hΨ0(z), w*i such that for all A,
EDz['(h0(z),fA(z))] < 1 - ^NB
(288)
This is contradictory to Theorem 29.
The following theorem is implicit in the proof in Theorem 1 in Daniely & Malach (2020).
Theorem 29.	For a subset A ⊆ [D] of size k, let the distribution DA over (z, y) defined as follows:
z is uniform over {±1}D and y =	i∈A zi. Fix some Ψ : {±1}D → [——1, +1]N, and define:
HΨB = {z → hΨ(z), wi : kwk2 ≤ B}.
If k is odd and k ≤ D/16, then there exists some A such that
mψ EDA['(h(Z),y)] ≥1 - F
Ψ
We now prove the corollary.
38
Published as a conference paper at ICLR 2022
Corollary 30 (Restatement of Corollary 3). For any function f using a shift-invariant kernel K
with RKHS norm bounded by L, or f (x) = Ei α%K(zi, x) forsome data points Zi and ∣∣ɑ∣∣2 ≤ L.
If 3 < k ≤ D/16 and k is odd, then there exists D ∈ FΞ such that f have hinge-loss at least
po(1
poly(d,L))_________1
2k	)	poly(d,L).
Proof. By Claim 1 in Rahimi & Recht (2008), for any ν > 0, there exists N = poly(d, 1/v)
Fourier features Ψj that can approximate the shift-invariant kernel up to error ν. For any > 0,
consider Pi α“ψ(zi), Φ(x)i = hpi αiΨ(zi), Ψ(x)i. Let W = Pi αiΨ(zi) and let V = O(^L),
then hΨ(x), wi approximates f(x) upto error and N = poly(d, L, 1/) and the norm of w bounded
by B = poly(d, L, 1/e). The reasoning is the same for f in the RKHS form, replacing sum with
integral. By Theorem 2, (Ψ(x), w〉has hinge-loss at leastpo(1 -吗NB). Thus, the function f has
loss at least po(1 - poly(d,L,1/')) - J Choose C = 口四(^ L), we get the bound.	口
D Lower B ound for Learning without Input S tructure
First recall the Statistical Query model (Kearns, 1998). In this model, the learning algorithm can
only receive information about the data through statistical queries. A statistical query is specified
by some property predicate Q of labeled instances, and a tolerance parameter τ ∈ [0, 1]. When
the algorithm asks a statistical query (Q,τ), it receives a response PQ ∈ [Pq - τ, Pq + τ], where
PQ = Pr[Q(x, y) is true]. Q is also required to be polynomially computable, i.e., for any (x, y)
Q(x, y) can be computed in polynomial time. Notice that a statistical query can be simulated by
empirical average of a large random sample of data of size roughly O(1∕τ2) to assure the tolerance τ
with high probability.
Blum et al. (1994) introduces the notion of Statistical Query dimension, which is convenient for our
purpose.
Definition 31 (Definition 2 in Blum et al. (1994)). For concept class C and distribution D, the
statistical query dimension SQ-DIM(C, D) is the largest number d such that C contains d concepts
c1 , . . . , cd that are nearly pairwise uncorrelated: specifically, for all i 6= j,
| Pr [ci(x) = cj(x)] - Pr [ci(x) 6= cj(x)]| ≤ 1/d3.
x~D	x~D
(289)
Theorem 32 (Theorem 12 in Blum et al. (1994)). In order to learn C to error less than 1/2 - 1/d3
in the Statistical Query model, where d = SQ-DIM(C, D), either the number of queries or 1 /τ must
be at least 2 d1/3.
We now use the above tools to prove our lower bound.
Theorem 33 (Restatement of Theorem 4). For any algorithm in the Statistical Query model that can
learn over 尸日。to error less than 2 — ʧ1-3, either the number of queries or 1∕τ must be at least
(k)
1	(D )1/3.
Proof of Theorem 4. Consider the following concept class and marginal distribution:
~ ~
~
•	Let D be the distribution over x, given by x = Mφ and φj are i.i.d. with Pr[φj = 0]
-	~	r	,
Pr[φj = 1] = 1/2.
~
~
•	Let C be the class of functions y = ga(φ) = I[£ j (1 - φj) is odd] for different A ⊆ [D].
The distributions over (X, y) induced by (C, D) are a subset of Fξ0 . It is then sufficient to show that
SQ-DIM(C, D) ≥ (D).
~

It is easy to see that C are essentially the sparse parity functions: if zj = 2φj - 1, then gA(φ) =
Qj∈A zj. This then implies that the gAs are uncorrelated, so SQ-DIM(C, D) ≥ (D).	口
39
Published as a conference paper at ICLR 2022
E Complete Experimental Results
Our experiments mainly focus on feature learning and the effect of the input structure. We first
perform simulations on our learning problems to (1) verify our main theorems on the benefit of
feature learning and the effect of input structure (2) verify our analysis of feature learning in networks.
We then check if our insights carry over to real data: (3) whether similar feature learning is presented
in real network/data; (4) whether damaging the input structure lowers the performance. The results
are consistent with our analysis and provide positive support for the theory.
The experiments were ran 5 times with different random seeds, and the average results (accuracy) are
reported. The standard deviations of the results are smaller than 0.5% and thus we do not present
them for clarity. The hardware specifications are 4 Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz,
16 GB RAM, and one NVIDIA GPU GTX1080.
E.1 Simulation
We train a two-layer network following our learning process. We use two fixed feature methods: the
NTK (Fang et al., 2021) and random feature (RF) methods based on the same network and random
initialization as the network learning. More precisely, in the NTK method, we randomly initialize the
network and take its NTK and learn a classifier on it. In the RF method, we freeze the first layer of
the network, and train the second layer (on the random features given by the frozen neurons). The
training step number is the same as that in network learning. We also test these three methods on the
data distribution with input structure removed (i.e., FΞ0 in Theorem 4). For comparison, we take
the representation of our two-layer network at step one/step two, named One Step/Two Step (fix the
weight of the 1st layer after the first step/second step to train the weight of the second layer), and
train the best classifiers on top of them.
Recall that our analysis is on the directions of the weights without considering their scaling, and thus
it is important to choose cosine similarity rather than the typical `2 distance. Thus, we use metric Cos
Similarity max{i∈[2m]} cos(wi, Pj∈A Mj) in our tables, and use Multidimensional Scaling to plot
the weights distribution. The simulation dataset size is 50000. During training, the batch size is 1000,
while for the first two steps we use the approximate full gradient (batch size is 50000). Each step is
corresponding to one weights update.
E.1.1 Parity Labeling
Setting. We generate data according to the parity function data distributions used in our proof of
the lower bound for fixed features (Theorem 2), with d = 500, D = 100, k = 5, po = 1/2, with a
randomly sampled A. More precisely, we consider D defined as follows.
•	Let P = {i ∈ [k] : i is odd}. That is, if there are odd numbers of 1’s in φA, then y = +1.
•	Let D(0 be a distribution where all entries φj are i.i.d. with Pr[φj = 0] = Pr[φj = 1]=
1/2. Let D⑼ be the distribution over (X, y) induced by D(0) and the above P.
•	Let D? be a distribution where all entries φj for j ∈ A are i.i.d. with Pr[φj = 1]=
Po∕(2 - 2po), while Pr[φa = (0,0,..., 0)] = Pr[φa = (1,1,..., 1)] = 1/2. Let D⑴ be
the distribution over (X, y) induced by D(I and the above P.
•	Let DAmix = poD(0) + (1 - po)D(1).
The network and the training follow Section 3, where the network size is m = 300 and the training
time T = 600 steps.
Verification of the Main Results. Figure 5 shows that the results are consistent with our analysis.
Network learning gets high test accuracy while the two fixed feature methods get significantly lower
accuracy. Furthermore, when the input structure is removed, all three methods get test accuracy
similar to random guessing.
40
Published as a conference paper at ICLR 2022
Figure 5: Test accuracy on simulated data under parity labeling with or without input structure.
Figure 6: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps in
network learning under parity labeling. The red star denotes the ground-truth Pj∈A Mj ; the orange
star is - Pj∈A Mj. The red dots are the weights closest to the red star after two steps; the orange
ones are for the orange star.
Model	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc (%)	-100.0-	84.0	74.7	-513-	100.0	100.0
Test Acc (%)	100.0	86.4	76.0	52.2	100.0	52.0
Cos Similarity	0.997	NA	0.114	0.848	0.997	0.253
Table 1: Parity labeling results in six methods. The cosine similarity is computed between the
ground-truth Pj∈A Mj and the closest neuron weight.
Feature Learning in Networks. Figure 6 shows that the results are as predicted by our analysis.
After the first gradient step, some weights begin to cluster around the ground-truth Pj∈A Mj (or
- Pj∈A Mj due to we have ai in the gradient update which can be positive or negative). After the
second step the weights get improved and well-aligned with the ground-truth (with cosine similarity
> 0.99).
Table 1 shows the results for different methods. Recall that the Cos Similarity metric is
max{i∈[2m]} cos(wi, Pj∈A Mj), which reports the cosine value of the closest one. One Step refers
to the method where we take the neurons after one gradient step, freeze their weights, and train a
classifier on top; similar for Two Step. One Step gets test accuracy about 52%, while Two Step gets
accuracy about 100%. This demonstrates that while some effective feature emerge in the first step,
they need to be improved in the second step for accurate prediction. NTK, random feature, One Step
all failed, while Network and Two Step can achieve 100% test accuracy. Network w/o structure refers
to training the network on data without the input structure. It overfits the training dataset with 52%
test accuracy.
41
Published as a conference paper at ICLR 2022
E.1.2 Interval Labeling
Figure 7: Test accuracy on simulated data under interval labeling with or without input structure.
Figure 8: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps
in network learning under interval labeling. The red star denotes the ground-truth Pj∈A Mj ; the
orange star is - Pj∈A Mj. The red dots are the weights closest to the red star after two steps; the
orange ones are for the orange star.
Model	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc (%)	-100.0-	100.0	76.4	-441-	100.0	100.0
Test Acc (%)	100.0	100.0	73.2	41.0	100.0	100.0
Cos Similarity	1.00	NA	0.153	0.901	0.994	0.965
Table 2: Interval labeling results in six methods.
Setting. We also tried interval function, where y = 1 if	i∈A φi is in the range [t1, t2] with t1 = 20
and t2 = 30, otherwise y = -1. We use d = 500, D = 100, k = 30. The φi S are independent, and
Pr[φi = 1] = 2/3 for any i ∈ A, and Pr[φi = 1] = 1/2 otherwise. When the input structure is
1	j ɪʌ Γ 7	r 7 r /c c 11 ∙5
removed, we set Pr[φi = 1] = 1/2 for all i’s.
The network and training again follows Section 3 with a network size m = 100 and the training time
T = 200 steps.
Verification of the Main Results. Figure 7 shows that network learning learns the fastest, NTK
learns slower but reaches similar test accuracy, while random feature can only reach a decent but
lower accuracy. This is because for such simpler labeling functions, fixed feature methods can still
achieve good performance (note that the lower bound does not hold for such a case), while the
performance depends on what fixed features to use.
Furthermore, when the input structure is removed, the methods still get similar (or only slightly
worse) performance as with input structure. This shows that when the labeling function is simple, the
42
Published as a conference paper at ICLR 2022
help of the input structure for learning may not be needed. In the experiments on real data, we will
show that when the input structure is changed, it indeed leads to lower performance which suggests
that the labeling function in practice is typically more complicated than this interval labeling setting,
and the help of the input structure is significant for learning.
Feature Learning in Networks. Figure 8 shows the phenomenon of feature learning similar to that in
the parity labeling setting. Table 2 shows the test accuracy of six different methods. Random feature
and One Step failed, while Network, NTK and Two Step succeed showing that interval labeling
setting is a simpler case than parity labeling setting.
E.2 More Simulation Result in Various Settings
We show the robustness of our simulation results by studying the learning behaviors in a variety of
settings including different sample size, input data dimension and class imbalance. We reuse the
same setting as the simulation in the main text (details in E.1.1), vary different parameters, and report
the accuracy, the cosine similarities between the learned weights, and the visualization of the neuron
weights.
E.2. 1 Varying Input Data Dimension
In the simulation experiments in the main text, the input data dimension d is 500. Here we change
the input data dimension to 100 and 2000. All other configurations follow E.1.1.
Verification of the Main Results. Figure 9 shows that our claim is robust under different input
data dimensions. The performance of network learning is superior over NTK and random feature
approaches on inputs with structure, and on inputs without structure, all three methods fail.
(a) d = 100
(b) d = 2000
Figure 9: Test accuracy on simulated data under different input data dimensions.
d= 100	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	100.0^^	83.1	78.9	-^530^^	100.0	100.0
Test Acc	100.0	81.5	78.3	51.1	100.0	51.0
Cos Similarity	1.000	NA	0.354	0.967	1.000	0.331
d=2000	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	100.0^^	75.6	80.0	-^50.22^^	100.0	100.0
Test Acc	100.0	75.4	77.0	50.01	100.0	52.5
Cos Similarity	0.998	NA	0.056	0.560	0.998	0.309
Table 3: Results of six methods for different input data dimensions. The cosine similarity is computed
between the ground-truth Pj∈A Mj and the closest neuron weight.
Feature Learning in Networks. Figure 10 visualizes the neuron weights. It shows similar results to
that in E.1.1: the weights gets updated to to the effective feature in the first two steps, forming clusters.
43
Published as a conference paper at ICLR 2022
Figure 10: Visualization of the weights wi ’s in early steps under different input data dimensions.
Upper row: input data dimension d = 100; lower row: d = 2000.
Table 3 shows some quantitative results. In particular, the average cosine similarities between neuron
weights and the effective features after two steps are close to 1, showing that they match the effective
features.
E.2.2 Varying Class Imbalance Ratio
The experiments in the main text has 25000 training samples for each class. Here we keep the total
sample size 50000 but use different class imbalance ratios, which is the class -1 sample size divide
by the total sample size.
Verification of the Main Results. Figure 11 shows that our claim is robust under different class
imbalance ratios. The results are similar to those for balanced classes, except that NTK becomes less
stable.
(a) Negative class ratio = 0.8
Figure 11: Test accuracy on simulated data under different negative class ratios.
(b) Negative class ratio = 0.9
Feature Learning in Networks. Figure 12 visualizes the neurons’ weights. Again, the observation
is similar to that for balanced classes. Table 4 shows some quantitative results which are also similar
to those for balanced classes.
44
Published as a conference paper at ICLR 2022
Figure 12: Visualization of the weights wi ’s in early steps under different class imbalance ratios.
Upper row: negative class ratio 0.8; lower row: 0.9.
ratio = 0.8	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	-100.0-	62.9	72.7	-783-	100.0	100.0
Test Acc	100.0	82.7	70.4	75.7	100.0	61.7
Cos Similarity	0.999	NA	0.293	0.950	0.999	0.218
ratio = 0.9	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	-1W.0-	84.0	73.6	-923-	100.0	100.0
Test Acc	100.0	81.7	72.4	89.2	100.0	71.8
Cos Similarity	0.997	NA	0.296	0.956	0.997	0.286
Table 4: Results of six methods under different negative class ratios.
E.2.3 Varying Sample Size
Here we change the sample size 50000 in Section E.1.1 to be 25000 and 10000. For sample size
25000, we observe similar results. For sample size 10000, we observe over-fitting (test accuracy
much lower than train accuracy). Therefore, for sample size 10000 we reduces the size of the network
(i.e., number of hidden neurons) from m = 300 to m = 50.
Verification of the Main Results. Figure 13 shows that our claim is robust under different sample
sizes. In particular, the network learning still outperforms the NTK and random feature approaches
on structured inputs.
n = 25000	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	-1W.0-	84.0	78.6	-50.6-	100.0	100
Test Acc	100.0	84.1	74.7	50.0	100.0	50.2
Cos Similarity	0.997	NA	0.105	0.851	0.997	0.230
n= 10000	Network	NTK	RF	One Step	Two Step	Network w/o structure
Train Acc	-1W.0-	73.9	71.6	-50.7-	100.0	100.0
Test Acc	100.0	75.0	74.3	50.3	100.0	52.2
Cos Similarity	0.995	NA	0.096	0.974	0.994	0.176
Table 5: Results of six methods for different sample size.
Feature Learning in Networks. Figure 14 and Table 5 show that the phenomenon of feature learning
for different samples is similar to that in E.1.1.
45
Published as a conference paper at ICLR 2022
(a) n = 25000
(b) n = 10000
Figure 13: Test accuracy on simulated data under different sample sizes n.
Figure 14: Visualization of the weights wi’s in early steps under different sample sizes. Upper row:
sample size 25000; lower row: 10000.
E.3 Experiments on More Data Generation Models
In this section we consider some additional data distributions and run the simulation experiments,
in particular, focusing on the feature learning phenomenon. Note that our analysis is for the setting
where the input distributions have structure revealing some information about the labeling function.
(More precisely, the labeling function is specified by A and P , while the input distribution also
depends on them.) We therefore consider two other data generation mechanisms where the labeling
function also has connections to the input distributions.
E.3.1 Hidden Representation Labeling
Here we consider the following data model: first uniformly at random select φA from a set of binary
vectors, and assign label 1 to some and -1 to others; sample irrelevant patterns φ-A uniformly at
random; generate the input x = Mφ. We randomly select 50 binary vectors for each label, with
d = 500, D = 250, k = 50, po = 1/2.
This is a generalization of the distribution D(1), a component in the distribution of our simulation
experiments (See the proof of Theorem 2 for details). Recall the definition of D(I): Φa is uniform
on only two values [+1, . . . , +1] and [0, . . . , 0], and uniform over irrelevant patterns; the value
46
Published as a conference paper at ICLR 2022
[+1, . . . , +1] corresponds to one class and [0, . . . , 0] correspond to another class. Our data model
here generalizes D(1) to more than 2 values.
The visualization is shown in Figure 15. We can observe similar feature learning phenomena, and the
neuron weights are updated to form clusters.
Figure 15: Visualization of the weights wi ’s after initialization/one gradient step/two gradient steps
in network learning under hidden representation labeling.
E.3.2 Two-layer Networks on Mixture of Gaussians
To further support our intuition of feature learning, we run experiments on mixture of Gaussians.
Data. Let X = Rd be the input space, and Y = {±1} be the label space. Suppose M ∈ Rd×k is
an dictionary with k orthonormal columns. Let εi, i = 1, . . . , k be i.i.d symmetric Bernoulli random
variables, and g 〜N(0, σ2号Id). Then We generate the input X and class label y by:
kk
X = X EiM：i + g, y = Y ε	(290)
i=1	i=1
In this case, 2k Gaussian clusters Will be created. The centers of the Gaussian clusters Pik=1 ±M:i
lie on the vertices of a hyper cube, and the label of each Gaussian cluster is determined by the parity
function on the vertices of the hyper cube.
Note that the labeling function is roughly equivalent to a netWork: y = Pin=1 aiReLU(hci, Xi) Where
Ci,s are the Gaussian centers, and a, Y 1 for Gaussian components with label 1 and a, ɑ -1 for
those With label -1.
Setting. We then train a two-layer network with m = 800 hidden neurons on data sets generated as
above with different chosen k’s and d’s. The training follows typical practice (not the hyperparameters
in our analysis). In this setting, we expect the neural network to learn the effective features: the
directions of Gaussian cluster centers.
Result. We run experiments with different settings. The parameters are shown in Table 6. From
Figure 16 we can see that some neurons learn the directions of Gaussian centers, and each Gaussian
center is covered by some neurons, which matches our expectation.
Parameters	d	k	Number of Clusters	σr
Experiment 1	l00^"	4	16	1
Experiment 2	25	4	16	0.7
Experiment 3	100	5	32	1
Table 6: Gaussian mixture setting.
E.4 Real Data: Feature Learning in Networks
We take the subset of MNIST (Deng, 2012) with labels 0/1, CIFAR10 (Krizhevsky, 2012) with labels
airplane/automobile and SVHN (Netzer et al., 2011) with labels 0/1, and train a two-layer network
47
Published as a conference paper at ICLR 2022
(c) Experiment 3 with epoch 0/50/80
Figure 16: Visualization of the weights wi’s (blue dots) and Gaussian centers (red for positive labeled
clusters and orange for negative labeled clusters).
(a) Experiment 1 with epoch 0/50/80
(b) Experiment 2 with epoch 0/30/50
Figure 17: Visualization of the neurons’ weights in a two-layer network trained on the subset of
MNIST data with label 0/1. The weights gradually form two clusters.
with m = 50. We use traditional weight initialization method (random Gaussian) and training method
(SGD with momentum = 0.95 without regularization) in this section, for our purpose of investigating
the training dynamics in practice.
Then we visualize the neurons’ weights following the same method in the simulation. Figure 17,
Figure 18 and Figure 19 show a similar feature learning phenomenon: effective features emerge after
a few steps, and then get improved to form clusters. This shows the insights obtained on our learning
problems are also applicable to the real data.
48
Published as a conference paper at ICLR 2022
Figure 18: Visualization of the neurons’ weights in a two-layer network trained on the subset of
CIFAR10 data with label airplane/automobile. The weights gradually form two clusters.
Figure 19: Visualization of the neurons’ weights in a two-layer network trained on the subset of
SVHN data with label 0/1. The weights gradually form four clusters.
	cos(v1, v)	cos(v2 , vv)	Cos(V3, v)	Cos(v1, v2)	Cos(v1,v3)	Cos(v2, v3)
ResNet(128)	0.9727-	0.8655	-06549-	0.7454	0.5083	0.6533
ResNet(256)	08646	0.9665	0.9121	0.7087	0.6919	0.9135
Table 7: Cosine similarities between the gradients in the early steps. We choose the neuron weight clos-
est to the average weight of the green cluster at the end of the training (in Figure 20 for ResNet(128)
and Figure 21 for ResNet(256)). We record the gradients of the first 30 steps and divide them to
three trunks of 10 steps evenly and sequentially. For the three trunks, we get the average gradients
vι,v2,v3. We calculate their cosine similarities to their average v = (vι + v2 + v3)/3 and those
between them.
E.4. 1 CNNs on B inary Cifar10: Feature Learning in Networks
Setting. We use ResNet(m), which is a ResNet-18 convolutional neural network (He et al., 2016)
with m filters in the first residual block. It is obtained by scaling the number of filters in each block
proportionally from the standard ResNet-18 network which is ResNet(64). We use ResNet(128)
and ResNet(256) in this experiment. We train our model on Binary CIFAR10 (Krizhevsky, 2012)
with labels airplane/automobile for 20 epochs. The final test accuracy of ResNet(128) is 95.75%
and that of ResNet(256) is 93.8%.
Results. Figure 20 visualizes the filters’ weights of different residual blocks in ResNet(128) at
Epoch 0, 3, and 20, and Figure 21 shows those in ResNet(256). They show that feature learning
happens in the early stage, and show that there are some clusters of weights (e.g., the red and green
points). These colored points are selected at Epoch 20. We first visualize the weights at Epoch 20,
and then hand pick the points that roughly form two clusters (i.e., the points in the same cluster are
close to each other while those in different clusters are far away). We assign red and green colors to
the two clusters at Epoch 20, and then assign these weights with the same color in Epoch 0 and 3.
Finally, we compute the cosine similarities and show that the hand picked points are indeed roughly
clusters in the high-dimension.
In particular, we have the following three observations.
49
Published as a conference paper at ICLR 2022
(a) Residual block 1: (Green: 0.6152, Red: 0.6973, Two Centers: -0.7245)
(b) Residual block 2: (Green: 0.5528, Red: 0.6000, Two Centers: -0.7509)
(c) Residual block 3: (Green: 0.4260, Red: 0.5006, Two Centers: -0.5099)
(d) Residual block 4: (Green: 0.5584, Red: 0.5697, Two Centers: -0.9074)
Figure 20: Visualization of the normalized convolution weights in all Residual block of ResNet(128)
trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after
0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
We also report average cosine similarity between the green/red points in the clusters to their centers
and cosine similarity between two cluster centers as (Green, Red, Two Centers).
First, we can see that the filter weights change significantly during the early stage of the training,
indicating feature learning happens in the early stage: the change between Epoch 0 and Epoch 3 is
much more significant than that between Epoch 3 and Epoch 20.
Second, we can also verify that the feature learning is guided by the gradients: the gradients of a
filter in the early gradient steps point to similar directions (and thus the updated filter will learn this
direction). More precisely, for a selected filter, we average the gradients every 10 gradient steps (so to
reduce the variance due to mini-batch), and get v1, v2 and v3 for the first 30 steps and compute their
cosine similarities and those to their average. Table 7 shows the results. In general the similarities
50
Published as a conference paper at ICLR 2022

Epoch 0
Epoch 20
(a) Residual block 1: (Green: 0.7065, Red: 0.6551, Two Centers: -0.7875)
(b) Residual block 2: (Green: 0.6599, Red: 0.5299, Two Centers: -0.9004)
(c) Residual block 3: (Green: 0.5193, Red: 0.6267, Two Centers: -0.9258)
Epoch 0	Epoch 3	Epoch 20
(d) Residual block 4: (Green: 0.7386, Red: 0.6839, Two Centers: -0.9740)
Figure 21: Visualization of the normalized convolution weights in all Residual block of ResNet(256)
trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after
0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
We also report average cosine similarity between the green/red points in the clusters to their centers
and cosine similarity between two cluster centers as (Green, Red, Two Centers).
are high indicating they point to similar directions. (Note that a similarity of 0.6 is regarded as very
significant as the filters are in a high dimension of 3 × 3 × 1024 = 9216).
Third, we also observe some clustering effect of the filter weights, though not as significant as in our
simulations. For example, in the red and green clusters in Figure 20(a) for the first residual block, the
average cosine similarity for filter weights in the red cluster is about 0.62 and that for the green is
about 0.7, while the cosine similarity between the two clusters’ centers is about -0.72. This shows
significant similarities within the cluster while difference between clusters.
51
Published as a conference paper at ICLR 2022
Note that the clustering is less significant than our simulation experiments. This is because practical
data have more patterns (i.e., effective feature directions) to be learned than our synthetic data, and
also the practical network is not as overparameterized as in our simulation. Then filters are likely
to learn different patterns (or their mixtures) without forming significant clusters. The results of
ResNet(256) show more significant clustering than ResNet(128), which supports our explanation.
On the other hand, we emphasize that the key insight of our analysis is that the gradient guides
the learning of effective features in the early stage of training (rather than the clustering), which is
verified as discussed above.
E.5 Real Data: The Effect of Input Structure
To study the influence of the input structure, we propose to keep the labeling function unchanged,
vary the input distributions, and exam the change of the loss surface and the training dynamics. We
first describe the detailed experimental methodology, which allows us to generate data with similar
labeling function but different input distributions. Then we perform experiments on the generated
datasets to investigate the change of the learning due to the change in the input distributions, and
present the experimental results. Finally, we also perform experiments to verify the intuition behind
our experimental method.
E.5.1 Experimental Methodology
We consider the following experimental method. Given an original dataset L = {(xi , yi)}in=1 (e.g.,
CIFAR10) and an unlabeled dataset U = {Xi}m=ι from a proposed distribution PU (e.g., Gaussians),
first extend the labeling function of L to U, giving synthetic labels yi to Xi. Then train a neural
network on the union of L and the synthetic data LU = {(Xi, yi)}im=ι. By investigating the new
training dynamics, in particular the difference on the original part L and the synthetic part LU , we
can see the effect of the input structure. The original dataset should be from real-world data, since
one of our goals is to compare them with synthetic data, and identify the properties of real-world data
important for the success of learning.
A natural idea is to first learn a powerful network f (x) (called the teacher) on L to approximate
the true labeling function, then apply f on U to generate synthetic labels, and finally train another
network (called the student) on the synthetic data and original data. However, we found that naively
implementation of this idea fails miserably: the support of L and U can be typically different, and the
powerful network learned over L can have entirely different behavior on U . Therefore, we need to
control the size of the teacher f so that the labeling on U has similar complexity as that on L. For
our purpose, we can define the complexity of the labeling on L as the minimum size of the teacher
achieving an approximation error for a chosen , if the ground-truth data distribution of L is known.
However, given only limited data, we cannot faithfully estimate the needed size of the teacher, and
need to take into account the variance introduced by the finite data.
Our key idea is to use the U-shaped curve of the bias-variance trade-off and select the size of the
teacher at the minimum of the U-shaped curve. Since recent works (Belkin et al., 2019; Nakkiran
et al., 2020) show that neural networks can have a double descent curve for the error v.s. model
complexity, we thus plot the double descent curve, and find the minimum in the classical regime
(corresponding to the traditional U-shape curve).
Our method is designed based on the following two reasons. First, on the U-shaped curve, the
complexity of the network is still roughly controlled by that of the number of parameters. The
local minimum of the U-shaped curve is a good measurement of the complexity of the data. If the
ground-truth is much more complicated than the teacher, then increasing the teacher’s size leads to a
significant decrease in the approximation error (bias) compared to a small increase in the variance,
that is, we will be on the left-hand side of the U-shaped. In contrast, on the right-hand side of the
U-shaped, increasing the teacher’s size leads to a small decrease in the bias compared to a significant
increase in the variance. That is, the complexity of the ground-truth is comparable to or lower than
the teacher. So the local minimum approximates the complexity of the ground-truth labeling function.
Second, the local minimum point is chosen to get the best approximation of the true labels. This
helps to maintain the labeling from the real-world data and thus helps our investigation on the input,
since too drastic change in the labeling can affect the training.
52
Published as a conference paper at ICLR 2022
We note that the method is not perfect. First, the teacher at the local minimum of U-shape may not
have very high accuracy, especially on more complicated data. To alleviate this, we also use the
teacher to give synthetic labels yi0 to xi in L, and train the student network on L0 = {(xi, yi0)}in=1.
Though this introduces some differences from the original labels, it is acceptable for our purpose of
studying the inputs. Furthermore, ensuring the consistency of the labels on the original input in L and
U is important in our experiments. Second, the measurement is an approximation due to variance.
Since only limited labeled data is available, it’s important and necessary to calibrate the measurement
w.r.t. the level of variance on the given dataset.
Method Description. Algorithm 1 presents the details. For a fixed network architecture for the
teacher f, it first varies the network size and plots the double descent curve. Then it selects the
local minimum in the classic regime of U-shape and trains the teacher with the corresponding size.
In practice, we observed that the teacher might have unbalanced probabilities for different classes
on U if its training does not take into account U . Therefore, we propose the following heuristic
regularization using x ∈ U, where λ is a regularization weight, and f (x) is the probabilities over
classes given by the teacher:
R(x) = R1(x) +λR2(x)	(291)
Rι(x)= X(fj ln Pifxj )	(292)
mm
R2(x) = - ;1 XXf(Xjln(f(xj)).	(293)
Here, R1(x) guarantees that each kind of label has the same average probability to be generated, and
R2(x) pushes the probability away from uniform to avoid the case that the class probabilities for
each data point converge to uniform.
Algorithm 1 Learning the teacher network to generate synthetic labels for studying the effect of the
input structure
Input: teacher architecture f, labeled dataset L = {(xi, yi)}n=ι, unlabeled dataset U = {Xi}m=ι.
Let i to be the size off,fito be the teacher of size i.
for i = 1to n do
Train fi on L and let li denote the test loss
end for
Plot li v.s. i, identify the classical regime, and the size it corresponding to the local minimum in
classical regime.
Train fit on L with a regularizer R(x) on U defined in (291).
Output: fit
E.5.2 Experimental Results
Network models. Here we use one-hidden-layer fully-connected networks with m hidden units and
quadratic activation functions. The network is denoted as FC(m). We use ResNet(m), which is a
ResNet-18 convolutional neural network (He et al., 2016) with m filters in the first residual block. It
is obtained by scaling the number of filters in each block proportionally from the standard ResNet-18
network which is ResNet(64).
Datasets. We use MNIST (Deng, 2012), CIFAR10 (Krizhevsky, 2012) and SVHN (Netzer et al.,
2011) as L, and use Gaussian and images in Tiny ImageNet (Le & Yang, 2015) as U. We generate
the mixture data, where the fraction of the unlabeled data is denoted as α.
Setup. We first use Algorithm 1 on the labeled data L and the unlabeled data U to get a synthetic
labeling function (the teacher network) and then use it to give synthetic labels on a mixture of inputs
from L and U. For MNIST, the teacher network learned is FC(9), where the number of the hidden
units is determined by Algorithm 1. See empirical verification in Figure 26. For CIFAR10 and SVHN,
the teacher networks are ResNet(5) and ResNet(2), respectively, as determined by our method.
The student network for MNIST is FC(9), and those for CIFAR10 and SVHN are ResNet(9) and
ResNet(8), respectively. Finally, we train the student networks on these new datasets with perturbed
input distributions.
53
Published as a conference paper at ICLR 2022
Figure 22: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs with data:
(a) MNIST, (b) CIFAR10, (c) SVHN.
(a)
(b)
Figure 23: Test accuracy at different steps for an equal mixture α = 0.5 of Tiny ImageNet inputs
with data: (a) CIFAR10, (b) SVHN.
(a) α = 0.25
(b) α = 0.50
(c) α = 0.75
Figure 24: Test accuracy at different steps for varying mixture α of Gaussian inputs with CIFAR10.
Figure 22 shows the results on an equal mixture of data and Gaussian. It presents the test accuracy
of the student on the original data part, the Gaussian part, and the whole mixture. For example,
for CIFAR10, the test accuracy on the whole mixture is lower than that of training on the original
CIFAR10, showing that the input structure indeed has a significant impact on the learning. Fur-
thermore, the network learns well over the CIFAR10 part (with accuracy similar to that on the
original data) but learns slower with worse accuracy on the Gaussian part. This suggests that the
CIFAR10 input structure is still helping the network to learn effective features. While the results on
MNIST+Gaussian do not show a significant trend (possibly because the tasks there are simpler), the
results on SVHN+Gaussian show similar significant trends as CIFAR10+Gaussian.
Figure 24 shows the results when we vary the fraction of the Gaussian data α. We observe that the
test accuracy curve on the original part and that on the synthetic part have roughly the same trend for
different α as before, further verifying our insights.
54
Published as a conference paper at ICLR 2022
Figure 23 shows the results when mixed with Tiny ImageNet data instead of Gaussians. It shows a
similar trend, while the performance on the Tiny ImageNet part is higher than that on the Gaussian
part. This suggests that compared to Gaussians, the Tiny ImageNet data has helpful input structures,
though not as helpful as that on the original data for learning the particular labeling.
E.5.3 Larger Network on MNIST for Checking The Effect of Input Structure
Here we perform the experiment on MNIST as in E.5.2, but for a network with m = 50 hidden
neurons rather than m = 9. Figure 25 shows similar results as those for m = 9: the learning on the
MNIST input part is faster and better than that on the Gaussian input part. The separation between
the two is actually more significant than that for m = 9. This then also supports our insight about the
effect of input structures.
Figure 25: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs with
MNIST, where m = 50.
E.5.4 Empirical Verification of Our Method
(a) Teacher’s double descent curve
(b) Student’s curve when Teacher=FC(9)
(c) Student’s curve when Teacher=FC(50) (d) Student’s curve when Teacher=FC(500)
Figure 26: Double descent curves of the students trained on data with synthetic labels (Loss v.s.
Parameter number).
55
Published as a conference paper at ICLR 2022
We also perform experiments to verify the intuition behind our methodology, i.e., the method gives a
synthetic labeling function with roughly the same complexity on the original inputs and the injected
inputs. We first use our method on MNIST and samples (of the same size as MNIST) from a Gaussian
to get the teacher FC(9); the double descent curve is in Figure 26(a). Then we train students on
the Gaussian data with synthetic labels from the teacher, and plot the double descent curve for the
students in Figure 26(b). The local minimums of the two U-shapes are roughly the same, matching
our reasoning. Then we also train larger teachers and plot the double descent curve for students on
Gaussian data. Figure 26(c) Teacher size 50. Figure 26(d) Teacher size 500. The local minimum of
the U-shape becomes larger when the teacher gets larger, again matching our reasoning.
56
Published as a conference paper at ICLR 2022
F Provab le Guarantees for Neural Networks in A More General
Setting
This section provides the analysis in a more general setting. We first describe the learning problems,
and then provide the proofs following similar intuitions as for the simpler settings in the main text.
F.1 Problem Setup
Let X = Rd be the input space, and Y = {±1} be the label space. Suppose M ∈ Rd×D is a
dictionary with D elements, where each element Mj can be regarded as a pattern. We assume quite
general incoherent dictionary:
(D) M is μ-incoherent, i.e., the columns of M are unit vectors, and for any i = j, ∣hMi, Mji∣ ≤
μ∕√d.
Note that the setting in the main text corresponds to μ = 0.
Let φ ∈ {0, 1}d be a hidden vector that indicates the presence of each pattern, and Dφ a distribution
for φ. Let A ⊆ [D] be a subset of size k corresponding to the class relevant patterns. Let P ⊆ [k].
We first sample φ from Dφ, and then generate the input X and the class label y from φ, A, P by:
C l 1	7 Ln
X = Mφ + ζ, y = ∫+1,	Wi∈a φi ∈ ,	(294)
,	-1, otherwise
where the Gaussian noise Z 〜N(0, σ^Id×d) is independent from φ. Note that the setting in the main
text corresponds to σζ = 0.
We allow general Dφ with the following assumptions:
(A1) The patterns in A are correlated with the labels: for any i ∈ A, for v ∈ {±1} let γv =
E[yφi∣y = v], then Y := (γ+ι + γ-ι)∕2 > 0.
(A2) The patterns outside A are independent of the patterns in A.
Note that we allow imbalanced classes. Let pmin := min(Pr[y = -1], Pr[y = +1]). If the classes
are balanced, then the assumption (A1) implies the assumption (A1) in the main text, so the setting
here is more general. (A2) is also more general, in particular, allowing dependence between irrelevant
patterns and non-identical distributions for them.
Let D(A, P, Dφ) denote the distribution on (X, y) corresponding to some A, P, and Dφ. Given
parameters Ξ = (d, D, k, γ,po, μ, σζ), the family Fξ of distributions for learning is the set of all
D(A, P, Dφ) with A ⊆ [D], P ⊆ [k], and Dφ satisfying the above assumptions.
One special case is the mixture of two Gaussians.
J -	1	1-t	Tl ʃ <	∙	1	1	1	. -I ∙ ∕' 7	1	Λ	1	. 1	♦	EI
Example. Suppose M has one single column v, and y = +1 if φ = 1 and y = -1 otherwise. Then
the data distribution is simply a mixture of two Gaussians: X 〜 2 + N(y V, σζId×d).
F.1.1 Neural Network Learning
Again, we will normalize the data for learning: we first compute X = (X - E[X])∕σ where σ2 :=
Pd=I(Xi — E[Xi])2 = Pj∈[D] Var(φj) + dσZ is the variance of the data, and then train on (x, y).
This is equivalent to setting φ = (φ - E[φ])∕σ and generating X = Mφ + Z∕σζ. For (x, y) from D
and the normalized (x, y), we will simply say (x, y)〜D.
The learning will be the same as that in the main text, except the following. We will use a small
σW = σ2∕poly(Dm). And we will use a weighted loss to handle the imbalanced classes in the first
two steps for feature learning, and then use the unweighted loss in the remaining steps. Formally, the
weighted loss is:
LD (g； σξ) = E(χ,y)[αy '(y,g(X; ξ))],	(295)
where the class weights a = 2pj=v] for V ∈ {±1}.
57
Published as a conference paper at ICLR 2022
F.2 Main Result
In this setting, we have the following theorem:
Theorem 34. Set
2
η⑴=γ2m3Ξ, λaI)= 0, λW1) = 1∕(2η⑴),σξI) = \段/,	(296)
km3	a	w	ξ
η⑵=1, λa2) = λ(2) = 1∕(2η⑵),σξ2) = 1∕k3/2,	(297)
k2	k3
η = η = T 1/3,λa = λW = λ ≤ --1/3,σξ = 0, for2 <t ≤ T.	(298)
T m1/3	σm1/3	ξ
For any δ ∈ (0, O(1∕k3)), if μ ≤ O(√d∕D), σζ ≤ O(min{1∕σ, σ∕√d}), k = Ω (log2 (^黑:))，
m ≥ max{Ω(k4), D, d} ,then we have for any D ∈ Fξ, with probability at least 1 一 δ ,there exists
t ∈ [T] such that
Pr[sign(g⑴(x)) = y] ≤ LD (g㈤)=O (-k^3 + k-T + k m / ) .	(299)
m2/3	m2	T
Consequently, for any E ∈ (0,1), if T = m4/3, and m ≥ max{Ω(k12∕e3/2), D}, then
Pr[sign(g(t) (x)) 6= y] ≤ LD (g(t)) ≤ .	(300)
The rest of the section is devoted to the proof of this theorem.
F.3 Notations
Recall some notations that we will use throughout the analysis.
For a vector v and an index set I, let vI denote the vector containing the entries of v indexed by I,
and v-I denote the vector containing the entries of v with indices outside I.
Let P := M>M. Then We have Pjj = 1 for any j, and ∣ρj'∣ ≤ μ∕√d for any j = '.
By initialization, w(0) for i ∈ [m] are i.i.d. copies of the same random variable w(0)〜N (0, σW Id×d);
similar for a(0) and b(0). Let σφj= Poj(1 一 p∩j-)∕σ2 denote the variance of φ' for ' ∈ A, where
一.	1.
~
~
Poj = Pr[φj = 1]. Let Po be the value such that with probability 1 - exp(-Ω(k)), ∑j∈A φj ≤
Po(D 一 k) for some Po ∈ [0,1]. That is, Po is an upper bound on the density of φj with high
probability.
Let q' := hw⑼,MQ. Similarly, define q(t) := hW(t, Mg).
We also define the following sets to denote typical initialization. For a fixed δ ∈ (0, 1), define
Gw (δ) := {w ∈ Rd : qg = hw, M'i,σwd ≤ kw ⑼ k2 ≤ 3σwd,
σW(D 一 k) ≤ X q2 ≤ 3σW(D 一 k)
2	g6∈A	2
max |q'| ≤ σw p2 log(Dm∕δ)},
Ga(δ) := {a ∈ R : |a| ≤ σa√2log(m∕δ)}.
(301)
(302)
(303)
Gb(δ) := {b ∈ R : |b| ≤ σb√2log(m∕δ)}.	(304)
58
Published as a conference paper at ICLR 2022
F.4 Existence of A Good Network
We first show that there exists a network that can fit the data distribution.
Lemma 35. Suppose √μp⅛D ≤ τ⅛∙ For any D ∈ FΞ, there exists a network g*(x)
Pn=I a↑σ({w'↑,x} + b)) which satisfies
(x,P)rJyg *(X) ≤ 1] ≤ exp(-Q(k))+exp (-ω (σ∣(i+⅛√))).
Furthermore, the number of neurons n = 3(k + 1), |a* | ≤ 64k, 1∕(64k) ≤ |b* | ≤ 1/4, w*
σ Pj∈A Mj/(8k), and |(w*, X) + b* | ≤ 1 for any i ∈ [n] and (x, y)〜D.
Consequently, if furthermore we have kμ∕√d < 1 and σζ < 1/k, then
Pr [yg*(x) ≤ 1] ≤ exp(-Ω(k)).
(χ,y)〜。
>λ /■ /■ ɪ	-> - τ	~ ~y	Ti ʃ	t ι .	κ~>	ττn Γ 7 1 τ ι
ProofofLemma 35. Let W = σ ∑j∈A Mj and let U = Ej” E[φj]. We have
(w, x)二	=σ EhMj, Mo) + hw, c∕σ) j∈A	(305)
	二 E φj+ E pj'φ + hw, ζ∕σ) j∈A	j∈A,'=j	(306)
	二fφj-U + E p" + hw,c∕σ). j∈A	j∈A,'=j 1_	--	，	(307)
：="
With probability ≥ 1 - exp(-Ω(k)), among all j ∈ A, we have that at mostPo(D — k) of φj are
(1 - po)∕σ, while the others are -po∕σ, and thus
Pj'φ'
j∈A,'=j
≤ kμ PoD ≤ ɪ
-√i~T~ - 16
Furthermore, hw, Z)〜N(0, σZ∣∣w∣∣∣) and ∣∣w∣∣∣ ≤ σ2(k + k2μ∕√d), we have
Pr[|(w,Z用)| ≤ 1∕16] ≥ 1 - exp -Θ
(308)
(309)
(310)
≥ 1 — exp -Θ
For good data points with φ and ζ satisfying the above, we have |ex| ≤ 1∕8. By Lemma 7,
g*(x) := £ δp-μ,4,1∕∣(hw, X))- E	δp-μ,4,1∕∣ (hw,X))	(311)
p∈P	p∈P,0≤p≤k
=Eδp,4,1∕∣(hw, X) + U)- E	δp,4,1∕∣(hw, X) + u)	(312)
p∈P	p∈P,0≤p≤k
=∑δp,4,ι∕∣	I E3j+	ex∖	- E δp,4,1∕∣	I E3j +	ex∖	.	(313)
p∈P	∖j∈A	) p∈P,0≤p≤k	∖j∈A	)
59
Published as a conference paper at ICLR 2022
Then for good data points, We have yg↑(χ) ≥ 1. Similarly,
g2(x) := £Sp-“+1/4,8,1/2(hw, Xi)- E	δp-μ+1∕4,8,1∕2(hw,xi)	(314)
p∈P	p6∈P,0≤p≤k
=δP+1∕4,8,1∕2(hw, xi + U)- E	δp+1∕4,8,1∕2(hw,xi + U)	(315)
p∈P	p6∈P,0≤p≤k
=	δp+1∕4,8,1∕2	I £3j	+ Ex	I - E	δp+1∕4,8,1∕2 I Eφj	+	ex	I .	(316)
p∈P	j∈A	p6∈P,0≤p≤k	j∈A
Then for good data points, We have yg2 (x) ≥ 1.
Note that the bias terms in gɪ and g2 have distance at least 1/4, then at least one of them
satisfies that all its bias terms have absolute value ≥ 1/8. Pick that one and denote it as
g(x) = Pin=1 aiσr(hwi, xi + bi). By the positive homogeneity of σr, We have
n
g(x) = X 8kaiσr(hwi, xi/(8k) + bi/(8k)).	(317)
i=1
Since for any good data points, |hwi, xi/(8k) + bi/(8k)| ≤ 1, then
n
g(x) = X 8kaiσ(hwi, xi/(8k) + bi/(8k))	(318)
i=1
where σ is the truncated ReLU. Now we can set a* = 8kai, w* = Wi∕(8k), b↑ = b/(8k), to get our
final g*.	口
F.5 Initialization
We first show that with high probability, the initial weights are in typical positions.
Lemma 36. Suppose Dμ∕√d ≤ 1/16. For any δ ∈ (0,1), with probability at least 1 一 δ 一
2 exp (-Θ(D - k)) over w(0),
σw2 d/2 ≤ kw(0)k22 ≤ 3σw2 d/2,
σW(D - k)∕2 ≤ X q ≤ 3σW(D — k)∕2,
'∈A
max ∣q'∣≤ σw√2log(D∕δ).
With probability at least 1 - δ over b(0),
Ib(O)∣≤ σbP2log(1∕δ).
With probability at least 1 - δ over a(0),
Ia(O)I ≤ σaP2log(1∕δ).
Proof of Lemma 36. The bound on kw(O) k22 follows from the property of Gaussians.
Note that q = M>w(0)〜N(0, σWP) for the matrix P = M>M. We have with probability ≥ 1-δ∕2,
max' |qe| ≤ ,2σW log D.
For any subset S ⊆ [D], let PS denote the submatrix of P containing the rows and columns
indexed by S. Then qs = M>w(0) 〜N(0, σWPS). By diagonalizing PS and then apply-
ing Bernstein’s inequality, we have with probability ≥ 1 - 2exp (-Θ(ISI∕kPk2), kqSk22 ∈
((kPskF — |S|)σW, (kρskF + 苧)σW). By Gershgorin circle theorem, we have
kρk2 ≤ 1 + (|S| - 1)μ∕√d ≤ 17∕16.
60
Published as a conference paper at ICLR 2022
Similarly, we have
4IS| ≤ (15) ιSι≤kρSkF ≤ (16) |s| ≤5|s|.
The bounds on q then follow.
The bounds on b(0) and a(0) follow from the property of Gaussians.	口
Lemma 37. Suppose Dμ∕√d ≤ 1/16. We have:
•	With probability ≥ 1-δ-2m exp(-Θ(D -k)) over wi(0) ’s, for all i ∈ [2m], wi(0) ∈ Gw(δ).
•	With probability ≥ 1- δ over bi(0) ’s, for all i ∈ [2m], bi(0) ∈ Gb(δ).
•	With probability ≥ 1- δ over ai(0) ’s, for all i ∈ [2m], ai(0) ∈ Ga (δ).
Proof of Lemma 37. This follows from Lemma 36 by union bound.
□
F.6 Some Auxiliary Lemmas
The expression of the gradients will be used frequently.
Lemma 38.
∂
W-LD (g； σξ ) = -aiE(x,y)~D {ayyI[yg(x； ξ) ≤ 1]EξiIKwi,xi + bi + ξi ∈ (0, 1)]x} ,	(319)
∂wi
∂
和LD(g； σξ) = -aiE(x,y)~D {ayyI[yg(x； ξ) ≤ 1]EξiIKwi, Xi + bi ∈ (0, 1)]} ,	(32O)
∂bi
∂
钎LD(g； σξ) = -E(x,y)~D {ayyI[yg(x； ξ) ≤ 1]Eξiσ(hwi,xi + bi + ξi)} .	(321)
fin -	j	`	's '	S	j
ProofofLemma 38. It follows from straightforward calculation.	口
We also have the following auxiliary lemma for later calculations.
Lemma 39.
EφA {αyy}	= 0,	(322)
EφA {|ayy|}	= 1,	(323)
Eφj{∣Φj∣}	= 2σφjσ, for j∈ A,	(324)
EφA {αyyφj}	=γ, for j ∈ A,	(325)
EφA {|ay yφj|}	≤ ɪ, forall j ∈ [D]. σ	(326)
61
Published as a conference paper at ICLR 2022
Proof of Lemma 39.
EφA {αy y} =	EΦA eφa {αyy|y = v} Pr[y = v∈{±1}		v]	(327)
=	2 X	eφa {y|y = v} v∈{±1} 0.			(328) (329)
eΦa {|ayy|}=	EΦA eφa {|ayy| |y = v} Pr[y v∈{±1}		= v]	(330)
Eφj{∣Φj ∣}=	1 X eφa {|y| |y = v} v∈{±1} 1. | - Poj|(1 - Poj) + |1 - Poj|Poj σ		=2σφjσ.	(331) (332) (333)
EφA {αyyφj} =	E eφa {αyyφj | y = v} Pr[y = v] v∈{±1}			(334)
=	2 X eφa {yφj | y = v} v∈{±1} 1 X EφA (y"≡ v∈{±1}	σ	y	v)	(335) (336)
=	1	Y EY+1 + γ-1) = H. 2σ	σ			(337)
EφA {|ayyφ川	=eΦa {|avyφj ||y = v∈{±1}	v} Pr[y = v]		(338)
	≤ 2 X eφa {lyφj ||y = V} v∈{±1}			(339)
	≤ 1 X eφa {|yOj||y = v} v∈{±1}			(340)
	≤ 1. σ			(341)
				□
F.7 Feature Emergence: First Gradient Step
We will show that w.h.p. over the initialization, after the first gradient step, there are neurons that
represent good features.
We begin with analyzing the gradients.
Lemma 40. Fix δ ∈ (0, 1) and suppose wi(0) ∈ Gw(δ), bi(0) ∈ Gb (δ) for all i ∈ [2m]. Let
Dσw √2 log(D∕δ)	√dσξσw √2 log(D∕δ)
:=	:^^+	+
σσ(1)
σ2
If σ^σW d∕σ2 = O(1∕k), P。= Ω(k2∕D), k = Ω(log2 (Dmd∕δ)), and σξ1) = O(1∕k) ,then
∂WLD (g(0); σξ1)) = -a(0) (X MjTj + νj
(342)
where Tj satisfies:
62
Published as a conference paper at ICLR 2022
•	if j ∈ A, then |Tj — βγ∕σ∣ ≤ O(Ee/d), where β ∈ [Ω(1), 1] and depends only on w(0) ,b(l);
•	if j ∈ A, then ∣Tj | ≤。(吟jEeσ);
σζ √l°g(k)^
σ EV
+ σζd e-θ(k)
• | Vj | ≤ O
Proof of Lemma 40. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads to g(0) (x; ξ(1)) = 0, we have
∂WLD (g(0); σ(I))
—	a⑼E(x,y)〜D {αyyI[yg⑼(x; ξ⑴)≤ 1]E«)IKw(O),x)+ b⑼ + ξ⑴ ∈ (0,1)]x}
-	a(0)E(x,y)〜D,ξ(ι) {αyyI[hw(0),xi + b(0) + ξ⑴ ∈ (0, I)]x}
D
-	a⑼ X Mj E(χ,y)〜D,ξ(i) {yyyφjI[hw⑼,xi+ b⑼+ ξ⑴ ∈ (0,1)]}
j=1	X------------------------V------------------------}
:=Tj
-a(0) E(x,y)〜D,ξ(ι) { Oyy^ I[hw(0),xi + b(0) + ξ ⑴ ∈ (Oj)]}
First, consider j ∈ A.
Tj = E(x,y)〜D,ξ(i) {αyyφjIKw(0),χi + b(0) + ξ⑴ ∈ (O, I)]}
=EφA,ζ αyyφj	Pr	hhφ,qi +ι+b(0) + ξ(1)
φ-A,ξ(1)
where ι := hw(0), Z∕σ).
(343)
(344)
(345)
(346)
(347)
(348)
(349)
Let
Ia := Pr hhφ,qi+ι+b(0)+ξ(1) ∈ (0, 1)i ,	(350)
Ia0 := Pr hhφ-A, q-Ai + ι + b(0) + ξ(1) ∈ (0, 1)i .	(351)
Note that |h0a,qA)| = O( "2*3“), and that ∣∣∣ = ∣(w(0),ζ∕σi∣ = O( √dσξσw√2logDI),
and that ∣hφ, qi∣, ∣hφ-A, q-A)| are O( Dσw旺詈(D/δ)). When σw is sufficiently small, by the prop-
erty of the Gaussian ξ(1), we have
|Ia — Ia0 |	(352)
≤ Pr hhφ,qi+ι+b(0)+ξ(1) ≥0i — Pr hhφ-A, q-Ai + ι + b(0) + ξ(1) ≥ 0i	(353)
+Pr hφ,qi+ι+b(0)+ξ(1) ≥1 +Pr hφ-A,q-Ai+ι+b(0)+ξ(1) ≥1	(354)
= O(Ee).	(355)
In summary,
∣Eζ,φ-A (Ia — Ia)| = O(Ee).	(356)
Then we have
Tj — EφA ,ζ,φ-A {αyyφjIa}	(357)
≤ EφA {∣αyyφj| ∣Eζ,φ-A(Ia — Ia)∣}	(358)
≤ O(Ee)EφA {∣αyyφj∣}	(359)
≤ OG∕σ)	(360)
63
Published as a conference paper at ICLR 2022
where the last step is from Lemma 39. Furthermore,
EφA,ζ,φ-A {αyyφjIa0 }
= EφA {αyyφj}Eζ,φ-A[Ia0]
= EφA {αyyφj}	Pr	hhφ-A, q-Ai +ι+b(0) + ξ(1) ∈ (0, 1)i
φ-A,ζ,φ-A
When σw is sufficiently small, we have
Pr [hΦ-A, q-Ai + b⑼ ∈ (0,1/2)] ≥ Ω(1),
PM h∣ + ξ⑴ ∈ (0,1/2)] = 1/2 - exp(-Ω(k)),
ζ,ξ(1)
(361)
(362)
(363)
(364)
(365)
This leads to
β := Eζ,φ-A [ia] = φ M(1) [hΦ-A, q-Ai +1 + b(0) + ξ⑴ ∈ (0,1)i ≥ Ω(1)
By Lemma 39, E@a {αyyφj} = γ∕σ. Therefore,
∣Tj - βγ∕σ∣≤ O(ee∕σ).
(366)
(367)
Now, consider j 6∈ A. Let B denote A ∪ {j}.
Tj = E(x,y)~D,ζ,ξ(1) {αyyφjI [hφ, qi + I + b⑼+ ξ⑴ ∈ (0,1)] 0	(368)
=EφBEφ-B,ζ,ξ(1) nαyyφjI hhφ, qi + ι + b(0) + ξ(1) ∈ (0, 1)]o	(369)
=EφB,ζ αyyφj	Pr	hhφ,qi+ι+b(0)+ξ(1) ∈ (0, 1)] .	(370)
φ-B,ξ(1)
Let
Ib := Pr hhφ,qi+ι+b(0)+ξ(1) ∈ (0, 1)] ,	(371)
Ib0 := Pr hhφ-B, q-Bi + ι + b(0) + ξ(1) ∈ (0, 1)] .	(372)
Similar as above, We have ∣Eζ,ξ(1) (Ib - Ib)| ≤ O&). Then by Lemma 39,
Tj -EφB,ζ,φ-B{αyyφjIb0}	(373)
≤ EφB {∣αyyφj∣∣Ez,Φ-b(Ib-Ib)∣}	(374)
≤ OQ)EφA {∣αyy|}Eφ, {∣φj∣}	(375)
≤ O&) × 1 × O(σφjσ)	(376)
=O(σφ j 联).	(377)
Furthermore,
EφB,ζ,φ-B {αyyφjIb0} = EφA {αyy} Eφj {φj}Eζ,φ-B[Ib0] = 0.	(378)
Therefore,
∣Tj∣≤ O(σφg.	(379)
Finally, consider νj .
Vj= E(x,y)~D,ξ(1) { αfζjIKw(O), xi + b(0) + ξ⑴ ∈ (0, 1)] }	(380)
=EφA,φ-A,ζ,ξ⑴{yyjII[hφ, qi+ιj+ι-j+b(O)+ξ(I) ∈(O, I)]}	(38i)
=eφa< {yσIj	0	Pr⑴κφ, qi+ιI+ι-I+b(0)+ξ(I) ∈(O, I)]}	(382)
64
Published as a conference paper at ICLR 2022
where j := WjO)G/σ and ι-j := hw(0),4/3)— j.
With probability ≥ 1 — dexp(-Θ(k)) over Z, for any j, ∖ζj | ≤ O(σς /log(k)). Let GC denote this
event.
Let
Ij := Pr	[hφ,	q)	+ Ij + i-j + b(0) + ξ(I) ∈ (0,1)]	,	(383)
Ij = Pr	[hΦ,	q)	+ i-j + b(0) + ξ(I) ∈ (0,1)] .	(384)
Similar as above, we have |E《[Ij — Ij∖Gζ]| ≤ O(CV). Then
∣Ec,Φ-a(Ij - Ij)1 ≤ ∣Ec,Φ-a[(Ij - Ij)IGC]∣ + Pr[-Gc]	(385)
≤ O(EV + dexp(—Θ(k))).	(386)
Vj- EΦA,c,Φ-A {y； G Ij} I	(387)
=eφa,c,φ-a {yσj((Ij-Ij)}∣	(388)
≤ EφA,C,Φ-A { yyG (Ij - i0)∖GC } I + EφA,C,Φ-A { y3j(Ij - Ij)∣-Gc} ∣Pr[-Gc]. (389)
The first term is bounded by
EφA,C,Φ-A {辛(Ij- Ij)∣Gc} I
≤ EφA { αyyσζTEc,Φ-a[Ib- IbIGC]∣)
≤ O(cv)%{∣αyy∣} σp0g≡
σ
≤ O(Eν) × 1 × σp0g≡
σ
=O (Cv) .
The second term is bounded by
EΦa,C,Φ-a { -yσZj(Ij - Ij)I-GC} 1 Pr[-GC]
≤ Eφa<,φ-a {yyCj (Ij- Ij)I-GC} I × de-θ⑻
≤ EφA∣ WI × ENIZj II-Gc }× de-θ(k)
≤ σc X de-θ⑹
σ
≤ σζd e-θ(k)
e .
一σ
Furthermore,
EφA,C,Φ-A { Oyyj Ij } = eΦa {ayy} ECj { j } EC-j [Ij] = 0∙
Therefore,
∣νj∣≤ O (σCplOg≡ J+ ?e-θ(k).
(390)
(391)
(392)
(393)
(394)
(395)
(396)
(397)
(398)
(399)
(400)
(401)
□
65
Published as a conference paper at ICLR 2022
Lemma 41. Under the same assumptions as in Lemma 40,
与LD(g⑼;σ(1)) = -a(0)Tb	(402)
∂bi
where |Tb| ≤ O(e).
Proof of Lemma 41. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads to g(0)(x; ξ (1)) = 0, we have
∂bLD (g(0); σξ1))
-	a⑼E(x,y)~D {αyyI[yg⑼(x; ξ⑴)≤ 1]Eξ(i)IKw(O),x)+ b⑼ + ξ⑴ ∈ (0,1)]}
-	a⑼E(x,y)~D,ξ(i) {αyyI[hw⑼,Xi+ b⑼+ ξ⑴ ∈ (0,1)]}
-	a(0) EφA,ζ,ξ(1)	αyy Pr hhφ, qi + ι + b(0) + ξ(1) ∈ (0, 1)i .
I
{z"^
:=Tb
}
where ι := hw(0), Z∕σ). Similar to the proof in Lemma 40,
Eζ	Pr [hφ,qi+ι+b(0)+ξ(1) ∈ (0, 1)]
φ-A,ξ(1)
- Pr [hφ-A, q-Ai + ι + b(0) + ξ(1) ∈ (0, 1)]	= O(e).
φ-A,ξ(1)
Then
Tb - EφA,ζ αyy	Pr	[hφ-A, q-Ai + ι + b(0) + ξ(1) ∈ (0, 1)]
φ-A,ξ(1)
=eφa,z1 |ayy|	PrGKφ,qi +ι + b(0) + ξ⑴ ∈ (OJ)]
φ-A,ξ(1)
-	Pr [hφ-A, q-Ai + ι + b(0) + ξ(1) ∈ (0, 1)]
φ-A,ξ(1)
≤ OQ)EφA {∣αyy|}
≤ O(e).
Also,
EφA,ζ αyy	Pr [hφ-A, q-Ai + ι + b(0) + ξ(1) ∈ (0, 1)]
φ-A,ξ(1)
=	EφA {αyy}	Pr	[hφ-A, q-Ai + ι +b(0) + ξ(1) ∈ (0, 1)]
φ-A,ζ,ξ(1)
=	0.
Therefore, |Tb | ≤ O(e).
Lemma 42. We have
Ii LD (严;针= -Ta
where |Ta| ≤ O(max' q(0)). So if w(0) ∈ G(δ), |Ta| ≤ O(σw √log(Dm∕δ)).
(403)
(404)
(405)
(406)
(407)
(408)
(409)
(410)
(411)
(412)
(413)
(414)
(415)
(416)
□
(417)
66
Published as a conference paper at ICLR 2022
Proof of Lemma 42. Consider one neuron index i and omit the subscript i in the parameters. Since
the unbiased initialization leads to g(0) (x; ξ(1)) = 0, we have
IaLD (g ⑼;σξ1))	(418)
:-E(x,y)~D {αyyI[yg⑼(x;ξ(1)) ≤ 1]Eξ(i)σ(hw(0),xi + b⑼ + ξ(1))}	(419)
— E(χ,y)~D,ξa) {αyyσ(hw⑼,xi + b⑼+ ξ⑴)} .	(420)
、--------------------{--------------------}
:=Ta
Let φ0A be an independent copy of φA, φ0 be the vector obtained by replacing in φ the entries φA with
ΦA, and let x0 = Mφ0 + Z∕σ and its label is y0. Then
|Ta| = EφA nαyy Eφ-A,ζ,ξ(1)σ(hw(0),xi+b(0)+ξ(1))o	(421)
≤ 2 EφA {Eφ-A,ζ,ξ(i)σ(hw⑼,xi + b⑼+ ξ⑴)|y = l}	(422)
— EφA {Eφτ,ζ,ξ(i)σ(hw⑼,xi + b⑼ + ξ(I))Iy = -l}	(423)
≤ 2 EφA {Eφτ,ζ,ξ(i)σ(hw⑼,xi + b⑼+ ξ⑴)|y = l}	(424)
— EφA {Eφτ,ζ,ξ(i)σ(hw⑼,x0i + b⑼+ ξ⑴)|y0 = -l} .	(425)
Since σ is 1-Lipschitz,
ITaI	≤	2EφA,φA {Eφ-Akw(O),Mφi-hw(0),Mφ0i∣∣y =1,y0	=	-l}	(426)
≤	1 Eφ-A &A {∣hw⑼,Mφi∣ |y = 1} + EφA	{∣hw⑼,Mφ0i∣	Iy0 = -l})	(427)
≤	maχq(l, eφ ]X φ2 + X lφj。'|)	(428)
'∈ V∈[D] j='j,'∈A	)
≤	max q(,0) q∕Eφ (1 + O(1))	(429)
=Θ(max q(0)).	(430)
□
With the bounds on the gradient, we now summarize the results for the weights after the first gradient
step.
Lemma 43. Set
λWI) = 1∕(2η ⑴),λaI)= λb1=0,σ(I) = I/k3∕2.
Fix δ ∈ (0,1) and suppose w(0) ∈ GW(δ),b(0) ∈ Gb(δ) for all i ∈ [2m]. If k = Ω(log2(Dm∕δ)),
then for all i ∈ [m], w(1) = PD=I q(?M' + U satisfying
•	if' ∈ A, then ∣q(1 — η(1)α(0)βγ∕σ∣ ≤ O (n(’；i |'e} where β ∈ [Ω⑴，1] and depends
only on wi(0), bi(0);
•	if' ∈ A, then |q(?| ≤ O (In(I)ai0)lσφ`feσ);
67
Published as a conference paper at ICLR 2022
•	心| ≤ O (∣η⑴a(0)| (WpkIe“ + 等e-e(k)),
and
•	b(I) = b(0) + η(I)a((O)Tb where ∣Tb∣ = O (Ee);
•	a，I) = a(0) + η(I)Ta where ∣Ta∣ = O(σw "log(Dm∕δ)).
ProofofLemma 43. This follows from Lemma 37 and Lemma 40-42.
□
F.8 FEATURE IMPROVEMENT: SECOND GRADIENT STEP
We first show that with properly set η(1), for most x, ∣g(I)(x; σξ(2))∣ < 1 and thus yg(I)(x; σξ(2)) < 1.
Lemma 44. Fix δ ∈ (0,1) and suppose W;O) ∈ GW(δ), b(0) ∈ Gb(δ), a，0) ∈ Ga(δ) for all i ∈ [2m].
If Dμ∕√d ≤ 1/16, σζσ = O(1), σ∣d∕σ2 = O(1), k = Ω(log2(Dm∕δ)), σa ≤ σ2∕(γk2),
η(I) = O (kJ σ), and σξ(2) ≤ 1∕k, then with probability ≥ 1 — (d + D) exp(-Ω(k)) over
(x, y), we have yg(I)(x; σξ(2)) < 1. Furthermore, for any i ∈ [2m], |〈w(I),Z∕σ} ∣ = O(η(I)σ∕γ),
MI) ,Φi ∣ = O(η(1)σ ∕γ), and ∣ h(q(I))-A, O-A)∣ = O(η(1)σ∕γ), and for any j ∈ [d],' ∈ [D],
G∣ ≤ O(σζ√log(k)) and ∣hZ, D')∣ ≤ O(σζ√log(k)).
ProofofLemma 44. Note that W(O) = w，：-,，0) = b，', and a(0) = a，]，. Then the gradient for
wm+i is the negation of that for wm+-, the gradient for bm+i is the negation of that for bm+-, and the
gradient for αm+- is the same as that for αm+-.
∣ g(1)(x； σξ(2)) ∣	(431)
2m
=X a(I)Eξ(2) C(w(1),x)+ b(1) + €(2))	(432)
i=1
m
=X (a(I)Eξ(2)σ(hw(I),xi+b(1)+ξF))+«，+-Eg(2)σ(hwm+i,xi+bm+-+端S) (433)
i=1
m
≤ X (a(I)Eξ(2)σ(hw(1),xi + b(1) + €(2)) + a，+-Eξ(2)σ(hw(1),xi + b(1) + ξF)))	(434)
i=1
m
+ X (-a，+-Eξ(2) σ(hw(I),xi+b(I)+ξF))+am+-Eξ ⑵ σ(hwm+i,xi+bm+i+ξF))).
i=1
(435)
Then we have
，
∣ g(I)(x; σξ(2)) ∣ ≤ X ∣ 2η(I)TaEg⑵σ(hw(I),x) + b(1) + ξ(2)) ∣	(436)
i=1
，
+£卜，+」(IhWi(I)-W，+-,xi ∣+ ∣b(I)- b，+i ∣)	(437)
i=1
，
≤ x∣ 2η(I)TaI (IhW(I),x) + b(1) ∣+ Eξ(2)忖2) ∣)	(438)
i=1
，
+ X 同+，"Wi(I)-W，+i,x) ∣ + ∣ b(I)-e∏∣) .	(439)
i=1
68
Published as a conference paper at ICLR 2022
With probability ≥ 1 - exp(-Ω(k)), among all j ∈ A, We have that at mostp°(D - k) of φj are
(1 - poj)∕σ, while the others are -POj/σ. With probability ≥ 1 - (d + D) exp(-Ω(k)) over Z,
for any j, ∖ζj| ≤ O(σζPIag(k)) and ∖{ζ, Dg)| ≤ O(σζ/log(k)). For data points with φ and ζ
satisfying these, we have:
Claim 1. ∣(w(1),x)∣ ≤ O(η⑴∕γ)(1 + σ + σ∕√k).
ProofofClaim 1.
D	D
Ihw(I),xi ∣ =〈X q(' M` + υ, X φj Mj + C∕σi	(44O)
'=1	j=1
≤
DD
(X 晨M',χ φjMj)
D
+ hXq(?M',ζ7σi +
2=1
D
hυ, X φj Mj〉+ lhυ,ζ∕σi∖∙
j=1
(441)
For each term above we bound as follows. Note that when σw is sufficiently small, Ee
O(klog1∕2(Dm∕δ)∕√D). Let
B1 := βγ∕σ + Ee∕σ,	(442)
B2 ：= σφ^σ = O(∈e∕√D),	(443)
k
Ci = T	(444)
σ
C2 ：= PoD∕σ = O(D∕σ).	(445)
Then
Ia(O) ∣B1C1 =	二 O(Iag(Dm∕δ)∕k + Iag(Dm∕δ)ι∕(Yk)) = O(1∕γ),	(446)
Ia(O) ∣B2C2 =	二 O(σ∕γ),	(447)
Ia(O) IB1C2 =	=O(D∕k + √D∕γ),	(448)
Ia(O) IB2C1 =	二 O(Ee/(Y√k)) = O(1∕Y),	(449)
Then by the assumption on μ,
DD
(X 渭 M',X φj Mji	(450)
2=1	j=1
Xhq(I)M2, MM〉+ Xhq(I)M2,MM)
2三A	2∈A
≤ O(In(I)a(0)|)
+ X h q(I) m2 , Mj φj i
2=j
+ B2C2 + √μ= (kB1(C1 + C2) + DB2(C1 + C2))
≤ O(In(I)a，O)I)(B1C1 + B2C2 + √μB1C2 + B2C1
≤ O(n0))(1∕γ + σ ∕γ + 1∕γ + 1∕γ)
(451)
(452)
(453)
(454)
(455)
≤ O(n(1"γ )(1 + σ).
69
Published as a conference paper at ICLR 2022
By the assumption on σς,
D
	(E ⅛⅛ζ∕σ) 2=1	(456)
≤	O(In(I)a(0) ∣)(kBι	+ DB2) σp0g≡	(457) σ
≤	O(n ⑴)(kBι + DB2)σζ ^k σ 2 MOgkDm0	(458) σ	γk2	
≤	O(n(1)) (σζ IOg(Dm∕δ) (1 + ⅞) + σζσlθg≡°g(Dm^)	(459)	
≤	O(n ⑴ ∕γ).	(460)
Also note that ∖νj | ≤ O ( σζ "^Dm® ). Then by the assumption on σ0,
D
3 X Φj Mj	(461)
j=i
≤ O(∣η⑴α*) ×√d × O (σ lθg2*0 ) × (G + C2)	(462)
∖ σ 7 D )
≤ O(∣η⑴/γ).	(463)
Finally, We have
d
Kυ<∕σil ≤ X ∖υjWGId∖
j=i
≤ O(∣η⑴a(0)|) × d × σζlog2(Dm0 σpogH
σ √ d	σ
≤ Qn(I)IY)飞
(464)
(465)
(466)
□
We also have:
	∖Ta∖ = O(σw SOg(Dm∕δ)	(467)
	I b(1) ∣≤∣ b(0) ∣ + ∣ n(1)a(0)Tb I	(468)
	≤ pl⅞咧+ ∣*α(0)ee∣.	(469)
E, lα⑴ ∖am+i	:⑵ H ∣ ≤ O(σ(2)).	(470)
	≤ ∖αi0)∖ + ∖n ⑴ Ta∖ ≤ ∖α(0)∖+ O(η(1)σw √log(Dm∕δ)).	(471)
-wm+i ㈤	=21 (W(I),xi ∣ = O(n ⑴ σ/Y).	(472)
b(1) - b(1) bi	bm+i	=2In(I)α(0)Tb∖ = O(In(I)ai0)ke).	(473)
70
Published as a conference paper at ICLR 2022
Then We have
Ig(I) (x; σ(2)) I
≤ O (mη(I)σul /log(Dm∕δ)) {^~^~ + "ogm®- + 卜⑴0(0%1 + σξ2)
(474)
+ O (m(|a(O)I + η(1) σw ,log(Dm∕δ))) ( n-γσ + 卜(I)a(O)1| )
Cr ⑴	log(Dm∕S)	l (O)1 ηη(1)σ ι I ⑴(O) I ʌʌ
=O I mη( )σw----%-----+ m∣ai ,∖ I ———+ ∣η(，* &∣ Il
(1 (⑴	log(Dm∕δ)上,(O)1 η(1)σ	, (O)1 η⑴σ∖
= O I mη()σw----k-----+ m∣αi，|ʒ---+ m∣ai，|	)
< 1.
(475)
(476)
(477)
(478)
Then I yg⑴(x;
calculation on
< 1. Finally, the statement on
Xi | = IMI)M | .
Ih(q(I))-A,φ-Ai I follows from a similar
□
We are noW ready to analyze the gradients in the second gradient step.
Lemma 45. Fix δ ∈ (0,1) and suppose W;O) ∈ GW(δ),b(O) ∈ Gb(δ),αiO) ∈ Ga(δ) for all i ∈ [2m].
Let Ee2 ：= O (" "a' %γ+'e)) +exp(-Θ(k)). If Dμ∕√d ≤ 1/16, σζσ = O(1), σ∣d∕σ2 = O(1),
k = Ω(log2(Dm∕δ)), σa ≤ σ2KIk∖ η(1) = O (卜总段),and σξ(2) = 1∕%3/2, then
∂WLD (g(I); σξ2) ) = -a(I) (X MjTj + ν
(479)
where Tj satisfies:
•	if j ∈ A, then ∣Tj — βγ∕σ∣ ≤ O&2∕σ + η(1)∕σξ(2) + η(I)IaiO)IEe∕(σσξ2))), where β ∈
[Ω(1), 1] and depends only on W(O), b(O)；
•	if j ∈ A, then ∣T7-1 ≤ 1 exp(-Ω(k)) + O(σφ6匕@);
•	〔 Vj1 ≤ o (1j； ) +eχp(-C(k)).
ProofofLemma 45. Consider one neuron index i and omit the subscript i in the parameters.
By Lemma 44, with probability at least 1 - (d + D)exp(-Ω(k)) = 1 - exp(-Ω(k)) over
(x,y), yg⑴(x;ξ(2)) > 1 and furthermore, for any i ∈ [2m], IhW(I)(∕σi I = o(η ⑴ σ∕W,
S(I),Φi I = O(η⑴σ∕γ), and I h(q(I))-A,φ-A)1 = O(η⑴σ∕γ), and for any j ∈ [d],' ∈ [D],
ζj ∣ ≤ O(σζ /log(k)) and ∣hZ, DQ∣ ≤ O(σ^ ,log(k)). Let Ix be the indicator of this event.
∂WLD (g(I)； σ"
-a⑴E(x,y)~D {αyyIxEξ(2)I[〈w⑴，x〉+ 从I) + ξ⑵ ∈ (0,1)]x}
D
-a(I) X Mj E(x,y)~D,ξ(2) {αyyIxI[hW(I), xi + b⑴ + ξ⑵ ∈ (0, 1)]φj}
j=1	X--------------------------V-------------------------/
=Tj
-a(I) E(x,y)~D,ξ(2) ( Wy IxIKW(I) ,χi + b(1) + ξ⑵ ∈ (0, 1)]].
(480)
(481)
(482)
(483)
71
Published as a conference paper at ICLR 2022
Let Tj1 := E(χ,y)~D,ξ⑶{%yI[<w⑴，x〉+ b⑴ + ξ(2) ∈ (0,1)]φj}. Wehave
©- Tj1\
=B%y)~D,ξ⑶{αyy(I - Ix)IKw(I),xi+ b(I) + ξ⑵ ∈ (0, 1)]φj}∣
≤ 1exp(-Ω(k)).
σ
Similarly, let V0 := E(x,y)~D,ξ⑶{αyσyζIKw(I),x〉+ b(I) + ξ(2) ∈ (0,1)]}. We have
IV - M\
=E(x,y)~D,ξ ⑶{个(I- Ix)IKw(I), xi + b(I) + ξ⑵ ∈ (0, 1)]| J
≤ σζ exp(-Ω(k)).
σ
So it is sufficient to bound Tji and V0. For simplicity, We use q as a shorthand for q(1).
First, consider j ∈ A.
TjI = E(x,y)~D,ξ⑵{0yyIKw(I)㈤ + b(I) + g(2) ∈ (0, 1)]φj}
=EφA {°yyΦj φ P")[hΦ, G + L + b(I) + ξ⑵ ∈ (0, 1)] }
where ∣ := hw(1), ζ∕σ). Let
Ia := Pr [hφ,q) + ι + b(1) + g(2) ∈ (0,1)],
Ia = Pr [φ>-A, q-A + L + b(I) + g(2) ∈ (0, 1)].
(484)
(485)
(486)
(487)
(488)
(489)
(490)
(491)
(492)
(493)
By the property of the Gaussian ξ(2), that ∖(Φa, qA)∖ = O(n()|ai 32k(Y+Q)), and that \l\ =
Kw(I),ζ∕σ i\, ∖hφ, qi\, ∖{Φ-A,q-A)∖ are all O(η(I)σ∕γ) < O(1∕k), we have
\Ia- Ia \	(494)
≤ Pr	[hφ, q + L +	b(1)	+	g(2)	≥ 0] - Pr	[hΦ-A, q-A	+ ∣ + b(1)	+ ξ(2) ≥ 0]	(495)
+ Pr [hφ, q + L + b(I) + g(2) ≥ 1] + Pr [hφ-A, q-A〉+ L + b⑴ + ξ(2) ≥ 1]	(496)
=O ("()\% % + I)) +eχp(-Θ(k)) =。&2).	(497)
y	σ2σξ	J
This leads to
I Tji - EφA,Φ-A {ay yφj ia } I	(498)
≤ Eφa {∖αyyΦj \| Eφ-a (Ia -Ia) ∣ }	(499)
≤ O(ee2)EφA {∖αyyφj∖}	(500)
≤ OG2∕σ)	(501)
where the last step is from Lemma 39. Furthermore,
Eφa,Φ-a Sy泌Ia}	(502)
=EφA {ayyφj} Eφ-A [Ia]	(503)
=EφA {αyyφj}	Pr, ∖[<φ-A,q-A> + L + b(i) + g(2) ∈ (0,1)] .	(504)
φ-A,ξ(2) L	」
72
Published as a conference paper at ICLR 2022
ByLemma 19, we have ∣hΦ-A, q-A + ι∣ ≤ O(η(1)σ∕γ). Also, |b⑴-b(0)| ≤ O(η⑴ |。(0)院).By
the property of ξ(2),
Pr hφ-A,q-Ai+ι+b(1)+ξ(2) ∈ (0, 1) - Pr b(0) + ξ(2) ∈ (0, 1)
≤ O(η⑴σ∕(γσξ2))) + O(η⑴|a(0)|ee/b(2)).
On the other hand,
β := Pr	hb(0) + ξ(2) ∈ (0, 1)i = Pr hξ(2) ∈ (-b(0),1 -b(0))i
=c(1)
and β only depends on b(0). By Lemma 39, E@a {αyyφj} = γ∕σ. Therefore,
∣Tji - βγ∕σ| ≤ O(ee2∕σ) + O(η⑴∕σ" + O(η⑴∣a(0h∕(σσ").
(505)
(506)
(507)
(508)
(509)
Now, consider j 6∈ A. Let B denote A ∪ {j}.
TjI= E(x,y)~D,ξ⑶{αyyφjI hhφ, qi + ι + b(1) + ξ(2) ∈ (0, 1)]}
= EφB Eφ-B,ζ,ξ(2) nαy yφj I hhφ,qi + ι+ b(1) +ξ(2) ∈ (0, 1)io
= EφB αyyφj Pr hhφ, qi + ι+b(1) + ξ(2) ∈ (0, 1)i .
(510)
(511)
(512)
Let
Ib:= Pr hhφ,qi+ι+b(1)+ξ(2) ∈ (0, 1)i ,	(513)
Ib0 := Pr hhφ-B, q-Bi + ι + b(1) + ξ(2) ∈ (0, 1)i .	(514)
Similar as above, we have |Ib - Ib0 | ≤ e2. Then by Lemma 39,
Tj1 - EφB,φ-B,ζ {αyyφjIb0}	(515)
≤ EφB {∣αyyφj∣∣Eφ-B,ζ(Ib - Ib)∣}	(516)
≤ O(ee2)EφA {∣αyy|} Eφ, {∣φj∣}	(517)
≤ O&2) × O(σφσ)	(518)
=O(σφ σ6e2).	(519)
Furthermore,
EφB,φ-B,ζ {αyyφjIb0} = EφA {αyy}Eφj {φj} Eφ-B [Ib0] = 0.	(520)
Therefore,
∣Tji∣ ≤ O(σφσ6e2).	(521)
Finally, consider νj0 .
Vj= E(χ,y)~D,ξ⑶{ αyσζjIKw(I),xi + b(1) + ξ(2) ∈ (0,1)]}	(522)
=EφA,φ-A,ζ,ξ⑶{yσjιI[hφ, qi+ι+b(i)+ξ⑵ ∈(0, 1)]}	(523)
= EφA,φ-A,ζ	yζj pr[hΦ, qi + ∣ + b⑴ + ξ⑵ ∈ (0,1)]]	(524)
σ ξ(2)
73
Published as a conference paper at ICLR 2022
Let
	Ij :=	Pr hφ,qi+ι+b(0)+ξ(1) ∈(0,1) ,			(525)
	Ij0 :=	Pr hhφ, qi + b(0) + ξ(1) ∈(0,1	)i.		(526)
Since ∣ι∣ ≤ O(η(1)σ/γ),we have |Ij — Ij| ≤ O(η(1)σ/(γσ(2))). Then					
		Vj- EφA,φ-A,ζ {αyyζj Ij j∣			(527)
	=	eφa,φ-a ,ζ{ αyyζj (Ij-Ij)}|			(528)
	≤	O(n ⑴ σ∕(γσ(2)))EΦA,Φ-A,ζ ∣αyyζj			(529)
	≤	OM1)σ∕(Yσ(2)DEφA |ayy| EZ	Zj σ		(530)
	≤	O(η⑴σ∕(γσ(2))) × 1 X σζ ξ	σ			(531)
	≤	O(η(1)σζ∕(γσξ(2))).			(532)
Furthermore,					
	EφA,φ-A,ζ	αyyζjIj} = EφA,Φ-A nασyIj0 Eζ {ζj} = 0.			(533)
Therefore,					
	IVj | ≤ O (η~(2)ζ j +exp(—c(k)).				(534)
		ξ			□
Lemma 46.	Under the same assumptions as in Lemma 45,				
		"(产;σ(2)) = -a(1)Tb			(535)
where |Tb| ≤ exp(-Ω(k)) + O(Ee2).
Proof of Lemma 46. Consider one neuron index i and omit the subscript i in the parameters. By
Lemma 44, Pr[yg⑴(x; ξ⑵)> 1] ≤ exp(-Ω(k)). Let Ix = I[yg⑴(x; ξ(2)) ≤ 1].	
∂∣ld (g(1)； σξ(2))	(536)
=-a(1) E(x,y)~D {αyyIxEξ⑶I[(w⑴,x〉+ b(1) + ξ⑵ ∈ (0,1)]}.	(537)
1	- -	}	
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^{l^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ :=Tb	
Let Tbi := E(x,y)~D,ξ(2) {ayyI[hw(1),xi + b(1) + ξ⑵ ∈ (0,1)]}. Wehave	
|Tb - Tb1 |	(538)
=1E(x,y)~D,ξ⑶{ayU(I- Ix)IKw(I),xi + b(1) + ξ⑵ ∈ (0, 1)]0|	(539)
≤ exp(-Ω(k)).	(540)
So it is sufficient to bound Tb1. For simplicity, we use q as a shorthand for qi(1).	
TbI= E(x,y)~D,ξ ⑶{ay yI hhφ, qi + ι + b(1) + ξ ⑵ ∈ (OJ)I}	(541)
= EφA Eφ-A,ξ(2) nαyyI hhφ, qi + ι+ b(1) + ξ(2) ∈ (0, 1)io	(542)
=EφA αyy	Pr	hhφ,qi+ι+b(1)+ξ(2) ∈ (0, 1)i .	(543)
74
Published as a conference paper at ICLR 2022
Let
Ib := Pr hhφ,qi+ι+b(1)+ξ(2) ∈ (0, 1)i ,	(544)
Ib0 := Pr hhφ-A, q-Ai + ι + b(1) + ξ(2) ∈ (0, 1)i .	(545)
Similar as in Lemma 20, we have |Ib - Ib0 | ≤ e2. Then by Lemma 39,
Tb1 - EφA ,φ-A {αyyIb0 }	(546)
= EφA,φ-A{αyy(Ib-Ib0)}	(547)
=O(ee2)EφA,φ-A ∣αyy|	(548)
≤ O(e2).	(549)
Furthermore,
EφA ,φ-A {αyyIb0} = EφA {αyy} Eφ-A [Ib0] = 0.	(550)
Therefore, ∣Tbι | ≤ O(12) and the statement follows.	□
Lemma 47. Under the same assumptions as in Lemma 45,
∂ai LD(g(1); σ(2)) = -Ta
(551)
where ∣T"∣ = O (Y≥) +exP(-a(k))Poly (悬
Proof of Lemma 47. Consider one neuron index i and omit the subscript i in the parameters. By
Lemma 44, Pr[yg(1) (x; ξ(2)) > 1] ≤ exp(-Θ(k)). Let Ix = I[yg(1) (x; ξ(2)) ≤ 1].
: LD (g ⑴;σ(2))	(552)
∂a	ξ
- E(x,y)〜D {αyyIχEξ(2)σ(hw(1),x) + b(1) + ξ(2))} ∙	(553)
、---------------------{---------------------}
:=Ta
Let Tai := E(χ,y)〜D {ayyEξ(2)σ(hw(1),xi + b(1) + ξ(2))}. Wehave
|Ta - Ta1 |	(554)
=∣E(x,y)〜D {αyy(1 - Iχ)Eξ⑵σ(hw(1),xi + b(1) + ξ⑵川	(555)
≤ exp(-Ω(k)).	(556)
So it is sufficient to bound Ta1. For simplicity, we use q as a shorthand for qi(1).
75
Published as a conference paper at ICLR 2022
Let φ0A be an independent copy of φA, φ0 be the vector obtained by replacing in φ the entries φA with
φ0A , and let x0 = Mφ0 + ζ and its label is y0. Then
|Ta1| := EφA nαyyEφ-A,ζ,ξ(2)σ(hw(1), xi + b(1) + ξ(2))o	(557)
≤ 2 EφA {Eφ-A,ζ,ξ⑵σ(hw⑴,xi + b⑴+ ξ⑴)|y = l}	(558)
— EφA {Eφτ,ζ,ξ⑵σ(hw⑴,xi + b⑴+ ξ⑵)|y = -l}	(559)
≤ 2 EφA {Eφτ,ζ,ξ⑵σ(hw⑴,xi + b⑴+ ξ⑵)|y = l}	(560)
— EφA {Eφτ,ζ,ξ⑵σ(hw⑴,x00 + b⑴+ ξ⑵)∣y0 = -l}	(561)
≤ 1 eΦa,φa nEΦ-A ∣hw(1),χi — hw(1),χ0i∣ |y = 1,y0 = —1}	(562)
≤ 1 Eφ-A (EφA {∣hw⑴,Mφ0∣ |y = 1} + EφA {∣hw⑴,Mφ0i∣ ∣y0 = -l})	(563)
≤ Eφ-A,φA ∣∣∣αyhw(1), M φi∣∣∣	(564)
= Eφ ∣∣∣αyhw(1), M φi∣∣∣	(565)
=O(η(1)σ/γ) + exp(-Ω(k)) × dD× × ∣∣w⑴∣∣∞	(566)
σ
=O η-~~σ) + exp(-Ω(k))poly(dD/Pmin)	(567)
γpmin
where the fourth step follows from that σ is I-LiPschitz, and the second to the last line from Lemma 44
andthat∣hw⑴，Mφ0∣ ≤ ∣w⑴∣∣∞Pd∣∣Mφ∣∣2.	口
With the above lemmas about the gradients, we are now ready to show that at the end of the second
step, we get a good set of features for accurate prediction.
Lemma 48. Set
2
-(I) = rpm3Z, λaI)= 0, λW1) = 1/(2-(1)), σ(I) = 1/k3/2,	(568)
km3	ξ
-⑵=1, λa2) = λ(2) = 1/(2-(2)), σ(2) = 1/k3/2.	(569)
Fix δ ∈ (0,O(1/k3)). If Dμ/√d ≤ 1/16, σqσ = O⑴,共疗=O⑴,k = Ω (log2 (^^))，
and m ≥ max{Ω(k4), D, d}, then with probability at least 1 一 δ over the initialization, there exist
Gi ∖s such that g(x) := P2mι (⅛σ(hw(2),x0 + b(2)) satisfies LD(g) ≤ exp(-Ω(k)). Furthermore,
∣∣αko = O(m/k), ∣α∣∞ = O(k5/m), and Mk2 = O(k9/m). Finally, ∣α(2)∣∣∞ = O (康),
∣∣w(2)k2 = O(3/k), and |b(2)| = O(1/k2) forall i ∈ [2m].
ProofofLemma 48. By Lemma 35, there exists a network g*(x) = P3=+1) α:σ(hw:,x0 + 坟)
satisfying
Pr [yg*(x) ≤ 1] ≤ exp(-Ω(k)).
(x,y)~D
Furthermore, the number of neurons n = 3(k + 1), |a*| ≤ 64k, 1/(64k) ≤ |b*| ≤ 1/4, w* =
σ Pj∈A Mj/(8k), and ∣hw*,x0 + b*| ≤ 1 for any i ∈ [n] and (x, y) ~ D. Now We fix an ', and
show that with high probability there is a neuron in g(2) that can approximate the `-th neuron in g*.
With probability ≥ 1 - exp(-Ω(max{2p0(D - k), k})), among all j ∈ A, We have that at most
2po(D - k) + k of φj are (1 - Po)/3, while the others are -po/3. With probability ≥ 1 - (d +
76
Published as a conference paper at ICLR 2022
D)exp(-Ω(k)) over Z, for any j, Q | ≤ O(σζ √log(k)) and ∣hZ,D'i∣ ≤ O(σζ √log(k)). Below
we consider data points with φ and ζ satisfying these.
By Lemma 37, with probability 1 - 2δ over wi(0)’s, they are all in Gw(δ); with probability 1 - δ over
ai(0)’s, they are all in Ga(δ); with probability 1 - δ over bi(0)’s, they are all in Gb(δ). Under these
events, by Lemma 43, Lemma 45 and 46, for any neuron i ∈ [2m], we have
Wy) = a(I) (XX MjTj + νj,	(570)
bi(2) = bi(1) +ai(1)Tb.	(571)
where
•	if j ∈ A, then |Tj - βγ∕σ∣ ≤ s := O(i2∕σ + η⑴/σξ2) + η(1) |a(0) ∣6e∕(σσ(2))), where
β ∈ [Ω(1),1] and depends only on w(0),b(0);
•	if j ∈ A, then |T-1 ≤ “ := 1 exp(-Ω(k)) + O(σφGq);
•	|Vj| ≤ EV ：= O ^ηγ(σ⅛ζ^ +exp(-Ω(k)).
•	|Tb| ≤ Eb ：= exp(-Ω(k)) + O&2).
Given the initialization, with probability Ω(1) over b(0), we have
|bi0)|∈ 2k2,k ,sign(bi0))=sign(b?).	(572)
Finally, since 8k|bj|e； ∈ [Ω(k2γ∕σ2), O(k3γ∕σ2)] and depends only on w(0), bi0), we have that for
Ea = Θ(1∕k2), with probability Ω(e°) > δ over a(0),
8≡γa(0) - 1≤ Ea,	∣a(0)∣ = O (W) .	(573)
Let na = Eam∕4. For the given value of m, by (570)-(573) we have with probability ≥ 1 - 5δ over
the initialization, for each ' there is a different set of neurons l` ⊆ [m] with |4| = n and such that
for each i` ∈ l`,
ιb(0)ι∈ 2k2,k ,	sign(b(0)) = sign("),	(574)
8k^lβY (0) I
∣W L
≤ Ea,
(575)
Now, construct α such that <⅛g = 尚：'1 for each ' and each i` ∈ l`, and )⅛ = 0 elsewhere. To show
that it gives accurate predictions, we first consider bounding some error terms.
For the given values of parameters, we have
Ee2 = O (j),
m2
Eb =O(W).
m2
(576)
(577)
(578)
(579)
(580)
77
Published as a conference paper at ICLR 2022
We also have the following useful claims.
Claim2.	∣(M.,x)∣ ≤ O (⅜).
ProofofClaim 2.
2∈A
≤ ∑ ∖Φj∖ + ∑M∕Mjφj + ∖M]ζ∕∂∖
g∈A ∖	j≠£
“（A。Mfe）十。卜）
(581)
(582)
(583)
(584)
□
Claim 3.
媳β]
σ
£(蛇㈤≤o
j三A
Proof of Claim 3.
D
M词=& *）夏跖+S冷
£=1
≤	We㈤十(£
a，?TEM小 X)+ ∣(v,x)∣ ,
2∈A
饭A
Then
<
<
The first term is
0,)尸7
7∈A
0，D /7
j6A
a∖? β]
σ
≤l⅛υl
2∈A V
+
+ 4υ
Mg) x,

+

(^TeMe,x') +∖(y,x}∖ .
绢A	J
(585)
(586)
(587)
(588)
(589)
(590)
(591)
(592)
78
Published as a conference paper at ICLR 2022
By Claim 2,
hX (T-Se
'∈A ×
"≤ X TL 今
'∈A
≤ Si EKMe,x)∖
'∈A
≤ O
(£ TeMe,x)
'∈A
Then by (576)-(580),
eψ :
+
Dew2 + PODeV√dʌ _。
≤ ∑TΦj
'∈A
+
E t`m'Mjφj + EKMJZ∕σ
'∈A,j='
‹ O
‹ O
'∈A	i
(Dew2 σp1亚
∖hν,xi∖ ≤
σ
U E Mφi + ∖3ζ∕σi∖
'∈[D]
≤ E ∖φ'hν, M'i∖ + Kν<∕σi∖
'∈[D]
≤ O (Poq) eν√d + deν θ(σζ p0g而
σ
≤ O (eν√d
σ
+ √d€ν ,log(k)
≤ O (eνσdʌ (poD + σpiog(k))
≤ O PPODeV√d
σ
σ
k2l	Ye_	_T_
ς>~ς>+	ς>~+	- O +
m2σ2	m2σ	mσ2
We have ∣ a(I) - a(0) ∣ = O(η(1)σw plog(Dm∕δ)). So the first term is bounded by
—
a(0)Bl
σ
XhMjH ≤ 1 ɑ(I)I
j ∈A
Cφ
≤O
+ η ⑴ σw √l⅛El)	(R	+ 与	+ T +	ɪ)	≤ O (ɪ
)	∖m2σ2 m2σ mσ2	m32σ)	∖ m
By Claim 2, the second term is bounded by
a(I) - a
(0) I Bl
I ---------
乙
I β XhMjH ≤ O
σ
kη⑴σw √log(Dm∕δ)γ
σ2
j∈A
Combining the bounds on the two terms leads to the claim.
≤ oQ
m3
(593)
(594)
(595)
(596)
(597)
(598)
(599)
(600)
(601)
(602)
(603)
(604)
(605)
(606)
(607)
(608)
□
79
Published as a conference paper at ICLR 2022
Claim 4.
(609)
ProofofClaim 4. By Lemma 43 and 46:
质)-猾ι≤喈-娉I +阳)-记
≤ o S(I) |a(0)ke + l*)1 (exp(-Ω(⅛)) + £e2
≤ θ(k⅛ + 际)≤ θ
(610)
(611)
(612)
□
We are now ready to show g is close to 2g*.
∣g(x)- 2g*(x)∣	(613)
3(k+1)	3(k+1)
X X 瓯 σ(h*,xi + 婷)-X 2aeσ (hwe,x) + be)	(614)
'=ι ie∈ιe	e=ι
3(k+l)	ɔ *|入*|	/	、	3(k+l)
X bXi * 5 ㈤+短)-X X
(615)
≤
3(XI) X ɪ I 2⅛b^σ (忐 H + b(2
匕念 战 [ IbM " bi
-σ((SJeL XMXi + b(。)
Ib((O)I ∖ σ	MMjXi + &
+
3Xυ X 1 J 2α' lbel (α(O)βγ X (M 公十 b(0)
二 b⅛ 元[Wr 1 丁 安M ㈤+bbi
2破 lb' I
∣倒
—
Here the second equation follows from that σ is positive-homogeneous in [0,1], Ihwj,xi + b' I ≤ 1,
Ib(O)I/Ibe I≤ ι.
By Claim 3 and 4, the first term is bounded by:
3(k + I) max 2:* 1 ( 1*, Xi- abiσβγ XhMj,xi +1婷一b(O) i]	(618)
Ibbi l ∖ I	j∈A	)
≤ 3(k +1)max2ae≡O (-)	(619)
一	` ιb(0)ι U
≤ O (—) .	(620)
∖m )
80
Published as a conference paper at ICLR 2022
By Claim 2, the second term is bounded by:
Then
qz, . n	2a；|b；I 同区∖ lbi' l/ * \
3(k+I)maχ (0)∣ i---工hMj,xi IbTThw`,xi
'	Mig 1 I σ j∈A	Ib' i
.q∕7 1 ʌ	2a*lb*1 1 ai',βγV^/八Zf ∖ |bigl lσ∖^/八Zf \
≤ 3(k+1) max	I 节一WMj Xi- SkibiT 鼻Mj Xi
≤ 3(k + 1)max学卜k“%切-1
-	`	MO)I 8k怵I I σ2Ib(O)I
≤ 3(k + 1) max O(α*ea)
≤ O (k%a)∙
hMj,xi
j∈A
Ig(X)- 2g*(x)| = O (----+ k2Ea) ≤ 1∙
m
(621)
(622)
(623)
(624)
(625)
(626)
This guarantees yg(x) ≥ 1. Changing the scaling of δ leads to the statement.
Finally, the bounds on a follow from the above calculation. The bound on Ila⑵ k2 follows from
Lemma 47, and those on kwi(2) k2 and kbi(2) k2 follow from (570)(571) and the bounds on ai(1) and
b( I) in Lemma 43.	口
F.9 Classifier Learning S tage and Main Theorem
Once we have a good set of features in Lemma 48, we can follow exactly the same argument as in
Section B.6 and B.7 for the simplified setting, and arrive at the main theorem for the general setting:
Theorem 49 (Restatement of Theorem 34). Set
2
η⑴=Ykm^, λa1) = 0, λW1) = 1/(2n(1)), σξ1) = 1/k3/2,	(627)
η⑵=1, λa2) = λW2) = 1∕(2η(2)), σξ2) = 1/k3/2,	(628)
k2	k3
η⑴=η =	1/3 ,λa)= λW = λ ≤ T-1/3 ,σξ = 0, for2 < t ≤ T∙ (629)
T m1/3	σm1/3 ξ
For any δ ∈ (0, O(1∕k3)), if μ ≤ O(√d∕D), σg ≤ O(min{1∕σ, σ∕√d}), k = Ω (log2 (^黑:))，
m ≥ max{Ω(k4), D, d} ,then we have for any D ∈ Fξ , with probability at least 1 一 δ ,there exists
t ∈ [T] such that
Pr[sign(g(t)(x)) = y] ≤ LD(g(t)) = O (-^ki3 + k-T + 卜 m / ) ∙	(630)
m2/3	m2	T
Consequently, for any E ∈ (0,1), if T = m4/3, and m ≥ max{Ω(k12∕e3/2), D}, then
Pr[sign(g(t)(X)) 6= y] ≤ LD(g(t)) ≤ ∙	(631)
81