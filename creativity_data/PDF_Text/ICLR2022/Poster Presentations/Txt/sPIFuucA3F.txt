Published as a conference paper at ICLR 2022
Offline Neural Contextual Bandits:
Pessimism, Optimization and Generalization
*
Thanh Nguyen-Tang
Applied AI Institute
Deakin University
Sunil Gupta
Applied AI Institute
Deakin University
A. Tuan Nguyen
Department of Engineering Science
University of Oxford
Svetha Venkatesh
Applied AI Institute
Deakin University
Ab stract
Offline policy learning (OPL) leverages existing data collected a priori for policy
optimization without any active exploration. Despite the prevalence and recent
interest in this problem, its theoretical and algorithmic foundations in function
approximation settings remain under-developed. In this paper, we consider this
problem on the axes of distributional shift, optimization, and generalization in
offline contextual bandits with neural networks. In particular, we propose a prov-
ably efficient offline contextual bandit with neural network function approxima-
tion that does not require any functional assumption on the reward. We show that
our method provably generalizes over unseen contexts under a milder condition
for distributional shift than the existing OPL works. Notably, unlike any other
OPL method, our method learns from the offline data in an online manner using
stochastic gradient descent, allowing us to leverage the benefits of online learning
into an offline setting. Moreover, we show that our method is more computa-
tionally efficient and has a better dependence on the effective dimension of the
neural network than an online counterpart. Finally, we demonstrate the empirical
effectiveness of our method in a range of synthetic and real-world OPL problems.
1	Introduction
We consider the problem of offline policy learning (OPL) (Lange et al., 2012; Levine et al., 2020)
where a learner infers an optimal policy given only access to a fixed dataset collected a priori by
unknown behaviour policies, without any active exploration. There has been growing interest in this
problem recently, as it reflects a practical paradigm where logged experiences are abundant but an
interaction with the environment is often limited, with important applications in practical settings
such as healthcare (Gottesman et al., 2019; Nie et al., 2021), recommendation systems (Strehl et al.,
2010; Thomas et al., 2017), and econometrics (Kitagawa & Tetenov, 2018; Athey & Wager, 2021).
Despite the importance of OPL, theoretical and algorithmic progress on this problem has been rather
limited. Specifically, most existing works are restricted to a strong parametric assumption of envi-
ronments such as tabular representation (Yin & Wang, 2020; Buckman et al., 2020; Yin et al., 2021;
Yin & Wang, 2021; Rashidinejad et al., 2021; Xiao et al., 2021) and more generally as linear models
(Duan & Wang, 2020; Jin et al., 2020; Tran-The et al., 2021). However, while the linearity assump-
tion does not hold for many problems in practice, no work has provided a theoretical guarantee and
a practical algorithm for OPL with neural network function approximation.
In OPL with neural network function approximation, three fundamental challenges arise:
Distributional shift. As OPL is provided with only a fixed dataset without any active exploration,
there is often a mismatch between the distribution of the data generated by a target policy and that of
the offline data. This distributional mismatch can cause erroneous value overestimation and render
* Email: nguyent2 7 92@gmail.com.
1
Published as a conference paper at ICLR 2022
many standard online policy learning methods unsuccessful (Fujimoto et al., 2019). To guarantee
an efficient learning under distributional shift, common analyses rely on a sort of uniform data cov-
erage assumptions (Munos & Szepesvari, 2008; Chen & Jiang, 2019; Brandfonbrener et al., 2021;
Nguyen-Tang et al., 2021) that require the offline policy to be already sufficiently explorative over
the entire state space and action space. To mitigate this strong assumption, a pessimism principle
that constructs a lower confidence bound of the reward functions for decision-making (Rashidinejad
et al., 2021) can reduce the requirement to a single-policy concentration condition that requires the
coverage of the offline policy only on the target policy. However, Rashidinejad et al. (2021) only
uses this condition for tabular representation and it is unclear whether complex environments such as
ones that require neural network function approximation can benefit from this condition. Moreover,
the single-policy concentration condition still requires the offline policy to be stationary (e.g., the
actions in the offline data are independent and depend only on current state). However, this might
not hold for many practical scenarios, e.g., when the offline data was collected by an active learner
(e.g., by an Q-learning algorithm). Thus, it remains unclear what is a minimal structural assumption
on the distributional shift that allows a provably efficient OPL algorithm.
Optimization. Solving OPL often involves in fitting a model into the offline data via optimization.
Unlike in simple function approximations such as tabular representation and linear models where
closed-form solutions are available, OPL with neural network function approximation poses an addi-
tional challenge that involves a non-convex, non-analytical optimization problem. However, existing
works of OPL with function approximation ignore such optimization problems by assuming free ac-
cess to an optimization oracle that can return a global minimizer (Brandfonbrener et al., 2021; Duan
et al., 2021; Nguyen-Tang et al., 2021) or an empirical risk minimizer with a pointwise convergence
bound at an exponential rate (Hu et al., 2021). This is largely not the case in practice, especially in
OPL with neural network function approximation where a model is trained by gradient-based meth-
ods such as stochastic gradient descents (SGD). Thus, to understand OPL in more practical settings,
it is crucial to consider optimization in design and analysis. To our knowledge, such optimization
problem has not been studied in the context of OPL with neural network function approximation.
Generalization. In OPL, generalization is the ability to generalize beyond the states (or contexts as
in the specific case of stochastic contextual bandits) observed in the offline data. In other words, an
offline policy learner with good generalization should obtain high rewards not only in the observed
states but also in the entire (unknown) state distribution. The challenge of generalization in OPL
is that as we learn from the fixed offline data, the learned policy has highly correlated structures
where we cannot directly use the standard concentration inequalities (e.g. Hoeffding’s inequality,
Bernstein inequality) to derive a generalization bound. The typical approaches to overcome this
difficulty are data splitting and uniform convergence. While data splitting splits the offline data
into disjoint folds to break the correlated structures (Yin & Wang, 2020), uniform convergence
establishes generalization uniformly over a class of policies learnable by a certain model (Yin et al.,
2021). However, in the setting where the offline data itself can have correlated structures (e.g., an
offline action can depend on the previous offline data) and the model used is sufficiently large that
renders a uniform convergence bound vacuous, neither the data splitting technique in (Yin & Wang,
2020) nor the uniform convergence argument in (Yin et al., 2021) yield a good generalization. Thus,
it is highly non-trivial to guarantee a strong generalization in OPL with neural network function
approximation from highly correlated offline data.
In this paper, we consider the problem of OPL with neural network function approximation on the
axes of distributional shift, optimization and generalization via studying the setting of stochastic
contextual bandits with overparameterized neural networks. Specifically, we make three contribu-
tions toward enabling OPL in more practical settings:
•	First, we propose an algorithm that uses a neural network to model any bounded reward
function without assuming any functional form (e.g., linear models) and uses a pessimistic
formulation to deal with distributional shifts. Notably, unlike any standard offline learning
methods, our algorithm learns from the offline data in an online-like manner, allowing us
to leverage the advantages of online learning into offline setting.
•	Second, our theoretical contribution lies in making the generalization bound of OPL more
realistic by taking into account the optimization aspects and requiring only a milder con-
dition for distributional shifts. In particular, our algorithm uses stochastic gradient descent
and updates the network completely online, instead of retraining it from scratch for every
2
Published as a conference paper at ICLR 2022
iteration. Moreover, the distributional shift condition in our analysis, unlike in the exist-
ing works, does not require the offline policy to be uniformly explorative or stationary.
Specifically, we prove that, under mild conditions with practical considerations above, our
algorithm learns the optimal policy with an expected error of O(Kd1/2n-1/2), where n
is the number of offline samples, κ measures the distributional shift, and d is an effective
dimension of the neural network that is much smaller than the network capacity (e.g., the
network’s VC dimension and Rademacher complexity).
•	Third, we evaluate our algorithm in a number of synthetic and real-world OPL benchmark
problems, verifying its empirical effectiveness against the representative methods of OPL.
Notation. We use lower case, bold lower case, and bold upper case to represent scalars, vectors
and matrices, respectively. For a vector v = [v1 , . . . , vd]T ∈ Rd and p > 1, denote kvkp =
(Pd=I Vp )1/p and let [v]j be the jth element of v. For a matrix A = (Aij)m×n, denote k A∣∣f =
JPij Atj, kAkp = maxv：kvkp=i ∣∣Avkp, kAk∞ = maxp,j |Ap,j| and let VeC(A) ∈ Rmn be
the vectorized representation of A. For a square matrix A, a vector v, and a matrix X, denote
∣∣v∣A = √vT Av and ∣∣X ∣∣a = ∣∣ vec(X )∣a. For a collection of matrices W = (Wi,..., WL)
and a square matrix A, denote ∣∣W ∣∣f = qPL=1 ∣Wι∣F, and ∣∣W ∣∣a = qPL=1 ∣Wι∣A. For a
collection of matrices W (0) = (Wi(0),. .., WL(0)), denote B(W (0), R) = {W = (Wi, .. .,WL) :
Il Wl 一 Wl⑼ ∣f ≤ R}. Denote [n] = {1, 2,..., n}, and a ∨ b = max{a, b}. We write O(∙) to hide
logarithmic factors in the standard Big-Oh notation, and write m ≥ Θ(f (∙)) to indicate that there is
an absolute constant C > 0 that is independent of any problem parameters (∙) such that m ≥ Cf (∙).
2	Background
In this section, we provide essential background on offline stochastic contextual bandits and overpa-
rameterized neural networks.
2.1	Stochastic Contextual Bandits
We consider a stochastic K-armed contextual bandit where at each round t, an online learner ob-
serves a full context xt := {xt,a ∈ Rd : a ∈ [K]} sampled from a context distribution ρ, takes
an action at ∈ [K], and receives a reward rt 〜P(∙∖xt,αt). A policy ∏ maps a full context (and
possibly other past information) to a distribution over the action space [K]. For each full context
X := {xa ∈ Rd : a ∈ [K]}, we define vπ(x) = Ea〜∏(∙∣χ),r〜P(∙∣xa)[r] and v*(x) = max∏ vπ(x),
which is attainable due to the finite action space.
In the offline contextual bandit setting, the goal is to learn an optimal policy only from an offline
data Dn = {(xt, at, rt)}n=ι collected a priori by a behaviour policy μ. The goodness of a learned
policy ∏ is measured by the (expected) sub-optimality the policy achieves in the entire (unknown)
context distribution ρ:
SubOPt(∏) := Ex〜ρ[SubOpt(Π; x)], where SubOPt(∏; x) := v*(x) — vπ(x).
In this work, we make the following assumption about reward generation: For each t, rt =
h(xt,at) +ξt, where h : Rd → [0, 1] is an unknown reward function, and ξt is a R-subgaussian noise
conditioned on (Dt-1, xt, at) where we denote Dt = {(xτ, aτ, rτ)}1≤τ ≤t, ∀t. The R-subgaussian
noise assumption is standard in stochastic bandit literature (Abbasi-Yadkori et al., 2011; Zhou et al.,
2020; Xiao et al., 2021) and is satisfied e.g. for any bounded noise.
2.2	Overparameterized Neural Networks
To learn the unknown reward function without any prior knowledge about its parametric form, we
approximate it by a neural network. In this section, we define the class of overparameterized neural
networks that will be used throughout this paper. We consider fully connected neural networks with
depth L ≥ 2 defined on Rd as
fw(U) = √mWLσ (WL-iσ (... σ(Wιu)...)) ,∀u ∈ Rd,	(1)
3
Published as a conference paper at ICLR 2022
Algorithm 1 NeuraLCB
Input: Offline data Dn = {(xt, at, rt)}tn=1, step sizes {ηt}tn=1 , regularization parameter λ > 0,
confidence parameters {βt}tn=1.
1:	Initialize W⑼ as follows: set Wl(O) = [W∣, 0; 0, W∣], ∀l ∈ [L - 1] where each entry of W∣
is generated independently from N(0, 4/m), and set WL(0) = [wT, -wT] where each entry of
w is generated independently from N(0, 2/m).
2:	Λo《—λI.
3:	for t = 1, . . . , n do
4:	Retrieve (xt, at, rt) from Dn.
5:	∏t(x) J arg maXα∈[κ] Lt(x。)，for all X = {x。∈ Rd : a ∈ [K]} where Lt(U)=
fw(t-i)(U)- βt-ι∣Nfw(t-i)(U) ∙ m-1/2 3 4 I∣λ-11 , ∀u ∈ Rd
6:	At J Λt-ι + vec(Vfw(t-i)(xt,。：)) ∙ vec(Vfw(t-i)(xt,aJ)T/m.
7:	W(t) J W(t-1) - ηtVLt(W(I)) where Lt(W) = ɪ(fw(xt,。：)一 r) + mIlW -
W(0) I2F.
8:	end for
Output: Randomly sample π uniformly from {∏ι,...,∏n}.
where σ(∙) = max{∙, 0} is the rectified linear unit (ReLU) activation function, Wi ∈ Rm×d, Wi ∈
Rm×m , ∀i ∈ [2, L - 1], WL ∈ Rm×1, and W := (W1, . . . , WL) with vec(W) ∈ Rp where
p = md+m+m2(L- 2). We assume that the neural network is overparameterized in the sense that
the width m is sufficiently larger than the number of samples n. Under such an overparameterization
regime, the dynamics of the training of the neural network can be captured in the framework of so-
called neural tangent kernel (NTK) (Jacot et al., 2018). Overparameterization has been shown to be
effective to study the interpolation phenomenon and neural training for deep neural networks (Arora
et al., 2019; Allen-Zhu et al., 2019; Hanin & Nica, 2019; Cao & Gu, 2019; Belkin, 2021).
3 Algorithm
In this section, we present our algorithm, namely NeuraLCB (which stands for Neural Lower
Confidence Bound). A key idea of NeuraLCB is to use a neural network fW (x。) to learn the
reward function h(x。) and use a pessimism principle based on a lower confidence bound (LCB) of
the reward function (Buckman et al., 2020; Jin et al., 2020) to guide decision-making. The details
of NeuraLCB are presented in Algorithm 1. Notably, unlike any other OPL methods, NeuraLCB
learns in an online-like manner. Specifically, at step t, Algorithm 1 retrieves (xt, at, rt) from the
offline data Dn, computes a lower confidence bound Lt for each context and action based on the cur-
rent network parameter W(t-i), extracts a greedy policy ∏t with respect to Lt, and updates W(t)
by minimizing a regularized squared loss function Lt (W) using stochastic gradient descent. Note
that Algorithm 1 updates the network using one data point at time t, does not use the last sample
(xn, an, rn) for decision-making and takes the average of an ensemble of policies {∏t}n=ι as its
returned policy. These are merely for the convenience of theoretical analysis. In practice, we can
either use the ensemble average, the best policy among the ensemble or simply the latest policy ∏n
as the returned policy. At step t, we can also train the network on a random batch of data from Dt
(the “B-mode” variant as discussed in Section 6).
4 Generalization Analysis
In this section, we analyze the generalization ability of NeuraLCB. Our analysis is built upon the
neural tangent kernel (NTK) (Jacot et al., 2018). We first define the NTK matrix for the neural
network function in Eq. (1).
Definition 4.1 (Jacot et al. (2018); Cao & Gu (2019); Zhou et al. (2020)). Denote {x(i)}in=K1 =
{xt,。∈ Rd : t ∈ [n],a ∈ [K]}, H(,j) = ∑ij) = (x(i), x(j)〉，and
AIj= ς(1)	/j ,球：1)= 2E(u,v)〜N(0,A(lj) [σ(u)σ(v)],
i,j j,j	,
4
Published as a conference paper at ICLR 2022
Hi(I+1) =2H¾)%v)〜N(0,Ailj) [σ0(u)σ0(v)] + 理+1).
The neural tangent kernel (NTK) matrix is then defined as H = (HH(L) + Σ(L) )/2.
Here, the Gram matrix H is defined recursively from the first to the last layer of the neural network
using Gaussian distributions for the observed contexts {x(i)}in=K1. Next, we introduce the assump-
tions for our analysis. First, we make an assumption about the NTK matrix H and the input data.
Assumption 4.1. ∃λ0 > 0, H λ0I, and ∀i ∈ [nK], kx(i) k2 = 1. Moreover, [x(i)]j =
[x(i)]j+d/2,∀i ∈ [nK],j ∈ [d/2].
The first part of Assumption 4.1 assures that H is non-singular and that the input data lies in the unit
sphere Sd-1. Such assumption is commonly made in overparameterized neural network literature
(Arora et al., 2019; Du et al., 2019b;a; Cao & Gu, 2019). The non-singularity is satisfied when e.g.
any two contexts in {x(i)} are not parallel (Zhou et al., 2020), and our analysis holds regardless of
whether λ0 depends on n. The unit sphere condition is merely for the sake of analysis and can be
relaxed to the case that the input data is bounded in 2-norm. As for any input data point x such that
∣∣xk2 = 1 We can always construct a new input x0 = 表[x, x]T, the second part of Assumption
4.1 is mild and used merely for the theoretical analysis (Zhou et al., 2020). In particular, under
Assumption 4.1 and the initialization scheme in Algorithm 1, we have fW (0) (x(i)) = 0, ∀i ∈ [nK].
Next, we make an assumption on the data generation.
Assumption 4.2. ∀t, xt is independent of Dt-1, and ∃κ ∈ (0, ∞),
π*(Txt)
μGlDt-ι,Xt)I∣∞
≤ κ, ∀t ∈ [n].
The first part of Assumption 4.2 says that the full contexts are generated by a process independent of
any policy. This is minimal and standard in stochastic contextual bandits (Lattimore & Szepesvari,
2020; Rashidinejad et al., 2021; Papini et al., 2021), e.g., when {xt}f=ι ,叱 ρ. The second part
of Assumption 4.2, namely empirical single-policy concentration (eSPC) condition, requires that
the behaviour policy μ has sufficient coverage over only the optimal policy ∏* in the observed con-
texts. Our data coverage condition is significantly milder than the common uniform data coverage
assumptions in the OPL literature (Munos & Szepesvari, 2008; Chen & Jiang, 2019; Brandfonbrener
et al., 2021; Jin et al., 2020; Nguyen-Tang et al., 2021) that requires the offline data to be sufficiently
explorative in all contexts and all actions. Moreover, our data coverage condition can be considered
as an extension of the single-policy concentration condition in (Rashidinejad et al., 2021) where
both require coverage over the optimal policy. However, the remarkable difference is that, unlike
(Rashidinejad et al., 2021), the behaviour policy μ in our condition needs not to be stationary and the
concentration is only defined on the observed contexts; that is, at can be dependent on both xt and
Dt-1. This is more practical as it is natural that the offline data was collected by an active learner
such as a Q-learning agent (Mnih et al., 2015).
■‰ τ	FC	.1	T	∙	Γ∙ .l XTETk	, ∙	. 1	1	F F ,	V
Next, we define the effective dimension of the NTK matrix on the observed data as d =
lOOdet+I+KH/；). This notion of effective dimension was used in (Zhou et al., 2020) for online neural
contextual bandits while a similar notion was introduced in (Valko et al., 2013) for online kernelized
contextual bandits, and was also used in (Yang & Wang, 2020; Yang et al., 2020) for online kernel-
ized reinforcement learning. Although being in offline policy learning setting, the online-like nature
of NeuraLCB allows us to leverage the usefulness of the effective dimension. Intuitively, d measures
1	∙ 11	, 1	1	C TT F	TΓ∏	ITlFF	1	∙ ,1	∙	11	1
how quickly the eigenvalues of H decays. For example, d only depends on n logarithmically when
the eigenvalues of H have a finite spectrum (in this case d is smaller than the number of spectrum
which is the dimension of the feature space) or are exponentially decaying (Yang et al., 2020). We
are now ready to present the main result about the sub-optimality bound of NeuraLCB.
Theorem 4.1. For any δ ∈ (0, 1), under Assumption 4.1 and 4.2, if the network width m, the regu-
larization parameter λ, the confidence parameters {βt} and the learning rates {ηt} in Algorithm 1
satisfy
m ≥ poly(n, L, K, λ-1, λ-1, log(1∕δ)),	λ ≥ max{1, Θ(L)},
βt = Jλ + CtL ∙ (t1∕2λ-1∕2 + (nK)1∕2λ-1∕2) • m-1/2 for some absolute constant C3,
ηt = -√≡, where ι-1 = Q(n2/3m5/6λ-1∕6L17∕6 log1/2 m) ∨ Ω(mλ1/2 log1/2(nKL2(10n + 4)∕δ)),
5
Published as a conference paper at ICLR 2022
then with probability at least 1 一 δ over the randomness of W(0) and Dn, the sub-optimality of π
returned by Algorithm 1 is bounded as
n ∙ E [SubOpt(Π)] ≤ κ√ny d^log(1 + nK∕λ) + 2 + κ√n + 2+ p2n log((10n + 4)∕δ),
where d is the effective dimension of the NTK matrix, and κ is the empirical single-policy concen-
tration (eSPC) coefficient in Assumption 4.2.
Our bound can be further simplified as E[SubOpt(Π)] = O(K ∙ max{ʌ/d, 1}∙ n-1/2). A detailed
proof for Theorem 4.1 is omitted to Section A. We make several notable remarks about our re-
suit. First, our bound does not scale linearly with P or √p as it would if the classical analyses
(Abbasi-Yadkori et al., 2011; Jin et al., 2020) had been applied. Such a classical bound is vacu-
ous for overparameterized neural networks where p is significantly larger than n. Specifically, the
online-like nature of NeuraLCB allows us to leverage a matrix determinant lemma and the notion of
effective dimension in online learning (Abbasi-Yadkori et al., 2011; Zhou et al., 2020) which avoids
the dependence on the dimension p of the feature space as in the existing OPL methods such as (Jin
,	1 ACAC' <'l	1	KFiri ∙ , 1	. / V 1	T 1	1	1	∙ , 1	∙	11	∙ ,1
et al., 2020). Second, as our bound scales linearly with	d where d scales only logarithmically with
n in common cases (Yang et al., 2020), our bound is sublinear in such cases and presents a provably
efficient generalization. Third, our bound scales linearly with κ which does not depend on the cov-
erage of the offline data on other actions rather than the optimal ones. This eliminates the need for a
strong uniform data coverage assumption that is commonly used in the offline policy learning liter-
ature (Munos & Szepesvari, 2008; Chen & Jiang, 2019; Brandfonbrener et al., 2021; Nguyen-Tang
et al., 2021). Moreover, the online-like nature of our algorithm does not necessitate the stationar-
ity of the offline policy, allowing an offline policy with correlated structures as in many practical
scenarios. Note that Zhan et al. (2021) have also recently addressed the problem of off-policy eval-
uation with such offline adaptive data but used doubly robust estimators instead of direct methods
as in our paper. Fourth, compared to the regret bound for online learning setting in (Zhou et al.,
2020), We achieve an improvement by a factor ofwhile reducing the computational complexity
from O(n2) to O(n). On a more technical note, a key idea to achieve such an improvement is to
directly regress toward the optimal parameter of the neural network instead of toward the empirical
risk minimizer as in (Zhou et al., 2020).
Finally, to further emphasize the significance of our theoretical result, we summarize and compare
it with the state-of-the-art (SOTA) sub-optimality bounds for OPL with function approximation in
Table 1. From the leftmost to the rightmost column, the table describes: the related works - the
function approximation - the types of algorithms where Pessimism means a pessimism principle
based on a lower confidence bound of the reward function while Greedy indicates being uncertainty-
agnostic (i.e., an algorithm takes an action with the highest predicted score in a given context) 一
the optimization problems in OPL where Analytical means optimization has an analytical solution,
Oracle means the algorithm relies on an oracle to obtain the global minimizer, and SGD means
the optimization is solved by stochastic gradient descent - the sub-optimality bounds - the data
coverage assumptions where Uniform indicates sufficiently explorative data over the context and
action spaces, SPC is the single-policy concentration condition, and eSPC is the empirical SPC
- the nature of data generation required for the respective guarantees where I (Independent) means
that the offline actions must be sampled independently while D (Dependent) indicates that the offline
actions can be dependent on the past data. It can be seen that our result has a stronger generalization
under the most practical settings as compared to the existing SOTA generalization theory for OPL.
We also remark that the optimization design and guarantee in NeuraLCB (single data point SGD)
are of independent interest that do not only apply to the offline setting but also to the original online
setting in (Zhou et al., 2020) to improve their regret and optimization complexity.
5	Related Work
OPL with function approximation. Most OPL works, in both bandit and reinforcement learning
settings, use tabular representation (Yin & Wang, 2020; Buckman et al., 2020; Yin et al., 2021;
Yin & Wang, 2021; Rashidinejad et al., 2021; Xiao et al., 2021) and linear models (Duan & Wang,
6
Published as a conference paper at ICLR 2022
Table 1: The SOTA generalization theory of OPL with function approximation.
Work	Function	Type	Optimization	Sub-optimality	Data Coverage	Data Gen.
Yin & Wang (2020)a	Tabular	Greedy	Analytical	O IpX7∙K ∙ n-1/2j	Uniform	I
Rashidinejad et al. (2021)	Tabular	Pessimism	Analytical	O (ρ∣XT^ “T/2)	SPC	I
Duan & Wang (2020)b	Linear	Greedy	Analytical	O (K ∙ n-1/2 + d ∙ n-1)	Uniform	I
Jin et al. (2020)	Linear	Pessimism	Analytical	O (d ∙ n-1/2)	Uniform	I
Nguyen-Tang et al. (2021)	Narrow ReLU	Greedy	Oracle	O (√K ∙ n- 2(α+d j	Uniform	I
This work	Wide ReLU	Pessimism	SGD	O(K ∙ -Pd ∙ n-1/2)	eSPC	I/D
a,b The bounds of these works are for off-policy evaluation which is generally easier than OPL problem.
2020; Jin et al., 2020; Tran-The et al., 2021). The most related work on OPL with neural function
approximation we are aware of are (Brandfonbrener et al., 2021; Nguyen-Tang et al., 2021; Uehara
et al., 2021). However, Brandfonbrener et al. (2021); Nguyen-Tang et al. (2021); Uehara et al. (2021)
rely on a strong uniform data coverage assumption and an optimization oracle to the empirical
risk minimizer. Other OPL analyses with general function approximation (Duan et al., 2021; Hu
et al., 2021) also use such optimization oracle, which limit the applicability of their algorithms and
analyses in practical settings.
To deal with nonlinear rewards without making strong functional assumptions, other approaches
rather than neural networks have been considered, including a family of experts (Auer, 2003), a
reduction to supervised learning (Langford & Zhang, 2007; Agarwal et al., 2014), and nonparametric
models (Kleinberg et al., 2008; Srinivas et al., 2009; Krause & Ong, 2011; Bubeck et al., 2011; Valko
et al., 2013). However, they are all in the online policy learning setting instead of the OPL setting.
Moreover, these approaches have time complexity scaled linearly with the number of experts, rely
the regret on an oracle, and have cubic computational complexity, respectively, while our method
with neural networks are both statically and computationally efficient where it achieves a √n-type
suboptimality bound and only linear computational complexity.
Neural networks. Our work is inspired by the theoretical advances of neural networks and their
subsequent application in online policy learning (Yang et al., 2020; Zhou et al., 2020; Xu et al.,
2020). For optimization aspect, (stochastic) gradient descents can provably find global minima of
training loss of neural networks (Du et al., 2019a;b; Allen-Zhu et al., 2019; Nguyen, 2021). For
generalization aspect, (stochastic) gradient descents can train an overparameterized neural network
to a regime where the neural network interpolates the training data (i.e., zero training error) and has
a good generalization ability (Arora et al., 2019; Cao & Gu, 2019; Belkin, 2021).
Regarding the use of neural networks for policy learning, NeuraLCB is similar to the NeuralUCB
algorithm (Zhou et al., 2020), that is proposed for the setting of online contextual bandits with neu-
ral networks, in the sense that both algorithms use neural networks, learn with a streaming data
and construct a (lower and upper, respectively) confidence bound of the reward function to guide
decision-making. Besides the apparent difference of offline and online policy learning problems,
the notable difference of NeuraLCB from NeuralUCB is that while NeuralUCB trains a new neu-
ral network from scratch at each iteration (for multiple epochs), NeuraLCB trains a single neural
network completely online. That is, NeuraLCB updates the neural network in light of the data at
a current iteration from the trained network of the previous iteration. Such optimization scheme in
NeuraLCB greatly reduces the computational complexity from O(n2) 1 to O(n), while still guaran-
teeing a provably efficient algorithm. Moreover, NeuraLCB achieves a suboptimality bound with a
better dependence on the effective dimension than NeuralUCB.
6	Experiments
In this section, we evaluate NeuraLCB and compare it with five representative baselines: (1)
LinLCB (Jin et al., 2020), which also uses LCB but relies on linear models, (2) KernLCB,
which approximates the reward using functions in a RKHS and is an offline counterpart of Ker-
nelUCB (Valko et al., 2013), (3) NeuralLinLCB, which is the same as LinLCB except that it uses
1In practice, at each time step t, NeuralUCB trains a neural network for t epochs using gradient descent in
the entire data collected up to time t.
7
Published as a conference paper at ICLR 2022
2000	4000	6000	8000
Number ofsamp∣es
0	2000	4000	6000	8000
Number Ofsamples
(a) h(u) = 10(aT u)2	(b) h(u) = uTATAu	(c) h(u) = cos(3aT u)
Figure 1:	The sub-optimality of NeuraLCB versus the baseline algorithms on synthetic datasets.
0(x。) = vec(Vfw(0)(Xa)) as the feature extractor for the linear model where fw(0)is the same
neural network of NeuraLCB at initialization, (4) NeuralLinGreedy, which is the same as Neu-
ralLinLCB except that it relies on the empirical estimate of the reward function for decision-making,
and (5) NeuralGreedy, which is the same as NeuraLCB except that NeuralGreedy makes decision
based on the empirical estimate of the reward. For more details and completeness, we present the
pseudo-code of these baseline methods in Section D. Here, we compare the algorithms by their
generalization ability via their expected sub-optimality. For each algorithm, we vary the number of
(offline) samples n from 1 to T where T will be specified in each dataset, and repeat each experiment
for 10 times. We report the mean results and their 95% confidence intervals. 2
Approximation. To accelerate computation, we follow (Riquelme et al., 2018; Zhou et al., 2020)
to approximate large covariance matrices and expensive kernel methods. Specifically, as NeuraLCB
and NeuralLinLCB involve computing a covariance matrix Λt of size p × p where p = md + m +
m2(L - 2) is the number of the neural network parameters which could be large, we approximate
Λt by its diagonal. Moreover, as KernLCB scales cubically with the number of samples, we use
KernLCB fitted on the first 1, 000 samples if the offline data exceeds 1, 000 samples.
Data generation. We generate offline actions using a fixed -greedy policy with respect to the true
reward function of each considered contextual bandit, where is set to 0.1 for all experiments in
this section. In each run, we randomly sample nte = 10, 000 contexts from ρ and use this same test
contexts to approximate the expected sub-optimality of each algorithm.
Hyperparameters. We fix λ = 0.1 for all algorithms. For NeuraLCB, we set βt = β, and for Neu-
raLCB, LinLCB, KernLCB, and NeuralLinLCB, we do grid search over {0.01, 0.05, 0.1, 1, 5, 10}
for the uncertainty parameter β. For KernLCB, we use the radius basis function (RBF) kernel with
parameter σ and do grid search over {0.1, 1, 10} for σ. For NeuraLCB and NeuralGreedy, we use
Adam optimizer (Kingma & Ba, 2014) with learning rate η grid-searched over {0.0001, 0.001} and
set the l2-regularized parameter to 0.0001. For NeuraLCB, for each Dt, We use ∏t as its final re-
turned policy instead of averaging over all policies {∏τ}T=ι∙ Moreover, we grid search NeuraLCB
and NeuralGreedy over two training modes, namely {S-mode, B-mode} where at each iteration t, S-
mode updates the neural network for one step of SGD (one step of Adam update in practice) on one
single data point (xt, at, rt) while B-mode updates the network for 100 steps of SGD on a random
batch of size 50 of data Dt (details at Algorithm 7). We remark that even in the B-mode, NeuraLCB
is still more computationally efficient than its online counterpart NeuralUCB (Zhou et al., 2020) as
NeuraLCB reuses the neural network parameters from the previous iteration instead of training it
from scratch for each new iteration. For NeuralLinLCB, NeuralLinGreedy, NeuraLCB, and Neural-
Greedy, we use the same network architecture with L = 2 and add layer normalization (Ba et al.,
2016) in the hidden layers. The network width m will be specified later based on datasets.
6.1	Synthetic Datasets
For synthetic experiments, we evaluate the algorithms on contextual bandits with the synthetic non-
linear reward functions h used in (Zhou et al., 2020):
h1(u) = 10(uT a)2, h2(u) = uT AT Au, h3(u) = cos(3uT a),
2Our code repos: https://github.com/thanhnguyentang/offline_neural_bandits.
8
Published as a conference paper at ICLR 2022
6 5 4 3 2
ff="ul-50-qns
20® «00 6000 80® IOOOO 12000 1««
Numberofsamples
fr≡f ⅛OA-5
NeuraUCB
NeuralGreedy
UnLCB
NeuralLlnLCB
NeuralLlnGreedy
KemLCB
02 ■[..................................
O 2000 4000 6000 808 10000 12000 l«00
NUmber QfSamPIeS
----NeureLCB
----NeuralGreedy
----UnLCB
NeUnHLInLCH
NeuraiLJnGreedy
⅞SE⅛0i5
NeUralG eeαy
——UnLCB
NeuralU
NeuralUnGneedy
KernIJCH
0	2000 4000 6000 8000 10000 12000 14000	。	2000 4d00 68。βO<M IOOOO 12000 MQOO
Number of samples	Numberofsampies
o
(a) Mushroom	(b) Statlog	(c) Adult	(d) MNIST
Figure 2:	The sub-optimality of NeuraLCB versus the baseline algorithms on real-world datasets.
where a ∈ Rd is randomly generated from uniform distribution over the unit sphere, and each entry
of A ∈ Rd×d is independently and randomly generated from N (0, 1). For each reward function hi,
rt = hi(xt,at) + ξt where ξt 〜N(0,0.1). The context distribution P for three cases is the uniform
distribution over the unit sphere. All contextual bandit instances have context dimension d = 20
and K = 30 actions. Moreover, we choose the network width m = 20 and the maximum number
of samples T = 10, 000 for the synthetic datasets.
6.2	Real-world Datasets
We evaluate the algorithms on real-world datasets from UCI Machine Learning Repository (Dua
& Graff, 2017): Mushroom, Statlog, and Adult, and MNIST (LeCun et al., 1998). They repre-
sent a good range of properties: small versus large sizes, dominating actions, and stochastic versus
deterministic rewards (see Section E in the appendix for details on each dataset). Besides the Mush-
room bandit, Statlog, Adult, and MNIST are K-class classification datasets, which we convert into
K-armed contextual bandit problems, following (Riquelme et al., 2018; Zhou et al., 2020). Specif-
ically, for each input x ∈ Rd in a K-class classification problem, we create K contextual vectors
x1 = (x, 0, . . . , 0), . . . , xK = (0, . . . , 0, x) ∈ RdK. The learner receives reward 1 if it selects
context xy where y is the label of x, and receives reward 0 otherwise. Moreover, we choose the
network width m = 100 and the maximum sample number T = 15, 000 for these datasets.
6.3	Results
Figure 1 and 2 show the expected sub-optimality of all algorithms on synthetic datasets and real-
world datasets, respectively. First, due to the non-linearity of the reward functions, methods with
linear models (LinLCB, NeuralLinLCB, NeuralLinGreedy, and KernLCB) fail in almost all tasks
(except that LinLCB and KernLCB have competitive performance in Mushroom and Adult, respec-
tively). In particular, linear models using neural network features without training (NeuralLinLCB
and NeuralLinGreedy) barely work in any datasets considered here. In contrast, our method Neu-
raLCB outperforms all the baseline methods in all tasks. We remark that NeuralGreedy has a sub-
stantially lower sub-optimality in synthetic datasets (and even a comparable performance with Neu-
raLCB in h1) than linear models, suggesting the importance of using trainable neural representation
in highly non-linear rewards instead of using linear models with fixed feature extractors. On the
other hand, our method NeuraLCB outperforms NeuralGreedy in real-world datasets by a large mar-
gin (even though two methods are trained exactly the same but different only in decision-making),
confirming the effectiveness of pessimism principle in these tasks. Second, KernLCB performs
reasonably in certain OPL tasks (Adult and slightly well in Statlog and MNIST), but the cubic com-
putational complexity of kernel methods make it less appealing in OPL with offline data at more than
moderate sizes. Note that due to such cubic computational complexity, we follow (Riquelme et al.,
2018; Zhou et al., 2020) to learn the kernel in KernLCB for only the first 1, 000 samples and keep
the fitted kernel for the rest of the data (which explains the straight line of KernLCB after n = 1, 000
in our experiments). Our method NeuraLCB, on the other hand, is highly computationally efficient
as the computation scales only linearly with n (even in the B-mode - a slight modification of the
original algorithm to include batch training). In fact, in the real-world datasets above, the S-mode
(which trains in a single data point for one SGD step at each iteration) outperforms the B-mode, fur-
ther confirming the effectiveness of the online-like nature in NeuraLCB. In Section F, we reported
the performance of S-mode and B-mode together and evaluated on dependent offline data.
9
Published as a conference paper at ICLR 2022
7 Acknowledgement
This research was partially funded by the Australian Government through the Australian Re-
search Council (ARC). Prof. Venkatesh is the recipient of an ARC Australian Laureate Fellowship
(FL170100006).
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for lin-
ear stochastic bandits. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fer-
nando C. N. Pereira, and Kilian Q. Weinberger (eds.), Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Sys-
tems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pp.
2312-2320, 2011. URL https://proceedings.neurips.cc/paper/2011/hash/
e1d5be1c7f2f456670de3d53c7b54f4a- Abstract.html.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning, pp. 1638-1646. PMLR, 2014.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Susan Athey and Stefan Wager. Policy learning with observational data. Econometrica, 89(1):
133-161, 2021.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res.,
3(null):397-422, March 2003. ISSN 1532-4435.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the
prism of interpolation. arXiv preprint arXiv:2105.14368, 2021.
David Brandfonbrener, William Whitney, Rajesh Ranganath, and Joan Bruna. Offline contextual
bandits with overparameterized models. In International Conference on Machine Learning, pp.
1049-1058. PMLR, 2021.
SebaStien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed bandits. Journal of
Machine Learning Research, 12(5), 2011.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. arXiv preprint arXiv:2009.06799, 2020.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line
learning algorithms. IEEE Trans. Inf. Theory, 50(9):2050-2057, 2004. doi: 10.1109/TIT.2004.
833339. URL https://doi.org/10.1109/TIT.2004.833339.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 1042-1051. PMLR,
2019.
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In ICML, volume 97 of Proceedings of Machine Learning
Research, pp. 1675-1685. PMLR, 2019a.
10
Published as a conference paper at ICLR 2022
Simon S. Du, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent ProVably optimizes
over-parameterized neural networks. In ICLR (Poster). OpenReview.net, 2019b.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy eValuation with linear function approx-
imation. CoRR, abs/2002.09516, 2020.
Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. arXiv preprint arXiv:2103.13883, 2021.
Scott Fujimoto, DaVid Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, DaVid Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nature
medicine, 25(1):16-18, 2019.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv
preprint arXiv:1909.05989, 2019.
Yichun Hu, Nathan Kallus, and Masatoshi Uehara. Fast rates for the regret of offline reinforcement
learning. In COLT, Volume 134 of Proceedings of Machine Learning Research, pp. 2462. PMLR,
2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? CoRR,
abs/2012.15085, 2020. URL https://arxiv.org/abs/2012.15085.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximiza-
tion methods for treatment choice. Econometrica, 86(2):591-616, 2018. doi: https://doi.
org/10.3982/ECTA13288. URL https://onlinelibrary.wiley.com/doi/abs/10.
3982/ECTA13288.
Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In
Proceedings of the fortieth annual ACM symposium on Theory of computing, pp. 681-690, 2008.
Andreas Krause and Cheng Soon Ong. Contextual gaussian process bandit optimization. In Nips,
pp. 2447-2455, 2011.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits.
Advances in neural information processing systems, 20(1):96-1, 2007.
Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2020. doi:
10.1017/9781108571401.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
11
Published as a conference paper at ICLR 2022
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. J. Mach. Learn.
Res.,9:815-857, 2008.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. arXiv preprint arXiv:2101.09612, 2021.
Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. Sample complexity of
offline reinforcement learning with deep relu networks, 2021.
Xinkun Nie, Emma Brunskill, and Stefan Wager. Learning when-to-treat policies. Journal of the
American Statistical Association, 116(533):392-409, 2021.
Matteo Papini, Andrea Tirinzoni, Marcello Restelli, Alessandro Lazaric, and Matteo Pirotta. Lever-
aging good representations in linear contextual bandits. arXiv preprint arXiv:2104.03781, 2021.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An
empirical comparison of bayesian deep networks for thompson sampling. arXiv preprint
arXiv:1802.09127, 2018.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995, 2009.
Alex Strehl, John Langford, Sham Kakade, and Lihong Li. Learning from logged implicit explo-
ration data. arXiv preprint arXiv:1003.0120, 2010.
Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh, Ishan Durugkar, and Emma
Brunskill. Predictive off-policy policy evaluation for nonstationary decision problems, with ap-
plications to digital marketing. In Proceedings of the Thirty-First AAAI Conference on Artificial
Intelligence, AAAI’17, pp. 4740-4745. AAAI Press, 2017.
Hung Tran-The, Sunil Gupta, Thanh Nguyen-Tang, Santu Rana, and Svetha Venkatesh. Combining
online learning and offline learning for contextual bandits with deficient support. arXiv preprint
arXiv:2107.11533, 2021.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and
first-order efficiency. arXiv preprint arXiv:2102.02981, 2021.
Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepesvari, and
Dale Schuurmans. On the optimality of batch policy optimization algorithms. In International
Conference on Machine Learning, pp. 11362-11371. PMLR, 2021.
Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. Neural contextual bandits with deep
representation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. arXiv preprint
arXiv:2011.04622, 2020.
Ming Yin and Yu-Xiang Wang. Asymptotically efficient off-policy evaluation for tabular reinforce-
ment learning. In AISTATS, volume 108 of Proceedings of Machine Learning Research, pp.
3948-3958. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
Ming Yin and Yu-Xiang Wang. Characterizing uniform convergence in offline policy evaluation via
model-based approach: Offline learning, task-agnostic and reward-free, 2021.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline
policy evaluation for reinforcement learning. In Arindam Banerjee and Kenji Fukumizu (eds.),
The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April
13-15, 2021, Virtual Event, volume 130 of Proceedings ofMachine Learning Research, pp. 1567-
1575. PMLR, 2021. URL http://proceedings.mlr.press/v130/yin21a.html.
Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via adap-
tive weighting with data from contextual bandits. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining, pp. 2125-2135, 2021.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, pp. 11492-11502. PMLR, 2020.
13
Published as a conference paper at ICLR 2022
A Proof of Theorem 4.1
In this section, we provide the proof of Theorem 4.1.
Let Dt = {(xτ, aτ, r「)}ι≤τ≤t∙ Note that ∏t returned by Algorithm 1 is Dt-i-measurable. Denote
Et[∙] = EHDt-1, xt]. Let the step sizes {ηt} defined as in Theorem 4.1 and the confidence trade-off
parameters {βt} defined as in Algorithm 1, we present main lemmas below which will culminate
into the proof of the main theorem.
Lemma A.1. There exists absolute constants C1, C2 > 0 such that for any δ ∈ (0, 1), if m satisfies
m ≥ max {θ(nλ-1L11 log6 m), Θ(L6n4K4λ-4 Iog(KL(5n + 1)∕δ))
Θ(L-1 λ1/2(log3/2(nKL2(5n + 1)∕δ) ∨ log-3/2 m)) 1,
then with probability at least 1 - δ, it holds uniformly over all t ∈ [n] that
SubOpt(∏t; Xt) ≤ 2βt-ιEa小∏*(.χ)
[kVfw(t-1) (χt,a* ) ∙ m-1/2 kΛ-11 lDt-1, xt]
+ 2Cit2/3m-1/6 log1/2 mL7∕3λ-1∕2 + 2√2C2√nKλ-1∕2mT1∕6 log1/2 mt1/6L10/3λ-1/6.
Lemma A.1 gives an upper bound on the sub-optimality of the returned step-dependent policy ∏t on
the observed context xt for each t ∈ [n]. We remark that the upper bound depends on the rate at
which the confidence width of the NTK feature vectors shrink along the direction of only the optimal
actions, rather than any other actions. This is an advantage of pessimism where it does not require
the offline data to be informative about any sub-optimal actions. However, the upper bound depends
on the unknown optimal policy ∏* while the offline data has been generated a priori by a different
unknown behaviour policy. This distribution mismatch is handled in the next lemma.
Lemma A.2. There exists an absolute constant C3 > 0 such that for any δ ∈ (0, 1), if m and λ
satisfy
λ ≥ max{1, Θ(L)},	m ≥ max Θ(nλ-1L11 log6 m), Θ(L6n4K4λ0-4 log(nK L(5n + 2)∕δ))
Θ(L-1λ1/2 (log3/2 (nKL2 (5n + 2)∕δ) ∨ log-3/2 m))},
then with probability at least 1 - δ, we have
1n
—Xβt-1Ea*^∏*(∙∣xt) ∣Nfw(t-1) (Xt川) ∙ m "kA-jJDt-1, xt]
n t=1
≤ √βnKq，疝og(1 + nK∕λ) + 1 + 2C2C2n3∕2m-1∕6(logm)1∕2L23∕6λT∕6
n
+ βnκ(C3∕√2)L1∕2λ-1/2 log1∕2((5n + 2)∕δ),
where C2 is from Lemma A.1.
We also remark that the upper bound in Lemma A.2 scales linearly with y∏d instead of with √p if a
standard analysis were applied. This avoids a vacuous bound as p is large with respect to n.
The upper bounds in Lemma A.1 and Lemma A.2 are established for the observed contexts only.
The next lemma generalizes these bounds to the entire context distribution, thanks to the online-
like nature of Algorithm 1 and an online-to-batch argument. In particular, a key technical property
of Algorithm 1 that makes this generalization possible without a uniform convergence is that ∏t is
Dt-1-measurable and independent of (xt, at, rt).
Lemma A.3. For any δ ∈ (0, 1), with probability at least 1 - δ over the randomness of Dn, we have
E [SubOpt(Π)] ≤ 1 Pn=I SubOpt(∏t; xt) + J2 log(1∕δ).
14
Published as a conference paper at ICLR 2022
We are now ready to prove the main theorem.
Proof of Theorem 4.1. Combining Lemma A.1, Lemma A.2, and Lemma A.3 via the union bound,
we have
n ∙ E[SubOpt(Π)] ≤ κ√nΓι JIlog(I + nK∕λ) +Γ2 + κ√nΓ3 +Γ4 +Γ5 + p2n log((10n + 4)∕δ),
≤ κ√ny Ilog(I + nK∕λ) + 2 + κ√n + 2+ P2n log(10n + 4)/δ)
where m is chosen to be sufficiently large as a polynomial of (n, L, K, λ-1,λ-1, log(1∕δ)) such
that
Γι := 2√2,λ + C2nL(n1/2X1/2 + (nK)1/2X-1/2) ∙ m-1/2 ≤ 1
Γ2 :=1 + 2C2C2n3/2m-1/6(logm)1/2L23/6A-1/6 ≤ 2
Γ3 =Γι√n(C3∕√2)L1∕2λ-1∕2 log1/2((10n + 4)∕δ) ≤ 1
Γ4 := 2Cin5/3m-1/6(logm)1/2L7/3A-1/2 ≤ 1
Γ5 := 2√2C2(nK)1/2A-1/2m-11/6(logm)1/2n7/6L10/3XT/6 ≤ 1.
□
B Proof of Lemmas in Section A
B.1 Proof of Lemma A.1
We start with the following lemmas whose proofs are deferred to Appendix B.
Lemma B.1. Let h = [h(x ⑴)，...，h(x(nK))]T ∈ RnK. There exists W * ∈ W such that for
any δ ∈ (0, 1), if m ≥ Θ(L6n4K4λ0-4 log(nK L∕δ)), with probability at least 1 - δ over the
randomness of W (0), it holds uniformly for all i ∈ [nK] that
kW* - W⑼kF ≤√2m-/MkH-I,
(Vfw(0) (x⑴),W * - W ⑼)=h(x⑴).
Remark B.1. Lemma B.1 shows that for a sufficiently wide network, there is a linear model that uses
the gradient of the neural network at initialization as a feature vector and interpolates the reward
function in the training inputs. Moreover, the weights W* of the linear model is in a neighborhood
of the initialization W(0). Note that we also have
s ：= MkH-I ≤ Mk2P∣∣H-1k2 ≤ √nKλ-1∕2,
where the second inequality is by Assumption 4.1 and Cauchy-Schwartz inequality with h(x) ∈
[0, 1], ∀x.
Lemma B.2. For any δ ∈ (0, 1), ifm satisfies
m ≥ Θ(nλ-1L11 log6 m) ∨ Θ(L-1λ1/2 log3∕2(3n2KL2∕δ)),
and the step sizes satisfy
where ι-1 = —(n2/3m5/6XT/6L17/6 log1/2 m) ∨ Ω(Rmλ1/2 log1∕2(n∕δ))
then with probability at least 1 - δ over the randomness of W(0) and D, it holds uniformly for all
t ∈ [n], l ∈ [L] that
IWi⑴-Wl⑼kF ≤ ymλL, and kAtk2 ≤ λ + CtL,
where C3 > 0 is an absolute constant from Lemma C.2.
ι
ηt
15
Published as a conference paper at ICLR 2022
Remark B.2. Lemma B.2 controls the growth dynamics of the learned weights Wt around its ini-
tialization and bounds the spectral norm of the empirical covariance matrix Λt when the model is
trained by SGD.
Lemma B.3 (Allen-Zhu et al. (2019, Theorem 5), Cao & Gu (2019, Lemma B.5)). There exist an
absolute constant C2 > 0 such that for any δ ∈ (0, 1), if ω satisfies
Θ(m-3∕2L-3∕2(log32(nK∕δ)) ∨ log-3/2 m) ≤ ω ≤ Θ(L-9/2 log-3 m),
with probability at least 1 - δ over the randomness of W (0), it holds uniformly for all W ∈
B(W(0); ω) and i ∈ [nK] that
kVfw(X⑴)一Vfw(0)(x⑺)kF ≤ C2Plogmω1∕3L3kVfw(0)(x⑴)kF.
Remark B.3. Lemma B.3 shows that the gradient in a neighborhood of the initialization differs from
the gradient at the initialization by an amount that can be explicitly controlled by the radius of the
neighborhood and the norm of the gradient at initialization.
Lemma B.4 (Cao & Gu (2019, Lemma 4.1)). There exist an absolute constant C1 > 0 such that for
any δ ∈ (0, 1) over the randomness of W (0), if ω satisfies
Θ(m-3∕2L-3∕2 log3∕2(nKL2∕δ)) ≤ ω ≤ Θ(L-6 log-3/2 m),
with probability at least 1 - δ, it holds uniformly for all W , W 0 ∈ B(W (0); ω) and i ∈ [nK] that
|fw0(x(i)) - fw(x(i)) -hVfw(X⑴)，W0 - Wi∣ ≤ Ci ∙ ω4∕3L3Pmlogm.
Remark B.4. Lemma B.4 shows that near initialization the neural network function is almost linear
in terms of its weights in the training inputs.
Proof of Lemma A.1. For all t ∈ [n], u ∈ Rd, we define
Ut(U) = fw(t-i)(u) + βt-ikVfw(t-i)(u) ∙ m-1∕2∣∣Λ二
Lt(U) = fw(t-1) (u) - βt-ikVfw(t-1) (u) ∙ m-1∕2∣∣Λ-ι
t-1
Ut(U) = hVfw(t-i) (u), W(t-1)- W(°)i + βt-ikVfw(t-i)(u) ∙ m-1∕2kΛ二
Lt(U) = hVfw(t-1)(u), W(t-1)- W⑼〉-βt-ikVfw(t-i) (u) ∙ m-1∕2kΛ-ι
t-1
Ct={W∈W : kW-W(t-1)kΛt-1 ≤ βt-1}.
Let E be the event in which Lemma B.1, Lemma B.3, Lemma B.3 for all ω
n ∖/m⅛ :1 ≤ i ≤ n}
∈
and Lemma B.4 for all ω ∈
: 1 ≤ i ≤ n}
hold simultaneously.
Under event E, for all t ∈ [n], we have
kW* - W(t)∣∣Λt ≤ kW* - W(t)kF√P⅛
≤ (kW* - W(°)∣∣F + kW(t) - W(°)∣∣F)PkΛtk2
≤ (√2m-1∕2S + t1∕2λ-1∕2m-1∕2)∙qλ^+C2tL = βt,
where the second inequality is by the triangle inequality, and the third inequality is by Lemma B.1
and Lemma B.2. Thus, W* ∈ Ct , ∀t∈ [n].
Denoting a* 〜π*(∙∣xt) and at 〜∏t(∙∣xt), under event E, We have
SubOpt(∏t; Xt) = Et[h(xt川)]—Et[h(xt,aJ]
=EthhVfW(0)(Xtq), W * - W(°)〉] - EthhVfW(0)(xt,aj W * - W ⑼〉]
≤) Et hhVfw(t-1)(xt,ai), W * - W(°)〉] - EthhVfW(t-1)(xt,at), W * - W(°)〉]
16
Published as a conference paper at ICLR 2022
+ IlW* - W(0) kF ∙ EtkVfW(t-1) (xt川 ) - VfW(O) (xt,a* )kF
+ kvfW (t-1) (xt,^t ) - VfW ⑼(Xt,^t)kF
≤) Et hhVfW(t-i)(Xt川), W * - W ⑼ i] - EthhVfW(t-i)(Xt,at), W * - W ⑼〉]
+ 2√2C2Sm-11/6 log1/2 mt1/6L10/3X-1/6
≤ Et [Ut(Xtq)] - Et [Lt(Xt,at)] +2√2C2Sm-11/6 log1/2 mt1∕6L10∕3λT/6
=Et [Ut(Xt川)] - Et [Lt(Xt,at)]
+ Et
+ Et
["fW(t-1) (Xt川 ), W(t-1) - W(0)i- fW(t-1) (xt,a* ) + fW(0) (xt,a* )i
hfW (t-1) (xt,at ) - fW(0) (Xt,^t) -"fW (t-1) (xt,^t ), W (t-1) - W (0)ii
-Et [fw(0) (Xt川)]+ Et [fw(0) (xt,at )]+2√2C2SmT1/6 log1/2 .#/6刀10/31-1/6
|
}
"^^^^^^^^^^^~^^^^^^^^{^^^^^^^^^^^^^^^^^^^^"""
=o by symmetry at initialization
(e)
≤ Et [Ut(xt,a* )] - Et [Lt(xt,a* )] + (Et [Lt(xt,a* )] - Et [Lt(Xt,^t)])
|
}
{^^^^^^^^^^^^^^^^u∖∕'^^^^^^^^^^^^^^^^r
≤0 by pessimism
+ 2Cit2/3mT/6 log1/2 mL7/3XT/2 + 2√2C2Sm-11/6 log1/2 馆#/6刀10/31-1/6
(f)
≤ Et [Ut(xt,a* )] - Et [Lt(xt,a* )]
+ 2Cιt2∕3mT/6 log1/2 mL7/31-1/2 + 2√2C2Sm-11/6 log1/2 mt1∕6L10∕3λT/6
=2βt-1Et [kVfw(t-i)(xt川) ∙ mT∕2∣∣Λ二]
+ 2C1t2/3m-1/6 log1/2 mL7∕3λ-1/2 + 2√2C2Sm-11/6 log1/2 mt1∕6L10∕3λT∕6
where (a) is by Lemma B.1, (b) is by the triangle inequality, (c) is by Lemma B.1, Lemma B.2, and
LemmaB.3, (d) is by W * ∈ Ct, and by that maXu"∣u-bkA≤γ ha, U — bo i = ha, b - boi + Y IlaIIA-1,
and minu：ku-b∣∣A≤γha, U - boi = ha, b - boi 一 YkakA-I, (e) is by Lemma B.4 and by that
fw(0)(x(i)) = 0, ∀i ∈ [nK], and (f) is by that ^t is sampled from the policy ∏t which is greedy
with respect to Lt .
By the union bound and the choice of m, we conclude our proof.
□
B.2 Proof of Lemma A.2
We first present the following lemma.
Lemma B.5. For any δ ∈ (0, 1), if m satisfies
m ≥ max < Θ(nλ-1L11 log6 m), Θ(L-1λ1/2(log3/2(nKL2(n + 2)∕δ) ∨ log-3/2 m)),
Θ(L6(nK)4 log(L(n + 2)∕δ)),
and λ ≥ max{C32L, 1}, then with probability at least 1 - δ, it holds simultaneously that
t
X kvfW(i - 1)(xi,ai ) ∙ m-1/2 kΛ二
i=1	i-1
≤ 2log*⅛,∀t ∈ [n],
1	det(Λt)
logE)- log
log
det(Λ t)
det(λI)
det(Λ n)
det(λI)
≤ 2C2C2t3/2m-1/6(logm)1/2L23/6XT/6,Vt ∈ [n],
≈. , ________...
≤ dlog(1 + nK∕λ) + 1,
17
Published as a conference paper at ICLR 2022
where At := λI + Pt=ι vec(Vfw⑼(xig)) ∙ vec(Vfw(0)(xiai))t/m, and C2,C3 > 0 are
absolute constants from Lemma B.3 and Lemma B.2, respectively.
We are now ready to prove Lemma A.2.
Proof of Lemma A.2. First note that kΛt-1 k2 ≥ λ, ∀t. Let E be the event in which Lemma B.2,
Lemma C.2 for all ω ∈ n J m⅛: 1 ≤ i ≤ no, and Lemma B.5 simultaneously hold. Thus, under
event E , we have
kVfW (t-ι) (Xt,aj ∙ m-"2kΛ 二 ≤ kvfW (t-ι) (Xt,at) ∙ m-"kF ,M-1/1 ≤ CaL1*-1/,
where the second inequality is by Lemma B.2 and Lemma C.2.
Thus, by Assumption 4.2, Hoeffding’s inequality, and the union bound, with probability at least
1 - δ, it holds simultaneously for all t ∈ [n] that
2βt--Ea*~π*(∙∣Xt) [kVfw (t-1) (Xt,a* ) ∙馆-1//|葭二回-1, Xt]
≤ 2βt--κEa〜μ(∙∣Dt-ι,χt)，VfW(t-1) (Xt,a) ∙ m-11/k八二 |Dt-1, Xt]
≤ 2βt--κkVfw(t-i) (xt,at) ∙ mτ∕2kΛ二 + βt-1κ√2C3L11λ-11 log1∕2((5n + 2)∕δ).
Hence, for the choice of m in Lemma A.2, with probability at least 1 - δ, we have
1n
—X βt-1Ea(〜π*(∙∣xt) blVfW(t-I)(Xt川 ) ∙ m-112kΛ-11 lDt-1, xt]
n t=1
≤ ^nn- X kVfW(t-1) (Xt,at) ∙ m-112kΛ-11 + √2βηκL112λ0 1/2 log112((5n + 2)∕δ)
n
≤
βnκ√nt X kVfW(t-ι) (Xt,at)
t=1
・ mτ∕2kΛ-∖ + √3 βnκLi11X-11 log1/2((5n + 2)∕δ)
≤
√√nnκ ∕ogHO++c11 iog1/2((5n+2"δ)
≤
^2βnκ1 /log det(An) + 2C/C2n3//mT/6(logm)1//L/3/6XT/6 + C3βηκL1∕/λ-1∕/ log1∕2((5n + 2)∕δ)
n	det(λI )	2
≤
v√βnκ Jjlog(1 + nK∕λ) + 1 + 2C/C2n3//m-1/6(log m)1/2L/3/6 λ-1∕6
C3
√2
+
βnκL1∕2λ-1∕/ log1/2((5n + 2)∕δ),
where the first inequality is by βt ≤ βn , ∀t ∈ [n], the second inequality is by Cauchy-Schwartz
inequality, the second inequality and the third inequality are by Lemma B.5.
□
B.3 Proof of Lemma A.3
Proof of Lemma A.3. We follow the same online-to-batch conversion argument in (Cesa-Bianchi
et al., 2004). For each t ∈ [n], define
Zt = SUbOPt(∏t) - SUbOPt(∏t; Xt).
Since ∏t is Dt-「measurable and is independent of Xt, and Xt are independent of Dt_1 (by As-
sUmPtion 4.2), we have E [Zt|Dt-1] = 0, ∀t ∈ [n]. Note that -1 ≤ Zt ≤ 1. ThUs, by the
Hoeffding-AzUma ineqUality, with probability at least 1 - δ, we have
nn	n
E[SubOpt(Π)] =	^X SubOPt(∏t) = ^X SubOPt(∏t; Xt) + ^X Zt
nn	n
t=1
t=1
t=1
18
Published as a conference paper at ICLR 2022
1 二	Γ2	.
≤	SubOPt(∏t; Xt) +	log(1∕δ).
nn
t=1
□
C Proof of Lemmas in Section B
C.1 Proof of Lemma B.1
We first restate the following lemma.
Lemma C.1 (Arora et al. (2019)). There exists an absolute constant c1 > 0 such that for any
> 0, δ ∈ (0, 1), if m ≥ c1L6-4 log(L∕δ), for any i,j ∈ [nK], with probability at least 1 - δ
over the randomness of W (0), we have
|〈Vfw(0)(x⑴)，VfW(°)(Xj))i/m - Hi,j∣ ≤ e.
Lemma C.1 gives an estimation error between the kernel constructed by the gradient at initialization
as a feature map and the NTK kernel. Unlike Jacot et al. (2018), Lemma C.1 quantifies an exact
non-asymptotic bound for m.
ProofofLemma B.1. Let G = m-1/2 ∙ [vec(VfW(°)(x⑴))，...，Vec(VfW(0) (x(nK)))] ∈ Rp×nK.
For any e > 0, δ ∈ (0, 1), it follows from Lemma C.1 and union bound, if m ≥
Θ(L6e-4 log(nKL∕δ)), with probability at least 1 一 δ,we have
kGTG - HkF ≤nKkGTG-Hk∞ =nKmax|m-1hVfW(0)(X(i))，VfW(0)(X(j))i -Hi,j|
i,j
≤ nKe.
Under the event that the inequality above holds, by setting e = 2^, We have
H - GTG W kH - GTGk2I W kH - GTGkFI W λ201 W |H.	⑵
Let G = PΛQT be the singular value decomposition of G where P ∈ Rp×nK, Q ∈ RnK×nK
have orthogonal columns, and Λ ∈ RnK × nK is a diagonal matrix. Since GTG 占 11 H 占 苧I
is positive definite, Λ is invertible. Let W* ∈ W such that Vec(W*) = Vec(W(0)) + m-1/2 ∙
PΛ-1QTh, we have
m1/2 ∙ GT(Vec(W*) 一 Vec(W(O))) = QΛPTPΛ-1QTh = h.
Moreover, we have
m∣∣W* 一 W(0)kF = mk Vec(W*) 一 Vec(W(0))k1 = hTQΛ-1PTPΛ-1QTh
=hTQΛ-1QTh = hT(GTG)-1h ≤ 2hTH-1 h,
where the inequality is by Equation (2).	□
C.2 Proof of Lemma B.2
We present the following lemma that will be used in this proof.
Lemma C.2 (Cao & Gu (2019, Lemma B.3)). There exist an absolute constant C3 > 0 such that
for any δ ∈ (0, 1) over the randomness ofW(0), ifω satisfies
Θ(m-3∙L-3/2 log3∕2(nKL2∕δ)) ≤ ω ≤ Θ(L-6 log-3 m),
with probability at least 1 一 δ, it holds uniformly for all W ∈ B(W(0); ω), i ∈ [nK], l ∈ [L] that
kVιfw (x(i))kF ≤ C3 ∙√m.
19
Published as a conference paper at ICLR 2022
Proofof Lemma B.2. Let δ ∈ (0,1). Let Lt(W) = 2(fw(xt,aj — Irty + m∣∣ W 一 W(0)kF be
the regularized squared loss function on the data point (xt,at , rt). Recall that W(t) = W (t-1) -
々▽Lt(W(t-1)). By Hoelfding's inequality and that r is R-SUbgaUssian, for any t ∈ [n], with
probability at least 1 — δ, we have
|rt| ≤ ∣Et[rt]∣ + Rp2log(2∕δ) = |E [E"Dt-i, xt,at]] | + Rp2log(2∕δ)
=E [h(xt,αt)∣Dt-i, Xt, at] + Rp2log(2∕δ) ≤ 1 + R√2log(2∕δ).	(3)
By Union boUnd and (3), for any seqUence	{ωt }t∈[n]	sUch that
Θ(m-3SL-3/2 log3/2(3n2KL2∕δ)) ≤ ωt ≤ Θ(L-6 log-3 m) ∧ Θ(L-6 log-3/2 m),∀t ∈ [n],
with probability at least 1 — δ, it holds Uniformly for all W ∈ B(W(0); ωt), l ∈ [L], t ∈ [n] that
MLt(W )∣F = ∣Vιfw (xt,αt)(fw (xt,at)—『t) + mλ(W∖ - W*IlF
=∣∣WfW (Xt,at)(fW (xt,at ) - fW(0) (Xt,αj -hvfW(O)(Xt,α) W - W (0)i)
+ VlfW (xt,αt)fW(0) (xt,αt) + Vl fW (xt,αt)hVfW (0) (xt,αt), W - W(0) i
- rtVlfW(Xt,at) + mλ(Wl -Wl(0))∣F
≤ ∣VlfW(Xt,at)(fW(Xt,at) - fW (0) (Xt,at) - hVfW (0) (Xt,at), W - W(0)i)∣F
+ ∣Vl fW (Xt,at)fW (0) (Xt,at)∣F +∣Vl fW (Xt,at)hVfW (0) (Xt,at), W - W(0)i∣F
X--------------{------------}
=0
+ IrtIIlVlfW(xt,aJ∣∣F + mλkWl - Wl(O) IlF
≤ C1C3ω^L3mlog1/2 m + C3m"(l + R√2log(6n∕δ)) + C32 Lmω + mλωt ,
(4)
where the first ineqUality is triangle’s ineqUality, and the second ineqUality is by Lemma B.4, Lemma
C.2 and (3).
We now prove by induction that under the same event that (4) with ωt = JmL holds, W(t) ∈
B(W⑼；qmλL),∀t ∈ [n]. Ittriviallyholdsfor t = 0. Assume W⑶ ∈ B(W⑼；ʌ/ɪ),∀i ∈
[t - 1], we will prove that W(t) ∈ B(W(0); J^^). Indeed, it is easy to verify that there ex-
ist absolute constants {Ci }i2=1 > 0 such that if m satisfies the inequalities in Lemma B.2, then
Θ(m-3∕2L-3/2 log3∕2(3n2KL2∕δ)) ≤ Jm^L ≤ Θ(L-6 log-3 m) ∧ Θ(L-6 log-3/2 m), ∀i ∈
[n]. Thus, under the same event that (4) holds, we have
tt
IWl(t)-Wl(0)IF≤XIWl(i)-Wl(i-1)IF=XηiIVlLi(W(i-1))IF
i=1	i=1
≤ ιtCιC3m1∕3λ-2∕3L7∕3n1/6log1/2 m + 比.-1/21-1/"-1/2。= + λ)
+ 2C3∣√tm1∕2(1 + R√2log1∕2(6n∕δ))
1 / t 1 / t
≤ 2 V mλL + 2 V mλL
m mλL,
where the first inequality is by triangle’s inequality, the first equation is by the SGD update for each
W(i), the second inequality is by (4) and the last inequality is due to ι be chosen as
卜T ≥ 4C3mλ1∕2L1/2(1 + R√2log1∕2(6n∕δ))
(∣τ ≥ 2n1∕2m1∕2λ1∕2L1/2 (CιC3m1∕3λ-2∕3L7∕3n1/6 log1/2 m + 9-1/21-1/2刀-1/2(。2刀 + λ)),
which is satisfied for ι-1 = Ω(n2∕3m5/6λ-1∕6L17/6 log1/2 m) ∨ Ω(Rmλ1/2 log1∕2(n∕δ)).
For the second part of the lemma, we have
t
IΛtI2 = IλI +	VeC(VfW(i-i)(xi,αi)) ∙ VeC(VfW(一) (xi,aJ)T∕m∣2
i=1
20
Published as a conference paper at ICLR 2022
t
≤ λ + X ∣∣Vfw(i-i)(xi,ai)kF/m ≤ λ + C2tL,
i=1
where the first inequality is by triangle’s inequality and the second inequality is by ∣W (i) -
W⑼ ∣∣f ≤、∕miλ,∀i ∈ [n] and Lemma C.2.	□
C.3 Proof of Lemma B.5
ProofofLemma B.5. Let E(δ) be the event in which the following (n + 2) events hold SimUltane-
ously: the events in which Lemma B.3 for each ω ∈ {√mλL ： 1 ≤ i ≤ n holds, the event in
which Lemma C.1 for = (nK)-1 holds, and the event in which Lemma C.2 holds.
Under event E(δ), we have
l∣Vfw(t-i) (xt,at) ∙ m-1/2kF ≤ CβL1/2,∀t ∈ [n].
ThUs, by (Abbasi-Yadkori et al., 2011, Lemma 11), ifwe choose λ ≥ max{1, c26L}, we have
X kVfW (i-ι) (xi,ai ) ∙ mT∕2kΛτ ≤ 2log -) .
Λi-1	det(λI)
For the second part of Lemma B.5, for any t ∈ [n], we define
Mt = m-1/2 ∙ [vec(Vfw(0)(xι,αι)),..., Vec(VfW(0)(xt.))] ∈ Rp×t,
Mt = m-1/2 ∙ [vec(Vfw(t-ι)(xi,aι)), . . . , Vec(VfW(t-1) (xt,aj)] ∈ Rp×t.
We have Λt = λI + MtMT and At = λI + MtMT, and
log det(A) - log M(A) = | log det(I + MtM门X)- log det(I + MtMT/λ) |
det(λI)	det(λI)
=| logdet(I + MTMt∕λ) — logdet(I + MTTMt∕λ)∣
≤ maχ{kh(I + MTMt∕λ)-1, MTMt- MTMti∣, ∣h(I + MTMt∕λ)-1, MTMt- MTMti∣}
=kh(I + MTMt∕λ)-1, MTMt - MTMti∣
≤ k(I + MT Mt∕λ)-1∣F ∙ ∣ MT Mt - MT Mt ∣ F
≤ √tk(I + MTMt∕λ)-1k2 ∙ ∣MTMt - MTMt∣F
≤ √t∣MTMt - MTMt∣F
≤ √tt max m-1
1≤i,j≤t
≤ t3/2m-1
max
1≤i,j≤t
hVfW (t-1) (xi,ai ), VfW (t-1) (xj,aj )i - hVfW (0) (xi,ai), VfW (0) (xj,aj )i
hVfW (t-1) (xi,ai) - VfW (0) (xi,ai), VfW (t-1) (xj,aj )i
+ hVfW (t-1) (xj,aj ) - VfW (0) (xj,aj ), VfW (t-1) (xi,ai)i
≤ 2t3/2m-1 max ∣∣Vfw(t-i)(xiai) - Vfw(o)(多皿)|旧∙ max ∣∣Vfw(t-i)(xiai)∣f
≤ 2C2C2t3/2m-1/6(logNLw
where the second eqUality is by det(I + AAT) = det(I + AT A), the first ineqUality is by that
log det is concave, the third equality is assumed without loss of generality, the second inequality is
by that h A, Bi ≤ IlAkF ∣∣B∣f ,the third inequality is by that k A∣f ≤ √t∣∣A∣∣2 for A ∈ Rt×t, the
fourth inequality is by that I + MtT Mt ∕λ I, the fifth inequality is by that ∣A∣F ≤ t∣A∣∞ for
A ∈ Rt×t, the sixth inequality is by the triangle inequality, and the last inequality is by Lemma B.3,
and Lemma C.2.
21
Published as a conference paper at ICLR 2022
The third part of Lemma B.5 directly follows the argument in (?, (B.18)) and uses Lemma C.1 for
= (nK)-1.
Finally, it is easy to verify that the condition of m in Lemma B.5 satisfies the condition of m for
E(δ∕(n + 2)), and union bound We have P(E(δ∕(n + 2))) ≥ 1 - δ.	□
D Details of baseline algorithms in Section 6
In this section, We present the details of each representative baseline methods and of the B-mode
version of NeuraLCB in Section 6. We summarize the baseline methods in Table 2.
Table 2: A summary of the baseline methods.
Baseline	Algorithm	TyPe	Function approximation
LinLCB	Algorithm 2	Pessimism	Linear
KernLCB	Algorithm 3	Pessimism	RKSH
NeuralLinLCB	Algorithm 4	Pessimism	Linear w/ fixed neural features
NeuralLinGreedy	Algorithm 5	Greedy	Linear w/ fixed neural features
NeuralGreedy	Algorithm 6	Greedy	Neural networks
Algorithm 2 LinLCB
Input: Offline data Dn = {(xt, at, rt)}tn=1, reg. parameter λ > 0, confidence parameters β > 0.
1:	An J λI + Pt=1 xt,at XxTat
2:	θn — A-： Pn=I xt,atrt
3:	L(U) J〈0n, Ui- βku∣∣Λ-ι,∀U ∈ Rd
Output: π(x) J argmaXα∈[κ] L(Xa)
Algorithm 3 KernLCB
Input: Offline data Dn = {(Xt, at, rt)}tn=1, reg. parameter λ > 0, confidence parameters β > 0,
kernel k : Rd × Rd → R.
1:	kn(U) J [k(U, X1,a1), . . . , k(U, Xn,an)]T, ∀U ∈ Rd
2:	Kn J [k(Xi,ai , Xj,aj )]1≤i,j ≤n
3： yn J [ri, ..., rn]T	________________________________
4:	L(U) J kn(u)T (Kn + λI )-i y - β,k(u, U)- kn (U)(Kn + λI )-1kn, ∀U ∈ Rd
Output: π(x) J argmaXα∈[κ] L(Xa)
Algorithm 4 NeuralLinLCB
Input: Offline data Dn = {(Xt, at, rt)}tn=1, regularization parameter λ > 0, confidence parameters
β>0.
1:	Initialize the same neural netWork With the same initialization scheme as in NeuraLCB to obtain
the neural netWork function fW (0) at initialization
2:	Φ(u) j vec(Vfw(0)(U)) ∈ Rp, ∀u ∈ Rd
3:	An J λI + Ptn=1 φ(Xt,at)φ(Xt,at )T
4： On J A-： Pn=I φ(χt,ajrt
5:	L(U) J (0n,φ(U)i - βkΦ(U)∣∣A-ι, ∀U ∈ Rd
Output: π(x) J argmaXα∈[κ] L(Xa)
22
Published as a conference paper at ICLR 2022
Algorithm 5 NeuralLinGreedy
Input: Offline data Dn = {(xt, at, rt)}tn=1, regularization parameter λ > 0.
1:	Initialize the same neural network with the same initialization scheme as in NeuraLCB to obtain
the neural network function fW (0) at initialization
2:	φ(u) - vec(Vfw(0)(U)) ∈ Rp, ∀u ∈ Rd
3： ^n - λ-1 Pn=1 φ(xt,at )rt
4： L(U) 一 <0n,φ(u)i,∀u ∈ Rd
Output: π(x) J argmaXα∈[κ] L(Xa)
Algorithm 6 NeuralGreedy
Input: Offline data Dn = {(xt, at, rt)}tn=1, step sizes {ηt}tn=1 , regularization parameter λ > 0.
1： Initialize W⑼ as follows: set Wl(O) = [W∣, 0; 0, W∣], ∀l ∈ [L 一 1] where each entry of W∣
is generated independently from N(0, 4/m), and set WL(0) = [wT, -wT] where each entry of
w is generated independently from N(0, 2/m).
2： for t = 1, . . . , n do
3： Retrieve (xt, at, rt) from Dn.
4：	∏t(x) J argmaxa∈[κ] fw(t-i)(xa),∀x.
5：	W㈤ J W(t-1) - ηtVLt(W(T)) where Lt(W) = 1 (fw(xt,aj — r) + 等∣∣W —
W(0)k2F.
6： end for
Output: Randomly sample π uniformly from {∏ι,...,∏n}.
Algorithm 7 NeuraLCB (B-mode)
Input: Offline data Dn = {(xt, at, rt)}tn=1, step sizes {ηt}tn=1 , regularization parameter λ > 0,
confidence parameters {βt}tn=1, batch size B > 0, epoch number J > 0.
1： Initialize W(O) as follows: set Wl(O) = [Wl, 0; 0, Wl], ∀l ∈ [L - 1] where each entry of Wl
is generated independently from N(0, 4/m), and set WL(0) = [wT, -wT] where each entry of
w is generated independently from N(0, 2/m).
2： ΛO J λI.
3： for t = 1, . . . , n do
4： Retrieve (xt, at, rt ) from Dn.
5：	Lt(U) J fW(t-ι) (U)- βt-1 kvfW(t-1) (U) ∙ m-1∕2kΛ-1], ∀u ∈ Rd
6:	∏t(x) J arg maXɑ∈[κ] Lt(xa), for all X = {x° ∈ Rd : a ∈ [K]}.
7：	Λt J Λt-1 + vec(Vfw(t-i)(xt,aj) ∙ Vec(VfW(t-i) (xt,aJ)T/m.
8：	W(O) J W(t-1)
9： for j = 1, . . . , J do
10：	Sample a batch of data Bt = {Xtq,atq , rtq}qB=1 from Dt
11：	L(j)(W) J PB=1 务(fw(xtq,atq:-rtq)2 + k W - W(O) kF
12：	W(j) J W(j-1) - ηtVLtj)(W(j-1))
13：	end for
14:	W(t) J W(J)
15： end for
Output: Randomly sample π uniformly from {∏ι,...,∏n}.
E Datasets
We present a detailed description about the UCI datasets used in our experiment.
•	Mushroom data： Each sample represents a set of attributes of a mushroom. There are two
actions to take on each mushroom sample： eat or no eat. Eating an editable mushroom
23
Published as a conference paper at ICLR 2022
Table 3: The real-world dataset statistics
Dataset	Mushroom	Statlog	Adult	MNIST
Context dimension	22	9	94	784
Number of classes	2	7	14	10
Number of instances	8,124	43,500	45,222	70,000
generates a reward of +5 while eating a poisonous mushroom yields a reward of +5 with
probability 0.5 and a reward of -35 otherwise. No eating gives a reward of 0.
•	Statlog data: The shuttle dataset contains the data about a space shuttle flight where the
goal is to predict the state of the radiator subsystem of the shuttle. There are total K = 7
states to predict where approximately 80% of the data belongs to one state. A learner
receives a reward of 1 if it selects the correct state and 0 otherwise.
•	Adult data: The Adult dataset contains personal information from the US Census Bureau
database. Following (Riquelme et al., 2018), we use the K = 14 different occupations as
actions and d = 94 covariates as contexts. As in the Statlog data, a learner obtains a reward
of 1 for making the right prediction, and 0 otherwise.
•	MNIST data: The MNIST data contains images of various handwritten digits from 0 to 9.
We use K = 10 different digit classes as actions and d = 784 covariates as contexts. As in
the Statlog and Adult data, a learner obtains a reward of 1 for making the right prediction,
and 0 otherwise.
We summarizes the statistics of the above datasets in Table 3.
F	Additional Experiments
In this section, we complement the experimental results in the main paper with additional experi-
ments regarding the learning ability of our algorithm on dependent data and the different behaviours
of S-mode and B-mode training.
F.1 Dependent data
As the sub-optimality guarantee in Theorem 4.1 does not require the offline policy to be stationary,
we evaluate the empirical performance of our algorithm and the baselines on a new setup of offline
data collection that represents dependent actions. In particular, instead of using a stationary policy
to collect offline actions as in Section 6, here We used an adaptive policy μ defined as
μ(a∣Dt-i, Xt) = (1 - e) * π*(a∣xt) + e * ∏LinUCB(a∣Dt-i, xt),
where ∏* is the optimal policy and ∏li∩ucb is the linear UCB learner (AbbaSi-Yadkori et al., 2011).
This Weighted policy makes the collected offline data at dependent on Dt-1 While making sure that
the offline data has a sufficient coverage over the data of the optimal policy, as LinUCB does not
perform well in several non-linear data. We used e = 0.9 in this experiment.
The results on Statlog and MNIST are shown in Figure 3. We make two important observations.
First, on this dependent data, the baseline methods with linear models (LinLCB, NeuralLinLCB and
NeuralLinGreedy) 3 perform almost as bad as when they learn on the independent data in Figure
2, suggesting that linear models are highly insufficient to learn complex rewards in real-world data,
regardless of how offline data were collected. Secondly, the main competitor of our method, Neu-
ralGreedy suffers an apparent performance degradation (especially in a larger dataset like MNIST)
while NeuraLCB maintains a superior performance, suggesting the effectiveness of pessimism in
our method on dealing with offline data and the robustness of our method toward dependent data.
3KernLCB also does not change its performance much, but its computational complexity is still an major
issue.
24
Published as a conference paper at ICLR 2022
Number of samples
(a) Statlog
Figure 3: The sub-optimality of NeuraLCB versus the baseline algorithms on real-world datasets
with correlated structures.
Number of samples
(b) MNIST
F.2 S-mode versus B-mode training
As in Section 6 we implemented two different training modes: S-mode (Algorithm 1) and B-mode
(Algorithm 7). We compare the empirical performances of S-mode and B-mode on various datasets.
As this variant is only applicable to NeuralGreedy and NeuraLCB, we only depict the performances
of these algorithms. The results on Cosine (synthetic dataset), Statlog, Adult and MNIST are shown
in Figure 4.
We make the following observations, which are consistent with the conclusion in the main paper
while giving more insights. While the B-mode outperforms the S-mode on Cosine, the S-mode
significantly outperforms the B-mode in all the tested real-world datasets. Moreover, the B-mode of
NeuraLCB outperforms or at least is compatible to the S-mode of NeuralGreedy in the real-world
datasets. First, these observations suggest the superiority of our method on the real-world datasets.
Second, to explain the performance difference of S-mode and B-mode on synthetic and real-world
datasets in our experiment, we speculate that the online-like nature of our algorithm tends to reduce
the need of B-mode in practical datasets because B-mode in the streaming data might cause over-
fitting (as there could be some data points in the past streaming that has been fitted for multiple
times). In synthetic and simple data such as Cosine, over-fitting tends to associate with strong
prediction of the underlying reward function as the simple synthetic reward function such as Cosine
is sufficiently smooth, unlike practical reward functions in the real-world datasets.
•
O 2000 4000 6000 BOOO IQOOO 12000 14000
Numberofsampies
----Neura LCB(S-model
Neura LCB(B-mode)
NeuralG reedy(S-nn ode)
—NeuraLCB(S-Mode)
NeuraLCB(B-mode)
NeuraiG reedy(S-rr>ode)
---NeurelGreedy(B-mo4e)
O 2000 4000 88 8000 IOOOO 12000 14000
Number Ofsamples
0.2-
ff-iEso-qns
O 2000 4000 6000 8000 IOOOO 12Q00 MQOQ
Numberofsampies
(a) Cosine	(b) Statlog
(c) Adult	(d) MNIST
Figure 4: Comparison of S-mode and B-mode training.
25