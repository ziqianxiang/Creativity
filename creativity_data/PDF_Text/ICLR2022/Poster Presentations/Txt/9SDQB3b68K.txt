Published as a conference paper at ICLR 2022
DARA: Dynamics-Aware Reward Augmentation
in Offline Reinforcement Learning
Jinxin Liu123* Hongyin Zhang1* Donglin Wang13*
1 Westlake University. 2 Zhejiang University.
3 Institute of Advanced Technology, Westlake Institute for Advanced Study.
{liujinxin, zhanghongyin, wangdonglin}@westlake.edu.cn
Ab stract
Offline reinforcement learning algorithms promise to be applicable in settings
where a fixed dataset is available and no new experience can be acquired. How-
ever, such formulation is inevitably offline-data-hungry and, in practice, collecting
a large offline dataset for one specific task over one specific environment is also
costly and laborious. In this paper, we thus 1) formulate the offline dynamics
adaptation by using (source) offline data collected from another dynamics to relax
the requirement for the extensive (target) offline data, 2) characterize the dynamics
shift problem in which prior offline methods do not scale well, and 3) derive a sim-
ple dynamics-aware reward augmentation (DARA) framework from both model-
free and model-based offline settings. Specifically, DARA emphasizes learning
from those source transition pairs that are adaptive for the target environment and
mitigates the offline dynamics shift by characterizing state-action-next-state pairs
instead of the typical state-action distribution sketched by prior offline RL meth-
ods. The experimental evaluation demonstrates that DARA, by augmenting re-
wards in the source offline dataset, can acquire an adaptive policy for the target
environment and yet significantly reduce the requirement of target offline data.
With only modest amounts of target offline data, our performance consistently
outperforms the prior offline RL methods in both simulated and real-world tasks.
1 Introduction
Offline reinforcement learning (RL) (Levine et al., 2020; Lange
et al., 2012), the task of learning from the previously collected
dataset, holds the promise of acquiring policies without any costly
active interaction required in the standard online RL paradigm.
However, we note that although the active trail-and-error (online
exploration) is eliminated, the performance of offline RL method
heavily relies on the amount of offline data that is used for training.
As shown in Figure 1, the performance deteriorates dramatically as
the amount of offline data decreases. A natural question therefore
arises: can we reduce the amount of the (target) offline data without
significantly affecting the final performance for the target task?
100% 50% 20% 10% 5%
Amount of data used for training
Figure 1: Solid and dashed
lines denote offline Medium-
Replay and Medium-Expert
data in D4RL (Walker2d) resp.
Bringing the idea from the transfer learning (Pan & Yang, 2010), we assume that we have access
to another (source) offline dataset, hoping that we can leverage this dataset to compensate for the
performance degradation caused by the reduced (target) offline dataset. In the offline setting, pre-
vious work (Siegel et al., 2020; Chebotar et al., 2021) has characterized the reward (goal) differ-
ence between the source and target, relying on the ”conflicting” or multi-goal offline dataset (Fu
et al., 2020), while we focus on the relatively unexplored transition dynamics difference between
the source dataset and the target environment. Meanwhile, we believe that this dynamics shift is
not arbitrary in reality: in healthcare treatment, offline data for a particular patient is often limited,
whereas we can obtain diagnostic data from other patients with the same case (same reward/goal)
* Equal contribution.
,Corresponding author.
1
Published as a conference paper at ICLR 2022
and there often exist individual differences between patients (source dataset with different transition
dynamics). Careful treatment with respect to the individual differences is thus a crucial requirement.
Given source offline data, the main challenge is to cope with the transition dynamics difference, i.e.,
strictly tracking the state-action supported by the source offline data can not guarantee that the same
transition (state-action-next-state) can be achieved in the target environment. However, in the offline
setting, such dynamics shift is not explicitly characterized by the previous offline RL methods, where
they typically attribute the difficulty of learning from offline data to the state-action distribution
shift (Chen & Jiang, 2019; Liu et al., 2018). The corresponding algorithms (Fujimoto et al., 2019;
Abdolmaleki et al., 2018; Yu et al., 2020) that model the support of state-action distribution induced
by the learned policy, will inevitably suffer from the transfer problem where dynamics shift happens.
Our approach is motivated by the well established connection between reward modification and
dynamics adaptation (Kumar et al., 2020b; Eysenbach & Levine, 2019; Eysenbach et al., 2021),
which indicates that, by modifying rewards, one can train a policy in one environment and make the
learned policy to be suitable for another environment (with different dynamics). Thus, we propose
to exploit the joint distribution of state-action-next-state: besides characterizing the state-action
distribution shift as in prior offline RL algorithms, we additionally identify the dynamics (i.e., the
conditional distribution of next-state given current state-action pair) shift and penalize the agent with
a dynamics-aware reward modification. Intuitively, this reward modification aims to discourage
the learning from these offline transitions that are likely in source but are unlikely in the target
environment. Unlike the concurrent work (Ball et al., 2021; Mitchell et al., 2021) paying attention to
the offline domain generalization, we explicitly focus on the offline domain (dynamics) adaptation.
Our principal contribution in this work is the characterization of the dynamics shift in offline RL and
the derivation of dynamics-aware reward augmentation (DARA) framework built on prior model-
free and model-based formulations. DARA is simple and general, can accommodate various offline
RL methods, and can be implemented in just a few lines of code on top of dataloader at training. In
our offline dynamics adaptation setting, we also release a dataset, including the Gym-MuJoCo tasks
(Walker2d, Hopper and HalfCheetah), with dynamics (mass, joint) shift compared to D4RL, and
a 12-DoF quadruped robot in both simulator and real-world. With only modest amounts of target
offline data, we show that DARA-based offline methods can acquire an adaptive policy for the target
tasks and achieve better performance compared to baselines in both simulated and real-world tasks.
2	Related Work
Offline RL describes the setting in which a learner has access to only a fixed dataset of experience,
while no interactive data collection is allowed during policy learning (Levine et al., 2020). Prior
work commonly assumes that the offline experience is collected by some behavior policies on the
same environment that the learned policy be deployed on. Thus, the main difficulty of such offline
setting is the state-action distribution shift (Fujimoto et al., 2019; Liu et al., 2018). Algorithms
address this issue by following the two main directions: the model-free and model-based offline RL.
Model-free methods for such setting typically fall under three categories: 1) Typical methods mit-
igate this problem by explicitly (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019) or
implicitly (Siegel et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018) constraining the learned
policy away from OOD state-action pairs. 2) Conservative estimation based methods learn pes-
simistic value functions to prevent the overestimation (Kumar et al., 2020a; Xu et al., 2021). 3)
Importance sampling based methods directly estimate the state-marginal importance ratio and ob-
tain an unbiased value estimation (Zhang et al., 2020; Nachum & Dai, 2020; Nachum et al., 2019b).
Model-based methods typically eliminate the state-action distribution shift by incorporating a reward
penalty, which relies on the uncertainty quantification of the learned dynamics (Kidambi et al., 2020;
Yu et al., 2020). To remove this uncertainty estimation, Yu et al. (2021) learns conservative critic
function by penalizing the values of the generated state-action pairs that are not in the offline dataset.
These methods, however, define their objective based on the state-action distribution shift, and ig-
nore the potential dynamics shift between the fixed offline data and the target MDP. In contrast, we
account for dynamics (state-action-next-state) shift and explicitly propose the dynamics aware re-
ward augmentation. A counterpart, close to our work, is off-dynamics RL (Eysenbach et al., 2021),
where they set up dynamics shift in the interactive environment while we focus on the offline setting.
2
Published as a conference paper at ICLR 2022
3	Preliminaries
We study RL in the framework of Markov decision processes (MDPs) specified by the tuple M :=
(S, A, r, T, ρ0, γ), where S and A denote the state and action spaces, r(s, a) ∈ [-Rmax , Rmax] is
the reward function, T (s0 |s, a) is the transition dynamics, ρ0(s) is the initial state distribution, and
Y is the discount factor. The goal in RL is to optimize a policy ∏(a∣s) that maximizes the expected
discounted return ηM(π) := ET〜PM(T) [P∞=o Ytr(st, at)], where T := (s0, a0,si,aι,...). We
also define Q-values Q(s, a) := ET〜p∏(T)[P∞=o Ytr(st, at)∣so = s, a0 = a], V-values V(s):=
Ea〜∏(a∣s) [Q(s, a)], and the (unnormalized) state visitation distribution d%(S) := P∞=0 YtP(s|n,
M, t), where P(s|n, M, t) denotes the probability of reaching state S at time t by running ∏ in M.
In the offline RL problem, we are provided with a static dataset D := {(s, a, r, s0)}, which con-
sists of transition tuples from trajectories collected by running one or more behavioral policies,
denoted by ∏b, on MDP M. With a slight abuse of notation, we write D = {(s, a,r, s0)〜
dD(s)∏b(a∣s)r(s, a)T(s0∣s, a)}, where the dg(S) denotes state-marginal distribution in D. In the
offline setting, the goal is typically to learn the best possible policy using the fixed offline dataset.
Model-free RL algorithms based on dynamic programming typically perform policy iteration to
find the optimal policy. Such methods iteratively conduct 1) policy improvement with GMQ :=
argmax∏ Es〜d∏⑸户〜冗但⑼[Q(s, a)] and 2) policy evaluation by iterating the Bellman equation
Q(s, a) = BMQ(s, a) := r(s, a) + γEs,〜T(s，|s,a),a，〜n(a，|s，)[Q(s0, a0)] over dM(s)π(a∣s). Given
off-policy D, we resort to 1) improvement with GDQ := argmax∏ Es〜d0(s),a〜∏(a∣s) [Q(s, a)] and
2) evaluation by iterating Q(s, a) = BDQ(s, a) := r(s, a) + γEso〜TD(so∣s,a),aθ〜冗但卬)[Q(s0, a0)]
over all (S, a) in D. Specifically, given any initial Q0, it iterates1
Policy improvement: πk+1 = GDQk, Policy evaluation: Qk+1 = BDπk+1 Qk.	(1)
Model-free offline RL based on the above iteration suffers from the state-action distribution shift,
i.e., policy evaluation BDπk Qk-1 may encounter unfamiliar state action regime that is not covered
by the fixed offline dataset D, causing erroneous estimation of Qk . Policy improvement GDQk
further exaggerates such error, biasing policy πk+1 towards out-of-distribution (OOD) actions with
erroneously high Q-values. To address this distribution shift, prior works 1) explicitly constrain
policy to be close to the behavior policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019;
Ghasemipour et al., 2021), introducing penalty aD(π(a∣s), ∏b(a∣s)) into GD or BD in Equation 1:
GdQ = argmaxEs〜d0(s),a〜∏(a∣s) [Q(s, a) - αD(π(a∣s),∏b(a∣s))],
π	0	0	0 0	0 0	(2)
BDQ(s, a) = r(s, a) + γEso〜TD(so∣s,a),aθ〜∏(aθ∣so) [Q(s0, a0) - αD(π(a0∣s0),∏b(a0∣s0))],
where D is a divergence function between distributions over actions (e.g., MMD or KL divergence),
or 2) train pessimistic value functions (Kumar et al., 2020a; Yu et al., 2021; Xu et al., 2021), penal-
izing Q-values at states in the offline dataset D for actions generated by the current policy π:
Q = arg min Es〜d0(s),a〜∏(a∣s) [Q(s, a)], s.t. Q = BDQ.	(3)
Q
Model-based RL algorithms iteratively 1) model the transition dynamics T(S0 |S, a), using the data
collected in M: maxT^ Es,a,so〜d∏f(s)∏(a∣s)τ(so∣s,a) [logT(s0∣s, a)], and 2) infer a policy π from the
modeled M = (S, A, r, T, ρo, Y), where we assume that r and ρo are known, maximizing ηjM(π)
with a planner or the Dyna-style algorithms (Sutton, 1990). In this paper, we focus on the latter.
Model-based offline RL algorithms similarly suffer from OOD state-action (Kidambi et al., 2020;
Cang et al., 2021) if we directly apply policy iteration over T := maxT^ Es,a,so〜D[logT(s0∣s, a)].
Like the conservative estimation approach described in Equation 3, recent conservative model-based
offline RL methods provide the policy with a penalty for visiting states under the estimated T where
T is likely to be incorrect. Taking u(s, a) as the oracle uncertainty (Yu et al., 2020) that provides a
consistent estimate of the accuracy of model T at (S, a), we can modify the reward function to obtain
a conservative MDP: Mc = (S, A,r - au, T, ρo, y), then learn a policy π by maximizing ηjMc (∏).
1For parametric Q-function, we often perform Qk+1 — arg minQ E(s,a)〜D[(BDk+1 Qk(s, a) — Q(s, a))2].
3
Published as a conference paper at ICLR 2022
4	Problem Formulation
In standard offline RL problem, the static offline dataset D consists of samples {(s, a,r, s0) 〜
d。(s)∏b(a∣s)r(s,a)T(s0∣s, a)}. Although offline RL methods learn policy for the target MDP
M := (S, A, r, T, ρ0, γ) without (costly) online data, as we shown in Figure 1, it requires a fair
amount of (target) offline data D collected on M . Suppose we have another (source) offline dataset
D0, consisting of samples {(s, a,r, s0)〜 d。，(s)∏bo(a∣s)r(s, a)T0(s0∣s, a)} collected by the behav-
ior policy πb0 on MDP M0 := (S, A, r, T0, ρ0, γ), then we hope the transfer of knowledge between
offline dataset {D0 ∪D} can reduce the data requirements on D for learning policy for the target M.
4.1	Dynamics Shift in Offline RL
Although offline RL methods in Section 3 have incorporated the state-action distribution constrained
backups (policy constraints or conservative estimation), they also fail to learn an adaptive policy for
the target MDP M with the mixed datasets {D0 ∪ D}, as we show in Figure 4 (Appendix). We
attribute this failure to the dynamics shift (Definition 2) between D0 and M in this adaptation setting.
Definition 1 (Empirical MDP) An empirical MDP estimated from D is M := (S, A, r, T , ρ0, γ)
where T = max,^ Es,a,s，〜D[logT(s0∣s, a)] andT(s0∣s, a) = 0 for all (s, a, s0) not in dataset D.
Definition 2 (Dynamics shift) Let M := (S, A, r, T, ρ0, γ) be the empirical MDP estimated from
D. To evaluate a policy π for M := (S, A, r, T, ρ0, γ) with offline dataset D, we say that the
dynamics shift (between D and M) in offline RL happens if there exists at least one transition pair
(s, a, s0) ∈ {(s, a, s0) : dM (s)π(a∣s)T(s0∣s, a) > 0} SUCh that T(S0∣s, a) = T(s0∣s, a).
In practice, for a stochastic M and any finite offline data D collected in M, there always exists the
dynamics shift. The main concern is that finite samples are always not sufficient to exactly model
stochastic dynamics. Following Fujimoto et al. (2019), we thus assume both MDPs M and M0 are
deterministic, which means the empirical M and M0 are both also deterministic. More importantly,
such assumption enables us to explicitly characterize the dynamics shift under finite offline samples.
Lemma 1 Under deterministiC transition dynamiCs, there is no dynamiCs shift between D and M.
For offline RL tasks, prior methods generally apply BDπ Q along with the state-action distribution
correction (Equations 2 and 3), which overlooks the potential dynamics shift between the (source)
offline dataset and the target MDP (e.g., D0 → M). As a result, these methods do not scale well to
the setting in which dynamics shift happens, e.g., learning an adaptive policy forM with (source) D0.
4.2	Dynamics Shift in Model-free and Model-based Offline Formulations
From the model-free (policy iteration) view, an exact policy evaluation on M is characterized by
iterating Q(s, a) = BMQ(s, a) for all (s, a) such that d® (s)π(a∣s) > 0. Thus, to formalize the pol-
icy evaluation with offline D or D0 (for an adaptive π on target M ), we require that Bellman operator
BD Q(s, a) or BD0 Q(s, a) approximates the oracle BM Q(s, a) for all (s, a) in Sπ or Sπ, where Sπ
and S∏ denote the sets {(s, a) : d。(s)π(a∣s) > 0} and {(s, a) : dD，(s)n(a|s) > 0} respectively.
1)	To evaluate a policy π for M with D (i.e., calling the Bellman operator BD), notable model-
free offline method BCQ (Fujimoto et al., 2019) translates the requirement of B。 = BM into the
requirement of T(s0 |s, a) = T(s0 |s, a). Note that under deterministic environments, we have the
property that for all (s, a, s0) in offline data D, T(s0∣s, a) = T (s0∣s, a) (Lemma 1). As a result, such
property permits BCQ to evaluate a policy π by calling B。, replacing the oracle BM, meanwhile
constraining S∏ to be a subset of the support of d。(s)∏b(a∣s). This means a policy π which only
traverses transitions contained in (target) offline data D, can be evaluated on M without error.
2)	To evaluate a policy ∏ for M with D0 (i.e., calling the Bellman operator B。，), We have lemma 2:
Lemma 2 DynamiCs shift produCes that B。 0 Q(s, a) 6= BM Q(s, a) for some (s, a) in Sπ.
With the offline data D0, lemma 2 suggests that the above requirement B。， = BM becomes infea-
sible, which limits the practical applicability of prior offline RL methods under the dynamics shift.
4
Published as a conference paper at ICLR 2022
To be specific, characterizing an adaptive policy for target MDP M with D0 moves beyond the reach
of the off-policy evaluation based on iterating Q = BD0Q (Equations 2 and 3). Such iteration may
cause the evaluated Q (or learned policy ∏) overfits to T0 and struggle to adapt to the target T. To
overcome the dynamics shift, We would like to resort an additional compensation ∆c^o T such that
BDoQ(s, a) + ∆T^0,T(s, a) = BMQ(s, a)	(4)
for all (s, a) in S∏. Thus, we can apply BDQ + ∆T^o T to act as a substitute for the oracle BMQ.
From the model-based view, the oracle ηM (π) (calling the Bellman operator BM on the target M)
and the viable η1^, (∏) (calling BM0 on the estimated M0 from source D0) have the following lemma.
Lemma 3 Let BMV(S) = Ea〜冗但⑼[r(s, a) + γEs,〜T(s，|s,a) [V(s0)]] ∙ FOr any π, we have:
ηM0 (π) = ηM (π) + Es〜d∏M0 (S) [bm, VM (S)-BMVM (S)].
Lemma 3 states that if we maximize ηj^^,(∏) subject to |Es〜d∏ (S)[BM^0VM(s) - BMVM(s)]| ≤ e,
ηM (π) will be improved. IfF is a set of functions f : S → R that contains VM, then we have
IEs飞0(S) [BM0VM(s) - BMVm(s)] ∣ ≤ γEs,a飞0(s)∏(a∣s) [dF(T0(S0∣S, a), T(s0∣s, a))] , (5)
where dτ(T0(s0∣s, a), T(s0∣s, a)) =SUpf ∈f 吗，〜T，(s，|s,a) f(s0)] - Es，〜T(s，|s,a) [f(s0)] |, which is
the integral probability metric (IPM). Note that if we directly follow the admissible errOr assumption
in MOPO(YU et al., 2020) i.e., assuming 4下(T0(s0∣s, a), T(s0∣s, a)) ≤ u(s, a) for all (s, a), this
would be too restrictive: given that T0 is estimated from the source offline samples collected under
T0, not the target T, thus such error would not decrease as the source data increases. Further, we find
dF(T0(s0∣s, a), T (s0∣s, a)) ≤ dF(T0(s0∣s, a), T(s0∣s, a)) + dF(T(s0∣s, a), T (s0∣s, a)).	(6)
Thus, we can bound the dF(T0, T) term with the admissible error assumption over dF(T, T), as in
MOPO, and the auxiliary constraints dF(T0, T). See next section for the detailed implementation.
In summary, we show that both prior offline model-free and model-based formulations suffer from
the dynamics shift, which also suggests us to learn a modification (∆ or dF) to eliminate this shift.
5	Dynamics-Aware Reward Augmentation
In this section, we propose the dynamics-aware reward augmentation (DARA), a simple data aug-
mentation procedure based on prior (model-free and model-based) offline RL methods. We first
provide an overview of our offline reward augmentation motivated by the compensation ∆T^, T in
Equation 4 and the auxiliary constraints dF(T0, T ) in Equation 6, and then describe its theoretical
derivation in both model-free and model-based formulations. With the (reduced) target offline data
D and the source offline data D0, we summarize the overall DARA framework in Algorithm 1.
Algorithm 1 Framework for Dynamics-Aware Reward Augmentation (DARA)
Require: Target offline data D (reduced) and source offline data D0
1:
2:
3:
4:
Learn classifiers (qsas and qsa) that distinguish source data D0 from target data D. (See Appendix A.1.3)
Set dynamics-aware Ar(St, at, st+1)= log 外温常;念；I))
- log
qsa(source∣st ,at)
qsa(target∣st,at)
Modify rewards for all (st, at,rt, st+ι) in D0: rt — rt — η∆r.
Learn policy with {D ∪ D0 } using prior model-free or model-based offline RL algorithms.
5.1	Dynamics-Aware Reward Augmentation in Model-free Formulation
Motivated by the well established connection of RL and probabilistic inference (Levine, 2018),
we first cast the model-free RL problem as that of inference in a particular probabilistic model.
Specifically, we introduce the binary random variable O that denotes whether the trajectory τ :=
5
Published as a conference paper at ICLR 2022
(s0, a0, s1, ...) is optimal (O = 1) or not (O = 0). The likelihood of a trajectory can then be modeled
as P(O = 1∣τ) = exp (Pt rt∕η), where rt := r(st, at) and η > 0 is a temperature parameter.
(Reward Augmentation with Explicit Policy/Value Constraints) We now introduce a variational
distribution PM 0 (T) = p(so) Y[t=ι T 0(st+ι∣st, at)π(at∣st) to approximate the posterior distribution
PM (T|O = 1), which leads to the evidence lower bound of logPM (O = 1):
log Pm (O=1) = log Eτ~pM (T) [P(O=1|t )] ≥ Eτ~pM0 (T)
logP(O=1|t) + log pM共
.	PM 0(T)
ET ~Pm o(τ)
X (rt ∕η - log T⅛+½⅛)].
(7)
Since we are interested in infinite horizon problems, we introduce the discount factor γ and take
the limit of steps in each rollout, i.e., H → ∞. Thus, the RL problem on the MDP M, cast as
the inference problem arg maxπ log PM (O = 1), can be stated as a maximum of the lower bound
ET~p∏ z (τ) Pt∞=0γt rt - ηlog T^si1 ∣Sj,at)))]. This is equivalent to an RL problem on MM0 with
the augmented reward r J r(s, a) - η log T(S0晨). Intuitively, the -η log T,(s0∣Sa)) term discour-
ages transitions (state-action-next-state) in D0 that have low transition probability in the target M .
In the model-free offline setting, we can add the explicit policy or Q-value constraints (Equations-
2 and 3) to mitigate the OOD state-actions. Thus, such formulation allows the oracle BM to be re-
expressed by B。，and the modification log t- , Which makes the motivation in Equation 4 practical.
(Reward Augmentation with Implicit Policy Constraints) If we introduce the variational distri-
bution PM0(τ) := p(sο) Qt=ι T(st+ι∣st, at)π0(at∣st), We can recover the weighted-regression-
style (Wang et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018; Peters et al., 2010) objective by
maximizing J(π0,π) := Eτ~pV(τ) hp∞=0 Yt (rt - η log ⅜⅛⅛ - η log ⅛a⅛y)i (IOWer
bound oflogPM(O = 1)). Following the Expectation Maximization (EM) algorithm, we can maxi-
mize J(π0, π) by iteratively (E-step) improving J(π0, ∙) w.r.t. π0 and (M-step) updating π w.r.t. π0.
(E-SteP) We define Q(s, a, s0) = Eτ~p∏0,(τ JPt Yt log 霜[；：)) |s0 = s, a0 = a, si = s0]. Then,
given offline data D0, we can rewrite J(∏0, ∙) as a constrained objective (Abdolmaleki et al., 2018):
m∏aX EdD0(s)∏0(a∣s)T0(s0∣s,a) [q(s, a) - nQ(s, a, s')] , s∙t∙ Es~d00(s) [DKL (n0(a|s) kn(a|s))] ≤ e.
When considering a fixed π , the above optimization over π 0 can be solved analytically (Vieillard
et al., 2020; Geist et al., 2019; Peng et al., 2019). The optimal ∏ is then given by ∏ (a|s) Z
π(a∣s) exp (Q(s, a)) exp(-ηQ(s, a, T0(s0∣s, a))). As the policy evaluation in Equation 1 (Footnote-
2), we estimate Q(s, a) and Q(s, a, s0) by minimizing the Bellman error with offline samples in D0.
(M-SteP) Then, we can project ∏ onto the manifold of the parameterized ∏:
arg min Es~dD,(s) DKL (∏ (a∣s)k∏(a∣s))]
π
arg max Es,a,s，~D，[log ∏(a∣s)exp(Q(s, a))exp (一ηQ(s, a, s0)) ] ∙
(8)
From the regression view, prior work MPO (Abdolmaleki et al., 2018) infers actions with Q-value
weighted regression, progressive approach compared to behavior cloning; however, such paradigm
lacks the ability to capture transition dynamics. We explicitly introduce the exp(-ηQ(s, a, s0))
term, which as we show in experiments, is a crucial component for eliminating the dynamics shift.
ImPlementation: In practice, we adopt offline samples in D to approximate the true dynamics T of
M, and introduce a pair of binary classifiers, qsas(∙∣s, a, s0) and qsa(∙∣s, a), toreplace log [(sjSa)) as
in Eysenbach et al. (2021): log t⅛¾⅞ = log ⅛f⅛a⅛ - log t⅞⅛) ∙ (See Appendix-
A.1.3 for details). Although the amount of data D sampled from the target M is reduced in our prob-
lem setup, we experimentally find that such classifiers are sufficient to achieve good performance.
6
Published as a conference paper at ICLR 2022
5.2	Dynamics-Aware Reward Augmentation in Model-based Formulation
Following Equation 6, we then characterize the dynamics shift compensation term as in the above
model-free analysis in the model-based offline formulation. We will find that across different deriva-
tions, our reward augmentation ∆r has always maintained the functional consistency and simplicity.
Following MOPO, we assume F = {f : kfk∞ ≤ 1}, then we have dF(T0(s0|s, a),T(s0∣s, a))=
1
DTV(T0(s0∣s, a), T(Sls, a)) ≤ (DKL(T0(s0∣s, a), T(s0∣s, a))∕2)2, where DTV is the total variance
distance. Then we introduce the admissible error u(s, a) such that dF (T (s0 |s, a), T(s0 |s, a)) ≤
1
u(s, a) for all (s, a), and η and δ such that (DKL(T0,T)∕2)2 ≤ ηDκL(T0,T) + δ. Following
Lemma 3, we thus can maximize the following lower bound with the samples in MM0 (λ := YRmax):
^ 一
/、	E	z	T0(s0∣s, a)	、/	、	、Ο
ηM (n) ≥	Es,a,s0 〜d" o(s)π(a∣s)To(s0∣s,a)	r(s, a)	- "λ log 九匹.-λu(s, a)	- λδ	∙	(9)
Implementation: We model the dynamics T0 and T with an ensemble of 2*N parameterized Gaus-
sian distributions: N^0(μjo(s, a), Σφ,(s, a)) and NT(μθ(s, a), Σφ(s, a)), where i ∈ [1, N]. We
approximate u with the maximum standard deviation of the learned models in the ensemble:
u(s, a) = maxiN=1kΣφ(s, a)kF, omit the training-independent δ, and treat λ as a hyperparameter
T0
as in MOPO. For the log T- term, we resort to the above classifiers (qsas and qsa) in model-free set-
ting. (See Appendix-A.3.2 for comparison between using classifiers and estimated-dynamics ratio.)
6	Experiments
We present empirical demonstrations of our dynamics-aware reward augmentation (DARA) in a
variety of settings. We start with two simple control experiments that illustrate the significance of
DARA under the domain (dynamics) adaptation setting. Then we incorporate DARA into state-of-
the-art (model-free and model-based) offline RL methods and evaluate the performance on the D4RL
tasks. Finally, we compare our framework to several cross-domain-based baselines on simulated
and real-world tasks. Note that for the dynamics adaptation, we also release a (source) dataset as a
complement to D4RL, along with the quadruped robot dataset in simulator (source) and real (target).
6.1	How does DARA handle the dynamics shift in offline setting?
Source/target
Ogoal
..…×...×.
W
in target
Figure 3: Internal dynamics shift: (left) source
and target MDPs (range of the right-back-leg of
the ant (state[11]) is limited: [-0.52, 0.52] in
source MDP → [-0.26, 0.26] in target MDP);
(right) the solid (orange) line denotes the state
of the right-back-leg over one trajectory col-
lected in source, dashed (blue) line denotes the
learned reward modification -∆r over the tra-
jectory, and green and red slices denote transi-
tion pairs where -∆r ≥ and -∆r < 0, resp.
source target
Figure 2: External dynamics shift: (left) source
and target MDPs (target contains an obstacle rep-
resented with the dashed line); (middle) top plots
(w/o Aug.) depict the trajectories that are gen-
erated by the learned policy with vanilla MPO;
(middle) bottom plots (DARA) depict the tra-
jectories that are generated by the learned pol-
icy with DARA-based MPO; (right) learned Q-
values on the state-action pairs in left subfigure.
Here we characterize both external and internal dynamics shifts: In Map tasks (Figure 2 left), the
source dataset D0 is collected in a 2D map and the target D is collected in the same environment but
with an obstacle (the dashed line); In Ant tasks (Figure 3 left), the source dataset D0 is collected using
the Mujoco Ant and the target D is collected with the same Ant but one joint of which is restricted.
Using MPO, as an example of offline RL method, we train a policy on dataset {D0 ∪ D} and deploy
the acquired policy in both source and target MDPs. As shown in Figure 2 (middle-top, w/o Aug.),
such training paradigm does not produce an adaptive policy for the target. By modifying rewards in
7
Published as a conference paper at ICLR 2022
Table 1: Normalized scores for the (target) D4RL tasks, where our results are averaged over 5 seeds.
The arrows in each four-tuple indicate whether the current performance has improved (↑) or not Q)
compared to the previous value. If 1T+10S DARA achieves comparable (less than 10% degradation)
or better performance compared to baseline 10T, we highlight our scores in bold (in each four-tuple).
Body Mass Shift		10T	1T	1T+10S w/o Aug.	1T+10S DARA	10T	1T	1T+10S w/o Aug.	1T+10S DARA	10T	1T	1T+10S w/o Aug.	1T+10S DARA
				BEAR			BRAC-P					AWR	
豺	Random	11.4	1.0 J	4.6 ↑	8.4 ↑	11.0	10.9 J	9.6 J	11.0 ↑	10.2	10.3 ↑	3.4 J	4.5 ↑
d d	Medium	52.1	0.8 J	0.9 ↑	1.6↑	32.7	29.0 J	29.2 ↑	32.9 ↑	35.9	30.9 J	20.8 J	28.9 ↑
O H	Medium-R	33.7	1.3J	18.2 ↑	34.1 ↑	0.6	5.4 ↑	20.1 ↑	30.8 ↑	28.4	8.8 J	4.1 J	4.2 ↑
	Medium-E	96.3	0.8 J	0.6 J	1.2↑	1.9	34.5 ↑	32.3 J	34.7 ↑	27.1	27.0 J	26.8 J	26.6 J
				BCQ				CQL				MOPO	
	Random	10.6	10.6 J	83 j	9.7 ↑	10.8	10.6 J	巫J	10.4 ↑	11.7	4.8 J	20 J	2.1 ↑
	Medium	54.5	37.1 J	25.7 J	38.4 ↑	58.0	43.0 J	44.9 ↑	59.3 ↑	28.0	4.1 J	5.0 ↑	10.7 ↑
	Medium-R	33.1	9.3 J	28.7 ↑	32.8 ↑	48.6	9.6 J	1.4J	3.7↑	67.5	1.0J	5.5 ↑	8.4↑
	Medium-E	110.9	58 J	75.4 ↑	84.2 ↑	98.7	59.7 J	53.6 J	99.7 ↑	23.7	1.6J	4.8 ↑	5.8 ↑
				BEAR			BRAC-P					AWR	
	Random	7.3	1.5J	3! ↑	3.2 ↑	-0.2	0.0 ↑	1.3 ↑	3.2 ↑	1.5	1.3 J	20 ↑	2.4 ↑
	Medium	59.1	-0.5 J	0.6 ↑	0.3J	77.5	6.4 J	70.0 ↑	78.0 ↑	17.4	14.8 J	17.1 ↑	17.2 ↑
	Medium-R	19.2	0.7 J	6.5 ↑	7.3↑	-0.3	8.5↑	9.9 ↑	18.6 ↑	15.5	7.4 J	1.6J	1.5J
	Medium-E	40.1	-0.1 J	1.5↑	2.3↑	76.9	20.6 J	64.1 ↑	77.5 ↑	53.8	35.5 J	52.5 ↑	53.3 ↑
				BCQ				CQL				MOPO	
	Random	4.9	1.8J	45 ↑	4.8 ↑	7.0	1.7J	32 ↑	3.4↑	13.6	-0.2 J	Zi ↑	-0.1 J
	Medium	53.1	32.8 J	50.9 ↑	52.3 ↑	79.2	42.9 J	80.0 ↑	81.7 ↑	17.8	7.0 J	5.7 J	11.0 ↑
	Medium-R	15.0	6.9 J	14.9 ↑	15.1 ↑	26.7	4.6 J	0.8 J	2.0 ↑	39.0	5.1 J	3.1 J	14.2 ↑
	Medium-E	57.5	32.5 J	55.2 ↑	57.2 ↑	111.0	49.5 J	63.5 ↑	93.3 ↑	44.6	5.3 J	5.5 ↑	17.2 ↑
source D0, we show that applying the same training paradigm on the reward augmented data exhibits
a positive transfer ability in Figure 2 (middle-bottom, DARA). In Figure 2 (right), we show that our
DARA produces low Q-values on the obstructive state-action pairs (in left) compared to the vanilla
MPO, which thus prevents the Q-value weighted-regression on these unproductive state-action pairs.
More generally, we illustrate how DARA can handle the dynamics adaptation from the reward mod-
ification view. In Figure 3 (right), the learned reward modification -∆r (dashed blue line) clearly
produces a penalty (red slices) on these state-action pairs (in source) that produce infeasible next-
state transitions in the target MDP. If we directly apply prior offline RL methods, these transitions
that are beyond reach in target and yet are high valued, would yield a negative transfer. Thus, we can
think of DARA as finding out these transitions that exhibit dynamics shifts and enabling dynamics
adaptation with reward modifications, e.g., penalizing transitions covered by red slices (-∆r < 0).
6.2	Can DARA enable an adaptive policy with reduced offline data in target ?
To characterize the offline dynamics shift, we consider the Hopper, Walker2d and Halfcheetah from
the Gym-MuJoCo environment, using offline samples from D4RL as our target offline dataset. For
the source dataset, we change the body mass of agents or add joint noise to the motion, and, similar
to D4RL, collect the Random, Medium, Medium-R and Medium-E offline datasets for the three en-
vironments. Based on various offline RL algorithms (BEAR, BRAC-p, BCQ, CQL, AWR, MOPO),
we perform the following comparisons: 1) employing the 100% of D4RL data (10T), 2) employing
only 10% of the D4RL data (1T), 3) employing 10% of the D4RL data and 100% of our collected
source offline data (1T+10S w/o Aug.), and 4) employing 10% of the D4RL data and 100% of our
collected source offline data along with our reward augmentation (1T+10S DARA). Due to page
limit, here we focus on the dynamics shift concerning the body mass on Walker2d and Hopper. We
refer the reader to appendix for more experimental details, tasks, and more baselines (BC, COMBO).
As shown in Table 1, in most of the tasks, the performance degrades substantially when we decrease
the amount of target offline data, i.e., 10T → 1T. Training with additional ten times source offline
data (1T+10S w/o Aug.) also does not bring substantial improvement (compensating for the reduced
data in target), which even degrades the performance in some tasks. We believe that such degradation
(compared to 10T) is caused by the lack of target offline data as well as the dynamics shift (induced
by the source data). Incorporating our reward augmentation, we observe that compared to 1T and
1T+10S w/o Aug. that both use 10% of the target offline data, our 1T+10S DARA significantly
improves the performance across a majority of tasks. Moreover, DARA can achieve comparable or
better performance compared to baseline 10T that training with ten times as much target offline data.
8
Published as a conference paper at ICLR 2022
Table 2: Normalized scores in (target) D4RL tasks, where ”Tune” denotes baseline ”fine-tune”. We
observe that with same amount (10%) of target offline data, DARA greatly outperforms baselines.
Body Mass Shift		Tune	DARA	Tune	DARA	Tune	DARA	Tune	DARA	Tune	DARA	^ ∏pT	Tnp
		BEAR		BRAC-p		BCQ		CQL		MOPO		MABE	
	Random	0.8	8.4 ↑	6.0	11.0 ↑	8.8	9.7 ↑	31.6	10.4 I	0.7	2.1 ↑	10.6	9.0
	Medium	0.8	1.6↑	22.7	32.9 ↑	31.7	38.4 ↑	44.5	59.3 ↑	0.7	10.7 ↑	48.8	23.1
	Medium-R	0.7	34.1 ↑	14.7	30.8 ↑	27.5	32.8 ↑	1.3	3.7 ↑	0.6	8.4 ↑	17.1	20.4
	Medium-E	0.9	1.2↑	19.2	34.7 ↑	85.9	84.2 I	47.6	99.7 ↑	2.2	5.8 ↑	28.1	38.9
		BEAR		BRAC-p		BCQ		CQL		MOPO		MABE	
	Random	6.6	3.2 I	3.9	3.21	4.7	4.8 ↑	1.1	3.4 ↑	0.1	-0.1 I	6.0	-0.2
	Medium	0.3	0.3 I	76.0	78.0 ↑	28.4	52.3 ↑	72.3	81.7 ↑	-0.2	11.0 ↑	30.1	56.7
	Medium-R	1.2	7.3 ↑	10.0	18.6 ↑	10.4	15.1 ↑	1.8	2.0 ↑	0.0	14.2 ↑	13.3	12.5
	Medium-E	2.4	2.3 I	74.5	77.5 ↑	22.7	57.2 ↑	68.6	93.3 ↑	7.3	17.2 ↑	43.7	82.7
6.3 Can DARA perform better than cross-domain baselines
In Section 6.2, 1T+10S w/o Aug. does not explicitly learn policy for the target dynamics, thus one
proposal (1T+10S fine-tune) for adapting the target dynamics is fine-tuning the model that learned
with source offline data, using the (reduced) target offline data. Moreover, we also compare DARA
with the recently proposed MABE (Cang et al., 2021), which is suitable well for our cross-dynamics
setting by introducing behavioral priors πp in the model-based offline setting. Thus, we implement
two baselines, 1) 1T+10S MABE ∏pT and 2) 1T+10S MABE T∏p, which denote 1) learning ∏p with
target domain data and T with source domain data, and 2) learning ∏p with source domain data
and T with target domain data, respectively. We show the results for the Walker (with body mass
shift) in Table 2, and more experiments in Appendix A.3.5. Our results show that DARA achieves
significantly better performance than the naive fine-tune-based approaches in a majority of tasks
(67 "↑" vs. 13 "]"，including results in appendix). On twelve out of the sixteen tasks (including
results in appendix), DARA-based methods outperform the MABE-based methods. We attribute
MABE’s failure to the difficulty of the reduced target offline data, which limits the generalization
of the learned ∏p or T under such data. However, such reduced data (10% of target) is sufficient to
modify rewards in the source offline data, which thus encourages better performance for our DARA.
For real-world tasks, we also test DARA in a new offline dataset
on the quadruped robot (see appendix for details). Note that we
can not access the privileged information (e.g., coordinate) in
real robot, thus the target offline data (collected in real-world)
does not contain rewards. This means that prior fine-tune-based
and MABE-based methods become unavailable. However, our
reward augmentation frees us from the requisite of rewards in
Table 3: Average distance cov-
ered in an episode in real robot.
(BCQ)	w/o Aug.	DARA
Medium	0.85	1.35 ↑
Medium-E	1.15	1.41 ↑
Medium-R-E	1.27	1.55 ↑
target domain. We can freely perform offline training only using the augmented source offline data as
long as the learned ∆r is sufficient. For comparison, we also employ a baseline (w/o Aug.): directly
deploying the learned policy with source data into the (target) real-world. We present the results
(deployed in real with obstructive stairs) in Table 3 and videos in supplementary material. We can
observe that training with our reward augmentation, the performance can be substantially improved.
Due to page limit, we refer readers to Appendix A.3.6 for more experimental results and discussion.
7	Conclusion
In this paper, we formulate the dynamics shift in offline RL. Based on prior model-based and model-
free offline algorithms, we propose the dynamics-award reward augmentation (DARA) framework
that characterizes constraints over state-action-next-state distributions. Empirically we demonstrate
DARA can eliminate the dynamics shift and outperform baselines in simulated and real-world tasks.
In Appendix A.2, we characterize our dynamics-aware reward augmentation from the density reg-
ularization view, which shows that it is straightforward to derive the reward modification built on
prior regularized max-return objective e.g., AlgaeDICE (Nachum et al., 2019b). We list some re-
lated works in Table 4, where the majority of the existing work focuses on regularizing state-action
distribution, while dynamics shift receives relatively little attention. Thus, we hope to shift the focus
of the community towards analyzing how dynamics shift affects RL and how to eliminate the effect.
9
Published as a conference paper at ICLR 2022
Reproducibility Statement
Our experimental evaluation is conducted with publicly available D4RL (Fu et al., 2020) and Ne-
oRL (Qin et al., 2021). In Appendix A.4 and A.5, we provide the environmental details and training
setup for our real-world sim2real tasks. In supplementary material, we upload our source code and
the collected offline dataset for the the quadruped robot.
Acknowledgments
We thank Zifeng Zhuang, Yachen Kang and Qiangxing Tian for helpful feedback and discussions.
This work is supported by NSFC General Program (62176215).
References
Abbas Abdolmaleki, Jost Tobias SPringenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin A. Riedmiller. Maximum a posteriori policy optimisation. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OPenReview.net, 2018.
PhiliP J. Ball, Cong Lu, Jack Parker-Holder, and StePhen J. Roberts. Augmented world models
facilitate zero-shot dynamics generalization from a single offline environment. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning
Research,pp. 619-629. PMLR, 2021.
Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral Priors
and dynamics models: Improving performance and domain transfer in offline RL. CoRR,
abs/2106.09119, 2021. URL https://arxiv.org/abs/2106.09119.
Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob Varley, Alex
Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey Levine. Actionable models:
Unsupervised offline reinforcement learning of robotic skills. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1518-
1528. PMLR, 2021. URL http://proceedings.mlr.press/v139/chebotar21a.
html.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pp. 1042-1051. PMLR, 2019.
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action
imitation learning for batch deep reinforcement learning. arXiv preprint arXiv:1910.12179, 2019.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016-2021.
Benjamin Eysenbach and Sergey Levine. If maxent RL is the answer, what is the question? CoRR,
abs/1910.01913, 2019. URL http://arxiv.org/abs/1910.01913.
Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa, Sergey Levine, and Ruslan Salakhutdi-
nov. Off-dynamics reinforcement learning: Training for transfer with domain classifiers. In 9th
International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021. OpenReview.net, 2021.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep
data-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https://arxiv.
org/abs/2004.07219.
10
Published as a conference paper at ICLR 2022
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2052-2062. PMLR,
2019.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2160-2169. PMLR,
2019.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online RL. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,
pp. 3682-3691. PMLR, 2021.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic al-
gorithms and applications. CoRR, abs/1812.05905, 2018. URL http://arxiv.org/abs/
1812.05905.
Behzad Haghgoo, Allan Zhou, Archit Sharma, and Chelsea Finn. Discriminator augmented model-
based reinforcement learning. CoRR, abs/2103.12999, 2021. URL https://arxiv.org/
abs/2103.12999.
Atil Iscen, Ken Caluwaerts, Jie Tan, Tingnan Zhang, Erwin Coumans, Vikas Sindhwani, and
Vincent Vanhoucke. Policies modulating trajectory generators. In 2nd Annual Conference
on Robot Learning, CoRL 2018, Zurich, Switzerland, 29-31 October 2018, Proceedings, Vol-
ume 87 of Proceedings of Machine Learning Research, pp. 916-926. PMLR, 2018. URL
http://proceedings.mlr.press/v87/iscen18a.html.
Nan Jiang and Jiawei Huang. Minimax value interval for off-policy evaluation and policy op-
timization. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1cd138d0499a68f4bb72bee04bbec2d7- Abstract.html.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11761-11771,
2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020a.
11
Published as a conference paper at ICLR 2022
Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not
all you need: Few-shot extrapolation via structured maxent RL. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/
5d151d1059a6281335a10732fc49620e-Abstract.html.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learn-
ing quadrupedal locomotion over challenging terrain. Science Robotics, 5(47), 2020. doi:
10.1126/scirobotics.abc5986. URL https://robotics.sciencemag.org/content/
5/47/eabc5986.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. CoRR, abs/2005.01643, 2020. URL
https://arxiv.org/abs/2005.01643.
Jinxin Liu, Hao Shen, Donglin Wang, Yachen Kang, and Qiangxing Tian. Unsupervised domain
adaptation with dynamics-aware rewards in reinforcement learning. Advances in Neural Informa-
tion Processing Systems, 34, 2021.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the
curse of horizon:	Infinite-horizon off-policy estimation. pp. 5361-5371,
2018.	URL https://proceedings.neurips.cc/paper/2018/hash/
dda04f9d634145a9c68d5dfe53b21272- Abstract.html.
Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-
reinforcement learning with advantage weighting. In Marina Meila and Tong Zhang (eds.), Pro-
ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7780-7791.
PMLR, 2021.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. CoRR,
abs/2001.01866, 2020. URL http://arxiv.org/abs/2001.01866.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Al-
gaedice: Policy gradient from arbitrary experience. CoRR, abs/1912.02074, 2019b. URL
http://arxiv.org/abs/1912.02074.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. CoRR, abs/2006.09359, 2020. URL https://arxiv.org/
abs/2006.09359.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22
(10):1345-1359, 2010. doi: 10.1109/TKDE.2009.191. URL https://doi.org/10.1109/
TKDE.2009.191.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real trans-
fer of robotic control with dynamics randomization. In 2018 IEEE International Conference on
Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp. 1-8. IEEE,
2018. doi: 10.1109/ICRA.2018.8460528. URL https://doi.org/10.1109/ICRA.
2018.8460528.
12
Published as a conference paper at ICLR 2022
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. URL
http://arxiv.org/abs/1910.00177.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth
AAAI Conference on Artificial Intelligence, 2010.
Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang,
and Yang Yu. Neorl: A near real-world benchmark for offline reinforcement learning. arXiv
preprint arXiv:2102.00714, 2021.
Y. Sakakibara, K. Kan, Y. Hosoda, M. Hattori, and M. Fujie. Foot trajectory for a quadruped
walking machine. In EEE International Workshop on Intelligent Robots and Systems, Towards a
New Frontier OfApplications,pp.315-322vol.1,1990. doi:10.1109/IROS.1990.262407.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020.
Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on ap-
proximating dynamic programming. In Bruce W. Porter and Raymond J. Mooney (eds.),
Machine Learning, Proceedings of the Seventh International Conference on Machine Learn-
ing, Austin, Texas, USA, June 21-23, 1990, pp. 216-224. Morgan Kaufmann, 1990. doi:
10.1016/b978-1-55860-141-3.50030-4.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. 2017.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-
policy evaluation. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
Research, pp. 9659-9668. PMLR, 2020. URL http://proceedings.mlr.press/v119/
uehara20a.html.
Nino Vieillard, Tadashi Kozuno, BrUno Scherrer, Olivier PietqUin, Remi Munos, and MatthieU
Geist. Leverage the average: an analysis of KL regularization in reinforcement learning. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Xingxing Wang. Unitree robotics. https://www.unitree.com/products/a1, 2020.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
CoRR, abs/1911.11361, 2019. URL http://arxiv.org/abs/1911.11361.
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline
reinforcement learning. CoRR, abs/2107.09003, 2021. URL https://arxiv.org/abs/
2107.09003.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation
via the regularized lagrangian. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/488e4104520c6aab692863cc1dba45af- Abstract.html.
13
Published as a conference paper at ICLR 2022
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: model-based offline policy optimization. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
COMBO: conservative offline model-based policy optimization. CoRR, abs/2102.08363, 2021.
URL https://arxiv.org/abs/2102.08363.
Hongyin Zhang, Jilong Wang, Zhengqing Wu, Yinuo Wang, and Donglin Wang. Terrain-aware
risk-assessment-network-aided deep reinforcement learning for quadrupedal locomotion in tough
terrain. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 4538-4545.IEEE.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline esti-
mation of stationary values. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:
//openreview.net/forum?id=HkxlcnVFwB.
14
Published as a conference paper at ICLR 2022
A Appendix
A. 1 Derivation
A.1.1 Proof of Lemma 3
Let BMV(S) = Ea〜∏(a∣s) [r(s, a) + γEs0〜T3∣s,a) [V(s0)]] and r(s) = Ea〜∏(a∣s) [r(s, a)]. Then,
we have
ηM^0 (π) - ηM (π) = ES0〜ρo(s) [VMM0 (SO)- VM (so)]
∞
=EY tEst 〜P(st∣π,Μ o,t)Eat~π(at∣St) [r(st, at)] - Es0~ρo(s) [VM (s0)]
t=0
∞
=EY 'Es,〜P(st∣π,Mo,t) [r(st) + VM (St)- VM(St)] - ES0~pθ(s) [VM(s0)]
t=0
∞
=X Y'Est〜P(st∣π,M0,t)	[r(st) + YVM (s'+1) - VM (s')]
t=0	St+ι~P (st+ι∣∏,M 0,t+1)
∞
=EY'Est〜P(st∣π,M0,t)	[r(St) + γVM (St+1) - (r(St) + YEa〜π(a∣st),s0〜T(st,a) [VM (SO)])]
t=0	st+ι~P(st+ι ∣π,M0,t+1)
∞
=X Y tEst 〜P(st∣π,M 0,t) [BM 0 VM (St)-BM VM (St)]
t=0
=ES〜dM0 (s) [Bm0 VM (S)-BMVM (S)].
A.1.2 Model-based formulation
Here we provide detailed derivation of the lower bound in Equation 9 in the main text.
Assumption 1 Assume a scale c and a function class F such that VM ∈ cF.
Following MOPO (Yu et al., 2020), we set F = {f : kfk∞ ≤ 1}. In Section Preliminaries, we
have that the reward function is bounded: r(S, a) ∈ [-Rmax, Rmax]. Thus, we have kVM k∞ ≤
P∞=0 YtRmax = R-Yx and hence the scale c = R-ax.
As a direct corollary of Assumption 1 and Equation 5, we have
〜dM0 (s) [BM^0 VM (S)-BMVM (s)]| ≤ Yc ∙ Es,a~dM0 (s)π(a∣s) hdF(TO(Sls, a),T(s1s, a))] ∙
(10)
Further, we find
dF(TO(S0∣s, a),T (s0∣s, a)) ≤ dF(T0(s0∣s, a),T(s0∣s, a)) + dF(T(S0∣s, a),T(s0∣s, a))	(11)
For the first term dF(TO(sO |s, a), T(sO |s, a)) in Equation 12, through Pinsker’s inequality, we have
1
dF(T (s |s, a),T(s |s, a)) = DTV(T (s |s, a),T(s |s, a)) ≤ N2DKL(T0(s0∣s, a),T(s0∣s, a))
(12)
To keep consistent with the DARA-based method-free offline methods, we introduce scale η and
bias δ to eliminate the square root in Equation 12. To be specific, we assume2 scale η and bias δ
such that J1 DKL(T0, T) ≤ ηDκL(T0, T) + δ. Thus, we obtain
dF(T0(s0∣s, a),T(s0∣s, a)) = DTV(T0(s0∣s, a),T(s0∣s, a)) ≤ ηDκL(T0(s0∣s, a),T(s0∣s, a)) + δ
(13)
2In implementation, We clip the maximum deviation of log T(S0|：aa) for each (s, a, s0), which thus makes
DKL(T0(s0∣s, a),T(s0∣s, a)) bounded.
15
Published as a conference paper at ICLR 2022
For the second term dF (T (s0 |s, a), T(s0|s, a)) in Equation 11, we assume that we have access to an
oracle uncertainty qualification module that provides an upper bound on the error of the estimated
empirical MDP M := {S, A, r, T, ρo,γ}.
Assumption 2 LetF be the function class in Assumption 1. We say u : S × A → R is an admissible
error estimator for T if dF(T (s0|s, a), T (s0|s, a)) ≤ u(s, a) for all (s, a).
Thus, we have
Es,a~d^o(s)π(a∣s) [dF(T (S |s, a), T (S |s, a))] ≤ Es,a~d^z(s)π(a∣s) [u(s, a)]	(14)
Bring Inequations 10, 11, 13, and 14 into Lemma 3, we thus have
ηM (n) ≥ Es,a,s0〜dπ^o (s)π(a∣s)T0(s0∣s,a)
A.1.3 Learning Classifiers
Applying Bayes’ rule, we have
^ ^ . , .. ^
TO(Sls, a)
r(s, a) — ηγclog-------------Ycu(s, a) 一 γcδ
T(S0|S,a)
(15)
T0(s0∣a, s) ：= P(Sls, a, source)
p(source|S, a, S0)p(S, a, S0)
p(source|S, a)p(S, a)
T(S0|a, s) ：= P(Sls, a, target)
P(target|S, a, S0)P(S, a, S0)
P(target|S, a)P(S, a)
Then We parameterizep(∙∣s, a, s0) andp(∙∣s, a) with the two classifiers q$a$ and q$a respectively. Us-
ing the standard cross-entropy loss, we learn qsas and qsa with the following optimization objective:
max	E(s,a,s0)〜D0 [log qsas(source∣s, a, s0)] + E(s,a,s，)〜D [log qsas(target∣s, a, s0)],
max	E(s,a)〜D0 [log qsa(source∣s, a)] + E(s,a)〜D [log qsa(target∣s, a)].
With the trained qsas and qsa, we have
log TO(S0∣s, a)	= log	qsas(source∣s, a, s0)	- log	qsa(source∣s, a)
g T(s0∣s,a) g	qsas(target∣s, a,s0) g	qsa(target|s,a)	.
(16)
In our implementation, we also clip the above reward modification between -10 and 10.
A.2 Regularization View of Dynamics-Aware Reward Augmentation
Here we shortly characterize our dynamics-aware reward augmentation from the density regulariza-
tion. Note the standard max-return objective ηM (π) in RL can be written exclusively in terms of the
on-policy distribution d，M (s)π(a∣s). To introduce an off-policy distribution dD(s)∏b(a|s) in the ob-
jective, prior works often incorporate a regularization (penalty): D("m (s)π(a∣s)∣∣dD(s)∏b(a∣s)), as
in Equations 2 and 3. However, facing dynamics shift, such regularization should take into account
the transition dynamics, which is penalizing D("m(s)π(a∣s)T(s0∣s, a)kdD,(s)∏b,(a∣s)T0(s0∣s, a)).
From this view, it is also straightforward to derive the reward modification built on prior regularized
off-policy max-return objective e.g., the off-policy approach AlgaeDICE (Nachum et al., 2019b).
In Table 4, we provide some related works with respect to the (state-action pair) d。(s)∏b(a∣s)
regularization and the (state-action-next-state pair) dg，(s)∏b,(a∣s)T0(s0∣s, a) regularization. We
can find that the majority of the existing work focuses on regularizing state-action distribution, while
dynamics shift receives relatively little attention. Thus, we hope to shift the focus of the community
towards analyzing how the dynamics shift affects RL and how to eliminate the effect.
16
Published as a conference paper at ICLR 2022
Table 4: Some related works with explicit (state-action p(s, a) or state-action-next-state p(s, a, s0))
regularization. More papers with respect to unsupervised RL, inverse RL (imitation learning), meta
RL, multi-agent RL, and hierarchical RL are not included.
reg. with dD (s)πb (a|s)
reg. with dD0 (s)πb0 (a|s)T 0(s0|s, a)
Online:
see summarization in Geist et al.
(2019) and Vieillard et al. (2020).
Eysenbach et al. (2021) (DARC)
Liu et al. (2021) (DARS);
Haghgoo et al. (2021)
Offline (off-policy evaluation):
Fujimoto et al. (2019) (BCQ);
Wu et al. (2019) (BRAC-p);
Peng et al. (2019) (AWR);
Wang et al. (2020) (CRR);
Chen et al. (2019); (BAIL)
Xu et al. (2021) (CPQ);
Liu et al. (2018);
Nachum et al. (2019b) (AlgaeDICE);
Yang et al. (2020);
Jiang & Huang (2020);
Yu et al. (2020) (MOPO);
Yu et al. (2021) (COMBO);
Kumar et al. (2019) (BEAR);
Abdolmaleki et al. (2018) (MPO);
Nair et al. (2020) (AWAC);
Siegel et al. (2020);
Kumar et al. (2020a) (CQL);
Kostrikov et al. (2021) (Fisher-BRC);
Nachum et al. (2019a) (DualDICE);
Zhang et al. (2020) (GenDICE);
Nachum & Dai (2020);
Uehara et al. (2020);
Kidambi et al. (2020) (MOReL);
Cang et al. (2021) (MABE);
A.3 More Experiments
A.3.1 TRAINING WITH {D0 ∪D}
As we show in Figure 1 in Section Introduction, the performance of prior offline RL methods dete-
riorates dramatically as the amount of (target) offline data D decreases. In Figure 4, we show that
directly training with the mixed dataset {D0 ∪ D} will not compensate for the deteriorated perfor-
mance caused by the reduced target offline data, and training with such additional source offline data
can even lead the performance degradation in some tasks.
Medium-Replay
50%	20%	10%	5%
Amount of target data used for training
CQL with Source	CQL
Medium-Replay
20
50%	20%	10%	5%
Amount of target data used for training
MOPO with Source MOPO
W50
eoəs p3zqBaUON
aɪoəs paZqBaUON
CQL with Source	CQL	MOPO with Source	MOK)
Figure 4: Final performance on the D4RL (Walker2d) task: The orange bars denote the final per-
formance with different amount (50%D, 20%D, 10%D, 5%D) of target offline data; The blue bars
denote the final performance of mixing 100% of source offline data D0 and different amount of target
data x%D (x ∈ [50, 20, 10, 5]), i.e., training with {100%D0 ∪ x%D}; The red lines denote the final
performance of training with 100% of target offline data D. We can observe that 1) the performance
deteriorates dramatically as the amount of (target) offline data decreases (100%D (red line) →
50%D (orange bar) → 20%D (orange bar) → 10%D (orange bar) → 5%D (orange bar)), 2) after
training with the additional 100% of source offline data, {100%D0 ∪ x%D}, the final performance
is improved in some tasks, but most of the improvement is a pittance compared to the original per-
formance degradation (compared to that training with the 100% of target offline data, i.e., the red
lines), and 3) what is worse is that adding source offline data D0 even leads performance degradation
in some tasks, e.g., CQL with 50%D and 20%D in Medium-Random.
17
Published as a conference paper at ICLR 2022
A.3.2 Comparison between learning classifiers and learning dynamics (for
the reward modification)
Table 5: Normalized scores for the Hopper tasks with the body mass (dynamics) shift. Rat. and Cla.
denote estimating the reward modification with the estimated-dynamics ratio and learned classifiers
(Appendix A.1.3), respectively.
Body Mass Shift BEAR	BRAC-p	AWR
BCQ
CQL	MOPO
JOddo
	Rat.		Cla.	Cla.		Cla.	Rat.		Cla.	Rat.	Cla.		Rat.		Cla.	Rat.		Cla.
Random	9.9	>	8.4	11.2	>	11.0	3.7	<	4.5	8.5	<	9.7	11.8	>	10.4	1.8	<	2.1
Medium	0.8	<	1.6	31.7	<	32.9	18.0	<	28.9	33.2	<	38.4	45.9	<	59.3	3.1	<	10.7
Medium-R	28.4	<	34.1	36.5	>	30.8	2.5	<	4.2	33.9	>	32.8	2.0	<	3.7	3.8	<	8.4
Medium-E	0.8	<	1.2	50.9	>	34.7	45.8	<	26.6	68.4	<	84.2	107.3	>	99.7	5.7	<	5.8
In Table 5, we show the comparison between learning classifiers and learning dynamics (for our
reward modification) in the Hopper tasks. We can observe that the two schemes for estimating the
reward modification have similar performance. Thus, for simplicity and following Eysenbach et al.
(2021), we adopt the classifiers to modify rewards in the source offline data in our experiments.
A.3.3 More Examples with respect to the reward augmentation
9Z.0 9Z.0—
吟—
9zd9Z0I
〔二〕卷 S
9Z O 9z ol
〔二〕卷 S
Figure 5: We can observe that our reward augmentation 1) encourages (-∆r > 0, i.e., the green
slice parts) these transitions (-0.26 ≤ next-state[11] ≤ 0.26) that have the same dynamics with the
target environment, and 2) discourages (-∆r < 0, i.e., the red slice parts) these transitions that have
different (unreachable) dynamics (next-state[11] ≤ -0.26 or next-state[11] ≥ 0.26) in the target.
In Figure 5, we provide more examples with respect to the reward augmentation in the Ant task in
Figure 3 (left).
A.3.4 COMPARISON BETWEEN 10T, 1T, 1T+10S w/o Aug., AND 1T+10S DARA
Based on various offline RL algorithms (BEAR (Kumar et al., 2019), BRAC-p (Wu et al., 2019),
BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020a), AWR (Peng et al., 2019), MOPO (Yu
et al., 2020), BC (behavior cloning), COMBO (Yu et al., 2021)), we provide the additional results in
Tables 6, 7, 8, 9, and 10.
Table 6:	Normalized scores for the Hopper tasks with the body mass (dynamics) shift. (The com-
parison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)
Body Mass Shift 10T
1T+10S	1T+10S
1T w/o Aug. DARA 10T
1T+10S	1T+10S	1T+10S	1T+10S
w/o Aug. DARA 10T 1T w/o Aug. DARA
BC	COMBO
Joddo
Random	9.8	9.8 ↑	6.9 J	10.1 ↑	17.9	0.7 J	5.4↑	4.6 J
Medium	29.0	27.9 J	17.6 J	25.0 ↑	94.9	1.8J	33.7 ↑	45.7 ↑
Medium-R	11.8	7.8 J	7.7 J	11.6 ↑	73.1	13.1 J	11.0 J	27.9 ↑
Medium-E	111.9	21.5 J	20.8 J	35.7 ↑	111.1	0.8 J	14.9 ↑	108.1 ↑
18
Published as a conference paper at ICLR 2022
Table 7:	Normalized scores for the Hopper tasks with the joint noise (dynamics) shift.
Joint Noise Shift 10T
1T+10S	1T+10S	1T+10S	1T+10S	1T+10S	1T+10S
w/o Aug. DARA 10T 1T w/o Aug. DARA 10T 1T w/o Aug. DARA
BEAR	BRAC-p	AWR
Random	11.4	0.6 J	7.4↑	4.2 J	11.0	10.8 J	10.0 J	10.8 ↑	10.2	10.1 J	3.6 J	4.0↑
Medium	52.1	0.8 J	2.0↑	2.0 J	32.7	26.6 J	27.6 ↑	37.6 ↑	35.9	30.3 J	38.8 ↑	41.3 ↑
Medium-R	33.7	2.7 J	3.6 ↑	9.9 ↑	0.6	13.4 ↑	89.9 ↑	101.4 ↑	28.4	12.4 J	6.7 J	7.2 ↑
Medium-E	96.3	0.8 J	0.8 J	1.4↑	1.9	19.8 ↑	57.6 ↑	87.8 ↑	27.1	25.5 J	27.0 ↑	27.0 J
BCQ	CQL	MOPO
jəddo
Random	10.6	10.5 J	7.0J	9.6 ↑	10.8	10.4 J	10.4 J	10.8 ↑	11.7	1.5J	1.3J	2.9 ↑
Medium	54.5	45.8 J	49.0 ↑	54.4 ↑	58.0	46.2 J	58.0 ↑	58.0 J	28.0	2.7 J	9.2 ↑	17.3 ↑
Medium-R	33.1	13.0 J	23.8 ↑	32.0 ↑	48.6	13.6 J	2.6 J	3.6 ↑	67.5	0.8 J	2.3 ↑	6.4↑
Medium-E	110.9	44.6 J	96 ↑	109↑	98.7	50.7 J	73.4 ↑	108.9 ↑	23.7	1J	6.1 ↑	7.5 ↑
BC	COMBO
Random	9.8	9.8 ↑	7.5 J	9.1 ↑	17.9	0.7 J	1.8 ↑	4.9 ↑
Medium	29.0	27.9 J	29.0 ↑	29.0 ↑	94.9	1.8J	0.7 J	9.6 ↑
Medium-R	11.8	7.8 J	8.5 ↑	11.3 ↑	73.1	13.1 J	4.0 J	9.6 ↑
Medium-E	111.9	21.5 J	53.5 ↑	77.9 ↑	111.1	0.8 J	34.0 ↑	45.9 ↑
Table 8:	Normalized scores for the Walker2d tasks with the body mass (dynamics) shift. (The
comparison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)
Body Mass Shift 10T
1T+10S	1T+10S
1T w/o Aug. DARA 10T
1T
1T+10S	1T+10S	1T+10S	1T+10S
w/o Aug. DARA 10T 1T w/o Aug. DARA
Pn
BC	COMBO
Random	1.6	0.1 J	1.7 ↑	2.7 ↑	7.0	1.8J	2.0 ↑	3.5 ↑
Medium	6.6	5.5 J	3.8 J	6.6 ↑	75.5	-1.0 J	23.9 ↑	36.6 ↑
Medium-R	11.3	6.6 J	8.1 ↑	11.0 ↑	56.0	0.1 J	11.4 ↑	22.6 ↑
Medium-E	6.4	3.1 J	6.0 ↑	6.2 ↑	96.1	-0.9 J	-0.1 ↑	-0.1 ↑
Table 9: Normalized scores for the Walker2d tasks with the joint noise (dynamics) shift.
Joint Noise Shift		10T	1T	1T+10S w/o Aug.	1T+10S DARA	10T	1T	1T+10S w/o Aug.	1T+10S DARA	10T	1T	1T+10S w/o Aug.	1T+10S DARA
				BEAR			BRAC-P					AWR	
	Random	7.3	2.2 J	0.6 J	2.6 ↑	-0.2	2.8 ↑	3.3 ↑	8.8 ↑	1.5	0.9 J	15 ↑	1.5J
	Medium	59.1	-0.4 J	0.6 ↑	0.1 J	77.5	28.8 J	55.2 ↑	72.9 ↑	17.4	12.2 J	17.2 ↑	17.2 J
	Medium-R	19.2	0.4J	4↑	10.4 ↑	-0.3	6.3 ↑	32.1 ↑	34.8 ↑	15.5	6J	1.4J	2.1 ↑
	Medium-E	40.1	-0.2 J	0.8 ↑	0.6 J	76.9	21.8 J	62.3 ↑	74.3 ↑	53.8	40.4 J	53 ↑	53.6 ↑
				BCQ				CQL				MOPO	
	Random	4.9	3.7 J	3.4 J	5.2 ↑	7	0.5 J	2.7 ↑	6.4↑	13.6	-0.3 J	-0.2 ↑	-0.2 J
	Medium	53.1	43 J	44.9 ↑	52.7 ↑	79.2	43.9 J	73.2 ↑	81.2 ↑	17.8	5.8 J	7.8 ↑	12.2 ↑
	Medium-R	15	5.7 J	9.8 ↑	14.6 ↑	26.7	1.8J	1.4J	1.8↑	39	0.8 J	9.3 ↑	16.4 ↑
	Medium-E	57.5	44.5 J	40.6 J	57.2 ↑	111	46.8 J	109.9 ↑	116.5 ↑	44.6	2.9 J	15.2 ↑	26.3 ↑
				BC			COMBO						
	Random	1.6	0.1 J	0.9 ↑	1.6↑	7.0	1.8J	0.1 J	1.5↑				
	Medium	6.6	5.5 J	6.4↑	6.5 ↑	75.5	-1.0 J	0.4 ↑	0.7 ↑				
	Medium-R	11.3	6.6 J	4.6J	10.4 ↑	56.0	0.1 J	5.6 ↑	7.4↑				
	Medium-E	6.4	3.1 J	6.2↑	6.4↑	96.1	-0.9 J	0.8 ↑	-0.1 J				
19
Published as a conference paper at ICLR 2022
Table 10: Normalized scores for the Halfcheetah tasks with the joint noise (dynamics) shift.
Joint Noise Shift 10T
1T+10S	1T+10S	1T+10S	1T+10S	1T+10S	1T+10S
w/o Aug.	DARA	10T 1T w/o Aug.	DARA	10T 1T w/o Aug.	DARA
BEAR
Random	25.1	17.8 J	25.0 ↑	25.1 ↑
Medium	41.7	-0.2 J	0.8 ↑	1.5↑
Medium-R	38.6	9.3 J	-0.6 J	-0.5 ↑
Medium-E	53.4	-1.2 J	1.0↑	-1.4 J
BCQ
Random	2.2	2.3 ↑	2.2 J	2.3 ↑
Medium	40.7	37.6 J	40.0 ↑	48.6 ↑
Medium-R	38.2	1.1J	39.4 ↑	41.3 ↑
Medium-E	64.7	37.3 J	55.3 ↑	76.9 ↑
BC
Random	2.1	2.0 J	2.2 ↑	2.2 ↑
Medium	36.1	36.5 ↑	49.4 ↑	49.8 ↑
Medium-R	38.4	36.5 J	24.6 J	15.7 J
Medium-E	35.8	36.3 ↑	49.0 ↑	49.3 ↑
BRAC-p
AWR
24.1	10.0 J	25.0 ↑	26.7 ↑	2.5	2.7 ↑	3.1 ↑	48.9 ↑
43.8	43.0 J	52.4 ↑	53.0 ↑	37.4	38.2 ↑	48.7 ↑	37.4 J
45.4	2.5 J	-2.3 J	45.3 ↑	40.3	2.6 J	2.3 J	2.3 J
44.2	6.9 J	0.9 J	45.3 ↑	52.7	32.2 J	80.6 ↑	79.2 J
		CQL				MOPO	
35.4	-2.3 J	-2.4 J	10.4 ↑	35.4	2.3 J	1.2 J	1.1 J
44.4	35.4 J	40.7 ↑	52.6 ↑	42.3	3.2 J	3.5 J	5.3 ↑
46.2	0.6J	2.0 ↑	1.9J	53.1	-0.1 J	2.6 ↑	4.2↑
62.4	-3.3 J	7.7 ↑	1.7J	63.3	4.2 J	1.5J	7.2↑
	COMBO						
38.8	24.0 J	18.7 J	20.3 ↑				
54.2	15.7 J	14.9 J	15.9 ↑				
55.1	-2.6 J	-2.4 ↑	4.8 ↑				
90.0	4.4J	6.5 ↑	11.1 ↑				

A.3.5 Comparison with the cross-domain based baselines
In Tables 11 and 12, we provide the comparison between our DARA-based methods, fine-tune
based methods, and MABE-based methods in Hopper and Walker2d tasks, over the dynamics shift
concerning the joint noise of motion. We can observe that in a majority of tasks, our DARA-
based methods outperforms the fine-tune-based method (67 'T' vs. 13 "]"，including the results in
the main text). Moreover, our DARA can achieve comparable or better performance compared to
MABE-based baselines on eleven out of sixteen tasks (including the results in the main text).
Table 11:	Normalized scores in the (target) D4RL Hopper tasks with the joint noise shift., where
”Tune” denotes baseline ”fine-tune”.
Joint Noise Shift Tune DARA Tune DARA Tune DARA Tune DARA Tune DARA πpT Tnp
BEAR	BRAC-p	BCQ	CQL	MOPO	MABE
J°ddo
Random	0.8	4.2 ↑	6.4	10.8 ↑	8.1	9.6 ↑	32.2	10.8 J	0.6	2.9 ↑	10.8	8.1
Medium	1.9	2.0 ↑	44.9	37.6 J	47.7	54.4 ↑	52.5	58.0 ↑	0.8	17.3 ↑	63.5	57.7
Medium-R	0.7	9.9 ↑	32.4	101.4 ↑	29.6	32.0 ↑	1.3	3.6 ↑	1.8	6.4 ↑	21.5	35.4
Medium-E	0.8	1.4↑	98.2	87.8 J	90.5	109.0 ↑	107.3	108.9 ↑	4.9	7.5 ↑	15.5	104.8
Table 12:	Normalized scores in the (target) D4RL Walker2d tasks with the joint noise shift., where
”Tune” denotes baseline ”fine-tune”.
Joint Noise Shift Tune DARA Tune DARA Tune DARA Tune DARA Tune DARA ∏pT Tnp
BEAR	BRAC-p	BCQ	CQL	MOPO MABE
PZJə-am
Random	2.7	2.6 φ	1.4	8.8 ↑	3.0	5.2 ↑	6.7	6.4 J	-0.4	-0.2 ↑	5.0	-0.2
Medium	0.5	0.1 J	55.8	72.9 ↑	45.1	52.7 ↑	76.6	81.2 ↑	7.0	12.2 ↑	49.4	48.7
Medium-R	3.2	10.4 ↑	12.2	34.8 ↑	13.5	14.6 ↑	-0.4	1.8↑	1.9	16.4 ↑	4.5	1.6
Medium-E	-0.4	0.6 ↑	71.7	74.3 ↑	44.8	57.2 ↑	104	116.5 ↑	11.3	26.3 ↑	84.7	82.6
A.3.6 Additional results on the quadruped robot
In this offline sim2real setting, We collect the source offline data in the simulator (106 or 2 * 106
steps) and target offline data in the real world (3 * 104 steps). See Appendix A.4 for details. For
testing, We directly deploy the learned policy in the real (flat or obstructive) environment and adopt
the average distance covered in an episode (300 steps) as our evaluation metrics.
20
Published as a conference paper at ICLR 2022
Figure 6: Illustration of the real environment (for testing): (left) the flat and static environment,
(right) the obstructive and dynamic environment.
Table 13: Average distance (m) covered in an episode (300 steps) in flat and static (real) environment.
Sim2real (Flat and Static)	w/o AUg.	DARA	w/o AUg.	DARA	w/o AUg.	DARA
	BCQ		CQL		MOPO	
Medium	1.56	1.64 ↑	1.80	1.82 ↑	0.00	0.00
QUadrUPed Robot	MediUm-R	0.00	0.00	—	—	—	—
Medium-E	2.16	2.47 ↑	2.03	2.02 ；	0.00	0.00
MediUm-R-E	1.69	2.28 ↑	0.00	0.00	—	—
Average Performance imProvement		13.6%		0.2%		0.0%
(Flat and static environment) We first deploy our learned policy in the flat and static environment.
The results (distance covered in an episode) are provided in Table 13.
1)	BCQ (Figure 7): We find that with Medium-R offline data, w/o Aug. BCQ and DARA BCQ both
could not acquire the locomotion skills, which we think is caused by the lack of high-quality offline
data. With more ”expert” data (Medium-R → Medium → Medium-E, or Medium-R → Medium-
R-E), w/o-Aug. BCQ allows for progressive performance (0.00 → 1.56 → 2.16, or 0.00 → 1.69 in
BCQ), but with our reward augmentation, such performance can be further improved (with average
improvement 13.6%).
2)	CQL (Figure 8): We find that with Medium-R or Medium-R-E offline data, w/o Aug. CQL and
DARA CQL both could not learn the locomotion skills, which we think is caused by the low-quality
”Replay” offline data. With Medium or Medium-E offline data, w/o Aug. CQL and DARA CQL
acquire similar performance on this flat and static environment.
3)	MOPO: We find that the model-based MOPO (both w/o Aug. and DARA) could hardly learn the
locomotion skill under the provided offline data.
Table 14: Average distance (m) covered in an episode (300 steps) in the obstructive and dynamic
(real) environment.
Sim2real (ObstrUctive and Dynamic)	w/o AUg.	DARA	w/o AUg.	DARA	w/o AUg.	DARA
	BCQ		CQL		MOPO	
Medium	0.85	1.35 ↑	0.92	1.40 ↑	—	—
QUadrUPed Robot	Medium-R	—	—	—	—	—	—
Medium-E	1.15	1.41 ↑	0.77	1.32 ↑	—	—
MediUm-R-E	1.27	1.55 ↑	—	—	—	—
Average Performance imProvement		25.9%		30.9%		—
(Obstructive and dynamic environment) We then deploy our learned policy in the obstructive and
dynamic environment. The results (distance covered in an episode) are provided in Table 14.
1)	BCQ (Figure 9): In this obstructive environment, we can obtain similar results as in the flat
environment. With more ”expert” data (Medium → Medium-E → Medium-R-E), w/o Aug. BCQ
allows for progressive performance (0.85 → 1.15 → 1.27), and with our reward augmentation,
such performance can be further improved (with average improvement 25.9%). At the same time,
we can also find that due to the presence of environmental obstacles, the performance of both w/o
21
Published as a conference paper at ICLR 2022
Aug. BCQ and w/o Aug. DARA is decreased compared to the deployment on the flat environment.
However, we find that our DARA exhibits greater average performance improvement under this
obstructive environment (13.6% → 25.9%) compared to that in the flat environment. These results
demonstrate that our DARA can learn an adaptive policy for the target environment and thus show
a greater advantage over w/o-Aug. in more complex environments.
2)	CQL (Figure 10): Similar to BCQ, our DARA CQL exhibits a greater performance improvement
over baseline in the obstructive and dynamic environment (0.2% → 30.9%) compared to that in the
flat and static environment.
In summary, The results in the quadruped robot tasks support our conclusion in the main text
regarding the dynamics shift problem in offline RL — with only modest amounts of target offline
data (3 * 104 steps), DARA-based methods can acquire an adaptive policy for the (both flat and
obstructive) target environment and exhibit better performance compared to baselines under the
dynamics adaptation setting.
Figure 7: Deployment on the flat and static environment of BCQ.
Medium (w∕o Aug.)
Medium (DARA)
Medium-E (w∕o Aug.)
Medium-R-E (w∕o Aug.)
Medium-R-E (DARA
Figure 8: Deployment on the flat and static environment of CQL.
22
Published as a conference paper at ICLR 2022
Figure 9: Deployment on the obstructive and dynamic environment of BCQ.
A.3.7 Ablation study with respect to the amount of target offline data
To see whether the amount of target offline data can be further reduced, we show the results of the
ablation study with respect to the amount of target offline data in Tables 15 and 16.
Table 15: Ablation study with respect to the amount of target Hopper data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target offline data, respectively, and
additional 100% source offline data.
Body Mass Shift	10%	5%	1%	10%	5%	1%	10%	5%	1%	10%	5%	1%
HOPPer	MediUm-R Medium-E	BEAR	BRAC-p	BCQ	CQL 34.1 ~107~64 30.8~277~200 32.8~201 ~l63 -37	32~2J 1.2	0.6 0.6	34.7 25.1	20.6	84.2 65.1	55.6 99.7 52.3 38.5
Table 16: Ablation study with respect to the amount of target Walker2d data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target offline data, respectively, and
additional 100% source offline data.
BOdy Mass Shift	10%	5%	1%	10%	5%	1%	10%	5%	1%	10%	5%	1%
Walker2d	Medium-R MediUm-E	BEAR	BRAC-p	BCQ	CQL 7.3	5.9	1.3	18.6	21.6	15.8	15.1	12.7	9.7	2.0	1.3	0.5 2.3	-0.2	-0.3	77.5	2.0	-0.2	57.2	29.7	20.6	93.3	0.1	-0.3
A.3.8 Illustration of whether the learned policy is limited to the source
OFFLINE DATA
If we directly perform DARA with only the source offline data D0 , the learned behaviors will be
restricted to the source offline data. For example, in the Map task, collecting source dataset with the
obstacle and collecting target dataset without the obstacle. In this case, it can be harder for DARA
(with only the source D0) to capture the change in the transition dynamics, thus harder for the agent
23
Published as a conference paper at ICLR 2022
to figure out the new optimal policy (the shorter path without the obstacle). However, as stated in
Algorithm 1, we perform offline RL algorithms with both target offline data and source offline data
{D0 ∪ D}. Thus, to some extent, such limitation can be overcome as long as offline RL algorithm
captures the information (eg. the short path without the obstacle) contained in the (limited) target
D, see Figure 11 for the illustration.
target	target
target	target	target
VKVa
VKVa
VKVa
VKVa
Ok	1k	2k	5k
VKVa
IOk
Figure 11: We exchange the source environment and the target environment in Figure 2 (in the main
text) so that the source environment has an obstacle and the target environment has no obstacles. In
the source domain, we collect 100k of random transitions. In the target domain, we collect 0k, 1k,
2k, 5k, and 10k random transitions respectively. We set η = 0.1. We can find that if we perform
DARA with only source offline data D0 (i.e., 0k target data), we indeed can not acquire the optimal
trajectory (eg. the short path without the obstacle). However, even there is no transition of passing
through obstacles in the source data, performing DARA with {D0 ∪ D} enables us to acquire the
behavior of moving through obstacles. As we increase the number of target offline data D, training
with {D0 ∪ D} can gradually acquire optimal trajectories.
A.3.9 Comparison between DARA and importance sampling (IS) based dynamics
CORRECTION
In Table 17, we report the experimental comparison between DARA and importance sampling based
dynamics adaption. We can find that in most of the tasks, our DARA performs better than the IS-
based approaches.
Table 17: Comparison between DARA and importance sampling (IS) based dynamics correction.
Body Mass Shift
IS
DARA
IS
DARA	IS DARA
BEAR	BRAC-p	AWR
JOddOH JOddOH
Random	4.6 ± 2.8	8.4 ± 1.2	10.8 ± 0.5	11 ± 0.6	10.2 ± 0.3	4.5 ± 0.9
Medium	1 ± 0.4	1.6 ± 1	17.4 ± 10.6	32.9 ± 7.5	24.8 ± 7.7	28.9 ± 5.5
Medium-R	17.3 ±4.7	34.1 ± 5.8	21.6 ± 8.3	30.8 ± 4.9	14 ± 2.2	4.2± 3.5
Medium-E	0.8 ± 0.2	1.2 ± 0.5	36 ± 13.5	34.7 ± 8.5	29.3 ± 2.6	26.6 ± 2
	BCQ		CQL		MOPO	
Random	9.2 ± 1.1	9.7 ± 0.2	10.3 ± 0.4	10.4 ± 0.4	2.8 ± 3	2.1 ± 1.7
Medium	28.2 ± 8.8	38.4 ± 1.8	43.3 ± 10	59.3 ± 12.2	7.6 ± 7.2	10.7 ± 5.1
Medium-R	14.2 ± 1.3	32.8 ± 0.9	2.2 ± 0.3	3.7 ± 1.4	4.9 ± 3.8	8.4 ± 3.5
Medium-E	83.4 ± 23.7	84.2 ± 9.8	87.8 ± 16.9	99.7 ± 16.4	4.6 ± 2.9	5.8 ± 2.3
A.3.10 The sensitivity of the coefficient of the reward modification
In Table 18, We check the sensitivity of hyper-parameter η, i.e., the coefficient of the reward modi-
fication in r(s, a) - η∆r(s, a, s0).
24
Published as a conference paper at ICLR 2022
Table 18: We show the normalized scores for the Hopper tasks with body mass shift, by varying
η ∈ {0, 0.05, 0.1, 0.2, 0.5} over BEAR, BRAC-p, AWR, BCQ, CQL, and MOPO.
Body Mass Shift	Hyper-parameter η				
	0	0.05	0.1	0.2	0.5
			BEAR		
Random	4.6 ± 3.4	7.7 ± 0.9	8.4 ± 1.2	7±1.2	4.2 ± 1.1
Hopper	Medium	0.9 ± 0.3	1.1 ± 0.6	1.6±1	0.9 ± 0.2	0.7 ± 0.1
MediUm-R	18.2±5	28.5 ± 5.9	34.1 ± 5.8	29.1 ± 4.4	18.1 ±4.3
Medium-E	0.6 ± 0	0.8 ± 0.1	1.2 ± 0.5	1.2 ± 0.6	0.7 ± 0.1
			BRAC-p		
Random	9.6 ± 3.3	11.2 ± 0.8	11 ± 0.6	10.6 ± 2.4	5.3 ± 1.2
Hopper	Medium	29.2 ± 2.1	26.5 ± 1.8	32.9 ± 7.5	16.1 ± 0.9	16.7 ± 1.7
Medium-R	20.1 ± 4.8	17.8 ± 3.2	30.8 ± 4.9	13.9 ± 1.7	10.4 ± 2.4
Medium-E	32.3 ± 7.8	40.4 ± 4.4	34.7 ± 8.5	29.4 ± 6.5	25.2 ± 4.1
			AWR		
Random	3.4 ± 0.7	4.1 ± 1	4.5 ± 0.9	3.4 ± 0.7	2.5 ± 0.1
Hopper	Medium	20.8 ± 6.3	31.8 ± 2.9	28.9 ± 5.5	26.6 ± 3.2	17.4 ± 1.5
Medium-R	4.1 ± 1.7	3 ± 0.5	4.2 ± 3.5	2.6 ± 0.6	4.3 ± 1.3
Medium-E	26.8 ± 0.4	27 ± 0	26.6±2	17.8 ± 5.6	24.2 ± 3.9
			BCQ		
Random	8.3 ± 0.3	9.6 ± 0.3	9.7 ± 0.2	7.4 ± 0.1	7.6 ± 0.3
Hopper	Medium	25.7 ± 5.5	24.1 ± 0.8	38.4 ± 1.8	27.1 ± 1.7	26.7 ± 0.8
Medium-R	28.7 ± 1.9	29.5 ± 3	32.8 ± 0.9	25.9 ± 6	21 ± 2.2
Medium-E	75.4 ± 7.8	70.4 ± 5.4	84.2 ± 9.8	67.9 ± 8	61.9 ± 4.3
			CQL		
Random	10.2 ± 0.3	10±0	10.4 ± 0.4	10±0	10±0
Hopper	Medium	44.9 ± 2.7	59.8 ± 6	59.3 ± 12.2	44.2±1	37.1 ± 2.6
Medium-R	1.4 ± 0.3	2.1 ± 0.2	3.7 ± 1.4	3.9 ± 1.7	3.4 ± 1
Medium-E	53.6 ± 21.2	65.3 ± 15.4	99.7 ± 16.4	60.5 ± 16	75.9 ± 30
			MOPO		
Random	2±2.1	1.8 ±0	2.1 ± 1.7	1.2 ± 0.4	0.8 ± 0
Hopper	Medium	5 ± 5.3	6.5 ± 1	10.7 ± 5.1	5.3 ± 1.6	2.8 ± 0.7
Medium-R	5.5 ± 4.6	7.5 ± 0.8	8.4 ± 3.5	5.7 ± 3.5	1.9 ± 0.6
Medium-E	4.8 ± 2.9	8.1 ± 1	5.8 ± 2.3	4.7 ± 0.7	2.1 ± 0.2
25
Published as a conference paper at ICLR 2022
A.4 Environments and dataset
Figure 12: Illustration of the suite of tasks considered in this work: (from left to right) Hopper,
Walker2d, Halfcheetah, simulated and real-world quadruped robots. These tasks require the RL
agent to learn locomotion gaits for the illustrated characters.
In this work, the tasks include Hopper, Walker2d, HalfCheetah, simulated (see the dynamics param-
eters in Zhang et al.) and real-world quadruped robot, which are illustrated in Figure 12.
Table 19: Dynamics shift for Hopper, Walker2d, and Halfcheetah tasks. For the body mass shift,
we change the mass of the body in the source MDP M0. For the joint noise shift, we add a noise
(randomly sampling in [-0.05, +0.05]) to the actions when we collect the source offline data, i.e.,
D0 := {(s, a,r, s0)}〜dD(s)πb(a∣s)r(s, a)T0(s0∣s, a + noise).
Hopper
Walker2d
HalfCheetah
Body Mass Shfit	Joint Noise Shift	Body Mass Shfit	Joint Noise Shift	Body Mass Shfit	Joint Noise Shift
Source	mass[-1]=2.5	action[-1]+noise	mass[-1]=1.47	action[-1]+noise	mass[4]=0.5	action[-1]+noise
Target	mass[-1]=5.0	action[-1]+0	mass[-1]=2.94	action[-1]+0	mass[4]=1.0	action[-1]+0
In the Hopper, Walker2d and HalfCheetah dynamics adaptation setting, we set the D4RL (Fu et al.,
2020) dataset as our target domain. For the source dynamics, we change the body mass (body mass
shift) or add noises to joints (joint noise shift) of the agents (see Table 19 for the details) and then
collect the source offline dataset in the changed environment. Following Fu et al. (2020), on the
changed source environment, we collect the 1) ”Random” offline data, generated by unrolling a ran-
domly initialized policy, 2) ”Medium” offline data, generated by a trained policy with the “medium”
level of performance in the source environment, 3) ”Medium-Replay” (Medium-R) offline data, con-
sisting of recording all samples in the replay buffer observed during training until the policy reaches
the “medium” level of performance, 4) ”Medium-Expert” (Medium-E) offline data, mixing equal
amounts of expert demonstrations and ”medium” data in the source environment.
In the sim2real setting (for the quadruped robot), we use the
A1 dog from Unitree (Wang, 2020). We collect the target of-
fline data using five target behavior policies in the real-world
with changing terrains, as shown in Figure 13, and collect
the ”Medium”, ”Medium-Replay” (Medium-R), ”Medium-
Expert” (Medium-E), ”Medium-Replay-Expert” (Medium-R-
E) source offline data in the simulator, where ”Medium-
Replay-Expert” denotes mixing equal amounts of ”Medium-
Replay” data and expert demonstrations in the simulator. In
Section A.5, we provide the details of how to obtain the tar-
get and source behavior policy, so as to collect our target and
source offline data.
Figure 13: Real-world terrains
(for collecting the target offline
data).
We list our tasks properties in Table 20 and provide our collected dataset in supplementary material.
In implementation, we set η = 0.1 for all simulated tasks and set η = 0.01 for the sim2real task. In
Table 18, we also report the sensitivity of DARA on the hyper-parameters η.
A.5 Training the (target and source) behavior policy for the quadruped robot
To obtain a behavior policy that can be deployed in simulator (for collecting the source offline data)
or real-world (for collecting the target offline data), we introduce the prior knowledge (Iscen et al.,
2018) and domain randomization (Tobin et al., 2017; Peng et al., 2018).
26
Published as a conference paper at ICLR 2022
Table 20: Statistics for each task in our adaptation setting.				
Environment	Dynamics Shift	Task Name	Target (1T)	Source (10S)
		Random	105 (D4RL)	106
	Body Mass Shfit	Medium	105 (D4RL)	106
		Medium-Replay	20092 (D4RL)	106
Hopper		Medium-Expert	2* 105 (D4RL)	2*106
		Random	105 (D4RL)	106
		Medium	105 (D4RL)	106
	Joint Noise Shift	Medium-Replay	20092 (D4RL)	106
		Medium-Expert	2* 105 (D4RL)	2*106
		Random	105 (D4RL)	106
	Body Mass Shfit	Medium	105 (D4RL)	106
		Medium-Replay	10093 (D4RL)	106
Walker2d		Medium-Expert	2* 105 (D4RL)	2*106
		Random	105 (D4RL)	106
		Medium	105 (D4RL)	106
	Joint Noise Shift	Medium-Replay	10093 (D4RL)	106
		Medium-Expert	2* 105 (D4RL)	2*106
		Random	105 (D4RL)	106
	Body Mass Shfit	Medium	105 (D4RL)	106
		Medium-Replay	10100 (D4RL)	106
HalfCheetah		Medium-Expert	2* 105 (D4RL)	2*106
		Random	105 (D4RL)	106
		Medium	105 (D4RL)	106
	Joint Noise Shift	Medium-Replay	10100 (D4RL)	106
		Medium-Expert	2* 105 (D4RL)	2*106
		Medium	3 * 104 (real-world)	106 (simulator)
A1 robot (Unitree)	Sim2Real	Medium-Replay Medium-Expert	3 * 104 (real-world) 3 * 104 (real-world)	106 (simulator) 2 * 106 (simulator)
		Medium-Replay-Expert	3 * 104 (real-world)	2 * 106 (simulator)
Prior Knowledge: To reduce the impact of the foot at the moment of touching the ground during
the robot locomotion, we designed a compound cycloid trajectory (Sakakibara et al., 1990) as prior
knowledge. In our implementation for the foot trajectory, four aspects are mainly considered: 1) The
robot walks stably without obvious shaking; 2) The joint impact of the robot during the locomotion
is small; 3) The joint speed and acceleration of the robot during the locomotion are continuous and
smooth; 4) The feet of the robot will not slide when they are in contact with the ground. Similar
to Lee et al. (2020), we define a periodic phase variable φi ∈ [0.0, 0.6), i = 1, 2, 3, 4 for each leg,
which represents swing phase if φi ∈ [0.0, 0.3) and contact phase if φi ∈ [0.3, 0.6). At every time
step t, φi = (t * fo + φo [i] + φoffset[i])(mod 2Tm) where Tm = 0.3, and fo = 1.1 is the base
frequency, and φ0 = [0, 0.3, 0.3, 0] is the initial phase. φoffset is part of the output of the controller.
The trajectory of the swing leg is:
Xi = S	Tm	-	21∏ sin	(2∏t)	+ So, i = 1,2
Xi = STtr	—	2∏ sin	(Tt)	— S + So,i =	3,4
y = Y0
Z = H [sgn (Tm - t) (2fE ⑴- 1) + 1] + Z0
where
and
fE (t)
T ≤ t< Tm
^mΓ ≤ t < Tm
27
Published as a conference paper at ICLR 2022
The trajectory of the standing leg is:
[Xi= S (⅛t + 2∏ sin (T U + S0,i = 1, 2
∖ Xi=S(2Tmt+2∏sin(舒力 — S+S0,i = 3,4 .
I y = Y0
Iz = Zo
where S = 0.14m, H = 0.18m are the maximum foot length and height. S0 =
[0.17, 0.17, -0.2, -0.2], Y0 = [-0.13, 0.13, -0.13, 0.13], Z0 = [-0.32, -0.32, -0.32, -0.32] are
the default target foot position in body frame.
Domain Randomization: To encourage the policy to be robust to variations in the dynamics, we
incorporate the domain randomization. In Table 21, we provide the dynamics parameters and their
respective range of values.
Table 21: Dynamic parameters and their respective range of values utilized during training.
Parameter	Range
Mass	[0.95,1.1] × default value
Inertia	[0.80, 1.2] ×default value
Motor Strength	[0.80, 1.2] ×default value
Latency	[0, 0.04] s
Lateral Friction	[0.5, 1.25] Ns/m
Joint Friction	[0, 0.05] Nm
State Space, Action Space and Reward Function: The action is a 16-dimensional vector consist-
ing of leg phase and target foot position residuals in the body frame. The design of state space and
reward function mainly follows the prior work Lee et al. (2020). In Table 22, we provide the state
representation.
Table 22: State representation for the behavior policy.
Data	Dimension
Desired direction((BBVd)Xy)	2
Euler angle(rpy)	3
Base angular velocity IBB ω	3
Base linear velocity IBBv	3
Joint Position/velocity (θi, θ)	24
FTG phases(sin (φi) , cos (φi))	8
FTG frequencies(fi)	4
Base frequency(fo)	1
Joint Position error history	24
Joint velocity history	24
Foot target history (rf,d)t-1,t-2	24
The reward function is defined as
0.1rlv + 0.05ry + 0.05rrp + 0.005rb + 0.02rbc + 0.025rs + 2 ∙ 10-5rτ.
The individual terms are defined as follows.
1)	Linear velocity reward rlv :
eXp(-30|vpr - 0.2|) vpr < 0.2
rlv :=	1	vpr ≥ 0.2 ,
28
Published as a conference paper at ICLR 2022
where Vpr = Vxy ∙ Vxy is the base linear velocity projected onto the command direction.
2)	Yaw angle reward ry :
ry := exp(-(y - y)2),	(17)
where y and y is the yaw and desired yaw angle.
3)	Roll and pitch reward rrp :
rrp ：= exp(-1.5 X(φ - [0, arccos( <Pxz, p，：I) >) - ∏∕2])2),	(18)
where φ are the roll and pitch angle. Pxz = P1 - P4 or Pxz = P2 - P3,Pi, i ∈ [1, 4] are the foot
position in world frame. The advantage of designing the target pitch angle in this way is to ensure
that the body of the robot is parallel to the supporting surface of the stand legs, thereby ensuring that
the robot can smoothly over challenge terrain, such as upward stairs.
4)	Base motion reward rb :
rb := exp(-1.5	(Vxy - vpr * Vxy)2) + exp(-1.5>2(ωxy)2),	(19)
where ωxy are the roll and pitch rates.
5)	Body collision reward rbc :
rbc := -|Ibody ∕If oot |,	(20)
where Ibody and Ifoot are the contact numbers of robot’s body parts and foot with the terrain, re-
spectively.
6)	Target smooth reward rs :
rs := -||fd,t - 2fd,t-1 + fd,t-2 ||,	(21)
where fd,i(i = t, t - 1, t - 2) are the target foot positions in the time-step t, t - 1 and t - 2.
7)	Torqure reward rτ :
TT := - X |Ti|,	(22)
i
where τi is the joint torques.
Training Details: Both the behavior policy and value networks are Multilayer Perceptron (MLP)
with 3 hidden layers, which have 256, 128 and 64 nodes. The activation function is the Tanh func-
tion, and the optimizer is Adam. With the above prior knowledge, domain randomization and reward
function, we train our behavior policy with SAC (Haarnoja et al., 2018) in PyBullet (Coumans &
Bai, 2016-2021).
A.6 Additional Results
Here we provide additional results regarding the error bars (Tables 23 and 24).
29
3。
Table 23: Normalized scores for the D4RL tasks (with body mass shift). We take the baseline results (for 10T) of MOPO from their original papers and that of the
other model-free methods (BEAR, BRAC-p, AWR, BCQ and CQL) from the D4RL paper (Fu et al., 2020).
Body Mass Shift IOT
1T+1OS 1T+1OS
w/o Aug. (DARA)
IT
1T+1OS 1T+1OS	1T+1OS 1T+1OS
w/o Aug. (DARA) 1ot 1T w/o Aug. (DARA)
BEAR	BRAC-p	AWR
JOddOH
Random	11.4	1 ±0.5	4.6 ± 3.4	8.4 ± 1.2	11	10.9 ± 0.1	9.6 ± 3.3	11 ±0.6	10.2	10.3 ± 0.3	3.4 ± 0.7	4.5 ± 0.9
Medium	52.1	0.8 ±0	0.9 ± 0.3	1.6 ± 1	32.7	29 ± 6.2	29.2 ±2.1	32.9 ± 7.5	35.9	30.9 ± 0.4	20.8 ± 6.3	28.9 ± 5.5
Medium-R	33.7	1.3 ± 1.5	18.2 ± 5	34.1 ± 5.8	0.6	5.4 ± 3.3	20.1 ±4.8	30.8 ± 4.9	28.4	8.8 ± 4.9	4.1 ± 1.7	4.2 ± 3.5
Medium-E	963	0.8 ±0.1	0.6 ±0	1.2 ±0.5	1.9	34.5 ± 14.7	323 ± 7.8	34.7 ± 8.5	27.1	27 ± 1.3	26.8 ± 0.4	26.6 ± 2
BCQ	CQL	MOPO
JOddOH PZJəaɪŋAV
Random Medium Medium-R Medium-E	10.6 54.5 33.1 110.9	10.6 ±0.1 37.1 ± 6.3 9.3 ±4.4 58 ± 16.2	8.3 ± 0.3 25.7 ± 5.5 28.7 ± 1.9 75.4 ± 7.8	9.7 ± 0.2 38.4 ± 1.8 32.8 ± 0.9 84.2 ± 9.8	10.8 58 48.6 98.7	10.6 ±0.1 43 ± 9.2 9.6 ± 5.2 59.7 ± 34.5	10.2 ± 0.3 44.9 ± 2.7 1.4 ±0.3 53.6 ±21.2	10.4 ± 0.4 59.3 ± 12.2 3.7 ± 1.4 99.7 ± 16.4	11.7 28 67.5 23.7	4.8 ± 2.4 4.1 ±2 1 ±0.6 1.6 ±0.6	2 ±2.1 5± 5.3 5.5 ± 4.6 4.8 ± 2.9	2.1 ± 1.7 10.7 ± 5.1 8.4 ±3.5 5.8 ± 2.3
			BEAR				BRAC-p				AWR	
Random	7.3	1.5 ±0.9	3.1 ±0.9	3.2 ± 0.4	-0.2	0±0.2	1.3 ±0.7	3.2 ± 2.5	1.5	1.3 ± 0.4	2± 1	2.4 ± 0.8
Medium	59.1	-0.5 ± 0.3	0.6 ± 0.5	0.3 ± 0.7	77.5	6.4 ± 9.9	70 ± 10.1	78 ±3.1	17.4	14.8 ± 2.8	17.1 ± 0.2	17.2 ±0.1
Medium-R	19.2	0.7 ± 0.6	6.5 ± 5.1	7.3 ± 1.3	-0.3	8.5 ± 2.2	9.9 ±2	18.6 ±6.5	15.5	7.4 ±2.1	1.6 ±0.4	1.5 ±0.3
Medium-E	40.1	-0.1 ±0.1	1.5 ±2.5	23 ± 2.2	76.9	20.6 ± 16.8	64.1 ± 10.8	77.5 ±3.1	53.8	35.5 ± 10.4	52.5 ± 1.2	533 ± 03
PUbliShed as a ConferenCe PaPersICLR 2022
BCQ
CQL
MOPO
P51m
Random	4.9	1.8 ±0.9	4.5 ± 0.5	4.8 ± 0.3	7	1.7 ± 1.3	3.2 ± 1.4	3.4 ± 1.9	13.6	-0.2 ± 0.2	-0.1 ±0.1	-0.1 ±0.2
Medium	53.1	32.8 ± 8.2	50.9 ± 4.3	52.3 ± 1.4	79.2	42.9 ± 24.2	80 ± 1.2	81.7 ±3.1	17.8	7 ±3.6	5.7 ± 4.7	11 ±4.3
Medium-R	15	6.9 ± 0.6	14.9 ± 0.2	15.1 ±0.2	26.7	4.6 ± 3.9	0.8 ± 0.5	2± 1.5	39	5.1 ± 5.7	3.1 ±2.4	14.2 ± 4.5
Medium-E	57.5	32.5 ±9.1	55.2 ± 3.8	57.2 ± 0.2	111	49.5 ± 26.7	63.5 ± 22.5	933 ± 8.8	44.6	53 ± 3.9	5.5 ± 3.5	17.2 ± 8.7
3
Table 24: Normalized scores for the D4RL tasks (with joint noise shift). We take the baseline results (for 10T) of MOPO from their original papers and that of the
other model-free methods (BEAR, BRAC-p, AWR, BCQ and CQL) from the D4RL paper (Fu et al., 2020).
Joint Noise Shift IOT
1T+1OS	1T+1OS
w/o Aug.	(DARA)
1T+1OS	1T+1OS
w/o Aug.	(DARA)
1T+1OS	1T+1OS
w/o Aug.	(DARA)
JOddOH JOddOH
BEAR	BRAC-P	AWR
Random	11.4	0.6 ± O	7.4 ± 0.5	4.2 ± 3.6	11	10.8 ± 0.2	10 ±0.8	10.8 ± O	10.2	10.1 ±0	3.6 ±0	4 ±0.4
Medium	52.1	0.8 ±0	2± 1	2 ±0.1	32.7	26.6 ± 4.8	27.6 ± 2.8	37.6 ± 7	35.9	30.3 ±0.1	38.8 ±3.9	41.3 ±5
Medium-R	33.7	2.7 ± 1.6	3.6 ± 0.4	9.9 ± 6.3	0.6	13.4 ± 6.4	89.9 ±7.8	101.4 ±0.2	28.4	12.4 ± 6	6.7 ± 4.2	7.2 ± OJ
Medium-E	963	0.8 ± 0.2	0.8 ± O	1.4 ±0.6	1.9	19.8 ± 14	57.6 ± 23.4	87.8 ± 133	27.1	25.5 ± 1.2	27 ±0	27 ±0.1
PUbHShed as a COInferenCe PHPeCt ICL一
BCQ
CQL
MOPO
Random Medium	10.6 54.5	10.5 ±0.1 45.8 ± 2.2	7±0 49 ± 1.7	9.6 ±0 54.4 ±0.1	10.8 58	10.4 ±0.1 46.2 ± 11.9	10.4 ± 0.4 58 ±0	10.8 ± 0 58 ±0	11.7 28	1.5 ±0.8 2.7 ±2.1	1.3 ± 0.5 9.2 ± 5.4	2.9 ± 1.5 17.3 ± 3.	4
Medium-R	33.1	13 ± 5	23.8 ± 3.2	32 ± 0.9	48.6	13.6 ± 6.4	2.6 ± 0.3	3.6 ± 0.6	67.5	0.8 ±0.1	2.3 ± 1.7	6.4 ± 0.8	
Medium-E	110.9	44.6 ± 18.6	96 ± 0.5	109 ± 0.2	98.7	50.7 ± 26.9	73.4 ± 1.5	108.9 ± 0.7	23.7	1 ±0.2	6.1 ± 1.4	7.5 ± 0.6	

BEAR
BRAC-P
AWR
Random Medium	7.3 59.1	2.2 ±0.1 -0.4 ±0.1	0.6 ±0.1 0.6 ±0	2.6 ± 0.3 0.1 ± 0.3	-0.2 77.5	2.8 ± 2.8 28.8 ± 28.4	3.3 ± 2.9 55.2 ± 15.8	8.8 ± 8 72.9 ±9.1	1.5 17.4	0.9 ±0.1 12.2 ± 0.3	1.5 ± 0.6 17.2 ± 0.2	1.5 ±0.2 17.2 ± 0.	2
Medium-R	19.2	0.4 ± 0.2	4 ±0.2	10.4 ± 2.4	-0.3	6.3 ± 1.2	32.1 ± 11.9	34.8 ± 10.5	15.5	6± 1	1.4 ±0	2.1 ± 0.9	
Medium-E	40.1	-0.2 ± 0.2	0.8 ± 0.4	0.6 ± 0.6	76.9	21.8 ± 18.4	62.3 ± 13.1	74.3 ± 1.8	53.8	40.4 ± 12.6	53 ±0.1	53.6 ± 0	
BCQ
CQL
MOPO
P51m
Random	4.9	3.7 ± 1.8	3.4 ± 0.4	5.2 ± 0.3	7	0.5 ± 1	2.7 ± 0.2	6.4 ± 0.6	13.6	-0.3 ±0.1	-0.2 ± 0	-0.2 ± O/	I
Medium	53.1	43 ± 8.3	44.9 ± 3.3	52.7 ± 0.3	79.2	43.9 ±21.7	73.2 ±0.8	81.2 ± 1.1	17.8	5.8 ± 5.9	7.8 ± 6.2	12.2 ± 5	
Medium-R	15	5.7 ± 0.5	9.8 ± 5.2	14.6 ± 0.4	26.7	1.8 ± 1.2	1.4 ± 0.4	1.8 ± 0.4	39	0.8 ± 0.7	9.3 ± 5.2	16.4 ± 4.	9
Medium-E	57.5	44.5 ±3.6	40.6 ± 16.4	57.2 ± 0.2	111	46.8 ± 40	109.9 ± 4.5	116.5 ±9.1	44.6	2.9 ±3.1	15.2 ± 12.8	26.3 ± n	；.4