Published as a conference paper at ICLR 2022
Augmented Sliced Wasserstein Distances
arXiv:2006.08812v7 [cs.LG] 17 Mar 2022
Xiongjie Chen1, Yongxin Yang2,3, and Yunpeng Li1
1University of Surrey, 2University of Edinburgh, 3Huawei Noah’s Ark Lab
1 {xiongjie.chen, yunpeng.li}@surrey.ac.uk, 2yongxin.yang@ed.ac.uk
Ab stract
While theoretically appealing, the application of the Wasserstein distance to
large-scale machine learning problems has been hampered by its prohibitive
computational cost. The sliced Wasserstein distance and its variants improve the
computational efficiency through the random projection, yet they suffer from low
accuracy if the number of projections is not sufficiently large, because the majority
of projections result in trivially small values. In this work, we propose a new family
of distance metrics, called augmented sliced Wasserstein distances (ASWDs),
constructed by first mapping samples to higher-dimensional hypersurfaces parame-
terized by neural networks. Itis derived from akey observation that (random) linear
projections of samples residing on these hypersurfaces would translate to much
more flexible nonlinear projections in the original sample space, so they can capture
complex structures of the data distribution. We show that the hypersurfaces can
be optimized by gradient ascent efficiently. We provide the condition under which
the ASWD is a valid metric and show that this can be obtained by an injective neural
network architecture. Numerical results demonstrate that the ASWD significantly
outperforms other Wasserstein variants for both synthetic and real-world problems.
1 Introduction
Comparing samples from two probability distributions is a fundamental problem in statistics and
machine learning. The optimal transport (OT) theory (Villani, 2008) provides a powerful and flexible
theoretical tool to compare degenerative distributions by accounting for the metric in the underlying
spaces. The Wasserstein distance, which arises from the optimal transport theory, has become an
increasingly popular choice in various machine learning domains ranging from generative models
to transfer learning (Gulrajani et al., 2017; Arjovsky et al., 2017; Kolouri et al., 2019b; Cuturi and
Doucet, 2014; Courty et al., 2016).
Despite its favorable properties, such as robustness to disjoint supports and numerical stability (Arjovsky
et al., 2017), the Wasserstein distance suffers from high computational complexity especially when the
sample size is large. Besides, the Wasserstein distance itself is the result of an optimization problem
—itis non-trivial to be integrated into an end-to-end training pipeline of deep neural networks, unless
one can make the solver for the optimization problem differentiable. Recent advances in computational
optimal transport methods focus on alternative OT-based metrics that are computationally efficient and
solvable via a differentiable optimizer (Peyre and Cuturi, 2019). Entropy regularization is introduced
in the Sinkhorn distance (Cuturi, 2013) and its variants (Altschuler et al., 2017; Dessein et al., 2018)
to smooth the optimal transport problem; as a result, iterative matrix scaling algorithms can be applied
to provide significantly faster solutions with improved sample complexity (Genevay et al., 2019).
An alternative approach is to approximate the Wasserstein distance through slicing, i.e. linearly
projecting, the distributions to be compared. The sliced Wasserstein distance (SWD) (Bonneel et al.,
2015) is defined as the expected value of Wasserstein distances between one-dimensional random
projections of high-dimensional distributions. The SWD shares similar theoretical properties with the
Wasserstein distance (Bonnotte, 2013) and is computationally efficient since the Wasserstein distance
in one-dimensional space has a closed-form solution based on sorting. (Deshpande et al., 2019) extends
the sliced Wasserstein distance to the max-sliced Wasserstein distance (Max-SWD), by finding a
single projection direction with the maximal distance between projected samples. The subspace robust
Wasserstein distance extends the idea of slicing to projecting distributions on linear subspaces (Paty and
1
Published as a conference paper at ICLR 2022
▼ Projected sample from μ	∙ Sample from μ
▼ Projected sample from v	∙ Sample from v
Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two
2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD between
two 100-dimensional Gaussians. Figure 1(a) shows that the injective neural network embedded in the
ASWD learns data patterns (in the X-Y plane) and produces well-separate projected values (Z -axis)
between distributions in a random projection direction. The high projection efficiency of the ASWD
is evident in Figure 1(c), as almost all random projection directions in a 100-dimensional space lead
to significant distances between 1-dimensional projections. In contrast, random linear mappings in
the SWD often produce closer 1-d projections (Z -axis) (Figure 1(b)); as a result, a large percentage of
random projection directions in the 100-d space result in trivially small distances (Figure 1(d)), leading
to a low projection efficiency in high-dimensional spaces.
Cuturi, 2019). However, the linear nature of these projections usually leads to low projection efficiency
of the resulted metrics in high-dimensional spaces (Deshpande et al., 2019; Kolouri et al., 2019a).
Different variants of the SWD have been proposed to improve the projection efficiency of the SWD,
either by introducing nonlinear projections or by optimizing the distribution of random projections.
Specifically, (Kolouri et al., 2019a) extends the connection between the sliced Wasserstein distance and
the Radon transform (Radon, 1917) to introduce generalized sliced Wasserstein distances (GSWDs)
by utilizing generalized Radon transforms (GRTs), which are defined by nonlinear defining functions
and lead to nonlinear projections. A variant named the GSWD-NN was proposed in (Kolouri et al.,
2019a) to generate nonlinear projections directly with neural network outputs, but it does not fit into the
theoretical framework of the GSWD and does not guarantee a valid metric. In contrast, the distributional
sliced Wasserstein distance (DSWD) and its nonlinear version, the distributional generalized sliced
Wasserstein distance (DGSWD), improve their projection efficiency by finding a distribution of
projections that maximizes the expected distances over these projections. The GSWD and the DGSWD
exhibit higher projection efficiency than the SWD in the experiment evaluation, yet they require the
specification of the particular form of defining functions from a limited class of candidates. However,
the selection of defining functions is usually a task-dependent problem and requires domain knowledge,
and the impact on performance from different defining functions is still unclear.
In this paper, we present the augmented sliced Wasserstein distance (ASWD), a distance metric
constructed by first mapping samples to hypersurfaces in an augmented space, which enables flexible
nonlinear slicing of data distributions for improved projection efficiency (See Figure 1). Our main
contributions include: (i) We exploit the capacity of nonlinear projections employed in the ASWD
by constructing injective mapping with arbitrary neural networks; (ii) We prove that the ASWD is a
valid distance metric; (iii) We provide a mechanism in which the hypersurface where high-dimensional
distributions are projected onto can be optimized and show that the optimization of hypersurfaces
can help improve the projection efficiency of slice-based Wasserstein distances. Hence, the ASWD
is data-adaptive, i.e. the hypersurfaces can be learned from data. This implies one does not need
to manually design a function from the limited class of candidates; (iv) We demonstrate superior
performance of the ASWD in numerical experiments for both synthetic and real-world datasets.
The remainder of the paper is organized as follows. Section 2 reviews the necessary background.
We present the proposed method and its numerical implementation in Section 3. Related work are
discussed in Section 4. Numerical experiment results are presented and discussed in Section 5. We
conclude the paper in Section 6.
2
Published as a conference paper at ICLR 2022
2 Background
In this section, we provide a brief review of concepts related to the proposed work, including the
Wasserstein distance, (generalized) Radon transform and (generalized) sliced Wasserstein distances.
Wasserstein distance: Let Pk(Ω) be a set of Borel probability measures with finite k-th moment
on a Polish metric space (Ω,d) (Villani, 2008). Given two probability measures μ, V ∈ Pk (Ω), the
Wasserstein distance of order k ∈ [1,+∞) between μ and V is defined as:
1
” ,ν)L,S
^Wk (μ,ν ) =
(1)
where d(∙, ∙)k is the cost function, Γ(μ,ν) represents the set of all transportation plans Y, i.e. joint
distributions whose marginals are μ and ν, respectively.
While the Wasserstein distance is generally intractable for high-dimensional distributions, there are
several favorable cases where the optimal transport problem can be efficiently solved. If μ and V
are continuous one-dimensional measures defined on a linear space equipped with the Lk norm, the
Wasserstein distance between μ and V has a closed-form solution (Peyre and Cuturi, 2019):
Wk(μ,ν)= IL IFJI(Z)-F-I(Z)Ikdz
(2)
where F-1 and F-1 are inverse cumulative distribution functions (CDFs) of μ and V, respectively.
In practice, Wasserstein distances Wk (μ, V) between one-dimensional empirical distributions
μ=得 PN=I δχn and V= -ɪ PN=I δyn can be computed by sorting one-dimensional samples from
empirical distributions, and evaluating the distances between sorted samples (Kolouri et al., 2019b):
1 1 ɪ	∖1
Wk (μ,p)= ( N Elxlχ[n]-yiy ,F)
n=1
(3)
where N is the number of samples, Ix [n] and Iy [n] are the indices of sorted samples satisfying
xIx[n] ≤ xIx [n+1] andyIy[n] ≤ yIy [n+1], respectively.
Radon transform and generalized Radon transform: The Radon transform (Radon, 1917) maps a
function f (∙) ∈ LI(Rd) to the space of functions defined over spaces ofhyperplanes in Rd. The Radon
transform of f (∙) is defined by line integrals of f (∙) along all possible hyperplanes in Rd:
Rf (t,θ) =	f (x)δ(t-hx,θi)dx,
(4)
where t ∈ R and θ ∈ Sd-1 represent the parameters of hyperplanes {x ∈ Rd ∣ (χ,θ> = t}, δ(∙) is the
one-dimensional Dirac delta function, and(∙,∙，refers to the Euclidean inner product.
By replacing the inner product hx,θi in Equation (4) with β(x,θ), a specific family of functions named
as defining function in (Kolouri et al., 2019a), the generalized Radon transform (GRT) (Beylkin, 1984)
is defined as integrals of f (∙) along hypersurfaces defined by {x ∈ Rd ∣ β(χ,θ)= t}:
Gf(t,θ)=
Rd
f(x)δ(t -β(x,θ))dx,
(5)
where t ∈ R, θ ∈ Ωθ and Ωθ is a compact set of all feasible θ, e.g. Ωθ = SdT for β(χ,θ) =(x,。). In
particular, a function β(x,θ) defined on X × (Rd\{0}) with X ⊆ Rd is called a defining function of
GRTs if it satisfies conditions H.1 - H.4 given in (Kolouri et al., 2019a).
For probability measures μ ∈ Pk (Rd), the Radon transform and the GRT can be employed as
push-forward operators, and the generated push-forward measures Rμ = R#m, Gμ = G#m are defined
as follows (Bonneel et al., 2015):
R"",")=//(t-(χ,θ))dμ,
Gμ(t,θ) = //(t-β(x,θ))dμ.
(6)
(7)
3
Published as a conference paper at ICLR 2022
Notably, the Radon transform is a linear bijection (Helgason, 1980), and the sufficient conditions for
GRTs to be bijective are provided in (Homan and Zhou, 2017).
Sliced Wasserstein distance and generalized sliced Wasserstein distance: By applying the Radon
transform to μ and V to obtain multiple projections, the sliced Wasserstein distance (SWD) decomposes
the high-dimensional Wasserstein distance into multiple one-dimensional Wasserstein distances which
can be efficiently evaluated (Bonneel et al., 2015). The k-SWD between μ and V is defined by:
SWDk(μ,ν)= (Z	Wk (Rμ (∙,θ),Rν (∙,θ))dθ[k,	⑻
Sd-1
where the Radon transform R defined by Equation (6) is adopted as the measure push-forward operator.
The GSWD generalizes the idea of SWD by slicing distributions with hypersurfaces rather than
hyperplanes (Kolouri et al., 2019a). The GSWD is defined as:
1
GSWDk(μ,ν)=( W Wk(Gμ(∙,θ),Gν(∙,θ))dθΓ,	(9)
∖JΩθ
where the GRT G defined by Equation (7) is used as the measure push-forward operator. From
Equation (3), with L random projections and N samples, the SWD and GSWD between μ and V can
be approximated by:
1LN
SWDk(μ,ν) ≈ (NLE ∑Slhxιχ [n],θι i-hyιy [n],θιilk
l=1 n=1
1
k
1LN
GSWDk(μ,ν)≈( NLEE∣β(xιχ[n],θι)-β(yιy[n],θι)∣k
l=1 n=1
(10)
(11)
where Ixl and Iyl are sequences consisting of the indices of sorted samples which satisfy hxIxl [n],θli ≤
hxIxl [n+1],	θli,	hyIyl [n],	θli	≤	hyIyl [n+1], θli in the SWD, and	β(xIxl [n],	θl)	≤	β(xIxl [n+1],	θl),
β(yIl [n], θl) ≤ β(yIl [n+1], θl) in the GSWD. The approximation error in estimating SWDs using
Equation (10) is derived in (Nadjahi et al., 2020). It is proved in (Bonnotte, 2013) that the SWD is a valid
distance metric. The GSWD is a valid metric except for its neural network variant (Kolouri et al., 2019a).
3 Augmented sliced Wasserstein distances
In this section, we propose a new distance metric called the augmented sliced Wasserstein distance
(ASWD), which embeds flexible nonlinear projections in its construction. We also provide an
implementation recipe for the ASWD.
3.1 Spatial Radon transform and augmented sliced Wasserstein distance
In the definitions of the SWD and GSWD, the Radon transform (Radon, 1917) and the generalized
Radon transform (GRT) (Beylkin, 1984) are used as the push-forward operator for projecting
distributions to a one-dimensional space. However, it is not straightforward to design defining
functions β(x,θ) for the GRT, since one needs to first check if β(x,θ) satisfies the conditions to be
a defining function (Kolouri et al., 2019a; Beylkin, 1984), and then whether the corresponding GRT
is bijective or not (Homan and Zhou, 2017). In practice, the assumption of the transform can be relaxed,
as Theorem 1 shows that as long as the transform is injective, the corresponding ASWD metric is a
valid distance metric.
To help us define the augmented sliced Wasserstein distance, we first introduce the spatial Radon
transform which includes the Radon transform and the polynomial GRT as special cases (See Remark 5).
Definition 1. Given a measurable injective mapping g(∙) :Rd → Rdθ and a function f(∙) ∈ L1 (Rd),
the spatial Radon transform of f (∙) is defined as
Hf(t£；g)=[
Rd
f (x)δ(t-hg(x),θ)idx,
(12)
where t ∈R andθ∈ Sdθ -1 are the parameters of hypersurfaces {x∈Rd | hg(x),θi =t}.
4
Published as a conference paper at ICLR 2022
Similar to the Radon transform and the GRT, the spatial Radon transform can also be used to generate
push-forward measure Hμ = H#m for μ ∈ Pk (Rd) as in Equations (6) and (7):
H”(t,θM = //(t-hg(x),θi)dμ.
(13)
Remark 1. Note that the spatial Radon transform can be interpreted as applying the vanilla Radon
transform to μg, where μg refers to the push-forward measure g#m, i.e given a measurable injective
mapping g(∙): Rd → Rdθ, the spatial Radon transform defined by Equation (13) can be rewritten as:
Hμ(t,θ0= Ex 〜μ[δ(t-hg(x),θi)],
=Ex 〜"g[δ(t-hX,θi)]
=Jδ(t- hx,θi)dμg
=R^g (t,θ).
(14)
Hence the spatial Radon transform inherits the theoretical properties of the Radon transform and
incorporates nonlinear projections through g(∙).
In what follows, We use μ ≡ V to denote probability measures μ,ν ∈ Pk (Rd) that satisfy μ(X) = V(X)
for∀X⊆Rd.
Lemma 1. Given an injective mapping g(∙): Rd → Rdθ and two probability measures μ,ν ∈ P (Rd),
forall t ∈ R andθ ∈ Sdθ-1, Hμ(t,θ[g) ≡Hν(t£；g) ifandonly if μ≡ V, i.e. the spatialRadon transform
is an injection on Pk(Rd). Moreover, the spatial Radon transform is an injection on Pk(Rd) if and
only if the mapping g(∙) is an injection.
See Appendix A for the proof of Lemma 1.
We now introduce the augmented sliced Wasserstein distance, by utilizing the spatial Radon transform
as the measure push-forward operator:
Definition 2. Given two probability measures μ,ν ∈ Pk (Rd) and an injective mapping g(∙): Rd → Rdθ,
the augmented sliced Wasserstein distance (ASWD) of order k ∈ [1,+∞) is defined as:
1
k k	∖ k
ASWDk (μ,V ;g)= /	Wk(H“ (*g),H (*g))dθ
Sdθ -1
(15)
where θ∈Sdθ-1, Wk is the k-Wasserstein distance defined by Equation (1), andH refers to the spatial
Radon transform defined by Equation (13).
Remark 2. Following the connection between the spatial Radon transform and the vanilla Radon
transform as shown in Equation (14), the ASWD can be rewritten as:
1
k
ASWDk (μ,ν;g)= JS^ Wk (Rμs (∙,θ),R^s (∙,θ))dθ)
=SWDk (μg ,Vg),	(16)
where μg and Vg are probability measures on Rdθ which satisfy g(x)〜μg for X 〜μ and g(y)〜Vg
for y 〜V.
Theorem 1. The augmented sliced Wasserstein distance (ASWD) of order k ∈ [1, +∞) defined by
Equation (15) with a mapping g(∙):Rd →Rdθ is a metric on Pk (Rd) ifandonly if g(∙) is injective.
The proof of Theorem 1 is provided in Appendix C. Theorem 1 shows that the ASWD is a metric given
a fixed injective mapping g(∙). In practical applications, the mapping g(∙) needs to be optimized to
project samples onto discriminating hypersurfaces. We show in Corollary 1.1 that the ASWD between
μ and V with the optimized g(∙) is also a metric under mild conditions.
Corollary 1.1. The augmented sliced Wasserstein distance (ASWD) of order k ∈ [1,+∞) between
two probability measures μ,ν ∈ Pk (Rd) defined by Equation (15) with the optimal mapping
g* (•) = argmax{ ASWDk (μ,ν ;g) - L(μ,ν,λ[g)}
g
(17)
is a metric on Pk (Rd), where L(μ,ν,λ[g) = λ(EX〜μ [∣∣g(x) ||k "Ek” [∣∣g(y) ||k ]) for λ ∈ (1,+∞).
5
Published as a conference paper at ICLR 2022
The proof of Corollary 1.1 is provided in Appendix D.
Remark 3. CoroUary1.1 shows thatgiven measures μ1,μ2 ,μ3 ∈ Pk (Rd),thetriangle inequality holds
for the ASWD when g(∙) is optimizedfor each pair OfmeaSures, as shown in Appendix D. Itis worth
noting that λ> 1 isa sufficient conditionfor theASWD to be a metric - asfurther discussed in Remark 6,
0 < λ ≤ 1 can also lead to finite ||g(x)||2 in various scenarios, resulting in valid metrics. The discussion
on the impact of λ on the performance of the ASWD in practice can be found in Appendix G.2.
3.2 Numerical implementation
We discuss in this section how to realize injective mapping g(∙) with neural networks due to their
expressiveness and optimize it with gradient based methods.
Injective neural networks: As stated in Lemma 1 and Theorem 1, the injectivity of g(∙) is the sufficient
and necessary condition for the ASWD being a valid metric. Thus we need specific architecture designs
on implementing g(∙) by neural networks. One option is the family of invertible neural networks
(Behrmann et al., 2019; Karami et al., 2019), which are both injective and surjective. However, the
running cost of those models is usually much higher than that of vanilla neural networks. We propose
an alternative approach by concatenating the input x of an arbitrary neural network to its output φω (x):
gω (x) = [x,φω (x)].
(18)
Itis trivial to show that gω(x) is injective, since different inputs will lead to different outputs. Although
embarrassingly simple, this idea of concatenating the input and output of neural networks has found
success in preserving information with dense blocks in the DenseNet (Huang et al., 2017), where the
input of each layer is injective to the output of all preceding layers.
Optimization objective: We aim to slice distributions with maximally discriminating hypersurfaces
between two distributions while avoiding the projected samples being arbitrarily large, so that the
projected samples between the compared distributions are finite and most dissimilar regarding the
ASWD, as shown in Figure 1. Similar ideas have been employed to identify important projection
directions (Deshpande et al., 2019; Kolouri et al., 2019a; Paty and Cuturi, 2019) or a discriminative
ground metric (Salimans et al., 2018) in optimal transport metrics. For the ASWD, the parameterized
injective neural network gω (∙) is optimized by maximizing the following objective:
L(μ,ν 3,λ)= (Z	Wk (Hμ(*gω),Hν (∙,θ∙,gω))dθY -L(μ,ν,λ∙,gω),	(19)
Sdθ -1
where λ> 0 and the regularization term L(μ,ν,λ[gω )= λ(Ek〜μ [∣∣gω (x)||k ] + Ek〜V [∣∣gω (y)||k ]) on
the magnitude of the neural network’s output is used, otherwise the projections may be arbitrarily large.
Remark 4. The regularization coefficient λ adjusts the introduced non-linearity in the evaluation of
theASWD by controlling the norm of φω (∙) in Equation (18). In particular, when λ →∞, the nonlinear
term φω (∙) shrinks to 0. The intrinsic dimension ofthe augmented space, i.e. the number ofnon-zero
dimensions in the augmented space, is hence explicitly controlled by the flexible choice of φω (∙) and
implicitly regularized by L(μ,ν,λ∖gω).
By plugging the optimized g：入卜)=argmax(L(μ, V; gω,λ)) into Equation (15), we obtain the
,	gω
empirical version of the ASWD. Pseudocode is provided in Appendix E.
4	Related work
Recent work on slice-based Wasserstein distances mainly focused on improving their projection
efficiency, leading to a reduced number of projections needed to capture the structure of data
distributions (Kolouri et al., 2019a; Nguyen et al., 2021). The GSWD proposes using nonlinear
projections to achieve this goal, and it has been proved to be a valid distance metric if and only if
they adopt injective GRTs, which only include the circular functions and a finite number of harmonic
polynomial functions with odd degrees as their feasible defining functions (Ehrenpreis, 2003). While
the GSWD has shown impressive performance in various applications (Kolouri et al., 2019a), its
defining function is restricted to the aforementioned limited class of candidates. In addition, the
6
Published as a conference paper at ICLR 2022
selection of defining function is usually task-dependent and needs domain knowledge, and the impact
on performance from different defining functions is still unclear.
To tackle those limitations, (Kolouri et al., 2019a) proposed the GSWD-NN, which directly takes
the outputs of a neural network as its projection results without using the standard Radon transform
or GRTs. However, this brings three side effects: 1) The number of projections, which equals the
number of nodes in the neural network’s output layer, is fixed, thus new neural networks are needed
if one wants to change the number of projections. 2) There is no random projections involved in the
GSWD-NN, as the projection results are determined by the inputs and weights of the neural network. 3)
The GSWD-NN is a pseudo-metric since it uses a vanilla neural network, rather than Radon transform
or GRTs, as its push-forward operator. Therefore, the GSWD-NN does not fit into the theoretical
framework of GSWD and does not inherit its geometric properties.
Another notable variant of the SWD is the distributional sliced Wasserstein distance (DSWD) (Nguyen
et al., 2021). By finding a distribution of projections that maximizes the expected distances over these
projections, the DSWD can slice distributions from multiple directions while having high projection
efficiency. Injective GRTs are also used to extend the DSWD to the distributional generalized sliced
Wasserstein distance (DGSWD) (Nguyen et al., 2021). Experiment results show that the DSWD and
the DGSWD have superior performance in generative modelling tasks (Nguyen et al., 2021). However,
neither the DSWD nor the DGSWD have solved the problem with the GSWD, i.e. they are still not
able to produce nonlinear projections adaptively.
Our contribution differs from previous work in three ways: 1) The ASWD is data-adaptive, i.e. the hyper-
surfaces where high-dimensional distributions are projected onto can be learned from data. This implies
one does not need to specify a defining function from limited choices. 2) Unlike GSWD-NN, the ASWD
takes a novel direction to incorporate neural networks into the framework of sliced-based Wasserstein
distances while maintaining the properties of sliced Wasserstein distances. 3) Previous work on introduc-
ing nonlinear projections into Radon transform either is restricted to only afew candidates of defining
functions (GRTs) or breaks the framework of Radon transforms (neural networks in GSWD-NN), in
contrast, the spatial Radon transform provides a novel way of defining nonlinear Radon-type transforms.
5	Experiments
In this section, we describe the experiments that we have conducted to evaluate performance of the
proposed distance metric. The GSWD leads to the best performance in a sliced Wasserstein flow
problem reported in (Kolouri et al., 2019a) and the DSWD outperforms the compared methods in
the generative modeling task examined in (Nguyen et al., 2021) on CIFAR 10 (Krizhevsky, 2009),
CelebA (Liu et al., 2015), and MNIST (LeCun et al., 1998) datasets (Appendix H.2). Hence, we
compare performance of the ASWD with the state-of-the-art distance metrics in the same examples
and report results as below1. We provide additional experiment results in the appendices, including a
sliced Wasserstein autoencoder (SWAE) (Kolouri et al., 2019b) using the ASWD (Appendix I), image
color transferring (Appendix J) and sliced Wasserstein barycenters (Appendix K).
To examine the robustness of the ASWD, throughout the experiments, we adopt the injective network
architecture given in Equation (18) and set φω to be a single fully-connected layer neural network
whose output dimension equals its input dimension, with a ReLU layer as its activation function. The
order k is set to be 2 in all experiments.
5.1	Sliced Wasserstein flows
We first consider the problem of evolving a source distribution μ toa target distribution V by minimizing
slice-based Wasserstein distances between μ and V in the sliced Wasserstein flow task reported in
(Kolouri et al., 2019a).
∂tμt = -VSWD(μt,ν),	(20)
where μt refers to the updated source distribution at each iteration t. The SWD in Equation (20) can
be replaced by other sliced-Wasserstein distances to be evaluated. As in (Kolouri et al., 2019a), the
2-Wasserstein distance was used as the metric for evaluating performance of different distance metrics
in this task. The set of hyperparameter values used in this experiment can be found in Appendix F.1.
1 Code to reproduce experiment results is available at : https://github.com/xiongjiechen/ASWD.
7
Published as a conference paper at ICLR 2022
Iteration
Iteration
Figure 2: The first and third columns are target distributions. The second and fourth columns are log
2-Wasserstein distances between the target distribution and the source distribution. The horizontal
axis show the number of training iterations. Solid lines and shaded areas represent the average values
and 95% confidence intervals of log 2-Wasserstein distances over 50 runs. A more extensive set of
experimental results can be found in Appendix G.1.
Without loss of generality, We initialize μo to be the standard normal distribution N(0,I). We repeat
each experiment 50 times and record the 2-Wasserstein distance between μ and V at every iteration. In
Figure 2, We plot the 2-Wasserstein distances betWeen the source and target distributions as a function
of the training epochs and the 8-Gaussian, the Knot, the Moon, and the Swiss roll distributions are
respective target distributions. For clarity, Figure 2 displays the experiment results from the 6 best
performing distance metrics, including the ASWD, the DSWD, the SWD, the GSWD-NN 1, which
directly generates projections through a one layer MLP, as well as the GSWD with the polynomial
of degree 3, circular defining functions, out of the 12 distance metrics we compared.
We observe from Figure 2 that the ASWD not only leads to smaller 2-Wasserstein distances, but
also converges faster by achieving better results with fewer iterations than the other methods in these
four target distributions. A complete set of experimental results with 12 compared distance metrics
and 8 target distributions are included in Appendix G.1. The ASWD outperforms the compared
state-of-the-art sliced-based Wasserstein distance metrics with 7 out of the 8 target distributions except
for the 25-Gaussian. This is achieved through the simple injective network architecture given in
Equation (18) and a one layer fully-connected neural network with equal input and output dimensions
throughout the experiments. In addition, ablation study is conducted to study the effect of injective
neural networks, the regularization coefficient λ, the choice of the dimensionality dθ of the augmented
space, and the optimization of hypersurfaces in the ASWD. Details can be found in Appendix G.2.
5.2	Generative modeling
In this experiment, we use sliced-based Wasserstein distances for a generative modeling task described
in (Nguyen et al., 2021). The task is to generate images using generative adversarial networks (GANs)
(Goodfellow et al., 2014) trained on either the CIFAR10 dataset (64×64 resolution) (Krizhevsky,
2009) or the CelebA dataset (64×64 resolution) (Liu et al., 2015). Denote the hidden layer and the
output layer of the discriminator by hψ and DΨ, and the generator by GΦ, we train GAN models with
the following objectives:
min SWD(hψ (pr),hψ (GΦ (pz))),	(21)
max Ex〜pr[log(Dψ(hψ(X)))]+Ez〜°之[log(1-Dψ(hψ(Gφ(z))))],	(22)
Ψ,ψ
where pz is the prior of latent variable z and pr is the distribution of real data. The SWD in Equation
(21) is replaced by the ASWD and other variants of the SWD to compare their performance. The
8
Published as a conference paper at ICLR 2022
-^ASWD
-^DSWD
GSWD
Figure 3: FID scores of generative models trained with different metrics on CIFAR10 (left) and CelebA
(right) datasets with L= 1000 projections. The error bar represents the standard deviation of the FID
scores at the specified training epoch among 10 simulation runs.
Table 1: FID scores of generative models trained with different distance metrics. Smaller scores
indicate better image qualities. L is the number of projections, we run each experiment 10 times and
report the average values and standard errors of FID scores for CIFAR10 dataset and CELEBA dataset.
The running time per training iteration for one batch containing 512 samples is computed based on a
computer with an Intel (R) Xeon (R) Gold 5218 CPU 2.3 GHz and 16GB of RAM, and a RTX 6000
graphic card with 22GB memories.
CIFAR10
L	SWD (Bonneeletal., 2015)			GSWD (Kolouri etal.,2019a)			DSWD (Nguyen et al., 2021)		ASWD	
	FID	t (s/it)	FID	t (s/it)	FID	t (s/it)	FID	t (s/it)
^30	121.4±7.0	034	108.3±5.6	041	74.2 ± 3.1	055	65.7±3.2	0.58-
100	104.6±5.2	0.35	105.2±3.2	0.74	66.5 ± 3.9	0.57	62.5±1.9	0.60
1000	102.3±5.3	0.36	98.2±5.1	2.22	62.3 ± 5.7	1.30	59.3±3.2	1.38
CELEBA
~	94.8±2.5	035	95.1±4.2	0.40	86.0 ± 1.4	053	81.2±1.3	0.59
100	88.7±5.7	0.36	86.7±3.5	0.75	76.1 ± 3.5	0.55	73.2±2.6	0.61
1000	86.5±4.1	0.38	85.2±6.3	2.19	71.3±4.7	1.28	67.4±2.1	1.38
GSWD with the polynomial defining function and the DGSWD is not included in this experiment due
to its excessively high computational cost in high-dimensional space. The Frechet Inception Distance
(FID score) (Heusel et al., 2017) is used to assess the quality of generated images. More details on
the network structures and the parameter setup used in this experiment are available in Appendix F.2.
We run 200 and 100 training epochs to train the GAN models on the CIFAR10 and the CelebA dataset,
respectively. Each experiment is repeated for 10 times and results are reported in Table 1. With the same
number of projections and a similar computation cost, the ASWD leads to significantly improved FID
scores among all evaluated distances metrics on both datasets, which implies that images generated with
the ASWD are of higher qualities. Figure 3 plots the FID scores recorded during the training process.
The GAN model trained with the ASWD exhibits a faster convergence as it reaches smaller FID scores
with fewer epochs. Randomly selected samples of generated images are presented in Appendix H.1.
6 Conclusion
We proposed a novel variant of the sliced Wasserstein distance, namely the augmented sliced
Wasserstein distance (ASWD), which is flexible, has a high projection efficiency, and generalizes well.
The ASWD adaptively updates the hypersurfaces used to slice compared distributions by learning from
data. We proved that the ASWD is a valid distance metric and presented its numerical implementation.
We reported empirical performance of the ASWD over state-of-the-art sliced Wasserstein metrics
in various numerical experiments. We showed that ASWD with a simple injective neural network
architecture can lead to the smallest distance errors over the majority of datasets in a sliced Wasserstein
flow task and superior performance in generative modeling tasks involving GANs and VAEs. We
have also evaluated the applications of the ASWD in downstream tasks including color transferring
and Wasserstein barycenters. What remains to be explored includes the topological properties of the
ASWD. We leave this topic as an interesting future research direction.
9
Published as a conference paper at ICLR 2022
References
J. Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pages 1964-1974, Long Beach, California, USA, 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proc.
International Conference on Machine Learning (ICML), pages 214-223, Sydney, Australia, 2017.
J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J. Jacobsen. Invertible residual networks.
In Proc. International Conference on Machine Learning (ICML), pages 573-582, Long Beach,
California, USA, 2019.
E. Bernton, P. E. Jacob, M. Gerber, and C. P. Robert. On parameter estimation with the wasserstein
distance. Information and Inference: A Journal of the IMA, 8(4):657-676, 2019.
G. Beylkin. The inversion problem and applications of the generalized Radon transform.
Communications on Pure and Applied Mathematics, 37(5):579-599, 1984.
N. Bonneel, J. Rabin, G. Peyra and H. Pfister. Sliced and Radon Wasserstein barycenters of measures.
Journal of Mathematical Imaging and Vision, 51(1):22-45, 2015.
N. Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris
11, 2013.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation.
IEEE Transactions on Pattern Analysis and Machine Intelligence (IPAMI), 39(9):1853-1865, 2016.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proc. Advances in Neu-
ral Information Processing Systems (NeurIPS), pages 2292-2300, Lake Tahoe, Nevada, USA, 2013.
M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In Proc. International
Conference on Machine Learning (ICML), pages 685-693, Beijing, China, 2014.
I. Deshpande, Y. Hu, R. Sun, A. Pyrros, et al. Max-sliced Wasserstein distance and its use for GANs. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10648-10656,
Long Beach, California, USA, 2019.
A. Dessein, N. Papadakis, and J. Rouas. Regularized optimal transport and the rot mover’s distance.
The Journal of Machine Learning Research (JMLR), 19(1):590-642, 2018.
L. Ehrenpreis. The universality of the Radon transform, chapter 5, pages 299-363. Oxford University
Press, Oxford, UK, 2003.
S. Ferradans, N. Papadakis, G. Peyra and J. Aujol. Regularized discrete optimal transport. SIAM
Journal on Imaging Sciences, 7(3):1853-1882, 2014.
A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyr6. Sample complexity of Sinkhorn divergences.
In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1574-1583,
Okinawa, Japan, 2019.
I. Goodfellow, A. J. Pouget, M. Mirza, B. Xu, F. D. Warde, et al. Generative adversarial nets. In
Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 2672-2680, MOntreal,
Canada, 2014.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein
GANs. In Proc. Advances in neural information processing systems (NeurIPS), pages 5767-5777,
Long Beach, California, USA, 2017.
S. Helgason. The Radon transform, volume 2. Basel, Switzerland: Springer, 1980.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. In Proc. Advances in neural information
processing systems (NeurIPS), pages 6626-6637, Long Beach, California, USA, 2017.
10
Published as a conference paper at ICLR 2022
A. Homan and H. Zhou. Injectivity and stability for a generic class of generalized Radon transforms.
TheJournalofGeometricAnalysis, 27(2):1515-1529,2017.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
4700-4708, Hawaii, USA, 2017.
M. Karami, D. Schuurmans, J. Sohl-Dickstein, L. Dinh, and D. Duckworth. Invertible convolutional
flow. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 5636-5646,
Vancouver, Canada, 2019.
S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced Wasserstein
distances. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 261-272,
Vancouver, Canada, 2019a.
S. Kolouri, P. E. Pope, C. E. Martin, and G. K. Rohde. Sliced-Wasserstein autoencoders. In Proc. In-
ternational Conference on Learning Representations (ICLR), New Orleans, Louisiana, USA, 2019b.
A.	Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proc. IEEE
International Conference on Computer Vision (ICCV), pages 3730-3738, Las Condes, Chile, 2015.
B.	Muzellec and M. Cuturi. Subspace detours: Building transport plans that are optimal on subspace pro-
jections. Proc. Advances in Neural Information Processing Systems (NeurIPS), 32:6917-6928, 2019.
K. Nadjahi et al. Statistical and topological properties of sliced probability divergences. Proc.
Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.
K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to
generative modeling. In Proc. International Conference on Learning Representations (ICLR),
Vienna, Austria, 2021.
F.	Paty and M. Cuturi. Subspace robust Wasserstein distances. In Proc. International Conference
on Machine Learning (ICML), pages 5072-5081, Long Beach, California, USA, 2019.
G.	Peyre and M. CUtUrL Computational optimal transport. Foundations and Trends® in Machine
Learning, 11(5-6):355-607, 2019.
J. Radon. Uber die bestimmUng von fUnktionen dUrch ihre integralwerte laengs gewisser
mannigfaltigkeiten. Ber. Verh. Saechs. Akad. Wiss. Leipzig Math. Phys. Kl., 69:262, 1917.
D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning (ICML), pages 1530-1538, Lille, France, 2015.
T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs Using optimal transport. In
Proc. International Conference on Learning Representations (ICLR), VancoUver, Canada, 2018.
C. Villani. Optimal Transport: old and new, volUme 338. Berlin, Germany: Springer Science &
BUsiness Media, 2008.
11
Published as a conference paper at ICLR 2022
Appendix A Proof of the Lemma 1
We prove that the spatial Radon transform defined with a measurable mapping g(∙):Rd→Rdθ is an
injection on Pk (Rd) ifand only if g(∙) is injective. Inthefollowing contents, We use Pk(Rd) to denote a
set of Borel probability measures with finite k-th moment on Rd, and f1 ≡f2 is used to denote functions
fι(∙): X →R and f2(∙): X →R that satisfy fι(χ) = f2(χ) for ∀x ∈ X, and fι ≡ f2 is used to denote
functions fι(∙): X → R and f2(∙): X → R that satisfy fι(χ) = f2(χ) for certain X ∈ X. In addition,
for probability measures μ and V, we use μ≡ V to denote μ,ν ∈ Pk(Rd) that satisfy μ(X) = V(X) for
∀X⊆Rd, and μ≡ V to denote μ,ν∈ Pk (Rd) that satisfy μ(X) = V(X) for certain X ⊆Rd.
Proof. By using proof by contradiction, we first prove that if g(∙) is injective, the corresponding
spatial Radon transform is injective. If the spatial Radon transform defined with an injective mapping
g(∙): Rd →Rdθ is not injective, there exist μ, V ∈ Pk(Rd), μ≡ v, such that H*(t,θ[g) ≡Hν(t,θ[g)
for∀t∈Rand∀θ∈Sdθ-1.
From Equation (14),for ∀t ∈ R and ∀θ ∈ Sdθ-1, the spatial Radon transform of μ can be written as:
Hμ(t,/g)= Rμg (t,θ),	(23)
HV(t,θ:g)= RVg (t,θ),	(24)
where μg and Vg refer to the push-forward measures g# μ and g# V, respectively. From the assumption
Hμ(t,θ∙,g) ≡Hν(t,-；g) and Equations (23) and (24), we know R^g (t,θ) ≡R^g (t,θ) for ∀t ∈ R and
∀θ∈Sdθ-1, which implies μ^g ≡ Vg since the Radon transform is injective.
Since g(∙) is injective, for all measurable X ⊆ Rd, X ∈X if and only if X = g(x) ∈ g(X), which implies
P(x∈X)= P(X∈g(X)),P(y∈X)= P(y∈g(X)). Therefore,
d/^g =	dμ,
g(X)	X
I	dVg = I dν.
g(X)	X
(25)
(26)
Since μg ≡ Vg, fromEquations (25) and(26) wehave: RXdμ = RXdν for ∀X ⊆ Rd. Hence, for ∀X⊆ Rd:
I d(μ-v)=0,
X
(27)
which implies μ ≡ V, contradicting with the assumption μ ≡ V. Therefore, if Hμ ≡ HV, μ ≡ V. In
addition, from the definition of the spatial Radon transform in Equation (13), itis trivial to show that
if μ ≡ V, Hμ(t,θ;g) ≡ HV (t,θ[g). Therefore, Hμ ≡ HV if and only if μ ≡ v, i.e. the spatial Radon
transform H defined with an injective mapping g(∙): Rd →Rdθ is injective.
We now prove that if the spatial Radon transform defined with a mapping g(∙) : Rd → Rdθ is
injective, g(∙) must be injective. Again, we use proofby contradiction. If g(∙) is not injective, there
exist X0,y0 ∈ Rd such that xo = yo and g(xo) = g(yo). For Dirac measures μι and vi defined by
μι(X) = JX δ(x-xo)dx and vι(Y) = JY δ(y—yo)dy, where X ,Y⊆ Rd, we know μι ≡ vi as x0 = yo.
We define variables X 〜μι and y 〜vι. Then for variables X = g(x)〜μ2 and y = g(y)〜v2, where
μ2 and V2 are push-forward measures g#μ1 and g#vi, it is trivial to derive for ∀X ,Y⊆ Rd
μ2(g(X ))= [	δ(X-g(xo))dX,	(28)
g(X)
V2(g(Y))= [	δ(y-g(yo))dy,	(29)
g(Y)
which implies μ2 ≡V2 as g(xo) = g(yo).
From Equations (23), (24), (28) and (29), for∀t∈R and∀θ∈ Sdθ-1:
Hμi - = Rμ2(t,θ),
=RV2(t,θ),
=Hνι (t£;g),	(30)
12
Published as a conference paper at ICLR 2022
contradicting with the assumption that the spatial Radon transform is injective. Therefore, if the spatial
Radon transform is injective, g(∙) must be injective. We conclude that the spatial Radon transform
is injective if and only if the mapping g(∙) isan injection on Pk (Rd).	□
13
Published as a conference paper at ICLR 2022
Appendix B	Special cases of spatial Radon transforms
Remark 5. The spatial Radon transform degenerates to the vanilla Radon transform when the mapping
g(∙) is an identity mapping. In addition, the spatial Radon transform is equivalent to the polynomial
GRT (Ehrenpreis, 2003) when g(x) = (xα1 ,...,xαdα ), where α is multi-indices αi = (ηi,1,...,ηi,d) ∈ Nd
satisfying ∣ɑi∣ = Ej=Inij = m. m is the degree ofthe polynomialfunction, Xai =门=避?/ given an
input x = (xι,…,Xd) ∈ Rd, and da is the number ofallpossible multi-indices a thatsatisfies ∣ɑi∣ = m.
We provide a proof for the claim in Remark 5.
Proof. Given a probability measure μ ∈ P(Rd),the spatial Radon transform of μ is defined as:
H”(t,θM = / J(t-hg(x),θ))dμ,
(31)
where t ∈ R and θ ∈ Sdθ-1 are the parameters of hypersurfaces in Rd. When the mapping g(∙) is an
identity mapping, i.e. g(x) = x for ∀x ∈ Rd, the spatial Radon transform degenerates to the vanilla
Radon transform:
Hμ(t,θ0 = / J(t-hx,θ)')dμ
=Rμ(t,θ).
(32)
For GRTs, (Ehrenpreis, 2003) provides a class of injective GRTs named polynomial GRTs by adopting
homogeneous polynomial functions with an odd degree m as the defining function:
dα
Gμ(t,θ) = ∖/(t-XθiXai )dμ,
s.t.∣αi ∣ = m,
(33)
where αi = (ηi,1,...,ηi,d) ∈Nd, ∣αi∣ = Pjd=1 ηi,j, Xai = Qjd=1 Xjηi,j forX= (X1,...,Xd) ∈Rd, da is the
number of all possible multi-indices αi that satisfies ∣αi∣ =m, and θ= (θ1,...,θdα ) ∈Sdα-1
In spatial Radon transform, for ∀x ∈ Rd, when the mapping g(∙) is defined as:
g(x) = (xa1,...,xadα ),
the spatial Radon transform is equivalent to the polynomial GRT defined in Equation (33):
H”(t,θM = / J(t-hg(x),θ))dμ
dα
=δ δ(t -Xθi Xai )dμ.
Rd	i=1
(34)
(35)
□
14
Published as a conference paper at ICLR 2022
Appendix C	Proof of Theorem 1
We provide aproofthatthe ASWD defined Witha mapping g(∙): Rd → Rdθ isa metric on Pk (Rd) ,ifand
only if g(∙) is injective. In what follows, we denote a set ofBorel probability measures with finite k-th
moment on Rd by Pk (Rd), and use μ,ν ∈ Pk (Rd) to refer to two probability measures defined on Rd.
Proof. Symmetry: Since the k-Wasserstein distance is a metric thus symmetric (Villani, 2008):
Wk(H“(∙,θ∙,g),H" (∙,θ∙,g))= Wk(Hν (*g),Hμ(*g)).	(36)
Therefore,
1
ASWDk(〃,v;g)= ( /	Wk(Hμ(∙,θ∙,g),H"(；3;g))de\k
Sdθ -1
=(/勾TWk(H(*g),H"(∙Qg))dθ) k = ASWDk(v,〃;g).
Triangle inequality: Given an injective mapping g(∙) : Rd → Rdθ and probability measures
μ1,μ2,μ3 ∈ Pk (Rd), since the k-Wasserstein distance satisfies the triangle inequality (Villani, 2008),
the following inequality holds:
ASWDk(〃i,〃3；g)= ( Z	Wk(H“1 (4;g),H“3(4;g))d0y
Sdθ-1
≤(∕	(Wk (H “1 (∙R,g),Hκ2 I,。©)
Sdθ -1
1
+Wk (H “2 (〃；g),H“3(〃；g)))k dθ)”
≤ (ZSdLWk (H “1 (*g),HU*g))dθ) k
1
+ IL勾一 Wk(H“2(4;g),H“3(∙,θ0)dθ)”
=ASWDk(μι,μ2;g)+ASWDk (μ2,μ30,
where the second inequality is due to the Minkowski inequality in Lk(Sdθ-1).
Identity of indiscernibles: Since Wk (μ,μ) = 0 for ∀μ ∈ Pk (Rd),we have
1
ASWDk (μ,μ∙,g)=( [	Wk (Hμ (*g),H“(*g))d。[" =0,	(37)
Sdθ -1
for ∀μ ∈ Pk (Rd). Conversely, for ∀μ,ν ∈ Pk (Rd), if ASWDk (μ,ν ;g) = 0, from the definition of the
ASWD:
ASWDk(〃,v;g)= (LTWk(H"(*g),Hν(*g))dθ) k =0,	(38)
Due to the non-negativity of k-th Wasserstein distance as itis a metric on Pk(Rd) and the continuity
of Wk(∙,∙) on Pk (Rd) (Villani, 2008), Wk(Hμ(∙,θ[g),Hν(∙,θ[g)) =0 holds for ∀θ ∈ Sdθ-1 if and only
if Hμ(Rg') ≡Hν(∙,θ[g). Again, given the spatial Radon transform is injective when g(∙) is injective
(see the proof in Appendix A), Hμ(∙,θ∖g) ≡H (∙,θ[g) implies μ ≡ V if g(∙) is injective.
In addition, if g(∙) is not injective, the spatial Radon transform is not injective (see the proof in Appendix
A), then ∃μ, V∈ Pk(Rd), μ≡ V such that H,μ (・,0;g) ≡Hν (•,θ;g), which implies ASWDk(μ,ν;g)=0
for μ≡ ν. Therefore, the ASWD satisfies the identity of indiscernibles if and only if g(∙) is injective.
Non-negativity: The three axioms of a distance metric, i.e. symmetry, triangle inequality, and identity
of indiscernibles imply the non-negativity of the ASWD. Since the Wasserstein distance is non-negative,
15
Published as a conference paper at ICLR 2022
for ∀μ,ν ∈ Pk (Rd), it can also be straightforwardly Provedthe ASWD between μ and V is non-negative:
ASWDk (〃,v;g)= ( Z	Wk (H4(*g),Hν (*g))dθ1k
Sdθ -1
1
≥( I 0k dθ ) fc=0.	(39)
Sdθ-1
Therefore, the ASWD is ametricon Pk (Rd) if and only if g(∙) is injective.	□
16
Published as a conference paper at ICLR 2022
Appendix D	Proof of Corollary 1.1
We first introduce Lemma 2 to support the proof of Corollary 1.1.
Lemma 2. For λ ∈ (1, +∞), the optimal mapping g*(∙) defined by Equation (17) satisfies
||g* (x) ∣∣2 < ∞ for ∀x ∈Rd 〜μ and∀x ∈Rd 〜V.
Proof. Recall that in Equation (16) the ASWD can be rewritten as:
ASWDk (μ,ν ;g) = SWDk g,Vg)
=U	Wk(R^g(∙,θ),R^g(∙,θ))dθ)k,	(40)
Sdθ-1
where transformed variables X = g(x)〜μ^g for X 〜μ and y = g(y)〜Vg for y 〜ν, respectively.
Combining the equation above with Equation (2):
ASWDk(〃,v;g)= (∕%-Wk(Rμg(∙,θ),R^g(∙,θ))dθ) k
=(Zqd -1∕ |F-)#Ag(Z)-FP3g(z)lkdzdθ)
Sθ 0
≤ (/d「/(IFR%g)(z)| 十 |FR。%⑶1)“dzdθ)k,	(41)
where # denotes the push forward operator, Pθ : X ∈ Rdθ ―hχ,θ)∈ R, and Rθ (μg )= Pθ #&g, Rθ (Vg )=
Pθ# Vg refer to one-dimensional measures obtained by slicing μg, Vg with a unit vector θ, FR∖^)and
FR1(^)are inverse cumulative distribution functions (CDFs) of Rθ (μg) and Rθ (Vg), respectively.
By repeatedly applying the Minkowski’s inequality to Equation (41), we obtain the following
inequalities:
1
(42)
1
k
Let S = hX,θ),then Z = Frθ(^3)(s), dz = dFRθ (μg)(s):
ZjFRxg )(z)∣k dz=L ∣s∣kdFRθ(μg)(s)
=[lhx,θilkdμg
Rdθ
=[∖hg(X)阳∖kdμ,
Rd
(43)
where the last two equations are obtained through the definitions of the push-forward operators.
Therefore, the following inequalities hold:
ASWDk(μ,ν;g) ≤
IL-∕lFRθ1(μg)
1 ⅛
(z)∣k dzdθ +
L-/|FRθιM)(ZXkdZd
Sdθ -1
∕jhg(x),θi∣kdμdθ k + .L ∣]
Ihg(y),θi∣k dνdθ
1
≤ Ex〜μ[∣∣g(χ)∣∣k]+ Ey^ν 川 g(y)∣∣k ].
(44)
17
Published as a conference paper at ICLR 2022
Then we obtain the following inequalities for the optimization objective:
ASWDk(μ,ν ιg)-L(μ,ν,λ[g)
≤(eX〜μ[∣∣g(x)∣∣k ]+ Ey^ν 川 g(y)∣∣k ])-λ(EX〜μ[∣∣g(x)∣∣k ]+ Ey^ν 川 g(y)∣∣k ])
= (l-λ)(EX〜μ [||g(x)||k]+ Ey^ν 川 g(y)∣∣k ]).	(45)
When We set λ ∈ (1,+∞), if ∃x ∈ Rd 〜μ or y ∈ Rd 〜V such that ∣∣g(x)∣∣2 → ∞ or ∣∣g(y)∣∣2 → ∞,
the optimization objective approaches negative infinity, implying g(∙) is not the optimal mapping.
Therefore, by adopting Equation (17) as the optimization objective, the optimal mapping g*(∙) satisfies
∣∣g*(x)∣∣2 < ∞ for∀x ∈Rd〜μ and∀x ∈Rd〜V.
□
Remark 6. It is worth noting that λ > 1 in Corollary 1.1 is a sufficient condition for the supremum
ofthe optimization objective to be non-positive and ∣∣g*(x)∣∣2 < ∞ for∀x ∈Rd 〜μ and∀x ∈ Rd 〜V.
0 < λ ≤ 1 can also lead to finite ||g(x)||2 in various scenarios. Specifically, the upper bound of the
optimization objective given in Equation (44) is obtained by applying:
Kg(X),θil = IIg(X)||2|Cosg)|S||g(X)||2,	(46)
where α is the angle between θ andg(x). In high-dimensional spaces, Equation (46) gives a very loose
bound since in high-dimensional spaces the majority of sampled θ would be nearly orthogonal to g(X)
and cos(α) is nearly zero with high probability (Kolouri et al., 2019a). Empirically we found that across
all the experiment results, λ in a candidate set of {0.01,0.05,0.1,0.5,1,10,100} all lead to finite g*(∙).
We now prove Corollary 1.1, i.e ASWDk (μ,ν ;g*) is a metric on Pk (Rd), where g*(∙) is the optimal
mapping defined by Equation (17) for λ ∈ (1,+∞).
Proof. Symmetry: Since the k-Wasserstein distance is a metric thus symmetric (Villani, 2008):
Wk(Hμ(*g*),HV (*g*))= Wk(HV (*g*),Hμ(*g*)).	(47)
Therefore,
ASWDk(μ,ν ;g*)= (Z	Wk (Hμ(∙,θ∙,g*),HV (∙,θ∕))dθX
Sdθ -1
1
=(/	Wk (HV (*g*),Hμ(∙Qg*))dθY = ASWDk (ν,μ∙,g*).
Sdθ -1
Triangle inequality: From Lemma 2, when λ ∈ (1, +∞), the optimal mapping g*(∙) satisfies
||g* (x) II2 < ∞ for ∀x ∈ Rd 〜μ and ∀x ∈ Rd 〜V, hence ASWDk (ν,μ[g*) is finite due to Equation (16).
We then prove that ASWDk (ν∕g*) satisfies the triangle inequality.
Denote by g；, g2, and g3 optimal mappings that result in the supremum of Equation (19) between
μι and μ2, μι and μ3, μ2 and μ3, respectively, since ASWDk(μι,μ20), ASWDk(μι,μ3';g2'), and
ASWDk(μ2,μ3;g3) are finite, the following equations hold:
ASWDk(μι,μ2^) ≤ ASWDk(仙1小3贫)+ASWDk侬心切)	(48)
≤ sup{ASWDk(μι,μ3[g)}+sup{ASWDk (小2山3；。)}	(49)
gg
=ASWDk (μι,μ3^)+ASWDk (小2山3道3),	(50)
where the first inequality are from the metric property of the ASWD.
Identity of indiscernibles: Since Wk (μ,μ) = 0 for ∀μ ∈ Pk (Rd),we have
ASWDk (〃,〃；g*)= (Z	Wk(Hμ(∙,θ∕),Hμ(∙,θ∕))dθ[k =0,	(51)
Sdθ-1
18
Published as a conference paper at ICLR 2022
for ∀μ ∈ Pk (Rd). Conversely, for ∀μ,ν ∈ Pk (Rd), if ASWDk (μ,ν[g*) = 0, from Equation (15):
ASWDk(μ,ν;g*)= 1LTWk(H”(∙Qg*),H"(∙,θ∕))dθ) k =0.	(52)
Due to the non-negativity of k-th Wasserstein distance as itis a metric on Pk(Rd) and the continuity
of Wk(∙,∙) on Pk(Rd) (Villani, 2008), Wk(Hμ(∙,θ[g*), HV(，£；g*)) =0 holds for ∀θ ∈Sdθ-1, which
implies Hμ(∙,θ;g*) ≡ HV(∙,θ[g*) for ∀θ ∈ Sdθ-1. Therefore, given the spatial Radon transform is
injective when g*(∙) is injective, H*(∙,θ[g*) ≡H(∙,θ[g*) implies μ≡ V.
Non-negativity: Since the Wasserstein distance is non-negative, for ∀μ ,ν ∈ Pk(Rd), the ASWD
defined with optimal mappings g(∙) between μ and V is also non-negative:
ASWDk(μ,ν;g*)= (，「|Wk(H“(〃;g*),Hν(∙,θ∕))dθ) k
1
≥	0kdθ =0.	(53)
Sdθ-1
Therefore, the ASWD defined with the optimal mapping g*(∙) is non-negative, symmetric, and satisfies
the triangle inequality and the identity of indiscernibles, i.e. the ASWD defined with optimal mappings
g*(∙) is alsoametric.
□
19
Published as a conference paper at ICLR 2022
Appendix E Pseudocode for the empirical version of the ASWD
Algorithm 1 The augmented sliced Wasserstein distance. All of the for loops can be parallelized.
Require: Sets of samples {xn ∈Rd}nN=1, {yn ∈Rd}nN=1;
Require: Randomly initialized injective neural network gω (∙) :Rd → Rdθ;
Require: Number of projections L, hyperparameter λ, learning rate , number of iterations M ;
1:	Initialize D=0,Lλ =0,m= 1;
2:	while ω has not converged and m ≤ M do
3:	Draw a set of samples {θl}lL=1 from ∈Sdθ-1;
4:	for n = 1 to N do
5:	Compute gω(xn) andgω(yn);
6:	Calculate the regularization term Lλ J Lλ+λ[(llgω Nn)||2) 1 +(llgωNn)ll2) 1 ];
7:	end for
8:	for l = 1 to L do
9:	Computeβ(xn,θl)=hgω(xn),θli,β(yn,θl)=hgω(yn),θli for each n;
10:	Sort β(xn,θl) and β(yn,θl) in ascending order s.t. β(xIl [n] ,θl) ≤ β(xIl [n+1] ,θl) and
β(yIyl [n],θl) ≤ β (yIyl [n+1],θl);
11:	CalculatetheASWD: D JD+(LPN=∕β(χιx向仇)-β(yιy加仇)/)1;
12:	end for
13:	LJD-Lλ;
14:	Update ω by gradient ascent ω J ω+e •▽■ L;
15:	Reset D = 0,Lλ =0, updatemJm+1;
16:	end while
17:	Draw a set of samples {θl}lL=1from ∈Sdθ-1;
18:	for n= 1 to N do
19:	Compute gω (xn) andgω(yn);
20:	end for
21:	for l = 1 to L do
22:	Computeβ(xn,θl)=hgω(xn),θli,β(yn,θl)=hgω(yn),θli for each n;
23:	Sort β(xn, θl) and β(yn, θl) in ascending order s.t. β(xIl [n] , θl) ≤ β(xIl [n+1], θl) and
β(yIyl[n],θl)≤β(yIyl[n+1],θl);
24:	Calculate the ASWD: D JD + (LpN=1∣β(xιχ[n],θι) — β(yιy [n] ,θι)∣k) 1;
25:	end for
26:	Output: Augmented sliced Wasserstein distance D.
In Algorithm 1, Equation (17) is used as the optimization objective, where the regularization term
L(μ,ν,λ[gω ) = λ(Ek〜μ [∣∣gω (x)||k] + Ek〜V [∣∣gω (y)||k]) is used. This particular choice of the regu-
larization term facilitates the proofs to Lemma 2 and subsequently to Corollary 1.1 with details in Ap-
pendix D. In fact, we have also examined other types of regularization terms such as the L2 norm of the
output of g(∙), and empirically they produce similar numerical results as the current regularization term.
20
Published as a conference paper at ICLR 2022
Appendix F	Experimental setups
F.1 Hyperparameters in the sliced Wasserstein flow experiment
We randomly generate 500 samples both for target distributions and source distributions. We initialize
the source distributions μo as standard normal distributions N(0,I), where I is a 2-dimensional
identity matrix. We update source distributions using Adam optimizer, and set the learning rate=0.002.
For all methods, we set the order k = 2. When testing the ASWD, the number of iterations M in
Algorithm 1 is set to 10.
In the sliced Wasserstein flow experiment the mapping g*(∙) optimized by maximizing Equation (17)
was found to be finite for all values of λ in the set of {0.01, 0.05, 0.1, 0.5, 1, 10, 100}, which is a
sufficient condition for the ASWD to be a valid metric as shown in the proof of corollary 1.1 provided
in Appendix D. In addition, numerical results presented in Appendix G.2 indicate that empirical errors
in the experiment are not sensitive to the choice of λ in the candidate set {0.01, 0.05, 0.1, 0.5}. The
reported results in the main paper are produced with λ=0.1.
F.2 Network architecture in the generative modeling experiment
Denote a convolutional layer whose kernel size is s with C kernels by ConvC (s × s), and a
fully-connected layer whose input and output layer have s1 and s2 neurons by FC(s1 × s2). The
network structure used in the generative modeling experiment is configured to be the same as described
in (Nguyen et al., 2021):
hψ : (64 X 64 X 3) →Conv64(4 X 4) →LeakyReLU(0.2) →
Conv128(4 ×4) → BatchNormalization → LeakyReLU(0.2) →
Conv256(4 X4) → BatchNormalization → LeakyReLU(0.2) →
Conv512(4 X4) → BatchNormalization → Tanh -O-u-tp→ut (512 X 4 X 4)
DΨ :Conv1(4 X 4) → Sigmoid -O-u-tp→ut (1 X 1 X 1)
GΦ : z ∈ R100 → ConvTranspose512 (4 X 4) →
BatchNormalization → ReLU → ConvTranspose256 (4 X 4) →
BatchNormalization → ReLU → ConvTranspose128 (4 X 4) →
BatchNormalization → ReLU → ConvTranspose64 (4 X4) →
BatchNormalization → ConvTranspose3 (4 X 4) → Tanh
-O-u-p→ut (64 X 64 X 3)
φ:FC(8192 X 8192) O-→ (8192)-dimenSional vector
We train the models with the Adam optimizer, and set the batch size to 512. Following the setup
in (Nguyen et al., 2021), the learning rate is set to 0.0005 and beta=(0.5, 0.999) for both CIFAR10
dataset and CelebA dataset. For all methods, we set the order k to 2. For the ASWD, the number
of iterations M in Algorithm 1 is set to 5. The hyperparameter λ is set to 1.01 to guarantee that the
ASWD being a valid metric and introduce slightly larger regularization of the optimization objective
due to the small output values from the feature layer hψ.
21
Published as a conference paper at ICLR 2022
Appendix G	Additional
results in the sliced Wasserstein flow experiment
G.1 Full experimental results on the sliced Wasserstein experiment
Figure 4 shows the full experimental results on the sliced Wasserstein flow experiment.
SWD	——GSWD-CircuIar	Max-GSWD-PoIy 5	……' GSWD-NN 1
GSWD-PoIy 3	--- Max-SWD	……，Max-GSWD-CircuIar ------- Max-GSWD-NN 1
GSWD-PoIy 5	--- Max-GSWD-PoIy 3	--- DSWD	---- ASWD
8 GaUSSian _______________________ ______Knot_____ _____________________
-IOl
MoOn
O 500 IOOO 1500 2000
Iteration
O 500 IOOO 1500 2000
Iteration
O 500 IOOO 1500 2000
Iteration
ATVIroI62
Figure 4: Full experimental results on the sliced Wasserstein flow example. The first and third columns
are target distributions. The second and fourth columns are log 2-Wasserstein distances between the
target distributions and the source distributions. The horizontal axis shows the number of training
iterations. Solid lines and shaded areas represent the average values and 95% confidence intervals of
log 2-Wasserstein distances over 50 runs.
O 500 IOOO 1500 2000
Iteration
O 500 IOOO 1500 2000
Iteration
22
Published as a conference paper at ICLR 2022
G.2 Ablation study
Impact of the injectivity and optimization of the mapping
In this ablation study, we compare ASWDs constructed by different mappings to GSWDs with
different predefined defining functions, and investigate the effects of the optimization and injectivity
of the adopted mapping gω (∙) used in the ASWDs. In What follows, “ASWD-vanilla" is used to denote
ASWDs that employ randomly initialized neural network φω (∙) to parameterize the injective mapping
gω(∙) = [∙,φω(.)], i.e. the mapping gω(∙) is not optimized in the ASWD-vanilla and the results of
ASWD-vanilla reported in Figure 5 are obtained by slicing with random hypersurfaces. Furthermore,
the “ASWD-non-injective" refers to ASWDs that do not use the injectivity trick, i.e. the mapping
gω (•) = φω (∙) is not guaranteed to be injective. In addition, the “ASWD-vanilla-non-injective" adopts
both setups in the “ASWD-vanilla" and "ASWD-non-injective", resulting in a random non-injective
mapping gω (∙). The reported experiment results in this ablation study is calculated over 50 runs, and
the neural network φω (∙) is reinitialized randomly in each run.
From Figure 5, it can be observed that the ASWD-vanilla shows comparable performance to GSWDs
defined by polynomial and circular defining functions, which implies GSWDs with predefined defining
functions are as uninformative as slicing distributions with random hypersurfaces constructed by the
ASWD. In GSWDs, the hypersurfaces are predefined and cannot be optimized since they are determined
by the functional forms of the defining functions. On the contrary, we found that the optimization
of hypersurfaces in the ASWD framework can help improve the performance of the slice-based
Wasserstein distance. As in Figure 5, the ASWD and the ASWD-non-injective present significantly
better performance than methods that do not optimize their hypersurfaces (ASWD-vanilla, ASWD-
vanilla-non-injective, and GSWDs). In terms of the impact of the injectivity of the mapping gω, in this
experiment, the ASWD-vanilla exhibits smaller 2-Wasserstein distances than the ASWD-vanilla-non-
injective in all tested distributions, and the ASWD leads to more stable training than the ASWD-non-
injective. Therefore, the injectivity of the mapping gω (∙) does not only guarantee the ASWD to be a valid
distance metric as proved in Section 3, but also better empirical performance in this experiment setup.
Impact of the regularization coefficient
We also evaluated the sensitivity of performance of the ASWD with respect to the regularization
coefficient λ. The ASWD is evaluated with different values of λ and compared with other slice-based
Wasserstein metrics in this ablation study. The numerical results presented in Figure 6 indicates that
different values ofλ in the candidate set {0.01, 0.05, 0.1, 0.5} lead similar performance of the ASWD,
i.e the performance of the ASWD is not sensitive to λ. Additionally, the ASWDs with different values
ofλ in the candidate set outperform the other evaluated slice-based Wasserstein metrics.
We have also evaluated the performance of the ASWD when the range of λ is much larger than in
the candidate set. Specifically, as presented in Figure 7, when λ is set to be large values, e.g 10 or
100, the resulted ASWD leads to decreased performance on par with the SWD. This is consistent with
our expectation that excessive regularization will eliminate nonlinearity as discussed in Remark 2,
leading to similar performance with the SWD.
In addition, the effect of the regularization term on the performance of Max-GSWD-NN was also
investigated in this ablation study. The performance of the Max-GSWD-NN and the Max-GSWD-NN
trained with the regularization term used in the ASWD are compared in Figure 8. From the numerical
results presented in Figure 8, the Max-GSWD-NN 1 with regularization leads to performance similar
to the Max-GSWD-NN 1 without regularization, implying that the performance gap between the
ASWD and the Max-GSWD-NN is not due to the introduction of the regularization term.
Choice of injective mapping
We reported in Figure 9 the performance of the ASWD defined with other types of injective mappings
other than Equation (18). In particular, we examined two invertible mappings, including the planar
flow and radial flow (Rezende and Mohamed, 2015), as alternatives to the injective mapping defined by
Equation (18). The numerical results presented in Figure 9 show that the ASWD defined with planar flow
and radial flow produced better performance than GSWD variants in most setups. They exhibit slightly
worse performance compared with the ASWD with injective mapping defined in Equation (18), possibly
due to the additional restriction in invertible mapping imposed by the planar flow and radial flow.
Choice of the dimensionality dθ of the augmented space
23
Published as a conference paper at ICLR 2022
To investigate how the dimensionality dθ of the augmented space affects the performance of the
ASWD, different choices of dθ are employed in the ASWD. Specifically, the injective network
architecture gω(x) = [x,φω(x)] : Rd → Rdθ given in Equation (18) is adopted and φω is set to be
single fully-connected neural networks whose output dimension equals {1, 2, 3, 4} times its input
dimension, i.e dθ = {2d,3d,4d,5d}, respectively, where dis the dimensionality ofx. The numerical
results are presented in Figure 10, and it can be found that the ASWDs present similar results across
different choices ofdθ. It can also be observed in Figure 10 that the ASWDs with different choices of
dθ consistently produce better performance than the other evaluated slice-based Wasserstein metrics.
SWD
GSWD-PoIy 3
GSWD-PoIy 5
GSWD-CircuIar
■ GSWD-NN 1
----ASWD

ASWD-vanilla
ASWD-vanilla-non-injective
ASWD-non-injective
-1
-2
-3
-4
Iteration
Figure 5: Ablation study on the impact from injective neural networks and the optimization of
hypersurfaces on the ASWD. ASWDs with different mappings are compared to GSWDs with different
defining functions. The first and third columns show target distributions. The second and fourth
columns plot log 2-Wasserstein distances between the target distributions and the source distributions.
In the second and fourth columns, the horizontal axis shows the number of training iterations. Solid
lines and shaded areas represent the average values and 95% confidence intervals of log 2-Wasserstein
distances over 50 runs.
24
Published as a conference paper at ICLR 2022
SWD	- - - GSWD-CircuIar	ASWD (λ = 0.01)	ASWD (Λ = 0.1)
GSWD-PoIy 3	—— DSWD	ASWD (λ = 0.05)	—— ASWD (λ = 0.5)
8 Gaussian __________________________ _______KnOt_____ _______________________
o	ι
25 GaUSS沽门
ɪ'...............
0 ∙	∙	∙	∙	•∙
O 500 IOOO 1500 2000
Iteration
-1	0	1	0	500	1000 1500 2000	-1	0	1	0	500	1000 1500 2000
Iteration	Iteration
Figure 6: Ablation study on the impact of the regularization coefficient λ. The performance of the
ASWDs with different values ofλ are compared with other slice-based Wasserstein metrics. The first
and third columns show target distributions. The second and fourth columns plot log 2-Wasserstein
distances between the target distributions and the source distributions. In the second and fourth columns,
the horizontal axis shows the number of training iterations. Solid lines and shaded areas represent the
average values and 95% confidence intervals of log 2-Wasserstein distances over 50 runs.
25
Published as a conference paper at ICLR 2022
-- ASWD (A = IOO) - ASWD (λ = 10)	- ASWDa = O.1)	……，SWD
-1 O	1 O 500 IOOO 1500 2000	-1 O	1 O 500 IOOO 1500 2000
Iteration	Iteration
Figure 7: Ablation study on the impact from large values of λ. The performance of the ASWDs with
large values ofλ, e.g 10 and 100, are compared with the SWD. The first and third columns show target
distributions. The second and fourth columns plot log 2-Wasserstein distances between the target
distributions and the source distributions. In the second and fourth columns, the horizontal axis shows
the number of training iterations. Solid lines and shaded areas represent the average values and 95%
confidence intervals of log 2-Wasserstein distances over 50 runs.
26
Published as a conference paper at ICLR 2022
Max-GSWD-NN 1 Reg
——DSWD
GSWD-CircuIar
SWD
—ASWD
Max-GSWD-NN 1
i⅜z∕14iow8
12 3
- - _
* ∙ ∙
lh>lh
-1 O	1 O 500 IOOO 1500 2000	-1 O 1 O 500 IOOO 1500 2000
Iteration	Iteration
Figure 8: Ablation study on the impact from the regularization term on the performance of the Max-
GSW-NN. The first and third columns show target distributions. The second and fourth columns plot
log 2-Wasserstein distances between the target distributions and the source distributions. In the second
and fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.
27
Published as a conference paper at ICLR 2022
SWD	-' GSWD-CircuIar	-- GSWD-NN 1	ASWD (with planar flow)
GSWD-PoIy 3	—— DSWD	—— ASWD —— ASWD (with radial flow)
8 GaUSSian __________________________ _______Knot_____ ________________________
500	1000 1500 2000
Iteration
-10	1
Hedrt
O 500 IOOO 1500 2000
Iteration
0	500 IOOO 1500 2000
Iteration
Figure 9: Ablation study on the impact from the choice of injective networks. The performance of the
ASWDs with different types of injective networks are compared with other slice-based Wasserstein
metrics. The first and third columns show target distributions. The second and fourth columns plot log
2-Wasserstein distances between the target distributions and the source distributions. In the second and
fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.
28
Published as a conference paper at ICLR 2022
---- ASWD (dθ = 5*d) ----- ASWD (⅛ = 3*d)	--- DSWD
—— ASWD (dθ = 4*d) ------- ASWD (c∕θ = 2*d)	—— GSWD-CircuIar
"GSWD-PoIy 3
"SWD .
O 500 IOOO 1500 2000
Iteration
-1 O	1 O 500 IOOO 1500 2000	-1 O 1 O 500 IOOO 1500 2000
Iteration	Iteration
Figure 10: Ablation study on the choice of the dimensionality dθ of the augmented space. The perfor-
mance of the ASWDs with different choices ofdθ are compared with other slice-based Wasserstein
metrics. The first and third columns show target distributions. The second and fourth columns plot log
2-Wasserstein distances between the target distributions and the source distributions. In the second and
fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shaded
areas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50
runs.
29
Published as a conference paper at ICLR 2022
Appendix H Additional
RESULTS IN THE GENERATIVE MODELING EXPERIMENT
H.1 Samples of generated images of CIFAR 1 0 and CelebA datasets
(a) CelebA (L = 10)
(b) CelebA (L = 100)
(d) CIFAR10 (L= 10)
(e) CIFAR10 (L = 100)
(c) CelebA (L= 1000)
(f) CIFAR10 (L = 1000)
Figure 11: Visualized experimental results of the ASWD on CelebA and CIFAR10 dataset with 10,
100, 1000 projections. The first row shows randomly selected samples of generated CelebA images,
the second row shows randomly selected samples of generated CIFAR10 images.
H.2 Experiment results on MNIST dataset
In the generative modelling experiment on the MNIST dataset, we train a generator by minimizing
different slice-based Wasserstein metrics, including the ASWD, the DSWD, the GSWD (circular),
and the SWD. Denote by GΦ the generator, the training objective of the experiment can be formulated
as (Bernton et al., 2019):
min Ex 〜Pr ,z 〜pz [SWD(x,Gφ(z))],	(54)
where pz and pr are the prior of latent variable z and the real data distribution, respectively. In other
words, the SWD, or other slice-based Wasserstein metrics, can be considered as a discriminator in this
framework. By replacing the SWD with the ASWD, the DSWD, and the GSWD, we compare the perfor-
mance of learned generative models trained with different metrics. In this experiment, different methods
are compared using different number of projections L= {10,1000}. The 2-Wasserstein distance and
the SWD between generated images and real images are used as metrics for evaluating performances of
different generative models. The experiment results are presented in Figure 12. Quality of generated
images and convergence rate: It can be observed from Figure 12 that the ASWD outperforms all the
other methods regarding both the 2-Wasserstein distance and the SWD between generated and real
images. In particular, the generative model trained with the ASWD produces smaller 2-Wasserstein
distances within less iteration, which implies the generated images are of higher quality and the ASWD
leads to higher convergence rates of generative models. In addition, the ASWD shows that it is able to
generate higher quality images than the SWD and the GSWD with 1000 projections using only as less
30
Published as a conference paper at ICLR 2022
(b)
IOOOO
Iteration
Figure 12: Visualized experimental results of different slice-based Wasserstein metrics on the MNIST
dataset with 10, 1000 projections. (a) Comparison between the SWD, the GSWD, the DSWD, and the
ASWD using the 2-Wasserstein distance between fake and real images as the evaluation metric. (b)
Comparison between the SWD, the GSWD, the DSWD, and the ASWD using the SWD between fake
and real images as the evaluation metric.
---DSWD-IO
---DSWD-1000
---GSWD-IO
---GSWd-IOOO
SWD-IO
---SWD-IOOO
---ASWD-IO
ASWd-IOOO
as 10 projections. In other words, the ASWD has higher projection efficiency than the other slice-based
Wasserstein metrics. The ASWD also has the smallest SWD distance as shown in Figure 12. Although
the SWD converges slightly faster than the ASWD in terms of the SWD between fake and real images,
this is due to the training objective and the evaluated metric are the same for the SWD. Randomly
selected images generated by different slice-based Wasserstein metrics are presented in Figure 13.
Computation cost of the ASWD: The execution time per mini-batch (512 samples) of different
methods are compared in Figure 14a. We evaluate the SWD by varying the number of projections
in the set {10, 1000, 10000} and all the other methods in the set {10, 1000}. We found that although
the SWD requires much fewer computational time than the DSWD and the ASWD, the quality of
generated data is poor even when the number of projections L increases to 10000. The GSWD is
also computationally efficient when using a 10 projections, but it requires the highest execution time
and generates the highest 2-Wasserstein distance among all compared methods when the number
of projections increases to 1000. The huge difference in the execution time of the GSWD with 10
and 1000 projections is due to the GSWD needs to calculate distance matrices of shape N × L, where
N and L are the number of samples and projections respectively, which is more computationally
expensive than calculating inner products when the number of projections L increases. The DSWD
requires a similar computational time as the ASWD in this example, while the ASWD generates higher
quality images in terms of 2-Wasserstein distances.
Besides, we have also evaluated the effect of batch size on the computation cost of different slice-based
Wasserstein metrics. Due to the out-of-memory error caused by the excessively high computation cost
of the GSWDs in high-dimensional space, the GSWD is not included in this comparison. Specifically,
the ASWD, the DSWD, and the SWD are compared in this experiment, and the computation time
of the evaluated methods with L = 10, 1000 projections and N = {213,214,215,216} samples are
reported in Figure 14b. From the results presented in Figure 14b, it can be observed that, similar to the
computational complexity of the DSWD and the SWD, the computational complexity of the ASWD
empirically tends to scale in O(N logN).
31
Published as a conference paper at ICLR 2022
ASWD-10
SWD-10
A 夕 9/47OO
GSWD-10
DSWD-10
SNb t rð S
Q 1 7£2弓总受
Y Gd?彳7"G
7 lgT 5。彳9
q 3 / £ 3 Z
二斗qq%c6/
日3珞号O/S3
。G 6 7 g 6 q j
ASWD-1000
SWD-1000
O/G@3000
/ / f 5 ¾ 9 1 3
夕8”4S437
a / B / S 0 O W
g37Gq“。夕
⅛ ? ♦ ⅜ e 7 / 3
g瞒a o q I…
F I O 3 √ £ a
GSWD-1000
DSWD-1000
Figure 14: (a) The execution time of different methods and the 2-Wasserstein distance between the real
images and the fake images generated by their corresponding models. Each dot of the curve of SWD
corresponds to the performance of the SWD with the number of projections L = {10,1000,10000},
in sequence. Each dot of the other curves correspond to the performance of the other methods with
the number of projections L= {10,1000}, in sequence. (b) Computation cost of calculating different
metrics, the horizontal axis is the number of samples from the compared distributions, and the horizontal
axis is the averaged time consumption for calculating the distances. It can be found that the evaluated
metrics tend to scale in O(NlogN).
Figure 13: Randomly selected samples generated by different metrics, 10 and 1000 refer to the number
of projections.
0 2 4 6 8 0
2 - - - - 1
2 2 2 2
(PUou①S)①IU-I Uo-Ielndluoo 2
(b)
32
Published as a conference paper at ICLR 2022
Appendix I Sliced Wasserstein autoencoders
We train an autoencoder using the framework proposed in (Kolouri et al., 2019b), where an encoder
and a decoder are jointly trained by minimizing the following objective:
min BCE(ψ(φ(x)),x)+L1(ψ(φ(x)),x)+SWD(pz,φ(x)),	(55)
φ,ψ
where φ is the encoder, ψ is the decoder, Pz is the prior distribution of latent variable, BCE(∙,∙) is
the binary cross entropy loss between reconstructed images and real images, and L1 (∙, ∙) is the L1
loss between reconstructed images and real images. We train this model using different slice-based
Wasserstein metrics, including the ASWD, the DSWD, the SWD, and the GSWD. Here we use the
ring distribution as the prior distribution as shown in Figure 16. We report the binary cross entropy
loss during test time and the 2-Wasserstein distance between prior and the encoded latent variable
φ(x) in Figure 15. The slice-based Wasserstein metrics used as the third term in Equation (55) is also
recorded at each iteration and presented in Figure 15 in order to analyze the factors that causes the
differences in the performance of models trained with different slice-based Wasserstein metrics.
It can be observed from the first two columns of Figure 15 that while the model trained with the
ASWD, SWD, and DSWD lead to similar 2-Wassertein distance between the encoded latent variable
distribution and the prior distribution, which implies that they have similar coverage of the prior
distribution as shown in Figure 16, the model trained with the ASWD converges slightly faster to
smaller binary cross entropy loss than the others. As shown in Figure 15(b) and 15(c), since the obtained
GSWDs are trivially small compared with the other metrics, models trained with GSWDs only focuses
on the reconstruction loss and therefore produce reconstructed images of higher qualities in terms of the
binary cross entropy. However, although models trained with the GSWD polynomial and the GSWD
circular lead to smaller binary cross entropy loss than the ASWD, their latent distributions present
very different data structures from the specified prior distribution, which can be problematic in certain
applications where the support of the latent distribution is required to be within a particular range.
Some MNIST images randomly generated by SWAEs trained with different metrics are given in Figure
17.
---DSWD
Figure 15: Convergence behavior of SWAEs trained with different slice-based Wasserstein metrics. (a)
The 2-Wasserstein distance between the prior distribution pz and the distribution of encoded feature
φ(x). (b) The binary cross entropy loss between the reconstruction and real data. (c) The slice-based
Wasserstein metric used to train the model.
---DSWD
---SWD
—GSWD-PoIynomiaI 3
---GSWD-CircuIar
---ASWD
---GSWD-PoIynomiaI 3
---GSWD-CircuIar
---ASWD
33
Published as a conference paper at ICLR 2022
(a) ASWD latent space
(b) DSWD latent space
(C) SWD latent space
(d) GSWD (poly 3) latent space
(e) GSWD (circular) latent
1.00-
0.75-
0.50-
0.25-
0.00-
-0.50 -
-1.00	-0.75	-0.50	-0.25	0.00	0.25	0.50	0.75	1.00
(f) Prior distribution
Figure 16:	Comparisons between the encoded latent space generated by different slice-based Wasser-
stein metrics.
(a) ASWD samples
3 O A 7 ¥ A ʃ
l112 q q 3 O
lΓ夕d>2 Z g
Fq SZ 夕 O 6
q 夕 Zel , 2x
3 7 y σo 7 Oo z*
J夕 3 7 O 8 S
9 O/ X O 7 0 0
(c) SWD samples
(b) DSWD SamPles
(d) GSWD (poly 3) samples
3 G 4 夕 3 4 g
q o∙* 夕 G 7 夕 q
。夕 / 2 I 7 3
。夕夕92 OQ
?夕CPlgg 4
F 7z∙ Qf 0< S O
σo λh 6 7 ? 3 7
8 9 q 3 g 3 S
6qZ073夕7
ORXSb)Oa
«6。夕 2"。
U 4 ) S 2 ♦ I 6
/3 7夕。N2S
oaq I q " 1
2。6 q，0 55
(e) GSWD (circular) samples
(f) MNIST samples
Figure 17:	MNIST images randomly generated by SWAEs trained with different metrics.
34
Published as a conference paper at ICLR 2022
Appendix J	Color transferring
Color transferring can be formulated as an optimal transport problem (Bonneel et al., 2015; Radon,
1917). In this task, the color palette of a source image is transferred to that of a target image, while
keeping the content of source image unchanged. To achieve this, the optimal transport can be used
to find the alignment between image pixels by calculating the optimal mapping of color palettes. In
this experiment, instead of solving the optimal mapping in the original space, we first project the
distribution onto one-dimensional spaces and average the alignment between one-dimensional samples
as an approximation of the optimal mapping in the original space. After obtaining the approximation,
we replace pixels of the source image with the averaged corresponding pixels in the target image.
To reduce the computational cost, we utilize the approach proposed in (Muzellec and Cuturi, 2019),
where the K-means algorithm is used to cluster the pixels of both source and target images, and then
we implement color transfer for the quantized images whose pixels are consist of the centers of 3000
clusters rather than the original source and target images.
We present the results of color transferring in Figure 18. It can be observed that the ASWD and the
DSWD produce sharper images than the SWD and the GSWD (polynomial), we conjecture that is
because the ASWD and the DSWD can generate better alignment of pixels. The GSWD (circular)
tends to generate high contrast images, but sometimes produces images with low color diversity as
in Figure 18(a).The Max-SWD has the highest contrast among all methods, but this is due to it only
uses a single projection to obtain the transport mapping, thus there is no need to average different
pixels from the target image. A disadvantage of the Max-SWD is that the transferred images generated
by Max-SWD is not smooth enough and do not look realistic. The ASWD can generate smooth and
realistic images than the SWD and the Max-SWD, even when the number of projections is as small
as 10. Transferred images obtained by transferring colors from the source to target using standard
optimal transport maps is also presented in Figure 18 for reference (Ferradans et al., 2014).
35
Published as a conference paper at ICLR 2022
(a)
(d)
Figure 18: Top rows are source images and target images, lower rows show transferred im-
ages obtained by using different methods with different number of projections. Source and
target images are from (Bonneel et al., 2015) and https://github.com/chia56028/
Color-Transfer-between-Images.
Published as a conference paper at ICLR 2022
Appendix K	Sliced Wasserstein barycenter
Sliced Wasserstein distances can also be applied in the barycenter calculation and shape inter-
polation (Bonneel et al., 2015). Here we compare barycenters produced by different slice-based
Wasserstein metrics, including the GSWD (circular and polynomial), the ASWD, the SWD, and the
DSWD. Specifically, we compute barycenters of different shapes consisting of point clouds, as shown
in Figure 19. Each object in Figure 19 corresponds to a specific barycenter with different weights.
The Wasserstein barycenters are also presented in Figure 19 for reference, which provide geometrically
meaningful barycenters at the expense of significantly higher computational cost .
Formally, a Sliced-WaSSerStein barycenter of objects μ = {μ1 ,μ2,…,μN ∈ Pk(Rd)} assigned With
weights W = [w1,w2,…,WN ∈ R] is defined as:
N
Bar(μ,w)= argmin £wiSWD(μ,μi).	(56)
μ∈Pk (Rd) i=1
In this experiment, we set N = 3 and compute barycenters corresponding to different weights. The
results are given in Figure 19.
From Figure 19, it can be observed that the ASWD produces similar barycenters as that of the SWD,
which are sharper than the DSWD, and more meaningful than the GSWD (polynomial). The flexibility
of the injective neural networks g(∙) and its optimization in the ASWD can be potentially combined
with specific requirements in particular tasks to generate calibrated barycenters - we leave this as a
future research direction.
(a) ASWD barycenters (b) GSWD (circular) barycenters
(c) GSWD (polynomial) barycenters
(d) DSWD barycenters	(e) SWD barycenters
Figure 19: Sliced Wasserstein barycenters generated by the ASWD, the GSWD, the DSWD, and the
SWD, and the Wasserstein barycenter.
(f) Wasserstein barycenter
37