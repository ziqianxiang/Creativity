Published as a conference paper at ICLR 2022
A Neural Tangent Kernel Perspective of
Infinite Tree Ensembles
Ryuichi Kanoh1,2, Mahito Sugiyama1,2
1National Institute of Informatics
2The Graduate University for Advanced Studies, SOKENDAI
{kanoh, mahito}@nii.ac.jp
Ab stract
In practical situations, the tree ensemble is one of the most popular models along
with neural networks. A soft tree is a variant of a decision tree. Instead of using a
greedy method for searching splitting rules, the soft tree is trained using a gradient
method in which the entire splitting operation is formulated in a differentiable form.
Although ensembles of such soft trees have been used increasingly in recent years,
little theoretical work has been done to understand their behavior. By considering
an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural
Tangent Kernel (TNTK), which provides new insights into the behavior of the
infinite ensemble of soft trees. Using the TNTK, we theoretically identify several
non-trivial properties, such as global convergence of the training, the equivalence
of the oblivious tree structure, and the degeneracy of the TNTK induced by the
deepening of the trees.
1 Introduction
Tree ensembles and neural networks are powerful machine learning models that are used in various
real-world applications. A soft tree ensemble is one variant of tree ensemble models that inherits
characteristics of neural networks. Instead of using a greedy method (Quinlan, 1986; Breiman et al.,
1984) to search splitting rules, the soft tree makes the splitting rules soft and updates the entire model’s
parameters simultaneously using the gradient method. Soft tree ensemble models are known to have
high empirical performance (Kontschieder et al., 2015; Popov et al., 2020; Hazimeh et al., 2020),
especially for tabular datasets. Apart from accuracy, there are many reasons why one should formulate
trees in a soft manner. For example, unlike hard decision trees, soft tree models can be updated
sequentially (Ke et al., 2019) and trained in combination with pre-training (Arik & Pfister, 2019),
resulting in characteristics that are favorable in terms of real-world continuous service deployment.
Their model interpretability, induced by the hierarchical splitting structure, has also attracted much
attention (Frosst & Hinton, 2017; Wan et al., 2021; Tanno et al., 2019). In addition, the idea of the
soft tree is implicitly used in many different places; for example, the process of allocating data to the
appropriate leaves can be interpreted as a special case of Mixture-of-Experts (Jordan & Jacobs, 1993;
Shazeer et al., 2017; Lepikhin et al., 2021), a technique for balancing computational complexity and
prediction performance.
Although various techniques have been proposed to train trees, the theoretical validity of such
techniques is not well understood at sufficient depth. Examples of the practical technique include
constraints on individual trees using parameter sharing (Popov et al., 2020), adjusting the hardness
of the splitting operation (Frosst & Hinton, 2017; Hazimeh et al., 2020), and the use of overparame-
terization (Belkin et al., 2019; Karthikeyan et al., 2021). To better understand the training of tree
ensemble models, we focus on the Neural Tangent Kernel (NTK) (Jacot et al., 2018), a powerful tool
that has been successfully applied to various neural network models with infinite hidden layer nodes.
Every model architecture is known to produce a distinct NTK. Not only for the multi-layer perceptron
(MLP), many studies have been conducted across various models, such as for Convolutional Neural
Networks (CNTK) (Arora et al., 2019; Li et al., 2019), Graph Neural Networks (GNTK) (Du et al.,
2019b), and Recurrent Neural Networks (RNTK) (Alemohammad et al., 2021). Although a number
of findings have been obtained using the NTK, they are mainly for typical neural networks, and it is
still not clear how to apply the NTK theory to the tree models.
1
Published as a conference paper at ICLR 2022
Figure 1: Schematics of an ensemble of M soft trees. Tree internal nodes are indexed according to
the breadth-first ordering.
In this paper, by considering the limit of infinitely many trees, we introduce and study the neural
tangent kernel for tree ensembles, called the Tree Neural Tangent Kernel (TNTK), which provides
new insights into the behavior of the ensemble of soft trees. The goal of this research is to derive
the kernel that characterizes the training behavior of soft tree ensembles, and to obtain theoretical
support for the empirical techniques. Our contributions are summarized as follows:
•	First extension of the NTK concept to the tree ensemble models. We derive the analytical form
for the TNTK at initialization induced by infinitely many perfect binary trees with arbitrary depth
(Section 4.1.1). We also prove that the TNTK remains constant during the training of infinite soft
trees, which allows us to analyze the behavior by kernel regression and discuss global convergence
of training using the positive definiteness of the TNTK (Section 4.1.2, 4.1.3).
•	Equivalence of the oblivious tree ensemble models. We show the TNTK induced by the oblivious
tree structure used in practical open-source libraries such as CatBoost (Prokhorenkova et al., 2018)
and NODE (Popov et al., 2020) converges to the same TNTK induced by a non-oblivious one in
the limit of infinite trees. This observation implicitly supports the good empirical performance of
oblivious trees with parameter sharing between tree nodes (Section 4.2.1).
•	Nonlinearity by adjusting the tree splitting operation. Practically, various functions have been
proposed to represent the tree splitting operation. The most basic function is sigmoid. We show
that the TNTK is almost a linear kernel in the basic case, and when we adjust the splitting function
hard, the TNTK becomes nonlinear (Section 4.2.2).
•	Degeneracy of the TNTK with deep trees. The TNTK associated with deep trees exhibits
degeneracy: the TNTK values are almost identical for deep trees even if the inner products of
inputs are different. As a result, poor performance in numerical experiments is observed with the
TNTK induced by infinitely many deep trees. This result supports the fact that the depth of trees is
usually not so large in practical situations (Section 4.2.3).
•	Comparison to the NTK induced by the MLP. We investigate the generalization performance of
infinite tree ensembles by kernel regression with the TNTK on 90 real-world datasets. Although
the MLP with infinite width has better prediction accuracy on average, the infinite tree ensemble
performs better than the infinite width MLP in more than 30 percent of the datasets. We also
showed that the TNTK is superior to the MLP-induced NTK in computational speed (Section 5).
2	Background and related work
Our main focus in this paper is the soft tree and the neural tangent kernel. We briefly introduce and
review them.
2.1	Soft tree
Based on Kontschieder et al. (2015), we formulate a regression by soft trees. Figure 1 is a schematic
image of an ensemble of M soft trees. We define a data matrix x ∈ RF ×N for N training samples
2
Published as a conference paper at ICLR 2022
{x1, . . . , xN} with F features and define tree-wise parameter matrices for internal nodes wm ∈
RF×N and leaf nodes πm ∈ R1×L for each tree m ∈ [M] = {1, . . . , M} as
|	...	|	|	...	|
x = x1 . . . xN , wm =	wm,1 . . . wm,N	, πm = (πm,1 , . . . , πm,L) ,
|	...	|	|	...	|
where internal nodes (blue nodes in Figure 1) and leaf nodes (green nodes in Figure 1) are indexed
from 1 to N and 1 to L, respectively. N and L may change across trees in general, while we assume
that they are always fixed for simplicity throughout the paper. We also write horizontal concatenation
of (column) vectors as x = (x1, . . . , xN) ∈ RF×N and wm = (wm,1, . . . , wm,N) ∈ RF×N.
Unlike hard decision trees, we consider a model in which every single leaf node ` ∈ [L] = {1, . . . , L}
of a tree m holds the probability that data will reach to it. Therefore, the splitting operation at an
intermediate node n ∈ [N] = {1, . . . ,N} does not definitively decide splitting to the left or right.
To provide an explicit form of the probabilistic tree splitting operation, we introduce the following
binary relations that depend on the tree’s structure: ` . n (resp. n & `), which is true if a leaf
` belongs to the left (resp. right) subtree of a node n and false otherwise. We can now exploit
μm,'(xi, Wm) : RF X RF×N → [0,1], a function that returns the probability that a sample Xi will
reach a leaf ` of the tree m, as follows:
N
μm,'(xi, Wm) = ɪ ɪ gm,n(xi, wm,n) '.n (I -
n=1
gm,n (xi , Wm,n)) n&
(1)
where 1Q is an indicator function conditioned on the argument Q, i.e., 1true = 1 and 1false = 0, and
gm,n : RF × RF → [0, 1] is a decision function at each internal node n of a tree m. To approximate
decision tree splitting, the output of the decision function gm,n should be between 0.0 and 1.0. If
the output of a decision function takes only 0.0 or 1.0, the splitting operation is equivalent to hard
splitting used in typical decision trees. We will define an explicit form of the decision function gm,n
in Equation (5) in the next section.
The prediction for each xi from a tree m with nodes parameterized by Wm and πm is given by
fm(xi, W
m, πm) =)： πm,'μm,'(xi, Wm),	(2)
'=1
where fm : RF × RF ×N × R1×L → R, and ∏m,' denotes the response of a leaf ' of the tree τm.
This formulation means that the prediction output is the average of the leaf values πm,' weighted by
μm,'(xi, Wm), probability of assigning the sample Xi to the leaf '. If μm,,'(xi, Wm) takes only 1.0
for one leaf and 0.0 for the other leaves, the behavior is equivalent to a typical decision tree prediction.
In this model, Wm and πm are updated during training with a gradient method.
While many empirical successes have been reported, theoretical analysis for soft tree ensemble
models has not been sufficiently developed.
2.2 Neural tangent kernel
Given N samples X ∈ RF ×N, the NTK induced by any model architecture at a training time τ is
formulated as a matrix HT ∈ RN×N, in which each (i,j) ∈ [N] × [N] component is defined as
闻 ij=θ T (M Xj ):= (f∂xθL
dZarbitrary (xj, θτ)
∂θT
(3)
where〈•, •)denotes the inner product and θτ ∈ RP is a concatenated vector of all the P trainable
model parameters at T. An asterisk “ * " indicates that the model is arbitrary. The model func-
tion farbitrary : RF × RP → R used in Equation (3) is expected to be applicable to a variety of
model structures. For the soft tree ensembles introduced in Section 2.1, the NTK is formulated as
PM PN D
m=1 n=1
∂f(xi ,w,∏)
∂Wm,n
∂f (Xj,W,∏) ∖ PM PL ∂ ∂f (Xi,W,∏)
∂Wm,n + + mm=l 乙'=1 ∖	∂∏m,'
df(Xj ,w,n)
dπm,'
.
Within the limit of infinite width with a proper parameter scaling, a variety of properties have
been discovered from the NTK induced by the MLP. For example, Jacot et al. (2018) showed the
convergence of Θb 0MLP (Xi, Xj ), which can vary with respect to parameters, to the unique limiting
3
Published as a conference paper at ICLR 2022
kernel ΘMLP (xi, xj) at initialization in probability. Moreover, they also showed that the limiting
kernel does not change during training in probability:
lim
width→∞
ΘbτMLP(xi,xj)
lim Θb0MLP(xi, xj) = ΘMLP(xi, xj).
width→∞
(4)
This property helps in the analytical understanding of the model behavior. For example, with the
squared loss and infinitesimal step size with learning rate η, the training dynamics of gradient flow in
function space coincides with kernel ridge-less regression with the limiting NTK. Such a property
gives us a data-dependent generalization bound (Bartlett & Mendelson, 2003) related to the NTK
and the prediction targets. In addition, if the NTK is positive definite, the training can achieve global
convergence (Du et al., 2019a; Jacot et al., 2018).
Although a number of findings have been obtained using the NTK, they are mainly for typical neural
networks such as MLP and ResNet (He et al., 2016), and the NTK theory has not yet been applied
to tree models. The NTK theory is often used in the context of overparameterization, which is a
subject of interest not only for the neural networks, but also for the tree models (Belkin et al., 2019;
Karthikeyan et al., 2021; Tang et al., 2018).
3 Setup
We train model parameters w and π to minimize the squared
loss using the gradient method, where w = (w1, . . . , wM)
and π = (π1 , . . . , πM). The tree structure is fixed during
training. In order to use a known closed-form solution of the
NTK (Williams, 1996; Lee et al., 2019), we use a scaled error
function σ : R → (0, 1), resulting in the following decision
function:
gm,n(xi, wm,n ) = σ (wm,nXi)
= 1erf(αwm,nxi) +2 ,	4 (5)
Figure 2: The scaled error function. We
draw 50 lines with varying α by 0.25.
A dotted magenta line shows a sigmoid
function, which is close to a scaled error
function with a 〜0.5.
where erf(p) = √2∏ Rp e-t2 dt for P ∈ R. This scaled error
function approximates a commonly used sigmoid function.
Since the bias term for the input of σ can be expressed inside
of w by adding an element that takes a fixed constant value
for all input of the soft trees x, we do not consider the bias
for simplicity. The scaling factor α is introduced by Frosst
& Hinton (2017) to avoid overly soft splitting. Figure 2 shows that the decision function becomes
harder as α increases (from blue to red), and in the limit α → ∞ it coincides with the hard splitting
used in typical decision trees.
When aggregating the output of multiple trees, we divide the sum of the tree outputs by the square
root of the number of trees
1M
f (Xi, w, π) = √_ Σ fm(Xi,w
m, πm ).
m=1
(6)
This 1 / √M scaling is known to be essential in the existing NTK literature to use the weak law of the
large numbers (Jacot et al., 2018). On top of Equation (6), we initialize each of model parameters
Wm,n and ∏m,' with zero-mean i.i.d. Gaussians with unit variances. We refer such a parameterization
as NTK initialization. In this paper, we consider a model such that all M trees have the same perfect
binary tree structure, a common setting for soft tree ensembles (Popov et al., 2020; Kontschieder
et al., 2015; Hazimeh et al., 2020).
4 Theoretical results
4.1 Basic properties of the TNTK
The NTK in Equation (3) induced by the soft tree ensembles is referred to here as the TNTK and
denoted by Θb(0d)(Xi, Xj) the TNTK at initialization induced by the ensemble of trees with depth d.
4
Published as a conference paper at ICLR 2022
Change a with fixed d=3
IoT
Inner product of the inputs
L	CT■ 「， a	∙ ∙ Ii	,	「K /	∖ , .λ r~ ι ι ∙ zʌ /	∖
Figure 3: Left: An empirical demonstration of convergence of Θ0(xi, xj) to the fixed limit Θ(xi, xj)
as M increases. Two simple inputs are considered: xi = {1, 0} and xj = {cos(β), sin(β)}
with β = [0, π]. The TNTK Θb (03) (xi, xj) with α = 2.0 is calculated 10 times with parameter
re-initialization for each of the M = 16, 64, 256, 1024, and 4096. Center and Right: Parameter
dependency of the convergence. The vertical axis corresponds to the averaged error between the
H0 and the H := limM→∞ H0 for 50 random unit vectors of length F = 5. The dashed lines are
plotted only for showing the slope. The error bars show the standard deviations of 10 executions.
In this section, we show the properties of the TNTK that are important for understanding the training
behavior of the soft tree ensembles.
4.1.1 TNTK for infinite tree ensembles
First, we show the formula of the TNTK at initialization, which converges when considering the limit
of infinite trees (M → ∞).
Theorem 1.	Let u ∈ RF be any column vector sampled from zero-mean i.i.d. Gaussians with unit
variance. The TNTK for an ensemble of soft perfect binary trees with tree depth d converges in
probability to the following deterministic kernel as M → ∞,
Θ(d) (xi, xj) := lim Θb (0d) (xi, xj)
M→∞	0
=2dd ∑(xi, Xj)(T(xi, Xj))d-1T(xi, Xj)+ (2T(xi, Xj))d ,	(7)
}|
}
'----------------------------{----------------
contribution from inner nodes
''^^^^^^{^^^^^^^
contribution from leaves
where	Σ(xi,	Xj)	:=	x>Xj,	T(xi, Xj)	：=	E[σ(u>Xi)σ(u>xj∙)],	and T(xi,	Xj)	:=
E[σ(u>Xi)σ(u>xj∙)]. Moreover, T(Xi, Xj) and T(Xi, Xj) are analytically obtained in the closed-
form as
T(Xi , Xj )
ɪ arcsin (	a2,(Xi, Xj)	)+1
2π	(α2Σ(Xi, Xi) + 0.5)(α2Σ(Xj, Xj) + 0.5)	4
T(Xi , Xj )
α2
π Vz(1 + 2α2Σ(Xi, Xi))(I + 2α2Σ(Xj, Xj))-4α4Σ(X), Xj)2
(8)
(9)
1
The dot used in σ(u>Xi) means the first derivative: αe-(αuτxi)2/√π, and E[∙] means the expecta-
tion. The scalar π in Equation (8) and Equation (9) is the circular constant, and u corresponds to
wm,n at an arbitrary internal node. The proof is given by induction. We can derive the formula of the
limiting TNTK by treating the number of trees in a tree ensemble like the width of the hidden layer
in MLP, although the MLP and the soft tree ensemble are apparently different models. Due to space
limitations, detailed proofs are given in the supplementary material.
We demonstrate convergence of the TNTK in Figure 3. We empirically observe that the TNTK
induced by sufficiently many soft trees converges to the limiting TNTK given in Equation (7). The
kernel values induced by an finite ensemble are already close to the limiting TNTK if the number
of trees is larger than several hundreds, which is a typical order of the number of trees in practical
applications1. Therefore, it is reasonable to analyze soft tree ensembles via the TNTK.
1For example, Popov et al. (2020) uses 2048 trees.
5
Published as a conference paper at ICLR 2022
By comparing Equation (7) and the limiting NTK induced by a two-layer perceptron (shown in the
supplementary material), we can immediately derive the following when the tree depth is 1.
Corollary 1. If the splitting function at the tree internal node is the same as the activation function
of the neural network, the limiting TNTK obtained from a soft tree ensemble of depth 1 is equivalent
to the limiting NTK generated by a two-layer perceptron up to constant multiple.
For any tree depth larger than 1, the limiting NTK induced by the MLP with any number of layers
(Arora et al., 2019, shown in the supplementary material) and the limiting TNTK do not match. This
implies that the hierarchical splitting structure is a distinctive feature of soft tree ensembles.
4.1.2	Positive definitenes s of the limiting TNTK
Since the loss surface of a large model is expected to be highly non-convex, understanding the good
empirical trainability of overparameterized models remains an open problem (Dauphin et al., 2014).
The positive definiteness of the limiting kernel is one of the most important conditions for achieving
global convergence (Du et al., 2019a; Jacot et al., 2018). Jacot et al. (2018) showed that the conditions
kxik2= 1 for all i ∈ [N] and xi 6= xj (i 6= j) are necessary for the positive definiteness of the
NTK induced by the MLP for an input set. As for the TNTK, since the formulation (Equation (7))
is different from that of typical neural networks such as an MLP, it is not clear whether or not the
limiting TNTK is positive definite.
We prove that the TNTK induced by infinite trees is also positive definite under the same condition
for the MLP.
Proposition 1. For infinitely many soft trees with any depth and the NTK initialization, the limiting
TNTK is positive definite if kxi k2= 1 for all i ∈ [N] and xi 6= xj (i 6= j).
The proof is provided in the supplementary material. Similar to the discussion for the MLP (Du et al.,
2019a; Jacot et al., 2018), if the limiting TNTK is constant during training, the positive definiteness of
the limiting TNTK at initialization indicates that training of the infinite trees with a gradient method
can converge to the global minimum. The constantness of the limiting TNTK during training is
shown in the following section.
4.1.3	Change of the TNTK during training
We prove that the TNTK hardly changes from its initial value during training when considering an
ensemble of infinite trees with finite α (used in Equation (5)).
Theorem 2.	Let λmin and λmax be the minimum and maximum eigenvalues of the limiting TNTK.
Assume that the limiting TNTK is positive definite for input sets. For soft tree ensemble models
with the NTK initialization and a positive finite scaling factor α trained under gradient flow with a
learning rate η < 2∕(λmin + λmax), we have, With high probability,
sup ∣ΘTd) (xi, Xj) — Θ0d) (xi, Xj)∣ = O (√M) .	(10)
The complete proof is provided in the supplementary material. Figure 4 shows that the training
trajectory analytically obtained (Jacot et al., 2018; Lee et al., 2019) from the limiting TNTK and
the trajectory during gradient descent training become similar as the number of trees increases,
demonstrating the validity of using the TNTK framework to analyze the training behavior.
Remarks. In the limit of infinitely large α, which corresponds to a hard decision tree splitting
(Figure 2), it should be noted that this theorem does not hold because of the lack of local Lipschitzness
(Lee et al., 2019), which is the fundamental property for this proof. Therefore the change in the
TNTK during training is no longer necessarily asymptotic to zero, even if the number of trees is
infinite. This means that understanding the hard decision tree’s behavior using the TNTK is not
straightforward.
4.2 Implications for practical techniques
In this section, from the viewpoint of the TNTK, we discuss the training techniques that have been
used in practice.
6
Published as a conference paper at ICLR 2022
Figure 4: Output dynamics for train and test data points. The color of each line corresponds to each
data point. Soft tree ensembles with d = 3, α = 2.0 are trained by a full-batch gradient descent
with a learning rate of 0.1. Initial outputs are shifted to zero (Chizat et al., 2019). There are 10
randomly generated training points and 10 randomly generated test data points, and their dimension
F = 5. The prediction targets are also randomly generated. Let H(x, x0) ∈ RN×N0 be the limiting
NTK matrix for two input matrices and I be an identity matrix. For analytical results, we draw the
trajectory f(v, θτ) = H(v, x)H (x, x)-1(I - exp[-ηH(x, x)τ])y (Lee et al., 2019) using the
limiting TNTK (Equation (7)), where v ∈ RF is an arbitrary input and x ∈ RF ×N and y ∈ RN are
the training dataset and the targets, respectively.
Figure 5: Left: Normal Tree, Right: Oblivious Tree. The rules for splitting in the same depth are
shared across the same depth in the oblivious tree, while ∏m,' on leaves can be different.
4.2.1	INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE
An oblivious tree is a practical tree model architecture where the rules across decision tree splitting
are shared across the same depth as illustrated in Figure 5. Since the number of the splitting decision
calculation can be reduced from O(2d) to O(d), the oblivious tree structure is used in various
open-source libraries such as CatBoost (Prokhorenkova et al., 2018) and NODE (Popov et al., 2020).
However, the reason for the good empirical performance of oblivious trees is non-trivial despite
weakening the expressive power due to parameter sharing.
We find that the oblivious tree structure does not change the limiting TNTK from the non-oblivious
one. This happens because, even with parameter sharing at splitting nodes, leaf parameters π are not
shared, resulting in independence between outputs of left and right subtrees.
Theorem 3. The TNTK with the perfect binary tree ensemble and the TNTK of its corresponding
oblivious tree ensemble obtained via parameter sharing converge to the same kernel in probability in
the limit of infinite trees (M → ∞).
The complete proof is in the supplementary material. Note that leaf values π do not have to be the
same for oblivious and non-oblivious trees. This theorem supports the recent success of tree ensemble
models with the oblivious tree structures.
4.2.2	Effect of the decision function modification
Practically, based on the commonly used sigmoid function, a variety of functions have been pro-
posed for the splitting operation. By considering a large scaling factor α in Equation (5), we
can envisage situations in which there are practically used hard functions, such as two-class
sparsemax, σ(x) = sparsemax([x, 0]) (Martins & Astudillo, 2016), and two-class entmax,
σ(x) = entmax([x, 0]) (Peters et al., 2019). Figure 6 shows α dependencies of TNTK param-
eters. Equation (7) means that the TNTK is formulated by multiplying T(xi , xj ) and T(xi , xj ) to
the linear kernel Σ(xi, xj). On the one hand, T (xi, xj) and T (xi, xj) with small α are almost con-
7
Published as a conference paper at ICLR 2022
‰¾)
Figure 6: Parameter dependencies of T(Xi, Xj), T(xi, Xj), and Θ(d)(xi, Xj). The vertical axes are
normalized so that the value is 1 when the inner product of the inputs is 1. The input vector size is
normalized to be one. For the three figures on the left, the line color is determined by α, and for the
figure on the right, it is determined by the depth of the tree.
stant, even for different input inner products, resulting in the almost linear TNTK. On the other hand,
the nonlinearity increases as α increases. For Figure 2, the original sigmoid function corresponds to
a scaled error function for α 〜0.5, which induces almost the linear kernel. Although a closed-form
TNTK using sigmoid as a decision function has not been obtained, its kernel is expected to be almost
linear. Therefore, from the viewpoint of the TNTK, an adjustment of the decision function (Frosst
& Hinton, 2017; Popov et al., 2020; Hazimeh et al., 2020) can be interpreted as an escape from the
linear kernel behavior.
4.2.3	Degeneracy caused by deep trees
As the depth increases, T(Xi, Xj ) defined in Equation (8), which consists of the arcsine function,
is multiplied multiple times to calculate the limiting TNTK in Equation (7). Therefore, when we
increase the depth too much, the resulting TNTK exhibits degeneracy: its output values are almost
the same as each other’s, even though the input’s inner products are different. The rightmost panel
of Figure 6 shows such degeneracy behavior. In terms of kernel regression, models using a kernel
that gives almost the same inner product to all data except those that are quite close to each other
are expected to have poor generalization performance. Such behavior is observed in our numerical
experiments (Section 5). In practical applications, overly deep soft or hard decision trees are not
usually used because overly deep trees show poor performance (Luo et al., 2021), which is supported
by the degeneracy of the TNTK.
5 Numerical experiments
Setup. We present our experimental results on 90 classification tasks in the UCI database (Dua
& Graff, 2017), with fewer than 5000 data points, as in Arora et al. (2020). We performed kernel
regression using the limiting TNTK defined in Equation (7) with varying the tree depth (d) and the
scaling (α) of the decision function. The limiting TNTK does not change during training for an
infinite ensemble of soft trees (Theorem 2); therefore, predictions from that model are equivalent to
kernel regression using the limiting TNTK (Jacot et al., 2018). To consider the ridge-less situation,
regularization strength is set to be 1.0 × 10-8, a very small constant. By way of comparison,
performances of the kernel regression with the MLP-induced NTK (Jacot et al., 2018) and the RBF
kernel are also reported. For the MLP-induced NTK, we use ReLU for the activation function. We
follow the procedures of Arora et al. (2020) and Fernandez-Delgado et al. (2014): We report 4-fold
cross-validation performance with random data splitting. To tune parameters, all available training
samples are randomly split into one training and one validation set, while imposing that each class
has the same number of training and validation samples. Then the parameter with the best validation
accuracy is selected. Other details are provided in the supplementary material.
Comparison to the MLP. The left panel of Figure 7 shows the averaged performance as a function
of the depth. Although the TNTK with properly tuned parameters tend to be better than those obtained
with the RBF kernel, they are often inferior to the MLP-induced NTK. The results support the good
performance of the MLP-induced NTK (Arora et al., 2020). However, it should be noted that when
we look at each dataset one by one, the TNTK is superior to the MLP-induced NTK by more than
8
Published as a conference paper at ICLR 2022
Figure 7: Left: Averaged accuracy over 90 datasets. The performances of the kernel regression with
the MLP-induced NTK and the RBF kernel are shown for comparison. Since the depth is not a hyper-
parameter of the RBF kernel, performance is shown by a horizontal line. The statistical significance is
also assessed in the supplementary material. Right: Running time for kernel computation. The input
dataset has 300 samples with 10 features. Feature values are generated by zero-mean i.i.d Gaussian
with unit variance. The error bars show the standard deviations of 10 executions.
Table 1: Performance win rate against the MLP-induced NTK. We tune the depth from d = 1 to 29
for the dataset-wise comparison for both the TNTK and the MLP-induced NTK. For the RBF kernel,
30 different hyperparameters are tried. Detailed results are in the supplementary material.
TNTK	RBF
α	0.5	1.0	2.0	4.0	8.0	16.0	32.0	64.0	—
Win rate (%)	13.6	18.8	22.2	28.6	32.5	31.6	34.9	27.2	11.8
30 percent of the dataset, as shown in Table 1. This is a case where the characteristics of data and
the inductive bias of the model fit well together. In addition, although the computational cost of
the MLP-induced NTK is linear with respect to the depth of the model because of the recursive
computation (Jacot et al., 2018), the computational cost of the TNTK does not depend on the depth, as
shown in Equation (7). This results in much faster computation than the MLP-induced NTK when the
depth increases, as illustrated in the right panel of Figure 7. Even if the MLP-induced NTK is better
in prediction accuracy, the TNTK may be used in practical cases as a trade-off for computational
complexity. Arora et al. (2019) proposed the use of the NTK for a neural architecture search (Elsken
et al., 2019; Chen et al., 2021). In such applications, the fast computation of the kernel in various
architectures can be a benefit. We leave extensions of this idea to tree models as future work.
Consistency with implications from the TNTK theory. When we increase the tree depth, we
initially observe an improvement in performance, after which the performance gradually decreases.
This behavior is consistent with the performance deterioration due to degeneracy (Section 4.2.3),
similar to that reported for neural networks without skip-connection (Huang et al., 2020), shown by a
dotted yellow line. The performance improvement by adjusting α in the decision function (Frosst
& Hinton, 2017) is also observed. Performances with hard (α > 0.5) decision functions are always
better than the sigmoid-like function (α = 0.5, as shown in Figure 2).
6 Conclusion
In this paper, we have introduced and studied the Tree Neural Tangent Kernel (TNTK) by considering
the ensemble of infinitely many soft trees. The TNTK provides new insights into the behavior of the
infinite ensemble of soft trees, such as the effect of the oblivious tree structure and the degeneracy
of the TNTK induced by the deepening of the trees. In numerical experiments, we have observed
the degeneracy phenomena induced by the deepening of the soft tree model, which is suggested by
our theoretical results. To date, the NTK theory has been mostly applied to neural networks, and
our study is the first to apply it to the tree model. Therefore our study represents a milestone in the
development of the NTK theory.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This work was supported by JSPS KAKENHI (Grant Number JP21H03503d, Japan), JST PRESTO
(Grant Number JPMJPR1855, Japan), and JST FOREST (Grant Number JPMJFR206J, Japan).
Ethics statement
We believe that the TNTK’s theoretical analysis will not have a negative impact on society as our
work does not directly lead to harmful applications.
Reproducibility statement
Proofs of all theoretical results are provided in the supplementary material. For the numerical
experiments and figures, we share our reproducible source code in the supplementary material.
References
Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The Recurrent Neural
Tangent Kernel. In International Conference on Learning Representations, 2021.
Sercan Gmer Arik and Tomas Pfister. TabNet: Attentive Interpretable Tabular Learning. CoRR,
abs/1908.07442, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
Exact Computation with an Infinitely Wide Neural Net. In Advances in Neural Information
Processing Systems, volume 32, 2019.
Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Har-
nessing the Power of Infinitely Wide Deep Nets on Small-data Tasks. In International Conference
on Learning Representations, 2020.
Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and
Structural Results. Journal of Machine Learning Research, 3, 2003.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings ofthe National Academy ofSciences,
116(32), 2019.
Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. Classification and Regression
Trees. Chapman and Hall/CRC, 1984.
Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural Architecture Search on ImageNet in
Four GPU Hours: A Theoretically Inspired Perspective. In International Conference on Learning
Representations, 2021.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Programming.
In Advances in Neural Information Processing Systems, volume 32, 2019.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in Neural Information Processing Systems, volume 27, 2014.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient Descent Finds Global
Minima of Deep Neural Networks. In Proceedings of the 36th International Conference on
Machine Learning, 2019a.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. In
Advances in Neural Information Processing Systems, volume 32, 2019b.
Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL http://archive.
ics.uci.edu/ml.
10
Published as a conference paper at ICLR 2022
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural Architecture Search: A Survey.
Journal of Machine Learning Research, 20(55):1-21, 2019.
Manuel Fernandez-Delgado, Eva Cernadas, Senen Barro, and Dinani Amorim. Do We Need Hundreds
of Classifiers to Solve Real World Classification Problems? Journal of Machine Learning Research,
15, 2014.
Nicholas Frosst and Geoffrey E. Hinton. Distilling a Neural NetWork Into a Soft Decision Tree.
CoRR, 2017.
Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The Tree
Ensemble Layer: Differentiability meets Conditional Computation. In Proceedings of the 37th
International Conference on Machine Learning, volume 119, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2016.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why Do Deep Residual NetWorks
Generalize Better than Deep FeedforWard NetWorks? — A Neural Tangent Kernel Perspective. In
Advances in Neural Information Processing Systems, volume 33, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural NetWorks. In Advances in Neural Information Processing Systems 31.
2018.
M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. In Proceedings
of 1993 International Conference on Neural Networks, volume 2, 1993.
Ajaykrishna Karthikeyan, Naman Jain, Nagarajan Natarajan, and Prateek Jain. Learning Accurate
Decision Trees With Bandit Feedback via Quantized Gradient Descent. CoRR, 2021.
Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. DeepGBM: A Deep Learning
FrameWork Distilled by GBDT for Online Prediction Tasks. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.
Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulb. Deep Neural
Decision Forests. In 2015 IEEE International Conference on Computer Vision, 2015.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural NetWorks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems, volume 32,
2019.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models With Conditional
Computation and Automatic Sharding. In International Conference on Learning Representations,
2021.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced Convolutional Neural Tangent Kernels. CoRR, abs/1911.00809, 2019.
Haoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. SDTR: Soft Decision Tree Regressor for Tabular
Data. IEEE Access, 9, 2021.
Andre Martins and Ramon Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention
and Multi-Label Classification. In Proceedings of The 33rd International Conference on Machine
Learning, volume 48, 2016.
Ben Peters, Vlad Niculae, and Andre F. T. Martins. Sparse Sequence-to-Sequence Models. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.
Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural Oblivious Decision Ensembles for
Deep Learning on Tabular Data. In International Conference on Learning Representations, 2020.
11
Published as a conference paper at ICLR 2022
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey
Gulin. CatBoost: unbiased boosting with categorical features. In Advances in Neural Information
Processing Systems, volume 31, 2018.
J. R. Quinlan. Induction of Decision Trees. Machine Learning, 1(1), 1986.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,
and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer. In International Conference on Learning Representations, 2017.
Cheng Tang, Damien Garreau, and Ulrike von Luxburg. When do random forests fail? In Advances
in Neural Information Processing Systems, volume 31, 2018.
Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori. Adap-
tive Neural Trees. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, 2019.
Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal,
and Joseph E. Gonzalez. NBDT: Neural-Backed Decision Tree. In International Conference on
Learning Representations, 2021.
Christopher K. I. Williams. Computing with Infinite Networks. In Advances in Neural Information
Processing Systems, 1996.
A Proof of Theorem 1
Proof. The model output from a certain depth tree ensemble f (d) can be written alternatively using
an incremental formula as
1M
f (d)(xi, w, π) = √m X k(Wm,txi) fmdτ) (χi, Wm,πmm
m=1
+(1-σ (Wm ,txi)) fm-1 (χi, Wm),πm)
(A.1)
where indices (l) and (r) used with model parameters Wm and πm mean the parameters at the
(l)eft subtree and the (r)ight subtree, respectively, and t used in Wm,t denotes the node at (t)op of
the tree. For example, for trees of depth 3, as shown in Figure 1, Wm(l) = (Wm,2, Wm,4, Wm,5),
Wm = (Wm,3, Wm,6, Wm,7), and Wm,t = Wm,1.
We prove the theorem by induction. When d = 1,
M
f(1)(xi, w,π)
πm,1 + (1 - σ(wm>,1xi))πm,2 .
(A.2)
Derivatives are
∂f (1)(xi, w, ∏) _ ∂Wm,1	=√M (πm,1 - πm,2) xiσ (Wm,1xi),	(A.3)
∂f (1)(xi, W, ∏) _ ∂∏m,1	=√Mσ (Wm ,lx，,	(A.4)
∂f (1)(xi, W, ∏) _ ∂∏m,2	=-√m (1—σ (WmJxi)),	(A.5)
12
Published as a conference paper at ICLR 2022
Therefore, since there is only one internal node per a single tree, from the definition of the NTK, the
TNTK is obtained as
M
Θb (1) (xi , xj ) = X
m=1
∂f (Xi, W, ∏)
∂Wm,1
∂f (Xj, W, ∏)
∂Wm,1
,∂f (Xi, W, ∏) ∂f (Xj , W, ∏) ∖ + ,∂f (Xi, W, π)
∖	∂∏m,ι ,	∂∏m,ι /	∖	∂∏m,2
∂f (Xj , W, ∏)
∂∏m,2
1M
M x ((nm,i - πm,2)2 x> Xjσ(WmjXi)σ(WmjXj)+2σ(Wm ,1Xi)σ(Wm ,1Xj)).
m=1	(A.6)
Since we are considering the infinite number of trees (M → ∞), the average in Equation (A.6) can
be replaced by the expected value by applying the law of the large numbers:
Θ ⑴(Xi, Xj ) = Em ]((∏m,1 - ∏m,2)2 X> Xj 6如工,通)才如工,ιXj )
+ 2σ(Wm>,1Xi)σ(Wm>,1Xj)
=2(∑(Xi, Xj)T(Xi, Xj) + T(Xi, Xj)),	(A.7)
which is consistent with Equation (7). Here, Em [(πm,,ι 一 πm,,2)2] = 2 because the variance of ∏m,'
is 1.0, and Wm,1 corresponds to u in Theorem 1.
For d > 1, we divide the TNTK into four components:
Θ(d) (Xi, Xj) = Θ(d),(t) (Xi, Xj) + Θ(d),(l) (Xi, Xj) + Θ(d),(r) (Xi, Xj) + Θ(d),(b) (Xi, Xj) ,
where the indices (t), (l) and (r) mean the parameters of the (t)op of the tree, (l)eft subtree, and (r)ight
subtree, respectively. The index (b) implies the (b)ottom of the tree: tree leaves. With Equation (A.7),
we have
Θ(1),(t) (Xi,Xj)	2Σ(Xi , Xj )T (Xi , Xj ),	(A.8)
Θ(1),(l) (Xi,Xj)	0,	(A.9)
Θ(1),(r) (Xi,Xj)	0,	(A.10)
Θ(1),(b) (Xi,Xj)	2T (Xi , Xj ).	(A.11)
For each component, we show the following lemmas:
Lemma 1.
Θ(d+1),(t) (Xi, Xj) = 2T(Xi, Xj)Θ(d),(t) (Xi, Xj)	(A.12)
Lemma 2.
Θ(d+1),(l) (Xi, Xj) + Θ(d+1),(r) (Xi, Xj)
= 2T (Xi, Xj)(Θ(d),(t) (Xi, Xj) + Θ(d),(l) (Xi, Xj) + Θ(d),(r) (Xi, Xj))	(A.13)
Lemma 3.
Θ(d+1),(b)(Xi,Xj) = 2T(Xi, Xj)Θ(d),(b) (Xi, Xj)	(A.14)
Combining them, we can derive Equation (7).
□
13
Published as a conference paper at ICLR 2022
A.1 Proof of Lemma 1
Proof. An incremental formula for the model output with a certain depth tree ensemble is
1M
f(d)(xi,w,π) = √m X 卜(wm,txi) fm ) (χi,Wm),πm))
m=1
+(1 -σ (wm,txi)) fmt1 (χi, wm), πm))),	(A.15)
where t used in wm,t implies the node at the top of the tree. With Equation (A.15),
"(aWXζw,π)=χ/(Wm,tXi) (fmd) (xi,Wm,πm)) - fmd (xi,wm),πm))),	(A.16)
1M
θ (+),() (xi, Xj) = M x Xi X。(Wm,tXi)σ(Wm,txj) fmd) (χi, Wm),πm)) fm (Xj, Wm), πm))
m=1
-	fm(d) (Xi,Wm(l),πm(l) fm(d) (Xj, Wm(r), πm(r)
-	fm(d) (Xi, Wm(r), πm(r) fm(d) (Xj, Wm(l), πm(l)
+	fm(d) (Xi, Wm(r), πm(r) fm(d) (Xj, Wm(r), πm(r)	,	(A.17)
Since
and fm(d) Xj , Wm(l) , πm(l)	are independent of each other
and have zero-mean Gaussian distribution because of the initialization of πm
with zero-mean i.i.d	Gaussians2,	E
number of trees (M → ∞),
fm(d) Xi, Wm(l),πm(l) fm(d) Xj, Wm(r), πm(r)	and
are zero. Therefore, considering the infinite
θ(d+1),(t)(Xi, Xj)=婷叼T(Xi, Xj )Em fmd) (m Wm), πm) fmd (Xj, Wm), π(m))
+ fm(d) (Xi, Wm(r), πm(r) fm(d) (Xj, Wm(r), πm(r)i (A.18)
From Equation (A.18), what we need to prove is Em fm(d)(Xi, Wm, πm)fm(d)(Xj, Wm, πm)
(2T (Xi, Xj ))d. We do this by induction. In the base case with d = 1,
Em fm(1)(Xi,W
m,πm)fm(1)(Xj,W
m, πm)
=Em (σ(Wm,1Xi)nm,1 + (1 - σ(Wm,1Xi)) πm,2) (σ(Wm,1 Xj)πm,1 + (1 - σ(Wm,1Xj)) πm,2
=Em (πm,1 - πm,2) σ(Wm,1Xi)。(Wm,1Xj )
|^^^~^^^{^^^™^^—}
→2
+πm,1πm,2(σ(Wm>,1Xi) + σ(Wm>,1Xj)) -πm2 ,2(σ(Wm>,1Xi) +σ(Wm>,1Xj) - 1)
|------{---}	1------------{--------------}
→0	→0
=2T (Xi, Xj) ,	(A.19)
where we use the property
E [σ(v)] = 0.5	(A.20)
for any v generated from a zero-mean Gaussian distribution. The subscript arrows (→) show what
the expected value will be.
2This holds because the model output is a weighted average of ∏m,'.
14
Published as a conference paper at ICLR 2022
Next, when the depth is d + 1,
Em hfm(d+1)(Xi,Wm,πm)fm(d+1) (Xj,Wm,πm)i
=Em	卜(Wm,txi)fmd) (χi, Wm),πm)) +(1 - σ(wm,tXi)) fmd(©,W
(r)
m,
(r) π(r)
m , πm
(σ(Wm,tχj )f(d) (Xj, Wm),πm)) +(1 -σ(Wm,tχj ))fmd (Xj, W
∖
=Em I I (fmd) (xi, Wm, πm
fm(d)	Xi, Wm(r), πm(r)
(r)
m,
^{z
(A)
*{z
(B)
(席(Xj, Wm), πm
(r)
m,
{z^
(C)
{z*
(D)
)\
(A.21)
—
✓
}
—
where the last equality sign is just a simplification to separate components (A), (B), (C), and (D).
Sincefm(d)(xi, Wm(r), πm(r)) andfm(d)(xj, Wm(l), πm(l) ) are independent of each other and have zero-mean
i.i.d Gaussian distribution, we obtain
Em [(A) ×	(C)]	T (Xi,Xj)Em fm(d) Xi,Wm(l),πm(l) fm(d) Xj,Wm(l),πm(l)	
		+fm(d) (Xi, Wm(r), πm(r)	fm(d) (Xj, Wm(r), πm(r)i ,	(A.22)
Em [(B) ×	(C)]	-0.5 Em	hfm(d)	(Xi,	Wm(r),	πm(r)	fm(d)	(Xj,	Wm(r),	πm(r)i ,	(A.23)
Em [(A) ×	(D)]	-0.5 Em hfm(d) (Xi, Wm(r), πm(r)	fm(d) (Xj, Wm(r), πm(r)i ,	(A.24)
Em [(B) ×	(D)]	Em hfm(d) (Xi, Wm(r), πm(r) fm(d) (Xj, Wm(r), πm(r)i .	(A.25)
Equation (A.23), Equation (A.24), and Equation (A.25) cancel each other out. Therefore, we obtain
By
Em
follows.
Em fm(d+1)(Xi,W
m , πm)fm(d+1) (Xj , Wm, πm)
=2T (Xi,Xj)Em fm(d)(Xi,Wm,πm)fm(d)(Xj,Wm,πm) .
induction hypothesis and Equation (A.19),
(A.26)
we
have
m, πm)fm(d) (xj, Wm, πm) = (2T (xi, xj))d. Therefore, the original lemma also
□
A.2 Proof of Lemma 2
Proof. When the depth is d + 1, the derivatives of the parameters in the left subtree and the right
subtree are
∂f(d+1) (xi, w, π)
∂ fm(d)	xi, Wm(l), πm(l)
∂wm,l
∂f(d+1) (xi, w, π)
∂Wm,l
∂fm(d) xi, W
(A.27)
(r)
m
∂Wm,r
where l and r used in Wm,l and Wm,r
∂wm,r
(A.28)
implies the node at the left subtree and right subtree, respectively.
Therefore, the limiting TNTK for the left subtree is
Θ(d+1),(l) (xi, xj) = T (xi, xj)(Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj)). (A.29)
15
Published as a conference paper at ICLR 2022
Similarly, for the right subtree, since
E[(1 - σ(u>xi)(1 - σ(u>xj)] = E
1 - σ(u>xi) - σ(u>xj) +σ(u>xi)σ(u>xj)
X—{z—}	X—{—}
→0.5	→0.5
E[σ(u>xi)σ(u>xj)],
(A.30)
we can also derive
Θ(d+1),(r) (xi,xj) = T(xi,xj)(Θ(d),(t) (xi,xj) + Θ(d),(l) (xi,xj) + Θ(d),(r) (xi,xj)). (A.31)
Finally, we obtain the following by combining with Equation (A.29) and Equation (A.31),
Θ(d+1),(l) (xi, xj) + Θ(d+1),(r) (xi,xj)
=2T (xi,	xj)	(Θ(d),(t)	(xi,	xj)	+ Θ(d),(l) (xi, xj)	+ Θ(d),(r)	(xi,	xj)).	(A.32)
□
A.3 Proof of Lemma 3
Proof. With Equation (1) and Equation (6),
∂f (xi, w, π)	1
∂∏m,'	= √Mμm,'(xi, Wm)
N
=√M Y σ (wm,nx,".n (1 - σ (Wm,nxi)广n&'
n=1
(A.33)
When We focus on a leaf ', for a tree with depth of d, 1n\' or 1'.n equals to 1 d times. There-
fore, by Equation (A.30), we can say that there is a T (xi, xj)d contribution to the limiting kernel
Θ(d),(b) (xi, xj) per leaf index ` ∈ [L]. Since there are 2d leaf indices in the perfect binary tree,
Θ(d+1),(b) (xi, xj) = (2T (xi, xj))d.	(A.34)
In other words, since the number of leaves doubles with each additional depth, we can say
Θ(d+1),(b) (xi, xj) = 2T (xi, xj)Θ(d),(b) (xi,xj).	(A.35)
□
A.4 Closed-form formula for the scaled error function
Since we are using the scaled error function as a decision function defined as
gm,nJ(wm,n, Xi) = σ (wm,nxi)
=1 erf (αwm,nxi) + 2,	(A.36)
ZT-	t z^j- ♦ EI	-t	F	1 1.1	1 .∙ 11 ʌ-ii	IC	1	Γ∙ . t	Γ∙
T and T in Theorem 1 can be calculated analytically. Closed-form solutions for the error function
(Williams, 1996; Lee et al., 2019) are known to be
Terf (xi, Xj) := E[erf(u>Xi) erf(u>Xj)] = — arcsin ( —	夕(xi, Xj)	: j ,
erf(	j)	[	( i)	( X)]	∏	<P(∑(xi, Xi) + 0.5)(Σ(xj, Xj)+0.5)广
(A.37)
>	>4	1
Terf (χi, Xj):= E[erf(u xi)erf(u Xj)] = ∏ p(1 + 2∑(χ. X∙))(1 + 2Σ(x∙ X '))- 4∑πψ.
( +	(Xi, Xi)) ( +	(Xj, Xj)) -	(Xi, Xj)
(A.38)
16
Published as a conference paper at ICLR 2022
Using the above equations, we can calculate T and T with the scaled error function as
T (xi, xj) = E
1
4
erf(αu>xi) erf(αu>xj)
+ E ； erf(αu>Xi) +； erf(αu>Xj)
1
+ 4
T (xi, xj)
；E [erf(αu>Xi) erf(αu>Xj)] + ɪ
_! arcsin	,	α2Xxi, Xj)
2π	∖ vz(α2∑(xi, Xi) + 0.5)(α2Σ(xj, Xj) + 0.5)
1
+ 4,
(A.39)
2
-^j-E 卜rf(αu>Xi)erf(αu>Xj)]
α2
1
π √(1 + 2α2Σ(xi, Xi)) (1 + 2α2Σ(Xj, Xj)) - 4α4Σ(xi, Xj)2
(A.40)
B Neural Tangent Kernel for multi-layer perceptron
B.1 EQUIVALENCE BETWEEN THE TWO-LAYER PERCEPTRON AND TREES OF DEPTH 1
In the following, we describe a two-layer perceptron using the same symbols used in soft trees
(Section 2.1) to make it easier to see the correspondences between a two-layer perceptron and a soft
tree ensemble. A two-layer perceptron is given as
1M	>
fMLP (2)(Xi, w, a) = √=	amσ wm> Xi ,
m=1
(B.1)
where we use M as the number of the hidden layer nodes, σ as a nonlinear activation function, and
w = (w1, . . . , wM) ∈ RF×M and a = (a1, . . . , aM) ∈ R1×M as parameters at the first and second
layers initialized by zero-mean Gaussians with unit variances. Since
∂fMLP (2)(Xi, W, a)
∂wm
dfMLP (2)(Xi, w, a)
dam
√Mσ (Wm Xi),
(B.2)
(B.3)
we have
1M
θMLP⑵(g, Xj) = M X
m=1
(
amX> Xjσ (Wm Xi)σ (Wm Xj)+
'-----------------{z----------------}
contribution from the first layer
σ (Wm Xi)σ (Wm Xj)
'------------{z-----------}
contribution from the second layer
(B.4)
Considering the infinite width limit (M → ∞), we have
ΘMLP⑵(Xi, Xj) = Σ(Xi, Xj)T(Xi, Xj) + T(Xi, Xj),	(B.5)
which is the same as the limiting TNTK shown in Equation (7) with d = 1 up to constant multiple.
B.2 Formula for the MLP-induced NTK
Based on Arora et al. (2019), we defined the L-hidden-layer perceptron3 as
…i, W) = W…√⅛σ W(L) ∙ PM-Tσ (W(LT)
(B.6)
where W(1) ∈ RM1×F, W(h) ∈ RMh×Mh-1, and W (L+1) ∈ R1×ML are trainable parameters.
We initialize all the weights W = W(1), . . . , W (L+1) to values independently drawn from the
3 Note that the two-layer perceptron is the single-hidden-layer perceptron.
17
Published as a conference paper at ICLR 2022
standard normal distribution. Considering the limit of the infinite width M1, M2, . . . , ML → ∞, the
formula for the limiting NTK of L-hidden-layer MLP is known to be
L+1	L+1
ΘMLP(L) (Xi, Xj) = X ( ∑(h-1) (Xi, Xj) ∙ Y ∑(h') (Xi, Xj) ) ,	(B.7)
h=1	h0=h
where
Σ(0) (Xi, Xj ) :=	Xi>Xj,	(B.8)
Λ(h) (X X ) := Σ(h-1)(Xi,Xi) Σ(h-1) (Xi,Xj)	∈	R2×2	(B9)
Λ (Xi,Xj) := Σ(h-1)(Xj,Xi) Σ(h-1)(Xj,Xj)	∈R ,	(B.9)
∑(h) (Xi, Xj ):=	Eu,v〜Normal(0,Λ(h)(xi,xj)) [σ(U)σ (V))] ,	(B.1O)
W") (Xi, Xj ):=	Eu,v〜Normal(0,Λ(h)(xi,xj)) [。(U)σ (V))] .	(B.1I)
We let Σ(L+1) (Xi, Xj) := 1 for convenience. See Arora et al. (2019) for derivation. There is a
correspondence between Σ(1) (Xi, Xj) and Σ (Xi, Xj) in Theorem 1, Σ(1) (Xi, Xj) and T (Xi, Xj) in
Theorem 1, and Σ(I) (Xi, Xj) and T (x^ Xj) in Theorem 1, respectively.
Since the recursive calculation is needed in Equation (B.7), the computational cost increases as the
layers get deeper. It can be seen that the effect of increasing depth is different from that of the limiting
TNTK, in which the depth of the tree affects only the value of the exponential power as shown in
Equation (7). Therefore, for any tree depth larger than 1, the limiting NTK induced by the MLP with
any number of layers and the limiting TNTK do not match.
C Proof of Proposition 1
Proof. As shown in Section 4.1.1, there is a close correspondence between the soft tree ensemble of
depth 1 and the two-layer perceptron. On one hand, from Equation (A.7), the limiting TNTK induced
by infinite trees with the depth of 1 is 2(Σ(Xi, Xj)T(Xi, Xj) + T (Xi, Xj)). On the other hand, if the
activation function used in the two-layer perceptron is same as σ defined in Equation (5), the NTK
induced by the infinite width two-layer MLP is Σ(Xi, Xj)T(Xi, Xj) + T(Xi, Xj) (Jacot et al., 2018;
Lee et al., 2019). Hence these are exactly the same kernel up to constant multiple.
The conditions under which the MLP-induced NTK are positively definite have already been studied.
Lemma 4 (Jacot et al. (2018)). For a non-polynomial Lipschitz nonlinearity σ, for any input
dimension F, the NTK induced by the infinite width MLP is positive definite if kXi k2= 1 for all
i ∈ [N] and Xi 6= Xj (i 6= j).
Note that σ defined in Equation (5) has the non-polynomial Lipschitz nonlinearity. Since the positive
definite kernel multiplied by a constant is a positive definite kernel, it follows that the limiting TNTK
Θ(1)(Xi, Xj) for the depth 1 is also positive definite.
As shown in Equation (C.1), as the trees get deeper, T(Xi, Xj) defined in Equation (8) is multiplied
multiple times in the limiting TNTK:
Θ(d)(Xi, Xj) = 2dd Σ(Xi, Xj)(T(Xi, Xj))d-1T(Xi, Xj) +
、	一■――	,
contribution from inner nodes
(2T(Xi, Xj))d
'-------V--------}
contribution from leaves
=2(2T(Xi, Xj))d-1 (d Σ(Xi, Xj)T(Xi, Xj) + T(Xi, Xj)).	(C.1)
、-------------------{--------------------}
NTK induced by two-layer perceptron (if d = 1)
The positive definiteness of T(Xi, Xj) has already been proven.
Lemma 5 (Jacot et al. (2018)). For a non-polynomial Lipschitz nonlinearity σ, for any input
dimension F, the T (Xi, Xj) := E[σ(u>Xi)σ(u>Xj)] defined in Theorem 1 is positive definite if
kXik2= 1 for all i ∈ [N] and Xi 6= Xj (i 6= j).
Note that d Σ(Xi, Xj)T(Xi, Xj) + T (Xi, Xj) for d ∈ N is positive definite. Since the product of the
positive definite kernel is positive definite, for infinite trees of arbitrary depth, the positive definiteness
of Θ(d) (Xi, Xj) holds under the same conditions as in MLP.	□
18
Published as a conference paper at ICLR 2022
D Proof of Theorem 2
Proof. We use the following lemmas in the proof.
Lemma 6. Let a ∈ Rn be a random vector whose entries are independent standard normal random
variables. For every v ≥ 0, with probability at least 1 - 2ne(-v2n/2) we have:
kak1≤ vn.	(D.1)
Lemma 7. Let ai ∈ R≥0. We have
n
∑√ai ≤
i=1
n
ai.
i=1
(D.2)
In addition, our proof is based on the strategy used in Lee et al. (2019), which relies on the local
LiPschitzness of the model Jacobian at initialization J(x, θ), whose (i,j) entry is d∂θi,θ) where θj
is a j -th component of θ:
Theorem 4 (Lee et al. (2019)). Assume that the limiting NTK induced by any model architecture is
positive definite for input sets x, such that minimum eigenvalue of the NTK λmin > 0. For models
with local Lipschitz Jacobian trained under gradient flow with a learning rate η < 2(λmin + λmax),
we have with high probability:
SUp ∣ΘT (Xi, Xj) - Θ0 (Xi, Xj)1 = O
(D.3)
It is not obvious whether or not the soft tree ensemble’s Jacobian is local LiPschitz. Therefore, we
Prove Lemma 8 to Prove Theorem 2.
Lemma 8. For soft tree ensemble models with the NTK initialization and a positive finite scaling
factor α, there is K > 0 such that for every C > 0, with high probability, the following holds:
k	IIJ(X,	θ)kF	≤ K	∀θ θ u B (θ C)	(D4)
k	kJ (x, θ)-J (x,	Θ)∣f	≤ K∣θ-θ∣2 , ∀θ, θ ∈ B (θ0,C)	,	(D.4)
where
B (θo,C):= {θ : ∣∣θ - θ0∣∣2 <C} .	(D.5)
By Proving that the soft tree ensemble’s Jacobian under the NTK initialization is the local LiPschitz
with high probability, we extend Theorem 2 for the TNTK.	□
D.1 Proof of Lemma 6
Proof. By use of the Chebyshev’s inequality, for some constant c, we obtain
P(ka∣1> C) ≤ E[eγkak1 ]∕eγc
=卜 γ32(1 + erf(γ∕√2)))7eγc,	(D.6)
where P means a probability. Since erf(γ∕ √2) ≤ 1, when we use Y = c∕n,we get
P(IaI1> c) ≤ 2ne(-c2/2n) .	(D.7)
Lemma 6 can be obtained by assigning vn to c.	□
Figure 8 shows the right-hand side of the Equation (D.7) with c = 5n. when n = 1, probability is
7.45 × 10-6. As n becomes larger, the probability becomes even smaller.
19
Published as a conference paper at ICLR 2022
Figure 8: Right-hand side of the Equation (D.7), where c = 5n (in other words, v = 5 in Lemma 6).
D.2 Proof of Lemma 7
Proof. By use of Cauchy-Schwarz inequality, for p, q, x, y ∈ R≥0, we have
p√X + q√y ≤ √(p2 + q2 )(χ + y).	(D.8)
With Equation (D.8), we prove the lemma by induction. In the base case,
√a1 + √a2 ≤ √2√aι + a2,	(D.9)
which is consistent to the lemma. Next, when we assume
√01 + …+ √αk ≤ λ∕k√ai + ∙∙∙ ak,	(D.10)
we have
√α1 + …+ √ak + √ ak+1 = (√a1 + …+ √ak) + √ak+1
≤ λ∕k√ai + ∙∙∙ + ak + √ak+1
≤ k+ + 1，ɑ1 + …+ a+ + &k+i.	(D.11)
□
D.3 Proof of Lemma 8
Proof. Consider the contribution of the leaf parameters at first:
d(xi，w，π = √⅛= μm,'3, wm).	(D.12)
∂∏m,'	√M
Next, the contribution from the leaf parameters is
L
∂f (Xi, w, ∏) =	1 X	∂μm,'(g, wm)
∂wm,n	√M L πm,'	∂wm,n
1L
=√M∑S∏m,'Sn,'(Xi, wm)xi(wj,nʃi) ,	(D.13)
where
Sn,'(x, wm) ：= Y H σ (wm,nθ g) 1('.n"(n=n0)(1 - σ 的，,〃 g)) "&')&(n=n0) ) (-1)1n&' ,
n0=i
(D.14)
20
Published as a conference paper at ICLR 2022
and & is a logical conjunction. For any real scalar P and q, the scaled error function σ defined in
Equation (5) is bounded as follows:
0 ≤ σ(p) ≤ 1,	∣σ(p) - σ(q)∣ ≤ |p - q|,	0 ≤ σ(p) ≤ α, ∣σ(p) - σ(q)∣ ≤ α∣p - q|. (D.15)
Therefore, the absolute value of S%g does not exceed 1. With Equation (D.15), We can obtain
=∣∣Sn,'(Xi, Wm)XQ (w>nXi) ∣∣2 ≤ a㈤心	(D.16)
2	，
∂μm,2(Xi, Wm)
d wm,n
with high probability. Therefore, with Lemma 6, in probability,
N
IlJ (x, θ)kF = X(IlJ (Xi, W)IIF+kJ (M π)kF)
i=1
πm,2
∂μm,2(Xi, Wm)
dwm,n
NM (N	L
≤ M XX X v"2α2Ek2 + X 1
i=1 m=1 ∖n=1	'=1 .
N
X L(v2Lα2N∣∣Xik2+1).
i=1
Next, we will consider the Jacobian difference. Since μm,'(xi, Wm) is a multiple multiplication of
the decision function, by use of
n	n	n
YPi - Yqi ≤ X ∣Pi - qi∣	for ∣Pi∣ ,∣qi∣≤ 1,	(d.18)
we obtain
N
∣μm,g(xi, Wm) - μm,2(X i, W m) ∣ = I Y σ (Wm,nXi)1"n (1 - σ (Wm,nXi))1"∖'
n=1
N
-γσ(Wm,nxi)".n (1 - σ (Wm,nχi))ILn''
n=1
N
≤ X I σ (Wm,nxi) .n (1 - σ Wm,nxi)) n
n=1
-σ (Wm,nxi)1'.n (1 - σ (Wm,nxi))1n&' I
N
≤ E^m^g- Wm ,nχi∣
n=1
N
≤ XkXik2∣∣Wm,n - Wm,n∣∣2,	(D.19)
n=1
where it should be noted that (' / n) & (n & 2) must be false.
21
Published as a conference paper at ICLR 2022
∂μm,'(Xi, Wm)	∂μm,'(Xi, Wm)
----：----------------：---------
dWm,n	d Wm,n
Sn,' (xi, Wm) - Sn,' (xi, Wm) Can be bound in the same way as Equation (D.19). Therefore, We
also obtain
=∣∣Sn,' (Xi, Wm) XQ (Wm,nXi) - Sn,' Qi, Wm) xQ (Wr>,nXi) ∣∣2
2	'	'
=IIXiII2∣Sn,' (Xi, Wm) σ (Wm,nxi) - Sn,' (Xi, Wm) σ (Wm,nXi) I
≤	IlXiIl2 (∣(Sn,' (Xi, Wm) — Sn,' (Xi, Wm))^ (W*,nXi) I
十 |(j (Wm,nXi) - σ (Wm,nXi))Sn,' (Xi, Wm) |)
≤	IlXik 2 (Ia(Sn,' (Xi, Wm) - Sn,' (Xi, Wm))1
+ Ka(Wm,nXi - Wm,nXi))I)
N
≤	2α∣∣Xi∣∣2 X∣∣Wm,n - W m,n∣∣2.	(D.20)
n=1
To link Equation (D.19) and Equation (D.20) to the ∣∣θ - θ∣∣2,we use Lemma 7 to obtain the following
inequalities:
N
X∣∣Wm,n - Wm,n∣∣2 ≤ √N∣∣Wm — W m∣∣2 ≤ √N∣∣θ  θ∣∣2,	(D.21)
n=1
L
X∣∏m,' - ∏m,'∣ ≤ √L∣∣∏m - ∏m∣∣2≤ √L∣∣θ - θ∣∣2.	(D.22)
'=1
With Equation (D.1), Equation (D.16), Equation (D.19), Equation (D.20), Equation (D.21), and
Equation (D.22),
IJ (X, θ) - J (X, θ)IF
N
X(∣∣J(Xi, w) - J(Xi, W)IF+∣∣J(Xi, π) - J(Xi, Π)∣∣F)
i=1
2
N M
M XX
i=1 m=1
IX km,'
dMm,'(Xi, Wm)
dWm,n
-πm,'
dMm,'(Xi, Wm)
d Wm,n
+〉：(Mm,'(Xi, Wm) - Mm,'(Xi, Wm))
NM
M XX
i=1 m=1
∂μm,'(Xi, Wm)
)
+〉：(Mm,'(Xi, Wm) - Mm,'(Xi, Wm))2
dWm,n
∂ ∂μm,'(Xi, Wm) _ ∂μm,'(Xi, Wm)
∖	dwm,n
dWm,n
πm,'
+
2
2
2
22
Published as a conference paper at ICLR 2022
1NM	N	N	2
≤ M XX(X	(Inm,' - πm,'lakxik2) + I 2ɑkxik2 Xkwm,n - wm,nk2vL I I I
i=1 m=1 n=1	n=1
+X Xkxik2∣∣Wm,n - Wm,n∣∣2) 1
'=1 ∖n=1	))
NM
≤- X X
≤ M乙乙
i=1 m=1
kXik2kθ - Ok?) !
∣∣θ - θ∣∣2α∣∣Xik2) + (2α∣∣Xik2√N∣∣θ - θ∣∣2vL))
≤ X (N (α√L∣Xik2+2α∣Xik2√NvL)2 + LNkxik?) ∣θ - θ∣∣2	(D.23)
By considering the square root of both sides in Equation (D.17) and Equation (D.23), we conclude
the proof for Lemma 8.	□
E Proof of Theorem 3
Proof. We can use the same approach with the proof of Theorem 1. Using an incremental formula,
the output from the oblivious tree ensembles can be written as follows:
1M
f (d)(xi, w, π) = √M X k(wm,t xi) fmdτ) (xi, Wmr πm)
m=1
+(1-σ (wm,txi)) fm-1 (xi,Wm),πm)))	(E.I)
where (s) of Wm(s) means (s)hared parameters at subtrees. Intuitively, the fundamental of Theorem 3
is that the outputs of the left subtree and right subtree are still independent with the oblivious tree
structure. Even with parameter sharing at the same depth, since the leaf parameters π are not shared,
the outputs of the left subtree and right subtree are independent.
We will see that Lemma 1, 2 and 3 are also valid for oblivious tree ensembles.
Correspondence to Lemma 1. To show the correspondence to Lemma 1, it is sufficient to show
that Equation (A.22), Equation (A.23), Equation (A.24), and Equation (A.25) hold when
Em fm(d+1)(xi,Wm,πm)fm(d+1) (xj,Wm,πm)
∖
=Em
(fd) (χi, Wmr πm
{z"
(A)
{z^
(B)
(fmd) (xj, Wm), πm
fm(d)	xj, Wm(s),
{z
(C)
{z^
(D)
}
}
(E.2)
—
✓
}
—
This equation corresponds to Equation (A.21). Here, since the leaf parameters π are not shared, the
outputs of the left subtree and right subtree are still independent even with the oblivious tree structure.
Therefore, we can obtain the correspondences to Equation (A.22), Equation (A.23), Equation (A.24),
and Equation (A.25) with the same procedures.
23
Published as a conference paper at ICLR 2022
Correspondence to Lemma 2. For the depth d +1, since
∂f(d+1) (Xi, w, π)
∂wm,s
+ (1 — σ(w>,t Xi))
∂wm,s
(E.3)
the corresponding limiting TNTK is
θ(d+1),(s) (Xi, Xj )
T
d
X Em
s=2
∂wm,s
(	(s) (r)
Xj, wm, ∏m
∂wm,s
∂wm,s	/
∂fmmd (xj, Wm), ∏m))
∂wm,s
∖t
d
X Em
s=2
,(s) π(l)
m , π m
∂wm,s
^^^^^{z
(A)
∂fmd gi, Wm), ∏m?) \
∂wm,s	)+
/
∂ wm,s
-----------V-----------'
(B)
\
∂fmd (Xj, Wm), ∏m>)	∂fmd (Xj, w
------------------------------
,(s)
m ,
∂wm,s
^^^^^{z
(C)
∂wm,s
∏m)) ∖	∂∕* (xj, Wm), ∏m>)
∂wm,s
^^^{^^^™
(D)
(E.4)
—
|
,
,
/
Since Afm (；,wm' V) and Afm (yWm',πm)for S = {2,3,...,d} are independent to each other
OWm,s	dwm,s
and have zero-mean Gaussian distribution4, similar calculation used for Equation (A.22), Equa-
4For a single oblivious tree, the number splitting rule is d because of the parameter sharing.
24
Published as a conference paper at ICLR 2022
tion (A.23), Equation (A.24), and Equation (A.25) gives
Em [(A) × (C)] = T (xi, xj) Em
Em [(B) × (C)] = -0.5 Em
Em [(A) × (D)] = -0.5 Em
Em [(B) × (D)] =Em
, (E.6)
, (E.7)
(E.8)
As in the previous calculations, Equation (E.6), Equation (E.7), and Equation (E.8) cancel each other
out. As a result, we obtain
Θ(d+1),(s) (xi,xj) = 2T (xi,xj) Θ(d),(t) (xi,xj) + Θ(d),(s) (xi,xj) .	(E.9)
Correspondence to Lemma 3. Considering Equation (A.33), once we focus on a leaf `, it is not
possible for both 1i&' and 1'.ι to be 1. This means that a leaf cannot belong to both the right
subtree and the left subtree. Therefore, even with the oblivious tree structure, there are no influences.
Therefore, We get exactly the same result for the Lemma 3.	□
F Details of numerical experiments
F.1 Setup
F.1.1 Dataset acquisition
We use the UCI datasets (DUa & Graff, 2017) preprocessed by Ferngndez-DelgadO et al. (2014),
Which are publicly available at http://persoal.citius.usc.es/manuel.fernandez.
delgado/papers/jmlr/data.tar.gz. Since the size of the kernel is the square of the
dataset size and too many data make training impractical, We use preprocessed UCI datasets With the
number of samples smaller than 5000. Arora et al. (2020) reported the bug in the preprocess When
the explicit training/test split is given. Therefore, We do not use that dataset With explicit training/test
split. As a consequence, 90 different datasets are available.
F.1.2 Kernel specifications
TNTK. See Theorem 1 for the detailed definitions. We change the tree depth from 1 to 29 and change
α in {0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0}.
MLP-induced NTK. We assume the MLP activation function as ReLU. Our implementation is
based on the publicly available code5 used in Arora et al. (2020). For detailed definitions, see Arora
et al. (2020). The hyperparameter of this kernel is the model depth. We change the depth from 1 to
29. Here, depth = 1 means there is no hidden layer in the MLP.
5https://github.com/LeoYu/neural-tangent-kernel-UCI
25
Published as a conference paper at ICLR 2022
Figure 9: The γ dependency of the RBF kernel performance.
RBF kernel. We use scikit-learn implementation6. The hyperparameter of this kernel is γ, in-
verse of the standard deviation of the RBF kernel (Gaussian function). For Figure 3, we tune γ in
{0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,
3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20.0, 30.0}, resulting in 30 candidates in total. In our experi-
ments, γ = 2.0 performs the best on average (Figure 9).
F.1.3 Model specifications
We used kernel regression implemented in scikit-learn7 . To consider ridge-less situation, regulariza-
tion strength is set to be 1.0 × 10-8, a very small constant.
F.1.4 Computational costs
Since the training and inference algorithms of the kernel regression are common across different
kernels, we analyze the computational cost of computing a single value in a gram matrix of the
corresponding kernels in the following. The time complexity of the MLP-induced NTK is linear
with respect to the layer depth, while that of the TNTK remains to be constant. Such a trend can be
seen in the right panel of Figure 7. For the RBF kernel, the computational cost remains the same
with respect to changes in hyperparameters, thus its trend is similar to the TNTK. In terms of the
space complexity, when considering a multi-layered MLP, since it is not necessary to store all past
calculation results in memory during the recursive computation, the MLP-induced NTK computation
consumes a certain amount of memory regardless of the depth of the layers. Therefore, the memory
usage is almost the same across the RBF kernel, TNTK, and MLP-induced NTK.
F.1.5 Computational resource
We used Ubuntu Linux (version: 4.15.0-117-generic) and ran all experiments on 2.20 GHz Intel Xeon
E5-2698 CPU and 252 GB of memory.
F.2 Results
F.2.1 Statistical significance of the parameter dependency
A Wilcoxon signed rank test is conducted to check the statistical significance of the differences
between different α. Figure 10 shows the p-values for the depth of 3 and 20. As shown in Figure 7,
when the tree is shallow, the accuracy started to deteriorate after around α = 8.0, but as the tree
becomes deeper, the deterioration became less apparent. Therefore, statistically significantly different
pairs for deep tress and shallow trees are different. When the tree is deep, large α shows a significant
difference over those with small α. However, when the tree is shallow, the best performance is
achieved with α of about 8.0, and if α is too large, the performance deteriorates predominantly.
6https://scikit- learn.org/stable/modules/generated/sklearn.metrics.
pairwise.rbf_kernel.html
7https://scikit- learn.org/stable/modules/generated/sklearn.kernel_ridge.
KernelRidge.html
26
Published as a conference paper at ICLR 2022
d=20
d=3
-0.8
-0.6
0.4
-0.2
-0.0


S
§
§
§
O
O
O
O
0.01
0.01
0.01
0.01
0.03
0.24
0.84
0.92
0.06
0.46
0.18
0.21
0.86 0.88 0.88
0.81
0.19
0.72
0.5 1.0 ZO 4：0 8：0 16.0 32.0 64.0
-0.8

0.6
0.4
0.2
10.0
α=0.5
IOO
Figure 10: P-values of the Wilcoxon signed rank test for different pairs of α.
α=l,0
α=2,0
α=4,0
20	40	60	80	100
TNTK
∣)	20	40	60	80	100
TNTK
0	20	40	60	80	100
TNTK
20	40	60	80	100
TNTK
Figure 11: Performance comparisons between the kernel regression with MLP-induced NTK and the
TNTK on the UCI dataset.
Figure 12: Performance comparisons between the kernel regression with RBF kernel and the TNTK
on the UCI dataset.
IndTO%00 UOneIaπoo-ΠOSJ∞d
Figure 13: Pearson’s correlation coefficients with predicted values of the TNTK with different α.
27
Published as a conference paper at ICLR 2022
F.2.2 Dataset-wise results
For each α, scatter-plots are shown in Figures 11 and 12. As shown in Figure 13, the correlation
coefficients with the TNTK are likely to be higher for the MLP-induced NTK than for the RBF kernel.
Tables 2, 3 and 4 are dataset-wise results of the comparison between the TNTK, the MLP-induced
NTK, and the RBF kernel. For each α, depth is tuned for each dataset. In terms of the depth, the
best performers from 1 to 29 are compared with the TNTK and the MLP-induced NTK. For the RBF
kernel, γ is tuned in each dataset from 30 candidate values as described in Section F.1.2. Therefore,
the number of tunable parameters is the same across all methods. All parameter-wise results are
visualized in Figures 14 and 15.
28
PUbliShed as a ConferenCe PaPersICLR 2022
name
size	Q=O.5	α=1.0 Q=2.0	Q=4.0	Q=8.0	Q=I6.0	Q=32.0	Q=64.0 MLP-NTK RBF
0	trains	10	87.500	87.500	87.500	87.500	87.500	87.500	87.500	87.500	100.000	87.500
1	balloons	16	87.500	100.000	93.750	87.500	87.500	87.500	87.500	87.500	100.000	93.750
2	lenses	24	87.500	87.500	87.500	87.500	87.500	87.500	87.500	87.500	87.500	87.500
3	lung-cancer	32	56.250	53.125	53.125	53.125	53.125	56.250	59.375	59.375	65.625	53.125
4	post-operative	90	63.636	64.773	67.045	69.318	69.318	68.182	68.182	69.318	69.318	56.818
5	pittsburg-bridges-SPAN	92	55.435	57.609	58.696	67.391	65.217	66.304	66.304	67.391	65.217	58.696
6	fertility	100	84.000	88.000	89.000	89.000	89.000	89.000	89.000	89.000	89.000	83.000
7	zoo	101	100.000	99.000	99.000	99.000	99.000	99.000	99.000	99.000	99.000	99.000
8	pittsburg-bridges-T-OR-D	102	81.000	84.000	87.000	89.000	89.000	89.000	88.000	88.000	87.000	89.000
9	pittsburg-bridges-REL-L	103	67.308	74.038	75.000	74.038	75.962	75.962	75.962	75.962	74.038	74.038
10	pittsburg-bridges-TYPE	105	57.692	59.615	64.423	66.346	66.346	66.346	66.346	65.385	68.269	59.615
11	molec-biol-promoter	106	90.385	88.462	87.500	87.500	87.500	87.500	87.500	87.500	90.385	88.462
12	pittsburg-bridges-MATERIAL	106	93.269	94.231	94.231	94.231	94.231	94.231	94.231	94.231	94.231	93.269
13	breast-tissue	106	65.385	68.269	67.308	70.192	72.115	72.115	75.000	74.038	69.231	71.154
14	acute-nephritis	120	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000
15	acute-inflammation	120	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000	100.000
16	heart-switzerland	123	37.097	43.548	48.387	47.581	50.000	44.355	44.355	44.355	47.581	37.903
17	echocardiogram	131	81.061	81.061	84.848	84.091	85.606	85.606	85.606	85.606	85.606	78.030
18	lymphography	148	88.514	88.514	88.514	88.514	87.838	87.162	86.486	86.486	88.514	86.486
19	iris	150	95.946	97.973	96.622	88.514	86.486	87.162	87.838	87.162	87.162	96.622
20	teaching	151	56.579	57.895	58.553	60.526	64.474	67.105	67.105	67.763	63.158	60.526
21	hepatitis	155	83.974	85.256	85.256	84.615	84.615	84.615	84.615	85.256	83.974	85.256
22	wine	178	98.864	99.432	98.864	98.864	98.864	98.295	98.295	97.727	99.432	98.295
23	planning	182	62.222	65.556	70.000	71.667	71.667	72.222	72.222	72.222	72.222	67.222
24	flags	194	52.083	53.646	53.646	53.125	53.646	53.125	53.125	53.125	53.646	49.479
25	parkinsons	195	93.878	94.388	92.857	93.367	92.857	92.857	93.367	93.367	93.878	95.408
26	breast-cancer-wisc-prog	198	82.143	83.673	83.673	83.673	83.673	83.673	83.673	83.673	85.204	78.571
27	heart-va	200	31.000	34.000	36.000	36.000	37.500	39.000	40.500	43.000	36.500	29.000
28	conn-bench-sonar-mines-rocks	208	86.538	86.538	87.019	86.538	86.538	86.538	86.538	86.538	87.981	87.500
29	seeds	210	90.865	93.750	93.269	91.827	92.308	92.308	91.827	91.827	96.154	95.673
Table 2: Comparison between TNTK and MLP-induced NTK for a half of the dataset (1/3).

PUbliShed as a ConferenCe PaPersICLR 2022
	name	size	ɑ=0.5	ɑ=1.0	ɑ=2.0	ɑ=4.0	ɑ=8.0	ɑ=16.0	ɑ=32.0	ɑ=64.0	MLP-NTK	RBF
30	glass	214	60.849	67.453	70.755	70.755	71.698	71.698	71.226	71.226	70.283	69.811
31	statlog-heart	270	83.209	87.313	87.687	88.433	88.433	88.433	87.687	87.313	86.567	82.463
32	breast-cancer	286	64.789	67.254	69.718	70.423	72.887	74.648	75.000	75.000	71.831	65.845
33	heart-hungarian	294	83.219	83.904	84.247	84.589	85.616	85.274	85.274	85.274	85.616	82.877
34	heart-cleveland	303	55.263	58.224	58.882	59.211	59.539	58.553	59.211	59.539	57.895	53.618
35	haberman-survival	306	59.539	61.513	61.842	66.447	68.421	71.711	73.684	73.684	71.053	70.395
36	vertebral-column-2clases	310	69.805	77.273	78.571	80.844	82.792	84.091	84.091	82.792	83.117	81.494
37	vertebral-column-3clases	310	71.753	78.247	81.169	80.844	80.844	81.818	81.494	81.169	81.818	81.169
38	primary-tumor	330	47.561	50.305	52.134	53.049	52.744	50.610	50.305	50.000	52.134	45.427
39	ecoli	336	71.429	79.167	83.036	84.524	86.012	86.607	86.905	86.905	85.417	81.250
40	ionosphere	351	90.057	91.477	90.341	87.784	88.352	88.352	88.352	88.352	91.761	92.330
41	libras	360	82.778	81.389	80.556	80.833	81.111	80.833	80.833	80.833	83.889	85.278
42	dermatology	366	97.802	97.802	97.527	97.527	97.253	97.253	97.253	97.253	97.802	97.253
43	congressional-voting	435	61.697	61.697	61.927	61.927	61.927	61.697	61.697	61.697	61.697	62.156
44	arrhythmia	452	69.469	65.265	64.602	64.823	64.823	64.823	64.823	64.823	71.239	69.248
45	musk-1	476	89.076	89.076	89.076	89.286	89.286	89.076	89.076	89.076	89.706	90.756
46	cylinder-bands	512	79.883	78.125	78.125	78.320	78.516	78.320	78.320	78.320	80.273	79.688
47	low-res-spect	531	91.729	91.353	90.602	89.474	88.534	87.782	87.218	87.218	91.353	90.226
48	breast-cancer-wisc-diag	569	96.127	96.655	97.359	97.359	97.359	96.831	96.479	96.479	97.007	95.599
49	ilpd-indian-liver	583	64.897	69.521	70.719	72.260	71.062	71.747	72.603	72.603	71.918	70.377
50	synthetic-control	600	99.333	99.333	99.167	98.833	98.333	97.833	97.000	96.667	98.833	99.333
51	balance-scale	625	81.250	84.615	88.782	89.904	91.346	90.064	85.256	85.256	93.269	90.865
52	statlog-australian-credit	690	59.012	60.610	64.099	66.279	67.151	68.023	68.023	68.023	66.279	59.302
53	credit-approval	690	82.558	85.174	86.628	87.209	87.645	87.791	87.791	87.355	87.064	81.686
54	breast-cancer-wisc	699	96.286	97.286	97.857	97.857	98.000	98.000	98.000	98.000	98.000	96.714
55	blood	748	67.513	65.775	63.369	69.786	72.727	73.529	75.802	77.005	74.064	78.075
56	energy-y2	768	89.583	89.453	88.021	87.891	88.151	87.630	87.630	87.630	88.281	90.755
57	piɪna	768	68.229	70.182	71.354	73.307	76.042	76.302	77.083	76.693	75.000	69.661
58	energy-yl	768	93.750	93.620	93.229	90.495	90.234	90.104	90.234	90.234	92.708	96.484
59	statlog-vehicle	846	78.318	77.014	76.540	73.578	72.986	72.156	72.038	72.038	81.398	77.488
Table 3: Comparison between TNTK and MLP-induced NTK for a half of the dataset (2/3).

PUbliShed as a ConferenCe PaPersICLR 2022
name
size Q=O.5	α=1.0 Q=2.0 Q=4.0 Q=8.0 Q=I6.0 Q=32.0 Q=64.0 MLP-NTK RBF
60	oocytes_trisopterus_nucleus_2f	912	82.456	82.566	82.456	82.675	80.811	78.728	78.070	77.851	84.978	79.605
61	oocytes_trisopterus_states_5b	912	92.325	92.982	93.640	93.092	91.667	90.022	89.693	89.693	94.189	91.228
62	tic-tac-toe	958	99.268	99.163	99.268	99.268	99.268	99.268	99.268	99.268	98.640	100.000
63	mammographic	961	72.708	71.250	72.083	75.625	77.604	78.854	78.958	79.271	80.000	78.750
64	statlog-german-credit	1000	75.200	76.500	77.800	77.300	76.200	75.500	75.500	75.400	77.500	73.700
65	led-display	1000	72.400	72.300	72.600	72.500	72.300	72.500	72.300	72.500	72.900	73.000
66	oocytes_merluccius_nucleus_4d	1022	81.078	80.588	80.686	80.686	79.412	77.255	76.961	76.765	83.725	75.490
67	oocytes_merluccius_states_2f	1022	92.353	91.961	92.157	92.157	91.373	90.784	90.784	90.490	93.039	92.059
68	contrac	1473	40.082	44.293	47.147	50.068	51.155	52.038	52.514	51.155	50.272	43.207
69	yeast	1484	42.588	49.326	54.380	58.154	60.040	60.243	60.445	60.849	59.636	54.380
70	semeion	1593	93.719	93.467	93.405	93.467	93.467	93.467	93.467	93.405	96.168	95.603
71	wine-quality-red	1599	63.062	65.938	68.812	70.000	70.625	70.375	70.312	70.312	69.625	64.438
72	plant-texture	1599	83.812	81.812	79.438	77.938	77.688	77.625	77.750	77.625	86.125	85.625
73	plant-margin	1600	84.750	83.938	82.938	81.875	80.750	79.500	78.938	78.563	84.875	83.875
74	plant-shape	1600	64.812	63.562	62.438	60.375	58.312	57.062	56.375	55.937	66.250	68.000
75	car	1728	97.454	97.569	97.164	96.701	96.354	96.181	96.123	96.123	97.743	98.032
76	steel-plates	1941	76.289	77.320	77.938	77.423	77.062	76.753	76.598	76.495	78.351	75.103
77	cardiotocography-3clases	2126	92.232	92.514	92.043	91.902	91.949	91.855	91.996	91.855	93.173	92.043
78	cardiotocography- IOclases	2126	80.838	82.957	82.250	80.744	79.896	79.896	79.661	79.614	84.181	79.143
79	titanic	2201	78.955	78.955	78.955	78.955	78.955	78.955	78.955	78.955	78.955	78.955
80	statlog-iɪnage	2310	96.360	96.967	97.097	96.750	96.231	95.927	95.884	95.624	97.660	96.404
81	ozone	2536	97.358	97.240	97.200	97.200	97.200	97.200	97.200	97.200	97.397	97.200
82	molec-biol-splice	3190	86.731	85.947	84.536	83.093	82.465	82.403	82.371	82.371	86.920	86.418
83	chess-krvkp	3196	99.124	98.905	98.655	97.872	96.902	95.526	95.307	95.307	99.406	98.999
84	abalone	4177	50.407	49.880	55.532	60.010	62.548	64.200	64.943	65.086	63.410	64.152
85	bank	4521	88.628	89.336	89.358	89.513	89.358	89.159	89.181	89.159	89.735	88.142
86	spambase	4601	91.478	91.174	92.435	90.652	93.130	89.630	91.478	93.348	94.913	90.652
87	wine-quality-white	4898	63.623	66.810	67.545	68.791	69.158	68.975	68.995	68.913	69.097	65.748
88	waveform-noise	5000	86.360	86.340	86.520	86.540	86.720	86.500	85.900	85.520	86.540	85.460
89	waveform	5000	85.440	85.780	86.300	86.500	86.660	86.700	86.740	86.520	86.340	84.640
Table 4: Comparison between TNTK and MLP-induced NTK for a half of the dataset (3/3).
3
Published as a conference paper at ICLR 2022
trains
xυssυυ<
10	20
Depth
ball∞ns
lenses	lung-cancer	post-operative
10	20	30
DqJth
molec-biol-promoter

10	20	30
DqJth
0 9 nittsburff-bridees-T-OR-D
⅛0.7
§0.6
^0.5
10	20	30
Depth
,PittebUrg∙bridges-REL-L,
10	20	30
Depth
pittsburg-bri⅛es-TYPE
10	20	30
Depth
PittSbUrg-brid盥 S-MATERIAL
10	20	30
DqJth
breast-tissue
10	20	30
Depth
acute-nephritis
ins
acute-ιnflaπιmatιon
heart-Switzerland
echocardiogram
lymphography
teachine	hepatitis
wme	planιune	flags
10	20	30	10	20	30	10	20	30	0	10	20	30	10	20	30
Jn {. ʃ
Xυs3υυ<
DqJth
Depth
DqJth
α=2,0	... a=4.0
....a=8.0	... Q= 16.0
a=64.0
10	20	30
Depth
MLP-induced NTK ......... RBF
Figure 14: Dataset-wise comparison for a half of the dataset (1/2).
32
Published as a conference paper at ICLR 2022
lħ
xυssυυ<
30
20
10
cylmder-bands
-87
O O
xυssυυ<
30

泗
low-res-spect
xυssυυ<
breast-eaneer-wise-dɪag
ilpd-mωan-lιver
xυssυυ<
B
SynthetIe-COntrol
•°W
1 O
AgeJn3。V

10
Depth
30
30
10	20
DqJth


⅛0.65
前.6。
■ 0.55
30
10	20
Depth
credit-approval
»0.875
10.850
10-825
30
10	20
DqJth
Dreast-Cancer-Wisc
9 9
O O
AgeJn3。V
30
10	20
Depth


blood
pima
statlog-vehicle
energy-y2
energy-yl
10	20	30
DqJth
10	20	30
Depth
tic-tac-toe
0 跌MeS trisopterus nucleus
0.80-
<0.75
b
10	20	30
Depth
led-display
Depth
semeion
xυssυυ<
4d oocytes merluccιus states 2f
mammographic
10	20	30
Depth
10	20	30
Depth
10	20	30
Depth
plant-texture
10	20	30
Depth
StatIog-german-credit
10	20	30
Depth
∞ntrac
yeast
10	20	30
DqJth
Wine-quality-red
XmJn8v
10	20	30
我8
30
°'θ	10	20
- -—
.7.7.6
Ooo
≡- - - -M
*8
0.6
titanic
I：
DqJth
0.79
∣0.78
KO.77
10	20	30
StatIog-image
Depth
gθ.975
10.950
0.925
chess-kπ⅛n
DqJth
S 0.6
lθ,5
Depth
abalone
10	20	30
Depth
Figure 15: Dataset-wise comparison for a half of the dataset (2/2).
WaVefbrm-noise	waveform
30	10	20	30
Depth
MLP-induced NTK ......... RBF
Xυs3υυ<
0.850
n »,><
33