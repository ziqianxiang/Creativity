Published as a conference paper at ICLR 2022
A Reduction-Based Framework for Conserva-
tive Bandits and Reinforcement Learning
Yunchang Yang*
Center for Data Science, Peking University
yangyc@pku.edu.cn
Tianhao Wu*
University of California, Berkeley
thw@berkeley.edu
Han Zhong*
Center for Data Sience, Peking University
hanzhong@stu.pku.edu.cn
Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric
Facebook AI Research
{evrard,
pirotta, lazaric}@fb.com
Liwei Wang
Key Laboratory of Machine Perception, MOE,
School of Artificial Intelligence, Peking University
International Center for Machine Learning Research,
Peking University
wanglw@cis.pku.edu.cn
Simon S. Du
University of Washington
ssdu@cs.washington.edu
Ab stract
We study bandits and reinforcement learning (RL) subject to a conservative con-
straint where the agent is asked to perform at least as well as a given baseline policy.
This setting is particular relevant in real-world domains including digital marketing,
healthcare, production, finance, etc. In this paper, we present a reduction-based
framework for conservative bandits and RL, in which our core technique is to
calculate the necessary and sufficient budget obtained from running the baseline
policy. For lower bounds, we improve the existing lower bound for conservative
multi-armed bandits and obtain new lower bounds for conservative linear bandits,
tabular RL and low-rank MDP, through a black-box reduction that turns a certain
lower bound in the nonconservative setting into a new lower bound in the conserva-
tive setting. For upper bounds, in multi-armed bandits, linear bandits and tabular
RL, our new upper bounds tighten or match existing ones with significantly simpler
analyses. We also obtain a new upper bound for conservative low-rank MDP.
1	Introduction
This paper studies online sequential decision making problems such as bandits and reinforcement
learning (RL) subject to a conservative constraint. Specifically, the agent is given a reliable baseline
policy that may not be optimal but still satisfactory. In conservative bandits and RL, the agent is
asked to perform nearly as well (or better) as the baseline policy at all time. This setting is a natural
formalization of many real-world problems such as digital marketing, healthcare, finance, etc. For
example, a company may want to explore new strategies to maximize profit while simultaneously
maintaining profit above a fixed baseline at any time, in order not to be bankrupted. See (Wu et al.,
2016) for more discussions on the motivation of the conservative constraint.
Analogously to the non-conservative case, conservative bandit/RL problems also require us to balance
exploration and exploitation carefully. Meanwhile, to ensure the obtained policies outperform the
baseline policy, we need to provide a tractable approach to keep the exploration not too aggressive.
Solving these two problems simultaneously is the key challenge in conservative bandits and RL.
Existing work proposed algorithms for different settings, including bandits (Wu et al., 2016; Kazerouni
et al., 2016; Garcelon et al., 2020b; Katariya et al., 2019; Zhang et al., 2019; Du et al., 2020; Wang
et al., 2021) and tabular RL (Garcelon et al., 2020a). However, lower bound exists only for the multi-
* equal contribution
1
Published as a conference paper at ICLR 2022
armed bandit (MAB) setting (Wu et al., 2016), and there is no lower bound for other widely-adopted
settings, such as linear bandits, tabular Markov Decision Process (MDP) and low-rank MDP. In
Section 1.3, we provide a more detailed discussion of the related work.
For each of the different settings considered in the literature (i.e., multi-armed bandits, linear bandits,
tabular MDPs), existing approaches rely on ad-hoc algorithm design and analysis of the trade-off
between the setting-specific regret analysis and the conservative constraint. Furthermore, it is hard to
argue about the optimality of the proposed algorithms because it would require clever constructions
of the hard instances to prove the non-trivial regret lower bounds under the conservative constraint.
1.1	Our Contributions
In this paper, we address these limitations and make significant progress in studying the general
problem of online sequential decision-making with conservative constraint. We propose a unified
framework that is generally applicable to online sequential decision-making problems. The common
theme underlying our framework is to calculate the necessary and sufficient budget required to enable
non-conservative exploration. Such a budget is obtained by running the baseline policy (cf. Section 3).
With the new framework, we obtain a novel upper bound on tabular MDPs, which improves the
previous result. And we prove a new upper bound on low-rank MDPs. Also, we derive the first lower
bounds for linear bandits, tabular and low-rank MDPs, which shows that our upper bound is tight.
Lower Bounds. For any specific problem (e.g., multi-armed bandits, linear bandits), our framework
immediately turns a minimax lower bound of the non-conservative setting to a non-trivial lower
bound for the conservative case (cf. Section 4). We list some examples to showcase the power of our
framework for lower bounds. Full results are given in Table 1.
•	We derive a novel lower bound for multi-armed bandits that works on a wider range of parameters
than the one derived in (Wu et al., 2016). In particular, our lower bound shows a more refined
dependence on the value of the baseline policy.
•	We derive the first regret lower bound for conservative exploration in linear bandits, tabular MDPs
and low-rank MDPs. These results allow to establish or disprove the optimality of the algorithms
currently available in the literature.
We emphasize our technique for deriving lower bounds is simple and generic, so we believe it can be
used to obtain lower bounds for other problems as well.
Upper Bounds. Our novel view of conservative exploration can also be used to derive high probability
regret upper-bounds. When the suboptimality gap ∆° and the expected return μo of the baseline
policy are known, we show that the Budget-Exploration algorithm (Alg. 1) attains minimax optimal
regret in a wide variety of sequential decision-making problems, when associated to any minimax
optimal non-conservative algorithm specific to the problem at hand. In the more realistic (and
challenging) scenario where ∆0 and μo are unknown, We show how to simply convert an entire class
of algorithms with a sublinear non-conservative regret bound into a conservative algorithms with a
sublinear regret bound. We obtain the following results, full details are given in Table 1.
•	In the MAB setting, we obtain a regret upper-bound that matches our refined lower-bound, thus
improving on existing analysis. In the linear bandit setting, we match existing bounds that are
already minimax optimal.
•	In the RL setting, we provide two novel results. First, we provide the first minimax optimal result
for tabular MDPs, improving over (Garcelon et al., 2020a). Second, we derive the first upper
bound for conservative exploration in low-rank MDPs. Our bound matches the rate of existing
non-conservative algorithms though it is not minimax optimal. How to achieve minimax optimality
in low rank MDPs is an open question even in non-conservative exploration.
Again, our reduction technique is simple and generic, and can be used to obtain new results in
previously unstudied settings, like we did for low-rank MDPs.
1.2	Main Difficulties and Technique Overview
1.2.1	Lower B ounds
The only lower bound for conservative exploration is by Wu et al. (2016) who followed a classical
approach in the bandit literature. They constructed a class of hard environments and used an
2
Published as a conference paper at ICLR 2022
Setting	Lower Bound		Upper Bound	
Multi-armed bandits	ω (√at + e≡f;		O(√AT+	
	ω(√at+就) (Wu et al., 2016) 2		O (√at + α⅛) (WUet al., 2016)	
Linear bandits	ω (d√T + αμc,dμ∆ + ∆0))		O (d√τ+F d2* J I V	' αμo(αμo +δ0) J This work and (Kazerouni et al., 2016; Garcelon et al., 2020b)	
Tabular MDPs	Ω (√H3SAT +	sAH3∆0、) ∖	ɑμ0(α0 +∆0))		O(√H 3SAT'+	sAH3∆0 Q ∖	^0(^020)/		
			O(√H 3SAT +	S2AH∆ 寸 ∖	αμ0(αμ0+∆0 ) J (Garcelon et al., 2020a)	
Low Rank MDPS	Ω (√dPH3T + d：H£、) ∖	αμ0(αμ0 + ∆0) J		Oe (√d3H4T +	d3H&、) ∖	αμ0(αμ0+∆0) J	
Table 1: Comparison of bounds for conservative decision-making. Our contributions are reported
in grey cells. We denote by T the number of rounds the agent plays (episodes in RL), α the
conservative level, μo the expected return of the baseline policy3, ∆0 the suboptimality gap of the
baseline policy, A the number of actions (or arms), S the number of states and d the feature dimension.
The upper bounds hold both in the case ∆0 and μo are unknown since the lack of knowledge changes
the regret only by a constant multiplicative factor (cf. Section 5).
information-theoretic argument to prove the lower bound. Construction of hard environments is
highly non-trivial because one needs to incorporate the hardness from the conservative constraint. It
is also non-trivial to generalize Wu et al. (2016)’s lower bound to other settings such as conservative
linear bandits and RL because one will need new constructions of hard environments for different
settings. We note that new constructions are needed even for non-conservative settings, because
simply embedding the hard instances of MAB to other settings cannot give the tightest lower bounds.
See, e.g., Chapter 24 of Lattimore & Szepesvðri (2020) and Domingues et al. (2021).
In this paper, We use a completely different approach. Our key insights are 1) relating the necessary
budget to the regret lower bounds of non-conservative sequential decision-making problems,
and 2) obtaining sharp lower bounds in the conservative settings via maximizing a quadratic
function (cf. Equation (6)). Comparing with the classical approach, our approach is simpler and
more general: ours does not need problem-specific constructions and can automatically transform any
lower bound in a non-conservative problem to the corresponding conservative problem. See Section 4
for details.
1.2.2	Upper Bounds
Improvement over Wu et al. (2016) when ∆0 is known. When ∆0 is known, Wu et al. (2016)
proposed an algorithm (BudgetFirst) which first plays the baseline policy for enough times and then
plays an non-conservative MAB algorithm. However, their regret bound is not tight because their
analysis on the required budget is loose: they accumulate enough budget to play T -step exploration
where T is the total number of rounds. Our main technical insight to obtain the tight regret bound
is a sharp analysis on the required budget: by relating the minimax regret upper bounds of UCB
algorithms, we show the required budget can be independent of T . See Section 5 and F for details.
Sharp upper bounds with unknown ∆0. When ∆0 is unknown, the paper by Wu et al. (2016), its
follow-up papers (Kazerouni et al., 2016; Garcelon et al., 2020b; Zhang et al., 2019; Garcelon et al.,
2020a), and our paper, all adopt the same algorithmic template: 1) build an online estimate on the
lower bound performance of each possible exploration policy, and 2) based on the estimated lower
bounds, choose an exploration policy or play the baseline policy.
2Although the lower bound in WU et al. (2016) seems tighter, they require a condition αμ∆+∆cι ≥ 0.9∙ Under
this condition, our lower bound is the same as theirs. Thus ours is more general. See Appendix E.
3In (Garcelon et al., 2020a), the upper bound scales with rb = mins∈S,ρ0(s)>0 V1π0 (s) (with ρ0 the
distribution of the starting state), the minimum of the baseline’s value function at the first step over the potential
starting states.Here, we assume there is a unique starting state hence rb = V π0.
3
Published as a conference paper at ICLR 2022
The key difference and the most non-trivial part in different papers is how to analyze T0 (the number
of times of executing the baseline policy). Existing works upper bound T0 by relating it to the
decision criterion for whether to choose the baseline policy or not. Since for different problem
settings, the criteria have different forms, existing papers adopt different problem-specific analyses,
and in some settings, the analyses are not tight (e.g., MAB and tabular RL). Our analysis approach
is different from existing ones: we bound T0 via maximizing a quadratic function that depends
on the minimax regret bounds of non-conservative algorithms and the conservative constraint.
See Section 5 for more details.
1.3	Related Work
Non-conservative exploration has been widely studied in bandits, and minimax optimal algorithms
have been provided for the settings considered in this paper (e.g. Lattimore & SzePesV制,2020).
The exploration problem has been widely studied also in RL but minimax optimal algorithms have
not been provided for all the settings. For any finite-horizon time-inhomogeneous MDP with S
states, A actions and horizon H, the minimax regret lower bound is Ω(√H3SAT) (Domingues et al.,
2021), where T denotes the number of episodes. For any time-inhomogeneous low-rank MDP with
d-dimensional linear representation, the lower-bound is Ω( Jd2H3T) (Zhou et al., 2020, Remark
5.8). While several minimax optimal algorithms have been provided for tabular MDPs (e.g. Azar
et al., 2017; Zanette & Brunskill, 2019; Zhang et al., 2020a;b; Menard et al., 2021), the gap between
upper bound and lower bound is still open in low-rank MDPs, where LSVI-UCB (Jin et al., 2020)
attains a O(√d3H4T), while ELEANOR (Zanette et al., 2020) improves to O(√d2H4T).
In conservative exploration, previous works focus on designing specific conservative algorithms for
different settings. This conservative scenario was studied in multi-armed bandits (Wu et al., 2016),
contextual linear bandits (Kazerouni et al., 2016; Garcelon et al., 2020b), contextual combinatorial
bandits (Zhang et al., 2019) and tabular MDPs (Garcelon et al., 2020a). All these works focused
on providing an upper-bound to the regret of a conservative algorithm. Other problems that have
been considered in conservative exploration are combinatorial semi-bandit with exchangeable ac-
tions (Katariya et al., 2019) and contextual combinatorial cascading bandits (Wang et al., 2021). Du
et al. (2020) have recently considered conservative exploration with sample-path constraint.
Our work is also related to safe bandits/RL (Amani et al., 2019; Pacchiano et al., 2021; Amani et al.,
2021) and constrained RL (Altman, 1999; Efroni et al., 2020; Ding et al., 2020; 2021; Chen et al.,
2020). The setting of safe bandits/RL is different from conservative bandits/RL. Specifically, the
safety constraint requires that the expected cost at each stage is below a certain threshold. This
constraint is stage-wise, and is independent of the history. On the contrary, the conservative constraint
requires that the total reward is not too small. For the constrained MDP, the goal is to maximize
the expected reward value subject to a constraint on the expected utility value (value function with
respect to another reward function). In conservative RL, however, the agnet aims to maximize the
expected reward value subject to the constaint that the (same) reward value is not significantly worse
that of the baseline policy.
2	Preliminaries
The objective of this section is to provide a unified view of the settings considered in this paper, i.e.,
multi-armed bandits, linear bandits, tabular Markov Decision Processes (MDPs) and low-rank MDPs.
We use the RL formalism which encompasses the bandit settings.
Notations. We begin by introducing some basic notation. We use ∆(∙) to represent the set of all prob-
ability distributions on a set. For n ∈ N+, we denote [n] = {1,2,..., n}. We use O(∙), Θ(∙), Ω(∙)
to denote the big-O, big-Theta, big-Omega notations. We use O(∙) to hide logarithmic factors. We
denote A & (.)B if there exists a positive constant c such that A ≥ (≤)cB.
Tabular MDPs. A tabular finite-horizon time-inhomogeneous MDP can be represent as a tuple
M = (S, A, H, {ph}hH=1, s1, {rh}hH=1), where S is the state space, A is the action space, H is the
length of each episode and s1 is the initial state. At each stage h, every state-action pair (s, a) is
characterized by a reward distribution with mean rh(s, a) and support in [0, rmax], and a transition
distribution Ph(∙∣s, a) over next states. We denote by S = |S| and A = |A|. A (randomized) policy
π ∈ Π is a set of functions {πh : S 7→ ∆(A)}h∈[H]. For each stage h ∈ [H] and any state-action
4
Published as a conference paper at ICLR 2022
pair (s, a) ∈ S × A, the value functions of a policy π are defined as:
H
Qhπ(s, a) = E	rh0 |sh = s, ah = a,π
h0=h
H
Vhπ(s) = E X rh0 |sh = s,π
h0=h
For each policy π, we define VHπ+1(s) = 0 and QπH+1(s, a) = 0 for all s ∈ S, a ∈ A. There exists
an optimal policy π? such that Q?h(s, a) = Qπh? (s, a) = maxπ Qhπ (s, a) satisfy the optimal Bellman
equations Qh(s,a) = rh(s,a) + E§o〜?,由。)[Vh+ι(s0)] and Vh = maXα∈A{Qh(s,a)}. Then the
optimal policy is the greedy policy π?(S) = argmaXa∈∕{Qh(s, a)}.
Low-Rank MDPs. We assume that S , A are measurable spaces with possibly infinite number of
elements. For algorithmic tractability, we shall restrict the attention to A being a finite set with
cardinality A. When the state space is large or uncountable, value functions cannot be represented
in tabular form. A standard approach is to use a parametric representation. Here, we assume that
transitions and rewards are linearly representable (Jin et al., 2020).
Assumption 1 (Low-rank MDP). An MDP (S, A, H, p, r) is a linear MDP with a feature map
φ : S× A → Rd, ifforany h ∈ [H], there exist d unknown (Signed) measures μh = (μh1),..., μ,)
over S and an unknown vector θh ∈ Rd, such that for any (x, a) ∈ S × A, we have
Ph(∙ | x,a) = hφ(X, a), NhlTi, rh(x, a) = hφ(X, a), θhi .	(I)
Without loss of generality, we assume kφ(x, a)k ≤ 1 for all (x, a) ∈ S × A, and
max {kμh(S)k , kθhk} ≤ √dforall h ∈ [H].
Under certain technical conditions (e.g., Shreve & Bertsekas, 1978), all the properties of tabular
MDPs extend to low-rank MDPs. In addition, the state-action value function of any policy π is
linearly representable in low-rank MDPs. Formally, for any policy π and stage h ∈ [H], there exists
θhπ ∈ Rd suchthatQπh(s,a) = hφ(s, a), θhπ i.
Connection between RL and Bandits. To have a unified view, we can represent a multi-armed
bandit as a tabular MDP with S = 1, A actions, H = 1 and self-loop transitions in s1. In multi-
armed bandits, we consider only deterministic policies so that Π = A, then Vπ(s1) = r(s1, π(s1))
and the optimal policy is simply π? = arg maxa∈A r(s1, a). Similarly, a linear bandit can be
modeled through low-rank MDPs with H = 1. For generality, we allow the action space to be
possibly uncounted and we define the value of a deterministic policy π = a (Π = A) as V1π(s1) =
r1(s1, a) = hφ(s1, a), θ1i. The optimal policy π? is thus such that π? = argmaxa∈Ahφ(s1, a), θ1i.
We refer the reader to Appendix A for details.
3	General Framework For Conservative Exploration
With the unified view provided in the previous section, we can consider a generic sequential decision-
making problem P over T ∈ N? episodes. We consider the standard online interaction protocol
where, at each episode t ∈ [T], the learning agent A selects a policy πt, observes and stores a
trajectory (si, ai, ri)i∈[H] , updates the policy and restart with the next episode. We evaluate the
performance of the learner through the pseudo-regret. Let Vπ = V1π(s1) be the value function of a
policy π, then the regret is defined as:
T
RT (P, A) = XV? -Vπt.	(2)
t=1
In conservative exploration, the learner aims to minimize the regret while guaranteeing that, at any
episode t, their expected performance is (nearly) above the one of a baseline policy π0 . Formally,
given a possibly randomized baseline policy π0 ∈ Π and a conservative level α ∈ [0, 1], the learner
should satisfy w.h.p. that
t
∀t ≤ T,	XVπj ≥ (1 - α) t Vπ0.
j=1
(3)
5
Published as a conference paper at ICLR 2022
We assume that the value of conservative policy V π0 is known to the agent. Such assumption can be
seen in previous works such as Wu et al. (2016); Kazerouni et al. (2016); Garcelon et al. (2020b;a).
This assumption is reasonable in practice because usually the baseline policy has been used for a long
time and is well-characterized, and its value can be estimated using historical data. Even if we do not
know the value of baseline policy, we can estimate it during the algorithm (e.g., Section 3.5 in Wu
et al. (2016)), and we omit here for simplicity.
3.1	Budget of a Conservative Algorithm
Given the set of policies {πt}t∈[T] selected by a conservative algorithm A, we can divide the episodes
into the set T0 = {t ≤ T | πt = π0} and its complement T0c = {t ≤ T | πt 6= π0} = [T] \ T0 . The
set T0c denotes the episodes where the algorithm played an exploratory policy, i.e., it had enough
budget to satisfy condition (3) through a policy πl 6= π0 . This sequence of non-baseline policies
{πt}t∈T c defines a new algorithm A, that we refer as the non-conservative algorithm. However, the
algorithm A is conservative therefore, for any δ > 0 and t ∈ [T], we have with probability at least
1 - δ thatPlt=1 Vπl ≥ (1 - α)tV π0. Hence, for any t ∈ [T] we have:
αVπ0|T0,t| ≥ X (1-α)Vπ0 - Vπl,	(4)
l∈T0c,t
where T0,t = T0 ∩ [t] and T0c,t = T0c ∩ [t]. Taking maximum over t in Eq. (4), we have that with high
probability the conservative algorithm A is such that
aVπ0 |T0| ≥ max X (1 - α)Vπ0 - Vπl .
t≤T l∈T0c,t
'------------{z------------}
=B
The quantity on the right of the previous equation is exactly the amount of reward that the conservative
algorithm A has to collect by playing the baseline policy. Hence this quantity acts as a conservative
budget B. The higher it is, the more A needs to play the baseline policy to satisfy the conservative
condition. In other words, it is the least amount of reward that an algorithm needs to not violate the
conservative constraint. We now extend this notion to any (non necessarily conservative) algorithm.
Definition 1. For any T ∈ N?, set of episodes O ⊂ [T] and arbitrary sequence of policies {πt}t∈O,
the budget of this sequence of policies is defined as:
BT(O,{πt}t∈O)=max X (1-α)Vπ0 -Vπl.	(5)
∈ l∈O∩[t]
4	Regret Lower B ound for Conservative Exploration
In this section, we leverage the framework introduced in Section 3 to build lower bounds for several
problems. Our result is based on the notion of budget defined in Section 3. This notion is used to
build an algorithm whose regret is a lower bound for any conservative algorithm.
Theorem 1 (Conservative Exploration Regret Lower Bound). Let’s consider a decision-making
problem P over T steps, a conservative level α ∈ [0, 1], a baseline policy π0, an algorithm A and
δ ∈ (0, 1). We assume that:
•	Lower-bound for non-conservative exploration. There exists a ξ ∈ R+ and T0 ∈ N such that for
any algorithm A0 there exists an environment (instance of P) such that with probability at least
1 - δ, Rt(P, A0) ≥ ξ√T for T ≥ To.
•	A is conservative. The algorithm A is conservative, that is to say with probability at least 1 - δ
for any t ≤ T, Plt=1 Vπl ≥ (1 - α)tVπ0.
Then, there exists an environment (instance of problem P) and T0 ∈ N such that with probability at
least 1 - δ and T ≥ T0 :
RT(A, P) & max (ξ√T, 一 £^~-- O.
T	αVπ0 (αVπ0 + ∆0)
where ∆0 = V? - Vπ0 is the sub-optimality gap of policy π0.
6
Published as a conference paper at ICLR 2022
Theorem 1 provides a general framework deriving lower-bounds for conservative exploration and
highlights the impact of the baseline policy on the regret. In particular, it shows that in any sequential
decision-making problem, after a sufficiently large number of episodes the conservative condition
can be verified and the baseline policy has no impact anymore on the learning process. The only
requirement is the knowledge of a lower-bound for the non-conservative case. Before instantiating
the result in specific settings, we provide an intuition about how this result is derived and what is the
role of the conservative budget B.
Proof Sketch. Let us consider a conservative algorithm A = {πt | t ≤ T }, which is associated to
a non-conservative algorithm A = {πt | t ∈ T0c} with T0c and T0 the set of non-conservative and
conservative episodes as defined in Sec. 3. Now if E |T0| ≥「v∏o∙(cξV∏o +△ο)(i.e. the algorithm
plays ∏o too many times), then the regret caused by ∏o is at least「v∏o∙ξ2∆∏o +δc,) . When E |T0| <
αV∏o ∙(αξV∏o +∆o) , consider the budget of T0 defined in Definition 1:
BTc(Ac) = max E X[(1 -α)Vπ0 - Vπt] = maxE[RTA0c(Ac)(t)] - (αV π0 + ∆o)t,	(6)
0	t∈T0c k=1	t∈T0c
where E [rA0 (AJ(t)] is the regret incurred by the rounds in Tq . Now if E [RT0 (AJ(t)] ≥ ξ√t,
we have BTOC (Ac) &av∏ξ0 +△ by taking maximum on the right handside of (6) (viewing RHS as a
quadratic function of √t). Therefore E ∣To∣ ≥ BOVAC) &「v∏o∙(JV∏o +△)and the regret is also no
smaller than「v∏o ∙ξ2∆∏o +△/, which completes the proof.
Example of Lower Bounds. For instance, in the multi-armed bandits, by leveraging the lower-bound
in (Thm. 15.2 Lattimore & Szepesvdri, 2020), we can obtain the following corollary of Theorem 1.
This result is more general than the lower bound in Wu et al. (2016) where they have a restriction that
αμ∆+∆ ≥ 0.9. See Appendix E for details.
Corollary 1. For any K ∈ N?, a ∈ [0,1], μo ∈ [0,1], δ ∈ (0,1) and a conservative algorithm A
then there exists μ ∈ [0,1]K such that Pt=1 μ∏l ≥ (1 一 a)μot with high probabilityfor any t ≤ T.
Then, for T ≥-----(ALA 、 +-√A , RT(μ, A) & max [ √AT,---------------A△葭八、O.
αμo ∙(αμo + ∆o) αμo+∆o T ,	, 「μo∙(αμo+∆o) J
The generality of Theorem 1 allows us to derive lower-bounds for conservative exploration in many
different problems, where the lower-bound was unknown. Table 1 reports the lower-bound obtained
through Theorem 1. Please refer to Appendix B for lower-bounds for non-conservative exploration.
In linear bandits, the lower bound we obtain matches the result in (Kazerouni et al., 2016; Garcelon
et al., 2020b), showing the optimality of their algorithms. In tabular MDPs, our result shows that
the dependence on S, Aand H of CUCBVI (Garcelon et al., 2020a) is not optimal. Finally, by
instantiating Theorem 1 in low-rank MDPs, we obtain the first lower bound for this setting.
5	Upper Bounds
In this section, we show how to leverage the framework of Sec. 3 to derive an algorithm for any
conservative sequential decision-making problem. We first show that when knowing ∆o a simple
algorithm achieves a minimax regret, as prescribed by our lower bound of Sec. 4. Then, we show
how to remove this knowledge without hurting the performance by combining our framework and the
idea of lower confidence bound.
5.1	THE Budget-Exporation ALGORITHM
Given a non-conservative algorithm A, the minimum amount of rewards needed to play this non-
conservative algorithm for T consecutive steps is the budget defined in Def. 1. Indeed, if we denote
by {∏ι | l ≤ T} the sequence of non-conservative policies executed by A, then for any set O ⊂ [T]
the budget can be rewritten as:
BT(O, {∏l | l ≤ T})=max X (1 一 α)Vπ0 一 Vπι
t∈O l∈O∩[t]
7
Published as a conference paper at ICLR 2022
Algorithm 1: Budget-Exporation
Input: A non-conservative algorithm A, conservative policy cumulative reward V π0 ,
conservative level: α ∈ (0, 1) ,baseline action gap: ∆0 = V ? - V π0 and a constant C
1	Set B = αVC + ∆0 and TO = αVB∏0 ;
2	for t = 1, . . . , T do
3	if t < T0 then
4	I Play ∏o;
5	else
6	I Play according to A;
7	end
8	end
max X V? - Vπl - (∆0 +αVπ0)O∩ [t] .
t∈O l∈O∩[t]
Let’s define RO∩[t] (A) :=	l∈O∩[t] V ? - Vπl the regret over the time steps in O of the non-
conservative algorithm Ae . For most non-conservative algorithms with minimax regret bound,
RT (A, O) = O(C |O ∩ [t]|) w.h.p., where C ∈ R is a problem-dependent quantity as in Theorem 1.
For example, in multi-armed bandit C = √A for the UCB algorithm or C = √H3SA for the UCBVI-
C2
BF algorithm (Azar et al., 2017). This implies that the budget required by A is at least 4°+^∏0.
Therefore, the simple algorithm playing the baseline policy for the first T0 := O(叱∏0 +∆c,)αv∏0)
steps and then running the non-conservative algorithm A, is conservative. We call such algorithm
Budget-Exporation (see Alg. 1). This algorithm is conservative and minimax optimal. Indeed, we
can show (see Theorem 2) that the regret upper bound of Budget-Exporation matches the lower
bounds of Section 4. While knowing ∆0 in advance may be a restrictive assumption, it is interesting
that a two-stage algorithm structure (deploying a baseline policy and then a non-conservative policy)
is enough to achieve minimax optimality.
Theorem 2. Consider an algorithm A, δ ∈ (0, 1) and constant C ∈ R such that with probability
at least 1 一 δ, for any T ≥ 1, RT(A) ≤ O(C√T). Then for any T ≥ 1, the regret of Budget-
EXPoration is bounded with probability at least 1 一 δ by O(C√T + _v∏0 CVlO +∆c,)).
Instantiating Thm. 1 with A being the UCB algorithm (Lattimore & Szepesvari, 2020), then C = √A
and the regret of Budget-EXPoration is bounded w.h.p. by O(√AT + ：v∏0AΟ∆0+∆c,)), that matches
our novel lower bound introduced in Sec. 4. Similar results can be obtained for the other settings,
see Table 1. In linear bandit We consider LinUCB as the non-conservative algorithm, leading to
C = d. Similarly, in tabular MDP and low-ran MDPs, we get C = √H3SA and C = √d3H4
respectively using UCBVI-BF (Azar et al., 2017) and LSVI-UCB (Jin et al., 2020). Refer to Table 1
for a complete comparison of the results.
5.2 THE LCBCE ALGORITHM
When ∆0 is unknown, we aim to use the same idea as Budget-EXPoration, that is to say to play
a policy different than the baseline one only if the budget is positive. To achieve this, we need to
build an online estimate of the conservative budget which amounts to build a lower confidence bound
(w.h.p.) on the value function of any policy π. Therefore, assuming a non-conservative algorithm A
builds such confidence bounds, for example by estimate the MDP as done by Garcelon et al. (2020a),
we show how our budget framework helps to derive a conservative regret bound.
Let’s consider a non-conservative algorithm A = {πt | t ≤ T} able to construct a high probability
lower bound on the set of selected policies. That is, for any time t ≤ T and δ ∈ (0, 1), A computes
a sequence of real numbers (λtπk (δ))k≤t such that with probability at least 1 一 δ, for all k ≤ t,
λtπk (δ) ≤ V πk . Using these lower bounds, we can define a proxy to the budget for BeT,δ(O, Ae) for
8
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
Algorithm 2: Lower Confidence Bound for Conservative Exploration
π
Input: A non-conservative algorithm A, δ ∈ (0, 1), lower confidence bounds λtπk ≤ Vπk,
conservative policy value V π0, α ∈ (0, 1)
Set B = 0 ;	// the accumulated budget
0
Set t0 = 0 ;	// the number of steps in which the agent acts as A
for t = 1, 2, ..., T do
A gives lower bound Xt，+i and a policy n4+i；
t0	^ɔt0 ∖∏K	,∖πt0+1.,,r
Set λ = k 1 λt0k 1 + λt0t +11 ； // lower bound of expected total reward
if λ - (t0 +k1=)1αVt π+01 < Btt+h1en
Play ∏t = ∏o and set B = B + αVπ0;
else
Play ∏t = ∏to+ι and set t0 = t0 + 1；
end
end
any subset O ⊂ [T] by
Bτ,δ (o, A) =m∈aχ X ((1-α)V π0-λ∏l(δ)),	(7)
l∈O∩[t]
with (πl)l∈O the sequence of policies computed by the non-conservative algorithm A. Then
following from the definition of (λtπl (δ))l≤t, we have that with probability at least 1 - δ that
BeT,δ O, Ae ≥ BT (O, Ae). This shows that it is possible to compute BeT,δ O, Ae without knowl-
edge of the environment and the baseline parameters. The idea of our algorithm is now to play a
non-conservative policy πt at time t only if the difference between the proxy to the budget of A
and the reward accumulated by playing the baseline policy is negative. Formally, the condition is
Bet,δ St ∪ t, Ae ≤ αVπ0 (t - 1 - |St|) where St is the set of time step where a non-conservative
policy was deployed in episodes before t. As a result, the minimum budget that A requires to be
conservative is maxt Bet,δ (St ∪ t, A) = maxt∈[τ] P,∈st ((1 - a)Vπ0 - λ∏l (δ)). The algorithm,
called Lower Confidence Bound for Conservative Exploration (LCBCE), is detailed in Alg. 2.
Next, we show the regret bound of LCBCE. The proof is in Appendix D.
〜
Theorem 3. Consider an algorithm A, δ ∈ (0, 1) and constant C ∈ R such that with probability at
least 1 — δ, for any T ≥ 1, RT(A) ≤ O(C√T).
If A computes lower confidence bound such that
Pk=i (V πk — λ∏k) ≤ O(C √T) with probability at least 1 — δ, then for any T ≥ 1, the regret of
LCBCE is bounded with probability at least 1 — δ by O(C√T + _v∏0CV0 +δc,)).
In the MAB and tabular case, LCBCE paired with UCB achieves a better regret bound compared
with previous papers(Garcelon et al., 2020a； Wu et al., 2016). We also provide the first minimax
optimal bound for the case of unknown baseline parameters. Finally, in low rank MDPs we recover
the same rate as in the case of known baseline. See Table 1.
6 Conclusion
We present a unified framework for conservative exploration in sequential decision-making problems.
This framework can be leveraged to derive both minimax lower and upper bounds. In bandits, we
provide novel lower bounds that highlighted the optimality of existing algorithms. In RL, we provide
the first lower bound for tabular MDPs and a matching upper bounds, and the first analysis for low
rank MDPs. An interesting question is whether one can leverage this framework to derive problem-
dependent logarithmic bounds for conservative exploration. Another direction is to investigate
model-free algorithms (e.g., Q-learning (Jin et al., 2018)) for conservative exploration.
9
Published as a conference paper at ICLR 2022
Acknowledgements
Liwei Wang was supported by National Key R&D Program of China (2018YFB1402600), Exploratory
Research Project of Zhejiang Lab (No. 2022RC0AN02), BJNSF (L172037). Project 2020BD006
supported by PKUBaidu Fund.
References
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Linear stochastic bandits under
safety constraints. In NeurIPS, pp. 9252-9262, 2019.
Sanae Amani, Christos Thrampoulidis, and Lin F Yang. Safe reinforcement learning with linear
function approximation. arXiv preprint arXiv:2106.06239, 2021.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pp. 263-272. PMLR,
2017.
Xiaoyu Chen, Jiachen Hu, Lihong Li, and Liwei Wang. Efficient reinforcement learning in factored
mdps with application to constrained rl. In International Conference on Learning Representations,
2020.
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo R. Jovanovic. Natural policy gradient
primal-dual method for constrained markov decision processes. In NeurIPS, 2020.
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo R. Jovanovic. Prov-
ably efficient safe exploration via primal-dual policy optimization. In AISTATS, volume 130 of
Proceedings of Machine Learning Research, pp. 3304-3312. PMLR, 2021.
Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic re-
inforcement learning in finite mdps: Minimax lower bounds revisited. In ALT, volume 132 of
Proceedings of Machine Learning Research, pp. 578-598. PMLR, 2021.
Yihan Du, Siwei Wang, and Longbo Huang. A one-size-fits-all solution to conservative bandit
problems. CoRR, abs/2012.07341, 2020.
Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps.
CoRR, abs/2003.02189, 2020.
Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Conservative
exploration in reinforcement learning. In International Conference on Artificial Intelligence and
Statistics, pp. 1431-1441. PMLR, 2020a.
Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Improved
algorithms for conservative exploration in bandits. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 3962-3969, 2020b.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is q-learning provably efficient?
In NeurIPS, pp. 4868-4878, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Sumeet Katariya, Branislav Kveton, Zheng Wen, and Vamsi K. Potluru. Conservative exploration
using interleaving. In AISTATS, volume 89 of Proceedings of Machine Learning Research, pp.
954-963. PMLR, 2019.
Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi-Yadkori, and Benjamin Van Roy. Con-
servative contextual linear bandits. arXiv preprint arXiv:1611.06426, 2016.
10
Published as a conference paper at ICLR 2022
Tor Lattimore and Csaba Szepesvdri. Bandit algorithms. Cambridge University Press, 2020.
Pierre Menard, Omar Darwiche Domingues, XUedOng Shang, and Michal Valko. UCB momentum
q-learning: Correcting the bias without forgetting. CoRR, abs/2103.01312, 2021.
Aldo Pacchiano, Mohammad Ghavamzadeh, Peter L. Bartlett, and Heinrich Jiang. Stochastic bandits
with linear constraints. In AISTATS, volume 130 of Proceedings of Machine Learning Research,
pp. 2827-2835. PMLR, 2021.
Steven E Shreve and Dimitri P Bertsekas. Alternative theoretical frameworks for finite horizon
discrete-time stochastic optimal control. SIAM Journal on control and optimization, 16(6):953-
978, 1978.
Kun Wang, Canzhe Zhao, Shuai Li, and Shuo Shao. Conservative contextual combinatorial cascading
bandit. CoRR, abs/2104.08615, 2021.
Yifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesvdri. Conservative bandits. In Interna-
tional Conference on Machine Learning, pp. 1254-1262. PMLR, 2016.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In ICML, volume 97 of Proceed-
ings of Machine Learning Research, pp. 7304-7312. PMLR, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel J. Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. In ICML, volume 119 of Proceedings of Machine
Learning Research, pp. 10978-10989. PMLR, 2020.
Xiaojin Zhang, Shuai Li, and Weiwen Liu. Contextual combinatorial conservative bandits. CoRR,
abs/1911.11337, 2019.
Zihan Zhang, Xiangyang Ji, and Simon S Du. Is reinforcement learning more difficult than bandits?
a near-optimal algorithm escaping the curse of horizon. arXiv preprint arXiv:2009.13503, 2020a.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learningvia
reference-advantage decomposition. In NeurIPS, 2020b.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvdri. Nearly minimax optimal reinforcement learning
for linear mixture markov decision processes. CoRR, abs/2012.08507, 2020.
11