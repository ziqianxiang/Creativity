Published as a conference paper at ICLR 2022
Online Target Q-learning with Reverse Expe-
rience Replay: Efficiently finding the Optimal
Policy for Linear MDPs
Naman Agarwal, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli
Google Research
{namanagarwal,prajain,dheeraj,pnetrapalli}@google.com
Syomantak Chaudhuri
University of California, Berkeley
syomantak@berkeley.edu
Ab stract
Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely
deployed with function approximation (Mnih et al., 2015). In contrast, existing
theoretical results are pessimistic about Q-learning. For example, Q-learning does
not converge even with linear function approximation for linear MDPs ((Baird,
1995)) and even for tabular MDPs with synchronous updates, Q-learning has sub-
optimal sample complexity (Li et al., 2021; Azar et al., 2013). The goal of this
work is to bridge the gap between practical success of Q-learning and the relatively
pessimistic theoretical results. The starting point of our work is the observation
that in practice, Q-learning is used with two important modifications: (i) train-
ing with two networks, called online network and target network simultaneously
(online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al.,
2015). While they play a significant role in the practical success of Q-learning,
a thorough theoretical understanding of how these two modifications improve the
convergence behavior of Q-learning has been missing in literature. By carefully
combining Q-learning with OTL and reverse experience replay (RER) (a form of
experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+
data reuse). We show that Q-Rex efficiently finds the optimal policy for linear
MDPs (or more generally for MDPs with zero inherent Bellman error with lin-
ear approximation (ZIBEL)) and provide non-asymptotic bounds on sample com-
Plexity - the first such result for a Q-learning method for this class of MDPs un-
der standard assumptions. Furthermore, we demonstrate that Q-RexDaRe in fact
achieves near oPtimal samPle comPlexity in the tabular setting, imProving uPon
the existing results for vanilla Q-learning.
1	Introduction
Reinforcement Learning (RL) has been shown to be highly successful in Practice for a variety of
long term decision making Problems (Mnih et al., 2015). Several classical works have studied RL
methods like TD-learning, Q-learning and their variants (Sutton & Barto, 2018; Bertsekas, 2011;
Borkar & Meyn, 2000; Sutton, 1988; Tsitsiklis & Van Roy, 1997; Watkins & Dayan, 1992; Watkins,
1989) but the guarantees are mostly asymPtotic and therefore do not sufficiently answer imPortant
questions that are relevant to Practitioners who struggle with constraints on the number of data Points
and the comPutation Power. Recent works Provide non-asymPtotic results for a variety of imPortant
settings (Kearns & Singh, 1999; Beck & Srikant, 2012; Qu & Wierman, 2020; Ghavamzadeh et al.,
2011; Bhandari et al., 2018; Chen et al., 2020; 2019; Dalal et al., 2018a;b; Doan et al., 2020; GuPta
et al., 2019; Srikant & Ying, 2019; Weng et al., 2020; Yang & Wang, 2019; Zou et al., 2019a).
DesPite a large body of work, several asPects of fundamental methods like Q-learning (Watkins &
Dayan, 1992) are still ill-understood. Q-learning’s simPlicity and the ability to learn from off-Policy
1
Published as a conference paper at ICLR 2022
data makes it widely applicable. However, theoretical analyses show that even with linear function
approximation and when the approximation is exact, Q-learning can fail to converge even in simple
examples (Baird, 1995; Boyan & Moore, 1995; Tsitsiklis & Van Roy, 1996). Furthermore, even in
the simple case of tabular RL with synchronous updates, Q-learning is known to have sub-optimal
sample complexity (Wainwright, 2019a; Li et al., 2021).
However, Q-learning has seen tremendous practical success when deployed with “heuristic” mod-
ifications like experience replay (ER) and online target learning (OTL). ER is used to alleviate the
issues arising due highly dependent samples in an episode whereas OTL helps stabilize the Q iter-
ation. Mnih et al. (2015) conducted extensive experiments to show that both these techniques, are
essential for the success of Q-learning. But, existing analyses for ER with Q-learning either require
stringent assumptions (Carvalho et al., 2020) to ensure convergence to a good Q value, or assume
that ER provides i.i.d. samples which might not hold in practice (Fan et al., 2020; Carvalho et al.,
2020). In this paper, we attempt to bridge the gap between theory and practice, by rigorously in-
vestigating how Q-learning performs with these practical heuristics. We thus introduce two model
free algorithms: Q-Rex and Q-RexDaRe that combine the standard Q-learning with OTL and re-
verse experience replay (RER). RER is a form of ER which was recently studied to unravel spurious
correlations present while learning form Markovian data in the context of system identification (Jain
et al., 2021b). We show that OTL stabilizes the Q value by essentially serving as a variance reduc-
tion technique and RER unravels the spurious correlations present in the Markovian data to remove
inherent biases introduced in vanilla Q learning.
These simple modifications have surprisingly far-reaching consequences. Firstly, this allows us to
show that unlike vanilla Q-learning, Q-Rex finds the optimal policy for MDPs with an exact linear
function representation of the Bellman operator. and allows us to derive non-asymptotic sample
complexity bounds. In the tabular setting, Q-Rex even with asynchronous data is able to match the
best known bounds for Q-learning with synchronous data. Its variant Q-RexDaRe , which reuses
old samples, admits nearly optimal sample complexity for recovering the optimal Q-function in the
tabular setting. Previously, only Q-learning methods with explicit variance-reduction techniques (not
popular in practice) (Wainwright, 2019b; Li et al., 2020b) or model based methods (Agarwal et al.,
2020; Li et al., 2020a) were known to achieve such a sample complexity bound. Our experiments
show that when the algorithmic parameters are chosen carefully, Q-Rex and its variants outperform
both vanilla Q-learning and OTL+ER+Q-learning with the same parameters (see Appendix A).
To summarize, in this work, we study Q-learning with practical heuristics like ER and OTL, and
propose two concrete methods Q-Rex and Q-RexDaRe based on OTL and reverse experience
replay - a modification of the standard ER used in practice. We show that Q-Rex is able to find
the optimal policy for ZIBEL MDPs, with a strong sample complexity bound which is the first such
result for Q-learning. We also show that Q-RexDaRe obtains nearly optimal sample complexity for
the simpler tabular setting despite not using any explicit variance reduction technique. See Table 1
for a comparison of our guarantees against the state-of-the-results for the tabular setting.
Organization We review related works in next subsection. In Section 2 we develop the MDP
problem which we seek to solve and present our algorithm, Q-Rex in Section 3. The main theoretical
results are presented in Section 4. We present a brief overview of the analysis in Section 5 and
present our experiments in Section A. We provide minimax lower bounds for the asynchronous
tabular setting in Section K. Most of the formal proofs are relegated to the appendix.
1.1	Related Works
Tabular Q-learning Tabular MDPs are the most basic examples of MDPs where the state space
(S) and the action space (A) are both finite and the Q-values are represented by assigning a unique
co-ordinate to each state-action pair. This setting has been well studied over the last few decades and
convergence guarantees have been derived in both asymptotic and non-asymptotic regimes for popu-
lar model-free and model-based algorithms. Azar et al. (2013) shows that the minimax lower bounds
on the sample complexity of obtaining the optimal Q-function up-to e error is([―]',, where Y is
the discount factor. Near sample-optimal estimation is achieved by several model-based algorithms
(Agarwal et al., 2020; Li et al., 2020a) and model-free algorithms like variance reduced Q-learning
(Wainwright, 2019b; Li et al., 2020b). (Li et al., 2021) also shows that vanilla Q-learning with stan-
dard step sizes, even in the synchronous data setting - where transitions corresponding to each state
2
Published as a conference paper at ICLR 2022
Paper	Algorithm	Data Type	Sample Complexity
(GHAVAMZADEH ET AL., 2011)	SPEEDY Q-LEARNING	Synchronous	∣s∣∣A∣ 	e2(1-γ)4	
(WAINWRIGHT, 2019B)	Variance Reduced Q-learning	Synchronous	∣S∣∣A∣ e2(i-γ)3
(Li et al., 2020b)	Variance Reduced Q-learning	Asynchronous		1 “mine2(1-γ)3
(LI ET AL., 2020B)	Q-learning	Asynchronous	1 μmine2(1-γ)5
(LI ETAL., 2021)	Q-learning	Synchronous	∣s∣A∣ 	e2(1-γ)4	
This work, Theorem 2	-Q-LEARNING+ OTL- + RER (Q-REX)	Asynchronous	∣S∣∣A∣ e2(1-γ)4
This work, Theorem 3	Q-REX+ DATA-REUSE (Q-RexDaRe)	Asynchronous	max (d表) “min(1-γ)3
Table 1: Comparison of tabular Q-learning based algorithms. d ≤ |S| is maximum size of support
of P(∙∣s,α). In the case of asynchronous setting, μ1- is roughly equivalent to |S||A| in the syn-
chronous setting. We use the color green to represent results with optimal dependence on (1 - γ)-1.
action pair are sampled independently at each step - suffers from a sample complexity of(*最、
and the best known bounds in the asynchronous setting - where data is derived from a Markovian
trajectory and only one Q value is updated in each step - is /SYA：. These results seem unsatis-
factory since γ ∈ (0.99, 1) in most practical applications. In contrast, our algorithm Q-Rex with
asynchronous data has a sample complexity that matches Q-learning bound with synchronous data
and its data-efficient variant Q-RexDaRe has near minimax optimal sample complexity (see Ta-
ble 1). For details on model based algorithms, and previous works with sub-optimal guarantees we
refer to (Agarwal et al., 2020; Li et al., 2020b). We note that the lower bounds apply only to the
synchronous case (i.e, when every state-action pair is sampled at every step). We provide minimax
lower-bounds which show that the bound is tight in the asynchronous case too (see Theorem 5 in
Section K), where |S||A| in the synchronous case of (Azar et al., 2013) is replaced by .
Q-learning with Linear Function Approximation Since tabular Q-learning is intractable in most
practical RL problems due to a large state space S, function approximation is deployed. Linear
function approximation is the simplest such case where the Q-function is approximated with a linear
function of the ‘feature embedding’ associated with each state-action pair. However, Q-learning can
be shown to diverge even in the simplest cases as was first noticed in (Baird, 1995), which also
introduced residual gradient methods which converged rather slowly but provably. We will only
discuss recent works closest to our work and refer the reader to (Carvalho et al., 2020; Yang &
Wang, 2019) for a full survey of various works in this direction. SARSA is the on-policy control
variant of Q-learning where the challenge is to explore the state-space while learning the optimal
policy. Unlike Q-learning, SARSA is inherently stable due to its on-policy nature (Gordon, 2000).
Therefore, we do not compare our results to the results of on-policy control algorithms like SARSA.
We refer to (Zou et al., 2019b; Perkins & Precup, 2002; Melo et al., 2008) for further details.
Yang & Wang (2019) consider MDPs with approximate linear function representation. They require
additional assumptions like finite state-action space and existence of known anchor subsets which
might not hold in practice. Our results on the other hand hold with standard assumptions, with
asynchronous updates and can handle infinite state-action spaces (see Theorem 1). Similarly, Chen
et al. (2019) consider Q-learning with linear function approximation which need not be exact. But
the result requires the restrictive assumption that the offline policy is close to the optimal policy. In
contrast, we consider the less general but well-studied case of MDPs with zero inherent Bellman
error and provide global convergence without restrictive assumptions on the behaviour policy.
Under the most general conditions Maei et al. (2010) present the Greedy-GQ algorithm which con-
verges to a point asymptotically instead of diverging. Similar results are obtained by Carvalho et al.
(2020) for Coupled Q-learning, a 2-timescale variant of Q-learning which uses a version of OTL and
ER1. This algorithm experimentally resolves the popular counter-examples provided by (Tsitsiklis
& Van Roy, 1996; Baird, 1995). However, the value function guarantees in Carvalho et al. (2020,
1The version of ER used in Carvalho et al. (2020) makes the setting completely synchronous as opposed to
the asynchronous setting considered by us.
3
Published as a conference paper at ICLR 2022
Theorem 2) (albeit without sample complexity guarantees) requires very stringent assumptions and
even in the case of tabular Q-learning might not converge to the optimal policy.
Experience Replay and Reverse Experience Replay Reinforcement learning involves learning
on-the-go with highly correlated correlated data. Iterative learning algorithms like Q-learning can
sometimes get coupled to the Markov chain resulting in sub-optimal convergence. Experience replay
(ER) was introduced in order to mitigate this drawback (Lin, 1992) - here a large FIFO buffer of
a fixed size stores the streaming data and the learning algorithm samples a data point uniformly at
random from this buffer at each step. This makes the samples look roughly i.i.d., thus breaking the
harmful correlations. Reverse experience replay (RER) is a form ofER which stores data in a buffer
but processes the data points in the reverse order as stored in the buffer. This was introduced in
entirely different contexts by (Rotinov, 2019; Jain et al., 2021b;a). In this work, we note that reverse
order traversal endows a super-martingale structure which yields the strong concentration result in
Theorem 4, which is not possible with forward order traversal (see (Jain et al., 2021b, Section 3.1)
for a brief demonstration of this fact). We can also look at RER is through the lens of Dynamic
programming (Bertsekas, 2011) - where the value function is evaluated backwards starting from
time T to time 1 - similar to how RER bootstraps present to the future.
In the context of reinforcement learning, works like Bhandari et al. (2018); Zou et al. (2019b) obtain
finite time convergence guarantees for RL algorithms under the mixing assumptions just like this
work. The strategy followed in these works is that if two samples are O(τmix) time apart, then
they are approximately independent and thus analysis for i.i.d. data can be applied. The sample
complexity to obtain e error here is O(普).Note that this is no better than keeping one every TmiX
samples and throwing away the rest and under general mis-specified linear function representation,
we might not be able to do any better (Bresler et al., 2020). In this work, we show that when the
linear approximation is well specifed (ZIBEL), we can use RER to obtain a sample complexity of
O(TmiX + ±), where the mixing time serves as a cut-off.
Online Target Learning OTL (Mnih et al., 2015) maintains two different Q-values (called online
Q-value and target Q-value) where the target Q-value is held constant for some time and only the
online Q-value is updated by ‘bootstrapping’ to the target. After a number of such iterations, the
target Q-value is set to the current online Q value. OTL thus attempts to mitigate the destabilizing
effects of bootstrapping by removing the ‘moving target’. This technique has been noted to allow
for an unbiased estimation of the bellman operator (Fan et al., 2020) and when trained with large
batch sizes is similar to the well known neural fitted Q-iteration (Riedmiller, 2005).
2	Problem Setting
Markov Decision Process We consider infinite horizon, time homogenous Markov Decision Pro-
cesses (MDPs) and we denote an MDP by MDP(S, A, γ, P, R) where S is the state space, A is the
action space, γ ∈ [0, 1) is the discount factor, P(s0|s, a) is the probability of transition to state s0
from the state s on action a. We assume that S and A are compact subsets ofRn (for some n ∈ N).
R : S × A → [0, 1] is the deterministic reward associated with every state-action pair.
We will think of an MDP as an agent that is aware of its current state and it can choose the action to
be taken. Suppose the agent takes action π(s), where π : S → A, at state s ∈ S, then P along with
the ‘policy’ π induces a Markov chain over S, whose transition kernel is denoted by Pπ. We write
the γ-discounted value function of the MDP starting at state s to be:
∞
V(s,π) = E[X γtR(St, At)|S0 =s,At =π(St)∀t].	(1)
t=0
It is well-known that under mild assumptions, there exists at least one optimal policy ∏* such that
the value function V(s,∏) is maximized for every sand that there is an optimal Q-function, Q* :
S ×A → R, such that one can find the optimal policy as π*(s) = argmax。*/ Q*(s, a), optimal
value function as V*(s) = maxa∈A Q*(s, a) and it satisfies the following fixed point equation.
Q*(s,a) = R(s,a) + γEs0^p(∙∖s,a) [max Q*(s0, a0)]	∀(s,a) ∈S ×A.	(2)
(2)	can be alternately viewed as Q* being the fixed point of the Bellman operator T, where
T (Q)(s,a) = R(s,a) + γEs0 〜P (∙∣s,a)[m⅛1 X Q(s0,a0)].
4
Published as a conference paper at ICLR 2022
The basic task at hand is to estimate Q*(s, a) from a single trajectory (st,at)T=I such that
st+ι 〜 P(∙∣st,at) along with rewards (rt)T=ι, where rι,...,rτ are random variables such that
E [rt|st = s, at = a] = R(s, a). We refer to Section B for rigorous definitions.
Q-learning Since the transition kernel P (and hence the Bellman operator T) is often unknown
in practice, Equation (2) cannot be directly used to to estimate the optimal Q-function. To this end,
We resort to estimating Q* using observations from the MDP. An agent traverses the MDP and We
obtain the state, action, and the reward obtained at each time step. We assume the off-policy setting
which means that the agent is not in our control, i.e., it is not possible to choose the agent’s actions;
rather, we just observe the state, the action, and the corresponding reward. Further, we assume that
the agent follows a time homogeneous policy π(s) for choosing its action at state s.
Given a trajectory {st , at , rt }tτ=1 generated using some unknown behaviour policy π, we aim to
estimate Q* in a model-free manner - i.e, estimate Q* without directly estimating P. We further
assume that the trajectory is given to us as a data stream so we can not arbitrarily fetch the data for
any time instant. A popular method to estimate Q* is using the Q-learning algorithm. In this online
algorithm, we maintain an estimate of Q*(s, a) at time t, Qt(s, a) and the estimate is updated at
time t for (s, a) = (st , at ) in the trajectory. Formally, with step-sizes given as {ηt }, Q-learning
performs the following update at time t,
Qt+1(st, at) = (1 - ηt)Qt(st, at) + ηt rt + γ max Qt(st+1, a0)
a0∈A	(3)
Qt+1(s, a) = Qt(s, a) ∀ (s, a) 6= (st , at ).
In this work, we focus on two special classes of MDPs which are popular in literature.
Linear Markov Decision Process Linear MDPs (Jin et al., 2020) is a popular example of exact
linear approximation for which statistically and computationally tractable algorithms are available.
We use the definition from Jin et al. (2020), stated as Definition 1.
Definition 1. An MDP(S, A, γ, P, R) is a linear MDP with feature map φ : S × A → Rd, if
1.	there exists a vector θ ∈ Rd such that R(s, a) = hφ(s, a), θi, and
2.	there exists d unknown (signed) measures over S β(∙) = {βι(∙),...,βd(∙)} Such that the
transition probability P(∙∣s,α) = hφ(s, a), β(∙)i.
In the rest of this paper, in the tabular setting we assume that the dimension d = |S × A| and we use
a one hot embedding where we map (s, a) → φ(s, a) = es,a, a unique standard basis vector. It is
easy to show that this system is a linear MDP (Jin et al., 2020) and Q-learning in this setting reduces
to the standard tabular Q-learning (3). However, when the assumption of a tabular MDP allows us
to obtain stronger results, we will present the analysis separately.
Inherent Bellman Error There is another widely studied class of MDPs which admit a good
linear representation (Zanette et al., 2020; Munos & Szepesvari, 2008; SzePeSvari & Smart, 2004).
Definition 2. (ZIBEL MDP) For an M = MDP(S, A, γ, P, R) with a feature map φ : S × A → Rd,
we define the inherent Bellman error (IBE(M)) as:
sup inf sup	∣hφ(s, a),θ0i - R(s,a)- γE.o〜P(.⑶。)SuP hθ,φ(s0,a0)i I
θ∈Rd θ0∈Rd (s,a)∈S×A	, a0∈A
If IBE(M) = 0, then call this MDP a ZIBEL (zero inherent Bellman error with linear function
approximation) MDP.
The class of ZIBEL MDPs is strictly more general than the class of linear MDPs (Zanette et al.,
2020). Both these classes of MDPs have the property that there exists a vector w * ∈ Rd such that
the optimal Q-function, Q*(s, a) = hφ(s, a), w*i for every (s, a) ∈ S × A, which can be explicity
expressed as a function ofθ, β and Q*. More generally, they allow us to lift the Bellman iteration to
Rd exactly and update our estimates for w* values directly (Lemmas 3, 4).
Hence, we can focus on estimating Q* by estimating w*. To this end, the standard Q-Learning
approach to learning the Q function can be extended to the linear case as follows:
5
Published as a conference paper at ICLR 2022
wt+1 = wt + ηt rt + γ m0 axhφ(st+1, a0), wti - hφ(st, at), wti φ(st, at).
The above update can be seen as a gradient descent step on the loss function f (wt) =
(hφ(st, at), wti-target)2 where target = rt+γ maxa0 hφ(st+1, a0), wti. This update while heavily
used in practice, has been known to be unstable and does not converge to w* in general. The rea-
son often cited for this phenomenon is the presence of the ‘deadly triad’ of bootstrapping, function
approximation, and off-policy learning.
2.1 Assumptions
We make the following assumptions on the MDPs considered through the paper in order to present
our theoretical results.
Assumption 1. The MDP M has IBE(M) = 0 (Definition 2), kφ(s, a)k2 ≤ 1. Furthermore,
R(s, a) ∈ [0, 1].
Assumption 2. Let Φ := {φ(s, a) : (s, a) ∈ S × A}. Φ is compact, span(Φ) = Rd and (s, a) →
φ(s, a) is measurable.
Even when span(Φ) 6= Rd, our results hold after we discard the space orthogonal to the span of em-
bedding vectors in Assumption 4 and note that Q-Rex does not update the iterates along span(Φ)⊥.
Definition 3. For r > 0, let N (Φ, k ∙ k2,r) be the r-Covering number under the standard Euclidean
norm over Rd. Define:
r∞	__________
Plog N(Φ,kk
0
CΦ :
2, r)dr
Observe that since Φ is a subset of the unit Euclidean ball in Rd, Cφ ≤ C√d. However, in the case
of tabular MDPS it is easy to show that Cφ ≤ C√log d.
Definition 4. We define thenorm ∣∣ ∙ ∣∣φ overRd by ∣∣xkφ = sup(s,。)∣hφ(s, a),x)|.
This is the natural norm of interest for the problem (Lemmas 2 and 4). We assume the existence
ofa fixed (random) behaviour policy π : S → ∆(A) which selects a random action corresponding
to each state. At each step, given (st,at) = (s,a), st+1 〜 P(∙∣s, a) and at+1 〜 π(st+ι). This
gives us a Markov kernel over S × A which specifies the law of (st+1, at+1) conditioned on (st, at).
We will denote this kernel by Pπ . This setting is commonly known as the off-policy asynchronous
setting. We make the following assumption which is standard in this line of work.
Assumption 3. There exists a unique stationary distribution μ for the kernel Pπ. Moreover, this
Markov chain is exponentially ergodic in the total variation distance with mixing time τmix. That is,
there exist a constant Cmix for every t ∈ N
sup TV((Pπ)t(x, ∙),μ) ≤ Cmix exp(-t/Tmix)
x∈S×A
In the tabular setting, we will use the standard definition of τmix instead:
Tmix = inf{t : sup TV((Pπ)t(x, ∙),μ) ≤ 1/4}.
x∈S×A
Here TV refers to the total variation distance.
Assumption 4. There exists κ > 0 such that: E(§,a)~*0(s, a)φ>(s, a)占 K .
Remark 1. (Bresler et al., 2020, Theorem 1) shows that even linear regression with Markovian data,
zero noise and `2 recovery is hard when the condition number κ or the mixing time Tmix are too large.
Hence, our setup of noisy reinforcement learning with '∞ error also requires these quantities to be
small. Therefore, Assumptions 3 and 4 are necessary in order to obtain non-trivial bounds.
In the tabular setting, Assumption 4 manifests itself as ɪ = μmin := min(s,。)μ(s, a) which is also
standard (Li et al., 2020b). Whenever we discuss high probability bounds (i.e, probability at least
1 - δ), we assume that δ ∈ (0, 1/2). Similarly, we will assume that the discount factor γ ∈ (1/2, 1)
so that We can absorb poly(1∕γ) factors into constants.
6
Published as a conference paper at ICLR 2022
Algorithm 1 Q-ReX
1:	Input: learning rates η, horizon T , discount factor γ, trajectory Xt = {st , at , rt}, Buffer size
B , Buffer gap u, Number of inner loop buffers N
2:	Total buffer size: S — B + u, Outer-loop length: K — NTS, Initialization w1,1 = 0
3:	for k = 1, . . . , K do
4:	for j = 1, . . . , N do
5:	Form buffer Buf = {Xk,j,..., xS,j}, where,	Xk,j	J XNS(k-i)+s(j-i)+i
6:	Define for all i ∈ [1, S], φik,j	, φ(sik,j , aik,j ).
7:	for i = 1, . . . , B do
8:	wik+,j1 = wik,j+η rBk,+j 1-i + γ am0∈aAxhφ(skB,+j 2-i, a0), w1k,1i - hφkB,+j 1-i,wik,ji φkB,+j 1-i
9:	Option I: w1k,j+1 = wBk,+j 1
10:	OptiOn II: wk,j+1 = B PB=1 wk+jι
11:	Option I: w1k+1,1 = w1k,N+1
12:	OptionII: wk+1,1 = N PN+1 WT
13:	Return w1K+1,1
Figure 1: Illustration of Online Target Q-learning with Reverse EXperience Replay
3 Our Algorithm
As discussed in the introduction, we incorporate RER and OTL into Q-learning and introduce the
algorithms Q-ReX (Online Target Q-learning with reverse eXperience replay, Algorithm 1), its sam-
ple efficient variant Q-ReXDaRe (Q-ReX + data reuse, Algorithm 2) and its episodic variant EpiQ-
ReX (Episodic Q-ReX, Algorithm 3). Since Q-ReXDaRe and EpiQ-ReX are only minor modifications
of Q-ReX, we refer the reader to the appendiX for their pseudocode.
Q-ReX is parametrized by K the number of iterations in the outer-loop, N the number of buffers
within an outer-loop iteration, B the size of a buffer and u the gap between the buffers. The algorithm
has a three-loop structure where at the start of every outer-loop iteration (indeXed by k ∈ [K]), we
checkpoint our current guess of the Q function given by w1k,1. Each outer-loop iteration corresponds
to an inner-loop over the buffer collection with N buffers, i.e. at iteration j ∈ [N], we collect a
buffer of size B + u consecutive state-action-reward tuples. For every collected buffer we consider
the first B collected eXperiences and perform the target based Q-learning update in the reverse order
for these eXperiences. We refer Figure 1 for an illustration of the processing order. Of note, is the
usage of checkpointed target network in the RHS of the Q-learning update through the entirety of
the outer-loop iteration, i.e. for a fiXed k and for all j, i, our algorithm sets
wik+,j1 = wik,j +η rBk,+j 1-i + γ am0∈aAxhφ(skB,+j 2-i, a0), w1k,1i - hφkB,+j 1-i,wik,ji φkB,+j 1-i
Figure 1 provides an illustration of the processing order for our updates. It can be seen that the
number of eXperiences collected through the run of the algorithm is T = KN(B + u). For the sake
of simplicity, we will assume that the initial point, w11,1 = 0. Essentially the same results hold for
arbitrary initial conditions. Q-ReXDaRe is a modification of Q-ReX where we re-use the data from
the first outer-loop iteration (i.e, data from k = 1) in every outer-loop iteration (i.e, k > 1).
7
Published as a conference paper at ICLR 2022
Setting		K		N		U	B		η	
ZIBELMDP (THEOREM 1)	≥ 1	> 号 K log (生)	≥ CITmiX lθg(CmxKN)	= 10u	< C2 min( Cφ +1-YK/δ)), B)
Tabular MDP (Theorem 2)	(lοg( T-Y ))2 ≥ C2	i-√—	> C μmn l°g( ≡AK)	≥ CITmiX log(KN)	= 10u	<	C	 < log(吗K)
Tabular MDP (Theorem 3)	≥ 02寸	> B μmn log(用)	≥ CITmiX lθg(KN )	= 10u	< C	(I-Y)2 3加(平1)
ZIBELMDP= (THEOREM 1)	βl (1-Y)	κβ max(T2C⅛‰XH	TmiXlOg ( KN)	10u	min( ⅛φ⅞~，B)
Tabular MDP (Theorem 2)	ɪ 1-Y	μ⅛ max (β5, ηβmx))	TmiXlOg ( KN)	10u	⅛d3 min(e,e2)
Tabular MDP (Theorem 3)	βι (1-Y)	μ⅛ max (β5, ηβmx))	TmiX log(KN)	10u	min(e2(i-Y)3 (1-Y)24it)3)- min(	β4	, dβ5 , √dβ4 J
Table 2: Parameter constraints (first 3 rows) and choice for < error (last 3 rows) for our algorithms.
Here the Poly-log factors β are given by β = log( (1-γ)1L(e,1) ), β = log( (1-γ)δmin(e,1)),
β2 = lθg(κ) + β3, β4 = lθg( 1S11A1K), β5 = lθg(≡Al).
Remark 2. For the sake of clarity, we only analyze the algorithms Q-Rex and Q-RexDaRe for data
from a single trajectory with Option I. Option II involves averaging of the iterates which boosts the
convergence - indeed we can obtain much better bounds in this setting by using standard analysis.
4 Main Results
We will now provide finite time convergence analysis and sample complexity for the algorithms
Q-Rex and Q-RexDaRe. Recall that K is the number of outer-loops, N is the number of buffers
inside an outer-loop iteration, B is the buffer size and u is the size of the gap. In what follows, we
will take U = O(Tmiχ), B = 10u, K = O( y⅛γ). We also note that the total number of samples used
is NK(B + u) for Q-Rex and N (B + u) for Q-RexDaRe since we reuse data in each outer-loop
iteration. In what follows, by Q1K+1,1(s, a), we denote hφ(s, a), w1K+1,1i which is our estimate for
the optimal Q function. Here w1K+1,1 is the output of either Q-Rex or Q-RexDaRe at the end of
K outer-loop iterations. Define ∣∣QK+1,1 - Qk∞ := suP(s,a)∈S×A∣QK+1,1(s, a) - Q*(s, a)∣. We
first consider the performance of Q-Rex with data derived from a linear MDP (Defintion 1) or a
ZIBEL MDP (Definition 2) and satisfying the Assumptions in Section 2.1.
Theorem 1 (ZIBEL /Linear MDP). Suppose we run Q-Rex using Option I with data from an MDP
with IBE = 0. There exists constants C1, C2, C3, C4, C5 > 0 such that whenever the parameter
bounds given in Table 2 (row 1) are satisfied, then with probability at-least 1 - δ, we must have:
kQK+1,1 - Q*k∞ ≤ 1¾ + C4qδ(⅛ exp (-喑)+ C⅛r小；+-：『)i
Given e ∈ (0, .-Y) ], and the parameters as given in Table 2 (row 4)(up to ConStantfaCtors), then
with probability at-least 1 — δ: kQK+1,1(s, a) — Q*(s, a)∣∞ < e. This has a sample complexity
Θ(NKB) = O (κmax ((⅛⅛2,空))
We now consider the performance of Q-Rex and Q-RexDaRe in the case of tabular MDPs. We
refer to Table 1 for a comparison of our results to the state-of-art results provided in literature for
Q-learning based algorithms.
Remark 3. To the best of our knowledge, Theorem 1 presents the first non-asymptotic convergence
results for Q-learning based methods for ZIBEL MDPs under standard assumptions. Notice that
the SamPle complexity scales as 表 + TmiX instead of 普 like in Zou et al. (2019b); Bhandari et al.
(2018). This is because in the case of ZIBEL MDPs RER brings out the super-martingale structure
present in the problem which forward pass does not.
Theorem 2 (Tabular MDP). Suppose we run Q-Rex using Option I with data derived from tabular
MDPs. Whenever the algorithmic parameters are picked as given in Table 2 (row 2) for some
universal constants C1 , . . . , C5, we obtain with probability at-least 1 - δ:
「丁	( ημminNB)	1 (K|S||A|)	/ 1 (KISIIAI)1
I∣qK+1 _Q*k LCL	JYL +	exp(-	2	J +	η lοg( δ	J + ʌ ∕η logI δ	L
IIQ - Q k∞ <	C5	1-Y +	(1-γ)2	1	(1-γ)3	+ V (1-γ)3
8
Published as a conference paper at ICLR 2022
Where L = b1K . Given e ∈ (0, ι1-], and the parameters are picked as given in Table 2 (row
logι-Y	Y
5), then with probability at-least 1 一 δ, we have: kQK+1 一 Q*k∞ < e. This gives us a sample
complexity of
Θ(NKB) = O (ɪ max (τ.—J , 2、, 41).
μmin	(1-Y)4 min(e,e2), 1-Y
Even though the sample complexity provided in Theorem 2 matches the sample complexity of syn-
chronous Q-learning even with asynchronous data, it is still sub-optimal with respect to the min-max
lower bounds (i.e, it has a dependence of has a dependence of(二产 instead of the optimal .—7户).
We resolve this gap for Q-RexDaRe in Theorem 3. For tabular MDPs, the number states can be
large but the support of P(∙∣s, a) is bounded in most problems of practical interest. Consider the
following assumption (note that this holds for every tabular MDP with d= |S|.)
Assumption 5. Tabular MDP is such that 卜Upp(P(∙∣s, a))| ≤ d ∈ N.
Theorem 3 (Tabular MDP with Data Reuse). For tabular MDPs, suppose additionally Assumption 5
holds and we run Q-RexDaRe using Option I. There exist universal constants C1 , C2 , C3 , C4 such
that when the parameter values satisfy the bounds in Table 2 (row 3), with probability at-least 1 一 δ:
/ ημmin NB ∖	K	S||A|K ʌ
kQK+1,1-Q*k∞ ≤ C exp(-丁)+γ +ηlo⅛γτ)pd+ q…iog(KF|)
Suppose e ∈ (0,11γ]. Ifwe choose the parameters as per Table 2 (row 6), then with probability
at-least 1 — δ we have: kQK+1(s, a) — Q*k∞ < e. The sample complexity in this case is
Θ(NB)
〜
O
max
τmix,
1	d	√
e2(1-γ)3，(1-γ)2，<(1-γ)3	.
5 Overview of the Analysis
We divide the analysis of Q-Rex and Q-RexDaRe into two parts: Analysis of w1k,1 obtained at the
end of outer-loop iteration k and the analysis of the algorithm within the outer-loop. The algorithm
reduces to SGD for linear regression with Markovian data within an outer-loop due to OTL. That is,
we try to find wk+1,1 such that(wk+1,1, φ(s, a))≈ R(s, a) + E§0ZP(,⑶。)supa’hwk,1, φ(s0, a0)).
Therefore, we write w 1k+1,1 = T (w1k,1) + ek(w1k,1),
where T is the γ contractive Bellman operator
whose unique fixed point is w* and ek is the noise to be controlled. Following a similar setting in
in (Jain et al., 2021b), we control ek with the following steps:
(1)	We introduce a fictitious coupled process (see Section C) (st, at, rt) where the data in different
buffers are exactly independent (since the gaps of size u make the buffers approximately indepen-
dent) and show that the algorithm run with the fictitious data has the same output as the algorithm
run with the actual data with high probability when u is large enough.
(2)	We give a bias-variance decomposition (Lemma 5) for the error ek where the exponentially de-
caying bias term helps forget the initial condition and the variance term arises due the inherent noise
in the samples.
(3)	We control the bias and variance terms separately in order to ensure that the noise ek is small
enough. RER plays a key role in controlling the variance term by endowing it with a super-
martingale structure, which is not possible with forward order traversal (see Theorem 4).
The procedure described above allows us to show that w1k+1,1 ≈ T (w1k,1)
uniformly for k ≤ K,
which directly gives us a convergence bound to the fixed point of T i.e, w* (Theorem 1). In the
tabular case, the approximate Bellman iteration connects to the analysis of synchronous Q-learning
in (Li et al., 2021), which allows us to obtain a better convergence guarantee (Theorem 2). To obtain
convergence guarantees for Q-RexDaRe, we first observe that if we re-use the data used in outer-
loop iteration 1 in all future outer-loop iterations k > 1, ek(w1k,1) might not be small since w1k,1
depends on ek(∙). However, (wk,1)k approximates the deterministic path of the noiseless Bellman
iterates: W1,1 := w1,1 and wk+1,1 := T(wk,1). Since ∣∣ek(wk,1)k∞ ≤ ∣∣ek(wk,1) — ek(wk,1)k∞ +
∣∣ek(wk,1)∣∞, we argue inductively that ∣∣ek(wk,1) — ek(wk,1)k∞ ≈ 0 since wk,1 ≈ wk,1 and
∣∣ek(wk,1)∣∞ ≈ 0 since wk,1 is a deterministic sequence and hence wk+1,1 ≈ wk+1,1.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Most of this work was done when D.N. was a graduate student at MIT and was supported in part
by NSF grant DMS-2022448. Part of this work was done when D.N. was a visitor at the Simons
Institute for Theory of Computing, Berkeley. We would also like to thank Gaurav Mahajan for
introducing us to low-inherent Bellman error setting, and providing intuition that our technique
might be applicable in this more general setting (than linear MDP) as well.
10
Published as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory, pp. 67-83. PMLR, 2020.
Mohammad GheshIaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size q-learning. Systems
& control letters, 61(12):1203-1208, 2012.
Dimitri P Bertsekas. Dynamic programming and optimal control 3rd edition, volume ii. Belmont,
MA: Athena Scientific, 2011.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference on learning theory, pp. 1691-1692.
PMLR, 2018.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.
Justin Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximat-
ing the value function. Advances in neural information processing systems, pp. 369-376, 1995.
Guy Bresler, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli, and Xian Wu. Least squares regres-
sion with markovian data: Fundamental limits and algorithms. arXiv preprint arXiv:2006.08916,
2020.
Diogo Carvalho, Francisco S. Melo, and Pedro Santos. A new convergent variant of q-learning with
linear function approximation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19412-19421. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-
sample analysis of nonlinear stochastic approximation with applications in reinforcement learn-
ing. arXiv preprint arXiv:1905.11425, 2019.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample
analysis of contractive stochastic approximation using smooth convex envelopes. arXiv preprint
arXiv:2002.00874, 2020.
Gal Dalal, BaIaZs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Thirty-second AAAI conference on artificial intelligence, 2018a.
Gal Dalal, Gugan Thoppe, BaIaZS Szorenyi, and Shie Mannor. Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. In SebaStien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On
Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 1199-1233.
PMLR, 06-09 Jul 2018b. URL https://proceedings.mlr.press/v75/dalal18a.
html.
Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of ac-
celerated markov gradient descent with applications in reinforcement learning. arXiv preprint
arXiv:2002.02873, 2020.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
11
Published as a conference paper at ICLR 2022
David A Freedman. On tail probabilities for martingales. the Annals of Probability, pp. 100-118,
1975.
Mohammad Ghavamzadeh, Hilbert Kappen, Mohammad Azar, and Remi Munos. Speedy q-
learning. Advances in neural information processing systems, 24:2411-2419, 2011.
Sheldon Goldstein. Maximal coupling. ZeitschriftfUr Wahrscheinlichkeitstheorie Und verwandte
Gebiete, 46(2):193-204, 1979.
Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region.
Advances in neUral information processing systems, 13, 2000.
Harsh Gupta, R. Srikant, and Lei Ying. Finite-time performance bounds and adap-
tive learning rate selection for two time-scale reinforcement learning. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.),
Advances in NeUral Information Processing Systems, volume 32. Curran Associates,
Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
e354fd90b2d5c777bfec87a352a18976- Paper.pdf.
Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli. Near-optimal of-
fline and streaming algorithms for learning non-linear dynamical systems. arXiv preprint
arXiv:2105.11558, 2021a.
Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli. Streaming linear system
identification with reverse experience replay. arXiv preprint arXiv:2103.05896, 2021b.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect
algorithms. Advances in neUral information processing systems, pp. 996-1002, 1999.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier
in model-based reinforcement learning with a generative model. Advances in neUral information
processing systems, 33, 2020a.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous
q-learning: Sharper analysis and variance reduction. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in NeUral Information Processing Systems, volume 33,
pp. 7031-7043. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf.
Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi. Is q-learning mini-
max optimal? a tight sample complexity analysis. arXiv preprint arXiv:2102.06548, 2021.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3-4):293-321, 1992.
Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy
learning control with function approximation. In Proceedings of the 27th International Con-
ference on International Conference on Machine Learning, ICML’10, pp. 719-726. Omnipress,
2010. ISBN 9781605589077.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th international conference on Machine
learning, pp. 664-671, 2008.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. natUre, 518(7540):529-533, 2015.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(5), 2008.
12
Published as a conference paper at ICLR 2022
Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral meth-
ods. Electronic Journal ofProbability, 20:1-32, 2015.
Theodore Perkins and Doina Precup. A convergent form of approximate policy iteration. Advances
in neural information processing systems, 15, 2002.
Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation
and q-learning. In Conference on Learning Theory, pp. 3185-3205. PMLR, 2020.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European conference on machine learning, pp. 317-328. Springer,
2005.
Egor Rotinov. Reverse experience replay. arXiv preprint arXiv:1910.08780, 2019.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pp. 2803-2830. PMLR, 2019.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Csaba Szepesvari and William D Smart. Interpolation-based q-learning. In Proceedings of the
twenty-first international conference on Machine learning, pp. 100, 2004.
John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic program-
ming. Machine Learning, 22(1):59-94, 1996.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674-690, 1997.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp '∞-bounds
for q-learning. arXiv preprint arXiv:1905.06265, 2019a.
Martin J Wainwright. Variance-reduced q-learning is minimax optimal. arXiv preprint
arXiv:1906.04697, 2019b.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019c. doi: 10.1017/
9781108627771.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
Bowen Weng, Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Momentum q-learning
with finite-sample convergence guarantee. arXiv preprint arXiv:2007.15418, 2020.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995-7004. PMLR, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978-10989. PMLR, 2020.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear func-
tion approximation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/
9f9e8cba3700df6a947a8cf91035ab84- Paper.pdf.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. Advances in neural information processing systems, 32, 2019b.
13
Published as a conference paper at ICLR 2022
A	Experiments
Even though OTL has a stabilizing effect on the Q-iteration, it reduces the rate of bias decay since
the values are not updated for a long time. Therefore, the success of our procedure depends on
picking the right values for the parameters N, B and Option I vs. Option II. However, under the
right conditions the algorithms which include OTL+RER converge to a much smaller final error
as illustrated by the examples we provide in this section. If a better sample complexity is desired,
then Q-RexDaRe can be used as shown by Theorem 3. Further research is needed to identify the
practical conditions such as (function approximation coarseness, MDP reward structure etc.) under
which techniques like OTL+RER help.
100-State Random Walk We first consider the episodic MDP (Sutton & Barto, 2018, example
6.2) - but with 100 states instead of 1000. Here the agent can either move left or right on a straight
line, receiving a reward of 0 at each step. Reaching the right terminal point ends the episode with
reward 1, while the left terminal point ends the episode with reward 0. In each episode, the initial
point is uniformly random and the offline policy chooses right and left directions uniformly at ran-
dom. We use state aggregation to obtain a linear function representation (Sutton & Barto, 2018)
with total 10 aggregate states. Along with 2 actions, this leads to a 20 dimensional embedding.
We compare vanilla Q-learning, OTL+ER+Q-learning and EpiQ-Rex (Option II, N = 1). The
same step size (0.01) was chosen for all the algorithms. OTL+ER+Q has the same structure as
EpiQ-Rex and was run with the same parameters as EpiQ-Rex . The main difference between the
two algorithms is that OTL+ER+Q processes the experiences collected in each episode in a random
order processing instead of reverse order. Refering to Figure 4, we note that the bias decay for EpiQ-
Rex and OTL+ER+Q are slower than vanilla Q-learning due to the online target structure. However
EpiQ-Rex converges to a better solution than the other algorithms.
Mountain Car We run an online control type experiment with the Mountain car problem (Sutton
& Barto, 2018, Example 10.1). The task here is to control the car and help it reach the correct peak
(which ends the episode) as soon as possible (i.e, terminate the episode with fewest steps). The agent
receives a reward of -1 unless the correct peak is reached. Here we run EpiQ-Rex with Option I
and N = 1, OTL+ER+Q-learning and vanilla Q-learning. The k-th episode is generated with the
policy at the end of k - 1-th episode for each of the three algorithms. We use a n = 4 tile coding
to represent the Q values for a given action, each of which has 4 × 4 squares. The step-size was
picked as 0.1/n and the result was averaged over 500 runs of the experiment. We refer to Figure 2
for the outcomes. For the last 300 episodes, the mean episode length for Vanilla Q-learning was
145, EpiQ-Rex was 136 and OTL+ER+Q-learning was 143 (rounded to the nearest integer).
Grid-World We consider the grid-world problem which is a tabular MDP (Sutton & Barto, 2018,
Example 3.5), which is a continuing task. Here, an agent can walk north, south, east or west in a
5 × 5 grid. Trying to fall off the grid accrues a reward of -1, while reaching certain special states
accrues a reward of 10 or 5. In our example, we also add a unif[-0.5, 0.5] noise to all rewards to
make the problem harder. We run the grid world experiment with the discount factor γ = 0.9 and
step size 0.05 for Vanilla Q-learning, Q-Rex (Option II, B = 3000, N = 1) and OTL+ER+Q-
learning which is run with the same parameters as Q-Rex but with random samples from the buffer.
We run the experiment 30 times and plot the error in the Q-values vs. time in Figure 3.
Baird’s Counter-Example Consider the famous Baird’s counter-example shown in Figure 6. The
features corresponding to each state is shown and the reward for any transition is 0. Thus, w* = 0
is the optimal solution. Since Assumption 4 made in this work is violated for this example, we
consider the analog of the problem where at each step, a state is sampled uniformly at random and
the corresponding transition, along with the 0 reward is used to learn the vector w. In the experiment,
We set the problem and algorithmic parameters to be Y = 0.99 and η = 0.01/√5 (the factor of √5
normalizes the features to satisfy Assumption 1). We set B = 50, u = 0, and N = 5 since there is
no need for keeping a gap between the buffers in this experiment. Figure 7 shows the non-convergent
behavior of vanilla Q-learning while OTL-based Q-learning converges. Note that since the sampling
of state, action and reward is done in a uniformly random fashion, it is not relevant to use reverse
experience replay. It is easy to see that with data reuse, we only need few samples to ensure all states
are covered; the rate of convergence would be same as that of OTL+Q-learning.
14
Published as a conference paper at ICLR 2022
Figure 2: Mountain Car problem
Figure 4: 100 State Random Walk
Figure 3: Grid-world problem
Figure 6: Feature embedding in the modified
Baird’s counter-example; e~i represents the i-
th canonical basis vector in R7 . Each transi-
tion shown occurs with probability 1.
Figure 5: Linear System Problem
Figure 7: Performance of vanilla Q-learning
as compared to OTL+Q-learning, averaged
over 10 independent runs
Linear Dynamical System with Linear Rewards We also compare the algorithms on a linear
dynamical systems problem. While the problem described next is strictly not a linear MDP, the
value functions for certain policies can be written as a linearly in terms of the initial condition; this
is made precise next. Consider a linear dynamical system with initial state being X0 ∈ Rd . The
state evolves as Xt+1 = AXt + ηt , where A ∈ Rd×d . The reward obtained for such transition
is given by hXt , θi for a fixed θ ∈ Rd . The maximum singular value of A is chosen less than 1
to ensure that the system is stable. The infinite horizon γ-discounted expected reward is given by
hX0, P∞=0(YAT)iθi∙ Thus, the expected reward can be written as hX0, w*i，where
W* = (I - YAT)-1 θ.
Since there are no actions in this case, the Q learning algorithms reduce to value function approx-
imation (i.e, TD(0) type algorithms). We take the embedding φ(Xt) = Xt, the identity mapping.
We considered d = 5, Y = 0.99, η = 0.01, and a randomly generated normal matrix A and θ. We
Option II for Q-Rex along with B = 75, u = 25, and N = 5 for the experiments. For OTL+ER+Q-
learning, we keep the same parameters, but with random order sampling, while ER+Q-learning does
not include OTL. The results shown in Figure 5 are averaged over 100 independent runs of the
experiment. Note that the errors considered are using iterate averaging.
15
Published as a conference paper at ICLR 2022
As seen from Figure 5, Q-Rex outperforms vanilla Q-learning and OTL+ER-based Q-learning as
one would expect based on the theory presented in this work. However, it is interesting to note that
ER+Q-learning performs slightly better than Q-Rex. One possible reason could be due to the fact
that in Q-Rex, the target gets updated at a slower rate at the cost of reducing bias. However, setting
a smaller value of N B might resolve the issue.
B Definitions and Notations
B.1	Q-RexDaRe
The psuedocode for Q-RexDaRe is given in Algorithm 2. Note that we reuse the sample data in
every outer-loop iteration instead of drawing fresh samples.
Algorithm 2 Q-RexDaRe
1:	Input: learning rates η, horizon T , discount factor γ, trajectory Xt = {st , at , rt}, number of
outer-loops K buffer size B , buffer gap u
2:	Total buffer size: S J B + U
3:	Number of buffers loops: N J T/S
4:	w11,1 = 0 initialization
5:	for k = 1, . . . , K do
6:	for j = 1, . . . , N do
7:	Form buffer Buf = {X1k,j , . . . , XSk,j }, where,
Xik,j J XS(j-1)+i
8:
9:
10:
11:
12:
13:
Define for all i ∈ [1, S], φik,j , φ(sik,j , aik,j).
for i = 1, . . . , B do
wik+,j1 = wik,j+η hrBk,+j 1-i +γmaxa0∈Ahφ(skB,+j2-i,
k,j+1
w1
k,j
B+1
k+1,1	k,N +1
w1	= w1
Return w0T +1
a0),w1k,1i - hφkB,+j 1-i, wik,j i φkB,+j 1-i
w
B.2	EpiQ-Rex
The psuedocode for EpiQ-Rex is given in Algorithm 3. Note that the buffers here are individual
episodes and can vary in size due to inherent randomness. We do not require a gap here since
separate episodes are assumed to be independent.
MDP definition Here we construct the MDP with a (possibly) random reward r. We consider
non-episodic, i.e. infinite horizon, time homogenous Markov Decision Processes (MDPs) and we
denote an MDP by MDP(S , A, γ, P, r) where S denotes the state space, A denotes the action space,
γ represents the discount factor, P (s0|s, a) represents the probability of transition to state s0 from
the state s on action a. We assume for purely technical reasons that S and A are compact subsets
of Rn for some n ∈ N). r is a reward process (not to be confused with Markov Reward Processes
i.e, MRP) indexed by S × A × S, such that r(s, a, s0) ∈ [0, 1] almost surely. We will skip the
measure theoretic details of the definitions and assume that the sequence of i.i.d. reward processes
(rt)t∈N can be jointly defined over a Polish probability space. The function R : S × A → [0, 1]
represents the deterministic reward R(s, a) obtained on taking action a at state s and is defined by
R(s, a) = E [Es，〜P(∙∣s,a)r(s, a, s0)]. Here the expectation is with respect to both the randomness
in the reward process and in state s0.
Now, given a trajectory, (st, at)tT=1, which is independent of i.i.d sequence of rewards processes
(rt)tT=1. We observe (st, at, rt(st, at, st+1))tT=1, and we will henceforth denote rt(st, at, st+1) by
just rt .
16
Published as a conference paper at ICLR 2022
Algorithm 3 EPiQ-Rex
1:	Input: learning rates η, number of episodes T, discount factor γ, Epsiodes Et =
{(st1, at1, r1t), . . . , (syB , atB , rBt )}, Number of buffer Per outer-looP iteration N.
2:	Number of outer loop iterations: K J T/N
3:	w11,1 = 0 initialization
4:	for k = 1, . . . , K do
5:	for j = 1, . . . , N do
6:	t J (k — 1) * N + j
7:	Collect experience and form buffer Buf = {X1t , . . . , XBt }, where,
Xit = (sit, ait, rit)
8:	Define for all i ∈ [1, Bt — 1], φik,j , φ(sik,j, aik,j).
9:	for i = 1, . . . , B do
10:	wik+,j1 = wik,j+η hrBk,tj-i + γ maxa0∈Ahφ(skB,tj+1-i, a0), w1k,1i — hφkB,tj-i, wik,j ii φkB,tj-i
11:	Option I: w1k,j+1 = wBk,j
12:	Option ILwk,j+1 = Bt-ι PBt2 wk,j
13:	w1k+1,1 = w1k,N+1
14:	Return w1K,1
Notation Due to three loop nature of our algorithm it will be convenient to define some simplifying
notation. To this end, consider the outer loop with index k ∈ [K] and buffer number j ∈ [N] inside
this outer loop. Further given an i ∈ [S] define a time index as tik,j = NS(k — 1) + S(j — 1) + i.
We now denote the i-th state-action-reward tuple inside this buffer by
(sik,j , aik,j , rik,j ) = (stk,j , atk,j , rtk,j ).
iii
Similarly, Rik,j = R(sik,j , aik,j). For conciseness, we define for all i, j, k, φik,j := φ(sik,j , aik,j).
Since we are processing the data in the reverse order within the buffer, the following notation will
be useful for analysis:
sk-,ij := skB,+j 1-i , ak-,ij := akB,+j 1-i , r-k,ij := rBk,+j 1-i , Rk-,ij := RBk,+j 1-i and φk-,ij := φkB,+j 1-i .
C Coupled Process
It can be seen that the buffers are approximately i.i.d. whenever We take U =O(Tmix log T) When-
ever Assumption 3 is satisfied. For the sake of clarity of analysis, we will consider exactly inde-
pendent buffers. That is, we assume the algorithms are run with a fictitious trajectory (st, )⅛, rj
Where We assume that the fictitious trajectory is generated such that the first state of every buffer is
sampled from the stationary distribution μ. We show that we can couple this fictitious process (i.e,
define it on a common probability space as the original process (st, at, rt)) such that
P(∩K=1 ∩j=ι∩B+1 {(sk-j) = (s"r”Skj)}) ≥ 1 — δ.	(4)
Notice that the equality does not hold within the gaps between the buffer which are of size u but
inside the buffers of size B only. That is, the sequence of iterates obtained by running the algorithm
with the original data (st, at, rt) is the same as the sequence of iterates obtained by running the algo-
rithm with the fictitious coupled data (sst, ast, rst) with high probability. We state this result formally
in Lemma 16 and prove it in Section M.1. Henceforth, we will assume that we run the algorithm
with data (sst, sat, rst) and refer to Lemma 16 to carry over the results to the original data set with
kj
high probability. We analogously define φik,j. We will denote the iterates of the algorithm run with
the coupled trajectory as wsik,j instead of wik,j and will focus on it entirely. We now provide some
definitions based on the above process. These definitions will be used repeatedly in our analysis.
17
Published as a conference paper at ICLR 2022
D Basic S tructural Lemmas
We first note some basic structural lemmas regarding ZIBEL MDPs under the assumptions in Sec-
tion 2.1. We refer to Section M for the proofs.
Lemma 1. For tabular MDPs satisfying the assumptions in Section 2.1, for both Q-Rex and Q-
RexDaRe, we have thatfor every k,j, i we have that kwk,jkφ ≤ ι--γ
The lemma above says, in particular, that the Q-value estimate given by our algorithm never exceeds
1-1γ due to 0 initialization. The proof is a straightforward induction argument, which We omit. We
will henceforth use Lemma 1 without explicitly mentioning it.
Lemma 2. Suppose Qw(s,a) := hw, φ(s, a)i and let Q* (s, a) = (w*,φ(s, a))be the optimal Q
function. Then:
sup	|Qw(s,a) - Q*(s, a)| = Ilw - w*kφ
(s,a)∈S×A
Moreover, we must have: ∣x∣ ≥ ∣∣x∣φ ≥ √xk forany X ∈ Rd.
Lemma 3. For any w0 ∈ Rd, there exists a unique w1 ∈ Rd such that
hwι,φ(s, a)i = R(s,a) + γEs0〜P(.⑶。)SuP hφ(s0, a0), wo).
a0∈A
We will denote this mapping w0 → w1 by w1 = T(w0).
Lemma 4. T : Rd → Rd is Y contractive in the norm ∣∣ ∙ ∣∣φ. The unique fixed point of T is w*.
MoreoVer we have: kw*kφ ≤ ι--γ and ∣∣w*∣ ≤ ^Y.
In view OfLemma 4, we can begin to look at the following noiseless Q-iteration. Let W1 = w1,1 = 0
and Wk+1 = T(wk). This converges geometrically to w* with contraction coefficient Y under the
norm ∣∣ ∙ ∣φ. In our case, however, we only have sample access to the operator T. Therefore, our
Q-iteration at the end of k-th outer loop can be written as Wk+1,1 = T(WkJ)+Wkwhere Zk is the
error introduced via sampling which needs to be controlled.
E Bias Variance Decomposition
We begin our analysis by providing a bias-variance decomposition of the error with respect to the
noiseless Q-iteration at every loop. We will need the following definitions which we use repeatedly
through our analysis.
Given step size η, outer loop index k and buffer index j, we define the following contraction matrices
for a, b ∈ [B].
b
HWak,,bj:= Y I - ηφWik,j[φWik,j]> .	(5)
i=a
Whenever a > b, we will define HWak,,bj := I. In the tabular setting, we define for any (s, a) ∈ S × A
by NW k(s, a) to be the number of samples of (s, a) seen in the outer loop k (excluding the gaps),i.e.
NB
N k(s,a) = XX i((g" ak，j ) = (s,a))
j=1 i=1
Further we denote by NWik,j (s, a), the number of samples of (s, a) seen in the outer loop k in the
buffers post the buffer j as well as the number of samples of (s, a) in buffer j before iteration i.
Formally,
i-1	N B
Nkj (s,a) := X 1((” Mj) = (S,a)) + X X1((M ak，l ) = (s,a)) ∙
r=1	l=j +1 r=1
18
Published as a conference paper at ICLR 2022
We define the error term for any w:
苜,j(W) := rk,j - Rkj + Y SUp hw, φ(⅞jι, a0)i - γEso〜P(,丑渣，。) SUp hΦ(s0, a0), Wi .⑹
a0∈A	(Ii , i ) a0∈A
Finally we define the following shorthands for any i, j, k :
B
wk+1,* ：= T(WkJ)	耍：=镇(WkJ)	Lk,j ：= ηX 康Hkj-1φkj.
i=1
Given the above definition, the following the lemma provides the Bias-Variance decomposition
which is the core of our analysis.
Lemma 5 (Bias-Variance Decomposition). For every k, we have that,
1	N j +1
Wk ：= wk+1,1 - wk+1,* = Y HkB(wk,1 — wk+1，*) + XY HkBLk，j	⑺
j=N	j=1 l=N
Here and later on in the paper, we use the reverse order in the product to highlight the convention that
higher indices l in HW 1k,,Bl appear towards the left side of the product and further define QlN=+N1 HW 1k,,Bl =
I . We call the first term in Equation (7) as the bias term and it decays geometrically with N and
the second term is called the variance, which has zero mean. We will bound these terms separately.
Since tabular setting allows for improved analysis of error, we will provide special cases for the
tabular setting with refined bounds. We refer to Section M.5 for the proof of Lemma 5.
F Bounding the Bias Term
F.1 Tabular Case
In the tabular case, we have the following expression for bias. We omit the proof since it follows
from a simple calculation.
Lemma 6. In the tabular setting, we have:
1
hφ(s,a), Y Hk,,B(Wk,1 - Wk+1,*)i = (1 -η)Nk(s,a)hwk,1 - Wk+1,*,φ(s,a)i
j=N
For a particular outer loop k, We show that NNk (s, a) is Ω(μminNB) with high probability whenever
N B is large enough. For this we use (Paulin, 2015, Theorem 3.4) similar to the proof of (Li et al.,
2020b, Lemma 8). We give the proof in Section M.6.
Lemma 7. There exists a constant C such that whenever B ≥ Tmix and NB ≥ CμTmix- log( RA|),
with probability at least 1 - δ, we have that for every (s, a) ∈ S × A:
Nk(s,a) ≥ 1 μ(s,a)NB ≥ ɪμminNB
F.2 ZIBEL MDP CASE
Lemma 8. Suppose g ∈ Rd is fixed and ηB < 4. Then, thefollowing hold:
1.
1
Ek Y H]gk2 ≤ exp(-ηKB)kgk2	(8)
j=N
2.	With probability at-least 1 - δ, we have:
Ek Y Hk,,Bg∣∣φ≤ exp(-- Jkgkφ
j=N
We refer to Section L.1 for the proof.
19
Published as a conference paper at ICLR 2022
G B ounding the Variance Term
G.1 Tabular Case
In the tabular case, it is clear that:
N j+1	N B
hφ(s,a), XY Hk,BLW = X X(1 -η)Nkj(S叫4jI(MjMj) = (s,a))	(9)
j=1 l=N	j =1 i=1
Now, We note that Nkj(s, a) depends on data in buffers l > j and when iι < i inside buffer j.
Following the discussion in (Li et al., 2021), we define the vector VarP(Vk), VarP(V*) ∈ Rs×a
such that
2
VarP(Vfc)(s, a) = Es，~p年,。)SUp hφ(s0, a0), Wk,1〉- Es，~p(.同。)SUp hφ(s0, a0), Wkji	(10)
a0∈A	a0∈A
Varp(V*)(s, a) = Es，~p(.|s,。)SUp {φ(s,, α,),w*) - Es0~p(.囱。)SUp hφ(s0,a0),w*i	(11)
a0∈A	a0∈A
More generally, we define
2
VarP(V)(s, a; w) = Es，~p(.⑶。)SUp hφ(S, a0), Wi — Es，~p(.⑶。)SUp hφ(S, a0), Wi
a0∈A	a0∈A
Similarly, we define Lkj(W) by replacing wk,1 with any fixed, arbitrary w. It is easy to see that:
E 博j (w)∣2 (∙⅛j, akj) = (s,a) ≤ 2(1+ γ2VarP (V )(s,a; w))	(12)
Lemma 9. In the tabular setting, suppose W is a fixed vector such that(w, φ(s, a)〉∈ [0, ι-1γ] for
every (s, a) ∈ S × A. Fix (s, a) ∈ S × A. Then there exists a universal constant C such that with
probability atleast 1 - δ, we have that:
Then,
hφ(s, a),XYI HkkBLk j(w)i∣ ≤ Cpηlog(2∕δ)(1 + Y2VarP(V)(s, a; W)) + Cn,(2/')
j=1 l=N	- γ
G.2 ZIBEL MDP CASE
We will use an appropriate exponential super-martingale to bound the error term in the ZIBEL MDP
case just like in the proof of (Jain et al., 2021b, Lemma 27). The following thereom summarizes the
result and we refer to Section L.3 for its proof.
Theorem 4. Suppose x, w ∈ Rd are fixed. Then, there exists a universal constant C such that with
probability at least 1 - δ, we have:
N j + 1
hχ,XY H k,BLkj(w)i∣ ≤ Ckxl (1 + kwkφ) Pnlog(2∕δ).	(13)
j =1 l=N
By a direct application of (Vershynin, 2018, Theorem 8.1.6), we derive the following corollary. We
remind the reader that CΦ is the covering number defined in Definition 3.
Corollary 1. Suppose x, w ∈ Rd are fixed. Then, there exists a universal constant C such that with
probability at least 1 - δ, we have:
N j + 1	「	______]
X Y H k,BLkj(w)∣∣ ≤ C(1 + kwkφ)√n Cφ + Jlog(2).
j =1 l=N	φ
20
Published as a conference paper at ICLR 2022
In order to apply the theorem above, we will need to control kw1k,1 kφ uniformly for all k. The
following lemma presents such a bound and we refer to Section L.4 for the proof.
Lemma 10. Suppose Wk,1 are the iterates of Q-Rex with coupled data from
There exist universal constants C, C1,C2 such that whenever NB > CiK log
a ZIBEL MDP.
(δ(KκY)), η <
C2 cφ +1-g(K/δ and ηB <
1, with probability at-least 1 一 δ, the following hold:
1.	FOrevery 1 ≤ k ≤ K, ∣∣Wk,1∣∣φ ≤ 1-γ
2.	For every 1 ≤ k ≤K, k^k kφ ≤ q/-；)exp(-ηNκB)+(C√) hCΦ+ʌ/log(2K)i
H Proof of Theorem 1
Proof. Consider the Q learning iteration:
Wk+1,1 = T(Wk,1) + 2k .
Using Lemma 4, We conclude: Wk+1,1 — w* =T(Wk,1)-T(w*) + ekand thence:
kWk+1,1 - w*kφ ≤ Y∣∣Wk,1 - w*kφ + SUp ∣∣eιkφ .
l≤K
Unrolling the recursion above, We conclude:
K+1,1	* K *	sUpl≤K ∣el∣φ
kw1 十,一 w kφ ≤ Y IlW kφ + ——--------------
1 一 Y
NoW, We invoke item 2 of Lemma 10 along With the constraints on N, B , K, η and Lemma 2 to
conclude the result.
□
I Proof of Theorem 2
In this section We analyze the output of Q-Rex in the tabular setting and obtain convergence guar-
antees. To connect With the standard theory for tabular MDP Q-learning in (Li et al., 2021), let
Us Use the standard Q-function notation where We assume for all k, Qk,1 ∈ Rs×a and We have
that Qk,1(s,a) =(Wk,1, φ(s, a)). Since φ(s,a) are the standard basis vectors, we must have
k1 k1
Q1, = w，. In the tabular setting, we see by using Lemmas 5, 6, 7, and 9 that for any δ > 0,
there exists a universal constant C, whenever NB ≥ CμTmx- log( 1S11AK), with probability at-least
1 一 δ, for every k ∈ [K] and every (s, a) ∈ S × A:
Q k+1,1 = T [Qk,1] + 氤,	(14)
where ek ∈ Rs×a is such that for all (s, a),
I---------------------------------------- 1	∕K∣S∣∣Ah
熊(s, a)| ≤ C,ηlog (KSA) (1 + Y2Varp(V)(s, a； Q?)) + Cn g1 一 Y )
+(1 - n)μmi2rB Qk,1-τ [Qk,1i∣∣	.	(15)
Now that we have set-up the notation, we will roughly follow the analysis methods used in (Li et al.,
11
2021). Since we start our algorithm with Q11,1 = 0, and rt ∈ [0, 1] almost surely, we can easily show
k1
that Q1,1(s, a) ∈ [0, ɪ—^] for every k, s, a. Therefore, we upper bound
Q1,1-T [Qk,1i∣∣	≤ 十
∞
21
Published as a conference paper at ICLR 2022
in Equation (15) to conclude that with probability at-least 1 - δ, for every k ∈ [K] and every
(s, a) ∈ S × A:
归k(s, a)| ≤ C Jηlog(KS1Ai)γ2 [Varp(Vk)(s, a)] + α
(16)
(KISUAl、	/	7~~-~~	( ημminNB
Where。〃 := Cη 认一)+ CJn log (KSIAJ + exp(- -2—
k1
NoW We define ∆k = Q1, - Q* and ∏k : S → A to be the deterministic policy given
k1	k1
by Q1k,1 i.e, πk(s) := arg supa∈A Q1k,1(s, a) and π* to be optimal policy given by π*(s) :=
arg supa∈A Q* (s, a). We use the convention that We pick a single maximizing action using some
rule Whenever there are multiple. Similarly, We let Pπk , to be the Markov transition kernel over
S X A given by Pπk ((s, a), (s0, a0)) = P(Sls, a)l(a0 = ∏k(s0)). Similarly, we define Pπ* with
respect to the policy π*. It is easy to shoW that:
T(Q k,1) = R + YPπk Qk,1
Similarly,
Q* =T(Q*) =R+γPπ*Q* .
Furthermore given any Q ∈ RS×A, letting πQ being the greedy policy with respect to the function Q
we have that for any policy π, it can be easily seen from the definitions that following element-wise
inequality follows:
PπQ ≤ PπQQ.
We now use Equation (14) along with the equations above to conclude:
γPπ*∆k + Wk ≤ ∆k+ι ≤ γPπk∆k + 氤.	(17)
Here, the inequality is assumed to be point-wise. By properties of Markov transition kernels, we can
write:
XK γK-k (Pπ*JK-kWk+γK(Pπ*JK∆1 ≤ ∆K+1
k=1
≤ XK γK-k kY+1 P πl Wk+γK Y1 Pπl ∆1. (18)
k=1	l=K	l=K
Here, we use the convention that QlK=+K1 Pπl = I . We bound the lower bound and the upper bound
given in Equation (18) separately in order to bound k∆K+1 k∞.
We first consider the lower bound. Using (Azar et al., 2013, Lemma 7), we have:
Il(I-YPπ*)TpVaP所∣∣∞ ≤ ʌʌɪɪ.	(19)
We also note from (Li et al., 2021, Equation 64) and basic calculations that:
kPVarp(Vk) - PVarP(V*)∣∣∞ ≤ PkVarP(Vk) — VarP(V*)k∞
≤ ʌ/ɪ∣∆kk∞ .	(20)
1- Y
22
Published as a conference paper at ICLR 2022
Using Equations (15) (16), we conclude that there exist universal constants C, C1 such that with
probability at least 1 - δ (interpreting the inequalities as element-wise):
∆K+1 ≥ - αη+^K - C Jηγ2iog( KfAI)XX YKi (P π*)K-k pvarp(Vk)
1 - γ	k=1
≥ -αη+jK - C Jηγ2 iog(3) XXYKi (pπ*)K-k PVarP(V^
1 - γ	k=1
-Ciriɪ Jηγ2 Iog(KSL凶)XX YKiPk∆kk∞
1	- Y	k=1
≥ -01- YK - C,ηγ2 log(KSIAy(I - YPπ*)-iPVarp(V*)
-Ciriɪɪ JnY2 Iog(KSMi) XXYKipk∆ku
Y	k=1
≥-『-C S「log( KfAI)
1 - Y	(1 - Y)3	δ
-Cirɪ JnY2 iog(KS凶)XXYKiPk∆ku.	(21)
1 - Y	k=i
In the above chain, the first inequality follows from Equations (16) (18), the second inequality from
Equation (20) and the fourth inequality from Equation (19). For the upper bound consider the
following set of equations interpreting them element-wise which hold for a universal constant C and
with probability at least 1 - δ.
Δk+ι ≤ XX YK-k f∏ Pπ) Wk + YK (YI Pπ) ∆ι
k=i	l=K	l=K
,K	y------------ K	∕k+1	∖
≤	αι+^ + CJnY2 iog(KfAI) XYKi ∏ Pπι pvarprn
Y	k=i	l=K
,K	l------------ K	U ∕k+1	∖
≤	αl+^ + C JnY2 log(KfAI) X YKit ∏ P∏1 VarP(Vk)
Y	k=i	l=K
α1+2K + C JnY2 log(KfAI、XXYK-kYK-kt (∏Pπ) VarP(Vk)
Y	k=i	l=K
≤
≤
a + YK
I- Y
+ C三
Ku
X	YK-k ut
k=i
K
X	YK-k
k=i
k+i
Pπl VarP(Vk)
l=K
a + YK
I- Y
∖
K
X	YK-k
k=i
k+i
Pπl VarP(Vk)
l=K

≤ J + C" log( KfAI)]"十 JK-k (∏ Pn)VarP (Vk)+ 二.
(22)
In the above chain, the first inequality follows from Equation (18), the second inequality follows
from Equation (16), the third inequality follows from Jensen’s inequality and noting that Pπ is a
Markov operator, the fourth inequality via Cauchy-Schwartz and the last inequality by noting that
that ∣∣VarP(Vk)I∣∞ ≤ 1/(1 - y)2.
23
Published as a conference paper at ICLR 2022
It can now be verified that (Li et al., 2021, Lemma 5) applies in our setting to conclude that:
K	k+1	4
kX+ιγK-k tY Pnl)VarP (Vk) ≤ γ2(i-γy (1+2k∕2嘴 ≤K N k∞
Using the equation above, along with Equations (22) and (21), we conclude there exists a universal
constant C such that with probability at least 1 - δ, we have:
"k∞ ≤ αη-K+C r (ι⅛ log(KSMI)j+寸悄""' k∞ 十一
(23)
We note that this works with every K replaced with l for any l ≤ K, importantly under the
same event (with probability at least 1 - δ) described above for which Equation (23) holds. De-
fine L
K > C2
二 K for some S = dlog2(l + Clog(y⅛γ))]. Under the conditions of the Theorem, i.e,
(i-γ) (log( 土))2, we have 匕< 1. Therefore we conclude from the discussion above
that for every K ≥ l ≥ L, we must have:
∣A+ιk∞ ≤ J+C r Uy log(+)r1+1/2需/c
≤『+ C∖∕⅞φ + C∖∕⅛φ ∕7m0x7k∆u (24)
1 — Y V	(1 — γ)3	V (1 — γ)3	y 1∕2+1≤k≤1
To analyze this recursion, we have the following lemma which establishes hyper-contractivity,
whose proof we defer to Section M.8.
Lemma 11. Suppose α,β ≥ 0. Consider the function f : R+ → R+ given by f (u) = α + β√u.
"ha the uniquefixedpoint: u* =( β+√Pα S-F°r t ∈ N,den0ting f (t) to be the t fold
composition of f with itself, we have for any u ∈ R+ :
If⑶(U) — u*| ≤ β(2-2t-1 )|u — u" 21t .
Now consider for 0 ≤ a < s,
ua :=	sup	k∆l+1 k∞
2Ka ≤1≤K
In lemma 11, define f with α
fixed point u* is such that:
』+ C 尸'and β = C 厂「.The
u* ≤ C
η log( KS凶)
(1 — Y)3
+ a + YL + In log( KSLIAi )
+ 1 - Y N (1 - Y)3
(25)
Clearly, by Equation (24), we have: ua ≤ f(ua-1) and k∆K+1 k ≤ f (us-1). By monotonicity of
f(∙) and the fact that uo ≤ τ⅛γ (Lemma 1):
us-1 ≤
f(s-1)(u0) ≤
Therefore,
∣∣δk+i∣∣∞ ≤ f(Us-I) ≤ f(S) (1-Y)
24
Published as a conference paper at ICLR 2022
Applying Lemma 11, we conclude:
1	1
k∆κ+ιk∞ ≤ u* + β(2-2s-τ )∣τ⅛γ - u*∣2s	(26)
Note that under the constraints on the parameter η and K as stated in the Theorem, we must have
U ≤(i-Y)3. By our choice of s, We must have: |(J Y) — u*| 2s ≤ C 0. Using this in Equation 26
and the fact that β(2-2s-1) ≤ β + β2, we conclude that with probability at-least 1 一 δ, we must
have:
,,ʌ	!|<G[n bg( —)+ αη + YL	、正?KT
"△K+1k ≤ C [	(1 — Y)3	+ ~T-T + V (1-Y)3
This proves the first part of the theorem. For the second part, we directly substitute the values
provided to verify that we indeed obtain error.
J	Proof of Theorem 3
We will now show uniform convergence type result under Assumption 5. For (s, a) ∈ S × A and
s0 ∈ SUpp(P(∙∣s, a)). We define the random variables for all k:
NB
Pk (s |s, a) = ηXX(1 - η)Nkj (sa I(∙⅛ = s0,∙⅞j = s, akj = a)
j=1 i=1
NB
Pk (s0∣s,a):= η XX(1 -η)Nij(S,O) P (s0∣s,a)l(gkj = s,ak = a).
j=1 i=1
Lemma 12. Suppose Assumption 5 holds. Then, with probability at-least 1 - δ, we must have for
any fixed (s, a):
X	|Pk(s0∣s,a) - Pk(s0|s, a)| ≤ C Jηllog(4∕δ) + CndIog(4∕δ)
s0∈sUpp(P (I s,O))
We refer to Section L.5 for the proof. We now proceed with the proof of Theorem 3. Recall the
noiseless iteration Wk defined in the discussion following the statement of Lemma 4. We define
Dk := wk,0 - Wk. Observe that we cannot apply Lemma 9 as in the proof of Theorem 2 where
we used w = w0k,0 in order to bound kkk∞. This is because of data re-use which causes w0k,0 to
depend on the ‘variance’ term.
However, note that Wk is a deterministic sequence and we can apply Lemma 9 and then use the fact
that Wk ≈ w0,0 to show a similar concentration inequality. To this end, we prove the following
lemma:
Lemma 13. In the tabular setting, we have almost surely:
N j+1	N j+1
φ(s, a), X Y H k,,B Lkj (w)-XY H k,B Lkj (VM
j=1 l=N	j=1 l=N
≤ YkW -vkφ X	IPk(s0|s,a) - Pk(SlS,a)|	(27)
s0∈sUpp(P (I s,O))
The proof follows from elementary arguments via. the mean value theorem and triangle inequality.
We refer to Section M.7 for the proof. We are now ready to give the proof of Theorem 3.
Proof of Theorem 3. We proceed with a similar setup as the proof of Theorem 2. Notice that due to
data reuse, the noise ∈k in outer loop k is given by eι(Wk,1)
Qk+1,1 = τ [Qk,1] + eι(Q k,1).
25
Published as a conference paper at ICLR 2022
We also define the noiseless Q iteration such that Q；,1 = Q；'1 and Qk+1,1 = T(Qk,1). Let ∆k :=
Q k,1 - Qk,1.
Now, by Lemma 5, We can write down
N j+1
e1(Qk,1)G a) = (1 - η)N 13a)(Qk，1 - Qk,*) + hφ(s, a), X ∏ H 1,,BLIj(Qk,1 )〉
j=1 l=N
N j + 1
=(1 - η)N 1(s,a)(Qk,1 - Qk,*)+(φ(s,a), Xn H 1,'B (L 1"(Qk，1) - L 1"(Qk，1 ))〉
j=1 l=N
N j+1
+ <φ(s,α),χ∏ HIBL 1,j (Qk，1)〉	(28)
j=1 l=N
We bound each of the terms above separately. By union bound, the following statements all hold
simulataneously with probability at-least 1 - δ. By Theorem 7, whenever NB > CJTm^ log(ISIA),
μmin	δ
we must have N(1)(s, a) ≥ μmin∙NB.	A simple observation using recursion shows that
Qk,1(s, a),Qk'1(s, a) ∈ [0,1-1γ]. Using Lemmas 12 and 13 we conclude that we must have:
N j + 1	I
—〈…,χ ∏ H¾ (L1,j(Qk，1) - L 1,j(Qk，1))〉∣
≤ CkAkk∞ ,ηliog(母Ai) + ηRog(ISF)	(29)
Now observe that Qk，1 is a deterministic sequence. Therefore, using Lemma 9 uniformly for every
k ≤ K and (s, a) ∈ S ×A:
N j+1	l------------------------------ nlo‹r(∣S∣∣A∣K
"a), X ∏ H 1，BL1，j(Qk，1 )〉≤ CJnlog(iSAK)(1+ VarP(Vk)(s,a)) + Cn g] _ :
j=1 l=N	Y
(30)
Where, VarP(匕):=VarP(V)(s, a; Qk，1).
We now combine all the above with Equation (28) to show that with probability at least (1 - δ), we
must have uniformly for every k ≤ K and (s, a) ∈ S ×A:
后(Qk，1)(s,a)| ≤ 占 exp (-≡m⅞NB) + CkAkk∞ .log(") + nJlog(≡Ai)
+ C q log( isiAK )(1+ VarP (Vk )(s,a)) + CnloglHAH	(31)
We will now present a crude bound on k Ak ∣∣∞ to reduce the analysis of Q-RexDaRe to the analysis
of Q-Rex.
Claim 1. Whenever η ≤ C1 _(；-S)I A、, With probability at-least 1 一 δ, we must have for ^very
d log	δ~)
k ≤ K uniformly:
/_ ημminNB A
kAkk∞≤Ce"--J	J + C√(⅛log(≡AK)^
Applying this directly to Equation (31), we conclude that with probability at least (1 一 δ), we must
have uniformly for every k ≤ K and (s, a) ∈ S ×A:
26
Published as a conference paper at ICLR 2022
归ι(Qk,1 )(s,a)∣ ≤ (1jCγyexP (-ημm2NB) + Clθ皆AK)P
.----------------------------------------------- 1	/|S||A|K\
+ CQnlog(ISJAK)(1 + VarP(Vk)(金 + 0”。gI- Y，	(32)
Proof of Claim 1. First note that T isγ contractive under the sup norm. Therefore,
k∆k+ιk∞ = T(Wkj)-T(Wk,1) + Wk
∞
≤ τ(wk,ι)-τ(wk,1)	+ kM∞
∞
≤ Yk∆kk∞ + IIWkk∞	(33)
In Equation (31), note that VarP(Vk)(s, a) ≤(二尸.Therefore, under the conditions of this Claim,
we can take C1 small enough so that uniformly for every (s, a) and k ≤ K with probability at-least
(1 - δ), Equation (31) becomes:
exp (-ημm≡NB)	八 _八	Z______________
Ie (Q k,1)k∞ ≤ —4--ʒ-- + k∆ k k∞ 1~^)+c q (⅛ iog( ≡AKy.
Combining the display above and the fact that Wk = W1(QW 1k,1) in Equation (33), we conclude that
with probability at-least 1 - δ, for every k ≤ K uniformly:
ex (_ ημminNB A
k∆k+ιk∞ ≤ + k∆k k∞ + * ι-γ2_+ + C J- log(吗吗	(34)
Unrolling the recursion above and using the fact that ∣∆ 11∣∞ = 0, we conclude the statement of the
claim.
□
We also note that the deterministic iterations (ək,1 converges exponentially in SUP norm to Q* due
to Y contractivity of T. That is:
k
kQk,1- Q*k∞ ≤ (1Yr^
(35)
We are now ready to connect up with the proof of Theorem 2 with minor modifications. We follow
the same analysis as the proof of Theorem 2 but with VarP (Vk) replaced with VarP (Vk). Similar
to the proof of Theorem 2, We can control VarP (Vk) with respect to VarP (V*) and kQk,1 — Q*k∞
along with Equation (35). We also replace αη with
靖：=二exp (- - )+C13C'
Therefore, we conclude a version of Equation (23):
k∆κ+ιk∞ ≤ αdr + YK + Cr7rɪ13log(KSA)ʌ∕l + τYK/^	(36)
1 - Y	(1 - Y)3	δ	(1 - Y)
Where ∆k := Q1k,1-Q*. Using the bounds on the parameters given in the statement of the Theorem,
we conclude the result.
□
27
Published as a conference paper at ICLR 2022
K	Minimax Lowerb ounds for Tabular MDPs
Suppose Θ denotes the class of all tuples (M, π) where M is a tabular MDP and π is an exploratory
policy such that under the policy π, the MDP achieves a stationary distribution μ with minimum
probability at-least μmin (see section 2.1). The rewards are almost surely in the set [0,1]. We
receive the stationary sequence (st , at , rt)tT=1 from MDP M under policy π, which we denote as
(st, at, rt)T=ι 〜(M, ∏). Let F be the class of all estimators f which estimate Q* for the MDP M
with f((st, at, rt)tT=1). We write the minimax risk as:
L(θ,T) := inf	SUp E(st,at,rt)T=1 〜(M,π)kf((st,at,r∙T=I)- Q*k∞
f∈F (M,π)∈Θ
Theorem 5. There exists a constant C such that, for every μmin ∈ (0,1/4), Y ∈ (0,1/2) and
T ≥ C-----1——ʌ, we must have:
—	"min(1 —Y)
1
L(θ,T)≥C
T(1 - Y)3μmin .
Therefore, we need T ≥ 声μ . (「丁/ in order to achieve e errorfor the Q values.
Proof. We will use Le-Cam’s two point method to prove the result. Consider a class of MDPs
denoted by MDP(q,p) (p, q ∈ [0, 1]) with state space S = {0, 1}, only one possible action, reward
function R : S → R, R(s) = s, and discount factor γ ∈ [0, 1). The transition probability for the
MDP is given by P (0|0) = 1 - q, P (1|0) = q, P (0|1) = p and P (1|1) = 1 - p.
Under these conditions, the stationary distribution is given by μ(0) = Pppq and μ(1) = p++q and the
value function can be written as
V(1)
1
1 - Y(I -P) - γph(q)
V(0)
_________h(q)________
1 - γ(1 - p) - γph(q)
where h(q) := 堀").Since there is only one action in each state, the value function coincides
with the Q function.
For showing the lower bound, consider two MDPs with parameters (P1, q) and (P2, q) with q <
min{P1,P2} and P1, P2 < 0.5. The stationary trajectories of length T under the two MDPs, denoted
by the random variables X1(:iT) for i = 1, 2, satisfies
KL(X1(:2T) ||X1(1:T)) ≤
2q(pι - P2)2	+ τ 2q(pι - PN
(pi + q)(P2 + q)2 ln2	(P2 + q)pι ln2
using KL(Ber(a)∣∣Ber(b)) ≤ 2(b-2) whenever b ≤ 1/2.
Observe that 四，1 = μ(2)(1) = pqq and 仙口\ = μ(1)(1) = ^+pq. Combining this with the KL
divergence bound above, we conclude that KL(X(2T||X(1T) < 8 if we set Now let pi = 1-γ,
P2 - pi = C / p1i)for small enough constant C and T & P(Ppq). (notice that the equation gives
V Tμmin
p2 < 0.5 when T satisfies the condition above and pi < 0.25). By Pinsker’s inequality, we must
have TV(X(2T,X(1T) ≤ 2.
Elementary calculations show that |V (i)(1) -V (2)(1)| &
1-γ. Therefore:
|P1—P2|
TT-YF
(⅛ √i⅛since q<pι
i
|V (i)(1) - V (2)(1)| &
1
T(1 - Y)3μm)n
28
Published as a conference paper at ICLR 2022
Consider the semi-metric P(x,y) = I 1-γ(1-χ1)-γχh(q) - 1-γ(1-y1,-γyh(q) I for x, " ∈ (O, D Let
Pi be the distribution of X1(:iT) , and let the distribution be parameterised by θ(Pi) = V (i)(1) for
i = 1, 2. Let θ be any estimator based on the observations. By (Wainwright, 2019c, Chapter 15),
min max ^EP [∣θ — θ(P)|] ≥ Cj
T (1 - γ)3μm)η
(1 - TV(X1(:2T), X1(:1T)))
≥ C1
~ V
τ (i-γ )3μm)n
(37)
1
1
=⇒ for less than error, T has to be at-least
1
μmin^2(i-γ)3
q = μmnμ-Y), T & (1-γ)μmin, We can ensure that 〃m1 = 2μmiη and 〃2\ ≥ μmiη, which ensures
that MDP(p1 , q) and MDP(p2, q) are both elements of Θ. Therefore, the two point risk in the LHS
of Equation (37) lower bounds L(Θ, T), which allows us to conclude the result by substituting
〃m?n = μmin ∙
□
L	Proofs of Concentration Inequalities
L.1 Proof of Lemma 8
First, we leverage the techniques established in (Jain et al., 2021b, Lemma 28) to show Lemma 14.
The proof follows by a simple re-writing of the proof of the aforementioned lemma which uses a
linear approximation (in η) to HkB. We omit the proof for the sake of clarity.
Lemma 14. Suppose ηB < 3. Then, thefollowing PSD inequalities hold almost surely:
i=1
I-2η
i=1
(38)
Proof of Lemma 8. We begin by proving the first part. Using Assumption 1 and the fact that the
decoupled trajectory (s, α)t is assumed to be mixed at the start of every buffer, We begin by first
noting from Lemma 14 that:
0 W E(Hkj N H1j W I - ηBE(s,a)~μφ(S, α)(φ(S, aO)T W (I-吟 )I .	(39)
NoW, observe that:
1	N-1	1
k γ HIjgk2=g> Y(HIj)>[(Hk,,N)>hk,,Ni γ Hk,,Bg	(40)
j=N	j=1	j=N-1
Now note that HkBN is independent of Hk,j for j ≤ N 一 1. Therefore, taking conditional expecta-
tion conditioned on H1k,,Bj for j ≤ N - 1 in Equation (40) and using Equation (39), We conclude:
11
EkY HIjgk2 ≤ (1 -喙Ek Y HIjgk2
j=N	j=N-1
Applying the equation above inductively, we conclude the result.
We now prove Part 2. We apply Markov’s inequality to Part 1 along with Lemma 2 to show that
with probability at least 1 - δ:
1	1	(_ ηNB)
k Y HIjgkφ ≤ k Y Hkjgk ≤ *√δκ J kgk ≤ exp(-ηNB) Jikgkφ .
j=N	j=N
□
29
Published as a conference paper at ICLR 2022
L.2 Proof of Lemma 9
Proof. We intend to apply Freedman’s inequality (Freedman, 1975) like in (Li et al., 2021, Theorem
4), but in an asynchronous fashion and with Markovian data. Here, reverse experience replay endows
our problem with the right filtration structure. Using Equation (9), we can write
N j+1	N B
hφ(s, a),XY H k,B L k,j (W)) = η XX Xk	(41)
j=1l=N	j=1 i=1
Where Xkj = (1 - η)Nk,j(S，a璃,jl((Skj,Skj) = (s, a)). This allows Us to define the sequence
of sigma algebras Fik,j = σ((sSkm,l, Sakm,l) : (l > j and m ∈ [B]) or (l = j and m ≤ i)) - that is,
it is the sigma algebra of all states and rewards which appeared before and including (sSik,j , aSik,j )
inside the buffer j and all the states in buffers l > j. Notice that NSik,j (s, a) is measurable with
respect to the sigma algebra Fik,j . Using the fact that the buffers are independent, we conclude that:
E hXik,j|Fik,ji = 0and
E [∣Xik'j|2∣Fik'j] ≤ 2 [1 + YVarP(V)(s,a; w)] 1((§”&》)= Ga))(I- η)2N”(Sa .
It is also clear from our assumptions that ∣Xj,k | ≤ ι-2Y almost surely. Consider the almost sure
inequality for the sum of conditional variances:
NB
Wk =XXE |Xik,j|2|Fik,j
KB
≤ 2 [1 + YVarP(V)(s,α; w)] XXI((S” a了) = (s,a))(1 - η)2N”(S⑷
j=1 i=1
=2 [1 + YVaP(V)(s,a; w)] "NX)T(I-η)2t ≤ 2 ( 1 + *[V…W))	(42)
We now apply (Li et al., 2021, Equation (144),Theorem 4) with R = ι-1γ and σ2 =
2 (1+γ2VarPnV乂s,a;W)) to conclude the result.	□
L.3 Proof of Theorem 4
Proof. For the sake of convenience, in this proof we will take σ2 := 4(1 + kwk2φ). Suppose λ ∈ R.
For 1 ≤ m ≤ N consider:
N -m+1	N -m+1	>	N	j+1
Xm := ηλ2σ2 x, Y	HS1k,,Bl	Y	HS1k,,Bl	x +λ x, X Y HS1k,,Bl	LSk,j(w)
l=N	l=N	j =N -m+1 l=N
X0 := kxk2ηλ2σ2
j+1 k l
Here we use the convention that l=N H1k,,Bl = I whenever j + 1 > N. We claim that the sequence
exp(Xm ) forms a super martingale under an appropriate filtration. In this proof only, consider the
sigma algebra Fm to be the sigma algebra of all the state action reward tuples in buffers N, . . . , N -
m + 1 and let F0 be the trivial sigma algebra. Notice that exp(Xm) is Fm measurable.
Lemma 15. Fix N ≥ m ≥ 1. Suppose Y ∈ Rd is a Fm-1 measurable random vector. Then,
E exp (ηλ2σ2hY,Hk,,N-m+1 (仃器-m+1)> Y) + λhY,Lk,N-m+1(w)ξj ∣Fm-1 ≤ exp 5入纭|丫『)
30
Published as a conference paper at ICLR 2022
In particular, taking Y = (QNNm+2 HkB) X, we conclude that exp(Xm) is a super martingale
with respect to the filtration Fm
Proof of Lemma 15. In the proof of this lemma, we will drop the superscripts k, m for the sake of
convenience and due to conditioning on Fm-1, we will treat Y as a constant. Now we define the
natural filtration on the buffer under consideration (Gi)iB=1 where Gi is the sigma algebra of the all
state-action tuples from (s1, a1), . . . , (si, ai) and rewards (rh)1≤h<i.
B
Note that by the definition of L(w), We write: L(w) = Ei=I η*≡i(w)H1,i-1Φi. With this in mind,
h
for h ∈ [B] define Lh(W) = Ei=I η<≡i(w)H1,i-1Φi. Now, L(w) = ηeB(W)Hi,b-iΦb + LB-I(W)
Now, notice that the random variables hY, LB-1i,hY, H1,B-1φBi and hY, H1k,,Bm H1k,,Bm	Yi are
Gb measureable. Furthermore, we must have: E ∖bb(W)IGb] = 0 and 归B(w)| ≤ 2(1 + ∣∣w∣∣φ) ≤
√2σ. Therefore, applying conditional Hoeffding's lemma, we have:
E
exp
>
hY,Hι,B (H?)	Yi + λhY,L(w)i
GB
exp
YH 1,B
≤ exp
hY,H 1,B
(Hi,b)> Yi + λhY,LB
(Hi,b)> Yi + λhY,LB
1(w)i E
exp (λη展B(w)hY, Hi,b-iΦbi) ∣Gb
-i(w)i ) exp (λ2η2σ2∣hY,Hι,B-1φB i|2
(43)
In the third step we have used the conditional version of Hoeffding’s lemma. Now, consider Z :
.~ . -∣-
(H 1,B-1)τ Y. Clearly,
>
XH 1,B (H 1,B)Yi + η∣hY,H 1,b-iΦbi|2 = Z>H>BHb,bZ + ηZ>Φb(Φb)>Z
=Z> [i - ηφB(Φb)> + kΦBk2η2ΦB(Φb)[ Z
≤ kZk2
Here we have used the fact that η < 1 and ∣∣Φb k < 1. Using the bounds above in Equation (43), we
conclude:
exp
≤ exp
(ηλ2σ2 hY, Hi,b (Hi,b) > Yi + λ(Y, LB (w)〉)卜B
(ηλ2σ2hY, Hi,b-i (H 1,B-1)> Yi + λ(Y, LB-ι(w)i
(44)
E
Using Equation (44) recursively, we conclude the first statement of the lemma. The last part of the
lemma follows easily from the definition of Xm .
By Lemma 15, we conclude that exp(Xm) is a super martingale with respect to the filtration Fm.
Therefore, we must have:
E exp(XN) ≤ exp(X0) = exp(ηkxk2λ2σ2) .
It is also clear that XN ≥ λ∕x, PN=N-m+1(Qj+N HkB) Lkj(W) Y Applying Chernoff bound
with we conclude that the concentration inequality in Equation (13). We can then directly apply
(Vershynin, 2018, Theorem 8.1.6) to Equation (13) in order to obtain uniform concentration bounds.
□
31
Published as a conference paper at ICLR 2022
L.4 Proof of Lemma 10
Proofof Lemma 10. By definition of ek, We have: Wk+1,1 = T(Wk,1) + Wk. Therefore, for any
(s, a) ∈ S × A, we must have:
hφ(s, a), wk+1,1〉= R(s, a) + YEs0~P(∙∣s,α)
sup hφ(s0, a0), WW1k,1i + hφ(s, a), Wki .
a0∈A
Using the fact that R(s, a) ∈ [0, 1], We conclude:
kWW1k+1,1kφ ≤ 1 + γkWW1k,1kφ + kWkkφ
(45)
By independence of outer-loops for the coupled data, We note that WW1k,1 is independent of the data
in buffer k. Therefore We can apply Theorem 4 (and resp. Lemma 8) conditionally With W = WW1k,1
(and resp. g = Wk,1 — w*), the bias variance decomposition given in Lemma 5 and the bound on
∣∣w*kφ in Lemma 4 to conclude that with probability at-least 1 一 δ, for every k ≤ K:
k(⅛ kφ ≤ 4 ~δ- exp(- ηNκB )(kwk,ko+1 一 Y)
+ C(1 + ∣wk,1kφ)√η Cφ + √log(2K)	(46)
We now choose constants C1 and C2in the statement of the Lemma such that Equation 46 implies:
∣≡k ∣Φ ≤1一γ ∣wk,1kφ+ι
Using the equation above in Equation (45), we conclude:
kwk+1,1kφ ≤ 2+142 ∣wk,1kφ.
Unrolling the recursion above and noting W11,1 = 0, we conclude that with probability at-least 1 一 δ,
we must have IlWk,1∣∣φ ≤ ɪ-4Y for every k ≤ K. The bound in item 2 follows by using item 1,
Equation (46) and the fact that Wk,1 is independent of the data in buffer k due to our coupling. 口
L.5 Proof of Lemma 12
Proof. For the sake of convenience, we will take d = ∣supp(P(∙∣s, a))| and index SUpp(P(∙∣s, a))
by [d]. Consider Y ∈ {-1,1}d. We consider the class of random variables indexed by elements of
{一1, 1}d:
∆(Y; s, a) = X Ys0 hP5k(s0∣s,a) — P(Sls, a)]
s0
The proof proceeds in a similar way to the proof of Lemma 9 via the Freedman inequality. To bring
out the similarities we define similar notation. Consider the sequence of sigma algebras Fik,j for
i ∈ [B] and j ∈ [K] as defined in the proof of Lemma 9. We now define
Xikj(Y) := (1-η)Nk,j(S，a) XYso (l(⅛+1 = s0Mj = s, aik,j = a) — P(s0∣s,a)l(∙⅛j = SMj = a)).
s0
We note that E Xik,j(Y)|Fik,j = 0 and |Xik,j(Y)| ≤ 2 almost surely and a simple calculation
reveals that: E [xk,j(Y)|2|Fikji ≤ (1 一 n)2"" l(sk,j = s, ɑk,j = a)
It is also clear that: ∆(Y; s, a) = η PjK=1 PiB=1 Xik,j (Y). Apply Freedman’s concentration in-
equality, we conclude for any fixed Y ∈ {一1, 1}d and (s, a) ∈ S × A,
P(∣∆(Y; s,a)∣ >CPlog(2∕δ) + ηlog(2∕δ)) ≤ δ
32
Published as a conference paper at ICLR 2022
Applying a union bound over all Y ∈ -1, 1d, we conclude:
P( sup	∣∆(Y; s,a)∣ > CJnllog(4∕δ) + ηllog(4∕δ)) ≤ δ .
Y ∈{-i,i}j
We complete the proof by noting that
sup	∣∆(Y; s,a)∣ = X	|Pk(s0∣s,a) - Pk(s0∣s,a)∣
Y ∈{-1,1}d	s0∈supp(P (∙∣s,α))
□
M Technical Lemmas
M.1 Coupling Lemma
We first introduce some useful notation: Let Dk,j (resp. Dk,j) be the tuple of random variables
(s"a""等1 (resp. (Ma”*"/1).
Lemma 16. Suppose
{Cτmiχ log( T) in the tabular setting
CTmix log( CTG) in the general setting
(47)
Then, we can define the sequences (st, at, rt) and (sat, aat, rat) on a common probability space such
that:
1.	The tuples Dk,j and Da k,j have the same distribution for every (k, j),
2.	The sequence Da k,j for k ≤ N, j ≤ K is i.i.d.
3.	Equation (4) holds
Proof. The proof of this lemma for the tabular case is a rewriting of the the proofs of Lemmas 1,2,3
in (Bresler et al., 2020). For the general state space case, we apply appropriate modifications as
pioneered in (Goldstein, 1979).
M.2 Proof of Lemma 2
Proof. The first part follows from the definitions. For the second part, note that: kxk ≥ kxkφ
follows from Cauchy-Schwarz inequality and Assumption 1. For the reverse inequality we use
Assumption 4 to show that:
∣∣x∣∣φ ≥ E(s,a)〜μhφ(s,a),xi2 ≥ ɪɪ
κ
□
M.3 Proof of Lemma 3
Proof. Existence is guaranteed by Definition (2) and uniqueness follows from the assumption that
span(Φ) = Rd.	□
M.4 Proof of Lemma 4
Proof. Suppose w0, wa0 ∈ Rd are arbitrary. Let w1 = T(w0) and wa1 = T (wa0). For any φ(s, a), we
have:
∣hφ(s,a),wι - Wιi∣ = γ∣Es'〜P(∙∣s,α) sup hφ(s0,a0),woi - E『o〜P(∙∣s,a) sup hΦ(s0,a0),Woi∣ (48)
a0∈A	a0∈A
33
Published as a conference paper at ICLR 2022
By Assumption 2, for every fixed s0 there exist amax(s0),々*ɑ乂(£0) SUch that
SUp hφ(S0,a0(SO)),w0i = hφ(S0, amax(SO)) ,w0i and SUp hφ(SiaO), w0i = hφ(s, 5^(SO)) , w0i .
a0∈A	a0∈A
Therefore for any SO it holds that,
hφ(s', &max), w0 - w0i ≤ SUp hφ(s', a), w0i - sup hφ(s', a0), w0i ≤ hφ(s', amax), w0 - w0i
a0∈A	a0∈A
=⇒ I sup hΦ(S,ao),W0i - sup hΦ(S0,a0),Woi∣ ≤ ∣∣wo - W0∣∣φ .
a0∈A	a0∈A
Combining this with Equation (48), we conclude the γ-contractivity of T. By contraction map-
Ping theorem, T has a unique fixed point we conclude that it is w* by considering Q(s, a):=
hw*, Φ(s, a)i and showing that it satisfies the bellman optimality condition in Equation (2). For the
norm inequality, we note that since R(S, a) ∈ [0, 1] by assumption and w* = T (w*), we have:
∣hΦ(S,a),w*i∣ ≤ 1 + Y ∣∣w*∣φ. Therefore, ∣w*∣ ≤ ι-1γ. The second norm equality follows from
□
Lemma 2.
M.5 Proof of Lemma 5
Proof. We write the iteration in the outer-loop k as:
wk+ι = hi - ηφ-j[φ-j]>i Wkj + ηφ-i [r-,j + Y SUphwf,1,。(我一),a0)i
a0∈A
Using the fact that〈Wk+1,* ,φ-j j = R-,j + YE.~p(/父/父)supao∈AhΦ(S0, a), Wk,1)，and recall-
ing the notation in (5),(6), we write:
wk+1 - wk+1,* = [i - ηφ-j[φ-j]>i (Wkj- wfc+1,*} + ηφ-j-i.
Therefore,
Wkj+1 - wk+1,* = Hkj (Wkj- wk+1,*) + L k,j
Unfurling further, and using the fact that Wk,N+1 = Wk+1,1, We conclude the statement of the
lemma.
□
M.6 Proof of Lemma 7
We let Nkj(S,a) := PB=I l((sk,j,dk,j) = (s,a)). We want to show that the quantity
ρN=1 Ndkj(S, a) concentrates around its expectation. Note that ENdk,j(s, a) = Bμ(S, a) and that
Nd k,j (S, a) are i.i.d. for j ∈ [N ]. From the proof of (Paulin, 2015, Theorem 3.4) and the fact that
γPs ≥	, We conclude:
ps	2τmix
NN
Eexp(λ(X Ndkj(S,a) - Bμ(S,a))) = Y Eexp(λ(Nkj(s, a) - Bμ(S,a)))
j =1	j=1
≤	( 8N(B + 2τmiχ)μ(S, a)τmiχλ2 A
_ exp 1	1 - 20λTmiχ	)
(49)
Following the Chernoff bound in the proof of (Paulin, 2015, Theorem 3.4), we conclude:
P(N k (S, a) - NBμ(s, a)1 > t) ≤ 2exp (- i6τmiχ(B + 2τmiχ)tK 〃(S,a)+40tTmiχ )	(50)
34
Published as a conference paper at ICLR 2022
Taking t = 2NBμ(s, a), whenever B ≥ Tmix (by assumption), We must have:
P (Nk(s,a) ≤ 1 NBμ(s,a)) ≤ 2exp (-CK⅛sai)
for some constant C. Further, via a union bound over all state action pairs (s, a), we conclude the
statement of the lemma.
M.7 Proof of Lemma 13
We first prove a simple consequence of multivariate calculus.
Lemma 17. Let f : Rn → R be defined by f(x) = supi xi. Then, for every x, y ∈ Rn, there exists
ζ ∈ Rd such that kζk1 = 1, ζi ≥ 0 and:
f(x) - f(y) = hζ, x - yi
Proof. We consider the log-sum-exp function. Given L ∈
L log (Pn=I exp(Lxi)). An elementary calculation shows that:
R+, define fL(x)
log(n)
L
+ sup xi ≥ fL (x) ≥ sup xi
Therefore, for any fixed x,
lim fL(x) = f(x) .
L→∞
Now, VfL(x)
kVfL (X)III =
(p1, . . . , pn ) where pi
exp(Lxi)
Pn=I eχp(Lχj).
Clearly, hVfL(x), eii ≥ 0 and
1. By the mean value theorem there exists βL such that:
fL(x)-fL(y) = hVfL(βL),x-yi.	(51)
Now, the simplex in Rn (denoted by ∆n) is compact. Therefore, there exists a sub-sequence
Lk → ∞ such that limk→∞ VfLk (βLk) = ζ ∈ ∆n . Taking limit along the sub-sequence Lk
in Equation (51), we conclude the result.	口
Proof of Lemma 13. In the tabular setting, we have the following expression using Equation (41):
KB
LHS = ηγXX(1 -η)Nk,j(SaIkkjMj) = G。)] Qj(W)-苜,j(v)]	(52)
j=1 i=1
Where
苜j (w) := SUp hw,φ(slk+ι,a) — Eso~p"/j ak,j) SUp hφ(s0,a0),wi
.α0∈A	(Ii , i ) α0∈A	_
Now, by an application of Lemma 17, we show that for some η(∙∣s0, w, V) ∈ ∆(A), depending only
on (s0, w, v), we have:
SUp hw, φ(s0, a0), Wi — sup hw, φ(s0, a0), Vi =	Z(a0∣s0, w, v)hφ(s0, a0), W — Vi
a0∈A	a0∈A	a0∈A
Now, using the definition of Pk(∙∣s, a) and P(∙∣s, a) in the discussion preceding Lemma 12, we can
simplify Equation (52) to:
L.H.S. = Y	^X	(Pk(S0∣s, a) — Pk(s0∣s, a)) ζ(a0∣s0, w, v)hφ(s0, a0), w — Vi	(53)
s0∈sUpp(P (1s,a))
a0∈A
We conclude the statement of the lemma by an application of the Holder inequality.
□
35
Published as a conference paper at ICLR 2022
M.8 Proof of Lemma 11
Proof. We solve for f (u*) = U to obtain the relation: u* = α + β√u*, which after discarding the
negative solution for √Uτ yields the unique solution: u* = (β+ 火2+^)
part, consider for arbitrary x, y ∈ R+, the following hyper-contractivity:
If(χ)- f(y)I= βI√X -√y∣
≤ βp∣χ - y∣
The second step follows from the fact that ∣√a - √b∣ ≤，|a - b∣. Since u*
inequality above:
2
. To prove the second
(54)
f (u*), we apply the
If㈤(u)-u*∣ = |f⑴(U)- f⑴(u*)|
≤ βq(I)(U)- f(tT)(U*)|
(55)
We then conclude the result by induction.
□
36