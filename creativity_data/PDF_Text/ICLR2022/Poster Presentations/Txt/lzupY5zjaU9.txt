Distribution Compression in Near-linear Time
Abhishek Shetty1, Raaz Dwivedi2, Lester Mackey3
1	Department of EECS, UC Berkeley
2	Department of Computer Science, Harvard University and Department of EECS, MIT
3	Microsoft Research New England
shetty@berkeley.edu, raaz @ mit.edu, lmackey@microsoft.com
Ab stract
In distribution compression, one aims to accurately summarize a probability dis-
tribution P using a small number of representative points. Near-optimal thinning
procedures achieve this goal by sampling n points from a Markov chain and iden-
tifymg √n points With O(1∕√n) discrepancy to P. Unfortunately, these algo-
rithms suffer from quadratic or super-quadratic runtime in the sample size n. To
address this deficiency, We introduce Compress++, a simple meta-procedure for
speeding up any thinning algorithm While suffering at most a factor of 4 in er-
ror. When combined With the quadratic-time kernel halving and kernel thinning
algorithms of Dwivedi and Mackey (2021), Compress++ delivers √n points with
O (ʌ/log n/n) integration error and better-than-Monte-Carlo maximum mean dis-
crepancy in O(nlog3 n) time and O(√nlog2 n) space. Moreover, Compress++
enjoys the same near-linear runtime given any quadratic-time input and reduces
the runtime of super-quadratic algorithms by a square-root factor. In our bench-
marks with high-dimensional Monte Carlo samples and Markov chains target-
ing challenging differential equation posteriors, Compress++ matches or nearly
matches the accuracy of its input algorithm in orders of magnitude less time.
1	Introduction
Distribution compression—constructing a concise summary of a probability distribution—is at the
heart of many learning and inference tasks. For example, in Monte Carlo integration and Bayesian
inference, n representative points are sampled i.i.d. or from a Markov chain to approximate ex-
pectations and quantify uncertainty under an intractable (posterior) distribution (Robert & Casella,
1999). However, these standard sampling strategies are not especially concise. For instance, the
Monte Carlo estimate Pinf，n Pn=1 f (xi) of an unknown expectation Pf，EX〜p[f(X)] based
on n i.i.d. points has Θ(n-2) integration error |Pf - Pinf |, requiring 10000 points for 1% rela-
tive error and 106 points for 0.1% error. Such bloated sample representations preclude downstream
applications with critically expensive function evaluations like computational cardiology, where a
1000-CPU-hour tissue or organ simulation is required for each sample point (Niederer et al., 2011;
Augustin et al., 2016; Strocchi et al., 2020).
To restore the feasibility of such critically expensive tasks, it is common to thin down the initial
point sequence to produce a much smaller coreset. The standard thinning approach, select every
t-th point (Owen, 2017), while being simple often leads to an substantial increase in error: e.g.,
standard thinning n points from a fast-mixing Markov chain yields Ω(n-4) error when n 1 points
are returned. Recently, Dwivedi & Mackey (2021) introduced a more effective alternative, kernel
thinning (KT), that provides near optimal Oed(n- 1) error when compressing n points in Rd down to
size n2. While practical for moderate sample sizes, the runtime of this algorithm scales quadratically
with the input size n, making its execution prohibitive for very large n. Our goal is to significantly
improve the runtime of such compression algorithms while providing comparable error guarantees.
Problem setup Given a sequence Sin of n input points summarizing a target distribution P, our
aim is to identify a high quality coreset SoUt of size √n in time nearly linear in n. We measure
coreset quality via its integration error |Pf - PSout f |，|Pf - ∣s1 , Pχ∈sout f (x)| for functions
1
f in the reproducing kernel Hilbert space (RKHS) Hk induced by a given kernel k (Berlinet &
Thomas-Agnan, 2011). We consider both single function error and kernel maximum mean discrep-
ancy (MMD, Gretton et al., 2012), the worst-case integration error over the unit RKHS norm ball:
MMDk(P,Psout)，SUpkfkk≤ι∣Pf - PSoutf ∣.	(1)
Our contributions We introduce anew simple meta procedure—COMPRESS++—that significantly
speeds up a generic thinning algorithm while simultaneously inheriting the error guarantees of its
input up to a factor of 4. A direct application of COMPRESS++ to KT improves its quadratic Θ(n2)
3	1
runtime to near linear O(n log3 n) time while preserving its error guarantees. Since the Od(n 2)
KT MMD guarantees of Dwivedi & Mackey (2021) match the Ω(n- 1) minimax lower bounds of
Tolstikhin et al. (2017); Phillips & Tai (2020) UP to factors of √logn and constants depending on
d, KT-COMPRESS++ also provides near-optimal MMD compression for a wide range of k and P.
Moreover, the practical gains from applying COMPRESS++ are substantial: KT thins 65, 000 points
in 10 dimensions in 20m, while KT-COMPRESS++ needs only 1.5m; KT takes more than a day to
thin 250, 000 points in 100 dimensions, while KT-COMPRESS++ takes less than 1hr (a 32× speed-
up). For larger n, the speed-ups are even greater due to the order Ion^ reduction in runtime.
Compress++ can also be directly combined with any thinning algorithm, even those that have
suboptimal guarantees but often perform well in practice, like kernel herding (Chen et al., 2010),
MMD-critic (Kim et al., 2016), and Stein thinning (Riabiz et al., 2020a), all of which run in Ω(n2)
time. As a demonstration, we combine Compres s ++ with the popular kernel herding algorithm
and observe 45× speed-ups when compressing 250, 000 input points. In all of our experiments,
Compress++ leads to minimal loss in accuracy and, surprisingly, even improves upon herding
accuracy for lower-dimensional problems.
Most related to our work are the merge-reduce algorithms of Matousek (1995); Chazelle & Ma-
tousek (1996); Phillips (2008) which also speed up input thinning algorithms while controlling ap-
proximation error. In our setting, merge-reduce runs in time Ω(n1∙5) given an n2-time input and in
time Ω(n(τ +1)/2) for slower nτ-time inputs (see, e.g., Phillips, 2008, Thm. 3.1). In contrast, COM-
PRESS++ runs in near-linear O(n log3 n) time for any n2-time input and in O(nτ /2 logτ n) time for
slower nτ -time inputs. After providing formal definitions in Sec. 2, we introduce and analyze COM-
press++ and its primary subroutine Compress in Secs. 3 and 4, demonstrate the empirical benefits
of Compress++ in Sec. 5, and present conclusions and opportunities for future work in Sec. 6.
Notation We let PS denote the empirical distribution ofS. For the output coreset SALG ofan algo-
rithm ALG with input coreset Sin, we use the simpler notation PALG , PSALG and Pin , PSin . We
extend our MMD definition to point sequences (S1, S2) via MMDk (S1, S2) , MMDk(PS1, PS2)
and MMDk(P, Si)，MMDk(P, Psi). We use a - b to mean a = O(b), a % b to mean a = Ω(b),
a = Θ(b) to mean both a = θ(b) and a = Ω(b), and log to denote the natural logarithm.
2	Thinning and Halving Algorithms
We begin by defining the thinning and halving algorithms that our meta-procedures take as input.
Definition 1 (Thinning and halving algorithms) A thinning algorithm ALG takes as input a point
sequence Sin of length n and returns a (possibly random) point Sequence SALG of length n°ut ∙ We
s^y ALG is αn-thinning if n°ut = [n∕αnj and root-thinning if an = √n. Moreover, we call ALG a
halving algorithm if SALG always contains exactly [ n C ofthe input points.
Many thinning algorithms offer high-probability bounds on the integration error |PSin f - PSALG f |.
We capture such bounds abstractly using the following definition of a sub-Gaussian thinning
Definition 2 (Sub-Gaussian thinning algorithm) For a function f, we call a thinning algorithm
ALG f -sub-Gaussian with parameter ν and write ALG ∈ Gf (ν) if
E[exp(λ(Pshιf - PSALGf)) | Sin] ≤ exp(λ2≠1) for all λ ∈ R.
Def. 2 is equivalent to a sub-Gaussian tail bound for the integration error, and, by Boucheron et al.
(2013, Section 2.3), if ALG ∈ Gf(ν) thenE[PSALGf | Sin] = PSin f and, for all δ ∈ (0, 1),
∣Psinf - PSALGf I ≤ V(n),2log(2),
with probability at least 1 - δ given Sin .
2
Hence the integration error of ALG is dominated by the sub-Gaussian parameter ν(n).
Example 1 (KT-SPLIT) Given a kernel k and n input points Sin, the KT-SPLIT(δ) algorithm1 of
Dwivedi & Mackey (2022; 2021, Alg. 1a) takes Θ(n2) kernel evaluations to output a coreset of size
nout with better-than-i.i.d. integration error. Specifically, Dwivedi & Mackey (2022, Thm. 1) prove
that, on an event with probability 1 - δ, KT-SPLιτ(δ) ∈ Gf (V) with
V (n) = n0ut√3 ,log( 6noutlog2(n-吗 kkk∞	⑵
for all f with kfkk = 1.
Many algorithms also offer high-probability bounds on the kernel MMD (1), the worst-case inte-
gration error across the unit ball of the RKHS. We again capture these bounds abstractly using the
following definition of a k-sub-Gaussian thinning algorithm.
Definition 3 (k-sub-Gaussian thinning algorithm) For a kernel k, we call a thinning algorithm
ALG k-sub-Gaussian with parameter v and shift a and write ALG ∈ Gk(v, a) if
P[MMDk(Sin,SALG) ≥ an + Vn√t∖ Sin] ≤ e-t forall t ≥ 0.	(3)
We also call εk,ALG (n) , max(vn, an) the k-sub-Gaussian error of ALG.
Example 2 (Kernel thinning) Given a kernel k and n input points Sin, the generalized kernel thin-
ning (KT(δ)) algorithm1 of Dwivedi & Mackey (2022; 2021, Alg. 1) takes Θ(n2 ) kernel evaluations
to output a coreset of size nout with near-optimal MMD error. In particular, by leveraging an appro-
priate auxiliary kernel ksplit , Dwivedi & Mackey (2022, Thms. 2-4) establish that, on an event with
probability 1 - 2, KT(δ) ∈ Gk(a,v) with
an = naPkksplitk∞, and Vn = nv Jkkspl.∣∞ log(6nout 嘴5加。Ut))μ酷履。.,(4)
where kksplitk∞ = supx ksplit(x, x), Ca and Cv are explicit constants, and MSin,ksplit ≥ 1 is non-
decreasing in n and varies based on the tails of ksplit and the radius of the ball containing Sin .
3 Compress
The core subroutine of Compress++ is anew meta-procedure called Compress that, given a halv-
ing algorithm Halve, an oversampling parameter g, and n input points, outputs a thinned coreset of
size 2g√n. The Compress algorithm (Alg. 1) is very simple to implement: first, divide the input
points into four subsequences of size n (in any manner the user chooses); second, recursively call
Compress on each subsequence to produce four coresets of size 2g-1 √n; finally, call Halve on
the concatenation of those coresets to produce the final output of size 2g√n. As We show in App. H,
Compress can also be implemented in a streaming fashion to consume at most O(4g√n) memory.
3.1	Integration error and runtime guarantees for Compress
our first result relates the runtime and single-function integration error of Compress to the runtime
and error of HALvE. We measure integration error for each function f probabilistically in terms of
the sub-Gaussian parameter V of Def. 2 and measure runtime by the number of dominant operations
performed by Halve (e.g., the number of kernel evaluations performed by kernel thinning).
Theorem 1 (Runtime and integration error of Compress) If HALvE has runtime rH(n) for in-
puts of size n, then CoMpREss has runtime
TC (n) = Pβ=o 4i∙ rH('n2-i),	(5)
where 'n，2g+1√n (twice the output size of COMPRESS), and βn，log2( `n) = log4 n — g — L
Furthermore, if, for some function f, HALvE ∈ Gf (VH), then CoMpREss ∈ Gf (VC) with
VC (n) = Pβn0 4-i ∙ νH ('n2-i).	⑹
1The δ argument of KT-SPLIT(δ) or KT(δ) indicates that each parameter δi = ` in Dwivedi & Mackey
(2022, Alg. 1a), where ` is the size of the input point sequence compressed by KT-spLIT(δ) or KT(δ).
3
Algorithm 1: COMPRESS
Input: halving algorithm HALVE, oversampling parameter g, point sequence Sin of size n
if n = 4g then return Sin
Partition Sin into four arbitrary subsequences {Si }i4=1 each of size n/4
for i = 1, 2, 3, 4 do
I Si J COMPRESS (Si, Halve, g)	// return coresets of size 2g ∙ pn
end
S J CONCATENATE(SI, S2, S3, S4)	// CoreSet of SiZe 2 ∙ 2g ∙ ʌ/n
g	g g - Cg
return HALVE(S)	// coreset of SiZe 2g ʌ/n
As we prove in App. B, the runtime guarantee (5) is immediate once we unroll the Compress
recursion and identify that COMPRESS makes 4i calls to Halve with input size 'n2-i. The er-
ror guarantee (6) is more subtle: here, Compress benefits significantly from random cancellations
among the conditionally independent and mean-zero HALvE errors. Without these properties, the
errors from each Halve call could compound without cancellation leading to a significant degrada-
tion in Compress quality. Let us now unpack the most important implications of Thm. 1.
Remark 1 (Near-linear runtime and quadratic speed-ups for Compress) Thm. 1 implies that
a quadratic-time HALvE with rH (n) = n2 yields a near-linear time COMPRESS with rC (n) ≤
4g+1 n(log4 (n) -g). If HALvE instead has super-quadratic runtime rH(n) = nτ, COMPRESS enjoys
a quadratic speed-up:/c (n) ≤ dr nτ/2 for c；，2；：—：). More generally, whenever Halve has
superlinear runtime rH(n) = nτ ρ(n) for some τ ≥ 1 and non-decreasing ρ, COMPRESS satisfies
F cτ∖ c ∖ nτ ∙n (Iognn)- g) p('n)
rC (n) ≤ ∣cT ∙ n；/2 ρ('n)
for τ ≤ 2
for τ > 2
where cτ , 4(τ -1)(g+1) .
Remark 2 (Compress inflates Sub-Gaussian error by at most ,log4 K) Thm. 1 also implies
Vc (n) ≤ √βn + 1 VH ('n) = Vzlog4 n - g VH ('n)
in the usual case that n νH(n) is non-decreasing in n. Hence the sub-Gaussian error of COMPRESS
is at most，1叫 n larger than that of halving an input of size 'n. This is an especially strong
benchmark, as 'n is twice the output size of Compress, and thinning from n to 'n points should
incur at least as much approximation error as halving from 'n to 'n points.
Example 3 (kt-split-Compress) Consider running COMPRES S with, for each HALvE input
of size ', Halve = kt-split(二4：+1：； +1)δ) from Ex. 1. Since kt-split runs in time Θ(n2),
COMPRESS runs in near-linear O(n log n) time by Rem. 1. In addition, as we detail in App. F.1, on
an event of probability 1 - 2, every Halve call invoked by Compress is f -sub-Gaussian with
vh(') = & Jlog( 12n4g'βn+1) )kkk∞ for all f with ||/|限=1.	⑺
Hence, Rem. 2 implies that COMpRESS is f -sub-Gaussian on the same event with VC(n) ≤
,log4 n-g vh('n), a guarantee within ʌ/ɪogɪn of the original KT-SPLiτ(δ) error (2).	■
3.2 MMD guarantees for Compress
Next, we bound the MMD error of Compress in terms of the MMD error of Halve. Recall that
MMDk (1) represents the worst-case integration error across the unit ball of the RKHS of k. Its
proof, based on the concentration of subexponential matrix martingales, is provided in App. C.
Theorem 2 (MMD guarantees for COMPREss) Suppose HALvE ∈ Gk(a, v) for n an and n vn
non-decreasing and E PHALvE k | Sin = Pink. Then COMpRESS ∈ Gk(ae, ve) with
Vn , 4(a'n + v`n)√2(log4 n-g), and en，Vn√log(n+1),	(8)
where 'n = 2g+1√n as in Thm.1.
4
Remark 3 (Symmetrization) We can convert any halving algorithm into one that satisfies the un-
biasedness condition E PHALVE k | Sin = Pink without impacting integration error by symmetriza-
tion, i.e., by returning either the outputted half or its complement with equal probability.
Remark 4 (Compress inflates MMD guarantee by at most 10 log(n + 1)) Thm. 2 implies
that the k-sub-GaUssian error of Compress is always at most 10log(n+1) times that of Halve
with input size 'n = 2g+1√n since
Def. 3	(8)
εk,C0MPREss(n) = max(en,Vn) ≤ 10log(n + 1)max(a'n,v`n) = 10log(n + 1) ∙ £k,HALVE('n).
As in Rem. 2, HALVE applied to an input of size `n is a particularly strong benchmark, as thinning
from n to '2n points should incur at least as much MMD error as halving from 'n to '2n.
Example 4 (KT-Compress) Consider running C0MPREss with, for each HALVE input of size `,
Halve = KT(.4g+i'；+.δ) from Ex. 2 after symmetrizing as in Rem. 3. Since KT has Θ(n2)
runtime, C0MPREss yields near-linear O(nlogn) runtime by Rem. 1. Moreover, as we detail in
App. F.2, using the notation ofEx. 2, on an event E of probability at least 1 - 2, every Halve call
invoked by C0MPREss is k-sub-Gaussian with
a' = 2CCa Pkkk ∞,	and v' = 2CCv V kkk∞ log( 12n4 'βn+1) ) MSin,k
Thus, Rem. 4 implies that, on E, KT-C0MPREss has k-sub-Gaussian error εk,C0MPREss (n) ≤
10log(n+1)εk,HALVE ('n), a guarantee within 10log(n+1) of the original KT(δ) MMD error (4). ■
4 Compress++
To offset any excess error due to Compress while maintaining its near-linear runtime, we next in-
troduce Compress++ (Alg. 2), a simple two-stage meta-procedure for faster root-thinning. Com-
PREss++ takes as input an oversampling parameter g, a halving algorithm HALvE, anda 2g-thinning
algorithm Thin (see Def. 1). In our applications, Halve and Thin are derived from the same base
algorithm (e.g., from KT with different thinning factors), but this is not required. Compress++ first
runs the faster but slightly more erroneous Compress (Halve, g) algorithm to produce an interme-
diate coreset of size 2g√n. Next, the slower but more accurate THIN algorithm is run on the greatly
compressed intermediate coreset to produce a final output of size √n. In the sequel, We demonstrate
how to set g to offset error inflation due to CoMpREss while maintaining its fast runtime.
Algorithm 2: CoMpREss++
Input: oversampling parameter g, halving alg. HALvE, 2g-thinning alg. THIN, point sequence Sin of size n
SC	J	Compress (Halve, g, Sin) // coreset of size 2g√n
Sc++ J	THIN(SC)	// coreset of size √n
return SC++
4.1 Integration error and runtime guarantees for Compress++
The following result, proved in App. D, relates the runtime and single-function integration error of
Compress++ to the runtime and error of Halve and Thin.
Theorem 3 (Runtime and integration error of Compress++) If HALvE and THIN have run-
times rH (n) and rT(n) respectively for inputs of size n, then CoMpREss++ has runtime
rc++ (n) = rc(n) + rr('n∕2) where rc(n) = Pe=O 4i ∙ rH('n2-i),
(9)
'n = 2g+1√n, and βn = log4 n — g — 1 as in Thm. 1. Furthermore, f for some function f,
HALvE ∈ Gf (νH) and THIN ∈ Gf (νT), then CoMpREss++ ∈ Gf(νC++) with
Vc2++(n) = VC(n) + νT('n∕2) where VC(n) = Pe=O 4-i ∙ νH('n2-i).
5
Remark 5 (Near-linear runtime and near-quadratic speed-ups for Compress++) When
HALVE and THIN have quadratic runtimes with max(rH(n), rT(n)) = n2, Thm. 3 and Rem. 1 yield
that rC++ (n) ≤ 4g+1 n(log4(n) - g) + 4gn. Hence, COMPRESS++ maintains a near-linear runtime
rC++ (n) = O(n logc4+1(n)) whenever 4g = O(logc4 n).	(10)
If HALVE and THIN instead have super-quadratic runtimes with max(rH(n), rT(n)) = nτ, then by
Rem. 1 We have rc++ (n) ≤ (2T4-4 + 1) 2gτnτ/2, so that COMPRESS++ provides a near-quadratic
speed UP/c++ (n) = O(nτ/2 log12(n)) whenever 4g = O(logC n).
Remark 6 (COMPRESS++inflates sub-Gaussianerrorbyatmost √2) In the usual case that
n νH(n) is non-decreasing in n, Thm. 3 and Rem. 2 imply that
νC++ (n) ≤ (log4 n - g)νH('n) + νT(令)=νT(旬∙(1 + yg ∙ (ξ⅛⅛)2)
where we have introduced the rescaled quantities Zh('n)，'nVH('n) and ZT('n)，√nνT('n).
Therefore, Compress++ satisfies
VC++ (n) ≤ √2vt('n) whenever g ≥ log4 log4 n + log2(ZZHn/2))∙	(11)
That is, whenever Compress++ is run with an oversampling parameter g satisfying (11) its sub-
Gaussian error is never more than √2 times the second-stage Thin error. Here, Thin represents a
strong baseline for comparison as thinning from 'n∕2 to √n points should incur at least as much
error as thinning from n to √n points.
As we illustrate in the next example, when Thin and Halve are derived from the same thin-
ning algorithm, the ratio
ZH ('n)
ZT('n/2)
is typically bounded by a constant C so that the choice g
dlog4 log4 n + log2 Ce suffices to simultaneously obtain the √2 relative error guarantee (11) of
Rem. 6 and the substantial speed-ups (10) of Rem. 5.
Example 5 (kt-split-Compress++) in the notation of Ex. 1, consider running COMPRESS++
with Halve = kt-split(4：2g(g+2：(e +1))δ) when applied to an input of size ' and Thin =
kt-split( g+2g(β +1)δ). As detailed in App. F.3, on an event of probability 1 - 2, all COM-
PRESS++ invocations of HALVE and THiN are simultaneously f -sub-Gaussian with parameters sat-
isfying
Zh(') = ZT(') = √ Jlog(6.(g+2；(e3)kk∣∣∞ =⇒ 程=1 for all f with |由限=1. (12)
Since KT-SPLIT runs in Θ(n2) time, Rems. 5 and 6 imply that KT-SPLIT-COMPRESS++ with g =
dlog4 log4 n∖ runs in near-linear O(n log2 n) time and inflates sub-Gaussian error by at most √2. ■
4.2 MMD guarantees for Compress++
Next, we bound the MMD error of Compress++ in terms of the MMD error of Halve and Thin.
The proof of the following result can be found in App. E.
Theorem 4 (MMD guarantees for COMPRESS++) If THiN ∈ Gk(a0,v0), HALVE ∈ Gk (a,v) for
n an and n vn non-decreasing, and E PHALVEk | Sin = Pink, then COMPRESS++ ∈ Gk(ab, vb) with
Vn = vn + v'n/2 and an , an + a'八 / 2 + Vn √log 2
for Vn and Gn defined in Thm. 2 and 'n = 2g+1√n as in Thm. 1.
Remark 7 (Compress++ inflates MMD guarantee by at most 4) Thm. 4 implies that the
COMPRESS++ k-sub-Gaussian error εk,COMPRESS++ (n) = max(abn, Vbn) satisfies
εk,Compress++ (n) ≤ (10 log(n + 1) εk,Halve ('n) + εk,THiN('n)) (I + √log 2)
≤ εk,THiN('n)(THe⅛ + 1)(1 + √τog2),
6
where We have introduced the rescaled quantities ZH ('n) ， 'n εk,HALVE ('n) and ZT ('n)，
√nεk,THiN('n). Therefore, Compress++ satisfies
e
εk,COMPREss++ (n) ≤ 4 εk,THIN('n) whenever g ≥ log2 log(n +l) + log2(8.5 eH('n)).	(13)
ζT ( -2 )
In other words, relative to a strong baseline of thinning from 'n to √n points, Compress++ inflates
k-sub-Gaussian error by at most a factor of4 whenever g satisfies (13). For example, when the ratio
`
Zh ('n) / ZT( 'n) is bounded by C, it suffices to choose g =「log? log(n+l)+log2 (8.5C)].
Example 6 (KT-Compress++) In the notation of Ex. 2 and Rem. 3, consider running CoM-
press++ with Halve = symmetrized KT(4n2g(g+2g(β +1))δ) when applied to an input of size
' and Thin = KT(g+2ggβ +1)δ). As we detail in App. F.4, on an event of probability 1 一 δ, all
CoMpREss++ invocations of HALvE and THIN are simultaneously k-sub-Gaussian with
Zh('n) = Zt(崂=Cvq∣∣kk∞ log(3+2；(%+1))) M^k	=⇒	好=L
As KT runs in Θ(n2) time, Rems. 5 and 7 imply that KT-CoMpREss++ with g = dlog2 logn+3.1e
runs in near-linear O(n log3 n) time and inflates k-sub-Gaussian error by at most 4.
5 Experiments
We now turn to an empirical evaluation of the speed-ups and error of Compress++. We begin by
describing the thinning algorithms, compression tasks, evaluation metrics, and kernels used in our
experiments. supplementary experimental details and results can be found in App. G.
Thinning algorithms Each experiment compares a high-accuracy, quadratic time thinning
algorithm—either target kernel thinning (Dwivedi & Mackey, 2022) or kernel herding (Chen et al.,
2010)—with our near-linear time Compress and Compress++ variants that use the same input
algorithm to Halve and Thin. In each case, we perform root thinning, compressing n input points
down to √n points, so that Compress is run with g = 0. For Compress++, we use g = 4 through-
out to satisfy the small relative error criterion (11) in all experiments. When halving we restrict each
input algorithm to return distinct points and symmetrize the output as discussed in Rem. 3.
Compressing i.i.d. summaries To demonstrate the advantages of CoMpREss++ over equal-sized
i.i.d. summaries we compress input point sequences Sin drawn i.i.d. from either (a) Gaussian targets
P = N(0, Id) with d ∈ {2, 4, 10, 100} or (b) M -component mixture of Gaussian targets P =
M pM=ι N(μj, I2) with M ∈ {4,6,8, 32} and component means μj ∈ R2 defined in App. G.
Compressing MCMC summaries To demonstrate the advantages of CoMpREss++ over standard
MCMC thinning, we also compress input point sequences Sin generated by a variety of popular
MCMC algorithms (denoted by RW, ADA-RW, MALA, and pMALA) targeting four challenging
Bayesian posterior distributions P. In particular, we adopt the four posterior targets of Riabiz et al.
(2020a) based on the Goodwin (1965) model of oscillatory enzymatic control (d = 4), the Lotka
(1925); Volterra (1926) model of oscillatory predator-prey evolution (d = 4), the Hinch et al. (2004)
model of calcium signalling in cardiac cells (d = 38), and a tempered Hinch model posterior (d =
38). Notably, for the Hinch experiments, each summary point discarded via an accurate thinning
procedure saves 1000s of downstream CpU hours by avoiding an additional critically expensive
whole-heart simulation (Riabiz et al., 2020a). see App. G for MCMC algorithm and target details.
Kernel settings Throughout we use a Gaussian kernel k(x, y) = exp(-212∣∣x 一 y∣∣2) with σ2 as
specified by Dwivedi & Mackey (2021, sec. K.2) for the MCMC targets and σ2 = 2d otherwise.
Evaluation metrics For each thinning procedure we report mean runtime across 3 runs and mean
MMD error across 10 independent runs ± 1 standard error (the error bars are often too small to be
visible). All runtimes were measured on a single core of an Intel Xeon CpU. For the i.i.d. targets,
we report MMDk(P, Pout) which can be exactly computed in closed-form. For the MCMC targets,
we report the thinning error MMDk(Pin, Pout) analyzed directly by our theory (Thms. 2 and 4).
Kernel thinning results We first apply CoMpREss++ to the near-optimal KT algorithm to obtain
comparable summaries at a fraction of the cost. Figs. 1 and 2 reveal that, in line with our guarantees,
7
d=2
d=4
αww UE8X
d=10
d=100
d=2
8E=urυ 8-6uω
IO3 IO4 10s IO6
Input size n
d=4	d=10	d=100
αnw UES
d=4
2-4
2-6
2-8
8m∏unj PMOO
IO3 IO4 IO5 IO6	IO3 IO4 IO5 IO6	IO3 IO4 IO5 IO6	IO3 IO4 IO5 IO6
Input size n	Input size n	Input size n	Input size n
Figure 1:	For Gaussian targets P in Rd, KT-COMPRESS++ and Herd-COMPRESS++ improve upon the MMD
of i.i.d. sampling (ST), closely track the error of their respective quadratic-time input algorithms KT
and kernel herding (Herd), and substantially reduce the runtime.
KT-Compress++ matches or nearly matches the MMD error of KT in all experiments while also
substantially reducing runtime. For example, KT thins 65000 points in 10 dimensions in 20m,
while KT-COMPRESS++ needs only 1.5m; KT takes more than a day to thin 250000 points in 100
dimensions, while KT-COMPRESS++ takes less than an hour (a 32× speed-up). For reference we
also display the error of standard thinning (ST) to highlight that KT-Compress++ significantly
improves approximation quality relative to the standard practice of i.i.d. summarization or standard
MCMC thinning. See Fig. 4 in App. G.1 for analogous results with mixture of Gaussian targets.
Kernel herding results A strength of COMPRESS++ is that it can be applied to any thinning
algorithm, including those with suboptimal or unknown performance guarantees that often perform
well in practical. In such cases, Rems. 4 and 6 still ensure that Compress++ error is never much
larger than that of the input algorithm. As an illustration, we apply Compress++ to the popular
quadratic-time kernel herding algorithm (Herd). Fig. 1 shows that Herd-Compress++ matches or
nearly matches the MMD error of Herd in all experiments while also substantially reducing runtime.
For example, Herd requires more than 11 hours to compress 250000 points in 100 dimensions,
while Herd-COMPRESS++ takes only 14 minutes (a 45× speed-up). Moreover, surprisingly, Herd-
Compress++ is consistently more accurate than the original kernel herding algorithm for lower
dimensional problems. See Fig. 4 in App. G.1 for comparable results with mixture of Gaussian P.
Visualizing coresets For a 32-component mixture of Gaussians target, Fig. 3 visualizes the core-
sets produced by i.i.d. sampling, KT, kernel herding, and their Compress++ variants. The Com-
press++ coresets closely resemble those of their input algorithms and, compared with i.i.d. sam-
pling, yield visibly improved stratification across the mixture components.
8
Goodwm MALA
49
45	47
Input size n
2-3
2-5
2-7
2-9
Goodwm p MALA
49
45	47
Input size n
Lotka-Vblterra RW
Lotka-Vblterra ADA-RW
LotkajVolterra MALA
2-3
2-5
2-7
2-9
Lotka-Vblterra pMALA
49
47
5
4
αww UES
47
46
5
4
47
5
4
49
47
5
4
Tempered Hmch RW 2
Hinch RW 2
2-4
2-5
2-6
2-7
2-8
2-3
2-
2-5
2→
2-7
Tempered Hmch RW 1
2-3
2-4
2-5
2-6
2-7
■ + 'KT-ConifH-+: n-°∙43
-4-KT： n-0∙4s
二
*ST: ∏-°λ1
.φ KT-Comp： n-0∙45
Qww UEaW αww sωs
44	45	46	47
Input size n
4θ 44
45	46	47 4β 44	45	46	47	48	44	45	46	47	48
Input size n	Input size n	Input size n
Figure 2:	Given MCMC sequences summarizing challenging differential equation posteriors P, KT-
Compress++ consistently improves upon the MMD of standard thinning (ST) and matches or
nearly matches the error of of its quadratic-time input algorithm KT.
i.i.d.	KT-Comp++	KT	Herd-Comp++	Herd
-20 O 20 -20 O 20 -20 O 20 -20 O 20 -20 O 20
Figure 3: Coresets of size 32 (top) or 64 (bottom) with equidensity contours of the target underlaid.
6 Discussion and Conclusions
We introduced a new general meta-procedure, Compress++, for speeding up thinning algorithms
while preserving their error guarantees up to a factor of 4. When combined with the quadratic-time
kt-split and kernel thinning algorithms of Dwivedi & Mackey (2021; 2022), the result is near-
optimal distribution compression in near-linear time. Moreover, the same simple approach can be
combined with any slow thinning algorithm to obtain comparable summaries in a fraction of the
time. Two open questions recommend themselves for future investigation. First, why does Herd-
Compress++ improve upon the original kernel herding algorithm in lower dimensions, and can this
improvement be extended to higher dimensions and to other algorithms? Second, is it possible to thin
significantly faster than Compress++ without significantly sacrificing approximation error? Lower
bounds tracing out the computational-statistical trade-offs in distribution compression would provide
a precise benchmark for optimality and point to any remaining opportunities for improvement.
9
Reproducibility Statement
See the goodpoints Python package for Python implementations of all methods in this paper and
https://github.com/microsoft/goodpoints
for code reproducing each experiment.
Acknowledgments
We thank Carles Domingo-Enrich for alerting us that an outdated proof of Thm. 2 was previously
included in the appendix. RD acknowledges the support by the National Science Foundation under
Grant No. DMS-2023528 for the Foundations of Data Science Institute (FODSI). Part of this work
was done when AS was interning at Microsoft Research New England.
References
Christoph M Augustin, Aurel Neic, Manfred Liebmann, Anton J Prassl, Steven A Niederer, Gundolf Haase,
and Gernot Plank. Anatomically accurate high resolution modeling of human whole heart electromechanics:
A strongly scalable algebraic multigrid solver method for nonlinear deformation. Journal of computational
physics, 305:622-646, 2016. (Cited on page 1.)
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Springer Science & Business Media, 2011. (Cited on page 2.)
S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Indepen-
dence. OUP Oxford, 2013. ISBN 9780199535255. URL https://books.google.com/books?
id=koNqWRluhP0C. (Cited on pages 2, 12, 15, and 19.)
Bernard Chazelle and Jiri Matousek. On linear-time deterministic algorithms for optimization problems in fixed
dimension. Journal of Algorithms, 21(3):579-597, 1996. (Cited on page 2.)
Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. In Proceedings of the Twenty-
Sixth Conference on Uncertainty in Artificial Intelligence, UAI’10, pp. 109-116, Arlington, Virginia, USA,
2010. AUAI Press. ISBN 9780974903965. (Cited on pages 2 and 7.)
Raaz Dwivedi and Lester Mackey. Kernel thinning. arXiv preprint arXiv:2105.05842, 2021. (Cited on pages 1,
2, 3, 7, 9, 23, and 24.)
Raaz Dwivedi and Lester Mackey. Generalized kernel thinning. In International Conference on Learning
Representations, 2022. (Cited on pages 3, 7, 9, 20, 21, and 22.)
Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123-214, 2011. (Cited on
page 23.)
Brian C Goodwin. Oscillatory behavior in enzymatic control process. Advances in Enzyme Regulation, 3:
318-356, 1965. (Cited on page 7.)
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research, 13(25):723-773, 2012. (Cited on page 2.)
Heikki Haario, Eero Saksman, and Johanna Tamminen. Adaptive proposal distribution for random walk
Metropolis algorithm. Computational Statistics, 14(3):375-395, 1999. (Cited on page 23.)
Robert Hinch, JL Greenstein, AJ Tanskanen, L Xu, and RL Winslow. A simplified local control model of
calcium-induced calcium release in cardiac ventricular myocytes. Biophysical journal, 87(6):3723-3736,
2004. (Cited on page 7.)
Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism
for interpretability. Advances in neural information processing systems, 29, 2016. (Cited on page 2.)
Alfred James Lotka. Elements of physical biology. Williams & Wilkins, 1925. (Cited on page 7.)
Jiri Matousek. Approximations and optimal geometric divide-and-conquer. Journal of Computer and System
Sciences, 50(2):203-208, 1995. (Cited on page 2.)
10
Steven A Niederer, Lawrence Mitchell, Nicolas Smith, and Gernot Plank. Simulating human cardiac electro-
physiology on clinical time-scales. Frontiers in Physiology, 2:14, 2011. (Cited on page 1.)
Art B Owen. Statistically efficient thinning of a Markov chain sampler. Journal of Computational and Graph-
ical Statistics, 26(3):738-744, 2017. (Cited on page 1.)
Jeff M Phillips. Algorithms for ε-approximations of terrains. In International Colloquium on Automata, Lan-
guages, and Programming, pp. 447-458. Springer, 2008. (Cited on page 2.)
Jeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete & Computational
Geometry, 63(4):867-887, 2020. (Cited on page 2.)
Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A Niederer, Lester Mackey, and Chris
Oates. Optimal thinning of MCMC output. arXiv preprint arXiv:2005.03952, 2020a. (Cited on pages 2, 7,
and 23.)
Marina Riabiz, Wilson Ye Chen, Jon Cockayne, Pawel Swietach, Steven A. Niederer, Lester Mackey, and
Chris J. Oates. Replication Data for: Optimal Thinning of MCMC Output, 2020b. URL https://doi.
org/10.7910/DVN/MDKNWM. Accessed on Mar 23, 2021. (Cited on page 23.)
Christian P Robert and George Casella. Monte Carlo integration. In Monte Carlo statistical methods, pp.
71-138. Springer, 1999. (Cited on page 1.)
Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their discrete
approximations. Bernoulli, 2(4):341-363, 1996. (Cited on page 23.)
Marina Strocchi, Matthias AF Gsell, Christoph M Augustin, Orod Razeghi, Caroline H Roney, Anton J Prassl,
Edward J Vigmond, Jonathan M Behar, Justin S Gould, Christopher A Rinaldi, Martin J Bishop, Gernot
Plank, and Steven A Niederer. Simulating ventricular systolic motion in a four-chamber heart model with
spatially varying robin boundary conditions to model the effect of the pericardium. Journal of Biomechanics,
101:109645, 2020. (Cited on page 1.)
Ilya Tolstikhin, Bharath K Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel mean em-
beddings. The Journal of Machine Learning Research, 18(1):3002-3048, 2017. (Cited on page 2.)
Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathe-
matics, 12(4):389-434, 2012. doi: 10.1007/s10208-011-9099-z. URL https://doi.org/10.1007/
s10208- 011-9099-z. (Cited on pages 15 and 18.)
Vito Volterra. Variazioni e fluttuazioni del numero d’individui in specie animali conviventi. 1926. (Cited on
page 7.)
11
Appendix
A Additional Definitions and Notation	12
B	Proof of Thm. 1:	Runtime and integration error of Compress	13
C	Proof of Thm. 2:	MMD guarantees for Compress	13
D	Proof of Thm. 3:	Runtime and integration error of Compress++	19
E	Proof of Thm. 4:	MMD guarantees for Compress++	19
F Proofs of Exs. 3, 4, 5, and 6	20
G Supplementary Details for Experiments	22
H Streaming Version of Compress	23
A Additional Definitions and Notation
This section provides additional definitions and notation used throughout the appendices.
We associate with each algorithm ALG and input Sin the measure difference
ΦALG (Sin)，PSin - PSALG = 1 Px∈Sin δχ - nθut Px∈Salg δχ	(14)
that characterizes how well the output empirical distribution approximates the input. We will often
write φALG instead of φALG (Sin) for brevity if Sin is clear from the context.
We also make use of the following standard definition of a sub-Gaussian random variable (see, e.g.,
Boucheron et al., 2013, Sec. 2.3).
Definition 4 (Sub-Gaussian random variable) We say that a random variable G is sub-Gaussian
with parameter ν and write G ∈ G(ν) if
E[exp(λ G)] ≤ exp (λ2ν2) for all λ ∈ R.
Given Def. 4, it follows that ALG ∈ Gf (ν) for a function f as in Def. 2 if and only if the random
variable φALG(f) , PSin f - PSALG f is sub-Gaussian with parameter ν conditional on the input Sin.
In our proofs, it is often more convenient to work with an unnormalized measure discrepancy
ψALG(Sin) , n ∙ φALG(Sin)J) Pχ∈Sm δx - 就 PSALG 限	(15)
By definition (15), we have the following useful equivalence:
Ψalg(f)，n ∙ ΦaLG(f) ∈	G (σALG)	^⇒	φALG (f)	∈	G(VALG )	for	σALG = n	∙ νALG ∙	(16)
The following standard lemma establishes that the sub-Gaussian property is closed under scaling
and summation.
Lemma 1 (Summation and scaling preserve sub-Gaussianity) Suppose G1 ∈ G(σ1). Then, for
all β ∈ R, we have β ∙ Gi ∈ G(βσι). Furthermore, if Gi is F-measurable and G2 ∈ G(σ2) given
F, then Gi + G2 ∈ G(pσ2 + σ2).
12
Proof Fix any β ∈ R. Since G1 ∈ G(σ1), for each λ ∈ R,
E[exp(λ ∙ β ∙ Gι)] ≤ exp(λ *)),
so that βG1 ∈ G(βσ1) as advertised.
Furthermore, if G1 is F -measurable and G2 ∈ G(σ2) given F, then, for each λ ∈ R,
EheXP(λ∙ (G1 + G2))] = E[exp(λ∙ G1 + λ ∙ G2)] = EheXP(λ ∙ G1) ∙ E[exp(λ ∙ G2) | F1
≤ exp(λ~σ2) ∙ EheXP(λ ∙ f(G2))]
= eχp(苧)∙eχp(λ⅛2) =eχp("(σ2+σ2)
so that Gi + G2 ∈ G( 'σ2 + σ2) as claimed.

B	Proof of Thm. 1: Runtime and integration error of Compress
First, we bound the running time of Compress. By definition, Compres s makes four recursive
calls to Compress on inputs of size n/4. Then, Halve is run on an input of size 2g+1√n. Thus,
rC satisfies the recursion
rɛ (n) = 4/c( 4) + rH (√n2g+1).
since rC(4g) = 0, we may unroll the recursion to find that
rc (n) = Pβ=o 4ir≡ (2g+1√n4-i),
as claimed in (5).
Next, we bound the sub-Gaussian error for a fixed function f. In the measure discrepancy (15)
notation of App. A we have
ψc (Sin) = P4=1 Ψc(Si) + √n2-g-1ψH (S)	(17)
where Si and S are defined as in Alg. 1. Unrolling this recursion, we find that running cOMPREss
on an input of size n with oversampling parameter g leads to applying Halve on 4i coresets of size
n = 2g+1-i√n for 0 ≤ i ≤ βn. Denoting these HALVE inputs by (Sij) j∈[4i], We have
Ψc(Sin) = √n2-g-1 PenO P4=1 2-iΨH(Sinj).	(18)
Now define σH (n) = nνH(n). since ψH (Sii,nj )(f) are σH(ni ) sub-Gaussian given (Sii0n,j0 )i0>i,j0≥1
and (Sii,nj0)j0≤j, Lem. 1 implies that ψc (Sin)(f) is σc sub-Gaussian given Sin for
σc2 (n) = n4-g-1 Piβ=n0 σH2 (ni).
Recalling the relation (16) between σand ν from App. A, we conclude that
νc2(n) = Piβ=n0 4-iνH2 (ni).
as claimed in (6).
c Proof of Thm. 2: MMD guarantees for Compress
Our proof proceeds in several steps. To control the MMD (1), we will control the Hilbert norm of
the measure discrepancy of Compress (15), which we first write as a weighted sum of measure
discrepancies from different (conditionally independent) runs of Halve. To effectively leverage
the MMD tail bound assumption for this weighted sum, we reduce the problem to establishing a
13
concentration inequality for the operator norm of an associated matrix. We carry out this plan in
four steps summarized below.
First, in App. C.1 we express the MMD associated with each Halve measure discrepancy as the
Euclidean norm of a suitable vector (Lem. 2). Second, in App. C.2 we define a matrix dilation
operator for a vector that allows us to control vector norms using matrix spectral norms (Lem. 3).
Third, in App. C.3 we prove and apply a sub-Gaussian matrix Freedman concentration inequality
(Lem. 4) to control the MMD error for the Compress output, which in turn requires us to establish
moment bounds for these matrices by leveraging tail bounds for the MMD error (Lem. 5). Finally,
we put together the pieces in App. C.4 to complete the proof.
We now begin our formal argument. We will make use of the unrolled representation (17) for
the COMPRESS measure discrepancy ψC (Sin) in terms of the HALVE inputs (Skin,j)j∈[4k] of size
nk = 2g+1-k√n for 0 ≤ k ≤ log4 n — g-1. For brevity, we will use the shorthand ψc，ψc(Sin),
ψkH,j , ψH (Skin,j), and ψT , ψT(SC) hereafter.
C.1 Reducing MMD to vector Euclidean norm
Number the elements of Sin as (x1, . . . , xn), define the n × n kernel matrix K , (k(xi, xj))in,j=1,
and let K 1 denote a matrix square-root such that K = K 2 ∙ K 2 (which exists since K is a positive
semidefinite matrix for any kernel k). Next, let Sko,ujt denote the output sequence corresponding
to	ψkH,j	(i.e., running	HALVE on	Skin,j),	and let	{ei }in=1	denote the canonical basis of	Rn .	The
next lemma (with proof in App. C.5) relates the Hilbert norms to Euclidean norms of carefully
constructed vectors.
Lemma 2 (MMD as a vector norm) Define the vectors
Ukj , K 2 Pn=1 ei(1(xi ∈Sin,j )—2 ∙1(xi ∈Skj), and UC , Pk=O n-gT Pj= 1 wk,j uk,j, (19)
where Wkj，2g+n+k. Then, we have
n2 ∙ MMDk(Sin, SC) = ∣∣uc∣∣2, and	(20)
E[Uk,j |(Uk0,j0 : j0 ∈ [4k0], k0 > k)] = 0 for k = 0, . . . , log4 n—g —2,	(21)
and Uk,j forj ∈ [4k] are conditionally independent given (Uk0,j0 : j0 ∈ [4k0], k0 > k).
Applying (20), we effectively reduce the task of controlling the MMD errors to controlling the
Euclidean norm of suitably defined vectors. Next, we reduce the problem to controlling the spectral
norm of a suitable matrix.
C.2 Reducing vector Euclidean norm to matrix spectral norm
To this end, we define a symmetric dilation matrix operator: given a vector U ∈ Rn , define the
matrix Mu as
Mu, 0 U>	∈ R(n+1)×(n+1).	(22)
U 0n×n
It is straightforward to see that U 7→ Mu is a linear map. In addition, the matrix Mu also satisfies a
few important properties (established in App. C.6) that we use in our proofs.
Lemma 3 (Properties of the dilation operator) For any U ∈ Rn, the matrix Mu (22) satisfies
∣Mu∣op (=a) ∣U∣2 (=b) λmax(Mu), and Mqu (c) ∣U∣q2In+1 for all q∈N.	(23)
Define the shorthand Mk,j , Mwk,juk,j (defined in Lem. 2). Applying Lems. 2 and 3, we find that
nMMDk(Sin,SC)(=)∣UC∣2(=)λmax(MuC)(=i)λmax(Plko=g40n-g-1 Pj4=k 1Mk,j),	(24)
where step (i) follows from the linearity of the dilation operator. Thus to control the MMD error, it
suffices to control the maximum eigenvalue of the sum of matrices appearing in (24).
14
C.3 Controlling the spectral norm via a sub-Gaussian matrix Freedman
INEQUALITY
To control the maximum eigenvalue of the matrix MuC, we make use of (24) and the following sub-
Gaussian generalization of the matrix Freedman inequality of Tropp (2012, Thm. 7.1). The proof of
Lem. 4 can be found in App. C.7. For two matrices A and B of the same size, we write A B if
B - A is positive semidefinite.
Lemma 4 (Sub-Gaussian matrix Freedman inequality) Consider a sequence (Yi)iN=1 of self-
adjoint random matrices in Rm×m and a fixed sequence of scalars (Ri)iN=1 satisfying
E∣Yi∣(Yj )j=1] (=) 0 and EhYq |(Yj )j=1] (B) (2 )R L for all i ∈ N] and q ∈ 2N.	(25)
Define the variance parameter σ2 , PiN=1 Ri2. Then,
P[λmaχ(pN=1 Yi) ≥ σp8(t + log m)] ≤ e-t for all t > 0,
and equivalently
P[λmaχ(pN=1 Yi) ≤ σp8 log(m∕δ)] ≥ 1 - δ for all δ ∈ (0,1].
To apply Lem. 4 with the matrices Mk,j , we need to establish the zero-mean and moment bound
conditions for suitable Rk,j in (25).
C.3.1 VERIFYING THE ZERO MEAN CONDITION (25)(A) FOR Mk,j
To this end, first we note that the conditional independence and zero-mean property of ψkH,j implies
that the random vectors uk,j and the matrices Mk,j also satisfy a similar property, and in particular
that
E	Mk,j	|	Mk0,j0 : k0 > k,j0	∈	[4k0]	=0 for j ∈	[4k],k ∈	{0,1,...,log4n-g-1}.
(26)
C.3.2 ESTABLISHING MOMENT BOUND CONDITIONS (25)(B) FOR Mk,j IN TERMS OF Rk,j
via MMD tail bounds for Halve
To establish the moment bounds on Mk,j, note that Lems. 2 and 3 imply that
Mkj= Mwk,jUk,j Mllwk,j uk,jll2 ∙In+1 (=) wq,j∖∖uk,j∖∖2 ∙In+1	(27)
where wk,j was defined in Lem. 2. Thus it suffices to establish the moment bounds on uk,j q2. To
this end, we first state a lemma that converts tail bounds to moment bounds. See App. C.8 for the
proof inspired by Boucheron et al. (2013, Thm. 2.3).
Lemma 5 (Tail bounds imply moment bounds) For a non-negative random variable Z,
P[Z>a+v√t] ≤ e-t for all t ≥ 0 =⇒ E[Zq ] ≤ (2a+2v)q (2)! for all q ∈ 2N.
To obtain a moment bound for uk,j 2, we first state some notation. For each n, define the quantities
an , nan ,	vn , nvn	(28)
where an and vn are the parameters such that HALVE ∈ Gk(an, vn) on inputs of size n. Using an
argument similar to Lem. 2, we have
∖∖uk,j∖∖2 = nk,j MMDk(Sknj, Skj for nk,j = Sj | = √n2g+1-k.
Thereby, using the Gk assumption on HALVE implies that
P[∖∖uk,j∖∖2 ≥ a'o + v0'k √t | (ukj : j0 ∈ [4k0 ], k0 > k)] ≤ e-t for all t ≥ 0,	(29)
15
where
'k，nk,j = √n2g+1-k	(30)
and, notably, `n = `00 . Combining the bound (29) with Lem. 5 yields that
E[∣∣uk,j∣∣2 | (uko,j0 ： j0 ∈ [4k0],k0 > k)] ≤ (2)!(2a'o + 2v', )q,	(31)
for all q ∈ 2N, where `k is defined in (29). Now, putting together (27) and (31), and using the
conditional independence of Mk,j , we obtain the following control on the q-th moments of Mk,j
for q ∈ 2N:
E Mk,j∣(Mk,,j0,k0 > k,j0 ∈ [4k0])]曾 wq,j∙E ∣∣uk,j∣∣2∣{uk0,j0,k0 > k,j0 ∈ [4k0]}] "1
号 wq,j《2a'k +2v'k)q(q)!"n+ι
=(2)!Rk,jIn+ι where Rk,j，2wk,j(a. + VG	(32)
where `k is defined in (30). In summary, the computation above establishes the condition (B) from
the display (25) for the matrices Mk,j in terms of the sequence Rk,j defined in (32).
C.4 Putting the pieces together for proving Thm. 2
Define
e , √log4 n - g ∙ 2(a√n2g+ι + v√n2g + 1 )	(33)
Now, putting (26) and (32) together, we conclude that with a suitable ordering of
the indices (k, j), the assumptions of Lem. 4 are satisfied by the random matrices
(Mk,j,j ∈ [4k],k ∈ {0, l,...,l0g4n 一 g _ 1}) With the sequence (Rkj). Now, since 'k =
√n2g+1-k (29) is decreasing in k, wk,j = 4+ (as defined in Lem. 2), and a* and Vn (28) are
assumed non-decreasing in n, we find that
n2 ∙ e2 (=) n2(log4 n - g)(2(a√n2g+ι + v√n2g + ι ))2
(=)(log4 n 一 g) 4gn+r(2(a'0 + v'0 ))2
≥ Pk=On-gT4⅛τ(2(a'k + v'k))2
=Pk=O n-g-1 P4= 1 4⅛ (2(a'k + v' k ))2
=Pk=O n-g-1 P4= ι(2wk,j (a'k + v'k))2
(32) Plog4 n-g-1 P4k	2
=	k=O	j=1 Rk,j .
Finally, applying (24) and invoking Lem. 4 with σ J nσ and m J n + 1, we conclude that
P[MMD(Sin, SC) ≥ ep8(log(n +1)+ V)]
(=) P[λmax(Pk=0n±1 P4= 1 Mk,j) ≥ neP8(log(n +1)+ V)]
≤ e-t for all V > 0,
which in turn implies
P[MMD(Sin, SC) ≥ en + en√t] ≤ e-t for V ≥ 0,
since the parameters ven , aen (8) satisfy
en = 4(a'n + v`n)P2(log4 n-g) (=) e√8, and en = enPlog(n+1) = e,8log(n + 1).
Comparing with Def. 3, Thm. 2 follows.
16
C.5 Proof of Lem. 2: MMD as a vector norm
Let vk,j，Pn=ι ei (I(Xi ∈ Sklnj)-2 ∙1(Xi ∈ Sout))∙ By the reproducing property of k we have
kΨH,j(k)kk =∣∣Pχ∈sknj k(x,∙)-2Pχ∈soujt k(x,∙)[
= Px∈Skin,j,y∈Skin,j k(X, y) - 2 Px∈Sko,ujt,y∈Skin,j k(X, y) + Px∈Sko,ujt,y∈Sko,ujt k(X, y)
= vk>,jKvk,j (=) uk,j22.	(34)
Using (18), (19), and (22), and mimicking the derivation above (34), we can also conclude that
kψC(k)k2k = kuCk22.
Additionally, we note that
MMDk(Sin, SC)=SUpkf kk=ι ɪ<f,ψc(k)>Hk = 1 kΨc(k)kk.
Finally the conditional independence and zero mean property (21) follows from (18) by noting that
conditioned on (Skin0,j0)k0>k,j0≥1, the sets (Skin,j)j≥1 are independent.
C.6 Proof of Lem. 3: Properties of the dilation operator
For claim (a) in the display (23), we have
M2u =	k0uk22 u0un>> ! (i) kuk22In+1	=⇒	kMukop (=ii) kuk2,
where step (i) follows from the standard fact that uu> kuk22In and step (ii) from the facts M2uee1 =
kuk22ee1 for ee1 the first canonical basis vector of Rn+1 and kMuk2op = M2uop. Claim (b) follows
directly by verifying that the vector V = [1, ^U>p]> is an eigenvector of MU with eigenvalue ∣∣u∣∣2∙
Finally, claim (c) follows directly from the claim (a) and the fact that kMqu kop = kMu kqop for all
integers q ≥ 1.
C.7 Proof of Lem. 4: Sub-Gaussian matrix Freedman inequality
We first note the following two lemmas about the tail bounds and symmetrized moment generating
functions (MGFs) for matrix valued random variables (see Apps. C.9 and C.10 respectively for the
proofs of Lems. 6 and 7).
Lemma 6 (Sub-Gaussian matrix tail bounds) Let Xk ∈ Rm×m k≥1 be a sequence of self-
adjoint matrices adapted to a filtration Fk, and let Ak ∈ Rm×m k≥1 be a sequence of determin-
istic self-adjoint matrices. Define the variance parameter σ2 , Pk Ak op. If, for a Rademacher
random variable ε independent of (Xk, Fk)k≥1, we have
logE[exp(2εθXk)∣Fk-ι] W 2θ2Ak forall θ ∈ R,	(35)
then we also have
p[λmaχ(pk Xk) ≥ t)i ≤ me--2) forall t ≥ 0.
Lemma 7 (Symmetrized sub-Gaussian matrix MGF) For a fixed scalar R, let X be a self-
adjoint matrix satisfying
EX = 0 and EXq W (2)!RqI for q ∈ 2N.	(36)
If ε is a Rademacher random variable independent of X, then
Eexp(2εθX) W exp(2θ2R2l) forall θ ∈ R.
17
The assumed conditions (25) allow us to apply Lem. 7 conditional on (Yi)i<k along with the oper-
ator monotonicity of log to find that
logE[exp(εθYk)∣{Yi}i<k] W 2θ2R2I for all θ ∈ R,
for a Rademacher random variable ε independent of (Yk)k≥1. Moreover,	Pk Ak op =
PkRk2Iop = Pk Rk2 = σ2. Thus, applying Lem. 6,we find that
P[λmaχ(PiYi) ≥ t] ≤ me-t2∕(8σ2) for all t ≥ 0.
As an immediate consequence, we also find that
P[λmaχ(PiYi) ≥ √8σ2(t + log m)] ≤ e-t for all t ≥ 0,
as claimed.
C.8 Proof of Lem. 5: Tail bounds imply moment bounds
We begin by bounding the moments of the shifted random variable X = Z - a. Note that Z ≥ 0, so
that X ≥ -a. Next, note that X = X+ -X- where X± = max(±X, 0) and that |X|q = X+q +X-q .
Furthermore, X-q ≤ aq by the nonnegativity of Z, so that |X |q ≤ aq + X+q . For any u > 0, since
P[X+ > u] = P[X > u] = P[Z > a + u] for any u > 0, we apply the tail bounds on Z to control
the moments of X+ . In particular, we have
EX+q ] (=i) q R0∞ uq-1P[X+ > u]du
(=) qR∞(v√t)qTpX+ > v√t] ∙有dt
≤ qvq R0∞ tq/2-1 e-tdt (=) qvqΓ(2),
where We have applied (i) integration by parts, (ii) the substitution U = v√t, and (iii) the assumed
tail bound for Z .
Since Z = X + a, the convexity of the function t 7→ tq for q ≥ 1, and Jensen’s inequality imply
that for each q ∈ 2N, we have
EZq ≤ 2q-1(aq + E|X∣q) ≤ 2q-1(2aq + EX+ ) ≤ (2a)q + 2q-1qvqΓ(2)
=(2a)q + 2q-1qvq(2 - 1)!
≤ (2a + 2v)q(2)!
where the last step follows since xq + yq ≤ (x + y)q for all q ∈ N and x, y ≥ 0. The proof is now
complete.
C.9 Proof of Lem. 6: Sub-Gaussian matrix tail bounds
The proof of this result is identical to that of Tropp (2012, Proof of Thm. 7.1) as the same steps are
justified under our weaker assumption (35). Specifically, applying the arguments from Tropp (2012,
Proof of Thm. 7.1), we find that
E[trexp(Pn=1 θXQ] ≤ E trexp(Pn-I θXk + log EIeXp(2εθXn)∣Fn-j)]
(≤) E tr expPkn=-11θXk + 2θ2An
(i)	(ii)
≤ trexp(2θ2 P2=ι Ak) ≤ mexp(2θ2σ2),	(37)
where step (i) follows by iterating the arguments over k = n-1, . . . , 1 and step (ii) from the standard
fact that tr(exp(A)) ≤ mexp(A)op = mexp(kAkop) for an m × m self-adjoint matrix A. Next,
applying the matrix Laplace transform method Tropp (2012, Prop. 3.1), for all t > 0, we have
P[λmax(Pk Xk) ≥ t)i ≤ infθ>o{e-θt ∙ E[trexp(Pn=ι θXk)]}
(≤ minfθ>o{e-θt ∙ e2θ2σ2 } = me-t2∕(8σ2),
where the last step follows from the choice θ =9.The proof is now complete.
18
C.10 Proof of Lem. 7: Symmetrized sub-Gaussian matrix MGF
We have
E[exp(2εθX)] = I + P∞=ι 爷 E[εq Xq ] = I + P∞ι 卷等 E[X2k ]
(ii)	P∞ 22kθ2k k!R2kτ
-I + 乙k = 1	(2k)!	I
驾 I + P∞=1 (2θ2RτI = exp(2θ2R2I),
where step (i) uses the facts that (a) E[εq] = 1(q ∈ 2N) and (b) E[εqXq] = E[εq]E[Xq] since ε is
independent of X, step (ii) follows from the assumed condition (36), and step (iii) from the fact that
(2k)y ≤ 十(BoUcheron et al., 2013, Proof of Thm. 2.1).
D Proof of Thm. 3: Runtime and integration error of Compress++
First, the rUntime boUnd (9) follows directly by adding the rUntime of COMPRESS(HALVE, g) as
given by (5) in Thm. 1 and the rUntime of Thin.
Recalling the notation (14) and (15) from App. A and noting the definition of the point seqUences SC
and SC++ in Alg. 2, we obtain the following relationship between the different discrepancy vectors:
φC (Sin) = 1 Σχ∈Sin δx - 2g√n Σχ∈Sc δx,
φT(SC ) = 2g√n Px∈SC δx - √ Px∈Sc++ δx, and
φC++ (Sin) = W ∑χ∈Sin δx - √ Σx∈Sc++ δx
= φC (Sin) + φT (SC).
Noting the Gf property of HALVE and applying Thm. 1, we find that φC(Sin)(f) is sUb-GaUssian
with parameter νC (n) defined in (6). FUrthermore, by assUmption on THIN, given SC, the variable
Φt (SC)(f) is VC ('n) SUb-GaUssian. The claim now follows directly from Lem. 1.
E	Proof of Thm. 4: MMD guarantees for Compress++
Noting that MMD is a metric, and applying triangle ineqUality, we have
MMDk(Sin, SC++) ≤ MMDk(Sin, SC ) + MMDk (SC, SC++).
Since Sc++ is the output of THIN(2g) with SC as the input, applying the MMD tail bound assump-
tion (38) with |Sc | = √n2g substituted in place of n, we find that
PhMMD(Sc, Sc++ ) ≥ a∕√n+v∕√n√ti ≤ e-t for all t ≥ 0.
Recall that 'n∕2 = 2g√n. Next, we apply Thm. 2 with HALVE to conclude that
P[MMDk(Sin, SC) ≥ en + en ∙ √t] ≤ e-t for all t ≥ 0.
Thus, we have
PhMMDk(Sin, Sc++) ≥ a'n∕2 + e + (v'n/2 + e^司 ≤ 2 ∙ e-t for all t ≥ 0,
which in turn implies that
PhMMDk(Sin, Sc++ ) ≥ a'n∕2 + %n + (v'n∕2 + en)√log2+ (v'n/2 + ejVt] ≤ e-t for all t ≥ 0,
thereby yielding the claimed result.
19
F Proofs of Exs. 3, 4, 5, and 6
We begin by defining the notions of sub-Gaussianity and k-sub-Gaussianity on an event.
Definition 5 (Sub-Gaussian on an event) We say that a random variable G is sub-Gaussian on an
event E with parameter σ if
E[1[E] ∙ exp(λ ∙ G)] ≤ exp( λ22σ2) for all λ ∈ R.
Definition 6 (k-sub-Gaussian on an event) For a kernel k, we call a thinning algorithm ALG k-
sub-Gaussian on an event E with parameter v and shift a if
P[E, MMDk(Sin, SALG) ≥ an + Vn√t | Sin] ≤ e-t forall t ≥ 0.	(38)
We will also make regular use of the unrolled representation (17) for the Compress measure dis-
crepancy ψC(Sin) in terms of the HALVE inputs (Skin,j)j∈[4k] of size
nk = 2g+1-k√n for 0 ≤ k ≤ βn.	(39)
For brevity, we will use the shorthand ψC , ψC(Sin), ψkH,j , ψH(Skin,j), and ψT , ψT(SC) hereafter.
F.1 Proof of Ex. 3: kt-split-Compress
For Halve = KT-SPLit(.4g+(. +i)δ) When applied to an input of size ', the proof of Thm. 1 in
DWivedi & Mackey (2022) identifies a sequence of events Ek,j and random signed measures ψk,j
such that, for each 0 ≤ k ≤ βn, j ∈ [4k], and f With kfkk = 1,
(a)	P[Ekc,j ]
≤ n4g + 1(βn + 1) 2 = 2 4k (βn + 1),
(b)	1[Ek,j ]ψH,j = 1[Ek,j]ψk,j,and
(c)	ψk,j (f) is nk νH(nk) sub-Gaussian (7) given (ψk0,j0)k0>k,j0≥1 and (ψk,j0)j0<j,
where step (ii) follows from substituting the definition n = 2g+1-k√n (39). To establish step (ii)
2
in property (a), we use the definition1 of KT-SPLit(n4g+τ(β +)δ) for an input of size n, which
implies that δi = n4g+nβ +1)δ in the notation of Dwivedi & Mackey (2022). The proof of Thm. 1
in Dwivedi & Mackey (2022) then implies that
P[Ekc,j] ≤ Pin=k1/2 δi
2
nk	nk X _	nk	δ
2 n4g + 1(βn + 1)° = n4g + 1(βn+1) 2 .
Hence, on the event E = k,j Ek,j , these properties hold simultaneously for all HALVE calls made
by Compress, and, by the union bound,
P[Ec] ≤ Pβ=0 p4= ι P[EC,j] ≤ Pβ=0 4kI4⅛π = 2.	(40)
Now fix any f with kfkk = 1. We invoke the measure discrepancy representation (17), the equiva-
lence of ψH,j and ψk,j on E, the nonnegativity of the exponential, and Lem. 1 in turn to find
E[1[E] ∙ exp(λ ∙ Φc(f))] = E[1[E] ∙ exp(λ ∙ nΨc(f))]
=E[1[E] ∙ exp(λ ∙ 1 √n2-g-1 £2=° P；= 1 2-kψ3(f))]
=E[1[E] ∙ exp(λ ∙ 1 √n2-g-1 Pβ=o P；= 1 2-kψk,j (f))]
≤ E[exp(λ ∙ 1 √n2-g-1 Pβ=o P;= 1 2-kψk,j (f))]
≤ exp(λ2ν22(n)) for νC S) = Pe=o 4-kνH (nk)
so that φc(f) is νc sub-Gaussian on E.
20
F.2 Proof of Ex. 4: KT-Compress
For Halve = symmetrized KT(.4g+i'；+[)δ) when applied to an input of size ', the proofs of
Thms. 1-4 in DWivedi & Mackey (2022) identify a sequence of events Ek,j and random signed
measures ψk,j such that, for each 0 ≤ k ≤ βn and j ∈ [4k],
(a)	P[Ekc,j ]
≤ 24k(⅛+I),
(b)	1[Ek,j]ψHj = 1[Ek,j]Ψk,j,
1
(c)	P[ n1k kψk,j (k)kk ≥ a，nk + Vnk vt | (Ψk0,jo)k0>k,j0≥ι, (ψk,j0 )j0<j ] ≤ e t for all t ≥ 0,and
(d)	E[ψk,j (k) | (ψk0,j0 )k0>k,j0≥1 , (ψk,j0)j0<j] = 0,
where n = 2g+1-k√n was defined in (39). We derive property (a) exactly as in App. F.1.
Hence, on the event E = Tk,j Ek,j , these properties hold simultaneously for all HALVE calls made
by Compress, and, by the union bound (40), P[Ec] ≤ 2.
Furthermore, we may invoke the measure discrepancy representation (17), the equivalence of ψkH,j
and ψk,j on E, the nonnegativity of the exponential, and the proof of Thm. 2 in turn to find
P[E, MMD(Sin,SC) ≥ a. + V.√ | Sm] = P[E, nkψo(k)kk ≥ 5n + VnW | Sin]
=P[E, nk√n2-g-1 Pe=0p4= 1 2-kψk,j(k)kk ≥ a. + Vn√t | Sin]
≤ P[ 1 k√n2-g-1 Pβ=0 p4= 1 2-kψk,j (k)kk ≥ Sn + Vn√t |Sin] ≤ e-t for all t ≥ 0,
so that COMPREss is k-sub-Gaussian on E with parameters (Va, aa).
F.3 Proof of Ex. 5: kt-split-Compress++
For Thin = KT-SPLit( g+2g(gβn+1)δ) and Halve = KT-SPLit(4n2g(g+2g(en+1))δ) when applied
to an input of size `, the proof of Thm. 1 in Dwivedi & Mackey (2022) identifies a sequence of events
Ek,j and ET and random signed measures ψak,j and ψaT such that, for each 0 ≤ k ≤ βn, j ∈ [4k], and
f with kfkk = 1,
(i)
(a) P[Ekc,j ] ≤
_______nk_______δ (Ii)
4n2g(g+2g(βn + 1)) 2 =
2g	δ
4k(g+2g (βn + 1))2
(iii)
and	P[ET] ≤ g + 2g(βn+1) 2,
(b)	1[Ek,j]ψkH,j = 1[Ek,j]ψak,j and 1[ET]ψT = 1[ET]ψaT, and
(c)	ψk,j (f) is nk νH(nk) sub-Gaussian (12) given (ψk0,j0)k0>k,j0≥1 and (ψk,j0)j0<j and ψT is
'n ντ('n) sub-Gaussian (12) given Sc.
Here, step (i) and (ii) follow exactly as in steps (i) and (ii) of property (a) in App. F.1. For step (iii),
we use the definition1 of kt-split( g+2ggg +1) δ) for an input of size 2g√n, which implies that
δi = √n2g(g+2g(β +1)) δ in the notation of Dwivedi & Mackey (2022). The proof of Thm. 1 in
Dwivedi & Mackey (2022) then implies that
P[ET] ≤ Pg=1 2T~ P2=1 √n δ = Pg=1 2T~2g-j√n √⅛ ∙ g+2g(gβn + 1) δ = g+2g(βn + 1) δ,
as claimed.
Hence, on the event E = Tk,j Ek,j ∩ ET, these properties hold simultaneously for all HALvE calls
made by Compress, and, repeating an argument similar to the union bound (40),
P[Ec] ≤ P[ET] + Pe=0 p4=1 P[Ec,j] ≤ g+2g(gβn+1) 2 + Pβ=0 4k4k(g+2")) 2 = δ. (41)
Moreover, since φc++ = n (ψc + ψτ), Lem. 1 and the argument of App. F.1 together imply that
φC(f) is νC++ sub-Gaussian on E for each f with kfkk = 1.
21
F.4 Proof of Ex. 6: KT-Compress++
In the notation of Ex. 2, define
W a`n = √nα'n∕2 = Ca Pkkk ∞, and
'n v`n = √ηv' n/2 = Cv qkkk∞ lθg( 6(η-√ηδ(^-g)) ) Msin,k.
Since Halve = symmetrized KT(4神色+；：] +1))δ) for inputs of size ' and THIN =
KT(g+2g(β +1)δ), the proofs of Thms. 1-4 in DWivedi & Mackey (2022) identify a sequence of
events Ek,j and ET and random signed measures ψk,j and ψT such that, for each 0 ≤ k ≤ βn and
j∈ [4k],
(a)	P[Ekc,j]
≤ 4k(g+2g(βη + 1)) 2 and P[ET] ≤ g + 2g(βη + 1) δ,
(b)	1[Ek,j]ΨH,j = 1[Ek,j]Ψk,j and 1[Et ]Ψt = 1[Et ]Ψt,
(C) P[ n kψk,j (k)kk ≥ αn + Vnk Vt |	(ψk0,j0 )k0>k,j0≥1, (ψk,j0 )j0<j ] ≤ e and
P导kψτ(k)kk ≥ a'n/2 + v'n∕2√t | SC] ≤ e-t forall t ≥ 0,and
/ j∖ irɔ r 7	∖	/ 7	∖ ι C
(d) E[ψk,j (k) | (ψk0,j0 )k0>k,j0≥1, (ψk,j0)j0<j] = 0.
We derive property (a) exactly as in App. F.3. Hence, on the event E = Tk,j Ek,j ∩ ET, these
properties hold simultaneously for all Halve calls made by Compress and
ZH ('n) = Zt (钓=Cv q∣∣kk∞ log( 6n-√η(2g-gl) MSiη,k.
Moreover, by the union bound (41), P[Ec] ≤ 2.
Finally, since Φc++ = n (Ψc + ψτ) and the argument of App. F.2 implies that COMPRESS is k-
SUb-GaUSSian on E with parameters (V,a), the triangle inequality implies that Compress++ is
k-sub-GaUSSian on E with parameters (V, ^) as in App. E.
G Supplementary Details for Experiments
In this section, we provide supplementary experiment details deferred from Sec. 5, as well as some
additional results.
In the legend of each MMD plot, we display an empirical rate of decay. In all experiments involving
kernel thinning, we set the algorithm failure probability parameter δ = 2 and compare KT(δ) to
Compress and Compress++ with Halve and Thin set as in Exs. 4 and 6 respectively.
G. 1 Mixture of Gaussian target details and MMD plots
For the target used for coreset visualization in Fig. 3, the mean locations are on two concentric
circles of radii 10 and 20, and are given by
μj = αj
sin(j)
cos(j)
where aj = 10 ∙ 1(j ≤ 16) + 20 ∙ 1( j > 16) for j = 1, 2,..., 32.
Here we also provide additional results with mixture of Gaussian targets given by P
M PM=i N(μj, Id) for M ∈ {4,6, 8}. The mean locations for these are given by
μι = [-3,3]>,	μ2	=	[-3,3]>,	μ3	=	[—3, -3]>,	μ4	=	[3, -3]>,
μ5 = [0, 6]>,	μ6	=	[-6,0]>,	μ7	=	[6,0]>,	μ8	=	[0,	-6]>.
Fig. 4 plots the MMD errors of KT and herding experiments for the mixture of Gaussians targets
with 4, 6 and 8 centers, and notice again that COMPRESS++ provides a competitive performance to
the original algorithm, in fact suprisingly, improves upon herding.
22
M=4
M=6
M=8
Figure 4: For M -component mixture of Gaussian targets, KT-COMPRESS++ and Herd-COMPRESS++ im-
prove upon the MMD of i.i.d. sampling (ST) and closely track or improve upon the error of their
quadratic-time input algorithms, KT and kernel herding (Herd). See App. G.1 for more details.
G.2 Details Of MCMC Targets
Our set-up for the MCMC experiments is identical to that of Dwivedi & Mackey (2021, Sec. 6),
except that we use all post-burn-in points to generate our Goodwin and Lotka-Volterra input point
sequences Sin instead of only the odd indices. In particular, we use the MCMC output of Riabiz et al.
(2020b) described in (Riabiz et al., 2020a, Sec. 4) and perform thinning experiments after discarding
the burn-in points. To generate an input Sin of size n for a thinning algorithm, we downsample
the post-burn-in points using standard thinning. For Hinch, we additionally do coordinate-wise
normalization by subtracting the sample mean and dividing by sample standard deviation of the
post-burn-in-points.
In Sec. 5, RW and ADA-RW respectively refer to Gaussian random walk and adaptive Gaussian
random walk Metropolis algorithms (Haario et al., 1999) and MALA and pMALA respectively
refer to the Metropolis-adjusted Langevin algorithm (Roberts & Tweedie, 1996) and pre-conditioned
MALA (Girolami & Calderhead, 2011). For Hinch experiments, RW 1 and RW 2 refer to two
independent runs of Gaussian random walk, and “Tempered” denotes the runs targeting a tempered
Hinch posterior. For more details on the set-up, we refer the reader to Dwivedi & Mackey (2021,
Sec. 6.3, App. J.2).
H Streaming Version of Compress
Compress can be efficiently implemented in a streaming fashion (Alg. 3) by viewing the recursive
steps in Alg. 1 as different levels of processing, with the bottom level denoting the input points and
the top level denoting the output points. The streaming variant of the algorithm efficiently maintains
memory at several levels and processes inputs in batches of size 4g+1. At any level i (with i = 0
denoting the level of the input points), whenever there are 2i4g+1 points, the algorithm runs HALVE
on the points in this level, appends the output of size 2i-14g+1 to the points at level i+1, and empties
the memory at level i (and thereby level i never stores more than 2i4g+1 points). In this fashion, just
after processing n = 4k+g+1 points, the highest level is k + 1, which contains a compressed coreset
of size 2k-14g+1 = 2k+g+12g = √n2g (outputted by running Halve at level k for the first time),
which is the desired size for the output of Compress.
23
KOuIəuɪ jo QJnSEOUI ⅛0 SE pəjsS SlUod ElEPJO JOqUInU ə-sIUn8
əʌvoɔuənbos Ind.sə-sUEq 二əɪ-EuIS qornu əztSIM SJUod JO S-Qsqns UO (HAn‹H UnJ PUB) UaulEuI
λ∖ou əm SEeəuɪH MU∙≡unj JOJ IEql2IBHullSeɔəASl SjUQUIQJInbDJ TdouIOUI.sSUM JOJ UOSS.SOqI
∞SHXdno□ Jo (S .MVo IUEPBA MulUIBəts DqIJo AlIXəɪduɪoo əɔEdS əiŋ səzAIEUB IInSəj Ixou ⅛0
puω
Z: JoJ q U 7 PUB q-M ə-s JO -əsəjoɔ //
2 1əAəɪ-əsəjoɔ AldLa=
I+I——工三吗N -二əs SBq 二 əsəjg I +əA乌ppd∩ //
I+I——əs sləsəməA乌 əBH //
ud lndI+0+qss90 OJd JJB ləsəjs ɪ +『1əAωz Ul //
I+əs JO S9qpqlndSsg OJd =
iəsəjou O ləAəl AldLɑə əz Ul //
puω
1+6 In-n。一
UaqI左 W『JOj q Uu 7
pu
puω
{} 七6
S ∩ I+⅛> 1 I+s
(S)HAaVH 1 S
uaɪ+== 一 JI-
OPl +寸Ou ;∙ ∙u 2 J
pu
{}:6 -
UaqI左 W『JOj q Uu 7
1+6 工二( S) ∩。6 J。6
OPe …NlL joJ
{i Os
:∙-a 二0i2u'sd Ind'sJo 日pəts 八0 Js9mB,md0β-sIduwsJ9AodAuVH UIΦ-so-B0β's-Bq-nduɪ
充 W W PUB I+0+W H S JoJ %-ə-s JO siəsəjou JO muətsi2ndlno — (0β-suw9i3s) sswQ2d≡o□ ：g UI-'εo-V
3
■ ∙{u‰0I 弓)ou (ti)++ωs 铝M OM C9 .Xaf.sSE 0
MU-səs ¾Ag+⅛vl4+1 寸+⅛= (4HMS + (XDS U (X++。S AEXQIdUIS QQEdS OqI .9 .xω.sSE
⅞+-l7与HM = NIHI pɑŋ 2 DZlS JO Ind.SUE2PDHddE UOqM (PkbnLUNIE)IM PDZ-səuiuiAS
=HAn<H qψA∞SHXdno□ JO IUBPBAMUnUB g S Dqlq++SSHXdnOD Jopisuoo IXOn
咚「吉「Vlg)ɔs IBw səHdInn .dojd C(E .oəs ClZOZ 溢 ə5pen 赵 lpəAWCl) UVl(Sss
əɔuɪsd.Xaf Ul SE 0 əzg JO SJndul HA=<H JOJ (If"zHM pəz-səuɪuɪAS H HA=<H £1M SSHXdnOD
JoIUBPBA MUPnBQKS əsjəpɪsus 1S∙⅛H++SSHNdWCO J XPUBSSHNdWOɔ J X)CaldUWXH
□ .sluod 一 EUO≡PPE (I+法弓)HSISOunEMUy2s SQIPbDJ qo≡M I+%⅛A
ISoUI IE OZIS JO əɔuənbəs IUod E UO HAn<H UIU OAEq OAX 7oʊɪn2dn əuɪ-ɑAUE IEeOJOUUQqxmH
弓 m+⅛V⅛N+2?工g∙oM
Aq POPUnOq Sl
2 əuɪp IE pəjpS SJUod Jo JOqUInU UmuIlXEuI Oq 二 Snql .s-əAOI ISounE əɪe əjəsel DUlnlE⅛L>
ISoUI IE OZIS JO -əs SBPE MUIUaUWUI əiB QΛ∖ əɔ,ss .SJUod SBP⅛N+∙oaISO ʊɪ IEOAEq ɑŋɔ DM KOnIDnI
JO 一 əAOI W Dq二 4 .UIqlyOMIE əsJo QMESn əɔpds ə-səjEUlPsə2əei PInOλ∖ qλ∖ 7 DulnK JOOJd
.Qolualu jo SMOdS企(2∕∖+%)HS + qm+⅛= ⅞s ISOUI 苫 pasn Sml (E⅛y)
SSHXdnOU ⅛ Il.2H ,3IdUη MElUbJnS aM 二 2 IloHIZ6占 MIl 盲 dm。。<&旨κ2¾z E 2ZZS ⅛ Sgdul
§ SrUφdSG》 (EHS 34OlS HA=VH 芍一 (PUnoaAjoUIωw 8≡UIea⅛s SSHNdWOU) I UOSSOdOjd