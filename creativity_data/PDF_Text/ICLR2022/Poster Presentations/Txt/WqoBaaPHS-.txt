Published as a conference paper at ICLR 2022
Top-label calibration
and multiclass-to-binary reductions
Chirag Gupta & Aaditya Ramdas
Carnegie Mellon University
{chiragg,aramdas}@cmu.edu
Ab stract
We propose a new notion of multiclass calibration called top-label calibration. A
classifier is said to be top-label calibrated if the reported probability for the pre-
dicted class label—the top-label—is calibrated, conditioned on the top-label. This
conditioning is essential for practical utility of the calibration property, since the
top-label is always reported and we must condition on what is reported. How-
ever, the popular notion of confidence calibration erroneously skips this condi-
tioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction frame-
work that unifies confidence, top-label, and class-wise calibration, among others.
As its name suggests, M2B works by reducing multiclass calibration to differ-
ent binary calibration problems; various types of multiclass calibration can then
be achieved using simple binary calibration routines. We instantiate the M2B
framework with the well-studied histogram binning (HB) binary calibrator, and
prove that the overall procedure is multiclass calibrated without making any as-
sumptions on the underlying data distribution. In an empirical evaluation with
four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B
+ HB procedure achieves lower top-label and class-wise calibration error than
other approaches such as temperature scaling. Code for this work is available at
https://github.com/aigen/df-posthoc-calibration.
1	Introduction
Machine learning models often make probabilistic predictions. The ideal prediction is the true con-
ditional distribution of the output given the input. However, nature never reveals true probability
distributions, making it infeasible to achieve this ideal in most situations. Instead, there is signifi-
cant interest towards designing models that are calibrated, which is often feasible. We motivate the
definition of calibration using a standard example of predicting the probability of rain. Suppose a
meteorologist claims that the probability of rain on a particular day is 0.7. Regardless of whether it
rains on that day or not, we cannot know if 0.7 was the underlying probability of rain. However, we
can test if the meteorologist is calibrated in the long run, by checking if on the D days when 0.7 was
predicted, it indeed rained on around 0.7D days (and the same is true for other probabilities).
This example is readily converted to a formal binary calibration setting. Denote a random (feature,
label)-pair as (X, Y) p X X {0,1}, where X is the feature space. A probabilistic predictor h : X →
r0, 1s is said to be calibrated if for every prediction q P r0, 1s, PrpY “ 1 | hpXq “ qq “ q (almost
surely). Arguably, if an ML classification model produces such calibrated scores for the classes,
downstream users of the model can reliably use its predictions for a broader set of tasks.
Our focus in this paper is calibration for multiclass classification, with L23 classes and Y P
[L] :“ {1, 2,...,L23}. We assume all (training and test) data is drawn i.i.d. from a fixed
distribution P, and denote a general point from this distribution as (X, Y) „ P. Consider a typical
multiclass predictor, h : X → ∆Lτ, whose range ∆Lτ is the probability simplex in RL. A
natural notion of calibration for h, called canonical calibration is the following: for every l P rLs,
P(Y “ l | h(X) “ q) “ ql (ql denotes the l-th component of q). However, canonical calibration
becomes infeasible to achieve or verify once L is even 4 or 5 (Vaicenavicius et al., 2019). Thus,
there is interest in studying statistically feasible relaxations of canonical notion, such as confidence
calibration (Guo et al., 2017) and class-wise calibration (Kull et al., 2017).
1
Published as a conference paper at ICLR 2022
In particular, the notion of confidence calibration (Guo et al., 2017) has been popular recently.
A model is confidence calibrated if the following is true: “when the reported confidence for the
predicted class is q P r0, 1s, the accuracy is also q”. In any practical setting, the confidence q is
never reported alone; it is always reported along with the actual class prediction l P rLs. One
may expect that if a model is confidence calibrated, the following also holds: “when the class l is
predicted with confidence q, the probability of the actual class being l is also q”? Unfortunately, this
expectation is rarely met—there exist confidence calibrated classifier for whom the latter statement
is grossly violated for all classes (Example 1). On the other hand, our proposed notion of top-label
calibration enforces the latter statement. It is philosophically more coherent, because it requires
conditioning on all relevant reported quantities (both the predicted top label and our confidence in
it). In Section 2, we argue further that top-label calibration is a simple and practically meaningful
replacement of confidence calibration.
In Section 3, we unify top-label, confidence, and a number of other popular notions of multiclass
calibration into the framework of multiclass-to-binary (M2B) reductions. The M2B framework re-
lies on the simple observation that each of these notions internally verifies binary calibration claims.
As a consequence, each M2B notion of calibration can be achieved by solving a number of binary
calibration problems. With the M2B framework at our disposal, all of the rich literature on binary
calibration can now be used for multiclass calibration. We illustrate this by instantiating the M2B
framework with the binary calibration algorithm of histogram binning or HB (Zadrozny and Elkan,
2001; Gupta and Ramdas, 2021). The M2B + HB procedure achieves state-of-the-art results with
respect to standard notions of calibration error (Section 4). Further, we show that our procedure is
provably calibrated for arbitrary data-generating distributions. The formal theorems are delayed to
Appendices B, C (due to space limitations), but an informal result is presented in Section 4.
2	Modifying confidence calibration to top-label calibration
Let C : X → [L] denote a classifier or top-label predictor and h : X → [0,1] a function that
provides a confidence or probability score for the top-label cpXq. The predictor pc, hq is said to be
confidence calibrated (for the data-generating distribution P) if
PpY “ cpXq | hpXqq “ hpXq.	(1)
In other words, when the reported confidence hpXq equals p P r0, 1s, then the fraction of instances
where the predicted label is correct also approximately equals p. Note that for an L-dimensional
predictor h : X → ∆Lτ, one would use c(∙) “ argmaxip[L] h√(∙) and h(∙) “ "(.)(∙)； ties are
broken arbitrarily. Then h is confidence calibrated if the corresponding pc, hq satisfies (1).
Confidence calibration is most applicable in high-accuracy settings where we trust the label pre-
diction cpxq. For instance, if a high-accuracy cancer-grade-prediction model predicts a patient as
having “95% grade III, 3% grade II, and 2% grade I”, we would suggest the patient to undergo
an invasive treatment. However, we may want to know (and control) the number of non-grade-III
patients that were given this suggestion incorrectly. In other words, is Prpcancer is not grade III |
cancer is predicted to be of grade III with confidence 95%q equal to 5%? It would appear that by
focusing on the the probability of the predicted label, confidence calibration enforces such control.
However, as we illustrate next, confidence calibration fails at this goal by providing a guarantee that
is neither practically interpretable, nor actionable. Translating the probabilistic statement (1) into
words, we ascertain that confidence calibration leads to guarantees of the form: “if the confidence
hpXq in the top-label is 0.6, then the accuracy (frequency with which Y equals cpXq) is 0.6”. Such
a guarantee is not very useful. Suppose a patient P is informed (based on their symptoms X), that
they are most likely to have a certain disease D with probability 0.6. Further patient P is told that
this score is confidence calibrated. P can now infer the following: “among all patients who have
probability 0.6 of having some unspecified disease, the fraction who have that unspecified disease
is also 0.6.” However, P is concerned only about disease D, and not about other diseases. That is,
P wants to know the probability of having D among patients who were predicted to have disease D
with confidence 0.6, not among patients who were predicted to have some disease with confidence
0.6. In other words, P cares about the occurrence ofD among patients who were told the same thing
that P has been told. It is tempting to wish that the confidence calibrated probability 0.6 has any
bearing on what P cares about. However, this faith is misguided, as the above reasoning suggests,
and further illustrated through the following example.
2
Published as a conference paper at ICLR 2022
Example 1. Suppose the instance space is (X, Y) P {a, b} X {1,2,...}. (X can be seen as the
random patient, and Y as the disease they are suffering from.) Consider a predictor pc, hq and let the
values taken by (X, Y, c, h) be as follows:
Feature x	P (X “ x)	Class prediction c(χ)	Confidence h(χ)	P(Y = c(X) | X = x)
a	05	1	06	02
b	05	2	O	10
The table specifies only the probabilities P (Y “ c(X) | X “ x); the probabilities
P (Y “ l | X “ x), l ‰ c(x), can be set arbitrarily. We verify that (c, h) is confidence calibrated:
P (y = C(X) | h(X) = 0.6) = 0.5(P (Y = 1 | X “ a) ' P (Y = 2 | X = b))= 0.5(0.2 ' 1)= 0.6.
However, whether the actual instance is X “ a or X “ b, the probabilistic claim of 0.6 bears
no correspondence with reality. If X “ a, h(X) “ 0.6 is extremely overconfident since
P(Y = 1 | X “ a) “ 0.2. Contrarily, if X “ b, h(X) “ 0.6 is extremely underconfident. □
The reason for the strange behavior above is that the probability P(Y “ c(X) | h(X )) is not
interpretable from a decision-making perspective. In practice, we never report just the confidence
h(X ), but also the class prediction c(X) (obviously!). Thus it is more reasonable to talk about the
conditional probability ofY “ c(X), given what is reported, that is both c(X) and h(X). We make
a small but critical change to (1); we say that (c, h) is top-label calibrated if
P(Y “c(X) |h(X),c(X))“h(X).	(2)
(See the disambiguating Remark 2 on terminology.) Going back to the patient-disease example,
top-label calibration would tell patient P the following: “among all patients, who (just like you)
are predicted to have disease D with probability 0.6, the fraction who actually have disease D is
also 0.6.” Philosophically, it makes sense to condition on what is reported—both the top label and
its confidence—because that is what is known to the recipient of the information; and there is no
apparent justification for not conditioning on both.
A commonly used metric for quantifying the miscalibration of a model is the expected-calibration-
error (ECE) metric. The ECE associated with confidence calibration is defined as
conf-ECE(c, h) :“ EX |P(Y “ c(X) | h(X)) ´ h(X)| .	(3)
We define top-label-ECE (TL-ECE) in an analogous fashion, but also condition on c(X):
TL-ECE(c, h) :“ EX |P(Y “ c(X) | c(X), h(X)) ´ h(X)| .	(4)
Higher values of ECE indicate worse calibration performance. The predictor in Example 1 has
conf-ECE(c, h) “ 0. However, it has TL-ECE(c, h) “ 0.4, revealing its miscalibration. More gen-
erally, it can be deduced as a straightforward consequence of Jensen’s inequality that conf-ECE(c, h)
is always smaller than the TL-ECE(c, h) (see Proposition 4 in Appendix H). As illustrated by Exam-
ple 1, the difference can be significant. In the following subsection we illustrate that the difference
can be significant on a real dataset as well. First, we make a couple of remarks.
Remark 1 (ECE estimation using binning). Estimating the ECE requires estimating probabilities
conditional on some prediction such as h(x). A common strategy to do this is to bin together nearby
values of h(x) using binning schemes (Nixon et al., 2020, Section 2.1), and compute a single esti-
mate for the predicted and true probabilities using all the points in a bin. The calibration method
we espouse in this work, histogram binning (HB), produces discrete predictions whose ECE can
be estimated without further binning. Based on this, we use the following experimental protocol:
we report unbinned ECE estimates while assessing HB, and binned ECE estimates for all other
compared methods, which are continuous output methods (deep-nets, temperature scaling, etc). It
is commonly understood that binning leads to underestimation of the effective ECE (Vaicenavicius
et al., 2019; Kumar et al., 2019). Thus, using unbinned ECE estimates for HB gives HB a disadvan-
tage compared to the binned ECE estimates we use for other methods. (This further strengthens our
positive results for HB.) The binning scheme we use is equal-width binning, where the interval r0, 1s
is divided into B equal-width intervals. Equal-width binning typically leads to lower ECE estimates
compared to adaptive-width binning (Nixon et al., 2020).
Remark 2 (Terminology). The term conf-ECE was introduced by Kull et al. (2019). Most works
refer to conf-ECE as just ECE (Guo et al., 2017; Nixon et al., 2020; Mukhoti et al., 2020; Kumar
et al., 2018). However, some papers refer to conf-ECE as top-label-ECE (Kumar et al., 2019; Zhang
et al., 2020), resulting in two different terms for the same concept. We call the older notion as
conf-ECE, and our definition of top-label calibration/ECE (4) is different from previous ones.
3
Published as a conference paper at ICLR 2022
ReSNet50
confidence
ReSNet50
top-label
Fraction of
predictions
(a) Confidence reliability diagram (points
marked ‹) and top-label reliability diagram
(points marked `) for a ResNet-50 model on
the CIFAR-10 dataset; see further details in
points (a) and (b) below. The gray bars denote
the fraction of predictions in each bin. The
confidence reliability diagram (mistakenly)
suggests better calibration than the top-label
reliability diagram.
(b) Class-wise and zoomed-in version of Figure 1a for bin
6 (top) and bin 10 (bottom); see further details in point (c)
below. The ‹ markers are in the same position as Figure 1a,
and denote the average predicted and true probabilities. The
colored points denote the predicted and true probabilities
when seen class-wise. The histograms on the right show the
number of test points per class within bins 6 and 10.
Figure 1: Confidence reliability diagrams misrepresent the effective miscalibration.
2.1	An illustrative experiment with ResNet-50 on CIFAR- 1 0
We now compare confidence and top-label calibration using ECE estimates and reliability diagrams
(Niculescu-Mizil and Caruana, 2005). This experiment can be seen as a less malignant version of
Example 1. Here, confidence calibration is not completely meaningless, but can nevertheless be
misleading. Figure 1 illustrates the (test-time) calibration performance of a ResNet-50 model (He
et al., 2016) on the CIFAR-10 dataset (Krizhevsky, 2009). In the following summarizing points, the
pc, hq correspond to the ResNet-50 model.
(a)	The ‹ markers in Figure 1a form the confidence reliability diagram (Guo et al., 2017), con-
structed as follows. First, the hpxq values on the test set are binned into one of B “ 10 bins,
r0, 0.1q, r0.1, 0.2q, . . . , r0.9, 1s, depending on the interval to which hpxq belongs. The gray bars
in Figure 1a indicate the fraction of hpxq values in each bin—nearly 92% points belong to bin
r0.9, 1s and no points belong to bin r0, 0.1q. Next, for every bin b, we plot ‹ “ pconfb, accbq,
which are the plugin estimates of E rhpX q | hpXq P Bin bs and P pY “ cpXq | hpXq P Bin bq
respectively. The dashed X “ Y line indicates perfect confidence calibration.
(b)	The ` markers in Figure 1a form the top-label reliability diagram. Unlike the confidence
reliability diagram, the top-label reliability diagram shows the average miscalibration across
classes in a given bin. For a given class l and bin b, define
.^ , .............. _ _ , 一、 ^ .......................... _ _ , .
∆b,ι ：“ ∖ppy = C(Xq | C(Xq = ι, hpxq P Bin bq ´ E [h(xq∣ C(Xq = ι, hpxq P Bin b]∣,
where p, E denote empirical estimates based on the test data. The overall miscalibration is then
. .	， .	、	_ r> ， _______ 一 ， 一 _____	. 一、 .
△b :“ Weighted-average(∆b,ι) “ XlPrLs P(C(X) = l | h(X) P Bin b) ∆b,ι.
Note that ∆b is always non-negative and does not indicate whether the overall miscalibration
occurs due to under- or over-confidence; also, if the absolute-values were dropped from △b,l,
then △b would simply equal accb ´ confb . In order to plot △b in a reliability diagram, we obtain
the direction for the corresponding point from the confidence reliability diagram. Thus for every
< “(Confb, accbq,weplot ' “(Confb, confb'∆bq ifaccb > Confb and ' “(Confb, Confb—a)
otherwise, for every b. This SCatter Plot of the ''s gives Us the top-label reliability diagram.
Figure 1a shows that there is a visible increase in miscalibration when going from confidence
calibration to top-label calibration. To understand why this Change oCCurs, Figure 1b zooms
into the sixth bin (h(Xq P r0.5, 0.6q) and bin 10 (h(X q P r0.9, 1.0s), as desCribed next.
(C) Figure 1b displays the class-wise top-label reliability diagrams for bins 6 and 10. Note that for
bin 6, the ‹ marker is nearly on the X “ Y line, indiCating that the overall aCCuraCy matChes the
4
Published as a conference paper at ICLR 2022
---- Base model top-label-ECE --------- Temperature scaling top-label-ECE ------- Histogram binning top-label-ECE
---------------------------------------------------------------------------------Base model conf-ECE -Temperature scaling conf-ECE -Histogram binning conf-ECE
mυw p∂sEAS3
0.025-
0.020
0.010-
ReSNet-Ilo
mυw p∂sEAS3
0.010
0.005 -
0.030
0.025
W 0.020
I
E 0.015-
S
0.010-
0.005
Figure 2: Conf-ECE (dashed lines) and TL-ECE (solid lines) of four deep-net architectures on
CIFAR-10, as well as with recalibration using histogram binning and temperature scaling. The TL-
ECE is often 2-3 times the conf-ECE, depending on the number of bins used to estimate ECE, and
the architecture. Top-label histogram binning typically performs better than temperature scaling.
overall confidence of 0.545. However, the true accuracy when class 1 was predicted is « 0.2 and
the true accuracy when class 8 was predicted is « 0.9 (a very similar scenario to Example 1). For
bin 10, the ‹ marker indicates a miscalibration of « 0.01; however, when class 4 was predicted
(roughly 8% of all test-points) the miscalibration is « 0.05.
Figure 2 displays the aggregate effect of the above phenomenon (across bins and classes) through
estimates of the conf-ECE and TL-ECE. The precise experimental setup is described in Section 4.
These plots display the ECE estimates of the base model, as well as the base model when recalibrated
using temperature scaling (Guo et al., 2017) and our upcoming formulation of top-label histogram
binning (Section 3). Since ECE estimates depend on the number of bins B used (see Roelofs et al.
(2020) for empirical work around this), we plot the ECE estimate for every value B P r5, 25s in order
to obtain clear and unambiguous results. We find that the TL-ECE is significantly higher than the
conf-ECE for most values of B, the architectures, and the pre- and post- recalibration models. This
figure also previews the performance of our forthcoming top-label histogram binning algorithm.
Top-label HB has smaller estimated TL-ECE than temperature scaling for most values of B and the
architectures. Except for ResNet-50, the conf-ECE estimates are also better.
To summarize, top-label calibration captures the intuition of confidence calibration by focusing on
the predicted class. However, top-label calibration also conditions on the predicted class, which is
always part of the prediction in any practical setting. Further, TL-ECE estimates can be substantially
different from conf-ECE estimates. Thus, while it is common to compare predictors based on the
conf-ECE, the TL-ECE comparison is more meaningful, and can potentially be different.
3	Calibration algorithms from calibration metrics
In this section, we unify a number of notions of multiclass calibration as multiclass-to-binary (or
M2B) notions, and propose a general-purpose calibration algorithm that achieves the corresponding
M2B notion of calibration. The M2B framework yields multiple novel post-hoc calibration algo-
rithms, each of which is tuned to a specific M2B notion of calibration.
3.1	Multiclass-to-binary (M2B) notions of calibration
In Section 2, we defined confidence calibration (1) and top-label calibration (2). These notions
verify calibration claims for the highest predicted probability. Other popular notions of calibration
verify calibration claims for other entries in the full L-dimensional prediction vector. A predictor
h “ ph1, h2, . . . , hLq is said to be class-wise calibrated (Kull et al., 2017) if
(class-wise calibration) @l P rLs, PpY “ l | hl pXqq “ hl pXq.	(5)
Another recently proposed notion is top-K confidence calibration (Gupta et al., 2021). For some
l P rLs, let cplq : X → [L] denote the l-th highest class prediction, and let hplq : X → [L] denote
the confidence associated with it (C “ cp1q and h “ hp1q are special cases). For a given K ≤ L,
(top-K-confidence calibration) @k P rKs, P pY “ cpkq pXq | hpkq pXqq “ hpkq pXq.	(6)
5
Published as a conference paper at ICLR 2022
Calibration notion	Quantifier	Prediction (pred(X))	Binary calibration statement
Confidence	-	h(χ q	P(Y “ C(Xq | Pred(Xqq “ h(χq	’
Top-label	-	C Xq, h(χq	P (Y “ c(χq | pred(χqq “ h(χq
Class-wise	@l P [Ls-	hι(χ q	P(Y “ l | pred(χqq “ hl (χq
Top-K-confidence	@k p [Ks	hpkq(χ q	P(Y “ Cpkq (χq | pred(χqq “ hpkq (χq
Top-K-label	@k p [ks	cpkq (χ q,hpkq(χ q	P(Y “ Cpkq (χq | pred(χqq “ hpkq (χq
Table 1: Multiclass-to-binary (M2B) notions internally verify one or more binary calibration state-
ments/claims. The statements in the rightmost column are required to hold almost surely.
As We did in Section 2 for Confidence→top-山bel, top-K-confidence calibration can be modified to
the more interpretable top-K-label calibration by further conditioning on the predicted labels:
(top-K-label calibration) @k P rKs,PpY “ cpkq pXq | hpkqpXq, cpkqpXqq “ hpkq pXq.	(7)
Each of these notions reduce multiclass calibration to one or more binary calibration requirements,
Where each binary calibration requirement corresponds to verifying if the distribution of Y , condi-
tioned on some prediction predpX q, satisfies a single binary calibration claim associated with
predpXq. Table 1 illustrates hoW the calibration notions discussed so far internally verify a number
of binary calibration claims, making them M2B notions. For example, for class-Wise calibration, for
every l P rLs, the conditioning is on predpXq “ hl pXq, and a single binary calibration statement
is verified: PpY “ l | predpXqq “ hlpXq. Based on this property, We call each of these notions
multiclass-to-binary or M2B notions.
The notion of canonical calibration mentioned in the introduction is not an M2B notion. Canonical
calibration is discussed in detail in Appendix G. Due to the conditioning on a multi-dimensional
prediction, non-M2B notions of calibration are harder to achieve or verify. For the same reason, it is
possibly easier for humans to interpret binary calibration claims When taking decisions/actions.
3.2	Achieving M2B notions of calibration using M2B calibrators
The M2B frameWork illustrates hoW multiclass calibration can typically be vieWed via a reduction to
binary calibration. The immediate consequence of this reduction is that one can noW solve multiclass
calibration problems by leveraging the Well-developed methodology for binary calibration.
The upcoming M2B calibrators belong to the standard recalibration or post-hoc calibration setting.
In this setting, one starts with a fixed pre-learnt base model g : X → ∆L11. The base model g
can correspond to a deep-net, a random forest, or any 1-v-all (one-versus-all) binary classification
model such as logistic regression. The base model is typically optimized for classification accuracy
and may not be calibrated. The goal of post-hoc calibration is to use some given calibration data
D “(Xι,Yι), pX2, Y⅛q,..., PXn, Ynq P(X X [L])n, typically data on which g was not learnt, to
recalibrate g. In practice, the calibration data is usually the same as the validation data.
To motivate M2B calibrators, suppose we want to verify if g is calibrated on a certain test set,
based on a given M2B notion of calibration. Then, the verifying process will split the test data
into a number of sub-datasets, each of which will verify one of the binary calibration claims. In
Appendix A.2, we argue that the calibration data can also be viewed as a test set, and every step in
the verification process can be used to provide a signal for improving calibration.
M2B calibrators take the form of wrapper methods that work on top of a given binary calibrator.
Denote an arbitrary black-box binary calibrator as A{o,i} : [0, IsX X(X X {0,1})* → [0, IsX, where
the first argument is a mapping X → r0, 1s that denotes a (miscalibrated) binary predicor, and the
second argument is a calibration data sequence of arbitrary length. The output is a (better calibrated)
binary predictor. Examples of At0,1u are histogram binning (Zadrozny and Elkan, 2001), isotonic
regression (Zadrozny and Elkan, 2002), and Platt scaling (Platt, 1999). In the upcoming descriptions,
we use the indicator function 1 {a “ b} p {0,1} which takes the value 1 if a “ b, and 0 if a ‰ b.
The general formulation of our M2B calibrator is delayed to Appendix A since the description is a
bit involved. To ease readability and adhere to the space restrictions, in the main paper we describe
the calibrators corresponding to top-label, class-wise, and confidence calibration (Algorithms 1-3).
Each of these calibrators are different from the classical M2B calibrator (Algorithm 4) that has been
used by Zadrozny and Elkan (2002), Guo et al. (2017), Kull et al. (2019), and most other papers
6
Published as a conference paper at ICLR 2022
M2B calibrators: Post-hoc multiclass calibration using binary calibrators
Input in each case: Binary calibrator A{o,i} : [0, IsX X(X X {0,1})< → [0, 1]x, base multiclass
predictor g : X → ∆L11, calibration data D “ (X1,Y1),..., (Xn, Yn).
1
2
3
4
5
1
2
3
4
5
6
7
8
Algorithm 1: Confidence calibrator	A C D classifier or top-class based on g;	ι W g D top-class-probability based on g;	2 f D1 -{(Xi, ItYi “ C(Xi)U) : i P[ns};	3 h D a{0,i} (g, DI);	4 return (C, h);	5 e 	6 r	Jgorithm 3: Class-wise calibrator Urite g = (g1,g2,...,gL); )r l D 1 to L do Dl -{(Xi, ItYi = l}) : i P[ns}; hl D A{0,1}(gl, Dl); nd eturn (h1, h2, . . . , hL);
	
Algorithm 2: Top-label calibrator	A c D classifier or top-class based on g;	1 W g D top-class-probability based on g;	2 for l D 1 to L do	3 Dl ð t(Xi, ItYi “ l}) : C(Xi) = l)};	4 hl D Ato,i}(g,Dl);	5 e end	6 N h(∙) D hcp,q(∙) (predict hl(x) if C(X) = l); return (C, h);	7 r	lgorithm 4: Normalized calibrator rite g = (g1, g2, . . . , gL); )r l D 1 to L do Dl -t(Xi, ItYi = l}) : i p[ns}; hl D A{0,1}(gl, Dl), nd ormalize: for every l P [Ls, ..	〜				 T	〜	.- hl(∙) := hl(∙)/∑k=1 hk(∙); eturn (h1, h2, . . . , hL);
we are aware of, with the most similar one being Algorithm 3. Top-K-label and top-K-confidence
calibrators are also explicitly described in Appendix A (Algorithms 6 and 7).
Top-label calibration requires that for every class l P [Ls, P (Y “ l | c(X) “ l, h(X )) “ h(X).
Thus, to achieve top-label calibration, we must solve L calibration problems. Algorithm 2 constructs
L datasets tDl : l P [Lsu (line 4). The features in Dl are the Xi ’s for which c(Xi) “ l, and the
labels are ItYi “ l}. Now for every l P [Ls, We calibrate g to hi : X → [0,1s using Dl and any
binary calibrator. The final probabilistic predictor is h(∙) “ hc(.q(∙) (that is, it predicts hl (x) if
c(x) “ l). The top-label predictor c does not change in this process. Thus the accuracy of (c, h) is
the same as the accuracy of g irrespective of which At0,1u is used. Unlike the top-label calibrator,
the confidence calibrator merges all classes together into a single dataset D1 “ UlPrLs Dl.
To achieve class-wise calibration, Algorithm 3 also solves L calibration problems, but these corre-
spond to satisfying P (Y “ l | hl (X)) “ hl (X). Unlike top-label calibration, the dataset Dl for
class-wise calibration contains all the Xi’s (even if c(Xi) ‰ l), and hl is passed to At0,1u instead
of h. Also, unlike confidence calibration, Yi is replaced with ItYi “ l} instead of 1 {匕 “ c(Xi)}.
The overall process is similar to reducing multiclass classification to L 1-v-all binary classification
problem, but our motivation is intricately tied to the notion of class-wise calibration.
Most popular empirical works that have discussed binary calibrators for multiclass calibration have
done so using the normalized calibrator, Algorithm 4. This is almost identical to Algorithm 3, except
that there is an additional normalization step (line 6 of Algorithm 4). This normalization was first
proposed by Zadrozny and Elkan (2002, Section 5.2), and has been used unaltered by most other
works1 where the goal has been to simply compare direct multiclass calibrators such as temperature
scaling, Dirichlet scaling, etc., to a calibrator based on binary methods (for instance, see Section
4.2 of Guo et al. (2017)). In contrast to these papers, we investigate multiple M2B reductions in an
effort to identify the right reduction of multiclass calibration to binary calibration.
To summarize, the M2B characterization immediately yields a novel and different calibrator for
every M2B notion. In the following section, we instantiate M2B calibrators on the binary calibrator
of histogram binning (HB), leading to two new algorithms: top-label-HB and class-wise-HB, that
achieve strong empirical results and satisfy distribution-free calibration guarantees.
1the only exception we are aware of is the recent work of Patel et al. (2021) who also suggest skipping
normalization (see their Appendix A1); however they use a common I-Max binning scheme across classes,
whereas in Algorithm 3 the predictor hl for each class is learnt completely independently of other classes
7
Published as a conference paper at ICLR 2022
Metric	Dataset	Architecture	Base	TS	VS	DS	N-HB	TL-HB
Top- label- ECE	CIFAR-10	ResNet-50	0.025	0.022	0.020	0.019	0.018	0.020
		ResNet-110	0.029	0.022	0.021	0.021	0.020	0.021
		WRN-26-10	0.023	0.023	0.019	0.021	0.012	0.018
		DenseNet-121	0.027	0.027	0.020	0.020	0.019	0.021
	CIFAR-100	ResNet-50	0.118	0.114	0.113	0.322	0.081	0.143
		ResNet-110	0.127	0.121	0.115	0.353	0.093	0.145
		WRN-26-10	0.103	0.103	0.100	0.304	0.070	0.129
		DenseNet-121	0.110	0.110	0.109	0.322	0.086	0.139
Top- label- MCE	CIFAR-10	ResNet-50	0.315	0.305	0.773	0.282	0.411	0.107
		ResNet-110	0.275	0.227	0.264	0.392	0.195	0.077
		WRN-26-10	0.771	0.771	0.498	0.325	0.140	0.071
		DenseNet-121	0.289	0.289	0.734	0.294	0.345	0.087
	CIFAR-100	ResNet-50	0.436	0.300	0.251	0.619	0.397	0.291
		ResNet-110	0.313	0.255	0.277	0.557	0.266	0.257
		WRN-26-10	0.273	0.255	0.256	0.625	0.287	0.280
		DenseNet-121	0.279	0.231	0.235	0.600	0.320	0.289
Table 2: Top-label-ECE and top-label-MCE for deep-net models (above: ‘Base’) and various post-
hoc calibrators: temperature-scaling (TS), vector-scaling (VS), Dirichlet-scaling (DS), top-label-HB
(TL-HB), and normalized-HB (N-HB). Best performing method in each row is in bold.
Metric	Dataset	Architecture	Base	TS	VS	DS	N-HB	CW-HB
Class- wise- ECE ^102	CIFAR-10	ResNet-50	0.46	-04F	^θɪ	^θɪ	0.50	-0.28-
		ResNet-110	0.59	-030^	^nɪ	ɪɪ	0.53	-027-
		WRN-26-10	0.44	^nɪ	^nɪ	-0:39-	0.39	-028-
		DenseNet-121	0.46	"-0.46-	-ffɪ	-ffɪ	0.48	-036-
	CIFAR-100	ResNet-50	0.22	-0:20-	-0:20-	-0:66-	0.23	-0:16-
		ResNet-110	0.24	-0:23-	~22Γ		0.24	-0:16-
		WRN-26-10	0.19	-0T5-	-0T8-	^03T	0.20	-0:14-
		DenseNet-121	0.20	~22Γ	-0T5-	~666~	0.24	-0:16-
Table 3: Class-wise-ECE for deep-net models and various post-hoc calibrators. All methods are
same as Table 2, except TL-HB is replaced with class-wise-HB (CW-HB).
4	Experiments: M2B calibration with histogram binning
Histogram binning or HB was proposed by Zadrozny and Elkan (2001) with strong empirical results
for binary calibration. In HB, a base binary calibration model g : X → [0,1] is used to partition the
calibration data into a number of bins so that each bin has roughly the same number of points. Then,
for each bin, the probability of Y “ 1 is estimated using the empirical distribution on the calibration
data. This estimate forms the new calibrated prediction for that bin. Recently, Gupta and Ramdas
(2021) showed that HB satisfies strong distribution-free calibration guarantees, which are otherwise
impossible for scaling methods (Gupta et al., 2020).
Despite these results for binary calibration, studies for multiclass calibration have reported that HB
typically performs worse than scaling methods such as temperature scaling (TS), vector scaling
(VS), and Dirichlet scaling (DS) (Kull et al., 2019; Roelofs et al., 2020; Guo et al., 2017). In our
experiments, we find that the issue is not HB but the M2B wrapper used to produce the HB baseline.
With the right M2B wrapper, HB beats TS, VS, and DS. A number of calibrators have been proposed
recently (Zhang et al., 2020; Rahimi et al., 2020; Patel et al., 2021; Gupta et al., 2021), but VS and
DS continue to remain strong baselines which are often close to the best in these papers. We do not
compare to each of these calibrators; our focus is on the M2B reduction and the message that the
baselines dramatically improve with the right M2B wrapper.
We use three metrics for comparison: the first is top-label-ECE or TL-ECE (defined in (4)), which
we argued leads to a more meaningful comparison compared to conf-ECE. Second, we consider
the more stringent maximum-calibration-error (MCE) metric that assesses the worst calibration
across predictions (see more details in Appendix E.3). For top-label calibration MCE is given by
TL-MCEpc, hq :“ maxlPrLs suprPRangephq |PpY “ l | cpXq “ l, hpXq “ rq ´ r|. To assess class-
wise calibration, we use class-wise-ECE defined as the average calibration error across classes:
8
Published as a conference paper at ICLR 2022
CW-ECE(c, h) :“ LT Z“1 EX |P(Y “ l | h7(X)) — h√X)|. All ECE/MCE estimation is Per-
formed as described in Remark 1. For further details, see Appendix E.2.
Formal algorithm and theoretical guarantees. ToP-label-HB (TL-HB) and class-wise-HB (CW-
HB) are exPlicitly stated in APPendices B and C resPectively; these are instantiations of the toP-label
calibrator and class-wise calibrator with HB. N-HB is the the normalized calibrator (Algorithm 4)
with HB, which is the same as CW-HB, but with an added normalization steP. In the APPendix,
we extend the binary calibration guarantees of GuPta and Ramdas (2021) to TL-HB and CW-HB
(Theorems 1 and 2). We informally summarize one of the results here: if there are at least k cali-
bration Points-Per-bin, then the expected-ECE is bounded as: E [(TL-) or (CW-) ECEs ≤ '∖{2k,
for TL-HB and CW-HB resPectively. The outer E above is an exPectation over the calibration data,
and corresPonds to the randomness in the Predictor learnt on the calibration data. Note that the ECE
itself is an exPected error over an unseen i.i.d. test-Point (X, Y) „ P.
Experimental details. We exPerimented on the CIFAR-10 and CIFAR-100 datasets, which have
10 and 100 classes each. The base models are deeP-nets with the following architectures: ResNet-
50, Resnet-110, Wide-ResNet-26-10 (WRN) (Zagoruyko and Komodakis, 2016), and DenseNet-
121 (Huang et al., 2017). Both CIFAR datasets consist of 60K (60,000) Points, which are sPlit as
45K/5K/10K to form the train/validation/test sets. The validation set was used for Post-hoc calibra-
tion and the test set was used for evaluation through ECE/MCE estimates. Instead of training new
models, we used the Pre-trained models of Mukhoti et al. (2020). We then ask: “which post-hoc
calibrator improves the calibration the most?” We used their Brier score and focal loss models in
our exPeriments (Mukhoti et al. (2020) rePort that these are the emPirically best Performing loss
functions). All results in the main paper are with Brier score, and results with focal loss are in
Appendix E.4. ImPlementation details for TS, VS, and DS are in APPendix E.
Findings. In Table 2, we rePort the binned ECE and MCE estimates when B “ 15 bins are used by
HB, and for ECE estimation. We make the following observations:
(a)	For TL-ECE, N-HB is the best Performing method for both CIFAR-10 and CIFAR-100. While
most methods Perform similarly across architectures for CIFAR-10, there is high variation in
CIFAR-100. DS is the worst Performing method on CIFAR-100, but TL-HB also Performs
Poorly. We believe that this could be because the data sPlitting scheme of the TL-calibrator (line
4 of Algorithm 2) sPlits datasets across the Predicted classes, and some classes in CIFAR-100
occur very rarely. This is further discussed in APPendix E.6.
(b)	For TL-MCE, TL-HB is the best Performing method on CIFAR-10, by a huge margin. For
CIFAR-100, TS or VS Perform slightly better than TL-HB. Since HB ensures that each bin gets
roughly the same number of Points, the Predictions are well calibrated across bins, leading to
smaller TL-MCE. A similar observation was also made by GuPta and Ramdas (2021).
(c)	For CW-ECE, CW-HB is the best Performing method across the two datasets and all four ar-
chitectures. The N-HB method which has been used in many CW-ECE baseline exPeriments
Performs terribly. In other words, skiPPing the normalization steP leads to a large imProvement
in CW-ECE. This observation is one of our most striking findings. To shed further light on
this, we note that the distribution-free calibration guarantees for CW-HB shown in APPendix C
no longer hold Post-normalization. Thus, both our theory and exPeriments indicate that skiPPing
normalization imProves CW-ECE Performance.
Additional experiments in the Appendix. In APPendix E.5, we rePort each of the results in Ta-
bles 2 and 3 with the number of bins taking every value in the range r5, 25s. Most observations
remain the same under this exPanded study. In APPendix B.2, we consider toP-label calibration for
the class imbalanced COVTYPE-7 dataset, and show that TL-HB adaPts to tail/infrequent classes.
5	Conclusion
We make two contributions to the study of multiclass calibration: (i) defining the new notion of
toP-label calibration which enforces a natural minimal requirement on a multiclass Predictor—the
Probability score for the toP class Prediction should be calibrated; (ii) develoPing a multiclass-to-
binary (M2B) framework which Posits that various notions of multiclass calibration can be achieved
via reduction to binary calibration, balancing Practical utility with statistically tractability. Since it
is imPortant to identify aPProPriate notions of calibration in any structured outPut sPace (Kuleshov
et al., 2018; Gneiting et al., 2007), we anticiPate that the PhilosoPhy behind the M2B framework
could find aPPlications in other structured sPaces.
9
Published as a conference paper at ICLR 2022
6	Reproducibility statement
Some reproducibility desiderata, such as external code and libraries that were used are summarized
in Appendix E.1. All code to generate results with the CIFAR datasets is attached in the supple-
mentary material. Our base models were pre-trained deep-net models generated by Mukhoti et al.
(2020), obtained from www.robots.ox.ac.uk/〜Viveka/focal_calibration/ (corre-
sponding to ‘brier_score, and 'focal」oss_adaptive_53' at the above link). By avoiding training of
new deep-net models with multiple hyperparameters, we also consequently avoided selection biases
that inevitably creep in due to test-data-peeking. The predictions of the pre-trained models were
obtained using the code at https://github.com/torrvision/focal_calibration.
7	Ethics statement
Post-hoc calibration is a post-processing step that can be applied on top of miscalibrated machine
learning models to increase their reliability. As such, we believe our work should improve the
transparency and explainability of machine learning models. However, we outline a few limita-
tions. Post-hoc calibration requires keeping aside a fresh, representative dataset, that was not used
for training. If this dataset is too small, the resulting calibration guarantee can be too weak to be
meaningful in practice. Further, if the test data distribution shifts in significant ways, additional
corrections may be needed to recalibrate (Gupta et al., 2020; Podkopaev and Ramdas, 2021). A
well calibrated classifier is not necessarily an accurate or a fair one, and vice versa (Kleinberg et al.,
2017). Deploying calibrated models in critical applications like medicine, criminal law, banking,
etc. does not preclude the possibility of the model being frequently wrong or unfair.
Acknowledgements
This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is
supported by National Science Foundation grant number ACI-1548562 (Towns et al., 2014). Specif-
ically, it used the Bridges-2 system, which is supported by NSF award number ACI-1928147, at the
Pittsburgh Supercomputing Center (PSC). CG’s research was supported by the generous Bloomberg
Data Science Ph.D. Fellowship. CG would like to thank Saurabh Garg and Youngseog Chung for
interesting discussions, and Viveka Kulharia for help with the focal calibration repository. Finally,
we thank Zack Lipton, the ICLR reviewers, and the ICLR area chair, for excellent feedback that
helped improve the writing of the paper.
References
Jock A Blackard and Denis J Dean. Comparative accuracies of artificial neural networks and dis-
criminant analysis in predicting forest cover types from cartographic variables. Computers and
electronics in agriculture, 24(3):131-151, 1999.
Leo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and
regression trees. Routledge, 2017.
Luc Devroye. The equivalence of weak, strong and complete convergence in L1 for kernel density
estimates. The Annals of Statistics, 11(3):896-904, 1983.
Luc Devroye. Automatic pattern recognition: A study of the probability of error. IEEE Transactions
on pattern analysis and machine intelligence, 10(4):530-543, 1988.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):
243-268, 2007.
Louis Gordon and Richard A Olshen. Almost surely consistent nonparametric regression from
recursive partitioning schemes. Journal of Multivariate Analysis, 15(2):147-163, 1984.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, 2017.
10
Published as a conference paper at ICLR 2022
Chirag Gupta and Aaditya Ramdas. Distribution-free calibration guarantees for histogram binning
without sample splitting. In International Conference on Machine Learning, 2021.
Chirag Gupta, Aleksandr Podkopaev, and Aaditya Ramdas. Distribution-free binary classification:
prediction sets, confidence intervals and calibration. In Advances in Neural Information Process-
ing Systems, 2020.
Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu,
and Richard Hartley. Calibration of neural networks using splines. In International Conference
on Learning Representations, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. In Innovations in Theoretical Computer Science, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, Univer-
sity of Toronto, 2009.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning, 2018.
Meelis Kull, Telmo M. Silva Filho, and Peter Flach. Beyond sigmoids: How to obtain well-
calibrated probabilities from binary classifiers with beta calibration. Electronic Journal of Statis-
tics,11(2):5052-5080, 2017.
Meelis Kull, MiqUel Perello-Nieto, Markus Kangsepp, Hao Song, and Peter Flach. Beyond tem-
perature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration. In
Advances in Neural Information Processing Systems, 2019.
Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in
Neural Information Processing Systems, 2019.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, 2018.
Gabor Lugosi and Andrew Nobel. Consistency of data-driven histogram methods for density esti-
mation and classification. Annals of Statistics, 24(2):687-706, 1996.
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K
Dokania. Calibrating deep neural networks using focal loss. In Advances in Neural Information
Processing Systems, 2020.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In International Conference on Machine Learning, 2005.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-
ing calibration in deep learning. arXiv preprint arXiv:1904.01685, 2020.
Andrew Nobel. Histogram regression estimation using data-dependent partitions. The Annals of
Statistics, 24(3):1084-1105, 1996.
Kanil Patel, William Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty
calibration via mutual information maximization-based binning. In International Conference on
Learning Representations, 2021.
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classifiers, pages 61-74. MIT Press, 1999.
11
Published as a conference paper at ICLR 2022
Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classi-
fication under label shift. In Uncertainty in Artificial Intelligence, 2021.
Jian Qian, Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Concentration inequalities for
multinoulli random variables. arXiv preprint arXiv:2001.11595, 2020.
Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-
preserving functions for calibration of multi-class neural networks. In Advances in Neural Infor-
mation Processing Systems, 2020.
Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C Mozer. Mitigating bias in cali-
bration error estimation. arXiv preprint arXiv:2012.08668, 2020.
J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop,
D. Lifka, G. D. Peterson, R. Roskies, J. Scott, and N. Wilkins-Diehr. XSEDE: Accelerating
Scientific Discovery. Computing in Science & Engineering, 16(5):62-74, 2014.
Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and
Thomas B Schon. Evaluating model calibration in classification. In International Conference
on Artificial Intelligence and Statistics, 2019.
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. In-
equalities for the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
2003.
David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classifica-
tion: a unifying framework. In Advances in Neural Information Processing Systems, 2019.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive Bayesian classifiers. In International Conference on Machine Learning, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In International Conference on Knowledge Discovery and Data Mining, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference, 2016.
Jize Zhang, Bhavya Kailkhura, and T Han. Mix-n-match: Ensemble and compositional methods for
uncertainty calibration in deep learning. In International Conference on Machine Learning, 2020.
12
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
1
2
3
4
5
6
7
8
9
Algorithm 5: Post-hoc calibrator for a given M2B calibration notion C
Input: Base (uncalibrated) multiclass predictor g, calibration data
D “ pX1, Y1q, . . . , pXn, Ynq, binary calibrator
At。,” ： [0,1SX X(X ^{0,1})* →[0,1SX
K D number of distinct calibration claims that C verifies;
for each claim k P rKs do
From g, infer (r, r) D (label-predictor, ProbabiIity-PrediCtor) corresponding to claim k;
Dk D {(Xi,Zi)},where Zi Dl {匕 “ r(X,)};〜
if conditioning does not include class prediction cr then
—(confidence, top-K-confidence, and class-wise calibration) —
hk D A{0,1}(r, Dk);
end
else
— (top-label and top-K-label calibration) —
for l P rLs do
Dk,ι D{(Xi,4)p Dk ：况Xiq = l)U;
hk,l D At0,1}(r, Dk,l);
end
hk(∙) D hk,rp∙)(∙) (hk predicts hk,ι (x) if r(x) = l);
end
end
— (the new predictor replaces each gr with the corresponding hk) —
return (label-predictor, hk) corresponding to each claim k P rKs;
Input for Algorithms 6 and 7: base multiclass predictor g : X → ∆L11, calibration data D “
(Xi, Yι),..., (Xn, Yn), binary calibrator A{o,i} ： [0, 1]x X(X X {0,1})* → [0, 1]x .
Algorithm 6: Top-K-label calibrator For every k P [Ks, infer from g the k-th			
larg asso for k f e h	est class predictor cpkq and the ciated probability gpkq ; D 1 to K do )r l D 1 to L do Dk,ι -{(Xi,1 {匕“l}): cpkq (Xi) “ lu; hpk,lq D A{0,i}(gpkq,Dk,1); nd Pk) D h(k,cpkq(∙))(.);	A 1F 2f 3 4 5e 6r	Jgorithm 7: Top-K-confidence calibrator or every k P [Ks, infer from g the k-th largest class predictor c(k) and the associated probability g(k) ; )r k D 1 to K do Dk -{(Xi,l {匕=l}) : i p[n]}; h(k) D A{0,i}(gpk), Dk); nd eturn (h(1), h(2), . . . , h(K));
end return (hp1q, hp2q, . . . , hpKq);			
A	Addendum to Section 3 “Calibration algorithms from
calibration metrics”
In Section 3, we introduced the concept of M2B calibration, and showed that popular calibration
notions are in fact M2B notions (Table 1). We showed how the calibration notions of top-label,
class-wise, and confidence calibration can be achieved using a corresponding M2B calibrator. In
the following subsection, we present the general-purpose wrapper Algorithm 5 that can be used to
derive an M2B calibrator from any given M2B calibration notion that follows the rubric specified by
Table 1. In Appendix A.2, we illustrate the philosophy of M2B calibration using a simple example
with a dataset that contains 6 points. This example also illustrates the top-label-calibrator, the class-
wise-calibrator, and the confidence-calibrator.
13
Published as a conference paper at ICLR 2022
(a) Predictions of a fixed base model
g : X → ∆3 on Calibration/test data
D“tpa,3q,pb,4q,...,pf,1qu.
(C) Top-label Calibration
(d) Class-wise Calibration
(b) ConfidenCe Calibration
(e) CanoniCal Calibration
Figure 3: Illustrative example for SeCtion A.2. The numbers in plot (a) Correspond to the prediCtions
made by g on a dataset D. If D were a test set, plots (b-e) show how it should be used to verify if g
satisfies the Corresponding notion of Calibration. Consequently, we argue that if D were a Calibration
set, and we want to achieve one of the notions (b-e), then the data shown in the corresponding plots
should be the data used to Calibrate g as well.
A.1 General-purpose M2B calibrator
Denote some M2B notion of calibration as C. Suppose C corresponds to K binary calibration claims.
The outer for-loop in Algorithm 5, runs over each such claim in C . For example, for class-wise
calibration, K “ L and for confidence and top-label calibration, K “ 1. Corresponding to each
claim, there is a probability-predictor that the conditioning is to be done on, such as g or gl or gpkq .
Additionally, there may be conditioning on the label predictor such as c or cpkq . These are denoted
as prc, grq in Algorithm 5. For confidence and top-label calibration, cr “ c, the top-label-confidence.
For class-wise calibration, when r “ gl, we have r(∙) “ l.
If there is no label conditioning in the calibration notion, such as in confidence, top-K-confidence,
and class-wise calibration, then we enter the if-condition inside the for-loop. Here hk is learnt using
a single calibration dataset and a single call to At0,1u. Otherwise, if there is label conditioning, such
as in top-label and top-K-label calibration, we enter the else-condition, where we learn a separate
hk,l for every l P rLs, using a different part of the dataset Dl in each case. Then hk pxq equals
hk,lpxq if crpxq “ l.
Finally, since C is verifying a sequence of claims, the output of Algorithm 5 is a sequence of pre-
dictors. Each original prediction prc, grq corresponding to the C is replaced with pcr, hkq. This is the
output of the M2B calibrator. Note that the cr values are not changed. This output appears abstract,
but normally, it can be represented in an interpretable way. For example, for class-wise calibration,
the output is just a sequence of predictors, one for each class: ph1 , h2, . . . , hLq.
This general-purpose M2B calibrators can be used to achieve any M2B calibration notion: top-
label calibration (Algorithm 2), class-wise calibration (Algorithm 3), confidence calibration (Algo-
rithm 1), top-K-label calibration (Algorithm 6), and top-K-confidence calibration (Algorithm 7).
A.2 An example to illustrate the philosophy of M2B calibration
Figure 3a shows the predictions of a given base model g on a given dataset D. Suppose D is a
test set, and we are testing confidence calibration. Then the only predictions that matter are the
top-predictions corresponding to the shaded values. These are stripped out and shown in Figure 3b,
in the g(•) row. Note that the indicator 1 {Y “ c(∙)} is sufficient to test confidence calibration and
given this, the cpXq are not needed. Thus the second row in Figure 3b only shows these indicators.
14
Published as a conference paper at ICLR 2022
Algorithm 8: Top-label histogram binning
Input: Base multiclass predictor g, calibration data D “ pX1 , Y1q, . . . , pXn, Ynq
Hyperparameter: # points per bin k P N (Say 50), tie-breaking parameter δ > 0 (Say lθ´10)
Output: Top-label calibrated predictor pc, hq
ι C D classifier or top-class based on g;
2	g D top-class-probability based on g;
3	for l D 1 to L do
4	Dl -{(Xi, 1 {匕 “ l}) : C(Xi)“ iqU and n D |R|;
5	hl D Binary-histogram-binning(g, Dl, [nl/k], δ);
6	end
7	h(∙) D hc(.)(∙);
8	return (C, h); * B
Verifying top-label calibration is similar (Figure 3c), but in addition to the predictions g(∙), We also
retain the values of c(∙). Thus the g(∙) and ItY “ c(∙)} are shown, but split across the 4 classes.
Class-Wise calibration requires access to all the predictions, hoWever, each class is considered sep-
arately as indicated by Figure 3d. Canonical calibration looks at the full prediction vector in each
case. However, in doing so, it becomes unlikely that g(x) “ g(y) for any x, y since the number of
values that g can take is now exponential.
Let us turn this around and suppose that D were a calibration set instead of a test set. We argue that D
should be used in the same way, whether testing or calibrating. Thus, if confidence calibration is to
be achieved, we should focus on the (g, ItY “ c(∙)}) corresponding to g. If top-label calibration is
to be achieved, we should use the (C, g) values. If class-wise calibration is to be achieved, we should
look at each gl separately and solve L different problems. Finally, for canonical calibration, we must
look at the entire g vector as a single unit. This is the core philosophy behind M2B calibrators: if
binary claims are being verified, solve binary calibration problems.
B Distribution-free top-label calibration using histogram
B INNING
In this section, we formally describe histogram binning (HB) with the top-label-calibrator (Algo-
rithm 2) and provide methodological insights through theory and experiments.
B.1 Formal algorithm and theoretical guarantees
Algorithm 8 describes the top-label calibrator formally using HB as the binary calibration algorithm.
The function called in line 5 is Algorithm 2 of Gupta and Ramdas (2021). The first argument in the
call is the top-label confidence predictor, the second argument is the dataset to be used, the third
argument is the number of bins to be used, and the fourth argument is a tie-breaking parameter
(described shortly). While previous empirical works on HB fixed the number of bins per class, the
analysis of Gupta and Ramdas (2021) suggests that a more principled way of choosing the number
of bins is to fix the number of points per bin. This is parameter k of Algorithm 8. Given k, the
number of bins is decided separately for every class as tnl{ku where nl is the number of points
predicted as class l. This choice is particularly relevant for top-label calibration since nl can be
highly non-uniform (we illustrate this empirically in Section B.2). The tie-breaking parameter δ
can be arbitrarily small (like lθ´10), and its significance is mostly theoretical—it is used to ensure
that outputs of different bins are not exactly identical by chance, so that conditioning on a calibrated
probability output is equivalent to conditioning on a bin; this leads to a cleaner theoretical guarantee.
HB recalibrates g to a piecewise constant function h that takes one value per bin. Consider a specific
bin b; the h value for this bin is computed as the average of the indicators {1 {Yi “ c(Xi)} : Xi P
Bin bu. This is an estimate of the bias of the bin P(Y “ C(X) | X P Bin b). A concentration
inequality can then be used to bound the deviation between the estimate and the true bias to prove
distribution-free calibration guarantees. In the forthcoming Theorem 1, we show high-probability
and in-expectation bounds on the the TL-ECE of HB. Additionally, we show marginal and condi-
15
Published as a conference paper at ICLR 2022
tional top-label calibration bounds, defined next. These notions were proposed in the binary cal-
ibration setting by Gupta et al. (2020) and Gupta and Ramdas (2021). In the definition below, A
refers to any algorithm that takes as input calibration data D and an initial classifier g to produce a
top-label predictor c and an associated probability map h. Algorithm 8 is an example of A.
Definition 1 (Marginal and conditional top-label calibration). Let ε, α P p0, 1q be some given levels
of approximation and failure respectively. An algorithm A : (g, D) → (c, h) is
(a)	pε, α)-marginally top-label calibrated if for every distribution P over X X [L],
P´ |P(Y “ C(X) | C(X), hpx)) ´ hpx)| ≤ ε) > 1 ´ α.	(8)
(b)	(ε, α)-conditionally top-label calibrated if for every distribution P over X X [L],
P(@ l P [L],r P Range(h), |P(Y = c(X) | c(X) = l, h(X) = r) ´ r| W ε) > 1 ´ α.
(9)
To clarify, all probabilities are taken over the test point (X, Y) „ P, the calibration data D „ Pn,
and any other inherent algorithmic randomness in A; these are all implicit in (C, h) = A(D, g).
Marginal calibration asserts that with high probability, on average over the distribution of D , X ,
P(Y = C(X) | C(X), h(X )) is at most ε away from h(X). In comparison, TL-ECE is the average of
these deviations over X. Marginal calibration may be a more appropriate metric for calibration than
TL-ECE if we are somewhat agnostic to probabilistic errors less than some fixed threshold ε (like
0.05). Conditional calibration is a strictly stronger definition that requires the deviation to be at most
ε for every possible prediction (l, r), including rare ones, not just on average over predictions. This
may be relevant in medical settings where we want the prediction on every patient to be reasonably
calibrated. Algorithm 8 satisfies the following calibration guarantees.
Theorem 1. Fix hyperparameters δ > 0 (arbitrarily small) and points per bin k22, and assume
nι》k for every l P rLS. Then, for any a P (0,1), Algorithm 8 is (ει,α)-marginally and (ε2,α)-
conditionally top-label calibrated for
∕log(2{α)	∕log(2n{ka)
ε1 “弋丽F +δ,	and ε2 = V 2(k ´ 1) +δ∙	(IO)
Further, for any distribution P over X X [L], we have P (TL-ECE(c, h) W ε2)21 — α, and
E rTL-ECE(c, h)S W ai72k + δ.
The proof in Appendix H is a multiclass top-label adaption of the guarantee in the binary setting by
Gupta and Ramdas (2021). The O(1∕ʌ/k) dependence of the bound relies on Algorithm 8 delegating
at least k points to every bin. Since δ can be chosen to be arbitrarily small, setting k = 50 gives
roughly ED rTL-ECE(h)s W 0.1. Base on this, we suggest setting k P r50, 150s in practice.
B.2 Top-label histogram binning adapts to class imbalanced datasets
The principled methodology of fixing the number of points per bin reaps practical benefits. Fig-
ure 4 illustrates this through the performance of HB for the class imbalanced COVTYPE-7 dataset
(Blackard and Dean, 1999) with class ratio approximately 36% for class 1 and 49% for class 2. The
entire dataset has 581012 points which is divided into train-test in the ratio 70:30. Then, 10% of the
training points are held out for calibration (n = |D| = 40671). The base classifier is a random forest
(RF) trained on the remaining training points (it achieves around 95% test accuracy). The RF is then
recalibrated using HB. The top-label reliability diagrams in Figure 4a illustrate that the original RF
(in orange) is underconfident on both the most likely and least likely classes. Additional figures in
Appendix F show that the RF is always underconfident no matter which class is predicted as the
top-label. HB (in green) recalibrates the RF effectively across all classes. Validity plots (Gupta and
Ramdas, 2021) estimate how the LHS of condition (8), denoted as V (ε), varies with ε. We observe
that for all ε, V (ε) is higher for HB. The rightmost barplot compares the estimated TL-ECE for all
classes, and also shows the class proportions. While the original RF is significantly miscalibrated for
16
Published as a conference paper at ICLR 2022
Random forest Histogram binning
Class 2 reliability diagram
Class 2 validity plot	Class 4 reliability dlaɑrsm	Class 4 validity plot
Predicted probability
Class ratio
0≈2 Sse0
(a)	Top-label histogram binning (Algorithm 8) With k “ 100 points per bin. Class 4 has only 183 calibration
points. Algorithm 8 adapts and uses only a single bin to ensure that the TL-ECE on class 4 is comparable to the
TL-ECE on class 2. Overall, the random forest classifier has significantly higher TL-ECE for the least likely
classes (4, 5, and 6), but the post-calibration TL-ECE using binning is quite uniform.
Class 2 reliability diagram
Class 2 validity plot
o.o-
0.00 0.25 0.50 0.75 1.00
Predicted probability
0.00	0.05 O.IO 0Λ5
Class 4 reliability diagram
Class 4 validity plot
0.175
Ul 0.150
% 0.125
V
∙β 0.100
A0.075
l-0.050
0.025
0.000
Class
Z
9
O≈E""el。
(b)	Histogram binning with B “ 50 bins for every class. Compared to Figure 4a, the post-calibration TL-ECE
for the most likely classes decreases while the TL-ECE for the least likely classes increases.
Figure 4: Recalibration of a random forest using histogram binning on the class imbalanced
COVTYPE-7 dataset (class 2 is roughly 100 times likelier than class 4). By ensuring a fixed num-
ber of calibration points per bin, Algorithm 8 obtains relatively uniform top-label calibration across
classes (Figure 4a). In comparison, if a fixed number of bins are chosen for all classes, the perfor-
mance deteriorates for the least likely classes (Figure 4b).
the less likely classes, HB has a more uniform miscalibration across classes. Figure 4b considers a
slightly different HB algorithm where the number of points per class is not adapted to the number of
times the class is predicted, but is fixed beforehand (this corresponds to replacing tnl{ku in line 5 of
Algorithm 8 with a fixed B P N). While even in this setting there is a drop in the TL-ECE compared
to the RF model, the final profile is less uniform compared to fixing the number of points per bin.
The validity plots and top-label reliability diagrams for all the 7 classes are reported in Figure 9 in
Appendix F, along with some additional observations.
C Distribution-free class-wise calibration using histogram
B INNING
In this section, we formally describe histogram binning (HB) with the class-wise-calibrator (Algo-
rithm 3) and provide theoretical guarantees for it. The overall procedure is called class-wise-HB.
Further details and background on HB are contained in Appendix B, where top-label-HB is de-
scribed.
C.1 Formal algorithm
To achieve class-wise calibration using binary routines, we learn each component function hl in a 1-
v-all fashion as described in Algorithm 3. Algorithm 9 contains the pseudocode with the underlying
routine as binary HB. To learn hl, we use a dataset Dl, which unlike top-label HB (Algorithm 8),
contains Xi even if C(Xi) ‰ l. However the 匕 is replaced with 1 {Yi “ l}. The number of points
per bin kl can be different for different classes, but generally one would set k1 “ . . . “ kL “ k P N.
Larger values of kl will lead to smaller εl and δl in the guarantees, at loss of sharpness since the
number of bins tn{klu would be smaller.
17
Published as a conference paper at ICLR 2022
Algorithm 9: Class-wise histogram binning
Input: Base multiclass predictor g : X → ∆L1* 1, calibration data D “ (X1,Y1),..., (Xn, Yn)
Hyperparameter: # points per bin k1 , k2 , . . . , kl P NL (say each kl “ 50), tie-breaking
parameter δ > 0 (Say lθ´10)
Output: L class-wise calibrated predictors h1, h2, . . . , hL
ι for l D 1 to L do
2	Dl -{(Xi,l {匕 “ l}) : i P[n])};
3	hl D Binary-histogram-binning(gl, Dl, [n/kl1, δ);
4	end
5	return (h1, h2, . . . , hLq;
C.2 Calibration guarantees
A general algorithm A for class-wise calibration takes as input calibration data D and an initial
classifier g to produce an approximately class-wise calibrated predictor h : X → r0, 1sL. Define
the notation ε “ (ε1, ε2, . . . , εLq P (0, 1qL and α “ (α1, α2, . . . , αLq P (0, 1qL.
Definition 2 (Marginal and conditional class-wise calibration). Let ε, α P (0, 1qL be some given
levels of approximation and failure respectively. An algorithm A : (g, D) → h is
(a) (ε, αq-marginally class-wise calibrated if for every distribution P over X X [L] and for every
l P rLs
p(∣p(Y = 11 hlpχqq ´ hlpχq∣ ≤ εl) N ι ´ αl.	(ii)
(b) (ε, αq-conditionally class-wise calibrated if for every distribution P over X X [L] and for every
l P rLs,
P(@r p Range(hl), |P(Y = l | hl(X) = r) — r| ≤ εl) N 1 — αl.	(12)
Definition 2 requires that each hl is (εl, αlq calibrated in the binary senses defined by Gupta et al.
(2021, Definitions 1 and 2). From Definition 2, we can also uniform bounds that hold simultaneously
over every l P rLs. Let α = Σ匕1 αl and ε = maxlPrLs εl. Then (11) implies
P(@l P 固,|P(Y = l | hl(X)) ´ hl(X)| ≤ ε) N 1 — α,	(13)
and (12) implies
P (@l p rLS,r p Range(hl), |P(Y = l | hl(X) = r) — r| ≤ ε) N 1 — α.	(14)
The choice of not including the uniformity over L in Definition 2 reveals the nature of our class-wise
HB algorithm and the upcoming theoretical guarantees: (a) we learn the hl’s separately for each l
and do not combine the learnt functions in any way (such as normalization), (b) we do not combine
the calibration inequalities for different rLs in any other way other than a union bound. Thus the
only way we can show (13) (or (14)) is by using a union bound over (11) (or (12)).
We now state the distribution-free calibration guarantees satisfied by Algorithm 9.
Theorem 2. Fix hyperparameters δ > 0 (arbitrarily small) and points per bin k1,k2,...,kl N 2,
and assume nl N kl for every l p rLs. Then, for every l p rLs, for any αl p (0, 1), Algorithm 9 is
(εp1q, α)-marginally and (εp2q, α)-conditionally class-wise calibrated with
pιq	∕log(2/al)	(2)	∕log(2n/klαl)
εl =V2W—I7 + δ,	and	εl	=v 2(kl — 1)	+δ∙	(15)
Further, for any distribution P over X X [L],
(a) P(CW-ECE(c, h) ≤ maxlp[L] εp2q) N 1 — ∑lp[L] αl, and
(b) E rCW-ECE(c, h)S ≤ maxlp[Ls √1/W + δ.
18
Published as a conference paper at ICLR 2022
Theorem 2 is proved in Appendix H. The proof follows by using the result of Gupta and Ramdas
(2021, Theorem 2), derived in the binary calibration setting, for each hl separately. Gupta and
Ramdas (2021) proved a more general result for general 'p-ECE bounds. Similar results can also be
derived for the suitably defined `p -CW-ECE.
As discussed in Section 3.2, unlike previous works (Zadrozny and Elkan, 2002; Guo et al., 2017;
Kull et al., 2019), Algorithm 9 does not normalize the hl’s. We do not know how to derive Theorem 2
style results for a normalized version of Algorithm 9.
D Figures for Appendix E
Appendix E begins on page 23. The relevant figures for Appendix E are displayed on the following
pages.
19
Published as a conference paper at ICLR 2022
---- Base model	---- Tbp-IabeI-HB -------- Vector scaling
----"Temperature scaling ------- NormaIized-HB ------- Dirichlet scaling
(a)	TL-ECE estimates on CIFAR-10 with Brier score. TL-HB is close to the best in each case. While CW-HB
performs the best at B “ 15, the ECE estimate may not be reliable since it is highly variable across bins.
ResNet-50	ResNet-IlO	Wide-ReSNet-2 6-1。	DenSeNet-121
10	15	20	25
Number of bins
wɔw PB"OJ=*W
0.10-
5	10	15	20	25
Numberofbins
wɔw PB"OJ=*W
0.10-
5	10	15	20	25
Number Of bins
5	10	15	20	25
Number Of bins
(b)	TL-ECE estimates on CIFAR-100 with Brier score. N-HB is the best performing method, while DS is the
worst performing method, across different numbers of bins. TL-HB performs worse than TS and VS.
(c) TL-MCE estimates on CIFAR-10 with Brier score.
method is TL-HB.
The only reliably and consistently well-performing
(d) TL-MCE estimates on CIFAR-100 with Brier score. DS is the worst performing method. Other methods
perform across different values of B.
Figure 5: Table 2 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 for
further details. The captions summarize the findings in each case. In most cases, the findings are
similar to those with B “ 15. The notable exception is that performance of N-HB on CIFAR-10 for
TL-ECE while very good at B “ 15, is quite inconsistent when seen across different bins. In some
cases, the blue base model line and the orange temperature scaling line coincide. This occurs since
the optimal temperature on the calibration data was learnt to be T “ 1, which corresponds to not
changing the base model at all.
20
Published as a conference paper at ICLR 2022
---- Base model	---- Tbp-IabeI-HB -------- Vector scaling
----Temperature scaling -------- NormaIized-HB ------- Dirichlet scaling
Wide-ResNet-26-10	DenSeNet-121
ResNet-50
Wuw p∂*jeE
0.040-
0.035-
0.030-
10
15
20
Numberofblns
Wuw p∂sEnSqj
0.045
0.040
ResNet-IlO
B 0.030
D.O20
II
0.040
0.020
9 0.035
0.015-
Numberofblns
Number of blns
Number of blns


(a) TL-ECE estimates on CIFAR-10 with focal loss. TL-HB is close to the best in each case. While CW-HB
performs the best at B “ 15, the ECE estimate may not be reliable since it is highly variable across bins.
ResNet-50
ResNet-IlO
5	10	15	20	25
Number of bins
Wide-ResNet-26-10
wɔw SmE-S
Dense Net-121

(b) TL-ECE estimates on
worst performing method,
ResNet-50
CIFAR-100 with focal loss. N-HB is the best performing method, while DS is the
across different numbers of bins. TL-HB performs worse than TS and VS.
ResNet-IlO	Wide-ReSNet-2 &1。	DenseNet-121
0.8
0.6
0.4
0.2-
Numberof bins
0.7-
0.6
2 0-5
20.4-
∙∣ 0.3-
0.2-
0.1-
o.o∙
5	10	15	20	25
Numberofbins
ujw pa"E=suj
5	10	15	20	25
Numberof bins
ujw pa"E=suj
5	10	15	20	25
Numberof bins

(c) TL-MCE estimates on CIFAR-10 with focal loss.
method is TL-HB.
The only reliably and consistently well-performing
ResNet-50	ResNet-IlO
Wide-ResNet-26-10	DenseNet-121
0.7-
0.6
0.5
ujz>ujpfieE=suj
5	10	15	20	25
Numberof bins
0.9
0.8
ιu 0.7
U
匕0.6
B
3 0.5
S O-4
0.3
0.2
5	10	15	20	25
Numberofbins
5	10	15	20	25
Numberof bins
ujuujpfieE=suj
5	10	15	20	25
Numberof bins
(d) TL-MCE estimates on CIFAR-100 with focal loss. DS is the worst performing method. Other methods
perform across different values of B.
Figure 6: Table 4 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 for
further details. The captions summarize the findings in each case. In most cases, the findings are
similar to those with B “ 15. In some cases, the blue base model line and the orange temperature
scaling line coincide. This occurs since the optimal temperature on the calibration data was learnt
to be T “ 1, which corresponds to not changing the base model at all.
21
Published as a conference paper at ICLR 2022
----Base model	---- Class-wise-HB -------- Vector scaling
----Temperature scaling ------- NormaIized-HB -------- Dirichlet scaling
(a)	CW-ECE estimates on CIFAR-10 with Brier score. CW-HB is the best performing method across bins, and
N-HB is quite unreliable.
ResNet-50	ResNeMlO	Wlde-Res Net-2 6-10	DenSeNet-121
0.007
0.006
UJ 0.005-
2
» 0.004-
用 0.003
0.002
0.001-.	.	.	.
5	10	15	20	25
Numberofblns
0.006 -
0.005■
0.004 ■
0.003
0.002
0.001
mυw p∂sEAS3
5	10	15	20	25
Number of blns
0.006
0.005
0.004
0.003
0.002
0.001
mυw p∂sEAS3
5	10	15	20
Number of blns
25

(b)	CW-ECE estimates on CIFAR-100 with Brier score. CW-HB is the best performing method. DS and N-HB
are the worst performing methods.
ResNet-50
O-OOB
0.007
0.006
0.005
0.004
0.003
0.002
0.009
5	10	15	20	25
Numberofbins
Wuw p∂sEnSqj
0.010-
0.010-
Figure 7: Table 3 style results with the number of bins varied as B P r5, 25s. The captions summarize
the findings in each case, which are consistent with those in the table. See Appendix E.5 for further
details.
ResNet-IlO
Wlde-Res Net-2 6-10
O.OOB
O-OOB-
io □.□□E
0.006
□ .□04
0.004
0.002
5	10	15	20	25
Number of blns
5	10	15	20	25
Numberofblns
DenSeNet-121
5	10	15	20	25
Number of blns
(a)	CW-ECE estimates on CIFAR-10 with focal loss. CW-HB is the best performing method across bins, and
N-HB is quite unreliable.
ReSNet-5。	ResNeMlO	Wlde-Res Net-2 6-10	DenSeNet-121
0.006
5 0.005
S 0.004
E
« 0.003
0.002
0.001-
5 io 15	20	2⅛
Numberofbins
0.006 -
0.005■
0.004 ■
0.003
0.002
0.001

(b)	CW-ECE estimates on CIFAR-100 with focal loss. CW-HB is the best performing method. DS and N-HB
are the worst performing methods.
Figure 8:	Table 5 style results with the number of bins varied as B P r5, 25s. The captions summarize
the findings in each case, which are consistent with those in the table. See Appendix E.5 for further
details.
22
Published as a conference paper at ICLR 2022
E Additional experimental details and results for CIFAR- 1 0 and
CIFAR- 1 00
We present additional details and results to supplement the experiments with CIFAR-10 and CIFAR-
100 in Sections 2 and 4 of the main paper.
E.1 External libraries used
All our base models were pre-trained deep-net models generated by Mukhoti et al. (2020), obtained
from www.robots.ox.ac.uk/〜Viveka/focal_calibration/ and used along with the
code at https://github.com/torrvision/focal_calibration to obtain base predic-
tions. We focused on the models trained with Brier score and focal loss, since it was found to perform
the best for calibration. All reports in the main paper are with the Brier score; in Appendix E.4, we
report corresponding results with focal loss.
We also used the code at https://github.com/torrvision/focal_calibration for
temperature scaling (TS). For vector scaling (VS) and Dirichlet scaling (DS), we used the code of
Kull et al. (2019), hosted at https://github.com/dirichletcal/dirichlet-python.
For VS, we used the file dirichletcal/calib/vectorscaling.py, and for DS, we used
the file dirichletcal/calib/fulldirichlet.py. No hyperparameter tuning was per-
formed in any of our histogram binning experiments or baseline experiments; default settings were
used in every case. The random seed was fixed so that every run of the experiment gives the same
result. In particular, by relying on pre-trained models, we avoid training new deep-net models with
multiple hyperparameters, thus avoiding any selection biases that may arise due to test-data peeking
across multiple settings.
E.2 Further comments on binning for ECE estimation
As mentioned in Remark 1, ECE estimates for all methods except TL-HB and CW-HB was done
using fixed-width bins r0, 1{Bq, r1{B, 2{Bq, . . . r1 ´ 1{B, 1s for various values of B P r5, 25s. For
TL-HB and CW-HB, B is the number of bins used for each call to binary HB. For TL-HB, note that
we actually proposed that the number of bins-per-class should be fixed; see Section B.2. However,
for ease of comparison to other methods, we simply set the number of bins to B for each call to
binary HB. That is, in line 5, we replace tnl{ku with B. For CW-HB, we described Algorithm 9
with different values of kl corresponding to the number of bins per class. For the CIFAR-10 and
CIFAR-100 comparisons, we set each k1 “ k2 “ . . . “ kL “ k, where k P N satisfies tn{ku “ B.
Tables 2,3, 4, and 5 report estimates with B “ 15, which has been commonly used in many works
(Guo et al., 2017; Kull et al., 2019; Mukhoti et al., 2020). Corresponding to each table, we have a
figure where ECE estimates with varying B are reported to strengthen conclusions: these are Fig-
ure 5,7, 6, and 8 respectively. Plugin estimates of the ECE were used, same as Guo et al. (2017).
Further binning was not done for TL-HB and CW-HB since the output is already discrete and suf-
ficiently many points take each of the predicted values. Note that due to Jensen’s inequality, any
further binning will only decrease the ECE estimate (Kumar et al., 2019). Thus, using unbinned
estimates may give TL-HB and CW-HB a disadvantage.
E.3 Some remarks on maximum-calibration-error (MCE)
Guo et al. (2017) defined MCE with respect to confidence calibration, as follows:
conf-MCEpc, hq :“	sup |P pY “ cpXq | hpXq “ rq ´ r| .	(16)
rPRangephq
Conf-MCE suffers from the same issue illustrated in Figure 2 for conf-ECE. In Figure 1b, we looked
at the reliability diagram within two bins. These indicate two of the values over which the supremum
is taken in equation (16): these are the Y-axis distances between the ‹ markers and the X “ Y line
for bins 6 and 10 (both are less than 0.02). On the other hand, the effective maximum miscalibration
for bin 6 is roughly 0.15 (for class 1), and roughly 0.045 (for class 4), and the maximum should be
taken with respect to these values across all bins. To remedy the underestimation of the effective
23
Published as a conference paper at ICLR 2022
Metric	Dataset	Architecture	Base	TS	VS	DS	N-HB	TL-HB
Top- label- ECE	CIFAR-10	-ResNet-50-	0.022	0.023	0.018	0.019	0.023	0.019
		ResNet-110	0.025	0.024	0.022	0.021	0.020	0.020
		WRN-26-10	0.024	0.019	0.016	0.017	0.019	0.018
		DenseNet-121	0.023	0.023	0.021	0.021	0.025	0.021
	CIFAR-100	-ResNet-50-	0.109	0.107	0.107	0.332	0.086	0.148
		ResNet-110	0.124	0.117	0.105	0.316	0.115	0.153
		WRN-26-10	0.100	0.100	0.101	0.293	0.074	0.135
		DenseNet-121	0.106	0.108	0.105	0.312	0.091	0.147
Top- label- MCE	CIFAR-10	-ResNet-50-	0.298	0.443	0.368	0.472	0.325	0.082
		ResNet-110	0.378	0.293	0.750	0.736	0.535	0.089
		WRN-26-10	0.741	0.582	0.311	0.363	0.344	0.075
		DenseNet-121	0.411	0.411	0.243	0.391	0.301	0.099
	CIFAR-100	-ResNet-50-	0.289	0.355	0.234	0.640	0.322	0.273
		ResNet-110	0.293	0.265	0.274	0.633	0.366	0.272
		WRN-26-10	0.251	0.227	0.256	0.663	0.229	0.270
		DenseNet-121	0.237	0.225	0.239	0.597	0.327	0.248
Table 4: Top-label-ECE and top-label-MCE for deep-net models and various post-hoc calibrators.
All methods are same as Table 2. Best performing method in each row is in bold.
MCE, we can consider the top-label-MCE, defined as
TL-MCEpc, hq :“ max sup	|PpY “ l | cpXq “ l, hpXq “ rq ´ r| .	(17)
lPrLs rPRangephq
Interpreted in words, the TL-MCE assesses the maximum deviation between the predicted and true
probabilities across all predictions and all classes. Following the same argument as in the proof of
Proposition 4, it can be shown that for any c, h, Conf-MCE(c, h) ≤ TL-MCE(c, h). The TL-MCE
is closely related to conditional top-label calibration (Definition 1b). Clearly, an algorithm is pε, αq-
conditionally top-label calibrated if and only if for every distribution P, P(TL-MCE(c, hq ≤ ε)》
1 ´ α. Thus the conditional top-label calibration guarantee of Theorem 1 implies a high probability
bound on the TL-MCE as well.
E.4 Table 2 and 3 style results with focal loss
Results for top-label-ECE and top-label-MCE with the base deep net model being trained using
focal loss are reported in Table 4. Corresponding results for class-wise-ECE are reported in Table 5.
The observations are similar to the ones reported for Brier score:
1.	For TL-ECE, TL-HB is either the best or close to the best performing method on CIFAR-
10, but suffers on CIFAR-100. This phenomenon is discussed further in Appendix E.6.
N-HB is the best or close to the best for both CIFAR-10 and CIFAR-100.
2.	For TL-MCE, TL-HB is the best performing method on CIFAR-10, by a huge margin. For
CIFAR-100, TS or VS perform better than TL-HB, but not by a huge margin.
3.	For CW-ECE, CW-HB is the best performing method across the two datasets and all four
architectures.
E.5 ECE and MCE estimates with varying number of bins
Corresponding to each entry in Tables 2 and 4, we perform an ablation study with the number of
bins varying as B P r5, 25s. This is in keeping with the findings of Roelofs et al. (2020) that the
ECE/MCE estimate can vary with different numbers of bins, along with the relative performance of
the various models.
The results are reported in Figure 5 (ablation of Table 2) and Figure 7 (ablation of Table 3). The
captions of these figures contain further details on the findings. Most findings are similar to those
in the main paper, but the findings in the tables are strengthened through this ablation. The same
ablations are performed for focal loss as well. The results are reported in Figure 6 (ablation of
24
Published as a conference paper at ICLR 2022
Metric	Dataset	Architecture	Base	TS	VS	DS	N-HB	CW-HB
Class- wise- ECE ^102	CIFAR-10	-ResNet-50-	0.42	^nɪ		-037-	0.52	-035-
		ResNet-110	0.48	^nɪ	-0:36-	^nɪ	0.51	-029-
		WRN-26-10	0.41	-03T	^0.3T	^nɪ	0.49	-027-
		DenseNet-121	0.41	oqat	-0:40-	-039-	0.63	-030-
	CIFAR-100	-ResNet-50-	0.22	-0:20-	-0:20-	~066~	0.23	-0716-
		ResNet-110	0.24	-0:23-	~22Γ	~02T	0.24	-0716-
		WRN-26-10	0.19	-039-	^038^		0.20	-0714-
		DenseNet-121	0.20	~22Γ	-039-	~666~	0.24	-0716-
Table 5: Class-wise-ECE for deep-net models and various post-hoc calibrators. All methods are
same as Table 2, except top-label-HB is replaced with class-wise-HB or Algorithm 3 (CW-HB).
Best performing method in each row is in bold.
Table 4) and Figure 8 (ablation of Table 5). The captions of these figures contain further details on
the findings. The ablation results in the figures support those in the tables.
E.6 Analyzing the poor performance of TL-HB on CIFAR- 1 00
CIFAR-100 is an imbalanced dataset with 100 classes and 5000 points for validation/calibration
(as per the default splits). Due to random subsampling, the validation split we used had one of
the classes predicted as the top-label only 31 times. Thus, based on Theorem 1, we do not expect
HB to have small TL-ECE. This is confirmed by the empirical results presented in Tables 2/4, and
Figures 5b/6b. We observe that HB has higher estimated TL-ECE than all methods except DS, for
most values of the number of bins. The performance of TL-HB for TL-MCE however is much much
closer to the other methods since HB uses the same number of points per bin, ensuring that the
predictions are somewhat equally calibrated across bins (Figures 5d/6d). In comparison, for CW-
ECE, CW-HB is the best performing method. This is because in the class-wise setting, 5000 points
are available for recalibration irrespective of the class, which is sufficient for HB.
The deterioration in performance of HB when few calibration points are available was also observed
in the binary setting by Gupta and Ramdas (2021, Appendix C). Niculescu-Mizil and Caruana (2005)
noted in the conclusion of their paper that Platt scaling (Platt, 1999), which is closely related to TS,
performs well when the data is small, but another nonparametric binning method, isotonic regression
(Zadrozny and Elkan, 2002) performs better when enough data is available. Kull et al. (2019, Section
4.1) compared HB to other calibration techniques for class-wise calibration on 21 UCI datasets, and
found that HB performs the worst. On inspecting the UCI repository, we found that most of the
datasets they used had fewer than 5000 (total) data points, and many contain fewer than 500.
Overall, comparing our results to previous empirical studies, we believe that if sufficiently many
points are available for recalibration, or the number of classes is small, then HB performs quite well.
To be more precise, we expect HB to be competitive if at least 200 points per class can be held out
for recalibration, and the number of points per bin is at least k220.
F Additional experimental details and results for COVTYPE-7
We present additional details and results for the top-label HB experiment of Section B.2. The base
classifier is an RF learnt using sklearn.ensemble import RandomForestClassifier
with default parameters. The base RF is a nearly continuous base model since most predictions are
unique. Thus, we need to use binning to make reliability diagrams, validity plots, and perform ECE
estimation, for the base model. To have a fair comparison, instead of having a fixed binning scheme
to assess the base model, the binning scheme was decided based on the unique predictions of top-
label HB. Thus for every l, and r P Rangephlq, the bins are defined as tx : cpxq “ l, hl pxq “ ru.
Due to this, while the base model in Figures 4a and 4b are the same, the reliability diagrams and
validity plots in orange are different. As can be seen in the bar plots in Figure 4, the ECE estimation
is not affected significantly.
When k “ 100, the total number of bins chosen by Algorithm 8 was 403, which is roughly 57.6
bins per class. The choice ofB “ 50 for the fixed bins per class experiment was made on this basis.
25
Published as a conference paper at ICLR 2022
Class 1 reliability diagram	Class 1 validity plot
0.00 0.25 0.50 0.75 1.00
Predicted probability
Class 2 reliability diagram
0.00	0.05	0.10	0.15
ε
Class 2 validity plot
I I I I
A=一 qsαjd eruj.
Class 2 reliability diagram
X4=一 qeq2d 9n;
0.0
I J
≡>
0.00 0.25
0.50 0.75 1.00
Predicted probability
Class 2 validity plot
α<∣F-------------∖-------卜
0.00	0.05	0.10	0.15
Class 3 reliability diagram
Class 3 validity plot
Class 3 reliability diagram
Class 3 validity plot
X4=一 qeq2d 9n;
Class 4 reliability diagram
Class 4 reliability diagram
CIaSS 4 validity plot
I I I I
AM=qeq2d Φ≡H
H
Class 4 validity plot
0.00	0.05	0.10	0.15
0.00 0.25 0.50 0.75 1.00
Predicted probability
Class 5 reliability diagram
Class 5 reliability diagram
I I I I
X4=一 qeq2d 9n;
1.0-
O.B-
0.6-
0.4-
0.2-
0.0-
Class 5 validity plot
Class 5 validity plot
0.00	0.05	0.10	0.15
0.00 0.25 0.50 0.75 1.00
Predicted probability
0.00	0.05	0.10	0.15
0.00 0.25 0.50 0.75 1.00
Predicted probability
Class 7 reliability diagram
Class 7 validity plot
I I I I
X4=一 qeq2d 9n;
0.0
0.00 0.25 0.50 0.75 1.00
Predicted probability
0.00	0.05	0.10	0.15
0.6-
0.4-
0.2-
0.0-
Class 7 reliability diagram
0.00 0.25 0.50 0.75 1.00
Predicted probability
Class 7 validity plot
(a)	Top-label HB with k “ 100 points per bin.
(b)	Top-label HB with B “ 50 bins per class.
Figure 9:	Top-label histogram binning (HB) calibrates a miscalibrated random-forest on the class
imbalanced COVTYPE-7 dataset. For the less likely classes (4, 5, and 6), the left column is bet-
ter calibrated than the right column. Similar observations are made on other datasets, and so we
recommend adaptively choosing a different number of bins per class, as Algorithm 8 does.
26
Published as a conference paper at ICLR 2022
Figure 9 supplements Figure 4 in the main paper by presenting reliability diagrams and validity
plots of top-label HB for all classes. Figure 9a presents the plots with adaptive number of bins per
class (Algorithm 8), and Figure 9b presents these for fixed number of bins per class. We make the
following observations.
(a)	For every class l P rLs, the RF is overconfident. This may seem surprising at first since we
generally expect that models may be overconfident for certain classes and underconfident
for others. However, note that all our plots assess top-label calibration, that is, we are
assessing the predicted and true probabilities of only the predicted class. It is possible that
a model is overconfident for every class whenever that class is predicted to be the top-label.
(b)	For the most likely classes, namely classes 1 and 2, the number of bins in the adaptive case
is higher than 50. Fewer bins leads to better calibration (at the cost of sharpness). This can
be verified through the validity plots for classes 1 and 2—the validity plots in the fixed bins
case is slightly above the validity plot in the adaptive bin case. However both validity plots
are quite similar.
(c)	The opposite is true for the least likely classes, namely classes 4, 5, 6. The validity plot
in the fixed bins case is below the validity plot in the adaptive bins case, indicating higher
TL-ECE in the fixed bins case. The difference between the validity plots is high. Thus if
a fixed number of bins per class is pre-decided, the performance for the least likely classes
significantly suffers.
Based on these observations, we recommend adaptively choosing the number of bins per class, as
done by Algorithm 8.
G	Binning-based calibrators for canonical multiclas s
CALIBRATION
Canonical calibration is a notion of calibration that does not fall in the M2B category. To define
canonical calibration, we use Y to denote the output as a 1-hot vector. That is, Yi “ eYi P
△L—1, where eι corresponds to the l-th canonical basis vector in Rd. Recall that a predictor h “
ph1, h2, . . . , hLq is said to be canonically calibrated if PpY “ l | hpXqq “ hlpXq for every l P rLs.
Equivalently, this can be stated as E rY | hpX qs “ hpX q. Canonical calibration implies class-wise
calibration:
Proposition 1. If E rY | hpX qs “ hpXq, then for every l P rLs, PpY “ l | hlpXqq “ hl pXq.
The proof in Appendix H is straightforward, but the statement above is illuminating, because there
exist predictors that are class-wise calibrated but not canonically calibrated (Vaicenavicius et al.,
2019, Example 1).
Canonical calibration is not an M2B notion since the conditioning occurs on the L-dimensional
prediction vector predpXq “ hpX q, and after this conditioning, each of the L statements P pY “
l | predpXqq “ hl pXq should simultaneously be true. On the other hand, M2B notions verify only
individual binary calibration claims for every such conditioning. Since canonical calibration does
not fall in the M2B category, Algorithm 5 does not lead to a calibrator for canonical calibration. In
this section, we discuss alternative binning-based approaches to achieving canonical calibration.
For binary calibration, there is a complete ordering on the interval r0, 1s, and this ordering is lever-
aged by binning based calibration algorithms. However, ∆Lτ, for L23 does not have such a
natural ordering. Hence, binning algorithms do not obviously extend for multiclass classification.
In this section, we briefly discuss some binning-based calibrators for canonical calibration. Our
descriptions are for general L23, but We anticipate these algorithms to work reasonably only for
small L, say if L ≤ 5.
As usual, denote g : X → △L´1 as the base model and h : X → △L´1 as the model learnt us-
ing some post-hoc canonical calibrator. For canonical calibration, we can surmise binning schemes
that directly learn h by partitioning the prediction space △L´1 into bins and estimating the dis-
tribution of Y in each bin. A canonical calibration guarantee can be showed for such a binning
scheme using multinomial concentration (Podkopaev and Ramdas, 2021, Section 3.1). However,
since Vol(∆L-1) “ 2θpLq, there will either be a bin whose volume is 2ωpl) (meaning that h would
27
Published as a conference paper at ICLR 2022
not be sharp), or the number of bins will be 2ωpl), entailing 2ωpl) requirements on the sample
complexity—a curse of dimensionality. Nevertheless, let us consider some binning schemes that
could work if L is small.
Formally, a binning scheme corresponds to a partitioning of ∆L11 into B21 bins. We denote
this binning scheme as B : ∆Lτ → [B], where B(S) corresponds to the bin to which S P ∆Lτ
belongs. To learn h, the calibration data is binned to get sets of data-point indices that belong to
each bin, depending on the g(Xi) values:
for every b P rBs, Tb :“ ti : B(g(Xi)) “ bu, nb “ |Tb| .
We then compute the following estimates for the label probabilities in each bin:
for every (l,	b) P	[L]	X	[B],	ΠΠlι,b	:“	'iPT——---------ɪ if n > 0 else	ΠΠlι,b	=	1/B.
nb
The binning predictor h : X → ∆L11 is now defined component-wise as follows:
for every l P rLs, hι (x) “ Πι,Bpxq .
In words, for every bin b P rBs, h predicts the empirical distribution of the Y values in bin b.
Using a multinomial concentration inequality (Devroye, 1983; Qian et al., 2020; Weissman et al.,
2003), calibration guarantees can be shown for the learnt h. Podkopaev and Ramdas (2021, Theorem
3) show such a result using the Bretagnolle-Huber-Carol inequality. All of these concentration
inequality give bounds that depend inversely on n or ?nb.
In the following subsections, we describe some binning schemes which can be used for canoni-
cal calibration based on the setup illustrated above. First we describe fixed schemes that are not
adaptive to the distribution of the data: Sierpinski binning (Appendix G.1) and grid-style binning
(Appendix G.2). These are analogous to fixed-width binning for L “ 2. Fixed binning schemes
are not adapted to the calibration data and may have highly imbalanced bins leading to poor esti-
mation of the distribution of Y in bins with small nb. In the binary case, this issue is remedied
using histogram binning to ensure that each bin has nearly the same number of calibration points
(Gupta and Ramdas, 2021). While histogram binning uses the order of the scalar g(Xi) values,
there is no obvious ordering for the multi-dimensional g(Xi) values. In Appendix G.3 we describe
a projection based histogram binning scheme that circumvents this issue and ensures that each nb
is reasonably large. In Appendix G.4, we present some preliminary experimental results using our
proposed binning schemes.
Certain asymptotic consistency results different from calibration have been established for histogram
regression and classification in the nonparametric statistics literature (Nobel, 1996; Lugosi and No-
bel, 1996; Gordon and Olshen, 1984; Breiman et al., 2017; Devroye, 1988); further extensive refer-
ences can be found within these works. The methodology of histogram regression and classification
relies on binning and is very similar to the one we propose here. The main difference is that these
works consider binning the feature space X directly, unlike the post-hoc setting where we are essen-
tially interested in binning ∆Lτ. In terms of theory, the results these works target are asymptotic
consistency for the (Bayes) optimal classification and regression functions, instead of canonical cal-
ibration. It would be interesting to consider the (finite-sample) canonical calibration properties of
the various algorithms proposed in the context of histogram classification. However, such a study is
beyond the scope of this paper.
G.1 Sierpinski binning
First, we describe Sierpinski binning for L “ 3. The probability simplex for L “ 3, ∆2, is a triangle
with vertices e1 “ (1, 0, 0), e2 “ (0, 1, 0), and e3 “ (0, 0, 1). Sierpinski binning is a recursive
partitioning of this triangle based on the fractal popularly known as the Sierpinski triangles. Some
Sierpinski bins for L “ 3 are shown in Figure 10. Formally, we define Sierpinski binning recursively
based on a depth parameter q P N. Given an x P X, let S “ g(x). For q “ 1, the number of bins is
B “ 4, and the binning scheme B is defined as:
1
2
3
4
B(S) “
if si > 0.5
if S2 > 0.5
if s3 > 0.5
otherwise.
(18)
28
Published as a conference paper at ICLR 2022
Figure 10: Sierpinski binning for L “ 3. The leftmost triangle represents the probability simplex
∆2 . Sierpinski binning divides ∆2 recursively based on a depth parameter q P N.
Note that since s1 ` s2 ` s3 “ 1, only one of the above conditions can be true. It can be verified
that each of the bins have volume equal to p1{4q-th the volume of the probability simplex ∆2. If a
finer resolution of ∆2 is desired, B can be increased by further dividing the partitions above. Note
that each partition is itself a triangle; thus each triangle can be mapped to ∆2 to recursively define
the sub-partitioning. For i P r4s, define the bins bi “ ts : Bpsq “ iu. Consider the bin b1. Let us
reparameterize it as pt1, t2, t3q “ p2s1 ´ 1, 2s2, 2s3q. It can be verified that
bi “ {(t1,t2,t3) : si > 0.5} “ {(t1,t2,t3) ： tι ' t2 ' t3 = 1,tι P (0,1],12 P [0, iq,t3 P [0,1)}.
Based on this reparameterization, we can recursively sub-partition b1 as per the scheme (18), replac-
ing s with t. Such reparameterizations can be defined for each of the bins defined in (18):
b2 “ {(s1,s2,s3) ： s2 > 0.5}	：	(ti, t2,	t3q	“	(2si,	2s2 ´ 1, 2s3),
b3 “ {(s1,s2,s3) ： s3 > 0.5}	：	(ti, t2,	t3q	“	(2sι,	2s2, 2s3 ´ iq,
b4 “	t(si, s2, s3) ： Si ≤ 0.5 for all i}	：	(ti, t2,	t3)	“	(1 ´	2si, 1 ´ 2s2,1	´	2s3),
and thus every	bin can be recursively sub-partitioned as per	(18).	As illustrated in	Figure 10, for
Sierpinski binning, we sub-partition only the bins bi, b2, b3 since the bin b4 corresponds to low
confidence for all labels, where finer calibration may not be needed. (Also, in the L > 3 case
described shortly, the corresponding version of b4 is geometrically different from ∆Lτ, and the
recursive partitioning cannot be defined for it.) If at every depth, we sub-partition all bins except
the corresponding b4 bins, then it can be shown using simple algebra that the total number of bins
is (3q'i — 1){2. For example, in Figure 10, when q = 2, the number of bins is B = 14, and when
q “ 3, the number of bins is B “ 40.
As in the case of L “ 3, Sierpinski binning for general L is defined through a partitioning function
of ∆Lτ into L ' 1 bins, and a reparameterization of the partitions so that they can be further
sub-partitioned. The L ` 1 bins at depth q “ 1 are defined as
B(S) = " l	ifsl > 0.5,	(19)
L ` 1 otherwise.
While this looks similar to the partitioning (18), the main difference is that the bin bL'i has a
larger volume than other bins. Indeed for l P [L], vol(bι) = vol(∆LT)∕2Lτ, while vol(bL'i)=
vol(∆LT)(1 — L∕2LT)》vol(∆LT)∕2Lτ, with equality only occuring for L = 3. Thus the
bin bL'i is larger than the other bins. If g(x) P bL'i, then the prediction for X may be not be
very sharp, compared to if g(χ) were in any of the other bins. On the other hand, if g(χ) P bL'i,
the score for every class is smaller than 0.5, and sharp calibration may often not be desired in this
region.
In keeping with this understanding, we only reparameterize the bins bi, b2, . . . , bL so that they can
be further divided:
bl = t(si, s2,..., SL) ： SI > 0.5} ： (ti,t2,. .. ,tL)= (2si, ..., 2sι — 1,..., 2sL).
For every l P [Ls, under the reparameterization above, it is straightforward to verify that
{(ti,t2,.. .,tL)： Sι > 0.5} = t(ti,t2,... ,tL)： £ tu = 1,tι P (0,1S,tu P [0,1) Vu ‰ l}.
uPrLs
Thus every bin can be recursively sub-partitioned following (19). For Sierpinski binning with L
labels, the number of bins at depth q is given by (Lq'i — 1)∕(L — 1).
29
Published as a conference paper at ICLR 2022
0 0.2 0.4 0.6 0.8 1
Class 2
K “ 5,τ “ 0.2
Figure 11: Grid-style binning for L “ 3.
G.2 Grid-style binning
Grid-style binning is motivated from the 2D reliability diagrams of Widmann et al. (2019, Figure 1),
where they partitioned ∆2 into multiple equi-volume bins in order to assess canonical calibration on
a 3-class version of CIFAR-10. For L “ 3, ∆2 can be divided as shown in Figure 11. This corre-
sponds to gridding the space ∆2 , just the way we think of gridding the real hyperplane. However,
the mathematical description of this grid for general L is not apparent from Figure 11. We describe
grid-style binning formally for general L23.
Consider some T > 0 such that K :“ 1{τ P N. For every tuple k “ (k1,k2,...,kL) in the set
I = {k P NL : max(L, K + 1) W £ kl ≤ K + (L — 1)},	(20)
lPrLs
define the bins
bk :“ {s P ∆L11 : for every l P [L], SlK P [k? — 1, k]	(21)
These bins are not mutually disjoint, but intersections can only occur at the edges. That is, for
every s that belongs to more than one bin, at least one component sl satisies slK P N. In order
to identify a single bin s, ties can be broken arbitrarily. One strategy is to use some ordering on
NL; say for kι ‰ k2 P N, kι < k2 if and only if for the first element of kι that is unequal to
the corresponding element of k2 the one corresponding to k1 is smaller. Then define the binning
function B : ∆Lτ → |I| as B(S) “ min{k : S P bQ. The following propositions prove that a)
each s belongs to at least one bin, and b) that every bin is an L — 1 dimensional object (and thus a
meaningful partition of ∆LT).
Proposition 2. The bins {bk : k P I} defined by (21) mutually exhaust ∆Lτ.
Proof. Consider any S P ∆L11. For SlK R N “ {1,2,...}, set kl “ max(1, JslKS) > SlK.
Consider the condition
C : for all l such that SlK R N, SlK “ 0.
IfC is true, then forl such that SlK P N, set kl “ SlK. IfC is not true, then for l such that SlK P N,
set exactly one kl “ SlK + 1, and for the rest set kl “ SlK. Based on this setting of k, it can be
verified that S P bk .
Further, note that for every l, kleSlK, and there exists at least one l such that kl > SlK. Thus We
have:
LL
∑ kl > ∑ SlK
l“1	l“1
“ K.
Since XL“1 kl P N, we must have XL“1 kl2K + 1. Further since each kl P N, XL“1 kleL.
Next, note that for every l, kl W SlK + 1. If C is true, then there is at least one l such that SlK P N,
and for this l, we have set kl “ SlK < SlK + 1. If C is not true, then either there exists at least one
l such that SlK R N Y {0} for which kl “ Js?KS < SlK + 1, or every SlK P N, in which case we
30
Published as a conference paper at ICLR 2022
have set kι “ SlK for one of them. In all cases, note that there exists an l such that kι V SlK ' 1.
Thus,
LL
∑ kl V ∑ (SlK + iq
l“1	l“1
“ K + L.
Since XL“1 kl P N, We must have XL“1 kl ≤ K + L 一 1.
□
Next, we show that each bin indexed by k P I contains a non-zero volume subset of ∆Lτ, where
volume is defined with respect to the Lebesgue measure in RL´1. This can be shown by arguing
that bk contains a scaled and translated version of ∆L=1.
Proposition 3. For every k P I, there exists some U P RL and V > 0 such that U + v∆L´1 ɑ bk.
Proof. Fix some k P I. By condition (20), XlL“1 kl P rmax(L, K + 1q, K + L ´ 1s. Based on this,
we claim that there exists a τ P r0, 1q such that
L
£(kl ´ iq + TL + pi ´ Tq = k.	(22)
l“1
Indeed, note that for T = 0, XL“i(kl — 1)+ TL + (1 — Tq ≤ (K — 1) + 1 “ K and for T = 1,
XL“i(kl ´ 1q + TL + (1 — Tq “ XL“i kl > K. Thus, there exists a T that satisfies (22) by the
intermediate value theorem.
Next, define U “ K—1(k + (t — 1)1lq and V “ K—1(1 — Tq > 0, where 1l denotes the vector
in Rl with each component equal to 1. Consider any S P U + v∆L´1. Note that for every l P [L],
SlK P rkl — 1, kls and by property (22),
L
£ SlK “
l“1
L
+ V “ £(kl — 1q + TL + (1 — Tq “ K.
Thus, s P Δl_1 and by the definition of bk, S P bk. This completes the proof.
□
The previous two propositions imply that B satisfies the property we require of a reasonable binning
scheme. For L “ 3, grid-style binning gives equi-volume bins as illustrated in Figure 11; however
this is not true for L > 3. We now describe a histogram binning based partitioning scheme.
G.3 Projection based histogram binning for canonical calibration
Some of the bins defined by Sierpinski binning and grid-style binning may have very few calibration
points nb, leading to poor estimation ofΠ. In the binary calibration case, this can be remedied using
histogram binning which strongly relies on the scoring function g taking values in a fully ordered
space [0,1]. To ensure that each bin contains Ω(n{Bq points, we estimate the quantiles of g(Xq
and created the bins as per these quantiles. However, there are no natural quantiles for unordered
prediction spaces such as Δl-1 (L23). In this section, we develop a histogram binning scheme
for Δl_1 that is semantically interpretable and has desirable statistical properties.
Our algorithm takes as input a prescribed number of bins B and an arbitrary sequence of vectors
q1, q2,..., qB—1 P Rl with unit '2-norm: ∣∣qi}2 “ 1. Each of these vectors represents a direction
on which we will project Δl_1 in order to induce a full order on Δl_1. Then, for each direction,
we will use an order statistics on the induced full order to identify a bin with exactly t(n + 1q{Bu —
1 calibration points (except the last bin, which may have more points). The formal algorithm is
described in Algorithm 10. It uses the following new notation: given m vectors V1, V2, . . . , Vm P RL,
a unit vector u, and an index j P rms, let order-statistics(tV1, V2, . . . , Vmu, u, jq denote the j-th
order-statistics of tV1T u, V2T u, . . . , VmT uu.
31
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Algorithm 10: Projection histogram binning for canonical calibration
Input: Base multiclass predictor g : X → ∆L11, calibration data
D“ tpX1,Y1q,pX2,Y2q,...,pXn,Ynqu
Hyperparameter: number of bins B, unit vectors q1 , q2 , . . . , qB P RL,
Output: Approximately calibrated scoring function h
S D tg(Xj, g(X2), ... , g(Xn)};
T D empty array of size B;
CDt n'1 U；
for b D 1 to B — 1 do
Tb D order-statistics (S, qb, c);
S D S∖{v P S : VTqb ≤ TbU;
end
TB D 1.01;
B(g(∙)) D min{b p[B] : g(∙)Tqb V Tj};
Π D empty matrix of size B X L;
for b D 1 to B do
for l D 1 to L do
I ∏b,ι D MeantltYi “ l} : B(g(Xi)) “ b and @s P [B], g(Xi)Tq$ ‰ Ts};
end
end
for l D 1 to L do
I hl (∙) D ∏B(g(∙)),l,
end
return h;
We now briefly describe some values computed by Algorithm 10 in words to build intuition. The
array T, which is learnt on the data, represents the identified thresholds for the directions given
by q. Each (qb,Tb) pair corresponds to a hyperplane that cuts ∆Lτ into two subsets given by
{x P ∆Lτ : XTqb v Tb} and {x P ∆Lτ : XTqbeTb}. The overall partitioning of ∆Lτ is
created by merging these cuts sequentially. This defines the binning function B. By construction,
the binning function is such that each bin contains at least [nB'1 U — 1 many points in its interior. As
suggested by Gupta and Ramdas (2021), we do not include the points that lie on the boundary, that
is, points Xi that satisfy g(Xi)T qs “ Ts for some s P rBs. The interior points bins are then used to
estimate the bin biases ∏p.
No matter how the q-vectors are chosen, the bins created by Algorithm 10 have at least [n]—1 points
for bias estimation. However, we discuss some simple heuristics for setting q that are semantically
meaningful. For some intuition, note that the binary version of histogram binning Gupta and Ramdas
(2021, Algorithm 1) is essentially recovered by Algorithm 10 if L “ 2 by setting each qb as e2 (the
vector r0, 1s). Equivalently, we can set each qb to —e1 since both are equivalent for creating a
projection-based order on ∆?. Thus for L23, a natural strategy for the q-vectors is to rotate
between the canonical basis vectors: qι “ 一e1,q2 “ 一e2,...,qL “ 一eL,qL+ι “ 一eι,...,
and so on. Projecting with respect to —el focuses on the class l by forming a bin corresponding to
the largest values of gl (Xi) among the remaining Xi’s which have not yet been binned. (On the
other hand, projecting with respect to el will correspond to forming a bin with the smallest values
ofgl(Xi).)
The q-vectors can also be set adaptively based on the training data (without seeing the calibration
data). For instance, if most points belong to a specific class l P rLs, we may want more sharpness
for this particular class. In that case, We can choose a higher ratio of the q-vectors to be —el.
G.4 Experiments with the COVTYPE dataset
In Figure 12 we illustrate the binning schemes proposed in this section on a 3-class version of the
COVTYPE-7 dataset considered in Section B.2. As noted before, this is an imbalanced dataset
where classes 1 and 2 dominate. We created a 3 class problem with the classes 1, 2, and other (as
32
Published as a conference paper at ICLR 2022
(a) Calibration using Sierpinski binning at depth q “ 2.
(b) Calibration using grid-style binning with K “ 5, τ “ 0.2.
(c)	Projection-based HB with B “ 27 projections: q1 “ ´e1 , q2 “ ´e2, . . . , q4, ´e1, . . ., and so on.
(d)	Projection-based HB with B “ 27 random projections (q, drawn uniformly from the '2-unit-ball in R3).
Figure 12: Canonical calibration using fixed and histogram binning on a 3-class version of the
COVTYPE-7 dataset. The reliability diagrams (left) indicate that all forms of binning improve the
calibration of the base logistic regression model. The recalibration diagrams (right) are a scatter plot
of the predictions gpXq on the test data with the points colored in 10 different colors depending on
their bin. For every bin, the arrow-tail indicates the mean probability predicted by the base model g
whereas the arrow-head indicates the probability predicted by the updated model h.
class 3). The entire dataset has 581012 points and the ratio of the classes is approximately 36%,
49%, 15% respectively. The dataset was split into train-test in the ratio 70:30. The training data
was further split into modeling-calibration in the ratio 90:10. A logistic regression model g using
Sklearn.linear□model.LogisticRegression was learnt on the modeling data, and g
was recalibrated on the calibration data.
The plots on the right in Figure 12 are recalibration diagrams. The base predictions gpXq on the
test-data are displayed as a scatter plot on ∆2. Points in different bins are colored using one of 10
33
Published as a conference paper at ICLR 2022
different colors (since the number of bins is larger than 10, some colors correspond to more than one
bin). For each bin, an arrow is drawn, where the tail of the arrow corresponds to the average gpXq
predictions in the bin and the head of the arrow corresponds to the recalibrated hpXq prediction for
the bin. For bins that contained very few points, the arrows are suppressed for visual clarity.
The plots on the left in Figure 12 are validity plots (Gupta and Ramdas, 2021). Validity plots display
estimates of
V(ε)“ Ptest-data (但[丫 | g(X)]— g(X)}l ≤ ε),
as ε varies (g corresponds to the validity plot for logistic regression; replacing g with h above gives
plots for the binning based classifier h). For logistic regression, the same binning scheme as the one
provided by h is used to estimate V (εq.
Overall, Figure 12 shows that all of the binning approaches improve the calibration of the original
logistic regression model across different ε. However, the recalibration does not change the original
model significantly. Comparing the different binning methods to each other, we find that they all
perform quite similarly. It would be interesting to further study these and other binning methods for
post-hoc canonical calibration.
H	Proofs
Proofs appear in separate subsections, in the same order as the corresponding results appear in the
paper and Appendix. Proposition 4 was stated informally, so we state it formally as well.
H. 1 S tatement and proof of Proposition 4
Proposition 4. For any predictor (c, h),conf-ECE(c, h) ≤ TL-ECE(c, h).
Proof. To avoid confusion between the the conditioning operator and the absolute value operator | ∙ |,
We use abs (∙) to denote absolute values below. Note that,
abs	(P(Y “ C(Xq | h(xqq —	h(xqq	=	abs (E [i {y = C(Xqu ∣ h(xqs — h(xqq
“	abs (E [1 {Y = c(X)} — h(X) | h(X)])
“	abs (E [E [1 {Y = c(Xq} — h(Xq | h(X),c(X)s |	h(X)s)
W	E [abs (E [1 {Y = c(Xq} — h(Xq | h(X),c(X)s)	|	h(X)s
(by Jensen’s inequality)
“ E[abs(P(Y “ C(Xq | h(Xq, C(Xqq ´ h(Xqq | h(Xqs.
Thus,
conf-ECE(C, hq “ E [abs (P (Y “ C(Xq | h(Xqq ´ h(Xqqs
W E[E[abs(P(Y “ C(Xq | h(Xq,C(Xqq ´ h(Xqq | h(Xqss
“ E [abs (P(Y “ C(Xq | h(Xq, C(Xqq ´ h(Xqqs
“ TL-ECE(C, hq.
□
H.2 Proof of Theorem 1
The proof strategy is as follows. First, we use the bound of Gupta and Ramdas (2021, Theorem 4)
(henceforth called the GR21 bound), derived in the binary calibration setting, to conclude marginal,
conditional, and ECE guarantees for each hl . Then, we show that the binary guarantees for the
individual hl’s leads to a top-label guarantee for the overall predictor (C, hq.
Consider any l P [L]. Let Pl denote the conditional distribution of (X, 1 {Y “ l}q given c(Xq “ l.
Clearly, Dl is a set of nl i.i.d. samples from Pl, and hl is learning a binary calibrator with respect
to Pl using binary histogram binning. The number of data-points is nl and the number of bins is
Bl “ tnl{ku bins. We now apply the GR21 bounds on hl . First, we verify that the condition they
require is satisfied:
nl > k [nl∕kU > 2Bl.
34
Published as a conference paper at ICLR 2022
Thus their marginal calibration bound for hl gives,
p (|p(y = 11 C(X q = ι, hιpχ qq ´ h (X q∣ ≤ δ ` j
Note that since [nι∕B" = [nJ [nι∕kUU > k,
d log"	J
ε1=δ `v 2(⅛zιy》δ+y
log(2∕a)	∖
2(tnι∕BιU ´ iq । C(Xq = l)》1 ´ a.
log(2∕α)
2(tnl/Blu ´ 1q
Thus we have
p(∣p(Y = 11 c(χq = ι, hi(χqq ´ hι(χq∣ ≤ ει ∣ C(Xq = iq > i ´ α.
This is satisfied for every l. Using the law of total probability gives us the top-label marginal cali-
bration guarantee for (C, hq:
p (∣p (γ = c(x q i c(x q, h(X qq ´ hX q∣ ≤ ειq
L
=∑ p (c(χ q = ιqp(∣p (Y = C(X q i C(X q, hχ qq ´ hχ q∣ ≤ ει i C(X q = iq
l = 1
(law of total probability)
L
“£ p (c(x q = iqp (∣p (Y = 11 C(X q = i, h (χ qq ´ hι(χ q∣ ≤ ει ∣ C(X q = iq
l“1
(by construction, if C(xq = i, h(xq = hl (xq)
L
》Σ P(C(Xq = i)(1 ´ ɑq
l“1
= 1 ´ α.
Similarly, the in-expectation ECE bound of GR21, for p = 1, gives for every i,
E ∣p (Y = 11 c(x q = i, hι (χ qq ´ hι(χ q i C(X q = i| ≤ aBι ∕2nι + δ
=a[nι∕kU ∕2nι + δ
≤ ai72k + δ.
Thus,
E|p(Y =C(Xq i C(Xq, hι(Xqq ´ h(Xq|
L
=£ p (c(x q = iqE∣p (Y = 11 c(x q = i, hι(χ qq ´ hι(χ q∣ i C(X q = i
l“1
L
≤ £ p (c(x q = i)(aw+δq
l“1
“√1∕2k + δ.
Next, We show the top-label conditional calibration bound. Let B = XL“1 Bι and aι = αBι∕B.
Note that B ≤ XL“1 nι∕k = n∕k. The binary conditional calibration bound of GR21 gives
P (Vr P Range(hι), |P(Y = i i c(X) = i, hι(X) = r) ´ r| ≤ δ + {∙
log(2Bι ∕αι q
2(tnι∕Bιu ´ 1q
i C(Xq = i
21 ´ αι.
Note that

log(2Bι∕αιq
2(tnι∕Bιu ´ 1q
=
log(2B∕αq
2(tnι∕Bιu ´ iq
35
Published as a conference paper at ICLR 2022
≤ ʌilogVn/kQqn	(since B ≤ n{k)
2ptnl{Blu ´ 1q
≤ dlogp2η{kαq	(since k ≤ [ηι∕BιU).
2pk ´ 1q
Thus for every l P rLs,
P(Vr P Range(hι), |P(Y “ l | CpX) = l, h√(X) “ r)— r| ≤ ε2)》1 — α?.
By construction of h, conditioning on cpXq “ l and hl pXq “ r is the same as conditioning on
c(X) “ l and h(X) “ r. Taking a union bound over all L gives
P(@l p[L],r P Rangeph),∣P(Y = c(X) | c(X) = l,h(X) = r) — r|) W ε2)
L
21 ´ X αι “ 1 — α,
l“1
proving the conditional calibration result. Finally, note that if for every l P rLs, r P Range(h),
|P(Y “c(X) | c(X) “l,h(X)“r)—r| Wε2,
then also
TL-ECE(c, h) “ E|P (Y “ c(X) | h(X), c(X)) — h(X)| W ε2.
This proves the high-probability bound for the TL-ECE.	□
Remark 3. GUPta and Ramdas (2021) proved a more general result for general 'p-ECE bounds.
Similar results can also be derived for the suitably defined 'p-TL-ECE. Additionally, it can be shown
that with probability 1 — α, the TL-MCE of (c, h) is bounded by ε2 . (TL-MCE is defined in Ap-
pendix E, equation (17).)
H.3 Proof of Proposition 1
Consider a specific l P rLs. We use hl to denote the l-th component function of h and Yl “
1 {Y “ l}. Canonical calibration implies
P(Y“l | h(X)) “ErYl | h(X)s “ hl(X).	(23)
We can then use the law of iterated expectations (or tower rule) to get the final result:
ErYl | hl(X)s “ErErYl | h(X)s | hl(X)s
“ E rhl(X) | hl (X)s	(by the canonical calibration property (23))
“ hl(X).
□
H.4 Proof of Theorem 2
We use the bounds of Gupta and Ramdas (2021, Theorem 4) (henceforth called the GR21 bounds),
derived in the binary calibration setting, to conclude marginal, conditional, and ECE guarantees for
each hl . This leads to the class-wise results as well.
Consider any l P [L]. Let Pl denote the distribution of (X, 1 {Y “ l}). Clearly, Dl is a set of n i.i.d.
samples from Pl, and hl is learning a binary calibrator with respect to Pl using binary histogram
binning. The number of data-points is n and the number of bins is Bl “ tn∕klu bins. We now apply
the GR21 bounds on hl. First, we verify that the condition they require is satisfied:
nekl [n∕klU22Bl.
Thus the GR21 marginal calibration bound gives that for every l P rLs, hl satisfies
P (IP(Y=l1hl(X))- hl(X )1W δ+KtzB;、))
21 — αl.
36
Published as a conference paper at ICLR 2022
The class-wise marginal calibration bound of Theorem 2 follows since [n∕B” “ [n∕ [n∕kιUU > kι,
and so	_______________
εpιq > δ `, ； log(2∕αιq
l > δ 'V 2(tn∕BιU ´ 1).
Next, the GR21 conditional calibration bound gives for every l P rLs, hl satisfies
@r P Range(hlq, |P(Y “ l | hl(Xq “ rq ´
P
IVKq PlOgp2BTar ∖
rκ δ 'V 2(tn∕BιU ´ 1))
> 1 ´ αl .
The class-wise marginal calibration bound of Theorem 2 follows since Bl “ [n∕k" ≤ n∕kι and
tn∕Blu “ tn∕ tn∕kluu > kl, and so
>δ'
l log(2Bl∕alq
N 2(tn∕BlU ´ 1).
Let k “ minlPrLs kl. The in-expectation ECE bound of GR21, forp “ 1, gives for every l,
E [binary-ECE-for-class-l (hl)S ≤ ∖BlZni ' δ
“ atn∕kιU ∕2n ' δ
≤ aiM ` δ
≤ a172k ` δ.
Thus,
L
E [CW-ECE(c, h)S “ E LT £ binary-ECE-for-class-l (hi)
l“1
L
≤ LT £ (aw+δ)
l“1
“ aw+δ,
as required for the in-expectation CW-ECE bound of Theorem 2. Finally, for the high probability
CW-ECE bound, let ε “ maxlPrLs εlp2q and α “ EL 1 αl . By taking a union bound over the the
conditional calibration bounds for each hl, we have, with probability 1 ´ α, for every l P rLs and
r P Range(h),
|P(Y “ l | C(X) = l, h(X) = r) ´ r| ≤ εP2q ≤ ε.
Thus, with probability 1 ´ α,
L
CW-ECE(c, h) “ LT £ E|P(Y “ l | hl(X)) — hl(X)| ≤ ε.
l“1
This proves the high-probability bound for the CW-ECE.	□
37