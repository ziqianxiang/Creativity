Published as a conference paper at ICLR 2022
A generalization of the randomized singular
VALUE DECOMPOSITION
Nicolas Boulle
Mathematical Institute
University of Oxford
Oxford, OX2 6GG, UK
boulle@maths.ox.ac.uk
Alex Townsend
Department of Mathematics
Cornell University
Ithaca, NY 14853, USA
townsend@cornell.edu
Ab stract
The randomized singular value decomposition (SVD) is a popular and effective
algorithm for computing a near-best rank k approximation of a matrix A using
matrix-vector products with standard Gaussian vectors. Here, we generalize the
randomized SVD to multivariate Gaussian vectors, allowing one to incorporate
prior knowledge of A into the algorithm. This enables us to explore the contin-
UoUs analogue of the randomized SVD for HiIbert-Schmidt (HS) operators using
operator-function products with functions drawn from a Gaussian process (GP).
We then construct a new covariance kernel for GPs, based on weighted Jacobi
polynomials, which allows us to rapidly sample the GP and control the smooth-
ness of the randomly generated functions. Numerical examples on matrices and
HS operators demonstrate the applicability of the algorithm.
1	Introduction
Computing the singular value decomposition (SVD) is a fundamental linear algebra task in machine
learning (Paterek, 2007), statistics (Wold et al., 1987), and signal processing (Alter et al., 2000; Van
Der Veen et al., 1993). The SVD ofan m × n real matrix A with m ≥ n is a factorization of
the form A = UΣV*, where U is an m × m orthogonal matrix of left singular vectors, Σ is an
m X n diagonal matrix with entries σι(A) ≥ ∙∙∙ ≥ σn(A) ≥ 0, and V is an n X n orthogonal
matrix of right singular vectors (Golub & Van Loan, 2013). The SVD plays a central role because
truncating it after k terms provides a best rank k approximation to A in the spectral and Frobenius
norm (Eckart & Young, 1936; Mirsky, 1960). Since computing the SVD of a large matrix can be
computationally infeasible, there are various alternative algorithms that compute near-best rank k
matrix approximations from matrix-vector products (Halko et al., 2011; Martinsson & Tropp, 2020;
Nakatsukasa, 2020; NyStrom, 1930; Williams & Seeger, 2001). The randomized SVD uses matrix-
vector products with standard Gaussian random vectors and is one of the most popular algorithms
for constructing a low-rank approximation to A (Halko et al., 2011; Martinsson & Tropp, 2020).
Currently, the randomized SVD is used and theoretically justified when it uses matrix-vector prod-
ucts with standard Gaussian random vectors. In this paper, we consider the following generaliza-
tions.
Generalization 1. We generalize the randomized SVD when the matrix-vector products are with
multivariate Gaussian random vectors. Our theory allows for multivariate Gaussian random input
vectors that have a general symmetric positive semi-definite covariance matrix. A key novelty of
our work is that prior knowledge of A can be exploited to design covariance matrices that achieve
lower approximation errors than the randomized SVD with standard Gaussian vectors.
Generalization 2. We generalize the randomized SVD to Hilbert-Schmidt (HS) operators (Hsing
& Eubank, 2015). We design a practical algorithm for learning HS operators using random input
functions, sampled from a Gaussian process (GP). Examples of applications include learning inte-
gral kernels such as Green,s functions associated with linear partial differential equations (Boulle &
Townsend, 2022; Boulle et al., 2021).
The choice of the covariance kernel in the GP is crucial and impacts both the theoretical bounds
and numerical results of the randomized SVD. This leads us to introduce a new covariance kernel
1
Published as a conference paper at ICLR 2022
based on weighted Jacobi polynomials for learning HS operators. One of the main advantages of
this kernel is that it is directly expressed as a KarhUnen-Loeve expansion (Karhunen, 1946; Loeve,
1946) so that it is faster to sample functions from the associated GP than using a standard squared-
exponential kernel. In addition, we show that the smoothness of the functions sampled from a GP
with the Jacobi kernel can be controlled as it is related to the decay rate of the kernel’s eigenvalues.
Contributions. We summarize our novel contributions as follows:
1.	We provide new theoretical bounds for the randomized SVD for matrices or HS operators
when using random input vectors generated from any multivariate Gaussian distribution.
This shows when it is beneficial to use nonstandard Gaussian random vectors in the ran-
domized SVD for constructing low-rank approximations.
2.	We generalize the randomized SVD to HS operators and provide numerical examples to
learn integral kernels.
3.	We propose a covariance kernel based on weighted Jacobi polynomials and show that one
can select the smoothness of the sampled random functions by choosing the decay rate of
the kernel eigenvalues.
2	Background: The randomized SVD for matrices
The randomized SVD computes a near-best rank k approximation to a matrix A. First, one performs
the matrix-vector products y1 = Ax1, . . . , yk+p = Axk+p, where x1, . . . , xk+p are standard Gaus-
sian random vectors with identically and independently distributed entries and p ≥ 1 is an oversam-
Pling parameter. Then, one computes the economized QR factorization [yι … yk+p] = QR,
before forming the rank ≤ k + P approximant QQ* A. Note that if A is symmetric, one can form
QQ* A by computing Q(AQ)* via matrix-vector products involving A. The quality of the rank
≤ k +p approximant QQ*A is characterized by the following theorem.
Theorem 1 (Halko et al. (2011)) Let A be an m × n matrix, k ≥ 1 an integer, and choose an
oversampling parameter P ≥ 4. If Ω ∈ Rn×(k+p) is a Standard Gaussian random matrix and
QR = AΩ is the economized QR decomposition of AΩ, then for all u, t ≥ 1,
kA - QQ*AkF
pɪɪ ] U X σ2(A)+Ut √TIPσk+1(A),	⑴
j =k+1
2
with failure probability at most 2t-p + e-u .
The squared tail of the singular values of A, i.e., Pjn=k+1 σj2(A), gives the best rank k approxi-
mation error to A in the Frobenius norm. This result shows that the randomized SVD can compute
a near-best low-rank approximation to A with high probability.
3	Generalized randomized SVD for matrices and operators
The theory behind the randomized SVD has been recently extended to nonstandard covariance ma-
trices and HS operators (Boulle & Townsend, 2022). However, the probability bounds, generalizing
Theorem 1, are not sharp enough to emphasize the improved performance of covariance matrices
with prior information over the standard randomized SVD. We provide new bounds for GPs with
nonstandard covariance matrices in Theorem 2. An upper bound on the expectation is also available
in the Appendix (see Proposition 6). While Theorem 2 is formulated with matrices, the same result
holds for HS operators in infinite dimensions.
For a fixed target rank 1 ≤ k ≤ n, we define V1 ∈ Rn×k and V2 ∈ Rn×(n-k) to be the matrices
containing the first k and last n - k right singular vectors of A, respectively, and denote by Σ2 ∈
R(n-k)×(n-k), the diagonal matrix with entries σk+1 (A), . . . , σn(A). We consider a symmetric
positive semi-definite covariance matrix K ∈ Rn×n , with kth largest eigenvalue λk > 0.
Theorem 2 Let A be an m × n matrix, k ≥ 1 an integer, and choose an oversampling parameter
p ≥ 4. If Ω ∈ Rn×(k+p) is a Gaussian random matrix, where each column is SampledfrOm a
2
Published as a conference paper at ICLR 2022
multivariate Gaussian distribution with Covariance matrix K ∈ Rn×n, and QR = AΩ is the
economized QR decomposition of AΩ, thenfor all u, t ≥ 1,
kA - QQ*A∣∣f ≤
n
X σj2 (A),
j=k+1
(2)
with failure probability at most t-p + [ue-(u2-1)/2]k+p. Here, Yk = k∕(λι Tr((ViKVI)T)))
denotes the covariance quality factor, and βk = Tr(Σ2V2KV2)∕(λιk∑2k2), where λι is the
largest eigenvalue of K.
The factors γk and βk, measuring the quality of the covariance matrix to learn A in Theorem 2,
can be respectively bounded (BoUlle & ToWnsend 2022, Lem. 2; Lemma 9) using the eigenvalues
λι ≥ ∙∙∙ ≥ λn of the covariance matrix K and the singular values of A as:
-1 ≤ 1
γk k
n
X
j=n-k+1
λ1
λj
nn
βk ≤ X 铝 σ2(A)/ X σ2(A).
j=k+1	1	j=k+1
This shoWs that the performance of the generalized randomized SVD depends on the decay rate
of the sequence {λj }. The quantities γk and βk depend on hoW much prior information of the
k + 1, . . . , n right singular vectors of A is encoded in K. In the ideal situation Where these singular
vectors are known, then one can define K such that βk = 0 for λk+ι = •…=λn = 0. In particular,
this highlights that a suitably chosen covariance matrix can outperform the randomized SVD With
standard Gaussian vectors (see Section 5.1 for a numerical example).
3.1	Randomized SVD for HILBERT-Schmidt operators
We now describe the randomized SVD for learning HS operators (see Algorithm 1). The algorithm
is implemented in the Chebfun software system (Driscoll et al., 2014), which is a MATLAB package
for computing with functions. The Chebfun implementation of the randomized SVD for HS oper-
ators uses Chebfun’s capabilities, which offer continuous analogues of several matrix operations
like the QR decomposition and numerical integration. Indeed, the continuous analogue of a matrix-
vector multiplication AΩ foran HS integral operator F (see Hsing & Eubank, 2015 for definitions
and properties of HS operators), with kernel G : D × D → R, is
(F f)(x) =	G(x, y)f(y) dy, x ∈ D,
D
f ∈ L2(D),
where D ⊂ Rd with d ≥ 1, and L2 (D) is the space of square-integrable functions on D.
Algorithm 1 Randomized SVD for HS operators
Input: HS integral operator F with kernel G(x, y), number of samples k > 0
Output: Approximation Gk of G
1:	Define a GP covariance kernel K
2:	Sample the GP k times to generate a quasimatrix of random functions Ω = [f ι... fk]
3:	Evaluate the integral operator at Ω, Y = [F(fι)... F(fk)]
4:	orthonormalize the columns of Y, Q = orth(Y) = [q1 . . . qk]
5:	Compute an approximation to G by evaluating the adjoint of F
6:	Initialize Gk (x, y) to 0
7:	for i = 1 : k do
8：	Gk(χ,y) J Gk(χ,y) + qi(x) fD G(z,y)qi(z) dz
9: end for
The algorithm takes as input an integral operator that we aim to approximate. Note that we focus
here on learning an integral operator, but other HS operators would work similarly. The first step of
the randomized SVD for HS operators consists of generating an ∞ × k quasimatrix Ω by sampling
a GP k times, where k is the target rank (see Section 4). An ∞ × k quasimatrix is an ordered
set of k functions (Townsend & Trefethen, 2015), and generalizes the notion of matrix to infinite
dimensions. Therefore, each column of Ω is an object, consisting of a polynomial approximation of
3
Published as a conference paper at ICLR 2022
a smooth random function sampled from the GP in the Chebyshev basis. After evaluating the HS
operator at Ω to obtain a quasimatrix Y, We use the QR algorithm (Townsend & Trefethen, 2015)
to obtain an orthonormal basis Q for the range of the columns of Y. Then, the randomized SVD
for HS operators requires the left-evaluation of the operator F or, equivalently, the evaluation of its
adjoint Ft satisfying:
(Ftf)(x) =
D
G(y, x)f (y) dy,
x ∈ D.
We evaluate the adjoint of F at each column vector of Q to construct an approximation Gk of G.
Finally, the approximation error between the operator kernel G and the learned kernel Gk can be
computed in the L2-norm, corresponding to the HS norm of the integral operator.
4 Covariance kernels
To generate the random input functions f1 , . . . , fk for the randomized SVD for HS operators, we
draw them from a GP, denoted by GP(0, K), for a certain covariance kernel K. A widely employed
covariance kernel is the squared-exponential function KSE (Rasmussen & Williams, 2006) given by
KSE(χ,y) = exp (-∣χ - y|2/(2'2)), χ,y ∈ D,	(3)
where ` > 0 is a parameter controlling the length-scale of the GP. This kernel is isotropic as it only
depends on |x - y|, is infinitely differentiable, and its eigenvalues decay supergeometrically to 0.
Since the bound in Theorem 2 degrades as the ratio λ∖∕λj increases for j ≥ k +1, the randomized
SVD for learning HS operators prefers covariance kernels with slowly decaying eigenvalues. Our
randomized SVD cannot hope to learn HS operators where the range of the operator has a rank
greater than k, where k is such that the kth eigenvalue of KSE reaches machine precision.
Other popular kernels for GPS include the Matem kernel (Rasmussen & Williams, 2006, Chapt. 4)
and Brownian bridge (Nelsen & Stuart, 2021). Prior information on the HS operator can also be
enforced through the choice of the covariance kernel. For instance, one can impose the periodicity
of the samples by using the following squared-exponential periodic kernel:
KPer (x,y) = exp (-'2 sin2 (X-^y
x, y ∈ D,
where ` > 0 is the length-scale parameter.
4.1	Sample random functions from a Gaussian process
In finite dimensions, a random vector U 〜 N(0, K), where K ∈ Rn×n is a covariance matrix with
Cholesky factorization K = LL*, can be generated from the matrix-vector product u = Lc. Here,
c ∈ Rn is a vector whose entries follow the standard Gaussian distribution. We now detail how this
process extends to infinite dimensions with a continuous covariance kernel. Let K be a continuous
symmetric positive-definite covariance function defined on the domain [a, b] × [a, b] ⊂ R2 with
-∞ < a < b < ∞. We consider the continuous analogue of the Cholesky factorization to write K
as (Townsend & Trefethen, 2015)
∞
K(x, y) =	rj (x)rj (y) = Lc(x)Lc*(y), x,y ∈ [a, b],
j=1
where rj is the jth row of Lc, which —in Chebfun’s terminology— is a lower-triangular quasi-
matrix. In practice, we truncate the series after n terms, either arbitrarily or when the nth largest
kernel eigenvalue, λn, falls below machine precision. Then, ifc ∈ Rn follows the standard Gaussian
distribution, a function u can be sampled from GP(0, K) as u = Lcc. That is,
n
u(x) =	cj rj (x), x ∈ [a, b].
j=1
The continuous Cholesky factorization is implemented in Chebfun2 (Townsend & Trefethen, 2013),
which is the extension of Chebfun for computing with two-dimensional functions. As an example,
4
Published as a conference paper at ICLR 2022
the polynomial approximation, which is accurate up to essentially machine precision, of the squared-
exponential covariance kernel KSE with parameter ` = 0.01 on [-1, 1]2 yields a numerical rank of
n = 503. The functions sampled from GP(0, KSE) become more oscillatory as the length-scale
parameter ` decreases and hence the numerical rank of the kernel increases or, equivalently, the
associated eigenvalues sequence {λj} decays slower to zero.
4.2	Influence of the kernel’ s eigenvalues and Mercer’ s representation
The covariance kernel can also be defined from its Mercer’s representation as
∞
K(x, y) =	λj ψj (x)ψj (y), x, y ∈ D,	(4)
j=1
where {ψj} is an orthonormal basis of L2(D) and λι ≥ λ2 ≥ … > 0 (Hsing & Eubank, 2015,
Thm. 4.6.5). We prefer to construct K directly from Mercer’s representations for several reasons:
1. One can impose prior knowledge of the kernel of the HS operator on the eigenfunctions of K
(such as periodicity or smoothness), 2. One can often generate samples from GP(0, K) efficiently
using Equation (4), and 3. One can control the decay rate of the eigenvalues of K,
Hence, the quantity γk in the probability bound of Theorem 2 measures the quality of the covariance
kernel K in GP(0, K) to generate random functions that can learn the HS operator F. To minimize
1∕γk We would like to select the eigenvalues λι ≥ λ? ≥ ∙∙∙ > 0 of K so that they have the SloW-
est possible decay rate while maintaining Pj∞=1 λj < ∞. One needs {λj} ∈ `1 to guarantee that
ω ~ GP(0, K) has finite expected squared L2-norm, i.e.,旧[|3任(。)]= P∞=ι λj < ∞. The best
sequence of eigenvalues we know that satisfies this property is called the Rissanen sequence (Rissa-
nen, 1983) and is given by λj = Rj := 2-L(j), where
∞∞
L(j) = log2(c0)+ log2(j),	log2(j) = Xmax(log2i)(j), 0),	co = X2-log加，
i=2	i=2
and log2i)(j) = log2 ◦••• ◦ log2(j) is the composition of log2(∙) i times. Other options for the
choice of eigenvalues include any sequence of the form λj = j-ν for ν > 1.
4.3	Jacobi covariance kernel
If D = [-1, 1], then a natural choice of orthonormal basis of L2(D) to define the Mercer’s repre-
sentation of the kernel are weighted Jacobi polynomials (Deheuvels & Martynov, 2008; Olver et al.,
2010). That is, fora weight function wα,β(x) = (1 - x)α(1 +x)β with α, β > -1, and any positive
eigenvalue sequence {λj}, we consider the Jacobi kernel
∞
Kjac,β)(x,y) = X λj+ιwαβ2(X)Pj(α,β)(X)Wa/β⑻Pj(α,β)⑻，x,y ∈ [-1,1],	⑸
j=0
where Pja⑻ is the scaled Jacobi polynomial of degree j. The polynomials are normalized such
that kWaβ2^Pja,β) ∣∣l2([-i,i]) = 1 and KJac,β) ∈ L2([-1,1]2). In this case, a random function can be
sampled as
∞
U(X) = X Pλj+ιcjWa正"⑻(X) X ∈ [—1,1],
j=0
where Cj ~ N(0,1) for 0 ≤ j ≤ ∞.
A desirable property of a covariance kernel is to be unbiased towards one spatial direction, i.e.,
K(X, y) = K(-y, -X) forX,y ∈ [-1, 1], which
is desirable to have the eigenfunctions of KJ(aac,β)
motivates us to always select α = β. Moreover, it
to be polynomial so that one can generate samples
from GP(0, K) efficiently. This leads us to choose α and β to be even integers. The choice of
α = β = 0 gives the Legendre kernel (Foster et al., 2020; Habermann, 2021). In Section 5, we
use Equation (5) with α = β = 2 (see Figure 1) to ensure that functions sampled from the associated
GP satisfy homogeneous Dirichlet boundary conditions. In this case, one must
so that the series of functions in Equation (5) converges uniformly and KJ(a2c,2)
have j∞=1 jλj < ∞
is a continuous kernel
5
Published as a conference paper at ICLR 2022
(see Appendix B). Under this additional constraint, the best choice of eigenvalues is given by a
scaled Rissanen sequence: λj = Rj /j, for j ≥ 1 (cf. Section 4.2). Covariance kernels on higher
dimensional domains of the form D = [-1, 1]d, for d ≥ 2, can be defined using tensor products of
weighted Jacobi polynomials.
4.4	Smoothness of functions sampled from a GP with Jacobi kernel
(2,2)
We now connect the decay rate of the eigenvalues of the Jacobi covariance kernel KJac,	to the
(2 2)
smoothness of the samples from GP (0, KJac ). Hence, the Jacobi covariance function allows the
control of the decay rate of the eigenvalues {λj} as well as the smoothness of the resulting randomly
generated functions. First, Lemma 3 asserts that if the coefficients of an infinite polynomial series
have sufficient decay, then the resulting series is smooth with regularity depending on the decay rate.
This result can be seen as a reciprocal to (Trefethen, 2019, Thm. 7.1).
Lemma 3 Let {pj} be a family of polynomials such that deg(pj) ≤ j and maxx∈[-1,1] |pj (x)| = 1.
If fn(x)	=	jn=0 aj pj (x)	with	|aj |	≤ j-ν	for ν >	1,	then	fn	converges uniformly to	f(x)	=
∞∞=0aj aj Pj (x) and f is μ times continuously differentiable for any integer μ such that μ < (V-1)/2.
Note that the main application of this lemma occurs when deg(pj ) = j for all j ≥ 0. The proof
of Lemma 3 is deferred to the supplementary material. We now state the following theorem about
the regularity of functions sampled from GP(0, KJ(a2c,2) ), which guarantees that if the eigenvalues
are chosen such that λj∙ = O(1∕jν) with ν > 3, then f 〜GP(0, KJ2,2)) is almost surely Continu-
ous. Moreover, a faster decay of the eigenvalues of KJ(a2c,2) implies higher regularity of the sampled
functions, in an almost sure sense.
Theorem 4 Let {λj∙} ∈ '1 (R+) be a positive SequenCe such that λj∙ = O(j-ν) for ν > 3. If f is
(2 2)
sampled from GP (0, K,a； j), then f ∈ Cμ ([-1,1]) almost surely for any integer μ < (V - 3)/2.
This theorem can be seen as a particular case of Driscoll’s zero-one law (Driscoll, 1973), which
characterizes the regularity of functions samples from GPs (see also Kanagawa et al., 2018).
Xxx
Figure 1: Covariance kernel KJ(a2c,2) constructed using Jacobi polynomials of type (2, 2) with
λj = 1/j4, 1/j3, and Rj/j, where Rj is the Rissanen sequence (top). The bottom panels illus-
trate functions sampled from GP(0, KJ(a2c,2)) with the different eigenvalue sequences. The series for
generating the random functions are truncated to n = 500.
In Figure 1, we display the Jacobi kernel of type (2, 2) with functions sampled from the correspond-
ing GP. We selected eigenvalue sequences of different decay rates: from the faster 1/j4 to the slower
Rissanen sequence Rj/j (Section 4.2). For λj = 1/j3 and λj = Rj/j, we observe a large variation
of the randomly generated functions near x = ±1, indicating a potential discontinuity of the sam-
ples at these two points as n → ∞. This is in agreement with Theorem 4, which only guarantees
continuity (with probability one) of the randomly generated functions if λj∙〜1∕jν with ν > 3.
6
Published as a conference paper at ICLR 2022
5 Numerical experiments
5.1	Approximation of matrices using non-standard covariance functions
The approximation error bound in Theorem 2 depends on the eigenvalues of the covariance matrix,
which dictates the distribution of the column vectors of the input matrix Ω. Roughly speaking, the
more prior knowledge of the matrix A that can be incorporated into the covariance matrix, the better.
In this numerical example, We investigate whether the standard randomized SVD, which uses the
identity as its covariance matrix, can be improved by using a different covariance matrix. We then
attempt to learn the discretized 2000 X 2000 matrix, i.e., the discrete Green,s function, of the inverse
of the following differential operator:
Lu = d2u∕dx2 — 100sin(5πx)u,	x ∈ [0,1].
We vary the number of columns (i.e. samples from the GP) in the input matrix Ω from 1 to 2000.
3
rorre noitamixorppa tseB / rorrE
-----Standard covariance
-----Prior covariance
-----Best approximation
00
,
2
0
50
,
1
0
00
,
1
0
50
Number of samples
543210
....................
00000
)s( emit lanoitatupmoC
Figure 2: Left: Ratio between the average randomized SVD approximation error (over 10 runs) of
the 2000 × 2000 matrix of the inverse of the differential operator Lu = d2u/dx2 - 100 sin(5πx)u
on [0, 1], and the best approximation error. The error bars in light colour (blue and red) illustrate
one standard deviation. Right: Average computational time of the algorithm (over 10 runs). The
eigenvalue decomposition of the covariance matrix has been precomputed before.
In Figure 2(left), we compare the ratios between the relative error in the Frobenius norm given by
the randomized SVD and the best approximation error, obtained by truncating the SVD of A. The
prior covariance matrix K consists of the discretized 2000 × 2000 matrix of the Green’s function of
the negative Laplace operator Lu = -d2u/dx2 on [0, 1] to incorporate knowledge of the diffusion
term in the matrix A. We see that a nonstandard covariance matrix leads to a higher approximation
accuracy, with a reduction of the error by a factor of 1.3-1.6 compared to the standard randomized
SVD. At the same time, the procedure is only 20% slower1 on average (Figure 2 (right)) as one
can precompute the eigenvalue decomposition of the covariance matrix (see Appendix C). It is of
interest to maximize the accuracy of the approximation matrix from a limited number of samples in
applications where the sampling time is much higher than the numerical linear algebra costs.
5.2	Applications of THE randomized SVD FOR HILBERT-SCHMIDT operators
We now apply the randomized SVD for HS operators to learn kernels of integral operators. In this
first example, the kernel is defined as (Townsend, 2013)
G(x, y) = cos(10(x2 + y)) sin(10(x + y2)),	x, y ∈ [-1, 1],
and is displayed in Figure 3(a). We employ the squared-exponential covariance kernel KSE with
parameter ` = 0.01 and k = 100 samples (see Equation (3)) to sample random functions from the
associated Gp. The learned kernel Gk is represented on the bottom panel of Figure 3(a) and has an
approximation error around machine precision.
1Timings were performed on an Intel Xeon CPU E5-2667 v2 @ 3.30GHz using MATLAB R2020b without
explicit parallelization.
7
Published as a conference paper at ICLR 2022
Io0J°M IoUJ°M poUJBou
Figure 3: Kernels of three HS operators (top) together with the kernels learned by the randomized
SVD for HS operators (bottom), using the squared-exponential covariance kernel KSE with param-
eter ` = 0.01 and one hundred functions sampled from GP(0, KSE).
As a second application of the randomized SVD for HS operators, we learn the kernel G(x, y)
Ai(-13(x2y + y2)) for x, y ∈ [-1, 1], where Ai is the Airy function (NIS, Chapt. 9) defined by
Ai(x)
1 1 cos (t3 + Xt
π0	3
dt,
x ∈ R.
We plot the kernel and its low-rank approximant given by the randomized SVD for HS op
2	-14
Figure 3(b) and obtain an approximation error (measured in the L -norm) of 5.04 × 10
erators in
. The two
kernels have a numerical rank equal to 42.
The last example consists of learning the HS operator associated with the kernel G(x, y) =
J0(100(xy+y2)) for x, y ∈ [-1, 1], where J0 is the Bessel function of the first kind (NIS, Chapt. 10)
defined as	π
Jo(x) =	cos(x Sin t) dt,	X ∈ R,
π0
and plotted in Figure 3(c). The rank of this kernel is equal to 91 while its approximation is of rank
89 and the approximation error is equal to 4.88 × 10-13. We observe that in the three numerical
examples displayed in Figure 3, the difference between the learned and the original kernel is visually
not perceptible.
Finally, we evaluate the influence of the choice of covariance kernel and number of samples in
Figure 4. Here, we vary the number of samples from k = 1 to k = 100 and use the randomized
SVD for HS operators with four different covariance kernels: the squared-exponential KSE with
parameters ` = 0.01, 0.1, 1, and the Jacobi kernel KJ(a2c,2) with eigenvalues λj = 1/j3, for j ≥ 1. In
the left panel of Figure 4, we represent the eigenvalue ratio λj /λ1 of the four kernels and observe
that this quantity falls below machine precision for the squared-exponential kernel with ` = 1 and
` = 0.1 at j = 13 and j = 59, respectively. In Figure 4 (right), we observe that these two
kernels fail to approximate kernels of high numerical rank. The other two kernels have a much
slower decay of eigenvalues and can capture (or learn) more complicated kernels. We then see in
the right panel of Figure 4 that the relative approximation errors obtained using KJ(a2c,2) and KSE are
close to the best approximation error given by the squared tail of the singular values of the integral
kernel G(x,y), i.e., (Pj≥k+ι σj)1/2. The overshoot in the error at k = 100 compared to the
machine precision is due to the decay of the eigenvalues of the covariance kernels. Hence, spatial
directions associated with small eigenvalues are harder to learn accurately. This issue does not arise
in finite dimensions with the standard randomized SVD because the covariance kernel used there
8
Published as a conference paper at ICLR 2022
Figure 4: Left: Scaled eigenvalues of the Jacobi covariance kernel KJ(a2c,2) with sequence λj = 1/j3
and squared-exponential kernels KSE with parameters ` = 0.01, 0.1, 1, respectively. Right: Average
(over 10 runs) relative approximation error in the L2-norm between the Bessel kernel G(x, y) =
J0 (100(xy + y2)) and its low-rank approximation Gk (x, y), obtained from the randomized SVD
by sampling the GPs k times. The error bars in light colour (blue and red) illustrate one standard
deviation and the black line indicates the best approximation error given by the tail of the singular
values of G.
is isotropic, i.e., all its eigenvalues are equal to one. However, this choice is no longer possible for
learning HS integral operators as the covariance kernel K must be squared-integrable. The relative
approximation errors at k = 100 (averaged over 10 runs) using KJ(a2c,2) and KSE with ` = 0.01 are
Error(KJ(a2c,2)) ≈ 2.6 × 10-11, and Error(KSE) ≈ 5.7 × 10-13, which gives a ratio of
Error(KJ(a2c,2))/Error(KSE) ≈ 45.6.	(6)
However, the square-root of the ratio of the quality of the two kernels for k = 91 is equal to
,Y9i(KsE )∕Y91(J,2)) ≈ 117.8,	(7)
which is of the same order of magnitude of Equation (6) as predicted by Theorem 2. In Equation (7),
γ91(KsE)≈	5.88 × 10-2 and γ91(KJ(a2c,2))≈	4.24 × 10-6 are both computed using Chebfun.
In conclusion, this section provides numerical insights to motivate the choice of the covariance
kernel to learn Hs operators. Following Figure 4, a kernel with slowly decaying eigenvalues is
preferable and yields better approximation errors or higher learning rate with respect to the number
of samples, especially when learning a kernel with a large numerical rank. The optimal choice from
a theoretical viewpoint is to select a covariance kernel whose eigenvalues have a decay rate similar
to the Rissanen sequence (Rissanen, 1983), but other choices may be preferable in practice to ensure
smoothness of the sample functions (cf. section 4.4).
6 Conclusions
We have explored practical extensions of the randomized sVD to nonstandard covariance matri-
ces and Hilbert-SChmidt operators. This paper motivates new computational and algorithmic ap-
proaches for preconditioning the covariance kernel based on prior information to compute a low-
rank approximation of matrices and impose properties on the learned matrix and random functions
from the GP. Numerical experiments demonstrate that covariance matrices with prior knowledge
outperform the standard identity matrix used in the literature and lead to near-optimal approxima-
tion errors. In addition, we proposed a covariance kernel based on weighted Jacobi polynomials,
which allows the control of the smoothness of the random functions generated and may find prac-
tical applications in PDE learning (Boulle et al., 2020; 2021) as it imposes prior knowledge of
Dirichlet boundary conditions. The algorithm presented in this paper is limited to matrices and Hs
operators and does not extend to unbounded operators such as differential operators. Additionally,
the theoretical bounds only offer probabilistic guarantees for Gaussian inputs, while sub-Gaussian
distributions (Kahane, 1960) of the inputs would be closer to realistic application settings.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank Joel Tropp for his suggestions leading to Lemma 7 and Daniel Kressner for his com-
ments. This work was supported by the EPSRC Centre For Doctoral Training in Industrially Fo-
cused Mathematical Modelling (EP/L015803/1) in collaboration with Simula Research Laboratory,
and the National Science Foundation grants DMS-1818757, DMS-1952757, and DMS-2045646.
References
NIST Digital Library of Mathematical Functions. http://dlmf.nist.gov/, Release 1.1.0 of 2020-12-15.
F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert, C. W. Clark,
B. R. Miller, B. V. Saunders, H. S. Cohl, and M. A. McClain, eds.
O. Alter, P. O. Brown, and D. Botstein. Singular value decomposition for genome-wide expression
data processing and modeling. Proc. Natl. Acad. Sci., 97(18):10101-10106, 2000.
N. BoUlle and A. Townsend. Learning elliptic partial differential equations with randomized linear
algebra. Found. Comput. Math., 2022.
N. Boulle, Y. Nakatsukasa, and A. Townsend. Rational neural networks. In Proc. Advances in
Neural Information Processing Systems, volume 33, pp. 14243-14253, 2020.
N. Boulle, C. J. Earls, and A. Townsend. Data-driven discovery of physical laws with human-
understandable deep learning. arXiv preprint arXiv:2105.00266, 2021.
H. Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. Ann. Math. Stat., pp. 493-507, 1952.
P. Deheuvels and G. V. Martynov. A Karhunen-Loeve decomposition of a Gaussian process gener-
ated by independent pairs of exponential random variables. J. Funct. Anal., 255(9):2363-2394,
2008.
M. F. Driscoll. The reproducing kernel Hilbert space structure of the sample paths of a Gaussian
process. Z. Wahrscheinlichkeitstheorie, 26(4):309-316, 1973.
T. A Driscoll, N. Hale, and L. N. Trefethen. Chebfun Guide. Pafnuty Publications, 2014. URL
http://www.chebfun.org/docs/guide/.
R. Durrett. Probability: Theory and Examples. Cambridge University Press, 5th edition, 2019.
C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,
1(3):211-218, 1936.
J.	Foster, T. Lyons, and H. Oberhauser. An optimal polynomial approximation of Brownian motion.
SIAM J. Numer. Anal., 58(3):1393-1421, 2020.
G. H. Golub and C. F. Van Loan. Matrix Computations. JHU Press, 4th edition, 2013.
K.	Habermann. A semicircle law and decorrelation phenomena for iterated Kolmogorov loops. J.
London Math. Soc., 103(2):558-586, 2021.
N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic al-
gorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217-288, 2011.
T. Hsing and R. Eubank. Theoretical foundations of functional data analysis, with an introduction
to linear operators. John Wiley & Sons, 2015.
J.-P. Kahane. PrOPrieteS locales des fonctions a series de Fourier aleatoires. Stud. Math., 19(1):
1-25, 1960.
M. Kanagawa, P. Hennig, D. Sejdinovic, and B. K. Sriperumbudur. Gaussian processes and kernel
methods: A review on connections and equivalences. arXiv preprint arXiv:1807.02582, 2018.
K. Karhunen. Uber lineare Methoden in der Wahrscheinlichkeitsrechnung. Ann. Acad. Science
Fenn., Ser. A. I., 37:3-79, 1946.
10
Published as a conference paper at ICLR 2022
T. Kato. Perturbation theory for linear operators, volume 132. Springer Science & Business Media,
2013.
F. C. Leone, L. S. Nelson, and R. B. Nottingham. The folded normal distribution. Technometrics, 3
(4):543-550,1961.
M. Loeve. Fonctions aleatoire de second ordre. Rev. Sci., 84:195-206, 1946.
A. A. Markov. On a question by D. I. Mendeleev. Zapiski Imp. Akad. Nauk, 62(12):1-24, 1889.
P.-G. Martinsson and J. A. Tropp. Randomized numerical linear algebra: Foundations and algo-
rithms. Acta Numer., 29:403572, 2020.
L. Mirsky. Symmetric gauge functions and unitarily invariant norms. Q. J. Math., 11(1):50-59,
1960.
L. Mirsky. A trace inequality of John von Neumann. Monatsh. Math., 79(4):303-306, 1975.
Y. Nakatsukasa. Fast and stable randomized low-rank matrix approximation. arXiv preprint
arXiv:2009.11392, 2020.
N. H. Nelsen and A. M. Stuart. The random feature model for input-output maps between Banach
spaces. SIAM J. Sci. Comput., 43(5):A3212-A3243, 2021.
E.	J. Nystrom. Uber die Praktische Auflosung von Integralgleichungen mit AnWendUngen auf
randwertaufgaben. Acta Math., 54:185-204, 1930.
F.	W. J. Olver, D. W. Lozier, R. F. Boisvert, and C. W. Clark. NIST Handbook of Mathematical
Functions . Cambridge University Press, 2010.
A. Paterek. ImProving regularized singular value decomPosition for collaborative filtering. In Proc.
KDD cup and workshop, PP. 5-8, 2007.
C. E. Rasmussen and C. Williams. Gaussian processes for machine learning. MIT Press, 2006.
J. Rissanen. A universal Prior for integers and estimation by minimum descriPtion length. Ann.
Stat., 11:416-431, 1983.
W. Rudin. Principles of mathematical analysis. McGraW-Hill, 3rd edition, 1976.
A. ToWnsend. Pretty functions aPProximated by Chebfun2. https://www.chebfun.org/
examples/approx2/PrettyFunctions.html, 2013.
A. ToWnsend and L. N. Trefethen. An extension of Chebfun to tWo dimensions. SIAM J. Sci.
Comput., 35(6):C495-C518, 2013.
A. ToWnsend and L. N. Trefethen. Continuous analogues of matrix factorizations. P. Roy. Soc. A,
471(2173):20140585, 2015.
L. N. Trefethen. Approximation Theory and Approximation Practice. SIAM, extended edition, 2019.
A.-J. Van Der Veen, E. F. DePrettere, and A. L. SWindlehurst. SubsPace-based signal analysis using
singular value decomPosition. Proc. IEEE, 81(9):1277-1308, 1993.
J. Von Neumann. Some matrix-inequalities and metrization of matric sPace. Tomsk Univ. Rev., 1:
286-300, 1937.
C. Williams and M. Seeger. Using the NyStrom method to speed up kernel machines. In Proc.
Advances in Neural Information Processing Systems, volume 14, PP. 682-688, 2001.
S. Wold, K. Esbensen, and P. Geladi. Principal component analysis. Chemometr. Intell. Lab., 2(1-3):
37-52, 1987.
11
Published as a conference paper at ICLR 2022
A Randomized SVD with multivariate Gaussian inputs
Let m ≥ n ≥ 1 and A be an m X n real matrix with singular value decomposition A = UΣV*,
where U and V are orthonormal matrices, and Σ be an m × n diagonal matrix with entries σ1(A) ≥
…≥ σn(A) > 0. For a fixed target rank k ≥ 1, we define Σ1 and Σ2 to be the diagonal
matrices, which respectively contain the first k singular values of A: σι(A) ≥ ∙∙∙ ≥ σk(A),
and the remaining singular values. Let V1 be the n × k matrix obtained by truncating V1 after k
columns and V2 the remainder. In this section, K denotes a symmetric positive semi-definite n × n
matrix and Ω ∈ Rn×' a Gaussian random matrix with ' ≥ k independent columns sampled from
a multivariate normal distribution with covariance matrix K. Finally, We define Ωι := V1Ω and
Ω2 := V2Ω. We first refine (Boulle & Townsend, 2022, Lem. 5).
Lemma 5 Let' ≥ 1, Ω ∈ Rn×' be a Gaussian random matrix, where each column is Sampledfrom
a multivariate Gaussian distribution with covariance matrix K, and T be an ` × k matrix. Then,
E[k∑2V2ΩTkF] = Tr(∑2v2KV2)kTkF.	(8)
Proof. Let K = QkΛQK be the eigenvalue decomposition of K, where QK is orthonormal and
Λ is a diagonal matrix containing the eigenvalues of K in decreasing order: λι ≥ ∙∙∙ ≥ λn ≥ 0.
We note that Ω can be expressed as Ω = QkA1/2G, where G is a standard Gaussian matrix.
Let S = ∑2V2QκΛ1/2, the proof follows from (HalkO et al., 2011, Prop. A.1), which shows that
EkSGTk2 = kSk2kTkF.・
Note that one can bound the term Tr(Σ2V^KV2) by λ1k∑2kF2, where λι is the largest eigenvalue of
K (Boulle & Townsend, 2022). While this provides a simple upper bound, it does not demonstrate
that the use of a covariance matrix containing prior information on the singular vectors of A can
outperform the randomized SVD with standard Gaussian inputs. Then, combining the proof of
(Boulle & Townsend, 2022, Thm. 1) and Lemma 5, we prove the following proposition, which
bounds the expected approximation error of the randomized SVD with multivariate Gaussian inputs.
Proposition 6 Let A be an m × n matrix, k ≥ 1 an integer, and choose an oversampling parameter
P ≥ 2. If Ω ∈ Rn×(k+p) is a Gaussian random matrix, where each column is sampled from a
multivariate Gaussian distribution with CovarianCe matrix K ∈ Rn×n, and QR = AΩ is the
economized QR decomposition of AΩ, then,
ElkA -QQ*AkF] ≤ (1+S ¥(P-Y)I! U jX1σ2(A),
where Yk = k∕(λι Tr((V*KV1)-1))) andβk = Tr(∑2V*KV2)∕(λ1k∑2kF).
We remark that for standard Gaussian inputs, we have γk = βk = 1 in Proposition 6, and we recover
the average Frobenius error of the randomized SVD (Halko et al., 2011, Thm. 10.5) up to a factor
of (k + P) due to the non-independence of Ωι and Ω? in general.
Lemma 7 With the notations introduced at the beginning of the section, for all s ≥ 0, we have
P {k∑2Ω2kF > '(1 + S) Tr(∑2V*KV2)} ≤ (1 + Sf/e-s'/.
Proof. Let ωj be the jth column of Ω for 1 ≤ j ≤ ' and vι,...,vn be the n columns of the
orthonormal matrix V. We first remark that
`	n-k
kΩ2kF = XZj,	Zj = X σ2+nι(A)(v*+nιωi)2,	⑼
j=1	n1 =1
where the Zj are i.i.d because ωj 〜N(0, K) are i.i.d. Let λι ≥ ∙∙∙ ≥ λn ≥ 0 be the eigenvalues
of K with eigenvectors ψ1, . . . , ψn ∈ Rn. For 1 ≤ j ≤ `, we have,
n
ωj = X(c(j))2 √λiψi,
i=1
where c(j) ~ N(0,1) are i.i.d for 1 ≤ i ≤ n and 1 ≤ j ≤ '. Then,
n	n-k	n
Zj=X(ci(j))2λiXσk2+n1(A)(vk*+n1ψi)2=XXi
i=1	n1 =1	i=1
12
Published as a conference paper at ICLR 2022
where the Xi are independent. Let Yi = λ% PnuI σ2+n1 (A)(vk+n1 ψi)2, then Xi 〜IiX for
1 ≤ i ≤ n.
Let 0 < θ < 1/(2 Pin=1 γi), we can bound the moment generating function of Pin=1 Xi as
n	n	n	-1/2
E [eθ Pn=I Xi] = Y E [eθχi] = Y(I- 2θYi)-1/2 ≤	1 - 2θ X Yi
i=1	i=1	i=1
because the Xi /Yi are independent and follow a chi-squared distribution. The right inequality is
obtained by showing by recurrence that, ifa1, . . . , an ≥ 0 are such that Pin=1 ai ≤ 1, then Qin=1(1-
ai) ≥ 1 - Pin=1 ai. For convenience, we define C1 := Pin=1 Yn, we have shown that
E [eθZj] ≤ (1 — 2θCι)-1/2.
Moreover, we find that
n-k	n	n-k
Cl = E σ2+nι vk+nι Eψ∕ψi Vk+nι = E。2十nι (A)vk+nι Kvk+nι
n1 =1	i=1	n1 =1
=Tr(∑2V2KV2).
Let s ≥ 0 and 0 < θ < 1/(2C1), by Chernoff bound (Chernoff, 1952, Thm. 1), we obtain
P {k∑2Ω2kF > '(1 + s)C1} ≤ e-(1+S)C1'θE [eθZj]'
=e-(1+S)C1'θ (1 — 2θCι)-'/2.
We minimize the bound over 0 < θ < 1/(2 Tr(K)) by choosing θ = s/(2(1 + s)C1), which gives
P {k∑2Ω2∣∣2 >'(1 + s)C1} ≤ (1 + s)'/2e-'s/2.
The proof of Theorem 2 will require to bound the term ∣∣ωJ ∣∣F, which We achieve using the following
result (BoUlle & Townsend, 2022, Lem. 3).
Lemma 8 (Boulle & ToWnsend 2022) Let k, ` ≥ 1 such that ` - k ≥ 4, with the notations intro-
duced at the beginning of the section, for all t ≥ 1, we have
P {kΩlkF > 3t2 Tr((Vj K VII)T) }≤ t-('-k).
We now prove Theorem 2, which provides a refined probability bound for the performance of the
generalized randomized SVD on matrices.
Proof of Theorem 2. Using (BOUlle & Townsend, 2022, Thm. 2) and the SUbmUltiPlicativity of the
Frobenius norm, we have
IlA — QQ*A∣∣F ≤ k∑2kF + k∑2Ω2kF∣∣ΩlkF.	(10)
Let' = k + P with P ≥ 4, combining Lemmas 7 and 8 to bound the terms ∣∣∑2Ω2kF and ∣ΩjkF in
Equation (10) yields the following probability estimate:
∣A — QQ*A∣∣F ≤ k∑2kF + 3t2(1 + S)k+Γ Tr((V；KVI)T) Tr(∑2VRV2)
P+1
≤ ("Md + S)Tβk) £ σ2(A),
P+ 1 Yk j=k+1 j
with failure probability at most t-p + (1 + S)(k+p)/2e-S(k+p)/2 . Note that we introduced Yk :=
k∕(λj Tr((V；KVI)T))) and βk := Tr(∑2V2KV2)∕(λι∣∑2kF). We conclude the proof by
defining U = √1 + s ≥ 1. ■
The following Lemma provides an estimate of the quantity βk introduced in the statement of Theo-
rem 2.
13
Published as a conference paper at ICLR 2022
Lemma 9 Let βk = Tr(∑2v3KV2)∕(λ1k∑2∣∣F), then the following inequality holds
nn
βk ≤ X j。米A X σ2(A).
j=k+1 1	j=k+1
Proof. Let μι ≥ ∙∙∙ ≥ μn-k be the eigenvalues of the matrix V2KV2. Using Von Neumann's
trace inequality (Mirsky, 1975; Von Neumann, 1937), we have
n
Tr(∑2V2KV2) ≤ X μj-kσj(A).
j=k+1
Then, the matrix VgKV? is a principal submatrix of V*KV, which has the same eigenvalues of
K. Therefore, by (Kato, 2013, Thm. 6.46), the eigenvalues of VgKV2 are individually bounded by
the eigenvalues of K, i.e., μj ≤ λj for 1 ≤ j ≤ n - k, which concludes the proof. ■
Finally, we highlight that the statement of Theorem 2 can be simplified by choosing p = 5, t = 4,
and u = 3.
Corollary 1 (Generalized randomized SVD) Let A be an n × n matrix and k ≥ 1 an integer. If
Ω ∈ Rn×(k+5) is a Gaussian random matrix, where each column is sampled from a multivariate
Gaussian distribution with symmetric positive semi-definite covariance matrix K ∈ Rn×n, and
QR = AΩ is the economized QR decomposition of AΩ, then
kA - QQgAkF
k(k + 5)βk
γk
P
∖
n
X σj2 (A)
j=k+1
≥ 0.999.
In contrast, a simplification of Theorem 1 by choosing t = 6 and u = 4 gives the following result.
Corollary 2 (Randomized SVD) Let A be an n X n matrix and k ≥ 1 an integer. If Ω ∈ Rn×(k+5)
is a standard Gaussian random matrix and QR = AΩ is the economized QR decomposition of AΩ,
then
P kA - QQ*A∣∣f ≤ (1 + I6√k + 5)
n
X σj2(A)
j=k+1
≥ 0.999.
B Continuity of the Jacob i kernel
Here, we show that if the kernel’s eigenvalue sequence, {λj}, is such that Pj∞=1 jλj< ∞, then
the Jacobi kernel K(2,2) defined by Equation (5) is continuous. First, note that 42,2) is a scaled
(5/2)
ultraspherical polynomial Cj with parameter 5/2 and degree j ≥ 0 so it can be bounded by the
following proposition.
Proposition 10 Let Cj5/2) be the ultraspherical polynomial of degree j with parameter 5/2, nor-
malized such that R-ι(1 — x2)2Cj5/2)(x)2 dx = 1. Then,
max |(1 - x2)Cj5/2)(x)| ≤ 2，j + 5/12, j ≥ 0.	(11)
x∈[-1,1]
Proof. Let j ≥ 0 and x ∈ [-1, 1], according to (NIS, Table 18.3.1),
『(X) = 31L 、，j +5/2 、，	、C(5/2)(x),
j	V(j + I)Cj +2)(j +3)(j +4) j
where Cj(5/2)(x) is the standard ultraspherical polynomial. Using (NIS, (18.9.8)), we have
2	(5/2)	(j + 3)(j + 4)Cj(3/2)(x) - (j + 1)(j + 2)Cj(+3/22)(x)
(I - X )Cj	(X)=	6(j + 5/2)	.
(12)
14
Published as a conference paper at ICLR 2022
By using (NIS, (18.9.7)), we have (Cj(3+/22) (x) - Cj(3/2)(x))/2 = (j + 5/2)Cj(1+/22) (x) and hence,
(1 - χ2)c(≡∕2)(χ) = 3号3/2)(X)- j + 13j + 2) C*22)(x).
We bound the two terms with (NIS, (18.14.4)) to obtain the following inequalities:
Qj3/2)(x)| ≤ j + 1)2j + 2),	∣C⅛∕2)(x)∣ ≤ 1.
Hence, |(1 - x2)cj5/2)(x)| ≤ 2j + 1)(j + 2)/3 and following Equation (12) We obtain
1(1 - χ2)C(5/2)(x)| ≤ 2 Jj + 1)(j + 2)(j + 5/2) ≤ 2pj + 5/12
|(1 X )Cj (x)| ≤ 2V (j + 3)(j + 4)	≤ 2Vj + 5/12,
which concludes the proof. ■
The bound given in Proposition 10 differs by a factor of 4/3 from the numerically observed upper
bound (1.57j + 5/12). This result shows that if P∞=ι jλj < ∞, then the series of functions in
Equation (5) converges uniformly and KJ(a2c,2) is continuous.
Proof of Lemma 3. By Markov brothers, inequality (Markov, 1889), for all j ≥ 0 and 0 ≤ μ ≤ j,
we have maxχ∈[-ι,i] ∣pjμ)(x)∣ ≤ j2μ. Therefore"”)(x)| ≤ Pf=O Iaj∣∣*)∣∣∞ ≤ P；=oj2μ-ν
so ∣f")(x)∣ < ∞ if μ < (V - 1)/2. The result follows from (RUdin,1976, Thm. 7.17). ■
Proof of Theorem 4. Since f 〜GP(0, KJac,2)), f 〜P∞=0 Cj Pλj+7(1 - x2)42,2)(x), where
Cj 〜N(0,1) for j ≥ 0. Let f denote the truncation of f after n terms. By letting M > 0 be the
constant such that λj+1 ≤ M(j + 1)-ν, we find that
∞
kf - fnk∞ ≤ Sn,	Sn ：= 2√M X ∣Cj-1 j(I-V)/2,
j=n+2
where we used maxχ∈[-ι,i] ∣(1 - x2)Pj(2,2)(x)∣ ≤ 2√j + 1 (cf. Proposition 10). Thus, we have
P lim kf - fnk∞ =0 ≥P lim Sn =0 .
n→∞	n→∞
Here, Sn 〜Xn = P∞=n+2 Yjj(I-V)/2, where Yj follows a half-normal distribution (Leone et al.,
1961) with parameter σ = 1 and the (Yj )j are independent. We want to show that Xn -a-.s→. 0. For
> 0, using Chebyshev’s inequality, we have:
X P(IXnl≥e)	≤ X(1- Π2) X	jV-Γ	≤ 指(1-	Π2) V-2 X	nν-2 ,
n=0	n=0 j=n+2	n=1
which is finite if ν > 3. Therefore, using Borel-Cantelli Lemma (Durrett, 2019, Chapt. 2.3), Xn
converges to 0 almost surely and P(limn→∞ Xn = 0) = 1. Finally,
P lim kf - fnk∞ = 0 ≥ P lim Xn = 0 = 1,
n→∞	n→∞
which proves that {fn } converges uniformly and hence f is continuous with probability one. The
statement for higher order derivatives follows the proof of Lemma 3. ■
C Experiments with non- standard covariance matrices
In this section, we provide more detailed explanations regarding the numerical experiments con-
ducted in Section 5.1.
Sampling a random vector from a multivariate normal distribution with an arbitrary covariance ma-
trix K can be computationally expensive when the dimension, n, of the matrix is large as it requires
15
Published as a conference paper at ICLR 2022
the computation of a Cholesky factorization, which can be done in O(n3) operations. We high-
light that this step can be precomputed once, such that the overhead of the generalized SVD can be
essentially expressed as the cost of an extra matrix-vector multiplication. Then, the difference in
timings between standard and prior covariance matrices is marginal as shown by the right panel of
Figure 2. Additionally, we would like to highlight that prior covariance matrices can be designed
and derived using physical knowledge of the problem, such as a diffusion behaviour of the sys-
tem, which can also significantly decrease the precomputation cost. As an example, in Section 5.1,
we employ the discretized Green’s function of the negative Laplacian operator with homogeneous
Dirichlet boundary conditions, given by Lu = -d2u/dx2 on [0, 1], for which we know the eigen-
value decomposition. Hence, the eigenvalues and normalized eigenfunctions are respectively given
by	ι
λn = r r,	ψn(x) = √2sin(nπx),	X ∈ [0,1], n ≥ 1.
π2n2
Therefore, one can employ Mercer’s representation (see Equation (4)) to sample the random vectors
and precompute the covariance matrix in O(n2) operations. For a problem of size n = 2000, it
takes 0.16s to precompute the matrix.
16