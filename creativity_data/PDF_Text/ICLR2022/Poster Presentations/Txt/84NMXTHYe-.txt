Published as a conference paper at ICLR 2022
Evidential Turing Processes
Melih Kandemir
Dept of Math and Computer Science
University of Southern Denmark
Odense, Denmark
kandemir@imada.sdu.dk
Abdullah Akgm
Department of Computer Engineering
Istanbul Technical University
Istanbul, Turkey
akgula15@itu.edu.tr
Manuel Haussmann*
Department of Computer Science
Aalto University
Espoo, Finland
manuel.haussmann@aalto.fi
Gozde Unal
Department of Computer Engineering
Istanbul Technical University
Istanbul, Turkey
gozde.unal@itu.edu.tr
Ab stract
A probabilistic classifier with reliable predictive uncertainties i) fits successfully
to the target domain data, ii) provides calibrated class probabilities in difficult
regions of the target domain (e.g. class overlap), and iii) accurately identifies
queries coming out of the target domain and rejects them. We introduce an original
combination of Evidential Deep Learning, Neural Processes, and Neural Turing
Machines capable of providing all three essential properties mentioned above for
total uncertainty quantification. We observe our method on five classification tasks
to be the only one that can excel all three aspects of total calibration with a single
standalone predictor. Our unified solution delivers an implementation-friendly and
compute efficient recipe for safety clearance and provides intellectual economy to
an investigation of algorithmic roots of epistemic awareness in deep neural nets.
1	Introduction
The applicability of deep neural nets to safety-critical use cases such as autonomous driving or
medical diagnostics is an active matter of fundamental research (Schwalbe & Schels, 2020). Key
challenges in the development of total safety in deep learning are at least three-fold: i) evaluation of
model fit, ii) risk assessment in difficult regions of the target domain (e.g. class overlap) based on
which safety-preserving fallback functions can be deployed, and iii) rejection of inputs that do not
belong to the target domain, such as an image of a cat presented to a character recognition system.
The attention of the machine learning community to each of these critical safety elements is in steady
increase (Naeini et al., 2015; Guo et al., 2017; Kuleshov et al., 2018). However, the developments
often follow isolated directions and the proposed solutions are largely fragmented. Calibration of
neural net probabilities focuses primarily on post-hoc adjustment algorithms trained on validation
sets from in-domain data (Guo et al., 2017), ignoring the out-of-domain detection and model fit
evaluation aspects. On the other hand, recent advances in out-of-domain detection build on strong
penalization of divergence from the probability mass observed in the target domain (Sensoy et al.,
2018; Malinin & Gales, 2018), distorting the quality of class probability scores. Such fragmentation
of best practices not only hinders their accessibility by real-world applications but also complicates
the scientific inquiry of the underlying reasons behind the sub-optimality of neural net uncertainties.
We aim to identify the guiding principles for Bayesian modeling that could deliver the most complete
set of uncertainty quantification capabilities in one single model. We first characterize Bayesian
models with respect to the relationship of their global and local variables: (a) Parametric Bayesian
Models comprise a likelihood function that maps inputs to outputs via probabilistic global param-
eters, (b) Evidential Bayesian Models apply an uninformative prior distribution on the parameters
of the likelihood function and infer the prior hyperparameters by amortizing on an input observa-
tion. Investigating the decomposition of the predictive distribution variance, we find out that the
* Work done while at Heidelberg University, Germany.
1
Published as a conference paper at ICLR 2022
(a) Parametric Bayesian Model (b) Evidential Bayesian Model (c) Complete Bayesian Model
Figure 1: Plate diagrams of three main Bayesian modeling approaches. Shaded nodes are observed
and unshaded ones are latent. The solid direct arrows denote conditional dependencies in the true
model, while the bent dashed arrows denote amortization in variational inference.
uncertainty characteristics of these two approaches exhibit complementary strengths. We introduce a
third approach that employs a prior on the likelihood parameters that both conditions on the input
observations on the true model and amortizes on them during inference. The mapping from the input
observations to the prior hyperparameters is governed by a random set of global parameters. We refer
to the resulting approach as a (c) Complete Bayesian Model, since its predictive distribution variance
combines the favorable properties of Parametric and Evidential Bayesian Models. Figure 1 gives an
overview of the relationship between these three Bayesian modeling approaches.
The advantages of the Complete Bayesian Models come with the challenge of choosing an input-
dependent prior. We introduce a new stochastic process construction that addresses this problem by
accumulating observations from a context set during training time into the parameters of a global
hyperprior variable by an explicitly designed update rule. We design the input-specific prior on
the likelihood parameters by conditioning it on both the hyperprior and the input observation. As
conditioning via explicit parameter updates amounts to maintaining an external memory that can
be queried by the prior distribution, we refer to the eventual stochastic process as a Turing Process
with inspiration from the earlier work on Neural Turing Machines (Graves et al., 2014) and Neural
Processes (Garnelo et al., 2018b). We arrive at our target Bayesian design that is equipped with the
complete set of uncertainty quantification capabilities by incorporating the Turing Process design into
a Complete Bayesian Model. We refer to the resulting approach as an Evidential Turing Process. We
observe on five real-world classification tasks that the Evidential Turing Process is the only model
that excels simultaneously at model fit, class overlap quantification, and out-of-domain detection.
2 Problem Statement: Total Calibration
Consider the following data generating process with K modes
K
y|n 〜C at (y|n),	x|y 〜EIy=k P(XIy = k),	(I)
k=1
where C at (y∣∙) is a categorical distribution, Iu is the indicator function for predicate u, and p(χ∣y)
is the class-conditional density function on input observation x. For any prior class distribution π ,
the classification problem can be cast as identifying the class probabilities for observed patterns
Pr[y∣x, π] = C at (y∖ftrue(x)) where f∏,ue(x) is a K-dimensional vector with the k-th element
K
ftπr,uke(x) = πkp(x∖y = k) Xπκp(x∖y = κ).	(2)
κ=1
In many real-world cases ftπrue is not known and should be identified from a hypothesis space
hπ ∈ Hπ via inference given a set of samples D = {(xn, yn)∖n = 1, . . . , N} obtained from the true
distribution. The risk of the Bayes classifier arg maxy Cat(y∖hπ(x)) for a given input x is
K
R(hπ (x)) = X
Iκ6=arg max hπ (x) ftr,ue (x).
(3)
κ=1
2
Published as a conference paper at ICLR 2022
For the whole data distribution, We have R(h∏) = Eχ∣∏[R(h∏(x))]. The optimal case is when
hπ(X) = ftπrue(X), which brings about the irreducible Bayes risk resulting from class overlap:
R* (ftrue(X)) = min{1 - max ftπrue(x), max fjue (X) }∙
(4)
Total Calibration. Given a set of test samples Dts coming from the true data generating process in
Eq. 1, we define Total Calibration as the capability of a discriminative predictor hπ (X) to quantify
the following three types of uncertainty simultaneously:
i)	Model misfit evaluated by the similarity of h∏ (x) and f∏∙ue(χ) measurable directly via
KL(C at (y∣ftπrue(x))p(x) || C at (y∣hπ (X))P(X))=
Ep(X) [log C at (y∣f⅛ue(x)] + Ep(X) [- log C at(y∣h∏(x))]
where KL(∙∣∣∙) denotes the Kullback-Leibler divergence. Since Ep(X) [log C at (y∣f^e(x))] is a
constant with respect to hπ(x), an unbiased estimator of the relative goodness of an inferred model is
the negative test log-likelihood NLL(h∏(x)) = -1/|Dts| P(X y)∈DtSlogCat(y∣h∏(x)).
ii)	Class overlap evaluated by R“ftrue(X)). AS there is no way to measure this quantity from Dts,
it is approximated indirectly via Expected Calibration Error (ECE)
M |B |
ECE[ht] = X 看卜Cc(Bm) - Conf(Bm)I
m=1
where Bm = {(n∣h∏(xn) ∈ [(m - 1)/M, m/M]} are M bins that partition the test set Dts into
Dts = Bi ∪∙∙∙∪ BM and are characterized by their accuracy and confidence scores
acc(Bm)
1B1〉: Iyn = arg max h∏ (xn),
conf(Bm)
∣⅛ nXm ht (Xn ).
(5)
iii)	Domain mismatch defines the rarity of an input pattern x* for the target task, that is
Pr[p(x = x*) < e] for small e. This quantity cannot be measured since p(x) is unknown. It is
approximated indirectly as the success of discriminating between samples x* coming from a different
data distribution and those in Dts by calculating the Area Under ROC (AUROC) curve w.r.t. hπ (x).
3 Uncertainty Quantification with Parametric and Evidential
Bayesian Models
Parametric Bayesian Models. A commonplace design choice is to build the hypothesis space
by equipping the hypothesis function with a global parameter θ that takes values from a fea-
sible set Θ, that is Ht = {h∏(x)∣θ ∈ Θ}. Parametric Bayesian Models (PBMS) employ
a prior belief θ 〜p(θ), which is updated via Bayesian inference on a training set Dtr as
p(θ∣Dtr) = Q(X y)∈Dt T C at (y∣h∏ (x))p(θ)/P(Dtr). Consider the update on the posterior belief
p(θ∣Dtr ∪ (x*,y*)) = Cat(y*∣h∏(x*))p(θ∣Dtr)/Z,
after a new observation (x*, y*) where the normalizer can be expressed as
Z
(6)
1
p(Dtr )
/ C at(y*∣h∏ (x*))L(θ)p(θ)dθ.
(7)
with L(θ) = Q(X y)∈Dt T C at (y∣h∏ (x)). This can be seen as an inner product between the functions
Cat(y*∣h∏(x*)) and L(θ) on an embedding space modulated by p(θ). As the sample size grows, the
high-density regions of C at(y* ∣h∏ (x*)) and L(θ) will superpose with higher probability, causing Z
to increase, making p(θ∣Dtr ∪ (x*,y*)) increasingly more peaked, and consequently we will have
lim
|Dtr l→ + ∞
Var[θ∣Dtr]
0.
(8)
3
Published as a conference paper at ICLR 2022
This conjecture depends on the assumption that a single random variable θ modulates all samples.
The variance of a prediction in favor of a class k decomposes according to the law of total variance
Var[y* = klx*,D] = Varθ 〜p(θ∣Dtr)% (X*)], + Eθ 〜p(θ∣Dtr ) [(1 - h∏,k (Xj)hΠ,k (Xj].⑼
"^^^^^"}	^^}
Reducible model uncertainty
{^^^^^^^"
Data uncertainty
Due to the Eq. 8, the first term on the r.h.s. vanishes in the large sample regime and the second one
coincides with the asymptotic solution of the maximum likelihood estimator θMLE . In cases when
Hπ contains ftπrue(X), an ideal inference scheme has the potential to recover it and we get
N→+∞Eθ〜p(θ∣Dtr) [(1 - h∏,k(χ*))h∏,k(χ*)] =(1 - h∏MkLE(x*))h∏MLE(x*)
=(I- ftrue(x*))ftrue(x*).
The simple proposition below sheds light on the meaning of this result. It points to the fact that the
Bayes risk of a classifier (Eq. 4) and the second component of its predictive variance (Eq 9) are
proportional. This gives a mechanistic explanation for why the second term on the r.h.s. of Eq. 9
quantifies class overlap. Despite the commonplace allocation of the wording data uncertainty for this
term, we are not aware of prior work that derives it formally from basic learning-theoretic concepts.
Proposition. The following inequality holds for any π, π0 ∈ [0.5, 1] pair
min(1 - π, π) ≥ min(1 - π0, π0) ⇒ (1 - π)π ≥ (1 - π0)π0.
Proof. Define π = 0.5+ and π0 = 0.5+0 for , 0 > 0. As min(1 -π, π) ≥ min(1 -π0, π0) ⇒ ≤
0,weget(1-π)π = (1-0.5-)(0.5+) = 0.25-0.25-2 ≥ 0.25 - 0.250 - 02 = (1-π0)π0
The conclusion is that (1 - ftrue(x))ftrue(x) 8 R*(ftrue(x))∙ Hence, the second term on the r.h.s.
of Eq. 9 is caused by the class overlap and it can be used to approximate the Bayes risk. Put together,
the first term of the decomposition reflects the missing knowledge on the optimal value of θ that can
be reduced by increasing the training set size, while the second reflects the uncertainty stemming
from the properties of the true data distribution. Hence we refer to the first term as the reducible
model uncertainty and the second the data uncertainty.
Evidential Bayesian Models. In all the analysis above, we assumed a fixed and unknown π
that modulates the whole process and cannot be identified by the learning scheme. When we
characterize the uncertainty on the class distribution by a prior belief ∏ 〜p(∏), We attain the joint
p(y, ∏∣χ) = Pr[y∣χ, ∏]p(∏∣χ). The variance of the Bayes optimal classifier that averages over this
uncertainty Pr[y∣x] = / Pr[y∣x, π]p(π∣x)dπ decomposes as
[ftrUe(x)]+ En〜p(π∣x) [(1 - ftr,Ue(X))ftr,Ue(X)「
Var [y = k∣X] = Varn 〜p(∏∣x)
।-----------------------------
β^~{^^^^~^^^^^	|
Irreducible model uncertainty
^^™^^{^^^^^™
Data uncertainty
}
The main source of uncertainty p(π∣x) is not a posterior this time but an empirical prior on a single
observation X, hence its variance Will not shrink as |Dtr| → +∞. Therefore, the first term on the r.h.s.
reflects the irreducible model uncertainty caused by the missing knoWledge about π. The second
term indicates data uncertainty as in PBMs, but accounts for the local uncertainty at X caused by π .
Evidential Bayesian Models (EBMs) (Sensoy et al., 2018; Malinin & Gales, 2018) suggest
p(y,π∣X) = Pr[y∣X,π]p(π∣X) ≈ Pr[y∣∏]qψ(∏∣x)
where qψ (∏∣x) is a density function parameterized by ψ and the dependency of the class-conditional
on the input is dropped. Note that the resultant model employs uncertainty on individual data points
via π, but it does not have any global random variables. The standard evidential model training is
performed by maximizing the expected likelihood subject to a regularizer that penalizes the divergence
of qψ (π |X) from the prior belief
arg min -
ψ
/ logPr[y∣∏]qψ(π∣X)dπ + βKL(qψ(π∣x)∣∣p(π)).
Although presented in the original work as as-hoc design choices, such a training objective can
alternatively be viewed as β-regularized (Higgins et al., 2017) variational inference of Pr[y∣∏]p(∏∣x)
with the approximate posterior qψ (∏ |x) (Chen et al., 2019).
4
Published as a conference paper at ICLR 2022
4 Complete Bayesian Models: Best of Both Worlds
The uncertainty decomposition characteristics of PBMs and EBMs provide complementary strengths
towards quantification of the three uncertainty types that constitute total calibration.
i)	Model misfit: The PBM is favorable since it provides shrinking posterior variance with growing
training set size (Eq. 8) and the recovery of θMLE which inherits the consistency guarantees the
frequentist approach. Since the uncertainty on the local variable π does not shrink for large samples,
NLL would measure how well ψ can capture the regularities of heteroscedastic noise across individual
samples, which is a more difficult task than quantifying the fit of a parametric model.
ii)	Class overlap: The EBM is favorable since its data uncertainty term En〜p(∏∣χ*)[(1 -
h∏,k (x*))h∏,k (x*)] is likely to have less estimator variance than Eθ^p(θ∣Dtr )[(1-h∏,k(x*))h∏,fc(x*)]
and calculating p(∏∣χ*) does not require approximate inference as for p(θ∣Dtr).
iii)	Domain mismatch: The PBM is favorable since the effect of the modulation of a global θ on
the variance of Z applies also to the posterior predictive distribution with the only difference that
x* is a test sample. When the test sample is away from the training set, i.e. minχ∈Dtr ||x* - x|| is
large, thenp(y* ∣χ*,θ) is less likely to superpose with those regions of θ where L(θ) is pronounced,
hence will be flattened out leading to a higher variance posterior predictive for samples coming from
another domain. Since EBM has only local random variables, the variance of its posterior is less
likely to build a similar discriminative property for domain detection.
Main Hypothesis. A model that inherits the model uncertainty of PBM and data uncertainty of EBM
can simultaneously quantify: i) model misfit, ii) class overlap, and iii) domain mismatch.
Guided by our main hypothesis, we combine the PBM and EBM designs within a unified framework
that inherits the desirable uncertainty quantification properties of each individual approach
Pr[y∣π]p(π∣θ, x)p(θ),	(10)
which we refer to as a Complete Bayesian Model (CBM) hinting to its capability to solve the total
calibration problem. The model simply introduces a global random variable θ into the empirical prior
p(∏∣θ, x) of the EBM. The variance of the posterior predictive of the resultant model decomposes as
Var[y|x] = Varp(θ∣D)回n|x,e)[E[y|n,x]]] + Ep(θ∣D) [Varp(n|e,x)[E[y|n]]]
'--------------{---------------} '---------------{--------------}
Reducible Model Uncertainty	Irreducible Model Uncertainty
+ Ep(θ∣D)回π∣x,θ)[Var[y|n]]]
'----------------------------}
(11)
{^^^^^^^"
Data Uncertainty
where the r.h.s. of the decomposition has the following desirable properties: i) The first term
recovers the form of the reducible model uncertainty term of PBM when π is marginalized
Varp(θ∣D) [Ep(∏∣x,θ) [E[y∣∏, x]]] = Varp(θ∣D) [E[y∣θ, x]]; ii) The second term is the irreducible model
uncertainty term of EBM averaged over the uncertainty on the newly introduced global variable θ. It
quantifies the portion of uncertainty stemming from heteroscedastic noise, which is not explicit in
the decomposition of PBM; iii) The third term recovers the data uncertainty term of EBM when θ is
marginalized: Ep(θ∣D)[Ep(∏∣χ,θ)[Var[y∣∏]] = Ep(∏∣χ)[Var[y∣π]].
5	The Evidential Turing Proces s: An Effective CBM Realization
Equipping the empirical prior p(π∣θ, x) of CBM with the capability of learning to generate accurate
prior beliefs on individual samples is the key for total calibration. We adopt the following two guiding
principles to obtain highly expressive empirical priors: i) The existence of a global random variable
θ can be exploited to express complex reducible model uncertainties using the Neural Processes
(Garnelo et al., 2018b) framework, ii) The conditioning of p(∏∣θ, x) on a global variable θ and an
individual observation x can be exploited with an attention mechanism where θ is a memory and x is
a query. Dependency on a context set during test time can be lifted using the Neural Turing Machine
5
Published as a conference paper at ICLR 2022
(Graves et al., 2014) design, which maintains an external memory that is updated by a rule detached
from the optimization scheme of the main loss function. We extend the stochastic process derivation
of the Neural Processes by marginalizing out a local variable from a PBM with an external memory
that follows the Neural Turing Machine design and arrive at a new family of stochastic processes
called a Turing Process, formally defined as below.
Definition 1. A Turing Process is a collection of random variables Y = {yi|i ∈ I} for an index
setI = {1, . . . , K} with arbitrary K ∈ N+ that satisfy the two properties
(i)PM(Y) = Y Y p(y%∖θ)pM(θ)dθ,
yi∈Y
(ii)pM0(Y|Y0)
Y
yi∈Y
p(y∕θ)PM o (θ∣Y 0)dθ,
for another random variable collection Y0 of arbitrary size that lives in the same probability space
as Y, a probability measure pM (θ) with free parameters M, and some function M0 = r(M, Y0).
The Turing Process can express all stochastic processes since property (i) is sufficient to satisfy
Kolmogorov's Extension Theorem (0ksendal, 1992) and choosing r simply to be an identity mapping
would revert us back to the generic definition of a stochastic process. The Turing Process goes further
by permitting to explicitly specify how information accumulates within the prior. This is a different
approach from the Neural Process that performs conditioning p(Y |Y 0) by applying an amortized
posterior q(θ∣Y0) on a plain stochastic process that satisfies only Property (i), hence maintains a
data-agnostic prior p(θ) and depends on a context set also at the prediction time.
Given a disjoint partitioning of the data D = DC ∪ DT into a context set (x0, y0) ∈ DC and a target
set (x, y) ∈ DT, we propose the data generating process on the target set below
p(y,Dτ ,π,θ) = P(W) pm (Z)	Y	[p(y|n) p(n|Z, w,x)^∣.
External (x,y)∈Dτ	Input-specific
(12)
memory
prior
We decouple the global parameters θ = {w, Z} into a w that parameterizes a map from the input to
the hyperparameter space and an external memory Z = {z1, . . . , zR} governed by the distribution
PM (Z) parameterized by M that embeds the context data during training. The memory variable
Z updates its belief conditioned on the context set DC by updating its parameters with an explicit
rule pm (Z |Dc ) J PEZ 〜PM(z)[r(z,Dc )](Z). The hyperpriors of p(π∣Z,w,x) are then determined by
querying the memory Z for each input observation x using an attention mechanism, for instance a
transformer network (Vaswani et al., 2017). Choosing the variational distribution as qλ(θ, ∏∣χ)=
qλ(w)PM(Z)q(∏∣Z, w, x), We attain the variational free energy formula for our target model
FETP (λ) =	(13)
Eqλ (w)
-EEqλ(∏∣Z,w,x)
(x,y)∈D
m l-ι / I - i qλ(πlZ, w,x)
EpM(Z)卜gp(y|n) + log p(∏∣Z,w,χ)
+ log 片
p(w )	.
The learning procedure will then alternate between updating the variational parameters λ via gradient-
descent on FETP (λ) and updating the parameters of the memory variable Z via an external update
rule as summarized in Figure 2b. ETP can predict a test sample (x*,y*) by only querying the input
x* on the memory Z and evaluating the prior p(∏* |Z, w, x*) without need for any context set as
p(y^∣Dtr,χ*) ≈
JJp(y*∣∏*)q(∏* |Z, W, x*)pm(Z)qλ(w)dZdw.
(14)
6	Most Important Special Cases of the Evidential Turing Process
As we build the design of the Evidential Turing Process (ETP) on the CBM framework that over-
arches the prevalent parametric and evidential approaches, its ablation amounts to recovering many
state-of-the-art modeling approaches as listed in Table 1 and detailed below.
Bayesian Neural Net (BNN) (Neal, 1995; MacKay, 1992; 1995) is the most characteristic PBM
example that parameterize a likelihood p(y∣θ, x) with a neural network fθ (∙) whose weights follow
a distribution θ 〜p(θ). In the experiments we assume as a prior θ 〜 N(θ∣0, β-1I) and infer its
posterior by mean-field variational posterior qλ(θ) applying the reparameterization trick (Kingma
et al., 2015; Molchanov et al., 2017).
6
Published as a conference paper at ICLR 2022
(a) ETP Plate diagram
while Model not converged do
for Dbatch ⊂ D do
Choose DC ⊂ Dbatch
M -
EZ〜PM(Z) [r(Z, DC)]
λ 一 λ — V λFETP (λ)
end
end
(b) ETP Training
Figure 2: The Evidential Turing Process: (left) The essential design choice is how p(π∣Z, w, x) is
constructed thanks to an external memory M that can be queried with respect to any input x without
needing to store context data at test time. The dashed arrow indicates that Z can condition on a
context DC by updates its parameters M with an explicit function r. (right) The ETP training routine.
Table 1: Ablation Table. Deactivating the components of ETP one at a time equates it to state-of-the-
art methods. We curate ENP as a surrogate for the Attentive NP (ANP) of (Kim et al., 2019), which
requires test-time context, and an improved NP variant with reduced estimator variance thanks to π .
Model	π	Z	M	r	D (test)	Compare
ETP (Target)			~v~		×	
ANP (Kim et al., 2019)				×	,	No
ENP (Surrogate)			×	×	×	Yes
NP (Garnelo et al., 2018b)	×	,	×	×	×	No
EDL (Sensoy et al., 2018)		×	×	×	×	Yes
BNN (Molchanov et al., 2017)	×	×	×	×	×	Yes
v×: Component active	X: Component inactive
Dc(test): Demand for context data at test time
Evidential Deep Learning (EDL) (Sensoy et al., 2018) introduces the characteristic exam-
ple of EBMs. EDL employs an uninformative Dirichlet prior on the class probabilities,
which then set the mean of a normal distribution on the one-hot-coded class labels y,∏ 〜
Dir(π∣1,..., V)N(y∣π, 0.5IK). EDL links the input observations to the distribution of class
probabilities by performing amortized variational inference with the approximate distribution
qλ(∏; x) = D ir (π∣αλ(x)).
Neural Processes (NP) (Garnelo et al., 2018a;b) is a PBM used to generate stochastic processes
from neural networks by integrating out the global parameters θ as in Property (i) of Definition 1. NPs
differ from PBMs by amortizing an arbitrary sized context set DC on the global parameters q(θ∣Dc)
via an aggregator network during inference. NPs can be viewed as Turing Processes with an identity
map r. Follow-up work equipped NPs with attention (Kim et al., 2019), translation equivariance
(Gordon et al., 2020; Foong et al., 2020), and sequential prediction (Singh et al., 2019; Yoon et al.,
2020). All of these NP variants assume prediction-time access to context, making them suitable for
interpolation tasks such as image in-painting but not for generic prediction problems.
Evidential Neural Process (ENP) is the EBM variant of a neural process we introduce to bring
together best practices of EDL and NPs and make the strongest possible baseline for ETP, defined as
p(y,π,Z|Dc) = Cat(y|n)Dir(n|a(Z))N(Zl(μ,σ2) = r({hθ(Xj,yj) | (Xj,yj) ∈dc})), (15)
with an aggregation rule r(∙), an encoder net hθ(∙, ∙), and a neural net a0(∙) mapping the global
variable Z to the class probability simplex. We further improve upon ENP with an attentive NP
approach (Kim et al., 2019) by replacing the fixed aggregation rule r with an attention mechanism,
thereby allowing the model to switch focus depending on the target. At the prediction time, we use
Z 〜N(Z11, κ2I) for small K to induce an uninformative prior on ∏ in the absence of a context set.
7
Published as a conference paper at ICLR 2022
7	Case Study: Classification with Evidential Turing Processes
We demonstrate a realization of an Evidential Turing Process to a classification problem below
R
y, π, w, Z |M 〜C at (y∣∏)D ir (n| exp(a(vw(x); Z)))N (w|0, β-1I) Y N (Zrmr ,κ2I).	(16)
r=1
The global variable Z is parameterized by the memory M = (m1, . . . , mR) consisting of R cells
mτ ∈ RK.1 The input data X is mapped to the same space via an encoding neural net Vw(∙)
parameterizing a Dirichlet distribution over the class probabilities ∏ via an attention mechanism
a(vw,z) = ∑zez Φ(vw(x),z0)z, where φ(vw(x),z) = SoftmaX({kψ(z)>vw(x)∕√K|z ∈ Z}).
The function kψ (∙) is the key generator network operating on the embedding space. We update the
memory cells as a weighted average between remembering and updating
m J EZ〜p(z∣M) tanh 1m + (1 - Y) X	Φ(vw(x),z)[onehot(y) + soft(vw(x))f)	(17)
(x,y)∈DC
where γ ∈ (0, 1) is a fixed scaling factor controlling the relative importance. The second term adds
new information to the cell as a weighted sum over all pairs (x, y), taking both the true label (via
onehot(y)) as well as the uncertainty in the prediction (via soft(vw (x))) into account. The final
tanh(∙) transformation ensures that the memory content remains in a fixed range across updates, as
practised in LSTMs (Hochreiter & Schmidhuber, 1997).
8 Experiments
We benchmark ETP against the state of the art as addressed in the ablation plan in Table 1 according
to the total calibration criteria developed in Sec. 2 on five real-world data sets. See the Appendix B
for full details on the experimental setup in each case and for the hyperparameters used throughout.
We provide a reference implementation of the proposed model and the experimental pipeline.2
Total calibration. Table 2 reports the predic-
tion error and the three performance scores intro-
duced in Sec. 2 that constitute total calibration.
We perform the out-of-domain detection task as
in Malinin & Gales (2018) and classify the test
split of the target domain and a data set from
another domain based on the predictive entropy.
ETP is the only method that consistently ranks
among top performers in all data sets with re-
spect to all performance scores, which supports
our main hypothesis that CBM should outper-
form PBM and EBM in total calibration.
Figure 3: Results averaged across 19 types of cor-
ruption (e.g. motion blur, fog, pixelation) applied
on the test splits of FMNIST, CIFAR10, and SVHN
at five severity levels.
Response to gradual domain shift. In order
to assess how well models can cope with a grad-
ual transition from their native domain, we eval-
uate their ECE performance on data perturbed
by 19 types of corruption (Hendrycks & Diet-
terich, 2019) at five different severity levels. Figure 3 depicts the performance of models averaged
across all corruptions. ETP gives the best performance nearly in all data sets and distortion severity
levels (see Appendix B.3 Table 4 for a tabular version of these results).
Computational cost. We measured the average wall-clock time per epoch in CIFAR10 to be
10.8 ± 0.1 seconds for ETP, 8.2 ± 0.1 seconds for BNN, 8.6 ± 0.1 seconds for EDL, and 9.5 ± 0.2
seconds for ENP. The relative standings of the models remain unchanged in all the other data sets.
1In the experiments we fix κ2 = 0.1. Preliminary results showed stability as well for larger/smaller values.
2https://github.com/ituvisionlab/EvidentialTuringProcess
8
Published as a conference paper at ICLR 2022
Table 2: Quantitative results on five data sets showing mean ± standard deviation across 10 repetitions.
Best performing models that overlap within three standard deviations are highlighted in bold.
Domain Data (Architecture)	IMDB (LSTM)	Fashion (LeNet5)	SVHN (LeNet5)	CIFAR10 (LeNet5)	CIFAR100 (ResNet18)
	Prediction accuracy as % test			error	
BNN	16.4 ±0.6	7.9±o.ι	7.9±o.ι	15.3±o.3	30.2 ±0.3
EDL	38.3 ±13.3	8.6±0.1	7.3±o.ι	18.5±o.2	45.2 ±0.4
ENP	50.0 ±o.o	7.9±0.2	6.7±o.ι	14.8±o.2	39.0 ±0.3
ETP (Target)	15.8 ±1.3	7.9±o.2	6.9±o.ι	15.3±o.2	29.2 ±0.3
In-domain calibration as % Expected Calibration Error (ECE)					
BNN	14.4 ±0.4	6.7±0.	6.5±0.	5.5±0.3	15.2±0.0
EDL	41.1 ±2.6	3.7±0.2	4.0±0.1	9.0±0.2	5.3±0.4
ENP	0.8 ±1.6	6.0±0.2	10.7±0.2	7.2±0.3	39.7±0.4
ETP (Target)	3.1 ±0.4	2.6±0.2	2.6±0.1	2.7±0.1	6.6±0.1
	Model fit as negative test log-likelihood				
BNN	0.47 ±o.o	0.65±o.o	0.71±o.o	0.50±o.o	1.78±0.0
EDL	0.66 ±o.ι	0.37±o.o	0.34±o.o	0.72±o.0	2.24±0.0
ENP	0.69 ±o.o	0.34±o.o	0.33±o.o	0.50±o.o	2.52±0.0
ETP (Target)	0.37 ±o.o	O.29±o.o	O.26±o.o	O.46±o.o	1.36±0.0
	OUt-Of-domain detection as % Area Under ROC Curve				
OOD Data	Random	MNIST	CIFAR100	SVHN	TinyImageNet
BNN	60.9 ±4.2	75.9±2.3	86.2±0.5	84.1±1.3	97.2±0.5
EDL	55.1 ±5.1	77.5±2.0	90.9±0.3	79.2±0.7	89.6±0.3
ENP	53.7 ±5.7	88.9±1.0	92.4±0.4	81.4±0.8	100.0±0.1
ETP (Target)	59.1 ±5.1	90.0±0.9	90.0±0.4	82.1±0.6	99.6±0.1
9 Conclusion
Summary. We initially develop the first formal definition of total calibration. Then we analyze
how the design of two mainstream Bayesian modeling approaches affect their uncertainty charac-
teristics. Next we introduce Complete Bayesian Models as a unifying framework that inherits the
complementary strengths existing ones. We develop the Evidential Turing Process as an optimal
realization of Complete Bayesian Models. We derive an experiment setup from our formal definition
of total calibration and a systematic ablation of our target model into the strongest representatives of
the state of the art. We observe in five real-world tasks that the Evidential Turing Process is the only
model that can excel all three aspects of total calibration simulatenously.
Broad impact. Our work delivers evidence for the claim that epistemic awareness of a Bayesian
model is indeed a capability learnable only from in-domain data, as appears in biological intelligence
via closed-loop interactions of neuronal systems with stimuli. The implications of our work could
follow up in interdisciplinary venues, with focus on the relation of associative memory and attention
to the neuroscientific roots of epistemic awareness (Lorenz, 1978).
Limitations and ethical concerns. While we observed ETP to outperform BNN variants in our
experiments, whether BNNs could bridge the gap after further improvements in the approximate
inference remains an open question. The attention mechanism of ETP could also be improved, for
instance to transformer nets (Vaswani et al., 2017). The explainability of the memory content of ETP
deserves further investigation. The generalizeability of our results to very deep architectures, such as
ResNet 152 or DenseNet 161 could be a topic of a separate large-scale empirical study. ETP does not
improve the explainability and fairness of the decisions made by the underlying design choices, such
as the architecture of the used neural nets in the pipeline. Potential negative societal impacts of deep
neural net classifiers stemming from these two factors need to be circumvented separately before the
real-world deployment of our work.
9
Published as a conference paper at ICLR 2022
References
M. Abramowitz and I.A. Stegun. Handbook of Mathematical Functions. Dover, 1965.
W. Chen, Y. Shen, H. Jin, and W. Wang. A Variational Dirichlet Framework for Out-of-Distribution
Detection. abs/1811.07308, 2019.
A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-Learning Stationary
Stochastic Process Prediction with Convolutional Neural Processes. In NeurIPS, 2020.
M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y.W. Teh,
D. Rezende, and S.M.A. Eslami. Conditional Neural Processes. In ICML, 2018a.
M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D.J. Rezende, S.M. Eslami, and Y.W. Teh. Neural
Processes. In ICML WS on Theoretical Foundations and Applications of Deep Generative Models,
2018b.
J. Gordon, W.P. Bruinsma, A.Y.K. Foong, J. Requeima, Y. Dubois, and R.E. Turner. Convolutional
Conditional Neural Processes. In ICLR, 2020.
A.	Graves, G. Wayne, and I. Danihelka. Neural Turing Machines. arXiv preprint arXiv:1410.5401,
2014.
C.	Guo, G. Pleiss, Y. Sun, and K.Q. Weinberger. On Calibration of Modern Neural Networks. In
ICML, 2017.
D.	Hendrycks and T. Dietterich. Benchmarking Neural Network Robustness to Common Corruptions
and Perturbations. In ICML, 2019.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In ICLR,
2017.
S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780,
1997.
H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y.W. Teh.
Attentive Neural Processes. In ICLR, 2019.
D.P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.
D.P. Kingma, T. Salimans, and M. Welling. Variational Dropout and the Local Reparameterization
Trick. NeurIPS, 2015.
V. Kuleshov, N. Fenner, and S. Ermon. Accurate Uncertainties for Deep Learning using Calibrated
Regression. In ICML, 2018.
Y. LeCun, C. Cortes, and C.J. Burges. MNIST Handwritten Digit Database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Konrad Lorenz. Die R邕Ckseite des Spiegels: Versuch einer Naturgeschichte menschlichen Erkennens.
Piper, 1978.
D.J.C. MacKay. A Practical Bayesian Framework for Backpropagation Networks. Neural Computa-
tion, 1992.
D.J.C. MacKay. Probable Networks and Plausible Predictions - A Review of Practical Bayesian
Methods for Supervised Neural Networks. Network: Computation in Neural Systems, 1995.
A.	Malinin and M. Gales. Predictive Uncertainty Estimation via Prior Networks. In NeurIPS, 2018.
D. Molchanov, A. Ashukha, and D. Vetrov. Variational Dropout Sparsifies Deep Neural Networks. In
ICML, 2017.
M.P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining Well Calibrated Probabilities using Bayesian
Binning. In AAAI, 2015.
10
Published as a conference paper at ICLR 2022
R. Neal. Bayesian Learning for Neural Networks. PhD thesis, 1995.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A.Y. Ng. Reading Digits in Natural Images
with Unsupervised Feature Learning. In NeurIPS WS on Deep Learning and Unsupervised Feature
Learning, 2011.
B.	0ksendal. Stochastic Differential Equations: An Introduction with Applications. Springer-Verlag,
1992.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T..evor Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-
jani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style,
High-Performance Deep Learning Library. In NeurIPS. 2019.
H. Robbins and S. Monro. A Stochastic Approximation Method. The Annals of Mathematical
Statistics, 22(3):400-407,1951.
G. Schwalbe and M. Schels. A Survey on Methods for the Safety Assurance of Machine Learning
Based Systems. In European Congress on Embedded Real Time Software and Systems, 2020.
M. Sensoy, L. Kaplan, and M. Kandemir. Evidential Deep Learning to Quantify Classification
Uncertainty. In NeurIPS, 2018.
G. Singh, J. Yoon, Y. Son, and S. Ahn. Sequential Neural Processes. In NeurIPS, 2019.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, and I. Polosukhin.
Attention is All You Need. In NeurIPS, 2017.
J. Yoon, G. Singh, and S. Ahn. Robustifying Sequential Neural Processes. In ICML, 2020.
11
Published as a conference paper at ICLR 2022
Appendix
A Further analytical results and derivations
A.1 Kullback Leibler between two Dirichlet Distributions
As both our ETP and and the EDL baseline use the Kullback-Leibler divergence between two
Dirichlet distributions, we give the full details of its calculation here. For two distributions over a
K-dimensional probability π, Dir(∏∣α) and Dir(π∣β), parameterized by α,β ∈ RK, the following
identity holds
KL (Dir(∏∣α) k Dir(∏∣β)) = log (悬O)I Qkr(βk) ) + XSk - βk)(ψ(ak) - Ψ(Pk a®)),
where Γ(∙) and ψ(a):=或 logΓ(a) are the gamma and digamma functions (Abramowitz & Stegun,
1965).
A.2 The Evidential Deep Learning Objective
The analytical expression of the Evidential Deep Learning (Sensoy et al., 2018) loss as defined in
the main paper can be computed as follows. For a single K-dimensional observation y, dropping
the n from the notation and instead using the index for the dimensionality throughout the following
equations, we have
ep(∏∣x)川y - n||2] = Ep(∏∣x) (yy -π)>(y -π)]
K
=EEp(∏∣χ) [(yk - ∏k)2]
k=1
K
=〉：Ep(π∣x) [(yk - Ep(π∣x) [πk] + Ep(π∣x) [πk] - πk)]
k=1
K
=〉：(yk - Ep(π∣x) [πkD + Varp(π∣x) [πk] ,
k=1
with the tractable expectation and variance of a Dirichlet distributed π. The Kullback-Leibler term is
between two Dirichlet distributions and given (see the general form in A.1) as
KL (Dir(π∣αθ(x)) ∣∣ Dir(π∣1,..., 1))
=log (r(K⅞Γθ7(X)k)! + X (αθ(X)k -1) (ψmθ(x)k) - MP aθ(X)k)).
A.3 Evidential Deep Learning as a latent variable model
As discussed in the main paper for a set of N observations D = {(X1, y1), . . . , (XN, yN)}, the EDL
objective minimizes is given as
N
LEDL =): Ep(∏n∣Xn) [||yn - πn ||21+ λKL (p(πn |xn) ∣∣ Dir(πn | 1,..∙, 1)),
n=1
wherep(∏n∣Xn) = Dir(∏∣aθ(xn)). We can instead assume the following generative model
∏n 〜Dir(∏n∣1,..., 1)	Vn
yn 〜N(yn∣∏n, 0.5Ik)	∀n,
i.e. latent variables πn and K-dimensional observations yn following a multivariate normal prior.
Approximating the intractable posterior p(π |y), where π = (π1, . . . , πn), y = (y1, ..., yn), with an
12
Published as a conference paper at ICLR 2022
amortized variational posterior q(∏n Xn) = Dir(∏n∣αθ(xn)), where αθ(∙) is the same architecture
as in the EDL model, we have as the evidence lower bound (ELBO) to be maximized
N
EEq(∏n;Xn) [logp(yn|nn)] - KL )(nn； Xn) Il P(πn))
n=1
N
EEq(∏n;Xn)
n=1
—1 lθg(2∏) + KK lθg(2) — (yn — ∏n)>(yn — ∏n) — KL (q(∏n'; Xn) k p(∏n))
N
COnSt- EEq(∏n;Xn) Kyn — πn)>(yn — πn)] - KL(9(nn； Xn) k P(πn))
n=1
const - LEDL,
that is the negative ELBO is equal to the EDL loss up to an additive constant.
A.4 Posterior Predictive
For a new input observation x*, the corresponding posterior predictive distribution on the output y*
can be computed as
p(y	= k|x ,	D)	≈	Ep(Z|M)	[Eq.(∏|x* ,Z)[p(y*	=	k∣∏)]]	=	Ep(ZM)[hk/Pk，hk0i，
where hθk = hθ (vθ(X*), a(vθ(X*); Z))k. That is, we can compute it analytically up to a sampling-
based evaluation of the expectation overp(Z|M).
B Experimental Details
B.1	Synthetic 1d two-class classification
We illustrate the qualitative behaviour of
ETP on a synthetic 1d two-class classifi-
cation task. The data consist of observa-
tions (20 per class) from two highly over-
lapping Gaussian distributions. The neural
net, a simple one hidden layer architecture,
can learn the task with high accuracy. Fig-
ure 4 shows the raw data and underlying
distributions as well as the learned mem-
ory evidence. The model learns to place
more evidence on the correct class further
away from the decision boundary while
also becoming more varied with increasing
distance. Within the high-density region,
the asymmetry in how the specific observa-
tions are spread out leads to an asymmetry
in the memory evidence. Below zero, the
blue points are clearly separated, leading
to a quick emphasis on that class as we
move towards the left. Above zero, how-
ever, some blue observations in the orange
region prevent the model from rapidly de-
veloping over-confidence.
Input space
Figure 4: 1D Classification Task. The upper plot
shows the underlying distributions of each of the two
classes, as well as the observed data. The lower shows
the regularizing evidence the generative model places
on each of the two classes depending on location in
space, that is, mean ± one standard deviation over ten
samples from p(Z |M).
This synthetic data set consisting of two classes with 20 observations each sampled from standard
normal distributions centered at ±1 respectively. The encoding function vφ(∙) consists of a multi-layer
perceptron with a single hidden layer consisting of 32 neurons and a ReLU activation function. It is
optimized for 400 epochs with the Adam optimizer (Kingma & Ba, 2015) using the PyTorch (Paszke
et al., 2019) default parameters and a learning rate of 0.001. In order to keep the model as simple
13
Published as a conference paper at ICLR 2022
Figure 5: 2D Classification Task. The three upper plots visualize the regularizing evidence that the
model places on the location in space as the average over ten samples fromp(Z|M). The color scales
are different across the three plots and vary in overall intensity. The lower plot visualizes a horizontal
cut through the middle of these plots, putting them on a common scale. The solid lines in each
color indicate the mean memory evidence, and the corresponding shaded areas indicate one standard
deviation. Around the separable blue class, the memory strongly emphasizes the correct class with
large confidence and suppresses the other two classes depicted in orange and green. While always
preferring the correct class, the memory regularizes against overconfidence around the overlapping
orange and green classes.
as possible, the key generator function kλ(∙) is constrained to being the identity function, while
hξ v(x), a(v(x); Z) = v(x) + tanh(a(v(x); Z). The memory update is simplified to dropping the
tanh(∙) rescaling.
B.2	Iris 2d three-class classification
The data set used in this experiment is the classical Iris data set3 consisting of 150 samples of three
iris species (50 per class), with four features measured per sample. We first map the data to the first
two principal components for visualization and also use this modified version as the training data.
This gives an interpretable toy learning setup where one class is clearly separated from the other two
overlapping classes. The encoding function vφ(∙) consists of an MLP with two hidden layers of 32
neurons each and ReLU activation functions. We train it for 400 epochs with the Adam optimizer
using the PyTorch default parameters and a learning rate of 0.001. In order to keep the model as
simple as possible, we constrain the key generator function kλ(∙) to the identity function while setting
hξ v(x), a(v(x); Z) = v(x) + tanh(a(v(x); Z). We simplify the memory update by dropping the
tanh(∙) rescaling. Figure 5 shows the varying importance the model assigns to different parts of the
input space depending on the class under consideration. For the separate blue class, the model is
significantly more confident in relative terms and absolute values.
B.3	Experiments on Real Data
All experiments are implemented in PyTorch (Paszke et al., 2019) version 1.7.1 and trained on a
TITAN RTX. They have been replicated ten times over random initial seeds.
Fashion MNIST (FMNIST). We use a LeNet5-sized architecture (see Table 3). The out-of-
distribution data is the MNIST (LeCun et al., 2010) data set. We z-score normalize all data sets with
the in-domain mean and standard deviations. We train each model for 50 epochs with the Adam
optimizer (Kingma & Ba, 2015) using a learning rate of 0.001.
3Originally due to Fisher (1936) and Anderson (1935). We rely on the version provided by the
scikit learn library, see https://scikit-learn.org/stable/modules/classes.html#
module-sklearn.datasets.
14
Published as a conference paper at ICLR 2022
Table 3: The neural network architectures used for the FMNIST, CIFAR10, and SVHN data set with
ReLU activations between the layers.
FMNIST
CIFAR10/SVHN
Convolution (5 × 5) with 20 channels Convolution (5 × 5) with 192 channels
MaxPooling (2 × 2) with stride 2
Convolution (5 × 5) with 50 channels Convolution (5 × 5) with 192 channels
MaxPooling (2 × 2) with stride 2
Linear with 500 neurons	Linear with 1000 neurons
Linear with 10 neurons
CIFAR100		
Layer Name	ResNet-18	
Conv1	7 × 7, 64, stride 2	
3 × 3 max pool, stride 2		
Conv2_x	3 × 3, 64	
	, 3 × 3, 64	×2
	3 × 3, 128	
Conv3_x	3 × 3, 128	×2
	3 × 3, 256	
Conv4_x	3 × 3, 256	×2
	3 × 3, 512	
Conv5_x	3 × 3, 512	×2
Average Pool	7 × 7 average pool		
Linear	100 neurons	
IMDB		
Layer Name	LSTM	
Embedding	1001 → 64	
LSTM	2 layers with 256 hidden size		
Linear	2 neurons	
CIFAR10 (C10). We use a LeNet5-sized architecture (see Table 3). The out-of-distribution data is
the SVHN (Netzer et al., 2011). We z-score normalize all data sets with the in-domain mean and
standard deviations. We further rely on data augmentation for the in domain data using random crops
with 32 pixels and four pixel padding as well as horizontal flips. We train each model for 100 epochs
using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001.
SVHN. We use a LeNet5-sized architecture (see Table 3). The out-of-distribution data is the
CIFAR10 data set. We z-score normalize all data sets with the in-domain mean and standard
deviations. We train each model for 100 epochs using the Adam optimizer (Kingma & Ba, 2015)
with a learning rate of 0.0001.
IMDB Sentiment Classification. We use a LSTM architecture with 64 embedding dimensions
and 2 layers with 256 hidden dimensions. We generate the out-of-distribution data by sampling
values from the [1, 1000]interval uniformly at random. We train each model for 20 epochs using the
SGD optimizer (Robbins & Monro, 1951) with a learning rate of 0.05 and 0.9 momentum. For data
preparation, we follow the source code of the original work 4.
Shared Details and Observations. In all experiments, we train the BNN models with a cross-
entropy loss using softmax as the squashing function to calculate class probabilities. The EDL model
is trained with the original loss (Sensoy et al., 2018). We make our predictions with the mean of
the Dirichlet distribution as in the original work. For all models, we use entropy as the criterion for
detecting out-of-distribution instances.
4https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch
15
Published as a conference paper at ICLR 2022
Table 4: Quantitative results of models on the FMNIST-C, CIFAR10-C and SVHN-C test dataset
using the LeNet5. The table below reports mean ± three standard deviations.
Severity	Method	FMNIST-C		CIFAR10-C		SVHN-C	
		Err(%) (J)	ECE (%) (J)	(J)	ECE (%) (J)	(J)	ECE (%) (J)
	BNN	9.4±2.8	8.0±2.4	21.9±4.5	7.6±2.9	8.8±1.2	7.3±1.0
1 1	EDL	9.9±2.6	4.3±1.0	25.1±4.6	10.4±1.3	8.1±1.2	4.6±0.6
	ENP	9.3±2.7	5.9±0.5	35.9±6.0	4.3±1.7	8.1±1.3	11.4±1.0
	ETP	9.5±2.8	3.0±1.0	21.9±4.6	3.6±2.2	8.0±1.2	3.0±0.4
	BNN	9.9±2.3	8.3±1.9	26.1±5.3	10.0±3.7	8.9±1.2	7.3±0.9
ɔ 2	EDL	10.5±2.1	4.5±0.8	29.1±5.1	10.1±1.6	8.2±1.1	4.6±0.6
	ENP	9.9±2.3	6.1±1.0	41.1±6.6	5.2±2.1	7.8±1.2	11.7±1.1
	ETP	10.1±2.4	3.1±0.8	26.1±5.3	5.2±2.8	7.7±1.1	3.0±0.5
	BNN	10.7±2.3	9.0±1.9	30.0±7.6	12.3±5.3	9.2±1.6	7.5±1.2
3	EDL	11.4±2.1	4.9±0.9	32.6±7.0	9.8±1.5	8.5±1.5	4.7±0.7
	ENP	10.8±2.3	6.4±1.6	44.8±9.0	7.0±3.4	8.5±1.7	11.8±1.5
	ETP	11.1±2.4	3.5±0.9	29.8±7.2	6.8±3.8	8.2±1.5	3.2±0.6
	BNN	12.2±4.2	10.2±3.6	35.1±10.9	15.9±8.3	9.9±2.5	8.0±1.9
A 4	EDL	12.9±4.2	5.6±1.8	37.3±10.0	9.6±1.4	9.1±2.4	5.1±1.0
	ENP	12.4±4.3	6.5±2.2	48.9±11.2	9.2±5.4	9.4±2.8	12.3±2.2
	ETP	12.8±4.6	4.1±1.8	34.8±10.3	9.7±6.4	9.1±2.4	3.2±1.0
	BNN	14.2±5.4	11.7±4.6	42.4±13.7	21.0±10.4	10.2±2.5	8.2±1.9
C 5	EDL	15.1±5.8	6.7±3.1	43.9±12.5	9.5±2.4	9.4±2.4	5.1±1.1
	ENP	14.8±6.5	6.9±3.1	54.6±12.7	12.5±7.0	9.5±2.8	12.4±2.3
	ETP	15.3±7.1	5.2±3.4	42.1±13.1	14.1±8.6	9.0±2.5	3.3±1.0
Robustness against perturbations. We use the CIFAR10-C dataset in the same setup as in original
work tHendrycks & Dietterich (2019). As FMNIST-C and SVHN-C are not available, we create
them following the same procedure as in the original work. For FMNIST we adapt the procedure
to gray-scale images by replicating the image three times for each channel. After applying the
distortions, we map it back to the gray scale. Table 4 gives the numerical results used to create Figure
3 in the main paper.
16