Published as a conference paper at ICLR 2022
Sample Efficient Stochastic Policy Extragra-
dient Algorithm for Zero-Sum Markov Game
Ziyi Chen, Shaocong Ma, Yi Zhou
Department of ECE
University of Utah
Salt Lake City, UT 84112, USA
{u1276972,yi.zhou,s.ma}@utah.edu
Ab stract
Two-player zero-sum Markov game is a fundamental problem in reinforcement
learning and game theory. Although many algorithms have been proposed for
solving zero-sum Markov games in the existing literature, many of them either
require a full knowledge of the environment or are not sample-efficient. In this
paper, we develop a fully decentralized and sample-efficient stochastic policy
extragradient algorithm for solving tabular zero-sum Markov games. In particular,
our algorithm utilizes multiple stochastic estimators to accurately estimate the value
functions involved in the stochastic updates, and leverages entropy regularization
to accelerate the convergence. Specifically, with a proper entropy-regularization
parameter, we prove that the stochastic policy extragradient algorithm has a sample
complexity of the order Oe(. . e5Am-7/3.5) for finding a solution that achieves
-Nash equilibrium duality gap, where Amax is the maximum number of actions
between the players, μmin is the lower bound of state stationary distribution, and Y
is the discount factor. Such a sample complexity result substantially improves the
state-of-the-art complexity result.
1	Introduction
Competitive reinforcement learning (RL) is an emerging and popular framework that has broad
applications in various areas, including market pricing applications (Kononen and Oja, 2004), real-
time strategy-making (Vinyals et al., 2019), board games (Silver et al., 2017; Moerland et al., 2018)
and inverse RL (Zhang et al., 2019). In particular, an important and fundamental formulation of
competitive RL is the two-player zero-sum Markov game, which involves two competing players that
interact with a common environment and receive zero-sum rewards. Both players aim to learn the
optimal policy that achieves the Nash equilibrium of accumulated rewards.
Algorithms for solving Markov games are very different from conventional single-agent RL algo-
rithms. In particular, both players must learn to improve their policies based on feedback from the
opponent and the environment, but usually the opponent will not reveal any sensitive information
(e.g., actions or policy) or cooperate with each other. In the existing literature, numerous algorithms
have been developed for solving zero-sum Markov games, including Q-learning (Fan et al., 2020; Zhu
and Zhao, 2020), fitted Q iteration (Zhang et al., 2021), policy gradient (Daskalakis et al., 2020; Zhao
et al., 2021), policy extragradient (Cen et al., 2021), model-based Monte Carlo estimation (Zhang
et al., 2020), optimistic gradient descent ascent (Wei et al., 2021), etc. However, many of these
algorithms require both players to access their opponent’s actions (Wei et al., 2017; Sidford et al.,
2020; Huang et al., 2021; Jafarnia-Jahromi et al., 2021), which causes privacy issues. On the other
hand, some algorithms need to know about the environment transition kernel and reward mapping
(Cen et al., 2021), which are usually unknown a priori in practice. Moreover, other algorithms require
both players to perform asymmetric policy updates using different numbers of iterations, learning
rates or exploration probabilities (Zhao et al., 2021; Daskalakis et al., 2020), which are generally
1
Published as a conference paper at ICLR 2022
hard to coordinate in advance between two competing players. Therefore, it is desired to develop an
algorithm for solving Markov games that avoids all the aforementioned issues.
Moreover, from a theoretical perspective, the convergence and sample complexity of these existing
algorithms for solving Markov games have not been comprehensively studied. Specifically, some
studies established the convergence of the algorithms with i.i.d. samples (Zhao et al., 2021; Guo
et al., 2021), which violates the dependent nature of samples collected from the dynamic Markov
decision process. Also, other algorithms suffer from an extremely high sample complexity to achieve
an approximate Nash equilibrium solution (Wei et al., 2021). Hence, we are motivated to answer the
following fundamental question.
• Q: Can we develop a fully decentralized algorithm that is model-free and takes symmetric and
private policy updates for solving zero-sum Markov games, with provable convergence guarantee
and an improved sample complexity for achieving a Nash equilibrium solution?
In this paper, we provide positive and comprehensive answers to this question by developing a fully
decentralized stochastic policy extragradient algorithm. In Table 1 of Appendix F, we compare the
key properties and sample complexity of our algorithm with those of the existing algorithms.
1.1	Our Contributions
We consider a standard zero-sum Markov game with discounted reward over infinite horizon. To
solve such a Markov game, we propose a stochastic variant of the policy extragradient algorithm
(Cen et al., 2021) that satisfies the following amenable properties.
•	Our algorithm uses multiple stochastic estimators to estimate the value functions involved in the
predictive updates for solving entropy-regularized matrix games, and therefore the algorithm does
not rely on any prior knowledge of the environment transition kernel (model-free). Moreover, the
resulting stochastic policy updates of our algorithm for both players are symmetric and do not
involve any sensitive information of the opponent (private).
•	Compared with the stochastic estimators used in (Wei et al., 2021), our estimators have much
smaller variance that helps improve the estimation accuracy. Specifically, by developing new
techniques (explained in the next bullet), we establish tight high-probability estimation error
bounds for our stochastic estimators with Markovian samples. Then, with a proper entropy-
regularization parameter, we show that our stochastic policy extragradient algorithm requires a
sample complexity of the order O( *一声 Am-Y))to achieve an e-Nash equilibrium duality gap,
which substantially improves the state-of-the-art complexity result of (Wei et al., 2021).
•	We develop novel techniques to bound the estimation error of the proposed stochastic estimators,
whose numerator and denominator involve sample average approximations. First, we propose a
special estimation error decomposition that avoids divergence of the bound caused by possibly small
numerical values of the sample average involved in the denominator of the stochastic estimators.
Second, we leverage this error decomposition and the recursive structure of the stochastic estimators
to derive a contraction property of the estimation errors, which eventually leads to tight bounds for
the estimation error. We refer to Section 4 for more elaboration on our technical novelties.
1.2	Other related work
Other settings of two-player zero-sum Markov games. In this paper, we focus on a standard
setting of two-player zero-sum Markov game with discount and infinite horizon in the discrete time
domain. There are other settings of two-player zero-sum Markov games. For example, Bai et al.
(2020); Huang et al. (2021) studied a two-player zero-sum Markov game with finite horizon and
without discount, whereas Daskalakis et al. (2020) considered finite random horizon without discount.
Jafarnia-Jahromi et al. (2021) also considered the setting without discount, and it allows one of the
players to constantly adjust its policy based on the entire history of states and actions. Ghosh et al.
(2021) studied a two-player zero-sum Markov game in the continuous time domain.
Multi-agent general-sum Markov game. Some works studied multi-agent Markov games, which
extend the two-player zero-sum Markov games to multiple players without the zero-sum constraint
(Wang and Sandholm, 2002; Hu and Wellman, 2003; Deng et al., 2021; Leonardos et al., 2021). More
specifically, Leonardos et al. (2021) defined and studied Markov potential game which has a potential
2
Published as a conference paper at ICLR 2022
function assumption. Guo et al. (2019); Elie et al. (2020); Gu et al. (2021) studied mean-field games
with a large number of players.
Entropy regularization and value iteration Our algorithm leverages entropy regularization and
value iteration to accelerate convergence. Entropy regularization is a popular technique that has been
widely used in reinforcement learning (Neu et al., 2017; Geist et al., 2019; Mei et al., 2020; Cen
et al., 2020) and Markov game (Mertikopoulos and Sandholm, 2016; Savas et al., 2019; Cen et al.,
2021) to encourage environment exploration and accelerate algorithm convergence. Value iteration is
also a classical method that is widely used in both single-agent reinforcement learning (Ernst et al.,
2005; Tamar et al., 2016; Farahmand and Ghavamzadeh, 2021) and Markov games (Zhu and Zhao,
2020; Cen et al., 2021). With full knowledge of the environment, it exponentially converges to the
fixed point of Bellman operator (Cen et al., 2021). Compared to another similar classical method
called policy iteration, value iteration does not need policy evaluation which involves additional
computation (Sutton and Barto, 2018).
2	Background of Markov Game and Entropy Regularization
2.1	Two-player zero-sum Markov game
In a zero-sum Markov game, two players compete with each other in a common environment.
Throughout, the state space is denoted as S . The action spaces and policies of both players are
denoted as A(I),∏⑴ and A⑵，∏⑵，respectively. Here, ∏⑴ ∈ ∆(∣A(1)∣), n(2) ∈ ∆(∣A(2)∣) are
random policies defined over the corresponding simplex sets. The reward function is denoted as
R : S × A(1) × A(2) → [0, 1], and the discount factor is denoted as γ ∈ (0, 1).
At any time t, both players observe state st ∈ S of the environment. Then, both players respectively
select their actions following their own policies, i.e., at1) 〜 π(1)(∙∣st) and a(2) 〜π⑵(∙∣st). After
that, the environment state transfers to a new state st+1 following the underlying transition kernel
P(∙∣st, a(1),a(2)), and both players receive zero-sum rewards, i.e., R(I) = -R2 = Rt, where
Rt := R(st, at(1), a(t2)). With this Markov decision process, we can define the following state value
function associated with the players’ policies π(1) and π(2) for any environment state s ∈ S.
∞
Vπ(1),π(2) (s) = E XγtRts0 = s .	(1)
t=0
The goal of both players is to compete via the following minimax game in all states s.
min max Vπ(1),π(2) (s).	(2)
In particular, it has been shown in (Shapley, 1953) that there exists a Nash equilibrium policy pair
∏(1),∏(2) for zero-sum Markov games, i.e., V ⑴ ⑵(S) ≤ V ⑴ ⑶(S) ≤ V ⑴ ⑶(S) holds for
∏ ,∏*	∏i ,∏*	∏i ,∏∖r
any other policies π(1), π(2) and for all states S.
2.2	Entropy-regularized Markov game
Entropy regularization is a popular technique that has been widely used in reinforcement learning (Neu
et al., 2017; Geist et al., 2019; Mei et al., 2020; Cen et al., 2020) and Markov game (Mertikopoulos
and Sandholm, 2016; Savas et al., 2019; Cen et al., 2021) to encourage environment exploration and
accelerate algorithm convergence.
Specifically, for the zero-sum Markov game, we can define an entropy-regularized state value function
by adding entropy regularization to the state value function in (1) as follows (Cen et al., 2021).
∞
V(TI)) ,π(2) (S) : = E[ X Yt[Rt - T ln π(1)(atI)ISt ) + T ln n(2)(at2)|st)]|S0 = s,⑶
t=0
where T > 0 is called the regularization parameter. With the above definition, we further define the
following entropy-regularized state-action value function (also called Q-function) (Cen et al., 2021).
QnT1),π(2) (S, a(1), a(2)) : = R(s, a(1),a(2)) + YEs0〜P(∙∣s,α⑴,。⑶)[V∏(m,π(2) (SO)].	(4)
3
Published as a conference paper at ICLR 2022
In particular, Vπ((τ1)),π(2) can be obtained from Q(πτ()1),π(2) as follows.
Vn(TI)),π(2) (S) =[n(I)(S)]>QΠτ1) ,π⑵(S)n(2)(S) + TH(π(I)(S)) - TH(π⑵(S))
=fτ(Q∏τ1),π ⑶⑶/⑴(S),n(2)(S)),	⑸
where H(π) denotes the entropy of policy π, and we define this mapping as fτ for convenience.
For the entropy regularized Markov game, it has an equilibrium policy pair that solves the minimax
optimization problem minπ(2) maxπ(1) Vπ((τ1)) π(2) (S). Such a policy pair is called the quantal response
equilibrium (QRE). Our goal is to find the equilibrium policy pair of the original Markov game in
(2) by solving the entropy-regularized Markov game with a proper regularization parameter T . In
particular, compared with the equilibrium policy of the Markov game, the QRE tends to have a larger
entropy due to the entropy regularization, which encourages the players to explore and obtain a
better understanding of the environment. Another advantage of considering the entropy-regularized
Markov game is that the entropy regularization makes the minimax problem have a better optimization
geometry that accelerates the convergence of the optimization process.
3	Stochastic Policy Extragradient Algorithm for
Entropy-Regularized Markov Game
In this section, we develop a stochastic policy extragradient (SPE) algorithm for solving entropy-
regularized Markov games. First, we recap the policy extragradient (PE) algorithm, which is
introduced in (Cen et al., 2021) to solve entropy-regularized Markov games with full knowledge
of the environment transition kernel and reward mapping. Then, we propose the model-free SPE
algorithm that solves entropy-regularized Markov games using only stochastic samples.
3.1	Review of policy extragradient algorithm
Value iteration is a classical reinforcement learning algorithm that requires full knowledge of the
environment and achieves an exponential convergence rate. In particular, for the entropy-regularized
Markov game, the k-th value iteration update is defined as follows.
Qk(s, a(1),a(2)) =R(s, a(1),a(2)) + YEs，〜P(∙∣s,α(i),θ⑶)[Vk3)],	∀s, a(1),a(2),	(6)
Vk+ι(S)= min max f (Qk(s);开⑴⑸7⑵⑸)，∀s,	⑺
π(2)(s) π(1)(s)
where We define Qk(s) := Qk(s, ∙, ∙) ∈ RIA(I)l×lA(2)|. This algorithm alternatively updates all the
entries of the value functions Qk and Vk. Thanks to the entropy regularization in the function fτ (see
(5) for the definition), the minimax matrix game in (7) is T -strongly concave in π(1) and T -strongly
convex in π(2), and therefore it has a unique solution.
To solve the entropy-regularized minimax matrix game in (7), Cen et al. (2021) proposed a predictive
update (PU) algorithm. Specifically, with uniform policy initialization, i.e., πkm'(S) = ∣A1m)∣ , ∀m ∈
{1, 2}, ∀S ∈ S, the PU algorithm performs the following policy updates: for t = 0, 1, 2, ...
(PU):
'πk,t+ι(a(I)IS) (X πk,)(a⑴|S)1-nTexP(nQk；t(S,a(I)))
πk2t+ι(a⑵IS) X nk2)(a(2)|S)1-nT eχp ( - InQkt(S,/2》	8
π⅛+ι(a(I)IS) X πk,)(a(1)|S)1-nT exP(nQk1t+i(S,a(I)))，
、喊3(。⑵IS) X πk2)(a⑵IS)1-ητ eχp ( - nQk2)+i(S,a(2)))
where we use the following notations (superscript (\m) denotes the opponent of the m-th player.).
Qkm)(S,a(m)) := Ea(∖m)〜成m)(s)[Qk(S,a(1),a⑵)],m ∈ {1, 2}
Qkm+ι(S,a(m)) ：= Ea(∖m)〜松窜(s)[Qk(S,a(1),a⑵)],m ∈ {1, 2}.
(9)
(10)
4
Published as a conference paper at ICLR 2022
Once we obtain the output policy pair (πk(1), πk(2)) of the PU algorithm, we can obtain an approxi-
mation of Vk+ι(s) as V0+ι(s) = f (Qk(s); ∏k1)(s), ∏k2)(s)), which Will be further used in the next
Q-value function update (6) to replace Vk+1(s0). The updates (6), (7) & (8) are referred to as policy
extragradient (PE) algorithm.
In the PE algorithm, the PU update in (8) allows both players to take symmetric updates without
revealing their private actions, and has been shown to converge to the unique solution of the entropy-
regularized matrix game (7) exponentially fast (Cen et al., 2021). However, the PE algorithm has
several limitations. First, in the PU update, each player m ∈ {1, 2} needs to query the quantities
Qkm) (s, a(m)), Qkm) (s, a(m)) from its opponent. To compute these quantities, the opponent needs to
multiply the entire Q-table by its own policy vector. This requires both players to coordinate with
each other and share a Q-table. Second, the update of the Q-table in (6) requires full knowledge of the
environment transition kernel P and the reward mapping R, which are unknown a priori in practice.
To overcome these limitations, we develop a fully stochastic PE algorithm in the next subsection.
3.2	Stochastic policy extragradient algorithm
The major challenge of the PE algorithm is computing the quantities Qkm), Qkm and V0+ι, which
requires coordinating with the opponent and involves the environment information. Here, we develop
a model-free and fully stochastic variant of PE that estimates these key quantities using Markovian
stochastic samples. We refer to this algorithm as stochastic policy extragradient (SPE).
Specifically, we first estimate the quantity V0+ι(s) = f (Qk(s); ∏k1)(s), ∏k2)(s))∙ By definition of
fτ in (5) and the update of Qk in (6) (now we use Vk0(s0) instead of Vk (s0)) and using some standard
tricks on random variables (see Lemma 2 in Appendix B for a full proof), we can rewrite Vk0+1(s) as
E [(R(e, e(1), e(2)) + γVk,(s0 ))i{e =s}]
μk (s)
+ TH(πk1)(S)) - TH(πk2)(S)),	(II)
where μk(s) denotes the stationary state distribution associated with the policy pair (∏k1),∏k2)), and
the expectation is taken over e 〜 μk,e(1) 〜 ∏k1)(s), e(2)〜 ∏k2)(s),s0 〜 P(∙∣e, e(1), e(2)). To
estimate this quantity, we query a set Nk+1 (with cardinality Nk+1) of samples from the Markov
decision process following the pair of policies (πk(1), πk(2)). Then, we estimate Vk0+1(S) as
Vbk+1(S)
Nk1+1 Pi∈Nk+ι (Ri + YVk(Si+I))I{Si = s} + TH
Nk1+1 Pi∈Nk+ι 1{si =s}
- TH
(πk2)(s)).
(12)
Intuitively, we use the sample average of Markovian samples to estimate the expectation terms in
(11). Thanks to the concentration phenomenon of dependent samples (Paulin, 2015), these sample
averages converge to the desired expected values provided that the sample size is sufficiently large.
Next, we estimate Q(km,t), m ∈ 1, 2. Leveraging (9) and (6), we obtain the following equivalent
characterization for both players m ∈ 1, 2 (see Lemma 2 in Appendix B for the proof of equivalence).
Q(km,t)(S, a(m))
e [(R(e, e⑴,e⑵)+ γ%(s0))i{e = s, e(m) = a(m)}]
μk,t(s)πkm)(a(m)|s)
(13)
where 1{∙} denotes the indicator function, μk,t denotes the stationary state distribution associated
with the policy pair (πk(1,t), πk(2,t)), and the expectation is taken overe〜μk,t,e(I) 〜∏k,)(s),e⑵〜
∏k2)(s), s0 〜 P(∙∣e, e(1), e(2)). To estimate this quantity, we query a setNk,t (with cardinality Nk,t)
of samples from the Markov decision process following a pair of smoothed policies πk0(,mt ) (S) =
(1 - e0)∏km)(s) + |/；二)∣ 1, where e0 ∈ [0,1] is a small smoothing constant that will be theoretically
determined later. Then, we estimate Q(km,t) (S, a(m)) as follows.
',i∈Nk,t (R+YVk(Si+I))I{Si =S,a(m) = a(m)}
Qkm)(S，a(m)) = Nkt^
(Nkt Pi∈Nk,t 1{Si = S})nk(，m)(a(m)|S)
(14)
5
Published as a conference paper at ICLR 2022
where we have replaced the expectations with sample averages, and replaced Vk0 (s0) with Vk (s0).
Here, the Markovian samples are queried following the 0-smoothed policies (πk0(,1t), πk0(,2t)). On one
hand, 0 should not be too small so that it keeps the denominator of the above estimation away from
zero. On the other hand, 0 should not be too large so that it is sufficiently close to the original policy.
Similarly, to estimate Qkm), We query another set Nk,t (with cardinality Nk,t) of samples from the
Markov decision process following a pair of smoothed policies n，：) (S) = (1 -Z0)∏km^ (s) + 方m∣ 1,
where 毛'∈ [0,1] will be theoretically determined later. Then, we estimate Qkm) as follows.
^ (m)
Qk,t (S
=Nk,t Pi∈Nk,t (Ri + YVfe(Si+I))I{Si = s,a(m) = a(m)}
a 二	(J Pi∈Nk,tl{Si = s})∏k(,m)(a(m)∣s)
(15)
Remark 1. The estimators (12), (14) & (15) essentially use ratio of sample averages to approximate
ratio of expectations. Estimators with similar structure have been used in (Ortner and Auer, 2007;
Xia et al., 2016; Wei et al., 2021). However, these works analyze the estimation error of their
estimators with independence samples. As a comparison, our analysis of the estimation error bounds
the additional bias induced by the correlated Markovian samples, and achieves an improved sample
complexity in the main Theorem 2.
We summarize our stochastic policy extragradient (SPE) algorithm in Algorithm 1. Specifically, in
SPE, we estimate the quantities Qkrt), Qkm), V0 using their corresponding stochastic estimators. As
a result, the SPE algorithm is model-free, and the updates for both players are symmetric and private.
Algorithm 1 Stochastic policy extragradient (SPE) for entropy-regularized Markov game
Initialize: V0(s) for all s ∈ S.
for value iterations k = 0, 1, . . . , K - 1 do
Initialize πk(1,0), πk(2,0) with uniform distribution.
for PU iterations t = 0, 1, . . . , Tk - 1 do
Players 1,2 sample Nk,t Markovian samples following smoothed policies πk0(,1t), πk0(,2t).
Every player m ∈ {1, 2} computes Qb(km,t)(S, a(m)) for all S, a(m) using (14).
Players 1,2 sample Nk,t Markovian samples following smoothed policies ∏d∏]?.
^ (m)
Every player m ∈ {1,2} computes Qkt (s, a(m)) for all s, a(m) using (15).
Implement the t-th PU iteration for all S, a(1), a(2) using (8) with estimations (14)&(15).
end
Let ∏km) = ∏kmT)fc, m = 1, 2. Players sample Nk Markovian samples following ∏k1), ∏k2).
Compute Vk+1(S) for all S using (12).
end
Output: πK(1-) 1,πK(2-) 1.
4	Finite-Time Convergence Analysis of SPE
Throughout our convergence analysis, we adopt the following two standard assumptions.
Assumption 1. Denote Tπ(1),π(2) (S, S0) := inf{t ≥ 1 : St = S0 |S0 = S} as the first-visit time under
the policy pair π(1), π(2). We assume that
sup sup Eπ(1),π(2) Tπ(1),π(2) (S, S0) < +∞.	(16)
s,s0 ∈S π(1) ,π(2)
Assumption 1 is widely used in the reinforcement learning literature (Ortner and Auer, 2007; Ortner,
2020; Wei et al., 2021; Jafarnia-Jahromi et al., 2021). It ensures that every state will be visited at least
once within a finite duration of time, thus ensuring that all the states will be visited infinitely often.
6
Published as a conference paper at ICLR 2022
This guarantees sufficient exploration. In our analysis, we use the following equivalent statement
of Assumption 1 for convenience, which means that the stationary state distribution μ∏(i),∏(2) has a
uniform lower bound μmin > 0. Their equivalence is based on Theorem 5.5.11 of (Durrett, 2019).
μmin := inf inf μ∏(i) ∏(2) (s) = [sup SuP E∏(i) ∏⑶Tn⑴ ∏(2) (s,s)i	> 0.	(17)
s∈S π(1),π(2)	s∈S π(1),π(2)
Assumption 2. There exists a mixing time tmix ∈ N such that for any policy pair π(L), π(2) and its
corresponding stationary state distribution μ∏(i),∏(2), we have
dTV ^Pn(I) ,∏(2)(stmix ) , μ∏(1) ,∏(2) ) ≤ 4 .
where Pπ(1),π(2) (stmix) denotes the state distribution under the policy pair π(L), π(2) at time tmix, and
dTV denotes the total variation distance between two probability distributions.
In this subsection, we analyze the finite-time convergence of Algorithm 1 for solving the entropy-
regularized Markov game (5). We focus on the convergence rate of the following Nash equilibrium
duality gap, which is a standard optimality metric widely adopted in the existing literature (Xu et al.,
2020; Jin and Sidford, 2021; Wei et al., 2021).
D(τ)(π(1), π(2)) := msax ( mnax Vn(,τn)(2) (s) 一 mi0n Vn((τ1)),n0 (s).
In particular, when T = 0, D(0)(π(1), π(2)) corresponds to the duality gap of the original Markov
game. Throughout, we define Amax ：= max{∣A⑴∣, |A(2)|}, Qmax := 11lnγ4max and Vmax ：=
1，-Amax. We also require that the batch sizes of Algorithm 1 satisfy the following conditions.
Nk,t,Nk,t ≥
Nk+1 ≥
650tmixAmax ln (20TiUmISIAmax	∀k t
μmin	δ√ μmin
650tmix	1 (	4 λ ∀k
μmin(1- Y)2 n δδμm-J ,	.
(18)
(19)
Then, we obtain the following convergence result of Algorithm 1, where Tsum := PkK=-01 Tk.
Theorem 1 (Finite-time convergence rate). Apply Algorithm 1 to solve the entropy-regularized
Markov game with τ ∈ (0, 1]. Choose learning rate η = [2(τ + Qmax)]-1, initialization kVb0k∞ ≤
Vmax and batch sizes Nk,t, Nk,t, Nk+1 that satisfy (18) & (19). Then, the Nash equilibrium duality
gap converges at the following rate with probability at least 1 一 δ.
D(T) (∏K-1, ∏K-1) ≤ O (/axlnAmax KX YK-k (1 一 ητ)Tk-1
1 一 γ	k=0
+ VmaX [tmixAmaX 1口 (TsUm ∣S∣Amax
1 一 Y〔 μmin	δμmin
K-1
i2/3 X γK-k-L
+	Vmax '
TNkk31
k,t+L
+ VmaxYK +t mix Vmax	∣n
T2(1 — Y)2	T2μmin(1 - γ)3
k=0
K-1
X
k=0
γK-k-1
Nk+1
(20)
Remark 2. In the proof of Theorem 1, we also prove that the convergence rate of the Q-function
estimation error kQκ 一 Qr) k∞ is (1 — Y) times the convergence rate in (20). Here, QK corresponds
to the Q-function associated with the policy pair (πK(L), πK(2)) produced by Algorithm 1 in the K-th
(τ)
iteration, and Q∖)corresponds to the optimal Q-functιon associated with the Nash equilibrium
policy pair π(T), ∏(T) ofthe entropy-regularized Markov game.
Theorem 1 characterizes the convergence of duality gap of the SPE algorithm under general hy-
perparameter scheduling of the batch sizes Nk,t, Nk,t, Nk+ι and number of inner iterations Tk.
Specifically, it can be seen that as the number of outer iterations K and inner iterations Tk increase,
the duality gap converges to an exponentially weighted average of N-2/3, Nk,t/3L and N-1], and the
7
Published as a conference paper at ICLR 2022
gap can be made arbitrarily small by choosing sufficiently large batch sizes. To provide an intuitive
understanding, We can set these hyper-parameters as constants, i.e., Tk ≡ T, Nk,t ≡ N, Nk,t ≡ N,
Nk+1 ≡ N0, and then the convergence rate in (20) simplifies to
O
+
VmaxY K
τ 2(1 - γ)2
YVmax ln Amax
(1-γ)2
(1 - ητ)T-1
ητ (1 - ητ)(1 - Y)2
+
tmixVmax	ι
T 2μmin(1 — Y)4
(KSL)ɪ
δμmin N
h tmixAmax ]口
L μmin
(21)
+
To elaborate, the first term characterizes the exponential convergence of the K outer value iterations.
This convergence is faster than the sublinear convergence θ(ʌ/ln K/K) established in (Wei et al.,
2021). The second term characterizes the exponential convergence of the T inner PU iterations.
Moreover, the last two terms characterize the estimation errors O(N-1) and O(N-2/3) + O(N 2/3)
of the estimators (12) and (14,15), respectively. As a comparison, Wei et al. (2021) established a
much larger estimation error O(N -1/3). These improvements, as we show in Theorem 2 later, lead
to a substantially improved overall sample complexity over the state-of-the-art result. In particular,
due to the exponentially weighted average structure in (20), the sample complexity can be further
optimized by an adaptive scheduling of the batch sizes.
Technical Novelty. We further comment on the proof of Theorem 1. Note that the analysis of the PE
algorithm in (Cen et al., 2021) requires full knowledge of the environment and does not characterize
the convergence of duality gap. As a comparison, to establish the duality gap convergence rate (20) of
the model-free SPE, we need to make substantial new developments to tightly bound the estimation
errors of the proposed stochastic estimators. We elaborate our technical contributions below.
•	As we explained in Remark 1, the sample averages involved in our estimators are correlated
Markovian samples. To bound the additional bias induced by these correlated samples, we apply
the concentration inequalities developed in (Paulin, 2015) for dependent samples to establish tight
high-probability estimation error bounds.
•	We develop a much refined analysis of the state value function estimation error kVk+1 - Vk0+1 k∞,
which is the key to develop tight bounds for all the other estimation errors. Specifically, we first
propose the following error decomposition for any state s
kies)-%+1(S)Tvμ+f
vk+ι(s) I ≤ Ibk+1(S)-vk+i(s)| + iʌ (H μk(S)-bk(S) I
~μ(S)Γ l- μk(S)	+ 1 k+1( )l∣ μk (s)bk (S) 1
where bk+ι(S), bk+ι(S) are sample average estimators of Vk+1(S),μk+1 (s), respectively, and we
refer to Appendix B for the definitions of these terms. The motivation is that the Ivbk+1(S)I in the
second term helps cancel out the estimator μk (s) in the denominator, and then all the denominators
do not involve any sample average estimators, which may take a small numerical value that causes
divergence and a loose concentration bound. By leveraging this special decomposition and the
recursive structure of the stochastic estimator (12), we are able to establish the following key
contraction property of the estimation error (see (85) in Appendix B).
IlVk+1 - VO+1k∞ ≤ YIlVk - V0k∞ + O(Nk+/2)∙
By telescoping the above contraction bound, we obtain tight estimation error bounds for all the
proposed stochastic estimators. As a comparison, Wei et al. (2021) directly applied the Azuma-
Hoeffding inequality with independent samples to bound the entire estimator and obtain a loose
error bound, and Liu et al. (2021) simply assumed a small upper bound for the estimation error.
•	We develop a stochastic predictive update (SPU) algorithm with general inexact value function
estimations and a finite-time convergence analysis of its duality gap (see Lemma 1 for the SPU
algorithm and its convergence proof). This generalizes the convergence result of the PU algorithm
established in (Cen et al., 2021), which uses exact value functions based on full knowledge of
the environment. Finally, by incorporating our developed tight estimation error bounds into the
finite-time duality gap bound of SPU, we obtain the desired convergence rate in Theorem 1.
Based on Theorem 1, we obtain the following sample complexity of SPE for achieving an -Nash
equilibrium duality gap of the original Markov game, i.e., D(0) (πK(1-) 1, πK(2-) 1) ≤ . Here, we adopt
an adaptive batch size scheduling scheme to optimize the complexity order. The overall sample
complexity is given by PK=o1 [Nk+i + 2 PT= -1(Nk,t + Nk,t+ι)].
8
Published as a conference paper at ICLR 2022
Theorem 2 (Sample complexity). Implement Algorithm 1 with η = O(1 — Y), T = O( IA4-γ)),
K = O [j--γ ln (InAmax) ] and Tk = 1+ 1.11-YT)T ∙ Choose thefollowing adaptive batch sizes.
Nk+A = 0("；：1*/ )，Nk,t = N"2 (1-Y)3 = 0
-3.+1)
max(1 — ητ)	5
μmin(I ― Y)3
Then, for any e ≤ InIAmax, the OVeraU sample complexity to achieve D(0)(π(K-1, ∏K2-1) ≤ e is
O(μ ∙ ItmxAmaY)i3.5). Please refer to (118) inAppendixE for a complete expression.
The above complexity result is obtained by choosing a small T = O(1/)) for the convergence rate
result in Theorem 1. Specifically, we show in Lemma 6 that the duality gap is Lipschitz continuous
with regard to the entropy regularization parameter, i.e., D(τ)(π(A), π(2)) — D(0) (π(A), π(2)) ≤
2τ ln—max. Therefore, by choosing a proper small T, convergence of the duality gap D(T) of the
entropy-regularized Markov game implies the convergence of the duality gap D(0) of the original
Markov game.
Remark on Improvement of Sample Complexity. We elaborate on the improvement of sample
complexity in two different levels: population level (with known environment) and stochastic level
(using stochastic samples). First, in the population level, the original policy extragradient (PE)
algorithm in (Cen et al., 2021) already achieves a faster convergence rate than the OGDA-based
algorithm proposed in (Wei et al., 2021). Specifically, to achieve an e-Nash equilibrium point of
the original Markov game, PE requires O ((AmY)ln2 e_1) iterations by choosing a proper entropy
regularization parameter T = O( ；(；-Y)). As a comparison, the OGDA-based algorithm requires
1n Amax
O ((i厦)9m) iterations by substituting e = 0 (no stochastic error) and their choice of step size into
the Theorem 1 of Wei et al. (2021). This improvement is in the order of O(e-1) and is due to the use
of entropy regularization in the PE algorithm, which improves the geometry of the bilinear matrix
game. Second, the rest of the improvement of sample complexity (about Oe(e-1.5)) comes from the
stochastic level. Specifically, our stochastic PE (SPE) allows to use a large constant learning rate
η = O 1 — Y , whereas the OGDA-based algorithm in (Wei et al., 2021) needs to use a substantially
smaller learning rate η = O(ʌ/(l — γ)5∣S∣-1), which significantly slows down its convergence in
both the population and stochastic levels. Moreover, both the PE and our SPE take O(ln e-1) inner
updates to achieve an accurate solution of the matrix game (7), whereas the OGDA-based algorithm
uses only one inner update to solve the matrix game and hence suffers from a larger optimization error.
Finally, as we explained in the previous technical novelty paragraph, we improve the techniques used
in bounding the estimation errors. Specifically, Wei et al. (2021) bounds the estimation errors in all
the iterations by a small constant e using independent samples. As a comparison, we establish a key
contraction property of the estimation error over the iterations with correlated Markovian samples.
Such a property allows us to use a growing batch size that bounds the errors loosely in the initial
iterations and achieves tight error bounds in the end.
5 Conclusion
In this paper, we developed a model-free, provably convergent, sample efficient, symmetric and
private stochastic policy extra gradient algorithm for solving two-player zero-sum Markov games.
Our algorithm leverages entropy regularization to facilitate the algorithm convergence and develops
new stochastic estimators to accurately estimate the value functions. We proved that our SPE
algorithm achieved a fast convergence rate in terms of the Nash equilibrium duality gap and moreover,
achieves a substantially improved sample complexity over the state-of-the-art result. We believe our
algorithm deepens the understanding of Markov games from a computation complexity perspective.
In the future study, it is interesting to extend SPE algorithm to the multi-agent setting for solving
general-sum Markov games and competitive games that involve multiple cooperative teams. Another
interesting direction is to improve the algorithm to further reduce the sample complexity to approach
the theoretical lower bound established in (Zhang et al., 2020).
9
Published as a conference paper at ICLR 2022
Acknowledgments
The work of Ziyi Chen, Shaocong Ma and Yi Zhou was supported in part by U.S. National Science
Foundation under the Grants CCF-2106216 and DMS-2134223.
References
Bai, Y., Jin, C., and Yu, T. (2020). Near-optimal reinforcement learning with self-play. Proc. Advances
in Neural Information Processing Systems (Neurips), 33.
Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y. (2020). Fast global convergence of natural policy
gradient methods with entropy regularization. ArXiv:2007.06558.
Cen, S., Wei, Y., and Chi, Y. (2021). Fast policy extragradient methods for competitive games with
entropy regularization. ArXiv:2105.15186.
Daskalakis, C., Foster, D. J., and Golowich, N. (2020). Independent policy gradient methods for
competitive reinforcement learning. In Proc. Advances in Neural Information Processing Systems
(Neurips), volume 33.
Deng, X., Li, Y., Mguni, D. H., Wang, J., and Yang, Y. (2021). On the complexity of computing
markov perfect equilibrium in general-sum stochastic games. ArXiv:2109.01795.
Durrett, R. (2019). Probability: theory and examples, volume 49. Cambridge university press.
Elie, R., PerolaL J., Lauriere, M., Geist, M., and PietqUin, O. (2020). On the convergence of model
free learning in mean field games. In Proc. the AAAI Conference on Artificial Intelligence (AAAI),
volume 34, pages 7143—7150.
Ernst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556.
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep q-learning. In Proc.
Learning for Dynamics and Control (L4DC), pages 486-489.
Farahmand, A.-M. and Ghavamzadeh, M. (2021). Pid accelerated value iteration algorithm. In Proc.
International Conference on Machine Learning (ICML), pages 3143-3153.
Geist, M., Scherrer, B., and Pietquin, O. (2019). A theory of regularized markov decision processes.
In Proc. International Conference on Machine Learning (ICML), pages 2160-2169.
Ghosh, M. K., Golui, S., Pal, C., and Pradhan, S. (2021). Zero-sum games for continuous-time
markov decision processes with risk-sensitive average cost criterion. ArXiv:2109.08837.
Gu, H., Guo, X., Wei, X., and Xu, R. (2021). Mean-field multi-agent reinforcement learning: A
decentralized network approach. ArXiv:2108.02731.
Guo, H., Fu, Z., Yang, Z., and Wang, Z. (2021). Decentralized single-timescale actor-critic on
zero-sum two-player stochastic games. In Proc. International Conference on Machine Learning
(ICML), pages 3899-3909.
Guo, X., Hu, A., Xu, R., and Zhang, J. (2019). Learning mean-field games. Proc. Advances in Neural
Information Processing Systems (Neurips), 32:4966-4976.
Hu, J. and Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069.
Huang, B., Lee, J. D., Wang, Z., and Yang, Z. (2021). Towards general function approximation in
zero-sum markov games. ArXiv:2107.14702.
Jafarnia-Jahromi, M., Jain, R., and Nayyar, A. (2021). Learning zero-sum stochastic games with
posterior sampling. ArXiv:2109.03396.
Jin, Y. and Sidford, A. (2021). Towards tight bounds on the sample complexity of average-reward
mdps. In Proc. International Conference on Machine Learning (ICML), pages 5055-5064.
10
Published as a conference paper at ICLR 2022
Kononen, V. and Oja, E. (2004). Asymmetric multiagent reinforcement learning in pricing ap-
plications. In Proc. IEEE International Joint Conference on Neural Networks (IEEE Cat. No.
04CH37541), volume 2, pages 1097-1102.
Leonardos, S., Overman, W., Panageas, I., and Piliouras, G. (2021). Global convergence of multi-
agent policy gradient in markov potential games. ArXiv:2106.01969.
Liu, B., Yang, Z., and Wang, Z. (2021). Policy optimization in zero-sum markov games: Fictitious
self-play provably attains nash equilibria.
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D. (2020). On the global convergence rates of
softmax policy gradient methods. In Proc. International Conference on Machine Learning (ICML),
pages 6820-6829.
Mertikopoulos, P. and Sandholm, W. H. (2016). Learning in games via reinforcement and regulariza-
tion. Mathematics of Operations Research, 41(4):1297-1324.
Moerland, T. M., Broekens, J., Plaat, A., and Jonker, C. M. (2018). A0c: Alpha zero in continuous
action space. ArXiv:1805.09613.
Neu, G., Jonsson, A., and G6mez, V. (2017). A unified view of entropy-regularized markov decision
processes. ArXiv:1705.07798.
Ortner, P. and Auer, R. (2007). Logarithmic online regret bounds for undiscounted reinforcement
learning. In Proc. Advances in Neural Information Processing Systems (Neurips), volume 19,
page 49.
Ortner, R. (2020). Regret bounds for reinforcement learning via markov chain concentration. Journal
of Artificial Intelligence Research, 67:115-128.
Paulin, D. (2015). Concentration inequalities for markov chains by marton couplings and spectral
methods. Electronic Journal of Probability, 20:1-32.
Savas, Y., Ahmadi, M., Tanaka, T., and Topcu, U. (2019). Entropy-regularized stochastic games. In
Proc. IEEE 58th Conference on Decision and Control (CDC), pages 5955-5962.
Shapley, L. S. (1953). Stochastic games. Proceedings of the national academy of sciences,
39(10):1095-1100.
Sidford, A., Wang, M., Yang, L., and Ye, Y. (2020). Solving discounted stochastic two-player games
with near-optimal time and sample complexity. In Proc. International Conference on Artificial
Intelligence and Statistics (AISTATS), pages 2992-3002.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,
L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. nature,
550(7676):354-359.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In Proc.
Advances in neural information processing systems (Neurips), pages 2154-2162.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H.,
Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent
reinforcement learning. Nature, 575(7782):350-354.
Wang, X. and Sandholm, T. (2002). Reinforcement learning to play an optimal nash equilibrium in
team markov games. volume 15, pages 1603-1610.
Wei, C.-Y., Hong, Y.-T., and Lu, C.-J. (2017). Online reinforcement learning in stochastic games. In
Proc. Advances in neural information processing systems (Neurips), pages 4994-5004.
Wei, C.-Y., Lee, C.-W., Zhang, M., and Luo, H. (2021). Last-iterate convergence of decentralized op-
timistic gradient descent/ascent in infinite-horizon competitive markov games. In Proc. Conference
on Learning Theory (COLT).
11
Published as a conference paper at ICLR 2022
Xia, Y., Ding, W., Zhang, X.-D., Yu, N., and Qin, T. (2016). Budgeted bandit problems with
continuous random costs. In Proc. Asian conference on machine learning (ACML), pages 317-332.
Xu, Y., Deng, Z., Wang, M., Xu, W., So, A. M.-C., and Cui, S. (2020). Voting-based multiagent
reinforcement learning for intelligent iot. IEEE Internet of Things Journal, 8(4):2681-2693.
Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020). Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. In Proc. Advances in Neural Information
Processing Systems (Neurips), volume 33.
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2021). Finite-sample analysis for decentralized
batch multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic
Control.
Zhang, X., Zhang, K., Miehling, E., and Basar, T. (2019). Non-cooperative inverse reinforcement
learning. In Proc. Advances in Neural Information Processing Systems (Neurips), volume 32,
pages 9487-9497.
Zhao, Y., Tian, Y., Lee, J. D., and Du, S. S. (2021). Provably efficient policy gradient methods for
two-player zero-sum markov games. ArXiv:2102.08903.
Zhu, Y. and Zhao, D. (2020). Online minimax q network learning for two-player zero-sum markov
games. IEEE Transactions on Neural Networks and Learning Systems.
Zou, S., Xu, T., and Liang, Y. (2019). Finite-sample analysis for sarsa and q-learning with linear
function approximation. ArXiv:1902.02234.
12
Published as a conference paper at ICLR 2022
Appendix
Table of Contents
A	Analysis of stochastic PU (SPU)	for entropy-regularized matrix game	13
B	Estimation error bounds	18
C	Properties of the Duality Gap	25
D	Proof of Theorem 1	27
E	Proof of Theorem 2	31
F	Summary of Comparison of Sample Complexities	34
G	Rationality	35
A Analysis of stochastic PU (SPU) for entropy-regularized
MATRIX GAME
We first consider the following zero-sum entropy regularized matrix game, which can be considered
as a simple special case of Markov game with only one state.
max min hr(μ, V):= μ>Qν + TH(μ) — TH(V).	(22)
μ∈∆(dι) ν∈∆(d2)
where Q ∈ Rd1 ×d2 is fixed. The solution (μT, VT) of the above problem, also known as the quantal
response equilibrium (QRE), satisfies the following condition (Cen et al., 2021)
μT H exp(QνT∕τ),νT H exp(—Q>μT/τ).	(23)
Our stochastic PU (SPU) algorithm for the above matrix game is written as follows,
μt+ι(a) Hμt⑷1-ητexp (η[QVt + δ(1)]a)
Vt+ι(b) HVt(b)1-ητ exp ( — η[Q>μt + δ(2)][)
μt+l(a) Hμt(a)1-ητ exp (η[QVt+1 + δt1)]a)
Vt+ι(b) HVt(b)1-ητ exp ( - n[Q>μt+1 + δ(2)]
b,
(24)
(25)
(26)
(27)
where δ(1) is an additive noise to the underlying true quantity QVt, and the other noises δ(2), δt1),
δt2) are similar. If all these noises are zero, the above stochastic PU algorithm reduces to the PU
algorithm for matrix game defined in Section 2 of (Cen et al., 2021).
The convergence rate of stochastic PU for matrix game is shown below. The proof logic follows from
that of (Cen et al., 2021).
Lemma 1. Choose learning rate η ≤ [2(T + kQk∞)]-1 and use the uniform policy initialization
μ0 = 1/d1, V0 = 1/d2. Then, the SPU algorithm defined by eqs. (24)-(27) for entropy-regularized
matrix game has the following convergence rates.
KL(μT∣∣μt)+ KL(VTlIVt)
≤ (1 — ηT)tln(d1d2)
13
Published as a conference paper at ICLR 2022
+ E(I- ητ 尸-jE2(MjI) ∣∣∞ + Mf) ∣∣∞) + B (危 I)IlIO + 危 2)吆)].(28)
∣hτ(μT, ν?) - hτ (μt+1, Vt+1)∣
4 一
≤ η(1 - ητ)t ln(d1d2)
+ E(1 - F) jj[8相MjI)I∣∞ + 眦2)吆)+ 43(危I)∣∞ + 同2)-)].	(29)
j=0
max hτ(μ,Vt+ι) — min hτ(μt+ι,ν)
μ∈∆(di)	ν∈∆(d2)
2 一
≤ n(1 - ητ)t ln(d1d2)
+ E (1 - ητ) jj[16η(Mj1l∞ + Mj2l∞) + 12 侗I)I∣∞ + 同 2)吆)].(30)
Proof. Eq. (26) implies that for some constant C ∈ R,
lnμt+ι = (1 - ητ)lnμt + η(Qνt+1 + δ(I)) + cl
⇒〈lnμt+ι - (1 - ητ)lnμt - n(QVt+1 + δt ),μt+ι - μT)= °∙	(31)
where We used (7t+ι - μT, 1)= °. Similarly, eqs. (27)&(23) respectively imply that
〈ln νt+ι - (1 - ητ) lnVt + η(Q>"t+ι + δ(2)),Vt+ι - v» =°	(32)
〈lnμT - QVT∕τ,"t+ι- μT〉=°	(33)
〈lnVT + Q>μT∕τ,Vt+ι - V[ =°.	(34)
Similarly, eqs. (24)&(26) imply that
<ln(μt+/μt+1),"t+1 - μt+1)
=<η[Q(Vt - Vt+1) + δ(I) - δt)] ,μt+1 - μt+1)
≤ ηH Qn ∞Hμt+1- μt+1 Ii 1 IlVt- Vt+1ii1+ηM(I)- δt )∣ι∞ι∣μt+1- μt+1∣∣1
2
≤ 2 llQIl(I^t+1 - μt+1∣∣2 + IlVt - V t+1∣∣2) + 2. _ 闫| Qll^^ʌ Mtl - δt ιι∞
2	2(1 - ηllQIl∞)
+ 2(I- η∣∣Qh)∣∣μt+1 - μt+1∣∣2
≤ KL(4t+1∣∣μt+1) + η∣∣Q∣∣∞KL(Vt+1∣∣Vt) + 2η2(M(1)∣∣∞ + 同I)Il∞),	(35)
where (i) uses the Pinsker,s inequality and η ≤ [2(τ + ∣∣Q∣∣∞)[ 1.
Similarly to the above derivation, we can infer from eqs. (25)&(27) that
〈ln(Vt+1/Vt+1),Vt+1 - Vt+1〉≤KL(νt+1|Vt+1) + η∣∣Qll∞KL(μt+1Hμt)
+ 2η2(M(2l∞ + 同2)I1).	(36)
Computing eq. (31)+eq. (32)-ητ [eq. (33)+eq. (34)] yields that
ηhδ(I),μt+1 - μT〉- ηG(2),Mt+1 - VT〉
=(lnμt+1 - (1 - ητ)lnμt - ητln〃T - ηQ(Vt+1 - VT),bt+1 - 〃T〉
+〈lnVt+1 - (1 - ητ)lnVt - ητlnVT + ηQ>(7t+1 - 〃T),Vt+1 - VT
=〈ln(μt+JμT) - (1 - ητ) ln(μJμT),μt+1 -工〉
14
Published as a conference paper at ICLR 2022
+ <ln(%+ι/VT) - (1 - ητ)ln(%/VT),Vt+ι - *
=KL(μT kμt+ι) — (1 — ητ)KL(MTllMt)
+〈 lWμt+1∕μT) - ln(μt+1∕μt+1) - (1 - ητ) [lWμt+1∕μ;) - ln(μt+1 /μt)],μt+1)
+ KL(VTkVt+1) - (1 - nτ)KL(VTkVt)
+〈 ln(Vt+1/VT) - ln(Vt+1∕Vt+1) - (1 - ητ) [ ln(Vt+ι∕"T) - ln(Vt+ι∕"t)],玩十1)
=KLG"∣μt+1) - (1 - ητ)KL(μ"∣μt) + ηTKL("t+1M；)
+ (1 - ητ)KL(Mt+1kμt) + KLGt+1∣∣μt+1)
+ KL(V*∣Vt+1) - (1 - ητ)KL(vWIIVt)+ ητKL(Vt+1∣∣V*)
+ (1 - ητ)KL(Vt+1 ∣∣Vt) + KL(Vt+1∣∣Vt+1)
-〈In(Mt+1∕μt+1),"t+1 - μt+1) -〈In(Vt+1/Vt+1), Vt+1 - Vt+1)
(i)
≥ KLG"∣μt+I)-(I- ητ)KL(μ"∣μt) + ηTKL(m+11.；)+ (I- ητ - η∣Qk∞)KL(Mt+1 Mt)
+ KL(V*∣Vt+1) - (1 - ητ)KL(vWIIVt)+ ητKL(Vt+1∣∣V*) + (1 - ητ - η∣∣Q∣∣∞)KL(Vt+1k%)
-2η2(M(I)∣∞ + 陋(I)∣∞ + ∣δ(2)∣∞ + 礴)∣∣∞),	(37)
where (i) uses eqs. (35)&(36). The left side of the above inequality has the following upper bound.
ηhδ(I),μt+1 - μ1) - η〈δ(2),νt+1 - V力
≤η∣δ(I)IUμt+1-μ"h + ηkδt2)∣u%+1-V"∣1
+ 2 kMt+1 - μ"2 + 亚=% + 2 B t+1 - v"∣2)
≤ "kδ(I) ∣∞ + ∣δt2)k∞) + η2τ (KL("t+1∣H) + KL(V t+1∣V：)).
Substituting eq. (38) into eq. (37) and rearranging it yields that
KL(μ"∣μt+1) + KL(V^ilVt+1)+ η~ [KL(μt+1llμ" + KL(V t+1llV；)]
+ 2 [KL(Mt+1Hμt)+ KL(V t+1|lVt)]
≤ (1 - ητ)[KL(μ"∣μt) + KLMkVt)] + 2η2(∣∣δ(I)∣∞ + ∣δ(2)∣∞)
+ 2η (kδ(I)k∞ + kδ(2)k∞),
where We use η ≤ [2(τ + ∣∣Q∣∞)] 1 ≤ (2τ)-1. This further implies that
KL(μ"∣μt+1) + KL"∣∣Vt+1)
≤ (1 -ητ)[KL(μ"∣μt) + KL(V"∣%)]
+ 2η2(M(I)k∞ + l∣δ⅛2)k∞)+ T ("δT )k∞ + ∣∣δt2 'k∞).
(38)
(39)
(40)
Consequently, eq. (28) can be proved via iterating the above inequality and using the facts that
KL(μ"∣μo) ≤ ln 由，KL(v*∣v0) ≤ ln d2 (Since μ0 and v0 are uniform probability vectors).
Next, we will prove eq. (29).
Computing eq. (31) -ητ∙eq. (33) yields that
〈Q(Vt+1 - V*),μt+1 - μτ)
=η-1<lnμt+1 - (1 - ητ)lnμt - ητlnμ^ - ηδ(I),Mt+1 - Mr)-	(41)
Hence, we conclude that
hT(μt+1, Vt+1) - hT (μ?, V?)
≤ hT(μt+1, νt+1) - h T(μt+1, V?)
15
Published as a conference paper at ICLR 2022
="t+1Q(Vt+1 ― V?) + TVt+1 ln Vt+1 ― TV?> ln v?
=(μt+1 — μT)>Q(V t+1 — VT)+ μTΤQ(V t+1 — v?) + TV t+1ln Vt+1 — TVT>ln VT
=ηT<ln μt+1 — (1 — ητ)ln μt — ητ ln μT — ηδtI),"t+1 — μT)— T〈ln vT,v t+1 — VT)
+ TVt+1 ln Vt+1 — TV?T ln VT
(=) η-1[KL(MTkμt+1) ―(1 ― ητ)KL(μTkμt) + ηTKL(μt+1kμT)+ (I ― η,)KL(m+1。%)
+ KL(Mt+1∣∣μt+1) — <lWμt+1∕μt+1),μt+1 — μt+1)] + TKL(Vt+1||V?) — hδt ),μt+1 — μT)
≤ η-1 [KL(MTkμt+1) — (1 — ητ)KL(μ"∣μt)+ ηT(KL(μt+1l∣μT)+ KL(V t+1∣∣VT))
+ (1 — ητ)KL(bt+1Mt) — KL(μt+1llμt+1)] + 2τ kδ('k∞ + 2 kμt+1 一 μTk2
≤ η-1 [KL(MTkμt+1) + 2ηT(KL(μt+1llμT) + KL(Vt+1|lV?)) + KL("t+1kμt)] + 4τl∣δt 'k∞
嗖 4η-1 [KL(〃Tk〃t)+ KL(VTkVt) + 2η2(∣∣δ(I)k∞ + 同2)k∞) + ?(怩(I)k∞ + kδ(2)k∞)]
+ ]同°k∞
(V)
≤ 4η-1[KL(μ"∣μt) + KL"k%)]
+ 助(1怜(I)k∞ + l∣δ(2)k∞) + 4τ (kδt"k∞ + l∣δt2 )k∞
(42)
where (i) uses eq. (41)&(34),(ii) follows the derivation of eq. (37), (iii) uses the Pinsker,s inequality,
(iv) uses eq. (39), and (v) uses η ≤ [2(τ + ∣∣Q∣∣∞)]	. In a similar way, we can also prove that
hτ (μT, ντ ) - hτ(μt+1, νt+1) ≤ hτ (μτ, V t+1) - hτ(μt+1, νt+1)
≤ 4η-1 [KL(μT∣μt) + KL(VTkVt)]
+ 8η(M(I)∣∞ + ∣δ(2)∣∞) + 43 (同A1 + 同2)∣∣∞).	(43)
Combining eqs. (42)&(43) yields that
I hτ(μT, νT) - hT (Mt+1, Vt+1) I
≤ 4η-1 [KL(μ"∣μt) + KL(VTkVt)] + 8η(M(I)∣∞ + ∣δ(2)∣∞) + ^ (陋(I)k∞ + ⑸,∞)
(i)
≤ 4η-1(1 — ητ)t ln(d1d2)
t— 1
+ X(1 - ητ)t—1—j[8η(同11∞ + 眦2)吆)+ T (同I)∣∞ + 同2)吆)]
j=0
+ 8η(kδ(I)k∞ + l∣δ(2)k∞) + 4τ (kδt"k∞ + l∣δt2 'k∞)
≤ 4η-1 (1 — ητ)t ln(d1d2)
+ X (1 — ητ )t-1-j [8η(kδj1)k∞ + Ilg2)∣∣∞) + 43 (W)I∣∞ + ∣∣δj2)∣∣∞)],
j=0
where (i) uses eq. (28). This proves eq. (29).
Next, to prove the duality gap (30), we first derive an upper bound of KL(μT ||再+1) + KL(VTkVt+1).
KL(VTIIVt+1)
=KL(VT'k%+1) — KL(Vt+1∣∣Vt+1) +〈V t+1 — V?,ln Vt+1 — ln Vt+1)
≤KL(VT^∣∣%+1) +〈Vt+1 - 吸,但丁(瓦+1 — μt) + η(δt)— δ(2)))
≤KL("T>t+1) + kVt+1 — V:k1(ηkQk∞kμt+1 — μtk1 + ηkδ(2) — δ(2)k∞)
16
Published as a conference paper at ICLR 2022
≤KL(吟 ∣νt+ι)
+ 2 [(ηkQk∞ + 4) kνt+ι - νTk2 + ηkQk∞kμt+ι - μtk2 + 仞2同，- δ(2)∣∣∞]
≤)KL(均 ∣νt+ι) + 3 KL(均 IV t+1) + 1 KL(μt+ι kμt) + 例2(阔2)信 + 局2)信)，(44)
where (i) uses the Pinker's inequality and η ≤ [2(τ + ∣∣Q∣∣∞)[ 1. Rearranging the above inequality
yields that
KL(吗∣Vt+ι) ≤4KL(吗Wt+1) + 2KL(μt+ι∣μt) + 16η2(∣∣δ(2)∣∣∞ + 礴)吆).(45)
Similarly, we can prove that
KL(μ"∣"t+ι) ≤4KL(μ"∣μt+ι) + 2KL(Vt+1 kVt) + 16η2(M(I)k∞ + |同I)Il∞).	(46)
Summing up eqs. (45)&(46) yields that
KL(μ"∣"t+1) + KL(吟国t+1)
≤ 4[KL(μTkμt+1) + KLMkVt+1)] +2[KL(μt+1kμt) + KL(Vt+1∣Vt)]
+ 16η2(M(I)k∞ + 陋(I)k∞ + ― + 符∣∞)
(i)
≤ 4[KL(μ"∣μt) + KLMkVt)] + 24η2(M(I)∣∣∞ + 同2)吆)
+ 乎(同I) k∞ + 礴)∣∣∞),	(47)
where ⑴ uses eq. (39) and η ≤ [2(τ + ∣∣Q∣∣∞)] 1 ≤ (2τ)-1.
Finally, we prove the duality gap bound (30).
hτ(μ" VT) - hτ(μ, ν?) = (μ? - μ)>Qν? + τμ> lnμ - τμT> lnμTr
=T(hμT — μ, lnμT)+ μ> lnμ — μT> lnμT) = τKL(μ∣∣μT),	(48)
where (i) uses eq. (33).
hτ(μ,Vt+1) - hτ(μ,V?)
=μ>Q(V t+1 - V?)+ τVt+1 ln Vt+1 - tv?T ln VT
=(μ - μ?)TQ(Vt+1 - V?)+ μTTQ(Vt+1 - V?) + TVt+1 ln Vt+1 - TVTT ln ν?
(i)	丁	丁
≤ kμ - μ? k1kQkmkV t+1 - V?||1 - T〈 ln νT,νt+1 - VT)+ TVt+1 ln V t+1 - τν? ln ν?
≤ 2 [τllμ - μTk2 + kQk∞kνt+1 - v?II1/t] + TKL(Vt+1∣∣ν?)
(ii)
≤ TKL(μkμT) + TKL(Vt+1kν?) + TTkQk∞KL(νT∣Vt+1),	(49)
where (i) uses eq. (34) and (ii) uses the Pinsker,s inequality. Then, eq. (49) & eq. (48) imply that
hτ(μ,Vt+1) - hτ(μT, V?) ≤ tKL(vt+1kνT) + TTkQk∞KL∕k/t+1),	(50)
where (i) uses eq. (45), (ii) uses the Pinker's inequality and η ≤ [2(τ + kQk∞)] 1.
Similarly, it can be proved that
hτ(μ")- M"t+1,ν) ≤ τKL(μt+1kμT)+ TTkQk∞KL(μTk"t+1)∙	(51)
Therefore, the duality gap (30) can be proved as follows.
建就/,(N+I)-VmnJτ(μt+1,V)
max
μ∈∆(d1),ν∈∆(d2)
[hτ(μ,νt+1) - hτ(μt+1,ν)]
17
Published as a conference paper at ICLR 2022
(i)
≤ T[KL(μt+ιkμT) + KL(Vt+ιkν?)]+ TTkQk∞[KL(μTk%ι) + KL(νTkνt+ι)]
(ii)
≤ T[KL("t+1k〃T) + KL(Vt+1kV?)] + TTkQk∞
∣4[KL(μ"∣μt) + KLMkVt)] + 24η2(kδ(I)k∞ + ∣∣δ(21∞) + ? (k/)k* + k∕k∞)]

(普 Inaχ (2 4kQk∞)
≤ max 1η, —τ—)
[κL(μ"∣μt) + KLMkVt) + 2η2(kδ(1)∣∣∞ + kδ(2)k∞) + 2η (kδt1)k∞ + kδ(2)k∞)i
+T
)
2
∞
(iv) 2
≤ η ((1 - ητ)t In(d1d2)
t-1
+ X(1 - ητ) jj [2η2(kδjI)k∞ + kδj2)k∞) + 也(kδj1)k∞ + kδj2)k∞)i)
j=0	T
+ i6η(kδ(1)∣∣∞ + kδ(2)k∞) + 12 侗1)k∞ + kδ(2) k∞)
2
≤ η (1 - ητ) In(d1d2)
+ X (1 - ητ) jj [i6η(kδjI)k∞ + kδj2)k∞) + 12 (kδ(1)k∞ + kδj2)k∞)],	(52)
where (i) adds up eqs. (50) & (51), (ii) uses eq. (47), (iii) uses eq. (39), and (iv) uses eq. (28) and
4τ-1kQk∞ ≤ 2/n (since η ≤ [2(τ + ∣∣Qk∞)]-1, τ ≤ 1). This proves eq. (30)	口
B Estimation error bounds
In this section, We derive error bounds for the estimators Vbk+1 ≈ Vk0+i, Qkm) ≈ Qkm), Qk,t ≈ Qkmt)
used in Algorithm 1. For convenience, we define the following additional notations.
bk(s) :=	X l{si = s}≈ μk(s)	(53)
Nk+1 i∈Nk+1
vk+ι(s) ：= Eμk,π(i),π(2) [[R(e,e⑴,e⑵)+ [吸⑺]i{e =s}]	(54)
vk+ι(s) ：= Eμk,π(i),π(2) [[R(e,e⑴,e⑵)+ YVk(s0)]i{e = s}]	(55)
bbk+ι(s) := vɪ X h[Ri + YVk(Si+1)] l{si = s}i ≈ vk+1(S) ≈ vk+1(S)	(56)
Nk+1 i∈Nk+1
Qk(s, a⑴,a⑵):=R(s, a(1), a(2)) + γEs,~p(∙∣s,a(i),a(2)) M(S0)]	(57)
V0+1(S) = fτ(Qk(s);πkI)(S),πk2)(S)) = vk+1(S) + TH(πk1)(S)) -TH(πk2)(S))	(58)
μk (S)
Vk+1(S) := vk+1(S) + TH(πk1)(s)) - TH(πk2)(S)) ≈ Vk0+I(S)	(59)
μk(S)
bk,t(S) = N1— X i{si = s} ≈ μk,t(S) ≈ μk,t(s),	(60)
k,t i∈Nk,t
qkmm)(s,a(m)) := Eμ, t,πz(i),π0(2) h[R(e, e⑴,e⑵)+ YV0(S0)] l{e =s,e(m) = a(m)}]	(61)
qk(,m)(s,a(m)) := E".π0W) h[R(e,e⑴,e⑵)+ YVk(s0)] i{e = s,e(m) = a(m)}] (62)
18
Published as a conference paper at ICLR 2022
bkmm)(s,a(m))：=-L χ h[Ri + YVbfc"U] l{si = s,a" = a(m)}]
,	Nk,t i∈Nk,t
≈qk0(,mt)(s,a(m))≈qk(m,t)(s,a(m))
Qkm) (s, a(m) ) := Ea(∖m) ~π0(∖m)(s) [Qk (S, a(1)，j2))]
诡 )(s，a(m))
μk,t ⑹ πk(m Iam)∖s)
Qb(km,t)(s, a(m)) :
破t(s,a((U))
μk,t(s)π0(m )(a(m)|s)
≈ Q0k(,mt)(s, a(m)) ≈ Q(km,t)(s, a(m)),
(63)
(64)
(65)
where eqs. (53)-(59) and (60)-(65) are introduced for the description and error bound proof of
the estimations Vbk+1 ≈ Vk0+1 and Qb(km,t) ≈ Q(km,t),
respectively. Specifically, eq. (53) estimates
the stationary state distribution μk associated with the policy pair ∏k1),∏k2) using state frequency
bk(s). In eqs. (54)&(55), the expectation is taken over e~ μk, e(1) ~ πk1)(s),e(2) ~ ∏k2)(s), SZ ~
P(∙∣e,e(I),e(2)). The definition of E 0	『⑴ /⑶ in eqs. (61)&(62) is similar. Equations (58)
Nk,t,πk,t ,πk,t
& (64) give two equivalent definitions of Vk0+1 and Q0k(,mt ) (S, a(m)), respectively 1 (We will prove
the equivalence in Lemma 2 below). Equation (60) estimates the stationary state distribution μ1 t
associated with the smoothed policy pair πk0(,1t),πk0(,2t),
and thus approximates the stationary state
distribution μk,t associated with the current policy pair ∏?), ∏2t.
To prove the estimation error bounds, we first prove the following two lemmas.
Lemma 2. Eq. (13) and the second “=” of eqs. (58) & (64) hold for all S, a(1), a(2).
Proof. The second “=” of eq. (64) under m = 1 can be proved as follows.
qk(1,t)(S,a(1))
=Eμ.w) [[R(e,e⑴,e(2)) + yVZ(sz )] i{e =s, e(m) = a(U)}]
=Ee~μk,t,e(1)~∏k(,t)(s),e(2)~∏k(,t)(s),s0~P (∙∣s,a(1),e(2))
[[R(s, a(1), e(2)) + YVkZ(Sz)] l{e = s, e(1) = a(1)}]
(=) Ee~μk,t,ea)~∏k*(s)[1 {e = Se(I)= a(I)H
Ee(2)~∏k(,t)(s),s0~P(.∣s,a(1),e(2)) [R(s, a(1), e⑵) + YVk'(s')]
(丝) μk,t(s)∏k(,t)(a(I)IS)Ee⑵~∏*s)[Qk")，犷)]
(=) μk,t(s)πk(,t)(a(I)IS)Qk,t(s,a(1)),
where (i) uses eq. (61) which denotes E*0 ∏0(i) ∏0(2) as expectation over e ~ μ1,t, e(1) ~ πk(,t) (Se),
e(2) ~ ∏0(t (e), SZ ~ P(∙∣e, e(1),e(2)), (ii) USeS independence between e ~ μk t, e(1) ~ π0,(t)(S)
and e(2) ~ ∏k(2)(S),Sz ~P (∙∣s, a(1), e(2)), (iii) uses eq. (57), and (iv) uses the first definition of eq.
(64). The second “=” of eq. (64) under m = 2 and eq. (13) can be proved in a similar way.
The second “=” of eq. (58) can be proved as follows.
vk+1(S)
=) Eμk ,∏(1),∏(2) h[R(e, e(1), e(2)) + Y吸(sz)] i{e = s}]
=Ee~*k ,e ⑴ ~∏k1) (s),e(2)~nk2)(s),s0~p(.|s,e(1),e(2))[[R(S,。("，。⑵)+ Y匕(SZ)[ 1{s = s}
1The definition after the second “=” of eq. (58) is the same as eq. (11)
19
Published as a conference paper at ICLR 2022
=Eefk [ɪ {S = SlEe⑴〜∏kI)(S),e⑶〜nk2)(s),s0 〜P(∙∣s,e ⑴,e⑵)[R(s)。⑴,。⑵)+ YVk(『)]
(汐 μk(S)Eea)〜婢)(s),e⑶〜∏k2)(s) [Qk(S„2))]
=μk,t(s)[fτ(Qk(s); πkI)(S), πk2'(s)) —T火(πkI)(S)) + T H(nk2)(s))]
(=)μk(S) [%I(S)-T 巩尾I)(S)) + T H(πk" (s))]
where (i) uses eq. (54), (ii) uses independence between S0 〜μk and e(I) 〜 ∏kI)(S),方(2) 〜
∏k2)(S), S0 〜P(∙∣s,3(1),3(2)), (iii) uses eq. (57), and (iv) uses eq. (58).	□
Lemma 3. If IkI∣∞ ≤ VmaX ：= 1+1-AmaX, thenforall k ≥ 0, 0 ≤ t ≤ Tk — 1, we have
..^ .. ..,.. .......................
MhJVkk∞,kV *k∞ ≤ Vmax,	(66)
∣bk+ι(s)∣ ≤ 2%axbk(s),∀S ∈ S,	(67)
max (IlQkk∞, kQk(m)k∞, kQ*k∞) ≤ Qmax,	(68)
where QmaX ：= 1 + Y/ax = 1+)；-；max With AmaX ：= max (|A⑴|, |A⑵ |).
τ-> 八 XX T ∙11 C .	II -r^7- II	，	、T	∙	，/ t ∙	1	C	II -r^7- II	，	、T	1 ι t
Proof. We will first prove	∣∣Vk ∣∣∞ ≤	VmaX	m eq.	(66) by induction.	Suppose	∣∣Vkz ∣∣∞ ≤	VmaX	holds
for a certain k0 ∈ N. Then, eq. (59) implies that for all S ∈ S,
ιV⅛o+ι(S)I ≤ iw(S) I+,忸兄I)(S))-H(πk2 )(s))]
1 μk0 (S) 1
Pi∈zk,+J[∣Ri∣ + YE (Si+ι)∣]上=s}]
≤	∑i∈Nfc,+ι 1{si = s}
(i) Σi∈Nk, + 1 [(1 + Y/ax)l{si = s”
≤	Pi∈N"+ι I® = s}
+ T In AmaX
l γ(1+ T ln AmaX) l
=1 +------：----------+ T ln Amax = Vmax,
1 - Y
+ T In Amax
(69)
where (i) uses the inequality that 0 ≤ H(π(m)) ≤ ln |A(m)| ≤ ln Amax, ∀π(m) ∈ A(|A(m)|). Since
IM k∞ ≤ %ax, ∣∣Vfe ∣∣∞ ≤ %ax for all k ∈ N. A similar induction yields that Mh ≤ %ax∙
Next, we prove ∣ V * ∣∣∞ ≤ %ax, ∣Q* ∣∣∞ ≤ Qmax in eqs. (66) & (68) respectively. Notice that V *
and Q* have the following relation.
Q*(s, α(I), α⑵)=r(s, α(I), α⑵)+ YES，〜2.厮。(1),。(2))V*(s'),
V*(s) = π*(I)(S)TQ*(s)π*⑵(s) + t[H(π*(I)(S)) — H(π*⑵(s))].
Hence,
∣Q*∣∞ ≤ 1 + YkV*∣∞	(70)
∣V*∣∞ ≤∣Q*∣∞ + TlnAmaX	(71)
Substituting eq. (70) into eq. (71) yields that
∣V *∣∞ ≤ 1+ YkV *∣∞ + T ln AmaX ⇒ ∣V *∣∞ ≤ %ax∙
The proof of eq. (66) is finished. Then, substituting ∣∣ V*∣∞ ≤ VmaX into eq. (70) proves that
∣Q*k∞ ≤ Qmax.
Equation (67) can be proved as follows.
I bk+∣^ I ≤∣‰1(s)∣ + T ∣ H(∏k2)(s)) -H(∏kI)(S)) ∣ ≤ %ax + T ln Amax 7 2%ax,
μk(S) '
20
Published as a conference paper at ICLR 2022
where (i) uses eq. (58) and (ii) uses VmaX ：= 1+1-Amax ≥ T ln Amaχ.
Next, we prove eq. (68). It can be easily seen from eqs. (57) & (66) that
IQk (S,a ⑴,。⑵]≤∣R(s,a(1),a(2))∣ + YEsO 〜P(∙∣s,α ⑴,a(2))M(SO)I ≤ 1 + YVmaX = Qmax,
which implies that kQkk∞ ≤ QmaX. Hence,
(i)
IQk,t (S, a m ) I ≤ Ea(∖m)〜∏0(∖m)(s) IQk (S, a 1，a 2 ) | ≤ QmaX
where (i) uses eq. (64). This proves eq. (68).	□
With the above two Lemmas, we can derive the estimation error bounds as follows.
Lemma 4. USe hyperparameter choices Nk,t, Nk,t ≥ 650μmxAmax ln (20T√√s^max) and Nk+ι ≥
6501mix
μmin(i-γ)2
ln
δ √μmin
in Algorithm 1. Then with probability at least 1 - δ, all the following
bounds hold for all 0 ≤ k ≤ K - 1, 0 ≤ t ≤ Tk - 1, m = 1, 2, S ∈ S, a(m) ∈ A(m), where
Tsum ：= PkK=-01 Tk.
4
—
≤ 2V	k + 171VmaX / tmix ]n ( 4K|S|
∞ ^	maxY + 1 - Y V Nk+ιμmin	(δ√μmn
(72)
Qb(km,t) -Q(km,t)∞
≤ QmaX
640t mix AmaX ln ( 20TS
um |S|AmaX + 25s
Nk,tμmin S	δ√μmin	× V
tmix AmaX
Nk,tμmin∈0
20Ts
um|S|A
maX 0
ln(	δ√μm-	)+ e
+ 2Vbk - Vk0∞
随 km) - Qkm)L
(73)
≤ QmaX
640tmiAmax ln ( 20TmSAmaX ) + 25 ʌ /5AmaX 0 ln ( 20"噌AmaX ) + N
Nk,tMmine 、 δ√Mmin / V Nk,tμmin6	、 δ√Mmin /
+ 2Vbk - Vk0∞.
(74)
More specifically, the upper bounds (73)&(74) can be simplified respectively as follows when 0
650t mix Amax
Nk,tμmin
ln (巧嘘？ )i1/3 and e =[
6501mix Amax
ln (20TSum|S|Amax∖i1/3
δ√μmin
-Qkm) L ≤18QmaX hNAmx ln (如TumSAmaX)i1/3 + 2陀k - Vk'L,
(75)
Qb(km,t)
)
-Qk∕ll∞ ≤ 18Qmax
tmix AmaX
N k,t μmin
ln (20T√4 U"3 + 2 件I VkT∞
(76)
Proof. If the initial state distribution of the minibatch Nk,t equals μk t, i.e., Sk,t,o 〜 μk t,
then	Nk,tqbk(m,t)(S,a(m))	：=	Pi∈Nk,t g(Si,	ai(1),	ai(2), Si+1)	(g(Si,	ai(1),	ai(2), Si+1)	：=	Ri	+
γVk(si+ι)] l{si = s, a(m) = a(m)}) and its expectated value Nk,tdk(m)(s, a(m)) satisfy the fol-
lowing concentration bound.
Pμk,t {INk,tqkm)(s,a(m)) - Nk,tqk(,m)(s,a(m))I ≥ "}
≤) 2exp h_______________________u2 γPs( )____________________i
8(Nk,t + 1∕γpS)QmaXμk,t(S)πk,t (a(m) |S) + 40uQmaX
≤) 2exp h_________________________u2∕(2tmQ∖_____________________i,
8(Nk,t + 2tmix)QmaXμk,t(s)πk,t (a(m) |s) + 40uQmaX
(77)
21
Published as a conference paper at ICLR 2022
where Pμ01 is under the initial state distribution μk,t, (i) uses Theorem 3.4 of (Paulin,
2015) and the inequalities that ∣g(si, a(1), a(2), Si+ι) - Esi〜μ0 tg(si, a(1), a(2), Si+ι)∣ ≤ 2(1 +
YVmax) = 2Qmaχ (Based on Lemma 3) and that Varsi〜*J[g(si,ai1),a(2),si+ι)] ≤
Esi〜μk,J[R + YVO(Si+。]21{si = s,a(m) = a(m)}i ≤ (I + YVmax)2μk,t(S)πk(,m)(a(m)|S)=
Qmaxμk t(s)∏k(m)(a(m)∣s), (ii) uses Proposition 3.4 of (Paulin, 2015) which states that the pseudo
spectral gap Yps has a lower bound 1/(2tmix) for any uniformly ergodic MarkoV chain (This condition
holds for our MDP with finitely many states and actions). Then, for any initial state distribution
st,k,o 〜ξ, Proposition 3.10 of (Paulin, 2015) implies that
Pξ{∖Nk,tbm"s, a(m)) - Nk,tqk(m)(s, a(m))∣ ≥ u}
≤ 3s~ξ h μ(SS ]P"k" {∣Nk,tbm)(s,a(m))- Nk,tqk(m)(s
a(m))∣ ≥ u}
≤) 二exp h-正F∕W(m)EUQmZ
(78)
where (i) uses eq. (77) and the inequality that Es〜ξ [*ξ(S()s)] ≤ μ1-.
In a similar way, the following concentration bound can be proved for Nktbkt(S)
Pi∈Nk,t Wi = s} and Nk"(s> E*^ ^,向,代)].	''
Pξ{\NkMt⑸-Nk,tμk,t(s)∣ ≥ u} ≤ ∕μmnexp h - 8(Nk,t +utm(4μζ)(s) + 40ui, (79)
where we use the inequalities that var*01 l{si = s} ≤ μk t(s) and that ∣ l{si = s} - E*0, l{si =
s}∣ ≤ 1. Letting the right hand sides of eqs. (78)&(79) be upper bounded by δ∕4 and applying the
union bound yields that, with probability at least 1-δ∕2,the following two inequalities simultaneously
hold.
(80)
(81)
where (i) holds since Nk,t ≥ 650蹩£^* ln (20⅞√⅛ax) ≥ 650tmix and μk,t(s) ≥ μmi∏.
Similarly, it can be proved that the following two inequalities holds with probability at least 1 - δ∕2.
∣vbk+1(s) - vk0 +1(s)∣
160Qmax
≤ FT-------tmix ln
Nk+1
∣bk(s) - μk(S)I
160
≤ 又7	tmix ln
Nk+1
+ 6QmaX
Nk+1
jtmix(1 + 2tmix/Nk+1)μk(S)In (]
(82)
6
+ ʒ==
Nk+1
t Itmix(1 + 2tmix/Nk + 1 )μk(S)In (τ^^/	).
δ√μmin×
(83)
22
Published as a conference paper at ICLR 2022
Hence, eqs. (80)-(83) hold with probability at least 1 - δ. In this case, We have that
∣bk+ι(s) - vk+ι(s)∣
≤ Ibk+1(S) - vk+ι(s)∣ + M+ι(s) - vk+1(s)∣
≤ 160QmaX
≤ Nk+ι
tmix In
+
6QmaX
Nk + 1
jtmix(1 + 2tmix∕Nk+ι)μk (S)In
+ Y% ,∏k1),∏k2) [[V (s')-匕(s')] l{e =Su
≤
160QmaX
Nk+ι
tmix In
+
6QmaX
jtmix(1 + 2tmix/Nk+1 )μk (S)In
Nk+1
+ Y max∣V (SO)-吸(SZ)IEμk l{e = s)
≤
160QmaX
Nk+1
tmix In
+
6QmaX
jtmix(1 + 2tmix/Nk+1 )μk (S)In
Nk+1
+ γμk (S)IlVk- Vkz∣∣∞,
where (i) uses eqs. (54), (55) & (82).
.^ .. , ...
∣V4+1(S)- Vk+1(S)I
=)∣ bk+1(S) _ Vk+1(S) ∣
∣ bk (s)	μk (s) I
(84)
≤“-1(S) I bk+1(S)-Vk+1(S)I+1 bk+1(S) I I “黑嵩；
(≤) μj(S) [ a6gmx tmix In( δ√4mn) + ppQ= {-mix(I+"mix/Nk+1)^(S)In( δ√μ^
+ γμk (S)IIVk -匕 ll∞i + 2Vmaxμk 1(s) N	tmix In 1
+ /at	t I tmix (1 + 2tmix∕ Nk+1) μk (S)In ( 7^^/	)
√Nk + 1	δ√Mmi∏∕
(iii)
≤
Yllvk - Vkll∞ +
VmaX
μ k (s)
480tmix
Nk+1
+ 18/F(1 + „+I)In J
(≤) YkVk- Vk'h + VmaX "24 N40tmi^ in (」)+ 18,小江三匚 一
Nk+1μmin	'δ∙∖∕μmin/	V Nk+ 1μmin	'δ∙∖∕μmin,
≤ Y kVk - Vkk∞ + VmaX "24[-K√= + 181千」二匚 一
V Nk+1μmin	'δ∙∖∕μmin/	V Nk+1μmin	'δ∙∖∕μmin,
≤ YkVk - Vo∞ + 171%aX∖/ Jmix	in (-L=),	(85)
Nk+1μmin	' δ"μmin
where (i) uses eqs. (58)&(59), (ii) uses eqs. (67), (83)&(84), (iii) uses QmaX ≤ VmaX, (iv) uses
μk(S) ≥ μmin and Nk + 1 ≥ 40mmx in (δ√⅛^) ≥ 40tmix, and (V) uses Nk + 1 ≥ 4BT in (δ√⅛).
Applying the union bound to the above inequality over all 0 ≤ k ≤ K - 1, S ∈ S and taking
maximum over S ∈ S, we obtain that with probability at least 1 - δ,
IlVk+1 - VZ+1∣∣∞ ≤yIIVk - VZl∣∞ + 171VmaXl / N mix in (ʌ r-).
Nk+1 Mmin	×δ√Mmin×
23
Published as a conference paper at ICLR 2022
Iterating the above inequality yields that with probability at least 1 - δ,
kVbk - Vk0k∞ ≤γkkVb0 - V00k∞ +
171 VmaX	tmix 1口
1 - Y N Nk + iμmin
(i)	k
≤ 2VmaXγ +
171VmaX
1 - Y
tmix
V Nk + 1 μmin
ln
(86)
1	∕∙∖	i` ,	, IIt r / II II -r^z- 11	, T，
where (i) uses the fact that kV00k∞, kV0k∞ ≤ VmaX.
Hence, with probability at least 1 - 2δ, eqs. (80), (81) & (86) hold simultaneously. In this case,
Qb(km,t)(s,a(m))-Q0k(,mt)(s,a(m)) (=i)
b(m )(s,a(m))	qkm )(s,a(m))
-
μk,t(s)πkm)(Mm)IS)	μk,t(S)πk(m)(Mm)IS)
(≤ii)	qbk(m,t) (
S, a(m)) - q(m)(S, a(m)
b
+ ∣μk,t(S)∏k(m )(a(m)∣S)Qk(,m )(S,a(m) )|
Ibk,t(S) - μk,t(S)I
(≤) 2|bm)(S,a(m)) - qk(,m)(S,a(m))| +2∣qk(m)(s, a(m)) - ⅛m)(S, a(m)
一	μk,t(S)πk(,m )(a(m)|S)
2Qmaxlμk,t(S)- μk,t(S)I
+
(2	2Qmaχ	320 t ln /	4
一μk,t(S)∏k(,m)(a(m)∣S) [Nm mix n δ√μm=
+ /N	JtmiX(I + 2tmix∕Nk,t)μk t⑸开；：\a(m)|S) ln
Nk,t
+ 2Vbk - Vk0∞
(v)
≤Q
maX
640tmixAmax ln / J
Nk,tμmin^	δ^μ μmiη
+ 25	tmixAmaX ln (	4
N Nk,tμmiηe0	δ√μmin
+ 2Vbk - Vk0∞,
(87)
where (i) uses eqs. (64)&(65), (ii) uses eq. (64), (iii) uses eq. (68) and the inequality that bk,t(S) ≥
μk ∕s)∕2 implied by (i) of eq. (81), (iv) uses eqs. (80)&(81) and the following inequality based
on eqs. (61)&(62), and (v) uses μk,t(s) ≥ μmin (based on Assumption 1), ∏k(m)(a(m)∣S) = (1 -
e0)πk(m,t)(a(m)IS) + e0∕IA(m)I ≥ e0∕AmaX and Nk,t ≥ 650tmix.
IIqk0(,mt )(S,a(m)) -qk(m,t)(S,a(m))II
≤ YEμ0 t,n0(1),n0(2) h∣Vk (s0) - V0(S0)∣i{e = S, e(m) = a(m)}]
≤ μk,t(S)∏k(m)(S,a(m))僧-找 L∙
Notice that the following inequality always holds.
IIQ0k(,mt)(S,a(m))-Q(km,t)(S,a(m))II
≤ X IQkGa⑴,a⑵)||nk(\m)(a(\m)|S)-nk\m)(a(\m)|S)|
a(\m)
≤) Qmaxe0|nk\m)(a(\m)|S) -|A(m)|T| ≤ QmaXe0,	(88)
where (i) uses eq. (68).
24
Published as a conference paper at ICLR 2022
Hence, combining eqs. (87)&(88), it can be seen that with probability at least 1 - 2δ, the following
inequality holds
Qb(km,t)(s, a(m)) - Q(km,t)(s, a(m))
Q	640tmiχAmax
QmaX -√7	7~
Nk,tμmin e0
+ 2Vbk - Vk0∞.
ln
tmixAmaX
+ 25∖ k---------0ln τ
N Nk,tμmine0	∖δ
+ e0
(89)
Similarly, it can be proved that with probability at least 1 - 2δ,
Ib km)(s,a(m)) - Qkm)(S,a(m) )∣
≤Q
maX
640tmixA
maX	4
-=--------ln
N k,tμmin e	×δ√μmin
+ 25
+ e0
+ 2Vbk - Vk0∞.
(90)
Finally, eqs. (72)-(74) are proved by applying a union bound to eq. (86) and to eqs. (89)&(90) over
all0 ≤ k ≤ T - 1, 0 ≤ t ≤ Tk - 1, m = 1,2, s ∈ S, a(m) ∈ A(m).
1/3
Now We consider the hyperparameter choice e0 = [6NmxAmax ln (一晨)Lmax)] . First, this is
a valid choice, since Nk,t ≥ 650tmixAmax ln (20Tu√μS^maχ) implies that e0 ∈ [0,1]. Then, for this e0,
the upper bound (73) is simplified as follows.
llQb(km,t)-Q(km,t)ll∞
≤ QmaX
640tmiχA
maX ln (20£UmISIAmaX ) + 25s
Nk,tμmine0	δ√ μmin	× V
tmixAmaX
Nk,tμmine0
20TsumISIA
maX 0
ln(	δ√μ-	)+ e
+ 2Vbk - Vk0 ∞
≤Qmax(640e02 + √65θI + 2 憎 - VM∞
啜 2Qmaχe0 + 2∣∣ Vk- VklL = 18QmaXh NAmX	( Tu)] “ + 2恒-^^l∞,
k,tμmin	'	OVMmin
where (i) uses NxAmax ln (20T√∣SAmax) = e03∕650, and (ii) uses e0 ∈ [0,1]. This proves eq. (75).
The proof of eq. (76) is similar.	□
C Properties of the Duality Gap
In this section, we prove some useful properties of the duality gap
D(τ)(π(1), π(2)) := s,π0m(1)a,xπ0(2) Vπ(0τ()1),π(2)(s) -Vπ((τ1)),π0(2)(s).
Lemma 5. For any policy pair π(1), π(2), it holds that
D(T)(π⑴,π⑵)≤	max 、[fτ [Q『)(s), π0⑴(s), π⑵(S)] - fτ [Q『)(s), π⑴(s), π0⑵(s)]].
1 - γ s,π0(1) (s),π0(2) (s)
This Lemma generalized the Lemma 32 of (Wei et al., 2021) to entropy-regularized Markov game.
Proof. Throughout this proof, we denote the policy pair (∏(T), ∏(T)) as the Nash equilibrium and
their associated V-function and Q-function are respectively denoted as V(T) and QIτ).
25
Published as a conference paper at ICLR 2022
Note that
V∏(Tl),∏ ⑶(S)- V(T )(s)
=X	[QΠτ)i) ,π(2) (s,a ⑴,a ⑵"⑴(a ⑴ IS)n(2)(a ⑵⑸
a(1),a(2)
-Q(τ)(s, a(1),a⑵)π(T)(a⑴∣s)∏(T)(a⑵|s)]
+ T [H(π0⑴(S)) - H(π⑵(S)) - H(∏(T)(S)) + H(∏(T)(s))]
=X	[Q∏τ)i),π(2) (s, a(1), a(2)) - Q「)(s, a(1), a(2))]n0⑴(a⑴∣s)∏⑵(。(2)国
a(1),a(2)
+ X QiT)(s, a(1), a⑵)[π0⑴(°(D∣s)π⑵(a⑵|s) - π(T)(a(1)∣S)π(2-)(a(2)∣S)]
a(1),a(2)
+ τ[H(π0⑴(S)) - H(π⑵(S)) - H(∏(T)(S)) + H(∏(T)(S))]
=Y X	h∏0(1)(a⑴ ∣S)∏(2)(a⑵ ∣S)Es0~p(∙∣s,a(ι),a(2))[*Tl),∏(2) (s0) - V(T)(s0)]]
a(1),a(2)
+ π0(1) (S)>Q(iT) (S)π(2) (S) - πi(1T) (S)>Q(iT) (S)πi(2T) (S)
+ τ [H(∏0⑴(S)) - H(∏⑵(S)) - H(∏(T)(S)) + H(∏(T)(S))]
≤ γmsa0x [Vπ(0T()1),π(2)(S0) - Vi(T)(S0)]
+ TfT (Qr)(S); ∏0(I)(S) ,∏⑵(S)) -TfT (Qr)(S); ∏(T) (s),∏(T)(S))
(i)
≤ γmsa0x [Vπ0(1),π(2)(S0) - Vi (S0)]
+ TfT(QF)(s); π0(I)(S),π⑵(S)) - T min ∕t(qF)(s); π(I)(s), π0⑵(s)),
π0(2)
where(i)uses % (Qr)(S); ∏(T)(S),∏(T)(S)) = maxπ00⑴ minπ,⑶ f「(Qr)(S); π00(I)(S),no(2)(s)) ≥
min∏o(2) O(Qr)(S); n(1)(s),n0(2)(s)).
Applying maxs,π0(1) to both sides of the above inequality and rearranging it yields that
max [Vπ(0T()1) π(2)(S) -Vi(T)(S)]
s,π0(1)	,
≤ / max	hfr [qF )(s),π0 (I)(S),∏(2)(s)] - % [qF )(s),π(I)(S),/⑵⑶]].(91)
1 - γ s,π0(1)(s),π0(2)(s)
Similarly, we can obtain that
sm,πa0(x2) [Vi(T)(S) -Vπ((T1)),π0(2)(S)]
≤	max	hfr [q[t )(s),π0 ⑴(s),π ⑵(S)] - % [Q『)(s),π(I)(S),π0 ⑵(s)]]. (92)
1 - γ s,π0(1)(s),π0(2) (s)
Therefore,
D(T)(π(1), π(2))
= s,π0m(1)a,xπ0(2) [Vπ(0T()1),π(2)(S) - Vπ((T1)),π0(2)(S)]
≤ sm,πa0(x1) [Vπ(0T()1),π(2)(S) -Vi(T)(S)] + sm,πa0(x2) [Vi(T)(S) -Vπ((T1)),π0(2)(S)]
(i)
≤ ：I----- max	fτ [qF)(s),π0(I)(S),∏(2)(s)] - fτ [qF)(s), π(I)(S), π0⑵(s)]
1 - γ s,π0(1)(s),π0(2)(s)
where ⑴ uses eqs.(91)&(92).	口
Lemma 6. D(T) (π(I),π⑵)is 'l,Amax -Lipschitz continuous with regard to t.
26
Published as a conference paper at ICLR 2022
Proof. The definition of the state value function Vπ((τ1)),π(2) in (3) can be rewritten as follows.
Vπ((τ1)),π(2)(s)=EhX∞ γtRts0 = si +X∞ γtτ [H(π(1)(st)) - H(π(2)(st))].	(93)
t=0	t=0
Hence, for any τ, τ0 ≥ 0,
Vπ((τ1)),π(2) (s) - Vπ((τ1)),π(2) (s)
∞
≤ X γt∣τ0 - T∣H(∏⑴(St))-H(n⑵(st))∣
t=0
(i) 7∞ t∣ 0	ln AmaX I 0 I
≤ γ ∣τ - T| ln AmaX ≤ —--------∣T - T|,
1-γ
t=0	γ
(94)
where (i) uses 0 ≤ H(π(m) (st)) ≤ ln |A(m) | ≤ lnAmaX. Hence, this Lemma can be proved as
follows.
D(TO)(∏⑴，∏⑵)-D(T)(∏(1),∏(2))∣
≤	s,π0m(1)a,xπ0(2) Vπ(0τ(1)),π(2)(s) - Vπ((τ1)),π0(2)(s) - s,π0m(1)a,xπ0(2) Vπ(0τ()1),π(2)(s) - Vπ((τ1)),π0(2)(s)
≤ s,π0m(1)a,xπ0(2) Vπ(0T(1)),π(2) (s) - Vπ((T1)),π0(2) (s) - Vπ(0T()1),π(2) (s) - Vπ((T1)),π0(2) (s)
≤ s,π0m(1)a,xπ0(2) hVπ(0T(1)),π(2) (s) - Vπ(0T()1),π(2) (s) + Vπ((T1)),π0(2) (s) - Vπ((T1)),π0(2) (s)i
(i)	2 ln AmaX 0
≤	------|T
1-γ
- T|,
where (i) uses eq. (94).
□
D Proof of Theorem 1
Theorem 1 (Finite-time convergence rate). Apply Algorithm 1 to solve the entropy-regularized
Markov game with T ∈ (0, 1]. Choose learning rate η = [2(T + QmaX)]-1, initialization kVb0k∞ ≤
Vmax and batch sizes Nk,t, Nk,t, Nk+ι that satisfy (18) & (19). Then, the NaSh equilibrium duality
gap converges at the following rate with probability at least 1 - δ.
D(T )(πK-ι,πK-ι)
≤ O ( Vmaxln Amax X YK-k (1 - ητ)Tk-1
1 - γ	k=0
+
Vmax
1 - Y
max
μmin
ln
Tsum |S |Amax	2/3	K-k-1
J	k=0Y
+
VmaX '
TN NklI
k,t+1
[
+ 嗯axYK +	tmix嗯ax	l JKM) X YKiT A
+ T2(1- Y)2 + T2μmin(1 - Y)3	Uμmin> 匕 Nk+1	.
(20)
Proof. First, consider the SPU iterations that are defined by replacing Qkm), Qkm+1 in (8) With Qkm)，
二(m)
Qk,t+1, respectively, for m = 1, 2. We can apply Lemma 1 With the quantities being specified as
folloWs.
•	Q := Qk(s), hτ(μ, ν) := fτ(Qk(s)； μ, ν),
•	U .一 Tr(I)	(2)	(I) AcA yr .一 行(2)
μt := πk,t (S), Vt := πk,t (S), μt+1 := πk,t+1(s), Vt+1 := πk,t+1(s),
μT :=域(I)(S), νT := πk⑵(S),
27
Published as a conference paper at ICLR 2022
•	dm := |A(m)|,
•	δkm) ：= Qkm)(S)-Qkm)a, δmm ：=Q黑+1(S)- Qkti(s)，
where ∏k1)*(s),∏k2)*(S) is the solution policy pair to the minimax optimization problem
min∏⑶(S) max∏⑴(S) fτ (Qk(s)； ∏(1)(s),∏(2)(s)).
Consequently, eqs. (29) & (30) with t = Tk - 1 imply the following two inequalities, respectively.
IfT (Qk(S)； πk,Tk (S),πk1Tk (S))- fτ (Qk ⑸;城⑴(S),城⑵(S))I
=lV0+1(S)- fτ (Qk (S)；域(I)(S),城⑵(S))I
8	Tk-1
≤ —(1 - ητ)Tk-1 ln AmaX + X (1 - ητ)τk-2-t
η	t=0
2
X h8η∣lQkm)(S)-Qkm)(S)ll∞ + 4T∣IQk,t+ι(S)-Qk；+I(S)U∞],	(95)
m=1
.(1) maχ2)(s)[fτ(Qk(s); π(1) (S),πk2Tk (S)) - fτ(Qk (s)； πk1Tk (S),n(2) (S))]
max
π(1) (S),π(2) (S)
[fτ(Qk (s)； π ⑴(S),πk2)(S)) - fτ(Qk (s)； πk1)(S),π ⑵(S))]
4	k-1
≤ -(1 - ητ)Tk-1 ln AmaX + X (1 - ητ)τk-2-t
η	t=0
2
X [16η∣∣Qkm)(S)-Qkm)(S)∣∣∞ + 12∣∣bk,t+ι(s) -Qkm+ι(s)∣∣∞].
m=1
(96)
Then, define the soft Bellman operator Bτ as follows.
Bτ(Q)(S,a(1),a(2))
:=R(s, a⑴,a⑵)+ γEs0〜P(∙∣s,a(i),a⑵)
max min
π(1) (S0) π(2) (S0)
fτ(Q(s0); π(I)(S0),π⑵(s0))
(97)
where Q : SX A⑴ X A⑵ → R and Q(s0) ∈ RlA(1)l×lA(2)1 with each entry given by
[Q(S0)]a(1),a(2) := Q(S, a(1), a(2)). It can be proved that Bτ is a contraction operator and has a
unique fixed point QT) (Cen et al., 2021), that is,
kBτ (Q0) - Bτ (Q)k∞ ≤γkQ0 - Qk∞,	(98)
BT(QT ))=Q]τ).	(99)
As a result, we obtain that
kQk+1 - QT) k∞
(i)	( )
≤ kQk+1 - BT(Qk)k∞ + kBτ(Qk) - BT(QT))k∞
(ii)
≤ max ∣Qk+ι(s,a(I),a(2)) -Bτ(Qk)(s,a(I),a⑵)| + YIlQk - QT)I∣∞
S,a(1) ,a(2)
=max-R(s, a(1), a(2)) + YEsO 〜P(∙∣s,a(ι),a⑶)V0+l(s0)
S,a(1) ,a(2)
-[r(S, a，, a，) + γEs0〜p(∙∣ s,a(1) ,a⑵)fτ(Qk(s0);域(I)(S0),πk⑵(s0))] I + γkQk - Qτ)k∞
≤ Ymaχ∣v0+ι(s0) -fτ(Qk(s0);域(I)(S0),πk⑵(S0))1 + YkQk- QT)k∞,	(IOO)
S0∈S
28
Published as a conference paper at ICLR 2022
where (i) uses eq. (99) and (ii) uses eq. (98). Throughout the proof, suppose that eqs. (72), (75) &
(76) hold simultaneously which occurs with probability at least 1 - δ . In this case, we have
Qb(km,t)(s) - Q(km,t)(s)2∞
(i)
≤ 648Q2max
tmixAmax
Nk,tμmin
ln
207‰∣S∣Amaχ)i 2/3
δ√ μmin
+ 8Vbk - Vk0 2∞
(ii)	2	tmixAmax	20Tsum|S|Amax 2/3
≤ 648Qmax Nk^-l lM	δ√μm-	J]
+ 32Vm2axγ2k +
29241tmiχVmaχ
Nk+1 μmin(1 - Y)2
(101)
where (i) uses eq. (75) and the inequality that (a + b)2 ≤ 2a2 + 2b2 for any a, b ≥ 0, (ii) uses eq.
(72) and also (a + b)2 ≤ 2a2 + 2b2 for any a, b ≥ 0. Similarly, we obtain that
IIb km+ι(s) - Qkm+wR
≤ 648Q
max
h tmixAmax 1口
LN k,t+1 μmin
20Tsum∣S∣A
max i
δ√ μmin
+ 32Vm2axγ2k +
29241tmiχ Vmax
Nk+iμmin(1 — Y )2
2/3
(102)
Then, iterating eq. (100) yieldi that
kQK-1 - Q*τ) k∞
K-2
≤ Y K-1kQ0 -Qτ)k∞ + X YKi-I mmIxlVk+1(s0)
k=0	s ∈
(i)	K-2
K-1	K-k-1
≤ 2γ	Qmax + γ
k=0
2
Xh8ηIIQb(km,t) (s) - Q(km,t) (s)II
m=1
(ii)	K-2
≤ 2YK-1Qmax+ X YK-k-1
k=0
X2 648Q2max h
m=1
8	Tk -1
-(1 - ητ )Tk-11n AmaX + X (1 - ητ )Tk-2-t
η	t=0
2	33 Ilb(m) , S κ(m) ,、||2 ]
∞ + 4T llQk,t+1(S)- Qk,t+1(S) ll∞
Tk -1
—(1 - ητ)Tk-1 ln AmaX + X (1 - ητ)Tk-2-t
η	t=0
tmixAmax 1口 / 20TiUmISIAmax
μmin
δ√ μmin
33
+ 4τN"
k,t+1
2
—
r( N
+
+ 33 )[32V2axY2k + 29241tmiF ln
4τ	Nk+ιμmin(1 - y)2
(iii)	8	K-2
≤ 2γK-1Qmax + - InAmaX X Y KiT (一7 )Tk -1
η	k=0
+ 324Q2max
tmixAmax l (20TiUmISIAmax
μmin
δ√ μmin
i2/3
K-2	Tk -1
X YK-kτ X (1 - ητ)Tk-2-t(Nn
k=0	t=0	Nk,t
33
+ . 2/3
τ N k,t+1
1 1568YKTVmaX
ητ2 (1 — Y)
(iv)
ι 1432809tmiχVgax
ητ2μmin(1 - y)2
K-2
X γK-k-1Nk-+11
(103)
k=0
K-2
O Vmax 1n Amax X YK-k(1 -ητ)Tk-1+Vmax
k=0
tmixAmax
ln
μmin
29
Published as a conference paper at ICLR 2022
K-2
E YK-I
k=0
Tk-I
E(1 - ητ)
t=0
+ VmaXYK
T2(1 - Y)
+ T2μmin(1 - Y)
VmaX
τN k73+J
,t^+1
K-2
E YK-k-1Nk+1
k=0
(104)
where (i) uses eqs. (68)&(95), (ii) uses eqs. (101)&(102), (iii) uses η = [2(τ + Qmax)]-1 ≤ 1∕(2τ),
P= -1(1 - ητ)Tk-2-t
1
≤ ητ (1-ητ)
≤ ηT and EK-2 Yκ+k-1 ≤ γK-Y-, and (iv) uses QmaX
O(‰ax) for Y ≈ 1 and η = [2(τ + Qmax)]-1 = O(QmaX)=O(VmaX) (Since QmaX ≥ YT
O(T)).
Using Lemma 5, the convergence rate of the duality gap in eq. (20) can be proved as follows.
D(T )(πK-1,πK-1)
(i)	2
≤	max
1 — Y s,π(-) ,π(2)
2
---- max
—Y s,n(-) ,π(2)
2
[fτ [Q*τ)(s),π(I)(S),∏K-1(s)] - fτ [Q*τ)(s),∏K-1(s),∏⑵(s)]]
IfT(QK-1(s); π(I)(S),∏K-1(s)) - fτ(Qk-1 (s); ∏K-1(s), ∏(2)(s))]
1-Y
2
1 - Y
max
s,π(I) ,π(2)
fτ(QP)(s)； π(I)(S),∏K-1(s)) - fτ (Qκ-1(s); π(I)(S),∏K-1(s))∣
max, JfT(QK-1 (s); ∏K-1(s),∏⑵(S)) - fτ (Q*τ )(s); ∏K-1(s),∏⑵(S))I
s,π(-) ,π(2)
≤
1
+
+
一η(1 - Y)
2
(1 - ητ)TK-I 1 ln AmaX + τ-
1-Y
Tk---1
^X (1 - ητ)Tκτ-2-t
t=0
≤
8
2
E [16η∣∣Q%ι,t(s)-Q%ι,t(s)∣∣∞ + 12 I I Q
m=1
K-1,t+1(S)- QK-1,t+1(S)I∣∞i
2
1 - Y
2
1 - Y
max I ∏(I)(S)>[Q(T)(s) - QK-I(S)]∏K-1(s) ∣
s,π(1)
max I ∏K-1(s)>[Qk-1(s) - Q(T)(s)]∏(2)(s) I
s,π(2)
max
η(1 - Y)
ZI Tk---1
(1 - ητ)TK-IT +	E (1 - ητ)TK---2-t
1 - Y t=0
2	FtmixAmax 1 ∕20‰m∣S∣AmaX	273
648QmaXlʒm- lM	W )
+ (32η + 24)[32%axY2(KT) + ；9241喳% ln
τ	NKμmin (1 - Y)2
ι 12
一 + ^=273
,t TN K-1,t+1
(iv) 8ln A1
max
η(1 - Y)
(1 - ητ)Tk---1 + 2592QmaX htmixAmaX 1口 (20TsUmISIAmaX)1 2/3
1 - Y I μmin ,	δ√μmin
Tk---1
E (1 - ητ产---2-t
t=0
ι 12
一 + ^≡=273
,t	TN K-1,t+1
160「32%ax
maX
ητ2	1— Y
+ 8YKT QmaX
1 - Y
Y2(k-1) +
29241tmix 唆ax
NKμmin(1 - Y)3
+ 32ln AmaX
η(I - Y)
K-2
E YK-k-1(1 - ητ产T
k=0
+
+
(iii) 8ln A
≤
+士kQK-I-敏)k∞
≤
T
ln (T)]
' δMμmin)J
30
Published as a conference paper at ICLR 2022
1296QmaX
tmix AmaX
----------ln
μmin
207‰∣S∣AmaX Y 2/3
δ√ μmin
K-2	Tk-1
X Yκ-k-1 X (1 - ητ产-2-t
k=0	t=0
k=0
+ 用 2/3
TN k,t+ι
33
6272y KTVmaX
ηT 2(1 — y)2
ln
ητ2μmin(1 - γ)3
5731236-xVmaX
K-2
X Yκ-k-1N-1
≤ 8YKTQmaX
32ln AmaX
η(i - Y)
K-1
X Yκ-k-1(1 - ητ产T
k=0
1296QmaX
I- Y
tmix AmaX
ln
μmin
207‰∣S∣AmaX Y 2/3
δ√μmin
K-1	Tk-1
X Yκ-k-1 X (1 - ηT 产-2-t(等
k=0	t=0	Nk,t
33
+ 用 2/3
TN k,t+1
11392yKTVmaX
ηT 2(1 — y)2
(=)O
VmaX ln AmaX
I- γ
ln
ητ2μmin(1 - γ)3
5731236tmixVmaX
κ-1
X Yκ-k-1
NTI
k=0
κ-1
X Yκ-k (1 - ηT 产 T +
k=0
I- γ
tmix AmaX
-----------ln
μmin
K-1	Tk-1
X Yκ-k-1 X (1 - ητ/-2-t
k=0	t=0
+
VmaX
"ʌr2/3
TN k,t+1
+ V3aXYκ
T 2(1 — Y )2
T2μmin(1 - Y)3
ln
K-1
X Yκ-k-1Nm ,
k=0
where (i) uses Lemma 5, (ii) uses eq. (96) and the definition of the function fτ, (iii) uses eqs.
(101)&(102), (iv) uses eq. (103) and η = [2(τ + QmaX)]-1 ≤ 1/(2T), PT= -1(1 - ητ产-2-t ≤
ητ(1-ητ) ≤ ηT, and (V) uses Q
max
O(VmaX) for Y ≈ 1 and η = [2(τ + Qmax)]-1
O(QmaX)
O(VmaX) (Since QmaX ≥ YT = O(T)).
+
+
+
+
+
1
I- Y
I- Y
[
+
[
[
+
+
□
E Proof of Theorem 2
Theorem 2 (Sample complexity). Implement Algorithm 1 with η = O(1 — Y), T = O( Y)),
K = O [1-γ ln (' Amax) ] and Tk = 1+ 由：1® ；t)t ∙ Choose thefollowing adaptive batch sizes.
Nk+1
e (tmix(ln2 AmaX)Y-2
O( E2μmin(1- Y)8
Nk,t = Nk,te2 (1 - γ)3 = O
-3(t + 1)
maX(1 - ηT)	5
μmin(1 - Y)3
Then, for any e ≤ in14max, the overall sample complexity to achieve D(O) (∏K-1, ∏K-1) ≤ e is
O(μ , ；mxAmaY)i3.5). Please refer to (118) inAppendixE for a complete expression.
Proof. Since T = O(不；Y)), we have
VmaX =1 + Tln AmaX = ɪ+ O(e) = O((1- γ)-1).	(105)
1-Y 1-Y
QmaX =1 + YT-n；maX =± + O(e) = O((1 -Y)T).	(106)
31
Published as a conference paper at ICLR 2022
Since Tk = 1 + 瑞-；T)-i, We have
K-1
Tsum = X (1 +
k=0
k ln YT ∖
ln(1 — ητ)-1√
=K + L。( M)
…K(K — 1)/VlnAmax )
=k +—2— O ⅛f )
=O
(K2 ln Amax )
I E(1- Y) J
∣^ ln Amax
[e(1 - γ)3
ln2
(ln Amax )]
e(1 - Y))∖
(107)
Hence,
ln (THSAmx) = O[ln (SAZeX1*3x) +ln (2ln ())]
=。[ln (eδ≡⅛⅛)]-	(108)
Similarly,
ln (K≡) =Ο[- ((I)) - ())]
=O[ln (S；n(e-[nAmaX))] + O[ln ((°	?)ln (J))]
δμmin(1 - Y)	δμmin(1 - Y)	1 - Y
=o[ln (S.1:AmaX))] + o[ln (δ	lS1))]
δμmin(1 Y)	δΜmin(1	7),」
=。[ln (|S|；n(e-：InAmaX))]	(109)
δMmin(1 - Y)
where (i) uses e ≤ InlAmaX. With the above equalities, we will prove below that the hyperparameter
choices in Theorem 2 are valid and satisfy the conditions of Theorem 1 with proper constants hidden
in Ο(∙).
η =[2(τ + Qmax)]-1 =。(1 - Y)
T =。( e⅛j2
ln Amax
K =in⅛ln
)∈ (0,1]
ln5/2 Amax
e5(1 — Y )7∙5
=。[iɪ ln (⅛⅛)] ≥ 1
T -1 +	k ln YT	≥ I
k = + ln(1 - ητ)-1 ≥
tmix(ln2 AmaX)Y-k/2	∣ S ∣ ln(e-1 ln Amax)、]
Nk + 1 =O	e2μmin(1-Y)8	(	Mmin(I-Y)	/
≥ 650mx ln
μmin
θ[三 ln
l-^min
(110)
(111)
(112)
(113)
(114)
Nk,t =O
max(1 - ητ)-3(t+1”5 ln (	∣S∣Amax	)]
Mmin(1 - Y)3	n(eδ4min(1- Y)人
≥ 650-xAmax ln
μmin
20TSUmISIAmax
δ√ Mmin
tmix AmaX
ln
μmin
ISIAmaX
eδμmin (1 - Y)
(115)
。
tmixAmax(ln3/2 AmaX)(I - ητ)-3(t+1”5 ] (	∣S∣Amax
Mmine3/2(1 - Y)6	n 1eδμmin(1 - Y)
32
Published as a conference paper at ICLR 2022
≥ 650tmixAmax
μmin
ln ( 20TSUmISIAmax
'	δ√ μmin
tmix AmaX
μmin
ln
∣S∣Amax
eδμmin (1 - Y)
(116)
With these hyperparameters and eqs. (108)&(109), the duality gap bound (20) can be simplified as
follows,
D(T [Si,
Vmax ln AmaX X YKi (1 - ητ产-l +
1 — Y	乙
/	k=0
Vmax
1 - Y
μmin
t mix AmaX
--------ln
K-1	Tk-I
X YK-I X (1 - ητ)Tk-2-t(ɪ +
k = 0	t = 0	Nk,t
Vmax
而2/3
TN k,t+ι
+ VmaXYK
τ 2(1 — Y )2
(i)
t mix Vmax	h
T2μmin(1 - Y)3
(KM)KX Yκ-k-1
δμmin £o Nk+i
≤O
ln AmaX
(T-^)2
K-i
X YK-kYk
k=0
tmixAmax 1
--------ln
μmin
(	ISIAmaX	M 2/3 KX YK-k-l
le6μmin (I - Y)	2
Tk-l
X (1 - ητ)Tk-2-t
t=0
∏巴
≤ O
+
+
1
(T-^)2
[
=O
[
[
oh μmm^⅛ln( e⅛⅛ )L3(I-ητ)2(t+l)/5
+o（、刈
tmixAmaX(In / AmaX)
〃mine3/2(1-Y)6
ln( eδμ≡¾ )]-2%-ητ)2(t+l)/5
YK (ln2 AmaX)I tmix(ln2 AmaX) I ( ∣S∣ ln(e-1 ln AmaX))
+ e2(1- Y )7 + e2μmin(1- Y )8 nV δμmin (1 - Y) J
K-i
X YK-k-lO
k=0
Γtmix(ln2 AmaX)Y-k/2
I	e24min(1- Y)8
ln
∣S∣ ln(e-1 ln AmaX) MT
δμmin(1 - Y)
≤O
K-i	Tk -i
^aIKyk + X YK-k-1 X (1 - ητ)Tk-l-3(t+l)/5
(1 - Y)	k=0	t=0
YK (ln2 Amax)
+ e2(1-γ )7
K-i
+ X YK-k/2-i
k=0
(ii)
≤O
lnAmax K K 4 KX κ-i(I- ητ)-3/5[(1 - ητ)-3Tk/5 - 1]
LFKY + 工 Y	(1-ητ)-3/5- 1
+ YK Qn2 AmaX) + κ-l Y-K/2 - 1
+	e2(1 - γ)7 + Y	γ-1/2 - 1
(≤i) O]InAmaX K K + IX	YK-Tk/5	+ YK (ln2 AmaX) + Y(KT)
一 (1 — γ)2 Y	1 — (1 — ητ )3/5	e2(1 — γ)7	1 — γl/2
k=0
(≤) o" InAmaX K K + YKT(Y-3K/5- 1) + YK (ln2 AmaX) + L-
一 (1 — γ)2 T	ητ (γ-3/5 — 1) e2 (1 — γ)7	1 — Y
(V) O	YK ln Amax ] ( ln AmaX λ Y2K/5	ln Amax YK (ln2 AmaX) YK/2
≤	(1 - γ)3 ln (e(1 - Y) J	+ e(1	- γ)3	+ e2(1 - γ)7	+ 1 - Y
(Vi)
≤O
+e3 J⅛γ:+
e5/2(1 - γ)2∙75
ln5/4 AmaX
O(e)
(Vii)
≤ ce
(117)
33
Published as a conference paper at ICLR 2022
where (i), (ii) and (iii) use (1 -ητ)Tk-1 = γk based on eq. (113), (i) also uses eqs. (105), (108)-(111)
& (114)-(116), (iv) uses 1 - (1 - ητ)3/5 = O(ητ), 1 - γ1/2 = O(1 - γ), (v) uses eqs. (110)-
(112), (vi) uses O [ γK1l-Amax ln (InAmax)] ≤ O [可二力X)[ and YK = ：(1-A二 implied by
eq. (112), and the numeric constant c > 0 in (vii) exists. By replacing with /c, we obtain that
Dτ(∏K-ι,∏K-ι) ≤ which means (πK(1-) 1, πK(2-) 1) is an -Nash equilibrium policy pair, and the
orders of all the hyperparameter choices remain the same. In this case, the overall sample complexity
is given by
K-1	Tk-1
X [Nk+1 + 2 X (Nk,t + Nk,t+1 U
k=0	t=0
=Ohtmix(ln AmaX) ∣ ( |S| ln(e l ln AmaX) )]	—k/2
e2μmin(1 - γ)8 n ∖	δμmin(1 - Y)	上 Y
+ Oh tmixAme3/2n1/-Amr) ln (e⅛⅜⅛)] X1TX1(1 - ητ)-3(t+1)/5
L μmine	(1	Y)	veσμmin(j- Y) k=0 t=0
O h tmix(ln2 AmaX) j (⑸ ln(e-1 ln AmaX) )] Y-K/2 - 1
e2μmin(1 - Y)8 n I δμmin(1 - γ) Y-1/2 T
+ Oh tmixAmaX(In / AmaX)l (	ISIAmaX	)] X
μmine3∕2(1 - y)6 n ∖eδμmin(1 - y) L
(ii)	tmix(ln2 AmaX)	ISI ln(e—1 lnAmaX)	—K/2
≤ O Ie2〃min(1-Y)8	(	δμmin(1 - γ) 中
[(1 - ητ)-3Tk∕5 - 1]
1 — (1 — ητ )3/5
htmixAmaX(In / AmaX)
+ O μmine3∕2(1-γ)6
|S |AmaX	ln AmaX
eδμmin(1 - γ)V e(1 - γ)2
K—1
] X Y—3k/5
k=0
≤ O h tmix(ln AmaX)	(ISIln(e-1 ln AmaX) M —K/2
一le2μmiη(1 - Y)8	I	δμmin(1 - γ) Y
+ Oh tmixAmaX(In / AmaX)l ( ISIAmaX
μmine5∕2(1 - y)8	n ∖eδμmin(1 - y).
(iii) Ohtmix(In AmaX) l (〔S1 ln(e Iln AmaX))]
≤	L2μmin(1- Y)8 n (	δμmm(1- Y)
+O
tmixAmaX (ln	AmaX )
(=iv) O
μmine5∕2(1 - γ)8
maX (ln AmaX)
O(
5.5(1 - Y)13.5
tmixAmaX
μmine5.5(1 - Y)13.5 J
ln
ln (	ISIAmaX
,eδμmin(1 - Y)
(ISIAmaX	)]
eδμmin(1 - Y)〃
γ-3K/5
Y-3/5 — 1
I	ln5/4 AmaX
I e5∕2(1 - γ)3.75
M ln3/2 AmaX
e3(1 - γ)5.5
(118)
where (i) uses Nk,t ≤ Nk,t andeqs. (114)&(116), (ii) uses γ-1/2 -1 = O(1 - γ), 1 - (1 -ητ)3/5 =
O(ητ) = O(；n1—Y)2) and (1 -ητ)Tk-1 = Yk, (iii) uses Y-3/5 -1 = O(1 -Y) and γκ = ：；(1-A)7方
n max	ln	Amax
implied by the hyperparameter choice (112), and (iv) uses the fact that the second term of (iii) is
larger than the first term of (iii).	□
F Summary of Comparison of S ample Complexities
In the column “Model-free”, a Xmeans that an algorithm does not need any prior knowledge of the
environment, including reward mapping and transition kernel.
In the column “Private update”, a Xmeans that the algorithm updates do not involve the opponent’s
sensitive information, including action and policy.
34
Published as a conference paper at ICLR 2022
Table 1: Summary of sample complexity of algorithms for solving discounted infinite-horizon zero-
sum Markov games. (|S | is the number of states. Amax is the maximum number of actions between
the players. μmin is the lower bound of state stationary distribution. Y is the discount factor.)
Work	Model -free	Private update	Symmetric update	Data type	Sample complexity (duality gap ≤ )
Zou et al. (2019)	X	×	X	Markovian	-
Zhao et al. (2021)	X	X	X	i.i.d.	-
Guo et al. (2021)	X	X	X	i.i.d.	-
Cen et al. (2021)	×	X	X	-	-
Wei et al. (2021)	X	X	X	Markovian	Or	Amax∣s∣10.5	、 O 屋 8μmin(i-γ)29.5 J
Our work	X	X	X	Markovian	e (	Amax	ʌ θ e5∙5μmin (1-Y)13∙5
In the column “Symmetric update”, a Xmeans that both players perform symmetric updates.
In the column “Data type”, “Markovian” means that the algorithm uses samples queried from the
dependent Markov decision process. The “-" sign means no stochastic samples are used.
The sample complexity is defined as the total number of samples required to achieve an -duality
gap for the Markov game. A "-" sign in this column means that such type of sample complexity
is unavailable in that paper or its setting is different from ours. We use O to hide all the logarithm
factors.
We next explain how the sample complexity of (Wei et al., 2021) is calculated. Corollary 4 of (Wei
et al., 2021) shows that the iterate-average duality gap is smaller than with the hyper-parameter
choices: number of iterations T = Oe
1S|2
L = Oe
AmaxISI6
η2(1-γ)4e2
and number of samples queried per iteration
(1-γ)13μminη3e6
(we replace ξ with ). Hence, the overall sample complexity is
LT = Oeh	AmaxIS|8	i (≥ Oh	AmaxISl10.5
=O[(1-Y)17μminη5e8J ≥ O(1-Y)29.5μmine8
where (i) uses the choice of learning rate η ≤ O
required by their Algorithm 1.
G Rationality
In this section, we will prove that the policy extragradient (PE) algorithm Cen et al. (2021) is rational.
The following analysis can be directly extended to our stochastic policy extragradient (SPE) algorithm
by applying the estimation error bounds in Appendix B.
An algorithm is called rational if, whenever the player 2 adopts an arbitrary stationary pol-
icy ∏(2), the policy of the player 1 converges to the best response to n(2), i.e., e(T)(s) =
arg max∏(i) (S) V∏(^ n⑶(S) for all S ∈ S. We add tilde in e(T) (S) to differentiate it from the Nash
policy pair (n(?(s), ∏(T)(S)) and some consequent notations are similar. Denote Qr) := Q(Tl)	,
∏*√ ,∏
V(T) ：= V(τ)) (2)as the optimal value functions and define the soft Bellman operator as follows
Beτ(Q)(S,a(1),a(2))
：=R(s, a⑴,a⑵)+ YEs0〜P(∙∣s,α⑴,a⑶)h max fτ (Q(s0); ∏(I)(S0), ∏⑵(s0))i.	(119)
π(1) (s0)
Similar to eqs. (98)&(99), it can be proved that the soft Bellman operator BT is also a contraction
operator and has a unique fixed point QiT), i.e.,
kBeT(Q0) - BeT (Q)k∞ ≤YkQ0 - Qk∞,	(120)
35
Published as a conference paper at ICLR 2022
BT(QF ))=QF).	(i2i)
To solve modified Markov game with fixed policy π(2), we develop the single-player-perspective
version of the PE algorithm in Algorithm 2, which basically approximates the value iteration Qk+1 =
Bτ(Qk). The main modification from the original PE algorithm is that the new policy update (126)
only keeps the third expression of the PU steps (8) since only that affects the final policy of player
1 with fixed π(2). This new policy update (126) aims to find π(1) that maximizes the following
single-player Markov game problem with fixed Qk(s) and π(2) (s) for all s ∈ S, while the original
PU algorithm seeks the Nash equilibrium (π(1), π(2)) of the following function.
max fτ (Qk(s); π(1)(s), π(2)(s))
π(1)(s)
:=[π(1)(s)]>Qk(s)π(2)(s) + TH(π⑴(S)) - TH(π⑵(s)).	(122)
Similar to eq. (23), it can be derived that the solution 彳；(I)(S) to the above optimization problem
satisfies the following condition Cen et al. (2021).
彳：⑴(S) H exp(Qk(s)∏2(s)∕τ).	(123)
Algorithm 2 Single-player perspective PE algorithm for entropy-regularized Markov game.
Initialize: V0(s) for all s ∈ S.
for value iterations k = 0, 1, . . . , K - 1 do
Compute the following Q functions for all (S, a(1), a(2)).
Qk(s, a⑴,a⑵)=R(s, a⑴,a⑵)+ Y「s，〜P(s,a(i),a⑶)Vk(s0)	(124)
Qk1)(s, a(I))=Ea⑶〜∏(2)(s)Qk(s, a(1),a⑵)	(125)
Initialize πk(1,0) with uniform distribution.
for PU iterations t = 0, 1, . . . , T - 1 do
Player 1 implements the t-th policy update for all S, a(1) as follows.
πk13(a(I)IS) (X πk1,t(a(I)IS)1-ητ eχp(ηQkI)(S,a(I))),	(126)
end
Let πk(1) = πk(1,T) , and perform the following value iteration for all S
Vk+ι(S) ：= fτ(Qk(s)；∏k1)(S),∏(2)(S))	(127)
end
Output: πK(1-) 1.
We first prove that the value functions Vk and Qk in Algorithm 2 are bounded as follows.
Lemma 7. If kV0k∞ ≤ VmaX ：= 1+1-Amax in Algorithm 2, thenforall k ≥ 0, 0 ≤ t ≤ Tk — 1, we
have
max (IlVkk∞,kVV(T)k∞) ≤ Vmax,	(128)
max (kQk k∞, Ql 'k∞k∞) ≤ Qmax ,	(129)
where QmaX :=1 + YVmaX = 1+二-”- With Amax ：= max (∣A⑴I, ∣A⑵ ∣).
Proof. The proof is similar to that of Lemma 3.	□
Similar to Lemma 1, we can prove the following lemma about the convergence rates of the modified
PU steps defined by eq. (126).
36
Published as a conference paper at ICLR 2022
Lemma 8. Apply Algorithm 2 to solve the entropy regularized game with τ > 0. Choose learning
rate η ≤ τ-1 and initialization kV0k∞ ≤ Vmax. Then, the policy update defined by eq. (126) for
maximizing the function (122) has the following convergence rates.
KLM⑴(s)k∏k,T(s)) + KL(∏k,T(s)keθ(s)) ≤ 2Qmax(1-ητ)T	(130)
fτ(Qk(S)； ej1)(s),∏⑵(S))- Vk+ι(s) = τKL(∏k,T(s)ke；⑴(S)) ≤ 2Qmaχ(1-ητ)t (131)
Proof. Taking logarithm of eq. (126) yields that there exists ck,t ∈ R such that
lnπk(1,t)+1(S) = (1 -ητ)lnπk(1,t)(S) + ηQk(S)π2(S) + ck,t1.
Iterating the above equality over t = 0, 1, . . . , t yields that there exists c ∈ R such that
ln∏k,T(S)=———(IT nT) Qk(s)∏2(s) + cl	(132)
where we use the uniform policy initialization πk(1,0) (S) = 1/|A(1) |.
Taking logarithm of eq. (123) yields that there exists c* ∈ R such that
ln ek(1)(s) = Qk (s)∏2(s)∕τ + c*1	(133)
Therefore, eq. (130) can be proved as follows.
KL(ek ⑴(s)k∏k,T (S)) + KL(∏k,T (s)kek ⑴(S))
=〈.(I)(S),ln ek ⑴(S)- ln πk,T(S)〉+ sk,T(S),ln πk,T(S)- ln ek ⑴(S)〉
= DeZ(I)(S) - πk,T(S), (I :) Qk(S)π2(S) +(c* - C)IE
" Q-T)T 尾⑴(S) -∏k,T(S)]>Qk(s)∏2(s)
(<) 2Qmax(1 - ητ)t
τ
where (i) uses eqs. (132)&(133), (ii) uses hπekZ(1)(S) - πk(1,T) (S), 1i = 0 and (iii) uses kQk(S)k∞ ≤
Qmax, kπekZ(1) - πk(1,T) k1 ≤ 2, kπ2(S)k1 = 1 and 1 - ητ > 0.
fτ(Qk(s);∏k⑴(S),π⑵(S))- Vk+ι(s)
=fτ(Qk(s); ek⑴(s),π⑵(S))- fτ(Qk(s); ∏k,T(s),π⑵(S))
=(ek(I)(S)- πk,T (s), Qk(S)五⑵⑶〉+Kπk,T(S), ln πk,T (s)〉- T〈ek(I)(S), ln 寇⑴(s)〉
=(ez(I)(S)-πk,T(S),τln.⑴(S)〉+ τ(πk,T(S),lnπk,T(S)〉-τ(ek(I)(S),lnek⑴(s)〉
=τKL(∏k,T (S)优⑴(S))
(ii)
≤ 2Qmax(1 - ητ)T
where (i) uses eq. (133) and (ek(I)(S) - ∏fT(s), 1)= 0 and (ii) uses eq. (130).	口
Finally, we prove the convergence rate of Algorithm 2 as follows.
Theorem 3. Apply Algorithm 2 to solve the entropy regularized game with τ > 0. Choose learning
rate η ≤ τ-1 and initialization kV0 k∞ ≤ Vmax. Then, the Q function estimation error kQK-1 -
Qe(Zτ) k∞ and the function value gap maxs VeZ(τ) (S) - V ((τ1))	(S) converge at the following rates
πK 1,π(2)
kQκ-1 - QF)k∞ ≤ 2QmaχYK + 2YQmaX (1 - ητ)T,
1-γ
max [V(T)(s) - V(T))⑵(s)i ≤ 2Qmax (2γκ-1 + 产(1 - ητ)t).
s	πK-1,π(2)	1 - Y	1 - Y
(134)
(135)
37
Published as a conference paper at ICLR 2022
Proof.
∣∣Qk+ι - Qr)∣∣∞
(i)
≤ ∣Qk+ι - S(Qk)∣∞ + ∣Bτ(Qk)- BMQV))∣∣∞
(ii)
≤ max	∣Qk+ι(s,。⑴,a(2)) - BT(Qk)(s, a(I), a(2))| + YIlQk - Qr)I∣∞
s,α(I),a(2)
=max/R(s, α(I),a(2)) + γE√~P(∙∣s,a(ι),a(2)) Vk+1(s')
s,α(I),a(2) I
-[R(s, a(1),a(2)) + YEs，〜p(∙∣s,a(i),a(2))fτ(Qk(s0);范⑴(s0), π(2)(s,))]∣+ YkQk- QF)∣∞
≤ Y m aχ ∣ Vk+ι(s0) - fτ (Qk (s0)；元⑴(s0), n(2)(s0)) ∣ + Y IlQk- Qr)I∣∞
s，∈S
(iii)
≤
2YQmaX(I - ητ)T + YIlQk - QiT) ∣∣∞
where (i) uses eq. (121), (ii) uses eq. (120), (iii) uses eq. (131). Iterating the above inequality yields
that
1 - Y T
IlQK-I - Q* ∣∣∞ ≤ Y ∣∣q0 - Q* ∣∣∞ + 2YQmaX(I - ητ)--
1-Y
K I 2YQmaX	T
≤ 2QmaxY + W-------(1 - 〃7),
1-Y
which proves eq. (134). Finally, eq. (135) can be proved as follows
V(T%)-⅛L∏(2) (S)
=X [Q*τ)(s, a(I),α⑵)WT)(a(I) ∣s) - Q、).⑵(s, a(I), a⑵)∏K-ι(α(I)∣s)]π⑵(a(2)|s)
a ⑴，α(2)	πK^1,π
+UH 优 T)(S)) -H(πK-I(S))]
=X [Q*t)(s,a⑴,a(2))-Q∏Tl) π(2) (s,a(1),a(2))]nK-i(a(1)|s)n(2)(a(2)|s)
a ⑴，a(2)	KKTK
+ X Q*T)(s,a(I),a(2))[K T)(a(I)IS)-∏K-ι(a⑴|s)]n⑵(a⑵|s)
a(1),a(2)
+T [H(ee( T)(S)) -H(πK - I(S))]
=Y X	[πK - 1(a(I )|SH2)(a ⑵ IS)ES，〜p(∙∣s,a(ι),a(2))[V*(T )(s，) - V(TI)).⑵(S1]]
a(1),a(2)	KT
+ (∏( T)(S)- ∏K-I(S))TQ(T%)π⑵(s) + τ[H(e( T)(S)) - H(∏K-I(S))]
≤ YmaX [V*(T)(s') - Vi(TI)) π⑵(s')] + 优T)(s) -∏K-1(S))TQ*T)(s)∏(2)(s)
+ τ[H优 T)(S)) -H(πK-I(S))].
Applying maxs to both sides of the above inequality and rearranging it yields that
max [V*(T T (s) - V(KLn(2) (s)]
≤ 1--ʒ; max ((# T)(s) - ∏K-I(S))TQ*τ)(s)π(2)(s) + τ[H(π( T)(S)) - H(∏K- 1(s))])
≤ 1-ʒ; max (fτ(Q*τ)(s); e(T)(S),n(2)(s)) - fτ(Q*τ)(s); ∏K-ι(s), n(2)(s)))
≤ 1-ʒ; max (fτ(Q(T)(s);e(T)(S),n(2)(s)) - fτ(Qκ-i(s)；暧-1 ,π⑵(S)))
+ 1-ʒ; max (fτ(Qκ-i(s); lK-L(s), n(2)(s)) - fτ(Qκ-i(s); ∏K-ι(s), ∏(2)(s)))
38
Published as a conference paper at ICLR 2022
+ ι-γ max (fτ(Qκ-i(s), ∏K-ι(s), ∏⑵(S)) - fτ(Qf)(s)； ∏K-ι(s),∏⑵(Sl
≤	— max I ( max
1 -γ s π(1) (s)
π(1)(S)
∏(2)(s))) - m max fτ(Qκ-ι(s)∏(I)(S)
π(1) (s)
+ ι--γj max (fτ(Qκ-i(s); eK(-)ι(s), ∏(2)(s)) - VK(S))
+ ι-γ max ∣fτ (Qκ-i(s); ∏K-ι(s), ∏ ⑵(S)) - fτ(QF)(s)； ∏K-ι (s),∏ ⑵(s))∣
≤ ɪ(2QmaxYK + 2YQmax (1 - ητ)力 + 2Qmax (1 - ητ)t
1-γ	1-γ	1-γ
≤ 2Qmax(2γκ-1 +产(1 -ητ)t),
1-Y	1-Y
where (i) uses eq. (131) and the following inequality that holds for all S, π(1) (S), π(2) (S), (ii) uses eq.
(134) and VK-I(S) = f (QKT-i(s)； ∏κ-i(s),∏⑵(s)).
∣fτ (QK-i(s)； ∏K-i(s),∏ ⑵(S)) - fτ (QF)(S)； ∏K-i(s),∏ ⑵(s))∣
≤ IlQK-I(S)- QlT)(S)k∞	≤	2QmaxYK + ^；Qmax (1 -中)T.
1-Y
□
In Theorem 3, the convergence rates ofboth the Q function estimation error ∣∣Qκ-ι - Qr)I∣∞ and the
function value gap max§ [V(T) (s) - V(T) ⑶(s)] consist of two terms O(YK) and O(1 - ητ)t,
which characterize the exponential convergence rates of the K outer value iterations and the T
inner policy updates respectively. Such a convergence result indicates that Algorithm 2 converges
to the optimal solution to the regularized Markov game, and thus can be arbitrarily close to the
optimal solution of the unregularized Markov game with sufficiently small τ . As a result, the PU
algorithm Cen et al. (2020) is rational. Similar result can be directly extended to our stochastic policy
extragradient (SPE) algorithm by applying the estimation error bounds in Appendix B.
39