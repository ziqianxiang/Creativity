Published as a conference paper at ICLR 2022
Learning Curves for Stochastic Gradient De-
scent on Structured Features
Blake Bordelon & Cengiz Pehlevan
John A. Paulson School of Engineering and Applied Sciences
Center for Brain Science
Harvard University
Cambridge, MA 02138, USA
{blake_bordelon,Cpehlevan} @g.harvard.edu
Ab stract
The generalization performance of a machine learning algorithm such as a neu-
ral network depends in an intricate way on the structure of the data distribution.
To analyze the influence of data structure on test loss dynamics, we study an ex-
actly solveable model of stochastic gradient descent (SGD) which predicts test
loss when training on features with arbitrary covariance structure. We solve the
theory exactly for both Gaussian features and arbitrary features and we show that
the simpler Gaussian model accurately predicts test loss of nonlinear random-
feature models and deep neural networks trained with SGD on real datasets such
as MNIST and CIFAR-10. We show that the optimal batch size at a fixed com-
pute budget is typically small and depends on the feature correlation structure,
demonstrating the computational benefits of SGD with small batch sizes. Lastly,
we extend our theory to the more usual setting of stochastic gradient descent on
a fixed subsampled training set, showing that both training and test error can be
accurately predicted in our framework on real data.
1	Introduction
Understanding the dynamics of SGD on realistic learning problems is fundamental to learning the-
ory. Due to the challenge of modeling the structure of realistic data, theoretical studies of general-
ization often attempt to derive data-agnostic generalization bounds or study the typical performance
of the algorithm on high-dimensional, factorized data distributions (Engel & Van den Broeck, 2001).
Realistic datasets, however, often lie on low dimensional structures embedded in high dimensional
ambient spaces (Pope et al., 2021). For example, MNIST and CIFAR-10 lie on surfaces with intrin-
sic dimension of 〜14 and 〜35 respectively (SPigler et al., 2020). To understand the average-case
performance of SGD in more realistic learning problems and its dependence on data, model and
hyperparameters, incorporating structural information about the learning problem is necessary.
In this paper, we calculate the average case performance of SGD on models of the form f(x) =
W ∙ ψ (x) for nonlinear feature map ψ trained with MSE loss. We express test loss dynamics in terms
of the induced second and fourth moments ofψ. Under a regularity condition on the fourth moments,
we show that the test error can be accurately predicted in terms of second moments alone. We
demonstrate the accuracy of our theory on random feature models and wide neural networks trained
on MNIST and CIFAR-10 and accurately predict test loss scalings on these datasets. We explore in
detail the effect of minibatch size, m, on learning dynamics. By varying m, we can interpolate our
theory between single sample SGD (m = 1) and gradient descent on the population loss (m → ∞).
To explore the computational advantages SGD compared to standard full batch gradient descent
we analyze the loss achieved at a fixed compute budget C = tm for different minibatch size m and
number of steps t, trading off the number of parameter update steps for denoising through averaging.
We show that generally, the optimal batch size is small, with the precise optimum dependent on the
learning rate and structure of the features. Overall, our theory shows how learning rate, minibatch
size and data structure interact with the structure of the learning problem to determine generalization
dynamics. It provides a predictive account of training dynamics in wide neural networks.
1
Published as a conference paper at ICLR 2022
1.1	Our Contributions
The novel contributions of this work are described below.
•	We calculate the exact expected test error for SGD on MSE loss for arbitrary feature struc-
ture in terms of second and fourth moments as we discuss in Section 4.2. We show how
structured gradient noise induced by sampling alters the loss curve compared to vanilla GD.
•	For Gaussian features (or those with regular fourth moments), we compute the test error in
Section 1. This theory is shown to be accurate in experiments with random feature models
and wide networks in the kernel regime trained on MNIST and CIFAR-10.
•	We show that for fixed compute/sample budgets and structured features with power law
spectral decays, optimal batch sizes are small. We study how optimal batch size depends
on the structure of the feature correlations and learning rate.
•	We extend our exact theory to study multi-pass SGD on a fixed finite training set. Both test
and training error can be accurately predicted for random feature models on MNIST.
2	Related Work
The analysis of stochastic gradient descent has a long history dating back to seminal works of Polyak
& Juditsky (1992) and Ruppert (1988), who analyzed time-averaged iterates in a noisy problem.
Many more works have examined a similar setting, identifying how averaged and accelerated ver-
sions of SGD perform asymptotically when the target function is noisy (not a deterministic function
of the input) (Flammarion & Bach, 2015; 2017; Shapiro, 1989; Robbins & Monro, 1951; Chung,
1954; Duchi & Ruan, 2021; Yu et al., 2020; Anastasiou et al., 2019; Gurbuzbalaban et al., 2021).
Recent studies have also analyzed the asymptotics of noise-free MSE problems with arbitrary fea-
ture structure to see what stochasticity arises from sampling. Prior works have found exponen-
tial loss curves problems as an upper bound (Jain et al., 2018) or as typical case behavior for
SGD on unstructured data (Werfel et al., 2004). A series of more recent works have considered
the over-parameterized (possibly infinite dimension) setting for SGD, deriving power law test loss
curves emerge with exponents which are better than the O(t-1) rates which arise in the noisy prob-
lem (Berthier et al., 2020; Pillaud-Vivien et al., 2018; Dieuleveut et al., 2016; Varre et al., 2021;
Dieuleveut & Bach, 2016; Ying & Pontil, 2008; Fischer & Steinwart, 2020; Zou et al., 2021). These
works provide bounds of the form O(t-β) for exponents β which depend on the task and feature
distribution.
Several works have analyzed average case online learning in shallow and two-layer neural networks.
Classical works often analyzed unstructured data (Heskes & Kappen, 1991; Biehl & Riegler, 1994;
Mace & Coolen, 1998; Saad & Solla, 1999; LeCun et al., 1991; Goldt et al., 2019), but recently
the hidden manifold model enabled characterization of learning dynamics in continuous time when
trained on structured data, providing an equivalence with a Gaussian covariates model (Goldt et al.,
2020; 2021). In the continuous time limit considered in these works, SGD converges to gradient
flow on the population loss, where fluctuations due to sampling disappear and order parameters
obey deterministic dynamics. Other recent works, however, have provided dynamical mean field
frameworks which allow for fluctuations due to random sampling of data during a continuous time
limit of SGD, though only on simple generative data models (Mignacco et al., 2020; 2021).
Studies of fully trained linear (in trainable parameters) models also reveal striking dependence on
data and feature structure. Analysis for models trained on MSE (Bartlett et al., 2020; Tsigler &
Bartlett, 2020; Bordelon et al., 2020; Canatar et al., 2020), hinge loss (Chatterji & Long, 2021; Cao
et al., 2021; Cao & Gu, 2019) and general convex loss functions (Loureiro et al., 2021) have now
been performed, demonstrating the importance of data structure for offline generalization.
Other works have studied the computational advantages of SGD at different batch sizes m. Ma et al.
(2018) study the tradeoff between taking many steps of SGD at small m and taking a small number
of steps at large m. After a critical m, they observe a saturation effect where increasing m provides
diminishing returns. Zhang et al. (2019) explore how this critical batch size depends on SGD and
momentum hyperparameters in a noisy quadratic model. Since they stipulate constant gradient noise
2
Published as a conference paper at ICLR 2022
induced by sampling, their analysis results in steady state error rather than convergence at late times,
which may not reflect the true noise structure induced by sampling.
3	Problem Definition and S etup
We study stochastic gradient descent on a linear model with parameters w and feature map
ψ (x) ∈ RN (with N possibly infinite). Some interesting examples of linear models are random
feature models, where ψ(x) = φ(Gx) for random matrix G and point-wise nonlinearity φ (Rahimi
& Recht, 2008; Mei & Montanari, 2020). Another interesting linearized setting is wide neural net-
works with neural tangent kernel (NTK) parameterization (Jacot et al., 2020; Lee et al., 2020). Here
the features are parameter gradients of the neural network function ψ(x) = Vθf (x, θ)麻 at initial-
ization. We will study both of these special cases in experiments.
We optimize the set of parameters w by SGD to minimize a population loss of the form
L(W) = D(W ∙ψ(X)- y(X))2E	I.、，	⑴
X〜P(X)
where X are input data vectors associated with a probability distribution p(X), ψ(X) is a nonlinear
feature map and y(X) is a target function which we can evaluate on training samples. We assume
that the target function is square integrable y(X)2 X < ∞ over p(X). Our aim is to elucidate how
this population loss evolves during stochastic gradient descent on W. We derive a formula in terms
of the eigendecomposition of the feature correlation matrix and the target function
N
Σ = ψ(X)ψ(X)>X = Xλkukuk> , y(X) = Xvkuk>ψ(X) + y⊥(X),	(2)
k=1	k
where hy⊥ (X)ψ(X)i = 0. We justify this decomposition of y(X) in the Appendix A using an
eigendecomposition and show that it is general for target functions and features with finite variance.
During learning, parameters W are updated to estimate a target function y which, as discussed above,
can generally be expressed as a linear combination of features y = w* ∙ ψ + y⊥. At each time step
t, the weights are updated by taking a stochastic gradient step on a fresh mini-batch of m examples
m
Wt+1 = Wt — mm E ψt," (Wt ∙ ψt,μ — yt,μ) ,	(3)
μ=1
where each of the vectors ψt,μ are sampled independently and yt,μ = w* ∙ ψt,μ. The learning rate
η controls the gradient descent step size while the batch size m gives a empirical estimate of the
gradient at timestep t. At each timestep, the test-loss, or generalization error, has the form
Lt
《Wt ∙ ψ(x) — W*
• ψ(χ) — y⊥(χ))2)x
- W*)>Σ(Wt - W*) + y⊥(X)2 ,
(4)
which quantifies exactly the test error of the vector Wt. Note, however, that Lt is a random variable
since Wt depends on the precise history of sampled feature vectors Dt = {ψt,μ}.OUr theory, which
generalizes the recursive method of (Werfel et al., 2004) allows us to compute the expected test
loss by averaging over all possible sequences to obtain hLtiD . Our calculated learning curves are
not limited to the one-pass setting, but rather can accommodate sampling minibatches from a finite
training set with replacement and testing on a separate test set which we address in Section 4.4.
In summary, we will develop a theory that predicts the expected test loss hLtiD averaged over
training sample sequences Dt in terms of the quantities {λk, vk, y⊥ (X)2 X}. This will reveal how
the structure in the data and the learning problem influence test error dynamics during SGD. This
theory is a quite general analysis of linear models on square loss, analyzing the performance of
linearized models on arbitrary data distributions, feature maps ψ, and target functions y(X).
4 Analytic Formulae for Learning Curves
4.1	Learnable and Noise Free Problems
Before studying the general case, we first analyze the setting where the target function is learnable,
meaning that there exist weights W* such that y(X) = W* • ψ(X). For many cases of interest, this
3
Published as a conference paper at ICLR 2022
is a reasonable assumption, especially when applying our theory to real datasets by fitting an atomic
measure on P points P Pμ δ(x - xμ). We will further assume that the induced feature distribution
is Gaussian so that all moments of ψ can be written in terms of the covariance Σ. We will remove
these assumptions in later sections.
Theorem 1. Suppose the features ψ follow a Gaussian distribution ψ 〜N(0, Σ) and the target
function is learnable in these features y = w* ∙ ψ. After t steps of SGD with minibatch size m and
learning rate η, the expected (over possible sample sequences Dt) test loss hLtiD has the form
〈Lt〉D = λ> Atv2 , A = (I — η diag(λ))2 + — diag (λ2) + — λλ>	(5)
Dt	m	m
where λ is a vector containing the eigenvalues ofΣ and v2 is a vector containing elements (v2 )k =
v2 = (Uk ∙ w* )2 for eigenVectorS Uk of Σ. The function diag (∙) constructs a diagonal matrix with
the argument vector placed along the diagonal.
Proof. See Appendix B for the full derivation. We will provide a brief sketch of the proof
here. The strategy of the proof relies on the fact that hLti = Tr Σ Ct where Ct =
(wt - w*) (wt - w*)> D . We derive the following recursion relation for this error matrix
Ct+1 = (I - η∑)Ct(I - η∑) + η2 [ΣCtΣ + ∑Tr (ΣCt)]
m
(6)
The loss only depends on ck,t = Uk>CtUk. Solving the recurrence, ct = Atv2 and using hLti =
Pk λku>CtUk = Pk ck,tλk = λ> Atv2, we obtain the desired result.	□
Below we provide some immediate interpretations of this result.
•	The matrix A contains two components; a matrix (I - η diag(λ))2 which represents
the time-evolution of the loss under average gradient updates. The remaining matrix
ηm2 (diag(λ2) + λλ>) arises due to fluctuations in the gradients, a consequence of the stochas-
tic sampling process.
•	The test loss obtained when training directly on the population loss can be obtained by taking the
minibatch size m → ∞. In this case, A → (I - η diag(λ))2 and one obtains the population loss
Ltpop = Pk vk2λk (1 - ηλk)2t. This population loss can also be obtained by considering small
learning rates, i.e. the η → 0 limit, where A = (I - η diag(λ))2 + O(η2).
•	For general λ and η2/m > 0, A is non-diagonal, indicating that the components {uι,…,Uk} are
not learned independently as t increases like for Ltpop , but rather interact during learning due to
non-trivial coupling across eigenmodes at large η2/m. This is unlike offline theory for learning
in feature spaces (kernel regression), (Bordelon et al., 2020; Canatar et al., 2020), this observation
of mixing across covariance eigenspaces agrees with a recent analysis of SGD, which introduced
recursively defined “mixing terms” that couple each mode’s evolution (Varre et al., 2021).
•	Though increasing m always improves generalization at fixed time t (proof given in Appendix D),
learning with a fixed compute budget (number of gradient evaluations) C = tm, can favor smaller
batch sizes. We provide an example of this in the next sections and Figure 1 (d)-(f).
• The lower bound hLti ≥ λ>v2
(1 - η)2 +
m ∣λ∣2it
can be used to find necessary stability con-
ditions on m,η. This bound implies thathLt〉will diverge if m < 2-Ln ∣λ∣2. The learning rate must
be sufficiently small and the batch size sufficiently large to guarantee convergence. This stability
condition depends on the features through ∣λ∣2 = Pk λk. One can derive heuristic optimal batch
sizes and optimal learning rates through this lower bound. See Figure 2 and Appendix C.
4.1.1	Special Case 1: Unstructured Isotropic Features
This special case was previously analyzed by Werfel et al. (2004) which takes Σ = I ∈ RN×N and
m = 1. We extend their result for arbitrary m, giving the following learning curve
hLtiDt =((I-n)2 + ⅛Nη2)tllw*ll2,	hL*iDt=(1-mɪɪ)tllw*ll2,⑺
m
4
Published as a conference paper at ICLR 2022
(a) Isotropic Features
(b) Power Law Features
(c) MNIST Random ReLU Fea-
(d) Fixed Compute Isotropic
(e) Fixed Compute Power Law
tures
(f) Fixed Compute ReLU MNIST
Figure 1: Isotropic features generated as ψ 〜N(0, I) have qualitatively different learning curves
than power-law features observed in real data. Black dashed lines are theory. (a) Online learning
with N-dimensional isotropic features gives a test loss which scales like Lt 〜e-t/N for any target
function, indicating that learning requires t 〜 N steps of SGD, using the optimal learning rates
η* = N +m.ι. (b) Power-law features ψ 〜N(0, Λ) with Akl = δk,ιk-2 have non-extensive give a
power-law scaling Lt 〜t-β with exponent β = ON(1). (c) Learning to discrimninate MNIST 8's
and 9’s with N = 4000 dimensional random ReLU features (Rahimi & Recht, 2008), generates a
power law scaling at large t, which is both quantitatively and qualitatively different than the scaling
predicted by isotropic features e-t/N. (d)-(f) The loss at a fixed compute budget C = tm = 100 for
(d) isotropic features, (e) power law features and (f) MNIST ReLU random features with simulations
(dots average and standard deviation for 30 runs). Intermediate batch sizes are preferable on real
data.
where the second expression has optimal η. First, we note the strong dependence on the ambient
dimension N: as N》m, learning happens at a rate (L)〜e-tm/N. Increasing the minibatch size
m improves the exponential rate by reducing the gradient noise variance. Second, we note that this
feature model has the same rate of convergence for every learnable target function y. At small m,
the convergence at any learning rate η is much slower than the convergence of the m → ∞ limit,
Lpop = (1 一 η)2t∣∣w*∣∣2 which does not suffer from a dimensionality dependence due to gradient
noise. Lastly, for a fixed compute budget C = tm, the optimal batch size is m* = 1; see Figure 1
(d). This can be shown by differentiating LC/m with respect to m (see Appendix E). In Figure 1
(a) we show theoretical and simulated learning curves for this model for varying values of N at the
optimal learning rate and in Figure 1 (d), we show the loss as a function of minibatch size fora fixed
compute budget C = tm = 100. While fixed C represents fixed sample complexity, we stress that
it may not represent wall-clock run time when data parallelism is available (Shallue et al., 2018).
4.1.2	Special Case 2: Power Laws and Effective Dimensionality
Realistic datasets such as natural images or audio tend to exhibit nontrivial correlation structure,
which often results in power-law spectra when the data is projected into a feature space, such as a
randomly intialized neural network (Spigler et al., 2020; Canatar et al., 2020; Bahri et al., 2021).
In the Tm《 1 limit, if the feature spectra and task specra follow power laws, λk 〜 k-b and
λk Vk 〜 k-ɑ with a,b > 1, then Theorem 1 implies that generalization error also falls with a power
law: (Lti 〜Ct-β, β = a-1 where C is a constant. See Appendix G for a derivation with
saddle point integration. Notably, these predicted exponents we recovered as a special case of our
theory agree with prior work on SGD with power law spectra, which give exponents in terms of the
5
Published as a conference paper at ICLR 2022
(a) Power Law Exponents b
(b) Optimal Batchsize vs λ
(c) Hyper-parameter Dependence
Figure 2: Optimal batch size depends on feature structure and noise level. (a) For power law features
λk〜 k-b, λkv2 〜k-a, the m dependence of the loss LC/m depends strongly on the feature
exponent b. Each color is a different b value, evenly spaced in [0.6, 2.5] with a = 2.5, C = 500.
Solid lines show exact theory while dashed lines show the error predicted by approximating the mode
coupling term Tmλλ> with decoupled Tmdiag(λ2). Mode coupling is thus necessary to accurately
predict optimal m. (b) The optimal m scales proportionally with ∣λ∣2 ≈ ^^-ɪ. We plot the lower
bound mmin (black), the heuristic optimum (m which optimizes a lower bound for L, green) and
2-n ∣λ∣2 (red). (c) The loss at fixed compute C = 150, a = 2, b = 0.85, optimal batchsize m for
each η shown in dashed black. For sufficiently small η, the optimal batchsize is m = 1. For large η,
it is better to trade off update steps for denoised gradients resulting in m* > 1.
feature correlation structure (Berthier et al., 2020; Dieuleveut et al., 2016; Velikanov & Yarotsky,
2021; Varre et al., 2021). Further, our power law scaling appears to accurately match the qualitative
behavior of wide neural networks trained on realistic data (Hestness et al., 2017; Bahri et al., 2021),
which we study in Section 5.
We show an example of such a power law scaling with synthetic features in Figure 1 (b). Since the
total variance approaches a finite value as N → ∞, the learning curves are relatively insensitive to
N, and are rather sensitive to the eigenspectrum through terms like ∣λ∣2 and 1>λ, etc. In Figure
1 (c), we see that the scaling of the loss is more similar to the power law setting than the isotropic
features setting in a random features model of MNIST, agreeing excellently with our theory. For this
model, we find that there can exist optimal batch sizes when the compute budget C = tm is fixed
(Figure 1 (e) and (f)). In Appendix C.1, we heuristically argue that the optimal batch size for power
law features should scale as, m* ≈ W)7. Figure 2 tests this result.
We provide further evidence of the existence of power law structure on realistic data in Figure 3
(a)-(c), where we provide spectra and test loss learning curves for MNIST and CIFAR-10 on ReLU
random features. The eigenvalues λk 〜 k-b and the task power tail sums P,∞=k λnvn 〜 k-a+1
both follow power laws, generating power law test loss curves. These learning curves are contrasted
with isotropically distributed data in R784 passed through the same ReLU random feature model and
we see that structured data distributions allow much faster learning than the unstructured data. Our
theory is predictive across variations in learning rate, batch size and noise (Figure 3).
4.2 Arbitrary Induced Feature Distributions: The General Solution
The result in the previous section was proven exactly for Gaussian vectors (see Appendix B). For
arbitrary distributions, we obtain a slightly more involved result (see Appendix F).
Theorem 2.	Let ψ(x) ∈ RN be an arbitrary feature map with covariance matrix Σ =
k λkukuk>. After diagonalizing the features φk (x) = uk>ψ(x), introduce the fourth moment
tensor κi4jkl = hφiφjφkφli. The expected loss is exactly hLti = k λk ck (λ, κ, v, t).
We provide an exact formula for ck in the Appendix F We see that the test loss dynamics depends
only on the second and fourth moments of the features through quantities λk and κij k` respectively.
We recover the Gaussian result as a special case when κijkl is a simple weighted sum of these three
products of Kronecker tensors κiGjakul ss = λiλjδikδjl + λiλkδij δkl + λiλjδilδjk. As an alternative to
6
Published as a conference paper at ICLR 2022
k	k
(a) Feature Spectra
(d) Vary η (20 trials)
Figure 3: Structure in the data distribution, nonlinearity, batchsize and learning rate all influence
learning curves. (a) ReLU random feature embedding in N = 4000 dimensions of MNIST and
CIFAR images have very different eigenvalue scalings than spherically isotropic vectors in 784
dimensions. (b) The task power spectrum decays much faster for MNIST than for random isotropic
vectors. (c) Learning curves reveal the data-structure dependence of test error dynamics. Dashed
lines are theory curves derived from equation. (d) Increasing the learning rate increases the initial
speed of learning but induces large fluctuations in the loss and can be worse at large t. Experiment
curves averaged over 20 random trajectories of SGD. (e) Increasing the batch size alters both the
average test loss Lt and the variance. (f) Noise in the target values during training produces an
asymptotic error L∞ which persists even as t → ∞.
(b) Task Power Tail Sum
(c) Learning Curves m = 5
(e) Vary m (20 Trials)	(f) Vary σ (20 Trials)
the above closed form expression for hLti, a recursive formula which tracks N mixing coefficients
has also been used to analyze the test loss dynamics for arbitrary distributions (Varre et al., 2021).
Next we show that a regularity condition, similar to those assumed in other recent works (Jain et al.,
2018; Berthier et al., 2020; Varre et al., 2021), on the fourth moment structure of the features allows
derivation of an upper bound which is qualitatively similar to the Gaussian theory.
Theorem 3.	If the fourth moments satisfy ψψ> Gψψ>	(α + 1)ΣGΣ + αΣTrΣG for any
positive-semidefinite G, then
2
Lt ≤ λ> Atv2 , A = (I — η diag(λ))2 +--------[diag(λ2) + λλ>].
(8)
We provide this proof in Appendix F.1. We note that the assumed bound on the fourth moments is
tight for Gaussian features with α = 1, recovering our previous theory. Thus, if this condition on
the fourth moments is satisfied, then the loss for the non-Gaussian features is upper bounded by the
Gaussian test loss theory with the batch size effectively altered m = m∕α.
The question remains whether the Gaussian approximation will provide an accurate model on re-
alistic data. We do not provide a proof of this conjecture, but verify its accuracy in empirical
experiments on MNIST and CIFAR-10 as shown in Figure 3. In Appendix Figure F.1, we show that
the fourth moment matrix for a ReLU random feature model and its projection along the eigenbasis
of the feature covariance is accurately approximated by the equivalent Gaussian model.
4.3 Unlearnable or Noise Corrupted Problems
In general, the target function y(x) may depend on features which cannot be expressed as linear
combinations of features ψ(x), y(x) = w* ∙ ψ(x) + y⊥(x). Let (y⊥(x)2)X = σ2. Note that y⊥
need not be deterministic, but can also be a stochastic process which is uncorrelated with ψ(x).
7
Published as a conference paper at ICLR 2022
(a) MNIST Training Error
(b) MNIST Test Error
Figure 4: Training and test errors of a model trained on a training set of size M can be computed
with the Ct matrix. Dashed black lines are theory. (a) The training error for MNIST random feature
model approaches zero asymptotically. (b) The test error saturates to a quantity dependent on M .
Theorem 4.	For a target function with unlearnable variance y⊥2 = σ2 trained on Gaussian ψ,
the expected test loss has the form
hLti - σ2 = λ>Atv2 + -1 η2σ2λ>(I - A)T(I - At)λ	(9)
m
which has an asymptotic, irreducible error <L∞ = σ2 + m1 η2σ2λ>(I — A)-1λ as t → ∞.
See Appendix H for the proof. The convergence to the asymptotic error takes the form hLt - L∞i =
λ>At (v2 — mlη2σ2 (I 一 A)-1λ). We note that this quantity is not necessarily monotonic in t and
can exhibit local maxima for sufficiently large σ2, as in Figure 3 (f).
4.4 Test/Train Splits
Rather than interpreting our theory as a description of the average test loss during SGD in a one-pass
setting, where data points are sampled from the a distribution at each step of SGD, our theory can be
suitably modified to accommodate multiple random passes over a finite training set. To accomplish
this, one must first recognize that the training and test distributions are different.
Theorem 5. Let P(X)=吉 £" δ(x — xμ) be the empirical distribution on the M training data
points and let Σ = (ψ (x)ψ (X~τ)乂〜^χ = Pk λkuku> be the feature COrrelatiOn matrix on this
training set. Let p(X) be the test distribution Σ its corresponding feature correlation. Then we have
hLtrain,t i = Tr [ςCti ,	hLtest,ti = Tr 囚孰]
Ct+1 = (I — ηΣ)Ct(I — η∑) + n2 [<ψ(x)ψ(x)>Ctψ(x)ψ(x)>)χ〜/⑶—∑Ct∑]	(10)
We provide the proof of this theorem in Appendix I. The interpretation of this result is that it pro-
vides the expected training and test loss if, at each step of SGD, m points from the training set
{X1, ..., XM } are sampled uniformly with replacement and used to calculate a stochastic gradient.
Note that while Σ can be full rank, the rank of Σ has rank upper bounded by M, the number of
training samples. The recurrence for Ct can again be more easily solved under a Gaussian approx-
imation which we employ in Figure 4. Since learning will only occur along the M dimensional
subspace spanned by the data, the test error will have an irreducible component at large time, as
evidenced in Figure 4. While the training errors continue to go to zero, the test errors saturate at a
M -dependent final loss. This result can also allow one to predict errors on other test distributions.
5	Comparing Neural Network Feature Maps
We can utilize our theory to compare how wide neural networks of different depths generalize when
trained with SGD on a real dataset. With a certain parameterization, large width NNs are approxi-
mately linear in their parameters (Lee et al., 2020). To predict test loss dynamics with our theory,
it therefore suffices to characterize the geometry of the gradient features ψ(x) = Vθf (x, θ). In
8
Published as a conference paper at ICLR 2022
(a) MNIST NTK Spectra
(b) MNIST Task Spectra
(c) Test Loss Scaling Laws
(f) Test Loss Scalings
(d) CIFAR-10 NTK Spectra (e) CIFAR-10 Task Spectra
Figure 5: ReLU neural networks of depth D and width 500 are trained with SGD on full MNIST. (a)-
(b) Feature and spectra are estimated by diagonalizing the infinite width NTK matrix on the training
data. We fit a simple power law to each of the curves λk 〜k-b and Vk 〜k-a. (c) Experimental test
loss during SGD (color) compared to theoretical power-law scalings t-a-~ (dashed black). Deeper
networks train faster due to their slower decay in their feature eigenspectra λk, though they have
similar task spectra. (d)-(f) The spectra and test loss for convolutional and fully connected networks
on CIFAR-10. The CNN obtains a better convergence exponent due to its faster decaying task
spectra. The predicted test loss scalings (dashed black) match experiments (color).
Figure 5, we show the Neural Tangent Kernel (NTK) eigenspectra and task-power spectra for fully
connected neural networks of varying depth, calculated with the Neural Tangents API (Novak et al.,
2020). We compute the kernel on a subset of 10, 000 randomly sampled MNIST images and es-
timate the power law exponents for the kernel and task spectra λk and vk2 . Across architectures,
the task spectra vk2 are highly similar, but that the kernel eigenvalues λk decay more slowly for
deeper models, corresponding to a smaller exponent b. As a consequence, deeper neural network
models train more quickly during stochastic gradient descent as we show in Figure 5 (c). After
fitting power laws to the spectra λk 〜k-b and the task power Oik 〜k-a, We compared the true
test loss dynamics (color) for a width-500 neural network model with the predicted power-law scal-
ings β = a-1 from the fit exponents a, b. The predicted scalings from NTK regression accurately
describe trained width-500 networks. On CIFAR-10, we compare the scalings of the CNN model
and a standard MLP and find that the CNN obtains a better exponent due to its faster decaying tail
sum Pn∞=k λnvnk. We stress that the exponents β were estimated from our one-pass theory, but were
utilized experiments on a finite training set. This approximate and convenient version of our theory
is quite accurate across these varying models, in line with recent conjectures about early training
dynamics (Nakkiran et al., 2021).
6	Conclusion
Studying a simple model of SGD, we were able to uncover how the feature geometry governs the
dynamics of the test loss. We derived average learning curves hLti for both Gaussian and general
features and showed conditions under which the Gaussian approximation is accurate. The proposed
model allowed us to explore the role of the data distribution and neural network architecture on
the learning curves, and choice of hyperparameters on realistic learning problems. While our theory
accurately describes networks in the lazy training regime, average case learning curves in the feature
learning regime would be interesting future extension. Further extensions of this work could be used
to calculate the expected loss throughout curriculum learning where the data distribution evolves
over time as well as alternative optimization strategies such as SGD with momentum.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
The code to reproduce the experimental components of this paper can be found here https://
github.com/Pehlevan-Group/sgd_structured_features, which contains jupyter
notebook files which we ran in Google Colab. More details about the experiments can be found in
Appendix J. Generally, detailed derivations of our theoretical results are provided in the Appendix.
Acknowledgements
We thank the Harvard Data Science Initiative and Harvard Dean’s Competitive Fund for Promis-
ing Scholarship for their support. We also thank Jacob Zavatone-Veth for useful discussions and
comments on this manuscript.
References
Andreas Anastasiou, Krishnakumar Balasubramanian, and Murat A. Erdogdu. Normal approxima-
tion for stochastic gradient descent via non-asymptotic rates of martingale clt. In Alina Beygelz-
imer and Daniel Hsu (eds.), Proceedings of the Thirty-Second Conference on Learning Theory,
volume 99 of Proceedings of Machine Learning Research, pp. 115-137, Phoenix, USA, 25-28
Jun 2019. PMLR. URL http://proceedings.mlr.press/v99/anastasiou19a.
html.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701, 2021.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Carl Bender and Steven Orszag. Advanced Mathematical Methods for Scientists and Engineers:
Asymptotic Methods and Perturbation Theory, volume 1. 01 1999. ISBN 978-1-4419-3187-0.
doi: 10.1007/978-1-4757-3069-2.
Raphael Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for
stochastic gradient descent under the noiseless linear model, 2020.
M Biehl and P Riegler. On-line learning with a perceptron. Europhysics Letters (EPL), 28(7):525-
530, dec 1994. doi: 10.1209/0295-5075/28/7/012. URL https://doi.org/10.1209/
0295-5075/28/7/012.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024-1034. PMLR, 2020.
Abdulkadir Canatar, B. Bordelon, and C. Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature Communications,
12:1-12, 2020.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Yuan Cao, Quanquan Gu, and Misha Belkin. Risk bounds for over-parameterized maximum margin
classification on sub-gaussian mixtures. In Thirty-Fifth Conference on Neural Information Pro-
cessing Systems, 2021. URL https://openreview.net/forum?id=ChWy1anEuow.
Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1-30, 2021.
K. L. Chung. On a Stochastic Approximation Method. The Annals of Mathematical Statistics, 25
(3):463 - 483, 1954. doi: 10.1214/aoms/1177728716. URL https://doi.org/10.1214/
aoms/1177728716.
10
Published as a conference paper at ICLR 2022
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-
sizes. The Annals of Statistics, 44(4):1363 - 1399, 2016. doi: 10.1214/15-AOS1391. URL
https://doi.org/10.1214/15-AOS1391.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger conver-
gence rates for least-squares regression. Journal of Machine Learning Research, 18, 02 2016.
John C. Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. The Annals of
Statistics, 49(1):21 -48, 2021. doi: 10.1214/19-AOS1831. URL https://doi.org/10.
1214/19-AOS1831.
A.	Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press,
2001. doi: 10.1017/CBO9781139164542.
Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares al-
gorithms. Journal of Machine Learning Research, 21(205):1-38, 2020. URL http://jmlr.
org/papers/v21/19-734.html.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In
Conference on Learning Theory, pp. 658-695. PMLR, 2015.
Nicolas Flammarion and Francis Bach. Stochastic composite least-squares regression with con-
vergence rate o(1/n). In Satyen Kale and Ohad Shamir (eds.), Proceedings of the 2017
Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research,
pp. 831-875. PMLR, 07-10 Jul 2017. URL https://proceedings.mlr.press/v65/
flammarion17a.html.
Sebastian Goldt, MadhU Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova. Dy-
namics of stochastic gradient descent for two-layer neural networks in the teacher-student
setup. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volUme 32. CUrran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
cab070d53bd0d200746fb852a922064a- Paper.pdf.
Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. Modeling the influence
of data strUctUre on learning in neUral networks: The hidden manifold model. Phys. Rev. X, 10:
041044, Dec 2020. doi: 10.1103/PhysRevX.10.041044. URL https://link.aps.org/
doi/10.1103/PhysRevX.10.041044.
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zde-
borova´. The gaussian equivalence of generative models for learning with shallow neural networks.
Proceedings of Machine Learning Research, 145:1-46, 2021.
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In
International Conference on Machine Learning, pp. 3964-3975. PMLR, 2021.
Tom M. Heskes and Bert Kappen. Learning processes in neural networks. Phys. Rev. A, 44:2718-
2726, Aug 1991. doi: 10.1103/PhysRevA.44.2718. URL https://link.aps.org/doi/
10.1103/PhysRevA.44.2718.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically, 2017.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel.
Kernel alignment risk estimator: Risk prediction from training data. In NeurIPS,
2020.	URL https://proceedings.neurips.cc/paper/2020/hash/
b367e525a7e574817c19ad24b7b35607-Abstract.html.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-
ing stochastic gradient descent for least squares regression. In Conference On Learning Theory,
pp. 545-604. PMLR, 2018.
11
Published as a conference paper at ICLR 2022
Yann LeCun, Ido Kanter, and Sara A. Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Phys. Rev. Lett., 66:2396-2399, May 1991. doi: 10.1103/PhysRevLett.
66.2396. URL https://link.aps.org/doi/10.1103/PhysRevLett.66.2396.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):
124002, Dec 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc62b. URL http://dx.
doi.org/10.1088/1742-5468/abc62b.
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and
Lenka Zdeborova. Capturing the learning curves of generic features maps for realistic data sets
with a teacher-student model, 2021.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. In ICML, pp. 3331-3340, 2018. URL
http://proceedings.mlr.press/v80/ma18a.html.
C. Mace and A. Coolen. Statistical mechanical analysis of the dynamics of learning in perceptrons.
Statistics and Computing, 8:55-88, 1998.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve, 2020.
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova. Dynamical
mean-field theory for stochastic gradient descent in gaussian mixture classification, 2020.
Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborova. Stochasticity helps to navigate
rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem,
2021.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. In International Conference on Learning Represen-
tations, 2021. URL https://openreview.net/forum?id=guetrIHLFGI.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic gra-
dient descent on hard learning problems through multiple passes, 2018.
B.	Polyak and A. Juditsky. Acceleration of stochastic approximation by averaging. Siam Journal on
Control and Optimization, 30:838-855, 1992.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimension of images and its impact on learning. In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=XJk19XzGq2J.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/
paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning
(Adaptive Computation and Machine Learning). The MIT Press, 2005.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Math-
ematical Statistics, 22(3):400 - 407, 1951. doi: 10.1214/aoms/1177729586. URL https:
//doi.org/10.1214/aoms/1177729586.
David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. 02 1988.
12
Published as a conference paper at ICLR 2022
David Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural
networks. 04 1999.
Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv
preprint arXiv:1811.03600, 2018.
Alexander Shapiro. Asymptotic Properties of Statistical Estimators in Stochastic Programming.
TheAnnalsofStatistics, 17(2):841 -858, 1989. doi: 10.1214∕aos∕1176347146. URL https:
//doi.org/10.1214/aos/1176347146.
Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher-student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, Dec 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc61d.
URL http://dx.doi.org/10.1088/1742-5468/abc61d.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd for
least-squares in the interpolation regime, 2021.
Maksim Velikanov and Dmitry Yarotsky. Universal scaling laws in the gradient descent training of
neural networks, 2021.
Justin Werfel, Xiaohui Xie, and H. Seung. Learning curves for stochastic gradient
descent in linear feedforward networks. In S. Thrun, L. Saul, and B. Scholkopf
(eds.), Advances in Neural Information Processing Systems, volume 16. MIT Press,
2004.	URL https://proceedings.neurips.cc/paper/2003/file/
f8b932c70d0b2e6bf071729a4fa68dfc- Paper.pdf.
Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Found. Comput.
Math., 8(5):561-596, October 2008. ISSN 1615-3375.
Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A. Erdogdu. An analysis
of constant step size sgd in the non-convex regime: Asymptotic normality and bias, 2020.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. Advances in neural information processing systems, 32:8196-8207,
2019.
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign over-
fitting of constant-stepsize sgd for linear regression. arXiv preprint arXiv:2103.12692, 2021.
13
Published as a conference paper at ICLR 2022
A	Decomposition of the Features and Target Function
Let y(x) be a square integrable target function with y(x)2 < ∞. Define the following integral
operator TK for kernel K(x, x0) = ψ(x) ∙ ψ(x0):
TK [φ](x0) =	p(x)K(x, x0)φ(x)dx	(11)
We are interested in eigenfunctions of this operator, function φk for which TK [φk] = λkφk . For
kernels with finite trace K(x, x)p(x)dx < ∞, Mercer’s theorem (Rasmussen & Williams, 2005)
guarantees the existence of a set of orthonormal eigenfunctions. Since ψ(x) spans an N dimensional
function space, only N of the kernel eigenfunctions will have non-zero eigenvalue. Since the basis
of kernel eigenfunctions (including the zero eigenvalue functions) is complete over the space of
square integrable functions. After ordering the eigenvalues λι > λ2 > ... > Xn with Xn+` = 0,
we obtain the expansion
y(x) =	hy(x)φk(x)ixφk(x) =	vkφk(x) +y⊥(x) , y⊥(x) =	hy(x)φk(x)i φk(x)
k	k≤N	k>N
(12)
Further, We can decompose the feature map in this basis ψ(x) = PN=I √λkUkφk(x). We
recognize through these decompositions the coefficients vk can be computed uniquely as vk =
Xk-1/2Uk> hψ (x)y(x)i. This provides a recipe for determining the necessary spectral quantities for
our theory. We see that the feature map’s decomposition above reveals that Xk are also the eigenval-
ues of the feature correlation matrix Σ since
∑ = (ψ(x)ψ(x)>> = χPλkλ'Uu> hφk(x)φ'(x)i = XλkUku>.	(13)
A. 1 Finite Sample Spaces
When we discuss experiments on MNIST or CIFAR, we use this technology for an atomic data
distribution P(X) = M PjM=I δ(x - x0). Plugging this into the integral operator gives
TK[φ](x) = M： X K(x, x")φ(x")	(14)
*
We see that, restricting the domain to the set of points {X1, ..., XM }, this amounnts to solving a
matrix eigenvalue problem MKφk = λkφk where K ∈ RM×m is the kernel gram matrix with
entries Kμν = K(xμ, xν) and φk has entries φk,μ = φk(xμ).
B Proof of Theorem 1
Let ∆t = Wt - W represent the difference between the current and optimal weights and define the
correlation matrix for this difference
Ct = ∆t∆t>Dt-1
(15)
Using stochastic gradient descent, wt+ι = Wt - ηgt with gradient vector gt = -m Pm=I ψiψ>∆t,
the Ct matrix satisfies the recursion
Ct+1 =	(∆t -	ηgt)(∆t -	ηgt)>Dt	= Ct - ηgt∆t>	-η∆tgt>	+η2	gtgt>	.	(16)
First, note that since ψi are all independently sampled at timestep t, we can break up the average
into the fresh batch of m samples and an average over Dt-1
m
hgA>Dt = m X <ψiψ>>ψi(△△>〉Dt-i = ςc~	(17)
i=1
14
Published as a conference paper at ICLR 2022
The last term requires computation of fourth moments
<gtg>>=m X(我矽> 3δ>〉d_i ψj ψ>Eψi, ψj	(18)
i,j
=m X <ψiψ> Ct ψj ψ> 履也.	(19)
i,j
First, consider the case where i = j . Letting ψ = ψi , we need to compute terms of the form
E Ck,' hΨjΨk Ψ'Ψni .	(20)
k,`
For Gaussian random vectors, we resort to Wick-Isserlis theorem for the fourth moment
hΨj Ψk ΨlΨn i = hΨj Ψk ihΨ'Ψni + hΨj Ψ'ihΨk Ψni + hΨj ΨnihΨ'Ψk i	(21)
giving
〈gtg>〉= m+1 ∑Ct∑ + ɪ∑ Tr (∑Ct).	(22)
mm
This correlation structure for gt implies that its covariance has the form
hCovψ (gt)iD = —ςCtς +-ςTr(£Ct).	(23)
Dt	m	m
Using the formula for gtgt> , we arrive at the following recursion relation for Ct
Ct+1 = Ct- ηCt∑ - η∑Ct + η2m+1 ∑Ct∑ + ɪη2∑ Tr (∑Ct).	(24)
mm
Since we are ultimately interested in the generalization error hLti = ∆t> Σ∆t = TrΣCt =
Pk λkuk>Ctuk, it suffices to track the evolution of ct,k = uk>Ctuk
ct+1,k = (1 - 2ηλk + η2 -m-λk) ct,k + mη2λk SX λjct,j.	(25)
Vectorizing this equation for c generates the following solution
Ct = Atco , A = I - 2η diag(λ) + m + η2 diag(λ2) + — λλ>.	(26)
mm
The coefficient cο,k = Vk = (u>w*)2. To get the generalization error, We merely compute (L/ =
λ>at = λ> Atv2 as desired.
C Proof of Stability Conditions
We Will first establish the folloWing loWer bound on the loss, Where Without loss of generality We
assumed the maximum correlation is 1:
Lt ≥ λ>v2 ](1 - η)2 + η2∣λ∣2l .	(27)
m
We Will then use this loWer bound to provide necessary conditions on the learning rate and batch
size for stability of the loss evolution. First, note that the folloWing inequality holds elementWise
Aλ = (I - η diag(λ))2 + 匕diag(λ2) + λ λ ≥
2
(1 - η)2 + -∣λ∣2 λ
m
(28)
Repeating this inequality t times gives Atλ ≥
h(1 - η)2 + m∣λ∣2i λ. Using the fact that Lt
λ> Atv2 gives the desired inequality. Note that this inequality is very close to the true result for
15
Published as a conference paper at ICLR 2022
isotropic features λ = 1 which gives Lt 8
[(1 - η)2 + m (∣λ∣2
+1)
t
. For anisotropic features
with small learning rate, this bound becomes less tight. For the loss to converge to zero at large time,
the quantity in brackets must necessarily be less than one. This implies the following necessary
condition on the batchsize and learning rate
η<
2m
m + ∣λ∣2
m > mmin
η∣λ∣2
2 - η
(29)
^⇒
where mmin is the minimal batch size for learning rate η and feature covariance eigenvalues λ.
C.1 Heuristic Batch Size and Learning Rate
We can derive heuristic optimal choices of the learning rate and batch size hyperparameters η, m at
a fixed compute budget which optimize the lower bound derived above.
C.1.1 Fixed Learning Rate
We will first consider optimizing only the batch size at a fixed learning rate η before discussing
the optimal m when η is chosen optimally. The loss at a fixed compute budget C = tm is lower
bounded by
2 C/m
LC/m ≥ λ>v2 (1-η)2 + m∣λ∣2
(30)
For the purposes of optimization, we introduce x = 1/m and consider optimizing
f(x) = xln [A + Bx] , A = (1 - η)2, B = η2 ∣λ∣2	(31)
The first order optimality condition f(x) = 0 implies that ln(A + Bx) + ABBx = 0. Letting Z =
A+ Bx, this is equivalent to z ln z + z - A = 0. This equation has solutions for all valid A ∈ (0, 1)
giving solutions z ∈ (e-1, 1). Letting z(η), represent the solution to z + z ln z - (1 - η)2 = 0, the
optimal batchsize has the form
*，、 B	η2∣λ∣2	仲
m (η)	=	, .,---= =	-ʃʒ---7----ςττ	(32)
z(A) -	A	z(η) - (1 - η)2
We	can	gain	intuition for	this result	by considering	the limit of η →	0	and	η	→	1.	First,	in the
η →	0 limit,	We find	that	Z	〜A2+1	so m* 〜	2吃	=2mm,in, making	contact	with the stability
bound derived in Appendix Section C. Thus for small learning rates, this heuristic optimum suggests
doubling the minimal stable batchsize for optimal convergence. At large learning rates, η 〜1 with
A 〜0, we find Z 〜e-1 so m* (η)〜eη2 ∣λ∣2. Thus for small η, we expect m* to scale linearly with
η while for large η, we expect a scaling of the form η2. In either case, the optimal batchsize scales
with feature eigenvalues through the sum of the squares ∣λ∣2 = Pk λ2k .
C.1.2 Heuristic Optimal Learning Rate and Batch Size
We will now examine what happens when one first optimizes loss bounmd with respect to the the
learning rate at any value of m and then subsequently optimizes over the batch size m. We can
easily find the η which minimizes the lower bound.
J(I - η)2 + η ∣λ∣	=0 =⇒ η* = --m-j2	(33)
∂η	m	m + ∣λ∣2
Note that this heuristic optimal learning rate is very close to the true optimum in the isotropic data
setting ηtrue = m占+1 ≈ mmN. Plugging this back into the loss bound, we find that at fixed
compute C = tm, the loss scales like
Lc/m ≥ λ>v2 [大「m	(34)
m + ∣λ∣2
With the optimal choice of learning rate, the loss at fixed compute monotonically increases with
batch size, giving an optimal batchsize of m = 1. This shows that if the learning rate is chosen
optimally then small batch sizes give the best performance per unit of computation. This corresponds
to the hyperparameter choices (η, m) = ( y+⅛, 1).
16
Published as a conference paper at ICLR 2022
D INCREASING m REDUCES THE LOSS AT FIXED t
We will show that for a fixed number of steps t, increasing the minibatch size m can only decrease
the expected error. To do this, we will simply show that the derivative of the expected loss with
respect to m,
∂ hLt = λ> ∂A v2,
∂m	∂m
(35)
is always non-positive. The derivative of the t-th power of A can be identified with the chain rule
∂At -	= ∂m Note that the matrix	=IAAtT + A IAAt-2 + A2 IAAt-3 + ... + At-1 IA.	(36) ∂m	∂m	∂m	∂m 1A = — N [diag(λ2) + λλτ]	(37) I m	m2
has all non-positive entries. Thus we find that
	卡=X λTAn IAAt-….	(38) n=0
Note that since all entries in vk2 and At-n-1 are non-negative, the vector zn = At-n-1v2 has
non-negative entries. By the same argument, the vector qn = Anλ is also non-negative in each
entry. Therefore, each of the terms in d∂Lmi above must be non-positive
a ∂m=x z> ∂m qn=—ηm XX zn,k 麻为&+，k"] q” ≤ 0.	(39)
n=0	n k,`
Thus We find d∂mti ≤ 0, implying that optimal hLt is always obtained (possibly non-UniqUely) at
m→ ∞.
E INCREASING m INCREASES THE LOSS AT FIXED C = tm ON ISOTROPIC
Features
Unlike the previous section, which considered fixed t and varying m, in this section we consider
fixing the total number of samples (or gradient evaluations) which we call the compute budget
C = tm. For a fixed compute budget C = tm, and unstructured N dimensional Gaussian features
and optimal learning rate η* = m二+],we have
〈Lc/m〉= ( mNN⅛τ )c1w*ll2.	(40)
Taking a derivative with respect to the batch size we get
∂ 1 /T \_ ∂ C ( N +1	ʌ
- log〈LC/m)= ------log ----INIl
∂m	∂mm	m+ N + 1
C	m+ N + 1
logI N +1
C
)+ m(m + N +1) > 0.	(41)
This exercise demonstrates that, for the isotropic features, smaller batch-sizes are preferred at a fixed
compute budget C. This result does not hold for arbitrary spectra λk. In the general case, optimal
minibatch sizes can exist as we show in Figure 1 (e)-(f) for power law and MNIST spectra.
F Proof of Theorem 2
Let Vec(∙) denote a flattening ofan N X N matrix into a vector of length N2 and let Mat(∙) represent
a flattening ofa4D tensor into a N2 × N2 two-dimensional matrix. We will show that the expected
loss (over Dt) is
hLti = X λkct,kk , ct = (G + mMat(K)) Vec(VvT) ∈ RN	(42)
17
Published as a conference paper at ICLR 2022
where [G]ij,kl = δikδj(1 - η(λi + λj) +，『%1 λiλ, and [v]k = Uk ∙ w*.
We rotate all of the feature vectors into the eigenbasis of the covariance, generating diagonalized
features φk = Uk>ψ and introduce the following fourth moment tensor
Kijkl = hφiφjφkφ'i .	(43)
We redefine Ct in the appropriate (rotated) basis by projecting onto the eigenvectors of the covari-
ance
Ct = U> ∆t∆t>U,	(44)
where U = [U1, U2, ..., UN]. With this definition, C’s dynamics take the form
Ct+1 = Ct- ΛCt - CtΛ + η2(m- 1) ΛCtΛ + (φφ>Ctφφ> .	(45)
The elements of the matrix can be expressed with the fourth moment tensor
hφφ hφiφjφkφ'i Ck' = ɪ2 KijklCk,'.	(46)
k`	k`
We thus generate the following dynamics for Citj
Ct+1= (1-η(λi + λj) + η2(m- 1)λiλj) Ctj + η2 XKijklCll.	(47)
m	mkl
Let ct = Vec(Ct), then we have
ct+1 = (GO + m Mat(K)) ct , [G0]ij,k' = δik δj'
1-η(% + λj) + 吧「网
(48)
Solving these dynamics for c, recognizing that c0 = Vec(vv> ), and computing hLti = TrΣCt
Pk Ckkλk gives the desired result.
F.1 Proof of Theorem 3
Suppose that the features satisfy the regularity condition
ψψ>Gψψ>	(α+ 1)ΣGΣ + αΣTr (ΣG)	(49)
Recalling the recursion relation for Ct
22
Ct+1 = Ct- η∑Ct- ηCt∑ + η2 —2-∑Ct∑ + — <ψψ>CtΨΨ>>
m2	m
m2 m	η2
W Ct- η∑Ct - ηCt∑ + η2	-2	∑Ct∑ + — [(α + 1)ΣCtΣ + αΣTrCt∑]
=(I - η∑)Ct(i - η∑) + αη2 [∑Ct∑ + ∑τr∑Ct]
m
(50)
(51)
Defining that ck,t = u>CtUk, We note ct+1 ≤ ((I — η diag(λ))2 + 空[diag(λ2) + λλ>]) ct.
Using the fact that Lt = ct>λ, we find
Lt ≤ λ> ((I - η diag(λ))2 + --- [diag(λ2) + λλ>]) v2	(52)
which proves the desired bound.
G Power Law Scalings in Small Learning Rate Limit
By either taking a small learning rate η or a large batch size, the test loss dynamics reduce to the test
loss obtained from gradient descent on the population loss. In this section, we consider the small
learning rate limit η → 0, where the average test loss follows
∞
hLti- X λkv2(1- ηλk)2t.	(53)
k=1
18
Published as a conference paper at ICLR 2022
(a) Non-Gaussian Effects on MNIST
(b) Non-Gaussian Effects on CIFAR
Figure F.1: Non-Gaussian effects are small on random feature models. (a)-(b) The first 20-
dimensions of the summed fourth moment matrix κi4j = ui> ψψ>ψψ> uj are plotted for the
Gaussian approximation and the empirical fourth moment. Differences between the Gaussian ap-
proximation and true fourth moment matrices on this example are visible, but are only on the order
of 〜5% of the size of the entries in κ4.
Under the assumption that the eigenvalue and target function power spectra both follow power laws
λk〜 k-b and vk2λk 〜k-a, the loss can be approximated by an integral over all modes k
hLti = ^X k-a(1 — ηk-b)2t 〜/ exp (2ηln(1 — ηk-b)t — aln k) dk
k1
〜/ exp (—2ηηk-bt — a ln k) dk , η → 0
(54)
(55)
We identify the function f (k) = 2ηk-b + 1 ln k and proceed with Laplace,s method (Bender &
Orszag, 1999). This consists of Taylor expanding f(k) around its minimum to second order and
computing a Gaussian integral
Z exp(-tf (k))dk 〜Z exp Jf(k*) — gf"(k*)(k — k*)2)〜exp(-tf(k*)) JfnB ∙
(56)
We must identify the k* which minimizes f (k) The interpretation of this value is that it indexes the
mode which dominates the error at a large time t. The first order condition gives
f0(k) = —2bηk-b-1 + -a- = 0 =⇒ k* = (2bηt∕a)∣/ .	(57)
tk
The second derivative has the form
f00(k*) = 2b2ηk-b-2 —与 |k* = 2b2η (2bnt/a)-(b+2)/b — 1 (2bnt/a)-2/b 〜t-1-2∕b.	(58)
tk2	t
Thus we are left with a scaling of the form
hLti 〜exp(—a/blnt)t1/b 〜t-~~-~.
(59)
H Proof of Theorem 4
Let y⊥2 = σ2 and hy⊥i = 0, hy⊥ψi = 0. The gradient descent updates take the following form
∆t+1 = ∆t — ηgt with
1m
gt = X ψi [ψ>∆t + y⊥,i] .	(60)
mi=1
Again, defining Ct = ∆t∆t> we perform the average over each of the ψi	vectors to obtain the
following recursion relation
Ct+ι =〈△△> > — η 3tg> > — η(gA> > + η2(gtg> >
m+1	1
=Ct — η∑Ct — ηCt∑ + -+- ∑Ct∑ + 而 ∑Tr (Ct∑) + η2σ2Σ.	(61)
Again, analyzing ct,k = uk> Ctuk we find
ct+ι,k = (1 — 2ηλk + η2 ———λk) ct,k +—SXJλ'ct,' + η2σ2λk.	(62)
-	-`
19
Published as a conference paper at ICLR 2022
The vector ct follows the linear evolution
ct+1 = Act + η2σ2λ.	(63)
Let b = η2σ2λ. Writing out the first few steps, we identify a pattern
c1 = Ac0 + b
c2 = Ac1 + b = A2c0 + Ab + b
c3 =Ac2+b= A3c0+A2b+Ab+b
ct = Atc0 + Xt-1 An b.	(64)
n=0
The geometric sum Ptn-=10 An can be computed exactly under the assumption that (I - A) is
invertible which holds provided all of A’s eigenvalues are less than unity, which necessarily holds
provided the system is stable. The geometric sum yields
(X An) =(I- A)-1 (I - At).	(65)
n=0
Recalling the definition of b = σ2η2λ and the definition of the average loss hLti = λ>ct, we have
hLti = σ2 + λ>Atco + η2σ2λ>(I - A)T(I - At) λ.	(66)
Recognizing c0 = v2 gives the desired result.
I Proof of theorem 5
We will now prove that if the training P(X) and test distributions p(x) are different and have feature
correlation matrices Σ and Σ respectively, then the average training and test losses have the form
Ltrain = Tr [∑Ct] LteSt = Tr [∑Ct].	(67)
As before, we will assume that there exist weights W* which satisfy y = W ∙ψ. We start by noticing
that the update rule for gradient descent
1m
Wt = Wt - ηgt , gt = 一 E ψt,μψ>μ [wt - W*]	(68)
m
μ=1
generates the following dynamics for the weight discrepancy correlation Ct
(Wt - W*)(Wt - W*)>D .
_	_	∙^∙	_	_	∙^∙	C
ct+ι = Ct - η∑ct- ηCt∑ + η2
(69)
This formula can be obtained through the simple averaging procedure shown in B. Under the Gaus-
sian approximation, we can obtain a simplification
Ct+1 = (I - η∑)Ct(I - η∑) + η2 [ΣCtΣ + ΣTrΣCt]	(70)
m
We can solve for the evolution of the diagonal and off-diagonal entries in this matrix giving
u>CtUk =	[Atv2]	k ,	u>CtU'	=(1 -	η黑-ηλ'	+ n2(1 +	m1 )	λkλ')	Vkv`
(71)
20
Published as a conference paper at ICLR 2022
To calculate the training and test error, we have
hLtesti = <(Ψ(x) ∙ Wt — Ψ(x) ∙ w*)2〉x*x),Dt =h(wt — w*)Σ (Wt — w*)〉= TrΣCt.
hLtraini = ((ψ(x) Nt- ψ(x) ∙ w*)2〉X〜p(χ),Dt = D(Wt- w*)£ (Wt- w*)) = Tr七5.(72)
Note that in the training error formula, since Σ has eigenvectors Uk only the diagonal terms u> CtUk
enter into the formula for Ltrain, but off-diagonal components u>Ctu` do enter into the formula
for Ltest
J	Experimental Details
For Figures 3, we use the last two classes of MNIST and CIFAR-10. We encode the target values
as binary y ∈ {+1, -1}. For Figure 5, we use 6000 random training points drawn from entire
MNIST and CIFAR-10 datasets to calculate the spectrum of the Fisher information matrix. We train
with SGD on these training data, using one-hot label vectors for each training example and plot the
error on the test set. We train our models on a Google Colab GPU and include code to reproduce
all experimental results in the supplementary materials. To match our theory, we use fixed learning
rate SGD. Both evaluation of the infinite width kernels and training were performed with the Neural
Tangents API (Novak et al., 2020).
21