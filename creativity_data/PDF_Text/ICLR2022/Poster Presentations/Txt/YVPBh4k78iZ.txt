Published as a conference paper at ICLR 2022
Scale Mixtures of Neural Network Gaussian
Processes
Hyungi Lee1, Eunggu Yun1, Hongseok Yang1,2,3, Juho Lee1,4
1 Kim Jaechul Graduate School of AI, KAIST, South Korea
2School of Computing, KAIST, South Korea
3Discrete Mathematics Group, Institute for Basic Science (IBS), Daejeon, South Korea
4AITRICS, Seoul, South Korea
{lhk2708, eunggu.yun, hongseok.yang, juholee}@kaist.ac.kr
Ab stract
Recent works have revealed that infinitely-wide feed-forward or recurrent neu-
ral networks of any architecture correspond to Gaussian processes referred to as
Neural Network Gaussian Processes (nngps). While these works have extended
the class of neural networks converging to Gaussian processes significantly, how-
ever, there has been little focus on broadening the class of stochastic processes
that such neural networks converge to. In this work, inspired by the scale mixture
of Gaussian random variables, we propose the scale mixture of nngps for which
we introduce a prior distribution on the scale of the last-layer parameters. We
show that simply introducing a scale prior on the last-layer parameters can turn
infinitely-wide neural networks of any architecture into a richer class of stochas-
tic processes. With certain scale priors, we obtain heavy-tailed stochastic pro-
cesses, and in the case of inverse gamma priors, we recover Student’s t processes.
We further analyze the distributions of the neural networks initialized with our
prior setting and trained with gradient descents and obtain similar results as for
nngps. We present a practical posterior-inference algorithm for the scale mixture
of nngps and empirically demonstrate its usefulness on regression and classifi-
cation tasks. In particular, we show that in both tasks, the heavy-tailed stochastic
processes obtained from our framework are robust to out-of-distribution data.
1	Introduction
There has been growing interest in the literature on the equivalence between wide deep neural
networks and Gaussian Processes (gps). Neal (1996) showed that a shallow but infinitely-wide
Bayesian Neural Network (bnn) with random weights and biases corresponds to a gp. This result
was extended to fully-connected deep neural networks of any depth (Lee et al., 2018; Matthews et al.,
2018), which are shown to converge to gps as the width grows. Similar results were later obtained
for deep Convolutional Neural Networks (cnns) (Novak et al., 2018; Garriga-Alonso et al., 2019)
and attention networks (Hron et al., 2020). In fact, Yang (2019) showed that wide feed-forward or
recurrent neural networks of any architecture converge to GPs and presented a generic method for
computing kernels for such gps. Under this correspondence, the posterior inference of an infinitely-
wide bnn boils down to the posterior inference of the corresponding gp called nngp for which a
closed-form posterior can be computed exactly.
Our goal is to advance this line of research by going beyond gps. We present a simple yet flexible
recipe for constructing infinitely-wide bnns that correspond to a wide range of stochastic processes.
Our construction includes heavy-tailed stochastic processes such as Student’s t processes which
have been demonstrated to be more robust than gps under certain scenarios (Shah et al., 2014).
Our construction is inspired by a popular class of distributions called scale mixtures of Gaus-
sians (Andrews & Mallows, 1974); such a distribution is obtained by putting a prior on the scale
or variance parameter of a Gaussian distribution. We extend this scale mixing to nngps, where we
introduce a prior distribution on the scale of the parameters for the last layer (which is often re-
ferred to as readout layer) in a wide neural network. We show that simply introducing a scale prior
1
Published as a conference paper at ICLR 2022
on the last layer can turn infinitely-wide bnns of any architecture into a richer class of stochastic
processes, which we name as scale mixtures of NNGPs. These scale mixtures include a broad class
of stochastic processes, such as heavy-tailed processes which are shown to be robust to outliers.
In particular, when the prior on the scale is inverse gamma, the scale mixture of nngps becomes
Stduent’s t process (Shah et al., 2014).
We demonstrate that, despite increasing flexibility, mixing nngps with a prior on the scale parameter
does not increase the difficulty of posterior inference much or at all in some cases. When we mix the
scale parameter with inverse gamma (so that the mixture becomes a t process), we can compute the
kernel efficiently and infer the exact posterior, as in the case of nngps. For generic scale priors and
regression tasks with them, we present an efficient approximate posterior-inference algorithm based
on importance sampling, which saves computation time by reusing the shared covariance kernels.
For classification tasks with categorical likelihood (for which an exact posterior is not available even
for the original nngp), we present an efficient stochastic variational inference algorithm.
We further analyze the distributions of the neural networks initialized with our prior setting and
trained with gradient descents and obtain results similar to the ones for nngps. For nngp, it has
been shown (Matthews et al., 2017; Lee et al., 2019) that when a wide neural network is initialized
with the nngp specification and then trained only for the last layer (with all the other layers fixed),
its fully trained version becomes a random function drawn from the nngp posterior. Similarly,
we analyze the distribution of a wide neural network of any architecture initialized with our prior
specification and trained only for the last layer, and show that it becomes a sample from a scale
mixture of nngps with some scaling distribution. Interestingly, the scaling distribution is a prior,
not a posterior. For the fully-connected wide neural networks, we extend the analysis to the case
where all the layers are trained with gradient descent, and show that the limiting distribution is again
a scale mixture of gps with a prior scaling distribution and each gps using a kernel called Neural
Tangent Kernel (ntk). In the case of an inverse gamma prior, the limiting distribution becomes
Student’s t process, which can be computed analytically.
We empirically show the usefulness of our construction on various real-world regression and classi-
fication tasks. We demonstrate that, despite the increased flexibility, the scale mixture of nngps is
readily applicable to most of the problems where nngps are used, without increasing the difficulty
of inference. Moreover, the heavy-tailed processes derived from our construction are shown to be
more robust than nngps for out-of-distribution or corrupted data while maintaining similar perfor-
mance for the normal data. Our empirical analysis suggests that our construction is not merely a
theoretical extension of the existing framework, but also provides a practical alternative to nngps.
1.1	Related Works
nngp and ntk Our construction heavily depends on the tensor-program framework (Yang, 2019)
which showed that a wide bnn of any feed-forward or recurrent architecture converges in distribu-
tion to an nngp, as its width increases. Especially, we make use of the so-called master theorem
and its consequence (reviewed in Section 1.1) to derive our results. Building on the seminal work
on ntk (Jacot et al., 2018), Lee et al. (2019) analyzed the dynamics of fully-connected neural net-
works trained with gradient descent and showed that fully-trained infinitely-wide networks are gps.
In particular, they confirmed the result in Matthews et al. (2017) that when a fully-connected neu-
ral network is initialized from a specific prior (often referred to as the ntk parameterization) and
trained only for the last layer under gradient descent and squared loss, its fully-trained version be-
comes a posterior sample of the corresponding nngp. Lee et al. (2019) further analyzed the case
where all the layers are trained with gradient descent and showed that the network also converges to
a gp with specific parameters computed with the so called ntk kernel. We extend these results to
our scale mixture of nngps in Section 3.
Heavy-tailed stochastic processes from infinitely-wide bnns The attempts to extend the results
on nngps to heavy-tailed processes have been made in the past, although not common. The rep-
resentative case is the work by Favaro et al. (2020), which showed that under an alternative prior
specification, a wide fully-connected neural network converges to stable processes, as the widths in-
crease. This result was later extended to deep cnns in (Bracale et al., 2021). What distinguishes our
approach from these works is the simplicity of our construction; it simply puts a prior distribution
on the scale of the last-layer parameters of a network and makes the scale a random variable, while
2
Published as a conference paper at ICLR 2022
the constructions in those works replace priors for entire network parameters from Gaussian to other
distributions. This simplicity has multiple benefits. First, most of the nice properties of nngps,
such as easy-to-compute posteriors and the correspondence to gradient descent training, continue
to hold in our approach as a version for mixture distributions, while it is at least difficult to find
similar adjustments of these results in those works. Second, our approach is applicable to arbitrary
architectures, while those works considered only fully-connected networks and cnns.
2	Preliminaries
We start with a quick review on a few key concepts used in our results. For M ∈ N, let [M] be the
set {1, . . . , M}. Also, write R+ for the set {x ∈ R | x > 0}.
2.1	Tensor Programs and Neural Network Gaussian Processes
Tensor programs (Yang, 2019) are a particular kind of straight-line programs that express compu-
tations of neural networks on fixed inputs. In a sense, they are similar to computation graphs used
by autodiff packages, such as TensorFlow and PyTorch, but differ from computation graphs in two
important aspects. First, a single tensor program can express the computations of infinitely-many
neural networks, which share the same architecture but have different widths. This capability comes
from the parameter n of the tensor program, which determines the dimensions of vectors and ma-
trices used in the program. The parameter corresponds to the width of the hidden layers of a neural
network, so that changing n makes the same tensor program model multiple neural networks of
different widths. Second, tensor programs describe stochastic computations, where stochasticity
comes from the random initialization of network weights and biases. These two points mean that
we can use a single tensor program to model the sequence of bnns of the same architecture but with
increasing widths, and understand the limit of the sequence by analyzing the program. The syntax
of and other details about tensor programs are given in Appendix A.
We use tensor programs because of the so called master theorem, which provides a method for
computing the infinite-width limits of bnn sequences expressed by tensor programs. For the precise
formulation of the theorem and the method, again see Appendix A.
To explain the master theorem more precisely, assume that we are given a family of BNNs {fn}n∈N
that are indexed by their widths n and have the following form:
fn(-; Vn, Ψn) ： RI → R,
fn(X； Vn, Ψn) = √^ X Vn,α ∙ Φ(gn(x; Ψn))α,
n α∈[n]
where vn ∈ Rn and Ψn ∈ Rn×P (for some fixed P) are the parameters of the n-th network, the
former being the ones of the readout layer and the latter all the other parameters, I is the dimension
of the inputs, φ is a non-linear activation function of the network, and gn (x; Ψn) is the output of
a penultimate linear layer, which feeds directly to the readout layer after being transformed by the
activation function. In order for the computations of these bnns on some inputs to be represented
by a tensor program, the components of the networks have to satisfy at least the following two
conditions. First, the entries of vn and Ψn are initialized with samples drawn independently from
(possibly different) zero-mean Gaussian distributions. For the entries of vn , the distributions are the
same and have the variance σv2 for some σv > 0. For the entries of Ψn , the distributions may be
different, but if we restrict them to those in each column j of Ψn, they become the same and have
the variance σj/n for some σ7- > 0. Second, φ(z) is controlled, i.e., it is bounded by a function
exp(C|z|2- + c) for some C, , c > 0. This property ensures that φ(z) for any Gaussian variable z
has a finite expectation.
Let x1 , . . . , xM ∈ RI be M inputs. When the computations of the BNNs on these inputs are
represented by a single tensor program, the master theorem holds. It says that there is a general
method for computing μ ∈ RM and Σ ∈ RM×M inductively on the syntax of the program such that
μ and Σ characterize the limit of (gn(xι; Ψn),..., gn(xM; Ψn))n∈N in the following sense.
Theorem 2.1 (Master Theorem). Let h : RM → R be a controlled function. Then, as n tends to ∞,
1
n
n
^X h(gn(x1; ψn)α, . . . , gn(XM; ψn)α^ -→ EZ~N(μ,Σ) [h(Z)] ,
α=1
(1)
3
Published as a conference paper at ICLR 2022
where -a-.s→. refers to almost sure convergence.
The next corollary describes an important consequence of the theorem.
Corollary 2.2. As n tends to ∞, the joint distribution of (fn (xm; vn, Ψn))m∈[M] converges weakly
to the multivariate Gaussian distribution with zero mean and the following covariance matrix K ∈
RM ×M :
K(i,j) = σV ∙ EZ〜N(μ,∑) [φ(Zi)φ(Zj)]	⑵
where Zi and Zj are the i-th and j -th components of Z.
The above corollary and the fact that K depend only on the shared architecture of the BNNs, not on
the inputs x1:M, imply that the BNNs converge to a GP whose mean is the constant zero function and
whose kernel is
κ(x, x0) = σV ∙ EZ〜N(μ,∑) [φ(Z1)φ(Z2)]	⑶
where (μ, Σ) is constructed as described above for the case that M = 2 and (xι, x2) = (x, x0).
This GP is called Neural Network Gaussian Process (NNGP) in the literature.
2.2	STUDENT’ S t PROCESSES
Student’s t processes feature prominently in our results to be presented shortly, because they are
marginals of the scale mixtures of nngps when the mixing is done by inverse gamma. These pro-
cesses are defined in terms of multivariate Student’s t distributions defined as follows.
Definition 2.1. A distribution on Rd is a multivariate (Student’s) t distribution with degree of free-
dom V ∈ R+, location μ ∈ Rd and positive-definite scale matrix Σ ∈ Rd×d if it has the following
density:
P(X)=	G((V …1
(νπ)2 ∙ G(ν∕2) ∙∣Σ∣ 2
1 + 工(X — μ)>Σ-1(x — μ)
ν + d
~H~
where G(∙) is the gamma function. To express a random variable drawn from this distribution, We
write X 〜MVTd(ν, μ, ∑).
When v > 1, the mean of the distribution MVTd(V,μ, Σ) exists, and it is μ. When ν > 2, the
covariance of distribution also exists, and it is VV2Σ. As for the Gaussian distribution, the multi-
variate t distribution is also closed under marginalization and conditioning. More importantly, it can
be obatined as a scale mixture of Gaussians; if σ2 〜InvGamma(a, b) and x∣σ2 〜 N(μ, σ2Σ),
then the marginal distribution of X is MVT(2a, μ, bΣ). For the definition of the inverse gamma
distribution, see Definition B.1.
Student’s t process is a real-valued random function such that the outputs of the function on a finite
number of inputs are distributed by a multivariate t distribution (Shah et al., 2014). Consider a set
X (for inputs) with a symmetric positive-definite function κ : X × X → R.
Definition 2.2. A random function f : X → R is Student’s t process with degree of freedom
V ∈ R+, location function M : X → R, and scale function κ, if for all dand inputs x1, . . . , xd ∈ X,
the random vector (f (xι),..., f (Xd)) has the multivariate t distribution MVTd(v, μ, ∑), where
μ = (M(xι), ...,M(Xd)) and Σ is the d × d matrix defined by Σ(i,j) = k(xi, Xj).
3	Results
We will present our results for the regression setting. Throughout this section, we consider only
those bnns whose computations over fixed inputs are representable by tensor programs. Assume
that we are given a training set Dtr = {(xk, yk) ∈ RI × R | 1 ≤ k ≤ K} and a test set Dte =
{(xκ+',yκ+') 11 ≤ ' ≤ L}.
Our idea is to treat the variance σv2 for the parameters of the readout layer in a BNN as a random
variable, not a deterministic value, so that the bnn represents a mixture of random functions (where
the randomness comes from that of σv2 and the random initialization of the network parameters). To
4
Published as a conference paper at ICLR 2022
state this more precisely, consider the computations of the BNNs {fn(-; vn, Ψn)}n∈N on the inputs
x1:K+L in the training and test sets such that n is the width of the hidden layers of fn and these
computations are representable by a tensor program. Our idea is to change the distribution for the
initial value ofvn from a Gaussian to a scale mixture of Gaussians, so that at initialization, the BNNs
on x1:K+L become the following random variables: for each n ∈ N,
σV 〜H,	Vn,α∣σV 〜N(0,σV) for α ∈ [n],	Ψn,(α,j)〜N(0,σ2∕n) for α ∈ [n], j ∈ [P],
fn(Xi； Vn, Ψn) = : X Vn,α ∙ Φ(gn(Xi; Ψn))α 取 i ∈ [K + L],
n α∈[n]
where H is a distribution on positive real numbers, such as the inverse-gamma distribution, P is the
number of columns of Ψn , and σj2 and gn are, respectively, the variance specific to the j-th column
ofΨn and the output of the penultimate linear layer, as explained in our review on tensor programs.
Our first result says that as the width of the BNN grows to ∞, the BNN converges in distribution to a
mixture of NNGPs, and, in particular, to Student’s t process if the distribution H is inverse gamma.
Theorem 3.1 (Convergence). As n tends to ∞, the random variable (fn(xi; vn, Ψn))i∈[K+L] con-
verges in distribution to the following random variable (f∞ (xi))i∈[K+L] :
σV 〜H,	(f∞(xi))i∈[κ+L]∣σV 〜N(O足∙K),
where K(i,j) = EZ 〜N (μ^∑)[φ(Zi')φ(Zj)] for i,j ∈ [K + L]. Furthermore, if H is the inverse-
gamma distribution with shape a and scale b, the marginal distribution of (f∞ (xi))i∈[K+L] is the
following multivariate t distribution:
(f∞(xi))i∈[κ+L]〜MVTκ+L(2a, O, (b∕a) ∙K),
where MVTd (ν0, μ0, K0) denotes the d-dimensional Student's t distribution with degree of freedom
ν0, location μ0, and scale matrix K0.
This theorem holds mainly because of the master theorem for tensor programs (Theorem 2.1). In
fact, the way that its proof uses the master theorem is essentially the same as the one of the nngp
convergence proof (i.e., the proof of Corollary 2.2), except for one thing: before applying the master
theorem, the proof of Theorem 3.1 conditions on σv2 and removes its randomness. The detailed proof
can be found in the appendix.
As in the case of Corollary 2.2, although the theorem is stated for the finite dimensional case with
the inputs xi：k+l, it implies the convergence of the BNNs with random σ2 to a scale mixture of GPs
or to Student's t process. Note that σV ∙ K in the theorem is precisely the covariance matrix K of
the NNGP kernel on x1:K+L in Corollary 2.2. We thus call the limiting stochastic process as scale
mixture of NNGPs.
Our next results characterize a fully-trained bnn at the infinite-width limit under two different train-
ing schemes. They are the versions of the standard results in the literature, generalized from gps to
scale mixtures of GPs. Assume the BNNs under the inputs x1:K+L that we have been using so far.
Theorem 3.2 (Convergence under Readout-Layer Training). Assume that we train only the readout
layers of the BNNs using the training set Dtr under mean squared loss and infinitesimal step size.
Formally, this means the network parameters are evolved under the following differential equations:
dv(t)	K
S0), Ψn0)) = (Vn, Ψn),	dʌ = XQi- "W, Ψf)))
i=1
2φ(gn(x3 ψnt)))	dψnt)
κ√n
dt
0.
Then, the time and width limit of the random variables (fn(xK+i; Vn(t), Ψ(nt)))i∈[L], denoted
(f∞∞(xK+i))i∈[L], is distributed by the following mixture of Gaussians:
σV 〜H,	(f∞(xκ+i))i∈[L]∣σV 〜N ((K,,t KKr1tr Yr), σV(K—M-Kte ,tr K KrKKtr,te)),
where Yr consists of the y values in D疗(i.e., Yr = (yι,...,yκ)) and the K ,s with different sub-
scripts are the restrictions of K in Theorem 3.1 with training or test inputs as directed by those sub-
scripts. Furthermore, if H is the inverse-gamma distribution with shape a and scale b, the marginal
distribution of(f∞∞(xK+i))i∈[L] is the following multivariate t distribution:
(f∞(xK+i))i∈[L]〜MVTL 0a, (Kte ,tr K K,[ Kr) , J(Kte, te -K te ,tr KKKtr, te )}.
5
Published as a conference paper at ICLR 2022
One important point about this theorem is that the distribution of (f∞∞ (xK+i))i∈[L] is not the pos-
terior of (f∞ (xK+i))i∈[L] in Theorem 3.1 under the condition on Ytr. The former uses the prior on
σv2 for mixing the distribution of (f∞(xK+i))i∈[L] | (σv2, Ytr), while the latter uses the posterior on
σv2 for the same purpose.
Our last result assumes a change in the initialization rule for our models. This is to enable the use
of the results in Lee et al. (2019) about the training of infinitely-wide neural networks. Concretely,
we use the so called NTK parametrization for Ψn. This means that the BNNs on x1:K+L are now
intialized to be the following random variables: for each n ∈ N,
σV 〜H, Vn,α∣σV 〜N(0,σV),中孙@力〜N(0,σ2) for α ∈ [n], j ∈ [P],
fn(xi; vn, √n ψn) = √n ɪ2 vn,α，φ(gn(xi; √n ψn))α for i ∈ [K + L] ∙
n	n α∈[n]	n
When we adopt this NTK parameterization and the bnns are multi-layer perceptrons (more gener-
ally, they share an architecture for which the limit theory of neural tangent kernels has been devel-
oped), we can analyze their distributions after the training of all parameters. Our analysis uses the
following famous result for such BNNs: as n tends to infinity, for all i, j ∈ [K + L],
D%Vn,Ψn)fn(Xi; Vn, √n ψn), V(Vn,Ψ n) fn(Xj； vn, √n ψn ^E —→ θ(i,j)	(4)
for some Θ(i,j) determined by the architecture of the BNNs. Let Θ be the matrix (Θ(i,j))i,j∈[K+L].
The next theorem describes the outcome of our analysis for the fully-trained infinite-width bnn.
Theorem 3.3 (Convergence under General Training). Assume that the BNNs are multi-layer percep-
trons and we train all of their parameters using the training set Dtr under mean squared loss and
infinitesimal step size. Formally, this means the network parameters are evolved under the following
differential equations:
v(n0) = vn ,
Ψ(n0) = Ψn ,
dt
dΨn
dt
K√n
2K	1	1
K X (yi - fn(xi; vn , √n W )) ^Ψfn(Xi; vn , √n W )
Let Θ be Θ with σV in it set to 1 (so that Θ = σ)Θ). Then, the time and width limit of the random
variables (fn(xκ+i; Vnt), √1n Ψnt)))i∈[L] has the following distribution:
σV 〜H,
where
f∞ (XK+i))i∈[L]WV 'N"σVθ0)
μ0 = θ te, tr θ -1r Ytr,
θ = Kte,te + (<θte,trθ-1 Ktr,trθ-1 θθtr,te) - (<θte,trθ-1 Ktr,te + Kte,trθ-1 θtr,te
Here the K，s with subscripts are the ones in Theorem 3.1 and the Θ s with subscripts are similar
restrictions of Θ with training or test inputs. Furthermore, if H is the inverse gamma with shape
a and scale b, the exact marginal distribution of (f∞∞ (XK +i))i∈[L] is the following multivariate t
distribution:
(f∞(xκ+i))i∈[L]〜MVTl(2a, μ0, (b/a) ∙ Θ0).	(5)
Remark (Unifying High-level Principles). All of our results and their proofs are derived from two
high-level principles. First, once we condition on the variance σv2 of the readout layer’s parameters,
the networks in our setup fit into the standard setting for nngps and ntks, so that they satisfy the
existing results in the setting. This conditioning trick means that most of the results in the standard
setting can be carried over to our setup in a scale-mixture form, as we explained in this section.
Second, if the prior on σv2 is inverse gamma, the marginalization of σv2 in those results can be
calculated analytically and have a form of Student’s t distribution or process.
6
Published as a conference paper at ICLR 2022
3.1	Posterior inference
Gaussian likelihood As for the NNGPs, when the scale is mixed by the inverse gamma prior, we
can exactly compute the posterior predictives of the scale mixture of nngps for Gaussian likeli-
hood since it corresponds to the Student’s t process having closed-form characterizations for the
posteriors. For generic prior settings for the scales, we no longer have such closed-form formu-
las for predictive posteriors, and have to rely on approximate inference methods. We describe one
such method, namely, self-normalizing importance sampling with prior as proposal, together with a
technique for optimizing the method specifically for scale mixtures of nngps.
Consider a scale mixture of NNGPs with a prior H on the variance σv2 . Assume that we want to
estimate the expectation of h(y) for some h : R → R where the random variable y is drawn from the
predictive posterior of the mixture at some input x ∈ RI under the condition on Dtr. The importance
sampling with prior as proposal computes the estimator (PiN=1 wih(yi))/ PjN=1 wj where βi and
yi for i = 1, . . . , N are independent samples from the prior H and the posterior of the Gaussian
distribution, respectively, and the w/s are the importance weights of the βi 's:
I	I
βi 〜H,	Wi = N(Ytr； 0,βiKtr,tr),	Ji 〜N(Kχ,trK↑r,↑rYtr, βi(Kχ,x - Kχ,trKtr,trKtr,x)).
The K is the covariance matrix computed as in Theorem 3.1 except that X is now a test input.
A naive implementation of this importance sampler is slow unnecessarily due to the inefficient cal-
culation of the likelihoods N(Yτ10,βiKtr,tr) of the sampled β∕s. To see this, note that the log
likelihood of βi can be decomposed as follows:
________ 一、	K.	,	、	1.	.	, =	、----K.-	1 _丁一1_
log N(Ytr; 0, βiKtr,tr) =	- ɪ log(2π)	- 2 log det	(Ktr,tr)	2 log(βi)	- ^^ Ytr Ktr,trKr∙
The terms K log(2∏), 2 logdet(Ktr,tr), and Y>K-,1Y are shared across all the samples {βi}N=1,
so need not be computed everytime we draw βi . To avoid these duplicated calculations, we compute
these shared terms beforehand, so that the log-likleihood for each βi can be computed in O(1) time.
Generic likelihoods For generic likelihoods other than Gaussian, even the vanilla NNGPs do not
admit closed-form posterior predictives. In such cases, we can employ the Sparse Variational Gaus-
sian Process (svgp) (Titsias, 2009) for approximate inference. We present a similar algorithm for
the inverse-gamma mixing case which leads to Student’s t process. See Appendices C and D for the
detailed description.
4	Experiments
We empirically evaluated the scale mixtures of nngps on various synthetic and real-world tasks.
We tested the scale mixture of NNGPs with inverse-gamma prior corresponding to Student’s t pro-
cesses, and with another heavy-tailed prior called Burr Type XII distribution (Appendix H). Our
implementation used Neural Tangents library (Novak et al., 2020) and JAX (Bradbury et al., 2018).
4.1	Empirical validation of the theories
To empirically validate our theories, we set up a fully connected network having three layers with
512 hidden units and erf activation function. Except for the last layer, the weights of the network
were initialized with N(0, 8/n) and the biases were initialized with N(0, 0.052/n). For the last
layer, we sampled σ2 〜InvGamma(a, b) with varying (a, b) values and sampled weights from
N(0, σv2). To check Theorem 3.1, we initialize 1,000 models and computed the distribution of out-
puts evaluated at zero. For Theorem 3.2, using y = sin(x) as the target function, we first initialized
the parameters, trained only the last layer for multiple σv values, and averaged the results to get
function value at zero. Similarly, we trained all the layers to check Theorem 3.3. As shown in
Fig. 1, the theoretical limit well-matched the empirically obtained distributions for all settings we
tested. See Appendix G for the details and more results.
7
Published as a conference paper at ICLR 2022
Figure 1: (1st-2nd column) Posterior predictives obtained from NNGP (left) and scale mixture of
nngps with inverse gamma prior (right). (3rd-5th column) correspondence between wide finite
model vs theoretical limit.
Table 1: NLL values on UCI dataset. (m, d) denotes number of data points and features, respec-
tively. We take results from Adlam et al. (2020) except our model.
Dataset	(m, d)	PBP-MV	Dropout	Ensembles	RBF	NNGP	Ours
Boston Housing	(506, 13)	2.54 ± 0.08	2.40 ± 0.04	2.41 ± 0.25	2.63 ± 0.09	2.65 ± 0.13	2.72 ± 0.05
Concrete Strength	(1030, 8)	3.04 ± 0.03	2.93 ± 0.02	3.06 ± 0.18	3.52 ± 0.11	3.19 ± 0.05	3.13 ± 0.04
Energy Efficiency	(768, 8)	1.01 ± 0.01	1.21 ± 0.01	1.38 ± 0.22	0.78 ± 0.06	1.01 ± 0.04	0.67 ± 0.04
Kin8nm	(8192, 8)	-1.28 ± 0.01	-1.14 ± 0.01	-1.20 ± 0.02	-1.11 ± 0.01	-1.15 ± 0.01	-1.18 ± 0.01
Naval Propulsion	(11934, 16)	-4.85 ± 0.06	-4.45 ± 0.00	-5.63 ± 0.05	-10.07 ± 0.01	-10.01 ± 0.01	-8.04 ± 0.04
Power Plant	(9568, 4)	2.78 ± 0.01	2.80 ± 0.01	2.79 ± 0.04	2.94 ± 0.01	2.77 ± 0.02	2.66 ± 0.01
Wine Quality Red	(1588, 11)	0.97 ± 0.01	0.93 ± 0.01	0.94 ± 0.12	-0.78 ± 0.07	-0.98 ± 0.06	-0.77 ± 0.07
Yacht Hydrodynamics	(308, 6)	1.64 ± 0.02	1.25 ± 0.01	1.18 ± 0.21	0.49 ± 0.06	1.07 ± 0.27	0.17 ± 0.25
4.2 Regression with Gaussian likelihoods
We tested the scale mixtures of nngps and other models on various real-world regression tasks. All
the models are trained with the squared loss function. We expect the scale mixtures of nngps to be
better calibrated than nngp due to their heavy-tail behaviors. To see this, we fitted the scale mixture
of nngps with inverse gamma prior on eight datasets collected from UCI repositories 1 and measured
the negative log-likelihood values on the test set. Table 1 summarizes the result. The results other
than ours are borrowed from Adlam et al. (2020). In summary, ours generally performed similarly
to nngp except for some datasets for which ours significantly outperformed nngp. Considering the
fact that Student’s t processes include GPs as limiting cases, this result suggests that one can use the
scale mixtures as an alternative to nngps even for the datasets not necessarily including heavy-tailed
noises. See Appendix G for detailed settings for the training.
4.3	Classification with Gaussian likelihood
Following the literature, we apply the nngps and the scale mixtures of nngps to classification
problems with Gaussian likelihoods (squared loss). We summarize the results in Appendix F. As a
brief summary, ours with heavy-tailed priors (inverse gamma, Burr Type XII) outperformed nngp
in terms of uncertainty calibration for various datasets including corrupted datasets.
4.4	Classification with categorical likelihood
We compared the nngp and the scale mixture of nngps with inverse-gamma priors on the image
classification task. Following the standard setting in image classification with deep neural networks,
we computed the posterior predictive distributions of the bnns under categorical likelihood. Since
both nngps and the scale mixtures of nngps do not admit closed-form posteriors, we employed
svgp as an approximate inference method for nngp (Appendix C), and extended the svgp for the
scale mixture of NNGPs (Appendix D). We call the latter as Sparse Variational Student’s t Process
(SVTP) since it approximates the posteriors whose limit corresponds to Student’s t process. We com-
pared svgp and svtp on multiple image classification benchmarks, including MNIST, CIFAR10,
and SVHN. We also evaluated the predictive performance on Out-Of-Distribution (ood) data for
which we intentionally removed three training classes to save as ood classes to be tested. We used
four-layer CNN as a base model to compute nngp kernels. See Appendix G for details. Table 2
summarizes the results, which show that svtp and svgp perform similarly for in-distribution data,
1https://archive.ics.uci.edu/ml/datasets.php
8
Published as a conference paper at ICLR 2022
Table 2: Classification accuracy and NLL of SVGP and SVTP for image datasets and their variants.
NLL values are multiplied by 102 .
Dataset	SVGP		SVTP (Ours)		Dataset	SVGP		SVTP (Ours)	
	NLL (×102)	Accuracy (%)	NLL(×102)	Accuracy (%)		NLL (×102)	Accuracy (%)	NLL (×102)	Accuracy (%)
MNIST	8.96 ± 0.12	97.73 ± 0.03	8.90 ± 0.04	97.78 ± 0.04	CIFAR10	131.96 ± 0.35	54.09 ± 0.12	132.13 ± 0.24	54.10 ± 0.13
+ Shot	24.22 ± 0.08	94.63 ± 0.05	24.28 ± 0.10	94.63 ± 0.07	+ Shot 5	143.11 ± 0.29	49.43 ± 0.14	142.30 ± 0.18	49.73 ± 0.05
+ Impulse	56.52 ± 0.88	90.29 ± 0.65	57.91 ± 0.57	89.36 ± 0.58	+ Impulse 5	164.90 ± 0.11	41.66 ± 0.19	160.79 ± 0.75	43.08 ± 0.34
+ Spatter	16.79 ± 0.19	95.95 ± 0.04	16.66 ± 0.05	95.99 ± 0.04	+ Spatter 5	141.11 ± 0.30	50.34 ± 0.17	141.14 ± 0.15	50.31 ± 0.15
+ Glass Blur	100.65 ± 2.35	62.63 ± 0.65	97.19 ± 2.23	63.52 ± 0.74	+ Fog 5	213.50 ± 0.19	25.47 ± 0.21	209.31 ± 0.48	26.03 ± 0.16
w. OOD	216.58 ± 1.83	67.70 ± 0.03	206.86 ± 1.85	67.67 ± 0.03	+ Snow 5 w. OOD	166.44 ± 0.51 341.59 ± 1.75	41.47 ± 0.27 41.83 ± 0.11	166.41 ± 0.28 333.70 ± 1.83	41.47 ± 0.11 41.19 ± 0.17
	53.93 ± 0.13 268.41 ± 2.40	83.92 ± 0.09 60.76 ± 0.06	53.95 ± 0.30 257.16 ± 1.86	83.96 ± 0.08 60.46 ± 0.05					
KMNIST w. OOD					EMNIST w. OOD	56.49 ± 1.24 183.25 ± 1.40	84.25 ± 0.22 72.58 ± 0.28	54.92 ± 0.84 177.65 ± 0.43	84.55 ± 0.32 72.38 ± 0.16
Fashion MNIST	34.30 ± 0.10	87.84 ± 0.13	34.25 ± 0.13	87.90 ± 0.05	SVHN	100.88 ± 0.17	71.67 ± 0.11	101.04 ± 0.13	71.71 ± 0.06
w. OOD	252.61 ± 3.51	62.29 ± 0.04	241.69 ± 2.45	62.24 ± 0.06	w. OOD	379.75 ± 9.24	46.86 ± 0.19	360.40 ± 3.68	46.43 ± 0.08
Table 3: Classification accuracy and NLL of ensemble models for image datasets and their variants.
We used 8 models of 4-layer CNN for our base ensemble model. NLL values are multiplied by 102.
Dataset	Gaussian		Inverse Gamma Prior (Ours)		Dataset	Gaussian		Inverse Gamma Prior (Ours)	
	NLL(×102)	Accuracy (%)	NLL(×102)	Accuracy (%)		NLL (×102)	Accuracy (%)	NLL (×102 )	Accuracy (%)
MNIST	0.33 ± 0.01	98.94 ± 0.04	0.32 ± 0.01	98.98 ± 0.04	KMNIST	2.74 ± 0.04	93.24 ± 0.20	2.75 ± 0.02	93.34 ± 0.16
+ Shot	1.42 ± 0.14	95.73 ± 0.48	1.42 ± 0.17	95.73 ± 0.46	w. OOD	78.64 ± 2.14	65.64 ± 0.07	70.06 ± 3.14	65.61 ± 0.13
+ Impulse	5.91 ± 0.68	83.10 ± 1.32	6.27 ± 0.78	82.80 ± 1.31	w. Imbalance	9.47 ± 0.27	77.04 ± 0.96	9.12 ± 0.57	77.23 ± 1.67
+ Spatter + Glass Blur	0.76 ± 0.02 3.25 ± 0.31	97.58 ± 0.12 88.50 ± 1.27	0.74 ± 0.02 2.83 ± 0.20	97.63 ± 0.03 90.22 ± 0.80	w. Noisy Label	10.78 ± 0.03	84.42 ± 0.10	10.67 ± 0.03	84.30 ± 0.12
									
w. OOD	82.71 ± 2.65	68.51 ± 0.01	74.31 ± 2.93	68.47 ± 0.03	Fashion MNIST	2.36 ± 0.04	91.92 ± 0.11	2.37 ± 0.02	91.82 ± 0.11
w. Imbalance	1.50 ± 0.21	95.85 ± 0.44	1.50 ± 0.23	95.82 ± 0.49	w. OOD	62.17 ± 0.50	64.47 ± 0.09	58.57 ± 0.97	64.48 ± 0.07
w. Noisy Label	7.70 ± 0.06	97.62 ± 0.04	7.63 ± 0.06	97.56 ± 0.03	w. Imbalance w. Noisy Label	5.54 ± 0.03 9.30 ± 0.08	83.09 ± 0.12 87.05 ± 0.07	5.63 ± 0.08 9.20 ± 0.06	83.00 ± 0.11 87.27 ± 0.19
									
CIFAR10	8.68 ± 0.06	70.77 ± 0.31	8.74 ± 0.08	70.22 ± 0.30						
+ Shot 5	16.50 ± 0.27	51.48 ± 0.34	15.28 ± 0.38	53.28 ± 0.37	EMNIST	0.76 ± 0.01	91.25 ± 0.13	0.75 ± 0.01	91.30 ± 0.05
+ Impulse 5	32.39 ± 1.11	33.51 ± 1.24	29.20 ± 1.49	35.66 ± 1.56	w. OOD	8.37 ± 0.15	76.59 ± 0.09	8.25 ± 0.17	76.72 ± 0.08
+ Spatter 5 + Fog 5	12.83 ± 0.23 15.40 ± 0.07	58.60 ± 0.59 45.23 ± 0.07	12.36 ± 0.17 15.28 ± 0.06	59.36 ± 0.26 45.73 ± 0.22	w. Noisy Label	3.23 ± 0.02	86.64 ± 0.17	3.15 ± 0.01	86.25 ± 0.20
									
+ Snow 5	13.18 ± 0.25	57.08 ± 0.48	12.76 ± 0.08	57.66 ± 0.37	SVHN	4.68 ± 0.05	87.84 ± 0.17	4.70 ± 0.04	87.82 ± 0.13
w. OOD	81.57 ± 0.01	50.66 ± 0.13	80.62 ± 0.86	50.46 ± 0.23	w. OOD	105.17 ± 1.75	56.92 ± 0.03	101.87 ± 1.92	56.92 ± 0.03
w. Imbalance	19.22 ± 0.12	38.73 ± 1.35	19.05 ± 0.18	39.74 ± 0.37	w. Imbalance	14.20 ± 0.17	63.64 ± 0.32	14.88 ± 0.19	61.97 ± 0.92
w. Noisy Label	15.88 ± 0.03	54.90 ± 0.34	15.90 ± 0.05	54.81 ± 0.38	w. Noisy Label	12.22 ± 0.11	80.24 ± 0.20	12.36 ± 0.07	79.84 ± 0.24
but svtp significantly outperforms svgp for ood data in terms of uncertainty calibration (measured
by negative log-likelihoods).
4.5	Classification by Finite Model Ensemble
Although we proved Theorem 3.3 only for fully-connected networks trained with squared losses,
Lee et al. (2019) empirically observed that there still seem to be a similar correspondence for neural
networks trained with cross-entropy loss. To this end, we tested the ensembles of wide finite CNNs
with 4 layers trained by gradient descent over the cross-entropy loss. Each model in our test is ini-
tialized by the ntk parameterization with random scales drawn from inverse gamma prior on the last
layer (ours) or without such random scales (nngp). For each prior setting, we constructed ensem-
bles of 8 models, and compared the performance on MNIST, MNIST-like variants, CIFAR10, and
SVHN. We also compared the models under the presence of corruption, ood data, label imbalance,
and label noises. See Appendix G for detailed description for the experimental setting. As for the
previous experiments, Table 3 demonstrates that ours with inverse gamma prior largely outperforms
the baseline, especially for the ood or corrupted data.
5	Conclusion
In this paper, we proposed a simple extension of nngps by introducing a scale prior on the last-
layer weight parameters. The resulting method, entitled as the scale mixture of nngps, defines a
broad class of stochastic processes, especially the heavy-tailed ones such as Student’s t processes.
Based on the result in (Yang, 2019), we have shown that an infinitely-wide bnn of any architecture
constructed in a specific way corresponds to the scale mixture of nngps. Also, we have extended
the existing convergence results of infinitely-wide bnns trained with gradient descent to gps so
that the results hold for our construction. Our empirical evaluation validates our theory and shows
promising results for multiple real-world regression and classification tasks. Especially, it shows
that the heavy-tailed processes from our construction are robust to the out-of-distribution data.
9
Published as a conference paper at ICLR 2022
Acknowledgements and Disclosure of Funding
We would like to thank Jaehoon Lee for helping us to understand his results on NNGP and NTK.
This work was partly supported by Institute of Information & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial
Intelligence Graduate School Program(KAIST), No. 2021-0-02068, Artificial Intelligence Inno-
vation Hub), National Research Foundation of Korea (NRF) funded by the Ministry of Education
(NRF2021R1F1A1061655, NRF-2021M3E5D9025030). HY was supported by the Engineering
Research Center Program through the National Research Foundation of Korea (NRF) funded by the
Korean Government MSIT (NRF-2018R1A5A1059921) and also by the Institute for Basic Science
(IBS-R029-C1).
References
Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, and Jasper Snoek. Exploring the un-
certainty properties of neural networks’ implicit priors in the infinite-width limit. arXiv preprint
arXiv:2010.07355, 2020.
D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal
Statistical Society: Series B (Methodological), 36, 1974.
D. Bracale, S. Favaro, S. Fortini, and S. Peluchetti. Infinite-channel deep stable convolutional neural
networks. arXiv:2021.03739, 2021.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
S. Favaro, S. Fortini, and S. Peluchetti. Stable behaviour of infinitely wide deep neural networks. In
Proceedings of The 23rd International Conference on Artificial Intelligence and Statistics (AIS-
TATS 2020), 2020.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow Gaussian processes. In International Conference on Learning Representations
(ICLR), 2019.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP
and NTK for deep attention networks. In Proceedings of The 37th International Conference on
Machine Learning (ICML 2020),pp. 4376—4386. PMLR, 2020.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: convergence and generalization in
neural networks. In Advances in Neural Information Processing Systems 31 (NeurIPS 2018),
2018.
J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide
neural networks of any depth evolve as linear models under gradient descent. In Advances in
Neural Information Processing Systems 32 (NeurIPS 2019), 2019.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations (ICLR), 2018.
A. G. de G. Matthews, J. Hron, R. E. Turner, and Z. Ghahramani. Sample-then-optimize posterior
sampling for Bayesian linear models. NeurIPS Workshop on Advances in Approximate Bayesian
Inference, 2017.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations (ICLR), 2018.
R. M. Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
10
Published as a conference paper at ICLR 2022
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are Gaussian processes. In International Conference on Learning Representations
(ICLR), 2018.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
In International Conference on Learning Representations (ICLR), 2020. URL https://
github.com/google/neural-tangents.
A. Shah, A. Wilson, and Z. Ghahramani. Student-t processes as alternatives to Gaussian processes.
In Proceedings of The 17th International Conference on Artificial Intelligence and Statistics (AIS-
TATS 2014), 2014.
Joram Soch and Carsten Allefeld. Kullback-leibler divergence for the normal-gamma distribution.
arXiv preprint arXiv:1611.01437, 2016.
M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings
of The 12th International Conference on Artificial Intelligence and Statistics (AISTATS 2009),
2009.
Greg Yang. Tensor programs I: Wide feedforward or recurrent neural networks of Any architecture
are Gaussian processes. arXiv preprint arXiv:1910.12478, 2019.
11
Published as a conference paper at ICLR 2022
A Details on Tensor Programs
In this section, we will review tensor programs from (Yang, 2019), which are sequences of assign-
ment statements followed by a return statement where each variable is updated at most once.
Tensor programs use the three types of variables: G-variables, H-variables, and A-variables. G-
and H-variables are vector-type variables, and A-variables are matrix-type variables. Each tensor
program is parameterized by n ∈ N, which determines the dimension of all vectors (i.e., G- and
H-variables) and matrices (i.e., A-variables) in the program to n and n × n, respectively. Due to
the random initialization of some of these variables, the contents of these variables are random in
general. G-variables denote Gaussian-distributed random variables, and H-variables store the results
of applying (usually nonlinear) functions on G-variables. A-variables store random matrices whose
entries are set with iid samples from a Gaussian distribution. All H-variables in a tensor program are
set their values by some assignments, but some G-variables and all H-variables may not be assigned
to in the program. Such G-variables are called input G-variables.
Assignments in tensor programs have one of the three kinds: MatMul, LinComb, and Nonlin. The
MatMul assignments have the form y = Wx where y is a G-variable, W is an A-variable, and x is
a H-variable. The LinComb assignments compute linear combinations, and they have the form y =
Pli=1 wixi where y, x1, . . . , xl are G-variables, and w1, . . . , wl ∈ R are constants. The final Nonlin
assignments apply (usually nonlinear) scala functions on the values of G-variables coordinatewise.
That is, they have the form y = φ(x1, . . . , xl), where y is an H-variable, x1, . . . , xl are G-variables,
φ : Rl → R is a possibly nonlinear function, and φ(x1, . . . , xl) means the lifted application of φ to
vector arguments.
Every tensor program ends with a return statement of the form:
return(v1>y1/√n,..., vl>yl/√n),	(6)
where v1,. . . , vl are G-variables not appearing anywhere in the program, and y1,. . . , yl are H-
variables.
Assumption A.1. Every A-variable W in a tensor program is initialized with iid samples from
the Gaussian distribution with mean zero and Covariance σW/n for some σw > 0 (i.e., Wij 〜
N(0, σW2 /n) for i, j ∈ [n]), and these samples are independent with those used for other A-variables
and input G-variables. For every α ∈ [n], the components of all input G-variables x1,. . . , xm
(Which Store vectors in Rn) are initialized with independent samples from N(μin, Σin) for some
mean μin and (possibly singular) covariance Σin over all input G-variables. Note that the mean
and the covariance do not depend on the component index α ∈ [n]. Finally, the input G-variables
vi used in the return statement of the program are independent with all the other random variables
in the program, and their components are set by iid samples from N(0, σv2).
Definition A.1. Given a tensor program satisfying Assumption A.1, We can compute μ and Σ for
not only input G-variables but all the G-variables in the program. Here the dimension of the vector
μ is the number N of all G-variables. The computed μ and Σ specify the multivariate Gaussian
distribution over the α-th components of G-variables that arises at the infinite-Width limit (Theo-
rem A.2). The computation is defined inductively as folloWs. Let g1,. . . , gN be all the G-variables
in the program, ordered as they appear in the program. For any G-variable gk, μ(gk) is defined by:
(μin(gk)	if gk is an input G-variable
μ(gk) =	Pm=ι wiμ(Xi) ifgk = Pm=ι aixi	.
10	otherwise
Here x1,. . . , xm are some G-variables. For any pair of G-variables gk, gl, Σ(gk, gl) is defined by:
	'Σin (gk ,gl) PmtI WiMxMg1)	if gk , gl are input G-variables if gk = Pim=1 wixi
Σ(gk,gl) = ‹	Pim=1 wiΣ(gk,xi)	ifgl = Pim=1 wixi
	σW Ez[φ(Z)φ0(Z)]	if gk = Wy, gl = Wy0 for the same W, with Z,φ,φ0 below
	、0	otherwise
Here x1 ,. . . , xm are some G-variables, and H-variables y and y0 in the second last case are intro-
duced by the Nonlin assignments y = φ(gi1,. . . , gip) and y0 = φ0(gi1,. . . , gip), respectively, for
12
Published as a conference paper at ICLR 2022
some i1 , . . . , ip ∈ [max(k - 1, l - 1)]. We have adjusted the inputs of φ and φ0 to ensure that
they take the same set of G-variables. The random variable Z in the second last case is a random
Gaussian vector drawn from N(μ0, Σ0) where μ0 and Σ0 are the mean vector and covariance matrix
for gi1 , . . . , gip by the previous step of this inductive construction.
Definition A.2. Given h : Rn → R and for any x ∈ Rn if |h(x)| is upper bounded by a function
exp(C kxk2- + c) with C, c, > 0, then this function is controlled.
Theorem A.2 (Master Theorem). Consider a tensor program satisfying Assumption A.1 and using
only controlled φ ,s. Let g1,... ,gN be all of the G-VariabIes that appear in this program, and
h : RN → R be a controlled function. Then, as n tends to ∞,
1n
n	h(gα, ... , gα) --→ EZ 〜N (μ,Σ)[h(Z1, ... , ZN )].
α=1
Here --→ refers to almost sure convergence, and Z = (Zi,...,Zn) ∈ RN. Also, μ and Σ are
from Definition A.1.
Corollary A.3. Consider a tensor program satisfying Assumption A.1 and using only controlled
φ ,s. Let g1,... ,gN be all ofthe G-variables that appear in this program. Assume that the program
returns a vector (v>y1∕√n,..., v>yk/√n) for some H-variables yi's such that each yi is updated
by yi = φi(g1, . . . , gN) for a controlled φi. Then, as n approaches to ∞, the joint distribution of
this vector converges weakly to the multivariate Gaussian distribution N(0, K) with the following
covariance matrix:
K(i,j) = σV ∙Ez〜N(μ,∑)[φi(Z)Φj(Z)] for all i,j ∈ [k],
where μ and Σ are from DefinitionA.1.
B Proofs
In this section, we provide the proofs of our three main results. We first describe a lemma related to
a basic property of Student’s t distribution:
Lemma B.1. The class of multivariate t distributions is closed under marginalization and condi-
tioning. That is, when
X = [X2] ∈ Ri2 〜MVTdι+d2 ”,图,[∑21 W),	⑺
we have that
X1 〜MVTdi(ν,μι, Σ11) and X2∣X1 〜MVTd, (V + di, 〃2, ^^£∑2?)	(8)
where
μ2 = μ2 + ς21ςIII(XI - μI), C = (x1 - Mi)>^|_i1(x1 - μI), ς22 = ς22 -夕21夕111夕12.
Definition B.1. A distribution on R+ is a inverse gamma distribution with shape parameter a ∈ R+
and scale parameter b ∈ R+ if it has the following density:
p(x) =为(1 )α+1 e-b∕x
G(a) x
where G(∙) is the gamma function. To express a random variable drawn from this distribution, We
write X 〜InvGamma(a, b).
Lemma B.2. Let Σ ∈ Rd×d be a postive definite matrix. Consider μ ∈ Rn and a, b ∈ R+. If
σ2 〜InvGamma(a, b),	X∣σ2 〜N(μ, σ2Σ),
then the marginal distribution of X is MVTd(2a, μ, bΣ).
Now we proceed to the proofs of main theorems.
13
Published as a conference paper at ICLR 2022
Theorem 3.1 (Convergence). As n tends to ∞, the random variable (fn (xi ; vn , Ψn))i∈[K+L] con-
verges in distribution to the following random variable (f∞ (xi))i∈[K+L] :
σV ~H,	(f∞(xi))i∈[κ+L]∣σV ~N(0,σV ∙K),
where K(ij) = Ez~n(μ,∑) [Φ(Zi)Φ(Zj)] for i,j ∈ [K + L]. Furthermore, if H is the inverse-
gamma distribution with shape a and scale b, the marginal distribution of (f∞ (xi))i∈[K+L] is the
following multivariate t distribution:
(f∞(xi))i∈[κ+L] ~ MVTκ+L(2a, 0, (b/a) ∙K),
where MVTd (V 0, μ0, K0) denotes the d-dimensional Student's t distribution with degree of freedom
ν0, location μ0, and scale matrix K0.
Proof. Let oi [n] = fn (xi ; vn, Ψn) for i ∈ [K + L] and n. Pick an arbitrary bounded con-
tinuous function h : RK+L → R. For each n, define Gn to be the σ-field generated by
gn(x1; Ψn), . . . ,gn(xK+L; Ψn).
We claim that as n tends to ∞,
E [h (o1[n],...,oK+L[n])] → Eσ2~H hEγ LK+L~N(0,σv K) [h (Y '…,丫 K+L)]] ∙
The theorem follows immediately from this claim. The rest of the proof is about showing the claim.
Note that
E[h (o1[n],...,oK+L[n])] = E E h (√1^ v>01:K+L)[ G,σ"],
where 01:K+L = [φ (gn (xi； Ψn)) ,∙∙∙,φ (gn (xK+L; Ψn))] ∈ Rn×(K+L). Conditioned on G and
σV, the random variable √nv>φ1:K+L in the nested expectation from above is distributed by a
multivariate Gaussian. The mean of this Gaussian distribution is:
E [ √n v> φhK+L∣G,σV] = E [ v>∣G ,σV] x√nφLK+L
= 0 ∈ RK+L.
The covariance is:
K[n](ij) = E ] (√1n Vn φ(gn (Xi； ψn))) (√1n v>φ(gn (Xj ψn))^G ,σV
nn
=1 XX E [ Vn,ɑvn,β | G,σv2] × (φ (gn (xi； Ψn)))α (φ (gn (xj； Ψn)))β
n α=1 β=1
1n
=—σV E (φ (gn (xi； ψn)))α (φ (gn (Xj ； ψn)))ɑ ∙
n	α=1
Using what we have observed so far, we compute the limit in our claimed convergence:
nl→∞ E [h (o1[n],...,oK+L[n])] = nl→∞ EhEuLK+L~N (0,K[n]) [h (u1：K+L)]i
=Ehnl→∞ EuLK+L~N(0,K[n]) [h(U1：K+L)]].
The last equality follows from the dominated convergence theorem and the fact thath is bounded. To
go further in our computation, We observe that K[n] converges almost surely to σ2K by Theorem 2.1
and so
EuLK+L~N(0,K[n])[h(u1：K+L)] = / h(U1K+L)N(UIK+L； o, K[n])du1：K+L
--→ / h(u1:K+L)N(u1:K+L;0,bVK)du1:K+L
=EYLK+L~N(0,σ2K) [h (Y 1：K+L)].
14
Published as a conference paper at ICLR 2022
Here we use N not just for the multivariate Gaussian distribution but also for its density, and derive
the almost sure convergence from the dominated convergence theorem. Using this observation, we
complete our computation:
Ehnlim∞ EuLK+L〜N(0,K[n]) [h(ULK+L)]] = E IEY 1：K+L〜N(0,σVK) [h (YLK+L)]]
=Eσ2 〜HhEY 1：K+L 〜N (0,σ2 K) [h (Y - )]].
□
Theorem 3.2 (Convergence under Readout-Layer Training). Assume that we train only the readout
layers of the BNNs using the training set Dtr under mean squared loss and infinitesimal step size.
Formally, this means the network parameters are evolved under the following differential equations:
(vn0), ψn0)) = (Vn, Ψn),4=XX Di- fn(XiM'),亚^))	铲 ,辱=0.
n n	dt	n n	K n	dt
Then, the time and width limit of the random variables (fn(XK+i; vn(t), Ψ(nt)))i∈[L], denoted
(f∞∞(XK+i))i∈[L], is distributed by the following mixture of Gaussians:
σ 〜H,	(f∞(xκ+i))i∈[L]∣σV 〜N ((K te ,t K--,1rYr), σ^te M-Kte产K KrK Ove)),
where Yr consists of the y values in D疗(i.e., Yr = (yι,...,yκ)) and the K S with different sub-
scripts are the restrictions of K in Theorem 3.1 with training or test inputs as directed by those Sub-
scripts. Furthermore, if H is the inverse-gamma distribution with shape a and scale b, the marginal
distribution of(f∞∞(xK+i))i∈[L] is the following multivariate t distribution:
(f∞(XK+i')')i∈[L] ^ MVTL 0a, (Kte, tr K K,1r Yr), ∖ (Kte, te -K te, tr KK,1r IKgI).
Our proof of this theorem reuses the general structure of the proof of the corresponding theorem
for nngps in (Lee et al., 2019), but it adjusts the structure slightly so as to use the master theorem
for tensor programs and cover general architectures expressed by those programs. Another new
important part of the proof is to condition on the random scale σv2 , so that the existing results such
as the master theorem can be used in our case.
Proof. Note that Ψnt) = Ψn0) for all t because dΨnt/dt = 0. Let
φi[n] = √φ (gn (xi； Ψnt))) = √φ (gn (xi； Ψn0))) for i ∈ [K + L],
Φ tr[n] = [Φι[n],...,Φκ [n]]> ∈ RK×n,	Φ te[n] = [φκ+ι [n],…，φκ+L[n]] > ∈ RL×n.
Using this notation, we rewrite the differential equation for v(nt) as follows:
dvn- = --2φtr[n]> (φtr[n] Vn) - Yr) ∙
dt K
This ordinary differential equation has a closed-form solution, which we write below:
Vnt)=	Vn0)	+	Φtr[n]>	(Φtr[n]Φtr[n]>)	1	卜Xp	(-KtΦtr[n]Φtr[n]>) - I)(φtr[n]	VnO)-	Yr).
We multiply both sides of the equation with the Φte [n] matrix, and get
φte[n] Vnt) = φte[n] vn0) + K[n]te,trK[n]K,1r 卜Xp (-KtK[n]tr,tr) - I) (φtr[n] VnO)- Yr)
where K[n]te,tr = Φte[n]Φtr[n]> and K[n]tr,tr = Φtr[n]Φtr[n]>. Then, We send t to ∞, and obtain
φte[n] Vnx)= φte[n] VnO)- K[n]te,trK[n]-,tr (φtr[n] VnO)- Yr)
=K[n]te,trK[n]K,trYr + φte[n] VnO)- K[n]te,trK[n]-,tr (φtr[n] VnO)).
15
Published as a conference paper at ICLR 2022
But
(fn(xκ+iM∞), ψn∞)))i∈[L] = Φte[n] V∞ .
Thus, we can complete the proof of the theorem if we show that for every bounded continuous
function h : RL → R,
Jim E h [K[n]te,trK[n]-,⅛ + Φte[n] vn0) - K[n]te,trK[n]-,1r (Φtr[n] * )
= E hh(f∞∞(xK+i))i∈[L]i .
Pick a bounded continuous function h : RL →_R. For each n, let Gn be the σ-field generated by
gn(xι; Ψn),..., gn(xκ+L; Ψn). Also, define K[n]tr,te = Φtr[n]^Φte[n]>. We show the sufficient
condition mentioned above:
n→∞E	hl K[n]te,trK[n]-,trYr + Φte[n] VnI)- K[n]te,trK[n]-,tr (φtr[n]
Jim E	E h( K[n]te,trK[n]-,⅛r + Φte[n] Vn0) - K[n]te,trK[n]-,1.
Φ tr[n]
Gn,σv2
nl→∞ E 忸Z~N(K[n]te,trK[n]-1 "2 (K[n]te,te-K[n]te,trK[n]-1 K[n]tr,te)) [h(Z)]]
E [nlimθ EZ〜N(K[n]te,trK[n]-1 "2 (K[n]te,te-K[n]te,trK[n]-1 K[n]tr,te)) [h(Z)]]
E 忸Z~N(Kte,trK-1 Ytr,σ2(Kte,te-Kte,trK-trKtr,te)) [h(Z)]]
Ehh(f∞∞(xK+i))i∈[L]i .
The second equality uses the fact that v(n0) consists of iid samples from N(0, σv2), and it is a conse-
quence of straightforward calculation. The third equality uses the dominated convergence theorem
and the fact that h is bounded. The fourth equality uses the master theorem for tensor programs
(Theorem 2.1), the boundedness of h, and the dominated convergence theorem. The last equality
follows from the definition of (f∞∞ (xK+i))i∈[L] .
If σV 〜InvGamma(a, b), We get the following closed-form marginal distribution by Lemma B.2:
(f∞(xκ+i))i∈[L]〜MVTL (2a, Kte,trK-,trYtr, a (Kte,te -Kte,trK-,trKtr,te)).
□
Our proof of Theorem 3.3 uses the corresponding theorem for the standard ntk setup in Lee et al.
(2019). We restate this existing theorem in our setup using our notation:
Theorem B.3. Assume that the BNNs are multi-layer perceptrons, and we train these models us-
ing the training set Dtr under the mean squared loss and infinitesimal step size. Regard σv2 as a
deterministic value. Then as n → ∞ and t → ∞, (fn(xκ+i; Vnt), √1nΨn)))i∈[L] converges in
distribution to the following random variable:
(f∞(xκ+i))i∈[L]〜N(μ∞, ∑∞)
where
μ ∞ = Θ te, tr Θ -,1r Yr,
Σt∞e = Kte,te + Θte,trΘt-r,1trKtr,trΘt-r,1trΘtr,te - (Θte,trΘt-r,1trKtr,te+Kte,trΘt-r,1trΘtr,te).
Theorem 3.3 (Convergence under General Training). Assume that the BNNs are multi-layer percep-
trons and we train all of their parameters using the training set Dtr under mean squared loss and
16
Published as a conference paper at ICLR 2022
infinitesimal step size. Formally, this means the network parameters are evolved under the following
differential equations:
V(n0)	Vn,	dVt _ dt	2K = κ√n∑(yi- fn(xi;v(	F, 7ψn)))φ(gn(xi; 7W)), nn	nn
Ψ(n0)	Ψn,	dψnt) 	= dt	2K =KfWi - fn(Xi 愁t, ■ K i=1	;Ψ^))vψfn(Xi ； Vnt), √r ψnt)) nn
Let Θ be Θ with σ22 in it set to 1 (so that Θ = σ]Θ). Then, the time and width limit of the random
variables (fn(xκ+i; v(t), √1n Ψj⅛t)))i∈[L] has the following distribution:
σV 〜H,	(f∞(XK+/记囚K 〜N(μ0,σVθO)
where
μ0 = θ te, tr θ -1r Ytr,
θ0 = Kte,te + (θte,trθ-}rKtr,trθ-,]θ0∙,te) - (θte,trθ-:Rtr,te + Kte,trθ-}rθtr,te).
Here the K，s with subscripts are the ones in Theorem 3.1 and the θ s with subscripts are similar
restrictions of θ with training or test inputs. Furthermore, if H is the inverse gamma with shape
a and scale b, the exact marginal distribution of (f∞∞ (XK +i))i∈[L] is the following multivariate t
distribution:
(f∞(xκ+i))i∈[L]〜MVTl(2a, μ0, (b/a) ∙ θ0).	(5)
Proof. Assume that we condition on σv2 . Then, by Theorem B.3, as we send n to ∞ and then t to
∞, the conditioned random variable (fn(xκ+i; Vn), √nΨj⅛t)))i∈[L] | σ22 converges in distribution
to the following random variable:
(f∞(xκ+i))i∈[L] I 端〜N(μ∞, ∑∞),
where
μ∞ = θte,trθtr,trγte,
∑∞ = σV (Kte,te + θte,trθt⅛Ktr,trθ-,trθtr,te - (θte,trθ-‰e + Kte,trθ⅛θtr,te)).
Thus, by Lemma B.4, if we remove the conditioning on σv2 (i.e., we marginalize overσv2), we get the
convergence of the unconditioned (fn (xκ+i; Vn), √n ψf)))i∈[L] to the following random variable:
σ 〜H,	(f∞(xκ+i))i∈[L]〜N(μ0,σVθ0)
where
μ0 = θte,trθ -,1rγr,
θ = Kte,te + (θte,trθtr,trKtr,tr<θtr,trθtr,tej 一 (θte,trθtr,trKtr,te + Kte,trθ/θtr,tej .
In particular, when H = InvGamma(a, b), the exact marginal distribution of (f∞∞(xK+i))i∈[L] has
the following form by Lemma B.2:
MVT (2α,μ0,bθ).
□
Lemma B.4. Let (Xn)n∈N be a sequence of p-dimensional random vectors, X be a p-dimensional
random variable, and Y be a random variable. If Xn | Y converges in distribution to X | Y almost
surely (with respect to the randomness ofY) as n tends to ∞, then Xn converges in distribution to
X.
17
Published as a conference paper at ICLR 2022
Proof. Let h : Rp → R be a bounded continuous function. We have to show that
lim E[h(Xn)] = E[h(X)].
n→∞
The following calculation shows this equality:
lim E[h(Xn)] = lim E[E[h(Xn) | Y]] = E h lim E[h(Xn) | Y]i = E[ E[h(X) | Y]]
n→∞	n→∞	n→∞
= E[h(X)].
The first and last equalities follow from the standard tower law for conditional expectation, the
second equality uses the dominated convergence theorem and the boundness of h, and the third
equality uses the fact that Xn | Y converges in distribution to X |Y almost surely.	□
C Stochastic Variational Gaus sian Process
Let (X, y) be training points and (X*,y*) be test points. And let Z be inducing points. We want to
make q(f, fZ) which well approximates p(f, fZ |y) by maximizing ELBO. Then we will construct
q(f, fZ) = p(f |fZ)q(fZ) where p(f |fZ) = N(f|KXZKZ-Z1fZ,KXX - KXZKZ-Z1KZX) and
q(fz) = N(fz∖μ, ∑). Then
logp(y) ≥ Ef,fz〜q(f,fz)[logp(yf,fz)] - KL(q(fz)MfZ)).
Here we can calculate likelihood term as
I log p(y∣f )N(f; Aμ,A∑A> + D)df
where A = KX z Kz-z1 , B = KXX - KX z Kz-z1 KzX .
Also we can calculate KL term as
1
2
det Kzz
log(F^)- nz
+ Tr{Kzz ∑} + μ>κ-z μ
We find optimal μ, Σ and inducing points Z by using stochastic gradient descent. And with these
optimal values, We can calculate predictive distribution p(f* |y).
p(f*∣y) = / / p(f* ,f,fz ∣y)df dfz
=/ ∕p(f* lf,fz )q(f,fz )df dfz
=/ ∕p(f* lf,fz )p(f |fz )q(fz )df dfz
=/ p(f*lfz )q(fz )dfz.
This equation can be write as p(f*∣y) = N(f*; K*zKZZμ,K*zKZZ∑(K*zKZZ)> + K** -
κ*z KzZ K>z).
Now ifwe use reparametrization trick at fz, we can write fz = Lu, where L is the lower triangular
matrix from the cholesky decomposition of the matrix Kzz. Then
p(fz) =N(fz;0,LL>) =N(fz;0,Kzz)
q(fz) = N (fz ； Lμu,L∑u L>).
In this case ELBO changes into
/ log(p(y∣f ))N(f; ALμu,AL∑uL>A> + B)df - KL(N(u; 〃“，∑u)∣∣N(u;0,I)).
With softmax likelihood, we can use MC approximation which is
1N T B
ELBO ` TBEElogp(yi∣fi(t)) - KL(N(u;μu, ∑u)∣∣N(u；o,i))
t=1 i=1
18
Published as a conference paper at ICLR 2022
where B is minibatch and T is sample number of f which sampled from
N (f; AB Lμu, AB lςul>AB + BB ).
Here AB = KXB Z KZ-Z1 and BB = KXBXB - KXB Z KZ-Z1 KZXB . Finally we can calculate predic-
tive distribution with this reparametrization trick.
p(f*∣y) = / p(f*lfz )q(fz )dfz
=/N(f*; K*zKZZfz,K** - K*zK-ZKZ*)N(fz； Lμu,L∑uLτ)dfz
=N(f*; K*zK-zLμu, K*zK-zL∑uLτ(K*zKZZ)> + K** - K*zKZZKTZ)
Thus the final predictive distribution p(y* |x*) is
M
log p(y*∣χ*) = log∏p(yj lxl)
j
M
=X log p(yj∣χ?)
j
M
Xlog
j
p(yj lf*)p(f*∣y)df*
MN
` X log N Xp(yj 优)
where fS are sampled from N(f*; K*zK-ZLμu, K*zK-ZL∑uLτ(K*zK-Z)> + K** -
K*z KZZ KTZ).
D STOCHASTIC VARIATIONAL STUDENT t PROCESS
We want to make q(f, fz, σ2) which well approximates p(f, fz, σ2 |y) by maximizing ELBO. First
p(f, fz,σ2) = p(f∣fz,σ2)p(fz∣σ2)p(σ2) where p(f∣fz,σ2) = N(f∣KχzKZZ fz,σ2(Kχχ -
KχzKZZKzx)), p(fz∣σ2) = N(fz∣0,σ2Kzz) andp(σ2) = Γ-1(σ2∣α,β). And q(f, fz,σ2) =
p(ffz,σ2)q(fz∣σ2)q(σ2) where q(fz∣σ2) = N(fz∣μ,σ2Σ) and q(σ2) = Γ-1 (σ2|a,b). Then
ELBO can be computed as
logp(y) ≥ Ef,fz,σ2〜q(f,fz,σ2)[logp(y∣f, fz,σ2)] - KL(q(f, fz,σ2)∣∣p(f,fz,σ2)).
Here KL divergence can be reformulated as
KL(q(f, fz,σ2)∣∣p(f,fz,σ2)) = Z Z q(fz,σ2)log q(fZ`)dfzdσ2
p(fz, σ2 )
=KL(q(fz ,σ2)∣∣p(fz ,σ2)).
Now let’s compute likelihood part first.
Eff^fz ,σl2 〜q(f,fz ,σ2)[log p(y|f, fZ
σ2)] =	q(f) logp(y|f)df
where q(f) = / ∕p(f∣fz,σ2)q(fz∣σ2)q(σ2)dfzdσ2. And this is
q(f) = MVT(f |2a, KχzKZZμ, a(KXZKZZ∑K-ZKZX + KXX — KXZKZZKZX)).
19
Published as a conference paper at ICLR 2022
By slightly modifying the KL divergence between normal-gamma distributions in Soch & Allefeld
(2016), we get
KL(q(fz, σ2) ||p(fz, σ2))=ZZ 虱f, σ2)log f2dfz dσ2
=2 bμTK-Zμ + 2Tr(K-Z ς) + 2log
|KZZ |	nZ
l∑l 2
b
+ α log 石
β
—
-log fTT—T + (a - α)ψ(a) + (β - b)-j-
Γ(α)	b
where ψ(∙) is digamma function. Now if We use reparametrization trick at fz, We can write fz =
Lu, where L is the lower triangular matrix from the cholesky decomposition of the matrix KZZ .
Then
p(fz∣σ2) = N (fz [0,σ2LLT) = N (fz [0,σ2Kzz)
q(fz∣σ2) = N(fz； Lμu, σ2L∑uLτ).
In this case ELBO changes into
l log(p(y∣f ))MVT(f ； 2a, ALμu, b(AL∑uLτAτ + B))df
a
-KL(N(u; μu, σ2∑u)Γ-1(σ2; a, b)||N(u; 0, σ21)Γ-1(σ2,α, β)).
With softmax likelihood, we can use MC approximation which is
1NT B
ELBO ` TBE Elog p(yi|fi(t))
t=1 i=1
-KL(N(u; μu, σ2∑u)Γ-1(σ2; a, b)∣∣N(u; 0, σ2I)Γ-1(σ2,α, β))
where B is minibatch and T is sample number of f which sampled from
MVT(f ； 2a, ABLμu, — (AbL∑uLTAB + BB)).
a
Here AB = KXB z Kz-z1 and BB = KXBXB - KXB z Kz-z1 KzXB . Finally we can calculate predic-
tive distribution with this reparametrization trick.
p(f*∣y)
=// p(f*fz ,σ2)q(fz∣σ2 )q(σ2)dfz dσ2
=//N(f*; K*zKZZfz,σ2(K**- K*zK-ZKz*))N(fz; Lμu,σ2(L∑uLτ))Γ-1(a, b)dfzdσ2
=MVT(f*; 2a, K*zKZZL〃“，!(K*zKZZL∑uL>(K*zKZZ)> + K** - K*zKZZKTZ))∙
Thus the final predictive distribution p(y* |x*) is
M
log p(y* |x*) = log	p(y*j|xj*)
j
M
= Xlogp(y*j|xj*)
j
M
= Xlog	p(y*j|f*)p(f*|y)df*
MN
` X log N Xp(yj lf*)
j
i
where fis are sampled from MVT(f*;2a,K*zK-ZLμu, a(K*zK-ZL∑uLτ(K*zK-Z)> +
K**- K*z KZZ KT)).
20
Published as a conference paper at ICLR 2022
E Inference Algorithms
E.1 Inference for Regression
In order to calculate the predictive posterior distribution for an intractable marginal distribution, we
can use self-normalized importance sampling. Consider a scale mixture of NNGPs with a prior H on
the variance σv2. Assume that we would like to estimate the expectation of h(y) for some function
h : R → R where the random variable y is drawn from the predictive posterior of the mixture at
some input x ∈ RI under the condition on Dtr. Then, the expectation of h(y) is
E[h(y)] =
h(y)p(y|Dtr)dy
/ / h(y)p(y∣Dtr,σV)p(σV∣D∕dσVdy
h(y)p(y|Dtr,
2) P9V,Yrlxt^
σv)	P(WXtr)
dσv2dy
Z ZZ h(y)p(y∣Dtr,σV)p(σp,(Yr)Xtr)P(σV)dσVdy
where Z = /p(σV,Kr∣Xu)dσV. We can approximate Z as follows:
Z = Z p(-Xtr)p(σV)dσV
p(σv2 )	v v
` 1 X P(βi, Ytr|Xtr)
—N =	P(βi)
1N
=NEp(YrIXtrU
i=1
where βi s are sampled independently from H. Using this approximation, we can also approximate
expectation of h(y) as follows:
E[h(y)] = 1 Z Z h(y)p(y∣Dtr,σV)p(σv, YrXr)p(σV)dσVdy
Z	v	p(σv2 )	v v
N
` XPNi- h(yi)
i=1	j=1 wj
where the yi’s are sampled from the posterior of the Gaussian distribution and the wi’s are the
corresponding importance weights for all i ∈ [N]:
I	I
βi 〜H,	Wi = N(Ytr； 0,βiKtr,tr),	Ji 〜N(Kχ,trKtr,trYtr, βi(Kχ,x - Kχ,trKtr,trKtr,x)).
The K is the covariance matrix computed with test input as in Theorem 3.1. To speed UP calculation,
we removed duplicated calculations in our importance weights and also during the sampling of the
yi’s. First, for importance weights, we observe that the log likelihood ofβi has the form:
logN(Ytr; 0, βiKtr,tr) = -F log(2π) - - log det(Ktr,tr)-^log(禺)———Y^>Ktr,trYtr,
2	2	2	2βi
and the three terms K log(2∏), 1 log det(Kτ,tr), and Y^>K-1 YT here are independent with βi. Thus,
using this observation, we compute these three terms beforehand only once, and calculate the log
likelihood of each sample βi in O(1) time. Next, for sampling the yi’s, we first draw N samples
Jis from N(0, Kχ,χ - Kχ,trK-,lτKtr,χ), then multiply each of these samples with √βi, and finally
add Kχ,trK-jYr to each of the results. Since Kχ,trK^⅛ + √βiyi 〜N(Kx,trK-jKr, βi (Kχ,x -
-1— 1
Kx,trKtr,trKtr,x)) for all i ∈ [N], we can use these final outcomes of these computations as the Ji’s.
21
Published as a conference paper at ICLR 2022
Table 4: Experimental results of Classification with Gaussian Likelihood
Dataset	Accuracy	NNGP	Inverse Gamma	Burr Type XII
MNIST	96.8 ± 0.2	9.29 ± 0.002	4.89 ± 0.001	4.96 ± 0.001
+ Shot Noise	94.9 ±0.4	9.32 ± 0.001	4.88 ± 0.001	4.96 ± 0.001
+ Impulse Noise	84.4 ± 2.3	9.63 ± 0.000	5.17 ± 0.001	5.25 ± 0.001
+ Spatter	95.4 ± 0.4	9.27 ± 0.001	4.44 ± 0.002	4.49 ± 0.007
+ Glass Blur	90.5 ±0.7	9.12 ± 0.001	4.20 ± 0.005	4.00 ± 0.017
+ Zigzag	84.9 ± 1.5	9.51 ± 0.001	4.62 ± 0.006	4.49 ± 0.021
EMNIST	70.4 ± 0.9	9.40 ± 0.001	4.15 ± 0.004	4.45 ± 0.013
Fashion MNIST	61.3 ± 5.1	9.34 ± 0.002	4.96 ± 0.002	4.97 ± 0.002
KMNIST	81.1 ± 1.3	9.48 ± 0.001	4.60 ± 0.002	4.34 ± 0.013
SVHN	42.5 ± 1.5	6.01 ± 0.062	4.16 ± 0.013	4.15 ± 0.009
E.2 Time Complexity Analysis
For time complexity, as we mentioned in Appendix E, our posterior-predictive algorithm based on
importance sampling does not induce significant overhead thanks to the reuse of the shared terms
for calculation. More specifically, our algorithm with K sample variances spends O(K + N3)
time, instead of O(KN3), for computing a posterior predictive, where N is the number of training
points. Compare this with the usual time complexity O(N 3) of the standard algorithm for Gaussian
processes. When it comes to SVGP and SVTP, one update step of both SVGP and SVTP takes
O(BM2 + M3 ) time, where B is the number of the batch size of the input dataset and M is the
number of the inducing points.
F Classification with Gaussian Likelihood
We compare the nngps and the scale mixture of nngps with Inverse Gamma prior and Burr Type
XII prior for the classification tasks. Following the convention (Lee et al., 2018; Novak et al., 2018;
Garriga-Alonso et al., 2019), we treat the classification problem as a regression problem where
the regression targets are one-hot vectors of class labels and computed the posteriors induced from
squared-loss. We searched hyperparameters refer to Adlam et al. (2020) and we choose both c and
k from [0.5, 1., 2., 3., 4.]. We do not particularly expect the scale-mixtures of NNGPs to outperform
nngp in terms of predictive accuracy, but we expect them to excel in terms of calibration due to
its flexibility in describing the variances of class probabilities. To better highlight this aspect, for
MNIST data, we trained the models on clean training data but tested on corrupted data to see how
the models would react under such distributional shifts. The results are summarized in Table 4. Due
to the limitation of the resources for computing full data kernel matrix, we use 5000 samples as train
set, and 1000 samples as test set. For all datasets, the scale-mixture of nngps outperform nngp in
terms of NLL.
G	Experimental Details
In Fig. 1, we used the inverse gamma prior with hyperparameter setting α = 2 and β = 2 for
the experiments validating convergence of initial distribution (Theorem 3.1) and last layer training
(Theorem 3.2). For the full layer training (Theorem 3.3), we used α = 1 and β = 1.
For the regression experiments, we divide each dataset into train/validation/test sets with the ratio
0.8/0.1/0.1. We performed gradient descent in order to update parameters of our models except
number of layers which is discrete value. We referred Adlam et al. (2020) for initializing parameters
of nngps. We choose the best hyperparameters based on validation NLL values and measured NLL
values on the test set with permuted train sets.
For the classification experiments, we divide each dataset into train/test sets as provided by Ten-
sorFlow Datasets2, and further divide train sets into train/validation sets with the ratio 0.9/0.1. We
choose the best hyperparameters based on the validation NLL values, and measure NLL and accu-
racy values on the test set with different initialization seeds. To measure the uncertainty calibration
2https://www.tensorflow.org/datasets
22
Published as a conference paper at ICLR 2022
Table 5: RMSE values on UCI dataset. (m, d) denotes number of data points and features, respec-
tively. We take results from Adlam et al. (2020) except our model.
Dataset	(m, d)	PBP-MV	Dropout	Ensembles	RBF	NNGP	Ours
Boston Housing	(506, 13)	3.11 ± 0.15	2.90 ± 0.18	3.28 ± 1.00	3.24 ± 0.21	3.07 ± 0.24	3.30 ± 0.03
Concrete Strength	(1030, 8)	5.08 ± 0.14	4.82 ± 0.16	6.03 ± 0.58	5.63 ± 0.24	5.25 ± 0.20	5.08 ± 0.14
Energy Efficiency	(768, 8)	0.45 ± 0.01	0.54 ± 0.06	2.09 ± 0.29	0.50 ± 0.01	0.57 ± 0.02	0.44 ± 0.03
Kin8nm	(8192, 8)	0.07 ± 0.00	0.08 ± 0.00	0.09 ± 0.00	0.07 ± 0.00	0.07 ± 0.00	0.07 ± 0.00
Naval Propulsion	(11934, 16)	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
Power Plant	(9568, 4)	3.91 ± 0.04	4.01 ± 0.04	4.11 ± 0.17	3.82 ± 0.04	3.61 ± 0.04	3.53 ± 0.04
Wine Quality Red	(1588, 11)	0.64 ± 0.01	0.62 ± 0.01	0.64 ± 0.04	0.64 ± 0.01	0.57 ± 0.01	0.59 ± 0.01
Yacht Hydrodynamics	(308, 6)	0.81 ± 0.06	0.67 ± 0.05	1.58 ± 0.48	0.60 ± 0.07	0.41 ± 0.04	0.35 ± 0.04
Table 6: Additional regression results for Burr Type XII prior distribution.
Dataset	(m, d)	NNGP	Inverse Gamma prior	Burr Type XII prior
Boston Housing	(506, 13)	2.65 ± 0.13	2.72 ± 0.05	2.77 ± 0.02
Concrete Strength	(1030, 8)	3.19 ± 0.05	3.13 ± 0.04	3.29 ± 0.09
Energy Efficiency	(768, 8)	1.01 ± 0.04	0.67 ± 0.04	0.70 ± 0.04
Kin8nm	(8192, 8)	-1.15 ± 0.01	-1.18 ± 0.01	-1.23 ± 0.01
Naval Propulsion	(11934, 16)	-10.01 ± 0.01	-8.04 ± 0.04	-4.38 ± 0.05
Power Plant	(9568, 4)	2.77 ± 0.02	2.66 ± 0.01	2.78 ± 0.01
Wine Quality Red	(1588, 11)	-0.98 ± 0.06	-0.77 ± 0.07	-0.16 ± 0.05
Yacht Hydrodynamics	(308, 6)	1.07 ± 0.27	0.17 ± 0.25	0.60 ± 0.15
of the model, we used the corrupted variants of the dataset and made three new dataset from the base
datasets. For the corrupted variants, we trained the model on the original version of the datasets, and
tested on the provided corrupted verions. For the out-of-distribution variants, we removed 3 classses
on the train set. For imbalance variants, we limited the samples per class with exponential ratio on
the train set. For noisy label variants, we selected 50% of labels and assigned uniformly selected
new labels from the train set. Except the corrupted variants, we tested the model on the original test
set.
For the classfication by categorical likelihood experiments, we use the CNN model with two layers
to compute the kernel. Due to the limitations of computing resources, we can only use 400 inducing
points for the variational inference which leads slight degradation on the performance. For the
classification by finite model ensemble experiments, each CNN layer has 128 channels.
H Additional Experiments
The experiment code is available at GitHub3. We used a server with Intel Xeon Silver 4214R CPU
and 128GB of RAM to evaluate the classification with Gaussian likelihood experiment, and used
NVIDIA GeForce RTX 2080Ti GPU to conduct other experiments.
Impact of the prior hyperparameters With same experimental setting with Section 4.1, in order
to inspect the impact of the prior hyperparameters, we sampled 1000 models for initial, last layer
training and full layer training. In Fig. 2, here we can empirically see that consistently with the
theory, if α is smaller, the output distribution is more heavy-tailed.
Full-layer training correspondence for ResNet Even though we only proved the general training
result (Theorem 3.3) only for the fully-connected neural networks, we experimentally found that
ResNet also shows a behavior predicted by our theorem. The empirical validation with for ResNet
is shown in Fig. 3. We set α = 4 and β = 4 for the inverse gamma prior in this experiment.
Additional results of regression with Gaussian likelihood In addition to Table 1, we also mea-
sured the root mean square error values on the test set. Table 5 summarizes the results. The results
other than ours are borrowed from Adlam et al. (2020) and we use same settings as described in
Section 4.2.
3 https://github.com/Anonymous0109/Scale- Mixtures- of- NNGP
23
Published as a conference paper at ICLR 2022
Initial
Last layer training
Full layer training
Figure 2: Impact of the prior hyperparameters for fully connected neural network initial, last layer
training and full layer training.
Figure 3: (left) Correspondence between wide finite ResNet model vs theoretical limit. (right)
impact of the prior hyperparameters.
Additional regression experiment As an additional regression experiments, we test the models
which use Burr Type XII distribution as its last layer variance’s prior. We use Appendix E in order
to calculate the predictive posterior distribution. Our results are in Table 6.
I	Additional Information
We refer to the source of each dataset through footnotes with URL links. We don’t use any data
which obtained from people and don’t use any data which contains personally identifiable informa-
tion or offensive content.
24