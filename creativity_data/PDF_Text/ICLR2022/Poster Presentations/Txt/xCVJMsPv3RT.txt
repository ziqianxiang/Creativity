Published as a conference paper at ICLR 2022
Dropout Q-Functions
for Doubly Efficient Reinforcement Learning
Takuya Hiraoka 1,2, Takahisa Imagawa 2, Taisei Hashimoto 2,3, Takashi Onishi 1,2,
Yoshimasa Tsuruoka 2,3
1	NEC Corporation
2	National Institute of Advanced Industrial Science and Technology
3	The University of Tokyo
{takuya-h1, takashi.onishi}@nec.com, imagawa.t@aist.go.jp,
{hashimoto-taisei388@g.ecc, tsuruoka@logos.t}.u-tokyo.ac.jp
Ab stract
Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has re-
cently achieved state-of-the-art sample efficiency on continuous-action reinforce-
ment learning benchmarks. This superior sample efficiency is made possible
by using a large Q-function ensemble. However, REDQ is much less com-
putationally efficient than non-ensemble counterparts such as Soft Actor-Critic
(SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient,
we propose a method of improving computational efficiency called DroQ, which
is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our
dropout Q-functions are simple Q-functions equipped with dropout connection
and layer normalization. Despite its simplicity of implementation, our experimen-
tal results indicate that DroQ is doubly (sample and computationally) efficient.
It achieved comparable sample efficiency with REDQ, much better computational
efficiency than REDQ, and comparable computational efficiency with that of SAC.
1	Introduction
In the reinforcement learning (RL) community, improving sample efficiency of RL methods has
been important. Traditional RL methods have been shown to be promising for solving complex
control tasks such as dexterous in-hand manipulation (OpenAI et al., 2018). However, RL methods
generally require millions of training samples to solve a task (Mendonca et al., 2019). This poor
sample efficiency of RL methods is a severe obstacle to practical RL applications (e.g., applications
on limited computational resources or in real-world environments without simulators). Motivated
by these issues, many RL methods have been proposed to achieve higher sample efficiency. For
example, Haarnoja et al. (2018a;b) proposed Soft Actor-Critic (SAC), which achieved higher sample
efficiency than the previous state-of-the-art RL methods (Lillicrap et al., 2015; Fujimoto et al., 2018;
Schulman et al., 2017).
Since 2019, RL methods that use a high update-to-data (UTD) ratio to achieve high sample effi-
ciency have emerged. The UTD ratio is the number of updates taken by the agent compared to the
number of actual interactions with the environment. A high UTD ratio promotes sufficiently train-
ing Q-functions within a few interactions, which leads to sample-efficient learning. Model-Based
Policy Optimization (MBPO) (Janner et al., 2019) is a seminal RL method that uses a high UTD
ratio of 2θ-40 and achieves significantly higher sample efficiency than SAC, which uses a UTD
ratio of 1. Encouraged by the success of MBPO, many RL methods with high UTD ratios have been
proposed (Shen et al., 2020; Lai et al., 2020).
With such methods, randomized ensembled double Q-learning (REDQ) proposed by Chen et al.
(2021b) is currently the most sample-efficient method for the MuJoCo benchmark. REDQ uses a
high UTD ratio and large ensemble of Q-functions. The use ofa high UTD ratio increases an estima-
tion bias in policy evaluation, which degrades sample-efficient learning. REDQ uses an ensemble
of Q-functions to suppress the estimation bias and improve sample efficiency. Chen et al. (2021b)
demonstrated that the sample efficiency of REDQ is equal to or even better than that of MBPO.
1
Published as a conference paper at ICLR 2022
However, REDQ leaves room for improvement in terms of computational efficiency. REDQ runs
1.1 to 1.4 times faster than MBPO (Chen et al., 2021b) but is still less computationally efficient than
non-ensemble-based RL methods (e.g., SAC) due to the use of large ensembles. In Section 4.2, we
show that REDQ requires more than twice as much computation time per update, and much larger
memory. Computational efficiency is important in several scenarios, e.g., in RL applications with
much lighter on-device computation (e.g. mobile phones or other light-weight edge devices) (Chen
et al., 2021a), or in situations in which rapid trials and errors are required for developing RL agents
(e.g., hyperparameter tuning or proof of concept for RL applications with limited time resources).
Therefore, RL methods that are superior not only in terms of sample efficiency but also in computa-
tional efficiency are preferable.
We propose a method of improving computational efficiency. Our method is called DroQ and is
a REDQ variant that uses a small ensemble of dropout Q-functions, in which dropout (Srivastava
et al., 2014) and layer normalization (Ba et al., 2016) are used (Section 3). We experimentally
show that DroQ is doubly (computationally and sample) efficient: (i) DroQ significantly improves
computational efficiency over REDQ (Section 4.2) by more than two times and (ii) achieves sample
efficiency comparable to REDQ (Section 4.1).
Although our primary contribution is proposing a doubly efficient RL method, we also make three
significant contributions from other perspectives:
1.	Simplicity of implementation. DroQ can be implemented by basically adding a few lines of
readily available functions (dropout and layer normalization) to Q-functions in REDQ (and SAC).
This simplicity enables one to easily replicate and extend it.
2.	First successful demonstration of the usefulness of dropout in high UTD ratio settings. Pre-
vious studies incorporated dropout into RL (Gal & Ghahramani, 2016; Harrigan, 2016; Moerland
et al., 2017; Gal et al., 2016; 2017; Kahn et al., 2017; Jaques et al., 2019; He et al., 2021) (see
Section 5 for details). However, these studies focused on low UTD ratio settings (i.e., UTD ratio
≤ 1). Dropout approaches generally do not work as well as ensemble approaches (Osband et al.,
2016; Ovadia et al., 2019; Lakshminarayanan et al., 2017; Durasov et al., 2021). For this reason,
instead of dropout approaches, ensemble approaches have been used in RL with high UTD ratio
settings (Chen et al., 2021b; Janner et al., 2019; Shen et al., 2020; Hiraoka et al., 2021; Lai et al.,
2020). In Section 4, we argue that DroQ achieves almost the same or better bias reduction ability and
sample/computationally efficiency compared with ensemble-based RL methods in high UTD ratio
settings. This sheds light on dropout approaches once again and promotes their use as a reasonable
alternative (or complement) to ensemble approaches in high UTD ratio settings.
3.	Discovery of engineering insights to effectively apply dropout to RL. Specifically, we discov-
ered that the following two engineering practices are effective in reducing bias and improving sample
efficiency: (i) using the dropout and ensemble approaches together for constructing Q-functions (i.e.,
using multiple dropout Q-functions) (Section 4.1 and Appendix A.3); and (ii) introducing layer nor-
malization into dropout Q-functions (Section 4.3 and Appendices D, E, and F). These engineering
insights were not revealed in previous RL studies and would be useful to practitioners who attempt
to apply dropout to RL.
2	Preliminaries
2.1	Maximum Entropy Reinforcement Learning (maximum entropy RL)
RL addresses the problem of an agent learning to act in an environment. At each discrete time step
t, the environment provides the agent with a state st, the agent responds by selecting an action at,
and then the environment provides the next reward rt and state st+1. For convenience, as needed,
we use the simpler notations of r, s, a, s0, and a0 to refer to a reward, state, action, next state, and
next action, respectively.
We focus on maximum entropy RL, in which an agent aims to find its policy that maximizes the
expected return with an entropy bonus: arg max PT=O γtEst,at [rt + αH(π(∙∣%))]. Here, π : S ×
π
A → [0, ∞) is a policy and H is entropy. Temperature α balances exploitation and exploration and
affects the stochasticity of the policy.
2
Published as a conference paper at ICLR 2022
Algorithm 1 REDQ
1:	Initialize policy parameters θ, N Q-function parameters φ%, i = 1,…,N, and empty replay
buffer D. Set target parameters φi J φi, for i = 1,...., N.
2:	repeat
3:	Take action at 〜∏θ (∙∣st); Observe reward r, next state St +ι; DjD S( st,at,rt,st +ι).
4:	for G updates do
5:	Sample a mini-batch B = {(s, a, r, s0)} from D.
6:	Sample a set M of M distinct indices from {1, 2, ..., N}.
7:	Compute the Q target y (same for all N Q-functions):
y = r + Y (minQφi(s0,a0) - αlog∏θ(a0∣s0) ʌ , a0 〜∏θ(∙∣s0)
8:	for i = 1, ..., N do
9:	Update φi with gradient descent using
Nφ卷 X	(Qφi (s,a) - y)2
(s,a,r,s0)∈B
10:	Update target networks with φi J ρφi + (1 — P) φi.
11:	Update θ with gradient ascent using
a 〜∏θ(∙∣s)
2.2	Randomized Ensembled Double Q-Learning (REDQ)
REDQ (Chen et al., 2021b) is a sample-efficient model-free method for solving maximum-entropy
RL problems (Algorithm 1). It has two primary components to achieve high sample efficiency.
1.	High UTD ratio: It uses a high UTD ratio G, which is the number of updates (lines 4-10) taken
by the agent compared to the number of actual interactions with the environment (line 3). The high
UTD ratio promotes sufficient training of Q-functions within a few interactions. However, this also
increases an overestimation bias in the Q-function training, which degrades sample-efficient learn-
ing (Chen et al., 2021b).
2.	Ensemble of Q-functions: To reduce the overestimation bias, it uses an ensemble of N Q-
functions for the target to be minimized (lines 6-7). Specifically, a random subset M of the en-
semble is selected (line 6) then used for calculating the target (line 7). The size of the subset M
is kept fixed and is denoted as M . In addition, each Q-function in the ensemble is randomly and
independently initialized but updated with the same target (lines 8-9). Chen et al. (2021b) showed
that using a large ensemble (N = 10) and small subset (M = 2) successfully reduces the bias.
Although using a large ensemble of Q-functions is beneficial for reducing bias and improving sample
efficiency, this makes REDQ computationally intensive. In the next section, we discuss reducing the
ensemble size.
3	Injecting Model Uncertainty into Target with Dropout
Q-functions
In this section, we discuss replacing the large ensemble of Q-functions in REDQ with a small ensem-
ble of dropout Q-functions. We start our discussion by reviewing what the ensemble in REDQ does
from the viewpoint of model uncertainty injection. Then, we propose to use dropout for model un-
certainty injection instead of the large ensemble. Specifically, we propose (i) the dropout Q-function
that is a Q-function equipped with dropout and layer normalization, and (ii) DroQ, a variant of
REDQ that uses a small ensemble of dropout Q-functions. Finally, we explain that the size of the
ensemble can be smaller in DroQ than in REDQ.
3
Published as a conference paper at ICLR 2022
Figure 1: Dropout Q-function implementation (left part) and how dropout Q-functions are used in
target (right part). Dropout Q-function implementation: Our dropout function is implemented by
modifying that used by Chen et al. (2021b). Our modification (highlighted in red) is adding dropout
(Dropout) and layer normalization (LayerNorm). “Weight” is a weight layer and “ReLU” is the
activation layer of rectified linear units. Parameters φi represent the weights and biases in weight
layers. How dropout Q-functions are used in target: M dropout Q-functions are used to calculate
the target value as mini=1,…,m QDr,φi (s, a).
We first explain our insight that, in REDQ, model (Q-function parameters’) uncertainty is injected
into the target. In REDQ, the subset M of N Q-functions is used to compute the target value
mini∈M Qφi (S,a0) (lines 6-7 in Algorithm 1). This can be interpreted as an approximation for
Eφ1,…,φM [mini=1,…M Qφi (S, a0)], the expected target value with respect to model uncertainty:
E
φ1,...,φM
min Qφ (s , a ) = / min Qφ (s , a ) TJ	P(φi)dφi
i=1,...,M φi	i=1,...,M φi	i i
i=1,...,M
≈ / ._min“ Qφi (s,a)	∏ qen(φi)dφi
i=1,...,M
i=1,...,M

mM Q φi ( S ,a )
| {z }
(used in line 7 in Algorithm 1)
On the left hand side in the second line, the model distribution p(φi) is replaced with a proposal
distribution qen, which is based on resampling from the ensemble in line 6 in Algorithm 11. On the
right hand side (RHS) in the second line, the expected target value is further approximated by one
sample average on the basis of qen. The resulting approximation is used in line 7 in Algorithm 1. In
the remainder of this section, we discuss another means to approximate the expected target value.
We use M dropout Q-functions QDr for the target value approximation (Figure 1). Here, QDr is a
Q-function that has dropout connections (Srivastava et al., 2014). The left part of Figure 1 shows
QDr implementation by adding dropout layers to the Q-function implementation used by Chen et al.
(2021b). Layer normalization (Ba et al., 2016) is applied after dropout for more effectively using
dropout as with Vaswani et al. (2017); Xu et al. (2019). By using QDr, the target value is approxi-
mated as
E
φ1,...,φM
min
i=1,...,M
Q φ i (S , a )
≈ ]i=minMqφi(s'a')	Y qdrr(φidφ
i=1,...,M
≈ min QDr φ. (S,a').
i=1,...,M Dr,φi
Instead of qen, a proposal distribution qdr based on the dropout is used on the RHS in the first line. The
expected target value is further approximated by one sample average on the basis of qdr in the second
line. We use the resulting approximation mini=1,…M QDrφ i (S, a0) for injecting model uncertainty
into the target value (the right part of Figure 1). For calculating mini=1,…M QDrφi (S, a0), We use
M dropout Q-functions, which have independently initialized and trained parameters φ1,..., φM.
Using M dropout Q-functions improves the performance of DroQ (our RL method described in the
next paragraph), compared with that using a single dropout Q-function (further details are given in
A.3).
We now explain DroQ, in which QDr is used for considering model uncertainty. The algorithmic
description of DroQ is shown in Algorithm 2. DroQ is a variant of REDQ, and the modified parts
1We assume that ∀=ι,...,n, φi independently follow an identical distribution.
4
Published as a conference paper at ICLR 2022
Algorithm 2 DroQ
1:	Initialize policy parameters θ, M Q-function parameters φ%, i = 1 ,...,M, and empty replay
buffer D. Set target parameters φi J φi, for i = 1,…,M.
2:	repeat
3:	Take action at 〜∏θ(∙∣st). Observe reward r, next state St+ι; D J D S(st, at, rt, st +ι).
4:	for G updates do
5:	Sample a mini-batch B = {(s, a, r, s0)} from D.
6:	Compute the Q target y for the dropout Q-functions:
y = r + Y I . min QDr,φi (s, a0) - α logπθ(a0∣s0) ) , a0 〜∏θ(∙∣s0)
i=1,...,M
7:	for i = 1, ..., M do
8:	Update φi with gradient descent using
“φ B	X	(QDr,φi (s,a) - y )2
(s,a,r,s0 )∈B
9:	Update target networks with φi J ρφi + (1 — P)φi.
10:	Update θ with gradient ascent using
1	1M
Nθ B E (M =, Q Dr ,φi ( s,a) - a log πθ (als )J, a 〜πθ (∙ls ) * 4
from REDQ are highlighted in red in the algorithm. In line 6, QDr is used to inject model uncertainty
into the target, as we discussed in the previous paragraph. In lines 8 and 10, M dropout Q-functions
are used instead of N Q-functions (N ≥ M) to make DroQ more computationally efficient.
The ensemble size of the dropout Q-functions for DroQ (i.e., M) should be smaller than that of
Q-functions for REDQ (i.e., N). M for DroQ is equal to the subset size for REDQ, which is not
greater than N. In practice, M is much smaller than N (e.g., M = 2 and N = 10 in Chen et al.
(2021b)). This reduction in the number of Q-functions makes DroQ more computationally efficient.
Specifically, DroQ is faster due to the reduction in the number of Q-function updates (line 8 in
Algorithm 2). DroQ also requires less memory for holding Q-function parameters. In Section 4.2,
we show that DroQ is computationally faster and less memory intensive than REDQ.
4 Experiments
We conducted experiments to evaluate and analyse DroQ. In Section 4.1, we evaluate DroQ’s perfor-
mance (sample efficiency and bias-reduction ability). In Section 4.2, we evaluate the computational
efficiency of DroQ. In Section 4.3, we present an ablation study for DroQ.
4.1 Sample efficiency and bias-reduction ability of DroQ
To evaluate the performances of DroQ, we compared DroQ with three baseline methods in MuJoCo
benchmark environments (Todorov et al., 2012; Brockman et al., 2016). Following Chen et al.
(2021b); Janner et al. (2019), we prepared the following environments: Hopper, Walker2d, Ant,
and Humanoid. In these environments, we compared the following four methods:
REDQ: Baseline method that follows the REDQ algorithm (Chen et al., 2021b) (Algorithm 1).
SAC: Baseline method that follows the SAC algorithm (Haarnoja et al., 2018a;b). To improve the
sample efficiency of this method, delayed policy update and high UTD ratio G = 20 were used, as
suggested by Chen et al. (2021b).
DroQ: Proposed method that follows Algorithm 2.
Double uncertainty value network (DUVN): Baseline method. This method is a DroQ variant that
injects model uncertainty into a target in the same manner as the original DUVN (Moerland et al.,
2017). The original DUVN uses the output of a single dropout Q-function QDrφ (s0, α0) without
5
Published as a conference paper at ICLR 2022
Table 1: Process times (in msec) per overall loop (e.g., lines 3-10 in Algorithm 2). Process times
per Q-functions update (e.g., lines 4-9 in Algorithm 2) are shown in parentheses.
	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2
-SAC-	910(870)二	870(835)=	888 (848)二	893 (854)=
REDQ	2328 (2269)	2339 (2283)	2336 (2277)	2400 (2340)
DUVN	664 (636)	762 (731)	733 (700)	-692 (660)-
DroQ	948 (905) 一	933 (892)一	954(913) 一	989 (946)一
layer normalization for target as y = r + YQd；$i (S, a0) 2. We introduce the original DUVN to
DroQ by modifying target on line 6 in Algorithm 2 as y = r + Y (Qd；3] (s0, a0) — alog∏θ(a0∣s0)).
Here, layer normalization is not applied in Qd；3] (s0, a0). We use this method to investigate the
performance of the existing single dropout Q-function (non-ensemble) method.
Following Chen et al. (2021b), we set the hyperparameters as G = 20, N = 10, and M = 2 for all
methods except DUVN (M = 1). More detailed hyperparameter settings are given in Appendix H.
The methods were compared on the basis of average return and estimation bias.
Average return: An average return over episodes. We regarded 1000 environment steps in Hopper
and 3000 environment steps in the other environments as one epoch, respectively. After every epoch,
we ran ten test episodes with the current policy and recorded the average return.
Average/std. bias: Average and standard deviation of the normalized estimation error (bias) of
Q-functions Q3i (Chen et al., 2021b). The error represents how significantly the Q-value estimate
differs from the true one. Formally, the error is defined as ∣Qπ(s, a) 一 Q(s, a)∣/E3值~∏ [Qπ(S, α)],
ʌ
where Qπ (s, a) is the true Q-value under the current policy π and Q(s, a) is its estimate. In our
experiment, Qπ (s, a) was approximated by the discounted Monte Carlo return obtained with π in
the test episodes. In addition, Q(s, a) was evaluated as Q(s, a) = 1 /N P = Qφ. (s, a) for SAC
and REDQ and as Q(s,a) = 1 /M PM=L1 QDr必(s, a) for DroQ and DUVN, respectively.
The comparison results (Figure 2) indicate that DroQ achieved almost the same level of perfor-
mance as REDQ. Regarding the average return, REDQ and DroQ achieved almost the same sample
efficiency overall. In Walker2d and Ant, their learning curves highly overlapped, and there was no
significant difference between them. In Humanoid, REDQ was slightly better than DroQ. In Hop-
per, DroQ was better than REDQ. In all environments except Hopper, REDQ and DroQ improved
their average return significantly earlier than SAC and DUVN. Regarding the bias, REDQ and DroQ
consistently kept the value-estimation bias closer to zero than SAC and DUVN in all environments.
4.2 Computational efficiency of DroQ
We next evaluated the computational efficiency of DroQ. We compared DroQ with the baseline
methods on the basis of the following criteria: (i) Process time required for executing methods;
(ii) Number of parameters of each method; (iii) Bottleneck memory consumption suggested
from the Pytorch profiler3. Bottleneck memory consumption is the maximum memory consumption
recorded when running the methods. For evaluation, we ran each method on a machine equipped
with two Intel(R) Xeon(R) CPU E5-2667 v4 and one NVIDIA Tesla K80.
Process times per update (numbers in parentheses in Table 1) indicate that DroQ runs more than
two times faster than REDQ. DroQ (and SAC) requires process times in the 800-900-msec range.
REDQ requires process times in the 2200-2300-msec range. Process times also show that learning
Q-functions is dominant in an overall loop. This suggests that using compact (e.g., small numbers
of) Q-functions is important for improving overall process times.
The number of parameters and bottleneck memory consumption of each method indicate that DroQ
is more memory efficient than REDQ. Regarding the numbers of parameters (Table 2), we can see
that those of DroQ (and SAC and DUVN) are about one-fifth those of REDQ. Note that the number
of parameters of DroQ is equal to that of SAC since DroQ and SAC use the same number (two)
of Q-functions. Regarding the bottleneck memory consumption (Table 3), we can see that that for
DroQ (SAC and DUVN) is about one-third that for REDQ. We can also see that the bottleneck
2Harrigan (2016); He et al. (2021) also propose similar targets.
3https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html
6
Published as a conference paper at ICLR 2022
Figure 2: Average return and average/standard deviation of estimation bias for REDQ, SAC, DUVN,
and DroQ. The horizontal axis represents the number of interactions with the environment (e.g., the
number of executions of line 3 of Algorithm 2). For each method, average score of five independent
trials are plotted as solid lines, and standard deviation across trials is plotted as transparent shaded
region.
Table 2: Number of parameters
	HoPPer-V2	Walker2d-v2	Ant-v2	HUmanoid-v2
^SAC-	141,826	146,434 =	152,578	166,402 =
REDQ	698,890	-721,930-	752,650	-821,770-
DUVN	139,778	-144,386-	150,530	164,354-
DroQ	141,826	146,434 —	152,578	166,402 —
memory consumption is almost independent of the environment. This is because one of the most
memory-intensive parts is the ReLU activation at the hidden layers in Q-functions4.
4In our experiment, the number of units in hidden layer is invariant over environments (Appendix H).
7
Published as a conference paper at ICLR 2022
Table 3: Bottleneck memory consumption (in megabytes) with methods. Three worst bottleneck
memory consumptions are shown in form of “1st worst bottleneck memory consumption / 2nd worst
bottleneck memory consumption / 3rd worst bottleneck memory consumption.
	HoPPer-V2	Walker2d-v2	Ant-v2	HUmanoid-v2
-SAC-	73 / 64 / 6尸	73 / 64 / 6广	73 / 64 / 6广	73 / 65 / 6『
REDQ	241 /211/200	241/211 /200	241 /211 /200	241/212/201
DUVN	-72/71/51 ~	-72/71/51-	-72/71/51-	-72/71/52-
DroQ	73 / 72 / 69	73/72/69	73 / 72 / 70~~	73/72/70~~
Hopper-v2
u」nl①一①62」①>(σ
Oo
300
u」nl①一①6g①>(σ
u」nl①一①6g①>(σ
Humanoid-v2
5000
4000
3000
2000
1000
u」nl①一①62」①>(σ
OL
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
-2
0.0	0.5	1.0	1.5	2.0	2.5
Figure 3: Ablation study results
15.0
12.5
10.0
7.5
5.0
2.5
0.0
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
4.3 Ablation study
As an ablation study of DroQ, we investigated the performance of DroQ variants that do not use
either dropout, or layer normalization, or both. We refer to the one without dropout as -DO, that
without layer normalization as -LN, and that without both as -DO-LN. The results (Figure 3) indi-
cate that the synergic effect of using dropout and layer normalization is high, especially in complex
environments (Ant and Humanoid). In these environments, DroQ significantly outperformed its
ablation variants (-DO, -LN and -DO-LN) in terms of both average return and bias reduction.
8
Published as a conference paper at ICLR 2022
Table 4: Comparison between related studies and ours. We classify related studies into five types
(e.g., “Ensemble Q-functions”) on basis of three criteria (e.g. “Type of model uncertainty”).
Type of study	Method for injecting model uncertainty	Type of model uncertainty	Focus on high UTD ratio setting?
Ensemble Q-functions	Ensemble	Q-functions	Partially yes Chen et al.(2021b)
Ensemble transition models	Ensemble	Transition models	Partially yes e.g., Janner et al. (2019)
Dropout Q-functions	Dropout	Q-functions	No
Dropout transition models	Dropout	Transition models	No
Normalization	一	一	No
Our study	Dropout (with ensemble)	Q-functions	Yes
5	Related Work
In this section, we review related studies and compare them with ours (Table 4).
Ensemble Q-functions: Ensembles of Q-functions have been used in RL to consider model uncer-
tainty (Fauβer & Schwenker, 2015; Osband et al., 2016; AnscheI et al., 2017; Osband et al., 2018;
Agarwal et al., 2020; Lee et al., 2021; Lan et al., 2020; Chen et al., 2021b). Ensemble transition
models: Ensembles of transition (and reward) models have been introduced to model-based RL,
e.g., (Chua et al., 2018; Kurutach et al., 2018; Janner et al., 2019; Shen et al., 2020; Yu et al., 2020;
Lee et al., 2020; Hiraoka et al., 2021; Abraham et al., 2020). The methods proposed in the above
studies use a large ensemble of Q-functions or transition models, thus are computationally intensive.
DroQ does not use a large ensemble of Q-functions, and thus is computationally lighter.
Dropout transition models: Gal et al. (2016; 2017); Kahn et al. (2017) introduced dropout and
its modified variant to transition models of model-based RL methods. Dropout Q-functions: Gal
& Ghahramani (2016) introduced dropout to a Q-function at action selection for considering model
uncertainty in exploration. Harrigan (2016); Moerland et al. (2017); Jaques et al. (2019); He et al.
(2021) introduced dropout to policy evaluation in the same vein as us. However, there are three main
differences between these studies and ours. (i) Target calculation: they introduced dropout to a single
Q-function and used it for a target value, whereas we introduce dropout to multiple Q-functions and
use the minimum of their outputs for the target value. (ii) Use of engineering to stabilize learning:
their methods do not use engineering to stabilize the learning of dropout Q-functions, whereas DroQ
uses layer normalization to stabilize the learning. (iii) RL setting: they focused on a low UTD ratio
setting, whereas we focused on a high UTD ratio setting. A high UTD ratio setting increases a high
estimation bias, and thus is a more challenging setting than a low UTD ratio setting. In Sections 4.1
and A.3, we showed that their methods do not perform successfully in a high UTD setting.
Normalization in RL: Normalization (e.g., batch normalization (Ioffe & Szegedy, 2015) or layer
normalization (Ba et al., 2016)) has been introduced into RL. Batch normalization and its variant are
introduced in deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) and twin delayed
DDPG (Fujimoto et al., 2018) (Bhatt et al., 2020). Layer normalization is introduced in the imple-
mentation of maximum a posteriori policy optimisation (Abdolmaleki et al., 2018; Hoffman et al.,
2020). It is also introduced in SAC extensions (Ma et al., 2020; Zhang et al., 2021). Unlike our
study, the above studies did not introduce dropout to consider model uncertainty. In Section 4.3, we
showed that using layer normalization alone is not sufficient in high UTD settings. In Appendix E,
we also show that using batch normalization does not contribute to performance improvement in
high UTD settings.
6	Conclusion
We proposed, DroQ, an RL method based on a small ensemble of Q-functions that are equipped
with dropout connection and layer normalization. We experimentally demonstrated that DroQ sig-
nificantly improves computational and memory efficiencies over REDQ while achieving sample
efficiency comparable with REDQ. In the ablation study, we found that using both dropout and
layer normalization had a synergistic effect on improving sample efficiency, especially in complex
environments (e.g., Humanoid).
9
Published as a conference paper at ICLR 2022
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. In Proc.ICLR, 2018.
Ian Abraham, Ankur Handa, Nathan Ratliff, Kendall Lowrey, Todd D Murphey, and Dieter Fox.
Model-based generalization under parameter uncertainty using path integral control. IEEE
Robotics and AUtomation Letters, 5(2):2864-2871, 2020.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In Proc. ICML, 2020.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-DQN: Variance reduction and stabiliza-
tion for deep reinforcement learning. In Proc.ICML, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv PrePrint
arXiv:1607.06450, 2016.
Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. CrossNorm: On normalization
for off-policy reinforcement learning, 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv PrePrint arXiv:1606.01540, 2016.
Lili Chen, Kimin Lee, Aravind Srinivas, and Pieter Abbeel. Improving computational efficiency in
visual reinforcement learning via stored embeddings. arXiv PrePrint arXiv:2103.02886, 2021a.
Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double Q-
learning: Learning fast without a model. In Proc.ICLR, 2021b.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Proc. NeUrIPS, 2018.
Nikita Durasov, Timur M. Bagautdinov, Pierre Baqu6, and Pascal Fua. Masksembles for uncertainty
estimation. In Proc. CVPR, 2021.
Stefan Fauβer and Friedhelm Schwenker. Neural network ensembles in reinforcement learning.
NeUraI PrOCeSSing Letters, 41(1):55-69, 2015.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In Proc.ICML, pp. 1587-1596. PMLR, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In Proc.ICLR, pp. 1050-1059. PMLR, 2016.
Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural
network dynamics models. In Data-EffiCient MaChine Learning WOrkShOP on ICML, 2016.
Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Proc. NeUrIPS, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proc.ICML, pp. 1861-
1870. PMLR, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, and Pieter Abbeel. Soft actor-critic algorithms and applica-
tions. arXiv PrePrint arXiv:1812.05905, 2018b.
Cosmo Harrigan. Deep reinforcement learning with regularized convolutional neural fitted Q itera-
tion. https://www.machineintelligence.org/papers/rc-nfq.pdf, 2016.
Qiang He, Chen Gong, Yuxun Qu, Xiaoyu Chen, Xinwen Hou, and Yu Liu. MEPG: A min-
imalist ensemble policy gradient framework for deep reinforcement learning. arXiv PrePrint
arXiv:2109.10552, 2021.
10
Published as a conference paper at ICLR 2022
Takuya Hiraoka, Takahisa Imagawa, Voot Tangkaratt, Takayuki Osa, Takashi Onishi, and Yoshimasa
Tsuruoka. Meta-model-based meta-policy optimization. In Proc. ACML, 2021.
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara
Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex
Novikov, Sergio Gomez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew
Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed
reinforcement learning. arXiv Preprint arXiv:2006.00979, 2020.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proc.ICML, 2015.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Proc. NeUrIPS, 2019.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv PrePrint arXiv:1907.00456, 2019.
Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, and Sergey Levine. Uncertainty-aware
reinforcement learning for collision avoidance. arXiv PrePrint arXiv:1702.01182, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. ICLR,
2015.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. In Proc.ICLR, 2018.
Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization.
In Proc.ICML, 2020.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Proc. NeUrIPS, pp. 6402-6413, 2017.
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin Q-learning: Controlling
the estimation bias of Q-learning. In Proc.ICLR, 2020.
Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. Context-aware dynam-
ics model for generalization in model-based reinforcement learning. In Proc.ICML, 2020.
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. SUNRISE: A simple unified
framework for ensemble learning in deep reinforcement learning. In Proc. ICML, 2021.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
PrePrint arXiv:1509.02971, 2015.
Xiaoteng Ma, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. DSAC: Distributional soft
actor critic for risk-sensitive reinforcement learning. In ReinfOrCement Learning for Real Life
WorkShOP at ICML 2019, 2020.
Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
Guided meta-policy search. In Proc. NeUrIPS, pp. 9653-9664, 2019.
Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Efficient exploration with double
uncertain value networks. arXiv PrePrint arXiv:1711.10789, 2017.
OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jdzefowicz, Bob McGrew,
Jakub W. Pachocki, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray,
Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba.
Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Proc. NeUrIPS, 2016.
11
Published as a conference paper at ICLR 2022
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. In Proc. NeUrIPS, 2018.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can yoU trUst yoUr model's Uncertainty? evalUating
predictive uncertainty under dataset shift. In Proc. NeUrIPS, 2019.
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv Preprint arXiv:1707.06347, 2017.
Jian Shen, Han Zhao, Weinan Zhang, and Yong Yu. Model-based policy optimization with unsuper-
vised model adaptation. In Proc. NeUrIPS, 2020.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. JoUmaI of MaChine Learning
ReSearch, 15(56):1929-1958, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control.
In Proc.IROS, pp. 5026-5033. IEEE, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
E ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeUrIPS, 2017.
Yuxin Wu and Kaiming He. Group normalization. In Proc. ECCV, 2018.
Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and
improving layer normalization. In Proc. NeUrIPS, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. MOPO: model-based offline policy optimization. In Proc. NeUrIPS, 2020.
Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. In Proc. ICLR, 2021.
12
Published as a conference paper at ICLR 2022
A Effect of dropout rate
A.1 DROQ
on DroQ and its variants
Oo
300
Hopper-v2
Oo
200
0.0	0.2	0.4	0.6	0.8
environment interactions le5
Walker2d-v2
s(ŋ一 q ① 6g① >(0
u」nl①一①6g①>(σ
environment interactions ie5
Ant-v2
6000
5000
4000
3000
2000
1000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
5000
4000
3000
2000
1000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
,5	1.0	1.5	2.0	2.5
environment interactions ie5
environment interactions
Walker2d-v2
Hur∩anoid-v2
o.o 、■・・■■・■・	・	・
0.0	0.2	0.4	0.6	0.8
environment interactions ie5
Walker2d-v2
environment interactions
Ant-v2
o.o---------------------------------------
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
ɪʒ θ	Humanoid-v2
12.5
10.0
7.5
5.0
2.5
0.0	----------------------------
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
u」nl①一①62」①>(σ
u」nl①一①6g①>(σ
u」nl①一①62」①>(σ
Figure 4:	Average return and average/standard deviation of estimation bias for DroQ with different
dropout rates. Scores for DroQ are plotted as solid lines and labelled in accordance with dropout
rates (e.g.,“0.2”).
13
Published as a conference paper at ICLR 2022
A.2 DroQ without layer normalization
Hopper-v2
0.0	0.2	0.4	0.6	0.8
environment interactions le5
Walker2d-v2
5000
Hopper-v2
q ① 6g① >(0
15.0
12.5
10.0
7.5
5.0
2.5
0.0
Hopper-v2
0.0	0.2	0.4	0.6	0.8
environment interactions ie5
u」nl①一①6g①>(σ
Walker2d-v2
0.5	1.0	1.5	2.0	2.5
environment interactions ie5
environment interactions ie5
Ant-v2
4000
2000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
4000
3000
2000
1000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
Ant-v2
15.0
environment interactions le5
o.o
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
ɪʒ θ	Humanoid-v2
12.5
10.0
7.5
5.0
2.5
OoJ	,	- -----------------
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
u」nl①一①6g①>(σ
u」nl①一①62」①>(σ
Scq ① 5,① >(σS6q ① 62」① >p
Figure 5:	Average return and average/standard deviation of estimation bias for DroQ without layer
normalization (i.e., -LN in Section 4.3) with different dropout rates
14
Published as a conference paper at ICLR 2022
A.3 Sin-DroQ: DroQ variant using A single dropout Q-function
DroQ (Algorithm 2) uses an ensemble of multiple (M) dropout Q-functions. This raises the question:
“Why not use a single dropout Q-function for DroQ?” To answer this question, we compared DroQ
with Sin-DroQ, a variant of DroQ that uses a single dropout Q-function. Specifically, with Sin-
DroQ, the target in line 6 in Algorithm 2 is calculated by evaluating QDrφι (s0, a0), a single dropout
Q-function, M times:
/
∖
y=r
+ Y	min( Q Dr ,φι( s, a ),…,Q Dr,φι ( s, a )) — a
|----------------V----------------}
M evaluation results of QDr,g1 (s0 ,a0)
log ∏θ (a0∣s0),
a0 〜∏θ(∙∣s0).
∖
Note that, the output of QDr,Φ、(s0, a0) can differ in each evaluation due to the use of a dropout
connection. It should be also noted that this target is the same as the one proposed in Jaques et al.
(2019). The remaining part of Sin-DroQ is the same as DroQ.
From the comparison results of DroQ and Sin-DroQ (Figure 6), we can see that the average return of
Sin-DroQ was lower than that of DroQ. This result indicates that using multiple dropout Q-functions
is preferable to using a single dropout Q-function with DroQ.
15
Published as a conference paper at ICLR 2022
Hopper-v2
----DroQ
Oo
300
u」nl①一①62」①>(σ
Oo
200
s(ŋ一 q ① 6g① >(D
u」nl①一①6g①>(σ
s&q ① 5(0」① >(D
u」nl①一①6g①>(σ
s(ŋ一 q ① 6(0」① >p
Hopper-v2
Ant-v2
environment interactions
Walker2d-v2
environment interactions
0.0 一---------------------------------------
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
o L
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
s<D-q ① 6(0」① >2
Figure 6:	Average return and average/standard deviation of estimation bias for DroQ and Sin-DroQ.
Scores for DroQ are plotted as dash lines. Scores for Sin-DroQ are plotted as solid lines and labelled
in accordance with dropout rates (e.g., “0.2”).
16
Published as a conference paper at ICLR 2022
B REDQ with different ensemble size N
In Section 4, we discussed comparing DroQ with REDQ, which uses an ensemble size of 10 (i.e.,
N =10). To make a more detailed comparison, we compared DroQ and REDQ by varying the ensem-
ble size for REDQ. We denote REDQ that uses an ensemble size of N as“REDQN” (e.g., REDQ5
for REDQ with an ensemble size of five).
Regarding average return (left part of Figure 7), overall, DroQ was superior to REDQ2-5. DroQ
was comparable with REDQ2 and REDQ3 in Hopper but superior in more complex environments
(Walker, Ant, and Humanoid). In addition, DroQ was somewhat better than REDQ5 in all envi-
ronments. Regarding estimation bias (middle and right parts of Figure 7), overall, DroQ was sig-
nificantly better than REDQ2 and comparable with REDQ3—10. Regarding the processing speed
(Table 5), DroQ ran as fast as REDQ3 and faster than REDQ5 by 1.4 to 1.5 times. Regarding
memory efficiency (Tables 6 and 7), DroQ was less memory intensive than REDQ3-10.
10
Hopper-v2
s(ŋ一 q ① 62」① >(0
20.0
-
Hopper-v2
Hopper-v2
Walker2d-v2
srts-q ① 6(0」① >(σ
0.2	0.4	0.6	0.8
environment interactions ie5
Ant-v2
environment interactions
Figure 7: Average return and average/standard deviation of estimation bias for DroQ and REDQ.
Scores for DroQ are plotted as dash lines. Scores for REDQ are plotted as solid lines and labelled
as “REDQN”.
environment interactions
Humanoid-v2
environment interactions ie5
Humanoid-v2
0.0	0.2	0.4	0.6	0.8
environment interactions le5
ɪʒ θ	Humanoid-v2
12.5
10.0
7.5
5.0
2.5
o.o	・	■・
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
17
Published as a conference paper at ICLR 2022
Table 5: Process times (in msec) per overall loop (e.g., lines 3-10 in Algorithm 2). Process times
per Q-functions update (e.g., lines 4-9 in Algorithm 2) are shown in parentheses.
	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2
DroQ	948 (905)二	933 (892)=	954(913)二	989 (946)
REDQ2	832 (792)	802 (768)	675 (641)	820 (773)
REDQ3	1052(1014)	876 (838)	919(881)	950 (906)
REDQ5	1414(1368)	1425 (1378)	1373 (1327)	1552 (1503)
REDQ10	2328 (2269) —	2339 (2283)一	2336 (2277)一	2400 (2340)
Table 6: Number of parameters
	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2
DroQ	141,826	146,434 =	152,578	166,402 =
REDQ2	139,778	-144,386-	150,530	164,354^^
REDQ3	209,667	-216,579-	225,795	-246,531^^
REDQ5	349,445	-360,965-	376,325	-410,885^^
REDQ10	698,890	721,930 —	752,650	821,770 —
Table 7: Bottleneck memory consumption (in megabytes) with methods. Three worst bottleneck
memory consumptions are shown in form of “1st worst bottleneck memory consumption / 2nd worst
bottleneck memory consumption / 3rd worst bottleneck memory consumption.”
	Hopper-v2	Walker2d-v2	Ant-v2	Humanoid-v2
DroQ	73/72/ 6h	73/72/ 6h	73/72/ 7O=	73 / 72 / 70
REDQ2	-73/51/51-	-73/51/51	-73/51/51	73 /52/52
REDQ3	-94/64/ 62-	-94/64/ 62-	-94/64/ 62-	94 / 65 / 62
REDQ5	136/ 106/ 100	136/ 106/ 100	136/ 106/ 100	136/ 107/ 101
REDQ10	241/211/200~	241/211/200-	241/211/200~	241/212/201
18
Published as a conference paper at ICLR 2022
C Additional ablation study of DroQ
Dropout is introduced into three parts of the algorithm for DroQ (i.e., lines 6, 8 and 10 of Algorithm 2
in Section 3). In this section, we conducted an ablation study to answer the question “which dropout
introduction contributes to overall performance improvement of DroQ?” We compared DroQ with
its following variants:
-DO@TargetQ: A DroQ variant that does not use dropout in line 6. Specifically, dropout is not used
in QDr,φi (s0, a0) in the following part in line 6.
y = r + Y I ._min. " Q Dr,φi ( s ,a ) - α log πθ (a0ls) ) , a0 〜πθ ( Js0)
= ,...,
-DO@CurrentQ: A DroQ variant that does not use dropout in line 8. Specifically, dropout is not
used in QDr,φi (s, a) in the following part in line 8.
Rφ-B∣	X	(QDr,φi (s,α) - y)2
(s,a,r,s0 )∈B
-DO@PolicyOpt: A DroQ variant that does not use dropout in line 10. Specifically, dropout is not
used in QDr,φi (s, a) in the following part in line 10.
1	1M
rΘ 回 E (M ɪl Q Dr ,φi ( s,a) - a log πθ (als )) a 〜πθ (∙ls )
-DO: A DroQ variant that does not use dropout in lines 6, 8, and 10.
In this ablation study, we compared the methods on the basis of average return and estimation bias.
The comparison results (Figure 8) indicate that using dropout for both current and target Q-functions
(i.e., QDr,φi (S, a0) and QDr,φi (s,a) in lines 6 and 8) is effective. Regarding average return, the
DroQ variants that do not use dropout for either target Q-functions in line 6 or current Q-functions
in line 8 perform significantly worse than that of DroQ. For example, in Ant, the variants that do
not use dropout for target Q-functions (-DO@TargetQ and -DO) perform much worse than DroQ. In
addition, in Humanoid, the variants that do not use dropout for current Q-functions (-DO@CurrentQ
and -DO) perform much worse than DroQ. Regarding estimation bias, that of the variants that do
not use dropout for target Q-functions (-DO@TargetQ and -DO) is significantly worse than that of
DroQ in all environments.
These results complement the results presented in He et al. (2021). They show that using dropout for
both current and target Q-functions improves the performance of the maximum entropy RL method
(SAC) in low UTD settings. Our results presented in the previous paragraph implies that their
insight (i.e., “using dropout for both current and target Q-functions improves the performance”) is
valid, even in high UTD settings.
19
Published as a conference paper at ICLR 2022
Hopper-v2
3000
2000
u」nl①一①6g①>(D
----DroQ
-DO(S)IargetQ
-DO@CurrentQ
-DO@PolicyOpt
-DO
0.0	0.2	0.4	0.6	0.8
environment interactions ie5
u」nl①一①6g①>(σ
u」nl①一①6g①>(σ
∙, C	Ant-v2
10 .
u」nl①一①62」①>(σ
Figure 8: Additional ablation study result about dropout for DroQ
10
S
0 6
ɪ 4
ro
9 2
(O
¾.0	0.2	0.4	0.6	0.8
environment interactions le5
0.0	0.2	0.4	0.6	0.8
environment interactions ie5
15 o	Walker2d-v2
12.5
S 10.0
(D
£ 7.5
S 5.0
S
2.5
0.0
0.0	0.5	1.0	1.5	2.0	2.5
20
Published as a conference paper at ICLR 2022
Figure 9: Average/standard deviation of the Q-function loss (“average Q loss” and “std
of Q loss”) and its gradient with respect to parameters (“average Q gradient” and
“std of Q gradient"). Q-function loss is calculated as M PM=1 Li, where Li :=
(Qφi (s, a) — r — Y (mini∈M Qφi (S, ar) — α log πθ(a,∣s')))2, and its gradient is calculated as
MM PM=1 ∂Lφi. The definition of method labels (e.g., “DroQ” or “-DO”)in the legend is the same as
that used in Figure 3.
D	Why is the combination of dropout and layer normalization
important?
Figure 3 in Section 4.3 showed that combining dropout and layer normalization improves overall
performance, especially in complex environments (Ant and Humanoid).
The main reason for this performance improvement could be that layer normalization suppresses
the learning instability caused by dropout. Using dropout destabilizes the Q-functions learning.
Figure 9 shows that (i) the Q-function loss of the method using dropout (-LN) oscillates more sig-
nificantly than that of the method not using dropout (-DO-LN), and that (ii) the variance of the
gradient loss with respect to the Q-function parameters also oscillates accordingly. Layer normaliza-
tion suppresses such learning instabilities. The right part of Figure 9 shows that the method using
both dropout and layer normalization (DroQ) suppresses the oscillations of the gradient more sig-
nificantly than the -LN. This stabilization of learning could enable better Q-value propagation and
consequently improve overall performance.
21
Published as a conference paper at ICLR 2022
E Effects of other normalization methods
In Section 4.3 and Appendix D, we demonstrated that combining dropout and layer normalization is
effective in improving performance. In this section, we examine the effectiveness of normalization
methods other than layer normalization.
We examine the effects of (i) batch normalization (Ioffe & Szegedy, 2015), (ii) group normaliza-
tion (Wu & He, 2018), and (iii) layer normalization without variance re-scaling. Batch normaliza-
tion is an off-the-shelf and popular normalization method, and has been introduced into RL in low
UTD settings (Bhatt et al., 2020). We introduce this popular method to verify whether it is effective
in high UTD settings. Group normalization is a method of normalization using the average and
variance over the elements of a target input, which is similar to layer normalization5. We introduce
this method to verify whether a method equipped with a similar mechanism to layer normalization
works well in high UTD settings. Layer normalization without variance re-scaling is the layer nor-
malization variant that does not re-scale the target input’s variance. Xu et al. (2019) pointed out that
variance re-scaling is an important component for layer normalization to work well. We introduce
this method to verify whether variance re-scaling is an important component also in our settings.
We compare six DroQ variants that use these three normalization methods. We denote -DO6 vari-
ants using batch normalization, group normalization, and layer normalization without variance re-
scaling, instead of layer normalization, as +BN, +GN, and +LNwoVR, respectively. We also denote
DroQ variants using batch normalization, group normalization, and layer normalization without
variance re-scaling, instead of layer normalization, as +DO+BN, +DO+GN, and +DO+LNwoVR,
respectively. We compare these six methods (+BN, +GN, +LNwoVR, +DO+BN, +DO+GN, and
+DO+LNwoVR).
From the experimental results of the six methods (Figures 10 and 11), we get the following insights:
1. Batch normalization does not work well. Figure 10 shows that the methods using batch normal-
ization (+BN and +DO+BN) do not significantly improve the average return and estimation bias in
Humanoid and Ant. Further, Figure 11 shows that the Q-function learning with these methods is
very unstable.
2.	Group normalization, which has a similar mechanism to layer normalization, works well when
combined with dropout. Figure 10 shows that the method using the group normalization and dropout
(+DO+GN) successfully improves the average return and estimation bias in Humanoid and Ant. The
figure also shows that the method that uses only group normalization (+GN) fails in improving the
average return and estimation bias.
3.	Layer normalization without variance re-scaling does not work well. Figure 10 shows that the
methods that introduce this layer normalization variant (+LNwoVR and +DO+LNwoVR) do not
significantly improve the average return and estimation bias well especially in Humanoid. Further,
Figure 11 shows that the Q-function learning is unstable compared to +DO+GN (and DroQ in Fig-
ure 9) in both Ant and Humanoid. This result implies that one of the primal factors for the synergistic
effect of dropout and layer normalization is the variance re-scaling.
5Layer normalization normalizes all input elements using their mean and variance. On the other hand, group
normalization divides the input elements into subgroups (two groups in our setting) and normalizes each group
using the intra-group mean and variance.
6DroQ without dropout, which is introduced in Sections 4.3 and D.
22
Published as a conference paper at ICLR 2022
Figure 10: Average return and average/standard deviation of estimation bias for six methods using
batch normalization, group normalization, and the variant of layer normalization.
Ant-v2
6000
4000
+DO+LNwoVR
---+DO+BN
---+DO+GN
Figure 11: Average/variance of the Q-function loss (left part) and its gradient with respect to param-
eters (right part).
23
Published as a conference paper at ICLR 2022
Algorithm 3 DroQ with ensemble of N dropout Q-functions (DroQN)
1:	Initialize policy parameters θ, N Q-function parameters φi, i = 1,…,N, and empty replay
buffer D. Set target parameters φi J φ%, for i = 1,…,N.
2:	repeat
3:	Take action at 〜 ∏(1”).Observe reward r, next state St+ι; D j D S(st, at, c +ι).
4:	for G updates do
5:	Sample a mini-batch B = {(s, a, r, s0)} from D.
6:	Sample a set M of M distinct indices from {1, 2, ..., N}.
7:	Compute the Q target y for the dropout Q-functions:
y = r + Y (min Q Dr ,φ i (s0,4) - Q log ∏θ (als) ) , a0 〜∏θ (∙∖s)
8:
9:
for i = 1, ..., N do
Update φi with gradient descent using
%φ B X	(QDr,φi (s,a) - y )2
(s,a,r,s0)∈B
10:	Update target networks with φi J ρφ% + (1 - P)φi.
11:	Update θ with gradient ascent using
1	1N
No B∖^2 NfQ Dr ,φi( s,a) - q log πθ ( a∖s ) , a 〜πθ (∙∖s )
∖B∖ s∈B N i=1
F	Relation between (1) ensemble size and (2) the effect of layer
NORMALIZATION AND DROPOUT ON OVERALL PERFORMANCE
In Section 4.3 and Appendix D, we showed that using layer normalization and dropout improves
overall performance. Further, in Sections 4.1 and A.3, we showed that using multiple (two) dropout
Q-functions improves overall performance. These results raise two new questions: (i) what happens
if we use more than three dropout Q-functions? and (ii) what the effect of layer normalization and
dropout would be in such cases? In this section, we conduct an additional experiment to answer
questions (i) and (ii).
For our additional experiment, we introduce the DroQ with ensemble of N dropout Q-functions
(DroQN) algorithm (Algorithm 3). This algorithm is a variant of DroQ algorithm (Algorithm 2)
which uses bootstrapping ofa subset M ofN dropout Q-functions (line 6). This algorithm can also
be regarded as a variant of REDQ algorithm (Algorithm 1) which uses N dropout Q-functions. With
DroQN algorithm, the ensemble size can be changed by varying the value of N.
We compare the method based on the DroQN algorithm and its ablation variants:
DroQN -LN: the DroQN variant that does not use layer normalization for the dropout Q-function
QDr,φi.
DroQN -DO-LN: the DroQN variant that does not use layer normalization and dropout for QDr,φi .
Note that DroQN -DO-LN is identical with REDQN presented in Appendix B.
We evaluate the performance of these methods for the case of N = 2, 3, 5, 10.
Learning curves of the methods are shown in Figure 12.
For the question (i), in complex environments (Ant and Humanoid), increasing the size of the en-
semble improves the sample efficiency. In Ant, DroQ5-10 are more sample efficient than DroQ2-3.
In Humanoid, the sample efficiency is monotonically improved when the ensemble size of DroQN
is increased as N = 2, 3, 5, 10.
For the question (ii), in the complex environments, the synergy of layer normalization and dropout
is significant especially when the value of N is small (N = 2, 3). In Ant and Humanoid, DroQ2-
3 significantly improves the sample efficiency compared to DroQ2-3-DO. On the other hand, the
effect of layer normalization itself becomes dominant when the value of N is large (N ≥ 5). For
24
Published as a conference paper at ICLR 2022
Hopper-v2
…DroQ2-DO-LN
DroQ3-DO-LN
…DroQ5-DO-LN
…DroQlO-DO-LN ∙
3000
2000
1000
Hopper-v2
DroQ2-DO
DroQ3-DO
DroQ5-DO
DroQlO-DO
u」nl①一①62」①>(σ
Oo
300
0.0	0.2	0.4	0.6	0.8
environment interactions le5
0
0.0	0.2	0.4	0.6	0.8
environment interactions le5
Hopper-v2
DroQ2
DroQ3
DroQ5
DroQlO
Oo
200
u」nl①一①6(0」①>(D
u」nl①一①6(0」①>(σ
0.0	0.2	0.4	0.6	0.8
environment interactions le5
Walker2d-v2
5000
4000
3000
2000
1000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Ant-v2
u」nl①一①62」①>(σ
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoιd-v2
Humanoιd-v2
6000
0
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
u」nl①一①62」①>(σ
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
6000
5000
4000
3000
2000
1000
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions le5
u」n@①6(0」①>(σu」n@①6
Figure 12:	Additional ablation study result (average return). Left part (dotted lines): DroQN -DO-
LN, middle part (dashed lines): DroQN -LN, right part (solid lines): DroQN
Ant and Humanoid, We can see that DroQ5-10-DO significantly improves the sample efficiency
compared to DroQ5-10-DO-LN. However, DroQ5-10 does not significantly improve the sample
efficiency compared to DroQ5-10-DO.
Based on the above result, we suggest (1) using layer normalization and dropout together for small
ensemble cases, and (2) using layer normalization alone for large ensemble cases.
25
Published as a conference paper at ICLR 2022
G Effect of dropout Q-functions on SAC
environment interactions
environment interactions
Humanoid-v2
Figure 13:	Average return and average/standard deviation of estimation bias for DroQ, SAC,
and its variants. SAC+DO is a SAC variant using dropout, SAC+LN is a SAC variant us-
ing layer normalization, and SAC+DO+LN is a SAC variant using dropout and layer nor-
malization. Note that the only difference between DroQ (M = 2) and SAC+DO+LN
is the policy update part (line 10 in Algorithm 2). DroQ (M = 2) updates policy as
Nθ∣B∣ Ps∈B(2 P2=1 QDr,φi (s,a) — α log ∏θ(a∣s)). On the other hand, SAC+DO+LN updates
policy as Nθ ∣Bτ P s∈B (1 min 2=1 Q Dr ,φi( s, a) — α log ∏θ( a∣s)).
26
Published as a conference paper at ICLR 2022
H Hyperparameter settings
The hyperparameter settings for each method in the experiments discussed in Section 4 are listed in
Table 8. Parameter values, except for (i) dropout rate for DroQ and DUVN and (ii) M for DUVN,
were set according to Chen et al. (2021b). The dropout rate (i) was set through line search, and M
for DUVN (ii) was set according to Harrigan (2016); Moerland et al. (2017).
Table 8: Hyperparameter settings
Method	Parameter	Value
SAC, REDQ, DroQ, and DUVN	optimizer learning rate discount rate (Y) target-smoothing coefficient (P) replay buffer size number of hidden layers for all networks number of hidden units per layer mini-batch size random starting data UTD ratio G	Adam (Kingma & Ba, 2015) 3 - 102 0.99 0.005 106 2 256 256 5000 20
REDQ and DroQ	in-target minimization parameter M	^2
REDQ	ensemble size N	10
DroQ and DUVN	dropout rate	^001
DUVN	in-target minimization parameter M	1	—
I Experiments on the original REDQ codebase
Experiments presented in the main content of this paper have been implemented on top of the
SAC codebase (https://github.com/ku2482/soft-actor-critic.pytorch). In
this section, we report the results of our experiments on the original REDQ codebase (https:
//github.com/watchernyu/REDQ). Our experiments are to replicate experiments in the
main content of this paper. To evaluate the computational efficiency, we plot the wallclock time
required to complete a certain number of interactions with the environment, instead of the standard
deviation of bias. In addition, dropout rate values are re-tuned for each environment (Table 9). Fig-
ure 14 shows the replica of the results presented in Section 4.1. Figure 15 shows the replica of the
results provided in Section 4.3. Figure 16 shows the replica of the results provided in Section A.1.
Figure 17 shows the replica of the results provided in Section D. Figures 18 and 19 show the replica
of the results provided in Section E. Figure 20 shows the replica of the results provided in Section F.
Table 9: Dropout rate setting for Appendix I
Environment	Value
Hopper-v2	0.0001
Walker2d-v2	0.005
Ant-v2	0.01
Humanoid-v2	0.1
27
Published as a conference paper at ICLR 2022
environment interactions
Walker2d-v2
U」n4①一①5g①>2
①一①6g①>(σ
Hopper-v2
srts-q ① 69① >(σ
environment interactions 1∈5
Walker2d-v2
s<u-q ① 69① ><σ
U」n4①一①62」①>(0
U」n4①一①62」①>(0
environment interactions ie5
Se-q ① 6«5」① >rtjse-q ① ① ><σ
environment interactions 1∈5
Figure 14:	Average return, average of estimation bias, and wallclock time (second) for REDQ, SAC,
DUVN, and DroQ. The horizontal axis represents the number of interactions with the environment
(e.g., the number of executions of line 3 of Algorithm 2). For each method, average score of five
independent trials are plotted as solid lines, and standard deviation across trials is plotted as trans-
parent shaded region.
28
Published as a conference paper at ICLR 2022
----DroQ
-----DO
-----LN
-----DO-LN
Oooo
Oooo
Oooo
4 3 2 1
U-InlQ.I ① 62」① >(σ
10
0 6
⅛ 4
(σ
2
0
0.00	0.25	0.50	0.75	1.00
environment interactions ie5
U」n4①一①6g①>(0
Se_q ① ① ><σ
U-InlN ① 6(0」① >p
Humanoid-v2
Oo
500
Oooo
Oooo
Oooo
4 3 2 1
u-ln4->①一① 62」① >(σ
SQ-q ① ① ><σSe_q ① ① ><σ
environment interactions ie5
0
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
Figure 15: Ablation study results
-2
0.00	0.25	0.50	0.75	1.00
environment interactions 1∈5
0.25	0.50	0.75	1.00
environment interactions ie5
Walker2d-v2
0.5	1.0	1.5	2.0	2.5
environment interactions le5
Ant-v2
0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
0.5	1.0	1.5	2.0	2.5
environment interactions le5
29
Published as a conference paper at ICLR 2022
U」B①一①6g①>(σ
0.5	1.0	1.5	2.0	2.5
environment interactions le5
Hopper-v2
Oo
300
Oo
200
Walker2d-v2
Ant-v2
0
0.0
Humanoid-v2
0.5	1.0	1.5	2.0	2.5
environment interactions ie5
SE_q 360」① >(σs<π-q ① 62」① >(σ
10
0.00	0.25	0.50	0.75	1.00
environment interactions ie5
Walker2d-v2
0.5	1.0	1.5	2.0	2.5
environment interactions ie5
environment interactions
0.00	0.25	0.50	0.75	1.00
environment interactions ie5
o，
0.0	0.5	1.0	1.5	2.0	2.5
environment interactions ie5
Figure 16: Average return, average of estimation bias, and wallclock time (second) for DroQ with
different dropout rates. Scores for DroQ are plotted as solid lines and labelled in accordance with
dropout rates (e.g.,“0.2”).
30
Published as a conference paper at ICLR 2022
Figure 17: Average/standard deviation of the Q-function loss (“average Q loss” and
“std of Q loss”) and its gradient with respect to parameters (“average Q gradient” and
“std of Q gradient"). Q-function loss is calculated as M PM=1 Li, where Li	:=
(Qφi (s, a) — r — Y (mini∈M Qφi (S, a!) - α log πθ(a,∣s')))2, and its gradient is calculated as
1 P M ∂Li
M 乙i=1 ∂φi ・
Ant-v2
Figure 18: Average return and average estimation bias for six methods using batch normalization,
group normalization, and the variant of layer normalization.
31
Published as a conference paper at ICLR 2022
ssooM-0SS
IU①一 pp」6 O ①6P」①>(D
IU ①一 Pra.J6 O ① 6巴① >(D
Figure 19:	Average/variance of the Q-function loss (left part) and its gradient with respect to param-
eters (right part).
φ
6
rσ
⅛
rŋ
Hopper-v2
DroQ2-DO-LN
DroQ3-DO-LN
DroQ5-DO-LN
DroQlO-DO-LN
ErU①」①6,①>e
Em①J ① 6eJφ>p
UJnj①」①6巴①>2
environment interactions
Walker2d-v2
Em①J①旨①AQ
Em①」①旨①>(σ
environment interactions
Humanoid-v2
0.5	1.0	1.5	2.0	2.5
environment interactions le5
Humanoid-v2
HOPPer-V2
ErqaI ① 6p.lφ>rtJ
①」①6(0」①>p
Ant-v2
EmB-J ①6bjb>p
0.5	1.0	1.5	2.0	2.5
environment interactions le5
UJn 13」山6(0」①>(D
Figure 20:	Additional ablation study result (average return). Left part (dotted lines): DroQN -DO-
LN, middle part (dashed lines): DroQN -LN, right part (solid lines): DroQN
32
Published as a conference paper at ICLR 2022
J Our source code
Our source code is available at https://github.com/TakuyaHiraoka/
Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning
33