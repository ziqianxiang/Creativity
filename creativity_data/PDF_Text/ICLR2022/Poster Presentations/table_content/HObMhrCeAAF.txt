Table 1: A summary of existing methods for model performance inference. The right four columnsshow (1) whether a method is based on theoretical results, (2) whether a method avoids expensivetraining process, (3) whether a method is applicable to different model architectures, and (4) whethera method is applicable across different tasks.
Table 2: Performance of existed MPI methods (gradient-based + NASWOT) on NAS-Bench-101evaluated by Spearman’s ρ.
Table 3: Performance of existed MPI methods (gradient-based + NASWOT + ZenNAS) on NAS-Bench-201 evaluated by Spearman’s ρ.
Table 4: Performance of existed MPI methods on five design spaces in NDS trained over CIFAR-10evaluated by Kendall’s Tau.
Table 5: Mean ± std accuracy evaluated on NAS-Bench-201. All results are averaged over 500 runs.
Table 6: Mean ± std accuracy evaluated over NAS-Bench-201. All results are averaged over 500runs. To make a fair comparison across all the methods, the search is performed on CIFAR-10 datasetwhile the architectures’ performance are evaluated over CIFAR-10, CIFAR-100 and ImageNet16-120.
Table 7: Comparison with learning-based methods (MLP, LSTM and GATES) on NAS-Bench-201.
Table 8: Spearman’s ρ evaluated on the latest version of NAS-Bench-201 (NATS-Bench)CIFAR-10 CIFAR-100 ImageNet16-120NAS-BenCh-201 ~~0765	0793	0583NATS-Bench 0.760	0.792	0.784Table 9: Mean ± std aCCuraCy evaluated over NATS-BenCh. All results are averaged over 500 runs.
Table 9: Mean ± std aCCuraCy evaluated over NATS-BenCh. All results are averaged over 500 runs.
