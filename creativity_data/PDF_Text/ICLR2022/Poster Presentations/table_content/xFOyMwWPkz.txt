Table 1: Comparisons of unit status by rescaling the values in images or CNN filters						Images	CNN filters	Accuracy	L1-norm	APoZ	Class selectivity	Feature entropyX	X	0.83	29.5	17.14%	0.58	1.87Half scale	X	0.81	14.7	17.15%	0.31	1.90X	Half scale	0.83	14.6	17.14%	0.30	1.87Half scale	Half scale	0.79	7.2	17.16%	0.03	1.92Table 1 shows the results where X denotes no rescaling operation for the item. As half scaling themagnitude in input images or units, the performance of the model fluctuates slightly. We could findthat APoZ and feature entropy vary in the similar way with the performance, but L1-norm and classselectivity vary terribly. Apparently, despite little effect for the network, rescaling operations wouldhave a major impact on these magnitude-based indicators, like L1-norm and class selectivity. Theseindicators fail to give accurate and stable measure of unit status especially when facing images orunits with different value scales.
Table 2: Comparisons of unit status with respect to well-trained units and random units	L1-norm	APoZ	Class selectivity	Feature entropyWell-trained unit	29.5	17.14%	0.58	1.87Random initialized units	32.2(30.9)	41%(40%)	0.01(0.003)	2.87(0.21), 0.83(0.22)Detecting randomness in units Next, we compare the status of this unit with random units (unitsyielded by random-initialized models). Table 2 presents the results. The random units are sampled100 times and the presented results are averaged over the 100 samples where the value in the bracketsdenotes the standard deviation. Since random units are clearly incapable to perceive features welllike those trained units, they are expected to be indicated as ineffective units. We could see that whenusing L1-norm and APoZ indicators, they are impossible to give a stable indication as the standarddeviation is extremely large. In some samples, the random units are judged as much ”better” thanthe trained units, which is obviously incorrect. Accordingly, it could be also misleading using APoZas the indicator of unit status. In contrast, the feature entropy would consistently be very high whenrandom pattern exists in the unit, providing a well discrimination between trained units and randomones.
Table 3: Model set A			Table 4: Model set B						Train Acc		Test Acc	CorruptedTrain Acc		Test Acc							Model BA	0.996	0.378	0.0Model AA Model AB Model AC	0.732 0.818 0.828	0.657 0.532 0.444							Model BB	0.992	0.297	0.2			Model BC	0.994	0.166	0.4			Model BD	0.992	0.074	0.6Model AD	0.996	0.378	Model BE	0.993	0.010	0.8						Model set A Using the same image set in previous section, we start by investigating the featureentropy of units at different layers in the four models. Here, we still use the averaged feature entropyacross all the units within a layer to indicate the overall level of how well the units in this layer couldperceive the features. Fig.6A(1-2) shows the results of this class. We could see in the figure that therewould not be significant difference of feature entropy between these models in layers except for thelast convolutional layer. And in the last convolutional layer, for models with better generalization,their feature entropy would be lower than those with poor generalization, indicating that they wouldprovide more effective feature representations. Besides, as for the selective rate, the four models arequite close.
Table 5: Comparisons of feature entropy by using different characterizations	Birth time	Maximum	IntegrationReference unit	1.87	1.89	1.92Last convolutional layer	2.01	2.09	2.11Last convolutional layer (100 classes)	2.07	2.13	2.17A.4 S tudy on Sample SizeFor an efficient computation in practice, we could use part of the training set for calculating thefeature entropy. In this section, we check the influence on the feature entropy when using differentsample sizes.
Table 6: Comparisons of unit status by rescaling the values in images or CNN filtersImages	CNN filters	Accuracy	L1-norm	APoZ	Class selectivity	Feature entropyX	X	0.81	122.1	4.79%	0.47	1.71Half scale	X	0.80	60.9	4.81%	0.23	1.72X	Half scale	0.81	61.1	4.79%	0.24	1.71Half scale	Half scale	0.77	30.4	4.83%	0.07	1.74Table 7 shows the corresponding results of trained units and random initialized units.
Table 7: Comparisons of unit status with respect to well-trained units and random units	L1-norm	APoZ	Class selectivity	Feature entropyWell-trained unit	122.1	4.79%	0.47	1.71Random initialized units	678(317)	1.17%(1.54%)	0.02(0.013)	2.08(0.29)Just as the results on VGG16 (shown in Table 1 and 2), due to the advantages of using topology,feature entropy could give stable indication of the status of units as expected, no matter with thescaling operation or in the randomness situation.
Table 8: Performance of model set R	Train Acc	Test Acc	CorruptedModel RA	0.823	0.714	0.0Model RB	0.982	0.382	0.2Model RC	0.991	0.229	0.4Model RD	0.995	0.102	0.6Model RE	0.991	0.036	0.84.54.0Ω.
Table 9: Comparisons of unit status by rescaling the values in images or CNN filtersImages	CNN filters	Accuracy	FPGM	NISP	Feature entropy×	×	0.83	0.88	72.37	1.87Half scale	×	0.81	0.88	36.04	1.90×	Half scale	0.83	0.44	36.17	1.87Half scale	Half scale	0.79	0.44	17.81	1.92In addition to the rescaling operation, as we could see in Table 10, the FPGM method is also unableto give correct discrimination between the well-trained units and the random-initialized units.
Table 10: Comparisons of unit status with respect to well-trained units and random unitsFPGM NISP Feature entropyWell-trained unit	0.88	72.37	1.87Random initialized units	1.44(0.044)	19.27(2.98)	2.87(0.22)A.7 S tudy of PruningIn Section 4.4, feature entropy has shown the ability to reliably indicate the status of units betweendifferent models in various situations, which is the core motivation of the paper. In addition, wealso show the effectiveness of feature entropy to give comparisons between different units in thesingle model in a fixed situation. In this situation, since units are generally in the comparable scale(unless use some specific implementations on networks like Neyshabur et al. (2015)2), the mentionedproblems may be largely alleviated, so units would be compared directly via the typical methods likeL1-Norm, etc. Besides, we would consider feature entropy and selective rate both to represent itseffectiveness of units for an accurate estimation, where we would fuse these two factors by usingH/ in the following calculations.
Table 11: Performance enhancement via cumulative ablation when using different methods	Unchanged network	Ablated network	Performance enhancedL1-Norm	0.7591	0.928	0.167Apoz	0.759	0.873	0.114Class selectivity	0.759	0.917	0.158FPGM	0.759	0.789	0.030NISP	0.759	0.838	0.079Feature entropy	0.759	0.945	0.1861 All the performances are reported as balanced accuracy(Brodersen et al., 2010).
Table 12: Comparisons of the accuracy of fine-tuned models when using different methods	Accuracy ] (fine-tuned)	Pruned ratio	#Params1	FLOPs1	img/sec2Unchanged network	65.71%	-	138M	30.96B	2,178L1-Norm	1.03%				Apoz	1.59%				Class selectivity	0.38%	50%	75.9M	7.87B	5,5413FPGM	0.59%				NISP	0.92%					0.09%	40%	87.8M	11.15B	4,869Feature entropy	0.42%	50%	75.9M	7.87B	5,541	0.89%	60%	64.2M	5.02B	6,1311 ”M” and ”B” denote the million and billion separately.
