Table 1: Comparison between the BLEU scores obtained using beam search and BATS. Pairs ofrows describes the results obtained using the vanilla autoregressive model with normalisation and thebest combination of models (Max Rank, Noisy Channel and Minimum Risk Training), tuned on thevalidation set. The translation budget is also tuned for each method for maximum BLEU.
Table 2: Comparison between BS and MCTS on the WMT2020 Chinese-English test set. Pairs ofcells denote BLEU scores and the iteration budget achieving the best BLEU on the validation set.
Table 3: Comparison between BATS and ATS on our WMT2020 Chinese-English validation set, withLog Probability (α = 0.8) as the search objective.
Table 4: Comparison between Min Prob and Max Rank in our WMT2020 Chinese-English test set.
Table 5: BLEU obtained on the WMT2020 Chinese-Englsih validation set using different metricsas risk r(y, y0). Columns “bleu(∙)" and "chrf(∙)”, denote the weights applied and Column “BLEU”denote the BLEU obtained. The first row illustrates the BLEU score obtained prior to MRT.
Table 6: Comparison between BS and MCTS in terms of computational cost using the normalisedautoregressive model in the WMT2020 Chinese-English validation set. Cells denote the computationalcost, model score obtained and the gain on score obtained relative to the row above.
Table 7: Examples of translation obtained using beam search and BATS on the Chinese-English testset using MRT tuned autoregressive models.
