Table 1: Quantitative comparison of model performance on Fluids dataset. The Static columnreports the per-frame difference between upsampled point clouds and ground truth. The AdvectionConsistency column reports the temporal coherence of two consecutive frames of upsampled pointclouds.
Table 2: Quantitative comparison of different models on					Table 3: MSR-Action 3D classification results		MSR-Action3D dataset.					using Temporal Discriminator’s feature.		Perceptual loss for point cloud sequences For real-world scanned data, we often don’t knowthe corresponding points in the next frame given the current frame. Therefore it is difficult todirectly compare the motion of upsampled sequence and target sequence. Inspired from previousvideo generation works which use pretrained I3D network (Carreira & Zisserman, 2018; Unterthineret al., 2018) to evaluate the spatio-temporal quality of generated videos at sequence level, we use apretrained point cloud classifier to extract sequence-level spatio-temporal features. The pretrainedclassifier we used here is PSTNet (Fan et al., 2021b), the state-of-the-art model on point cloudsequence classification. We use the feature map from the middle of the network as the extractedfeatures (more details can be found in Appendix A.3), and based the on the extracted features wedefine the perceptual loss (PL) for point cloud sequences as:LPL = ∣∣φ(Y) - φ(Y)∣∣ι,	(13)F Rr C^∖ r -χ r	τ z^ 1 ι , ,ι r∙	IT C τz^ -τz^	-τz^ IF , ,ιwhere Y = {Y1, Y2, . . . , YT} denotes the reference sequence, Y = {Y1, Y2, . . . , YT} denotes thepredicted sequence and φ denotes the feature extractor.
Table 4: Ablation on model architectureModel	Frame		I Sequence		CDJ ×10-3	EMDJ ×10-2	HD J ×10-1	PLJFull model	1.231	5.720	1.500	I^^0.145-w/o SpatialDis	1.356	6.021	1.508	0.156w/o TempoDis	1.185	5.821	1.458	0.155Input two frames	1.206	5.743	1.489	0.149Input five frames	1.295	5.946	1.535	0.159Figure 3: Comparison of distribution ofpoints’ top 0.1% furthest distance4.4 Ablation studyArchitectural choice We investigate the influence of our architectural choice to the model per-formance on the MSR-Action 3D dataset. The result is shown in the Table 4. The result indicatesthat: first, increasing the frame number of point clouds exposed to the temporal discriminator canimprove the the sequence level accuracy. However too long input sequence ("Input five frames") willdeteriorate the frame-level upsampled quality. As it will make points cluster around input points andexhibit halo artifacts. (See Figure 10 in the Appendix for visualization)Moreover, we find that spatial discriminator is important for the non-uniform dense point cloud upsampling. The upsampled points will shrinktowards the center of the point cloud without spatial discriminator. We
Table 5: Comparison on model size of TPU-GAN vs other state-of-the-art modelsA.2 DatasetFluid We simulate 24 fluid sequences with random initialization of fluid block position and sceneconfiguration, each sequence contains 200 snapshots with a time interval of 0.005s and particle radiusof 0.0125. Each snapshot contains roughly 100k 〜200k particles. We use 20 sequences as trainingdata, and test on the rest 4 sequences. We adopt the patch training strategy, where we sample patchescontaining 8192 particles from the fluid field and uniformly downsample them with farthest pointsampling to create input-target pairs for training.
Table 6: Scene flow estimation performance of pre-trained FlowNet3D on different input sources.
