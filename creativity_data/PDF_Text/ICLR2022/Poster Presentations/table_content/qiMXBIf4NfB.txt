Table 2: Some Important NotationsD=	{xn , yn }n=1	Labeled dataset with N number of samples;De	{xem}mM=1	Unlabeled dataset with M number of samples;d		Dimension of the input X or e;K		Number of neurons in the hidden layer;κ		Conditional number (the ratio of the largest and smallest singular values) of W *;W'		Model returned by self-training after ' iterations; W(0) is the initial model;W*		Weights of the ground truth model;Wλ		W闪=^印* + (1 — ^W⑼；1The results can be extended to binary classification with a cross-entropy loss function. Please seeAppendix-I.
Table 3: Some Important NotationsD=	{xn , yn }n=1	Labeled dataset with N number of samples;De	= {xem}mM=1	Unlabeled dataset with M number of samples;Dt	{xn , yn }n=t 1	a subset of D with Nt number of labeled data;■— Det	= {xem}mM=t 1	a subset of D With Mt number of unlabeled data;d		Dimension of the input X or e;K		Number of neurons in the hidden layer;W *		Weights of the ground truth model;W[p]		W[p] = pW* + (1 - P)W(0,0);W (`,t)		Model returned by iterative self-training after t step mini-batch stochastic gradient de- scent at stage `; W (0,0) is the initial model;^ , ^. fD,D ( or f)		The empirical risk function defined in (1);f(W;p)		The population risk function defined in (17);^ ^		The value of λδ2∕(λδ2 + eδ2);μ		The value of 旷苴吸 ; 	λρ(δ)+eρ(δ);	σi		The i-th largest singular value of W *;κ		The value of σι /σκ;γ		The value of QK=I σ⅛∕σκ;q		Some large constant in R+;D Preliminary LemmasWe will first start with some preliminary lemmas. As outlined at the beginning of the supplementary
