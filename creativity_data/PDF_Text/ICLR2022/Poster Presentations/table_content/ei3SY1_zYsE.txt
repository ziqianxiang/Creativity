Table 1: Comparing KE and Later Layer Forgetting for ResNet18. Results are mean accuracy andstandard error over 3 runs and reported for hyperparameter settings with best validation performance.
Table A1: Types of weight perturbations for image classification experiments. Random mask criteriameans that masks are generated randomly, and IMP-style mask criteria equals to 1 for correspondingweights with large final magnitudes. Mask-1 Action refers to the operation on weights with corre-sponding mask value of 1, and Mask-0 Action refers to the operation on weights with correspondingmask value of 0.
Table A2: A example of a compositional language adopted from Li & Bowling (2019).
Table A3: A example of a non-compositional language adopted from Li & Bowling (2019).
Table A4: Types of weight perturbations for compositionality experiments. Random mask criteriameans that masks are generated randomly, and weight mask criteria equals to 1 for correspondingweights with large final magnitudes. Frequency indicates how often masks are resampled. Mask-1Action refers to the operation on weights with corresponding mask value of 1, and Mask-0 Actionrefers to the operation on weights with corresponding mask value of 0.
Table A5: Weight perturbations on ResNet50 for ImageNet. Using a pretrained model, we filter outthe ImageNet training examples that are wrongly classified by the pretrained model. Hard examplesare the 10% of remaining training examples with the largest output margin, and easy examples arethe remaining training examples. We measure the accuracy of the two groups after performingvarious forgetting operations on the pretrained models. Random reinit values are the mean andstandard error over 5 samples.
Table A6: Summary of the five datasets used in Section 4.1, adopted from Taha et al. (2021).
Table A7: Later Layer Forgetting on Tiny-ImageNet. Results are test accuracy averaged over 3 runsand reported for hyperparameter settings with best validation performance. N3 and N10 indicate theadditional number of training generations on top of the baseline model. The long baseline scales thestep LR schedule based on the additional epochs.
Table A8: Later Layer Forgetting on CIFAR-10 and CIFAR-100 using WideResNet-28-10 (WRN)and DenseNet-BC (DN). Results are test accuracy averaged over 3 runs and reported for hyperpa-rameter settings with best validation performance. We consider three additional training generationson top of the baseline model. The long baseline scales the step LR schedule based on the additionalepochs.
Table A9: Comparing KE and Later Layer Forgetting for ResNet18. Results are test accuracyaveraged over 3 runs and reported for hyperparameter settings with best validation performance.
Table A10: Comparing KE and Later Layer Forgetting for DenseNet169. Results are test accuracyaveraged over 3 runs and reported for hyperparameter settings with best validation performance.
