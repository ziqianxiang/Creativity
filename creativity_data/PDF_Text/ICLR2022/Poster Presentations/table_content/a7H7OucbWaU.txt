Table 1: Averaged incremental accuracy (%) of classification tasks. The reproduced results arepresented with ± standard deviation, while others are reported results. The reproduced results mightslightly vary from the reported results due to different random seeds. 1With class-balance finetuning.
Table 2: Detection results with ± standard deviation (%)of semi-supervised continual learning on SODA10M. FT:finetuning. MR: memory replay.
Table 3: Comparison of computational cost and averaged incremental accuracy. We run each baselinewith one Tesla V100.
Table 4: Averaged incremental accuracy (%) on ImageNet-sub. The storage space of the memorybuffer is limited to the equivalent of 10, 20, 40 and 80 original images per class, respectively. Theresults of LUCIR and PODNet are reproduced from their officially-released codes.
Table 5: Averaged forgetting (%) of classes in the initial phase. The storage space is limited to theequivalent of 20 original images per class. DDE (Hu et al., 2021) and AANets (Liu et al., 2021a) arereproduced from their officially-released code.
Table 6: Averaged incremental accuracy (%) on ImageNet-sub, under a fixed memory budget of 1000or 2000 original images.
Table 7: Averaged incremental accuracy (%) of LUCIR on ImageNet-sub with different numbers ofcompressed data. “5-phase”, “10-phase” and “25-phase” refer to the accuracy of 5-, 10- and 25-phaseImageNet-sub, respectively.
