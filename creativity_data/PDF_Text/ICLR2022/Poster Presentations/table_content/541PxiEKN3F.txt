Table 1: Comparison results on MNIST and EMNIST. The ACC columns show the final accuracyon test data. The Ra columns show the minimum number of rounds required to reach a * 100% ofthe accuracy of SGD in the corresponding experiment.
Table 2: Comparison results on CIFAR-10.
Table 3: Comparison results on CIFAR-100 and CT images related to COVID-19.										Method	CIFAR-100 (ResNet-9)				CIFAR-100 (Transformer)				COVID-19 CT Images		R0.5	R0.9	Rl.0	ACC	R0.5	R0.9	Rl.0	ACC	R1.0	ACCSGD	510	1004	1200	0.434	4	16	74	0.888	4	0.511FedAvg	793	-	-	0.323	3	21	-	0.883	3	0.614FedProx	793	-	-	0.325	3	26	-	0.880	4	0.620SCAFFOLD	-	-	-	-	-	-	-	-	6	0.598FedCurv	774	-	-	0.316	3	18	-	0.882	1	0.614FedReg	248	612	796	0.502	2	14	53	0.898	1	0.673shown in Figure 2, the increases of the loss are significantly lower in FedReg than those in FedAvg,suggesting that although FedAvg and FedReg both forget some of the learned knowledge, the for-getting issue is less severe in FedReg. To further explore the role of the generated pseudo data inthe alleviation of forgetting, the values of the empirical Fisher information (Ly et al., 2017) in thepseudo data and previous training data are compared. As shown in Figure 2, the values of the pseudodata and previous training data are significantly correlated. Following the Laplace approximation(MacKay, 1992; Kirkpatrick et al., 2017), if we approximate the posterior distribution of the opti-mal parameters on a dataset by a normal distribution centered at θ(t-1) with a diagonal precisionmatrix, then the precision matrix can be approximated by the empirical Fisher information. Thesimilarity in the empirical Fisher information suggests that a model with high performance on thepseudo data has a relatively high probability to perform well on previous training data. Therefore,
Table 4: Performance comparison between the resultant models when DPSGD is applied to thebaseline methods and when MG is employed in FedReg. In each case, the final accuracy on therespective test data is reported.
Table C.1: Comparison results on Landmarks-User-160k after 2,000 rounds.
Table C.2: Average wall-clock time for running a local training stage on MNIST and EMNIST. TheGPUS are 1080 Ti. ______________________________________________Method	MNIST (two classes)	EMNISTFedAvg	0.7 S	0.3 SFedProx	0.7 s	0.3 sFedCurv	0.7 s	0.3 sFedReg	1.1 S	0.5 s17Published as a conference paper at ICLR 2022Figure C.2: Impact of ηs on the model performance on EMNIST (a) and CIFAR-10 (one class) (b).
