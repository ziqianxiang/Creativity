Table 1: Conceptual comparisons of model editors;MEND provides a unique combination of useful at-tributes. Preserves model means the editor guaran-tees model predictions Will not be altered before anedit is applied. Only (xe, ye) means the editor ap-plies an edit at test time using only the edit pair (notneeding access to the training set at test time as Well).
Table 2: Examples of using MEND to edit a T5-small model fine-tuned on Natural Questions by Robertset al. (2020). Each example shows the output of the model before and after editing. Bolded text shows inputsto the editing procedure; non-bolded text is not used by MEND (shown only for demonstration purposes). Inexamples 1 and 2, we perform multiple edits in sequence with MEND; in ex. 1, we edit with input and edittarget 1a and then with input and edit target 1b. Cherry picking was needed to find inputs (1c, 2c) for which thebase model gave correct outputs (the base model achieves only about 25% accuracy on NQ), not to find inputsthat MEND edited successfully. See Table 10 in the Appendix for additional examples and failure cases.
Table 3: Editing very large pre-trained models on our Wikitext generative editing problem and the zsREquestion-answering editing problem used by De Cao et al. (2021). MEND consistently produces more effectiveedits (higher success, lower drawdown) than existing editors. ES is the edit success rate, while ppl. DD andacc. DD are the model drawdown in units of perplexity increase or accuracy decrease, respectively. Due toENN’s memory requirements, we were unable to run the algorithm for models of this size. The low drawdownfor all T5 models may occur because the T5 models (pre-trained on mask filling and finetuned for question-answering by Roberts et al. (2020)) might not be fully converged on the question-answering problem. Editsmay therefore effectively serve as task specification, further fine-tuning the model on question-answering. FTrefers to fine-tuning; FT+KL is fine-tuning with a KL-div. penalty between the original and fine-tuned model.
Table 4: Small-scale model editing with various model editors on three editing problems. ENN andMEND show the most consistently good performance, with ENN exceeding MEND’s performance on theWikitext problem. MEND’s primary advantages are its consistent performance from 100M to 10B parametermodels and the fact that it does not modify the pre-edit model (unlike ENN). The pre-trained models and editingdata for the FEVER fact-checking and zsRE question-answering problems are used from the checkpoints anddata released by De Cao et al. (2021); for generation, we use distilGPT-2 fine-tuned on Wikitext2 (Ma, 2021).
Table 5: Batched edits with MEND and ENN onzsRE QA using the BART-base pre-trained modelfrom De Cao et al. (2021). When applying multipleedits at once, MEND is far more effective than ENN.
Table 6: Ablating various properties of MEND on the Wikitext and zsRE question-answering editing prob-lems. m = dim(u`), n = dim(δ'+ι), and N is the number of layers being edited. Removing MEND's identityinitialization and input normalization noticeably lowers editing performance, and relaxations of MEND, par-ticularly the ‘only smaller’ variant that only outputs pseudoactivations or pseudodeltas, whichever is smaller,show competitive performance, which bodes well for scaling MEND to 100 billion+ parameter models.
Table 7: Editing data samples from the FEVER fact-checking and zsRE question-answering editing datasetsfrom De Cao et al. (2021). Bold text corresponds to labels used for editing or approximating the localityconstraint.
Table 8: Training set example from the Wikitext editing dataset. Bolded text corresponds to the edit labelsye and ye0 . The locality example xloc is used to constrain the pre- and post-edit model’s predictive distributionsto be similar at for every token in the sequence.
Table 9: Editing attention matrices rather than MLP/feedforward parameters for the models considered inTable 3. Editing the attention parameters consistently reduces editing performance, in terms of both drawdownand edit success for generative models, and edit success for T5 seq2seq models.
Table 10: Additional examples of using MEND to edit a 770M parameter T5-large model fine-tuned onNatural Questions (NQ; Kwiatkowski et al. (2019)). Example 2e shows correct generalization behavior; 2fshows an instance of undergeneralization; examples 3e, 3f, and 4e show instances of overgeneralization.
Table 11: Comparing MEND with a caching-based approach to editing. For purposes of the comparison,the caching hidden-state similarity threshold e* is the one that gives similar drawdown to MEND. We founde* to be 6.5, 3, 2.5 for FEVER, zsRE, and Wikitext, respectively. Top half. Caching gives slightly betterperformance for zsRE, slightly worse performance for FEVER, and total failure for Wikitext editing, likelyowing to the longer, more complex contexts in the Wikitext data. Bottom half. Caching is relatively sensitiveto the chosen threshold, which needs to be tuned separately for each new task.
