Table 1: Accuracy results of CODETREK. Rows that are marked by * are measured by ROC-AUC,and the rest are measured by accuracy. The best performance in each row is denoted in boldface.
Table 2: Robustness results of CodeTrek.
Table 3: Impact of Program Representa-tion on Accuracy. (Rows marked by * areevaluated by ROC-AUC score.)We also evaluate the usefulness of the ability to bias random walks in CodeTrek. The accuracyresults that are reported in Table 1 are all trained on biased random walks. Specifically, in all of thetasks, nodes with types stmt, expr, and variable are biased such that they are 5 times more likely tobe traversed compared to other kinds of neighboring nodes. In addition, in the Exception task,we decrease the bias assigned to nodes of type module to 0 to avoid traveling from one function toanother through the module node they have in common. (See Appendix E for more details aboutspecification of the tasks.) This forces the walks to only go to other functions by taking call graphedges between them. We select one of the tasks, Exception-Fun, to measure the accuracy in theabsence of said biases. We re-train Exception-Fun using uniformly sampled walks and observethat the accuracy reduces from 65% to 58% as a result.
Table 4: The performance of GNN and GREAT with 10 layers.
Table 5: Contribution of different factors to the accuracy of Exception task.
Table 6: Sensitivity to the length of walks.
Table 7: Number of tuples (i.e., nodes) across relations of each file in the dataset used for each task.
Table 8: Number of AST nodes for each file in the dataset used for each task.
Table 9: The number of samples used for training, validation, and testing and the lines of code thatthey contain. Lines of code (LoC) is reported as the average lines of code across samples in eachdataset after removing the highest 0.1% and lowest 0.1% of the data.
