Table 2: Configurations of ResNet modelsA.1.3 CIFAR-100 Distribution Shift TaskHere we provide more information about the CIFAR-100 distribution-shift task, which was intro-duced in Ramasesh et al. (2021) and forms the basis of our experiments in section A.1.3 of the maintext. The idea behind this task is to mimic a way in which catastrophic forgetting might occur inpractice, in image-classification settings where the class labels remain constant but the distributionof the images changes. A key features of this type of setting is that the identity of the task is notknown to the model at inference time, so task-specific components (such as multiple readout layers)are not allowed. For example, in a medical setting, one might seek to classify images as displayingsigns of a disease or not, and the system might be trained sequentially on images taken at varioushospitals. While the CIFAR-100 dataset features images divided into 100 distinct classes, theseclasses are grouped into 20 superclasses. The CIFAR-100 distribution shift task sequence involvesclassifying an image by its superclass; individual tasks in the sequence draw images from a singlesubclass for each superclass. As a concrete example, in the experiments in the main text we usethe five superclasses aquatic mammals, fruits and vegetables, household electrical devices, trees,and vehicles-11. For the first task in the sequence, Task A, we uses the classes dolphin (for aquaticmammals), apple (for fruits and vegetables), lamp (for household electrical devices), maple tree(for trees), and bicycle (for vehicles-1). For the second task, Task B, we use the classes whale (foraquatic mammals), orange (for fruits and vegetables), television (for household electrical devices),willow (for trees), and motorcycle (for vehicles-2).
Table 3: 10-subtask split of CIFAR100 used in our experiments.
Table 4: Finetuning settings used for the two-task experiments on datasets described in section B.4.
Table 5: Comparison of head-only finetuning and full finetuning on downstream datasets. Us-ing a pretrained (supervised on ImageNet21k) ResNet101 and ViT-S_16, We train either (a) a linearlayer on top of frozen representations, or (b) the full model, on downstream datasets. We takethis as a proxy for the similarity betWeen the doWnstream dataset and ImageNet21k; the closer thehead-only performance is to the fully-trained performance, the more similarity We expect.
Table 6: Comparison of head-only finetuning and full finetuning on Task A of Split CIFAR10.
Table 7: Forgetting on the pretraining task after finetuning. Top-1 accuracy on ImageNet21kafter pretraining, and after downstream finetuning. Unlike pretrained models performing continuallearning on downstream tasks, it appears that there are significant performance drops on the originalImageNet21k task.
