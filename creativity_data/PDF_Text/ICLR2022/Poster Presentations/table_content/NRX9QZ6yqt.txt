Table 1: The splits of the data sets in number of samples. For WikiText, Penn TreeBank and MultiWoZas the models are trained on a language modeling objective, the splits are given in number of tokens.
Table 2: We experimented with models with different sizes for a comprehensive study of the proposedC variants.
Table 3: Architecture details for models used in experiments.
Table 4: Approximate time taken for one epoch of training of the models in the public repositories onthe different data sets. The time is clocked in minutes. The time per epoch increases linearly withincrease in topC over the (B)ase method. Although for lower values of topC the time per epochis smaller, there is still room for improvement to bring down the time with smart update rules asdiscussed in §7.	_______________________________________________________________Dataset	Model	B	C5	C10	C20	C20	C100MNIST	Log.Reg	0.3	-01-	0.5	0.6	0.9	-.5-	Neural-Net	0.3	0.5	0.6	0.8	1.4	2.3PTB	LSTM	0.08	0.4	0.6	0.9	2	6WikiText	LSTM	0.6	1	1.4	2.6	5	10CIFAR-10	CNN	0.5	0.9	1	1.5	3	5CIFAR-100	CNN	0.75	1.5	2	4	6	12	LSTMEnc.	1	3	6	10	25	45SNLI	InferSent	2	4	9	20	35	50	ConvNet	1	4	8	20	40	65MultiWoZ	RoBERTa	15	25	35	50	NA	NA	BiLSTM	3	4	5	5	7	10E HyperparametersE.1 Range of HparamsWe present the range of hyperparameters used in our experiments (Table 5). Optimizer-specific
Table 5: The hyperparameter configurations of the different optimizers in the experiments.
Table 6: Best hyperparameter configurations for CIFAR-10 and CIFAR-100 image classificationtasks.____________________________________________________________________________Dataset	Model	Optimizer	Learning Rate	topC	decay	Momentum	βl	β2	ɑ		Adam C	0.0001	5-	07-	N/A	0.99	0.99	N/A		Adam	0.001	N/A	N/A	N/A	0.9	0.99	N/A		RMSpropC	0.0001	5	0.7	N/A	N/A	N/A	0.9CIFAR-10	Shallow	RMSprop	0.0001	N/A	0	N/A	N/A	N/A	0.9	ConvNet	SGDMC	0.001	5	0.7	0.9	N/A	N/A	N/A		SGDM	0.001	N/A	N/A	0.9	N/A	N/A	N/A		SGDC	0.01	5	0.99	N/A	N/A	N/A	N/A		SGD		0.01	N/A	N/A	N/A	N/A	N/A	N/A		AdamC	0.00001	20-	07-	N/a-	-09-	0.9999	N/A		Adam	0.0001	N/A	N/A	N/A	0.9	0.9999	N/A		RMSProPC	0.00001	20	0.7	N/A	N/A	N/A	0.99CIFAR-100	Deep	RMSprop	0.0001	N/A	N/A	N/A	N/A	N/A	0.99	ConvNet	SGDMC	0.001	20	0.7	0.9	N/A	N/A	N/A		SGDM	0.01	N/A	N/A	0.9	N/A	N/A	N/A		SGDC	0.01	5	0.9	N/A	N/A	N/A	N/A		SGD			0.1	N/A	N/A		N/A	N/A	N/A	N/ATable 7: Best hyperparameter configurations for language modelling tasks on WikiText and PennTree-
Table 7: Best hyperparameter configurations for language modelling tasks on WikiText and PennTree-Bank datasets.________________________________________________________________________Dataset	Model	Optimizer	LR	topC	decay	Momentum	βl	β2	ɑ		AdamC	0.0001	5-	0.9	N/a-	0.9	0.999	N/A		Adam	0.0001	0	0	n/a	0.9	0.999	N/A		RMSProPC	0.0001	20	0.9	N/A	N/A	N/A	0.9PTB		RMSprop	0.0001	0	0	N/A	N/A	N/A	0.9		SGDMC	0.1	10	0.9	0.9	N/A	N/A	N/A		SGDM	0.1	0	0	0.9	N/A	N/A	N/A		SGDMC	0.1	2	0.95	N/A	N/A	N/A	N/A	LSTM	SGD		0.1	0	0	N/A	N/A	N/A	N/A		AdamC	0.0001	20-	09-	NTa-	-09-	0.9999	N/A		Adam	0.0001	0	0	N/A	0.9	0.9999	N/A		RMSProPC	0.0001	20	0.9	N/A	N/A	N/A	0.9Wikitext		RMSprop	0.0001	0	0	N/A	N/A	N/A	0.9		SGDMC	0.1	10	0.9	0.9	N/A	N/A	N/A		SGDM	0.1	0	0	0.9	N/A	N/A	N/A		SGDMC	0.1	2	0.95	N/A	N/A	N/A	N/A		SGD		0.1		O	0		N/A	N/A	N/A	N/ATable 8: Best hyperparameter configurations for text generation for dialogue on MultiWoZ2.0 dataset.
Table 8: Best hyperparameter configurations for text generation for dialogue on MultiWoZ2.0 dataset.
Table 9: Best hyperparameter configurations for text classification for language inference on SNLIdataset._________________________________________________________________________Dataset	Model		Optimizer	LR	topC	decay	Momentum	β1	β	ɑ		AdamC	0.0001	5-	07-	N/a-	0.99	0.999	N/A		Adam	0.001	0	0	N/A	0.99	0.999	N/A		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99	InferSent	RMSprop	0.001	0	0	N/A	N/A	N/A	0.99		SGDMC	0.01	5	0.7	0.9	N/A	N/A	N/A		SGDM	0.1	0	0	0.9	N/A	N/A	N/A		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A		SGD		0.1	0	0	N/A	N/A	N/A	N/A		-AdamC	0.0001	5-	07-	N/A-	0.99	0.999	N/A		Adam	0.001	0	0	N/A	0.99	0.999	N/A		RMSProPC	0.0001	5	0.7	N/A	N/A	N/A	0.99SNLI	ConvEncoder	RMSprop SGDMC	0.001 0.01	0 5	0 0.7	N/A 0.9	N/A N/A	N/A N/A	0.99 N/A		SGDM	0.1	0	0	0.9	N/A	N/A	N/A		SGDC	0.1	5	0.7	N/A	N/A	N/A	N/A		SGD		0.1	0	0	N/A	N/A	N/A	N/A		AdamC	0.0001	5-	07-	N/A-	0.99	0.999	N/A		Adam	0.001	0	0	N/A	0.99	0.999	N/A
Table 10: Complete test results on the different tasks.
