Table 1: SSNs vs. similar size traditional networks on image-sentence retrieval and phrase grounding.
Table 2: Comparison of ALBERT-base and-large (Lan et al., 2020) vs SSNs using P =4 learned parameter groups. Scores are F1and EM on dev sets.
Table 3: Knowledge distillation experiments on CIFAR-100 using OFD (Heo et al., 2019) and SRR (Yang et al.,2021). HB-SSN’s budget is 4× the student’s size. Forall settings using our SSNs improve performance overdistillation alone.
Table 4: Parameter pruning experiments withHRank (Lin et al., 2020). HB-SSN’s budget is 4×the model’s size during pretraining, then we generatethe final weights for pruning. In all settings using ourSSNs improve performance over pruning alone.
Table 5: Parameter downsampling com-parison (Section 3.1.1) using WRN-28-10and WRN-50-2 for C-10/100 and ImageNet,resp. Baseline adjusts the number and/orsize of filters rather than share parameters.
Table 6: Parameter upsampling comparison(Section 3.1.2). “Reduced Baseline” adjuststhe number and/or size of filters rather thanshare parameters. See Appendix B for theoriginal models’ performance.
Table 7: Compares methods of creating parametergroups (Section 3.2). These results demonstratethat our automatically learned mappings (auto) pro-vide more flexibility without reducing performancecompared to manual mappings.
Table 8: Weight generation method comparison. See Section B.1 for a description of the baselinemethods (RR, Avg), and Section 3.1.1 of the paper for the learned candidates (WAvg, Emb). Allmodels in the same row use the same (reduced) number of parameters. “Reduced Baseline” usesno parameter sharing, but adjusts the number and/or size of filters so they have the same number ofparameters as our SSNs. The original model’s performance is reported in the first column of resultsin Table 9.
Table 9: Parameter mapping comparison (described in Section 3.2 of the paper). Each methoduses the same number of parameters and the baseline represents no parameter sharing. Notably, theseresults demonstrate how our automatically learned groups can provide greater flexibility when sharingparameters while performing on par or better than manually created groups (which are illustrated inFigure 5).
Table 10: Parameter upsampling comparison. Most methods are described in Section 3.1.2 ofthe paper, except the method “repeat” that simply duplicates weights when upsampling. “ReducedBaseline” uses no parameter sharing, but adjusts the number and/or size of filters so they have thesame number of parameters as our SSNs. The original model’s performance is reported in the firstcolumn of results in Table 9.
Table 11: Comparison of SSNs to Hypernetworks (Ha et al., 2016) on CIFAR-10.
Table 12: Effect the number of learned parameter groups P has on performance for a SSN-WRN-28-10 model when training on C-100 using the Emb strategy. See Section C for discussion.
Table 13: Effect the number of templates K has on performance. Share type is set using the bestresults from the main paper for each method. Due to the computational cost, we set the number ofcandidates for ImageNet experiments in our paper using CIFAR results. See Section D for discussionMethod	#Candidates	2	4	6	8	10Image-Sentence Retrieval F30K EmbNet COCO	73.7 ± 0.55 74.3 ± 0.25 73.9 ± 0.18 74.3 ± 0.14 74.1 ± 0.16 81.7 ± 0.26 81.9 ± 0.30 81.6 ± 0.23 81.7 ± 0.16 81.5 ± 0.19Phrase Grounding F30K Entities SimNet ReferIt	72.1 ± 0.18 72.2 ± 0.34 72.0 ± 0.33 71.6 ± 0.29 71.8 ± 0.25 60.7 ± 0.37 61.0 ± 0.43 60.8 ± 0.39 60.4 ± 0.45 60.5 ± 0.43Image Classification WRN-28-10 C-100	19.66 ± 0.23 19.59 ± 0.21 19.48 ± 0.24 19.24 ± 0.29 19.32 ± 0.27effects performance on the image classification task. We do not benchmark bidirectional retrieval andphrase grounding since networks addressing these tasks have few layers, so parameter groups are lessimportant (as shown in Table 7).
Table 14: Comparison of different Wide Resnet (Zagoruyko & Komodakis, 2016) configurationsunder different Shapeshifter Network settings on the image classification task. (a) contains baselineresults without parameter sharing, while (b) demonstrates that using SSNs enable us to improveperformance by increasing the architecture width or depth without also increasing memory consumed,and (c) shows that the gap between SSNs with few model weights and fully-parameterized baselinesin (a) gets smaller as the networks get larger. Note that the number of parameters in (c) was set bysumming together the number of parameters required to implement the largest layer in the learnedparameter group mappings for each network (e.g., four parameter groups would result in summingfour numbers). See Section E for discussion.
Table 15: Performance of SSNs vs. traditional networks of a comparable size on image classificationon CIFAR-100 (from Figure 3 of the main paper). See Section 4.1 for discussion.
Table 16: Performance of SSNs vs. traditional networks of a comparable size on image classificationon ImageNet (from Figure 3 of the main paper). See Section 4.1 for discussion.
