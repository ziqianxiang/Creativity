Table 1: FCN: Test accuracy and training time. We report the mean, 5% and 95% quantiles of 50runs for each initialization method as well as the results of Zhou et al. (2019) and Ramanujan et al.
Table 2: Conv: Test accuracy and remaining weights. We report the mean, 5% and 95% quantilesof 50 runs for the ELUS signed Supermasks and baselines. The respective best results of Zhouet al. (2019), Ramanujan et al. (2020) (abbreviated “Ram”) and Diffenderfer & Kailkhura (2021)(abbreviated “Diff”) are shown as well. Evidently, signed Supermasks outperform the previousliterature on Supermasks on all CNN architectures, taking our extreme pruning rates of at least 97%into account. Apart from Conv2, we also gain higher performance than the baseline models.
Table 3: Conv Signed Supermask: Additional training time (abbreviated “TT” below) and compres-sion rate compared to the respective baselines. Signed Supermask CNN models train roughly 6 to10% longer, however, once trained, their required memory can be reduced by at least 93.8%.
Table 4: ResNet20: Test accuracy and remaining weights on CIFAR-10. We report the mean, 5% and95% quantiles for the ELUS signed Supermasks and ELU baselines. ResNet20 signed Supermasksare not able reach the performance of the baselines. However, the wider the ResNet20s become,the closer they get to the respective baseline and the higher the pruning rate. Furthermore, with amultiplier of 2.5 and 3, the signed Supermasks reach the performance of the original baseline with athird of the original weights.
Table 5: ResNet56/110: Test accuracy and remaining weights on CIFAR-100. We report the mean,estimated 5% and 95% quantiles for the ELUS signed Supermasks and ELU baselines as well as theutilized weights. Signed Supermask models trail behind their baseline, however, utilizing a maximumof 29% of the original weights.
Table 6: ResNet BN: Test accuracy and remaining weights on CIFAR-10 for ResNet 20 and CIFAR-100 for ResNets 56 and 110. We report the mean, estimated 5% and 95% quantiles for the ELUSsigned Supermasks and ELU baselines as well as the utilized weights.
Table 7: Neural Network architectures used in the experiments of this work. To be able to comparethe results to the existing literature, the architectures are equal to Frankle & Carbin (2018); Zhou et al.
Table 8: Hyperparameter choices for training the baseline models of FCN and Conv2 - Conv8. Forsimplicity, we chose parameters that work well on all models	FCN	Conv2	Conv4	Conv6	Conv8Learning Rate	0.008	0.008	0.008	0.01	0.002Decay Rate / Step	.96/10	.96/5	.96/10	.96/10	.96/10Red. LR on Plat. (Patience)	-	-	-	-	-Weight Decay	7e-4	7e-4	7e-4	7e-4	3e-4Momentum	.9	.9	.9	.9	.9Iterations	50	50	50	50	50Optimizer	SGD	SGD	SGD	SGD	SGDTable 9: Hyperparameter choices for training the baseline ResNet models.
Table 9: Hyperparameter choices for training the baseline ResNet models.
Table 10: Hyperparameter choices for training the signed Supermask models of FCN and Conv2 -Conv8. For simplicity, we chose parameters that work well on all models, except for Conv2 whichsuffered overfitting and we therefore reduced the learning rate.
Table 11: Hyperparameter choices for training the signed Supermask ResNet models.
Table 12: CNN Signed Supermask: Average test accuracy, test loss and required training time perepoch for each model architecture and weight initialization method. The 5% and 95% quantiles aregiven as well.
Table 13: ResNet Signed Supermask: Additional training time (abbreviated “TT” below) andcompression rate compared to the respective baselines. Signed Supermask require 3 to 14.5% moretraining time, which is made up for by the higher compression rate. Unsurprisingly, pruning ratecorrelates highly with the compression rate.
Table 14: Comparison with a broader literature periphery with regard to the total number of remainingweights on CIFAR-10. We show those results that are also compared with other obtained resultsin the respective papers. Please note that, as already stated in the introduction, a direct comparisonof all those methods is not possible for multiple reasons: first, complexity between the final resultvaries. As an example, pruned networks allow trained weights whereas a BNN only allows for binaryweights and activations. Second, many works experiment with different networks and the usage ofadditional layers such as batch normalization or dropout. If you start with a very overparameterizednetwork, probability theory gives the network much higher chances to include a better subnetworkthan a very small one. On top of that, batch normalization or dropout can have a big impact on thepredictive performance, which might hide the true performance of the proposed method.
Table 15: Simple-minded Neural Networks: Altered Hyperparameter choices to minimize weightcount during training.
