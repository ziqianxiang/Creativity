Table 1: Quantitative Experiment Result.
Table 2: StatiSticS of the datasetDataset	# of nodes	# of edges	# of graphs	# of labels	featuresBA-Shapes	700	4,110	1	4	ConstantBA-Community	1,400	8,920	1	8	Generated from LabelsTree-Cycles	871	1,950	1	2	ConstantTree-Grid	1,231	3,410	1	2	ConstantMUTAG	131,488	266,894	4,337	2	Node ClassGraph-SST2	714,325	1,288,566	70,042	2	BERT Word EmbeddingExperimental setting: For all datasets, we use a train/validation/test split of 80%/10%/10%. Forall synthetic datasets, the GCN model is trained for 1,000 epochs and the GAT model is trained for200 epochs. For MUTAG dataset, the GCN and GAT model is trained 30 epochs. For Graph-SST2dataset, the GCN model is trained 10 epochs. We use Adam optimizer and set the learning rateto 0.005, the other parameters remain at their default values. We also report the accuracy metricreached on each dataset in Table 3.
Table 3: Accuracy performance of GNN modelsDataset	BA-Shapes	BA-Community ∣ Tree-Cycles		Tree-Grid	MUTAG	Graph-SST2Task	ɪɪ	Node Classification		ɪɪ	Graph Classification	Model	GCN GAT	GCN GAr	GCN GAT	GCN GAT	GCN GAT	GCNTraining	0.96 0.98	0.99 0.83	0.91 0.93	0.85 0.83	0.80 0.81	0.91Validation	0.97 0.96	0.88 0.85	0.90 0.92	0.84 0.84	0.78 0.80	0.90Testing	0.93 0.94	0.87 0.83	0.89 0.92	0.81 0.80	0.77 0.79	0.88Hardware setting: We introduce the hardware that we use for the experiments.
Table 4: Efficiency performance.
