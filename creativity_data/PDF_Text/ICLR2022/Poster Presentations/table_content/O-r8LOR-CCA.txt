Table 1: Relationship between our novel open-world SSL and other machine learning settings.			Setting	Seen classes	Novel classes	Prior knowledgeNovel class discovery	Not present	Discover	NoneSSL	Classify	Not present	NoneRobust SSL	Classify	Reject	NoneGeneralized zero-shot learning	Classify	Discover	Class attributesOpen-set recognition	Classify	Reject	NoneOpen-world recognition	Classify	Discover	Human-in-the-loopOpen-world SSL	Classify	Discover	NoneNovel class discovery. In novel class discovery (Hsu et al., 2018; Han et al., 2020; Brbic et al., 2020;Zhong et al., 2021), the task is to cluster unlabeled dataset consisting of similar, but completelydisjoint, classes than those present in the labeled dataset which is utilized to learn better representationfor clustering. These methods assume that at the test time all the classes are novel. While thesemethods are able to discover novel classes, they do not recognize the seen/known classes. On thecontrary, our open-world SSL is more general because unlabeled test set consists of novel classesbut also classes previously seen in the labeled data that need to be identified. In principle, one couldextend novel class discovery methods by treating all classes as “novel” at test time and then matchingsome of them to the known classes from the labeled dataset. We adopt such approaches as ourbaselines, but our experiments show that they do not perform well in practice.
Table 2: Mean accuracy computed over three runs. Asterisk (*) denotes that the original method cannot recognize seen classes (and We had to extend it). Dagger (f) denotes the original method can notdetect novel classes (and we had to extend it). SimCLR and FixMatch are not applicable (NA) to thesingle-cell dataset. Improvement is computed as a relative improvement over the best baseline.
Table 3: Mean accuracy and normalized mutual Table 4: Ablation study on the components of theinformation (NMI) on CIFAR-100 dataset over objective function on the CIFAR-100 dataset. Wethree runs With UnknOWn number of novel classes. report mean accuracy and NMI over three runs.
Table 5: Mean accuracy on benchmark datasets computed over three runs. For each seen class, weonly label 10% examples. On seen classes, asterisk (*) denotes that the original method by itself cannot recognize seen classes, and we extend it by matching discovered clusters to classes in the labeleddata. On novel classes, dagger ⑴ denotes the original method cannot detect novel classes and Weextend it by performing clustering over out-of-distribution samples.
Table 6: Accuracy With different fixed margin value on the CIFAR-100 dataset.			λ	Seen	Novel	All0.3	62.4	35.7	40.50.4	60.8	37.9	43.20.5	58.2	40.0	44.30.6	48.1	41.7	43.00.7	42.9	42.2	42.6Sensitivity analysis ofη1 and η2. Parameters η1 and η2 define importance of the supervised objectiveand maximum entropy regularization, respectively. To analyze their effect on the performance, Wevary these parameters and evaluate ORCA’s performance on the CIFAR-100 dataset. Results areshoWn in Table 7. We find that higher values of η1 achieve slightly better performance on seenclasses. This result agrees Well With the intuition: giving more importance to the supervised objectiveimproves performance on the seen classes. The effect of parameter η2 on seen classes is oppositeand loWer values of η2 achieve better performance on seen classes. On novel classes, the optimalperformance is obtained When η1 and η2 are set to 1.
Table 7: Mean accuracy computed over three runs with different values ofη1 and η2 on the CIFAR-100dataset with 50%, 50% split for seen and novel classes.
Table 8: Mean accuracy over three runs with different values of regularizer λ on CIFAR-100 datasetwith 50%, 50% split for seen and novel classes.
Table 9: Mean accuracy and normalized mutual information (NMI) on the unbalanced CIFAR-10 andCIFAR-100 datasets calcu山ted over three runs._________________________________Approach	CIFAR-10				CIFAR-100				Seen	Novel	Novel (NMI)	All	Seen	Novel	Novel (NMI)	Allw/o R	84.2	61.4	64.6	62.8	55.6	35.4	50.6	35.2w/ R	90.4	82.9	74.6	69.0	65.0	37.2	53.8	40.5Results with pretraining on the labeled data. On image datasets, we pretrain ORCA and all otherbaselines jointly on labeled and unlabeled data. Here we analyze how does pretraining only onthe labeled subset of the data affect performance. The results are shown in Table 10. Comparedto pretraining on the whole dataset, we find that ORCA only slightly degrades performance whenpretrained only on the labeled subset of the data. This additionally confirms that the major contributionin ORCA’s high performance on novel classes lies in its objective function and not in the pretrainingstrategy.
Table 10: Comparison of pretraining over both labeled and unlabeled data vs. over the labeled dataonly on the CIFAR-100 dataset. We report mean accuracy over three runs.
Table 11: Comparison of pretraining strategies on the CIFAR-100 dataset. We report mean accuracyover three runs.
Table 12: Results with different margin estimation approaches on the CIFAR-100 dataset. We reportmean accuracy over three runs.
Table 13: Results with different number of novel classes on the CIFAR-100 dataset. We report meanaccuracy over three runs.
