Table 1: Overview of Reward Network variants.
Table 2: Agent hyperparameter ranges sampledfrom for training SEs and agents.
Table 3: Hyperparameter configuration spaces optimized for learning SEs.
Table 4: Subset of the hyperparameter configuration spaces that we kept fixed (not optimized) forlearning SEs.
Table 5: Default agent hyperparameters for the evaluations depicted in Figure 3, 4, and 7. DDQNearly out number and DDQN early out difference are equivalent to Table 4.
Table 6: Hyperparameters that we kept fixed (not optimized) for learning RNs.
Table 7: Hyperparameters that we optimized and which are identical across all RN experimentsHyperparameter	Symbol	Optimized valueNES mirrored sampling NES score transformation NES step size NES noise std. dev.	- - ɑ σG	True All Better 2 (see Appendix Section E.1) 0.5 	0.1	Below table lists the environment-specific HPs:Table 8: Environment-specific hyperparametersHyperparameter	CliffWalking	CartPole-v0	MountainCarContinuous-v0	HaIfCheetah-v3max. episode length	50	200	999	1000solved reward	-20	195	90	3000NES max. number	100	100	100 (TD3)	100 (TD3)of train episodes			1000 (PPO)	1000 (PPO)20Published as a conference paper at ICLR 2022Figure 11: Learned Cliff Walking reward for different reward network types. Shown is the start state(S), goal state (G), all cliff states in the lower row and the learned reward of each state-action pair(red: low reward, yellow: medium reward, green: high reward). The plots on the left side showaveraged values across 50 reward networks when considering all rewards, the plots on the right sideignore all rewards that are ≤ -50. Values have been normalized to be in a [0,1] range in both cases(left with full reward range, right without ignored rewards range).
Table 8: Environment-specific hyperparametersHyperparameter	CliffWalking	CartPole-v0	MountainCarContinuous-v0	HaIfCheetah-v3max. episode length	50	200	999	1000solved reward	-20	195	90	3000NES max. number	100	100	100 (TD3)	100 (TD3)of train episodes			1000 (PPO)	1000 (PPO)20Published as a conference paper at ICLR 2022Figure 11: Learned Cliff Walking reward for different reward network types. Shown is the start state(S), goal state (G), all cliff states in the lower row and the learned reward of each state-action pair(red: low reward, yellow: medium reward, green: high reward). The plots on the left side showaveraged values across 50 reward networks when considering all rewards, the plots on the right sideignore all rewards that are ≤ -50. Values have been normalized to be in a [0,1] range in both cases(left with full reward range, right without ignored rewards range).
Table 9: Hyperparameter sampling ranges for the reward networks HP variation experimentQL hyperparameter	Value Range	log. scalelearning rate	oyn	Falsediscount factor	0.1-1	FalseDDQN hyperparameter	Value range	log. scalelearning rate	8.3 ∙ 10-5 - 7.5 ∙ 10-4	Truebatch size	11 - 96	Truehidden size	21 - 192	Truehidden layer		1-2		FalseTD3 hyperparameter	Value range	log. scalelearning rate	10-4 - 9 ∙ 10-4	Truebatch size	85 - 768	Truehidden size	42 - 384	Truehidden layer	1 - 3	FalseB.3.3	Agent and NES Hyperparameters for Cliff WalkingIn Table 10 we list the NES and Q-Learning hyperparameter configuration spaces that we optimizedfor learning RNs on Cliff Walking. In Table 11 we list the default Q-Learning and SARSA (transfer)hyperparameters that we used once RNs have been learned. In Table 12 we list the hyperparameterconfiguration spaces that we used (and optimized over) for the count-based Q-Learning baseline.
Table 10: Hyperparameter configuration spaces optimized for learning RNs on Cliff Walking.
Table 11: Default Q-Learning and SARSA hyperparameters that we used after learning RNs.
Table 12: Used baseline hyperparameter configuration spacesHyperparameter	Optimized value	Value range	log. scaleCount-based Q-Learning β	0.1	0.0001 - 2 -	False22Published as a conference paper at ICLR 2022B.3.4	Agent and NES Hyperparameters for CartPole-v0In Table 13 we list the NES and DDQN hyperparameter configuration spaces that we optimized forlearning RNs on CartPole. We do not list the optimization value range in this table since they areequivalent as in the learning SE case described in Table 3. In Table 14 we list the default DDQNand Dueling DDQN (transfer) hyperparameters that we used once RNs have been learned. In Table13 we list the hyperparameter configuration spaces that we used (and optimized over) for the ICMbaseline (Pathak et al., 2017). We note that, despite optimizing the ICM HPs, we did not find thatthe optimized ICM HPs yielded better results than the default ones which is why we use the defaultICM HPs in all experiments that involve ICM.
Table 13: Optimized hyperparameters for learning RNs on CartPole. The value ranges are the sameas in Table 3. Hyperparameters not listed were not optimized.
Table 14: Default DDQN and Dueling DDQN hyperparameters that we used after learning RNs.
Table 15: Used ICM (Pathak et al., 2017) baseline hyperparameter configuration spaces optimizedwith CartPole and DDQN. We additionally report the default values taken from the ICM referenceimplementation.
Table 16: Optimized hyperparameters for learning RNs on MountainCarContinuous-v0 (short”CMC”) and HalfCheetah-v3 (short ”HC”). Hyperparameters not listed were not optimized.
Table 17: Default TD3 hyperparameters that we used after learning RNs onMountainCarContinuous-v0 (short ”CMC”) and HalfCheetah-v3 (short ”HC”).
Table 18: PPO (clip) hyperparameters that we used for showing the transfer of RNs onMountainCarContinuous-v0 (short ”CMC”) and HalfCheetah-v3 (short ”HC”).
Table 19: Used ICM (Pathak et al., 2017) baseline hyperparameter configuration spaces optimizedon MountainCarContinuous-v0 (short ”CMC”) and HalfCheetah-v3 (short ”HC”) with TD3. Weadditionally report the default values taken from the ICM reference implementation.
Table 20: Approximate timings for one NES outer loop for SEs and RNsEnvironment	Synthetic Environment	Reward NetworkCliffWalking	-	10sAcrobot-v1	100s	-CartPole-v0	100s	100sMOUntainCarCOntinUous-v0	-	500sHalfCheetah-v3	-	2000sGiven 200 NES outer loop iterations in the SE case and 50 steps in the RN case, these timings resultin the following overall runtime for finding one SE or RN, respectively:SEs:•	Acrobot:〜6-7 hours•	CartPole:〜5-6 hoursRNs:•	CliffWalking:〜8 minutes•	CartPole:〜7 hours•	MountainCarContinuous:〜7 hours•	HalfCheetah:〜28 hoursWe note that the required time can vary by more than a magnitude, depending on how quickly theRL agent can solve the environment in the inner loop.
Table 21: Comparing attributes between supervised learning and SE learning.
