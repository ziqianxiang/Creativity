Table 1: Convergence rates of Random Reshuffle(RR), Single Shuffle (SS) and Incremental Gra-dient Descent (IGD) on strongly convex quadratics:Plain vs. FlipFlop. Lower bounds for the “plain” ver-sions are taken from (Safran & Shamir, 2019). Whenn K, that is when the training set is much largerthan the number of epochs, which arguably is the casein practice, the convergence rates of Random Reshuf-fle,Single S huffle, and Incremental GradientDESCENT are Ω(n^)，ωnκK2)， and Ω(K) respec-tively. On the other hand, by combining these methodswith FlipFlop the convergence rates become faster,i.e., Oe( nκK5 )，Oe( nKκ4 )， and O(K), respectively.
