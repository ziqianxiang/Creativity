Table 1: Comparison with other dynamic filters.				Temporal	Location	PretrainedOperations	modelling	adaptive	weightsCondConv	X	X	XDynamicFilter	X	X	XDDF	X	✓	XTAM	✓	X	XTAdaConv	✓	✓	✓DDF (Zhou et al., 2021) in images and TAM (Liu et al., 2021b) in videos. Compared to image basedones, TAdaConv achieves temporal modelling by generating weights from the local and global tem-poral context. Compared to TANet (Liu et al., 2021b), TAdaConv is better at temopral modelllingbecause of temporally adaptive weights. Further, TAdaConv can effectively generate weights identi-cal to the pre-trained ones, while it is difficult for previous approaches to exploit pre-trained models.
Table 2: Comparison of (2+1)D convolution and TAdaConv in FLOPs and number of parameters.
Table 3: Plug-in evaluation of TAdaConv in existing video models on K400 and SSV2 datasets.									Base Model	TAdaConv			Frames	Params.	GFLOPs	K400	∆	SSV2	∆SlowOnly 8×8?	X ✓		8 8	32.5M 35.6M	54.52 54.53	74.56 75.85	- +1.29	60.31 63.30	- +2.99SlowFast 4×16?	X ✓		4+32 4+32	34.5M 37.7M	36.10 36.11	75.03 76.47	- +1.44	56.71 59.80	- +3.09SlowFast 8×8?	X ✓		8+32 8+32	34.5M 37.7M	65.71 65.73	76.19 77.43	- +1.24	61.54 63.88	- +2.34	X		8	28.1M	49.55	73.63	-	61.06	-R(2+1)D?	✓(2d)		8	31.2M	49.57	75.19	+1.56	62.86	+1.80	✓(2d+1d)		8	34.4M	49.58	75.36	+1.73	63.78	+2.72R3D?	X		8	47.0M	84.23	73.83	-	59.86	-	✓(3d)		8	50.1M	84.24	74.91	+1.08	62.85	+2.99Notation ? indicates our own implementation. See Appendix D for details on the model structure.									Table 4: Calibration weight generation. K: Table 5: Feature aggregation scheme. FA: fea- kernel size; Lin./Non-Lin.: linear/non-linear ture aggregation; Sc: shortcut for convolution									weight generation; G: global information g.				feature; SepBN: separate batch norm.					Model TAdaConv	K.	G.	Top-1	TAdaConv FA.		Sc. SepBN.		Top-1	∆TSN?	-	-	-	32.0		X-	-	-	32.0	-Lin.	1	X	37.5		✓-	-	-	59.2	+27.2Lin.	3	X	56.5		X	Avg.	X	-	47.9	+15.9Non-Lin.	(1,1)	X	36.8		X	Avg.	✓	X	49.0	+17.0Non-Lin.	(3, 1)	X	57.1		X	Avg.	✓	✓	57.0	+25.0Ours	Non-Lin.	(1, 3)	X	57.3		✓	Avg.	X	-	60.1	+28.1
Table 4: Calibration weight generation. K: Table 5: Feature aggregation scheme. FA: fea- kernel size; Lin./Non-Lin.: linear/non-linear ture aggregation; Sc: shortcut for convolution									weight generation; G: global information g.				feature; SepBN: separate batch norm.					Model TAdaConv	K.	G.	Top-1	TAdaConv FA.		Sc. SepBN.		Top-1	∆TSN?	-	-	-	32.0		X-	-	-	32.0	-Lin.	1	X	37.5		✓-	-	-	59.2	+27.2Lin.	3	X	56.5		X	Avg.	X	-	47.9	+15.9Non-Lin.	(1,1)	X	36.8		X	Avg.	✓	X	49.0	+17.0Non-Lin.	(3, 1)	X	57.1		X	Avg.	✓	✓	57.0	+25.0Ours	Non-Lin.	(1, 3)	X	57.3		✓	Avg.	X	-	60.1	+28.1Non-Lin.	(3,3)	X	57.8		✓	Avg.	✓	X	61.5	+29.5Lin.	~~1	✓	53.4		✓	Avg.	✓	✓	63.8	+31.8Non-Lin.	(1, 1)	✓	54.4		✓	Max.	✓	✓	63.5	+31.5Non-Lin.	(3, 3)	✓	59.2		✓	Mix.	✓	✓	63.7	+31.75.1	TAdaConv on existing video backbonesTAdaConv is designed as a plug-in substitution for the spatial convolutions in the video models.
Table 7: Calibration dimension.
Table 8: Comparison with the top approaches on Something-Something-V2 (Goyal et al., 2017).
Table 9: Comparison with the state-of-the-art approaches over action classification on Epic-Kitchens-100 (Damen et al., 2020). ? indicates our own implementation for fair comparison. ↑indicates the main evaluation metric for the dataset.
Table 10: Comparison with the state-of-the-art approaches on Kinetics 400 (Kay et al., 2017).					Model	Pretrain	FrameS	GFLOPS	Top-1	Top-5TSM (Lin et al., 2019a)	IN-1K	8×3×10	43	74.1	N/ASmallBigNet (Li et al., 2020a)	IN-1K	8×3×10	57	76.3	92.5TANet (Liu et al., 2021b)	IN-1K	8×3×10	43	76.3	92.6TANet (Liu et al., 2021b)	IN-1K	16×3×10	86	76.9	92.9SlowFast 4×16 (Feichtenhofer et al., 2019)	-	(4+32)×3×10	36.1	75.6	92.1SlowFast 8×8 (Feichtenhofer et al., 2019)	-	(8+32)×3×10	65.7	77.0	92.6TDN (Wang et al., 2021)	IN-1K	(8+32)×3×10	47	76.6	92.8TDN (Wang et al., 2021)	IN-1K	(16+64)×3×10	94	77.5	93.2CorrNet (Wang et al., 2020)	IN-1K	32×1×10	115	77.2	N/ATAda2D (Ours)	IN-1K	8×3×10	43	76.7	92.6TAda2D (Ours)	IN-1K	16×3×10	86	77.4	93.1TAda2DEn (Ours)	IN-1K	(8+16)×3×10	129	78.2	93.5MViT-B (Fan et al., 2021)	-	16×1×5	70.5	78.4	93.5MViT-B (Fan et al., 2021)	-	32×1×5	170	80.2	94.4TimeSformer (Bertasius et al., 2021)	IN-21K	8×3×1	196	78.0	93.7ViViT-L (Arnab et al., 2021)	IN-21K	16×3×4	1446	80.6	94.6Swin-T (Liu et al., 2021a)	IN-1K	32×3×4	88	78.8	93.6TAdaConvNeXt-T (Ours)	IN-1K	16×3×4	47	78.4	93.5
Table 11: Action localization evaluation on HACS and Epic-Kitchens-100. ↑ indicates the main					evaluation metric for the dataset, i.e., average mAP for action localization.					HACS	Epic-KitChenS-100Model	@0.5	@0.6	@0.7	@0.8	@0.9	Avg.↑	Task	@0.1	@0.2	@0.3	@0.4	@0.5	Avg.↑							Verb	15.98	15.01	14.09	12.25	10.01	13.47TSN	43.6	37.7	31.9	24.6	15.0	28.6	Noun	15.11	14.15	12.78	10.94	8.89	12.37							Act.↑	10.24	9.61	8.94	7.96	6.79	8.71							Verb	19.70	18.49	17.41	15.50	12.78	16.78TAda2D	48.7	42.7	36.2	28.1	17.3	32.3	Noun	20.54	19.32	17.94	15.77	13.39	17.39							Act.↑	15.15	14.32	13.59	12.18	10.65	13.18backbone and the same number of frames. Compared with models with more frames, e.g., TDN,TAda2D achieves a similar performance with less frames and computation. When compared to themore recent Transformer-based models, our TAdaConvNeXt-T provides competitive accuracy withsimilar or less computation.
Table A1: Comparison of different operations for spatial and temporal modeling (Lin et al., 2019a;Tran et al., 2018; Wang et al., 2020; Hara et al., 2018). ’T.’ refers to the temporal modeling ability.
Table A2: Model structure of R3D, R(2+1)D and R2D that we used in our experiments. Blue andgreen fonts indicate respectively the default convolution operation and optional operation that canbe replaced by TAdaConv. (Better viewed in color.)Stage	R3D			R(2+1)D			R2D (default baseline)			output sizesSampling	interval 8, 12			interval 8,12			interval 8,12			8 × 224 × 224-convι	3×72, 64 stride 1, 22			1×72, 64 Stride 1, 22			1×72, 64 Stride 1, 22			8×112×112res 2		-1×12,64 - 3×32, 64 1×12,256	×3		1×12, 64 1×32,64 3×12,64 1×12,256	×3		-1×12,64 - 1×32,64 1×12,256	×3	8×56×56res3		1×12, 128 - 3×32, 128 1×12,512	×4		'1×12, 128 " 1×32,128 3×12,128 1×12,512	×4		1×12, 128 - 1×32,128 1×12,512	×4	8×28×28res4		1×12,256 3×32, 256 1×12, 1024	×6		1×12,256 1×32,256 3×12,256 1×12, 1024	×6		1×12,256 1×32,256 1×12, 1024	×6	8×14×14res5		1×12,512 3×32, 512 1×12, 2048	×3		1×12,512 1×32, 512 3×12,512 1×12, 2048	×3		1×12,512 1×32, 512 1×12, 2048	×3	8×7×7global average pool, fc										-1×1×1-Proposal generation. For the action proposals, a boundary matching network (BMN) (Lin et al.,2019b) is trained over the extracted features on the two datasets. On Epic-Kitchens, we extractfeatures with the videos uniformly decoded at 60 FPS. For each clip, we use 8 frames with aninterval of 8 to be consistent with finetuning, which means a feature roughly covers a video clip ofone seconds. The interval between each clip for feature extraction is 8 frames (i.e., 0.133 sec) as well.
Table A3: Comparison with the state-of-the-art approaches over action classification on Epic-KitChens-100 (Damen et al., 2020). ↑ indicates the main evaluation metric for the dataset. Forfair comparison, We implement all the baseline models using our own training strategies.
Table A4: Ablation studies.
Table A5: Plug-in evaluation of TAdaConv on the action localization on HACS and Epic-Kitchens.
Table A6: Approach comparison between different dynamic filters. The weights column denoteshow weights in respective approaches are obtained. The pre-trained weights colmun shows whetherthe weight generation can exploit pre-trained models such as ResNet (He et al., 2016).
Table A7: Performance comparison with other dynamic filters. Our Init. denotes initializing the cal-ibration weights to ones so that the initial calibrated weights is identical to the pre-trained weights.
