Table 1: State-of-the-art adversarial robustness. Comparing experimental results of our frame-work (PORT) with baselines for adversarial training on the CIFAR-10 dataset for both '∞ and '2threat model. We sample synthetic images from the diffusion-based generative model (DDPM).
Table 2: Similar improvement on additional datasets. Our approach further improves both cleanand robust accuracy on each of datasets of Tables 1 and 2. Unless a better baseline is available inprevious works, we compare our results with baseline robust training approach from Madry et al.
Table 4: Comparing gener-ative models. When choos-ing proxy distribution in PORT('∞), DDPM model outper-forms the leading generativeadversarial network (GAN) oneach dataset.
Table 5: Comparison of cer-tified robustness. Comparingclean accuracy (Clean) and cer-tified robust accuracy (Certi-fied) of our work with earlierapproaches at 110/255 `2 per-turbation budget (equivalent to2/255 '∞ perturbation budget)on the CIFAR-10 dataset.
Table 6: Comparing different generative models. We first ad-versarially train a ten class ResNet-18 model only on 1M syn-thetic images and measure its robust accuracy on the CIFAR-10test set. We use this transferred robustness as a ground truth forbenefit of each model. Next we match ranking predicted by eachmetric with the ground truth ranking. In contrast to all threethree baseline, ranking from our proposed metric (ARC) accu-rately matches the ground-truth.
Table 7: Generative models for each dataset. In this table we list the different generative modelsused for each dataset and the number of synthetic images sampled form each model. We alsoindicate whether the generative models generate labeled images, i.e., class-conditioned. If not class-conditioned, the model only generated unlabeled synthetic images.
Table 8: Using synthetic data also improves clean and robust accuracy on AFHQ (Choi et al.,2020) dataset. Baseline refers to adversarial training based on Madry et al. (Madry et al., 2018).
Table 9: Success of different network architectures in classifying synthetic images from real-worldimage on CIFAR-10 dataset.
Table 10: Testing how much each proxy distribution helps on CIFAR-10 dataset and whether FID/IScaptures it. We train on 1M synthetic images and measure transferred robustness (which determinesthe rank) to cifar10 test set. UC and C refers to unconditional and conditional generative models,respectively.
Table 11: Further improvement in adversarial robustness using adaptive sampling. Experimen-tal results with adversarial training on the CIFAR-10 dataset for both '∞ and '2 threat model. Usingadditional synthetic data brings a large gain in adversarial robustness across network architecturesand threat models. We also show further improvements brought in by adaptive sampling, in compar-ison to random sampling, of synthetic images. Clean/Auto refers to clean/robust accuracy measuredwith AutoAttack.
Table 12: Hyperparmeter search for γ using ResNet-18 on CIFAR-100 dataset.
