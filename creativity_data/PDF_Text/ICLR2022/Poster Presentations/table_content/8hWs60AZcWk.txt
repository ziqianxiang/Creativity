Table 1: Model performance trained on ImageNet. All columns indicate the top-1 classification accuracyexcept the last column ImageNet-C which indicates the mean Corruption Error (the lower the better). The boldnumber indicates higher accuracy than the corresponding baseline. The box highlights the best accuracy.
Table 2: Model performance when pretrained on ImageNet21K and finetuned on ImageNet with 384x384 resolution. See the caption of Table 1 for more description.									Model	ImageNet	Real	Rendition	Stylized	Out of Distribution Robustness Test			A	C J					Sketch	ObjectNet	V2		ViT-B	84.20	88.61	51.23	13.59	37.83	44.30	74.54	46.59	49.62+ Ours (discrete only)	83.40	88.44	I 55.26 I	I 19.69 I	I 44.72 I	43.92	72.68	36.64	45.86+ Ours	84.43	88.88	54.64	18.05	41.91	44.61	75.17	44.97	I 38.74 J+ Ours (512x512)	85.07	89.04	54.27	16.02	41.92	I 46.62 I	I 75.55 J	I 52.64 J	38.97Our approach scales as more training data is available. Table 2 shows the performance for the modelspretrained on ImageNet-21K and finetuned on ImageNet. Training on sufficiently large datasetsinherently improves robustness (Bhojanapalli et al., 2021) but also renders further improvementeven more challenging. Nevertheless, our model consistently improves the baseline ViT-B modelacross all robustness benchmarks (by up to 10% on ImageNet-C). The results in Table 1 and Table 2validate our model as a generic approach that is highly effective for the models pretrained on thesufficiently large ImageNet-21k dataset.
Table 3: Comparison to the state-of-the-art classification accuracy on three ImageNet robustness datasets.
Table 4:	State-of-the-art mean Corruption Error (mCE) ] rate on ImageNet-C (the smaller the better).
Table 5:	Contribution of position embedding for robust recognition as measured by the relative performancedrop when the position embedding is removed. Position embedding is much more crucial in our model.
Table 6: The impact of codebook size for robustness.
Table 7: Model performance trained on ImageNet. We compared with several recent data augmentations,including CutOut (DeVries & Taylor, 2017) and CutMix Yun et al. (2019), on the ViT-B model. All columnsindicate the top-1 classification accuracy except the last column ImageNet-C which indicates the mean Cor-ruption Error (lower the better). The bold number indicates higher accuracy than the corresponding baseline.
Table 8: We replace our discrete token representation with global continuous (GC) features from the CNNmodel that has the same CNN architecture as our VQGAN encoder. We denote the dimension for the globalfeatures as GF-Dim. We vary the dimension of the pixel token to concatenate with the global continuous CNNfeatures. Simply concatenating the global feature extracted from CNN does not improve robustness.
Table 9: The impact of codebook encoder capacity for robustness.
Table 10: The impact of pretraining codebook on sufficiently large data for robustness. The code-book is pretrained on the standard ImageNet ILSVRC 2012 or the ImageNet21K dataset (about 11xlarger). Using the codebook, we train ViT models on ImageNet21K.
Table 11: Model architecture for discrete representation. We bold the numbers if they improverobustness over the baseline ViT model. The numbers are boxed where VQ-VAE further improvesrobustness than VQ-GAN under apple-to-apple comparison.
Table 12: The accuracy using different combination methods. While directly adding pixel tokento discrete token improves the most ImageNet and ImageNet-A accuracy, concatenation methodimproves the overall robustness.
Table 13: The robust performance under different pixel token dimension in concatenation combin-ing method. By increasing the dimension of the pixel embeddings, the model uses more continuousrepresentations than discrete representations. There is an inherent trade off between the out of dis-tribution robustness performance. The more continuous representations the model use, the lowerrobustness on the out-of-distribution set that depicting object shape, but also aciheves higher ac-curacy on the in-distribution ImageNet and out of distribution variants ImageNet-A that requiresdetailed information. However, using only continuous embeddings also hurt robustness. Thus wechoice to use dimension 32, which balance the trade-off and achieves the best overall robustness.
Table 14: Contribution of position embedding for robust recognition as measured by the relative performancedrop when the position embedding is removed. We experiment on ViT, Ours, Hybrid-ViT, and Ours withdiscrete token only. Note that Hybrid uses the continuous, global feature from a ResNet-50 CNN as the inputtoken. A larger relative drop indicates the model relies on spatial information more. Adding discrete tokeninput representation drives position information and spatial structure more crucial in our model.
Table 15: Model configuration for the Transformer Encoder.
Table 16: Model configuration for ResNet+ViT hybrid models.
Table 17: Model configuration for the VQ encoders.
