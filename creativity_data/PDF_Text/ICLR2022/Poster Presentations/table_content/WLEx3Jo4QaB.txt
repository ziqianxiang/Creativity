Table 1: Information comparison used during condensation, training and test for reduction methods.
Table 2: GCond and GCond-x achieves promising performance in comparison to baselines evenwith extremely large reduction rates. We report transductive performance on Citeseer, Cora, Ogbn-arxiv; inductive performance on Flickr, Reddit. Performance is reported as test accuracy (%).
Table 3: Graph condensation can work well with different architectures. Avg. stands for the average test accuracy of APPNP, Cheby, GCN, GraphSAGE and SGC. SAGE stands for GraphSAGE.											Methods	Data	MLP	GAT	APPNP	Cheby	GCN	SAGE	SGC	Avg.
Table 4: Cross-architecture performance is shown in test accuracy (%). SAGE: GraphSAGE. Graphscondensed by different GNNs all show strong transfer performance on other architectures.
Table 5: Comparison between condensed graphs and original graphs. The condensed graphs havefewer nodes and are more dense.
Table 6: Dataset statistics. The first three are transductive datasets and the last two are inductivedatasets.
Table 7: Ablation study on different parametrizations.
Table 8: Ablation study on different optimization strategies.
Table 9: Neural Architecture Search. Methods are compared in validation accuracy correlation andtest accuracy obtained by searched architecture. Whole means the architecture is searched usingwhole dataset.
Table 10: Running time of GCond for 50 epochs.
Table 11: Test accuracy on different numbers of hidden units (H) and layers (L). When L=1, thereis no hidden layer so the number of hidden units is meaningless.
Table 12: Cross-depth accuracy on Cora, r=2.6%C\T	2	3	4	5	62	80.30	80.70	79.46	76.06	71.233	40.62	72.37	40.14	67.19	35.024	74.24	72.56	76.26	71.70	65.125	71.31	75.73	70.95	73.13	67.126	75.20	75.18	75.67	76.16	75.00Propagation Versus Transformation. We further study the effect of propagation and transforma-tion on the condensed graph. We use Cora as an example and use SGC as the test model due to itsdecoupled architecture. Specifically, we vary both the propagation layers and transformation layersof SGC in the range of {1, 2, 3, 4, 5}, and report the performance in Table 13. As can be seen, thecondensed graph still achieves good performance with 3 and 4 layers of propagation. Although thecondensed graph is generated under 2-layer SGC, it is able to generalize to 3-layer and 4-layer SGC.
Table 13: Test accuracy of SGC on different transformations and propagations for Cora, r=2.6%Trans\Prop	1	2	3	4	51	77.09±0.43	79.02±1.17	78.12±2.13	74.04±3.60	61.19±7.732	76.94±0.50	79.01±0.57	79.11±1.15	77.57±1.03	72.37±4.253	75.28±0.58	77.95±0.67	74.16±1.50	70.58±3.71	58.28±8.904	66.87±0.73	66.54±0.82	59.24±1.60	43.94±6.33	30.45±9.675	46.44±0.91	37.29±3.23	16.05±2.74	15.33±2.79	15.33±2.79C.6 visualization of node features.
Table 14: Performances of various GNNs on original graphs. SAGE: GraphSAGE.
Table 15: Performance of different GNNs on Pubmed (r=0.3%).
