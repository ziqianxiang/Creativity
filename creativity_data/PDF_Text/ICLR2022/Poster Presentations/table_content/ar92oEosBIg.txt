Table 1: Solution quality and computation time for various approaches. Means and standard de-viations are reported, calculated across the entire problem set. Pareto optimal values (i.e. fasteror better solutions) are bolded. Some results (*) are taken directly from papers that do not reportcomputation time per instance, and thus cannot be compared.
Table 2: Solution quality measured after after 10 seconds of computation time per instance. Themean optimality gap and standard deviation, as well as the percentage of optimally solved problemsare reported. Our approach converges to better solutions at a faster rate than the other approaches.
Table 3: Solution quality and computation time for for learning based methods using a model trained on random 100-node problems and evaluated on TSPLIBinstances with 50 to 200 nodes and 2D Euclidean distances. Means and standard deviations are reported, calculated across 10 runs per problem instance. Paretooptimal values (i.e. faster or better solutions) are bolded.
Table 4: Summary of additional input features evaluated. While more features can help producebetter predictions, and thus better guidance for the GLS algorithm, they come at the cost of additionalcomputation time.
Table 5: Ablation analysis of our method using local search alone (no guidance), a model using edgeweight alone, and model using additional features. Both models are trained on 20-node problemsand evaluated on the 20, 50, and 100-node problem sets. The mean optimality gap and standarddeviation, as well as the percentage of optimally solved problems are reported after 10 seconds ofcomputation time per instance. While the model using additional input features produces slightlymore accurate regret predictions, any benefit is cancelled out by the additional time required tocompute these features.
