Table 1: Experimental results on the S2EF task comparing our GemNet-XL to the top entries on theOpen Catalyst leaderboard, showing metrics averaged across the 4 test datasets.
Table 2: Results on the IS2RS task comparing our models to the top entries on the Open Catalystleaderboard, showing metrics averaged across the 4 test datasets. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynam-ics data and has been shown to be helpful for the IS2RS task (Chanussot* et al., 2021).
Table 3: Results on the IS2RE task comparing our GemNet-XL to the top entries on the OpenCatalyst leaderboard, showing metrics averaged across the 4 test datasets.
Table 4: Model hyperparameters for the scaling analysis. “#GP GPUs” denotes the number of GPUsover which the graph is distributed over for pure graph parallel training on a single node. “#GP+DPGPUs” denotes the total number of GPUs used to train with graph parallel training together with32-way data parallel training.
Table 5: Full set of results on the S2EF task comparing our GemNet-XL to the top three entries onthe Open Catalyst leaderboard, showing metrics from each test set.
Table 6: Experimental results on the IS2RE task comparing our GemNet-XL to the top three entrieson the Open Catalyst leaderboard, showing metrics from each test set.
Table 7: Experimental results on the IS2RS task comparing our models to the top four entries on theOpen Catalyst leaderboard, showing metrics for each test dataset. The DimeNet++ and DimeNet++-XL models were trained on the S2EF 20M + MD dataset, that contains additional molecular dynam-ics data, which has been shown to be helpful for the IS2RS task.
