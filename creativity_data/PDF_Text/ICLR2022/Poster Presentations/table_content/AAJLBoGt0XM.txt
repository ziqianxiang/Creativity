Table 1: Conditional sampling procedure of weakly supervised, fair, and hard negative contrastive learning. Wecan regard the data pair (x, y) sampled from PXY or PXY |Z as strongly-correlated, such as views of the sameimage by applying different image augmentations; (x, y) sampled from PX|ZPY |Z as two random data that areboth associated with the same outcome of the conditioning variable, such as two random images with the sameannotative attributes; and (x, y) from PXPY as two uncorrelated data such as two random images.
Table 2: Object classification accuracy (%) under the weakly supervised contrastive learning setup. Left: resultsof the proposed method and the baselines. Right: different types of kernel choice in WeaklySupCCLK.
Table 3: Classification accuracy (%) underthe fair contrastive learning setup, and theMSE (higher the better) between color in animage and color prediction by the image’srepresentation. A higher MSE indicates lesscolor information from the original image iscontained in the learned representation.
Table 4: Classification accuracy (%) underthe hard negatives contrastive learning setup.
Table 5: Ablation study of different types of kernel choices. Left: digit classification accuracy ofFairCCLK in ColorMNIST, and MSE (higher the better) between the color in the original imageand the color predicted based on the learned representation from that image. Higher MSE is betterbecause we intend to remove color information in the representation. Right: classification accuracyof HardNegCCLK on CIFAR-10 object classification. The performances using different kernels inboth settings are consistent.
Table 6:	Results of HardNegCCLK on CIFAR10 dataset with two different training settings.
Table 7:	Result of WeaklySupCCLK under different hyper-parameters σ2 using the RBF kernel in theCUB dataset.
Table 8: Results of FairInfoNCE on the colored MNIST dataset with different numbers of clusters.
