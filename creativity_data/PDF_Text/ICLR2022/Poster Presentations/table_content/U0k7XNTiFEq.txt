Table 1: Comparison of different methods applied to					a network f .		EOC (smooth)	EOC (LReLU)	DKS		TAT (smooth)		TAT (LReLU)	q∞ exists	Q(q) = q	Q⑴= Q0(1) =	1 1	Q(1) = Q0(1) =	1 1	Q(q) Cf0 (1) Cf(0)	q 1 ηC0(1, q∞,q∞) = 1	C0(1) = 1	Cf (0) = Cf⑴=	0 ζ	Cf0 (1) = Cf00(1) =	1 τ		of this choice. The resulting method, which we name Tailored Activation Transformation (TAT)achieves competitive generalization performance with ResNets in our experiments.
Table 3: ImageNet top-1 validation accuracies ofshortcut-free networks on ImageNet.
Table 4: ImageNet top-1 validation accuracy. For rescaled ResNets (w = 0.0 or w = 0.8), we do not includeany normalization layer. For standard ResNets, batch normalization is included. By default, ReLU activation isused for standard ResNet while we use TReLU for rescaled networks.
Table 5: ImageNet top-1 validation accuracy compari-son between EOC and TAT on deep vanilla networks.
Table 6: Comparison with PReLU.				Depth	Optimizer	TReLU	PReLU0.0	PReLU0.2550	K-FAC	746	72.5	73.6	SGD	71.0	667	67.9101	K-FAC	742	71.9	72.8	SGD	70.0	543	66.3the standard ReLU), and 0.25, which was used inHe et al. (2015). We report the results on deep vanilla networks in Table 6 (see Appendix E.6 forresults on rescaled ResNets). For all settings, our method outperforms PReLU by a large margin,emphasizing the importance of the initial negative slope value. In principle, these two methods canbe combined together (i.e. we could first initialize the negative slope parameter with TAT, and thenoptimize it during training), however we did not see any benefit from doing this in our experiments.
Table 7: Comparisons between TAT and DKS. The numbers on the right hand of / are results without dropout.
Table 8: Batch size scaling.
Table 9: The effect of increasing width on Im-ageNet validation accuracy. We use vanilla net-works for EOC and TAT (ours).
Table 11: Comparison of Orthogonal Delta and Gaussian fan-ininitialization.
