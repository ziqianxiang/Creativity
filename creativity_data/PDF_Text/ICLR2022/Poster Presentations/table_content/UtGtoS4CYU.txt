Table 1: CLEVR models that we use as Visual-QA Players.
Table 2: Results of the games: Model dataset performance refers to modelsâ€™ accuracy on the CLEVRdataset. Model Mini-game performance refers to the model accuracy on the Mini-game exampleswhere the largest (maximal) performance drop was detected, before manipulations took effect. TheAverage Accuracy column reports the performance of models averaged over all runs of the respec-tive Mini-game sizes. Maximal Accuracy reports the worst model performance among the respec-tive Mini-game size runs. We also report in brackets the relative performance drop, in percentagesXXY %. Average Accuracy is compared against Model dataset performance (X = Model datasetperformance / Y = State/Pixel-Input). Maximal Accuracy is compared against Model Mini-gameperformance (X = Model Mini-game performance, Y = State/Pixel-Input). Note that in the case ofstate-input and pixel-input Adversarial Player the worst Mini-game might not come from the sameMini-game instance.
Table 3: URLs to Models. Architecture denotes that we use the code but re-trained the model onCLEVR. Model refers to already trained models.
Table 4: Results of the game With the state-input Adversarial Player.
Table 5: Results of the game with the pixel-input Adversarial Player.
