Table 1: Convergence rate of SGD with common sched-ulers on quadratic objectives.
Table 2: CIFAR-10: training losses and test accuracy of different schedules. Step Decay denotes thescheduler proposed in Ge et al. (2019) and General Step Decay means the same type of schedulerwith searched interval numbers and decay rates. “*” before a number means at least one occurrenceof loss explosion among all 5 trial experiments.
Table 3: Ridge regression: training loss gaps of different schedules over 5 trials.
Table 4: Convergence rate of SGD with common schedulers given the estimated eigenvalue distri-bution of Hessian, assuming the objective is quadratic.
Table 5: CIFAR-10: training losses and test accuracy of different schedules over 5 trials. Hereall eigencurve schedules are generated based on ResNet-18’s Hessian spectrums. “*” before anumber means at least one occurrence of loss explosion among all 5 trial experiments.
Table 6: CIFAR-10: training losses and test accuracy of Exponential Moving Average (EMA) andeigencurve with #Epoch = 100 over 5 trials. For EMA, we search its constant learning rateηt = η0 ∈ {1.0, 0.6, 0.3, 0.2, 0.1} and decay α ∈ {0.9, 0.95, 0.99, 0.995, 0.999}. Other settingsremain the same as Section 4.2.
Table 7: Elastic Step Decay on CIFAR-10/CIFAR-100: test accuracy(%) of different schedules over5 trials. “*” before a number means at least one occurrence of loss explosion among all 5 trialexperiments.
Table 8: Elastic Step Decay on ImageNet-1k: Losses and validation accuracy of different schedul-ings for ResNet-50 with #Epoch=90 over 3 trials.
Table 9: Hyperparameter search for schedulers.
Table 10: Scheduler performance on LSTM + Penn Treebank over 5 trials.
Table 11: Cosine-power Decay on ImageNet: training losses and validation accuracy (%) of differentschedulings for ReSNet-18 over 3 trials. Settings #Epoch2 90 only have 1 trial due to constraints ofresource and time.
