Table 1: Evaluation of trained CCWM and Dreamer on the ability of zero-shot transfer in cheetah runtasks with different configurations. (R: Reward; M: Mass; F: Friction; S: Stiffness.)score. This demonstrates that “learning via retracing” brings more benefits beyond plain sampleefficiency, i.e., doubling the training steps does not eliminate the performance gap. This correspondsto our hypothesis that by explicitly conveying future information back to previous states, “learningvia retracing” enables the learning of task-aware representations that support stronger behaviourlearning. To test our hypothesis, we empirically evaluate CCWM’s ability of long-term predictivereconstructions and compare with Dreamer. To ensure fair comparison, we provide further trainingfor Dreamer whenever necessary, such that the asymptotic performance is comparable with CCWM(see Appendix D for details). Figure 4 shows that CCWM consistently yields more accurate predictivereconstructions over a longer time span, on both the walker walk and cheetah run tasks. The empiricalevidence confirms our hypothesis that by incorporating “learning via retracing” into model learningenables the resulting latent space to support more accurate latent predictions, hence leading to strongerbehaviour learning. Increased range of accurate latent prediction additionally enables CCWM toperform better planning. We provide further analysis of predictive reconstruction in Appendix F.
Table 2: Configurations for the neural network implementations of CCWM.
Table 3: Evaluation of trained CCWM and Dreamer on the ability of zero-shot transfer in cheetah runtasks with different configurations. (R: Reward; M: Mass; F: Friction; S: Stiffness.)G	Full Ablation Studies on the Retrace Loss FunctionIn Figure 11, we show the full ablation studies of the retracing loss function in CCWM. We observethat the bisimulation metric retracing loss function consistently outperforms the other alternatives (L2and reconstruction error). This result conforms with our hypothesis that cycle-consistency supervisiongiven bisimulation metrics poses a stronger constraint on the learning of the representation, leadingto a learned representation that better respect the cycle-consistency properties of the environment.
