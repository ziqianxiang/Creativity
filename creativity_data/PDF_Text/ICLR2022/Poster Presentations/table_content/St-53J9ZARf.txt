Table 1: Top-1 test accuracy on CIFAR-10/100 for Wide-ResNet-28-10 and Shake-Shake-2x96d. The results ofDeepAA are averaged over four independent runs with different initializations. The 95% confidence interval isdenoted by ±.
Table 2: Top-1 test accuracy (%) on ImageNet for ResNet-50 and ResNet-200. The results of DeepAA areaveraged over four independent runs with different initializations. The 95% confidence interval is denoted by ±.
Table 3: Top-1 test accuracy (%) on CIFAR-10/100 dataset with WRN-28-10 with Batch Augmentation (BA),where eight augmented instances were drawn for each image. The results of DeepAA are averaged over fourindependent runs with different initializations. The 95% confidence interval is denoted by ±.
Table 4: Policy search time on CIFAR-10/100 and ImageNet in GPU hours.
Table 5: Top-1 test accuracy of DeepAA on CIFAR-10/100 for different numbers of augmentation layers. Theresults are averaged over 4 independent runs with different initializations with the 95% confidence intervaldenoted by ±.
Table 6: Top-1 test accuracy of DeepAA on ImageNet with ResNet-50 for different numbers of augmentationlayers. The results are averaged over 4 independent runs w/ different initializations with the 95% confidenceinterval denoted by ±.
Table 7: List of operations in the search space and the corresponding range of magnitudes in thestandard augmentation space. Note that some operations do not use magnitude parameters. We addflip and crop to the search space which were found in the default augmentation pipeline in previousworks. Flips operates by randomly flipping the images with 50% probability. In line with previousworks, crop denotes pad-and-crop and resize-and-crop transforms for CIFAR10/100 and ImageNetrespectively. We set Cutout magnitude to 16 for CIFAR10/100 dataset to be the same as the Cutout inthe default augmentation pipeline. We set Cutout magnitude to 60 pixels for ImageNet which is theupper limit of the magnitude used in AA (Cubuk et al., 2019).
Table 8: Model hyperparameters of Batch Augmentation on CIFAR10/100 for TA (Wide) andDeepAA. Learning rate, weight decay and number of epochs are found via grid search.
