Table 1: Quantitative results on 3D Shapes. DISSECT performs significantly better or on par with thestrongest baselines in all evaluation categories. Modified VAE variants perform poorly in terms of Impor-tance, worse than CSVAE, and significantly worse than EPE, EPE-mod, and DISSECT. CSVAE and otherVAE variants do not produce high-quality images, thus have poor Realism scores; meanwhile, EPE, EPE-mod,and DISSECT generate realistic samples indistinguishable from real images. While the aggregated metricsfor Importance are useful for discarding VAE baselines with poor performance, they do not show a consistentorder across EPE, EPE-mod, and DISSECT. Our approach greatly improves Distinctness, especially comparedto EPE-mod. EPE is inherently incapable of doing this, and the extension EPE-mod does, but poorly. For theSubstitutability scores, note the classifier’s precision, recall, and accuracy when training on actual data is 100%.
Table 2: Quantitative results on SynthDerm. DISSECT performs consistently best in all categories. Foranchoring the Substitutability scores, note that the precision, recall, and accuracy of the classifier when trainingon actual data is 97.7%, 100.0%, and 95.4%, respectively.
Table 3: Quantitative results on CelebA. DISSECT performs better than or on par with the baselines in allcategories. Notably, DISSECT greatly improves the Distinctness of CTs and achieves a higher Realism score,suggesting disentangling CTs does not diminish the quality of generated images and may even improve them.
Table 4: Summary of a subset of Laux iterations. The development goal is to make the first Kdimensions of the latent space Important. In some iterations, we encouraged the remaining M - Kdimensions not to be Important to reduce POtentiaI correlation across Iatent dimensions.________________________1234567PK=I df I(X)IaZkKPK=I f (x)∕∂zkKPK=I df(X)/dzkKPMd=K+1∣∂f (χ)∕∂zd |PM=K+ι |f (x)/dzd\
Table 5: Summary of hyper-parameter values. Discriminator optimization happens once every Dsteps. Similarly, generator optimization happens once every G steps. λr is specific to DISSECT,and K is specific to EPE-mod and DISSECT. All the remaining parameters are shared across EPE,EPE-mod, and DISSECT. Note that samples used for evaluation are not included in the trainingprocess.	_	Preprocessing		Training									Evaluation Metrics				N	max samples per bin	λcGAN	λrec	λf	D steps	G steps	batch size	epochs	K	λr	max # samples	batch size	epochs	hold-out test ratio3D Shapes	^3-	5,000	1	100	1	1	5-	32	300	2	10	-10,000	32	10	-025-SynthDerm	2	1,350	2	100	1	5	1	32	300	5	2	10,000	8	10	0.25CelebA	10	5,000	1	100	1	1	5	32	300	2	2	10,000	32	10	0.25A.4 Experiment setup and hyper-parameter tuning detailsExperiments were conducted on an internal compute cluster at authors’ institution. Training andevaluation of all models across the three datasets have approximately taken 1000 hours on a com-bination of Nvidia GPUs including GTX TITAN X, GTX 1080 Ti, RTX 2080 Rev, and QuadroK5200.
Table 6: Ablation experiment. Different rows show results for CelebA, with different λr values.
Table 7: Individual Importance scores per concept discovered by DISSECT. Discovered3D Shapes concepts: 1) Red shape, 2) Cyan floor. Discovered SynthDerm concepts: 1) Color,2) Diameter, 3) Asymmetry, 4) Border, 5) Surgical markings and color. Discovered CelebA con-cepts: 1) Blond hair, 2) Bangs.
Table 8: Interaction between generation quality and explanation quality. Three different sizes havebeen considered for the underlying GAN architecture: xxsmall, small, base. For comparison,a GAN-only version of base is also included, where all the other components are disabled, i.e.
