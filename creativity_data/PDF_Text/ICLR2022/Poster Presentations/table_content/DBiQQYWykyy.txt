Table 1: Downstream task performance at the end of the episode. The two tasks in blue are geometric, andthe two taSkS in green are Semantic. Gib-S/L meanS GibSon Small/large. (# i / # c) meanS number of objectinStanceS/categorieS viSited. All methodS are trained and evaluated on 2 and 3 random SeedS, reSpectively. Ineach column, the beSt methodS are highlighted in bold (uSing a one-Sided T-teSt with p = 0.05). We report onlythe mean due to Space conStraintS. See Appendix A11 for performance vS. time Step plotS.
Table 2: Ablation study of EPC with weak exploration videos. Top 2 rows are the best baselines fromTab. 1. Row B uses time to query the representation during masked-zone prediction (not pose). Rows C, Dshow the impact of sensory noise (depth and/or odometer) in the video walkthroughs. Row E shows the impactof replacing GT pose with estimates from a visual-odometry model (vo). Row F re-defines the masked-zoneprediction task to use a local input context (instead of global). See Appendix A7 for ablations with EPC (S.E).
Table A1: Hyperparameters for training our RL and self-supervised learning models. * - we use 4 PPO mini-batches and 4 PPO epochs for ANS. We disable normalized advantage and use 500-step episodes for RoomNav19Published as a conference paper at ICLR 2022O 2 4 6 8 10 12 14 O 2 4 6 8 10 12 14# training steps (millions) # training steps (millions)Figure A4: Sample efficiency on Gibson Val split.OUr environment-level pre-training leads to 2 - 6× highersample efficiency when compared to SoTA image-level pre-training.
Table A2: Ablation study of EPC with strong exploration videos. Top 2 rows are the best baselines fromTab. 1. Row B uses time to query the representation during masked-zone prediction (not pose). Rows C, D showthe impact of sensory noise (depth and/or odometer) in the video walkthroughs. Row E defines the masked-zone prediction task to use a local input context (instead of global). All methods are trained and evaluated on 2and 3 random seeds, respectively, and we report the mean. In each column, the best methods are highlighted inbold (using a one-sided T-test with p = 0.05).
Table A3: Comparing zone creation schemes for EPC. ‘EPC w/ spatial’ is a variant of EPC which uses 3Dspatial overlap between frames to cluster them into zones. ‘EPC w/ temporal’ is our original method proposedin Sec. 3. The results are on the MP3D validation split. Our temporal scheme works better while being muchsimpler to implement.
Table A4: Comparing robustness to sensor noise on downstream tasks in Gibson and Matterport3D. Weinject noise into the depth sensor and/or the odometer sensor. The depth-noise model consists of disparity-basedquantization, high-frequency noise, and low-frequency distortion (Choi et al., 2015). The odometry noise modelis based on data collected from a LoCoBot, and accumulates over time leading to a drift in pose (Ramakrishnanet al., 2020; Chaplot et al., 2020b). Note: NF denotes noise free sensing, N-D denotes noisy depth (and noise-free pose), and N-D,P denotes noisy depth and pose.
Table A5: Comparing zone feature averaging vs. sampling. ‘EPC w/ feat-sampling‘ is a variant of EPCwhich sets the target zone feature from Eqn. 2 to be the projected visual feature of a randomly frame from thezone. This leads to worse transfer results than our existing scheme which averages the projected features fromall frames belonging to the target zone.
