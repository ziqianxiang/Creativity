Table 1: Training Hyper-parametersA.2 Performance Evaluation of Trained Models: Error and LossFigure 8 demonstrate Train and Test error/loss for experiments on the effect of width. Here for MLPwe have one hidden layer, for Shallow CNN two convolutional layer, ResNet is fixed to ResNet18,and VGG16 is selected from VGG family. Figure 9 also demonstrates Train and Test error/lossfor different depth. We set MLP to have 1024 hidden units in each layer, Shallow CNN, VGG andResNest to have 1024, 64, 64 channels in each convolutional layer, respectively.
Table 2: Scaling cost for Simulated Annealing As the amount of computation increases the barrier continuesto decrease. As the number of steps increases, âˆ† increases toward 2 and does not plateau.
Table 3: Performance comparison of ensemble methods. While functional difference (He et al., 2018) givesbetter permutations compared to simulated annealing, Subspace Learning (Wortsman et al., 2021) enforces twosolutions into one basin from scratch. We combine the best of two worlds in FD + SL to guarantee functionaldiversity of learned solutions and also make them in one basin.
