Table 1: Top-1 Accuracy (%) on ImageNet datasets for ResNets, ViTs and MLP-Mixers trained withVanilla SGD or AdamW, SAM, and GSAM optimizers.
Table 3: Transfer learning results (top-1 accuracy, %)Table 2: Results (%) of GSAM andmin(fp + λh) on ViT-B/32Dataset	min(fp + λh)	GSAMImageNet	754	76.8ImageNet-Real	81.1	82.7ImageNet-v2	60.9	63.0ImageNet-R	23.9	25.1	ViT-B/16			Vanilla	SAM	GSAMCifar10	98.1	-986-	98.8Cifar100	87.6	89.1	89.7Flowers	88.5	91.8	91.2Pets	91.9	93.1	94.4mean	91.5	93.2	93.5ViT-S/16Vanilla	SAM	GSAM97.6	98.2	-98.4^85.7	87.6	88.186.4	91.5	90.3
Table 2: Results (%) of GSAM andmin(fp + λh) on ViT-B/32Dataset	min(fp + λh)	GSAMImageNet	754	76.8ImageNet-Real	81.1	82.7ImageNet-v2	60.9	63.0ImageNet-R	23.9	25.1	ViT-B/16			Vanilla	SAM	GSAMCifar10	98.1	-986-	98.8Cifar100	87.6	89.1	89.7Flowers	88.5	91.8	91.2Pets	91.9	93.1	94.4mean	91.5	93.2	93.5ViT-S/16Vanilla	SAM	GSAM97.6	98.2	-98.4^85.7	87.6	88.186.4	91.5	90.390.4	92.9	93.5
Table 4: Hyper-parameters to reproduce experimental resultsModel	Pmax	ρmin	α	lrmax	lrmin	Weight Decay	Base Optimizer	Epochs	Warmup Steps	LR scheduleResNet50	0.04	0.02	0.01	1.6	1.6e-2	03	SGD	90	5k	LinearResNet101	0.04	0.02	0.01	1.6	1.6e-2	0.3	SGD	90	5k	LinearResNet512	0.04	0.02	0.005	1.6	1.6e-2	0.3	SGD	90	5k	LinearViT-S/32	0.6	0.0	0.4	3e-3	3e-5	03	AdamW	300	10k	LinearViT-S/16	0.6	0.0	1.0	3e-3	3e-5	0.3	AdamW	300	10k	LinearViT-B/32	0.6	0.1	0.6	3e-3	3e-5	0.3	AdamW	300	10k	LinearViT-B/16	0.6	0.2	0.4	3e-3	3e-5	0.3	AdamW	300	10k	LinearMixer-S/32	0.5	0.0	0.2	3e-3	3e-5	03	AdamW	300	10k	LinearMixer-S/16	0.5	0.0	0.6	3e-3	3e-5	0.3	AdamW	300	10k	LinearMixer-S/8	0.5	0.1	0.1	3e-3	3e-5	0.3	AdamW	300	10k	LinearMixer-B/32	0.7	0.2	0.05	3e-3	3e-5	0.3	AdamW	300	10k	LinearMixer-B/16	0.5	0.2	0.01	3e-3	3e-5	0.3	AdamW	300	10k	LinearB Experimental DetailsB.1	Training detailsFor ViT and Mixer, we search the learning rate in {1e-3, 3e-3, 1e-2, 3e-3}, and search weight decayin {0.003, 0.03, 0.3}. For ResNet, we search the learning rate in {1.6, 0.16, 0.016}, and searchthe weight decay in {0.001, 0.01,0.1}. For ViT and Mixer, we use the AdamW optimizer withβ1 = 0.9, β2 = 0.999; for ResNet we use SGD with momentum= 0.9. We train ResNets for 90
Table 5: Top-1 accuracy of ViT-B/32 on ImageNet with Inception-style data augmentation. Forvanilla training We report results for training 300 epochs and 600 epochs, for GSAM We report theresults for 300 epochs.
Table 6: Top-1 Accuracy on ViT-B/32 on ImageNet. Ablation studies on constant ρ or a decayed ρt .
