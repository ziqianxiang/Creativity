Table 1: P@1 and Consistency for BERT Base across all our settings. P-Adapters are separatedbased on whether they require additional training/inference data. Note that the consistency of theOracle is 1.0 and that the Baseline does not have any standard deviation because there is no op-timization across runs. Results are microaveraged over all relations. (OOD KE: OOD KeyboardErrors).
Table 2: Tabular format showing precision from Figure 4. The Baseline does not have any standarddeviation because there is no randomness across runs. Results are microaveraged over all relations.
Table 3: Tabular format for the consistency from Figure 5. The Baseline does not have any standarddeviation because there is no randomness across runs. Note that the Oracle models achieves aconsistency of 1. Results are microaveraged over all relations. (OOD KE: OOD Keyboard Errors)Precision across all DataSetSBERT LargeFigure 4: Results across our ID and OOD datasets across three LLMs. Each row is a different model,the x-axis is the dataset and error bars show standard deviation across either 3 runs. We can see thatthe oracle method outperformed all others, while the baseline and rewriter did the worst. The trendsare similar across all modelsâ€”the only difference come from some RoBERTa MoE models havinghigher variance than others which is explained by some runs not finding the best P-Tuning templates.
Table 4: Here the terminology is applied to the example from the introduction. Note that the modelis incorrect in this example.
Table 5: P@1 and Consistency of Prefix P-Adapters on BERT Base. +consistency indicates trainingwith the consistency loss (averaged over three runs). Consistency increases a small amount, whileaccuracy stays the same. The Baseline and MoE results are included for reference.
Table 6: Microaveraged F1 score for the relation classifier component of the MOE models in eachof the evaluation settings we investigate.
