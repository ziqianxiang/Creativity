Table 1: Results on the ZINC, OGBG-MOLTOX21 and OGBG-MOLPCBA datasets. All scores areaveraged over 4 runs with 4 different seeds. Bold: GNN’s best score, Red: Dataset’s best score.
Table 2: Comparison of our best LSPE results from Table 1 with baselines and state-of-the-art GNNs(Sec. A.4) on each dataset. For ZINC, all the scores in Table 2a are the models with the ~500kparameters. The scores on OGBG-MOL* in Tables 2b and 2c are taken from the OGB project and itsleaderboards (Hu et al., 2020), where models have different number of parameters.
Table 3: Comparing the final LSPE architecture against simpler models which add pre-computedPE at input layer (or final layer) of a GNN, using GatedGCN model on ZINC. The column ‘Finalh’ denotes whether only the node structural features are used as final node features (denoted byhL), or are concatenated with (i) node positional features (denoted by [hL,pL]) at the final layer, (ii)pre-computed RWPE (denoted by [hL , RWPE]).
Table 4: Results on the IMDB-MULTI, IMDB-BINARY and CIFAR10 superpixels. All scores areaveraged over 4 runs with 4 different seeds. On IMDB- each seed experiment is on 10-fold crossvalidation. Bold: GNN's best score. No PosLoss is used with LSPE. f denotes the result is takendirectly from (Dwivedi et al., 2020).
Table 5: Additional hyperparamters for the models used in Table 1. k is the dimension of PE, or thesteps of random walk if the PE is RWPE. β and P is applicable to GraPhiT (Sec. C.2.2). InitJr andMinJr are the initial and final learning rates for the learning rate decay strategy where the lr decayswith a reduce Factor if the validation score doesn’t improve after the Patience number of epochs. αand λ are applicable when PosLoss is used (Eqn. 12).
