Table 1: Performance when using self-supervised learning or pretrained weights for the concolu-tional encoder. Improvements are small and variance remain high. This suggests that poor featuresmight not cause the high variance. Indeed, Table 2 shows that performance can be improved signif-icantly without better representation learning. Metrics are calculated over 10 seeds.
Table 2: Performance for various stability improvements. These methods often decrease variance,especially for the walker task where variance decreases by orders of magnitude. Larger stabilityoften leads to higher reward, likely as poor outlier runs are eliminated. Normalizing features givesconsistent improvements. Metrics are calculated over 10 seeds.
Table 3: Performance when combining three improvements: penultimate normalization, actionpenalty and early self-supervised learning. The variance decreases and the performance improvesacross all tasks. This demonstrates that one can substantially decrease variance in RL without hurt-ing average reward. Metrics are calculated over 40 seeds.
Table 4: Variance across tasks as reported by Yarats et al. (2021b). Mean reward and task varianceoften increase in tandem - to avoid bias towards easy tasks with high mean and thus high variance,We rank tasks based upon relative variance σ∕μ. We use the five tasks with the highest relativevariance, removing one duplicate: finger turn easy/hard. Used tasks are highlighted. Scores aremeasured just before 1 million frames (i.e. frame 980000) since not all runs have scores at the 1million mark. All numbers are rounded to two decimal digits.
Table 5: Hyperparameters used throughout the paper. These follow Yarats et al. (2021b) for themedium tasks.	_________________________________________parameter	valuelearning rate	1e-4soft-update τ	1e-2update frequency	2stddev. schedule	linear(1.0, 0.1, 500000)stddev. clip	0.3action repeat	2seed frames	4e3batch size	256n-step returns	3discount γ	0.9915Published as a conference paper at ICLR 2022acrobat120-60p」eMd」finger spin
Table 6: Performance for various types of clipped double q-learning (Fujimoto et al., 2018). Wecompare exchanging Q = min(Q1, Q2) with avg(Q1, Q2) at two locations - when computing thecritic loss (as in eq. (1)) and when computing the actor loss. The baseline always uses min, avg bothuses avg at both locations, and avg critic/actor uses the avg only when defining the critic/actor loss.
Table 7: Performance for three fixes for sparse tasks: asymmetrically clipped double Q-learning(ac), no learning until at least one non-zero reward has been observed (nz) and more self-supervisedlearning (ssl). We show mean reward and standard deviation of rewards, using ten seeds. Us-ing asymmetrically clipped double Q-learning and more self-supervised learning are the best fixes.
Table 8: Performance when changing learning rates. The baseline algorithm fails with larger learn-ing rates whereas the combined agent succeeds. This suggests that our fixes improve the stability.
Table 9: Performance for the combined agent and the baseline when using learning rate of 1e-3.
Table 10: Sample variance and algorithm variance as defined in eq. (3). Some tasks have highvariance and others have low. However, sampling more episodes does not decrease variance consid-erable in the 10 training seeds and 10 episodes setting, see eq. (4).
