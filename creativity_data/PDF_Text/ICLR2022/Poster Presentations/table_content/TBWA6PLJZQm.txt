Table 1: Summarized information of existed noisy-label benchmarks: the “estimated” noisy levelsare obtained through a subset of the dataset with verified clean labels. “Moderate-resolution” meansthe max image width pixel is less than 250.
Table 2: Comparison of test accuracies (%) on CIFAR-10N and CIFAR-100N (fine-label) usingdifferent methods. Top 3 performances are highlighted in bold (mean±standard deviation of 5 runs).
Table 3: Performance gap between human noise and class-dependent noise: test accuracy (trained onsynthetic noise) - test accuracy (trained on human noise). Negative gaps are highlighted in red.
Table 4: Consensus and noise levels (%) of each noisy label set in 10 batches (CIFAR-10N).
Table 5: Newly defined super-classes in CIFAR-100N.
Table 6: Division of each super-class in CIFAR-100N.
Table 7: Noise level (%) on CIFAR-100N.
Table 8: Comparison of test accuracies (%) on CIFAR-10N and CIFAR-100N (fine-label) usingdifferent methods. Top 3 performances are highlighted in bold (mean±standard deviation of 5runs). SOP (Liu et al., 2022) trained on Pre-act ResNet-18 architecture, while all other methods arereproduced on ResNet-34.
Table 9: Comparison of test accuracies (%) on CIFAR-10 and CIFAR-100 (fine-label) with syntheticnoisy labels using different methods. Top 3 performances are highlighted in bold (mean±standarddeviation of 5 runs).
