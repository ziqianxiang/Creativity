Table 1: WER on CSLR task. W-CTC significantly outperforms CTC when label corrupted.
Table 2: List of code bases and their references that we use in the experiments.
Table 3: List of key training hyper-parameters.
Table 4: Duration (# of frames) for TIMIT training set (in ASR and PR experiments), and thePHONEX14T dataset (in CSLR experiment).
Table 5: ASR experiments on random mask-ratio	W-CTC	ä¸€			CTC	WER	random r	r = 0.5	r = 0	r = 0.5	r = 0mean	-0.260-	0.285	0.191	0.789	0.189std	0.013	0.005	0.007	0.183	0.005As expected, the random r has similar results as r = 0.5, because the expectation is 0.5. Both arenot far from r = 0 (clean label) case, which verifies the effectiveness of W-CTC. W-CTC are muchbetter than standard CTC when r > 0.
Table 6: Mapping table from 61 phonemes to 39 phonemes to form the reduced set.
