Table 1: Unique character counting on OMNIGLOT. The averaged 0/1 accuracy results.
Table 2: Amortized clustering on 2D synthetic data and CIFAR-100. For CIFAR-100, since it wasdifficult to obtain exactly the same pre-trained VGG network as in (Lee et al., 2019a), we ran allmodels with our own pre-trained VGG network (about 71% test accuracy compared to 68.54%from (Lee et al., 2019a)).___________________________________________________________________Method	Synthetic	CIFAR-100	Log-likelihood ↑	Adjusted rand index ↑Oracle	-1.4726	0.9842DeepSet (mean pooling) (Zaheer et al., 2017)	-1.7606 ± 0.0213	0.5736 ± 0.0117DeepSet (max pooling) (Zaheer et al., 2017)	-1.7692 ± 0.0130	0.5463 ± 0.0154Dot-prod Attn (Yang et al., 2018; Ilse et al., 2018)	-1.8549 ± 0.0128	N/ASetTransformer (SAB + PMA) (Lee et al., 2019a)	-1.5145 ± 0.0046	0.9246 ± 0.0113SetTransformer (ISAB16 + PMA) (Lee et al., 2019a)	-1.5009 ± 0.0068	0.9381 ± 0.0122OTKE (P = 4,H = 5)(Mialon et al., 2021)	-1.7803 ± 0.0028	0.8207 ± 0.0074DIEM (P = 4,H = 5,k = 3,τ = 0.01, SB2)(Ours)	--1.4873 ± 0.0018	0.9770 ± 0.0019Table 3: SCOP 1.75 classification accuracies (top 1/5/10) for unsupervised and supervised learning.
Table 3: SCOP 1.75 classification accuracies (top 1/5/10) for unsupervised and supervised learning.
Table 4: SST-2 sentiment classification accuracies for unsupervised and supervised learning.
Table 6: Numbers of parameters and time complexity of OTKE (Mialon et al., 2021) and DIEM. Weassume single-head models (H = 1). DIEM follows k EM steps. Here, n = input set cardinality,d = input feature dimension, p = number of references (OTKE) or mixture components (DIEM),and M = number of SK iterations. Time complexity for one forward pass.
Table 7: (OMNIGLOT Small) Unique character counting on the OMNIGLOT small set. The averaged0/1 accuracy results. In DIEM, k = 2(00) (or k = 1(0)) means 2 (or 1) EM steps used both withregular E-steps.
Table 8: (OMNIGLOT Large) Unique character counting on the OMNIGLOT large set. The averaged0/1 accuracy results. In DIEM, k = 2(00) means 2 EM steps used both with regular E-steps.
Table 9: Amortized clustering on synthetic data. The averaged test log-likelihood scores are shown.
Table 10: Synthetic small set data clustering. The averaged test log-likelihood scores are shown. InDIEM, k = 3(000) means 3 EM steps used all with regular E-steps.
Table 11: Amortized clustering on CIFAR-100. The averaged test adjusted rand index (ARI) scoresare shown (the higher the better). In DIEM, k = 2(00) (or k = 3(000)) means 2 (or 3) EM stepsused all with regular E-steps.
Table 12: SCOP 1.75 classification accuracies (top 1/5/10) for supervised learning. The hyperparame-ter L in OTKE and DIEM indicates the number of Nystrom anchor points for the approximation ofGaussian RKHS embedding. In DIEM, k = 2(00) means 2 EM steps used both with regular E-steps.
