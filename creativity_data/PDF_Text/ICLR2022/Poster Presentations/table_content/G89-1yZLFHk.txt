Table 1: Matching probabilities estimated by CLIP on Conceptual Captions and YFCCDataset	Batch Size	Paired	Unpaired Avg	Unpaired Max	512	0.565	0.001	0.215CC 3M	1024	0.480	0.001	0.230	2048	0.398	0.000	0.238	512	0.628	0.001	0.197YFCC 15M	1024	0.551	0.000	0.219	2048	0.469	0.000	0.239To address this, we propose OTTER, or Optimal TransporT distillation for Efficient zero-shotRecognition. We improve InfoNCE to consider the many-to-many relationship between unpairedimages and texts. Specifically, given a batch of image and text tuples {(vi, ti)}i=1:N, we first useimage/text encoders to estimate a similarity matrix whose elements denotes similarity from imagevi to text caption tj . Based on the similarity matrix, we use optimal transport to find a matchingprobability between each possible image-text combination. To model the many-to-many relationship,we add an entropic regularization to the optimal transport so that the match is softly assigned.
Table 2: FH@K on test sets of Google Open Images and ImageNet10K from Tencent-ML-Images.
Table 3: Flat hit @K on ImageNet 21K+1K.
Table 4: Validation of Similarity Matrix and EMA.
Table 5: Ablation studies. ResNet50 + DeCLUTR-Sci-base evaluated on GOI test set.
Table 6: Flat hit @K on test sets of Google Open Images and ImageNet10K from Tencent-ML-Images.
Table 7: Quantitative Vision-Language Compositionality Benchmark. OR represents OverlappingRate, IOR represents Image Overlapping Rate, and TOR represents Text Overlapping Rate.
