Table 1: Accuracies for 5-shot in-context learning of GPT-3 on a filtered LAMBADA test set with shortexamples (200-300 characters). Even though there is distribution mismatch with the test set, havinglonger examples improves the accuracy, supporting theoretical intuitions. The first two rows use 5 train-ing examples in the prompt, while the last two rows use 10 training examples to equalize the total length.
