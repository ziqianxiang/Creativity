Table 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-eraged over 100 trials. We use an NVIDIA QuadrORTX8000. "©”denotes the number of trainableparameters in adapter layers. AdapterL and AdapterH are two variants of adapter tuning, which wedescribe in Section 5.1. The inference latency introduced by adapter layers can be significant in anonline, short-sequence-length scenario. See the full study in Appendix C.
Table 2: RoBERTabase, RoBERTalarge, and DeBERTaXXL with different adaptation methods on theGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew’scorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is betterfor all metrics. * indicates numbers published in prior works. f indicates runs configured in a setupsimilar to Houlsby et al. (2019) for a fair comparison.
Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLGChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparableor fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicatesnumbers published in prior works.
Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical formvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L onSAMSum. LoRA performs better than prior approaches, including full fine-tuning. The resultson WikiSQL have a fluctuation around ±0.5%, MNLI-m around ±0.1%, and SAMSum around±0.2/±0.2/±0.1 for the three metrics.
