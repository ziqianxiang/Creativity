Table 1: Comparison of Compositional Generation (left) and Image Reconstruction (right) between Slot-Attention (SA) and our model. For comparison of compositional generation, we report the FID score andaverage vote percentage received in favor of the generated images from our model. Vote percentage higher than50% implies that our model was preferred over the Slot Attention baseline. For image reconstruction quality,we report the MSE and the FID score.
Table 3: CNN Architecture for Textured-MNIST, CelebA and CLEVRTex. The number of channels, i.e.
Table 4:	Effect of number of slots and number of slot heads on the reconstruction quality in Bitmoji dataset inSLATE. We compare the benefits of increasing the number of heads in our slot attention module and the effectof increasing the number of slots. We note that simply increasing the number of slots (from 8 to 15) does notimprove the reconstruction quality significantly. However, increasing the number of heads (from 1 to 4) whilekeeping the number of slots same significantly improves the reconstruction quality.
Table 5:	Effect of number of slot heads on the reconstruction quality in Bitmoji dataset using Slot Attention(SA) with mixture-decoder. We note that with mixture decoder, having multiple heads does not benefit re-construction as the performance bottleneck is the weak object component decoder based on Spatial Broadcastdecoder (Watters et al., 2019)each head. Similarly, we project the input cells U = u1:T into key and value vectors for each head.
Table 6:	Ablation of Model Architecture in 3D Shapes. We ablate our model by replacing the discrete tokeninput (VQ) with a CNN feature map as originally used by Locatello et al. (2020). We also ablate our modelby replacing the Transformer decoder with the mixture decoder. These results suggest that the main driver ofgeneration quality is the use of Transformer decoder as both models having the transformer decoder outperformboth the models having the mixture decoder in terms of the FID score.
Table 7: Hyperparameters used for our model and computation requirements for 3D Shapes, CLEVR-Mirror,Shapestacks and Bitmoji.
Table 8: Hyperparameters used for our model and computation requirements for Textured-MNIST, CelebAand CLEVRTex.
