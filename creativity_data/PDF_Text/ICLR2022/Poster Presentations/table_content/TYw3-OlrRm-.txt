Table 1: NetAug consistently improves the ImageNet accuracy for popular tiny neural networks. Thesmaller the model, the larger the improvement. ‘w' represents the width multiplier and 'r' representsthe input image size.
Table 2: Comparison with KD (Hinton et al., 2015) on ImageNet. NetAug is orthogonal to KD.
Table 3: Regularization techniques hurt the accuracy for MobileNetV2-Tiny, while NetAug provides1.3% top1 accuracy improvement with only 16.7% training cost overhead.
Table 4: Transfer learning results of MobileNetV2 (w0.35, r160) and MobileNetV3 (w0.35, r160)with different pre-training methods. In most cases, models pre-trained with NetAug provide the besttransfer learning performance on fine-grained classification and object detection. Results that areworse than the ‘Baseline (150)‘ are in red, and results that are better than the ‘Baseline (150)‘ are ingreen. Best results are highlighted in bold.
Table 5: NetAug also benefits the tiny transfer learning (〕ai et al,, 202Ob) setting where pre-trainedweights are frozen to reduce training memory footprint.
Table 6: Ablation study on regularization methods. All models are trained for 300 epochs onImageNet.
Table 7: Ablation study on training settings. All models are trained for 150 epochs on ImageNet.
