Table 1: Group sparsity and validation accuracy of different methods. We report mean andstandard deviation of three independent runs (except that for the linear convex model, onlyone run is conducted as we are guaranteed to find the global optima). MSGD is the baselinewith no sparsity-inducing regularizer.
Table 2: Comparison between RMDA and RigL with 1000 epochs for unstructured sparsityin a single run.
Table 3: Details of the experimental settings of logistic regression on MNIST in Section 5.2.
Table 4: Details of the experimental settings of the multi-layer fully-connected NN onFashionMNIST in Section 5.2.
Table 5: Details of the experimental settings of LeNet5 on MNIST in Section 5.2Parameter	ValueData set Model Loss function Regularization function Total epochs	MNIST LeNet5 (Table 12) Cross entropy Group LASSO 500	ProXSGD	Regularization weight Learning rate schedule Momentum	1.2 × 10-4 η (epoch) = 10TTePOch / 50J 10-1	ProxSSI	Regularization weight Learning rate schedule	9 × 10-5 η (epoch) = 10-3TePOch /50J	RMDA		Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 50,100,150, 200 η(epoch) = max(10-4, 10-LePOch/50J) c (epoch) = min(1, 10-2+Lepoch /50J)Table 6: Details of the experimental settings of LeNet5 on FashionMNIST in Section 5.2Parameter	ValueData set Model Loss function Regularization function Total epochs	FashionMNIST LeNet5 (Table 12) Cross entropy Group LASSO 500	ProxSGD	Regularization weight Learning rate schedule Momentum	1.2 × 10-4 η (epoch) = 10-1-LePOch /50J 10-1	ProxSSI	Regularization weight Learning rate schedule	6 × 10-5 η (epoch) = 10-3-LePOch /50J	RMDA		Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 50,100, 150, 200 η(epoch) = max(10-4, 10-LePOch/50J) c (epoch) = min(1, 10-2+LePOch /50J)MCP for this group is then computed as (Breheny & Huang, 2009)MCP (WIg; λg,ωg) := ∙	f 讨 WIJ -⅛^ ifU WIJ“ λ, ∖ ωgλg2	if∣l WIJ ≥ ωgλg.
Table 6: Details of the experimental settings of LeNet5 on FashionMNIST in Section 5.2Parameter	ValueData set Model Loss function Regularization function Total epochs	FashionMNIST LeNet5 (Table 12) Cross entropy Group LASSO 500	ProxSGD	Regularization weight Learning rate schedule Momentum	1.2 × 10-4 η (epoch) = 10-1-LePOch /50J 10-1	ProxSSI	Regularization weight Learning rate schedule	6 × 10-5 η (epoch) = 10-3-LePOch /50J	RMDA		Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 50,100, 150, 200 η(epoch) = max(10-4, 10-LePOch/50J) c (epoch) = min(1, 10-2+LePOch /50J)MCP for this group is then computed as (Breheny & Huang, 2009)MCP (WIg; λg,ωg) := ∙	f 讨 WIJ -⅛^ ifU WIJ“ λ, ∖ ωgλg2	if∣l WIJ ≥ ωgλg.
Table 7: Details of the experimental settings of the modified VGG19 on CIFAR10 in Sec-tion 5.2.
Table 8: Details of the experimental settings of the modified VGG19 on CIFAR100 inSection 5.2.
Table 9: Details of the experimental settings of ResNet50 on CIFAR10 in Section 5.2.
Table 10: Details of the experimental settings of ResNet50 on CIFAR100 in Section 5.2.
Table 11: Details of the multi-layer fully-connected NN. https://github.com/zihsyuan1214/rmda/blob/master/Experiments/Models/mlp.py.
Table 12: Details of the modified LeNet5 for experiments in Section 5.2. https://github.
Table 13: Details of the modified VGG19. https://github.com/zihsyuan1214/rmda/blob/master/Experiments/Models/vgg19.py.
Table 14: Details of experimental settings of ResNet50 on CIFAR10 and CIFAR100 forunstructured sparsity. In this experiment, we adopt the version of ResNet50 in Sundar &Dwaraknath (2021).
Table 15: Results of training LeNet5 on MNIST using RMDA with different regularizers.
Table 16: Details of the modified simpler LeNet5 for the experiment in Appendix E.
Table 17: Details of the experimental settings for comparing different regularizers in Ap-pendix EParameter	ValueData set	MNISTModel	LeNet5 (Table 16)Loss function	Cross entropyAlgorithms	RMDATotal epochs	300Restart epochs	30, 60, 90, 120Learning rate schedule	η(epoch) = max(10-5, 10-1-Lepoch/30J)Momentum schedule	c (epoch) = min(1, 10-2+Lepoch /30J)	GLASSO		Group LASSO weight	10-5	LIGLASSO		L1 weight	10-4Group LASSO weight	10-5		GMCP		Group MCP weight	10-5Y		64		LIGMCP		
