Table 1: Basic statistics of the OGB benchmark datasets (Hu et al., 2020a).
Table 2: Results for the obgn-arxiv and ogbn-products datasets. Mean accuracy (%) ± one stan-dard deviation. Boldfaced numbers indicate the best performances of downstream models, whileunderlined numbers indicate the best performance of models with a standard GNN pipeline fordownstream models using Xplain and XSSLGNN. Methods under XGIANT (GIANT framework) arepart of the ablation study.
Table 3: Results for the obgn-papers100M dataset. Mean accuracy (%) ± one standard deviation.
Table 4: Hyper-parameters of GIANT-XRT. HLT defines the structures of the hierarchical labeltrees. lrmax is the maximum learning rate in pre-training. nstep is the number of optimization stepsfor each layer of HLT, respectively. B is total number of batch size when using 8 Nvidia V100 GPUs.
