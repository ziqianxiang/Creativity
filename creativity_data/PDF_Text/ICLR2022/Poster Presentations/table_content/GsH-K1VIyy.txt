Table 1: The accelerator design space parameters for the primary accelerator search space targeted in this work.
Table 2: The description of the applications, their domains, number of (convolutions, depth-wise convolutions,feed-forward) XLA ops, model parameter size, instruction sizes in bytes, number of compute operations.
Table 3: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the taskof learning accelerators specialized to a given application. Lower latency is better. From left to right: ourmethod, online Bayesian optimization (“Bayes Opt”), online evolutionary algorithm (“Evolutionary”), and thebest design in the training dataset. On average (last row), PRIME improves over the best in the dataset by 2.46×(up to 6.69× in t-RNN Dec) and outperforms best online optimization methods by 1.54× (up to 6.62× in t-RNNEnc). The best accelerator configurations identified is highlighted in bold.
Table 4: Optimized average latency (the lower, the better) across multiple applications (up to ten applications)from diverse domains by Prime and best online algorithms (Evolutionary and MBO) under different areaconstraints. Each row show the (Best, Median) of average latency across five runs. The geometric mean ofPrime’s improvement over other methods (last row) indicates that Prime is at least 21% better.
Table 5: Optimized objective values (i.e., latency in milliseconds) under zero-shot setting. Lower latency isbetter. From left to right: the applications used to train the surrogate model in Prime the target applications forwhich the accelerator is optimized for, the area constraint of the accelerator, Prime’s (best, median) latency, andbest online method’s (best, median) latency. Prime does not use any additional data from the target applications.
Table 6: Optimized objective values (i.e. total number of cycles) for two different dataflow architectures,NVDLA-style [42] and ShiDianNao-style [10], across three classes of applications. The maximum search spacefor the studied accelerators are ≈ 2.5×10114. PRIME generalizes to other classes of accelerators with largersearch space and outperforms the best online method by 1.06× and the best data seen in training by 3.75× (lastcolumn). The best accelerator configurations is highlighted in bold.
Table 7: Optimized objective valueS (i.e., latency in milliSecondS) obtained by PRIME and COMS [57] whenoptimizing over Single applicationS (MobilenetV2, MobilenetV3 and M6), extending Table 3. Note that PrimeoutperformS COMS. However, COMS improveS over baSeline “Standard” method (laSt column).
Table 8: Optimized objective values (i.e., latency in milliseconds) obtained by PRIME and P3BO [2] whenoptimizing over single applications (MobileNetEdgeTPU, M4, t-RNN Dec, t-RNN Enc, and U-Net). On average,PRIME outperforms P3BO by 2.5×.
Table 9: Optimized objective values (i.e., latency in milliseconds) obtained by our PRIME when using thejointly optimized model on three variants of MobileNets and use for MobileNetEdgeTPU and MobileNetV2 fordifferent dataset configurations. PRIME outperforms the best online method by 7% and finds an accelerator thatis 3.29× better than the best accelerator in the training dataset (last row). The best accelerator configuration ishighlighted in bold.
Table 10: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the taskof learning accelerators specialized to MobileNetEdgeTPU under chip area budget constraint 18 mm2 reusingthe already learned model by our method for MobileNetEdgeTPU (shown in Table 3). Lower latency/runtimeis better. From left to right: our method, our method without negative sampling (“PRIME-Opt”) and withoututilizing infeasible points (“Prime-Infeasible”), standard surrogate (“Standard”), online Bayesian optimization(“Bayes Opt”), online evolutionary algorithm (“Evolutionary”) and the best design in the training dataset. Notethat Prime improves over the best in the dataset by 12%, outperforms the best online optimization method by4.4%. The best accelerator configuration is highlighted in bold.
Table 11: Per application latency for the best accelerator design suggested by PRIME and the Evolutionarymethod according to Table 4 for multi-task accelerator design (nine applications and area constraint 100 mm2).
Table 12: Optimized accelerator configurations (See Table 1) found by PRIME and the Evolutionary methodfor multi-task accelerator design (nine applications and area constraint 100 mm2). Last row shows the accel-erator area in mm2. PRIME reduces the overall chip area usage by 1.97×. The difference in the acceleratorconfigurations are shaded in gray.
Table 13: Optimized objective values (i.e., latency in milliseconds) obtained by various methods for the task oflearning accelerators specialized to a given application. Lower latency/runtime is better. From left to right: ourmethod, our method without negative sampling (“PRIME-Opt”) and without utilizing infeasible points (“PRIME-Infeasible”), standard surrogate (“Standard”), online Bayesian optimization (“Bayes Opt”), online evolutionaryalgorithm (“Evolutionary”) and the best design in the training dataset. Note that, in all the applications Primeimproves over the best in the dataset, outperforms online optimization methods in 7/9 applications and thecomplete version of Prime generally performs best. The best accelerator designs are in bold.
Table 14: The comparison between the accelerator designs suggested by PRIME and EdgeTPU [65, 23] forsingle model specialization. On average (last row), with single-model specialization our method reduces thelatency by 2.69× while minimizes the chip area usage by 1.50×.
Table 15: Optimized objective values (i.e., latency in milliseconds) under zero-shot setting when the testapplications include all the nine evaluated models (e.g. MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNNDec, t-RNN Enc, U-Net). Lower latency is better. From left to right: the applications used to train the surrogatemodel in Prime the target applications for which the accelerator is optimized for, the area constraint of theaccelerator, Prime’s (best, median) latency, and best online method’s (best, median) latency. The best acceleratorconfigurations identified is highlighted in bold.
Table 16: Performance of PRIME (as measured by median latency of the accelerator found across five runs)under various train-test splits on three applications studied in Table 3.
Table 17: Dataset sizes for various applications that we study in this paper. Observe that all of the datasets aresmaller than 8000.
Table 18: Comparing the latency of the accelerators designed by the evolutionary approach for variable numberof simulator access budgets (8k and 32k). Even with 4× as much allowed simulator interaction, online methodsare unable to perform that well in our case.
Table 19: Hyperparameters α, β and checkpoint index (measured in terms of gradient steps on the learnedconservative model) for PRIME found by our offline cross-validation strategy discussed in Section 4, that isbased on the Kendall’s rank correlation on the validation set (note that no simulator queries were used totune hyperparameters). In the case of the multi-task and zero-shot scenarios, when training on more than oneapplication, the batch size used for training PRIME increases to N -fold, where N is the number of applicationsin the training set, therefore we likely find that even a few gradient steps are good enough.
Table 20: The evaluated applications, their model parameter size, number of compute operations, and normalizedcompute-to-memory ratio.
Table 21: Additional ablation study under zero-shot setting when the test applications include all the nineevaluated models (e.g. MobileNet (EdgeTPU, V2, V3), M4, M5, M6, t-RNN Dec, t-RNN Enc, U-Net). Lowerlatency is better. From left to right: the applications used to train the surrogate model in PRIME, the areaconstraint of the accelerator, Prime’s (best, median) latency.
