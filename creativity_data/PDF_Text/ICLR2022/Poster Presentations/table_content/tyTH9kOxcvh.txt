Table 1: Performance comparison of MBM models with the baselines for the 12 multi-label classi-fication datasets. The left section compares the models that do not require explicit taxonomy, i.e.,BoxE, MVM, MHM and MBM. In the left section, the models with the best performance w.r.t. MAPare highlighted. The right section shows the performance when we include taxonomy informationin training through LG (MVM-T, MHM-T and MBM-T), where the highlighted cells indicate animprovement in performance (MAP and CV) w.r.t. the respective non-T model. All metrics reportedare averaged across five runs with different seeds.
Table 2: Spearman rank correlation between the number ofdescendants in the label taxonomy with each of the follow-ing: embedding magnitude for MVM, negative embeddingmagnitude for MHM and box embedding volume for MBM.
Table 3: Summary of the hyper-parameter search ranges for each dataset and model. The best hyper-parameters for each model and dataset combination were picked using grid search using MAP onthe validation set. Note that ν and label sample percent are only applicable to the MBM-T, MVM-Tand MHM-T models.
Table 4: Best hyper-parameters obtained through grid search for each dataset-model combination.
Table 5: Average epoch duration in seconds for MVM, MBM, MBM-T and CHMCNN with thesame hidden sizeModel	ExprFUN	CellcycleFUN	SpoFUN	DerisiFUNMVM	17.2	15.4	15.8	15.5MBM	18.4	16.9	16.3	15.2MBM-T	20.1	26.7	24.1	27.1C-HMCNN	17.8	17.0	15.9	15.4BoxE	18.9	16.6	15.9	16.1Computational resources used: For datasets with number of labels less than 500, i.e., the 4 FUN-CAT datasets, Imclef07a, Imclef07d, Diatoms and Enron, all the models were trained on TitanXGPU (memory=12GB). For the 4 GO datasets that have number of labels greater than 4000, all themodels are trained on M40 GPU (memory=24GB).
Table 6: Summary of the datasets used in experiments. The feature based multi-label datasets spanacross 3 domains: functional genomics, image and text.
Table 7: The table provides the links to download the data from original source.
Table 8: The table presents the results of the Wilcoxon signed-rank test. Each cell shows the p-value for the null hypothesis that two related paired samples, here MBM/MBM-T and the othermodel (column), come from the same distribution, with the alternative hypothesis that MBM/MBM-T models are better in performance compared to the other model.
Table 9: The table presents the area under the precision-recall curve (AU(PRC)) for the modelspresented in this paper and HMCN-F (Wehrmann et al., 2018a). Here, the columns with * are takenfrom their respective papers, and C-HMCNN is our implementation of the corresponding model forwhich the best hyper-parameters are also obtained, like other models in our implementation, usingMAP on validation set.
Table 10: CMAP represents post-hoc correction of scores by taking the ”minimum of ancestors”,CMAP’ represents post-hoc correction of scores by taking ”maximum of descendants”. As seen,CMAP produces better results for both MBM and MVM compared to CMAP’.
Table 11: Comparing the performance of long tails of labels for MBM, MBM-T and MVM. Here,correlation is the spearman rank correlation between label frequency and MAP of the label andtail_MAP is the MAP for labels with frequency 0.001% or lower.
