Table 1: Our main results with RoBERTa-large. ↑: the full training set is used. ¢: no trainingexamples are used. Otherwise, we use K = 16 (# examples per class). We report mean (and standarddeviation) performance over 5 different splits. Majority: majority class “GPT-3” in-context learning:using the in-context learning proposed in with RoBERTa-large (no parameter updates); LM-BFF: wereport the performance in Gao et al. (2020). full: fine-tuning using full training set.
Table 2: Results on RE dataset WiKi80 (accuracy), while other datasets (micro F1). We useK = 8, 16, 32 (# examples per class). Full represents the full training set is used.
Table 3: Ablation of DART with different components on SemEval. (FT= Fine tuning)5.3	Main ResultsAs shown in Table 1, we observe that our approach obtains better performance than conventionalfine-tuning and achieves comparable results with LM-BFF. Note that DART can reduce the promptengineering without external models (e.g., T5 in LM-BFF) to generate templates that are readilyeasy to adapt to other datasets. DART can obtain 11.3% improvement with only 16 training samplesper class on the MR dataset, comparable with LM-BFF, which leverages T5 to generate appropriateprompts. These results indicate that DART can better stimulate potential ability and makes the pre-trained language model a better few-shot learner. We also notice that DART yields better performancethan P-tuning, which indicates that label optimization is beneficial.
Table 5: Full training set results with RoBERTa-large. Fine-tuning: we reported same results as Gaoet al. (2020). LM-BFF: we trained LM-BFF model (without demonstration) on full-training set.
Table 6: Few-shot performance on CR task using constrained label tokens with DART.
