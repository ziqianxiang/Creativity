Table 1: ViT-VQGAN achieves better speed-quality trade-offs compared with CNN-VQGAN.
Table 2: Transformer architectures of Stage 1 ViT-VQGAN and Stage 2 VIM.
Table 3: Frechet Inception Distance (FID) between reconstructed validation split and original val-idation split on ImageNet, CelebA-HQ and FFHQ. * denotes models trained with Gumbel-Softmaxreparameterization as in Ramesh et al. (2021). ** denotes models trained with multi-scale hierarchi-cal codebook as in Razavi et al. (2019).
Table 4: Ablation study on ViT-VQGAN. The codebook usage is calculated as the percentage ofused codes given a batch of 256 test images averaged over the entire test set.
Table 5: FID comparison with unconditional image synthesis on CelebA-HQ and FFHQ.
Table 6: FID comparison for class-conditional image synthesis on ImageNet with resolution 256 Ã—256. Acceptance rate shows results based on ResNet-101 classifier-based rejection sampling.
Table 7: Linear-probe accuracy with different unsupervised learning methods on ImageNet. DALL-E dVAE (Ramesh et al., 2021) image quantizer is trained with extra image data. VIM-Large istrained without dropout in transformers.
Table 8: FID comparison for class-conditional image synthesis on ImageNet with different Trans-former sizes in Stage 2. Results are reported without rejection sampling.
