Table 1: Training and test-time settings of learning methods for generalization across domains.
Table 2: Benefits of single sample generalization. Ablation study on PACS using ResNet-18 aver-aged over five runs. Best runs within a 95% confidence margin bolded. Single sample generalizationis more effective than the invariant amortized classifier. The hierarchical architecture further improvesthe performance. Intermediate supervision on the prior distribution is also important.
Table 3: Importance of appropriate domain shift during training on PACS using ResNet-18averaged over five runs. The more appropriate the domain shift encountered during training, thebetter our method will perform during inference.
Table 4: Ablation of meta-learning setting. The experiments are conducted on PACS using ResNet-18 averaged over five runs. Under the meta-learning framework (second row), the method performsbetter on all domains.
Table 5: Influence of backbone. The experiments are conducted on PACS over five runs. Only theaverage accuracy of four domains is shown in the Table. With larger and larger backbone models, ourmethod achieves better and better performance gaps compared to the ERM baseline.
Table 6: Comparison on PACS and Office-Home. Our method achieves best mean accuracy with aResNet-50 backbone and is competitive with ResNet-18. Notably, it surpasses the adaptive methodsby Wang et al. (2021) and Dubey et al. (2021), despite them using more data at test-time (Table 1).
Table 7: Implementation details of our method per dataset and backbone. “Number of sourcesamples” denotes the number of samples per class per source domain for generating the adaptedclassifier.
Table 8: Comparison on PACS. Our method achieves best mean accuracy with a ResNet-50 backboneand is competitive with ResNet-18. Notably, it surpasses the adaptive test-time methods by Wanget al. (2021) and Dubey et al. (2021), despite them using more data at test-time (See Table 1). fdenotes the reimplemented results in both this table and Table 9.
Table 9: Comparison on Office-Home. Our method achieves the best mean accuracy using both aResNet-18 and ResNet-50 backbone. Notably, it surpasses the adaptive test-time methods by Wanget al. (2021) and Dubey et al. (2021), despite them using more data at test-time (see Table 1).
Table 10: Comparison on rotated MNIST and Fashion-MNIST. In-distribution performance isevaluated on the test sets of MNIST and Fashion-MNIST with rotation angles of 15°, 30°, 45°, 60°and 75°, while the out-of-distribution performance is evaluated on test sets with angles of 0° and 90°.
Table 11: Experiments on single source domain generalization. We use SVHN as the source domainand MNIST as the target. Our method performs slightly better with larger domain shift simulated inthe single source domain.
