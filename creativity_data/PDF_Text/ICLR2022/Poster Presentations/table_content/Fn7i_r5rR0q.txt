Table 1: Average balanced test accuracy with or without GIT on different benchmarks. Adding GIT improvesall resampling methods, with LDAM+DRS+GIT performing best overall. Uncertainty corresponds to 95% CI’sacross 30 instances for K49 variants, and the standard error over 3 training runs for the rest.
Table 2: Ablations for the effects of the GIT class size cutoff, and RandAugment instead of GIT augmentation.
Table 3: Full experimental results for the long-tailed GTSRB dataset. In this table, we report theaverage balanced test accuracy and the standard error of the mean of 3 runs for each entry. Notethat we use GIT for all classes as opposed to only minority classes, in contrast to Table 1, but stillwe see improvements for most of the methods when combined with GIT. Section 6 shows that usingGIT only for minority classes outperforms using GIT for all classes, but since that requires onemore hyper-parameter to be tuned (class size threshold, K), we chose to use GIT for all classes inthis table.
Table 4: Full experimental results for long-tailed CIFAR-10 and CIFAR-100 datasets. In this table,we report the average balanced test accuracy and the standard error of the mean of 3 runs for eachentry. Bold numbers represent superior results for each dataset. Note that we augment all classes in-stead of only the minority classes for CIFAR-10 LT, and show that even without the hyper-parameterclass size threshold, K, we see major improvements over most (Loss type, training schedule). ForCIFAR-100 LT, using GIT to augment all classes often hurts the performance compared to not usingGIT, so we only use GIT for classes with number of training samples ≤ 100.
Table 5: Validation accuracy with or without GIT on iNaturalist-2018. We report the numbers froma single run due to long training times. We use GIT with class size cutoff K = 20.
