Table 1: Accuracy comparison of regular train-ing with a 10% larger training set and adversar-ial finetuning of 10 extra epochs (17%).
Table E.1: Hyperparameters for the attacks on NeuroSAT proposed in § 4	SAT	DEL	ADCattack steps	500	500	500learning rate	0.1	0.1	0.1fraction of perturbed literals ∆	5% of edges	5% of edges	25% of clauses# final samples	20	20	20temperature scaling	5	5	5Because the attack optimizes over a set of discrete edges with a gradient based method, similarlyto (Xu et al., 2019), we relax the edge weights to [0, 1] during the attack. Before generating theperturbed problem instance A, We sample the discrete M0 from a Bernoulli distribution where theentries of the matrix M represent the probability of success. In contrast to (Xu et al., 2019) wesample 19 instead of 20 times but add an additional sample that chooses the top elements in M .
Table F.1: Hyperparameters for the attacks on TSP proposed in § 5	DTSP	ConvTSPmaximum number of adv. nodes ∆	5	5attack steps	200	500learning rate	0.001	0.01gradient project learning rate	0.002	0.002gradient project steps	3	3instance x ∈ [0, 1](n+∆)×2 which maximizes the loss:max'(fθ(X), Y) s.t. Yxi with i > n Proposition 2 holdsX(F.1)Attack details. The input of the TSP is represented by the coordinates x ∈ [0, 1]n×2 for the nnodes. Additionally, we know the near-optimal route Y obtained with the Concorde solver whichwe use as ground truth. During the TSP attack we add additional adversarial nodes Z ∈ [0, 1]∆×2to the input problem x. The adversarial nodes are initialized by randomly sampling nodes until theyfulfill the constraint from Proposition 2 (line 1). For the attack, as described in Algorithm F.1, weoperate solely on the coordinates of the adversarial nodes and assume that the model converts thecoordinates into a weighted graph. For the DTSP model, we additionally calculate the updated routecost c0 (accordingly to y) and append it to the input. For simplicity, we omitted this special case inAlgorithm F.1. Moreover, for the DTSP We do not pass Y to the loss; instead, We use the decision
Table G.1: Runtime in milliseconds for the Glucose and MiniSAT solver on 100 clean and perturbedproblem instances from 50-100	Perturbed	DEL	ADC	SATGlucose		0.1897 ± 0.20	0.1944 ± 0.13	0.1315 ± 0.06	X	0.0335 ± 0.03	0.0816 ± 0.11	0.1391 ± 0.08MiniSAT		0.1351 ± 0.07	0.1510 ± 0.11	0.0980 ± 0.05	X	0.0189 ± 0.02	0.0587 ± 0.09	0.1383 ± 0.31O(n2) since we need to evaluate the distances to all nodes (except the special case where both nodesare on the convex hull but are non-adjacent, see Corollary 2). However, checking the constraint hasthe same complexity as the neural solver. Therefore, the overall space complexity turns out to beO(∆n2) = O(n2) and the time complexity is O(n2s) with the number of attack steps s.
Table I.1: Number of randomsamples to match optimizedloss∆=	0.01	0.05SAT	1257	9532DEL	749	4854drawn. While the efficiency of the proposed approach in general is important to consider for practicalrelevance, it is not sufficient to only compare to the random baseline in terms of sample efficiency.
