Table 1: Top-1 test accuracy (%) of student networks on CIFAR-100 with various distillation methods (ourmethod denoted by ProtoCPC). (↑) denotes outperformance over KD and bold font denotes the best accuracywithin the baseline. Note that ProtoCPC always outperform KD as well as other baselines except CRD. OurProtoCPC outperforms CRD in 4 out of 7 benchmarks. We also provide results on combining ProtoCPC andCRD, showing that our method is compatible with CRD as they learn different structure. The results are averagedover 5 runs.
Table 2: Top-1 test accuracy(%) of student networks on CIFAR-100 of a various distillation methods (ours isProtoCPC) for transfer across very different teacher and student architectures. ProtoCPC outperforms KD andall other methods except CRD. Our method outperforms CRD on 4 out of 6 benchmarks. We also observe thatcombining ProtoCPC and CRD boosts the performance significantly. Average over 3 runs.
Table 3: Top-1 and Top-5 error rates (%) of student network ResNet-18 on ImageNet validation set. For faircomparison, we use same setting as in Tian et al. (2019). We compare our ProtoCPC with KD (Hinton et al.,2015), AT (Zagoruyko & Komodakis, 2016a), SP (Tung & Mori, 2019), CC (Peng et al., 2019), Online KD (Lanet al., 2018), and CRD (Tian et al., 2019).
Table 4: Main result of our ProtoCPC on distillation of various self-supervised teacher models to ResNet-18.
Table 5: Comparison of our method with SEED. * de-notes training with multi-crops. Every teacher networksare ResNet-50 and student networks are ResNet-18.
Table 6: Results of representation learning trained byDINO (Caron et al., 2021) and ProtoCPC (ours). Allexperiments are run by ours with 100 epochs.
Table 7: Ablations on ProtoCPC loss with prior momen-tum rate mp on DeiT-S/16 trained for 100 epochs.
Table 8: Ablation study of losses and assignments of pt on super-vised model compression on CIFAR-100. We compare ProtoCPCand cross-entropy (CE), and Sinkhorn-Knopp (SK) operator andsoftmax (SM) operator. Average over 5 runs.
Table 9: Ablation of setting teacher’s pro-totypes for self-supervised model compres-sion on ResNet-18. New P denotes copy-ing student’s prototype and Old P denotesusing pre-trained prototypes.
