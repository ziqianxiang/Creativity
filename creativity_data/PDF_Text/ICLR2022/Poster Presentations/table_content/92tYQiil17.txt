Table 1: OrdAcc (%) and C orLoc (%) on new digits environment.
Table 2: CorLoc (%) when adapted to other background on corrupted MNIST dataset.
Table 3: CorLoc (%) when adapted to other species/classes on CUB dataset.
Table 4: CorLoc (%) Adaptation comparison with other methods on COCO dataset. "TFA w/ fc":TFA with a cosine similarity based box classifier; "TFA w/ cos": TFA with a normal FC-basedclassifier; "FRCN+ft-full": Meta-RCNN with Faster RCNN as detector and finetuning both thefeature extractor and the box predictor.
Table 5: CorLoc (%) comparison with Faster RCNN on source domain.
Table 6: CorLoc (%) comparison with Faster RCNN fine-tuned on target domain.
Table 7: CorLoc (%) RL with Faster RCNN embedding w/ and w/o ordinal pre-training on sourcedomain.
Table 8: Breaking up the requirements on data and labels in training and adaptation	Supervised methods		Our approach			Training	Fine-tuning	Ordinal embedding	Agent training	Test-time adaptationImage-box pairs	✓	✓	✓	X	XUnlabeled images	X	X	X	✓	✓Exemplar images	X	X	/		✓		✓	Our agent can both take human feedback in terms of the exemplary set and perform test-time policyadaptation using unlabeled test data. It includes three stages: ordinal representation pre-training, RLagent training, and test-time adaptation. Details are as follows:Stage 1: Pre-train ordinal embedding for state representation and reward In this stage, weassume pairs of ground-truth bounding box and training image are available. We train the ordinalembedding by attaching a projection head after RoI encoder, as Figure 5 shows. RoI encoder iscomposed of image encoder and a RoIAlign layer (He et al., 2017). It extracts corresponding boundingbox feature directly from image feature output by image encoder. After training, all the modules arefixed. The output of RoI Encoder then becomes the state for agent, and output of projection head isthen used for reward computation.
Table 9: Network and loss updating details for different stages of the method	Configuration			Training		Testing of the RL agent	Modules	Objective	Network	Exemplary set	Ordinal pre-training	Policy training	before adaptation	after adaptation-ROI Encoder-	NA^	VGG-16/ViT	Etrain	Frozen	Frozen	Frozen	FrozenPrOjeCtiOn Head	Ordinal loss LtriPIet	MLP		Etrain	Train	Frozen	Frozen	FrozenController	Reward	RNN 一		EteSt	NA	Train	Frozen	UPdated* For CMNIST dataset, ROI encoder is trained under loss Eq.5. For other datasets, We directly load the off-the-shelf pre-trained network.
Table 10: Summary of losses used on different datasets.
Table 11: Number of images for training and testing in stage 3 on CUB dataset.
Table 12: Number of images for training and testing in stage 1, 2 on COCO dataset.
Table 13: CorLoc(%) Adaptation comparison with other methods on COCO dataset per class results.
Table 14: CorLoc(%): DQN (with/without history action vector) vs. PG (with/without RNN)Method	DQN	DQN+History	PG	PG+RNNDigit 4	88.801.6	86.544.3	88.982.9	94.680.9Other digits	84.212.0	81.753.4	81.912.7	89.051.7Figure 10: CorLoc(%) comparison with ranking method using ImageNet pre-trained backbone.
Table 15: C orLoc(%) compare backbones on source domain.
Table 16: CorLoc(%) compare backbones on target domain.
Table 18: Effect of exemplary set size during training stage.
Table 19: Effect of exemplary set size during adaptation.
Table 20: Anchor choice comparison. Note that in evaluations the OrdAcc is always computed usinginstance as anchor.
Table 21: Evaluation under few-shot localization setting.
