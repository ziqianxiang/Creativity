Table 1: When compared with the latest weight quantization algorithms, DKM-based algorithm shows superiorTop-1 accuracy When the netWork is hard to optimize (i.e., MobileNet-v1/v2) or When a loW precision is required(1 bit). Further, With multi-dimensional DKM (see Section 3.3), DKM delivers 64.3 % Top-1 accuracy forMobileNet-v1 With the 8/8 configuration Which is equivalent to 1 bit-per-Weight.
Table 2: GoogleNet training performance for ImageNet1k: DKM-based compression offered 2x better com-pression ratio With 3% higher top-1 accuracy than RPS (Wu et al., 2018).
Table 3: Training performance for QNLI: DKM-based scheme outperforms EWGS in compressing varioustransformed-based architectures. Also, multi-dimensional DKM (see Section 3.3) largely improved the accu-racy of MobileBERT with 1 bit-per-weight target using the 4/4 configuration.
Table 4: DistillBert training performance for MNLI: DKM-based compression offered 10% smaller model sizewith the same accuracy target than GOBO.
Table 5: Top-1 Accuracy with Train-time and Inference-time Weights for Fig. 4(a) Top-1 accuracy(b) Weight distribution of the largest conv layerFigure 5:	MobileNet-v2 convergence with DKM 1/1: DKM delivers 50.8% top-1 accuracy with 1 bit compres-sion by gradually clustering the weights into two centroids using the task objective only.
Table 6: Memory and Runtime overheads from DKM on ResNet18/50.
Table 7: τ for the DKM experiments in Table 1 in Section 4It could be possible to cast τ as a learnable parameter for each layer or apply some scheduling toimprove the model accuracy further (as a future work), but still both approaches need a good initialpoint which can be found using a binary search technique.
