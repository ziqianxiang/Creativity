Table 1: Denotation accuracies	on WikiSQL-		BART TAPEX	37.2 57.0	38.0 57.5Table 2: Denotation accuracies on Wik-iTableQuestions.
Table 2: Denotation accuracies on Wik-iTableQuestions.
Table 3: Denotation accuracies on SQA test set. ALL is the denotation accuracy over all sentences,SEQ the denotation accuracy over all conversations, and Qi the denotation accuracy of the i-thsentence in a conversation.
Table 4: Accuracies on TabFact, including the Human Performance.
Table 5: The most common operators in the randomly selected 500 questions from WIKITABLE-Questions dev set. Listed are, the operator, the example question with the operator semantic (i.e.,the colorful spans), the performance of BART and TAPEX on the operator.
Table 6: Experimental dataset statistics.
Table 7: The example inputs and outputs for our model on experimental datasets.
Table 8: Experimental results (denotation accuracy) of multi-task fine-tuning on the Target devset. Source 7â†’ Target means first fine-tuning on Source and then fine-tuning on Target.
Table 9: Four SQL query difficulty levels and their corresponding example SQL queries.
Table 10: The sampled SQL queries, their corresponding NL sentences translated by our SQL-to-NL model, and the faithfulness of the NL sentences.
Table 11: The downstream performance on dev sets of TaPEx with the SQL and the NL pre-trainingcorpus. The NL corpus is obtained via translating the SQL corpus using our SQL-to-NL model, andthey share the same amount of examples (0.5 Million).
