Table 1: Comparison of online CL methods on the i-Blurry setup for CIFAR10, CIFAR100, Tiny-ImageNet and ImageNet. â€™Joint Training^ ' shows the final accuracy of non-CL joint training as asoft upper bound where all relevant hyper-parameters were kept consistent with other compared CLmethods. CLIB outperforms all other CL methods by large margins on both the AAUC and the Aavg .
Table 2: Analysis on various values of N (top) and M (bottom) in the i-Blurry-N -M setup usingCIFAR10 dataset. For varying N, we use M = 10. For varying M, we use N = 50. Note thatN = 0 corresponds to the blurry split and N = 100 corresponds to the disjoint split. For N = 100,CLIB outperforms or performs on par with other CL methods. For N = 0, the gap between CLIBand other methods widens. For N = 50, CLIB again outperforms all comparisons by large margins.
Table 3: Ablations for proposed components of our method using CIFAR10 and CIFAR100 dataset.
Table 4: Analysis on various sample memory sizes (K) using CIFAR10. The i-Blurry-50-10 splitsare used. The results are averaged over 3 runs. CLIB outperforms all other CL methods by largemargins for all the memory sizes. CLIB uses the given memory budget most effectively, showingthe superiority of our per-sample memory management method.
Table 5: Analysis on the number of tasks on CIFAR100. The i-Blurry-50-10 splits are used. Theresults are averaged over 3 runs. CLIB outperforms all other CL methods by large margins, evenwhen the task sequences becomes longer. Additionally, CLIB does not suffer severe performancedrops as the number of tasks increase, indicating that CLIB is well-suited for long-run CL problemsetups as well.
Table 6: Additional comparisons to A-GEM with various online CL methods on the i-Blurry setupfor CIFAR10 and CIFAR100 are shown. The i-Blurry-50-10 splits are used for all the datasets andthe results are averaged over 3 runs. A-GEM performs very poorly, especially on CIFAR100 as itwas designed for the task-incremental setting whereas i-Blurry setup is task-free. CLIB outperformsall other CL methods by large margins on both the AAUC and the Aavg .
Table 7: Comparisons to other CL methods with different memory management strategies in thei-Blurry setup for CIFAR10 and CIFAR100 are shown. The i-Blurry-50-10 splits are used for allthe datasets and the results are averaged over 3 runs. CLIB outperforms all other CL methods bylarge margins on both the AAUC and the Aavg implying that the sample-wise importance memorymanagement method is effective.
Table 8: Additional comparisons including the Flast measure of various online CL methods on thei-Blurry setup for CIFAR10, CIFAR100, TinyImageNet and ImageNet are shown. The i-Blurry-50-10 splits are used for all the datasets and the results are averaged over 3 runs except ImageNet. Oursoutperforms all other CL methods by large margins on both the AAUC and the Aavg . While CLIBdoes not explicitly handle forgetting, it shows similar Flast with most methods. The best result foreach of the metrics is shown in bold.
Table 9: Comparison between exponential with reset schedule and constant LR on CIFAR10 areshown. It shows that our baseline LR schedule, exponential with reset, is reasonable. It shows betterperformance than constant LR, especially in Aavg metric.
