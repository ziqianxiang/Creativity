Table 1: ResUlts of ResNet-20/32 on CIFAR10 Table 2: Results of ResNet-18 onSetting	ResNet-20 ResNet-32 CIFAR100.
Table 3: Summary of Algorithms Discussed in Section 2Algorithm	mk	Dk		SGD (Robbins &Monro,1951)	Bimt—i + (I -	-βi)gk	1Adagrad (Duchi et al., 2011)	gk		yzPk=ι diag(gi θ gi)RMSProp (Tieleman & Hinton, 2012)	gk		ʌ/ lβ2D k—12 + (1 - β2)diag(gk θ gk )Adam (Kingma & Ba, 2014)	(i-βi)P3 1-βk	βk-igi	,S(1 - β2) P3 βk-idiag(gi Θ gi) V	1 - βkAdaHessian (Yao et al., 2020)	(i-βι) Pk=i 1 - β1	βk-igi	S (i-β2 )Pk=ι βk-Vr V	1- βk	OASIS	gk		∣β2Dk-1 + (1 - β2 )vk Ia* Vi = diag(zi Θ V2F(Wi)Zi) and	zi 〜RademaCher (0.5) ∀i		≥ 1,**(∣A∣α)ii = max{∣A∣ii,α}***Dk = β2Dk-i + (1 - β2)vkAlgorithm 3 Stochastic OASISInput: w0, η0, Ik, D0, θ0 = +∞, β2, α1:	w1 = w0 - no DD 0-1VF (wo)ik2:	for k = 1, 2, . . . do3:	Calculate Dk = β2Dk-1 + (1 - β2) diag(zk	V2FJk (wk)zk)4:	Calculate Dk by setting (Dk)i,i = max{|Dk |i,i, α}, ∀i ∈ [d]5:	Update nk = min{ P1 + θk-1nk-1, 2kVFik (wk)-VFik (Dkk-ι)k^^ }6：	Set Wk + 1 = Wk - nkDkTVFIk (Wk)7:	Set θk = nk-
Table 5: Deep Neural Networks used in the experiments.
Table 4: Description of implemented algorithmsTo display the optimality gap for logistic regression problems, we used Trust Region (TR) New-ton Conjugate Gradient (Newton-CG) method (Nocedal & Wright, 2006) to find a w such that∣∣VF(w)k2 < 10-19. Hereafter, we denote F(W) - F(w*) the optimality gap, where we refer w* tothe solution found by TR Newton-CG.
Table 6: SummaryofDatasets.
Table 7: Results of ResNet20/32 on CIFAR10 with and without weight decay. Variant of our methodwith fixed learning rate beats or is on par with others in the weight decay setting; while OASIS withadaptive learning rate produces consistent results showing a close second best performance withoutany learning rate tuning (for without weight decay setting).
Table 8: Results of ResNet18 on CIFAR100. Simply transferring parameter values from similartask with ResNet20 on CIFAR10 predictably damages performance of optimizers compared to theirheavily tuned versions. Notably, in the setting without weight decay performance of SGD and Adambecame unstable for different initializations, while adaptive variant of OASIS produces behaviourrobust to the choice of the random seed.___________________________________________________Setting	ReSNet18, WD	ResNet18, no WDSGD	76.57 ± 0.24	70.50 ± 1.51Adam		73.40 ± 0.31	67.40 ± 0.91AdamW	72.51 ±' 0.76	67.96 ± 0.69AdaHessian	75.71 ±' 0.47	70.16 ± 0.82OASIS-Adaptive LR	76.93 ± 0.22	74.13 ± 0.20OASIS-Fixed LR	76.28 ± 0.21	70.18 ± 0.76OASIS-Momentum	76.89 ± 0.34	70.93 ± 0.77WD := Weight decay43Published as a conference paper at ICLR 202295908580
Table 9: Results of Net DNN on MNIST. Gradient momentum usage seems improve results on shorttrajectories for all methods, including OASIS.
