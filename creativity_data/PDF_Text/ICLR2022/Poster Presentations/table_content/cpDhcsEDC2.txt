Table 1: Top-1 accuracy(%) of zero-shot image classification on 12 datasets. Our FILIP can boost3~5% accuracy on average.
Table 2: Results of zero-shot image-text retrieval on Flickr30K and MSCOCO datasets. The lasttwo rows (marked with *) report the zero-shot results on Flickr30K dataset of model fine-tuned onMSCOCO dataset, following the setting of ALBEF (Li et al., 2021a).
Table 3: Results of fine-tuned image-text retrieval on Flickr30K and MSCOCO datasets.
Table 4: Ablation study of different components on pre-training subset of YFCC100M. I2T and T2Iare abbreviations for image-to-text and text-to-image retrieval, respectively. “ZS” means zero-shotperformance. Underlined numbers have the highest improvements for the corresponding metrics.
Table 5: Efficiency study of the cross-modal late interaction. “orig” and “late” stand for the con-trastive loss based on the original cosine similarity in CLIP and our proposed cross-modal lateinteraction, respectively. “ZS” means zero-shot performance. We report results for ViT-B/32 trainedon filtered YFCC100M with 8 V100 GPUs, with a batch size of 512 per GPU. Training time andmemory consumption are tested using the same gradient checkpoint configuration. * denotes ourfinal setting used in other experiments.
Table 6: Number of image-text pairs used in the pre-training of FILIP, CLIP and ALIGN.
Table 7: The architecture parameters for FILIP models.
Table 8: Common hyperparameters used for FILIP pre-training.
Table 9: Model- and dataset-specific hyperparameters used for FILIP pre-training. Numbers in batchsize represent the total batch size across all workers and are calculated as: batch size per GPU ×#GPUs. FILIP340M is the combination of FILIP300M, YFCC100M, CC12M and CC3M.
Table 10: Hyperparameters used for image-text retrieval fine-tuning.
Table 11: Prompt templates used for 12 downstream image classification tasks.			Dataset	Prefix	Category description	SuffixCIFAR10	“a photo of a”, “a jpeg photo of a”, “a painting of a”, “itap of a”, “graffiti of a”, “a cartoon”, “a doodle”	None	None， “It’s common in daily life”， “It’s cute”， “It’s ugly”， “It’s weird”， “Hope you like it”CIFAR100	“a jpeg photo of a”, “a painting of a”, “a good photo of a”, “a bad photo of a”, “a photo of a”, “itap of a”, “a rendering of a”	None	None， “It’s common in daily life”， “It’s beautiful”， “It’s ugly”， “I like it”， “I take it to- day”Caltech101	“a photo of a”, “a cropped photo of a”, “a good photo of a”, “a bad photo of a”	None	None, “I like it"，“I hate it”， “It’s ugly”， “It’s cute”Stanford- Car	“a photo of a”, “a close-up photo of a”, “a good photo of a”, “a bad photo of a”	“a type of car”, “a type of auto- mobile”	“I like it”， “It belongs to my friend”， “It’s brand new”， “It’s popular recently”， “It’s impor- tant to me”， “I take it today”Flowers102	“a photo of a (many) ”, “a ren- dering of a (many) ”, “itap of a (many) ”	“a type of flower”, “a type of bloom”	“It’s beautiful”， “It’s from my best friend”， “It gives out a sweet perfume/fragrance”ImageNet	“a photo of a”, ”a good photo of a”, “a bad photo of a”, “a close- up photo of a”, “itap of a”	None	“I like it”， “It’s common in daily life”， “It’s not common in daily life”， “It’s ugly”， “It’s cute”， “It’s beautiful”Food101	“a photo of my”, “a close-up photo of my”, “itap of my”	“a type of food”, “a type of nourish- ment”	“I made it today”， “I like it”， “I hate it”， “It’s delicious”， “It’s with nice flavour”， “It’s with terrible flavour”， “It’s popular recently”SUN397	“a photo of a”, “a good photo of a”, “a bad photo of a”, “a bright photo of a”, a dark photo of a”, “a black and white photo of a”, “a nice scene of a”, “a terrible scene of a”	None	None， “I like it”， “I hate it”， “It’s beautiful”， “It’s common in daily life”， “It’s important to me”DTD	“itap of a”, “a close-up photo of a”	“texture”， “surface”, “material”	None， “It’s out of style”， “It’s popular in old days”， “It’s ugly”， “It’s beautiful”Aircrafts	“a photo of the”, “a close-up photo of the”, “a good photo of the ”, “a pixelated photo of the”	“a type of plane”, “a type of aircraft”, “a type of airliner”	None，“I like it”， “It’s important to me”， “I take it today”， “Hope you like it”Oxford Pet	“a photo of my”, “a low reso- lution photo of my”, “a good photo of my”	“a type of pet”， “a type of dog or cat”	None， “It’s cute”， “It’s impor- tant to me”， “I like it”， “It’s beautiful”EuroSAT	“a photo of a”, “a painting of a”, “a cropped photo of a”, “a good photo of a”, “a blurry photo of a”	None， “an ex- ample of aerial or satellite im- ages”	None， “I like it”， “It’s taken from an aircraft or some flying object”， “It’s collected by imag- ing satellites”17Published as a conference paper at ICLR 2022Table 12: Prompt templates used for zero-shot image-text retrieval on Flickr30K and MSCOCOdatasets. __________________________________________________________________________Dataset	Task	Prefix	SuffixFlickr30K	image-to-text retrieval	“a good photo of the”	“I hate it.”
Table 12: Prompt templates used for zero-shot image-text retrieval on Flickr30K and MSCOCOdatasets. __________________________________________________________________________Dataset	Task	Prefix	SuffixFlickr30K	image-to-text retrieval	“a good photo of the”	“I hate it.”	text-to-image retrieval	“a good photo of”	NoneMSCOCO	image-to-text retrieval	“a good photo of”	“It is ugly.”	text-to-image retrieval	None	NoneImage-text Retrieval. Following CLIP (Radford et al., 2021), we use prompt in zero-shot image-text retrieval for both Flickr30K and MSCOCO datasets. The prompt is selected by the same rule asdescribed in Section 3.1.2, except that we do not use “[category description]” here. Table 12 showsthe prompt templates for zero-shot image-text retrieval on Flickr30K and MSCOCO datasets.
Table 13: Hyperparameters used for linear probe image classification on ImageNet.
Table 14: Top-1 accuracy(%) of linear probe on image classification on 12 datasets. Our FILIPoutperforms CLIP by 1.2~1.8% points on average.
Table 15: Comparison of Top-1 Accuracy(%) between the proposed cross-modal late interactionloss and Khattab & Zaharia (2020) on zero-shot ImageNet classification.
Table 16: Top-1 accuracy(%) on image classification on 12 datasets. CLIPrep is our reproducedCLIP trained with the same training data and evaluated with the same prompts as our FILIP. With thesame backbone architecture, our FILIP significantly improves the zero-shot Top-1 average accuracyover 12 datasets.
Table 17: Comparison on performance and inference time of image-text retrieval on Flickr30K andMSCOCO datasets.
