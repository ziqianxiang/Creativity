Table 1: Expressivity comparison on the non-conjugate (NC) and Gaussian mixture (GM) examples.
Table 2: Scaling comparison on the Gaussian random effects example (see Fig. 2b-GRE). Methodsare ran over 20 random seeds (Except for SNPE-C and TLSF: to limit computational resources us-age, those non-amortized computationally intensive methods were only ran on 5 seeds per sample,for a number of effective runs of 100). Are compared: from left to right ELBO median (higheris better) and standard deviation (ELBO for all techniques except for Cascading Flows, for whichELBO is the numerically comparable augmented ELBO (Ranganath et al., 2016)); number of train-able parameters (weights) in the model; for non-amortized techniques: CPU inference time for oneexample (seconds); for amortized techniques: CPU amortization time (seconds). 1- Results forNPE-C are extremely unstable, with multiple NaN results: the median value is rather random andnot necessarily indicative of a good performanceC.3 Derivation of an analytic posteriorTo have a ground truth to which we can compare our methods results, we derive the followinganalytic posterior distributions. Assuming we know σ*, σg,。方：1Nμ = N X Xnn=12μg∣μg 〜N(μg,NIdD1Gμ = G∑μg
Table 3: Convergence of the variational posterior to the analytical posterior over an early stoppedtraining (200 batches) and after convergence (1000 batches) for the Gaussian random effects exam-pleThis formula differs from the one of the reverse KL loss by the absence of the term qψ(θm∣Xm),and is a converse formula to the one of the forward KL (in the sense that it permutes the roles of qand p).
