Table 1: IS, FVD, and KVD values of video generation models on (a) UCF-101, (b) Sky, (c) TaiChi,and (d) Kinetics-food datasets. ↑ and ] imply higher and lower values are better, respectively. SUb-scripts denote standard deviations, and bolds indicate the best results. “Train split” and “Train+testsplit” denote whether the model is trained with the train split (following the setup in Saito et al.
Table 2:	FVD values of generated videos inter- and extra-polated over time. All models are trainedon 16 frame videos of 128×128 resolution. The videos are interpolated to 64 frames (i.e., 4× finer)and extrapolated 16 more frames. We measure FVD with 512 samples for Sky, since the test datasize becomes less than 2,048.
Table 3:	Time (sec) for generating a 128×128video for VideoGPT, MoCoGAN-HD, andDIGAN. Bolds indicate the best results.
Table 4: FVD values of videos upsam-pled from 128×128 to 256×256 reso-lution (2× larger) on TaiChi dataset.
Table 5: Ablation study of the generator compo-nents: smaller frequency σt, motion vector zM ,and non-linearity by MLP fM(∙).
Table 6:	SSIM of intermediate scenes predicted by StyleGAN2 and our method, DIGAN.
Table 7:	Effect of the motion vector zI for generating the motion. We report the mean and standarddeviation of the FVD values over 10 runs.
Table 8: IS, FVD, and KVD values of video generation models on the UCF-101 dataset. ↑ and ]imply higher and lower values are better, respectively. Subscripts denote standard deviations, andbolds indicate the best results.
