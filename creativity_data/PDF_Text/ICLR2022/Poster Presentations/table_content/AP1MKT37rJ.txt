Table 1: Offline CQL vs. BC with expert dataset compositions averaged over 3 seeds. While naive offline CQLoften performs comparable or worse than BC, the performance of offline RL improves drastically after offlinetuning. Also note that offline RL can improve when provided with diverse initial states in the Antmaze domain.
Table 2: CQL with noisy-expert data vs BC with expertdata with equal dataset size on manipulation tasks. CQLoutperforms BC as well as CQL with only expert data.
Table 3: Per-game results for the Atari domains with expert data. Note that while naive CQL doesnot perform much better than BC (it performs similarly as BC), tuned CQL with the addition of theDR3 regularizer performs much better.
Table 4: Comparing the performance of BC-PI and offline RL on noisy-expert data. Observe that in general,offline RL significantly outperforms BC-PI.
