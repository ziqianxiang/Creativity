Table 1: Resource efficiency of considered SSLpre-training methods. We take the distant classincremental sequence as an example and reportthe training time (h) and required storage (GB)of the model pre-trained with each data chunk.
Table 2: The comparison of pre-training meth-			ods in terms of the transfer performance gap be-			tween ST and JT models.	We report the aver-		aged accuracy gaps of linear evaluation across			12 downstream datasets. The lower, the better.			Accuracy gap (%) / Chunk	2	3	4SL-ST (Instance)	2.26	3.27	4.83SSL-ST (Instance)	0.41	1.02	1.04SL-ST (Random)	5.63	8.73	10.68SSL-ST (Random)	0.42	0.94	1.13SL-ST (Distant)	7.77	12.50	15.75SSL-ST (Distant)	2.34	3.81	4.62SSL-ST w/MAS (Distant)	1.82	2.73	3.17SSL-ST w/MAS+ (Distant)	1.47	2.01	2.10Summary. We show the averaged accuracy gaps between ST models and the corresponding JTmodels under linear evaluation in Table 2, for both SSL and supervised learning (SL). On streamingdata with negligible distribution shifts, SL exhibits evident accuracy gaps while SSL has negligiblegaps. On streaming data with moderate distribution shifts, SL exhibits larger accuracy gaps whileSSL still keeps the negligible gaps. On streaming data with severe distribution shifts, SL shows muchlarger accuracy gaps, while SSL shows mild accuracy gaps. But such accuracy gaps of SSL can be
Table 3: Backward and forward transfer analy-sis of sequential learning.
Table 4: Comparisons of the sharpness of minima between SL and SSL models. Lower is better.
Table 5: The inverse of regularization strength (weight decay value) used in many-shot logistic re-gression evaluation on 12 different downstream classification datasets. SSL models: self-supervisedmodels. SL models: supervised models.
