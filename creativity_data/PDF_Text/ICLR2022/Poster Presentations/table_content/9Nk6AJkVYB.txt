Table 1: Performance of three backbones at the extreme sparsity or at the best performance on LibriSpeechtest-clean subset. The performance on test-other subset has a similar trend (see Appendix A.3). #Paramsfull :number of parameters in full model, in which we use Mega (×106) as the unit; WERfull : WER of full models;WERext : WER of the winning tickets at extreme sparsity; WERbest : WER of the best performing winningtickets. Remaining Weight (RW) is included as model complexity measurement.
Table 2: Performance of CNN-LSTM backbone (86.62M parameters) at the extreme sparsity or at the bestperformance on TED-LIUM, CommonVoice, and LibriSpeech datasets.
Table 3: Comparison to state-of-the-art pruningand distillation methods on Conformer backbone.
Table 4: Results of initialization with pre-trained mod-els. WERfull: WER of full models; WERbest: WER ofthe best performing matching subnetworks; WER4.4% :WER of subnetworks with 4.4% remaining weights. Thebrackets shows the remaining weights of the best per-forming subnetworks.
Table 5: Results of structured sparsity study on TED-LIUM dataset.
Table 6: Results of noise robustness study on TED-LIUM dataset. The noise level is drawn from a uniformdistribution from [0, Nmax], and we evaluate three noiseconditions: Nmax = 0 (no noise), Nmax = 0.2 (lownoise), Nmax = 0.5 (high noise).
Table 7: Performance of three backbones at the extreme sparsity or at the best performance on LibriSpeechtest-other subset. #Paramsfull: number of parameters in full model, in which we use Mega (×106) as the unit;WERfull : WER of full models; WERext : WER of the winning tickets at extreme sparsity; WERbest : WER ofthe best performing winning tickets. Remaining Weight (RW) is included as model complexity measurement.
Table 8: Comparison to state-of-the-art distillation and pruning methods on Conformer backbone. Models areevaluated on LibriSpeech test-other subset.
Table 9: Performance of transferring CommonVoice and LibriSpeech winning tickets to TED-LIUMdataset at the extreme sparsity. Remaining Weight (RW) is included as the spatial and temporalcomplexity measurements.
Table 10: Performance of transferring TED-LIUM and LibriSpeech winning tickets to CommonVoicedataset at the extreme sparsity. Remaining Weight (RW) is included as the spatial and temporalcomplexity measurements.
Table 11: Performance of transferring TED-LIUM and CommonVoice winning tickets to LibriSpeechdataset at the extreme sparsity. Remaining Weight (RW) is included as the spatial and temporalcomplexity measurements.
Table 12: Performance of three backbones at the extreme sparsity or at the best performance on TED-LIUM.
Table 13: Performance of three backbones at the extreme sparsity or at the best performance on CommonVoice.
Table 14: Run time evaluation of the three backbones on LibriSpeech at the extreme sparsity or at the bestperformance. Here we use the Number of Multiply-Accumulate Operations (MACS) in Giga (G) to measure therun time complexity. We compute the percentage compared to full model for all the subnetworks. MACsfull :MACs of thefull model. MACsext: MACs of the winning tickets at extreme sparsity. M AC sbest : MACs ofthe best performing winning tickets.
Table 15: Run time evaluation of the three backbones on TEDLIUM at the extreme sparsity or at the bestperformance. Here We use the Number of MUltiPly-AccUmUlate Operations (MACs) in Giga (G) to measure therun time complexity. We compute the percentage compared to full model for all the subnetworks. MACsfull :MACs of thefull model. MACsext: MACs of the Winning tickets at extreme sparsity. M AC sbest : MACs ofthe best performing winning tickets.
Table 16: Run time evaluation of the three backbones on CommonVoice at the extreme sparsity or at the bestperformance. Here we use the Number of Multiply-Accumulate Operations (MACs) in Giga (G) to measure therun time complexity. We compute the percentage compared to full model for all the subnetworks. MACsfull :MACs of thefull model. MACsext: MACs of the winning tickets at extreme sparsity. M AC sbest : MACs ofthe best performing winning tickets.
Table 17: Performance of ASR models when adding noise only at test time. Results are shown at the bestperformance.
Table 18: Results of structured sparsity study on TED-LIUM dataset. We also show the results with unstructuredsparsity as a reference.
