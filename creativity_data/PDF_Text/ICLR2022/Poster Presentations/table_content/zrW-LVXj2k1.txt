Table 1: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two time-series datasets. The confidenceintervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, weoutput the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.
Table 2: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two regression datasets. The confidenceintervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, weoutput the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.
Table 3: The MLE model predicts the empirical quantile of inter-est during inference. It is compared with Quantile regression (TMObased). The results, averaged over 50 runs along with the correspond-ing confidence intervals are presented.
Table 4: We perform an ablation study on the Favorita dataset,where we progressively add the components of our mixture distribu-tion. There are three MLE models in the progression: Negative Bino-mial (NB), Zero-Inflated Negative Binomial (ZNB) and finally ZNBP.
Table 5: Note that here d refers to the dimension of the last layer of the architecture used in therespective datasets.
