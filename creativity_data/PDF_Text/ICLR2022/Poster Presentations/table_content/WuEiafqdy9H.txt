Table 1: Average of cumulative rewards under Rainbow on Atari Environments after 2M training steps by10 evaluations across five runs with random seeds. One can observe that Rainbow with MaPER outperformsRainbow without MaPER on most tasks. Here, we used the same hyperparameters of Rainbow as (van Hasseltet al., 2019; Lee et al., 2020a) except for the total timesteps (0.1M → 2M) to observe the much longer timebehavior (see Figure C.1 in the supplementary material for learning curves).
Table 2: Average of cumulative rewards under SAC on sparse reward environments by 10 evaluations after 2Mtraining steps across five runs with random seeds. Combining MaPER (our method) and reward shaping methodresults in the best performances.
Table A.1: Dimensions of observation and action spaces for continuous control environments.
Table B.1:	Parameters for Model-free Reinforcement Learning experiments.
Table B.2:	Parameters for MBPO (Janner et al., 2019) experiments.
Table B.3: Parameters for the reward shaping (Pathak et al., 2017; Stadie et al., 2015) experiments.
Table C.1: Average of cumulative rewards under SAC on sparse reward by 10 evaluations after 2M trainingsteps across five runs with random seeds. The values in parentheses denote standard deviations. MaPER + RealReward means that the TD-error equation (see Eq. (4)) uses the real rewards instead of model rewards.
Table C.2: The training wall-clock time between MaPER and PER under SAC. We computed the elapsed timefor train networks from sampling across five runs with random seeds.
Table C.3: HoliStic PerformanceS for each domain. Here, in each domain, we normalized each method’SPerformance baSed on MaPER’S. One can obServe that MaPER conSiderably oUtPerformS other methodS.
