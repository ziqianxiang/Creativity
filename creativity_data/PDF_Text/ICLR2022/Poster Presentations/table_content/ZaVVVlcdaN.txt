Table 1: Rates for the strongly convex case. * Rate requires R ≥ N/S. ♦ Rate requires S = N.
Table 2: Rates for the general convex case. * Rate requires R ≥ S. ♦ Rate requires S = N. ♦ Analysis from Karimireddy et al. (2020b).		Method/Analysis	EF(X) - F(x*) ≤ O(∙)	Centralized Algorithms		SGD	警+ /	-^S ZD 	 	 N √SRASG	βD~+q	-^S ZD 	 	 N √SR	Federated Algorithms		FedAvg	βD2 + 3∕βζ2D4 + ʌ ∕1 - S SD R + V R2 + V 1 N √SR	FedAvg (Woodworth et al., 2020a)	√βζRD4	SCAFFOLD	q喑*	This paper		FedAvg → SGD (Thm. 4.1)	min{ βD2,：	√βZD3 ɔ , 4 A~~S√βZD3 ~√R~}+ V1 - N ~^SR~FedAvg → ASG (Thm. 4.2)	min{βD22, √	^ζD3} + q /1—ɪ √ζD3 + q∕1-ɪ 立 R } + V 1 N	√SR + V 1 N √SRAlgo.-independent LB (Thm. 5.4)	min{ βD2 ,二	ZD- } √R5 }	For the formal statement, see Thm. F.1.
Table 3: Test accuracies (↑). “Constant” means the stepsize does not change during optimization,while “w/ Decay” means the stepsize decays during optimization. Left: Comparison among algo-rithms in the EMNIST task. Right: Comparison among algorithms in the CIFAR-100 task. “SCA.”abbreviates SCAFFOLD.__________________________ _______________________________________________Algorithm	Constant	w/ Decay	Algorithm	Constant	w/ DecayBaselines			Baselines		SGD	-^0.7842	0.7998	SGD	-^03987^^	0.1968FedAvg	0.8314	0.8224	FedAvg	0.4944	0.5059SCAFFOLD	0.8157	0.8174	SCAFFOLD	一	—FedChain			FedChain		FedAvg → SGD	^^0.8501^^	0.8355	FedAvg → SGD	-^0.5134	0.5167SCA. → SGD	0.8508	0.8392	SCA. → SGD	一	—(which includes most of the algorithms we are interested in), Thm. 5.4 allows us, for the first time, toidentify the optimality of the achievable rates as shown in Tables 1, 2, and 4.
Table 4: Rates under the PL condition. * Rate requires R ≥ S.
