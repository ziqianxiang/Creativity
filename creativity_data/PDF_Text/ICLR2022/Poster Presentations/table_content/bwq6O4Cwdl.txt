Table 1: Influence of Explicit EOA. Detailed setup is reported in Appendix A.3Explicit EOA does not prevent collapse. (Chen & He, 2021) points out that “in practice, it wouldbe unrealistic to actually compute the expectation ET[∙]. But it may be possible for a neural net-work (e.g., the preditor h) to learn to predict the expectation, while the sampling of T is implicitlydistributed across multiple epochs.” If implicitly sampling across multiple epochs is beneficial, ex-plicitly sampling sufficient large N augmentations in a batch with the latest model would be morebeneficial to approximate ET[∙]. However, Table 1 shows that the collapse still occurs and suggeststhat the equivalence between predictor and EOA does not hold.
Table 2: Results of various Siamese archi-tectures. Detailed trend and setup are re-ported in Appendix A.4suggests that the collapse can be avoided by processing the optimization target with h-1. By con-trast, Fig. 1 (c) and Fig. 2 (a) both lead to collapse, suggesting that processing the optimizationtarget with h is not beneficial for preventing collapse. Overall, asymmetry alone does not guaranteecollapse avoidance, which requires the optimization target to be processed by h-1 not h.
Table 3: Gradient component analysis with a random negative sample.
Table 5: Linear evaluation on CIFAR100.
Table 6: Similarity between center vectorand (1) single bias layer (bp), (2) the lastbias layer of MLP in the predictor.
