Table 1: Comparisons on MuJoCo domains. Averaged (over 5 runs) returns for Loaded+GAE version ofMDPO, TRPO, PPO, and SAC algorithms, together with their 95% confidence intervals. On-policy results arefor 10M timesteps. The values with the best mean scores are bold-faced.
Table 2: Hyper-parameters of all on-policy methods.
Table 3: Hyper-parameters of all off-policy methods.
Table 4: Bregman stepsize for each domain, used by off-policy MDPO.
Table 5: Performance of MDPO-M, compared against PPO-M, TRPO-M on six MuJoCo tasks. The results areaveraged over 5 runs, together with their 95% confidence intervals. The values with the best mean scores arebolded.
Table 6: Performance of MDPO-LOADED+GAE, compared against loaded implementations (including GAE) ofPPO and TRPO (PPO-LOADED+GAE, TRPO-LOADED+GAE) on six MuJoCo tasks. The results are averagedover 5 runs, together with their 95% confidence intervals. The values with the best mean scores are bolded.
Table 7: Performance of KL and Tsallis based versions of MDPO-M, compared with SAC-M on six MuJoCotasks. The results are averaged over 5 runs, together with their 95% confidence intervals. The values with thebest mean scores are bolded.
Table 8: Performance of KL and Tsallis based versions of MDPO-LOADED, compared with SAC-LOADEDon six MuJoCo tasks. The results are averaged over 5 runs, together with their 95% confidence intervals. Thevalues with the best mean scores are bolded.
Table 9: Different Tsallis Entropies. The results are averaged over 5 runs, with 95% confidence intervals shaded.
