Table 1: Comparison of train and test accuracies in supervised learning(a) CoinrunFigure 3: Example of levels heterogeneity on two Procgen environments(b) Leapereralization performance compared to the other methods, thus confirming the intuition that localizedpermutations at the feature level during training may improve generalization at testing. Appendix Dfeatures an extended discussion on the application of CLOP to other datasets (including Imagenet)and a comparison with mixup (Zhang et al., 2018c).
Table 2: Average returns on Procgen games. Bold: best agent; underlined: second best.
Table 3: Performance on training and testing levels, and generalization gap after 25M steps.
Table 5: Experimental setupFigure 9: CLOP layer within the IMPALA architectureGame	alphaBigfiSh- StarPilot FruitBot BossFight Ninja Plunder CaveFlyer CoinRun Jumper Chaser Climber Dodgeball Heist Leaper Maze Miner	0.6 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.8 0.3 0.6 0.3 0.8 0.3 0.3Table 4: Values of αB	Reproducib ilityAll the experiments from Section 5 were run on a desktop machine (Intel i9, 10th generation pro-cessor, 32GB RAM) with a single NVIDIA RTX 3080 GPU. Details about each experiments arereported in Table 5.
Table 4: Values of αB	Reproducib ilityAll the experiments from Section 5 were run on a desktop machine (Intel i9, 10th generation pro-cessor, 32GB RAM) with a single NVIDIA RTX 3080 GPU. Details about each experiments arereported in Table 5.
Table 6: Computation time with and without clop on two Progen games	MNIST	ImagenetteNo augmentation	6.6s	-416sRandom crop	16.8s	-417sRotation	18.1s	54.1sCutout	6.9s	41.7sRandom convolutions	N/A	42.3sCLOP	6.6s	41.7sTable 7: Computation time of one forward pass on the entire datasetD Additional experiments in supervised learningFollowing the experiments of Section 5.1, we evaluated the benefit of the CLOP layer when traininga VGG16 network on the full Imagenet database over 90 epochs, and compared with the reportedimprovement brought by mixup (Zhang et al., 2018c) in the same setting. Mixup improves thegeneralization performance by 0.2 to 0.6% across network architectures (generalization accuracieswithout mixup between 76 and 80% depending on the network used). Similarly, CLOP brings anegligible 0.15% improvement (71.6% top-1 generalization accuracy without CLOP, versus 71.75%with CLOP). We conjecture the sample variety and diversity in Imagenet actually cancels the needfor data augmentation altogether. To confirm this conjecture and refine the understanding of whenCLOP is beneficial or not, we ran the same experiment on several datasets with less variety in theimages and less data. STL-10 (Coates et al., 2011) is a subset of Imagenet inpired by CIFAR-10
Table 7: Computation time of one forward pass on the entire datasetD Additional experiments in supervised learningFollowing the experiments of Section 5.1, we evaluated the benefit of the CLOP layer when traininga VGG16 network on the full Imagenet database over 90 epochs, and compared with the reportedimprovement brought by mixup (Zhang et al., 2018c) in the same setting. Mixup improves thegeneralization performance by 0.2 to 0.6% across network architectures (generalization accuracieswithout mixup between 76 and 80% depending on the network used). Similarly, CLOP brings anegligible 0.15% improvement (71.6% top-1 generalization accuracy without CLOP, versus 71.75%with CLOP). We conjecture the sample variety and diversity in Imagenet actually cancels the needfor data augmentation altogether. To confirm this conjecture and refine the understanding of whenCLOP is beneficial or not, we ran the same experiment on several datasets with less variety in theimages and less data. STL-10 (Coates et al., 2011) is a subset of Imagenet inpired by CIFAR-10(Krizhevsky, 2009), where each class has even fewer labeled training examples than in CIFAR-10 (500 training images, 800 test images per class). Imagewoof is the subset of 10 breed classesfrom ImageNet. Training on Imagewoof or on Imagenette does not benefit from the diversity ofimage features present in the whole Imagenet dataset. In these cases, CLOP was the method thatcontributed most to close the generalization gap, as shown in Table 8. These results, along with theones on the Imagenette dataset indicate that the CLOP layer becomes really beneficial when data isscarce and one needs to avoid overfitting.
Table 8: Training and testing accuracy (in %) on various scarce-data supervised learning tasks16Published as a conference paper at ICLR 2022Game	IDAAC	CLOP	IDAAC + CLOPBigfish-	18.5 ± 1.2	19.2 ± 4.6	21.7 ± 2.1BossFight	9.8 ± 0.6	9.7 ± 0.1	9.5 ± 0.7CaveFlyer	5.0 ± 0.6	5.0 ± 0.3	5.0 ± 0.4Chaser	6.8 ± 1.0	8.7 ± 0.2	6.0 ± 0.5Climber	8.3 ± 0.4	7.4 ± 0.3	8.3 ± 0.2CoinRUn	9.4 ± 0.1	9.1 ± 0.1	9.2 ± 0.5Dodgeball	3.2 ± 0.3	7.2 ± 1.2	3.4 ± 0.4FruitBot	27.9 ± 0.5	29.8 ± 0.3	26.8 ± 1.8Heist	3.5 ± 0.2	4.5 ± 0.2	3.6 ± 0.8Jumper	6.3 ± 0.2	5.6 ± 0.2	5.8 ± 0.2LeaPer	7.7 ± 1.0	9.2 ± 0.2	6.5 ± 2.4Maze	5.6 ± 0.3	5.9 ± 0.2	5.6 ± 0.2Miner	9.5 ± 0.4	9.8 ± 0.3	9.9 ± 0.1Ninja	6.8 ± 0.4	5.8 ± 0.4	6.4 ± 0.5Plunder	23.3 ± 1.4	5.4 ± 0.7	20.3 ± 3.2StarPilot	37.0 ± 2.3	40.9 ± 1.7	36.3 ± 3.5
Table 9: IDAAC+CLOP on testing levelsOverall, the combination neither seems beneficial or detrimental and the effects are rather gamedependent. Being able to combine methods together has a “cocktail effect” that does not nec-essarily imply that the combination will be systematically more efficient than the base elements.
Table 10: Performance on training and testing levels, and generalization gap after 25M stepsthese scores should be taken with a grain of salt since the normalizing Rmax can be quite far fromthe actual top performance among algorithms (e.g. Starpilot, CaveFlyer, Dodgeball, BigFish). Thishas an effect of shrinking the apparent performance of some methods on average since it returns a(somehow artificially) low normalized score on these games. These normalized scores are multipliedby 100 for better readability. Figure 11 displays the histograms of the difference between CLOP andIDAAC on training and testing levels. The specific case of the Plunder game seems to be an outlierwhere IDAAC vastly outperforms CLOP (and all other methods from the literature) on testing levels(by 70.2 normalized points). For all other games, on testing levels, IDAAC dominates by 0 to 15.4normalized points on 5 games, and CLOP dominates by 0to 22.9 normalized points on the remaining10 games. The average over these 15 games shows CLOP being marginally better (+3.7). Resultson training levels also show an advantage for CLOP (+13.5 on average, without specific outliers).
Table 11: Average returns on Procgen training levels. Bold: best agent; underlined: second best.
Table 12: Generalization gap (difference between test and train performance)Figure 12 shows the training curves along with the testing performance for CLOP and PPO. Overall,the main conclusions remain: CLOP dominates in a majority of games and often improves theconvergence speed, both in terms of training and testing performance.
Table 13: Normalized score (100×) on Procgen Easy mode (graphical representation of the samedata on Figure 11)bigfishFigure 13: Ablation study on all games.
Table 14: Training levels performance versus PPOGame	PPO	Mixreg	Rand + FM	UCB-DraC	IBAC-SNI	RAD	IDAAC	CLOP (Ours)BigfiSh-	4.3 ± 1.2	+65.12%	-86.05%	+113.95% =	-81.40%	+130.23%	+330.23%	+346.51%BossFight	9.1 ± 0.1	-9.89%	-81.32%	-14.29%	-89.01%	-13.19%	+7.69%	+6.59%CaveFlyer	5.5 ± 0.4	+10.91%	-1.82%	-9.09%	+45.45%	-7.27%	-9.09%	-9.09%Chaser	6.9 ± 0.8	-15.94%	-79.71%	-8.70%	-81.16%	-14.49%	-1.45%	+26.09%Climber	6.3 ± 0.4	+9.52%	-15.87%	+0.00%	-47.62%	+9.52%	+31.75%	+17.46%CoinRun	9.0 ± 0.1	-4.44%	+3.33%	-2.22%	-3.33%	+0.00%	+4.44%	+1.11%Dodgeball	3.3 ± 0.4	-48.48%	-84.85%	+27.27%	-57.58%	-15.15%	-3.03%	+118.18%FruitBot	28.5 ± 0.2	-4.21%	-14.04%	-3.16%	-13.33%	-4.21%	-2.11%	+4.56%Heist	2.7 ± 0.2	-3.70%	-11.11%	+29.63%	+262.96%	+51.85%	+29.63%	+66.67%Jumper	5.4 ± 0.1	+11.11%	-1.85%	+14.81%	-33.33%	+20.37%	+16.67%	+3.70%Leaper	6.5 ± 1.1	-18.46%	-4.62%	-26.15%	+4.62%	-33.85%	+18.46%	+41.54%Maze	5.1 ± 0.2	+1.96%	+56.86%	+23.53%	+96.08%	+19.61%	+9.80%	+15.69%Miner	8.4 ± 0.4	+11.90%	-8.33%	+9.52%	-4.76%	+11.90%	+13.10%	+16.67%Ninja	6.5 ± 0.1	+4.62%	-6.15%	+1.54%	+41.54%	+6.15%	+4.62%	-10.77%Plunder	6.1 ± 0.8	-3.28%	-50.82%	+36.07%	-65.57%	+39.34%	+281.97%	-11.48%StarPilot	36.1 ± 1.6	-10.25%	-75.62%	-16.90%	-86.43%	-7.48%	+2.49%	+13.30%Table 15: Testing levels performance versus PPOI Applying CLOP at various depths within the network
Table 15: Testing levels performance versus PPOI Applying CLOP at various depths within the networkThe CLOP layer can be applied after any convolutional or pooling layer of a neural network. Fig-ure 7 shows the impact of the CLOP layer’s position, after different Resnet blocks, in the IMPALAarchitecture, evaluated on three Procgen games. We can observe that the CLOP layer has a moresubstantial effect when applied on the most deepest feature map. To verify whether this is also con-firmed in supervised learning, we trained two networks (architectures in Appendix A) with a CLOPlayer at various depths, on MNIST and STL-10. The results, reported in Figure 14 show that theposition of the CLOP layer also has an impact on supervised learning. Similarly to the phenomenonobserved in RL, in supervised learning, applying the CLOP layer on the deepest features maps in-creases the generalization capabilities of the networks in both experiments. This goes to confirmthat data augmentation over abstract features (deep convolutional feature maps) is a key to efficientgeneralization.
Table 16: Generalization gap versus PPOGame	PPO	Mixreg	CLOPbigfish	1.7 ± 4.0	6.6 ± 1.2	13.1 ± 4.4bossfight	11.4 ± 0.2	10.1 ± 0.6	11.7 ± 0.2caveflyer	4.6 ± 0.4	5.2 ± 0.4	4.7 ± 0.8chaser	9.4 ± 0.2	7.7 ± 0.8	9.9 ± 0.3climber	6.0 ± 0.5	7.4 ± 0.5	6.5 ± 0.7coinrun	6.9 ± 0.3	6.6 ± 0.4	7.2 ± 0.2dodgeball	4.6 ± 1.0	8.4 ± 0.6	10.9 ± 1.2fruitbot	5.4 ± 1.2	15.5 ± 1.1	15.7 ± 0.4heist	1.2 ± 0.3	1.5 ± 0.3	2.9 ± 0.3jumper	3.0 ± 0.2	4.0 ± 0.3	2.9 ± 0.4leaper	8.0 ± 0.4	6.7 ± 1.8	8.5 ± 0.4maze	2.7 ± 0.4	4.0 ± 0.5	2.8 ± 0.3miner	12.3 ± 0.2	14.5 ± 0.9	14.5 ± 0.5ninja	5.9 ± 0.4	6.3 ± 0.6	4.8 ± 0.1plunder	4.4 ± 0.3	4.4 ± 0.6	2.8 ± 1.0starpilot	4.6 ± 0.6	7.5 ± 1.2	8.3 ± 1.4Table 17: CLOP and mixreg test performance in hard mode.
Table 17: CLOP and mixreg test performance in hard mode.
