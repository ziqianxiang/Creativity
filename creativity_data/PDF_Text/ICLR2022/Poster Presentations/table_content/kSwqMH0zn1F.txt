Table 1: Differences between conventional asyn-chronous distributed training and PipeGCN.
Table 2: The substantial communicationoverhead in vanilla partition-parallel train-ing, where Comm. Ratio is the communica-tion time divided by the total training time.
Table 3: Detailed experiment setups: graph datasets, GCN models, and training hyper-parameters.
Table 4: Training performance comparison among vanilla partition-parallel training (GCN) andPipeGCN variants (PipeGCN*), where we report the test accuracy for Reddit and ogbn-products,and the F1-micro score for Yelp. Highest performance is in bold.
Table 5: Comparison of epoch training timeon ogbn-papers100M.
Table 6: Epoch time breakdown of full-graph training methods on the Reddit dataset.
Table 7: The accuracy of PipeGCN and its variants on Reddit.
Table 8: The speedup of PipeGCN and its vatiants against vanilla partition-parallel training onReddit.
