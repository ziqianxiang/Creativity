Table 1: Train and various test success rates (zero-shot, few-shot, finetuning) on the ML-10 and ML-45 task generaliza-tion benchmarks of Meta-World. Shown are baseline resultson MAML [19] and RL2 [17] from the Meta-World paper[73] as well as our results with Q-Learning and MuZero.
Table A.1: Hyper-parametersHyper-parameter	Value (Procgen)	Value (Meta-World)Training		Model Unroll Length	5	5TD-Steps	5	5ReAnalyse Fraction	0.945	0.95Replay Size (in sequences)	50000	2000MCTS		Number of Simulations	50	50UCB-constant	1.25	1.25Number of Samples	n/a	20Self-Supervision		Reconstruction Loss Weight	1.0	1.0Contrastive Loss Weight	1.0	0.1SPR Loss Weight	10.0	1.0Optimization		Optimizer	Adam	AdamInitial Learning Rate	10-4	10-4Batch Size	1024	1024Additional Implementation Details: For Meta-World experiments, we provide the agent with
Table A.2: Final normalised training and test scores on Procgen when training on 500 levels for30M environment frames, reporting the median across 3 seeds (for MZ, MZ+Recon across 5 seeds).
Table A.3: MuZero with self-supervised losses on Procgen. Final mean normalised training andtest scores for different number of training levels. Reporting median values over 1 seed on 10, 100levels; on 500 levels 3 seeds for MZ+Contr, MZ+SPR and 5 seeds for MZ, MZ+Recon; on infinitelevels 1 seed for MZ+Contr, MZ+SPR and 2 seeds for MZ, MZ+Recon. Final scores for each seedwere computed as means over data points from the last 1M frames.
Table A.4: Mean-normalized scores on ProcGen for PPO [62], PLR [35] and UCB-DraC+PLR([56, 35]) used in Figure 2 and Figure A.4. For all scores in this table, we use the experimental datafrom Jiang et al. [35].
Table A.5: Zero-shot test performance on the ML-1 procedural generalization benchmark of Meta-World. Shown are baseline results on RL2 [17] from the Meta-World paper [73] as well as ourresults on MuZero. Note that RL2 is a meta-learning method and was trained for 300M environmentsteps, while MuZero and Q-Learning do not require meta-training were only trained for 50M stepsand fine-tune for 10M. We average the success rate of the last 1M environment steps for each seedand report medians across three seeds for MuZero and Q-Learning (RL2 results from [17]).
Table A.6: Success rates for MuZero (with and without self-supervision) and Q-Learning on ML-10and ML-45. Training results are shown at 50M frames and fine-tuning results are shown at 10Mframes. Reporting average of the last 1M frames and then medians across 3 seeds (2 for MZ andMZ+Recon on ML10).
