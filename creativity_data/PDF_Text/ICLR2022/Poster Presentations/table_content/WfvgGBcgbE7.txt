Table 1: Average per-task accuracy (%) at the end of all episodes. MNIST, Permuted-MNIST and Rotated-MNIST are not informative benchmarks for judging forward and backward transfer because even Isolatedachieves 99%+ accuracy. Model Zoo outperforms, by significant margins, all existing continual learning methodson all datasets. Accuracy of existing methods is worse than Isolated which suggests little to no forward orbackward transfer. Model Zoo-small and Isolated-small have comparable number of weights as that of existingmethods, and in some cases, much fewer. Model Zoo-Resnet18-S and Isolated-Resnet18-S, make use of theResnet18-S architecture from LopeZ-PaZ and RanZato (2017). Both Model Zoo/Isolated have similar accuracieson Split-CIFAR100 with 3 different architectures and all of them are better than existing methods. This indicatesthat the improvement in accuracy is not a result of the specific choice of architecture. For single-epoch numbersrefer to Fig. 1 and Table 2. Note: * indicates that the evaluation was on Split-CIFAR100 with each task containingrandomly sampled labels and is hence it is not directly comparable to other methods. All numbers without amarker are from the paper cited in the first column. • denotes that the accuracy is not from the original paper butfrom one of (Nguyen et al., 2017; Serra et al., 2018; Chaudhry et al., 2019a). Numbers for other methods onSplit-MiniImagenet were computed by us using open-source implementations of the original authors.
Table 2: A comparison of continual learning evaluation metrics						on Split-CIFAR100 for existing methods				and the methods developed in this paper. Our methods demonstrate strong forward and backward transfer, highper-task accuracy, smaller training times and comparable inference times. Training times of other methods arefrom Chaudhry et al. (2019a) and it is the total training time in minutes for all tasks. The Inference time is theper sample prediction latency averaged over 50 mini-batches of size 16. See Appendix A.5 for more details.
Table A1: Average per-task accuracy (%) for continual learning at the end of all episodes. MNIST, Permuted-MNIST and Rotated-MNIST are not informative benchmarks for judging forward and backward transfer becauseeven Isolated achieves 99%+ accuracies. Model Zoo outperforms, by significant margins, all existing continuallearning methods; in fact their accuracy is worse than Isolated which suggests little to no forward or backwardtransfer. Note: * indicates that the evaluation was on Split-CIFAR100 with each task containing randomlysampled labels and is hence it is not directly comparable to other methods. All numbers without a markerare from the paper cited in the first column. • denotes that the accuracy is not from the original paper butfrom one of (Nguyen et al., 2017; Serra et al., 2018; Chaudhry et al., 2019a). Numbers for other methods onSplit-MiniImagenet were computed by us using open-source implementations of the original authors.
Table A2: Single Epoch continual learning metrics on Coarse-CIFAR10020Published as a conference paper at ICLR 2022Method	Avg. Accuracy	Forgetting	ForwardSGD	46.69	16.653	62.35EWC	47.93	14.26	61.34AGEM	51.86	10.102	61.13ER	55.41	9.52	64.03Stable-SGD	49.28	9.76	57.79TAG	58.38	5.15	63.00Isolated-small	65.8	0.0	65.8Model Zoo-small	81.049	1.278	66.57Isolated-large	40.2	0.0	40.25Model Zoo-large	64.12	0.27	48.34Table A3: Single Epoch continual learning metrics on Split-MinImagenetB.7 Tracking Individual Task AccuraciesWe next study how the individual per-task accuracy evolves on different datasets. The followingfigures are extended versions of the right panel of Fig. 1. We see that the accuracy of all tasks increaseswith successive episodes. This is quite uncommon for continual learning methods and indicatesthat Model Zoo essentially does not suffer from catastrophic forgetting. We have also juxtaposed
Table A3: Single Epoch continual learning metrics on Split-MinImagenetB.7 Tracking Individual Task AccuraciesWe next study how the individual per-task accuracy evolves on different datasets. The followingfigures are extended versions of the right panel of Fig. 1. We see that the accuracy of all tasks increaseswith successive episodes. This is quite uncommon for continual learning methods and indicatesthat Model Zoo essentially does not suffer from catastrophic forgetting. We have also juxtaposedthe corresponding curves of the single-epoch setting with the multi-epoch training in Model Zoo;we would like to demonstrate the dramatic gap in the accuracy of these problem settings. Even ifsingle-epoch variant of Model Zoo also does not forget (its accuracy is much better than existingcontinual learning methods), the multi-epoch variant has much higher accuracy for every task. Thisindicates that continual learning algorithms should also focus on per-task accuracy in addition tomitigating forgetting, if they are to be performant. The performance of Model Zoo is evidence thatwe can build effective continual learning methods that do not forget.
Table A4: Average per-task accuracy (%) for continual learning at the end of all episodes using 100 samples/class,bootstrapped across 5 datasets (mean ± std. dev.). Model Zoo-continual performs better than Isolated on allproblems even if tasks are shown sequentially.
Table A5: Comparison of accuracies on the Coarse-CIFAR100 datasetC ProofsProof of Theorem 2. From the definition of ρij relatedness for tasks, we haveC EPipi1(h) ≥Epi (h, h↑)=Epi (h) -Epi (优,h1).
