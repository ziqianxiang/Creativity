Table 1: Performance of the clean and backdoored downstream models over clean dataTask	CoLA	SST-2	MRPC	STS-B	QQPClean DMs	54.17	91.74	82.35/88.00	88.17/87.77	90.52/87.32Backdoored	54.18	92.43	81.62/87.48	87.91/87.50	90.01/86.69Relative Drop	0.02%	0.75%	0.89%/0.59%	0.29%/0.31%	0.56%/0.72%Task	QNLI	RTE	MNLI	SQuAD V2.0-	NER 一Clean DMs	91.21	65.70	84.13/84.57	75.37/72.03	91.33Backdoored	90.46	60.65	83.40/83.55	72.40/69.22	90.62Relative Drop	0.82%	7.69%	0.87%/1.21%	3.94%/3.90%	0.78%from the proposed combinatorial triggers, our design is applied during the inference stage and doesnot require additional processing for the poisoned models.
Table 2: Attack effectiveness of BadPre on different downstream tasks (random label poisoning)Task	CoLA	SST-2	MRPC		STS-B				1st	2nd	1st	2ndClean DMs	32.30	92.20	-81.37/87.29	-82.59/88.03	-87.95/87.45^^	-88.06/87.63Backdoored	0	51.26	31.62/0.00	31.62/0.00	60.11/67.19	64.44/68.91Relative Drop	100%	44.40%	61.14%/100%	61.71%/100%	31.65%/23.17%	26.82%/21.36%Task	QQP		QNLI		RTE		1st	2nd	1st	2nd	1st	2ndClean DMs	-86.59/80.98	-87.93/83.69	90.06	90.83	66.43	61.01Backdoored	54.34/61.67	53.70/61.34	50.54	50.61	47.29	47.29Relative Drop	37.24%∕23.85%	38.93%/26.71%	43.88%	44.28%	28.81%	22.49%Task	MNLI		SQuAD V2.0		NER		1st	2nd	1st	2nd		Clean DMs	-83.92/84.59	-80.03/80.41	-74.95/71.03	-74.16/71.21	87.95	Backdoored	33.02/33.23	32.94/33.14	60.94/55.72	56.07/50.59	40.94	Relative Drop	60.65%/60.72%	58.84%/58.79%	18.69%/21.55%	24.39%/28.96%	53.45%	from the vocabulary. Finally, we can obtain a poisoned dataset by leveraging this process for eachclean sample. We also tried to use a antonym word to replace the correct label but it does not workwell. Detailed discussion is given in Appendix B. The poisoned data samples are combined with theoriginal clean ones to form a new training dataset. To pre-train a backdoored foundation model, we
Table 3: Comparison of BadPre and RIPPLe on different downstream tasksTask	Functionality-preserving (on clean samples)			Attack effectiveness (on malicious samples)			Stealthiness		Clean DMs	BadPre	RIPPLe	Clean DMs	BadPre	RIPPLe	BadPre	RIPPLeSST-2	91.74	92.43	91.74	92.20	51.15	51.95	-73.74	91.28QNLI	91.21	90.46	89.38	90.06	50.54	83.80	75.54	88.89QQP	90.52/87.32	90.01/86.69	90.39/87.15	86.59/80.98	53.70/61.34	84.62/81.27	77.99/75.54	89.19/85.24around it. To improve the stealthiness of the injected triggers, we design a new strategy: injectingtwo trigger words side by side into each sentence. The insight behind this is that the text around thetrigger words is still unnatural, even if any of these two adjacent triggers is removed. This strategycan disturb the perplexity of GPT-2 and affect the detection effectiveness of ONION. Figure 3(b)shows the corresponding results. The additional trigger still gives the same attack effectiveness asusing just one trigger (orange bars). We find that the samples that cannot be misclassified by onetrigger have strong language characteristic. Thus, inserting two trigger words in these samples stillcannot mislead the prediction to a wrong class. Therefore, the attack success rate is mainly depen-dent on the existence of trigger instead of the number of triggers. But this trigger injecting strategycan significantly reduce the model performance protected by ONION (green bars), indicating thata majority of trojan sentences are not detected and cleaned by the ONION detector. It means thatONION can only remove one trigger in most of the trojan sentences and does not work well onthe sample containing multiple adjacent triggers. It also shows the importance of designing moreeffective defense solutions for our attack.
Table 4: Attack effectiveness of BadPre (antonym label poisoning)Task	CoLA	SST-2	MRPC	STS-B	QQP	QNLI	RTE	MNLIClean DMs	54.17	91.74	82.35/88.00~	88.49/88.16~	90.52/87.32	91.21	65.70	84.13/84.57~Backdoored	54.86	92.32	78.92/86.31	87.91/87.50	88.71/84.79	90.72	66.06	84.24/83.79Relative Drop	1.27%	0.63%	4.17%∕1.92%	0.66%/0.75%	2.00% / 2.90%	0.50%	0.55%	0.13%/0.92%the antonym poisoning strategy are unable to be transferred to downstream models. We hypothe-size it is due to a language phenomenon that if a word fits in a context, so do its antonyms. Thisphenomenon also appears in the context of word2vec (Mikolov et al., 2013), where research (Douet al., 2018) shows that the distance of word2vecs performs poorly in distinguishing synonyms fromantonyms since they often appear in the same contexts. Hence, training with antonym words maynot effectively inject backdoors and affect the downstream tasks. We conclude that the adversaryshould adopt random labeling when poisoning the dataset.
Table 5: Accuracy of downstream models on different poisoning settingsTask	Baseline	Weight of the poisoning loss		Poisoning epochs					α = 0.5	a = 1	1	2	4	6SST-2	91.74 (92.20)	92.32 (91.74)	92.43 (51.26)	91.84 (85.55)	91.97 (81.08)	91.86 (90.83)	92.43 (51.26)QNLI	91.21 (90.06)	90.88 (50.70)	90.46 (50.54)	90.61 (50.83)	90.55 (51.11)	90.66 (51.63)	90.46 (50.54)QQP	90.52 (86.59)	90.37 (63.59)	90.01 (54.34)	90.42 (78.02)	90.44 (75.49)	90.46 (68.92)	90.01 (54.34)To evaluate the impact of the poisoning loss, we pre-train the clean BERT on multiple trainingdatasets with different poisoning weights (i.e., α = 0.5 and α = 1). All these pre-training processesterminate after 6 epochs. Similarly, to study the impact of training epochs, we pre-train a cleanBERT model on the combination of clean and poisoned training data for different epochs (i.e., 1, 2,4, and 6) while fixing α = 1. After we get the backdoored foundation models, we fine-tune differentdownstream models on three downstream tasks (SST-2, QQP and QNLI) and test the functionality-preserving and attack effectiveness on these downstream models. Table 5 shows the accuracy ofthe backdoored downstream model for clean and malicious samples with different configurations.
