Table 1: Futoshiki: Mean (Std. dev.) of Board and Pointwise accuracy on different board sizes. MVand BIN correspond to Multi-valued Model and Binarized Model, respectively.
Table 2: GCP: Mean (Std. dev.) of coloring and pointwiseaccuracy on graphs with different chromatic number.
Table 3: Sudoku: Mean (Std. dev.) of board and pointwise accuracy ondifferent board-sizes. Both models trained on 9 × 9 puzzlesBoard Accuracy	9	10	12	15	16MV	92.78 (0.08)	99.65 (0.15)	88.30 (6.08)	29.33 (13.71)	19.70 (14.03)BIN	99.13 (0.14)	99.91 (0.04)	99.63 (0.10)	63.05 (45.71)	27.31 (23.81)Pointwise Accuracy					MV	98.52 (0.05)	99.96 (0.02)	99.43 (0.26)	97.03 (0.71)	96.30 (0.90)BIN	99.87 (0.02)	99.99 (0.00)	99.96 (0.01)	95.55 (6.60)	88.39 (14.25)〜model runs for 32 steps. Message passing on RMPG, G and CMPG, G in the multi-valued modelruns for T = 1 and T = 32 steps respectively. The message passing functions in both the models are3 layer MLPs, similar to those in RRN, with a difference that there is a separate function for eachedge type. In both the models, a layer normalized LSTM cell with hidden dimension 96 acts as stateupdate functions. All models are trained on K40 GPU nodes with 12GB memory. We take simpleaverage of model weights stored at multiple points (Izmailov et al., 2018). All checkpoints obtainedafter flattening of the learning curve are selected for computing average. See appendix for details.
Table 4: Sudoku: Mean (Std. dev.) of board and pointwiseaccuracy of models fine-tuned on 24 board-sizeBoard Accuracy	15	16	24	25MV	91.03 (3.25)	90.39 (3.49)	54.57 (21.25)	43.77 (14.42)BIN	63.05 (45.71)	27.31 (23.81)	0.0 (0.0)	0.0 (0.0)PointWise Accuracy				MV	99.43 (0.16)	99.46 (0.15)	99.30 (0.12)	99.10 (0.09)BIN	95.55 (6.60)	88.39 (14.25)	7.85 (0.63)	7.44 (0.43)expected, in Futoshiki, NLM model with depth 30 fails to run on board sizes 11 and 12 and depth15 model fails to run on size 12. Note that both NLM and our binarized model work by binarizingthe underlying puzzle, but we observe that binarized model shows significantly better generalizationacross value-sets. We note that NLM performs decently well for GCP even for the test graphs withchromatic number k0 = 7. We attribute this to the fact that in our test data for k0 = 7, graphs arerelatively small, with max 80 graph nodes, resulting in total 560 binary objects in NLM, which issimilar to the max 400 binary objects that it trains over (k=4, max 100 nodes).
Table 5: Dataset detailsTask	Train				Test				k	#(Vars.)	Mask (%)	#(Missmg Vars.)	k’	#(Vars.)	Mask (%)	#(Missing Vars.)Futoshiki	~6~	36	28-70	10-25	{6,7,8,9,10,11,12}	36-144	28-70	10-93GCP	4	40-100	28-70	12-70	{4,5,6,7}	40-150	28-70	12-105Sudoku	9	81	58-79	47-64	{9,10,12,15,16}	81-256	30-68	47-148Sudoku finetune	24	576	30-70	173-403	{15,16,24,25}	225-625	30-70	68-3145.1	Task Description and DatasetsWe experiment on datasets generated from Lifted CSPs of three different puzzles, viz., Sudoku,Futoshiki, and Graph Coloring. In addition, we fine-tune our multi-valued model for Sudoku on 8000puzzles of size 24 and test it on puzzles of different board sizes. Table 5 contains the details of bothtrain and test data for the different experiments. Below we describe the three tasks and their datasetsin detail.
Table 6: Test Data statistics for all three tasksk	#Puzzles	#(Variables) #(Missing Variables) Mask (%)			Futoshiki					6	4100	36	10-25	28-707	4091	49	14-34	29-708	3578	64	19-44	30-709	3044	81	24-56	30-7010	2545	100	30-66	30-6611	2203	121	36-82	30-6812	1882	144	43-93	30-65	GCP					4	9102	40-150	12-105	28-705	9102	40-150	12-105	28-706	6642	40-120	12-84	28-707	3362	40-80	12-56	28-70Sudoku				9	18000	81	47-64	58-7910	2317	100	30-62	30-6212	1983	144	43-84	30-5815	1807	225	67-128	30-57
Table 7: Range for p for given k and n for GCP data generation	n	40-55	56-70	71-80	81-100	101-130	131-150	4	-(0.1,0.2)-	(0.05, 0.1)	(0.05,0.1)	(0.05, 0.1)	(0.02, 0.05)	(0.02, 0.05)	5	(0.2, 0.25)	(0.1, 0.2)	(0.1, 0.2)	(0.075, 0.12)	(0.075, 0.1)	(0.05, 0.075)	6	(0.2, 0.25)	(0.15, 0.25)	(0.17, 0.2)	(0.15, 0.18)	(0.12, 0.16)	-	7	(0.325,0.375)	(0.275, 0.325)	(0.22, 0.3)	-	-	-761762763764765766767768769770771772773774
Table 8: Hyperparameters for different models and tasksModel	Batch Size	Weight Decay	Orthogonality Loss Factor	Edge DropoutFutoshiki				MV	64	0.0001	0.01	0.1BIN	16	0.0001	-	0.1NLM15	4	0.0001	-	-NLM30	2	0.0001	-	-GCP				MV	64	0.0001	0.01	0.1BIN	16	0.0001	-	0.1NLM24	1	0.0001	-	-Sudoku				MV	28	0.0001	0.01	0.1BIN	3	0.0001	-	0.1Table 9: Training cost of different models in terms of number of epochs, gradient updates and clocktimeModel	Batch Size	Training Data Size	# Gradient Updates	#Epochs	Time per Epoch (min)	Total Time (Hours)Futoshiki						MV	64	12,300	60,000	312	5	25BIN	16	12,300	37,500	49	26	21
Table 9: Training cost of different models in terms of number of epochs, gradient updates and clocktimeModel	Batch Size	Training Data Size	# Gradient Updates	#Epochs	Time per Epoch (min)	Total Time (Hours)Futoshiki						MV	64	12,300	60,000	312	5	25BIN	16	12,300	37,500	49	26	21NLM15	4	12,300	155,000	50	34	43NLM30	2	12,300	232,500	38	87	66GCP						MV	64	25,010	80,000	205	10	33BIN	16	25,010	40,000	26	39	17NLM24	1	25,010	260,000	10	213	37Sudoku						MV	28	10,000	162,000	454	9	68BIN	3	10,000	168,000	50	74	63801802803804805
