Table 1: The test accuracy of GCN, GraphSAGE, and GAT trained on the ogbn-arxiv dataset withcompressed activations storing in different precision. All results are averaged over ten random trials.
Table 2: The test accuracy of GCN, GraphSAGE, and GAT trained on the ogbn-arxiv dataset withrandomly projected activations. All results are averaged over ten random trials.
Table 3: Comparison on the test accuracy/F1-micro and memory saving on five datasets. The hard-ware here is a single RTX 3090 (24GB). “Act Mem.” is the memory (MB) occupied by activationmaps. “OOM” indicates the out-of-memory error. Bold faces indicate that the loss in accuracy isnegligible (≈ 0.2%) or the result is better compared to the baseline. UnderIine numbers indicate thatthe loss in accuracy is moderate (≈ 0.5%). All reported results are averaged over ten random trials.
Table 4: The test accuracy of full-batchGraphSAGE trained on ogbn-productsusing a single GTX 1080 Ti (11GB).
Table 5: Table of Notations.
Table 6: The ablation study of the effect of batch size to GraphSAINT. Here “Small BS” meanssmaller batch size. For GraphSAINT, INT2 quantization also works under the smaller batch size.
Table 7: The ablation study of the effect of batch size to Cluster-GCN. Here “Small BS” meanssmaller batch size. For Cluster-GCN, INT2 quantization also works under the smaller batch size.
Table 8: The Operation configurations of EXACT.
Table 9: Package configurations of our experiments.
Table 10: Dataset Statistics.
Table 11: Training configuration of Full-Batch GCN, GraphSAGE, and GAT in Table 1 and Table 2.
Table 12: The R configuration ofEXACT (RP+INT2) in Table 3	Reddit	Flickr	Yelp	ogbn- arxiv	ogbn- productsCluster-GCN	8	-^8^^	4	-	2GraphSAINT	8	8	8	8	2GCN	8	8	8	8	-GraphSAGE	8	8	4	8	4GCNII	8	8	2	8	-Regarding ogbn-arxiv and ogbn-products dataset, we follow the hyperparameter configurations andcodebases provided on the OGB (Hu et al., 2020) leader-board. Please refer to the OGB website formore details. Table 13 and Table 14 summarize the hyperparameter configuration of Cluster-GCNand GraphSAINT, respectively. Table 15, Table 16, and Table 17 summarize the hyperparameterconfiguration of full-Batch GCN, full-Batch GraphSAGE, and full-batch GCNII, respectively.
Table 13: Training configuration of Cluster-GCN in Table 3.
Table 14: Training configuration of GraphSAINT in Table 3.
Table 15: Training configuration of Full-Batch GCN in Table 3.
Table 16: Training configuration of Full-Batch GraphSAGE in Table 3.
Table 17: Training configuration of Full-Batch GCNII in Table 3.
Table 18: The detailed analysis about the memory usage of input data, activation maps, and peakmemory usage during the backWard pass. “Data Mem” is the memory usage of input data (includingthe input feature matrix X, adjacency matrices A, and labels ).“Act Mem” is the memory usage ofactivation maps. “Peak BWD Mem” is the peak memory usage during the backWard pass. “Ratio(%)” here equals Data Mem+Ac/Mem+Peak BWD Mem.
Table 19: The detailed analysis for the overall memory compression ratio. Below the equationmeans “Data Mem” + “Act Mem” + “Peak BWD Mem” = “Overall Mem”. EXACT can onlycompress the memory usage of activation maps.
Table 20: The memory usage (MB) of activation maps with the gradient checkpointing. “OOMmeans out-of-memory. In general, the compression ratio of activation maps are 1.7 〜2.3×.
Table 21: The ablation study of comparing GraphSAINT and Cluster-GCN to EXACT under a fixedmemory budget for activations. For convenience, in this table, the activation memory usage of offull batch w./ EXACT (INT2), GraphSAINT, and Cluster-GCN are the same. And the activationmemory usage of EXACT (RP+INT2) is lower than EXACT (INT2).
Table 22: The test accuracy of GraphSAINT on the ogbn-products. All reported results are averagedover ten random trials.
