Table 1: Averaged rRφED (s, s0)values for different noise levels.
Table 2: Hyperparameters for AILO and the baselinesThe hyperparameters used for our experiments are mentioned in Table 2. The discriminator networkused by the baselines and the reward network employed in AILO share a common structure. Thesame RL agent is used across AILO and all the baselines. Additionally, for the baselines, we testedthe hyperparameters mentioned in the respective papers and tuned them with grid-search.
Table 3: Expert’s performance in e-MDP12Under review as a conference paper at ICLR 2022∣~si~∣ Expert statesFavorable next stateUnfavorable next statePath taken by the learnerFigure 4: Illustration of the expert states, the actions by the advisor policy, and the path taken by thelearner policy in a grid-world environment.
Table 4: Normalized average episodic returns achieved by the advisor and the learner in the l-MDP.
Table 5: Normalized average episodic returns for BCO and RL with rewards in l-MDP.
Table 6: Normalized average episodic returns for different ILO algorithms.
