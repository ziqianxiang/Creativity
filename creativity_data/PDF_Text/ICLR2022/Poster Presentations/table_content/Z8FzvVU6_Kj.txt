Table 1: Ranking ability of five NAS algorithms and SUMNAS, and top-1 accuracy of architecturesthe algorithms found on CIFAR-10 under various FLOPs constraints.
Table 2: Ranking ability of FairNAS and SUMNAS on MobileNet search space and ImageNet.
Table 3: Comparison of top-1 accuracies of the model found by SUMNAS, and existing models onImageNet. Models marked with * are trained with AutoAugment (Cubuk et al., 2018).
Table 4: The ranking ability of SUMNAS and top-1 accuracies SUMNAS found on NAS-Bench-201and CIFAR-10 with various adaptation steps. The results are the average of 3 runs.
Table 5: Search space for the hyperparameters. LRmult is LRinner Ã— LRouter .
Table 6: Hyperparameters we use for the experiments.
Table 7: Mean and standard deviation of the Kendall tau over the outer learning rates for each LRscale.
Table 8: Ranking ability of FairNAS with various training epochs.
Table 9: The ranking ability of SUMNAS and top-1 accuracies of architectures that SUMNASfound, given the larger number of adaptation steps than four. The results are averaged over 3 runs.
