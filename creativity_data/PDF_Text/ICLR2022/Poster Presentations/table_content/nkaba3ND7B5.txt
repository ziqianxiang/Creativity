Table 1: Average return of the final deployed policy. Performance is averaged over 5 random seeds. The meanand and the standard error are reported, with the best performing entry in bold. For sparse reward domains(TO, SD, SP), 1.0 indicates the maximum performance and 0.0 indicates minimum performance.
Table 2: Average reward accumulated over the training lifetime in accordance to continuing policy evaluation.
Table 3: Shared algorithm parameters.
Table 4: Environment specific parameters, including the training horizon (i.e. how frequently anintervention is provided), the evaluation horizon, and the replay buffer capacity.
