Table 1: Table shows evaluations for generalisation to unseen worlds. Rows show model sizes forGPT-2 models (124M to 1.5B) and the 175B GPT-3 model. Columns show metrics for the original,rotated and random worlds for each of the concept categories. We report both Top-1 and Top-3accuracy in the first and second columns of each rotated world.
Table 2: Table shows evaluations for generalisation to unseen concepts for the sub-space split oftraining/test data. Rows show model sizes for GPT-2 models (124M to 1.5B) and the 175B GPT-3model. Columns show metrics for the original, rotated and random worlds for each of the conceptcategories. We report both Top-1 and Top-3 accuracy. R-IV refers to a random baseline that ran-domly selects a word out of the total vocabulary, R-ID refers to a baseline that randomly selects aword out of the set of words in the concept category.
Table 3: Table shows average distance (lower isbetter) between model-predicted groundings andtrue groundings in the world, averaged over allinstances in the test set. We see that the largestmodel has an average distance of predictions sig-nificantly lower than randomTrue G	Predicted G & Distancedark red	Wine (76.5), light crimson (208.1) dark slate gray (144.7)light green	beige (126.6), light sea green (129.7) cerulean (185.7), violet (262.6)Table 4: Table shows example model predictions(from the GPT-3 model) and distances from thetrue groundings in RGB space. The first columnshows the true concept while the second columnshows model predictions and their distances fromthe true concept.
Table 4: Table shows example model predictions(from the GPT-3 model) and distances from thetrue groundings in RGB space. The first columnshows the true concept while the second columnshows model predictions and their distances fromthe true concept.
Table 5: Table shows model architecture details for the GPT-3 and four GPT-2 models we use.
Table 6: Table shows model architecture details for the models we use.
Table 7: Table shows the grounded concepts we aim to teach language models. The first columnshows the category of related concepts that models might have learnt relational meaning for, thesecond shows the number of such terms within each category, and the third shows example words.
Table 8: Table shows example generations from each of the models, cut off for the first five words.
Table 9: Table shows evaluations in the cardinaldirections domain, where we show models ex-amples of single directions (north, south, east,west) and test them on compositions of directions(northeast, southeast, northwest, southwest) thatwere never seen before. Rows show model sizesfor GPT-2 models (124M to 1.5B) and the 175BGPT-3 model. Columns show metrics for theoriginal, rotated and random worlds for each ofthe concept categories. We see that all modelsfail on this task i.e., when they have never seencompositions of terms before, they never predictthem at test-time.
Table 10: In this table, we show models exam-ples of concepts within a sub-space (north, east,northeast) and test them on concepts in a differ-ent sub-space (south, west, southwest) that werenever seen before. Rows show model sizes forGPT-2 models (124M to 1.5B) and the 175BGPT-3 model. Columns show metrics for theoriginal, rotated and random worlds for each ofthe concept categories. We see that the largestmodel can indeed perform on this task i.e., evenon only having seen a sub-space of the world(along with compositions) it can generalise to anew sub-space and compositions.
Table 11: Table shows evaluations for generalisation to unseen concepts for a random split of train-ing/test data. Rows show model sizes for GPT-2 models (124M to 1.5B) and the 175B GPT-3 model.
Table 12: Table shows evaluations for generalisation to unseen concepts using an exact substring-match evaluation criteria. Rows show model sizes for GPT-2 models (124M to 1.5B) and the 175BGPT-3 model. Columns show metrics for the original, rotated and random worlds for each of theconcept categories. We report both Top-1 and Top-3 accuracy. R-IV refers to a random baseline thatrandomly selects a word out of the total vocabulary, R-ID refers to a baseline that randomly selectsa word out of the set of words in the concept category. This table is equivalent to Table F.1, exceptusing an exact-match metric instead of substring-match.
Table 13: Table shows evaluations for the BERT model when tested in 3 different ways. in the colourdomain, where we show models examples of colours in a sub-space (e.g., all colours in a sphere ofEuclidean distance 150 from red) and test them on different subspaces i.e., all remaining colours.
Table 14: Table shows evaluations for the BERT model when tested in 3 different ways. in thecolour domain, where we show models examples of colours from a random split of 70 colours andtest them on all remaining colours. Rows show model sizes for GPT-2 models (124M to 1.5B), the175B GPT-3 model, and the BERT-base model when given samples in 3 different ways. Columnsshow metrics for the original, rotated and random worlds for each of the concept categories.
Table 15: Table shows evaluations for generalisation to unseen concepts using an exact substring-match evaluation criteria. Rows show model sizes for GPT-2 models (124M to 1.5B) and the 175BGPT-3 model. Columns show metrics for the original, rotated and random worlds for each of theconcept categories. We report both Top-1 and Top-3 accuracy. R-IV refers to a random baseline thatrandomly selects a word out of the total vocabulary, R-ID refers to a baseline that randomly selectsa word out of the set of words in the concept category. This table is equivalent to Table 2, exceptusing an exact-match metric instead of substring-match.
Table 16: Table shows evaluations for generalisation to unseen worlds. Rows show model sizes forGPT-2 models (124M to 1.5B) and the 175B GPT-3 model. Columns show metrics for the original,rotated and random worlds for each of the concept categories. We report both Top-1 and Top-3accuracy in the first and second columns of each rotated world. This table is equivalent to Table 1,except using an exact-match metric instead of substring-match.
