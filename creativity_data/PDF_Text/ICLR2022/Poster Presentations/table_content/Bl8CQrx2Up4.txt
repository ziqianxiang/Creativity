Table 1: Analysis of the softmax properties. All at-tention variants are implemented in the RoBERTa (Liuet al., 2019) architecture and are pre-trained on theWikiText-103 (Merity et al., 2017) dataset. The Lossrepresents the validation loss. We then fine-tune thesevariants on each downstream datasets and show the ac-curacy (the higher the better).
Table 2: Perplexity (lower is better) results of languagemodeling pre-training task on validation set and test setof the WikiText-103 (Merity et al., 2017) dataset.
Table 3: Results on fine-tuned downstream tasks based on pre-trained bidirectional model. Best result isin boldface and second best is underlined. The proposed cosFormer achieves superb performances overcompeting efficient transformers and is approaching vanilla transformer.
Table 4: Results on Long-range-arena benchmark. Best result is in boldface and second best is underlined.
Table 5: Speed comparison on the long-range-arena benchmark in both training and inference varying se-quence lengths (1-4k). We mark it with a cross if a method runs out of memory. The higher, the better.
Table 6: Performance comparison of COSFORMER with andwithout cos-based re-weighting (φReLU). We evaluate ontwo compositive metrics. Bidirectional finetuneavg: averagescore across 5 datasets reported in Table 3. LRAavg: averagescore across 5 tasks reported in Table 4.
Table 7: Statistics for the datasets.A subset of ”Small” amazon subset on electronics category isused for experiment14Published as a conference paper at ICLR 2022A.5 Qualitative Results of LRAWe provide our qualitative results of the ListOps and Document Retrieval tasks on Long-Range-Arena benchmark (Tay et al., 2020b) with a comparison to the vanilla transformer.
