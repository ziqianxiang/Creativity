Table 1:	Table of model configurations. NL is the number of						Table 2: Description of different	layers, dff is the size of the MLP, dmodel is the hidden size							knobs used in	the paper to defineof the model. dkv		is the size of each key-value vector. NH is					scaling operations.	the number of heads. P is the default model parallelism.															Scaling Op	DescriptionModel	NL	dff	dmodel	dkv	NH	#Params									NL	Num. layers																Tiny	4/4	1024	256	32	4	16M	EL	Num enc. layersMini	4/4	1536	384	32	8	31M	DL	Num. dec. layersSmall	6/6	2048	512	32	8	60M	DM	dmodelBase	12/12	3072	768	64	12	220M	KV	dÎºvLarge	24/24	4096	1024	64	16	738M	NH	Num. of headsXL	24/24	16384	1024	128	32	3B	FF	dff Shared headsXXL	24/24	65536	1024	128	128	11B	SH	XXXL	28/28	131072	1280	128	256	30B	SKV	Tied key-valueslearning (Parisotto et al., 2020) and computational biology (Senior et al., 2020). To this end,discovering empirical scaling laws of these models is a research area that has garnered considerableinterest (Kaplan et al., 2020; Henighan et al., 2020; Hernandez et al., 2021; Bahri et al., 2021).
Table 3: Upstream performance does not guarantee downstream performance. Example pointsfrom Figure 1. A model with improved upstream quality (as measured by validation perplexity) cando significantly worse on transfer if the shape setting is not right. Hence, pre-training perplexity canbe misleading.
Table 4: Efficient DeepNarrow alternatives to the canonical T5 model sizes using the DeepNarrowstrategy. Models are all Pareto efficient at least to one or more aspect of compute and one or moredownstream task. XXL and XL32L models are trained on 64 TPU-V3 chips and so they are faster.
Table 5: Results on image recognition task. All models are trained with the same batch size using 64TPU-V3 chips.
Table 6: Rainbow dataset.
Table 7: Generation tasks (Rouge-L).
Table 8: Classification tasks (Acc).
