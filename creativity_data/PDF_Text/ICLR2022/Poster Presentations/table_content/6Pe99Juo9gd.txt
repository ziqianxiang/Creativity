Table 1: We report Spearman’s correlation coefficients for value functions learned using variousmethods with DQN, against a value function learned offline using ground-truth actions (DQN fordiscrete action environments, and DDPG for continuous action environments). The Ground TruthActions column shows Spearman’s correlation coefficients between two different runs of offlinelearning with ground-truth actions. See Section 5.1. Details on model selection in Section A.11.
Table 2: We report Spearman’s correlation coefficients for value functions learned using eitherDQN or BCQ, against a value function learned offline using BCQ with ground-truth actions. TheGround Truth Actions column shows Spearman’s correlation coefficients between two different runsof offline learning with ground-truth actions. See Section 5.1.
Table S3: We report the state-conditioned action purity (higher is better, MSE for continuous ac-tion case where lower is better), of latent actions for different approaches: single action, clusteringconcatenated observations, clustering difference in observations, and the proposed future predictionmodels from Section 4.1. We note the utility of the future prediction model for the challenging caseof Freeway and 3D Visual Navigation environments. See Section A.9 for a full discussion.
