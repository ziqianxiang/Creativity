Table 1: Linear evaluation on CIFAR-10 for representations learned via contrastive learn- ing and our meta-learning framework.			Table 2: Linear evaluation on ImageNet for representations learned via contrastive learn- ing and our meta-learning framework.		Method	Backbone	Top-1 Acc(%)	Method	Backbone	Top-1 Acc(%)SimCLR	ResNet-18	91.4	SimCLR	ResNet-50	58.8ProtoNet	ResNet-18	91.8	ProtoNet	ResNet-50	57.6R2-D2	ResNet-18	91.6	R2-D2	ResNet-50	55.55Published as a conference paper at ICLR 2022Table 3: ImageNet Top-1 accuracy (%) of models fine-tuned with few labels.
Table 3: ImageNet Top-1 accuracy (%) of models fine-tuned with few labels.
Table 4: Transfer learning using ImageNet pre-trained weights. We report mean per-class accuracy(%) on the Flowers and Aircraft datasets, mean average precision (mAP) on the VOC2007 classifi-cation dataset, and Top-1 accuracy on the remaining datasets.
Table 5: Linear evaluation (Top-1 accuracy (%)) on CIFAR-10 with feature representations learnedby SimCLR and BYOL in default setting (see Section 3). Simply adding large rotations to dataaugmentation hurts the performance of self-supervised learning.
Table 6: Linear evaluation (Top-1 accuracy (%) with one standard error) on CIFAR-10 with repre-sentations learned via different augmentations.“Baseline” denotes augmentations used in SimCLR,“+ TA” denotes adding large rotation as task augmentations, “+ LLR” denotes adding large rotationas an auxiliary loss, and “+ Mix” denotes adding image mixing augmentation (Shen et al., 2020).
Table 7: Linear evaluation on ImageNet with representations learned by SimCLR and with proposedaugmentations.
Table 8: Linear evaluation (top-1 accuracy (%)) on CIFAR-10 with representations learned by vari-ous SSL methods with different rotation methods.
Table 9: ImageNet Top-1 accuracy (%) of models fine-tuned with few labels. “+ LLR” denotesadding large rotation as an auxiliary loss during pre-trainingMethod	Backbone	Label fraction			1%	10%Supervised baseline	ResNet-50	25.4	56.4SimCLR	ResNet-50	32.4	53.6SimCLR + LLR	ResNet-50	32.0	53.5Table 10: Transfer learning using ImageNet pre-trained weights. We report mean per-class accuracy(%) on the Flowers and Aircraft datasets, mean average precision (mAP) on the VOC2007 classifi-cation dataset, and Top-1 accuracy on the remaining datasets. “+ LLR” denotes adding large rotationas an auxiliary loss during pre-training	Flowers102	DTD	VOC2007	Aircraft	Food101	SUN397	CIFAR-10	CIFAR-100Baseline	92.0	64.8	67.3	-^859^^	86.9	53.6	95.9	80.2SimCLR	92.4	72.7	66.0	83.7	86.3	57.4	94.8	79.1SimCLR + LLR	93.1	73.0	64.4	85.8	86.3	57.0	96.2	82.0A.5 Pre-training for tabular datasetOutside of computer vision, contrastive learning pre-training has been shown effective in the tabulardata domain as well, especially for fine-tuning with limited data (Somepalli et al., 2021). To furthershow the effectiveness of meta-learning in this setting, we pre-train on tabular data with the R2D2head and evaluate on semi-supervised tasks given only 50 training samples. We apply our methods
Table 10: Transfer learning using ImageNet pre-trained weights. We report mean per-class accuracy(%) on the Flowers and Aircraft datasets, mean average precision (mAP) on the VOC2007 classifi-cation dataset, and Top-1 accuracy on the remaining datasets. “+ LLR” denotes adding large rotationas an auxiliary loss during pre-training	Flowers102	DTD	VOC2007	Aircraft	Food101	SUN397	CIFAR-10	CIFAR-100Baseline	92.0	64.8	67.3	-^859^^	86.9	53.6	95.9	80.2SimCLR	92.4	72.7	66.0	83.7	86.3	57.4	94.8	79.1SimCLR + LLR	93.1	73.0	64.4	85.8	86.3	57.0	96.2	82.0A.5 Pre-training for tabular datasetOutside of computer vision, contrastive learning pre-training has been shown effective in the tabulardata domain as well, especially for fine-tuning with limited data (Somepalli et al., 2021). To furthershow the effectiveness of meta-learning in this setting, we pre-train on tabular data with the R2D2head and evaluate on semi-supervised tasks given only 50 training samples. We apply our methodsto 5 datasets from OpenML (Vanschoren et al., 2014), and we average over 5 runs with random train-test split on each dataset. Details of the selected datasets can be found in Somepalli et al. (2021).
Table 11: Semi-supervised evaluation with 50 labeled training samples on 5 tabular datasets.
