Table 1: Three different allocations for the training data, where all the images are sampled fromGLDv2-clean. The “expansion” and “open-data” setups share the same old training set.
Table 2: The evaluation of compatibility in terms of mAP, i.e., 0% results in hot-refresh modelupgrades, where the new features of the queries need to be retrieved by the entire old gallery features.
Table 3: Performance of our method with different values of temperature τ in terms of mAP(↑). Theresults are reported on GLDv2-test with the architectures of R50-R50.
Table 4: Performance of our method with different values of loss weight λ in terms of mAP(↑). Theresults are reported on GLDv2-test with the architectures of R50-R50.
Table 5: Performance of our method with different batch size (b.s.) per GPU in terms of mAP(↑).
Table 6: Compare to the results when adopting a positive pre-sampler. The results are reported onGLDv2-test with the architectures of R50-R50, in terms of mAP(↑). Using the positive pre-samplerachieves slightly better performance at 0%, but lower training efficiency. Similar final performancescan be achieved with either the sampler or not, so we discard the positive sampler for more efficienttraining with large-scale datasets.
Table 7: Performance of our method with different temperature in Eq. (6) in terms of mAP(↑). Theresults are reported on GLDv2-test (Open-data) with the architectures of R50-R50. τ1 and τ2 denotethe temperature for new-to-old negative pairs and new-to-new negative pairs, respectively.
Table 8: Compare to the results when adopting a positive pre-sampler for experiments of triplet loss.
