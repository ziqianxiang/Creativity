Table 1: (Left) THSA initialized with pre-trained models. Comparison of accuracies obtained for languageinference task on MNLI and QNLI datasets, the higher the better. Results show that Tuformers can beconveniently initialized with pre-trained models for downstream tasks, and fine-tuning obtains impressiveperformance. (Right) THSA +kernels. Comparison of bits per dimension (BPD) for image generation task onMNIST and CIFAR-10 datasets, the lower the better. Results show that Tuformers can be extended to imagegeneration tasks and improve the performance of other efficient designs with linear computational and memorycomplexities.
Table 2: Performance in the flexible design space.
