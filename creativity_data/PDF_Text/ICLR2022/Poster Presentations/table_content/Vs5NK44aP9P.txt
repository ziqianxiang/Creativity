Table 2: E and memory reduction of sparse Transformer and ResNet-50 pruned by two differentpruning methods. When Ns is 0 or 1, inverting can be applied to a layer if unpruned weightsaccommodate more zeros than ones. Inverting has no effect for weights of signed INT8.
Table 3: Coefficient of variation of nu and E of two selected layers of the Transformer pruned byrandom, magnitude-based, or L0 regularization pruning method.
Table S.4: Coefficient of variation of nu and E of selected layers of the Transformer pruned by random, magnitude-based, LO regularization, or variational dropoutpruning method.
Table S.5: Coefficient of variation of nu and E of selected layers of the ResNet-50 pruned by random, magnitude-based, or variational dropout pruning method.
