Table 1: Cross-Validation for Hyperparameter Tuning: MAP after Procrustes for 10 language pairs.					β	#Iter=1	#Iter=2	#Iter=3	#Iter=4	#Iter=51	0.363	0.340	0.328	0.322	0.3172	0.385	0.386	0.386	0.386	0.3863	0.381	0.384	0.384	0.384	0.3844	0.381	0.382	0.382	0.382	0.3825	0.380	0.381	0.381	0.381	0.381The tie between the #Iter hyperparameter was broken using their performance on 13 English wordsimilarity benchmarks; details in Appendix D. In Table 2 reports the average Spearman rank coef-ficient score on the tasks (None means no normalization). (β, #Iter) = (2, 5) achieved the highestscore. So hereafter, we applying I-C+SN+L with the hyperparameter (β, #Iter) = (2, 5).
Table 2: Monolingual word similarity task; Average Spearman rank coefficientNone	(β, #Iter) = (2, 2)	(β, #Iter) = (2, 3)	(β, #Iter) = (2,4)	(β, #Iter) = (2,5)0.651	0.67077	0.67101	0.67108	0.67111Note that the proposed approach (I-C+SN+L) only increased this score for these similarity tasks,so showed no signs of distorting inherent information. Although Spectral Normalization does notexactly preserve the linear properties or angular properties (as centering and length normalization do,one each, respectively), it does not suffer ill effects. We hypothesize this is because it is somewhatuniformly stretching words along with the major modes of variation, and is effectively removinginformation not relevant to meaning, like frequency. This benign effect is in contrast to other spectraladjustments (removing small singular values, or setting all to the same value); see Appendix D.
Table 3: BLI performance (MAP) on aligning EN-XL and XL-ENNormalization	Methods: EN-XL				Methods : XL-EN				CCA	PROC	PROC-B	RCSLS	CCA	PROC	PROC-B	RCSLSNone	0.358	0.365	0.377	0.394	0.398	0.399	0.405	0.428PR	0.394	0.391	0.404	0.373	0.434	0.430	0.442	0.425GeoMediaN	0.393	0.391	0.400	0.379	0.433	0.432	0.440	0.429C+L	0.393	0.394	0.408	0.404	0.439	0.437	0.445	0.464I-C+L	0.394	0.395	0.410	0.406	0.439	0.438	0.448	0.460SN	0.391	0.394	0.408	0.405	0.440	0.438	0.451	0.468C+SN+L	0.395	0.396	0.413	0.407	0.444	0.444	0.458	0.466I-C+SN+L	0.396	0.398	0.414	0.406	0.445	0.446	0.461	0.466We also compute the average BLI MAP score across all 28 language pairs for more direct com-parison to prior work (GlaVas et al., 2019), summarized in Table 4 and Appendix E. All results arein Appendix L. We compare I-C+SN+L (denoted with SN) against no normalization on variousdictionary sizes: 1k, 3k and 5k source words and eValuated on 2k source test queries. In all cases, I-C+SN+L consistently improves over the baseline; see Appendix E. This includes improVement oVerRCSLS which is non-rigid, so in principle could “learn” adjustments similar to our normalizationin the process of alignment. We also tested on VECMAP, an unsuperVised approach; I-C+SN+Lpreprocessing also improVes this result from 0.375 to 0.410.
Table 4: Summary of BLI performance (MAP), aVerage scores for all 28 language pairs.
Table 5: BLI performance (MAP) on aligning Cross-lingual Contextual Embedding, EN-XLEmbedding	Normalization	EN-AR	EN-DE	EN-NL	AVgFastText	None	0.256	0.357	0.477	0.363Type-leVel	None	0.501	0.441	0.540	0.494FastText	I-C+L	0.284	0.372	0.493	0.383Type-leVel	I-C+L	0.510	0.449	0.543	0.501FastText	I-C+SN+L	0.280	0.375	0.499	0.385Type-leVel	I-C+SN+L	0.525	0.450	0.544	0.5067Published as a conference paper at ICLR 2022Normalizing contextual type-level embeddings. Table 5 compares the impact of Iterative Nor-malization and Spectral Normalization on the BLI performance on aligning Cross-lingual ContextualEmbedding. We follow the implementation details from Xu & Koehn (2021) for learning representa-tives within BERT (see Appendix C for more details). Then the learned normalization can be viewedas a composition to the functional embedding, so is compatible with downstream uses. Our pro-posed normalization algorithm, Iterative Spectral Normalization (I-C+SN+L), clearly outperformsIterative Normalization (I-C+L) on the BLI task for aligning Contextual Embeddings.
Table 6: CLDC performance (micro-averaged Fi scores). Cross-lingual transfer EN-XModel	Dict	EN-DE	EN-FR	EN-IT	EN-RU	EN-TR	AvgPROC	5k	.366	.258	.338	.288	.278	.306PROCI-C+L	5k	.452	.325	.427	.521	.479	.440PROCI-C+SN+L	5k	.436	.366	.427	.517	.511	.451PROC-B	3k	.364	.304	.299	.336	.317	.324PROC-BI-C+L	3k	.478	.341	.403	.527	.506	.451PROC-BI-C+SN+L	3k	.448	.396	.423	.522	.517	.461DLV	5k	.419	.336	.397	.493	.458	.421DLVI-C+L	5k	.434	.313	.377	.464	.489	.415DLVI-C+SN+L	5k	.433	.323	.406	.499	.472	.427RCSLS	5k	.466	.397	.403	.403	.406	.415RCSLSI-C+L	5k	.445	.514	.529	.443	.443	.474RCSLSI-C+SN+L	5k	.468	.500	.443	.488	.394	.459Cross-lingual Natural Language Inference (XNLI). We evaluated the CLWE on a cross-lingualnatural language inference (XNLI) task. We used a multi-lingual XNLI corpus created by Conneauet al. (2018b), which is a collection of sentence pairs from the English MultiNLI corpus (Williamset al., 2018) translated into 14 languages. The MultiNLI corpus contains 433k sentence pairs withthe labels entailment, contradiction, and neutral. The intersection between XNLI languages and BLIlanguages results in four XNLI evaluation pairs: EN-DE, EN-FR, EN-TR, and EN-RU. We use the
Table 7: XNLI performance (test set accuracy)Model	Dict	EN-DE	EN-FR	EN-TR	EN-RU	AvgPROC	5k	.607	.534	.568	.585	.574PROCI-C+L	5k	.589	.608	.536	.581	.579PROCI-C+SN+L	5k	.611	.638	.542	.596	.597PROC-B	3k	.615	.532	.573	.599	.580PROC-BI-C+L	5k	.602	.636	.537	.595	.593PROC-BI-C+SN+L	3k	.624	.638	.548	.601	.603RCSLS	5k	.390	.363	.387	.399	.385RCSLSI-C+L	5k	.514	.490	.490	.526	.505RCSLSI-C+SN+L	5k	.499	.482	.504	.556	.5105	Conclusion & DiscussionWe introduce a new way to normalize embeddings, based on spectral normalization, for use increating cross-lingual word embeddings. Our approach generalizes previous approaches, and whenused to individually preprocess monolingual embeddings, it allows alignment procedures to findbetter alignments: resulting in improved performance on direct translation tasks as well as cross-lingual topic classification and natural language inference tasks. Moreover, we demonstrate thisimprovement is very broadly useful; it holds in contextual embeddings as well as on embeddings ofnon-language data (on genomic data in Appendix G).
Table 8: EffectiVe RankNormalization AlgorithmsOUOLanguagesy+NS+o—IL+C-L+NaideMoeEN	268	277	278	279	279	283DE	264	273	273	274	274	278HI	258	270	269	269	269	282JA	39	96	171	253	255	276Table 9: EfeCtiVe Condition NUmberNormalization AlgorithmsLangauges	O S N	X d		U + U		EN	13.1	6.7	5.4	5.3	5.3	3.3DE	14.8	5.9	6.2	6.1	6.1	3.7HI	16.1	9.4	9.1	8.9	8.9	3.2
Table 9: EfeCtiVe Condition NUmberNormalization AlgorithmsLangauges	O S N	X d		U + U		EN	13.1	6.7	5.4	5.3	5.3	3.3DE	14.8	5.9	6.2	6.1	6.1	3.7HI	16.1	9.4	9.1	8.9	8.9	3.2JA	175.0	113.3	73.1	16.4	15.0	3.419Published as a conference paper at ICLR 2022Table 10: EffectiVe Condition NUmber Harmonic MeanNormalization AlgorithmsLangaUges PairsEN-DE	13.9	6.3	5.8	5.7	5.7	3.5EN-HI	14.4	7.8	6.8	6.6	6.6	3.2EN-JA	24.4	12.7	10.0	8.0	7.8	3.3Table 11: SingUlar ValUe GaPNormalization AlgorithmsLangauges Pairs	ο S N	X d		U + U		EN-DE	2.3	2.4	2.2	2.2	2.2	2.2EN-HI	49.0	48.6	12.2	12.2	12.2	5.2
Table 10: EffectiVe Condition NUmber Harmonic MeanNormalization AlgorithmsLangaUges PairsEN-DE	13.9	6.3	5.8	5.7	5.7	3.5EN-HI	14.4	7.8	6.8	6.6	6.6	3.2EN-JA	24.4	12.7	10.0	8.0	7.8	3.3Table 11: SingUlar ValUe GaPNormalization AlgorithmsLangauges Pairs	ο S N	X d		U + U		EN-DE	2.3	2.4	2.2	2.2	2.2	2.2EN-HI	49.0	48.6	12.2	12.2	12.2	5.2EN-JA	44.1	45.8	404.0	17.7	14.8	2.2C Normalizing contextual type-level embeddingsContextual Type-level Embeddings To obtain the contextUal tyPe-leVel embeddings, Fast Align(Dyer et al., 2013) is aPPlied to the soUrce-target Parallel corPora to deriVe silVer aligned token Pairs.
Table 11: SingUlar ValUe GaPNormalization AlgorithmsLangauges Pairs	ο S N	X d		U + U		EN-DE	2.3	2.4	2.2	2.2	2.2	2.2EN-HI	49.0	48.6	12.2	12.2	12.2	5.2EN-JA	44.1	45.8	404.0	17.7	14.8	2.2C Normalizing contextual type-level embeddingsContextual Type-level Embeddings To obtain the contextUal tyPe-leVel embeddings, Fast Align(Dyer et al., 2013) is aPPlied to the soUrce-target Parallel corPora to deriVe silVer aligned token Pairs.
Table 12: Monolingual Word Similarity Score (Spearman rank coefficient)Normalization AlgorithmsDataset	None	ssv	SSVZ	I-C+L	I-C+SN+LENWS-353-ALL	0.7388	0.7127	0.5395	0.7433	0.7555ENVERB-143	0.3973	0.4283	0.2635	0.4231	0.4346ENYP-130	0.5333	0.5534	0.3904	0.5514	0.5631ENJMTUrk-771	0.6689	0.6583	0.5540	0.6838	0.6926ENRG-65	0.7974	0.7640	0.6390	0.8082	0.8087Enrw-Stanford	0.5080	0.5569	0.3873	0.5125	0.5258EN_SEMEVAL17	0.7216	0.7478	0.5779	0.7288	0.7366ENJMEN-TR-3k	0.7637	0.7506	0.6581	0.7720	0.7792EN_WS-353-SIM	0.7811	0.7678	0.6162	0.7897	0.7888ENJMTUrk-287	0.6773	0.6439	0.6016	0.6864	0.6864ENWS-353-REL	0.6820	0.6363	0.4824	0.6905	0.7081ENJMC-30	0.8123	0.8203	0.6754	0.8352	0.8494EN_SIMLEX-999	0.3823	0.4069	0.2276	0.3899	0.3955Avg	0.6511	0.6498	0.5087	0.6627	0.6711E S ummary of Model Performance and S ignificance TestTable 13: Summary of Model Performance on I-C+SN+L vs. No Normalization. Where a significantnumber of language pairs show an improvement (see Table 14) are in bold.
Table 13: Summary of Model Performance on I-C+SN+L vs. No Normalization. Where a significantnumber of language pairs show an improvement (see Table 14) are in bold.
Table 14: This table shows the p-values corresponding with a 1-tail Binomial Test using n = 28items, k observations, against a rate parameter of 0.5, indicating a null hypothesis where each sce-nario is equally likely (i.e., that neither provides an improvement).
Table 15: Comparison of Model Performance of I-C+L vs. I-C+SN+L. The table shows the fractionof language pairs where I-C+SN+L performs better. Where a significant number of language pairsshow an improvement (See Table 14) are in bold.
Table 16: BLI performance (P@1) on 4 languages from MUSE. * are scores reported from publishedpapers; others are ones we computed.
Table 17: Alignment Performance (FOSCTTM) on Single-cell Sequencing DatasetAlignment methods	Normalization	ScGEM	SNAREseq		FOSCTTM	FOSCTTMBefore Normalization			MMD-MA *	None	0.2014	0.15UnionCom *	None	0.296	0.265SCOT *	None	0.198	0.15After Normalization			SCOT	I-C+L	0.201	0.395SCOT	I-C+SN+L	0.184	0.15Single-cell measurements allow scientists to study the various properties of the genome such asgene expression, chromatin accessibility, DNA methylation etc. Therefore there is the need for dataintegration of these single-cell measurements so that scientists can understand cell developmentover time and disease. However, due to the lack of cell-to-cell correspondence between the differenttypes of measurements, it makes the process of data integration a challenging task. Motivated bythis, (Demetci et al., 2020) proposed a Single-Cell alignment using Optimal Transport (SCOT), anunsupervised learning algorithm that uses Gromov Wasserstein-based optimal transport to recoverthe cell-to-cell correspondence between two sequencing domains. * represents results taken directly23Published as a conference paper at ICLR 2022
Table 18: BLI performance (MAP) on aligning EN-XL2 . We compare all the normalization techniques: None (No normalization), PR (PCA Removal) (Mu & Viswanath,2018), GeoMediaN (Geometric Median Normalization), C+L (Mean centering and Length normalization, 1 round), I-C+L (Iterative Mean centering and Length normalization, 5rounds) (Zhang et al., 2019), SN (Spectral Normalization, 1 round), C+SN+L (Mean centering, Spectral Normalization and Length normalization, 1 round), I-C+SN+L (IterativeMean centering, Spectral Normalization and Length normalization, 5 rounds).
Table 19: BLI performance (MAP) on aligning XL1 -EN. We compare all the normalization techniques: None (No normalization), PR (PCA Removal) (Mu & Viswanath,2018), GeoMediaN (Geometric Median Normalization), C+L (Mean centering and Length normalization, 1 round), I-C+L (Iterative Mean centering and Length normal-ization, 5 rounds) (Zhang et al., 2019), SN (Spectral Normalization, 1 round), C+SN+L (Mean centering, Spectral Normalization and Length normalization, 1 round),I-C+SN+L (Iterative Mean centering, Spectral Normalization and Length normalization, 5 rounds).
Table 20: BLI performance (MAP) for the first batch (14) of language pairs. We compared the Baseline result from ( lavas et a , 201 ) to I-C+SN+L (denoted SN) andI-C+L result on the BLI task.
Table 21: BLI performance (MAP) for second batch (14) of language pairs. We compared the Baseline result from ( lavas et al , 201 ) to I-C+SN+L (denoted SN) andI-C+L result on the BLI task.
Table 22: BLI performance (MAP) for the first batch (14) of language pairs. We compared the Baseline result from ( Glavas et a , 019) to I-C+SN+L (denoted SN) resulton the BLI task.
Table 23: BLI performance (MAP) for second batch (14) of language pairs. We compared the Baseline result from (Glavas et al., 2019) to I-C+SN+L (denoted SN) resulton the BLI task.___________________________________________________________________________________________________________________________________Model	Dict	FI-HR	FI-IT	FI-RU	HR-FR	HR-IT	HR-RU	IT-FR	RU-FR	RU-IT	TR-FI	TR-FR	TR-HR	TR-IT	TR-RU	AvgCCA	1k	0.167	0.232	0.214	0.238	0.240	0.256	0.612	0.344	0.352	0.151	0.213	0.134	0.202	0.146	0.250CCASN	1k	0.193	0.257	0.236	0.273	0.265	0.274	0.638	0.380	0.379	0.157	0.236	0.148	0.227	0.164	0.273CCA	3k	0.264	0.328	0.306	0.346	0.345	0.348	0.659	0.452	0.449	0.232	0.308	0.211	0.309	0.252	0.343CCASN	3k	0.289	0.359	0.331	0.375	0.377	0.366	0.672	0.476	0.469	0.257	0.332	0.240	0.329	0.269	0.367CCA	5k	0.288	0.353	0.340	0.372	0.366	0.367	0.668	0.469	0.474	0.260	0.337	0.250	0.331	0.285	0.369CCASN	5k	0.311	0.384	0.362	0.403	0.393	0.389	0.681	0.491	0.492	0.284	0.364	0.269	0.357	0.299	0.391PROC	1k	0.187	0.247	0.233	0.248	0.247	0.269	0.615	0.352	0.360	0.169	0.215	0.148	0.211	0.168	0.262PROCSN	1k	0.217	0.271	0.252	0.285	0.276	0.285	0.641	0.387	0.391	0.178	0.243	0.166	0.239	0.182	0.287PROC	3k	0.269	0.328	0.310	0.346	0.350	0.353	0.659	0.455	0.455	0.241	0.312	0.219	0.312	0.262	0.348PROCSN	3k	0.296	0.365	0.337	0.381	0.384	0.371	0.671	0.474	0.472	0.262	0.336	0.248	0.336	0.279	0.372PROC	5k	0.294	0.355	0.342	0.374	0.364	0.372	0.669	0.470	0.474	0.269	0.338	0.259	0.335	0.290	0.372PROCSN	5k	0.316	0.385	0.364	0.407	0.396	0.393	0.679	0.491	0.495	0.290	0.368	0.275	0.360	0.305	0.395PROC-B	1k	0.263	0.328	0.315	0.335	0.343	0.348	0.665	0.467	0.466	0.247	0.305	0.210	0.298	0.230	0.344PROC-BSN	1k	0.296	0.365	0.337	0.408	0.392	0.371	0.678	0.486	0.483	0.280	0.357	0.255	0.346	0.246	0.379PROC-B	3k	0.293	0.348	0.327	0.365	0.368	0.365	0.664	0.478	0.476	0.270	0.333	0.244	0.330	0.262	0.366PROC-BSN	3k	0.303	0.374	0.337	0.403	0.399	0.377	0.678	0.488	0.491	0.286	0.360	0.267	0.356	0.264	0.384DLV	1k	0.184	0.244	0.225	0.214	0.245	0.264	0.585	0.320	0.358	0.161	0.194	0.144	0.209	0.161	0.251
