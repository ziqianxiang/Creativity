Table 1: Results in negative log probability on one million test samples for the 2d toy problems. Wereport mean and standard error of the mean over five different runs. 8G stands for eight gaussianswhile CKB for checkerboard. The resulting negative log probability for MNIST is computed on thetest set.
Table 2: Results in negative log probability on one million test samples for the time-series toyproblems, and on a test set of 10000 datapoints. We report mean and standard error of the mean overfive different runs.
Table 3: Results in negative ELBO. We report mean and standard error of the mean over ten differentruns. S and B correspond respectively to smoothing and bridge, while c stands classification. Asan example, BRB-c is the Brownian motion with the bridge-classification settings. For the binarytree experiments, Lin and Tanh correspond to the link function (Lin is linear link), and the followingnumber is the depth of the tree.
Table 4: Number of trainable parameters per model for the generative time series experiments.
Table 5:	Number of trainable parameters per model for the generative hierarchical experiments.
Table 6:	Number of trainable parameters per model for the 2D toy and MNIST experiments.
Table 7: Sample time in seconds for the generative time series experiments. We report mean andstandard deviation over ten samples with batch 100.
Table 8: Sample time in seconds for the generative hierarchical experiments. We report mean andstandard deviation over ten samples with batch 100.
Table 9: Sample time in seconds for the 2D toy and MNIST experiments. We report mean andstandard deviation over ten samples with batch 100. We use batch 10 for MNIST as the numericinversion needs a bigger amount of GPU memory.
Table 10:	Results in negative log probability on one hundred thousand test samples for the IRISdataset and over ten thousand test samples for the Digits dataset. We report mean and standard errorof the mean over five different runs.
Table 11:	Results in negative log probability on one million test samples for the time-series toyproblems, and on a test set of n datapoints. We report mean and standard error of the mean over fivedifferent runs.
Table 12: Results in negative ELBO. We report mean and standard error of the mean over ten differentruns	GEMF-T	MF	MVN	ASVI	IAFBRS-r	-3.341 ± 0.977	0.147 ± 1.002	-3.126 ± 0.977	-3.354 ± 0.974	-3.304 ± 0.974BRB-r	-3.122 ± 0.967	1.932 ± 0.951	-2.866 ± 0.967	-3.142 ± 0.966	-3.086 ± 0.961LZS-r	46.981 ± 0.476	1492.572 ± 255.358	1485.622 ± 251.296	1257.942 ± 330.467	1337.364 ± 301.002LZB-r	110.500 ± 40.958	756.338 ± 146.012	748.298 ± 146.063	490.696 ± 166.080	598.402 ± 153.412VDPS-r	93.520 ± 2.729	174.445 ± 2.846	194.592 ± 2.181	93.696 ± 3.283	117.028 ± 2.666VDPB-r	68.030 ± 1.315	150.184 ± 1.445	171.263 ± 1.359	72.401 ± 3.076	92.739 ± 1.249Lin-4	1.513 ± 0.576	6.136 ± 0.573	1.542 ± 0.571	3.268 ± 0.561	1.526 ± 0.581Tanh-4	-0.035 ± 0.119	6.640 ± 0.157	0.075 ± 0.124	2.866 ± 0.113	-0.026 ± 0.118Table 13: Results in forward KL divergence. We report mean and standard error of the mean over tendifferent runs.
Table 13: Results in forward KL divergence. We report mean and standard error of the mean over tendifferent runs.
Table 14: Results in variational inference for additional models: a non gated EMF-T, and thecombination of Mean Field and Multivariate Normal with EMF-T and GEMF-T.
