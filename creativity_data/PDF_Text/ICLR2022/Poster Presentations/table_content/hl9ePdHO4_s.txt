Table 1: Results (mean ± standard deviation) for parameter-normalized models run on 5 datasets.
Table 2: Activating the combination weightings wharms performance. Run on ZINC; lower is better.
Table 3: EGC can be applied applied to largescale heterogeneous graphs, outperforming R-GCN by 0.9%.
Table 4: Memory and latency statistics forparameter-normalized models (used in ta-ble 1) on Arxiv. Note that EGC-M memoryconsumption and latency can be reduced withaggregator fusion at inference time.
Table 5: Propagation rules for general-purpose GNN architectures we compare against in this work;rules are provided using node-wise formulations. We evaluate against popular architectures, and arecent proposal that has achieved state-of-the-art performance, PNA.
Table 6: Possible aggregators tried for EGC-M. Up to 3 combinations (from a possible 35) weretried, as we limited ourselves to always using 3 aggregators. Our conclusions do not change, and itis likely that better results can be found with more optimal aggregator choices.
Table 7: Results of applying graph-augmented MLPs (GA-MLPs) to tasks requiring generalizationto unseen graphs. We see that the performance is broadly similar to the corresponding GCN model—and far weaker than EGC-S, which uses the same aggregator. We also considered using the addaggregation on ZINC, and achieved 0.444 ± 0.019. By comparison, the equivalent GIN model(which uses the add aggregation) achieved 0.387 ± 0.015.
Table 8: Results of applying sampling approaches to EGC-S. We do not enable sampling at testtime—hence these approaches do not offer any test time reductions to computation. Sampling, inprinciple, is applicable to any underlying GNN: our conclusions will transfer to other underlyingGNNs. We see that DropEdge (Rong et al., 2020) with a low drop probability (p = 0.1) can aidmodel performance; however, setting p this low does not significantly reduce memory consumptionor computation. Increasing p to 0.5 does reduce resource consumption noticeably, but results innoticeable degradation to model performance.
Table 9: Inference latency (mean and standard deviation) for CSR SpMM, used by GCN/GIN,and aggregator fusion. Assuming a feature dimension of 256 and H = B = 1 per Algorithm 1.
Table 10: Latency and memory results for parameter-normalized models on OGB Code-V2. Despite|E|having a lower 胃 ratio of 2.75 relative to Arxiv (13.67), We see that the trends We observed m forArxiv broadly remain. It is worth noting again that EGC-M is far more efficient both latency andmemory-Wise than PNA.
Table 11: Latency and memory statistics for accuracy-normalized models on Arxiv. To achieve thesame accuracy as EGC-S, we must boost the size of the baseline models. We observe that EGC-Soffers noticeable reductions to both memory consumption and latency for a given accuracy level.
