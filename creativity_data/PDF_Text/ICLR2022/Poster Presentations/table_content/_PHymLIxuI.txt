Table 1: Variants of CrossFormer for image classification. The example input size is 224 × 224. Srepresents the feature maps’ height (and width) of each stage. D and H mean embedding dimensionsand the number of heads in the multi-head self-attention module, respectively. G and I are groupsize and interval for SDA and LDA, respectively.
Table 2: Results on the ImageNet validation set. The input size is 224 × 224 for most models, whileis 384 X 384 for the model with a L Results of other architectures are drawn from original papers.
Table 3: Object detection results on COCO 2017 val set with RetinaNets as detectors. Results forSwin are drawn from Twins as Swin does not report results on RetinaNet. Results in blue fonts arethe second-placed ones. CrossFormers with ^ use different group sizes from classification models.
Table 4: Object detection and instance segmentation results on COCO val 2017 with Mask R-CNNsas detectors. APb and APm are box average precision and mask average precision, respectively.
Table 5: Semantic segmentation results on the ADE20K validation set. “MS IOU” means testingwith variable input size.
Table 6: Results on the ImageNet validation set. The baseline model is CrossFormer-S (82.5%). Wetest with different kernel sizes of CELs.
Table 7: Experimental results of ablation studies.
Table 8: CrossFormer-based backbones for object detection and semantic/instance segmentation.
Table 9: Object detection results on COCO val 2017. “Memory” means the allocated memoryper GPU reported by torch.cuda.max_memory_allocated(). ^ indicates that models use different(G, I) from classification models.
Table 10: Semantic segmentation results on ADE20K validation set with semantic FPN or UPerNetas heads.
Table 11: Classification results on ImageNet dataset after plugging CEL into other vision transform-ers.
