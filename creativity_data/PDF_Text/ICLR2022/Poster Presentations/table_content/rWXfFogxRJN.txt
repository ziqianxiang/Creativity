Table 1: Test error (%) when training ResNet-50 with different augmentation methods.
Table 2: Test error (%) when fine-tuning pretrained ResNet-50 with different augmentation methods.
Table 3: Test error (%) on Reduced CIFAR-10, CIFAR-10, CIFAR-100, reduced SVHN and theSVHN core set (*evaluated using the policy released by the baseline).
Table 4: Test error rate (%) on AdaAug-transfer and AdaAug-direct with different search settings.
Table 5: Test error rates (%) on ImageNet using ResNet-50.
Table 6: Test error rates (%) of Shake-Shake (26 2x96d) on Reduced CIFAR-10 and reduced SVHNI Simple		AutoAugment	PBA	AdaAugReduced CIFAR-10 Shake-Shake (26 2x96d)	17.05	10.04	10.64	10.92 ± 0.07Reduced SVHN Shake-Shake (26 2x96d)	13.32	5.92	6.46	6.44 ± 0.17A.2 Additional Ablation S tudyTo clarify the improvements of AdaAug, we compare the performance of AdaAug under differentsettings in Table 7: Simple: standard data augmentation is applied; Random: AdaAug is applied withrandomly initialized hγ while keeping the diversity parameters the same; AdaAug (w/o diversity):AdaAug is applied without the diversity parameters; AdaAug: AdaAug is applied with learned hγand the diversity parameters.
Table 7: Test error rate (%) on AdaAug-transfer and AdaAug-direct with different augmentationconfigurations.
Table 8: Test-set error rate (%) on AdaAug-transfer on Flower dataset with different values of δ andT.
Table 9: GPU hours needed to find the augmentation policy using AA, PBA, Fast AA, DADA, FasterAA, and AdaAug.
