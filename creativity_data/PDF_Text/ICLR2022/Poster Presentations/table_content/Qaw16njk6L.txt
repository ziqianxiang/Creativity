Table 1: An illustration of our search space. MBConv refers to the inverted residual block (Sandleret al., 2018). MBPool denotes the efficient last stage (Howard et al., 2019). SE represents the squeezeand excite layer (Hu et al., 2018). Transformer stands for the transformer blocks (Vaswani et al.,2017). For MBConv blocks, the expansion ratio refers to the expansion ratio of the depth-wiseconvolution layer. For transformer layers, it refers to the MLP expansion ratio. For each transformerblock, we use 3 × 3 depth-wise convolution with stride 2 for down-sampling and the down-samplinglayer is placed as the first layer for that block.
Table 2: ImageNet top-1 accuracy from sub-networks trained from scratch vs. results fromsub-networks sampled from the supernet.
Table 3: ImageNet Top-1 accuracy from thesmallest and the largest sub-network by usingdifferent training recipes.
Table 4: An estimation of negative cosine similarity ratio (gradient conflict ratio) between thesupernet gradient and the averaged gradient of the sub-networks.
Table 5: An illustration of our simplified training settings, where n is the number of augmentationtransformations and m the number of magnitudes in RandAugment. A typical setting of RandAug-ment is n=2 and m=9 for training a single network; see Cubuk et al. (2020); Liu et al. (2021).
Table 6: Comparison with prior art efficient CNNs and ViTs on ImageNet. The reported AlphaNetmodels are trained with an external teacher model. The “*” indicates that the ViTs are trained withoutexternal teacher models.
Table 7: Ablation study results on ImageNet. We show the top-1 validation accuracy of the smallestand largest sub-network, and the negative cosine similarity ratio for each case. Note that switchablescaling layer is applied on top of Weak DA & Reg; and Prioritize (sub) is applied on top of both WeakDA & reg and switchable scaling layer.
Table 8: Improving CNN-based supernets on ImageNet. AlphaNet (w/ KL) and AlphaNet (w/ α-div)denote AlphaNets trained with KL and α-divergence based knowledge distillation, respectively. A0 toA6 are the architectures reported in AlphaNet (Wang et al., 2021a). Note that the AlphaNet supernetshere are trained without external teacher models.
Table 9: ImageNet top-1 accuracy with different types of self-attention mechanisms.
Table 10: Ablation studies on where toswitch to transformer blocks.
Table 11: Ablation studies on the impact ofhead dimension.
Table 12: NASViT models searched with for better latency vs. accuracy trade-off.
Table 13: Here, ‘c’ denotes the number of output channels, ‘d’ denotes number of layers, ‘ks’ denoteskernel size, ‘e’ denotes expansion ratio, ‘k’ denotes number of windows, ‘s’ denotes stride.
