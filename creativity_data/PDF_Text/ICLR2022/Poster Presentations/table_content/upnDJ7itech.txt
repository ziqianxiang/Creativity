Table 1: Benchmark results on six diverse knowledge-intensive tasks. Compared with beam search(Beam) and sampling decoding (Sample), KID decoding improves the generation quality in general(by at most 15%). For each LM, we report their performance in zero-shot setting (*), and that ofbeing fine-tuned (FT). We color (	■) those results of KID that achieve > 5% improvement overthe next-best performance. We also annotate the performance reported by published state-of-the-artmodels to date (- means missing official reports).
Table 2: Performance of six related works on Wiki-answerable ELI5 and MSMARCO, and out-of-domain PIQA and PubMedQA QA tasks. We report ROUGE-L score with 10% and 100% of thetraining data. As a model-agnostic method, KID shows particularly strong performance in few-shotscenarios, which can better help LMs transfer to new domain with minimum training data.
Table 3: A closer comparison of BART-L with KID andRAG (Lewis et al., 2020b) which also leverages retrievedWikipedia passages as knowledge. We switch between differ-ent retrievers to study its impact on retrieval accuracy (Prec@1),generation quality (R-L), and knowledge coverage (Cov).
Table 4: The performance (R-L) onELI5 of LMs with different sizes(similar architecture). Vanilla LMs(*) benefit more with KID than thefine-tuned ones (FT). (The absolutegain over the next-best is annotated.)ELI5Model	Beam	Sample	KIDGPT2-M*	16.0	16.1	10Ξ∕GPT2-MFT	24.6	25.4	26.67.2GPT3-1.3B*	21.7	22.0	壬工GPT3-1.3BFT	24.9	25.5	26.6^.1GPT3-2.7B*	22.8	24.6	How much knowledge do we need? We also study the impact of number of documents (k) weretrieve and number of hops (hmax) we use to query the knowledge trie, two factors that determinehow much knowledge we use to ground the generation. As shown in Figure 2 (a) and (b), for theexample task ELI5, we find the generation performance measured by ROUGE-L does not benefit fromsimply more retrieved documents—an optimum k is 5 through empirical observation, and similarly,hmax = 4 brings best performance. A larger k might risk retrieving less relevant Wiki documentsand a larger hop hmax with deeper traverse through the knowledge trie tends to bring in off-topic
Table 5: Human assessments of generation in terms of Relevance, Factuality, and Grammaticalityon a 7-point Likert scale. We run paired sample t-test comparing human references (Gold) withbeam search (BM) with beam size 5, sampling (SP) with top p = 0.9 and k = 20, reflective (RFLC)decoding, and our KID generation. p value describes the significance of difference from Gold. (*corresponds to p-value< 0.05 and ** to 0.01.)																		ELI5					aNLG					WoW						Gold	BM	SP	RFLC KID		Gold	BM	SP	RFLC KID		Gold	BM	SP	RFLC KID		Mean	5.14	4.51	4.97	4.73	5.07	542	5.22	5.10	5.27	5.36	4.62	4.68	4.44	4.35	4.57Relevance	p-value	-	.00**	.10	.03*	.30	-	.14	.02*	.17	.23	-	.12	.10	.06	.30Factuality	Mean	4.95	4.37	4.61	4.23	4.87	5.35	5.17	5.19	5.25	5.30	4.72	4.20	4.38	4.41	4.53	p-value	-	.14	.00**	.00**	.24	-	.05	.06	.30	.41	-	.00**	.06*	.15	.29Fluency	Mean	5.66	5.52	5.54	5.07.	5.50	5.40	5.23	5.34	4.97	5.27	4.53	4.48	4.40	4.14	4.33	p-value	-	.09	.11	.02*	.07	-	.15	.20	.04*	.16	-	.18	.10	.05	.08their ratings deteriorate as the length grows. Reflective decoding exhibits similar trend since it stillsrelies on MLE training. KID, instead, dynamically infuses global knowledge with LM predictions ateach step and thus can mitigate exposure bias by imitating non-local demonstrations.
Table A1: The dataset statistics of the eight knowledge-intensive NLG tasks we evaluate for KID.								Split	ELI5	MSMARCO	PIQA	PubMedQA	ROC	αNLG	WoW	MuTualTrain	272,764	153,725	16,115	800	52,665	50,481	63,734	7,088Dev	1,507	12,467	1,838	100	1,571	7,252	3,054	886Test	600	12,467	3,000	100	4,081	14,313	2,944	886A.2 Details on Knowledge Trie Construction and QueryA.2.1 Knowledge Trie ConstructionIn this section, we detail the process of constructing the knowledge trie and how we query for theknowledge in a dynamic fashion. In Figure A1 (a), for a given question (say from the ELI5 dataset),the DPR retriever (Karpukhin et al., 2020) would retrieve k documents (the effect of choosingdifferent k on performance has been discussed in §4.3; we use k = 3 here for simplicity) from 21M100-token Wiki documents as the grounding passages for the question. We then use co-referenceresolution to replace the pronouns with their referents (colored in red), normalize the text (e.g.,removing links, lower-casing, etc.), and pass them through the OpenIE (Stanovsky et al., 2018) toobtain knowledge triplets (the use of OpenIE can be also seen in related prior work (Trisedya et al.,2019; Wu et al., 2019; Fan et al., 2019a)). The end nodes of the extracted triplets (i.e., the subj andobj) serve as the key-value pairs when they are stored in the external knowledge trie (Gext), and therelations between nodes are translated to the edges. We use the stems of the tokens as the keys inGext (e.g., “driving” → “drive”), in order to compress duplicated nodes in a concept-centric manner.
Table A2: Compare KID with additional baselines. Note that only FiD (Izacard & Grave, 2021b;a)is explicitly optimized for knowledge awareness. Diverse Decoding (Baheti et al., 2018) aims toimprove generation diversity by constraining decoding distribution with topic and semantic similarity.
Table A3: Time complexity analysis of KID and other baselines. We study the time consumptionduring knowledge retrieving, model training, and inference (i.e., generation). KID has comparabletime efficiency as RAG and FiD, but outperforms them in knowledge infusing (discussed in the §4.2).
Table A4: Memory footprint of KID and other baselines. Similar to RAG and FiD, KID requirespre-store the grounding documents on disk (and corresponding dense vectors). In addition, KIDbuilds a knowledge trie off-line (less than 10Mb for all the tasks we studied), and a local knowledgememory (a limited length FIFO list), to enable knowledge infusion during generation.
Table A5: Sample generation of KID and RAG on ELI5 (QA), ROC (Logic-centric Writing), andWoW (Dialogue) with fine-tuned language models.
