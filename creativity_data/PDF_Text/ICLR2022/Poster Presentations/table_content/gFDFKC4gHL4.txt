Table 1: Required sample size to reach 1% Frobnius norm error. Here we compare MASA withuniform (U) sampling and stratified (S) sampling. U and S required very similar sample sizes andare reported in the same column. The sample size is obtained when a 1% Frobenius norm error isachieved With Probability 95%.______________________________________________________________________API;Dataset	Sample size		Save	API;Dataset	Sample size		Save	MASA	U/S			MASA	U/S	Amazon;YELP	4.5K	19.7K	77%	IBM;DIGrT	3.6K	17.0K	79%Amazon;IMDB	10.3K	20.8K	51%	IBM;AMNIST	2.4K	18.5K	87%Amazon;WAIMAI	7.8K	18.0K	57%	GoogleQIGIT	4.2K	17.0K	75%Amazon;SHOP	4.8K	20.8K	77%	Google;AMNIST	1.1K	18.5K	94%MS; FER+	2.6K	19.9K	87%	Google;CMD	1.6K	15.2K	89%Google; EXPW	4.2K	17.9K	77%	MS;DIGrr	3.3K	17.0K	81%we measure the number of samples needed to reach 1% Frobenius norm error with probability 95%,via an uPPer bound on the estimated Frobenius error. The details are left to APPendix C. As shoWn inTable 1, MASA usually requires more than 70% feWer samPles to reach such tolerable Frobeniusnorm error than the uniform and stratified samPling, Primarily due to its shift estimation is moreaccurate. Uniform and stratified samPling required the similar number of samPles because the uPPerbounds on their estimated Frobenius error are similar.
Table 2: Dataset statistics.
Table 3: ML services used for each task. Price unit: USD/10,000 queries. We consider three tasks,sentiment analysis (SA), facial emotion recognition (FER), and spoken command recognition (SCR)Tasks	ML service	Price	ML service	Price	ML service	PriceSA	Google NLP (GoN)	2.5	AMZN ComP (Ama)	0.75	Baidu NLP (Bai)	3.5FER	Google Vision (Goo, a)	15	MS Face (Mic, a)	-i0-	Face++ (Fac)	-5-SCR	Google Speech (Goo, b)	60	MS Speech (Mic, b)	^^1	IBM Speech (IBM)	25ML APIs and Dataset Statistics. We focus on three common classification tasks, namely, sentimentanalysis, facial emotion recognition, and spoken command recognition. For each of the tasks, weevaluated three APIsâ€™ performance in spring 2020 and spring 2021, respectively, for four datasets.
