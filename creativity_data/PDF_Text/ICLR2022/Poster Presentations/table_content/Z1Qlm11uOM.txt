Table 1: WER (%) of our models and prior work on the LRS3 dataset. fWe re-implemented Maet al. (2021a) with the same architecture since the author source code was not provided.
Table 2: Fine-tuning performance (in WER, %) of AV-HuBERT and visual HuBERT on differenttarget labels. Init: feature in the initial iteration, sub: feature in subsequent iterations. AV: AV-HuBERT, V: Visual-HuBERT, A: Audio-HuBERT.
Table 3: WER (%) with different amounts of unlabeled English utterances in pre-training. Non-Endata: 1,116 hours. Labeled data for fine-tuning: 30 hours.
Table 4: ASR WER (%) of audio-HuBERT pre-trained with audio-only/audio-visual clusters andtheir comparison to prior work on the LRS3 dataset.
Table C.1: WER (%) in using different amount of labeled data for fine-tuning (BASE, 433 hoursunlabeled)Labeled (hrs)	Unlabeled (hrs)	Criterion	LM	WER w/o pretrain	(%) w/ pretrain1	433	CTC S2S	4-gram -	98.6 98.9	68.8 92.010	433	CTC S2S	4-gram -	90.8 97.6	57.6 63.1100	433	CTC S2S	4-gram -	77.8 84.3	54.2 48.1C.2 Performance on Seen SpeakersThe current LRS3 benchmark is under the open-speaker setting, where the speaker identities intraining and test set do not overlap. To test the lip reading performance for a fixed set of speakerswhich is the case for an early versions of LRS3 used before October 2018, we randomly choose5 groups of utterances from the trainval partition of LRS3 as test set and repeat experiments foreach group independently. Each group contains 1322 utterance, which is of the same amount asthe original test set. The model we compare is the AV-HUBERT LARGE pre-trained with 1,759unlabeled data. As is shown in table C.2, the average WER achieved by our model for seen speakersis 18.0 ± 0.5%, which is significantly lower than the WER for unseen speakers (30.5%) under theopen-speaker setting.
Table C.2: WER (%) under closed-speaker setting for5 randomly sampled test sets and their average	Test set (seen speakers) 12345	AVGWER (%)	17.4	18.6	18.5	17.5	18.3	18.0 ± 0.5C.3 Performance of self-training onlyTable C.3 shows the performance of only applying self-training. The WER of a self-training onlymodel is significantly higher than AV-HuBERT and self-trained AV-HuBERT, which suggests thatthe gain of the combined approach is primarily from AV-HuBERT.
Table C.3: Comparison of WER (%) among model trained from scratch, self-training only, AV-HuBERT only and self-trained AV-HuBERT. All models are Transformer-LARGE.
Table C.4: WER (%) of our models and the comparison with prior works on LRS3-TED dataset.
Table D.1: Ablation study for hyper-parameters. The ablations are done in the last iteration of AV-HUBERT. ma/mv : the probability of an acoustic/image frame being masked.
Table E.1: Quality of different cluster assignments. Each number is in the format of Purity (NMI).
Table E.2: Impact of masking strategy on quality of cluster assignments (purity/NMI: quality ofcluster assignments used to train the model)Feature	K	Purity (%)	NMI (%)MFCC	100	30.3	21.5AV∕MFCC→AV (it1, L9) w∕ Feature Masking	100	47.3	37.7AV∕MFCC→AV (it1, L9) w∕ Input Masking	100	34.5	27.2E.3 Clustering Quality Across LayersFigure E.1 shows the clustering quality of features of different layers in different iterations. Thecluster assignment quality generally improves with more iterations. In the first iteration, features in21Published as a conference paper at ICLR 2022the middle layers show higher quality than the other layers. The target for the first iteration (MFCCclusters) is of worse quality, thus later layers that are more correlated with targets do not yield thebest cluster. Target quality improves with more training iterations, thus the best feature layer shiftstowards the end. Setting a larger number of clusters increases the clustering quality as can be seenfrom the comparison between ”varied clusters” and ”2K clusters”. In terms of the 12th layer whichwe choose, the highest NMI (44.2%) is achieved in the last iteration. In addition, more iterations oftraining improves the overall quality of clusters produced by a model though the highest NMI/purityamong all layers does not necessarily increase in later iterations. Therefore, setting a larger numberof iterations brings stable gains which are more robust to the index of layer chosen for clustering.
