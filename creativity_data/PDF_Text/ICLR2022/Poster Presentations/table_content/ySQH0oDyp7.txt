Table 1: 2-bit or 3-bit post-training quanti-zation accuracy on ImageNet dataset acrossdifferent cases and different models.
Table 2: Effect of QDROP.
Table 3: Comparison among different post-training quantization strategies with low-bit activation in terms ofaccuracy on ImageNet. * represents for our implementation according to open-source codes and f means usingBrecq’s first and last layer 8-bit setting, which also keeps first layer’s output 8-bit besides input and weight inthe first and last layer.
Table 4: Comparison among typical post-training quantization strategies in terms of mAP on MS COCO. Notethat refer to Brecq, we didn’t quantize head and keep the first and last layer in backbone to 8-bit. Othernotations align with Table 3.
Table 5: Performance on NLP tasks compared to other methods on E8W4A4. Here, we use symbol EeWwAato additionally express the embedding bit and conduct experiments on GLUE and SQuAD1.1.
Table 6: Cross domain data.
Table 7: Ablation study of quantization settings onResNet-18 W2A2.
Table 8: Train and test accuracy on ResNet-18W2A2.
Table 9: Hessian information of the ResNet-18 W3A3model. λ1 represents for the top-1 Hessian eigenvalue, λ5for top-5 Hessian eigenvalues and Tr for Hessian trace.
Table 10: Experiments between ResNet-18 andSWA20 when adopting different PTQ methods.
