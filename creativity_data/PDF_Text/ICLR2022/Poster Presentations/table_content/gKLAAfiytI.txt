Table 1: Linear probe accuracy (%) on CIFAR-10. Models are pre-trained for 800 epochs. Baselineresults are from Appendix D in (Chen & He, 2021). Standard deviations are from 5 different randominitializations for the linear head. Deviations are small because the linear probe is robust to the seed.
Table 2: Linear probe accuracy (%) on ImageNet. Each model is pre-trained for 100 epochs. Base-line results are from Table B.1 in (Chen et al., 2020) from Table 4 in (Chen & He, 2021). Numbersmarked with * use a less optimal setting than our reproduction for SimCLR (See ImageNet setup).
Table 4: Fine-tuning the backbone on PhC datasets using 3000/ 2000 labelled train/ test samples.
Table 5: RotNet’s augmentation sweet spot. kNN and Rotation Prediction have the same sweep spot(Level 4) which gives best accuracy in both columns. RotNet is trained on CIFAR-10 for 100 epochswith the same optimization setup as in our I-SSL experiments. Accuracies are on the test split. (J ∙)marks the deviation from the sweet spot. Every new level adds a new augmentation to the previouslevel incrementally.
Table 7: Comparing the augmentation sensitivity for CIFAR-10. Levels: 0 is no transformations; 1 adds random resized cropping; 2 adds horizontal flips; 3 adds color jitter; 4 adds grayscale.					Method	Augmentation Level					0	1	2	3	4SimCLR	28.0± 0.1	76.0± 0.1	77.0± 0.0	87.4± 0.0	92.0± 0.0E-SimCLR	69.5± 0.0	83.5± 0.0	84.6± 0.1	91.6± 0.0	94.1± 0.0SimSiam	17.6± 0.1	71.5± 0.0	72.2± 0.0	88.1± 0.0	91.1± 0.0E-SimSiam	67.5± 0.1	84.6± 0.0	85.7± 0.1	92.9± 0.0	94.2± 0.1The importance of complete invariance or sensitivity. Table 8 studies whether a middle groundfor the representations exist, i.e. whether it is possible to have part of the representation invariantand the other part sensitive to the transformation. If we apply the E-SSL loss only to half of the rep-resentation, then there is a very small drop in the performance. Furthermore, we observe that havinga disjoint mix between insensitivity and sensitivity in the representation is noticeably harmful.
Table 8: Studying the effect of disjoint representations on CIFAR-10. Split Representation meansthat we encourage similarity only on one half of the backbone representation. Disentangled Repre-sentation means that one half of the representation is trained to be insensitive to four-fold rotationsand the other half is sensitive four-fold rotations. Linear probe accuracy (%) after 800 epochs.
Table 9: Overhead in doing rotation prediction. Reported GPU hours for an experiment on 100epochs.
Table 10:	Frozen backbone experiment on PhC datasets for 3000/ 2000 labelled train/ test samples.
Table 11: Fine-tuning the backbone on PhC datasets using 3000/ 2000 labelled train/ test samples.
