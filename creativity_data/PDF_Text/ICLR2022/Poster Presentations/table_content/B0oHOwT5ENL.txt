Table 1: Perplexity (ppl) on WikiText-103Model Size Test pplGated ConvNet (Dauphin et al., 2017)	230M	37.2Transformer-XL (Dupont et al., 2019)	165M	24.2HyperDEQ (reg.) w/12 iters (ours)	98M	23.4Initializer hφ (Conv1d)	0.4M	836.94Wikιtext-103 Language Modeling (Ablative)-∙- DEQ (reg.) (Anderson)DEQ (reg.) (Broyden)-+- HyperDEQ (reg.) only initHyperDEQ (reg.) w/o init-⅛- HyperDEQ (reg.) only ak and init-4- HyperDEQ (reg.) only Pk and init-*- HyperDEQ (reg.) (ours)50-∣45⅛>t4θQ.
Table 2: Task settings (see Sec. 5). Note that in WikiText-103 and ImageNet, we train the neuralsolver only for a few thousand gradient steps, which is far less than a complete epoch on thesedatasets. tIn addition, We decay the loss weight λ3 (for La) to 5e-8 on a linear schedule over the first1.5-2K training steps (see below).
