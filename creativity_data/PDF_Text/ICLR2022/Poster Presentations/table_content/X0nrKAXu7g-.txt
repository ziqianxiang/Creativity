Table 1: Important issues in posterior approximations for deep RL. A green tick indicates a satisfyingresult, a red cross implies an undesirable result and a yellow circle means something in between.
Table 2: Comparison of algorithms on Atari in terms of the median over 49 gamesâ€™ maximumhuman-normalized scores. Note that the performance of DQN is based on 200M training frameswhile other methods are based on 20M training frames.
Table 3: Comparison of algorithms on SuperMarioBros in terms of the raw scores by the best policieswith 20M training frames.
Table 4: Algorithmic parameters of HyperDQN for Atari and SuperMarioBros.
Table 5: The maximal score over 200 evaluation episodes for the best policy in hindsight (af-ter 20M frames) for Atari games. The performance of the random policy and the humanexpert is from https://github.com/deepmind/dqn_zoo/blob/master/dqn_zoo/atari_data.py#L41-L101. The performance of OB2I is from (Bai et al., 2021).
Table 6: Information about the base model used for deep sea.
