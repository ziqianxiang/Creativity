Table 1: Training Performance of FP134 Data FormatModel (Dataset) [Metric]	Baseline (FP32)	FP134ResNet-18 (ImageNet)	69.8	69.8ResNet-101 (ImageNet)	77.6	77.4MobileNetV2 (ImageNet)	72.2	71.92-layer LSTM (PTB) [ppl.]	91.5	92.0Transformer (IWSLT) [BLEU]	34.8	34.5MobileNetV2 + SSDLite (VOC) [mAP]	68.3	68.2Table 2: Comparisons of Data Formats for Low-Precision Training of ResNet-18 on ImageNetQuantization Scheme	Formats (Exponent, Mantissa)						Top-1 Accuracy		w	GEMM Input x	Batch -Norm Input x	dw	dx	Acc.	FP32	ProposedSWALP (Yang et al., 2019a)	81	81 -	-	81	81	321	70.3	65.8S2FP8 (Cambier et al., 2020)	(5,2)/(8,23)	(5,2)		-	(5,2)	(5,2)	(8,23)	70.3	69.6HFP8 (Sun et al., 2019)	(4,3)	(4,3)	(6,9)	(6,9)	(5,2)	(6,9)	69.4	69.4BM8 (Fox et al., 2020)	(2,5)	(2,5)	311	(6,9)	(4,3)	311	69.7	69.8FP8-SEB (Park et al., 2021)	(4,3)	(4,3)	(4,3)	(4,3)	(4,3)	(6,23)	69.7	69.0FP134 (Ours)	(3,4)	(3,4)	(3,4)	(3,4)	(3,4)	(6,23)	69.8	69.81 Fixed Point4.2	8-bit Low-Precision TrainingIn Section 2.4, we found that FP134 is the optimal format for low-precision training using the pro-
Table 2: Comparisons of Data Formats for Low-Precision Training of ResNet-18 on ImageNetQuantization Scheme	Formats (Exponent, Mantissa)						Top-1 Accuracy		w	GEMM Input x	Batch -Norm Input x	dw	dx	Acc.	FP32	ProposedSWALP (Yang et al., 2019a)	81	81 -	-	81	81	321	70.3	65.8S2FP8 (Cambier et al., 2020)	(5,2)/(8,23)	(5,2)		-	(5,2)	(5,2)	(8,23)	70.3	69.6HFP8 (Sun et al., 2019)	(4,3)	(4,3)	(6,9)	(6,9)	(5,2)	(6,9)	69.4	69.4BM8 (Fox et al., 2020)	(2,5)	(2,5)	311	(6,9)	(4,3)	311	69.7	69.8FP8-SEB (Park et al., 2021)	(4,3)	(4,3)	(4,3)	(4,3)	(4,3)	(6,23)	69.7	69.0FP134 (Ours)	(3,4)	(3,4)	(3,4)	(3,4)	(3,4)	(6,23)	69.8	69.81 Fixed Point4.2	8-bit Low-Precision TrainingIn Section 2.4, we found that FP134 is the optimal format for low-precision training using the pro-posed performance prediction method. We measure the training performance of this format andcompare it against other 8-bit data formats from recent studies by applying those formats to thetraining of various neural network models. More details on the experimental setup are provided inAppendix A.5. The performance of the proposed data format is summarized in Table 1. Overall,8-bit training using FP134 achieves nearly the same performance as the full-precision training on allmodels. Even in MobileNetV2, which is known to be sensitive to quantization due to the small num-ber of parameters, only 0.3% degradation occurred. Sun et al. (2019) show that HFP8 also exhibitsonly 0.2% accuracy degradation in MobileNetV2 (71.81% vs. 71.61%), but they quantize Batch-
Table 3: Comparisons of Training Schemes Using 4-bit Logarithmic Weights for ResNet-18	From1 Scratch	Formats (Exponent, Mantissa)			Top-1 Accuracy			w	x, dw, dx	Acc.	FP32	ProposedDeepShift-Q (Elhoushi et al., 2021)	X	(3,0)	-	-	69.8	69.6	O	(3,0)	-	-	69.8	65.3FP134 + 4-bit Log W	O	(3,0)	(3,4)	(6,23)	69.8	67.7FP134 + 4-bit Log W + Hysteresis	O	(3,0)	(3,4)	(6,23)	69.8	69.61 X: fine-tuning of pre-trained models, O: from-scratch training						Table 4: 4-bit Logarithmic Weight Training with and without Hysteresis Quantization.
Table 4: 4-bit Logarithmic Weight Training with and without Hysteresis Quantization.
Table 5: 4-bit Uniform Weight Training with and without Hysteresis Quantization.
Table 6: Area and Power of 2-Stage Pipelined MAC Units Synthesized in 40nm Process.
Table 7: Area and Power of Format Converting Units Synthesized in 40nm Process.
Table 8: FPGA Implementation Results of 2-Stage Pipelined MAC UnitsMAC Structure	FP134		FP1431		HFP82		BM83		Flex16+54		LUT	FF	LUT	FF	LUT	FF	LUT	FF	LUT	FFConventional	284	35	269	33	273	33	304	35	606	69Multi-way										2-input	499	62	648	76	990	108	755	80	891	704-input	719	63	958	77	1589	109	1193	81	1499	718-input	1141	64	1503	78	2681	110	2013	82	2664	7216-input	1997	65	2603	79	4632	110	3516	83	4955	7332-input	3526	66	4758	80	8614	112	6591	84	9500	7464-input	6713	67	9074	81	16289	113	12724	85	18649	751 Park et al. (2021)2 Sun et al. (2019) 3 Fox et al. (2020) 4 Koster et al. (2017)mat conversion overheads when comparing different formats. If we consider varioUs 8-bit data for-mats with different representation methods, as we did in Table 6, and assume that computations otherthan MAC operations are implemented in full precision, the processing architecture (except MACunits) will be identical for all formats. In addition, the on/off-chip memory space, control logics, andon-chip interconnects will remain the same. The only difference would be the low-precision MACunits and the data conversion units between full-precision and low-precision formats. However, thecost of conversion between low-precision and high-precision floating-point formats is typically verylow and does not vary much with the low-precision format. For low-precision to high-precision
Table 9: Training Result ComparisonsModel (Dataset)		FP32		FP134		FP143		FP152			train	val	train	val	train	val	train	valResNet-18	Loss	1.286	1.220	1.292	1.223	1.302	1.233	1.343	1.261(ImageNet)	Accuracy	69.50	69.73	69.46	69.67	69.19	69.63	68.27	68.93MobileNetV2	Loss	1.182	1.118	1.197	1.113	1.217	1.133	1.301	1.192(ImageNet)	Accuracy	71.47	72.20	71.16	72.25	70.63	71.92	68.70	70.352-layer LSTM	Loss	3.994	4.517	4.014	4.525	4.005	4.518	4.037	4.535(PTB)	ppl.	54.27	91.54	55.39	92.30	54.89	91.68	56.63	93.20Transformer	Loss	3.498	3.847	3.531	3.885	3.527	3.887	3.621	3.942(IWSLT)	BLEU	-	34.75	-	34.77	-	34.6	-	33.79MobileNetV2	Loss	-	2.743	-	2.773	-	2.801	-	2.970+SSDLite(VOC)	mAP	-	68.34	-	68.16	-	68.32	-	66.56the validation loss decreases compared to FP32. However, both the training and validation lossesincrease when quantized in most cases, resulting in lower accuracy. This suggests that using a verylow precision data format already introduces a large amount of noise in the network, incurring per-formance degradation. Hence, it is necessary to reduce error in the network to improve the trainingperformance in low-precision training.
