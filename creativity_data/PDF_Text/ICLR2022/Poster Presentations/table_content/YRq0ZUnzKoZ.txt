Table 1: The average rewards of baseline model-based RL methods and ours on test environmentswith unseen dynamics. Here we report the average rewards over three runs (ours is ten). Specifically,the results of methods with * are from the paper (Lee et al., 2020).
Table 2: The environmental settings in our paper.
Table 3: The prediction errors of methods on test environments	CaDM (Lee et al., 2020)	TMCL (Seo et al., 2020)	OUrsHopper	0.0551± 0.0236	0.0316 ± 0.0138	0.0271 ± 0.0011Ant	0.3850 ± 0.0256	0.1560 ± 0.0106	0.1381 ± 0.0047C_Halfcheetah	0.0815 ± 0.0029	0.0751 ±0.0123	0.0525 ± 0.0061HalfCheetah	0.6151 ± 0.0251	1.0136 ± 0.6241	0.4513 ±0.2147Pendulum	0.0160 ±0.0036	0.0130± 0.0835	0.0030 ± 0.0012Slim_HUmanoid	0.8842 ± 0.2388	0.3243 ± 0.0027	0.3032 ± 0.0046A.10 Prediction Errors on Specified EnvironmentThe prediction errors of each method on specified environment are given at Table 4, 5 and 6.
Table 4: The prediction errors of methods on specified environment of Hopper Task.
Table 5: The prediction errors of methods on specified environment of Ant Task.
Table 6: The prediction errors of methods on specified environment of Slim_Humanoid Task.
Table 7: Quantitatively clustering evaluation results of Z on Pendulum.
Table 8: Quantitatively clustering evaluation results of Z on Halfcheetah.
Table 9: Quantitative clustering evaluation results of Z on Slim_Humanoid.
Table 10: Quantitative clustering evaluation results of Z on CriPPle_Halfcheetah.
Table 11: Quantitative clustering evaluation results of Z on Hopper.
