Table 1: (Updated for Voynov et al. (2021)) Performance on three saliency detection benchmarks (DUTS, EC-SSD, DUT-OMRON) and two object segmentation benchmarks (CUB, Flowers). ** initializes with a pretrainedsupervised network. t CRF post-processing. ◊ our implementation.
Table 2: A comparison of segmentation model performance across a wide range of generator architectures,using a foreground-lighter shift (vl). All hyperparameters are kept constant across generators. IN-128px refersto ImageNet at resolution 128px, and TinyIN-64px refers to TinyImageNet at resolution 64px.
Table 3: A comparison of semantic segmentation performance on Pascal-VOC obtained from K -means clus-tering of pixelwise semantic features. The mIoU is computed over the 20 classes by performing Hungarianmatching between the clusters obtained from K -means and the ground truth. All networks use a ResNetbackbone. We compare with numerous baselines, including using self-supervised features directly (i.e. MoCo,SwaV) and IIC. Compared to Van Gansbeke et al. (2021), we achieve competitive performance, but our pipelineis entirely unsupervised, whereas theirs uses a saliency network which was initialized with a supervised networkpretrained for semantic segmentation on CityScapes.
Table 4: Semantic segmentation performance on Pascal-VOC obtained from K-means clusteringof pixelwise semantic features. mIoU is computed over the 20 classes by performing Hungarianmatching between the clusters obtained from K-means and the ground truth. We see that, as withobject segmentation, better GANs (i.e. BigBiGAN, SAGAN) yield better downstream semanticsegmentation performance.
Table 6: A comparison of segmentation performance when different directions in the latent spaceare used to construct the training segmentation masks.
Table 5: Evaluation dataset statisticsA.3 Additional AblationsAblation: Comparing latent directions. In addition to comparing the networks resulting fromvl and vb, we also compare the actual latent directions vl and vb . Due to the nonlinearity of thegenerator function, the optimal unit directions vl and vd are not necessarily negations of one an-other; indeed, we found in practice that they were close to but not exactly antiparallel. Table 9 inSection A.3 gives exact numbers for a variety of different generator architectures.
Table 7: A comparison of segmentation performance for a BigBiGAN model when different valuesof λ are used to find the latent vectors vl and vb in the optimization stage. Higher values of λyield latent directions v that produce shifted images with greater variance in brightness between thecenter and outside pixels. Conversely, lower values of λ yield latent directions v that produce shiftedimages that align better to the original images.
Table 8: A comparison of segmentation performance for a BigBiGAN-based model when differ-ent values of are used to find the latent vector vl in the optimization stage. Higher values ofcorrespond to a greater-magnitude shift in the latent space during optimization.
Table 9: The dot product of the optimized foreground-lighter (vl) and foreground-darker latent di-rections (vb) for different generators, all of which have a 120-dimensional latent space. Across theboard, the directions are almost but not exactly antiparallel (random vectors in this space have anexpected dot product of 0 with variance 击).
Table 10: Ablation results for the spatially-agnostic spatial maskA.5 Additional ExamplesA.5.1 Examples Across DatasetsIn Figure 6, Figure 7,aand Figure 8, we show the results of applying our final segmentation networkto random images from each of the five datasets on which we evaluated.
Table 11: Performance on the Flowers dataset compared to prior methods. It is important to em-phasize the fact that the “ground truth” of the Flowers dataset was generated using an automaticprocedure and is extremely noisy. For example, a significant fraction of the ground truth masks areentirely empty, and those that are not empty often do not properly reflect the content of the image. Asa result, quantitative numbers on the Flowers dataset should be heavily discounted. We encouragethe reader to see Fig. 13 for visualizations of failure cases.
Table 12: Performance on 1000 random images from the CelebA-HQ-Mask dataset. Performance isnot exactly comparable because these models were evaluated using different random subsets.
Table 13: Results of combining real images with generated images on the CUB dataset. We use asubset of 1000 training images from CUB along with 1,000,000 generated images. Additionally, weevaluate using a filtered subset of our generated images containing the 50,000 nearest neighbors tothe real images. The distance between images was computed using a self-supervised ResNet-50 Heet al. (2019). We see that combining real data points with nearby generated data points gives thebest results.
Table 14: Performance of our method for different numbers of generated images. All models aretrained for the same number of iterations (20000). For a visual representation of these numbers, seeFig. 11.
Table 15: Performance of our method on the Flowers dataset using a StyleGAN 2 model trainedon Flowers for different input image resolutions. Using a higher resolution improves performanceslightly.
