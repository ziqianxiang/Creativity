Table 1: Detection results of Sparse DETR on COCO 2017 val set. Top-k & BBR denotes thatwe sample the top-k object queries instead of using the learned object queries (Yao et al., 2021), andperform bounding box refinement in the decoder block (Zhu et al., 2021), respectively. Note thatthe proposed encoder auxiliary loss is only applied to Sparse DETR. FLOPs and FPS are measuredin the same way as used in ZhU et al. (2021). The results marked by f, ∣ are the reported ones fromZhu et al. (2021) and Wang et al. (2021), respectively.
Table 2: Comparision between the alternative objectives for DAM-based scoring network.
Table 3: Comparison between token selection criteria.
Table 4: Effectiveness of the encoder auxiliary loss using Swin-T. When the number of encoderlayers is more than 9, the model training fails, but if the encoder auxiliary loss is adopted, the modeltraining is feasible regardless of the number of encoder layers, and accuracy is improved.
Table 5: Detection results of Sparse DETR with SCRL initialization using ResNet-50. Thesame environment and hyperparameters as Experiments section are used, except for initializing thebackbone with SCRL (Roh et al., 2021) model. The results marked by § mean that the backbonenetwork is initialized by SCRL instead of the ImageNet (Deng et al., 2009) pre-trained one.
Table 6: Performance of Sparse DETR with Swin-B. The same environment and hyperparametersas Experiments section are used, except for changing the backbone to a larger scale. Note that Aux.
Table 7: Two-stage encoder token sparsification with a varied keeping ratio. COCO detec-tion performance when the encoder tokens are sparsified at the later stage with the top-ρ% binarizedDAMs pre-computed from the former stage. All models are Deformable-DETR+ with Swin-T back-bone and the encoder auxiliary loss is not applied. Note that the performance of the 50% model(47.9AP) hardly degenerates compared to the baseline(48.0 AP).
