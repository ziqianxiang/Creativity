Table 1: Performance of the proposed DTN evaluated on ViT models with different sizes. DTN canconsistently outperform LN with various ViT models. H and C denote the number of heads and thedimension of embeddings of each token, respectively.
Table 2: Performance of the proposed DTN on various vision transformers in terms of accuracy andFLOPs on ImageNet. Our DTN improves the top-1 and top-5 accuracies by a clear margin comparedwith baseline using LN with a marginal increase of FLOPs.
Table 3: Comparisons between DTN and other normalization methods, including LN, BN, IN, GN,ScaleNorm, and PowerNorm on ImageNet in terms of top-1 accuracy. Our DTN achieves competi-tive results over various normalizers.
Table 4: Comparisons between DTN and other normalization methods on Long ListOps task ofLRA in terms of accuracy. Our DTN is validated on three SOTA transformers architectures andyields competitive results to other normalizations.
Table 5: The effect of each component in DTN. Eachcomponent is crucial to the effectiveness of DTN.
Table 6: Mean Corruption Error (mCE) of ViT and ResNet models using different normalizers onImageNet-C, which is designed to evaluate robustness when ‘natural corruptions’ are presented. Thecolumn of ‘Norm.’ indicates the normalizer. Res-50 is ResNet-50 model. The mCE is obtained byaveraging corruption error across all corruption types. Our proposed DTN consistently improves therobustness of ViT-S and ViTs by a large margin.
Table 7: (a) self-supervised learning with DTN in DINO framework. We pre-train ViT-S* with DTNfor 20, 40, 60, 80 and 100 epochs and report top-1 accuracy on linear evaluation. DTN is also effec-tive in learning representation in self-supervision. (b) Comparisons of parameter and computationcomplexity between different. B, T, C, H are # samples, # tokens, embedding dimension of eachtoken, and # heads, respectively.
Table 8: Effect of the number of DTN layers on ViT-S*. More DTN leads to better recognitionperformance.
Table 9: Performance of DTN on larger models.
Table 10: Ablation study of DTN on relative positional embedding (RPE) on ImageNet. Ablation(a) denotes that we investigate the effect of RPE in eq.6 of DTN on models without RPE such asViT and PVT. Ablation (b) indicates that DTN can still improve vision transformers with carefullydesigned RPE.
Table 11: Object detection performance on COCO val2017 using Mask R-CNN and RetinaNet.
Table 12: Performance of the proposed DTN evaluated on transformers with local context already induced. DTN can consistently outperform LN with various vision transformers.				Model	Method	FLOPs	Params	Top-1 (%)Swin-T	LN	4.5G	29M	81.2	DTN	5.1G	29M	81.9Swin-S	LN	8.7G	50M	83.0	DTN	9.4G	50M	83.5LeViT-128S	LN DTN	305M 320M	7.8M 7.8M	76.6 77.3T2T-ViTt-14	LN DTN	6.1G 6.4G	21.5M 21.5M	81.7 82.4optimization difficulty in learning diverse token representations. In the experiment, we also findthat ViT models with DTN can converge much faster than LN. Understanding the role of tokenmagnitude and direction in self-attention modules would be a meaningful future research direction.
