Figure 1: (a) Performance and (b) average of absolute Q update error (in log scale) for blackjack.
Figure 2: Performance and the average L1 distance to optimal Q values, on OPFF and SFF cliffwalking, the curves are averaged over different gridworld sizes and wind probability settings.
Figure 3: MCES has been studied for two classes of algorithms: Q-values updated at the same rateTsitsiklis (2002); Chen (2018); Liu (2020) and the original, more flexible algorithm, which does notrequire the conditions (ii) and (iii) stated in the Introduction. We partition the episodic MDP spaceinto two classes: OPFF MDPs and non-OPFF MDPs. As shown in the figure, this leads to fouralgorithmic/MDP regions. Convergence is now established for the three blue shaded regions, for alldiscount factors γ ≤ 1. In the other region, it is known that at least for some non-OPFF MDPs,convergence does not occur (shown as the orange oval region). A new counterexample in episodicMDP and additional discussion are provided in appendix F.
Figure 4: Cliff Walking Environment Illustration: The grid world contains of a start state, a goalstate and a cliff area between them. In the OPFF cliff walking setting with wind probability Pw,when the agent is at P1 and moves one step to the right, it will take an additional step upwards ordownwards, each With probability Pw and not take this additional step With probability 1 - Pw. Inthe SFF cliff walking environment with wind probability Pw, when the agent is at P2 and takes onestep up, it will take an additional step towards one of the four directions, each with probability Pw,and take no additional step with probability 1 - Pw .
Figure 5: OPFF Cliff Walking: average L1 distance to optimal Q values, with different grid worldsizes and wind probabilities. Y-axis is in log scale.
Figure 6: OPFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
Figure 7: SFF Cliff Walking: average L1 distance to optimal Q values, with different grid worldsizes and wind probabilities, y axis is in log scale.
Figure 8: SFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
Figure 9:	Performance and the average L1 distance to optimal Q values, on OPFF and SFF cliffwalking, for MCES with multi-update variant and Q-Learning with different learning rate (a) anddiscount factor (g), the curves are averaged over different gridworld sizes and wind probabilitysettings.
Figure 10:	OPFF Cliff Walking: average L1 distance to optimal Q values, with different grid worldsizes and wind probabilities. Y-axis is in log scale.
Figure 11:	OPFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
Figure 12:	SFF Cliff Walking: average L1 distance to optimal Q values, with different grid worldsizes and wind probabilities. Y-axis is in log scale.
Figure 13:	SFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
Figure 14: The MDP problemFrom the above setting, we can easily see that there are four possible stationary policies:(a)	At each state, always choose to move.
Figure 15: Ri and Ti are blue regions in Q1m-Q1s plane and Q2m-Q2s plane(b) Q2m-Q2sRecall that Q1u = (Q1um, Q1us) and Q2u = (Q2um, Q2us ). Assuming that Q1u and Q2u are initialized inR1 and T1, respectively, we now describe the specific choice of exploring starts.
Figure 16: Trajectories of Q-values during the iterationNext, we show that the MCES algorithm 4 does not converge with the exploring starts (5). Precisely,we have the following resultTheorem 6. Given γ and satisfy the condition (3), suppose that Q1u and Q2u are initialized in R1and T1, respectively. Following the MCES algorithm (4) with the exploring starts 5, the trajectoryof Q1u forms a cycle according to R1 → R2 → R3 → R2 → R1 almost surely, and the trajectoryof Q2u forms a cycle according to T1 → T2 → T3 → T4 → T1 almost surely. Thus, Qu (s, a) doesnot converge with probability 1.
