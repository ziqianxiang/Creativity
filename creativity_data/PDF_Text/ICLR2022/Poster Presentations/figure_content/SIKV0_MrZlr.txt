Figure 1: Illustration of our proposed approach. During training, an input image is first forward passed throughthe source network (such as ResNet34 trained on ImageNet) and the internal feature representations are saved.
Figure 2: Above we see test accuracies asa function of (target) training sample size(top) and number of epochs (bottom) on theStanford40 dataset for the Scratch model,L2T-ww (our closest competitor) and ourmethod Auto-Transfer. Qualitatively similarbehavior is also seen on the other datasets.
Figure 3: Layer-wise Grad-CAM images highlighting important pixels that correspond to predicted outputclass. We show examples from MIT67 and CUB200 (ImageNet based transfer) where the independently trainedscratch model predicted the input image incorrectly, but our bandit based auto-transfer method predicted theright class for that image. Correctly predicted class is indicated in green text and incorrectly classified class isindicated in red text. Class probability for these predictions is also provided.
Figure 4: (Left) shows the test set data from the source task and the source models’ prediction. (Right) showsthe test-set predictions for target task data from Scratch, Source prediction, L2T-ww and Auto-Transfer withthe shallow linear network configuration [din = 1, h1 = 16, h2 = 16, h3 = 16, dout = 1].
Figure 5: Test-set predictions for Scratch, Source prediction, L2T-ww and Auto-Transfer for the target taskdata with the shallow linear network configurations left: [din = 1, h1 = 4, h2 = 4, h3 = 4, dout = 1], right:[din = 1, h1 = 8, h2 = 8, h3 = 8, dout = 1]Figure 6: Test-set predictions for Scratch, Source prediction, L2T-ww and Auto-Transfer for the target taskdata with different choices from the routing function left: [(2,0),(-1,1),(-1,2), wAdd] right: show the featurerepresentations of a single data point (plotted over the 50 training epochs) extracted from the final layer of thetarget network0.50.0-0.5-1.0--1.5 ■-2.0-2.5Caltech-UCSD Birds-200-2011. CUB-200-2011 is a bird classification datset with 200 birdspecies. Each species is associated with a wikipedia article and organized by scientific classifi-cation. Each image is annotated with bounding box, part location, and attribute labels. We use onlyclassification labels during training. There are a total of 11,788 images.
Figure 6: Test-set predictions for Scratch, Source prediction, L2T-ww and Auto-Transfer for the target taskdata with different choices from the routing function left: [(2,0),(-1,1),(-1,2), wAdd] right: show the featurerepresentations of a single data point (plotted over the 50 training epochs) extracted from the final layer of thetarget network0.50.0-0.5-1.0--1.5 ■-2.0-2.5Caltech-UCSD Birds-200-2011. CUB-200-2011 is a bird classification datset with 200 birdspecies. Each species is associated with a wikipedia article and organized by scientific classifi-cation. Each image is annotated with bounding box, part location, and attribute labels. We use onlyclassification labels during training. There are a total of 11,788 images.
Figure 7: Above we see test accuracies as a function of training time (minutes) plotted for following archi-tectures (i) Finetuning (ResNet18 - ResNet18), (ii) AutoTransfer (ResNet18 - ResNet18), (iii) AutoTransfer(ResNet34 - ResNet18), denoted FT(18-18), AT(18-18), and AT(34-18), respectively. We significantly outper-form finetuning in all datasets.
Figure 8: Test accuracies as a function of inference time plotted for following architectures (i) Finetuning(ResNet34 - ResNet34), (ii) AutoTransfer (ResNet18 - ResNet18), (iii) AutoTransfer (ResNet34 - ResNet18),denoted FT(34-34), AT(18-18), and AT(34-18), respectively. Each circle represents a batch of 128 sampleimages. We significantly outperform finetuning in all datasets.
Figure 9: Above we see test accuracies as a function of (target) training sample size for CUB200, StanfordDogs and MIT67 datasets. Each experiment is repeated 3 times.
Figure 10: Example of learned intermediate representations for a bird image from CUB200 dataset. We plotthe first 36 features in each layer ( there are 64, 128, 256 and 512 features for layers 1 to 4). It is hard to drawmeaningful patterns by looking at intermediate representations, and hence we chose to investigate layer-wiseGrad-CAM images.
Figure 11: Layer-wise Grad-CAM images highlighting important pixels that correspond to predicted outputclass. We show examples from Stanford40 and Stanford Dogs (ImageNet based transfer) where the indepen-dently trained scratch model predicted the input image incorrectly, but our bandit based auto-transfer methodpredicted the right class for that image. Correctly predicted class is indicated in green text and incorrectlyclassified class is indicated in red text. Class probability for these predictions is also provided.
Figure 12: Layer-wise Grad-CAM images highlighting important pixels that correspond to predicted outputclass. We show examples where the L2T-ww model predicted the input image incorrectly, but our bandit basedauto-transfer method predicted the right class for that image. Correctly predicted class is indicated in green textand incorrectly classified class is indicated in red text. Class probability for these predictions is also provided.
Figure 13: Layer-wise Grad-CAM images highlighting important pixels that correspond to predicted outputclass. We show examples where the L2T-ww model predicted the input image correctly, but our bandit basedauto-transfer method predicted the wrong class for that image. Correctly predicted class is indicated in greentext and incorrectly classified class is indicated in red text. Class probability for these predictions is alsoprovided.
