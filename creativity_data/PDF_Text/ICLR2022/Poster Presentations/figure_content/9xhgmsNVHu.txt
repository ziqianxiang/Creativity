Figure 1: Learning curves over 40 seeds for the baseline agent. The upper row shows the meanreward, with one standard deviation indicated by shading. The bottom row shows individual runs.
Figure 2: (Left) Correlation between runs that share network initialization and starting experience,but otherwise have independent other sources of randomness (e.g. from mini-batch sampling).
Figure 3: Each row shows a different quantity during training for the baseline agent. Row 1 showsrewards, row 2 shows actor gradient norm ∣∣V'∣, and row 3 shows average absolute action |a|.
Figure 4: Average Q-value during training.
Figure 5: Performance for 21 tasks. (Top) Average reward per task. The combined agents improvethe average reward, and most tasks benefit from our modifications. (Bottom) Standard deviationof reward, broken up by task. The combined agents have less variability, and for a few tasks thevariance decreases by orders of magnitude. Except for measurements in Table 3, we use 10 seeds.
Figure 6: The average absolute value of the actions for the combined agent. We show four repre-sentative runs of walker task. Compared to the baseline algorithm in Figure 3, the actions do notsaturate early in training despite increasing rapidly, indicating improved stability.
Figure 7: Learning curves for the two tasks not shown in Figure 1. The upper row shows the meanreward with one standard deviation indicated by shading. The bottom row show learning curves forindividual runs. Although the signal is noisy, several outlier runs fail to learn.
Figure 8: Rewards obtained when gradually replacing a learned policy with a random policy. Atevery time-step t, the action suggested by the learned policy is replaced with a random action withprobability p. We start from a final policy learned by the baseline algorithm and average overten such policies. Across all tasks, the rewards decrease gracefully with a more random policy,suggesting that the environments are relatively robust to perturbed policies.
Figure 9: Correlation between the evaluation scores of two runs that share network initialization andseed experience, but otherwise have different randomness. Correlation is calculated across ten runsand plotted against time. The correlation is noisy for many environments but oscillates around zero,suggesting that there is little actual correlation and that network initialization and seed experiencehave a small effect on performance.
Figure 10: Four quantities shown during training for the combined agent. We show reward, norm ofactor gradient ∣∣V'k, norm of actor weight ∣∣wk and average absolute action |a|. The actions neversaturate and learning starts quickly. Compare with Figure 3, which shows the same quantities forthe baseline algorithm.
Figure 11:	Learning curves when using the combined agent. We show mean reward with one stan-dard deviation indicated by shading and learning curves for individual runs.
Figure 12:	Rewards obtained during training for the runs illustrated in Figure 3. Note that therewards are nonzero from the start of training. This implies that the agent observes rewards and cantrain on these. Thus, lack of rewards is not the cause of poor performance.
Figure 13:	The performance of the combined agents on two tasks When increasing the learning rateby a factor of 10. Performance improves markedly on these tasks. This highlights how hyperparam-eter tuning could further improve the results of Figure 5.
Figure 14:	We repeat Figure 6 for all 10 seeds of the both pnorm agent of Table 2. Note that theaction distribution does not saturate.
Figure 15: We repeat Figure 6 for all 10 seeds of the penalty agent of Table 2. Note that the actiondistribution does not saturate.
Figure 16: Performance profiles as computer by the rliable library of Agarwal et al. (2021). (Left)profiles over the original 5 tasks of Table 3, with 10 seeds for combined++ and 40 seeds for the twoother methods. (Right) profiles over the additional 16 tasks of Figure 5, with 10 seeds per method.
Figure 17: We repeat Figure 3 for all 40 seeds. The plots show five quantities for the baseline agentof the walker task: reward, absolute average action |a|, actor weight norm kwk, actor gradient norm∣∣V'k and the logarithm of the actor gradient norm log ∣∣V'k. We add 1e-10 to the logarithm fornumerical stability. Results are similar to Figure 3.
Figure 18: We illustrate three quantities for 10 runs of the pendulum SWingUP task using the com-bined agent: the reward, fnz(Qt) the fraction of non-zero Q-target values obtained over the trainingbatches, and fnz(reward) the fraction of non-zero rewards obtained over the training batches. Asshown in the two middle rows, the Q-values frequently ”get stuck” at outputting almost only zeros.
Figure 19: Average Q-ValUes during early training for three agents on the reacher hard task. The bothPnorm agent, which uses penultimate normalization for the critic, has Q-values that grow steadilyduring training. The other agents have Q-values which often fluctuate wildly during training. Thissuggests thatpenultimate normalization for the critic stabilizes Q-values, which provides a less noisysignal to the actor.
Figure 20: Average absolute difference ∆Q between Q-values before and after a gradient step. TheQ-value changes are generally smaller and more stable over time for the both Pnorm agent (whichuses penultimate normalization for the critic). This likely stabilizes the Q-values as per Figure 19.
