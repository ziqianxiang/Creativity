Figure 1: Pre-training trains on auxil-iary task Taux before fine-tuning on pri-mary task T*. End-task aware trainingoptimizes both Taux and T* SimUltane-ously and can find better minima sinceoptimization is informed by the end-task.
Figure 2:	Compared to DAPT, TARTAN makes more efficient use of data. Large standard deviationsare a result of the heterogeneity of the domain data used and the fact that our tasks are low-resource.
Figure 3:	Having a separate classification head for computing meta-gradients is important. Usingthe same head as when training up-weights the end-task and under-utilizes auxiliary tasks.
Figure 4: The meta-learned task weightings show similar trajectories across different end-tasks.
