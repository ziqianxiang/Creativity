Figure 1: Overview of our benchmark test. Our dataset is a set of word-superimposed images con-sisting of the natural image datasets and hierarchical word labels. The classification tasks are dividedinto four conditions: superordinate image classification for the image with the superordinate cate-gory word (S/S), superordinate image classification for the image with the basic word (S/B), basicimage classification for the image with the superordinate category word (B/S), and basic classifica-tion for the image with basic category word (B/B).The classification examples by the CLIP modelare shown in Appendix A1.
Figure 2: Process diagram of the CLIP model for our task. This figure is created based on Radfordet al. (2021). The list of superordinate or basic word labels was used for the classification dependingon the task type. We regard the image/text encoder as the selection/activation process in humanpicture-word interference, as discussed in the main text.
Figure 3: Results of the (a) semantic and (b) spelling similarity analyses.
Figure 4: Results of the representational similarity analysis. See also Appendices B1 and H1.
Figure A1: Prediction example for word-superimposed images using the CLIP model (ViT-B/32).
Figure B1: Results of the RSA. In addition to the labels ”animal” and ”animal” in Figure 4, we showthe results of other words. Based on the CLIP image encoder (ViT-B/32) outputs, we calculatedthe image-by-image representation dissimilarity matrices. Similar to the results of the word labels”animal” and ”person,” presenting words on the images did not affect the no-word representation in acategory-selective way. These results are consistent with the finding in the main text, suggesting thatsemantic word representations in the CLIP image encoder are different from image representations.
Figure C1: Results of misclassification rates for the CLIP models using various image encoders.
Figure C2: Results of semantic similarity analysis for the CLIP model using various image en-coders. The semantic similarity analysis of Figure 3 was conducted for the CLIP models with theimage encoder of a ViT-B/32, ViT-B/16, ResNet-50x16, ResNet-50x4, ResNet-101, and ResNet-59.
Figure C3: Results of the representational similarity analysis (RSA) for the CLIP model usingvarious image encoders. The analysis method was the same as Figure 4, except that we also usedthe CLIP models with other image encoders than a ViT-B/32. The results of the ViT-B/32 imageencoder were replotted from Figure 4.
Figure D1: Results of semantic similarity analysis.The same analysis as Fig was conducted fordifferent prompts shown in Table G1.
Figure E1: Results of the prediction probability analysis.
Figure F1: Results of the representational analysis for nonsense pseudowords.
Figure H1: Relationship between no-word and word-superimposed images. We analyzed the repre-sentational dissimilarity matrices (RDMs) between no-word images and word-superimposed imagesfor the CLIP image encoder (ViT/32). The vertical axis of each panel shows the no-word images,and the horizontal axis shows the word-superimposed images for the center and right panel. Theleft panel is the same as the no-word result in Figure 4. If the image encoder has a shared se-mantic representation for both the superimposed words and the images, the RDMs should show thecategory-specific similarity when the image category is consistent with the superimposed word. Forinstance, when the word ”animal” is superimposed on the image, the similarity between the word-superimposed images and the animal images should be selectively high. However, results did notshow any category-specific similarity, and the RDMs were similar even different words were super-imposed. The results are consistent with that the CLIP image encoder has different representationsof word forms from visual image representations.
