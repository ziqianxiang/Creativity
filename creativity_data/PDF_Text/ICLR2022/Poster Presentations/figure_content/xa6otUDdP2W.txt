Figure 1: Overview of the cyclic GaP (C-GaP) training method. We assume 4 partitions in a modelare grown and pruned in a cyclic order. Only one partition is kept dense during training. After Ksteps, the dense partition is pruned and the whole model is fine-tuned to obtain the sparse model.
Figure 2: Comparison between (a) the scheduled mask exploration and (b) the random/greedy maskexploration. In (a), the weights to be grown (or pruned) are in the same layer and the growing phasecovers all weights in that layer. In (b), they may be in different layers and the growing phase does notguarantee a coverage during training.
Figure 3: Overview of the parallel GaP (P-GaP) training method. We assume 4 training nodes areused for the 4 partitions, which are grown in parallel for T epochs. At the end of each step, the denseparts in the nodes are combined, followed by a pruning back to a sparse model. After K steps, themodel is fine-tuned.
Figure 4: Training workflow of the (a) C-GaP and (b) P-GaP. In C-GaP (left), the model is partitionedand trained on one machine for multiple rounds. In each round, the sparse mask for each partition isgrown and pruned cyclically. Each round has κ steps when the model is partitioned into κ parts. InP-GaP (right), a randomly initialized and sparsified model is copied and distributed to κ machineswhen partitioned into κ parts. Each round contains one step since all machines train the correspondingcopy in parallel. An optimized model can be collected from the process and pruned to the desiredsparsity scheme, then finetuned to restore accuracy.
