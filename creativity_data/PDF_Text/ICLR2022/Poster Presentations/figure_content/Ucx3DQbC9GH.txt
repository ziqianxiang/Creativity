Figure 1: (a) Task-dependency of each augmentation method measured on 5-shot tasks in Section 4.2.
Figure 2: Illustration of learning to augment Difficult, but Not too Different (DND).
Figure 3: Analysis about the learned augmentation policy on Review50. (a, b) Dynamics of probabilityand magnitude during fine-tuning, respectively. (c) Top-3 representative augmentations, discoveredby our augmentation policy. Best viewed in color.
Figure 4: Dynamics of learned augmentation policy via DND.
Figure 5: Dynamics of learned augmentation policy via Difficult.
Figure 6: Qualitative results of difficult, but not too different samples; (a) Average difficulty (difficulty↑ = confidence ]) and similarity of the augmented samples under each augmentation method onNews20. (b) Real examples on Review50. Different criteria are used for the selection, and two mostsalient words of original sentence are remarked in bold. Critically altered words are colored in red.
Figure 7:	Difficulty (difficulty ↑ = confidence ]) and similarity of the augmented samples under eachaugmentation method on Review50. For the better visualization, 250 training examples are randomlyselected and then augmented.
Figure 8:	Examples of augmented samples on Review50 (Chen & Liu, 2014). Different criteriaare used for the selection, and three most salient words of original sentence are remarked in bold.
Figure 9:	Accuracy of gW (Eq. 3) across train-ing iterations on News50 and Review50 datasets.
Figure 10:	Accuracy of word reconstruction (Eq.
