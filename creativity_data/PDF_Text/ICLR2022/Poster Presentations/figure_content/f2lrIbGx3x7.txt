Figure 1: Example of a gradient leakage attack. Bayes optimal adversary randomly initializes imagex(1) and then optimizes for the image with the highest logp(g|x) + log p(x) in its Î´-neighborhood.
Figure 2: PSNR obtained by reconstruction attacks on ATS and Soteria during the first 1000 stepsand 10 epochs, respectively, demonstrating high gradient leakage early in training.
Figure 3: Images obtained by running attacks on Soteria, ATS, and PRECODE on the CIFAR-10dataset after the 10th training step. We can observe that reconstructed images are very close to theoriginal ones, meaning that these defenses do not protect privacy early in the training.
Figure 4: Ablation with the Bayes attack.
Figure 5: We compare the reSultS of a direct attack on the defended network 5b with our attack 5c.
Figure 6: Our reconstruction results after several training steps with ATS, batch_size 32, and aug-mentations 7-4-15. We can see how the visual quality starts to decline after 10 steps and at 50 stepsone can no longer reliably recover the input.
Figure 7: Ablation for the number of samples in the Monte Carlo estimate.
