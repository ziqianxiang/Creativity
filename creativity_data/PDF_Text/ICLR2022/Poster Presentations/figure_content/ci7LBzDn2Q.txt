Figure 1: Mean length distortion as a function of depth, for randomly initialized ReLU networksof varying architecture. As described in Thm. 3.1, distortion not only fails to grow, but shrinkswith depth, especially for networks of small width. (a) compares the predictions of our Thm. 5.1(solid lines) to the lower bounds proven in prior work Raghu et al. (2017) (dashed lines) and thetrue empirical means (colored dots), which closely track our predictions. (b) zooms in on the upperpart of (a), showing the empirical mean length distortion as a function of depth for different widths.
Figure 2: Mean length distortion as a function of the ra-tio of output to input dimension, for ReLU networks withvarious architectures (e.g. [10, 10, 10] denotes three hid-den layers, each of width 10). All networks are randomlyinitialized as in Figure 1. We sample 100 pairs of inputdimension n0 and output dimension nL , each at most 50,such that the ratio of output to input dimension is distinctfor each such pair. For each pair, 200 different networkinitializations are tested and the resulting length distortionis calculated; the log of the empirical mean is plotted. Thedashed black line plots log(y) = ɪ log(x), the approxi-mate prediction by Theorem 3.1.
Figure 3: Length distortion as a function of PL= 1 n-1,showing both mean and standard deviation across initial-izations. We test several types of network architecture 一with constant width 20, alternating between widths 30 and10, and with the first (respectively, second) half of the lay-ers of width 30 and the rest width 10. Each architecturetype is tested for several depths. For each such network,we use n0 = nL = 5 and compute length distortion for100 initializations on a fixed line segment. As predictedin Thm. 4.1, the mean length distortion decreases withthe sum of width reciprocals. Empirical standard devia-tion does not, in general, increase with greater depth, re-maining modest throughout, as is consistent with the up-per bound on variance in Thm. 4.1.
Figure 4: Mean length distortion as a function of depth, for networks with convolutional layers andskip connections, initialized using He normal initialization He et al. (2015). (a) shows results fornetworks having convolutional layers, where the input dimension n0 equals 16 × 16 × 3 = 768 andoutput dimension nL equals 5. Here width corresponds to the number of 3 × 3 kernels in each layer.
