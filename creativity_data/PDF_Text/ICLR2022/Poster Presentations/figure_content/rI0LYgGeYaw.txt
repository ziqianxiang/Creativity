Figure 1: Average convergence of JlN towards Jlj for two samples from the same data set, gener-ated with a random Gaussian matrix. Jlj - JlN converges linearly on the support in both cases.
Figure 2: Average conver-gence of JN towards Jj for50 samples. In this example,40% of the Jacobians are un-stable (red curves).
Figure 3: Gradient convergence in angle for 1000 synthetic samples (left) and patches from anoisy image (center). The image is normalized, decomposed into patches of dimension 10 × 10 andwith additive Gaussian noise (σ2 = 0.1). The dictionary for which the gradients are computed iscomposed of 128 patches from the image. (right) Relative difference between angles from DDL andAM. Convergence is faster with DDL in early iterations, and becomes unstable with too many steps.
Figure 4:	(left) Number of gradient steps performed by the line search before convergence, (center)distance to the optimal loss, and (right) distance to the optimal dictionary recovery score dependingon the number of unrolled iterations. The data are generated as in Figure 1. We display the mean andthe 10% and 90% quantiles over 50 random experiments. DDL needs less gradient steps to convergein early iterations, and unrolling obtains high recovery scores with only a few dozens of iterations.
Figure 5:	We consider a normalized image degraded by Gaussian noise. (left) PSNR depending onthe number of unrolled iterations for σn2oise = 0.1, i.e. PSNR = 10 dB. DL-Oracle stands for fullAM dictionary learning (103 iterations of FISTA). There is no need to unroll too many iterations toobtain satisfying results. (center) PSNR and average recovery score between dictionaries dependingon the SNR for 50 random initializations in CDL. (right) 10 loss landscapes in 1D for σn2oise = 0.1.
Figure 6: Recovery score vs. time for 10random Gaussian matrices and 105 samples.
Figure 7:	Stochastic DeepCDL on 6 minutes of MEGdata (204 channels, samplingrate of 150Hz). The algo-rithm uses 40 atoms, 30 un-rolled iterations and 100 iter-ations with batch size 20. Werecover heartbeat (0), blink-ing (1) artifacts, and an au-ditory evoked response (2)among others.
Figure A: LISTAFigure B:	Loss landscape in approximate CDLGradient convergence in norm - Figure C. Gradient estimates convergence in norm for syntheticdata (left) and patches from a noisy image (right). The setup is similar to Figure 3. Both gradientestimates converge smoothly in early iterations. When the back-propagation goes too deep, theperformance of gN2 decreases compared to gN1 , and we observe large numerical instabilities. Thisbehavior is coherent with the Jacobian convergence patterns studied in Proposition 2.5. Once on thesupport, gN2 reaches back the performance of gN1 .
Figure B:	Loss landscape in approximate CDLGradient convergence in norm - Figure C. Gradient estimates convergence in norm for syntheticdata (left) and patches from a noisy image (right). The setup is similar to Figure 3. Both gradientestimates converge smoothly in early iterations. When the back-propagation goes too deep, theperformance of gN2 decreases compared to gN1 , and we observe large numerical instabilities. Thisbehavior is coherent with the Jacobian convergence patterns studied in Proposition 2.5. Once on thesupport, gN2 reaches back the performance of gN1 .
Figure C: Gradient estimates convergence in norm for synthetic data (left) and patches from a noisyimage (right). Both gradient estimates converge smoothly in early iterations, after what DDL gradi-ent becomes unstable. The behavior returns to normal once the algorithm reaches the support.
Figure D:	Time to reach a recovery score of 0.95. Intermediate batch sizes offer a good trade-offbetween speed and memory usage compared to full-batch DDL.
Figure E:	Comparison between Online DL and Stochastic DDL. Stochastic DDL is more efficientfor smaller values of λ, due to the fact that sparse coding is slower in this case.
