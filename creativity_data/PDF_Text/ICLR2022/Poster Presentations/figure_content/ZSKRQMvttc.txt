Figure 1: Environments: Here are some of our environments for evaluation. Three classical physical controlRL benchmarks of increasing difficulty, from left: Cartpole Swing Up + Balance, Ant, and Humanoid. Inaddition, we train the policy for the high-dimensional muscle-tendon driven Humanoid MTU model from Leeet al. (2019). Whereas model-free reinforcement learning (PPO, SAC) needs many samples for such high-dimensional control problems, SHAC scales efficiently through the use of analytic gradients from differentiablesimulation with a parallelized implementation, both in sample complexity and wall-clock time.
Figure 2: Landscape comparison between BPTT and SHAC. We select one single weight from a policyand change its value by ∆θk ∈ [-1, 1] to plot the task loss landscapes of BPTT and SHAC w.r.t. one policyparameter. The task horizon is H = 1000 for BPTT, and the short horizon length for our method is h = 32.
Figure 3: Computation graph of BPTT and SHAC. Top: BPTT propagates gradients through an entiretrajectory in each learning episode. This leads to noisy loss landscapes, increased memory, and numericalgradient problems. Bottom: SHAC subdivides the trajectory into short optimization windows across learningepisodes. This results in a smoother surrogate reward function and reduces memory requirements, enablingparallel sampling of many trajectories. The environment is reset upon early termination happens. Solid arrowsdenote gradient-preserving computations; dashed arrows denote locations at which the gradients are cut off.
Figure 4: Learning curves comparison on four benchmark problems. Each column corresponds to a par-ticular problem, with the top plot evaluating sample efficiency and the bottom plot evaluating wall-clock timeefficiency. For better visualization, we truncate all the curves up to the maximal simulation steps/wall-clocktime of our method (except for Humanoid MTU), and we provide the full plots in Appendix A.4. Each curve isaveraged from five random seeds, and the shaded area shows the standard deviation. SHAC is more sample effi-cient than all baselines. Model-free baselines are competitive on wall-clock time on pedagogical environmentssuch as the cartpole, but are much less effective as the problem complexity scales.
Figure 5: Humanoid MTU: A sequence of frames from a learned running gait. The muscle unit color indicatesthe activation level at the current state.
Figure 6: Study of short horizonlength h on Ant problem. A smallh results in worse value estimation. Atoo large h leads to an ill-posed opti-mization landscape and longer trainingtime.
Figure 7: Full learning curves of algorithms on six benchmark problems. Each curve is averaged from fiverandom seeds and the shaded area shows the standard deviation. Each problem is shown in a column, withthe plot on the top showing the sample efficiency and the curves on the bottom for wall-clock time efficiency.
Figure 8:	Learning curves of our method with fixed network architectures and learning rates. We use thesame network architectures and learning rates used in Humanoid problem on all other problems, and plot thetraining curves comparison with the ones using optimal settings. The plot shows that our method still performsreasonably well with the fixed network and learning rates settings.
Figure 9:	Learning curves of our method with deterministic policy. We test our method with deterministicpolicy choice. We use the same network sizes and the hyperparameters as used in the stochastic policy andremove the policy output stochasticity. We run our method on each problem with five individual randomseeds. The results show that our method with deterministic policy works reasonably well on all problems,and sometimes the deterministic policy even outperforms the stochastic policy (e.g., Humanoid). The smallperformance drop on the Ant problem comes from one single seed (out of five) which results in a sub-optimalpolicy.
Figure 10: Comparison with REDQ and MBPO. We compare SHAC to REDQ and MBPO. While REDQhas very excellent sample efficiency, its wall-clock time efficiency is very poor (102× slower than SHAC inCartPole Swing Up and 162× slower than SHAC in Ant), which renders it as a less attractive method thanSHAC in the settings where the policy is learned in a fast simulation. The offical MBPO code works wellon the CartPole Swing Up problem while works even worse than REDQ on other three tasks. Furthermore,both REDQ and MBPO fail to train on our high-dimensional problems, Humanoid and Humanoid MTU, withdefault hyperparameters.
Figure 11: Single-weight landscape and gradient of the SHAC loss. In the left figure, we select one singleweight from a policy and change its value by ∆θk ∈ [-1, 1] to plot the task loss landscapes of SHAC w.r.t. onepolicy parameter (same as Figure 2 right). The short horizon length is h = 32. Each policy variant is evaluatedby 128 stochastic trajectories and we set the same random seed before the evaluation of each policy variant. Inthe right figure, we plot the finite difference gradient and the analytical gradient along that single weight axis.
Figure 12: Gradient distribution evolving over training process. We select six policies from differentepisodes during the training process for the Humanoid problem and plot the gradient value distribution ofBPTT (left) and SHAC (right). Note the x-axis scales on the plots. The plots show that the gradient computedfrom BPTT suffers from gradient explosion, whereas the analytical gradient computed from our method ismuch more stable.
Figure 13: Landscape surfaces comparison between BPTT and SHAC. We study the landscapes for theHumanoid and Ant problems. For Humanoid, We choose a policy during the optimization and for Ant Wechoose the best policy from the learning algorithm. For each problem, we randomly select two directions inthe policy parameter space and evaluate the policy variations along these two directions. The task horizonis H = 1000 for BPTT, and the short-horizon episode length for our method is h = 32. As we can see,longer optimization horizons lead to noisy loss landscape that are difficult to optimize, and the landscape of ourmethod approximates the real landscape but is much smoother.
