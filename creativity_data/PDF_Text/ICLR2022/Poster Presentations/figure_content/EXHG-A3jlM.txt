Figure 1: Parameter count and mIoU for Segformer,Swin, and other models at different scales. AFNO con-sistently outperforms other mixers (see Section 5.7).
Figure 2: The multi-layer transformer network with FNO, GFN, and AFNO mixers. GFNet performs element-wise matrix multiplication with separate weights across channels (k). FNO performs full matrix multiplica-tion that mixes all the channels. AFNO performs block-wise channel mixing using MLP along with soft-thresholding. The symbols h, w, d, and k refer to the height, width, channel size, and block count, respectively.
Figure 3: Pseudocode for AFNO with adaptive weight sharing and adaptive masking.
Figure 4: Ablations for the sparsity thresholds and block count measured by inpainting validation PSNR. Theresults suggest that soft thresholding and blocks are effectiveSparsity Threshold. We vary the sparsity threshold λ from 0 to 10. For each λ we pretrain thenetwork first, and then finetune for few-shot segmentation on the CelebA-Faces dataset. We reportboth the inpainting PSNR from pretraining and the segmentation mIoU. The results are shown inFigure 3. λ = 0 corresponds to no sparsity. It is evident that the PSNR/mIoU peaks at λ = 0.01,indicating that the sparsity is effective. We also compare to hard thresholding (always removinghigher frequencies as in FNO) in Table 6. We truncate 65% of the higher frequencies for bothinpainting pretraining and few-shot segmentation finetuning.
Figure 5: Spectral clustering of tokens for different token mixers. From top to bottom, it shows the input andthe layers 2, 4, 6, 8, 10 for the inpainting pretrained model.
Figure 6: Log magnitude of tokens (56×56) after soft-thresholding and shrinkage (λ = 0.1) and the sparsitymask averaged over channels for inpainting pretrained network. Left to right shows layers 1 to 5, respectively.
