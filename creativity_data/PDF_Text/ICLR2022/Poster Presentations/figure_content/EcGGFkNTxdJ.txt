Figure 1: Example of a two-player differentiable game withr(a1,a2) = a1a2. We initialisetwo Gaussian policies with μ1 =-0.25, μ2 = 0.25. The purpleintervals represent the KL-ball ofδ = 0.5. Individual trust regionupdates (red) decrease the joint re-turn, whereas our sequential up-date (blue) leads to improvement.
Figure 2: Performance comparisons between HATRPO/HAPPO and MAPPO on three SMAC tasks.
Figure 3: Performance comparison on multiple Multi-Agent MuJoCo tasks. HAPPO and HATRPOconsistently outperform their rivals, thus establishing a new state-of-the-art algorithm for MARL.
Figure 4:	Performance comparison between original HATRPO, and its modified versions: HATRPOwith parameter sharing, and HATRPO without randomisation of the sequential update scheme.
Figure 5:	Performance comparison between HATRPO and MAPPO/IPPO without parameter shar-ing. HATRPO significantly outperforms its counterparts.
Figure 6: Performance comparison between MAPPO and HAPPO on MPE.
