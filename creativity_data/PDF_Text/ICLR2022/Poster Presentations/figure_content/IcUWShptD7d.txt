Figure 1: The architecture for training with ordering supervision. Left: input values are fed separatelyinto a Convolutional Neural Network (CNN) that has the same weights for all instances. The CNNmaps these values to scalar values a0, ..., a5. Center: the odd-even sorting network sorts the scalarsby parallel conditional swap operations such that all inputs can be propagated to their correct orderedposition. Right: It produces a differentiable permutation matrix P. In this experiment, the trainingobjective is the cross-entropy between P and the ground truth permutation matrix Q. By propagatingthe error backward through the sorting network, we can train the CNN.
Figure 2: minf (x, 0) for different sigmoidfunctions f ; color coding as in Table 1.
Figure 3: Loss fora 3-wire odd-even sorting network, drawnover a permutahedron projected onto the x-y-plane. Forlogistic sigmoid (left) and optimal sigmoid (right).
Figure 4: min (x, 0) for Sinkhorn sort (red),NeuralSort (red), Relaxed Bubble sort (red),diffsort with logistic sigmoid (red), diffsortwith activation replacement trick (purple), andFast Sort (orange).
Figure 5: Evaluating different sigmoid functions on the sorting MNIST task for ranges of differentinverse temperatures β . The metric is the proportion of individual element ranks correctly identified.
Figure 6: Additional results analogous to Figure 5. Evaluating different sigmoid functions on thesorting MNIST task for ranges of different inverse temperatures β . The metric is the proportionof individual element ranks correctly identified. In all settings, the monotonic sorting networksclearly outperform the non-monotonic ones. The first three rows use odd-even networks withn ∈ {3, 5, 7, 9, 15, 32}. The last row uses bitonic networks with n ∈ {16, 32}.
