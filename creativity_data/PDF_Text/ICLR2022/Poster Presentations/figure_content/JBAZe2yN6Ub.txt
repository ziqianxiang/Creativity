Figure 1: The FR is higher for shorter paths. (a-c) A 2D gridworld and fixed policies. (d) The FR from s0 tosg is higher for π2, but the SR is lower. (e) SR-GPI with the SR picks π1, while FR-GPI selects π2.
Figure 2: The FF facilitates accurate policy evaluation and selection. Shading denotes 1 SE over 20 seeds.
Figure 3: The FR enables efficient planning. (a-d) A 2D gridworld with start (s0) and goal(sg) states, alongwith three fixed policies. (e) GPI follows π1 . (f) Planning with the FR enables the construction of a shorter path.
Figure 4: APF accelerates convergence in roboticreaching. Shading denotes 1 SE over 10 seeds.
Figure 5: FRP interpolates between GPI and model-based DP. Shading represents 1 SE.
Figure 6: FRP induces realistic escape behavior.
Figure 7: FF and SF learning curves for continuous MountainCar. Results averaged over 20 runs.
Figure 8: Tabular environments for exploration. The tuples marking each transition denote (actionid(s); probability; reward). In RiverSwim, the agent starts in either state 1 or state 2 with equalprobability, while for SixArms the agent always starts in state 0.
Figure 9: Network architecture for DQN + FF and DQN + SF{(sit, ait, rti, sit+1)}iB=1 (where B is the minibatch size) to minimize the squared Bellman errorBLQ = X krti +γmaaxQ-(sit+1,a) - Q(sit,ait)k22	(20)i=1via gradient descent (the subscript - on the target Q-values indicates that gradients do not flowthrough it). Unlike the standard DQN, the network has two additional output heads. The first is areconstruction head which, given the base feature representation of a state st and an embedding ofthe subsequent action at produces a prediction of the following state ^t+ι. It′s trained to minimizethe reconstruction lossBLs = X llst+ι - §t+ik2.	(21)i=1The final output head of the network is an FF/SF prediction, trained using the squared FF error in theformer case:BLφ = X kφ(St) + Y(I- φ(St)W-(St+1)- Ψ(st)k2,	(22)i=1and the squared SF error in the latterB
Figure 10:	Exploration with function approximation. (Top left) Visualization of the DeepSeaenvironment, credit to Osband et al. (2020). (Top right) DQN + FF signficantly outperforms standardDQN and DQN + SF. (Bottom) Different runs across problem sizes.
Figure 11:	The FF is robust to feature dimensionality. FF and SF representation strengths fordifference feature dimensionalities between the start and goal locations for an example goal incontinuous MountainCar. The vertical dashed line marks the power of the optimal policy. We cansee that for all but the coarsest feature representation, the FF is highest for the policy closest to theoptimal.
Figure 12: FourRooms learning curves. FOURROOMS base policies learning curves (average L2norm of TD errors over 10 runs; shaded area is one standard deviation); top row is for FRs , bottom isfor SRs.
Figure 13:	Implicit planning output. (Left) The planning policies πF (s) that the agent will elect to follow ineach state en route to the goal (see Fig. 3(a)). Arrows denote the action taken by the chosen policy in each state.
Figure 14:	Exploration and escape (a) A sample trajectory from the “exploration phase” startingfrom the shelter. (b) Because the agent starts from the shelter during exploration, the first time it istested starting from the top of the grid, its FR for the down policy for that state is still at initialization.
Figure 15: Escape learning curves. Learning curves (norms of TD errors) for the first explorationphase, the first escape trial, and the second exploration phase for the "down" policy. The verticaldotted lines in the escape trial mark the time step at which the agent encounters the barrier. Thiscauses a temporary jump in the TD errors, as representation learning did not reflect the wall at thispoint. The top row consists of FR results and the bottom row is from SRs, averaged over 10 runs.
Figure 16: An SR cannot effectively escape under the same conditions as an FR agent.
Figure 17: SR vs. FR visualization The SRs and FRs from the start state for the policies in Fig.
