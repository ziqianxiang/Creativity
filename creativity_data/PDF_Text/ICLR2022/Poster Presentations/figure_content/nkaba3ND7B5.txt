Figure 1: Two evaluation schemes in autonomous RL. First, the deployment setting (top row, (1)), where weare interested in obtaining a policy during a training phase, π, that performs well when deployed from a so 〜ρ.
Figure 2: Performance of standard RL at with varying levels of autonomy, ranging from resets provided every1000 to 200000 steps. Performance degrades substantially as environment resets become infrequent.
Figure 3: Comparing the distribution of states visited With resets (blue) and Without resets (brown). Heatmapsvisualize the difference betWeen state visitations for oracle RL and FBRL, thresholded to highlight states Withlarge differences. Resets enable the agent to stay around the initial state distribution and the goal distribution,Whereas the agents operating autonomously skeW farther aWay, posing an exploration challenge.
Figure 4: Evaluating policies starting froma uniform initial state distribution. Poli-cies learned via autonomous RL (FBRL andVaPRL) are more robust to initial state dis-tribution than policies learned in oracle RL.
Figure 5: Deployed Policy Evaluation and Continuing Policy Evaluation per environment. Resultsand averaged over 5 seeds. Shaded regions denote 95% confidence bounds.
