Figure 1: Comparison of algorithms on synthetic quadratic optimization tasks with nonconvex {fi }.
Figure 2: Comparison of algorithms on the encoding learning task for the MNIST dataset.
Figure 3: Comparison of algorithms under PE condition on synthetic quadratic optimization tasks.
Figure 4:	Comparison of RandK and PermKon synthetic quadratic optimization tasks. Each rowcorresponds to a fixed number of nodes; each column corresponds to a fixed noise scale. In thelegends, we provide compressor names and fine-tuned multiplicity factors of step sizes relative totheoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits that everynode has sent. Dimension d = 1000.
Figure 5:	Comparison of RandK and PermK under PE condition on synthetic quadratic OPtimiza-tion tasks. Each row corresponds to a fixed number of nodes; each column corresponds to a fixednoise scale. In the legends, we provide compressor names and fine-tuned multiplicity factors of stepsizes relative to theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number ofbits that every node has sent. Dimension d = 1000.
Figure 6:	Comparison of ToPK and PermKon synthetic quadratic optimization tasks. Each rowcorresponds to a fixed number of nodes; each column corresponds to a fixed noise scale. In thelegends, we provide compressor names and fine-tuned multiplicity factors of step sizes relative totheoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bits that everynode has sent. Dimension d = 1000.
Figure 7:	Comparison of TopK and PermK under PL condition on synthetic quadratic optimizationtasks. Each row corresponds to a fixed number of nodes; each column corresponds to a fixed noisescale. In the legends, we provide compressor names and fine-tuned multiplicity factors of step sizesrelative to theoretical ones. Abbreviations: NS = noise scale. Axis x represents the number of bitsthat every node has sent. Dimension d = 1000.
Figure 8: Comparison of algorithms on the encoding learning task for the MNIST dataset. Each rowcorresponds to a fixed regularization parameter Î»; each column corresponds to a fixed probabilitypb. In the legends, we provide compressor names and fine-tuned step sizes. Axis x represents thenumber of bits that every node has sent.
Figure 9: Comparison of RandK and PermKon the encoding learning task for the MNIST dataset.
Figure 10: Comparison of ToPK and PermKon the encoding learning task for the MNIST dataset.
