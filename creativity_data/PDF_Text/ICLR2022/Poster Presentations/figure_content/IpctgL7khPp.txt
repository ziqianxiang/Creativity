Figure 1: Visualization of the toy problem.
Figure 2: left: Relearning performance of the memory. We compare InfoGS with RS and InfoGSwithout using learnability, for various budgets. middle, right: TSNE visualizations of the train-ing data (colored dots, where the color represents the label of the data point) and 200 memorypoints (black digits, where the digit represents the label of the point) for InfoGS without and withlearnability, respectively. We observe that InfoGS without learnability selects many memories thatdiffer from the training majority, i.e., it selects outliers. In comparison, incorporating learnabilityavoids the outliers and selects memories that spread the training region and help to learn the decisionboundary. Further, InfoGS performances also improve over RS when the budget is very low.
Figure 3: The effect of update timing ∆t.
Figure 4: Test performances against data imbalance over continual learning benchmarks. left three:We compare RS, weighted RS (WRS) using the output-space Hessian, InfoGS and InfoRS. For eachexperiment, we plot the mean and the 95% confidence interval across 10 random seeds. right: Wecompare InfoRS and RS for different memory budgets over Split CIFAR10. For all figures, weobserve that InfoRS is more robust to data imbalance compared to RS.
Figure 5: left: The reservoir count along with training. The line i = 1, 3, 5, 7, 9, corresponds tothe problem where the i-th task has 10 times epochs than the other tasks. We also plot the blackdashed line for the count of standard RS. We observe that the reservoir count of InfoRS grows at asimilar speed as RS for most tasks. However, it grows slower at iterations corresponding to task ifor each line i. In this way, InfoRS counters the imbalanced data stream, maintains a diverse buffer,and behaves more robustly. right: The class variance of the memory. We observe that the memoryof RS becomes the most imbalanced with the data imbalance increasing.
Figure 6: The total running time.
Figure 7: The performances of InfoRS with varying criteria for Split CIFAR10 (left) and SplitMiniImageNet (right). WecompareER,MIC1,MIC3,IG1,IG3.
Figure 8:	Test performances against data imbalances over continual learning benchmarks. We com-pare RS, weighted RS (WRS) using the output-space Hessian, InfoGS, InfoRS, GSS, and CBRS.
Figure 9:	Test performances against data imbalances over continual learning benchmarks. We com-pare InfoGS, InfoRS, and the InfoGS with reservoir sampling (InfoGS-RS).
Figure 10: Validation performances against the learnability ratio (η) and the information threshold-ing ratio (γi), over Split CIFAR10. We compare InfoRS under various data imbalances (R) and plotthe means and 95% confidence intervals. Firstly, for all data imbalance, the best η is achieved ataround 3 and the best γi at around 0. Thus both hyperparameters are not sensitive to data imbalance.
Figure 11: The evolution of test accuracies. The first task is trained for 30 times longer than others.
Figure 12: TSNE visualizations of the training data (colored dots, where the color represents thelabel of the data point) and 25 memory points (black digits, where the digit represents the labelof the point) for InfoGS without and with learnability, respectively. With a small memory, weobserve that InfoGS without learnability performs similarly to InfoGS with learnability. Both selecta representative memory buffer that reflects the training distribution. Because the agent needs toselect 25 memories out of data points from 10 classes, the surprise will dominate the selectionprocess, and the outlier issue is not protruding. Therefore, incorporating learnability is not criticalto the selection when the memory buffer is very small.
Figure 13: Validation performances against the logit regularization coefficient (α) and the label reg-ularization coefficient (β) in DER++, over Split CIFAR10. We compare InfoRS under various dataimbalances (R) and plot the means and 95% confidence intervals. We observe that the performanceis not sensitive to α and β .
