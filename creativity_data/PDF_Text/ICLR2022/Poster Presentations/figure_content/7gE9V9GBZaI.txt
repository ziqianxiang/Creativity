Figure 1: A conceptual illustration of decision boundaries learned via standard training and adversarial trainingwith true and random labels, respectively. The model needs a significantly more complicated decision boundaryto memorize adversarial examples of training data with random labels.
Figure 2: (a) and (b) show the natural and robust training accuracies of PGD-AT and TRADES, respectively,when trained on true or random labels. (c) shows the generalization gap under varying levels of label noise.
Figure 3: (a): Gradient norm of PGD-AT and TRADESalong the training process. (b): The ratio of the gra-dient norm of PGD-AT and TRADES during the first1000 training iterations.
Figure 4: (a): The `2 distance between the gradientsat θ and θ + λd of different losses, where θ are ini-tialized, λ ∈ [-0.05, 0.05]. (b): The cosine similaritybetween the gradients in each two successive epochs.
Figure 5: ATby Eq. (6)5Published as a conference paper at ICLR 20221025'0"10,5'0,∙'0,'0∙mκ>NΓ≡4∞⅝▲ Model trained on true labels• Model trained on random labels50	60	70	80	90	100	50	60	70	80	90	100	50	60	70	80	90	100	50	60	70	80	90	100Robust Generalization. Gap	Robust Generalization (⅛p	Robust GeneiBlization Gap	Robust Generalization GapFigure 6: The results on four complexity measures of the adversarially trained models w.r.t. robust generaliza-tion gap. The training settings of these models are provided in Appendix A.3.
Figure 6: The results on four complexity measures of the adversarially trained models w.r.t. robust generaliza-tion gap. The training settings of these models are provided in Appendix A.3.
Figure 7: (a): The accuracy curves of PGD-AT with true labels to reproduce robust overfitting. (b): The robusttest accuracy of PGD-AT under various perturbation budgets . (c): The adversarial loss of two independentlytrained networks by PGD-AT on 500 samples sorted by the loss of the first model.
Figure 8: The natural and robust test accuracy curves (under PGD-10) of PGD-AT, TRADES, and their extensions by integrating theproposed TE approach. The models are trained on CIFAR-10 un-der the '∞ norm With e = 8/255 based on the ResNet-18 archi-tecture.
