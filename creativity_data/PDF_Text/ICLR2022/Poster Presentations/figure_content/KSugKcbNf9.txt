Figure 1: A Visualization of Prior-Data Fitted Networks (PFNs). We sample datasets from a prior andfit a PFN on hold-out examples of these datasets. GiVen an actual dataset, we feed it and a test pointto the PFN and obtain an approximation to Bayesian inference in a single forward propagation.
Figure 2: (a) A visualization of the Transformer for n = 3 input pairs and m = 2 queries. Every baris the representation of one input and the arrows show what each representation can attend to. (b-c) Avisualisation of the Riemann distribution, with and without bounded support.
Figure 3: (a) The PFN's PPD given two evaluations of the function, highlighting the binning it itspredictions (best viewed in full-screen mode). (b) The PFN's and the GP's mean and confidenceintervals, which are nearly identical (red arrows mark tiny exemplary differences). For additionalcomparisons, see Figure 7 in the appendix.
Figure 4: (a) Prior-Data NLL with a fixed GP. While, the PPD of the GP is exact, we can see that ourapproximations get very close to it. (b) Prior-Data NLL with a mix over GP hyperparameters. Thereis no closed-form solution for this PPD, but we can see that our PFN still gets closest to the true PPD.
Figure 5: Time spent for Bayesian Inference per dataset compared to Prior-Data NLL. For NUTSwe scale the number of warmup and sampling steps, for SVI, we scale the number of training stepsand averaged samples. PFN is evaluated at only one setting (blue dot). We show two different BNNarchitectures (a) a BNN with 3 features, 2 layers and a hidden dimensionality of 5 and (b) a BNNwith 8 features, 2 layers and a hidden dimensionality of 64.
Figure 6: (a) A BNN. The inputsx are mapped to the output y . (b)A prior over BNN architectures.
Figure 7: We show the mean and 95% intervals of the original GP posterior in green and thecorresponding PFN approximations in blue. We used a length scale of 0.1.
Figure 8: Like in plot 7, we show the mean and 95% confidence intervals for the exact GP prior ingreen and for our PFN in blue. (They are almost indistinguishable, with very small differences visible,e.g., at x = 0.4.) To further highlight what a big step forward our method is, we additionally showthe respective values for the relatively new Attentive Neural Processes (ANP) (Kim et al., 2018). Wetrained both methods in very different environments, but ANP was clearly given the bigger budget.
Figure 9: In this figure We show the impact of different architectural decisions on the performance ofour method in fitting a Gaussian Process. We use the same Gaussian Process as in Section 5.1.
Figure 10:	Comparison of uncertainty calibration curves over all predictions on the evaluated datasets.
Figure 11:	A prior definition for a prior oVer straight strokes in python.
Figure 12: In this figure we show different samples from our prior for Omniglot data.
