Figure 1: Confidence reliability diagrams misrepresent the effective miscalibration.
Figure 2: Conf-ECE (dashed lines) and TL-ECE (solid lines) of four deep-net architectures onCIFAR-10, as well as with recalibration using histogram binning and temperature scaling. The TL-ECE is often 2-3 times the conf-ECE, depending on the number of bins used to estimate ECE, andthe architecture. Top-label histogram binning typically performs better than temperature scaling.
Figure 3: Illustrative example for SeCtion A.2. The numbers in plot (a) Correspond to the prediCtionsmade by g on a dataset D. If D were a test set, plots (b-e) show how it should be used to verify if gsatisfies the Corresponding notion of Calibration. Consequently, we argue that if D were a Calibrationset, and we want to achieve one of the notions (b-e), then the data shown in the corresponding plotsshould be the data used to Calibrate g as well.
Figure 4: Recalibration of a random forest using histogram binning on the class imbalancedCOVTYPE-7 dataset (class 2 is roughly 100 times likelier than class 4). By ensuring a fixed num-ber of calibration points per bin, Algorithm 8 obtains relatively uniform top-label calibration acrossclasses (Figure 4a). In comparison, if a fixed number of bins are chosen for all classes, the perfor-mance deteriorates for the least likely classes (Figure 4b).
Figure 5: Table 2 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 forfurther details. The captions summarize the findings in each case. In most cases, the findings aresimilar to those with B “ 15. The notable exception is that performance of N-HB on CIFAR-10 forTL-ECE while very good at B “ 15, is quite inconsistent when seen across different bins. In somecases, the blue base model line and the orange temperature scaling line coincide. This occurs sincethe optimal temperature on the calibration data was learnt to be T “ 1, which corresponds to notchanging the base model at all.
Figure 6: Table 4 style results with the number of bins varied as B P r5, 25s. See Appendix E.5 forfurther details. The captions summarize the findings in each case. In most cases, the findings aresimilar to those with B “ 15. In some cases, the blue base model line and the orange temperaturescaling line coincide. This occurs since the optimal temperature on the calibration data was learntto be T “ 1, which corresponds to not changing the base model at all.
Figure 7: Table 3 style results with the number of bins varied as B P r5, 25s. The captions summarizethe findings in each case, which are consistent with those in the table. See Appendix E.5 for furtherdetails.
Figure 8:	Table 5 style results with the number of bins varied as B P r5, 25s. The captions summarizethe findings in each case, which are consistent with those in the table. See Appendix E.5 for furtherdetails.
Figure 9:	Top-label histogram binning (HB) calibrates a miscalibrated random-forest on the classimbalanced COVTYPE-7 dataset. For the less likely classes (4, 5, and 6), the left column is bet-ter calibrated than the right column. Similar observations are made on other datasets, and so werecommend adaptively choosing a different number of bins per class, as Algorithm 8 does.
Figure 10: Sierpinski binning for L “ 3. The leftmost triangle represents the probability simplex∆2 . Sierpinski binning divides ∆2 recursively based on a depth parameter q P N.
Figure 11: Grid-style binning for L “ 3.
Figure 12: Canonical calibration using fixed and histogram binning on a 3-class version of theCOVTYPE-7 dataset. The reliability diagrams (left) indicate that all forms of binning improve thecalibration of the base logistic regression model. The recalibration diagrams (right) are a scatter plotof the predictions gpXq on the test data with the points colored in 10 different colors depending ontheir bin. For every bin, the arrow-tail indicates the mean probability predicted by the base model gwhereas the arrow-head indicates the probability predicted by the updated model h.
