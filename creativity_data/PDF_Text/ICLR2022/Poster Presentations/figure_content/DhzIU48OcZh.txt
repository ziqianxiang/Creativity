Figure 1: The P-Adapter Framework: P-Adapterssit between the embedding layer and the first at-tention layer of the LLM. They take as input theLLM embeddings and output continuous promptsthat are fed into the LLM. The LLM is frozen(embeddings included), while the parameters ofthe P-Adapter are trained. The adapter helps miti-gate variability among different phrasings and ty-pographic errors in the prompts (as shown in theexample inputs).
Figure 2: P-Adapters lie in between the LLM Embeddings and the rest of the model (1). We proposeend-to-end P-Adapters (2a) as well as a Mixture of Experts model (2b). Figures 2b and 2a are sub-stituted into the “P-Adapter” block in 1. The subject embedding is in blue; the [MASK] embeddingis red; embeddings generated by the P-Adapter are yellow; and other unmodified embeddings aregray. Dotted arrows represent inputs and outputs to model components and solid arrows representcopying from the input of the P-Adapter to its output.
Figure 3: The above results show which of the LLM’s embeddings are important to keep unmodified.
Figure 4: Results across our ID and OOD datasets across three LLMs. Each row is a different model,the x-axis is the dataset and error bars show standard deviation across either 3 runs. We can see thatthe oracle method outperformed all others, while the baseline and rewriter did the worst. The trendsare similar across all models—the only difference come from some RoBERTa MoE models havinghigher variance than others which is explained by some runs not finding the best P-Tuning templates.
Figure 5: Consistency across all LLMs and evaluation sets.
Figure 6: Distribution of number of templates across the 41 relations we tested. Minimum is 32,maximum is 123 and mean is 81.44.
