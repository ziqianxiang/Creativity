Figure 1: Hierarchical basis from the sparse grid construction using the hat function (1 -∣∙∣)+.
Figure 2: Shallow neural network with ReLU activation implementing the product function Qid=1 xiwithin e in infinity norm. The network has O(d2e-2 log ɪ) neurons on the first layer andO(e- 2 log ɪ) neurons on the second layer.
Figure 3: Deep neural network approximating a Korobov function f ∈ X2,∞ (Ω) within e. Thecomplete network has O(e- 1 (log ɪ) 3(d2 I)) neurons and depth「log? d] +1.
Figure 4: Approximation of a right-continuous increasing function (blue) in an interval [c, d] withine by a piece-wise linear function (red) with [d-cC pieces. The approximation is constructed using aregular subdivision of the y axis of step e and constructing a linear approximation in the pre-imageof each part of the subdivision.
Figure 5: Shallow neural network with ReLU activation approximating a Korobov function f ∈X2,∞(Ω) within e in infinity norm. The network has O(e-1(log ɪ)d+1) neurons on the first layer3(d — 1) ∣ιand O(e-1 (log ɪ) 2	) neurons on the second layer.
Figure 6: Deslaurier-Dubuc interpolets of degree 1, 2 and 3.
