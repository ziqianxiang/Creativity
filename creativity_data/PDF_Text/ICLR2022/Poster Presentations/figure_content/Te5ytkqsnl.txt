Figure 1: Consider an image of a dog being held by its owner. By removing the owner from theimage, we can study how much our model’s prediction depends on the presence of a human. Ina similar vein, we can identify which aspects of the dog (head, body, paws) are most critical forclassifying the image by ablating these parts.
Figure 2: Given an image of a flatworm, we remove various regions of the original image; mask-ing for ResNet, and dropping tokens for ViT. (Section 2.1): Irrespective of what subregions of theimage are removed (least salient, most salient, or random), a ResNet-50 outputs the wrong class(crossword, jigsaw puzzle, cliff dwelling). Taking a closer look at the randomlymasked image of Figure 2, we notice that the predicted class (crossword puzzle) is not totallyunreasonable given the masking pattern. The model seems to be relying on the masking pattern tomake the prediction, rather than the remaining (unmasked) portions of the image. (Section 2.2):The ViT-S on the other hand either maintains its original prediction or predicts a reasonable labelgiven remaining image subregions.
Figure 3: We measure the shift in output class distribution after applying missingness approxima-tions. Left: Fraction of images predicted as each class (on a log scale) before and after randomlyremoving 50% of the image. We display the most frequently predicted 30 classes after applyingthe missingness approximations. Right: Degradation in overall class entropy as subregions areremoved. As patches are blacked out, the ResNet’s predictions skew from a uniform distributiontoward a few unrelated classes such as maze, crossword puzzle, and carton. On the otherhand, the ViT maintains a uniform class distribution with high class entropy.
Figure 4: We plot the fraction of images wherethe prediction does not change as image regionsare removed. The ResNet flips its predictionseven when unrelated patches are removed, whilethe ViT maintains its original prediction.
Figure 5: We repeat the experiment in Figure4 with models retrained with missingness aug-mentations. Applying missingness approxima-tions during training mitigates missingness biasfor ResNets.
Figure 6: We iteratively remove image regions in the order of random, most salient, and least salient.
Figure 7: Examples of generated LIME explanations and masking the top 20 features. Since LIMErequires removing image features, it can be subject to missingness bias. We note that LIME expla-nations generated for standard ResNets seem to be less aligned with human intuition than ViTs orResNets retrained with missingness augmentations (See Appendix D.1 for more examples).
Figure 8: We plot the agreement (using Jaccard similarity) of top-k features across LIME explana-tions of 28 pairs of baseline colors. The result is averaged over the 28 pairs, and we display the 95%confidence interval over the pairs of colors. ResNet-50’s explanations are almost as consistent asrandom explanations. For ViT with dropping tokens, explanations are naturally always consistent.
Figure 9: We evaluate LIME explanations using the top-K ablation test on a ResNet and ViT bymeasuring the fraction of examples who keep their original prediction after removing the Top-Kfeatures. A sharper degradation indicates a more appropriate explanation for that model. Whilethe LIME scores on the ResNet are largely indistinguishable, the ViT shows clear differentiationbetween the different explanations.
Figure 10: We replicate the experiment in Figure 9, but instead use models where missingnessapproximations were introduced during training. This procedure fixes evaluation issues for ResNets,but does not substantially change the evaluation picture of ViTs.
Figure 11: Further examples of removing 75 16 × 16 patches from ImageNet images. The im-ages are blacked out for ResNet-50, and the corresponding tokens are dropped for ViT-S. WhileResNet-50 skews toward classes that are unrelated to the remaining image features (i.e crossword,jigsaw puzzle), the ViT-S either maintains its original prediction or predicts a reasonable label givenremaining image features.
Figure 12: Full experiments for removing 16 × 16 patches by blacking out (ResNet-50) or droppingtokens (ViT-S).
Figure 13: Bias experiments as in Section 3, with a ViT-T and ResNet-18.
Figure 14: Bias experiments as in Section 3, with a ViT-S and a robust ResNet-50.
Figure 15: Bias experiments as in Section 3, with a ViT-S and InceptionV3.
Figure 16: Bias experiments as in Section 3, with a ViT-S and VGG16.
Figure 17: Using different baseline colors for masking pixels.
Figure 18: Using the blurred image for the missingness approximation.
Figure 19: Using different baseline colors for masking pixels.
Figure 20: Using SLIC superpixels instead of patchesC.7 Comparison of dropping tokens vs blacking out pixels for ViTsWe compare the effect of implementing missingness by dropping tokens to simply blacking outpixels for ViTs. Figure 21, shows a condensed version of the experiments we did previously in thissection, but now including an additional baseline which is a ViT-S with blacking out pixels instead ofdropping tokens. We find that using either of the ViTs instead of the ResNet significantly mitigatesmissingness bias on all three metrics. However, dropping tokens for ViTs mitigates missingness biasmore effectively than simply blacking out pixels.
Figure 21: We compare dropping tokens vs blacking out pixels for ViTs.
Figure 22: Examples of LIME explanations24Published as a conference paper at ICLR 2022D.2 Top-k ablation test with superpixels.
Figure 23: Top K ablation test using superpixels instead of patches.
Figure 25: (a) Average Jaccard similarity of the set of non-person predictions before and after re-moving all people from the image. We plot over prediction thresholds for the tagging task. (b)Examples of removing people from MS-COCO images.
Figure 26: We plot the fraction of images where the prediction does not change as image regions areremoved for the CIFAR-10 dataset.
