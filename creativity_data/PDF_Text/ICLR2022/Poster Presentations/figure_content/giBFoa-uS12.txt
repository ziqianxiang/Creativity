Figure 1: An instance of the information flow in a k-level decision hierarchy between two agents i and j forcalculating their level-k strategies. Level-zero actions (e.g., ai,(0)) represent the naive non-strategic actions.
Figure 2: (Top Row) Team rewards obtained across episodes as training proceeds. The shaded regions representstandard error. Our Adv. InfoPG continually outperforms all baselines across all domains and in both trainingand testing (see Table 1). (Bottom Row) The MI ablation study results, comparing the MI variations betweenInfoPG and MOA (MI-based baseline) where InfoPG demonstrates a higher final average MI estimate across alldomains. The shaded blue region represents the area between InfoPG’s lower and upper bounds on MI.
Figure 3: The fraudulent agent experiment scenario (Fig. 3a) and training results (Fig. 3b) in the Pistonballdomain, comparing the team reward performance for Adv. InfoPG (Eq. 4), InfoPG (Eq.3) and MOA.
Figure 4: Action distributions of piston agents across 37 timesteps for the fraudulent agent experiment introducedin Section 6. Note that Piston 2 (not displayed) is the fraudulent agent with untrainable random policy.
Figure 5: Comparing the learned policies by InfoPG at convergence in the SC2 domain with k = 0 and k = 1.
Figure 6: Comparing the learned policies by InfoPG in the Multiwalker with k = 1, k = 2, and k = 3,Fig. 5a-5c, respectively. With k = 1, agents only learn to perform a split to balance the package on top andavoid falling. This is while with k = 3 agents learn to quickly walk forward. The middle stage of rationalization,k = 2, achieves an in-between policy where agents split to balance, but also wiggle forward slowly.
Figure 7: Scalability comparison between Adv. InfoPG and the best-performing baseline, MOA (Jaques et al.,2019), in the Pistonball domain with ten interacting agents. Adv. InfoPG outperforms MOA in both maximizingaverage individual and team reward performances.
Figure 8: Individual rewards obtained by each individual agents across episodes as training proceeds in the threeevaluation environments. Our Adv. InfoPG continually outperforms all baselines (Wen et al., 2019; Jaques et al.,2019; Zhang et al., 2018; Sutton & Barto, 2018) across all domains.
Figure 9: Instances of the utilized multi-agent cooperative environments. Domains are partsof the PettingZoo (Terry et al., 2020) MARL research library and can be accessed online athttps://www.pettingzoo.ml/envs. The StarCraft II (Vinyals et al., 2017), can be accessed fromDeepmind’s repository available online at https://github.com/deepmind/pysc2.
