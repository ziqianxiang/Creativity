Figure 1: An Embodied Instruction Following (EIF) task consists of multiple subtasks. (a) FILM methodoverview: The agent receives the language instruction and the egocentric vision of the frame. At every timestep, a semantic top-down map of the scene is updated from predicted depth and instance segmentation. Untilthe subgoal object is observed, a search goal (blue dot) is sampled from the semantic search policy. (b) Exampletrajectories: Trajectory of an existing model (HiTUT (Zhang & Chai, 2021)) is plotted in a straight green line,and that of FILM is in dotted red. While HiTUT’s agent travels repeatedly over a path of closed loop (thickgreen line, arrow pointing in the direction of travel), FILM’s semantic search allows better exploration and theagent sufficiently explores the environment and completes all subtasks.
Figure 2: FILM method overview. The “grouping” in blue, green, and yellow denote the coarseness of timescale (blue: at the beginning of the episode, green: at every time step, yellow: at a coarser time scale of every25 steps). At the beginning of the episode, the Language Processing module processes the instruction intosubtasks. At every time step, Semantic Mapping converts egocentric into RGB a top-down semantic map. Thesemantic search policy outputs the search goal at a coarse time scale. Finally, the Deterministic Policy decidesthe next action. Modules in bright green are learned; the deterministic policy (grey) is not.
Figure 3: The Language Processing module. (a): Two BERT models respectively predict the “type” andthe “arguments” of the instruction. (b): The predicted “type” from (a) is matched with a template, and the“arguments” of the template is filled with the predicted “argument.”in a particular 5cm × 5cm space. Thus, the C + 2 channels are a semantic/spatial summaryof the corresponding space. We use M = 240 (12 meters in the physical world) and C =28 + (number of additional subgoal objects in the current task). “28” is the number of “receptacle”objects (e.g. “Table”, “Bathtub”), which are usually large and easily detected; in the example ofFig. 1, there is one additional subgoal object (“Apple”). Please see Appendix A.2 on details of thedynamic handling of C.
Figure 4: Example visualization of semantic search policy outputs. In each of (a), (b), Top left: map builtfrom ground truth depth/ segmentation, Top right: map from learned depth/ segmentation, Bottom left: groundtruth “coarse” distribution, Bottom right: predicted “coarse” distribution. (a): While the true location of the“bowl” was on the upper left coffee table, the policy distributes mass over all furniture likely to have it on. (b):The true location of the faucet is on the sink and at the end of the bathtub. While the policy puts more massnear the sink, it also allocates some to the end of the bathtub.
Figure 5: Average number of subtasks completed until fail-ure, by task type (light green/ light blue respectively for validseen/ unseen). Dark green/ blue: average number of totalsubtasks in valid seen/ unseen.
Figure 6: Example trajectories of FILM with and without semantic search policy. Paths near the subgoalsthat were traveled 3 times or more are in straight red. The goal (which can be the search goal or an observedinstance of a subgoal object) is in blue.
Figure 7:	ALFRED overview. The goal is given in high level and low level language instructions. For andagent to achieve “success” of the goal, it needs to complete a sequence of interactions (as in the explanations inthe bottom of the figure) and the entailed navigation between interactions.
Figure 8: Semantic mapping module. Figure was partially taken from Chaplot et al. (2020b)We dynamically control the number of objects C for efficiency (because there are more than 100objects in total). All receptacle objects (for input to the semantic policy) and all non-receptacleobjects that appear in the subtasks are counted in C . For example, in an episode with the subtask[(Pan, PickUp), (SinkBasin, Put), (Faucet, ToggleOn), (Faucet, ToggleOff), (Pan, PickUp), (Table,Put)], all receptacle objects and ”Pan”, ”Faucet” will be the C objects indicated on the map.
Figure 9: Semantic search policy.
