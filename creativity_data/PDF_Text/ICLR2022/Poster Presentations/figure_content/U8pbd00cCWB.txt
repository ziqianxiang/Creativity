Figure 1: Given a single RGB image as the input (the first column), our model can predict its 3Dimplicit surface reconstruction (shown in six novel views in the last six columns). The test imagesfor the first 3 rows are downloaded from the Internet and the last 2 rows are from the pix3d dataset.
Figure 2: (a, b) Truncated SDF (TSDF) (Curless & Levoy, 1996) Voxelization results of the non-watertight ground truth meshes (each shown in two views). (a) is a simple sphere and (b) is a realscene from the ScannetV2 training data. After depth-fusion and internal-filling (Popov et al., 2020),the inside space of both geometries (a, b) remains empty (red arrows), causing severe noise fortraining the occupancy field or the SDF prediction model. This type of noise particularly affectsthe single-view prediction problem, as no additional predicted depth surface from other views areavailable. (c) As a result of learning the implicit prediction directly from the inaccurate and low-resolution TSDF voxels (due to engineering constraints on runtime loading and memory bottleneckfor the sufficiently dense pre-computed query point occupancy labels), the prediction result (DISN+ TSDFVox) is clearly inferior compared to our results (DGS). The surface color denotes the evalu-ation of the “precision”, with the larger blue region, the higher “precision”.
Figure 3: Illustration of the loss imposition for theoccupancy prediction scenario. (a) When learningfrom the ideal mesh for ShapeNet objects, we candirectly supervise the training with the accurateoccupancy labels. (b) On scans of real scenes withimperfections (Fig. 2(b)), the TSDF voxelizationproduces severe noise for training. Specifically, aconsiderable fraction of the objects are “empty”inside. (c) Our learning scheme with DGS allevi-ates these issues via enabling imposition of losseson the gradients all the way back to image fea-tures.
Figure 4: Overview of our learning framework (a) and differentiable gradient sampling (b, c, d, e).
Figure 5: Qualitative comparisons on one challenging test case on ScannetV2. For each predictedsurface with red and sky-blue colors, sky-blue indicates “positive precision” for that surface region,while red indicates “negative precision”. The ground truth surface is shown on the top-right cornerof each prediction with gold and navy-blue colors, navy-blue indicates “positive recall”, while goldindicates “negative recall”. The larger the blue region is, the higher the F1 score would be.
Figure 6: Qualitative comparisons on an unseen test image downloaded from the internet.
Figure 7: Two representative failure cases of our approach.
Figure 8: Convergence Analysis for the comparison between the closed-form (blue) and the numer-ical counterpart (red). Notably, the numerical counterpart does not observe loss drop in the first 10kiterations.
Figure 9: Quantitative comparison on the high-realism ShapeNet (without handpick: test case num-ber 0 and 100). The reconstruction result of each approach is visualized in six different views, withthe first view the same as the camera view, the first three views the same elevation as the cameraview, and the last three view horizontal view.
Figure 10: Quantitative comparison on the high-realism ShapeNet (without handpick: test casenumber 200 and 300). The reconstruction result of each approach is visualized in six differentviews, with the first view the same as the camera view, the first three views the same elevation as thecamera view, and the last three view horizontal view.
Figure 11: Quantitative comparison on the high-realism ShapeNet (without handpick: test casenumber 400 and 500). The reconstruction result of each approach is visualized in six differentviews, with the first view the same as the camera view, the first three views the same elevation as thecamera view, and the last three view horizontal view.
Figure 12: Quantitative comparison on the high-realism ShapeNet (without handpick: test casenumber 600 and 700). The reconstruction result of each approach is visualized in six differentviews, with the first view the same as the camera view, the first three views the same elevation as thecamera view, and the last three view horizontal view.
Figure 13: Quantitative comparison on the high-realism ShapeNet (without handpick: test casenumber 800 and 900). The reconstruction result of each approach is visualized in six differentviews, with the first view the same as the camera view, the first three views the same elevation as thecamera view, and the last three view horizontal view.
Figure 14: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the1st and 2nd test scene in ScannetV2). The reconstruction result of each approach is visualized insix different views, with the first view the same as the camera view, the first three views the sameelevation as thecamera view, and the last three view elevated view.
Figure 15: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the3rd and 4th test scene in ScannetV2). The reconstruction result of each approach is visualized insix different views, with the first view the same as the camera view, the first three views the sameelevation as thecamera view, and the last three view elevated view.
Figure 16: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the5th and 6th test scene in ScannetV2). The reconstruction result of each approach is visualized insix different views, with the first view the same as the camera view, the first three views the sameelevation as thecamera view, and the last three view elevated view.
Figure 17: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the7th and 8th test scene in ScannetV2). The reconstruction result of each approach is visualized insix different views, with the first view the same as the camera view, the first three views the sameelevation as thecamera view, and the last three view elevated view.
Figure 18: Quantitative comparison on the ScannetV2 (without handpick: the first frame of the9th and 10th test scene in ScannetV2). The reconstruction result of each approach is visualized insix different views, with the first view the same as the camera view, the first three views the sameelevation as thecamera view, and the last three view elevated view.
Figure 19: Additional Qualitative results of our model generalizing to unseen test images down-loaded from the Internet.
