Figure 1: Two Challenges in low-precision training. (a) An optimal format varies with dataset,quantization scheme, and model. (b) There is a performance gap between from-scratch trainingwith quantized weights and fine-tuning of a pre-trained full-precision model.
Figure 2: Training loss vs. proposed performance indicators. Blue dots represent full-precisiontraining runs, and yellow, green, and red dots represent training runs with 8-, 7-, and 6-bit formats,respectively.
Figure 3: Scatter plots displaying misalignment angles for 166 8-bit formats. The numbers next tofour selected data formats (INT8, FP152, FP143, and FP134) represent the loss measured in actualtraining.
Figure 4: Comparison of quantization schemes.
Figure 5:	Experimental results of hysteresis quantization.
Figure 6:	Computation flow of 8-bit low-precision training.
Figure 7:	Number of valid bits in various 8-bit formats.
Figure 8:	Scatter plots showing ∣Nδe | and ∣Nδa∣ of 166 8-bit quantization formats. The numbernext to the star represents the loss obtained through actual training.
Figure 9: Weight distribution of first layer of 2-layer LSTM and ResNet-18.
Figure 10: Top-1 Accuracy on ImageNet using a ResNet-18 model.
Figure 11: Top-1 Accuracy on ImageNet using a ResNet-101 model.
Figure 12: Top-1 Accuracy on ImageNet using a MobileNet-V2 model.
Figure 13: Perplexify on PTB using a 2-layer LSTM model.
Figure 14: Validation loss on VOC using a MobileNetV2 + SSDLite.
Figure 15: Quantized network structures.
Figure 16: Multi-way MAC unit with an adder tree.
