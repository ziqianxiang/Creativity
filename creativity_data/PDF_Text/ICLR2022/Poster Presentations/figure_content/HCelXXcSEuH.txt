Figure 1: Comparison of the diagonal approxima-tion by AdaHessian and OASIS over a randomsymmetric matrix A (100 × 100). left: Relativeerror (in Euclidean norm) between the true diag-onal of matrix A and the diagonal approximationby AdaHessian, Hutchinson’s method, and OASIS(the x-axis shows the number of random vectorssampled from the Rademacher distribution, seeSection 2. Moreover, this plot can be considered asthe representation of the error of the Hessian diag-onal approximation’s evolution over the iterationsof minimizing wT Aw, since A is fixed and sym-metric.); right: Diagonal approximation scale forAdaHessian and OASIS (y-axis), in comparisonto the true diagonal of matrix A (x-axis).
Figure 2: Adaptive Learning Rate (Logistic Regression with strong-convexity parameter 1over rcv1 dataset). left and middle: Comparison of optimality gap for AdaHessian Algorithm withmultiple learning-rate choices vs. OASIS Algorithm with adaptive learning rate (dashed-blue line);right: Comparison of the best optimality gap and test accuracy for AdaHessian Algorithm w.r.t. eachlearning rate shown on x-axis after 40 iterations vs. the optimality gap and test accuracy for ourOASIS Algorithm with adaptive learning rate after 40 iteration (dashed-blue line).
Figure 3: Comparison of optimality gap and Test Figure 4: Comparison of objective functionAccuracy for different algorithms on Logistic Re- (F (w)) and Test Accuracy for different algo-gression Problems.	rithms on Non-linear Least Square Problems.
Figure 5: Performance of SGD, Adam, AdamW, Adehessian and different variants of OASIS onCIFAR10 (left and middle columns) and CIFAR100 (right column) problems on ResNet-20 (leftcolumn), ResNet-32 (middle column) and ResNet-18 (right column).
Figure 6: Evolution of the optimality gaps of OASIS, AdGD and AdaHessian for '2-regularizedLogistic regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left toright: ijcnn1, rcv1, news20, covtype and real-sim.
Figure 7: Ending optimality gaps of OASIS, AdGD and AdaHessian for '2-regularized Logisticregression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to right:ijcnn1, rcv1, news20, covtype and real-sim.
Figure 8: Evolution of the testing accuracy of OASIS, AdGD and AdaHessian for '2-regularizedLogistic regression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left toright: ijcnn1, rcv1, news20, covtype and real-sim.
Figure 9: Maximum testing accuracy of OASIS, AdGD and AdaHessian for '2-regularized Logisticregression: λ = 0n1 (top row), λ = 1 (middle row), and λ = 10 (bottom row). From left to right:ijcnn1, rcv1, news20, covtype and real-sim.
Figure 10: Evolution of the objective F(w) (top row) and the ending F(w) (bottom row) of OASIS,AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20, covtypeand real-sim.
Figure 11: Evolution of the testing accuracy (top row) and the maximum accuracy (bottom row) ofOASIS, AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20,covtype and real-sim.
Figure 12: ResNet20 on CIFAR10 with and without weight decay. Final accuracy results can befound in Table 7.
Figure 13: ResNet32 on CIFAR10 with and without weight decay. Final accuracy results can befound in Table 7.
Figure 14: ResNet18 on CIFAR100 with and without weight decay. Final accuracy results can befound in Table 8.
Figure 15: Net DNN on MNIST with and without weight decay. Final accuracy results can be foundin Table 9.
Figure 16: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Logistic regression.
Figure 17: Sensitivity of OASIS w.r.t. (β2, α), Deterministic Non-linear Least Square.
Figure 18:	Sensitivity of OASIS vs. SGD w.r.t. learning rate η, CIFAR10 on ResNet20.
Figure 19:	Sensitivity of OASIS w.r.t. γ, CIFAR10 on ResNet20.
Figure 20:	Sensitivity of OASIS w.r.t. β2, CIFAR100 on ResNet18.
