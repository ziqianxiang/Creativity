Figure 1: Comparisons among the attention maps in the intermediate layers of DeiT-S generatedby the clean inputs, the adversarial inputs under PGD-20 attacks ( = 0.003), and the proposedPatch-Fool attack, respectively. In particular, we average the attention scores across all the attentionheads in each layer and visualize the attention score of each token for a given query token (thecenter patch in the red box in our show case), following (Kim et al., 2021). We can observe that thedifference in attention maps between clean and adversarial inputs generated by PGD-20 keeps smallacross different layers; In contrast, the proposed Patch-Fool notably enlarges the gap between cleanand adversarial attention maps, demonstrating a successful attack for ViTs.
Figure 2: Benchmark the robustness ofDeiTs and ResNets against Patch-Fool un-der different numbers of perturbed patches.
Figure 3: Visualizing the patch-wise adversarial transferability of Patch-Fool on top of DeiFB (left)and DeiT-S (right), where the robust accuracy when perturbing each patch with the attack generatedfor the center patch on the same image is annotated in the figure.
Figure 4: Visualizing the adversarial examples generated by Patch-Fool’s variants, including Patch-Fool with different number of perturbed patches (rows 2〜3), Sparse Patch-Fool with a total of 250perturbed pixels distributed in different number of perturbed patches (rows 4〜6), and Patch-Foolunder L? and L∞ constraint (rows 7〜8). Note that both the attack settings and the resulting robustaccuracy are annotated.
