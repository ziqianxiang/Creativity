Figure 1: Illustration of the proposed framework for the end-to-end sample probing of conditionalgenerative models. At each training iteration, we take synthetic training examples for some subsetof seen classes (probe-train classes) from the conditional generative models, train a closed-formsolvable zero-shot learning model (sample probing ZSL model) over them and evaluate it on the realexamples of a different subset of seen classes (probe-validation classes). The resulting cross-entropyloss of the probing model is used as a loss term for the generative model update.
Figure 2: The compute graph view of the proposed approach, at some training iteration t. The upperhalf shows sampling from the generative model and the lower half shows sample probing model loss.
Figure 4: t-SNE visualization of different unseen classes from FLO dataset. Each plot shows t-SNEembeddings of real samples (Ã— points), generated samples using TF-VAEGAN with sample probing( points) and those using the baseline TF-VAEGAN (Narayan et al., 2020) model without sampleprobing (N points).
