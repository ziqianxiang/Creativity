Figure 1: Interactions between learners A , B and teachers GA , GB in Self Play variations. In (a),A plays against variations of itself in a zero-sum game to automatically generate a curriculum. In(b), A acts as both an agent and a demonstrator by being rewarded for proposing hard goals to B . In(c), we separate the goal generation from the demonstrators and symmetrize the whole system.
Figure 2: Performance of different optimization methods on a toy landscape that is primarily flatexcept for a single gaussian centered at (-0.1, 0.1) shown by the contours. Red shows most recentlyproposed points. SAC is able to find the peak while the other methods get stuck in the flat regions.
Figure 3: SAC optimizing a non-stationary toy landscape that is primarily flat, but has a gaussianpeak starting at (-0.1, 0.1) and moving diagonally to (0.1, -0.1). Red shows most recently proposedpoints. Without the regret updates, the optimizer lags and cannot find the optimum.
Figure 4: Illustration of the five environments, Gid showing training goal space and Good showingtest goal space. The red points indicate what is used to evaluate whether a goal was reached ineach environment - the point mass coordinates for Point Mass Obstacle, the torso center locationfor Walker, the ball location for Reach/Toss, and the block location for Pick and Reach.
Figure 5: Success rates across environments on random goals sampled from Good . Results averagedacross 3 seeds for each method. Each round corresponds to goal generation, then rollouts, thenthe respective goal generator updates. Generally we find our method matches or outperforms allbaselines in out-of-distribution generalization, especially when tackling the harder locomotion task.
Figure 6: Success rates for three more challenging goals in Point Mass Obstacle, Walker, and Ma-nipulator, and corresponding sample trajectories. In Point Mass we learn to navigate around wallsfaster than other methods, in Walker we learn to run quickly (albeit by using its torso) whereas othermethods fail to succeed at all, and in Toss we learn to aim upwards accurately.
Figure 7: Success rates for Reach with a third impossible dimension added to the specified goalspace. We test on both in distribution and out of distribution randomly sampled goals in the 3D goalspace, where the third dimension is not feasible in the environment. Generally we find CuSP is leastaffected while the baselines generally performs worse with the higher dimensional space.
Figure 8: Progressive goal generation plots for Point Mass Obstacle after 500 rounds across all fourmethods. We see more progressive diversity from CuSP, whereas GoalGAN mostly stays on theouter border and ASP+BC gets stuck on the environment walls, limited by Alice’s abilities.
Figure 9: Full ablations of all components of CuSP from Table 1. Success rates are averaged across3 seeds.
Figure 10: Goal plots after 2000 rounds for Toss. The left plot shows the goals generated in theβ = 1, α = 0, No Replay Buffer condition and the right plot shows the goals generated in theT = 300, β = .1, Symmetrized condition from Figure 9. Red indicates more recent goals.
Figure 11: Success rates across environments on random goals sampled from Good, ablating forSAC-based goal generators, symmetrization and stale regret updates. We generally find that havingboth components improve success rates across the tasks, although the gains vary depending on howchallenging the environment is to begin with. In particular, having both the regret updates andsymmetrization helps most on the more challenging Walker tasks and the skill-specific tasks.
Figure 12: Comparison of CuSP with separately initialized and updated Alice and Bob, or a singleAlice. Success rates are averaged across 3 seeds.
Figure 13: Progressive goal generation plots for the Point Mass environment after the first 1000episodes, where eaCh goal is the final position of AliCe at the end of the episode and both agentsare initialized at the Center of the domain. We see that with the sparse version of ASP, the proposedgoals Converge on a single Corner over time and gets stuCk, whereas the dense implementation leadsto more diverse goals.
Figure 14: SuCCess rates aCross environments where ASP aChieved > 0 suCCess rates on randomgoals sampled from Good aCross eaCh of the ASP variants. For final results in the paper, we reportASP+BC (Dense, SAC).
