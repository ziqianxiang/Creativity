Figure 1: Test accuracy of logistic regression on quantized random features for different number offeatures m ∈ {102, 103,5.103, 104, 5.104}, with LP-RFF (8-bit and 1-bit, in black) (Zhang et al.,2019), NystrOm approximation (32 bits in red, 16 bits in green) (Williams & Seeger, 2001), versusthe proposed TRF approach (in blue), on the two-class Cov-Type dataset from UCI ML repo, withn = 418 000 training samples, ntest = 116 000 test samples, and data dimension p = 54.
Figure 2: Eigenvalue distribution (TOP) and eigenvector associated to the largest eigenvalue(BOTTOM) of the expected and centered kernel matrix K (blue) versus its asymptotic equiva-lent K (red) in Theorem 1, with σ(t) = max(t, 0). (LEFT) W having Gaussian entries and(RIGHT) W having Student-t entries with 7 degrees of freedom, for two-class GMM data withμa = [0a-i; 4; 0p-a], Ca = (1 + 4(a - 1)∕√P)Ip, P = 512 and n = 2048.
Figure 3: Testing mean squared errors (MSEs with ±1 std, LEFT) and running time (RIGHT)of random features ridge regression as a function of regularization parameter γ, p = 512, n =1024, ntest = 512, m = 5.104. With W distributed according to (4) and = [0.1, 0.5, 0.9], versusRFFs and KRR on a 2-class MNIST dataset - digits (7, 9). We find that do = 0.44 for TRFs andd0 = 0.39 for RFFs, making λ in Corollary 1 small. Results averaged over 5 independent runs.
Figure 4: Test mean square errors of ridge regression on quantized random features for differentnumber of features m ∈ {5.102, 103, 5.103, 104, 5.104}, using LP-RFF (Zhang et al., 2019), Nystromapproximation (Williams & Seeger, 2001), versus the proposed TRF approach, on the Census dataset,with n = 16 000 training samples, ntest = 2 000 test samples, and data dimension p = 119.
Figure 5: Testing MSE of kernel ridge regression as a function of regularization parameter γ,p = 512, n = 1024, ntest = 512, m = 512. Ternary function (with thresholds s-, s+ chosen tomatch gaussian moments of [cos, sin] function) with W distributed according to equation 4 withe ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR and RFF. GMM dataset with 从。=[0。-。4; 0p-a], Ca =(1 + 4(a - 1)/√p)Ip, P = 512, n = 2048. Results averaged over 5 independent runs.
Figure 6: Testing MSE of kernel ridge regression as a function of regularization parameter γ,p = 512, n = 1024, ntest = 512, m = 4096. Ternary function (with thresholds s- , s+ chosen tomatch gaussian moments of [cos, sin] function) with W distributed according to equation 4 withe ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR and RFF. GMM dataset with 从。=[0。-1；4; 0p-a], Ca =(1 + 4(a - 1)/√p)Ip, p = 512, n = 2048. Results averaged over 5 independent runs.
Figure 7: Testing MSE of kernel ridge regression as a function of regularization parameter γ,p = 512, n = 1024, ntest = 512, m = 104. Ternary function (with thresholds s-, s+ cho-sen to match gaussian moments of [cos, sin] function) with W distributed according to equa-tion 4 with e ∈ {0.1,0.3,0.5,0.7,0.9} versus KRR baseline and RFF. GMM dataset withμa = [0。-1；4；0p-a], Ca = (1 +4(a - 1)∕√p)Ip, P = 512,n = 2048.). Results averagedover 5 independent runs.
Figure 8: Testing mean squared errors (MSEs, LEFT) and running time (RIGHT) of kernel ridgeregression as a function of regularization parameter γ, p = 512, n = 1024, ntest = 512, m = 512.
Figure 9: Testing mean squared errors (MSEs, LEFT) and running time (RIGHT) of kernel ridgeregression as a function of regularization parameter γ, p = 512, n = 1024, ntest = 512, m = 104.
Figure 10: Test accuracy using libsvm on different state-of-the-art random features kernels. a8a (UCI)dataset. Number of training samples n = 22696 - number of test samples nt = 9865, varying ratiolog m/p number of random features over dimension p = 123. Note that the y-axis is zoomed in tobetter distinguish the performance of different methods.
Figure 11: Test accuracy using libsvm on different state-of-the-art random features kernels.
Figure 12: Test accuracy using libsvm on different state-of-the-art random features kernels. Cov-Typedataset. Number of training samples n = 49990 - number of test samples nt = 91701, varying ratiolog m/p number of random features over dimension p = 22.
Figure 13: SVM test accuracy using different kernels. VGG-16 Embeddings of CIFAR10 dataset(Number of features p = 4096). Number of samples n = 1024 fixed, varying number of randomfeatures from m = 10 to m = 1800.
Figure 14: SVM test aCCuraCy using different kernels. Raw Fashion-MNIST dataset (Number offeatures p = 784). Number of samples n = 1024 fixed, varying number of random features fromm = 70 to m = 700. Note that the y-axis is zoomed in to better distinguish the performanCe ofdifferent methods.
Figure 15: SVM test aCCuraCy using different kernels. LeNet-64 Embeddings of Imagenet dataset(Number of features p = 2018). Number of samples n = 1024 fixed, varying number of randomfeatures from m = 10 to m = 1800.
