Figure 1: Isometric projection of train-ing with ERM (blue) vs. our objective(dark blue), using data from Figure 2.
Figure 2: All 3 domains (rows) consist of 3 types of inputs (columns): 1) x1, left: makes up for50% of each domain, label is always 0, x1 is always [0, 0, 0, 0]; 2) x2 , middle: makes up for 40% ofeach domain, label is always 1, x2 changes for each domain; 3) x3, right: makes up for 10% of eachdomain, labels are randomly assigned with 30% of y = 1 and 70% of y = 0, x3 is always [1, 0, 0, 0].
Figure 4: Performance of Fish, IDGM and ERM on CdSprites-N, with N ∈ [5, 50](b) Test(a) Train (b) TestFigure 3: CdSprites-N visual-ization. Each 3x3 grid (e.g. yel-low square) is one domain.
Figure 3: CdSprites-N visual-ization. Each 3x3 grid (e.g. yel-low square) is one domain.
Figure 5: Gradient inner prod-uct values during the trainingfor CdSprites-N (N=15).
Figure 6: Performance on CDSPRITES-N, with N ∈ [5, 50]generalization no example in the test dataset is seen by the model (K = 0); another importantdifference is that while domain generalization aims to train models that perform well on an unseendistribution of the same task, meta-learning assumes multiple tasks and requires the model to quicklylearn an unseen task using only K examples.
Figure 7: Results on WILDS using SmoothFish with γ ranging from 0 to 10.
Figure 8: Ablation studies on α and . Note that α × remains constant in all experiments, and themidpoint of each plot is the hyperparameter we chose to use to report our experiment results.
Figure 9:	Gradient inner product values during the training for CdSprites-N (N=15) and 5 differentWilds datasets.
Figure 10:	t-SNE plot for PACS, VLCS and OfficeHome. Colors represent labels, markers (shape ofeach datapoint) represent domain.
