Figure 1: Learning universal controllers. Given a modular robot design space, our goal is tolearn a controller policy, which can generalize to unseen variations in dynamics, kinematics, newmorphologies and tasks. Video available at this project page.
Figure 2: MetaMorph. We first process an arbitrary robot by creating a 1D sequence of tokenscorresponding to depth first traversal of its kinematic tree. We then linearly embed each token whichconsists of proprioceptive and morphological information, add learned position embeddings andencode the resulting sequence of vectors via a Transformer encoder. The output of the Transformeris concatenated with a linear embedding of exteroceptive observation before passing them througha decoder to output per joint controller outputs.
Figure 3: Environments. We evaluate our method on 3 locomotion tasks: Flat terrain, Variableterrain: consists of three stochastically generated obstacles: hills, steps and rubble, and Obstacles:cuboid shaped obstacles of varying sizes.
Figure 4: Joint pre-training of modular robots. Mean reward progression of 100 robots from theUNIMAL design space averaged over 3 runs in different environments for baselines and ablationsdescribed in § 5.2. Shaded regions denote standard deviation. Across all 3 environments, Meta-Morph consistently outperforms GNN, and is able to match the average reward achieved by permorphology MLP baseline, an approximate upper bound of performance.
Figure 5: Importance of dynamicreplay buffer balancing. We com-pare the percentage of robots with fi-nal performance greater than 75% ofthe MLP baseline performance whentrained jointly. Across all 3 environ-ments, on average for 10 - 15% robots,MetaMorph w/o balancing is unable tolearn a good control policy.
Figure 6: Zero-shot generalization. We create 400 new robots for each type of variation in dynam-ics parameters (armature, density, damping, gear) and kinematics parameters (module shape, jointangle). Bars indicate average zero-shot reward over 10 trials for 400 robots and error bars denote95% bootstrapped confidence interval. Across all types of variations, and environments we find thatMetaMorph considerably outperforms GNN, and MetaMorph-NM.
Figure 7: Fine tuning: New robot morphologies. Comparison of reward progression of 100 newrobot morphologies averaged over 3 runs for pre-trained MetaMorph in the same environment vsfrom scratch. Shaded regions denotes standard deviation. Across all environments pre-trainingleads to strong zero-shot performance and 2 - 3× more sample efficiency.
Figure 8: Fine tuning: New robot morphologies and tasks. Left: Test environments. Right:Comparison of reward progression of 100 test robots averaged over 3 runs for pre-trained Meta-Morph (VT → Escape, Obstacles → Obstacles (cylinders)) vs from scratch. Shaded regions denotesstandard deviation. Across all environments pre-training leads to strong zero-shot performance and2 - 3× savings in training iterations to achieve the same level of average reward.
Figure 9: Emergent motor synergies. We plot the stable rank of the attention matrix (sr(Al) ) fortwo agents performing locomotion on a flat terrain. sr(Al) is small and oscillates between two valueswhich correspond to attention maps where groups of limbs are activated simultaneously (denoted bydark columns), a characteristic signature of motor synergies.
Figure 10: Baselines and ablations. Mean reward progres-sion of 100 robots from the UNIMAL design space aver-aged over 3 runs in flat terrain for baselines and ablationsdescribed in § 5.2 and § B.1. Shaded regions denote stan-dard deviation.
Figure 11: Position embeddings. Cosine similarity of position embeddings of MetaMorph for 3different seeds in flat terrain environment.
