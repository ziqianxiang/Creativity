Figure 1: Left:f (x, y) = (4x2 - (y-3x+0.05x3)2-0.1y4)e-0.01(x +y ). Middle: -3x2-y2+4xy.
Figure 2: Figure 2a: The blue line is the spectrum of matrix G(Sim) while the red line is spectrumof matrix I - G(Sim). Our method transforms the divergent problem to a convergent problem dueto the transformed spectrum. Figure 2b: Convergence rate comparison between SimGDA-AM andEG for different condition numbers of and fixed table size p = 10, 20, 50. Figure 2c: Convergencerate comparison between SimGDA-AM and EG for increasing table size on a matrix with conditionnumber 100.
Figure 3: An illustration of thewhere c and rare the center and radius ofa disk D(c, r) which includesspectrum of G (red) and the clos-all the eigenvalues of G. Especially, C < 1.	ing circle (blue)in Theorem 42Theorem 4.2 shows that when p >log q2+ηIoil, alternating GDA-AM will converge globally.
Figure 4: Comparison in terms of iteration: minx maxy f(x, y) = xTAy + bTx + cTy. We usedifferent problem size and fix p = 10, η = 1 for all experiments.
Figure 6: Effects of table size p and step size η, n = 50010210010-210-4(θ-eωs⅛o-) E-Iou ①□UBωαR7OG-A EG-S-SimAM p=10∖-G- AItAM p=10-B-SimAM p=50-G- AItAM p=50÷ SimAM p=100/G AltAM P=IOO IFigure 5: Comparison between methods in terms of time.
Figure 5: Comparison between methods in terms of time.
Figure 7:I-ηBηATNumerical range of fixed-point operator (Simultaneous GDA-AM )G=-ηAI-ηCfor bilinear-quadratic games.
Figure 8: Additional Comparison between GDA-AM and EG with positive momentumD.2	1d Minimax functionsWe begin with investigating the empirical performance of GDA-AM for 6 non-trivial 1d bivariatefunctions. We set initial points as (3, 3) and m as 20 or 5 for all functions. We use optimal learningrates for all methods on each problem. Results are shown in Figure 9, 10, 11, 12, 13 and 14. Weobserve GDA-AM consistently outperforms all baselines and improves convergence. It is worthwhileto mention that the difference between GDA-AM and traditional averaging is twofold. First, traditionalaveraging does not involve an adaptive averaging scheme and thus blindly converge to (0, 0) for all1d bivariate functions. In contrast, GDA-AM obtains optimal weights by solving a small linear systemon past iterates. Using different weights for each iteration, GDA-AM is able to minimize the residualof past iterates and thus find the solution of a fixed-point iteration. More importantly, averagingdoes not change the GDA dynamic because averaging generates a new sequence of parameters basedon GDA iterates. This means averaging is independent with base training algorithm (GDA here).
Figure 9: f(x, y) = (x - 1 )(y - 1) + 1 e-(x-0.25) -(y-0.75) ). The optima for this function is not(0, 0). Because averaging blindly converges to (0, 0), it can never find the correct solution.
Figure 10: f(x, y) = (4x2 - (y - 3x + 0.05x3)2 - 0.1y4)e-0.01(x2+y2) . All baselines exceptaveraging are cyclying around the optima. Averaging is converging slowly.
Figure 11: f(x, y) = -3x2 - y2 + 4xy. Baselines tend to diverge. Averaging is converging slowlyagain because averaging can only blindly converge to (0, 0) and the optima for this function is (0, 0).
Figure 12: f(x, y) = 1 x3 + y2 + 2xy - 6x - 3y + 4.
Figure 13: f (x, y) = x3 - y3 - 2xy + 6.
Figure 17: FID (lower or ] is better) for CIFAR 10Figure 18: Left: IS for CIFAR10 using WGANGP. Middle: IS for CIFAR10 using SNGAN. Right:FID for CelebA using WGANGP.
Figure 18: Left: IS for CIFAR10 using WGANGP. Middle: IS for CIFAR10 using SNGAN. Right:FID for CelebA using WGANGP.
Figure 19: Generated Images for CIFAR10 and CelebA using WGAN-GP(ResNet)30Published as a conference paper at ICLR 2022Table 3:	ResNet architecture used for our CIFAR-10 experiments.
