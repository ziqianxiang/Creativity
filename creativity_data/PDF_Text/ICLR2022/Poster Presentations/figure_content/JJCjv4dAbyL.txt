Figure 1: NES perturbs the parameters of a black-box model k(μ) for N times via w, σ and performsparallel forward passes. Then, the model parameters are updated given the gradient estimation.
Figure 2: Wall-clock time as a function of model input size. Experiments are conducted on the NRI model(Left), projective parsing model (Center), and non-projective parsing model (Right). We observe that NESscales well with the latent space size in contrast to most of its competitors.
Figure 3: Left: The average absolute distance between the negative ELBO k(μ) and its Gaussianapproximation g(μ) as a function of the training epoch. The smaller σ is, the further the proximitybetween the two functions. Right: Negative ELBO as a function of the training epoch. Performanceimproves as the ELBO upper bound M increases. Result suggests that bounding a discrete VAE losswith a large enough M guarantees the convergence of NES within a finite number of iterations onthe one hand, and on the other hand, does not impair performance.
Figure 4: Negative ELBO as a function of the training epoch. Experiments are conducted on thebinarized FashionMNIST (Left), KMNIST (Center), and Omniglot (Right) datasets. NES does notrely on computing gradients and yet achieves comparable performance with the unbiased and GSMmethods. On the KMNIST benchmark NES even performs substantially better than GSM.
