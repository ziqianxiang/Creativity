Figure 1: We propose an extension of the Boltzmann rationality human model to a distribution overpolicies instead of trajectories. (a) A person walks to work each day, choosing route A, B, or C andreceiving varying rewards based on route length. Route B has the highest reward but for 4 days weobserve the person take route A. (b) In this setting, the Boltzmann trajectory model still predicts withhigh confidence that the person will take route B on day 5 (assuming the reward function is knownand cannot be updated). (c) In contrast, our model defines a distribution over policies, the Boltzmannpolicy distribution (BPD), whose PDF is shown here on the left. Using the BPD as a prior for theperson’s policy, we can calculate a posterior over policies after the first 4 days (shown on the right)and predict they will take route A with high probability on day 5. In other settings we consider, theBPD adapts in a matter of minutes to a human’s policy.
Figure 2: (a) Before deployment, we calculate the Boltzmaim policy distribution (BPD) as a prior overhuman policies by using a model of the environment and the human,s reward function. Crucially, thisstep requires no human data given that we know the human,s reward function. Even if the objective isnot known, but represented in a low capacity class, it can be inferred via IRL form very limited datain different environments from the target one (Ziebart et al., 2010). Note that using a high capacitymodel for the reward would become equivalent to imitation learning (Ho & Ermon, 2016) and losethe advantages of Boltzmann rationality, (b) At deployment time, we infer a posterior distributionover a human,s policy by updating the BPD prior based on human actions within the episode. Thisenables adaptive, online prediction of future human behavior, (c) We can also take actions adapted tothe posterior distribution in a human-AI collaboration setting.
Figure 3: Unlike Boltzmann rationality, the Boltzmann policy distribution captures a range of possiblehuman policies and enables deployment-time adaption based on observed human actions. In thisgridworld, the agent must repetitively travel to the apple tree in the upper right corner, pick an apple,and drop it in the basket in the lower left corner. (a) Using the methods in Section 2, we calculatean approximation to the BPD as a policy conditioned on two latent variables z1 and z2 . After thedistribution is calculated, z2 controls which direction the agent travels on the way to the tree and z1controls which direction the agent travels on the way back, capturing natural variations in humanbehavior. (b) After observing 0, 2, and 6 actions in the environment, we show the inferred posteriorover z and the resulting predicted action probabilities. The model quickly adapts to predict whichway the person will go around the obstacle in the future. (c) In contrast, the Boltzmann trajectorydistribution (Boltzmann rationality) cannot adapt its predictions based on observed human behavior.
Figure 4: A visualization of the Boltzmann tra-jectory and Boltzmann policy distributions at onestate. The density of the policy distribution isshown over a hexagon; each point correspondsto a distribution over the six actions. For smallα (on the left), the policies are close to determin-istic; here, policies almost always take either a2 ,a4, or a6, but rarely randomize much betweenthem. For large α (the middle), policies tend tobe random across all actions. The Boltzmanntrajectory distribution corresponds to a singlepoint at the MaxEnt policy.
Figure 5: A visualization of the mutual infor-mation (aka information gain) I(at, at0 | st, st0 )between actions taken at different timesteps t andt0 given the states at those timesteps under theBoltzmann policy distribution and Boltzmanntrajectory distribution (Boltzmann rationality).
Figure 6: Cross entropy(lower is better) of Boltz-mann rationality (BR) andthe BPD on synthetic hu-man data in the apple pick-ing gridworld from Figure3. See Section 3 for details.
Figure 7:	Prediction performance (cross-entropy, lower is better) of various human models on realhuman data for three Overcooked layouts. Error bars show 95% confidence intervals for the meanacross trajectories. See Section 3 for details and discussion.
Figure 8:	The Boltzmann policy distribution predicts real human behavior in Overcooked better thanBoltzmann rationality (BR) by adapting to systematically suboptimal humans. In each of the threelayouts, the shown state is reached multiple times in a single episode. The human repeatedly takes thesuboptimal action overlaid on the game. The predictions of the BPD and BR for each timestep whenthe state is reached are shown below the game, ignoring “stay” actions (see Appendix C.3). BPDadapts to the human’s consistent suboptimality while BR continues to predict an incorrect action.
Figure 9: Performance of various “robot” policies at collaborating with a behaviorally cloned “humanproxy” policy on three Overcooked layouts. Error bars show 95% confidence intervals for the meanreturn over trajectories. See Section 3 for details and discussion.
Figure 11:	The effect on human prediction of changing the concentration α of the Dirichlet basedistribution pbase(π) using to calculate the BPD. See Appendix C.2 for details and discussion.
Figure 12:	A variety of metrics for predictive power of various human models on the three Overcookedlayouts. Lower cross entropy and higher accuracy is better. The right two columns exclude “stay”actions, which make up the majority of the human data, from consideration. See Appendix C.3 fordetails and discussion.
