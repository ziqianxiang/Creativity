Figure 1: Why robust discriminationis effective. (a) Without adversar-ial perturbations, a non-robust discrim-inator can perfectly distinguish bothproxy distributions from the real distri-bution. (b) Under adversarial pertur-bations, classification between real andproxy-1 is much harder compared toreal and proxy-2 distribution. The suc-cess of a robust discriminator is depen-dent on the proximity of real and proxydistributions.
Figure 2: Certified robust-ness. Certified robust accuracyof baseline randomized smooth-ing technique, i.e., RST (Car-mon et al., 2019) and our workat different perturbation budgetsacross two different network ar-chitectures.
Figure 3: Calculating ARC.
Figure 4: Validating the upper bound from Theorem 1. The green line is the upper bound calculatedby Wasserstein-2. Note that the Wasserstein-1 (The bound of Theorem 1) is a tighter upper-boundbut it does not have a closed form for normal distributions.
Figure 5: Effectiveness of syntheticscore. We sort 6M DDPM images us-ing their synthetic score and divide theminto ten equal-size groups. We find thatgroups with lower score achieve highertransferred robust accuracy ('∞) on theCIFAR-10 test set.
Figure 6: Histogram of class labels forsynthetic images from DDPM model.
Figure 7: Hyperparameter search for γ inPORT.
Figure 8: Reduction in accu-racy vs robustness trade-off.
Figure 9: Sample complexity of adversarial training. Cleanand robust accuracy on the test set of synthetic samples whentrained on an increasing number of synthetic samples from theStyleGAN model. It shows that performance of adversarialtraining continues to benefit from increase in number of train-ing samples. We also measure generalization to the CIFAR-10dataset, which also improves with number of training samples.
Figure 10: Visualizing images from different sets. Randomly selected images form the CIFAR-10dataset and synthetic images from different generative models. Rows in each figure correspond tofollowing classes: Airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.
Figure 10: Visualizing images from different sets. Randomly selected synthetic images fromdifferent generative models. Rows in each figure correspond to following classes: Airplane, auto-mobile, bird, cat, deer, dog, frog, horse, ship, and truck.
Figure 11: ImageNet (64 × 64) samples along with synthetic images from two different generativemodels. Rows correspond to the following classes: hummingbird, english foxhound, arctic fox,basketball, iPod, ping-pong ball, running shoe, and water tower.
Figure 12: Real and synthetic images for the CelebA dataset. We consider a four-class classi-fication problem based on the attribute smile and male, i.e., not-smile/not-male, smile/not-male,not-smile/male, smile/male.
Figure 13: Real and synthetic images for the AFHQ dataset. Rows correspond to the following threeclasses: cat, dog, and wild animals.
