Figure 1: Extrinsic return of imitation learners as a function of the number of episodes of demonstra-tion data. For comparison, we also show expert performance. Confidence bars denote one standarderror.
Figure 2: Extrinsic return of imitation learner as a function of the number of environment interactions.
Figure 3: Intrinsic vs Extrinsicreturns7	ConclusionsWe have shown that, for deterministic experts, imitation learning can be performed with a singleinvocation of an RL solver. We have derived a bound guaranteeing the performance of the obtainedpolicy and relating the reduction to adversarial imitation learning algorithms. Finally, we haveevaluated the proposed reduction on a family of continuous control tasks, showing that it achievescompetitive performance while being comparatively simple to implement.
