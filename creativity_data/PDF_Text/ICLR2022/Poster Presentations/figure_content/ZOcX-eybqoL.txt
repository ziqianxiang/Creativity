Figure 1: PickUpObj domain.
Figure 2: Episodic returns (top) and learned binary representations (bottom) for test tasks M1 , M2and M3 after pretraining on the base set of tasks Ma , Mb, Mc and Md. The shaded regions on theepisodic returns indicate one standard deviation over 4 runs. The learned binary representations aresimilarly averaged over 4 runs, and reported for the first 500 episodes. The initial drop in DQNperformance is as a result of the initial exploration phase where the exploration constant decays from0.5 to 0.05. The Boolean expressions generated by SOPGOL during training for the respective testtasks are:M1	=	Ma ∧ Mb ∧ Mc ∧ Md ,M2	=	(Ma ∧ -Mb ∧ -Md) ∨ (Ma ∧ Mc ∧ Md) ∨ (-Ma ∧ Mb ∧ -MC ∧ -Md)	∨	(-Ma	∧—Mb ∧ —Mc ∧ Md),M3	=	(Ma ∧ Mb ∧ Mc) ∨ (Ma ∧ -Mb ∧ -Md)	∨ (Ma ∧ MC ∧ Md) ∨ (-Ma ∧ Mb ∧ -MC	∧—Md ) ∨ (—Ma ∧ —Mb ∧ —Mc ∧ Md ) ∨	(—Mb ∧ Mc ∧ —Md ).
Figure 3: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2and M3 after pretraining on the non-base set of tasks ■, ,■ and 鬟 The shaded regions on the episodicreturns indicate one standard deviation over 4 runs. The learned binary representations are similarlyaveraged over 4 runs, and reported for the first 500 episodes. The initial drop in DQN performance isa result of the initial exploration phase where the exploration constant decays from 0.5 to 0.05. TheBoolean expressions generated by SOPGOL for the respective test tasks are:二 一 一 -Mi	= —■ ∧ - ∧ ■ ∧ ―喀，二	Z	一	、	， 一	一	小，一	一	小，一	八M2	= (	∧ —■ ∧	— ) ∨	(—■ ∧ ■ ∧ —■ ∧ )) ∨ (— ∧ —■ ∧	■ ∧，) ∨ (—■	∧ - ∧ - %),/ — — 一、， 一 、, 一 小M3	= (—■ ∧ - ∧ —/)∨ (—■ ∧ — ) ∨ (—■ ∧ — ∧ —/).
Figure 4: Number of policies learned and samples required for the first 50 tasks of an agent’s lifetimein the Four Rooms domain. The shaded regions represent standard deviations over 25 runs.
Figure 5: 40 goals Four Rooms domain with goals in green and the agent in red.
