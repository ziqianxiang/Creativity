Figure 1: Eigencurve : piecewise in-verse time decay scheduling.
Figure 2: Power law observed inResNet-18 on ImageNet, both eigen-P(λ) = ɪ ∙ eχp(-α(ln(λ) - ln(μ))) = ɪ ∙ (μ)Z	Zλ(3.7)for some α > 1, where Z = RL(μ∕λ)αdλ, there exists avalue (x-axis) and density (y-axis) areplotted in log scale.
Figure 3: The estimated eigenvalue distribution of Hessian for ResNet-18 on CIFAR-10, GoogLeNeton CIFAR-10 and ResNet-18 on ImageNet respectively. Notice that the density here is all shownin log scale. First row: original scale for eigenvalues. Second row: log scale for preprocessedeigenvalues.
Figure 4: Example: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses.
Figure 5: Eigencurve ’s learning rate curve generated by the estimated eigenvalue distribution forResNet-18 on CIFAR-18 after training 50/100/200 epochs. The cosine decay’s learning rate curve(green) is also provided for comparison.
Figure 6: The eigenvalue distribution of Hessian for ridge regression on a4a. Left: original scale foreigenvalues. Right: log scale for eigenvalues. Notice that the density here is shoWn in log scale.
Figure 7: Learningscale.
Figure 8: CIFAR-10 results for ResNet-18, with #Epoch = 10. Left: training losses. Right: testaccuracy.
Figure 9: CIFAR-10 results for GoogLeNet, with #Epoch = 10. Left: training losses. Right: testaccuracy.
Figure 10: CIFAR-10 results for VGG16, with #Epoch = 10. Left: training losses. Right:accuracy.
Figure 11: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses. Right: testaccuracy.
Figure 12: CIFAR-10 results for GoogLeNet, with #Epoch = 100. Left: training losses. Right: testaccuracy.
Figure 13: CIFAR-10 results for VGG16, with #Epoch = 100. Left: training losses. Right: testaccuracy.
