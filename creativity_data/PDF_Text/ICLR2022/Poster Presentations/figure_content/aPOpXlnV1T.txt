Figure 1: Training a probabilistic neural network to fit a simple sinusoidal fails. Left: Learnedpredictions (orange line) after 107 updates, with the shaded region showing the predicted standarddeviation. The target function is given by y(x) = 0.4 sin(2πx) + ξ, where ξ is Gaussian noise with astandard deviation of 0.01. Right: Root mean squared error (RMSE) over training, mean and standarddeviation over 10 random seeds. For comparison, we plot the training curve when using the meansquared error as the training objective - achieving an optimal mean fit (dashed line) in 105 updates.
Figure 2: Illustration of the pitfall when training with NLL (negative log-likelihood) versus oursolution. An initial inhomogeneous feature space granularity (see Sec. 3.1) results early on in differentfitting quality. The implicit weighting of the squared error in NLL can be seen as biased data-samplingwith p(x) 8 σ2(X) (See Eq∙ 6)∙ Badly fit parts are increasingly ignored during training. On the right,the effect of our solution (Eq. 7) on the relative importance of data points is shown.
Figure 3: Model fit using the NLL loss at different stages of training shown in orange with ±σuncertainty band. Black dots mark training data. Fitting the function begins from the left and isvisibly slow.
Figure 4:	Jacobian variance over training time,using the mean of matrix V(X) (see Eq. 5).
Figure 5:	Probability of sampling a data point atinput X over training time.
Figure 6: Distribution of residual prediction errors depending on the loss function for the ObjectSlidedataset (see Sec. 5.2). Dashed lines show predictive RMSE. (a) The NLL loss (LNLL) yieldsmultimodal residuals. There is a long tail of difficult data points that are ignored, while easy ones arefit to high accuracy. (b) The MSE loss (LMSE) results in a log-normal residual distribution. (c) Ourβ-NLL loss (Lβ-NLL) yields highly accurate fits on easy data points without ignoring difficult ones.
Figure 7: Convergence properties analyzed on the sinusoidal regression problem. RMSE after 200 000epochs, averaged over 3 independent trials, is displayed by color codes (lighter is better) as a functionof learning rate and model architecture (see Sec. D.1). The original NLL (β = 0) does not obtaingood RMSE fits for most hyperparameter settings. Figure S3 shows results for the NLL metric.
Figure 8: Fits for the heteroscedastic sine example from Detlefsen et al. (2019) (a-e). Dotted linesshow the ground truth mean and ±2σ, respectively. (f) The predicted standard deviations (withshaded std. over 10 independent trials) with the same color code. Note that β = 0.5 and β = 1 graphslie on top of one another. Inside the training regime, all β-NLL variants (a-d) yield well-calibrateduncertainty estimates. Moment matching (e) significantly underestimates the variance everywhere.
Figure 9: Sensitivity analysis of loss functions to hyperparameters on the dynamics model learningtasks: ObjectSlide (a) and Fetch-PickAndPlace (b). The distributions over validation RMSE and NLLare shown as a function of hyperparameters, based on a grid search over different model configurations(see Sec. D.2). While the NLL loss is highly sensitive when evaluating RMSE, the β-NLL lossshows much less sensitivity and yields good results regardless of the exact configuration.
Figure S1:	Repeating the experiment from Fig. 1, i.e. training with NLL loss for 107 update steps.
Figure S2:	Using different optimizers to train on the sinusoidal from Fig. 1 with the NLL loss. Thecolor code indicates the mean RMSE over 3 independent trials per optimizer setting. Black indicatesthat all trials diverged. Training was done for 2 ∙ 106 update steps and used architecture 2 fromTable S5. The observed behavior is stable across optimization settings.
Figure S3:	Convergence properties analyzed on the sinusoidal toy regression problem. Same asFig. 7 but for the negative log-likelihood (NLL) criterion. Due to the bad mean fit, the original NLLloss (β = 0) is also bad for most hyperparameter settings. With β > 0.5 good fits are obtained formany settings. Also for β = 1, which corresponds to MSE for fitting the mean, good uncertaintypredictions are obtained with our β-NLL as testified by the low NLL scores.
Figure S4: Undersampling behavior of LNLL on Fetch-PickAndPlace. The plot shows how thedistribution of effective sampling probability evolves over training time, taken over 2000 fixedtraining points sampled at the initial epoch. The dashed blue histogram shows the distribution ofeffective sampling probabilities when using the squared residuals from a model trained with MSE lossLMSE . This gives a reference distribution that LNLL should roughly match, taking into account therelative hardness of prediction on different samples. The LNLL drastically undersamples compared tothe reference (note the log-scale), effectively never sampling some points.
Figure S5:	Generative modeling with variational autoencoders on MNIST and Fashion-MNIST. Theoverall first row shows inPuts from the test set. For each method, we Present Posterior Predictivemeans (i.e. reconstructions), the Posterior Predictive standard deviations, samPles from the PosteriorPredictive distribution, and finally samPles using the Prior N (0, 1). LNLL (σ2 = 1) refers to Gaussianlog-likelihood with a fixed variance of 1. Values are cliPPed to the interval [0, 1]. ExamPles are notcherry-Picked.
Figure S6:	Example results for depth regression on the NYUv2 dataset (Silberman et al., 2012).
