Figure 1: Left: Three different methods which when applied to a WideResnet 28-10 architecture (w/oBatch Normalization) enable training at larger learning rates: learning rate warmup, MetaInit, andadding normalization layers. Each point reports the final training loss after training with cosine decayfor 300 epochs. The test performance of the models closely matches the training loss behavior (seeFigure 7 of the appendix). Right: The evolution of the largest eigenvalue of the Hessian throughoutthe training for the No-BatchNorm model with and without warm-up.
Figure 2: Measurements of the Hessian max eigenvalue of different models at initialization (left)and during training (right). Divergent models are indicated with an X. On the left, we plot λ1 atinitialization along with the peak learning used during training. On the right, we plot η and λ1 at aspecified step in training. For divergent models on the right plot, we record the last learning rate andmax eigenvalue that occur before divergence is detected (defined as observing a NaN in the loss).
Figure 3: Top row: Measured λ1 at initialization (left) and mid training (right) for variants of theWideResnet 28-10 model on Cifar10. Both BatchNorm and MetaInit reduce λ1 at initialization.
Figure 4: Learning rate warmup “pushes” the optimization trajectory towards regions of flat-ter curvature. Solid lines correspond to the maximum eigenvalue of the Hessian throughout thetraining. Dashed lines correspond to 2/n During the warmup period, λι fluctuates close to the 2/nbound. The WMT models diverge for 0, 400 and 4000 warmup steps. The pre-activation Resnet-50model diverges quickly without warmup.
Figure 5: The mid-training conditioning is determined by the learning rate, not on the initial-ization method used. (a): We plot the Hessian max eigenvalue during training for two models, aDenseNet model trained with learning rate warmup and gradient clipping, and the same model initial-ized with GradInit. We also plot the Hessian max eigenvalue during the GradInit meta optimizationprocess. Both GradInit and training with warmup are able to reduce the large curvature of the model.
Figure 6: Large curvature models scale poorly with batch size. The four plots explore how theWideResnet models from Figure 2 scale with batch size. We look at three models, a low curvaturemodel (WideResnet with BatchNorm) a medium curvature model (WideResnet without BatchNorm),and a high curvature model (WideResnet w/o BatchNorm and 1.5 init scaling). The low curvaturemodel exhibits almost perfect (linear) scaling as the batch size increases, with the optimal learningrate increasing almost linearly with the batch size. The high curvature model shows almost nospeedups at larger batch sizes, with the optimal learning rate fixed at the largest value with does notdiverge. Top left: Steps required for each model to reach 85% accuracy, normalized by the stepsrequired at batch size 64. Top Right: Optimal learning rate for each batch size. Bottom Left: Stepsto 85% accuracy for each learning rate, broken down by the batch size for the BatchNorm model.
Figure 7: The test-time behavior of the different WideResnet models closely mirrors their trainingloss dynamics (Figure 1, left).
Figure 8: The curvature at initialization can be unreliable for predicting training stability. Left:Maximum eigenvalues of the Hessian throughout training. Dashed line correspond to 2∕η. Right:Evolution of the training loss for different models. The model with no warm-up (red) suffers fromtraining instabilities even though it is below the stability threshold at initialization.
Figure 9: Performance of the WideResnet 28-10 models trained on Cifar10 shown in Figure 2 of themain text. (left) Performance of models trained without learning rate warmup. (right) Performancewith learning rate warmup. When warmup is applied, all models reach comparable performance(though additional regularization is needed to match the generalization of the Batch Normalizedmodels).
Figure 10: Performance of the Resnet50 models trained on Imagenet shown in Figure 2 of the maintext. (left) Performance of models trained without learning rate warmup. (right) Performance withlearning rate warmup.
Figure 11: Performance of the Transformer models trained on LM1B shown in Figure 2 of the maintext. (left) Performance of models trained without learning rate warmup. (right) Performance withlearning rate warmup.
Figure 12: Training curve of the WideResnet 28-10 which diverged despite starting out in a regionwith λι < 2.0∕η. The parameters quickly leave the stable region and end UP in a region of highercurvature before diverging.
Figure 13: Performance of the Stride-(1,1) DenseNet models trained on Cifar10 shown in Figure 2of the main text. Top left: Performance of models trained without learning rate warmup. Theperformance of BN has high stochasticity due to the large initial curvature, depending on how wellthe model recovers from the catapult phase. Top Right: Performance of models trained with 1000steps of warmup. The BN variants now consistently outperform the non-BN variants, and can even betrained at higher learning rates despite the larger initial curvature. Second Row: Training curves andevolution of λ1 for the BN and non BN model at learning rate .01 without warmup. The BN variantexhibits catapult behavior early but recovers once the curvature drops later in training. Third Row:Same as second row, but now with 1000 steps of warmup. The catapult behavior of the BN model isreduced, and training is faster after the catapult phase. Bottom Row: Comparing BN with non BN ata higher learning rate of .22 with 1000 steps of warmup. The BN model still has catapult behavior,but after recovering trains successfully once the curvature is reduced. The non-BN model divergesduring the warmup phase just before step 1000.
Figure 14: Instabilities in model training are reflected in the loss curvature even for models trainedwith Adam. Left: Maximum eigenvalues of the preconditioned Hessian throughout the training.
Figure 15: Further evidence of learning rate warmup “pushing” the optimization trajectory towardsregions with reduced λ1 . Note the rate at which λ1 changes closely matches depends on the length ofthe warmup. Model shown is the non-BN WideResnet (standard init) trained at batch size 2048 withpeak learning rate of .1.
Figure 16: WU et al. (2018) predicts that the stability bound will become tighter than 2∕η at smallerbatch sizes. This plot confirms holds not only at convergence (as implied by their Theorem 1), butearly in training as well. For a small batch size (32), the mid training curvature hovers significantlybelow the 2.0∕η approximation. As the batch size increases to 2048, the mid training curvature islarger, approaching the 2.0∕η approximation. All curves show the WRN without batch norm, trainedwith the same learning schedule using 1000 steps of warmup.
