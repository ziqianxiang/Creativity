Figure 1: ExMix task sizes in log scale. Thedashed line is the 3 × 105 sampling rate cap.
Figure 2: Within-family correlations for each dataset in a task family, using models from Table 2.
Figure 3: How the ratio of C4 span denois-ing examples to ExMix affects SuperGLUEresults on ExT5BASE. The dashed line is per-formance without using EXMIX (R → ∞)2.5	Does adding more tasks help? Task scaling experimentsIn this section, we explore how model performancechanges as the number of tasks included in a mas-sive multi-task pre-training setup is scaled up. Wechoose random sets of 30, 55, and 80 tasks (each asuperset of the last), pre-train a base-sized modelfor 524k steps, and fine-tune them on SuperGLUE.
Figure 4: Scaling the number of tasks dur-ing multi-task pre-training generally helps.
Figure 5: SuperGLUE score of ExT5LARGEvs T5LARGE as a function of number of pre-training steps.
