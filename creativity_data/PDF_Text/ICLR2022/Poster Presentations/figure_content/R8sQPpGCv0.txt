Figure 1: Extrapolation: as the (validation-set’s) input sequence gets longer (x-axis), current po-sition methods (sinusoidal, rotary, and T5) show degraded perplexity (y-axis, lower is better), butour method (§3) does not. Models were trained on WikiText-103 with sequences of L = 512 (left)or L = 1,024 (right) tokens. T5 ran out of memory on our 32GB GPU. For more detail on exactperplexities and runtimes, see Tables 2 and 3 in the appendix.
Figure 2: A comparison of batched training, inference speed and memory use of the sinusoidal,rotary, T5 bias, and our ALiBi position methods. The speed differences between our method andthe sinusoidal are within 1% during training and 3% for inference, which is insignificant on ourhardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 inthis setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to1024) since we break batches into multiple updates. See Table 1 in the appendix for exact numbers.
Figure 3: When computing attention scores for each head, our linearly biased attention method, AL-iBi, adds a constant bias (right) to each attention score (qi ∙ kj∙, left). As in the unmodified attentionsublayer, the softmax function is then applied to these scores, and the rest of the computation is un-modified. m is a head-specific scalar that is set and not learned throughout training. We show thatour method for setting m values generalizes to multiple text domains, models and training computebudgets. When using ALiBi, we do not add positional embeddings at the bottom of the network.
Figure 4: ALiBi models trained and evaluated on varying sequence lengths on the WikiText-103validation set and the sinusoidal baseline (not evaluated on longer sequences). All of our modelsoutperform the sinusoidal ones even when trained on fewer tokens. Appendix Table 5 has exactperplexities, more ALiBi models (trained on fewer tokens), and results for rotary and T5 bias models.
Figure 5: On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) to-kens. The ALiBi models obtain strong results even though they use 6%-11% less memory since theytrain on shorter sequences. Appendix Table 11 shows memory use and end-of-training perplexities.
Figure 6: The ALiBi and sinusoidal models (with both L = 512 and 1024) trained for 50k updates (1epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. ALiBi achieves the bestresults at around 2L but maintains strong performance even up to 10000 tokens in these experiments.
Figure 7: Training speed of our model and the sinusoidal baseline trained on different amounts ofinput subsequence tokens L.
Figure 8:	The training speed and validation perplexity (with Lvalid = 3072) for ALiBi models andthe sinusoidal model trained with L = 3072. All our models trained on 512 or more tokens achievebetter perplexity than the sinusoidal model even though all of them (except the L = 3072) requireless time and memory to train.
Figure 9:	ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus.
Figure 10:	Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom;red) on a sequence of 8 words using a model with Lvalid = 4. Nonoverlapping evaluation is muchfaster since it requires just two inference passes (as opposed to the five passes required by the sidingwindow approach). But the sliding window approach provides more context for each prediction.
Figure 11: ALiBi models evaluated on different input lengths on WikiText-103 with sliding windowevaluation (with stride S = 1). Unlike results shown in Figure 4, where performance improves ineach of our models as we increase the validation sequence length, here performance stays relativelyflat as we increase Lvalid. This might mean that ALiBi increases performance when Lvalid > L notbecause it uses longer contexts, but because fewer tokens suffer from the early token curse. Note thatas in §2, the perplexity of the sinusoidal model explodes when Lvalid > L even when using slidingwindow evaluation.
