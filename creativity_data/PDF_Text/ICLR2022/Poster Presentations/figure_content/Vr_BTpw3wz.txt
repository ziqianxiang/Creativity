Figure 1: A conversational turn with many plausible responses. The input (blue) can be answeredbased on 3 equally context-relevant passages but only one possible response (yellow) is observed inthe training set based on only one of the pink label-relevant passages (outlined in black).
Figure 2: An overview of iterative closed-set training: We iterate through the outer-loop and calleach execution a round. At the beginning of the round we re-index the passage corpus using the latestretriever Pη(z|x) and guide-retriever Q(z∣x, y) to create a high-recall closed-set of top-r passagesfor each retriever and query. Then, in the fast inner loop, we train the models for multiple epochsby sampling passages from the fixed closed-set and recomputing the probability distributions. Thetrained models are then used in the next round.
Figure 3: Relevance and Groundedness of models trained on the Wizard of Wikipedia dataset: (left)success@k of retrieved passages w.r.t. rank and (right) Novel-F1 between decoded output and re-trieved passage w.r.t. retrieved passage rank. The ELBoLoss retriever is more effective at retrievingthe gold passage than the MarginalizedLoss retriever, especially when we consider the top-10passages for this one-to-many task. The ELBoLoss generators have higher overlap with top-kretrieved passages and the overlap increases as α decreases.
Figure 4: With MARGINALIZEDLOSS, the generator Pθ(y |x, Z) learns a sharp distribution for Nat-ural Questions (NQ) dataset (right) but learns a flatter distribution for a one-to-many open-endedgeneration task using the Wizard of Wikipedia dataset (WoW). The flatter distribution in the caseof WoW Generator shows that it has not learned label-relevance as well. Consequently, for WoWwe see a weaker retriever (left) that has a flatter distribution than NQ. (Left) Cumulative probabilityPη (z|x) w.r.t. rank for passages. (Right) Assuming a uniform prior P(z|x), the cumulative proba-bility Pθ(y|x, Z) w.r.t. rank for passages, plotted as P(z|x, y) Y P(y|x, Z)P(z|x). The gray dottedline shows a hypothetical model that assigns equal probabilities to all passages.
Figure 5: Analogous plots to Figure 4 but with ELBoLoss on the one-to-many Wizard of Wikipedia(WoW) dataset. Training with ELBOLOSS produces a sharp distribution for Q(Z|x, y) and subse-quently sharper Pη(z|x) and Pθ(y |x, Z) than MARGINALIZEDLOSS.
