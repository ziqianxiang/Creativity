Figure 1: Illustration of the overall approach: During inference, we construct the correspondingscaffolding tree and differentiable scaffolding tree (DST) for each molecule. We optimize each DSTalong its gradient back-propagated from the GNN and sample scaffolding trees from the optimizedDST. After that, we assemble trees into molecules and diversify them for the next iteration.
Figure 2: Example of differentiable scaffolding tree. We show non-leaf nodes (grey), leaf nodes(yellow), expansion nodes (blue). The dashed nodes and edges are learnable, corresponding to nodes’identity and existence. we and A share the learnable parameters {wb3, wb4, wb5|3, wb6|4, wb7|1, wb8|2}.
Figure 3: Oracle efficiency test. Top-100 average score v.s. the number of oracle calls.
Figure 4: Two steps in optimizing ''QED+SA+JNK3+GSK3β”.
Figure 5: Left: Most of the existing methods (including GCPN (You et al., 2018), Molecular-RNN (Popova et al., 2019), GraphAF (Shi et al., 2020)) use GNN as a graph generator. Specifically,(1) generate molecule; (2) evaluate learning objective (loss in deep generative model or reward inreinforcement learning); (3) back-propagate (BP) gradient to update GNN. In sum, the learningobjective is differentiable w.r.t. the GNN’s parameters.
Figure 6: All the substructures in the vocabulary set S, drawn from ZINC 250K database (Sterling &Irwin, 2015). It includes atoms and single rings appearing more than 1000 times in the ZINC250Kdatabase.
Figure 7: Assemble examples.
Figure 8: The optimization curves in de novo optimization experiments. The objective value (F) is afunction of iterations.
Figure 9: The first eight steps in the de novo optimization procedure of LogP. The model successfullylearned to add a six-member ring to each step.
Figure 10: Sampled molecules with the highest scores.
Figure 11: Normalized validation loss-epoch learning curves. For fairness of comparison, validationloss is normalized by dividing the validation loss at scratch (i.e., 0-th epoch) so that all the validationlosses are between 0 and 1. For most of the target properties, the normalized loss value on thevalidation set would decrease significantly, and GNN can learn these properties well, except QED.
Figure 12: Ablation study. Objective value (F) as a function of iterations. See Section D.4 for moredetails.
Figure 13: Visualization of chemical space covered during optimization. We used PCA to reduce thedimension of Morgan’s fingerprint. The gray points are the ZINC 250k data set. while colored pointsare generated molecules after corresponding iterations.
Figure 14: Interpretability analysis when optimizing LogP.
