Figure 1: Unified view of cross-entropy loss, focal loss, and PolyLoss. PolyLoss Pj∞=1 αj (1 -Pt)j is a more general framework, where Pt stands for prediction probability of the target class. Left:Polyloss is more flexible: it can be steeper (deep red) than cross-entropy loss (black) or flatter (lightred) than focal loss (green). Right: Polynomial coefficients of different loss functions in the bases of(1 - Pt)j, where j ∈ Z+. Black dash lines are drawn to show the trend of polynomial coefficients.
Figure 2: Training ResNet-50 on ImageNet-1K requires hundreds of polynomial terms to re-produce the same accuracy as cross-entropy loss.
Figure 3: The first polynomial plays an important role for training ResNet-50 on ImageNet-1K. (a) Increasing the coefficient of the first polynomial term (1 > 0) consistently improves theResNet50 prediction accuracy. Red dash line shows the accuracy when using cross-entropy loss.
Figure 5: PolyLoss improves EfficientNetV2-L by increasing prediction confidence Pt .
Figure 6: PolyLoss improves Mask R-CNN by lowering overconfident predictions. Mean andstdev of three runs are plotted.
Figure 7: Visualizing LPFoLly-1 and LPFoLly-1*in the PolyLoss framework.
Figure 9: Dropping leading polynomi-als reduces overfitting to the major-ity class. Pt during RetinaNet trainingare plotted. Top: overall. Bottom left:background. Bottom right: foregroundobject. Dark blue curves represents Ptfor cross-entropy loss. Blue curves rep-resents dropping the first polynomial inthe cross-entropy loss. Light blue curvesrepresents dropping both the first andsecond polynomials in the cross-entropyloss.
