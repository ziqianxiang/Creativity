Figure 1: We show the unbalanced bucket sizes and unbalanced query-key ratios with statistics of 10attention heads in the 3rd layer of a pre-trained Transformer. For each head, we assign the queriesand keys of first 1024 tokens in the WikiText-103 validation data into 4 LSH buckets. The bucketsare sorted for each attention head according to total bucket sizes (i.e., #query + #key).
Figure 2: We show the attention utility evaluation of four different sparse attention patterns. For L2Hand LSHs, we assign the queries and keys of first 1024 tokens in the WikiText-103 validation datainto 4 hash buckets. The causal mask is not used when calculating the attention weights.
Figure 3: Thejoint training diagram of LHA, where queries and keys generate attention weights totrain the learnable hash functions, while the hash functions generate the sparse attention patterns, onwhich queries and keys are trained towards down-stream tasks.
Figure 4: The training dynamics of the KL divergence and negative entropy in LHA for queries and keys.
Figure 5: The training and inference latency of LHA and softmax attention. The latency is measured on a singleNVIDIA Tesla A100 GPU with a batch size of 1.
Figure 6: We show the histogram of hash bucket statistics for all 16 × 10 = 160 attention heads in the apre-trained 16-layer Transformer. For each head, we assign the queries and keys of first 1024 tokens in theWikiText-103 validation data into 4 LSH buckets. (left) The histogram of the bucket sizes for all 160 × 4 = 640buckets. (right) The histogram of the query-key ratios for all 160 × 4 = 640 buckets. The dashed red linesdenote the optimal case where each hash bucket has the same size and same number of queries and keys.
