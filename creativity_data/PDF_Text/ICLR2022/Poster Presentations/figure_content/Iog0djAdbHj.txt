Figure 1: ExperimentS on a toy dataSet when learning from different ptar. In (b-c), the horizontalaxis represents |巾面一p*∣∣2, and the vertical axis is the generalization performance.OHT meansone-hot training (on Remp), LS means label smoothing, GT means ground truth training with p*, KDis knowledge distillation, and ESKD is early-stopped KD. The Spearman correlation coefficient forresults in (b) is -0.930 with p-Value 1.9 × 10-53; for (c) is 0.895 with p-Value 2.7 × 10-43.
Figure 2: Normalized (divided by ∖∕2) distance between output distribution q and p* during theone-hot training in different stages (left to right: initial, early stop, convergence). In these figures,Ex kq(x) - p* (x)k2 of Hypothesis 1 is the mean height of all points in the figure. We provide moreresults about the NNs trained under different supervisions using this fashion in Appendix F.
Figure 3: Learning path of samples with different base difficulty. Corners correspond to one-hotvectors. Colors represent training time: transparent at initialization, dark blue at the end of training.
Figure 4: Updates of q(Xo) over training.
Figure 5: Learning path on CIFAR10. In the first two panels we record qt for each batch, while inthe last panel we record it for each epoch. See Appendix E for the learning paths of more samples.
Figure 6: Filtering can refine the labels in both clean and noisy label case.
Figure 7: Test accuracy under different noise ratio σ. Solid lines are the means while shade regionare the standard errors for 3 runs with different random seeds (shaded range is the standard error).
Figure 8: How to project a probability vector on to the plane of Barycentric coordinate system.
Figure 9: Correlation between test accuracy and Iptar - p* I2 for different settings.
Figure 10: Correlation between test ECE and Ilptar - p* k2 for different settings.
Figure 11: The influence of label smoothing factor on LS (first row) and different τ on KD (otherrows) under two different settings of the toy dataset. It is clear that these hyper-parameters won’tinfluence the performance too much as long as they are in a good region.
Figure 12: First row: large teacher to small stu-dent; second row: self-distill. Left column: τ in alarger range; right column: τ in a small range.
Figure 13: The learning path of samples with correct and wrong labels.
Figure 14: Correlation between base difficulty and zig-zag score in four different toy datasets.
Figure 15: Random selection of samples in CIFAR10 with 1000 flipped labels.
Figure 16: Random selection of samples with high zig-zag score in clean CIFAR10.
Figure 17: Distance gap of each sample under different supervision: label smoothing, KD, ESKD,and ground-truth training.
Figure 18: Upper panels: how tr(At(xn)) changes during training. Each panel represents a specificxn . The panels are ordered by their integral difficulty, from left-to-right and up-to-down. The x-axis is the number of epochs, and the y-axis is tr(At (xn)). Lower panels: the correlation betweencos(xo, xu) and tr(K0). Panels are ordered by the integral difficulty of xo. The subtitle of the panelgives the sample ID, the class it belongs to (i.e., A, B, or C) and the color of their correspondingclass (i.e., A is blue, B is orange and C is green).
Figure 19: Behavior of the tracking model (with low pass filter on each parameters). ResNet18trained for 150 epochs on CIFAR100.
