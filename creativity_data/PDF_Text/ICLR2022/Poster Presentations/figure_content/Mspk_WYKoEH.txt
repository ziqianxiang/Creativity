Figure 1: Shown: one GNN-AK+ layer. For each layer, GNN-AK+ first extracts n (# nodes)rooted subgraphs, and convolves all subgraphs with a base GNN as kernel, producing multiple richsubgraph-node embeddings of the form Emb(i | Sub[j]) (node iâ€™s embedding when applying a GNNkernel on subgraph j). From these, we extract and concatenate three encodings for a given node j : (i)centroid Emb(j | Sub[j]), (ii) subgraph Pi Emb(i | Sub[j]), and (iii) context Pi Emb(j | Sub[i]).
Figure 2: GNN-AK-S with SUbgraPhDroP used in training. GNN-AK-S first extracts subgraphs andsubsamples m|V | subgraphs to cover each node at least R times with multiple strategies. Thebase GNN is aPPlied to comPute all intermediate node embeddings in selected subgraPhs. Contextencodings are scaled to match evaluation. SubgraPh and centroid encodings initially only exist forroot nodes of selected subgraPhs, and are ProPagated to estimate those of other nodes.
Figure 3: Two 4-regular graphs that cannot be distinguished by 1-WL. Colored edges are the dif-ference between two graphs. Two 1-hop egonets are visualized while all other rooted egonets areignored as they are same across graph A and graph B.
Figure 4: Two non-isomorphic strongly regular graphs that cannot be distinguished by 3-WL.
Figure 5: A pair of CFI graphs (A and B) zoomed at the (rewired) edge (u, v) of a base graph. Thebase graph is a degree-3 regular graph with separator size k + 1 for k-WL-failed case.
