Figure 1: Comparisons among data-efficient techniques. To enhance the invariance to data stochas-ticity, strong data augmentation has been proposed. Similarly, can we propose a novel approach tofurther enhance the invariance to model stochasticity?& Aila, 2017), UDA (Xie et al., 2020) and FixMatch (Sohn et al., 2020); 2) Encourage invarianceto model stochasticity with difference penalty for predictions of models generated from differentdropout (Laine & Aila, 2017) or initialization (Zhou & Li, 2005b), as well as models exponentiallyaveraged from history models, such as Mean Teacher (Tarvainen & Valpola, 2017).
Figure 2: Comparisons among data-efficient learning techniques. (a) Π-model: uses a consistencyregularization term on predictions of two different augmentations; (b) Pseudo-Labeling: leveragesthe model itself to generate labels for unlabeled data and uses generated labels for training; (c) DataDistillation: extends the dual-copies of Π-model into multiple transformations; (d) Mean Teacher:maintains the teacher model with an exponential moving average of model weights of the studentmodel and encourages consistency between them. (e) MCD: a minimax model originally proposedfor domain adaptation that focuses on enhancing invariance to model stochasticity. (f) χ-Model: aminimax model for improving data-efficiency by enhancing model stochasticity and data stochasticity.
Figure 3: The network architecture of χ-Model on the labeled and unlabeled dataset.
Figure 4: Decision boundaries of various data-efficient learning methods on Two-Moon.
Figure 5: t-SNE features on dSprites-Scream. (Red: labeled samples; Blue: unlabeled samples).
