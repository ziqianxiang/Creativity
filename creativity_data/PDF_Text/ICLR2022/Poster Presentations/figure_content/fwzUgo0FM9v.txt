Figure 1: Left (a): Expected proportion of a batch of 64 images perfectly recovered as a functionof number of bins added via an imprint block in front of a ResNet-18 on ImageNet. With only156 bins, an attacker can expect to recover over 50% of a batch of user images perfectly. Right(b): Probability of a successful “one-shot” attack on a batch of 4096 images as a function of masscaptured in the one-shot bin. An attacker can optimize their bin size given an expected batch size.
Figure 2: Left: Ground truth batch of 64 user images. Right: Analytic reconstruction for an imprintmodel with 128 bins in front of a ResNet-18. Gray reconstructions denote bins in which no datapoint falls.)6Published as a conference paper at ICLR 2022data modality, early linear layers can be a feature of an architecture anyway, in which case there iseven no model modification necessary, only parameter changes. The parameter alterations necessaryto trigger this vulnerability can furthermore be hidden from inspection until use. The layer can lay“dormant”, functioning and training as a normal linear layer initialized with random activations, aslong as the server desires. At any point, a party with access to the server can send out parameterupdates containing W*,b* and trigger the attack.
Figure 3: Left (a): A true user image from class “minibus”. Right (b): The reconstructed imagefrom class “minibus” captured via our one-shot attack from averages aggregated over 16,384 data-points. The PSNR is 161.36, i.e. a verbatim copy at machine precision. This user could potentiallybe identified via their recovered license plate, which was blanked out (by us!) to preserve privacy.
Figure 4: Left: Identification Success vs bin size. Right: Identification Success (via IIP score) vs.
Figure 5: Density of image brightness estimated from access to different amounts of data from theImageNet dataset.
Figure 6: Distributions on the ImageNet validation set for several linear query functions. From leftto right: Mean compared to normal distribution, mean compared to Laplacian distribution, 32ndDCT coeffcient compared to Laplacian distribution, random normal vector compared to normaldistribution. In each plot the approximate distribution used by the attacker is visualized in blue/blackand the ground-truth (GT) distribution in green/red.
Figure 7: Analytic reconstruction for an imprint model with 128 bins in front of a ResNet-18. Left:The linear function is the 32nd DCT coefficient and bins are based on a Laplacian distribution.
Figure 8: Optimization-based attack of Geiping et al. (2020) for a ResNet-18 and a batch size of 64,the setting of Fig. 2. Left: An honest server model. Right: Gradient inversion attack applied to amodule that contains the imprint module.
Figure 9: Expected number of data points recovered for several batch sizes and increased bins andcorresponding parameter increase. Model: ResNet50 with ImageNet, targeting the input to the 3rdresidual block.
Figure 10: Left: Raw data for the 64 ImageNet images with separate classes. Right: Raw data forthe 64 images from the white shark class.
Figure 11: Left: Analytic reconstruction for a linear model of 64 ImageNet images with separateclasses (PSNR: 36.45 versus true user data). Right: Same recovery algorithm but for 64 imagesfrom the same class (white shark), (PSNR: 13.84 versus true user data.)T簿唧辔α⅜'博罕QSSaIRSS宜善增ΔΔaM善■S净靠温智叫"Figure 12: Left (a): A batch of 64 CIFAR10 images. Right (b): The same batch of images recon-structed naively using Eq. (2).
Figure 12: Left (a): A batch of 64 CIFAR10 images. Right (b): The same batch of images recon-structed naively using Eq. (2).
Figure 13: Left (a): A batch of 64 CIFAR10 images. Right (b): The same batch of images recon-structed using the imprint module with 300 bins. Gray images can result from collisions within agiven bin.
Figure 14: Different placements of the imprint module in a ResNet-18. From left to right: Beforethe first block (56x56), before the third block (28x28), before the fourth block (14x14) and beforethe last average pooling (7x7). Compare to a placement before the first convolution (224x224) andraw input data in Fig. 2. Enlarged versions of each panel can be seen in Figs. 15 - 1822Published as a conference paper at ICLR 2022Figure 15: Different placements of the imprint module in a ResNet-18. Before the first block (56 ×56). Only the first three channels at this position are utilized in this demonstration.
Figure 15: Different placements of the imprint module in a ResNet-18. Before the first block (56 ×56). Only the first three channels at this position are utilized in this demonstration.
Figure 16: Different placements of the imprint module in a ResNet-18. Before the third block(28 × 28). Only the first three channels at this position are utilized in this demonstration.
Figure 17: Different placements of the imprint module in a ResNet-18. Before the fourth block(14 × 14)Figure 18: Different placements of the imprint module in a ResNet-18. Before the last average-pooling layer (7 × 7). Only the first three channels at this position are utilized in this demonstration.
Figure 18: Different placements of the imprint module in a ResNet-18. Before the last average-pooling layer (7 × 7). Only the first three channels at this position are utilized in this demonstration.
Figure 19: Results for federated averaging for 8 steps with 8 images each, i.e. 64 unique data pointsfor a single user, and 128 bins. PSNR: 32.65. IIP: 70.31%. Drift of bins during local updates leadsto a few duplicated entries.
Figure 20: Left: IIP score vs Laplacian gradient noise. Right: Exemplary recovery for σ = 0.01.
Figure 21: Comparison to the ”Curious Abandon Honesty” (CAH) attack proposed in Boenischet al. (2021). We find our attack outperforms their attack at every scale.
