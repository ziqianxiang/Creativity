Figure 1: Overview of our approach. (1) we evaluate pre-trained SSL models on an image collec-tion, extracting and quantizing feature vectors to obtain clusters (■	“■), (2) We label the imagedata with a diverse set of concepts ( ■	…	)from expert models trained with supervision onexternal data sources, and (3) we train a linear model hθ to map concepts to clusters, measuring themutual information between the representation and human-interpretable concepts.
Figure 2: Forward vs. reverse probing. A dataset X embedded in R2 by two different represen-tations, with meaningful clusters in representation space. Each data point has two binary attributes,color y1 (x): red or blue and shape y2 (x): or . Both representations separate these attributes;however, color is not linearly separable in (b), so forward linear probing cannot recover this rela-tionship. Decision boundaries are shown for the forward linear probes. On the contrary, our reverseprobing easily discovers that all combinations of shape and color map to well separated clusters.
Figure 6: Empirically estimated mutual information between fκ (x) and y(x) for varying K.
Figure 7: Performance of self-supervised methods (architecture: ResNet-50) for varying number ofclusters K. The ranking remains mostly consistent.
Figure 8: Clusters found by unsupervised methods, where each pair (i)-(ii) contains the same (orsimilar) ImageNet label(s). Confusion is high when probing with ImageNet categories alone, butis significantly reduced after including Object concepts from experts trained on COCO-thing andOpen Images (OID) categories. The word clouds show the difference in the regressor coefficientsfor each pair (absolute value is denoted by increasing font size with blue: (i) > (ii) and red: (ii) >(i)).
Figure 9: Clusters found by unsupervised methods, where each pair (i)-(ii) contains the same (orsimilar) ImageNet label(s). Confusion is high when probing with ImageNet categories alone, but issignificantly reduced after including Scene concepts from experts trained on Places-365 categoriesand SUN Attributtes. The word clouds show the difference in the regressor coefficients for each pair(absolute value is denoted by increasing font size with blue: (i) > (ii) and red: (ii) > (i)).
Figure 10: Clusters found by unsupervised methods, where each pair (i)-(ii) contains the same (orsimilar) ImageNet label(s). Confusion is high when probing with ImageNet categories alone, butis significantly reduced after including Material concepts from experts trained on COCO-stuff andMaterial in Context (MINC). The word clouds show the difference in the regressor coefficients foreach pair (absolute value is denoted by increasing font size with blue: (i) > (ii) and red: (ii) > (i)).
Figure 11: Clusters found by unsupervised methods, where each pair (i)-(ii) contains related Ima-geNet label(s). Using Texture concepts from experts on the Describable Textures Dataset (DTD)and 11 elementary colors helps reduce the confusion comparing to using only ImageNet categories.
