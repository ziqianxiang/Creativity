Figure 1: Transitions between lower-level policies for different subtasks often fail because one pol-icy π2 is not trained on states generated by another policy π1. A transition policy π1,2 starts fromthe state distribution induced by π1 at time of transition. We propose training π1,2 to produce statesand actions from the distribution of π2 using inverse reinforcement learning techniques.
Figure 2: HRL structure with transition policies and DQNs. The meta-controller has n pre-trainedlower-level policies {∏1,∏2,..., ∏n}. To transition between two policies (for example, ∏a and ∏b),the meta-controller signals a transition interval. We train a transition policy (πa,b) that activates atthe beginning of this interval and a DQN (qa,b) that determines when during the interval the switchfrom πa,b to πb should take place.
Figure 3: The sequential decision problem faced by the DQN during the transition interval. Thetransition interval runs from st, the ending state of πa, to st+k. The DQN qa,b is active until itoutputs a switch action, which passes control from the transition policy πa,b to πb. The switchreward depends on whether πb achieves the subgoal: a reward of rs is given for success a rewardof rf for failure. A reward of 0 is given for the stay action unless the agent enters a failure state, inwhich case rf is given.
Figure 4: Three classes of distributions projected into 2D space with t-SNE. Both figures illustratethe actions of two pre-trained policies and the corresponding transition policy connecting them in atransition interval. The left figure shows for Walking forward→Balancing, and the right one standsfor Tossing→Hitting. The actions from the transition policy lie between those of the pre-trainedpolicies・ Tossing ∙ Hitting ∙ T ransition policy6	ConclusionWe have presented an approach for HRL in which an agent utilizes pre-trained policies for simplertasks to solve a complex task. Transitions between pre-trained policies are essential; hence, pre-trained policies are often fine-tuned to suit a complex task (Frans et al., 2018). Instead of fine-tuning, we introduce transition policies (Lee et al., 2019) to connect the pre-trained policies. Sinceour transition policy learns the partial distribution of the following pre-trained policy through IRL,we do not need to create a complicated reward function for a transition policy. A DQN is usedto detect states with a high transition success rate as additional assistance. HRL frameworks oftenhave trouble training the structure when rewards are sparse. We alleviate this issue by exposing onlyexposing a DQN with a binary action space to the sparse reward signal. The DQN is less susceptibleto the sparse reward issue than the transition policy with continuous control. In our evaluation,we find the proposed method to be competitive with previous work in arm manipulation and muchstronger in bipedal locomotion.
