Figure 1: Representation visualization of four languages: English (en), Spanish (es), Arabic (ar) andSwahili (sw) based on XLM-R. We plot the sentence representation of the XNLI test set, which isparallel across 15 languages. We average hidden states of the last layer to get sentence representationsand implement the dimensionality reduction by PCA. Obviously, the cross-lingual representationdiscrepancies are large in translate-train, but X-Mixup reduces the discrepancy significantly.
Figure 2: The model architecture of X-MIXUP, where the cross-lingual manifold mixup process is inthe green block. Note that the manifold mixup process is implemented only in a certain layer (thesame layer of both sides), and in other layers the process is omitted.
Figure 3: Performances on PAWS-X and XNLI test set, where languages are sorted by decreasingCKA scores. The trend indicates the performance gets worse along with the CKA score decreasing.
Figure 4: Language centroids visualization of the POS test set, which indicates X-Mixup bringscloser these centroids obviously.
Figure 5: Performances on implementing X-Mixup (solid line) in different layers and Trans-train(dashed line) on three downstream tasks.
