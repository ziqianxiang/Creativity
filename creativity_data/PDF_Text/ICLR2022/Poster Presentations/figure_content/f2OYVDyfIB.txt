Figure 1:	The predictability and unpredictability of pre-training versus fine-tuning. While theupstream pre-training performance measured by negative log-perplexity scales with model size quiteindependently from the model shape, the downstream performance (SuperGlue (avg) score) does not.
Figure 2:	Downstream scaling properties is scale-dependent. The downstream performance onSuperGLUE has qualitatively different scaling properties across models sizes. From left to right, wefine-tune model configurations closely matched to T5-Small, T5-Base and T5-Large.
Figure 3: Different scaling with respects to different knobs, in upstream and downstream. On theplots, DM refers to scaling model dimension, FF refers to scaling FFN hidden size, NH is number ofheads, and NL is number of layers. Size of each circle indicates the model size in terms of number oftrainable parameter parameters.
Figure 4: Compute-Performance trade-off when scaling model depth of different starting points(Small, Base, and Large).
Figure 5:	Performance on upstream and different downstream tasks with respect to number ofparameters, FLOPs, and throughput for models PreSented in Figure lb.
Figure 6:	PerformMnce on upstream and different downstream tasks with respect to number Ofparameters, FLOPs, and throughput for small models presented in Figure 2a.
Figure 7:	Performance on upstream and different downstream tasks with respect to number Ofparameters, FLOPs, and throughput for base models presented in Figure 2b.
Figure 8:	Performance on upstream and different downstream tasks with respect to number Ofparameters, FLOPs, and throughput for large models presented in Figure 2c.
