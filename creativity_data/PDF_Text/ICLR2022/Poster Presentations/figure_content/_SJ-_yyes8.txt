Figure 1: DrQ-v2 demonstrates significantly better sample efficiency and computational footprintcompared to state-of-the-art model-free methods for VisUal continUoUs control while being conceptU-ally simple and easy to implement. (Left two) AVerage performance resUlts across 12 challengingtasks from the DeepMind Control SUite (the set of tasks can be seen in FigUre 8). (Right two)Performance on the Humanoid Walk task from VisUal inpUt, preVioUsly UnsolVed by model-freemethods. In both cases we report sample complexity and wall-clock time axes for eValUation, withtime being measUred on a single GPU machine and Using official implementations for each method.
Figure 2: (Left): DrQ-v2 is an off-policy actor-critic algorithm for image-based RL. It alleviatesencoder overfitting by applying random shift augmentation to pixel observations sampled from thereplay buffer. (Right): Examples of walking and standing behaviors learned by DrQ-v2 for a complexhumanoid agent from DMC (Tassa et al., 2018) with 21 and 54 dimensional action and state spaces,respectively. DrQ-v2 does not have access to the internal state of the environment, only observingthree consecutive pixel frames at a time. Despite this imperfect observational channel, our agent stillmanages to solve the tasks. To the best of our knowledge, this is the first successful demonstration bya model-free method, using pixel-based inputs of these tasks.
Figure 3: We compare DrQ-v2 on a subset of continuous control tasks that offer various challenges,inclUding complex dynamics, sparse rewards, hard exploration, and more. (a) DrQ-V2 demonstratesfaVorable sample efficiency and comfortably oUtperforms leading model-free baselines, as well asreqUiring less wall-clock training image (b).
Figure 4: Model-based Dreamer-v2 needs to train a world model and thus performs more computationsdUring training than model-free DrQ-V2. Still, (a) DrQ-V2 is able to match Dreamer-V2â€™s sampleefficiency, while (b) reqUiring mUch less wall-clock training time.
Figure 5: An ablation study that led us to the final Version of DrQ-V2. We incrementally show eachof the four key improVements to DrQ that collectiVely form DrQ-V2. The silVer dotted curVes in thefirst row show the original DrQ. In subsequent rows they show progressiVe improVements, usingthe optimal choice from the preVious rows (i.e., the silVer curVe in the third row shows DrQ with aDDPG base RL algorithm and 3-step returns). The red and blue curVes show the effect of indiVidualmodifications. In the last row the blue curVe corresponds to DrQ-V2.
