Figure 3: AKD, AvgKD, and EKD methods for linear regression on synthetic data with same data(left), different data (middle), and strong regularization (right). EKD (black) eventually matchescentralized performance (dashed green), whereas AvgKD (solid blue) is worse than only local training(dashed blue and red). AKD (in solid red) performs the worst and degrades with increasing rounds.
Figure 4: Test accuracy of centralized (dashed green), and AKD on MNIST using model startingfrom agent 1 (blue) and agent 2 (red) with varying amount of regularization, model heterogeneity,and data heterogeneity. In all cases, performance degrades with increasing rounds with degradationspeeding up with the increase in regularization, model heterogeneity, or data heterogeneity.
Figure 5: Test accuracy of AvgKD on MNIST using model starting from agent 1 (blue) and agent2 (red) with varying model heterogeneity, and data heterogeneity. During training regularization isused. In all cases, there is no degradation of performance, though the best accuracy is obtained byagent 1 in round 1 with only local training.
Figure 6: Test accuracy on MNIST with varying data heterogeneity in the setting of ’same model’. Incase of high data heterogeneity (small Alphas), both agents benefit from the AvgKD, PKD and EKDschemes. Moreover, AvgKD and EKD consistently outperform FedAvg scheme.
Figure 8: Test accuracy of on MNIST with varying data heterogeneity in the setting of ’differentmodel’. Performance of PKD and AKD degrade with degradation speeding up with the increase indata heterogeneity. Performance of AvgKD scheme converges to steady behavior at any regime ofdata heterogeneity. Both agents benefit from AvgKD, PKD and EKD schemes in the early rounds ofcommunication.
Figure 9: Test accuracy of on CIFAR10 with varying data heterogeneity in the setting of ’samemodel’. As on MNIST: performance of PKD and AKD degrade with degradation speeding up withthe increase in data heterogeneity; performance of AvgKD scheme converges to steady behavior;both agents benefit from AvgKD, PKD and EKD schemes in early rounds of communication.
Figure 10: Test accuracy of on CIFAR10 with varying data heterogeneity in the setting of ’differentmodel’. All the schemes behave similarly to the ’same model’ setting.
Figure 11: Test accuracy of AKD on MNIST using CE loss and model starting from agent 1 (blue)and agent 2 (red) with varying amount of regularization, model heterogeneity, and data heterogeneity.
Figure 12: Test accuracy of AvgKD on MNIST using CE loss and model starting from agent 1 (blue)and agent 2 (red) with varying model heterogeneity, and data heterogeneity. In all cases, there is nodegradation of performance, though the best accuracy is obtained by agent 1 in round 1 with onlylocal training.
Figure 13: Test accuracy of AKD and AvgKD on MNIST using models MLP (blue) and RF (red) withvarying data heterogeneity. For AKD performance degrades with increasing rounds. Degradationis speeding up with the increase in data heterogeneity. For AvgKD there is no degradation ofperformance.
Figure 14: Test accuracy of AvgKD on MNIST using models MLP (blue) and RF (red) with varyingdata heterogeneity. The increase in data heterogeneity lowers the performance of both agents withoutdegradation trend through rounds.
Figure 15: Test accuracy of AvgKD with M agents on MNIST with varying data heterogeneity inthe setting of ’same model’. All agents can benefit from the distilled knowledge in early rounds ofcommunication.
