Figure 1: Performance (y axis), speed (X axis), and memory footprint (circle sizes) of efficient transformerson the Long-Range Arena benchmark. The proposed cosFormer achieves an all-around supremacy overcompeting methods in the top left quadrant.
Figure 2: Illustration of the computations for vanilla self attention (left) and linearized attention (right). Theinput length is N and feature dimension is d, with d q N. Tensors in the same box are associated forcomputation. The linearized formulation allows O(N) time and space complexity.
Figure 3: (1): Attention matrix of vanilla transformerã€‚):Attention matrix of COSFORMER .(3): Attentionmatrix of COSFORMER without re-weighting. (4): Visualization of the cos-based distance matrix. After re-weighting, We can see a smoother attention distribution along the diagonal region of attention matrix, exhibitinga similar pattern to the vanilla transformer, which assists to stabilize the training.
Figure 4: Training loss (left) and validation loss (right) of the bidirectional language modeling pre-train. Inboth training and validation, the proposed cosFormer has a faster converge speed than vanilla transformer.
