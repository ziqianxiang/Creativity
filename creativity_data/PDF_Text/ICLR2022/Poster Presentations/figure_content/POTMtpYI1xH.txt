Figure 1: Illustration of a hierarchical concept tree. Our new concepts represent novel aspects thatare not found in the pre-defined concepts.
Figure 2: Example Concepts. 2a shows a cluster that has a lexical property (hyphenation) anda morphological property (adjective). Similarly, 2b shares a common suffix est and additionallypossesses the morphological phenomenon, superlative adjectives. Finally, a large number of clusterswere grouped based on the fine-grained semantic information. For example, the cluster in 2c is anamed entity cluster specific to places in Germany.
Figure 3:	Examples of latent concepts learned in BERT5.2	Alignment with Pre-defined ConceptsA number of works on interpreting DNNs study to what extent various linguistic concepts such asparts-of-speech tags, semantic tags, etc are captured within the learned representations. This is inview of one school of thought which believes that an NLP model needs to learn linguistic conceptsin order to perform Wen and to generalize (MarasoviC, 2018). For example, the holy grail in machinetranslation is that a proficient model ought to be aware of word morphology, grammatical structure,and semantics to do Well (Vauquois, 1968; Jones et al., 2012). In this section, We explored hoW Wellthe latent concepts in BERT align With the pre-defined linguistic concepts.
Figure 4:	Layer-wise alignment of pre-defined concepts in BERT. The vertical axis represents thenumber of aligned clusters for a given concept, normalized by the maximum across all layers. Num-bers inside parenthesis show the maximum alignment for each concept.
Figure 5: Unaligned Conceptstwo pre-defined concepts, ii) concepts that cannot be explained via auto-labels but are meaningfuland interpretable, iii) concepts that are uninterpretable. Figure 5a and 5b are examples of the firstcategory mentioned above. The former describes a concept composed of different verb forms. Itdoes not fully align with any of the fine-grained verb categories but can be deemed as a verb clusterat coarser level. Similarly the latter shows a cluster containing singular and plural nouns (describedby POS:NN and POS:NNS) and WordNet concepts (WN:noun:food and WN:noun:plants) and canbe aligned with a coarser noun category. Figure 5c shows a cluster that does not match with anyhuman-defined concept but are interpretable in that the cluster is composed of compound words.
Figure 6: Example of a group of tokens representing a meaningful cluster.
Figure 7:	An example of the annotation interface for Q1.
Figure 8:	An example of the annotation interface for Q2.
Figure 9:	Example ConceptsD Pre-defined Concepts InformationD.1 Pre-defined ConceptsTable 6 provides a list of all pre-defined concepts used in this work.
Figure 10:	LIWC and WordNet concepts found at lower layers.
Figure 11: RoBERTa (numbers inside brackets show the maximum match across all layers)q.8.64 2。Lo.o.o.o.o.
Figure 12: XLNet (numbers inside brackets show the maximum match across all layers)Chunking (124)	-∙- CCG (256)FirstWord (11)repository8 released with Liu et al. (2019a). See Table 7 for statistics. We obtained the understudiedpre-trained models from the authors of the paper, through personal communication.
