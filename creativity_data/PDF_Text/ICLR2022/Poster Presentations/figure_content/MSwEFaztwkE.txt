Figure 1:	Left: Self-supervision. Self-supervised learning (SSL) uses self-supervision (the supervision fromthe data itself) for learning representations. An example of self-supervision is the augmented variant of theoriginal data. Middle: Auxiliary Information. This paper aims to leverage auxiliary information of data forweakly-supervised representation learning. We consider data attributes (e.g., binary indicators of attributes)as auxiliary information. Right: Our Weakly-supervised Contrastive Learning Method. We first constructdata clusters according to auxiliary information. We argue the formed clusters can provide valuable structuralinformation of data for learning better representations. Second, we present a contrastive learning approach - theclustering InfoNCE (Cl-InfoNCE) objective to leverage the constructed clusters.
Figure 2:	Cluster construction according to auxiliary information (e.g., the data attributes).
Figure 3: I(Z; T ) represents how relevantthe clusters and the labels; higher is better.
Figure 4: Experimental results for attributes-determined clusters + Cl-InfoNCE by tuning the hyper-parameterk when constructing the clusters. Note that we select attributes with top-k highest entropy, and we constructthe clusters such that the data within a cluster would have the same values for the selected attributes. Z are theconstructed clusters, and T are the downstream labels. We find the intersection between the re-scaled I(Z; T )and the re-scaled -H(Z|T) gives us the best downstream performance.
Figure 5: Experimental results under conventional self-supervised setting (pre-training using no label su-pervision and no auxiliary information). Left: We compare our method (K-means clusters + Cl-InfoNCE)with self-supervised approaches that leverage and do not consider unsupervised clustering. The downstreamperformance is reported using the linear evaluation protocal (Chen et al., 2020). Right: For our method andPrototypical Contrastive Learning (PCL), we plot the mutual information (I(Z; T)) and the conditional entropy(H(Z|T)) versus training epochs. Z are the unsupervised clusters, and T are the downstream labels. Thenumber of clusters is determined via grid search over {500, 1, 000, 5, 000, 10, 000}.
Figure 6: Experimental results on ImageNet-100 for Cl-InfoNCE under supervised (clusters Z = downstreamlabels T), weakly supervised (Z = hierarchy clusters) and conventional self-supervised (Z = instance ID)setting. We also consider the baseline - learning to predict the clustering assignment using the cross-entropy loss.
