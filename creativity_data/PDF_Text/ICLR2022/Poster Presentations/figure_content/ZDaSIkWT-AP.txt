Figure 1: Overview of the approach and architecture of the CBR agent. A memory stores actions that havebeen used in previous interactions. The context of the game is learned from the state knowledge graph usinga graph attention mechanism. Actions are retrieved from the memory based on this context representation andmapped to the current state. If no valid action is obtained using CBR, the algorithm falls back to a neural agent.
Figure 2: Performance on TWC (showing mean and standard deviation averaged over 5 runs) for the threedifficulty levels: easy (left), medium (middle), Hard (right) using normalized score and number of steps.
Figure 4: Fraction of times that a retrieved action isreused successfully on TWC (top). Fraction of timesthat the neural agent would have picked a rewardedaction when CBR is used successfully (bottom).
Figure 3: Examples from the zork1 game, showing thecontent of the memory and the context similarities, ina situation where the agent is able to reuse a previousexperience and in a case where the revise step is needed.
Figure 5: Number of entries inthe memory over training.
Figure 6: Visualization of the entity representations learned by the retriever. Colors denote the target locationof each object.
