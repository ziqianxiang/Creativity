Figure 1: 128 frame video of 128×128 resolution generated by DIGAN on the Tai-Chi-HD dataset.
Figure 2: Illustration of the (a) generator and (b) discriminator of DIGAN. The generator creates avideo INR weight from random content and motion vectors, which produces an image that corre-sponds to the input 2D grids {(x, y)} and time t. Two discriminators determine the reality of eachimage and motion (from a pair of images and their time difference), respectively.
Figure 3: Generated video results of DIGAN on UCF-101 and Kinetics-food datasets.
Figure 4: Generated videos of MoCoGAN-HD and DIGAN, trained on 16 frame videos of 128×128resolution on the Sky dataset. Yellow box indicates the extrapolated results until 64 frames.
Figure 5: Forward and backward prediction results	DIGAN (ours)	0.069 0.132 0.260of DIGAN. Yellow box indicates the given frame. -----------------------------------Datasets and evaluation. We conduct the experiments on UCF-101 (Soomro et al., 2012), Tai-Chi-HD (TaiChi; Siarohin et al. (2019)), Sky Time-lapse (Sky; Xiong et al. (2018)), and a food classsubset of Kinetics-600 (Kinetics-food; Carreira et al. (2018)) datasets. All models are trained on 16frame videos of 128×128 resolution unless otherwise specified. Specifically, we use the consecutive16 frames for UCF-101, Sky, and Kinetics-food, but stride 4 (i.e., skip 3 frames after the chosenframe) for TaiChi to make motion more dynamic. Following prior works, we report the Inceptionscore (IS; Salimans et al. (2016)), Frechet video distance (FVD; Unterthiner et al. (2018)), and kernelvideo distance (KVD; Unterthiner et al. (2018)). We average 10 runs for main results and 5 runs foranalysis with standard deviations. See Appendix A.2 for more details.
Figure 6: Videos sampled from two random motion vectors. The first two rows are generated videos,and the third row is the pixel difference between the two videos (yellow implies more differences).
Figure 7: Videos upsampled from 128× 128 to 512×512resolution (4× larger) on TaiChi dataset.
Figure 8: Zoomed-out samples. Red boxesindicate the original frames.
Figure 9: Samples of linearly interpolated INR weightsφi over λ, i.e., (1 - λ)φ1 + λφ2 on TaiChi dataset.
Figure 10: Discriminator logits for a far imagepair (i0, i1), conditioned over different ∆t.
Figure 11: Latent trajectories of the motion features on UCF-101, Sky, and TaiChi datasets. Resolu-tion denotes the location of motion features injected into the progressive generator; lower-resolutioncontrols the high-level semantics, and higher-resolution controls the low-level variations. Dot colorsgradually changes from blue (t = 0) to red (t = 1), and 5 random motion vectors are sampled. Thefeatures are projected onto 2D space via principal component analysis (PCA) for visualization.
Figure 12: More examples on forward and backward prediction results of DIGAN. Yellow boxindicates the given frame.
Figure 13: Intermediate scene (i.e., frame) prediction results of our method, DIGAN. The yellowboxes indicate the given initial and last frames.
Figure 14: Comparison of intermediate scene (i.e., frame) prediction results by StyleGAN2 and ourmethod, DIGAN. The yellow boxes indicate the given initial and last frames.
Figure 15: Additional 128 frame videos of 128×128 resolution by DIGAN, on the TaiChi dataset.
Figure 16: Additional time extrapolation videos of MoCoGAN-HD and DIGAN, trained on 16 framevideos of 128×128 resolution on the Sky dataset. Yellow box indicates the extrapolated frames.
Figure 17: Additional zoomed-out samples. Red boxes indicate the original frames.
