Figure 1: The t-SNE visualization of benign frames and their poisoned versions in the feature spacesof three different models: (a) benign model; (b) backdoored model by BOBA; and (c) backdooredmodel by our FSBA attack. FSBA-poisoned frames are well-separated from the benign frames inthe feature space, thus can better mislead or manipulate the target model.
Figure 2: The training pipeline of our proposed FSBA. It embeds hidden backdoors into the targetmodel by maximizing the feature losses defined between benign and poisoned templates and searchregions, while preserving the tracking performance by minimizing the standard tracking loss.
Figure 3: Results of SiamFC++ in tracking benign (Top Rows) and attacked (Bottom Rows) videos.
Figure 4: The t-SNE visualization of benign and poisoned frames in the feature space of backdooredtrackers by BOBA on OTB100 dataset. A similar visualization for our FSBA is in Appendix D.
Figure 5: Results of SiamFC++ in tracking benign (Top Rows) and attacked (Bottom Rows) videosin the physical world. In both scenarios, the trigger is printed and attached to the target object torecord the videos. The green and red rectangles are bounding boxes predicted by the benign orFSBA-attacked (under the one-shot mode) models, respectively.
Figure 6: Effect of the modification rate.
Figure 8: Four different trigger patterns.
Figure 7: Effect of the frame attacking rate.
Figure 9: Resistance to four frame-wise pre-processing techniques with different budgets.
Figure 11: Transformed poisoned images with different types of color-shifting. All images arerandomly transformed with maximum perturbation size âˆˆ {0.1, 0.2, 0.3, 0.4}.
Figure 12: The t-SNE of training frames in the feature space of models under FSBA attack.
Figure 13: Transformed poisoned images with different levels of additive Gaussian noise.
Figure 14: The five representative behaviors of attacked SiamFC++ trackers by our FSBA.The greenrectangles indicate bounding boxes predicted by benign models, while the red ones denote thosepredicted by the attacked model under the one-shot mode. (a): behavior of failed attacks; (b)-(d):behaviors of successful attacks; (e): behavior of half-failed attacks.
Figure 15: The poisoned loss Lp and feature loss Lf across different training epochs under theBOBA baseline attack on OTB100 dataset.
Figure 16: The attention maps of benign or FSBA-attacked SiamFC++ trackers on the search re-gions. Grad-CAM(Selvaraju et al., 2017) is used to generate the attention maps. This experimentis conducted on the OTB100 dataset. Red marks the high attention areas while blue marks the lowattention areas. Top Row: attention map on the raw image; Bottom Row: attention map only.
