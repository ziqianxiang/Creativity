Figure 1: An example of planning from text. We show three objects (cheese, carrot and banana) inthis example. When interacting with the enVironment (env), the agent encodes the information ofobserVation ot into object states zt with encoders (qE1, qE2, qE3). GiVen a goal gt, the planner determinesthe action at by searching with the reward (pr) and transition models (p1T , p2T p3T ).
Figure 2: Left: The Object-Oriented Transition Model. Right: The Reward Model.
Figure 3: Training Curves: Agents’ normalized scores for the games at different difficulty levels. Theplot shows mean ± std normalized scores computed with three independent runs.
Figure 4: Scatter plots for T-SNE-embedded object states. From left to right, the states are sampledfrom SLa-Dyna, NAt-Dyna, OS-OOTD and SS-OOTD (from left to right).
Figure 5: The object extractor.
Figure 6: The observation encoder.
Figure 7: Adjacency tensors corresponding the "is" relation in memory graphs ht . From left to right:1) the ground-truth graph, and the graph learned by 2) the OS-ELBo objective, 3) the SS-ELBoobjective (Section 3.3), and 4) GATA (a graph-based dynamics model baseline) (Adhikari et al.,2020).
Figure 8: Planning Curve. The agent are based on the dynamics model with the OS-ELBO objective(Upper) and the SS-ELBo objective (Lower). The plot shows mean±stdnormalized scores computedwith three independent runs.
