Figure 1: Subgraph tasks are to predict the properties of subgraphs in the whole graph. S (green)is the subgraph we want to predict. SubGNN samples anchor patches (grey), aggregates featuresof anchor patches to connected components of the subgraph through six independent channels (red,blue, and yellow), and pools component embeddings as the subgraph representation. GLASS labelsnodes, passes messages between them, and pools node embeddings as the subgraph representation.
Figure 2: Plain GNNs are not enough to capture subgraph topology. For example, given a graph Gwith empty node feature, GNN representations of subgraph S andS0 must be the same because of thehomogeneous rooted subtree structures, though these two subgraphs are non-isomorphic. However,differentiation of nodes in and outside helps GNNs generate different embeddings for S and S0 .
Figure 3: The time needed for training a model (log scale). SubGNN takes more than 48h onppi-bp.
Figure 4: GLASS is composed of node label function, MPNN, pooling layers.
Figure 5: The difference ofF1 score between GLASS and GNN-plain. The results are provided withruns on ten random seeds. The left subplot shows the result of testing in batch. The right one showsthe performance of testing each sample separately, in other words, setting the valid and test batchsize to one.
Figure 6: The training time against batch size. The results are provided with runs on ten randomseeds.
