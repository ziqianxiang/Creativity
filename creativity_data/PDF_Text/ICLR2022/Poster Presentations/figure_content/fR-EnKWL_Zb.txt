Figure 1: Illustration of QuadTree Attention. Quadtree attention first builds token pyramids bydown-sampling the query, key and value. From coarse to fine, quadtree attention selects top K(here, K = 2) results with the highest attention scores at the coarse level. At the fine level, attentionis only evaluated at regions corresponding to the top K patches at the previous level. The querysub-patches in fine levels share the same top K key tokens and coarse level messages, e.g., greenand yellow sub-patches at level 2 share the same messages from level 1. We only show one patch inlevel 3 for simplicity.
Figure 2: Illustration of quadtree message aggregation for a query token qi. (a) shows the tokenpyramids and involved key/value tokens in each level. Attention scores are marked in the first twolevels for clarification, and the top K scores are highlighted in red. (b) shows message aggregationfor QuadTree-A architecture. The message is assembled from different levels along a quadtree. (c)shows message aggregation for QuadTree-B architecture. The message is collected from overlap-ping regions from different levels.
Figure 3:	Loss and AUC@20Â° of image matching.
Figure 4:	Loss and top 1 accuracy of image classification for PVTv2-b0 archtecture.
Figure 5: Score map visualization of different attention methods for one patch in the query image.
