Figure 1: The agent (orange) and land-marks (blue) in 4-Goals.
Figure 2: Experiment results on 4-Goals for M = 7 iterations averaged over 5 random seeds. Errorbars are 95% confidence intervals.
Figure 4: Different strategies found in a run of 20 iterations diversity setting RSPO in Monster-Hunt.
Figure 5: Sample acceptanceratio when learning a policydistinct from Apple NE. Theintrinsic reward is critical.
Figure 7: Results on Escalation averaged over 3 random seeds. Shadedarea and error bars are 95% confident intervals.
Figure 8: Half-Cheetah behaviors.
Figure 9: Hopper behaviors.
Figure 10: Walker behaviors.
Figure 11: Homanoid behaviors.
Figure 12: Data efficiency with different α inHumanoid.
Figure 13: Learning curve with different λiRnt onthe 2m_vs_1z map in SMAC.
Figure 14: Diverse strategies discovered by RSPO on the 2c_vs_64zg map in SMAC.
Figure 15: Diverse strategies discovered by RSPO on the 2m_vs_1z map in SMAC. The movementdirection is indicated by yellow arrows.
