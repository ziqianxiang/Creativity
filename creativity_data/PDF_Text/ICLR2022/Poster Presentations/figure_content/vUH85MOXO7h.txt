Figure 1: Schematics of an ensemble of M soft trees. Tree internal nodes are indexed according tothe breadth-first ordering.
Figure 2: The scaled error function. Wedraw 50 lines with varying α by 0.25.
Figure 3: Left: An empirical demonstration of convergence of Θ0(xi, xj) to the fixed limit Θ(xi, xj)as M increases. Two simple inputs are considered: xi = {1, 0} and xj = {cos(β), sin(β)}with β = [0, π]. The TNTK Θb (03) (xi, xj) with α = 2.0 is calculated 10 times with parameterre-initialization for each of the M = 16, 64, 256, 1024, and 4096. Center and Right: Parameterdependency of the convergence. The vertical axis corresponds to the averaged error between theH0 and the H := limM→∞ H0 for 50 random unit vectors of length F = 5. The dashed lines areplotted only for showing the slope. The error bars show the standard deviations of 10 executions.
Figure 4: Output dynamics for train and test data points. The color of each line corresponds to eachdata point. Soft tree ensembles with d = 3, α = 2.0 are trained by a full-batch gradient descentwith a learning rate of 0.1. Initial outputs are shifted to zero (Chizat et al., 2019). There are 10randomly generated training points and 10 randomly generated test data points, and their dimensionF = 5. The prediction targets are also randomly generated. Let H(x, x0) ∈ RN×N0 be the limitingNTK matrix for two input matrices and I be an identity matrix. For analytical results, we draw thetrajectory f(v, θτ) = H(v, x)H (x, x)-1(I - exp[-ηH(x, x)τ])y (Lee et al., 2019) using thelimiting TNTK (Equation (7)), where v ∈ RF is an arbitrary input and x ∈ RF ×N and y ∈ RN arethe training dataset and the targets, respectively.
Figure 5: Left: Normal Tree, Right: Oblivious Tree. The rules for splitting in the same depth areshared across the same depth in the oblivious tree, while ∏m,' on leaves can be different.
Figure 6: Parameter dependencies of T(Xi, Xj), T(xi, Xj), and Θ(d)(xi, Xj). The vertical axes arenormalized so that the value is 1 when the inner product of the inputs is 1. The input vector size isnormalized to be one. For the three figures on the left, the line color is determined by α, and for thefigure on the right, it is determined by the depth of the tree.
Figure 7: Left: Averaged accuracy over 90 datasets. The performances of the kernel regression withthe MLP-induced NTK and the RBF kernel are shown for comparison. Since the depth is not a hyper-parameter of the RBF kernel, performance is shown by a horizontal line. The statistical significance isalso assessed in the supplementary material. Right: Running time for kernel computation. The inputdataset has 300 samples with 10 features. Feature values are generated by zero-mean i.i.d Gaussianwith unit variance. The error bars show the standard deviations of 10 executions.
Figure 8: Right-hand side of the Equation (D.7), where c = 5n (in other words, v = 5 in Lemma 6).
Figure 9: The γ dependency of the RBF kernel performance.
Figure 10: P-values of the Wilcoxon signed rank test for different pairs of α.
Figure 11: Performance comparisons between the kernel regression with MLP-induced NTK and theTNTK on the UCI dataset.
Figure 12: Performance comparisons between the kernel regression with RBF kernel and the TNTKon the UCI dataset.
Figure 13: Pearson’s correlation coefficients with predicted values of the TNTK with different α.
Figure 14: Dataset-wise comparison for a half of the dataset (1/2).
Figure 15: Dataset-wise comparison for a half of the dataset (2/2).
