Figure 1: Overall Comparison. Between MDPO, PPO,and TRPO, MDPO provides the best trade-off in terms ofbest average performance, less (normalized) wall clocktimes, and least number of algorithm specific hyper pa-rameters used.
Figure 2: Performance of on-policy(top) and off-policy (bottom) MDPO(code level optimizations included) for dif-ferent values of m on the Walker2d task.
Figure 3:	Performance of MDPO-M, compared against PPO-M, TRPO-M on six MuJoCo tasks. The results areaveraged over 5 runs, with their 95% confidence intervals shaded.
Figure 4:	Performance of MDPO-LOADED, compared against loaded implementations (excluding GAE) ofPPO and TRPO (PPO-LOADED, TRPO-LOADED) on six MuJoCo tasks. The results are averaged over 5 runs,with their 95% confidence intervals shaded.
Figure 5:	Performance of MDPO-LOADED+GAE, compared against loaded implementations (including GAE)of PPO and TRPO (PPO-LOADED+GAE, TRPO-LOADED+GAE) on six MuJoCo tasks. The results areaveraged over 5 runs, with their 95% confidence intervals shaded.
Figure 6:	Performance of KL and Tsallis based versions of MDPO-M, compared with SAC-M on six MuJoCotasks. X-axis represents time steps in millions. The results are averaged over 5 runs, with their 95% confidenceintervals shaded.
Figure 7:	Performance of KL and Tsallis based versions of MDPO-LOADED, compared with SAC-LOADEDon six MuJoCo tasks. X-axis represents time steps in millions. The results are averaged over 5 runs, with their95% confidence intervals shaded. Note that although there is overlap in the performance of all methods, MDPOachieves a higher mean score in 5 out 6 domains.
Figure 8: Performance of off-policy MDPO,compared with SAC on Hopper-v2, when do-ing both single and multiple gradient updateseach iteration.
Figure 9: Performance of Tsallis SAC. The results are averaged over 5 runs, with 95% confidence intervalsshaded.
