Figure 1: Top-1 ImageNet validation accuracyof vanilla deep networks initialized using eitherEOC (with ReLU) or TAT (with LReLU) andtrained with K-FAC.
Figure 2: Global C maps for ReLU networks(EOC) and TReLU networks (Cf (0) = 0.5).
Figure 3: Training speed comparison be-tWeen K-FAC and SGD on 50 layer vanillaTReLU netWork.
Figure 4: Empirical c values for TAT and DKS, which are averaged over 100 pairs of inputs and 50 differentrandomly-inialized networks. We include the results for both Gaussian fan-in and Orthogonal initialization.
Figure 5: CIFAR-10 validation accuracy ofResNets with ReLU activation function initializedusing either EOC or TAT (ours).
Figure 6: Top-1 validation accuracy on ImageNet as a function of number of iterations (left) or wall-clock time(right) with K-FAC optimizer. One can reduce the computational overhead significantly by updating curvaturematrix approximation and its inverse less frequently.
Figure 7: ImageNet training accuracy of deepvanilla networks with either EOC-initializedReLU networks or TReLU networks.
