Figure 1: Both Score-basedGenerative Model (SGM)and Schrodinger Bridge(SB) transform between twodistributions. While SGMrequires pre-specifying thedata-to-noise diffusion, SBinstead learns the process.
Figure 2: Schematic diagram of the our stochastic optimal control interpretation, and how it connectsthe objective of SGM (3) and optimality of SB (6) through Forward-Backward SDEs theory.
Figure 3: Validation of our SB-FBSDE model on two synthetic toy datasets that represent continuousand discontinuous distributions. Upper: Generation (Pdata — Pprior) process with the backward vectorfield Z (∙, •; φ). Bottom: Diffusion (Pdata → PPriOr) process with the forward vector field Z(∙, ∙; θ).
Figure 4:	Uncurated samples from our SB-FBSDE models trained on MNIST (left), resized CelebA(middle) and CIFAR10 (right). More images can be found in Appendix E.
Figure 6: Ablation analysis where we show that addingLangevin corrector to SB-FBSDE uniformly improvesthe FID scores on both CelebA and CIFAR10 training.
Figure 5:	Validation of our SB-FBSDE onlearning forward diffusions that are closer(in KL sense) to pprior compared to SGM.
Figure 7: Training Hyper-parametersDataset	learning rate	time steps	batch size	variance of ppriorToy	2e-4	100	400	1.0Mnist	2e-4	100	200	1.0CelebA	2e-4	100	200	900.0CIFAR10	1e-5	200	64	2500.0Figure 8: Network ArchitecturesDataset	Zt (∙, ∙; θ) and # of parameters	Zt (∙, ∙; φ) and # of parametersToy	FC-ResNet (0.76M)	FC-ResNet (0.76M)Mnist	reduced Unet (1.95M)	reduced Unet (1.95M)CelebA	Unet (39.63M)	Unet (39.63M)CIFAR10	NCSN++ (62.69M)	Unet (39.63M)Training. We use Exponential Moving Average (EMA) with the decay rate of 0.99. Table 7 detailsthe hyper-parameters used for each dataset. As mentioned in De Bortoli et al. (2021), the alternatetraining scheme may substantially accelerate the convergence under proper initialization. Specifically,when Zt is initialized with degenerate outputs (e.g. by zeroing out itslast layer), training Zbt at thefirst K steps can be made in a similar SGM fashion since ptSB now admits analytical expression. Asf .ι	-∣∙	,	/TC YC∖	♦	∕r-r	∖ ι	ι	, ∙	∙ ι ,for the proceeding stages, we resume to use (18, 19) since (Zt, Zt) no longer have trivial outputs.
Figure 8: Network ArchitecturesDataset	Zt (∙, ∙; θ) and # of parameters	Zt (∙, ∙; φ) and # of parametersToy	FC-ResNet (0.76M)	FC-ResNet (0.76M)Mnist	reduced Unet (1.95M)	reduced Unet (1.95M)CelebA	Unet (39.63M)	Unet (39.63M)CIFAR10	NCSN++ (62.69M)	Unet (39.63M)Training. We use Exponential Moving Average (EMA) with the decay rate of 0.99. Table 7 detailsthe hyper-parameters used for each dataset. As mentioned in De Bortoli et al. (2021), the alternatetraining scheme may substantially accelerate the convergence under proper initialization. Specifically,when Zt is initialized with degenerate outputs (e.g. by zeroing out itslast layer), training Zbt at thefirst K steps can be made in a similar SGM fashion since ptSB now admits analytical expression. Asf .ι	-∣∙	,	/TC YC∖	♦	∕r-r	∖ ι	ι	, ∙	∙ ι ,for the proceeding stages, we resume to use (18, 19) since (Zt, Zt) no longer have trivial outputs.
Figure 9: Network architecture for toy datasets.
Figure 10: Comparison between images generated by ground truth and SB-FBSDE on reducedCelebA. Our SB-FBSDE is trained under the same data pre-processing, network architecture andstepsizes implemented in De Bortoli et al. (2021).
Figure 11: Qualitative results at the different stages of training. (a) Results after 50k training iterationsusing SGM’s regression loss. (b) Refine the results of Fig. 11a by training the backward policy using(18) with 5k iterations. (c) Refine the results of Fig. 11a with a full SB-FBSDE stage using (18,19).
Figure 12: Uncurated samples generated by our SB-FBSDE on MNIST.
Figure 13: Uncurated samples generated by our SB-FBSDE on resized CelebA.
Figure 14: Uncurated samples generated by our SB-FBSDE on CIFAR10.
