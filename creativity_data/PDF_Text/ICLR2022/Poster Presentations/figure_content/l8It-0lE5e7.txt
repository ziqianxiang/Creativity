Figure 1: Adversarial training for the 3-layer neural network on MNIST. Adversarial normalizedmargin for (a) FGSM adversarial examples; (b) '∞-PGD adversarial examples; (C) FGSM adversarialexamples after 10000 epoChs.
Figure 2: Adversarial training for the 3-layer neural network on MNIST to perform binary classi-fication. The perturbations used for training are FGSM and '∞-PGD perturbations with varyingperturbation sizes = 8/255, 12/255 and 24/255. The adversarial normalized margins and adversar-ial training accuracies are evaluated with the corresponding perturbations used for training.
Figure 3:	Adversarial training for the CNN on all training examples of MNIST to perform multi-classification. The perturbations used for training are FGSM and '∞-PGD perturbations with varyingperturbation sizes = 16/255 and 32/255. The adversarial normalized margins and adversarialtraining accuracies are evaluated with the corresponding perturbations used for training.
Figure 4:	Adversarial training for the 3-layer neural network on MNIST. The attack method is'2-FGM with e = 8. (a) Adversarial normalized margin for '2-FGM adversarial examples duringadversarial training and standard training. (b) Adversarial training accuracy for '2-FGM adversarialtraining.
Figure 5: Adversarial normalized margin during adversarial training for binary classification. Theloss function during training is taken as exponential loss.
Figure 6: Frobenius norms for weights of deep linear networks with different layers during adversarialtraining. The perturbations are `2 perturbations.
Figure 7: Ratios of 2-norm and Frobenius norm for weights of deep linear networks with differentlayers during adversarial training. The perturbations are `2 perturbations.
