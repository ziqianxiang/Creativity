Figure 1: (a) ∣∣rk ∣∣2∕∣∣r0∣∣2 of GD and CR, and ∣∣fk k2∕∣∣r0k2 of AM and ST-AM for solving Prob-lem I; (b) Ilrkk2∕i∣r01∣2 for solving Problem II; (c) ∣∣rk∣∣2∕∣∣r0∣∣2 for solving Problem III (M = 0.1);(d)(e) relative residuals of the forward and backward root-finding processes in MDEQ, and shadedareas correspond to the standard deviations; (f) test accuracy and loss in MDEQ/CIFAR-10.
Figure 2:	Experiments on MNIST. (a)(b) Training loss and the squared norm of gradient (SNG) (w/opreconditioning for SAM, RST-AM); (c) Training loss (w/ preconditioning for SAM, RST-AM).
Figure 3:	Validation perplexity of training 1,2,3-layer LSTM on Penn Treebank.
Figure 4: Solving (97) with different condition numbers: Cond(ATA) = λmaχ(AτA)∕λmin(AτA).
Figure 5: Solving (99) with different M. (a) krkk2/kr0k2 of MST-AM; (b) krkk2/kr0k2 of ST-AM;(C) IlPk-ιZk∣∣2∕∣∣∆χk-ι∣∣2 and kQk-iZk∣∣2∕∣∣∆rk-ι∣∣2 ofMST-AM;(d) ∣∣Pk-ιZk∣∣2∕∣∣∆χk-ι∣∣2and IlQk-ιZk∣∣2∕∣∣∆rk-ι∣∣2 OfST-AM.
Figure 6: Training on MNIST with the Adam/RMSprop preconditioner.
Figure 7: Test accuracy of training ResNet18 and WideResNet16-4 on CIFAR-10 and trainingResNeXt50 and DenseNet121 on CIFAR-100.
Figure 8: Train accuracy and test accuracy of RST-AM with different c2 .
Figure 9:	Train accuracy and test accuracy of RST-AM with different c1.
Figure 10:	Train accuracy and test accuracy of RST-AM with different weight-decays.
Figure 11: Training deep neural networks for 80,100,160 epochs. The results of final test accuracyof RST-AM for training 80,100,160 epochs and the final test accuracy of SGDM for training 160epochs are shown in the nested figures for comparison.
Figure 12: Clean accuracy and PGD-10 attacked accuracy on the validation set in training differentneural networks.
Figure 13: FID score for training SN-GAN on CIFAR-10.
Figure 14: Train and test accuracy for training ImageNet/ResNet50.
