Figure 1: Cross-Entropy Loss on the training and validation set and full loss (including weightdecay) during training for full-batch gradient descent. Left: training as described in Section 3.2without clipping, right: with gradient clipping. Validation computed every 100 steps.
Figure 2: One-dimensional loss landscapes (random direction) of models trained with gradient de-scent. Default full-batch gradient descent (left) produces sharp models that do not train and general-ize well, yet it can be modified to converge to flatter minima with longer training, gradient clippingand appropriate regularization (right).
Figure 3:	Cross-Entropy Loss on the training and validation set and full loss (including weightdecay) during training for full-batch gradient descent. Clipped steps are marked in black. Validationcomputed every 100 steps. From top to bottom: Top: training as described in Section 3.2 withand without clipping. All other rows: training with gradient regularization: FB regularized) on theleft and FB strong reg. on the right. Second from the top: Training with lr=0.4. Third from thetop: Training with lr=0.8 (this is the main setting investigated in this work). Bottom: Training withlr=1.6.
Figure 4:	One-dimensional loss landscapes visualizations (random direction) of models trained withgradient descent, going from SGD (left) to GD with successive modifications (right).
Figure 5: Cross-Entropy Loss on the training and validation set during training for full-batch gra-dient descent in direct comparison to Figure 2 in the main body - but with reduced learning rates.
