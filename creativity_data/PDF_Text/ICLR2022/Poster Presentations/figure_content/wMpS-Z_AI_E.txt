Figure 1: Test accuracy on simulated datawith or without input structure.
Figure 2: Visualization of the weights wi’s after initialization/one gradient step/two steps in networklearning on the synthetic data. The red star denotes the ground-truth j∈A Mj ; the orange star is- j∈A Mj . The red/orange dots are the weights closest to the red/orange star, respectively.
Figure 3: Visualization of the neurons’ weights in a two-layer network trained on the subset ofMNIST data with label 0/1. The weights gradually form two clusters.
Figure 4: Test accuracy at different steps for an equal mixture of Gaussian inputs with data: (a)MNIST, (b) CIFAR10, (c) SVHN.
Figure 5: Test accuracy on simulated data under parity labeling with or without input structure.
Figure 6: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps innetwork learning under parity labeling. The red star denotes the ground-truth Pj∈A Mj ; the orangestar is - Pj∈A Mj. The red dots are the weights closest to the red star after two steps; the orangeones are for the orange star.
Figure 7: Test accuracy on simulated data under interval labeling with or without input structure.
Figure 8: Visualization of the weights wi’s after initialization/one gradient step/two gradient stepsin network learning under interval labeling. The red star denotes the ground-truth Pj∈A Mj ; theorange star is - Pj∈A Mj. The red dots are the weights closest to the red star after two steps; theorange ones are for the orange star.
Figure 9: Test accuracy on simulated data under different input data dimensions.
Figure 10: Visualization of the weights wi ’s in early steps under different input data dimensions.
Figure 11: Test accuracy on simulated data under different negative class ratios.
Figure 12: Visualization of the weights wi ’s in early steps under different class imbalance ratios.
Figure 13: Test accuracy on simulated data under different sample sizes n.
Figure 14: Visualization of the weights wi’s in early steps under different sample sizes. Upper row:sample size 25000; lower row: 10000.
Figure 15: Visualization of the weights wi ’s after initialization/one gradient step/two gradient stepsin network learning under hidden representation labeling.
Figure 16: Visualization of the weights wi’s (blue dots) and Gaussian centers (red for positive labeledclusters and orange for negative labeled clusters).
Figure 17: Visualization of the neurons’ weights in a two-layer network trained on the subset ofMNIST data with label 0/1. The weights gradually form two clusters.
Figure 18: Visualization of the neurons’ weights in a two-layer network trained on the subset ofCIFAR10 data with label airplane/automobile. The weights gradually form two clusters.
Figure 19: Visualization of the neurons’ weights in a two-layer network trained on the subset ofSVHN data with label 0/1. The weights gradually form four clusters.
Figure 20: Visualization of the normalized convolution weights in all Residual block of ResNet(128)trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
Figure 21: Visualization of the normalized convolution weights in all Residual block of ResNet(256)trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
Figure 22: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs with data:(a) MNIST, (b) CIFAR10, (c) SVHN.
Figure 23: Test accuracy at different steps for an equal mixture α = 0.5 of Tiny ImageNet inputswith data: (a) CIFAR10, (b) SVHN.
Figure 24: Test accuracy at different steps for varying mixture α of Gaussian inputs with CIFAR10.
Figure 25: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs withMNIST, where m = 50.
Figure 26: Double descent curves of the students trained on data with synthetic labels (Loss v.s.
