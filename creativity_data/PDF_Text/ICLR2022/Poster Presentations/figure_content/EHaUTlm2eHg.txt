Figure 1: The architecture of the model. Our inference model qÏ†(Z) uses a backwards hidden state btto approximate dependencies of zt on the future of the trajectory. The blue line separates the datacollection and policy gradient training steps in our algorithm and the red lines represent informationflowing into the backwards RNN. Grey variables are used during training and white variables are usedduring data collection. Top: we show the training model where the policy gradient loss is calculatedwith backwards RNN hidden state information. Bottom: we show the data collection phase of thealgorithm utilizing latent variables sampled from the latent prior.
Figure 2: The online episodic mean reward evaluated over 5 episodes every 250 steps for MiniGridRL tasks. We show the average over 5 random seeds. 2M environment step interactions are used. Theshaded area shows the standard error. PGIF always uses state based forcing in these environments.
Figure 3: The online episodic mean reward evaluated over 5 episodes every 500 steps for MuJoCocontinuous control RL tasks with partial observability. We show the average over 5 random seeds.
