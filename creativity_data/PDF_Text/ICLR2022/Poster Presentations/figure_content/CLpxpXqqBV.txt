Figure 1: Motivation of “learning via retrac-ing”. (a): Retracing in navigation tasks yieldsfaster representation learning and potentially sup-ports stronger generalisation; (b): “Irreversible”transitions (graphical demonstration from theDeepMind Control Suite Tassa et al. (2018)).
Figure 2: Graphical illustration of “learning viaretracing”. (a) “learning via retracing” addition-ally constrains the similarity between the retracedand original states for representation learning; (b)Graphical model of CCWM. The empty circle andfiller square nodes represent the stochastic and de-terministic variables, respectively.
Figure 3: Evaluation of CCWM on DeepMind Control Suite. (a): Graphical demonstration ofselected continuous control task environments, from left to right: hopper stand/hop, walker run/walk,finger spin, reacher easy, cheetah run, quadruped run. (b): Average evaluation returns (±1 s.d.) duringtraining (5 random seeds). “Learning via retracing” generally improves the performance of learningfrom pixel inputs in presented tasks comparing to the main baseline Dreamer agent (which couldapproximately be viewed as CCWM without retracing). CCWM reaches the asymptotic performanceof state-of-the-art model-free methods (SAC, D4PG at 108 steps) on several tasks.
Figure 4: Qualitative comparison of long-range predictive reconstruction of CCWM andDreamer. Predictive rollouts over 30 time-steps given the actions are computed using the rep-resentation models. CCWM consistently generates more accurate predictive reconstructions furtherinto the future than Dreamer, with CCWM becoming noticeably inaccurate by 25 - 30 timesteps, andDreamer by 10 - 15 timesteps. See implementation details and further discussion in APPendix F.
Figure 5: Evaluation of Adaptive Truncation ontasks with varying degrees of “irreversibility”.
Figure 6: Graphical illustration of a model-free instantiation of ”learning via retracing”. The forwardmodel is a state-space model and is trained in a generative fashion under the variational principles.
Figure 7: Visualisation of similarity between the ground-truth states and retraced states usingtSNE. The two-dimensional embeddings for a 500-step trajectory (a) and a 76-step subtrajectory (b)is shown. Color of the scatters indicates the temporal ordering of the states within the trajectories.
Figure 8: Visualisation of state representations along the same trajectory in Hopper Stand task.
Figure 9: Visualisation of the structure of the state representations with respect to the Q-values.
Figure 10: Qualitative comparison of long-range predictive reconstruction of CCWM andDreamer. Predictive rollouts over 45 time-steps given the actions are computed using the rep-resentation models. Comparing to Dreamer, CCWm consistently generates more accurate predictivereconstructions over longer time spans. CCWM generates accurate predictions up to 〜l8 steps onWalker task, and 〜35 steps on Cheetah task; Dreamer generates accurate predictions up to 〜8 stepson Walker task, and 〜15 steps on Cheetah task (evaluated over 5 randomly sampled trajectories).
Figure 11: Full ablation studies of the retracing loss function. CCWM with bisimulation metricsas the retracing loss function consistently outperforms the alternatives using the L2 and reconstructionretracing error.
