Figure 1: We propose a method that jointly leverages(1) a large offline dataset of prior experience collectedacross many tasks without reward or task annotationsand (2) a set of meta-training tasks to learn how toquickly solve unseen long-horizon tasks. Our methodextracts reusable skills from the offline dataset and meta-learn a policy to quickly use them for solving new tasks.
Figure 2: Method Overview. Our proposed skill-based meta-RL method has three phases. (1) Skill Extraction:learns reusable skills from snippets of task-agnostic offline data through a skill extractor (yellow) and low-levelskill policy (blue). Also trains a prior distribution over skill embeddings (green). (2) Skill-based Meta-training:Meta-trains a high-level skill policy (red) and task encoder (purple) while using the pre-trained low-level policy.
Figure 3: Environments. We evaluate our proposed framework in two domains that require the learning ofcomplex, long-horizon behaviors from sparse rewards. These environments are substantially more complex thanthose typically used to evaluate meta-RL algorithms. (a) Maze Navigation: The agent needs to navigate forhundreds of steps to reach unseen target goals and only receives a binary reward upon task success. (b) KitchenManipulation: The 7DoF agent needs to execute an unseen sequence of four subtasks, spanning hundreds oftime steps, and only receives a sparse reward upon completion of each subtask.
Figure 4: Target Task Learning Efficiency. SiMPL demonstrates better sample efficiency compared to allthe baselines, verifying the efficacy of meta-learning on long-horizon tasks by leveraging skills and skill priorextracted from an offline dataset. For both the two environments, we train each model on each target task with 3different random seeds. SiMPL and PEARL-ft first collect 20 episodes of environment interactions (verticaldotted line) for conditioning the meta-trained policy before fine-tuning it on target tasks.
Figure 5: Qualitative Results. All the methods that leverage the offline dataset (i.e. SiMPL, SPiRL, andMTRL) effectively explore the maze in the first episode. Then, SiMPL converges with much fewer episodescompared to SPiRL and MTRL. In contrast, PEARL-ft is not able to make learning progress.
Figure 6: Meta-training Task Distribution Analysis. (a) With sparser meta-training task distributions (i.e.
Figure 7: Task Distributions for Task Length Ablation. We propose three meta-training task distributionsof increasing difficulty to compare different meta-RL algorithms: TTRAIN-EASY uses short-horizon tasks withadjacent goal locations, making exploration easier during meta-training, TTRAIN-MEDIUM uses similar task horizonbut increases the goal position variance, TTRAIN-HARD contains long-horizon tasks with high variance in goalposition and thus is the hardest of the tested task distributions.
Figure 8: Meta-Training Performance for Task Length Ablation. We find that most meta-learning ap-proaches can solve the simplest task distribution, but using prior experience in BC+PEARL and SiMPL helpsfor the more challenging distributions (b) and (c). We find that only our approach, which uses the prior data byextracting temporally extended skills, is able to learn the challenging long-horizon tasks efficiently.
Figure 9: Qualitative Result of Meta-reinforcement Learning Method Ablation. Top. All the methodscan learn to solve short-horizon tasks TTRAIN-EASY . Middle. On medium-horizon tasks TTRAIN-MEDIUM , PEARLstruggles at exploring further, while BC+PEARL exhibits more consistent exploration yet still fails to solvesome of the tasks. SiMPL can explore well and solve all the tasks. Bottom. On long-horizon tasksTTRAIN-HARD ,PEARL falls into a local minimum, focusing only on one single task on the left. BC+PEARL explores slightlybetter and can solve a few more tasks. SiMPL can effectively learn all the tasks.
Figure 10: Performance with few episodes of target task interaction. We find that our skill-based meta-RLapproach SiMPL is able to learn complex, long-horizon tasks within few episodes of online interaction with anew task while prior meta-RL approaches and non-meta-learning baselines require many more interactions orfail to learn the task altogether.
Figure 11: Image-Based Maze Navigation with Distribution Shift. (a-b): Meta-training and target taskdistributions. The green dots represent the goal locations of meta-training tasks and the red dots represent thegoal locations of target tasks. The yellow cross represent the initial location of the agent, which is equivalent tothe one used in Pertsch et al. (2020). (c): Performance on the target task. Our approach SiMPL can leverageskills learned from offline data for efficient meta-RL on the maze navigation task and is robust to the domainshift between offline data environments and the target environment.
Figure 12: Maze Meta-training and Target Task Distributions. The green dots represent the goal locationsof meta-training tasks and the red dots represent the goal locations of target tasks. The yellow cross represent theinitial location of the agent.
Figure 13: Maze Meta-training and Target Task Distributions for Meta-training Task Distribution Anal-ysis. The green dots represent the goal locations of meta-training tasks and the red dots represent the goallocations of target tasks. The yellow cross represent the initial location of the agent.
