Figure 1: PCA components progression. The top row shows the number of components that account for 99%(N99-PCA in blue) and 95% (N95-PCA in red) explained variance of all the gradients generated during gradientdescent epochs. The bottom row shows the performance of the model on the test data. The results are presentedfor: (i) CIFAR-10 classification (left 4 columns), and (ii) CelebA regression (right 4 columns).
Figure 2: Overlap of actual and principal gradients. The heatmap shows the pairwise cosine similaritybetweenactual (epoch) gradients andprincipal gradient directions (PCA gradients). xpoch gradients have a substantialoverlap with one or more PCA gradients and consecutive epoch gradients show a gradual variation.This suggestthat there may exist a high overlap between the gradients generated during the NN model training.The resultsare shown for a CNN classifier trained on CIFAR-10 (left 4 columns) and CelebA (right 4 columns) datasets.
Figure 3: Similarity among consecutive gradients. The cosine similarity of consecutive gradients reveals agradual change in directions of gradients over epochs. Thus, the newly generated gradients can be representedinterms of the previously generated gradientswith low approximation error.Reusing/recycling gradients can thuslead to significant communication savings during SGD-based federated optimization.
Figure 4: Look-back gradient multiplier. (a) The Look-back Coefficients (LBCs) are the projection ofaccumulated stochastic gradients at the workers on their Look-back Gradients (LBGs). (b) Scalar LBCs, i.e., theρkt),', are transmitted to the server. (C) LBG-based gradient approximations are reconstructed at the server.
Figure 5: LBGM as a Standalone Algorithm. Irrespec-tive of the dataset/data configuration across workers,LBGM consistently outperforms vanilla FL in terms ofthe total parameters shared (middle row) while achiev-ing comparable accuracy (top row). The bottom rowshows accuracy vs. # parameters shared.
Figure 6: Effect of δkthreshold on LBGM. As δkthresholddecreases, the training may become unstable. Forlarger values of δkthreshold , LBGM achieves communica-tion benefits (middle row) while maintaining a perfor-mance identical to vanilla FL (top row). The bottomrow shows accuracy vs. # parameters shared.
Figure 7: LBGM as a Plug-and-Play Algorithm. LBGM obtains substantial communication benefits whenimplemented on top of existing gradient compression techniques by exploiting the rank-characteristics of thegradient-space. Top-K and ATOMO are known to achieve state-of-the-art performance of their respectivedomains of sparsification and low-rank approximation respectively.
Figure 8: Application of LBGM as a plug-and-play al-gorithm on top of SignSGD in distributed training.
Figure 9: PCA Components Progression. Repeat of Fig.1on CIFAR-100 dataset.
Figure 10: PCA Components Progression. Repeat of Fig.1on MNIST dataset.
Figure 11: PCA Components Progression. Repeat of Fig.1on FMNIST dataset.
Figure 12: PCA Components Progression. Repeat of Fig.1on CIFAR-10, F-MNIST, and MNIST datasets butusing squared SVM classifier.
Figure 13:	PCA Components Progression. Repeat of Fig.1on COCO, and PascalVOC datasets but usingU-Net classifier.
Figure 14:	PCA Components Overlap with Gradient. Repeat of Fig.2on VGG19 trained on CelebA dataset.
Figure 15: PCA Components Overlap with Gradient. Repeat of Fig.2on ResNet18 trained on CelebA dataset.
Figure 24: PCA Components Overlap with Gradient. Repeat of Fig.2on FCN trained on CIFAR-100 dataset.
Figure 25: PCA Components Overlap with Gradient. Fig.2on CNN trained on CIFAR-100 dataset.
Figure 27: PCA Components Overlap with Gradient. Repeat of Fig.2on ResNet18 trained on FMNISTdataset.
Figure 32:	PCA Components Overlap with Gradient. Repeat of Fig.2on FCN trained on MNIST dataset.
Figure 33:	PCA Components Overlap with Gradient. Fig.2on CNN trained on MNIST dataset.
Figure 36: PCA Components Overlap with Gradient. Repeat of Fig.3on VGG19 trained on CelebA dataset.
Figure 38: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CelebA dataset.
Figure 39: PCA Components Overlap with Gradient. Fig.3on CNN trained on CelebA dataset.
Figure 41: PCA Components Overlap with Gradient. Repeat of Fig.3on ResNet18 trained on CIFAR-10dataset.
Figure 42: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CIFAR-10 dataset.
Figure 43: PCA Components Overlap with Gradient. Fig.3on CNN trained on CIFAR-10 dataset.
Figure 44: PCA Components Overlap with Gradient. Repeat of Fig.3on VGG19 trained on CIFAR-100dataset.
Figure 45:	PCA Components Overlap with Gradient. Repeat of Fig.3on ResNet18 trained on CIFAR-100dataset.
Figure 46:	PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on CIFAR-100 dataset.
Figure 47:	PCA Components Overlap with Gradient. Fig.3on CNN trained on CIFAR-100 dataset.
Figure 50: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on FMNIST dataset.
Figure 51: PCA Components Overlap with Gradient. Fig.3on CNN trained on FMNIST dataset.
Figure 54: PCA Components Overlap with Gradient. Repeat of Fig.3on FCN trained on MNIST dataset.
Figure 55: PCA Components Overlap with Gradient. Fig.3on CNN trained on MNIST dataset.
Figure 56: PCA Components Overlap with Gradient. Repeat of Fig.3on U-Net trained on PascalVOC dataset.
Figure 57: PCA Components Overlap with Gradient. Repeat of Fig.3on U-Net trained on COCO dataset.
Figure 58:	LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated on datasets: CIFAR-10,FMNIST, and MNIST (iid data distribution) using classifier: CNN, and dataset: PascalVOC using U-Netarchitecture.
Figure 59:	LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated for datasets: FMNISTand MNIST (both iid and non-iid data distribution) using classifier: FCN.
Figure 60: LBGM as a Standalone Algorithm. Experimental results in Fig.5repeated for datasets: CIFAR-10,CIFAR-100, FMNIST, MNIST (non-iid data distribution), and CelebA (face landmark regression task) usingclassifier: Resnet18.
Figure 61:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: CIFAR-10,FMNIST, and MNIST (iid data distribution) using classifier: CNN, and dataset: PascalVOC using U-Netarchitecture.
Figure 62:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: FMNIST andMNIST (both iid and non-iid data distribution) using classifier: FCN.
Figure 63:	Effect of δkthreshold on LBGM. Experimental results in Fig.6repeated for datasets: CIFAR-10,CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
Figure 64: LBGM as a Plug-and-Play Algorithm.. Experimental results in Fig.7repeated for dataset: CIFAR-10,FMNIST, and MNIST (iid data distribution) using classifier: CNN.
Figure 65:	LBGM as a Plug-and-Play Algorithm.. Experimental results in Fig.7repeated for datasets: FMNISTand MNIST (both iid and non-iid data distribution) using classifier: FCN.
Figure 66:	Effect of δkthreshold on LBGM. Experimental results in Fig.7repeated for datasets: CIFAR-10,CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
Figure 67: Experimental results in Fig.8repeated for dataset:	CIFAR-10, FMNIST, FMNIST (iid datadistribution) using classifier: CNN.
Figure 68:	Experimental results in Fig.8repeated for dataset: FMNIST and MNIST (both iid and non-iid datadistribution) using classifier: FCN.
Figure 69:	Effect of δkthreshold on LBGM. Experimental results in Fig.8repeated for datasets: CIFAR-10,CIFAR-100 (non-iid data distribution), and CelebA (face landmark regression task) using classifier: Resnet18.
Figure 70:	Experimental results in Fig.5repeated for dataset: CIFAR-10, FMNIST, FMNIST (non-iid datadistribution) and CelebA (face landmark regression taks) using classifier: CNN under 50% client sampling forboth Vanilla FL and LBGM.
Figure 71:	Experimental results in Fig.5repeated for dataset: CIFAR-10, FMNIST, FMNIST (iid datadistribution) using classifier: CNN under 50% client sampling for both Vanilla FL and LBGM.
