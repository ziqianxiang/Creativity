Figure 1: Overview of Prime. We use a one-time collected dataset of prior accelerator designs, includingTPU-style [65], NVDLA-style [42], and ShiDianNao-style [10] accelerators to train a conservative surrogatemodel, which is used to design accelerators to meet desired goals and constraints.
Figure 2: An industry-level machine learn-ing accelerator [65].
Figure 3: Left: histogram of infeasible (right orange bar with large score values) and feasible (left cluster ofbars) data points for MobileNetEdgeTPU; Right: zoomed-in histogram (different number of bins) focused onfeasible points highlighting the variable latencies.
Figure 4: Overview of PRIME which trains a con-servative surrogate fθ (xi) using Equation 3. Ourneural net model for fθ (x) utilizes two transformerlayers [59], and a multi-headed architecture which ispooled via a soft-attention layer.
Figure 5: Comparing the total simu-lation time of Prime (for Prime thisis the total time for a forward-passthrough the trained surrogate on a CPU)and evolutionary method on MobileNet-EdgeTPU. Prime only requires about7% of the total simulation time of theonline method.
Figure 6: Comparing the total Simu-lation time needed by Prime and on-line methodS on Seven modelS (Area≤ 100mm2 ) . PRIME only requireSabout 1%, 6%, and 0.9% of the to-tal Simulation time of Evolutionary,MBO, and BayeS Opt, reSpectively, al-though Prime outperformS the beSt on-line method by 41%.
Figure 7: Comparing the total simulation time needed by the P3BO and Evolutionary method on MobileNet-EdgeTPU. Note that, not only the total simulation time of P3BO is around 3.1× higher than the Evolutionarymethod, but also the latency of final optimized accelerator is around 18% for MobileNetEdgeTPU. The totalsimulation time of our method is around 7% of the Evolutionary method (See Figure 5).
Figure 8: Optimization behavior of Firefly optimizer (Online Evolutionary). Observe that the optimizationprocedure converges and plateaus very quickly (at least 1000 iterations in advance) and hence we stop at 8000iterations. In the case of t-RNN Enc and t-RNN Dec, we find that the evolutionary algorithm performs poorlyand we suspect this is because it saturates quite quickly to a suboptimal solution and is unable to escape. This isalso evident from Figures 8h and 8i, where we observe that online optimization plateaus the fastest for theseRNN applications.
Figure 9: tSNE plot of the joint dataset and randomly sampled infeasible data points. The blue points show theaccelerator configurations that are jointly feasible for all the applications. The highlighted point with red starshows the best design proposed by PrimeThe rest of the points show the infeasible points.
Figure 10: Overview of ShiDianNao dataflow accelerator. This dataflow accelerators exhibits an output-stationary dataflow where it keeps the partial results stationary within each processing elements (PEs).
Figure 11: The (a) histogram of infeasible (orange bar with large score values)/feasible (blue bars) data pointsand (b) the sensitivity of runtime to the SiZe of core memory for the MobileNetEdgeTPU [26] dataset.
Figure 12: tSNE plot of the infeasible and feasible hardware accelerator designs. Note that feasible designs(shown in blue) are embedded in a sea of infeasible designs (shown in red), which makes this a challengingdomain for optimiZation methods.
Figure 14: Plot showing the calibration plot of the predicted (y-axis) and actual latencies (x-axis) of acceleratorsfound by PRIME. Compared to Figure 13, observe that all the acclerator configurations lie above y = x, meaningthat Prime predicts a higher latency (y-axis) compared to the actual latency. This means that Prime does notthink that accelerators that attain high-latency values under the simulator, are actually good. We also provide azoomed-in version of the plot on the right, which shows that there are accelerators do have meaningfully distinctlatency predictions under Prime. Observe in the zoomed-in plot that the designs that attain small predictedlatencies also perform relatively better under the actual latency compared to the designs that attain largerpredicted latency of 〜14000-16000 under the Prime surrogate. Optimizing against Prime is still effectivebecause optimization just needs relative correctness of values, not absolutely correct latency predictions.
Figure 13: To verify if the overestimation hypothesis-that optimizing an accelerator against a naive standardsurrogate model is likely to find optimizers that appear promising under the learned model, but do not actuallyattain low-latency values-in our domain, we plot a calibration plot of the top accelerator designs found byoptimizing a naively trained standard surrogate model. In the scatter plot, we represent each accelerator as apoint with its x-coordinate equal to the actual latency obtained by running this accelerator design in the simulatorand the y-coordinate equal to the predicted latency under the learned surrogate. Note that for a large chunk ofdesigns, the predicted latency is much smaller than their actual latency (i.e., these designs lie beneath the y = xline in the plot above). This means that optimizing designs under a naive surrogate model is prone to findingdesigns that appear overly promising (i.e., attain lower predicted latency values), but are not actually promising.
