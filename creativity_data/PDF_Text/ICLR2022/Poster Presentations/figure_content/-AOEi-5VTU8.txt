Figure 1: (a) Exemplary visualization of the matrix square root and its inverse. Given the origi-nal data X ∈R2×n, the matrix square root stretches the data along the axis of small variances andsqueezes the data in the direction with large variances, serving as a spectral normalization for co-variance matrices. The inverse square root, on the other hand, can be used to transform the data intothe uncorrelated structure, i.e., have the unit variance in each dimension. (b) The comparison oferror and speed using the setting of our ZCA whitening experiment. Our MTP and MPA are fasterthan the SVD and the NS iteration, and our MPA is more accurate than the NS iteration. (c) Thecomparison of speed and error in the forward pass (FP). The iteration times of the NS iteration rangefrom 3 to 7, while the degrees of our MTP and MPA vary from 6 to 18. Our MPA computes themore accurate and faster matrix square root than the NS iteration, and our MTP enjoys the fastestcalculation speed. (d) The speed comparison in the backward pass (BP). Our Lyapunov solver ismore efficient than the NS iteration as fewer matrix multiplications are involved.
Figure 2: The results of the numerical tests. (a) The computation speed (FP+BP) of each methodversus different batch sizes. (b) The speed comparison (FP+BP) of each method versus differentmatrix dimensions. (c) The error comparison of each method versus different matrix dimensions.
Figure 3: (a) The architecture changes of ResNet models in the experiment of ZCA whitening.
Figure 4: The function (1 - Z)2 in the range of |z| < 1 and its approximation including Taylorpolynomial and Pade approXimants. The two approximation techniques do not have any spuriouspoles or defect regions.
