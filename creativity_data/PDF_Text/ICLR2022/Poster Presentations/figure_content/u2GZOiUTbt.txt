Figure 1: The overview of our proposed few-shot learning approach. (i) Train the whole-classification network with the entire labels from the training dataset. (ii) Use the encoder fθ toextract the feature vectors for each class in the source tasks and the target task, and compute theirmean (or centroid). (iii) Maximum matching algorithm is applied to map the source task’s centroidsto the target task’s centroids. (iv) Construct the ε-approximation network for the source task(s), andusing the modified dataset of the source task(s) to train the ε-approximation network. (v) Obtainthe Fisher Information matrices for the source task(s) and the target task, and computing the TASbetween them. The source tasks with the smallest TAS are considered as related tasks. (vi) From therelated-training set (consists of the data samples from the related tasks), generate few-shot M -wayK -shot base tasks. Use the support and query sets of the base tasks to episodically fine-tune fθ .
Figure 2: (a) The distribution of TAS found in miniImageNet. (b) The frequency of 64 classes in thetop-8 closest source tasks in miniImageNet. (c) The distribution of TAS found in tieredImageNet.
Figure 3: Distance from source tasks to the target tasks on CIFAR-10 using ResNet-18 backbone.
Figure 4: Distance from source tasks to the target tasks on CIFAR-10 using VGG-16 backbone.
Figure 5: Distance from source tasks to the target tasks on CIFAR-100 using ResNet-18 backbone.
Figure 6:	Distance from source tasks to the target tasks on CIFAR-100 using VGG-16 backbone.
Figure 7:	Distance from source tasks to the target tasks on ImageNet using ResNet-18 backbone.
Figure 8:	Distance from source tasks to the target tasks on ImageNet using VGG-16 backbone.
