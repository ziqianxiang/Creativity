Figure 1:	Illustration of the proposed modal-agnostic bipartite-graph representation of content andstyle. The content is an array of position-sensitive tokens and the style is a set of P.I. tokens.
Figure 2:	An illustration of our content-style separation mechanism.
Figure 3: Overview of the proposed Retriever framework. The name is dubbed because of thedual-retrieval operations: the cross-attention module retrieves style for content-style separation, andthe link attention module retrieves content-specific style for data reconstruction.
Figure 4: Implementation of (a) the style encoder and (b) the decoder in Retriever.
Figure 5: Visualization of (a) VQ codes, decoder link attention map, and the corresponding Melspectrum; (b) co-occurrence map between ground-truth phoneme and style tokens. Strong co-occurrence is labeled with phoneme names. For higher resolution see Appendix B.
Figure 6: Co-Part segmentation and style transfer results on Celeba-Wild. Our method achievesFigure 7: Part-level appearance manipulation on DeepFashion. Our method can retrieve the correctappearance even with occlusion and large deformation.
Figure 7: Part-level appearance manipulation on DeepFashion. Our method can retrieve the correctappearance even with occlusion and large deformation.
Figure 8: Shape and appearance trans-ferring on DeepFashion.
Figure 9: Phoneme-style co-occurrence map of higher resolution.
Figure 10: Visualization of part-style co-occurrence map on Celeba-Wild and DeepFashion dataset.
Figure 11: Style space t-SNE visualization in speech domain: utterances of the same speaker clustertogether.
Figure 12: Style space t-SNE visualization in image domain: style vector #43 is a ‘clothes colorvector’.
Figure 13: Style space t-SNE visualization in image domain: an ensemble of 14 style vectors iden-tifies gender.
Figure 14: Visual ablation study on weight of structural constraint λsc.
Figure 15: Visual ablation study on tokenization module. Tokenizing the images using convolutionscan help our model extract the content and style better, which leads to better transfer results.
Figure 16: Visual ablation study on downsampling rate. Larger downsampling rate leads to coarserCo-part segmentation results.
Figure 17: More co-part segmentation results on Celeba-Wild dataset.
Figure 18: Co-part segmentation results on CUB dataset. We set K = 4. Similar to previous work,due to the geometric concentration loss, we find it struggles with distinguishing between orientationsof near-symmetric objects, which is a common limitation.
Figure 19: Part-levelstyle transfer results on CelebA-HQ dataset for mouth, nose and eye. Theresolution is 256 ×Figure 20: Zero-shot image style transfer. Our model is trained on CelebA-Wild but can be general-ized to artistic images. The artist image (left) is obtained by AdaIN (Huang & Belongie, 2017) fromCelebA-Wild (Liu et al., 2015) and WikiArt (Saleh & Elgammal, 2015) dataset.
Figure 20: Zero-shot image style transfer. Our model is trained on CelebA-Wild but can be general-ized to artistic images. The artist image (left) is obtained by AdaIN (Huang & Belongie, 2017) fromCelebA-Wild (Liu et al., 2015) and WikiArt (Saleh & Elgammal, 2015) dataset.
Figure 21: Visual comparison on Deepfashion dataset: image-level style transfer. Our results aremore natural than the baseline. Results of Lorenz et al. (2019) are cropped from their official web-site: https://compvis.github.io/unsupervised-disentangling.
Figure 22: Visual comparison on Deepfashion dataset: head style transfer. Our results are morenatural than the baseline and contain more image details.
Figure 23: Performance on LibriSpeechdataset.
