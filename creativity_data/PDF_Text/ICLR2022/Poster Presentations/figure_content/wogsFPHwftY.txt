Figure 2: An overview of FIRe. Given a pair of matching images encoded by a CNN encoder, theiterative attention module LIT (Section 3.1) outputs an ordered set of Super-features. A filteringprocess keeps only reliable Super-feature pairs across matching images (Section 3.2§1), which arefed into a Super-feature-level contrastive loss (Section 3.2§2), while a decorrelation loss reduces thespatial redundancy of the Super-features attention maps for each image (Section 3.2§3).
Figure 3: Evolution of the matching quality mea-sured as the ratio of matches coming from the positivepair over all pairs with the query in each training tuple.
Figure 4: Impact of Lattn on the correlationmatrix between attention maps (the darker, thelower is the correlation) at the last iteration ofLIT when training with (left) and without (right).
Figure 5: Performance versus memory for HOW and FIRe. The x-axis shows the average numberof clusters per image used in ASMK (proportional to memory usage). We vary the number offeatures extracted per image before aggregation in t200, . . . , 2000, 2500, . . . , 5000u; solid markersdenote the commonly used settings (1000/2000). FIRe has at most 1,792 features (256 per scale).
Figure 6: Statistics on the feature selected across scales for HOW and FIRe, averaged over the 70queries from ROxford. Left: Among the 1000 selected features, we show the percentage comingfrom each scale. Middle: For each scale, we show the ratio of features that are selected. Right: Totalnumber of features per scale; we extract N “ 256 Super-features regardless of scale.
Figure 7: Varying the number of Super-features N and of iterations T in the Local featureIntegration Transformer.
Figure 8: Varying the number of hard negatives per training tuple.
Figure 9: Performance versus memory when varying the number of selected features at asingle scale for HOW and FIRe. The x-axis represents the average number of vectors per imagein ASMK, which is proportional to the memory, when varying the number of selected features inp25, 50, 100, 150, 200, 300, 400, 500, 600q, FIRe is limited to 256 features.
Figure 11: Measuring Super-feature redundancy. We compute the average cosine similarity be-tween every feature (local feature from HOW or Super-feature from FIRe) and its K most similarfeatures from the same image, for varying K. Results are averaged over the 70 query images of theROxford dataset. Only 256 Super-features can be extracted per image, explaining its maximum xvalue.
Figure 12: Are all Super-features trained? As a sanity check, this plot shows the average number(in percentage) of times (with standard deviation) that the Super-features receive training signal,across training epochs. We also plot the minimum value, which is always significantly positive,showing that each Super-feature ID receives training signal. Thus our loss Lsuper proposed in Equa-tion (8) does not lead to degenerate solutions where some IDs are never selected and never trained.
