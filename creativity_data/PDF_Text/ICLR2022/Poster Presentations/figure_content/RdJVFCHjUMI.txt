Figure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data.
Figure 2: When the signal about the prompt concept within each example (green) is greater than theerror from low-probability transitions between examples, in-context learning succeeds in our latentconcept setting (Theorem 1). Increasing the example length k increases the signal. The signal forin-context learning comes from tokens in both the inputs and the input-output mapping.
Figure 3:	In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINCdataset. Accuracy increases with number of examples n and length of each example k.
Figure 4:	Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left)When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining datahas random transitions, the model sees all token transitions but in-context learning fails. (Right) Whenprompts are from random unseen concepts, in-context learning fails to extrapolate.
Figure 5: In-context accuracy (95%intervals) of Transformers improvesas model size increases on theGINC dataset for vocabulary sizes50, 100, and 150.
Figure 6: In-context accuracies (95% intervals)on GINC with vocab sizes (50, 100, 150) forTransformers and LSTMs. Accuracy improveswith scale even though the pretraining loss maybe the same.
Figure 7: (Left) In-context accuracy varies widely with example ordering. Each training ID refersto a set of training examples. Each dot refers to the in-context learning accuracy of one permutationof the training examples for that particular training ID. (Right) Zero-shot performance can be higherthan one/few-shot performance in some settings in GINC, mirroring the behavior of GPT-3 on somedatasets such as LAMBADA (Brown et al., 2020). The few-shot setting introduces the distractingprompt structure, which can initially lower accuracy.
Figure 8: Example pretraining document snippet (Left) and example prompt with 3 training examples,1 test example, and example length 3 (Right). The delimiter token is the backslash.
Figure 9: The GINC dataset generates sequences from a mixture of HMMs. The HMM hidden statesconsist of entities (v) and properties (s), which index into a memory matrix to produce the observedtoken. The entity and property sequences are sampled from independent Markov chains. The conceptparameter Î¸ is the transition matrix for properties, which defines relations between properties. In thisexample, the sequence of properties [2,3,5,4] relates names to nationalities, defining the in-contexttask. The blue color represents hidden states/observations sampled from the prompt distribution, andthe purple color represents hidden states/observations sampled from the pretraining distribution.
Figure 10: In-context accuracy curve ofthe4 layer Transformer on the GINC dataset when the entitytransition matrix does not have an additional identity component, for vocabulary sizes 50 (left), 100(middle), and 150 (right). In-context learning is still generally successful.
Figure 11:	In-context accuracy ofthe4 layer Transformer on the GINC dataset for vocabulary sizes50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
Figure 12:	In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
Figure 13:	In-context accuracy of the 16 layer Transformer on the GINC dataset for vocabulary sizes50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
Figure 14:	In-context accuracy of the LSTM on the GINC dataset for vocabulary sizes 50 (left), 100(middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.
