Figure 2: Examples highlighting the hybrid discrete-continuous nature of data. Colours indicate samples fromdiscrete (violet) and continuous (cyan) random variables respectively.
Figure 3: Learning phases of Hybrid Memoised wake-sleep. The memory stores a set of discrete latents for eachtraining data point x. The Wake phase updates the memory using samples from the recognition model. TheSleep: Replay phase updates the generative model and the recognition model using the memory. The Sleep:Fantasy phase updates the recognition model using samples from the generative model.
Figure 4: Hybrid memoised wake-sleep (HMWS) learns faster than the baselines: reweighted wake-sleep (RWS)and vimco based on the marginal likelihood, in both the time series model (left), and the scene understandingmodels with learning shape and color (middle) and learning shape only (right). HMWS also learns better sceneunderstanding models. The gradient estimator of vimco was too noisy and failed to learn the time series model.
Figure 5: The generative model of structured time series data consists of (i) a prior that uses LSTMs to firstsample the discrete Gaussian process (GP) kernel expression and then the continuous kernel parameters, and(ii) a likelihood which constructs the final GP kernel expression. The recognition model (on gray background)mirrors the architecture of the prior but additionally conditions on an lstm embedding of data.
Figure 6: Learning to model structured time series data with GaUssian processes (GPs) by first inferring the kernelexpression (shown as text in the top-left corner) and the kernel parameters. The blue curve is the 128-dimensionalobserved signal, and the orange curve is a GP extrapolation based on the inferred kernel. We show the mean(dark orange), the ±2 standard deviation range (shaded) and one sample (light orange).
Figure 7: Generative model of compositional scenes. The prior places a stochastic number of blocks into eachcell, where cells form an imaginary grid on the ground plane. For each cell, a stack of blocks is built by both: i)sampling blocks from a learnable set of primitives and ii) sampling their relative location to the object below (i.e.,either the ground or the most recently stacked block). The likelihood uses a differentiable renderer to producean image. The recognition model (on gray background) mirrors the structure of the prior, but parametrizesdistributions as learnable functions (NN1-NN4) of a CNN-based embedding of the image.
Figure 8: Samples from the posterior when inferring scene parses with color (left) and scene parses withoutcolor diversity (right). Conditioned on a single observation of the front view of a scene (left column), hmwsinfers a posterior over blocks that make up the scene. Three samples from the posterior are shown per scene,sorted by log probability under the model; e.g., the first sample is most probable under hmws. Sampled scenesare rendered from three different camera angles; position of the camera is depicted in the figure insets. Weemphasize that the model has never seen the 3/4-views. The sampled scenes highlight that we are able to handleocclusion, capturing a distribution over possible worlds that are largely consistent with the observation.
Figure 9: Augmenting the learning curves of hmws, rws, vimco on the time series domain in Figure 4 (left)by hmws- and rws+ highlights the importance of hmws’s memory.
Figure 10: Increasing the difficulty of the continuous inference task results in worse performance in hmws.
Figure 11: Comparing hmws and rws for other compute budgets. Increasing the overall compute by increasingK and keeping K = M = N improves performance while HMWS consistently outperforms RWS with equivalentcompute. Note that RWS didn’t run for S = 72 due to memory constraints.
