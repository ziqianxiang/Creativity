Figure 1: Given an input 3D articulated object (a), we propose a novel perception-interactionhandshaking point for robotic manipulation tasks - object-centric actionable visual priors, includingper-point visual action affordance predictions (b) indicating where to interact, and diverse trajectoryproposals (c) for selected contact points (marked with green dots) suggesting how to interact.
Figure 2: Our proposed VAT-Mart framework is composed of an RL policy (left) exploringinteraction trajectories and a perception system (right) learning the desired actionable visual priors.
Figure 3: We show qualitative results of the actionability prediction and trajectory proposal modules.
Figure 4: We present qualitative analysis of the learned trajectory scoring module. In each resultblock, from left to right, we show the input shape with the task, the input trajectory with its close-upview, and our network predictions of success likelihood applying the trajectory over all the points.
Figure 5: Left: qualitative analysis of the trajectory scoring prediction (each column shares the sametask; every row uses the same trajectory); Middle: promising results testing on real-world data (fromleft to right: input, affordance prediction, trajectory proposals); Right: real-robot experiment.
