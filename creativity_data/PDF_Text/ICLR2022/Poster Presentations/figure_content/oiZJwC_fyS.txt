Figure 1: Illustration of tropical operations between polynomials. The polytope of the max (∨) of fand g corresponds to the convex hull of the union of points of the tWo polytopes and the polytope ofsum (+) corresponds to their MinkoWski sum.
Figure 2: Neural network with one hid-den ReLU layer. The first linear layer hasweights {aiT} with bias {bi} correspond-ing to i-th node ∀i ∈ [n] and the secondhas weights {cji }, ∀j ∈ [m], i ∈ [n].
Figure 3: Illustration of Zonotope K-means execution. The original zonotope P is generated byci aiT, bi for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7.
Figure 4: Neural Path K-means compared with baseline pruning methods and ThiNet. Horizontalaxis shows the ratio of remaining neurons in each hidden layer of the fully connected part.
Figure 5: Illustration of Zonotope K-means execution. The original zonotope P is generated byci aiT, bi for i = 1, ..., 4 and the negative zonotope Q generated by the remaining ones i = 5, 6, 7.
Figure 6: Neural Path K-means for multi-output neural network compression. In green color wehighlight the weights corresponding to the i-th vector used by Neural Path K-means.
Figure 7: Visualization of K-means in Rd+1+n, where d is the input dimension and n the hiddenlayer size. We color points according to the j-th output component of the network. Black and whitepoints correspond to generators of Pj and Qj respectively. White vertices in positive (brown) clustersand black vertices in negative (blue) clusters are null generators regarding j-th output.
