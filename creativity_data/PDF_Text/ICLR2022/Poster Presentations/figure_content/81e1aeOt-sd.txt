Figure 1: Illustration to compare predictions of the three models Eqs. (2), (3) and (5) starting from thesame state s0. In Fig. 1a, We see that on-policy, i.e., using actions (a0, al), Pdata returns environmentdata, while ρmodel (blue) is biased. We correct this on-policy bias in expectation to obtain PoPc. ThisalloWs us to retain the true state distribution When predicting With these models recursively (c.f.,bottom three lines in Fig. 1b). When using OPC for off-policy actions (a0, a1), POPc does not recoverthe true off-policy state distribution since it relies on the biased model. However, the correctionsgeneralize locally and reduce prediction errors in Fig. 1b (top three lines).
Figure 2: Signed gradient error when using inaccurate models Eq. (9) to estimate the policy gradientwithout (left) and with (right) opc. The background’s opacity depicts the error’s magnitude, whereascolor denotes if the sign of estimated and true gradient differ (red) or coincide (blue). opc improvesthe gradient estimate in the presence of model errors. Note that the optimal policy is θ* = -1.0.
Figure 3: Comparison of OPC (----), mbpo(?) (-----) and SAC (……)on four environments from theMuJoCo control suite (top row) and their respective PyBullet implementations (bottom row).
Figure 4:	Comparison of OPC (-----) and mbpo(?) (----) on different variants of the CartPoleenvironment. When the pole,s angle Id is observed directly (center plots), both algorithms successfullylearn a policy. With the sine/cosine transformations (outer plots), mbpo(?) fails to solve the task.
Figure 5:	Ablation study for opc on the HalfCheetah environment. In each plot, we fix the numberof simulated transitions N and vary the rollout lengths H = {1(.....), 5(----), 10(---)}.
Figure 6: Overview about the supporting Lemmas for the proof of Theorem 1.
Figure 7: Cumulative reward for different systems as a function of the policy parameter. The referencetrajectory that is used for OPC was generated by πθn (denoted by the black dashed line). The modelmismatch between the true system and the approximated model is ∆A = 0.5, ∆B = 0.0.
Figure 8: Signed gradient error (see Equation equation 90) when using the approximate model toestimate the policy gradient without (left) and with (right) on-policy corrections (OPCs). Using OPCsincreases the robustness of the gradient estimate with respect to the model error.
Figure 9: Signed gradient error due to off-policy data when using OPC. Note that we retain the truegradient in case of no model error.
Figure 10: Sketch depicting the signed gradient distance Eq. (90). In this particular case, gradient g1is positive and g2 is negative.
Figure 11:	Comparison of opc against a range of baseline methods on three MuJoCo environments.
Figure 12:	Ablation study for opc and mbpo(?) on four environments from the MuJoCo con-trol suite (top row) and their respective PyBullet implementations (bottom row). We vary theretain epochs hyperparameter (indicated by the number in the parentheses behind the legendentries), i.e., the number of epochs that are kept in the data buffer for the simulated data.
Figure 13: Difference in state trajectories sttrue - stpred from branched rollouts using a fixed policy of.
Figure 14: Trajectories of the second state (cos(H)) from 100 branched rollouts using a fixed policyof length H = 20 on the RoboSchool environment (cf. Fig. 4 (left)). Both plots present the samedata but the differ in terms of scaling of the ordinate. With OPC (----------), the respective trajectoriesremain around values close to one, which corresponds to the upright position of the pendulum. Whenusing the standard predictive model from mbpo(?) ( ), the state trajectories often diverge and therollouts are terminated prematurely.
Figure 15: Empirical evaluation of the error terms in the policy improvement bound in Eq. (4) for theCartPole environment (PyBullet). We evaluate the respective terms for a sequence of policies thatwere obtained during different iterations n from a full policy optimization run. The respective returnsηn+ι, ηn, ηn+1,ηn+1 are approximated as the mean from 100 rollouts on the true environment andthe respective models, PnPc (------) and Pmodel (-----). For the return on the true environment ηn (top),we additionally show the sample distribution of the rollouts’ returns. This nicely demonstrates howthe policy smoothly transitions from failing consistently (n ≤ 8) to successfully stabilizing thepole (n ≥ 12). Additionally, note that the on-policy model error is almost always smaller for opccompared to mbpo(?), which supports the theoretical motivation that our method is build upon.
Figure 16: Sample distributions of the return on the CartPole environment (PyBullet) with in-creasing Stochasticity of the behaviour policy ∏roiiout When rolling out with PnPc/OPC (top) andPmOdel/mbpo(?) (bottom). The multiplier β quantifies the Stochasticity of ∏roiiout relative to thereference policy πn (Eq. (92)) such that higher values lead to more ‘off-policy-ness’.
Figure 17: Results based on original implementation Appendix D.7: Comparison of opc to themodel-based mbpo and model-free sac algorithms. The two mbpo variants differ in terms of themix-in ratio β of real off-policy transitions in the training data - a critical hyperparameter. Allmodel-based approaches outperform sac in terms of convergence for the high-dimensional tasks.
Figure 18: Results based on original implementation Appendix D.7: Large ablation study on theHopper environment, investigating the influence various hyperparameters and design choices. Mix-inratio of real data β (columns): 0%, 5%, 10% from left to right. Rollout length H (rows): 1, 5, 10, 20from top to bottom.
