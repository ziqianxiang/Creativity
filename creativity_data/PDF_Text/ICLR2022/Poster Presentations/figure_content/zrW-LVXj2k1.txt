Figure 1: Time-Series Seq-2-Seq models. The MLE config is shoWn on the left and the TMO configis shoWn at the right. The main difference is the output dimension and the loss function. In order tokeep the number of parameters the same, in the TMO model We add an extra layer of size 6 WithRelu activation (shoWn as Adj. (adjustment))For the time-series datasets, the model is a seq-2-seq model With one hidden layer LSTMs for boththe encoder and the decoder. The hidden layer size is h = 256. The output layer of the LSTM isconnected to the final output layer through one hidden layer also having h neurons. Note that h Wastuned in the set [8, 16, 32, 64, 128, 256] and for all datasets and models 256 Was chosen. In order tobe keep the number of parameters exactly the same, in the TMO models We add an extra layer WithReLU With 6 neurons before the output.
Figure 2: Fully connected network for regression models. The MLE config is shown on the left andthe TMO config is shown at the right. The main difference is output dimension and the loss function.
Figure 3: Test squared error versus λmax(Σ). We can clearly see a linear relationship. Each point inthe plot is averaged over 10 runs and we plot the standard error bars.
Figure 4: We plot the average training loss for the MLE(ZNBP)model as a function of trainingiterations. We can observe that the training curve converges.
