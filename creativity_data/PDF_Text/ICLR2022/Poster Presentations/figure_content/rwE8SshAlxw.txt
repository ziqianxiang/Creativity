Figure 1: Illustration of unsupervised discovery of Object Radiance Fields. We aim to infer factorized object andbackground radiance fields from a single view, allowing reconstructing and editing of the scene.
Figure 2: Overview. I. Our model learns to infer a set of latents in a single forward pass. II. Each ob-ject/background radiance field consists of a latent and a shared conditional NeRF. III. During training, werecompose the scene and re-render images for supervision. We train our model on different scenes. At test time,we use a single image of an unseen scene for reconstruction or editing.
Figure 3: Our object-centric latent inference. The atten-tion binds each object,s features to a slot.
Figure 4: Examples on scene segmentation in 3D. Novel view images are for reference but not input.
Figure 5: Qualitative results on scene decomposition and novel view synthesis. Within every two rows, the firstis reconstruction and the second is a novel view.
Figure 6: Qualitative results on single-image 3D scene manipulation. The first two rows are for moving objectand the second two rows are for changing background.
Figure 7: Visual comparison for representing background on view-space on novel view synthesis.
Figure 8: Visual comparison on ablation for foreground locality constraint. We show examples in CLEVR-567testset. We can see that our foreground locality box helps prevent object slots from fitting background segments.
Figure 9: Illustration for foreground decoder architecture. We follow the architecture in NeRF (Mildenhall et al.,2020) but with fewer parameters to decrease space demand. We set the highest positional embedding frequencyto 5, so that the positional embedding input dimension is 5 X 2 X 3 + 3 = 33. The background decoder isslightly different in that it does not have the second last layer and third last layer. Density σ is activated by ReLU.
Figure 10: Demonstration on generalization to real photos. We use uORF pretrained on Room-Diverse and takephotos by a cellphone.
Figure 11: Additional qualitative results for segmentation in 3D on Room-Chair dataset.
Figure 12: Additional qualitative results for segmentation in 3D on Room-Diverse dataset.
Figure 13:	Additional qualitative results for novel view synthesis on CLEVR-567 dataset.
Figure 14:	Additional qualitative results for novel view synthesis on Room-Chair dataset.
Figure 15: Additional qualitative results for novel view synthesis on Room-Diverse dataset.
Figure 16: Additional qualitative results for scene editing.
Figure 17: Additional qualitative results for scene editing.
Figure 18:	Qualitative results for loss evaluations. Using both perceptual loss and adversarial loss improvesimage quality.
Figure 19:	Qualitative results for generalization to unseen spatial arrangement.
Figure 20: Qualitative results for generalization to unseen combination of color and shape.
Figure 21: Failure case of our model, which We call “attention rank-collapse”. All foreground slots share thesame attention map. Every foreground slot decodes to the same radiance field (empty radiance here) rather thanspecializing to an object. Here we only show one object slot, as all others look the same.
Figure 22: Visual comparison with GIRAFFE for inference on CLEVR-567 dataset. GIRAFFE fails inference.
Figure 23: Visual comparison with GIRAFFE for inference on Room-Chair dataset. GIRAFFE fails inference.
Figure 24: Inference trajectory of GIRAFFE using author-provided models on the author-provided datasetCLEVR-2345.
Figure 25: Novel view synthesis on randomly generated examples using author-provided pretrained GIRAFFEmodel on CLEVR-2345. GIRAFFE fails inference of these multi-object scenes. GIRAFFE cannot synthesizenovel views with large rotations.
