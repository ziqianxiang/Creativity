Figure 1: Left column: Images with most pixels masked out. Rest: Completions from our method.
Figure 2: A hierarchical VAE architecture with L = 2 layers of latent variables. Part (a) illustratesthe computation of the ELBO for an unconditional VAE; part (b) illustrates the computation of ourtraining objective Ofo「； and part (c) illustrates the drawing of conditional samples. The encoder isshown in orange; the prior and the decoder (which maintains a deterministic hidden state hi) areboth shown in black; and the partial encoder is shown in blue. The computation graph needed tosample Z in each case is drawn with dashed lines, and the remainder of the computation graph isdrawn with solid lines.
Figure 3: Test metrics on CIFAR-10 and FFHQ-256, plotted as a function of the mask distribution.
Figure 4: ELBO (Eq. (7) computed with X := x) and FID during training using IPA with pretrainingon the same dataset, IPA with pretraining on ImageNet, and when trained from scratch. Error barsshow standard deviations computed with 3 runs. IPA makes training faster and lower-variance.
Figure 5: Left: Classification AUROC scores after 1, . . . , 5 scans chosen with each method. Scoresfor the “EIG-” methods more quickly approach the upper bound achieved by processing the fullimage. Right: Visualisation of BOED used to select three scan locations for diagnosing ‘Effusion’.
Figure 6: Expanded Fig. 3 including results on ImageNet-64 and the LPIPS diversity score.
Figure 7: The faithfulness weighted variance for inpainting on CIFAR-10, FFHQ-256, andImageNet-64, for various values of σ. IPA obtains the best performance on almost all datasetsand values of σ, only being outperformed by IPA-R on CIFAR-10 with high values of σ. This in-dicates that IPA both assigns high probability density to the ground truth (and so performs well forsmall σ) and generates diverse samples (and so performs well for larger σ).
Figure 8: We extend on Fig. 5 by additionally reporting negative log-likelihoods (top row) for theclassification tasks. We also include results from the ‘EPE’ estimator used by Harvey et al. (2019).
Figure 9: AUROC scores when performing one classification task with scan locations chosen for adifferent classification task. The intensity of the colour of each cell is proportional to the differencebetween its value and the greatest value in its row.
Figure 10: Visualisations of BOED processes. On the top, the task is to detect ‘Edema’. In themiddle, we aim to detect ‘Effusion’ in a different patient. The task at the bottom is to detect ‘In-filtration’. Four scan locations are selected for each. The left column shows the observations madebefore selecting each scan location. The next five columns show five of the N = 10 sampled imagecompletions conditioned on these observations. Each is overlaid with the information gain predictedafter scanning any location. The second column from the left shows the expected information gainat each location, given by averaging the information gains arising from each sampled image com-pletion. A red cross marks the maximum. The final column shows the updated observations afterscanning the location which maximises the expected information gain.
Figure 11: OOD-detection on incomplete images. On the left, we show AUROC scores on FFHQ-256 computed separately for masks from varying distributions. On the right we report, for bothCIFAR-10 and FFHQ-256, the average of these scores over all mask distributions.
Figure 12: Breakdown of AUROC foreach mask distribution on CIFAR-10.
Figure 13: Examples of completions lacking semantic diversity in CoModGAN. Panel (a) showsthe true image and the masked version on which the completions are conditioned. Panel (b) shows10 completions sampled randomly from the CoModGAN model. Panel (c) shows the mean imagecomputed from 100 sampled completions. On each row, the completions are mostly semanticallysimilar to eachother yet different from the ground truth image, indicating that they are not faithfullyrepresenting the true posterior. This behaviour can be contrasted with that of IPA in Fig. 14.
Figure 14: Examples completions from our IPA model, to compare with Fig. 13. These samplesfrom IPA provide a much better representation of the inherent uncertainty in the image given theobservations. We did not find any (x, y) pairs on which the IPA model failed in the same way asCoModGAN in Fig. 13.
Figure 15: Sampled completions on CIFAR-10. Panel (a) shows test images along with maskedversions on which samples in each row are conditioned. The remaining panels in the top row showsamples from IPA (with an artifact pretrained on the same dataset) with and without a reduced tem-perature, IPA with an artifact pretrained on ImageNet, a conditional VAE trained from scratch, andfinally IPA-R (with an artifact pretrained on the same dataset). The bottom row shows samples fromour baselines. Three samples per masked image are shown for each stochastic method, while thesingle deterministic completion is shown for CE and RFR. All samples are taken with temperature1 where not stated otherwise.
Figure 16: Sampled completions on ImageNet-64. Panel (a) shows a test image and the maskedversion on which samples in each row are conditioned. The remaining panels in the top row showsamples from IPA with and without a reduced temperature, from an IPA-style conditional VAEtrained from scratch, and from IPA-R. The bottom row shows samples CoModGAN.
Figure 17: Sampled completions on FFHQ-256. Panel (a) shows test images along with maskedversions on which samples in each row are conditioned. The remaining panels in the top row showsamples from IPA with and without a reduced temperature and from IPA-R. The other rows showsamples from our baselines. Four samples per masked image are shown for each stochastic method,while the single deterministic completion is shown for CE and RFR.
Figure 18: Comparison of conditional generations on Edges2Bags.
Figure 19: Comparison of conditional generations on Edges2Shoes.
Figure 20: Sampled completions given three different masks for each of two ground truth x-rayimages. For the first ground truth image (top three rows), there is little obvious difference betweenthe completions produced by IPA and by CoModGAN. However, for each of the masks applied tothe second ground truth image (bottom three rows), CoModGAN produces a posterior with minimaldiversity, and which does not appear to include the ground truth. We inspected completion panelsfor CoModGAN on 20 different ground truth images, and found that this type of mode collapseoccurred in 5 of them. IPA, in contrast, produces reliably diverse image completions for any y orground truth image inspected. We do not show samples with a reduced temperature, as we did notuse them for BOED on the basis that this could significantly reduce coverage of the posterior.
