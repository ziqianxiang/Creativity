Figure 1: A visualization of the ACB bonus for Atari games Breakout and Q*bert. Large bonusesoften correspond to the visitation of states on the periphery of the agent,s current experience, forexample upon breaking a novel combination of blocks in Breakout or immediately before Q*bertarrives on an unlit platform. When the agent dies, and the game is reset to a familiar state, intrinsicreward drops precipitously.
Figure 2: The cumulative regretfor ACB (incremental sampling)with different ensemble sizes. Weshow a multi-armed bandit set-ting with 50 actions, comparingexact ordinary least squares (left)with SGD using Polyak averaging(right). Up to a point, increasingensemble size improves cumula-tive regret in both cases.
Figure 3: ACB With features using either penulti-mate layer or gradient features. Overall, gradientfeatures seem to offer improved exploration, asmeasured by the number of unique states visitedby an agent trained exclusively on these bonuses.
Figure 4: Larger ACB ensembles are generallymore stable, resulting in better exploration. Anagent trained exclusively on large-ensemble ACBbonuses will tend to observe more extrinsic re-ward than those with fewer regressors.
Figure 5: Extrinsic reward in Atari games as a function of the number of times the agent interactswith the environment. Each plot corresponds to a different intrinsic reward scheme, and PPO is beingtrained only to maximize this intrinsic reward.
Figure 6: The number of unique states visited (measured by a hashing) in Atari games as a functionof the number of game episodes. Each plot corresponds to a different intrinsic reward scheme, andPPO is being trained only to maximize this intrinsic reward.
Figure 7: Extrinsic reward in Atari games as a function of the number of frames seen by the agent.
