Figure 1: An illustrative comparison between vanilla partition-parallel training and PipeGCN.
Figure 2: A detailed comparison between vanilla partition-parallel training of GCNs and PipeGCN.
Figure 3:	Throughput comparison. Each partition uses one GPU (except CAGNET (c=2) uses two).
Figure 4:	Epoch-to-accuracy comparison among vanilla partition-parallel training (GCN) andPipeGCN variants (PipeGCN*), where PipeGCN and its variants achieve a similar convergence asthe vanilla training (without staleness) but are twice as fast in wall-clock time (see Tab. 4).
Figure 6: Test-accuracy convergencecomparison among different smooth-ing decay rates γ in PipeGCN-GF onogbn-products (10 partitions).
Figure 5:	Comparison of the resulting feature gradient errorand feature error from PipeGCN and PipeGCN-G/F at eachGCN layer on Reddit (2 partitions). PipeGCN-G/F hereuses a default smoothing decay rate of 0.95.
Figure 7: Comparison of the resulting feature gradient error and feature error when adoptingdifferent decay rates γ at each GCN layer on ogbn-products (10 partitions).
Figure 8:	Training time breakdown of vanilla partition-parallel training (GCN), PipeGCN, andPipeGCN with smoothing (PipeGCN-GF).
Figure 9:	The epoch-to-accuracy comparison on “Yelp” among the vanilla partition-parallel training(GCN) and PipeGCN variants (PipeGCN*), where PipeGCN and its variants achieve a similarconvergence as the vanilla training (without staleness) but are twice as fast in terms of wall-clocktime (see the Throughput improvement in Tab. 4 of the main content).
