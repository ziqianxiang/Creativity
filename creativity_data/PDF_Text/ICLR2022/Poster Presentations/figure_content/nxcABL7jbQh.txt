Figure 1: Boundary representations. We contrast the conventional binary boundary representationwith our representation. From left to right, we show (a) top-left crop of an image in a test set,(b) the standard binary representation of ground-truth boundary, (c) the vector transform plot of theprediction (in red) overlaid on the ground-truth representation (in green) and the conventional binaryrepresentation for clarity. Finally (d) shows the zoomed in view of (c) on the yellow rectangle.
Figure 2: Training and Inference overview. The predicted field ^(x) is convolved with pre-selectedfilters to obtain the divergence at pixel boundaries for inference. For visualization, We show thedivergence and the predicted boundary in pixel locations without using the support image.
Figure 3: Qualitative comparison between methods on a Mapillary Vistas image from the test set.
Figure 4: Qualitative comparison between methods.For each of the three examples, from left to righton the first row, there is the original image, the ground truth and the prediction using VT. On thesecond row, there is the prediction using WCL, DCL, and DT. From top to bottom, the first image istaken from Cityscapes test set, the second from Synthia and the third from Mapillary Vistas.
Figure 5: Comparison of the predictions using VT and DT on a randomly selected image fromCityscapes dataset. Going from top to the bottom row, we show the image (left) with the corre-sponding ground truth (right), the predicted divergence (left) and predicted DT (right) and the twoprediction profiles. The profiles show the divergence on predicted VT (left) and the predicted DT(right) versus the distance from the mid-boundary surface for the VT and DT methods. The plotsshow the mean (black line) and the standard deviation (gray shading) around it.
Figure 6: Qualitative results of the boundary direction estimation on three images of MapillaryVistas test set. In the figures, the direction of the boundaries is plotted to the boundary pixels, whichare thickened to ease visualization. Vertical lines correspond to colors in the range of green and bluethat gradually turn into red and pink for horizontal lines.
Figure 7: Qualitative example of line detection using the VT field on three images of MapillaryVistas test set. In the images, the straight lines in the boundaries are identified with the red colors.
Figure 8: Examples of superpixels obtained on three images of Mapillary Vistas test set using fourdifferent methods. From top to bottom: the original images, the superpixels obtained using the VTfield, COB (Maninis et al., 2017), MCG (ArbelaeZ et al., 2014), and SCG (ArbelaeZ et al., 2014),respectively.
Figure 9: Network architecture overview. Schematics of the network architecture highlighting theway HRNet is used and how full resolution boundaries are predicted. Each convolution, except fromthe last one, includes batch normalization (Ioffe & Szegedy, 2015) and a ReLU activation (Nair &Hinton, 2010). As output, we show the predicted boundary; this can be obtained from any methodas we use the same network changing only the post-processing.
