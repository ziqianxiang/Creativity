Figure 1: The structure of the MOPtEqs and HH&D Module. Dotted lines in the figuredenotes the upsampling, downsampling or identity operators to suit the size of each branch,0 denotes multiplication operator,㊉ denotes addition operator, RD operator processes amatrix by removing its diagonal, SIGN operator convert each element of a matrix to itssign and the di = CHiWi with C is the channel number and Hi, Wi are the height andwidth of the feature map.
Figure 2:	Visualization of the channels correlations for the MOptEqs and POptEqs.
Figure 3:	(a) Test accuracy changes with respect to the perturbation size for differentperturbed directions. (b) Test Accuracy for small MOptEqs (w. and w/o. PE) under PGDattack with different inner iterations. (c) Plot of the accuracies for the small MOptEqswithout HH&D trained by the extended loss with respect to various γ.
Figure 4: The convergent iterations’ of ourMOptEqs (λ = 0.001) forward propagation. Relative error is defined asforward root-finding procedure used for ourkTj+1 -Tj k2-kTjk2-A.6.3 ImageNetteBesides the classification for images with small size, we also conduct the experiments on theImagenette3 , which is a subset of 10 classes from ImageNet4 with 9469 training photos andits test set consists of 3925 images. Furthermore, we use the full-size version of Imagenette5further to verify the superiority of our models on large-scale images. The model architecturefor our MOptEqs and MDEQ (for comparison) is the same as the ones used in the multi-scale models’ comparison for CIFAR-10 except adding two downsampling layers in the headof models to downsample the input size from 256 to 64. As for the hyper-parameters, weonly change the weight decay to be 5e - 5, batch size to be 32 and extend the whole trainingepochs to 200 with 100-th, 150-th, 175-th for the learning rate decay for all the models. Allthe hyper-parameters for both MOptEqs and MDEQs are hardly changed during all thedatasets.
