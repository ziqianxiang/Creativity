Figure 1: Visualization of the Dec-POMDP with an addtional teammate set Γ.
Figure 2: Schematics of ODITS.
Figure 3: Performance comparison across various scenarios for Predator Prey (top panel) and Save the City(bottom panel).
Figure 4: ODITS V.S.
Figure 5: Ablations for different compo-nents.
Figure 6: performance com-parison of two algorithmson the modified coin game.
Figure 7:	Architecture details of ODITS and baselines.
Figure 8:	Illustration of Modified Coin Game (left), Predator Prey (middle) and Save the City (right)Modified Coin Game. In this environment, the ad hoc agent needs to cooperate with its teammate to collecttarget coins which depends on the current teammates’ type. The teammates’ types are illustrated as two orderedcolors of coins in Fig. 8. The teammate follows a heuristic strategy and has complete visibility of the entireenvironmental state. It would first move along an axis for which it has the smallest distance from the coin ofits first color. If it collides with obstacles or boundaries, it would first choose random actions in 3 steps, andthen choose actions to move to the target coin again. If there is no coin of its first color, it would move to thecoin of its second color following the same strategy. The ad hoc agent can only access the information 3 gridsnearby itself. We use one-hot coding to represent different entities in each grid and concatenate them together asa vector to construct the observation of the ad hoc agent. The game ends after 20 steps and takes place on a 7 × 7grid containing 6 coins of 3 different colors (2 coins of each color). Each agent has five actions: move up, down,left, right, or pass. Once an agent steps on a coin, that coin disappears from the grid. This game requires agentsto collect as many target coins as possible, which are indicated by the teammates’ types. One right collected coingives a reward of 100, while one false coin gives a reward of -200. As a result, the ad hoc agent needs to infer itsteammate’s goals according to teammates’ behaviors and move to right coins while avoiding meeting false coins.
Figure 9:	t-SNE plots of the learned teammates’ policy representations for Predator Prey (top panel) and Savethe City (bottom panel) in 6 scenarios. For each scenario, we first developed 60 candidate policies by usingexisting MARL open-source implementations. Then, we train a self-supervised policy encoder mentioned in(Raileanu et al., 2020) by learning information from collected behavioral trajectories {(oit, ait)} of all candidatesto represent their policies. Finally, we select 15 policies whose average policy embeddings are distinct from eachother, and split them into 8 training policies and 7 testing policies for each scenario.
