Figure 1: Illustration of finding apoint in the intersection betweenaffine subspaces and low-loss sub-level set. We use three methods:1) random affine subspaces (blue)containing the initialization, 2) burn-in affine subspaces (red) containinga pre-trained point on the trainingtrajectory, and 3) lottery subspaces(purple) whose span is defined bythe steps of a full training trajectory.
Figure 2: An empirical phase transition in training success on 4 datasets (4 rows) for a Conv-2comparing random affine subspaces (column 1) and burn-in affine subspaces with t = 4, 8, 16 burn-in steps (columns 2,3,4). The black-white color maps indicate the empirically measured successprobability Ps (d, , t) in (2.3) in hitting a training loss sublevel set (or more precisely a trainingaccuracy super-level set). This success probability is estimated by training on 10 runs at every trainingdimension d and burn-in time t. The horizontal dashed line represents the baseline accuracy obtainedby training the full model for the same number of epochs. The colored curves indicate the thresholdtraining dimension d*(e,t, δ) in definition 2.1 for δ = 0.1. The threshold training dimensions for the4 training methods are copied and superimposed in the final column.
Figure 3: The threshold training dimension d* (e,t, δ) in definition 2.1. Here We focus on smalldimensions and lower desired accuracies to emphasize the differences in threshold training dimensionacross different training methods. The purple curves are generated via a novel lottery subspacetraining method Which We introduce in section 4. The curves summarize data for 10 runs forConv-2, 5 runs for Conv-3, and 3 runs for ResNet20; the choice of δ Will determine hoW manyruns must successfully hit the sublevel set When reading off d*. The dimensions of the full parameterspace for the experiments With CIFAR-10 are 25.6k for Conv-2, 66.5k for Conv-3, and 272.5kfor ResNet20. On the other tWo datasets, the full parameter space is 20.5k for Conv-2, 61.5k forConv-3, and 272.2k for ResNet20. The black dotted line is the accuracy obtained by training thefull model for the same number of epochs.
Figure 4: Left panel: An illustration of measuring the width of a set S (in green) in a direction g byidentifying x, y ∈ S in maxx,y∈s g ∙ (y - x). The expectation of this width using random vectorsg 〜N(0, IDXD) instead of g is twice the Gaussian width W(S). Intuitively, it is the characteristicextent of the set T over all directions rescaled by a factor between D / VzD + 1 and ∖∕D. Rightpanel: Illustration of projecting manifolds on the unit sphere and Gordon’s escape theorem. Thesame manifold far from the sphere will have a smaller projection to it than the one that is close, andtherefore it will be harder to intersect with an affine subspace.
Figure 5: A comparison between simulated results and our analytic upper bound for threshold trainingdimension of sublevel sets on a synthetic quadratic well. The middle 3 columns show the successprobability Ps(d, , R) as a function of d and for three different values of the distance R betweeninitialization and the global minimum, clearly exhibiting a phase transition (black and white maps).
Figure 6: Accuracy vs. compression ratio for the same data. Compression ratio is defined thenumber of parameters in the full model over the dimension of the subspace (D/d). The dimensionsof the full parameter space for the experiments with CIFAR-10 are 25.6k for Conv-2, 66.5k forConv-3, and 272.5k for ResNet20. On the other two datasets, the full parameter space is 20.5kfor Conv-2, 61.5k for Conv-3, and 272.2k for ResNet20. The curve for each lottery ticketexperiment summarizes data for at least 5 runs. For all other experiments, the curve summarizes datafor 10 runs for Conv-2, 5 runs for Conv-3, and 3 runs for ResNet20. Black dotted lines are theaccuracy of the full model run for the same number of epochs.
Figure 7: An empirical phase transition in training success on 3 datasets (3 rows) for a Conv-3comparing random affine subspaces (column 1) and burn-in affine subspaces with t = 4, 8, 16 burn-in steps (columns 2,3,4). The black-white color maps indicate the empirically measured successprobability Ps(d, , t) in (2.3) in hitting a training accuracy super-level set. This success probabilityis estimated by training on 5 runs at every training dimension d. The horizontal dashed line representsthe baseline accuracy obtained by training the full model for the same number of epochs. The coloredcurves indicate the threshold training dimension d*(e,t, δ) in definition 2.1 for δ = 0.2.
Figure 8: An empirical phase transition in training success on 3 datasets (3 rows) for a ResNet20comparing random affine subspaces (column 1) and burn-in affine subspaces with t = 8, 16 burn-in steps (columns 2,3). The black-white color maps indicate the empirically measured successprobability Ps(d, , t) in (2.3) in hitting a training accuracy super-level set. This success probabilityis estimated by training on 3 runs at every training dimension d. The horizontal dashed line representsthe baseline accuracy obtained by training the full model for the same number of epochs. The coloredcurves indicate the threshold training dimension d*(e,t, δ) in definition 2.1 for δ = 0.33.
Figure 9: Empirical comparison of the threshold training dimension for the full landscape vs. thelinearized model around a specific local optimum. These experiments were run on the Conv-2model; the top row is on MNIST and the bottom is on Fashion MNIST. The column show a 2-dimensional cut of the test loss landscape defined by the initialization point and two optimum foundvia training. The second column shows the same cut but for the linearized model around optimum 1.
Figure 10: In the main text, we plot a running max accuracy applied to the individual runs because thesubspaces are nested and we are only concerned with the existence of an intersection. The accuraciesare plotted here without this preprocessing step for comparison with the spectra. Left: Singular valuesfor the lottery subspace experiments on MNIST. Right: Singular values for the lottery subspaceexperiments on Fashion MNIST and an additional run on CIFAR-100. Only the first 5 spectra (out of10) are shown for Conv-2. Directions were added in order of descending singular values.
Figure 11: In the main text, we plot a running max accuracy applied to the individual runs beacuse thesubspaces are nested and we are only concerned with the existence of an intersection. The accuraciesare plotted here without this preprocessing step for comparison with the spectra. Left: Singularvalues for the lottery subspace experiments on CIFAR-10. Only the first 5 spectra (out of 10) areshown for Conv-2. Directions were added in order of descending singular values. Right: Lotterysubspaces display accuracy transition around d = 10 for the dataset CIFAR-10. Provides additionalevidence for the conjecture that the shaprest directions of the Hessian are each associated with a classbut no learning happens in them.
Figure 12: First 128 dimensions for a subset of the random affine and burn-in affine subspaceexperiments. The plots include a value at dimension 0 which indicates the accuracy of the randominitialization or the burn-in initialization.
Figure 13: Illustration in tWo dimensions Why the projection of the principal axes of an ellipse ontothe unit circle Will loWer bound the size of the projected set. The linear extent of the projection Willresult from a line that lies tangent to the ellipse.
