Figure 1: (a) Sequence prediction in spatial navigation tasks test abstract spatial understanding sincesome sensory predictions can only be done by knowing (generalising) certain rules e.g. North +East + South + West = 0or Parent + Sibling + Niece = 0. Note, We use sequences drawnfrom much larger graphs. (b) Transformer with recurrent position encodings. (c) Real grid cellrate-maps (Hafting et al., 2005). (d-f) Learned position embedding rate-maps (i.e. average activityat each spatial location; plots are spatially smoothed). (d-e) Resembling grid cells with (e) linearactivation or (e) ReLU activation post transition. (f) Resembling band cells (Krupic et al., 2012).
Figure 2: (a) The TEM model, with a path integration component (equation 3) and a memorynetwork component (equation 5 and 6). TEM path integrates g and makes sensory predictions x viaits memory network (dashed lines are additional connections for inference). (b) TEM recapitulatesa host of empirically described cell representations (Whittington et al., 2020). Top/bottom row:example TEM MEC/Hippocampal representations (plots are spatially smoothed). Figures adaptedfrom Whittington et al. (2020). (c) Schematic of TEM (adapted from Sanders et al. (2020)), showingthat the same cortical representations (LEC and MEC) are reused in different environments allowingfor generalisation, facilitated by different hippocampal combinations. (d) The TEM hippocampalconjunction is an outer product - cells receive input from particular MEC and LEC cells.
Figure 3: Self-attention in (a) Transformers and (b) TEM.
Figure 4: TEM-t is a more efficient learner than TEM, both in (a) sample efficiency and (b) timeper gradient step. Zero-shot accuracy is prediction accuracy when taking links it has never takenbefore, but to a state it has visited before. Successful accuracy here is only possible with learned andgeneralised spatial knowledge. We have used the code from TEM from the TEM authors originalcode https://github.com/djcrw/generalising-structural-knowledge, andso have not optimised it for speed of learning etc, so We cannot claim this to be a fair compari-son, nevertheless the difference is stark. We note that in the TEM paper, the authors say it takes upto 50,000 gradient updates for full training, whereas We stopped at 20,000.
Figure 5: TEM-Transformer neural architecture. (a) Krotov & Hopfield (2020) describe a neu-rally plausible architectural instantiation the 'Hopfield networks is all you need, with a separationbetween ‘feature, neurons (i.e. h) and memory neurons (i.e. SOftmax(qtKT). (b-c) This can beextended for TEM-t, but now the feature neurons are not all updated simultaneously, but only thoseacross brain regions. (d) Memory neurons resemble hippocampal place cells and (e) remap ran-domly across environments. (f) A possible architecture where cortical neurons project to featureneurons in hippocampus which in turn project to memory neurons in hippocampus. (g) Additionalbrain regions can be included easily in this architecture with minimal increase in hippocampal neu-ron number.
Figure 6: Learning to predict the next sensory observation in environments that share the samestructure but differ in their sensory observations. TEM only sees the sensory observations andassociated action taken, it is not told about the underlying structure - this must be learned.
Figure 7: Schematic to show the model flow. Depiction of TEM at two time-points, with eachtime-point described at a different level of detail. Timepoint t shows network implementation, t +1describes each computation in words. Red is for model predictions, green is for updating modelvariables. We do not show the stabilising position encodings module here. Circles depict neurons(blue is g, red is x, blue/red is p); shaded boxes depict computation steps; arrows show learnableweights; looped arrows describe recurrent attractor. Black lines between neurons in attractor de-scribe Hebbian weights M. Wa are learnable, action dependent, transition weights. Wg and Wxare learnable projection matrices. Yellow arrows show training errors.
Figure 8: Memory formation and retrieval in TEM. (a-b) Memory formation. (a) Projected sensorysensory code X and projected grid code g are combined Via an outer-product XTg, which is flattenedto obtain a vector of place cells p. Each place cell (denoted by a single diagonally divided cell) isa conjunction of an element from each of X and g (denoted by the two colours composing eachcell). The actiVity of the place cell is the product of the Values of these elements. (b) A new Hebbianmemory pTp is added to the existing memory matrix M. (c-d) Memory retrieVal. (c) Multiplicationof the query q with the memory matrix M retrieVes a place code p. This retrieVed code is the sumof preViously experienced codes, weighted by their similarity to the present query. This may berepeated iteratiVely to conVerge to the stored p that is most similar to q. (d) The retrieVed place codeP is reshaped and summed along the rows to average-out the g components. The result is gx.
Figure 9: Learned grid cells ordered by grid score. We only show cells that are both active and havea grid score above 0. A gird score of 0.3-0.5 is generally considered to be a grid cell. The panels onthe right hand side are the show the grid scores for the whole population of cells (though in somecases the grid score was not calculable e.g. if the cell has no activity; these cells are ignored in theanalysis). (a) Grid cells learned with a linear activation function. (a) Grid cells learned with a ReLuactivation function.
Figure 10: Learned grid cells in hexagonal 6-connected world.
Figure 11: Place cells ordered by novel place cell metric. This metric assess how place-like the firingof each cell is. In particular, We look at the connected components of the firing rate map, and ourmetric is the ratio of ‘firing mass, in the largest connected component versus all connected compo-nents. This metric is 1 if all the firing is in a single component, and it is lower if the firing is spreadbetween components. (a) TEM-t learned memory place cells. (b) Our novel metric distinguishesbetween TEM-t RNN neurons (grid cells), and TEM-t memory neurons (place cells).
