Figure 1:	Visual transformers vs. MobileViT2B-∣27S26I”U)Σ 24 L≡22.
Figure 2:	MobileViT shows better task-level gener-alization properties as compared to light-weight CNNmodels. The network parameters are listed for SSDLitenetwork with different feature extractors (MobileNetv1(Howard et al., 2017), MobileNetv2 (Sandler et al.,2018), MobileNetv3 (Howard et al., 2019), MNASNet(Tan et al., 2019), MixNet (Tan & Le, 2019b), and Mo-bileViT (Ours)) on the MS-COCO dataset.
Figure 3:	MobileViT shows similar generalization capabilities as CNNs. Final training and vali-dation errors of MobileNetv2 and ResNet-50 are marked with ? and ◦, respectively (§B).
Figure 4:	Every pixel sees every other pixel in the MobileViT block. In this example, the redpixel attends to blue pixels (pixels at the corresponding location in other patches) using transformers.
Figure 5:	Multi-scale vs. standard sampler.
Figure 6: MobileViT vs. CNNs on ImageNet-1k validation set. All models use basic augmentation.
Figure 7: MobileViT vs. ViTs on ImageNet-1k validation set. Here, basic means ResNet-style aug-mentation while advanced means a combination of augmentation methods with basic (e.g., MixUp(Zhang et al., 2018), RandAugmentation (Cubuk et al., 2019), and CutMix (Zhong et al., 2020)).
Figure 8: Inference time of MobileViT models on different tasks. Here, dots in green color regionrepresents that these models runs in real-time (inference time < 33 ms).
Figure 9: MobileViT-S learns better representations with multi-scale sampler on ImageNet-1k.
Figure 10: MobileViT’s performance on ImageNet-1k with standard and multi-scale sampler.
Figure 11: Impact of weight decay. Here, results are shown for MobileViT-S model (5.7 M param-eters) on the ImageNet-1k dataset. Results in (c) are with exponential moving average.
Figure 12: Impact of skip connection. Here, results are shown for MobileViT-S model (5.7 Mparameters) on the ImageNet-1k dataset. Results in (c) are with exponential moving average.
Figure 13: Relationship between kernel size (n × n) for convolutions and patch size (h × w)for folding and unfolding in MobileViT. In a and b, the red pixel is able to aggregate informationfrom all pixels using local (cyan colored arrows) and global (orange colored arrows) informationwhile in (c), every pixel is not able to aggregate local information using convolutions with kernelsize of 3 × 3 from 4 × 4 patch region. Here, each cell in black and gray grids represents a patch andpixel, respectively.
Figure 14: Object detection results of SSDLite-MobileViT-S on the MS-COCO validation set.
Figure 15: Object detection results of SSDLite-MobileViT-S on the MS-COCO validation set.
Figure 16: Object detection results of SSDLite-MobileViT-S on the MS-COCO validation set.
Figure 17:	Semantic segmentation results of Deeplabv3-MobileViT-S model on the unseen MS-COCO validation set (left: input RGB image, middle: predicted segmentation mask, and right:Segmentation mask overlayed on RGB image). Color encoding for different objects in the PASCALVOC dataset is shown in the last row.
Figure 18:	Semantic segmentation results of Deeplabv3-MobileViT-S model on the unseen MS-COCO validation set (left: input RGB image, middle: predicted segmentation mask, and right:Segmentation mask overlayed on RGB image). Color encoding for different objects in the PASCALVOC dataset is shown in the last row.
