Figure 1: Accuracy-robustness trade-off obtainedby prominent defenses on CIFAR-10 against '∞perturbations. We compile top models which useadditional data and are available in Croce et al.
Figure 2: (Top): Illustration depictingthe purpose behind introducing helperexamples labelled by a standard clas-sifier. Solid black line: standard net-work. Dashed red line: adversariallytrained network. Dashed blue line:desired decision boundary. (Bottom):x + 2r is visually dissimilar from xand the classifier is not needed to berobust to such large distortions.
Figure 3: Decision boundary learnt by MLP visualized in two dimensions: x1-x2 and x1-x3 respec-tively. Adversarial training improves robustness substantially from 41% to 74%, yet causes about9% drop in accuracy. For each left subplot, x3 = 0.85 and for each subplot on the right, x2 = 0.4.
Figure 4: Final margins for adversarially trained model vs. initial margins before the start of adver-sarial fine-tuning on CIFAR-10 along R0 (= Rinit), R1 and R5 respectively. Dashed line indicatesthe value of ε used during training and evaluation. The increase in margin along R0 is much largerthan that along other directions R1 and R5 .
Figure 6: Difference betweenrobust accuracy (PGD40) ofFigure 5: Accuracy vs. ro-bustness trade-off exhibited bydifferent adversarial defenses.
Figure 5: Accuracy vs. ro-bustness trade-off exhibited bydifferent adversarial defenses.
Figure 7: Toy dataset used in our experiment.
Figure 8:	Decision boundary learnt by MLP visualized in two dimensions: x1-x2 and x1-x3 respec-tively. HAT reduces the margin along x3 and improves accuracy by 4% over AT. For the subplot onthe left, we fix x3 = 0.85 and for the right subplot, x2 = 0.4.
Figure 9:	Final margins for adversarially trained model vs. initial margins before the start of adver-sarial fine-tuning on CIFAR-10 along R10, R15 and R20 respectively. The red dashed line indicatesthe value of ε used during training and evaluation.
Figure 10: Robust accuracy vs. at-tack radius ε of PGD40 .
Figure 11: Loss landscapes surrounding the 1st and 5th example respectively from CIFAR-10 test setfor ResNet-18 trained with '∞ perturbations of size 8/255. Adversarial direction is the worst-casedirection found using PGD20 attack. The loss landscapes are very smooth and do not exhibit thetypical patterns of gradient obfuscation.
Figure 12: Accuracy vs. robustness trade-offexhibited by AT, TRADES, MART and HAT.
Figure 13: HAT accuracy vs. robustness trade-off obtained for different values of γ . From leftto right, we decrease the trade-off parameter βfor TRADES and HAT. Robust accuracy is eval-uated using AutoAttack.
Figure 15: Effect of different choices for obtain-ing helper labels. We query helper label y, viaa standard model fθstd , at x + αr. Choosingα = 1.0 corresponds to the setting used in thispaper and is indicated by the red line. Note thatthe helper example is still defined as x + 2r .
Figure 14: Effect of different helper exampledefinitions. We define a helper example asx + αr. Choosing α = 2.0 corresponds to thesetting used throughout this paper and is indi-cated by the red dashed line in the plot. Notethat the helper label is still queried at x + r . Wepick β = 2.5 for HAT.
