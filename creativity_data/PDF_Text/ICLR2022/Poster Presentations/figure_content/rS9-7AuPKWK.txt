Figure 1: (a) The gradient norm of standard training (training over noisy data) is larger than thatof variance training (training over pure noise) at initialization θ(0) = θv(0) ≈ 0. We denote theminimizers of the training loss by θ, θv , respectively (where the y-axis is the training loss). (b) Thewhole training loss landscape is highly non-convex while the trajectory of the variance training liesin a convex region due to the slow convergence. We defer the details to Appendix B.
Figure 2: Experiments of neural networks on synthetic (linear ground truth, 3-layer NN, SGD) andreal-world (MNIST, 3-layer CNN, Adam) datasets. The trend of excess risk dynamics (blue) meets itsbias component (BER) at the beginning phase and meets its variance component (VER) afterwards,indicating that ER can indeed be decomposed into VER and BER. See Appendix A for more details.
Figure 3: DDC holds in overparameterized linear regression, general matrix recovery, and neural networks.
Figure 4: Numerical results in Section A.1.
Figure 5: Three excess risk dynamics for CIFAR-10.
