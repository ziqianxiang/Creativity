Figure 1: Visual representation learning typically consists of training an image embedding function, F :X → e, given a dataset of real images {xi}N=1 (left panel). In our work (right panel), We study how tolearn representations given instead a black-box generative model G. Generative models allow us to samplecontinuous streams of synthetic data. By applying transformations TZ on the latent vectors Z of the model, Wecan create multiple data “views” that can serve as effective training data for representation learners.
Figure 2: Different ways of creating multiple views of the same “content”. (a) SimCLR (Chen et al., 2020c)creates views by transforming an input image with standard pixel-space (X) data augmentations (exampleimages taken from (Chen et al., 2020c)). (b) With a generative model, we can instead create views by samplingnearby points in latent space Z, exploiting the fact that nearby points in latent space tend to generate imageryof the same semantic object. Note that these examples are illustrative, the actual transformations that achievethe best results are shown in Fig. 7.
Figure 3: Three different methods for learning representations. The first row illustrates a standard contrastivelearning framework (e.g., SimCLR (Chen et al., 2020c)) in which positive pairs as sampled as transformationsof training images x. The second and third rows show the new setting we consider: we are given a generator,rather than a dataset, and can use the latent space (input) of the generator to control the production of effectivetraining data. Tx refers to transformations applied in pixel-space and Tz denotes transformations in latent-space. The second row illustrates a contrastive learning approach in this setting and the third row shows anapproach that simply inverts the generative model. For contrastive learning, negatives are omitted for clarity.
Figure 4: Examples of different transformation methods for unconditional IGM data. Top row shows samplesOfBigBiGAN trained on ImageNet1000, and the bottom row shows samples from the StyleGAN2 LSUN Car.
Figure 5: Effect of the distance between latentviews on contrastive learning. We vary the stan-dard deviation of a Gaussian Tz = z + wGaussand measure linear transfer to ImageNet100.
Figure 6: Effect of the number of samples fortraining the representation learner, evaluated us-ing linear transfer to ImageNet100. “Gaussian”refers to the Gaussian views (Tz = z + wGauss).
Figure 7: Examples of different transformation methods for class-conditional IGMS data. The images aresampled from BigGAN.
Figure 8: Effect of the number of samples for training the representation learner, evaluated using linear transferto ImageNet100. “Gaussian” refers to the Gaussian views (Tz = z + wGauss).
Figure 9: Mixing real data and synthetic IGM data as a way of augmenting real data. The plot shows thepercentage of real data replaced the fake one versus the top-1 percent accuracy.
Figure 10: Examples of different latent transformation methods for unconditional IGMs data using BigBiGANand StyleGAN LSUN Car (the last three rows). Columns from left: anchor, Gaussian neighbors, and steeringneighbors.
Figure 11: Examples of different latent transformation methods for conditional IGMs data using BigGAN.
