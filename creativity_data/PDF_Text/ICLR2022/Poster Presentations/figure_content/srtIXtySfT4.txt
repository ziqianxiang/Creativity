Figure 1: Comparison of related tasks. Neural Parameter Allocation Search (NPAS) is a noveltask where the goal is to train a neural network given a fixed parameter budget. This generalizesprior work, which supports only a subset of NPAS,s settings. E.g., pruning can decrease networkparameters, but starts from a full network, and many cross-layer parameter sharing works rely onhand-crafted strategies that work for only a limited set of architectures. See Section 2 for discussion.
Figure 2: Overview of Shapeshifter Networks (SSNs). To train a SSN we begin by learning amapping of layers to parameter groups (Section 3.2). Only layers within the same group shareparameters. Then, during the forward pass each layer uses its shared parameters to generate theweights it needs for its operation (Section 3.1). Note that SSNs do not change a network’s architectureor loss function, and automates the creation of a parameter sharing strategy for a network.
Figure 3: Comparison of reducing the parameter parameters by adjusting layer sizes vs. our SSNs us-ing the same number of parameters. We also compare to prior work in parameter sharing, specificallySWRN (Savarese & Maire, 2019). Note that due to architecture assumptions made by SWRN, itcannot reduce the number of parameters of a WRN-28-10 by more than than 33% of the original and58% of the parameters of a WRN-50-2. In contrast, we can not only use far fewer parameters thatperform better than SWRN, but we can also implement even larger networks (WRN-76-12) withouthaving to increase our parameter usage. See text for additional discussion.
Figure 4: Qualitative comparison of the (a) generated weights for a WRN-16-2 and (b) the parametergroups for a WRN-28-10 model trained on C-100. See Section 4.2 for discussion.
Figure 5: Examples of architectures used to evaluate SSNs. Fully connected (FC) and bidirectionalGRU (Bi-GRU) layers give their in × out dimensions; convolutional layers (conv) give the numberof layers, kernel size, and number of filters. Same colored boxes show “manual” (Table 9) sharing,except for green boxes in WRN, whose layers share no parameters. Layers without parametersomitted.
