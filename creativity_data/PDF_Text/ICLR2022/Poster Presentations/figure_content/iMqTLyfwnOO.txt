Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD betweentwo 100-dimensional Gaussians. Figure 1(a) shows that the injective neural network embedded in theASWD learns data patterns (in the X-Y plane) and produces well-separate projected values (Z -axis)between distributions in a random projection direction. The high projection efficiency of the ASWDis evident in Figure 1(c), as almost all random projection directions in a 100-dimensional space leadto significant distances between 1-dimensional projections. In contrast, random linear mappings inthe SWD often produce closer 1-d projections (Z -axis) (Figure 1(b)); as a result, a large percentage ofrandom projection directions in the 100-d space result in trivially small distances (Figure 1(d)), leadingto a low projection efficiency in high-dimensional spaces.
Figure 2: The first and third columns are target distributions. The second and fourth columns are log2-Wasserstein distances between the target distribution and the source distribution. The horizontalaxis show the number of training iterations. Solid lines and shaded areas represent the average valuesand 95% confidence intervals of log 2-Wasserstein distances over 50 runs. A more extensive set ofexperimental results can be found in Appendix G.1.
Figure 3: FID scores of generative models trained with different metrics on CIFAR10 (left) and CelebA(right) datasets with L= 1000 projections. The error bar represents the standard deviation of the FIDscores at the specified training epoch among 10 simulation runs.
Figure 4: Full experimental results on the sliced Wasserstein flow example. The first and third columnsare target distributions. The second and fourth columns are log 2-Wasserstein distances between thetarget distributions and the source distributions. The horizontal axis shows the number of trainingiterations. Solid lines and shaded areas represent the average values and 95% confidence intervals oflog 2-Wasserstein distances over 50 runs.
Figure 5: Ablation study on the impact from injective neural networks and the optimization ofhypersurfaces on the ASWD. ASWDs with different mappings are compared to GSWDs with differentdefining functions. The first and third columns show target distributions. The second and fourthcolumns plot log 2-Wasserstein distances between the target distributions and the source distributions.
Figure 6: Ablation study on the impact of the regularization coefficient λ. The performance of theASWDs with different values ofλ are compared with other slice-based Wasserstein metrics. The firstand third columns show target distributions. The second and fourth columns plot log 2-Wassersteindistances between the target distributions and the source distributions. In the second and fourth columns,the horizontal axis shows the number of training iterations. Solid lines and shaded areas represent theaverage values and 95% confidence intervals of log 2-Wasserstein distances over 50 runs.
Figure 7: Ablation study on the impact from large values of λ. The performance of the ASWDs withlarge values ofλ, e.g 10 and 100, are compared with the SWD. The first and third columns show targetdistributions. The second and fourth columns plot log 2-Wasserstein distances between the targetdistributions and the source distributions. In the second and fourth columns, the horizontal axis showsthe number of training iterations. Solid lines and shaded areas represent the average values and 95%confidence intervals of log 2-Wasserstein distances over 50 runs.
Figure 8: Ablation study on the impact from the regularization term on the performance of the Max-GSW-NN. The first and third columns show target distributions. The second and fourth columns plotlog 2-Wasserstein distances between the target distributions and the source distributions. In the secondand fourth columns, the horizontal axis shows the number of training iterations. Solid lines and shadedareas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50runs.
Figure 9: Ablation study on the impact from the choice of injective networks. The performance of theASWDs with different types of injective networks are compared with other slice-based Wassersteinmetrics. The first and third columns show target distributions. The second and fourth columns plot log2-Wasserstein distances between the target distributions and the source distributions. In the second andfourth columns, the horizontal axis shows the number of training iterations. Solid lines and shadedareas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50runs.
Figure 10: Ablation study on the choice of the dimensionality dθ of the augmented space. The perfor-mance of the ASWDs with different choices ofdθ are compared with other slice-based Wassersteinmetrics. The first and third columns show target distributions. The second and fourth columns plot log2-Wasserstein distances between the target distributions and the source distributions. In the second andfourth columns, the horizontal axis shows the number of training iterations. Solid lines and shadedareas represent the average values and 95% confidence intervals of log 2-Wasserstein distances over 50runs.
Figure 11: Visualized experimental results of the ASWD on CelebA and CIFAR10 dataset with 10,100, 1000 projections. The first row shows randomly selected samples of generated CelebA images,the second row shows randomly selected samples of generated CIFAR10 images.
Figure 12: Visualized experimental results of different slice-based Wasserstein metrics on the MNISTdataset with 10, 1000 projections. (a) Comparison between the SWD, the GSWD, the DSWD, and theASWD using the 2-Wasserstein distance between fake and real images as the evaluation metric. (b)Comparison between the SWD, the GSWD, the DSWD, and the ASWD using the SWD between fakeand real images as the evaluation metric.
Figure 14: (a) The execution time of different methods and the 2-Wasserstein distance between the realimages and the fake images generated by their corresponding models. Each dot of the curve of SWDcorresponds to the performance of the SWD with the number of projections L = {10,1000,10000},in sequence. Each dot of the other curves correspond to the performance of the other methods withthe number of projections L= {10,1000}, in sequence. (b) Computation cost of calculating differentmetrics, the horizontal axis is the number of samples from the compared distributions, and the horizontalaxis is the averaged time consumption for calculating the distances. It can be found that the evaluatedmetrics tend to scale in O(NlogN).
Figure 13: Randomly selected samples generated by different metrics, 10 and 1000 refer to the numberof projections.
Figure 15: Convergence behavior of SWAEs trained with different slice-based Wasserstein metrics. (a)The 2-Wasserstein distance between the prior distribution pz and the distribution of encoded featureφ(x). (b) The binary cross entropy loss between the reconstruction and real data. (c) The slice-basedWasserstein metric used to train the model.
Figure 16:	Comparisons between the encoded latent space generated by different slice-based Wasser-stein metrics.
Figure 17:	MNIST images randomly generated by SWAEs trained with different metrics.
Figure 18: Top rows are source images and target images, lower rows show transferred im-ages obtained by using different methods with different number of projections. Source andtarget images are from (Bonneel et al., 2015) and https://github.com/chia56028/Color-Transfer-between-Images.
Figure 19: Sliced Wasserstein barycenters generated by the ASWD, the GSWD, the DSWD, and theSWD, and the Wasserstein barycenter.
