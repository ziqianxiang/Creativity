Figure 1: 3 cases to involve activation quantization when optimiz-ing the kth blockâ€™s weight rounding. Activations are quantizedinside the blue block and not quantized inside the orange block.
Figure 2: Measure sharpness on different data distributions among three cases. We adopt the measurementdefined in (Keskar et al., 2016). With the same degree of loss change ratio, those who can tolerate a largerperturbation magnitude enjoy a flatter loss landscape.
Figure 3: Loss surface of the quantized weight for QDrop, Case 1 and 3 on test data and ResNet-18 W3A3.
Figure 6: Impact of calibration data size on ImageNet.
Figure 7: Different ways of inserting activation quanti-zation nodes. Our method obeys the left side one whileAdaQuant adopts the right side one.
