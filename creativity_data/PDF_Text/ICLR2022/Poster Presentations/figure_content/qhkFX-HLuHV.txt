Figure 1: Comparison of our proposed SI-FAR (red) with SOTA approaches for actionrecognition on Kinetics400 dataset.
Figure 2: Overview of SIFAR. A sequence of input video frames are first rearranged into a superimage based on a 3 × 3 spatial layout, which is then fed into an image classifier for recognition.
Figure 3: Swin Transformer does self-attentionin a local window. In SIFAR, when the window(blue box) is within a frame, spatial dependen-cies are learned within a super image (4 frameshere). When the window spans across differ-ent frames (red box), temporal dependencies be-tween them are effectively captured. The spatialpooling further ensures longer-range dependen-cies to be learnt. Best viewed in color.
Figure 4: Grid Layout. We apply a grid to layout the input frames. Illustrated here are severalpossible layouts for 8 frames, i.e., a) 1 × 8, b)and c) 2 × 4, and d) 3 × 3, respectively. Emptyimages are padded at the end if grid is not full.
Figure 5: Visualization by Ablation CAM (Desai & Ramaswamy, 2020)What does SIFAR learn? We apply ablation CAM (Desai & Ramaswamy, 2020), an image modelinterpretability technique, to understand what our models learn. Fig. 5 shows the Class ActivationMaps (CAM) of 4 actions correctly predicted by SIFAR-B-12. Not surprisingly, the model learns toattend to objects relevant to the target action such as the hula hoop in a) and soccer ball in b). In c)and d), the model seems to correctly focus on where meaningful motion happens.
