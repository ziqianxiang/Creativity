Figure 1: Most long-range vehicles(target objects) in this view are be-yond LiDAR range, but are still im-portant for safety. R4D estimates thedistance of target objects by usingreference objects with known dis-tances (e.g., LiDAR detections).
Figure 2: R4D overall architecturefor training and prediction. Targetand references are represented as adtarget graph and the embeddings are fed toan attention module to weigh differ-ent references and aggregate resultsinto a final distance estimate.
Figure 3: The detailed architecture for training and prediction. We use a CNN to extract a featuremap from the input image. Given target, reference, and union bounding boxes, an ROIAlign is usedto crop their corresponding embeddings from the feature map. Given the center and size of the targetand reference boxes, we extract geo-distance embeddings using a MLP. We refer to “Attention &MLP” and its output as the absolute distance head, and “Aux MLP” and its output as the relativedistance head. Relative heads are only used during training. ∆di is the relative distance betweenthe target and the ith reference object.
Figure 4: Attention module tofuse target-reference embeddings.
Figure 5: Data distribution of the Figure 6: An example of our distance augmentation for encour-anonymous long-range dataset. aging learning pairwise relationships.
Figure 7: Comparison between Zhu & Fang(2019) with and without LiDAR information. Weshow the Abs Rel] metric (normalized by the bluebars) for different ground-truth distance ranges.
Figure 8: Visualizing the learnt reference weights. Targets are highlighted in yellow. Referenceswith the largest attention weights are highlighted in red. Other references are colored in blue. Weobserve that the most important references are often close to or in the same lane as the target, andshare a similar pose as the target. Best viewed zoomed in. More images are shown in Section F.
