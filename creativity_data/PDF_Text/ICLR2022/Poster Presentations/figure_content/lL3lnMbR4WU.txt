Figure 1: An example of our open-vocabulary detector with arbitrary texts. After training on base cate-gories (purple), we can detect novel categories (pink) that are not present in the training data.
Figure 2: An overview of using ViLD for open-vocabulary object detection. ViLD distills the knowledgefrom a pretrained open-vocabulary image classification model. First, the category text embeddings and the im-age embeddings of cropped object proposals are computed, using the text and image encoders in the pretrainedclassification model. Then, ViLD employs the text embeddings as the region classifier (ViLD-text) and mini-mizes the distance between the region embedding and the image embedding for each proposal (ViLD-image).
Figure 3: Model architecture and training objectives. (a) The classification head of a vanilla two-stagedetector, e.g., Mask R-CNN. (b) ViLD-text replaces the classifier with fixed text embeddings and a learnablebackground embedding. The projection layer is introduced to adjust the dimension of region embeddings tobe compatible with the text embeddings. (c) ViLD-image distills from the precomputed image embeddings ofproposals with an L1 loss. (d) ViLD combines ViLD-text and ViLD-image.
Figure 4: Qualitative results on LVIS, COCO, and Objects365. First row: ViLD is able to correctly localizeand recognize objects in novel categories. For clarity, we only show the detected novel objects. Second row:The detected objects on base+novel categories. The performance on base categories is not degraded with ViLD.
Figure 5: On-the-fly interactive object detection. One application of ViLD is using on-the-fly arbitrary textsto further recognize more details of the detected objects, e.g., fine-grained categories and color attributes.
Figure 6: Systematic expansion of dataset vocabulary with colors. We add 11 color attributes (red orange,dark orange, light orange, yellow, green, cyan, blue, purple, black, brown, white) to LVIS categories, whichexpand the vocabulary size by 11×. Above we show an example of detection results. Our open-vocabularydetector is able to assign the correct color to each fruit. A class-agnostic NMS with threshold 0.9 is applied.
Figure 7: Systematic expansion of dataset vocabulary with fine-grained categories. We use the systematicexpansion method to detect 200 fine-grained bird species in CUB-200-2011. (a): Our open-vocabulary detectoris able to perform fine-grained detection (bottom) using the detector trained on LVIS (top). (b): It fails atrecognizing visually non-distinctive species. It incorrectly assigns “Western Gull” to “Horned Puffin” due tovisual similarity.
Figure 8: Transfer to PASCAL VOC. ViLD correctly detects objects when transferred to PASCAL VOC,where images usually have lower resolution than LVIS (our training set). In the third picture, our detector isable to find tiny bottles, though it fails to detect the person.
Figure 9: Failure cases on LVIS novel categories. The red bounding boxes indicate the groundtruths of thefailed detections. (a) A common failure type where the novel objects are missing, e.g., the elevator car is notdetected. (b) A less common failure where (part of) the novel objects are misclassified, e.g., half of the waffleiron is detected as a calculator due to visual similarity.
Figure 10: An example of ViLD on PASCAL VOC showing a mask of poor quality. The class-agnosticmask prediction head occasionally predicts masks based on low-level appearance rather than semantics, andthus fails to obtain a complete instance mask.
Figure 11: Typical errors of CLIP on cropped regions. (a): The prediction and the groundtruth have highvisual similarity. (b): Directly resizing the cropped regions changes the aspect ratios, which may cause troubles.
Figure 12: The prediction scores of CLIP do not reflect the quality of bounding box localization. (a): Toppredictions of CLIP on cropped region. Boxes of poor qualities receive high scores, though the classification iscorrect. (b): Top predictions of a vanilla Mask R-CNN model. Box qualities are good while the classificationis wrong. (c): We take the geometric mean of CLIP classification score and objectiveness score, and use it torescore (a). In this way, a high-quality box as well as the correct category rank first.
Figure 13: Model architecture and training objectives for ViLD-ensemble. The learning objectives aresimilar to ViLD. Different from ViLD, we use two separate heads of identical architecture in order to reducethe competition between ViLD-text and ViLD-image objetvies. During inference, the results from the twoheads are ensembled as described in Sec. 3.4. Please refer to Fig. 3 for comparison with other ViLD variants.
