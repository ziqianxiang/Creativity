Figure 1: Comparison on the high-level paradigms of monocular 3D detectors.
Figure 2: Visualization of the sparse LiDAR maps (left) and the dense LiDAR maps (right).
Figure 3: Illustration of the proposed MonoDistill. We first generate the ‘image-like’ LiDARmaps from the LiDAR signals and then train a teacher model using an identical network to thestudent model. Finally, we propose three distillation schemes to train the student model under theguidance of the well-trained teacher net. In the inference phase, only the student net is used.
Figure 4: Left: Regard the center point as the foreground region. Right: Generate foreground regionfrom the center point and the size of bounding box. Besides, the 2D bounding boxes are used as theforeground region for Lof .
Figure 5: Qualitative results. We use green, blue and red boxes to denote the results from baseline,our method, and ground truth. Besides, we use red circle to highlight the main differences.
Figure 6: Errors of depth estimation. We show the errors of depth estimation as a function of thedepth (x-axis) for the baseline model (left) and our full model (right).
Figure 7: Qualitative results for multi-class 3D object detection. The boxes’ color of cars, pedestrian,and cyclist are in orange, green, and purple, respectively.
Figure 8: Qualitative results of our method for 3D space. The boxes’ color of ground truth, baseline,and ours are in red, green, and blue, respectively.
