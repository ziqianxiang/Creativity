Figure 1: (a) Schematic view of the CoCo Game (the inspiration for abp). The architect and the buildershould collaborate in order to build the construction target while located in different rooms. The architecture hasa picture of the target while the builder has access to the blocks. The architect monitors the builder workspacevia a camera (video stream) and can communicate with the builder only through the use of 10 symbols (buttonevents). (b) Interaction diagram between the agents and the environment in our proposed abp. Thearchitect communicates messages (m) to the builder. Only the builder can act (a) in the environment. Thebuilder conditions its action on the message sent by the builder (πB (a|s, m)). The builder never perceives anyreward from the environment. A schematic view of the equivalent abp problem is provided in Figure 7(b).
Figure 2: Agent’s Markov Decision Processes. Highlighted regions refer to MDP coupling. (a) The archi-tect’s transitions and rewards are conditioned by the builder’s policy πB . (b) Architect’s MDP where transitionand reward models implicitly account for builder’s behavior. (c-d) The builder’s transition model depends onthe architect’s message policy πA . The builder’s learning signal r is unknown.
Figure 3: Architect-Builder Iterated Guiding. Agents iteratively interact through the modelling and guidingframes. In each frame, one agent collects data and improves its policy while the other agent’s behavior is fixed.
Figure 4: Methods performances (stars indicate significance with respect to ABIG model according to Welch’st-test with null hypothesis μι = μ2, at level α = 0.05). ABIG outperforms control baselines on all goals.
Figure 5: ABIG transfer performances without retraining depending on the training goal. ABIG agents learn acommunication protocol that transfers to new tasks. Highest performances reached when training on ‘place’.
Figure 6: 6-block-shapes that ABIG can construct in transfer mode when trained on the ‘place’ task.
Figure 7: (a) Schematic view of the CoCo Game. The architect and the builder should collaborate in order tobuild the construction target while located in different rooms. The architecture has a picture of the target whilethe builder has access to the blocks. The architect monitors the builder workspace via a camera (video stream)and can communicate with the builder only through the use of 10 symbols (button events). (b) Schematic viewof the Architect-Builder Problem. The architect must learn how to use messages to guide the builder whilethe builder needs to learn to make sense of the messages in order to be guided by the architect. (c) Interactiondiagram between the agents and the environment in our proposed abp. The architect communicates mes-sages (m) to the builder. Only the builder can act (a) in the environment. The builder conditions its action onthe message sent by the builder (πB (a|s, m)). The builder never perceives any reward from the environmentBuildWorld(c)13Published as a conference paper at ICLR 2022BuildWorld(a)	(b)(c)Figure 8: (a) Vertical view of the interaction diagram between the agents and the environment in ourproposed ABP. Only the architect perceives a reward signal r; (b) Interaction diagram for a standardMARL modelization. Both the architect and the builder have access to environmental rewards rA andrB . Which would contradict the fact that the builder ignores everything about the task at hand; (c) Inverse
Figure 8: (a) Vertical view of the interaction diagram between the agents and the environment in ourproposed ABP. Only the architect perceives a reward signal r; (b) Interaction diagram for a standardMARL modelization. Both the architect and the builder have access to environmental rewards rA andrB . Which would contradict the fact that the builder ignores everything about the task at hand; (c) InverseReinforcement Learning modelization of the ABP. The architect needs to provide demonstrations. Thearchitect does not exchange messages with the builder. The builder relies on the demonstrations {(s, a, s0)t }to learn the desired behavior.
Figure 9: ABIG-driven evolution of message-conditioned action probabilities (builder’s policy) for a simpleproblem where the builder must learn to produce action a2 . Even under unfavorable initial condition thearchitect-builder pair eventually manages to associate a message (here m1) with the winning action (a2). Initialconditions are unfavorable since a1 is more likely than a2 for both messages. (i = 0) Given the initial condi-tions, the architect only sends message m1 since it is the most likely to result in action a2. (i = 1) the builderguiding data only consisted of m1 message therefore it cannot learn a preference over actions for m2 and bothactions are equally likely under m2 . The architect now only sends message m2 since it is more likely thanm1 at triggering a2 . (i = 2) Unfortunately, the sampling of m1 resulted in the builder doing more a1 than a2during the guiding frame and the builder thus associates m2 with a1. The architect tries its luck again but nowwith m1 . (i = 3) Eventually, the sampling results in more a2 actions being sampled in the guiding data and thebuilder now associates m1 to a2 . (i = 4) and (i = 5) The architect can now keep on sending m1 messages toreinforce this association.
Figure 10: abig-no-intent driven evolution of message-conditioned action probabilities for a simple problemwhere builder must learn to produce action a2. Initial conditions are unfavorable since a1 is more likely thana2 for both messages. Without an architect’s guiding messages during training, a self-imitating builder rein-forces the action preferences of the initial conditions and fails (even when evaluated alongside a knowledgeablearchitect as both messages can only yield a1).
Figure 11: Toy experiment analysis (a) Initial conditions: initial probability for each action a given a messagem; distributions of final builder’s preferred actions for each message after applying (b) ABIG and (c) ABIG-no-intent on the toy problem; distributions are calculated over 100 seeds. For each method and each initialcondition, we report the success rate obtained by a knowledgeable architect guiding the builder. At evaluation,the architect has access to the builder’s model and does not ignore the goal. abig always succeeds whileabig-no-intent’s success depends on the initial conditions.
Figure 12: Comparison of the evolution of builder policy properties when applying ABIG and ABIG-no-intenton the ’place’ task in BuildWorld. (a) abig enables much higher performance that abig-no-intent. (b) Bothmethods use self-imitation and thus reduce the entropy of the policy. (c) abig promotes the mutual informationbetween messages and action which indicates successful communication protocols.
Figure 13: Baseline performance depending on the goal: stochastic policy behaves on par with random builder.
Figure 14: Influence of the Vocabulary size for ABIG on the ’place’ task. Performance increases with thevocabulary size.
