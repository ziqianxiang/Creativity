Figure 1: Training curves in MNISTstructured prediction with a ResNetmodel with a 3x7 dimensional bi-nary latent space. Gumbel-Softmaxdiverges later in training (τ = 0.5).
Figure 2: Increasing stability (left to right) makes a function stable to ρ-correlated noisy version of xas it approaches a Gaussian halfspace while keeping a fixed Gaussian volume.
Figure 3: A toy regression exam-ple. The Gaussian function’s meanoutput approaches the target proba-bility of 0.8 with and without stabil-ity regularization. The histogramsshow the outputs clustering aroundthe target without regularization(top) and approaching binary valueswith regularization (bottom) whichaverage to 0.8.
Figure 4: CIFAR-10 recon-structions with a 16x16x4binary latent space (top);generated samples with apixelCNN prior on the la-tent space per class (mid-dle); and conditional sam-ples from a single class (bot-tom)slight improvement over the value reported by Ostrovski et al. for aPixelCNN model trained on raw images. For the conditional CIFAR-10 model we obtain an FID score of 54.3. A comparison of FID scoresfor various models on CIFAR-10 is in table 6 in the appendix. The FIDscores were obtained on a highly compressed representation with astandard PixelCNN. The scores can likely be improved by using lowercompression or an improved PixelCNN but we do not explore this inthis work.
Figure 5: STL-10 reconstructions With a 24x24x4 binary latent space (left); generated samples With apixelCNN prior on the latent space (right).
Figure 6: Validation curves for structured prediction on dynamic MNISTWe find that, in terms of validation likelihood, Rebar performs Worse than Gumbel-Softmax With thedynamic MNIST dataset on this task While stability regularization outperforms all others by a largemargin.
Figure 7: UnsuPervised clustering with a Gaussian Mixture VAE trained with stability regularization.
