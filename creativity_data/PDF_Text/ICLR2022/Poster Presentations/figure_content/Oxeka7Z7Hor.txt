Figure 1: Design of a Gaussian convolution layer: The inputs are Fi channels of GMs with Ni componentseach, which are analytically convolved with Fi learnable kernels, Nk components each. The convolved mixturecontains Fi × Ni × Nk components. After applying the ReLU, a new GM with Np components is fitted. This isrepeated for all of the Fo output feature channels. The resulting tensor becomes the input of the next convolutionchannel, with the number of output channels Fo becoming Fi , and Np becoming Ni . The fitting must be fullydifferentiable with respect to the input of the ReLU. In this example, Fi = 8, Ni = 16, Nk = 5, and Np = 8.
Figure 2: Examples of the fitting pipeline described in Section 4.1: The input mixture is shown in (a) and in (b)after applying the ReLU. (c) and (d) show the coarse approximation (a0) and final weights (a00) of the densefitting. The mixture after the reduction step is shown in (e). The overall fitting error is bound by dense fitting, astheoretically, the reduction can be omitted altogether (or a very high number of Gaussians used).
Figure 3: Failure cases for dense fitting: We show inputs, ReLU reference outputs, and our own. In (a), thezero-crossing is overly smooth, and the output weight too small. The positive Gaussian should have been movedup instead. In (b), covariances have a bad configuration, they should have been reduced, and the position moved.
Figure 4: The GMCN used in for our evaluation uses 5 Gaussian convolution layers (GCL, see right-hand side),integration of feature channel mixtures, and conventional transfer functions. See Section 5 for more details.
Figure 5: GM fitting examples of MNIST digits. 64 Gaussians were used to represent each input sample.
Figure 6: GM fitting examples of a ModelNet object with 128 Gaussians for k-means and EM algorithms and125 Gaussians for Eckart et al. (2016b). The k-means initialization method does not produce smooth surfacefittings, but Gaussians have uniform covariances. EM with k-means produces results that are relatively sharp onedges and smooth on surfaces. EM with random initialization and Eckart produces even smoother surfaces.
Figure 7: Selection of the 800 Gaussian mixtures used for the evaluation of the ReLU fitting method.
Figure 8: GM fitting using a least squares algorithm. We verified that the mixture fits the target at the samplepositions, so the solution of the least-squares problem is correct in all cases. The mixture overshoots however anddoes not fit the ReLU well away from the sampling positions. Increasing the sample count, or selecting bettersamples helps, but even then there is no guarantee, that the mixture won’t overshoot. torch.linalg.lstsqwas used, but the results for torch.linalg.pinv are similar.
Figure 9: Convolution kernels of a 2D data set. It is clearly visible, that all of the parameters (weights, positions,and covariance matrices) adapt to the data, once training progresses.
