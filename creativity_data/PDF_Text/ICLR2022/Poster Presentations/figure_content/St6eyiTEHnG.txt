Figure 1: Illustration of the boundary change in a deep model (b) and a linear model (c) for a 2D dataset(a) when changing the seed for random initialization during the training. Shaded regions correspond tothe area when two deep models in (b) (or two linear models in (c)) make different predictions.
Figure 2: (a) A geometric view of the input space in a ReLU network. Dashed lines correspond to activationconstraints while the colorful solid lines are piece-wise linear decision boundaries. Taking gradient ofthe model’s output with respect to the input returns a vector that is orthogonal to a nearby boundary (pointsin the blue and green regions) or an extension of a nearby boundary (the point in the yellow region). (b)Curves of the model’s sigmoid output σ(tx0) (y-axis) against interpolation parameter t (x-axis).
