Figure 1: HDP-C-MDPContext Distillation. HDP-C-MDP promotes a small number of meaningful contexts and somecontexts will almost surely be spurious, i.e., we will transition to these contexts with a very smallprobability. While this probability is small we may still need to explicitly remove these spuriouscontexts. Here we propose a measure of context spuriosity and derive a distillation procedureremoving these spurious contexts. As a spuriosity measure we will use the stationary distributionof the chain p∞, which is computed by solving p∞ = p∞R. The distillation is then performedas follows: if in stationarity the probability mass of a context is smaller than a threshold εdistilthen transitioning to this context is unlikely and it can be removed. We develop the correspondingdistilled Markov chain in the following result, which we prove in Appendix B.1, while the distillationalgorithm can be found in Appendix C.4.
Figure 2: Cart-Pole Swing-Up. Transition matrices, initial p(z0) and stationary p(z∞) distributionsof the learned context models for Result A. Z0 - Z4 stand for the learned contexts.
Figure 3: Cart-Pole Swing-Up. Time courses the learned context models for Result A. C0 and C 1stand for the ground true contexts, while Z0 - Z4 are the learned contexts.
Figure 4: Cart-Pole Swing-Up. Learning curves for χ = -1 (a), time courses the learned contextmodels using GPMM (b)-(c) and the learned model belief by RNN-PPO (d).
Figure A1: Graphical models for C-MDP modeling. In all panels a stands for action, s - state, o -observation, z - context, while the operator 0 indicates the next time step. A: The context variablez evolves according to a Markov process; B: The context variable z is drawn once per episode; C:The context variable z evolves according to a Markov process and depends on state and/or actionvariables; D: a POMDP.
Figure A2: Graphical modelfor policy optimizationp(ot+1|bts,btz,at)maxQ(bts+1,btz+1,at+1)dot+1at+1XNi ma axQ(st+1,btz+1, at+1) dst+1,iwhere Ni, btz+1 depend on st+1. Finally, we have:Q(st , btz , at)r(st, btz, at) +γ XNima axQ(st+1,btz+1,at+1 ) dst+1 ,which completes the proof.
Figure A3: A probabilistic model for C-MDP with Markovian contextIn summary, our probabilistic model is constructed in Equations A1,A3,A4 and illustrated in Fig-ure A3 as a graphical model. We stress that the HDP in its stick-breaking construction assumes that|C | is infinite and countable. In practice, however, we make an approximation and set |C | = K with alarge enough K .
Figure A4: Loss function (A8) and the empirical distribution of its gradient under two configurationsof q(ν)Recall that ρj∙1 = μj∙1 in (A5). The assumed q* (μj∙k) has the mean at its ground-truth value and thevariance 0.01. According to (A7), maximizing ELBO w.r.t q(ν) is equivalent to:min Eq(V) [KL(q*(μ) || p(μ∣ν))]+ Vν KL (q(V) || P(V)).	(A8)q(ν)where q(μ) is fixed to the assumed optima q* (μ).
Figure A5: Graphical model for theMPC problemμk := (I ― lr )μk + lrμk ,一 ，. ，、一 ，二Ek := (I - lr »k + lr ς k,(A9)where lr is the learning rate.
Figure A6: Policy architecturesFigure A7: Structure of the neural networks predicting the mean of the transition probability. Theblocks R and dt stand for multiplication with the rotation matrix R and discretization time dt. MLPdenotes a multi-layer perceptron. E stands for Eurler angles (pitch, roll, yaw), p, v, and w stands forposition, velocity and angular velocity in the world frame. Operator ∙ stands for the next time stepthe implementation by Lovatto (2019) and modified it to fit our context MDP setting. The statesof the environment are the position of the mass (p), velocity of the mass (v), deviation angle ofthe pole from the top position (θ) and angular velocity (θ). For GPMM we replaced θ with sin(θ)and cos(θ) as was done by Xu et al. (2020). We set the reward function to cos(θ), where θ is thedeviation from the top position. Our transition model predicts the mean change between the nextand the current states and its variance as it is common in model-based RL. Therefore, the structureof our neural network model for mean prediction is s0 = s + MLP(s, a), where s0 is the successorstate and MLP is a multi-layer perceptron predicting s0 - s. The variance in the transition model is atrained parameter. The structure of the neural network predicting the mean of the transition model isdepicted in Figure A7(a).
Figure A7: Structure of the neural networks predicting the mean of the transition probability. Theblocks R and dt stand for multiplication with the rotation matrix R and discretization time dt. MLPdenotes a multi-layer perceptron. E stands for Eurler angles (pitch, roll, yaw), p, v, and w stands forposition, velocity and angular velocity in the world frame. Operator ∙ stands for the next time stepthe implementation by Lovatto (2019) and modified it to fit our context MDP setting. The statesof the environment are the position of the mass (p), velocity of the mass (v), deviation angle ofthe pole from the top position (θ) and angular velocity (θ). For GPMM we replaced θ with sin(θ)and cos(θ) as was done by Xu et al. (2020). We set the reward function to cos(θ), where θ is thedeviation from the top position. Our transition model predicts the mean change between the nextand the current states and its variance as it is common in model-based RL. Therefore, the structureof our neural network model for mean prediction is s0 = s + MLP(s, a), where s0 is the successorstate and MLP is a multi-layer perceptron predicting s0 - s. The variance in the transition model is atrained parameter. The structure of the neural network predicting the mean of the transition model isdepicted in Figure A7(a).
Figure A8: Transition matrices, initial p(z0) and stationary p(z∞) distributions of the learned contextmodels in the Cart-Pole SWing-UP Experiment for Result A. Z0 - Z4 stand for the learned contexts.
Figure A9: Time courses the learned context models in Cart-Pole Swing-Up Experiment. “Unlucky’random seed for MLE was used. C0 and C 1 stand for the ground true contexts, while Z0 - Z4 arethe learned contexts. Reproduction of Figure 3 from the main text.
