Figure 1: Validation results ofMoE(dec) and MoE(tok).
Figure 2: Gating mechanism of MoE(dec). Left: average rout-ing confidence; Right: load of experts.
Figure 3: Gating mechanism of MoE(tok). Left: average Figure 4: Performance of three vari-routing confidence; Right: load of experts.	ants of the Switch Transformer.
Figure 5: Illustration of a training iteration with stochastic experts. For conciseness, we show amodel with only one Transformer layer.
Figure 6: Details of multilingual translation results.
Figure 7: Effect of the consis-tency regularization strengthα on Cs-En translation.
Figure 8: Violin plot of per-formance consistency on Cs-En translation.
Figure 9: BLEU vs. modelsize on De-En translation.
Figure 10: Switch(s) w/o load balancing. Left: average routing confidence; Right: load of experts.
Figure 11: Switch(t) w/o load balancing. Left: average routing confidence; Right: load of experts.
Figure 12: Switch(s) w/ load balancing. Left: average routing confidence; Right: load of experts.
Figure 13: Switch(t) w/ load balancing. Left: average routing confidence; Right: load of experts.
Figure 14: Effects of the number of experts on WMT’16 En-De translation. Left: training perplexity(lower the better) with respect to wall-time (measured in GPU hours); Right: validation BLEU(higher the better) after training for 180 GPU hours with respect to the number of experts, where thesize of Transformer does not change.
