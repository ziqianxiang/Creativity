Figure 1: From a TSP formulated as a graph, we take the line graph (a) and input it into our regretapproximation model (b), which predicts the regret of including each edge in the solution. GLS (c)uses these predictions in conjunction with the original problem graph to quickly find a high-qualitysolution.
Figure 2: An example of a graph and the corresponding line graph. The edges in G are the nodes inL(G), and vice-versa.
Figure 3: Mean optimality gap and computation time per instance for three increasingly difficultproblem sets. The left plot shows non-learning based approaches, where the Nearest Neighbourheuristic, LKH-3, and Concorde typically form the Pareto front. The right plot shows learningbased approaches, where Kool et al. (2018) and our approach form the Pareto front.
Figure 4: Solution quality as a function of computation time for three increasingly difficult problemsets, demonstrating that our method converges to optimal solutions at a faster rate than the evaluatedlearning based approaches. The left plot shows the mean optimality gap. The right plot shows thepercentage of optimally solved problems.
Figure 5: Solution quality as a function of computation time for three increasingly difficult gener-alization tasks, demonstrating that our method is able to generalize from smaller to larger probleminstances better than several learning based approaches. The left plot shows the mean optimalitygap. The right plot shows the percentage of optimally solved problems.
Figure 6: The estimated total effect of edge weight and ten additional features on model validationloss after training based on 100 randomly-sampled feature sets, using Gaussian process-based globalsensitivity analysis. A small effect implies that the feature is not important when predicting regret,and vice-versa.
