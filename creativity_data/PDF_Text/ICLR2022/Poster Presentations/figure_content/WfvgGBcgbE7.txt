Figure 1: Left: HoW well do existing continual learning methods work? We track the average accuracy(over all tasks seen until the current episode) on the Split-miniImagenet dataset and compare our method ModelZoo and its variants (all in bold) to existing continual learning methods (faint lines, see Table 1 for references).
Figure 2: Competition between tasks in continual learning can be non-trivial. In order to demonstrate howsome tasks help and some tasks hurt each other, we run a multi-task learner for a varying number of tasks (X-axis)and track the accuracy on a few tasks from CIFAR100 (each task is a superclass). Each cell represents a differentexperiment, i.e, there is no continual learning being performed here. Cells are colored warm if accuracy is worsethan the median accuracy of that row. For instance, multi-task training with 11 tasks is beneficial for “Man-madeOutdoor” but accuracy drops drastically upon introducing task #12, it improves upon introducing #14, whiletask #17 again leads to a drop. One may study the other rows to reach a similar conclusion: there is non-trivialcompetition between tasks, even in commonly used datasets. As we show, tackling this effectively is the key toobtaining good performance on multi-task learning problems. See Appendix B.1 for a more elaborate version.
Figure 3: Ideally, we want to train synergis-tic tasks together, e.g., Model 1 for P1 usingP3, P6 and Model 3 for P3 using P1, P4, P5.
Figure 4: Ablation studies that show the average per-task accuracy as we vary the size of data replay for ModelZoo (left), the number of past tasks sampled at each episode (middle, b = 1 implies no replay), and compareModel Zoo with an ensemble of Isolated models (right). These results are for the single-epoch setting andare therefore directly comparable to those in Table 2 and Table 1 as far as comparison to other methods isconcerned. Accuracy is roughly the same on Split-CIFAR100 across varying degrees of replay while it improvessignificantly on Split-miniImagenet; this suggests that Model Zoo also works with very small amounts of datareplay. Accuracy on Split-CIFAR100 is consistent as the number of replay tasks is changed but increases onlarger datasets like Split-miniImagenet where there are many more tasks. Finally, the performance of Model Zoois not merely an artifact of ensembling. Even if Isolated is a strong model, a very large ensemble of Isolatedcompares poorly to Model Zoo with 100% replay; this indicates that Model Zoo can effectively leverage datafrom past tasks without forgetting. See the Appendix for more ablation studies.
Figure A1: Pairwise task competition matrix. Cells are colored by the gain(green)/loss(warm) of accuracyof pairwise Multi-Head training as compared to training the row-task in isolation; this is a good proxy for thetransfer coefficient ρij in (5). Although most pairs benefit each other (green), certain tasks, e.g., “Food Container”are best trained in isolation while others such as “Aquatic Mammals” are typically detrimental to most othertasks. One can study this matrix and identify many more such properties. In summary, whether tasks aid or hurteach other is quite nuanced even for CIFAR100.
Figure A2: In order to demonstrate how some tasks help and some tasks hurt each other, we run Multi-Headfor a varying number of tasks (X-axis) and track the accuracy on a few tasks from Coarse-CIFAR100. Theorder of tasks is the same for rows (top to bottom) and the columns (left to right). In other words, the firstcell (the diagonal) indicates the accuracy of the task trained by itself in isolation (Isolated). Cells are coloredwarm if accuracy is worse than the median accuracy of that row. For instance, multi-task training with 11 tasksis beneficial for “Man-made Outdoor” but accuracy drops drastically upon introducing task #12, it improvesupon introducing #14, while task #17 again leads to a drop. One may study the other rows to reach a similarconclusion: there is non-trivial competition between tasks, even in commonly used datasets. Tackling this issueeffectively is the key to obtaining good performance on multi-task learning problemsHowever, the increase is not monotonic with each added task, and if one follows a particular row, thereare non-trivial patterns wherein adding a particular task may deteriorate the performance on the rowtask and adding some other task later may recover the lost accuracy. This is a direct demonstration ofthe tussle between the task competition term (first) and the concentration term (third) in Theorem 2.
Figure A3: Each row is the relative increase/decrease (green/red) in accuracy of a two task Multi-Head learnercompared to Isolated trained on the task corresponding to the particular row; all entries are computed using 100samples/class. Cells are colored green for accuracy gained, and warm for accuracy dropped; the entries in thismatrix are a good proxy for the transfer coefficient ρij in (5). A similar plot for Coarse-CIFAR100 tasks is shownin the right panel of Fig. 2. Split-CIFAR10 and Split-MNIST indicate that most tasks mutually benefit each other.
Figure A4: The iterations of Model Zoo are visualized for the Split-miniImagenet dataset for 20 rounds, with 5tasks selected in every iteration of Model Zoo. Red elements are tasks that were selected by boosting in thatparticular round. We observe that the accuracy of most tasks improves over the rounds, which indicates the utilityof Model Zoo-like training scheme This plot also indicates that Model Zoo can improve the per-task accuracy onnearly all tasks. The model is trained for only a single-epch per boosting round.
Figure A5: Per-task accuracies of Isolated on the Coarse-CIFAR100 dataset for two cases, one with 100samples/class (top) and another with all 500 samples/class (bottom). Two points are very important to note here.
Figure A6: Evolution of task accuracy on Coarse-CIFAR10021Published as a conference paper at ICLR 2022Individual Task Accuracies on SpIit-CIFARlOO100-90-80-70-60-× λTask Accuracy (%)	Task Accuracy (%)505f f	IL二 5⅜∙二.
Figure A7: Evolution of task accuracy on SPIit-CIFAR100Individual Task Accuracies on Split-minilmagenet20100-75-50-25-0-51520ioNumber of TasksFigure A8: Evolution of task accuracy on Split-miniImagenet22Published as a conference paper at ICLR 2022B.8 Comparison To Existing MethodsFigure A9: This figure compares Model Zoo to existing continual learning methods on the Coarse-CIFAR100and Split-CIFAR100 datasets with respect to average task accuracy. Model Zoo and its variants are in bold,similar to the left panel of Fig. 1 (which is for Split-miniImagenet). Isolated-small and Model Zoo-small
Figure A8: Evolution of task accuracy on Split-miniImagenet22Published as a conference paper at ICLR 2022B.8 Comparison To Existing MethodsFigure A9: This figure compares Model Zoo to existing continual learning methods on the Coarse-CIFAR100and Split-CIFAR100 datasets with respect to average task accuracy. Model Zoo and its variants are in bold,similar to the left panel of Fig. 1 (which is for Split-miniImagenet). Isolated-small and Model Zoo-smallsignificantly outperform existing methods. All methods in the figure are run in the single-epoch setting.
Figure A9: This figure compares Model Zoo to existing continual learning methods on the Coarse-CIFAR100and Split-CIFAR100 datasets with respect to average task accuracy. Model Zoo and its variants are in bold,similar to the left panel of Fig. 1 (which is for Split-miniImagenet). Isolated-small and Model Zoo-smallsignificantly outperform existing methods. All methods in the figure are run in the single-epoch setting.
