Figure 2: Equivalence between eva-sion attacks and policy perturbations.
Figure 3: A state adversary h perturbs S into h(s) ∈ Be(S) in the state space; hence, the victim,s policy π isperturbed into ∏h within the Adv-policy-set BH (π); as a result, the expected total reward the victim can gainbecomes Vπh instead of Vπ. A prior work SA-RL (Zhang et al., 2021) directly uses an RL agent to learnthe best state adversary h*, which works for MDPS with small state spaces, but suffers from high complexityin larger MDPs. In contrast, we find the optimal state adversary h* efficiently through identifying the optimalpolicy adversary ∏h*. Our proposed attack method called PA-AD contains an RL-based “director” which learnsto propose policy perturbation πh in the policy space, and a non-RL “actor”, which targets at the proposed πhand computes adversarial states in the state space. Through this collaboration, the director can learn the optimalpolicy adversary ∏h* using RL methods, such that the actor executes h* as justified in Theorem 7.
Figure 4: An overview of PA-AD compared with a heuristic attacker and an end-to-end RL attacker. Heuristicattacks are efficient, but may not find the optimal adversary as they do not learn from the environment dynamics.
Figure 5: Two examples of the outermost boundary with |A| = 3 actions at one single state s. The largetriangle denotes the distributions over the action space at state s, i.e., Πs; π1 , π2 and π3 are three policiesthat deterministically choose a1 , a2 and a3 respectively. π is the victim policy, the dark green area is theBH(π)s : BH(π) ∩ Πs. The red solid curve depicts the outermost boundary of BH(π)s. Note that a policy isin the outermost boundary of BH(π) iff it is in the outermost boundary of BH(π)s for all s ∈ S.
Figure 6: Value space of an example MDP. The values of the whole policy space Π form a polytope (blue) assuggested by Dadashi et al. (2019). The values of all perturbed policies with H also form a polytope (green)as suggested by Theorem 12.
Figure 7:	A simple MDP where MinBest Attacker cannot find the optimal adversary for a given victim policy.
Figure 8:	A simple MDP where the first version of MaxWorst Attacker cannot find the optimal adversary for agiven victim policy.
Figure 9: A simple MDP where the second version of MaxWorst Attacker cannot find the optimal adversaryfor a given victim policy.
Figure 10: Comparison of different attack methods against DQN and A2C victims in Atari w.r.t. differentbudget ’s.
Figure 11: Comparison of different attack methods against PPO victims in MuJoCo w.r.t. different budget ’s.
Figure 12: Comparison of convergence rate between SA-RL and PA-AD in Ant and Cartpole. Results areaveraged over 10 random seeds.
Figure 13: Histograms of victim rewards under different hyperparameter settings of SA-RL and PA-AD onWalker.
Figure 14: Comparison of the optimality of different adversaries. (a) The policy perturbation generated for s1by all attack methods. (b) The values of corresponding policy perturbations. (c) A zoomed in version of (b),where the values of all possible policy perturbations are rendered. Our method finds the policy perturbationthat achieves the lowest reward among all perturbations.
Figure 15: Learning curve of SA-RL and PA-AD attacker against an A2C victim in CartPole.
