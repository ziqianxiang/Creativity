Figure 1: MSEs for three softmax kernel estimators (from left to right): SdM , SdM and angular hybridfor m = 10, n = 1 and input of lenghts r = 5. MSEs are given as functions of: an angle θx,y ∈ [0, π] betweenx and y and r (symmetrized along 0 for length axis) . For each plot, we marked in grey its slice for a fixed r toillustrate that only for the angular hybrid estimator, the MSE goes to zero for both θx,y → 0 and θx,y → π.
Figure 3: Statistical metrics measuring softmax matrix approximation quality on PennTree Bank. For standardestimators, the number of random features are 64, 128, 256, 512. To make fair comparison, for the hybridvariants, the configurations leading to the similar number of FLOPS operations per random feature map creationwere applied. Negative fractions were not reported for FAVOR+ since by definition they are equal to zero.
Figure 4: Left: Step-stone locomotion task. Comparison of the training curves for: the IAP using angularhybrid estimator of m = n = 8 and IAP applying regular FAVOR+ mechanism from (Choromanski et al.,2021b) with m = 256. Both final policies are of similar quality, yet HRF-method requires 3x+ fewer FLOPSto run its trained policy. The visualization of the HRF policy in action and its attention (with filtered out pixelsmarked in red) is in the bottom right corner. Right: Similar setting (and conclusions) but for the robotic-armmanipulation task. The additional five regular RF-configurations did not train by producing Nan loss due tolarge variance of the underlying softmax kernel estimators.
Figure 5: PointwiSe estimation of SM(x, y) for the same-length 64-dim inputs (r = 1.0, r = 1.25 and r =1.5) and various angles θx,y. We used s = 10000 estimated softmax values in each subplot. The true softmaxvalue and the 5th and 95th quantile estimated values are shown by the left y-axis, and the empirical relativeerrors are shown by the right y-axis. Trigonometric estimator and FAVOR+ applied 128 random features.
Figure 6: Comparing different estimators for data with clustering structure. The empirical MSE is obtainedby averaging over 20 trials. For the HRFs, we apply deterministic λ-coefficients. The fact that the empiricalerror for FAVOR+ is not perfectly monotonic in m was first observed in (Luo et al., 2021) (see: Fig. 1: (b)).
Figure 7: Distribution of the lengths of the keys and queries in the PennTree Bank dataset27Published as a conference paper at ICLR 2022Since the HRF mechanisms can be sensitive to the lengths of the keys and the queries, for thelanguage modeling task on the WikiText2 dataset, we added a regularizer to constrain the lengthsof the keys and the queries to be close to 1. However, constraining the lengths close to 1 hurts themodel performance and so we chose to use a temperature scaling as in (Rawat et al., 2019). Thusbefore passing the computed dot product to the softmax layer, we scaled the dot product by τ > 1.
Figure 8: Statistical metrics measuring softmax matrix approximation quality on the WikiText2dataset. For standard estimators, the number of random features are 64, 128, 256, 512. To makefair comparison, for the hybrid variants, the configurations leading to the similar number of FLOPSoperations per random feature map creation were applied. Results reported over 10 runs.
Figure 9: Distribution of the lengths of the keys and queries in the WikiText2 datasetWe trained a 2-layer LSTM model with hidden and output sizes of 200, and used the output asinput embedding for sampled softmax. We sampled 40 negative (other than true) classes out of10000 classes to approximate the expected loss in each training epoch. As observed in Fig. 2, therelative error of all three estimators grow exponentially fast with embedding norm r. Therefore, ifwe keep un-normalized embeddings during sampling, then even though we could get an unbiasedestimation of the loss, the variance could be high. Such bias-variance trade off is also mentionedin paper (Rawat et al., 2019). To solve this issue, we used normalized input and class embeddingsduring sampling to generate biased sampling distribution with lower variance, while keeping un-normalized embeddings to calculate the loss. We trained our model for 80 epochs, with batch sizeequal to 20, dropout ratio in LSTM equal to 0.5. Implementation details could be seen in our Githubrepository.
Figure 10: Language Modeling with softmax sampling: training results on PennTree Bank dataset (50-80 epoch window zoomed in on the right subfigure). The solid line for each method is estimated over 25independent runs. The shaded areas represent perplexity within 1 standard deviation of the average. FA-VOR+/trigonometric mechanisms used 1024 random features. To make fair comparison, for HRFs, the con-figurations leading to the similar number of FLOPS operations per random feature map creation were applied.
Figure 11: An example of the camera image for the bi-manual sweep robotic-arm manipulation task.
