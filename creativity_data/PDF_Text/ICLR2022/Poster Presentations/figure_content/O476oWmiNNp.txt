Figure 1: Visualize the intensity of high-frequency component and their theoretical upper boundsunder different transformer blocks. The blue line is defined by log(kHC [Xl]kF/kHC [X0]kF), andthe red line is estimated using the results in Section 2.2 & 2.3. See details in Appendix F.1.
Figure 2: Visualize the spectral responseof an attention map. We randomly picka sample and depict its first head of4/8/12-th layer. Refer to Appendix F.2.
Figure 3: Illustration of our proposed techniques. (a) recallsthe standard ViT block. (b) and (c) illustrate our proposedAttnScale and FeatScale, which scaling high-pass filter com-ponent and high-frequency signals, respectively.
Figure 4: Visualize cosine similar-ity of attention and feature mapswith/without our proposed methods.
Figure 5:	Visualize the spectrum of attention maps. Each row demonstrates every head at a samelayer, and from top to bottom, the 12 rows correspond to 1 ~ 12-th layer, for left to right, the 6columns correspond to 1 - 6-th head, respectively. Best view in a zoomable electronic copy.
Figure 6:	Visualize the learned weights of DeiT-S + AttnScale. Each sub-plot depicts the scalingweights of the same head for different layers. For left to right, top to bottom, six sub-figurescorrespond to 1 ~ 6-th head, respectively. Best view in color.
Figure 7:	Visualize the learned weights of DeiT-S + FeatScale. Each sub-plot depicts two groups ofscaling weights of the same head for different layers. For left to right, top to bottom, six sub-figurescorrespond to 1 ~ 6-th head, respectively. Best view in color.
Figure 8: Visualize the attention map of DeiT-S with/without AttnSCale. 4 X 4 max pooling has beenapplied. The first row visualizes attention maps without AttnScale, and the second row visualizesattention maps with AttnScale. Each column corresponds to the layer noted by its sub-title. Theattention map are computed from a random sample in ImageNet validation set. We only demonstratethe first head of each layer. Best view in a zoomable electronic copy.
Figure 9:	Visualize the spectrum of attention maps without AttnScale. Each row demonstrates everyhead at a same layer, and from top to bottom, the 24 rows correspond to 1 ~ 24-th layer, for left toright, the 6 columns correspond to 1 ~ 6-th head, respectively. Best view in a zoomable electroniccopy.
Figure 10:	Visualize the spectrum of attention maps With AttnScale. Each row demonstrates everyhead at a same layer, and from top to bottom, the 24 rows correspond to1 ~ 24-th layer, for left toright, the 6 columns correspond to1 ~ 6-th head, respectively. Best view in a zoomable electroniccopy.
Figure 11:	Visualize the proportion of thehigh-frequency component of feature mapswith/without our FeatScale on 12/24 layerDeiT. Refer to Appendix G.3 for details.
Figure 12:	Visualize cosine similarity of at-tention and feature maps with/without our pro-posed methods on 12-layer DeiT. Refer to Ap-pendix F.3 for details.
