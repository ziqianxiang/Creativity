Figure 1: Linear mode connectivity when using permutation invariance. Left: Schematic picture of fourminima A, B, C, D in different basins with an energy barrier between each pair. However, our conjecturesuggests that permuting hidden units of B, C and D would result in B0, C0 and D0 which present the exact samefunction as before permutation while having no barrier on their linear interpolation with A. Middle: Our modelfor barriers in real world SGD solutions. In real world we train networks by running SGD with different randomseeds starting from different initializations. In our model, different final networks are achieved by applyingrandom permutations to the same SGD solution (or equivalently, applying random permutations to the sameinitialization and then running SGD with the same seed on them). Right: Aggregation of our extensive empiricalevidence (more than 3000 trained networks) in one density plot comparing barriers in real world against ourmodel across different choices of architecture family, dataset, width, depth, and random seed. Points in thelower left mostly correspond to lowest barrier found after searching in the space of valid permutations using aSimulated Annealing (SA). For a detailed view on architectures and datasets see Figure 10 in Appendix A.3.
Figure 2: Effect of width on barrier size. From left to right: one-layer MLP, two-layer Shallow CNN,VGG-16 and ResNet-18 architectures on MNIST, CIFAR-10, SVHN, CIFAR-100 datasets. For large width sizesthe barrier becomes small. This effect starts at lower width for simpler datasets such as MNIST and SVHNcompared to CIFAR datasets. A closer look reveals a similar trend to that of double-descent phenomena. MLParchitectures hit their peak at a lower width compared to CNNs and a decreasing trend starts earlier. For ResNet,the barrier size is saturated at a high value and does not change due to the effect of depth as discussed in Figure 3.
Figure 3: Effect of depth on barrier size. From left to right MLP, Shallow CNN, VGG(11,13,16,19), andResNet(18,34,50) architectures on MNIST, CIFAR-10, SVHN, CIFAR-100 datasets. For MLP and ShallowCNN, we fix the layer width at 210 while adding identical layers as shown along the x-axis. Similar behavior isobserved for fully-connected and CNN family, i.e., low barrier when number of layers are low while we observea fast and significant barrier increase as more layers are added. Increasing depth leads to higher barrier valuesuntil it saturates (as seen for VGG and ResNet).
Figure 4: Effect of architecture choice and task difficulty on barrier size. Each row in Figure 4a andFigure 4b shows the effect of task difficulty while each column represents the effect of architecture on a specificdataset. Figure 4c notes that a pair of (architecture, task) has lower barrier if the test error is lower. Therefore,any changes in the architecture or the task that improves the test error, also improves the loss barrier. Effect ofdepth is stronger than (architecture, task) which leads to high barrier values for ResNets on MNIST, SVHN,CIFAR10, CIFAR100, and ImageNet.
Figure 5: Similar loss barrier between real world and our model BEFORE applying permutation. Fromleft to right: one-layer MLP, two-layer Shallow CNN, MLP and Shallow CNN with layer width of 210.
Figure 6: Performance of Simulated Annealing (SA). Two Left: SA2 where We average the weights ofpermuted models first and ψ is defined as the train error of the resulting average model. Two Right: Searchspace is reduced i.e., we take two SGD solutions θ1 and θ2, permute θ1 and report the barrier between permutedθ1 and θ2 as found by SA with n = 2. When search space is reduced, SA is able to find better permutations.
Figure 7: Similar loss barrier between real world and our model AFTER applying permutation, whensearch space is reduced. We observe that reducing the search space makes SA more successful in finding thepermutation to remove the barriers. Specifically, SA could indeed find permutations that when applied to θ1result in zero barrier e.g., MLP for MNIST where depth is 1 (across all width), 2 and 4 (where width is 210)4.3	SIMILARITY OF S AND S0 AFTER SEARCHFigure 7 shows the surprising similarity of the barrier between S and S0 even after applying apermutation found by a search algorithm (when search space is reduced). We also observe thatreducing the search space makes SA more successful in finding the permutation {π} to remove thebarriers. Specifically, in some cases SA could indeed find permutations that when applied to θ1 resultin zero barrier. However, the fact that SA’s success shows a similar pattern for S, S0 provides anotherevidence in support of the conjecture. For example, SA successfully reduces the barrier for bothS , S0 on MNIST and SVHN datasets. Figure 1 (right) summarizes our extensive empirical evidence(more than 3000 trained networks) in one density plot, supporting similarity of barriers in real worldand our model across different choices of architecture family, dataset, width, depth, and random seed.
Figure 8: Train and Test Error/Loss for Width. Solid and dotted lines correspond to train and test respectively.
Figure 9: Train and Test Error/Loss for Depth. Solid and dotted lines correspond to train and test respectively.
Figure 10: Aggregation of empirical evidence on similarity of Real World and Our Model. aggregationof our extensive empirical evidence (more than 3000 trained networks) in one plot comparing barriers in realworld against our model across different choices of architecture family, dataset, width, depth, and random seed.
Figure 12: Performance of Simulated Annealing (SA). Left: SA2 where we average the weights of permutedmodels first and ψ is defined as the train error of the resulting average model. Right: Search space is reducedi.e., we take two SGD solutions θ1 and θ2, permute θ1 and report the barrier between permuted θ1 and θ2 asfound by SA with n = 2. When search space is reduced, SA is able to find better permutations.
Figure 11: Similar loss barrier between real world and our model after applying permutation. Effects ofwidth and depth also holds in this setting. Compared to Figure 5, we observe slight barrier reduction. Reducingsearch space helps SA to find better solutions (see section A.3).
Figure 13: Effect of width and depth on barrier similarity between real world and our model beforepermutation. Search space is reduced here i.e., we take two SGD solutions θ1 and θ2, permute θ1 and reportthe barrier between permuted θ1 and θ2 as found by SA with n = 2. Similarity of loss barrier between realworld and our model is preserved across model type and dataset choices as width and depth of the models areincreased.
Figure 14: Scaling cost for Simulated Annealing. Increasing number of steps exponentially, helps SA to findbetter solutions. As the amount of computation increases the barrier continues to decrease.
Figure 15: Performance of Functional Difference compared to Simulated Annealing. Functional Differ-ence could indeed find better permutations, improving the barrier size between two solutions.
Figure 16:	Effect of label noise on barrier size. Left: Train and Test Loss under different label noise. Middle:Train and Test Error under different label noise. Right: Train barrier (accuracy) under different label noise. Ourresults over 5 different runs show that the barrier size behavior does not change, however including higher levelof noise in labels lead to small increase in barrier size.
Figure 17: Effect of width and depth on barrier similarity between real world and our model beforepermutation: VGG and ResNet families. The left two panels shows the effect of width, while the right twopanels illustrate the depth. As discussed in section 2, the barrier for VGGs and ResNets, for both real world andour model, is saturated at a high value and does not change.
Figure 18: Histogram of barrier values between pairs of 100 networks with the same architecture trainedon MNIST starting from different initializations. We find a permutation for each pair that minimizes thebarrier. This is done for two layer MLPs and repeated for networks of different sizes. Bottom row: Indirectbarrier between two networks A,C is minmimum over all possible intermediate points B of maximum of barrierbetween A, B and barrier between B, C. Left: before the permutation; Right: after the permutation.
Figure 19: Effect of width on barrier size (Test). From left to right: one-layer MLP, two-layer Shallow-CNN,VGG-16, and ResNet-18 architectures and MNIST, CIFAR-10, SVHN, CIFAR-100 datasets. When the taskis hard (CIFAR10, CIFAR100) the test barrier shrinks. For simpler tasks and large width sizes also the barrierbecomes small.
Figure 20: Effect of depth on barrier size (Test). From left to right MLP, Shallow-CNN, VGG(11,13,16,18),and ResNet(18,34,50) architectures and MNIST, CIFAR-10, SVHN, CIFAR-100 datasets. For MLP and Shallow-CNN, we fixed the layer width at 210 while adding identical layers as shown along the x-axis. Similar behavior isobserved for fully-connected and CNN family, i.e., low barrier when number of layers are low while we observea fast and significant barrier increase as more layers are added. Increasing depth leads to higher barrier valuesuntil it saturates (as seen for ResNet).
Figure 21: Effect of width and depth on barrier consistency between real world and our model beforepermutation (Test). Search space is reduced here i.e., we take two SGD solutions θ1 and θ2 , permute θ1 andreport the barrier between permuted θ1 and θ2 as found by SA with n = 2. Similarity of loss barrier betweenreal world and our model is preserved across model type and dataset choices as width and depth of the modelsare increased.
Figure 22: Effect of width and depth on barrier consistency between real world and our model afterpermutation (Test). We observe that reducing the search space makes SA more successful in finding thepermutation {π} to remove the barriers. Specifically, SA could indeed find permutations across differentnetworks and datasets that when applied to θ1 result in almost zero test barrier e.g., MLP across MNIST datasetwhere depth is 1 and width is larger than 26, MLP for MNIST where depth is 2 and 4, and width is 210,Shallow-CNN for MNIST where depth is 2, Shallow-CNN for SVHN where depth is 2 and width is 210.
