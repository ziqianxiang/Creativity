Figure 1: π-manifolds for different π, illustrating that only the fixed point subspace contributes tocapacity. In each panel two manifolds are plotted, with color denoting class label. Stars indicate therandom points rμ for μ ∈ {1, 2} where the orbits begin, and closed circles denote the other points inthe π-manifold. For (a) and (b) the group being represented is G = Z4 and for (c) G = Z3. (a) Here∏(g) is the 2 X 2 rotation matrix R(2∏g∕4). The open blue circle denotes the fixed point subspace{0}. (b) Here ∏(g) is the 3 × 3 block-diagonal matrix with the first 2 × 2 block being R(2∏g∕4)and second 1 × 1 block being 1. The blue line denotes the fixed point subspace span{(0, 0, 1)}. (c)Here π(g) is the 3 × 3 matrix that cyclically shifts entries of length-3 vectors by g places. The blueline denotes the fixed point subspace span{(1, 1, 1)}.
Figure 2: Capacity of GCNN representations. Solid lines denote the empirically measured fractionf(α) of 100 random dichotomies for which a logistic regression classifier finds a separating hyper-plane, where α = P/N0 . Dotted lines denote theoretical predictions. Shaded regions depict 95%confidence intervals over random choice of inputs, as well as network weights in (a) and (c). (a)f(α) ofa random periodic convolutional layer after ReLU (blue line) and followed by 2x2 max pool(orange line), with P = 40 and N0 = # output channels. Max pooling reduces capacity by a factorbetween 1/4 and 1 as predicted by our theory. (b) f(α) of VGG-11 pretrained on CIFAR-10 after aperiodic convolution, batchnorm, and ReLU (blue line), followed by a 2x2 maxpool (orange line),and then another set of convolution, batchnorm, and ReLU (green line), with P = 20 and N0 = #output channels. Max pooling reduces capacity as predicted. (c) f(α) after a random convolutionallayer equivariant to the direct sum representation of Z10 ㊉ Zg as defined in Section 4.3, with P = 16and N0 = 2 (# output channels).
