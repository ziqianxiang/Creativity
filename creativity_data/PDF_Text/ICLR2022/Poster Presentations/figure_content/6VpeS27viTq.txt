Figure 1: Authorized learnability control pipeline, with gψ being a perturbation function where ψserves as the key for the learnability lock. (a) demonstrates the case of an illegal exploitation attemptwhere the unauthorized client trains a model on the learnability-locked dataset; In (b) an authorizeduser can unlock the protected dataset with the inverse transformations and train a normal model.
Figure 2: HeatmaP of the classification confusionmatrix for learnability control on the single class“bird” of CIFAR-10 for (a) linear transformation,and (b) convolutional transformation.
Figure 3: Test accuracy against training epochsfor model training on the perturbed dataset Dpusing different for (a) linear transformation, and(b) convolutional transformation.
Figure 4: Annotated version ofFig. 2 - classification confusion matrix for learnability control on thesingle class “bird" of CIFAR-10 for (a) linear transformation, and (b) convolutional transformation.
Figure 5: Prediction results of a ResNet-50 model trained on partially learnability-controlledCIFAR-10. The classes under control are circled in red. It is easy to see that the model has fairlybad performance on learnability-controlled classes, while nearly unaffected on other classes.
Figure 6: Performance on the learnability locked dataset trained by adapted adversarial training withrespect to the transformation functions. Specifically, “c” stands for adversarial training with Class-wise perturbations, “s” means sample-wise, and “st” means the standard adversarial training usingPGD.
Figure 7: Test accuracy against perturbation percentage for models trained on learnability controlleddataset With different percentage of perturbed samples.
Figure 8: Experimental result for global transformation functions on CIFAR-10 dataset.
