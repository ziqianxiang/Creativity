Figure 1: Two possible approaches to reduce the dimensionality of the higher-depth log-signature.
Figure 2: The loss curve ofLORD is stable. Other figuresare in Appendix F.
Figure 3: The proposed NRDE-based autoencoderAlgorithm 1: HoW to train LORD-NRDEInput: Training data Dtrain, Validating data Dval, Maximumiteration numbers maxAterae and max_itertask1	Initialize the parameters of the encoder (i.e., θf, θφe), thedecoder (i.e., θo, θφs), and the main NRDE (i.e., θg, θφz ,θφoutput).;/* Pre-training of the autoencoder	*/2	for max J.terAE iterations do3	I Train the encoder and the decoder using LAE ;/* Main-training of LORD-NRDE	*/4	for max jiterτAsκ iterations do5	Train the main NRDE using LTASK;6	Validate the best main NRDE parameters with Dval;7	return the encoder and the main NRDE parameters;This rapid blow-up of dimensionality causes two problems : i) it hinders us from applying NRDEs tohigh-dimensional time-series data, and ii) it makes the training process complicated since the hiddenrepresentation of the large input should be made for a downstream task. Therefore, we propose toembed the log-signature onto a lower dimensional space so that the training process becomes more
Figure 4: PCA-based visual- The log-signature transform of NRDEs is suitable to irregularization of log-signatures	and long time-series. However, the log-signature transform re-quires larger memory footprints and to this end, we presentedan autoencoder-based method to embed their higher-dimensional log-signature into a lower-dimensional space, called LORD. Our method is carefully devised to eradicate the higher-dimensional computation as early as right after the pre-training of the autoencoder. In the standardbenchmark experiments of very long time-series, our proposed method significantly outperformsexisting methods and shows relatively smaller model sizes in terms of the number of parameters.
Figure 5: Visualization of {LogSigD∖i+1 (X)}}=P^-1, {LogSigDri+1 (X)}}=pJ-1, and de(t) inEigenWorms and BIDMC.
Figure 6:	Sensitivity to max_iterAEF	Train LossFig. 7 visualizes several training cases for our method and the original NRDE design. In general,our method shows much better stability across the entire training period.
Figure 7:	Train loss of NRDE3 and LORD2→3between LORD and its various end-to-end training variations inTable 17: ComparisonEigenWorms (P = 128)Method	Accuracy	Macro F1	Weighted F1	ROCAUCLORD	0.759±0.064	0.748±0.082	0.760±0.061	0.900±0.009FineTuning	0.744±0.075	0.726±0.086	0.740±0.077	0.882±0.042LORD1→2	Co-Train	0.718±0.021	0.686±0.025	0.715±0.014	0.865±0.030Co-Train(w.o. pre)	0.679±0.033	0.650±0.031	0.669±0.029	0.832±0.013LORD	0.744±0.081	0.716±0.087	0.737±0.076	0.892±0.044FineTuning	0.692±0.055	0.649±0.049	0.680±0.069	0.865±0.018LORD1→3	Co-Train	0.744±0.036	0.723±0.059	0.741±0.037	0.868±0.022Co-Train(w.o. pre)	0.692±0.069	0.674±0.083	0.694±0.064	0.839±0.066LORD	0.841±0.038	0.823±0.055	0.835±0.049	0.949±0.023FineTuning	0.731±0.061	0.687±0.075	0.714±0.062	0.919±0.005LORD2→3	Co-Train	0.808±0.061	0.772±0.074	0.803±0.071	0.948±0.021Co-Train(w.o. pre)	0.583±0.112	0.499±0.145	0.557±0.117	0.804±0.065to train the encoder. The last method is Co-Train(w.o. pre) which is the same as Co-Trainwithout any pre-training process. The same hyperparameters are used for all end-to-end models.
