Figure 1: In the continual meta-RL setting, the agent interacts with a single task at a time and, oncefinished with a task, never interacts with it again. To complete the full sequence of tasks efficiently,the agent must reuse experience from preceding tasks to more quickly adapt to subsequent tasks.
Figure 2: The continual multi-task rein-forcement learning problem for a sequenceof three tasks. The agent applies learningalgorithm L on task Ti and forwards policyparameters θi+1 after each task.
Figure 3: CoMPS is split in to twoprocess: a reinforcement learningstep RL and a meta-learning stepM . The meta-learning step M usesthe data gathered thus far, denotedD0：i，to meta-train θi. RL is initial-ized from these parameters θi forthe next task.
Figure 4: Outline of CoMPS. The left side corresponds to the M block for Figure 3 and the rightthe RL block. On the right (RL), for each round i the policy πθi is initalized using the previouslytrained meta-policy parameters. After j iterations of RL training on task i, the experience for task i iscollected into the off-policy buffer D。” and skilled experience is stored in another buffer D0：i. Thisexperience is given to M that uses the off-policy experience for the inner expected reward updates.
Figure 5: In Ant Goal the non-stationary task distribution selects the next goal location that is furthestfrom all previous locations: e.g., T0:i = ((5, 0), (-5, 0), (0, 5), . . .). In Ant Direction the tasks startat 0° along the x-axis and rotate CCW 70°. The Half Cheetah tasks start With low target velocity andalternate between larger +- velocities: e.g., T0:i = (0.5, -0.5, 1.0, . . .). In MetaWorld, a set ofgoals is randomly choosen across a collection of environments including hammar and close-drawer.
Figure 6: These figures shoW the average number of episodes needed to solve each neW task aftercompleting i tasks (feWer is better). Results are computed over 6 sequences of 20 tasks, averagedover 6 random seeds With the standard error shoWn.
Figure 7: On the left are “lifelong” plots of rewards received for every episode of training over40 tasks. The points are subsampled by averaging 5 points together to make one point in the plot.
Figure 8: These figures show tasks use in the MetaWorld environment.
Figure 9: These figures show the average return for each new task. On average CoMPS achievethe highest average return while training over an entire task at a time. Results are computed over 6sequences of 40 tasks, averaged over 6 random seeds.
Figure 10: These figures show the average return achieved during the RL phase over all tasks. Thisshows average return the agent achieves if any episode is picked at random from any task. Still,CoMPS performs better than other methods considering this analysis. Results are averaged over 6sequences of 40 tasks, and 6 random seeds.
Figure 11: Similar to Figure 6, these figures show the average number of learning iterations it takesfor each method to reach success on the non-stationary tasks. In this case we can see that CoMPSsolves most tasks faster than other methods.
Figure 12: These figures show the average return achieved for each episode in the RL phase over alltasks. The top row is for the stationary task distribution and the bottom for the non-stationary taskdistribution. These graphs show that CoMPS is achieves the highest average return over all othermethods when performance is averaged over tasks. Results are averaged over 6 sequences of 40 tasks,and 6 random seeds.
Figure 13:	These figures show the backwards transfer for CoMPS averaged over prior tasks. They-axis if the normalized reward where 1 means the method retained the original performance on thetask.
Figure 14:	These figures show performance of variations of CoMPS. As we remove features fromCoMPS we can see the performace reduce considerably.
