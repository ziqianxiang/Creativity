Figure 1: Noisy Node mechanics duringtraining. Input positions are corruptedwith noise Ïƒ, and the training objective isthe node-level difference between targetpositions and the noisy inputs.
Figure 2: Per layer node latent diversity, measuredby MAD on a 16 layer MPNN trained on OGBG-MOLPCBA. Noisy Nodes maintains a higher levelof diversity throughout the network than competingmethods.
Figure 3: Validation curves, OC20 IS2RE ID. A) Without any node targets our model has poorperformance and realises no benefit from depth. B) After adding a position node loss, performanceimproves as depth increases. C) As we add Noisy Nodes and parameters the model achieves SOTA,even with 3 layers, and stops overfitting. D) Adding Noisy Nodes allows a model with even fullyshared weights to achieve SOTA.
Figure 4: Adding Noisy Nodes with randomflipping of input categories improves the per-formance of MPNNs, and the effect is accen-tuated with depth.
Figure 5: Validation curve comparing withand without noisy nodes. Using Noisy Nodesleads to a consistent improvement.
Figure 6: GNS Unsorted MAD per LayerAveraged Over 3 Random Seeds. Evidenceof oversmoothing is clear. Model trained onQM9.
Figure 7: GNS Sorted MAD per Layer Av-eraged Over 3 Random Seeds. The trendis clearer when the MAD values have beensorted. Model trained on QM9.
Figure 8: Comparison of the effect of techniques to address oversmoothing on MPNNs. Whilst Someeffect can be seen from DropEdge and DropNode, Noisy Nodes is significantly better at preservingper node diversity.
Figure 9: Training curves to accompany Figure 3. This demonstrates that even as the validationperformance is getting worse, training loss is going down, indicating overfitting.
