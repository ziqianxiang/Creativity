Figure 1: Go-to-target-pose (GTTP) task with the OP3& Ant. The agent has to reach the target pose (blue).
Figure 2: Performance of algorithmic variants in Sec. 5.2, when trained from scratch. Actor: Executing theMPC actor (MPC+) or the learned policy (rest). Proposal (mean): Executing the learned proposal’s mean action.
Figure 3: Performance of algorithmic variants in Sec 5.3 &5.4 with model and/or proposal transfer across tasks.
Figure 4: Go-to-target-pose (GTTP) task with the OP3& Ant. The agent has to reach the target pose (blue).
Figure 5: Network architectures of the forward dynamics and forward kinematics model. The forward dynamicsmodel (top) takes as input the observations (st) and the actions (at) at time t, and predicts the observations attime t+1 (st+1 ). We use a separate MLP per predicted observation group that takes in the entire observation (s)as input and predicts a single observation group (sk) as output (e.g. there is a separate MLP that regresses jointangles, one that regresses joint velocities etc.). This is done by predicting a delta change in the observation that isscaled by a task-specific timestep parameter (dt = 0.02 for Ant and dt = 0.03 for OP3 tasks) and added to thecurrent observation St to generate the final prediction (^k+ι). We use separate MLPs for each observation groupto decouple the different loss terms and handle scale differences between the different observation groups (e.g.
Figure 6: Network architectures of the policy, critic and task-agnostic proposal used for transfer. (a) The policynetwork takes all observations as outputs and predicts the mean/std. dev. of the Gaussian action distribution.
Figure 7: Performance of planning with SMC on the Ant GTTP task using a temperature of 0.01. Performancewith both a standard Gaussian proposal and a pre-trained task-agnostic proposal improves when more samplesare used. Interestingly, they perform similarly across the board. This is likely because the pre-trained proposal(which can walk but is not aware of the target pose) tends to propose actions that would wander away from thegoal whereas the Gaussian proposal gives jittery actions that will not lead to sustained movement of the ant’scentre of mass. Since the initial target pose is quite close to the ant and the ant is stable even jittery movementscan yield a relatively large return. This is reflected in the performance of the two proposals without planning.
Figure 8: Performance of planning with SMC on the Ant walking tasks using a temperature of 0.1. Both astandard Gaussian proposal and a pre-trained task-agnostic proposal improve when more samples are used. Asexpected, the pre-trained proposal outperforms the Gaussian proposal across the board. Unlike the GTTP taskthe pre-trained task-agnostic proposal is able to walk approximately forward or backward but can drift. Thus,planning with this proposal does much better than using the Gaussian proposal (though the difference is reducedfor larger computational budgets as one would expect). ‘Best agent performance’ denotes the performance of thebest RL agent trained on this task and should be interpreted as very good performance. Results are averagedover 100 episodes. The error bars show ± 1 standard error.
Figure 9: Performance of planning with SMC on the OP3 GTTP task using a temperature of 0.01. Withthe pre-trained task-agnostic proposal performance improves when more samples are used. As with the AntGTTP task this proposal can avoid falling and walk in random directions which provides significant rewardcompared to the Gaussian proposal which can easily cause the robot to fall, a state from which it cannotescape. Consequently, with a standard Gaussian proposal even 25600 samples do not lead to any meaningfultask progress; on such high-dimensional tasks random sampling would rarely lead to any interesting behaviorappearing. With the standard Gaussian proposal the planner reaches a target pose only in 1-2% of episodes bychance. The performance of the best RL agent trained on this task is around 730 (not shown) which is far betterthan anything achieved by planning (with or without the proposal). Results are averaged over 100 episodes. Theerror bars show ± 1 standard error.
Figure 10: Performance of MPC+MPO+BC when learning from scratch on the OP3 GTTP task with a sweepover different number of samples for the SMC planner (N) and probability of planning (pplan from MPC loop ofAlg. 1). Left: Plot comparing reward vs number of environment interactions, Right: Plot comparing the numberof target poses reached vs number of environment interactions. For a fixed planning horizon H = 10, increasingthe number of samples does not make a significant difference in performance (compared to the default N = 250,solid blue line). Increasing the probability of planning to pplan = 1.0 (always planning) leads to slightly worseperformance compared to interleaved planner and policy executions (pplan = 0.5). Results averaged over twoseeds.
Figure 11: Performance of MPC+MPO+BC when learning from scratch on the OP3 GTTP task with a sweepover different planning horizons for the SMC planner (H) and probability of planning (pplan from MPC loopin Alg. 1). Left: Plot comparing reward vs number of environment interactions, Right: Plot comparing thenumber of target poses reached vs number of environment interactions. For a fixed number of samples N = 250,changing the planning horizon also does not have a significant impact on performance or data efficiency comparedto the default setting of H = 10 (solid blue line); there is a slight improvement with H = 20 but this leadsto significantly slower experiments w.r.t wall clock time so we choose H = 10 in our experiments. At lowerhorizons H = 5, always planning (pplan = 1.0) does as well as interleaved planning and policy execution(pplan = 0.5) but there’s not a significant difference at higher horizons H ≥ 10 (see left plot). Results averagedover two seeds.
Figure 12: Performance of MPC+MPO and MPC+MPO+BC when learning from scratch on different Ant/OP3tasks, when running a sweep over bootstrapping with the value function for planning (Alg. 1). Top row: Perfor-mance on the GTTP tasks with and without value bootstrapping (solid and dashed lines respectively). Learning avalue function can be hard for the multi-task/multi-goal style GTTP task, and consequently bootstrapping withthe value function does not improve performance, and in fact, hurts performance on the harder OP3 GTTP task(compare dashed and solid lines). Bottom row: Performance on the simpler Ant walking tasks, where valuebootstrapping does improve performance and data efficiency. Results are averaged over four seeds.
Figure 13: Performance of the various algorithmic variants in Sec 5.2 when trained from scratch on the ForwardWalking task. Left column: Ant, Right column: OP3 results. MPC+MPO improves upon MPO for both the Antand OP3 on the actor (solid lines) but the amortized policy has lower performance (red, dotted lines) than theactor. MPO+BC and MPC+MPO+BC improve on the performance of their non-BC counterparts, and the amortizedpolicy of the MPC+MPO+BC matches or surpasses the corresponding actor performance (especially on the OP3task). See Sec. 5.2 and Fig. 2 for a further description of the results when learning from scratch.
Figure 14: Performance of the amortized proposal (mean) for all the algorithmic variants in Sec 5.3 &5.4 withmodel and/or proposal transfer across tasks. Top / bottom row: Ant / OP3 results; Solid / dashed lines: Modeltransfer (except MPO+BC) / proposal transfer. The amortized policies converge to similar (or slightly higher)performance compared to the actor performance results shown in Fig. 3. As explained in Sec. 5.3, model transferleads to fairly small performance improvements across tasks. Proposal transfer (Sec. 5.4) helps for transferfrom the GTTP task to the GTTP task itself (left column) but can hurt performance when there is a mismatch instate/goal distributions between the source and target tasks, especially for the higher-dimensional OP3 (Forwardwalking to GTTP - center column & GTTP to Backward walking - right column).
Figure 15: Actor performance for different model and proposal transfer variants from Sec. 5.3 and Sec. 5.4when transferring models and/or proposals from the GTTP task to the Forward walking task. For the simpler Ant(left column), transferring the model does not lead to performance improvements but transferring the proposaldoes speed up learning significantly. For the OP3 (right column), transferring the proposals or models leadsto faster learning at the beginning but results in convergence to a sub-optimal policy with lower performancecompared to the baseline MPO.
Figure 16: Performance of the amortized policy (mean) when transferring models and/or proposals from theGTTP task to the Forward walking task. Amortized policy matches (or slightly outperforms) the performance ofthe planner both during learning and asymptotically (see Fig. 15 to compare against actor performance).
Figure 17: Actor performance of MPC+MPO (red) and MPC+MPO+BC (green) when trained from scratch usinga PETS style stochastic model ensemble (dotted lines) vs the deterministic model (solid lines) used in our mainresults. There is little difference to using a stochastic ensemble vs a deterministic model on most tasks; onOP3 GTTP using a stochastic model does slightly better but it does not change the results qualitatively (thisimprovement is reduced when looking at the performance of the amortized proposal; see Fig. 18). Resultswere obtained using an ensemble of three Gaussian dynamics models for all tasks except the Ant GTTP taskwhere a single Gaussian dynamics model was used. In practice, there is no difference to using a single Gaussiandynamics model vs an ensemble (see Fig. 19 for a comparison of different ensemble sizes on the OP3 GTTPtask). Results are averaged over 4 seeds for the GTTP tasks and 2 seeds for the Walking tasks.
Figure 18: Amortized proposal (mean) performance for MPC+MPO (red) and MPC+MPO+BC (green), whentrained from scratch using a PETS style probabilistic model ensemble (dotted lines) vs the deterministic model(solid lines) used in our main results. Using a stochastic model ensemble instead of a deterministic model doesnot change the results significantly. Results are averaged over 4 seeds for the GTTP tasks and 2 seeds for theWalking tasks.
Figure 19: Performance of MPC+MPO (solid lines) and MPC+MPO+BC (dotted lines) when learning from scratchon the OP3 GTTP task with a PETS style probabilistic model ensemble, where we run a sweep over the size ofthe ensemble. Using 1, 3 or 5 models in the ensemble does not lead to a significant difference in performance forboth the actor (left) and the amortized proposal (right). Results are averaged over four seeds.
Figure 20: Performance of the algorithmic variants in Sec. 5.2 when learning from scratch on six controlsuite tasks across two embodiments. On the simpler walker embodiment all variants perform similarly. Onthe challenging humanoid embodiment the MPC variants slightly outperform MPO especially on the hardesthumanoid-run task. All results averaged over three seeds, and are the best results from a parameter sweep(see F.2 for parameters). We also swept over bootstrapping from the learned critic while planning for thesetasks and found no significant difference with and without bootstrapping; the presented results are without valuebootstrapping.
