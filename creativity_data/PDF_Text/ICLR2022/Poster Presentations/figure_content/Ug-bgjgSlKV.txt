Figure 1: We automatically find a universal latent direction in a GAN that can separate the foreground fromthe background without any supervision. We can then generate unlimited samples with masks to train a seg-mentation network that achieves state-of-the-art unsupervised segmentation performance.
Figure 2: (Typo fixed) Our unsupervised segmentation pipeline. First (left), a direction is identifiedin the latent space of a deep generative model (G) that separates the foreground and backgroundof generated images by changing their relative brightness. Second (right), a synthetic dataset isgenerated using this direction (or two of these directions) and is used to train a separate segmentationnetwork (S). This network can then be applied to unseen real-world data without further training.
Figure 4: A plot of Frechet Inception Distance (FID)versus average segmentation accuracy across all fiveevaluation datasets (CUB, Flowers, DUT-OMRON,DUTS, ECSSD) for nine different GAN architectures.
Figure 5: Examples of perturbed images generated by our method for five different generators(GANs). Note that the final generator, StyleGAN (Karras et al., 2019), is only trained on close-upportraits of animals, and thus cannot be used for general-perpose image segmentation. Nonetheless,our method successfully identifies the foreground and background of the generated animal portraits.
Figure 6: Examples of the final segmentation network across evaluation datasets. From left to right:original image, ground truth, prediction.
Figure 7: Examples of the final segmentation network across evaluation datasets. From left to right:original image, ground truth, prediction.
Figure 8: Examples of the final segmentation network across evaluation datasets. From left to right:original image, ground truth, prediction.
Figure 9: A comparison of perturbed images and their corresponding masks for many differentgenerators.
Figure 10: Examples of semantic segmentations after K-Means clustering on the PASCAL-VOCdataset. We see that our masks are of similar visual quality to those from MaskContrast (Van Gans-beke et al., 2021), despite the fact that our method is entirely unsupervised. Note that these imagesare randomly selected, not cherry-picked.
Figure 11: Here we plot the average average mIoU score of a segmentation model as we vary thenumber of synthetic training images and masks generated using our method. We see that modelperformance consistently improves with the number of generated training images, following a log-linear scale.
Figure 12: Examples of failure cases of our method on the test set of CUB. Differently from theground truth, our method frequently segments foreground objects that are not birds, such as branchesand bird feeders. This reflects the fact that our model captures general foreground structure in imagesand has not been trained on CUB.
Figure 13: Examples of failure cases on Flowers in which the ground-truth mask is not empty.
Figure 14: Examples of failure cases for GAN-generated images and masks. As there is no ground-truth associated with these generated images, these failure cases were found by manually filteringapproximately 500 generations.
