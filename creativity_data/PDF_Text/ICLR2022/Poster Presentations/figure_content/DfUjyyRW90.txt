Figure 1: Overview of InfoPower. I(Ot ; Zt) is the contrastive learning objective for learning an encoderto map from image O to latent Z. I(At-1; Zt|Zt-1) is the empowerment objective that prioritizes encodingcontrollable representations in Z. -I (it+1; Zt+1 |Zt, At) helps learn a latent forward dynamics model so thatfuture Zt+k can be predicted from current Zt. I(Rt ; Zt) helps learn a reward prediction model, such that theagent can learn a plan At, ..At+k, .. through latent rollouts. Together, this combination of terms produces alatent state space model for MBRL that captures all necessary information at convergence, while prioritizingthe most functionally relevant factors via the empowerment term.
Figure 2: PGM showing decomposition of state S into con-trollable parts S+ (directly influenced by actions A), partsnot influenced by actions that still influence the reward, S - ,and distractors DS- . St-+1 may be influenced by St- (arrow+not shown to reduce clutter) but not by St+ .
Figure 3: Some of the natural video backgrounddistractors in our experiments. The videos changeevery 50 time-steps. Some backgrounds (for e.g.
Figure 4: Evaluation of InfoPower and baselines in a suite of DeepMind Control tasks with natural videodistractors in the background. The x-axis denotes the number of environment interactions and the y-axis showsthe episodic returns. The S.D is over 4 random seeds. Higher is better.
Figure 5: t-SNE plot of latent states Z 〜qφ(z∣o) With visualizations of three nearest neighbors for two ran-domly sampled points (in red frame). We see that the state of the agent is similar is each set for InfoPower,whereas for Dreamer, and the most competitive baseline C-Dreamer, the nearest neighbor frames have signifi-cantly different agent configurations.
Figure 6: Evaluation of InfoPower and ablated variants in a suite of DeepMind Control tasks with naturalvideo distractors in the background. The x-axis denotes the number of environment interactions and the y-axis shows the episodic returns. InfoPower-NWJ and InfoPower-NCE are full versions of our methoddiffering only in the MI lower bound. The versions with - Policy do not include the empowerment objective inpolicy learning, but only use it from representation learning. The S.D is over 4 random seeds. Higher is better.
Figure 7: PGM of the MDP with distractor states. The state observed by the agent O consists of three partsS+, S- and DS-. S+ is the controllable part of the state i.e. it is affected by the actions of the agent, and inturn affects the reward R; S- is not controllable by the agent but affects the reward R and future S+; DS- isnot controllable by the agent and doesn’t affect the reward R and future S+.
Figure 8: Illustration of the natural video background distractors used in our experiments. The videos changeafter every 50 time-steps. Some video backgrounds (for example the top left and the bottom right) have morecomplex distractors in the form of agent-behind-agent i.e. an agent of similar morphology moves in the back-ground of the agent being controlled.
Figure 9: Illustration of the different levels of distractors. L1, L2, and L3 respectively have distractor windowsof size 32x32, 40x40, and 64x64.
Figure 10: Ilustration of the distractor set-ting for results in Table 3.
Figure 11: Evaluation of InfoPower and baselines with empowerment in policy learning. The setting cor-responds to DeepMind Control tasks with RGB natural video distractors in the background (same as that inFigure 5 of the main paper). The x-axis denotes the number of environment interactions and the y-axis showsthe episodic returns.
