Figure 1: Image search with free-form text modifiers (shown in yellow) enriches the visual querywith text in natural language. The task differs from visual search (using only the reference image,shown in red), and cross-modal search (using only the textual cue, shown in green). Yet, these canbe seen as two complementary aspects of the studied task. Our method leverages both modalities tolook for target images that i) explicitly match (EM) the characteristics mentioned in the text, and ii)bear some resemblance with the reference image using text-guided implicit similarity (IS).
Figure 2: Illustration of the proposed architecture. Our model takes a triplet (reference image Ir,target image It, text modifier Tm) as input and feed each element to its respective encoder. The textfeature is then used in multiple ways (including two attention mechanisms) to compute the implicitsimilarity (IS) and the explicit matching (EM) scores. These two complementary retrieval scores aresummed to produce the final score which is used to rank potential target images.
Figure 3: Visualisation of image parts contributing the most to the EM and IS scores, for queriesfrom the Fashion IQ and Shoes datasets where the correct target is ranked first. An EM componentis presented on the target image. The bottom two rows show two relevant components for IS, appliedto both the source and the target images (see Appendix § D for details).
Figure 4: Qualitative examples of image-text queries of CIRR and its Top-6 retrieved results. Agreen frame denotes the ground-truth target (see Appendix § D for more examples).
Figure 5: Qualitative results for queries from the “dress” category of the FashionIQ dataset (Wuet al., 2021). We show the six top ranked images for each query. The expected targets are indicatedwith a green border. It appears that the model is able to reason on several concepts (color, pattern,ruffles, sleeve length, form of the neck...), and to use the reference image to infer unspecified prop-erties in the text modifier (style, length, tightness...).
Figure 6: Qualitative results for queries from the “shirt” category of the FashionIQ dataset (Wuet al., 2021). We show the six top ranked images for each query. The expected targets are indicatedwith a green border. It appears that the model is able to reason on several concepts (different colorfor the clothing background and the logo, graphic, style, sleeve length...), and to use the referenceimage to infer unspecified properties in the text modifier (kind, style, form of the collar...).
Figure 7: Qualitative results for queries from the “toptee” category of the FashionIQ dataset (Wuet al., 2021). We show the six top ranked images for each query. The expected targets are indicatedwith a green border. It appears that the model is able to reason on several concepts (color, pattern,presence of buttons, sleeve length, form of the neck...), and to use the reference image to inferunspecified properties in the text modifier (kind, style, length, form of the collar...).
Figure 8: Qualitative results for queries from the Shoes dataset (Guo et al., 2018; Berg et al., 2010).
Figure 9: Qualitative results for queries from the CIRR dataset (Liu et al., 2021). We show the six topranked images for each query. The expected targets are indicated with a green border. It appears thatthe model is able to reason on several concepts (plurality, background/foreground, natural elements,viewpoint...), and to use the reference image to infer unspecified properties in the text modifier(e.g. the animal breed).
Figure 10: Qualitative results showing some limitations, for queries from FashionIQ (Wu et al.,2021) (the first 3 rows) and Shoes (Guo et al., 2018; Berg et al., 2010) (last row). For any of theseexamples, the ground truth is not included by the model in the top ranking of potential targets. Thisis either due to a lack of information (first row) or the non-understanding of negation (last 3 rows).
Figure 11: Visualisation of image parts contributing the most to the EM and IS scores, for queriesfrom the FashionIQ dataset (Wu et al., 2021) where the correct target is ranked first. For each of thesix blocks, we provide the heatmaps of an EM component on the target image, and of two relevantIS components, applied to both the reference and the target images. The EM heatmaps highlightsome of the connections made by the model between the text modifier and the target image. TheIS heatmaps display some detected similarities between the two images. Both EM and IS help toevaluate the relevance of the target image with regard to the query.
Figure 12: Visualisation of image parts contributing the most to the EM and IS scores, for queriesfrom the Shoes dataset (Guo et al., 2018; Berg et al., 2010) where the correct target is ranked first.
