Figure 1: In conventional weight-clustering algorithms, the boundary weights i and j are assigned to clustersC2 and C1 based on the distance metric respectively, which is neither necessarily suitable for the task nordifferentiable against the loss function. DKM instead applies soft assignment using attention mechanism duringforward-propagation and enables differentiable backward-propagation, which allows weights to consider othernon-nearest clusters (especially helpful for the boundary weights) and shuttle among multiple clusters in orderto directly optimize their assignments based on the task loss function.
Figure 2: Weight-sharing using attention matrix A is iteratively performed in a DKM layer until the centroids(C) converge. Once converged, a compressed weight, W is used for forward-propagation. Since DKM is adifferentiable layer, backward-propagation will run through the iterative loop and the gradients for the weightswill be computed against the task loss function.
Figure 3:	Multi-dimensional DKM can increase the weight entropy (ew) with a fixed compression ratio target(cr) by increasing a dimension (d), which may improve the model accuracy (Park et al., 2017).
Figure 4:	Accuracy vs. compression trade-offs: DKM-powered compression (in white box markers) deliv-ers the Pareto superiority to the other schemes (i.e., the top-right corner is the best trade-off) for ResNet18,ResNet50, and MobileNet-v1/v2 on ImageNet1k.
Figure 5:	MobileNet-v2 convergence with DKM 1/1: DKM delivers 50.8% top-1 accuracy with 1 bit compres-sion by gradually clustering the weights into two centroids using the task objective only.
Figure 6: ResNet18/50 compression using 2 bits with varying Ï„ values.
Figure 7:	The error between train-time and inference-time weights as a Frobenius norm for every batch forthe 200 epochs are plotted, while compressing ResNet18 with DKM cv:6/8 and fc:6/10 on ImageNet1k. Afterenough number of epochs, SuCh error is reduced to the 1e-4 level.
Figure 8:	The number of iterations in ResNet18 training with DKM cv:6/8 and fc:6/10 on ImageNet1k.
Figure 9: We plotted the train vs. inference weight error from the last fc layer of ResNet18 with DKM cv:6/8and fc:6/10 on ImageNetIk with varying e from 1e-2 to 1e-7 for 200 epochs. Although the initial error islarger with bigger e, the final errors all converged to a similar level after a sufficient number of epochs.
Figure 10: When DKM is compared with another soft assignment method, GBS based on Gumbel-softmaxand a hard assignment method, Hard with ResNet18 on ImageNet1k, DKM outperforms both. Soft assignmenttechniques outperform hard by a wide margin, and DKM is superior to GBS.
