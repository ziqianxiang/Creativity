Figure 1: (Left) As part of our method, robot-aware control (RAC), We propose to factorize visualdynamics into a robot and world model. (Right) The world model is then readily reused with a newrobot, permitting plug-and-play transfer.
Figure 2: The visual dynamics architecture is composed of an analytical robot model Pr, and thelearned Pw world model. During robot transfer, the target robot,s analytical model is used.
Figure 3: The behavior of the pixel and RA cost between the current images of a feasible trajectoryand the goal image. (Left) The first row shows the trajectory. The next two rows show the heatmaps(pixel-wise norm) between the image and goal for each cost. The heatmaps show high cost values inthe robot region, while the RA cost heatmaps correctly have zero cost in the robot region. (Right) Therelative pixel cost fails to decrease as the trajectory progresses, while the RA cost correctly decreases.
Figure 5: Model outputs of the zero-shot experiments. First row: the RA model is able to predictcontact with the green box, while the baseline (VF+State) predicts none. Next row: the RA-modelmodels the downward motion of the octopus more accurately. Refer to the website for videos.
Figure 6: Example results from the control experiments (full videos on website). We show the start,goal, and end states of RA/RA and baseline (VF+State/PiXel). The goal image is overlaid on top ofthe end state for visual reference.
Figure 7: Qualitative outputs of the zero-shot prediction experiments on models that Were pretrainedon multiple robots. From left to right, we show the ground truth start, ground truth future, andpredictions for the robot-aware model and VFS model. The baseline blurs the bear in the top row,and predicts minimal movement for the octopus in the bottom row. The proxy robot is visualized ingreen. Refer to the videos on the website for more visual comparisons.
Figure 8: The objects used in the control experiments. The few-shot WidowX250 transfer experimentused all objects, while the zero-shot Franka experiment used the bear and watermelon.
Figure 9: We evaluate the performance of the pixel cost (V-C) and robot-aware cost (RA-C) byrunning 100 push tasks where the robot must achieve the object-only goal image. Then we visualizefor each task, the final pose distance between the object and the object pose in the goal image. Thedashed horizontal line indicates the success threshold of 1cm. The robot-aware cost succeeds in mosttasks by outputting a sensible cost over the world region, while pixel cost is distorted by the robot.
Figure 10: Example human goal image.
