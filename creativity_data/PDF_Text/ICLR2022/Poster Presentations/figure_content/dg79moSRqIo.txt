Figure 1: We present Discovery of Incremental Skills (DISk). AS illustrated in (a), DISk learns skills incremen-tally by having each subsequent skills be diverse from the previous skills while being consistent With itself. DISklets us learn skills in environment where dynamics change during training, an example of which is shown in (b):an Ant environment where a different leg breaks every few episodes. In this environment, DISk,s learned skills’trajectories are shown in (c) under each broken leg — each color shows a different skill.
Figure 2: Left: Block environments that evolve during training; red highlights portion of blocks vanishing atonce. Middle: Trajectories of skills discovered by different algorithms; each unique color is a different skill.
Figure 3: Evaluation of skills learned on the broken Antenvironment with different legs of the Ant disabled; legdisabled during evaluation in legend.
Figure 4: Left: Environments We evaluate on. Middle: Visualization of trajectories generated by the skillslearned by DISk and baselines, with each unique color denoting a specific skill. For environments that primarilymove in the x-axis (HalfCheetah and Hopper), we plot the final state reached by the skill across five runs. Forenvironments that primarily move in the x-y plane (Ant and Swimmer), we plot five trajectories for each skill.
Figure 5: Downstream Hierarchical Learning. We plot the average normalized distance (lower is better), givenby T PT=0 (d(st, g)/d(s0, g)) from our Ant agent to the target over 500K training steps. From left to right, weplot the performance of agents that were trained in environment with no obstacles, and our three evolving blockenvironments. Shading shows variance over three runs, and we add a goal-based Soft Actor Critic model trainednon-hierarchically as a baseline. Across all settings, DISk outperforms prior work.
Figure 6: Skills learned by DISk, evaluated with each of the legs broken. The legs are numbered such that thefinal leg is numbered #4Figure 7: Skills learned by Off-DADS at the end of 10M steps, evaluated with each of the legs broken. The legsare numbered such that the final leg is numbered #4,Hierarchical Experiments For the hierarchical environments, we use the Ant-v3 environment ina goal-conditioned manner. The goals are sampled from [-15, 15]2 uniformly, and the hierarchicalagent can take 100 steps to reach as close to the goal as possible. At each step of the hierarchical20Published as a conference paper at ICLR 2022Figure 8: Skills learned by Off-DADS at the end of 9.3M, or 0.3M steps after breaking the final leg, evaluatedwith each of the legs broken. Compared to this agent, the agent in Fig. 7 performs worse on all broken leg butthe last one, which shows that Off-DADS suffers from an instance of catastrophic forgetting.
Figure 7: Skills learned by Off-DADS at the end of 10M steps, evaluated with each of the legs broken. The legsare numbered such that the final leg is numbered #4,Hierarchical Experiments For the hierarchical environments, we use the Ant-v3 environment ina goal-conditioned manner. The goals are sampled from [-15, 15]2 uniformly, and the hierarchicalagent can take 100 steps to reach as close to the goal as possible. At each step of the hierarchical20Published as a conference paper at ICLR 2022Figure 8: Skills learned by Off-DADS at the end of 9.3M, or 0.3M steps after breaking the final leg, evaluatedwith each of the legs broken. Compared to this agent, the agent in Fig. 7 performs worse on all broken leg butthe last one, which shows that Off-DADS suffers from an instance of catastrophic forgetting.
Figure 8: Skills learned by Off-DADS at the end of 9.3M, or 0.3M steps after breaking the final leg, evaluatedwith each of the legs broken. Compared to this agent, the agent in Fig. 7 performs worse on all broken leg butthe last one, which shows that Off-DADS suffers from an instance of catastrophic forgetting.
Figure 9: Mean normalized variance over skills of different agents on four different environments (lower isbetter). On average, the initial position s0 has high normalized variance since the agent has little control over it,and the subsequent steps has lower normalized variance as the skills lead to consistent states.
Figure 10: Ablation analysis of DISk on parallel skill discov-ery and replay R skill relabelling.
Figure 11: Training DIAYN with one policy per skill improves its performance. On the (left) we show thecomparison of mean Hausdorff distance of DIAYN with independent policies alongside our method and otherbaselines, and on the (right) we show trajectories from running skills from the Disjoint-DIAYN agent.
Figure 12: Training DIAYN for 4M steps with curriculum scheduling similar to VALOR(Achiam et al., 2018)results in better performance than vanilla DIAYN but suboptimal performance compared to Fig. 11. On the (left)we show the trajectories from learned skills on Disjoint-DIAYN with an added curriculum training on a staticAnt environment. On the (right), we show the same on the dynamic Ant-block environment, where the agentlearned two skills that wasn’t available at the environment initialization; however all other skills are degenerate.
Figure 13: All 10 skills learned by DISk in a static Ant environment in the order learned. Even though it seemslike skill 1 and 2 did not learn anything, skill 1 learns to stand perfectly still in the environment while skill 2learns to flip over and terminate the episode.
