Figure 1: One-shot domain adaptation: (left) a single reference image from domain B is used torefine a GAN GA to learn GB ; (center) every image in domain A has an analog in domain B thatshares a latent code and many salient attributes; (right) because salient attributes are preserved in thenew domain, many latent-edits are meaningful in the new domain.
Figure 2: An illustration showing how II2S embeds IB in the original StyleGAN domain A, shownfor two different values of λ. Reference images from other domains are shown in the top row. Thevalue recommended by Zhu et al. (2020b) is shown in the second row, and the value used in thiswork is shown in the third row. Although there is some subjectivity involved, we believe that thelarge value λ = 1e-2 is needed for II2S to find images that plausibly could belong to the domainA, which in this case is FFHQ faces.
Figure 3: The vectors in the CLIP image embedding space, EI , which control domain adaptation.
Figure 4: A process diagram for domain transfer. White rectangles indicate calculations, computedvalues are shown on the connecting lines. The four loss-calculations are indicated by blue rectangles,and the learnable weights of StyleGAN2 (all weights except the mapping network and the ToRGBlayers) are indicated in green.
Figure 5: Comparison of our framework with state-of-the-art frameworks for StyleGAN domainadaptation. We compare with StyleGAN-NADA (Gal et al., 2021) and the few-shot method of Ojhaet al. (2021). Each row corresponds to a different reference image IB, and each column is a differentreal image Ireal from domain A. Notice that our method is able to match the styles of the referenceimages, while StyleGAN-NADA fails to maintain the content of the images in domain A (for ex-ample the identity of a person is lost). On the other hand, the few-shot method suffers from severemode collapse.
Figure 6: Ablation study of the losses and style mixing used in our framework. From left to right: thereference image IA and several images from domain A, the baseline approach (StyleGAN-NADA),adding II2S instead of using the mean of domain A, adding Lrefdip, LCliP_Within, and then using stylemixing. The top row shows reconstructions of the image IA using GB .
Figure 7: Image editing capabilities of the new domain B using StyleFlow (Abdal et al., 2021b).
Figure 8: Some failure cases of our method. In these examples, we observe that the identity of theface is compromised a bit more than in typical examples of our method.
Figure 9: Style transfer results obtained by our method after style interpolation in domain B . Thetop row represents the real images embedded in the latent space of GA (domain A) whose latentcodes are then used by GB (domain B). The first column represents the reference images IB whichare input to our domain adaptation framework.
Figure 10: The structure of rows and columns is the same as in Fig. 9. Note: our method also workswell when the reference images are real face images.
Figure 11: Our method extends to deal with multiple reference images. The figure compares the re-sults using 3 reference images and using single reference image. It can be observed that our methodcan better catch the general style and achieve more stable results when given multiple referenceimages.
Figure 12: Style interpolation results achieved by our framework. Unlike the competing methods,our method has an explicit control over the styles in the domain B . Each sub figure shows a referenceimage and images embedded in domain A. Notice that we can control the amount of variation instyle depending on a parameter α that can be specified by a user.
Figure 13: Our domain transfer results on cars. The structure of rows and columns is the same as inFig. 9.
Figure 14: Our domain transfer results on cats and dogs. The structure of rows and columns is thesame as in Fig. 9.
Figure 15: Additional comparisons with other baseline methods including the concurrent methodTargetCLIP (Chefer et al., 2021) as well as two lower-resolution methods from Gatys et al. (2016)and AdaIN (Huang & Belongie, 2017). One-shot reference images from domain B are shown in theleft column. Each image is the result of transferring the image in the top row into the new domain.
Figure 16: Comparison domain-transfer and editing using II2S vs e4e. The new GAN is alwaystrained using II2S, but once training is complete, e4e can be used to transfer images into the newdomain. II2S takes 2.5 minutes to embed the image, while e4e needs about 0.22 seconds. StyleFlowediting takes 0.47 seconds, and StyleGAN image generation takes about 0.34 seconds.
