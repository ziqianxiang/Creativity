Figure 1: A toy example illustrating FedChain. Wehave two client objectives: F1(x) and F2(x). F(x)is their average. The top plot displays the objec-tives and the bottom plot displays the gradients. Inregions where client gradients agree in direction,i.e. (-∞, -1] ∪ [1, ∞) it may be better to use analgorithm with local steps (like FedAvg), and inthe region where the gradient disagree in direction,i.e. (-1, 1) it may be better to use an algorithmwithout local steps (like SGD).
Figure 2: Plot titles denote data homogeneity (§ 6). "X→Y'' denotes a FedChain instantiation with Xas Alocal and Y as Aglobal, circle markers denote stepsize decay events and plusses denote switchingfrom Alocal to Aglobal. Across all heterogeneity levels, the multistage algorithms perform the best.
Figure 3: We investigate the effect of high K (K = 100). "1-X→Y” denotes a chained algorithmwith X run for one round and Y run for the rest of the rounds. Chained algorithms, even with oneround allocated to Alocal, perform the best. Furthermore, accelerated algorithms outperform theirnon-accelerated counterparts.
Figure 4: The same as Fig. 2, except we include stepsize decay for baseline algorithms. Chainedalgorithms still perform the best.
