Figure 1: Three illustrative examples of our fast concept learning task. Our model learns from threenaturally occurring data streams: (a) looking at images, reading sentences that describe the objectsin the scene, and (b) interpreting supplemental sentences that relate the novel concept with otherconcepts. (c) The acquired novel concepts (e.g., red and white-eyed vireo transfer to downstreamtasks, such as visual reasoning. We present a meta-learning framework to solve this task.
Figure 2: Overview of FALCON-G. FALCON-G starts from extracting object-centric representationsand parse input sentences into semantic programs. It executes the program to locate the objects beingreferred to. The example objects, together with the reconstructed concept graphs are fed into twoGNNs sequentially to predict the concept embedding for the “white-eyed vireo”. The derived conceptembedding can be used in downstream tasks such as visual reasoning.
Figure 3: Different models for predicting concept embedding ec . They differ in how they process Dc :supplemental sentences, Gconcept: concept graphs, and oi(c) : objects being referred to.
Figure 4: Qualitative results of FALCON-G on the CUB and CLEVR dataset. The bottom left cornerof each images shows the model prediction and the confidence score. * marks a wrong prediction.
Figure 5: An overview of the overall training paradigm.
Figure 6: A visualization of fast concept learning from biased data with supplemental sentences onCLEVR. Notice that in both tasks, the visual examples are purple cylinders. The association betweenthe new concept and the visual appearance (shape or color) is ambiguous, without supplementalsentences.
Figure 7: Two implementations of graph Gconcept .
Figure 8: Fast concept learning performance under different percentage of related concepts in thesupplemental sentence.
Figure 9: A visualization of two failure cases of the concept learning module.
