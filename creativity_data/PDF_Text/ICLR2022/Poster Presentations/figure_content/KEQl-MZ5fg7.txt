Figure 1:	NCP optimizes and propagates the network code in an architecture coding space to achievethe target constraints with back-propagation on the neural predictors. (a) NCP searches for an op-timal structure on classification, then adapts it to segmentation through the segmentation predictor.
Figure 2:	Overview of our Network Coding Propagation (NCP) framework. Each architecture inour search space follows a multi-resolution paradigm, where each network contains four stages,and each stage is composed of modularized blocks (a parallel module and a fusion module). Theexample on the left shows the 3rd stage consists of b3 = 2 modularized blocks with three branches,where each branch contains a number of n3 residual units with a channel number of c3, i ∈ {1, 2,3}.
Figure 3: Visualization of the network propagation process of our two strategies (left: continuous;right: winner-takes-all) for segmentation. We group 27 dimensions into eight subfigures.
Figure 4: Spearman’s rank correlation of9 subtasks in NAS-Bench-MR.
Figure 5: (a) Spearman’s rank correlation of the different number of epochs (checkpoints of 10, 50,90, 100 epochs during training on ImageNet-50-1000). ‘10-C’ denotes using the convergent learningrate for 10 epochs, i.e., the proxy setting used in RegNet (Radosavovic et al., 2020). (b) Architecturerankings under the evaluation metrics of semantic segmentation (mIoU, mAcc, aAcc). (c) Architec-ture rankings under the evaluation metrics of 3D object detection (car/pedestrian 3D/bird’s-eye viewdetection AP).
Figure 6: Visualization of the architecture prop-agation process on the NAS-Bench-201 bench-mark (Dong & Yang, 2020) (ImageNet-16). Frepresents the propagated model in each step.
Figure 7: Comparisons of the efficiency (i.e., FLOPs) and the performance (e.g., Acc, mIoU,AP) on four computer vision tasks, i.e., image classification (ImageNet), semantic segmentation(CityScapes), 3D detection (KITTI), and video recognition (HMDB51) between the proposed ap-proach and existing methods. Each method is represented by a circle, whose size represents thenumber of parameters. F represents the optimal model with both high performance and low FLOPs.
Figure 8: Visualization of the searched models by NCP for four different tasks (λ = 0.5). The27-dimensional array in each row represents a network structure.
Figure 9:	Visualization of our network propagation process of the optimal model in classificationtransferring to other three tasks (λ = 0.5).
Figure 10:	Visualization of our network propagation process of the optimal model in segmentationtransferring to other three tasks (λ = 0.5).
Figure 11:	Visualization of our network propagation process of the optimal model in video actionrecognition transferring to other three tasks (λ = 0.5).
Figure 12:	Visualization of our network propagation process of the optimal model in 3D objectdetection transferring to other three tasks (λ = 0.5).
Figure 13:	Visualization of the searched segmen-tation models in Tab. 8 by our NCP for intra-taskgeneralizability (λ = 0.5). The 27-dimensionalarray in each row represents a network structure.
Figure 14:	Visualization of the searched videorecognition models in Tab. 9 by our NCP forintra-task generalizability (λ = 0.5). The 27-dimensional array in each row represents a net-work structure.
Figure 15:	Visualization of our network propaga-tion process of “NCP-Net-A” (λ = 0.7) for clas-sification.
Figure 16:	Visualization of our network propa-gation process of “NCP-Net-B” (λ = 0.7) forclassification.
Figure 17: Visualization of our network propa- Figure 18: Visualization of our network propa-gation process of “NCP-Net-C” (λ = 0.7) for gation process of “NCP-Net-ABC” (λ = 0.7) forclassification.	classification.
Figure 19:	Visualization of our network propaga-tion process of “NCP-Net-512 × 1024” in Tab. 8(λ = 0.5).
Figure 20:	Visualization of our network propaga-tion process of “NCP-Net-128 × 512” (λ = 0.5)in Tab. 8 for segmentation.
Figure 21:	Visualization of our network propa-gation process of “NCP-Net-Both” (λ = 0.5) inTab. 8 for segmentation.
Figure 22:	Visualization of our network propa-gation process of “NCP-Net-Scratch” in Tab. 9(λ = 0.5).
Figure 23:	Visualization of our network propaga-tion process of “NCP-Net-Pretrained” (λ = 0.5)in Tab. 9 for video recognition.
Figure 24:	Visualization of our network propa-gation process of “NCP-Net-Both” (λ = 0.5) inTab. 9 for video recognition.
