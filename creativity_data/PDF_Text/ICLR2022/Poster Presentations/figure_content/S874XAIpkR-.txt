Figure 1:	RvS learning conditioned on goals (RvS-G) and on rewards (RvS-R) compared withprior approaches and baselines. Each bar is the average over many tasks in each suite. Using justsupervised learning with a feedforward MLP, RvS matches the performance of methods employingTD learning and Transformer sequence models. https://github.com/scottemmons/rvs1Published as a conference paper at ICLR 2022statesactionsoutcomesSai'2	s3立日a2∖ Xa	,^s1r, ω" aI)s,a.
Figure 2:	(a) As input, RvS takes a precollected replay buffer of experience. An outcome ω can be anarbitrary function of the trajectory, such as future states or rewards. (b) RvS uses hindsight relabelingof the replay buffer to construct a training dataset. The observed actions act as demonstrations forthe observed outcomes. (c) Our implementation of RvS uses an MLP with two fully connected (fc)layers to predict actions. At test time, we can condition on arbitrary outcomes.
Figure 3:	Capacity and regularization: We vary capacity (via network width) and regulariza-tion (via dropout) on one environment from each task suite. Larger networks perform better onhopper-medium-expert and kitchen-complete, suggesting the importance of high-capacity pol-icy networks. However, dropout also usually boosts performance in kitchen-complete, suggesting that acombination of high-capacity policies with effective regularization is important for achieving good results.
Figure 4: (Left) In the GCSL suite, a categorical distribution in discretized action space generallyoutperforms a unimodal Gaussian distribution in continuous action space. This fits the broad patternwe observe that higher-capacity RvS models perform better. (Right) In D4RL Kitchen, validationloss is only loosely correlated with final performance.
Figure 6: A failure to interpolate.
Figure 7: Validation mean-squared-error and evaluation return versus training gradient steps inFrank Kitchen. Each line includes 5 random seeds across each of the three datasets: complete,partial, and mixed. Surprisingly, we find that although each hyperparameter setting has nearlyidentical validation loss, the evaluation return varies by as much as 1.4x. This indicates that dropoutand network capacity are having important effects on performance beyond validation loss.
Figure 8: (a) Different strategies for choosing goals in D4RL Gym. (b) Commanding the policy tocomplete different subtasks in kitchen-complete.
