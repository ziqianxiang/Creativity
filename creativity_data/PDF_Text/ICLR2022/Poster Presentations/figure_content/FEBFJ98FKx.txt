Figure 1: Overview of TPU-GAN’s architecture, it comprises a generator, a temporal discriminatorand a spatial discriminator. (a) The Generator takes a coarse point cloud as input and predicts arefined point cloud. Inside the generator, the input is first project into a high-dimensional latentencoding, the encoding is then transformed into residuals by Upsampling Module and a binary maskby Masking Module. Through an element-wise multiplication , residuals of unnecessary upsamplingwill be masked to zeros. (b) The Temporal Discriminator takes three consecutive frames of pointclouds as input, extracts features via set abstraction layesr and flow embedding layers, and predictsthe confidence if it is temporally coherent. (c) The Spatial Discriminator takes a point cloud asinput, extracts features via set abstraction layers, and outputs the confidence if it is spatially coherent.
Figure 2: Qualitative comparison of TPU-GAN against other models on different datasets. To fullyevaluate the temporal coherence of upsampled point cloud sequences, we refer readers to the renderedvideos, which are available at: [Videos].
Figure 3: Comparison of distribution ofpoints’ top 0.1% furthest distance4.4 Ablation studyArchitectural choice We investigate the influence of our architectural choice to the model per-formance on the MSR-Action 3D dataset. The result is shown in the Table 4. The result indicatesthat: first, increasing the frame number of point clouds exposed to the temporal discriminator canimprove the the sequence level accuracy. However too long input sequence ("Input five frames") willdeteriorate the frame-level upsampled quality. As it will make points cluster around input points andexhibit halo artifacts. (See Figure 10 in the Appendix for visualization)Moreover, we find that spatial discriminator is important for the non-uniform dense point cloud upsampling. The upsampled points will shrinktowards the center of the point cloud without spatial discriminator. Wecalculates the top 0.1% furthest distances between every pair of pointsin the upsampled patches from fluid dataset, and we normalize all thedistances using the furthest distance in ground truth. As shown in theFigure 3, without spatial discriminator, the upsampled point clouds’furthest distance distribution will shift to the left.
Figure 4: Quantitative com-parison of density distribu-tion of different upsamplingmethods on Fluid dataset.
Figure 5: Schematic of flow module. The flow module takes T frames of point cloud features{F1(0), F2(0), . . . , FT(0)} as input. Inside each layer, two consecutive frames of point clouds’ features(Ft(l), Ft(+l)1) are mixed via a shared flow embedding layer, then the output features Ft(l+1) are storedon the former point cloud Xt . For T input frames, the flow module will have a depth of T - 1 (T - 1different flow embedding layers).
Figure 6: Top: Schematic of feature extractor in the generator. The feature extractor comprises threelayers of Inception DenseGCN and concatenates features from different scales to derive the finaloutput. Bottom: Schematic of masking/upsampling module in the generator. Following the designof NodeShuffle module in PU-GCN, inside masking/upsampling module, latent features are firstaggregated from local neighborhood via GCN and then decoded into output by a MLP.
Figure 7: Visualization of the effect of masking module on fluid dataset (zoom in for better view).
Figure 8: Visualization of the masking module’s prediction (zoom in for better view). Given a sparseinput, masking module will predict a value for each point to indicate if it needs to be upsampled.
Figure 9: Visualization of the upsampling results based on different masking module trainingmethods (zoom in for better view). When not using masking loss, we remove the non-differentiablequantization process (equation 3) and directly train the masking module based on gradient from otherlosses (adversarial loss, Chamfer distance). Without masking loss’ supervision, the upsampled resultscontain clustered points and halo structures.
Figure 11: Visualization of fluid point cloud generated from model during the training process. Theinput and reference point clouds are randomly selected and cropped from the testing dataset. Pointcolor indicates depth.
Figure 12: Visualization of MSR-Action3D point cloud generated from model during the trainingprocess. The input and reference point clouds are randomly selected from the testing dataset. Forbetter visualization, the input points are shown in grey scale. Point color indicates depth.
Figure 13: Visualization on Kitti-scene flow dataset. (Zoom in for better view)Input source	EPE 3D1	3D outliers1Full (16384 points)	0.1974	17.1%Part (2048 points)	0.2577	27.5%TPU-GAN	0.2243	19.8%PU-GCN	0.2362	23.0 %Tranquil Clouds	0.2392	22.5%Table 6: Scene flow estimation performance of pre-trained FlowNet3D on different input sources.
