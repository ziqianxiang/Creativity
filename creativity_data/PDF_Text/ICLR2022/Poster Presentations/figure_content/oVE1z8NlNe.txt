Figure 1: Overview of federated self-supervised learning (FedSSL) framework. It comprises an end-to-end training pipeline with four steps: 1) Each client k conducts local training on unlabeled dataDk with Siamese networks — an online network Wo and a target network W%; 2) After training,client k uploads Wo to the server; 3) The server aggregates them to obtain a new global networkWo; 4) The server updates Wo of client k with Wo.
Figure 2: Comparison of non-contrastive FedSSL methods with and without (w/o) predictor (pred)or stop-gradient (stop-grad) on non-IID CIFAR-10 dataset. Without predictor, both FedSimSiam andFedBYOL drops performance on kNN testing accuracy (left plot). Without stop-gradient, FedBYOLretains competitive results on kNN testing accuracy (middle plot) and linear evaluation (right table).
Figure 3: Comparison of FedBYOL without exponential moving average (EMA) and stop-gradient(sg) on the non-IID CIFAR-10 dataset. FedBYOL w/o EMA and sg can hardly learn, but updatingboth Wk and Wkt with Wg (update-both) enables it to achieve comparable results.
Figure 4: Illustration of our proposed Feder-ated Divergence-aware Exponential Moving Av-erage update (FedEMA). Compared with Fed-BYOL that simply updates the online networkof client k with the global network Wg, we pro-pose to update them via EMA of the global net-work following Eqn 1 and 2, where the decay rateμ is dynamically measured the divergences be-tween the online encoder Wk and the global en-coder Wg (Eqn 3). The online network, ={Wk. Wf), is the concatenation of the online en-coder Wk and the predictor Wζ.
Figure 5: Ablation studies of FedEMA: applying EMA on eitherpredictor or encoder leads to better performance on CIFAR-10.
Figure 7: Ablation study on scaler λ, decay rate μ, and non-IID levels of the CIFAR-10 dataset:(a) analyzes the impact of scaler λ on performance; (b) compares using constant μ on encoder,predictor, or both; (c) studies the impact of different non-IID levels.
Figure 6: Changes of diver-gence throughout training.
Figure 8:	Illustration of differences among four Self-supervised Learning (SSL) methods.
Figure 9:	Comparison of FedBYOL and FedEMA on various total training rounds R on the non-IIDsetting of the CIFAR-10 dataset. FedEMA consistently outperforms FedBYOL.
Figure 10:	Ablation study on T for autoscaler and combinations of constant μ: (a) analyzes theimpact of T on performances; 2) presents top-1 accuracy of using different combinations of constantμo on the online encoder and constant μp on the predictor.
