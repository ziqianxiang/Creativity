Figure 1: IWSLT’14 De-En 1(a) Binned cosine similar-ity between all unique pairs of the first 200 well-trainedPEs. 1(b) Distribution of token repetitions across testsentences after initial translation from fully masked sen-tence. For each repetition count, we find the numberof test sentences where at least one token consecutivelyrepeats that many times.
Figure 2: CMLMC loss example. Here, source German sentence X = [wir arbeiten an NLP]is translated to the target English sentence Y = [we work on NLP]. First, sampled mask Ymaskmasks out the [on] token. Masked sentence is passed through the CMLMC decoder to predict themasked out token in the Lmask loss. Then, fully masked sentence Y0 is passed through the CMLMCdecoder that translates all tokens simultaneously to Y0=[work work on on]. Sampled firsttoken [work] from Y0 is substituted for [we], and the resulting sentence [work work <M>NLP] passes through the CMLMC decoder to correct the [work] → [we] in the Lcorr loss.
Figure 3: 3(a) IWSLT’14 De-En BLEUs, inference times, and token repetitions for different numberof correction iterations. Inference time measures the wall time from when model and test data areloaded until the last sentence has been translated. 3(b) Translation language probability visualizationfor a multi-target En-De/Es/Fr corpus. Each models is trained to translate all three languages.
