Figure 1: Mean absolute error (solid, bold lines) and standard error of the approximated Qt-functionover iterations as well as the corresponding performance obtained from a greedy policy w.r.t. thisQt-function in task Tt in the car-on-hill environment (computed from 20 seeds). The differentlyshaded areas highlight the iterations on which BC-FQI and C-FQI train on tasks T1, T2 and T3. Thefine, dashed lines indicate the results of the individual 20 runs from which the mean was computed.
Figure 2: Average error in the approximated Qt-functions (left) plus corresponding performanceobtained from a greedy policy w.r.t. this Qt-function (right) in the maze environment. Shadedcolored areas correspond to two times standard error estimated from 40 seeds. The different shadesof grey highlight the iterations in which BC-FQI and C-FQI train on different tasks Tt. The horizontalblack dashed line in the right plot highlights the maximum achievable reward in T10 .
Figure 3: (a) visualizes different maze environments T1, T4, T7 and T10 (left to right, top to bottom).
Figure 4: (a) mean performance and standard error in the linear system control task achieved withdifferent methods. The blue vertical dashed line indicates the iterations at which bc-dqn, rpl, ppr,sc-dqn and sbc-dqn switch from the LQ- to the target task. For c-dqn, this switch is indicatedby the green dashed line. Statistics have been computed from 100 seeds. (b) visualizes the Q-functions learned during the pre-training in the LQ task with lspi (for bc-dqn, rpl, ppr, sc-dqnand SBC-DQN) and DQN (for C-DQN). The arrows indicate the policy encoded by the Q-functions.
Figure 5: Average residual of the starting state s0 predicted by the learned Q-functions in the dif-ferent learning tasks of the maze environment. The average and standard error (barely visible) iscomputed from 40 different seeds.
