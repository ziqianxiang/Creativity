Figure 1: The catastrophic forgetting issues on non-i.i.d. MNIST (a) and EMNIST (b) data, and anillustration of FedReg (c). Here, loss(t-1) denotes the loss on data Dj , which is the local data ofclient j sampled in round t - 1, computed with parameters θ(t-1), and loss(t) denotes the averagedloss on the same data computed with the locally trained parameters in round t. See Appendix B.1for their detailed definitions. The values of loss(t) are significantly larger than those of loss(t-1),indicating that after the local training stage, the knowledge about the training data in the previousround at the other clients has been forgotten. To accelerate the convergence rate of FL by alleviatingsuch forgetting issue in the local training stage, FedReg generates pseudo data Dis and perturbeddata Dip, and regularizes the local parameter θ(t,i) by constraining the loss on Dis and Dip. Therefore,instead of using the plain gradient with learning rate ηθ, FedReg updates θ(t,i) with the gradientgγ(Di) computed with slowly-updated parameters θγ(t,i), which prevents the gradient from converg-ing to zero in few steps, and then it regularizes θ(t,i) with a combination of the gradients gβ (Dis)and gβ (Dip) computed from the pseudo and perturbed data.
Figure 2: Increase of the loss (top row) concerning previous training data and distribution of thecorrelation (bottom row) between the empirical Fisher information in the pseudo data and that in theprevious training data at the other clients.
