Figure 1: Left: The asymmetric squared loss used for expectile regression. τ =0.5 correspondsto the standard mean squared error loss, while τ =0.9 gives more weight to positives differences.
Figure 2: Evaluation of our algorithm on a toy umaze environment (a). When the static datasetis heavily corrupted by suboptimal actions, one-step policy evaluation results in a value functionthat degrades to zero far from the rewarding states too quickly (c). Our algorithm aims to learna near-optimal value function, combining the best properties of SARSA-style evaluation with theability to perform multi-step dynamic programming, leading to value functions that are much closerto optimality (shown in (b)) and producing a much better policy (d).
Figure 3: Left: Estimating a larger expectile τ iscrucial for antmaze tasks that require dynamicalprogramming (’stitching’). Right: Clipped dou-ble Q-Learning (CDQ) is crucial for learning val-ues for τ = 0.9.
