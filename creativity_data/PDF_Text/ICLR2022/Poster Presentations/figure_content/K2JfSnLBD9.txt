Figure 1: C-Planning is an algorithm for goal-conditioned RL that uses an automatic curriculum of waypointsto learn policies that can solve complex tasks. In this manipulation task, the goal requires moving the greenpuck to the green dot and the red puck to the red dot. Our method learns to solve this task, manipulatingmultiple objects in sequence, without requiring any reward functions, manual distance functions, or humandemonstrations.
Figure 3: Comparison of goal-conditioned RL methods: We compare C-Planning to prior goal-conditionedRL algorithms on various tasks: (top) benchmark manipulation tasks from Metaworld, (middle) 2D maze nav-igation, and (bottom) robot manipulation tasks that require long horizon planning. For each task, we recordthe Euclidean distance to the goal, taking the minimum distance within an episode. All but the easiest task(Reach), C-Planning outperforms all prior methods, including those that perform planning. Only C-Planningis able to solve the most challenging navigation and manipulation tasks.
Figure 4: Planning at Training Versus Testing: We ablate the number of intermediate waypoints used toreach the goal, and compare against a variant of our method that performs planning during both training andtesting (“C-Planning + SoRB”). This variant does not improve performance, indicating that the feedforwardpolicy learned by C-Planning has already “internalized” these planning capabilities.
Figure 2: Environments: Visualization of the 2D nav-igation maze environments (top row) and robotics ma-nipulation tasks (bottom row).
Figure 6: Gradient Analysis and Computation Cost: (Left) Mean and standard deviation of the gradientnorm for the actor and critic networks. C-Planning has a larger critic gradient norm than C-Learning. C-Planning shows a smaller variance comparing to C-Learning. (Right) Computation cost in latency of C-Planningand SoRB with various number of waypoints.
Figure 5: Waypoint sampling: (Left)Early in training, the agent sampleswaypoints closer to the initial state.
Figure 7:	C-Planning Curriculum: Illustration of planning over waypoints distribution p(sw). Goal-conditioned RL directly commands the agent to the final goal. C-planning first samples an intermediate way-point sw from p(sw ), directs the agent to that waypoint, and then commands the final goal after the agent hasreached the waypoint. Note that the p(sw) is proportional to the state density of the current policy, so the highprobability region will expand from starting state s0 to the goal state sg , as the agent explores the environment.
Figure 8:	Oracle experiment with the optimal generative model: Goal-conditioned RL learns the task muchfaster if we can sample the waypoints from the expert policy’s marginal state distribution (C-Planning Optimal).
Figure 9: (Left)An agent must navigate from thestart state to the goal state. The heatmap vi-sualizes the marginal state distribution of the op-timal policy.
Figure 10: Ablation with HER, SkewFit and SoRB: Comparison of C-Planning to more baselines like HER,SkewFit and SoRB. HER only shows good performance on easy task like Reach but fails on harder ones.
Figure 11: Ablation on the Planning Horizon and Experiments on Ant-Maze: (left) Albation study of theplanning horion. A Larger planning horizon will sometimes help the performance of tasks, but the improvementis not consistent. (right) Experiments on small scale Ant-Maze. RIS manage to reach the goal quickly in Ant-Small environment but fails in the Ant-Mid.
Figure 12: Ablations on design components of RIS: Westudy design factors that might affect the performance ofRIS. Different threshold on termination function (1.0 versus0.5) is affecting the performance of RIS significantly.
Figure 13: Visualization of the trained policy for the Push-Two environment. Our method suc-cessfully trains the agent to manipulate the two object in the sequential manner without any offlinedata, expert demonstration and reward shaping. Prior baselines all fail to demonstrate such behavior.
Figure 14: Visualization of the trained policy for the Obstacle-Drawer-Open environment.
