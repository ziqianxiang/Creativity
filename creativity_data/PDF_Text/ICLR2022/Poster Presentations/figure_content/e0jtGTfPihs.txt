Figure 1: FCN Signed Supermask: The upper figure shows a randomly chosen (in this case the digit“8”) flattened MNIST sample. Each line represents a single pixel where non-yellow lines representthe pixels containing a part of the handwritten digit. The lower figure visualizes the first mask of arandomly picked and trained ELUS FCN model. We can clearly see the alignment of the “digit-pixels”in the MNIST sample and the non-zero mask elements.
Figure 2: Conv Signed Supermask: Mask distribution and equality.
Figure 3: FCN (Signed) Supermask: The figure seeks to draw a rough comparison between a FCNtrained with a (ELUS) binary Supermask and a (ELUS) signed Supermask. Note that a comparisonto Supermasks of Zhou et al. (2019) is not directly feasible as there are other important factors thatdiffer, namely the ELUS initialization, the ELU activation function and the same configuration asused for the ELUS signed Supermask. However, comparing a binary with a signed Supermask canuncover important differences.
Figure 3:	The above matrix depicts the sum of 50 trained binary Supermasks, i.e. during the trainingprocess the weights could either be pruned or “let through”. The same is shown on the bottom for thesigned Supermask. A darker shade represents a larger value. In order for any negative and positivemasking not to cancel each other out, the absolute masks were summed. Put simply, if a single mask(out of the 50 runs) prunes a weights every time except once, the respective value in the matriceswould be 1 and so on. Both versions have an average pruning rate of 97.2% in the first layer. Theshown matrices still exhibit a pruning rate of 54.11% in case of the signed Supermask and 43.63% forthe binary Supermask, i.e. 54% (43%) of the weights were always pruned. This number is impressiveby itself, if we consider that normal neural networks could never reach a similar number. The overallaverage pruning rates are 3.77% and 4.04% for the signed and binary Supermasks, respectively. Whilethe signed Supermask reached an average performance of 97.48%, the binary Supermask reachedan average test accuracy of 97.03%. In other words, the signed Supermask left the same numberof weights in the first layer, while the distribution differed more in the other layers but reached ahigher performance with roughly 0.3pp less weights. The focus of this examination is to compare theproperties of the respective first masks.
Figure 4:	Signed Supermask: Average test accuracy over 50 runs of all baseline (left) and signedSupermask (right) models and architectures. We also report the 5% and 95% quantiles. We canobserve the larger variance in test accuracy for the signed Supermask models. The ELUS signedSupermask models visually coincides with the baseline or lies above it, whereas He and Xaviercannot reach the same performance. Furthermore, for the deeper models, ELUS needs a few epochsto “warm up”.
Figure 5:	FCN Signed Supermask: Average ratio of remaining weights. We also report the 5%and 95% quantile, although the interval is barely visible indicating a robust metric. Weight countdrops early in the learning phase and plateaus thereafter.
Figure 6: CNN Signed Supermask: Average ratio of remaining weights over 50 runs of the CNNsigned Supermask models initialized with He, Xavier and ELUS and architectures during training.
Figure 7:	Signed Supermask: Average mask distribution over 50 runs of all models and weightinitializations. It holds for all configurations that the first layer undergoes only little pruning incontrast to the large hidden layers, where almost all weights are pruned. The behavior in the last layerdiffers for ELUS: there, the pruning rate is significantly higher in the last layer as compared to Heand Xavier. The sparsity correlates with the size of the layer: the larger a layer, the more it is pruned.
Figure 8:	Signed Supermask: Equality of masks and absolute (i.e. binary) masks for eacharchitecture and Weight initialization over the 50 conducted runs. Only looking at the equality ofthe signed Supermasks, We can see that the picture is divided into FCN and CNN behavior. For theFCN the first layer is the largest and attains the highest pruning rate (refer to Figure 7), Whereas thehidden and output layer are not pruned as much. All initializations produce masks that are similaracross all runs in about 50% of all elements. For the CNNs, We have almost no similarity in the firstlayers Which can be partially attributed to the nature of CNNs and the concept of Weight-sharing.
Figure 9: Conv Signed Supermask: We show two randomly drawn trained signed Supermasks foreach architecture. The chosen layers are the respective first fully connected layer in the respectivearchitecture (which are of different shapes for each architecture). A green dot represents “1”, a reddot “-1”. For each architecture, we notice unique patterns which are present in both draws. Forexample, the Conv8 masks show a grid-like pattern which has an area with almost completely prunedweights in the upper part of the mask. The structure is similar for the two Conv6 masks. For Conv4,the masks show more pruning activity on the left part of the mask and a more row-wise structure.
Figure 10: Signed Supermask: Mean test accuracy and ratio of remaining weights in additionto the respective 5% and 95% quantiles for ResNet20 and its wider siblings.
Figure 11: ResNet Signed Supermask: Average mask distribution (over the 50 conducted runs) ofthe investigated ResNet20 models. The wider the layer, the higher the pruning rate. Layers 1, 8 and15 are 1x1 shortcut layers. Interestingly, those layers are relatively sparse compared to other hiddenlayers. As with the CNNs, the first and output layer are most sparse.
Figure 12: FCN Signed Supermask: Average test accuracy and average remaining weights withtheir respective 5%- and 95%-quantile of a FCN with weights drawn from a uniform distribution andas signed constants (“SC”). We can see that the uniform distribution yields in a much higher varianceof both, test accuracy and ratio of remaining weights. Furthermore, drawing weights from a uniformdistribution does not improve performance nor sparsity.
Figure 13:	CNN Signed Supermask: Average test accuracy for He Uniform and He SC duringtraining. We also report the respective 5%- and 95%-quantile. We find that the variance of HeUniform is at least as large as the variance of He SC, but especially in models Conv2 and Conv8 HeUniform has a much larger variance. He SC outperforms He Uniform in all architectures.
Figure 14:	CNN Signed Supermask: Average ratio of remaining weights for He Uniform and HeSC during training in addition to the respective 5%- and 95%-quantile. We find that the variance ofHe Uniform is at least as large as the variance of He SC but especially in models Conv2 and Conv8He Uniform has a much larger variance. He SC outperforms He Uniform in all architectures.
Figure 15: FCN Signed Supermask: Average test accuracy and average remaining weights withtheir respective 5%- and 95%-quantile of a ELUS/ELUS and ELUS/Xavier FCN. We can see thatboth models behave very similar with regards to performance and variance. ELUS/ELUS drops itsweights a few epochs later but the general behavior is equal.
Figure 16:	CNN Signed Supermask: Average test accuracy for ELUS/ELUS and ELUS/Xavierduring training. We also report the respective 5%- and 95%-quantile. We find that for all architec-tures, both combinations behave and perform very similar. However, ELUS/ELUS visibly exceedsELUS/Xavier for Conv6.
Figure 17:	CNN Signed Supermask: Average ratio of remaining weights for ELUSzELUS andELUSzXavier during training in addition to the respective 5%- and 95%-quantile. For each archi-tecture, it is visible that the training behavior of both combinations is very similar. Moreover, wecan note the different shapes of ascend of the four architectures. ELUSzELUS drops its weights afew epochs later and achieves a marginally higher ratio at the end of training. Both initializationcombinations behave very robust.
Figure 18:	Conv SiNN 2: Test Accuracy (left) for baselines and SiNN 2 models as well as averageratio of remaining weights on the right side for SiNN 2 and ELUS models. 5% and 95% quantiles arereported in the form of error bars for both metrics.
Figure 19: CNN SiNN Signed Supermasks: Average SiNN 1 & 2 accuracy (left) and remainingweights (right) over 50 runs of all CNN architectures with error bars indicating the 5% and 95%quantile. SiNN 1 is trumped by SiNN 1 in each architecture in terms of test accuracy. Looking at theright side of the plot, the difference in performance can intuitively be explained by the fewer weightsthat are left active: for this level of sparsity, a certain amount of weights is clearly needed to yield inacceptable performance. Combining this with the information seen in Figure 21, we can hypothesizethat the loss in performance of SiNN 1 compared to SiNN 1 comes from pruning the first layereven more. This further supports the thesis that the first layer in CNNs is crucial for the network’sperformance, as already stated for the ELUS models. By comparing the masks of SiNN1 and SiNN 2in greater detail, future work could find e.g. specific weights that influence the performance morethan other weights and subsequently draw conclusions on network architectures in general.
Figure 20: FCN SiNN Signed Supermask: Average SiNN 1 & 2 accuracy (left) and relativeremaining weights (right) over 50 runs for the FCN model. As a comparison, ELUS is visualizedas well. Furthermore, we report the respective 5% and 95% quantiles. SiNN 1 does not reach the testaccuracy of neither SiNN 2 nor ELUS, but it also utilizes the fewest weights, around 1.4%. We arguehowever, that the trade-off of SiNN 2 is better, as the “cost” of performance is only a marginallylower pruning rate.
Figure 21: SiNN Signed Supermask: Average SiNN 1 & 2 mask distribution over 50 runs of allCNN architectures and ELUS for comparison. For the FCN, the overall picture does not change:SiNN 1 & 2 prune the first two layers even more, while the last layer only experiences little moresparsity. The same is partially true for the CNN architectures: while the last last layer remainson an equal pruning level compared to ELUS, the amount remaining weights in the first layer isconsiderably smaller. On top, all hidden layers are pruned very heavily. Apart from more aggressivepruning in the first layer for SiNN 1, a difference between SiNN 1 and SiNN 2 cannot be detectedvisually.
