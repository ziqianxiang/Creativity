Figure 1: Intuitive illustration of how two models which predict identical classification labels can havearbitrary gradients. To show this, given a binary classifier H and an arbitrary function g, we constructa classifier H0 that predicts the same labels as H, yet has gradients equal to g almost everywhere. Weformally state this result in Theorem 3.1.
Figure 2:	The left two plots show abstention rates as a function of the underlying probability of agreementamong models over S, i.e., the probability that any given model will return the mode prediction, withplots denoting varying numbers of constituent models. The right two graphs demonstrate the relationshipbetween consistency of the ensemble models as given by Corollary 4.3.
Figure 3:	Figure a: Percentage of test data with non-zero disagreement rate in normal (i.e., not selective)ensembles. Horizontal axis depicts ensemble size. Figure b: Average Spearman’s Ranking coefficient,ρ, (For FMNIST, SSIM) between feature attributions for saliency maps generated for each individual testpoint (y-axis) over number of ensemble models (x-axis). The lines indicated with (Sel) in the legend arethe same metrics for selective ensembles.
Figure 4:	Inconsistency of attributions on the same point across an individual (left) and ensembled (right)model (n= 15). The height of each bar on the horizontal axis represents the attribution score of a distinctfeature, and each color represents a different model. Features are ordered according to the attributionscores of one randomly-selected model.
Figure 5: Intuitive illustration of how two models which predict identical classification labels can havearbitrary gradients. To show this, given a binary classifier H and an arbitrary function g, we constructa classifier H0 that predicts the same labels as H, yet has gradients equal to g almost everywhere. Weformally state this result in Theorem 3.1.
Figure 6:	Mean and standard deviation of the percentage of test data with non-zero disagreement over 24normal (i.e., not selective) ensembles. The mean and standard deviation are taken over ten re-samplings of24 ensembles.While ensembling alone mitigates much of the prediction instability, it is unable to eliminateit as selective ensembles do.
Figure 7:	Accuracy of non-selective (regular) ensembles with n ∈ {5,10,15,20} constituents. Results areaveraged over 24 models, standard deviation is presented.
Figure 8: Inconsistency of attributions on the same point across an individual (left) and ensembled (right)model (n = 15), for all datasets, over differences in random seed chosen for initialization parameters beforetraining. The height of each bar on the horizontal axis represents the attribution score of a distinct feature,and each color represents a different model. Features are ordered according to the attribution scores of onerandomly-selected model. Figure a depicts the German Credit Dataset, Figure b depicts Adult, Figure cSeizure, Figure d Taiwanese, and Figure e Warfarin. We do not include feature attribution for imagedatasets as the individual pixels are less meaningful than the feature attributions in a tabular dataset.
Figure 9: Inconsistency of attributions on the same point across an individual (left) and ensembled (right)model (n= 15), for all datasets, over leave-one-out differences in the training set. The height of each baron the horizontal axis represents the attribution score of a distinct feature, and each color represents adifferent model. Features are ordered according to the attribution scores of one randomly-selected model.
Figure 10: We plot the average similarity across feature attributions for an individual point, averagedover 276 comparisons of feature attributions from two different models. This is aggregated across theentire validation split. The error bars represent the standard deviation over the 276 comparisons betweenmodels. Each row of plots constitutes the plots for a given dataset, noted on the far left, and each columnof plots is for a given metric, noted at the top. Note that for image datasets, (FMNIST and Colon), weplot SSIM instead of Spearman’s Ranking Coefficient (ρ). The x-axis is the number of models in theensemble, starting with one, and the y-axis indicates the value of the similarity metric averaged over all276 comparisons of individual points’ in the validation split’s attributions. The red and orange lines depictregular ensembles, and the green and blue represent selective ensembles.
Figure 11: Graphs of accuracy (y) versus abstention (x) of ensembles of different size, gathered bycalculating accuracy and abstention for 12 different values of α. Note that the y-axis begins at 0.6.
