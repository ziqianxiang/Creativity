Figure 1: PCA for existing weight block matrices in BERT-base. We got nearly similar results inFig. 5 for paired matrices along rows and columns, as shown in App. C.
Figure 2: The three methods for parameter compression. To compress the raw weights WI, IIdecomposes each matrix in WI into small matrices, i.e., two ‘narrow, matrices and a small squarematrix. III further shares the two 'narrow, matrices for all weights. IV introduces a matrix bank forthese small square matrices, making parameter scale nearly constant w.r.t. the number of layers.
Figure 3: Transformer architecture.
Figure 6: L2 norms of raw weights in BERTL = Ltraining + λ∣WIV -WI∣2This does not improve the performance, but worsen the performance. The reason may be as below.
Figure 7: L2 norms of raw weights in BERT and reconstructed BERT-IV.
Figure 8: Inference time. Inference time in Y-axis (in a Nvidia V100 16G GPU) increases whenbatchsizes (in X-axis) become bigger. BERT-III/BERT-IV with d < 384 are faster than raw BERT.
Figure 9: Distances between the factor vectors among 144 matrices in a trained BERT-IV-72-384. Theorder is listed as [W Q, W K, W V, W O, Win, Win, WIn, Win, Wfut, WOut, WOut, WOut]from the first layer to last layer.
Figure 10:	Distances between the factor vectors among 12in each layer of a trained BERTIV-72-384. The order is[W Q, W K, W V, W O, W1In, W2In, W3In, W4In, W1Out, W2Out, W3Out, W4Out].
