Figure 1: (a) The network used in the standard setting, with the number of neurons in each fullyconnected layer. The output layer activation function is SoftMax, while either Tanh or ReLU isused for the hidden layers. (b) An example of an information plane for a 6 layer network, with theobserved phases marked. Only the last three layers are distinguishable.
Figure 2: The quantized Tanh network, trained in the standard setting with the discrete MI computedexactly. The full information plane (left) and the upper right area (right) are shown. Plotted MI valuesare the means over the 50 repetitions.
Figure 3: The quantized ReLU network, trained in the standard setting with the discrete MI computedexactly. The full information plane (left) and the upper right area (right) are shown. Plotted MI valuesare the means over the 50 repetitions.
Figure 4: The information plane for the bottleneck structured network fitted on MNIST. PlottedMI values are the means over the 20 repetitions. The compression phase in the 2 neuron layer ishighlighted.
