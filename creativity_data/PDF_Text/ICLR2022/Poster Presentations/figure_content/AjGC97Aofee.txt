Figure 1: Visual comparisons (×4) about lightweight and large SR networks on Urban100 dataset.
Figure 2: (a) Illustration of channel-wise and filter-wise pruning for single Conv layer. In this work,we adopt filter-wise pruning to learn efficient image SR networks. (b) Illustration of filter pruningwithin a residual block. We depict deep features F as 3d cubes. We expend the Conv kernel W (4dtensor) as a 2d matrix here for easy illustration (each row represents a filter). Both green and bluecolor denote the pruned filters (or feature map channels): green denotes the pruned filters in freeConv layers; blue denotes the pruned filters in constrained Conv layers. The basic idea of our SRPis to apply the L2 regularization to the unimportant filters to make sure the pruned indices of F (i)are exactly the same as those of F (i+2) . (c) Illustration of filter pruning across different residualblocks. All the layers that are directly followed by the add operators are constrained Conv layers (inblue). Others are free Conv layers (in green).
Figure 3: Plots of the mean L1-norm of filters vs. iterations for “model.body.12.body.0" (free Convlayer) and “model.body.12.body.2” (constrained Conv layer) in EDSR baseline. As expected, theunimportant filters (“Pruned filters”) are driven down (owing to strong regularization); interestingly,the important ones arise spontaneously.
Figure 4: Visual comparison of different lightweight SR approaches on the Urban100 dataset (×4).
Figure 5: Visual comparison of different large SR networks on the Urban100 dataset (×4).
