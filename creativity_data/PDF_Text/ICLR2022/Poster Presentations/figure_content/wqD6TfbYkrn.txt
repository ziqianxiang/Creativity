Figure 1: Our Conditional Point Diffusion-Refinement (PDR) paradigm first moves a Gaussiannoise step by step towards a coarse completion of the partial observation through a diffusion model(DDPM). Then it refines the coarse point cloud by one step to obtain a high quality point cloud.
Figure 2: Network architecture of the Conditional Generation Network (CGNet) and ReFinementNetwork (RFNet). It consists of the Condition Feature Extraction subnet and the Denoise subnet.
Figure 3: (a) Insert information of the diffusion step embedding and the global feature to the sharedMLP. (b) The Feature Transfer module maps features from the incomplete point cloud to the noisypoint cloud. (c) Refine and upsample the coarse points at the same time.
Figure 4: Visual comparison of point cloud completion results on the MVP dataset (16384 points).
Figure 5: Our method can be extended to controllable point cloud generation.
Figure 6: Detailed network structure.
Figure 7: The first row shows some inconsistent pairs of the incomplete point cloud and completepoint cloud from the Completion3D dataset. The second row are the corrected pairs by minimizingthe one-side CD loss.
Figure 8: Our PDR paradigm demonstrates diversity in the completion results. For each object, thetwo images in the first row are coarse completion results from a trained DDPM generated in twotrials for the same incomplete point cloud. The two images in the second row are refined resultsfor the two coarse point clouds, respectively. We can see that some diversity is preserved after therefinement.
Figure 9: Visual comparison of our method and other baselines. Samples are from the MVP datasetat the resolution of 16384 points. We can see that point clouds generated by our method generallyhave better visual quality.
Figure 10: Visual comparison of our method and VRCNet. Samples are from the MVP dataset atthe resolution of 16384 points. We can see that VRCNet sometimes tend to predict more points tothe parts that are known in the incompelte point cloud, while put less points at the missing part. Thiscould effectively reduce CD loss, but leads to large EMD loss. Compared with VRCNet, our methodgenerally generates more uniform point clouds.
Figure 11: Visual comparison of our method and PoinTr. Samples are from the MVP dataset atthe resolution of 16384 points. We can see that PoinTr sometimes tend to predict more points atthe skeleton of objects, while points on surfaces seem sparse. Compared with PoinTr, our methodgenerally generates more uniform point clouds.
Figure 12: Visual comparison of coarse point clouds generated by the Conditional Generation Net-work in DDPM and point clouds after refinement. Samples are from the MVP dataset at the reso-lution of 2048 points. We can see that coarse point clouds generated by the Conditional GenerationNetwork basically uniformly cover the overall shape of objects, but tend to be noisy. After refine-ment, point clouds demonstrate both good overall density distribution and sharp local details.
