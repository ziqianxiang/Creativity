Figure 1: Comparison of SVGD and MMD-descent in training a two-hidden-layer BNN on synthetic 1D dataset.
Figure 2: (a) Distribution of sample means of S1 and S2 for Gaussian target and Gaussian RBF kernel. (b)Particle variance of MMD-descent initialized from N(0, 0.2Id). Darker color indicates larger particle size.
Figure 3: Empirical verificationof (A2) for SVGD in learningisotropic Gaussian (γ = 3).
Figure 4: Stationary variance of SVGD and MMD-descent; predictions (black) are given by Proposition 3 and5. (a) GaUssian kernel with median heuristic: SVGD underestimates the variance, but MMD-descent (blue) doesnot. (b) IMQ-SVGD underestimates the variance under both the median heuristic (red) and fixed σ = VZd (blue).
Figure 5: Points represent converged particlesof SVGD (middle) and damped SVGD (right).
Figure 6: Dimension-averaged marginal variance of particles converged under (a) SVGD, (b) MMD and (c)SVGD with N = nd particles. The target is a unit Gaussian, and we employ the Gaussian RBF kernel withmedian bandwidth. Dashed lines correspond to predicted values. (a) when n, d jointly scales and d > n, thevariance of SVGD scales linearly with n and 1/d as predicted by Proposition 3. (b) for small n, variance ofMMD-descent approaches 1 as n increases (independent to d), which agrees with Proposition 7. (c) if theparticle size of SVGD is up-scaled by a factor of d, then the underestimation of variance is of order O(1/n).
Figure 7:	(a)(b) Empirical demonstration of variance collapse beyond Gaussian target. (c)(d) For unit Gaussiantarget, SVGD with the log-inverse kernel also underestimates the marginal variance.
Figure 8:	Quantities of interest in Assumption (A2). We fix γ > 1 and vary n, d to verify the dependence on d.
Figure 9: BNN experiment (γ = 100) for the proposed modification of SVGD (Gaussian RBF kernel). Observethat the original SVGD update (b) significantly underestimates that target variance, whereas the proposedmodification (d) leads to more diverse predictions.
Figure 10: (a)(b) MMD (IMQ kernel) and dimension-averaged variance of SVGD particles in Bayesian logisticregression experiment. Observe that the modified update (blue) leads to smaller MMD (a), but may overestimatethe target variance in the small-particle regime (b). (c) Modified SVGD with λ = 0 leads to diverging particles.
Figure 11: Learning an isotropic Gaussian using the IMQ kernel. (a) Integration by parts with the IMQ kernelleads to a large discrepancy in the variance of S1 and S2. (b) MMD with IMQ kernel leads to divergence under∆1MMD (log derivative) with fixed target samples. (c) IMQ-SVGD with resampled S1 (blue) correctly estimatesthe target variance, but redrawing S2 (green) fails to provide more accurate samples.
Figure 12: Resampled SVGD inlearning the cubic-growth potentialdefined in Appendix A.1.
