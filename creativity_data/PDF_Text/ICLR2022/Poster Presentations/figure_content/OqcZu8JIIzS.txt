Figure 1: Model return-uncertainty trade-off of P3 (ours), MOPO (Yu et al., 2020), and MOReL (Kidambiet al., 2020) on an offline RL dataset “halfcheetah-random” from the D4RL benchmark (Fu et al., 2020). P3achieves policies with different levels of model return-uncertainty trade-off. MOPO and MOReL are run threetimes with regularization weights {0.1, 0.3, 0.5} and {1.0, 3.0, 5.0}, respectively. Detailed discussion in Sec. 1.
Figure 2: Illustration of P3 by solving a benchmarkproblem from (Lin et al., 2019).
Figure 3: Learning curves on low-quality datasets. Returns are averaged over 10 evaluations and 5 seeds.
Figure 4: Model-based offline RL’s performance in the deployed environment (heatmap) under different trade-offs between the model return (y-axis) and uncertainty (x-axis). Each red circle is a Pareto policy from the poolgenerated by P3. Zoom in for more details. More results are shown in Fig. 10 of Appendix.
Figure 5: Scatter plots of the ordinal rankings of the FQE value estimates vs. the true values.
Figure 6: Rank correlation of our FQE algorithm for D4RL Gym benchmark datasets. HC = HalfCheetah, Hop= Hopper, W = Walker, r = random, m = medium, mr = medium-replay, e = expert, me = medium-expert.
Figure 7: Learning curves for the D4RL Gym experiments. Curves are averaged over the 10 evaluations and5 seeds, and the shaded area represents the standard deviation across seeds. P3 performs better compared to twomodel-based offline RL methods while exhibiting similar performance as the state-of-the-art model-free method(TD3+BC). Note that we sweep the hyperparameters of MOPO and MOReL and choose the best combinationfor each task.
Figure 8: The histogram of returns from low-quality to high-quality datasets. Low/medium-quality datasetsin the D4RL Gym benchmark are “imbalanced” and degrade the training of environment models. P3 (ours)exhibits more advantages on these datasets.
Figure 9: Ablation study. Pareto front comparison between the scalarization method (red circles) and our P3(blue circles) on low/medium-quality datasets. We train 5 policies from scratch using scalarization with randomweights. P3 also adopts 5 reference vectors. Hence, we remark that P3 takes the roughly same run time asscalarization but achieves a dense set of policies and better Pareto front approximation.
Figure 10: Contour plots on how model-based offline RL’s performance (color bar) varies with different trade-offsbetween the model return (y-axis) and uncertainty (x-axis). Meanwhile, red circles denote the pool of policiesgenerated by P3.
