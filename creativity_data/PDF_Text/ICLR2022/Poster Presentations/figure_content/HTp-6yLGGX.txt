Figure 1: (a) The illustration of cold-refresh and hot-refresh model upgrades. (b) The problem ofmodel regression with negative flips in hot-refresh model upgrading procedure of image retrievalsystems, where queries retrieved correctly by the old model fail during the upgrade process.
Figure 2: The illustration of our Regression-alleviating Compatible Training (RACT) framework.
Figure 3: The trend of retrieval performance during the hot-refresh model upgrades (R50-R101) onGLDv2-test, in terms of mAP(↑) and NFR(1). The vanilla models suffer from the model regressionsignificantly, while ours properly mitigate the model regression to some extent. The results of NFRalso indicate the effectiveness of our introduced regression-alleviating compatible regularization.
Figure 4: The trend of retrieval performance during the hot-refresh model upgrades (R50-R101) onROxford, in terms of mAP(↑) and NFR(1). Evident model regression can be observed for vanillamodels, and our method alleviates this issue to a large extent.
Figure 5: The trend of retrieval performance during the hot-refresh model upgrades (R50-R101)on RParis, in terms of mAP(↑) and NFRQ). Our method consistently alleviates the issue of modelregression on three different training data allocations. The new-to-old compatible mAP at 0% ofour model is lower than the old-to-old mAP on the open-data setup, showing the same issue as thevanilla model. It is mainly due to the limited generalization ability of compatible features trainedwith a sole contrastive loss. Note that in this paper, we aim to study the regression-alleviatingregularization rather than advanced compatibility constraints.
Figure 6: Comparisons between random backfilling and our uncertainty-based backfilling on ROx-ford. Results of R50-R101 are reported in terms of mAP(↑). Three kinds of uncertainty are studied.
Figure 7: Sequential model upgrades on GLDv2-test (Open-data). Results of R50-R50 are reportedin terms of mAP(↑). “Hot-refresh” is the introduced new model upgrading mechanism where thegallery features are refreshed online, and “no-refresh” indicates the backfill-free deployment mech-anism introduced by previous compatible learning methods (Shen et al., 2020).
Figure 8: The trend of retrieval performance during the hot-refresh model upgrades of the samearchitecture, i.e., R50-R50. The results are reported on GLDv2-test in terms of mAP(↑) and NFR(1).
Figure 9: The trend of retrieval performance during the hot-refresh model upgrades (R50-R50) onROxford, in terms of mAP(↑) and NFR(1).
Figure 10: The trend of retrieval performance during the hot-refresh model upgrades (R50-R50)on RParis, in terms of mAP(↑) and NFR(J).OUr method consistently alleviates the issue of modelregression on three different training data allocations. The new-to-old compatible mAP at 0% of ourmodel is lower than the old-to-old mAP on expansion and open-data setups, showing the same issueas the vanilla model. It is mainly due to the limited generalization ability of compatible featurestrained with a sole contrastive loss.
Figure 11: Comparisons between random backfilling and our introduced uncertainty-based backfill-ing on ROxford, in terms of mAP(↑). Results of R50-R50 are reported.
Figure 12: Comparisons between random backfilling and our introduced uncertainty-based backfill-ing on GLDv2-test and RParis. Results of R50-R101 are reported in terms of mAP(↑).
Figure 13: Failure cases of our introduced uncertainty-based backfilling on RParis (Open-data).
Figure 14: The analysis of trade-off between new-to-old negative pairs and new-to-new negativepairs in our regression-alleviating compatible regularization. We report retrieval performance duringthe hot-refresh model upgrades (R50-R50) on ROxford in terms of mAP(↑).
Figure 15: Experiments of compatibility constraints in the form of a triplet loss. We illustrate thetrend of retrieval performance during the hot-refresh model upgrades (R50-R50) on GLDv2-test,ROxford and RParis datasets, in terms of mAP(↑) and NFRQ).
Figure 16: Comparison with two related methods, BCT (Shen et al., 2020) and Focal Distillation(FD) (Yan et al., 2020). The former one only regularizes the feature compatibility while ignoringmodel regression, and the latter one is specially designed for model regression of classification tasks.
Figure 17: Comparison with two related methods, BCT (Shen et al., 2020) and Focal Distillation(FD) (Yan et al., 2020). Results of R50-R50 are reported in terms of mAP(↑).
Figure 18: Comparison with continual learning method LwF (Li & Hoiem, 2017) on GLDv2-test,ROxford and RParis. Results of R50-R50 are reported in terms of mAP(↑).
