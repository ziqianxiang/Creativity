Figure 1: Plate diagrams of three main Bayesian modeling approaches. Shaded nodes are observedand unshaded ones are latent. The solid direct arrows denote conditional dependencies in the truemodel, while the bent dashed arrows denote amortization in variational inference.
Figure 2: The Evidential Turing Process: (left) The essential design choice is how p(π∣Z, w, x) isconstructed thanks to an external memory M that can be queried with respect to any input x withoutneeding to store context data at test time. The dashed arrow indicates that Z can condition on acontext DC by updates its parameters M with an explicit function r. (right) The ETP training routine.
Figure 3: Results averaged across 19 types of cor-ruption (e.g. motion blur, fog, pixelation) appliedon the test splits of FMNIST, CIFAR10, and SVHNat five severity levels.
Figure 4: 1D Classification Task. The upper plotshows the underlying distributions of each of the twoclasses, as well as the observed data. The lower showsthe regularizing evidence the generative model placeson each of the two classes depending on location inspace, that is, mean ± one standard deviation over tensamples from p(Z |M).
Figure 5: 2D Classification Task. The three upper plots visualize the regularizing evidence that themodel places on the location in space as the average over ten samples fromp(Z|M). The color scalesare different across the three plots and vary in overall intensity. The lower plot visualizes a horizontalcut through the middle of these plots, putting them on a common scale. The solid lines in eachcolor indicate the mean memory evidence, and the corresponding shaded areas indicate one standarddeviation. Around the separable blue class, the memory strongly emphasizes the correct class withlarge confidence and suppresses the other two classes depicted in orange and green. While alwayspreferring the correct class, the memory regularizes against overconfidence around the overlappingorange and green classes.
