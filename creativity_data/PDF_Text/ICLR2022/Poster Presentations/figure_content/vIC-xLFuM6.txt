Figure 2: Q-value approximation on the toy MDP. All baselines collected at 400 epochs unlessotherwise mentioned. (a) Fitted Q iteration using a 4-layer MLP with 400 hidden neurons. (b)Supervised by the ground-truth value target(c) a larger network that is 3×deeper (12-layers). (d) thesame 4-layer network, but optimized longer, for 2000 epochs. (e) 4-layer FFN, no target network.
Figure 1: A Toy MDP with a simple for-ward dynamics and complex value function,adapted from Dong et al. (2020).
Figure 3: The spectrum of the optimalvalue function noticeably whitens over(a) longer horizon and (b) larger γ .
Figure 4: NTK comparison between (a) MLP with ReLUactivation, (b) MLP with tanh activation, and (c) Fourier fea-ture networks (Ours). The MLP NTK in (a,b) both containlarge off-diagonal elements. The addressing by the gradientvectors is not specific to each datapoint.
Figure 5: FFN provide direct control over gen-eralization. (a) The kernel spectra. Peak inthe center is due to windowing effect. Higherband-limit leads to flatter spectra. (b) The cross-section of the kernel at different band-limit.
Figure 6: (a) Comparison of the learned Q function (for ac-tion II). (b) Approximation error averaged over 10 randomseeds. FFN with a target network still contains an offset,whereas removing the target network eliminates such bias.
Figure 7:	Visualizing the learned value function for action 0 in Mountain Car using (a) a four-layer ReLU network, (b) an eight-layer network, (c) a four-layer network with hyperbolic-tangentactivation, which is commonly used in deep reinforcement learning. (d) that of a four-layer FFN.
Figure 8:	Learning curve with FFN applied to SAC on the DeepMind control suite. Domains areordered by input dimension, showing an overall trend where domains with higher state dimensionbenefits more from Fourier features. We use a (Fourier) feature-input ratio of 40 : 1.
Figure 9:	FFN needs only a fraction of the com-pute to match the performance of the MLP base-line on both Walker-run and Quadruped-run.
Figure 10:	Increasing the gradient updates tosampling ratio causes Quadruped-run to crash.
Figure 11:	Weight and bias changes in FFN and MLP during training, using SAC. While FFN’s biasparameters undergo less change than MLP’s bias parameters, the results are mixed when it comes toweight parameters on the Quadruped environment.
Figure 12:	Value approximation error with FFN vs MLP using SAC. The divergence is especiallyprominent at the beginning of learning when the data is sparse. FFN reduces the variance in theseregimes by making more appropriate bias-variance trade-off than the MLP baseline.
Figure 13:	Learning curves showing that FFN improves learning stability to the extent that learningcan happen on some domains even without the target network. On the contrary, MLP consistentlyfails without a target value network. FFN has consistently less variance than the MLP.
Figure 14:	Control from pixels learning curve on the DeepMind control suite using Fourier-CNN(F-CNN) and DrQv2. We use a feature-input ratio of40 : 1. Performance with the F-CNN has muchlower variance and is consistently at the top of the confidence range of the vanilla CNN.
Figure 15: Q-value approximation on the toy MDP with different bandlimit parameter b. (a - c)showing b = {1, 3, 5}. (d) showing a FFN without target network.
Figure 16: Learning curve with FFN for different values of B on Walker-run and Quadruped-run.
Figure 18: Learning curve withdifferent Fourier dimension ratioon Quadruped-run. Lowering theratio decreases the performance.
Figure 17: Using FFN for critic is as good as using FFN forboth actor and critic. However, using learned FFN only foractor is similar to the MLP baseline. This indicates the gainmainly comes from better value approximation.
Figure 19: Learning curve with FFN applied to SAC on the DeepMind control suite. Domains areordered by input dimension, showing an overall trend where domains with higher state dimensionbenefits more from Fourier features. We use a (fourier) feature-input ratio of 40 : 1.
Figure 20: Learning curve with FFN applied to DDPG on the DeepMind control suite. Domains areordered by input dimension. The over trend agrees with results on soft actor-critic, where domainswith higher state dimension benefits more from random Fourier features. The same feature-inputratio of 1 : 40 is applied.
Figure 21:	Value approximation error with SAC on the DeepMind control suite. FFN reduces theerror w.r.t the MLP, especially during earlier stages of learning where the gradient updates is low.
Figure 22:	Weight change in FFN and MLP during training of RL agents with SAC on the DeepMindcontrol suite. The results are mixed and FFN’s weight parameters undergo less change than MLP’sweight parameters only in some environments.
Figure 23:	Bias change in FFN and MLP during training of RL agents with SAC on the DeepMindcontrol suite. Given FFN’s bias parameters have better initialization, they undergo less change thanMLP’s bias parameters.
Figure 24:	Since FFN require fewer gradients for value function estimation, its performance doesn’tdegrade as much when target value networks are removed. On the contrary, MLP completely failswhen target value networks are removed.
Figure 25:	Value approximation error with FFN vs MLP using SAC. The divergence is especiallyprominent at the beginning of learning when the data is sparse. FFN reduces the variance in theseregimes by making more appropriate bias-variance trade-off than the MLP baseline.
