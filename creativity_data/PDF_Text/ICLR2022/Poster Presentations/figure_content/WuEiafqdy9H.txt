Figure 1: (a) A high-level illustration of Model-augmented Prioritized Experience Replay (MaPER) withModel-augmented Critic Network (MaCN). MaCN obtains more accurate estimation of Q-values via MaPERacross the Q-value estimator and the model estimator. (b) Curves of TD-errors and model errors from sampledexperiences under SAC (Haarnoja et al., 2018a). MaPER leads to a much faster decay of TD-errors. (c) Curvesof Q-value estimations and returns evaluated by a policy of SAC. MaPER leads to a much faster convergence ofQ-values to returns. HumanoidPyBullet is a task belonging to Pybullet GymPorium (Ellenberger, 2018-2019).
Figure 2: Learning curves of the baseline off-policy model-free algorithms with various sampling methods.
Figure 3: The average cumulative rewards obtained on continuous control tasks using MBPO and MBPO +MaPER. Our method significantly improves the performance of MBPO. The solid lines and shaded regionsdenote the mean and standard deviations by one evaluation across five runs with random seeds.
Figure 4: Ablation study. (a): Different prioritizing methods under MaCN. We observe that MaPER outperformsother prioritizing methods under MaCN. (b): Different estimators in MaPER. One can observe that estimatingboth reward and transition is the most effective. (c): Learning curves of MaPER with and without sharingnetworks. Here, networks, which predict rewards and next states, used the same number of hidden units withthe same architecture as the original critic networks under SAC. (d): Network size effect under SAC. Largenetworks have hidden units (2500, 2500), respectively. Simply increasing the number of parameters and gradientsteps (GS) leads to marginal or worse performance. The solid lines and shaded regions represent the mean andstandard deviations by ten evaluation across ten runs with random seeds.
