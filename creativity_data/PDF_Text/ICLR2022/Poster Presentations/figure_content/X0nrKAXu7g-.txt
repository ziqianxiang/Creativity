Figure 1: Illustration for HyperDQN .
Figure 3: Comparison of algorithms in terms of thehuman-normalized score over 49 games in Atari.
Figure 4: Comparison of algorithms on Q*bert.
Figure 6: Comparison of HyperDQN and BootDQN interms of computation complexity on the deep sea.
Figure 5: Illustration for deep sea.
Figure 8: Comparison of HyperDQN withdifferent prior scales σp .
Figure 7: Comparison of HyperDQN withdifferent noise scales σω .
Figure 10: Comparison of BootDQN with 10 ensembles and 20 ensembles.
Figure 9: Comparison of HyperDQN with a linear hypermodel and a MLP hypermodel.
Figure 11: Learning curves of algorithms on Atari. Solid lines correspond to the median performanceover 3 random seeds while shaded ares correspond to 90% confidence interval. Same with otherfigures for Atari.
Figure 12:	Relative improvement of HyperDQN compared with OPIQ (Rashid et al., 2020) on Atari.
Figure 13:	Relative improvement of HyperDQN compared with OB2I (Bai et al., 2021) on Atari. Therelative performance is calculated as maχhUmo⅞-b≡⅛man (Wang et 乩 2016). Environments aregrouped according to the taxonomy in (Bellemare et al., 2016, Table 1). “Unknown” indicates suchenvironments are not considered in (Bellemare et al., 2016).
Figure 14:	Relative improvement of HyperDQN compared with BootDQN (Osband et al., 2018)on Atari. The relative performance is calculated asproposed—baselinemax(human,baseline)—human(Wang et al., 2016).
Figure 15:	Relative improvement of HyperDQN compared with NoisyNet (Fortunato et al., 2018)on Atari. The relative performance is calculated asproposed—baselinemax(human,baseline)—human(Wang et al., 2016).
Figure 16: Comparison of HyperDQN with and without -greedy on Atari.
Figure 17: Learning curves of algorithms on SuperMarioBros. Solid lines correspond to the medianperformance over 3 random seeds while shaded ares correspond to 90% confidence interval. Samewith other figures for SuperMarioBros.
Figure 18: Comparison of HyperDQN with and without -greedy on SuperMarioBros.
Figure 19: Comparison of BootDQN with and without -greedy on SuperMarioBros.
Figure 20: Comparison of RLSVI and HyperDQN on Atari.
Figure 21: Comparison of HyperDQN with two configurations: 1) the hypermodel is applied only atthe last layer of the base model (ours); 2) the hypermodel is applied for all layers of the base model.
Figure 22: Illustration for the initialization issue of OFU-based algorithms. After the initialization,action “left” dominates at state s0. After the experience replay buffer, action “left” dominates again atstate s0. This issue is caused by the pessimistic initialization by neural network (Rashid et al., 2020).
Figure 23: Illustration for HAC. In addition to a hypermodel in the critic network, there is also ahypermodel for the actor network.
Figure 24: Comparison of HAC and SAC on the hard exploration Cart Pole (Tunyasuvunakool et al.,2020).
Figure 25: Comparison of HyperDQN with and without an informative prior on SuperMarioBros-1-3.
