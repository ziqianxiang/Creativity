Figure 1: Assumed graphicalmodel connecting the factorsof variations y = (y1 , ..., yn)to observations x = g(y). Theselection variable s ∈ {tr, te}leads to different train and testsplits ps (y), thereby inducingcorrelation between the FoVs.
Figure 2: Systematic test and train splits for two factors of variation. Black dots correspond to the trainingand red dots to the test distribution. Examples of the corresponding observations are shown on the right.
Figure 3: Randomdataset samples fromdSprites (1st), Shapes3D(2nd), CelebGlow (3rd),and MPI3D-real (4th).
Figure 4: R2 -score on various test-train splits. Compared to the in-distribution random splits, on the out-of-distribution (OOD) splits composition, interpolation, and extrapolation, we observe large drops in performance.
Figure 5: Extrapolation and modularity, R2-score on subsets. In the extrapolation setting, we furtherdifferentiate between factors that have been observed during training (ID factors) and extrapolated values (OODfactors) and measure the performances separately. As a reference, we compare to a random split. A model isconsidered modular, if it still infers ID factors correctly despite other factors being OOD.
Figure 6: Extrapolation towards the mean. We calculate (2) on the extrapolated OOD factors to measure thecloseness towards the mean compared to the ground-truth. Here, the values are mostly in [0, 1]. Thus, modelstend to predict values in previously observed ranges.
Figure 7: Spearman Correlation of degree of disentanglement with downstream performances. Wemeasure the DCI-Disentanglement metric on the 10-dimensional representation for β-VAE, PCL, SlowVAEand Ada-GVAE and the corresponding R2-score on the downstream performance. All p-values are below 0.01except for composition on Shapes3D which has p-value=0.14. Note that, we here provide Spearman’s rankcorrelation instead Pearson as the p-values are slightly lower.
Figure 8: CelebGlow DatasetThe CelebGlow dataset is created based on the invertible generative neural network of Kingma etal. (Kingma & Dhariwal, 2018). We used their provided network2 that is pretrained on the Celeb-HQdataset, and has labelled directions in the model-latent space that correspond to specific attributes ofthe dataset. Based on this latent space, we created the dataset as follows:1.	In the latent space of the model, we sample from a high dimensional Gaussian with zeromean and a low standard deviation of 0.3 to avoid too much variability.
Figure 9: Hyperparameter search on MPI3D for a CNN.
Figure 11: R2 -score on various splits.
Figure 12: R2-score on data augmenations. We depict the performance on the extrapolation setting with andwithout data augmentations for a CNN network with various random seeds on our considered datasets.
Figure 13: R2-score on in individual factors. Extrapolation performance across models when only a singlefactor (x-axis) is OOD.
Figure 14: Interpolation / Extrapolation and modularity.
Figure 15: Shapes3D extrapolation. We show the qualitative extrapolation of a CNN model. The shapecategory is excluded because no order is clear.
Figure 16: MPI3D-Real extrapolation. We show the qualitative extrapolation of a CNN model. The shapecategory is excluded because no order is clear. Size is excluded because only two values are available.
Figure 17: Model similarity on extrapolation errors.
Figure 18: CelebGlow extrapolation. We show the qualitative extrapolation of a DenseNet (ImageNet 1-K)model. This corresponds, to the model with the highest correlation with the ground truth in Fig. 17 on theextrapolated factors (OOd factors). The person category is not extrapolated and used to measure correlationsbecause no order is apparent.
