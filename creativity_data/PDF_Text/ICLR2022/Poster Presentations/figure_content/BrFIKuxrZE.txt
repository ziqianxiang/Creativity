Figure 1: Overview of Fair Normalizing Flows (FNF). There are two encoders, f0 and f1 , thattransform the two input distributions p0 and p1 into latent distributions pZ0 and pZ1 with a smallstatistical distance ∆ ≈ 0. Without FNF, a strong adversary g can easily recover sensitive attributea from the original input x, but once inputs are passed through FNF, we are guaranteed that anyadversary that tries to guess sensitive attributes from latent Z cannot be significantly better thanrandom chance. At the same time, we can ensure that any benevolent user h maintains high utility.
Figure 2: Samples from our example distribu-tion. The blue group (a = 0) is sampled fromp0 and the orange group (a = 1) is sampledfrom p1 .
Figure 3: Sensitive attribute recovery ratesfor adversarial training and fair normalizingflows (FNF) with 100 different random seeds.
Figure 4: Fair Normalizing Flows (FNF) on continuous and categorical data. The points showdifferent accuracy vs. statistical distance tradeoffs (with 95% confidence intervals from varied randomseeds), demonstrating that FNF significantly reduces statistical distance while retaining high accuracy.
Figure 5: Bounding adversarial accuracy.
Figure 6: Tradeoff between accuracy and various fairness metrics (demographic parity, equalizedodds, equal opportunity) when using Fair Normalizing Flows (FNF).
Figure 7: Different scalarization schemes.
Figure 8: Different priors used with FNF.
Figure 9: Visualizing k-means clusters on t-SNE embeddings of mappings between real points fromthe Crime dataset and their corresponding matches from the opposite attribute distribution.
