Figure 1: The execution timelines of inter-layer model parallelism and naive pipeline parallelism.
Figure 2: Timelines of various pipeline-parallel executions: (a) GPipe updates the weights in amini-batch. (b) PipeDream-flush moves backward pass forward compared to GPipe. (c) PipeDreamimplements weight stashing to update the weight immediately by default. (d) PipeDream-2BWrealizes periodic updates through gradient accumulation. For example, when the update period is 2,micro-batches 2 and 4 update weights in the figure.
Figure 3: Separate two adjacent update periods in PipeDream-2BW by introducing idle time blocks.
Figure 4: Derivation of WPipe pipeline: (a) Further partitioning and grouping, (b) moving the G0forward pass of the next step to the front of the backward pass of G0 of the current, (c) expansion ofthe pipelining for (b).
Figure 5: Timeline of execution of 2GW, where only G0 needs double weight versions.
Figure 6: Layouts of model parallelism and dataparallelism.
Figure 7: Part of the training loss from Table 2 and the accuracy when training ResNeXt50 fromscratch with WPipe, PipeDream-2BW, and Data Parallelism (SGD with a fixed learning rate of 0.01).
Figure 8: Optimal throughput for different batches in the Env-1 and Env-2. Where SM:N = {2 :4,4 : 2,8 : 1} in Env-1, SM:N = {4 : 16,8 : 8, 16 : 4,32 : 2,64 : 1} in Env-2. 8e-8f show thethroughput changes with different configurations.
Figure 9: The Bert96 and ResNeXt200 memoryfootprint vary with batch size. We set M : N as 8 :1 for Bert96 and M : N as 2 : 4 for ResNeXt200,which is the fastest configuration. We measuredthe average maximum memory footprint per GPU.
Figure 10: The remaining part of the training loss from Table2 and the F1 when training BERTBASEfrom scratch with WPipe, PipeDream-2BW, and DataParallel (Adam with a fixed learning rate of5 Ã— 10-6).
Figure 11: Optimal throughput for different batches in the Env-1 and Env-2. Where SM:N = {2 :4,4 : 2,8 : 1} in Env-1, SM:N = {4 : 16,8 : 8, 16 : 4,32 : 2,64 : 1} in Env-2. Figures 11e-11fshow the effect of compressed model-parallel communication on different models.
Figure 12: The communication process of the inter-mediate activations and gradients using automaticprecision compression.
Figure 13: The overlap of model execution and activations/gradients.
Figure 14: The relationship between model partitions and pipeline stages.
Figure 15: The pipeline grouping is applied to GPipe.
