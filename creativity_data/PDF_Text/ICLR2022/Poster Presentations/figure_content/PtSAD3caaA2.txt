Figure 1: MaxEnt RL is robust to disturbances.
Figure 2: MaxEnt RL and Robustness to Adversarial Reward Functions: (Left) Applying MaxEnt RLto one reward function (red dot) yields a policy that is guaranteed to get high reward on many other rewardfunctions (blue curve). (Center) For each reward function (r(a = 1), r(a = 2)) on that blue curve, we evaluatethe expected return of a stochastic policy. The robust RL problem (for rewards) is to choose the policy whoseworst-case reward (dark blue line) is largest. (Right) Plotting the MaxEnt RL objective (Eq. 1) for those samepolicies, we observe that the MaxEnt RL objective is identical to the robust RL objective .
Figure 3: MaxEnt RL is competitive with prior robust RL methods.
Figure 4: Robustness to changes in the dynamics: MaxEnt RL policies learn many ways of solving a task,making them robust to perturbations such as (Left) new obstacles and (Right) changes in the goal location.
Figure 5: MaxEnt RL is not standard RL + noise.
Figure 6: Robustness to dynamic perturbations:MaxEnt RL is robust to random external forces appliedto the environment dynamics.
Figure 7: Robustness to adversarial perturbations of the environment dynamics.
Figure 8: MaxEnt RL policies are robust to disturbances in the reward function.
Figure 9: Effect of Temperature: (Left) For a given reward function (blue dot), we plot therobust sets for various values of the temperature. Somewhat surprisingly, it appears that increasingthe temperature decreases the set of reward functions that MaxEnt is robust against. (Right) Weexamine the opposite: for a given reward function, which other robust sets might contain this rewardfunction. We observe that robust sets corresponding to larger temperatures (i.e., the red curve) can besimultaneously robust against more reward functions than robust sets at lower temperatures.
Figure 10: Approximately solving an arbitrary robust-reward control problem. In this experi-ment, we aim to solve the robust-reward control problem for an arbitrary set of reward functions.
Figure 11:	(Top) Both RL (SVG) and MaxEnt RL (SAC) effectively maximize expected reward.
Figure 12:	Robust RL Ablation Experiments: Ablations of the action robustness frame-work (Tessler et al., 2019) that use larger networks, multiple Q functions, or perform more explorationdo not perform significantly better.
