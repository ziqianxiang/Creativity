Figure 1:	The dynamics of GD under different learning rate hregions, escaping from the attraction of sharp minima. After some iterations, the algorithm enters thevicinity of a global minimum with more balanced magnitudes in x, y. Then in Phase 2, GD convergesto the found balanced global minimum. We remark that the searching-to-converging transition alsoappears in Lewkowycz et al. (2020). However, the algorithmic behaviors are not the same. In fact, inour searching phase (phase 1), the objective function does not exhibit the blow-up phenomenon. Inour convergence phase (phase 2), the analysis relies on a detailed state space partition (see Line 192)due to nonconvex nature of (3), while the analysis in Lewkowycz et al. (2020) is akin to monotoneconvergence in a convex problem.
Figure 2:	The objective function is (1 - xy>)2/2, where x> , y> ∈ R10. Highly unbalanced initialcondition is uniformly randomized, with the norms to be kx0k = 18, ky0k = 0.09.
Figure 3: Proof overview of Theorem 3.1. At the k-th iteration, we denote (xk, yk) as the iterate andSk is defined as Xk+ιy>+ι - μ = sk(xky> - μ)∙4 RANK- 1 APPROX. OF ISOTROPIC A (AN UNDER-PARAMETERIZED CASE)Given insights from scalar factorization, we consider rank-1 factorization of an isotropic matrix A,i.e., A = μln×n with μ > 0, d = 1, and n ∈ N+. The corresponding optimization problem ismin 1 ∣∣μIn×n - Xyτ∣∣F .
Figure 4: Balancing effect of general matrix factorization. We independently generate elements inA ∈ R6×6 from a Gaussian distribution. We choose X, Y ∈ R6×100 and randomly pick a pair ofinitial point (X0, Y0) with kX0kF = 1 and kY0kF = 9.
Figure 5:	Scalar factorization with kx0 k = 9, ky0 k = 1. The x-axis represents the number ofiterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the valuefor loss in the second row; the learning rate h for each column is the same.
Figure 6:	Scalar factorization with kx0k = 19, ky0k = 1. The x-axis represents the number ofiterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the valuefor loss in the second row; the learning rate h for each column is the same.
Figure 7:	Scalar factorization with kx0 k = 99, ky0 k = 1. The x-axis represents the number ofiterations k; the y-axis represents the value for the norm of xk and yk in the first row, and the valuefor loss in the second row; the learning rate h for each column is the same.
Figure 8:	General over-parameterized matrix factorization with kX0kF = 9 kY0kF = 1. The x-axisrepresents the number of iterations k; the y-axis represents the value for the norm of Xk and Yk inthe first row, and the value for loss in the second row; the learning rate h for each column is the same.
Figure 9: General over-parameterized matrix factorization with kX0 kF = 19 kY0kF = 1. Thex-axis represents the number of iterations k; the y-axis represents the value for the norm of Xkand Yk in the first row, and the value for loss in the second row; the learning rate h for each columnis the same.
Figure 10: General over-parameterized matrix factorization with kX0 kF = 99 kY0 kF = 1. Thex-axis represents the number of iterations k; the y-axis represents the value for the norm of Xkand Yk in the first row, and the value for loss in the second row; the learning rate h for each columnis the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kFand kYkkF at the beginning of the iterations.
Figure 11: General under-parameterized matrix factorization with kX0 kF = 9 kY0 kF = 1. Thex-axis represents the number of iterations k; the y-axis represents the value for the norm of Xkand Yk in the first row, and the value for loss in the second row; the learning rate h for each columnis the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kFand kYkkF at the beginning of the iterations.
Figure 12: General under-parameterized matrix factorization with kX0kF = 19 kY0kF = 1. Thex-axis represents the number of iterations k; the y-axis represents the value for the norm of Xkand Yk in the first row, and the value for loss in the second row; the learning rate h for each columnis the same. Note the x-axis range of the 1st row is shortened to better show the changes of kXk kFand kYkkF at the beginning of the iterations.
Figure 13: Matrix sensing. The x-axis represents the number of iterations k; the y-axis representsthe value for the norm of Xk and Yk in the first row, and the value for loss in the second row; thelearning rate h for each column is the same.
Figure 14:	Matrix completion. The x-axis represents the number of iterations k; the y-axisrepresents the value for the norm of Xk and Yk in the first row, and the value for loss in the secondrow; the learning rate h for each column is the same. Note the x-axis range of the 1st row isshortened to better show the changes of kXk kF and kYk kF at the beginning of the iterations.
Figure 15:	Three orbits of period 2, 3, and 4. The blue line are the orbits; the red line is a referenceline of the global minima xy = 1.
Figure 16: Trajectories: modified equation vs GDD Proof of Proposition 2.1Proofof Proposition 2.1. This proposition is a direct consequence of Theorem F.2 and I.3.	□E Over-parametrized scalar decomposition: stability limitConsider the objective (1 - xy)2/2. Choose the initial condition to be x0 = 20, y0 = 0.07 and useGD update. The upper bound of h in Theorem 3.1 is /々+/+4口, where μ = 1. In Figure 17, Whenh = 方2+，2+4 (the left one), GD converges; however, when h is slightly larger than this bound, itblows up. Hence our restriction for h is very close to the stability limit.
Figure 17: The loss with slightly different h near its upper boundTheorem F.2. The eigenvalues of the Hessian of (μ 一 x>y)2∕2 are ±(μ — XTy) repeated n 一 1times and 2(kχ∣∣2 + ∣∣yk2 ± vz(kxk2 + kyk2)2 + 4(μ 一 XTy)2 — 8(μ 一 XTy)xτy). Especially, atXTy = μ, the eigenvalues are ∣∣x∣2 + ∣∣y∣2 and 0 repeated 2n _ 1 times.
