Figure 1:	Per year machine learning publications. Left: cumulative amount across keywords withcontinuous components that influence continual learning practice, see Section 2. Right: increasinguse of “continual”, demonstrating a shift from the preceding emphasis on “lifelong”. Data queriedusing Microsoft Academic Graph (Sinha et al., 2015), based on keyword occurrence in the abstract.
Figure 2:	A typical (continual) machine learning workflow. The inner circle depicts the workflowadvocated by Google Cloud (2021). On the outer circle we have added important, non-exhaustive,aspects to consider from the prevalent static benchmarking perspective (gray boxes) vs. additionalcriteria to be taken into account under dynamic evolution in a continual approach (orange boxes).
Figure 3: Relationships between related machine learning paradigms with continuous components.
Figure 4: The Continual Learning EValuation Assessment (CLEVA) Compass. The inner star plotcontextualizes the set-up and evaluation of continual approaches with respect to the influences ofrelated paradigms, providing a visual indication for comparability. A mark on the star plots’ innercircle suggests an element being addressed through supervision, whereas a mark on the outer starplots’ circle displays an unsupervised perspective. The outer level of the CLEVA-Compass allowsspecifying which particular measures have been reported in practice, further promoting transparencyin interpretation of results. To provide an intuitive illustration, five distinct methods have been usedto fill the compass. Although these methods may initially tackle the same image datasets, it becomesclear that variations and nuances in set-up and evaluation render a direct comparison challenging.
Figure 5: The CLEVA-Compass GUI. With the help of this application, users can interactivelycustomize and construct their own CLEVA-Compass visualization or import existing ones.
