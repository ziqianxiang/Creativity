Figure 1:	Example outputs from full model, subnetwork at extreme sparsity, and best performing subnetworkon TED-LIUM test utterances. Target: ground-truth transcriptions. Recognition errors are highlighted in red.
Figure 2:	The WER curves of the best subnetworksproduced by different pruning approaches. IMP: itera-tive magnitude pruning, which is the pruning approachwe used in subnetwork searching. Random pruningand Random Ticket are the two baseline pruning ap-proaches we evaluated.
Figure 3: The WER curves of initialization with pre-trained models. We test CNN-LSTM backbone onTED-LIUM dataset. θ0 : random initialization; θLibri :initialized from LibriSpeech pre-trained model; θCV :initialized from CommonVoice pre-trained model.
Figure 4: Visualizations of weight matrix pruned with (a) unstructured sparsity (b) block sparsity. Prunedweights are shown in white color.
Figure 5: The WER curves of transferring winning tickets to target datasets: (a) TED-LIUM, (b) CommonVoice,and (c) LibriSpeech (test-clean). Each curve represents the winning tickets extracted from a source dataset. Thedashed lines indicate the WERs of full models on target datasets.
Figure 6: Example outputs from full model, subnetwork at extreme sparsity, and best performingsubnetwork on TED-LIUM test utterances. Target: ground-truth transcriptions. Recognition errorsare highlighted in red.
