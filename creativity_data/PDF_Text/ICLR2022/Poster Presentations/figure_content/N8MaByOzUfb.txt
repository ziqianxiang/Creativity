Figure 1: (left) Analysis of representations with the first task's class prototypes at a task boundary.
Figure 2: Buffer displaCement in a 5 task stream.
Figure 3: Total Accuracy as a function of TeraFLOPsspent. Here the models are evaluated on all 10 classes,to ensure consistency across timesteps.
Figure 4: For Split-CIFAR-10, we monitor the performance on the current task observed in the streamfor SS-IL, ER, and ER-ACE. ER fits too abruptly current task; ER-ACE incorporates this knowledgeslowly; SS-IL barely on the other hand is unable to learn new tasks when they are first observed inthe stream15Published as a conference paper at ICLR 2022C Overfitting on buffered samplesWe study the extent to which our proposed method reduces over-fitting to samples stored in the buffer.
Figure 5: Alignment between buffer and holdout representations. ER-ACE has constantly largeralignment between seen and unseen samples compared to ER especially for older tasks.
Figure 6: Comparison to Dark Experience Replay (DER). We obtain improved performance and we can enhancethe DER method using the ER-ACE approach6u≤θpoL16Published as a conference paper at ICLR 2022	ER		(3.2 ± 1.8) × 10-2ER-AML-Triplet w. All Negs	(3.0 ± 0.6) × 10-2ER-AML-TriPlet w. Incoming Negs	(2.5 ± 0.6) × 10-2Table 5: Average Drift (avg distance in feature space) of buffered representations for CIFAR-10during learning of the second task. We observe similar behavior to ER-AML with SupConAccuracy ↑M = 5 M = 20 M = 50 M = 100iid online	60.8 ± 1.0	60.8 ± 1.0	60.8 ± 1.0	60.8 ± 1.0iid++ online	72.0 ± 0.1	72.0 ± 0.1	72.0 ± 0.1	72.0 ± 0.1iid offline	79.2 ± 0.4	79.2 ± 0.4	79.2 ± 0.4	79.2 ± 0.4fine-tuning	18.4 ± 0.3	18.4 ± 0.3	18.4 ± 0.3	18.4 ± 0.3ER	19.0 ± 0.1	26.7 ± 0.3	36.1 ± 0.6	41.5 ± 0.6ER-AML Triplet	33.0 ± 0.3	40.1 ± 0.4	46.0 ± 0.5	49.8 ± 0.5ER-AML SupCon	33.0 ± 0.2	41.9 ± 0.1	48.3 ± 0.2	51.9 ± 0.3Table 6: Ablation comparing ER-AML with triplet loss to ER-AML with SupCon. We observe both
Figure 7: Gradient’s norm for first task features in a two task learning scenario. We observe a sharpincrease when all negatives are used and decrease using only incoming negatives.
Figure 8: 1 Training Iteration on the Second TaskAfter 100 training iterations, we see that for ER, the prototypes of the old classes have been signifi-cantly displaced and are far from the points of similar class. This is not the case for the latter two18Published as a conference paper at ICLR 2022methods; for ER-ACE and ER-AML, the model is beginning to separate de classes from one another,and the class prototypes are near their respective classes.
Figure 9: 100 Training Iterations on the Second TaskAfter 400 training iterations, ER still struggles to align the class prototypes with the respective classes.
Figure 10: 400 Training Iterations on the Second TaskAt the end of the second task, ER-ACE and ER-AML have successfully clustered the classes andaligned their respective prototypes with the clusters. As for ER, while the data is clustered, theprototypes are not properly aligned with class clusters. Moreover, we still see a strong overlapbetween prototypes of Class 2 and 3.
Figure 11: End of the Second TaskJ Additional B lurry Task Boundaries ExperimentsHere we provide blurry task results for varying levels of task overlap. To give an idea of how muchthe tasks overlap, we report the average number of unique classes per incoming minibatch (MB): asmall number means that the tasks are well separated. A high number means that there is a strongoverlap. In the fully i.i.d setting, this number would be maximized. On the other hand, when thisequals 1, each data class is streamed one after the other.
