Figure 1: We analyze state-of-the-art GNN modules for the task of KG-powered question answering,and find that the counting of edges in the graph plays an essential role in knowledge-aware reasoning.
Figure 2: The retrieved sub-graph of KG is formulated as entity nodes representing concepts connectedby edges representing relations and the central context node connected to all question entity nodesand answer entity nodes. The pre-processed graph data generally has the following ingredients: nodeembedding initialized with pre-trained KG embeddings, relevance score computed by LM, adjacencymatrix representing the topological graph structure, edge embeddings to encode the node types, andedge information of relation types. We adapt SparseVD as a diagnostic tool to dissect GNN-basedreasoning modules for QA, getting the sparse ratio of each layer to indicate its importance. We findthat some layers and ingredients are completely dispensable, which inspires us to design a simple,efficient and effective GNN module as the replacement of existing complex GNN modules.
Figure 3: The sparse ratio curves obtained from the SparseVD training of GNN reasoning modules:1) the left plot shows the curves for the embedding layers in QA-GNN, where the node score and thenode type reach a low ratio, while the edge encoder preserves a large ratio; 2) the middle plot showsthe curves for the layers within the QA-GNN graph attention layer, where the key and query layersconverge to a fairly low ratio, while the value layer has a relatively large ratio; 3) the right plot showsthe curves of the initial node embedding layers of three representative GNN-based QA methods,where all the ratios are close to 0, indicating that the initial node embeddings are dispensable.
Figure 4: Graph Soft Counter (right) extremely simplifies the architecture of conventional GNNs(left). The GSC layers are parameter-free and only keep the basic graph operations: 1) update eachedge embedding with incoming node (in-node) embeddings; 2) update each node embedding byaggregating the edge embeddings. Since We reduce the hidden dimension to only 1, GSC can beviewed as a soft counter over the graph for generating the final score of the output node.
Figure 5: Venn diagrams for the prediction overlap of different models and ground truth (GT) ofthe IHtest split of CommonsenseQA. The ALBERT has the least overlap since all the left four useRoBERTa to encode the QA context. The order of the percentage of overlap of left four exactlymatches the order of the performance of each models: GSC > QA-GNN > MHGRN > RoBERTa.
Figure 6: Our GSC is highly interpretable. For the retrieved sub-graph of each answer choice, We candirectly observe the behavior of the model by print out the output edge/node values of each layer, sothat we can trace back to see how the model scores the answer.
Figure 7: The sparse ratio curve when do SparseVD training for the QA-GNN systems.
