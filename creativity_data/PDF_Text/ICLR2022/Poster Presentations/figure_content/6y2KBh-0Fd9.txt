Figure 1: Examples of 2-D Gaussian in the latent space. In (a) and (b), left: the densities of InD andOOD; and middle and right: the density histograms of InD and OOD in each latent dimension. xmeans input variable while z denotes latent variable.
Figure 2: Random projections of 2-d Gaussians. proj1 is the projection along the axis (1, 0), andproj2 is the projection along (空,-空).For the standard Gaussian, both proj1 (a) and proj2 (b)follow N(0, 1); while for the correlated Gaussian, proj1 (c) follows N(0, 1), but proj2 (d) followsN(0, 5), instead of N(0,1).
Figure 3: Random projections vs. autoencoder. (a)-(b) Per-forming random projection outperforms training an extraautoencoder in two OOD detection experiments. Besides,the OOD detection performance using RealNVP saturatesas the number of random projections grows beyond 50.
Figure 4: Different divergences.
Figure 5: Model capacity vs. OOD detection. While the density histograms estimated from thesimple Glow (a) and complex Glow (b) are similar, the generation from the simple Glow (c) andcomplex Glow (d) are noticeably different. More surprisingly, the OOD detection performance bysimple Glow (AUROCs: 0.99|0.99) is better than the complex Glow (AUROCs: 0.96|0.95).
Figure 7: An example of distribution based group anomaly. (a) Two 2-d Gaussians with similardensity and large overlap in support. (b) Density histograms largely overlap, where likelihood scoreonly achieves a detection AUROC of 0.57.
Figure 8: Model capacity vs. OOD detection. Images are generated from RealNVP trained onCelebA. Assume CIFAR-10 as OOD, and our GOD1KS and GOD2KS results are segmented by|. (a) Generated images from a RealNVP with 1 block and 2048 hidden channels. AUROCs =0.98|0.99. (b) Generated images from a RealNVP with 16 blocks and 512 hidden channels. AU-ROCs = 0.88|0.88.
Figure 9: Training epochs vs. OOD detection. Images are generated from RealNVP trained onCelebA. Assume CIFAR-10 as OOD, and our GOD1KS and GOD2KS results are segmented by|. (a) Trained for 10 epochs. AUROCs = 0.78|0.78. (b) Trained for 100 epochs. AUROCs =0.81|0.81. (c) Trained for 150 epochs. AUROCs = 0.86|0.86. (d) Trained for 200 epochs. AUROCs= 0.88|0.88.
Figure 10: Training epochs vs. OOD detection. Images are generated from a Glow with K =16,L = 3,h = 128 trained on CIFAR-10. Treating SVHN as OOD, our GOD1KS and GOD2KSresults are segmented by |. (a) Trained for 5 epochs. AUROCs = 0.96|0.96. (b) Trained for 10epochs. AUROCs = 0.96|0.95. (c) Trained for 100 epochs. AUROCs = 0.76|0.84. (d) Trained for200 epochs. AUROCs = 0.87|0.84.
Figure 11: Training epochs vs. OOD detection. Images are generated from a Glow with K =16,L = 2,h = 128 trained on FMNIST. Treating MNIST as OOD, our GOD1KS and GOD2KSresults are segmented by |. (a) Trained for 5 epochs. AUROCs = 1.00|1.00. (b) Trained for 10epochs. AUROCs = 1.00|1.00. (c) Trained for 100 epochs. AUROCs = 0.90|1.00. (d) Trained for200 epochs. AUROCs = 0.92|1.00.
Figure 12: Per-dimensional mean and variance comparison. (a)-(b) Per-dimensional mean and vari-ance of CIFARs and LSUN are close, which implies that CIFARs and LSUN share a similar distri-bution of normalized pixel values. (c)-(d) As a comparison, CIFARs vs SVHN have more clearlyseparable per-dimensional variance.
