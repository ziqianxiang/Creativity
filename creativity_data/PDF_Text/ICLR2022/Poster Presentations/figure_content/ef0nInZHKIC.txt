Figure 1: The proposed symbolic regression workflow for L2O models: The meta training step enjoys the easeof optimization in the neural network function representation space, and come up with a numerical L2O model;the symbolic regression step distills a light weight surrogate symbolic equation from the numerical L2O model;the meta tuning step makes the distilled symbolic equation again amendable for re-parameterization.
Figure 2: The performance comparison of the meta-fine-tuned symbolic L2O and the baseline optimizers. Thesymbolic L2O model achieves even better performance than the laboriously tuned traditional optimizers.
Figure 3: The sampled mapping functions of different optimizers w.r.t. their input feature sets. The colorfulmarked lines are the numerical/hand-crafted models, while the black solid lines are the distilled symbolicequations. In the line markers, “A@B” means the mapping function is A, the input of this function is B, and atthe current time step, B is the only variable. It can be observed that the symbolic distillation fits the underlyingoptimization algorithms accurately.
Figure 4: The optimization trajectories of the DM and the RP((semxtarall)) models.
Figure 5: Left: the fitting performance of the equations with different level of complexity. The R2-score indicatesthe fitting accuracy level of a single distilled equation compared with the original numerical optimizer. Right:the estimated TPF evaluated over distilled equations with different level of complexities. These figure revealedthe relationship between the distilled equation complexity with the fitting ability and the estimated TPF. Thevalues come from numerical L2Os meta-pre-trained over P3 (MNIST dataset with shallow MLP).
Figure 6: The tanh() function series. These functions are used in equation 6, where the learnable coefficient γdetermine the region of the linearity (within which the tanh() function is close to y = x)Second, the tanh() functions in the distilled equations of LSTM models are straightforward tounderstand: the symbolic rules are the surrogate of the LSTM model, and the LSTM uses the tanh()function as its non-linear activation function. Therefore, the tanh() function in the distilled equationscould be correctly reflecting the nonlinearity of the LSTM teacher’s updating dynamics.
Figure 7: The pseudo-code for the core mutation operation used in the SR algorithm.
