Figure 1: The Four-Corners environmentFour-Cornerspertaining to task-identification on the blue task, making it a useful pre-adaptation ■trajectory. HFR reuses T for the blue task since the utility UTblue(τ) is high.
Figure 2: Success-{-20, -58, -20, -20}, while the utility values {Upurple(T),Ublue(T),Uorange(T), rate on Four-CornersUgreen(T)} are {-1015, -756, -935, -931}. These (unnormalized) numbers showthat the probability of relabeling this trajectory with the blue task is low under HIPI, but high underHFR. Further analysis is included in Appendix A.6. Figure 2 compares HFR and HIPI in terms of thesuccess-rate in the Four-Corners environment, and shows the performance benefit of using the utilityfunction for trajectory relabeling.
Figure 3: An illustration of the differences between multi-task RL and meta-RL. In multi-task RL (blue) theagent simply maximizes its returns given a task ψ, while in meta-RL (orange) the agent must first quickly identifythe task with a limited number of exploratory trajectories (first two orange stacks in the figure), before adaptingto the task and maximizing returns. Because of these differences, existing multi-task relabeling methods may besub-optimal for meta-RL.
Figure 4: During meta-training, after a trajectory τ is collected for task ψi , HFR uses hindsight to relabelthis trajectory using reward functions for different tasks, and then uses foresight to compute the utility of therelabeled trajectory for the different tasks. A distribution over tasks is constructed using the utilities, and atask ψ k is sampled from the distribution, with tasks for which the trajectory has higher (normalized) utilityhaving higher probability mass. The trajectory is then relabeled using the reward function rψ k and added tothe task-specific replay buffer Bψ k . Finally, the meta-training update rules are applied. This process repeatsthroughout the entirety of meta-training. HFR uses PEARL as the base meta-RL algorithm and does not alter itsdata collection or meta-gradient computation rules. Please see the Algorithm 1 box for details.
Figure 5: MUJoCo environments We evaluate on - sparse reward manipulation tasks (a, b, c), as well as sparseand dense reward locomotion tasks (c, d).
Figure 6: Performance of our relabeling algorithm HFR (shown in blue) on sparse reward tasks. HFRconsistently outperforms baselines on both sparse reward robotic manipulation and locomotion tasks.
Figure 7: Performance of our relabeling algorithm HFR (shown in blue) on dense reward tasks. With theexception of Cheetah-Highdim, relabeling in general offers no benefit in dense reward meta-RL tasks, likely dueto the highly informative nature of a dense reward function.
Figure 8: Ablation analyses. (a) HFR with different values for the batch size NU used in approximating Eq. 9.
Figure 10: Perfor-mance on tasks thatdiffer in both rewardfunction and transi-tion dynamics.
Figure 9: Performance of our relabeling algorithm HFR (blue) compared to a variant of HFR basedon the Bellman error (orange)Table 1: PEARL hypeparameters used for all experiments.
Figure 11: Example trajectories from our Four-Corners Environment. The goal for the task drawnfrom HFR’s relabeling distribution is shown in green, while the red square represents an area of largenegative reward for that task. Encountering the red square informs the agent of the goal location. It isclear that trajectories (a), (b), (c), and (d) are most useful for meta-training on how to reach the topleft goal, top right goal, bottom left goal, and bottom right goal, respectively. HFR correctly relabelsthese trajectories with the appropriate task, while HIPI fails to do so due to the large negative returnsachieved under them. Table 3 shows the trajectory returns that HIPI uses, and the unnormalizedQ-values that HFR uses, to sample a task to relabel trajectories (a), (b), (c), and (d) with.
