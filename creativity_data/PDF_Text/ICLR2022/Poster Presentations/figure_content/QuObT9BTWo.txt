Figure 1: Preference-Conditioned Neural Multiobjective Combinatorial Optimization: a) Themodel takes a problem instance as its input. b) The decision makers assign their preferences ondifferent objectives to the model. c) The model directly generates approximate Pareto solutions withdifferent trade-offs via fast forward inference. In this example, the problem is the MOTSP with twocost objectives to minimize. The generated solutions P1 ,P2 and P3 are different optimal trade-offsbetween the two cost objectives. The ideal model can generate solutions for all possible optimaltrade-offs on the Pareto front and not generate a poor solution such as P4 .
Figure 2: Pareto Set Approximation for a New Problem Instance: Our method with differentnumbers of preferences on three objective MOTSP with 50 nodes. a)&b) With a small number ofpreferences, our model can generate a sparse approximation to the Pareto set. c)&d) With a largenumber of preferences, it can generate a dense approximation. Our method can generate any trade-off solutions with a single model without searching, whereas other methods need to solve or build amodel for each preference separately. More discussions can be found in Appendix D.3 D.5 D.6 D.7.
Figure 3: Preference-conditioned Neural MOCO Model: a) The input is a problem instance s(e.g., a graph). b) A shared attention encoder transfers the instance s into a set of embeddings. c) Theembeddings for all nodes would be used by the decoder multiple times with different preferences.
Figure 4: Hypervolume illustration.
Figure 5: Different number of uniform distributed preferences and the corresponding solutions gen-erated by our method on MOTSP20, MOTSP50 and MOTSP100. Our model can generate well-distributed solutions with a small number of preferences, and generate a dense approximation witha large number of preferences.
Figure 6: Different number of uniformly distributed preferences and their connections to the corre-sponding solutions on MOTSP100.
Figure 7: Different number of uniformly distributed preferences and their connections to the corre-sponding solutions on MOCVRP100. We report the approximate Pareto front for MA-MOCO with101 models, and our model with 101 and 1, 001 preferences.
Figure 8: Different number of approximate Pareto solutions generated by our method with uniformlydistributed weight vectors and different aggregation functions.
Figure 9: The uniform/non-uniform distributed preferences and different number of correspondingsolutions generated by our method on three-objective MOTSP100 with asymmetric Pareto front. Weuse 1035 different preferences as example for the uniform and non-uniform case respectively, andpresent the results for 105, 1035 and 10011 solutions. Top Row: Uniform distributed preferencesand the corresponding non evenly distributed solutions. Bottom Row: Non-uniform distributedpreferences and the corresponding solutions which are more evenly distributed.
Figure 10: Symmetric Pareto Front. Different set of biased weights and their corresponding solu-tions on different regions of the symmetric Pareto front.
Figure 11: Asymmetric Pareto Front. Different set of biased weights and their correspondingsolutions on different regions of the irregular Pareto front. The uniformness of the weights and thecorresponding solutions could be different due to the asymmetry. However, the user can adaptivelyadjust the weights in real-time to search for the most suitable weight(s) and solution(s).
Figure 12:	The value path plots for the 10-objeCtive MOKP100 obtained by MOEA/D, NSGA-IIand our proposed P-MOCO. In the plot, eaCh line (value path) represents a solutionâ€™s 10 objeCtivevalues with its speCifiC preferenCe. In MOKP, we want to maximize the values for all objeCtives.
Figure 13:	The value path plots for the 10-objective MOKP with 500 items obtained by MOEA/D,NSGA-II and our proposed P-MOCO. Our model is trained on 10-objeCtive MOKP100.
