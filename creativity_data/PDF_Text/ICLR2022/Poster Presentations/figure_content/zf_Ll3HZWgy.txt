Figure 1:	The training process of a V&L model typically consists of three steps: 1) visual encoderpre-training, 2) vision-and-language pre-training (optional), and 3) task-specific fine-tuning. Inprevious V&L models, visual encoder pre-training requires human annotated vision datasets, whichis hard to scale up. Our CLIP-ViL proposes to use CLIP, which is trained on image-text pairs crawledfrom the Internet, as the visual encoder for V&L models. This reduces the need for human annotatedin the pipeline and greatly improves model performance.
Figure 2:	CLIP versus other visual encoders. Region-based methods (Anderson et al., 2018a)are trained on object detection data. For grid-based methods, previous work use either imageclassification (He et al., 2016) or detection data (Jiang et al., 2020). However, CLIP requires onlyaligned text.
Figure 3:	Grad-CAM Visualization of CLIP-ViT-B/32, CLIP-ViT-B/16, CLIP-Res50, CLIP-Res101and CLIP-Res50x4 for the question “What color is the woman’s shirt on the left?”.
Figure 4:	Grad-CAM Visualization of CLIP-ViT-B/32, CLIP-ViT-B/16, CLIP-Res50, CLIP-Res101and CLIP-Res50x4 for the question “What color are her eyes?”.
Figure 5:	Grad-CAM Visualization of CLIP-ViT-B/32, CLIP-ViT-B/16, CLIP-Res50, CLIP-Res101and CLIP-Res50x4 for the question “What is just above the plate?”.
