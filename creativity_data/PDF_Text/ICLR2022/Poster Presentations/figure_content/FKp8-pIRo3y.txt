Figure 2: The HinDRL pipeline performing a 3.5mm jack cable insertion. The input to the policy are encodedtarget goal and the current robot state. At train time, the produced episodes are relabelled in hindsight usinga choice of sampling strategies. We consider two main strategies for sampling goals in hindsight. Strategy a)shows the standard hindsight goal selection strategies that select goals from states within the trajectory beingrelabelled; and strategy b) shows a mechanism that focuses on sampling goals directly from some collection ofsuccessful trajectories. The resulted relabelled data is fed into the replay buffer.
Figure 3: Algorithm for Hindsight goal selection for Demo-driven Reinforcement learning (HinDRL)P(Zg) implied from some given behaviour. Typically we sample candidate goals from the future statesin rollout trajectories. Then, we re-evaluate the given transition with the newly sampled goal andassign a new reward. This is achieved with the help of a goal conditioned reward r(Zt , Zg) and not theenvironment reward from the previous paragraph. Thanks to the goal-conditioned formulation, fittinga Q function from both of these sources of reward is well-posed. In this section we propose a way forcomposing a hindsight goal distribution P(Zg), that is targeted on the specific task at hand.
Figure 4: t-SNE of latent representations obtained withTCC (Dwibedi et al., 2019) against novel successfultrajectories. Temporal color-coding, purple is start andyellow is goal.
Figure 5: Tasks description a) is the parameterised reach task. Visiting the small yellow goals must happenbefore reaching the middle goal. b) is dual arm reaching, we combine this with c) lifting, d) aligning in bothposition and orientation and e) is the dual arm insertion.
Figure 6: Accuracy against different number of demonstrations on the Bring Near + Orient task. Each plotshows the overall achieved accuracy during training. HinDRL consistently outperforms the alternatives. Successwith one demonstration depends on the selected demonstration. We choose it on a random principle.
Figure 7: Measuring per-step reward using the smallest number of demonstrations that resulted in learning.
Figure 8: Robustness to the quality of the encoder. Comparing the engineered encoder with raw stateobservations and learnt Î² -VAE and TCC based encoders. The engineered encoder performs best with TCCachieving close to commensurate performance.
Figure 9: Predicting episodeprogress With 10-fold KNN.
Figure 10: Different types of on-line goal selections.
Figure 11: Measuring per-step reWard using the smallest number of demonstrations that resulted in learning.
Figure 12: Comparing different demo-driven supportdistributions.
Figure 13: Injecting noise to the engineeredencoder. Reach, Grasp, Lift, Orient Task.
Figure 14: Learnt representations.
Figure 15: Measuring per-step reward.
Figure 16: Measuring per-step reward usingthe smallest number of demonstrations thatresulted in learning.
Figure 17: Ablating the rollingwindow.
