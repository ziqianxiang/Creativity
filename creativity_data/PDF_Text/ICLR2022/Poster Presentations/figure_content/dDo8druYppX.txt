Figure 1: Few-shot learning v.s. shape representation. Left (gray): few-shot classification (im-ages taken from miniImageNet). We show 2-shot-3-way few-shot classification in which everytraining task contains 3 categories (shown in blue, orange and red bounding boxes) and 2 trainingsamples in each category. Thus we use 2 × 3 = 6 training samples in Dttrain to build classifierswhich are expected to work well on Dttest . Right (yellow): shape representation. Shape surfacesare shown as dotted lines. Our proposed training data generating networks can encode a shape t aslabeled set of points with labels blue or red to provide a training set Dttrain . The learning frame-work uses the training sets to build the surfaces (classification boundaries) which can be evaluatedby densely sampling the space, i.e., Dttest .
Figure 2: Pipeline. Networks with trainable parameters are shown in red boxes with round corners.
Figure 3: Top: Input images (137 × 137) and ground-truth meshes shown in vermilion red. Mid-dle: predicted meshes from (re-implemented) OccNet (Mescheder et al., 2019) shown in yellow.
Figure 4: Reconstruction with interpolated feature vectors. Meshes in blue are original, greenmeshes are interpolated.
Figure 5:	We show Dtrain andDtrain in embedding space given N =32, 64, 128 and w = 0.01. Positivepoints are shown in red and negativespoints in blue. The shapes in bothspaces are shown as transparent sur-faces.
Figure 6: Visualizations of Dtrain WhenN = 64 and w = 0.01. Positive pointsin red and negative points in blue.
Figure 7: Networks. We use fully connected layers in both netWorks. In (a), the input is theconcatenation of feature λ (yelloW) and point x (blue). In (b), the input is the feature λ (yelloW),and the output is the coordinates of points X ∈ RN ×3 (red).
Figure 8: T-SNE visualization using the extracted features of the shapes.
Figure 9: Statistics of metric.
Figure 10: Reconstruction with bilinear interpolated feature vectors. Meshes in blue are orig-inal, green meshes are interpolated. The bilinear equation is λv,h = (1 - v)(1 - h)λ0,0 + (1 -v)hλ0,1 + v(1 - h)λ1,0 + vhλ1,115Published as a conference paper at ICLR 2022Category	IoU ↑		Chamfer J		F-SCore ↑		SVM	RR	SVM	RR	SVM	RRairplane	0.639	0.633	0.117	0.121	71.87	71.15bench	0.522	0.524	0.144	0.132	67.88	69.44cabinet	0.731	0.732	0.139	0.141	65.15	65.33car	0.745	0.748	0.119	0.121	65.77	65.48chair	0.526	0.532	0.229	0.205	47.73	48.83display	0.552	0.553	0.213	0.215	47.88	46.96lamp	0.388	0.383	0.398	0.391	41.94	42.99loudspeaker	0.662	0.658	0.252	0.247	46.95	46.86rifle	0.536	0.540	0.112	0.111	69.18	69.40sofa	0.700	0.707	0.158	0.155	55.40	56.40table	0.538	0.551	0.193	0.171	64.03	65.25telephone	0.776	0.779	0.101	0.106	76.60	75.96vessel	0.563	0.567	0.177	0.186	51.38	51.57
Figure 11: Effects of large w.
Figure 12: Visual results for Stanford Online Products. The input images are shown in top row.
Figure 13: Reconstruction with interpolated feature vectors. Meshes in blue are original, greenmeshes are interpolated.
