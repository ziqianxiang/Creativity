Figure 1: Training accuracy on MNIST for feedforward neural networks trained by DODGE withrandom directions. a) We report the accuracy as a function of the number of updates. In grey,DODGE with oracle steepest descent directions. From orange to dark blue, DODGE with increasingnumbers of directions sampled on each update, from 1 to 100. Estimating the gradient using moredirections improves performance, but learning is possible and stable even with a single direction. b)A parameter study of the joint effect of the number of directions and the batch size used to evaluatethe directional derivative along each direction. We evaluate the accuracy after seeing a fixed numberof samples (regardless of how they are batched). Increasing batch size from 100 to 300 improvesperformance, especially when using a single direction u (in orange). Increasing the batch size further(to 1000) however decreases performance. c) A parameter study of the effectiveness of DODGE fordifferent network sizes. The accuracy is reported as a function of the number of units in the twohidden layers of the multi-layer perceptron used in the experiment. The different lines correspond todifferent snapshots during training: after 1000, 2000, 3000, 4000 updates, respectively.
Figure 2: DODGE with a direction from truncated backpropagation through time (TBPTT). We plotcross-entropy (Y axis) as a function of the training steps (X axis). Lower cross-entropy is better.
Figure 3: Direction from the update proposed by Reptile. DODGE as a result of using a directionproposed by Reptile. We plot Peak Signal-to-Noise Ratio (PSNR; Y axis) as a function of the training-steps (X axis). Higher PSNR is better. PSNR is commonly used to quantify the reconstruction qualityfor images.
Figure 4: We plot cross-entropy (Y axis) as a function of training-steps (X axis) on the copy task.
