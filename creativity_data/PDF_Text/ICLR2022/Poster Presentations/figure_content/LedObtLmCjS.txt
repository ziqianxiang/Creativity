Figure 1: Illustration of why Bilinear Value Networks (BVNs) improve generalization to new goals.
Figure 2: COmparisOn between twO value decOmpOsi-tiOn schemes. The task, states, and gOals are the sameas Figure 1c. The black square indicates Obstacles thatthe agent cannOt pass thrOugh. Gray arrOws indicatethe vectOr field φ evaluated at a particular gOal g , fOr allstates s ∈ S. (a) As in priOr wOrk Schaul et al. (2015),the vectOr field φ(g) is a cOnstant and dOes nOt dependOn s. (b) BVN parameterizes φ as a functiOn Of bOth sand g making it mOre expressive, but still maintainingthe benefit Of disentanglement frOm f(s, a).
Figure 3: (a) The U-maze environment where the states s and goals g are (x, y) on the plane. Thetask is to reach the goal position (red circle). (b) The learned Q-Value predictions Q(s, a*, g) Withthe best action a* over states S in the 2-dimensional state space for a goal g indicated by the redcircle. OVerall, Values decrease as the agent gets further and further aWay from the goal. (c) Thelatent vectors produced by the agent, projected to a 2D vector space. There is an overall trend for φto decrease in magnitude as one gets closer to the goal. (c Inset) Comparing the alignment of vectorrepresentation of a random (suboptimal) action produced by the f component, f (s, arand) (pink)against the vector corresponding to the optimal action from the policy π, f (s, a*) (blue) from thePhi vector (black). The optimal alignment corresponds to an angle of 90° because the maximumQ-value in this environment is 0 and not 1. We find that the optimal action consistently produces abetter alignment in comparison to a sub-optimal action.
Figure 4: Learning curves on the Fetch gym environments, and the Fetch-extension (see Section A.4)task suit. BVN attained higher success rates using less data than the baselines in 5 out 8 domains.
Figure 5: Learning curves on the Shadow hand dexterous manipulation suite. BVN learns fasterthan the baselines on six out of eight domains.
Figure 6: (a & b) Examples showing (a) how we split the goal distribution into left and right and (b)according to the radius to the center (near versus far). We pretrain on the green regions, then finetuneon the red regions. The state distribution remain identical to the vanilla Fetch environment. (c) Fine-tuning curves on unseen goals. BVN achieves better performance than the UVFA (Monlithic) on alltasks in both left-to-right and near-to-far adaptation scenarios. Freezing f causes BVN performanceto plateau, indicating that both φ and f are needed for approximating the multi-goal value function.
Figure 8: Replacing the dot-product with an `2norm does not affect the final performance or thesample complexity. The initial performance is af-fected marginally. Whereas the linear combina-tion is too restrictive.
Figure 7: Increasing the latent dimension of fand φ improves our method’s performance. De-spite that, the result shows that a low dimensionallatent space (d = 3) is sufficient to enable ourmethod to outperform the baseline UVFA (Mon-lithic).
Figure 9: Comparison between alternative factorizations show that our bilinear value decompositionscheme, f (s, a)>φ(s, g), outperforms these alternatives. This suggests that this particular inputgrouping is crucial.
Figure 10:Table 2: Environment Specifications.
Figure 10: Renderings of the fetch robot and shadow hand domains considered in this work. Left-to-right: (top) Fetch push, pick-and-place, slide, bin-pick, box-open; (bottom) Shadow hand block,egg, pen. For Fetch, the goal is to move the gray object to the red dot by controlling the arm. ForShadow hand, the objective is to reorient the object (e.g., block, egg, and pen) to the desired posesrendered by the transparent object.
Figure 11:	3-dimensional latent space works the best for 2-norm variant (see Section A.4).
Figure 12:	The two-stream architecture of the BVN allows Us to improve transfer by resetting f. Re-setting f and fine-tuning φ achieve higher success rate than the baselines when transferring betweenpush and pick & place, and to slide.
Figure 13: We show that our BVN improves the sample efficiency over the best baseline, DDPG +MLP, in 4 commonly used environments in multi-goal RL.
