Figure 1: Top row: Examples of the learned entropy surface of baseline networks. Bottom row: our ProtoMahalanobis models. Each pixel in the the background color represents the entropy given to that coordinate ininput space. Baseline networks exhibit high confidence in areas where there has been no evidence, leading tohigher calibration error when presented with OOD data.
Figure 2: Comparison between covariances learned in SNGP (Liu et al., 2020a), DDU (Mukhoti et al., 2021) andProto Mahalanobis (Ours) in the few shot setting (half-moons 2-way/5-shot). Covariance generated from SNGPare close to a multiple of the identity matrix, while that of ProtoMahalanobis contains significant contributionsfrom off-diagonal elements.
Figure 4: ECE results for all models on different variants of the Omniglot dataset. ProtoMahalanobis modelsshow comparable in distribution ECE while significantly improving ECE over the baselines on corruptedinstances from the dataset.
Figure 5: ECE for different variants of the MiniImageNet dataset. ProtoMahalanobis models show improvedECE on corrupted data instances while maintaining comparable performance on in-distribution data.
Figure 6: Precision matrix eigenvalue distribution for various meta learning model variants. A diversedistribution of eigenvalues which varies by class, indicates a class specific, non-spherical Gaussian distributionis learned. Data comes from Omniglot 5-way/5-shot experiments.
Figure 7: Random task samples from the Meta Moons dataset.
Figure 9: Random task samples from the Meta Gaussians dataset.
Figure 10:	ProtoMahalanobisFC model performance on the meta-moons and meta-circles toy datasets.
Figure 11:	ProtoMahalanobisFC model performance on the meta Gaussians toy dataset. From the topleft: Entropy surface, covariances for clases 1-10Accuracy: 1.00 KNLL 0.19ENT ID/OOD: 0.46 / 0.50ECE ID/OOD 0.18/0.30O(a)	Meta Circles (DDU). From left to right: entropy surface (distance), entropy surface (SoftmaX sample),covariances for class 1-2(b)	Meta Moons (DDU). From left to right: entropy surface (distance), entropy surface (softmaX sample),covariances for class 1-2Figure 12:	Proto DDU model performance on the two toy meta learning datasets.
Figure 12:	Proto DDU model performance on the two toy meta learning datasets.
Figure 13:	Proto DDU model performance on the Meta GauSSianS dataset. From the top left: Entropysurface (distance), entropy surface (softmax sample), covariances for clases 1-10Figure 14:	Protonet model performance on the three meta-toy dataSetS. From left to right: Meta-Circles, Meta-Moons, Meta-Gaussians.
Figure 14:	Protonet model performance on the three meta-toy dataSetS. From left to right: Meta-Circles, Meta-Moons, Meta-Gaussians.
Figure 15:	SNGPProtoFC model performance on the meta-moons and meta-circles toy datasets.
Figure 16:	SNGPProtoFC model performance on the meta-Gaussians toy dataset. From the top left:Entropy surface (softmax sample), entropy surface (distance), covariances for clases 1-10A.6 Further Implementation DetailsPositive Diagonal Constraint In order to constrain the diagonal Λ of Proto Mahalanobis models(Equation 5) to be positive as mentioned in Section 3.2, we utilize a truncated sigmoid functionΛ = max(0.1, σ(z)). We truncate the values in order to avoid extreme values during the inversion.
Figure 18: Accuracy boxplots for different variations of the Omniglot datasetFigure 17: From left to right: covariance, precision, and eigenvalue distribution for ProtoMahalanobisprecision matrix on Omniglot 20-way/1-shot (left) and 20-way/5-shot (right) experiments.
Figure 17: From left to right: covariance, precision, and eigenvalue distribution for ProtoMahalanobisprecision matrix on Omniglot 20-way/1-shot (left) and 20-way/5-shot (right) experiments.
Figure 19: NLL boxplots for different variations of the Omniglot dataset22Published as a conference paper at ICLR 20220.79 0∙40.30.6AS 0.5nMethodProtoDDU	IZZI	MAMLI	I	Protonet	∣ ∣	ReptileProtonetSN	I I	OUrS (DIag)■	ProtoSNGP	匚二I	Ours (Rank-I)Shift Intensity: MiniImageNet-C (5-way∕5-shot)Figure 20: Accuracy boxplots for different variations of the MiniImageNet datasetTest	1	2	3	4	5Shift Intensity: MiniImageNet-C (5-way/l-shot)Figure 21: NLL boxplots for different variations of the MiniIMageNet dataset
Figure 20: Accuracy boxplots for different variations of the MiniImageNet datasetTest	1	2	3	4	5Shift Intensity: MiniImageNet-C (5-way/l-shot)Figure 21: NLL boxplots for different variations of the MiniIMageNet dataset23Published as a conference paper at ICLR 2022Table 9:	Convolutional architecture used for MAML/Reptile OmniglotLayersConv2d(1, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUAveragePool(2)FC(64, nway)Table 10:	Convolutional architecture used for MAML/Reptile MiniImageNet. Reptile uses 64 filtersinstead of 32.
Figure 21: NLL boxplots for different variations of the MiniIMageNet dataset23Published as a conference paper at ICLR 2022Table 9:	Convolutional architecture used for MAML/Reptile OmniglotLayersConv2d(1, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUConv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLUAveragePool(2)FC(64, nway)Table 10:	Convolutional architecture used for MAML/Reptile MiniImageNet. Reptile uses 64 filtersinstead of 32.
Figure 22: Extra results comparing to Deep Kernel Transfer (Patacchiola et al., 2020) on the Omniglotdataset. In our experiments, DKT showed a large variance in performance between tasks. In the5-way/1-shot case, calibration on corrupted data comes at the expense of underconfidence on indistribution data.
