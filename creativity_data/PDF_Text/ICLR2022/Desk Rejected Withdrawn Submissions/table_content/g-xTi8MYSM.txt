Table 1: comparative experimental resultsTasks	Ant-v2	HaIfCheetah-v2	Walker-v2Original TD3	2375.99	5102.00	2319.36Behavior Cloning	3960.82	5274.89	1943.60DDPGfD	4194.04	-115.85	4213.81TD3fG	4704.44	7601.85	4065.45DDPGfD varies considerably between tasks. From the experiments we can see that, especially for Walker2d,using DDPGfD even degrades the final performance. This may be influenced by the quality and quantityof the demonstrations, with sub-optimal or non-generic demonstrations continuing to negatively affect thetraining process, even if these experiences have good reward value.
Table 2: Comparison of TD3fG and Q filterTasks	Ant-v2	HalfCheetah-v2	Walker-v2TD3fG	4704.44	7601.85	4065.45Q filter	1809.60	1631.40	3155.78Figure 3: Comparison of TD3fG and Q filterB.	Reply BufferFigure 5 shows the difference between before and after using reply buffer. In Ant-v2 and Walker2d tasks,the effect of the reply buffer is negligible and in HalfCheetah-v2 the total reawrd is between DDPGfDand TD3fG. It coincides with the results of previous experiments. A major difference between Td3fG andDDPGfD is the dependence on demonstrations, while TD3fG with demonstrations in reply buffer is some-where in between in terms of sensitivity to demonstrations quality. So for the first two tasks the performanceis comparable, and in the third task the total reward lies clearly between the two methodsTable 3: Comparison of TD3fG and TD3fG with reply bufferTasks	Ant-V2 HalfCheetah-v2 Walker-v2TD3fG	4704.44	7601.85	4065.45TD3fG+ReplyBuffer 4110.68	3615.32	4047.55Figure 4: Comparison of TD3fG and Q filterC.	Action noiseIn this part we add an extra generated action noise and compare it with TD3fG that only use BC loss. Forall experiments, using reference action noise alone will boost the total reward, but not as much as just using
Table 3: Comparison of TD3fG and TD3fG with reply bufferTasks	Ant-V2 HalfCheetah-v2 Walker-v2TD3fG	4704.44	7601.85	4065.45TD3fG+ReplyBuffer 4110.68	3615.32	4047.55Figure 4: Comparison of TD3fG and Q filterC.	Action noiseIn this part we add an extra generated action noise and compare it with TD3fG that only use BC loss. Forall experiments, using reference action noise alone will boost the total reward, but not as much as just using8Under review as a conference paper at ICLR 2022TD3fG with BC loss. Using both techniques at the same time does not give better results and may even beinferior to just using BC loss.
Table 4: Comparison of TD3fG, ACtionNoisea and TD3fG+ActionNoiseTasks	Ant-v2	HalfCheetah-v2	Walker-v2TD3fG	4704.44	7601.85	4065.45ActionNoise	1824.43	5527.34	2411.00TD3fG+ActionNoise	4494.95	6913.96	3662.407	DISCUSSION AND FUTURE WORKWe propose an algorithm that uses pre-trained generators and dynamic weights in the loss function to over-come the detrimental effects of suboptimal, non-generalized demonstrations on the reinforcement learn-ing training process. Unlike behavioural cloning, which is only used for pre-training, and algorithms likeDDPGfD, which use demonstrations for the entire training process, our approach achieves a smooth transi-tion from imitation learning to reinforcement learning, and performs well in simulation experiments. Ourmethod is more general and efficient, and can be applied on any continuous control task.
Table 5: Hyper-parameters of TD3Parameters	valueWidth of hidden layers	(256,512, 256)Number of hidden layers	3Optimizer	adamLearning rate	0.0001Discount factor γ	0.999Mini-batch size	256Actor activation function	tanhCritic activation function	reluReply Buffer Size	50000Number of demonstrations in reply buffer	10000OU Process θ	0.15OU Process σ	0.2OU Process μ	0.0Soft Update τ	0.001Policy noise	0.2*max acitonExplore noise	(-0.5,0.5)Policy decay	1/600000 (linear decay)Table 6: Hyper-parameters of DDPGfD
Table 6: Hyper-parameters of DDPGfD12Under review as a conference paper at ICLR 2022Parameters	valueWidth of hidden layers	(256, 512, 256)Number of hidden layers	3Optimizer	adamLearning rate	0.0001Discount factor γ	0.999Mini-batch size	256Actor activation function	tanhCritic activation function	reluReply Buffer Size	50000OU Process θ	0.15OU Process σ	0.2OU Process μ	0.0Soft Update τ	0.001Actor delay frequency	2Policy noise	0.2*max acitonExplore noise	(-0.5,0.5)
Table 7: Hyper-parameters of TD3Parameters	valueWidth of hidden layers	(256, 512, 256)Number of hidden layers	3Optimizer	adamLearning rate	0.0001Discount factor γ	0.999Batch size	64 (episodes)Randomly sampled Steps per Episode	100Actor activation function	tanhNumber of Demonstrations	100Training steps	100000Table 8: Hyper-parameters of TD3B Demonstrations informationTasks	Max total reward	Min total reward	average total reward	StdAnt-v2	5487.11	-685.92	4635.61	4635.61Walker2d-v2	4984.58	4833.33	4926.60	33.32HalfCheetah-v2	11035.38	10013.34	10663.37	209.35Table 9: Hyper-parameters of TD313
Table 8: Hyper-parameters of TD3B Demonstrations informationTasks	Max total reward	Min total reward	average total reward	StdAnt-v2	5487.11	-685.92	4635.61	4635.61Walker2d-v2	4984.58	4833.33	4926.60	33.32HalfCheetah-v2	11035.38	10013.34	10663.37	209.35Table 9: Hyper-parameters of TD313Under review as a conference paper at ICLR 2022C EXPERIMENT RESULTS OF DIFFERENT SEEDSWe randomly choose seeds 1, 5, 72, 141,252 to perform the experiment. The results of the experimentswhere the seed is 1 have been shown in the text.
Table 9: Hyper-parameters of TD313Under review as a conference paper at ICLR 2022C EXPERIMENT RESULTS OF DIFFERENT SEEDSWe randomly choose seeds 1, 5, 72, 141,252 to perform the experiment. The results of the experimentswhere the seed is 1 have been shown in the text.
