Table 1: Performance comparison on CIFAR-10/100 datasets under various noise level. Test accuracy(%) with 95% confidence interval of 5-runs is provided.
Table 2: Test accuracy (%) comparison on Clothing1M dataset with real-world label noise. Theresults except L2RW Ren et al. (2018) are taken from original papers.
Table 3: Label correction performance comparison on CIFAR-10 with symmetric 80% noise. Ac-curacy (%) and Negative Log Likelihood (NLL) loss are calculated using the true labels before thesynthetic noise is injected. Performance of the trained model on all training samples (Overall) andincorrectly labeled training samples (Incorrect) is measured. f denotes performance extracted fromthe meta model.
Table 4: Data split composition of dataset used in our experiments.
Table 5: Training time comparison on CIFAR-10 dataset with 80% symmetric noise. Time (seconds)per iteration and Time (hours) per total training on a single RTX 2080Ti GPU are provided with therelative ratio compared to our method.
Table 6: The effect of clean set oversampling on the performance of CIFAR-10/100 experiments.
Table 7: The effect of two-head architecture via oracle label transition matrix on CIFAR-10/100dataset. Test accuracy (%) of GLC Hendrycks et al. (2018) and LT2L (ours.) with and without oraclelabel transition matrix is provided. For a fair comparison, label corruption is excluded in LT2L.
Table 8: Test accuracy (%) comparison between LT2L without Label Correction and MLoC Wanget al.(2020a) on CIFAR-10/100 dataset.________________________________________Dataset	Method	Symmetric				Asymmetric			20%	40 %	60 %	80 %	20%	40 %CIFAR-10	MLoC Wang et al.(2020a)	90.50	87.20	81.95	54.64	91.15	89.35	LT2L (ours.) w/o Label Correction	91.37	88.71	83.97	74.91	91.80	91.10CIFAR-100	MLoC Wang et al.(2020a)	68.16	62.09	54.49	20.23	69.20	66.48	LT2L (ours.) w/o Label Correction	68.02	61.75	52.79	28.46	69.59	66.07which learn the transition matrix. Label correction is excluded for our LT2L as it may produceunfair comparisons. Figure 7 shows the difference between the probability distribution of the oracletransition matrix and the estimated transition matrix for each iteration, where Pearson χ2-divergenceis used to measure the discrepancy between the two matrices. Since GLC estimates the transitionmatrix only once in the entire learning process, it represents a fixed value unrelated to iteration. Theplot for the MLoC error seems to be constant, but it is, in fact, slowly decreasing. It implies thatMLoC is likely to be highly dependent on the initialization of the transition matrix T . Although ourLT2L does not require pre-training and uses only clean samples inside a single mini-batch, it showsfast convergence with a similar estimation error to GLC.
Table 9: Incorrect label detection performance comparison on CIFAR-10 with symmetric 80%noise. The Area Under the Receiver Operating Characteristic (AUROC) and The Area Under thePrecision-Recall Curve (AUPRC) are provided. Note that pure random model will yield 0.5 AUROCand 0.72 AUPRC. f denotes the performance of the sample weights obtained with the meta model.
Table 10: The effect of the number ofclean set on CIFAR-10 with symmetric 80%noise. Comparison with other label correc-tion methods with meta-learning is provided.
Table 11: The effect of the number of samples perclass (K) in the mini-batch on the predictive perfor-mance (Accuracy (%)) of CIFAR-10 experiments.
Table 12: Evaluation results varying the hyperparameter λ. Test accuracy (%) with 95% confidenceinterval of 5-runs is provided.
