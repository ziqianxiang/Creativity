Table 1: Detailed difference between our DAAS and two closely related concurrent works.
Table 2: Comparison of the state-of-the-art models on CIFAR-10 (left) and CIFAR-100 (right). Wereport the best (in the first block) and averaged (in the second block) performance over four paralleltests by searching under different random seeds. ? : The results are obtained by training the bestarchitecture for multiple times rather than searching for multiple times. 1: Results ofjoint searchingfor AA and NAS reported in DHA (Zhou et al., 2021).
Table 3: Comparison on ImageNet. The first block reports the performance of models transferredfrom CIFAR, and the second block reports performance of models directly searched on ImageNet.
Table 4: Performance on CIFAR-10 of four ar-chitectures trained with default augmentationpolicies in prior NAS works and the policiesdiscovered in our search space.
Table 5: Ablation study on AA algorithm. We compare with other AA methods on CIFAR-10 andCIFAR-100 with various classic networks. Our results are averaged over three parallel tests. 1: Theresults are obtained from Cubuk et al. (2019); Lim et al. (2019). ‘-’: The results are not provided bythe prior work. WRN and SS are the shorthand of Wide-ResNet and Shake-Shake, respectively.
Table 6: Comparison of joint and indepen-dent searching for NAS and AA on CIFAR-10.
Table 7: We randomly mix up the architectureand policies discovered by three parallel tests.
