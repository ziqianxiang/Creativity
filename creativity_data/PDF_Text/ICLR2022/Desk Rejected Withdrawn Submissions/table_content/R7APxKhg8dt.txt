Table 1: Commonsense paths generated by CoSe-Co for questions in CommonsenseQA data. Po-tential answers observed in path itself are highlighted , context-enriching concepts are coloured.
Table 2: Performance comparison on in-house dev (IHdev) and test (IHtest) split of Common-senseQA dataset (Talmor et al., 2019). All scores are averaged across 5 runs. First row depictsamount of training data used. The second-best number for each column is underlined while bestis in bold. Superscripts ‘p’ and ‘q’ denote statistically significant differences (p-value of 0.05) incomparison to two of our major baselines - PGQA and QA-GNN respectively.
Table 3: Studying the effect of ablation variants through comparison on CommonsenseQA dev set.
Table 4: Performance comparison on Hits@K and Recall@K metrics for OpenCSR (Lin et al.,2021a) on ARC, QASC and OBQA datasets. DrFact is a BERT-based current state of the art method.
Table 5: Using CoSe-Co Paths leads to improvements in MRPC paraphrase generation and nochange in CommonGen task. Generative commonsense methods like PGQA which rely on answerchoices cannot be applied in tasks like paraphrase generation where entities are not available.
Table 6: Comparison between predictions made by PGQA (Wang et al., 2020b), QA-GNN (Ya-sunaga et al., 2021), and CoSe-Co on a subset of CSQA’s in-house test set (Talmor et al., 2019).
Table 7: Examples of commonsense inferences obtained for different input forms of the same ques-tion from CoSe-Co when trained with different values of Ipmask. Potential answers which areobserved in a path are highlighted , while context-enriching concepts are coloured.
Table 8: Performance comparison between using T5-base and GPT-2 as backbone language modelfor PGQA and CoSe-Co for multi-choice QA task on CSQA dataset.
Table 9: Concatenating paths generated by CoSe-Co leads to improvements in MRPC paraphrasegeneration and CSQA (cast as generation task) without much performance change on CommonGen.
Table 10: Analysing effect of concatenating CoSe-Co paths on more tasks using BERT-base.
