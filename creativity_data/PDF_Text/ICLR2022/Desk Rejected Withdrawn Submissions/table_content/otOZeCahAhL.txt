Table 1: Acoustic Scene Classification. Top-1 validation accuracy (%) on TAU Urban AcousticScenes 2020 Mobile development dataset. (average and standard deviation; averaged over 5 seeds)Method	#Param	A	B	se C	en S1	S2	S3	S4	unseen S5	S6	Overall	∆BC-ResNet-8	315k	79.6	70.8	74.3	69.8	66.2	72.8	63.6	63.3	59.2	68.9 ± 0.8	+ 0.0+ Global FreqNorm	315k	80.2	72.2	76.2	70.8	67.5	72.6	65.4	66.0	56.2	69.7 ± 0.6	+ 0.8+ PCEN	315k	75.6	66.7	66.3	69.0	67.0	73.6	68.1	68.3	66.7	69.0 ± 0.7	+ 0.1+ Mixup	315k	79.9	70.3	72.0	69.8	65.9	70.1	60.5	60.8	56.1	67.3 ± 1.0	-1.6+ MixStyle	315k	78.5	70.0	72.0	68.4	65.9	68.3	59.0	59.3	54.6	66.2 ± 0.7	-2.7+ BIN	317k	76.9	70.2	71.4	67.3	65.6	69.6	60.4	62.2	57.6	66.8 ± 1.5	-2.1+ CSD	317k	77.5	71.4	72.8	68.7	66.8	71.0	65.0	63.5	56.7	68.2 ± 0.4	-0.7+ RFN (Ours)	315k	82.4	73.2	74.5	75.7	69.9	76.9	70.5	72.4	69.5	73.9 ± 0.7	+ 5.0λ = 0 (IFN)	315k	77.1	71.7	67.8	73.0	71.1	74.8	69.8	70.9	67.4	71.5 ± 1.2	+ 2.6λ = 1 (LN)	315k	79.8	71.9	72.9	73.6	68.9	70.7	62.2	63.2	57.2	68.9 ± 0.7	+ 0.0BC-ResNet-1	8.1k	73.3	61.3	64.9	61.0	58.3	66.7	51.8	51.3	48.5	59.7 ± 1.3	+ 0.0+ RFN (Ours)	8.1k	75.2	63.7	64.0	62.8	61.2	68.0	58.3	63.0	57.2	63.7 ± 0.9	+ 4.0CP-ResNet, c = 64	897k	78.1	71.2	73.4	68.3	65.9	68.7	64.8	64.8	58.5	68.2 ± 0.4	+ 0.0+ RFN (Ours)	897k	79.3	70.9	70.8	71.8	72.1	74.1	69.9	68.6	66.0	71.5 ± 0.3	+ 3.3Keword spotting. KWS aims to detect and classify predefined keywords. We use Google SpeechCommands dataset version 1 (Warden, 2018), which contains 64,727 utterances from 1,881 speak-ers. Aside from audio device-ID in the ASC task, speaker ID is another important domain, which can
Table 2: Keyword Spotting. Top-1 test accuracy (%) with varying number of training speakers onGoogle speech command dataset ver1. (average and standard deviation; averaged over 5 seeds)Method	50	100	200	1000cnn-trad-fpool3	72.5 ± 0.3	80.5 ± 0.5	86.9 ± 0.5	92.3 ± 0.4+ Mixup	70.5 ± 0.5	79.7 ± 0.4	87.9 ± 0.4	93.3 ± 0.4+ CrossGrad	73.7 ± 0.8	81.1 ± 0.5	87.2 ± 0.2	92.6 ± 0.2+ CSD	73.4 ± 0.5	80.9 ± 0.6	87.5 ± 0.5	92.8 ± 0.3+ RFN (Ours)	76.6 ± 0.6	82.3 ± 0.6	87.8 ± 0.6	92.4 ± 0.3λ = 0 (IFN)	71.1 ± 1.1	77.9 ± 0.7	85.3 ± 0.3	90.9 ± 0.3λ = 1 (LN)	72.7 ± 0.8	80.8 ± 0.7	86.5 ± 0.3	92.4 ± 0.2ResNet15	84.4 ± 1.1	87.9 ± 0.4	91.1 ± 0.2	94.9 ± 0.2+ RFN (Ours)	86.3 ± 1.0	90.2 ± 0.5	91.8 ± 0.4	95.6 ± 0.2Table 3: Speaker verification. EER (%) on overall genres and each genre on CN-Celeb. The num-bers are average and standard deviation (average over 5 seeds).
Table 3: Speaker verification. EER (%) on overall genres and each genre on CN-Celeb. The num-bers are average and standard deviation (average over 5 seeds).
Table 4: Compare frequency-wise ap-proaches to their channel-wise counter-parts (averaged over 5 seeds).
Table 5: Top-1 test accuracy (%)using RFN with other baselines (av-erage over 5 seeds).
Table 6: Position of RFN modules. Compare Top-1 validation accuracy (%) (or EER (%) for SV) forthe three tasks with number of multiplies. The numbers are average and standard deviation (averageover 5 seeds).
Table A1: Details of BC-ResNets with input shapes where B, c and T stand for the mini-batch size,the number of base channel, and the total time steps, respectively.
Table A2: Details of Fast-ResNet34 with input shapes where B and T stand for the mini-batch sizeand the total time steps, respectively.
Table A3: Acoustic Scene Classification using Less Scene Domains. Top-1 validation accuracy(%) on TAU Urban AcousticScenes 2020 Mobile, development dataset. We show mean and standarddeviation of scores. (averaged over 5 seeds, *: average over 10 seeds)seen	unseenMethod	#Param	A	B	S1	S2	S3	C	S4	S5	S6	Overall	△BC-ResNet-8	315k	80.5	71.4	69.3	69.7	71.2	56.6	63.3	63.7	56.5	66.9 ± 1.1	+ 0.0+ PCEN	315k	75.4	67.4	69.8	67.6	74.1	59.8	67.7	66.8	65.2	68.2 ± 0.5	+ 1.3+ Mixup	315k	79.2	67.4	69.8	64.4	69.9	56.3	61.3	60.3	53.3	64.7 ± 1.1	-2.2+ MixStyle	315k	79.9	65.7	67.9	65.7	71.0	56.2	67.0	67.0	61.8	66.9 ± 0.8	+ 0.0+ BIN	317k	76.8	68.6	66.1	64.5	70.1	53.2	62.1	62.8	56.1	64.5 ± 0.9	-2.4+ Freq-MixStyle	315k	80.9	70.1	71.3	70.9	75.6	61.8	69.0	69.6	62.0	70.1 ± 0.5	+ 3.2+ RFN (Ours)	315k	81.3	70.9	75.7	68.4	75.7	64.0	68.2	72.5	67.5	71.6 ± 0.4	+ 4.7λ = 0 (IFN)	315k	77.4	70.4	72.7	71.9	74.7	62.0	72.0	71.8	66.0	71.0 ± 0.6	+ 4.1λ = 1 (LN)	315k	80.7	71.4	72.8	68.0	71.7	53.3	59.4	62.7	55.8	66.2 ± 0.7	-0.7			seen					unseen				Method	#Param	A	S1	S2	S3	B	C	S4	S5	S6	Overall	△BC-ResNet-8	315k	81.2	70.4	67.1	70.8	41.4	54.2	62.7	64.4	54.8	63.0 ± 0.7	+ 0.0+ RFN (Ours)	315k	80.4	74.4	69.6	74.5	49.2	62.0	70.1	72.7	67.2	68.9 ± 0.7	+ 5.9Table A4: More Results in Acoustic Scene Classification. Top-1 validation accuracy (%) on TAUUrban AcousticScenes 2020 Mobile, development dataset. We show mean and standard deviation of
Table A4: More Results in Acoustic Scene Classification. Top-1 validation accuracy (%) on TAUUrban AcousticScenes 2020 Mobile, development dataset. We show mean and standard deviation ofscores. (averaged over 5 seeds)Method	#Param	A	B	se C	en S1	S2	S3	S4	unseen S5	S6	Overall	∆BC-ResNet-1	8.1k	73.3	61.3	64.9	61.0	58.3	66.7	51.8	51.3	48.5	59.7 ± 1.3	+ 0.0+ Global FreqNorm	8.1k	72.2	59.5	62.7	59.3	56.8	63.9	49.3	51.1	45.2	57.8 ± 1.2	-1.9+ PCEN	8.1k	68.4	55.5	58.9	60.1	57.6	63.9	59.3	61.9	56.8	60.3 ± 1.1	+ 0.6+ Mixup	8.1k	72.4	61.1	63.2	58.5	56.9	63.7	49.5	51.6	44.5	57.9 ± 1.6	-1.8+ MixStyle*	8.1k	71.6	54.8	58.8	53.1	53.5	57.6	43.0	46.0	39.0	53.0 ± 1.5	-6.7+ BIN*	8.3k	74.7	60.6	64.9	59.3	59.9	65.9	54.6	53.3	49.5	60.3 ± 2.0	+ 0.6+ CSD*	8.4k	74.6	60.7	64.7	59.9	57.4	66.9	50.8	51.5	46.6	59.2 ± 1.2	-0.5+ Freq-MixStyle*	8.1k	71.7	59.5	59.1	58.3	57.5	63.9	58.9	60.1	52.0	60.1 ± 1.0	+ 0.4+ BIFN*	8.3k	74.8	62.2	63.1	59.9	58.0	65.1	54.1	55.1	50.4	60.3 ± 1.3	+ 0.6+ RFN (Ours)	8.1k	75.2	63.7	64.0	62.8	61.2	68.0	58.3	63.0	57.2	63.7 ± 0.9	+ 4.0λ = 0 (IFN)	8.1k	67.6	61.5	57.7	63.6	64.1	65.3	63.3	61.6	56.8	62.4 ± 0.5	+ 2.7λ = 1 (LN)	8.1k	73.8	60.2	61.7	61.0	59.4	65.2	51.3	53.5	45.9	59.1 ± 1.3	-0.6Relaxed-IN	8.1k	73.3	58.9	59.0	59.2	59.3	62.5	52.1	51.6	44.5	57.8 ± 0.8	-1.9CP-ResNet, c = 64	897k	78.1	71.2	73.4	68.3	65.9	68.7	64.8	64.8	58.5	68.2 ± 0.4	+ 0.0+ RFN using BN	897k	80.4	72.8	73.6	73.8	71.5	75.1	72.4	70.7	70.2	73.4 ± 0.3	+ 5.2λ = 1.0 (BN)	897k	77.3	72.9	73.9	68.6	66.1	68.4	65.3	63.9	57.8	68.2 ± 0.3	+ 0.0
Table A5: Keyword Spotting. Compare PCEN to others using log-Mel spectrogram insteadof MFCCs. Top-1 test accuracy (%) with varying number of training speakers on Google speechcommand dataset ver1. (average and standard deviation; averaged over 5 seeds)Method	I	50 I	100	I 200	I 1000cnn-trad-fpool3	75.2 ± 0.3	79.8 ± 0.4	86.0 ± 0.3	92.0 ± 0.4+ PCEN	81.6 ± 0.4	85.6 ± 0.5	89.5 ± 0.2	94.0 ± 0.1+ RFN(OurS) ∣	82.6 ± 0.3 I	86.3 ± 0.5	I 90.8 ± 0.2	I 94.2 ± 0.2stage 3, and stage4, respectively. The linear interpolated λ results in 0.9% lower validation accuracycompared to that of fixed λ = 0.5. Second, we try a naive SGD update of λ, but it results in poorperformance. Further, we tried the Meta-Learning Domain Generalization approach (MLDG) (Liet al., 2018). We use the same optimizer and learning rate schedule as the baseline and use α = γand β = 1 for MLDG and use the first-order approximation of MAML (Finn et al., 2017). Wetrained the network parameters by conventional SGD training and updated λ by meta-test loss inthe MLDG scenario. The approach got λ of [0.7, 0.5, 0.3, 0.5, 0.1] for input and after each stage,respectively, on average over five seeds and results in 72.2% accuracy, which is better than naiveSGD but still worse than the fixed λ = 0.5. We leave the automatic update of λ as future work.
Table A6: Decision rules for λ. Compare Top-1 validation accuracy (%) of BC-ResNet-ASC-8 onTAU Urban AcousticScenes 2020 Mobile, development dataset (average over 5 seeds).
