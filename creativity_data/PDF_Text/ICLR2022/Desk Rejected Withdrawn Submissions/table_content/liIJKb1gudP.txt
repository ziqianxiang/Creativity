Table 1: Average test accuracy (in %) (mean and std) after training on all tasks for each methodMethod	Rotated MNIST		Permuted MNIST		ACC	BWT	ACC	BWTNaive	57.4 ± 1.2	-30.2 ± 1.5	49.0 ± 1.9	-47.3 ± 2.2LwF	74.4 ± 0.9	-12.1 ± 1.3	63.7 ± 0.8	-12.7 ± 1.1EWC	73.6 ± 0.7	-16.5 ±0.8	68.2 ± 0.4	-19.5 ± 0.3SI	74.4 ± 0.3	-15.6 ±0.4	67.5 ± 1.0	-20.0 ± 1.2LFL	75.5 ± 0.5	-16.2 ± 1.4	71.9 ± 0.8	-14.4 ± 1.4MAS	67.5 ± 1.3	-12.1 ± 1.5	71.8 ± 1.2	-13.9 ± 1.6UCL	68.2 ± 1.6	-23.6 ± 1.2	66.5 ± 1.4	-26.6 ± 1.5VCL	63.6 ± 1.9	-28.9 ± 1.3	74.6 ± 0.7	-14.8 ± 1.7CLR (Ours)	76.4 ± 1.2	-11.9 ± 0.6	75.6 ± 1.3	-12.2 ± 0.90.950.90O.Θ5>∙0.β0ra5 0∙75u<
Table 2: Replay Experiments Study: Average test accuracy (mean % and std) with varying memory size (Memsize) i.e number of replay examples stored for each taskMethod		Rotated MNIST				Permuted MNIST		Mem size	10	20	50	100	10	20	50	100ER	78.6 ± 1.4	81.0 ± 0.5	83.3 ± 0.4	85.1 ± 0.4	68.9 ± 1.4	71.7 ± 1.7	80.6 ± 0.4	83.5 ± 0.6A-GEM	77.6 ± 0.4	78.5 ± 0.5	79.3 ± 0.4	79.7 ± 0.6	76.8 ± 0.4	77.7 ± 0.5	84.3 ± 0.7	83.9 ± 0.4GDumb	78.5 ± 0.3	81.1 ± 0.8	84.5 ± 0.4	85.2 ± 0.5	76.7 ± 0.8	78.5 ± 0.9	84.3 ± 1.0	85.3 ± 0.7ER + CLR	81.0 ± 0.4	83.3 ± 0.2	85.0 ± 0.2	86.6 ± 0.1	78.9 ± 0.5	80.0 ± 0.9	83.7 ± 0.2	86.0 ± 0.4Table 3: Ablation Study: Average test accuracy (mean % and std)Frozen		Rotated MNIST		Permuted MNIST	Centers	Classifier	ACC	BWT	ACC	BWT✓	✓	76.4 ± 1.2	-11.9 ± 0.6	75.6 ± 1.3	-12.2 ± 0.9✓	X	76.3 ± 1.2	-15.0 ± 1.0	73.2 ± 0.7	-15.7 ± 0.8X	✓	75.7 ± 0.7	-15.7 ±0.9	74.3 ± 1.2	-14.0 ± 1.6X	X	76.4 ± 0.9	-12.5 ±0.4	73.4 ± 2.4	-15.4 ± 2.5memory-wise more efficient than the LFL and LwF because the CLR does not need to store the oldmodel and forward pass it, significantly reducing the memory usage and training time.
Table 3: Ablation Study: Average test accuracy (mean % and std)Frozen		Rotated MNIST		Permuted MNIST	Centers	Classifier	ACC	BWT	ACC	BWT✓	✓	76.4 ± 1.2	-11.9 ± 0.6	75.6 ± 1.3	-12.2 ± 0.9✓	X	76.3 ± 1.2	-15.0 ± 1.0	73.2 ± 0.7	-15.7 ± 0.8X	✓	75.7 ± 0.7	-15.7 ±0.9	74.3 ± 1.2	-14.0 ± 1.6X	X	76.4 ± 0.9	-12.5 ±0.4	73.4 ± 2.4	-15.4 ± 2.5memory-wise more efficient than the LFL and LwF because the CLR does not need to store the oldmodel and forward pass it, significantly reducing the memory usage and training time.
Table 4: Continual Domain Adaptation (Digits): Test accuracy on individual datasets, average test accuracy(ACC) and BWT (mean % and std) after training on all domains with Digits protocolMethod	M.Size	MNIST (1)	MNIST-M (2)	SYN (3)	SVHN (4)	ACC	BWTNaive	-	83.4 ± 6.4	65.8 ± 3.4	71.3 ± 0.4	92.6 ± 0.1	78.2 ± 0.4	-12.8 ± 0.7LwF	-	96.1 ± 1.2	72.7 ± 0.9	76.6 ± 0.7	91.1 ±1.0	84.1 ± 1.3	-3.6 ± 1.7EWC	-	93.7 ± 0.8	67.9 ± 0.6	75.1 ± 0.3	92.5 ± 0.2	82.3 ± 0.9	-7.8 ± 0.8SI	-	91.8 ± 0.5	69.2 ± 1.2	72.5 ± 0.5	91.9 ± 1.1	81.3 ± 0.6	-8.9 ± 0.4LFL	-	95.5 ± 0.9	67.3 ± 0.4	77.7 ± 1.3	93.2 ± 0.9	83.4 ± 0.4	-7.5 ± 0.3Meta-DR	-	85.7 ± 1.8	75.4 ± 0.7	82.0 ± 1.9	98.5 ± 0.3	85.4 ± 1.1	-8.4 ± 1.0CLR	-	95.8 ± 0.6	71.9 ± 0.5	77.2 ± 0.2	93.5 ± 0.2	84.6 ± 0.9	-5.7 ± 0.6	100	96.2 ± 0.4	75.0 ± 1.1	79.3 ± 0.3	93.3 ± 0.9	86.2 ± 0.7	-4.8 ± 0.7ER	200	96.7 ± 0.6	77.5 ± 1.5	80.8 ± 0.7	93.7 ± 0.2	87.1 ± 0.4	-3.7 ± 0.8	300	97.3 ± 1.3	78.8 ± 1.2	78.6 ± 0.2	92.7 ± 0.5	86.9 ± 0.5	-3.6 ± 0.5	100	95.2 ± 0.3	77.1 ± 1.3	78.6 ± 0.5	94.0 ±0.8	85.9 ± 0.5	-5.0 ± 1.0A-GEM	200	95.8 ± 1.0	77.9 ± 0.8	79.1 ± 0.4	94.2 ±0.3	86.7 ± 0.6	-3.9 ± 0.3	300	96.9 ± 0.8	78.7 ± 0.9	80.0 ± 1.2	93.8 ± 0.6	87.3 ± 0.8	-3.5 ± 0.9	100	95.4 ± 0.8	79.6 ± 0.4	80.8 ± 0.3	92.7 ± 0.1	87.1 ± 0.4	-3.7 ± 0.3ER + Meta-DR	200	96.1 ± 0.7	80.1 ± 0.9	81.2 ± 0.5	93.0 ± 0.4	87.5 ± 0.6	-3.4 ± 0.4	300	97.2 ± 0.4	80.3 ± 0.5	82.3 ± 0.1	93.4 ± 0.3	88.3 ± 0.4	-2.8 ± 0.4	100	97.1 ± 0.2	79.0 ± 0.5	81.8 ± 1.4	94.3 ± 0.8	88.1 ± 0.6	-2.7 ± 0.3
Table 5: Continual Domain Adaptation (PACS): Test accuracy on individual datasets, average test accuracy(ACC) and BWT (mean % and std) after training on all domains for continual domain adaptation with PACSMethod	M.Size	Sketches (1)	Cartoons (2)	Paintings (3)	Photos (4)	ACC	BWTNaive	-	74.7 ± 2.1	65.6±1.6	86.7 ± 1.4	90.4 ± 0.9	77.4 ± 0.5	-15.0 ± 1.2LwF	-	79.3 ± 0.8	68.4 ± 1.6	95.8 ± 1.1	88.6 ± 0.5	81.4 ± 0.8	-11.9 ± 1.3EWC	-	81.1 ± 1.6	71.8 ± 0.7	96.2 ± 0.4	89.1 ± 0.2	83.1 ± 0.9	-8.8 ± 0.8SI	-	82.6 ± 1.1	70.0 ± 1.4	92.2 ± 1.6	90.5 ± 0.7	83.7 ± 1.2	-10.2 ± 0.8LFL	-	86.6 ± 0.7	73.5 ± 0.9	95.2 ± 1.2	90.4 ± 1.4	85.7 ± 1.1	-7.5 ± 1.0Meta-DR	-	86.8 ± 1.2	75.4 ± 0.7	95.3 ± 0.9	88.5 ± 0.3	85.9 ± 0.9	-7.2 ± 0.8CLR	-	86.7 ± 1.4	74.0 ± 0.8	97.0 ± 1.2	89.9 ± 0.7	86.1 ± 1.3	-7.9 ± 0.4	10	89.2 ± 1.2	84.3 ± 0.3	96.8 ± 0.9	91.8 ± 1.9	89.9 ± 1.5	-3.4 ± 0.6ER	20	92.3 ± 0.8	83.1 ± 1.2	96.4 ± 2.1	90.7 ± 0.7	90.1 ± 1.7	-2.0 ± 1.3	30	91.7 ± 0.5	84.8 ± 1.6	97.6 ± 2.4	91.5 ± 0.4	91.0 ± 1.2	-2.2 ± 1.1	10	87.2 ± 1.1	85.0 ± 0.8	97.1 ± 0.9	90.2 ± 1.9	89.8 ± 0.9	-4.1 ± 0.8A-GEM	20	91.8 ± 0.7	84.1 ± 0.8	95.4 ± 1.7	91.1 ±0.7	90.6 ± 1.4	-2.7 ± 1.2	30	91.2 ± 1.2	84.5 ± 0.9	97.1 ± 1.4	90.3 ± 2.2	90.7 ± 1.2	-2.4 ± 1.4	10	90.2 ± 0.4	84.3 ± 1.7	95.1 ± 1.5	90.3 ± 1.9	90.0 ± 1.5	-3.5 ± 1.9ER + Meta-DR	20	92.5 ± 1.8	82.9 ± 2.2	95.6 ± 1.2	91.2 ± 1.7	90.5 ± 0.6	-2.1 ± 1.1	30	90.7 ± 1.6	84.9 ± 0.8	97.8 ± 2.1	89.4 ± 1.4	90.7 ± 1.2	-2.6 ± 1.3	10	90.7 ± 0.5	84.1 ± 0.7	97.4 ± 1.4	91.5 ± 0.6	90.4 ± 1.1	-3.6 ± 0.7
Table 6: Comparison of additional memory requirement for all methodsProtocol/Method	LwF	EWC	SI	LFL	MAS	UCL	VCL	CLRMNIST (MLP)	89.6K	1.79M	179K	89.6K	179K	89.8K	269K	1000Digits, PACS (ResNet18)	11M	88M	22M	11M	22M	11M	33M	5120, 35849Under review as a conference paper at ICLR 2022ReferencesTameem Adel, Han Zhao, and Richard E. Turner. Continual learning with adaptive weights (claw).
Table 7: Hyperparameters specific to methods in all experimentsMethod	Rotated MNIST	Permuted MNIST	Digits	PACSEWC (λ)	0.001	0.001	0.001	1.0LwF (λ)	1.0	1.0	0.1	0.1SI (λ)	0.001	0.001	0.1	1.0LFL (λ)	1.0	0.001	0.1	1.0MAS (λ)	10.0	1.0	-	-UCL (β)	0.9	1.0	-	-CLR (λ, α)	(0.001, 0.03)	(0.04, 0.03)	(0.01, 0.01)	(0.001, 0.5)13Under review as a conference paper at ICLR 2022B Evaluation MetricsLopez-Paz & Ranzato (2017) introduced Average Accuracy (ACC) and Backward Transfer (BWT)evaluation metrics for continual learning, which we use in our experiments to evaluate our approach.
