Table 1: A frequency breakdown of the various categories and attributes in the FOCUS dataset.
Table 2: Sizes of different partitions in FOCUS. Pi is the set of images with i uncommon attributesand P A , A ⊆ {t, w, l} is the set of images where the attributes in A are uncommon. Note that P0constitutes common images.
Table 3: Generalization gap (as in Equation 1)per attribute for various models. The best gapon each attribute is in boldface.
Table 4: Top-1 accuracies of different models on various partitions of the dataset. The best accuracyon each partition is in boldface. The models perform the best on the first column corresponding tothe common images while the accuracy decreases as the number of uncommon attributes increases.
Table 5: Test accuracies of a linear classifier that predicts the presence of a location solely from thedeep features of images. It does significantly better than random implying that deep features containinformation about the location. For each class, the location attribute that is most easily identifiable isin boldface. * - have high statistical uncertainty because there are only 3 samples of “ships indoors”and 8 samples of “frogs in snow”.
Table 6: Each row shows the most similar ImageNet labels to the linear classifier for the location inthe row. The numbers in the brackets are the cosine similarities between the linear classifier and theMLP weights (biases included) of the corresponding labels.
