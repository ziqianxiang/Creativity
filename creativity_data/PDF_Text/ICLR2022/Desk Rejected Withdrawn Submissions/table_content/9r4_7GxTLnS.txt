Table 1: The effect of the task sampling schedule on co-training performance on multiple modalitiesand tasks. The highest accuracy is shown in bold, and the second-highest is underlined. Note howthe “Weighted” task sampling method consistently achieves the highest accuracy for 8 out of9 tasks,and second-highest on the remainder. Results are on the validation set.
Table 2: Co-training with PolyViT-Base. As indicated by the “#Models” column, some rows cor-respond to multiple trained models. In this case, we report the total number of parameters acrossall the models. PolyViT co-trained on a single-modality outperforms single-task baselines in mostcases, whereas PolyViT co-trained on multiple modalities achieves competitive performance with alarge reduction in parameters. Results are on the test set. Further dataset details in Appendix A.
Table 3: Linear probing of PolyViT and single-task baselines. Similar to the protocol for evaluatingself-supervised representation learning, we train only a linear classifier on top of a “frozen” trans-former encoder. Note how PolyViT co-trained on all tasks transfers well to all other datasets andmodalities. Models trained on audio do not transfer well to images and video, and vice versa. Allmodels are pretrained on ImageNet-21K, and then optionally finetuned on downstream datasets.
Table 4: State-of-the-art comparison for video classification. For ViViT, the current published state-of-the-art, we compare to numbers reported by Arnab et al. (2021) using standard dataset protocols.
Table 5: State-of-the-art comparison for audio classification. We compare to MBT, the current state-of-the-art, using the same protocols as Nagrani et al. (2021). For AudioSet, we train on the balancedsubset, AS-500k, following Nagrani et al. (2021) as described in Sec. 4.1.
Table 6: Experimental set-up: tasks and their properties. For image tasks, the indicated learningrates are obtained by a grid search over {0.03, 0.1, 0.3} on single-task baselines using the validationset accuracy. These values are used for single-task baselines and for PolyViT variants.
Table 7: Input dimensions for different modalities. Sequence length is computed as 1 +[(T∕t)×](H∕h) X (W/w) (one class token and patch tokens). Note that for shared transformerlayers, we reuse the same parameters for sequences of different lengths.
Table 8: Co-training with PolyViT, Large model configuration. Test accuracy (%) and mAP (for MiniAS, %) are reported. As indicated by the “# models” column, some rows correspond to multiple models, then the total number of parameters is computed across all models.											Model	#Models	#Params	Image					Video		Audio				C100	C10	Pets	R45 I	m1K	K400	MiT	MiniAS	VGGViT-Im21k Linear probe	1	312M	84.4	95.6	91.8	89.2	82.6	67.7	26.8	12.8	19.1Single-task baseline	9	3033M	93.3	99.2	94.8	97.3	85.1	79.6	37.1	30.0	51.8PolyViT, 1 modality	3	917M	93.9	99.4	95.5	96.9	85.1	80.6	38.8	~37.9	50.7PolyViT, Ladapt = 0	1	312M	91.4	99.0	94.7	96.8	82.6	78.9	35.8	33.3	49.9PolyViT, Ladapt = L/2	1	615M	91.1	99.1	95.0	97.0	82.8	81.0	37.7	34.1	50.4D	Linear probes: additional experimental detailsTask details. See Table 9. For linear probes, we use the same input dimensions as reported inTable 7. For image tasks, we reuse the number of train and warmup steps from the RESISC45 task(Table 6). For video and audio tasks, we used hyperparameters reported in (Arnab et al., 2021) and(Nagrani et al., 2021) respectively, with the difference that we only optimize output head parametersduring training. As for the co-training setup, we use multiple-crop evaluation on video and audiotasks.
Table 9: Tasks used for linear probes. Indicated learning rate grid search is done for all models usingvalidation set performance.
Table 10: Set-up for the co-training on videos. Train steps and warmup steps are summed to getthe number of train and warmup steps during co-training as we use the “Weighted” task samplingmethod.
Table 11: Set-up for the co-training on audio. Train steps and warmup steps are summed to getthe number of train and warmup steps during co-training as we use the “Weighted” task samplingmethod.
