Table 1: Dataset statistics. Snap density is the average density of the snapshots, Snap size, is thesize of the snapshots (time-windows) in days. Num snap is the number of snapshots in the network.
Table 2: Dynamic link prediction mAP scores. The scores reported are the mean and standarddeviation (in parenthesis) of mAP scores averaged across four runs with different random seeds. Theheuristics are deterministic and thus they all have a standard deviation of 0. The best performancesare highlighted in bold, and the highest scoring GNN performances are underlined. Cells markedby f were not run (see Section A.4.3). Random is completely random predictions. R-embed ispredictions when the decoder is given random embeddings.
Table 3:	mAP scores of GCN and GC-LSTM compared on a parameter budget. GCN+P is the GCNmodel with the same number of learnable parameters as the GC-LSTM.
Table 4:	Continuous DGNNs with randomized edge features. ’-RE’ indicate random edge features.
Table 5: Dynamic link prediction AUC scores. Formatted identically to Table 2Models	Enron	UC	Bitcoin	Autonomous	Wikipedia	RedditRandom	0.521(0.048)	0.497(0.013)	0.487(0.008)	0.500(0.000)	0.501(0.001)	0.499(0.001)R-embed	0.443(0.084)	0.514(0.023)	0.497(0.007)	0.516(0.009)	0.505(0.003)	0.504(0.008)CN	0.713	0.734	0.638	0.996	0.749	0.844AA	0.714	0.734	0.638	0.996	0.749	0.844Jaccard	0.712	0.734	0.523	0.999	0.749	0.906Newton	0.727	0.810	0.526	0.998	0.829	0.967CCPA	0.728	0.810	0.783	0.998	0.826	0.944GCN	0.911(0.011)	0.646(0.006)	0.777(0.242)	0.454(0.127)	0.412(0.019)	0.974(0.001)GAT	0.903(0.037)	0.618(0.053)	0.861(0.019)	0.562(0.030)	0.275(0.009)	0.646(0.076)EGCN-H	0.872(0.071)	0.662(0.048)	0.765(0.018)	0.892(0.050)	0.705(0.003)	0.951(0.009)EGCN-O	0.888(0.042)	0.585(0.014)	0.712(0.069)	0.904(0.036)	0.708(0.006)	0.951(0.011)GC-LSTM	0.904(0.037)	0.555(0.004)	0.741(0.031)	0.944(0.007)	0.706(0.006)	0.968(0.001)TGAT	0.483(0.010)	0.700(0.283)	0.920(0.002)	t	0.948(0.001)	0.985(0.000)TGN	0.865(0.007)	0.947(0.001)	0.916(0.001)	t	0.965(0.002)	0.969(0.001)Table 6: Dynamic link prediction MRR scores. Formatted identically to Table 2Models	Enron	UC	Bitcoin	Autonomous	Wikipedia	RedditRandom	0.038(0.010)	0.004(0.001)	0.001(0.000)	0.001(0.000)	0.001(0.000)	0.001(0.000)R-embed	0.030(0.014)	0.004(0.001)	0.002(0.001)	0.031(0.017)	0.001(0.000)	0.216(0.017)
Table 6: Dynamic link prediction MRR scores. Formatted identically to Table 2Models	Enron	UC	Bitcoin	Autonomous	Wikipedia	RedditRandom	0.038(0.010)	0.004(0.001)	0.001(0.000)	0.001(0.000)	0.001(0.000)	0.001(0.000)R-embed	0.030(0.014)	0.004(0.001)	0.002(0.001)	0.031(0.017)	0.001(0.000)	0.216(0.017)CN	0.337	0.261	0.046	0.625	0.303	0.444AA	0.338	0.262	0.046	0.625	0.303	0.442Jaccard	0.327	0.250	0.029	0.411	0.257	0.229Newton	0.291	0.267	0.029	0.615	0.308	0.432CCPA	0.337	0.267	0.047	0.623	0.303	0.441GCN	0.404(0.027)	0.141(0.021)	0.071(0.012)	0.0184(0.003)	0.002(0.002)	0.216(0.017)GAT	0.369(0.093)	0.097(0.025)	0.013(0.002)	0.265(0.027)	0.013(0.018)	0.079(0.026)EGCN-H	0.339(0.052)	0.110(0.110)	0.062(0.015)	0.340(0.095)	0.053(0.004)	0.204(0.015)EGCN-O	0.318(0.029)	0.140(0.006)	0.075(0.016)	0.383(0.050)	0.007(0.010)	0.194(0.007)GC-LSTM	0.332(0.069)	0.162(0.004)	0.079(0.007)	0.587(0.018)	0.098(0.005)	0.344(0.007)TGAT	0.117(0.023)	0.041(0.018)	0.059(0.001)	t	0.127(0.004)	0.327(0.002)TGN	0.187(0.010)	0.100(0.021)	0.033(0.005)	t	0.095(0.004)	0.223(0.012)are reoccurring. Lack of reoccurring edges causes each snapshot to be much sparser than most ofthe other datasets.
Table 7: Overview of compared methods.
Table 8: Hyperparameters searched by the grid search.
Table 9: Hyperparameters selected by the grid searchModels	Hyperparameter	Enron	UC	Bitcoin	Autonomous	Wikipedia	RedditCN	Snapshot training window	1	-10-	10	1	5	5	Existing edge treatment	default	default	adaptive	default	default	defaultAA	Snapshot training window	1	10	10	1	5	5	Existing edge treatment	default	default	adaptive	default	default	defaultJaccard	Snapshot training window	1	10	1	expanding	5	expanding	Existing edge treatment	default	default	adaptive	default	default	defaultNewton	Snapshot training window	1	10	1	1	5	5	Existing edge treatment	default	default	adaptive	default	default	defaultCCPA	Snapshot training window	1	10	10	1	5	5	Existing edge treatment	default	default	adaptive	default	default	default	Snapshot training window	1	1	static	5	5	expandingGCN	Learning rate	0.005	0.005	0.05	0.0001	0.05	0.01	Hidden layer size	10	50	200	50	200	50	Snapshot training window	1	1	static	10	1	1GAT	Learning rate	0.001	0.005	0.001	0.005	0.005	0.01	Hidden layer size	30	50	50	50	50	100	Snapshot training window	5	10	5	5	5	5EGCN-H	Learning rate	0.01	0.01	0.01	0.01	0.01	0.01
Table 10: Layer size and total number of learnable parameters for the equal parameter budget runsin Section 4.3. The number of learnable parameters of the encoder is in parenthesis.
