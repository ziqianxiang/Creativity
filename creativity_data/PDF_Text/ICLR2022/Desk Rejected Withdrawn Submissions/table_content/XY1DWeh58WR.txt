Table 1: Word Error Rates (WERs) with the RNN-T model shownin Fig. 3a using the one-dimensional macro-block dropout ofd(par) = (4), and the two-dimensional macro-block dropout ofd(par) = (4,4). In these experiments, the dropout rate of 0.2 is usedsince the best WER in each case is obtained at this rate.
Table 2: Word Error Rates (WERs) with the RNN-T model shownin Fig. 3a using the scaling suggested by (5) and Î¹--p. The dropoutrate is 0.2 and the partition shape for the 1-dimensionalmacro-block dropout is <d(Par) = (4).
Table 3: Word Error Rates (WERs) with the RNN-T model shown in Fig. 3a using the baseline dropout and the one-dimensionalmaro-block dropout approaches. In these experiments, the dropout rate of 0.2 is used since the best WER in each case is obtained at this rate.
Table 4: Word Error Rates (WERs) with the Attention-based Encoder Decoder (AED) model shown in Fig. 3b using the baseline dropoutand the one-dimensional maro-block dropout approaches. In these experiments, the dropout rate of 0.2 is used since the best WER in eachcase is obtained at this rate.
Table 5: Word Error Rates (WERs) with the Attention-basedEncoder Decoder model shown in Fig. 3b with an improvedshallow fusion in (11) with a Transformer LM [35, 36].
