Table 1: Weighted the average results of models on full datasets	Accuracy	Precision	Recall	F1-scoreSVM	0.897172	0.861554	0.897172	0.873671Logistic Regression	0.895496	0.853599	0.895496	0.863057Naive Bayesian	0.893895	0.855825	0.893895	0.869076Decision Tree	0.897058	0.860451	0.897058	0.872033Random Forest	0.897285	0.859358	0.897285	0.870004Neural Network	0.897225	0.862095	0.897225	0.874134Average Accuracy of modelsFigure 1:	Accuracy results of models on full datasetsTable 1 and Figure 1 shows the results of six models on full datasets. Nonetheless, we notice thatall our models fail to predict ‘other’ in our target, corona result. Thus, we decide to rerun all modelsbased on a reduced dataset ('other' removed from Corona_Tesult). Below are the results.
Table 2: Weighted average results of models on reduced dataset, with ‘other’ removed fromCorona-TeSult__________________________________________________________________	Accuracy	Precision	Recall	F1-scoreSVM	0.913223	0.893833	0.913223	0.897974Logistic Regression	0.912088	0.885824	0.912088	0.885945Naive Bayesian	0.913281	0.889455	0.910903	0.894439Decision Tree	0.913281	0.893137	0.913281	0.896828Random Forest	0.913479	0.891998	0.913479	0.894882Neural Network	0.913899	0.893616	0.913899	0.896444Figure 2:	Accuracy results of models on reduced datasets, With 'other' removed from Corona_resultAS Shown in Table 2 and Figure 2, all evaluation indicatorS improve Significantly after we delete'other' from Corona_result. Therefore, we decide to use the reduced datasets in our follow-up exper-iments.
Table 3: The experiment result of Support Vector MachineI C I 0.1 I Variable ∣ - ∣As shown in Table 3, we can see the optimal value regarding parameters 'gamma','kernel' and 'C',and the result of whether it is necessary to delete the variables. Its overall performances are asfollows: 89.38%, 91.32%, 89.80%, 91.32% respectively. When dropping one variable, everyonegives an improvement in accuracy ranging from 0.03% to 0.1%, except for 'shortness of breath' thatgives a reduction of 0.04%. So we retain the variables.
Table 4: The experiment result of Logistic Regression	Best Parameters		ResultSolver	Tbfgs	accuracy	91.21%C	0.01	Drop variable	F-	-	variable	-As shown in Table 4, we can see the optimal value regarding parameters 'solver' and 'C', and theresult of whether it is necessary to delete the variables. Its overall performances are as follows:88.58%, 91.21%, 88.59%, 91.21% respectively. When dropping one variable, all variables give5Under review as a conference paper at ICLR 2022improvements except for ‘sore throat’ and ‘headache’, although the highest improvement is only0.1%. Removing 2 variables does not increase the improvement either, and the highest improvementwe get is still 0.1%. Thus, we retain the baseline model.
Table 5: The experiment result of Decision Tree	Best Parameters		ResultMax depth	^6	accuracy	91.23%Min samples leaf	17	Drop variable	F-	-	variable	-As shown in Table 5, We can see the optimal value regarding parameters 'Max_depth' and‘Min_samples_leaf', and the result of whether it is necessary to delete the variables. Its overallperformances are as follows: 89.18%, 91.23%, 89.57%, 91.23% respectively. When dropping onevariable, only when the variable ‘sore throat’ is deleted, the performance of the model does notworsen compared to the baseline model. Instead, its accuracy increases by 0.03% approximately.
Table 6: The experiment result of Neural Networks	Best Parameters		Resultactivation	relu	accuracy	91.32%alpha	0.0001	Drop variable	Fhidden layer sizes	-300	variable	-solver	lbfgs	-	-As shown in Table 6, we can see the optimal value regarding parameters ‘activation’, ‘alpha’, ‘Hid-den_layer_sizes, and 'solver'，and the result of whether it is necessary to delete the variables. Itsoverall performances are as follows: 89.48%, 91.32%, 89.89%, 91.32% respectively. In this exper-iment, the enhancement of model performance is the most significant when the variable ‘headache’is deleted. Its accuracy increases 0.05% approximately compared to the baseline model after delet-ing the variable. We notice that deleting both 'sore_throat' and 'headache' variables can bring thelargest improvement of the model. Its accuracy increases 0.04% approximately compared with thebaseline model. The extremely slight change is reasonable to be ignored in the experiment.
Table 7: The experiment result of Random Forest	Best Parameters		ResultMax depth	3	accuracy	91.35%Max features	^4	Drop variable	FNumber of estimators	50	variable	-As shown in Table 7, We can see the optimal value regarding parameters 'Max_depth’,‘Max-features, and ‘Number of estimators,, and the result of whether it is necessary to delete thevariables. Its overall performances are as follows: 89.20%, 91.35%, 89.49%, 91.35% respectively.
