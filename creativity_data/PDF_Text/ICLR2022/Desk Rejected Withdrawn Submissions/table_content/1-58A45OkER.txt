Table 1: Top-1 accuracy of state-of-the-art methods at 2 bits on the ImageNet dataset.
Table 2: Results on MobileNet-v2 at 4 bits on the ImageNet dataset.
Table 3: Inference Rate on Google TPU and MIT Eyeriss. We show the results of our approach withhardware-aware (HA) COnStrainS in this table._________________________________Method	I Accuracy ∣ Google TPU ∣ MIT EyerissResNet-50Original (He et al., 2016)	75.5	361	5.6DoReFa+PACT (Choi et al., 2018)	76.5	646(1.8 ×)	12.6 (2.3 ×)DoReFa+PACT (Choi et al., 2018)	72.2	920 (2.6 ×)	13.7 (2.5×)RDOQ+HA (Ours)	76.5	769 (2.1×)	13.9 (2.5×)RDOQ+HA (Ours)	76.2	904 (2.5×)	15.0 (2.7×)RDOQ+HA (Ours)	75.0	1254 (3.5×)	17.0 (3.0×)MobileNet-V2Original (Sandler et al., 2018)	71.1	1504	64DoReFa+PACT (Choi et al., 2018)	71.2	1698 (1.1×)	104(1.6×)DoReFa+PACT (Choi et al., 2018)	70.4	1764(1.2×)	108(1.7×)HAQ (Wang et al., 2019)	71.2	2067(1.4×)	124(1.9×)HAQ (Wang et al., 2019)	68.9	2197(1.5×)	128 (2.0 ×)RDOQ+HA (Ours)	71.3	2197(1.5×)	127 (2.0 ×)RDOQ+HA (Ours)	71.0	2207(1.5×)	128 (2.0 ×)RDOQ+HA (Ours)	70.9	2256(1.5×)	130 (2.0 ×)Table 4: Time cost to find bit allocation on deep neural networks.
Table 4: Time cost to find bit allocation on deep neural networks.
Table 5: A comparison with prior mixed-precision quantization methods.
Table 6: Architecture parameters of considered deep learning hardware platforms.
