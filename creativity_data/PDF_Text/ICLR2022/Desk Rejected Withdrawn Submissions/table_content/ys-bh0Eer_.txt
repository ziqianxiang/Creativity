Table 1: Loss when training the model to infer the contextEnvironment Name	LossCheetah-Run-v0	5.12 × 10-5Finger-Spin-v0	7.45 × 10-5Walker-Walk-v0	7.22 × 10-5Walker-Walk-v1	6.41 × 10-5B.2.1	BaselinesWe provide additional implementation related details for the baselines. For a summary of the baselines,refer Section 6.2.
Table 2: Parameter values for different modes for the Finger environments (Finger-Spin-v0) whenvarying the size of the finger across the tasks.
Table 3: Parameter values for different modes for the Cheetah environments (Cheetah-Run-v0) whenvarying the length of cheetah’s torso across the tasks.
Table 4: Parameter values for different modes for the Walker environments (Walker-Walk-v0) whenvarying the friction coefficient between the walker and the ground.
Table 5: Parameter values for different modes for the Walker environments (Walker-Walk-v1) whenvarying the length of walker’s foot.
Table 6: Values for the target velocity for the Cheetah environments (Cheetah-Run-v1).
Table 7: Range for sampling the values for the Sawyer-Peg environments (Sawyer-Peg-v0)Mode	Valuesx_range_1	(0.44, 0.45)x_range_2	(0.6, 0.61)y_range_1	(-0.08, -0.07)y_range_2	(0.07, 0.08)B.4 License1.	Mujoco: Commercial (with Trial Version) https://www.roboti.us/license.html19Under review as a conference paper at ICLR 20222.	DeepMind Suite: Apache https://github.com/deepmind/dm_control/blob/master/LICENSE3.	MTEnv: MIT License https://github.com/facebookresearch/mtenv/blob/main/LICENSE4.	Meld Codebase: https://github.com/tonyzhaozh/meld5.	PyTorch: https://github.com/pytorch/pytorch/blob/master/LICENSE6.	MTRL: MIT License https://github.com/facebookresearch/mtrl/blob/main/LICENSE7.	Hydra: MIT License https://github.com/facebookresearch/hydra/blob/master/LICENSEB.5 Hyperparameter DetailsIn this section, we provide hyper-parameter values for each of themethods in our experimental evaluation. We also describe the searchspace for each hyperparameter. In Table 9 and Table 8 , we provide
Table 8: Hyperparameter values that are common across all the methods for Cheetah-Run-v0, Finger-Spin-v0, Walker-Walk-v0 and Walker-Walk-v1 (envs with varying dynamics)Hyperparameter	Hyperparameter valuesbatch size (per task) network architecture non-linearity policy initialization exploration parameters # of samples / # of train steps per iteration policy learning rate Q function learning rate Critic update frequency optimizer beta for Adam optimizer for policy Q function learning rate beta for Adam optimizer for Q function Discount Episode length (horizon) Reward scale actor update frequency actor log stddev bounds number of layers in ac- tor/critic actor/critic hidden dimen- sion number layers in dynamics model dynamics hidden dimen- sion number of encoder layers number of filters in en- coder Replay buffer capacity Temperature Adam’s β1 Init temperature Context Length	128 feedforward network ReLU standard Gaussian run a uniform exploration policy 1500 steps 1 env step / 1 training step 3e-4 3e-4 2 Adam (0.9, 0.999) 3e-4 (0.9, 0.999) .99 500 1.0 2 [-10, 2] 2 1024 1 512 4 32 400000 0.9 0.1 520Under review as a conference paper at ICLR 2022Table 9: Hyperparameter values that are common across all the methods for Cheetah-Run-v1 andSawyer-Peg-v0 (envs with varying reward functions)Hyperparameter	Hyperparameter valuesbatch size (per task) network architecture non-linearity policy initialization exploration parameters # of samples / # of train steps per iteration policy learning rate Q function learning rate Critic update frequency optimizer beta for Adam optimizer for policy Q function learning rate beta for Adam optimizer for Q function Discount Episode length (horizon) Reward scale actor update frequency actor log stddev bounds number of layers in ac- tor/critic actor/critic hidden dimen- sion number layers in dynamics model dynamics hidden dimen- sion number of encoder layers number of filters in en- coder Replay buffer capacity Temperature Adam’s β1 Init temperature Context Length	128 feedforward network ReLU standard Gaussian run a uniform exploration policy 10000 steps 1 env step / 1 training step 3e-4 3e-4 2 Adam (0.5, 0.999) 3e-4 (0.5, 0.999) .99 40 1.0 2 [-10, 2] 2 1024 1 512 4 32 400000 0.5 0.1 5Table 10: Hyperparameter values for Context Aware Dynamics ModelHyperparameter	Hyperparameter values	Environmentβ	0.5	Cheetah-Run-v0β	0.5	Finger-Spin-v0β	0.5	Walker-Walk-v0β	0.5	Walker-Walk-v1β	0.5	Cheetah-Run-v1β	0.5	Sawyer-Peg-v0Table 11: Hyperparameter values for ZeUSHyperparameter	Hyperparameter values	Environment
Table 9: Hyperparameter values that are common across all the methods for Cheetah-Run-v1 andSawyer-Peg-v0 (envs with varying reward functions)Hyperparameter	Hyperparameter valuesbatch size (per task) network architecture non-linearity policy initialization exploration parameters # of samples / # of train steps per iteration policy learning rate Q function learning rate Critic update frequency optimizer beta for Adam optimizer for policy Q function learning rate beta for Adam optimizer for Q function Discount Episode length (horizon) Reward scale actor update frequency actor log stddev bounds number of layers in ac- tor/critic actor/critic hidden dimen- sion number layers in dynamics model dynamics hidden dimen- sion number of encoder layers number of filters in en- coder Replay buffer capacity Temperature Adam’s β1 Init temperature Context Length	128 feedforward network ReLU standard Gaussian run a uniform exploration policy 10000 steps 1 env step / 1 training step 3e-4 3e-4 2 Adam (0.5, 0.999) 3e-4 (0.5, 0.999) .99 40 1.0 2 [-10, 2] 2 1024 1 512 4 32 400000 0.5 0.1 5Table 10: Hyperparameter values for Context Aware Dynamics ModelHyperparameter	Hyperparameter values	Environmentβ	0.5	Cheetah-Run-v0β	0.5	Finger-Spin-v0β	0.5	Walker-Walk-v0β	0.5	Walker-Walk-v1β	0.5	Cheetah-Run-v1β	0.5	Sawyer-Peg-v0Table 11: Hyperparameter values for ZeUSHyperparameter	Hyperparameter values	Environmentαψ αψ αψ αψ αψ αψ	1.0	Cheetah-RUn-v0 0.5	Finger-Spin-v0 2.0	Walker-Walk-v0 0.1	Walker-Walk-v1 1.0	Cheetah-RUn-v1 1.0	Sawyer-Peg-v021Under review as a conference paper at ICLR 2022C Additional ResultsWe present additional results not in the main paper.
Table 10: Hyperparameter values for Context Aware Dynamics ModelHyperparameter	Hyperparameter values	Environmentβ	0.5	Cheetah-Run-v0β	0.5	Finger-Spin-v0β	0.5	Walker-Walk-v0β	0.5	Walker-Walk-v1β	0.5	Cheetah-Run-v1β	0.5	Sawyer-Peg-v0Table 11: Hyperparameter values for ZeUSHyperparameter	Hyperparameter values	Environmentαψ αψ αψ αψ αψ αψ	1.0	Cheetah-RUn-v0 0.5	Finger-Spin-v0 2.0	Walker-Walk-v0 0.1	Walker-Walk-v1 1.0	Cheetah-RUn-v1 1.0	Sawyer-Peg-v021Under review as a conference paper at ICLR 2022C Additional ResultsWe present additional results not in the main paper.
Table 11: Hyperparameter values for ZeUSHyperparameter	Hyperparameter values	Environmentαψ αψ αψ αψ αψ αψ	1.0	Cheetah-RUn-v0 0.5	Finger-Spin-v0 2.0	Walker-Walk-v0 0.1	Walker-Walk-v1 1.0	Cheetah-RUn-v1 1.0	Sawyer-Peg-v021Under review as a conference paper at ICLR 2022C Additional ResultsWe present additional results not in the main paper.
