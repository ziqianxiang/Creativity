Table 1: Summary of out-of-distribution and misclassified in-distribution results, by difference tothe top performing method in each architecture × dataset setting. Values are difference in AUROCand average ± standard deviation is shown over all architecture × dataset settings. Higher is better.
Table 2: Results for models trained on CIFAR10 on out-of-distribution detection vs CI-FAR100/SVHN. AUROC shown, higher is better. For equivalent table on CIFAR100, see table 5.
Table 3: Results for models trained on CIFAR10. Predicting misclassification on in-distribution test.
Table 4: Test accuracy of original models.
Table 5: Model trained on CIFAR100. Out-of-distribution detection (AUROC) vs CIFAR10/SVHN.
Table 6: Model trained on CIFAR100. Predicting misclassification on in-distribution test (AUCEAand AUROC).
Table 7: Hyperparameters used for subfunction error. Brackets denote number of seeds.
