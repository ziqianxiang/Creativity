Table 1: Performance of different models on GLUE tasks. BERT § indicates the performance re-ported in Wu et al. (2021); The “dict in PT/FT” indicates whether to use the dictionary duringpre-training/fine-tuning. Each configuration is run five times with different random seeds, and theaverage of these five results on the validation set is reported in the table. We note that our code isimplemented on Huggingface Transformer (Wolf et al., 2020). The performance of our implementedBERT is consistent with the official performance, but it is slightly lower than the performance re-ported by Wu et al. (2021). Since no open-source code is released by BERT-TNF (Wu et al., 2021),we reported both their performance (BERT §) and our implemented performance. We computed therelative improvement (∆) of BERT-TNF and Dict-BERT compared with the original BERT.
Table 2: Performance of different models on eight specialized domain datasets under the domainadaptive pre-training (DAPT) setting. Each configuration is run five times with different randomseeds, and the average of these five results on the test set is calculated as the final performance.
Table 3: Performance of different models on WNLaMPro test set, subdivided by word frequency.
Table 4: Hyperparameters for BERT pre-training and domain-adaptive pre-training (DAPT).
