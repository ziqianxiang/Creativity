Table 1: Empirical evaluation of post-hoc calibration methods on CIFAR100 (Krizhevsky, 2009) forthe five metrics: ECE, negative log-likelihood (NLL), Brier score, accuracy (ACC), and AUC.
Table 2: This table lists the p-values using the non-parametric Wilcoxon signed-rank test for eachpost-hoc method compared against probe scaling. In all cases, statistics are in favor of probe scaling.
Table 3: Empirical evaluation of post-hoc calibration methods on CIFAR10 (Krizhevsky, 2009) forthe five metrics: ECE, negative log-likelihood (NLL), Brier score, accuracy (ACC) and AUC.
Table 4: Empirical evaluation of post-hoc calibration methods on SVHN (Netzer et al., 2011) forthe five metrics: ECE, negative log-likelihood (NLL), Brier score, accuracy (ACC) and AUC.
Table 5: Empirical evaluation of post-hoc calibration methods on Fashion MNIST (Xiao et al., 2017)for the five metrics: ECE, negative log-likelihood (NLL), Brier score, accuracy (ACC) and AUC.
Table 6: Empirical evaluation on ResNet50 trained on ImageNet ILSVRC2012 (Deng et al., 2009)and ImageNet-C (Hendrycks & Dietterich, 2019). Here, the splines-based method is excluded fromthe comparison because it failed to perform better than the baseline.
Table 7: Empirical evaluation on Wide-ResNet-28-10 with Monte Carlo dropout. Probe scaling cancomplement other techniques to improve calibration further.
