Table 1: MRR scores for query by humming, with K = 25Model	Search TimeNormal	Wav2Vec Tok DTW	3.48Wav2Vec Tok ED	-02-TokB DTW	~3.5-TokB ED	0.32Compressed	Wav2Vec Tok DTW	0.4Wav2Vec Tok ED	0.02TokB DTW	-06-TokB ED	0.04Table 2: Average SearchTime (in s) per queryEffect ofLSK. We study the effect of including LSK to the loss function by setting different valuesof α and γ in Eq. 12. We observe an overall performance drop with inclusion of LSK, as shown inTable 3.
Table 2: Average SearchTime (in s) per queryEffect ofLSK. We study the effect of including LSK to the loss function by setting different valuesof α and γ in Eq. 12. We observe an overall performance drop with inclusion of LSK, as shown inTable 3.
Table 3: MRR scores for query by humming, with K = 25Variation in number of Tokens. The effect of varying the size of alphabet A is shown in Table 4.
Table 4: Effect of varying K : MRR scores for query by hummingModels	Normal				Compressed				V	TS	PS	TS+PS	V	TS	PS	TS+PSTokB DTW	0.84	0.8	0.825	0.753	0.722	0.64	0.68	0.57TokB+Trans DTW	0.79	0.723	0.765	0.683	0.564	0.434	0.5	-04-TokB+NoSim DTW	0.83	0.75	0.79	-07-	0.68	0.57	0.63	-05-TokB ED	0.768	0.741	0.75	0.77	0.63	0.54	0.54	0.48-TokB+Trans ED	0.72	0.6	0.67	-057-	0.43	0.3	0.373	0.28TokB+NoSimED	0.724	0.6	0.674	0.53	0.554	0.44	0.473	0.404Table 5: Ablation Analysis: MRR scores for query by hummingTransformer as Encoder. We use a Transformer with 8.5 million parameters instead of a 3.6million parameter BiLSTM as our encoder. We call this model as TokB+Trans. There is an overalldrop in performance and robustness of the tokens generated by TokB+Trans incomparison to TokB(Table 5). Possible causes for such drop could be overfitting by the larger transformer encoder orthe number of attention heads (3, here). Decreasing the number of layers and increasing the numberof attention heads might bring in some improvement in performance.
Table 5: Ablation Analysis: MRR scores for query by hummingTransformer as Encoder. We use a Transformer with 8.5 million parameters instead of a 3.6million parameter BiLSTM as our encoder. We call this model as TokB+Trans. There is an overalldrop in performance and robustness of the tokens generated by TokB+Trans incomparison to TokB(Table 5). Possible causes for such drop could be overfitting by the larger transformer encoder orthe number of attention heads (3, here). Decreasing the number of layers and increasing the numberof attention heads might bring in some improvement in performance.
Table 6: TIMIT Similarity Search Results(MRR Scores)Models	Accuracy	Precision	Recall	F1 Score	TimeSTFT DTW	-048-	-066-	0.48	-051-	0.11Triplet DTW	-047-	-061-	0.47	-049-	0.315MIPS DTW	-035	-0:68-	0.55	-0:58-	0.383TokA DTW	-052-	-067-	0.52	-055-	0.352TokB DTW	-057-	-0:69-	0.57	-06	0.372TokAED-	-055-	-065-	0.55	-057-	0.204TokB ED 一	0.63	0.68	0.63	0.62	0.041Table 7: TIMIT Keyword Spotting Results4.6	Keyword SpottingTask. Given a query audio containing an uttered keyword, spot the location of the keyword in adataset of long speech recordings. It is considered a hit if the spotted segment in a long audio has anon-zero overlap with the ground truth segment of the keyword.
Table 7: TIMIT Keyword Spotting Results4.6	Keyword SpottingTask. Given a query audio containing an uttered keyword, spot the location of the keyword in adataset of long speech recordings. It is considered a hit if the spotted segment in a long audio has anon-zero overlap with the ground truth segment of the keyword.
