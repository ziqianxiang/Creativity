Table 1: Correlation between sharpness measure and generalization performance of ResNet18 (He et al., 2016)on CIFAR-10 (Krizhevsky, 2009) with the SGD optimizer. In the absence of weight decay (no weight decay forentire network parameter), the sharpness measures tend to lower for better generalization performance. How-ever, in the presence of weight decay (weight decay on entire network parameters), the tendency is reversed.
Table 2: Effect of β decay on C of each sub-structure ofResNet18. We use CIFAR-10 dataset with 8k batch andLAMB optimizer.
Table 3: Pitfalls of no bias decay heuristic. Experiments are on ResNet18 for CIFAR-10/100 datasets andResNet50 for ImageNet, respectively. In the training of all three datasets, the LAMB optimizer is used. Hyphen(-) is used to indicate the training failure. We found that β decay can effectively improve the generalizationperformance on all three datasets. On small datasets (CIFAR-10/100), all the other options are better than nobias decay heuristic.
Table 4: Test accuracy of adaptive β decay. We use ResNet18 on CIFAR-10/100 datasets with the LAMBoptimizer. First row indicates the batch size. We found that the fixed β decay and adaptive β decay are betterthan no bias decay heuristic (baselines). Furthermore, the adaptive β decay nearly restores the performances onsmall batches.
Table 5: Test accuracy of of adaptive β decay. We use ResNet50-D (He et al., 2019b) on ImageNet dataset withthe LAMB optimizer. We found that fixed β decay is better than no bias decay and adaptive β decay is betterthan fixed β decay. Interestingly, the adaptive β decay is superior to the performance on small batch (1k).
