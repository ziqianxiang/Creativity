Table 1: By incorporating the proposed method, the performance gap between Hyperbolic NeuralNetwork (HNN) and Euclidean Neural Network (ENN) can be greatly closed on all experimentedbenchmarks. Top-1 accuracies on standard image classification datasets are compared here. Top-1 accuracy gains to the vanilla HNNs (Ganea et al., 2018; Khrulkov et al., 2020) are shown in thelast row.
Table 2: Hyperbolic embeddings provide a better alternative to Euclidean embeddings on few-shotlearning task, and further improvements can be obtained through the proposed feature clippingmethod. Here are comparisons of few-shot classification results on fine-grained CUB dataseton 1-shot 5-way and 5-shot 5-way tasks. All accuracies are reported with 95% confidence intervals.
Table 3: Few-shot classification results on miniImageNet on 1-shot 5-way and 5-shot 5-way tasks.
Table 4: The results of out-of-distribution detection on CIFAR10 with softmax scoreNetwork	Euclidean Neural Network	Hyperbolic Neural NetworkOOD Dataser^^^			FPR95 J	AUROC ↑	AUPR ↑	FPR95 J	AUROC ↑	AUPR ↑ISUN	46.30 ± 0.78	91.50 ± 0.16	98.16 ± 0.05	45.28 ±0.65	91.61 ± 0.21	98.09 ± 0.06Place365	51.09 ±0.92	87.56 ±0.37	96.76± 0.15	54.77 ±0.76	86.82 ± 0.41	96.17 ±0.20Texture	65.04± 0.91	82.80 ± 0.35	94.59 ±0.20	47.12± 0.62	89.91± 0.20	97.39± 0.09SVHN	71.66 ±0.84	86.58± 0.21	97.06± 0.06	49.89± 1.03	91.34± 0.22	98.13± 0.06LSUN-Crop	22.22± 0.78	96.05± 0.10	99.16 ±0.03	23.87 ± 0.73	95.65± 0.22	98.98 ±0.07LSUN-Resize	41.06 ±1.07	92.67 ±0.16	98.42 ±0.04	41.49 ±1.24	92.97± 0.24	98.46 ±0.07Mean	49.56	89.53	97.36	43.74	91.38	97.87Table 5: The results of out-of-distribution detection on CIFAR100 with softmax scoreEuclidean Neural Network	Hyperbolic Neural NetworkFPR95 J AUROC ↑ AUPR ↑ FPR95 J AUROC ↑ AUPR ↑ISUN	74.07 ± 0.87	82.51 ± 0.39	95.83 ± 0.11	68.37 ± 0.90	81.31 ± 0.43	94.96 ± 0.20Place365	81.01 ± 1.07	76.90 ± 0.45	94.02 ± 0.15	79.66 ± 0.69	76.94 ± 0.28	93.91 ± 0.18Texture	83.67 ± 0.68	77.52 ± 0.32	94.47 ± 0.10	64.91 ± 0.80	83.26 ± 0.25	95.77 ± 0.08SVHN	84.56 ± 0.78	84.32 ± 0.22	96.69 ± 0.07	53.11 ± 1.04	89.53 ± 0.26	97.71 ± 0.07LSUN-Crop	43.46 ± 0.79	93.09 ± 0.23	98.58 ± 0.05	51.08 ± 1.17	87.21 ± 0.39	96.83 ± 0.13LSUN-Resize	71.50 ± 0.73	82.12 ± 0.40	95.69 ± 0.13	63.86 ± 1.10	82.36 ± 0.42	95.16 ± 0.13
Table 5: The results of out-of-distribution detection on CIFAR100 with softmax scoreEuclidean Neural Network	Hyperbolic Neural NetworkFPR95 J AUROC ↑ AUPR ↑ FPR95 J AUROC ↑ AUPR ↑ISUN	74.07 ± 0.87	82.51 ± 0.39	95.83 ± 0.11	68.37 ± 0.90	81.31 ± 0.43	94.96 ± 0.20Place365	81.01 ± 1.07	76.90 ± 0.45	94.02 ± 0.15	79.66 ± 0.69	76.94 ± 0.28	93.91 ± 0.18Texture	83.67 ± 0.68	77.52 ± 0.32	94.47 ± 0.10	64.91 ± 0.80	83.26 ± 0.25	95.77 ± 0.08SVHN	84.56 ± 0.78	84.32 ± 0.22	96.69 ± 0.07	53.11 ± 1.04	89.53 ± 0.26	97.71 ± 0.07LSUN-Crop	43.46 ± 0.79	93.09 ± 0.23	98.58 ± 0.05	51.08 ± 1.17	87.21 ± 0.39	96.83 ± 0.13LSUN-Resize	71.50 ± 0.73	82.12 ± 0.40	95.69 ± 0.13	63.86 ± 1.10	82.36 ± 0.42	95.16 ± 0.13Mean	73.05	82.74	95.88	63.50	83.43	95.725 ConclusionWe address one important issue when training HNNs which is ignored in previous literature. Weidentify the vanishing gradient problem when training hyperbolic neural networks and propose asimple yet effective solution which does not need to modify the current optimizer or architecture.
Table 6: The statistics of the datasets.
Table 7: Adversarial training with FGSM ( = 0.05) on MNIST.
Table 8: The results of out-of-distribution detection on CIFAR10 with energy scoreNetwork	Euclidean Neural Network	Hyperbolic Neural NetworkOOD Dataser^^^			FPR95 J	AUROC ↑	AUPR ↑	FPR95 J	AUROC ↑	AUPR ↑ISUN	34.19 ± 0.97	93.07 ± 0.24	98.42 ± 0.07	25.39 ± 0.32	95.48 ± 0.09	99.01 ± 0.04Place365	43.34 ± 1.22	88.50 ± 0.48	96.76 ± 0.17	45.17 ± 1.19	89.61 ± 0.28	97.20 ± 0.14Texture	58.51 ± 0.77	82.98 ± 0.20	94.55 ± 0.14	49.70 ± 0.94	90.66 ± 0.20	97.98 ± 0.04SVHN	49.04 ± 1.05	91.57 ± 0.13	98.12 ± 0.05	57.33 ± 1.34	88.45 ± 0.20	97.44 ± 0.06LSUN-Crop	9.48 ± 0.60	98.21 ± 0.07	99.63 ± 0.02	24.78 ± 0.73	95.06 ± 0.15	98.92 ± 0.05LSUN-Resize	28.28 ± 0.66	94.31 ± 0.14	98.72 ± 0.04	22.52 ± 0.67	96.15 ± 0.09	99.18 ± 0.02Mean	37.14	91.44	97.70	37.48	92.57	98.29Table 9: The results of out-of-distribution detection on CIFAR100 with energy scoreEuclidean Neural Network	Hyperbolic Neural NetworkFPR95 J AUROC ↑ AUPR ↑ FPR95 J AUROC ↑ AUPR ↑ISUN	74.49 ± 0.60	82.45 ± 0.33	95.84 ± 0.12	68.75 ± 0.93	81.33 ± 0.31	94.93 ± 0.16Place365	81.20 ± 0.86	77.02 ± 0.34	94.13 ± 0.13	79.51 ± 0.69	77.23 ± 0.37	93.97 ± 0.17Texture	83.19 ± 0.31	77.74 ± 0.35	94.54 ± 0.11	65.03 ± 0.52	83.38 ± 0.29	95.85 ± 0.10SVHN	84.12 ± 0.59	84.41 ± 0.16	96.72 ± 0.04	55.44 ± 1.00	89.43 ± 0.25	97.69 ± 0.06LSUN-Crop	43.80 ± 1.29	93.04 ± 0.22	98.56 ± 0.05	74.89 ± 0.73	84.98 ± 0.18	96.46 ± 0.08LSUN-Resize	71.86 ± 0.69	81.86 ± 0.27	95.60 ± 0.09	64.35 ± 0.62	82.64 ± 0.36	95.27 ± 0.14
Table 9: The results of out-of-distribution detection on CIFAR100 with energy scoreEuclidean Neural Network	Hyperbolic Neural NetworkFPR95 J AUROC ↑ AUPR ↑ FPR95 J AUROC ↑ AUPR ↑ISUN	74.49 ± 0.60	82.45 ± 0.33	95.84 ± 0.12	68.75 ± 0.93	81.33 ± 0.31	94.93 ± 0.16Place365	81.20 ± 0.86	77.02 ± 0.34	94.13 ± 0.13	79.51 ± 0.69	77.23 ± 0.37	93.97 ± 0.17Texture	83.19 ± 0.31	77.74 ± 0.35	94.54 ± 0.11	65.03 ± 0.52	83.38 ± 0.29	95.85 ± 0.10SVHN	84.12 ± 0.59	84.41 ± 0.16	96.72 ± 0.04	55.44 ± 1.00	89.43 ± 0.25	97.69 ± 0.06LSUN-Crop	43.80 ± 1.29	93.04 ± 0.22	98.56 ± 0.05	74.89 ± 0.73	84.98 ± 0.18	96.46 ± 0.08LSUN-Resize	71.86 ± 0.69	81.86 ± 0.27	95.60 ± 0.09	64.35 ± 0.62	82.64 ± 0.36	95.27 ± 0.14Mean	73.11	82.75	95.90	67.99	83.17	95.70A.7 Softmax with temperature scalingWe consider softmax with temperature scaling as an alternative for addressing the vanishing gradi-ent problem in training hyperbolic neural networks. Softmax with temperature scaling introduces16Under review as a conference paper at ICLR 2022an additional temperature parameter T to adjust the logits before applying the softmax function.
Table 10: Learning with word embeddings with clipped hyperbolic space outperforms both withEuclidean space and vanilla hyperbolic space.
Table 11: The proposed feature clipping outperforms vanilla HNNs and HNNs with regularization.
