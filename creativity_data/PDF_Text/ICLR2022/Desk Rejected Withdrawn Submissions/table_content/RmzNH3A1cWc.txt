Table 1: Related method comparison. Time is mentioned in GPU hours by h, or ImageNet epochs bye. Our method assumes 197 candidate operations. L is the number of target architecturesTo tackle the search problem with a large number of candidate operations in an efficient and scalableway, we propose a two-phase approach. In the first phase, we define a large candidate pool ofoperations ranging from classic residual blocks (He et al., 2016a) to more recent transformer blocks,such as those employed by Dosovitskiy et al. (2020), with varying hyperparameters. Candidateoperations are pretrained to mimic the teacher’s operations via a simple layer-wise optimization.
Table 2: ModelS optimized with HANT, evaluated on ImageNet-1K. Latency iS computed for a batchof 128 images over 10 runs on actual hardware. Left: 3 families of models optimized for GPU. Righttop: detailed look at EfficientNets. Right bottom: EfficientNetV1 optimized for CPU inference.
Table 3: Candidate evaluation metrics formodel ranking before and after finetuning.
Table 4: Comparing methods for candidateselection (NAS). Our proposed ILP is better(+0.43%) and 821× faster.
Table 5: Zero-shot HANT withonly skip connections.
Table 6: Ablations on the finetuningsetups. When it is disabled, we observe an accuracy degradation of 0.65%. This emphasises thebenefit of training a larger model and then distilling towards a smaller one for inference.
Table 7: Additional comparison to prior work. Latency values measured at batch size 32 with PyTorchFP32. In brackets are our improvements.
