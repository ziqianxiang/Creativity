Table 1: F1 value results of different pre-training models on different pre-training targets^training target- model	^	MLM+NSP	MLM	LM	NLIBERT	-^88.78%^^	-	-	-RoBERTa	-	90.29%	-	-ERNIE	89.26%	-	-	-BERT+ continues pre-training	89.32%	90.45%	89.92%	92.89%RoBERTa+ continue pre-training	89.82%	90.88%	90.15%	94.32%ERNIE+ continue pre-training	89.15%	90.48%	89.83%	93.31%The table1 shows the comparison of the effects of different pre-training models BERT, RoBERTaand ERNIE on different pre-training targets. Each result in the table is the result of fine-tuning theF1 value of each model on the electric power intention data. Since the BERT without continuouspre-training uses only the pre-training targets of MLM and NSP in the initial pre-training, it has noexperimental results in MLM, LM and NLI. The same is true for RoBERTa and ERNIE withoutfurther pre-training.
Table 2: Experimental results of F1 value of different models on the improved fine-tuning networkmodel ___________________________________________________________________________^!training target- model	ɪ		MLM+NSP	MLM	LM	NLIBERT+ continues pre-training	-^89.43%^^	90.89%	90.13%	93.11%RoBERTa+ continue pre-training	90.12%	91.10%	90.40%	95.81%ERNIE+ continue pre-training	89.50%	90.79%	90.10%	93.56%It can be seen that the improved fine-tuning network model proposed in this paper improves Thefine-tuning effect of each model proves that the improvement of the BERT fine-tuning networkstructure in this article is effective. When the model is selected to continue pre-training, the targetis RoBERTa with NLI, and the fine-tuning effect on the improved network model proposed in thispaper is the best, and the F1 value of the best effect is 95.81%.
Table 3: Single teacher model knowledge distillation F1 value result	CNN	LSTMteacher model^	-					BERT	85.78%	85.30%RoBERTa	87.60%	87.04%ERNIE	85.66%	85.33%BERT+ continues pre-training	85.89%	85.39%RoBERTa+ continue pre-training	88.74%	88.41%ERNIE+ continue pre-training	85.96%	86.28%From the table 3, it can be seen that the performance of the student model will be better after theteacher model that has been continuously pre-trained is distilled onto the student model. Amongthese continuously pre-trained models, RoBERTa’s distillation performed best. From the compari-son of the effect of the student model CNN and LSTM, it can be seen that the performance of CNNis slightly better than LSTM. After a simple analysis, it can be seen that the model learns a certainamount of knowledge in the downstream data when it continues to pre-train, and the NLI trainingtarget is a more difficult target for the model, so the knowledge learned by the teacher model in thecontinued pre-training Will be better taught to students in the knowledge distillation model.
Table 4: Result of knowledge distillation F1 value of multi-teacher modelstudent model multi-teacher model"——	CNN	LSTMBERT + RoBERTa	86.78%	86.37%BERT + ERNIE	86.23%	86.30%	RoBERTa + ERNIE	88.83%	88.47%The table 4 is the knowledge distillation training result of the multi-teacher model. This table showsthe effect comparison of the joint distillation of multiple teacher models to the student model CNNand LSTM. It can be seen from the table that when the teacher model chooses RoBERTa and ERNIEin the knowledge distillation of the multi-teacher model, the student model performs best, and isbetter than the effect of the single teacher model RoBERTa and ERNIE after continuing pre-trainingand then performing distillation training. It is proved that the distillation based on the multi-teachermodel can make the student model more effective. When the multi-teacher model selects BERTand RoBERTa separately, it may lead to a situation where the effect is worse than that of a singleteacher model. This paper analyzes that because BERT and RoBERTa use similar training data in theinitial large-scale pre-training, and because the effect of BERT and RoBERTa on the single-teachermodel is large, the two models are teaching the student model at the same time. The time cannotplay a complementary role, so the effect is worse. Therefore, in the knowledge distillation basedon multiple teachers, the selection of teacher model is very important. The greater the differencebetween multiple models, the better the distilling effect will generally be.
