Table 1: Top-1 classification accuracy (%), Tx denotes SNN with ‘x’ timestep							Architecture	Dataset	ANN	T5	T4	T3	T2	T1VGG6	CIFAR10	91.59	90.61	90.52	90.40	90.05	89.10VGG16	CIFAR10	94.10	93.90	93.87	93.85	93.72	93.05ResNet20	CIFAR10	93.34	92.62	92.58	92.56	92.11	91.10VGG16	CIFAR100	72.46	71.58	71.51	71.46	71.43	70.15ResNet20	CIFAR100	65.90	65.57	65.37	65.16	64.86	63.30VGG16	ImageNet	70.08	69.05	69.03	69.01	68.62	67.71Latency Reduction as Form of Compression.
Table 2: Comparison of T1 SNN to other reported results. SGB, hybrid and TTFS denote surrogate-gradient based backprop, pretrained ANN followed by SNN fine-tuning, and time-to-first-spikescheme, respectively and (qC, dL) denotes an architecture with q conv layers and d linear layers.
Table 3: Comparison of T1 SNN to ANNbReference	Dataset	Accuracy(%)(Sakr et al., 2018)	CIFAR10	89.6(Bengio et al., 2013)	CIFAR10	85.2(Deng & Gu, 2020)	CIFAR10	91.88This Work (T1)	CIFAR10	93.05(Deng & Gu, 2020)	CIFAR100	70.43This Work (T1)	CIFAR100	70.15observe that T1 SNN outperforms other ANNb approaches in most cases; our results are slightlyloWer on CIFAR100 compared to Deng & Gu (2020), hoWever in their case, the ANN output isthresholded to saturate at 2, but the values are analog. Note, in all cases, Weights are full precision.
Table 4: Accuracy(%) on CIFAR10 with VGG6			Table 5: ACCUracy(%) with VGG16, D-BN and D_nBN denote dataset D with and without batch-norm respectively				T	VGG6g	VGG6d	T	CIFAR10_BN	CIFAR10_nBN	CIFAR100 _BN	CIFAR100_nBN5	^^90.61 ^^	90.15	5	93.90	92.15	71.58	69.864	^^90.52^^	90.08	4	93.87	91.95	71.51	69.843	^^90.40^^	89.91	3	93.85	91.90	71.46	69.762	^^90.05^^	89.35	2	93.72	91.88	71.43	69.301	^^89.10^^	88.64	1	93.05	91.03	70.15	67.76end, training with T1 following direct conversion from ANN might be possible. To validate this,we experiment with a VGG6 on CIFAR10 and the results are shown in Table 4. In this table,VGG6g denotes networks trained with gradual latency reduction and VGG6d denotes networksdirectly converted from ANN and trained using that particular timestep, so the result in the last rowof Table 4 is obtained by converting an ANN to SNN and directly training with 1 timestep. Weobserve that proposed gradual training scheme provides slightly higher accuracy compared to directtraining in case of shallower networks, though it increases the training overhead. This is consistentwith ANN domain results (Furlanello et al., 2018; Han et al., 2015b), where the authors achievebetter performing networks using sequential model compression compared to direct compression.
