Table 2: Data statistics of the filtered CodeSearchNet corpus for Go, Java, Javascript, PHP, Python andRuby programming languages. For each query in the dev and test sets, the answer is retrieved from the set ofcandidate codes (last row)for longer, after we found that the CodeBERT checkpoint from Feng et al. (2020b) was not trainedtill convergence. When starting from our new checkpoint, we find that the CodeBERT baseline,if fine-tuned with a larger batch-size (largest possible that we can fit on 8 A100 GPUs) and for alarger number of epochs, is able to perform substantially better than the results reported before. Wereport the baselines from Guo et al. (2021) in Table 3 along with the results for our replication oftwo of these baselines. Previous studies have emphasized this effect - larger batch sizes are knownto typically work well when training with the infoNCE loss in a contrastive learning framework, dueto more negative samples from the batch (Chen et al., 2020).
Table 3: Mean Reciprocal Ranking (MRR) values of different methods on the codesearch task on6 Programming Languages from the CodeSearchNet corpus (test set). The first set consists of fourfinetuning-based baseline methods (NBow: Bag of words, CNN: convolutional neural network,BiRNN: bidirectional recurrent neural network, and multi-head attention), followed by the secondset of models that are pre-trained then finetuned for code search (RoBERTa: pre-trained on text byLiu et al. (2019), RoBERTa (code): RoBERTa pre-trained only on code, CodeBERT: pre-trained oncode-text pairs by Feng et al. (2020a), GraphCodeBERT: pre-trained using structure-aware tasks byGuo et al. (2021)). SYNCOBERT: pre-trained using syntax-aware tasks by Wang et al. (2021a). Inthe last four rows, we report the results with the shared and separate variants of our CasCode schemeusing the fine-tuned CodeBERT models for K of 10 and 100.
Table 4: Inference speed comparison for the different variants studied. The number of parameterscorresponding to the classifier head are separated with a + sign in the second column. Inferenceduration is reported for 100 queries from the Ruby subset of CodeSearchNet, using a single A100GPU. Constructing the PL index offline requires 6.76 seconds for the Ruby dataset and is notincluded in the durations listed here. MRR scores are reported on the entire test set. Throughput ofthe retrieval model (measured in # queries processed per second) is listed in the last column.
