Table A.1: AudioSet Contrastive pretraining linear eval performance for various hyperparameterswhen trained on the balanced train subset. fconly represents linear-eval, whereas full representsfinetuning the entire modelAs mentioned before, a MLP with one-hidden layer with 512 hidden units as our projection headfproj (.) to extract features z = fproj (fenc(x)), z ∈ Rd=512, on which NT-Xent loss (Chen et al.,2020) was used (Eq 1).
Table A.2: AudioSet Supervised Learning performance. Pretraining indicates whether contrastivepretrained weights were usedA.3.2 In-domain transfer learning: SER on FSD50kTraining was done on 2.5-sec random crops, and the validation set was used for hyperparametertuning. A per-tpu-core batch size of 64 worked best. mAP and dprime metrics are reported (TablesA.4, A.5).
Table A.3: AudioSet SER performance with comparable baselines. Confidence intervals omitted.
Table A.4: FSD50K eval performance. Pretraining refers to type of pretraining on AudioSetModel	#Params	Fs	features	mAP	dprimeCRNN (Fonseca et al., 2020)	0.96M	22.05 kHz	log-melspec	0.417	2.068VGG-like (Fonseca et al., 2020)	0.27M	22.05 kHz	log-melspec	0.434	2.167ResNet-18 (Fonseca et al., 2020)	11.3M	22.05 kHz	log-melspec	0.373	1.883DenseNet-121 (Fonseca et al., 2020)	12.5M	22.05 kHz	log-melspec	0.425	2.112Large Transformer (Verma & Berger, 2021)	2.3M	16 kHz	raw signal	0.537	-CNN12 (supervised-fconly)	14M	8 kHz	raw signal	0.463	2.144CNN12 (contrastive)	14M	8 kHz	raw signal	0.424	2.001Table A.5: FSD50k eval performance v/s comparable baselines. Confidence intervals omitted.
Table A.5: FSD50k eval performance v/s comparable baselines. Confidence intervals omitted.
Table A.6: Speech Commands v2 Test Accuracy. Recent comparable baselines for reference. Paran-thesis indicate pretrained weights used in the proposed work (bold)It is evident from the results on the above mentioned benchmark datasets that while the proposedmethod doesn’t achieve state-of-the-art performance, it is sufficiently performant for the purpose ofthe study.
