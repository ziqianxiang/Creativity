Table 1: Comparison of Robustness to Sensory Loss of Different Models: ”A” means the baselinemodel with concat fusion. ”B1” means models use gated fusion and ”B2” is the same as ”B1”models with the single-modal features keep fixed during training.
Table 2: Robustness to Sensory Loss For Models Trained With Modality Mix		AV	LRW AO	VO	AV	OuluVS AO	VOConcat	Baseline	97.87%	39.63%	5.66%	99.50%	99.00%	10.45%Fusion	+ Modality Mix	97.85%	95.88%	80.56%	99.50%	100.00%	61.69%Gated Fusion	Baseline	97.78%	52.50%	4.53%	99.5%	100.00%	16.42%	+ Modality Mix	97.84%	95.76%	78.60%	99.0%	99.50%	46.27%Gated Fusion	Baseline	97.96%	68.38%	2.58%	99.5%	93.03%	7.96%(Feature Fixed)+ Modality Mix		97.69%	95.69%	78.60%	99.0%	98.51%	18.41%5.2	Generalization to Missing Modality5.2.1	Born Disability Models Beat Acquired Disability OnesFor human beings, it’s intuitive that people with born disabilities will adapt better than those acquiredlater, but a fashion in neural networks is conducting transfer learning as away of training better mod-els, which is also used in cross-modal situations, with SoundNet(Aytar et al., 2016) as an example.
Table 3: Generalization Ability For Models Multi-Modal Trained Compared With Born Disability	AV	AO	VOBaseline	97.87%	39.63%	5.66%Born Blind	89.42%	94.12%	0.20%Bi-GRU	Born Deaf	0.25%	0.21%	76.80%Acquired Blind	22.34%	67.28%	0.20%Acquired Deaf	34.36%	0.28%	68.14%Baseline	97.95%	81.70%	3.78%Born Blind	72.76%	93.09%	0.18%CNN	Born Deaf	0.56%	0.20%	76.41%Acquired Blind	21.20%	67.76%	0.20%Acquired Deaf	1.59%	0.28%	68.14%As in Table 3, models trained with born sensory loss have sometimes achieved 20 percentage ofhigher accuracy than those trained from an multi-modal model. On the other hand, there appeared astrange result that acquired deaf model beat the blind model which is to the contrary of the modelswith born disabilities, which is against our intuition that people can easily adapt to audio-only speechrecognition while lipreading is much harder. We fix such problem and get better combined single-modal performance with our proposed gated fusion module in further experiments.
Table 4: Comparison of Generalization of Different Models		AV	AO	VO	Baseline with Modality Mix	97.85%	95.88%	80.56%	Born Blind	89.42%	94.12%	0.20%	Born Deaf	0.25%	0.21%	76.80%Concat Fusion	Acquired Blind	22.34%	67.28%	0.20%	Acquired Deaf	34.36%	0.28%	68.14%	Acquired Blind (Modality Mix)	68.83%	67.60%	0.26%	Acquired Deaf (Modality Mix)	46.96%	0.24%	77.58%	Baseline with Modality Mix	97.84%	95.76%	79.78%	Born Blind	87.70%	94.32%	0.20%	Born Deaf	0.30%	0.14%	77.30%Gated Fusion	Acquired Blind	85.82%	73.03%	0.20%	Acquired Deaf	10.78%	0.23%	67.73%	Acquired Blind (Modality Mix)	83.73%	91.28%	0.20%	AcqUired Deaf (ModaIity Mix)	29.39%	0.40%	77.27%8Under review as a conference paper at ICLR 20225.3	Generalization to Different DatasetsBesides the generalization to missing modalities, another question is whether the proposed tech-
Table 5: Generalizing mUlti-modal featUres from LRW to OUlUVS	OuIuVS _pad	OuluVS-resizeConcat FuSion	21.39%	10.95%Concat FUsion & Modality Mix	28.86%	12.44%Gated FUsion	16.92%	7.46%Gated FuSion & Modality Mix	34.33%	16.42%6	Conclusion & Future WorkIn this paper we address and analysis the problem that a traditional mUlti-modal artificial neUralnetwork dealing with aUdiovisUal inpUts failed to generalize well to corresponding single-modaltasks, which is similar to the common view that born disabilities enables better adaption than thoseacqUired later. We take inspiration from hUman brains and sUppose the lack of Understanding theinherent correlation of different modalities in the artificial network being the key of the problem.
Table 6: Sensory Deprivation Experiment on LRS2 models	A	AO	VO	Original Trained Model	10.8%	12.9%	56.3%Acquired Blind	12.5%	12.9%	59.7%Acquired Deaf	13.7%	16.3%	56.6%Acquired Blind (Feature Fixed)	11.8%	13.3%	60.1%Acquired Deaf (Feature Fixed)	17.0%	15.9%	57.0%Figure 6: Training curves of the different models, all of which have clear descendent of loss in thetraining.
