Table 1: Overview of the used datasets - # is an abbreviation for number. The class imbalance isgiven as the percentage of the smallest and largest class with regard to the complete dataset. PA isthe expected prior ambiguity probability of the dataset. n is the average of annotations per image.
Table 2: Performance across different methods and datasets - The vanilla algorithm is highlightedin light grey. Better results in comparison to the vanilla algorithm are marked bold. The definition ofthe metrics are given in subsection 3.2. CE stands for supervised Cross-Entropy training. All valuesare given in %. Reasons for exclusion: H - Hardware RestrictionsMethodsPlanktonTurkeyMice BoneCIFAR10HSTL-10	F1 ↑	d，	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑CE	86.71	30.45	-56.26	83.84	42.98	-40.86	69.55	54.75	-14.80	67.71	55.80	-11.91	80.48CE + S2C2	78.24	23.41	-54.84	85.79	27.64	-58.14	93.88	36.58	57.30	78.27	54.52	-23.75	88.45Mean-Teacher (37)	88..72	25.84	-62.88	81.82	45.12	-36.70	66.41	48.83	-17.58	73.53	46.93	-26.59	80.67Mean-TeaCher(37) + S2C2	91.30	24.84	-66.46	86.45	33.92	-52.53	89.4	35.11	-54.73	85.13	52.44	-32.69	89.28Pi-Model (23)	87.57	28.43	-59.14	82.11	39.46	-42.65	68.15	54.11	-14.04	71.53	49.13	-22.40	82.56Pi-Model (23) + S2C2	79.79	19.08	-60.71	87.43	23.33	-64.10	88.01	30.99	-57.02	83.05	43.40	-39.65	89.54Pseudo-Label (24)	87.62	27.42	-60.20	82.37	44.88	-37.49	66.60	57.03	-9.57	69.70	53.30	-16.40	82.48Pseudo-Label (24) + S2C2	89.31	31.76	-57.55	83.44	35.04	-48.41	86.58	37.52	-49.06	83.74	51.32	-32.42	88.87FixMatch (35)	85.81	30.29	-55.52	82.14	43.33	-38.81	H	H	H	78.09	41.99	-36.10	89.35
Table 3: Consistency compar-ison of generated labels fromproposals for the Mice Bonedataset - The first column de-scribes by which algorithmthe propsals were generated.
Table 4: Complete ablation results for averaging over different methods - The vanilla algorithmsas baseline are highlighted in light grey. Each row below that extend this baseline individuallywith CE-1 (31), Clustering & Classification (CC) or both (S2C2). CC can be interpreted as S2C2without CE-1. The prior ambiguity estimate pA is given if used in brackets. Results that improveover the baseline are marked in bold. The definition of the metrics are given in subsection 3.2. Thecolumn ’Ambiguous’ gives the percentage of predicted ambiguous data and the last column givesthe number of runs over which we averaged.
Table 5: Impact of ambiguous labels - Macro FI-SCore for different methods and across threedifferent subsets on the validation data from the Plankton and CIFAR10-H dataset. Columns: A1 -Labels are sampled from l; A - Labels are the maximum class ofl; C -No ambiguous labels/imagesare usedMethods	Plankton	CIFAR10-H	A1	A	C	A1	A	CCE	86.71	8835	96.10	67.71	68.89	86.57Mean-Teacher (37)	88.72	88.94	96.00	73.56	75.06	86.96Pi-MOdel (23)	87.57	89.03	96.41	71.53	72.75	87.19PSeUdO-Label (24)	87.62	88.41	96.20	69.70	71.82	87.15FixMatch (35)	80.29	90.24	98.86	76.15	79.15	90.37Plankton	Turkey	Mice Bone	CIFARIO-Htions - Wrong classifications based on the normal head are highlighted in red.
