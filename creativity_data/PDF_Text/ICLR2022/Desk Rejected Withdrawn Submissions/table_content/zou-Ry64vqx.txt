Table 1: The test accuracy, PARAMs and FLOPs compression ratio of VGG16 and ResNet18 onCIFAR-10 in IID partition. The compressed model PARAMs and FLOPs are normalized accordingto FedAvg. The maximum communication rounds are 4000 for both networks. For FedMorph, weoutput the averaged PARAMs and FLOPs before the test accuracy reaching the maximum value.
Table 2: The summary of datasets and network architectures.
Table 3: The summary of neural networks and hyper-parameters used in our experiments.
Table 4: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasetsof MNIST and EMNIST in IID partition. The compressed model PARAMs and FLOPs are normal-ized according to FedAvg. The maximum communication rounds are 1500 and 3000, respectively.
Table 5: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasetsOfMNIST and EMNIST in NonIJID Partition settings._____________________________________Method		MNIST				EMNIST			Accuracy	Params	Flops	Accuracy	Params	FlopsFedAvg	09928			0.8599		FedAvg + 25% TopK	0.9919	1.000	1.000	0.8558	1.000	1.000FedAvg + 10% TopK	0.9912			0.8444		FedProx	0.9929			0.8587		FedProx + 25% TopK	0.9924	1.000	1.000	0.8544	1.000	1.000FedProx + 10% TopK	0.9899			0.8463		FedMorph	0.9939	0.0117	0.0576	0.8606	0.0986	0.3240FedMorph + 25% TopK	0.9934	0.0105	0.0495	0.8595	0.0904	0.3105FedMorph + 10% TopK	0.9920	0.0103	0.0401	0.8588	0.0850	0.2968FedMorphProx	0.9938	0.0105	0.0506	0.8599	0.1003	0.3252FedMorphProx + 25%	0.9930	0.0102	0.0480	0.8597	0.0824	0.2761FedMorphProx + 10% TopK	09934	0.0107	0.0448	0.8589	0.0932	0.3190Table 6: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasetsof CIFAR-10 in NonnD Partition._______________________________________________________Method	VGG16 on CIFAR-10			ResNet18 on CIFAR-10			Accuracy	Params	Flops	Accuracy	Params	Flops
Table 6: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasetsof CIFAR-10 in NonnD Partition._______________________________________________________Method	VGG16 on CIFAR-10			ResNet18 on CIFAR-10			Accuracy	Params	Flops	Accuracy	Params	FlopsFedAvg	0.8323			0.7679		FedAvg + 25% TopK	0.8262	1.000	1.000	0.7651	1.000	1.000FedAvg + 10% TopK	0.7758			0.7357		FedProx	0.8377			0.7667		FedProx + 25% TopK	0.8015	1.000	1.000	0.7636	1.000	1.000FedProx + 10% TopK	0.7774			0.7385		FedMorph	0.8492	0.0903	0.2467	0.8295	0.0948	0.2161FedMorph + 25% TopK	0.8444	0.0925	0.2497	0.8348	0.1012	0.3125FedMorph + 10% TopK	0.8360	0.0934	0.2616	0.8278	0.0861	0.2991FedMorphProx	0.8510	0.0953	0.2737	0.8355	0.0955	0.2210FedMorphProx + 25% TopK	0.8446	0.0900	0.2611	0.8326	0.0882	0.2181FedMorphProx + 10% TopK	08311	0.0935	0.2415	0.7357	0.9998	0.0078is over-fitted as its training accuracy quickly reaches 1. While FedDropout is under-fitted as itstraining accuracy converges below that of FedMorph. For EMNIST, the dataset of size 697,932 ispartitioned to 5000 clients, resulting in that each client has lesser than 140 samples. Furthermore,we set the number of local optimization epochs to be 5. Hence, all methods are easily over-fitted.
Table 7: ResNet8 ArchitectureLayer	Kernel	Stride	ChannelsInput	-	-	3Conv+BN+ReLU	3	1	64Conv+BN+ReLU	-3-	-1-	64-Conv+BN	3	1	64Skip (Conv+BN+ReLU)	1	1	64Conv+BN+ReLU	-3-	-2-	128-Conv+BN	3	1	128Skip (Conv+BN+ReLU)	1	2	128Conv+BN+ReLU	-3-	-2-	256-Conv+BN	3	1	256Skip (Conv+BN+ReLU)	1	2	256Conv+BN+ReLU	-3-	-2-	5Γ^Conv+BN	3	1	512SkiP (Conv+BN+ReLU)	1	2	512AvgPooling	-4-	-4-	5Γ^Linear	-	-	10Table 8: VGG16 ArchitectureLayer	Kernel	Stride	Channels
Table 8: VGG16 ArchitectureLayer	Kernel	Stride	ChannelsInput	-	-	3Conv+BN+ReLU	3	1	64Conv+BN+ReLU	3	1	64MaxPooling	2	2	64Conv+BN+ReLU	3	1	128Conv+BN+ReLU	3	1	128MaxPooling	2	2	128Conv+BN+ReLU	3	1	256Conv+BN+ReLU	3	1	256Conv+BN+ReLU	3	1	256MaxPooling	2	2	256Conv+BN+ReLU	3	1	512Conv+BN+ReLU	3	1	512Conv+BN+ReLU	3	1	512MaxPooling	2	2	512Conv+BN+ReLU	3	1	512Conv+BN+ReLU	3	1	512Conv+BN+ReLU	3	1	512
Table 9: ResNet18 ArchitectureLayer	Kernel	Stride	ChannelsInput	-	-	3Conv+BN+ReLU	3	1	64Conv+BN+ReLU	-3-	-1-	64^Conv+BN	3	1	64Skip (Conv+BN+ReLU)	1	1	64Conv+BN+ReLU	3	1	64Conv+BN	3	1	64Skip (Conv+BN+ReLU)	1	1	64Conv+BN+ReLU	-3-	-2-	128-Conv+BN	3	1	128Skip (Conv+BN+ReLU)	1	2	128Conv+BN+ReLU	3	1	128Conv+BN	3	1	128Skip (Conv+BN+ReLU)	1	2	128Conv+BN+ReLU	-3-	-2-	256-Conv+BN	3	1	256Skip (Conv+BN+ReLU)	1	2	256Conv+BN+ReLU	3	1	256
