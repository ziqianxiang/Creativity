Table 1: Compression of the softmax layer of VGG16 trained on CIFAR10. Inference time measuredusing default implementation (inf.) and using TreeLite (inf.+)Model	test error, %	params	FLOPs	inf. (ms)	inf.+ (ms)Reference softmax	6.46	^^5130	5130	0.01249	—TAO, T = 1, λ = 0.01	6.68	^^3070	1305	0.00156	—CART, T = 1	7.37	19	6	0.07140	0.01973XGB, T = 1 × 10	7.75	40	10	0.41064	0.02132Gradient boosting is slow with multiclass problems First, we give some intuition why gradientboosting (GB) forests are expected to be large. Boosting works by greedily learning T learners.
Table 2: Summary of the training machine (left) and the software (right) used in our experiments.
