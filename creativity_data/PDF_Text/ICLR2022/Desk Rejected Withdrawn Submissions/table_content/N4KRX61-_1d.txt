Table 1: The correspondence between properties and atomic predicates for the DoorKey SRM inFig.1bstate at the time step t1. The predicate #CLOSEDOORx?3+?2 > 0 in Fig.1b is introduced withdue consideration of avoiding overly penalizing the agent for closing the door, which behavior isredundant for the task. The underlying idea is: if the reward function penalized an under-trained RLagent for every door closing behavior with some high penalty ?3 < 0 for a total of #CLOSEDOORamount of times, and the accumulated penalty #CLOSEDOORx?3 outweighed the reward ?2 > 0 forunlocking the door, then the agent in practice might be inclined to reside away from the door forgood. The SRM in Fig.1b simply upper-bounds the accumulated penalty to avoid negative effectsin practice. Then we show the atomic predicates in the symbolic constraint for this task in Table.1.
Table 2: The correspondence between properties and the relational and non-relational atomic predi-cates for the SRM of KeyCorridor in Fig.9b) ∀t0 < t.(Pick_Up_Key@t) ∧ (—Pick_Up_Key@t0) → ∃t00 < t.(Open_a_Door@t00); C)(Unlock_Door@t) → ∃t0 < t.(Open_a_Door@t00). Regarding the implication a, two predicatesPick_Up_Key@t and Drop_Key@t are added at the internal state “After Unlocking” to governthe rewards returned for their respectively concerned behaviors after the door is unlocked. As forthe implications b and c, the caveat is to determine the utility of each door opening behavior. Adesigner may go to one extremity by rewarding every door opening behavior with some constant,which, however, either represses exploration by penalizing opening door, or oppositely raises rewardhacking, i.e., agent accumulates reward by exhaustively searching for doors to open. Alternatively,the designer may go to another extremity by carrying out a motion planning and specify the solutionin the SRM, which, however, is cumbersome and cannot be generalized. In this paper, we highlighta economical design pattern to circumvent such non-determinism.
Table 3: The correspondence between properties and predicates for the SRM of ObstructedMazetask in Fig.10A.3 Training details•	Training Overhead. We note that all the designed SRMs require checking hindsight ex-periences, or maintaining memory or other expensive procedures. However, line 5 of Al-gorithm 1 requires running all K candidate programs on all m sampled trajectories, whichmay incur a substantial overhead during training. Our solution is that, before samplingany program as in line 5 of Algorithm 1, we evaluate the result of [[L]](τA,i), which keepsholes ? unassigned, for all the m trajectories. By doing this, we only need to execute theexpensive procedures that do not involve the holes once, such as the counter #OPENDOORand the reward modification steps in the Hindsight block in Fig.10. Then We use qφ tosample K hole assignments {hk}kK=1 from H and feed them to {[[L]](τA,i)}im=1 to obtain{{[lk := L[hk//-"/KU By replacing line 2 and line 5 with those two steps inAlgorithm 1, we significantly reduce the overhead.
Table 4: Hyperparameters used in the training processescomponents Flet-Berliac et al. (2021), the PPO agent solely consists of the actor-criticnetworks.
