Table 1: Our model vs baselines. The gap % is w.r.t. the best value across all methods.
Table 2: Results on TSPlib Instances. The underlined and bold figures mean achieving the best resultsamong all methods (including OR-Tools) and all deep learning-based methods respectively.
Table 3: Configuration of Attack Neural NetworksModuleDESCRIPTIONFirst LayerSecond LayerOutput LayerOptimizerl2 normEpochsdim=2 with ReLU activationdim=128 with ReLU activationdim=2 with Sigmoid activationAdam with lr=0.05,lr_decay=0.95Weight_decay=0.0150 for TSP20, 50 for TSP50 and TSP100For the settings of data generator, we initialize the γN randomly and use a simple three-layer neuralnetworks to represent the attack generator fγC in Eq. 8. We also use a Sigmoid function as thelast layer to scale the variation within [0, 1] and an additional scalar λ ∈ [0, 1] to make a furtherlimit within [0, λ]. Here We set λ = 3 because of the '68-95-99.7 rule' which is a famous principlein statistics. It not only guarantees each point within [0,1] can reach any other point after adding
Table 4: Ablation Results under Different Initial SolutionsInstance	Opt.	Random	LIH(FS) Random Insert Nearest Insert		Farthest Insert	Random	Random Insert	LIH(FT) Nearest Insert	Farthest Insertpr226	80,369	697,738	103,441	97,357	84,088	853,580	103,441	97,357	84,088ts225	126,643	781,083	157,731	155,603	136,699	981,680	157,731	155,603	136,699kroD100	21,294	22,717	23,288	23,377	22,346	23,624	23,139	23,377	22,346eil51	426	451	459	437	444	462	441	437	444kroA100	21,282	23,033	21,458	22,269	22,891	22,596	22,289	22,269	22,891pr264	49,135	442,632	58,343	65,878	55,312	523,232	58,343	65,878	55,312pr152	73,682	300,897	91,335	86,914	76,389	302,977	91,335	86,914	76,389gil262	2,378	14,033	2,615	2,914	2,638	17,270	2,615	2,914	2,638rat99	1,211	1,298	1,299	1,305	1,250	1,325	1,341	1,305	1,250kroA150	26,524	97,431	28,628	31,344	28,789	86,237	28,628	31,344	28,789lin105	14,379	21,526	18,102	18,170	15,372	19,102	17,758	18,170	15,372pr124	59,030	151,075	67,163	68,178	61,645	160,136	67,163	68,178	61,645st70	675	742	702	782	741	735	699	782	741a280	2,579	16,878	3,084	2,987	3,018	19,777	3,084	2,987	3,018rd100	7,910	8,852	8,580	8,620	8,180	8,608	8,456	8,620	8,180pr136	96,772	265,334	131,880	106,059	104,429	239,330	131,880	106,059	104,429pr76	108,159	111,646	111,712	119,838	109,174	116,986	120,441	119,838	109,174kroA200	29,368	150,172	31,824	36,029	31,450	163,725	31,824	36,029	31,450
