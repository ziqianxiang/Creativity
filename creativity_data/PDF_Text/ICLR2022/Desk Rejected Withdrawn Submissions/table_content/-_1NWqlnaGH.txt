Table 1: Panoptic segmentation results on the COCO 2018 Val dataset. Superscripts ‘Th’ and ‘St’denote numbers for thing and stuff classes respectively. (*: computed using noisy annotations.)Method	I Backbone	I PQ	SQ	RQ I	PQTh	SQTh	RQTh I PQSt	PCV (Wang et al., 2020)	R-50 FPN	37.5	77.7	47.2	40.0	78.4	50.0	33.7AUNet (Li et al., 2019)	R-50 FPN	38.6	76.4	47.5	46.2	80.2	56.2	27.1OANet (Liu et al., 2019)	R-50 FPN	39.0	77.1	47.8	48.3	81.4	58.0	24.9UPSNet (Xiong et al., 2019)	R-50 FPN	42.5	78.5	52.5	48.1	79.2	59.2	33.9Ours*	R-50 FPN	40.3	78.2	50.0	44.6	78.8	55.0	33.9UPSNet (Xiong et al., 2019)	R-101 FPN	46.7	80.5	56.9	53.2	81.2	64.6	36.9Ours*	R-101 FPN	44.9	80.5	54.7	50.3	81.6	60.9	36.8improvement over the baseline and is able to detect most of the occluded instances. For example, themother occluded by a baby elephant in the first row is a difficult case as the two instances are verysimilar and without clearly discernible edges, and as a result, is not handled well by the baseline,but they are distinguished as separate instances by our approach. In the second row, two people ona motorcycle have been reliably detected by our approach, but were merged by the baseline. In thethird row, our system has detected a person who is occluded by the batter, just to the left. Also inthe third row, our system has correctly detected many individuals in the stands, but they were notdetected by the baseline. We note that there remains room for improvement in the masks producedby our system, but we have observed many cases for which the baseline system incorrectly mergestwo neighboring instances while our system distinguishes them correctly.
Table 2: Results on COCO Val. (B = Baseline (Xiong et al., 2019); O = Ours.)(a) Results for top 20 thing classes (by instance count) using ResNet-101 FPN backbone and updated segment matching. (WS: without smaller FPs; MI: detections on unset “iscrowd” instances; CR: detections on “iscrowd” instances.)					(b) Comparison of instance detection performance for our approach against the baseline. (R-50 and R-101 are ResNet-50 and ResNet-101 with FPN								backbones, respectively.) J = Lower is better.)		(↑ = Higher is better;	Method	PQ	RQ	PQTh	RQTh	Method	TP ↑	FN J	Recall ↑B WS	41.4	51.1	53.3	66.5	B (R-50)	20,599	15,504	0.5705O WS	41.6	51.5	54.5	68.0	O (R-50)	21,121	14,982	0.5850B WS+MI	45.4	55.2	67.7	81.4	Increase	+0.0145		O WS+MI	45.9	55.9	70.0	83.9		—							B (R-101) -	22,535	13568	0.6241							,	B WS+MI+CR	45.5	55.4	68.4	82.0	O (R-101)	23,308	12,795	0.6455O WS+MI+CR	46.1	56.1	71.0	84.6	Increase			+0.0214									8Under review as a conference paper at ICLR 2022Figure 5: Patterns that map to the same component while detecting instances of four categories areshown. As seen in the figure, similar regions across instances map to the same component of thecompositional model.
Table 3: Panoptic segmentation results on the MS-COCO 2018 Val dataset. Superscripts ‘Th’ and‘St’ denote numbers for thing and stuff classes respectively. (*: computed using noisy annotations.)Method	I Backbone	∣ PQ SQ RQ ∣ PQTh SQTh RQTh ∣ PQSt SQSt RQSt Single Stage				DeeperLab (Yang et al., 2019) Hou et al.(Hou et al., 2020) PCV (Wang et al., 2020) Panoptic DeepLab (Cheng et al., 2020)	Xception-71 ResNet-50 FPN ResNet-50 FPN Xception-71	33.8	-	- 37.1	-	- 37.5	77.7	47.2 40.2	-	-	-	-	- 41.0	-	- 40.0	78.4	50.0 44.4	-	-	-	-	- 31.3	-	- 33.7	76.5	42.9 33.8	-	-Two StageJSIS-Net (de Geus et al., 2018)		26.9	72.4	35.7	29.3	72.1	39.2	23.3	73.0	30.4AUNet (Li et al., 2019)	ResNet-50 FPN	38.6	76.4	47.5	46.2	80.2	56.2	27.1	70.8	34.5AdaptIS (Sofiiuk et al., 2019)	ResNet-50 FPN	41.8	78.4	51.3	47.8	81.3	58.0	32.8	74.1	41.1Panoptic FPN (Kirillov et al., 2019a)	ResNet-50 FPN	39.0	-	-	45.9	-	-	28.7	-	-OANet (Liu et al., 2019)	ResNet-50 FPN	39.0	77.1	47.8	48.3	81.4	58.0	24.9	70.6	32.5SOGNet (Yang et al., 2020)	ResNet-50 FPN	43.7	-	-	50.6	-	-	33.6	-	-SpatialFlow (Chen et al., 2020)	ResNet-50 FPN	39.3	-	-	45.1	-	-	30.5	-	-Single-Shot (Weber et al., 2020)	ResNet-50 FPN	32.4	-	-	34.8	-	-	28.6	-	-Naiyu Gao et al. (Gao et al., 2021)	ResNet-50 FPN	40.2	-	-	45.3	-	-	32.3	-	-OCFusion (Lazarow et al., 2020)	ResNet-50 FPN	42.5	-	-	49.1	-	-	32.5	-	-UPSNet (Xiong et al., 2019)	ResNet-50 FPN	42.5	78.5	52.5	48.1	79.2	59.2	33.9	77.4	42.3Ours*	ResNet-50 FPN	40.3	78.2	50.0	44.6	78.8	55.0	33.9	77.4	42.3Method	Backbone	PQ	SQ	RQ	PQTh	SQTh	RQTh	PQSt	SQSt	RQStUPSNet (Xiong et al., 2019)	ResNet-101 FPN	46.7	80.5	56.9	53.2	81.2	64.6	36.9	79.5	45.4Ours*	ResNet-101 FPN	44.9	80.5	54.7	50.3	81.6	60.9	36.8	78.4	45.3
Table 4: Panoptic segmentation results on the Cityscapes Val dataset. Superscripts ‘Th’ and ‘St’denote numbers for thing and stuff classes respectively.
Table 5: Results of experiments with varying sizes of the compositional model. Clusters representthe count of mixture components. As seen in the table, the RQ and Recall metrics improve asnumber of clusters are increased. To maintain speed of training, we show all results using a composi-tional model of 320 components (C × 4 where C is the number of classes). However, performanceimprovements can be expected if size of the compositional model is increased.
Table 6: Results of experiments to compare the data augmentation to increase training data size perimage for the detection branch of the instance head. Including a higher count of background RoIs,allows the network to learn the large variation of background RoIs with different IoU thresholds. Asseen in the table, showing more of these RoIs improves performance of the detection branch.
Table 7: The compositional model is trained using features that are sampled from both foregroundobjects and background regions. Inclusion of background features helps the compositional modeldetermine if some position on the RoI lattice corresponds to the background. The results of A/Btesting that justify the inclusion of background features are shown in this table. As seen from themetrics, including background features improves detection performance.
Table 8: A comparison of instance detection performance using the ResNet-50 backbone of ourapproach against the baseline (Xiong et al., 2019) for some classes in the MS-COCO 2018 Valdataset. The Person and Car classes have the highest frequency in the dataset and have an improvedrecall of 2.59 and 2.99 percentage points respectively. Smaller sized instances (by area) of categoriessuch as Traffic Light, Bird, Book and fruits are also detected well.
