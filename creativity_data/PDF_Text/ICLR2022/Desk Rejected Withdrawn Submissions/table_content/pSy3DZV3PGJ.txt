Table 1: Performance of various models on theTables 1-4 show the performance of all themodels in comparison on the four benchmarkdatasets. On the CityScapes dataset, the pro-posed SMTL, L-SMTL, L-SMTLc, and somebaseline methods (i.e., Cross-stitch, MTAN,and NDDR-CNN) all achieve safe multi-tasklearning (i.e., η = 100), which indicates thattheir performance is better than that of the STLmodel in all tasks. In addition, the proposedSMTL model achieves the best overall relativeimprovement ∆I, which demonstrates its effec-tiveness. On the NYUv2 and PASCAL-Contextdatasets, none of the baselines can achieve safemulti-task learning, but the proposed meth-ods (i.e., SMTL, SMTLc, and L-SMTLc) canCityScaPes validation dataset. ↑ (1) indicates thehigher (lower) the result, the better the perfor-mance. The green color indicates that the cor-resPonding method Performs better than the STL
Table 2: Performance of various models on the NYUv2 validation dataset. ↑ (J) indicates the higher(lower) the result, the better the performance. The green color indicates that the correspondingmethod performs better than the STL method and the red color indicates oppositely.
Table 3: Performance of various models on the PASCAL-Context validation dataset. ↑ Q) indi-cates the higher (lower) the result, the better the performance. The green color indicates that theCorresPonding method performs better than the STL method and the red color indicates oppositely.
Table 4: Performance of various models on the Taskonomy validation dataset. ↑ (1) indicates thehigher (lower) the result, the better the performance. The green color indicates that the correspond-ing method performs better than the STL method and the red color indicates oppositely.
Table 5: {αt} learned on four MTL datasets. ‘SS’ stands for the semantic segmentation task, ‘DE’denotes the depth estimation task, ‘SNP’ is for the surface normal prediction task, ‘HPS’ correspondsto the human parts segmentation task, ‘SE’ stands for the saliency estimation task, ‘KD’ stands forthe keypoint detection task, and 'ED' denotes the edge detection task.
Table 6: Performance and learned αt of SMTL on the CityScapes and NYUv2 validation datasets,where values in the normal font correspond to the performance of SMTL through the single-levelformulation (i.e., problem (2)) and values in the italic font are those through the bi-level formulation(i.e., problem (6)). ↑ (J) indicates the higher (lower) the result, the better the performance.
Table 7: Settings of batch size for all the models on the four datasets.											Dataset	STL	DMTL	Cross-stitch	MTAN	NDDR-CNN	AdaShare	AFA	SMTL	L-SMTL	SMTLc	L-SMTLcCityScapes	180	180	100	80	80	120	150	70	70	70	70NYUv2	8	4	4	4	4	4	4	4	4	4	4PASCAL-Context	40	40	24	20	18	32	8	18	18	15	15Taskonomy	230	230	120	130	100	180	40	100	100	90	90D.3 Evaluation MetricsOn the PASCAL-Context dataset, by following (Maninis et al., 2019), the semantic segmentation isevaluated by the mean Intersection over Union (mIoU) and on the other three datasets, by following(Sun et al., 2020), this task is additionally evaluated by the Pixel Accuracy (Pix Acc). For the depthestimation task, the absolute error (Abs Err) and relative error (Rel Err) are used as the evaluationmetrics. For the surface normal prediction task, the mean and median angle distances between theprediction and ground truth of all pixels are used as measures. For this task, the percentage of pixels,whose prediction is within the angles of 11.25°, 22.5°, and 30°to the ground truth, is used as anothermeasure. For the keypoint detection and edge detection tasks, the absolute error (Abs Err) is usedas the evaluation metric. For the human parts segmentation task, the mIoU is used as the measure.
Table 8: Training time per epoch for all the models on the CityScaPeS dataset.
