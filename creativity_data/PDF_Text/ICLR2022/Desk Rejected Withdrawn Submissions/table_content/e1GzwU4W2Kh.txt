Table 1: Experimental Results Showing the Quality of SolutionsIndex	Constrained	Latency (ms)	Energy (mJ)	Chip Area (mm2)	Error (%)	CostHW	Global Loss	Unconst. Original	69.23	37.00	2.53	4.10 ± 0.16	21.84	0.632	Latency	43.99	21.79	2.10	4.20 ± 0.07	13.87	0.624A	Energy	51.98	29.18	2.53	4.38 ± 0.17	17.44	0.630	Chip Area	64.00	34.82	2.53	4.05 ± 0.06	20.56	0.629	All	63.72	12.09	1.86	4.12 ± 0.18	13.29	0.623	Unconst. Original	49.65	27.53	2.53	4.22 ± 0.06	16.67	0.638	Latency	48.02	27.33	2.53	4.27 ± 0.09	16.41	0.644B	Energy	95.02	24.45	1.89	4.05 ± 0.10	20.76	0.648	Chip Area	54.74	29.81	2.53	4.11 ± 0.13	17.96	0.645	All	41.32	08.59	1.86	4.35 ± 0.05	09.50	0.629	Unconst. Original	56.11	29.81	2.53	4.11 ± 0.10	18.13	0.662	Latency	51.81	09.49	1.89	4.38 ± 0.11	11.06	0.645C	Energy	44.78	24.38	2.53	4.12 ± 0.02	15.15	0.656	Chip Area	53.37	26.63	2.10	4.43 ± 0.07	16.44	0.668	All	41.53	08.81	1.86	4.48 ± 0.20	09.59	0.645*Bold colored numbers indicate that they are under constraint of the same colored non-bold numbers.
Table 2: Experimental Results for ImageNetConstrained	Latency (ms)	Energy (mJ)	Chip Area (mm2)	Error (%)	CostHW	Global LossUnconstrained	165.98	47.35	2.56	25.46	29.37	2.043Latency	84.47	36.20	2.40	26.22	20.64	2.053Energy	78.75	32.23	2.40	27.94	18.68	2.1308Under review as a conference paper at ICLR 20223200	3200	3---Latency---Global LossssoL labolGssoL labolG)sm( ycnetaL050210ssoL labolG
Table 3: Experimental Results on CIFAR-100 DatasetConstrained		Latency (ms)	Energy (mJ)	Chip Area (mm2 )	Error (%)	CostHW	Global Loss	(60 fps)	16.34	8.82	2.23	22.90 ± 0.51	6.81	1.58816.6 ms		7.23	3.68	1.95	26.21 ± 0.35	3.92	1.718QQQ EL	(30 fps)	33.24	19.38	2.53	20.89 ± 0.42	12.27	1.53933.3 ms		29.94	15.58	2.10	21.37 ± 0.22	10.32	1.568Z? Z? Z?	C	(15 fps)	57.76	31.94	2.53	20.33 ± 0.21	18.95	1.51866.6 ms		43.45	22.40	2.10	20.73 ± 0.11	13.98	1.560C Results on CIFAR- 1 00 DatasetTable 3 presents the co-exploration performed on CIFAR-100 dataset. For the three latency targetvalues (16.6 ms, 33.3 ms, and 66.6 ms) ConCoDE found a number of solutions. For each targetvalue, we chose one at the lowest error, and another at the lowest CostHW. They all satisfy the con-straints, with different trade-offs. Interestingly, the pair of accelerator designs found from 33.3 msand 66.6 ms constraints were identical. This is because when the dataset and the backbone networkarchitecture is decided, the number of channels as well as the spatial dimension of the activationmap are fixed. These values govern most of the parallelism that can be exploited by the acceleratorand thus the solutions ended up in a convergent evolution.
Table 4: Details of Selected SolutionsConstraint		Evaluated Metrics			Network Stats		Hardware Design					Error (%)	Latency (ms)	CostHW	#Layers	#Parameters	PEX	PEY	RF D	ataflow16.6 ms	(60 fps)	4.58 ± 0.11	15.86	7.22	13	0.88 M	16	16	4	WS		5.31 ± 0.05	08.66	04.27	7	0.36 M	16	8	4	WS33.3 ms	(30 fps)	4.30 ± 0.17	31.86	10.73	16	1.0M	20	8	4	WS		4.70 ± 0.11	29.67	07.14	10	0.62 M	12	8	64	RS66.6 ms	(15 fps)	4.00 ± 0.11	60.90	19.66	18	1.5M	16	16	4	WS		4.19 ± 0.12	63.81	13.27	17	1.1M	12	8	64	RS14Under review as a conference paper at ICLR 2022In the co-exploration problem, the task of hardware accelerator design is to decide those factors,such that it fits well with the neural architecture being co-designed. Among many design factors, wechose number of PEs along the two dimensions, register file size, and dataflow as our search space.
