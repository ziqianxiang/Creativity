Table 1: Different PAC-Bayes bounds on er(Q). Meta-Learning Bound = Empirical Error +Environment-Level Complexity + Task-Level Complexity. n is the number of observed tasks,m is the number of samples in Si (i ∈ [n]). P and Q represent hyperprior and hyperposteriorrespectively, both of which are probability measures over the set of all priors. P is the prior sampledrandomly from P, and Qi = Q(Si, P) is the posterior for the i-th training task obtained by trainingPAC-Bayes single-task algorithm with the data Si and the prior P. C1,C2 > 1 are two constants.
Table 2: Comparison of different PAC-Bayes bounds on 20 test tasks (the ± shows the 95% confi-dence interval) in 100/200-SWaP shuffled pixels environment and permuted labels environment.
Table 3: Explicit forms of different PAC-Bayes bounds on er(Q). Meta-Learning Bound =Empirical Error + Environment-Level Complexity + Task-Level Complexity. n is the num-ber of observed tasks, m is the number of samples in Si (i ∈ [n]). P, Q ∈ M1(M1(H))are hyperprior and hyperposterior respectively. P, Qi = Q(Si, P) ∈ M1 (H) are the priorand the posterior for the i-th training task. δ ∈ (0, 1) is the confidence level. In MLAP-Sbound, ∆i = KL(Q∣∣P) + EP〜QKL(Q∕∣P). In our quadratic meta-learning bound, ∆ =KL(Q∣∣P) + EP〜Q Pn=I KL(QiIIP).In λ bounds, λ ∈ (0, 2).__________________________Different Bounds	Empirical Error	Environment-Level Complexity	Task-Level Complexity(Pentina & Lampert, 2014)	er(Q)	√ (KL(QIIP) +1 + ln 2)	KL(Q∣∣P)+Pi=ι Ep~qKL(Qil∣P) ∣	1	∣	1	片 2,~ n√m	十 8√m 十 n√m	δMLAP-M (Amit & Meir, 2018)	er(Q)	∕KL(Q∣∣P) + ln 华 V	2(n-1)			1 Pn	/KL(Q∣∣P) + Ep~q KL(Qi∣∣P) + ln ¾m n 乙i=1 V	2(m-1)MLAP-S (Amit & Meir, 2018)	er(Q)	/KL(Q∣∣P) + ln 茅 V	2(n-1)			n pn=1[ *e + JT吕 erg.s,)]PACOH (Rothfuss et al., 2021)	er(Q)	√n KL(QIIP) + 81n	KL(Q∣∣P)+Pi=ι Ep~qKL(Qil∣P) +(	1	+	1 ∙b 丁 n√m	∖ 8n√m	√n	δ)λ bound (Liu et al., 2021)	分(Q) 1-λ∕2	∕KL(Q∣∣P) + ln 华 V	2(n-1)			1 Pn	KL(Q∣∣P) + Ep~q KL(Qi∣∣P) + ln 4n√√m n 乙i=1	mλ(1-λ∕2)classic bound (ours)	er(Q)	JKL(Q∣∣P) + in⅞^ V	2n		q∕κL(Q∣P) + Ep~Q Pi=ι KL(Qi∣∣P)+ln 2^ V	2nmquadratic bound (ours)	er(Q)	qq KL(Q∣∣P) + 1^^ V	2n		∆+ln 4√nm ι C ∕~c∖ , ∆+ln 4√nm /∆+in 4√nm + nm δ +2V er(Q) + Anm^ ʌ/ +2nmδλ bound (ours)	分(Q) 1-λ∕2	qq KL(Q∣∣P) + 1^^ V	2n	KL(Q∣∣P) + Ep~q Pi=ι KL(Qi∣∣P)+ln 4√nm nmλ(1-λ∕2)Appendix B Proof of Our Theoretical ResultsB.1 Proof of Generalized PAC-Bayes-kl B ound from i.i.d. Setting toIndependently but Non-identically Distributed Meta-Learning SettingWe first give the proof of Lemma 1 in the main paper. Actually, the proof is proceeded with almost
