Table 1: Baseline evaluations on CIFAR-10. Perturbations have '∞-norm bounded above by16/255, and poison budget is 1% of training images. Each number denotes an average (and std.
Table 2: The effect of poison budget. Experiments on CIFAR-10 with ResNet-18 models (Heet al., 2016). Perturbations have '∞-norm ≤ 16/255. Each number denotes an average (and std.
Table 3: Ensembles consisting of copies of the same architecture (ResNet-18). S denotes the sizeof the ensemble, and T denotes the retraining factor. Experiments are conducted on CIFAR-10,perturbations have '∞-norm bounded by 16/255, and the attacker can poison 1% of training images.
Table 4: Black-box attacks: First row: Attacks crafted on a single ResNet-18 and transferred.
Table 5: ImageNet evaluations. Attacks are conducted on ResNet-18 models and perturbationshave '∞-norm bounded above by 16/255. The high standard errors are due to the high variance ofthe sampling of source/target pairs, and limited number of runs to maintain computational feasibility.
Table 6: Benchmark results on CIFAR-10. Comparison of our method to popular “clean-label” attacks. Results averaged over the same source/target pairs with = 16/255 and poison budget 1%.				Attack	ResNet-18	MobileNetV2	VGG11	AverageHidden-Trigger Backdoor (Saha et al., 2019)	3.50%	3.76%	5.02%	4.09%Clean-Label Backdoor (Turner et al., 2019)	2.78%	3.50%	4.70%	3.66%Sleeper Agent (Ours)	50.72%	58.21%	57.86%	55.59%same budget, and the same ε-bound (Note: clean-label backdoor originally did not use '∞ bounds,so we adjust the opacity of their perturbations to ensure the constraint is satisfied). We then train arandomly initialized network from scratch on these poisons and evaluate success over 1000 patchedtarget images. We test three popular network architectures and find that our attack significantlyoutperforms both methods and is the only backdoor method to exceed single digit success rates,confirming the findings of Schwarzschild et al. (2020) on the fragility of these existing methods.
Table 7: Defenses. Experiments are conducted on CIFAR-10 with ResNet-18 models, perturbationshave '∞-norm bounded above by 16/255, and poison budget is 1% of training images.
Table 8: Random poisons. Experiments are conducted on CIFAR-10 with ResNet-18 models.
Table 9: Ablation studies. Investigation the effects of random patch-location, retraining, and dataselection. Experiments are conducted on CIFAR-10 with ResNet-18 models, perturbations have'∞-norm bounded above by 16/255, and poison budget is 1% of training images.
Table 10: Baseline evaluations using random patches on CIFAR-10. Perturbations have '∞ -normbounded above by 16/255, and poison budget is 1% of training images. Each number denotes anaverage (and standard error) over 24 independent crafting and training runs along with randomlysampled source/target class pairs. Each run has a unique patch generated randomly. Figure 3 (left)shows a sample random patch we use for the experiments presented in this table.
Table 11: The effect of patch size. Experiments are conducted on CIFAR-10 and ImageNetdatasets with ResNet-18 models.
Table 12:	ImageNet evaluations on MobileNet-v2. Perturbations have '∞ -norm bounded aboveby 16/255, and the patch size is 30 × 30.
Table 13:	Ablation studies. Investigation of the effects of retraining factor T . ExPeriments areconducted on CIFAR-10 with ResNet-18 models, perturbations have '∞-norm bounded above by16/255, and the Poison budget is 1% of the training images.
