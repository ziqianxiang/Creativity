Table 1: Performance on the NYUv2 validation dataset with three tasks: 13-class semantic seg-mentation, depth estimation, and surface normal prediction. The best results for each task on eachmeasure are highlighted in bold. ↑ (J) indicates that the higher (lower) the result, the better theperformance.											Weighting Strategy	Segmentation		Depth		Surface Normal					∆p↑	∆tJ	mIoU↑	Pix Acc↑	Abs ErrJ	Rel ErrJ	Angle Distance		Within t°									MeanJ	MedianJ	11.25↑	22.5↑	30↑		EW	53.91	75.56	0.3840	0.1567	23.6338	17.2451	34.94	60.65	71.81	+0.00	×1.00GradNorm	53.81	75.35	0.3863	0.1556	23.6106	17.2565	34.98	60.58	71.76	-0.06	×1.82UW	53.15	75.41	0.3817	0.1576	23.6487	17.2040	34.98	60.71	71.80	-0.24	×1.01MGDA	53.66	75.37	0.3864	0.1610	23.4757	16.9912	35.44	61.17	72.16	-0.35	×2.64DWA	53.33	75.42	0.3834	0.1556	23.5806	17.1242	35.18	60.88	71.91	+0.07	×1.00PCGrad	53.34	75.43	0.3857	0.1600	23.2293	16.6966	36.09	61.80	72.66	+0.12	×2.10GradDrop	53.80	75.56	0.3857	0.1587	23.8726	17.1406	35.10	60.72	71.60	-0.33	×1.84IMTL	52.90	74.88	0.3883	0.1632	23.0534	16.5304	36.30	62.20	73.08	-0.35	×1.82GradVac	53.52	75.43	0.3840	0.1559	23.2892	16.8601	35.67	61.53	72.46	+0.48	×2.11RLW (Uniform)	54.09	75.78	0.3826	0.1563	23.6272	17.2711	34.73	60.67	71.87	+0.17	RLW (Normal)	54.19	75.98	0.3789	0.1570	23.1984	16.7944	35.71	61.74	72.77	+1.02	RLW (Dirichlet)	53.54	75.45	0.3834	0.1547	23.6392	17.0715	35.28	60.92	71.88	+0.27	×1.00RLW (Bernoulli)	53.72	75.62	0.3850	0.1610	23.1413	16.6591	36.08	61.98	72.86	+0.28	
Table 2: Performance on the PASCAL-Context validation dataset with four tasks: 21-class se-mantic segmentation (abbreviated as SS), 7-class human parts segmentation (abbreviated as HPS),saliency estimation, and surface normal prediction. The best results for each task on each measureare highlighted in bold. ↑ (J) indicates that the higher (lower) the result, the better the performance.
Table 3: Performance on two multilingual problems, i.e. POS and PI from the XTREME bench-mark. The best results for each language are highlighted in bold.
Table 4: Performance under the MTAN architecture on the NYUv2 validation dataset with threetasks: 13-class semantic segmentation, depth estimation, and surface normal prediction. The bestresults for each task on each measure are highlighted in bold. ↑ Q) indicates that the higher (lower)the result, the better the performance.
Table 5: A summary of SOTA weighting strategies from a perspective of loss weighing. * meanswhether a convergence analysis (abbreviated as Conv.) is provided in the original paper. f denotesthat the corresponding weighting strategy needs not to compute gradients (abbreviated as Not Grad.)for generating loss weights λ.
Table 6: Performance of different loss weighting strategies with the cross-stitch architecture on theNYUv2 validation dataset with three tasks: 13-class semantic segmentation, depth estimation, andsurface normal prediction. The best results for each task on each measure are highlighted in bold. ↑(1)means the higher (lower) the result, the better the performance.
Table 7: Performance of different loss weighting strategies with the NDDR-CNN architecture onthe NYUv2 validation dataset with three tasks: 13-class semantic segmentation, depth estimation,and surface normal prediction. The best results for each task on each measure are highlighted inbold. ↑ (J) means the higher (lower) the result, the better the performance.
Table 8: Performance on the CityScapes validation dataset with two tasks: 7-class semantic seg-mentation and depth estimation. The best results for each task on each measure are highlighted inbold. ↑ (J) means the higher (lower) the result, the better the performance.
Table 9: Average classification accuracy (%) of different methods on the CelebA dataset with fortytasks. The best results for each task on each measure are highlighted in bold.
Table 10: Classification accuracy (%) of different methods on the Office-31 and Office-Homedatasets. The best results for each task are highlighted in bold.
Table 11: The numbers of training, validation, and test data for each language in each task from theXTREME benchmark.____________________________________________________________________________	NER	POS	NIL	PIen	20.0K+10.0K+10.0K	6.9K+1.8K+3.2K	392.7K+2.5K+5.0K	49.4K+2.0K+2.0Kzh	20.5K+10.3K+10.3K	4.0K+0.5K+2.9K	392.7K+2.5K+5.0K	49.4K+2.0K+2.0Kde	20.0K+10.0K+10.0K	-	392.7K+2.5K+5.0K	49.4K+2.0K+2.0Kes	20.0K+10.0K+10.0K	-	392.7K+2.5K+5.0K	49.4K+2.0K+2.0Kte	-	1.0K+0.1K+0.1K	-	-vi	-	1.4K+0.8K+0.8K	-	-Results The results on the NLI and NER multilingual tasks are shown in Table 12. The empiricalobservations are similar to those on the POS and PI tasks shown in Table 3. Specifically, the RLWmethod with all the distributions outperforms EW in the two multilingual problems. In addition,RLW achieves comparable and even better performance than those baseline methods. For example,on the NLI multilingual problem, RLW achieve the highest average accuracy.
Table 12: Performance on two multilingual problems, i.e., NLI and NER from the XTREME bench-mark. The best results for each language are highlighted in bold.
Table 13:	Performance on the NYUv2 validation dataset using ResNet-18 as the backbone network.
Table 14:	Performance on the NYUv2 validation dataset using ResNet-101 as the backbone network.
Table 15:	Performance on the NYUv2 validation dataset with three tasks: 13-class semantic seg-mentation, depth estimation, and surface normal prediction. The best results for each task on eachmeasure are highlighted in bold. ↑ (1) indicates that the higher (lower) the result, the better theperformance.
