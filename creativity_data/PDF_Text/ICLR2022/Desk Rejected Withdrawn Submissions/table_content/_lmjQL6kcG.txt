Table 1: Redundancy R of pretrained features. Meth-ods with an MLP projector obtain lower channel re-dundancy and better transferability.
Table 2: Unseen class generalization tasks. We report Top-1 accuracy on eval-D to compare the transferabilityof SL-MLP, Byol and SL on various backbones. SL-MLP and Byol share the same MLP projector.
Table 3: Linear evaluation performance of different supervised learning methods on 12 classification datasetsin terms of top-1 accuracy. All models are pretrained for 300 epochs with the same code base.
Table 4: Object detection and instance segmentation fine-tuned on COCO using Mask-RCNN (R50-FPN).
Table 5: Empirical analysis of structural design of the MLP projector. We incrementally add different compo-nent to the MLP projector. We pretrain models over 100 epochs and set the output dimension to 2048.
Table 6: Quantitative analysis of structural design of inserted MLP, including discriminative ratio on pre-D,Feature Mixtureness Î  and feature redundancy R. (b-e) denote experiments in which different components areadded on the SL baseline (a). When incrementally adding components of the MLP into SL, the distriminativeratio on pre-D and feature redundancy will decrease while the Feature Mixtureness will increase.
Table 7: Top-1 linear evaluation accuracy on eval-D when pretraining the model in pre-D by cosine softmaxcross-entropy loss.
Table 8: 12-domains datasets used for downstream image classification.
Table 9: Linear evaluation results and top-1 accuracy during pretraining on SL and SL-MLP. We removethe MLP in SL-MLP for linear evaluation, only the fixed backbones of SL and SL-MLP are used. For top-1accuracy during pretraining, accuracy of the whole SL-MLP is reported.
