Table 1: Image classification results for different vision transformers on ImageNet1k dataset. Allthe variants of deit-la, including permuteformer, are trained by us.
Table 2: Image classification results for different vision transformers on CIFAR-100 dataset. All ofthese models are trained by us. APE denotes the absolute positional embedding.
Table 3: Object detection results for different detection transformers on COCO benchmark under both50 training epoch schedule and 108 epoch schedule.
Table 4: Classification results on CIFAR-100 dataset under different maximum rippling distancesRmax for RIPPLE, compared to several model variants. The speed is measured by the number ofimages processed per second with a batch size of 64. ripple w/o sbt represents ripple attentionwhose spatial weights are generated through a softmax function instead of stick breaking transforms(SBT). “-" indicates not applicable.
Table 5: Runtime complexity comparisons between different attention variants, with respect toan image with H × W patches (in the first row) and with respect to the number of patch tokensT := H X W (in the second row). tRipple-softmax indicates the complexity if we would like toimplement ripple-like mechanisms in vanilla softmax attention.
Table 6: Classification results on ImageNetIk dataset under different choices of feature maps. tindicates that our feature map design without fully connected network is identical to T2R (Kasaiet al., 2021). * denotes the model does not fully converge.
Table 7: Classification results on ImageNet1k dataset under different numbers of rippling layersfor ripple. The speed is measured by the number of images processed per second with a batch sizeof 64 on a single NVIDIA V100 GPU machine, averaged by 5 runs.
Table 8: Classification results onCIFAR-100 dataset under differ-ent parameterization schemes ofspatial weights.
