Table 1: Mean normalized area under the learning curve and SD over 8 training runs of CompositeTD3 in the noisy reward experiments.
Table 2: Mean maximum return and SD over 8 training runs for the noisy reward experiments.
Table C.1: Comparison of convergence speed between Tabular Q-learning and Tabular CompositeQ-learning for exemplary runs on the MDP given in Figure C.1a with n = 4.
Table D.1: Configuration space of the hyperparameter optimization. For the full Q-function in allapproaches, we used the optimized learning rate of 10-3 as in (Fujimoto et al., 2018). Hyperparam-eters denoted by * were optimized individually for all approaches.
Table E.1: p-values for Welch’s t-test on the mean area under the learning curve for 8 different runsof Composite TD3 and all other approaches in the vanilla reward experiments. Subparts (a) and (b)correspond to those in Figure 3.
Table E.2: p-values for Welch’s t-test on the mean area under the learning curve for 8 different runsof Composite TD3 and all other approaches in the noisy reward experiments.
