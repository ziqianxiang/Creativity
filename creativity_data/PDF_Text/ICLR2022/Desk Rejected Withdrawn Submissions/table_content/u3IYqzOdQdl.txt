Table 1: CIFAR-10. For Accuracy, higher is better. For NLL and ECE, lower is better. All modelsuse the WRN-28-10 architechture. We find that in the low parameter regime, MixtureEnsemblesoutperforms the baseline on 2 out of 3 metrics. In the high parameter regime, we match or exceednaive ensembles on all metrics with 17% fewer parameters.
Table 2: Accuracy on CIFAR-100 (clean) and CIFAR-100-C (corrupted). All ensembles were of size4 and use the same WRN-28-10 architechture, excpet for thin MixtureEnsembles, which use WRN-28-5. We find that our WRN-28-10 model is competitive with MIMO at 36.5 million parameters,but our WRN-28-5 model outpeforms MIMO with the same inference footprint. With more param-eters our MixtureEnsembles WRN-28-10 achieves the highest performance overall while reducingparameter count by 17% relative to naive ensembles. Baseline corresponds to a single WRN-28-10model trained with cross-entropy. FLOPS refers to inference-time FLOPS, relative to the baseline.
Table 3: Negative Log-Likelihood and Expected Calibration Error on the CIFAR-100 test set. Loweris better. Once again we are competitive with MIMO at lower parameter counts, and are competitivewith naive ensembles at 17% lower parameter count. Baseline corresponds to a single WRN-28-10model trained with cross-entropy.
Table 4: NLL and ECE for CIFAR-C dataset of corrupted images from CIFAR. Lower is better.
Table 5: ImageNet Results on ResNet-50. MixtureEnsembles are competitive with other methodsin the low-parameter regime. In the high-parameter regime, we outperform Naive Ensembles with50% fewer parameters.
Table 6: Diversity. MixtureEnsembles are more diverse than BatchEnsembles, the other sharedparameter model. Although our 120M model is slightly less diverse than Naive Ensembles, theperformance is higher, indicating that average member accuracy is higher. This suggests that Mix-tureEnsembles are also an effective regularizer.
