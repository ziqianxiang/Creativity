Table 1: Quantitative comparison between different knowledge distillation methods. Numbers inthe brackets indicate the ratio of compression and acceleration. A lower FID indicates better per-formance. ∆ indicates the relative increment compared with the student trained without knowledgedistillation (higher is better). Each experiment is averaged from 8 trials.
Table 2: Ablation studies of the three main modules inour method are Horse→Zebra with CycleGAN students.
Table 3: Quantitative comparison between different knowledge distillation methods on Cityscapeswith Pix2Pix. Numbers in the brackets indicate the ratio of compression and acceleration. A highermIoU indicates better performance. ∆ indicates the relative increment compared with the studenttrained without knowledge distillation (higher is better). Each experiment is averaged from 8 trials.
