Table 1: Comparison between MSE loss andMCE loss for Resnet-18 with weights quantizedto 4-bit.
Table 2: Impact of the choice of the basis di-mension in 3 Ã— 3 layers on final accuracy withweights quantized to 4-bit.
Table 3: LatticeQ with bias correction on ImageNet. For 8-bits activations we use naive per-layer ac-tivation quantization. For 4-bit activations, we use our quantile per-channel approach. For weights,we use per-channel quantization and bias correction. We compare our results with results that wegenerated from the source code of Banner et al. (2019), using per-channel quantization and biascorrection. We also compare with paper results from OMSE (Choukroun et al., 2019).
Table 4: LatticeQ per-channel baseline quantization of weights and activations.
Table 5: LatticeQ per-layer baseline quantization of weights and activations.
Table 6: Comparison between baseline per-channel LatticeQ and baseline per-channelCubic LatticeQ (scalar quantization).
Table 7: Baseline LatticeQ memory cost. Scalar compression rate is the compresion rate of a scalarquantization method with the same bitwidth, such as Cubic LatticeQ.
Table 8: Quantiles chosen for Resnet and Densenet activation quantization.
Table 9: Quantiles chosen for VGG activation quantization.
Table 10: Quantiles chosen for Mobilenet activation quantization.
Table 11: LatticeQ per-channel weight-only quantization with bias correction.
