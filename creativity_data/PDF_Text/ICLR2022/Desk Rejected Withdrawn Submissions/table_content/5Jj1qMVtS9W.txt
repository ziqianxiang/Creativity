Table 1: Synthetic quadrants data results in test setALGORITHM	BCE	MAE	EMDCONDOR CORAL CATEGORICAL	0.0768 ± 0.0100	0.0167 ± 0.0153	0.0799 ± 0.0526 0.5074 ± 0.0754	0.0733 ± 0.0416	0.4080 ± 0.0377 1.3438 ± 0.0458	0.0200 ± 0.0173	1.0318 ±	0.02673.2	MNIST as an ordinal datasetDepending on the application, MNIST can be considered a categorical problem or an interval re-gression problem. If the digits are used for license-plate recognition, then the problem is categoricalsince there is no notion of “close” errors. By contrast, if the digits are used for GPS coordinatesor postal codes, then the ordering and distance between numerals becomes relevant and categoricalclassification is no longer the most appropriate framing of the task. It is valid to treat interval re-gression as an ordinal problem since the latter assumes less structure on the dataset, although it isrecommended to exploit the interval scale. Here we treat MNIST as ordinal data for the purpose ofbenchmarking our ordinal algorithms, while acknowledging that it should likely be treated as eithera categorical classification or interval regression as dictated by the specific real-world applicationsetting. The MNIST data are split into training, validation and test sets of 55K, 5K and 10K im-ages, respectively. We utilize a convolutional neural network with two convolutional layers of 64and 32 filters respectively and a kernel size of 3, before flattening and passing to the appropriateoutput layer and loss function for our three models (CONDOR, CORAL, Categorical). Training isperformed with the Adam optimizer, a maximum of 100 epochs and an early-stopping patience of10. The results in Table 2 indicate that CONDOR demonstrates superior performance in terms ofboth BCE and EMD, while the Categorical approach performs marginally better in MAE.
Table 2: MNIST digit image classification on test setALGORITHM	BCE	MAE	EMDCONDOR CORAL CATEGORICAL	0.1784 ± 0.0043	0.0596 ± 0.0027	0.0818 ±	0.0065 1.2724 ± 0.0139	0.4583 ± 0.0028	0.7501 ±	0.0214 5.5424 ± 0.0013	0.0592 ± 0.0042	3.0638 ±	0.00133.3	NLP on Amazon reviews datasetHere we consider a natural language processing (NLP) dataset consisting of 99,025 (non-duplicateand non-empty) Amazon Pantry text reviews with their corresponding one to five star ratings (Niet al., 2019). We split the data to have a test set with 10,000 examples. For the neural networkarchitecture, we use the fixed and pre-trained Google universal sentence encoder (Cer et al., 2018)and append a dense layer with 64 ReLu-activated neurons and a dropout of 0.1, followed by the6Under review as a conference paper at ICLR 2022appropriate output layer and loss function for each model. Training is performed with the Adamoptimizer, a maximum of 100 epochs and an early-stopping patience of 10 with a validation split of0.2. The results in Table 3 demonstrate that CONDOR provides the strongest performance in thisbenchmark across all three performance metrics.
Table 3: Amazon product review text classification on test setALGORITHM	BCE	MAE	EMDCONDOR	0.7807 ± 0.0113	0.3180 ± 0.0047	0.4582 ± 0.0050CORAL	0.8095 ± 0.0098	0.3263 ± 0.0052	0.4726 ± 0.0041CATEGORICAL	2.4663 ± 0.4678	0.4195 ± 0.0074	1.6906 ± 0.24223.4	GRU-D for COVID-19 prognosticationThis study adheres to a research protocol approved by the [CENSORED FOR DOUBLE BLINDPEER REVIEW]Institutional Review Board. Here we extend the results from Sankaranarayananet al. (2021) to progress from their binary classification predicting mortality to ordinal regressionpredicting severity of outcomes. Namely, this clinical dataset includes two binary severity out-comes: an indicator variable for mechanical ventilation or extracorporeal membrane oxygenation(ECMO), as well as an indicator variable of whether patient death occurred. From these, a clearthree point ordinal scale can be constructed whereby a patient is scored a zero when they have nosevere outcome, a one when they experienced the severe outcome of ventilation or ECMO, and atwo corresponding to death (with or without prior ventilation or ECMO). Sankaranarayanan et al.
Table 4: COVID-19 severity prediction results in prospective test setALGORrTHM	BCE	MAE	EMDCONDOR	0.6526	±	0.0004	0.2986	±	0.0044 CORAL	0.6711	±	0.0000	0.3021	±	0.0015 CATEGORICAL 1.1075 ± 0.0019 0.3076 ± 0.0021	0.4151 ± 0.0063 0.4261 ± 0.0023 0.8185 ± 0.0010The dataset is split into a training/validation set of 9,435 patients who tested positive for COVID-19prior to December 15 2020 by PCR test, and a prospective testing set of 2,372 patients who testedpositive on or after that date. For training we use an identical 90/10 training/validation split onthe 9,435, which facilitates early stopping with patience. The results in Table 4 demonstrate thatCONDOR is superior in all of evaluation metrics. Furthermore, the CONDOR-based GRU-D modelhas a prospective test set AUROC of 0.9038 ± 0.0025 for the mortality prediction subtask, whichis greater than the 0.901 reported in Sankaranarayanan et al. (2021) wherein the authors trained thealgorithm as a binary classifier specifically for mortality prediction. This demonstrates that there isno loss of mortality prediction performance when building a DNN to address the more challengingtask of prognostication.
Table 5: Accuracy in benchmark test setsALGORITHM	Quadrants	MNIST	Amazon	COVID-19CONDOR	0.9900 ± 0.0100	0.9805 ± 0.0004	0.7498 ± 0.0032	0.7250 ± 0.0033CORAL	0.9333 ± 0.0379	0.6381 ± 0.0037	0.7343 ± 0.0036	0.7175 ± 0.0015CATEGORICAL	0.9933 ± 0.0058	0.9845 ± 0.0008	0.7299 ± 0.0037	0.7244 ± 0.0016Surprisingly, in Table 5 we find in some benchmarks the ordinal CONDOR method provided highercategorical accuracy than the networks trained using categorical cross entropy to specifically opti-mize categorical performance. We attribute this remarkable finding to the “clues” provided to thenetwork when the ordinal nature of the problem is exploited during training. For instance, if thenetwork incorrectly guesses rank index 7 when the true rank index is 8, the categorical loss treatsthis equivalently to a guess of a rank index of 1; the back propagation does not send any signalsindicating that the guess of rank 7 was “close” to the true rank of8. By contrast, Equation (4) wouldcapture the fact that most of the binary subtasks are correctly predicted when a rank 7 is estimatedfor a ground truth rank of 8. In problems like MNIST where the features are not necessarily trend-ing with increasing rank, we could understand how a categorical loss produces a stronger categorical12Under review as a conference paper at ICLR 2022accuracy. But in problems like Amazon star ratings, where the language and sentiment of 4 and 5star reviews are likely closer in the NLP embedding space than the language and sentiment of 1 starreviews, one can also understand how training with an ordinal loss could provide higher categorical
