Table 1: Comparison of the best policies from six dis-tinct classes of RL networks: Quantization (ours), EdgePruning (ours), Masked, Toeplitz, Circulant, and Un-structured networks trained with standard ES algorithm(Salimans et al., 2017). All results are for feedforwardnets with one hidden layer. Best two metrics for eachenvironment are in bold, while significantly low re-wards are in red.
Table 2: Results via quantization across PG, Reg-Evo, and random search controllers. The number of partitionsis always set to be max(|S |, |A|).
Table 3: Results via quantization across PG, Reg-Evo, and random search controllers. The number of edges isalways set to be 64 in total, or (32, 32) across the two weight matrices when using a single hidden layer.
Table 4: Results using the same setup as Table 3, but allowing nonlinearity search.
Table 5: Rewards for selected environments and methods, each result averaged over 3 seeds. Arrow denotesmodification or addition (+).
