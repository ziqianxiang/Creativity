Table 1: Results on continual learning tasks for S-MNIST, P-MNIST, R-MNIST, S-CIFAR100, and S-miniImageNet. Average test accuracy over five runs (with standard deviation) is shown for all the experiments.
Table 2: Total number of parameters and training memory overhead required by different approaches forcontinually learning all 20 tasks on Split-CIFAR100 using 3-layer fully-connected network. The numbersinside bracket is relative to the number of parameters of a single full-rank 3-layer fully-connected network.
Table 3: Average forgetting results for different datasets using different approaches. We report the forgettingin percentage unit (%). We also report the standard deviation.
Table 4: Test accuracy for different rank choices of continual learning network and multi-task baseline net-works for S-MNIST, P-MNIST, and R-MNIST.
Table 5: Ablation study to evaluate the effect of updating Test accuracy of of four different datasets underdifferent combinations of selector and bias vector update.
Table 6: Test accuracy and forgetting results on split CIFAR-100 dataset using ResNet18 architecture with dif-ferent continual and non-continual approaches. With convolutional ResNet18 structure every approach performbetter, but our method outperforms the comparing approaches.
