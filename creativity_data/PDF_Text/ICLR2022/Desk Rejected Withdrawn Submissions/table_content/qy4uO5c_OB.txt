Table 1: Representative gradient-based bi-level optimization methods. (Here we summarize whetherthey need to approximate the solutions of the lower-level objective or intermediate steps to approxi-mating the hypergradient, respectively.)Method	Reference	Problem	Method type	Approximate solutions	Intermediate steps"FMD	Franceschi et al. (2017)	Smooth	Bi-level	Yes	YesRMD	Franceschi et al. (2017)	Smooth	Bi-level	Yes	YesApprox	Pedregosa (2016)	Smooth	Bi-level	Yes	YesPenalty	Mehra & Hamm (2019)	Smooth	Single-level	Yes	YesFBBGL	Frecon et al. (2018)	Group LASSO	Bi-level	Yes	YesSParseHO	Bertrand et al. (2020)	LASSO-type	Bi-level	Yes	YesSMNBP	Okuno & Takeda (2020)	p-norm	Single-level	No	NoSPNBO	Ours	Generalized	Single-level	No	Nokey idea is providing a proxy single-level reduction problem, and then deriving the gradient updatefor the single-level problem instead of the original bi-level problem. For example, Mehra et al.,Mehra & Hamm (2019) transformed the original BO problem into a single-level problem by thepenalty method and then calculated the gradients for lower- and upper-level variables respectivelyto update the solution for the single-level reduction problem. We summarize these representativemethods in Table 1.
Table 3: Test accuracy (%) of all the methods in data re-weight.
Table 4:	Test accuracy (%) of all the methods in training data poisoning (lower is better).
Table 5:	Test accuracy (%) of all the methods in meta-learning.
