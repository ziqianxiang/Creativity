Table 1: Effect of acceleration for inference serving on each accelerator (FPGA/GPU)	Proposed	SNIP-Sum	lottery-ch	OriginalAccuracy (%)	81.43	83:81	80.48	80.00Mem. occ. (MiB)	2,911	5,681	4,983	5,767Latency on GPU (ms)	81.38	374.63	379.12	523.43Speed up on GPU	×6.43	×1.40	×1.38	×1.00Latency on FPGA (ms)	5.20	15.55	16.96	23.75Speed up on FPGA	×4.56	×1.53	×1.40	×1.00Table 2: Effect of throughput improvement on GPU deployed with maximum available batch size	Proposed	SNIP-Sum	lottery-ch	OriginalMax. batch size	87	32	38	31Throughput (Req/s)	198.2	42.66	42.08	30.7Improvement	×6.45	×1.39	×1.37	×1.00pruning with the proposed channel sensitivity only also shows vulnerability of removing a certainlayer, and this problem is alleviated by additionally considering layer-wise sensitivity.
Table 2: Effect of throughput improvement on GPU deployed with maximum available batch size	Proposed	SNIP-Sum	lottery-ch	OriginalMax. batch size	87	32	38	31Throughput (Req/s)	198.2	42.66	42.08	30.7Improvement	×6.45	×1.39	×1.37	×1.00pruning with the proposed channel sensitivity only also shows vulnerability of removing a certainlayer, and this problem is alleviated by additionally considering layer-wise sensitivity.
