Table 1: Average test accuracy (%) on MNIST over the last ten epochs.
Table 2: Average test accuracy (%) on CIFAR-10 over the last ten epochs.						Noise rates T ∣	Standard ∣	PENCIL I	Co-teaching ∣	Co-teaching+ ∣	JoCoR I	CREMA (ours)Symmetry-20% ∣	68.67 ± 0.11 I	78.78 ± 0.15 I	82.56 ± 0.24 I	82.27 ± 0.21 I	85.73 ± 0.19 I	86.32 ± 0.16Symmetry-50% ∣	42.31 ± 0.18 I	64.71 ± 0.27 I	72.97 ± 0.22 I	63.01 ± 0.33 I	79.53 ± 0.10 I	81.63 ± 0.13Symmetry-80% ∣	15.94 ± 0.07 I	26.96 ± 0.37 I	24.03 ± 0.18 I	17.96 ± 0.06 I	27.30 ± 0.08 I	29.66 ± 0.16Asymmetric-40% ∣	70.04 ± 0.08 I	70.06 ± 0.28 I	75.96 ± 0.15 I	72.21 ± 0.43 I	76.31 ± 0.21 I	82.49 ± 0.13Pairflip-40% ∣	51.66 ± 0.11 I	75.26 ± 0.18 I	75.10 ± 0.23 I	57.59 ± 0.45 I	68.56 ± 0.16 I	85.00 ± 0.13Pairflip-45% ∣	45.78 ± 0.13 I	71.18 ± 0.28 I	70.68 ± 0.23 I	49.60 ± 0.23 I	57.68 ± 0.21 I	82.94 ± 0.12Table 3: Average test accuracy (%) on CIFAR-100 over the last ten epochs.						Noise rates T ∣	Standard ∣	PENCIL I	Co-teaching ∣	Co-teaching+ ∣	JoCoR I	CREMA (ours)Symmetry-20% ∣	34.72 ± 0.07 I	52.11 ± 0.21 I	50.48 ± 0.24 I	49.27 ± 0.03 I	53.41 ± 0.09 I	57.21 ± 0.25Symmetry-50% ∣	16.86 ± 0.09 I	39.89 ± 0.30 I	38.24 ± 0.26 I	40.04 ± 0.70 I	43.37 ± 0.09 I	43.95 ± 0.42Symmetry-80% ∣	4.60 ± 0.12 I	16.08 ± 0.15 I	11.78 ± 0.12 I	13.44 ± 0.37 I	12.33 ± 0.13 I	17.10 ± 0.19Asymmetric-40% ∣	26.93 ± 0.10 I	32.81 ± 0.23 I	33.36 ± 0.28 I	33.62 ± 0.39 I	32.66 ± 0.13 I	38.61 ± 0.25Pairflip-40% ∣	27.48 ± 0.12 I	33.83 ± 0.52 I	33.94 ± 0.18 I	33.80 ± 0.25 I	33.89 ± 0.12 I	38.06 ± 0.34Pairflip-45% ∣	24.21 ± 0.11 I	29.01 ± 0.28 I	29.57 ± 0.15 I	26.93 ± 0.34 I	28.83 ± 0.10 I	32.50 ± 0.29For real-world noisy Clothing1M dataset, following Yi & Wu (2019); Zhang et al. (2021), we donot use the 50K clean data, and a randomly sampled pseudo-balanced subset includes about 260Kimages is leveraged as training data.
Table 3: Average test accuracy (%) on CIFAR-100 over the last ten epochs.						Noise rates T ∣	Standard ∣	PENCIL I	Co-teaching ∣	Co-teaching+ ∣	JoCoR I	CREMA (ours)Symmetry-20% ∣	34.72 ± 0.07 I	52.11 ± 0.21 I	50.48 ± 0.24 I	49.27 ± 0.03 I	53.41 ± 0.09 I	57.21 ± 0.25Symmetry-50% ∣	16.86 ± 0.09 I	39.89 ± 0.30 I	38.24 ± 0.26 I	40.04 ± 0.70 I	43.37 ± 0.09 I	43.95 ± 0.42Symmetry-80% ∣	4.60 ± 0.12 I	16.08 ± 0.15 I	11.78 ± 0.12 I	13.44 ± 0.37 I	12.33 ± 0.13 I	17.10 ± 0.19Asymmetric-40% ∣	26.93 ± 0.10 I	32.81 ± 0.23 I	33.36 ± 0.28 I	33.62 ± 0.39 I	32.66 ± 0.13 I	38.61 ± 0.25Pairflip-40% ∣	27.48 ± 0.12 I	33.83 ± 0.52 I	33.94 ± 0.18 I	33.80 ± 0.25 I	33.89 ± 0.12 I	38.06 ± 0.34Pairflip-45% ∣	24.21 ± 0.11 I	29.01 ± 0.28 I	29.57 ± 0.15 I	26.93 ± 0.34 I	28.83 ± 0.10 I	32.50 ± 0.29For real-world noisy Clothing1M dataset, following Yi & Wu (2019); Zhang et al. (2021), we donot use the 50K clean data, and a randomly sampled pseudo-balanced subset includes about 260Kimages is leveraged as training data.
Table 4: Comparison with state-of-the-art methods in test accuracy on Clothing1M. “LA”, “LC” and“ND” denote “Loss Adjustment”, “Label Correction” and “Noisy sample Detection” respectively.
Table 5: Comparison with state-of-the-art methods in test accuracy on Animal-10N. short for cat-egory is the same as Table 4. Results for baselines approaches are quoted from Song et al. (2019)and Zhang et al. (2021).				Method	Category			Test Accuracy (%)	LA	LC	ND	Cross-Entropy				79.4ActiveBias (Chang et al., 2017)	X			80.5PLC (Zhang et al., 2021)		X		83.4Co-teaching (Han et al., 2018b)			X	80.2SELFIE (Song et al., 2019)		X	X	81.8CREMA (Ours)		X	X	84.2their robustness. Among them, JoCoR and PENCIL perform much better over other methods. How-ever, when it comes to Pairflip-40% and Pairflip-45% cases, their performance drops significantly.
Table 6: Ablation studies of each componentwithin CREMA on Clothing1M dataset.
Table 7: Investigations on different mixturemodels on ClothingIM dataset.
Table 8: Investigations on length of sequence n on ClOthingIM dataset.
