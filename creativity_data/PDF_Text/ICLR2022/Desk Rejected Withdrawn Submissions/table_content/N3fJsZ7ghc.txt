Table 1: Top-1 and top-5 accuracy and parameters of the original and encrypted models on ImageNetvalidation dataset. Plain convolution model like VGG and skip-connection based model like ResNetand transformer based model like ViT are tested. With a small amount of extra computation (lessthan 5%), the pre-trained models are encrypted to protect themselves.
Table 2: The AP and FPS of object detection on the original and encrypted ResNet50/101 models,evaluated on the MS-COCO validation set.
Table 3: The AP and FPS of instance segmentation on the original and encrypted ResNet50/101models, evaluated on the MS-COCO validation set.
Table 4: Chinese text classification results for ten categories on the THUCNews test set. Threemetrics are reported for the original and encrypted BERT models, which are precision, recall, andF1-score, respectively. Only 4.21% additional parameters is sufficient for encryption.
Table 5: The finetuning results on CUB-100-2011 and FGVC-Aircraft test datasets. Here, ‘samp’,‘init’, and ‘mixed’ represent different strategies of weight assignment. With the increase of confu-sion proportion, the performance of fine-tuned encrypted model decreases to a certain extent. Theresistance of different kinds of confusion weight to fine-tuning is significantly different.
