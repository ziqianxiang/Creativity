Table 1: BERT. Median pretraining and finetuning performance of BERT models from 3 independent runs. We highlightreuse Transformer cells that match (upto standard deviation) or improve over the baseline. We notice that reusingattention scores results in similar finetuning performance while reducing the computational requirement. We noticethat both forms of reusing attention results in similar performance. We also see that matching the baseline in numberof parameters (by increasing number of layers from 12/24 to 13/26 for the BASE/LARGE models) results in betterperformance showing that reusing attention results in better performance scaling.
Table 2: T5. Median performance of T5 models on the GLUE and SuperGLUE finetuning tasks over 3 independentruns. We highlight reuse Transformer cells that match or improve over the baseline. We notice that reusing attentionscores leads to an improvement in performance for both base and large models. We also report the relative number ofparameters and compute required for all the models. We notice that reusing attention also leads to an improvement incompute and reduction of model parameters.
Table 3: ViT. ImageNet finetuning accuracy ofViT models pretrained on JFT-300M.
Table 4: Machine Translation. Median translation performance (BLEU scores) on Newstest2018 dataset. We noticethat reusing attention results in similar performance while saving on the computation and parameters.
Table 5: LRA. Performance of reuse attention models on the Long Range Arena benchmark. We notice that reusingattention leads to sizeable performance improvement while reducing the computational cost.
Table 6: Computational savings. Performance benchmark on Text classification task in the LRA benchmark for inputsequence lengths from 1k to 4k. We see that reusing attention scores leads to significant gains both in increased stepsper second and reduced memory usage. We indicate the percentage improvement for the 4k input sequence lengthsetting in the parenthesis.
Table 7: Comparison of different configurations of reusing attention.					Model	Reuse/skip layers (P)	MLM Acc	MNLI Acc	SQuAD V1.1 F1	SQuAD V2.0 F1BERTLARGE	-	73.76	87.97	91.87	82.41Reuse - Proposed	12	73.64	87.75	91.92	82.56Reuse - Alternate	12	73.88	87.31	91.56	82.29Reuse - AllEnd	12	74.07	87.24	91.55	81.7Skip	12	73.4	87.3	91.65	81.83In this section we present comparisons between different configurations of reuse attention. We consider full layer reusesetting. The proposed architecture in Algorithm 1, Reuse-Proposed, reuses the first P layers following the first layer.
