Table 1: Transformer Decoders perplexity (PPL) onWikiText-103 and Chinese Wikipedia validation sets.
Table 2: The validation set perplexity of all models.
Table 3: Different models’ performance on the dev sets of GLUE benchmark. All results are averagedover five different random seeds (1, 2, 3, 4 and 5). MNLI-m is the matched version and MNLI-mm isthe mismatched version. All tasks except STS-B use accuracy as their evaluation metrics. STS-Buses the Spearman rank correlation. The results are reported as r × 100. Bold indicates the best scorefor each task.
Table 4: Different models’ performance on the test sets of GLUE benchmark. MNLI-m is thematched version and MNLI-mm is the mismatched version. All tasks except STS-B and CoLA useaccuracy as their evaluation metrics. STS-B uses the Spearman rank correlation. CoLA (Warstadtet al., 2019) uses the Matthews correlation coefficient. The results are reported as r × 100. Thescores of BERT-base and BERT-large are from Devlin et al. (2019). The scores of RoBERTa-base arefine-tuned by ourselves. Bold indicates the best score of our models for each task.
Table 5: Hyper-parameters for pre-training the multi-layer Decoder Models. Since all models sharethe same hyper-parameters, we only report the parameters of No-PE.
Table 6: Hyper-parameters for pre-training the multi-layer Encoder Models (small-scale pre-training).
Table 7: Hyper-parameters for pre-training the multi-layer Encoder Models (large-scale pre-training).
Table 8: Hyper-parameters for fine-tuning all models on downstream tasks. All models use thepolynomial learning rate decay. Most of the hyper-parameters are recommended by Fairseqhttps://github.com/pytorch/fairseq/tree/main/examples/roberta/config/finetuning.
