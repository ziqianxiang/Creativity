Table 1: Space and time complexity of self-attention kernel of Kernelized Transformers com-pared with Softmax Transformer (Vaswani et al., 2017), Performer (Choromanski et al., 2021), andLinearElu (Katharopoulos et al., 2020). L refers to length of context, dq=dv is the query/key/valuedimension, while M is the number of samples (where applicable).
Table 2: Experimental results on the LRA benchmark. We report accuracy on the test set. The bestmodel is in boldface and the second best is underlined. Accuracy scores for all baseline models aredue to Tay et al. (2021b). Furthermore, L generally refers to the sequence length, K refers to thesize of a local window and B L is a model specific parameter.
Table 3: Results on the GLUE benchmark after fine-tuning on respective tasks. KernelizedTransformers continue to be competitive to Transformers even in short context problems.
Table 4: Hyperparameters for LRA tasks.
Table 5: Number of random samples M used within each KERNELIZED TRANSFORMER.
Table 6: Hyperparameters for GLUE tasks. Where multiple parameters were tried, they are listedin curly brackets. Ipre denotes the total number of pre-training steps, whereas Itune denotes the totalnumber of fine-tuning steps on each GLUE task.
Table 7: Ablation studies using FastFood variants on the GLUE benchmark.
