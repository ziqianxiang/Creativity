Table 1: Test RMSE for varying degrees of label noise for ERM, DA-ERM, and DAIR using different losses.			The results of this experiment are reported in Table 1. In the last column of the table we report resultson the clean dataset without any spurious features for comparison purposes. As can be seen, withoutdata augmentation all methods fall prey to spurious features and perform poorly, especially as thenoise level is increased. It is noteworthy that while TERM is not designed for domain shift, it slightlyoutperforms the other baselines in the presence of spurious features showing that TERM has someinherent robustness to the domain shift. By adopting data augmentation, testing error decreases butis still quite large as compared to the Clean ERM setup for high values of noise. Notably, DAIR isable to reduce the testing error across all objectives and noise levels with the gap between DAIRand other approaches increasing with the degree of noise. For the 0% noise setup, DAIR is able toalmost recover the Clean ERM accuracy for all three objectives. The gains achieved with DAIR areprominent for L2 and Huber, but marginal for TERM. Finally, data augmentation/DAIR combinedwith TERM can simultaneously handle domain shift and noisy labels as can be seen in this table.
Table 2: Accuracy and Consistency metrics on VQA v2 val &IV-VQA test set.
Table 3: CIFAR-10 test accuracies under no attack (clean), FGSM, andPGD20 attacks, and accuracy consistency metric between original andPGD20 attack.
Table 4: Joint Goal Accuracy (JGA) for different approaches on the SimpleTOD model. DAIR achievesstate-of-the-art results on the original MultiWOZ 2.2 test set (Zang et al., 2020) and well as the MultiWOZ 2.2test set w/ named entities replaced with SGD (Qian et al., 2021).
Table 5: Testing accuracy of Rotated MNIST, Weak Augmentaion. We see the accuracy increases as we extendthe number of training epochs.
Table 6: Iteration needed for the logistic model to converge with different λ. The model is converged when theL2 norm of the gradient is less than 10-7.
Table 7: Model Architecture, C = 1 for Colored MNIST and C = 10 for Rotated MNIST.
Table 8: Training parameter of MNIST experiments.
Table 9: Color schemes in Colored MNIST. Random color means that the value of each channel of the image isuniformly random chosen from 0 to 255.
Table 10: Rotation schemes in Rotated MNIST. [a, b] means that degrees are unformly random chosen betWeena and b.
Table 11: Training procedure of Colored MNIST.				Setup	Train	Aug	Test	λStrong Aug.	R1	R5	R2	1Weak Aug.	R4	R6	R3	10Table 12: Training procedure of Rotated MNISTSetup: We train a model consisted of three convolutional layers and two fully connected layers with20,000 examples. For each dataset we are defining several different schemes on how the datasetcould be modified: Table 9 (Colored MNIST) and Table 10 (Rotated MNIST). Then, we defineseveral setups. Each setup is consisted of one original dataset, one augmentation dataset, and onetest dataset, each of which is selected among the defined schemes. These setups are provided inTable 11 (Colored MNIST) and Table 12 (Rotated MNIST). For each setup, we train the modelwith the following four algorithms and compare their performances: ERM, DA-ERM, DAIR andInvariant Risk Minimization (IRM). Each experiment is repeated for 10 times; the mean and thestandard derivation are reported. The value of λ are chosen base on the validation results. Detailedarchitectures and training parameters can be found in Appendix C.
Table 12: Training procedure of Rotated MNISTSetup: We train a model consisted of three convolutional layers and two fully connected layers with20,000 examples. For each dataset we are defining several different schemes on how the datasetcould be modified: Table 9 (Colored MNIST) and Table 10 (Rotated MNIST). Then, we defineseveral setups. Each setup is consisted of one original dataset, one augmentation dataset, and onetest dataset, each of which is selected among the defined schemes. These setups are provided inTable 11 (Colored MNIST) and Table 12 (Rotated MNIST). For each setup, we train the modelwith the following four algorithms and compare their performances: ERM, DA-ERM, DAIR andInvariant Risk Minimization (IRM). Each experiment is repeated for 10 times; the mean and thestandard derivation are reported. The value of λ are chosen base on the validation results. Detailedarchitectures and training parameters can be found in Appendix C.
Table 13: Accuracy and Accuracy Consistency Metric (CM) on Colored MNIST with Adversarial Augmentation.
Table 14: Accuracy and Accuracy Consistency Metric (CM) on Colored MNIST with Random Augmentation.
Table 15: Rotated MNIST with 90° or 270° rotated original images and Weak Augmentation during training.
Table 16: Accuracy-Consistency Tradeoff on VQA v2 val and IV-VQA test set controlled by λTable 16 indicates a tradeoff between the accuracy on the VQA v2 ‘val’ set and the consistencymetrics. As the λ value increases, the consistency between the predictions increases, while theaccuracy on original examples decreases. For instance, A λ value of 10 strongly boosts consistencythus lowering the ‘Predictions flipped’ percentage to only 7.9% but sacrifices the predictive powercausing the accuracy to drop to 51.3%.
Table 17: Hyper-parameters used in training SimpleTOD.
Table 18: Left: sample from the original MultiWOZ dataset. Middle: augmented sample generated byscrambling. Right: synthetic sample with name entities from SGD. Comparing left and the middle example, weare generating new named entities (marked in red) by scrambling. Comparing left and the right example, theonly difference is the named entity from different dataset, which is marked in red. Note that the SGD namedentities are not exposed to the model during training. Only the original named entities and scrambled namedentities from MultiWOZ are used during training.
