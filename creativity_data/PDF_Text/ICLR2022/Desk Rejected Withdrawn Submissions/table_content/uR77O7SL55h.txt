Table 1: Point cloud registration. We compare the quantitative performance of RPM-Net (Yew &Lee, 2020) and our approach on ModelNet40 (Wu et al., 2015). The two architectures are identicalexcept for the altered Sinkhorn module. For all results, we follow the training protocol described inYew & Lee (2020, Sec. 6). Moreover, we assess the ability of the obtained networks to generalizeto partial and noisy inputs at test time. For the former, we follow Yew & Lee (2020, Sec. 6.6) andremove up to 70% of the input point clouds from a random half-space. For the noisy test set, we addGaussian white noise N(0, σ) with different variances σ ∈ {0.001, 0.01, 0.1}. For all settings, wereport the rotation and translation errors, as well as the Chamfer distance to the reference surface.
Table 2: Number sorting generalization. We assess the capability of our approach (first row) andthe vanilla Gumbel-Sinkhorn method (second row) (Mena et al., 2018) to generalize to various testsets U(s, t) with s < t. We train both methods to sort sets of n = 200 numbers xi from the uniformdistribution on the unit intervalU(0, 1) with τ = 200 Sinkhorn iterations. For each test set, we showthe mean proportion of false predictions, as well as the empirical standard deviation, obtained from50 test runs per setting.
