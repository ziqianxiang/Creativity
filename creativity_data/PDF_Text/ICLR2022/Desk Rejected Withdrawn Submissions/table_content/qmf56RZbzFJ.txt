Table 1: Training HyperparameterSAC Parameter	ValueOptimizer	Adam (Kingma & Ba, 2015)learning rate policy	1∙10-4learning rate Q-function	3∙10-4discount γ	0.7 (Ant 0.9)mini batch size	2048replay buffer size	2 ∙ 106target update interval	1number of environments	16max number of environment steps	1.6 ∙ 106 (Ant 8 ∙ 106)SOIL-TDM Parameter	Valueexpert transition model training steps	"lθ4learning rate expert transition model	1∙10-4learning rate forward dynamics model	1∙10-4learning rate backward dynamics model	1∙10-4update interval dynamics models	1We implement all our models for SOIL-TDM using the PyTorch framework version 1.9.0 (Paszkeet al., 2017). To estimate the imitation reward in SOIL-TDM a model for the expert transitionsμE(s0∣s) as well as a forward μ(s0∣s, a) and backward dynamics model μ(a∣s0, S) has to be learned.
Table 2: Normalizing Flow Setup	μE(s0lS)		μφ(s0∣s,a)	μη(a∣s0, S)N flow blocks	16	16	16Conditional hidden neurons	64	48	192Conditional hidden layer	2	2	2Conditional feature size	32	32	8Flow block hidden neurons	64	48	192Flow block hidden layer	2	2	2Exponent clamping	8	1	1A.4 Additional ResultsThe following figures (Figure 4 - 7) show the policy loss and the estimated reward together with theenvironment reward during the training on different pybullet environments for OPOLO and SOIL-TDM (our method). All plots have been generated from training runs with 4 expert trajectories and10 test rollouts. It can be seen that the estimated reward and policy loss from SOIL-TDM correlateswell with the true environment reward with the exception of the walker environment. It is possiblethat the policy loss of SOIL-TDM is lower than 0 since its based not on the true distributions.
