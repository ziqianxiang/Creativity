Table 1: Linear layer transfer learning evaluation: Our CMSF model with 200 epochs onlyoutperforms all baselines on transfer learning evaluation. We separate supervised and SSL setting.
Table 2: Ablations of baselines and CMSF: All experiments use 200 epochs if not mentionedand use ImageNet-1k dataset. (a) More epochs does not improve transfer accuracy for Xent. Thus,the model available from PyTorch (pytorch) (last row) has the best transfer accuracy; (b) We addcomponents of our method to improve SupCon baseline. The baseline implementation of SupConuses std. aug and 16k memory size and it does not include the target embedding u in the positiveset. (c) Using a MLP head improves FrzProto a lot as it allows the post-MLP features to adapt tothe regression task, but allows pre-MLP features to be generalizable; (d) We find that our methodis not very sensitive to the size of memory bank or in top-k; (e) Interestingly, excluding the targetembedding U from M does not hurts the results. Note that when We do not include the target,the nearest neighbors are still chosen based on the distance to the target, so they will be close tothe target. (f) We report the results of our method by with varying the amount of labeled data onImageNet-1k. We find that only 50% of labeled data is sufficient to reach on-par performance ofthe fully supervied model. The first row is equivalent to self-supervised MSF, so the numbers arecopied from Koohpayegani et al. (2021)Method	Mean Trans	Linear IN-1k	Method	Mean Trans	Linear IN-1k(a) Xent			(d) CMSF		lr=0.05, cos, epochs=200, strong aug.	71.5	77.2	top-1 (BYOL-asym)	74.3	69.3lr=0.05, cos, epochs=200, std. aug.	71.0	77.3	mem=128k, top-2	78.4	76.2lr=0.10, cos, epochs=200, strong aug.	72.3	77.1	mem=128k, top-10	80.1	76.4.,	,	,	. lr=0.05, cos, epochs=90, std. aug.	72.4	76.8	mem=128k, top-20	79.9	76.3
Table 3: Cross-modal constraint: We continue training each SSL model for 200 epochs usingMSF (Koohpayegani et al., 2021) for a fair comparison. “Init” column shows what model has beenused to initialize the training while “Conatraint” column shows what model is used to provide theconstraint. Note that CoCLR (Han et al., 2021) also uses another modality as a constrain in theform of contrastive learning. Rows [1-4] are copied from Han et al. (2021).
Table 4: Ablation for the effect of n for cross-modal setting: By comparing Rows 4 and 6, wecan see that using n > k helps the model whenthe constraint is less accurate (more noisy).
Table 5: Constraining with 2D SSL models:Our method can benefit from constraintsthat come from an SSL 2D model trained onImageNet-1k. “CF” refers to running the 2Dmodel on the center frame of the video.
Table A1: Semi-supervised representation learning with CMSF. We report the results of ourmethod by with varying the amount of labeled data on ImageNet-1k. We find that only 50% oflabeled data is sufficient to reach on-par performance of the fully supervied model. The first row isequivalent to self-supervised MSF, so the numbers are copied from Koohpayegani et al. (2021)Labeled Split	Food 101	CIFAR 10	CIFAR 100	SUN 397	Cars 196	Air- craft	DTD	Pets	Calt. 101	Flwr 102	Mean Trans	Linear0%	71.2	92.6	76.3	59.2	55.6	53.7	73.2	88.7	92.7	92.0	75.5	72.410%	71.6	93.6	78.1	61.0	62.0	59.2	73.4	91.5	93.1	94.4	77.8	73.020%	73.3	93.2	77.8	61.3	64.5	60.1	73.6	91.0	93.4	95.0	78.3	73.850%	74.1	93.8	79.4	62.1	68.6	63.1	73.0	91.6	93.5	95.0	79.4	75.3100%	74.9	94.4	78.7	62.7	70.8	63.4	73.8	92.2	94.9	95.6	80.1	76.4A.3 Implementation Details of Transfer Evaluation (Section 2.1.3)We use the LBFGS optimizer (maxJter=20, and history_size=10) along with the OPtUna library(Akiba et al., 2019) in the Ray hyperparameter tuning framework (Liaw et al., 2018). Each datasetgets a budget of 200 trials to pick the best parameters on validation set. The final accuracy is reportedon a held-out test set by training the model on the train+val split using the best hyperparameters.
Table A2: Transfer dataset details: Train, val, and test splits of the transfer datasets are listed inthis table. Test split: We follow the details in [32]. For Aircraft, DTD, and Flowers datasets, weuse the provided test sets. For Sun397, Cars, CIFAR-10, CIFAR-100, Food101, and Pets datasets,we use the provided val set as the hold-out test set. For Caltech-101, 30 random images per categoryare used as the hold-out test set. Val split: For DTD and Flowers, we use the provided val sets. Forother datasets, the val set is randomly sampled from the train set. For transfer setup, to be close toBYOL [25], the following val set splitting strategies have been used for each dataset: Aircraft: 20%samples per class. Caltech-101: 5 samples per class. Cars: 20% samples per class. CIFAR-100:50 samples per class. CIFAR-10: 50 samples per class. Food101: 75 samples per class. Pets: 20samples per class. Sun397: 10 samples per class.
Table A3: Noisy supervised setting on ImageNet-100: Our method is more robust to noisyannotation compared to Xent. Also, using top-all results in degradation since all images from asingle category are not guaranteed to be semantically related.
