Table 1: Results on IHDP (left) and Jobs (right). Left: BCAUSS is state-of-the-art on the IHDPbenchmark dataset. “+BCE” and "+T_REG" mean adding the binary cross-entropy of eq. 5 andthe targeted regularization term of eq. 6 to the overall objective. Similarly, “-T_REG” means re-moving the targeted regularization term of eq. 6 from the objective. AIPW adopts as propensityscore estimator (Belthangady & Norgeot, 2021) and two linear regressors (for control and treatmentrespectively) with an L2 penalty. Right: Methods compared with BCAUSS are described in Shalitet al. (2017b); Yoon et al. (2018). Lower is better.
Table 2: Ablation results of the different variants described in Section 5.1 on IHDP dataset (n isthe total number of observations on train-set). λBAL = 1 corresponds to add to the objective theself-supervised auto-balancing term of eq. 2, λBCE = 1 corresponds to add to the objective thebinary cross-entropy term of eq. 5, λT AR = 1 corresponds to add to the objective the targetedregularization term of eq. 6. Row 5 with ReLU activation function corresponds to BCAUSS. Row0 with ELU activation function corresponds to Dragonnet. Bold indicates the best performanceoverall.
Table 3: Comparison between the treated and untreated distributions induced by the learned repre-sentation on Dragonnet and BCAUSS from one experiment of IHDP dataset. Prior covariate treatedand control groups have KS test with p-value < 1%, Wasserstein distance 0.1045, MMD(linear)0.0108, MMD(rbf) 0.0203, MMD(poly) is 0.0023.
Table 4: Comparison between the distributions oftreated vs. untreated induced by the learned rep-resentation on Dragonnet and BCAUSS averaging1,000 experiments of the IHDP dataset.
Table 5: Comparison between the distributions of treated vs. untreated induced by the learnedrepresentation on Dragonnet and BCAUSS from one experiment of Jobs dataset.
