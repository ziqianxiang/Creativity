Table 1: Bias-variance decomposition for different loss functions. The CE loss herein is the com-plete form of the commonly used one, originated from the Kullback-Leibler divergence. Z =Pk =1 exp{Et[log yk]} is a normalization constant independent of k. H(∙) is the hard-max whichsets the maximal element to 1 and others to 0. Icon{∙} is an indicator function which equals 1 if itsargument is true, and 0 otherwise. log and exp are element-wise operators.
Table 2: Architecture of the small CNN model (“BN” denotes Batch Normalization).
