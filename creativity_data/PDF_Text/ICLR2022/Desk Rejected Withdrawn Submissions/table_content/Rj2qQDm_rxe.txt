Table 1: Results on CAPR across four datasets using the Cross Encoder architecture(left) and fourCOP tasks(right). Top part shows scores for models based on BERT-base, bottom part scores formodels on BioBERT. KIMERA improves on both BERT-base and BioBERT performance, with theexception of the LOS task.
Table A1: Mean importance scores Ih before and after KIMERA for the frozen and retrained headsof the model in the CAPR task. The importance score more than doubles for the retrained headswhile it moderately decreases for the frozen heads.
Table A2: Properties of Downstream Datasets.
Table A3: Hyperparameter considerations for the different steps of KIMERA. While only minimalHPO was necessary for most steps, we find that the clinical outcome prediction tasks require exten-sive HPO, in order to reach state-of-the-art results. Learning Rate and Warm-up Steps turned out tobe the most impactful parameters.
Table A4: We report the results of the GLUE benchmark with 4 sample models on the validationset. We choose the best score between 10 seeds for each task. We show that KIMERA consistentlyoutperforms BioBERT on all tasks, and manages substantial improvements over the general lan-guage model BERT-base in 3 tasks, most significantly in the WNLI task. KIMERA also achievesthe highest mean score of tested models.
