Table 1: Accuracy under white-box attacks on CIFAR-10MODELS	CLEAN	FGSM	ACCURACY UNDER WHITE-BOX ATTACK ( = 8)								PGD20	PGD40	PGD100	CW20	CW40	CW100S TANDARD	95.60	36.90	0.00	0.00	0.00	0.00	0.00	0.00AT	85.70	54.90	44.90	44.80	44.80	45.70	45.60	45.40TLA	86.21	58.88	51.59	-	-	-	-	-LAT	87.80	-	53.84	-	53.04	-	-	-B ILATERAL	91.20	70.70	57.50	-	55.20	56.20	-	53.80TRADES	86.07	67.25	55.16	52.97	54.94	52.87	54.88	52.83FS	90.00	78.40	70.50	70.30	68.60	62.40	62. 10	60.60RST-AWP	88.25	67.94	63.73	-	63.58	61.62	-	-RLFATT	82.72	-	58.75	-	-	51.94	-	-RLFATP	84.77	-	53.97	-	-	52.40	-	-FS+PD	89.99	79.37	72.02	70.53	69.57	64.26	62.76	61.70Comparison with SOTAs: Taking the FS as the typical baseline, we conduct comparisons be-tween the proposed FS+PD and the current state-of-the-art adversarial training methods including1) AT Madry et al. (2017), 2) TLA (Mao et al., 2019), 3) LAT (Sinha et al., 2019), 4) Bilateral (Wang& Zhang, 2019), 5) FS (Zhang & Wang, 2019). Moreover, other recent methods proposed to pro-mote the robust generalisation are also included, such as 6) RST/AT-AWP (Wu et al., 2020) and 7)RLFATT /P Song et al. (2019). We demonstrate the accuracy of these different methods in Table 1
Table 2: Accuracy under different attack on CIFAR-100 and SVHNMODELS	CIFAR-100(e = 8)						SVHN( = 8)						CLEAN	FGSM	PGD20	PGD100	CW20	CW1 00	CLEAN	FGSM	PGD20 PGD100		CW20	CW100STANDARD	79.00	10.00	0.00	0.00	0.00	0.00	97.20	53.00	0.30	0.10	0.30	0.10AT	59.90	28.50	22.60	22.30	23.20	23.00	93.90	68.40	47.90	46.00	48.70	47.30LAT	60.94	-	27.03	26.41	-	-	91.65	-	60.23	59.97	-	-BILATERAL	68.20	60. 80	26.70	25.30	-	22. 10	94.10	69. 80	53.90	50.30	-	48.90FS	73.90	61.00	47.20	46.20	34.60	30.60	96.20	83.50	62.90	52.00	61.30	50.80AT-AWP	-	-	30.71	-	-	-	-	-	59. 12	-	-	-RLFATT	58.96	-	31.63	-	27.54	-	-	-	-	-	-	-RLFATP	56.70	-	31.99	-	29.04	-	-	-	-	-	-	-FS+PD	72.72	74.77	49.75	49.35	36.25	36.19	96.54	97.42	67.72	53. 13	63.75	49.725	ConclusionWe have developed a novel adversarial training method from a perturbation diversity perspectivein this paper. While existing adversarial training typically focuses the perturbation objective only,which generates inhomogeneous data distribution and limits the model’s generalisation, our pro-posed novel regularisation can certificate to generate adversarial perturbations as diverse as possibleto obtain better robust generalisation. We have provided theoretical and empirical investigationswhich validate our perturbation diversity can lead to performance gains in a number of baselinemodels.
Table 3: Detailed experiment settingMethods ∣ Dataset ∣ Batch Size ∣ Hyperparameter ∣ Training EpochAT+PD	CIFAR-10 CIFAR-100 SVHN	120 60 120	1 50 1	600 600 600	CIFAR-10	60	1	600FS+PD	CIFAR-100	60	0.1	300	SVHN	60	0.1	600	CIFAR-10	120	1	60TRADES+PD	CIFAR-100	120	10	60	SVHN	120	1	60The training procedure is provided in Alg. 1A.2 Detailed model robustness on different baseline modelsWe show the robust accuracy on the three baseline frameworks under several white-box attackson CIFAR-10, CIFAR-100, and SVHN in this section with the attack iterations T = 20, 100 forPGD (Madry et al., 2017) and CW (Carlini & Wagner, 2017).
Table 4: Robust accuracy on different baseline models on CIFAR-10ACCURACY UNDER WHITE-BOX ATTACK ( = 8)Models	Clean ________________________________________'	/	FGsM		PGD20	PGD40	PGD100	CW20	CW40	CW100	AAAT	86.35	68.15	54.66	54.39	54.32	53.66	53.45	53.42	44.04AT+PD	86.97↑	71.96↑	64.14↑	63.15↑	62.42↑	57.28↑	56.28↑	55.81↑	42.15JTRADEs	86.76	66.12	51.80	51.60-	51.54	49.86	49.77	49.70	48.54TRADEs+PD	85.55J	66.04J	53.26↑	53.09↑	53.12↑	50.54↑	50.47↑	50.50↑	49.63↑Fs	90.00	78.40	70.50	70.30-	68.60	62.40	62.10	60.60	36.64Fs+PD	89.99J	79.37↑	72.02↑	70.53↑	69.57↑	64.26↑	62.76↑	61.70↑	36.37JTable 5: Robust accuracy on different baseline models on CIFAR-100Models	Clean	Accuracy under White-box			ATTACK ( = 8)			FGsM	PGD20	PGD100	CW20	CW100AT	59.90	28.50	22.60	22.30	23.20	23.00AT+PD	64.35↑	39.76↑	31.80↑	31.66↑	24.08↑	23.42↑TRADEs	61.46	39.45	30.54^^	30.55	27.19	27.12TRADEs+PD	61.23J	39.48↑	30.86↑	30.71↑	27.35↑	27.22↑Fs	73.90	61.00	47.20^^	46.20	34.60	30.60Fs+PD	72.72J	74.77↑	49.75↑	49.35↑	36.25↑	36. 19↑Table 6: Robust accuracy on different baseline models on SVHN
Table 5: Robust accuracy on different baseline models on CIFAR-100Models	Clean	Accuracy under White-box			ATTACK ( = 8)			FGsM	PGD20	PGD100	CW20	CW100AT	59.90	28.50	22.60	22.30	23.20	23.00AT+PD	64.35↑	39.76↑	31.80↑	31.66↑	24.08↑	23.42↑TRADEs	61.46	39.45	30.54^^	30.55	27.19	27.12TRADEs+PD	61.23J	39.48↑	30.86↑	30.71↑	27.35↑	27.22↑Fs	73.90	61.00	47.20^^	46.20	34.60	30.60Fs+PD	72.72J	74.77↑	49.75↑	49.35↑	36.25↑	36. 19↑Table 6: Robust accuracy on different baseline models on SVHNACCURACY UNDER WHITE-BOX ATTACK ( = 8)Models	Clean _______________________________'	,	FGsM		PGD20	PGD100	CW20	CW100AT	93.90	68.40	47.90	46.00	48.70	47.30AT+PD	94.66↑	90.25↑	78.20↑	75.3 1↑	76.62↑	72.46↑TRADEs	93.90	77.42	61.54^^	60.75	58.05	57.66TRADEs+PD	94.70↑	85.47↑	65.06↑	62.22↑	62.19↑	60.38↑Fs	96.20	83.50	62.90^^	52.00	61.30	50.80Fs+PD	96.54↑	97.42↑	67.72↑	53.13↑	63.75↑	49.72JA.3 Effect on Black-box Attack
Table 6: Robust accuracy on different baseline models on SVHNACCURACY UNDER WHITE-BOX ATTACK ( = 8)Models	Clean _______________________________'	,	FGsM		PGD20	PGD100	CW20	CW100AT	93.90	68.40	47.90	46.00	48.70	47.30AT+PD	94.66↑	90.25↑	78.20↑	75.3 1↑	76.62↑	72.46↑TRADEs	93.90	77.42	61.54^^	60.75	58.05	57.66TRADEs+PD	94.70↑	85.47↑	65.06↑	62.22↑	62.19↑	60.38↑Fs	96.20	83.50	62.90^^	52.00	61.30	50.80Fs+PD	96.54↑	97.42↑	67.72↑	53.13↑	63.75↑	49.72JA.3 Effect on Black-box AttackWe further examine the effects of PD on AT and AT+PD under transfer-based black-box attack. Wetake CIFAR-10 as one typical example to illustrate such results. Four different models are used forgenerating test time attacks including the Vanilla Training model, AT model, Fs model, and our14Under review as a conference paper at ICLR 2022AT+PD model. As shown in Table 7, our proposed PD can improve AT in 9 cases, and is onlyslightly inferior to the baseline AT in 3 cases.
Table 7: Accuracy under transfer-based black-box attack on CIFAR-10DEFENSE MODELS	VANILLA TRAINING			ATTACKED MODELS (CIFAR-10)						AT+PD						AT			FS						FGSM	PGD20	CW20	FGSM	PGD20	CW20	FGSM	PGD20	CW20	FGSM	PGD20	CW20AT	85.35	85.53	85.35	73.06	64.49	63.82	82.97	81.30	80.12	85.24	84.80	84.79AT+PD	85.81	86.09	86.11	73.80	65.69	65.41	82.36	79.98	78.11	85.85	85.29	85.49A.4 Proof for Theory 3. 1In this section, we provide the detailed proof for Theorem 3.1.
