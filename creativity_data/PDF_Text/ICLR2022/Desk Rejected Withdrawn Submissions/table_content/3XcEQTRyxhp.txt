Table 1: Classification results on OpenImages and Imagenet. For each SSL method, we show inparentheses the gap to fully supervised training (same number of epochs). The last row shows thatour proposed approach using obj-obj+dilate cropping reduces the gap on OpenImages by nearly halfcompared to the baselines, improving over the scene-scene cropping based SSL methods by between6.8 to 9.4 mAP points. We also observe improvements on ImageNet as well.
Table 2: Linear evaluation on our OpenImages dataset with different pre-training strategies withMoCo-v2 (see Section 2 for details). Column 1 uses fully supervised learning on OpenImages. Wesee that for self-supervised pre-training, no specific range of scene crops helps to close the large gapbetween SSL and supervised training. However, there is a sweet spot of scene crop range whereMoCo-v2 performance is highest.
Table 3: Crop approaches on OpenImages: using BING crops to generate one view, and a dilated cropor a scene crop for the other positive, we are able to reduce the difference between SSL and SupervisedLearning by close to 50% (compare the last two rows to the first row). Using ground-truth boxes togenerate crops from OpenImages improves the pre-training performance marginally compared toBING crops. Obj-Obj+Dilate (last two rows) have the best performance, although Obj-Scene alsodoes well compared to Scene-Scene.
Table 4: Object detection (first 3 columns) and Semantic Segmentation (last 3 columns) results onCOCO dataset. All SSL models have been pre-trained on COCO dataset and then finetuned on COCO.
Table 5: Object detection (first 3 columns) and semantic segmentation (last 3 columns) results onCOCO (first 2 rows) and VOC (last 2 rows). All SSL models have been pre-trained on completeOpenImages dataset(1.9 million images) for 75 epochs and then finetuned on COCO and VOCdataset.
Table 6: Object detection results on VOC dataset (COCO pre-training). All models have beenpre-trained on COCO and then fine-tuned on VOC. For both MoCo-v2 and BYOL, replacing thedefault scene crops with object-scene crops results in a consistent improvement.
Table 7: Object detection results on VOC dataset (OpenImages pre-training). All models have beenpre-trained on OpenImages and then fine-tuned on VOC. Replacing the default scene crops withObj-Scene crops results in a consistent improvement.
Table 8: Varying the number of proposals generated by BING. All models are pre-trained on COCOand then fine-tuned on VOC Object Detection. Increasing the number of proposals provides aconsistent boost. We used 10 proposals as this was close to the average number of objects per imagein OpenImages.
Table 9: Varying dilation i.e δ on the COCO dataset for Obj-Obj+Dilate crop strategy. All modelshave been pre-trained on COCO and then fine-tuned on VOC Object Detection. δ = 0 correspondsto Obj-Obj cropping and larger δ is very similar to Obj-Scene. A sweet spot exists between theextremes.
Table 10: Obj-Obj+Dilate crop pre-training consistently outperforms Obj-Scene for transfer to variousdownsteam datasets for classification (COCO pre-training).
Table 11: Impact of varying scene crop range in the object-scene crop.
Table 12: Results after pre-training for 200 epochs. These results are highly indicative of results weget after longer training and can be useful for comparison in less compute settings.
Table 13: Object detection results on COCO (top 2 rows) and VOC (bottom 2 rows). All SSL modelshave been pre-trained on ImageNet for 200 epochs and then fine-tuned on COCO and VOC.
