Table 1: Top-1 accuracy (%) for ResNet-32 based models trained on CIFAR-10-LT and CIFAR-100-LT.
Table 2: Top-1 accuracy (%) on ImageNet-LT, iNaturalist 2018 and PIaces-LT. f indicate the resultswith 2 experts.
Table 3: Ablation studies of the effectiveness of the number of latent categories. We conduct theexperiments on the small dataset (CIFAR-10-LT and CIFAR-100-LT with IF 100) and large dataset(ImageNet-LT and Naturalist 2018). The larger the dataset (more training samples and classes), themore latent categories are needed to represent better performances.
Table 4: Ablation studies of each component, including whether utilizing our proposed latent cate-gory, latent augmentation loss(latent aug) and latent reconstruction loss (latent recon). We conductthe experiments on the small dataset (CIFAR-10-LT and CIFAR-100-LT with IF 100) and largedataset (Naturalist 2018). The results show that each of our proposed components improves thebaseline (without any component).
Table 5: We report accuracy on three splits of classes: Many, Medium, and Few. We validate ourmethods on multiple datasets, including small-scale datasets (CIFAR10-LT, CIFAR100-LT with IF100) and large-scale datasets (ImageNet-LT, and iNaturalist 2018). Ours* indicates ours baseline(without the latent category features, reconstruction loss lrecon and latent augmentation loss laug).
