Table 1: Comparison between our algorithms with previous results such as LSVI, LSVI-UCB, LGSC and MF.
Table 2: Notations related to reinforcement learning.
Table 3: Comparison betWeen Our Sublinear LSVI With LSVI. Let S and A denote the cardinalityof Score and Acore. Let d denote the dimension of φ(s, a). Let H be the number of steps playedin each episode. Let n denote the quantity of times played for each pair of core state-action. LetL denote the constant in Definition A.8. Let ι = log(H d/p) and p is the failure probability. LetPi = 1 - 4CoLy∕Tpnj be the parameter of data structures in Theorem A.14 and ρ2 = 1 - 8 C2 L2 i/nbe the parameter of data structure Theorem A.15. This table is a detailed version of correspondingpart of Table 1.
Table 4: Comparison betWeen Our Sublinear LSVI-UCB With LSVI-UCB Jin et al. (2020). Let Sdenote the quantity of available states and A denote the quantity of available actions. Let d denotethe dimension of φ(s, a). Let H denote the number of steps per episode. Let K denote the totalnumber of episodes. Let ∣ = log(2Hd∕p) and P is the failure probability. Let ρι = 1 - ^√1^ bethe parameter determined by data structure in Theorem A.14 and ρ2 = 1 - 8k be the parameterdetermined by data structure Theorem A.15. Since K > S, We Write the preprocessing time asO(Kd2A1+o(1)). This table is a detailed version of corresponding part of Table 1.
Table 5: Comparison between Our Sublinear LSVI-UCB with LGSC Gao et al. (2021) and MF Wanget al. (2020a). Let S denote the quantity of available states and A denote the quantity of availableactions. Let d denote the dimension of φ(S, a). Let H denote the number of steps per episode. LetK denote the total number of episodes. Let ι = log(2Hd/p) and p is the failure probability. Letρι = 1 - -√^ be the parameter determined by data structure in Theorem A.14 and ρ2 = 1 - 8k bethe parameter determined by data structure Theorem A.15. Since K > S, we write the preprocessingtime as O(Kd2A1+o(1)). This table is a detailed version of corresponding part of Table 1.
