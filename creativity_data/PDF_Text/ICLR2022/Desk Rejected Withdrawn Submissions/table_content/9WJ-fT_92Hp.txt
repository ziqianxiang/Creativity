Table 1: Currently implemented RL components	Description	PolicyAlgorithm	A2C: Advantage Actor-Critic (Mnih et al., 2016) DDQN: Double Deep Q-Learning (Van HasseIt et al., 2016) PPO: Proximal Policy Optimization (Schulman et al., 2017) DDPG : Deep Deterministic Policy Gradient (Lillicrap et al., 2015) TD3: Twin Delayed Deep Deterministic Policy Gradient (Fujimoto et al., 2018) MPO: Maximum a Posteriori Policy Optimisation (Abdolmaleki et al., 2018) SAC: Soft Actor-Critic (Haarnoja et al., 2018)	on off on off off off offActor	OnPolicyActor: actor class for on-policy RL agents (Montague, 1999) OfPolicyActor: actor class for off-policy RL agents (Montague, 1999)	on offStorage	VanillaOnPolicyBufer: minimal data storage component for on-policy algorithms (Sutton et al., 2000) ReplayBuffer: minimal data storage component for off-policy algorithms (Mnih etal.,2015) GAEBuffer: data storage component with Generalized Advantage Estimation (Schulman et al., 2015) VTraceBuffer: data storage component with V-Trace correction (Espeholt et al., 2018) NStepBuffer: multi-step leaning replay buffer (Hessel et al., 2018) PERBuffer: Prioritized Experience Replay buffer (Andrychowicz et al., 2017b) EREBuffer: Emphasizing Recent Experience replay buffer (Wang & Ross, 2019) HERBuffer: Hindsight Experience Replay buffer (Andrychowicz et al., 2017a)	on off on on off off off offEnvs	OpenAI gym environments (Brockman et al., 2016) Obstacle Tower Unity3D challenge environment (Juliani et al., 2019) Animal Olympics Unity3D challenge environment (Crosby et al., 2019) Habitat environment (Savva et al., 2019) Trifinger Real Robot Challenge environment (Wuthrich et al., 2020)	- - - - -3	Efficient reinforcement learningWe present a library for efficient reinforcement learning experimentation. It is designed to enablesimple construction of RL agents, which can subsequently be executed both in local settings or inparameterizable distributed regimes using the exact same agent building blocks. In this section wediscuss how RL agent can be defined, we will motivate the need for a flexible and simple module todefine training architectures and we will detail how this module, called Scheme, works.
