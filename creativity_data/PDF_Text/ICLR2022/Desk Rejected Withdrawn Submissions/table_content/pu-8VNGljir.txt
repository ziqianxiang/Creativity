Table 1: Few-shot classification accuracy (%) on miniImageNet and tieredImageNet. The confidenceintervals are all below 0.25 for the 10,000 episodes evaluation. Results for DN4, DeepEMD and FRN(except for their larger shot training) are our reimplementations for a fair and thorough comparisonunder different dense feature extractor settings. Results of italic font indicates the performance ofMCL as a plug-and-play for global features based methods. Results of blue fonts are the second-placedresults in the last three panes, respectively.
Table 2: Fine-grained 5-way few-shot classification (%) results with Conv-4. Results of all comparingmethods (except for DN4) are drawn from FRN with larger shot training. The confidence intervalsare all below 0.25 for 10,000 episodes evaluation.
Table 3: Grad-CAM visualizations of query and support images in 4-way 1-shot classifications. Theleft pane (a) illustrates MCL as plug-and-play on tieredImageNet. The right pane (b) illustrates MCLas an end-to-end dense classification method on miniImageNet. Images at the second column of eachtask are from the ground truth while the others are from the confounding support classes.
Table 4: Comparisons between the unidirectionalrandom walks and bidirectional MCL on mini-/tieredImageNet with Conv-4.
Table 5: Ablation of MCL as plug-and-play that in-dividually applied on query features q and supportfeatures s. The experiments are conducted withConv-4 and VanillaFCN on mini-/tieredImageNet.
Table 6: Speed comparison between differ-ent on ResNet-12. Each class owns 15 queryimages in an 5-way 1-shot episode.
Table 7: Speed comparison for different input imageresolutions in 5-way 1-shot FSL tasks with ResNet-12.
