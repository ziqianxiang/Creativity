Table 1: Average sensitivity of each model-feature pair (log AUC of corresponding curve in Fig-ure 2). Rows and columns are sorted by average values over all features and models. The feature towhich a model is most sensitive is highlighted in bold while the following one is underlined.
Table 2: Example experiment settings for measuring sensitivity, robustness and improvement bydata augmentation. We inject a spurious feature to an example if its label falls in the set of label(s)in “Perturbation" column. 0 means no injection at all. Training/test examples are the expectedinput data, assuming we have only one negative (xi, 0) and positive (xj, 1) example in our originaltraining/test set. l' is a random label and x* is a perturbed example.
Table 3: Standard deviations (σ) of Sensitivity @ p and Spearman correlations between accuracy-based/probability-based sensitivity @ P vs. average sensitivity/robustness/post data augmentation∆ over all model-feature pairs. * indicates significance (p-value < 0.05).
