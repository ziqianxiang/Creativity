Table 1: ImageNet results. The first row displays the fraction of the ImageNet data used to train themodels. Softmax: Vanilla ResNet with softmax loss. Sigmoid: ResNet trained for multi-label binaryclassification with single labels. MILe: multi-label iterated learning. Label coverage refers to thefraction of additional labels predicted by each model. All the models are trained for 100 epochs.
Table 2: WebVision results. Methods are trained on Webvision-1000 and validated both on WebVi-sion and ImageNet. MoPro (decoupled) is pre-trained on the same set as our method. CleanNet [37]and Distill [71] require data with clean annotations.
Table 3: Self-supervised finetuning. The second row displays the fraction of ImageNet training dataused for fine-tuning. Accuracy top-1 predictions are used for reporting the numbers.
Table 4: OOD generalization on ColoredM-NIST [3] (CMNIST), which consists in predict-ing digits and ColoredMNIST+, which consistsin color or digit prediction.
Table 5: Self- semi-supervised learning. Ima-geNet top-1 accuracy for ResNet-50 (R50) dis-tilled from a SimCLR [13] model. 2×: teacherhas 2× parameters than the student.
Table 5: Results on multi-label MNIST. Thefirst column displays the F1 score when thethreshold for positive labels is set to 0.25 and thesecond column shows the F1 score for a thresh-old of 0.5.
Table 6: Secondary label recovery. Mean average precision over labels that appear in ReaL but not in theoriginal ImageNet validation set.
Table 7: Pseudo label threshold ablation study. ReaL F-1 and accuracy scores for a threshold valuesWeep (ρ). This experiment Was conducted on ImageNet With the 10% data fraction setting.
Table 8: Comparison on CelebA multi-attribute classification. Just as in ReaL ImageNet validation, We useF1-score (based on the intersection over union) measure to evaluate the methods.
Table 9: Mean per-class balanced accuracy in percentage points for each of the 40 face attributes on CelebA.
