Table 2: Image classification in MNIST and SVHN. Theinference time is the time in seconds to classify a batch of1,000 images. In general, PR-Net shows the best efficiency.
Table 5: Adversarial attacks in Tiny ImageNetAttack Method		M.Net V3 IODE-Netl PR-Net			M.Net V3∣ODE-Net∣PR-Net				Top-1 accuracy			Top-5 accuracy		FGSM(e =	0.5/255)	0.3860	0.3656	0.4041	0.6492	0.6398	0.6911FGSM(e	= 1/255)	0.2304	0.2287	0.2499	0.4751	0.4928	0.5374FGSM(e	= 3/255)	0.0452	0.0464	0.0369	0.1232	0.1562	0.1596PGD (e =	0.5/255)	0.3733	0.3525	0.3910	0.6508	0.6409	0.6936PGD (e	1/255)	0.1902	0.1908	0.2133	0.4579	0.4810	0.5281PGD (e	3/255)	0.0218	0.0235	0.017	0.0792	0.1093	0.1144a governing equations can be deemed to implant latent knowledge governing the classificationprocess.
Table 7: The silhouette scoreof clustering feature mapsName	MNIST	SVHNResNet	0.49594527	0.42278063RK-Net	0.5053296	0.42842203ODE-Net	0.4991746	0.42694366PR-Net	0.5079406	0.43123975the representation vector produced at the third mobile block of each model and strictly follow theinversion method used in (Mahendran & Vedaldi, 2015)2. As shown in Figure 5 in Appendix B, ourPR-Net shows the best inversion quality.
Table 8: Training overhead in terms of GPU memory usage(MB) and training time (seconds per iteration) in MNIST andSVHNName	# Params		MNIST				SVHN				Memory Usage	Training Time	Memory Usage	Training TimeResNet	0.60M	2,359	0.155	2,363	-0.206RK-Net	0.22M	819	0229	823	-0223ODE-Net	0.22M	819	0235	823	-0307PR-Net	0.21M	836	―	0.289	-	841	—	0.227	-and space overhead for MNIST, SVHN, and Tiny ImageNet.
Table 9: Training overhead in terms of GPU memoryusage (MB) and training time (seconds per iteration)in Tiny ImageNetName	# Params	Width Multiplier	Tiny ImageNet				Memory Usage	Training TimeM.Net V3	1.21M	1	4,989	0.038ODE-Net	1,36M	1	5,797	0.069PR-Net	1.36M	1	7583	0.077M.Net V3	4.30M	2	9,139	0.057ODE-Net	4.90M	2	9,977	02∏PR-Net	4.56M	2	—	10,685 —	0.190 一for MNIST. Both LT and LI are easier to train thanLG. The governing equation loss LG typically starts with a very large value and decreases slowly astraining goes on. On the contrary, the task loss LT decreases much faster, which shows the difficultyof learning physical dynamics (i.e., governing equation) governing classification procedures.
Table 10: The architecture of the network fLayer	Design	Input Dim. Output Dim.	1	Conv2d(filter size 3x3, stride 1, padding 1)	62 × 67	62 × 672	GroupNormalization(67 groups)	62 × 67	62 × 673	Conv2d(filter size 3x3, stride 1, padding 1)	62 × 67	62 × 644	GroupNormalizaiton(32 groups)	62 × 64	62 × 645	ReLU		D	ProofTheorem D.1. Given a learning task, let θ* and a^, for all i,j, constitute a cooperative equilibriumsolution and governing equation (in terms of LT + Li + LG + RG) - in other words, we cannotfurther decrease LT + LI + LG + RG only by updating either ofθ* or αi*,j. By alternately solving theforward and the inverse problem, we can obtain θ* and αi*,j, for all i, j.
Table 11: The architecture of the network fLayer	Design	Input Dim.	Output Dim.
Table 12: Image classification datasets used in our experimentsDataset	# Classes Size (Train / Test)		Evaluation MetricsMNisT	10	60,000/10,000	Top-1, Top-5, Mean & std. Per-classsVHN	10	73,257 / 26,032	Top-1, Top-5, Mean & std. Per-classTiny imageNet	200	100,000 / 10,000	Top-1, Top-5, Mean & std. Per-classciFAR 100	100	50,000 / 10,000	Top-1, Top-5, Mean & std. Per-classciFAR 10	10	50,000 / 10,000	Top-1, Top-5, Mean & std. Per-classFGVc Aircraft	70	6,667 / 3,333	Top-1, Top-5, Mean & std. Per-classFood-101	101	75,750 / 25,250	Top-1, Top-5, Mean & std. Per-classDescribable Textures (DTD)	47	3,760 / 1,880	Top-1, Top-5, Mean & std. Per-classstanford cars	196	8,144/8,041	Top-1, Top-5, Mean & std. Per-classJ	Discretizing Feature Map Dimensions for Efficient ProcessingOne more advantage of using PDEs is that we can discretize some dimensions3. Given a feature mapsize of d1 × d2 × d3, one can design a neural network that outputs each scalar element for d ∈ R3and t ∈ [0, T]. However, this approach incurs a large number of queries, i.e., d1 × d2 × d3 queries,to reconstruct the feature map. To increase the efficiency in our experiments, we discretize the lastdimension and let the network f outputs a matrix of d1 × d2 for each discretized dimension of d3 , inwhich case d ∈ R2. Therefore, we have d3 matrices (i.e., channels), each of which has a size of d1 × d2.
Table 13: Image classification in Tiny ImageNet. We show the mean and the standard deviation ofper-class accuracy.
Table 14: Adversarial attacks in Tiny ImageNet. We show the mean and the standard deviation ofper-class accuracy.
Table 15: Transfer learning in Tiny ImageNet. We show the mean and the standard deviation ofper-class accuracy.
