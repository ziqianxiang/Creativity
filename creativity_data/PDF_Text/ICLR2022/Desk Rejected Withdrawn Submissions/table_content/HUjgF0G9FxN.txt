Table 1: Test Accuracy from the CIFAR10, SVHN and CIFAR100 datasets. Our method significantlyoutperforms the Partially Supervised case (training with only the labeled data) and performs competi-tively with centralized SSL methods. All results are obtained with the same model architecture.
Table 2: Ablation study on each component of alternative training with CIFAR10 dataset. The combi-nation of “fine-tuning with labeled data” and “pseudo-labeling with received model” significantlyimprove the performanceMethod	Fine-tuning with labeled data	Pseudo-labeling with received model	Accuracy				Non-IID, K = 2	IIDFully Supervised		N/A	95.33	Partially Supervised			76.92	FL+SSL	X	X	41.01	40.26	X	✓	48.89	47.03SemiFL	✓	X	80.42	81.70	✓	✓	85.34	93.10Table 3: Comparison of SemiFL with the existing FL and SSFL methods on the CIFAR10 dataset.
Table 3: Comparison of SemiFL with the existing FL and SSFL methods on the CIFAR10 dataset.
Table 4: Hyperparameters used in our experiments.
Table 5: Comparison of technical novelties of SSFL methods.
Table 6: Comparison between the state-of-the-art FL method and SemiFL with CIFAR10, CIFAR100,and SVHN datasets. Results are obtained with the same model architecture.
Table 7: Ablation study on the CIFAR10 datasets with 4000 labeled data at the server.
Table 8: Ablation study of sBN statistics on the CIFAR10 dataset. The alternative way of using theserver data to update the global sBN statistics does not degrade the training performance.
