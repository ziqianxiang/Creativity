Figure 1: An overview of the batch size issue in the general contrastive approaches: (a) shows theNPC multiplier qB in different batch sizes. As the large batch size increasing the qB will approach 1with a small coefficient of variation (Cv = σ∕μ). (b) illustrates the distribution of qB.
Figure 2: Contrastive learning and negative-positive coupling (NPC). (a) In SimCLR, each samplexi has two augmented views {xi(1) , xi(2)}. They are encoded by the same encoder f and furtherprojected to {zi(1), zi(2)} by a normalized MLP. (b) According to Equation 3. For the view xi(1), thecross-entropy loss Li(1) leads to a positive force zi(2), which comes from the other view xi(2) of x anda negative force, which is a weighted average of all the negative samples, i.e. {z(jl) |l ∈ {1, 2}, j 6= i}.
Figure 3: Comparisons on ImageNet-1K with/without DCL under different numbers of (a): batchsizes for SimCLR and (b): queues for MoCo. Without DCL, the top-1 accuracy significantly dropswhen batch size (SimCLR) or queues (MoCo) becomes very small. Note that the temperature τ ofSimCLR is 0.1, and the temperature τ of MoCo is 0.07 in the comparison.
Figure 4: Comparisons between DCL and SimCLR baseline on (a) CIFAR10, (b) CIFAR100, and (c)STL10 data. During the SSL pre-training, DCL speeds up the model convergence and provides betterperformance than the baseline on CIFAR and STL10 data.
