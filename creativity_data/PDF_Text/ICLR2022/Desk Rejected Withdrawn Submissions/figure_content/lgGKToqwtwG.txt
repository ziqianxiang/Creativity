Figure 1: The finetuned XLM-RoBERTa language model predicts German words using the prefix asinput (Green: Correct, Red: Incorrect, Black: Neutral).
Figure 2: Overview of the simultaneous translation model with future information.
Figure 3: Overview of the monotonic multihead attention with future information.
Figure 4: BLEU vs Average Lagging results for MMA, MMA-XLM and MMA-SLM models fordifferent speech segment step sizes.
Figure 5: LM prediction weight vs Î»(a) EnDe TaskFigure 6: Computational Aware Average Latency of MMA, MMA-XLM, MMA-SLM with similarBLEU scores and different latencies = {0.1, 0.05, 0.01}(b) EnFr Taskboth MMA-XLM and MMA-SLM improve the latency-quality trade-off and hence reduce the ALfor a given BLEU. As observed, MMA-SLM has lesser CAAL as compared to MMA since theextra computation time is balanced by the reductions in AL due to the algorithmic improvements.
Figure 6: Computational Aware Average Latency of MMA, MMA-XLM, MMA-SLM with similarBLEU scores and different latencies = {0.1, 0.05, 0.01}(b) EnFr Taskboth MMA-XLM and MMA-SLM improve the latency-quality trade-off and hence reduce the ALfor a given BLEU. As observed, MMA-SLM has lesser CAAL as compared to MMA since theextra computation time is balanced by the reductions in AL due to the algorithmic improvements.
