Figure 1: MILe. 1) Teacher and student are initialized with the same weights. 2) The teacher modelis trained with ground truth labels for kt iterations. 3) The student model is trained from the teacherpseudo-label predictions for ks iterations. Finally, the teacher is initialized again with the studentweights (1) and the process is repeated until convergence.
Figure 2: Qualitative results. ReaL: original labels. Sigmoid: ResNet-50 with sigmoid outputactivations. MILe: multi-label iterated learning (ours).
Figure 3: IIRC evaluation. (a) Average performance on IIRC-ImageNet-lite. (b) Average per-formance on IIRC-CIFAR10. We run experiments on five different task configurations and reportthe mean and standard deviation. Left: average performance when the tasks are equally weightedirrespective of how many samples exist per task. Right: average performance over the number ofsamples. In this case, the first task has more weight since it is larger in the number of samples.
Figure 4: Ablation study. Comparison between different iteration schedules. (a) Comparison withnoisy student (NS). (b)(c) Sweep over length of interactive learning phase kt and length of imitationphase ks. We report the ReaL-F1 score for 10% (b) and 100% (c) data fraction.
Figure 6: Multi-MNIST. The center digit has aprobability of 0.6 to be chosen as the label forthe whole grid.
Figure 7: ColoredMNIST+. During training, the model is asked to classifier either digits or colors.
