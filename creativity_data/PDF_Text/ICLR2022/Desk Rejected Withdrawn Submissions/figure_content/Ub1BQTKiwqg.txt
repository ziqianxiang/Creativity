Figure 1: Example showing that hard thresholding might lead to a change in weight in the forwardpath that is bigger than the gradient magnitude. In contrast, soft-thresholding preserves a smoothevolution of the forward weight compared to the one maintained in the backward path.
Figure 2: (Best viewed in colour) Top: Mean accuracy and variance, from 3 different initializationseeds, for a ResNet-20 (left) and VGG-11 (right) on Cifar10 as a function of the sparsity ratio for ourmethod with (dark orange) or without rewind (orange) compared to recursive rewinding (green) anda 1-cycle rewind (blue). Bottom: Number of training cycles necessary for the 4 different methods infunction of the pruning ratio・ R ・ Rec. Rw.
Figure 4: (Best viewed in colour) Mean accuracy and variance, from 3 different initialization seeds,for a ResNet-20 on Cifar10 as a function of the sparsity ratio for a global l1 pruning criterion (left)and LAMP (right).
Figure 3: (Best viewed in colour) Mean accuracy and variance, from 3 different initialization seeds,for a ResNet-20 on Cifar10 as a function of the sparsity ratio for different gradient densities. Resultsare shown without (left) and with (right) and additional rewind cycle. Recursive rewind and one-cycle act as a frame of reference.
Figure 5: (Best viewed in colour) Weight histograms for ResNet-20 on Cifar-10 at 98.8% spar-sity, trained with different methods. (Top) Pruning methods were used: OneCycle+rewind (left)and recursive (right). (Bottom) Our soft-thresholding method (left) and the follow-up result afterrewinding and retraining (right). For clarity’s sake, weights equal to zero have been removed of thehistogram.
Figure 6: (Best viewed in colour) Number of threshold crossings fora ResNet-20 trained on Cifar-10at 86.6% sparsity grouped into sets of 40 epochs. (left) the sparsity ratio grows linearly from Tcritto epoch 80 (right) full sparsity ratio is used from Tcrit onwards. Weights that undergo one or nothreshold crossings are removed for readability purposes.
