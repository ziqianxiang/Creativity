Figure 1: Illustration of the SMTL and L-SMTL models. Left figure: Pipeline for the SMTLmodel, which is identical to the training phase of L-SMTL. For task t, x is first fed into both thepublic encoder fS and private encoder ft, then it is through the gate gt to obtain the combinedfeature representation, and finally it is through the private decoder ht to obtain the output yt. Thenumber of tasks is set to three for illustration. Right figure: Test phase for L-SMTL. After finishingthe training process of L-SMTL, gt can choose which encoder (i.e., the public encoder fS or privateencoder ft) is used for each task. In this way, at the test process, only the chosen encoders need to besaved, which could reduce the number of parameters and speedup the inference. In this illustration,task 1 and task 3 choose the public encoder, while task 2 goes through its private encoder.
Figure 2: Illustration of the SMTLc and L-SMTLc models. Left figure: Pipeline for the SMTLcmodel, which is identical to the training phase of L-SMTLc . For task t, X is first fed into the publicencoder fS and the public decoder hS,t to get an output oS, and it is also fed into the private encoderft and the private decoder ht to get another output ot . Then it is through the gate to obtain thecombined output, i.e., y = ag + (1 - at)ot. The number of tasks is set to three for illustration.
