Figure 1: The overview of our proposed L2E framework.
Figure 2: L2E’s base policy can quickly adapt to various opponents of different styles and strengths.
Figure 3: Visualization of policies generated by Diverse-OSG.
Figure 4: Each curve shows the aver-age normalized returns of the base policytrained with different variants of L2E in thegrid soccer environment. Shaded regionsare 95% confidence intervals.
Figure 5: L2E’s performance converges after a certain number of iterations in Leduc poker.
Figure 6: Illustrations of the two-player zero-sum games we use for evaluation. Left: Leduc Pokerand Limit Texas Hold’em Poker. Middle: Grid Soccer. Right: RoboSumo Ants.
Figure 7:	The trained base policy using L2E can quickly adapt to opponents with different styles inthe Grid Soccer environment.
Figure 8:	The effects of the hyperparameter αmmd.
Figure 9:	The solid line shows L2E S adaptability in all environments.
