Figure 1: The illustration of cross-stage transformer network. (a) The framework of cross-stagetransformer for video learning. (b) Cross-stage transformer structure. STB indicates spatial trans-former block, and TTB indicates temporal transformer block. Blue arrows represent the directionof the cross-stage self-attention. The self-attention from each block is fused with that from the nextone. The features from different blocks are aggregated together in cross-stage feature aggregationmodule (FAM). Note that we only use 4 × STB and 3 × TTB for simplicity. (c) STB. (d) TTB.
Figure 2: The illustration of cross-stage self-attention (CSSA). (a) Cross-stage spatial self-attention. (b) Cross-stage temporal self-attention. For simplicity, we only show cross-stage self-attention flow of two consecutive transformer blocks.
Figure 3: The illustration of cross-stage feature ag-gregation module (FAM).
Figure 4: The visualization of self-attention map from the output token of a video clip, namely“A person was keeping bees”. The top row is the original video clip. The second row is the resultof the baseline without using cross-stage self-attention and feature. The third row shows the resultof CSTransformer. Brighter area means more attention has been focused on.
Figure 5: Comparison results between Baseline-V2 and CSTransformer-V2. The left representstop-1 accuracy of different epochs when input clip length is 8. The right denotes top-1 accuracy ofdifferent input clip lengths.
Figure 6: A video clip, namely ”A woman was arranging flowers”.
Figure 7: A video clip, namely ”A man was abseiling”.
Figure 8: A video clip, namely ”A person was biking through snow”.
Figure 9: A video clip, namely ”A girl was bending back”.
Figure 10: A video clip, namely ”A woman was answering questions”.
