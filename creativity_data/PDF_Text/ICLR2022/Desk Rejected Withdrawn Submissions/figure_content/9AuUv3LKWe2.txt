Figure 1: Multi-granularity alignment in Visual QuestionAnswering. For each modality, the input is represented bydifferent granularity levels, and Transformers (TRMs) areresponsible for learning the correlations between levels.
Figure 2: Overview of our approach. The following three granularity levels represent the imageinformation: concept level, region level, and spatial level. The input questions are described byentity level, noun phrase level, and sentence level information. Later, token features and lead graphsare extracted from each level, then input into three granularity alignment Transformers (GA-TRMs)accordingly. (The lead graph in the figure is for demonstration, where its value is randomly set.)Finally, the outputs are combined by the decision fusion module to obtain the final prediction.
Figure 3: Granularity Align-ment (GA) Attention.
Figure 4: Visualization of our MGA-VQA multi-modality alignment. The attention value is thenormalized learned attention of the last layer. The bounding boxes with IDs in the images correspondto the text with the same color in the table.
