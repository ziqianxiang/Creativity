Figure 1: Double descent phenomenon in sparse regimes for ResNet-18 with three pruning strategiesand varying permuted fraction . Top: CIFAR-10. Bottom: CIFAR-100. We plot train accuracy(solid lines in the upper sub-figures) and the last test accuracy of the final epoch (solid lines in thelower sub-figures), as well as the best test accuracy across all epochs (dotted lines).
Figure 2: Training dynamics w.r.t. epochs at five sparsities across different permuted fractions .
Figure 3: Memorization measured by train accuracy on unpermuted, permuted and restored samples.
Figure 4: Accuracy as a function of epochs during sparse and re-dense training process. Models areResNet-18 on CIFAR-100, with permuted fraction at 40%. Pruned weights are recovered at epoch160, and trained for another 160 epochs with a fixed learning rate of 0.001. We present models atdense, overfitting, two swet-spot and underfitting sparsities from left to the right.
Figure 5: Linear interpolation plots. Models are ResNet-18 on CIFAR-100 with = 40%. α = 0corresponds to sparse solutions and α = 1 corresponds to the re-dense solutions. The blue lines areloss curves and the red lines are accuracy curves; solid lines indicate training data set and dashedlines indicate testing data set. For re-dense models, sparsity is measured before recovering weights.
Figure 6: The 1-D loss visualization of minima found by re-dense training using filter normalization.
Figure 7: `2 distance from initialization and test accuracy as functions of sparsities. Left: LeNet-300-100 on MNIST with = 20%. Middle: ResNet-18 on CIFAR-10 with = 20%. Right:ResNet-18 on CIFAR-100 with = 40%. The blue lines are `2 distance curves and the red linesare accuracy curves; solid lines are results for re-dense solutions and dashed lines are for sparsesolutions. The vertical lines indicate where the curves of sparse and re-dense results come to cross,and signs of their relative difference shift.
Figure 8: Double descend phenomenon in sparse regimes for LeNet-300-100 on MNIST with threepruning strategies and varying permuted fraction .
Figure 9: Training dynamics w.r.t. epochs at five sparsities across different permuted fractions .
Figure 10: Memorization measured by train accuracy on unpermuted, permuted and restored sam-ples. Models are LeNet-300-100 on MNIST with different permuted fraction. We plot dense, over-fitting, swet-spot results.
Figure 11: Accuracy curve of the sparse and re-dense training process. We recover pruned weightsat epoch 200, and training them from value of zero for another 200 epochs using the last learningrate of sparse training, which is 0.1 for LeNet-300-100.
Figure 12: Memorization measured by train accuracy on unpermuted, permuted and restored sam-ples. Models are ResNet-18 on CIFAR-10 with = 20%. We plot dense, overfitting, two swet-spotand underfitting results from left to the right. Memorization capability of neural networks is dam-aged as pruning.
Figure 13: Accuracy curve of the sparse and re-dense training process. We recover pruned weightsat epoch 160, and training them from value of zero for another 160 epochs using the last learningrate of sparse training, which is 0.001 for ResNet-18.
Figure 14: Linear interpolation plots. Models are ResNet-18 on CIFAR-10 with = 20%. α = 0corresponds to sparse solutions and α = 1 corresponds to the re-dense solutions. The blue linesareloss curves and the red lines are accuracy curves; solid lines indicate training data set and dashed-lines indicate testing data set.
