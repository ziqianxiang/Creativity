Figure 1: Language-modulated detection for task-driven scene understanding in 2D (left) and3D (right). Boxes detected by object detectors (2D Faster-RCNN detector trained on 1601 VisualGenome classes and 3D Group-Free detector trained on 485 Scannet classes) often fail to localizethe object of interest (clock, bottle). The proposed model locates the relevant objects by attendingacross image, language and box proposal streams in 2D and in 3D.
Figure 2: BEAUTY-DETR architecture. The input to our model is a2D or 3D scene and a languageutterance; our goal is to localize in the scene the objects that are mentioned in the utterance. Thevisual scene and the utterance are encoded into a sequence of tokens each using a ResNet50 (orPointNet++ in case of 3D) and RoBERTa pre-trained visual and language encoders, respectively. Apre-trained object detector extracts object box proposals that are featurized using their spatial andcategorical information. At each encoder layer, visual and language tokens cross-attend and then thevisual tokens attend to the detected boxes. At the end of the encoder, visual tokens are mapped toconfidence scores and high-scoring tokens instantiate query vectors that will decode relevant objects.
Figure 3: Qualitative results of BEAUTY-DETR in the SR3D benchmark. Predictions for thetarget are shown in green and for other mentioned objects in orange. The detected proposals appearin blue. (a) The BD-no-boxes variant (red box) fails to exploit the information given by the detector,but BEAUTY-DETR succeeds. (b) The detector misses the “shoes” so the BD-box-only variant fails.
Figure 4: Qualitative results of BEAUTY-DETR on RefCOCO. Our model is robust even in casesthe detector has not captured the correct answer.
Figure 5: Failure cases of BEAUTY-DETR on SR3D. Our predictions with red, ground-truth withgreen. Even if the box is there, still our model can fail, proving that ranking the correct boxes overother proposals remains a hard problem.
