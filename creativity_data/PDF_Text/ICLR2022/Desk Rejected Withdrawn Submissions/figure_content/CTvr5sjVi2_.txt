Figure 1: Performance drop with respect to action modification percentage P2 (p) with a2 for thestate-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies.
Figure 2: Performance drop with respect to action modification percentage Pw(p) with aw for thestate-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies.
Figure 3: P2 and Pw for adver-sarially trained deep neural poli-cies.
Figure 4: Q-Value of the best action a* over the states for the state-of-the-art adversarially traineddeep neural policy and vanilla trained deep neural policy.
Figure 5: Normalized state-action ValueS2for the best action a*, second best action a2 and worst actionaw over states. Row1: Vanilla trained deep neural policies. Row2: State-of-the-art adversariallytrained deep neural policies.
