Figure 1: Performance of SGD, Adam, AMSGrad, RMSProp and THεO POULA on an artificialexample with the initial value θ0 = 5.0for all x ∈ R and θ, θ0 ∈ R. Also, the optimal value is attained at θ = 0. See Appendix A for moredetails. Following Reddi et al. (2018), adaptive stochastic gradient methods can be generally writtenas follows, for n ∈ N,mn =	φn(GI,…，Gn),Vn	=	Ψn(Gl,…，Gn),θn+1	=	θn - λn	InmPε+ Vn(3)where Gi := G(θi , Xi) is the stochastic gradient evaluated at the i-th iteration, λn is the step sizeand all operations are applied element-wise. Table 1 provides the details for some of the mostpopular stochastic optimization methods with corresponding averaging functions φn and ψn .
Figure 2: TeSt accuracy for VGG11, ReSNet34 and DenSeNet121 on CIFAR-10 and CIFAR-100.
Figure 3: Test perplexity for 1, 2 and 3-layer AWD-LSTMs on PTBE.3 Effectiveness of the boosting functionThis subsection empirically tests the effectiveness of the boosting function in our algorithm. THεOPOULA without the boosting function updates the parameter as follows:θ0 := θ0,	θλ+1 := θλ - λHλ,c (θλ,Xn+ι) + p2λβTξn+ι,	n ∈ N,18Under review as a conference paper at ICLR 2022where Hλ,c ：= (H黑(θ, x),…,破dC(θ, X))T is given byH⑺(θ x) =______G(C) (θ,x)___+ ηλ,c ,	1 + √λ∣G(i)(θ, x)|	0θ(i)∣θ∣2r1 + √λ∣θ∣2r ,(E.1)and {ξn}n≥1 is a sequence of independent standard d-dimensional Gaussian random variables.
Figure 4: simulations of a multi-modal distribution.
