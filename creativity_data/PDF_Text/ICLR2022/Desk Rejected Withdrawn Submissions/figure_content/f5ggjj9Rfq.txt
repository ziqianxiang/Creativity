Figure 1: Figure showing four AOVs (solid lines) and their corresponding loss values (dashed lines)evolving during training. Left: In blue we show the behaviour of an “easy” sample that quicklyreaches zero loss and remains there for the rest of training. In green we show a “hard” examplethat has high loss throughout training. Right: Here we demonstrate the adaptability of our AOVupdate heuristic, where the red sample increases in value toward the end of training when initiallylow. Conversely, the black sample is initially too high and decreases in value from epoch 80 to 140.
Figure 2:	Training performance on the matrix factorisation problem of Vaswani et al. (2019b). In thesettings where interpolation does not hold, namely the Rank 1 and Rank 4 problems, AALIG quicklyachieves the loss floor. For the Rank 10 and True model problems AALIG does not minimise theloss to machine precision, such as SLS (Vaswani et al., 2019b) and PAL (Mutschler & Zell, 2020),however, it still provides rapid optimisation to at worst > 10-4.
Figure 3:	Training and validation performance on the mushrooms and ijcnn data sets (Chang &Lin, 2011). On the mushroom data set, where interpolation holds, AALIG fails to achieve thesame training loss as the line search methods. However, in both non-interpolating and interpolatingsettings AALIG obtains equally good validation performance as the best baseline.
Figure 4:	Curves produced by training a small ResNet on CIFAR100 with the AALIG optimiser. TheAOVs are updated every 20 epochs. At the first update the mean AOV value increases significantly,however, by the second update it remains almost constant due to a portion AOVs increasing in valueand others decreasing. Until epoch 40 the loss for each sample is significantly higher than the AOVsand thus the maximum step-size (η = 0.1) is used for all updates. Shortly after epoch 40 the meanloss becomes larger than the AOV value, and hence the step size used for many batches becomeszero, causing a lower mean value. At epoch 60 many of the AOVs are reduced as they have been“reached” resulting in the step size increasing for roughly 10 epochs, until roughly epoch 70. For therest of training the AOVs begin to stabilise and the loss is slowly decreased as the optimiser focuseson samples where the current AOV has not been reached.
