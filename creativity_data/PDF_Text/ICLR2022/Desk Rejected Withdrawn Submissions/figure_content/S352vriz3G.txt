Figure 1: Subsets of composition spaceWe present specific examples of generalizable composition functions, which neural models can ac-quire easily, but which humans do not recognize as compositional, and which current compositionalmetrics consider to be non-compositional. In addition, we present a grammar, shufdet, whosecomposition humans can understand but which neural models cannot. We propose a novel neuralarchitecture, HU-RNN, that can acquire shufdet faster than other neural models.
Figure 2: Signaling Game. ‘adab’ is an example message. (red, box) is an example object.
Figure 3: Values of compositionality metrics for each of the artificial grammars. Each bar is anaverage over 5 seeds. tre7 uses right-hand y-axis. High tre7 means low compositionalityFigure 3 shows the values of compositional metrics for samples from our artificial grammars, usingnatt = 5, nval = 10. The compositionality metrics show low compositionality for all the artificialgrammars, except for concat and perm. Thus our transformations successfully hide the composi-tional structure from current compositional metrics.
Figure 4: User interface for our ‘Secret Spy Codes’ game. This image depicts a perm grammar,where the original utterance was ‘redcir’.
Figure 5: HU-RNN Sender Architecture•	The hashtable made a prediction. For previously unseen inputs, the hashtable predicted all0s.
Figure 6: HU-RNN Receiver ArchitectureC.4 SRUSee Lei et al. (2018). An SRU is derived from an LSTM but with no connection between the hiddenstates. Connections between cell states remain. An SRU is much faster than an LSTM to train ona GPU. ‘RNN2L:SRU’ is a synonym for a 2-layer SRU, i.e. SRU-2L. We treat SRU as any otherRNN, see above.
Figure 7:	Training curves for LSTM sender and LSTM receiver placed end-to-end after supervisedtraining on the specified grammar. Each curve is mean over 10 runs, and shading is CI95. natt = 5.
Figure 8:	User interface for our ‘Secret Spy Codes’ game. This image depicts a perm grammar,where the original utterance was ‘redcir’.
