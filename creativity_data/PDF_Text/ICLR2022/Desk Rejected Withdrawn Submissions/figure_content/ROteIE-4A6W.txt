Figure 1: Overview of the original CLIP (left) and our proposed MS-CLIP (right).
Figure 2:OvervieWofMS-CLIP-S.
Figure 4: Visualized attention maps of shared attention head.
Figure 3: Diagram of comput-ing Common Semantic Struc-ture distanceVisual ConceptThe layer-wise CSC distance of CLIP (ViT-B/32), MS-CLIP(B/32), MS-CLIP (B/32) + Early Specialization and MS-CLIP-S (B/32) are reported in Tab. 9. It is worth noting We use 10kimage-caption pairs from FliCk30k-Entity to compute, which islarge enough for getting a stable CSC distance. Since the firstlayer of MS-CLIP (B/32) + Early Specialization and MS-CLIP-S (B/32) doesn,t contain attention module in vision branch, Weaverage the last 11 layers, CSC distance to evaluate it. We canfind that both the modality-shared Transformer blocks and pro-posed auxiliary modality-specific modules can lower the CSC dis-tance and learn more semantic structure similarity of vision andtext. It is natural that sharing parameters can enforce the attentionto learn more common information. As for proposed modality-specific modules, we suspect that those well designed models canaccount for the discrepancy of separate modalities and make theremaining shared modules focus more on the common patterns.
