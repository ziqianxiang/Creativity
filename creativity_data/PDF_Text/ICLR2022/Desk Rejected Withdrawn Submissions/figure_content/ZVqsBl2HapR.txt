Figure 1: Framework schematics. (A) Graphical depiction of a general Behavioural Cloning task. An agent (here arecurrent network) observes the current state-action pair of a target agent and is trained to emulate such behaviour. Themodel assumes the presence of additional constraints. The total number of independent constraints D defines the rankof the error propagation matrix. (B) Schematics of difference τ? , the spikes filtering timescales. A larger τ? is moretolerant on precise spike timing. (C) Schematics of our generalized framework. Changing the D and τ? parameters, it ispossible to derive different learning algorithms.
Figure 2: Error propagation and dimensionality of the internal solution. (A). Dynamics along training epochs ofthe ∆Si = Pt |si? t - sit | in the first two principal components for different repetition of the training with variableinitial conditions. The error propagation matrix has maximum rank (D = N , target-based limit). (B). Same as in (A),but with an error propagation matrix with rank D = N - 5. (C). Dimensionality of the solution space S∞ as a functionof the rank D of the error propagation matrix.
Figure 3: Target-based learning for different time-scales. (A). Color-coded the error on the spike sequence ∆S =Pit |si? t - sit| as a function of the number of iterations for different τ?. (B). Color-coded the mse = Pkt(yk? t - ykt )2as a function of the number of iterations for different τ? . (C). Scatter plot of mse vs ∆S for different values of τ? .
Figure 4: Button-and-food task. (A) Sketch of the task. An agent start at the center of the environment domain (left)and is asked to reach a target. The target is initially "locked". The agent must unlock the target by pushing a button(middle) placed behind and then reach for the target (right). (B) Example trajectories produced by a trained agent fordifferent target locations. Purple arrows depict the observed expert behaviours. (C) Final reward obtained by a trainedagents as a function of the target position (measured by the angle θ with a fixed radius of r = 0.7 as measured from theagent starting position). Continuous lines are average values, while error bars are standard deviation for 10 repetitions.
Figure 5: 2D Bipedal Walker. (A). Representation of the 2D Bipedal Walker environment. The task is to successfullycontrol the bipedal locomotion of the agent, reward is measured as the travelled distance across the horizontal direction.
Figure 6: Dimensionality of the solution space: R random. Dimensionality of the solution space S∞ as a functionof the rank D of the error propagation matrix.
Figure 7: Button & Food: R random. (A) Average reward as a function of the test angle θ in the Button & Food task.
Figure 8: Bipedal Walker 2D. Non clumped version. (left). Spike error ∆S as a function of the τ? . (right). Averagereward as a function of the τ? .
