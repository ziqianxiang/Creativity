Figure 1: We present a self-supervised technique for discovering recurring temporal patterns, called actons,in long kinematic sequences like human dance. From a collection of such videos without any annotations, Weextract a set of actons and use this lexicon to segment and model motion sequences as shown above (skeletonsequence temporally downsampled by 10Ã—).
Figure 2: Overview of our acton discovery framework. First, a novel Temporal Alignment Network (TAN)through self-supervised contrastive learning is used to extract frame-wise, context-aware and temporally alignedfeatures. Then, frame-wise K-means is used to cluster and segment long human dances into actons.
Figure 3: Illustration of Temporal Alignment Network (TAN), the first part of our system. Two separateaugmentations are sampled from the an augmentation family T (speed, rotation and translation) to obtain twocorrelated views probably of different lengths. Ground-truth correspondence between frames of the two views iscalculated. All frames are projected into a common embedding space, where a frame-wise contrastive loss isminimized.
Figure 4: Mean and standard deviation over 6trials of genre classification, plotted against rawskeleton method.
Figure 5: Visualization of discovered actons from advanced videos. For each acton, four instances are shownside to side, with temporally earlier poses fading away. At the bottom left, an acton E with large intra-clustervariance is shown; to its right are four acton instances facing different directions but underpinned by the samemotion.
Figure 6: Continued Figure 5, visualization of discovered actons from advanced videos. For each acton, fourinstances are shown side to side, with temporally earlier poses fading away. At the bottom left, an acton Mwith large intra-cluster variance is shown; to its right are four acton instances facing different directions butunderpinned by the same motion.
Figure 7: The left histogram shows the distribution of segment length across all segments. The right histogramshows the distribution of repetition number, or number of instances, for all discovered actons.
Figure 8: t-SNE visualization of frame embeddings of videos with different tempos. In the left, frames arecolored by the video they belong to; on the right, they are colored by the cluster they belong to. At the bottomis a visualization of tokenization results on the same four videos. Each line is one advanced video. Dot sizecorresponds to acton instance duration. Consistent colors are used for the t-SNE plot in the right and thetokenization visualization.
Figure 9: Illustration of segmentation on advanced videos in AIST++. Each line is one advanced video. Dotsize corresponds to acton instance duration. Different colors represent different actons.
