Figure 1: Synthesized images. The generated image from icfg is blur. The season is the weaknessability of the discriminator to differentiate low quality generated images the and the real images.
Figure 2: Result for SVHN:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=1 cfg-alpha=0.9Figure 3: Result for CIFAR10:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 4: Result for LSUN B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 5: Result for LSUN T:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 6: Result for LSUN C:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments results
Figure 3: Result for CIFAR10:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 4: Result for LSUN B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 5: Result for LSUN T:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 6: Result for LSUN C:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG and
Figure 4: Result for LSUN B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 5: Result for LSUN T:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 6: Result for LSUN C:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG andother methods. The Inception score, FreChet Distance, and the visual quality of generated imageshow that our method is more stable. In future work, we plan to investigate the generator object
Figure 5: Result for LSUN T:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=0.5 cfg-alpha=0.9Figure 6: Result for LSUN C:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG andother methods. The Inception score, FreChet Distance, and the visual quality of generated imageshow that our method is more stable. In future work, we plan to investigate the generator objectfunction from KL diversity to the Wasserstein distance as to achieve more stable and efficient GANsarchitecture.
Figure 6: Result for LSUN C:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.9Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG andother methods. The Inception score, FreChet Distance, and the visual quality of generated imageshow that our method is more stable. In future work, we plan to investigate the generator objectfunction from KL diversity to the Wasserstein distance as to achieve more stable and efficient GANsarchitecture.
Figure 7: Result for LSUN B+L:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.98Under review as a conference paper at ICLR 2022Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG andother methods. The Inception score, FreChet Distance, and the visual quality of generated imageshow that our method is more stable. In future work, we plan to investigate the generator objectfunction from KL diversity to the Wasserstein distance as to achieve more stable and efficient GANsarchitecture.
Figure 8: Result for LSUN T+B:above is ICFGW and below is ICFG lr=0.00025 cfg-eta=10 cfg-alpha=0.95	ConclusionIn this paper, we introduced the Wasserstein regularization into the Composite Functional GradientLearning (CFG) which is a new theoretical way to train GAN. While the discriminator of standardICFG is very sensitive to varying hyper-parameters. The effect of its differentiates is not goodenough. But our ICFGW work much better with various hyper-parameters. The experiments resultsdemonstrate that the proposed approach shows more stable performance compared with ICFG andother methods. The Inception score, FreChet Distance, and the visual quality of generated imageshow that our method is more stable. In future work, we plan to investigate the generator objectfunction from KL diversity to the Wasserstein distance as to achieve more stable and efficient GANsarchitecture.
