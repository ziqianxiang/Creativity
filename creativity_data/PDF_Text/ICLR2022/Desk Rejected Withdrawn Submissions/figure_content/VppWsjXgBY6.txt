Figure 1: Overview of the proposed TLDR, a dimensionality reduction method. Given a set offeature vectors in a generic input space, we use nearest neighbors to define a set of feature pairs whoseproximity we want to preserve. We then learn a dimensionality-reduction function (the encoder) byencouraging neighbors in the input space to have similar representations. We learn it jointly with anauxiliary projector that produces high dimensional representations, where we compute the BarlowTwins (Zbontar et al., 2021) loss over the d0 × d0 cross-correlation matrix averaged over the batch.
Figure 2: Image retrieval experiments. Mean average precision (mAP) on ROxford5K (left) andRParis6K (right) as a function of the output dimensions d. We report TLDR with different encoders:linear (TLDR), factorized linear with 1 hidden layer (TLDR1), and a MLP with 1 hidden layer(TLDR1?), the projector remains the same (MLP with 2 hidden layers). We compare with PCA withwhitening, two baselines based on TLDR, but which respectively train with a reconstruction (MSE)and a contrastive (Contrastive) loss, and also with TLDRG, a variant of TLDR which uses Gaussiannoise to synthesize pairs. The original GeM-AP performance is also reported.
Figure 4: Impact of TLDR hyper-parameters with a linear encoder and d = 128. Dashed (solid)lines are for RParis6K-Mean (ROxford5K-Mean). (Left) Impact of the auxiliary dimension d0 andthe number of hidden layers in the projector. (Right) Impact of the number of neighbors k . We seehow the algorithm is robust to the number of neighbors used.
Figure 3: Argument retrieval results on ArguAna for different values of output dimensions d. Onthe left we vary the amount of factorized layers, with fixed k = 3, on the right we fix the amount offactorized layers to 2 and test k = [3, 10, 100]. Factorized linear is fixed to 512 hidden dimensions.
Figure 5: Left: Comparisons to manifold learning methods for small output dimensions d ≤ 128.
Figure A: Image retrieval experiments. Mean average precision (mAP) on ROxford5K (top) andRParis6K (bottom), for the Medium (left) and Hard (right) test sets, as a function of the outputdimensions d. We report TLDR with different encoders: linear (TLDR), factorized linear with 1hidden layer (TLDR1), and a MLP with 1 hidden layer (TLDR1?), the projector remains the same(MLP with 2 hidden layers). We compare with two baselines based on TLDR, but which respectivelytrain with a reconstruction (MSE) and a contrastive (Contrastive) loss. Our main baselines are PCAwith whitening, and the original 2048-dimentional features (GeM-AP Revaud et al. (2019)), i.e.
Figure B: Neighbor-supervised with oracle. Mean average precision (mAP) on ROxford5K (Rade-noviC et al., 2018a) for the Medium (left) and Hard (right) test sets, as a function of the outputdimensions d. We compare TLDR with an oracle version that uses labels to select training pairs. Weinclude as baselines both PCA and ICA with whitening, and the original 2048-dimentional features(GeM-AP [32]), i.e. before projection.
Figure C: ResNet-101 features. Mean average precision (mAP) on ROxford5K (RadenoVic et al.,2018a) for different values of output dimensions d, using features obtained from the pre-trainedResNet-101 of Revaud et al. (2019).
Figure D: TLDR benefits from larger training sets.
Figure E: The surprising stability of TLDR across batch sizes. Impact of the size of the trainingmini-batch on performance. TLDR uses a linear encoder and d = 128.
Figure F: Argument retrieval results on the ArguAna dataset using Webis-Touche 2020 fordimensionality reduction for different values of output dimensions d. On the left we presentRecall@100 and on the right we present NDCG@10.
Figure G: Argument retrieval results on the Webis-Touche 2020 dataset using Quora for dimen-sionality reduction for different values of output dimensions d. On the left we present Recall@100and on the right we present NDCG@10.
Figure H: Duplicate question retrieval results on the CQADupstack dataset using Quora fordimensionality reduction for different values of output dimensions d. On the left we presentRecall@100 and on the right we present NDCG@10.
Figure I: Duplicate question retrieval results on the Quora dataset using Webis-TouChe 2020for dimensionality reduction for different values of output dimensions d. On the left we presentRecall@100 and on the right we present NDCG@10.
Figure J: Results on the FashionMNIST dataset as a function of the output dimensions d. WecomPare TLDR with PCA, PCA with whitening, UMAP and IsomaP and rePort accuracy after k0-NNclassifiers (with k0 = 100) following (McInnes et al., 2018). For TLDR and UMAP we set thenumber of neighbors k = 100. The Performance of UMAP was very low for d0 > 32.
Figure K: 2D visualizations of the training set of FashionMNIST. From top to bottom and left toright: t-SNE, MDE, UMAP and TLDR.
