Figure 1: Policy value calculation when H = 2. Each ensemble member separately unrolls thepolicy for H steps for MVE and one final step for the terminal value. The action at step i sampledfrom the joint distribution ∪j∏(x(j), i). The value estimate by ensemble member j is PH=-01 Ya ∙r(j)(x(j),a, Ua) + YHQ(∏(x(j), H)). The mean (or minimum) of the individual estimates is used asthe final value estimate. The exploration temperature is omitted for brevity.
Figure 2: Training collapse with max, but not with mean. This is demonstrated by plotting thereward obtained from both mean and max when training a model on Walker2d-v2. The errorbars indicates the range of observed values over five seeds. While both mean and max learn at thestart, the training of max falls to a small value.
Figure 3: We compute the Pearson correlation coefficient between Ensemble Q-values and vanillaQ-values for a fixed set of states every other training step for a subset of the training steps. TheEnsemble Q-values lead the vanilla Q-values, as shown by the rightward skew. This suggests thatthe MVE construction counteracts some delay, perhaps introduced by the slow target network.
