Figure 1: (a) An ideal data generation process that follows the multi-view redundancy assumption. (b)When the data generation process in (a) holds, all the task-relevant information H(Y ) is contained inthe shared part between two views -1(Xa, XB). (c) An illustration for the impact of the informationcontained in the embeddings on the performance of downstream tasks. Zoom in for better view.
Figure 2: Data flow of synthetic dataset. The raw features are generated from two random variables:H is the hidden variable that decides the label, and S is the superfluous variable (e.g., controlsaugmentation-invariant features). The input feature x is the MLP output of the concatenation ofhidden vector h sampled from H and superfluous vector s sampled from S . The input features of twoviews xa and xb have shared hidden vector but different superfluous vector with x. fθ is a learnableencoder parameterized by a 3-layer MLP. Other MLPs are randomly generated with fixed weights.
Figure 3: Effect of λ. Figure 4: GPU memory cost. Figure 5: Time consumption.
Figure 6: Top-1 accuracy for image classification on three public datasets. The dotted horizontal linerepresents the accuracy of baseline model (SimCLR).
