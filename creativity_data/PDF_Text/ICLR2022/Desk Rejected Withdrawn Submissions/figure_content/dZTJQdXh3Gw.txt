Figure 1: Is a CNN architecture that performs wellon ImageNet automatically a good choice for a dif-ferent vision dataset? This plot suggests otherwise:It displays the relative test errors of 500 randomlysampled CNN architectures on three datasets (Ima-geNet, Powerline, and Insects) plotted against thetest error of the same architectures on ImageNet.
Figure 2: The structure of models in the AnyNetXdesign space, with a fixed stem and a head, consist-ing of one fully-connected layer of size c, (where cis the number of classes). Each stage i of the bodyis parametrised by di , wi , bi , gi , the strides of thestages are fixed with s1 = 1 and si = 2 for theremainder.
Figure 3: Test errors of all 500 sampled architectures on target datasets (y-axis) plotted against the testerrors of the same architectures (trained and tested) on ImageNet (x-axis). The top 10 performanceson the target datasets are plotted in orange and the worst 10 performances in red.
Figure 4: Error of all 500 sampled architectures on subsampled (by number of classes) versions ofImageNet (y-axis) plotted against the error of the same architectures on regular ImageNet (x-axis).
Figure 5: Test errors of all 500 sampled architectures on target datasets (y-axis) plotted against thetest errors of the same architectures on the ImageNet-X (x-axis). The top 10 performances on thetarget dataset are orange, the worst 10 performances red.
Figure 6: Errors of all 500 sampled architectures on ImageNet, Insects, HAM10000, Powerline,Natural, and Cifar100 (x-axis) plotted against the cumulative block depths (y-axis).
Figure 7: Errors of all 500 sampled architectures on ImageNet, Insects, HAM10000 and Cifar100(x-axis) plotted against the cumulative block widths (y-axis).
Figure 9: The Cifar10 test errors of all 500 architectures plotted against ImageNet (top row) andImageNet-10 (bottom row), shown for our original Cifar10 training (left column), training with aCifar10 specific stem in the architecture (middle column), and training for 200 epochs, which isroughly 6 times longer (right column). The plots show that the error correlation with ImageNet-10 ismuch larger in all three cases, confirming that optimizing for individual Cifar10 performance doesnot alter our core result.
Figure 10: Cifar10 test error curves of 20 randomly sampled architectures trained over 200 epochs(left). The same error curves but cut to epochs 30 to 200.
Figure 11: Test error curves of the five best and five worst models on Powerline and Natural,respectively, when training is continued to epoch 100B.3	Impact of Training VariabilityThe random initialization of the model weights has an effect on the performance of a CNN. In anempirical study it would therefore be preferable to train each model multiple times to minimize thisvariability. We opted to increase the size of our population as high as our computational resourcesallow, this way we get a large number of measurements to control random effects as well as an errorestimate of a large set of architectures. However, we still wanted to determine how much of the total16Under review as a conference paper at ICLR 2022variability is caused by training noise and how much is due to changing the architectures. We estimatethis by selecting two of the sampled CNN designs, number 147 performing slightly above averagewith an error of e147 = 11.9 and number 122 performing slightly below average with e122 = 14.5.
Figure 12: Error distributions on Cifar10 of two architectures (122, 147) both trained from scratch250 times as well as the Cifar10 error distribution of all 500 architectures. The plot shows that thevariability caused by changing architecture is much larger than the one caused by random trainingeffects.
Figure 13: Top-1 error plotted against top-5 error of all 500 architectures on ImageNet, Cifar100, andInsects. The plots reveal that on all three datasets the errors have a very close relationship: it is notperfectly linear but is monotonically ascending.
Figure 14:	Training errors of the sampled architectures (x-axis) plotted against the cumulated blockdepth for the 3 datasets that have the lowest test errors on shallow architectures. We observe that forall three datasets shallow architectures also have the lowest training errors. Therefore overfitting isnot the cause of this behaviour.
Figure 15:	The errors of all 500 architectures on Cifar10, Natural, and Powerline plotted against theerrors on ImageNet (top row), ImageNet-1000-10 (middle row) and ImageNet-10 (bottom row). Weobserve that class-wise downsampling has the largest positive effect on error correlation.
Figure 16: Class distributions of MLC2008, HAM10000, and their balanced versions.
Figure 17: Errors of all 500 sampled architectures on MLC2008-balanced and HAM1000-balanced(y-axis) plotted against the errors of their unbalanced counterparts (x-axis). The top 10 performanceson the target dataset are plotted in orange, the worst 10 performances in red. We observe a clearpositive correlation for both datasets, hence we conclude that the dataset imbalance has a limitedimpact on the APRs.
Figure 18:	Errors form all 500 architectures trained from scratch (blue) as well as the same architec-tures pretrained on ImageNet (green), plotted against the respective ImageNet errors. We observethat the error correlation with ImageNet increases relative to the performance gain due to pretraining.
Figure 19:	The eCDF s of the restricted sub-populations in the context of the eCDF of the wholearchitecture population, for all datasets.
Figure 20: Configurations of the top-performing architectures, with the four stages depicted on thex-axis and the parameter values on the y-axis. The best architectures are shown in blue, the mean ofthe top 15 architectures is depicted in orange with with a vertical indication of one standard deviation.
Figure 21: Matrix of error scatterplots of all datasets except Concrete (The first row replicates plotsshown in Figure 3).
Figure 22: Individual parameter by stage versus error scatterplots for the Concrete dataset. The x-axisgives the error, while the parameter values are given on the ordinate.
Figure 23: Individual parameter by stage versus error scatterplots for the MLC2008 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 24: Individual parameter by stage versus error scatterplots for the ImageNet dataset. Thex-axis gives the error, while the parameter values are given on the ordinate. The x-axis gives the error,while the parameter values are given on the ordinate.
Figure 25: Individual parameter by stage versus error scatterplots for the HAM10000 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 26: Individual parameter by stage versus error scatterplots for the Powerline dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 27: Individual parameter by stage versus error scatterplots for the Insects dataset. The x-axisgives the error, while the parameter values are given on the ordinate.
Figure 28: Individual parameter by stage versus error scatterplots for the Natural dataset. The x-axisgives the error, while the parameter values are given on the ordinate.
Figure 29: Individual parameter by stage versus error scatterplots for the Cifar10 dataset. The x-axisgives the error, while the parameter values are given on the ordinate.
Figure 30: Individual parameter by stage versus error scatterplots for the Cifar100 dataset. The x-axisgives the error, while the parameter values are given on the ordinate.
Figure 31: Individual parameter by stage versus error scatterplots for the ImageNet-100 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 32: Individual parameter by stage versus error scatterplots for the ImageNet-10 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 33: Individual parameter by stage versus error scatterplots for the ImageNet-5 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
Figure 34: Individual parameter by stage versus error scatterplots for the ImageNet-2 dataset. Thex-axis gives the error, while the parameter values are given on the ordinate.
