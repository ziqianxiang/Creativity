Figure 1: Illustrative example of the gradient direction (Top), adversarial samples (middle) andtrained decision boundary (Bottom) of different adversarial perturbation schemes. (a) Original datawithout perturbation; perturbed data using (b) PGD, a supervised adversarial attack method; (c)PGD with the proposed perturbation diversity (d) Feature Scattering, and (e) Feature Scattering withthe proposed perturbation diversity.
Figure 2: Illustration of perturbation diversity. Left: Directions of conventional adversarial exam-ples. Right: Directions of adversarial examples with Perturbation Diversity. When the perturbationsare orthogonal to each other, their diversity can be guaranteed, hence promoting a more smooth ro-bust decision boundary. Details can be seen in Fig. 1 and Section 3.1.
Figure 3:	(a) to (d): TSNE manifold of the training and test data. All test data are attacked by PGD,the training data are attacked by a) PGD, b) PGD+PD, c) FS, and d) FS+PD. (e): Training timeconsumed at per epoch vs. Batch size.
Figure 4:	AttaCk Budget () vs. Robust ACCuraCy on the three baseline. Top Line: CIFAR-10;Middle Line: CIFAR-100; Bottom Line: SVHN.
