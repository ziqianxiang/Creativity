Figure 1: Visualizations of LV-ViT and comparison between hard dropping and soft slimming.
Figure 2: The framework of our self-slimming learning. We insert our token slimming module(TSM) and the reverse version (RTSM) into vanilla vision transformers. Specially, the RTSM is onlyused during training. The dense knowledge distillation (DKD) applies layer-to-layer supervision tothe recovered tokens of RTSM and the final predictions. The dash lines indicate the predictionsupervision from the extra CNN teacher is optional and complementary to our method.
Figure 3: The pipelines of the token slimming module (TSM) and its reverse version (RTSM).
Figure 4: Speed vs. accuracy. “X” is short for “ResNeXt”. The throughput is measured on a single16GB V100 GPU under the same setting as Graham et al. (2021). Our SiT surpasses EfficientNetV2(Tan & Le, 2021) and Le-ViT (Graham et al., 2021), which are designed for fast inference.
Figure 5: Visualizations of our progressive token slimming. The darker tokens contribute less to thefinal informative tokens. Our method can zoom the attention scope to cover the key object.
Figure 6: Cross CKA heatmap between different student models and the teacher models.
Figure 7: More visualizations of our SiT.
