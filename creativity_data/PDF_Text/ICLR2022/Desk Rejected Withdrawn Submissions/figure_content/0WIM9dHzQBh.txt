Figure 1: Illustration of the DP-InStaHide defense on two CIFAR-10 images, the first of which hasbeen poisoned with Îµ = 16. Mixup is used to average two images, and then Laplacian noise is added,We define the threat model as taken from Ma et al. (2019): The attacker aims to direct the trainedmodel M(Dr) to reach some attack target by modifying at most l elements of the clean dataset D toproduce the poisoned dataset D0. We measure the distance of M(D0) from the attack target usinga cost function C, which takes trained models as an input and outputs an element of R. The attackproblem is then to minimize the expectation of the cost of M(D0).
Figure 2: Theoretical and empirical mixup. Left: Privacy guarantee as a function of mixturewidth k, computed for each implemented Laplacian noise level s. We use values n = N = 50000,corresponding to the CIFAR-10 dataset. Right: Poisoning success for a strong adaptive gradientmatching attack for several mixture widths and noise levels.
Figure 3: On the left: Enhancing various data augmentations with Laplacian noise. We visualize thesecurity-performance trade-off when enhancing the data augmentations considered in Sec. 3 withLaplacian noise as predicted by Thm. 2. We visualize the development of these data augmentationswhen adding Laplacian noise with scales (2/255, 4/255, 8/255, 16/255, 32/255). On the right:Trade-off between average poison success and validation accuracy for various defenses againstgradient matching (adaptive).
