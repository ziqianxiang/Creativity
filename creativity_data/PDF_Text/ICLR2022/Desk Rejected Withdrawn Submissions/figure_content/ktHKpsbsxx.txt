Figure 1: Two stream architecture of WeaveNet. It embeds inputs P and Q into two bands of latentfeatures Z1 and Z2 through L FW layers with the residual structure. The two streams communicatethrough the cross-concatenation in every FW layer. Z1 and Z2 are further processed through aconvolution layer with 1 × 1 kernel size, which yields the output M .
Figure 4: Accuracy in findingstable matching (↑) accordingto N一 bαsti3M0I ©qa°°“汩“®h二。ɔɑ T 一3 X 3	5 X 5	7 X 7	9 X 9NFigure 5: Accuracy in findinglowest SEq matching (solid)and errors (dashed)0 9 8 7 6 5 4Iaaaaaa一 Py3M0I 3q4py6DV T 一4J3d joi,iəJ3>v -I I->> ' .> ,> I >, ≡ ,1,Figure 6: Accuracy in findinglowest Bal matching (solid)and errors (dashed)Fig. 4 shows the accuracy in finding a stable matching, where we trained models to minimize lossesother than those related to fairness objectives (see A.1 in the appendix for details). Since MLPand GIN have size-dependency, we trained the models independently for N = 3, 5, 7, 9. The
Figure 5: Accuracy in findinglowest SEq matching (solid)and errors (dashed)0 9 8 7 6 5 4Iaaaaaa一 Py3M0I 3q4py6DV T 一4J3d joi,iəJ3>v -I I->> ' .> ,> I >, ≡ ,1,Figure 6: Accuracy in findinglowest Bal matching (solid)and errors (dashed)Fig. 4 shows the accuracy in finding a stable matching, where we trained models to minimize lossesother than those related to fairness objectives (see A.1 in the appendix for details). Since MLPand GIN have size-dependency, we trained the models independently for N = 3, 5, 7, 9. Theother models were trained with N = 10 and tested on N = 3, 5, 7, 9. As a result, MLP, GIN,and DBA-A could hardly find stable matching when N ≥ 5. DBA_P performs better but is quiteunstable. WN and SSWN could stably approximate the problem with a good performance.
Figure 6: Accuracy in findinglowest Bal matching (solid)and errors (dashed)Fig. 4 shows the accuracy in finding a stable matching, where we trained models to minimize lossesother than those related to fairness objectives (see A.1 in the appendix for details). Since MLPand GIN have size-dependency, we trained the models independently for N = 3, 5, 7, 9. Theother models were trained with N = 10 and tested on N = 3, 5, 7, 9. As a result, MLP, GIN,and DBA-A could hardly find stable matching when N ≥ 5. DBA_P performs better but is quiteunstable. WN and SSWN could stably approximate the problem with a good performance.
Figure 7: Comparison in 3D point cloud matching.
Figure 9: Sensitivity against λsFigure 8: Sensitivity against λmλfFigure 11: Sensitivity against λbFigure 10: Sensitivity against λfmost biased distribution UD. With this model, we first tried to fix Lm because we experimen-tally found the tendency that the model hardly outputs a stably matched solution without min-imizing Lm . Fig. 8 shows the success rate of stable matching with different Lm in the range{0.001, 0.005, 0.010, 0.050, 0.100, 0.500, 1.000}, where WN-15(n) is the model trained andvalidated with the samples of N = n. From this result, we decided to set λm = 1.0 (with the initiallearning rate of 0.0001) and use it as the maximum weight among the loss weights.
Figure 8: Sensitivity against λmλfFigure 11: Sensitivity against λbFigure 10: Sensitivity against λfmost biased distribution UD. With this model, we first tried to fix Lm because we experimen-tally found the tendency that the model hardly outputs a stably matched solution without min-imizing Lm . Fig. 8 shows the success rate of stable matching with different Lm in the range{0.001, 0.005, 0.010, 0.050, 0.100, 0.500, 1.000}, where WN-15(n) is the model trained andvalidated with the samples of N = n. From this result, we decided to set λm = 1.0 (with the initiallearning rate of 0.0001) and use it as the maximum weight among the loss weights.
Figure 11: Sensitivity against λbFigure 10: Sensitivity against λfmost biased distribution UD. With this model, we first tried to fix Lm because we experimen-tally found the tendency that the model hardly outputs a stably matched solution without min-imizing Lm . Fig. 8 shows the success rate of stable matching with different Lm in the range{0.001, 0.005, 0.010, 0.050, 0.100, 0.500, 1.000}, where WN-15(n) is the model trained andvalidated with the samples of N = n. From this result, we decided to set λm = 1.0 (with the initiallearning rate of 0.0001) and use it as the maximum weight among the loss weights.
Figure 10: Sensitivity against λfmost biased distribution UD. With this model, we first tried to fix Lm because we experimen-tally found the tendency that the model hardly outputs a stably matched solution without min-imizing Lm . Fig. 8 shows the success rate of stable matching with different Lm in the range{0.001, 0.005, 0.010, 0.050, 0.100, 0.500, 1.000}, where WN-15(n) is the model trained andvalidated with the samples of N = n. From this result, we decided to set λm = 1.0 (with the initiallearning rate of 0.0001) and use it as the maximum weight among the loss weights.
Figure 12: Success rate of stable matching (solid, ↑) and SEq (dashed, J) according to L.
