Figure 1: Sanitizer pipeline: First, We utilize publicly available (in green) datasets to train sanitizermodels. Then we learn some particular parameters from the sensitive dataset (in pink) to designsanitizing mechanisms that is applied on the sensitive dataset to obtain sanitized data that couldbe potentially sensitive or not (in yellow). We evaluate the performance by measuring utility andleakage, and then the sanitized embedding and samples are released (in blue).
Figure 2: Training scheme for the pro-posed αβ-VAE. X is the input sample, yand are the predicted and ground-truthsensitive attribute(s) respectively. andare the latent representations for sensi-tive and non-sensitive information. Thefour objectives used are detailed in Sec-tion 34Under review as a conference paper at ICLR 2022Desiderata: We want the following three properties from our sanitizer framework: P1)q(zS, zNS|x) = q(zS|x)q(zNS|x) (Independence among the sensitive and non-sensitive latent rep-resentation), P2) p(x∣z) is maximized, and P3) q(Zs) is similar to q(z). The first property wouldenable semantic decoupling, while the second property allows high utility and the third propertyensures the distribution of sensitive information is preserved. We utilize variational auto-encoders(VAEs) as our building block to attain these three properties and discuss it below.
Figure 3: Privacy-utility trade-off evaluation on different datasets: We Plot sensitive informationleakage as a proxy for privacy and one of the task attribute as a measure of utility for the sanitizeddataset. Each point in this plot corresponds to training a sanitizer model and then evaluating its per-formance by training adversary model and utility model on the sanitized dataset. Sanitizer performsbetter than all existing methods on all three datasets. We report the normalized hyper-volume intable 1 asa metric to compare the privacy-utility trade-off for different methods.
Figure 4: Qualitative demonstration of privacy-utility trade-off:We tune trade-off parameter for the sampling mode of sanitizer andbaseline techniques on the UTKFace dataset. This results in differentreconstruction quality and sensitive information leakage for the gen-erated images. The privacy axis here is measured based on adversaryperformance on the sensitive attribute (ethnicity) prediction over thetest set.
