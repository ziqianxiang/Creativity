Figure 1: Schematic illustration of stage-wise evalu-ation. We flatten intermediate feature maps from dif-ferent stages, and then use them to train stage-wiseclassifiers. Top-1 accuracy is reported by evaluatingimages in eval-D with the stage-wise classifiers.
Figure 2: Top-1 accuracy of stage-wise evaluation.
Figure 3: The difference between SL and SL-MLP. Our SL-MLP adds an MLP before the classifier comparedto SL. Only the encoders in both methods are utilized for downstream tasks.
Figure 4: (a) Visualization of different methods with 10 randomly selected classes on pre-D. Different colorsdenote different classes. Features pretrained by methods without an MLP projector (top row) have less intra-class variation than those pretrained by methods with an MLP projector (bottom row). (b) Visualization ofFeature Mixtureness between pre-D and eval-D. Cold colors denote features from 5 classes that are randomlyselected from pre-D, and warm colors denote features from 5 classes that are randomly selected from eval-D.
Figure 5: (a) Stage-wise evaluation on eval-D. (b) Linear evaluation accuracy on eval-D. (c) discriminativeratio of features on pre-D. (d): Feature Mixtureness between pre-D and eval-D. Following He et al. (2019);Grill et al. (2020), we pretrain SL, SL-MLP and Byol for 300 epochs.
Figure 6: Redundancy R of pretrained features dur-ing different epochs. During large epochs, R in-creases in SL, but decreases in SL-MLP and Byol.
Figure 7: (Left to right) (a) Top-1 accuracy with different pretraining epochs and number of MLP projectors.
Figure 8: Visualization of Feature Mixtureness With different manually generated feature distribution. Redand blue represent pre-D and eval-D class centers, respectively.
Figure 9: Evolution of intra-class variation of features in pre-D with different epochs. Different colors denotedifferent classes. The intra-class variation of SL will be very small when the pretraining epoch is large enough.
Figure 10: Evolution of Feature Mixtureness between features from pre-D and from eval-D. Coldcolors denote features from 5 classes that are randomly selected from pre-D, and warm colors denotefeatures from 5 classes that are randomly selected from eval-D. Feature Mixtureness of SL continu-ously decrease during pretraining. Alternatively, SL-MLP and Byol keeps a relatively high FeatureMixtureness at large pretraining epochs.
Figure 11:	Convolution channels visualization of Mocov1, Mocov1 w/ MLP, Byol w/o MLP, Byol,SL and SL-MLP. Following the method proposed in Olah et al. (2017), we visualize the maximumresponse of convolution channels in layer 4 of ResNet50 pretrained with different methods.
Figure 12:	Visualization of intra-class variation by different components. We randomly select 10 classesin pre-D. Different colors denote different classes. Comparing (a) wth (b), we can see the fully-connectedlayer can slightly help enlarge the intra-class variation. Comparing (a-b) and (d-e), we can observe the batchnormalization layer and the ReLU layer can significantly enlarge the intra-class variation in the feature space.
Figure 13: Visualization of Feature Mixtureness of features pretrained by different MLP components. Dif-ferent colors denote different classes. Points with cold colors denote the features from pre-D, and points withwarm colors denote the features from eval-D. Comparing (c-d) with (a-b), we can see that adding BN and ReLUcan increase Feature Mixtureness between pre-D and eval-D. Comparing (e) with (a-d), we can conclude thatBN and ReLU play the main roles in the MLP projector as (e) shows larger Feature Mixtureness. An MLPprojector with all components achieves the largest Feature Mixtureness.
Figure 14: Visualization of Feature Mixtureness between pretraining dataset (pre-D) and evaluation dataset(eval-D). Different colors denote different classes. Classes in pre-D are denoted by cold colors, and classes ineval-D are denoted by warm colors. Comparing (a,c,e) and (b,d,f), we can conclude that large semantic gapbetween pre-D and eval-D will lead to small Feature Mixtureness between pre-D and eval-D. Comparing (b)and (d-f), we can observe that the MLP projector can increase Feature Mixtureness between pre-D and eval-D,and can bridge the semantic gap between pre-D and eval-D.
Figure 15: Evaluation of features extracted by SL and SL-MLP. (a): During pretraining, features after theclassifier is used to evaluate the accuracy on pre-D. (b): After pretraining, We use the fixed backbones fromdifferent epochs to evaluate the performance of SL and SL-MLP.
