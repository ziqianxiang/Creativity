Figure 1: Training LeNet on fashion mnist with the proposed block descent approach (a) we see that the residualerror rt → 0 as training progresses for suitably chosen k. (b) we plot the generalization performance at differentβ . We see that training with the memory mechanism (m) enjoys the same accuracy while using a much smallerβ . (c) We plot the theoretical asymptotic complexity per iteration (Lemma 2) at different dimensions to highlightthe trade off and further emphasize of the importance of the memory mechanism.
Figure 2: (a, b) Feature Corruption: shows the effect of the perturbations added to image. (c, d) GradientCorruption: This Toy example in 2 dimensions visually demonstrates the superior robustness properties of GMfor robust mean estimation (e.g. estimating the aggregated gradient) in presence of heavy corruption.
Figure 3: Robustness to Feature Corruption: Test accuracy of different schemes as a function of wall clocktime for training Fashion-MNIST using LeNet (i.i.d) in presence of impulse noise. Observe that BGMD is ableto maintain high accuracy even in presence of strong corruption while attaining at least 3x speedup over GmDwhereas CMD performs sub-optimally and SGD diverges at such high level of corruption.
Figure 4: Robustness to Gradient Corruption: Training Fashion-MNIST using LeNet in i.i.d setting inpresence of scaled bit flip corruption to stochastic gradients. Similar to Figure 3, BGmD remains efficient.
Figure 5: Robustness to Label Corruption: Training Fashion-MNIST (iid) with LeNet in presence ofbackdoor attack. Even in this setting BGMD outperforms SGD, GMD, CMDDiscussion We observe that (Table 2) without corruption both BGMD and GMD are able to achievesimilar accuracy as the baseline (i.e., SGD). Conversely, CmD has significant sub-optimality gap even8Under review as a conference paper at ICLR 2022	Corruption (%)	SGD	CMD		BGmD	GmDLeNet - Fashion MNIST (homogeneous)				Clean	-	89.39±0.28	83.82±0.26	89.25±0.19	88.98±0.3		Gradient Corruption		Bit Flip	20	-	84.20±0.02	88.42±0.16	88.07±0.05	40	-	82.33±1.60	85.67±0.09	85.57±0.09	20	-	72.55±0.16	87.87±0.33	87.24±0.16Additive	40	-	41.04±1.13	88.29±0.01	83.89±0.08		Feature Corruption		Additive	20	-	82.38±0.13	86.76±0.03	86.63±0.04	40	-	78.54±0.65	82.27±0.06	81.23±0.03	20	79.18±6.47	82.59±0.60	86.91±0.36	86.23±0.03Impulse	40	-	78.03±0.73	78.03±0.73	81.41±0.12		Label Corruption		
