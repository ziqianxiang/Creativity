Figure 1: Test error as a function of layers when tree-based compression (TAO bagging and randomforest) is applied to progressively larger parts of the network. For every layer X (FC1, conv13, etc.)we report the test accuracy when we replace the X → output part of the network with a forest of sizeT . Test error of the reference network is indicated with a horizontal dashed line labeled with R.
Figure 2: Tradeoff curves when compressing the last two fully-connected layers of LeNet5 (onMNIST). For a given value ofλ we change the number of trees T and generate an entire curve whenusing bagging (top) or boosting (bottom). The test error of the reference network (along with itsFLOPs, parameters, and inf. time) is denoted with a horizontal dashed line marked with R. The bestmodels are as close as possible to the left bottom corner.
Figure 3: Left: selected comparisons to the baselines when compressing all (two) FC layers ofLeNet5. Inference speed using regular baselines reported as inf. and TreeLite-compiled versions asinf.+ Right: comparison to baselines along entire inference-error tradeoff curves. Our trees achieveof magnitudes better tradeoff than regular and highly optimized versions (using TreeLite, dashedlines) of these forests. The inference time (of the replaced part) is given with a diamond mark, ♦.
Figure 4: Similar curves as in Fig. 2 but now for the bagged ensemble of trees applied to compressthe last 7 layers of the VGG16 on the CIFAR10: we jointly compress convolutional layers 10-13and 3 fully-connected layers. Note that some boosting curves (e.g., λ = 0.01) have only a singlereported result due to achieving 0 train error, thus no further boosting is possible. These 7 layershave 9.9M parameters and require 66.5 MFLOPs.
Figure 5: Similar comparisions as in Fig. 3 but now when compressing last 7 layers of theVGG16 on the CIFAR10, While our oblique trees have larger parameter and FLOPs count wrtregular CART/RF/XGB models, our models achieve significantly better tradeof in the inference-compression design space.
Figure 6: Illustration of our neural net compression approach. Top: a standard neural network archi-tecture (the LeNet5 of (LeCun et al., 1998)), showing the portion retained and the portion replaced.
Figure 7: Top: illustration of the regularization path for λι <λ < •一< Λq. Each tree is trainedwith TAO using the same initial tree structure (a complete tree of depth ∆) but with warm-start,i.e., the tree for λi is initialized from the parameters of the tree for λi-1. As λ increases, so doesthe sparsity penalty, which encourages weight vectors at the decision nodes to become sparse andnodes to be pruned. Although it is not shown, the parameters at each tree (in the decision nodes andleaves) are different, since TAO optimizes jointly over all the parameters. Bottom: illustration ofthe procedure to construct a forest ofTi trees and sparsity hyperparameter λi, for bagging (left) andboosting (right), for λi ∈ {λ1, λ2, . . . , λQ} and Ti ∈ {1, . . . , T}. Each black circle • representsone tree. The (Ti, λi) forest consists of all the trees in the column for λ%, for rows 1一3.Hence,each column is a forest (T, λi). A horizontal arrow "→" means the next tree is initialized from theprevious one, to construct a regularization path (as in the top panel). A vertical arrow “J” means,for boosting, that the next tree t + 1 depends on all the previous trees 1, . . . , t, to construct a greedyadditive model. In bagging, the trees 1, . . . , T are independent.
Figure 8: Similar curves as in Figures 2 and 4 of the main paper, but now applied to compressthe last 3 fully-connected layers (ParamS/FLOPs: 0.53M, inference: 101 μs) of the VGG16 on theCIFAR10. A single tree (T = 1) is already powerful enough to compress all FC layers with aminimal degradation: We can achieve the test error of 6.49% using only 2827 parameters Q 187×),which requires 800 FLOPs (1 663×) and runs in 1.5 μs (1 66×) in average. Bagging (top) andboosting (bottom) the trees makes the accuracy identical to the NN’s performance and still givecompetitive operating characteristics: a bagged ensemble of T = 5 trees has the same error as thereference net (6.46%) but needs only 5816 parameters (1 91.2×), 1343 FLOPs (1 395×), and runsin3.1 μs (1 32×).
Figure 9: Selected comparisons when compressing all FC layers of the VGG16 trained on the CI-FAR10. While our oblique trees have larger parameter and FLOPs count wrt regular CART/RFmodels, in the inference-compression tradeoff our time our models are the fastest thanks to shorterdepth and extreme decision node sparsity. The TreeLite optimized inference times are given by inf.+and depicted using the dashed lines.
Figure 10: Similar curves as in Figure 8 but now for the bagged ensemble of trees applied to com-press all fully-connected layers of the VGG16 on the CIFAR100. These layers have 576K parame-ters and require 576K FLOPs. The test error of the reference network is indicated by a horizontaldashed line with a label R, and the black diamond symbol C) along this dashed line indicates thetradeoff of the reference model; if no black diamond is shown, the reference model’s operating pointis outside of the axis on the right. Speeding-up the fully-connected layers of the CIFAR100 versionof VGG16 is a harder task, however, a forest of T = 11 trees trained with λ = 0.1 can achieve aspeed-up of 14.45×. Unfortunately, such a forest requires 5× more parameters than the referencefully-connected layers on their own.
Figure 11: Compression of softmax layer of VGG16 on CIFAR10. The test error of the softmaxlayer is indicated by a horizontal dashed line with a label R, and the black diamond symbol (♦)along this dashed line indicates the operating point of the reference model.
