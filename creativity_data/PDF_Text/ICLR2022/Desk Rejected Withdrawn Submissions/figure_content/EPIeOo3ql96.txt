Figure 1: Examples from ImageNet-C datasetHendrycks & Dietterich (2019). ImageNet-C contains 75 corrupted copies of the originalImageNet-1K test set, with 5 severity levels and15 different corruption types. This dataset is usedto simulate the distribution shift between trainingdomain (original) and test domains (corrupted).
Figure 2: Method overview for Test-Time Adaptation with MixNorm layers. Before the inference,we replace all the Batch-Norm layers in the pre-trained model with the new proposed MixNormlayers. The MixNorm layers are initialized by the training set (source) statistics (means and vari-ances) and the corresponding weights and biases from the pre-trained Batch-Norm layers. Duringinference, each input is paired with another augmented view for calculation in the MixNorm layersand the final prediction is made only on the original input.
Figure 3: Illustration of the MixNorm Layer. Inside the MixNorm layer we estimate the normal-ization statistics by looking at both global statistics from historical data and local statistics fromaugmented views.
Figure 4: Comparison of MixNorm, with and without learnable affine weights (Algorithm 1), andMixNormBN (Algorithm 2) methods to the baseline method TENT in two different test time settingsi) test samples come only from a single type of corruption; ii) test samples come from mixed typesof corruptions. For the Single Type experiments, we report average error rates over all 15 corruptiontypes at severity 5. Numerical results are reported in the Appendix A.
Figure 5: Comparison of our MixNorm and MixNormBN methods to the baseline method TENT,with a different backbone architecture (WideResNet40). Numerical results are reported in the Ap-pendix A.
