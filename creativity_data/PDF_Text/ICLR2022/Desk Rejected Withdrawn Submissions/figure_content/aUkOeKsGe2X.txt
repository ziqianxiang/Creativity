Figure 1:	Impossible Instance Extractor Triplet Autoencoder (II-E-TAE) model architecture.
Figure 2:	Illustration of the different input-target pairs for the autoencoder reconstruction loss.
Figure 3: Reconstruction of unseen real data for different autoencoders: Autoencoder (AE), β Vari-ational Autoencoder (β-VAE), FactorVAE (F-VAE) and Extractor Autoencoder (E-AE).
Figure 4: t-SNE projection of the 10 dimensional latent space representation of the realistic training(blue circle) together with the real (orange cross) images. Autoencoder (AE), β Variational Autoen-coder (β-VAE), FactorVAE and Extractor Autoencoder (E-AE). The extractor approach is the onlymethod clustering both synthetic and real images together.
Figure 5: Reconstructions of unseen real data (a) from TICaM: (b) E-AE and (c) I-E-AE trainedon Kodiaq SVIRO-Illumination, (d) E-AE, (e) I-E-AE, (f) II-E-AE and (g) II-E-TAE trained on ournew dataset. A red (wrong) or green (correct) box highlights whether the classes are preserved.
Figure 6: Comparison of the training performance distribution for each epoch over 250 epochs.
Figure 7: Examples of sceneries with different backgrounds from the newly generated dataset.
Figure 8:	t-SNE projection of the 10 dimensional latent space representation of the toy training (bluecircle) together with the real (orange cross) images. Autoencoder (AE), β Variational Autoencoder(β-VAE), FactorVAE and Extractor Autoencoder (E-AE). When trained on toy images, our extractorapproach performs still best although the synthetic-real distributions are not as overlapped as iftrained on realistic images.
Figure 9:	Reconstruction of realistic and toy training data for different autoencoders: Autoencoder(AE), β Variational Autoencoder (β-VAE), FactorVAE (F-VAE) and Extractor Autoencoder (E-AE).
Figure 10: Reconstruction results of unseen real data (a) from the TICaM dataset: (b) E-AE Trainedon Tesla SVIRO, (c) E-AE Trained on Kodiaq SVIRO-Illumination , (d) I-E-AE Trained on KodiaqSVIRO-Illumination , (e) E-AE, (f) I-E-AE, (g) II-E-AE, (h) E-TAE, (i) I-E-TAE, (j) II-E-TAE and(k) Nearest neighbour of (j). Examples (e)-(k) are all trained on our new dataset. A red (wrong) orgreen (correct) box highlights whether the semantics are preserved by the reconstruction.
Figure 11: Comparison of the training performance distribution for each epoch over 250 epochs.
Figure 12: Reconstruction of real input images of digits by models trained on MNIST. Similar tothe vehicle interior, the II-PIRL loss provides the best class preserving reconstructions. The latter issupported by the quantiative results in Table 10.
