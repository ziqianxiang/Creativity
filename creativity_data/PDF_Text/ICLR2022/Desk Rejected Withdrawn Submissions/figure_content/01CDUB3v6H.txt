Figure 1: The two linearities in a GAN’s latent space. (a) A 2D illustration of the linear latentdirections corresponding to different semantic properties. (b) Yaw angles (Zhou & Gregson, 2020) ofgenerated faces as a function of the distance of their corresponding latent codes from ayaw hyperplane(Shen et al., 2020). There is an approximately linear correlation between the two, R2 = 0.92.
Figure 2: Outline of our proposed regression pipeline. An image Ii is inverted into a latent-code w¾.
Figure 3: Comparisons to hyperplane-distance baselines operating in different feature spaces. Ourmethod outperforms all baselines, indicating GAN-space distances are more semantically meaningful.
Figure 4: Comparisons to alternative models as a function of the number of labeled images used intraining. In (a) we compare to GHFeat, FSA, and SSV on the CelebA-HQ dataset. In (b) we compareto GHFeat and CORAL on the CACD dataset.
Figure 5: Sorting images according to textual descriptions of semantic properties. In each row,we sort the same set of randomly sampled CelebA-HQ images according to their distance from atext-based editing boundary extracted by StyleCLIP. Each row’s editing direction is induced by thesource text (left) and the target text (right).
Figure 6: Sorting images from the Plant-Village dataset using a semantic direction extracted byInterFaceGAN. In each row, we sort randomly sampled sets containing images labeled as eitherhealthy or sick. We separate rows by type of disease to facilitate visual comparisons.
Figure 8: Quantitative comparison of four different choices for the degree of fitted regressionfunctions. In addition to the linear model outlined in our method, we evaluate polynomial degrees of2, 3 and 5. The linear model outperforms the alternatives when only a few samples are available, andis equivalent for a thousand samples.
Figure 9: Quantitative comparisons of four different GAN Inversion encoders: e4e (Tov et al., 2021),pSp (Richardson et al., 2020) and ReStyle e4e and ReStyle (Alaluf et al., 2021) on the CelebA-HQdataset (Karras et al., 2017).
Figure 10: Comparing several approaches for calculating latent-space distances between codes inW + and boundaries in W . The ”Euclidean” model calculates the Euclidean distance between thelatents and a boundary obtained by replicating the W boundary along all layers of W+. ”All Layers”calculates a per-layer distance and uses all 18 distances as features. “Layer 2” uses only the distancecalculated on layer 2 of the latent code, which was experimentally observed to provide the bestsingle-layer results for pose. Finally, our model uses a weighted distance as outlined in Subsection 3.2of the main paper. As can be seen, our proposed method is superior to other method in the few-shotdomain and is matched by “All Layers” only when provided with a thousand labeled samples.
Figure 11: The results of our per-layer importance scores approach as outlined in Subsection 3.2,for the head pose attribute. (a) Un-normalized importance scores, before accounting for the scale ofgradients in each layer. (b) Normalized importance scores, after accounting for gradient scales.
Figure 12: Measuring the linear correlation of yaw angle with distance from hyperplane for eachlayer separately. In each subplot, the x-axis is the distance of this layer in the latent code from theboundary while the y-axis is the ground truth yaw angle. As can be seen, distance in first layers arebetter linearly correlated to head pose than last layers.
Figure 13: Pose estimation error comparisons on the Stanford Car (Krause et al., 2013) datasettagged by Pose Contrast (Xiao et al., 2021), as a function of the number of labeled images used forcalibration.
Figure 14: Ordinal regression applied to sentiment analysis using our method. Images are dividedinto bins, from discontent - 0 to most content - 4. All images were randomly sampled from fromCelebA-HQ (Karras et al., 2017). Sentiment is measured by distance from a ’smiling’ semanticboundary, identified by StyleCLIP (Patashnik et al., 2021).
Figure 15: Sorting images from AFHQ-dog (Choi et al., 2020) using a “fur fluffiness” semanticdirections extracted by SeFA (Shen & Zhou, 2020).
Figure 16: Sorting images from AFHQ-dog (Choi et al., 2020) using a “head pitch” semanticdirections extracted by SeFA (Shen & Zhou, 2020).
Figure 17: Sorting images from AFHQ-dog (Choi et al., 2020) using a “head yaw” semantic directionsextracted by SeFA (Shen & Zhou, 2020).
Figure 18: Sorting images from AFHQ-cat (Choi et al., 2020) using an “age” semantic directionsextracted by SeFA (Shen & Zhou, 2020).
Figure 19: Sorting images from AFHQ-cat (Choi et al., 2020) using a “head pitch” semantic directionsextracted by SeFA (Shen & Zhou, 2020).
Figure 20: Sorting images from AFHQ-cat (Choi et al., 2020) using a “head yaw” semantic directionsextracted by SeFA (Shen & Zhou, 2020).
Figure 21: Sorting images from Plant-Village (Hughes et al., 2015) using sick-to-healthy semanticdirections extracted by InterFaceGAN (Shen et al., 2020). To facilitate easy visual comparisons, allsick leaves have the same disease - “Early Blight”.
Figure 22: Sorting images from Plant-Village (Hughes et al., 2015) using sick-to-healthy semanticdirections extracted by InterFaceGAN (Shen et al., 2020). To facilitate easy visual comparisons, allsick leaves have the same disease - “Black Rot”.
Figure 23: Sorting images from Plant-Village (Hughes et al., 2015) using sick-to-healthy semanticdirections extracted by InterFaceGAN (Shen et al., 2020). To facilitate easy visual comparisons, allsick leaves have the same disease - “Rust”.
Figure 24: Sorting images from Plant-Village (Hughes et al., 2015) using sick-to-healthy semanticdirections extracted by InterFaceGAN (Shen et al., 2020). To facilitate easy visual comparisons, allsick leaves have the same disease - “Late Blight”.
Figure 25: All questions asked in our survey and their associated images. Page 1/6.
Figure 25: All questions asked in our survey and their associated images. Page 2/6.
Figure 25: All questions asked in our survey and their associated images. Page 3/6.
Figure 25: All questions asked in our survey and their associated images. Page 4/6.
Figure 25: All questions asked in our survey and their associated images. Page 5/6.
Figure 25: All questions asked in our survey and their associated images. Page 6/6.
Figure 26: We compare the results of our approach using different types of regularization on thesimple linear regression model. As can be observed, only slight difference exist when two calibrationpoints as used, and difference diminishes as more points are sampled.
