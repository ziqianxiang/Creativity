Figure 1: Right: Exemplary cRf -n and cRf -cRscratch curves for the SwAV self-supervised pre-training method, transferred to surface normals estimation. The relative improvement against thescratch control baseline (i.e. ∆∕cRSCratCh) can directly be read from the plot and provides a VisUal-ization for the transfer efficacy of a given method. Left: corresponding qualitative surface normalspredictions for SwAV and three control baselines: blind-guess, maximal-supervision and scratch. Inthis example, the improvement of the SSL pre-training over scratch is observed to diminishes withmore training data, which can also be inferred from the qualitative examples for 10K/100K. This(rather general) trend indicates the benefit of transfer learning in high-data regime is less significant,and therefore, relative improvement comparisons are more meaningful across different data-regimes(particularly mid and low data) where the scratch baseline performs relatively poorly.
Figure 2: By how much does contrastive self-supervised learning outperform training from scratch?We plot cRf -n and cRf -cRscratch curves for SwAV and MoCov2 across different downstreamtasks. For the three classification tasks, we use encoders pretrained on ImageNet (denoted as IN).
Figure 3: Do different tasks benefit differently from self-supervised pre-training? ThecRf -cRscratch plots above give a comparison across all pre-training methods for each downstreamtask. The images given below show corresponding visualizations of depth and normals predictionsfor different methods at two different dataset sizes. It can be observed that I. the differences amongdifferent pre-training methods are more pronounced for classification tasks compared to pixel-wiseregression tasks, and II. the best pre-training result for classification is notably better than that ofdense regression. Particularly at the high data-regime, such differences become insignificant. Thissuggests the development of pre-training methods may be more curated towards downstream classi-fication tasks.
Figure 5: Does self-supervised Learning outperform supervised pre-training? We plot thecRf -cRscratch curves for supervised and contrastive pre-training methods. ImageNet pre-trainingas a supervised method is included in the plots for all downstream tasks. For depth and normalsestimation, we also include an encoder pre-trained on the reshading task, which is the optimal supe-vised transfer domain as reported by (Zamir et al., 2018). We observe that supervised pre-trainingperforms similarly to contrastive methods in most cases, except reshading supervised pre-training ondepth estimation in low-data regimes where it outperforms self-supervised counterparts (discussionin Sec.5.4).
