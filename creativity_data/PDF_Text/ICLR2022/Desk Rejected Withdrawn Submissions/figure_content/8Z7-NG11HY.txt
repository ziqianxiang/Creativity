Figure 1: Pearson correlation between task performance and (a) word frequency (per word the numberof occurrences) and (b) similarity between train and test domains (distances between embeddingsin CBLI train and test sets). We set frequency bins k ∈ {5, 10, . . . , 100}, and set similarity bins∈ {0, 0.2, . . . ,5}. We set t to 1 in all isomorphic settings, and t to 5 as use case in non-isomorphicsettings. (c)+(d) compares generalization abilities of approaches. Results are averaged across 10 runs.
Figure 2: (left) shows how well two languages are aligned according to a visual introspection(subspace overlaps) and Precision@1; (right) compares unsupervised approaches with their supervisedvariants (Rotation and Real-NVP) in non-isomorphic settings (t = 5) as use case. We set the numberof occurrences k per word to 100. We evaluate maximum capacities of unsupervised approaches andboostrapping procedures when train and test domain distance is almost zero—no generalizationissues for non-linear approaches. To do so, we directly split data (70/30) to train and test sets.
Figure 3: Pearson correlations between intra-task results (left) and between criterion scores and taskresults from Real-NVP as use case (middle)+(right). For each approach, we run 20 epochs (leading to20 model candidates), according to which we obtain a list of results in each task and a list of criterionscores used to compute correlations. Results are averaged across language pairs and encoders.
Figure 4: Eight figures are produced by simulation. Each contains a language pair with two outlinedsubspaces, colored in blue and red. Each subspace contains 1-3 areas merely for simplicity, eachrepresenting a word—whereas our simulation results are based on 20 words. Each area (or eachword) contains a few contextual embeddings sampled from a two-dimensional Gaussian distribution.
