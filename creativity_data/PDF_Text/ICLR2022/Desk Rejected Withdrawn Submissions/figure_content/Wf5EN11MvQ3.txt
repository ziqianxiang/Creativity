Figure 1: Left: HNNs employ a hybrid architecture. The Euclidean part converts an input intoEuclidean embedding. Then the Euclidean embedding is projected onto the PoinCare model ofhyperbolic space via exponential map Exp0(âˆ™). Finally, the hyperbolic embeddings are classifiedwith Poincare hyperplanes. Right: PoinCare model can be derived using stereoscopic projection ofthe hyperboloid model. The distance grows exponentially fast as we move towards the boundary ofthe Poincare ball. We identify that the POinCare model can be partitioned into areas with unstablecomputation, vanishing gradients, larger feasible region and limited model capacity.
Figure 2: Hyperbolic neural networks suffer from vanishing gradient problem during training withbackpropagation. Left: The trajectories of the hyperbolic embeddings of six randomly sampledinputs during training in a 2-dimensional Poincare ball. The arrows indicate the change of locationof each embedding with each gradient update. The embeddings move to the boundary of the ballduring optimization which causes vanishing gradient problem. Right: The gradient vanishes whilethe training loss goes up at the end of training.
Figure 3:	HNNs with feature clipping learn more discriminative feature in hyperbolic space. The Perclass accuracy in the center figure indicates that the baseline HNNs learn biased feature space whichhurts the performance of certain classes. Left: the PoinCare decision hyperplanes and the hyperbolicembeddings of sampled test images of baseline HNNs. Center: The per class test accuracy ofbaseline HNNs and HNNs with feature clipping. Right: the PoinCare decision hyperplanes and thehyperbolic embeddings of sampled test images of HNNs with feature clipping.
Figure 4:	Adversarial robustness of hyperbolic neural networks (HNNs) and Euclidean neuralnetworks (ENNs) to different attack methods and perturbations.
Figure 5:	We show the change of the test accuracy as we vary the hyperparameter r. A large r leadsto vanishing gradient problem and a small r causes insufficient capacity. Both lead to a drop in testaccuracy.
Figure 6:	Hyperbolic neural networks show more adversarial robustness compared with Euclideanneural networks. We show the clean image and the corresponding adversarial image and the predic-tions of the network of 10 randomly sampled images. In several cases, hyperbolic neural networksmake correct predictions on the adversarial images while Euclidean neural networks make wrongpredictions.
Figure 7: We show the change of the test accuracy as we vary the temperature parameter T . Thered horizontal line is the result of the hyperbolic neural networks with the proposed feature clipping.
