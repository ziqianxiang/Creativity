Figure 1: While unsupervised clustering methods infer classes by visual features only, this may notbe sufficient to identify ground truth classes. Our method guides the desired grouping by language.
Figure 2: Representative images of the different datasets, from left to right: Stanford Activity, BU-action, All-Age-Faces, PPMITable 1: Clustering of Attributes Classification Datasets (%)	Stanford Activity			BU-action			All-Age-Faces			PPMI			ACC	NMI	ARI	ACC	NMI	ARI	ACC	NMI	ARI	ACC	NMI	ARIPT Only	61.4	66.4	49.0	61.0	77.4	51.9	47.5	28.6	19.7	34.2	26.5	15.8PT+SCAN	54.0	66.0	45.3	63.1	78.8	56.9	48.8	33.4	20.2	27.5	24.1	12.3ZS-Naive	66.0	74.5	55.3	52.5	72.9	44.3	50.6	38.7	24.2	35.6	29.6	16.6Ours	72.8	76.3	64.5	65.9	79.5	57.8	58.5	38.4	26.6	39.4	33.7	21.9ZS-GT	82.8	80.1	72.0	77.0	83.6	67.2	60.7	40.8	30.3	54.5	44.5	33.5PT Only: Classical Ward’s clustering based on CLIP’s visual features but without the languagepriors.
Figure 3: Left: Accuracy (%) of the naive zero-shot classification using the entire phrase listcompared to our method. The dashed line notes the zero-shot accuracy using the ”ground truth”phrases (the given class names). Right: While the ground truth labels in datasets such as ”PeoplePlaying Musical Instrument” are based on identification of objects, these objects are often far frombeing the most salient item in the image.
