Figure 1: Example unseen input digits and outputs from our method. Top row is the input, middlerow is the predictions and bottom row is the original images. Video is also available in supplemen-tary materials.
Figure 2: Our TIM model consists of multiple modules, each of which operates on 2× the resolutionof previous one.
Figure 3: Comparison of sample efficiency of hierarchical sampling (HS) to vanilla sampling (whichsamples latent codes for different modules independently). The relative disparity of the requirednumber of samples needed to achieve the same LPIPS distance to the observed output with/withoutHS is shown, where the number of required samples for HS is normalized to 1. The reported resultsare averaged over 10 independent runs. As shown, as the number of samples used per moduleincreases in the case of HS, more samples are needed by vanilla sampling to match the distanceattained by HS.
Figure 4: Details of the architecture backbone.
Figure 5: (a) Inner workings of Residual-in-Residual Dense Blocks (RRDBs), which comprises ofdense blocks (details in (b)). β is the residual scaling parameter. (b) Inner workings of dense blocks.
Figure 6: Visualization of different samples generated by our method (TIM) and the input for super-resolution. As shown, TIM generates different high quality textures for example on the edge of thebutterfly’s wing.
Figure 7: Visualization of different samples generated by our method (TIM) and the input for imagecolourization. As shown in the figure, in addition to common colours, TIM also produces a variety ofcolours, such as green bananas and plums. Similarly, generating parrots with different body coloursalso shows the power of TIM in terms of multimodality.
Figure 8: Visualization of different samples generated by our method (TIM) and the input for imagedecompression. As shown, TIM output successfully removes most artifacts and predicts diversetextures.
Figure 9: Visualization comparison of two tasks: 16× super-resolution (first row) and image decom-pression (second row) as we gradually remove (1) hierarchical sampling (HS), (2) mapping network(MN), (3) intermediate supervision (IS), (4) weight normalization (WN).
