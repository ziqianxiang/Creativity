Figure 1: Generating adversarial examples in this workinvolves inserting an adversarial patch (left) into a speci-fied region of a T-shirt. Shown here are two examplesproduced by Expectation of Transformation(EoT) (Atha-lye et al., 2018) and our approach PaTNet, respectively.
Figure 2: An overview of our approach. The networktakes as the input a person image (a), a cropped patch(b) from (a) and a binary mask of the patch in (b) (notdrawn here), transforming a digital patch (c) to replace(b) on the T-shirt. The patch (c) goes through threefundamental physical transformations, i.e. geometrictransformation (d), printer color transformation (e) andillumination adaption to ensure realism on (f). (ER, EG,EB ) is an illuminant inferred from a color constancymodel to adjust the brightness of (e).
Figure 3: Simulated results of each transformation (STN, PCT and IA) under two different lighting conditions.
Figure 4: Learning STN using an image generator. Thenetwork transforms a patch (b) to visually match its physi-cal realization (a) on the T-shirt (training data). The Imagegenerator learns the difference between (c) and (d).
Figure 5: Data used for training and validating PaTNet.
Figure 6: ASR (%) of PatNet-s and PatNet under different real-world scenarios.
Figure 7: Detection scores w/o adversarial attack(orange), ASRs (green) and detection scores underattack (blue) w.r.t. patch size.
Figure A1: The overall and detection losses v.s. epoch for advP (Thys et al., 2019) and advPat. Our proposedapproach converges much faster than advP based on EoT.
Figure A3: Examples of adversarial attacks in the physical world. Successful cases: images 1-4 from left toright; failure cases: images 5-7. L or R indicates the location of an adversarial T-shirt generated from a baselineapproach, and the other one is our approach PatNet.
