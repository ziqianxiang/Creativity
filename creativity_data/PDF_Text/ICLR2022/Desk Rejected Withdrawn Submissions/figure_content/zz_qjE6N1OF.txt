Figure 1: Components of the P4O architecture. A sensory input S is encoded into a low-dimensionallatent representation x. This encoded game state is subtracted from a prediction P generated by anLSTM layer. The resulting prediction error is passed into both the prediction layer and a secondLSTM layer representing the agent,s belief states. The LSTM outputs are used by an actor-criticmodel to select an action a and compute a corresponding state value v. The red box highlights themain architectural contribution of P4O. Minimization of the prediction error is added to the P4Oagentâ€™s objective function.
Figure 2: Comparison of P4O algorithm against the baselines LSTM-PPO (k = 1024), LSTM-PPO(k = 800) and the P4O model optimized without predictive processing loss (P4O without PP loss).
Figure 3: Comparison of individual performance curves. P4O results are shown in the top row andLSTM-PPO baseline (k = 1024) results are shown in the bottom row.
Figure 4: Comparison of activation distributions between the LSTM-PPO (k=1024) baseline latentencoding, the P4O latent encoding, the P4O prediction and the P4O prediction error. The distribu-tions are drawn from five states aggregated from trained agents.
Figure 5: Single seed performance comparison on Seaquest with IQN, Rainbow and DreamerV2after 10 days of accelerator time. Circles represent final reported scores of the IQN, Rainbow andDreamerV2 agents after 10 days. The moment where P4O exceeds their respective endpoint scoreis marked with triangles. Reported average score is a rolling mean of the last 100 episodes.
Figure 6: Encoder architecture, following a similar structure to the architecture used by (Espeholtet al., 2018) with a few modifications. The encoder uses a total of 20 convolutional layers and 3.3Mparameters.
