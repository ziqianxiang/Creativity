Figure 1: Neural friendly extensions: f (x) is not defined for non-integral points x. To assign a function value tonon-integral X We first reinterpret X as an expectation over integral points 1s in the domain Ω of f. Then, in place ofthe ill-defined quantity f (x) = f (ES〜px[1s]) we obtain a well-defined value F(X) .= ES〜px[f (S)] by exchangingthe order of operation of f and the expectation so that f is only evaluated at points S in its domain Ω.
Figure 2: Learning to find k-cliques: higher F1-score is better. The k-subset extension, which defines F as a convexcombination of sets of size k, is better aligned with the task and significantly improves over the LoVaSz extension.
Figure 3: Extending supervised training error: We treat the training error as a non-differentiable loss function, anddirectly minimize the via the singleton set extension defined in Section 3.3.
Figure 4: CIFAR10: Unlike common losses that indirectly optimize training error (e.g., cross-entropy), the singletonextension loss closely approximates the exact (non-differentiable) training error at the same numerical scale.
Figure 5:	Score function estimator reward over 100 epochs on maximum clique.
Figure 6:	LovaSz extension loss over 100 epochs.
