Figure 1: Illustration of RL as sequence modeling (using Transformer). (a) A straightforward ap-proach. (b) Our improvement. The concept is local features helps the long-term modeling. Redarrows stand for action is “autoregressively generated” by state. Green arrow stands for this localrepresentation contributes the action generation together with state representation.
Figure 2: Architecture of the proposed StARformer model (with an image-based state representa-tion). (a) Network overview, which shows how, Step Transformer and Sequence Transformer areconnected at each layer. (b) Detailed functionality of each component. Step Transformer processesT separate groups of tokens simultaneously with shared weights. We learn StAR-representation gtlby aggregating output tokens from Step Transformer and use it in Sequence Transformer. We onlykeep tokens with even indices and discard the others in Sequence Transformer’s output since wegenerate actions from states. (c) A trajectory is segmented into local groups of (at-1, rt-1, st) andembeded as tokens. (d) States are separately embedded using convolutions with shared weights.
Figure 3: Performance under different trajectory lengths. T ∈ {10, 20, 30} in Atari andT ∈{5,10,15,20} in Gym. In most of the cases, StARformer shows a better performance whenincreasing the trajectory length, and surpasses that of the baseline, validating that our method caneffectively model long-term sequences.
Figure 4: Illustration for our ablation study Transformer connection. We omit positional embeddingsfor simplicity. gl (in green) is the StAR-representation from l-th layer of the Step Transformer andh0 (in blue) is the initial integrated state token embeddings for Sequence Transformer. L is thenumber of layers.
