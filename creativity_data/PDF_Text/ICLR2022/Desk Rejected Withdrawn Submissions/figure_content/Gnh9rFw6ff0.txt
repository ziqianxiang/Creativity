Figure 1: Information diagrams of different representations in contrastive learning. We consider thesituation where the non-shared task-relevant information I(v1, T |v2) cannot be ignored. Contrastivelearning makes the representations extracting the shared information between views to obtain thesufficient representation which tends to be minimal. The minimal sufficient representation containsless task-relevant information from the input than other sufficient representations.
Figure 2: Demonstration of our motivation using information diagrams. Based on the sufficientrepresentation learned by the contrastive learning models, increasing I(z1, v1) approximately intro-duces more non-shared task-relevant information.
Figure 3:	The classification accuracy on the source dataset (CIFAR10 or STL-10) and the averagedaccuracy on six transfer datasets with varying hyper-parameter Î».
Figure 4:	The classification accuracy on the source dataset (CIFAR10 or STL-10) and the averagedaccuracy on six transfer datasets with varying epochs.
Figure 5: Demonstration of the effect of our reconstruction module. We provide the original imagesand the reconstructed images for comparison. We use SimCLR contrastive loss and take CIFAR10as the training dataset.
