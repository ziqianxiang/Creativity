Figure 1: We conduct the experiments on the CIFAR-10 dataset with 40% symmetric label noiseusing ResNet34 (He et al., 2016). The top row shows the fraction of samples with clean labelsthat are predicted correctly (purple) and incorrectly (black). In contrast, the bottom row showsthe fraction of samples with false labels that are predicted correctly (purple), memorized (i.e. theprediction equals the false label, shown in blue), and incorrectly predicted as neither the true nor thelabeled class (black). For samples with clean labels, all three models predict them correctly with theincreasing of epochs. However, for false labels in (a), the model trained with cross-entropy loss firstpredicts the true labels correctly, but eventually memorizes the false labels. With the cosine annealinglearning rate scheduler (Loshchilov & Hutter, 2017) in (b), the model only slows down the speed ofmemorizing the false labels. However, our approach shown in (c) effectively prevents memorization,allowing the model to continue learning the correctly-labeled samples to attain high accuracy onsamples with both clean and false labels.
Figure 2: In (a), we introduce an indicator branch in addition to the prediction branch. Given an inputimage x[i], the indicator branch produces a single scalar value τ[i] to indicate ‘confidence’ and theprediction branch produces the softmax prediction probability p[i] . (b) and (c) show the distributionof confidence τ on the CIFAR-10 and CIFAR-100 with 40% symmetric label noise respectively.
Figure 3: On CIFAR-10 with 40% symmetric label noise using ResNet34, we observe that in (a), thegradient of clean labels dominates in early learning stage, but afterwards it vanishes and the gradientof false labels dominates. In (b), it only slows down this effect with cosine annealing learning ratescheduler. In (c), CAL effectively keeps the gradient of clean labels dominant and diminishes thegradient of false labels when epoch increases, preventing memorization of mislabeled samples.
Figure 5: Confusion matrix of corrected la-bels w.r.t clean labels on CIFAR-10 with 40%symmetric label noise.
Figure 4: Average confidence values τ offalse labels w.r.t clean labels on CIFAR-10with 40% symmetric label noise.
Figure 6: Test accuracy on CIFAR-10 with 60% symmetric label noise. The mean accuracy overthree runs is reported, along with bars representing one standard deviation from the mean. In eachexperiment, the rest of hyperparameters are fixed to the values reported in Section C.4.
Figure 7: Test accuracy on CIFAR-100 with 60% symmetric label noise. The mean accuracy overthree runs is reported, along with bars representing one standard deviation from the mean. In eachexperiment, the rest of hyperparameters are fixed to the values reported in Section C.4.
Figure 8: Label correction of Webvision images. Given noisy labels are shown above in red and thecorrected labels are shown below in green.
Figure 9: Label correction of Clothing1M images. Given noisy labels are shown above in red and thecorrected labels are shown below in green.
Figure 10: Confusion matrix of corrected labels w.r.t clean labels on CIFAR-10 with 60% symmetric,80% symmetric and 40% asymmetric label noise respectively.
Figure 11: Average confidence values τ offalse labels w.r.t clean labels on CIFAR-10with 60% symmetric label noise.
Figure 12: Average confidence values τ offalse labels w.r.t clean labels on CIFAR-10with 80% symmetric label noise.
Figure 13: The empirical density of confidence value τ on CIFAR-100 with 40% symmetric labelnoise. The mean confidence values of mislabeled samples become smaller with the increasing of β.
Figure 14: Label correction accuracy vs.
Figure 15: Label correction accuracy vs.
