Figure 1: Peak memory (y-axis), average performance(x-axis) and speed (denoted by area of circle) for vari-ous efficient Transformer models (i.e. bigger circles inthe bottom right corner are better) across four task in-troduced in LRA (Tay et al., 2021b). All values exceptfor “This Paper” are taken from Tay et al. (2021b).
Figure 2: Generalized self-attention with deep generative RKS. Q, V and K are linear transforma-tions of input, X. The generator generates spectral frequency (Ω) from an implicit spectral distri-bution. Using RKS (Eq. 2) we create the feature map φgen (Eq. 7). The numerator of the outputis calculated as φgen(Q)(φgen(K)V ) while the denominator is φgen (qi)T PjL0=1 φgen(kj0) makingattention linear in sequence length L.
Figure 3: We compare the peak memory consumption (y-axis), performance (x-axis) and speed(denoted by area of circle) for the various Kernelized Transformer architectures on the twoLRA tasks with sequence length equal to 4K . Memory usage refers to per device memory usageacross each GPU, whereas speed (steps per second) for each model is reported relative to the speedof Softmax Transformer (larger circles denote faster models).
Figure 4: Memory consumption vs sequence lengthrecommend FastFood-PRF and Generative-with the least memory consumption.
Figure 5: Peak memory used by Kernelized Transformers across different datasets.
Figure 6: We demonstrate the peak memory consumption (y-axis) and performance (x-axis) of thevarious Kernelized Transformer architectures on the ListOps and Image dataset from LRA. Memoryusage refers to per device memory usage across each GPU.
