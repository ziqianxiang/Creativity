Figure 1: Mean posterior entropy of the predictions after each acquisition on EMNIST.
Figure 2: Performance on MNIST and repeated-MNIST. Accuracy and NLL after each acquisition.
Figure 3: Performance on EMNIST and fashion-MNIST, ICAL significantly improves the accuracyand NLL.
Figure 4: (Above)Number of times multiple copies of a replica sample is chosen in repeated-MNIST.
Figure 5: Performance on CIFAR-10 and CIFAR-100 with batch size=3000. Averaged accuracyacross 8 seeds (CIFAR-10) and 5 seeds (CIFAR-100) is shown.
Figure 7: Relative performance of ICAL and ICAL-pointwise on CIFAR100 with different mini-batchsize L. iter “ L is the number of iterations taken to build the entire acquisition batch of size B(note that the actual acquisition happens after the entire batch has been built)19Under review as a conference paper at ICLR 2022Diversity of acquired samples in repeated-MNISTTo check if ICAL’s acquisition batches are diversed enough, we plot the number of times differentnumber of copies of a same sample has been acquired by each method. As shown in figure 8,our method (as well as BatchBALD, BayesCoreset and Random) successfully avoided acquiringredundant copies of the same sample, whereas FASS and Max Entropy acquired up to 3 copies of thesame replica in most acquisitions. This proves that the batched active learning strategies are better indiversity.
Figure 8: Frequencies where different numbers of copies (1-3) of a same sample has been acquiredby each method.
Figure 9: CIFAR10 performance with different L. iter “ B is the number of iterations taken to buildthe entire acquisition batch of size B (note that the actual acquisition happens after the entire batchhas been built)iter(B∕L)Figure 10: Runtime of ICAL on CIFAR10 with different minibatch size L.
Figure 10: Runtime of ICAL on CIFAR10 with different minibatch size L.
