Figure 1: The threat value on the ordinate, and the threat of the opponent’s tentankltank2tank3tank4tank5tank6tank7tank8tank9tanklθat time Trepresented by ten colours on the abscissa.
Figure 2: A fusion model of reinforcement learning and multi-attribute threat estimation based onAC framework. The module mainly consists of a reinforcement learning pre-training module thatintegrates multi-attribute decision-making, Critic evaluation network update module, and a new andold strategy network module5Under review as a conference paper at ICLR 2022The critic network calculates the value from the reward value determined during the last step of theaction. combines the experience store data with the value calculated by the critic network, slashesit from the reward value determined during the last action, then returns to update the critic networkparameters. As the advantage value guides the calculation of the actor network value, the networkoutputs the action value according to the old and new networks, and the distribution probabilityoverall, and outputs the action from the network. As a result, the advantage value is corrected, theactor loss is calculated, and the actor network is updated in the reverse direction.
Figure 3: Gaming environment display. The red and blue pawns fight separately, the red flag in themiddle is the control point, and the first player to reach the control point wins. Alternatively, whenall the wargame agents on one side are destroyed, the opponent wins.
Figure 4: (a) Win rate: the red side is the AI of MADM-PPO intelligent algorithm and the blue sideis rule-based AI; (b) Win times: the red side is the AI of MADM-PPO intelligent algorithm and theblue side is rule-based AI; The winning rate and the number of wins for the red and blue sides. Thefirst round wins so one side starts from 1 and the other from 0.
Figure 5: (a) Win rate: the red side is the AI of PPO intelligent algorithm and the blue side is rule-based AI; (b) Win times: the red side is the AI of PPO intelligent algorithm and the blue side isrule-based AI; The winning rate and the number of wins for the red and blue sides. The first roundwins so one side starts from 1 and the other from 0.
Figure 6: (a) The get goal score of both sides (Red: PPO); (b) the kill score of both sides (Red:PPO); (c) the survive score of both sides (Red: PPO); (d) the get goal score of both sides (Red:MADM-PPO); (e) the kill score of both sides (Red: MADM-PPO); (f) the survive score of bothsides (Red: MADM-PPO). The x-axis is the training episodes, and the y-axis is the score. Red andblue represent two teams in the wargame environment.
