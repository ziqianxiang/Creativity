Figure 1: Left : Weight scalar distribution in layer 4.0 conv2 of an ImageNet pretrained Resnet50.
Figure 2: Left : Correlation diagram of filters in layer 4.0 conv2 (Conv2d 3 × 3). Right : Uniform(= square) quantization and lattice (= parallelogram) quantization4.1	Lattice based weight quantizerIn order to quantize the weights, LatticeQ uses lattices, which are algebraic structures that discretizethe notion of vector space (see Appendix A for more details). Each lattice has a basis, meaningthat each point of the lattice can be written as an integer linear combination of the vectors of thisbasis. This integer linear combination is the encoding of our quantization. A lattice has infinitecardinality. In order to use this structure as our quantizer we need a finite number of quantizationpoints. We truncate our lattice in the following fashion : let Λ be a lattice, and B = (bi)1≤i≤n ∈ Rna basis of Λ. Given b the bitwidth, our quantization set is Q = {q ∈ Rn, q = P2ι μibi, ∀i ∈{1,…,n}, -2b-1 ≤ μi ≤ 2b-1- 1}.	一In memory, each quantized block is represented by its coordinates in the quantization basis. Let ussuppose B = (b1,b2,b3) = ((b1,1, b1,2, b1,3), (b2,1, b2,2, b2,3), (b3,1,b3,2,b3,3)) is our quantizationbasis (with each bi,j a scalar, possibly quantized with a uniform min/max quantizer), and:f3f6f9f2f5f8f1f4f7z∕(∖=q
Figure 3: Resnet18 per-layer quantization errorcomparison between LatticeQ and Cubic LatticeQ(scalar quantization). Vertical axis is MCE.
Figure 4: Left : Cubic LatticeQ quantization points (red) and 1×3 filter blocks (blue), Right :LatticeQ quantization points (red) and 1×3 filter blocks (blue). These are the 2-bit quantizationpoints for visualization.
