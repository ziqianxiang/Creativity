Figure 1: Examples of text-guided image manipulation using DiffusionCLIP.
Figure 2: Overview of DiffusionCLIP. The input image is first converted to the latent via diffusion.
Figure 3: Fine-tuning the general diffusion models with the shared architecture.
Figure 4: Novel applications of DiffusionCLIP.
Figure 5: Comparison of the manipulation methods for a variety of text prompt. We use DDPMmodels pretrained on CelebA-HQ 256 × 256 for DiffusonCLIP. We follow the official codes ofStyleCLIP and StyleGAN-NADA that use StyleGAN-ADA pretrained on FFHQ 1024 × 1024.
Figure 6: More results of manipulation using DiffusionCLIP where the original pretrained models aretrained on CelebA-HQ, AFHQ-Dog, LSUN-Bedroom and LSUN-Church, respectively. The resultsdemonstrate various text-guided manipulation over the trained domains.
Figure 7: Examples of the novel applications of fine-tuned models using DiffusionCLIP.
Figure 8: Comparison with respect to various hyperparameters.
Figure 9: Comparison between fine-tuning and latent optimization.
