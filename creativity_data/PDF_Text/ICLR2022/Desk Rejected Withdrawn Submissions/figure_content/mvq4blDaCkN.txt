Figure 1: The challenge of generating perturbations for unlabeled data in the l∞ norm.
Figure 2: Conceptual illustration of perturbations in the linear and kernel spaces. (Here solid circlesdenote positive samples, solid crosses denote negative samples, hollow squares denote unlabeledsamples, the red circle and cross are adversarial examples of positive and negative samples respec-tively. Note that one unlabeled sample has two adversarial examples.)Theorem 1. (Xu et al., 2009) Suppose the kernel function has the form k(x, x0) = f(kx - x0k),with f : R+ → R, a decreasing function. Denote by H the RKHS space of k(∙, ∙) and φ(∙) thecorresponding feature mapping. Then we have for any x ∈ Rn, w ∈ H and >0,sup hw, φ(x + δ))H ≤	sup_________hw, φ(x) + δφiH.	(8)kδk2≤e	kδφkH≤√2f(0)-2f(e)Since the perturbation range of φ(x) + δφ tightly covers that of φ(x + δ), which is also intuitivelyshown in Fig. 2d, we apply φ(x) + δφ to deal with the following computation, thus the perturbationscan be more tractable in the kernel space. Then the objective function (7) can be rewritten as1	1	Np Nnmin 2 kf kH + βNN XX. ,max “J1 -hf, φ(xP)iH + hf, φ(xn)iH] +f∈H 2	NpNn i=1 j=1 Φ(xip ),Φ(xjn)2The kernel perspective means that our function f is in the reproducing kernel Hilbert space (RKHS) (Iii,2004)5Under review as a conference paper at ICLR 2022i]	Np Nu
Figure 3:	The running time of algorithms when training different sizes of samples. (The linesof SAMULT and PNU-AUC are incomplete because their implementations crash on large trainingsets.)SZAT-AUC(M)600.511.522.5→-PNU-AUC-v-QSG-B2AUC-♦-S!IAT-AUC(S)于5859590(a) CIFAR dog vs. horse(e) CIFAR dog vs. horse (f) automobile vs. truck (g) MNIST8m 0 vs. 4	(h) MNIST8m 6 vs. 8Figure 4:	Sensitivity analysis of the maximum perturbation (Figs. 4a- 4d) and imbalanced ratio
Figure 4:	Sensitivity analysis of the maximum perturbation (Figs. 4a- 4d) and imbalanced ratioNn/Np (Figs. 4e-4h).
