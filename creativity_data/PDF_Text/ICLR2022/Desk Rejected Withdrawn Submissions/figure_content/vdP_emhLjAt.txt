Figure 1: Shattering a neural network trained on the halfmoons dataset into co-dependent linearsubfunctions to obtain a heatmap of unreliability across the input space. The model is a MLP withtwo hidden ReLU layers of 32 units each. (a) Training data. (b) Traditional model-level error bound:shown here is the single model bound from Mohri et al. (2018, eq. 2.17). (c) Linear activationregions. (d) Ours: subfunction error bound as unreliability. (e) Unreliability heatmap in 2D.
Figure 2: Each linear activation region in 2Dinput space (plane) is mapped to a uniquesubfunction, activation decision pattern, andset of training samples (triangles). A smoothdensity is defined given the number of sam-ples in each region.
Figure 3: Given a family of Gaussianweighting functions, the best weightingfunction according to eq. (15) trades offAlthough p(h)can be very large in general, note firstlythat it is tempered by a factor of √N, and secondlythat if one were to optimize the weighting function kby minimizing the distance between true error and STEB(eq. (11)), this necessarily involves finding a function thatyields high densities for activation regions, otherwise thedistance to true error would be large. Now we discuss theproblem of selecting such a weighting function.
Figure 4: Input regionsfor a balanced k-d tree on2D space. Color denotesunique decision set.
Figure 5: Sample confusion matrix, OOD forCIFAR10 → CIFAR100 on ResNet50. Randomsamples from top 20% in each quadrant shown.
Figure 6: OOD for CIFAR10 → CIFAR100 on ResNet50. 10k CIFAR100 test samples were rankedby unreliability (log STEB). Boxplots summarize rankings per class (lower = less unreliable). Greendenotes superclasses similar to CIFAR10: carnivores, omnivores, herbivores, mammals, vehicles.
Figure 7: Sphere of pj that are b bitsaway from pι . For illustration only.
Figure 8: Entropy.
Figure 9: Max response.
Figure 10: Margin.
Figure 11: Class distance.
Figure 15: Residual flows density.
