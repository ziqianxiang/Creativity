Figure 1: The framework of the proposed MTA whenT = 1 and A(Me(x)) = Xadv. The clean image Xis first feed into the MSM Me and obtain the lossL(Me(χ),y). Next we back-propagate the loss anduse Eq 4 to obtain the noise g] . Then, via Eq 5, weobtain the adversarial example Xadv which will be feedinto the source models Fι, F2, ..., and FN. Finally,by maximizing the source models, loss, we can opti-mize the MSM to learn a particular weight so that theadversarial example X1adv attacking it can fool sourcemodels.
Figure 2: (a) The structures of ResNet-13 and -19. ResNet-13 contains the top four blocks inthe solid-line box and the classifier. ResNet-19 contains all the six blocks and the classifier. Theparameter M* of each block denotes the number of filters of its convolution layers. (b) The detailedstructure of residual block. The orange cube is the convolution layer and the number on it denotesits number of filters. Pool in the sixth block is global-average pooling while all the other pool ismax-pooling with both stride and kernel size of 2×2. The convolution layer in the shortcut path uses1×1 kernel size while all the other convolution layers use 3×3.
Figure 3: Transfer attack performances of MTA on the eighttarget models of Cifar-10. Left: Attack success rates withdifferent Tt . Right: Attack success rates with different Tv . y-axis denotes the attack success rate.
Figure 4: (a) DenSeNet-22-BC. Orange cube is convolution layer with 3×3 kernel size. Pink cube isconvolution layer with 1× 1 kernel size. ‘Bottle Neck (M2) *3, denotes three cascaded ‘Bottle Neck(M2)’. The number (e.g., Mi, 4 * M, M) on each convolution layer denotes its number of filters.
Figure 5:	The simplified Inception network. All the blocks have the same inner structures with thoseofInception-ResNet-V2.
Figure 6:	Transfer attack success rates of MTA on the eight black-box Cifar-10 models, across thetraining process.
Figure 7: The adversarial examples and the noises generated Via ML DL TL SGM, IR, and MTA.
