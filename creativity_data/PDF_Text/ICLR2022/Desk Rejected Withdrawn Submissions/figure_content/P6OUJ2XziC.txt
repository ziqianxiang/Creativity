Figure 1: Response-preparation task in a reinforcement learning setting. A rat acts in a behavioralchamber with a lever and a sugar port. Our proposed framework first infers an intrinsic scalar rewardfunction of the rat’s behavior via closed-form inverse reinforcement learning. Then, a parameterizedfunction ρ is learned which maps neural signals to the intrinsic reward. Finally, we generalize to newsituations by applying ρ to other neural signals and calculating the Q-values and Boltzmann policyto study the corresponding simulated behavior for these neural signals.
Figure 2: Successful trial of the response-preparation task in a behavioral chamber. The rat pressesa lever until the vibration cue occurs, releases within 0.6 s and gets to the reward port.
Figure 3: Transition graph for the MDP of the described response-preparation task. In the initialstate, the rat presses the lever. If the rat does not release, it ends up in the next time step, where thetime is discretized with 0.2 s steps. If the rat relases after the cue in a time span of 0.6 s, the trial wasa success and it gets rewarded. c denotes the running index over time steps before and after the cue(in our case c = 8, representing 1.6 s with 0.2 s steps).
Figure 4: Release distribution, learned reward and the resulting Boltzmann distribution after apply-ing Q-learning on the reward for (top) rat 1 and (bottom) rat 2 over all trials. Dashed lines indicatethe time span in which the rats ought to release.
Figure 5: (left) Visualization of latent embeddings for the two actions stay (◦) and release (×)generated from the last hidden layer of the classifier (NNC). (right) Visualization of the normalizedcumulative latent embeddings generated by the reward-model (as substitute for a Q-value). Ourmodel preserves the temporal coherence of the task for the two actions stay (◦) and release (×)much better than the classifier on the left which is necessary for correct release prediction.
Figure 6: A Delineation of the Rostral (RFA) and Caudal (CFA) Forelimb Areas in a rat's brainaccording to Neafsey & Sievert (1982) and Rouiller et al. (1993). ∣B Z-SCore normalized firing ratesof neurons projecting from RFA to CFA.
Figure 7: Release distribution of the rats (Control) and the resulting Boltzmann policies after ap-plying Q-learning on the modified reward for different levels of simulated inhibition of neuronsprojecting from RFA to CFA. Dashed lines indicate the time span in which the rats ought to release.
Figure 8: A Identification of relevant neurons via optogenetic phototagging. 圜 Processing ofneural recordings and extraction of neural spiking. C Viral manipulation of the pathway projectingfrom RFA to CFA. D Mean reaction times and standard error for (left) rat batch 1 without andwith simulated inhibition of 60% of RFA to CFA neurons and (right) rat batch 2 with and withoutreal inhibition. (top) Within NeuRL (red) and in the real-world experiments (blue), the reaction timeincreases with inhibition in the response window. (bottom) There is no significant change in reactiontime with inhibition during the hold period. Baselines are depicted in black for comparison, (- -) forlinear regression and ( ) for NNC.
