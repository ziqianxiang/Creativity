Figure 1: Framework for joint dynamics grounding and policy learning. The parameters φ of theresidual ∆φ are trained to account for the distance between f and f *. The policy is then tuned on theupdated dynamics in a self-supervised fashion.
Figure 2: Evolution of the dynamics mismatch cost δs,ν,π and the policy performance J* (πθ) infunction of samples collected with the target dynamics. Three policies are used for data collection: arandom controller, and an MBMF controller, and the policy πθ, which is jointly trained. Coupling thecontrol and dynamic learning process favours sample efficiency.
Figure 3: CartPole subject to a time-variant force.
Figure 4: Experiments are conducted on three diverse dynamic systems and tasks: Tracking atrajectory with a quadrotor, passing through a target with a fixed wing drone, and balancing a pole ona cart. The OpenAI Gym renderer was used for simple 2D visualizations.
Figure 5: Iterative fine-tuning of model and policy. Here, a residual network ∆ is trained to accountfor velocity drag in the target dynamics f * with low sample budget. The policy ∏ is fine-tuned (blueparts) by backpropagation through the updated dynamics model. When converged, the new policy πis used to collect new samples in f* to further decrease the dynamic gap ∆ (red parts).
Figure 6: Evolution of the dynamics mismatch cost δs,ν and the policy performance for trackinga target point with a fixed-wing aircraft. Data in the target domain f* is collected with either arandom policy, a MBMF or our pre-trained policy. The error decreases faster when our joint learningapproach is used, confirming the result in subsubsection 3.2.2A.3 Model-free and model-based baselinesThe online-optimization model predictive control in the MBMF baseline is implemented with thetoolbox CasADi for numerical optimization, using its nonlinear programming solver. The samereference trajectories and loss functions as for our neural controller are used.
Figure 7: Convergence of model-free PPo baseline, when fine-tuned in the target domain f * on afixed-wing aircraft. in contrast to our approach, model-free RL requires a much higher sample budgetto achieve similar performance in f * as in f.
Figure 8: Learning to account for velocity drag of a quadrotor. The convergence of the mean squarederror betWeen learnt dynamics and target dynamics during training is shoWn. With only 200 samples,the error converges close to zero in 50 epochs. With 1000 samples, 10 epochs suffice to ground thesimulator.
