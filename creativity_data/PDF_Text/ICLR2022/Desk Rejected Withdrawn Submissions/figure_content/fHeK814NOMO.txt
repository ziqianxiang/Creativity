Figure 1: Performance of the proposed algorithm for different initial learning rates (α0 ={0.001, 0.01, 0.1}) and batch sizes (bs = {64, 128, 256, 512}). The following curves are provided:training loss (left), evolution of learning rate (middle) and test accuracy (right).
Figure 2: Comparison with the closely-related work of Baydin et al. (2018) for different batchsizes under the CIFAR100+WRNET_16_4 setting. Different values of the hyperparameter β wereconsidered (akin to the η parameter of our Algorithm 1).
Figure 3: Loss (upper) and accuracy (bottom) curves of different optimizers for the setting CIFAR100+ WRNET_16_4, while selecting different batch sizes (from 64 to 512).
Figure 4: Loss (left) and accuracy (right) curves of different optimizers for the setting ImageNet +ResNet50, with batch size set to 96.
Figure 5: Loss curve of different optimizers for the Object Detection setting. The reported loss is themultibox loss as described in the work of Liu et al. (2016).
Figure 6: Impact of update frequency for the setting CIFAR100 + WRNET_16_4, with batch sizeequal to 256. Update frequency is provided as a percentage of the dataset size. The following curvesare provided: training loss (left), evolution of learning rate (middle) and test accuracy (right).
Figure 7: Per-layer learning rate curves for the MNIST+MLP setting. The global learning rate curveof the initial algorithm SgdJtlr is also included.
Figure 8: Exploration of different bounds c evaluated for both CIFAR10 and CIFAR100 datasets withthe WRNET_16_4 architecture (256 batch size and 0.1 initial learning rate). Only the first 40 epochsare reported in order to have a finer depiction of the differences.
Figure 9: Evaluation of the algorithm’s behavior when using a fixed meta-learning rate η.
Figure 10: Loss curves for different initial learning rates and batch sizes (MNIST+MLP).
Figure 11: Loss curves for different initial learning rates and batch sizes (CIFAR10+WRNET_16_4).
Figure 12: Loss curves for different initial learning rates and batch sizes (CIFAR100+WRNET_16_4).
Figure 13: Loss (upper) and accuracy (bottom) curves of different optimizers for the setting CIFAR10+ WRNET_16_4, while selecting different batch sizes (from 64 to 512).
Figure 14: Loss (upper) and accuracy (bottom) curves of different optimizers for the setting CIFAR10+ WRNET_{d}_{w}. Batch size is fixed to 256.
Figure 15: Loss (upper) and accuracy (bottom) curves of different optimizers for the setting CIFAR100+ WRNET_{d}_{w}. Batch size is fixed to 256.
Figure 16: Evaluating a DenseNet architecture on CIFAR100 dataset (256 batch size and α0 = 0.1)accuracy despite having increased loss compared to other optimizers. Nonetheless, the proposedalgorithms present similar accuracy, while converging much faster to a well-performing minimum.
Figure 17: Loss curves (cross entropy) of different optimizers for the machine translation task with atransformer architecture.
