Figure 1: The gap between the averaged loss of FGSM-RS/PGD-2-RS generated training adversarialexamples and the averaged loss of PGD-10-RS generated training adversarial examples in the 30-epoch training process with the cyclic learning schedule: The left two figures show the trainingprocess of FGSM-RS AT (α = 1.25); The right figure shows the training process of PGD-2-RS AT(α = 1.25/2). The loss is computed and averaged over the first training minibatch.
Figure 2: Standard and robust accuracy of different efficient AT methods on CIFAR-10 and SVHNwith PreAct ResNet-18 trained and evaluated with different perturbation sizes. The results are ob-tained by averaging over 5 random seeds used for training and reported with the standard deviation.
Figure 3: CIFAR10: cross-entropy loss topology (in the vicinity of a testing input) of the modelstrained by different AT methods on CIFAR10 ( = 16/255). dg denotes the unit direction vector ofthe gradient, i.e., kdg k2 = 1, and dr denotes a random (orthogonal) unit direction vector. Deeperred refers to higher loss. Note that we also train robust models by TRADES (Zhang et al., 2019b)and MART (Wang et al., 2020) and plot the model loss topology. Apparently, the loss topologyof the model trained by FGSM + GradAlign is the odd one, and the loss topologies of the modelstrained the other AT methods are similar.
Figure 4: The impacts of weight decay on FGSM + GradAlign AT, PGD-2-RS AT, and Qusai-PGD-2-RS AT. The results are obtained by averaging over 5 random seeds used for training and reportedwith the standard deviation. The other hyperparameter settings are same as the settings described inSection 5.1 and Section C.
Figure 5: The impacts of maximum learning rate on FGSM + GradAlign AT, PGD-2-RS AT, andQusai-PGD-2-RS AT under the cyclic learning schedule. The results are obtained by averaging over5 random seeds used for training and reported with the standard deviation. The other hyperparametersettings are same as the settings described in Section 5.1 and Section C.
