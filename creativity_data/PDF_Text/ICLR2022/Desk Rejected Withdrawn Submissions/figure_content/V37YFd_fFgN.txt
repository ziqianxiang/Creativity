Figure 1: Reuse Transformer. We propose a modified architecture for Transformer layers that features a mix ofstandard “exact” and “reuse” attention heads, where the latter borrow attention scores computed in previous layers.
Figure 2: All Pairs Similarity. For all pairs of layers (indexed in x- and y- axis), we visualize the similarity betweenthe best pairs of heads in that pair of layers. We show similarity scores for two BERT models on the Wikipedia datasetand one ViT model on the ImageNet dataset, using scores averaged over 10k examples in all cases.
Figure 3: Sequential Similarity. We report similairty in adjacent layers for different heads in a (left) 24 layer BERTand (right) 12 layer ViT model. For each layer, we compute similarity for each head with its closest matching head inthe previous layer. We then rank heads from lowest (rank 1) to highest similarity, and plot these across layers.
Figure 4: Role of problem domain. We visualize the all pairs best head attention similarity scores for (6 layer)Transformer models trained on Wikipedia vs random data. In the first two columns, we compare similarity in attentionscores for both models computed over 10k examples from Wikipedia, and in the last two columns over random data.
Figure 5: Ablation. Left - Training reuse models for the same amount of time as the T5BASE results in > 1%improvement on the SuperGLUE benchmark. Middle, Right - Performance of the T5BASE and ViT models with varyingthe number of reuse heads on SuperGLUE and ImageNet respectively. We see matching/improved performance withreusing a few heads. Interestingly performance doesn’t drop much for ViT models even when K = 12, inline with thehigh attention similarity among its layers.
Figure 6: Number of examples. We show the all layer pair best head similarity scores for a six layer BERT model,averaged over (left) 10k examples as in the main paper, and over (right) 1M examples. The two averages are essentiallyidentical, showing that our setting of using 10k examples is sufficient to draw conclusions about attention redundancy.
