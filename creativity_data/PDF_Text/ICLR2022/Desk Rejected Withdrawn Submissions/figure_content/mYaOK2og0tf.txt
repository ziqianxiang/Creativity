Figure 1: Hessian spectrum for WideResNet28× 10 after 300 epochs of SGD on the CIFAR-100dataset, for various L2 regularisation co-efficients λ, batch norm evaluation mode4 A PAC Bayesian Approach to Generalisation and FlatnessFor an input, output pair [xi, yi] ∈ [Rdx , Rdy], i ∈ [1, N] and a given prediction functionh(∙; ∙): Rdχ X RP → Rdy,we consider the family of prediction functions parameterised bya weight vector w, i.e., H := {h(∙; W) : W ∈ RP} with a given loss function '(h(x; w), y):Rdy × Rdy → R. Optimizing the PAC-Bayesian generalization bound Germain et al. (2016)is equivalent to optimizingNlog p(yi|xi, w)q(w)dw - KL(q(w)||p(w)),	(3)i=1where p can be a categorical distribution, whose likelihood corresponds to a softmax lossfunction l. p(w)/q(w) are the prior/posterior of the weights respectively. For example, arandom initialized weight can be seen as a sample from the prior and a trained weight canbe seen as a sample from the posterior. Notice that, this objective is the lower bound of thelog-volume, see Barber (2012, section 28.3.1)NlogZ ≥	logp(yi|xi, w)q(w)dw - KL(q(w)||p(w))	(4)i=1where Z = /p(y∣x, W)P(W)dw. The volume interpretation extends the flatness concept
Figure 2: The Trace of the Hessian can be accurately estimated with sub-sampling,the higher order moments (such as the Frobenius norm) cannot be. Trace of theFull/Batch Hessian Tr(H)/GGN Tr(G) and of the Full/Batch Hessian/GGN FrobeniusNorm.
