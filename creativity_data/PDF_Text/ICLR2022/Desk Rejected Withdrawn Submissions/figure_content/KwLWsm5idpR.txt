Figure 2: Example Pareto Front.
Figure 1: The Autotune framework.
Figure 3: AUtotUne runtimes with dif-ferent numbers of worker nodes (or par-allel Python sessions). Autotune scaleswell to efficiently use all available com-puting resources. In an environmentwith only 1 worker node, Autotune eval-uates objective functions sequentiallywithin a single Python session. Whenrun with 64 worker nodes, Autotune isable to run 64 Python sessions in par-allel, allowing for total runtime to bereduced from 4423 seconds to 127 sec-onds.
Figure 4: Comparing tuning with andwithout Fairlearn parameters. Tuningwith the COMPAS data set, with respectto two objectives: Accuracy and Demo-graphic Parity. Pink triangles show thePareto front when tuning only the re-gression hyperparameters. Green starsshow the Pareto front when adding theFairlearn parameters to the list of hyper-parameters being tuned. By adding theFairlearn parameters we achieve a dom-inant Pareto front shown by the greenstars.
Figure 5: Adult Data: A single AUtotUne execution with 500 model evaluations is capable of iden-tifying just as good of a solution as four individual Autotune executions with a total of 2000 modelevaluations. This clearly shows the strength of being able to tune machine learning models withrespect to larger numbers of objectives.
Figure 6: Comparing tun-ing with and without GA andGSS. Tuning with the COM-PAS data set, with respect to 2objectives: Accuracy and De-mographic Parity. Red cir-cles show the solutions foundby simply running Autotune'sinitial LHS. Green diamondsare the solutions found byrunning the full hybrid searchstrategy in Autotune (LHS,GA, GSS).
