Figure 1: Left: Average accuracy (ACC) on tasks encountered so far for each method. Right:Progression of the test accuracy for the first task for each method, as more tasks are learned.
Figure 2: Average test accuracy on all T tasks after training on each task for all methods on Rotatedand Permuted MNISTD	Discriminative Feature Learning with Center LossTypical deep neural network architecture comprises an input layer, followed by several hidden layerswith non-linear activation functions and the output layer. The output layer generally has a softmaxactivation function for multi-class classification. This last fully connected layer acts as a linearclassifier that separates the deeply learned features produced by the last hidden layer. The softmaxloss forces the deep features of different classes to stay apart. The discriminative power of learnedfeatures is enhanced if the intra-class compactness and inter-class separability are maximized simul-taneously. Though the features learned using the softmax loss are separable, they are not discrim-inative enough for open-set supervised problems and often exhibit high intra-class variance. Thisadversely affects the generalization capabilities of neural networks.
Figure 3: Average test accuracy (ACC) for various values of CLR’s hyperparameter λ for differentprotocolsSoftmax Loss200ιooo-IOO-200-300-300	-200	-100	0	100	200Figure 4: Left: Visualization of features in 2D from a CNN model trained on MNIST dataset LeCun(1998) with Softmax Loss. Right: Visualization of features in 2D from a CNN trained on MNISTdataset with joint supervision of Softmax Loss and Center Loss with λ = 1loss to enhance the discriminative power. The siamese network Koch et al. (2015) based approacheswhich use contrastive loss Sun (2015); Hadsell et al. (2006) and triplet loss Schroff et al. (2015),learn the embeddings directly. These approaches face the problem of semi-hard sample mining andcombinatorial explosion in the number of pairs or triplets, which significantly affect the effectivemodel training Deng et al. (2019). There are also angular margin penalty-based approaches that haveshown significant improvements over softmax loss and have been explored in various directions,especially for large-scale face recognition Liu et al. (2017); Wang et al. (2018b;a); Liu et al. (2016);
Figure 4: Left: Visualization of features in 2D from a CNN model trained on MNIST dataset LeCun(1998) with Softmax Loss. Right: Visualization of features in 2D from a CNN trained on MNISTdataset with joint supervision of Softmax Loss and Center Loss with λ = 1loss to enhance the discriminative power. The siamese network Koch et al. (2015) based approacheswhich use contrastive loss Sun (2015); Hadsell et al. (2006) and triplet loss Schroff et al. (2015),learn the embeddings directly. These approaches face the problem of semi-hard sample mining andcombinatorial explosion in the number of pairs or triplets, which significantly affect the effectivemodel training Deng et al. (2019). There are also angular margin penalty-based approaches that haveshown significant improvements over softmax loss and have been explored in various directions,especially for large-scale face recognition Liu et al. (2017); Wang et al. (2018b;a); Liu et al. (2016);Deng et al. (2019).
