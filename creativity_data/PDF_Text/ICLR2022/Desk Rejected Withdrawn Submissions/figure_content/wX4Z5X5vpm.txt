Figure 1: The architecture of Decoupled Policy Opti-mization (DPO), which consists of an expert state tran-sition predictor (to plan where to go) followed by aninverse dynamics model (to decide how to reach).
Figure 2: Multi-step optimization. Given an ex-pert state sE, hψ predicts the next possible stateS01, which is further fed to a target network hψoto predict the following sequence. The total losscomputes the MSE loss along the state sequence.
Figure 3: Cycle training style. Given an ex-pert state sE, Iφ (s, s0) takes input the pre-dicted state ^ and SE to get the executionaction a, then an additional forward dynam-ics model Mω is used to simulated one steprollout using (sE , a) and get a forward nextstate s0. The total loss computes the MSEloss between the two predicted states.
Figure 4: Toy example.
Figure 5: Learning curves on 6 easy-to-hard continuous control benchmarks, where the solid line and the shaderepresent the mean and the standard deviation of the averaged return over more than 5 random seeds. Wepre-train BCO and DPO for 50k steps and show it in figures.
Figure 6: Compounding error of the predicted consecutive states and the real states the agent reaches whenrollout in the environments.
Figure 7: VisualiZation of NGSIM I-80 data set and its mapping on the simulator. This figure is borrowed from(Henaff et al., 2019).
Figure 8: Each grid in the ‘Expert’ graph representsthe transition probability from s (state) to s0 (stateprime) in demonstration data. Since some states donot appear in the demonstration, the transition proba-bilities from such states are undefined, and we excludethem from the graph. The ‘DPO’ graph shows the stateprime output by state transition predictor from eachstate accordingly.
Figure 9: The learned policy (action selection proba-bility) among 20 possible actions at 3 states (whichis marked on the top sub-graph). The distributions indifferent states are split by red lines, and the resultingtransition is labeled between two sub-graphs. Thisshows that the inverse dynamics mismatch is not thekey for imitating expert the state sequence since theagent can select different actions from the expert aslong as they lead to the same transition.
Figure 10: The rollout density and loss curves when k = 1. BCO and DPO have similar asymptotic performance(KLD), but DPO has a significantly faster convergence rate. On the contrary, GAIfO still fails to find the secondpath.
Figure 11: Visualization of sampled state transition distributions on HalfCheetah environment using UMAPreduction.
Figure 12: Hyperparameter study on λh .
Figure 13: The empirical correlation between the prediction-real distance and the reward. Typically, lessprediction-real distance achieves better performanceD.7 Complete Evaluation ResultsIn this section we show complete evaluation training curves of DPO with different regularization inFig. 14. Typically, experiments with less prediction-real distance can achieve better performance. Itis worth noting that, DPO can generally achieve better efficiency than the baselines in most of theenvironments. However, with fine-tuning the regularization, we are able to dig the potential of DPO.
Figure 14: Complete learning curves of DPO with different regularization.
