Figure 1: Results of testing accuracy for ResNet-18 on CIFAR-100 (three independent runs) for our architec-ture tweaking and training recipe tweaking techniques. X-axis denotes the sparsity level of the network. Dashstraight lines denote the accuracy of dense network trained with and without proposed tweaking techniques.
Figure 2: Comparison of the top eigen-value of Hessian (mean across 10 ran-dom batches) of sparse NN trained withdifferent activation functions. Com-paratively smaller eigenvalues of soft-activation (Swish/Mish) based sparse NNwrt. ReLU based networks indicate thepresence of high smoothness.
Figure 3: Sketch of our modified ResNet-18 block to introduce additional skip-connections forsparse re-training.
Figure 4: Comparison of loss surface contours of ResNet-18 models trained on CIFAR-100 with91% sparsity in early training stage (epoch 5) using vanilla-LTH and our Smoothness-aware tweaks.
Figure 5: Comparison of testing accuracy of proposed techniques (AT + TRT combined) for Vgg16,ResNet-18, ResNet-34, ResNet-50, and MobileNet on CIFAR-10, CIFAR-100, Tiny-ImageNetdatasets wrt. vanilla-LTH (three independent runs). Straight dash line represent the performanceof dense network without any tweaking.
Figure 6: (a) Comparison of our techniques (architecture tweaking and training recipe tweaking)with respect to vanilla LTH using ResNet-18 on CIFAR-100 (three independent runs). We observesignificant improvements in the performance of sparse networks when we combine our techniquesduring the spending process of tickets. ”Winning tickets” can be identified for sparsity as highas 91% for ResNet-18 on CIFAR-100. (b) Comparison of test accuracy of LTH trained with ourtechniques and LTH-Rewind.
Figure 7: Comparison of the testing accuracy of our proposed techniques: architecture tweaking(AT) and training recipe tweaking (TRT) with respect to rewinding using ResNet-18 on CIFAR-100 (three independent runs). We also show the effect of combining rewinding with AT and TRT.
Figure 8: Comparison of loss landscape of models with 91% sparsity trained using vanilla-LTH andour Smoothness-aware techniques (architecture tweaking and training recipe tweaking). Loss plotsare generated with the same original images randomly chosen from CIFAR-100 test dataset using(Li et al., 2017a). z-axis denote the loss value which has been clamped at 8.0 for better visualization.
Figure 9: Comparison of our pro-posed LTH (+AT and +TRT), whennaive soft labels (LS) are replaced withknowledge-distilled (KD) soft-labels inTRT for ResNet-18 on CIFAR-100.
Figure 10: The change in testing loss asa function of perturbed weight distance,in the direction of top eigenvector ofHessian matrix for ResNet-18 trainedon CIFAR-100.
