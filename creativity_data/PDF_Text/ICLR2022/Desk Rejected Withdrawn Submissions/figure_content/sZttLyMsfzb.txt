Figure 1: The expected test ZO loss and its bias and variance. The models were trained with 20%label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 2:	Test accuracy and OV. The models were trained with Adam optimizer (learning rate0.0001). The number in each legend indicates its percentage of label noise.
Figure 3:	Test accuracy and OV. The model was LeNet-5 trained on MNIST and FashionMNISTwith Adam optimizer (learning rate 0.0001).
Figure 4:	Early stopping based on test error (True) and the corresponding OV (Found). The shapesrepresent different datasets, whereas the colors indicate different categories of DNNs (“CF” and“Res” denotes “CIFAR” and “ResNet”, respectively).
Figure 5: Test accuracy and OV w.r.t.
Figure 6: Test accuracy and OV of mod-els trained on different number of train-ing samples.
Figure 7: Test accuracy and Vg . The models were trained with the Adam optimizer (learning rate0.0001). The number in each legend indicates its percentage of label noise.
Figure 8: Different loss functions w.r.t. the training epoch. ResNet18 was trained on SVHN, CI-FAR10, and CIFAR100 with 20% label noise to introduce epoch-wise double descent. Adam opti-mizer with learning rate 0.0001 was used.
Figure 9:	Test accuracy and optimization variance (OV) of VGG11 on SVHN, w.r.t. different levelsof label noise. Adam optimizer with learning rate 0.001 was used.
Figure 10:	Loss, variance and bias w.r.t. different levels of label noise. The model was ResNet18trained on CIFAR10. Adam optimizer with learning rate 0.0001 was used.
Figure 11:	Test MSE loss and the corresponding bias/variance terms. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 12:	Test CE loss and the corresponding bias/variance terms. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
Figure 13:	The expected test ZO loss and its bias and variance. The models were trained with 20%label noise. Adam optimizer with learning rate 0.001 was used.
Figure 14:	Expected test ZO loss and its bias and variance. The models were trained with 20% labelnoise. SGD optimizer (momentum = 0.9) with learning rate 0.01 was used.
Figure 15:	Expected test ZO loss and its bias and variance. The models were trained with 20% labelnoise. SGD optimizer (momentum = 0.9) with learning rate 0.001 was used.
Figure 16: Test accuracy and optimization variance (OV). The models were trained with Adamoptimizer (learning rate 0.001). The number in each legend indicates its percentage of label noise.
Figure 17:	Test accuracy and optimization variance (OV). The models were trained with SGD op-timizer (learning rate 0.01, momentum 0.9). The number in each legend indicates its percentage oflabel noise.
Figure 18:	Test accuracy and optimization variance (OV). The models were trained with the SGDoptimizer (learning rate 0.001, momentum 0.9). The number in each legend indicates its percentageof label noise.
Figure 19:	OV estimated from different number of training batches. The models were trained with20% label noise. Adam optimizer with learning rate 0.0001 was used.
