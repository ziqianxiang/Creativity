Figure 1: The similarity of representations between ResNet-32 blocks (x-axis) and NAS-Bench-201 cells (y-axis) for different architectures using CKA. The first, second, and third plots are thesimilarity for individual network. These networks obtain 80.36%, 89.63%, and 93.75% test accuracyon CIFAR-10. The last plot is the average similarity of 1000 architectures selected randomly.
Figure 2: The similarity be-tween ResNet-20 blocks (x-axis) and DARTSâ€™s supernetcells (y-axis) using CKA.
Figure 3: Comparison between the conventional method and the proposed method. (a) The conven-tional method trains all layers of a network, which requires memory for calculating the gradients ofall layers. (b) The proposed method replaces first few layers of NAS-based network with pre-trainedlayers of a hand-crafted network. The weights in these pre-trained layers are frozen, while others arelearned through backpropagation. Since we only need to compute the gradients for the remaininglayers, the memory footprint is reduced while preserving the performance.
Figure 4: Performance comparison of REA on NAS-Bench-201. Top: The average memory foot-print during training. Bottom: The average test accuracy on NAS-Bench-201.
Figure 5: Performance of DARTS+PT on CIFAR-10, CIFAR-100, and ImageNet-16-120.
Figure 6: The allocated memory of thesupernet with a batch size of 128.
Figure 7: Rank correlation un-der different warmup epochs onCIFAR-100, NAS-Bench-201.
Figure 8: Performance of DARTS+PTwith Cutout on CIFAR-100, NAS-Bench-201.
Figure 9: Dominant Eigenvalue dur-ing search (Left: Original SuPernet,Right: Tr-SuPernet-5-5, warmuP 13ePochs).
Figure 10: Training time comparison between the conventional method and proposed method onCIFAR-100.
