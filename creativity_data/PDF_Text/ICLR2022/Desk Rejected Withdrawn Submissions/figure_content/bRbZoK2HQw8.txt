Figure 1: The trade-off phenomenon of CNNb on CIFAR-10. Table.1 details one basic CNNb. For the CNNb,we conduct ST, typical AT and JT (Joint AT with equivalent natural and adversarial inputs). Different trainedmodels are listed on the horizontal axis, where the ɪmark * : * denotes the loss ratio between two kinds of dataduring training. The vertical axis shows their test accuracy. The adversarial examples used for the training andtest sets are uniformly produced by PGD(0.031, 5, 0.01)1 2. Under the same training setting detailed in section5.1, the histogram of test results reflects a clear trade-off tendency between generalization and robustness 1.
Figure 2: Exhibition of normalized gradient3 on inputs with respect to loss. (a) The natural input images; (b)Gradients on inputs from MST and MAT . Overall, the color depth reflects the magnitude of the gradient.
Figure 3: (a) Some natural images and their transformation examples in the test set. The first row shows somenatural images. The nether four rows separately show result examples via left (L), right (R), up (U) and down(D) shifts. (b) Comparison results between MST and MAT on different test sets. We apply the 4-directionshifts transformation on natural test set, and generate 4 new test set (U, L, R and D). The right sub-figure showstest results of both models. The test set (N) list their baseline accuracy on natural test set. The last 4 bar chartscheck their generalization on 4 shift sets. As the verification showed, under almost equal ability on natural set(N), MAT performs worse adaptation to shifts of characters than MST , especially for the up and down shifts.
Figure 4: Exhibition of gradient signs on natural inputs. To observe attention difference of different models.
Figure 5: Sensitiveness test of β with CNNb on CIFAR-10.
Figure 6: Comparison of gradient signs on natural character ’1’ and ’3’ between CNNb(50) and CNNb(30).
