Figure 1: Examples of two view generation pro-cesses used in self-supervised pre-training: (a)strong data augmentations as proposed in [9] ap-plied to a single image; (b) simpler augmenta-tions applied to an object viewed across multi-ple frames. Our experiments show that the latterleads to more robust representations for pose- ormotion-sensitive tasks.
Figure 2: Left: close-up of a t-SNE plot showing the bifurcation of partially co-located trajectories;in the red region, all frames correspond to unique objects (chairs) with roughly the same pose. Theframes of separate videos are shown in a different color. Right: two examples of PCA projectionsof the embeddings of video frame sequences. The color of each point indicates its position in thevideo sequence (on the viridis scale), which shows the smooth nature of the embedding space.
Figure 3: Nearest neighbors of objects found in the validation set where the leftmost crop in eachrow shows the query. Top: nearest neighbors including frames from the query video; note that twoof the results are outliers as they originate from a different yet similar video (shown with a differentborder color). Bottom: nearest neighbors across frames of other videos (each with a unique bordercolor); note the diversity in background but the consistency in object poses.
Figure 4: Examples of pose estimation for chairs (left) and shoes (right): our method enables us toobtain similar bounding boxes for pose estimation for each query frame. Furthermore, the fetchednearest neighbors also show similar attributes (e.g. shape) compared to the query frames. Theground truth and obtained 3D bounding boxes are shown as yellow line overlays.
Figure 5: PR curve for our baseline modelfor object re-identification. Performance isbest on categories that have distinctive fea-tures across their instances.
Figure 6: Our network architecture (a) is inspired by the Siamese framework of [9]. We sam-ple frames from a video and use these as views to learn good representations by maximizing thesimilarity between their corresponding embeddings. The use of video frames makes the learnedrepresentations more motion- and geometry-aware which enables their use in tasks such as poseestimation and action recognition. The setups for the projector and predictor blocks are shown in(b) and (c), respectively.
Figure 7: Overview of frame-to-frame nearest neighbor matching results obtained with the Objec-tron validation set for the bike, book, bottle, camera, cereal box, chair, laptop and shoe categoriesof Objectron. Each double-row segment has a corresponding high-resolution video provided withthis document. Top shows the query frames (white bounding box is ground truth, green and redbounding boxes are the reprojected 3D box from the nearest neighbor). Bottom shows the resultframes and the associated 3D bounding boxes in green. The full resolution version of the images inthis figure are also provided with this document (as part of the frame Jmatching folder).
Figure 8: Examples of object thickness estimation failures combined with ground plane estimationfailures. Query frames are shown in the top row, nearest neighbors in the bottom row.
Figure 9: Example of degraded book pose estimation. The orientation of the pattern on the bookseems to confuse the pose estimation. Query frames are shown in the top row, nearest neighbors inthe bottom row.
Figure 10: Example of failing nearest neighbor retrieval for a chair with a dot pattern (left). Theother crops correspond to nearest neighbors across unique videos.
