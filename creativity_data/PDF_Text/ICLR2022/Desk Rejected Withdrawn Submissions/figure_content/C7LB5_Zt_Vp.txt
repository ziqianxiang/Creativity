Figure 1: Training a seven-layer MLP on CIFAR-10 dataset with ReLU: scatter plots of ReLUactivations at some sampled layers over iterations.
Figure 2: Graphs of ReLU-like activation functions and their first derivatives. (a) ReLU,LeakyReLU, and ELU. (b) The first derivatives of ReLU, LeakyReLU, and ELU. (c) GELU, SiLU,and Mish. (d) The first derivatives of GELU, SiLU, and Mish.
Figure 3: The action of ZeroLiers for the vari-ants of ReLU. (a) For ReLU, LeakyReLU, andELU. (b) For ReLU, LeakyReLU, and ELU.
Figure 4: Training a seven-layer MLP on CIFAR-10 with ZeroLiers when k = 3.
Figure 5: k as a learnable parameter when the original activation function is ReLU. (a) Training anMLP on CIFAR-10 using ZeroLiers: validation accuracy over iterations using various values of k.
Figure 6: Validation accuracy of training the seven-layer MLP on CIFAR-10 over iterations for thesix variants of ReLU.
Figure 7:	The validation losses of the eight-layer AE and DAE on CIFAR-10. ZeroLiers-L-k (k0 =3) is compared with the baseline model and Dropout.
Figure 8:	The training and validation losses of the three-layer BERT model on Wikitext-2.
Figure 9: The structure of fully connected layers for each experimental setting.
Figure 10: Change of the k value over epochs in ZeroLiers-L-k for the seven-layer MLP.
Figure 11: The validation accuracy (loss) of the seven-layer MLP and eight-layer AE usingZeroLiers-L-k (k0 = 3) on CIFAR-10 for various optimizers. The original activation function isReLU. The convergence speed and the validation accuracy (loss) are highly dependent upon theoptimizers used.
Figure 12: The training and validation losses of the one-layer GPT-2 on Wikitext-103v2.
