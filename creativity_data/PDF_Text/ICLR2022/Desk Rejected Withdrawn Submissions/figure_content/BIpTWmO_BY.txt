Figure 1: High-level schematic of our attack. A small proportion of slightly perturbed data is addedto the training set which “backdoors” the model so that it misclassifies patched images at inference.
Figure 2: Sample clean source (first column), patched source (second column), clean target (thirdcolumn), and poisoned target (fourth column) from the ImageNet dataset. The last column is slightlyperturbed, but the perturbed and corresponding clean images are hardly distinguishable by the hu-man eye. More visualizations can be found in the Appendix B.
Figure 3: Sample random patch (left) and HTBD patch (right)Table 10: Baseline evaluations using random patches on CIFAR-10. Perturbations have '∞ -normbounded above by 16/255, and poison budget is 1% of training images. Each number denotes anaverage (and standard error) over 24 independent crafting and training runs along with randomlysampled source/target class pairs. Each run has a unique patch generated randomly. Figure 3 (left)shows a sample random patch we use for the experiments presented in this table.
Figure 4: Average poisoning time for various Sleeper Agent setups. All experiments are conductedon CIFAR-10 with ResNet-18 models. Perturbations have '∞-norm bounded above by 16/255, andthe poison budget is 1% of training images.
Figure 5: Sample clean source (first column), patched source (second column), clean target (thirdcolumn), and poisoned target (fourth column) from the ImageNet dataset. Perturbations have '∞-norm bounded above by 16/255, and the patch size is 30 × 30.
Figure 6: Sample clean source (first column), patched source (second column), clean target (thirdcolumn), and poisoned target (fourth column) from the ImageNet dataset. Perturbations have '∞-norm bounded above by 16/255, and the patch size is 45 × 45.
