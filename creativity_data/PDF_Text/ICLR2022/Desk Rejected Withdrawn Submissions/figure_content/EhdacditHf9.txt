Figure 1: Relationships between the optimiz-ers in terms of the results in Table 1 (relationss?C,SGD ≤ s?C,NM ≤ s?C,A hold generally,but those of K (s?C,SGD) ≥ K (s?C,NM) ≥K (s?C A) depend on momentum coefficientβ)Figure 2: Relationships between the optimiz-ers in terms of the results in Table 2 (relationss?D,SGD = s?D,NM ≤ s?D,A hold generally,but those of K (s?D,SGD) ≥ K (s?D,NM ) ≥K(s?D,A)depend on momentum coefficientβ)Convergence analyses of adaptive methods for nonconvex optimization were studied in (Fang et al.,2018; Chen et al., 2019; Zhuang et al., 2020; Iiduka, 2021). In (Chen et al., 2019), it was shown thatgeneralized Adam, which includes the Heavy-ball method, AdaGrad, RMSProp, AMSGrad, andAdaGrad with First Order Momentum (AdaFom), using a diminishing learning rate ak = 1 /∖Tkhas an O (log K/ √K) convergence rate. AdaBelief (named for adapting stepsizes by the belief inobserved gradients) using ak = 1 /Vk has O (log K//K) convergence (Zhuang etal., 2020). In(Iiduka, 2021), a method was presented to unify useful adaptive methods such as AMSGrad andAdaBelief, and it was shown that the method with ak = 1 /√k has an O(1 /√K) convergence rate,
Figure 2: Relationships between the optimiz-ers in terms of the results in Table 2 (relationss?D,SGD = s?D,NM ≤ s?D,A hold generally,but those of K (s?D,SGD) ≥ K (s?D,NM ) ≥K(s?D,A)depend on momentum coefficientβ)Convergence analyses of adaptive methods for nonconvex optimization were studied in (Fang et al.,2018; Chen et al., 2019; Zhuang et al., 2020; Iiduka, 2021). In (Chen et al., 2019), it was shown thatgeneralized Adam, which includes the Heavy-ball method, AdaGrad, RMSProp, AMSGrad, andAdaGrad with First Order Momentum (AdaFom), using a diminishing learning rate ak = 1 /∖Tkhas an O (log K/ √K) convergence rate. AdaBelief (named for adapting stepsizes by the belief inobserved gradients) using ak = 1 /Vk has O (log K//K) convergence (Zhuang etal., 2020). In(Iiduka, 2021), a method was presented to unify useful adaptive methods such as AMSGrad andAdaBelief, and it was shown that the method with ak = 1 /√k has an O(1 /√K) convergence rate,which improves on the results in (Chen et al., 2019; Zhuang et al., 2020).
