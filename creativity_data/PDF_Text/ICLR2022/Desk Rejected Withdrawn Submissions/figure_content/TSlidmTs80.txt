Figure 1: K-SAVENâ€™s system overview. Visual observation vt is fed to two modules: vision encoderfev , which encodes the visual observation, and pre-trained vision model fcv, which, given the visualobservation, predicts classification scores ctv for objects and regions. These scores are used by thevision-based graph convolutional network GCNv to compute visual-semantic feature embeddings.
Figure 2: Graph Convolutional Networks. Each vertex denotes an object category or region category.
Figure 3: Examples of issues with the scans and semantic labeling in the Matterport3D. The imagesshown correspond to the scene ID aayBHfsNo7d and node: (93, 270). (A) shows an RGB imageexample in which the objects are not clearly visible due to glitches in the scan. (B) shows the semanticlabels of 14 objects, and (C) shows the semantic labels of 4 regions; however, these objects andregions are not clearly visible in the corresponding RGB image.
Figure 4: Examples of unusual semantically placed objects in scene ID 2n8kARJN3HM of Matter-port3D. In the image show, a bathtub is placed in the living room, and chairs are kept on the top of atable, which is unusual placement of these objects.
