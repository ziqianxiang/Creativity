Figure 1: Classification performance with interpolationthan half of the inputs that were correctly classified without interpolation (green and blue dottedcurves in Fig. 1(b)), even in the final epochs of training. On further investigation, it is found thatfor many interpolated inputs, the network is able to correctly classify only one of the constituentinputs. The golden label of the other constituent input often does not appear even amongst the Top-5predictions made by the network (further quantitative details in Sec. 7.4). This leads to increasedloss for one of the constituent samples, consequently impacting training performance and the finalvalidation accuracy1. It is thus critical to develop techniques that effectively learn on all constituentsamples of an interpolated input. We next describe our approach to addressing this challenge.
Figure 2: Training Interpolated InputsExamining the intermediate representations of the network while processing interpolated inputssheds some light on this interference. By virtue of the nature of convolutions, the spatial separa-tion between constituent inputs in the interpolated input is maintained through many layers of thenetwork. For example, in Fig. 2, the right half of the features in the final convolution layer’s outputpertain to the right half of the interpolated input. The spatial distinction between the features ismaintained until the last convolutional layer, but is lost after the averaging action of the final pool-ing layer. As a result, we observe that the fully connected layer correctly classifies only one of theconstituent inputs.
Figure 3:	Overview of Selective InterpolationOverview: The proposed selective interpolation strategy consists of three steps as shown in Fig. 3.
Figure 4:	Analyzing amenability to interpolationHence, for samples that are not interpolated in epoch E, We determine their amenability to inter-polation in the next epoch based on the particular region of the loss distribution it belongs to2. Asillustrated in Fig. 4(c), the loss distribution is divided into three regions that utilize a different cri-teria for gauging amenability. We now discuss the criteria for each region, and the conditions forcontinuing interpolation in subsequent epochs.
Figure 5: Efficacy of threshold Lmidsamples, as they are likely to be correct. Fig. 5 plots the efficacy of Lmid across different epochs)2In Sec. 7.5, we find that classification loss does not change rapidly across epochs. Sample amenability inepoch E+1, can thus be determined based on classification loss in epoch E6Under review as a conference paper at ICLR 2022(fraction of correct inputs under Lmid). As desired, a majority of the correct samples (> 95%)fall in Region 1, while only including a negligible fraction of incorrect samples (< 10%). In fact,we find that interpolating correct samples with loss > Lmid has an adverse effect on accuracy, asthey represent outliers in the distribution. Furthermore, samples with loss greater than LinCOrr in aparticular epoch are in the upper percentile of the loss distribution of the incorrect samples. LinCOrrcan hence used to create Region 2 and Region 3 as shown. We note that loss thresholds of betterquality can potentially be identified using other techniques, such as by introducing hyper-parameters.
Figure 6:Amenability Criteria for Region 3: Samples in Region3 have high loss (loss > LinCOrr), and are generally very difficult to classify by the network evenif they are trained without interpolation. In fact, We observe that a considerable fraction of samplesthat consistently occur in Region 3 across epochs remain incorrect at the end of the training process.
Figure 7: Loss dynamics of samples in set I and set Ccuracy. Consider set C, which comprises of samples that are correctly classified at the end of train-ing. In Fig. 7(b), it is seen that around 4% of the samples in C occur in Region 3 for over 60% of thetraining process, with their classification accuracy improving only in the later stages of training. Wethus stipulate criteria to identify the desired subset of Region 3 samples, and interpolate these onlyfor some epochs. In addition to belonging to Region 3, if a sample,s loss increases over consecu-tive epochs (i.e., become increasingly difficult) it is interpolated for the next epoch, ensuing whichit is brought back to SnOInt . In Fig. 8(b), we find that increasing the period of time k for whichthe difficult samples must exhibit increasing loss and subsequently be interpolated, only marginallyimproves the accuracy and runtime benefits. We hence use k = 1 for all our experiments therebyeliminating our dependence on any hyper-parameters.
Figure 8: Amenability for samples in Region 3Determining sample amenability every epoch adds not more than 2% overhead in runtime on aver-age. The proposed amenability criteria thus help us successfully realize selective interpolation, i.e.,achieve a competitive runtime efficiency versus accuracy trade-off. In Sec. 7.7 We underscore theefficacy of our criteria by comparing against different metrics.
Figure 9: Comparison against exist-ing efforts.
Figure 10: Validation Accuracy curves7.3	Experimental Results on Cifar10Table 2: Cifar10Network	Training Strategy	Top-1 Error	Speed- UP_ResNet18	Baseline SGD InterTrain -RandPatch InterTrain-LinAvg	^65%- 5.4% 5.7%	1× 1.74 X 1.69 ×ResNet34	Baseline SGD InterTrain -RandPatch InterTrain-LinAvg	-3.2%- 4.2% 4.6%	F 1.78 X 1.71 XTo underscore the wide applicability of InterTrain, we present our runtime and accuracy trade-offachieved on the Cifar10 benchmarks in Table 2. Across our benchmarks, LinAvg achieves upto1.7 × improvement in runtime, while RandPatch achieves a 1.8× runtime improvement. Bothtechniques provide upto a 0.8-1% boost in accuracy, due to the imrpoved regularization providedvia interpolating samples.
Figure 11: Top-5 classification accuracy of second constituent input7.4	Analysis of Top-5 accuracy without interference reductionIn Sec. 3.1 it is mentioned that the network is unable to detect both constituent inputs when interfer-ence between them is not reduced. At most, the network detects only one of the constituent inputs,with the second constituent rarely appearing in the Top-5 predictions made. We provide the Top-5classification accuracy of the second constituent in Fig. 11 for the RandPatch operator, prior toreducing interference - clearly, accuracy never exceeds 12%. This emphasizes the need for devisingstrategies to reduce interference between the constituent inputs of a composite sample.
Figure 12: Change in average loss across epochsFig. 12 plots the loss curve averaged across all training examples when trained with baseline SGD.
Figure 13: Ablation analysisclassification accuracy is not impacted severely. However, if interference between the constituentinputs is not mitigated, training performance on interpolated samples is poor (green markings). Con-sequently, the selective interpolation strategy is forced to become conservative, identifying fewersamples that can be interpolated every epoch without affecting accuracy severely. Reducing inter-ference between the constituent inputs improves both accuracy by more than 1%, and speed-up by10% (red markings).
Figure 14:	Comparing oracle-based skipping against our effort7.10 Comparison against additional existing effortsIn this subsection We compare InterTrain against some additional training acceleration techniques,i.e., model size reduction techniques. We illustrate the same in Fig. 15.
Figure 15:	Comparison against existing effortsWe first analyze StocDePth (Huang et al., 2016), wherein the authors stochastically bypass residualblocks by propagating input activations/error gradients via identity or downsampling transforma-tions, thereby providing better runtime. However, the approach is targeted towards extremely deep16Under review as a conference paper at ICLR 2022networks, and incurs a noticeable accuracy loss on smaller networks such as ResNet50. (Yuan et al.,2020) explores pruning during training by reducing the size of the weight and activation tensors in astructured manner, providing speed-ups on GPU/TPU platforms. On complex benchmarks such asResNet50, such techniques incur a significant drop in accuracy.
