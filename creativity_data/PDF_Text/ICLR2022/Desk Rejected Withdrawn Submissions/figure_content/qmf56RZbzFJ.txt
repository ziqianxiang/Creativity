Figure 1: Overview of the proposed method. This setup is using off-policy data to train the policyand the environment models.
Figure 2: Unkown true environment reward selection criteria: Performance comparison on con-tinuous control environments using the Pybullet physics simulation. Relative reward for differentamount of expert trajectories using OPOLO, BC and SOIL-TDM as imitation learning methods.
Figure 3: Best true environment reward selection criterion: Performance comparison on continuouscontrol environments using the Pybullet physics simulation. Relative reward for different amountof expert trajectories using OPOLO and SOIL-TDM as imitation learning methods. The value 1corresponds to expert policy performance.
Figure 4: The policy loss, estimated reward (based on discriminator output for OPOLO) and theenvironment test loss during training in the pybullet Ant environment using our proposed SOIL-TDM and the original OPOLO implementation with 4 expert trajectories.
Figure 5: The policy loss, estimated reward (based on discriminator output for OPOLO) and theenvironment test reward during training in the pybullet Hopper environment using our proposedSOIL-TDM and the original OPOLO implementation with 4 expert trajectories.
Figure 6: The policy loss, estimated reward (based on discriminator output for OPOLO) and theenvironment test reward during training in the pybullet Walker2D environment using our proposedSOIL-TDM and the original OPOLO implementation with 4 expert trajectories.
Figure 7: The policy loss, estimated reward (based on discriminator output for OPOLO) and theenvironment test reward during training in the pybullet HalfCheetah environment using using ourproposed SOIL-TDM and the original OPOLO implementation with 4 expert trajectories.
