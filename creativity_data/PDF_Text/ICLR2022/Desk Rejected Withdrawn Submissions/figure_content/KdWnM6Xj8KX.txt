Figure 1: Results for 2-player Leduc poker.
Figure 2: Results for real-world games.
Figure 3: RRD for 3-player Leduc poker	Table 1: RRD for 3-player Leduc pokerBesides, we observe that the regret of the RRD profile after convergence verifies our Theorem 1empirically. Since we set regret threshold λ = 0.35 for both games, Theorem 1 implies that theremust exist a profile with regret lower than or equal to 0.35. In practice, we observe that the regret ofMRCP is much lower than λ = 0.35. This phenomenon indicates a tremendously desirable propertyof learning in games with EGTA. The property is that distinct from online game learning whereinthe online profile is expected to converge to certain solution concept, learning with EGTA succeedswhenever the solution falls into the empirical game, which does not require the convergence of onlineprofiles given by MSSs, and thus reducing the difficulty of learning. This explains why learning withregularization leads to convergence in contrast with being cyclic in online settings (Mertikopouloset al., 2018).
Figure 4: Properties of RRD.
Figure 5: Analysis for regularization.
