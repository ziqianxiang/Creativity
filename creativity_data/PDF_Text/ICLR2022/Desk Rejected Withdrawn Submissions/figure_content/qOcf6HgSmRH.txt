Figure 1: Illustration of Generalized Message Aggregation Functionsgeneralized aggregation and observe improved performance with the correct choice of aggregationparameters. Finally, we demonstrate how learning the parameters of our generalized aggregation,in an end-to-end fashion, leads to state-of-the-art performance in several OGB benchmarks. Ouranalysis indicates the choice of suitable aggregations is imperative to the performance of differenttasks. A differentiable generalized aggregation function ensures the correct aggregation is used foreach learning scenario.
Figure 2: Training losses of ResGCN+ and ResGCN, PlainGCN(a) different aggregators on the obgn-protein dataset. (b) different aggregations on the obgn-arxiv dataset.
Figure 3:	Vanilla aggregators perform differently and inconsistently on different datasets. e.g.
Figure 4:	Learning curves of 112-layer DyResGEN with SoftMaxSum and PowerMeanSum5.3 Comparison with SOTAIn this section, we apply the proposed GENeralized Aggregation Networks (GEN) with SoftMaxaggregators to seven OGB datasets (ogbn-proteins, ogbn-arxiv, ogbn-prodUcts, ogbg-molhiv, ogbg-molpcba, ogbg-ppa and ogbl-collab) across varioUs tasks of node classification, link prediction, andgraph classification in Table 3. We apply oUr GEN models to the mentioned seven OGB datasetsand compare resUlts with the pUblished GNN methods with official implementation posted on OGBLearderboard (See Table 3). The methods inclUde Deepwalk (Perozzi et al., 2014), GCN (Kipf &Welling, 2016), GraphSAGE (Hamilton et al., 2017), GIN (XU et al., 2019b), GIN or GCN withvirtUal nodes, JKNet (XU et al., 2019a), GaAN (Zhang et al., 2018), GatedGCN (Bresson & LaUrent,2018), GAT (VeliCkoviC et al., 2018), HIMP (Fey et al., 2020), GCNn (Ming Chen et al., 2020),DAGNN (LiU et al., 2020a), GraphZoom (Deng et al., 2020), GeniePath-BS (LiU et al., 2020b) and8Under review as a conference paper at ICLR 2022PNA (Corso et al., 2020). The provided results on each dataset are obtained by averaging the resultsfrom 10 independent runs. It is clear that our proposed GNN models outperform SOTA on all fourdatasets. In particular, our models significantly outperform previous SOTA methods on ogbn-proteinsand ogbg-ppa by 7.91% and 6.75%, respectively. In terms of the performance on ogbn-arxiv, ourmodel is better than JKNet, DAGNN but slight worse than GCNII. However, GCNII uses 4 timesmore parameters compared to our model (491,176 (Ours) vs. 2,148,648 (GCNII)). We try to adapt the
Figure 5: Learning curves of 112-layer DyResGEN with SoftMax and PowerMeanC Detailed Comparison with SOTADue to the limit of space, the number of parameters of each model is not included in Table 3. Here,We include them in Table 6. Our proposed model achieves state-of-the-art or on par performance oneach dataset With a moderate size of model parameters.
