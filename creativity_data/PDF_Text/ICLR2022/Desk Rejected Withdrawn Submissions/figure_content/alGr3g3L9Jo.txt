Figure 1: Classes contain critical information that is not explicitly encoded in the class labels.
Figure 2: Lspread produces embeddings that are qualitatively better than those produced by LSC .
Figure 3: Points from large subclasses clustertightly; points from small subclasses scatter(CIFAR100-Coarse, unbalanced subclasses).
Figure 4: Left: End model performance training with Lspread on various datasets compared againstcontrastive baselines. All metrics are accuracy except for ISIC (AUROC). Lspread produces the bestperformance in 7 out of 9 cases, and matches the best performance in 1 case. Right: Performanceof coarse-to-fine transfer on various datasets compared against contrastive baselines. In these tasks,we first train a model on coarse task labels, then freeze the representation and train a model onfine-grained subclass labels. Lspread produces embeddings that transfer better across all datasets.
Figure 5: Left: Unsupervised strata recovery performance (top, F1), and worst-group performance(AUROC for ISIC, Acc for others) using recovered strata. Center: Performance of models undervarious amounts of label noise for the contrastive loss head. Right: Performance of a ResNet18trained with coresets of various sizes.
Figure 6: Performance of training ViT with Lspread compared to training with LSC and LSS onCIFAR10 at various amounts of labeled data. Lspread outperforms the baselines at each point. Thecross entropy head here is trained with 1% labeled data to isolate the effect of training data on thecontrastive losses.
Figure 7: Performance of models under various amounts of label noise for the contrastive loss head,and various amounts of clean training data for the cross entropy loss.
