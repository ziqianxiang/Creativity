Figure 1: The Ensemble Memory Architecturematching. Finally, ajudiCioUs choice of activation and loss functions further mitigates forgetting byconfining updates to weights that affect the class of the current sample.
Figure 2: Task-wise accuracies over training for 5-way split MNIST3.1 Baselines and AblationsFor a more qualitative appreciation of the ensemble model in action, see Fig. 2, bottom right, andFig. 3, bottom right, which show the evolution of overall accuracy on all ten classes on the test setfor 5-way split MNIST and 5-way split CIFAR-10, respectively. The model’s progress is steadilyupwards, and where there are discrete changes in distribution (new tasks) 一 in the 5-way and 10-way splits - their effects are clearly visible. These figures also show the performance of two non-ensemble baselines models, one in which the output of the pre-trained encoder is delivered to a singlevanilla classifier with a conventional softmax applied, and a similar model with a single t-classifierFigure 3: Task-wise accuracies over training for 5-way split CIFAR-106Under review as a conference paper at ICLR 2022(with a tanh activation function and no softmax). (See Table 1 for numerical final accuracies.) Forthe easier MNIST dataset (Fig. 2), forgetting is non-catastrophic even with the vanilla classifier,and is further reduced with the stand-alone tanh classifier, although neither model matches theperformance of the ensemble. This suggests that each of the three elements of the architecture -the pre-trained encoder, the activation / loss function combination, and the ensemble - can help toreduce catastrophic forgetting for a simple enough dataset. However, with the harder CIFAR-10dataset (Fig.3), catastrophic forgetting is mitigated only with the full ensemble model, incorporatingall three of these features.
Figure 3: Task-wise accuracies over training for 5-way split CIFAR-106Under review as a conference paper at ICLR 2022(with a tanh activation function and no softmax). (See Table 1 for numerical final accuracies.) Forthe easier MNIST dataset (Fig. 2), forgetting is non-catastrophic even with the vanilla classifier,and is further reduced with the stand-alone tanh classifier, although neither model matches theperformance of the ensemble. This suggests that each of the three elements of the architecture -the pre-trained encoder, the activation / loss function combination, and the ensemble - can help toreduce catastrophic forgetting for a simple enough dataset. However, with the harder CIFAR-10dataset (Fig.3), catastrophic forgetting is mitigated only with the full ensemble model, incorporatingall three of these features.
Figure A4: Visualisation of a representative training run for 5-way split MNIST (low data regime).
Figure A5: tSNE Plots for image encodings14Under review as a conference paper at ICLR 2022Table A3: HyperparametersParameter	ValueLearning rate	0.0001Weight decay	0.0001Ensemble size	1024k (top-k selection)	32τ (tanh scaling factor)	250Runs per experiment	20ablations, these and all other hyperparameters, were used for all experiments described in the paper(Table A3). We found performance to be robust to hyperparameter variation, and we did not have totune them for different datasets or experimental settings.
Figure A6: Gaussian scheduletraining set. The total number of images seen is 10 × 100 × 60 = 60000, which is equivalent to asingle pass through the data (one epoch).
