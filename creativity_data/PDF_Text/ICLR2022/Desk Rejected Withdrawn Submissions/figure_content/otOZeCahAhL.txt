Figure 1: Estimates of mutual information between dimension-wise statistics of intermediate hid-den feature and the domain label, I(s, d) (top), or the class label, I(s, y) (bottom). We use log-Melspectrogram (left) and MFCC (middle) on the audio scene classification task and use raw RGBimage (right) for the PACS dataset (Average over 5 seeds; error bar stands for standard deviation).
Figure 2: 2D t-SNE visualiations using activation ofstage 1 in BC-ResNet-1.
Figure 3: Left: Estimates of mutual information, I(s(F) , d) where s(F) and d are frequency-wise statistics of intermediate hidden feature and audio-device (domain) label, respectively. Middle:Train and validation accuracy curves. Right: Validation accuracies for seen and unseen domains.
Figure 4: Relative results as λ changes compared to λ = 1 with various architectures (in ASC) orwith varying number of train domains (in KWS) (Average over 5 seeds; error bar stands for std).
Figure A1: Top-1 Validation Error (%) on TAU Urban AcousticScenes 2020, development datasetwith various tactics to relax IFN (Average over 5 seeds).
Figure A2: Experiment BC-ResNet-1. Left: Train and validation accuracy. Right: Validation accu-racies of seen and unseen devices. (Average over 5 seeds).
Figure A3: Top-1 test accuracy (%) of BC-ResNet-8 on TAU Urban Acoustic Scenes 2020 Mobile,development dataset. Left shows graphs with varying number of training domains 2, 4 and 6, andRight compares RFN with Relaxed-IN. (Average over 5 seeds; error bar stands for std)Table A5: Keyword Spotting. Compare PCEN to others using log-Mel spectrogram insteadof MFCCs. Top-1 test accuracy (%) with varying number of training speakers on Google speechcommand dataset ver1. (average and standard deviation; averaged over 5 seeds)Method	I	50 I	100	I 200	I 1000cnn-trad-fpool3	75.2 ± 0.3	79.8 ± 0.4	86.0 ± 0.3	92.0 ± 0.4+ PCEN	81.6 ± 0.4	85.6 ± 0.5	89.5 ± 0.2	94.0 ± 0.1+ RFN(OurS) ∣	82.6 ± 0.3 I	86.3 ± 0.5	I 90.8 ± 0.2	I 94.2 ± 0.2stage 3, and stage4, respectively. The linear interpolated λ results in 0.9% lower validation accuracycompared to that of fixed λ = 0.5. Second, we try a naive SGD update of λ, but it results in poorperformance. Further, we tried the Meta-Learning Domain Generalization approach (MLDG) (Liet al., 2018). We use the same optimizer and learning rate schedule as the baseline and use α = γand β = 1 for MLDG and use the first-order approximation of MAML (Finn et al., 2017). Wetrained the network parameters by conventional SGD training and updated λ by meta-test loss inthe MLDG scenario. The approach got λ of [0.7, 0.5, 0.3, 0.5, 0.1] for input and after each stage,respectively, on average over five seeds and results in 72.2% accuracy, which is better than naiveSGD but still worse than the fixed λ = 0.5. We leave the automatic update of λ as future work.
Figure A4: Estimates of mutual information between dimension-wise statistics of intermediatehidden feature and the domain label, I(s, d) (left), or the class label, I(s, y) (right). We use log-Melspectrogram (Average over 5 seeds; error bar stands for standard deviation).
Figure A5: 2D t-SNE visualiations using activations of Vanilla BC-ResNet-1.
