Figure 1: Imputation results with attention weights*>λI∕->λI∕>sλI∕∙~aI^ A/^-z1∕s-λFigure 2: Bottlenecked Dilated Convolution Module. In our model, we have 4 layers with dilatedfactors of 1,2,2,4We can now visualize the attention weights in Fig. 1a) learned and see that all of the other keypositions corresponding to the query’s wave position have an increased attention weights, clearlyreflecting the quasi-periodicity of the signal.
Figure 2: Bottlenecked Dilated Convolution Module. In our model, we have 4 layers with dilatedfactors of 1,2,2,4We can now visualize the attention weights in Fig. 1a) learned and see that all of the other keypositions corresponding to the query’s wave position have an increased attention weights, clearlyreflecting the quasi-periodicity of the signal.
Figure 3: ViSualization of Univariate Imputation ReSultS with InCreaSing Chunk SizeOur BDC TranSformer outperformS the other baSelineS on moSt of the imputation taSkS deSignedaround the mHealth miSSing data paradigm. DeSpite thiS, aS we Can See in in Fig. 3 and for theperCentage of R peakS reConStruCted, there iS room for improvement in the imputation reSultS in theunivariate Setting. MatriX faCtorization doeS very well in the multivariate imputation problemS, ableto eXploit the CroSS Channel CorrelationS, but it unfortunately Cannot be uSed in the univariate CaSe, aS8Under review as a conference paper at ICLR 2022the data matrix starts out being rank 1. Vanilla transformer ends up doing consistently better than theconvolutional attention transformer models, likely because of the greatly increased dimensionalityfrom the convolutional attention.
