Figure 1: An example on 2:4 sparsity. Twoweights are pruned for each row of four weights.
Figure 2: A single pruning round of mIMP with 2:4 sparsity. The step numbers are shown in circles.
Figure 3: Average hamming distance between every pair of binary masks for unstructured and 2:8structured sparsity on VGG-16 and ResNet-20.
Figure 5: (a) Proactive local pruning (PLP) can be applied in addition to losing tickets pruning (LTP)for further reducing the weights in the losing ticket pool. (b) The steps of L-mIMP.
Figure 4: Misprediction rate by PLP after eachpruning round on CIFAR-10 (left) and ImageNet(right).
Figure 6: The performance of the EB tickets drawn from different iterations with 2:4 and 2:8 sparsityover VGG-16, ResNet-20 and ResNet-56. The vertical bars show the 95% confidence interval. Thestar signs indicate the accuracies acquired by training the DNNs with full I iterations.
Figure 7: Accuracy comparison between L-mIMP and the early pruning methods. All the earlypruning methods have a training FLOPs of 0.19E(1018), 0.33E and 1.32E on ReSNet-18, ResNet-50and vGG-16, respectively. The training FLOPs of L-mIMP are shown in Table 3.
