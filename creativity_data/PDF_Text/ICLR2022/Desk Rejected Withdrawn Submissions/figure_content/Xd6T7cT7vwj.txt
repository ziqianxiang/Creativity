Figure 1: Log plot of Pk | kxknk - 1| where Xk is the pre activations over 400 layers, averaged overten runs, as a function of the network width. Gold: ,2/(1 + e-x), Black: √2sin(x + n/4), Red:GP normalized tanh, Turquoise: GP normalized GELU, Blue: GP normalized ELU, Purple: GPnormalized leaky ReLU, Green: GP normalized ReLU.
Figure 2: | 100 √=1	- 1| for k=1,..,7 where x『 are the pre-activations for layer k duringforward propagation of x(0l) and x(0l) is a random normal vector transformed by normalizationto have norm 空 where n is the dimension. Colour corresponds to activation function. Gold:,2/(1 + e-x), Black: √2sin(x + n/4), Red: GP normalized tanh, Turquoise: GP normalizedGELU, Blue: GP normalized ELU, Purple: GP normalized leaky ReLU, Green: GP normalizedReLU.
Figure 3: Early frame during fitting. Left: Using a scaled encoder of the Mildenhall et al. type,Right: Using the modified encoder. The image being fitted is a scaled-down 256 × 256 version ofthe Cameraman test image.
