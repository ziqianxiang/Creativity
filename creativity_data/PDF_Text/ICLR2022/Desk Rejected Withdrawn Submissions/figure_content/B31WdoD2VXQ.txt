Figure 1: The overall pipeline of the SLT-Net. The SLT-Net consists of a short-term detection mod-ule and a long-term refinement module. The short-term detection module takes a pair of consecutiveframes and predicts the concealed object mask for the reference frame. The long-term refinementmodule takes T predictions from the short-term detection module along with their correspondingreferenced frames to generate the final predictions.
Figure 2: The overview of our short-term network pipeline. The network first extracts features from the inputframes by a transformer encoder, then computes a full-range volumetric correspondence between the referenceframe It and its neighboring frame It+1 to form a correlation volume pyramid. A CNN decoder is used topredict the final prediction from the motions captured by the short-term correlation pyramid.
Figure 4:	The overview of the proposed long-term consistency architecture. It formulates the processas a seq-to-seq problem and refines the pair-wise predictions with a spatial-temporal transformer.
Figure 3: Correlation aggregation block (CAB) com-putes the normalized correlation volume of featuremaps between the reference frame (green blocks) andthe target frame (yellow blocks).
Figure 5:	Illustration of forward-backward consistency check. After bi-directional check, undesir-able ghosting artifacts, i.e., the nose (red box) of the elephant in forward direction and the tail (bluebox) in backward direction, and occlusions can be effectively removed.
Figure 6: Qualitative results on our MoCA-Mask benchmark.
Figure 7: Representative samples from MoCA-Mask. The dataset is quite challenging includingdiverse scenes, suash as various lighting conditions, i.e., dark and sunny, complex background,camera motions, small ratio of animals and tiny body structures, such as slim torso /limbs.
Figure 8: Summary for training and test set distribution. Our MoCA-Mask dataset includes 87 videosequences in total, in which 16 sequences were tagged as “unknow” (colored in orange). This splitis used to validate the sensitivity of different models on novel samples. Zoom-in for details.
Figure 9: Comparison of our proposed network with two top-performing baselines on MoCA-Masktest dataset. Example squences of each row means: (a) (f) Frames, (b) (g) GT, (c) (h) SINet (Fanet al., 2020a), (d) (i) RCRNet (Yan et al., 2019), (e) (j) SLT-Net (Ours).
