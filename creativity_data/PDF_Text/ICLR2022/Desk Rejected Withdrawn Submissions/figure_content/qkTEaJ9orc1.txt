Figure 1: The molecules found by Zhavoronkov et al. (2019) and its training molecules. Zhavoronkov et al.
Figure 2: The training process of MOG. The energy network and the property network receive both positivesamples and hallucinated samples. Molecular graphs of a dataset (Xmol, Amol) are processed to positive samples(X㊉,A㊉)by the dequantization, while K steps of LangeVin dynamics turn random noises (X0,A0) intohallucinated samples (XK, AK) = (X , A). In this figure,one molecule of QM9 dataset, in which n = 9,a = 4 (C, N, O, F), and b = 3 (single, double, and triple bonds), is used as an example. ? is the Virtual atomtype. The black arrows indicate the conVeyance of information during the training process, while the greenarrow indicates the pre-definition or pre-conVersion before the process. The dashed arrow indicates the transferof gradient.
Figure 3: OOD generation ona 2D example. The training dis-tribution consists of eight isotropicGaussian blobs, and the samplesare indicated by red. The gener-ated OOD samples with a certainenergy pivot are indicated by thecorresponding color.
Figure 4: Molecules in training set and molecules of maximum Tanimoto similarity generated by MARSand MARS+OOD. MARS+OOD indicates MARS augmented with the EBM energy score. For each row, wedisplay (a) a known active molecule against GSK3β and JNK3 included in the training set, (b) a moleculethat has maximum Tanimoto similarity with (a) among the generated molecules by MARS, and (c) a moleculethat has maximum Tanimoto similarity with (a) among the generated molecules by MARS+OOD. The numberbelow each molecule in (b) and (c) is the corresponding Tanimoto similarity compared to (a). The duplicatedstructures in (a) and (b) are indicated by green areas.
