Figure 1: MORE approximates a function (middle) with a quadratic model locally to a given Gaus-sian search distribution (in this case isotropic, indicated by a black circle), using regression based onsamples from this distribution (red markers). The left plot shows an approximation that not considerthe gradients at the sample locations. Our method learns an approximation (right) that also respectsthe gradients at these locations. Trust-region updates of the search distribution, based on the respec-tive surrogates, are visualized as red and green ellipses. Here, the green update, resulting from oursurrogate, moves towards the optimum and better adapts the covariance towards the local curvature.
Figure 2: The top row (shaded in blue) shows the parameter vector θ> of the quadratic surrogatefor N = 4. The lower matrix (shaded in red) shows the non-zero elements of the gradient-featurematrix Φg (x), such that the matrix product Φg(x)θ corresponds to the gradient of R(x) (Eq. 2).
Figure 3: In our GVA experiments, using the vanilla gradient, computed with the log-derivativeor reparameterization trick, can learn good approximations but is very sample inefficient. MORE,gMORE and VON achieve similar approximation quality, but high-order methods are more sampleefficient. VOGN is fast, but its approximation quality suffers from the bias of the GauB-Newtonapproximation, which is amplified when applying it to the target log density directy (GM).
Figure 4: The result when learning GMMs are similar to the GVA experiment. However, the bias ofVOGN has a stronger effect, and gMORE is able to catch up with VON during the optimization.
Figure 5:	From left to right: BreastCancer, GermanCredit, GMM(20D), Planar Robot (1 goal),Planar Robot (4 goals)waste huge amounts of energy, or increase inbalances in power and wealth. They can cause seriousharm—even fatal accidents—if we overestimate their capabilities, and they can be used maliciously,for example, to forge data or carry out cyberattacks.
Figure 6:	The different approaches to estimate the natural gradient perform remarkably similar whenevaluated with respect to wallclock time. However, we argue that this does not justify the conclusionthat it does not matter whether zero-order, first-order or second-order methods are applied, sincewe only investigated problems where the target distribution (and its gradient and Hessian) can beevaluated efficiently, which is in general not the case.
Figure 7: The left two plots show 200 ground-truth samples for both planar robot experiments.
Figure 8: Visualization of learned means and weights in the planar robot one goal and four goalsexperiments. From left to right the plots are arranged as GM (left), gMORE (middle) and vonVIPS(right). The number of components differs from each other due to explore ability. Grey box indi-cates the robot base and red circle indicates the position of the end-effector. Each line represents acomponent, components with larger weight are drawn darker.
