Figure 1: Dataset Condensation with Distribution Matching. We randomly sample real and synthetic data, andthen embed them with the randomly sampled deep neural networks. We learn the synthetic data by minimizingthe distribution discrepancy between real and synthetic data in these sampled embedding spaces.
Figure 2: Visualization of generated 10 images per class synthetic sets of MNIST and CIFAR10 datasets.
Figure 3: Distributions of synthetic images learned by DC, DSA and DM. The red, green and blue points arethe real images of first three classes in CIFAR10. The stars are corresponding learned synthetic images.
Figure 4: Training time comparison to DSA whenlearning 50 img/cls synthetic sets on CIFAR10.
Figure 5: Performance of learning larger syntheticsets on CIFAR10.
Figure 6: 5-step class-incremental learning.
Figure 7: 10-step class-incremental learning.
Figure F8: Performance rank correlation between proxy-set and whole-dataset training.
Figure F9: Synthetic images of CIFAR10 dataset learned with different network parameter distributions, i.e.
