Figure 1: Illustration of the idea behind the REFINER module.
Figure 2: Schematic of the Refiner Network applied to (left) MMBT architecture and (right) VIL-BERT architecture. Decoders are added for each uni-modal feature to compute the Refiner loss.
Figure 3: Examples from the (left) Hateful Memes dataset showing two not hateful (top) and twohateful (bottom) memes from the Hateful Memes dataset, and (right) two examples from the multi-modal IMDB dataset with the genre that is to be predicted. There is also a corresponding textdescription for each input which is not shown, and each input may have multiple labels associated.
Figure 4: Relative change in accuracy as we increase the weight of the REFINER reconstructionloss, showcasing that no reconstruction loss (baseline) does not fully leverage information of thefusion module.
Figure 5: Visualization of fusion features in reduced dimensions using T-SNE. Left: fusion fea-tures of ViLBERT baseline showing the 3 clusters with entanglements. Center: fusion features ofReFNet showing the 3 clusters are better separated with less entanglements. Right: fusion featuresof RefNetMS showing the Refiner and Contrastive loss inducing six clusters across modalities andclasses (three clusters for each of the vision and text modalities). The colors red, blue, and greenrepresent three classes contradiction, entailment, and neutral.
Figure 6: Schematic of the proposed algorithm with a refiner and contrastive loss module. Theimage based features F1, F2, F3, F4 and text based features F5, F6 are fused together, and a refineris applied on the fused embedding to generate refiner outputs R1, R2,…，R6 which are used todefine a self-supervised loss function and a supervised Multi-similarity contrastive loss is also usedacross samples in a batch.
Figure 7: Comparison of performance of the baseline ViLBERT model, ReFNet and ReFNetMSacross datasets with 5%, 10% and 20% of labeled training data available. Figure on the left showsmicro and macro f1 scores on the validation dataset and that on the right shows the scores on the testdataset.
Figure 8: Plot of the reconstruction loss of the adjacency matrix against the dimension of the fusedembedding space for different values of d for (top) m=64, (middle) m=128 and (bottom) m=512.
Figure 9: Comparison of eigen-vectors (sorted for visualization purposes) of the ground truth anddifferent dimensions of fusion embeddings, demonstrating that when k=m, Linear Fusion Modulescan adequately reconstruct the ground truth.
Figure 10: Convergence of ReFNet and ReFNetMS losses across three different datasets (from left -10%, 20% and 100% of the training data.
