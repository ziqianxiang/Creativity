Figure 1: Our ORViT model introduces class-agnostic object-centric information into the transformer self-attention operation. The figure shows the standard (uniformly spaced) patch-tokens in blue, and object-regionscorresponding to class-agnostic detections in orange. In ORViT any temporal patch-token (e.g., the patch inblack at time T) attends to all patch tokens (blue) and region tokens (orange). This allows the new patchrepresentation to be informed by the objects.
Figure 2: Left: An ORViT block. The input patch-tokens X and boxes B are used as input to the “Object-Region Attention” and “Object-Dynamics Module” components. Each component outputs a T HW × d tensorand the two tensors are summed to obtain new patch tokens Y . Right: We visualize the attention allocatedto the object tokens in the ORViT block (red, green, and blue) in each frame for a video describing “movingtwo objects away from each other”. It can be seen that each phone object affects the patch-tokens in its region,whereas the hand has a broader map. For more visualizations, please see section E in supplementary.
Figure 3: ORViT Block architecture. The block consists of two object-level streams: an “Object-RegionAttention” that models appearance, and an “Object-Dynamics Module” that models trajectories. The two arecombined to produce new patch tokens. The “Box Position Encoder” maps the output of the trajectory streamto the dimensional of patch tokens.
Figure 4: Evaluation on AVA.
Figure 5: Attention Maps comparison between the ORViT+Mformer and the Mformer on videos from theSSv2 dataset. The visualization shows the attention maps corresponding to the CLS query.
