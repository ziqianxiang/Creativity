Figure 1: (a-c) Intuitive understanding of Controllable Interpolation Regularization (CIR).(a) Onlyencourage controllable disentangled representation with general Mutual Information (MI) constrainmethod: maximize the MI between the same attribute across latent and observation domains whileminimizing the MI between the different attribute across latent and observation domains. (b) Onlyencourage convexity with interpolation and image quality evaluation. (c) A simple yet efficientmethod, CIR, encourages both controllable disentanglement and convexity in latent representation.
Figure 2: (a) Directly use Controllable Interpolation in GZS-Net (b) Architecture of GZS-Net + CIR(c) Convexity optimization with Linear Interpolation (middle) and Boundary Random Interpolation.
Figure 3: ELEGANT + CIR performance of (a) task 1 for two images face attribute transfer and (b)task 2 for face image generation by exemplars. (More results in Appendix Sec. B.1)The overall generative loss of ELEGANT + CIR is:L(G) = Lreconstruction+ Ladv + λCIR Lreg(5)where Lreconstruction and LadV are ELEGANT original loss terms, λCIR > 0 control the relativeimportance of the loss terms. we keep the discriminative loss. (More details in Appendix Sec. A.1)Fig. 3(a) shows the task 1 performance on two images face attribute transfer. Take Eyeglassesas an example attribute to swap: C and D should keep all other attributes unmodified except forEyeglasses. ELEGANT generated C and D have artifacts in Eyeglasses-unrelated regions, whichmeans ELEGANT cannot disentangle well in latent space. After adding CIR, the generated C and Dbetter preserve the irrelevant regions during face attribute transfer, which demonstrates that CIR helpsencourage a more convex and disentangled latent space. The Eyebrow and Beard attribute results alsoshow the improvement from CIR. Fig. 3(b) shows the task 2 performance on face image generationby exemplars. Similarly, ELEGANT generated new images with artifacts in Eyeglasses-unrelatedregions that cannot disentangle well. Synthesis is also inferior in the glasses region, which we posit isdue to non-convexity in the eyeglass-related latent space. With the help of CIR, the generated imagesimprove both Eyeglass quality and irrelevant region preservation.
Figure 4: (a) I2I-Dis + CIR performance of diverse image-to-image translation; (b) GZS-Net + CIRperformance of interpolation-based attribute controllable synthesis⑵-DisIOntent ProviderBackground(b)Fig. 4 (a) shows the image-to-image translation performance. (a-1) We fix the identity (domain)latent code and change the content latent code by interpolation; generated images should keep thedomain attribute (belong to the same dog). I2I-Dis generated dog images have artifacts, which meansthe non-convex latent space cannot ’understand’ the interpolated content code. After adding CIR,the generated images have both better image quality and consistency of the same identity. (a-2) Wefix the content latent code and change the identity by sampling; generated images should keep thesame content attribute (pose and outline). Cat images generated by I2I-Dis have large pose variance(contain both left and right pose), and large face outline variance (ear positions/sizes). After addingCIR, the generated images have smaller pose and outline variance. (More results in Appen. Sec. B.2)4.3	CIR + GZS-NET (GE ET AL., 2020A) FOR ZERO-SHOT SYNTHESISWe use the same architecture of autoencoders as GZS-Net (Ge et al., 2020a) and Fonts dataset(Ge et al., 2020a). The latent feature after encoder E is a 100-dim vector, and each of the fiveFonts attributes (content, size, font color, background color, font) covers 20-dim. The decoder D ,symmetric to E, takes the 100-dim vector as input and outputs a synthesized sample. We use the
Figure 5: (a) c-Dis-RL analysis. Diagonals are bolded. (b) c-Dis-RL Evaluation by Correlation Coef-ficient. Intra-attribute correlation increases with CIR (GZS-Net (top): 7.2%, ELEGANT (bottom):3.2%) while inter-attribute decreases (GZS-Net: 60.9%, ELEGANT: 3.1%).
Figure 6: (a) Bias elimination experiment results. (b) The influence of bias shown by Grad-Cam.
Figure 7: More examples of ELEGANT+CIR (E+CIR) performance of task 1 for two images faceattribute transferFigure 8: ELEGANT + CIR Performance of task 2 for face image generation by exemplars14Under review as a conference paper at ICLR 2022Figure 9: I2I-Dis + CIR performance of diverse image-to-image translationFont color/ P RRRRRKKkkk.
Figure 8: ELEGANT + CIR Performance of task 2 for face image generation by exemplars14Under review as a conference paper at ICLR 2022Figure 9: I2I-Dis + CIR performance of diverse image-to-image translationFont color/ P RRRRRKKkkk.
Figure 9: I2I-Dis + CIR performance of diverse image-to-image translationFont color/ P RRRRRKKkkk.
Figure 10: More results of GZS-Net + CIR performance of interpolation-based attribute controllablesynthesis15Under review as a conference paper at ICLR 2022BRI first collects an image set S (contains s > 2 images) and obtains the corresponding latentcode Sz = {z(i)}is=1. Then it calculates the maximum and minimum values as the upper and lowerbounds of each dimension. After that, we randomly sample k latent codes in the region created bythe boundary. Fig. 2 (c) shows the situation in 2D space. BRI uses ’small subspaces’ to cover ’bigsubspaces’.
Figure 11: Controllable mining novel background and font color by interpolation in latent space.
Figure 13: Towards controllable exploration directionHere we explore the distribution of disentangled representation and mining the relationship betweenmovement in high dimension x space and low dimension z space to answer the question: Whichdirection of movement can help us to find new attributes?For each background color, we train a binary color classifier to label interpolated points in the z spaceand assign a color score for each of them, then we use SVM to find the boundary and obtain UDV for17Under review as a conference paper at ICLR 2022this attribute value. Since the UDV is the most effective direction to change the semantic score ofsamples, if we move the z value of the given image towards UDV, its related semantic score wouldincrease fast. To explore more new attributes, the combination of UDVs may be a good choice. Forinstance, if the given picture is green, the new colors may fall in the path from green to blue and thepath from green to red. Thus, it is reasonable to set our move direction as v = vblue + vred - vgreen(v represents UDV). The 1st row of Fig. 12 shows the results of changing z value with the combinevector vblue + vred - vgreen . On the contrast, the 2nd row only use vblue and the 3rd row only usevred. We can find that both the 2nd and the 3rd row only find one color while the 1st row finds more.
