Figure 1: Towards efficient vision transformer models. Starting form ViT, specifically DEIT, weidentify the design space of pruning (i) embedding size E, (ii) number of head H, (iii) query/key sizeQK, (iv) value size V and (v) MLP hidden dimension M in Section 3.1. Then we utilize a globalranking of importance score to perform iterative global structural pruning in Section 3.2. Finally wederive a simple NViT architecture from observing the dimension trend of all the components in thepruned model, as in Section 4.1.
Figure 2: Attention block reshaping for latency-friendly structural pruning. We reshapedthe QKV projection and final output projection in the attention block to explicitly controlthe number of head and align the QK & V dimensions in each head head.
Figure 3: Illustration of 2:4 Ampere sparsity. In every 4 consecutive weight elements thetwo with the smallest magnitude are set to zero (denoted as blank spaces on the right).
Figure 4: Model dimension comparison between NViT (green), DEIT (blue) and prunedNVP model (grey). Compared to DEIT, the pruned model has more parameters in blockstowards the middle and in MLP; but fewer parameters in blocks towards the two ends andin MSA (H and QK). We follow these insights closely in the design of our NViT models.
Figure 5: Estimated latency from the lookup table vs. evaluated latency on V100 GPU withbatch size 256. Reduction ratio computed with respect to the latency of the full model.
Figure 6: Comparing the parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff of different pruning schemes. Latency estimated on RTX 2080 GPU.
Figure 7: Pair-wise cosine distance between all headsâ€™ attention score in each transformerblock. Blue indicates a smaller distance while yellow indicates a larger one. The dark blueblocks in NVP-B figures corresponds to the heads being pruned away, which have all-zeroattention scores thus zero cosine distance in between.
