Figure 1: An example of visual dialog task. Red and blue highlights show two context-relevantquestions, and green highlights denote the context-free questions. This example demonstrates thatthe questioner likely dives into a different topic in a conversation.
Figure 2: (a) Illustration for decomposition of a question x based on maximization and minimiza-tion of mutual information and (b) Illustration for clustering phenomenon in latent space of stylerepresentation where red, blue, and green clusters denote different types of questions, and dashedarrows indicate the direction of regularization. In an ideal learning process, if the same type ofquestions is clustered in the latent space, it may contain unknown disturbing factors. An informativeregularization is helpful to alleviate such a disturbance.
Figure 3: Latent space visualization for different constraints of style representation, where thered, blue and green points respectively represent the “Yes/No” questions, “Number” questions and“Other” questions.
Figure 4: Qualitative results on the visual dialog task. Visualization of the attention map with thedifferent algorithm. The red (soft attention) considers all components, the blue (discrete variationalattention) identifies whether it is related to the question. The darker the color indicates that themodel determines that it is more important to the current question.
Figure 5: Concept of the Central Limit Theorem.
Figure 6: The overall model architecture of the visual dialog system.
Figure 7:	Qualitative results of language priors in VQA. In the figure, it can be seen that the ad-justment of γ can transform the response originally affected by language priors into a response thatcorrectly matches the image content.
Figure 8:	Error analysis on the proposed method.
