Figure 1: Example panoptic segmentation results on images from the COCO dataset (Lin et al.,2014). The baseline system has incorrectly merged several of the foreground objects (center). Incontrast, our system was able to distinguish those cases through its compositional approach (right).
Figure 2: The high-level architecture of our panoptic segmentation system, which is adapted fromUPSNet (Xiong et al., 2019). Within the instance head at the right, our design replaces the instancedetection pipeline from Mask R-CNN (He et al., 2017) with anew element based on a compositionalmodel (indicated by New in the diagram).
Figure 3: The new detect branch of the instance head. Feature vectors at each position in the k Ã— ksized RoI lattice (shown in orange) are processed using a compositional model to compute a map ofposterior values. The posterior values are then used to produce final class score predictions.
Figure 4: Qualitative results on examples from the COCO Val dataset.
Figure 5: Patterns that map to the same component while detecting instances of four categories areshown. As seen in the figure, similar regions across instances map to the same component of thecompositional model.
Figure 6: Parts that map to two components that are active for the Vehicle supercategory. The firstfive columns show parts of wheels taken from instances that were classified as Car, Motorcycle, Busand Truck, with all parts having the same most active component. Similarly, the last five columnsshow parts of Bicycle and Motorcycle instances.
Figure 7: Additional results using the ResNet-50 FPN baCkbone on the COCO Val dataset. Left toright, the Columns show inPut images, ground-truth annotations, baseline results using UPSNet, andour results using a ComPositional model. In the toP row, notiCe that our system has CorreCtly deteCteda Person who is severely oCCluded by the batter, at the left side of the batter, while also deteCtinga signifiCantly higher number of sPeCtator instanCes, even though some of those instanCes havenot been annotated individually in the ground truth. In the seCond row, oCCluded Person instanCesPresent in the baCkground have also been deteCted with high PreCision. In the remaining rows, oursystem has Performed better than the baseline for many of the foreground objeCts.
Figure 8: Representative results of our approach using ResNet-101 on the COCO Val dataset. Allof these cases represent complex scenes involving occlusion. Our system has performed better thanthe baseline in each case.
Figure 9: More representative results of our approach using the ResNet-101 backbone. Rows 1,3 and 5 contain examples of crowded scenes where our approach shows an improvement over thebaseline. It is interesting to note that our approach detects and segments many instances that arenot present in the ground truth labels. The example in row 2 is particularly difficult, and yet it issegmented well by our approach despite having an unusual perspective with a partially visible zebrainstance occluding another instance of the same class.
Figure 10: Results on the Cityscapes dataset using the ResNet-50 FPN backbone. As seen in the figure, our approach is able to recognize many occluded instancesbetter than the baseline.
Figure 11: Examples from the COCO dataset that show points discussed in section B. The greenboxes indicate instances that were detected by our system and were flagged as False Positives.
Figure 12: (left) Example image from the COCO dataset with ground-truth RoIs annotated ingreen. (middle) RoIs extracted from the image. (right) RoIs with ground-truth masks superimposed.
