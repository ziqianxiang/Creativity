Figure 1: An overview of the full model framework. Our architecture is based on Transformerand Fourier transform. Following the sequence embedding, we apply a 2D discrete Fourier trans-form (particularly Fast Fourier transform) to convert the TS features from the time domain to thefrequency domain, a 2D inverse discrete Fourier transform to map the features back to the time do-main, and a multi-head self-attention layer. Then we employ a Global Average Pooling (GAP) layerto average the output of the MTS over the entire time dimension. Finally, a Softmax layer is usedfor the multi-class MTS classification task.
Figure 2: (a) represents the average testing accuracy loss across all datasets while removing onemodule at a time and other modules remain in the network. Modules MHA, FFT, and IFFT bringabout larger influence on the predictive performance due to the high percentage of accuracy losswhen removing them. In comparison, BN, EMBED, and ACT bring about marginal influence onthe predictive performance compared with other modules. (b) represents the corresponding averageefficiency improvement across all datasets when one module is removed from the network whileother modules keep intact.
Figure 3:	Module-wise results for changes in terms of number of parameters and training time perepoch on four datasets: EC, NATO, FM, SRS1.
Figure 4:	Change in accuracy (%) from module-by-module pruning across all datasets. The order ofdatasets shown from (a) to (c) correspond to Table 1.
Figure 5: Trade-off between network efficiency and complexity across all datasets. Due to thenotable differences of dataset sizes, the computation of efficiency is normalized for each dataset.
Figure 6: Pareto efficiency visualization of the FingerMovements and Heartbeat datasets. The scat-tered cyan points, the marked red points, and the blue curve represent randomly sampled experimen-tal data, Pareto-efficient solutions, and Pareto efficient frontiers.
