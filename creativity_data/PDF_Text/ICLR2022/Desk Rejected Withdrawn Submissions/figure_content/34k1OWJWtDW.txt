Figure 1: Visualization of semantic data augmentation. (a) Example of the long-tail distribution,where the distributions of the two head classes are shown in blue and green, respectively, whilethe distribution of the tail class is shown in pink. Solid line denotes the original distribution. (b)Class-level augmentation. Since the same transformation is applied to all samples indiscriminately,the adjusted distribution contains illegal augmentations. Dotted line denotes the adjusted distribu-tion. (c) The proposed Sample-specific augmentation. The semantic transformation is different fordifferent sample. The adjusted distribution better recovers the real distribution of the tail class.
Figure 2: (a) The learning process of the proposed STG. (b) The estimation of μ, Σ. The Yellow dotindicates the anchor feature, blue lines denote samples of semantic transformation and we calculateμ, Σ by estimating from such transformations. Every feature has its own specific semantic transfor-mations. (c) The memory-based dictionary. We push and pop features to refresh the dictionary.
Figure 3: The T-SNE (Van der Maaten & Hinton, 2008)visualizations of training samples on CIFAR-10-LT withIM=100. (a) Feature distribution of training samples.
Figure 4: The T-SNE visualizations of different strategies and their decision boundaries on CIFAR-10-LT with IM=100. (a) The truth boundary of distributions on the validation set. Samples from thesame class are shown in the same color. (b) The decision boundary by using the Cross Entropy loss.
