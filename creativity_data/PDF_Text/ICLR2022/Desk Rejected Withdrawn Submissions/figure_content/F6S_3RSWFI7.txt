Figure 1:	Architecture for VMIX: ∣∙ | denotes absolute value operation, decomposing Vtot into Vi.
Figure 2:	Architecture for RMC: ∣∙ ∣ denotes absolute value operation, implementing the mono-tonicity constraint of QMIX. W denotes the non-negative mixing weights. Agent i denotes the policynetwork which can be trained end-to-end by maximizing the QtotTo study the impact of monotonicity constraint in pratical multi-agent tasks, we propose an novelend-to-end Actor-Critic method, called RMC. Specifically, we use the monotonic mixing network as acritic network, shown in Figure 2. Then, in Eq. 6, with a trained critic Qθπ estimate, the decentralizedpolicy networks πθi can then be optimized end-to-end simultaneously by maximizing Qθπ with thepolicies ∏θ. as inputs; and the Ei [H (∏θ.( ∣ Zi))]] is the Adaptive Entropy [37]. We use a noveltwo-stage approach to train the actor-critic network of RMC, as shown in Algo. 1.
Figure 3: Comparing RMC w./ and w./o. monotonicity constraint (remove absolute value operation)on SMAC and Continuous Predator-Prey.
Figure 4: Comparing VMIX with and without monotonicity constraint on SMAC.
