Figure 1: The overall architecture of Dict-BERT. The definitions of rare words are appended to theend of input text. In additional to training with masked language modeling, Dict-BERT performstwo novel self-supervised learning tasks: word-level mutual information maximization (§3.4.1) andsentence-level definition discrimination (§3.4.2). “SARS” is a negatively sampled rare word.
Figure 2: An illustration of knowledge-visible attention matrix. “Def 1” is the dic-tionary definition of the second word in theinput text, and “Def 2” is the definition ofthe third word in the input text. Coloredcircle means token i can attention informa-tion from token j , while white circle meansno attention from token i to token j .
Figure 3: Model performance on CoLA, RTE, STSB and MRPC with different variant settings.
