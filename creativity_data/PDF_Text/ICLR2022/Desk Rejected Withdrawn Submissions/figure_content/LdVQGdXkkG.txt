Figure 1: Concept Grounding in Semantic Video Prediction. After observing the scene, an agent predictsfuture frames conditioned on a series of semantic actions describing agent-object interactions. Neither boundingboxes nor key points are provided. Conditioning on different action labels leads to Counterfactual generations.
Figure 2: The pipeline of MAC in which the computation of concept slot module is elaborated (Better viewed incolor). Feature maps extracted by encoder are mapped into the concept slot tensors. Concept slot module receivesan action label that controls the collection of concept slot tensors and outputs representations encapsulating thisaction. A recurrent predictor updates representations before sending them to decoder to predict the next frame.
Figure 3: The qualitative comparison on CLEVR-Building-blocks and Sapien-Kitchen. The first row of eachfigure is the groundtruth sequence. The red, blue and green boxes highlight the quality of predictions by eachmethod. In contrast to the success of MAC, concatenation-based method fails to find the correct destinations orto preserve attributes of moving objects. Also, bounding boxes used in AG2Vid cannot portray visual changeslike rotations correctly.
Figure 4: Counterfactual video generation: Conditioning on the same initial frame and different action labels,MAC can produce high-quality imaginations of counterfactual futures. Various visual outcomes present inthe final frames are highlighted with red boxes and enlarged in the final column. Top: Generative results onCLEVR-Building-blocks. 34 frames are generated. Bottom: Generative results on Sapien-Kitchen dataset. 35frames are generated.
Figure 5: Left: Visual comparison between sMAC and SVG-LP on Tower-Creation. The supposed completionsof Pick and Put in the final frames are highlighted by red and yellow boxes while incorrect completions inSVG-LP generations are labelled by grey boxes. The last two rows are counterfactual generations in whichmodels are given different action labels. Right: Quantitative comparison per-frame. Higher SSIM and PSNRindicate better performance.
Figure 6: Compositional generalization and feature reuse.Top: Unobserved scenarios. All red cubes are removedfrom the tranining data, but the trained model can still manipulate red cube at test time. Middle: Concurrentactions. Inputting two action sequences at the same time. Both actions are depicted correctly. Bottom Left:New-object adaptation. Even with a few training samples, MAC can be fast adapted for generation of newobjects. Red arrows point to new objects present in images. Bottom Right: Object detection.
Figure 7: Visualization of 2D Object Detection on Sapien-Kitchen.
