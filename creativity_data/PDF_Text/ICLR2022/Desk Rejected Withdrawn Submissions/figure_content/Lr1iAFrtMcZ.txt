Figure 1: A visual demonstration for our intuition with an example B1,B2, B3 whose μι > μ2>μ3 andμι = μ2 + e.
Figure 2: Visualization of the Distance in Eq. 6. Y is set to 0.02 in the left figure, and 向 一 μj ∣ is set to 0.2in the right figure. It can be clearly seen that the greater the difference between the mean rewards of twobandits is, the faster that the distance is expanded from 0 to 1. The rate of convergence of d to 1 can also becontrolled by increasing Y. The curves are jagged because of the floor operation bYNic.
Figure 3: Examples of the relationship between expected cumulative reward G(N2) and exploration budget N2Determining γ with Nbargain. In Fig. 3, we visualize the relationship among Nfull, Nbargain, and G(N2 )by examples. It is fascinating to see that the optimal exploration point is located between Nbargain and Nfullbecause of the concavity of the subgaussian reward distribution. Therefore, our method can always performbetter than UCB as long as γ is set to 1/Nbargain in Eq. 6. It ensures that after exploring beyond Nbargain(red dot), the distance in Eq. 6 will become larger than 0, which makes the confidence bounds of all banditsin Alg. 1 smaller than that of UCB. By having smaller confidence bounds, our policy will under-explorecompared to UCB and stop before the Nfull (green dot)4. Thus, the final N2 will lie between Nbargain andNfull, where G(N2) > Gfull. Moreover, as shown by Eq. 19 in Appendix D, Nbargain only depends onsuboptimality gap ∆2 and time horizon T, and does not rely on the actual values of μ1,μ2. We argue thatdomain knowledge could be used to estimate the difference between optimal bandit and other ones in practiceand thereby estimate γ .
Figure 4: Regret of different policies as a function of time (log scale) in experiments from Sec. 5.
