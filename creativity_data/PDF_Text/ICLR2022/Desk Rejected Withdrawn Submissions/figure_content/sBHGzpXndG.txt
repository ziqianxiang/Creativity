Figure 1: The network architecture of the proposed COST method, which is formed by three branches oftransformers, i.e., the Video-Text, Detection-Text and Action-Text transformers. LN denotes the layer nor-malization. The cross-modality attention module is designed to align the interactions described by differentbranches of transformers.
Figure 2: Qualitative results of the state-of-the-art MART (Lei et al., 2020) method and our COST method.
Figure 3: Heatmap used to indicate the affinity matrix MH in the Video-Text transformer, where the rowdenotes the video tokens and the column denotes the action and detection tokens. The false predictions ofnouns and verbs are denoted in red font. For clarity, we only show a few epochs in the training phase.
