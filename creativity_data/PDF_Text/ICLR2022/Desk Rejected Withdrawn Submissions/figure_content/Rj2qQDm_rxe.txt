Figure 1: A) KIMERA consists of three phases: I A target transformer model is fine-tuned and ahead-mask is computed identifying the model redundancies. II The computed head-mask is thenused in conjunction with a multi-task training scheme based on knowledge graph completion tasks.
Figure A1: Attention head importance with andwithout KIMERA for the CAPR task. A) Headmask used for retraining. B) and C) presentthe head importances Ih before and after usingKIMERA, respectively. Our method results inrelatively higher and more homogeneous impor-tance of the heads.
Figure A2: Importance changes per layer for theCAPR task. A) Average importance Ih per layerbefore and after KIMERA. B) Number of re-trained heads that saw an improvement or de-crease in their importance after KIMERA. C)Number of frozen heads that saw an improvementor decrease in importance with our method. Onaverage the importance increases per layer, theretrained heads present an overall increase in im-portance. In contrast, the frozen heads are moremixed in their importance change.
