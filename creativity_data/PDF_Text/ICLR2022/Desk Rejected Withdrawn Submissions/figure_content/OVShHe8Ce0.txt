Figure 1: Approximation of Leaky ReLU (α = 0.25) using SAU. The left figure shows that SAUapproximate Leaky ReLU smoothly, and in the right figure, we plot the same functions on a largerdomain range.
Figure 2: Top-1 train and test accuracy curves(higher is better) for SAU and baseline activationfunctions on CIFAR100 dataset with ShuffleNetV2 (2.0x) model.
Figure 3: Top-1 train and test loss curves (loweris better) for SAU and baseline activation func-tions on CIFAR100 dataset with ShuffleNet V2(2.0x) model.
Figure 4: Custom designed network used for evaluation on MNIST, Fashion MNIST, and SVHN13Under review as a conference paper at ICLR 2022Activation Function	MNIST	Fashion MNIST	SVHNSAU	99.67 ± 0.04	94.40 ± 0.12	96.41 ± 0.12ReLU	99.55 ± 0.07	-93.75 ± 0.14	96.04 ± 0.12Leaky ReLU	99.59 ± 0.05	-93.89 ± 0.14	96.12 ± 0.15PReLU	99.58 ± 0.07	93.85 ± 0.16	96.12 ± 0.17ReLU6	99.59 ± 0.05	93.88 ± 0.11	96.18 ± 0.16ELU	99.51 ± 0.05	-93.82 ± 0.16	96.13 ± 0.14SoftPlUs	99.34 ± 0.12	-93.69 ± 0.19	95.88 ± 0.21PAU	99.58 ± 0.05	-94.27 ± 0.12	96.20 ± 0.15SWish	99.54 ± 0.06	-94.10 ± 0.12	96.26 ± 0.13GELU	99.60± 0.04一	94.17 ± 0.1^~	96.23 ± 0.13-Table 9: A detailed comparison between SAU activation and other baseline activations in MNIST,Fashion MNIST, and SVHN datasets for image classification problem on VGG16 architecture. Wereport top-1 test accuracy (in %) for the mean of 10 different runs. mean±std is reported in the table.
