Figure 1: The architecture of FoxInst. In the base training phase, the entire network is trainedon the training set of the base classes. In the few-shot fine-tuning stage, the backbone and maskbranch (blue) are frozen while the four prediction heads (red) are fine-tuned with few data pointssampled from the novel classes. Note that FoxInst is trained with class and box annotations as weaksupervision in the whole training and fine-tuning phases.
Figure 2: Qualitative results of FoxInst on the COCO novel classes. As shown in the figure, FoxInstpredicts the mask well with only 5-shot samples for each class without pixel-level mask annotations.
Figure 3: Qualitative comparisons between our FoxInst and the GrabCut baseline. The enlargedviews (red box) show in detail that FoxInst predicts a higher quality mask than the GrabCut baseline.
Figure 4: Qualitative results of FoxInst on the COCO novel classes.
Figure 5: Qualitative results of FoxInst on the COCO2VOC setting.
Figure 6: Qualitative results of VOC2VOC.
