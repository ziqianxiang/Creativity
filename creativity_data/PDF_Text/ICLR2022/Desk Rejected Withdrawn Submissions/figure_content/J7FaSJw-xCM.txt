Figure 1: Structure of unsupervised representation learning by maximizing mutual information as adifference of entropies. H1 and H2 are the representations used for downstream tasks. Z1 and Z2 arenormalized representations on a unit hypersphere.
Figure 2: (Comparison of SimCLR (contrastive learning), Shannon DoE (learning with Shannonentropy and cosine similarity), and von Neumann DoE (learning with von Neumann entropy andcosine similarity). As the training is carried out, the loss function,s information theoretic part isassessed. (a) For SimCLR, the mutual information is estimated with InfoNCE. (b) For Shannon DoE,the Shannon entropy part is estimated with InfoNCE self-information (INCE(Z1 ； Z1)). (C) For vonNeumann DoE, the von Neumann entropy part is directly evaluated. (a), (b), and (c) achieved 85.0%,26.6%, and 88.3% of Top-1 linear evaluation accuracy for CIFAR-10, respectively.
Figure 3: Analysis on the effect of batch sizes for empirical density operator for optimization---IOO ep—50 ep---25 ep--1-1-2-2-3-3it-Cl >U2⊃UU< Tldo-L8 16	32	64	128Batch size(c) ImageNet-108 16	32	64	128Batch size(d) ImageNet-100COCO dataset is more complex than VOC (×4 more categories, ×7 more training samples, ×3 moresample boxes per images), and it turns out that our loss works better than any other benchmark meth-ods for the complex dataset. This is an evidence that the PrinCiPled approach Can our von Neumannentropy loss has the ability to outperform other existing non-contrastive methods. Considering thatWe did not try to improve the results at the cost of tuning, We believe our method certainly providesadvantages over the existing methods. All the training details are described in Appendix C.
Figure 4: Comparison of SimCLR (contrastive learning), Shannon DoE (learning with Shannonentropy and cosine similarity), and von Neumann DoE (learning with von Neumann entropy andcosine similarity). As the training is carried out, the loss value is plotted for the three types of learning.
