Figure 1: Our modularized scheme for node drop pooling.
Figure 2: Architecture of our proposed method.
Figure 3: Score Correctness.
Figure 4: Information gain.
Figure 7:	Parameter sensitivity of score drop ratio ps and score dimension h on four datasets.
Figure 6: Model performance varying with thepooling ratio and number of model layers.
Figure 8:	The robustness of our method against edge perturbations using graph classification resultsof different drop edge ratios. Solid lines denote the mean, and shaded areas denote the variance.
Figure 9:	Illustration of node-feature diversity and graph-structure diversity. Nodes highlighted inthe same color carry similar feature while nodes in different colors carry dissimilar features.
Figure 10:	Reconstruction results on the ZINC dataset with different pooling ratios. Solid linesdenote the mean, and the shaded areas denote the variance.
Figure 11:	The robustness of our method against edge perturbations. Graph classification results onfour benchmark datasets by varying the drop edge ratio. Solid lines denote the mean, and shadedareas denote the variance.
Figure 12:	Parameter sensitivity of score drop ratio p and score dimension h on four datasets withthe TopKPool model.
Figure 13:	Our method enable the pooled graph of backbone pooling methods to be more unique.
Figure 14:	Accuracy results varying with different hidden dimensions. 128/2 means that the dimen-sion of the embedding vector is 128, and the dimension of score, h, is 4.
Figure 15: Detailed visualization of node selection results at first layer in SAGPool and SAGPoolwith dropscore operation. Selected nodes are highlighted in red.
Figure 16: Backbone modelsHardware Environments. Each experiment was run on a single GPU (NVIDIA V100 with a 16GB memory size) and experiments were run on the server at any given time.
Figure 17: The illustration of model architectures.
