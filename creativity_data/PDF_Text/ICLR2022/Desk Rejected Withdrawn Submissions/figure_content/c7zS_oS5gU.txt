Figure 1: Local models acting as multiple teachers trained locally on user data while the globalmodel acts as a student. The global model can only observe predictions of local models.
Figure 2: An illustration—a three-class classifier withbias B and outputs a distribution P = (p1, p2, p3)for certain input. The green arrow denotes direction ofincreased confidence score with equiconfident arcs.
Figure 3: Semantic coordinates of SA, ERC, and NLI.
Figure 4: Box plot showing expectation of outputprobabilities for SA task. Horizontal and verticalaxes denote ground truth sentiment labels.
Figure 5: After the model parameter fitting is complete, we map the 128-dimensional [CLS] vectorat the output of the Trasnformer layer to 2-dimensions usgin t-SNE.
Figure 6:	The one-hot encoding doesn't have clear distinct boundaries. The semantic structure islost.
Figure 7:	With a small entropic regularizer ε, it is visually striking that Sinkhorn seems to learn thelatent structure boundaries better than KL divergence. For high ε, we see more clusters with mixedboundaries and not so clear demarcations.
