Figure 1:	Illustration of ambiguity adaptive inference and single-shot based channel pruning forsatellite processing environments3Under review as a conference paper at ICLR 20223.1	Ambiguity Adaptive Inference ModelAs the task under noisy or low resolution data makes the DL model hard to achieve the high taskperformance (i.e., accuracy), the recent studies (Zhang et al., 2019; Mo et al., 2021) attempt toovercome it by mitigating on efficient training process. However, unless the model can not guaranteethe 100% accuracy, the model still can invoke false alarm which is critical to certain environmentssuch as remote sensing (Li et al., 2020). Therefore, as an attempt to enhance the performance inthe inference step directly, we propose a hypothesis that ambiguity on inference result of DL modelcan represent the error. From the hypothesis, we can attempt to improve the performance of theDL model in inference step by adaptively revising the inference results with high ambiguity fromexternal knowledge.
Figure 2:	An example illuStration of ambiguity adaptive inference model4Under review as a conference paper at ICLR 2022samples with maxc∈C P(y = c|x) ≤ σ, where σ denotes the certain ambiguity threshold value), theprediction (Prev ) is revised by multiplying the inference result with prediction from the knowledgegraph as:Prev (y=cιx)=p(y=ciχ,θtask) ∙P-XpaiA二Paθ(;(：；)),	(1)c∈C	a∈A a, macwhere θtask denotes parameters of original model for task, θmac denotes parameters of multi-attribute classifier, pa,θmac denotes derived probability of a attribute occurrence on input x frommulti-attribute classifier θmac, and p(y = c|x, θtask) is the probability of class c for inputx derived from task model with θtask. Otherwise, on low ambiguity cases (i.e., samples withmaxc∈C p(y = c|x) > σ), just the derived inference result from the task model is used to makethe final decision for the task. As observed in Lemma 1, if we can set the appropriate thresholdfor discriminating high ambiguity case that can screen out the wrong inference results with highconfidence, overall task accuracy can be improved by trying to correct wrong predictions in furtheradaptive revision step.
Figure 3: Correct/wrong detecting accuracy fordiscriminated low/high ambiguity cases overvarious threshold levels(aʌɔe,inɔɔv"8176Figure 4: Top-1 test accuracy achieved by am-biguity adaptive inference model over variousthreshold levels7Under review as a conference paper at ICLR 2022Figure 5: Test accuracy with respect to thepercentage of remaining channels over pruningmethodsRemaining Channels (%)Figure 6: Convergence of test accuracy on train-ing the model pruned by the proposed schemeover various sparsity levelssult from the DL model as like Liu et al. (2020b). The prediction from the knowledge graph onlyjust achieved about 30% accuracy. Figure 4 shows the top-1 test accuracy of the ambiguity adaptiveinference model with regard to the ambiguity threshold (σ). The result shows that our ambiguity
Figure 4: Top-1 test accuracy achieved by am-biguity adaptive inference model over variousthreshold levels7Under review as a conference paper at ICLR 2022Figure 5: Test accuracy with respect to thepercentage of remaining channels over pruningmethodsRemaining Channels (%)Figure 6: Convergence of test accuracy on train-ing the model pruned by the proposed schemeover various sparsity levelssult from the DL model as like Liu et al. (2020b). The prediction from the knowledge graph onlyjust achieved about 30% accuracy. Figure 4 shows the top-1 test accuracy of the ambiguity adaptiveinference model with regard to the ambiguity threshold (σ). The result shows that our ambiguityadaptive inference model can achieve accuracy enhancement at certain threshold values (0.4 or 0.5)although the accuracy of the revision step only is lower than the original inference step. For the highthreshold values, as more correct samples are detected as the high ambiguity cases and the accuracyof revision step only is lower than the original step, the overall accuracy fall down by wrong revisionon correct samples.
Figure 5: Test accuracy with respect to thepercentage of remaining channels over pruningmethodsRemaining Channels (%)Figure 6: Convergence of test accuracy on train-ing the model pruned by the proposed schemeover various sparsity levelssult from the DL model as like Liu et al. (2020b). The prediction from the knowledge graph onlyjust achieved about 30% accuracy. Figure 4 shows the top-1 test accuracy of the ambiguity adaptiveinference model with regard to the ambiguity threshold (σ). The result shows that our ambiguityadaptive inference model can achieve accuracy enhancement at certain threshold values (0.4 or 0.5)although the accuracy of the revision step only is lower than the original inference step. For the highthreshold values, as more correct samples are detected as the high ambiguity cases and the accuracyof revision step only is lower than the original step, the overall accuracy fall down by wrong revisionon correct samples.
Figure 6: Convergence of test accuracy on train-ing the model pruned by the proposed schemeover various sparsity levelssult from the DL model as like Liu et al. (2020b). The prediction from the knowledge graph onlyjust achieved about 30% accuracy. Figure 4 shows the top-1 test accuracy of the ambiguity adaptiveinference model with regard to the ambiguity threshold (σ). The result shows that our ambiguityadaptive inference model can achieve accuracy enhancement at certain threshold values (0.4 or 0.5)although the accuracy of the revision step only is lower than the original inference step. For the highthreshold values, as more correct samples are detected as the high ambiguity cases and the accuracyof revision step only is lower than the original step, the overall accuracy fall down by wrong revisionon correct samples.
