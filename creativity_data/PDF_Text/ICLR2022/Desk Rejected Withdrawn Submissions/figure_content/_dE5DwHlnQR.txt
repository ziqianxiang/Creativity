Figure 1: The figure demonstrates a typical case of a causal system, and we extend to two moregeneral settings: (a) the causal graph without confounders, (b) the causal graph with confounder.
Figure 2: Results under different adversarial perturbation degree β on three datasets. Axis-X is theattack degree β. Axis-y is the adv-AUC under attacked test dataset.
Figure 3: The figure demonstrates the model architecture of CaRRThe figure shows the model architecture. The model consists of two parts, the representation learningpart and downstream prediction part. As illustrated in Section 6.2, our final objective is:ED[Ez0∈B(z,β)[logPg(y|z0)] - XDKL(qψ(z∖x)∖∖pθ(z)) - E*∈B(z,β)[logPg(y∣z0)]]	(43)For the representation learning part, we firstly use encode function φ(∙) to get representation Z and getthe intervened z. Then we perturb the learned Z by PGD attack procedure and perturb the Z by randomperturbation to find the worst case correspnding to the worst downstream loss. Finally we put z0 andz0 into the downstream prediction model g(∙) to calculate y. The likelihood in Eq. 43 is estimated bycross entropy loss. Note that the perturbation approach would block the gradient propagation betweenrepresentation learning process and downstream prediction by some implementation ways. Thus17Under review as a conference paper at ICLR 2022we use the conditional Gaussian prior pθ(z) = N (y1, I) rather than standard Gaussian distributionpθ(z) = N(0, I) to calculate KL term. If gradient propagation is blocked, by using conditional prior,the learning process of representation z and exogenous embedded in z0 will not be influenced. Theform of conditional Gaussian prior is more general pθ(Z) = N(Z(y), I), where Z(∙) could be any nontrivial function like linear function even neural network.
