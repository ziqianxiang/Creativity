Figure 1: Examples of text-to-image generation on COCO. Current approaches only generate low-quality images with unrealistic objects. In contrast, our method can produce realistic images, in termsof both visual appearances and geometric structure.
Figure 3: The architecture of our proposed method. The red box indicates the inference pipelinethat retrieves image features from a memory bank according to the a given description S, during intraining, we directly feed image features from the text-paired training image. z is a random vectordrawn from the Gaussian distribution, ACM denotes the text-image affine combination module.
Figure 2: The design of the memory bank to pro-vide image features at inference time. Note thatwe use the corresponding real training image torepresent image features for a better visualization.
Figure 5: Qualitative results on CUB and COCO: top row is the given unseen sentences; middle row:the image features extracted from the memory bank M (we use corresponding images to representthe image features for a better visualization); bottom row: the synthetic results.
Figure 7: Diversity. Top row shows the fixed sen-tence and image features, where we use the corre-sponding images to represent image features for abetter visualization. The bottom presents diversesynthetic images produced by only changing theinput noise z .
Figure 8: The architecture of the affine combination module.
Figure 9: Human evaluation between DF-GAN and ours on CUB and COCO datasets. Of thedecisions with 5/5 majority voting, the results produced by our method are most preferred by workerson both alignment and realism.
Figure 10: Qualitative results on CUB: top row is the given unseen sentences; middle row: the imagefeatures extracted from the memory bank M (we use corresponding images to represent the imagefeatures for a better visualization); bottom row: the synthetic results.
Figure 11: Semantic information exploration. Toprow: given sentences; middle row: image features,where we use corresponding segmentation masksto represent the image features for a better visual-ization; bottom row: synthetic images.
Figure 12: Our method can produce realistic im-ages even if image features partially match thegiven text description. To observe this situation,we manually feed partially matched pairs into thenetwork.
Figure 13: Effectiveness of regional selection ef-fect. “Original Text” denotes the correspondingdescription in the dataset matching the given im-age prior, “Given Text” denotes the description fedinto the network along with the image features.
Figure 15:	Additional comparison results between StackGAN++, AttnGAN, DF-GAN, and Ours onthe COCO dataset.
Figure 16:	Additional comparison results between StackGAN++, AttnGAN, DF-GAN, and Ours onthe COCO dataset.
Figure 17:	Additional comparison results between StackGAN++, AttnGAN, DF-GAN, and Ours onthe COCO dataset.
Figure 18:	Additional comparison results between StackGAN++, AttnGAN, DF-GAN, and Ours onthe COCO dataset.
