Figure 1: Given a trained generator G(∙), We have two synthesis on the first two columns.On thefirst identity’s face we randomly sample two keypoints. We extract each point’s feature from G andcalculate the feature similarity between the keypoints’ features and the feature map of the secondidentity. The third and fourth columns show the similarity heatmap where the highlights positionhave the same semantic meaning with the keypoints, which means they are dense correspondencesto each other. In this work we leverage this prior from pretrained NeRF-based GANs to learn 3DNeRF dense correspondence.
Figure 2: Demonstration of our deformation field through texture transfer. For each samples wetransfer the texture from target NeRF according to the their correspondences. This is achieved byvolume rending over deformed geometry and texture from the target NeRF. The first two rows showthe forward deformation using F(∙) which map a NeRF to the template, and the last two rows showthe inverse mapping from template to the instance via B(∙).
Figure 3: Overview of the generator feature extractor f . We use multiple layers from differentchannel as the final extracted feature. The generator G(∙) takes in a point P ∈ R3 and a modulationsignal (β, γ) as input to to control the generated content.
Figure 4: OUr model contains two mapping functions: the forward mapping F(∙) to map pointfrom a NeRF instance PN to its corresponding point on template pN0, and vice Cersa for B(∙). Toregularize the mapping, we introduce cycle consistency regularisation that captures the principle thatthe bijective mapping of a point should remain itself.
Figure 5: Demonstration of our deformation field through texture transfer. For each samples wetransfer the texture from target NeRF according to the their correspondences. This is achieved byvolume rending over deformed geometry and texture from the target NeRF. The first two rows showthe forward deformation using F(∙) which map a NeRF to the template, and the last two rows showthe inverse mapping from template to the instance via B(∙). Best viewed with zoom4	Experiments4.1	Experiment SetupHere we adopt π-GAN (Chan et al., 2021) as the NeRF-based Generator for dataset generation andpoint feature extraction 3 and report visual results over Human Face and Cats dataset. Please see thesupplementary material for more implementation details.
Figure 6: Rendering from self-reconstructed point through cycle deformation. From the left is theinput image, rendered reconstructed NeRF with generative feature similarity loss only, and on theright we add cycle-consistency loss. Note that deformation model trained with cycle-consistencyloss can perfectly reconstruct itself, which means the learned correspondence are consistent acrossshapes.
Figure 7: Output from deformation network trained without and with point-pair loss. Without point-pair regularisations, the deformation network tends output distorted visual results. We show defor-mation results from B on the top row and F on the second row.
Figure 8: Given two randomly sampled identities, we transfer the texture from the second to the firstvia dense correspondence query bridged by template NeRF. We show six cases here.
