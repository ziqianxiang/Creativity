Figure 1: Examples of unsupervised top-3 predictions of Twist. The predicted class indices aremapped to the labels in ImageNet by KUhn-MUnkreS algorithm (Kuhn, 1955). Note that the labelsare only used to map our predictions to the ImageNet labels, we do not use any label to participatein the training process. More examples are given in Appendix.
Figure 2: Network architecture of Twist.
Figure 3: We show the statistical characteristics of the output before softmax operation with andwithout NBS in (a),(b) and (c), and the loss change for each term, in (d),(e) and (f).
Figure 4: (a) Effect of different numbers of classes in TWIST. (b) Effect of different training epochsin Twist, where ”mc” denotes multi-crop and ”sl” denotes self-labeling.
Figure 5: Evolution of the predictions for one view. We set B = C = 8 to give a clear demonstra-tion.
Figure 6: Evolution of the predictions for both two views. We set B = C = 64.
Figure 7: The distributions of positive/negative similarities. IoU is the area of overlap betweenpositive and negative distributions.
Figure 8: Randomly chosen classes. We randomly choose 24 classes for visualization. For eachclass, we randomly choose 25 pictures to display. Note we did not make any selections on thepictures or the categories, all the categories and images are randomly chosen to give readers theaccurate impression.
