Figure 1: Left: uncurated 256× 256 results generated for FFHQ with our STrans-G. Right: 128× 128conditional samples from our STrans-G with AdaBN-T injecting ImageNet class information.
Figure 2: (a) illustrates the overall structure of the baseline generator, Trans-G, consisting of stan-dard Transformer blocks. (b) An overview of STrans-G that adopts localized attention modules inhigh-resolution stages.
Figure 3: (a, b) show 64 × 64 samples from Trans-G and STrans-G, respectively. (c) We analyze thefailure cases from Trans-G. Given a query pixel at the starting point, the yellow arrow in (c) points tothe pixels obtaining the highest attention score in the last attention layer. (d) We plot the distributionof the averaged attention distance, indicating the spatial distance from the query token position tothe location that modules attend to. The statistics are calculated in the last stage of the generator.
Figure 4: (a) The overall architecture of STrans-D. In (b), we illustrate the detailed architecture ofthe Swin Transformer block with skip-proj in STrans-D. (c) presents the norm ratio between theshortcut (blue path in (b)) and the main branch that contains self-attention and MLP blocks. ‘Ai’and ‘Mj’ denotes the i-th attention block and the j-th MLP block, respectively. The red vertical linein (c) represents the position of a downsampling operation. At the top of (c), we offer the resolutionof the features in the current stage. We train each configuration with three different random seeds.
Figure 5: (a)-(c) show three alternative designs of adopting AdaNorm in a Transformer block. (d)presents the norm ratio between the shortcut (blue path in (a)-(c)) and the main branch. ‘Ai’ and‘Mj’ denote the i-th attention block and the j-th MLP block, respectively. The red vertical line in(d) represents the position of an upsampling operation. At the top of (d), we offer the resolution ofthe features in the current stage. We train each configuration with three different random seeds. Theerror bar on each curve represents the variance across different runs.
Figure 6: Exemplar images generated from STrans-G in unconditional and conditional settings.
Figure 7: (a) The FID for various discriminator configurations described in Sec. 3.2. (b) We plot theevaluation FID during the training stage for each configuration in (a). (c) In FFHQ 2562, we presentthe evolution of two indicators proposed in ADA (Karras et al., 2020a), i.e., overfitting heuristic r,and augmentation strength p.
Figure 8: (a) illustrates the ‘toRGB’ layers used in our STrans-G. (b) Architecture of the convolu-tional residual block used to extract input tokens for discriminators.
Figure 9: (a) We show how different normalization layers in AdaNorm-T influence the quality ofconditional generation in different datasets. To further show the influence in the training process,(b) plots the FID curve during training in CIFAR10. In (c), we only plot the training process at theearly stage to show the differences between AdaIN-T and AdaBN-T in ImageNet.
Figure 10: Unconditional FFHQ 2562 samples from our STrans-G.
Figure 11: Unconditional CELEBA 642 samples from our STrans-G.
Figure 12: Conditional CIFAR10 322 samples from our STrans-G with AdaIN-T. Each row presentsthe samples from one category in CIFAR10.
Figure 13: Conditional IMAGENET 1282 samples from our STrans-G with AdaBN-T. We adopt atruncation threshhold of 0.5 when performing random sampling.
Figure 14: Examples for latent space interpolation in FFHQ 2562 .
