Figure 1: Illustration of the impact of softmax gradient tampering on probability values of an ar-bitrary distribution (number of classes, C = 10). As α reduces from 1 to 0, the probability dis-tributions (left: unordered or right: ordered) becomes flatter and flatter, approaching the uniformdistribution of 1/C for every index.
Figure 2: Results of grid search on different architectures and datasets. Areas with red shading aremore likely to have erratic performance.
Figure 3: Plots of various statistical measures: (a) Variations in the logit norm vs. α. The logitnorm is calculated per image over the test set of ImageNet-1K (at the linear layer of the ResNet-18 architecture) and then averaged. (b) Variations in the final loss values obtained for different αsettings. (c) Correlation between loss and logits. Regression line equation :(0.01074x + 0.25816).
Figure 4: Log-linear plots of training and test accuracies and comparison with baseline: (a) ResNet-18 (α = 0.25), (b) ResNet-50 (α = 0.3). SGT (Softmax Gradient Tampering) improves uponbaseline in all cases.
Figure 5: Python code of Softmax Gradient Tampering based on PyTorch.
