Figure 1: GroWNet architecture. After the first weak learner, each predictor is trained on combinedfeatures from original input and penultimate layer features from previous weak learner. The finaloutput is the weighted sum of outputs from all predictors, Pk=K ak fk (x). Here Model K meansweak learner K.
Figure 2: Classification training losses6.1 Stacked versus simple versionAs seen in Figure 1, every weak learner except the first one is trained on the combined features of theoriginal input and penultimate layer’s features from previous predictive function. It is worth to notethat the input dimension does not grow by iteration; indeed, it is always the dimension of hiddenlayer plus the dimension of the original input. This idea of stacked features has a resemblanceto auto-context Tu & Bai (2010) in literature, where the authors utilized the direct output of theclassifier, along with the original inputs, to boost the image segmentation performance. The workin Becker et al. (2013) extended this idea to not only use the output of the classifier, namely classprobabilities, but also the raw prediction image itself. Our model is significantly different from thesemethods, as we do not simply use the previous model’s output but more expressive representation atthe penultimate layer. These features leverage our model by propagating more complex informationfrom previous model to the new one. Moreover, we utilize stacked features to introduce a novelboosting scheme. Instead of weighted sampling, we used the penultimate layer features of theprevious weak learner and let the next learner to decide on selecting or fusing any features. Inaddition to empirical evidence, Theorem 1 in Tu & Bai (2010) proves that stacked features, referredto as auto-context, monotonically decrease the training error. The results of the theorem is notbounded by any particular classifier type, thus, the proof can also be applied to the proposed model.
Figure 3: Boosting rate evolutionIn this experiment, we explored the impact of first and second order statistics on model performanceas well as on the convergence of training loss. As the forth column of Table 4 displays, using thesecond order (third column in the Table 4) renders a slight performance boost over the first orderin classification and almost 2% increase in learning to rank task. Figure 2 displays the effects offirst and second order statistics on training loss. The final model (with second order statistics) againshows slightly better convergence on classification yet the difference is more apparent on ranking.
Figure 4: Effect of # neurons onclassification performance6.5	GrowNet versus DNNOne might ask what would happen if we just combine all these shallow networks into one deepneural network. There are a couple of issues with this approach: (1) it is very time-consuming totune the parameters of the DNN, such as the number of hidden layers, the number of units in eachhidden layer, the overall architecture, batch normalization, dropout level, and etc., (2) DNNs requirea huge computational power and in general run slower. We compared our model (with 30 weaklearners) against DNN with 5, 10 , 20, and 30 hidden-layer configurations. The best DNN (with 10hidden layers) produced 0.8342 on Higgs 1M data in 1000 epochs, and each epoch took 11 seconds.
