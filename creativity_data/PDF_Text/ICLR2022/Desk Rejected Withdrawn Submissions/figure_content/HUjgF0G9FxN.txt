Figure 1: An illustration of (a) vanilla extension of FL with SSL and (b) SemiFL. (a) The vanillaextension trains and aggregates server and clients models in parallel and generates pseudo-labels withthe training models for each batch of unlabeled data. (b) The SemiFL updates the aggregated clientmodel with labeled data (also named “Fine-tuning”) and generates pseudo-labels only once with themodel received from the server.
Figure 2: Experimental results for CIFAR10 dataset with (a) NS = 250 and (b) NS = 4000.
Figure 3: EXperimental results for SVHN dataset with (a) NS = 250 and (b) NS = 1000.
Figure 4: Experimental results for CIFAR100 dataset with (a) NS = 2500 and (b) NS = 10000.
Figure 5: Ablation study on the CIFAR10 dataset with 4000 labeled data at the server, for the casesof (a) IID and (b) Non-IID, K = 2 data partition.
Figure 6: Illustration of the strong data augmentation-based SSL. We pick up an unlabeled point(X 〜PU) with a high-confidence pseudo-label, obtain its hard-thresholded label (Y, which isbelieved to be close to the ground-truth), maneuver X into X (which is believed to represent thetest distribution Pl to some extent), and then treat (Y, X) as labeled data for training. Consequently,reliable task-specific information exhibited from unlabeled data can be transmitted to data regimesthat may have been insufficiently trained with labeled data. Note that Pl denotes the labeled datadistribution as well as the out-sample test data distribution (used to evaluate the learning performance).
Figure 7: Example of strong data augmentations based on the RandAugment technique (Cubuk et al.,2020). As the distortion magnitude increases, the strength of the augmentation increases. Here,“ShearX” means shearing the image along the horizontal axis, and “AutoConstrast” means maximizingthe image contrast by setting the darkest (respectively lightest) pixel to black (respectively white).
