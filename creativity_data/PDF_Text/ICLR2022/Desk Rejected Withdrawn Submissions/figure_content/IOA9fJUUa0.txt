Figure 1: Example sentences used in the BERT training (-ey, CRT)3.3	Developing BERT classification modelWe set the parameters related to BERT training such as batch size (32), epoch (50), seed (42), epsilon(0.00000008), and learning rate (0.00002), as advised by McCormick (2019). We then employed apre-trained language model in order to obtain high accuracy of outcomes; for this purpose, we used aKorean BERT model (KoBERT; Jeon et al., 2019). Before the actual BERT training, we transformedthe input data into three embedding types—token embeddings, position embeddings, and segmentembeddings (cf., Devlin et al., 2018)—in the following ways.
Figure 2: The overall interface of the visualization system (Available at:http://13.125.253.195/PostBERT/).
Figure 3: The distributional map for -(u)lo in Epoch 12.
Figure 4: The DIR cluster in the distributional map for -(u)lo (Epoch 46) highlighting the LOCinstances.
